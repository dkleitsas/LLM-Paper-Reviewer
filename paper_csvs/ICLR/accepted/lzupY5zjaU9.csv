Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0011668611435239206,"In distribution compression, one aims to accurately summarize a probability dis-
tribution P using a small number of representative points. Near-optimal thinning
procedures achieve this goal by sampling n points from a Markov chain and iden-
tifying √n points with e
O(1/√n) discrepancy to P. Unfortunately, these algo-
rithms suffer from quadratic or super-quadratic runtime in the sample size n. To
address this deficiency, we introduce Compress++, a simple meta-procedure for
speeding up any thinning algorithm while suffering at most a factor of 4 in er-
ror. When combined with the quadratic-time kernel halving and kernel thinning
algorithms of Dwivedi and Mackey (2021), Compress++ delivers √n points with
O(
p"
ABSTRACT,0.002333722287047841,"log n/n) integration error and better-than-Monte-Carlo maximum mean dis-
crepancy in O(n log3 n) time and O(√n log2 n) space. Moreover, Compress++
enjoys the same near-linear runtime given any quadratic-time input and reduces
the runtime of super-quadratic algorithms by a square-root factor. In our bench-
marks with high-dimensional Monte Carlo samples and Markov chains target-
ing challenging differential equation posteriors, Compress++ matches or nearly
matches the accuracy of its input algorithm in orders of magnitude less time."
INTRODUCTION,0.003500583430571762,"1
INTRODUCTION"
INTRODUCTION,0.004667444574095682,"Distribution compression—constructing a concise summary of a probability distribution—is at the
heart of many learning and inference tasks. For example, in Monte Carlo integration and Bayesian
inference, n representative points are sampled i.i.d. or from a Markov chain to approximate ex-
pectations and quantify uncertainty under an intractable (posterior) distribution (Robert & Casella,
1999). However, these standard sampling strategies are not especially concise. For instance, the
Monte Carlo estimate Pinf ≜1"
INTRODUCTION,0.005834305717619603,"n
Pn
i=1 f(xi) of an unknown expectation Pf ≜EX∼P[f(X)] based
on n i.i.d. points has Θ(n−1"
INTRODUCTION,0.007001166861143524,"2 ) integration error |Pf −Pinf|, requiring 10000 points for 1% rela-
tive error and 106 points for 0.1% error. Such bloated sample representations preclude downstream
applications with critically expensive function evaluations like computational cardiology, where a
1000-CPU-hour tissue or organ simulation is required for each sample point (Niederer et al., 2011;
Augustin et al., 2016; Strocchi et al., 2020)."
INTRODUCTION,0.008168028004667444,"To restore the feasibility of such critically expensive tasks, it is common to thin down the initial
point sequence to produce a much smaller coreset. The standard thinning approach, select every
t-th point (Owen, 2017), while being simple often leads to an substantial increase in error: e.g.,
standard thinning n points from a fast-mixing Markov chain yields Ω(n−1"
INTRODUCTION,0.009334889148191364,"4 ) error when n
1
2 points
are returned. Recently, Dwivedi & Mackey (2021) introduced a more effective alternative, kernel
thinning (KT), that provides near optimal e
Od(n−1"
INTRODUCTION,0.010501750291715286,"2 ) error when compressing n points in Rd down to
size n
1
2 . While practical for moderate sample sizes, the runtime of this algorithm scales quadratically
with the input size n, making its execution prohibitive for very large n. Our goal is to significantly
improve the runtime of such compression algorithms while providing comparable error guarantees."
INTRODUCTION,0.011668611435239206,"Problem setup
Given a sequence Sin of n input points summarizing a target distribution P, our
aim is to identify a high quality coreset Sout of size √n in time nearly linear in n. We measure
coreset quality via its integration error |Pf −PSoutf| ≜|Pf −
1
|Sout|
P"
INTRODUCTION,0.012835472578763127,x∈Sout f(x)| for functions
INTRODUCTION,0.014002333722287048,"f in the reproducing kernel Hilbert space (RKHS) Hk induced by a given kernel k (Berlinet &
Thomas-Agnan, 2011). We consider both single function error and kernel maximum mean discrep-
ancy (MMD, Gretton et al., 2012), the worst-case integration error over the unit RKHS norm ball:"
INTRODUCTION,0.015169194865810968,"MMDk(P, PSout) ≜sup∥f∥k≤1|Pf −PSoutf|.
(1)"
INTRODUCTION,0.01633605600933489,"Our contributions We introduce a new simple meta procedure—COMPRESS++—that significantly
speeds up a generic thinning algorithm while simultaneously inheriting the error guarantees of its
input up to a factor of 4. A direct application of COMPRESS++ to KT improves its quadratic Θ(n2)
runtime to near linear O(n log3 n) time while preserving its error guarantees. Since the e
Od(n−1"
INTRODUCTION,0.01750291715285881,"2 )
KT MMD guarantees of Dwivedi & Mackey (2021) match the Ω(n−1"
INTRODUCTION,0.01866977829638273,"2 ) minimax lower bounds of
Tolstikhin et al. (2017); Phillips & Tai (2020) up to factors of √log n and constants depending on
d, KT-COMPRESS++ also provides near-optimal MMD compression for a wide range of k and P.
Moreover, the practical gains from applying COMPRESS++ are substantial: KT thins 65, 000 points
in 10 dimensions in 20m, while KT-COMPRESS++ needs only 1.5m; KT takes more than a day to
thin 250, 000 points in 100 dimensions, while KT-COMPRESS++ takes less than 1hr (a 32× speed-
up). For larger n, the speed-ups are even greater due to the order
n
log3 n reduction in runtime."
INTRODUCTION,0.019836639439906652,"COMPRESS++ can also be directly combined with any thinning algorithm, even those that have
suboptimal guarantees but often perform well in practice, like kernel herding (Chen et al., 2010),
MMD-critic (Kim et al., 2016), and Stein thinning (Riabiz et al., 2020a), all of which run in Ω(n2)
time. As a demonstration, we combine COMPRESS++ with the popular kernel herding algorithm
and observe 45× speed-ups when compressing 250, 000 input points. In all of our experiments,
COMPRESS++ leads to minimal loss in accuracy and, surprisingly, even improves upon herding
accuracy for lower-dimensional problems."
INTRODUCTION,0.021003500583430573,"Most related to our work are the merge-reduce algorithms of Matousek (1995); Chazelle & Ma-
tousek (1996); Phillips (2008) which also speed up input thinning algorithms while controlling ap-
proximation error. In our setting, merge-reduce runs in time Ω(n1.5) given an n2-time input and in
time Ω(n(τ+1)/2) for slower nτ-time inputs (see, e.g., Phillips, 2008, Thm. 3.1). In contrast, COM-"
INTRODUCTION,0.022170361726954493,"PRESS++ runs in near-linear O(n log3 n) time for any n2-time input and in O(nτ/2 logτ n) time for
slower nτ-time inputs. After providing formal definitions in Sec. 2, we introduce and analyze COM-
PRESS++ and its primary subroutine COMPRESS in Secs. 3 and 4, demonstrate the empirical benefits
of COMPRESS++ in Sec. 5, and present conclusions and opportunities for future work in Sec. 6."
INTRODUCTION,0.023337222870478413,"Notation We let PS denote the empirical distribution of S. For the output coreset SALG of an algo-
rithm ALG with input coreset Sin, we use the simpler notation PALG ≜PSALG and Pin ≜PSin. We
extend our MMD definition to point sequences (S1, S2) via MMDk(S1, S2) ≜MMDk(PS1, PS2)
and MMDk(P, S1) ≜MMDk(P, PS1). We use a ≾b to mean a = O(b), a ≿b to mean a = Ω(b),
a = Θ(b) to mean both a = O(b) and a = Ω(b), and log to denote the natural logarithm."
THINNING AND HALVING ALGORITHMS,0.024504084014002333,"2
THINNING AND HALVING ALGORITHMS
We begin by defining the thinning and halving algorithms that our meta-procedures take as input."
THINNING AND HALVING ALGORITHMS,0.025670945157526253,"Definition 1 (Thinning and halving algorithms) A thinning algorithm ALG takes as input a point
sequence Sin of length n and returns a (possibly random) point sequence SALG of length nout. We
say ALG is αn-thinning if nout = ⌊n/αn⌋and root-thinning if αn = √n. Moreover, we call ALG a
halving algorithm if SALG always contains exactly ⌊n"
THINNING AND HALVING ALGORITHMS,0.026837806301050177,2 ⌋of the input points.
THINNING AND HALVING ALGORITHMS,0.028004667444574097,"Many thinning algorithms offer high-probability bounds on the integration error |PSinf −PSALGf|.
We capture such bounds abstractly using the following definition of a sub-Gaussian thinning"
THINNING AND HALVING ALGORITHMS,0.029171528588098017,"Definition 2 (Sub-Gaussian thinning algorithm) For a function f, we call a thinning algorithm
ALG f-sub-Gaussian with parameter ν and write ALG ∈Gf(ν) if"
THINNING AND HALVING ALGORITHMS,0.030338389731621937,"E[exp(λ(PSinf −PSALGf)) | Sin] ≤exp

λ2ν2(n)"
THINNING AND HALVING ALGORITHMS,0.03150525087514586,"2

for all
λ ∈R."
THINNING AND HALVING ALGORITHMS,0.03267211201866978,"Def. 2 is equivalent to a sub-Gaussian tail bound for the integration error, and, by Boucheron et al.
(2013, Section 2.3), if ALG ∈Gf(ν) then E[PSALGf | Sin] = PSinf and, for all δ ∈(0, 1),"
THINNING AND HALVING ALGORITHMS,0.0338389731621937,"|PSinf −PSALGf| ≤ν(n)
q"
THINNING AND HALVING ALGORITHMS,0.03500583430571762,2 log( 2
THINNING AND HALVING ALGORITHMS,0.03617269544924154,"δ ), with probability at least 1 −δ given Sin."
THINNING AND HALVING ALGORITHMS,0.03733955659276546,Hence the integration error of ALG is dominated by the sub-Gaussian parameter ν(n).
THINNING AND HALVING ALGORITHMS,0.038506417736289385,"Example 1 (KT-SPLIT) Given a kernel k and n input points Sin, the KT-SPLIT(δ) algorithm1 of
Dwivedi & Mackey (2022; 2021, Alg. 1a) takes Θ(n2) kernel evaluations to output a coreset of size
nout with better-than-i.i.d. integration error. Specifically, Dwivedi & Mackey (2022, Thm. 1) prove
that, on an event with probability 1 −δ"
THINNING AND HALVING ALGORITHMS,0.039673278879813305,"2, KT-SPLIT(δ) ∈Gf(ν) with"
THINNING AND HALVING ALGORITHMS,0.040840140023337225,"ν(n) =
2
nout
√ 3 q"
THINNING AND HALVING ALGORITHMS,0.042007001166861145,log( 6nout log2(n/nout)
THINNING AND HALVING ALGORITHMS,0.043173862310385065,"δ
)∥k∥∞
(2)"
THINNING AND HALVING ALGORITHMS,0.044340723453908985,"for all f with ∥f∥k = 1.
■"
THINNING AND HALVING ALGORITHMS,0.045507584597432905,"Many algorithms also offer high-probability bounds on the kernel MMD (1), the worst-case inte-
gration error across the unit ball of the RKHS. We again capture these bounds abstractly using the
following definition of a k-sub-Gaussian thinning algorithm."
THINNING AND HALVING ALGORITHMS,0.046674445740956826,"Definition 3 (k-sub-Gaussian thinning algorithm) For a kernel k, we call a thinning algorithm
ALG k-sub-Gaussian with parameter v and shift a and write ALG ∈Gk(v, a) if"
THINNING AND HALVING ALGORITHMS,0.047841306884480746,"P[MMDk(Sin, SALG) ≥an + vn
√"
THINNING AND HALVING ALGORITHMS,0.049008168028004666,"t
 Sin] ≤e−t
for all
t ≥0.
(3)"
THINNING AND HALVING ALGORITHMS,0.050175029171528586,"We also call εk,ALG(n) ≜max(vn, an) the k-sub-Gaussian error of ALG."
THINNING AND HALVING ALGORITHMS,0.051341890315052506,"Example 2 (Kernel thinning) Given a kernel k and n input points Sin, the generalized kernel thin-
ning (KT(δ)) algorithm1 of Dwivedi & Mackey (2022; 2021, Alg. 1) takes Θ(n2) kernel evaluations
to output a coreset of size nout with near-optimal MMD error. In particular, by leveraging an appro-
priate auxiliary kernel ksplit, Dwivedi & Mackey (2022, Thms. 2-4) establish that, on an event with
probability 1 −δ"
THINNING AND HALVING ALGORITHMS,0.052508751458576426,"2, KT(δ) ∈Gk(a, v) with"
THINNING AND HALVING ALGORITHMS,0.05367561260210035,"an =
Ca
nout
p"
THINNING AND HALVING ALGORITHMS,0.05484247374562427,"∥ksplit∥∞,
and
vn =
Cv
nout q"
THINNING AND HALVING ALGORITHMS,0.056009334889148193,∥ksplit∥∞log( 6nout log2(n/nout)
THINNING AND HALVING ALGORITHMS,0.057176196032672114,"δ
) MSin,ksplit,
(4)"
THINNING AND HALVING ALGORITHMS,0.058343057176196034,"where ∥ksplit∥∞= supx ksplit(x, x), Ca and Cv are explicit constants, and MSin,ksplit ≥1 is non-
decreasing in n and varies based on the tails of ksplit and the radius of the ball containing Sin.
■"
COMPRESS,0.059509918319719954,"3
COMPRESS
The core subroutine of COMPRESS++ is a new meta-procedure called COMPRESS that, given a halv-
ing algorithm HALVE, an oversampling parameter g, and n input points, outputs a thinned coreset of
size 2g√n. The COMPRESS algorithm (Alg. 1) is very simple to implement: first, divide the input
points into four subsequences of size n"
COMPRESS,0.060676779463243874,"4 (in any manner the user chooses); second, recursively call
COMPRESS on each subsequence to produce four coresets of size 2g−1√n; finally, call HALVE on
the concatenation of those coresets to produce the final output of size 2g√n. As we show in App. H,
COMPRESS can also be implemented in a streaming fashion to consume at most O(4g√n) memory."
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.061843640606767794,"3.1
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS"
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.06301050175029171,"Our first result relates the runtime and single-function integration error of COMPRESS to the runtime
and error of HALVE. We measure integration error for each function f probabilistically in terms of
the sub-Gaussian parameter ν of Def. 2 and measure runtime by the number of dominant operations
performed by HALVE (e.g., the number of kernel evaluations performed by kernel thinning)."
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.06417736289381563,"Theorem 1 (Runtime and integration error of COMPRESS) If HALVE has runtime rH(n) for in-
puts of size n, then COMPRESS has runtime"
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.06534422403733955,"rC(n) = Pβn
i=0 4i · rH(ℓn2−i),
(5)"
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.06651108518086347,"where ℓn ≜2g+1√n (twice the output size of COMPRESS), and βn ≜log2( n"
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.0676779463243874,"ℓn ) = log4 n−g−1.
Furthermore, if, for some function f, HALVE ∈Gf(νH), then COMPRESS ∈Gf(νC) with"
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.06884480746791131,"ν2
C(n) = Pβn
i=0 4−i · ν2
H(ℓn2−i).
(6)"
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.07001166861143523,"1The δ argument of KT-SPLIT(δ) or KT(δ) indicates that each parameter δi =
δ
ℓin Dwivedi & Mackey
(2022, Alg. 1a), where ℓis the size of the input point sequence compressed by KT-SPLIT(δ) or KT(δ)."
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.07117852975495916,"Algorithm 1: COMPRESS
Input: halving algorithm HALVE, oversampling parameter g, point sequence Sin of size n
if n = 4g then return Sin
Partition Sin into four arbitrary subsequences {Si}4
i=1 each of size n/4
for i = 1, 2, 3, 4 do"
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS,0.07234539089848308,"eSi ←COMPRESS(Si, HALVE, g)
// return coresets of size 2g · p n"
END,0.073512252042007,"4
end
eS ←CONCATENATE( eS1, eS2, eS3, eS4)
// coreset of size 2 · 2g · √n
return HALVE( eS)
// coreset of size 2g√n"
END,0.07467911318553092,"As we prove in App. B, the runtime guarantee (5) is immediate once we unroll the COMPRESS
recursion and identify that COMPRESS makes 4i calls to HALVE with input size ℓn2−i. The er-
ror guarantee (6) is more subtle: here, COMPRESS benefits significantly from random cancellations
among the conditionally independent and mean-zero HALVE errors. Without these properties, the
errors from each HALVE call could compound without cancellation leading to a significant degrada-
tion in COMPRESS quality. Let us now unpack the most important implications of Thm. 1."
END,0.07584597432905485,"Remark 1 (Near-linear runtime and quadratic speed-ups for COMPRESS) Thm. 1 implies that
a quadratic-time HALVE with rH(n) = n2 yields a near-linear time COMPRESS with rC(n) ≤
4g+1 n(log4(n)−g). If HALVE instead has super-quadratic runtime rH(n) = nτ, COMPRESS enjoys
a quadratic speed-up: rC(n) ≤c′
τ nτ/2 for c′
τ ≜2τ(g+2)"
END,0.07701283547257877,"2τ −4 . More generally, whenever HALVE has
superlinear runtime rH(n) = nτ ρ(n) for some τ ≥1 and non-decreasing ρ, COMPRESS satisfies"
END,0.07817969661610269,rC(n) ≤
END,0.07934655775962661,"(
cτ · n (log4(n) −g) ρ(ℓn)
for τ ≤2
c′
τ · nτ/2 ρ(ℓn)
for τ > 2
where
cτ ≜4(τ−1)(g+1)."
END,0.08051341890315053,"Remark 2 (COMPRESS inflates sub-Gaussian error by at most
p"
END,0.08168028004667445,log4 n) Thm. 1 also implies
END,0.08284714119019837,"νC(n) ≤√βn + 1 νH(ℓn) =
p"
END,0.08401400233372229,log4 n −g νH(ℓn)
END,0.08518086347724621,"in the usual case that n νH(n) is non-decreasing in n. Hence the sub-Gaussian error of COMPRESS
is at most
p"
END,0.08634772462077013,"log4 n larger than that of halving an input of size ℓn. This is an especially strong
benchmark, as ℓn is twice the output size of COMPRESS, and thinning from n to ℓn"
POINTS SHOULD,0.08751458576429405,"2 points should
incur at least as much approximation error as halving from ℓn to ℓn"
POINTS SHOULD,0.08868144690781797,2 points.
POINTS SHOULD,0.08984830805134189,"Example 3 (KT-SPLIT-COMPRESS) Consider running COMPRESS with, for each HALVE input
of size ℓ, HALVE = KT-SPLIT(
ℓ2
n4g+1(βn+1)δ) from Ex. 1. Since KT-SPLIT runs in time Θ(n2),
COMPRESS runs in near-linear O(n log n) time by Rem. 1. In addition, as we detail in App. F.1, on
an event of probability 1 −δ"
POINTS SHOULD,0.09101516919486581,"2, every HALVE call invoked by COMPRESS is f-sub-Gaussian with"
POINTS SHOULD,0.09218203033838973,"νH(ℓ) =
4
ℓ
√ 3 q"
POINTS SHOULD,0.09334889148191365,log( 12n4g(βn+1)
POINTS SHOULD,0.09451575262543757,"ℓδ
)∥k∥∞
for all f with ∥f∥k = 1.
(7)"
POINTS SHOULD,0.09568261376896149,"Hence, Rem. 2 implies that COMPRESS is f-sub-Gaussian on the same event with νC(n) ≤
p"
POINTS SHOULD,0.09684947491248541,"log4 n−g νH(ℓn), a guarantee within
p"
POINTS SHOULD,0.09801633605600933,"log4 n of the original KT-SPLIT(δ) error (2).
■"
MMD GUARANTEES FOR COMPRESS,0.09918319719953325,"3.2
MMD GUARANTEES FOR COMPRESS"
MMD GUARANTEES FOR COMPRESS,0.10035005834305717,"Next, we bound the MMD error of COMPRESS in terms of the MMD error of HALVE. Recall that
MMDk (1) represents the worst-case integration error across the unit ball of the RKHS of k. Its
proof, based on the concentration of subexponential matrix martingales, is provided in App. C."
MMD GUARANTEES FOR COMPRESS,0.10151691948658109,"Theorem 2 (MMD guarantees for COMPRESS) Suppose HALVE ∈Gk(a, v) for n an and n vn
non-decreasing and E

PHALVEk | Sin

= Pink. Then COMPRESS ∈Gk(ea, ev) with"
MMD GUARANTEES FOR COMPRESS,0.10268378063010501,"evn ≜4(aℓn +vℓn)
p"
MMD GUARANTEES FOR COMPRESS,0.10385064177362893,"2(log4 n−g),
and
ean ≜evn
p"
MMD GUARANTEES FOR COMPRESS,0.10501750291715285,"log(n+1),
(8)"
MMD GUARANTEES FOR COMPRESS,0.10618436406067679,where ℓn = 2g+1√n as in Thm. 1.
MMD GUARANTEES FOR COMPRESS,0.1073512252042007,"Remark 3 (Symmetrization) We can convert any halving algorithm into one that satisfies the un-
biasedness condition E

PHALVEk | Sin

= Pink without impacting integration error by symmetriza-
tion, i.e., by returning either the outputted half or its complement with equal probability."
MMD GUARANTEES FOR COMPRESS,0.10851808634772463,"Remark 4 (COMPRESS inflates MMD guarantee by at most 10 log(n+1)) Thm.
2
implies
that the k-sub-Gaussian error of COMPRESS is always at most 10 log(n+1) times that of HALVE
with input size ℓn =2g+1√n since"
MMD GUARANTEES FOR COMPRESS,0.10968494749124855,"εk,COMPRESS(n)
Def. 3
=
max(ean, evn)
(8)
≤10 log(n + 1) max(aℓn, vℓn) = 10 log(n + 1) · εk,HALVE(ℓn)."
MMD GUARANTEES FOR COMPRESS,0.11085180863477247,"As in Rem. 2, HALVE applied to an input of size ℓn is a particularly strong benchmark, as thinning
from n to ℓn"
MMD GUARANTEES FOR COMPRESS,0.11201866977829639,2 points should incur at least as much MMD error as halving from ℓn to ℓn 2 .
MMD GUARANTEES FOR COMPRESS,0.11318553092182031,"Example 4 (KT-COMPRESS) Consider running COMPRESS with, for each HALVE input of size ℓ,
HALVE = KT(
ℓ2
n4g+1(βn+1)δ) from Ex. 2 after symmetrizing as in Rem. 3. Since KT has Θ(n2)
runtime, COMPRESS yields near-linear O(n log n) runtime by Rem. 1. Moreover, as we detail in
App. F.2, using the notation of Ex. 2, on an event E of probability at least 1 −δ"
MMD GUARANTEES FOR COMPRESS,0.11435239206534423,"2, every HALVE call
invoked by COMPRESS is k-sub-Gaussian with"
MMD GUARANTEES FOR COMPRESS,0.11551925320886815,"aℓ= 2Ca ℓ
p"
MMD GUARANTEES FOR COMPRESS,0.11668611435239207,"∥k∥∞,
and
vℓ= 2Cv ℓ q"
MMD GUARANTEES FOR COMPRESS,0.11785297549591599,∥k∥∞log( 12n4g(βn+1)
MMD GUARANTEES FOR COMPRESS,0.11901983663943991,"ℓδ
) MSin,k."
MMD GUARANTEES FOR COMPRESS,0.12018669778296383,"Thus, Rem. 4 implies that, on E, KT-COMPRESS has k-sub-Gaussian error εk,COMPRESS(n) ≤
10 log(n+1)εk,HALVE(ℓn), a guarantee within 10 log(n+1) of the original KT(δ) MMD error (4). ■"
MMD GUARANTEES FOR COMPRESS,0.12135355892648775,"4
COMPRESS++"
MMD GUARANTEES FOR COMPRESS,0.12252042007001167,"To offset any excess error due to COMPRESS while maintaining its near-linear runtime, we next in-
troduce COMPRESS++ (Alg. 2), a simple two-stage meta-procedure for faster root-thinning. COM-
PRESS++ takes as input an oversampling parameter g, a halving algorithm HALVE, and a 2g-thinning
algorithm THIN (see Def. 1). In our applications, HALVE and THIN are derived from the same base
algorithm (e.g., from KT with different thinning factors), but this is not required. COMPRESS++ first
runs the faster but slightly more erroneous COMPRESS(HALVE, g) algorithm to produce an interme-
diate coreset of size 2g√n. Next, the slower but more accurate THIN algorithm is run on the greatly
compressed intermediate coreset to produce a final output of size √n. In the sequel, we demonstrate
how to set g to offset error inflation due to COMPRESS while maintaining its fast runtime."
MMD GUARANTEES FOR COMPRESS,0.12368728121353559,Algorithm 2: COMPRESS++
MMD GUARANTEES FOR COMPRESS,0.12485414235705951,"Input: oversampling parameter g, halving alg. HALVE, 2g-thinning alg. THIN, point sequence Sin of size n
SC
←
COMPRESS(HALVE, g, Sin)
// coreset of size 2g√n
SC++
←
THIN(SC)
// coreset of size √n
return SC++"
MMD GUARANTEES FOR COMPRESS,0.12602100350058343,"4.1
INTEGRATION ERROR AND RUNTIME GUARANTEES FOR COMPRESS++"
MMD GUARANTEES FOR COMPRESS,0.12718786464410736,"The following result, proved in App. D, relates the runtime and single-function integration error of
COMPRESS++ to the runtime and error of HALVE and THIN."
MMD GUARANTEES FOR COMPRESS,0.12835472578763127,"Theorem 3 (Runtime and integration error of COMPRESS++) If HALVE and THIN have run-
times rH(n) and rT(n) respectively for inputs of size n, then COMPRESS++ has runtime"
MMD GUARANTEES FOR COMPRESS,0.1295215869311552,"rC++(n) = rC(n) + rT(ℓn/2)
where
rC(n)
(5)= Pβn
i=0 4i · rH(ℓn2−i),
(9)"
MMD GUARANTEES FOR COMPRESS,0.1306884480746791,"ℓn = 2g+1√n, and βn = log4 n −g −1 as in Thm. 1.
Furthermore, if for some function f,
HALVE ∈Gf(νH) and THIN ∈Gf(νT), then COMPRESS++ ∈Gf(νC++) with"
MMD GUARANTEES FOR COMPRESS,0.13185530921820304,"ν2
C++(n) = ν2
C(n) + ν2
T(ℓn/2)
where
ν2
C(n)
(6)= Pβn
i=0 4−i · ν2
H(ℓn2−i)."
MMD GUARANTEES FOR COMPRESS,0.13302217036172695,"Remark 5 (Near-linear runtime and near-quadratic speed-ups for COMPRESS++) When
HALVE and THIN have quadratic runtimes with max(rH(n), rT(n)) = n2, Thm. 3 and Rem. 1 yield
that rC++(n) ≤4g+1 n(log4(n) −g) + 4gn. Hence, COMPRESS++ maintains a near-linear runtime"
MMD GUARANTEES FOR COMPRESS,0.13418903150525088,"rC++(n) = O(n logc+1
4
(n))
whenever
4g = O(logc
4 n).
(10)"
MMD GUARANTEES FOR COMPRESS,0.1353558926487748,"If HALVE and THIN instead have super-quadratic runtimes with max(rH(n), rT(n)) = nτ, then by
Rem. 1 we have rC++(n) ≤(
4τ
2τ −4 + 1) 2gτnτ/2, so that COMPRESS++ provides a near-quadratic"
MMD GUARANTEES FOR COMPRESS,0.13652275379229872,"speed up rC++(n) = O(nτ/2 logcτ/2
4
(n)) whenever 4g = O(logc
4 n)."
MMD GUARANTEES FOR COMPRESS,0.13768961493582263,"Remark 6 (COMPRESS++ inflates sub-Gaussian error by at most
√"
MMD GUARANTEES FOR COMPRESS,0.13885647607934656,"2) In the usual case that
n νH(n) is non-decreasing in n, Thm. 3 and Rem. 2 imply that"
MMD GUARANTEES FOR COMPRESS,0.14002333722287047,"ν2
C++(n) ≤(log4 n −g)ν2
H(ℓn) + ν2
T( ℓn"
MMD GUARANTEES FOR COMPRESS,0.1411901983663944,"2 ) = ν2
T( ℓn"
MMD GUARANTEES FOR COMPRESS,0.1423570595099183,"2 ) ·

1 + log4 n−g"
G,0.14352392065344224,"4g
· ( ζH(ℓn)"
G,0.14469078179696615,ζT(ℓn/2))2
G,0.14585764294049008,where we have introduced the rescaled quantities ζH(ℓn) ≜ℓn
G,0.147024504084014,2 νH(ℓn) and ζT( ℓn
G,0.14819136522753792,2 ) ≜√n νT( ℓn
G,0.14935822637106183,"2 ).
Therefore, COMPRESS++ satisfies"
G,0.15052508751458576,"νC++(n) ≤
√"
G,0.1516919486581097,2νT( ℓn
G,0.1528588098016336,"2 )
whenever
g ≥log4 log4 n + log2( ζH(ℓn)"
G,0.15402567094515754,"ζT(ℓn/2)).
(11)"
G,0.15519253208868145,"That is, whenever COMPRESS++ is run with an oversampling parameter g satisfying (11) its sub-
Gaussian error is never more than
√"
G,0.15635939323220538,"2 times the second-stage THIN error. Here, THIN represents a
strong baseline for comparison as thinning from ℓn/2 to √n points should incur at least as much
error as thinning from n to √n points."
G,0.15752625437572929,"As we illustrate in the next example, when THIN and HALVE are derived from the same thin-
ning algorithm, the ratio
ζH(ℓn)
ζT(ℓn/2) is typically bounded by a constant C so that the choice g ="
G,0.15869311551925322,"⌈log4 log4 n + log2 C⌉suffices to simultaneously obtain the
√"
G,0.15985997666277713,"2 relative error guarantee (11) of
Rem. 6 and the substantial speed-ups (10) of Rem. 5."
G,0.16102683780630106,"Example 5 (KT-SPLIT-COMPRESS++) In the notation of Ex. 1, consider running COMPRESS++
with HALVE = KT-SPLIT(
ℓ2
4n2g(g+2g(βn+1))δ) when applied to an input of size ℓand THIN ="
G,0.16219369894982497,"KT-SPLIT(
g
g+2g(βn+1)δ). As detailed in App. F.3, on an event of probability 1 −δ"
G,0.1633605600933489,"2, all COM-"
G,0.1645274212368728,"PRESS++ invocations of HALVE and THIN are simultaneously f-sub-Gaussian with parameters sat-
isfying"
G,0.16569428238039674,"ζH(ℓ)=ζT(ℓ)= 2
√ 3 q"
G,0.16686114352392065,log( 6√n(g+2g(βn+1))
G,0.16802800466744458,"δ
)∥k∥∞=⇒ζH(ℓn)"
G,0.1691948658109685,ζT( ℓn
G,0.17036172695449242,"2 ) =1 for all f with ∥f∥k = 1.
(12)"
G,0.17152858809801633,"Since KT-SPLIT runs in Θ(n2) time, Rems. 5 and 6 imply that KT-SPLIT-COMPRESS++ with g =
⌈log4 log4 n⌉runs in near-linear O(n log2 n) time and inflates sub-Gaussian error by at most
√ 2. ■"
G,0.17269544924154026,"4.2
MMD GUARANTEES FOR COMPRESS++"
G,0.17386231038506417,"Next, we bound the MMD error of COMPRESS++ in terms of the MMD error of HALVE and THIN.
The proof of the following result can be found in App. E."
G,0.1750291715285881,"Theorem 4 (MMD guarantees for COMPRESS++) If THIN ∈Gk(a′,v′), HALVE ∈Gk(a,v) for
n an and n vn non-decreasing, and E

PHALVEk | Sin

= Pink, then COMPRESS++ ∈Gk(ba, bv) with"
G,0.176196032672112,"bvn ≜evn + v′
ℓn/2
and
ban ≜ean + a′
ℓn/2 + bvn
√log 2"
G,0.17736289381563594,for evn and ean defined in Thm. 2 and ℓn = 2g+1√n as in Thm. 1.
G,0.17852975495915985,"Remark 7 (COMPRESS++ inflates MMD guarantee by at most 4) Thm.
4
implies
that
the
COMPRESS++ k-sub-Gaussian error εk,COMPRESS++(n) = max(ban, bvn) satisfies"
G,0.17969661610268378,"εk,COMPRESS++(n) ≤(10 log(n + 1) εk,HALVE(ℓn) + εk,THIN( ℓn"
G,0.1808634772462077,2 )) (1 + √log 2)
G,0.18203033838973162,"≤εk,THIN( ℓn"
G,0.18319719953325556,2 )( 10 log(n+1)
G,0.18436406067677946,"2g
eζH(ℓn)
eζT( ℓn"
G,0.1855309218203034,"2 ) + 1)(1 + √log 2),"
G,0.1866977829638273,"where we have introduced the rescaled quantities eζH(ℓn) ≜
ℓn"
G,0.18786464410735124,"2 εk,HALVE(ℓn) and eζT( ℓn"
G,0.18903150525087514,"2 ) ≜
√n εk,THIN( ℓn"
G,0.19019836639439908,"2 ). Therefore, COMPRESS++ satisfies"
G,0.19136522753792298,"εk,COMPRESS++(n) ≤4 εk,THIN( ℓn"
G,0.19253208868144692,"2 )
whenever
g ≥log2 log(n + 1) + log2(8.5
eζH(ℓn)
eζT( ℓn"
G,0.19369894982497082,"2 )).
(13)"
G,0.19486581096849476,"In other words, relative to a strong baseline of thinning from ℓn"
G,0.19603267211201866,"2 to √n points, COMPRESS++ inflates
k-sub-Gaussian error by at most a factor of 4 whenever g satisfies (13). For example, when the ratio
eζH(ℓn)/eζT( ℓn"
G,0.1971995332555426,"2 ) is bounded by C, it suffices to choose g = ⌈log2 log(n+1)+log2(8.5C)⌉."
G,0.1983663943990665,"Example 6 (KT-COMPRESS++) In the notation of Ex. 2 and Rem. 3, consider running COM-"
G,0.19953325554259044,"PRESS++ with HALVE = symmetrized KT(
ℓ2
4n2g(g+2g(βn+1))δ) when applied to an input of size
ℓand THIN = KT(
g
g+2g(βn+1)δ). As we detail in App. F.4, on an event of probability 1 −δ"
G,0.20070011668611434,"2, all
COMPRESS++ invocations of HALVE and THIN are simultaneously k-sub-Gaussian with"
G,0.20186697782963828,eζH(ℓn) = eζT( ℓn
G,0.20303383897316218,"2 ) = Cv
q"
G,0.20420070011668612,∥k∥∞log( 6√n(g+2g(βn+1))
G,0.20536756126021002,"δ
) MSin,k
=⇒
eζH(ℓn)
eζT( ℓn"
G,0.20653442240373396,2 ) = 1.
G,0.20770128354725786,"As KT runs in Θ(n2) time, Rems. 5 and 7 imply that KT-COMPRESS++ with g= ⌈log2 log n+3.1⌉
runs in near-linear O(n log3 n) time and inflates k-sub-Gaussian error by at most 4.
■"
EXPERIMENTS,0.2088681446907818,"5
EXPERIMENTS"
EXPERIMENTS,0.2100350058343057,"We now turn to an empirical evaluation of the speed-ups and error of COMPRESS++. We begin by
describing the thinning algorithms, compression tasks, evaluation metrics, and kernels used in our
experiments. Supplementary experimental details and results can be found in App. G."
EXPERIMENTS,0.21120186697782964,"Thinning algorithms
Each experiment compares a high-accuracy, quadratic time thinning
algorithm—either target kernel thinning (Dwivedi & Mackey, 2022) or kernel herding (Chen et al.,
2010)—with our near-linear time COMPRESS and COMPRESS++ variants that use the same input
algorithm to HALVE and THIN. In each case, we perform root thinning, compressing n input points
down to √n points, so that COMPRESS is run with g = 0. For COMPRESS++, we use g = 4 through-
out to satisfy the small relative error criterion (11) in all experiments. When halving we restrict each
input algorithm to return distinct points and symmetrize the output as discussed in Rem. 3."
EXPERIMENTS,0.21236872812135357,"Compressing i.i.d. summaries To demonstrate the advantages of COMPRESS++ over equal-sized
i.i.d. summaries we compress input point sequences Sin drawn i.i.d. from either (a) Gaussian targets
P = N(0, Id) with d ∈{2, 4, 10, 100} or (b) M-component mixture of Gaussian targets P =
1
M
PM
j=1 N(µj, I2) with M ∈{4, 6, 8, 32} and component means µj ∈R2 defined in App. G."
EXPERIMENTS,0.21353558926487748,"Compressing MCMC summaries To demonstrate the advantages of COMPRESS++ over standard
MCMC thinning, we also compress input point sequences Sin generated by a variety of popular
MCMC algorithms (denoted by RW, ADA-RW, MALA, and pMALA) targeting four challenging
Bayesian posterior distributions P. In particular, we adopt the four posterior targets of Riabiz et al.
(2020a) based on the Goodwin (1965) model of oscillatory enzymatic control (d = 4), the Lotka
(1925); Volterra (1926) model of oscillatory predator-prey evolution (d = 4), the Hinch et al. (2004)
model of calcium signalling in cardiac cells (d = 38), and a tempered Hinch model posterior (d =
38). Notably, for the Hinch experiments, each summary point discarded via an accurate thinning
procedure saves 1000s of downstream CPU hours by avoiding an additional critically expensive
whole-heart simulation (Riabiz et al., 2020a). See App. G for MCMC algorithm and target details."
EXPERIMENTS,0.2147024504084014,"Kernel settings Throughout we use a Gaussian kernel k(x, y) = exp(−1"
EXPERIMENTS,0.21586931155192532,"2σ2 ∥x −y∥2
2) with σ2 as
specified by Dwivedi & Mackey (2021, Sec. K.2) for the MCMC targets and σ2 = 2d otherwise."
EXPERIMENTS,0.21703617269544925,"Evaluation metrics For each thinning procedure we report mean runtime across 3 runs and mean
MMD error across 10 independent runs ± 1 standard error (the error bars are often too small to be
visible). All runtimes were measured on a single core of an Intel Xeon CPU. For the i.i.d. targets,
we report MMDk(P, Pout) which can be exactly computed in closed-form. For the MCMC targets,
we report the thinning error MMDk(Pin, Pout) analyzed directly by our theory (Thms. 2 and 4)."
EXPERIMENTS,0.21820303383897316,"Kernel thinning results We first apply COMPRESS++ to the near-optimal KT algorithm to obtain
comparable summaries at a fraction of the cost. Figs. 1 and 2 reveal that, in line with our guarantees,"
EXPERIMENTS,0.2193698949824971,"45
47
49
Input size n 2
10 2
8 2
6 2
4"
EXPERIMENTS,0.220536756126021,Mean MMD d=2
EXPERIMENTS,0.22170361726954493,"ST: n
0.24"
EXPERIMENTS,0.22287047841306884,"KT-Comp: n
0.47"
EXPERIMENTS,0.22403733955659277,"KT-Comp++: n
0.50"
EXPERIMENTS,0.22520420070011668,"KT: n
0.51"
EXPERIMENTS,0.22637106184364061,"45
47
49
Input size n 2
9 2
7 2
5 2
3 d=4"
EXPERIMENTS,0.22753792298716452,"ST: n
0.23"
EXPERIMENTS,0.22870478413068845,"KT-Comp: n
0.42"
EXPERIMENTS,0.22987164527421236,"KT-Comp++: n
0.47"
EXPERIMENTS,0.2310385064177363,"KT: n
0.47"
EXPERIMENTS,0.2322053675612602,"45
47
49
Input size n 2
8 2
6 2
4 d=10"
EXPERIMENTS,0.23337222870478413,"ST: n
0.25"
EXPERIMENTS,0.23453908984830804,"KT-Comp: n
0.38"
EXPERIMENTS,0.23570595099183198,"KT-Comp++: n
0.41"
EXPERIMENTS,0.23687281213535588,"KT: n
0.43"
EXPERIMENTS,0.23803967327887982,"45
47
49
Input size n 2
7 2
6 2
5 2
4 2
3 d=100"
EXPERIMENTS,0.23920653442240372,"ST: n
0.25"
EXPERIMENTS,0.24037339556592766,"KT-Comp: n
0.31"
EXPERIMENTS,0.24154025670945156,"KT-Comp++: n
0.31"
EXPERIMENTS,0.2427071178529755,"KT: n
0.32"
EXPERIMENTS,0.24387397899649943,"103
104
105
106
Input size n"
S,0.24504084014002334,1s
M,0.24620770128354727,1m
M,0.24737456242707118,10m
HR,0.2485414235705951,1hr
HR,0.24970828471411902,"8hr
1day"
HR,0.2508751458576429,Single core runtime d=2
HR,0.25204200700116686,"KT
KT­Comp++
KT­Comp"
HR,0.2532088681446908,"103
104
105
106
Input size n"
S,0.2543757292882147,1s
M,0.2555425904317386,1m
M,0.25670945157526254,10m
HR,0.25787631271878647,1hr
HR,0.2590431738623104,"8hr
1day
d=4"
HR,0.2602100350058343,"103
104
105
106
Input size n"
S,0.2613768961493582,1s
M,0.26254375729288215,1m
M,0.2637106184364061,10m
HR,0.26487747957992996,1hr
HR,0.2660443407234539,"8hr
1day
d=10"
HR,0.26721120186697783,"103
104
105
106
Input size n"
S,0.26837806301050177,1s
M,0.26954492415402564,1m
M,0.2707117852975496,10m
HR,0.2718786464410735,1hr
HR,0.27304550758459745,"8hr
1day
d=100"
HR,0.2742123687281214,"45
47
49
Input size n 2
9 2
7 2
5 2
3"
HR,0.27537922987164526,Mean MMD d=2
HR,0.2765460910151692,"ST: n
0.25"
HR,0.2777129521586931,"Herd-Comp: n
0.43"
HR,0.27887981330221706,"Herd-Comp++: n
0.51"
HR,0.28004667444574094,"Herd: n
0.49"
HR,0.2812135355892649,"45
47
49
Input size n 2
8 2
6 2
4 d=4"
HR,0.2823803967327888,"ST: n
0.22"
HR,0.28354725787631274,"Herd-Comp: n
0.41"
HR,0.2847141190198366,"Herd-Comp++: n
0.46"
HR,0.28588098016336055,"Herd: n
0.45"
HR,0.2870478413068845,"45
47
49
Input size n 2
7 2
5 2
3 d=10"
HR,0.2882147024504084,"ST: n
0.25"
HR,0.2893815635939323,"Herd-Comp: n
0.38"
HR,0.29054842473745623,"Herd-Comp++: n
0.41"
HR,0.29171528588098017,"Herd: n
0.46"
HR,0.2928821470245041,"45
47
49
Input size n 2
6 2
5 2
4 2
3 d=100"
HR,0.294049008168028,"ST: n
0.25"
HR,0.2952158693115519,"Herd-Comp: n
0.32"
HR,0.29638273045507585,"Herd-Comp++: n
0.31"
HR,0.2975495915985998,"Herd: n
0.32"
HR,0.29871645274212366,"103
104
105
106
Input size n"
S,0.2998833138856476,1s
S,0.30105017502917153,"10s
1m"
M,0.30221703617269546,"10m
1hr"
HR,0.3033838973162194,10hr
HR,0.3045507584597433,Single core runtime d=2
HR,0.3057176196032672,"Herd
Herd-Comp++
Herd-Comp"
HR,0.30688448074679114,"103
104
105
106
Input size n"
S,0.3080513418903151,1s
S,0.30921820303383896,"10s
1m"
M,0.3103850641773629,"10m
1hr"
HR,0.3115519253208868,"10hr
d=4"
HR,0.31271878646441076,"103
104
105
106
Input size n"
S,0.31388564760793464,1s
S,0.31505250875145857,"10s
1m"
M,0.3162193698949825,"10m
1hr"
HR,0.31738623103850644,"10hr
d=10"
HR,0.3185530921820303,"103
104
105
106
Input size n"
S,0.31971995332555425,1s
S,0.3208868144690782,"10s
1m"
M,0.3220536756126021,"10m
1hr"
HR,0.323220536756126,"10hr
d=100"
HR,0.32438739789964993,"Figure 1: For Gaussian targets P in Rd, KT-COMPRESS++ and Herd-COMPRESS++ improve upon the MMD
of i.i.d. sampling (ST), closely track the error of their respective quadratic-time input algorithms KT
and kernel herding (Herd), and substantially reduce the runtime."
HR,0.32555425904317387,"KT-COMPRESS++ matches or nearly matches the MMD error of KT in all experiments while also
substantially reducing runtime. For example, KT thins 65000 points in 10 dimensions in 20m,
while KT-COMPRESS++ needs only 1.5m; KT takes more than a day to thin 250000 points in 100
dimensions, while KT-COMPRESS++ takes less than an hour (a 32× speed-up). For reference we
also display the error of standard thinning (ST) to highlight that KT-COMPRESS++ significantly
improves approximation quality relative to the standard practice of i.i.d. summarization or standard
MCMC thinning. See Fig. 4 in App. G.1 for analogous results with mixture of Gaussian targets."
HR,0.3267211201866978,"Kernel herding results
A strength of COMPRESS++ is that it can be applied to any thinning
algorithm, including those with suboptimal or unknown performance guarantees that often perform
well in practical. In such cases, Rems. 4 and 6 still ensure that COMPRESS++ error is never much
larger than that of the input algorithm. As an illustration, we apply COMPRESS++ to the popular
quadratic-time kernel herding algorithm (Herd). Fig. 1 shows that Herd-COMPRESS++ matches or
nearly matches the MMD error of Herd in all experiments while also substantially reducing runtime.
For example, Herd requires more than 11 hours to compress 250000 points in 100 dimensions,
while Herd-COMPRESS++ takes only 14 minutes (a 45× speed-up). Moreover, surprisingly, Herd-
COMPRESS++ is consistently more accurate than the original kernel herding algorithm for lower
dimensional problems. See Fig. 4 in App. G.1 for comparable results with mixture of Gaussian P."
HR,0.3278879813302217,"Visualizing coresets
For a 32-component mixture of Gaussians target, Fig. 3 visualizes the core-
sets produced by i.i.d. sampling, KT, kernel herding, and their COMPRESS++ variants. The COM-
PRESS++ coresets closely resemble those of their input algorithms and, compared with i.i.d. sam-
pling, yield visibly improved stratification across the mixture components."
HR,0.3290548424737456,"45
47
49
Input size n 2
9 2
7 2
5 2
3"
HR,0.33022170361726955,Mean MMD
HR,0.3313885647607935,Goodwin RW
HR,0.33255542590431736,"ST: n
0.23"
HR,0.3337222870478413,"KT-Comp: n
0.45"
HR,0.3348891481913652,"KT-Comp++: n
0.46"
HR,0.33605600933488916,"KT: n
0.50"
HR,0.3372228704784131,"45
47
49
Input size n 2
9 2
7 2
5"
HR,0.338389731621937,"2
3
Goodwin ADA-RW"
HR,0.3395565927654609,"ST: n
0.14"
HR,0.34072345390898484,"KT-Comp: n
0.43"
HR,0.3418903150525088,"KT-Comp++: n
0.47"
HR,0.34305717619603265,"KT: n
0.50"
HR,0.3442240373395566,"45
47
49
Input size n 2
9 2
7 2
5"
HR,0.3453908984830805,"2
3
Goodwin MALA"
HR,0.34655775962660446,"ST: n
0.43"
HR,0.34772462077012833,"KT-Comp: n
0.55"
HR,0.34889148191365227,"KT-Comp++: n
0.52"
HR,0.3500583430571762,"KT: n
0.55"
HR,0.35122520420070014,"45
47
49
Input size n 2
9 2
7 2
5"
HR,0.352392065344224,"2
3
Goodwin pMALA"
HR,0.35355892648774795,"ST: n
0.26"
HR,0.3547257876312719,"KT-Comp: n
0.50"
HR,0.3558926487747958,"KT-Comp++: n
0.48"
HR,0.3570595099183197,"KT: n
0.51"
HR,0.35822637106184363,"45
47
49
Input size n 2
9 2
7 2
5 2
3"
HR,0.35939323220536756,Mean MMD
HR,0.3605600933488915,Lotka-Volterra RW
HR,0.3617269544924154,"ST: n
0.40"
HR,0.3628938156359393,"KT-Comp: n
0.50"
HR,0.36406067677946324,"KT-Comp++: n
0.47"
HR,0.3652275379229872,"KT: n
0.51"
HR,0.3663943990665111,"44
45
46
47
48
Input size n 2
8 2
6 2
4"
HR,0.367561260210035,Lotka-Volterra ADA-RW
HR,0.3687281213535589,"ST: n
0.17"
HR,0.36989498249708286,"KT-Comp: n
0.43"
HR,0.3710618436406068,"KT-Comp++: n
0.46"
HR,0.37222870478413067,"KT: n
0.48"
HR,0.3733955659276546,"45
47
49
Input size n 2
10 2
8 2
6 2
4"
HR,0.37456242707117854,Lotka-Volterra MALA
HR,0.3757292882147025,"ST: n
0.44"
HR,0.37689614935822635,"KT-Comp: n
0.56"
HR,0.3780630105017503,"KT-Comp++: n
0.54"
HR,0.3792298716452742,"KT: n
0.57"
HR,0.38039673278879815,"45
47
49
Input size n 2
9 2
7 2
5 2
3"
HR,0.38156359393232203,Lotka-Volterra pMALA
HR,0.38273045507584597,"ST: n
0.31"
HR,0.3838973162193699,"KT-Comp: n
0.50"
HR,0.38506417736289383,"KT-Comp++: n
0.48"
HR,0.3862310385064177,"KT: n
0.52"
HR,0.38739789964994165,"44
45
46
47
48
Input size n 2
8 2
6 2
4"
HR,0.3885647607934656,Mean MMD
HR,0.3897316219369895,Hinch RW 1
HR,0.3908984830805134,"ST: n
0.52"
HR,0.3920653442240373,"KT-Comp: n
0.53"
HR,0.39323220536756126,"KT-Comp++: n
0.50"
HR,0.3943990665110852,"KT: n
0.51"
HR,0.39556592765460913,"44
45
46
47
48
Input size n 2
8 2
7 2
6 2
5 2
4"
HR,0.396732788798133,Hinch RW 2
HR,0.39789964994165694,"ST: n
0.48"
HR,0.3990665110851809,"KT-Comp: n
0.49"
HR,0.4002333722287048,"KT-Comp++: n
0.48"
HR,0.4014002333722287,"KT: n
0.48"
HR,0.4025670945157526,"44
45
46
47
48
Input size n 2
7 2
6 2
5 2
4"
HR,0.40373395565927656,"2
3 Tempered Hinch RW 1"
HR,0.4049008168028005,"ST: n
0.40"
HR,0.40606767794632437,"KT-Comp: n
0.45"
HR,0.4072345390898483,"KT-Comp++: n
0.44"
HR,0.40840140023337224,"KT: n
0.45"
HR,0.40956826137689617,"44
45
46
47
48
Input size n 2
7 2
6 2
5 2
4"
HR,0.41073512252042005,"2
3 Tempered Hinch RW 2"
HR,0.411901983663944,"ST: n
0.41"
HR,0.4130688448074679,"KT-Comp: n
0.45"
HR,0.41423570595099185,"KT-Comp++: n
0.43"
HR,0.41540256709451573,"KT: n
0.45"
HR,0.41656942823803966,"Figure 2: Given MCMC sequences summarizing challenging differential equation posteriors P, KT-
COMPRESS++ consistently improves upon the MMD of standard thinning (ST) and matches or
nearly matches the error of of its quadratic-time input algorithm KT. 20 10 0 10 20"
HR,0.4177362893815636,"i.i.d.
KT-Comp++
KT
Herd-Comp++
Herd"
HR,0.41890315052508753,"20
0
20 20 10 0 10 20"
HR,0.4200700116686114,"20
0
20
20
0
20
20
0
20
20
0
20"
HR,0.42123687281213534,Figure 3: Coresets of size 32 (top) or 64 (bottom) with equidensity contours of the target underlaid.
DISCUSSION AND CONCLUSIONS,0.4224037339556593,"6
DISCUSSION AND CONCLUSIONS"
DISCUSSION AND CONCLUSIONS,0.4235705950991832,"We introduced a new general meta-procedure, COMPRESS++, for speeding up thinning algorithms
while preserving their error guarantees up to a factor of 4. When combined with the quadratic-time
KT-SPLIT and kernel thinning algorithms of Dwivedi & Mackey (2021; 2022), the result is near-
optimal distribution compression in near-linear time. Moreover, the same simple approach can be
combined with any slow thinning algorithm to obtain comparable summaries in a fraction of the
time. Two open questions recommend themselves for future investigation. First, why does Herd-
COMPRESS++ improve upon the original kernel herding algorithm in lower dimensions, and can this
improvement be extended to higher dimensions and to other algorithms? Second, is it possible to thin
significantly faster than COMPRESS++ without significantly sacrificing approximation error? Lower
bounds tracing out the computational-statistical trade-offs in distribution compression would provide
a precise benchmark for optimality and point to any remaining opportunities for improvement."
REPRODUCIBILITY STATEMENT,0.42473745624270715,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.425904317386231,See the goodpoints Python package for Python implementations of all methods in this paper and
REPRODUCIBILITY STATEMENT,0.42707117852975496,https://github.com/microsoft/goodpoints
REPRODUCIBILITY STATEMENT,0.4282380396732789,for code reproducing each experiment.
REPRODUCIBILITY STATEMENT,0.4294049008168028,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.4305717619603267,"We thank Carles Domingo-Enrich for alerting us that an outdated proof of Thm. 2 was previously
included in the appendix. RD acknowledges the support by the National Science Foundation under
Grant No. DMS-2023528 for the Foundations of Data Science Institute (FODSI). Part of this work
was done when AS was interning at Microsoft Research New England."
REFERENCES,0.43173862310385064,REFERENCES
REFERENCES,0.4329054842473746,"Christoph M Augustin, Aurel Neic, Manfred Liebmann, Anton J Prassl, Steven A Niederer, Gundolf Haase,
and Gernot Plank. Anatomically accurate high resolution modeling of human whole heart electromechanics:
A strongly scalable algebraic multigrid solver method for nonlinear deformation. Journal of computational
physics, 305:622–646, 2016. (Cited on page 1.)"
REFERENCES,0.4340723453908985,"Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics.
Springer Science & Business Media, 2011. (Cited on page 2.)"
REFERENCES,0.4352392065344224,"S. Boucheron, G. Lugosi, and P. Massart. Concentration Inequalities: A Nonasymptotic Theory of Indepen-
dence. OUP Oxford, 2013. ISBN 9780199535255. URL https://books.google.com/books?
id=koNqWRluhP0C. (Cited on pages 2, 12, 15, and 19.)"
REFERENCES,0.4364060676779463,"Bernard Chazelle and Jiri Matousek. On linear-time deterministic algorithms for optimization problems in fixed
dimension. Journal of Algorithms, 21(3):579–597, 1996. (Cited on page 2.)"
REFERENCES,0.43757292882147025,"Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. In Proceedings of the Twenty-
Sixth Conference on Uncertainty in Artificial Intelligence, UAI’10, pp. 109–116, Arlington, Virginia, USA,
2010. AUAI Press. ISBN 9780974903965. (Cited on pages 2 and 7.)"
REFERENCES,0.4387397899649942,"Raaz Dwivedi and Lester Mackey. Kernel thinning. arXiv preprint arXiv:2105.05842, 2021. (Cited on pages 1,"
REFERENCES,0.43990665110851807,"2, 3, 7, 9, 23, and 24.)"
REFERENCES,0.441073512252042,"Raaz Dwivedi and Lester Mackey. Generalized kernel thinning. In International Conference on Learning
Representations, 2022. (Cited on pages 3, 7, 9, 20, 21, and 22.)"
REFERENCES,0.44224037339556593,"Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123–214, 2011. (Cited on
page 23.)"
REFERENCES,0.44340723453908987,"Brian C Goodwin. Oscillatory behavior in enzymatic control process. Advances in Enzyme Regulation, 3:
318–356, 1965. (Cited on page 7.)"
REFERENCES,0.44457409568261375,"Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexander Smola. A kernel
two-sample test. Journal of Machine Learning Research, 13(25):723–773, 2012. (Cited on page 2.)"
REFERENCES,0.4457409568261377,"Heikki Haario, Eero Saksman, and Johanna Tamminen.
Adaptive proposal distribution for random walk
Metropolis algorithm. Computational Statistics, 14(3):375–395, 1999. (Cited on page 23.)"
REFERENCES,0.4469078179696616,"Robert Hinch, JL Greenstein, AJ Tanskanen, L Xu, and RL Winslow. A simplified local control model of
calcium-induced calcium release in cardiac ventricular myocytes. Biophysical journal, 87(6):3723–3736,
2004. (Cited on page 7.)"
REFERENCES,0.44807467911318555,"Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to criticize! criticism
for interpretability. Advances in neural information processing systems, 29, 2016. (Cited on page 2.)"
REFERENCES,0.4492415402567094,"Alfred James Lotka. Elements of physical biology. Williams & Wilkins, 1925. (Cited on page 7.)"
REFERENCES,0.45040840140023336,"Jiri Matousek. Approximations and optimal geometric divide-and-conquer. Journal of Computer and System
Sciences, 50(2):203–208, 1995. (Cited on page 2.)"
REFERENCES,0.4515752625437573,"Steven A Niederer, Lawrence Mitchell, Nicolas Smith, and Gernot Plank. Simulating human cardiac electro-
physiology on clinical time-scales. Frontiers in Physiology, 2:14, 2011. (Cited on page 1.)"
REFERENCES,0.45274212368728123,"Art B Owen. Statistically efficient thinning of a Markov chain sampler. Journal of Computational and Graph-
ical Statistics, 26(3):738–744, 2017. (Cited on page 1.)"
REFERENCES,0.4539089848308051,"Jeff M Phillips. Algorithms for ε-approximations of terrains. In International Colloquium on Automata, Lan-
guages, and Programming, pp. 447–458. Springer, 2008. (Cited on page 2.)"
REFERENCES,0.45507584597432904,"Jeff M Phillips and Wai Ming Tai. Near-optimal coresets of kernel density estimates. Discrete & Computational
Geometry, 63(4):867–887, 2020. (Cited on page 2.)"
REFERENCES,0.456242707117853,"Marina Riabiz, Wilson Chen, Jon Cockayne, Pawel Swietach, Steven A Niederer, Lester Mackey, and Chris
Oates. Optimal thinning of MCMC output. arXiv preprint arXiv:2005.03952, 2020a. (Cited on pages 2, 7,
and 23.)"
REFERENCES,0.4574095682613769,"Marina Riabiz, Wilson Ye Chen, Jon Cockayne, Pawel Swietach, Steven A. Niederer, Lester Mackey, and
Chris J. Oates. Replication Data for: Optimal Thinning of MCMC Output, 2020b. URL https://doi.
org/10.7910/DVN/MDKNWM. Accessed on Mar 23, 2021. (Cited on page 23.)"
REFERENCES,0.45857642940490084,"Christian P Robert and George Casella. Monte Carlo integration. In Monte Carlo statistical methods, pp.
71–138. Springer, 1999. (Cited on page 1.)"
REFERENCES,0.4597432905484247,"Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and their discrete
approximations. Bernoulli, 2(4):341–363, 1996. (Cited on page 23.)"
REFERENCES,0.46091015169194866,"Marina Strocchi, Matthias AF Gsell, Christoph M Augustin, Orod Razeghi, Caroline H Roney, Anton J Prassl,
Edward J Vigmond, Jonathan M Behar, Justin S Gould, Christopher A Rinaldi, Martin J Bishop, Gernot
Plank, and Steven A Niederer. Simulating ventricular systolic motion in a four-chamber heart model with
spatially varying robin boundary conditions to model the effect of the pericardium. Journal of Biomechanics,
101:109645, 2020. (Cited on page 1.)"
REFERENCES,0.4620770128354726,"Ilya Tolstikhin, Bharath K Sriperumbudur, and Krikamol Muandet. Minimax estimation of kernel mean em-
beddings. The Journal of Machine Learning Research, 18(1):3002–3048, 2017. (Cited on page 2.)"
REFERENCES,0.4632438739789965,"Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathe-
matics, 12(4):389–434, 2012. doi: 10.1007/s10208-011-9099-z. URL https://doi.org/10.1007/
s10208-011-9099-z. (Cited on pages 15 and 18.)"
REFERENCES,0.4644107351225204,"Vito Volterra. Variazioni e fluttuazioni del numero d’individui in specie animali conviventi. 1926. (Cited on
page 7.)"
REFERENCES,0.46557759626604434,APPENDIX
REFERENCES,0.46674445740956827,"A Additional Definitions and Notation
12"
REFERENCES,0.4679113185530922,"B
Proof of Thm. 1: Runtime and integration error of COMPRESS
13"
REFERENCES,0.4690781796966161,"C Proof of Thm. 2: MMD guarantees for COMPRESS
13"
REFERENCES,0.47024504084014,"D Proof of Thm. 3: Runtime and integration error of COMPRESS++
19"
REFERENCES,0.47141190198366395,"E
Proof of Thm. 4: MMD guarantees for COMPRESS++
19"
REFERENCES,0.4725787631271879,"F
Proofs of Exs. 3, 4, 5, and 6
20"
REFERENCES,0.47374562427071176,"G Supplementary Details for Experiments
22"
REFERENCES,0.4749124854142357,"H Streaming Version of COMPRESS
23"
REFERENCES,0.47607934655775963,"A
ADDITIONAL DEFINITIONS AND NOTATION"
REFERENCES,0.47724620770128356,This section provides additional definitions and notation used throughout the appendices.
REFERENCES,0.47841306884480744,We associate with each algorithm ALG and input Sin the measure difference
REFERENCES,0.4795799299883314,"ϕALG(Sin) ≜PSin −PSALG = 1 n
P"
REFERENCES,0.4807467911318553,"x∈Sin δx −
1
nout
P"
REFERENCES,0.48191365227537925,"x∈SALG δx
(14)"
REFERENCES,0.4830805134189031,"that characterizes how well the output empirical distribution approximates the input. We will often
write ϕALG instead of ϕALG(Sin) for brevity if Sin is clear from the context."
REFERENCES,0.48424737456242706,"We also make use of the following standard definition of a sub-Gaussian random variable (see, e.g.,
Boucheron et al., 2013, Sec. 2.3)."
REFERENCES,0.485414235705951,"Definition 4 (Sub-Gaussian random variable) We say that a random variable G is sub-Gaussian
with parameter ν and write G ∈G(ν) if"
REFERENCES,0.4865810968494749,"E

exp(λ G)

≤exp

λ2ν2"
REFERENCES,0.48774795799299886,"2

for all
λ ∈R."
REFERENCES,0.48891481913652274,"Given Def. 4, it follows that ALG ∈Gf(ν) for a function f as in Def. 2 if and only if the random
variable ϕALG(f) ≜PSinf −PSALGf is sub-Gaussian with parameter ν conditional on the input Sin."
REFERENCES,0.49008168028004667,"In our proofs, it is often more convenient to work with an unnormalized measure discrepancy"
REFERENCES,0.4912485414235706,"ψALG(Sin) ≜n · ϕALG(Sin)
(14)
= P"
REFERENCES,0.49241540256709454,"x∈Sin δx −
n
nout
P"
REFERENCES,0.4935822637106184,"SALG δx.
(15)"
REFERENCES,0.49474912485414235,"By definition (15), we have the following useful equivalence:"
REFERENCES,0.4959159859976663,"ψALG(f) ≜n · ϕALG(f) ∈G(σALG) ⇐⇒ϕALG(f) ∈G(νALG)
for
σALG =n · νALG.
(16)"
REFERENCES,0.4970828471411902,"The following standard lemma establishes that the sub-Gaussian property is closed under scaling
and summation."
REFERENCES,0.4982497082847141,"Lemma 1 (Summation and scaling preserve sub-Gaussianity) Suppose G1 ∈G(σ1). Then, for
all β ∈R, we have β · G1 ∈G(βσ1). Furthermore, if G1 is F-measurable and G2 ∈G(σ2) given
F, then G1 + G2 ∈G(
p"
REFERENCES,0.49941656942823803,"σ2
1 + σ2
2)."
REFERENCES,0.5005834305717619,"Proof
Fix any β ∈R. Since G1 ∈G(σ1), for each λ ∈R,"
REFERENCES,0.5017502917152858,"E

exp(λ · β · G1)

≤exp

λ2(βσ1)2 2

,"
REFERENCES,0.5029171528588098,so that βG1 ∈G(βσ1) as advertised.
REFERENCES,0.5040840140023337,"Furthermore, if G1 is F-measurable and G2 ∈G(σ2) given F, then, for each λ ∈R,"
REFERENCES,0.5052508751458576,"E
h
exp
 
λ · (G1 + G2)
i
= E

exp(λ · G1 + λ · G2)

= E
h
exp(λ · G1) · E

exp(λ · G2) | F
i"
REFERENCES,0.5064177362893816,"≤exp

λ2σ2
2
2

· E
h
exp
 
λ · f(G2)
i"
REFERENCES,0.5075845974329055,"= exp

λ2σ2
1
2

· exp

λ2σ2
2
2

= exp

λ2(σ2
1+σ2
2)
2 
,"
REFERENCES,0.5087514585764294,"so that G1 + G2 ∈G(
p"
REFERENCES,0.5099183197199533,"σ2
1 + σ2
2) as claimed.
□"
REFERENCES,0.5110851808634772,"B
PROOF OF THM. 1: RUNTIME AND INTEGRATION ERROR OF COMPRESS"
REFERENCES,0.5122520420070011,"First, we bound the running time of COMPRESS. By definition, COMPRESS makes four recursive
calls to COMPRESS on inputs of size n/4. Then, HALVE is run on an input of size 2g+1√n. Thus,
rC satisfies the recursion"
REFERENCES,0.5134189031505251,"rC(n) = 4rC
  n"
REFERENCES,0.514585764294049,"4

+ rH(√n2g+1)."
REFERENCES,0.5157526254375729,"Since rC(4g) = 0, we may unroll the recursion to find that"
REFERENCES,0.5169194865810969,"rC(n) = Pβn
i=0 4irH(2g+1√"
REFERENCES,0.5180863477246208,"n4−i),"
REFERENCES,0.5192532088681447,as claimed in (5).
REFERENCES,0.5204200700116686,"Next, we bound the sub-Gaussian error for a fixed function f. In the measure discrepancy (15)
notation of App. A we have"
REFERENCES,0.5215869311551925,"ψC(Sin) = P4
i=1 ψC(Si) + √n2−g−1ψH( eS)
(17)"
REFERENCES,0.5227537922987164,"where Si and eS are defined as in Alg. 1. Unrolling this recursion, we find that running COMPRESS
on an input of size n with oversampling parameter g leads to applying HALVE on 4i coresets of size
ni = 2g+1−i√n for 0 ≤i ≤βn. Denoting these HALVE inputs by (Sin
i,j)j∈[4i], we have"
REFERENCES,0.5239206534422404,"ψC(Sin) = √n2−g−1 Pβn
i=0
P4i"
REFERENCES,0.5250875145857643,"j=1 2−iψH(Sin
i,j).
(18)"
REFERENCES,0.5262543757292882,"Now define σH(n) = nνH(n). Since ψH(Sin
i,j)(f) are σH(ni) sub-Gaussian given (Sin
i′,j′)i′>i,j′≥1
and (Sin
i,j′)j′≤j, Lem. 1 implies that ψC(Sin)(f) is σC sub-Gaussian given Sin for"
REFERENCES,0.5274212368728122,"σ2
C(n) = n4−g−1 Pβn
i=0 σ2
H(ni)."
REFERENCES,0.5285880980163361,"Recalling the relation (16) between σ and ν from App. A, we conclude that"
REFERENCES,0.5297549591598599,"ν2
C(n) = Pβn
i=0 4−iν2
H(ni)."
REFERENCES,0.5309218203033839,as claimed in (6).
REFERENCES,0.5320886814469078,"C
PROOF OF THM. 2: MMD GUARANTEES FOR COMPRESS"
REFERENCES,0.5332555425904317,"Our proof proceeds in several steps. To control the MMD (1), we will control the Hilbert norm of
the measure discrepancy of COMPRESS (15), which we first write as a weighted sum of measure
discrepancies from different (conditionally independent) runs of HALVE. To effectively leverage
the MMD tail bound assumption for this weighted sum, we reduce the problem to establishing a"
REFERENCES,0.5344224037339557,"concentration inequality for the operator norm of an associated matrix. We carry out this plan in
four steps summarized below."
REFERENCES,0.5355892648774796,"First, in App. C.1 we express the MMD associated with each HALVE measure discrepancy as the
Euclidean norm of a suitable vector (Lem. 2). Second, in App. C.2 we define a matrix dilation
operator for a vector that allows us to control vector norms using matrix spectral norms (Lem. 3).
Third, in App. C.3 we prove and apply a sub-Gaussian matrix Freedman concentration inequality
(Lem. 4) to control the MMD error for the COMPRESS output, which in turn requires us to establish
moment bounds for these matrices by leveraging tail bounds for the MMD error (Lem. 5). Finally,
we put together the pieces in App. C.4 to complete the proof."
REFERENCES,0.5367561260210035,"We now begin our formal argument. We will make use of the unrolled representation (17) for
the COMPRESS measure discrepancy ψC(Sin) in terms of the HALVE inputs (Sin
k,j)j∈[4k] of size
nk = 2g+1−k√n for 0 ≤k ≤log4 n−g−1. For brevity, we will use the shorthand ψC ≜ψC(Sin),
ψH
k,j ≜ψH(Sin
k,j), and ψT ≜ψT(SC) hereafter."
REFERENCES,0.5379229871645275,"C.1
REDUCING MMD TO VECTOR EUCLIDEAN NORM"
REFERENCES,0.5390898483080513,"Number the elements of Sin as (x1, . . . , xn), define the n × n kernel matrix K ≜(k(xi, xj))n
i,j=1,
and let K
1
2 denote a matrix square-root such that K = K
1
2 · K
1
2 (which exists since K is a positive
semidefinite matrix for any kernel k). Next, let Sout
k,j denote the output sequence corresponding
to ψH
k,j (i.e., running HALVE on Sin
k,j), and let {ei}n
i=1 denote the canonical basis of Rn. The
next lemma (with proof in App. C.5) relates the Hilbert norms to Euclidean norms of carefully
constructed vectors."
REFERENCES,0.5402567094515752,Lemma 2 (MMD as a vector norm) Define the vectors
REFERENCES,0.5414235705950992,"uk,j ≜K
1
2 Pn
i=1 ei

1(xi ∈Sin
k,j)−2·1(xi ∈Sout
k,j )

, and uC ≜Plog4 n−g−1
k=0
P4k"
REFERENCES,0.5425904317386231,"j=1 wk,juk,j,
(19)"
REFERENCES,0.543757292882147,"where wk,j ≜
√n
2g+1+k . Then, we have"
REFERENCES,0.544924154025671,"n2 · MMD2
k(Sin, SC) = ∥uC∥2
2,
and
(20)"
REFERENCES,0.5460910151691949,"E[uk,j|(uk′,j′ : j′ ∈[4k′], k′ > k)] = 0
for
k = 0, . . . , log4 n−g−2,
(21)"
REFERENCES,0.5472578763127188,"and uk,j for j ∈[4k] are conditionally independent given (uk′,j′ : j′ ∈[4k′], k′ > k)."
REFERENCES,0.5484247374562428,"Applying (20), we effectively reduce the task of controlling the MMD errors to controlling the
Euclidean norm of suitably defined vectors. Next, we reduce the problem to controlling the spectral
norm of a suitable matrix."
REFERENCES,0.5495915985997666,"C.2
REDUCING VECTOR EUCLIDEAN NORM TO MATRIX SPECTRAL NORM"
REFERENCES,0.5507584597432905,"To this end, we define a symmetric dilation matrix operator: given a vector u ∈Rn, define the
matrix Mu as"
REFERENCES,0.5519253208868145,"Mu ≜

0
u⊤
u
0n×n"
REFERENCES,0.5530921820303384,"
∈R(n+1)×(n+1).
(22)"
REFERENCES,0.5542590431738623,"It is straightforward to see that u 7→Mu is a linear map. In addition, the matrix Mu also satisfies a
few important properties (established in App. C.6) that we use in our proofs."
REFERENCES,0.5554259043173863,"Lemma 3 (Properties of the dilation operator) For any u ∈Rn, the matrix Mu (22) satisfies"
REFERENCES,0.5565927654609102,"∥Mu∥op
(a)
= ∥u∥2
(b)
= λmax(Mu),
and
Mq
u"
REFERENCES,0.5577596266044341,"(c)
⪯∥u∥q
2In+1 for all q ∈N.
(23)"
REFERENCES,0.558926487747958,"Define the shorthand Mk,j ≜Mwk,juk,j (defined in Lem. 2). Applying Lems. 2 and 3, we find that"
REFERENCES,0.5600933488914819,"n MMDk(Sin, SC)
(20)
= ∥uC∥2
(23)
= λmax(MuC)
(i)
= λmax(Plog4 n−g−1
k=0
P4k"
REFERENCES,0.5612602100350058,"j=1 Mk,j),
(24)"
REFERENCES,0.5624270711785297,"where step (i) follows from the linearity of the dilation operator. Thus to control the MMD error, it
suffices to control the maximum eigenvalue of the sum of matrices appearing in (24)."
REFERENCES,0.5635939323220537,"C.3
CONTROLLING THE SPECTRAL NORM VIA A SUB-GAUSSIAN MATRIX FREEDMAN
INEQUALITY"
REFERENCES,0.5647607934655776,"To control the maximum eigenvalue of the matrix MuC, we make use of (24) and the following sub-
Gaussian generalization of the matrix Freedman inequality of Tropp (2012, Thm. 7.1). The proof of
Lem. 4 can be found in App. C.7. For two matrices A and B of the same size, we write A ⪯B if
B −A is positive semidefinite."
REFERENCES,0.5659276546091015,"Lemma 4 (Sub-Gaussian matrix Freedman inequality) Consider a sequence (Yi)N
i=1 of self-
adjoint random matrices in Rm×m and a fixed sequence of scalars (Ri)N
i=1 satisfying"
REFERENCES,0.5670945157526255,"E
h
Yi|
 
Yj
i−1
j=1"
REFERENCES,0.5682613768961493,"i (A)
= 0 and E
h
Yq
i |
 
Yj
i−1
j=1"
REFERENCES,0.5694282380396732,"i (B)
⪯( q"
REFERENCES,0.5705950991831972,"2)!Rq
i I, for all i ∈[N] and q ∈2N.
(25)"
REFERENCES,0.5717619603267211,"Define the variance parameter σ2 ≜PN
i=1 R2
i . Then,"
REFERENCES,0.572928821470245,"P[λmax(PN
i=1 Yi) ≥σ
p"
REFERENCES,0.574095682613769,"8(t + log m)] ≤e−t
for all
t > 0,"
REFERENCES,0.5752625437572929,and equivalently
REFERENCES,0.5764294049008168,"P[λmax(PN
i=1 Yi) ≤σ
p"
REFERENCES,0.5775962660443408,"8 log(m/δ)] ≥1 −δ
for all
δ ∈(0, 1]."
REFERENCES,0.5787631271878646,"To apply Lem. 4 with the matrices Mk,j, we need to establish the zero-mean and moment bound
conditions for suitable Rk,j in (25)."
REFERENCES,0.5799299883313885,"C.3.1
VERIFYING THE ZERO MEAN CONDITION (25)(A) FOR Mk,j"
REFERENCES,0.5810968494749125,"To this end, first we note that the conditional independence and zero-mean property of ψH
k,j implies
that the random vectors uk,j and the matrices Mk,j also satisfy a similar property, and in particular
that"
REFERENCES,0.5822637106184364,"E

Mk,j |

Mk′,j′ : k′ > k, j′ ∈[4k′]

= 0
for
j ∈[4k], k ∈{0, 1, . . . , log4 n −g −1}. (26)"
REFERENCES,0.5834305717619603,"C.3.2
ESTABLISHING MOMENT BOUND CONDITIONS (25)(B) FOR Mk,j IN TERMS OF Rk,j
VIA MMD TAIL BOUNDS FOR HALVE"
REFERENCES,0.5845974329054843,"To establish the moment bounds on Mk,j, note that Lems. 2 and 3 imply that"
REFERENCES,0.5857642940490082,"Mq
k,j = Mq
wk,juk,j"
REFERENCES,0.5869311551925321,"(23)
⪯
wk,juk,j
q
2 · In+1
(20)
= wq
k,j
uk,j
q
2 · In+1
(27)"
REFERENCES,0.588098016336056,"where wk,j was defined in Lem. 2. Thus it suffices to establish the moment bounds on
uk,j
q
2. To
this end, we first state a lemma that converts tail bounds to moment bounds. See App. C.8 for the
proof inspired by Boucheron et al. (2013, Thm. 2.3)."
REFERENCES,0.5892648774795799,"Lemma 5 (Tail bounds imply moment bounds) For a non-negative random variable Z,"
REFERENCES,0.5904317386231038,"P[Z >a+v
√"
REFERENCES,0.5915985997666278,t]≤e−t for all t ≥0 =⇒E[Zq] ≤(2a+2v)q( q
REFERENCES,0.5927654609101517,2)! for all q ∈2N.
REFERENCES,0.5939323220536756,"To obtain a moment bound for
uk,j

2, we first state some notation. For each n, define the quantities"
REFERENCES,0.5950991831971996,"a′
n ≜nan,
v′
n ≜nvn
(28)"
REFERENCES,0.5962660443407235,"where an and vn are the parameters such that HALVE ∈Gk(an, vn) on inputs of size n. Using an
argument similar to Lem. 2, we have
uk,j

2 = nk,j MMDk(Sin
k,j, Sout
k,j )
for
nk,j = |Sin
k,j| = √n2g+1−k."
REFERENCES,0.5974329054842473,"Thereby, using the Gk assumption on HALVE implies that"
REFERENCES,0.5985997666277713,"P[
uk,j

2 ≥a′
ℓ′
k + v′ℓ′
k
√"
REFERENCES,0.5997666277712952,"t | (uk′,j′ : j′ ∈[4k′], k′ > k)] ≤e−t for all t ≥0,
(29) where"
REFERENCES,0.6009334889148191,"ℓ′
k ≜nk,j = √n2g+1−k
(30)"
REFERENCES,0.6021003500583431,"and, notably, ℓn = ℓ′
0. Combining the bound (29) with Lem. 5 yields that"
REFERENCES,0.603267211201867,"E[
uk,j
q
2 | (uk′,j′ : j′ ∈[4k′], k′ > k)] ≤( q"
REFERENCES,0.6044340723453909,"2)!(2a′
ℓ′
k + 2v′
ℓ′
k)q,
(31)"
REFERENCES,0.6056009334889149,"for all q ∈2N, where ℓk is defined in (29). Now, putting together (27) and (31), and using the
conditional independence of Mk,j, we obtain the following control on the q-th moments of Mk,j
for q ∈2N:"
REFERENCES,0.6067677946324388,"E

Mq
k,j


Mk′,j′, k′ > k, j′ ∈[4k′]
 (27)
⪯wq
k,j·E
uk,j
q
2

n
uk′,j′, k′ > k, j′ ∈[4k′]
o
·In+1"
REFERENCES,0.6079346557759626,"(31)
⪯wq
k,j·

(2a′
ℓ′
k + 2v′
ℓ′
k)q( q"
REFERENCES,0.6091015169194866,"2)!

·In+1 = ( q"
REFERENCES,0.6102683780630105,"2)!Rq
k,jIn+1 where Rk,j ≜2wk,j(a′
ℓ′
k + v′
ℓ′
k)
(32)"
REFERENCES,0.6114352392065344,"where ℓk is defined in (30). In summary, the computation above establishes the condition (B) from
the display (25) for the matrices Mk,j in terms of the sequence Rk,j defined in (32)."
REFERENCES,0.6126021003500584,"C.4
PUTTING THE PIECES TOGETHER FOR PROVING THM. 2"
REFERENCES,0.6137689614935823,Define
REFERENCES,0.6149358226371062,"eσ ≜
p"
REFERENCES,0.6161026837806302,"log4 n −g · 2(a√n2g+1 + v√n2g+1)
(33)"
REFERENCES,0.617269544924154,"Now,
putting
(26)
and
(32)
together,
we
conclude
that
with
a
suitable
ordering
of
the indices (k, j),
the assumptions of Lem. 4 are satisfied by the random matrices
 
Mk,j, j ∈[4k], k ∈{0, 1, . . . , log4 n −g −1}

with the sequence
 
Rk,j

.
Now, since ℓ′
k =
√n2g+1−k (29) is decreasing in k, wk,j =
ℓ′
k
4g+1 (as defined in Lem. 2), and a′
n and v′
n (28) are
assumed non-decreasing in n, we find that"
REFERENCES,0.6184364060676779,"n2 · eσ2 (33)
= n2(log4 n −g)(2(a√n2g+1 + v√n2g+1))2"
REFERENCES,0.6196032672112018,"(29)
= (log4 n −g)
n
4g+1 (2(a′
ℓ′
0 + v′
ℓ′
0))2"
REFERENCES,0.6207701283547258,"≥Plog4 n−g−1
k=0
n
4g+1 (2(a′
ℓ′
k + v′
ℓ′
k))2"
REFERENCES,0.6219369894982497,"= Plog4 n−g−1
k=0
P4k"
REFERENCES,0.6231038506417736,"j=1
n
4g+1+k (2(a′
ℓ′
k + v′
ℓ′
k))2"
REFERENCES,0.6242707117852976,"= Plog4 n−g−1
k=0
P4k"
REFERENCES,0.6254375729288215,"j=1(2wk,j(a′
ℓ′
k + v′
ℓ′
k))2"
REFERENCES,0.6266044340723453,"(32)
= Plog4 n−g−1
k=0
P4k"
REFERENCES,0.6277712952158693,"j=1 R2
k,j."
REFERENCES,0.6289381563593932,"Finally, applying (24) and invoking Lem. 4 with σ ←neσ and m ←n + 1, we conclude that"
REFERENCES,0.6301050175029171,"P[MMD(Sin, SC) ≥eσ
p"
REFERENCES,0.6312718786464411,8(log(n + 1) + t)]
REFERENCES,0.632438739789965,"(24)
= P[λmax(Plog4 n−g−1
k=0
P4k"
REFERENCES,0.6336056009334889,"j=1 Mk,j) ≥neσ
p"
REFERENCES,0.6347724620770129,8(log(n + 1) + t)]
REFERENCES,0.6359393232205367,"≤e−t
for all
t > 0,"
REFERENCES,0.6371061843640606,which in turn implies
REFERENCES,0.6382730455075846,"P[MMD(Sin, SC) ≥ean + evn
√"
REFERENCES,0.6394399066511085,"t] ≤e−t for t ≥0,"
REFERENCES,0.6406067677946324,"since the parameters evn, ean (8) satisfy"
REFERENCES,0.6417736289381564,"evn
(8)= 4(aℓn +vℓn)
p"
REFERENCES,0.6429404900816803,"2(log4 n−g)
(33)
= eσ
√"
REFERENCES,0.6441073512252042,"8,
and
ean
(8)= evn
p"
REFERENCES,0.6452742123687282,"log(n+1) = eσ
p"
REFERENCES,0.646441073512252,8 log(n + 1).
REFERENCES,0.6476079346557759,"Comparing with Def. 3, Thm. 2 follows."
REFERENCES,0.6487747957992999,"C.5
PROOF OF LEM. 2: MMD AS A VECTOR NORM"
REFERENCES,0.6499416569428238,"Let vk,j ≜Pn
i=1 ei

1(xi ∈Sin
k,j)−2·1(xi ∈Sout
k,j )

. By the reproducing property of k we have"
REFERENCES,0.6511085180863477,"∥ψH
k,j(k)∥2
k =
P"
REFERENCES,0.6522753792298717,"x∈Sin
k,j k(x, ·)−2 P"
REFERENCES,0.6534422403733956,"x∈Sout
k,j k(x, ·)

2 k
= P"
REFERENCES,0.6546091015169195,"x∈Sin
k,j,y∈Sin
k,j k(x, y) −2 P"
REFERENCES,0.6557759626604434,"x∈Sout
k,j ,y∈Sin
k,j k(x, y) + P"
REFERENCES,0.6569428238039673,"x∈Sout
k,j ,y∈Sout
k,j k(x, y)"
REFERENCES,0.6581096849474912,"= v⊤
k,jKvk,j
(19)
=
uk,j
2
2.
(34)"
REFERENCES,0.6592765460910152,"Using (18), (19), and (22), and mimicking the derivation above (34), we can also conclude that"
REFERENCES,0.6604434072345391,"∥ψC(k)∥2
k = ∥uC∥2
2."
REFERENCES,0.661610268378063,"Additionally, we note that"
REFERENCES,0.662777129521587,"MMDk(Sin, SC) = sup∥f∥k=1
1
n

f, ψC(k)"
REFERENCES,0.6639439906651109,Hk = 1
REFERENCES,0.6651108518086347,n∥ψC(k)∥k.
REFERENCES,0.6662777129521587,"Finally the conditional independence and zero mean property (21) follows from (18) by noting that
conditioned on (Sin
k′,j′)k′>k,j′≥1, the sets (Sin
k,j)j≥1 are independent."
REFERENCES,0.6674445740956826,"C.6
PROOF OF LEM. 3: PROPERTIES OF THE DILATION OPERATOR"
REFERENCES,0.6686114352392065,"For claim (a) in the display (23), we have"
REFERENCES,0.6697782963827305,"M2
u ="
REFERENCES,0.6709451575262544,"∥u∥2
2
0⊤
n
0n
uu⊤"
REFERENCES,0.6721120186697783,"!
(i)
⪯∥u∥2
2In+1
=⇒
∥Mu∥op
(ii)
= ∥u∥2,"
REFERENCES,0.6732788798133023,"where step (i) follows from the standard fact that uu⊤⪯∥u∥2
2In and step (ii) from the facts M2
uee1 =
∥u∥2
2ee1 for ee1 the first canonical basis vector of Rn+1 and ∥Mu∥2
op =
M2
u

op. Claim (b) follows"
REFERENCES,0.6744457409568262,"directly by verifying that the vector v = [1,
u⊤
∥u∥2 ]⊤is an eigenvector of Mu with eigenvalue ∥u∥2.
Finally, claim (c) follows directly from the claim (a) and the fact that ∥Mq
u∥op = ∥Mu∥q
op for all
integers q ≥1."
REFERENCES,0.67561260210035,"C.7
PROOF OF LEM. 4: SUB-GAUSSIAN MATRIX FREEDMAN INEQUALITY"
REFERENCES,0.676779463243874,"We first note the following two lemmas about the tail bounds and symmetrized moment generating
functions (MGFs) for matrix valued random variables (see Apps. C.9 and C.10 respectively for the
proofs of Lems. 6 and 7)."
REFERENCES,0.6779463243873979,"Lemma 6 (Sub-Gaussian matrix tail bounds) Let
 
Xk ∈Rm×m"
REFERENCES,0.6791131855309218,"k≥1 be a sequence of self-
adjoint matrices adapted to a filtration Fk, and let
 
Ak ∈Rm×m"
REFERENCES,0.6802800466744457,k≥1 be a sequence of determin-
REFERENCES,0.6814469078179697,"istic self-adjoint matrices. Define the variance parameter σ2 ≜
P"
REFERENCES,0.6826137689614936,"k Ak

op. If, for a Rademacher
random variable ε independent of (Xk, Fk)k≥1, we have"
REFERENCES,0.6837806301050176,"log E

exp(2εθXk)|Fk−1

⪯2θ2Ak
for all
θ ∈R,
(35)"
REFERENCES,0.6849474912485414,then we also have
REFERENCES,0.6861143523920653,"P
h
λmax
 P"
REFERENCES,0.6872812135355892,"k Xk

≥t)
i
≤me−t2/(8σ2)
for all
t ≥0."
REFERENCES,0.6884480746791132,"Lemma 7 (Symmetrized sub-Gaussian matrix MGF) For a fixed scalar R, let X be a self-
adjoint matrix satisfying"
REFERENCES,0.6896149358226371,"EX = 0
and
EXq ⪯( q"
REFERENCES,0.690781796966161,"2)!RqI
for
q ∈2N.
(36)"
REFERENCES,0.691948658109685,"If ε is a Rademacher random variable independent of X, then"
REFERENCES,0.6931155192532089,"E exp(2εθX) ⪯exp
 
2θ2R2I

for all
θ ∈R."
REFERENCES,0.6942823803967327,"The assumed conditions (25) allow us to apply Lem. 7 conditional on (Yi)i<k along with the oper-
ator monotonicity of log to find that"
REFERENCES,0.6954492415402567,"log E
h
exp(εθYk)|{Yi}i<k
i
⪯2θ2R2
kI
for all
θ ∈R,"
REFERENCES,0.6966161026837806,"for a Rademacher random variable ε independent of (Yk)k≥1.
Moreover,
P"
REFERENCES,0.6977829638273045,"k Ak

op
=
P"
REFERENCES,0.6989498249708285,"k R2
kI

op = P"
REFERENCES,0.7001166861143524,"k R2
k = σ2. Thus, applying Lem. 6, we find that"
REFERENCES,0.7012835472578763,P[λmax(P
REFERENCES,0.7024504084014003,"iYi) ≥t] ≤me−t2/(8σ2) for all t ≥0.
As an immediate consequence, we also find that"
REFERENCES,0.7036172695449242,P[λmax(P
REFERENCES,0.704784130688448,"iYi) ≥
p"
REFERENCES,0.705950991831972,"8σ2(t + log m)] ≤e−t
for all
t ≥0,
as claimed."
REFERENCES,0.7071178529754959,"C.8
PROOF OF LEM. 5: TAIL BOUNDS IMPLY MOMENT BOUNDS"
REFERENCES,0.7082847141190198,"We begin by bounding the moments of the shifted random variable X = Z −a. Note that Z ≥0, so
that X ≥−a. Next, note that X = X+−X−where X± = max(±X, 0) and that |X|q = Xq
++Xq
−.
Furthermore, Xq
−≤aq by the nonnegativity of Z, so that |X|q ≤aq + Xq
+. For any u > 0, since
P[X+ > u] = P[X > u] = P[Z > a + u] for any u > 0, we apply the tail bounds on Z to control
the moments of X+. In particular, we have"
REFERENCES,0.7094515752625438,"E

Xq
+

]
(i)
= q
R ∞
0
uq−1P[X+ > u]du"
REFERENCES,0.7106184364060677,"(ii)
= q
R ∞
0 (v
√"
REFERENCES,0.7117852975495916,"t)q−1P[X+ > v
√"
REFERENCES,0.7129521586931156,"t] ·
v
2
√ tdt"
REFERENCES,0.7141190198366394,"(iii)
≤qvq R ∞
0
tq/2−1e−tdt
(iv)
= qvqΓ( q 2),"
REFERENCES,0.7152858809801633,"where we have applied (i) integration by parts, (ii) the substitution u = v
√"
REFERENCES,0.7164527421236873,"t, and (iii) the assumed
tail bound for Z."
REFERENCES,0.7176196032672112,"Since Z = X + a, the convexity of the function t 7→tq for q ≥1, and Jensen’s inequality imply
that for each q ∈2N, we have
EZq ≤2q−1(aq + E|X|q) ≤2q−1(2aq + EXq
+) ≤(2a)q + 2q−1qvqΓ( q 2)"
REFERENCES,0.7187864644107351,= (2a)q + 2q−1qvq( q
REFERENCES,0.7199533255542591,2 −1)!
REFERENCES,0.721120186697783,≤(2a + 2v)q( q
REFERENCES,0.7222870478413069,"2)!
where the last step follows since xq + yq ≤(x + y)q for all q ∈N and x, y ≥0. The proof is now
complete."
REFERENCES,0.7234539089848308,"C.9
PROOF OF LEM. 6: SUB-GAUSSIAN MATRIX TAIL BOUNDS"
REFERENCES,0.7246207701283547,"The proof of this result is identical to that of Tropp (2012, Proof of Thm. 7.1) as the same steps are
justified under our weaker assumption (35). Specifically, applying the arguments from Tropp (2012,
Proof of Thm. 7.1), we find that"
REFERENCES,0.7257876312718786,"E

tr exp(Pn
k=1 θXk)

≤E

tr exp
Pn−1
k=1 θXk + log E

exp(2εθXn)|Fn−1
"
REFERENCES,0.7269544924154026,"(35)
≤E

tr exp
Pn−1
k=1 θXk + 2θ2An
"
REFERENCES,0.7281213535589265,"(i)
≤tr exp
 
2θ2 Pn
k=1 Ak
 (ii)
≤m exp
 
2θ2σ2
,
(37)
where step (i) follows by iterating the arguments over k = n−1, . . . , 1 and step (ii) from the standard
fact that tr(exp(A)) ≤m
exp(A)

op = m exp(∥A∥op) for an m×m self-adjoint matrix A. Next,
applying the matrix Laplace transform method Tropp (2012, Prop. 3.1), for all t > 0, we have"
REFERENCES,0.7292882147024504,"P
h
λmax
 P"
REFERENCES,0.7304550758459744,"k Xk

≥t)
i
≤infθ>0
n
e−θt · E

tr exp(Pn
k=1 θXk)
o"
REFERENCES,0.7316219369894983,"(37)
≤m infθ>0
n
e−θt · e2θ2σ2o
= me−t2/(8σ2),"
REFERENCES,0.7327887981330222,"where the last step follows from the choice θ =
t
4σ2 . The proof is now complete."
REFERENCES,0.733955659276546,"C.10
PROOF OF LEM. 7: SYMMETRIZED SUB-GAUSSIAN MATRIX MGF"
REFERENCES,0.73512252042007,We have
REFERENCES,0.7362893815635939,"E[exp(2εθX)] = I + P∞
q=1
2qθq"
REFERENCES,0.7374562427071178,"q! E[εqXq]
(i)
= I + P∞
k=1
22kθ2k"
REFERENCES,0.7386231038506418,(2k)! E[X2k]
REFERENCES,0.7397899649941657,"(ii)
⪯I + P∞
k=1
22kθ2k k!R2k"
REFERENCES,0.7409568261376897,"(2k)!
I"
REFERENCES,0.7421236872812136,"(iii)
⪯I + P∞
k=1
(2θ2R2)k"
REFERENCES,0.7432905484247374,"k!
I = exp(2θ2R2I),"
REFERENCES,0.7444574095682613,"where step (i) uses the facts that (a) E[εq] = 1(q ∈2N) and (b) E[εqXq] = E[εq]E[Xq] since ε is
independent of X, step (ii) follows from the assumed condition (36), and step (iii) from the fact that
2kk!
(2k)! ≤1"
REFERENCES,0.7456242707117853,"k! (Boucheron et al., 2013, Proof of Thm. 2.1)."
REFERENCES,0.7467911318553092,"D
PROOF OF THM. 3: RUNTIME AND INTEGRATION ERROR OF COMPRESS++"
REFERENCES,0.7479579929988331,"First, the runtime bound (9) follows directly by adding the runtime of COMPRESS(HALVE, g) as
given by (5) in Thm. 1 and the runtime of THIN."
REFERENCES,0.7491248541423571,"Recalling the notation (14) and (15) from App. A and noting the definition of the point sequences SC
and SC++ in Alg. 2, we obtain the following relationship between the different discrepancy vectors:"
REFERENCES,0.750291715285881,"ϕC(Sin) = 1 n
P"
REFERENCES,0.751458576429405,"x∈Sin δx −
1
2g√n
P"
REFERENCES,0.7526254375729288,"x∈SC δx,"
REFERENCES,0.7537922987164527,"ϕT(SC) =
1
2g√n
P"
REFERENCES,0.7549591598599766,"x∈SC δx −
1
√n
P"
REFERENCES,0.7561260210035006,"x∈SC++ δx,
and"
REFERENCES,0.7572928821470245,"ϕC++(Sin) = 1 n
P"
REFERENCES,0.7584597432905484,"x∈Sin δx −
1
√n
P"
REFERENCES,0.7596266044340724,x∈SC++ δx
REFERENCES,0.7607934655775963,= ϕC(Sin) + ϕT(SC).
REFERENCES,0.7619603267211202,"Noting the Gf property of HALVE and applying Thm. 1, we find that ϕC(Sin)(f) is sub-Gaussian
with parameter νC(n) defined in (6). Furthermore, by assumption on THIN, given SC, the variable
ϕT(SC)(f) is νC( ℓn"
REFERENCES,0.7631271878646441,2 ) sub-Gaussian. The claim now follows directly from Lem. 1.
REFERENCES,0.764294049008168,"E
PROOF OF THM. 4: MMD GUARANTEES FOR COMPRESS++"
REFERENCES,0.7654609101516919,"Noting that MMD is a metric, and applying triangle inequality, we have"
REFERENCES,0.7666277712952159,"MMDk(Sin, SC++) ≤MMDk(Sin, SC) + MMDk(SC, SC++)."
REFERENCES,0.7677946324387398,"Since SC++ is the output of THIN(2g) with SC as the input, applying the MMD tail bound assump-
tion (38) with |SC| = √n2g substituted in place of n, we find that"
REFERENCES,0.7689614935822637,"P
h
MMD(SC, SC++)≥a′
2g√n+v′
2g√n
√"
REFERENCES,0.7701283547257877,"t
i
≤e−t
for all
t ≥0."
REFERENCES,0.7712952158693116,"Recall that ℓn/2 = 2g√n. Next, we apply Thm. 2 with HALVE to conclude that"
REFERENCES,0.7724620770128354,"P[MMDk(Sin, SC) ≥ean + evn ·
√"
REFERENCES,0.7736289381563594,"t] ≤e−t
for all
t ≥0."
REFERENCES,0.7747957992998833,"Thus, we have"
REFERENCES,0.7759626604434072,"P
h
MMDk(Sin, SC++) ≥a′
ℓn/2 + ean + (v′
ℓn/2 + evn)
√"
REFERENCES,0.7771295215869312,"t
i
≤2 · e−t
for all
t ≥0,"
REFERENCES,0.7782963827304551,which in turn implies that
REFERENCES,0.779463243873979,"P
h
MMDk(Sin, SC++) ≥a′
ℓn/2 + ean + (v′
ℓn/2 + evn)√log 2 + (v′
ℓn/2 + evn)
√"
REFERENCES,0.780630105017503,"t
i
≤e−t
for all
t ≥0,"
REFERENCES,0.7817969661610268,thereby yielding the claimed result.
REFERENCES,0.7829638273045507,"F
PROOFS OF EXS. 3, 4, 5, AND 6"
REFERENCES,0.7841306884480747,We begin by defining the notions of sub-Gaussianity and k-sub-Gaussianity on an event.
REFERENCES,0.7852975495915986,"Definition 5 (Sub-Gaussian on an event) We say that a random variable G is sub-Gaussian on an
event E with parameter σ if"
REFERENCES,0.7864644107351225,E[1[E] · exp(λ · G)] ≤exp( λ2σ2
REFERENCES,0.7876312718786465,"2 )
for all
λ ∈R."
REFERENCES,0.7887981330221704,"Definition 6 (k-sub-Gaussian on an event) For a kernel k, we call a thinning algorithm ALG k-
sub-Gaussian on an event E with parameter v and shift a if"
REFERENCES,0.7899649941656943,"P[E, MMDk(Sin, SALG) ≥an + vn
√"
REFERENCES,0.7911318553092183,"t | Sin] ≤e−t
for all
t ≥0.
(38)"
REFERENCES,0.7922987164527421,"We will also make regular use of the unrolled representation (17) for the COMPRESS measure dis-
crepancy ψC(Sin) in terms of the HALVE inputs (Sin
k,j)j∈[4k] of size"
REFERENCES,0.793465577596266,"nk = 2g+1−k√n
for
0 ≤k ≤βn.
(39)"
REFERENCES,0.79463243873979,"For brevity, we will use the shorthand ψC ≜ψC(Sin), ψH
k,j ≜ψH(Sin
k,j), and ψT ≜ψT(SC) hereafter."
REFERENCES,0.7957992998833139,"F.1
PROOF OF EX. 3: KT-SPLIT-COMPRESS"
REFERENCES,0.7969661610268378,"For HALVE = KT-SPLIT(
ℓ2
n4g+1(βn+1)δ) when applied to an input of size ℓ, the proof of Thm. 1 in"
REFERENCES,0.7981330221703618,"Dwivedi & Mackey (2022) identifies a sequence of events Ek,j and random signed measures ˜ψk,j
such that, for each 0 ≤k ≤βn, j ∈[4k], and f with ∥f∥k = 1,"
REFERENCES,0.7992998833138857,"(a) P[Ec
k,j]
(i)
≤
n2
k
n4g+1(βn+1)
δ
2
(ii)
=
1
2
δ
4k(βn+1),"
REFERENCES,0.8004667444574096,"(b) 1[Ek,j]ψH
k,j = 1[Ek,j] ˜ψk,j, and"
REFERENCES,0.8016336056009334,"(c) ˜ψk,j(f) is nk νH(nk) sub-Gaussian (7) given ( ˜ψk′,j′)k′>k,j′≥1 and ( ˜ψk,j′)j′<j,"
REFERENCES,0.8028004667444574,"where step (ii) follows from substituting the definition nk = 2g+1−k√n (39). To establish step (ii)
in property (a), we use the definition1 of KT-SPLIT(
n2
k
n4g+1(βn+1)δ) for an input of size nk, which
implies that δi =
nk
n4g+1(βn+1)δ in the notation of Dwivedi & Mackey (2022). The proof of Thm. 1
in Dwivedi & Mackey (2022) then implies that"
REFERENCES,0.8039673278879813,"P[Ec
k,j] ≤Pnk/2
i=1 δi = nk"
NK,0.8051341890315052,"2
nk
n4g+1(βn+1)δ =
n2
k
n4g+1(βn+1)
δ
2."
NK,0.8063010501750292,"Hence, on the event E = T
k,j Ek,j, these properties hold simultaneously for all HALVE calls made
by COMPRESS, and, by the union bound,"
NK,0.8074679113185531,"P[Ec] ≤Pβn
k=0
P4k"
NK,0.808634772462077,"j=1 P[Ec
k,j] ≤Pβn
k=0 4k 1"
NK,0.809801633605601,"2
δ
4k(βn+1) = δ"
NK,0.8109684947491248,"2.
(40)"
NK,0.8121353558926487,"Now fix any f with ∥f∥k = 1. We invoke the measure discrepancy representation (17), the equiva-
lence of ψH
k,j and ˜ψk,j on E, the nonnegativity of the exponential, and Lem. 1 in turn to find"
NK,0.8133022170361727,E[1[E] · exp(λ · ϕC(f))] = E[1[E] · exp(λ · 1
NK,0.8144690781796966,nψC(f))]
NK,0.8156359393232205,= E[1[E] · exp(λ · 1
NK,0.8168028004667445,"n
√n2−g−1 Pβn
k=0
P4k"
NK,0.8179696616102684,"j=1 2−kψH
k,j(f))]"
NK,0.8191365227537923,= E[1[E] · exp(λ · 1
NK,0.8203033838973163,"n
√n2−g−1 Pβn
k=0
P4k"
NK,0.8214702450408401,"j=1 2−k ˜ψk,j(f))]"
NK,0.822637106184364,≤E[exp(λ · 1
NK,0.823803967327888,"n
√n2−g−1 Pβn
k=0
P4k"
NK,0.8249708284714119,"j=1 2−k ˜ψk,j(f))]"
NK,0.8261376896149358,"≤exp( λ2ν2
C(n)
2
)
for
ν2
C(n) = Pβn
k=0 4−kν2
H(nk)"
NK,0.8273045507584598,so that ϕC(f) is νC sub-Gaussian on E.
NK,0.8284714119019837,"F.2
PROOF OF EX. 4: KT-COMPRESS"
NK,0.8296382730455076,"For HALVE = symmetrized KT(
ℓ2
n4g+1(βn+1)δ) when applied to an input of size ℓ, the proofs of
Thms. 1–4 in Dwivedi & Mackey (2022) identify a sequence of events Ek,j and random signed
measures ˜ψk,j such that, for each 0 ≤k ≤βn and j ∈[4k],"
NK,0.8308051341890315,"(a) P[Ec
k,j] ≤1"
NK,0.8319719953325554,"2
δ
4k(βn+1),"
NK,0.8331388564760793,"(b) 1[Ek,j]ψH
k,j = 1[Ek,j] ˜ψk,j,"
NK,0.8343057176196033,(c) P[ 1
NK,0.8354725787631272,"nk ∥˜ψk,j(k)∥k ≥ank +vnk
√"
NK,0.8366394399066511,"t | ( ˜ψk′,j′)k′>k,j′≥1, ( ˜ψk,j′)j′<j] ≤e−t for all t ≥0, and"
NK,0.8378063010501751,"(d) E[ ˜ψk,j(k) | ( ˜ψk′,j′)k′>k,j′≥1, ( ˜ψk,j′)j′<j] = 0,"
NK,0.838973162193699,where nk = 2g+1−k√n was defined in (39). We derive property (a) exactly as in App. F.1.
NK,0.8401400233372228,"Hence, on the event E = T"
NK,0.8413068844807468,"k,j Ek,j, these properties hold simultaneously for all HALVE calls made
by COMPRESS, and, by the union bound (40), P[Ec] ≤δ 2."
NK,0.8424737456242707,"Furthermore, we may invoke the measure discrepancy representation (17), the equivalence of ψH
k,j
and ˜ψk,j on E, the nonnegativity of the exponential, and the proof of Thm. 2 in turn to find"
NK,0.8436406067677946,"P[E, MMD(Sin, SC) ≥˜an + ˜vn
√"
NK,0.8448074679113186,"t | Sin] = P[E, 1"
NK,0.8459743290548425,"n∥ψC(k)∥k ≥˜an + ˜vn
√"
NK,0.8471411901983664,t | Sin]
NK,0.8483080513418904,"= P[E, 1"
NK,0.8494749124854143,"n∥√n2−g−1 Pβn
k=0
P4k"
NK,0.8506417736289381,"j=1 2−k ˜ψk,j(k)∥k ≥˜an + ˜vn
√"
NK,0.851808634772462,t | Sin] ≤P[ 1
NK,0.852975495915986,"n∥√n2−g−1 Pβn
k=0
P4k"
NK,0.8541423570595099,"j=1 2−k ˜ψk,j(k)∥k ≥˜an + ˜vn
√"
NK,0.8553092182030338,"t | Sin] ≤e−t
for all
t ≥0,"
NK,0.8564760793465578,"so that COMPRESS is k-sub-Gaussian on E with parameters (˜v, ˜a)."
NK,0.8576429404900817,"F.3
PROOF OF EX. 5: KT-SPLIT-COMPRESS++"
NK,0.8588098016336057,"For THIN = KT-SPLIT(
g
g+2g(βn+1)δ) and HALVE = KT-SPLIT(
ℓ2
4n2g(g+2g(βn+1))δ) when applied
to an input of size ℓ, the proof of Thm. 1 in Dwivedi & Mackey (2022) identifies a sequence of events
Ek,j and ET and random signed measures ˜ψk,j and ˜ψT such that, for each 0 ≤k ≤βn, j ∈[4k], and
f with ∥f∥k = 1,"
NK,0.8599766627771295,"(a) P[Ec
k,j]
(i)
≤
n2
k
4n2g(g+2g(βn+1))
δ
2
(ii)
=
2g"
NK,0.8611435239206534,"4k(g+2g(βn+1))
δ
2
and
P[Ec
T]
(iii)
≤
g
g+2g(βn+1)
δ
2,"
NK,0.8623103850641773,"(b) 1[Ek,j]ψH
k,j = 1[Ek,j] ˜ψk,j
and
1[ET]ψT = 1[ET] ˜ψT, and"
NK,0.8634772462077013,"(c) ˜ψk,j(f) is nk νH(nk) sub-Gaussian (12) given ( ˜ψk′,j′)k′>k,j′≥1 and ( ˜ψk,j′)j′<j and ˜ψT is
ℓn"
NK,0.8646441073512252,2 νT( ℓn
NK,0.8658109684947491,2 ) sub-Gaussian (12) given SC.
NK,0.8669778296382731,"Here, step (i) and (ii) follow exactly as in steps (i) and (ii) of property (a) in App. F.1. For step (iii),
we use the definition1 of KT-SPLIT(
g
g+2g(βn+1)δ) for an input of size 2g√n, which implies that
δi =
g
√n2g(g+2g(βn+1))δ in the notation of Dwivedi & Mackey (2022). The proof of Thm. 1 in
Dwivedi & Mackey (2022) then implies that"
NK,0.868144690781797,"P[Ec
T] ≤Pg
j=1
2j−1"
NK,0.8693115519253208,"g
P2g−j√n
i=1
δi = Pg
j=1
2j−1"
NK,0.8704784130688448,"g 2g−j√n
1
√n2g ·
g
g+2g(βn+1)δ =
g
g+2g(βn+1)
δ
2,"
NK,0.8716452742123687,as claimed.
NK,0.8728121353558926,"Hence, on the event E = T"
NK,0.8739789964994166,"k,j Ek,j ∩ET, these properties hold simultaneously for all HALVE calls
made by COMPRESS, and, repeating an argument similar to the union bound (40),"
NK,0.8751458576429405,"P[Ec] ≤P[Ec
T] + Pβn
k=0
P4k"
NK,0.8763127187864644,"j=1 P[Ec
k,j] ≤
g
g+2g(βn+1)
δ
2 + Pβn
k=0 4k
2g"
NK,0.8774795799299884,"4k(g+2g(βn+1))
δ
2 = δ"
NK,0.8786464410735122,"2.
(41)"
NK,0.8798133022170361,"Moreover, since ϕC++ =
1
n(ψC + ψT), Lem. 1 and the argument of App. F.1 together imply that
ϕC(f) is νC++ sub-Gaussian on E for each f with ∥f∥k = 1."
NK,0.8809801633605601,"F.4
PROOF OF EX. 6: KT-COMPRESS++"
NK,0.882147024504084,"In the notation of Ex. 2, define ℓn"
NK,0.8833138856476079,"2 aℓn = √na′
ℓn/2 = Ca
p"
NK,0.8844807467911319,"∥k∥∞,
and ℓn"
NK,0.8856476079346558,"2 vℓn = √nv′
ℓn/2 = Cv
q"
NK,0.8868144690781797,∥k∥∞log( 6(n−√n(2g−g))
NK,0.8879813302217037,"δ
) MSin,k."
NK,0.8891481913652275,"Since HALVE
=
symmetrized KT(
ℓ2
4n2g(g+2g(βn+1))δ) for inputs of size ℓand THIN
=
KT(
g
g+2g(βn+1)δ), the proofs of Thms. 1–4 in Dwivedi & Mackey (2022) identify a sequence of"
NK,0.8903150525087514,"events Ek,j and ET and random signed measures ˜ψk,j and ˜ψT such that, for each 0 ≤k ≤βn and
j ∈[4k],"
NK,0.8914819136522754,"(a) P[Ec
k,j] ≤
2g"
NK,0.8926487747957993,"4k(g+2g(βn+1))
δ
2
and
P[Ec
T] ≤
g
g+2g(βn+1)
δ
2,"
NK,0.8938156359393232,"(b) 1[Ek,j]ψH
k,j = 1[Ek,j] ˜ψk,j
and
1[ET]ψT = 1[ET] ˜ψT,"
NK,0.8949824970828472,(c) P[ 1
NK,0.8961493582263711,"nk ∥˜ψk,j(k)∥k
≥
ank + vnk
√"
NK,0.897316219369895,"t
|
( ˜ψk′,j′)k′>k,j′≥1, ( ˜ψk,j′)j′<j]
≤
e−t and
P[ 2"
NK,0.8984830805134189,"ℓn ∥˜ψT(k)∥k ≥a′
ℓn/2 + v′
ℓn/2
√"
NK,0.8996499416569428,"t | SC] ≤e−t for all t ≥0, and"
NK,0.9008168028004667,"(d) E[ ˜ψk,j(k) | ( ˜ψk′,j′)k′>k,j′≥1, ( ˜ψk,j′)j′<j] = 0."
NK,0.9019836639439907,"We derive property (a) exactly as in App. F.3. Hence, on the event E = T
k,j Ek,j ∩ET, these
properties hold simultaneously for all HALVE calls made by COMPRESS and"
NK,0.9031505250875146,eζH(ℓn) = eζT( ℓn
NK,0.9043173862310385,"2 ) = Cv
q"
NK,0.9054842473745625,∥k∥∞log( 6(n−√n(2g−g))
NK,0.9066511085180864,"δ
) MSin,k."
NK,0.9078179696616102,"Moreover, by the union bound (41), P[Ec] ≤δ 2."
NK,0.9089848308051341,"Finally, since ϕC++ =
1
n(ψC + ψT) and the argument of App. F.2 implies that COMPRESS is k-
sub-Gaussian on E with parameters (˜v, ˜a), the triangle inequality implies that COMPRESS++ is
k-sub-Gaussian on E with parameters (ˆv, ˆa) as in App. E."
NK,0.9101516919486581,"G
SUPPLEMENTARY DETAILS FOR EXPERIMENTS"
NK,0.911318553092182,"In this section, we provide supplementary experiment details deferred from Sec. 5, as well as some
additional results."
NK,0.912485414235706,"In the legend of each MMD plot, we display an empirical rate of decay. In all experiments involving
kernel thinning, we set the algorithm failure probability parameter δ = 1"
NK,0.9136522753792299,"2 and compare KT(δ) to
COMPRESS and COMPRESS++ with HALVE and THIN set as in Exs. 4 and 6 respectively."
NK,0.9148191365227538,"G.1
MIXTURE OF GAUSSIAN TARGET DETAILS AND MMD PLOTS"
NK,0.9159859976662778,"For the target used for coreset visualization in Fig. 3, the mean locations are on two concentric
circles of radii 10 and 20, and are given by"
NK,0.9171528588098017,µj = αj
NK,0.9183197199533255,"
sin(j)
cos(j)"
NK,0.9194865810968494,"
where αj = 10 · 1(j ≤16) + 20 · 1(j > 16)
for j = 1, 2, . . . , 32."
NK,0.9206534422403734,"Here we also provide additional results with mixture of Gaussian targets given by P
=
1
M
PM
j=1 N(µj, Id) for M ∈{4, 6, 8}. The mean locations for these are given by"
NK,0.9218203033838973,"µ1 = [−3, 3]⊤,
µ2 = [−3, 3]⊤,
µ3 = [−3, −3]⊤,
µ4 = [3, −3]⊤,"
NK,0.9229871645274212,"µ5 = [0, 6]⊤,
µ6 = [−6, 0]⊤,
µ7 = [6, 0]⊤,
µ8 = [0, −6]⊤."
NK,0.9241540256709452,"Fig. 4 plots the MMD errors of KT and herding experiments for the mixture of Gaussians targets
with 4, 6 and 8 centers, and notice again that COMPRESS++ provides a competitive performance to
the original algorithm, in fact suprisingly, improves upon herding."
NK,0.9253208868144691,"45
47
49
Input size n 2
9 2
7 2
5 2
3"
NK,0.926487747957993,Mean MMD M=4
NK,0.9276546091015169,"ST: n
0.26"
NK,0.9288214702450408,"KT-Comp: n
0.46"
NK,0.9299883313885647,"KT-Comp++: n
0.52"
NK,0.9311551925320887,"KT: n
0.52"
NK,0.9323220536756126,"45
47
49
Input size n 2
9 2
7 2
5 2
3 M=6"
NK,0.9334889148191365,"ST: n
0.24"
NK,0.9346557759626605,"KT-Comp: n
0.46"
NK,0.9358226371061844,"KT-Comp++: n
0.51"
NK,0.9369894982497082,"KT: n
0.51"
NK,0.9381563593932322,"45
47
49
Input size n 2
9 2
7 2
5 2
3 M=8"
NK,0.9393232205367561,"ST: n
0.25"
NK,0.94049008168028,"KT-Comp: n
0.45"
NK,0.941656942823804,"KT-Comp++: n
0.52"
NK,0.9428238039673279,"KT: n
0.51"
NK,0.9439906651108518,"45
47
49
Input size n 4
4 4
3 4
2 4
1"
NK,0.9451575262543758,Mean MMD M=4
NK,0.9463243873978997,"ST: n
0.26"
NK,0.9474912485414235,"Herd-Comp: n
0.44"
NK,0.9486581096849475,"Herd-Comp++: n
0.51"
NK,0.9498249708284714,"Herd: n
0.49"
NK,0.9509918319719953,"45
47
49
Input size n 4
4 4
3 4
2"
NK,0.9521586931155193,"4
1
M=6"
NK,0.9533255542590432,"ST: n
0.23"
NK,0.9544924154025671,"Herd-Comp: n
0.43"
NK,0.9556592765460911,"Herd-Comp++: n
0.51"
NK,0.9568261376896149,"Herd: n
0.48"
NK,0.9579929988331388,"45
47
49
Input size n 4
4 4
3 4
2"
NK,0.9591598599766628,"4
1
M=8"
NK,0.9603267211201867,"ST: n
0.25"
NK,0.9614935822637106,"Herd-Comp: n
0.44"
NK,0.9626604434072346,"Herd-Comp++: n
0.51"
NK,0.9638273045507585,"Herd: n
0.48"
NK,0.9649941656942824,"Figure 4: For M-component mixture of Gaussian targets, KT-COMPRESS++ and Herd-COMPRESS++ im-
prove upon the MMD of i.i.d. sampling (ST) and closely track or improve upon the error of their
quadratic-time input algorithms, KT and kernel herding (Herd). See App. G.1 for more details."
NK,0.9661610268378062,"G.2
DETAILS OF MCMC TARGETS"
NK,0.9673278879813302,"Our set-up for the MCMC experiments is identical to that of Dwivedi & Mackey (2021, Sec. 6),
except that we use all post-burn-in points to generate our Goodwin and Lotka-Volterra input point
sequences Sin instead of only the odd indices. In particular, we use the MCMC output of Riabiz et al.
(2020b) described in (Riabiz et al., 2020a, Sec. 4) and perform thinning experiments after discarding
the burn-in points. To generate an input Sin of size n for a thinning algorithm, we downsample
the post-burn-in points using standard thinning. For Hinch, we additionally do coordinate-wise
normalization by subtracting the sample mean and dividing by sample standard deviation of the
post-burn-in-points."
NK,0.9684947491248541,"In Sec. 5, RW and ADA-RW respectively refer to Gaussian random walk and adaptive Gaussian
random walk Metropolis algorithms (Haario et al., 1999) and MALA and pMALA respectively
refer to the Metropolis-adjusted Langevin algorithm (Roberts & Tweedie, 1996) and pre-conditioned
MALA (Girolami & Calderhead, 2011). For Hinch experiments, RW 1 and RW 2 refer to two
independent runs of Gaussian random walk, and “Tempered” denotes the runs targeting a tempered
Hinch posterior. For more details on the set-up, we refer the reader to Dwivedi & Mackey (2021,
Sec. 6.3, App. J.2)."
NK,0.969661610268378,"H
STREAMING VERSION OF COMPRESS"
NK,0.970828471411902,"COMPRESS can be efficiently implemented in a streaming fashion (Alg. 3) by viewing the recursive
steps in Alg. 1 as different levels of processing, with the bottom level denoting the input points and
the top level denoting the output points. The streaming variant of the algorithm efficiently maintains
memory at several levels and processes inputs in batches of size 4g+1. At any level i (with i = 0
denoting the level of the input points), whenever there are 2i4g+1 points, the algorithm runs HALVE
on the points in this level, appends the output of size 2i−14g+1 to the points at level i+1, and empties
the memory at level i (and thereby level i never stores more than 2i4g+1 points). In this fashion, just
after processing n = 4k+g+1 points, the highest level is k + 1, which contains a compressed coreset
of size 2k−14g+1 = 2k+g+12g = √n2g (outputted by running HALVE at level k for the first time),
which is the desired size for the output of COMPRESS."
NK,0.9719953325554259,Algorithm 3: COMPRESS (Streaming) – Outputs stream of coresets of size 2g√n for n = 4k+g+1 and k ∈N
NK,0.9731621936989499,"Input: halving algorithm HALVE, oversampling parameter g, stream of input points x1, x2, . . ."
NK,0.9743290548424738,"S0 ←{}
// Initialize empty level 0 coreset
for t = 1, 2, . . . , do"
NK,0.9754959159859977,S0 ←S0 ∪(xj)t·4g+1
NK,0.9766627771295215,"j=1+(t−1)·4g+1
// Process input in batches of size 4g+1"
NK,0.9778296382730455,if t == 4j for j ∈N then
NK,0.9789964994165694,"Sj+1 ←{}
// Initialize level j + 1 coreset after processing 4j+g+1 input points
end
for i = 0, . . . , ⌈log4 t⌉+ 1 do"
NK,0.9801633605600933,if |Si| == 2i4g+1 then
NK,0.9813302217036173,"S ←HALVE(Si)
// Halve level i coreset to size 2i−14g+1"
NK,0.9824970828471412,"Si+1 ←Si+1 ∪S
// Update level i + 1 coreset: has size ∈{1, 2, 3, 4} · 2i−14g+1"
NK,0.9836639439906651,"Si ←{}
// Empty coreset at level i
end
end
if t == 4j for j ∈N then"
NK,0.9848308051341891,"output Sj+1
// Coreset of size √n2g with n ≜t4g+1 and t = 4j for j ∈N
end
end"
NK,0.9859976662777129,"Our next result analyzes the space complexity of the streaming variant (Alg. 3) of COMPRESS.
The intuition for gains in memory requirements is very similar to that for running time, as we now
maintain (and run HALVE) on subsets of points with size much smaller than the input sequence. We
count the number of data points stored as our measure of memory."
NK,0.9871645274212368,"Proposition 1 (COMPRESS Streaming Memory Bound) Let HALVE store sH(n) data points on
inputs of size n. Then, after completing iteration t, the streaming implementation of COMPRESS
(Alg. 3) has used at most sC(t) = 4g+3√"
NK,0.9883313885647608,t + sH(2g+1√
NK,0.9894982497082847,t) data points of memory.
NK,0.9906651108518086,"Proof
At time t, we would like to estimate the space usage of the algorithm. At the ith level of
memory, we can have at most 2i+24g data points. Since we are maintaining a data set of size at most
√"
NK,0.9918319719953326,"t4g at time t, there are at most log t"
NK,0.9929988331388565,"2
levels. Thus, the maximum number of points stored at time t
is bounded by
P0.5 log t
i=0
2i+24g ≤4g+3√ t."
NK,0.9941656942823804,"Furthermore, at any time up to time t, we have run HALVE on a point sequence of size at most
√"
NK,0.9953325554259043,"t2g+1 which requires storing at most sH(
√"
NK,0.9964994165694282,"t2g+1) additional points.
□"
NK,0.9976662777129521,"Example 7 (KT-COMPRESS and KT-COMPRESS++) First consider the streaming variant of
COMPRESS with HALVE = symmetrized KT(
ℓ
2n−ℓn δ) for HALVE inputs of size ℓas in Ex. 4. Since
sKT(n) ≤n (Dwivedi & Mackey, 2021, Sec. 3), Prop. 1 implies that sC(n) ≤4g+4√n."
NK,0.9988331388564761,"Next consider COMPRESS++ with the streaming variant of COMPRESS, with HALVE
=
symmetrized KT(
ℓ
2n−ℓn+2g√nδ) when applied to an input of size ℓ, and THIN = KT(
g
√n−2g+gδ)
as in Ex. 6. The space complexity sC++(n) = sC(n)+sKT(ℓn)=4g+4√n + ℓn ≤4g+5√n. Setting
g as in Ex. 6, we get sC++(n) = O(√n log2 n).
■"
