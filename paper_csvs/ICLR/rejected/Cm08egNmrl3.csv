Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005780346820809248,"Empirical risk minimization (ERM) based machine learning algorithms have suf-
fered from weak generalization performance on the out-of-distribution (OOD)
data when the training data are collected from separate environments with un-
known spurious correlations. To address this problem, previous works either ex-
ploit prior human knowledge for biases in the dataset or apply the two-stage pro-
cess, which re-weights spuriously correlated samples after they were identiﬁed
by the biased classiﬁer. However, most of them fail to remove multiple types
of spurious correlations that exist in training data. In this paper, we propose a
novel bi-level learning framework for OOD generalization, which can effectively
remove multiple unknown types of biases without any prior bias information or
separate re-training steps of a model. In our bi-level learning framework, we
uncover spurious correlations in the inner-loop with shallow model-based pre-
dictions and dynamically re-group the data to leverage the group distribution-
ally robust optimization method in the outer-loop, minimizing the worst-case risk
across all batches. Our main idea applies the unknown bias discovering process
to the group construction method of the group distributionally robust optimization
(group DRO) algorithm in a bi-level optimization setting and provides a uniﬁed
de-biasing framework that can handle multiple types of biases in data. In empir-
ical evaluations on both synthetic and real-world datasets, our framework shows
superior OOD performance compared to all other state-of-the-art OOD methods
by a large margin. Furthermore, it successfully removes multiple types of biases
in the training data groups that most other OOD models fail."
INTRODUCTION,0.011560693641618497,"1
INTRODUCTION"
INTRODUCTION,0.017341040462427744,"Conventional machine learning algorithms are relying on the empirical risk minimization (ERM)
method when they should learn from given data, and in many application areas, this approach has
shown successful performance with high prediction accuracy. However, if a model learns spurious
correlations during training, it can often fail with poor generalization performance, which is known
as the out-of-distribution (OOD) generalization problem. Furthermore, in recent studies, it has been
shown that ERM-based methods more easily learn such unstable correlations in the dataset and
result in a poor generalization performance on real-world applications (Beery et al., 2018; Ilyas
et al., 2019; Geirhos et al., 2018; de Haan et al., 2019; Koh et al., 2021)."
INTRODUCTION,0.023121387283236993,"To address this problem and obtain a robust de-biased model, many approaches have been proposed
for the cases where biases are known beforehand or not. When biases are known as a priori, some
studies applied adversarial training to remove biases from representations (Belinkov et al., 2019a;b)
or re-weighting training samples (Schuster et al., 2019), and assembling predictions of a biased
model and the base model for ensemble with a product of experts (Hinton, 2002; He et al., 2019;
Clark et al., 2019; Mahabadi et al., 2020). However, these works are designed for a speciﬁc type
of bias and thus require extra domain knowledge to generalize to new tasks. Moreover, without
such prior knowledge for biases in the data, they are hard to be applied to practical applications.
For the case of having no prior knowledge of spurious correlations, the most popular approach
is leveraging the prediction result of a shallow model or weak learner while assuming them as a
biased classiﬁer. Since predictions of a biased classiﬁer can provide useful clues for the spurious
correlation it has learned, to learn from the weak models’ mistakes, they down-weight the potentially
biased examples while training a robust model (Mahabadi et al., 2020). Although these works are"
INTRODUCTION,0.028901734104046242,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03468208092485549,"more general approaches and save a lot of human efforts for ﬁnding biases in established datasets,
we found that they still cannot effectively remove multiple types of biases existing in data groups
collected from different environments."
INTRODUCTION,0.04046242774566474,"Another type of effective OOD generalization approach is group distributionally robust optimization
(group DRO) algorithm which alleviates model biases by minimizing the worst-case risk over a set of
human-deﬁned training groups (Hu et al., 2018; Sagawa* et al., 2020). In this method, the choice of
how to group the training data allows us to introduce a prior knowledge of spurious correlations into
optimization. However, ﬁnding multiple types of biases and accordingly constructing data groups
are laborious processes. Therefore, a simple grouping algorithm is proposed in a recent study (Bao
et al., 2021), which splits the training dataset based on the prediction results of biased classiﬁers.
Our approach is also aimed to create data groups that are informative for the multiple underlying
biases in the training dataset so that minimizing the worst-case risk over all those data groups can
provide a robust classiﬁer. In practical settings where there is little or no prior information about the
biases, most de-biasing methods, which automatically identify potential biases in the training data,
cannot discover all spurious correlations existing in the dataset. Furthermore, splitting training data
into several static data groups cannot effectively represent the effect of multiple biases existing in
the dataset."
INTRODUCTION,0.046242774566473986,"In this work, ﬁrst, we propose a novel strategy for discovering bias and splitting training data
for a group-based de-biasing algorithm. Based on the prevailing automatic bias identifying ap-
proaches (Utama et al., 2020b; Sanh et al., 2021), we train a shallow model for each group in a batch
and dynamically re-group the environments according to the prediction correctness of the shallow
model over all other environments in the batch. This batch-wise dynamic data re-grouping strategy
allows us gradually uncover multiple unknown biases in the dataset while training a model. A shal-
low model tends to quickly overﬁt to surface form information, especially when they are trained with
a small training data setting (Utama et al., 2020b); therefore, if we re-group the samples in a batch,
based on its biased prediction results, we can more effectively account for the various unknown
biases in the training dataset. Furthermore, when this approach is combined with the group-based
de-biasing method (group DRO), we can train a more robust classiﬁer by minimizing the worst-case
risk over all interpolations of those dynamic data partitions."
INTRODUCTION,0.05202312138728324,"Second, we also propose a uniﬁed end-to-end learning framework for a stable classiﬁer. Our frame-
work is a bi-level learning process which extends the min-max objective of group DRO with the
unknown bias discovering and grouping method. In the inner level of optimization, it discovers
the biases in the environment of each batch by applying the dynamic data re-grouping method, and
these re-partitioned data groups are used for the group DRO algorithm, which minimizes the worst-
case risk in the outer level of optimization. We coin our novel learning framework as a Bi-level
Learning framework for OOD generalization (BLOOD) and evaluate its OOD performance in both
synthetic and real-world environments. In the empirical evaluation, our framework shows 47% per-
cent improvement on the Colored MNIST dataset and achieves the best results in real-world datasets
(Camelyon17-wilds, FMoW-wilds) compared to other OOD methods."
INTRODUCTION,0.057803468208092484,"The main contributions of our work are the following: (a) we show that dynamically re-grouping the
subset of environments, based on the predictions of the shallow model, gradually uncovers multiple
types of spurious correlations existing in a dataset; (b) we integrate automatic unknown bias iden-
tifying and grouping process to the group DRO by formulating a bi-level optimization objective;
(c) we propose a uniﬁed end-to-end learning framework which does not need prior knowledge of
multiple dataset biases to obtain robust models, but automatically removes various unknown biases
from out-of-distribution data."
METHOD,0.06358381502890173,"2
METHOD"
OVERVIEW,0.06936416184971098,"2.1
OVERVIEW"
OVERVIEW,0.07514450867052024,"Consider a set of N training environments Etr = {ei}N
i=1 where each environment ei is composed of
input-label pairs {(xe
k, ye
k)}n
k=1. Our main goal is to train a stable classiﬁer from these environments
so that it can be generalized to any new test dataset. We do not make any assumption on the biases
present (or not) in the dataset and rely on letting the shallow model discover them during training."
OVERVIEW,0.08092485549132948,Under review as a conference paper at ICLR 2022
OVERVIEW,0.08670520231213873,"For this type of problem, recent de-biasing methods (Utama et al., 2020b; Sanh et al., 2021; Liu
et al., 2021; Bao et al., 2021) apply a two-stage process: (a) train a separate biased classiﬁer by
learning from spurious correlations in the environments and use the prediction correctness of the
biased classiﬁer for identifying a biased sample (Utama et al., 2020b; Liu et al., 2021) or creating
new environment partitions for group-based de-biasing methods (Bao et al., 2021). (b) train a robust
classiﬁer by re-weighting biased samples or applying product-of-experts (Hinton, 2002; Sanh et al.,
2021), conﬁdence regularization (Utama et al., 2020a) to the outcome of the ﬁrst stage."
OVERVIEW,0.09248554913294797,"In our approach, we integrate these two stages into a single uniﬁed learning framework by leverag-
ing a bi-level optimization structure. With the bi-level setting, we can identify unknown biases in
the inner-level and remove the discovered biases with group DRO in the outer-level. In the inner-
level, to automatically discover unknown biases in the environments, we train a shallow model for
each environment in a batch and apply an environment-speciﬁc classiﬁer to partition all other en-
vironments in the batch, based on its prediction correctness. Since a shallow model is more prone
to rely on shallow heuristics, we can obtain a biased classiﬁer after few-shot learning in the batch,
and this procedure is iteratively performed while training a model. In the outer-level, the dynam-
ically partitioned environments, which are obtained from inner-level optimization, are provided to
the group DRO algorithm and used for minimizing the worst-case risk over those partitions. In the
next section, we describe each component of our bi-level learning structure with its corresponding
contribution."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.09826589595375723,"2.2
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.10404624277456648,"Our learning framework consists of two training objectives for bi-level optimization, in which the
inner objective is nested within the outer objective. The inner training objective learns spurious
correlations in the dataset, which are unknown, and uses them for re-grouping other environments.
The outer objective learns only stable correlations by minimizing the worst-case risk over these
groups."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.10982658959537572,"2.2.1
INNER OBJECTIVE: LEARNING BIAS TO DE-BIAS"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.11560693641618497,"The inner-level optimization is aimed to automatically discover unknown spurious correlations,
based on the prediction result of a biased classiﬁer (weak learner), and accordingly re-partitions
the environments to provide groups to the outer-level process. Therefore, we need to train a biased
model which mostly follows spurious correlations in each environment. Most of the other de-biasing
methods, exploiting the prediction results of a biased classiﬁer, use a pre-built biased model by train-
ing it with the full set or a small subset of training data. In contrast to those works, we do not train
a separate biased model; instead, we dynamically obtain a shallow model for each environment in
a batch during inner-level optimization. To obtain biased models while training stable classiﬁer, we
get insight from the deep neural network’s tendency to exploit simple patterns in the early stage of
the training, which is also observed in other researches (Arpit et al., 2017; Liu et al., 2020). Since
spurious correlations are commonly characterized as simple surface patterns, we expect that models’
rapid performance gain is attributed to their reliance on simple surface patterns (Utama et al., 2020a).
In the same context, after a model is trained with only a small number of samples from an environ-
ment, we expect it to perform as a biased classiﬁer, which achieves high accuracy mostly on the
biased examples while still performing poorly on the rest of the samples from other environments."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.12138728323699421,"For each batch learning step in the inner optimization, a shallow model is trained and used for
re-grouping as follows:"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.12716763005780346,"Step 1
: For each sampled environment ei in a batch, temporarily train an environment-speciﬁc
classiﬁer fφi with few-shot learning."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.1329479768786127,"Step 2
: For all sampled environments in the batch where j ̸= i, use the trained classiﬁer fφi to
partition each of them into two parts based on the prediction correctness of the classiﬁer."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.13872832369942195,"Training a shallow model
At the beginning of a model training, a model fθ is randomly initialized
with θ. While training a classiﬁer, we obtain a shallow model for each batch training step by applying
a few gradient descent steps to the model’s parameters θ, so that the model can quickly overﬁt to"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.14450867052023122,Under review as a conference paper at ICLR 2022
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.15028901734104047,Algorithm 1 BLOOD: Bi-level Learning Framework for OOD Generalization
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.15606936416184972,"Input: Step sizes α, β, γ; A set of training environments Etr = {ei}N
i=1
Initialize: θ and q = [q⊙
1 , q⊗
1 , · · · , q⊙
N, q⊗
N]
1: while not done do
2:
for all ei ∈Etr do
3:
φi = θ −α∇θLei(fθ)
{Inner-loop optimization for a shallow model}
4:
ej ∼Etr \ ei
{Sample training environment ej}
5:
G⊙
i→j, G⊗
i→j = Partition(fφi, ej)
{Partition ej into G⊙
i→j and G⊗
i→j}"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.16184971098265896,"6:
q⊙
j ←q⊙
j exp(γLG⊙
i→j(fφi)), q⊗
i ←q⊗
i exp(γLG⊗
i→j(fφi))
{Update group weights}"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.1676300578034682,"7:
end for
8:
q ←q/ P"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.17341040462427745,"i(q⊙
i + q⊗
i )
{Normalize group weights q}
9:
θ ←θ −β ∇θ
P
i
P
j q⊙
j LG⊙
i→j(fφi|θ) + q⊗
j LG⊗
i→j(fφi|θ)
{Outer-loop optimization}"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.1791907514450867,10: end while
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.18497109826589594,"surface form information while fθ is still conditioned on the previously learned θ. This temporarily
trained shallow model parameter φi, which are optimized for surface patterns of ei, is updated by
following stochastic gradient descent (SGD) step:"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.1907514450867052,"φi = θ −α∇θLei(fθ)
(1)"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.19653179190751446,"where α is a step size for the inner optimization, and Lei(fθ) is an expected loss of fθ over the data
sampled from ei. We show only a single gradient update procedure in Equation 1 for the notational
simplicity."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.2023121387283237,"Batch-wise dynamic data re-grouping
After a biased classiﬁer fφi is trained for the environment
ei, we dynamically re-partition all other training environments in the batch. The fφi is applied to all
other environments ej in the batch, where j ̸= i, and according to its prediction correctness, samples
in each environment ej are re-grouped into two parts, a correctly predicted sample group G⊙
i→j and
a incorrectly predicted sample group G⊗
i→j. These dynamically partitioned groups are then used for
training a stable classiﬁer by minimizing the worst-case risk over these groups in the outer-level."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.20809248554913296,"As a similar group-based de-biasing approach, Predict then Interpolate (PI) (Bao et al., 2021) also
uses a biased classiﬁer for re-partitioning other environments. Its main difference compared to our
method is that they fully train a separate model with the entire dataset and use it for partitioning all
environments into static groups. Once their data groups are statically assigned, it is ﬁxed throughout
the optimization process of group DRO. Although PI has shown its theoretical and empirical effec-
tiveness as a de-biasing method, its static grouping strategy, which relies on a fully-trained biased
classiﬁer, constrains it from discovering multiple types of spurious correlations in the environments."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.2138728323699422,"In contrast, our framework uses a shallow model, which is trained with a small subset of each envi-
ronment in the batch. Since we are using a different biased classiﬁer per batch, we are continuously
learning a bias and producing new data partitions for each batch, representing our dynamic data
re-grouping strategy. This dynamic re-grouping strategy provides various group combinations for
the outer-level optimization process, so that it can effectively remove various types of biases in the
environments."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.21965317919075145,"Furthermore, since our shallow model has overﬁtted to the surface patterns in a small subset of
data, it is more likely to capture multiple types of bias in the environments, as shown in other
research (Utama et al., 2020a). This property can be a disadvantage for a de-biasing method, which
applies re-weighting to the biased examples, because it reduces the effective training data size for a
model. However, in our approach, it enables us to uncover multiple types of biases in the dataset.
Because, multiple types of biases can be considered while minimizing the worst-case risk over
various types of group conﬁgurations."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.2254335260115607,"2.2.2
OUTER OBJECTIVE: MINIMIZING WORST-CASE RISK OVER DATA GROUPS"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.23121387283236994,"The goal of the outer-level objective is to remove spurious correlations that have been identiﬁed
during the inner-level optimization. For this purpose, we apply the group DRO algorithm in the"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.23699421965317918,Under review as a conference paper at ICLR 2022
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.24277456647398843,"outer-level, to iteratively minimize the worst-case risk over dynamically re-partitioned groups. Al-
though the baseline group DRO algorithm is an effective group-based de-biasing method, it still
requires data groups to be explicitly deﬁned by the prior knowledge on biases in the training data.
In our framework, we already automatically identiﬁed unknown biases in the environments and dy-
namically re-grouped environments in the inner-loop. Therefore, we can directly provide them to
the group DRO during the outer-level optimization."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.24855491329479767,"In our bi-level optimization setting, we integrate our dynamic re-grouping scheme with online group
DRO (Sagawa* et al., 2020) that minimizes the worst-case of convex combinations of the group
risks. The objective of our framework is formulated as following min-max problem:"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.2543352601156069,"min
θ
max
q X i X"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.26011560693641617,"j̸=i
q⊙
j LG⊙
i→j(fφi|θ) + q⊗
j LG⊗
i→j(fφi|θ)
(2)"
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.2658959537572254,"where q = [q⊙
1 , q⊗
1 , · · · , q⊙
N, q⊗
N] denotes a coefﬁcients vector for convex combination, and q⊙
i
and q⊗
i denotes group weights for correctly predicted sample group G⊙
i→j and incorrectly predicted
sample group G⊗
i→j, respectively. To solve this min-max problem, we interleave gradient-based
iterative updates on θ and q; update q with exponentiated gradient ascent, so that groups with high-
risk get high weights, and model parameters θ are updated with SGD."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.27167630057803466,"While training a robust classiﬁer with our bi-level learning framework, the parameters θ are opti-
mized by the group-wise losses obtained from shallow models in the inner-level. In the early stages
of learning, a shallow model can learn a bias after few-shot learning, then the loss difference be-
tween two groups, G⊙
i→j and G⊗
i→j, becomes larger, and optimizing θ over the worst-case group
risks can effectively prevent the model fθ from learning such biases."
BI-LEVEL LEARNING FRAMEWORK FOR DE-BIASING,0.2774566473988439,"We formulate a bi-level optimization problem by integrating Equation 1 and optimization objective 2
to the inner and outer optimization objectives of the bi-level learning setting. Our framework iter-
atively uncovers biases in the inner-loop and de-biases them in the outer-loop while optimizing to
obtain a robust classiﬁer. The full procedure of our learning framework is described in Algorithm 1."
EXPERIMENTS,0.2832369942196532,"3
EXPERIMENTS"
EXPERIMENTS,0.28901734104046245,"We evaluate our framework on both synthetic and real-world tasks and also compare the OOD
performance with other state-of-the-art algorithms, such as ERM, Invariant Risk Minimization
(IRM) (Arjovsky et al., 2019), Group DRO, and PI. IRM is an optimization framework based on
the theory of Invariant Causal Prediction (Peters et al., 2016), which learns representations that are
simultaneously optimal across all environments. For training a model with PI, we train environment-
speciﬁc classiﬁers for each of all training environments and randomly pick the environment for
evaluation."
EXPERIMENTS,0.2947976878612717,"3.1
SYNTHETIC TASKS: COLORED MNIST"
EXPERIMENTS,0.30057803468208094,"Figure 1: Image samples of Col-
ored MNIST task with additional
patch feature."
EXPERIMENTS,0.3063583815028902,"In this section, we demonstrate the multiple biases identiﬁca-
tion performance of our framework on a synthetic task, Col-
ored MNIST dataset. While following Arjovsky et al. (2019)
as a baseline, we also inject more spurious correlations to
the original Colored MNIST. For color attributes, green or
red are applied to have strong spurious correlations with the
class labels. We design these correlations to vary across the
environments so that the model which exploits the unstable
color attributes cannot guarantee the OOD generalization per-
formance. In detail, we consider two training environments
with coloring probability pe = 0.1 and pe = 0.2, respectively,
and this indicates that the color attribute is correlated with la-
bel as 1 −pe. We add noise with probability pc = 0.25 to
the shape attribute so that the shape attribute is less correlated with the target label, making the
correlation only 75%."
EXPERIMENTS,0.31213872832369943,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3179190751445087,"Table 1: Test accuracy (%) of different algorithms on the Colored MNIST task in 5 trials (mean ±
standard deviation). δgap indicates the generalization gap of the model between i.i.d and OOD test
environments. Note that the highest test accuracy for the i.i.d test environment does not guarantee
high performance on the OOD."
EXPERIMENTS,0.3236994219653179,"Algorithm
Bias: Color
Bias: Color & Patch
Test (i.i.d)
Test (OOD)
δgap
Test (i.i.d)
Test (OOD)
δgap
pe = 0.1
pe = 0.9
pe = 0.1
pe = 0.9"
EXPERIMENTS,0.32947976878612717,"ERM
88.6 ± 0.3
16.4 ± 0.8
-72.2
93.7 ± 0.3
14.0 ± 0.5
-79.7
IRM
71.4 ± 0.9
66.9 ± 2.5
-4.5
93.5 ± 0.2
13.4 ± 0.3
-80.1
Group DRO
89.2 ± 0.9
13.6 ± 3.8
-75.6
92.3 ± 0.3
14.1 ± 0.8
-78.2
PI
70.3 ± 0.3
70.2 ± 0.9
-0.1
85.4 ± 0.9
15.3 ± 2.7
-70.1
BLOOD (Ours)
70.5 ± 1.1
70.7 ± 1.4
0.2
68.3 ± 2.3
62.3 ± 3.3
-6.0"
EXPERIMENTS,0.3352601156069364,"Optimal
75
75
0
75
75
0"
EXPERIMENTS,0.34104046242774566,"Moreover, we inject additional spurious correlation by creating small patches of noise to the corner
of the image so that the locations of these patches are strongly correlated with the labels. Similar to
the coloring MNIST digits, we design the patch attributes to have unstable correlations across the
training environments with the probability pe. We add (3 × 3) patch in the top left corner or the
bottom right corner of the image, depending on the labels. The new patch features are independent
of other types of spurious correlations, but they have strong but unstable correlations with the target
labels. Figure 1 shows the image samples of our Colored MNIST task with additional patch bias."
EXPERIMENTS,0.3468208092485549,"For algorithm evaluation, we consider both independent and identically distributed (i.i.d) data and
the OOD test environments with pe = 0.9 where the correlation between color and label is reversed
compared with training environments. The model that exploits color attributes for the prediction can
achieve high accuracy in the i.i.d test environment, but it will fail in the OOD test. The purpose of
the Colored MNIST task is to classify digits solely based on the shape of digits without relying on
other spurious features, such as colors, to achieve good OOD generalization."
EXPERIMENTS,0.35260115606936415,"Table 1 shows evaluation results on the Colored MNIST task according to the types of injected
spurious correlations. In all cases, BLOOD achieves state-of-the-art OOD performance on Colored
MNIST. For the case with multiple types of biases (Color & Patch), BLOOD outperforms PI (Bao
et al., 2021) by 47%, which clearly demonstrate that BLOOD can more effectively de-bias multiple
types of spurious correlations in the dataset. Moreover, from the results, BLOOD consistently shows
superior generalization performance with low variance for both i.i.d and OOD test data."
EXPERIMENTS,0.3583815028901734,"Figure 2 shows the changes in the prediction accuracy on test environments while varying noise
probability for the shape and other biased attributes to the labels. Our framework shows robust
prediction performance against the change of various spurious correlations, which is closer to the
oracle pattern. In contrast, PI shows prediction patterns more relying on spurious correlations."
EXPERIMENTS,0.36416184971098264,"Figure 2: Visualization of various test accuracy on Colored MNIST with varying noise probability
for shape pc and for color and patch pe. Our BLOOD shows robustness performance against the
change of pe, while PI highly depends on the spurious correlations."
EXPERIMENTS,0.3699421965317919,Under review as a conference paper at ICLR 2022 (a) (b)
EXPERIMENTS,0.37572254335260113,"Figure 3: The Pearson correlation coefﬁcients between model’s predictions and bias features on the
Colored MNIST task with (a) a single spurious feature Color and (b) two types of spurious features,
Color and Patch. The prediction results of BLOOD show high correlations with the shape of digits
for both (a) and (b) cases, while PI fails to de-bias when there are two types of spurious correlations
in the data."
EXPERIMENTS,0.3815028901734104,"We also analyze how much each attribute affects the prediction results of a model. In Figure 3, for
each OOD method, the correlation between the predictions of a model and each attribute (Shape,
Color, Patch) are shown according to the training steps, on the environment with pe = 0.9. When
there is only Color bias, both PI and BLOOD successfully de-bias a model by reducing the cor-
relation between Color and the label. However, if an extra Patch attribute is added as spurious
correlation, only BLOOD reduces the effect of both Color and Patch attributes on model prediction
and recover correct Shape correlation. All other OOD methods, group DRO and PI, suffer from
biases and fail to discover true correlations across environments."
EXPERIMENTS,0.3872832369942196,"To verify the effectiveness of batch-wise bias identiﬁcation and dynamic re-grouping method of
BLOOD, in Figure 4, the grouped result of PI and BLOOD with each group’s correlations to the
bias attributes (Color, Patch) are visualized for the Colored MNIST task. Compared to PI, which
relies on pre-built biased classiﬁers, BLOOD use a shallow model trained with samples of each
environment in a batch; therefore, it dynamically produces new data partitions for each training
step, as shown in Figure 4b. Since grouping depends on the bias identiﬁcation performance of the
classiﬁer, each pair of groups(correct & incorrect) predicted by a learned bias, Pearson correlation
to the bias attribute should be positive for the correct group, G⊙
1→2, and negative for the incorrect
group, G⊗
1→2, for example, if correct one is 0.99 then the other is -0.97 (Bao et al., 2021)."
EXPERIMENTS,0.3930635838150289,"As shown in Figure 4b, in the early step of training, the shallow model of BLOOD learns Color
bias and splits the other environment, so that correlation of the correct group is 0.99 and the other
is -0.97. With these correlations, Color bias can be easily addressed with the following group DRO
algorithm in the outer-level. When there are multiple types of biases, BLOOD learns each of them in"
EXPERIMENTS,0.3988439306358382,"(a) PI
(b) BLOOD (Ours)"
EXPERIMENTS,0.4046242774566474,"Figure 4: Visualization of created groups with Pearson correlation coefﬁcient between the label and
bias features on Colored MNIST task. Our BLOOD dynamically re-groups the training data at each
learning step while PI constructs single static groups."
EXPERIMENTS,0.41040462427745666,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.4161849710982659,"an ordered way. The grouping of BLOOD at step 1 shows that Color bias is identiﬁed by a shallow
model, and environments are partitioned accordingly. At step 2, we can also verify that BLOOD has
learned Patch bias, and it re-groups the environments with 0.98, -0.95 Patch correlations. Therefore,
it is shown that BLOOD gradually discovers multiple types of biases in the dataset by adopting
the batch-wise shallow model as a biased classiﬁer. However, for the case of the PI in Figure 4a,
the training environments are re-grouped only once by a fully-trained environment-speciﬁc model.
This biased classiﬁer learns multiple biases together, therefore, its data partition should represent
the overall effect of multiple biases. In this setting, interpolating these static partitions may not be
enough to approximate oracle distribution. Because it does not have a chance to gradually reﬁne the
deﬁned groups to account for multiple types of biases. In this experiment, PI fails to remove two
biases in the dataset, as shown in Figure 3."
REAL-WORLD TASKS,0.42196531791907516,"3.2
REAL-WORLD TASKS"
REAL-WORLD TASKS,0.4277456647398844,"3.2.1
CAMELYON17-WILDS"
REAL-WORLD TASKS,0.43352601156069365,"In the ﬁeld of machine learning-based medical image processing, the OOD generalization is a critical
problem to obtain a universally applicable prediction model in several hospitals. Camelyon17-wilds
dataset (Bandi et al., 2018; Koh et al., 2021) is a medical image classiﬁcation benchmark that ex-
plicitly targets the OOD generalization problem. The main goal of Camelyon17-wilds is to achieve
high prediction accuracy for predicting the presence of tumor tissue on image patches taken from
hospitals not included in the training data."
REAL-WORLD TASKS,0.4393063583815029,"The training data consists of image patches from three hospitals, and the test data contains patches
from a hospital that does not exist in training data. Also, the hospital for the test data provides
the most visually unique patches among the data from other hospitals. The ﬁnal model selection is
performed based on the test accuracy on OOD validation data that has different distribution from
both the training and test data. The patches for validation data are also taken from a different
distribution with the training data but have more similar visual patterns with training data than the
test data."
REAL-WORLD TASKS,0.44508670520231214,"Table 2 shows the experimental results on the Camelyon17-wilds dataset. Although the ERM shows
the best train accuracy, it degrades on the OOD test data. From the results, BLOOD achieves the
state-of-the-art OOD generalization performance and also outperforms all other algorithms by a
large margin."
REAL-WORLD TASKS,0.4508670520231214,"Table 2: Train and OOD test accuracy (%) on the Camelyon17-wilds in 3 trials (mean ± standard
deviation). We consider average accuracy on the OOD test environment."
REAL-WORLD TASKS,0.45664739884393063,"ERM
IRM
Group DRO
PI
BLOOD (Ours)"
REAL-WORLD TASKS,0.4624277456647399,"Train acc
97.3 ± 0.1
97.1 ± 0.1
96.5 ± 1.4
93.2 ± 0.2
93.0 ± 1.8
OOD Test acc
66.5 ± 4.2
59.4 ± 3.7
70.2 ± 7.3
71.7 ± 7.5
74.9 ± 5.0"
FMOW-WILDS,0.4682080924855491,"3.2.2
FMOW-WILDS"
FMOW-WILDS,0.47398843930635837,"FMoW-wilds dataset (Christie et al., 2018; Koh et al., 2021) is another benchmark for OOD gen-
eralization, including satellite images taken from various locations and times. The dataset consists
of RGB satellite images and labels for the task of classifying 62 different functional purposes of
buildings and land, based on given images and metadata, which provide the location and time in-
formation of each image. The training data includes the times of images were taken, from 2002 to
2013, while the validation and test data include data collected from 2013 to 2015 and 2016 to 2017,
respectively. FMoW-wilds aims to evaluate whether a model can be generalized to the images which
will be obtained in the future."
FMOW-WILDS,0.4797687861271676,"There are ﬁve geographic regions in each data split where the images were taken. The OOD gen-
eralization performance of each algorithm is evaluated with the worst-region accuracy for the test
data where the data collection period does not overlap with the training data. Table 3 shows our
model evaluation results on the FMoW-wilds task. From the results, BLOOD achieves the highest
worst-region accuracy, outperforming all other OOD methods."
FMOW-WILDS,0.48554913294797686,Under review as a conference paper at ICLR 2022
FMOW-WILDS,0.4913294797687861,"Table 3: The worst-region accuracy (%) for the OOD test environment on the FMoW-wilds in 3
trials (mean ± standard deviation)."
FMOW-WILDS,0.49710982658959535,"ERM
IRM
Group DRO
PI
BLOOD (Ours)"
FMOW-WILDS,0.5028901734104047,"Wosrt-region acc
31.3 ± 0.17
32.8 ± 2.1
31.0 ± 1.6
31.2 ± 0.3
34.1 ± 2.5"
RELATED WORK,0.5086705202312138,"4
RELATED WORK"
RELATED WORK,0.5144508670520231,"De-biasing with explicit supervision.
Biases are always present as a part of the real dataset, and
they degrade a learning model’s prediction performance when it is applied to practical applications.
To mitigate such biases, some researches de-bias a model with adversarial training (Belinkov et al.,
2019a;b), data sample re-weighting (Schuster et al., 2019), or ensemble of biased models (He et al.,
2019; Clark et al., 2019; Mahabadi et al., 2020), when biases are known beforehand by the human
expert knowledge. However, these methods are not easily applicable to practical applications be-
cause prior knowledge for biases in the large-scale dataset is generally inaccessible. Compared to
these works, our framework can identify and remove unknown biases without any human annotated
bias information."
RELATED WORK,0.5202312138728323,"De-biasing by a biased classiﬁer.
To automatically identify unknown biases without domain ex-
pert knowledge, recent studies utilize a weak learner or shallow model, which is a biased classiﬁer,
for verifying whether the data sample is biased. Based on the predictions of the shallow model,
they consider re-weighting data samples (Liu et al., 2021), merging the predictions of the biased
model and main model with the product of experts (Sanh et al., 2021), or grouping training data
based on the prediction correctness of biased models (Bao et al., 2021). Most approaches pre-build
a fully-trained biased model to discover biases; however, our learning framework dynamically trains
a shallow model at every learning step to uncover multiple types of biases."
RELATED WORK,0.5260115606936416,"De-biasing with invariant representation.
Since the spurious correlations vary across environ-
ments, several researchers focus on ﬁnding stable correlations which are invariant over training
environments (Peters et al., 2016; Arjovsky et al., 2019; Ahuja et al., 2020; Chang et al., 2020; Lu
et al., 2021). IRM (Arjovsky et al., 2019), a symbolic learning paradigm based on this idea, as-
sumes that the model can learn invariant correlations if the classiﬁer on top of the feature embedder
is simultaneously optimal for all training environments. However, a recent study demonstrates the
potential degeneration of IRM in real-world scenarios (Rosenfeld et al., 2020). Compared to the
IRM, our BLOOD gradually removes various spurious correlations so that only stable correlations
can remain across all environments."
CONCLUSION,0.5317919075144508,"5
CONCLUSION"
CONCLUSION,0.5375722543352601,"In this paper, we propose a novel bi-level learning framework for OOD generalization, which is
robust to multiple types of dataset biases. Our framework automatically identiﬁes unknown biases
during the training process and gradually removes multiple types of biases without any prior knowl-
edge for biases in the dataset. To uncover unknown biases, we presented a novel strategy of training
a shallow model for batch-wise bias identiﬁcation. By dynamically re-grouping the environments
in the batch, we effectively de-biased a model, based on the prediction correctness of a shallow
model, by leveraging the group DRO algorithm. In our empirical evaluations, BLOOD achieves
the state-of-the-art OOD generalization performance on both real-world applications and synthetic
tasks. Also, our extensive analyses on the Colored MNIST task show the efﬁciency of our approach
for de-biasing multiple types of spurious correlations. As Future works, we are going to extend
BLOOD for the other types of tasks beyond classiﬁcation."
REFERENCES,0.5433526011560693,REFERENCES
REFERENCES,0.5491329479768786,"Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk min-
imization games. In International Conference on Machine Learning, pp. 145–155. PMLR, 2020."
REFERENCES,0.5549132947976878,Under review as a conference paper at ICLR 2022
REFERENCES,0.5606936416184971,"Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019."
REFERENCES,0.5664739884393064,"Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer
look at memorization in deep networks. In International Conference on Machine Learning, pp.
233–242. PMLR, 2017."
REFERENCES,0.5722543352601156,"Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke
Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al.
From detection of individual metastases to classiﬁcation of lymph node status at the patient level:
the camelyon17 challenge. IEEE transactions on medical imaging, 38(2):550–560, 2018."
REFERENCES,0.5780346820809249,"Yujia Bao, Shiyu Chang, and Regina Barzilay. Predict then interpolate: A simple algorithm to learn
stable classiﬁers. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pp. 640–650. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
bao21a.html."
REFERENCES,0.5838150289017341,"Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of
the European Conference on Computer Vision (ECCV), pp. 456–473, 2018."
REFERENCES,0.5895953757225434,"Yonatan Belinkov, Adam Poliak, Stuart M Shieber, Benjamin Van Durme, and Alexander M Rush.
On adversarial removal of hypothesis-only bias in natural language inference. In Proceedings of
the Eighth Joint Conference on Lexical and Computational Semantics (* SEM 2019), pp. 256–
262, 2019a."
REFERENCES,0.5953757225433526,"Yonatan Belinkov, Adam Poliak, Stuart M Shieber, Benjamin Van Durme, and Alexander M Rush.
Don’t take the premise for granted: Mitigating artifacts in natural language inference. In Proceed-
ings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 877–891,
2019b."
REFERENCES,0.6011560693641619,"Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant rationalization. In International
Conference on Machine Learning, pp. 1448–1458. PMLR, 2020."
REFERENCES,0.6069364161849711,"Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6172–
6180, 2018."
REFERENCES,0.6127167630057804,"Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don’t take the easy way out: Ensemble
based methods for avoiding known dataset biases. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pp. 4069–4082, 2019."
REFERENCES,0.6184971098265896,"Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In
Advances in Neural Information Processing Systems, pp. 11698–11709, 2019."
REFERENCES,0.6242774566473989,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias im-
proves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018."
REFERENCES,0.630057803468208,"He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by ﬁt-
ting the residual. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-
Resource NLP (DeepLo 2019), pp. 132–142, 2019."
REFERENCES,0.6358381502890174,"Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771–1800, 2002."
REFERENCES,0.6416184971098265,"Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised
learning give robust classiﬁers?
In International Conference on Machine Learning, pp. 2029–
2037. PMLR, 2018."
REFERENCES,0.6473988439306358,Under review as a conference paper at ICLR 2022
REFERENCES,0.653179190751445,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700–4708, 2017."
REFERENCES,0.6589595375722543,"Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp. 125–136, 2019."
REFERENCES,0.6647398843930635,"Pang Wei Koh, Shiori Sagawa, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua
Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, et al. Wilds: A benchmark
of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–
5664. PMLR, 2021."
REFERENCES,0.6705202312138728,"Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,
Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training
group information. In International Conference on Machine Learning, pp. 6781–6792. PMLR,
2021."
REFERENCES,0.6763005780346821,"Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. Advances in Neural Information Processing
Systems, 33, 2020."
REFERENCES,0.6820809248554913,"Chaochao Lu, Yuhuai Wu, Jo´se Miguel Hernández-Lobato, and Bernhard Schölkopf. Nonlinear
invariant risk minimization: A causal approach. arXiv preprint arXiv:2102.12353, 2021."
REFERENCES,0.6878612716763006,"Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. End-to-end bias mitigation by
modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 8706–8716, 2020."
REFERENCES,0.6936416184971098,"Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant pre-
diction: identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical Society. Series B
(Statistical Methodology), pp. 947–1012, 2016."
REFERENCES,0.6994219653179191,"Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization.
arXiv preprint arXiv:2010.05761, 2020."
REFERENCES,0.7052023121387283,"Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ryxGuJrFvS."
REFERENCES,0.7109826589595376,"Victor Sanh, Thomas Wolf, Yonatan Belinkov, and Alexander M Rush. Learning from others’ mis-
takes: Avoiding dataset biases without modeling them. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=Hf3qXoiNkR."
REFERENCES,0.7167630057803468,"Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola Ortiz, Enrico Santus, and
Regina Barzilay. Towards debiasing fact veriﬁcation models. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3419–3425, 2019."
REFERENCES,0.7225433526011561,"Prasetya Ajie Utama, Naﬁse Sadat Moosavi, and Iryna Gurevych. Mind the trade-off: Debias-
ing NLU models without degrading the in-distribution performance. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, pp. 8717–8729, Online, July
2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.770. URL
https://aclanthology.org/2020.acl-main.770."
REFERENCES,0.7283236994219653,"Prasetya Ajie Utama, Naﬁse Sadat Moosavi, and Iryna Gurevych. Towards debiasing nlu models
from unknown biases. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pp. 7597–7610, 2020b."
REFERENCES,0.7341040462427746,Under review as a conference paper at ICLR 2022
REFERENCES,0.7398843930635838,"A
EXPERIMENTAL DETAILS"
REFERENCES,0.7456647398843931,"A.1
COLORED MNIST"
REFERENCES,0.7514450867052023,"We consider two training environments that contain 25,000 MNIST images and one validation en-
vironment with 10,000 images. For the test environment, we use ofﬁcial test images of the MNIST
dataset with 10,000 data samples. The color and patch attributes are intended to have high correla-
tions with target labels, but these correlations are unstable as they vary across the training environ-
ments."
REFERENCES,0.7572254335260116,"We train a simple MLP with one hidden layer and ReLU activation function. We use a hidden size
of 390 and learning rates of α =1e−1 and β =1e−3 for inner optimization and outer optimization,
respectively. We consider three steps of gradient descent in the inner optimization in our experi-
ments. In experiments, we use an early stopping on validation environment with pe = 0.2 that has
similar distribution to the training distribution."
REFERENCES,0.7630057803468208,"A.2
CAMELYON17-WILDS"
REFERENCES,0.7687861271676301,"The Camelyon17-wilds is a binary classiﬁcation task whether the central region of a given (96×96)
image patch contains any tumor tissue. The dataset contains training data with 302,436 histopatho-
logical image patches, OOD validation data with 34,904 patches, and OOD test data with 85,054
patches. Each data split does not overlap with the hospitals where the data was collected."
REFERENCES,0.7745664739884393,"We use DenseNet-121 (Huang et al., 2017) without pre-trained parameters, training from scratch
on the Camelyon17-wilds dataset as the ofﬁcial setting from Koh et al. (2021). We found that the
ofﬁcial learning rate 1e−3 is too large in our case; thus, we use learning rates of 1e−5 for both inner
optimization and outer optimization and L2-regularization term of 1e−2. We consider only one step
of inner gradient descent to reduce the computational overhead."
REFERENCES,0.7803468208092486,Figure 5: Data examples of Camelyon17-wilds and FMoW-wilds.
REFERENCES,0.7861271676300579,"A.3
FMOW-WILDS"
REFERENCES,0.791907514450867,"The FMoW-wilds is an image classiﬁcation task that targets to predict 62 categories of building or
land use from given (224×224) images. The training split includes 76,863 images taken from 2002
to 2013, and the validation split contains 19,915 images from 2016 to 2018. The OOD test data
comprises 22,108 images from the years from 2016 to 2018. All data splits contain images from
ﬁve regions, Africa, Americas, Oceania, Asia, and Europe. We evaluate the model by measuring the
worst-region accuracy on the OOD test data."
REFERENCES,0.7976878612716763,"We train DenseNet-121 pre-trained on the ImageNet dataset on the FMoW-wilds images. We follow
an ofﬁcial hyperparameter from (Koh et al., 2021), optimizing with learning rates of 1e−4 for
inner and outer learning rate and without L2-regularization. We consider only one step of gradient
descent in the inner optimization."
REFERENCES,0.8034682080924855,Under review as a conference paper at ICLR 2022
REFERENCES,0.8092485549132948,"B
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.815028901734104,"B.1
SHALLOW MODEL ANALYSIS ON COLORED MNIST"
REFERENCES,0.8208092485549133,"We analyze the predictions of the shallow models and show our shallow models can keep discover
biases. Figure 6 shows the incorrect prediction results of shallow models as stacked bar charts. The
legend of each graph indicates the class index and attribute indices, which shows the class indices
highly correlated with the attributes. For example, the legend (y = 0, Color = 1, Patch = 0) means
samples whose color attribute is highly correlated with the label y = 1, and patch attribute is highly
correlated with y = 0."
REFERENCES,0.8265895953757225,"At the ﬁrst learning step (step 0) in Figure 6a, most of misclassiﬁed samples, predicted as ˆy = 1,
have the Color indices as 1, indicating that the shallow models exploit the Color for predictions.
The shallow models begin to leverage the Patch after a few learning steps. The stacked bar charts
demonstrate that the shallow models discover biases dynamically and a few steps of gradient descent
in the inner optimization can make the robust parameters θ become parameters for identifying biases."
REFERENCES,0.8323699421965318,"(a)
(b)"
REFERENCES,0.838150289017341,"(c)
(d)"
REFERENCES,0.8439306358381503,"Figure 6: Visualization of the number of incorrectly predicted samples by shallow models fφ1 and
fφ2 on the Colored MNIST. The stacked bar chart (a) and (b) indicates the misclassiﬁed examples
of environment e2 predicted by fφ1, and (c) and (d) indicates incorrect prediction of fφ2 on environ-
ment e1."
REFERENCES,0.8497109826589595,"Figure 7 includes (a) Pearson correlation between prediction and each attribute, (b) shallow model’s
conﬁdence for incorrect predictions, and (c) visualization of dominant spurious features for re-
grouping process. The conﬁdence is the prediction probability of the shallow models. At the be-
ginning of the learning steps, the conﬁdence is high, indicating high loss for misclassiﬁed samples,
while it decreases gradually. Considering the conﬁdence for misclassiﬁed samples of shallow mod-
els with Figure 6, the shallow models still discover biases from the inner optimization conditioned
on the parameters θ at the end of the learning steps while the conﬁdence is low."
REFERENCES,0.8554913294797688,Under review as a conference paper at ICLR 2022
REFERENCES,0.861271676300578,"Figure 7: Visualization of (a) Pearson correlation coefﬁcient between spurious features and mod-
els prediction, (b) conﬁdence scores of the shallow model’s wrong predictions, and (c) dominant
spurious attributes for re-grouping. Conﬁdence is measured as the average of the top 20% softmax
probabilities values. In (c), darker green indicates strong negative Pearson correlations between
attributes and target labels, representing the more dominant features in the crafted groups."
REFERENCES,0.8670520231213873,"B.2
CELEBA RESULTS"
REFERENCES,0.8728323699421965,"We evaluate BLOOD on the CelebA dataset, which provides rich information of attributes of each
image with 40 binary features. We split training data into two training environments: Male and Fe-
male, and the task is classifying whether input images have blonde hair or not. In table 4, we measure
the Pearson correlation coefﬁcient between each attribute and the targets across the environments,
Male and Female, and report three attributes Black_Hair, Brown_Hair, and Bushy_Eyebrows which
are highly correlated with labels, and one attribute, Attractive, which has relatively low correlation
coefﬁcients."
REFERENCES,0.8786127167630058,"Table 5 shows the test results on the CelebA task. We divide test data into four groups by using
each binary attribute and the target, and we measure the worst-group accuracy and average accuracy
across those groups. As Sagawa* et al. (2020) and Bao et al. (2021) reported, a model trained in"
REFERENCES,0.884393063583815,"Table 4: Pearson correlation coefﬁcient of the environment Male and Female between each attribute
and the label across four attributes."
REFERENCES,0.8901734104046243,"Attribute
Male
Female"
REFERENCES,0.8959537572254336,"Black_Hair
−0.0929
−0.2814
Brown_Hair
−0.0264
−0.2691
Bushy_Eyebrows
−0.0597
−0.1252"
REFERENCES,0.9017341040462428,"Attractive
0.0106
0.0499"
REFERENCES,0.9075144508670521,Under review as a conference paper at ICLR 2022
REFERENCES,0.9132947976878613,Table 5: Test accuracy across four attributes on CelebA dataset.
REFERENCES,0.9190751445086706,"ERM
DRO
PI
BLOOD (Ours)"
REFERENCES,0.9248554913294798,"Worst
Avg
Worst
Avg
Worst
Avg
Worst
Avg"
REFERENCES,0.930635838150289,"Attractive
67.2
85.8
90.8
92.2
90.0
91.9
89.9
92.0
Black_Hair
76.0
90.9
89.6
93.8
88.1
93.3
88.2
92.0
Brown_Hair
43.7
79.2
64.4
85.7
59.8
83.8
74.7
92.0
Bushy_Eyebrows
72.7
86.5
72.7
88.8
81.8
90.8
90.6
92.0"
REFERENCES,0.9364161849710982,"Average
60.1
83.6
84.3
90.8
87.0
91.4
87.1
91.7"
REFERENCES,0.9421965317919075,"Table 6: Test accuracy of BLOOD for four different groups on the CelebA task. N/A indicates that
there is no test data sample in the group."
REFERENCES,0.9479768786127167,"Attribute
y = 0, a = 0
y = 0, a = 1
y = 1, a = 0
y = 1, a = 1
Worst"
REFERENCES,0.953757225433526,"Attractive
91.65
92.08
89.90
94.48
89.90
Black_Hair
88.16
99.96
92.89
N/A
88.16
Brown_Hair
92.25
90.31
93.51
74.71
74.71
Bushy_Eyebrows
90.63
98.91
92.91
90.91
90.63"
REFERENCES,0.9595375722543352,"ERM learns spurious associations; thus, it is vulnerable to group shifts, showing degraded worst-
group accuracy than average accuracy for each attribute. Among the attributes, the performance
degradation of ERM is the most severe for Brown_Hair, and our algorithm achieves the highest
worst-group accuracy than any other baselines. Table 6 provides the detailed results of our algorithm
on the CelebA task."
REFERENCES,0.9653179190751445,"We analyze created groups by shallow models to verify their ability to discover biases. As shown in
Table 5, we can naively assume that the attribute Brown_Hair is a biased feature since the general-
ization performance for the test environment is signiﬁcantly degenerated. Figure 8 shows Pearson
correlation coefﬁcient between targets and attributes of created groups with incorrectly predicted
samples, (a) G⊗
F emale→Male and (b) G⊗
Male→F emale."
REFERENCES,0.9710982658959537,"The results in Figure 8 demonstrate that the shallow models successfully discover Brown_Hair even
though Black_Hair is the most correlated attribute with targets. Moreover, the groups are dynam-
ically constructed for every learning steps while the effect of bias feature on shallow models de-
creases as learning progresses."
REFERENCES,0.976878612716763,"We also analyze the shallow model’s conﬁdence on the CelebA dataset in Figure 9. As the analysis
of conﬁdence of shallow model on Colored MNIST in Figure 7, the conﬁdence decreases as learn-
ing progresses. In effect, the shallow models keep learning biases with few-show learning, their
conﬁdence scores are gradually decreased as the underlying robust model becomes more robust."
REFERENCES,0.9826589595375722,Under review as a conference paper at ICLR 2022 (a) (b)
REFERENCES,0.9884393063583815,"Figure 8: Pearson correlation coefﬁcients between the target labels and attribute values of created
misclassiﬁed groups, (a) regrouping the environment Male by shallow model trained on Female,
G⊗
F emale→Male and (b) G⊗
Male→F emale."
REFERENCES,0.9942196531791907,"Figure 9: Conﬁdence scores of the shallow model’s wrong predictions on disjoint environment.
Conﬁdence is measured as the average of the top 20% softmax probabilities values."
