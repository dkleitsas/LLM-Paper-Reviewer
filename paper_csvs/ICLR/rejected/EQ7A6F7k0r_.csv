Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035714285714285713,"The advent of noisy intermediate-scale quantum (NISQ) computers raises a cru-
cial challenge to design quantum neural networks for fully quantum learning tasks.
To bridge the gap, this work proposes an end-to-end learning framework named
QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quan-
tum embedding on a variational quantum circuit (VQC). The architecture of QTN
is composed of a parametric tensor-train network for feature extraction and a ten-
sor product encoding for quantum embedding. We highlight the QTN for quan-
tum embedding in terms of two perspectives: (1) we theoretically characterize
QTN by analyzing its representation power of input features; (2) QTN enables an
end-to-end parametric model pipeline, namely QTN-VQC, from the generation of
quantum embedding to the output measurement. Our experiments on the MNIST
dataset demonstrate the advantages of QTN for quantum embedding over other
quantum embedding approaches."
INTRODUCTION,0.007142857142857143,"1
INTRODUCTION"
INTRODUCTION,0.010714285714285714,"The state-of-the-art machine learning (ML), particularly based on deep neural networks (DNN),
has enabled a wide spectrum of successful applications ranging from the everyday deployment of
speech recognition (Deng et al., 2013) and computer vision (Sermanet et al., 2014) through to the
frontier of scientiﬁc research in synthetic biology (Jumper et al., 2021). Despite rapid theoretical
and empirical progress in DNN based regression and classiﬁcation (Goodfellow et al., 2016), DNN
training algorithms are computationally expensive for many new scientiﬁc applications, such as
new drug discovery (Smalley, 2017), which requires computational resources that are beyond the
computational limits of classical hardwares (Freedman, 2019). Fortunately, the imminent advent
of quantum computing devices opens up new possibilities of exploiting quantum machine learning
(QML) (Biamonte et al., 2017; Schuld et al., 2015; Schuld & Petruccione, 2018; Schuld & Killoran,
2019; Saggio et al., 2021; Dunjko, 2021) to improve the computational efﬁciency of ML algorithms
in the new scientiﬁc domains."
INTRODUCTION,0.014285714285714285,"Although the exploitation of quantum computing devices to carry out QML is still in its initial
exploratory stages, the rapid development in quantum hardware has motivated advances in quantum
neural networks (QNN) to run in noisy intermediate-scale quantum (NISQ) devices (Preskill, 2018;
Huggins et al., 2019; Huang et al., 2021; Kandala et al., 2017). A NISQ device means that not
enough qubits could be spared for quantum error correction, and the imperfect qubits have to be
directly used at the physical layer. Even though, a compromised QNN approach is proposed by
employing hybrid quantum-classical models that rely on the optimization of variational quantum
circuits (VQC) (Benedetti et al., 2019; Mitarai et al., 2018). The resilience of the VQC based
models to certain types of quantum noise errors and high ﬂexibility concerning coherence time and
gate requirements (McClean et al., 2018) admit many practical implementations of QNN on NISQ
devices (Chen et al., 2020b; Yang et al., 2021; Du et al., 2020; 2021; Skolik et al., 2021; Dunjko
et al., 2016; Jerbi et al., 2021; Ostaszewski et al., 2021). One notable limitation in the current QNN
training pipeline is that the quantum embedding is not fully realizable in a quantum computer, which
may impede the learning of the QNN. Hence, this work proposes QTN-VQC to enable an end-to-
end trainable QNN, including data embedding to quantum measurements, that are easily realizable
in quantum devices, where QTN stands for the quantum tensor network (Or´us, 2019; Huckle et al.,
2013; Biamonte et al., 2017; Murg et al., 2010) for generating quantum embedding."
INTRODUCTION,0.017857142857142856,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02142857142857143,"Ux|0⟩⨂S = |ψx⟩
Hθ|ψx⟩= |gθ(x)⟩ |0⟩ |0⟩ |0⟩"
INTRODUCTION,0.025,Classical data x
INTRODUCTION,0.02857142857142857,"Variational Quantum Circuit
Quantum Embedding Generation z1 z2 zS"
INTRODUCTION,0.03214285714285714,"Measurement
Hθ
ℳ(|gθ(x)⟩)
Hx"
INTRODUCTION,0.03571428571428571,Figure 1: An illustration of QNN based on VQC.
INTRODUCTION,0.039285714285714285,Ux|0⟩⨂S = |ψx⟩ |0⟩ |0⟩ |0⟩
INTRODUCTION,0.04285714285714286,Input y
INTRODUCTION,0.04642857142857143,(a) A dense layer for dimension reduction
INTRODUCTION,0.05,Ux|0⟩⨂S = |ψx⟩ |0⟩ |0⟩ |0⟩
INTRODUCTION,0.05357142857142857,(b) A Tensor-Train layer for dimension reduction
INTRODUCTION,0.05714285714285714,"Quantum 
Embedding"
INTRODUCTION,0.060714285714285714,"Quantum 
Embedding"
INTRODUCTION,0.06428571428571428,"x
Dense
TTN
x
Input y"
INTRODUCTION,0.06785714285714285,"Figure 2: Different paradigms for quantum embedding. (a) a dense layer is used to generate low-
dimensional vector x from a high-dimensional one y; (b) a TTN is used for dimension reduction."
INTRODUCTION,0.07142857142857142,"As shown in Figure 1, our QNN builds a unitary linear operator that consists of three main com-
ponents: (1) quantum embedding generation; (2) variational quantum circuit; (3) measurement.
Quantum embedding generation, also known as quantum encoding, applies a ﬁxed unitary linear
operator Hx transforming classical vectors x to quantum states |ψx⟩in a Hilbert space. This step
is an important aspect of designing quantum algorithms that directly impact the entire computation
cost of VQC and owns a characteristic of quantum superposition. Moreover, the VQC comprises
two types of quantum gates: (1) Controlled-NOT (CNOT) gates; (2) learnable parametric quantum
gates. The CNOT gates ensure the property of quantum entanglement through mutually connecting
the qubits, and the parametric quantum gates can be adjustable to best ﬁt the quantum input states.
The model parameters of VQC should be optimized by employing variants of gradient descent al-
gorithms during the training process. Those parametric quantum gates of VQC are similar to the
weights assigned to DNN, and such quantum circuits have been justiﬁed to be resilient to quantum
noises (Farhi et al., 2014; Kandala et al., 2017; McClean et al., 2016). Besides, the measurement
M(|gθ(x)⟩) aims at projecting the quantum output states |gθ(x)⟩to one classical output zi."
INTRODUCTION,0.075,"This work focuses on quantum embedding generation because it is quite related to the practical usage
in machine learning applications in terms of computational cost and representation capability of
classical input features. In particular, we design a novel quantum tensor network (QTN) for quantum
embedding generation. More speciﬁcally, the QTN consists of a tensor-train network (TTN) for
dimension reduction and a quantum tensor encoding framework for outputting quantum embeddings.
The dimension reduction is a necessary procedure before the quantum encoding because only a
small number of qubits could be supported on available NISQ computers at this moment. A typical
approach for dimension reduction relies on a classical fully-connected layer, also known as a dense
layer, to convert high-dimensional input vectors y into low-dimensional ones x. However, since a
dense layer cannot be physically mapped on a quantum computer, much overhead has to be incurred
by frequently communicating between classical and quantum devices during the end-to-end training
pipeline."
INTRODUCTION,0.07857142857142857,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.08214285714285714,"As shown in Figure 2 (b), one of our contribution is to leverage a tensor train network (TTN)
to replace the dense layer in Figure 2 (a). The beneﬁts of applying TTN arise from two aspects:
(1) TTN can maintain the representation power of the dense layer, which will be justiﬁed in our
theorems; (2) TTN is a tensor network and can be ﬂexibly placed in quantum computers, which
enables an end-to-end training process fully conducted in a quantum computer. Moreover, in this
work, a tensor product encoding (TPE) is delicately designed for generating quantum embedding,
which builds the relationship between a classical vector x and the corresponding quantum state |x⟩;
Besides, we further investigate the representation of QTN-VQC in terms of model size and non-
linear activation function used in TTN. We denote a QTN as the combination of TTN and TPE and
utilize QTN-VQC as a genuine end-to-end learning framework for QNN."
RELATED WORK,0.08571428571428572,"2
RELATED WORK"
RELATED WORK,0.08928571428571429,"The work (Schuld & Petruccione, 2018; Biamonte et al., 2017; Dunjko & Briegel, 2018) demonstrate
that VQC shows great promise in surpassing the performance of classical ML. Prominent examples
of VQC based models include quantum approximate optimization algorithm (QAOA) (Farhi et al.,
2014), and quantum circuit learning (QCL) (Mitarai et al., 2018). Various architectures and geome-
tries of VQC have been shown in tasks ranging from image classiﬁcation (Henderson et al., 2020;
Chen et al., 2020a; Kerenidis et al., 2020) to reinforcement learning (Chen et al., 2020b)."
RELATED WORK,0.09285714285714286,"As for quantum embedding, basis encoding is the process of associating classical input data in the
form of binary strings with the computational basis state of a quantum system (Leymann & Barzen,
2020). Similarly, amplitude encoding is a technique involving encoding data into the amplitudes of
a quantum state (Soklakov & Schack, 2006). Unfortunately, the computational cost of both quantum
embedding and amplitude encoding becomes exponentially expensive with the increasing number of
qubits (Schuld & Killoran, 2019). A new technique of angle embedding makes use of the quantum
gates to generate quantum states (Fu et al., 2011), but it cannot deal with the high-dimensional
feature inputs. Therefore, this work exploits the use of TTN for dimension reduction followed by a
TPE for generating quantum embedding."
RELATED WORK,0.09642857142857143,"In particular, this work employs the TTN for dimensionality reduction. The TTN model based
on TT decomposition in neural networks was ﬁrst proposed in (Oseledets, 2011), and it could be
ﬂexibly extended the convolutional neural network (CNN) (Garipov et al., 2016) and recurrent neural
network (RNN) (Tjandra et al., 2017). The empirical study of TTN on machine learning tasks
shows that TTN is capable of maintaining the DNN baseline results (Qi et al., 2020a; Yu et al.,
2017; Yang et al., 2017; Jin et al., 2020). However, to our best knowledge, no existing works have
applied TTN to QML. Besides, since the tensor network-based machine learning model like TTN is
closely related to quantum machine learning in terms of their model structures (Liu & Wang, 2018;
Gao et al., 2017), the QTN-VQC model can be directly regarded as the classical simulation of the
corresponding quantum machine learning. In addition to a classical dense layer, more complicated
architectures like AlexNet (Lloyd et al., 2020) could be used for dimension reduction, and we also
compare the performance between TTN and AlexNet-based models."
NOTATIONS,0.1,"3
NOTATIONS"
NOTATIONS,0.10357142857142858,"We denote RI as a I-dimensional real coordinate space, and RI1×I2×···IK refers to a space of K-
order tensors. The symbol W ∈RI1×I2×···×IK represents a K-order multi-dimensional tensor in
RI1×I2×···IK, and the symbols v ∈RI and W ∈RI×J represent a vector and a matrix, respectively."
NOTATIONS,0.10714285714285714,"For the notations of quantum computing, ∀v ∈RI, the symbol |v⟩denotes a quantum state associ-
ated with a 2I-dimensional vector in a Hilbert space. Particularly, |0⟩= [1 0]T and |1⟩= [0 1]T ."
NOTATIONS,0.11071428571428571,"The quantum gate RY (θ) means a Pauli-Y gate with a unitary operator as deﬁned in Eq. (1), which
implies a qubit rotates the Bloch sphere along the Y-axis by a given angle θ."
NOTATIONS,0.11428571428571428,"RY (θ) =

cos θ"
NOTATIONS,0.11785714285714285,"2
−i sin θ"
NOTATIONS,0.12142857142857143,"2
−i sin θ"
NOTATIONS,0.125,"2
cos θ 2 
(1)"
NOTATIONS,0.12857142857142856,"Moreover, the operator ⊗is a tensor product. Given the vectors vi ∈RI, the tensor product of I
vectors is deﬁned as ⊗I
i=1vi, which is a 2I-dimensional vector and can provide a compact represen-"
NOTATIONS,0.13214285714285715,Under review as a conference paper at ICLR 2022
NOTATIONS,0.1357142857142857,"r1
r2
rK−1
rK"
NOTATIONS,0.1392857142857143,"IK−2
IK−1
IK
I1
I2
I3"
NOTATIONS,0.14285714285714285,"r0
rK−2"
NOTATIONS,0.14642857142857144,"J1
J2
J3
JK−2
JK−1
JK"
NOTATIONS,0.15,"(a) Tensor-Train Network: given a set of TT-ranks 
, a circle 
represents a core tensor and each line is related to a dimension."
NOTATIONS,0.15357142857142858,"{r1, r2, . . . , rK} |0⟩ |0⟩ |0⟩"
NOTATIONS,0.15714285714285714,"(b)   Tensor Product Encoding: 
 denotes the Pauli-Y rotation gate
RY(πx)"
NOTATIONS,0.16071428571428573,RY(πx1)
NOTATIONS,0.16428571428571428,RY(πx2)
NOTATIONS,0.16785714285714284,RY(πxd)
NOTATIONS,0.17142857142857143,Figure 3: A demonstration of quantum tensor network for quantum embedding.
NOTATIONS,0.175,"tation for v1 ⊗v2 ⊗· · · ⊗vI. Similarly, the symbol |0⟩⊗S means a tensor product of S quantum
states of |0⟩. Furthermore, for a scalar v, the quantum state |v⟩can be written as:"
NOTATIONS,0.17857142857142858,"|v⟩= cos v|0⟩+ sin v|1⟩=

cos v
sin v"
NOTATIONS,0.18214285714285713,"
.
(2)"
NOTATIONS,0.18571428571428572,"4
QTN-VQC: OUR PROPOSED END-TO-END LEARNING FRAMEWORK"
NOTATIONS,0.18928571428571428,"This section introduces our proposed end-to-end learning framework, namely QTN-VQC in this
work. As shown in Figure 3, the QTN model includes two components (a) TTN and (b) TPE, which
will be separately introduced in Section 4.1 and Section 4.2. Moreover, Figure 4 illustrates the
framework of VQC and Section 4.3 is devoted to discussing the details of VQC."
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.19285714285714287,"4.1
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.19642857142857142,"We leverage TTN (Novikov et al., 2015) for the dimension reduction of input features. TTN re-
lies on the TT decomposition (Oseledets, 2011) and has been commonly employed in machine
learning tasks like speech processing (Qi et al., 2020b) and computer vision (Yang et al., 2017).
The TT decomposition assumes that given a set of TT-ranks {r0, r1, ..., rK}, a K-order tensor
W ∈RI1×I2×···×IK is factorized into the multiplication of 3-order tensors Xk ∈Rrk−1×Ik×rk.
In more detail, given a set of indices {i1, i2, ..., iK}, X(i1, i2, ..., iK) is decomposed as:"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.2,"X(i1, i2, ..., iK) = K
Y"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.20357142857142857,"k=1
Xk(ik),
(3)"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.20714285714285716,"where ∀ik ∈[Ik], Xk(ik) ∈Rrk−1×rk. Since r0 = r1 = 1, the term QK
k=1 Xk(ik) is a scalar value."
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.21071428571428572,"TTN employs the TT decomposition in a dense layer and is explicitly demonstrated in Figure 3 (a).
In more detail, for an input tensor X ∈RI1×I2×···×IK and an output tensor Y ∈RJ1×J2×···×JK,
we achieve"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.21428571428571427,"Y(j1, j2, ..., jK) = I1
X i1=1 I2
X"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.21785714285714286,"i2=1
· · · IK
X"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.22142857142857142,"iK=1
W ((i1, j1), (i2, j2), ..., (iK, jK)) X(i1, i2, ..., iK) = I1
X i1=1 I2
X"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.225,"i2=1
· · · IK
X iK=1 K
Y"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.22857142857142856,"k=1
Wk(ik, jk) ! · K
Y"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.23214285714285715,"k=1
Xk(ik) = K
Y k=1 Ik
X"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.2357142857142857,"ik=1
Wk(ik, jk)Xk(ik) ! = K
Y"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.2392857142857143,"k=1
Yk(jk), (4)"
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.24285714285714285,Under review as a conference paper at ICLR 2022
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.24642857142857144,"where Xk(ik) ∈Rrk−1×rk, and Yk(jk) ∈Rrk−1×rk which results in a scalar QK
k=1 Yk(jk) be-
cause of the ranks r0 = r1 = 1; W((i1, j1), (i2, j2), ..., (iK, jK)) is closely associated with
W(m1, m2, ..., mK) as deﬁned in Eq. (3), if each index mk = ik ×jk is set. The multi-dimensional
tensor W is decomposed into the multiplication of 4-order tensors Wk ∈Rrk−1×Ik×Jk×rk. A non-
linear activation function, e.g., Sigmoid, Tanh, and ReLU, is imposed upon the tensor Y. Compared
with a dense layer with QK
k=1 IkJk parameters, a TTN owns as few as PK
k=1 rkrk−1IkJk trainable
parameters."
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.25,"When a TTN is utilized for the dimension reduction, the high-dimensional input vector x ∈RI is
ﬁrst reshaped into a tensor X ∈RI1×I2×···×IK, and then we can represent X as a TT format that
goes through TTN. The outputs of TTN can be converted back to a tensor Y ∈RJ1×J2×···×JK,
which is further reshaped to a lower dimensional vector y ∈RJ. Here, we deﬁne QK
k=1 Ik = I and
QK
k=1 Jk = J. Moreover, the computational complexities of TTN and the related dense layer are in
the same scale, which is discussed in (Yang et al., 2017)."
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.25357142857142856,"Eq. (4) suggests that TTN is a multi-dimensional extension of a dense layer, where the trainable
weight matrix of a dense layer is changed to the learnable core tensors. Additionally, many empirical
studies demonstrate that a TTN is capable of maintaining the baseline results of the dense layer (Qi
et al., 2020b; Yang et al., 2017; Novikov et al., 2015; Qi et al., 2020a). More signiﬁcantly, since
TTN can be ﬂexibly mapped into a quantum circuit, the quantumness inherent in TTN brings great
advantages over other architectures like the dense layer. In other words, although TTN is treated
classically, it is possible to substitute equivalent quantum circuits for TTN when more qubits become
available (Du et al., 2020), which implies that QTN-VQC stands for a genuine end-to-end QNN
learning architecture on a quantum computer."
TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION,0.2571428571428571,"Furthermore, since the gradient exploding and diminishing problems are serious issues in the TTN
training. To avoid those training problems, we only consider 3-order core tensors and small TT-ranks
to conﬁgure a simple TTN in our experimental simulations. Our theoretical analysis of QTN-VQC
based on Theorem 3 in Section 5 suggests that the representation power is not related to TT-ranks
and the tensor order K, thus small TT-ranks and the tensor order K are preferred. In particular, a
lower K can signiﬁcantly reduce the computational cost and speed up the convergence rate."
TENSOR PRODUCT ENCODING,0.26071428571428573,"4.2
TENSOR PRODUCT ENCODING"
TENSOR PRODUCT ENCODING,0.2642857142857143,"In this subsection, we ﬁrst introduce Theorem 1, and then we derive our TPE associated with the
circuits in Figure 3 (b).
Theorem 1. Given the classical vector x = [x1, x2, ..., xI]T ∈RI, a TPE as shown in Figure 3 (b)
can result in a quantum state |x⟩with the following complete vector representation as:
 
⊗I
i=1RY (2xi)

|0⟩⊗I =

cos x1
sin x1"
TENSOR PRODUCT ENCODING,0.26785714285714285,"
⊗

cos x2
sin x2"
TENSOR PRODUCT ENCODING,0.2714285714285714,"
⊗· · · ⊗

cos xI
sin xI"
TENSOR PRODUCT ENCODING,0.275,"
= |x⟩.
(5)"
TENSOR PRODUCT ENCODING,0.2785714285714286,"Proof. Since each element xi in the vector x can be written as |xi⟩= cos xi|0⟩+ sin xi|1⟩, the
quantum state |x⟩can be written as:"
TENSOR PRODUCT ENCODING,0.28214285714285714,"|x⟩=

cos x1
sin x1"
TENSOR PRODUCT ENCODING,0.2857142857142857,"
⊗

cos x2
sin x2"
TENSOR PRODUCT ENCODING,0.2892857142857143,"
⊗· · · ⊗

cos xI
sin xI"
TENSOR PRODUCT ENCODING,0.29285714285714287,"
.
(6)"
TENSOR PRODUCT ENCODING,0.29642857142857143,"When the vector x goes through the quantum tensor network, which implies the following as:
RY (2xi)|0⟩= cosxi|0⟩+ sinxi|1⟩= |xi⟩.
(7)
The preceding equation, in turn, implies that Eq. (5)."
TENSOR PRODUCT ENCODING,0.3,"Theorem 1 builds a connection between the vector x and the quantum state |x⟩, and the resulting
|x⟩is taken as the quantum embedding as the inputs to VQC. Since ⊗I
i=1RY (2xi) is a reversely
unitary linear operator, there is no information loss incurred during the stage of quantum encoding.
Furthermore, if the input is multiplied with a constant π"
TENSOR PRODUCT ENCODING,0.30357142857142855,"2 , we obtain the following term as:
 
⊗I
i=1RY (πxi)

|0⟩⊗I =

cos(πx1)
sin(πx1)"
TENSOR PRODUCT ENCODING,0.30714285714285716,"
⊗

cos(πx2)
sin(πx2)"
TENSOR PRODUCT ENCODING,0.3107142857142857,"
⊗· · · ⊗

cos(πxI)
sin(πxI)"
TENSOR PRODUCT ENCODING,0.3142857142857143,"
,
(8)"
TENSOR PRODUCT ENCODING,0.31785714285714284,which corresponds to Figure 3 (b).
TENSOR PRODUCT ENCODING,0.32142857142857145,Under review as a conference paper at ICLR 2022 |0⟩ |0⟩ |0⟩ |0⟩
TENSOR PRODUCT ENCODING,0.325,"⨁
··· ·
⨁ ⨁ ⨁"
TENSOR PRODUCT ENCODING,0.32857142857142857,"RX(α1)
RY(β1)
RZ(γ1)"
TENSOR PRODUCT ENCODING,0.33214285714285713,"RX(α2)
RY(β2)
RZ(γ2)"
TENSOR PRODUCT ENCODING,0.3357142857142857,"RX(α3)
RY(β3)
RZ(γ3)"
TENSOR PRODUCT ENCODING,0.3392857142857143,"RX(α4)
RY(β4)
RZ(γ4) Z1 Z2 Z4 Z3"
TENSOR PRODUCT ENCODING,0.34285714285714286,"(a) Variational quantum circuit: the dashed square indicates repeated model with CNOT gates for entangling 
quantum states, and 
, 
, 
 represent Pauli-X, Y, Z gates with free parameters 
RX(α) RY(β) RZ(γ)
α, β, γ"
TENSOR PRODUCT ENCODING,0.3464285714285714,RY(β) = cos β
TENSOR PRODUCT ENCODING,0.35,"2
−sin β 2 sin β"
COS,0.3535714285714286,"2
cos β 2"
COS,0.35714285714285715,RX(α) = [ cos α
COS,0.3607142857142857,"2
−i sin α"
COS,0.36428571428571427,"2
−i sin α"
COS,0.3678571428571429,"2
cos α 2 ]"
COS,0.37142857142857144,RZ(γ) =
COS,0.375,"exp(−i γ 2 )
0"
COS,0.37857142857142856,"0
exp(i γ 2 )"
COS,0.3821428571428571,"Pauli-X
Pauli-Y
Pauli-Z ="
COS,0.38571428571428573,"1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
⨁·"
COS,0.3892857142857143,CNOT gate
COS,0.39285714285714285,(b) Matrix representation for the Quantum gates applied in the VQC
COS,0.3964285714285714,Figure 4: A framework of variational quantum circuit.
THE FRAMEWORK OF VARIATIONAL QUANTUM CIRCUIT,0.4,"4.3
THE FRAMEWORK OF VARIATIONAL QUANTUM CIRCUIT"
THE FRAMEWORK OF VARIATIONAL QUANTUM CIRCUIT,0.4035714285714286,"The framework of VQC is shown in Figure 4 (a), where 4 qubit wires are taken into account, and
the CNOT gates aim at mutually entangling the channels such that |x1⟩, |x2⟩, |x3⟩and |x4⟩lie
in the same entanglement state. The Pauli-X, Y, Z gates RX(·), RY (·) and RZ(·) with learnable
parameters (α1, β1, γ1), (α2, β2, γ2), (α3, β3, γ3), (α4, β4, γ4) are built to set up the learnable part.
Being similar to the unitary operators of RY (α), RX(β) and RZ(γ), which are deﬁned in Figure 4
(b), are separately associated with the rotations along X-axis and Z-axis by the given angles of β and
γ. Besides, the quantum circuits in the dash square can be repeatedly copied to compose a deeper
architecture. The outputs of VQC are connected to the measurement which projects the quantum
states into a certain quantum basis that becomes a classical scalar zi."
THE FRAMEWORK OF VARIATIONAL QUANTUM CIRCUIT,0.40714285714285714,"As for the end-to-end training paradigm for QTN-VQC, the learnable parameters come from the
VQC and TTN models, and they should be updated by applying the back-propagation algorithm
based on the Adam optimizer. Given D qubits and H depths, there are totally 3DH trainable
parameters for VQC. Consequently, there are PK
k=1 rk−1rkIkJk + 3DH parameters for QTN-
VQC. On the other hand, the Dense-VQC model possesses more model parameters than QTN-VQC
(QK
k=1 rk−1rkIkJk + 3DH vs. PK
k=1 rk−1rkIkJk + 3DH)."
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.4107142857142857,"5
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.4142857142857143,"This section focuses on analyzing the representation power of QTN-VQC. As shown in Figure 5,
given d qubits and a target quantum state |z⟩= ⊗D
d=1|zd⟩, since Hθ is known as a linear operator and
Tx is deﬁned as a deﬁnite mapping from input x to the unitary matrix Ux, the representation power
of QTN-VQC is determined by how TTN can approximate the classical vector T −1
x
(H−1
θ |z⟩). To
understand the expressiveness of TTN, we ﬁrst start with the discussion on the expressive capability
of Dense-VQC (a dense layer is taken for dimension reduction) and then generalize it to QTN-VQC.
Based on the universal approximation theorem (Cybenko, 1989; Barron, 1994) for a feed-forward
neural network, we derive the following theorem as:
Theorem 2. Given a target vector T −1
x
(H−1
θ |z⟩), there exists a feed-forward neural network fdense
with a dense layer connecting to D qubits, then"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.41785714285714287,"|| T −1
x
(H−1
θ |z⟩) −fdense(y) ||1 ≤
C
√"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.42142857142857143,"D
,
(9)"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.425,"where the activation function tanh(·) is imposed upon the dense layer, and C is a constant associ-
ated with the target vector T −1
x
(H−1
θ |z⟩)."
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.42857142857142855,Under review as a conference paper at ICLR 2022
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.43214285714285716,Dimension Reduction
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.4357142857142857,(TTN / Dense)
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.4392857142857143,Tensor Product Encoder Hx
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.44285714285714284,Variational Quantum Circuit Hθ
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.44642857142857145,|0⟩|0⟩|0⟩|0⟩|0⟩|0⟩
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.45,Figure 5: An illustration of analyzing the representation power of QTN-VQC.
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.45357142857142857,"Since TTN is a compact TT representation of a dense layer, by modifying Theorem 2 for TTN, we
can also derive the upper bound on the approximation error as follows:"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.45714285714285713,"Theorem 3. Given a target vector T −1
x
(H−1
θ |z⟩), there exists a TTN, denoted as fT T N, with a TT
layer connecting to D qubits, then"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.4607142857142857,"|| T −1
x
(H−1
θ |z⟩) −fT T N(y) ||1 ≤ K
Y k=1"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.4642857142857143,"C
√Dk
,
(10)"
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.46785714285714286,"where QK
k=1 Dk = D, the Sigmoid activation function is imposed upon the TTN model, K denotes
the multi-dimensional order, C is a constant associated with the target vector T −1
x
(H−1
θ |z⟩)."
CHARACTERIZING REPRESENTATION POWER OF QTN-VQC,0.4714285714285714,"Comparing the two upper bounds, it is observed that TTN can attain an identical upper bound as
the dense layer on the approximation error because QK
k=1 Dk = D. That implies that TTN can
at least maintain the representation power of a dense layer. Besides, the number of qubits D is a
key factor determining the upper bound on the approximation error. However, D is a small ﬁxed
number on a NISQ device, and a larger number of qubits D is expected to further improve the
representation power of QTN-VQC. However, the computational costs of classical simulation may
grow exponentially with the increasing number of qubits, and a small number of qubits have to be
considered in practice."
EXPERIMENTS AND RESULTS,0.475,"6
EXPERIMENTS AND RESULTS"
EXPERIMENTAL SETUPS,0.4785714285714286,"6.1
EXPERIMENTAL SETUPS"
EXPERIMENTAL SETUPS,0.48214285714285715,"We assess our QTN-VQC based end-to-end learning system on the standard MNIST. MNIST is a
dataset for the task of 10 digit classiﬁcation, where there are 50000 and 10000 28 × 28 image data
assigned for training and testing, respectively. The full MNIST dataset is challenging for quantum
machine learning algorithms, and many works only consider 2-digit classiﬁcation on the MNIST
task (Wang et al., 2021; Chen et al., 2020a). Moreover, the image data are separately reshaped
into 784 dimensional input vectors. Dense-VQC and PCA-VQC are taken as our experimental
baselines to compare with our QTN-VQC model. Dense-VQC denotes that a dense layer is used for
dimension reduction, and PCA-VQC refers to using principal component analysis (PCA) to extract
low-dimensional features before training the VQC parameters."
EXPERIMENTAL SETUPS,0.4857142857142857,"As for the experiments of QTN-VQC, the image data are reshaped into 3-order 7 × 16 × 7 tensors.
We set small TT-ranks as {1, 2, 2, 1} to reduce the computational cost of TTN. the image data are
represented as the TT format according to Eq. (3) before going through the TTN model. Since 8
qubits are used for the quantum encoding, the output of TTN needs to conﬁgure the tensor format
as 2 × 2 × 2, which results in 8 dimensional output vectors. Besides, the model parameters of
QTN-VQC are randomly initialized based on the Gaussian distribution, and the back-propagation
algorithm is applied to train the models. The Sigmoid function is utilized for the hidden layers of
TTN."
EXPERIMENTAL SETUPS,0.48928571428571427,"To be consistent with QTN-VQC, the weight of the dense layer for Dense-VQC is conﬁgured as the
shape of 784×8. Although Dense-VQC is a hybrid classical-quantum model, the training process of
Dense-VQC can also be set as an end-to-end pipeline and the weights of the dense layer are updated
during the training stage. The Sigmoid function is used for the dense layer. On the other hand, PCA"
EXPERIMENTAL SETUPS,0.4928571428571429,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUPS,0.49642857142857144,"is employed to reduce the feature dimension to 8, and the resulting low-dimensional features are
further encoded into quantum states. Consequently, PCA-VQC admits the VQC parameters solely
to be updated during the training stage. A standard AlexNet (Iandola et al., 2016) is employed to
constitute an AlexNet-VQC to compare the performance."
EXPERIMENTAL SETUPS,0.5,"Moreover, 6 VQC layers are constructed to form a deep model, and the outputs of the VQC model
are connected to 10 classes with a non-trainable matrix. The back-propagation algorithm based on
the Adam optimizer with a learning rate of 0.001 is employed for the model training. The loss of
cross-entropy (CE) is utilized as the objective function during the training stage, and it is also taken
as the metric to evaluate the model performance. We leverage the tools of Pennylane (Bergholm
et al., 2018) and PyTorch (Paszke et al., 2019) to simulate the model performance. In particular,
we separately simulate the model performance with noiseless quantum circuits and noisy quantum
circuits corrupted by quantum noises from IBM quantum machines."
EXPERIMENTAL RESULTS OF NOISELESS QUANTUM CIRCUIT,0.5035714285714286,"6.2
EXPERIMENTAL RESULTS OF NOISELESS QUANTUM CIRCUIT"
EXPERIMENTAL RESULTS OF NOISELESS QUANTUM CIRCUIT,0.5071428571428571,"Table 1 shows the ﬁnal results of the models on the test dataset. QTN-VQC owns much fewer model
parameters than Dense-VQC (328 vs. 6416) and attains even higher classiﬁcation accuracy than
Dense-VQC (91.43% vs. 88.54%) and lower loss values than Dense-VQC (0.3090 vs. 0.4132).
However, PCA-VQC with 144 trainable VQC parameters attains the worst performance by all met-
rics, which implies that a trainable quantum embedding is of signiﬁcance to boost experimental
performance. Although our empirical results cannot reach the state-of-the-art classiﬁcation perfor-
mance of classical ML algorithms, our empirical results demonstrate the advantages of QTN-VQC
over the PCA-VQC and Dense-VQC counterparts. With the development of more powerful quan-
tum devices supporting more qubits, the representation power of QTN-VQC can be improved and
better experimental results could be attained. Moreover, AlexNet-VQC achieves better results than
QTN-VQC (92.81%vs.91.43%), but it involves more model parameters than QTN-VQC."
EXPERIMENTAL RESULTS OF NOISELESS QUANTUM CIRCUIT,0.5107142857142857,Table 1: Empirical results on the MNIST test dataset under the noiseless quantum circuit setting.
EXPERIMENTAL RESULTS OF NOISELESS QUANTUM CIRCUIT,0.5142857142857142,"Models
Params
CE
Acc (%).
PCA-VQC
144
0.5877
82.48 ± 1.02
Dense-VQC
6416
0.4132
88.54 ± 0.73
AlexNet-VQC
3.25 × 106
0.2562
92.81 ± 0.47
QTN-VQC
328
0.3090
91.43 ± 0.51"
EXPERIMENTAL RESULTS OF NOISY QUANTUM CIRCUIT,0.5178571428571429,"6.3
EXPERIMENTAL RESULTS OF NOISY QUANTUM CIRCUIT"
EXPERIMENTAL RESULTS OF NOISY QUANTUM CIRCUIT,0.5214285714285715,"To empirically validate the effectiveness of our proposed VQC algorithm, we proceed with the
simulation of the practical experiments with noisy quantum circuits. More speciﬁcally, we follow
an established noisy circuit experiment with the NISQ device suggested by (Chen et al., 2020b). One
major advantage of the setups is to observe the robustness and preserve the quantum advantages of
a deployed VQC with physical settings being close to quantum processing unit (QPU) experiments
without an executive queuing time. As for the detailed setup, we ﬁrst use an IBM Q 20-qubit
machine to collect channel noise in the real scenario for a deployed VQC and upload the machine
noise into our Pennylane-Qiskit simulator (denoted as Accq20. We provide a depolarizing noisy
circuit simulation (denoted as Accdepo) based on a depolarizing channel attained from (Nielsen &
Chuang, 2010) with a noise level of 0.1. As shown in Table 2, the quantum noise brings about
the performance degradation of all models, but our proposed QTN-VQC consistently outperforms
PCA-VQC and Dense-VQC in the condition of noisy quantum circuits. In particular, QTN-VQC
can even outperform the AlexNet-VQC counterpart in noisy circuit conditions."
FURTHER DISCUSSIONS,0.525,"6.4
FURTHER DISCUSSIONS"
FURTHER DISCUSSIONS,0.5285714285714286,"The above experimental results show the advantages of QTN-VQC over Dense-VQC and PCA-
VQC in the scenarios with noiseless and noisy quantum circuits. Next, we will further discuss the
representation power of QTN-VQC based on two factors: (1) the activation function used in TTN;
(2) the number of qubits."
FURTHER DISCUSSIONS,0.5321428571428571,Under review as a conference paper at ICLR 2022
FURTHER DISCUSSIONS,0.5357142857142857,Table 2: Empirical results on the MNIST test dataset under the noisy quantum circuit setting.
FURTHER DISCUSSIONS,0.5392857142857143,"Models
Params
Accq20 (%)
Accdepo (%)
PCA-VQC
144
81.23 ± 1.34
83.12 ± 1.17
Dense-VQC
6416
84.55 ± 1.22
86.09 ± 1.04
AlexNet-VQC
3.25 × 106
87.46 ± 1.34
87.86 ± 1.08
QTN-VQC
328
88.12 ± 1.09
89.32 ± 1.07"
THE ACTIVATION FUNCTION USED IN TTN,0.5428571428571428,"6.4.1
THE ACTIVATION FUNCTION USED IN TTN"
THE ACTIVATION FUNCTION USED IN TTN,0.5464285714285714,"Table 3 compares the results of QTN-VQC based on different activation functions. Our simula-
tion on noiseless quantum circuits shows that the non-linear activation functions can bring more
performance gain than a linear one, but the Sigmoid function attains a better performance than the
Tanh and ReLU counterparts in our experiments. Our experiments also correspond to the universal
approximation theory for QTN-VQC in Theorem 3."
THE ACTIVATION FUNCTION USED IN TTN,0.55,Table 3: Comparing performance of QTN-VQC with and without activation function.
THE ACTIVATION FUNCTION USED IN TTN,0.5535714285714286,"Models
CE
Acc (%).
QTN-VQC (Linear)
0.4958
86.16 ± 0.65
QTN-VQC (Tanh)
0.4792
87.12 ± 0.51
QTN-VQC (ReLU)
0.3764
89.56 ± 0.49
QTN-VQC (Sigmoid)
0.3090
91.43 ± 0.54"
THE NUMBER OF QUBITS,0.5571428571428572,"6.4.2
THE NUMBER OF QUBITS"
THE NUMBER OF QUBITS,0.5607142857142857,"Finally, we investigate the effects of the number of qubits on the performance of QTN-VQC by
increasing the qubits from 8 to 12 and 16. Accordingly, the output of TTN is conﬁgured as a
tensor format of 2 × 3 × 2, and the model size is increased from 328 to 464 and 600, respectively.
Our experiments show that the baseline performance of QTN-VQC can be further improved by
increasing the number of qubits, which implies that more qubits are likely to possess higher accuracy."
THE NUMBER OF QUBITS,0.5642857142857143,Table 4: Comparing performance of QTN-VQC with fewer qubits.
THE NUMBER OF QUBITS,0.5678571428571428,"Models
Params
CE
Acc (%)
QTN-VQC (8 qubits)
328
0.3090
91.43 ± 0.51
QTN-VQC (12 qubits)
464
0.2679
92.36 ± 0.62
QTN-VQC (16 qubits)
600
0.2355
92.98 ± 0.52"
CONCLUSIONS,0.5714285714285714,"7
CONCLUSIONS"
CONCLUSIONS,0.575,"This work proposes a genuine end-to-end learning framework for quantum neural networks based on
QTN-VQC. QTN consists of a TTN for dimension reduction and a TPE framework for generating
quantum embedding. The TTN model is a compact representation of a dense layer to classically
simulate quantum machine learning algorithms. Our theorem on the representation of QTN-VQC
shows that the number of qubits is inversely related to the approximation error of QTN-VQC and the
non-linear activation plays an important role. Our experiments compare our proposed QTN-VQC
with Res-VQC, Dense-VQC, and PCA-VQC. Our simulated results demonstrate that QTN-VQC
obtains better experimental performance than Dense-VQC and PCA-VQC with both noiseless and
noisy quantum circuits, and it achieves marginally worse performance than AlexNet-VQC. Besides,
our results justify our theorem on the representation power of QTN-VQC."
CONCLUSIONS,0.5785714285714286,Under review as a conference paper at ICLR 2022
REFERENCES,0.5821428571428572,REFERENCES
REFERENCES,0.5857142857142857,"Andrew R Barron. Approximation and Estimation Bounds for Artiﬁcial Neural Networks. Machine
Learning, 14(1):115–133, 1994."
REFERENCES,0.5892857142857143,"Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized Quantum Cir-
cuits as Machine Learning Models. Quantum Science and Technology, 4(4):043001, 2019."
REFERENCES,0.5928571428571429,"Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, M Sohaib Alam, Shahnawaz
Ahmed, Juan Miguel Arrazola, Carsten Blank, Alain Delgado, Soran Jahangiri, et al.
Pen-
nylane: Automatic Differentiation of Hybrid Quantum-Classical Computations. arXiv preprint
arXiv:1811.04968, 2018."
REFERENCES,0.5964285714285714,"Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.
Quantum Machine Learning. Nature, 549(7671):195–202, 2017."
REFERENCES,0.6,"Samuel Yen-Chi Chen, Chih-Min Huang, Chia-Wei Hsing, and Ying-Jer Kao. Hybrid Quantum-
Classical Classiﬁer Based on Tensor Network and Variational Quantum Circuit. arXiv preprint
arXiv:2011.14651, 2020a."
REFERENCES,0.6035714285714285,"Samuel Yen-Chi Chen, Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Xiaoli Ma, and Hsi-Sheng
Goan. Variational Quantum Circuits for Deep Reinforcement Learning. IEEE Access, 8:141007–
141024, 2020b."
REFERENCES,0.6071428571428571,"George Cybenko. Approximation by Superpositions of A Sigmoidal Function. Mathematics of
Control, Signals and Systems, 2(4):303–314, 1989."
REFERENCES,0.6107142857142858,"Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael Seltzer, Geoff
Zweig, Xiaodong He, Jason Williams, et al. Recent Advances in Deep Learning for Speech
Research at Microsoft. In Proc. IEEE International Conference on Acoustics, Speech and Signal
Processing, pp. 8604–8608, 2013."
REFERENCES,0.6142857142857143,"Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive Power of Parametrized
Quantum Circuits. Physical Review Research, 2(3):033125, 2020."
REFERENCES,0.6178571428571429,"Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao, and Nana Liu. Quantum Noise Protects
Quantum Classiﬁers Against Adversaries. Physical Review Research, 3(2):023153, 2021."
REFERENCES,0.6214285714285714,"Vedran Dunjko. Inside Quantum Black Boxes. Nature Physics, pp. 1–2, 2021."
REFERENCES,0.625,"Vedran Dunjko and Hans J Briegel. Machine Learning & Artiﬁcial Intelligence in the Quantum
Domain: A Review of Recent Progress. Reports on Progress in Physics, 81(7):074001, 2018."
REFERENCES,0.6285714285714286,"Vedran Dunjko, Jacob M Taylor, and Hans J Briegel. Quantum-Enhanced Machine Learning. Phys-
ical Review Letters, 117(13):130501, 2016."
REFERENCES,0.6321428571428571,"Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A Quantum Approximate Optimization Algo-
rithm. arXiv preprint arXiv:1411.4028, 2014."
REFERENCES,0.6357142857142857,"David H Freedman. Hunting for New Drugs with AI. Nature, 576(7787):S49–S53, 2019."
REFERENCES,0.6392857142857142,"Yangguang Fu, Mingyue Ding, and Chengping Zhou. Phase Angle-Encoded and Quantum-Behaved
Particle Swarm Optimization Applied to Three-Dimensional Route Planning for UAV.
IEEE
Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 42(2):511–526,
2011."
REFERENCES,0.6428571428571429,"Xun Gao, Zhengyu Zhang, and Luming Duan. An Efﬁcient Quantum Algorithm for Generative
Machine Learning. arXiv preprint arXiv:1711.02038, 2017."
REFERENCES,0.6464285714285715,"Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate Tensoriza-
tion: Compressing Convolutional and FC Layers Alike. arXiv preprint arXiv:1611.03214, 2016."
REFERENCES,0.65,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep Learning, volume 1.
MIT Press, 2016."
REFERENCES,0.6535714285714286,Under review as a conference paper at ICLR 2022
REFERENCES,0.6571428571428571,"Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, and Tristan Cook. Quanvolutional
Neural Networks: Powering Image Recognition with Quantum Circuits. Quantum Machine In-
telligence, 2(1):1–9, 2020."
REFERENCES,0.6607142857142857,"Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut
Neven, and Jarrod R McClean. Power of Data in Quantum Machine Learning. Nature Communi-
cations, 12(1):1–9, 2021."
REFERENCES,0.6642857142857143,"Thomas Huckle, Konrad Waldherr, and Thomas Schulte-Herbr¨uggen. Computations in Quantum
Tensor Networks. Linear Algebra and its Applications, 438(2):750–781, 2013."
REFERENCES,0.6678571428571428,"William Huggins, Piyush Patil, Bradley Mitchell, K Birgitta Whaley, and E Miles Stoudenmire.
Towards Quantum Machine Learning with Tensor Networks. Quantum Science and Technology,
4(2):024001, 2019."
REFERENCES,0.6714285714285714,"Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and¡ 0.5 MB Model
Size. arXiv preprint arXiv:1602.07360, 2016."
REFERENCES,0.675,"Soﬁene Jerbi, Casper Gyurik, Simon Marshall, Hans J Briegel, and Vedran Dunjko. Variational
Quantum Policies for Reinforcement Learning. arXiv preprint arXiv:2103.05577, 2021."
REFERENCES,0.6785714285714286,"Xuanyu Jin, Jiajia Tang, Xianghao Kong, Yong Peng, Jianting Cao, Qibin Zhao, and Wanzeng Kong.
CTNN: A Convolutional Tensor-Train Neural Network for Multi-Task Brainprint Recognition.
Proc. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:103–112, 2020."
REFERENCES,0.6821428571428572,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly Accurate
Protein Structure Prediction with AlphaFold. Nature, 596(7873):583–589, 2021."
REFERENCES,0.6857142857142857,"Abhinav Kandala, Antonio Mezzacapo, Kristan Temme, Maika Takita, Markus Brink, Jerry M
Chow, and Jay M Gambetta.
Hardware-Efﬁcient Variational Quantum Eigensolver for Small
Molecules and Quantum Magnets. Nature, 549(7671):242–246, 2017."
REFERENCES,0.6892857142857143,"Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum Algorithms for Deep Convo-
lutional Neural Networks. In Proc. International Conference on Learning Representations, 2020."
REFERENCES,0.6928571428571428,"Frank Leymann and Johanna Barzen. The Bitter Truth About Gate-Based Quantum Algorithms In
the NISQ Era. Quantum Science and Technology, 5(4):044007, 2020."
REFERENCES,0.6964285714285714,"Jin-Guo Liu and Lei Wang. Differentiable Learning of Quantum Circuit Born Machines. Physical
Review A, 98(6):062324, 2018."
REFERENCES,0.7,"Seth Lloyd, Maria Schuld, Aroosa Ijaz, Josh Izaac, and Nathan Killoran. Quantum Embeddings for
Machine Learning. arXiv preprint arXiv:2001.03622, 2020."
REFERENCES,0.7035714285714286,"Jarrod R McClean, Jonathan Romero, Ryan Babbush, and Al´an Aspuru-Guzik.
The Theory of
Variational Hybrid Quantum-Classical Algorithms. New Journal of Physics, 18(2):023023, 2016."
REFERENCES,0.7071428571428572,"Jarrod R McClean, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and Hartmut Neven. Bar-
ren Plateaus in Quantum Neural Network Training Landscapes. Nature Communications, 9(1):
1–6, 2018."
REFERENCES,0.7107142857142857,"Kosuke Mitarai, Makoto Negoro, Masahiro Kitagawa, and Keisuke Fujii. Quantum Circuit Learning.
Physical Review A, 98(3):032309, 2018."
REFERENCES,0.7142857142857143,"Valentin Murg, Frank Verstraete, ¨Ors Legeza, and Reinhard M Noack. Simulating Strongly Corre-
lated Quantum Systems with Tree Tensor Networks. Physical Review B, 82(20):205105, 2010."
REFERENCES,0.7178571428571429,"Michael A Nielsen and Isaac Chuang. Quantum Computation and Quantum Information. Cam-
bridge, 2010."
REFERENCES,0.7214285714285714,"Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing Neural
Networks. In Proc. Advances in Neural Information Processing Systems, 2015."
REFERENCES,0.725,Under review as a conference paper at ICLR 2022
REFERENCES,0.7285714285714285,"Rom´an Or´us. Tensor Networks for Complex Quantum Systems. Nature Reviews Physics, 1(9):
538–550, 2019."
REFERENCES,0.7321428571428571,"Ivan V Oseledets. Tensor-Train Decomposition. SIAM Journal on Scientiﬁc Computing, 33(5):
2295–2317, 2011."
REFERENCES,0.7357142857142858,"Mateusz Ostaszewski, Lea M Trenkwalder, Wojciech Masarczyk, Eleanor Scerri, and Vedran Dun-
jko. Reinforcement Learning for Optimization of Variational Quantum Circuit Architectures.
arXiv preprint arXiv:2103.16089, 2021."
REFERENCES,0.7392857142857143,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An Imperative Style,
High-Performance Deep Learning Library. In Proc. Advances in Neural Information Processing
Systems, volume 32, pp. 8026–8037, 2019."
REFERENCES,0.7428571428571429,"John Preskill. Quantum Computing in the NISQ Era and Beyond. Quantum, 2:79, August 2018.
ISSN 2521-327X."
REFERENCES,0.7464285714285714,"Jun Qi, Hu Hu, Yannan Wang, Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui
Lee. Exploring deep hybrid tensor-to-vector network architectures for regression based speech
enhancement. arXiv preprint arXiv:2007.13024, 2020a."
REFERENCES,0.75,"Jun Qi, Hu Hu, Yannan Wang, Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui
Lee. Tensor-to-Vector Regression for Multi-Channel Speech Enhancement Based on Tensor-Train
Network. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing,
pp. 7504–7508, 2020b."
REFERENCES,0.7535714285714286,"Valeria Saggio, Beate E Asenbeck, Arne Hamann, Teodor Str¨omberg, Peter Schiansky, Vedran Dun-
jko, Nicolai Friis, Nicholas C Harris, Michael Hochberg, Dirk Englund, et al. Quantum Speed-ups
in Reinforcement Learning. In Quantum Nanophotonic Materials, Devices, and Systems 2021,
volume 11806, pp. 118060N. International Society for Optics and Photonics, 2021."
REFERENCES,0.7571428571428571,"Maria Schuld and Nathan Killoran. Quantum Machine Learning in Feature Hilbert Spaces. Physical
Review Letters, 122(4):040504, 2019."
REFERENCES,0.7607142857142857,"Maria Schuld and Francesco Petruccione.
Supervised Learning with Quantum Computers, vol-
ume 17. Springer, 2018."
REFERENCES,0.7642857142857142,"Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. An Introduction to Quantum Machine
Learning. Contemporary Physics, 56(2):172–185, 2015."
REFERENCES,0.7678571428571429,"Pierre Sermanet, David Eigen, Xiang Zhang, Micha¨el Mathieu, Rob Fergus, and Yann LeCun. Over-
feat: Integrated Recognition, Localization and Detection Using Convolutional Networks. In Proc.
International Conference on Learning Representations, 2014."
REFERENCES,0.7714285714285715,"Andrea Skolik, Soﬁene Jerbi, and Vedran Dunjko. Quantum Agents in The Gym: A Variational
Quantum Algorithm for Deep Q-Learning. arXiv preprint arXiv:2103.15084, 2021."
REFERENCES,0.775,"Eric Smalley. AI-Powered Drug Discovery Captures Pharma Interest. Nature Biotechnology, 35(7):
604–606, 2017."
REFERENCES,0.7785714285714286,"Andrei N Soklakov and R¨udiger Schack. Efﬁcient State Preparation for A Register of Quantum Bits.
Physical review A, 73(1):012307, 2006."
REFERENCES,0.7821428571428571,"Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Compressing Recurrent Neural Network
with Tensor-Train. In Proc. International Joint Conference on Neural Networks, pp. 4451–4458,
2017."
REFERENCES,0.7857142857142857,"Hanrui Wang, Yongshan Ding, Jiaqi Gu, Yujun Lin, David Z Pan, Frederic T Chong, and Song
Han.
Quantumnas: Noise-Adaptive Search for Robust Quantum Circuits.
arXiv preprint
arXiv:2107.10845, 2021."
REFERENCES,0.7892857142857143,Under review as a conference paper at ICLR 2022
REFERENCES,0.7928571428571428,"Chao-Han Huck Yang, Jun Qi, Samuel Yen-Chi Chen, Pin-Yu Chen, Sabato Marco Siniscalchi,
Xiaoli Ma, and Chin-Hui Lee. Decentralizing Feature Extraction with Quantum Convolutional
Neural Network for Automatic Speech Recognition. In Proc. IEEE International Conference on
Acoustics, Speech and Signal Processing, pp. 6523–6527, 2021."
REFERENCES,0.7964285714285714,"Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-Train Recurrent Neural Networks for
Video Classiﬁcation. In Proc. International Conference on Machine Learning, pp. 3891–3900,
2017."
REFERENCES,0.8,"R Yu, S Zheng, A Anandkumar, and Y Yue. Long-term Forecasting Using Tensor-Train RNNs.
arXiv preprint arXiv:1711.00073, 31, 2017."
REFERENCES,0.8035714285714286,Under review as a conference paper at ICLR 2022
REFERENCES,0.8071428571428572,"A
APPENDIX"
REFERENCES,0.8107142857142857,The section of appendix includes the proofs for Theorem 2 and Theorem 3.
REFERENCES,0.8142857142857143,"A.1
PROOF FOR THEOREM 2"
REFERENCES,0.8178571428571428,"Proof. Theorem 2 is derived from the modiﬁcation of the universal approximation theory proposed
by Barron (1994); Cybenko (1989). The universal approximation theory is shown in Lemma 1,
which suggests that a feed-forward neural network with d neurons can approximate any continuous
function with arbitrarily small ϵ."
REFERENCES,0.8214285714285714,"Lemma 1. Given a continuous target function ˆf : RI →R, we can employ a 2-layer neural network
with a non-linear activation f : RI →R, such that"
REFERENCES,0.825,"| ˆf −f| ≤
1
√"
REFERENCES,0.8285714285714286,"D
C ˆ
f,
(11)"
REFERENCES,0.8321428571428572,"where J denotes the number of neurons, and C ˆ
f is a constant associated with ˆf. In particular, for
r ≥1, Cf satisﬁes the following condition as:"
REFERENCES,0.8357142857142857,"|| ˆf||∞+
X"
REFERENCES,0.8392857142857143,"k,1≤k(k−1)"
REFERENCES,0.8428571428571429,"2
≤r
||Dk ˆf||∞≤C ˆ
f,
(12)"
REFERENCES,0.8464285714285714,"where Dk ˆf =
h
∇ˆf, ∇2 ˆf, ..., ∇k ˆf
iT
."
REFERENCES,0.85,"To associate Lemma 1 with our Theorem 2, the target function is replaced with the target vector
H−1
X H−1
θ |z⟩, then there is a neural network with a dense layer connected to D qubits such that"
REFERENCES,0.8535714285714285,"||H−1
X H−1
θ |z⟩−fdense(y)||1 ≤
1
√"
REFERENCES,0.8571428571428571,"D
C,
(13)"
REFERENCES,0.8607142857142858,"where C is related to the target vector H−1
X H−1
θ |z⟩."
REFERENCES,0.8642857142857143,"A.2
PROOF FOR THEOREM 3"
REFERENCES,0.8678571428571429,"Proof. Assume that X = fT T N(y), ˆ
X = H−1
X H−1
θ |z⟩and the TT decomposition of target vector is
{ ˆ
X1, ˆ
X2, ..., ˆ
XK}, then we obtain"
REFERENCES,0.8714285714285714,"||H−1
X H−1
θ |z⟩−fT T N(y)||1 = || ˆ
X −X||1 ≤ K
Y"
REFERENCES,0.875,"k=1
|| ˆ
Xk −Xk||1
(14)"
REFERENCES,0.8785714285714286,"On the other hand, we denote vec(Yk) and vec(Xk) as the vectorization of the tensors Yk and Xk,
respectively. We also deﬁne QK
k=1 Ik = I, Wk ∈RDk×Ik×rk−1×rk as the TTN parameters, and
also deﬁne Wk ∈RIk×rk−1rkDk as the matricization of Wk. Moreover, σ refers to a non-linear
activation function."
REFERENCES,0.8821428571428571,"Since vec(Xk) = σ(WT vec(Yk)) that corresponds to a dense layer, we can obtain that"
REFERENCES,0.8857142857142857,"|| ˆ
Xk −Xk||1 ≤
1
√Dk
C.
(15)"
REFERENCES,0.8892857142857142,"In sum, we can further obtain"
REFERENCES,0.8928571428571429,"||H−1
X H−1
θ |z⟩−fT T N(y)||1 ≤ K
Y"
REFERENCES,0.8964285714285715,"k=1
|| ˆ
Xk −Xk||1 ≤ K
Y k=1"
REFERENCES,0.9,"1
√Dk
C,
(16)"
REFERENCES,0.9035714285714286,"where QK
k=1 Dk = D."
REFERENCES,0.9071428571428571,Under review as a conference paper at ICLR 2022
REFERENCES,0.9107142857142857,Figure 6: A comparison of convergence rates for different models.
REFERENCES,0.9142857142857143,"B
APPENDIX"
REFERENCES,0.9178571428571428,"This section includes additional experimental simulations. First, we assess the settings of TT-ranks,
and then we compare the convergence rates of QTN-VQC and Dense-VQC in the experiments."
REFERENCES,0.9214285714285714,"B.1
EXPERIMENTS ON TT-RANKS FOR QTN-VQC"
REFERENCES,0.925,"Table 5 corresponds to the experiments of QTN-VQC with 8 qubits and the Sigmoid function. The
empirical results suggest that the larger TT-ranks cannot result in better results than the smaller ones.
The main reason is that the TT-ranks can correspond to a manifold, and there may potentially exist
an optimal manifold with smaller TT-ranks that corresponds to the best performance."
REFERENCES,0.9285714285714286,Table 5: Comparing performance of different TT-ranks for QTN-VQC
REFERENCES,0.9321428571428572,"TT-ranks
Params
CE
Acc (%)
{1, 2, 2, 1}
328
0.3090
91.43 ± 0.51
{1, 4, 4, 1}
768
0.3082
91.46 ± 0.53
{1, 6, 6, 1}
1464
0.3079
91.47 ± 0.52"
REFERENCES,0.9357142857142857,"B.2
A COMPARISON OF CONVERGENCE RATES"
REFERENCES,0.9392857142857143,"Next, we analyze the computational complexity for TTN for QTN-VQC. In more detail, given the
TT-ranks {r1, r2, ..., rK}, a multi-dimensional tensor W is factorized into several K-order tensors
Wk ∈Rrk−1×Ik×Jk×rk, the computational complexity of the feed-forward process is in the scale of
O(K maxk Ik maxk Jk(maxk rk)K). In contrast, the computational overhead for a dense layer is
in the scale of O(Q"
REFERENCES,0.9428571428571428,"k Ik
Q"
REFERENCES,0.9464285714285714,"k Jk). It means that smaller TT-ranks can reduce the computational cost
for QTN-VQC, which explains that smaller TT-ranks {1, 2, 2, 1} is conﬁgured in our experiments
of QTN-VQC."
REFERENCES,0.95,"Empirically, we compare the convergence rates of different models on the test data in our experi-
ments. In our experimental settings with the Tanh activation function and 8 qubits, the QTN-VQC
model consistently attains a faster convergence rate than the Dense-VQC and PCA-VQC counter-
parts. Moreover, Table 6 compares the absolute running time of QTN-VQC with Dense-VQC and
AlexNet-VQC. Since our experiments are conducted on the same GPUs and CPUs, the training time
of all models can be comparable. Our evaluation shows that QTN-VQC is marginally slower than
Dense-VQC, but it is much faster than AlexNet-VQC."
REFERENCES,0.9535714285714286,Under review as a conference paper at ICLR 2022
REFERENCES,0.9571428571428572,Table 6: Comparing performance of different TT-ranks for QTN-VQC
REFERENCES,0.9607142857142857,"Models
Dense-VQC
AlexNet-VQC
QTN-VQC
Time/epochs (mins)
58
75
61"
REFERENCES,0.9642857142857143,"C
EXPERIMENTS OF LABELED FACES IN THE WILD (LFW)"
REFERENCES,0.9678571428571429,"C.1
EXPERIMENTAL SETUPS"
REFERENCES,0.9714285714285714,"The LFW is a dataset for the task of unconstrained face recognition, which is composed of 13000
images with the shape of [154, 154, 3]. The shape of We randomly split all the datasets into 11000
training data, 2000 test data. 16 qubits are used for VQC, and the shape of the input tensor is set as
22 × 147 × 22. The other settings are kept the same as the conﬁgurations for the MNIST task."
REFERENCES,0.975,"C.2
EXPERIMENTAL RESULTS"
REFERENCES,0.9785714285714285,"Table 6 presents the simulation results under the noiseless quantum circuit condition, while Table
7 demonstrates the empirical results in the setting of noisy quantum circuits. The QTN-VQC out-
performs the Dense-VQC counterpart (92.15 vs. 91.27), and it owns much fewer model parameters
(2816 vs. 1.1 × 106). Although"
REFERENCES,0.9821428571428571,"The experimental results on the LFW dataset also highlight the advantages of QTN-VQC in terms
of fewer model parameters and better empirical performance."
REFERENCES,0.9857142857142858,Table 7: Simulation results on the LFW test dataset under a noiseless circuit condition.
REFERENCES,0.9892857142857143,"Models
Params
CE
Acc (%)
Dense-VQC
1.1 × 106
0.3011
91.27 ± 0.25
AlexNet-VQC
2.3 × 107
0.2875
93.21 ± 0.36
QTN-VQC
2816
0.2910
92.15 ± 0.43"
REFERENCES,0.9928571428571429,Table 8: Empirical results on the LFW test dataset under the noisy quantum circuit setting.
REFERENCES,0.9964285714285714,"Models
Params
Accq20 (%)
Accdepo (%)
Dense-VQC
1.1 × 106
88.65 ± 1.22
87.23 ± 1.04
AlexNet-VQC
2.3 × 107
89.76 ± 1.34
88.66 ± 1.08
QTN-VQC
2816
89.93 ± 1.09
89.64 ± 1.07"
