Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003401360544217687,"Discovery and learning of an underlying spatiotemporal hierarchy in sequential
data is an important topic for machine learning. Despite this, little work has
been done to explore hierarchical generative models that can ﬂexibly adapt their
layerwise representations in response to datasets with different temporal dynamics.
Here, we present Variational Predictive Routing (VPR) – a neural probabilistic
inference system that organizes latent representations of video features in a temporal
hierarchy, based on their rates of change, thus modeling continuous data as a
hierarchical renewal process. By employing an event detection mechanism that
relies solely on the system’s latent representations (without the need of a separate
model), VPR is able to dynamically adjust its internal state following changes in
the observed features, promoting an optimal organisation of representations across
the levels of the model’s latent hierarchy. Using several video datasets, we show
that VPR is able to detect event boundaries, disentangle spatiotemporal features
across its hierarchy, adapt to the dynamics of the data, and produce accurate time-
agnostic rollouts of the future. Our approach integrates insights from neuroscience
and introduces a framework with high potential for applications in model-based
reinforcement learning, where ﬂexible and informative state-space rollouts are of
particular interest."
INTRODUCTION,0.006802721088435374,"1
INTRODUCTION"
INTRODUCTION,0.01020408163265306,"A key theoretical beneﬁt of hierarchically-structured latent models is the ability to learn spatial and
temporal features that are characterised by an increasing level of abstraction in deeper layers. In most
current implementations of such models, state transitions are realized using recurrent neural networks
that operate at ﬁxed rates determined by the timescale of the recorded data (Wichers et al., 2018;
Castrejon et al., 2019; Kumar et al., 2019; Wu et al., 2021). Though quite successful, this approach
neglects the role of the transition rates in the process of learning spatiotemporal representations across
the model’s latent hierarchy. Furthermore, it is computationally inefﬁcient as more abstract features
tend to remain static for longer periods than features represented in lower levels. This redundancy in
state transitions introduces noise during both inference and future predictions, which is particularly
detrimental to the learning of long-term dependencies (Bengio et al., 1994).
Having the ability to update the representation of these high-level features only when a change
occurs in the environment can save signiﬁcant computational resources and minimize the effect of
accumulated noise in state transitions. Such an ‘event-based’ hierarchical model can also be beneﬁcial
for predicting future sequences (e.g. planning in model-based reinforcement learning) as the same
future point in time can be reached with shorter and more accurate state rollouts (Pertsch et al., 2020;
Zakharov et al., 2021). These beneﬁts have recently prompted a number of proposed models that
either perform transitions with temporal jumps of arbitrary length (Koutnik et al., 2014; Buesing et al.,
2018; Gregor et al., 2018; Saxena et al., 2021), or aim to identify signiﬁcant events (or key-frames) in
sequential data and model the transitions between these events (Schmidhuber, 1991; Chung et al.,
2017; Neitz et al., 2018; Jayaraman et al., 2018; Shang et al., 2019; Kipf et al., 2019; Kim et al.,
2019; Pertsch et al., 2020; Zakharov et al., 2021)."
INTRODUCTION,0.013605442176870748,"The large variety of different event criteria deﬁned in these studies demonstrates the lack of a widely
established deﬁnition of this concept in this literature. For instance, important events are either"
INTRODUCTION,0.017006802721088437,∗Corresponding author: zafeirios.fountas@huawei.com
INTRODUCTION,0.02040816326530612,Published as a conference paper at ICLR 2022
INTRODUCTION,0.023809523809523808,"selected as the points in time that contain maximum information about a full video sequence (Pertsch
et al., 2020), about the agent’s actions (Shang et al., 2019), as the most predictable (Neitz et al.,
2018; Jayaraman et al., 2018) or as the most surprising (Zakharov et al., 2021) points in time. A
clearer picture can be drawn when viewing events from the perspective of cognitive psychology,
where events are deﬁned as segments of time “conceived by an observer to have a beginning and
an end” (Zacks & Tversky, 2001). Neuroimaging evidence suggests that the human brain segments
continuous experience hierarchically in order to infer the structure of events, with timescales that
increase from milliseconds to minutes across a nested hierarchy in the cortex (Baldassano et al., 2017)
while their boundaries tend to track changes in scene features and mental representations (Hard et al.,
2006; Kurby & Zacks, 2008)."
INTRODUCTION,0.027210884353741496,"The processing of changes in the brain’s hierarchical representations can be mathematically described
using the framework of predictive coding (PC). According to PC, cortical representations of the
world’s state are constantly compared against new sensory information in multiple local inference
loops that communicate via prediction error signals (Mumford, 1992; Rao & Ballard, 1999; Friston,
2010). In a contemporary view of this framework, called predictive routing, inference is realized
via bottom-up propagation of sensory signals (rather than errors), which can be blocked by top-
down predictions if considered uninformative (Bastos et al., 2020). This simple connection between
salient representation changes and PC has led studies to propose plausible mechanisms for subjective
experience of time (Roseboom et al., 2019) and the formation of episodic memories (Fountas et al.,
2021) based on the detection of such changes."
INTRODUCTION,0.030612244897959183,"Inspired by these insights, we propose a variational inference hierarchical model, termed variational
predictive routing (VPR), that relies on a change detection mechanism to impose a nested temporal
hierarchy on its latent representations. We implement this model in the domain of video using
recurrent neural networks and amortised inference and show that our model is able to detect changes
in dataset features even in early learning stages, regardless of whether they are expected or unexpected
changes, with signiﬁcant beneﬁts for the learnt latent representations.
In summary, we make the following contributions. (a) We propose a mechanism that can detect
the onset of both expected and unexpected events individually. (b) We show that event boundary
discovery can rely on layerwise disentangled representations without the need of a dedicated learning
mechanism. (c) We formulate a generative model (VPR) that integrates various insights from
neuroscience and uses the detection mechanism to learn hierarchical spatiotemporal representations.
(d) We propose a dataset based on 3D Shapes (Burgess & Kim, 2018) that allows the analysis of
hierarchical latent spaces with nested temporal structure. (e) In different experiments and datasets,
we demonstrate the ability of VPR to discover changes, be cost-effective, adjust representations to the
dataset’s temporal factors of variation and produce farsighted and diverse rollouts in the environment,
all of which are key advantages for agent learning and planning."
RELATED WORK,0.034013605442176874,"2
RELATED WORK"
RELATED WORK,0.03741496598639456,"Hierarchical generative models
Our proposed model relates to the extensive body of literature
on hierarchical generative models. Incorporation of the hierarchy into variational latent models has
been used for improving the expressiveness of the variational distributions (Ranganath et al., 2016),
modeling image (Rasmus et al., 2015; Bachman, 2016; Vahdat & Kautz, 2020) and speech data (Hsu
et al., 2019). Notably, NVAE (Vahdat & Kautz, 2020) implement a stable fully-convolutional and
hierarchical variational autoencoder (VAE) with the use of separate deterministic bottom-up and
top-down information channels. NVAE incorporates similar architectural components to VPR but
is designed to model stationary, as opposed to temporal, data. A more closely related hierarchical
generative model is CW-VAE (Saxena et al., 2021). CW-VAE is designed to model temporal data
using a hierarchy of latent variables that update over ﬁxed time intervals. Unlike CW-VAE, our model
employs an adaptive strategy in which update intervals rely on event detection."
RELATED WORK,0.04081632653061224,"Furthermore, hierarchical generative models are also subject to research in the area of PC. Deep neural
PC was originally proposed in PredNet by Lotter et al. (2016), who implemented local inference loops
that propagate bottom-up prediction errors using stacked layers of LSTMs. Although PredNet is able
to predict complex naturalistic videos and to respond to visual illusions similar to the human visual
system (Watanabe et al., 2018), it does not perform well in learning high-level latent representations
(Rane et al., 2020). This led to its alternative implementations such as CortexNet (Canziani &
Culurciello, 2017). More recently, a series of studies including Song et al. (2020); Millidge et al."
RELATED WORK,0.04421768707482993,Published as a conference paper at ICLR 2022
RELATED WORK,0.047619047619047616,"(2020) showed that deep PC can be implemented in a single computational graph and can, under some
conditions, replace back-propagation with local learning updates. Although local inference loops in
PC are suitable for identifying event boundaries, none of the above models involve the learning of
temporal abstractions. An exception is the hybrid episodic memory model in Fountas et al. (2021),
where temporal abstraction was introduced as a means to model human reports of subjective time
durations. Finally, to our knowledge, there is currently no computational model of predictive routing
(Bastos et al., 2020) – the contemporary view of PC that inspired our work."
RELATED WORK,0.05102040816326531,"Hierarchical temporal structure
A number of hierarchical approaches that do not rely on ﬁxed
intervals have also been put forward. Chung et al. (2017) proposed Hierarchical Multiscale RNNs
and showed that a modiﬁed discrete-space LSTM can discover multiscale structure in data in an
unsupervised manner. This method relies on a parameterized boundary detector and introduces
additional operations to RNNs for controlling this detection, as well as copying and updating the
current hidden state of the network. HM-RNNs have been combined with more advanced state
transition functions (Mujika et al., 2017) and have been extended to a stochastic version using
variational inference (Kim et al., 2019). A crucial difference between VPR and HM-RNNs is that, in
our model, event boundaries are determined via a mechanism that does not involve learning. Instead,
VPR detects changes in latent representations of each hierarchical layer independently, and enforces a
nested temporal structure by blocking bottom-up propagation between the detected boundaries. As a
result, event detection performance is high even in the very ﬁrst stages of training while the emerging
temporal structures change organically along with the latent space throughout training."
VARIATIONAL PREDICTIVE ROUTING,0.05442176870748299,"3
VARIATIONAL PREDICTIVE ROUTING"
VARIATIONAL PREDICTIVE ROUTING,0.05782312925170068,"We introduce variational predictive routing (VPR) with nested subjective timescales – a hierarchical
event-based generative model capable of learning disentangled spatiotemporal representations using
video datasets. The representational power of VPR lends itself to several key components: (1) distinct
pathways of information ﬂow between VPR blocks, (2) selective bottom-up communication that is
blocked if no change in the layerwise features is detected, and (3) the resultant subjective-timescale
transition models that learn to predict feature changes optimally."
VARIATIONAL PREDICTIVE ROUTING,0.061224489795918366,"bottom-up
top-down"
VARIATIONAL PREDICTIVE ROUTING,0.06462585034013606,temporal
VARIATIONAL PREDICTIVE ROUTING,0.06802721088435375,change?
VARIATIONAL PREDICTIVE ROUTING,0.07142857142857142,"Figure 1: VPR block architecture. Distinct
information channels are mediated using dif-
ferent deterministic variables: dn
τ temporal,
cn
τ top-down, xn
τ bottom-up. In turn, they are
used to parametrise random variable sn
τ ."
VARIATIONAL PREDICTIVE ROUTING,0.07482993197278912,"Block architecture
VPR consists of computational
blocks that are stacked together in a hierarchy. Fig. 1
shows the insides of a single block with key vari-
ables and channels of information ﬂow. Each block
in layer n consists of three deterministic variables
(xn
τ , cn
τ , dn
τ ) that represent the three channels of com-
munication between the blocks: bottom-up (encod-
ing), top-down (decoding), and temporal (transi-
tioning), respectively. These variables are used to
parametrise a random variable sn
τ that contains the
learned representations in a given hierarchical level."
VARIATIONAL PREDICTIVE ROUTING,0.0782312925170068,"Communication between the blocks is a crucial com-
ponent of the system. Top-down decoding from level
n + 1 to n is realised by passing the latest context
cn+1 and sample from sn+1 through a neural net-
work to retrieve cn = fdec(cn+1, sn+1). Tempo-
ral transitioning is implemented with the use of a
recurrent GRU model (Cho et al., 2014), such that
dτ+1 = ftran(sτ, dτ). Finally, bottom-up encoding
of new observations iteratively computes layerwise observation variables, xn+1 = fenc(xn). Fig. 2
shows a three-level VPR model unrolled over ﬁve timesteps, demonstrating how these communication
channels interact over time."
VARIATIONAL PREDICTIVE ROUTING,0.08163265306122448,"Subjective timescales
As will be described next, each level n updates its state only when an event
boundary has been detected. This results in updates that can occur arbitrarily far apart in time –
driven only by the changes inferred from the observations and dependent on the model’s internal
representations. For this reason, we use the term subjective timescales to refer to level-speciﬁc"
VARIATIONAL PREDICTIVE ROUTING,0.08503401360544217,Published as a conference paper at ICLR 2022
VARIATIONAL PREDICTIVE ROUTING,0.08843537414965986,"o1
o2
o3
o4
o5"
VARIATIONAL PREDICTIVE ROUTING,0.09183673469387756,"Level 1
Level 2
Level 3"
VARIATIONAL PREDICTIVE ROUTING,0.09523809523809523,"Stacked model
Unrolled over time"
VARIATIONAL PREDICTIVE ROUTING,0.09863945578231292,"Figure 2: Example of a three-level VPR model unrolled over ﬁve timesteps.
are block
variables as demonstrated in Fig. 1.
indicates the latest top-down context from a level above.
indicates that bottom-up encoding channel is open, while
indicates that it is blocked."
VARIATIONAL PREDICTIVE ROUTING,0.10204081632653061,"succession of time and denote it by τn ∈{1, ..., Tn}, where Tn is the total number of states produced
in level n. The subjective time variable τn should not be confused with variable t ∈{1, ..., T} that
denotes the objective timescale (i.e. normal succession of time). Lastly, we set t = τn for n = 1."
VARIATIONAL PREDICTIVE ROUTING,0.1054421768707483,"Formal description
We consider sequences of observations {o1, ..., oT }, modelled using a col-
lection of latent variables s1:N
1:T1:N . The generative model of VPR can be written as a factorised
distribution,"
VARIATIONAL PREDICTIVE ROUTING,0.10884353741496598,"p(o1:T , s1:N
1:T1:N ) =
h
T1
Y"
VARIATIONAL PREDICTIVE ROUTING,0.11224489795918367,"τ1=1
p(oτ1|s1:N
τ1 )
ih
N
Y n=1 Tn
Y"
VARIATIONAL PREDICTIVE ROUTING,0.11564625850340136,"τn=1
p(sn
τn|sn
<τn, s>n
τ>n)
i
,
(1)"
VARIATIONAL PREDICTIVE ROUTING,0.11904761904761904,"where s1:N
τn
denotes all latest hierarchical states at timestep τn, p(sn
τn|sn
<τn, s>n
τ>n) represents a prior
distribution of the state sn
τn conditioned on past states in level n and all above states in the hierarchy
> n, and p(sN
1 ) = N(0, 1) is a diagonal Gaussian prior. To simplify the notation, we omit the
subscript n from τn and Tn in the following sections and use τ and T instead. Since the true posterior
p(s1:N
1:T |o1:T ) is intractable, we resort to approximate it using a parametrised approximate posterior
distribution q(s1:N
1:T |o1:T ) = QT
τ=1
QN
n=1 qφ(sn
τ |xn
τ , sn
<τ, s>n
τ
), where xn
τ is the layerwise encoding
of oτ and φ are the parameters of the posterior model."
VARIATIONAL PREDICTIVE ROUTING,0.12244897959183673,"Notably, the proposed generative model is not Markovian in both temporal and top-down directions
(s<τ and s>n) by virtue of the deterministic variables d and c that mediate block communica-
tions during the generative process. All of the model components are summarised as follows,"
VARIATIONAL PREDICTIVE ROUTING,0.12585034013605442,"Posterior model,
qφ(sn
τ |xn
τ , sn
<τ, s>n
τ
)
(2)"
VARIATIONAL PREDICTIVE ROUTING,0.1292517006802721,"Prior model,
pθ(sn
τ |sn
<τ, s>n
τ
)
(3)
Transition model,
dn
τ+1 = f n
tran(sn
τ , dn
τ )
(4)"
VARIATIONAL PREDICTIVE ROUTING,0.1326530612244898,"Encoder,
xn+1 = f n
enc(xn)
(5)"
VARIATIONAL PREDICTIVE ROUTING,0.1360544217687075,"Decoder,
cn−1 = f n
dec(sn, cn)
(6)"
VARIATIONAL PREDICTIVE ROUTING,0.13945578231292516,"Reconstruction,
oτ = frec(c0
τ)
(7)"
VARIATIONAL PREDICTIVE ROUTING,0.14285714285714285,"Finally, to train VPR, we deﬁne a variational lower bound over ln p(o1:T ), LELBO,"
VARIATIONAL PREDICTIVE ROUTING,0.14625850340136054,"LELBO = T
X"
VARIATIONAL PREDICTIVE ROUTING,0.14965986394557823,"τ=0
Eq(s1:N
τ
)[ln p(oτ|s1:N
τ
)] − N
X n=1 Tn
X"
VARIATIONAL PREDICTIVE ROUTING,0.15306122448979592,"τ=1
Eq(s>n
τ
,sn
<τ )[DKL[˜qφ(sn
τ )||˜pθ(sn
τ )]].
(8)"
VARIATIONAL PREDICTIVE ROUTING,0.1564625850340136,"where ˜qφ(sn
τ ) = qφ(sn
τ |oτ, sn
<τ, s>n
τ
) and ˜pθ(sn
τ ) = pθ(sn
τ |sn
<τ, s>n
τ
)."
VARIATIONAL PREDICTIVE ROUTING,0.1598639455782313,"Event detection
At the heart of VPR is a mechanism that is used for detecting predictable and
unpredictable changes in the observable features over time, and thus determine the boundaries of
events. Occurring at every level of the model, the event detection is used for controlling the structure"
VARIATIONAL PREDICTIVE ROUTING,0.16326530612244897,Published as a conference paper at ICLR 2022
VARIATIONAL PREDICTIVE ROUTING,0.16666666666666666,"of the unrolled model over time by allowing (Fig. 2
) or disallowing (Fig. 2
) propagation of
bottom-up information to the higher levels of the hierarchy.
Speciﬁcally, for some level n, the model retrieves the latest posterior belief state, pst
=
p(sn
τ+1|xn
τ , sn
<τ, s>n
τ
) ≡qφ(sn
τ |xn
τ , sn
<τ, s>n
τ
), which represents model’s belief over a subset of
observable features (represented in level n) at the latest observed timestep, τ. Given a new ob-
servation, xn
τ+1, VPR then computes an updated posterior belief under the static assumption,
qst = qφ(sn
τ+1|xn
τ+1, s>n
τ
, sn
<τ). Notice that qst is conditioned on the same set of latent states,
which implies that pst = qst iff xn
τ = xn
τ+1. This means that the model’s posterior belief over the
inferred features will remain constant if the features have not changed between τ and τ + 1. The
model’s update in the posterior belief is then measured using KL-divergence, Dst = DKL(qst||pst).
This key quantity can be seen as a measure of how much the relevant layerwise features have changed
since the last encoded timestep τ."
VARIATIONAL PREDICTIVE ROUTING,0.17006802721088435,"Next, VPR has two ways of identifying an event boundary. Criterion E (CE): predictable events are
detected with the use of the model’s transition model that computes the next subjective timestep’s
belief state, pch = pθ(sn
τ+1|sn
τ , sn
<τ, s>n
τ
). This prediction can be seen as the model’s prior belief
over the features it expects to observe next (the change assumption). Upon receiving the new
information xn
τ+1, the model then calculates the posterior belief, qch = qφ(sn
τ+1|xn
τ+1, sn
τ , sn
<τ, s>n
τ
).
Similarly to the computations for the static assumption, the KL divergence between these two
distributions is calculated, Dch = DKL(qch||pch).A predictable event is considered detected if
DKL(qst||pst) > DKL(qch||pch). Satisfying this criterion indicates that the model’s prediction
produced a belief state more consistent with the new observation xn
τ+1, suggesting that it contains a
predictable change in the features that are represented in the corresponding level of the hierarchy.
Criterion U (CU): events that are not or cannot be captured well by the transition models (i.e.
unexpected changes) require an additional method for their detection. To this end, VPR keeps a
moving average statistics of the latest τw timesteps over the observed values of Dst, and identiﬁes an
event when inequality Dst,τ+1 > γ Pτ
k=τ−τw Dst,k/τw (where γ is a threshold weight) is satisﬁed.
As will be shown, this criterion proved to be crucial in the beginning of the training when the model’s
representations and layerwise transition models are not yet trained."
VARIATIONAL PREDICTIVE ROUTING,0.17346938775510204,"Bottom-up communication
Event detection serves two primary functions in our model. First,
detecting layerwise events in a sequence of observations is used for triggering an update on a block’s
state (Fig. 2
) by inferring its new posterior state. Matching block updates with detectable changes
in layerwise features (event boundaries) prompts VPR to represent spatiotemporal features in levels
that most closely mimic their rate of change over time. Similarly, learning to transition between
states only when they signify a change in the features of the data allows VPR to make time-agnostic
(or jumpy) transitions – from one event boundary to another. Second, the detection mechanism is
used for blocking bottom-up communication (Fig. 2
) and thus stopping the propagation of new
information to the deeper levels of the hierarchy. This encourages the model to better organise its
spatiotemporal representations by enforcing a temporal hierarchy onto its generative process."
VARIATIONAL PREDICTIVE ROUTING,0.17687074829931973,"Practically, when a bottom-up information channel is blocked at level n, the variables of the model in
levels ≥n will remain unchanged. By virtue of the hierarchy, this similarly means that the top-down
information provided to level n −1 at the next timestep will stay constant (Fig. 2
). As a result, the
blockage mechanism encourages the model to represent slower changing features in the higher levels
of the hierarchy.
Importantly, there is no constraint on when and how often a state update can occur over time. In
other words, VPR is time-agnostic and is designed to model the changing parts of sequences with
temporal hierarchy, irrespective of how slow or fast their rates are. This leads to the model’s ability
to dynamically adjust to the datasets with different temporal structures and to learn more optimal,
disentangled, and hierarchically-structured representations. Fig. 3 shows the ability of VPR to adjust
the rate at which blockage occurs to datasets with slower and faster temporal dynamics."
EXPERIMENTS,0.18027210884353742,"4
EXPERIMENTS"
EXPERIMENTS,0.1836734693877551,"Using a variety of datasets and different instances of VPR, we evaluate its performance in the
following areas: (a) unsupervised event boundary discovery, (b) generation of temporally-disentangled
hierarchical representations, (c) adaptation to the temporal dynamics of incoming data, and (d) future
event prediction. We employ the following datasets and their corresponding VPR model instances:"
EXPERIMENTS,0.1870748299319728,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.19047619047619047,"Feature 2
Feature 3"
EXPERIMENTS,0.19387755102040816,Tr. iterations (x10K)
EXPERIMENTS,0.19727891156462585,"4
5
3
2
1
0
Training iterations (x10K)"
EXPERIMENTS,0.20068027210884354,Aver. intervals in L2
EXPERIMENTS,0.20408163265306123,F1 score
EXPERIMENTS,0.20748299319727892,"ai
aii"
EXPERIMENTS,0.2108843537414966,"Tr. iterations (x10K)
Tr. iterations (x10K) 0.6 0.4 0 0.8 0.6 1 0.4"
EXPERIMENTS,0.21428571428571427,"0.2
4
5
3
2
1
0
4
5
3
2
1
0
8
6
4
2
0 0.2"
EXPERIMENTS,0.21768707482993196,"0.8
CE+CU
CE"
EXPERIMENTS,0.22108843537414966,"CU
VTA"
EXPERIMENTS,0.22448979591836735,"Fast
Slow"
EXPERIMENTS,0.22789115646258504,"VTA
CW-
VPR"
EXPERIMENTS,0.23129251700680273,"c
b
d
1"
EXPERIMENTS,0.23469387755102042,"Figure 3: Performance of VPR’s event detection system using Moving Ball and 3DSD. (ai) VPR
adapts its rate of state updates to the temporal dynamics of the Moving Ball dataset, (aii) while
maintaining the same accuracy of event detection. (b) F1 score of event detection using different sets
of criteria in Moving Ball, with a comparison against VTA (Kim et al., 2019). (c) F1 score of levels
2 and 3 in detecting changes in their corresponding features using 3DSD. (d) Comparison against
baseline methods using 3DSD. Shaded regions indicate one standard deviation over 5 seeds."
EXPERIMENTS,0.23809523809523808,"Synthetic dataset is implemented to validate our approach for event boundary discovery and analyse
the roles of the chosen event criteria. The dataset consists of 1D sequences generated using a semi-
Markov process with random value jumps every 10 steps. The VPR instance employed is a minimal
version, where N = 1 , the transition model pch(sτ+1|sτ) is memoryless and the posterior state
calculation is replaced by a direct map qst(s1
τ) = qch(s1
τ) = oτ + c, where c is Gaussian noise."
EXPERIMENTS,0.24149659863945577,"Moving Ball consists of a coloured ball that travels in straight lines and bounces off walls within
the boundaries of an image. Importantly, the ball’s colour changes either upon a wall bounce or at a
random time with probability p = 0.1. As there are two dynamic features – position and colour of a
ball – we employ VPR with N = 2 (two levels) and measure the accuracy of the model to detect and
represent changes in the ball’s colour in level 2 of the hierarchy."
EXPERIMENTS,0.24489795918367346,"3D Shapes Dynamic (3DSD) is a dynamic extension to the 3D Shapes dataset (Burgess & Kim,
2018) – a large image collection of shapes generated using 6 underlying factors of variation. By
manipulating a subset of these factors, we produce sequences of images where different factors vary
over time with different periodicity. Speciﬁcally, we impose a temporal hierarchy onto three factors –
colours of objects, walls, and ﬂoor (from slow to fast, respectively). We test the ability of VPR with
N = 3 to detect and represent the changes in these features in levels 2 and 3 of the hierarchy."
EXPERIMENTS,0.24829931972789115,"Miniworld Maze. To evaluate the behaviour of VPR in a more perceptually challenging setting, we
use a 3D environment Gym-Miniworld (Chevalier-Boisvert, 2018). For this, we designed a maze
environment that consists of a sequence of connected hallways – each with a different colour and of a
random length – and collect sequecnes of agent observations when traversing these hallways."
EXPERIMENTS,0.25170068027210885,"To validate event boundary discovery in VPR, we compare against the temporal abstraction model that
employs a parametrised boundary detection system, VTA (Kim et al., 2019). Further, to emphasise
the beneﬁt of ﬂexible timescales we demonstrate the event detection accuracy of a CW-VPR – a
version of VPR where latent transitions occur at ﬁxed rates, analogous to CW-VAE (Saxena et al.,
2021). Finally, we compare VPR’s future event prediction accuracy against a model for long-term
video prediction, CW-VAE (Saxena et al., 2021)."
UNSUPERVISED EVENT DISCOVERY,0.25510204081632654,"4.1
UNSUPERVISED EVENT DISCOVERY"
UNSUPERVISED EVENT DISCOVERY,0.2585034013605442,"VPR’s event detection mechanism proves to be effective across three different datasets. Speciﬁcally,
it quickly achieves high F1 score in the Synthetic (Fig. 4), Moving ball (Fig. 3b; F1 = 0.97 ± 0.03),
and 3DSD (Fig. 3c; F1 = 0.97 ± 0.02) datasets. Furthermore, we observe a signiﬁcantly better
performance compared to the baseline models VTA (see Fig. 3b; Moving ball, Fig. 3d; 3DSD, Fig.4d;
Synthetic) and CW-VPR (see Fig. 3d; 3DSD)."
UNSUPERVISED EVENT DISCOVERY,0.2619047619047619,"Detection criteria analysis
As mentioned, the detection mechanism employs two criteria for
identifying expected (CE) and unexpected (CU) events. Using the Synthetic and Moving Ball
datasets, we show that the optimal performance is only achieved when both of these criteria are
used in the decision-making. First, we observe that the role of CU is particularly crucial in the early
stages of training, in order to provide initial supervision signal for the untrained transition model.
Fig. 4a shows the F1 score of the CE criterion at detecting event boundaries with (blue curve) and"
UNSUPERVISED EVENT DISCOVERY,0.2653061224489796,Published as a conference paper at ICLR 2022 c
UNSUPERVISED EVENT DISCOVERY,0.2687074829931973,F1 score
UNSUPERVISED EVENT DISCOVERY,0.272108843537415,Unexp. events (%)
UNSUPERVISED EVENT DISCOVERY,0.2755102040816326,Training steps
UNSUPERVISED EVENT DISCOVERY,0.2789115646258503,F1 score
UNSUPERVISED EVENT DISCOVERY,0.282312925170068,"Noise level a
b
d CU
CE VTA"
UNSUPERVISED EVENT DISCOVERY,0.2857142857142857,"VPR
CE or CU"
UNSUPERVISED EVENT DISCOVERY,0.2891156462585034,detected by:
UNSUPERVISED EVENT DISCOVERY,0.2925170068027211,VPR with only CE
UNSUPERVISED EVENT DISCOVERY,0.29591836734693877,F1 score
UNSUPERVISED EVENT DISCOVERY,0.29931972789115646,"Training steps
Training steps
Figure 4: Event discovery performance in synthetic data. Posterior noise is set to 0.0 in (a-b) and 0.4
in (c). Shaded regions indicate one standard deviation over 5 seeds."
UNSUPERVISED EVENT DISCOVERY,0.30272108843537415,"without (pink curve) the CU criterion enabled at the same time. It is evident that the transition model
cannot improve performance with a disabled CU. In contrast, expected event recognition with both
CE and CU quickly converges to optimality after some training. Notably, the better CE becomes in
detecting events, the less number of events are classiﬁed as unexpected (Fig. 4b). Furthermore, in
the later stages of learning, the roles of the two criteria switch, as CE becomes the driving force for
improving the model’s performance. Since detection with CU relies on a non-parametric threshold,
its performance has a theoretical limit and reduces substantially for high levels of posterior noise
(Fig. 4c-d). In contrast, CE continues to improve beyond CU’s supervision signal, being signiﬁcantly
more robust to noise. In addition, training VPR on the Moving Ball dataset using the different sets of
detection criteria demonstrates similar behaviour. Fig. 3b shows that employing both CE and CU
results in the signiﬁcantly better and faster convergence of the detection F1 score."
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.30612244897959184,"4.2
HIERARCHICAL TEMPORAL DISENTANGLEMENT"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.30952380952380953,"VPR employs its hierarchical structure to disentangle the temporal features extracted from the dataset.
By matching the rates of event boundaries with the rates of level updates, the model learns to represent
particular temporal attributes of the dataset in the appropriate layers of the hierarchy. We analyse
the property of hierarchical disentanglement by (1) producing rollouts using separate levels of the
model, (2) generating random samples from different levels, and (3) analysing factor variability of
the samples as a measure of disentanglement."
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3129251700680272,"Each layer of VPR is equipped with an event detection mechanism that guides the model to distribute
representations of temporal features across the hierarchy and learn jumpy predictions between event
boundaries. Fig. 5 shows layerwise rollouts of VPR in the Moving Ball and 3DSD datasets. For
Moving Ball in Fig. 5a, VPR learns to represent the ball’s position and colour in the two separate
levels (L1 and L2, respectively). L1 rollout accurately predicts positional changes of the ball, while"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3163265306122449,Rollout steps GT
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3197278911564626,"Rollout steps
(a) Moving Ball"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3231292517006803,"GT
L1
L2
L1
L2
L3"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.32653061224489793,"(b) 3D Shapes Dynamic
Figure 5: Layerwise rollouts using VPR. GT denotes ground-truth sequence, L1 rollouts made using
level 1, and so on. To produce layerwise rollouts, the model predicts the next state sn
τ+1 in the relevant
level n and decodes under ﬁxed states in all other levels. The produced rollouts illustrate model’s
ability to learn disentangled representations and produce accurate and feature-speciﬁc jumpy rollouts."
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3299319727891156,Published as a conference paper at ICLR 2022
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3333333333333333,"Samples from Level 1
Samples from Level 2
Samples from Level 3"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.336734693877551,"3D Shapes D.
Miniw. Maze"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3401360544217687,"Figure 6: Random samples taken from the different levels of VPR. The model generates diverse
images with respect to the spatiotemporal features represented in the sampled level, while keeping all
other features ﬁxed."
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3435374149659864,"keeping the colour constant. At the same time, L2 rollout produces jumpy predictions of the ball
colour under constant position. Similar behaviour can be observed in 3DSD, using three layers."
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3469387755102041,"VPR’s representational power can be further dissected by taking random samples from the different
levels of the hierarchy. Fig. 6 shows 3DSD and Miniworld Maze reconstructions of VPR conditioned
on sampling only one of the levels and ﬁxing all others. In line with the produced rollouts, 3D Shapes
samples indicate that L1 contains representations of the ﬂoor colour and object shape, L2 of wall
colour and angle, and L3 of object colour. For the Miniworld Maze samples, we similarly observe
that L1 represents the short-term feature of the agent’s position in a room, L2 encodes the colour of
the walls, while L3 provides the wider context of the agent’s location in a maze."
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.35034013605442177,"To quantify temporal feature disentanglement, we measure the average entropy in the distribution of
each of the features associated with the reconstructed samples, Hv = −1"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.35374149659863946,"M
PM
m
PI
i p(vi) log p(vi),
where I = 32 is the number of samples per trial, M = 100 is the number of sampling trials, and v is
a factor of variation extracted from each reconstructed image using a pre-trained extractor model.
Factor v will be associated with higher average entropy if the layerwise random samples produce
more uniform distributions of this factor across the reconstructed images. Fig. 7 shows that, for each
level, the average entropy is high only for a temporal feature that ranks the same in the order of the
dataset’s temporal hierarchy (factor 1: fastest, factor 3: slowest). This implies that VPR distributes
representations of features in a way that preserves the underlying temporal hierarchy of the dataset.
We contrast this result with instances of VPR that perform block updates over ﬁxed intervals (anal-
ogous to CW-VAE (Saxena et al., 2021)) in Fig. 7. It can be seen that different intervals result
in the different representational properties of the model, suggesting the difﬁculty of selecting the
appropriate interval values. Furthermore, ﬁxed intervals unnecessarily bound the unrolled structure of
the model, meaning that features that occur over arbitrary time intervals are likely to not be abstracted
to the higher levels. For instance, as shown in the Appendix Fig. 12, a ﬁxed-interval VPR model
cannot represent the ball colour in level 2 entirely, in contrast to VPR with subjective timescales."
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.35714285714285715,"Samples from level 1
Samples from level 2
Samples from level 3"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.36054421768707484,Average entropy (nats)
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.36394557823129253,Factor index (fast to slow)
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3673469387755102,"Update intervals:
subjective
= 2
= 4
= 6"
HIERARCHICAL TEMPORAL DISENTANGLEMENT,0.3707482993197279,"Figure 7: Feature disentanglement in 3DSD. VPR with subjective timescales ﬁnds the most appropri-
ate distribution of representations, in contrast to ﬁxed-interval models."
ADAPTATION TO TEMPORAL DYNAMICS,0.3741496598639456,"4.3
ADAPTATION TO TEMPORAL DYNAMICS"
ADAPTATION TO TEMPORAL DYNAMICS,0.37755102040816324,"Since VPR is an event-based system, it relies solely on the underlying change dynamics of a dataset –
layerwise computations are only initiated when an event boundary has been detected – and can thus"
ADAPTATION TO TEMPORAL DYNAMICS,0.38095238095238093,Published as a conference paper at ICLR 2022
ADAPTATION TO TEMPORAL DYNAMICS,0.3843537414965986,"adapt to datasets with different temporal dynamics. To test this property, we instantiate two versions
of the Moving Ball dataset – with fast and slow ball movements. Fig. 3a compares the average time
intervals between block updates (in level 2) of VPR trained using these two datasets. It can be seen
that VPR performs about 30% fewer block updates under slow ball movement compared to VPR
under fast ball movement, while maintaining high F1 score of event detection in both cases. We also
validate that both models produce the same hierarchical organisation of features reported in Section
4.2. This event-driven property implies signiﬁcant computational savings, promoting a more optimal
usage of computational resources with respect to the temporal dynamics of different datasets – which
will be even more signiﬁcant in VPR instances with a larger number of hierarchical levels."
FUTURE EVENT PREDICTION,0.3877551020408163,"4.4
FUTURE EVENT PREDICTION"
FUTURE EVENT PREDICTION,0.391156462585034,"The ability of VPR to learn jumpy and feature-speciﬁc transitions in its hierarchical levels has
beneﬁcial implications on the accuracy with which long-term event sequences can be predicted. To
test this, we perform jumpy layerwise rollouts of length 200 for the Moving Ball and 3DSD datasets
and evaluate them against the ground-truth event sequences – colour changes in the Moving Ball
(VPR level 2), and wall and object colour changes in the 3DSD (VPR levels 2 and 3). Here we
compare VPR against a powerful video prediction model, CW-VAE (Saxena et al., 2021), which
produces rollouts over the physical timescale and thus requires longer rollouts to reach an equivalent
of VPR’s jumpy predictions. For instance, predicting 200 events of object colour changes in the
3DSD corresponds to a physical rollout of 1600 steps. For the 3DSD, we ﬁnd that VPR make more
accurate event predictions of both wall (acc. 0.99 ± 0.017) and object (acc. 0.90 ± 0.04) colour
changes than CW-VAE (acc. 0.92 ± 0.05 and 0.71 ± 0.02, respectively). For the Moving Ball, we
ﬁnd that VPR signiﬁcantly outperforms CW-VAE at predicting changes in the ball colour, reaching a
perfect accuracy of 1.0 ± 0.0 against only 0.47 ± 0.09 by CW-VAE."
DISCUSSION,0.3945578231292517,"5
DISCUSSION"
DISCUSSION,0.3979591836734694,"Event duration estimation
The presented model in its current form is not able to generate far-
sighted rollouts over the physical timescale of the data. This process requires an additional method to
estimate the number of transitions in each layer n, before layer n + 1 performs a transition. This can
be implemented with a neural network that estimates the probability p(change|sn, sn+1), trained in a
self-supervised manner, using the criteria CE and CU. Analogous estimation has been employed in
previous studies as a parametrised event boundary detector (Chung et al., 2017; Mujika et al., 2017;
Kim et al., 2019). However, unlike these proposals, this estimation is not necessary for any of the
results presented here, nor for potential applications such as agent planning or solving downstream
tasks. To highlight this important difference, we chose to not explore event duration estimation here."
DISCUSSION,0.4013605442176871,"Expected and unexpected events
The criterion for event boundary detection used to identify
expected changes (CE) can be viewed as a model selection method based on the shortest belief update,
given the prior assumption that a change has either occurred or not. If qch ≡qst, then the inequality
CE is equivalent to maximizing the likelihood of the latent state given new observations. Indeed, we
can see in Appendix Fig. 16 that the absolute difference between the entropies Hch(sn) and Hst(sn)
is negligible compared to the difference between the corresponding relative entropies, indicating that
the CE criterion is predominantly governed by the latter difference. Furthermore, although detecting
the onset of anticipated events is crucial for constructing hierarchical temporal latent spaces, these
events do not denote salience, unless CU is also satisﬁed. Visual salience (in the form of attention
in humans) has been often directly linked to the value of DKL(q||p), i.e. Bayesian surprise, with
empirical evidence provided by Itti & Baldi (2006). Thresholding this quantity has been proposed as
a model of attention to perceptual changes, which determines which changes can be deemed salient
(Roseboom et al., 2019; Fountas et al., 2021), while the accumulation of such changes is shown to
correlate well with cortical activity when humans perform duration judgments (Sherman et al., 2020).
Finally, the distinction between expected and salient events, captured by CE and CU here, has been
extensively discussed by Yu & Dayan (2005), who proposed a connection with two neuromodulators
and an inextricably intertwined role in attention. Modeling hierarchical attention in VPR using CE
and CU constitutes an interesting avenue for future work and yet another potential example of how
cognitive neuroscience and machine learning can facilitate each other’s progress."
DISCUSSION,0.40476190476190477,Published as a conference paper at ICLR 2022
DISCUSSION,0.40816326530612246,ACKNOWLEDGMENTS
DISCUSSION,0.41156462585034015,"The authors would like to thank Yansong Chua for his valuable contributions and comments on the
early versions of the model presented in the current manuscript."
REFERENCES,0.41496598639455784,REFERENCES
REFERENCES,0.41836734693877553,"Philip Bachman. An architecture for deep, hierarchical generative models. In NIPS, 2016."
REFERENCES,0.4217687074829932,"Christopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan W Pillow, Uri Hasson, and Kenneth A
Norman. Discovering event structure in continuous narrative perception and memory. Neuron, 95
(3):709–721, 2017."
REFERENCES,0.42517006802721086,"Andr´e M. Bastos, M. Lundqvist, Ayan S. Waite, N. Kopell, and E. Miller. Layer and rhythm speciﬁcity
for predictive routing. bioRxiv, 2020."
REFERENCES,0.42857142857142855,"Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difﬁcult. IEEE transactions on neural networks, 5(2):157–166, 1994."
REFERENCES,0.43197278911564624,"Lars Buesing, Theophane Weber, S´ebastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert,
Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, et al. Learning and querying fast
generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018."
REFERENCES,0.43537414965986393,"Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018."
REFERENCES,0.4387755102040816,"Alfredo Canziani and Eugenio Culurciello. Cortexnet: a generic network family for robust visual
temporal representations. arXiv preprint arXiv:1706.02735, 2017."
REFERENCES,0.4421768707482993,"Lluis Castrejon, Nicolas Ballas, and Aaron Courville. Improved conditional vrnns for video prediction.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7608–7617,
2019."
REFERENCES,0.445578231292517,"Maxime Chevalier-Boisvert. gym-miniworld environment for openai gym. https://github.
com/maximecb/gym-miniworld, 2018."
REFERENCES,0.4489795918367347,"Kyunghyun Cho, Bart van Merrienboer, C¸ aglar G¨ulc¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder–decoder for
statistical machine translation. In EMNLP, 2014."
REFERENCES,0.4523809523809524,"Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.
In International Conference on Learning Representations, ICLR, 2017."
REFERENCES,0.4557823129251701,"Zafeirios Fountas, Anastasia Sylaidi, Kyriacos Nikiforou, Anil K. Seth, Murray Shanahan, and War-
rick Roseboom. A predictive processing model of episodic memory and time perception. bioRxiv,
2021. doi: 10.1101/2020.02.17.953133. URL https://www.biorxiv.org/content/
early/2021/07/09/2020.02.17.953133."
REFERENCES,0.45918367346938777,"Karl Friston. The free-energy principle: a uniﬁed brain theory? Nature reviews neuroscience, 11(2):
127–138, 2010."
REFERENCES,0.46258503401360546,"Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, and Theophane Weber. Temporal
difference variational auto-encoder. arXiv preprint arXiv:1806.03107, 2018."
REFERENCES,0.46598639455782315,"David R Ha and J¨urgen Schmidhuber. World models. ArXiv, abs/1803.10122, 2018."
REFERENCES,0.46938775510204084,"Bridgette M Hard, Barbara Tversky, and David S Lang. Making sense of abstract events: Building
event schemas. Memory & cognition, 34(6):1221–1235, 2006."
REFERENCES,0.47278911564625853,"Wei-Ning Hsu, Y. Zhang, Ron J. Weiss, H. Zen, Yonghui Wu, Yuxuan Wang, Yuan Cao, Ye Jia,
Z. Chen, Jonathan Shen, P. Nguyen, and Ruoming Pang. Hierarchical generative modeling for
controllable speech synthesis. ArXiv, abs/1810.07217, 2019."
REFERENCES,0.47619047619047616,Published as a conference paper at ICLR 2022
REFERENCES,0.47959183673469385,"Laurent Itti and Pierre F Baldi. Bayesian surprise attracts human attention. In Advances in neural
information processing systems, pp. 547–554. Citeseer, 2006."
REFERENCES,0.48299319727891155,"Dinesh Jayaraman, Frederik Ebert, Alexei A Efros, and Sergey Levine. Time-agnostic prediction:
Predicting predictable video frames. arXiv preprint arXiv:1808.07784, 2018."
REFERENCES,0.48639455782312924,"Taesup Kim, Sungjin Ahn, and Yoshua Bengio. Variational temporal abstraction. Advances in Neural
Information Processing Systems, 32:11570–11579, 2019."
REFERENCES,0.4897959183673469,"Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
CoRR,
abs/1412.6980, 2015."
REFERENCES,0.4931972789115646,"Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro Sanchez-Gonzalez, Edward Grefen-
stette, Pushmeet Kohli, and Peter Battaglia. Compile: Compositional imitation learning and
execution. In International Conference on Machine Learning, pp. 3418–3428. PMLR, 2019."
REFERENCES,0.4965986394557823,"Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. In
International Conference on Machine Learning, pp. 1863–1871. PMLR, 2014."
REFERENCES,0.5,"Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh,
and Durk Kingma. Videoﬂow: A conditional ﬂow-based model for stochastic video generation.
arXiv preprint arXiv:1903.01434, 2019."
REFERENCES,0.5034013605442177,"Christopher A Kurby and Jeffrey M Zacks. Segmentation in the perception and memory of events.
Trends in cognitive sciences, 12(2):72–79, 2008."
REFERENCES,0.5068027210884354,"William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video
prediction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016."
REFERENCES,0.5102040816326531,Andrew L. Maas. Rectiﬁer nonlinearities improve neural network acoustic models. 2013.
REFERENCES,0.5136054421768708,"Beren Millidge, Alexander Tschantz, and Christopher L Buckley. Predictive coding approximates
backprop along arbitrary computation graphs. arXiv preprint arXiv:2006.04182, 2020."
REFERENCES,0.5170068027210885,"Asier Mujika, Florian Meier, and Angelika Steger. Fast-slow recurrent neural networks. arXiv
preprint arXiv:1705.08639, 2017."
REFERENCES,0.5204081632653061,"David Mumford. On the computational architecture of the neocortex. Biological cybernetics, 66(3):
241–251, 1992."
REFERENCES,0.5238095238095238,"Alexander Neitz, Giambattista Parascandolo, Stefan Bauer, and Bernhard Sch¨olkopf. Adaptive skip
intervals: Temporal abstraction for recurrent dynamical models. arXiv preprint arXiv:1808.04768,
2018."
REFERENCES,0.5272108843537415,"Karl Pertsch, Oleh Rybkin, Jingyun Yang, Shenghao Zhou, Konstantinos Derpanis, Kostas Daniilidis,
Joseph Lim, and Andrew Jaegle. Keyframing the future: Keyframe discovery for visual prediction
and planning. In Learning for Dynamics and Control, pp. 969–979. PMLR, 2020."
REFERENCES,0.5306122448979592,"Roshan Prakash Rane, Edit Sz¨ugyi, Vageesh Saxena, Andr´e Ofner, and Sebastian Stober. Prednet
and predictive coding: A critical review. In Proceedings of the 2020 International Conference
on Multimedia Retrieval, ICMR ’20, pp. 233–241, New York, NY, USA, 2020. Association
for Computing Machinery.
ISBN 9781450370875.
doi: 10.1145/3372278.3390694.
URL
https://doi.org/10.1145/3372278.3390694."
REFERENCES,0.5340136054421769,"R. Ranganath, Dustin Tran, and David M. Blei.
Hierarchical variational models.
ArXiv,
abs/1511.02386, 2016."
REFERENCES,0.5374149659863946,"Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation
of some extra-classical receptive-ﬁeld effects. Nature neuroscience, 2(1):79–87, 1999."
REFERENCES,0.5408163265306123,"Antti Rasmus, Mathias Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning
with ladder networks. In NIPS, 2015."
REFERENCES,0.54421768707483,Published as a conference paper at ICLR 2022
REFERENCES,0.5476190476190477,"Warrick Roseboom, Zafeirios Fountas, Kyriacos Nikiforou, David Bhowmik, Murray Shanahan, and
Anil K Seth. Activity in perceptual classiﬁcation networks as a basis for human subjective time
perception. Nature communications, 10(1):1–9, 2019."
REFERENCES,0.5510204081632653,"Vaibhav Saxena, Jimmy Ba, and Danijar Hafner. Clockwork variational autoencoders. In Advances
in Neural Information Processing Systems, volume 34, 2021."
REFERENCES,0.5544217687074829,J¨urgen Schmidhuber. Neural sequence chunkers. 1991.
REFERENCES,0.5578231292517006,"Wenling Shang, Alex Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. Learning world
graphs to accelerate hierarchical reinforcement learning. arXiv preprint arXiv:1907.00664, 2019."
REFERENCES,0.5612244897959183,"Maxine T. Sherman, Zafeirios Fountas, Anil K. Seth, and Warrick Roseboom. Accumulation
of salient perceptual events predicts human subjective time.
bioRxiv, 2020.
doi: 10.1101/
2020.01.09.900423. URL https://www.biorxiv.org/content/early/2020/06/
26/2020.01.09.900423."
REFERENCES,0.564625850340136,"Yuhang Song, Thomas Lukasiewicz, Zhenghua Xu, and Rafal Bogacz. Can the brain do backpropa-
gation?—exact implementation of backpropagation in predictive coding networks. Advances in
neural information processing systems, 33:22566, 2020."
REFERENCES,0.5680272108843537,"Arash Vahdat and J. Kautz. Nvae: A deep hierarchical variational autoencoder. ArXiv, abs/2007.03898,
2020."
REFERENCES,0.5714285714285714,"Eiji Watanabe, Akiyoshi Kitaoka, Kiwako Sakamoto, Masaki Yasugi, and Kenta Tanaka. Illusory
motion reproduced by deep neural networks trained for prediction. Frontiers in psychology, 9:345,
2018."
REFERENCES,0.5748299319727891,"Nevan Wichers, Ruben Villegas, Dumitru Erhan, Honglak Lee, et al. Hierarchical long-term video
prediction without supervision. In International Conference on Machine Learning, pp. 6038–6046.
PMLR, 2018."
REFERENCES,0.5782312925170068,"Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, and Chelsea Finn. Greedy hierarchical
variational autoencoders for large-scale video prediction. CVPR, 2021."
REFERENCES,0.5816326530612245,"Angela J Yu and Peter Dayan. Uncertainty, neuromodulation, and attention. Neuron, 46(4):681–692,
2005."
REFERENCES,0.5850340136054422,"Jeffrey M Zacks and Barbara Tversky. Event structure in perception and conception. Psychological
bulletin, 127(1):3, 2001."
REFERENCES,0.5884353741496599,"Alexey Zakharov, Matthew Crosby, and Zafeirios Fountas. Episodic memory for subjective-timescale
models. In ICML 2021 Workshop on Unsupervised Reinforcement Learning, 2021."
REFERENCES,0.5918367346938775,Published as a conference paper at ICLR 2022
REFERENCES,0.5952380952380952,"A
EVENT DETECTION MECHANISM"
REFERENCES,0.5986394557823129,"A.1
ARCHITECTURE AND PARAMETERS"
REFERENCES,0.6020408163265306,"Fig. 8 shows the insides of the event detection mechanism implemented at each level of the model’s
hierarchy. Given the current timestep τ + 1, the mechanism at level n is triggered if the bottom-up
communication has not been blocked by the level below, n −1. Upon receiving new bottom-up infor-
mation xn
τ+1, the model proceeds in evaluating four key variables. First, the latest posterior is assigned
to be the new prior under the static assumption, pst = p(sn
τ+1|xn
τ , dn
τ , cn
τ ) ←qφ(sn
τ |xn
τ , dn
τ , cn
τ ). For
clarity, we use the deterministic variables dn
τ and cn
τ to represent sn
<τ and s>n
τ , respectively. Sec-
ond, the new posterior under the static assumption is computed using the deterministic variables
of the latest block, qst = qφ(sn
τ+1|xn
τ+1, dn
τ , cn
τ ). As explained in the main body, we then compute
the KL-divergence between the two states, Dst = DKL(qst||pst). Third, we trigger the transition
model to predict the next temporal context, dn
τ+1, in order to produce the prior under the change
assumption of the model, pch = pθ(sn
τ+1|dn
τ+1, cn
τ ). Lastly, the posterior under the change assump-
tion can also be computed using the new bottom-up encoding, qch = qθ(sn
τ+1|xn
τ+1, dn
τ+1, cn
τ ). As
with the static assumption, we calculate the KL-divergence between the prior and posterior states,
Dch = DKL(qch||pch)."
REFERENCES,0.6054421768707483,"Practically, prior state pch is computed only once, after which it is stored for subsequent comparisons
until the event criteria are satisﬁed and the block is updated.
Additionally, criterion CU (Dst,τ+1 > γ Pτ
k=τ−τw Dst,k/τw) involves two hyperparameters: τw
is the length of a sliding window used for calculating the moving average, and γ is the threshold
factor that multiplies the value of the moving average. In running the experiments, we found that
the optimal values are γ = 1.1 and τw = 100. These values also proved to be robust for use across
different datasets, as we kept their values constant for all of the reported experiments."
REFERENCES,0.608843537414966,"Decision mechanism
Latest block state"
REFERENCES,0.6122448979591837,New observation
REFERENCES,0.6156462585034014,"Prior prediction 2
1
3"
REFERENCES,0.6190476190476191,"Figure 8: Event detection mechanism relies on the computation of key variables shown in the ﬁgure.
Given the latest updated state of a VPR block at level n and timestep τ denoted as pst (and the
corresponding deterministic states dn
τ and cn
τ ), the model receives a new observation xn
τ+1 at some
timestep τ + 1. 1⃝This new observation is used to compute the model’s posterior belief, qst, using
the latest deterministic variables of the block. 2⃝Concurrently, VPR makes a prediction using its
generative model, producing pch (and the corresponding dn
τ+1) representing the model’s prior belief
about the features at the next subjective timestep. 3⃝Lastly, VPR produces a posterior belief state,
qch, under the updated temporal context variable dn
τ+1."
REFERENCES,0.6224489795918368,"A.2
DECISION METRICS"
REFERENCES,0.6258503401360545,"We can visualise the values calculated as part of the decision-making process in the event detection
mechanism in Figure 9. As described in Section 4.1, the CU criterion acts as the initial supervision
signal, which subsequently results in the rapidly improving transition model and thus the CE-based
detection. Figure 9 shows two examples of the computed values over the length of an observation
sequence at the early (left) and later (right) stages of training. It can be observed that at only 500
training iterations the decision-making is primarily driven by the CU criterion. At 18500 iterations,
their roles tend to switch, as the CE criterion becomes signiﬁcantly more accurate at detecting events."
REFERENCES,0.6292517006802721,Published as a conference paper at ICLR 2022
REFERENCES,0.6326530612244898,"0
5
10
Sequence timesteps 10
3 10
1"
REFERENCES,0.6360544217687075,Tr. iteration 500
REFERENCES,0.6394557823129252,"0
5
10
Sequence timesteps 100"
REFERENCES,0.6428571428571429,"103
Tr. iteration 18500"
REFERENCES,0.6462585034013606,DKL[qst||pst]
REFERENCES,0.6496598639455783,DKL[qch||pch]
REFERENCES,0.6530612244897959,"CU threshold
Object changes colour"
REFERENCES,0.6564625850340136,"Figure 9: Key KL-divergence values computed using the level 2 detection mechanism at different
stages of the training process using the Moving Ball dataset. Left-hand side graph shows the early
stages of the training (training iteration 500), while right-hand side the later stages (training iteration
18500). In line with the described detection criteria in Section 3, if one of the blue lines falls below
the orange line, an event is considered to be detected. As such, it can be seen that the decision-
making is dominated by the CU criterion (light blue) in the early stages (left). On the other hand,
as representations mature and the transition model learns, the CE criterion (dark blue) begins to
dominate the detection process (right)."
REFERENCES,0.6598639455782312,"B
MODEL ARCHITECTURE AND TRAINING"
REFERENCES,0.6632653061224489,The model consists of several components implemented using neural networks:
REFERENCES,0.6666666666666666,"• Bottom-up. Encoder is a combination of (a) a convolutional neural network that embeds
high-dimensional image data into a lower-dimensional representation, (b) stacked fully-
connected networks with residual connections f n
enc, and (c) fully-connected networks in
each layer that compress layerwise observation embeddings, xn, prior to being passed into a
posterior model."
REFERENCES,0.6700680272108843,"• Top-down. Decoder is a combination of (a) a transpose convolutional neural network, frec,
for reconstructing images using top-down information c0 and (b) stacked fully-connected
networks with residual connections f n
dec."
REFERENCES,0.673469387755102,"• Temporal. Layerwise transition models are reccurrent GRU models (Cho et al., 2014)
with hidden states of size |dn
τ | = 200 and an additional fully-connected network with |sn
τ |
neurons."
REFERENCES,0.6768707482993197,"• Prior and posterior models. These models are implemented using four fully-connected
networks and parametrise a diagonal Gaussian, thus outputting a vector of size |sn
τ | · 2."
REFERENCES,0.6802721088435374,"The convolutional components of the encoder and decoder are analogous to those used in Ha &
Schmidhuber (2018). Fully-connected top-down and bottom-up components are all made out of the
same building block, which consists of four fully-connected layers with a residual connection at the
output (e.g. xn+1 = f n
enc(xn) + 0.1 · xn) and Leaky ReLU activations (Maas, 2013). The number of
neurons is kept the same throughout and is equal to the dimensionality of the block’s input (e.g. |xn
τ |).
Component (c) of the bottom-up model, as well as the posterior and prior models, also consist of four
fully-connected layers but with no residual connections.
We use the same VPR architecture for the Moving Ball and 3DSD datasets. Speciﬁcally, the latent
states are of size |sn
t | = 20, while the temporal, top-down, and bottom-up deterministic variables are
set to be |xn
τ | = |cn
τ | = |dn
τ | = 200. In Moving Ball and 3DSD, observations ot ∈R64×64×3, so we
set the model’s input layer to have the same shape. For the Bouncing Balls dataset (see section C.3),
we increase the capacity of the model, such that |xn
τ | = 1024 and |sn
t | = 60.
For training, we use Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0005 and a
cosine decay to 0.00005 over a period of 15,000 iterations. We employ linear annealing of the KL
coefﬁcient from 0 to 1 over the ﬁrst 3000 iterations. Although we also used KL balancing (Vahdat &
Kautz, 2020) in the models presented in this paper, we ﬁnd that it does not have a signiﬁcant effect
on their resultant properties. Further, we use binary cross entropy reconstruction loss and sequences
of length 15 for the Moving Ball and Bouncing Balls datasets, and mean squared error loss and
sequences of length 50 for 3DSD and Miniworld Maze datasets. Batch size of 32 is used for all
datasets."
REFERENCES,0.6836734693877551,Published as a conference paper at ICLR 2022
REFERENCES,0.6870748299319728,"B.1
PSEUDOCODE OF EVENT DETECTION"
REFERENCES,0.6904761904761905,"Algorithm 1: Event detection and inference in VPR
for video = 1 to ∞do"
REFERENCES,0.6938775510204082,for τ = 1 to max length do
REFERENCES,0.6972789115646258,"Retrieve a new observation, oτ
Initialise an empty decision mask, list M"
REFERENCES,0.7006802721088435,"// Event detection (bottom-up)
for n = 1 to N do"
REFERENCES,0.7040816326530612,"Compute layerwise encoding via f n
enc, xn
τ"
REFERENCES,0.7074829931972789,"// Level 1 decision is always True
if n = 1 then"
REFERENCES,0.7108843537414966,"M.append(True)
continue"
REFERENCES,0.7142857142857143,"// If propagation is blocked
if M[n −1] is False then"
REFERENCES,0.717687074829932,"M.append(False)
continue"
REFERENCES,0.7210884353741497,"Compute variables qch, qst, pch, pst of level n"
REFERENCES,0.7244897959183674,"// CE or CU criterion
if DKL(qst||pst) > DKL(qch||pch) or Dst,τ > γ Pτ−1
k=τ−1−τw Dst,k/τw then
M.append(True)
else"
REFERENCES,0.7278911564625851,M.append(False)
REFERENCES,0.7312925170068028,"// Inference (top-down)
for n = N to 0 do"
REFERENCES,0.7346938775510204,if M[n] is False then
REFERENCES,0.7380952380952381,continue
REFERENCES,0.7414965986394558,if n = 0 then
REFERENCES,0.7448979591836735,"Compute c0
τ using decoder, c0
τ ←f 0
dec(s1
τ, c1
τ)
Compute reconstruction image, oτ ←frec(c0
τ)
break"
REFERENCES,0.7482993197278912,"Compute cn
τ using decoder, cn
τ ←f n
dec(sn+1
τ
, cn+1
τ
)
Infer the new posterior state sn
τ using the posterior model, qφ(sn
τ |xn
τ , sn
<τ, s>n
τ
)"
REFERENCES,0.7517006802721088,"C
ADDITIONAL RESULTS"
REFERENCES,0.7551020408163265,"C.1
EXPLORING INFORMATION PATHWAYS"
REFERENCES,0.7585034013605442,"VPR is equipped with two distinct generative information pathways – temporal and top-down – that
give the model ﬂexibility in generating diverse sequences. We investigate this property further by
performing layerwise rollouts using level n = nr of the model and setting d<nr
τ
= 0. We call the
rolled out level a target level. Setting the temporal context variable to zero for all the levels below the
target level effectively means that the model would have no temporal information about previously
inferred states in those levels, s<nr
<τ ."
REFERENCES,0.7619047619047619,Published as a conference paper at ICLR 2022
REFERENCES,0.7653061224489796,"Figures 10-11 show layerwise rollouts under empty temporal priors (in all levels below) using the
Moving Ball and 3DSD datasets. Additionally, we differentiate between rollouts with and without
sampling – (a) rollouts with sampling are decoded by taking random samples from all states below the
target level, s<nr
τ
; (b) rollouts without sampling are decoded using the means of the states’ Gaussian
distributions s<nr
τ
, instead."
REFERENCES,0.7687074829931972,"Moving Ball
Temporal information carried in level 1 relates to the ball’s position and direction
of travel. After depriving the model of this information and performing a rollout using level 2, we
observe a distinct behaviour, in which VPR correctly predicts the changes in the ball’s colour while
randomly assigning it a position within an image (Fig. 10a). When no sampling is done, the position
remains constant in the middle of the box (Fig. 10b)."
DSD,0.7721088435374149,"3DSD
As was seen in the main body of the paper, VPR learns to represent different features of the
3D Shapes dataset in the different levels of its hierarchy. As a result, performing layerwise rollouts
under empty temporal priors in the levels below a rolled out level produces interesting properties.
Rolling out level 3 while sampling during decoding (Fig. 11) creates a sequence in which the changes
in the object colour (a feature that is represented in level 3) are correctly predicted over the steps
of the rollout; however, all other features – including object shape, angle, wall and ﬂoor colours –
are decoded at random. Given that these features are represented in levels 1-2 and that no temporal
information was passed to these levels, the model predicts high uncertainty for latent variables s<nr
τ
and thus samples the represented factors at random. Level 2 rollouts show the same behaviour – while
the wall colour and object shape are correctly predicted over the timesteps of the rollout, the ﬂoor
colour (represented in level 1) is sampled at random. These rollouts once again demonstrate VPR’s
disentanglement properties, as well as its versatility in generating sequences of quality reconstructions
even when information from one of the channels is blocked."
DSD,0.7755102040816326,Rollout steps
DSD,0.7789115646258503,"GT
L1
L2"
DSD,0.782312925170068,"(a) with sampling
Rollout steps"
DSD,0.7857142857142857,"GT
L1
L2"
DSD,0.7891156462585034,"(b) without sampling
Figure 10: Layerwise rollouts under empty temporal priors in the Moving Ball dataset. GT denotes
the ground-truth sequence, L1 level 1 rollout, and so on. (a) Decoding performed while sampling at
all levels below the target level; (b) decoding is done using the means of the Gaussians for s<nr
t
."
DSD,0.7925170068027211,"C.2
FIXED-INTERVAL MODELS COMPARISON"
DSD,0.7959183673469388,"One of the advantages of VPR with subjective timescales (VPR-ST) over its equivalent ﬁxed-interval
models is the ability to abstract temporal features to the deeper levels of its hierarchy even when
changes in these features occur over arbitrary number of timesteps. The Moving Ball dataset allows
us to analyse this property of VPR, since the ball’s colour does not change over ﬁxed time intervals.
To test this, we train four different ﬁxed-interval VPR models that update their level 2 state every
2, 4, 6, and 8 timesteps. Figure 12 shows layerwise rollouts of these models, which are analogous
to Figure 5 where we analysed subjective-timescale VPR. As can be seen, the model struggles to
abstract the colour of the object entirely to level 2, in contrast to VPR-ST. Because the colour of the
object does not change with ﬁxed periodicity, ﬁxed-interval models do not seem to be suitable for
learning temporally disentangled features in such datasets."
DSD,0.7993197278911565,Published as a conference paper at ICLR 2022
DSD,0.8027210884353742,Rollout steps
DSD,0.8061224489795918,"GT
L1
L2"
DSD,0.8095238095238095,(a) with sampling L3
DSD,0.8129251700680272,Rollout steps
DSD,0.8163265306122449,"GT
L1
L2"
DSD,0.8197278911564626,(b) without sampling L3
DSD,0.8231292517006803,"Figure 11: Layerwise rollouts under empty temporal priors in the 3DSD dataset. GT denotes the
ground-truth sequence, L1 level 1 rollout, and so on. (a) Decoding performed while sampling at all
levels below the target level; (b) decoding is done using the means of the Gaussians for s<nr
t
."
DSD,0.826530612244898,Published as a conference paper at ICLR 2022
DSD,0.8299319727891157,Rollout steps
DSD,0.8333333333333334,"GT
L1
L2"
DSD,0.8367346938775511,(a) Trained with level 2 intervals of: 2 timesteps.
DSD,0.8401360544217688,Rollout steps
DSD,0.8435374149659864,"GT
L1
L2"
DSD,0.8469387755102041,(b) Trained with level 2 intervals of: 4 timesteps.
DSD,0.8503401360544217,Rollout steps
DSD,0.8537414965986394,"GT
L1
L2"
DSD,0.8571428571428571,(c) Trained with level 2 intervals of: 6 timesteps.
DSD,0.8605442176870748,Rollout steps
DSD,0.8639455782312925,"GT
L1
L2"
DSD,0.8673469387755102,(d) Trained with level 2 intervals of: 8 timesteps.
DSD,0.8707482993197279,"Figure 12: Layerwise rollouts using ﬁxed-interval VPR models with manually assigned intervals of 2,
4, 6, and 8. It can be observed that the ball colour does not get entirely represented in level 2 (L2) of
the model, in contrast to the VPR with subjective timescales in Figure 5."
DSD,0.8741496598639455,Published as a conference paper at ICLR 2022
DSD,0.8775510204081632,"C.3
VPR ON THE BOUNCING BALLS DATASET"
DSD,0.8809523809523809,"To further illustrate the effectiveness of the VPR’s event detection system, we additionally train it
using a more dynamically complex dataset, analogous to the one used in Kim et al. (2019). The
results demonstrate that VPR outperforms the VTA model (Kim et al., 2019) in determining event
boundaries, as seen from Figure 13. The resultant representational properties of VPR with respect to
the two temporal factors of variation (position and colours) proved to exhibit the same hierarchical
disentanglement features discussed in Section 4.2 and demonstrated using samples taken from the
different latent levels of the model in Figure 14."
DSD,0.8843537414965986,"VPR
VTA"
DSD,0.8877551020408163,"F1 score a
b
c"
DSD,0.891156462585034,"Training iterations (x1000)
Training iterations (x1000)
Training iterations (x1000)"
DSD,0.8945578231292517,True-positive rate
DSD,0.8979591836734694,False-positive rate
DSD,0.9013605442176871,"VPR
VTA"
DSD,0.9047619047619048,"Figure 13: Comparison of VPR against VTA on the task of event detection using the Bouncing Balls
dataset. (a) F1 score. (b) True-positive rate. (c) False-positive rate. Shaded region indicates one
standard deviation using 5 different seeds."
DSD,0.9081632653061225,"(a) Samples from Level 1.
(b) Samples from Level 2."
DSD,0.9115646258503401,"Figure 14: Random samples taken from the different levels of VPR. Similar to the analysis in Section
4.2, it is evident that VPR learns to represent the position of the balls in Level 1 and their individual
colours in Level 2 of the hierarchy."
DSD,0.9149659863945578,Published as a conference paper at ICLR 2022
DSD,0.9183673469387755,"C.4
CU SENSITIVITY ANALYSIS"
DSD,0.9217687074829932,"As shown, the CU threshold is an important component of the event detection mechanism that
incorporates two main hyperparameters – window size of the moving average τw and threshold
weight γ. Using the Moving Ball dataset, we evaluate the sensitivity of the VPR’s performance to the
different values of these two parameters in Figure 15. The graphs indicate an average achieved F1
score by VPR using the speciﬁed parameter value. VPR’s performance for each of the parameter
values is averaged across all VPR instances trained using all of the values of the other parameter.
In total, 100 different models were trained (5 sliding window sizes × 4 threshold weights × 5 runs
each).
We ﬁnd that changing the size of the moving average window does not have a signiﬁcant effect on
the model’s performance. This is not the case for the threshold weight parameter that resulted in the
deterioration of VPR’s performance when γ was increased to 1.2. This is likely due to the fact that a
signiﬁcant increase in the value of γ results in a more sparse event detection that poorly matches the
rate of feature changes. Overall, however, we ﬁnd that the two hyperparameters are quite robust, as
the same set of values (γ = 1.1 and τw = 100) was used across all of the datasets presented in the
paper."
DSD,0.9251700680272109,"Sliding window size (  )
Threshold weight (  )"
DSD,0.9285714285714286,F1 score
DSD,0.9319727891156463,"Figure 15: Average achieved F1 score against the two hyperparameters of the CU threshold: window
size τw and threshold weight γ. Shaded region indicates one standard deviation."
DSD,0.935374149659864,Published as a conference paper at ICLR 2022
DSD,0.9387755102040817,"C.5
DECOMPOSING THE KL TERMS OF THE CE CRITERION"
DSD,0.9421768707482994,"The KL terms of the CE criterion can be conveniently decomposed into the entropy and the cross
entropy terms,"
DSD,0.9455782312925171,DKL(qch||pch) = −H(qch)
DSD,0.9489795918367347,"entropy
+ Eqch(log pch)"
DSD,0.9523809523809523,"cross entropy
,
(9)"
DSD,0.95578231292517,DKL(qst||pst) = −H(qst)
DSD,0.9591836734693877,"entropy
+ Eqst(log pst)"
DSD,0.9625850340136054,"cross entropy
,
(10)"
DSD,0.9659863945578231,"where CE inequality can be re-written as,"
DSD,0.9693877551020408,"−H(qst) + Eqst(log pst) > −H(qch) + Eqch(log pch).
(11)"
DSD,0.9727891156462585,"By recording the differences between the entropy and cross entropy components (see Fig. 16), we
ﬁnd that decision-making is largely dominated by the cross entropy terms. This implies that the
‘decision-making’ inequality above can be approximated as,
Eqst(log pst) > Eqch(log pch),
(12)
since H(qst) −H(qch) ≈0. This interprets the model’s CE event detection as a selection of a more
well-predicted state based on the static or change assumptions."
DSD,0.9761904761904762,Training iterations (x1000) Nats
DSD,0.9795918367346939,"Entropy
Cross entropy"
DSD,0.9829931972789115,"Figure 16: Comparison of the difference in entropies |H(qch) −H(qst)| and cross entropies
|Eqch(log pch) −Eqst(log pst)| of the two competing hypotheses denoting whether or not a state
change has occurred at timestep t and layer n. Data for this graph was acquired by training multiple
instances of VPR with the Moving Ball dataset and for level n = 2."
DSD,0.9863945578231292,"C.6
EVENT DETECTION IN THE MINIWORLD MAZE."
DSD,0.9897959183673469,"Published as a conference paper at ICLR 2022 VPR L2
L3 VPR L2
L3 VPR"
DSD,0.9931972789115646,"L2
L3
GT
GT
GT
VTA
VTA
VTA time"
DSD,0.9965986394557823,"Figure 17: Comparison between VPR and VTA event detection using random sequences from the
MiniWorld Maze dataset. GT denotes the ground-truth sequence. L2 and L3 denote events that have
been captured by the second and third level of the VPR model respectively. L2 is able to detect events
that correspond to colour changes, L3 detects more sparse events relating to the agent’s location in
the maze, while the VTA model detects turns between corridors. Best seen in the electronic version
of the manuscript."
