Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010460251046025104,"Offline black-box optimization aims to maximize a black-box function using an
1"
ABSTRACT,0.0020920502092050207,"offline dataset of designs and their measured properties. Two main approaches have
2"
ABSTRACT,0.0031380753138075313,"emerged: the forward approach, which learns a mapping from input to its value,
3"
ABSTRACT,0.0041841004184100415,"thereby acting as a proxy to guide optimization, and the inverse approach, which
4"
ABSTRACT,0.005230125523012552,"learns a mapping from value to input for conditional generation. (a) Although
5"
ABSTRACT,0.006276150627615063,"proxy-free (classifier-free) diffusion shows promise in robustly modeling the inverse
6"
ABSTRACT,0.007322175732217573,"mapping, it lacks explicit guidance from proxies, essential for generating high-
7"
ABSTRACT,0.008368200836820083,"performance samples beyond the training distribution. Therefore, we propose
8"
ABSTRACT,0.009414225941422594,"proxy-enhanced sampling which utilizes the explicit guidance from a trained proxy
9"
ABSTRACT,0.010460251046025104,"to bolster proxy-free diffusion with enhanced sampling control. (b) Yet, the trained
10"
ABSTRACT,0.011506276150627616,"proxy is susceptible to out-of-distribution issues. To address this, we devise the
11"
ABSTRACT,0.012552301255230125,"module diffusion-based proxy refinement, which seamlessly integrates insights from
12"
ABSTRACT,0.013598326359832637,"proxy-free diffusion back into the proxy for refinement. To sum up, we propose
13"
ABSTRACT,0.014644351464435146,"Robust Guided Diffusion for Offline Black-box Optimization (RGD), combining the
14"
ABSTRACT,0.015690376569037656,"advantages of proxy (explicit guidance) and proxy-free diffusion (robustness) for
15"
ABSTRACT,0.016736401673640166,"effective conditional generation. RGD achieves state-of-the-art results on various
16"
ABSTRACT,0.01778242677824268,"design-bench tasks, underscoring its efficacy. Our code is here.
17"
INTRODUCTION,0.01882845188284519,"1
Introduction
18"
INTRODUCTION,0.0198744769874477,"Creating new objects to optimize specific properties is a ubiquitous challenge that spans a multitude
19"
INTRODUCTION,0.02092050209205021,"of fields, including material science, robotic design, and genetic engineering. Traditional methods
20"
INTRODUCTION,0.021966527196652718,"generally require interaction with a black-box function to generate new designs, a process that could
21"
INTRODUCTION,0.02301255230125523,"be financially burdensome and potentially perilous [1, 2]. Addressing this, recent research endeavors
22"
INTRODUCTION,0.02405857740585774,"have pivoted toward a more relevant and practical context, termed offline black-box optimization
23"
INTRODUCTION,0.02510460251046025,"(BBO) [3, 4]. In this context, the goal is to maximize a black-box function exclusively utilizing an
24"
INTRODUCTION,0.02615062761506276,"offline dataset of designs and their measured properties.
25"
INTRODUCTION,0.027196652719665274,"There are two main approaches for this task: the forward approach and the reverse approach. The
26"
INTRODUCTION,0.028242677824267783,"forward approach entails training a deep neural network (DNN), parameterized as Jϕ(·), using the
27"
INTRODUCTION,0.029288702928870293,"offline dataset. Once trained, the DNN acts as a proxy and provides explicit gradient guidance to
28"
INTRODUCTION,0.030334728033472803,"enhance existing designs. However, this technique is susceptible to the out-of-distribution (OOD)
29"
INTRODUCTION,0.03138075313807531,"issue, leading to potential overestimation of unseen designs and resulting in adversarial solutions [5].
30"
INTRODUCTION,0.032426778242677826,"The reverse approach aims to learn a mapping from property value to input. Inputting a high value
31"
INTRODUCTION,0.03347280334728033,"into this mapping directly yields a high-performance design. For example, MINs [6] adopts GAN [7]
32"
INTRODUCTION,0.034518828451882845,"to model this inverse mapping, and demonstrate some success. Recent works [4] have applied
33"
INTRODUCTION,0.03556485355648536,"proxy-free diffusion1 [8], parameterized by θ, to model this mapping, which proves its efficacy over
34"
INTRODUCTION,0.036610878661087864,1Classifier-free diffusion is for classification and adapted to proxy-free diffusion to generalize to regression.
INTRODUCTION,0.03765690376569038,"other generative models. Proxy-free diffusion employs a score predictor ˜sθ(·, ·, ω). This represents a
35"
INTRODUCTION,0.038702928870292884,"linear combination of conditional and unconditional scores, modulated by a strength parameter ω to
36"
INTRODUCTION,0.0397489539748954,"balance condition and diversity in the sampling process. This guidance significantly diverges from
37"
INTRODUCTION,0.04079497907949791,"proxy (classifier) diffusion that interprets scores as classifier gradients and thus generates adversarial
38"
INTRODUCTION,0.04184100418410042,"solutions. Such a distinction grants proxy-free diffusion its inherent robustness in generating samples.
39"
INTRODUCTION,0.04288702928870293,"2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
2.0
xd1 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 xd2"
INTRODUCTION,0.043933054393305436,"Negative Rosenbrock Function
Initial points
Proxy-free diffusion
Proxy-enhanced sampling 3600 3000 2400 1800 1200 600 0"
INTRODUCTION,0.04497907949790795,Figure 1: Motivation of explicit proxy guidance.
INTRODUCTION,0.04602510460251046,"Nevertheless, proxy-free diffusion, initially de-
40"
INTRODUCTION,0.04707112970711297,"signed for in-distribution generation, such as
41"
INTRODUCTION,0.04811715481171548,"synthesizing specific image categories, faces
42"
INTRODUCTION,0.049163179916317995,"limitations in offline BBO. Particularly, it strug-
43"
INTRODUCTION,0.0502092050209205,"gles to generate high-performance samples that
44"
INTRODUCTION,0.051255230125523014,"exceed the training distribution due to the lack
45"
INTRODUCTION,0.05230125523012552,"of explicit guidance2. Consider, for example,
46"
INTRODUCTION,0.053347280334728034,"the optimization of a two-dimensional variable
47"
INTRODUCTION,0.05439330543933055,"(xd1, xd2) to maximize the negative Rosenbrock
48"
INTRODUCTION,0.05543933054393305,"function [9]: y(xd1, xd2) = −(1 −xd1)2 −
49"
INTRODUCTION,0.056485355648535567,"100(xd2 −x2
d1)2, as depicted in Figure 1. The
50"
INTRODUCTION,0.05753138075313807,"objective is to steer the initial points (indi-
51"
INTRODUCTION,0.058577405857740586,"cated in pink) towards the high-performance
52"
INTRODUCTION,0.0596234309623431,"region (highlighted in yellow). While proxy-
53"
INTRODUCTION,0.060669456066945605,"free diffusion can nudge the initial points closer to this high-performance region, the generated points
54"
INTRODUCTION,0.06171548117154812,"(depicted in blue) fail to reach the high-performance region due to its lack of explicit proxy guidance.
55"
INTRODUCTION,0.06276150627615062,"To address this challenge, we introduce a proxy-enhanced sampling module as illustrated in Fig-
56"
INTRODUCTION,0.06380753138075314,"ure 2(a). It incorporates the explicit guidance from the proxy Jϕ(x) into proxy-free diffusion to
57"
INTRODUCTION,0.06485355648535565,"enable enhanced control over the sampling process. This module hinges on the strategic optimization
58"
INTRODUCTION,0.06589958158995816,"of the strength parameter ω to achieve a better balance between condition and diversity, per reverse
59"
INTRODUCTION,0.06694560669456066,"diffusion step. This incorporation not only preserves the inherent robustness of proxy-free diffusion
60"
INTRODUCTION,0.06799163179916318,"but also leverages the explicit proxy guidance, thereby enhancing the overall conditional generation
61"
INTRODUCTION,0.06903765690376569,"efficacy. As illustrated in Figure 1, samples (depicted in red) generated via proxy-enhanced sampling
62"
INTRODUCTION,0.0700836820083682,"are more effectively guided towards, and often reach, the high-performance area (in yellow).
63"
INTRODUCTION,0.07112970711297072,Forward diffusion
INTRODUCTION,0.07217573221757322,"Reverse diffusion
Final design"
INTRODUCTION,0.07322175732217573,Diffusion-based
INTRODUCTION,0.07426778242677824,proxy refinement
INTRODUCTION,0.07531380753138076,Proxy-enhanced sampling (optimizing    ) (a)
INTRODUCTION,0.07635983263598327,"(b)
Diffusion distribution
Proxy"
INTRODUCTION,0.07740585774058577,Probability flow ODE ...
INTRODUCTION,0.07845188284518828,Figure 2: Overall of RGD.
INTRODUCTION,0.0794979079497908,"Yet, the trained proxy is susceptible to out-of-
64"
INTRODUCTION,0.08054393305439331,"distribution (OOD) issues. To address this, we
65"
INTRODUCTION,0.08158995815899582,"devise a module diffusion-based proxy refinement
66"
INTRODUCTION,0.08263598326359832,"as detailed in Figure 2(b). This module seamlessly
67"
INTRODUCTION,0.08368200836820083,"integrates insights from proxy-free diffusion into
68"
INTRODUCTION,0.08472803347280335,"the proxy Jϕ(x) for refinement. Specifically, we
69"
INTRODUCTION,0.08577405857740586,"generate a diffusion distribution pθ(y|ˆx) on adver-
70"
INTRODUCTION,0.08682008368200837,"sarial samples ˆx, using the associated probability
71"
INTRODUCTION,0.08786610878661087,"flow ODE 3. This distribution is derived indepen-
72"
INTRODUCTION,0.08891213389121339,"dently of a proxy, thereby exhibiting greater ro-
73"
INTRODUCTION,0.0899581589958159,"bustness than the proxy distribution on adversarial
74"
INTRODUCTION,0.09100418410041841,"samples. Subsequently, we calculate the Kullback-
75"
INTRODUCTION,0.09205020920502092,"Leibler divergence between the two distributions
76"
INTRODUCTION,0.09309623430962342,"on adversarial samples, and use this divergence
77"
INTRODUCTION,0.09414225941422594,"minimization as a regularization strategy to fortify
78"
INTRODUCTION,0.09518828451882845,"the proxy’s robustness and reliability.
79"
INTRODUCTION,0.09623430962343096,"To sum up, we propose Robust Guided Diffusion for Offline Black-box Optimization (RGD), a novel
80"
INTRODUCTION,0.09728033472803348,"framework that combines the advantages of proxy (explicit guidance) and proxy-free diffusion (ro-
81"
INTRODUCTION,0.09832635983263599,"bustness) for effective conditional generation. Our contributions are three-fold:
82"
INTRODUCTION,0.09937238493723849,"• We propose a proxy-enhanced sampling module which incorporates proxy guidance into proxy-free
83"
INTRODUCTION,0.100418410041841,"diffusion to enable enhanced sampling control.
84"
INTRODUCTION,0.10146443514644352,"• We further develop diffusion-based proxy refinement which integrates insights from proxy-free
85"
INTRODUCTION,0.10251046025104603,"diffusion back into the proxy for refinement.
86"
INTRODUCTION,0.10355648535564854,"• RGD delivers state-of-the-art performance on various design-bench tasks, emphasizing its efficacy.
87"
INTRODUCTION,0.10460251046025104,"2Proxy-free diffusion cannot be interpreted as a proxy and thus does not provide explicit guidance [8].
3Ordinary Differential Equation"
PRELIMINARIES,0.10564853556485355,"2
Preliminaries
88"
OFFLINE BLACK-BOX OPTIMIZATION,0.10669456066945607,"2.1
Offline Black-box Optimization
89"
OFFLINE BLACK-BOX OPTIMIZATION,0.10774058577405858,"Offline black-box optimization (BBO) aims to maximize a black-box function with an offline dataset.
90"
OFFLINE BLACK-BOX OPTIMIZATION,0.1087866108786611,"Imagine a design space as X = Rd, where d is the design dimension. The offline BBO [3] is:
91"
OFFLINE BLACK-BOX OPTIMIZATION,0.1098326359832636,"x∗= arg max
x∈X J(x).
(1)"
OFFLINE BLACK-BOX OPTIMIZATION,0.1108786610878661,"In this equation, J(·) is the unknown objective function, and x ∈X is a possible design. In this
92"
OFFLINE BLACK-BOX OPTIMIZATION,0.11192468619246862,"context, there is an offline dataset, D, that consists of pairs of designs and their measured properties.
93"
OFFLINE BLACK-BOX OPTIMIZATION,0.11297071129707113,"Specifically, each x denotes a particular design, like the size of a robot, while y indicates its related
94"
OFFLINE BLACK-BOX OPTIMIZATION,0.11401673640167365,"metric, such as its speed.
95"
OFFLINE BLACK-BOX OPTIMIZATION,0.11506276150627615,"A common approach gradient ascent fits a proxy distribution pϕ(y|x) = N(Jϕ(x), σϕ(x)) to the
96"
OFFLINE BLACK-BOX OPTIMIZATION,0.11610878661087866,"offline dataset where ϕ denote the proxy parameters:
97"
OFFLINE BLACK-BOX OPTIMIZATION,0.11715481171548117,"arg min
ϕ E(x,y)∈D[−log pϕ(y|x)]."
OFFLINE BLACK-BOX OPTIMIZATION,0.11820083682008369,"= arg min
ϕ E(x,y)∈D log(
√"
OFFLINE BLACK-BOX OPTIMIZATION,0.1192468619246862,2πσϕ(x)) + (y −Jϕ(x))2
OFFLINE BLACK-BOX OPTIMIZATION,0.1202928870292887,"2σ2
ϕ(x)
.
(2)"
OFFLINE BLACK-BOX OPTIMIZATION,0.12133891213389121,"For the sake of consistency with terminology used in the forthcoming subsection on guided diffusion,
98"
OFFLINE BLACK-BOX OPTIMIZATION,0.12238493723849372,"we will refer to pϕ(·|·) as the proxy distribution and Jϕ(·) as the proxy. Subsequently, this approach
99"
OFFLINE BLACK-BOX OPTIMIZATION,0.12343096234309624,"performs gradient ascent with Jϕ(x), leading to high-performance designs x∗:
100"
OFFLINE BLACK-BOX OPTIMIZATION,0.12447698744769875,"xτ+1 = xτ + η∇xJϕ(x)|x=xτ ,
for τ ∈[0, M −1],
(3)"
OFFLINE BLACK-BOX OPTIMIZATION,0.12552301255230125,"converging to xM after M steps. However, this method suffers from the out-of-distribution issue
101"
OFFLINE BLACK-BOX OPTIMIZATION,0.12656903765690378,"where the proxy predicts values that are notably higher than the actual values.
102"
DIFFUSION MODELS,0.12761506276150628,"2.2
Diffusion Models
103"
DIFFUSION MODELS,0.12866108786610878,"Diffusion models, a type of latent variable models, progressively introduce Gaussian noise to data in
104"
DIFFUSION MODELS,0.1297071129707113,"the forward process, while the reverse process aims to iteratively remove this noise through a learned
105"
DIFFUSION MODELS,0.1307531380753138,"score estimator. In this work, we utilize continuous time diffusion models governed by a stochastic
106"
DIFFUSION MODELS,0.13179916317991633,"differential equation (SDE), as presented in [10]. The forward SDE is formulated as:
107"
DIFFUSION MODELS,0.13284518828451883,"dx = f(x, t)dt + g(t)dw.
(4)"
DIFFUSION MODELS,0.13389121338912133,"where f(·, t) : Rd →Rd represents the drift coefficient, g(·) : R →R denotes the diffusion
108"
DIFFUSION MODELS,0.13493723849372385,"coefficient and w is the standard Wiener process. This SDE transforms data distribution into noise
109"
DIFFUSION MODELS,0.13598326359832635,"distribution. The reverse SDE is:
110"
DIFFUSION MODELS,0.13702928870292888,"dx =

f(x, t) −g(t)2∇x log p(x)

dt + g(t)d ¯
w,
(5)"
DIFFUSION MODELS,0.13807531380753138,"with ∇x log p(x) representing the score of the marginal distribution at time t, and ¯
w symbolizing the
111"
DIFFUSION MODELS,0.13912133891213388,"reverse Wiener process. The score function ∇x log p(x) is estimated using a time-dependent neural
112"
DIFFUSION MODELS,0.1401673640167364,"network sθ(xt, t), enabling us to transform noise into samples. For simplicity, we will use sθ(xt),
113"
DIFFUSION MODELS,0.1412133891213389,"implicitly including the time dependency t.
114"
GUIDED DIFFUSION,0.14225941422594143,"2.3
Guided Diffusion
115"
GUIDED DIFFUSION,0.14330543933054393,"Guided diffusion seeks to produce samples with specific desirable attributes, falling into two cate-
116"
GUIDED DIFFUSION,0.14435146443514643,"gories: proxy diffusion [11] and proxy-free diffusion [8]. While these were initially termed classifier
117"
GUIDED DIFFUSION,0.14539748953974896,"diffusion and classifier-free diffusion in classification tasks, we have renamed them to proxy diffu-
118"
GUIDED DIFFUSION,0.14644351464435146,"sion and proxy-free diffusion, respectively, to generalize to our regression context. Proxy diffusion
119"
GUIDED DIFFUSION,0.14748953974895398,"combines the model’s score estimate with the gradient from the proxy distribution, providing explicit
120"
GUIDED DIFFUSION,0.14853556485355648,"guidance. However, it can be interpreted as a gradient-based adversarial attack.
121"
GUIDED DIFFUSION,0.14958158995815898,"Proxy-free guidance, not dependent on proxy gradients, enjoys an inherent robustness of the sampling
122"
GUIDED DIFFUSION,0.1506276150627615,"process. Particularly, it models the score as a linear combination of an unconditional and a conditional
123"
GUIDED DIFFUSION,0.151673640167364,"score. A unified neural network sθ(xt, y) parameterizes both score types. The score sθ(xt, y)
124"
GUIDED DIFFUSION,0.15271966527196654,"approximates the gradient of the log probability ∇xt log p(xt|y), i.e., the conditional score, while
125"
GUIDED DIFFUSION,0.15376569037656904,"sθ(xt) estimates the gradient of the log probability ∇xt log p(xt), i.e., the unconditional score. The
126"
GUIDED DIFFUSION,0.15481171548117154,"score function follows:
127"
GUIDED DIFFUSION,0.15585774058577406,"˜sθ(xt, y, ω) = (1 + ω)sθ(xt, y) −ωsθ(xt).
(6)"
GUIDED DIFFUSION,0.15690376569037656,"Within this context, the strength parameter ω specifies the generation’s adherence to the condition
128"
GUIDED DIFFUSION,0.1579497907949791,"y, which is set to the maximum value ymax in the offline dataset following [4]. Optimization of ω
129"
GUIDED DIFFUSION,0.1589958158995816,"balances the condition and diversity. Lower ω values increase sample diversity at the expense of
130"
GUIDED DIFFUSION,0.1600418410041841,"conformity to y, and higher values do the opposite.
131"
METHOD,0.16108786610878661,"3
Method
132"
METHOD,0.16213389121338911,"In this section, we present our method RGD, melding the strengths of proxy and proxy-free diffu-
133"
METHOD,0.16317991631799164,"sion for effective conditional generation. Firstly, we describe a newly developed module termed
134"
METHOD,0.16422594142259414,"proxy-enhanced sampling. It integrates explicit proxy guidance into proxy-free diffusion to enable
135"
METHOD,0.16527196652719664,"enhanced sampling control, as detailed in Section 3.1. Subsequently, we explore diffusion-based
136"
METHOD,0.16631799163179917,"proxy refinement which incorporates insights gleaned from proxy-free diffusion back into the proxy,
137"
METHOD,0.16736401673640167,"further elaborated in Section 3.2. The overall algorithm is shown in Algorithm 1.
138"
PROXY-ENHANCED SAMPLING,0.1684100418410042,"3.1
Proxy-enhanced Sampling
139"
PROXY-ENHANCED SAMPLING,0.1694560669456067,"Algorithm 1 Robust Guided Diffusion for Offline BBO
Input: offline dataset D, # of diffusion steps T."
PROXY-ENHANCED SAMPLING,0.1705020920502092,"1: Train proxy distribution pϕ(y|x) on D by Eq. (2).
2: Train proxy-free diffusion model sθ(xt, y) on D.
3: /*Diffusion-based proxy refinement */
4: Identify adversarial samples via grad ascent.
5: Compute diffusion distribution pθ(y|ˆx) by Eq. (12).
6: Compute KL divergence loss as per Eq. (13).
7: Refine proxy distribution pϕ(y|x) through Eq. (15).
8: /*Proxy-enhanced sampling */
9: Begin with xT ∼N(0, I)
10: for t = T −1 to 0 do
11:
Derive the score ˜sθ(xt+1, y, ω) from Eq. (6).
12:
Update xt+1 to xt(ω) using ω as per Eq. (7).
13:
Optimize ω to ˆω following Eq. (8).
14:
Finalize the update of xt with ˆω via Eq. (9).
15: end for
16: Return x∗= x0"
PROXY-ENHANCED SAMPLING,0.17154811715481172,"As discussed in Section 2.3, proxy-
140"
PROXY-ENHANCED SAMPLING,0.17259414225941422,"free diffusion trains an unconditional
141"
PROXY-ENHANCED SAMPLING,0.17364016736401675,"model and conditional models. Although
142"
PROXY-ENHANCED SAMPLING,0.17468619246861924,"proxy-free diffusion can generate samples
143"
PROXY-ENHANCED SAMPLING,0.17573221757322174,"aligned with most conditions, it tradition-
144"
PROXY-ENHANCED SAMPLING,0.17677824267782427,"ally lacks control due to the absence of
145"
PROXY-ENHANCED SAMPLING,0.17782426778242677,"an explicit proxy. This is particularly sig-
146"
PROXY-ENHANCED SAMPLING,0.1788702928870293,"nificant in offline BBO where we aim to
147"
PROXY-ENHANCED SAMPLING,0.1799163179916318,"obtain samples beyond the training dis-
148"
PROXY-ENHANCED SAMPLING,0.1809623430962343,"tribution. Therefore, we require explicit
149"
PROXY-ENHANCED SAMPLING,0.18200836820083682,"proxy guidance to achieve enhanced sam-
150"
PROXY-ENHANCED SAMPLING,0.18305439330543932,"pling control. This module is outlined in
151"
PROXY-ENHANCED SAMPLING,0.18410041841004185,"Algorithm 1, Line 8- Line 16.
152"
PROXY-ENHANCED SAMPLING,0.18514644351464435,"Optimization of ω. Directly updating
153"
PROXY-ENHANCED SAMPLING,0.18619246861924685,"the design xt with proxy gradient suffers
154"
PROXY-ENHANCED SAMPLING,0.18723849372384938,"from the OOD issue and determining a
155"
PROXY-ENHANCED SAMPLING,0.18828451882845187,"proper condition y necessitates the man-
156"
PROXY-ENHANCED SAMPLING,0.1893305439330544,"ual adjustment of multiple hyperparame-
157"
PROXY-ENHANCED SAMPLING,0.1903765690376569,"ters [6]. Thus, we propose to introduce
158"
PROXY-ENHANCED SAMPLING,0.19142259414225943,"proxy guidance by only optimizing the strength parameter ω within ˜sθ(xt, y, ω) in Eq. (6). As
159"
PROXY-ENHANCED SAMPLING,0.19246861924686193,"discussed in Section 2.3, the parameter ω balances the condition and diversity, and an optimized ω
160"
PROXY-ENHANCED SAMPLING,0.19351464435146443,"could achieve a better balance in the sampling process, leading to more effective generation.
161"
PROXY-ENHANCED SAMPLING,0.19456066945606695,"Enhanced Sampling. With the score function, the update of a noisy sample xt+1 is computed as:
162"
PROXY-ENHANCED SAMPLING,0.19560669456066945,"xt(ω) = solver(xt+1, ˜sθ(xt+1, y, ω)),
(7)"
PROXY-ENHANCED SAMPLING,0.19665271966527198,"where the solver is the second-order Heun solver [12], chosen for its enhanced accuracy through a
163"
PROXY-ENHANCED SAMPLING,0.19769874476987448,"predictor-corrector method. A proxy is then trained to predict the property of noise xt at time step t,
164"
PROXY-ENHANCED SAMPLING,0.19874476987447698,"denoted as Jϕ(xt, t). By maximizing Jϕ(xt(ω), t) with respect to ω, we can incorporate the explicit
165"
PROXY-ENHANCED SAMPLING,0.1997907949790795,"proxy guidance into proxy-free diffusion to enable enhanced sampling control in the balance between
166"
PROXY-ENHANCED SAMPLING,0.200836820083682,"condition and diversity. This maximization process is:
167"
PROXY-ENHANCED SAMPLING,0.20188284518828453,"ˆω = ω + η ∂Jϕ(xt(ω), t)"
PROXY-ENHANCED SAMPLING,0.20292887029288703,"∂ω
.
(8)"
PROXY-ENHANCED SAMPLING,0.20397489539748953,"where η denotes the learning rate. We leverage the automatic differentiation capabilities of Py-
168"
PROXY-ENHANCED SAMPLING,0.20502092050209206,"Torch [13] to efficiently compute the above derivatives within the context of the solver’s operation.
169"
PROXY-ENHANCED SAMPLING,0.20606694560669456,"The optimized ˆω then updates the noisy sample xt+1 through:
170"
PROXY-ENHANCED SAMPLING,0.20711297071129708,"xt = solver(xt+1, ˜sθ(xt+1, y, ˆω)).
(9)"
PROXY-ENHANCED SAMPLING,0.20815899581589958,"This process iteratively denoises xt, utilizing it in successive steps to progressively approach x0,
171"
PROXY-ENHANCED SAMPLING,0.20920502092050208,"which represents the final high-scoring design x∗.
172"
PROXY-ENHANCED SAMPLING,0.2102510460251046,"Proxy Training. Notably, Jϕ(xt, t) can be directly derived from the proxy Jϕ(x), the mean of the
173"
PROXY-ENHANCED SAMPLING,0.2112970711297071,"proxy distribution pϕ(·|x) in Eq. (2). This distribution is trained exclusively at the initial time step
174"
PROXY-ENHANCED SAMPLING,0.21234309623430964,"t = 0, eliminating the need for training across time steps. To achieve this derivation, we reverse the
175"
PROXY-ENHANCED SAMPLING,0.21338912133891214,"diffusion from xt back to x0 using the formula:
176"
PROXY-ENHANCED SAMPLING,0.21443514644351463,x0 = xt + sθ(xt) · σ(t)2
PROXY-ENHANCED SAMPLING,0.21548117154811716,"µ(t)
,
(10)"
PROXY-ENHANCED SAMPLING,0.21652719665271966,"where sθ(xt) is the estimated unconditional score at time step t, and σ(t)2 and µ(t) are the variance
177"
PROXY-ENHANCED SAMPLING,0.2175732217573222,"and mean functions of the perturbation kernel at time t, as detailed in equations (32-33) in [10].
178"
PROXY-ENHANCED SAMPLING,0.2186192468619247,"Consequently, we express
179"
PROXY-ENHANCED SAMPLING,0.2196652719665272,"Jϕ(xt, t) = Jϕ"
PROXY-ENHANCED SAMPLING,0.22071129707112971,xt + sθ(xt) · σ(t)2 µ(t)
PROXY-ENHANCED SAMPLING,0.2217573221757322,"
.
(11)"
PROXY-ENHANCED SAMPLING,0.22280334728033474,"This formulation allows for the optimization of the strength parameter ω via Eq. (8). For simplicity,
180"
PROXY-ENHANCED SAMPLING,0.22384937238493724,"we will refer to Jϕ(·) in subsequent discussions.
181"
DIFFUSION-BASED PROXY REFINEMENT,0.22489539748953974,"3.2
Diffusion-based Proxy Refinement
182"
DIFFUSION-BASED PROXY REFINEMENT,0.22594142259414227,"In the proxy-enhanced sampling module, the proxy Jϕ(·) is employed to update the parameter ω
183"
DIFFUSION-BASED PROXY REFINEMENT,0.22698744769874477,"to enable enhanced control. However, Jϕ(·) may still be prone to the OOD issue, especially on
184"
DIFFUSION-BASED PROXY REFINEMENT,0.2280334728033473,"adversarial samples [5]. To address this, we refine the proxy by using insights from proxy-free
185"
DIFFUSION-BASED PROXY REFINEMENT,0.2290794979079498,"diffusion. The procedure of this module is specified in Algorithm 1, Lines 3-7.
186"
DIFFUSION-BASED PROXY REFINEMENT,0.2301255230125523,"Diffusion Distribution. Adversarial samples are identified by gradient ascent on the proxy as per
187"
DIFFUSION-BASED PROXY REFINEMENT,0.23117154811715482,"Eq. (3) to form the distribution q(x). Consequently, these samples are vulnerable to the proxy
188"
DIFFUSION-BASED PROXY REFINEMENT,0.23221757322175732,"distribution. Conversely, the proxy-free diffusion, which functions without depending on a proxy,
189"
DIFFUSION-BASED PROXY REFINEMENT,0.23326359832635984,"inherently offers greater resilience against these samples, thus producing a more robust distribution.
190"
DIFFUSION-BASED PROXY REFINEMENT,0.23430962343096234,"For an adversarial sample ˆx ∼q(x), we compute pθ(ˆx), pθ(ˆx|y) via the probability flow ODE, and
191"
DIFFUSION-BASED PROXY REFINEMENT,0.23535564853556484,"p(y) through Gaussian kernel-density estimation. The diffusion distribution regarding y is derived as:
192"
DIFFUSION-BASED PROXY REFINEMENT,0.23640167364016737,pθ(y|ˆx) = pθ(ˆx|y) · p(y)
DIFFUSION-BASED PROXY REFINEMENT,0.23744769874476987,"pθ(ˆx)
,
(12)"
DIFFUSION-BASED PROXY REFINEMENT,0.2384937238493724,"which demonstrates inherent robustness over the proxy distribution pϕ(y|ˆx). Yet, directly applying
193"
DIFFUSION-BASED PROXY REFINEMENT,0.2395397489539749,"diffusion distribution to design optimization by gradient ascent is computationally intensive and
194"
DIFFUSION-BASED PROXY REFINEMENT,0.2405857740585774,"potentially unstable due to the demands of reversing ODEs and scoring steps.
195"
DIFFUSION-BASED PROXY REFINEMENT,0.24163179916317992,"Proxy Refinement. We opt for a more feasible approach: refine the proxy distribution pϕ(y|ˆx) =
196"
DIFFUSION-BASED PROXY REFINEMENT,0.24267782426778242,"N(Jϕ(ˆx), σϕ(ˆx)) by minimizing its distance to the diffusion distribution pθ(y|ˆx). The distance is
197"
DIFFUSION-BASED PROXY REFINEMENT,0.24372384937238495,"quantified by the Kullback-Leibler (KL) divergence:
198"
DIFFUSION-BASED PROXY REFINEMENT,0.24476987447698745,Eq[D(pϕ||pθ)] = Eq(x)
DIFFUSION-BASED PROXY REFINEMENT,0.24581589958158995,"Z
pϕ(y|ˆx) log
pϕ(y|ˆx)"
DIFFUSION-BASED PROXY REFINEMENT,0.24686192468619247,pθ(y|ˆx)
DIFFUSION-BASED PROXY REFINEMENT,0.24790794979079497,"
dy.
(13)"
DIFFUSION-BASED PROXY REFINEMENT,0.2489539748953975,"We avoid the parameterization trick for minimizing this divergence as it necessitates backpropagation
199"
DIFFUSION-BASED PROXY REFINEMENT,0.25,"through pθ(y|ˆx), which is prohibitively expensive. Instead, for the sample ˆx, the gradient of the KL
200"
DIFFUSION-BASED PROXY REFINEMENT,0.2510460251046025,"divergence D(pϕ||pθ) with respect to the proxy parameters ϕ is computed as:
201"
DIFFUSION-BASED PROXY REFINEMENT,0.252092050209205,Epϕ(y|ˆx)
DIFFUSION-BASED PROXY REFINEMENT,0.25313807531380755,d log pϕ(y|ˆx) dϕ
DIFFUSION-BASED PROXY REFINEMENT,0.25418410041841005,"
1 + log pϕ(y|ˆx)"
DIFFUSION-BASED PROXY REFINEMENT,0.25523012552301255,pθ(y|ˆx)
DIFFUSION-BASED PROXY REFINEMENT,0.25627615062761505,"
.
(14)"
DIFFUSION-BASED PROXY REFINEMENT,0.25732217573221755,"Complete derivations are in Appendix A. The KL divergence then acts as regularization in our loss L:
202"
DIFFUSION-BASED PROXY REFINEMENT,0.2583682008368201,"L(ϕ, α) = ED[−log pϕ(y|x)] + αEq(x)[D(pϕ||pθ)],
(15)"
DIFFUSION-BASED PROXY REFINEMENT,0.2594142259414226,"where D is the training dataset and α is a hyperparameter. We propose to optimize α based on the
203"
DIFFUSION-BASED PROXY REFINEMENT,0.2604602510460251,"validation loss via bi-level optimization as detailed in Appendix B.
204"
EXPERIMENTS,0.2615062761506276,"4
Experiments
205"
EXPERIMENTS,0.2625523012552301,"In this section, we conduct comprehensive experiments to evaluate our method’s performance.
206"
BENCHMARKS,0.26359832635983266,"4.1
Benchmarks
207"
BENCHMARKS,0.26464435146443516,"Tasks. Our experiments encompass a variety of tasks, split into continuous and discrete categories.
208"
BENCHMARKS,0.26569037656903766,"The continuous category includes four tasks: (1) Superconductor (SuperC) 4: The objective here
209"
BENCHMARKS,0.26673640167364016,"is to engineer a superconductor composed of 86 continuous elements. The goal is to enhance the
210"
BENCHMARKS,0.26778242677824265,"critical temperature using 17, 010 design samples. This task is based on the dataset from [1]. (2) Ant
211"
BENCHMARKS,0.2688284518828452,"Morphology (Ant): In this task, the focus is on developing a quadrupedal ant robot, comprising 60
212"
BENCHMARKS,0.2698744769874477,"continuous parts, to augment its crawling velocity. It uses 10, 004 design instances from the dataset
213"
BENCHMARKS,0.2709205020920502,"in [3, 14]. (3) D’Kitty Morphology (D’Kitty): Similar to Ant Morphology, this task involves the
214"
BENCHMARKS,0.2719665271966527,"design of a quadrupedal D’Kitty robot with 56 components, aiming to improve its crawling speed
215"
BENCHMARKS,0.2730125523012552,"with 10, 004 designs, as described in [3, 15]. (4) Rosenbrock (Rosen): The aim of this task is to
216"
BENCHMARKS,0.27405857740585776,"optimize a 60-dimension continuous vector to maximize the Rosenbrock black-box function. It uses
217"
BENCHMARKS,0.27510460251046026,"50000 designs from the low-scoring part [9].
218"
BENCHMARKS,0.27615062761506276,"For the discrete category, we explore three tasks: (1) TF Bind 8 (TF8): The goal is to identify an
219"
BENCHMARKS,0.27719665271966526,"8-unit DNA sequence that maximizes binding activity. This task uses 32, 898 designs and is detailed
220"
BENCHMARKS,0.27824267782426776,"in [16]. (2) TF Bind 10 (TF10): Similar to TF8, but with a 10-unit DNA sequence and a larger pool
221"
BENCHMARKS,0.2792887029288703,"of 50, 000 samples, as described in [16]. (3) Neural Architecture Search (NAS): This task focuses
222"
BENCHMARKS,0.2803347280334728,"on discovering the optimal neural network architecture to improve test accuracy on the CIFAR-10
223"
BENCHMARKS,0.2813807531380753,"dataset, using 1, 771 designs [17].
224"
BENCHMARKS,0.2824267782426778,"Evaluation. In this study, we utilize the oracle evaluation from design-bench [3]. Adhering to this
225"
BENCHMARKS,0.2834728033472803,"established protocol, we analyze the top 128 promising designs from each method. The evaluation
226"
BENCHMARKS,0.28451882845188287,"metric employed is the 100th percentile normalized ground-truth score, calculated using the formula
227"
BENCHMARKS,0.28556485355648537,"yn =
y−ymin
ymax−ymin , where ymin and ymax signify the lowest and highest scores respectively in the
228"
BENCHMARKS,0.28661087866108786,"comprehensive, yet unobserved, dataset. In addition to these scores, we provide an overview of each
229"
BENCHMARKS,0.28765690376569036,"method’s effectiveness through the mean and median rankings across all evaluated tasks. Notably,
230"
BENCHMARKS,0.28870292887029286,"the best design discovered in the offline dataset, designated as D(best), is also included for reference.
231"
BENCHMARKS,0.2897489539748954,"For further details on the 50th percentile (median) scores, please refer to Appendix C.
232"
COMPARISON METHODS,0.2907949790794979,"4.2
Comparison Methods
233"
COMPARISON METHODS,0.2918410041841004,"Our approach is evaluated against two primary groups of baseline methods: forward and inverse
234"
COMPARISON METHODS,0.2928870292887029,"approaches. Forward approaches enhance existing designs through gradient ascent. This includes: (i)
235"
COMPARISON METHODS,0.2939330543933054,"Grad: utilizes simple gradient ascent on current designs for new creations; (ii) ROMA [18]: imple-
236"
COMPARISON METHODS,0.29497907949790797,"ments smoothness regularization on proxies; (iii) COMs [5]: applies regularization to assign lower
237"
COMPARISON METHODS,0.29602510460251047,"scores to adversarial designs; (iv) NEMO [19]: bridges the gap between proxy and actual functions
238"
COMPARISON METHODS,0.29707112970711297,"using normalized maximum likelihood; (v) BDI [20]: utilizes both forward and inverse mappings to
239"
COMPARISON METHODS,0.29811715481171547,"transfer knowledge from offline datasets to the designs; (vi) IOM [21]: ensures consistency between
240"
COMPARISON METHODS,0.29916317991631797,"representations of training datasets and optimized designs.
241"
COMPARISON METHODS,0.3002092050209205,"Inverse approaches focus on learning a mapping from a design’s property value back to its input.
242"
COMPARISON METHODS,0.301255230125523,"High property values are input into this inverse mapping to yield enhanced designs. This includes: (i)
243"
COMPARISON METHODS,0.3023012552301255,"CbAS [22]: CbAS employs a VAE model to implicitly implement the inverse mapping. It gradually
244"
COMPARISON METHODS,0.303347280334728,"tunes its distribution toward higher scores by raising the scoring threshold. This process can be
245"
COMPARISON METHODS,0.3043933054393305,"interpreted as incrementally increasing the conditional score within the inverse mapping framework.
246"
COMPARISON METHODS,0.3054393305439331,"(ii) Autofocused CbAS (Auto.CbAS) [23]: adopts importance sampling for retraining a regression
247"
COMPARISON METHODS,0.3064853556485356,"model based on CbAS. (iii) MIN [6]: maps scores to designs via a GAN model and explore this
248"
COMPARISON METHODS,0.3075313807531381,"mapping for optimal designs. (iv) BONET [24]: introduces an autoregressive model for sampling
249"
COMPARISON METHODS,0.30857740585774057,"high-scoring designs. (v) DDOM [4]: utilizes proxy-free diffusion to model the inverse mapping.
250"
COMPARISON METHODS,0.30962343096234307,"Traditional methods as detailed in [3] are also considered: (i) CMA-ES [25]: modifies the covariance
251"
COMPARISON METHODS,0.3106694560669456,"matrix to progressively shift the distribution towards optimal designs; (ii) BO-qEI [26]: implements
252"
COMPARISON METHODS,0.3117154811715481,"Bayesian optimization to maximize the proxy and utilizes the quasi-Expected-Improvement acqui-
253"
COMPARISON METHODS,0.3127615062761506,"sition function for design suggestion, labeling designs using the proxy; (iii) REINFORCE [27]:
254"
COMPARISON METHODS,0.3138075313807531,"enhances the input space distribution using the learned proxy model.
255"
COMPARISON METHODS,0.3148535564853556,"4Previously, the task oracle exhibited inconsistencies, producing varying outputs for identical inputs. This
issue has now been rectified by the development team."
EXPERIMENTAL CONFIGURATION,0.3158995815899582,"4.3
Experimental Configuration
256"
EXPERIMENTAL CONFIGURATION,0.3169456066945607,"In alignment with the experimental protocols established in [3, 20], we have tailored our training
257"
EXPERIMENTAL CONFIGURATION,0.3179916317991632,"methodologies for all approaches, except where specified otherwise. For methods such as BO-qEI,
258"
EXPERIMENTAL CONFIGURATION,0.3190376569037657,"CMA-ES, REINFORCE, CbAS, and Auto.CbAS that do not utilize gradient ascent, we base our
259"
EXPERIMENTAL CONFIGURATION,0.3200836820083682,"approach on the findings reported in [3]. We adopted T = 1000 diffusion sampling steps, set the
260"
EXPERIMENTAL CONFIGURATION,0.32112970711297073,"condition y to ymax, and initial strength ω as 2 in line with [4]. To ensure reliability and consistency in
261"
EXPERIMENTAL CONFIGURATION,0.32217573221757323,"our comparative analysis, each experimental setting was replicated across 8 independent runs, unless
262"
EXPERIMENTAL CONFIGURATION,0.32322175732217573,"stated otherwise, with the presentation of both mean values and standard errors. These experiments
263"
EXPERIMENTAL CONFIGURATION,0.32426778242677823,"were conducted using a NVIDIA GeForce V100 GPU. We’ve detailed the computational overhead of
264"
EXPERIMENTAL CONFIGURATION,0.3253138075313807,"our approach in Appendix D to provide a comprehensive view of its practicality.
265"
EXPERIMENTAL CONFIGURATION,0.3263598326359833,Table 1: Results (maximum normalized score) on continuous tasks.
EXPERIMENTAL CONFIGURATION,0.3274058577405858,"Method
Superconductor
Ant Morphology
D’Kitty Morphology
Rosenbrock
D(best)
0.399
0.565
0.884
0.518
BO-qEI
0.402 ± 0.034
0.819 ± 0.000
0.896 ± 0.000
0.772 ± 0.012
CMA-ES
0.465 ± 0.024
1.214 ± 0.732
0.724 ± 0.001
0.470 ± 0.026
REINFORCE
0.481 ± 0.013
0.266 ± 0.032
0.562 ± 0.196
0.558 ± 0.013
Grad
0.490 ± 0.009
0.932 ± 0.015
0.930 ± 0.002
0.701 ± 0.092
COMs
0.504 ± 0.022
0.818 ± 0.017
0.905 ± 0.017
0.672 ± 0.075
ROMA
0.507 ± 0.013
0.898 ± 0.029
0.928 ± 0.007
0.663 ± 0.072
NEMO
0.499 ± 0.003
0.956 ± 0.013
0.953 ± 0.010
0.614 ± 0.000
IOM
0.524 ± 0.022
0.929 ± 0.037
0.936 ± 0.008
0.712 ± 0.068
BDI
0.513 ± 0.000
0.906 ± 0.000
0.919 ± 0.000
0.630 ± 0.000
CbAS
0.503 ± 0.069
0.876 ± 0.031
0.892 ± 0.008
0.702 ± 0.008
Auto.CbAS
0.421 ± 0.045
0.882 ± 0.045
0.906 ± 0.006
0.721 ± 0.007
MIN
0.499 ± 0.017
0.445 ± 0.080
0.892 ± 0.011
0.702 ± 0.074
BONET
0.422 ± 0.019
0.925 ± 0.010
0.941 ± 0.001
0.780 ± 0.009
DDOM
0.495 ± 0.012
0.940 ± 0.004
0.935 ± 0.001
0.789 ± 0.003
RGD
0.515 ± 0.011
0.968 ± 0.006
0.943 ± 0.004
0.797 ± 0.011"
EXPERIMENTAL CONFIGURATION,0.3284518828451883,Table 2: Results (maximum normalized score) on discrete tasks & ranking on all tasks.
EXPERIMENTAL CONFIGURATION,0.3294979079497908,"Method
TF Bind 8
TF Bind 10
NAS
Rank Mean
Rank Median
D(best)
0.439
0.467
0.436
BO-qEI
0.798 ± 0.083
0.652 ± 0.038
1.079 ± 0.059
9.1/15
11/15
CMA-ES
0.953 ± 0.022
0.670 ± 0.023
0.985 ± 0.079
7.3/15
4/15
REINFORCE
0.948 ± 0.028
0.663 ± 0.034
−1.895 ± 0.000
11.3/15
14/15
Grad
0.872 ± 0.062
0.646 ± 0.052
0.624 ± 0.102
9.0/15
10/15
COMs
0.517 ± 0.115
0.613 ± 0.003
0.783 ± 0.029
10.3/15
10/15
ROMA
0.927 ± 0.033
0.676 ± 0.029
0.927 ± 0.071
6.1/15
6/15
NEMO
0.942 ± 0.003
0.708 ± 0.022
0.737 ± 0.010
5.3/15
5/15
IOM
0.823 ± 0.130
0.650 ± 0.042
0.559 ± 0.081
7.4/15
6/15
BDI
0.870 ± 0.000
0.605 ± 0.000
0.722 ± 0.000
9.6/15
9/15
CbAS
0.927 ± 0.051
0.651 ± 0.060
0.683 ± 0.079
8.7/15
8/15
Auto.CbAS
0.910 ± 0.044
0.630 ± 0.045
0.506 ± 0.074
10.3/15
10/15
MIN
0.905 ± 0.052
0.616 ± 0.021
0.717 ± 0.046
10.4/15
10/15
BONET
0.913 ± 0.008
0.621 ± 0.030
0.724 ± 0.008
7.7/15
8/15
DDOM
0.957 ± 0.006
0.657 ± 0.006
0.745 ± 0.070
4.9/15
5/15
RGD
0.974 ± 0.003
0.694 ± 0.018
0.825 ± 0.063
2.0/15
2/15"
RESULTS AND ANALYSIS,0.3305439330543933,"4.4
Results and Analysis
266"
RESULTS AND ANALYSIS,0.33158995815899583,"In Tables 1 and 2, we showcase our experimental results for both continuous and discrete tasks.
267"
RESULTS AND ANALYSIS,0.33263598326359833,"To clearly differentiate among the various approaches, distinct lines separate traditional, forward,
268"
RESULTS AND ANALYSIS,0.33368200836820083,"and inverse approaches within the tables For every task, algorithms performing within a standard
269"
RESULTS AND ANALYSIS,0.33472803347280333,"deviation of the highest score are emphasized by bolding following [5].
270"
RESULTS AND ANALYSIS,0.33577405857740583,"We make the following observations. (1) As highlighted in Table 2, RGD not only achieves the top
271"
RESULTS AND ANALYSIS,0.3368200836820084,"rank but also demonstrates the best performance in six out of seven tasks, emphasizing the robustness
272"
RESULTS AND ANALYSIS,0.3378661087866109,"and superiority of our method. (2) RGD outperforms the VAE-based CbAS, the GAN-based MIN
273"
RESULTS AND ANALYSIS,0.3389121338912134,"and the Transformer-based BONET. This result highlights the superiority of diffusion models in
274"
RESULTS AND ANALYSIS,0.3399581589958159,"modeling inverse mappings compared to other generative approaches. (3) Upon examining TF
275"
RESULTS AND ANALYSIS,0.3410041841004184,"Bind 8, we observe that the average rankings for forward and inverse methods stand at 10.3 and
276"
RESULTS AND ANALYSIS,0.34205020920502094,"6.0, respectively. In contrast, for TF Bind 10, both methods have the same average ranking of 8.7,
277"
RESULTS AND ANALYSIS,0.34309623430962344,"indicating no advantage. This notable advantage of inverse methods in TF Bind 8 implies that the
278"
RESULTS AND ANALYSIS,0.34414225941422594,"relatively smaller design space of TF Bind 8 (48) facilitates easier inverse mapping, as opposed to the
279"
RESULTS AND ANALYSIS,0.34518828451882844,"more complex space in TF Bind 10 (410). (4) RGD’s performance is less impressive on NAS, where
280"
RESULTS AND ANALYSIS,0.34623430962343094,"designs are encoded as 64-length sequences of 5-category one-hot vectors. This may stem from
281"
RESULTS AND ANALYSIS,0.3472803347280335,"the design-bench’s encoding not fully capturing the sequential and hierarchical aspects of network
282"
RESULTS AND ANALYSIS,0.348326359832636,"architectures, affecting the efficacy of inverse mapping modeling.
283"
RESULTS AND ANALYSIS,0.3493723849372385,Table 3: Ablation studies on RGD.
RESULTS AND ANALYSIS,0.350418410041841,"Task
D
RGD
w/o proxy-e
w/o diffusion-b r
direct grad update
SuperC
86
0.515 ± 0.011
0.495 ± 0.012
0.502 ± 0.005
0.456 ± 0.002
Ant
60
0.968 ± 0.006
0.940 ± 0.004
0.961 ± 0.011
−0.006 ± 0.003
D’Kitty
56
0.943 ± 0.004
0.935 ± 0.001
0.939 ± 0.003
0.714 ± 0.001
Rosen
60
0.797 ± 0.011
0.789 ± 0.003
0.813 ± 0.005
0.241 ± 0.283
TF8
8
0.974 ± 0.003
0.957 ± 0.007
0.960 ± 0.006
0.905 ± 0.000
TF10
10
0.694 ± 0.018
0.657 ± 0.006
0.667 ± 0.009
0.672 ± 0.018
NAS
64
0.825 ± 0.063
0.745 ± 0.070
0.717 ± 0.032
0.718 ± 0.032"
ABLATION STUDIES,0.3514644351464435,"4.5
Ablation Studies
284"
ABLATION STUDIES,0.35251046025104604,"In this section, we present a series of ablation studies to scrutinize the individual contributions of
285"
ABLATION STUDIES,0.35355648535564854,"distinct components in our methodology. We employ our proposed approach as a benchmark and
286"
ABLATION STUDIES,0.35460251046025104,"methodically exclude key modules, such as the proxy-enhanced sampling and diffusion-based proxy
287"
ABLATION STUDIES,0.35564853556485354,"refinement, to assess their influence on performance. These variants are denoted as w/o proxy-e and
288"
ABLATION STUDIES,0.35669456066945604,"w/o diffusion-b r. Additionally, we explore the strategy of directly performing gradient ascent on
289"
ABLATION STUDIES,0.3577405857740586,"the diffusion intermediate state, referred to as direct grad update. The results from these ablation
290"
ABLATION STUDIES,0.3587866108786611,"experiments are detailed in Table 3.
291"
ABLATION STUDIES,0.3598326359832636,"Our analysis reveals that omitting either module results in a decrease in performance, thereby affirming
292"
ABLATION STUDIES,0.3608786610878661,"the importance of each component. The w/o diffusion-b r variant generally surpasses w/o proxy-e,
293"
ABLATION STUDIES,0.3619246861924686,"highlighting the utility of the proxy-enhanced sampling even with a basic proxy setup. Conversely,
294"
ABLATION STUDIES,0.36297071129707115,"direct grad update tends to produce subpar results across tasks, likely attributable to the proxy’s
295"
ABLATION STUDIES,0.36401673640167365,"limitations in handling out-of-distribution samples, leading to suboptimal design optimizations.
296"
ABLATION STUDIES,0.36506276150627615,"0
100
200
300
400
500
600
700
800
900
1000"
ABLATION STUDIES,0.36610878661087864,Diffusion step t 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
ABLATION STUDIES,0.36715481171548114,"Strength ratio 
/
0"
ABLATION STUDIES,0.3682008368200837,"Ant
TF10"
ABLATION STUDIES,0.3692468619246862,Figure 3: Dynamics of strength ratio ω/ω0.
ABLATION STUDIES,0.3702928870292887,"To further dive into the proxy-enhanced sam-
297"
ABLATION STUDIES,0.3713389121338912,"pling module, we visualize the strength ra-
298"
ABLATION STUDIES,0.3723849372384937,"tio ω/ω0—where ω0 represents the initial
299"
ABLATION STUDIES,0.37343096234309625,"strength—across diffusion steps t. This analysis
300"
ABLATION STUDIES,0.37447698744769875,"is depicted in Figure 3 for two specific tasks:
301"
ABLATION STUDIES,0.37552301255230125,"Ant and TF10. We observe a pattern of initial
302"
ABLATION STUDIES,0.37656903765690375,"decrease followed by an increase in ω across
303"
ABLATION STUDIES,0.3776150627615063,"both tasks. This pattern can be interpreted as
304"
ABLATION STUDIES,0.3786610878661088,"follows: The decrease in ω facilitates the genera-
305"
ABLATION STUDIES,0.3797071129707113,"tion of a more diverse set of samples, enhancing
306"
ABLATION STUDIES,0.3807531380753138,"exploratory capabilities. Subsequently, the in-
307"
ABLATION STUDIES,0.3817991631799163,"crease in ω signifies a shift towards integrating
308"
ABLATION STUDIES,0.38284518828451886,"high-performance features into the sample gen-
309"
ABLATION STUDIES,0.38389121338912136,"eration. Within this context, conditioning on
310"
ABLATION STUDIES,0.38493723849372385,"the maximum y is not aimed at achieving the
311"
ABLATION STUDIES,0.38598326359832635,"dataset’s maximum but at enriching samples with high-scoring attributes. Overall, this adjustment of
312"
ABLATION STUDIES,0.38702928870292885,"ω effectively balances between generating novel solutions and honing in on high-quality ones.
313"
ABLATION STUDIES,0.3880753138075314,"In addition, we visualize the proxy distribution alongside the diffusion distribution for a sample ˆx
314"
ABLATION STUDIES,0.3891213389121339,"from the Ant task in Figure 4, to substantiate the efficacy of diffusion-based proxy refinement. The
315"
ABLATION STUDIES,0.3901673640167364,"proxy distribution significantly overestimates the ground truth, whereas the diffusion distribution
316"
ABLATION STUDIES,0.3912133891213389,"closely aligns with it, demonstrating the robustness of diffusion distribution. For a more quantitative
317"
ABLATION STUDIES,0.3922594142259414,"analysis, we compute the expectation of both distributions and compare them with the ground
318"
ABLATION STUDIES,0.39330543933054396,"truth. The mean of the diffusion distribution is calculated as Epθ(y|ˆx)[y] = Epϕ(y|ˆx)
h
pθ(y|ˆx)
pϕ(y|ˆx)y
i
.
319"
ABLATION STUDIES,0.39435146443514646,"0.6
0.8
1.0
1.2
Y 0 1 2 3 4"
ABLATION STUDIES,0.39539748953974896,Prob Density
ABLATION STUDIES,0.39644351464435146,Proxy Distribution p (y|x)
ABLATION STUDIES,0.39748953974895396,Peak of Proxy Distribution
ABLATION STUDIES,0.3985355648535565,"Diffusion Distribution p (y|x)
Peak of Diffusion Distribution
Ground-truth"
ABLATION STUDIES,0.399581589958159,Figure 4: Proxy vs. diffusion distribution.
ABLATION STUDIES,0.4006276150627615,"The MSE loss for the proxy distribution is 2.88, while
320"
ABLATION STUDIES,0.401673640167364,"for the diffusion distribution, it is 0.13 on the Ant
321"
ABLATION STUDIES,0.4027196652719665,"task. Additionally, we evaluate this on the TFB10
322"
ABLATION STUDIES,0.40376569037656906,"task, where the MSE loss for the proxy distribution
323"
ABLATION STUDIES,0.40481171548117156,"is 323.63 compared to 0.82 for the diffusion distribu-
324"
ABLATION STUDIES,0.40585774058577406,"tion. These results further corroborate the effective-
325"
ABLATION STUDIES,0.40690376569037656,"ness of our proposed module.
326"
ABLATION STUDIES,0.40794979079497906,"Furthermore, we (1) investigate the impact of re-
327"
ABLATION STUDIES,0.4089958158995816,"placing our trained proxy model with alternative ap-
328"
ABLATION STUDIES,0.4100418410041841,"proaches, specifically ROMA and COMs, (2) analyze
329"
ABLATION STUDIES,0.4110878661087866,"the performance with an optimized condition y and
330"
ABLATION STUDIES,0.4121338912133891,"(3) explore a simple annealing approach of ω. For
331"
ABLATION STUDIES,0.4131799163179916,"a comprehensive discussion on these, readers are re-
332"
ABLATION STUDIES,0.41422594142259417,"ferred to Appendix E.
333"
HYPERPARAMETER SENSITIVITY ANALYSIS,0.41527196652719667,"4.6
Hyperparameter Sensitivity Analysis
334"
HYPERPARAMETER SENSITIVITY ANALYSIS,0.41631799163179917,"This section investigates the sensitivity of RGD to various hyperparameters. Specifically, we analyze
335"
HYPERPARAMETER SENSITIVITY ANALYSIS,0.41736401673640167,"the effects of (1) the number of diffusion sampling steps T, (2) the condition y, and (3) the learning
336"
HYPERPARAMETER SENSITIVITY ANALYSIS,0.41841004184100417,"rate η of the proxy-enhanced sampling. These parameters are evaluated on two tasks: the continuous
337"
HYPERPARAMETER SENSITIVITY ANALYSIS,0.4194560669456067,"Ant task and the discrete TFB10 task. For a detailed discussion, see Appendix F.
338"
RELATED WORK,0.4205020920502092,"5
Related Work
339"
RELATED WORK,0.4215481171548117,"Offline black-box optimization. A recent surge in research has presented two predominant ap-
340"
RELATED WORK,0.4225941422594142,"proaches for offline BBO. The forward approach deploys a DNN to fit the offline dataset, subsequently
341"
RELATED WORK,0.4236401673640167,"utilizing gradient ascent to enhance existing designs. Typically, these techniques, including COMs [5],
342"
RELATED WORK,0.4246861924686193,"ROMA [18], NEMO [19], BDI [20, 28], IOM [29] and Parallel-mentoring [30], are designed to
343"
RELATED WORK,0.42573221757322177,"embed prior knowledge within the surrogate model to alleviate the OOD issue. The reverse ap-
344"
RELATED WORK,0.42677824267782427,"proach [6, 31] is dedicated to learning a mapping from property values back to inputs. Feeding a high
345"
RELATED WORK,0.42782426778242677,"value into this inverse mapping directly produces a design of elevated performance. Additionally,
346"
RELATED WORK,0.42887029288702927,"methods in [22, 23] progressively tailor a generative model towards the optimized design via a proxy
347"
RELATED WORK,0.4299163179916318,"function and BONET [24] introduces an autoregressive model trained on fixed-length trajectories to
348"
RELATED WORK,0.4309623430962343,"sample high-scoring designs. Recent investigations [4] have underscored the superiority of diffusion
349"
RELATED WORK,0.4320083682008368,"models in delineating the inverse mapping. However, research on specialized guided diffusion for
350"
RELATED WORK,0.4330543933054393,"offline BBO remains limited. This paper addresses this research gap.
351"
RELATED WORK,0.4341004184100418,"Guided diffusion. Guided diffusion seeks to produce samples with specific desirable attributes.
352"
RELATED WORK,0.4351464435146444,"Contemporary research in guided diffusion primarily concentrates on enhancing the efficiency of
353"
RELATED WORK,0.4361924686192469,"its sampling process. [32] propose a method for distilling a classifier-free guided diffusion model
354"
RELATED WORK,0.4372384937238494,"into a more efficient single model that necessitates fewer steps in sampling. [33] introduce an
355"
RELATED WORK,0.4382845188284519,"operator splitting method to expedite classifier guidance by separating the update process into two
356"
RELATED WORK,0.4393305439330544,"key functions: the diffusion function and the conditioning function. Additionally, [34] presents an
357"
RELATED WORK,0.44037656903765693,"efficient and universal guidance mechanism that utilizes a readily available proxy to enable diffusion
358"
RELATED WORK,0.44142259414225943,"guidance across time steps. In this work, we explore the application of guided diffusion in offline
359"
RELATED WORK,0.4424686192468619,"BBO, with the goal of creating tailored algorithms to efficiently generate high-performance designs.
360"
CONCLUSION,0.4435146443514644,"6
Conclusion
361"
CONCLUSION,0.4445606694560669,"In conclusion, we propose Robust Guided Diffusion for Offline Black-box Optimization (RGD). The
362"
CONCLUSION,0.4456066945606695,"proxy-enhanced sampling module adeptly integrates proxy guidance to enable enhanced sampling
363"
CONCLUSION,0.446652719665272,"control, while the diffusion-based proxy refinement module leverages proxy-free diffusion insights
364"
CONCLUSION,0.4476987447698745,"for proxy improvement. Empirical evaluations on design-bench have showcased RGD’s outstanding
365"
CONCLUSION,0.448744769874477,"performance, further validated by ablation studies on the contributions of these novel components.
366"
CONCLUSION,0.4497907949790795,"We discuss the broader impact and limitation in Appendix G.
367"
REFERENCES,0.45083682008368203,"References
368"
REFERENCES,0.45188284518828453,"[1] Kam Hamidieh. A data-driven statistical model for predicting the critical temperature of a
369"
REFERENCES,0.45292887029288703,"superconductor. Computational materials science, 2018.
370"
REFERENCES,0.45397489539748953,"[2] Karen S Sarkisyan et al. Local fitness landscape of the green fluorescent protein. Nature, 2016.
371"
REFERENCES,0.45502092050209203,"[3] Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-Bench: bench-
372"
REFERENCES,0.4560669456066946,"marks for data-driven offline model-based optimization. arXiv preprint arXiv:2202.08450,
373"
REFERENCES,0.4571129707112971,"2022.
374"
REFERENCES,0.4581589958158996,"[4] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Diffusion models for
375"
REFERENCES,0.4592050209205021,"black-box optimization. Proc. Int. Conf. Machine Learning (ICML), 2023.
376"
REFERENCES,0.4602510460251046,"[5] Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective
377"
REFERENCES,0.46129707112970714,"models for effective offline model-based optimization. In Proc. Int. Conf. Machine Learning
378"
REFERENCES,0.46234309623430964,"(ICML), 2021.
379"
REFERENCES,0.46338912133891214,"[6] Aviral Kumar and Sergey Levine. Model inversion networks for model-based optimization.
380"
REFERENCES,0.46443514644351463,"Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2020.
381"
REFERENCES,0.46548117154811713,"[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
382"
REFERENCES,0.4665271966527197,"Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. Adv. Neur.
383"
REFERENCES,0.4675732217573222,"Inf. Proc. Syst (NeurIPS), 2014.
384"
REFERENCES,0.4686192468619247,"[8] Jonathan Ho and Tim Salimans.
Classifier-free diffusion guidance.
arXiv preprint
385"
REFERENCES,0.4696652719665272,"arXiv:2207.12598, 2022.
386"
REFERENCES,0.4707112970711297,"[9] HoHo Rosenbrock. An automatic method for finding the greatest or least value of a function.
387"
REFERENCES,0.47175732217573224,"The computer journal, 1960.
388"
REFERENCES,0.47280334728033474,"[10] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
389"
REFERENCES,0.47384937238493724,"Ben Poole. Score-based generative modeling through stochastic differential equations. Proc.
390"
REFERENCES,0.47489539748953974,"Int. Conf. Learning Rep. (ICLR), 2021.
391"
REFERENCES,0.47594142259414224,"[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Proc.
392"
REFERENCES,0.4769874476987448,"Adv. Neur. Inf. Proc. Syst (NeurIPS), 2021.
393"
REFERENCES,0.4780334728033473,"[12] Endre Süli and David F Mayers. An introduction to numerical analysis. Cambridge university
394"
REFERENCES,0.4790794979079498,"press, 2003.
395"
REFERENCES,0.4801255230125523,"[13] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
396"
REFERENCES,0.4811715481171548,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: an imperative
397"
REFERENCES,0.48221757322175735,"style, high-performance deep learning library. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2019.
398"
REFERENCES,0.48326359832635984,"[14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
399"
REFERENCES,0.48430962343096234,"and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
400"
REFERENCES,0.48535564853556484,"[15] Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine,
401"
REFERENCES,0.48640167364016734,"and Vikash Kumar. Robel: robotics benchmarks for learning with low-cost robots. In Conf. on
402"
REFERENCES,0.4874476987447699,"Robot Lea. (CoRL), 2020.
403"
REFERENCES,0.4884937238493724,"[16] Luis A Barrera et al. Survey of variation in human transcription factors reveals prevalent DNA
404"
REFERENCES,0.4895397489539749,"binding changes. Science, 2016.
405"
REFERENCES,0.4905857740585774,"[17] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. arXiv
406"
REFERENCES,0.4916317991631799,"preprint arXiv:1611.01578, 2017.
407"
REFERENCES,0.49267782426778245,"[18] Sihyun Yu, Sungsoo Ahn, Le Song, and Jinwoo Shin. Roma: robust model adaptation for offline
408"
REFERENCES,0.49372384937238495,"model-based optimization. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2021.
409"
REFERENCES,0.49476987447698745,"[19] Justin Fu and Sergey Levine. Offline model-based optimization via normalized maximum
410"
REFERENCES,0.49581589958158995,"likelihood estimation. Proc. Int. Conf. Learning Rep. (ICLR), 2021.
411"
REFERENCES,0.49686192468619245,"[20] Can Chen, Yingxue Zhang, Jie Fu, Xue Liu, and Mark Coates. Bidirectional learning for offline
412"
REFERENCES,0.497907949790795,"infinite-width model-based optimization. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022.
413"
REFERENCES,0.4989539748953975,"[21] Han Qi, Yi Su, Aviral Kumar, and Sergey Levine. Data-driven model-based optimization via
414"
REFERENCES,0.5,"invariant representation learning. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022.
415"
REFERENCES,0.5010460251046025,"[22] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling
416"
REFERENCES,0.502092050209205,"for robust design. In Proc. Int. Conf. Machine Learning (ICML), 2019.
417"
REFERENCES,0.5031380753138075,"[23] Clara Fannjiang and Jennifer Listgarten. Autofocused oracles for model-based design. Proc.
418"
REFERENCES,0.50418410041841,"Adv. Neur. Inf. Proc. Syst (NeurIPS), 2020.
419"
REFERENCES,0.5052301255230126,"[24] Satvik Mehul Mashkaria, Siddarth Krishnamoorthy, and Aditya Grover. Generative pretraining
420"
REFERENCES,0.5062761506276151,"for black-box optimization. In Proc. Int. Conf. Machine Learning (ICML), 2023.
421"
REFERENCES,0.5073221757322176,"[25] Nikolaus Hansen. The CMA evolution strategy: a comparing review. Towards A New Evolu-
422"
REFERENCES,0.5083682008368201,"tionary Computation, 2006.
423"
REFERENCES,0.5094142259414226,"[26] James T Wilson, Riccardo Moriconi, Frank Hutter, and Marc Peter Deisenroth. The reparame-
424"
REFERENCES,0.5104602510460251,"terization trick for acquisition functions. arXiv preprint arXiv:1712.00424, 2017.
425"
REFERENCES,0.5115062761506276,"[27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
426"
REFERENCES,0.5125523012552301,"ment learning. Machine learning, 1992.
427"
REFERENCES,0.5135983263598326,"[28] Can Chen, Yingxue Zhang, Xue Liu, and Mark Coates. Bidirectional learning for offline
428"
REFERENCES,0.5146443514644351,"model-based biological sequence design. In Proc. Int. Conf. Machine Lea. (ICML), 2023.
429"
REFERENCES,0.5156903765690377,"[29] Han Qi, Yi Su, Aviral Kumar, and Sergey Levine. Data-driven model-based optimization via
430"
REFERENCES,0.5167364016736402,"invariant representation learning. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022.
431"
REFERENCES,0.5177824267782427,"[30] Can Chen, Christopher Beckham, Zixuan Liu, Xue Liu, and Christopher Pal. Parallel-mentoring
432"
REFERENCES,0.5188284518828452,"for offline model-based optimization. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2023.
433"
REFERENCES,0.5198744769874477,"[31] Alvin Chan, Ali Madani, Ben Krause, and Nikhil Naik. Deep extrapolation for attribute-
434"
REFERENCES,0.5209205020920502,"enhanced generation. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2021.
435"
REFERENCES,0.5219665271966527,"[32] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho,
436"
REFERENCES,0.5230125523012552,"and Tim Salimans. On distillation of guided diffusion models. In Proc. Comp. Vision. Pattern.
437"
REFERENCES,0.5240585774058577,"Rec.(CVPR), 2023.
438"
REFERENCES,0.5251046025104602,"[33] Suttisak Wizadwongsa and Supasorn Suwajanakorn. Accelerating guided diffusion sampling
439"
REFERENCES,0.5261506276150628,"with splitting numerical methods. In Proc. Int. Conf. Learning Rep. (ICLR), 2023.
440"
REFERENCES,0.5271966527196653,"[34] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum,
441"
REFERENCES,0.5282426778242678,"Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proc. Comp.
442"
REFERENCES,0.5292887029288703,"Vision. Pattern. Rec.(CVPR), 2023.
443"
REFERENCES,0.5303347280334728,"[35] Can Chen, Yingxue Zhang, Jie Fu, Xue Liu, and Mark Coates. Bidirectional learning for offline
444"
REFERENCES,0.5313807531380753,"infinite-width model-based optimization. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022.
445"
REFERENCES,0.5324267782426778,"A
Derivation
446"
REFERENCES,0.5334728033472803,"This section provides a derivation of the gradient of the KL divergence. Let’s consider the KL
447"
REFERENCES,0.5345188284518828,"divergence term, defined as:
448"
REFERENCES,0.5355648535564853,"D(pϕ||pθ) =
Z
pϕ(y|ˆx) log
pϕ(y|ˆx)"
REFERENCES,0.5366108786610879,pθ(y|ˆx)
REFERENCES,0.5376569037656904,"
dy.
(16)"
REFERENCES,0.5387029288702929,"The gradient with respect to the parameters ϕ is computed as follows:
449"
REFERENCES,0.5397489539748954,dD(pϕ||pθ)
REFERENCES,0.5407949790794979,"dϕ
=
Z dpϕ(y|ˆx) dϕ"
REFERENCES,0.5418410041841004,"
1 + log pϕ(y|ˆx)"
REFERENCES,0.5428870292887029,"pθ(y|ˆx) 
dy"
REFERENCES,0.5439330543933054,"=
Z
pϕ(y|ˆx)d log pϕ(y|ˆx)"
REFERENCES,0.5449790794979079,"dϕ
(1 + log pϕ(y|ˆx)"
REFERENCES,0.5460251046025104,pθ(y|ˆx) ) dy
REFERENCES,0.547071129707113,= Epϕ(y|ˆx)
REFERENCES,0.5481171548117155,d log pϕ(y|ˆx) dϕ
REFERENCES,0.549163179916318,"
1 + log pϕ(y|ˆx)"
REFERENCES,0.5502092050209205,"pθ(y|ˆx) 
. (17)"
REFERENCES,0.551255230125523,"B
Hyperparameter Optimization
450"
REFERENCES,0.5523012552301255,"We propose adjusting α based on the validation loss, establishing a bi-level optimization framework:
451"
REFERENCES,0.553347280334728,"α∗= arg min
α
EDv[log pϕ∗(α)(yv|xv)],
(18)"
REFERENCES,0.5543933054393305,"s.t.
ϕ∗(α) = arg min
ϕ
L(ϕ, α).
(19)"
REFERENCES,0.555439330543933,"Within this context, Dv represents the validation dataset sampled from the offline dataset. The inner
452"
REFERENCES,0.5564853556485355,"optimization task, which seeks the optimal ϕ∗(α), is efficiently approximated via gradient descent.
453"
REFERENCES,0.5575313807531381,"C
Evaluation of Median Scores
454"
REFERENCES,0.5585774058577406,"While the main text of our paper focuses on the 100th percentile scores, this section provides an
455"
REFERENCES,0.5596234309623431,"in-depth analysis of the 50th percentile scores. These median scores, previously explored in [3], serve
456"
REFERENCES,0.5606694560669456,"as an additional metric to assess the performance of our RGD method. The outcomes for continuous
457"
REFERENCES,0.5617154811715481,"tasks are detailed in Table 5, and those pertaining to discrete tasks, along with their respective ranking
458"
REFERENCES,0.5627615062761506,"statistics, are outlined in Table 6. An examination of Table 6 highlights the notable success of the
459"
REFERENCES,0.5638075313807531,"RGD approach, as it achieves the top rank in this evaluation. This finding underscores the method’s
460"
REFERENCES,0.5648535564853556,"robustness and effectiveness.
461"
REFERENCES,0.5658995815899581,"D
Computational Overhead
462"
REFERENCES,0.5669456066945606,Table 4: Computational Overhead (in seconds).
REFERENCES,0.5679916317991632,"Process
SuperC
Ant
D’Kitty
NAS
Proxy training
40.8
74.5
24.7
7.8
Diffusion training
405.9
767.9
251.1
56.0
Proxy-e sampling
30.0
29.7
29.6
31.5
Diffusion-b proxy r
3104.6
4036.7
2082.8
3096.2
Overall cost
3581.3
4908.8
2388.2
3191.5"
REFERENCES,0.5690376569037657,"In this section, we analyze the computational overhead of our method. RGD consists of two core
463"
REFERENCES,0.5700836820083682,"components: proxy-enhanced sampling (proxy-e sampling) and diffusion-based proxy refinement
464"
REFERENCES,0.5711297071129707,"(diffusion-b proxy r). Additionally, RGD employs a trained proxy and a proxy-free diffusion model,
465"
REFERENCES,0.5721757322175732,"whose computational demands are denoted as proxy training and diffusion training, respectively.
466"
REFERENCES,0.5732217573221757,"Table 4 indicates that experiments can be completed within approximately one hour, demonstrating ef-
467"
REFERENCES,0.5742677824267782,"ficiency. The diffusion-based proxy refinement module is the primary contributor to the computational
468"
REFERENCES,0.5753138075313807,"overhead, primarily due to the usage of a probability flow ODE for sample likelihood computation.
469"
REFERENCES,0.5763598326359832,"However, as this is a one-time process for refining the proxy, its high computational cost is offset by its
470"
REFERENCES,0.5774058577405857,"non-recurring nature. In contexts such as robotics or bio-chemical research, the most time-intensive
471"
REFERENCES,0.5784518828451883,"part of the production cycle is usually the evaluation of the unknown objective function. Therefore,
472"
REFERENCES,0.5794979079497908,"the time differences between methods for deriving high-performance designs are less critical in
473"
REFERENCES,0.5805439330543933,"actual production environments, highlighting RGD’s practicality where optimization performance
474"
REFERENCES,0.5815899581589958,"are prioritized over computational speed. This aligns with recent literature (A.3 Computational
475"
REFERENCES,0.5826359832635983,"Complexity in [35] and A.7.5. Computational Cost in [28]) indicating that in black-box optimization
476"
REFERENCES,0.5836820083682008,"scenarios, computational time is relatively minor compared to the time and resources dedicated to
477"
REFERENCES,0.5847280334728033,"experimental validation phases.
478"
REFERENCES,0.5857740585774058,Table 5: Results (median normalized score) on continuous tasks.
REFERENCES,0.5868200836820083,"Method
Superconductor
Ant Morphology
D’Kitty Morphology
Rosenbrock
BO-qEI
0.300 ± 0.015
0.567 ± 0.000
0.883 ± 0.000
0.761 ± 0.004
CMA-ES
0.379 ± 0.003
−0.045 ± 0.004
0.684 ± 0.016
0.200 ± 0.000
REINFORCE
0.463 ± 0.016
0.138 ± 0.032
0.356 ± 0.131
0.553 ± 0.008
Grad
0.339 ± 0.013
0.532 ± 0.014
0.867 ± 0.006
0.540 ± 0.025
COMs
0.312 ± 0.018
0.568 ± 0.002
0.883 ± 0.000
0.419 ± 0.286
ROMA
0.364 ± 0.020
0.467 ± 0.031
0.850 ± 0.006
−0.121 ± 0.242
NEMO
0.319 ± 0.010
0.592 ± 0.001
0.882 ± 0.002
0.510 ± 0.000
IOM
0.343 ± 0.018
0.513 ± 0.024
0.873 ± 0.009
0.126 ± 0.443
BDI
0.412 ± 0.000
0.474 ± 0.000
0.855 ± 0.000
0.561 ± 0.000
CbAS
0.111 ± 0.017
0.384 ± 0.016
0.753 ± 0.008
0.676 ± 0.008
Auto.CbAS
0.131 ± 0.010
0.364 ± 0.014
0.736 ± 0.025
0.695 ± 0.008
MIN
0.336 ± 0.016
0.618 ± 0.040
0.887 ± 0.004
0.634 ± 0.082
BONET
0.319 ± 0.014
0.615 ± 0.004
0.895 ± 0.021
0.630 ± 0.009
DDOM
0.295 ± 0.001
0.590 ± 0.003
0.870 ± 0.001
0.640 ± 0.001
RGD
0.308 ± 0.003
0.684 ± 0.006
0.874 ± 0.001
0.644 ± 0.002"
REFERENCES,0.5878661087866108,Table 6: Results (median normalized score) on discrete tasks & ranking on all tasks.
REFERENCES,0.5889121338912134,"Method
TF Bind 8
TF Bind 10
NAS
Rank Mean
Rank Median
BO-qEI
0.439 ± 0.000
0.467 ± 0.000
0.544 ± 0.099
6.4/15
7/15
CMA-ES
0.537 ± 0.014
0.484 ± 0.014
0.591 ± 0.102
8.0/15
5/15
REINFORCE
0.462 ± 0.021
0.475 ± 0.008
−1.895 ± 0.000
9.7/15
9/15
Grad
0.546 ± 0.022
0.526 ± 0.029
0.443 ± 0.126
6.6/15
8/15
COMs
0.439 ± 0.000
0.467 ± 0.000
0.529 ± 0.003
7.7/15
8/15
ROMA
0.543 ± 0.017
0.518 ± 0.024
0.529 ± 0.008
7.6/15
5/15
NEMO
0.436 ± 0.016
0.453 ± 0.013
0.563 ± 0.020
8.3/15
8/15
IOM
0.439 ± 0.000
0.474 ± 0.014
−0.083 ± 0.012
9.3/15
8/15
BDI
0.439 ± 0.000
0.476 ± 0.000
0.517 ± 0.000
7.3/15
8/15
CbAS
0.428 ± 0.010
0.463 ± 0.007
0.292 ± 0.027
11.3/15
12/15
Auto.CbAS
0.419 ± 0.007
0.461 ± 0.007
0.217 ± 0.005
11.9/15
13/15
MIN
0.421 ± 0.015
0.468 ± 0.006
0.433 ± 0.000
7.0/15
7/15
BONET
0.507 ± 0.007
0.460 ± 0.013
0.571 ± 0.095
5.9/15
6/15
DDOM
0.553 ± 0.002
0.488 ± 0.001
0.367 ± 0.021
6.9/15
5/15
RGD
0.557 ± 0.002
0.545 ± 0.006
0.371 ± 0.019
4.9/15
4/15"
REFERENCES,0.5899581589958159,"E
Further Ablation Studies
479"
REFERENCES,0.5910041841004184,"In this section, we extend our exploration to include alternative proxy refinement schemes, namely
480"
REFERENCES,0.5920502092050209,"ROMA and COMs, to compare against our diffusion-based proxy refinement module. The objective
481"
REFERENCES,0.5930962343096234,"is to assess the relative effectiveness of these schemes in the context of the Ant and TFB10 tasks.
482"
REFERENCES,0.5941422594142259,"The comparative results are presented in Table 7. Our investigation reveals that proxies refined
483"
REFERENCES,0.5951882845188284,"through ROMA and COMs exhibit performance akin to the vanilla proxy and they fall short of
484"
REFERENCES,0.5962343096234309,"achieving the enhancements seen with our diffusion-based proxy refinement. We hypothesize that
485"
REFERENCES,0.5972803347280334,"the diffusion-based proxy refinement, by aligning closely with the characteristics of the diffusion
486"
REFERENCES,0.5983263598326359,"model, provides a more relevant and impactful signal. This alignment improves the proxy’s ability to
487"
REFERENCES,0.5993723849372385,"enhance the sampling process more effectively.
488"
REFERENCES,0.600418410041841,"Table 7: Comparative Results of Proxy Integration with COMs, ROMA, and ours."
REFERENCES,0.6014644351464435,"Method
Ant Morphology
TF Bind 10
No proxy
0.940 ± 0.004
0.657 ± 0.006
Vanilla proxy
0.961 ± 0.011
0.667 ± 0.009
COMs
0.963 ± 0.004
0.668 ± 0.003
ROMA
0.953 ± 0.003
0.667 ± 0.003
Ours
0.968 ± 0.006
0.694 ± 0.018"
REFERENCES,0.602510460251046,"Additionally, we contrast our approach, which adjusts the strength parameter ω, with the MIN method
489"
REFERENCES,0.6035564853556485,"that focuses on identifying an optimal condition y. The MIN strategy entails optimizing a Lagrangian
490"
REFERENCES,0.604602510460251,"objective with respect to y, a process that requires manual tuning of four hyperparameters. We
491"
REFERENCES,0.6056485355648535,"adopt their methodology to determine optimal conditions y and incorporate these into the proxy-free
492"
REFERENCES,0.606694560669456,"diffusion for tasks Ant and TF10. The normalized scores for Ant and TF10 are 0.950 ± 0.017 and
493"
REFERENCES,0.6077405857740585,"0.660 ± 0.027, respectively. The outcomes fall short of those achieved by our method as detailed
494"
REFERENCES,0.608786610878661,"in Table 7. This discrepancy likely stems from the complexity involved in optimizing y, whereas
495"
REFERENCES,0.6098326359832636,"dynamically adjusting ω proves to be a more efficient strategy for enhancing sampling control.
496"
REFERENCES,0.6108786610878661,"Last but not least, we explore simple annealing approaches for ω. Specifically, we test two annealing
497"
REFERENCES,0.6119246861924686,"scenarios considering the default ω as 2.0: (1) a decrease from 4.0 to 0.0, and (2) an increase from
498"
REFERENCES,0.6129707112970711,"0.0 to 4.0, both modulated by a cosine function over the time step (t). We apply these strategies to
499"
REFERENCES,0.6140167364016736,"the Ant Morphology and TF Bind 10 tasks, and the results are as follows:"
REFERENCES,0.6150627615062761,"Table 8: Results of Annealing Approaches.
Method
Ant Morphology
TF Bind 10
RGD
0.968
0.694
ω = 2.0
0.940
0.657
Increase
0.948
0.654
Decrease
0.924
0.647 500"
REFERENCES,0.6161087866108786,"The empirical results across both strategies illustrate their inferior performance compared to our
501"
REFERENCES,0.6171548117154811,"approach, thereby demonstrating the efficacy of our proposed method.
502"
REFERENCES,0.6182008368200836,"F
Hyperparameter Sensitivity Analysis
503"
REFERENCES,0.6192468619246861,"RGD’s performance is assessed under different settings of T, y, and η. We experiment with T values
504"
REFERENCES,0.6202928870292888,"of 500, 750, 1000, 1250, and 1500, with the default being T = 1000. For the condition ratio y/ymax,
505"
REFERENCES,0.6213389121338913,"we test values of 0.5, 1.0, 1.5, 2.0, and 2.5, considering 1.0 as the default. Similarly, for the learning
506"
REFERENCES,0.6223849372384938,"rate η, we explore values of 2.5e−3, 5.0e−3, 0.01, 0.02, and 0.04, with the default set to η = 0.01.
507"
REFERENCES,0.6234309623430963,"Results are normalized by comparing them with the performance obtained at default values.
508"
REFERENCES,0.6244769874476988,"As depicted in Figures 5, 6, and 7, RGD demonstrates considerable resilience to hyperparameter
509"
REFERENCES,0.6255230125523012,"variations. The Ant task, in particular, exhibits a more marked sensitivity, with a gradual enhancement
510"
REFERENCES,0.6265690376569037,"in performance as these hyperparameters are varied. The underlying reasons for this trend include:
511"
REFERENCES,0.6276150627615062,"(1) An increase in the number of diffusion steps (T) enhances the overall quality of the generated
512"
REFERENCES,0.6286610878661087,"samples. This improvement, in conjunction with more effective guidance from the trained proxy,
513"
REFERENCES,0.6297071129707112,"leads to better results. (2) Elevating the condition (y) enables the diffusion model to extend its reach
514"
REFERENCES,0.6307531380753139,"beyond the existing dataset, paving the way for superior design solutions. However, selecting an
515"
REFERENCES,0.6317991631799164,"optimal y can be challenging and may, as observed in the TFB10 task, sometimes lead to suboptimal
516"
REFERENCES,0.6328451882845189,"results. (3) A higher learning rate (η) integrates an enhanced guidance signal from the trained proxy,
517"
REFERENCES,0.6338912133891214,"contributing to improved performances.
518"
REFERENCES,0.6349372384937239,"In contrast, the discrete nature of the TFB10 task seems to endow it with a certain robustness
519"
REFERENCES,0.6359832635983264,"to variations in these hyperparameters, highlighting a distinct behavioral pattern in response to
520"
REFERENCES,0.6370292887029289,"hyperparameter adjustments.
521"
REFERENCES,0.6380753138075314,"500
750
1000
1250
1500
Diffusion step t 0.990 0.995 1.000 1.005 1.011"
REFERENCES,0.6391213389121339,Score ratio
REFERENCES,0.6401673640167364,"Ant
TF10"
REFERENCES,0.641213389121339,"Figure 5: The ratio of the
performance of our RGD
method with T to the per-
formance with T = 1000."
REFERENCES,0.6422594142259415,"0.5
1.0
1.5
2.0
2.5
Condition y/ymax 0.960"
REFERENCES,0.643305439330544,"1.000
1.004
1.008
1.012"
REFERENCES,0.6443514644351465,Score ratio
REFERENCES,0.645397489539749,"Ant
TF10"
REFERENCES,0.6464435146443515,"Figure 6: The ratio of the
performance of our RGD
method with y/ymax to the
performance with 1.0."
REFERENCES,0.647489539748954,"0.0025
0.005
0.01
0.02
0.04
Learning rate 0.985 0.995 1.005 1.015 1.020"
REFERENCES,0.6485355648535565,Score ratio
REFERENCES,0.649581589958159,"Ant
TF10"
REFERENCES,0.6506276150627615,"Figure 7: The ratio of the
performance of our RGD
method with η to the per-
formance with η = 0.01."
REFERENCES,0.6516736401673641,"G
Broader Impact and Limitation
522"
REFERENCES,0.6527196652719666,"Broader impact. Our research has the potential to significantly accelerate advancements in fields such
523"
REFERENCES,0.6537656903765691,"as new material development, biomedical innovation, and robotics technology. These advancements
524"
REFERENCES,0.6548117154811716,"could lead to breakthroughs with substantial positive societal impacts. However, we recognize that,
525"
REFERENCES,0.6558577405857741,"like any powerful tool, there are inherent risks associated with the misuse of this technology. One
526"
REFERENCES,0.6569037656903766,"concerning possibility is the exploitation of our optimization techniques to design objects or entities
527"
REFERENCES,0.6579497907949791,"for malicious purposes, including the creation of more efficient weaponry or harmful biological agents.
528"
REFERENCES,0.6589958158995816,"Given these potential risks, it is imperative to enforce strict safeguards and regulatory measures,
529"
REFERENCES,0.6600418410041841,"especially in areas where the misuse of technology could lead to significant ethical and societal harm.
530"
REFERENCES,0.6610878661087866,"The responsible application and governance of such technologies are crucial to ensuring that they
531"
REFERENCES,0.6621338912133892,"serve to benefit society as a whole.
532"
REFERENCES,0.6631799163179917,"Limitation. We recognize that the benchmarks utilized in our study may not fully capture the
533"
REFERENCES,0.6642259414225942,"complexities of more advanced applications, such as protein drug design, primarily due to our current
534"
REFERENCES,0.6652719665271967,"limitations in accessing wet-lab experimental setups. Moving forward, we aim to mitigate this
535"
REFERENCES,0.6663179916317992,"limitation by fostering partnerships with domain experts, which will enable us to apply our method
536"
REFERENCES,0.6673640167364017,"to more challenging and diverse problems. This direction not only promises to validate the efficacy
537"
REFERENCES,0.6684100418410042,"of our approach in more complex scenarios but also aligns with our commitment to pushing the
538"
REFERENCES,0.6694560669456067,"boundaries of what our technology can achieve.
539"
REFERENCES,0.6705020920502092,"NeurIPS Paper Checklist
540"
CLAIMS,0.6715481171548117,"1. Claims
541"
CLAIMS,0.6725941422594143,"Question: Do the main claims made in the abstract and introduction accurately reflect the
542"
CLAIMS,0.6736401673640168,"paper’s contributions and scope?
543"
CLAIMS,0.6746861924686193,"Answer: [Yes]
544"
CLAIMS,0.6757322175732218,"Justification: The abstract and introduction accurately reflect the paper’s contributions and
545"
CLAIMS,0.6767782426778243,"scope.
546"
CLAIMS,0.6778242677824268,"Guidelines:
547"
CLAIMS,0.6788702928870293,"• The answer NA means that the abstract and introduction do not include the claims
548"
CLAIMS,0.6799163179916318,"made in the paper.
549"
CLAIMS,0.6809623430962343,"• The abstract and/or introduction should clearly state the claims made, including the
550"
CLAIMS,0.6820083682008368,"contributions made in the paper and important assumptions and limitations. A No or
551"
CLAIMS,0.6830543933054394,"NA answer to this question will not be perceived well by the reviewers.
552"
CLAIMS,0.6841004184100419,"• The claims made should match theoretical and experimental results, and reflect how
553"
CLAIMS,0.6851464435146444,"much the results can be expected to generalize to other settings.
554"
CLAIMS,0.6861924686192469,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
555"
CLAIMS,0.6872384937238494,"are not attained by the paper.
556"
LIMITATIONS,0.6882845188284519,"2. Limitations
557"
LIMITATIONS,0.6893305439330544,"Question: Does the paper discuss the limitations of the work performed by the authors?
558"
LIMITATIONS,0.6903765690376569,"Answer: [Yes]
559"
LIMITATIONS,0.6914225941422594,"Justification: We discuss the limitations in Appendix G.
560"
LIMITATIONS,0.6924686192468619,"Guidelines:
561"
LIMITATIONS,0.6935146443514645,"• The answer NA means that the paper has no limitation while the answer No means that
562"
LIMITATIONS,0.694560669456067,"the paper has limitations, but those are not discussed in the paper.
563"
LIMITATIONS,0.6956066945606695,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
564"
LIMITATIONS,0.696652719665272,"• The paper should point out any strong assumptions and how robust the results are to
565"
LIMITATIONS,0.6976987447698745,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
566"
LIMITATIONS,0.698744769874477,"model well-specification, asymptotic approximations only holding locally). The authors
567"
LIMITATIONS,0.6997907949790795,"should reflect on how these assumptions might be violated in practice and what the
568"
LIMITATIONS,0.700836820083682,"implications would be.
569"
LIMITATIONS,0.7018828451882845,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
570"
LIMITATIONS,0.702928870292887,"only tested on a few datasets or with a few runs. In general, empirical results often
571"
LIMITATIONS,0.7039748953974896,"depend on implicit assumptions, which should be articulated.
572"
LIMITATIONS,0.7050209205020921,"• The authors should reflect on the factors that influence the performance of the approach.
573"
LIMITATIONS,0.7060669456066946,"For example, a facial recognition algorithm may perform poorly when image resolution
574"
LIMITATIONS,0.7071129707112971,"is low or images are taken in low lighting. Or a speech-to-text system might not be
575"
LIMITATIONS,0.7081589958158996,"used reliably to provide closed captions for online lectures because it fails to handle
576"
LIMITATIONS,0.7092050209205021,"technical jargon.
577"
LIMITATIONS,0.7102510460251046,"• The authors should discuss the computational efficiency of the proposed algorithms
578"
LIMITATIONS,0.7112970711297071,"and how they scale with dataset size.
579"
LIMITATIONS,0.7123430962343096,"• If applicable, the authors should discuss possible limitations of their approach to
580"
LIMITATIONS,0.7133891213389121,"address problems of privacy and fairness.
581"
LIMITATIONS,0.7144351464435147,"• While the authors might fear that complete honesty about limitations might be used by
582"
LIMITATIONS,0.7154811715481172,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
583"
LIMITATIONS,0.7165271966527197,"limitations that aren’t acknowledged in the paper. The authors should use their best
584"
LIMITATIONS,0.7175732217573222,"judgment and recognize that individual actions in favor of transparency play an impor-
585"
LIMITATIONS,0.7186192468619247,"tant role in developing norms that preserve the integrity of the community. Reviewers
586"
LIMITATIONS,0.7196652719665272,"will be specifically instructed to not penalize honesty concerning limitations.
587"
THEORY ASSUMPTIONS AND PROOFS,0.7207112970711297,"3. Theory Assumptions and Proofs
588"
THEORY ASSUMPTIONS AND PROOFS,0.7217573221757322,"Question: For each theoretical result, does the paper provide the full set of assumptions and
589"
THEORY ASSUMPTIONS AND PROOFS,0.7228033472803347,"a complete (and correct) proof?
590"
THEORY ASSUMPTIONS AND PROOFS,0.7238493723849372,"Answer: [NA]
591"
THEORY ASSUMPTIONS AND PROOFS,0.7248953974895398,"Justification: The paper does not include theoretical results.
592"
THEORY ASSUMPTIONS AND PROOFS,0.7259414225941423,"Guidelines:
593"
THEORY ASSUMPTIONS AND PROOFS,0.7269874476987448,"• The answer NA means that the paper does not include theoretical results.
594"
THEORY ASSUMPTIONS AND PROOFS,0.7280334728033473,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
595"
THEORY ASSUMPTIONS AND PROOFS,0.7290794979079498,"referenced.
596"
THEORY ASSUMPTIONS AND PROOFS,0.7301255230125523,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
597"
THEORY ASSUMPTIONS AND PROOFS,0.7311715481171548,"• The proofs can either appear in the main paper or the supplemental material, but if
598"
THEORY ASSUMPTIONS AND PROOFS,0.7322175732217573,"they appear in the supplemental material, the authors are encouraged to provide a short
599"
THEORY ASSUMPTIONS AND PROOFS,0.7332635983263598,"proof sketch to provide intuition.
600"
THEORY ASSUMPTIONS AND PROOFS,0.7343096234309623,"• Inversely, any informal proof provided in the core of the paper should be complemented
601"
THEORY ASSUMPTIONS AND PROOFS,0.7353556485355649,"by formal proofs provided in appendix or supplemental material.
602"
THEORY ASSUMPTIONS AND PROOFS,0.7364016736401674,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7374476987447699,"4. Experimental Result Reproducibility
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7384937238493724,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7395397489539749,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7405857740585774,"of the paper (regardless of whether the code and data are provided or not)?
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7416317991631799,"Answer: [Yes]
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7426778242677824,"Justification: We provide our code link in the abstract and detail our settings in Section 4.3.
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7437238493723849,"Guidelines:
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7447698744769874,"• The answer NA means that the paper does not include experiments.
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.74581589958159,"• If the paper includes experiments, a No answer to this question will not be perceived
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7468619246861925,"well by the reviewers: Making the paper reproducible is important, regardless of
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.747907949790795,"whether the code and data are provided or not.
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7489539748953975,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"to make their results reproducible or verifiable.
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7510460251046025,"• Depending on the contribution, reproducibility can be accomplished in various ways.
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.752092050209205,"For example, if the contribution is a novel architecture, describing the architecture fully
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7531380753138075,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75418410041841,"be necessary to either make it possible for others to replicate the model with the same
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7552301255230126,"dataset, or provide access to the model. In general. releasing code and data is often
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7562761506276151,"one good way to accomplish this, but reproducibility can also be provided via detailed
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7573221757322176,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7583682008368201,"of a large language model), releasing of a model checkpoint, or other means that are
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7594142259414226,"appropriate to the research performed.
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7604602510460251,"• While NeurIPS does not require releasing code, the conference does require all submis-
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7615062761506276,"sions to provide some reasonable avenue for reproducibility, which may depend on the
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7625523012552301,"nature of the contribution. For example
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7635983263598326,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7646443514644351,"to reproduce that algorithm.
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7656903765690377,"(b) If the contribution is primarily a new model architecture, the paper should describe
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7667364016736402,"the architecture clearly and fully.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7677824267782427,"(c) If the contribution is a new model (e.g., a large language model), then there should
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7688284518828452,"either be a way to access this model for reproducing the results or a way to reproduce
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7698744769874477,"the model (e.g., with an open-source dataset or instructions for how to construct
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7709205020920502,"the dataset).
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7719665271966527,"(d) We recognize that reproducibility may be tricky in some cases, in which case
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7730125523012552,"authors are welcome to describe the particular way they provide for reproducibility.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7740585774058577,"In the case of closed-source models, it may be that access to the model is limited in
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7751046025104602,"some way (e.g., to registered users), but it should be possible for other researchers
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7761506276150628,"to have some path to reproducing or verifying the results.
641"
OPEN ACCESS TO DATA AND CODE,0.7771966527196653,"5. Open access to data and code
642"
OPEN ACCESS TO DATA AND CODE,0.7782426778242678,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
643"
OPEN ACCESS TO DATA AND CODE,0.7792887029288703,"tions to faithfully reproduce the main experimental results, as described in supplemental
644"
OPEN ACCESS TO DATA AND CODE,0.7803347280334728,"material?
645"
OPEN ACCESS TO DATA AND CODE,0.7813807531380753,"Answer: [Yes]
646"
OPEN ACCESS TO DATA AND CODE,0.7824267782426778,"Justification: We provide a link to our source code in the abstract and thoroughly describe
647"
OPEN ACCESS TO DATA AND CODE,0.7834728033472803,"our experimental settings in Section 4.3.
648"
OPEN ACCESS TO DATA AND CODE,0.7845188284518828,"Guidelines:
649"
OPEN ACCESS TO DATA AND CODE,0.7855648535564853,"• The answer NA means that paper does not include experiments requiring code.
650"
OPEN ACCESS TO DATA AND CODE,0.7866108786610879,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
651"
OPEN ACCESS TO DATA AND CODE,0.7876569037656904,"public/guides/CodeSubmissionPolicy) for more details.
652"
OPEN ACCESS TO DATA AND CODE,0.7887029288702929,"• While we encourage the release of code and data, we understand that this might not be
653"
OPEN ACCESS TO DATA AND CODE,0.7897489539748954,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
654"
OPEN ACCESS TO DATA AND CODE,0.7907949790794979,"including code, unless this is central to the contribution (e.g., for a new open-source
655"
OPEN ACCESS TO DATA AND CODE,0.7918410041841004,"benchmark).
656"
OPEN ACCESS TO DATA AND CODE,0.7928870292887029,"• The instructions should contain the exact command and environment needed to run to
657"
OPEN ACCESS TO DATA AND CODE,0.7939330543933054,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
658"
OPEN ACCESS TO DATA AND CODE,0.7949790794979079,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
659"
OPEN ACCESS TO DATA AND CODE,0.7960251046025104,"• The authors should provide instructions on data access and preparation, including how
660"
OPEN ACCESS TO DATA AND CODE,0.797071129707113,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
661"
OPEN ACCESS TO DATA AND CODE,0.7981171548117155,"• The authors should provide scripts to reproduce all experimental results for the new
662"
OPEN ACCESS TO DATA AND CODE,0.799163179916318,"proposed method and baselines. If only a subset of experiments are reproducible, they
663"
OPEN ACCESS TO DATA AND CODE,0.8002092050209205,"should state which ones are omitted from the script and why.
664"
OPEN ACCESS TO DATA AND CODE,0.801255230125523,"• At submission time, to preserve anonymity, the authors should release anonymized
665"
OPEN ACCESS TO DATA AND CODE,0.8023012552301255,"versions (if applicable).
666"
OPEN ACCESS TO DATA AND CODE,0.803347280334728,"• Providing as much information as possible in supplemental material (appended to the
667"
OPEN ACCESS TO DATA AND CODE,0.8043933054393305,"paper) is recommended, but including URLs to data and code is permitted.
668"
OPEN ACCESS TO DATA AND CODE,0.805439330543933,"6. Experimental Setting/Details
669"
OPEN ACCESS TO DATA AND CODE,0.8064853556485355,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
670"
OPEN ACCESS TO DATA AND CODE,0.8075313807531381,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
671"
OPEN ACCESS TO DATA AND CODE,0.8085774058577406,"results?
672"
OPEN ACCESS TO DATA AND CODE,0.8096234309623431,"Answer: [Yes]
673"
OPEN ACCESS TO DATA AND CODE,0.8106694560669456,"Justification: We detail our setting in Section 4.3 and also discuss hyperparameter sensitivity
674"
OPEN ACCESS TO DATA AND CODE,0.8117154811715481,"in Appendix F.
675"
OPEN ACCESS TO DATA AND CODE,0.8127615062761506,"Guidelines:
676"
OPEN ACCESS TO DATA AND CODE,0.8138075313807531,"• The answer NA means that the paper does not include experiments.
677"
OPEN ACCESS TO DATA AND CODE,0.8148535564853556,"• The experimental setting should be presented in the core of the paper to a level of detail
678"
OPEN ACCESS TO DATA AND CODE,0.8158995815899581,"that is necessary to appreciate the results and make sense of them.
679"
OPEN ACCESS TO DATA AND CODE,0.8169456066945606,"• The full details can be provided either with the code, in appendix, or as supplemental
680"
OPEN ACCESS TO DATA AND CODE,0.8179916317991632,"material.
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8190376569037657,"7. Experiment Statistical Significance
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8200836820083682,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8211297071129707,"information about the statistical significance of the experiments?
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8221757322175732,"Answer: [Yes]
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8232217573221757,"Justification: To ensure reliability and consistency in our comparative analysis, each experi-
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8242677824267782,"mental setting was replicated across 8 independent runs, unless stated otherwise, with the
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8253138075313807,"presentation of both mean values and standard errors.
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8263598326359832,"Guidelines:
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8274058577405857,"• The answer NA means that the paper does not include experiments.
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8284518828451883,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8294979079497908,"dence intervals, or statistical significance tests, at least for the experiments that support
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8305439330543933,"the main claims of the paper.
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8315899581589958,"• The factors of variability that the error bars are capturing should be clearly stated (for
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326359832635983,"example, train/test split, initialization, random drawing of some parameter, or overall
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336820083682008,"run with given experimental conditions).
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8347280334728033,"• The method for calculating the error bars should be explained (closed form formula,
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8357740585774058,"call to a library function, bootstrap, etc.)
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8368200836820083,"• The assumptions made should be given (e.g., Normally distributed errors).
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8378661087866108,"• It should be clear whether the error bar is the standard deviation or the standard error
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8389121338912134,"of the mean.
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8399581589958159,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8410041841004184,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8420502092050209,"of Normality of errors is not verified.
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8430962343096234,"• For asymmetric distributions, the authors should be careful not to show in tables or
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8441422594142259,"figures symmetric error bars that would yield results that are out of range (e.g. negative
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8451882845188284,"error rates).
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8462343096234309,"• If error bars are reported in tables or plots, The authors should explain in the text how
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8472803347280334,"they were calculated and reference the corresponding figures or tables in the text.
709"
EXPERIMENTS COMPUTE RESOURCES,0.8483263598326359,"8. Experiments Compute Resources
710"
EXPERIMENTS COMPUTE RESOURCES,0.8493723849372385,"Question: For each experiment, does the paper provide sufficient information on the com-
711"
EXPERIMENTS COMPUTE RESOURCES,0.850418410041841,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
712"
EXPERIMENTS COMPUTE RESOURCES,0.8514644351464435,"the experiments?
713"
EXPERIMENTS COMPUTE RESOURCES,0.852510460251046,"Answer: [Yes]
714"
EXPERIMENTS COMPUTE RESOURCES,0.8535564853556485,"Justification: We have discussed these in Section 4.3.
715"
EXPERIMENTS COMPUTE RESOURCES,0.854602510460251,"Guidelines:
716"
EXPERIMENTS COMPUTE RESOURCES,0.8556485355648535,"• The answer NA means that the paper does not include experiments.
717"
EXPERIMENTS COMPUTE RESOURCES,0.856694560669456,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
718"
EXPERIMENTS COMPUTE RESOURCES,0.8577405857740585,"or cloud provider, including relevant memory and storage.
719"
EXPERIMENTS COMPUTE RESOURCES,0.858786610878661,"• The paper should provide the amount of compute required for each of the individual
720"
EXPERIMENTS COMPUTE RESOURCES,0.8598326359832636,"experimental runs as well as estimate the total compute.
721"
EXPERIMENTS COMPUTE RESOURCES,0.8608786610878661,"• The paper should disclose whether the full research project required more compute
722"
EXPERIMENTS COMPUTE RESOURCES,0.8619246861924686,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
723"
EXPERIMENTS COMPUTE RESOURCES,0.8629707112970711,"didn’t make it into the paper).
724"
CODE OF ETHICS,0.8640167364016736,"9. Code Of Ethics
725"
CODE OF ETHICS,0.8650627615062761,"Question: Does the research conducted in the paper conform, in every respect, with the
726"
CODE OF ETHICS,0.8661087866108786,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
727"
CODE OF ETHICS,0.8671548117154811,"Answer: [Yes]
728"
CODE OF ETHICS,0.8682008368200836,"Justification: We preserve anonymity and conform with the NeurIPS Code of Ethics.
729"
CODE OF ETHICS,0.8692468619246861,"Guidelines:
730"
CODE OF ETHICS,0.8702928870292888,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
731"
CODE OF ETHICS,0.8713389121338913,"• If the authors answer No, they should explain the special circumstances that require a
732"
CODE OF ETHICS,0.8723849372384938,"deviation from the Code of Ethics.
733"
CODE OF ETHICS,0.8734309623430963,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
734"
CODE OF ETHICS,0.8744769874476988,"eration due to laws or regulations in their jurisdiction).
735"
BROADER IMPACTS,0.8755230125523012,"10. Broader Impacts
736"
BROADER IMPACTS,0.8765690376569037,"Question: Does the paper discuss both potential positive societal impacts and negative
737"
BROADER IMPACTS,0.8776150627615062,"societal impacts of the work performed?
738"
BROADER IMPACTS,0.8786610878661087,"Answer: [Yes]
739"
BROADER IMPACTS,0.8797071129707112,"Justification: We discuss both potential positive and negative impacts in Appendix G.
740"
BROADER IMPACTS,0.8807531380753139,"Guidelines:
741"
BROADER IMPACTS,0.8817991631799164,"• The answer NA means that there is no societal impact of the work performed.
742"
BROADER IMPACTS,0.8828451882845189,"• If the authors answer NA or No, they should explain why their work has no societal
743"
BROADER IMPACTS,0.8838912133891214,"impact or why the paper does not address societal impact.
744"
BROADER IMPACTS,0.8849372384937239,"• Examples of negative societal impacts include potential malicious or unintended uses
745"
BROADER IMPACTS,0.8859832635983264,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
746"
BROADER IMPACTS,0.8870292887029289,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
747"
BROADER IMPACTS,0.8880753138075314,"groups), privacy considerations, and security considerations.
748"
BROADER IMPACTS,0.8891213389121339,"• The conference expects that many papers will be foundational research and not tied
749"
BROADER IMPACTS,0.8901673640167364,"to particular applications, let alone deployments. However, if there is a direct path to
750"
BROADER IMPACTS,0.891213389121339,"any negative applications, the authors should point it out. For example, it is legitimate
751"
BROADER IMPACTS,0.8922594142259415,"to point out that an improvement in the quality of generative models could be used to
752"
BROADER IMPACTS,0.893305439330544,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
753"
BROADER IMPACTS,0.8943514644351465,"that a generic algorithm for optimizing neural networks could enable people to train
754"
BROADER IMPACTS,0.895397489539749,"models that generate Deepfakes faster.
755"
BROADER IMPACTS,0.8964435146443515,"• The authors should consider possible harms that could arise when the technology is
756"
BROADER IMPACTS,0.897489539748954,"being used as intended and functioning correctly, harms that could arise when the
757"
BROADER IMPACTS,0.8985355648535565,"technology is being used as intended but gives incorrect results, and harms following
758"
BROADER IMPACTS,0.899581589958159,"from (intentional or unintentional) misuse of the technology.
759"
BROADER IMPACTS,0.9006276150627615,"• If there are negative societal impacts, the authors could also discuss possible mitigation
760"
BROADER IMPACTS,0.9016736401673641,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
761"
BROADER IMPACTS,0.9027196652719666,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
762"
BROADER IMPACTS,0.9037656903765691,"feedback over time, improving the efficiency and accessibility of ML).
763"
SAFEGUARDS,0.9048117154811716,"11. Safeguards
764"
SAFEGUARDS,0.9058577405857741,"Question: Does the paper describe safeguards that have been put in place for responsible
765"
SAFEGUARDS,0.9069037656903766,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
766"
SAFEGUARDS,0.9079497907949791,"image generators, or scraped datasets)?
767"
SAFEGUARDS,0.9089958158995816,"Answer: [NA]
768"
SAFEGUARDS,0.9100418410041841,"Justification: We do not release any datasets nor pre-trained models.
769"
SAFEGUARDS,0.9110878661087866,"Guidelines:
770"
SAFEGUARDS,0.9121338912133892,"• The answer NA means that the paper poses no such risks.
771"
SAFEGUARDS,0.9131799163179917,"• Released models that have a high risk for misuse or dual-use should be released with
772"
SAFEGUARDS,0.9142259414225942,"necessary safeguards to allow for controlled use of the model, for example by requiring
773"
SAFEGUARDS,0.9152719665271967,"that users adhere to usage guidelines or restrictions to access the model or implementing
774"
SAFEGUARDS,0.9163179916317992,"safety filters.
775"
SAFEGUARDS,0.9173640167364017,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
776"
SAFEGUARDS,0.9184100418410042,"should describe how they avoided releasing unsafe images.
777"
SAFEGUARDS,0.9194560669456067,"• We recognize that providing effective safeguards is challenging, and many papers do
778"
SAFEGUARDS,0.9205020920502092,"not require this, but we encourage authors to take this into account and make a best
779"
SAFEGUARDS,0.9215481171548117,"faith effort.
780"
LICENSES FOR EXISTING ASSETS,0.9225941422594143,"12. Licenses for existing assets
781"
LICENSES FOR EXISTING ASSETS,0.9236401673640168,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
782"
LICENSES FOR EXISTING ASSETS,0.9246861924686193,"the paper, properly credited and are the license and terms of use explicitly mentioned and
783"
LICENSES FOR EXISTING ASSETS,0.9257322175732218,"properly respected?
784"
LICENSES FOR EXISTING ASSETS,0.9267782426778243,"Answer: [Yes]
785"
LICENSES FOR EXISTING ASSETS,0.9278242677824268,"Justification: We have duly credited all utilized assets and adhered to their respective licenses
786"
LICENSES FOR EXISTING ASSETS,0.9288702928870293,"and terms of use.
787"
LICENSES FOR EXISTING ASSETS,0.9299163179916318,"Guidelines:
788"
LICENSES FOR EXISTING ASSETS,0.9309623430962343,"• The answer NA means that the paper does not use existing assets.
789"
LICENSES FOR EXISTING ASSETS,0.9320083682008368,"• The authors should cite the original paper that produced the code package or dataset.
790"
LICENSES FOR EXISTING ASSETS,0.9330543933054394,"• The authors should state which version of the asset is used and, if possible, include a
791"
LICENSES FOR EXISTING ASSETS,0.9341004184100419,"URL.
792"
LICENSES FOR EXISTING ASSETS,0.9351464435146444,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
793"
LICENSES FOR EXISTING ASSETS,0.9361924686192469,"• For scraped data from a particular source (e.g., website), the copyright and terms of
794"
LICENSES FOR EXISTING ASSETS,0.9372384937238494,"service of that source should be provided.
795"
LICENSES FOR EXISTING ASSETS,0.9382845188284519,"• If assets are released, the license, copyright information, and terms of use in the
796"
LICENSES FOR EXISTING ASSETS,0.9393305439330544,"package should be provided. For popular datasets, paperswithcode.com/datasets
797"
LICENSES FOR EXISTING ASSETS,0.9403765690376569,"has curated licenses for some datasets. Their licensing guide can help determine the
798"
LICENSES FOR EXISTING ASSETS,0.9414225941422594,"license of a dataset.
799"
LICENSES FOR EXISTING ASSETS,0.9424686192468619,"• For existing datasets that are re-packaged, both the original license and the license of
800"
LICENSES FOR EXISTING ASSETS,0.9435146443514645,"the derived asset (if it has changed) should be provided.
801"
LICENSES FOR EXISTING ASSETS,0.944560669456067,"• If this information is not available online, the authors are encouraged to reach out to
802"
LICENSES FOR EXISTING ASSETS,0.9456066945606695,"the asset’s creators.
803"
NEW ASSETS,0.946652719665272,"13. New Assets
804"
NEW ASSETS,0.9476987447698745,"Question: Are new assets introduced in the paper well documented and is the documentation
805"
NEW ASSETS,0.948744769874477,"provided alongside the assets?
806"
NEW ASSETS,0.9497907949790795,"Answer: [Yes]
807"
NEW ASSETS,0.950836820083682,"Justification: We plan to open-source our code and have ensured thorough documentation of
808"
NEW ASSETS,0.9518828451882845,"the code.
809"
NEW ASSETS,0.952928870292887,"Guidelines:
810"
NEW ASSETS,0.9539748953974896,"• The answer NA means that the paper does not release new assets.
811"
NEW ASSETS,0.9550209205020921,"• Researchers should communicate the details of the dataset/code/model as part of their
812"
NEW ASSETS,0.9560669456066946,"submissions via structured templates. This includes details about training, license,
813"
NEW ASSETS,0.9571129707112971,"limitations, etc.
814"
NEW ASSETS,0.9581589958158996,"• The paper should discuss whether and how consent was obtained from people whose
815"
NEW ASSETS,0.9592050209205021,"asset is used.
816"
NEW ASSETS,0.9602510460251046,"• At submission time, remember to anonymize your assets (if applicable). You can either
817"
NEW ASSETS,0.9612970711297071,"create an anonymized URL or include an anonymized zip file.
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9623430962343096,"14. Crowdsourcing and Research with Human Subjects
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9633891213389121,"Question: For crowdsourcing experiments and research with human subjects, does the paper
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9644351464435147,"include the full text of instructions given to participants and screenshots, if applicable, as
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9654811715481172,"well as details about compensation (if any)?
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9665271966527197,"Answer: [NA]
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675732217573222,"Justification: This paper does not engage in crowdsourcing or involve studies with human
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686192468619247,"participants.
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696652719665272,"Guidelines:
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9707112970711297,"• The answer NA means that the paper does not involve crowdsourcing nor research with
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717573221757322,"human subjects.
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728033472803347,"• Including this information in the supplemental material is fine, but if the main contribu-
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9738493723849372,"tion of the paper involves human subjects, then as much detail as possible should be
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9748953974895398,"included in the main paper.
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9759414225941423,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9769874476987448,"or other labor should be paid at least the minimum wage in the country of the data
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780334728033473,"collector.
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9790794979079498,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801255230125523,"Subjects
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811715481171548,"Question: Does the paper describe potential risks incurred by study participants, whether
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822175732217573,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832635983263598,"approvals (or an equivalent approval/review based on the requirements of your country or
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843096234309623,"institution) were obtained?
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853556485355649,"Answer: [NA]
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864016736401674,"Justification: This paper does not engage in crowdsourcing or research involving human
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9874476987447699,"subjects.
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884937238493724,"Guidelines:
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9895397489539749,"• The answer NA means that the paper does not involve crowdsourcing nor research with
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905857740585774,"human subjects.
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916317991631799,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9926778242677824,"may be required for any human subjects research. If you obtained IRB approval, you
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937238493723849,"should clearly state this in the paper.
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9947698744769874,"• We recognize that the procedures for this may vary significantly between institutions
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.99581589958159,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968619246861925,"guidelines for their institution.
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997907949790795,"• For initial submissions, do not include any information that would break anonymity (if
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989539748953975,"applicable), such as the institution conducting the review.
854"
