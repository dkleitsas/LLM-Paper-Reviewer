Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0027624309392265192,"Despite the recent advancements in offline reinforcement learning via supervised
learning (RvS) and the success of the decision transformer (DT) architecture in
various domains, DTs have fallen short in several challenging benchmarks. The
root cause of this underperformance lies in their inability to seamlessly connect
segments of suboptimal trajectories. To overcome this limitation, we present a novel
approach to enhance RvS methods by integrating intermediate targets. We introduce
the Waypoint Transformer (WT), using an architecture that builds upon the DT
framework and conditioned on automatically-generated waypoints. The results
show a significant increase in the final return compared to existing RvS methods,
with performance on par or greater than existing popular temporal difference
learning-based methods. Additionally, the performance and stability improvements
are largest in the most challenging environments and data configurations, including
AntMaze Large Play/Diverse and Kitchen Mixed/Partial."
INTRODUCTION,0.0055248618784530384,"1
Introduction"
INTRODUCTION,0.008287292817679558,"Traditionally, offline reinforcement learning (RL) methods that compete with state-of-the-art (SOTA)
algorithms have relied on objectives encouraging pessimism in combination with value-based methods.
Notable examples of this approach include Batch Conservative Q-Learning (BCQ), Conservative
Q-Learning (CQL), and Pessimistic Q-Learning (PQL) [Fujimoto et al., 2019, Kumar et al., 2020,
Liu et al., 2020]. However, these methods can be challenging to train and often require intricate
hyperparameter tuning and various tricks to ensure stability and optimal performance across tasks."
INTRODUCTION,0.011049723756906077,"Reinforcement learning via supervised learning (RvS) has emerged as a simpler alternative to
traditional offline RL methods [Emmons et al., 2021]. RvS approaches are based on behavioral
cloning (BC), either conditional or non-conditional, to train a policy. Importantly, these methods
eliminate the need for any temporal-difference (TD) learning, such as fitted value or action-value
functions. This results in a simpler algorithmic framework based on supervised learning, allowing
for progress in offline RL to build upon work in supervised learning. There are several successful
applications of RvS methods, including methods conditioned on goals and returns [Kumar et al.,
2019, Janner et al., 2021, Ding et al., 2019, Chen et al., 2021, Emmons et al., 2021]."
INTRODUCTION,0.013812154696132596,"However, RvS methods have typically struggled in tasks where seamlessly connecting (or ""stitching"")
appropriate segments of suboptimal training trajectories is critical for success [Kumar et al., 2022]. For
example, when tasked with reaching specific locations in the AntMaze maze navigation environment
or completing a series of tasks in the FrankaKitchen environment, RvS methods typically perform
significantly worse than TD learning methods such as Implicit Q-Learning [Fu et al., 2020, Kostrikov
et al., 2021]."
INTRODUCTION,0.016574585635359115,"In this study, we leverage the transformer architecture [Vaswani et al., 2017] to construct an RvS
method. As introduced by Chen et al. [2021], the decision transformer (DT) can perform conditional
behavioral cloning in the context of offline RL. However, similar to other RvS methods, DT proves
inferior in performance across popular Gym-MuJoCo benchmarks compared to other value-based
offline RL methods, with a 15% relative reduction in average return and lowered stability (Table 1)."
INTRODUCTION,0.019337016574585635,"To tackle these limitations of existing RvS methods, we introduce a waypoint generation technique
that produces intermediate goals and more stable, proxy rewards, which serve as guidance to steer a
policy to desirable outcomes. By conditioning a transformer-based RvS method on these generated
targets, we obtain a trained policy that learns to follow them, leading to improved performance and
stability compared to prior offline RL methods. The highlights of our proposed approach are as
follows:"
INTRODUCTION,0.022099447513812154,"• We propose a novel RvS method, Waypoint Transformer, using waypoint generation net-
works and establish new state-of-the-art performance, in challenging tasks such as AntMaze
Large and Kitchen Partial/Mixed [Fu et al., 2020] (Table 1). On tasks from Gym-MuJoCo,
our method rivals the performance of TD learning-based methods such as Implicit Q-
Learning and Conservative Q-Learning [Kostrikov et al., 2021, Kumar et al., 2020], and
improve over existing RvS methods."
INTRODUCTION,0.024861878453038673,"• We motivate the benefit of conditioning RvS on intermediate targets using a chain-MDP
example and an empirical analysis of maze navigation tasks. By providing such additional
guidance on suboptimal datasets, we show that a policy optimized with a behavioral cloning
objective chooses more optimal actions compared to conditioning on fixed targets (as in
Chen et al. [2021], Emmons et al. [2021]), facilitating improved stitching capability."
INTRODUCTION,0.027624309392265192,"• Our work also provides practical insights for improving RvS, such as significantly reducing
training time, solving the hyperparameter tuning challenge in RvS posed by Emmons et al.
[2021], and notably improved stability in performance across runs."
RELATED WORK,0.03038674033149171,"2
Related Work"
RELATED WORK,0.03314917127071823,"Many recent offline RL methods have used fitted value or action-value functions [Liu et al., 2020,
Fujimoto et al., 2019, Kostrikov et al., 2021, Kumar et al., 2020, Kidambi et al., 2020, Lyu et al.,
2022] or model-based approaches leveraging estimation of dynamics [Kidambi et al., 2020, Yu et al.,
2020, Argenson and Dulac-Arnold, 2020, Shen et al., 2021, Rigter et al., 2022, Zhan et al., 2021]."
RELATED WORK,0.03591160220994475,"RvS, as introduced in Emmons et al. [2021], avoids fitting value functions and instead leverages
behavioral cloning. In many RvS-style methods, the conditioning variable for the policy is based
on the return [Kumar et al., 2019, Srivastava et al., 2019, Schmidhuber, 2019, Chen et al., 2021],
but other methods use goal-conditioning [Nair et al., 2018, Emmons et al., 2021, Ding et al., 2019,
Ghosh et al., 2019] or leverage inverse RL [Eysenbach et al., 2020]. Recent work by Brandfonbrener
et al. [2022] has explored the limitations of reward conditioning in RvS. In this study, we consider
both reward and goal-conditioning."
RELATED WORK,0.03867403314917127,"Transformers have demonstrated the ability to generalize to a vast array of tasks, such as language
modeling, image generation, and representation learning [Vaswani et al., 2017, Devlin et al., 2018,
He et al., 2022, Parmar et al., 2018]. In the context of offline RL, decision transformers (DT) leverage
a causal transformer architecture to fit a reward-conditioned policy [Chen et al., 2021]. Similarly,
[Janner et al., 2021] frame offline RL as a sequence modeling problem and introduce the Trajectory
Transformer, a model-based offline RL approach that uses the transformer architecture."
RELATED WORK,0.04143646408839779,"Algorithms building upon the DT, such as online DT [Zheng et al., 2022], prompt DT [Xu et al.,
2022] and Q-Learning DT [Yamagata et al., 2022], have extended the scope of DT’s usage. Furuta
et al. [2021] introduce a framework for hindsight information matching algorithms to unify several
hindsight-based algorithms, such as Hindsight Experience Replay [Andrychowicz et al., 2017], DT,
TT, and our proposed method."
RELATED WORK,0.04419889502762431,"Some critical issues with DT unresolved by existing work are (a) its instability (i.e., large variability
across initialization seeds) for some tasks in the offline setting (Table 1) and (b) its relatively poor
performance on some tasks due to an inability to stitch segments of suboptimal trajectories [Kumar
et al., 2022] – in such settings, RvS methods are outperformed by value-based methods such as"
RELATED WORK,0.04696132596685083,"Implicit Q-Learning (IQL) [Kostrikov et al., 2021] and Conservative Q-Learning (CQL) [Kumar
et al., 2020]. We address both these concerns with our proposed approach, demonstrating notably
improved performance and reduced variance across seeds for tasks compared to DT and prior RvS
methods (Table 1)."
RELATED WORK,0.049723756906077346,"One of the areas of further research in RvS, per Emmons et al. [2021], is to address the complex
and unreliable process of tuning hyperparameters, as studied in Zhang and Jiang [2021] and Nie
et al. [2022]. We demonstrate that our method displays low sensitivity to changes in hyperparameters
compared to Emmons et al. [2021] (Table 2). Further, all experiments involving our proposed method
use the same set of hyperparameters in achieving SOTA performance across many tasks (Table 1)."
PRELIMINARIES,0.052486187845303865,"3
Preliminaries"
PRELIMINARIES,0.055248618784530384,"We assume that there exists an agent interacting with a Markov decision process (MDP) with states
st ∈S and actions at ∈A with unknown transition dynamics p(st+1 | st, at) and initial state
distribution p(s0). The agent chooses an action sampled from a transformer policy at ∼πθ(at |
st−k..t, Φt−k..t), parameterized by θ and conditioned on the known history of states st−k..t and a
conditioning variable Φt−k..t. Compared to the standard RL framework, where the policy is modeled
by π(at | st), we leverage a policy that considers past states within a fixed context window k."
PRELIMINARIES,0.058011049723756904,"The conditioning variable Φt is a specification of a goal or reward based on a target outcome ω. At
training time, ω is sampled from the data, as presented in Emmons et al. [2021]. At test time, we
assume that we can either generate or are provided global goal information ω for goal-conditioned
tasks. For reward-conditioned tasks, we specify a target return ω, following Chen et al. [2021]."
PRELIMINARIES,0.06077348066298342,"To train our RvS-based algorithm on an offline dataset D consisting of trajectories τ with conditioning
variable Φt, we compute the output of an autoregressive transformer model based on past and current
states and conditioning variable provided in each trajectory. Using a negative log-likelihood loss, we
use gradient descent to update the policy πθ. This procedure is summarized in Algorithm 1."
PRELIMINARIES,0.06353591160220995,Algorithm 1 Training algorithm for transformer-based policy trained on offline dataset D.
PRELIMINARIES,0.06629834254143646,"Input: Training dataset D = {τ1, τ2, τ3, ..., τn} of training trajectories.
for each τ = (s0, a0, Φ0, s1, a1, Φ1, ...) in D do"
PRELIMINARIES,0.06906077348066299,"Compute πθ(at | st−k..t, Φt−k..t) for all t
Calculate Lθ(τ) = −P"
PRELIMINARIES,0.0718232044198895,"t log πθ(at | st−k..t, Φt−k..t)
Backpropagate gradients w.r.t. Lθ(τ) to update model parameters
end for"
WAYPOINT GENERATION,0.07458563535911603,"4
Waypoint Generation"
WAYPOINT GENERATION,0.07734806629834254,"In this section, we propose using intermediate targets (or waypoints) as conditioning variables as
an alternative to fixed targets, proposed in Emmons et al. [2021]. Below, we motivate the necessity
for waypoints in RvS and present a practical technique to generate waypoints that can be used for
goal-conditioned (Section 4.2) and reward-conditioned tasks (Section 4.3) respectively."
ILLUSTRATIVE EXAMPLE,0.08011049723756906,"4.1
Illustrative Example"
ILLUSTRATIVE EXAMPLE,0.08287292817679558,"s(0)
s(h)
s(H)
a(2)
a(1)
a(1)"
ILLUSTRATIVE EXAMPLE,0.0856353591160221,"a(2)
a(2)
a(2) …
…"
ILLUSTRATIVE EXAMPLE,0.08839779005524862,"Figure 1: Chain MDP to motivate the benefit of inter-
mediate goals for conditional BC-based policy training."
ILLUSTRATIVE EXAMPLE,0.09116022099447514,"To motivate the benefits of using waypoints,
we consider an infinite-horizon, determinis-
tic MDP with H+1 states and two possible
actions at non-terminal states. A graphi-
cal representation of the MDP is shown in
Figure 1. For this scenario, we consider
the goal-conditioned setting where the tar-
get goal state during train and test time is
ω = s(H), and the episode terminates once we reach ω."
ILLUSTRATIVE EXAMPLE,0.09392265193370165,"In offline RL, the data is often suboptimal for achieving the desired goal during testing. In this
example, suppose we have access to a dataset D that contains an infinite number of trajectories"
ILLUSTRATIVE EXAMPLE,0.09668508287292818,"collected by a random behavioral policy πb where πb(at = a(1) | st) = λ > 0 for all st. Clearly, πb
is suboptimal with respect to reaching ω in the least number of timesteps; in expectation, it takes
H
1−λ timesteps to reach s(H) instead of H (optimal) since the agent ""stalls"" at the current state with
probability λ and moves to the next state with probability 1 −λ."
ILLUSTRATIVE EXAMPLE,0.09944751381215469,"Consider a global goal-conditioned policy πG(at | st, ω) that is optimized using a behavioral cloning
objective on D. Clearly, the optimal policy π∗
G(at | st, ω) = πb(at | st) ∀st since ω = s(H) is a
constant. Hence, the global goal-conditioned policy π∗
G is as suboptimal as the behavioral policy πb."
ILLUSTRATIVE EXAMPLE,0.10220994475138122,"Instead, suppose that we condition a policy πW (at | st, Φt) on an intermediate goal state Φt = st+K
for some chosen K <
1
1−λ (expected timesteps before πb executes a2), optimized using a behavioral
cloning objective on D. For simplicity, suppose our target intermediate goal state Φt for some current
state st = s(h) is simply the next state Φt = s(h+1). Based on data D from πb, the probability of
taking action a(2) conditioned on the chosen Φt and st is estimated as:"
ILLUSTRATIVE EXAMPLE,0.10497237569060773,"Prπb[at = a(2) | st = s(h), st+K = s(h+1)] = Prπb[at = a(2), st+K = s(h+1) | st = s(h)]"
ILLUSTRATIVE EXAMPLE,0.10773480662983426,Prπb[st+K = s(h+1) | st = s(h)]
ILLUSTRATIVE EXAMPLE,0.11049723756906077,"=
(1 −λ)λK−1
 K
1

[(1 −λ)λK−1]
=
1
 K
1
 = 1 K ."
ILLUSTRATIVE EXAMPLE,0.1132596685082873,"Hence, for the optimal intermediate goal-conditioned policy π∗
W trained on D, the probability of
choosing the optimal action a(2) is:"
ILLUSTRATIVE EXAMPLE,0.11602209944751381,"π∗
W (at = a(2) | st = s(h), Φt = s(h+1)) = 1 K ."
ILLUSTRATIVE EXAMPLE,0.11878453038674033,"Since π∗
G(at = a(2) | st = s(h), ω) = 1 −λ and we choose K such that 1"
ILLUSTRATIVE EXAMPLE,0.12154696132596685,"K > 1 −λ, we conclude:
π∗
W (at = a(2) | st, Φt) > π∗
G(at = a(2) | st, ω)."
ILLUSTRATIVE EXAMPLE,0.12430939226519337,"The complete derivation is presented in Appendix A. Based on this example, conditioning the actions
on reaching a desirable intermediate state is more likely to result in taking the optimal action compared
to a global goal-conditioned policy. Effectively, the conditioning acts as a ""guide"" for the policy,
directing it toward desirable intermediate targets in order to reach the global goal."
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.1270718232044199,"4.2
Intermediate Goal Generation for Spatial Compositionality"
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.1298342541436464,"In this section, we address RvS’s inability to ""stitch"" subsequences of suboptimal trajectories in order
to achieve optimal behaviour, based on analyses in Kumar et al. [2022]. In that pursuit, we introduce
a technique to generate effective intermediate targets to better facilitate stitching and to guide the
policy towards desirable outcomes, focusing primarily on goal-conditioned tasks."
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.13259668508287292,"start
region"
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.13535911602209943,stitching
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.13812154696132597,region
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.1408839779005525,"target
region"
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.143646408839779,"Figure 2: antmaze-large-play-v2 task to
navigate from the start location (circle) to the
target location (star). Blue and red colored
lines are training trajectories passing through
the start or end locations respectively."
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.1464088397790055,"Critically, the ability to stitch requires considering
experiences that are more relevant to achieving ap-
propriate short-term goals before reaching the global
goal. To illustrate this, we show a maze navigation
task from the AntMaze Large environment in Figure
2, where the evaluation objective is to reach a target
location from the start location [Fu et al., 2020]."
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.14917127071823205,"Analyzing the training trajectories that pass through
either the start location (blue) or target location (red),
less than 5% of trajectories extend beyond the stitch-
ing region into the other region, i.e., the target or start
regions respectively. Since trajectories seldom pass
through both the start and target regions, the policy
must ""stitch"" together subsequences from the blue
and red trajectories within the stitching region, where
the trajectories overlap the most. By providing inter-
mediate targets within this region, rather than condi-
tioning solely on the global goal, we can guide the
policy to connect the relevant subsequences needed
to reach the target effectively."
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.15193370165745856,"To obtain effective intermediate targets, we propose the goal waypoint network, explicitly designed to
generate short-term, intermediate goals. Similar to the illustrative example in Section 4.1, the purpose
of these intermediate targets is to guide the policy network πθ towards states that lead to the desired
global goal by facilitating stitching of relevant subsequences."
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.15469613259668508,"To that end, we represent the goal waypoint network Wϕ, parameterized by ϕ, as a neural network
that makes approximate K-step predictions of future observations conditioned on the current state,
st, and the target goal, ω. Formally, we attempt to minimize the objective in Equation 1 across the
same offline dataset D, where Lϕ is a mean-squared error (MSE) loss for continuous state spaces:"
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.1574585635359116,"arg min
ϕ X"
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.16022099447513813,"τ∈D
Lϕ(Wϕ(st, ω), st+K).
(1)"
INTERMEDIATE GOAL GENERATION FOR SPATIAL COMPOSITIONALITY,0.16298342541436464,"While our approach to intermediate target generation seems simple in relation to the complex problem
of modeling both the behavioral policy and transition dynamics, our goal is to provide approximate
short-term goals to facilitate the downstream task of reaching the global goal ω, rather than achieving
perfect predictions of future states under the behavioral policy."
PROXY REWARD GENERATION FOR BIAS-VARIANCE REDUCTION,0.16574585635359115,"4.3
Proxy Reward Generation for Bias-Variance Reduction"
PROXY REWARD GENERATION FOR BIAS-VARIANCE REDUCTION,0.1685082872928177,"In this section, we address the high bias and variance of conditioning variables used by prior
RvS methods in reward-conditioned tasks, such as Emmons et al. [2021] and Chen et al. [2021].
Analogously to Section 4.2 (i.e., for goal-conditioned tasks), we propose a technique to generate
intermediate reward targets for reward-conditioned tasks to mitigate these issues."
PROXY REWARD GENERATION FOR BIAS-VARIANCE REDUCTION,0.1712707182320442,"Existing methods rely on either an initial cumulative reward-to-go (desired return) or an average
reward-to-go target, denoted as ω. Importantly, the former is updated using rewards obtained during
the rollout, while the latter remains constant over time [Emmons et al., 2021, Chen et al., 2021].
However, using these conditioning variables during evaluation gives rise to two main issues: (a) the
Monte Carlo estimate of return used to compute the cumulative reward-to-go exhibits high variance
and (b) the constant average reward-to-go target introduces high bias over time. Based on our analyses
of the bias and variance of these approaches in Appendix C, we observe that these issues contribute
to decreased performance and stability across runs when evaluating RvS methods."
PROXY REWARD GENERATION FOR BIAS-VARIANCE REDUCTION,0.17403314917127072,"Although a potential approach to mitigate these issues is to leverage TD learning, such as the
Q-Learning Transformer [Yamagata et al., 2022], we restrict our work to RvS methods utilizing
behavioral cloning objectives due to the inherent complexity of training value-based methods. To
address the aforementioned concerns, we introduce a reward waypoint network denoted as Wϕ,
parameterized by ϕ. This network predicts the average and cumulative reward-to-go (ARTG, CRTG)
conditioned on the return, ω, and current state, st, using offline data D. To optimize this network, we
minimize the objective shown in Equation 2 using an MSE loss:"
PROXY REWARD GENERATION FOR BIAS-VARIANCE REDUCTION,0.17679558011049723,"arg min
ϕ X"
PROXY REWARD GENERATION FOR BIAS-VARIANCE REDUCTION,0.17955801104972377,"τ∈D
(
h
1
T −t
PT
t′=t γtrt
PT
t′=t γtrt
i⊤
−Wϕ(st, ω))2.
(2)"
PROXY REWARD GENERATION FOR BIAS-VARIANCE REDUCTION,0.18232044198895028,"By modeling both ARTG and CRTG, we address the high bias of a constant ARTG target and reduce the
variance associated with Monte Carlo estimates for CRTG. The construction of the reward waypoint
network is similar in motivation and the prediction task to a baseline network, used to mitigate high
variance in methods like REINFORCE [Sutton and Barto, 1999]. However, the distinguishing feature
of our reward waypoint network lies in its conditioning on the return, which allows for favorable
performance even on suboptimal offline datasets."
WAYPOINT TRANSFORMER,0.1850828729281768,"5
Waypoint Transformer W𝜙 ω ât"
WAYPOINT TRANSFORMER,0.1878453038674033,"W𝜙
st+2 ω ât+2 st"
WAYPOINT TRANSFORMER,0.19060773480662985,"st
st+2"
WAYPOINT TRANSFORMER,0.19337016574585636,"W𝜙
st+1 ât+1"
WAYPOINT TRANSFORMER,0.19613259668508287,"st+1
ω"
WAYPOINT TRANSFORMER,0.19889502762430938,"…
…
waypoint transformer"
WAYPOINT TRANSFORMER,0.20165745856353592,"Figure 3:
Waypoint Transformer architecture,
where Φt = Wϕ(st, ω) represents the output of
the goal or reward waypoint network."
WAYPOINT TRANSFORMER,0.20441988950276244,"We propose the waypoint transformer (WT), a
transformer-based offline RL method that lever-
ages the proposed waypoint network Wϕ and
a GPT-2 architecture based on multi-head at-
tention [Radford et al., 2019]. The WT pol-
icy πθ is conditioned on past states st−k..t and
waypoints (either generated goals or rewards)
Φt−k..t = Wϕ(st−k..t, ω) with a context win-
dow of size k, as shown in Figure 3."
WAYPOINT TRANSFORMER,0.20718232044198895,"Table 1: Normalized scores and training time per task on Gym-MuJoCo, AntMaze, and Kitchen tasks,
where bold highlighting indicates SOTA performance, defined by a method’s average performance
being contained within the interval of the method with highest average."
WAYPOINT TRANSFORMER,0.20994475138121546,"Environment
TD3 + BC
Onestep RL
CQL
IQL
BC
10% BC
RvS-R/G
DT
QDT
WT (Ours)
halfcheetah-medium-v2
48.3 ± 0.3
48.4 ± 0.1
44.0 ± 5.4
47.4 ± 0.2
42.6
42.5
41.6 ± 0.3
42.4 ± 0.2
42.3 ± 0.4
43.0 ± 0.2
hopper-medium-v2
59.3 ± 4.2
59.6 ± 2.5
58.5 ± 2.1
66.2 ± 5.7
52.9
56.9
60.2 ± 3.0
63.5 ± 5.2
66.5 ± 6.3
63.1 ± 1.4
walker2d-medium-v2
83.7 ± 2.1
81.8 ± 2.2
72.5 ± 0.8
78.3 ± 8.7
75.3
75.0
71.7 ± 1.8
69.2 ± 4.9
67.1 ± 3.2
74.8 ± 1.0
halfcheetah-medium-replay-v2
44.6 ± 0.5
38.1 ± 1.3
45.5 ± 0.5
44.2 ± 1.2
36.6
40.6
38.0 ± 0.7
35.4 ± 1.6
35.6 ± 0.5
39.7 ± 0.3
hopper-medium-replay-v2
60.9 ± 18.8
97.5 ± 0.7
95.0 ± 6.4
94.7 ± 8.6
18.1
75.9
73.5 ± 12.8
43.3 ± 23.9
52.1 ± 20.1
88.9 ± 2.4
walker2d-medium-replay-v2
81.8 ± 5.5
49.5 ± 12.0
77.2 ± 5.5
73.8 ± 7.1
26.0
62.5
60.6 ± 6.7
58.9 ± 7.1
58.2 ± 5.1
67.9 ± 3.4
halfcheetah-medium-expert-v2
90.7 ± 4.3
93.4 ± 1.6
91.6 ± 2.8
86.7 ± 5.3
55.2
92.9
92.2 ± 1.2
84.9 ± 1.6
-
93.2 ± 0.5
hopper-medium-expert-v2
98.0 ± 9.4
103.3 ± 1.9
105.4 ± 6.8
91.5 ± 14.3
52.5
110.9
101.7 ± 16.5
100.6 ± 8.3
-
110.9 ± 0.6
walker2d-medium-expert-v2
110.1 ± 0.5
113.0 ± 0.4
108.8 ± 0.7
109.6 ± 1.0
107.5
109.0
106.0 ± 0.9
89.6 ± 38.4
-
109.6 ± 1.0
gym-avg-v2
75.3 ± 4.9
76.1 ± 2.5
77.6 ± 3.4
76.9 ± 5.8
51.9
74.0
71.7 ± 4.9
65.3 ± 10.1
-
76.8 ± 1.2
antmaze-umaze-v2
78.6
64.3
74.0
87.5 ± 2.6
54.6
62.8
65.4 ± 4.9
53.6 ± 7.3
-
64.9 ± 6.1
antmaze-umaze-diverse-v2
71.4
60.7
84.0
62.2 ± 13.8
45.6
50.2
60.9 ± 2.5
42.2 ± 5.4
-
71.5 ± 7.6
antmaze-medium-play-v2
10.6
0.3
61.2
71.2 ± 7.3
0.0
5.4
58.1 ± 12.7
0.0 ± 0.0
-
62.8 ± 5.8
antmaze-medium-diverse-v2
3.0
0.0
53.7
70.0 ± 10.9
0.0
9.8
67.3 ± 8.0
0.0 ± 0.0
-
66.7 ± 3.9
antmaze-large-play-v2
0.2
0.0
15.8
39.6 ± 5.8
0.0
0.0
32.4 ± 10.5
0.0 ± 0.0
-
72.5 ± 2.8
antmaze-large-diverse-v2
0.0
0.0
14.9
47.5 ± 9.5
0.0
6.0
36.9 ± 4.8
0.0 ± 0.0
-
72.0 ± 3.4
antmaze-avg-v2
27.3
20.9
50.6
63.0 ± 8.3
16.7
22.5
53.5 ± 7.2
16.0 ± 2.1
68.4 ± 4.9
kitchen-complete-v0
-
-
43.8
62.5
65.0
4.0
50.2 ± 3.6
46.5 ± 3.0
-
49.2 ± 4.6
kitchen-partial-v0
-
-
49.8
46.3
38.0
66.0
51.4 ± 2.6
31.4 ± 19.5
-
63.8 ± 3.5
kitchen-mixed-v0
-
-
51.0
51.0
51.5
40.0
60.3 ± 9.4
25.8 ± 5.0
-
70.9 ± 2.1
kitchen-avg-v0
-
-
48.2
53.3 ± 7.5
51.5
36.7
54.0 ± 5.2
34.6 ± 9.2
-
61.3 ± 3.4
average
-
-
63.7
68.3 ± 6.9
40.1
50.6
62.7 ± 5.7
43.8 ± 7.3
-
71.4 ± 2.8
training time (min)
20
20
80
20
10
10
80
150
-
20"
WAYPOINT TRANSFORMER,0.212707182320442,"For goal-conditioned and reward-conditioned tasks, we train the goal and reward waypoint network
Wϕ respectively on offline dataset D independently of the policy. To train the WT policy, we use
Algorithm 1 to iteratively optimize its parameters θ. During this process, the trained weights ϕ of
Wϕ are frozen to ensure the interpretability of the waypoint network’s generated goal and reward
waypoints. To further simplify the design and improve computational efficiency, the WT is not
conditioned on past actions at−k..t (i.e., unlike the DT) and we concatenate Φt with st to produce
one token per timestep t instead of multiple tokens as proposed in Chen et al. [2021]."
EXPERIMENTS,0.2154696132596685,"6
Experiments"
EXPERIMENTS,0.21823204419889503,"We present a series of evaluations of WT across tasks involving reward and goal-conditioning, with
comparisons to prior offline RL methods. For this, we leverage D4RL, an open-source benchmark for
offline RL, consisting of varying datasets for tasks from Gym-MuJoCo, AntMaze, and FrankaKitchen
[Fu et al., 2020]."
EXPERIMENTS,0.22099447513812154,"Tasks in the AntMaze and FrankaKitchen environments have presented a challenge for offline RL
methods as they contain little to no optimal trajectories and perform critical evaluations of a model’s
stitching ability [Fu et al., 2020]. Specifically, in FrankaKitchen, the aim is to interact with a set of
kitchen items to reach a target configuration, but the partial and mixed offline datasets consist
of suboptimal, undirected data, where the demonstrations are unrelated to the target configuration.
Similarly, AntMaze is a maze navigation environment with sparse rewards, where the play and
diverse datasets contain target locations unaligned with the evaluation task. For our experiments on
these environments, we use goal-conditioning on the target goal state (i.e., ω = starget), constructing
intermediate targets with the goal waypoint network."
EXPERIMENTS,0.22375690607734808,"Gym-MuJoCo serves as a popular benchmark for prior work in offline RL, consisting of environments
such as Walker2D, HalfCheetah, and Hopper. Importantly, we evaluate across offline datasets with
varying degrees of optimality by considering the medium, medium-replay, and medium-expert
datasets [Fu et al., 2020]. For these tasks, we use reward-conditioning given a target return, construct-
ing intermediate reward targets using the reward waypoint network; as noted in Emmons et al. [2021],
we find that the notion of goals in undirected locomotion tasks is ill-defined."
EXPERIMENTS,0.2265193370165746,"Across all environments and tasks, we use the same set of hyperparameters, as reported in Appendix
B. To measure the stability (i.e., variability) in our method across random initializations, we run each
experiment across 5 seeds and report the mean and standard deviation."
COMPARING WT WITH PRIOR METHODS,0.2292817679558011,"6.1
Comparing WT with Prior Methods"
COMPARING WT WITH PRIOR METHODS,0.23204419889502761,"To evaluate the performance of WT, we perform comparisons with prior offline RL methods, including
conditional BC methods such as DT and RvS-R/G; value-based methods such as Onestep RL"
COMPARING WT WITH PRIOR METHODS,0.23480662983425415,"[Brandfonbrener et al., 2021], TD3 + BC [Fujimoto and Gu, 2021], CQL, and IQL; and standard
BC baselines. For all methods except DT, we use reported results from Emmons et al. [2021] and
Kostrikov et al. [2021]. We evaluate DT using the official implementation provided by Chen et al.
[2021] across 5 random initializations, though we are unable to reproduce some of their results."
COMPARING WT WITH PRIOR METHODS,0.23756906077348067,"Table 1 shows the results of our comparisons to prior methods. Aggregated across all tasks, WT (71.4
± 2.8) improves upon the next best method, IQL (68.3 ± 6.9), with respect to average normalized
score and achieves equal-best runtime. In terms of variability across seeds, there is a notable reduction
compared to IQL and most other methods."
COMPARING WT WITH PRIOR METHODS,0.24033149171270718,"In the most challenging tasks requiring stitching, our method demonstrates performance far exceeding
the next best method, IQL. On the AntMaze Large datasets, WT demonstrates a substantial relative
percentage improvement of 83.1% (play) and 51.6% (diverse). On Kitchen Partial and Mixed, the
improvement is 37.8% and 39.0% respectively. WT’s standard deviation across seeds is reduced by a
factor of more than 2x compared to IQL for these tasks."
COMPARING WT WITH PRIOR METHODS,0.2430939226519337,"Similarly, on reward-conditioned tasks with large performance gaps between BC and value-based
methods such as hopper-medium-replay-v2, WT demonstrates increased average performance by
105.3% compared to DT and 21.0% compared to RvS-R, with standard deviation reduced by a factor
of 10.0x and 5.3x respectively."
UTILITY OF WAYPOINT NETWORKS,0.24585635359116023,"6.2
Utility of Waypoint Networks"
UTILITY OF WAYPOINT NETWORKS,0.24861878453038674,"(a)
(b) (c)"
UTILITY OF WAYPOINT NETWORKS,0.2513812154696133,"0
200
400
600
800
1000
Timestep (t)"
UTILITY OF WAYPOINT NETWORKS,0.2541436464088398,"0.0
0.2
0.4
0.6
0.8
1.0
Proportion of Completed Runs (d)"
UTILITY OF WAYPOINT NETWORKS,0.2569060773480663,No Waypoints WT
UTILITY OF WAYPOINT NETWORKS,0.2596685082872928,"Figure 4: Shows the ant’s location across 100 rollouts
of (a) a WT policy and (b) a global goal-conditioned
transformer policy; (c) generated intermediate goals
by the waypoint network Wϕ, (d) the proportion of all
successful runs completed by timestep t."
UTILITY OF WAYPOINT NETWORKS,0.26243093922651933,"To analyze the utility and behavior of
waypoint networks, we qualitatively eval-
uate an agent’s performance across roll-
outs of trained transformer policies on
antmaze-large-play-v2. For this anal-
ysis, we consider a WT policy (using a goal
waypoint network with K = 30) and a
global goal-conditioned transformer policy
(i.e., no intermediate goals). Across both
models, the architecture and hyperparame-
ters for training are identical."
UTILITY OF WAYPOINT NETWORKS,0.26519337016574585,"The ant’s locations across 100 rollouts of
a WT policy (Figure 4a) and a global goal-
conditioned transformer policy (Figure 4b)
demonstrate that WT shows notably higher
ability and consistency in reaching the goal
location.
Specifically, without interme-
diate goals, the ant occasionally turns in
the wrong direction and demonstrates a
lesser ability to successfully complete a
turn based on the reduction of density at
each turn (Figure 4b). Consequently, the
WT achieves more than twice the evaluation return (72.5 ± 2.8) compared to the global goal-
conditioned policy (33.0 ± 10.3) and completes the task more quickly on average (Figure 4d)."
UTILITY OF WAYPOINT NETWORKS,0.26795580110497236,"Based on Figure 4c, we observe that the goal waypoint network provides goals that correspond to the
paths traversed in Figure 4a for the WT policy. This shows that the waypoint network successfully
guides the model toward the target location, addressing the stitching problem proposed in Figure 2.
While the global goal-conditioned policy is successful in passing beyond the stitching region into the
target region in only 45% of the rollouts, accounting for 82% of its failures to reach the target, WT is
successful in this respect for 87% of rollouts."
ABLATION STUDIES,0.27071823204419887,"6.3
Ablation Studies"
ABLATION STUDIES,0.27348066298342544,"Goal-Conditioned Tasks
On goal-conditioned tasks, we examine the behavior of the goal waypoint
network as it relates to the performance of the policy at test time by ablating aspects of its configuration"
ABLATION STUDIES,0.27624309392265195,"and training. For this analysis, we consider antmaze-large-play-v2, a challenging task that
critically evaluates the stitching capability of offline RL techniques."
ABLATION STUDIES,0.27900552486187846,"0
20
40
60
80
K 20 30 40 50 60 70"
ABLATION STUDIES,0.281767955801105,Normalized Score
ABLATION STUDIES,0.2845303867403315,"0.4
0.6
0.8
1.0
1.2
RMSE 20 30 40 50 60 70"
ABLATION STUDIES,0.287292817679558,Normalized Score
ABLATION STUDIES,0.2900552486187845,"Figure 5:
Normalized score attained by WT on
antmaze-large-play-v2 based on varying left: the
temporal proximity of generated goals, K, and right:
goal waypoint network RMSE on a held-out dataset."
ABLATION STUDIES,0.292817679558011,"To understand the effect of the configura-
tion of the goal waypoint network on test
performance, we ablate two variables rel-
evant to generating effective intermediate
goals: the temporal proximity of intermedi-
ate goals (K) and the validation loss of the
goal waypoint network."
ABLATION STUDIES,0.2955801104972376,"The normalized score attained by the agent
is shown as a function of K and the vali-
dation loss of the goal waypoint network
in Figure 5.
For this environment and
dataset, an ideal choice for K is around
30 timesteps. For all nonzero K, the per-
formance is reduced at a reasonably consistent rate on either side of K = 30. Importantly, when
K = 0 (i.e., no intermediate goals), there is a notable reduction in performance compared to all other
choices of K; compared to the optimal K = 30, the score is reduced by a factor of 2.2x."
ABLATION STUDIES,0.2983425414364641,"In Figure 5 (right), the normalized score shows the negligible change for values of held-out RMSE
between 0.4 and 0.6, corresponding to at least 1,000 gradient steps or roughly 30 sec of training,
with a sharper decrease henceforth. As the RMSE increases to over 1, we observe a relative plateau
in performance near an average normalized score of 35-45, roughly corresponding to performance
without using a waypoint network (i.e., K = 0 in Figure 5 (left))."
ABLATION STUDIES,0.3011049723756906,"Additionally, we perform comparisons between the goal waypoint network and manually constructed
waypoints as intermediate targets for WT, for which the methodology and results are shown in
Appendix D. Based on that analysis, we show that manual waypoints statistically significantly
improve upon no waypoints (44.5 ± 2.8 vs. 33.0 ± 10.3), but they remain significantly worse than
generated waypoints."
ABLATION STUDIES,0.30386740331491713,"0
20
40
60
80 100
Normalized Score 0.0 0.2 0.4 0.6 0.8 1.0"
ABLATION STUDIES,0.30662983425414364,Proportion of Runs
ABLATION STUDIES,0.30939226519337015,"ARTG
CRTG
WT"
ABLATION STUDIES,0.31215469613259667,"0
400
800
Timestep 0 40 80 120 160 200 240"
ABLATION STUDIES,0.3149171270718232,Standard Deviation
ABLATION STUDIES,0.31767955801104975,"CRTG
WT (CRTG)"
ABLATION STUDIES,0.32044198895027626,"Figure 6: Comparison of different reward-conditioning
methods on hopper-medium-replay-v2. Left: Per-
formance profiles for transformers using ARTG, CRTG,
and WT across 5 random seeds. Right: Standard devia-
tion in CRTG inputted to the model when updated with at-
tained rewards (CRTGt = ω−P"
ABLATION STUDIES,0.32320441988950277,"t γtrt) and using predic-
tions from the reward waypoint network (Wϕ(st, ω)2)
when average return is approximately held constant."
ABLATION STUDIES,0.3259668508287293,"Reward-Conditioned Tasks
On reward-
conditioned tasks, we ablate the choice of
different reward-conditioning techniques.
Specifically, we examine the performance
of WT and variance of the reward way-
point network in comparison to CRTG up-
dated using the rewards obtained dur-
ing rollouts and a static ARTG (i.e., as
done in Chen et al. [2021] and Em-
mons et al. [2021]).
We consider the
hopper-medium-replay-v2 task for this
analysis as there is (a) a large performance
gap between RvS and value-based meth-
ods, and (b) high instability across seeds
for RvS methods (e.g., DT) as shown in Ta-
ble 1. For all examined reward-conditioned
techniques, the transformer architecture
and training procedure are identical, and the target (normalized) return is 95, corresponding to
SOTA performance."
ABLATION STUDIES,0.3287292817679558,"To examine the distribution of normalized scores across different seeds produced by each of the de-
scribed reward-conditioning techniques, we construct performance profiles, displaying the proportion
of runs greater than a certain normalized score [Agarwal et al., 2021]. As shown in Figure 6 (left),
WT demonstrates increased performance and stability across random initializations compared to the
remaining reward-conditioning techniques."
ABLATION STUDIES,0.3314917127071823,"Additionally, we perform an analysis to determine whether using a reward waypoint network to
predict the CRTG as opposed to updating the CRTG using attained rewards as in Chen et al. [2021]
affects the variability of the conditioning variable passed to the policy network (i.e., not of the"
ABLATION STUDIES,0.3342541436464088,"performance as that is examined in Figure 6). Importantly, to account for performance differences
between the policies trained with either method that may influence the variability of the attained
CRTG, we sample a subset of runs for both methods such that the average performance is constant.
Based on Figure 6 (right), it is clear that as a function of the timestep, when accounting for difference
in average performance, the standard deviation in the CRTG predicted by WT grows at a slower rate
compared to updating CRTG with attained rewards."
ABLATION STUDIES,0.3370165745856354,"Transformer Configuration
Based on the work in Emmons et al. [2021], we balance between
expressiveness and regularization to maximize policy performance. We ablate the probability of node
dropout pdrop and the number of transformer layers L. To further examine this balance, we experi-
ment with conditioning on past actions at−k..t−1, similarly to the DT, to characterize its impact on
performance and computational efficiency. In this section, we consider antmaze-large-play-v2,
hopper-medium-replay-v2 and kitchen-mixed-v0, one task from each category of environ-
ments."
ABLATION STUDIES,0.3397790055248619,"Based on Table 2, we observe that the sensitivity to the various ablated hyperparameters is relatively
low in terms of performance, and removing action conditioning results in reduced training time and
increased performance, perhaps due to reduced distribution shift at evaluation. In context of prior
RvS work where dropout (pdrop = 0.1) decreased performance compared to no dropout by 1.5-3x on
AntMaze, the largest decrease in average performance on WT is only by a factor of 1.1x [Emmons
et al., 2021]."
ABLATION STUDIES,0.3425414364640884,"Table 2: Ablation of transformer configuration showing normalized score on MuJoCo (v2), AntMaze
(v2) and Kitchen (v0), including dropout (pdrop), transformer layers (L), and action conditioning (at),
where bolded hyperparameters (e.g., 0.150) are used for final models and bolded scores are optimal."
ABLATION STUDIES,0.3453038674033149,"pdrop
hopper-medium-replay
antmaze-large-play
kitchen-mixed
Average"
ABLATION STUDIES,0.34806629834254144,"0.000
75.5 ± 8.3
68.3 ± 5.9
72.9 ± 0.5
72.2 ± 4.9
0.075
89.8 ± 2.8
70.8 ± 4.5
71.8 ± 1.2
77.5 ± 2.8
0.150
88.9 ± 2.4
72.5 ± 2.8
70.9 ± 2.1
77.4 ± 2.4
0.225
75.7 ± 9.4
72.2 ± 2.7
71.2 ± 1.0
73.0 ± 4.4
0.300
74.7 ± 10.2
73.5 ± 2.5
69.2 ± 2.0
72.5 ± 4.9
0.600
58.4 ± 7.5
73.8 ± 5.2
66.5 ± 2.7
66.2 ± 5.1"
ABLATION STUDIES,0.35082872928176795,"L
hopper-medium-replay
antmaze-large-play
kitchen-mixed
Average"
ABLATION STUDIES,0.35359116022099446,"1
82.1 ± 8.8
72.1 ± 5.7
71.6 ± 1.6
75.3 ± 5.4
2
88.9 ± 2.4
72.5 ± 2.8
70.9 ± 2.1
77.4 ± 2.4
3
89.9 ± 1.6
71.8 ± 3.0
70.3 ± 2.1
77.3 ± 2.2
4
91.1 ± 2.8
65.8 ± 3.8
69.7 ± 1.0
75.5 ± 2.5
5
88.8 ± 4.5
66.7 ± 4.7
70.0 ± 0.8
75.2 ± 3.3"
ABLATION STUDIES,0.356353591160221,"at
hopper-medium-replay
antmaze-large-play
kitchen-mixed
Average"
ABLATION STUDIES,0.35911602209944754,"Yes
76.9 ± 9.0
66.5 ± 5.6
65.2 ± 2.8
69.5 ± 5.8
No
88.9 ± 2.4
72.5 ± 2.8
70.9 ± 2.1
77.4 ± 2.4"
DISCUSSION,0.36187845303867405,"7
Discussion"
DISCUSSION,0.36464088397790057,"In this study, we address the issues with existing conditioning techniques used in RvS, such as the
""stitching"" problem associated with global goals and the high bias and variance of reward-to-go
targets, through the automatic generation of intermediate targets. Based on empirical evaluations, we
demonstrate significantly improved performance and stability compared to existing RvS methods,
often on par with or outperforming TD learning methods. Especially on challenging tasks with
suboptimal dataset composition, such as AntMaze Large and Kitchen Partial/Mixed, the guidance pro-
vided by the waypoint network through intermediate targets (e.g., as shown in Figure 4) significantly
improves upon existing state-of-the-art performance."
DISCUSSION,0.3674033149171271,"We believe that this work can present a pathway forward to developing practical offline RL methods
leveraging the simplicity of RvS and exploring more effective conditioning techniques, as formalized
by Emmons et al. [2021]. In addition to state-of-the-art performance, we demonstrate several desirable
practical qualities of the WT: it is less sensitive to changes in hyperparameters, significantly faster to
train than prior RvS work, and more consistent across initialization seeds."
DISCUSSION,0.3701657458563536,"However, despite improvements across challenging tasks, WT’s margin of improvement on AntMaze
U-Maze and Kitchen Complete (i.e., easier tasks) is lower: its normalized scores are more comparable
to DT and other RvS methods. We believe this is likely due to stitching being less necessary in
such tasks compared to difficult tasks, rendering the impact of the waypoint network negligible. To
further characterize the performance of the waypoint networks and WT on such tasks is an interesting
direction for future work. In addition, there are several limitations inherited by the usage of the RvS
framework, such as manual tuning of the target return at test time for reward-conditioned tasks using
a grid search, issues with stochasticity, and an inability to learn from data with multimodal outcomes."
CONCLUSION,0.3729281767955801,"8
Conclusion"
CONCLUSION,0.3756906077348066,"We propose a method for reinforcement learning via supervised learning, Waypoint Transformer,
conditioned on generated intermediate targets for reward and goal-conditioned tasks. We show
that RvS with waypoints significantly surpasses existing RvS methods and achieves on par with
or surpasses popular state-of-the-art methods across a wide range of tasks from Gym-MuJoCo,
AntMaze, and Kitchen. With improved stability across runs and competitive computational efficiency,
we believe that our method advances the performance and applicability of RvS within the context of
offline RL."
CONCLUSION,0.3784530386740331,Acknowledgments and Disclosure of Funding
CONCLUSION,0.3812154696132597,This work was supported in part by NSF grant #2112926.
CONCLUSION,0.3839779005524862,"We thank Scott Emmons and Ilya Kostrikov for their discussions on and contributions to providing
results for prior offline RL methods."
REFERENCES,0.3867403314917127,References
REFERENCES,0.38950276243093923,"Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information
processing systems, 34:29304–29320, 2021."
REFERENCES,0.39226519337016574,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
Advances in neural information processing systems, 30, 2017."
REFERENCES,0.39502762430939226,"Arthur Argenson and Gabriel Dulac-Arnold.
Model-based offline planning.
arXiv preprint
arXiv:2008.05556, 2020."
REFERENCES,0.39779005524861877,"David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy
evaluation. Advances in neural information processing systems, 34:4933–4946, 2021."
REFERENCES,0.4005524861878453,"David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does
return-conditioned supervised learning work for offline reinforcement learning? arXiv preprint
arXiv:2206.01079, 2022."
REFERENCES,0.40331491712707185,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,
Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence
modeling. Advances in neural information processing systems, 34:15084–15097, 2021."
REFERENCES,0.40607734806629836,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.4088397790055249,"Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation
learning. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.4116022099447514,"Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for
offline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021."
REFERENCES,0.4143646408839779,"Ben Eysenbach, Xinyang Geng, Sergey Levine, and Russ R Salakhutdinov. Rewriting history with
inverse rl: Hindsight inference for policy improvement. Advances in neural information processing
systems, 33:14783–14795, 2020."
REFERENCES,0.4171270718232044,"Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.4198895027624309,"Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
Advances in neural information processing systems, 34:20132–20145, 2021."
REFERENCES,0.42265193370165743,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International conference on machine learning, pages 2052–2062. PMLR, 2019."
REFERENCES,0.425414364640884,"Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline
hindsight information matching. arXiv preprint arXiv:2111.10364, 2021."
REFERENCES,0.4281767955801105,"Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach,
and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint
arXiv:1912.06088, 2019."
REFERENCES,0.430939226519337,"Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencoders are scalable vision learners.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 16000–16009, 2022."
REFERENCES,0.43370165745856354,"Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence
modeling problem. Advances in neural information processing systems, 34:1273–1286, 2021."
REFERENCES,0.43646408839779005,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. Advances in neural information processing systems, 33:
21810–21823, 2020."
REFERENCES,0.43922651933701656,"Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit
q-learning. arXiv preprint arXiv:2110.06169, 2021."
REFERENCES,0.4419889502762431,"Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint
arXiv:1912.13465, 2019."
REFERENCES,0.4447513812154696,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020."
REFERENCES,0.44751381215469616,"Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. When should we prefer offline
reinforcement learning over behavioral cloning? arXiv preprint arXiv:2204.05618, 2022."
REFERENCES,0.45027624309392267,"Hao Liu and Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience. arXiv
preprint arXiv:2305.16554, 2023."
REFERENCES,0.4530386740331492,"Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. Advances in neural information processing
systems, 33:1264–1274, 2020."
REFERENCES,0.4558011049723757,"Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2206.04745, 2022."
REFERENCES,0.4585635359116022,"Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual
reinforcement learning with imagined goals. Advances in neural information processing systems,
31, 2018."
REFERENCES,0.4613259668508287,"Allen Nie, Yannis Flet-Berliac, Deon Jordan, William Steenbergen, and Emma Brunskill. Data-
efficient pipeline for offline reinforcement learning with limited data. Advances in Neural Informa-
tion Processing Systems, 35:14810–14823, 2022."
REFERENCES,0.46408839779005523,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,
and Dustin Tran. Image transformer. In International conference on machine learning, pages
4055–4064. PMLR, 2018."
REFERENCES,0.46685082872928174,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.4696132596685083,"Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based offline
reinforcement learning. arXiv preprint arXiv:2204.12581, 2022."
REFERENCES,0.4723756906077348,"Juergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map them to
actions. arXiv preprint arXiv:1912.02875, 2019."
REFERENCES,0.47513812154696133,"Jian Shen, Mingcheng Chen, Zhicheng Zhang, Zhengyu Yang, Weinan Zhang, and Yong Yu. Model-
based offline policy optimization with distribution correcting regularization. In Machine Learning
and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD
2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part I 21, pages 174–189. Springer,
2021."
REFERENCES,0.47790055248618785,"Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja´skowski, and Jürgen Schmidhuber.
Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877,
2019."
REFERENCES,0.48066298342541436,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Robotica, 17(2):
229–235, 1999."
REFERENCES,0.48342541436464087,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017."
REFERENCES,0.4861878453038674,"Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan.
Prompting decision transformer for few-shot policy generalization. In International Conference on
Machine Learning, pages 24631–24645. PMLR, 2022."
REFERENCES,0.4889502762430939,"Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. Q-learning decision transformer:
Leveraging dynamic programming for conditional sequence modelling in offline rl. arXiv preprint
arXiv:2209.03993, 2022."
REFERENCES,0.49171270718232046,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information
Processing Systems, 33:14129–14142, 2020."
REFERENCES,0.494475138121547,"Xianyuan Zhan, Xiangyu Zhu, and Haoran Xu. Model-based offline planning with trajectory pruning.
arXiv preprint arXiv:2105.07351, 2021."
REFERENCES,0.4972375690607735,"Siyuan Zhang and Nan Jiang. Towards hyperparameter-free policy selection for offline reinforcement
learning. Advances in Neural Information Processing Systems, 34:12864–12875, 2021."
REFERENCES,0.5,"Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In International
Conference on Machine Learning, pages 27042–27059. PMLR, 2022."
REFERENCES,0.5027624309392266,"A
Derivation for Illustrative Example"
REFERENCES,0.505524861878453,"We provide detailed derivations based on the simple deterministic MDP shown in Section 4.1, in
the context of an offline dataset D collected by a random behavioural policy πb. We show that
minimization of a maximum likelihood objective on πG yields πb, the behavioural policy. Note
πG(at | st, ω) = πG(at | st) as ω = s(H) is a constant (and as a result, at is conditionally
independent). To obtain the optimal policy π∗
G, we maximize the following objective:"
REFERENCES,0.5082872928176796,"arg max
πG E(st,at)∈D[log πG(at | st, ω)]"
REFERENCES,0.511049723756906,We simplify an expectation over an infinitely large dataset D collected by πb:
REFERENCES,0.5138121546961326,"E(st,at)∈D[log πG(at | st, ω)] = Est∈D[λ log πG(at = a(1) | st, ω) + (1 −λ) log πG(at = a(2) | st, ω)]"
REFERENCES,0.5165745856353591,"Since the actions are conditionally independent of the states, let ˆp = πG(at = a(1) | st, ω) for any
state st. Then:
E(st,at)∈D[log πG(at | st, ω)] = λ log ˆp + (1 −λ) log(1 −ˆp)
We can use calculus to maximize the above objective with respect to ˆp."
REFERENCES,0.5193370165745856,"d
dˆp [λ log ˆp + (1 −λ) log(1 −ˆp)] = λ"
REFERENCES,0.5220994475138122,ˆp −1 −λ 1 −ˆp
REFERENCES,0.5248618784530387,= λ(1 −ˆp) −(1 −λ)ˆp
REFERENCES,0.5276243093922652,ˆp(1 −ˆp)
REFERENCES,0.5303867403314917,"Setting the derivative to 0:
λ(1 −ˆp) −(1 −λ)ˆp = λ −λˆp −ˆp + λˆp = 0 =⇒
ˆp = λ"
REFERENCES,0.5331491712707183,"This yields an identical policy to the behavioural policy πb. Next, consider the derivation of the
probability of taking action a(2) conditioned on Φt and st based on data D from πb:"
REFERENCES,0.5359116022099447,"Prπb[at = a(2) | st = s(h), st+K = s(h+1)]"
REFERENCES,0.5386740331491713,"= Prπb[at = a(2), st+K = s(h+1) | st = s(h)]"
REFERENCES,0.5414364640883977,Prπb[st+K = s(h+1) | st = s(h)]
REFERENCES,0.5441988950276243,"In the above step, we used the definition of conditional probability. To compute these probabilities,
we recognize that to end up with st+K = s(h+1) from st = s(h), the agent must take action a(2)"
REFERENCES,0.5469613259668509,exactly once between timestep t and t + K −1; any more implies the agent has moved beyond s(h+1)
REFERENCES,0.5497237569060773,and any less implies the agent is still at s(h).
REFERENCES,0.5524861878453039,"The probability in the numerator can be written as a product of taking action a(2) at timestep t,
followed by taking action a(1) at timestep t + 1 to t + K −1:"
REFERENCES,0.5552486187845304,"Prπb[at = a(2), st+K = s(h+1) | st = s(h)] = (1 −λ)"
REFERENCES,0.5580110497237569,"t+K−1
Y"
REFERENCES,0.5607734806629834,"t′=t+1
λ"
REFERENCES,0.56353591160221,= (1 −λ)λK−1
REFERENCES,0.5662983425414365,"The probability in the denominator can be written as a product of taking action a(2) at exactly one
timestep t ≤t′ < t + K, followed by taking action a(1) at the remaining timesteps. This can be
modeled by a binomial probability where there are K slots to take action a(1), each with probability
1 −λ. Hence:"
REFERENCES,0.569060773480663,"Prπb[st+K = s(h+1) | st = s(h) =
K
1"
REFERENCES,0.5718232044198895,"
(1 −λ)λK−1"
REFERENCES,0.574585635359116,= K(1 −λ)λK−1
REFERENCES,0.5773480662983426,The overall probability is computed as:
REFERENCES,0.580110497237569,"Prπb[at = a(2), st+K = s(h+1) | st = s(h)]"
REFERENCES,0.5828729281767956,"Prπb[st+K = s(h+1) | st = s(h)]
=
(1 −λ)λK−1"
REFERENCES,0.585635359116022,K(1 −λ)λK−1 = 1
REFERENCES,0.5883977900552486,"K
We can apply a similar argument to show that π∗
W (i.e., at optimum) must clone the derived prob-
ability when a maximum likelihood objective is applied. Hence, for the optimal intermediate
goal-conditioned policy π∗
W , we know it obeys:"
REFERENCES,0.5911602209944752,"π∗
W (at = a(2) | st = s(h), Φt = s(h+1)) = 1 K"
REFERENCES,0.5939226519337016,"Since π∗
G(at = a(2) | st = s(h), ω) = πb(at = a(2) | st = s(h)) = 1 −λ and we choose
K <
1
1−λ =⇒
1
K > 1 −λ, we conclude that:"
REFERENCES,0.5966850828729282,"π∗
W (at = a(2) | st, Φt) > π∗
G(at = a(2) | st, ω)"
REFERENCES,0.5994475138121547,This concludes the derivation.
REFERENCES,0.6022099447513812,"B
Experimental Details"
REFERENCES,0.6049723756906077,"In this section, we provide more details about the experiments, including hyperparameter configura-
tion, sources of reported results for each method, and details of each environment (i.e., version). For
all experiments on WT, the proposed method, we run 5 trials with different random seeds and report
the mean and standard deviation across them. On AntMaze and Kitchen, we use goal-conditioning,
whereas reward-conditioning is used for Gym-MuJoCo. For all experiments on DT, including Gym-
MuJoCo, we run 5 trials with random initializations using the default hyperparameters proposed in
Chen et al. [2021] and used in the official GitHub repository. We are unable to reproduce some of the
results demonstrated in Chen et al. [2021] and reported in succeeding work such as Kostrikov et al.
[2021], Emmons et al. [2021]."
REFERENCES,0.6077348066298343,"B.1
Environments and Tasks"
REFERENCES,0.6104972375690608,"AntMaze
For AntMaze tasks, we include previously reported results for all methods except RvS-G
from Kostrikov et al. [2021]. The results for the RvS-G are from Emmons et al. [2021]. We run
experiments for DT (reward-conditioned, as per Chen et al. [2021]) and WT across 5 seeds. For all
reported results, including WT, AntMaze v2 is used as opposed to AntMaze v0."
REFERENCES,0.6132596685082873,"FrankaKitchen
On Kitchen, we include available reported results from Kostrikov et al. [2021]
for all methods except RvS-G and Emmons et al. [2021] for RvS-G, with results omitted for TD3 +
BC and Onestep RL as they are not available in other work or provided by the authors. Similarly
to AntMaze, we run experiments for DT and WT across 5 seeds. The target goal configuration for
WT is ""all"" (i.e., where all the tasks are solved), per Emmons et al. [2021]. For all reported results,
including WT, Kitchen v0 is used."
REFERENCES,0.6160220994475138,"Gym-MuJoCo
On the evaluated locomotion tasks, we use reported results from Kostrikov et al.
[2021] for all methods except RvS-R and Emmons et al. [2021] (RvS-R). We run experiments for DT
and WT across 5 seeds. The MuJoCo v2 environments are used for all methods."
REFERENCES,0.6187845303867403,"B.2
WT Hyperparameters"
REFERENCES,0.6215469613259669,"In Table 3, we show the chosen hyperparameter configuration for WT across all experiments. Consis-
tent with the neural network model in RvS-R/G with 1.1M parameters Emmons et al. [2021], the WT
contains 1.1M trainable parameters. For the most part, the chosen hyperparameters align closely with
default values in deep learning; for example, we use the ReLU activation function and a learning rate
of 0.001 with the Adam optimizer."
REFERENCES,0.6243093922651933,"In Table 4, we show the chosen hyperparameter configuration for the reward and goal waypoint
networks across all experiments. The reward waypoint network always outputs 2 values, the ARTG
and CRTG. In general, the goal waypoint network outputs the same dimension as the state since"
REFERENCES,0.6270718232044199,Table 3: Hyperparameters and configuration details for WT across all experiments.
REFERENCES,0.6298342541436464,"Hyperparameter
Value"
REFERENCES,0.6325966850828729,"Transformer Layers
2
Transformer Heads
16
Dropout Probability (attn)
0.15
Dropout Probability (resid)
0.15
Dropout Probability (embd)
0.0
Non-Linearity
ReLU
Learning Rate
0.001
Gradient Steps
30,000
Batch Size
1024"
REFERENCES,0.6353591160220995,"it makes k-step predictions. Depending on the environment, the goal waypoint outputs either a
2-dimensional location for AntMaze or a 30-dimensional state for Kitchen."
REFERENCES,0.638121546961326,"Table 4: Hyperparameters and configuration details for goal and reward waypoint networks across all
experiments."
REFERENCES,0.6408839779005525,"Hyperparameter
Value"
REFERENCES,0.643646408839779,"Number of Layers
3
Dropout Probability
0.0
Non-Linearity
ReLU
Learning Rate
0.001
Gradient Steps
40,000
Batch Size
1024"
REFERENCES,0.6464088397790055,"B.3
Evaluation Return Targets"
REFERENCES,0.649171270718232,"The target return for the Gym-MuJoCo tasks are specified in Table 5, in the form of normalized
scores. These were obtained typically by performing exhaustive grid searches over 4-6 candidate
target return values, following prior work [Chen et al., 2021, Emmons et al., 2021]. Typically, we
choose the range of the grid search based on the interval close to or higher than the state-of-the-art
normalized scores on each of the tasks."
REFERENCES,0.6519337016574586,Table 5: Normalized score targets for WT on reward-conditioned tasks in Gym-Mujoco.
REFERENCES,0.6546961325966851,"Task
Normalized Score Target"
REFERENCES,0.6574585635359116,"hopper-medium-replay-v2
95
hopper-medium-v2
73.3
hopper-medium-expert-v2
125
walker2d-medium-replay-v2
90
walker2d-medium-v2
85
walker2d-medium-expert-v2
122.5
halfcheetah-medium-replay-v2
45
halfcheetah-medium-v2
52.5
halfcheetah-medium-expert-v2
105"
REFERENCES,0.6602209944751382,"C
Analysis of Bias and Variance of Reward-Conditioning Variables"
REFERENCES,0.6629834254143646,"We analyze the bias and variance of existing reward-conditioning techniques: a constant average
reward-to-go (ARTG) target, as used in Emmons et al. [2021], and a cumulative return target updated
with rewards collected during the episode (CRTG), as in Chen et al. [2021]. By analyzing the bias and
variance of these techniques, we can determine the potential issues that may explain the performance
of methods that condition using these techniques."
REFERENCES,0.6657458563535912,"Consider the definitions of the true ARTG (Ra) and CRTG (Rc) below, based on a given trajectory τ
where the length of the trajectory |τ| = T. These definitions are used to train the policy,"
REFERENCES,0.6685082872928176,"Ra(τ, t) =
1
T −t T
X"
REFERENCES,0.6712707182320442,"t′=t
γtrt
(3)"
REFERENCES,0.6740331491712708,"Rc(τ, t) = T
X"
REFERENCES,0.6767955801104972,"t′=t
γtrt
(4)"
REFERENCES,0.6795580110497238,"At evaluation time, it is impossible to calculate rt′ for any t′ ≥t. As a result, we provide ARTG and
CRTG targets, θa and θc respectively. At evaluation time, the values of the ARTG and CRTG are
estimated and used as follows in Chen et al. [2021] and Emmons et al. [2021]:"
REFERENCES,0.6823204419889503,"ˆRa(τ, t) = θa
(5)"
REFERENCES,0.6850828729281768,"ˆRc(τ, t) = θc − t
X"
REFERENCES,0.6878453038674033,"t′=1
γtrt
(6)"
REFERENCES,0.6906077348066298,"Ideally, the errors of the estimated ˆRa and ˆRc are minimal so as to accurately approximate the true
average or cumulative reward-to-go respectively, but that is often infeasible. To characterize the error
of each of the evaluation estimates of ARTG and CRTG, consider the decomposition of the error for
a particular reward conditioning variable R and estimated evaluation ˆR presented in Theorem C.1."
REFERENCES,0.6933701657458563,"Theorem C.1. The general bias-variance decomposition of the expected squared error between a true
R(τ, t) (e.g., Equations 1 or 2) and an estimator ˆR(τ, t) (e.g., Equations 3 or 4) under the trajectory
distribution induced by an arbitrary policy π and unknown transition dynamics p(st+1 | st, at) is
given by:
Eτ[(R(τ, t) −ˆR(τ, t))2] = E[R(τ, t) −ˆR(τ, t)]2 + Var[ ˆR(τ, t) −R(τ, t)]"
REFERENCES,0.6961325966850829,"Proof. Similarly to the derivation of the standard bias-variance tradeoff, we expand terms and
separate into multiple expectations using the linearity of expectation. We leverage the definition of
the covariance, Cov(X, Y ) = E[XY ] −E[X]E[Y ], and variance, Var[X] = E[X2] −E[X]2, in
several steps.
Eτ[(R(τ, t) −ˆR(τ, t))2] = E[R(τ, t)2] + E[ ˆR(τ, t)2] −2E[ ˆR(τ, t)R(τ, t)]"
REFERENCES,0.6988950276243094,"We can simplify the first term using the definition of the variance and the third term using the
definition of the covariance."
REFERENCES,0.7016574585635359,"Eτ[(R(τ, t) −ˆR(τ, t))2] = Var[R(τ, t)] + E[R(τ, t)]2 + E[ ˆR(τ, t)2] −2E[ ˆR(τ, t)R(τ, t)]"
REFERENCES,0.7044198895027625,"= Var[R(τ, t)] + E[R(τ, t)]2 + E[ ˆR(τ, t)2] −2(Cov( ˆR(τ, t), R(τ, t))+"
REFERENCES,0.7071823204419889,"E[R(τ, t)] · E[ ˆR(τ, t)]))"
REFERENCES,0.7099447513812155,"Similarly, we simplify the E[ ˆR(τ, t)2] term using the definition of the variance and collect terms."
REFERENCES,0.712707182320442,"Eτ[(R(τ, t) −ˆR(τ, t))2] = (E[R(τ, t)] −E[ ˆR(τ, t)])2 + Var[ ˆR(τ, t)] −2Cov( ˆR(τ, t), R(τ, t))+
Var[R(τ, t)]"
REFERENCES,0.7154696132596685,"= E[R(τ, t) −ˆR(τ, t)]2 + Var[ ˆR(τ, t)] −2Cov( ˆR(τ, t), R(τ, t))+
Var[R(τ, t)]"
REFERENCES,0.7182320441988951,"Equivalently, since Var[X −Y ] = Var[X] + Var[Y ] −2Cov(X, Y ), we can rewrite the result as
follows."
REFERENCES,0.7209944751381215,"Eτ[(R(τ, t) −ˆR(τ, t))2] = E[R(τ, t) −ˆR(τ, t)]2 + Var[ ˆR(τ, t) −R(τ, t)]"
REFERENCES,0.7237569060773481,"0
200
400
600
800
1000
Timestep (t) 0 1 2 3 4 5"
REFERENCES,0.7265193370165746,Average Reward-to-Go
REFERENCES,0.7292817679558011,"ARTG Target (Ra( , t) =
a)"
REFERENCES,0.7320441988950276,"0
200
400
600
800
1000
Timestep (t) 0 1 2 3 4 5"
REFERENCES,0.7348066298342542,Average Reward-to-Go
REFERENCES,0.7375690607734806,"Figure 7: True average reward-to-go as a function of timestep t for 200 rollouts of a transformer
policy on hopper-medium-replay-v2 compared to the constant ARTG target θa (dotted black
line), for left: successful rollouts and right: unsuccessful rollouts."
REFERENCES,0.7403314917127072,This completes the derivation of the general bias-variance decomposition.
REFERENCES,0.7430939226519337,"Analysis of ARTG
Consider the decomposition of the error per the bias-variance tradeoff of the
ARTG. Trivially, the variance of the estimate in Equation 3 is zero and it is independent of R because
it is a constant value, and as a result, the expected error is composed entirely of the bias and irreducible
variance."
REFERENCES,0.7458563535911602,"Eτ[(Ra(τ, t) −ˆRa(τ, t))2] = Eτ[Ra(τ, t) −ˆRa(τ, t)]2 + Var[Ra(τ, t)]"
REFERENCES,0.7486187845303868,"= Eτ[Ra(τ, t) −θa]2 + Var[Ra(τ, t)]"
REFERENCES,0.7513812154696132,"Based on the terms that we are able to minimize, we can derive through calculus that the squared error
and bias of a constant estimator are minimized by the mean, i.e., when ˆθa = Eτ[Ra(τ, t)]. However,
in the offline RL setting, we cannot easily determine this value for an arbitrary trained policy π. As a
result, the bias of the technique during evaluation can lead to high error in estimating the true ARTG,
which may consequently cause reduced performance during evaluation of the policy."
REFERENCES,0.7541436464088398,"Empirically, we demonstrate that the high bias of this technique may lead to instability in achieved
return across rollouts on hopper-medium-replay-v2. Specifically, suppose that a rollout is classi-
fied as unsuccessful if it terminates before the time limit T = 1000, and analogously, a successful
rollout reaches t = T without termination. Based on these distinctions, we display the true ARTG
across 200 successful and unsuccessful rollouts of a trained transformer policy in Figure 7."
REFERENCES,0.7569060773480663,"Clearly, for all successful rollouts, the true ARTG matches the constant target closely across most
t ∈[1, T), with the exception of t ≈T. However, across most failed rollouts, the bias of the constant
target for small t ≈0 (i.e., when the policy is taking its first few actions) is relatively high. As t
grows, the bias seems to spike upwards significantly and the episode terminates shortly thereafter,
indicating an unsuccessful rollout."
REFERENCES,0.7596685082872928,"Hence, it is evident that when the true ARTG closely follows the prescribed constant ARTG target,
the policy consistently achieves state-of-the-art performance. However, whenever the target ARTG
underestimates or overestimates the true ARTG by a margin of greater than 0.4, the rollout tends to
be unsuccessful, often achieving less than half the return compared to successful rollouts."
REFERENCES,0.7624309392265194,"To that end, the reward waypoint network uses a neural network formulation to estimate Ra (i.e.,
without using a constant estimator θa) based on the state st and the target return ω. By training
the neural network to provide a less biased estimate of the ARTG, we show that we can achieve
substantial performance improvements over a constant estimate of ARTG on the same task. As shown
in Table 6, RvS-R and a constant ARTG both exhibit significantly lower average performance and
greater variability compared to WT (with a reward waypoint network)."
REFERENCES,0.7651933701657458,"Table 6: Normalized evaluation scores for different policies and ARTG estimation techniques on the
hopper-medium-replay-v2 task."
REFERENCES,0.7679558011049724,"Technique
Normalized Score"
REFERENCES,0.7707182320441989,"Transformer (constant ARTG)
66.5 ± 15.6
RvS-R (constant ARTG)
73.5 ± 12.8
WT (waypoint network)
88.9 ± 2.4"
REFERENCES,0.7734806629834254,"Analysis of CRTG
Consider a similar decomposition of the error using Theorem C.1 of the CRTG.
Though neither the bias nor the variance of ˆRc are necessarily zero, it is important to note that the
variance term can be large even if the bias is zero, i.e., if θc = E[PT
t′=1 γtrt]."
REFERENCES,0.7762430939226519,"Corresponding to a best-case scenario where ˆRc is unbiased, we consider a bound of the expected
error constructed using only the variance term. For simplicity, we assume that we incur the worst-case
variance term with minimal covariance between Rc and ˆRc. The resulting dominant quantities are
the variance of ˆRc and the irreducible variance of Rc."
REFERENCES,0.7790055248618785,"Eτ[(Rc(τ, t) −ˆR(τ, t))2] = E[R(τ, t) −ˆR(τ, t)]2 + Var[ ˆR(τ, t) −R(τ, t)]"
REFERENCES,0.7817679558011049,"≥Var[ ˆR(τ, t) −R(τ, t)] ≈Var[ t
X"
REFERENCES,0.7845303867403315,"t′=1
γt′rt′] + Var[R(τ, t)]"
REFERENCES,0.787292817679558,"0
200
400
600
800
1000
Timestep (t) 0 10000 20000 30000 40000 50000"
REFERENCES,0.7900552486187845,Variance
REFERENCES,0.7928176795580111,"Estimated CRTG
Predicted CRTG (WT)"
REFERENCES,0.7955801104972375,"Figure 8: Variance of estimated CRTG
(Equation 4) and predicted CRTG by
the reward waypoint network on the
hopper-medium-replay-v2."
REFERENCES,0.7983425414364641,"Prior work has shown that Monte Carlo estimates of return
exhibit high variance, motivating techniques such as us-
ing baseline networks for REINFORCE, k-step returns or
TD(λ) for TD-learning methods, etc., which aim to reduce
variance at the cost of potentially incurring bias Sutton and
Barto [1999]. In that vein, we demonstrate that our reward
waypoint network predicts a similar quantity as a baseline
network and inherits many of its desirable properties."
REFERENCES,0.8011049723756906,"Specifically,
while
the
baseline
network
predicts
PT
t′=t γtrt given st, we additionally condition our reward
waypoint network’s prediction on ω. In the offline RL
setting where datasets are often a mixture of suboptimal
and optimal trajectories, we require that the waypoint net-
work can differentiate between such trajectories. Hence,
by conditioning on the overall reward ω, the reward waypoint network is able to differentiate between
high and low return-achieving trajectories."
REFERENCES,0.8038674033149171,"Empirically, we demonstrate that the predicted CRTG from our reward waypoint network exhibits
lesser variance on hopper-medium-replay-v2 than the estimated CRTG (i.e., as in Equation 4).
As shown in Figure 8, the variance of both techniques are relatively similar until t ≈400, after which
the variance of the estimated CRTG appears to grow superlinearly as a function of t. In the worst
case, at t ≈T, the variance is nearly 5x larger than for the predicted CRTG."
REFERENCES,0.8066298342541437,"D
Additional Experiments"
REFERENCES,0.8093922651933702,"D.1
Analysis of Stitching Region Behavior"
REFERENCES,0.8121546961325967,"To add on to the analysis of the goal waypoint network presented in the main text, we analyze the
""failure"" regions of transformer policies with and without a goal waypoint network. That is, by
determining the final locations of the agent, we can examine where the agent ended up instead of the
target location. Similar to the analysis in Section 6.3, this analysis can inform the stitching capability
of our methods."
REFERENCES,0.8149171270718232,"Figure 9: End locations for antmaze-large-play-v2 during 100 rollouts of left: WT and right:
global goal-conditioned transformer policy."
REFERENCES,0.8176795580110497,"Based on Figure 9, it is clear that the WT does not get ""stuck"" (e.g., after taking the wrong turn)
as often as the policy conditioned on global-goals. Moreover, the number of ants ending up near
the beginning portions of the maze (i.e., the bottom left) is significantly smaller for WT, which
contributes to its doubled success rate. We believe these are primarily attributable to the guidance
provided by the goal waypoint network through a consistent set of intermediate goals to reach the
target location at evaluation time."
REFERENCES,0.8204419889502762,"Interestingly, we observe that WT displays an increased rate of failure around the final turn relative to
other regions in the maze. As there is a relative lack of density in other failure regions closer to the
beginning of the maze, we hypothesize that some rollouts may suffer from the ant getting ""stuck"" at
challenging critical points in the maze, as defined in Kumar et al. [2022]. This indicates an interesting
direction of exploration for future work and a technique to combat this could result in policies with
nearly 100% success rate in completing antmaze-large-play-v2."
REFERENCES,0.8232044198895028,"D.2
Target Reward Interpolation"
REFERENCES,0.8259668508287292,"Unlike traditional value-based approaches,
RvS methods such as DT, RvS, and WT
present a simple methodology to condition the policy to achieve a particular target re-
turn.
In theory, this allows RvS to achieve performance corresponding to the desired per-
formance level, provided that it has non-zero coverage within the offline training dataset."
REFERENCES,0.8287292817679558,"40
60
80
100
120
Target Normalized Return 20 40 60 80 100 120"
REFERENCES,0.8314917127071824,Achieved Normalized Return
REFERENCES,0.8342541436464088,"WT
RvS-R"
REFERENCES,0.8370165745856354,"Figure 10: Achieved normalized score
versus the target normalized score for
walker2d-medium-expert-v2
task
on WT and RvS-R."
REFERENCES,0.8397790055248618,"To examine RvS’s capability in this regard, we replicate
an analysis performed in Emmons et al. [2021] to examine
whether RvS can interpolate between modes in the data to
achieve a particular target return. In this case, we compare
WT to RvS-R on the walker2d-medium-expert-v2
task, in which the data is composed of two modes of policy
performance (i.e., medium and expert). The mode corre-
sponding to the ""medium"" data is centered at a normalized
score of 80, whereas ""expert"" performance is located at a
normalized score of 110."
REFERENCES,0.8425414364640884,"Based on Figure 10, WT shows a reasonably improved
ability to interpolate in some regions than RvS-R. Where
RvS-R displays a failure to interpolate from normalized
targets of 30-60 and WT does not, both tend to be unable
to interpolate between 70-100 (i.e., between the modes of
the dataset)."
REFERENCES,0.8453038674033149,"Although the reasons for the failure in interpolation are unclear across two RvS methods, it is a
worthwhile analysis for future work. We hypothesize that techniques such as oversampling may
mitigate this issue as this may simply be linked to low frequency in the data distribution."
REFERENCES,0.8480662983425414,"D.3
Comparisons to Manual Waypoint Selection"
REFERENCES,0.850828729281768,"We compare the performance of the proposed goal waypoint network with a finite set of manual
waypoints, hand-selected based on prior oracular knowledge about the critical points within the maze
for achieving success (i.e., turns, midpoints). Based on the selected manual waypoints, shown in
Figure 11, we use a simple algorithm to provide intermediate targets Φt based on a distance-based
sorting approach, shown in Algorithm 2."
REFERENCES,0.8535911602209945,"Figure 11: Manually selected waypoints (blue pluses) for antmaze-large-play-v2, the chosen
task to evaluate the proposed approach. As before, the start location is marked with a maroon dot,
and the target location is marked wit a gold star."
REFERENCES,0.856353591160221,"Algorithm 2 Manual waypoint selection with Wm and st using L2 distance and a given global goal
ω."
REFERENCES,0.8591160220994475,"Wc ←{wm : ||wm −ω||2 ≤||st −ω||2} {consider waypoints that brings agent closer to ω}
return arg minwc∈Wc ||wc −st||2"
REFERENCES,0.861878453038674,"With all configuration and hyperparameters identical to WT, we compare the performance of a global
goal-conditioned policy, WT with manual waypoints, and WT with the goal waypoint network on
antmaze-large-play-v2 in Table 7."
REFERENCES,0.8646408839779005,"The results demonstrate that WT clearly outperforms manual waypoint selection in succeeding in
the AntMaze Large environment. However, while comparing a global-goal conditioned policy and a
policy conditioned on manual waypoints, it is clear that the latter improves upon average performance
and variability across initialization seeds. We believe that this illustrates that (a) waypoints, whether
manual or generated, tend to improve performance of the policy and (b) finer-grained waypoints
provide more valuable information for the policy to succeed more."
REFERENCES,0.8674033149171271,"Table 7:
Normalized evaluation scores for different waypoint selection techniques on the
antmaze-large-play-v2 task."
REFERENCES,0.8701657458563536,"Technique
Normalized Score"
REFERENCES,0.8729281767955801,"No Waypoints
33.0 ± 10.3
Manual Waypoints
44.5 ± 2.8
Waypoint Network
72.5 ± 2.8"
REFERENCES,0.8756906077348067,"We believe that this provides further verification and justification for both the generation of intermedi-
ate targets and the procedure of generation through a goal waypoint network that performs k-step
prediction."
REFERENCES,0.8784530386740331,"D.4
Delayed Rewards"
REFERENCES,0.8812154696132597,"An important case to consider for reward-conditioned tasks is when the rewards are delayed (often,
provided at the end of the trajectory). By providing nearly no intermediate targets, it is often chal-
lenging to complete these tasks. To verify that our design choices to construct modeled intermediate"
REFERENCES,0.8839779005524862,"targets does not reduce performance on configurations in which such information is unavailable, we
evaluate WT on 3 MuJoCo tasks where all rewards are provided at the final timestep. Moreover,
we ablate the choice of removing actions from WT, which may have an effect on the performance
considering that intermediate rewards are not provided."
REFERENCES,0.8867403314917127,"To compare our performance with other methods, we report results for CQL, DT, and QDT
from Yamagata et al. [2022] for all non-expert tasks and from Chen et al. [2021] from
hopper-medium-expert-v2 in Table 8. The results demonstrate that our method is relatively
comparable to DT in performance and that removing actions does not significantly affect performance
in these cases. All BC methods significantly outperform CQL across the chosen delayed reward tasks."
REFERENCES,0.8895027624309392,"Table 8: Normalized scores across 5 seeds on delayed reward MuJoCo v2 tasks, where all rewards are
provided at the final timestep, on WT with action conditioning (at) and without action conditioning."
REFERENCES,0.8922651933701657,"Environment
CQL
DT
QDT
WT (at)
WT
hopper-medium-expert-v2
9.0
107.3 ± 3.5
-
103.9 ± 3.0
104.2 ± 2.4
halfcheetah-medium-replay-v2
7.8 ± 6.9
33.0 ± 4.8
32.8 ± 7.8
30.6 ± 2.6
30.9 ± 2.8
walker2d-medium-v2
0.0 ± 0.4
69.9 ± 2.0
63.7 ± 6.4
71.5 ± 0.9
70.8 ± 1.7"
REFERENCES,0.8950276243093923,"E
Baseline Reproduction"
REFERENCES,0.8977900552486188,"We reproduce the TD3+BC results reported in Fujimoto and Gu [2021] and subsequently in Kostrikov
et al. [2021], Emmons et al. [2021], which are not reproduced by Liu and Abbeel [2023]. For 8
MuJoCo tasks chosen from the same evaluation tasks as WT, we show the normalized score of
TD3+BC and the reported mean score as a function of the number of iterations of training. For all of
the tasks, it is evident that our reproduction aligns with the original reported result, whereas those
in Liu and Abbeel [2023] are consistently higher. We hypothesize that these findings are similar in
nature to our inability to reproduce results for DT in 1."
REFERENCES,0.9005524861878453,Iterations 40 60 80
REFERENCES,0.9033149171270718,Normalized Score
REFERENCES,0.9060773480662984,halfcheetah-medium-expert-v2
REFERENCES,0.9088397790055248,"Reported in WT
Reported in AT"
REFERENCES,0.9116022099447514,Iterations 41 42 43 44 45 46
REFERENCES,0.914364640883978,Normalized Score
REFERENCES,0.9171270718232044,halfcheetah-medium-replay-v2
REFERENCES,0.919889502762431,"Reported in WT
Reported in AT"
REFERENCES,0.9226519337016574,"Iterations
46.5 47.0 47.5 48.0 48.5 49.0"
REFERENCES,0.925414364640884,Normalized Score
REFERENCES,0.9281767955801105,halfcheetah-medium-v2
REFERENCES,0.930939226519337,"Reported in WT
Reported in AT"
REFERENCES,0.9337016574585635,Iterations 40 60 80 100
REFERENCES,0.93646408839779,Normalized Score
REFERENCES,0.9392265193370166,hopper-medium-expert-v2
REFERENCES,0.9419889502762431,"Reported in WT
Reported in AT"
REFERENCES,0.9447513812154696,"0.0
0.2
0.4
0.6
0.8
1.0
Iterations
1e6 20 40 60 80 100"
REFERENCES,0.9475138121546961,Normalized Score
REFERENCES,0.9502762430939227,hopper-medium-replay-v2
REFERENCES,0.9530386740331491,"Reported in WT
Reported in AT"
REFERENCES,0.9558011049723757,"0.0
0.2
0.4
0.6
0.8
1.0
Iterations
1e6 45 50 55 60 65 70"
REFERENCES,0.9585635359116023,Normalized Score
REFERENCES,0.9613259668508287,hopper-medium-v2
REFERENCES,0.9640883977900553,"Reported in WT
Reported in AT"
REFERENCES,0.9668508287292817,"0.0
0.2
0.4
0.6
0.8
1.0
Iterations
1e6 85 90 95 100 105 110"
REFERENCES,0.9696132596685083,Normalized Score
REFERENCES,0.9723756906077348,walker2d-medium-expert-v2
REFERENCES,0.9751381215469613,"Reported in WT
Reported in AT"
REFERENCES,0.9779005524861878,"0.0
0.2
0.4
0.6
0.8
1.0
Iterations
1e6 40 60 80"
REFERENCES,0.9806629834254144,Normalized Score
REFERENCES,0.9834254143646409,walker2d-medium-replay-v2
REFERENCES,0.9861878453038674,"Reported in WT
Reported in AT"
REFERENCES,0.988950276243094,"Figure 12: Reproductions of TD3+BC baseline for 8 MuJoCo tasks across 5 seeds for 1M gradient
steps, showing that all our reported results (blue, dotted line) lie within the reproduced confidence
intervals and most of the results reported in Liu and Abbeel [2023] (red, dotted line) do not."
REFERENCES,0.9917127071823204,"Moreover, to attempt to reduce the possible effect of hyperparameters on the performance difference,
we sweep 16 combinations of the number of actor and critic layers based on the original codebase
used by the TD3+BC authors here: https://github.com/sfujim/TD3_BC. Note that their original
implementation features a hardcoded 2 layer neural network for actor and critic. In 9, we show that
no combination of hyperparameters for the number of actor and critic layers is sufficient to reach the
performance reported by Liu and Abbeel [2023] and that as the number of layers increases beyond 3
or decreases beyond 2, the performance decreases on hopper-medium-replay-v2."
REFERENCES,0.994475138121547,"Table 9: Average normalized score of TD3+BC baseline across 5 seeds by varying number of actor
(LA) and critic network hidden layers (LC) on hopper-medium-replay-v2."
REFERENCES,0.9972375690607734,"LC
LA
1
2
3
4
1
48.7
80.1
73.1
73.9
2
55.2
64.4
50.2
61.3
3
59.8
59.1
69.6
64.6
4
42.9
60.5
63.6
54.4"
