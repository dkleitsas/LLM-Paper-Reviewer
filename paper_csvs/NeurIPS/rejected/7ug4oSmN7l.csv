Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011806375442739079,"Recently, neural networks (NN) have made great strides in combinatorial optimiza-
1"
ABSTRACT,0.0023612750885478157,"tion problems (COPs). However, they face challenges in solving the capacitated
2"
ABSTRACT,0.0035419126328217238,"arc routing problem (CARP) which is to find the minimum-cost tour that covers all
3"
ABSTRACT,0.004722550177095631,"required edges on a graph, while within capacity constraints. Actually, NN-based
4"
ABSTRACT,0.0059031877213695395,"approaches tend to lag behind advanced metaheuristics due to complexities caused
5"
ABSTRACT,0.0070838252656434475,"by non-Euclidean graph, traversal direction and capacity constraints. In this
6"
ABSTRACT,0.008264462809917356,"paper, we introduce an NN-based solver tailored for these complexities, which
7"
ABSTRACT,0.009445100354191263,"significantly narrows the gap with advanced metaheuristics while with far less
8"
ABSTRACT,0.010625737898465172,"runtimes. First, we propose the direction-aware attention model (DaAM) to in-
9"
ABSTRACT,0.011806375442739079,"corporate directionality into the embedding process, facilitating more effective
10"
ABSTRACT,0.012987012987012988,"one-stage decision-making. Second, we design a supervised reinforcement learning
11"
ABSTRACT,0.014167650531286895,"scheme that involves supervised pre-training to establish a robust initial policy for
12"
ABSTRACT,0.015348288075560802,"subsequent reinforcement fine-tuning. It proves particularly valuable for solving
13"
ABSTRACT,0.01652892561983471,"CARP that has a higher complexity than the node routing problems (NRPs). Finally,
14"
ABSTRACT,0.01770956316410862,"a path optimization method is introduced to adjust the depot return positions within
15"
ABSTRACT,0.018890200708382526,"the path generated by DaAM. Experiments show that DaAM surpasses heuristics
16"
ABSTRACT,0.020070838252656435,"and achieves decision quality comparable to state-of-the-art metaheuristics for the
17"
ABSTRACT,0.021251475796930343,"first time while maintaining superior efficiency, even in large-scale CARP instances.
18"
ABSTRACT,0.02243211334120425,"The code and datasets are provided in the Appendix.
19"
INTRODUCTION,0.023612750885478158,"1
Introduction
20"
INTRODUCTION,0.024793388429752067,"The capacitated arc routing problem (CARP) [7] is a combinatorial optimization problem, frequently
21"
INTRODUCTION,0.025974025974025976,"arising in domains such as inspection and search-rescue operations. Theoretically, the CARP is
22"
INTRODUCTION,0.02715466351829988,"established on an undirected connected graph G = (V, E, ER) that includes a set of nodes V
23"
INTRODUCTION,0.02833530106257379,"connected by a set of edges E, and an edge subset ER âŠ†E that needs to be served, called required
24"
INTRODUCTION,0.0295159386068477,"edges. Each required edge has a demand value which spends the capacity of the vehicle when it is
25"
INTRODUCTION,0.030696576151121605,"served. In this context, all vehicles start their routes from the depot node depot âˆˆV and conclude
26"
INTRODUCTION,0.031877213695395513,"their journey by returning to the same depot. The main goal of a CARP solver is to serve all required
27"
INTRODUCTION,0.03305785123966942,"edges with the lowest total path cost, while ensuring that the vehicle does not exceed its capacity Q.
28"
INTRODUCTION,0.03423848878394333,"According to [7], the CARP is recognized as an NP-Hard problem. Among all solver solutions, the
29"
INTRODUCTION,0.03541912632821724,"Memetic Algorithms (MA) [15, 25], first proposed in 2005, have still maintained unrivaled results in
30"
INTRODUCTION,0.03659976387249114,"solving the CARP challenge to this day. However, they have struggled with high time costs and the
31"
INTRODUCTION,0.03778040141676505,"exponential growth of the search space as the problem scale increases. Compared to the traditional
32"
INTRODUCTION,0.03896103896103896,"methods, NN-based solvers [16, 10, 20] are faster with the assistance of GPU, have therefore gained
33"
INTRODUCTION,0.04014167650531287,"increasing attention in recent years. However, unlike the decent performance in solving NRP, such as
34"
INTRODUCTION,0.04132231404958678,"vehicle routing problem (VRP) and travelling salesman problem (TSP), or ARP defined in Euclidean
35"
INTRODUCTION,0.04250295159386069,"graphs, such as rural postman problem (RPP) and Chinese postman problem (CPP), NN-based
36"
INTRODUCTION,0.043683589138134596,"methods usually lags far behind the traditional ones in solving CARP. This discrepancy is attributed
37"
INTRODUCTION,0.0448642266824085,"to the inability of existing methods to effectively reduce the high complexity of solving CARP:
38"
INTRODUCTION,0.04604486422668241,"â€¢ Lack of edge direction in embedding learning: ARP solvers need to determine the edges
39"
INTRODUCTION,0.047225501770956316,"to be traversed along with the direction of traversal, easy for humans to achieve in one step
40"
INTRODUCTION,0.048406139315230225,"but extremely challenging for computers. Existing methods didnâ€™t encode edge directionality
41"
INTRODUCTION,0.049586776859504134,"in embedding, making them have to build edge sequences and determine edgesâ€™ directions
42"
INTRODUCTION,0.05076741440377804,"separately and leading to path generation without sufficient consideration.
43"
INTRODUCTION,0.05194805194805195,"â€¢ Ineffective learning for solving CARP: CARP is more complex than NRPs and Euclidean
44"
INTRODUCTION,0.053128689492325853,"ARPs owing to the non-Euclidean input, edge direction, and capacity constraints. Thus,
45"
INTRODUCTION,0.05430932703659976,"learning methods for NRPs and Euclidean ARPs cannot be directly transferred to solve
46"
INTRODUCTION,0.05548996458087367,"CARP or work well even if adapted, leaving a lack of effective learning strategies for CARP.
47"
INTRODUCTION,0.05667060212514758,"In this paper, we aim to address both above issues and propose an NN-based solver for CARP that
48"
INTRODUCTION,0.05785123966942149,"competes with the state-of-the-art MA [25] while with far less runtimes. Firstly, we propose the
49"
INTRODUCTION,0.0590318772136954,"direction-aware attention model (DaAM). It computes embeddings for directed arcs decomposed
50"
INTRODUCTION,0.0602125147579693,"from undirected edges to align with the nature of ARP, thus avoiding missing direction information
51"
INTRODUCTION,0.06139315230224321,"and enabling concise one-stage decision-making. Secondly, we design a supervised reinforcement
52"
INTRODUCTION,0.06257378984651712,"learning method to learn effective heuristics for solving CARP. DaAM is pre-trained to learn an initial
53"
INTRODUCTION,0.06375442739079103,"policy by minimizing the difference from the decisions made by experts, and fine-tuned on larger-
54"
INTRODUCTION,0.06493506493506493,"scale CARP instances by Proximal Policy Optimization with self-critical strategy. Finally, to further
55"
INTRODUCTION,0.06611570247933884,"boost the path quality, we propose a path optimizer (PO) to re-decide the vehiclesâ€™ optimal return
56"
INTRODUCTION,0.06729634002361275,"positions by dynamic programming. In the experiments, our method approaches the state-of-the-art
57"
INTRODUCTION,0.06847697756788666,"MA with an average gap of 5% and is 4% better than the latest heuristics and gains high efficiency.
58"
RELATED WORK,0.06965761511216056,"2
Related Work
59"
GRAPH EMBEDDING LEARNING,0.07083825265643448,"2.1
Graph Embedding Learning
60"
GRAPH EMBEDDING LEARNING,0.07201889020070838,"Graph embedding [3] aims to map nodes or edges in a graph to a low-dimensional vector space.
61"
GRAPH EMBEDDING LEARNING,0.07319952774498228,"This process is commonly achieved via graph neural networks (GNNs) [31]. Kipf et al. [13]
62"
GRAPH EMBEDDING LEARNING,0.0743801652892562,"introduced graph convolutional operations to aggregate information from neighboring nodes for
63"
GRAPH EMBEDDING LEARNING,0.0755608028335301,"updating node representations. Unlike GCN, GAT [27] allowed dynamic node attention during
64"
GRAPH EMBEDDING LEARNING,0.07674144037780402,"information propagation by attention mechanisms. Other GNN variants [9, 30] exhibited a similar
65"
GRAPH EMBEDDING LEARNING,0.07792207792207792,"information aggregation pattern but with different computational approaches. In this paper, since
66"
GRAPH EMBEDDING LEARNING,0.07910271546635184,"an arc is related to the outgoing arc of its endpoint but irrelevant to the incoming arc of that, we use
67"
GRAPH EMBEDDING LEARNING,0.08028335301062574,"attention mechanisms to capture the intricate relationships between arcs for arc embedding learning.
68"
LEARNING FOR ROUTING PROBLEMS,0.08146399055489964,"2.2
Learning for Routing Problems
69"
LEARNING FOR ROUTING PROBLEMS,0.08264462809917356,"The routing problem is one of the most classic COPs, and it is mainly categorized into two types
70"
LEARNING FOR ROUTING PROBLEMS,0.08382526564344746,"according to the decision element: node routing problems and arc routing problems.
71"
LEARNING FOR ROUTING PROBLEMS,0.08500590318772137,"Node routing problems (NRPs), such as TSP and VRP, aim to determine the optimal paths that
72"
LEARNING FOR ROUTING PROBLEMS,0.08618654073199528,"traverse all nodes in the Euclidean space or graphs. As the solutions to these problems are context-
73"
LEARNING FOR ROUTING PROBLEMS,0.08736717827626919,"dependent sequences of variable size, they cannot be directly modeled by the Seq2Seq model [24].
74"
LEARNING FOR ROUTING PROBLEMS,0.0885478158205431,"To address this problem, Vinyals et al. [28] proposed the Pointer network (PN) to solving Euclidean
75"
LEARNING FOR ROUTING PROBLEMS,0.089728453364817,"TSP, which achieves variable-size output dictionaries by neural attention. Due to the scarcity of labels
76"
LEARNING FOR ROUTING PROBLEMS,0.09090909090909091,"for supervised learning, Bello et al. [2] modeled the TSP as a single-step reinforcement learning
77"
LEARNING FOR ROUTING PROBLEMS,0.09208972845336481,"problem and trained the PN using policy gradient [29] within Advantage Actor-Critic (A3C) [17]
78"
LEARNING FOR ROUTING PROBLEMS,0.09327036599763873,"framework. Nazari et al. [19] replaced the LSTM encoder in PN with an element-wise projection
79"
LEARNING FOR ROUTING PROBLEMS,0.09445100354191263,"layer and proposed the first NN-based method to solve the Euclidean VRP and its variants. To better
80"
LEARNING FOR ROUTING PROBLEMS,0.09563164108618655,"extract correlations between inputs, Kool et al. [14] utilized multi-head attention for embedding
81"
LEARNING FOR ROUTING PROBLEMS,0.09681227863046045,"learning and trained the model using REINFORCE [29] with a greedy baseline. To solve COPs
82"
LEARNING FOR ROUTING PROBLEMS,0.09799291617473435,"defined on graphs, Khalil et al. [11] proposed S2V-DQN to learn heuristics, employing structure2vec
83"
LEARNING FOR ROUTING PROBLEMS,0.09917355371900827,"[5] for graph embedding learning and n-step DQN [18] for training. While the mentioned NN-based
84"
LEARNING FOR ROUTING PROBLEMS,0.10035419126328217,"approaches have achieved comparable performance to metaheuristics, they cannot be directly applied
85"
LEARNING FOR ROUTING PROBLEMS,0.10153482880755609,"to solve ARP due to the modeling differences between ARP and NRP.
86"
LEARNING FOR ROUTING PROBLEMS,0.10271546635182999,"Arc routing problems (ARPs) involve determining optimal paths for traversing arcs or edges in
87"
LEARNING FOR ROUTING PROBLEMS,0.1038961038961039,"graphs, with variants like RPP, CPP, and CARP. Truong et al. [26] proposed a DRL framework to
88"
LEARNING FOR ROUTING PROBLEMS,0.1050767414403778,"address the CPP with load-dependent costs on Euclidean graphs and achieved better solution quality
89"
LEARNING FOR ROUTING PROBLEMS,0.10625737898465171,"than metaheuristics. However, CARPs are defined on non-Euclidean graphs. Unlike Euclidean graphs
90"
LEARNING FOR ROUTING PROBLEMS,0.10743801652892562,"with given node coordinates, non-Euclidean graphs require manually extracting and aggregating the
91"
LEARNING FOR ROUTING PROBLEMS,0.10861865407319952,"node representations, a task that is typically learnable. Although several NN-based algorithms have
92"
LEARNING FOR ROUTING PROBLEMS,0.10979929161747344,"been proposed, they still lag significantly behind traditional methods. Li et al. [16] pioneered the use
93"
LEARNING FOR ROUTING PROBLEMS,0.11097992916174734,"of the NN-based approach in solving the CARP by transforming it into an NRP. They first determined
94"
LEARNING FOR ROUTING PROBLEMS,0.11216056670602124,"the sequence of edges and then decided the traversal direction for each edge. Hong et al. [10] trained a
95"
LEARNING FOR ROUTING PROBLEMS,0.11334120425029516,"PN in a supervised manner to select undirected edges in each time step, and also determined the edge
96"
LEARNING FOR ROUTING PROBLEMS,0.11452184179456906,"traversal direction as post-processing. Ramamoorthy et al. [20] proposed to generate an initial tour
97"
LEARNING FOR ROUTING PROBLEMS,0.11570247933884298,"based on edge embeddings and then split it into routes within capacity constraint. These approaches
98"
LEARNING FOR ROUTING PROBLEMS,0.11688311688311688,"lack edge directionality encoding, leading to edge selection without sufficient consideration and
99"
LEARNING FOR ROUTING PROBLEMS,0.1180637544273908,"necessitating a two-stage decision process or an additional splitting procedure.
100"
BACKGROUND,0.1192443919716647,"3
Background
101"
BACKGROUND,0.1204250295159386,"The attention model (AM) [14] exhibits superior effectiveness in solving classic Euclidean COPs due
102"
BACKGROUND,0.12160566706021252,"to its attention mechanisms for extracting correlations between inputs. Therefore, we use the AM
103"
BACKGROUND,0.12278630460448642,"as the backbone and give a brief review in terms of the TSP. Given an Euclidean graph G=(V, E),
104"
BACKGROUND,0.12396694214876033,"the AM defines a stochastic policy, denoted as Ï€(x|S), where x = (x0, ..., x|V|âˆ’1) represents a
105"
BACKGROUND,0.12514757969303425,"permutation of the node indexes in V, and S is the problem instance expressing G. The AM is
106"
BACKGROUND,0.12632821723730814,"parameterized by Î¸ as:
107"
BACKGROUND,0.12750885478158205,"Ï€Î¸(x|S) =
Y|V|"
BACKGROUND,0.12868949232585597,"t=1 Ï€Î¸(xt|S, x0:tâˆ’1)
(1)"
BACKGROUND,0.12987012987012986,"where t denotes the time step. Specifically, the AM comprises an encoder and a decoder. The
108"
BACKGROUND,0.13105076741440377,"encoder first computes initial dh-dimensional embeddings for each node in V as h0
i through a learned
109"
BACKGROUND,0.1322314049586777,"linear projection. It then captures the embeddings of h0
i using multiple attention layers, with each
110"
BACKGROUND,0.1334120425029516,"comprising a multi-head attention (MHA) sublayer and a node-wise feed-forward (FF) sublayer.
111"
BACKGROUND,0.1345926800472255,"Both types of sublayers include a skip connection and batch normalization (BN). Assuming that
112"
BACKGROUND,0.1357733175914994,"l âˆˆ{1, ..., N} denotes the attention layer, the lth layer can be formulated as hl
i:
113"
BACKGROUND,0.13695395513577333,"hl
i = BNl( Ë†hi + FFl( Ë†hi));
Ë†hi = BNl(hlâˆ’1
i
+ MHAl
i(hlâˆ’1
0
, . . . , hlâˆ’1
|V|âˆ’1))
(2)"
BACKGROUND,0.1381345926800472,"The decoder aims to append a node to the sequence x at each time step. Specifically, a context
114"
BACKGROUND,0.13931523022432113,"embedding h(c) is computed to represent the state at the time step t. Then a single attention head is
115"
BACKGROUND,0.14049586776859505,"used to calculate the probabilities for each node based on h(c):
116"
BACKGROUND,0.14167650531286896,u(c)j =
BACKGROUND,0.14285714285714285,"(
C Â· tanh

d
âˆ’1"
BACKGROUND,0.14403778040141677,"2
h [WQh(c)]T WKhN
j

if j Ì¸= xtâ€²(âˆ€tâ€² < t)"
BACKGROUND,0.14521841794569068,"âˆ’âˆž
otherwise,"
BACKGROUND,0.14639905548996457,"pi = Ï€Î¸(xt = i|S, x0:tâˆ’1) = u(c)i/
X"
BACKGROUND,0.14757969303423848,"j u(c)j
(3)"
BACKGROUND,0.1487603305785124,"where WQ and WK are the learnable parameters of the last attention layer. u(c)j is an unnormalized
117"
BACKGROUND,0.14994096812278632,"log probability with (c) indicating the context node. C is a constant, and pi is the probability
118"
BACKGROUND,0.1511216056670602,"distribution computed by the softmax function based on u(c)j.
119"
METHOD,0.15230224321133412,"4
Method
120"
DIRECTION-AWARE ATTENTION MODEL,0.15348288075560804,"4.1
Direction-aware Attention Model
121"
DIRECTION-AWARE ATTENTION MODEL,0.15466351829988192,"In this section, we propose the direction-aware attention model (DaAM). Unlike previous methods
122"
DIRECTION-AWARE ATTENTION MODEL,0.15584415584415584,"that separately learn edge embeddings and determine edge directions, our model encodes direction
123"
DIRECTION-AWARE ATTENTION MODEL,0.15702479338842976,"information directly into the embedding, enabling one-stage decision-making. As shown in Fig. 1,
124"
DIRECTION-AWARE ATTENTION MODEL,0.15820543093270367,"the DaAM makes sequential decisions in two phases to select arcs. The first phase is a one-time
125"
DIRECTION-AWARE ATTENTION MODEL,0.15938606847697756,"transformation process, in which the arcs of the input graph are represented as nodes in the new
126"
DIRECTION-AWARE ATTENTION MODEL,0.16056670602125148,"directed complete graph. The second phase is executed at each time step, in which GAT is used to
127"
DIRECTION-AWARE ATTENTION MODEL,0.1617473435655254,"aggregate the inter-arc weights. Subsequently, AM is used to select the arc of the next action.
128"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.16292798110979928,"4.1.1
Arc Feature Formulation via Graph Transformation
129"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1641086186540732,"Graph Transformation
Motivated by the need to consider direction when traversing edges, we
130"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1652892561983471,"explicitly encode the edge direction by edge-to-arc decomposition. Let G=(V, E, ER) denotes
131"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.16646989374262103,Feature of Arc
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.16765053128689492,Node Embedding
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.16883116883116883,Context Node
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.17001180637544275,Probability
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.17119244391971664,Attention Queries
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.17237308146399055,Prediction
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.17355371900826447,Node in ð•
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.17473435655253838,Edge in ð„
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.17591499409681227,Arcs in ð´
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1770956316410862,Directed Edge in ð¸
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1782762691853601,Arc nodes in ðº
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.179456906729634,Execute on Each Decision
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1806375442739079,Attention Model
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.18181818181818182,(Encoder)
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.18299881936245574,Attention Model
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.18417945690672963,"(Decoder) â„Ž("")"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.18536009445100354,"$%&
â„Ž("") $ â„Ž'"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.18654073199527746,"'
ð‘'
â„Ž' $ â„Ž&"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.18772136953955135,"'
ð‘&
â„Ž& $ â„Ž("
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.18890200708382526,"'
ð‘(
â„Ž($ â„Ž)"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.19008264462809918,"'
ð‘)
â„Ž) $"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1912632821723731,"Graph Transformation (Execute Once) ð‘“* ð† ðž!""
ð‘› ð‘š ðº ð‘Žð‘Ÿð‘!"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.19244391971664698,"ð‘Žð‘Ÿð‘""
ð‘’#$
â€¦ ð‘Žð‘Ÿð‘!"" ð‘Žð‘Ÿð‘""! ð‘› ð‘š"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1936245572609209,"depot
MDS
|ð‘’#$|"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.19480519480519481,"Graph
Attention |ð‘’$$| |ð‘’%$|"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.1959858323494687,"|ð‘’&$|
ð‘“' ð‘“& ð‘“( ð‘“)"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.19716646989374262,"Figure 1: DaAM Pipeline consists of two parts. The first part transforms the input graph G by
treating the arcs on G as nodes of a new directed graph G, only executing once. The second part
leverages the GAT and AM to update arc embeddings and select arcs, executing at each time step."
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.19834710743801653,"the undirected connected graph as input, where V is the node set of G, E is the edge set of G,
132"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.19952774498229045,"and ER âŠ†E is the required edge set. Firstly, given that an edge has two potential traversal
133"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.20070838252656434,"directions, we decompose each edge enm =(costnm, demandnm, allow_servenm)âˆˆER into two
134"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.20188902007083825,"arcs {arcnm, arcmn} with opposite directions but the same cost, demand and serving state. Here
135"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.20306965761511217,"n, m are the indexes of node in V. To simplify the representation below, we replace nm and mn
136"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.20425029515938606,"with single-word symbols, such as i and j. In this way of edge decomposition, we obtain a set of arcs
137"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.20543093270365997,"denoted as AR. Secondly, we build a new graph G = (AR, E). Specifically, each arc in AR serves
138"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.2066115702479339,"as a node in G, and directed edge set E is created, with eij âˆˆE representing the edge from node
139"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.2077922077922078,"arci to arcj. The weight |eij| represents the total cost of the shortest path from the end node of arci
140"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.2089728453364817,"to the start node of arcj. In addition, we treat the depot as a self-loop zero-demand arc that allows
141"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.2101534828807556,"for repeated serving, denoted as arc0. Consequently, we transform the input graph G into a directed
142"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.21133412042502953,"complete graph G. By decomposing all edges in ER into arcs, it is natural to directly select the arcs
143"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.21251475796930341,"from G during the decision-making. Given that the Floyd-Warshall algorithm is used to calculate the
144"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.21369539551357733,"shortest path cost between any pair of nodes in G, the time complexity of our graph transformation is
145"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.21487603305785125,"max(O(|ER|2), O(|V|3)).
146"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.21605667060212513,Table 1: Feature Detail of arci at time step t for CARP.
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.21723730814639905,"No.
Features
Field
Description
No.
Features
Field
Description"
ARC FEATURE FORMULATION VIA GRAPH TRANSFORMATION,0.21841794569067297,"1
is_depoti
F2
Is arci the depot?
5
allow_serve(i)
t
F2
Is arci at time step t allowed to serve?"
COSTI,0.21959858323494688,"2
costi
R+
Cost of arci.
6
mdsstart(i)
Rd
Euclidean coordinates of arciâ€™s start node."
DEMANDI,0.22077922077922077,"3
demandi
R+
Demand of arci.
7
mdsend(i)
Rd
Euclidean coordinates of arciâ€™s end node."
DEMANDI,0.22195985832349469,"4
|extâˆ’1i|
R+
Edge weight from arcxtâˆ’1 to arci."
DEMANDI,0.2231404958677686,"Arc Feature Formulation
To establish a foundation for decision-making regarding arc selection,
147"
DEMANDI,0.2243211334120425,"the features of the arcs are constructed as input for the subsequent model. Specifically, multi-
148"
DEMANDI,0.2255017709563164,"dimensional scaling (MDS) is used to project the input graph G into a d-dimensional Euclidean
149"
DEMANDI,0.22668240850059032,"space. The Euclidean coordinates of arciâ€™s start and end nodes, denoted as mdsstart(i) and mdsend(i),
150"
DEMANDI,0.22786304604486424,"are then taken as the features of arci to indicate its direction. As shown in Table 1, at time step t,
151"
DEMANDI,0.22904368358913813,"arci can be featured as:
152"
DEMANDI,0.23022432113341204,"F (i)
t
= (is_depoti, costi, demandi, |extâˆ’1i|, allow_serve(i)
t , mdsstart(i), mdsend(i))
(4)"
DEMANDI,0.23140495867768596,"where xtâˆ’1 is the index of the selected arc at the last time step, and t âˆˆ[1, +âˆž). Our feature models
153"
DEMANDI,0.23258559622195984,"arcs rather than edges and encodes the direction attribute of arcs through MDS. Therefore, it is more
154"
DEMANDI,0.23376623376623376,"suitable than previous methods [10, 16] for ARPs that need to consider the direction of traversing.
155"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.23494687131050768,"4.1.2
Arc Relation Encoding via Graph Attention Network
156"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.2361275088547816,"Although AM is efficient in decision-making, according to Eq. (2), it cannot encode the edge weights
157"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.23730814639905548,"between nodes in G, an important context feature, during learning. Therefore, we use graph attention
158"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.2384887839433294,"network (GAT) [27] to encode such weights. At each time step t, for each arc arci, we integrate the
159"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.2396694214876033,"weights between arci and all arcs in AR along with their features into the initial embedding of arci.
160"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.2408500590318772,"cij = softmax
 
Î±(W[ F (i)
t
|| F (j)
t
|| |eji| ])

;
h0
i = Ïƒ
  X|AR|âˆ’1"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.24203069657615112,"j=0
cijWF (j)
t

(5)"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.24321133412042503,"where W is a shared learnable parameter, [Â·||Â·] is the horizontal concatenation operator, Î±(Â·) is a
161"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.24439197166469895,"mapping from the input to a scalar, and Ïƒ(Â·) denotes the activation function. h0
i denotes the initial
162"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.24557260920897284,"feature embedding of arci, which is taken as the input of subsequent AM. Since G is a complete
163"
ARC RELATION ENCODING VIA GRAPH ATTENTION NETWORK,0.24675324675324675,"graph, we use one graph attention layer to avoid over-smoothing [4].
164"
ARC SELECTION VIA ATTENTION MODEL,0.24793388429752067,"4.1.3
Arc Selection via Attention Model
165"
ARC SELECTION VIA ATTENTION MODEL,0.24911452184179456,"After aggregating the edge weights of G into the initial embeddings, we utilize AM to learn the final
166"
ARC SELECTION VIA ATTENTION MODEL,0.2502951593860685,"arc embeddings and make arc selection decisions. In the encoding phase described by Eq.2, for each
167"
ARC SELECTION VIA ATTENTION MODEL,0.2514757969303424,"arc {arci}, we leverage N attention layers to process the initial embeddings {h0
i } and obtain the
168"
ARC SELECTION VIA ATTENTION MODEL,0.2526564344746163,"output embeddings of the N th layer, i.e., {hN
i }. In the decoding phase, we define the context node
169"
ARC SELECTION VIA ATTENTION MODEL,0.2538370720188902,"applicable to CARP:
170"
ARC SELECTION VIA ATTENTION MODEL,0.2550177095631641,"hN
(c) =
h
|AR|âˆ’1X|AR|âˆ’1"
ARC SELECTION VIA ATTENTION MODEL,0.256198347107438,"i=0
hN
i , hN
xtâˆ’1, Î´t, âˆ†t
i
, t âˆˆ[1, +âˆž)
(6)"
ARC SELECTION VIA ATTENTION MODEL,0.25737898465171194,"where xtâˆ’1 indicates the chosen arc index at time step t âˆ’1 and x0 is arc0. Î´t is the remaining
171"
ARC SELECTION VIA ATTENTION MODEL,0.2585596221959858,"capacity at time step t, âˆ†t = âˆ†(Î´t > Q"
ARC SELECTION VIA ATTENTION MODEL,0.2597402597402597,"2 ) is a variable to indicate whether the vehicleâ€™s remaining
172"
ARC SELECTION VIA ATTENTION MODEL,0.26092089728453366,"capacity exceeds half. Finally, according to Eq.(3), the decoder of AM takes the context node hN
(c)
173"
ARC SELECTION VIA ATTENTION MODEL,0.26210153482880755,"and arc embeddings {hN
i } as inputs and calculates the probabilities for all arcs, denoted as pi. The
174"
ARC SELECTION VIA ATTENTION MODEL,0.2632821723730815,"serviceable arc selected at time step t, i.e., arcxt, is determined by sampling or greedy decoding.
175"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2644628099173554,"4.2
Supervised Reinforcement Learning for CARP
176"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.26564344746162927,"The decision-making of selecting arcs can be modeled as a Markov decision process with the
177"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2668240850059032,"following symbols regarding reinforcement learning:
178"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2680047225501771,"â€¢ State st is the newest path of arcs selected from G: (arcx0, ..., arcxtâˆ’1), while the terminal state
179"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.269185360094451,"is sT with T indicating the final time step.
180"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.27036599763872493,"â€¢ Action at is the selected arc at time step t, i.e., arcxt. Selecting the action at would add arcxt to the
181"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2715466351829988,"end of the current path st and tag the corresponding arcs of arcxt with their features allow_serve
182"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2727272727272727,"changed to 0. Notably, arc0 can be selected repeatedly but not consecutively.
183"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.27390791027154665,"â€¢ Reward rt is obtained after taking action at at state st, which equals the negative shortest path
184"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.27508854781582054,"cost from the last arc arcxtâˆ’1 to the selected arc arcxt.
185"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2762691853600944,"â€¢ Stochastic policy Ï€(at|st) specifies the probability distribution over all actions at state st.
186"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.27744982290436837,"We parameterize the stochastic policy of DaAM with Î¸:
187"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.27863046044864226,"Ï€(xt| S, x0:tâˆ’1) = Ï€Î¸(at|st)
(7)"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2798110979929162,"where S is a CARP instance.
Starting from initial state s0, we get a trajectory Ï„
=
188"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2809917355371901,"(s0, a0, r0, ..., rT âˆ’1, sT ) using Ï€Î¸. The goal of learning is to maximize the cumulative reward:
189"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.282172373081464,"R(Ï„) = PT âˆ’1
t=0 rt. However, due to the high complexity of CARP, vanilla deep reinforcement learn-
190"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2833530106257379,"ing methods learn feasible strategies inefficiently. A natural solution is to minimize the difference
191"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2845336481700118,"between the modelâ€™s decisions and expert decisions. To achieve this, we employ supervised learning
192"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.2857142857142857,"to learn an initial policy based on labeled data and then fine-tune the model through reinforcement
193"
SUPERVISED REINFORCEMENT LEARNING FOR CARP,0.28689492325855964,"learning.
194"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.28807556080283353,"4.2.1
Supervised Pre-training via Multi-class Classification
195"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.2892561983471074,"In the pre-training stage, we consider arc-selection at each time step as a multi-class classification
196"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.29043683589138136,"task, and employ the state-of-the-art CARP method MAENS to obtain high-quality paths as the label.
197"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.29161747343565525,"Assuming that yt âˆˆR|AR| denotes the one-hot label vector at time step t of any path, with y(k)
t
198"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.29279811097992914,"indicating each element. We utilize the cross-entropy loss to train the policy represented in Eq. (7):
199"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.2939787485242031,"L = âˆ’
XT âˆ’1 t=0"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.29515938606847697,X|AR|âˆ’1
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.29634002361275086,"k=0
y(k)
t
log
 
Ï€Î¸(arck|st)

(8)"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.2975206611570248,"We use the policy optimized by cross-entropy, denoted as Ï€s, to initialize the policy network Ï€Î¸ and
200"
SUPERVISED PRE-TRAINING VIA MULTI-CLASS CLASSIFICATION,0.2987012987012987,"as the baseline policy Ï€b in reinforcement learning.
201"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.29988193624557263,"4.2.2
Reinforcement Fine-tuning via PPO with self-critical strategy
202"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3010625737898465,"During the fine-tuning phase, we use Proximal Policy Optimization (PPO) to optimize our model
203"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3022432113341204,"Ï€Î¸(at|st) due to its outstanding stability in policy updates. Considering the low sample efficiency in
204"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.30342384887839435,"reinforcement learning, we employ a training approach similar to self-critical training [21] to reduce
205"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.30460448642266824,"gradient variance and expedite convergence. Specifically, We use another policy Ï€b to generate a
206"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.30578512396694213,"trajectory and calculate its cumulative reward, serving as a baseline function. Our optimization
207"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3069657615112161,"objective is based on PPO-Clip [23]:
208"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.30814639905548996,"E(s,a)âˆ¼Ï€b """
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.30932703659976385,"min
Ï€Î¸(a|s)"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3105076741440378,"Ï€b(a|s)
 
R(Ï„ Î¸
s ) âˆ’R(Ï„ b
s)

, clip
Ï€Î¸(a|s)"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3116883116883117,"Ï€b(a|s), 1âˆ’Ïµ, 1+Ïµ
  
R(Ï„ Î¸
s ) âˆ’R(Ï„ b
s)
# (9)"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.31286894923258557,"where s is used to replace current state st for symbol simplification, and a for at. clip(w, vmin, vmax)
209"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3140495867768595,"denotes constraining w within the range [vmin, vmax], and Ïµ is a hyper-parameter. Ï„ Î¸
s denotes a
210"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3152302243211334,"trajectory sampled by Ï€Î¸ with s as the initial state, while Ï„ b
s for the trajectory greedily decoded by Ï€b.
211"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.31641086186540734,"In greedy decoding, the action with the maximum probability is selected at each step. R(Ï„ Î¸
s ) âˆ’R(Ï„ b
s)
212"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.31759149940968123,"serves as an advantage measure, quantifying the advantage of the current policy Ï€Î¸ compared to Ï€b.
213"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.3187721369539551,"We maximize Eq. (9) through gradient descent, which forces the model to select actions that yield
214"
REINFORCEMENT FINE-TUNING VIA PPO WITH SELF-CRITICAL STRATEGY,0.31995277449822906,"higher advantages. The baseline policyâ€™s parameters are updated if Ï€Î¸ outperforms Ï€b.
215"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.32113341204250295,"4.3
Path Optimization via Dynamic Programming
216"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.32231404958677684,"The complexity of the problem is heightened by the increasing capacity constraint, making it challeng-
217"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3234946871310508,"ing for the neural network to make accurate decisions regarding the depot return positions. In this sec-
218"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3246753246753247,"tion, we propose a dynamic programming (DP) based strategy to assist our model in optimizing these
219"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.32585596221959856,"positions. Assuming that P is assigned with the terminal state sT = (arcx0, arcx1, ..., arcxT âˆ’1),
220"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3270365997638725,"representing a generated path.
Initially, we remove all the depot arcs in P to obtain a new
221"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3282172373081464,"path P
â€² = (arcxâ€²
0, arcxâ€²
1, ..., arcxâ€²
T â€²âˆ’1), where {xâ€²
i|i âˆˆ[0, T â€² âˆ’1]} denotes a subsequence of
222"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3293978748524203,"{xi|i âˆˆ[0, T âˆ’1]}. Subsequently, we aim to insert several new depot arcs into the path P
â€² to
223"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3305785123966942,"achieve a lower cost while adhering to capacity constraints. To be specific, we recursively find the
224"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3317591499409681,"return point that minimizes the overall increasing cost, which is implemented by the state transition
225"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.33293978748524206,"equation as follows:
226"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.33412042502951594,"f(P
â€²) = min
i (f(P
â€²
0:i) + SC(arcxâ€²
i, arc0) + SC(arc0, arcxâ€²
i+1) âˆ’SC(arcxâ€²
i, arcxâ€²
i+1))"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.33530106257378983,"s.t.
0 â‰¤i < T
â€² âˆ’1,
XT
â€²âˆ’1"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3364817001180638,"j=i+1 demandxâ€²
j â‰¤Q
(10)"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.33766233766233766,"where SC(arcxâ€²
i, arc0) = |exâ€²
i0| denotes the shortest path cost from arcxâ€²
i to the depot. Q is the
227"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.33884297520661155,"vehicle capacity. According to Eq. (10), we insert the depot arc arc0 after an appropriate position
228"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3400236127508855,"arcxâ€²
i, which meets with the capacity constraint of the subpath P
â€²
i+1:T
â€²âˆ’1. f(Â·) denotes a state
229"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3412042502951594,"featuring dynamic programming. By enumerating the position i, we compute the minimum increasing
230"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.34238488783943327,"cost f(P
â€²) utilizing its sub-state f(P
â€²
0:i). The final minimum cost for path P is f(P â€²) + g(P â€²), here
231"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3435655253837072,"g(P â€²) is the unoptimized cost of P â€². Since P
â€² includes only the required edges, i.e., T â€² = |ER|,
232"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.3447461629279811,"the time complexity of DP is O(|ER|2). During Path Optimization, we use beam search to generate
233"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.345926800472255,"two paths with the trained policy, one with capacity-constrained and one without. Both paths are
234"
PATH OPTIMIZATION VIA DYNAMIC PROGRAMMING,0.34710743801652894,"optimized using DP and the one with the minimum cost is selected as the final result.
235"
EXPERIMENTS,0.3482880755608028,"5
Experiments
236"
SETUP,0.34946871310507677,"5.1
Setup
237"
SETUP,0.35064935064935066,"Problem Instances.
We extracted sub-graphs from the roadmap of Beijing, China, obtained from
238"
SETUP,0.35182998819362454,"OpenStreetMap [8], to create CARP instances for both the training and testing phases. All instances
239"
SETUP,0.3530106257378985,"are divided into seven datasets, each representing different problem scales, as presented in Table 2.
240"
SETUP,0.3541912632821724,"Each dataset consists of 30,000 instances, further divided into two disjoint subsets: 20,000 instances
241"
SETUP,0.35537190082644626,"for training and the remaining for testing. For each instance, the vehicle capacity is set to 100.
242"
SETUP,0.3565525383707202,"Implementation Details.
Our neural network is implemented using the PyTorch framework and
243"
SETUP,0.3577331759149941,"trained on a single NVIDIA RTX 3090 GPU. The heuristics and metaheuristics algorithms are
244"
SETUP,0.358913813459268,"Table 2: Datasets information. |V| is the number of nodes, |ER| is the number of required edges.
demand represents the demand range for each required edge. Each dataset has 20,000 training
instances and 10,000 test instances."
SETUP,0.3600944510035419,"CARP instances
|V|
|ER|
demand
CARP instances
|V|
|ER|
demand"
SETUP,0.3612750885478158,"Task 20
25-30
20
5-10
Task 200
205-210
200
1000
Task 40
45-50
40
5-10
Task 300
305-310
300
1000
Task 60
65-70
60
5-10
Task 400
405-410
400
1000
Task 80
85-90
80
5-10
Task 500
505-510
500
1000
Task 100
105-110
100
5-10
Task 600
605-610
600
1000"
SETUP,0.3624557260920897,"Table 3: Solution quality comparison. All methods are evaluated on 10,000 CARP instances in each
scale. We measure the gap (%) between different methods and MAENS. Methods marked with an
asterisk were originally proposed for NRP, but we modified them to solve CARP. The best results are
indicated in bold, while the second-best results are underlined."
SETUP,0.36363636363636365,"Method
Task20
Task40
Task60
Task80
Task100
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)"
SETUP,0.36481700118063753,"MAENS [25]
474
0
950
0
1529
0
2113
0
2757
0
PS [6]
544
14.72
1079
13.56
1879
22.84
2504
18.49
3361
21.90
PS-Ellipse [22]
519
9.49
1006
5.89
1709
11.77
2299
8.80
3095
12.26
PS-Efficiency [1]
514
8.44
1007
6.00
1684
10.14
2282
8.00
3056
10.85
PS-Alt1 [1]
514
8.44
1007
6.00
1685
10.20
2283
8.04
3057
10.88
PS-Alt2 [1]
521
9.92
1009
6.21
1720
12.49
2314
9.51
3102
12.51
S2V-DQN* [11]
590
24.42
1197
26.02
1900
24.23
2820
33.43
3404
23.42
VRP-DL* [19]
528
11.39
1193
25.57
2033
32.96
2898
37.15
3867
40.26"
SETUP,0.3659976387249115,"DaAM (SL)
509
7.43
1066
12.24
-
-
-
-
-
-
DaAM (SL+RL)
495
4.48
1009
6.19
1639
7.16
2275
7.67
2980
8.06
DaAM (SL+RL+PO)
482
1.65
992
4.39
1621
5.98
2255
6.70
2958
7.28"
SETUP,0.36717827626918537,"evaluated on an Intel Core i9-7920X with 24 cores and a CPU frequency of 4.4GHz. We optimize
245"
SETUP,0.36835891381345925,"the model using Adam optimizer [12]. The dimension of MDS coordinates d is set to 8, and the
246"
SETUP,0.3695395513577332,"learning rate is set to 1eâˆ’4. We set Ïµ in the PPO training at 0.1. Notably, our PPO training does not
247"
SETUP,0.3707201889020071,"incorporate discounted cumulative rewards, i.e., Î³ is set to 1.
248"
SETUP,0.371900826446281,"Metrics and Settings.
For each method and dataset, We compute the mean tour cost across all test
249"
SETUP,0.3730814639905549,"instances, indicated by â€œCostâ€. Employing the state-of-the-art MAENS [25] as a baseline, we measure
250"
SETUP,0.3742621015348288,"the â€œCostâ€ gap between alternative algorithms and MAENS, indicated by â€œGapâ€. We compare our
251"
SETUP,0.3754427390791027,"method against the heuristic Path-Scanning algorithms (PS) [6, 22, 1] and two NN-based algorithms.
252"
SETUP,0.37662337662337664,"In the absence of publicly available code for prior NN-based CARP methods, we modify two NN-
253"
SETUP,0.3778040141676505,"based NRP solvers to suit CARP, i.e, S2V-DQN [11] and VRP-DL [19]. Note that, for S2V-DQN,
254"
SETUP,0.3789846517119244,"we replace structure2vec with GAT to achieve more effective graph embedding learning. For our
255"
SETUP,0.38016528925619836,"method, we incrementally add supervised pre-training (SL), reinforcement learning fine-tuning (RL),
256"
SETUP,0.38134592680047225,"and path optimization (PO) to assess the effectiveness of our training scheme and optimization,
257"
SETUP,0.3825265643447462,"respectively. Due to the excessively long computation times of MAENS on larger-scale datasets, SL
258"
SETUP,0.3837072018890201,"is only performed on Task20, Task 30, and Task40. The batch size for SL is set to 128. During the RL
259"
SETUP,0.38488783943329397,"stage, greedy decoding is used to generate solutions, and except for the Task20 dataset, we utilize the
260"
SETUP,0.3860684769775679,"training results obtained from the preceding smaller-scale dataset to initialize the model. The beam
261"
SETUP,0.3872491145218418,"width in the PO stage is set to 2. For each dataset, we compare the mean cost of different methods on
262"
SETUP,0.3884297520661157,"10,000 problem instances.
263"
EVALUATION RESULTS,0.38961038961038963,"5.2
Evaluation Results
264"
EVALUATION RESULTS,0.3907910271546635,"Solution Quality
Table 6 shows the result. Our algorithm outperforms all heuristic and NN-based
265"
EVALUATION RESULTS,0.3919716646989374,"methods across all scales, achieving costs comparable to MAENS, trailing by less than 8%. The
266"
EVALUATION RESULTS,0.39315230224321135,"advantage over PS demonstrates that neural networks can learn more effective policies than hand-
267"
EVALUATION RESULTS,0.39433293978748524,"crafted ones, attributed to our well-designed modeling approach. Moreover, as the problem scale
268"
EVALUATION RESULTS,0.3955135773317591,"increases, it becomes time-consuming to obtain CARP annotation by MAENS. Therefore, we leverage
269"
EVALUATION RESULTS,0.39669421487603307,"the model pre-trained on small-scale instances as the initial policy for RL fine-tuning on Task50,
270"
EVALUATION RESULTS,0.39787485242030696,"Task60, Task80, and Task100, yielding commendable performance. This proves the generalization of
271"
EVALUATION RESULTS,0.3990554899645809,"our training scheme across varying problem scales. The performance gap with MAENS highlights
272"
EVALUATION RESULTS,0.4002361275088548,"our algorithmâ€™s superiority in CARP-solving approaches.
273"
EVALUATION RESULTS,0.4014167650531287,"Table 4: Generalization to larger problem instances. All methods are evaluated on 10,000 CARP
instances in each scale. For DaAM, we employ the policy trained on Task100. The best results are
indicated in bold, while the second-best results are underlined."
EVALUATION RESULTS,0.4025974025974026,"Method
Task200
Task300
Task400
Task500
Task600
Cost
Cost
Cost
Cost
Cost"
EVALUATION RESULTS,0.4037780401416765,"PS-Ellipse [22]
4240
6563
8600
10909
13377
PS-Efficiency [1]
4233
6544
8583
10883
13338
PS-Alt1 [1]
4233
6544
8580
10884
13338
PS-Alt2 [1]
4244
6569
8606
10922
13393"
EVALUATION RESULTS,0.4049586776859504,"DaAM (SL+RL)
4189
6372
8610
10938
13340
DaAM (SL+RL+PO)
4132
6281
8473
10633
13100"
EVALUATION RESULTS,0.40613931523022434,"Task20
Task40
Task60
Task80
Task100
Datasets 10
0 10
1 10
2 10
3 10
4"
EVALUATION RESULTS,0.40731995277449823,Time (seconds)
EVALUATION RESULTS,0.4085005903187721,Run Time in Log Space vs Number of Edge
EVALUATION RESULTS,0.40968122786304606,"MAENS
PS
PS-Ellipse
PS-Efficiency
PS-Alt1
PS-Alt2
Ours (w/o PO)
Ours (w PO)"
EVALUATION RESULTS,0.41086186540731995,"Task200
Task300
Task400
Task500
Task600
Datasets 10
1 10
2 10
3"
EVALUATION RESULTS,0.41204250295159384,Time (seconds)
EVALUATION RESULTS,0.4132231404958678,Run Time in Log Space vs Number of Edge
EVALUATION RESULTS,0.41440377804014167,"PS-Ellipse
PS-Efficiency
PS-Alt1
PS-Alt2
Ours (w/o PO)
Ours (w PO)"
EVALUATION RESULTS,0.4155844155844156,"Figure 2: Run time comparison. For each dataset, the total run time of each method on 100 CARP
instances is shown."
EVALUATION RESULTS,0.4167650531286895,"274
Generalization Ability.
In Table 4, we assess DaAMâ€™s generalization on large-scale CARP in-
275"
EVALUATION RESULTS,0.4179456906729634,"stances using the policy trained on Task100. We remove MAENS and PS due to failing to run on
276"
EVALUATION RESULTS,0.41912632821723733,"large-scale graphs, and remove S2V-DQN and VRP-DL due to poor performance. Although DaAM
277"
EVALUATION RESULTS,0.4203069657615112,"is not trained on large-scale instances, it achieves or even exceeds the performance of PS, which
278"
EVALUATION RESULTS,0.4214876033057851,"shows its potential application on larger-scale CARP instances.
279"
EVALUATION RESULTS,0.42266824085005905,"Run Time.
We compare the total time required for solving 100 CARP instances across datasets
280"
EVALUATION RESULTS,0.42384887839433294,"Task20 to Task100 datasets using our method, MAENS, and PS algorithms, and show the run time in
281"
EVALUATION RESULTS,0.42502951593860683,"log space. For datasets Task200 to Task600, we compare the same metric using variants of PS and
282"
EVALUATION RESULTS,0.42621015348288077,"out method. For our method, we measured the solving time with and without PO. Fig. 2 demonstrates
283"
EVALUATION RESULTS,0.42739079102715466,"that our method exhibits a significant speed advantage over MAENS, even outperforming variants of
284"
EVALUATION RESULTS,0.42857142857142855,"PS [1] on most datasets. In comparison, the consumption time of MAENS increases exponentially as
285"
EVALUATION RESULTS,0.4297520661157025,"the problem scale increases. Our method efficiently generates paths for large-scale CARP instances
286"
EVALUATION RESULTS,0.4309327036599764,"by leveraging GPU data-level parallelism and CPU instruction-level parallelism.
287"
EVALUATION RESULTS,0.43211334120425027,Table 5: Costs of DaAM using different encoder.
EVALUATION RESULTS,0.4332939787485242,"Method
Task30
Task40
Task50
Task60"
EVALUATION RESULTS,0.4344746162927981,"MDS
743
1017
1338
1699
GAT
746
1019
1317
1684
MDS + GAT
741
1011
1322
1683"
EVALUATION RESULTS,0.43565525383707204,"Effectiveness of Combining MDS and GAT.
288"
EVALUATION RESULTS,0.43683589138134593,"To evaluate the combination of MDS and GAT
289"
EVALUATION RESULTS,0.4380165289256198,"for embedding exhibiting, we individually evalu-
290"
EVALUATION RESULTS,0.43919716646989376,"ate the performance of models using only MDS
291"
EVALUATION RESULTS,0.44037780401416765,"or GAT, as well as their combined performance.
292"
EVALUATION RESULTS,0.44155844155844154,"The experiment is conducted on Task30, Task40,
293"
EVALUATION RESULTS,0.4427390791027155,"Task50, and Task60 by comparing the average performance of 1,000 instances on each dataset. In the
294"
EVALUATION RESULTS,0.44391971664698937,"RL stage, we use the policy pre-trained on Task30 for initialization. Table 5 indicates that using MDS
295"
EVALUATION RESULTS,0.44510035419126326,"or GAT individually yields worse quality in most cases, highlighting that combining MDS and GAT
296"
EVALUATION RESULTS,0.4462809917355372,"enhances the modelâ€™s capacity to capture arc correlations. Fig. 3 depicts the convergence trends in
297"
EVALUATION RESULTS,0.4474616292798111,"these scenes, which shows that the synergy of MDS and GAT contributes to the stability of training.
298"
EVALUATION RESULTS,0.448642266824085,"Solution Visualization.
For a more intuitive understanding of the paths generated by different
299"
EVALUATION RESULTS,0.4498229043683589,"methods, we visualize and compare the results of our method with PS [6] and MAENS across four
300"
EVALUATION RESULTS,0.4510035419126328,"road scenes in Beijing. Fig. 4 visualizes all results alongside scene information. We observe that our
301"
EVALUATION RESULTS,0.45218417945690675,"model obtains similar paths with MAENS since we leverage the annotation generated by MAENS for
302"
EVALUATION RESULTS,0.45336481700118064,"0
20
40
60
80
740 760 780 800 820 840"
EVALUATION RESULTS,0.45454545454545453,Solution Cost
EVALUATION RESULTS,0.4557260920897285,Task30
EVALUATION RESULTS,0.45690672963400236,"MDS + GAT
MDS
GAT"
EVALUATION RESULTS,0.45808736717827625,"0
20
40
60
80
100
1010 1020 1030 1040 1050 1060 1070"
EVALUATION RESULTS,0.4592680047225502,Task40
EVALUATION RESULTS,0.4604486422668241,"MDS + GAT
MDS
GAT"
EVALUATION RESULTS,0.46162927981109797,"0
20
40
60
80
Epochs 1250 1500 1750 2000 2250 2500 2750"
EVALUATION RESULTS,0.4628099173553719,Solution Cost
EVALUATION RESULTS,0.4639905548996458,Task50
EVALUATION RESULTS,0.4651711924439197,"MDS + GAT
MDS
GAT"
EVALUATION RESULTS,0.46635182998819363,"0
10
20
30
Epochs 1700 1750 1800 1850 1900 1950"
EVALUATION RESULTS,0.4675324675324675,Task60
EVALUATION RESULTS,0.46871310507674147,"MDS + GAT
MDS
GAT"
EVALUATION RESULTS,0.46989374262101535,Figure 3: Convergence trends of different methods in reinforcement learning training.
EVALUATION RESULTS,0.47107438016528924,"MAENS
Ours (SL+RL+PO)
Path Scanning"
EVALUATION RESULTS,0.4722550177095632,"Taiping Street, Beijing
ð„! = 54; ð„= 100; ð•= 88"
EVALUATION RESULTS,0.4734356552538371,"Sanlihe, Beijing
ð„! = 125; ð„= 125; ð•= 103"
EVALUATION RESULTS,0.47461629279811096,"Beijing Used Car Market, Beijing"
EVALUATION RESULTS,0.4757969303423849,ð„! = 103; ð„= 103; ð•= 154
EVALUATION RESULTS,0.4769775678866588,"Area of â€œHengqi and Shunbaâ€, Beijing"
EVALUATION RESULTS,0.4781582054309327,ð„! = 79; ð„= 138; ð•= 116
EVALUATION RESULTS,0.4793388429752066,Cost=1925
EVALUATION RESULTS,0.4805194805194805,Cost=1584
EVALUATION RESULTS,0.4817001180637544,"Cost=1685
Cost=2948"
EVALUATION RESULTS,0.48288075560802834,Cost=2824
EVALUATION RESULTS,0.48406139315230223,"Cost=3208
Cost=2610"
EVALUATION RESULTS,0.4852420306965762,Cost=2122
EVALUATION RESULTS,0.48642266824085006,Cost=2255
EVALUATION RESULTS,0.48760330578512395,Cost=2335
EVALUATION RESULTS,0.4887839433293979,Cost=1933
EVALUATION RESULTS,0.4899645808736718,Cost=2079
EVALUATION RESULTS,0.4911452184179457,"Figure 4: Qualitative comparison in four real street scenes. The paths are marked in different colors,
with gray indicating roads that do not require service and red points indicating depots."
EVALUATION RESULTS,0.4923258559622196,"supervised learning. MAENS paths exhibit superior spatial locality, clearly dividing the scene into
303"
EVALUATION RESULTS,0.4935064935064935,"regions, whereas PS paths appear more random.
304"
CONCLUSION AND LIMITATIONS,0.4946871310507674,"6
Conclusion and Limitations
305"
CONCLUSION AND LIMITATIONS,0.49586776859504134,"In this paper, we propose a learning-based CARP solver that competes with state-of-the-art meta-
306"
CONCLUSION AND LIMITATIONS,0.4970484061393152,"heuristics. Firstly, we encode the potential serving direction of edges into embeddings, ensuring
307"
CONCLUSION AND LIMITATIONS,0.4982290436835891,"that edge directionality is taken into account in decision-making. Secondly, we present a supervised
308"
CONCLUSION AND LIMITATIONS,0.49940968122786306,"reinforcement learning approach that effectively learns policies to solve CARP. With the aid of
309"
CONCLUSION AND LIMITATIONS,0.500590318772137,"these contributions, our method surpasses all heuristics and achieves performance comparable to
310"
CONCLUSION AND LIMITATIONS,0.5017709563164109,"metaheuristics for the first time while maintaining excellent efficiency.
311"
CONCLUSION AND LIMITATIONS,0.5029515938606848,"Limitations and future work.
Decomposing undirected edges increases the decision elements,
312"
CONCLUSION AND LIMITATIONS,0.5041322314049587,"which complicates the problem and may widens the gap between DaAM and traditional state-of-the-
313"
CONCLUSION AND LIMITATIONS,0.5053128689492326,"art approaches as the problem instance scale increases. Our future work focuses on designing an
314"
CONCLUSION AND LIMITATIONS,0.5064935064935064,"efficient graph transformation method that does not significantly increase problem complexity.
315"
REFERENCES,0.5076741440377804,"References
316"
REFERENCES,0.5088547815820543,"[1] Rafael Kendy Arakaki and Fabio Luiz Usberti. An efficiency-based path-scanning heuristic for
317"
REFERENCES,0.5100354191263282,"the capacitated arc routing problem. Computers & Operations Research (COR), 103:288â€“295,
318"
REFERENCES,0.5112160566706021,"2019.
319"
REFERENCES,0.512396694214876,"[2] Irwan Bello*, Hieu Pham*, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural com-
320"
REFERENCES,0.51357733175915,"binatorial optimization with reinforcement learning. In International Conference on Learning
321"
REFERENCES,0.5147579693034239,"Representations (ICLR), 2017.
322"
REFERENCES,0.5159386068476978,"[3] Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of
323"
REFERENCES,0.5171192443919717,"graph embedding: Problems, techniques, and applications. IEEE transactions on knowledge
324"
REFERENCES,0.5182998819362455,"and data engineering (TKDE), 30(9):1616â€“1637, 2018.
325"
REFERENCES,0.5194805194805194,"[4] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving
326"
REFERENCES,0.5206611570247934,"the over-smoothing problem for graph neural networks from the topological view. In AAAI
327"
REFERENCES,0.5218417945690673,"conference on artificial intelligence (AAAI), 2020.
328"
REFERENCES,0.5230224321133412,"[5] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for
329"
REFERENCES,0.5242030696576151,"structured data. In International conference on machine learning (ICML), 2016.
330"
REFERENCES,0.525383707201889,"[6] Bruce L Golden, James S DeArmon, and Edward K Baker. Computational experiments with
331"
REFERENCES,0.526564344746163,"algorithms for a class of routing problems. Computers & Operations Research (COR), 10(1):47â€“
332"
REFERENCES,0.5277449822904369,"59, 1983.
333"
REFERENCES,0.5289256198347108,"[7] Bruce L Golden and Richard T Wong. Capacitated arc routing problems. Networks, 11(3):305â€“
334"
REFERENCES,0.5301062573789846,"315, 1981.
335"
REFERENCES,0.5312868949232585,"[8] Mordechai Haklay and Patrick Weber. Openstreetmap: User-generated street maps. IEEE
336"
REFERENCES,0.5324675324675324,"Pervasive computing, 7(4):12â€“18, 2008.
337"
REFERENCES,0.5336481700118064,"[9] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
338"
REFERENCES,0.5348288075560803,"graphs. In Advances in neural information processing systems (NeurIPS), 2017.
339"
REFERENCES,0.5360094451003542,"[10] Wenjing Hong and Tonglin Liu. Faster capacitated arc routing: A sequence-to-sequence
340"
REFERENCES,0.5371900826446281,"approach. IEEE Access, 10:4777â€“4785, 2022.
341"
REFERENCES,0.538370720188902,"[11] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial
342"
REFERENCES,0.5395513577331759,"optimization algorithms over graphs. In Advances in neural information processing systems
343"
REFERENCES,0.5407319952774499,"(NeurIPS), 2017.
344"
REFERENCES,0.5419126328217237,"[12] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
345"
REFERENCES,0.5430932703659976,"Conference on Learning Representations (ICLR), 2015.
346"
REFERENCES,0.5442739079102715,"[13] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
347"
REFERENCES,0.5454545454545454,"networks. In International Conference on Learning Representations (ICLR), 2017.
348"
REFERENCES,0.5466351829988194,"[14] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In
349"
REFERENCES,0.5478158205430933,"International Conference on Learning Representations (ICLR), 2019.
350"
REFERENCES,0.5489964580873672,"[15] Natalio Krasnogor and James Smith. A tutorial for competent memetic algorithms: model, taxon-
351"
REFERENCES,0.5501770956316411,"omy, and design issues. IEEE transactions on Evolutionary Computation (TEVC), 9(5):474â€“488,
352"
REFERENCES,0.551357733175915,"2005.
353"
REFERENCES,0.5525383707201889,"[16] Han Li and Guiying Li. Learning to solve capacitated arc routing problems by policy gradient.
354"
REFERENCES,0.5537190082644629,"In IEEE Congress on Evolutionary Computation (CEC), 2019.
355"
REFERENCES,0.5548996458087367,"[17] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,
356"
REFERENCES,0.5560802833530106,"Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
357"
REFERENCES,0.5572609208972845,"ment learning. In International conference on machine learning (ICML), pages 1928â€“1937,
358"
REFERENCES,0.5584415584415584,"2016.
359"
REFERENCES,0.5596221959858324,"[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
360"
REFERENCES,0.5608028335301063,"Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
361"
REFERENCES,0.5619834710743802,"Human-level control through deep reinforcement learning. Nature, 518(7540):529â€“533, 2015.
362"
REFERENCES,0.5631641086186541,"[19] Mohammadreza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin TakÃ¡c. Reinforcement
363"
REFERENCES,0.564344746162928,"learning for solving the vehicle routing problem. In Advances in neural information processing
364"
REFERENCES,0.5655253837072018,"systems (NeurIPS), 2018.
365"
REFERENCES,0.5667060212514758,"[20] Muhilan Ramamoorthy and Violet R. Syrotiuk. Learning heuristics for arc routing problems.
366"
REFERENCES,0.5678866587957497,"Intelligent Systems with Applications (ISWA), 21:200300, 2024.
367"
REFERENCES,0.5690672963400236,"[21] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-
368"
REFERENCES,0.5702479338842975,"critical sequence training for image captioning. In IEEE conference on computer vision and
369"
REFERENCES,0.5714285714285714,"pattern recognition (CVPR), 2017.
370"
REFERENCES,0.5726092089728453,"[22] LuÃ­s Santos, JoÃ£o Coutinho-Rodrigues, and John R Current. An improved heuristic for the
371"
REFERENCES,0.5737898465171193,"capacitated arc routing problem. Computers & Operations Research (COR), 36(9):2632â€“2637,
372"
REFERENCES,0.5749704840613932,"2009.
373"
REFERENCES,0.5761511216056671,"[23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
374"
REFERENCES,0.577331759149941,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
375"
REFERENCES,0.5785123966942148,"[24] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural
376"
REFERENCES,0.5796930342384888,"networks. In Advances in neural information processing systems (NeurIPS), 2014.
377"
REFERENCES,0.5808736717827627,"[25] Ke Tang, Yi Mei, and Xin Yao. Memetic algorithm with extended neighborhood search for
378"
REFERENCES,0.5820543093270366,"capacitated arc routing problems. IEEE Transactions on Evolutionary Computation (TEVC),
379"
REFERENCES,0.5832349468713105,"13(5):1151â€“1166, 2009.
380"
REFERENCES,0.5844155844155844,"[26] Cong Dao Tran and Truong Son Hy. Graph attention-based deep reinforcement learning for solv-
381"
REFERENCES,0.5855962219598583,"ing the chinese postman problem with load-dependent costs. arXiv preprint arXiv:2310.15516,
382"
REFERENCES,0.5867768595041323,"2023.
383"
REFERENCES,0.5879574970484062,"[27] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua
384"
REFERENCES,0.58913813459268,"Bengio. Graph Attention Networks. In International Conference on Learning Representations
385"
REFERENCES,0.5903187721369539,"(ICLR), 2018.
386"
REFERENCES,0.5914994096812278,"[28] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in neural
387"
REFERENCES,0.5926800472255017,"information processing systems (NeurIPS), 2015.
388"
REFERENCES,0.5938606847697757,"[29] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
389"
REFERENCES,0.5950413223140496,"ment learning. Machine learning, 8:229â€“256, 1992.
390"
REFERENCES,0.5962219598583235,"[30] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger.
391"
REFERENCES,0.5974025974025974,"Simplifying graph convolutional networks. In International conference on machine learning
392"
REFERENCES,0.5985832349468713,"(ICML), 2019.
393"
REFERENCES,0.5997638724911453,"[31] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
394"
REFERENCES,0.6009445100354192,"comprehensive survey on graph neural networks. IEEE transactions on neural networks and
395"
REFERENCES,0.602125147579693,"learning systems (TNNLS), 32(1):4â€“24, 2020.
396"
REFERENCES,0.6033057851239669,"A
Appendix
397"
REFERENCES,0.6044864226682408,"A.1
Source Code and Dataset
398"
REFERENCES,0.6056670602125147,"The source code of DaAM and the datasets used for testing are available at DaAM. Once the paper is
399"
REFERENCES,0.6068476977567887,"accepted, we will promptly release the source code and datasets.
400"
REFERENCES,0.6080283353010626,"A.2
Pseudocode of PPO with self-critical strategy
401"
REFERENCES,0.6092089728453365,"Algorithm 1 presents the pseudocode for the PPO training algorithm we used. In the code implemen-
402"
REFERENCES,0.6103896103896104,"tation, the trajectory Ï„ Î¸
s can be replaced by (s, a)â€™s original trajectory Ï„o for efficiency. Once Ï„o is
403"
REFERENCES,0.6115702479338843,"sampled, the cumulative rewards from any state s âˆˆÏ„o can be quickly computed.
404"
REFERENCES,0.6127508854781583,"Algorithm 1 PPO algorithm with self-critical strategy
Input: batch size B, number of episodes K, train instances P, test instances T
Initialize policies Ï€Î¸, Ï€b â†Ï€s"
REFERENCES,0.6139315230224321,"1: for episode k = 1 to K do
2:
Initialize data batch M, Mâ€² â†()
3:
while |M| < B do
4:
Sample a CARP instance S from P
5:
Sample Ï„o = (s0, a0, . . . , sT ) from S using Ï€b
6:
M â†M âˆª{(s0, a0), . . . , (sT âˆ’1, aT âˆ’1)}
7:
end while
8:
for each (s, a) âˆˆM do
9:
Generate trajectory Ï„ Î¸
s using Ï€Î¸ from s by sampling
10:
Generate trajectory Ï„ b
s using Ï€b from s by greedy decoding
11:
Compute advantage As = R(Ï„ Î¸
s ) âˆ’R(Ï„ b
s)
12:
Mâ€² â†Mâ€² âˆª{(s, a, As)}
13:
end for
14:
Update Ï€Î¸ using Adam over (9) based on Mâ€²"
REFERENCES,0.615112160566706,"15:
if Ï€Î¸ outperforms Ï€b on T then
16:
Ï€b â†Ï€Î¸
17:
end if
18: end for"
REFERENCES,0.6162927981109799,"A.3
Experimental Results of Additional Datasets
405"
REFERENCES,0.6174734356552538,"For small-scale problem instances, we generated two additional datasets, Task30 and Task50. In
406"
REFERENCES,0.6186540731995277,"Task30 the range of |V | is 25-30, while in Task50, it spans 55-60. Correspondingly, |ER| is set to
407"
REFERENCES,0.6198347107438017,"30 and 50, respectively The demand for each edge ranges from 5 to 10 in both tasks. Table 6 is the
408"
REFERENCES,0.6210153482880756,complete experimental data from the solution quality experiments.
REFERENCES,0.6221959858323495,"Table 6: Solution quality comparison. All methods are evaluated on 10,000 CARP instances in each
scale. We measure the gap (%) between different methods and MAENS. Methods marked with an
asterisk were originally proposed for NRP, but we modified them to solve CARP. The gray indicates
that MAENS is taken as the baseline when calculating â€œGapâ€. The best results are indicated in bold,
while the second-best results are underlined."
REFERENCES,0.6233766233766234,"Method
Task20
Task30
Task40
Task50
Task60
Task80
Task100
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)
Cost
Gap (%)"
REFERENCES,0.6245572609208973,"MAENS [25]
474
0.00
706
0.00
950
0.00
1222
0.00
1529
0.00
2113
0.00
2757
0.00"
REFERENCES,0.6257378984651711,"PS [6]
544
14.72
859
21.76
1079
13.56
1448
18.45
1879
22.84
2504
18.49
3361
21.90
PS-Ellipse [22]
519
9.49
798
13.03
1006
5.89
1328
8.67
1709
11.77
2299
8.80
3095
12.26
PS-Efficiency [1]
514
8.44
790
11.90
1007
6.00
1311
7.28
1684
10.14
2282
8.00
3056
10.85
PS-Alt1 [1]
514
8.44
791
12.04
1007
6.00
1312
7.36
1685
10.20
2283
8.04
3057
10.88
PS-Alt2 [1]
521
9.92
802
13.60
1009
6.21
1336
9.33
1720
12.49
2314
9.51
3102
12.51
S2V-DQN* [11]
590
24.42
880
24.65
1197
26.02
1520
24.32
1900
24.23
2820
33.43
3404
23.42
VRP-DL* [19]
528
11.39
848
20.11
1193
25.57
1587
29.87
2033
32.96
2898
37.15
3867
40.26"
REFERENCES,0.6269185360094451,"DaAM (SL)
509
7.43
785
11.18
1066
12.24
-
-
-
-
-
-
-
-
DaAM (SL+RL)
495
4.48
741
5.05
1009
6.19
1303
6.58
1639
7.16
2275
7.67
2980
8.06
DaAM (SL+RL+PO)
482
1.65
725
2.73
992
4.39
1283
5.07
1621
5.98
2255
6.70
2958
7.28 409"
REFERENCES,0.628099173553719,"A.4
Licences of Assets Used for Experiments
410"
REFERENCES,0.6292798110979929,"The code we used does not require special consent from the authors. We follow their licenses as
411"
REFERENCES,0.6304604486422668,"specified below:
412"
REFERENCES,0.6316410861865407,"â€¢ https://github.com/wouterkool/attention-learn-to-route: MIT Licence.
413"
REFERENCES,0.6328217237308147,"â€¢ https://github.com/Hanjun-Dai/graph_comb_opt: MIT Licence.
414"
REFERENCES,0.6340023612750886,"NeurIPS Paper Checklist
415"
CLAIMS,0.6351829988193625,"1. Claims
416"
CLAIMS,0.6363636363636364,"Question: Do the main claims made in the abstract and introduction accurately reflect the
417"
CLAIMS,0.6375442739079102,"paperâ€™s contributions and scope?
418"
CLAIMS,0.6387249114521841,"Answer: [Yes]
419"
CLAIMS,0.6399055489964581,"Justification: See Abstract and Introduction.
420"
CLAIMS,0.641086186540732,"Guidelines:
421"
CLAIMS,0.6422668240850059,"â€¢ The answer NA means that the abstract and introduction do not include the claims
422"
CLAIMS,0.6434474616292798,"made in the paper.
423"
CLAIMS,0.6446280991735537,"â€¢ The abstract and/or introduction should clearly state the claims made, including the
424"
CLAIMS,0.6458087367178277,"contributions made in the paper and important assumptions and limitations. A No or
425"
CLAIMS,0.6469893742621016,"NA answer to this question will not be perceived well by the reviewers.
426"
CLAIMS,0.6481700118063755,"â€¢ The claims made should match theoretical and experimental results, and reflect how
427"
CLAIMS,0.6493506493506493,"much the results can be expected to generalize to other settings.
428"
CLAIMS,0.6505312868949232,"â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
429"
CLAIMS,0.6517119244391971,"are not attained by the paper.
430"
LIMITATIONS,0.6528925619834711,"2. Limitations
431"
LIMITATIONS,0.654073199527745,"Question: Does the paper discuss the limitations of the work performed by the authors?
432"
LIMITATIONS,0.6552538370720189,"Answer: [Yes]
433"
LIMITATIONS,0.6564344746162928,"Justification: See section Conclusion and Limitations.
434"
LIMITATIONS,0.6576151121605667,"Guidelines:
435"
LIMITATIONS,0.6587957497048406,"â€¢ The answer NA means that the paper has no limitation while the answer No means that
436"
LIMITATIONS,0.6599763872491146,"the paper has limitations, but those are not discussed in the paper.
437"
LIMITATIONS,0.6611570247933884,"â€¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
438"
LIMITATIONS,0.6623376623376623,"â€¢ The paper should point out any strong assumptions and how robust the results are to
439"
LIMITATIONS,0.6635182998819362,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
440"
LIMITATIONS,0.6646989374262101,"model well-specification, asymptotic approximations only holding locally). The authors
441"
LIMITATIONS,0.6658795749704841,"should reflect on how these assumptions might be violated in practice and what the
442"
LIMITATIONS,0.667060212514758,"implications would be.
443"
LIMITATIONS,0.6682408500590319,"â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
444"
LIMITATIONS,0.6694214876033058,"only tested on a few datasets or with a few runs. In general, empirical results often
445"
LIMITATIONS,0.6706021251475797,"depend on implicit assumptions, which should be articulated.
446"
LIMITATIONS,0.6717827626918536,"â€¢ The authors should reflect on the factors that influence the performance of the approach.
447"
LIMITATIONS,0.6729634002361276,"For example, a facial recognition algorithm may perform poorly when image resolution
448"
LIMITATIONS,0.6741440377804014,"is low or images are taken in low lighting. Or a speech-to-text system might not be
449"
LIMITATIONS,0.6753246753246753,"used reliably to provide closed captions for online lectures because it fails to handle
450"
LIMITATIONS,0.6765053128689492,"technical jargon.
451"
LIMITATIONS,0.6776859504132231,"â€¢ The authors should discuss the computational efficiency of the proposed algorithms
452"
LIMITATIONS,0.6788665879574971,"and how they scale with dataset size.
453"
LIMITATIONS,0.680047225501771,"â€¢ If applicable, the authors should discuss possible limitations of their approach to
454"
LIMITATIONS,0.6812278630460449,"address problems of privacy and fairness.
455"
LIMITATIONS,0.6824085005903188,"â€¢ While the authors might fear that complete honesty about limitations might be used by
456"
LIMITATIONS,0.6835891381345927,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
457"
LIMITATIONS,0.6847697756788665,"limitations that arenâ€™t acknowledged in the paper. The authors should use their best
458"
LIMITATIONS,0.6859504132231405,"judgment and recognize that individual actions in favor of transparency play an impor-
459"
LIMITATIONS,0.6871310507674144,"tant role in developing norms that preserve the integrity of the community. Reviewers
460"
LIMITATIONS,0.6883116883116883,"will be specifically instructed to not penalize honesty concerning limitations.
461"
THEORY ASSUMPTIONS AND PROOFS,0.6894923258559622,"3. Theory Assumptions and Proofs
462"
THEORY ASSUMPTIONS AND PROOFS,0.6906729634002361,"Question: For each theoretical result, does the paper provide the full set of assumptions and
463"
THEORY ASSUMPTIONS AND PROOFS,0.69185360094451,"a complete (and correct) proof?
464"
THEORY ASSUMPTIONS AND PROOFS,0.693034238488784,"Answer: [NA]
465"
THEORY ASSUMPTIONS AND PROOFS,0.6942148760330579,"Justification: This papar does not include theoretical results.
466"
THEORY ASSUMPTIONS AND PROOFS,0.6953955135773318,"Guidelines:
467"
THEORY ASSUMPTIONS AND PROOFS,0.6965761511216056,"â€¢ The answer NA means that the paper does not include theoretical results.
468"
THEORY ASSUMPTIONS AND PROOFS,0.6977567886658795,"â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
469"
THEORY ASSUMPTIONS AND PROOFS,0.6989374262101535,"referenced.
470"
THEORY ASSUMPTIONS AND PROOFS,0.7001180637544274,"â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
471"
THEORY ASSUMPTIONS AND PROOFS,0.7012987012987013,"â€¢ The proofs can either appear in the main paper or the supplemental material, but if
472"
THEORY ASSUMPTIONS AND PROOFS,0.7024793388429752,"they appear in the supplemental material, the authors are encouraged to provide a short
473"
THEORY ASSUMPTIONS AND PROOFS,0.7036599763872491,"proof sketch to provide intuition.
474"
THEORY ASSUMPTIONS AND PROOFS,0.704840613931523,"â€¢ Inversely, any informal proof provided in the core of the paper should be complemented
475"
THEORY ASSUMPTIONS AND PROOFS,0.706021251475797,"by formal proofs provided in appendix or supplemental material.
476"
THEORY ASSUMPTIONS AND PROOFS,0.7072018890200709,"â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
477"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7083825265643447,"4. Experimental Result Reproducibility
478"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7095631641086186,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
479"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7107438016528925,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
480"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7119244391971665,"of the paper (regardless of whether the code and data are provided or not)?
481"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7131050767414404,"Answer: [Yes]
482"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7142857142857143,"Justification: This papar discusses the detail to reproduce the main experimental results of
483"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7154663518299882,"the paper.
484"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7166469893742621,"Guidelines:
485"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.717827626918536,"â€¢ The answer NA means that the paper does not include experiments.
486"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.71900826446281,"â€¢ If the paper includes experiments, a No answer to this question will not be perceived
487"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7201889020070839,"well by the reviewers: Making the paper reproducible is important, regardless of
488"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7213695395513577,"whether the code and data are provided or not.
489"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7225501770956316,"â€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
490"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7237308146399055,"to make their results reproducible or verifiable.
491"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7249114521841794,"â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.
492"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7260920897284534,"For example, if the contribution is a novel architecture, describing the architecture fully
493"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7272727272727273,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
494"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7284533648170012,"be necessary to either make it possible for others to replicate the model with the same
495"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7296340023612751,"dataset, or provide access to the model. In general. releasing code and data is often
496"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.730814639905549,"one good way to accomplish this, but reproducibility can also be provided via detailed
497"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.731995277449823,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
498"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7331759149940968,"of a large language model), releasing of a model checkpoint, or other means that are
499"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7343565525383707,"appropriate to the research performed.
500"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7355371900826446,"â€¢ While NeurIPS does not require releasing code, the conference does require all submis-
501"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7367178276269185,"sions to provide some reasonable avenue for reproducibility, which may depend on the
502"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7378984651711924,"nature of the contribution. For example
503"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7390791027154664,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
504"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7402597402597403,"to reproduce that algorithm.
505"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7414403778040142,"(b) If the contribution is primarily a new model architecture, the paper should describe
506"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7426210153482881,"the architecture clearly and fully.
507"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.743801652892562,"(c) If the contribution is a new model (e.g., a large language model), then there should
508"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7449822904368358,"either be a way to access this model for reproducing the results or a way to reproduce
509"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7461629279811098,"the model (e.g., with an open-source dataset or instructions for how to construct
510"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7473435655253837,"the dataset).
511"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7485242030696576,"(d) We recognize that reproducibility may be tricky in some cases, in which case
512"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497048406139315,"authors are welcome to describe the particular way they provide for reproducibility.
513"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508854781582054,"In the case of closed-source models, it may be that access to the model is limited in
514"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7520661157024794,"some way (e.g., to registered users), but it should be possible for other researchers
515"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7532467532467533,"to have some path to reproducing or verifying the results.
516"
OPEN ACCESS TO DATA AND CODE,0.7544273907910272,"5. Open access to data and code
517"
OPEN ACCESS TO DATA AND CODE,0.755608028335301,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
518"
OPEN ACCESS TO DATA AND CODE,0.7567886658795749,"tions to faithfully reproduce the main experimental results, as described in supplemental
519"
OPEN ACCESS TO DATA AND CODE,0.7579693034238488,"material?
520"
OPEN ACCESS TO DATA AND CODE,0.7591499409681228,"Answer: [Yes]
521"
OPEN ACCESS TO DATA AND CODE,0.7603305785123967,"Justification: The source code and datasets are provided in the Appendix.
522"
OPEN ACCESS TO DATA AND CODE,0.7615112160566706,"Guidelines:
523"
OPEN ACCESS TO DATA AND CODE,0.7626918536009445,"â€¢ The answer NA means that paper does not include experiments requiring code.
524"
OPEN ACCESS TO DATA AND CODE,0.7638724911452184,"â€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
525"
OPEN ACCESS TO DATA AND CODE,0.7650531286894924,"public/guides/CodeSubmissionPolicy) for more details.
526"
OPEN ACCESS TO DATA AND CODE,0.7662337662337663,"â€¢ While we encourage the release of code and data, we understand that this might not be
527"
OPEN ACCESS TO DATA AND CODE,0.7674144037780402,"possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
528"
OPEN ACCESS TO DATA AND CODE,0.768595041322314,"including code, unless this is central to the contribution (e.g., for a new open-source
529"
OPEN ACCESS TO DATA AND CODE,0.7697756788665879,"benchmark).
530"
OPEN ACCESS TO DATA AND CODE,0.7709563164108618,"â€¢ The instructions should contain the exact command and environment needed to run to
531"
OPEN ACCESS TO DATA AND CODE,0.7721369539551358,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
532"
OPEN ACCESS TO DATA AND CODE,0.7733175914994097,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
533"
OPEN ACCESS TO DATA AND CODE,0.7744982290436836,"â€¢ The authors should provide instructions on data access and preparation, including how
534"
OPEN ACCESS TO DATA AND CODE,0.7756788665879575,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
535"
OPEN ACCESS TO DATA AND CODE,0.7768595041322314,"â€¢ The authors should provide scripts to reproduce all experimental results for the new
536"
OPEN ACCESS TO DATA AND CODE,0.7780401416765053,"proposed method and baselines. If only a subset of experiments are reproducible, they
537"
OPEN ACCESS TO DATA AND CODE,0.7792207792207793,"should state which ones are omitted from the script and why.
538"
OPEN ACCESS TO DATA AND CODE,0.7804014167650531,"â€¢ At submission time, to preserve anonymity, the authors should release anonymized
539"
OPEN ACCESS TO DATA AND CODE,0.781582054309327,"versions (if applicable).
540"
OPEN ACCESS TO DATA AND CODE,0.7827626918536009,"â€¢ Providing as much information as possible in supplemental material (appended to the
541"
OPEN ACCESS TO DATA AND CODE,0.7839433293978748,"paper) is recommended, but including URLs to data and code is permitted.
542"
OPEN ACCESS TO DATA AND CODE,0.7851239669421488,"6. Experimental Setting/Details
543"
OPEN ACCESS TO DATA AND CODE,0.7863046044864227,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
544"
OPEN ACCESS TO DATA AND CODE,0.7874852420306966,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
545"
OPEN ACCESS TO DATA AND CODE,0.7886658795749705,"results?
546"
OPEN ACCESS TO DATA AND CODE,0.7898465171192444,"Answer: [Yes]
547"
OPEN ACCESS TO DATA AND CODE,0.7910271546635183,"Justification: See section Experiments.
548"
OPEN ACCESS TO DATA AND CODE,0.7922077922077922,"Guidelines:
549"
OPEN ACCESS TO DATA AND CODE,0.7933884297520661,"â€¢ The answer NA means that the paper does not include experiments.
550"
OPEN ACCESS TO DATA AND CODE,0.79456906729634,"â€¢ The experimental setting should be presented in the core of the paper to a level of detail
551"
OPEN ACCESS TO DATA AND CODE,0.7957497048406139,"that is necessary to appreciate the results and make sense of them.
552"
OPEN ACCESS TO DATA AND CODE,0.7969303423848878,"â€¢ The full details can be provided either with the code, in appendix, or as supplemental
553"
OPEN ACCESS TO DATA AND CODE,0.7981109799291618,"material.
554"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7992916174734357,"7. Experiment Statistical Significance
555"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8004722550177096,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
556"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8016528925619835,"information about the statistical significance of the experiments?
557"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8028335301062574,"Answer: [No]
558"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8040141676505312,"Justification: Since the experimental results are deterministic, we did not repeat the exper-
559"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8051948051948052,"iments multiple times. However, to reduce errors, we calculated the average over 10,000
560"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8063754427390791,"problem instances for each dataset of any scale.
561"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.807556080283353,"Guidelines:
562"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8087367178276269,"â€¢ The answer NA means that the paper does not include experiments.
563"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8099173553719008,"â€¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
564"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8110979929161747,"dence intervals, or statistical significance tests, at least for the experiments that support
565"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8122786304604487,"the main claims of the paper.
566"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8134592680047226,"â€¢ The factors of variability that the error bars are capturing should be clearly stated (for
567"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8146399055489965,"example, train/test split, initialization, random drawing of some parameter, or overall
568"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8158205430932703,"run with given experimental conditions).
569"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8170011806375442,"â€¢ The method for calculating the error bars should be explained (closed form formula,
570"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8181818181818182,"call to a library function, bootstrap, etc.)
571"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8193624557260921,"â€¢ The assumptions made should be given (e.g., Normally distributed errors).
572"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.820543093270366,"â€¢ It should be clear whether the error bar is the standard deviation or the standard error
573"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8217237308146399,"of the mean.
574"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8229043683589138,"â€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
575"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8240850059031877,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
576"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8252656434474617,"of Normality of errors is not verified.
577"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8264462809917356,"â€¢ For asymmetric distributions, the authors should be careful not to show in tables or
578"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8276269185360094,"figures symmetric error bars that would yield results that are out of range (e.g. negative
579"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8288075560802833,"error rates).
580"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8299881936245572,"â€¢ If error bars are reported in tables or plots, The authors should explain in the text how
581"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8311688311688312,"they were calculated and reference the corresponding figures or tables in the text.
582"
EXPERIMENTS COMPUTE RESOURCES,0.8323494687131051,"8. Experiments Compute Resources
583"
EXPERIMENTS COMPUTE RESOURCES,0.833530106257379,"Question: For each experiment, does the paper provide sufficient information on the com-
584"
EXPERIMENTS COMPUTE RESOURCES,0.8347107438016529,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
585"
EXPERIMENTS COMPUTE RESOURCES,0.8358913813459268,"the experiments?
586"
EXPERIMENTS COMPUTE RESOURCES,0.8370720188902007,"Answer: [Yes]
587"
EXPERIMENTS COMPUTE RESOURCES,0.8382526564344747,"Justification: See section Experiments.
588"
EXPERIMENTS COMPUTE RESOURCES,0.8394332939787486,"Guidelines:
589"
EXPERIMENTS COMPUTE RESOURCES,0.8406139315230224,"â€¢ The answer NA means that the paper does not include experiments.
590"
EXPERIMENTS COMPUTE RESOURCES,0.8417945690672963,"â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
591"
EXPERIMENTS COMPUTE RESOURCES,0.8429752066115702,"or cloud provider, including relevant memory and storage.
592"
EXPERIMENTS COMPUTE RESOURCES,0.8441558441558441,"â€¢ The paper should provide the amount of compute required for each of the individual
593"
EXPERIMENTS COMPUTE RESOURCES,0.8453364817001181,"experimental runs as well as estimate the total compute.
594"
EXPERIMENTS COMPUTE RESOURCES,0.846517119244392,"â€¢ The paper should disclose whether the full research project required more compute
595"
EXPERIMENTS COMPUTE RESOURCES,0.8476977567886659,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
596"
EXPERIMENTS COMPUTE RESOURCES,0.8488783943329398,"didnâ€™t make it into the paper).
597"
CODE OF ETHICS,0.8500590318772137,"9. Code Of Ethics
598"
CODE OF ETHICS,0.8512396694214877,"Question: Does the research conducted in the paper conform, in every respect, with the
599"
CODE OF ETHICS,0.8524203069657615,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
600"
CODE OF ETHICS,0.8536009445100354,"Answer: [Yes]
601"
CODE OF ETHICS,0.8547815820543093,"Justification:
602"
CODE OF ETHICS,0.8559622195985832,"Guidelines:
603"
CODE OF ETHICS,0.8571428571428571,"â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
604"
CODE OF ETHICS,0.8583234946871311,"â€¢ If the authors answer No, they should explain the special circumstances that require a
605"
CODE OF ETHICS,0.859504132231405,"deviation from the Code of Ethics.
606"
CODE OF ETHICS,0.8606847697756789,"â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
607"
CODE OF ETHICS,0.8618654073199528,"eration due to laws or regulations in their jurisdiction).
608"
BROADER IMPACTS,0.8630460448642266,"10. Broader Impacts
609"
BROADER IMPACTS,0.8642266824085005,"Question: Does the paper discuss both potential positive societal impacts and negative
610"
BROADER IMPACTS,0.8654073199527745,"societal impacts of the work performed?
611"
BROADER IMPACTS,0.8665879574970484,"Answer: [No]
612"
BROADER IMPACTS,0.8677685950413223,"Justification: The aim of our work is to provide a better solution for a class of combinatorial
613"
BROADER IMPACTS,0.8689492325855962,"optimization problems, though it is difficult to predict its impact on society.
614"
BROADER IMPACTS,0.8701298701298701,"Guidelines:
615"
BROADER IMPACTS,0.8713105076741441,"â€¢ The answer NA means that there is no societal impact of the work performed.
616"
BROADER IMPACTS,0.872491145218418,"â€¢ If the authors answer NA or No, they should explain why their work has no societal
617"
BROADER IMPACTS,0.8736717827626919,"impact or why the paper does not address societal impact.
618"
BROADER IMPACTS,0.8748524203069658,"â€¢ Examples of negative societal impacts include potential malicious or unintended uses
619"
BROADER IMPACTS,0.8760330578512396,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
620"
BROADER IMPACTS,0.8772136953955135,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
621"
BROADER IMPACTS,0.8783943329397875,"groups), privacy considerations, and security considerations.
622"
BROADER IMPACTS,0.8795749704840614,"â€¢ The conference expects that many papers will be foundational research and not tied
623"
BROADER IMPACTS,0.8807556080283353,"to particular applications, let alone deployments. However, if there is a direct path to
624"
BROADER IMPACTS,0.8819362455726092,"any negative applications, the authors should point it out. For example, it is legitimate
625"
BROADER IMPACTS,0.8831168831168831,"to point out that an improvement in the quality of generative models could be used to
626"
BROADER IMPACTS,0.8842975206611571,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
627"
BROADER IMPACTS,0.885478158205431,"that a generic algorithm for optimizing neural networks could enable people to train
628"
BROADER IMPACTS,0.8866587957497049,"models that generate Deepfakes faster.
629"
BROADER IMPACTS,0.8878394332939787,"â€¢ The authors should consider possible harms that could arise when the technology is
630"
BROADER IMPACTS,0.8890200708382526,"being used as intended and functioning correctly, harms that could arise when the
631"
BROADER IMPACTS,0.8902007083825265,"technology is being used as intended but gives incorrect results, and harms following
632"
BROADER IMPACTS,0.8913813459268005,"from (intentional or unintentional) misuse of the technology.
633"
BROADER IMPACTS,0.8925619834710744,"â€¢ If there are negative societal impacts, the authors could also discuss possible mitigation
634"
BROADER IMPACTS,0.8937426210153483,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
635"
BROADER IMPACTS,0.8949232585596222,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
636"
BROADER IMPACTS,0.8961038961038961,"feedback over time, improving the efficiency and accessibility of ML).
637"
SAFEGUARDS,0.89728453364817,"11. Safeguards
638"
SAFEGUARDS,0.898465171192444,"Question: Does the paper describe safeguards that have been put in place for responsible
639"
SAFEGUARDS,0.8996458087367178,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
640"
SAFEGUARDS,0.9008264462809917,"image generators, or scraped datasets)?
641"
SAFEGUARDS,0.9020070838252656,"Answer: [NA]
642"
SAFEGUARDS,0.9031877213695395,"Justification:
643"
SAFEGUARDS,0.9043683589138135,"Guidelines:
644"
SAFEGUARDS,0.9055489964580874,"â€¢ The answer NA means that the paper poses no such risks.
645"
SAFEGUARDS,0.9067296340023613,"â€¢ Released models that have a high risk for misuse or dual-use should be released with
646"
SAFEGUARDS,0.9079102715466352,"necessary safeguards to allow for controlled use of the model, for example by requiring
647"
SAFEGUARDS,0.9090909090909091,"that users adhere to usage guidelines or restrictions to access the model or implementing
648"
SAFEGUARDS,0.910271546635183,"safety filters.
649"
SAFEGUARDS,0.911452184179457,"â€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
650"
SAFEGUARDS,0.9126328217237308,"should describe how they avoided releasing unsafe images.
651"
SAFEGUARDS,0.9138134592680047,"â€¢ We recognize that providing effective safeguards is challenging, and many papers do
652"
SAFEGUARDS,0.9149940968122786,"not require this, but we encourage authors to take this into account and make a best
653"
SAFEGUARDS,0.9161747343565525,"faith effort.
654"
LICENSES FOR EXISTING ASSETS,0.9173553719008265,"12. Licenses for existing assets
655"
LICENSES FOR EXISTING ASSETS,0.9185360094451004,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
656"
LICENSES FOR EXISTING ASSETS,0.9197166469893743,"the paper, properly credited and are the license and terms of use explicitly mentioned and
657"
LICENSES FOR EXISTING ASSETS,0.9208972845336482,"properly respected?
658"
LICENSES FOR EXISTING ASSETS,0.922077922077922,"Answer: [Yes]
659"
LICENSES FOR EXISTING ASSETS,0.9232585596221959,"Justification: See the Appendix.
660"
LICENSES FOR EXISTING ASSETS,0.9244391971664699,"Guidelines:
661"
LICENSES FOR EXISTING ASSETS,0.9256198347107438,"â€¢ The answer NA means that the paper does not use existing assets.
662"
LICENSES FOR EXISTING ASSETS,0.9268004722550177,"â€¢ The authors should cite the original paper that produced the code package or dataset.
663"
LICENSES FOR EXISTING ASSETS,0.9279811097992916,"â€¢ The authors should state which version of the asset is used and, if possible, include a
664"
LICENSES FOR EXISTING ASSETS,0.9291617473435655,"URL.
665"
LICENSES FOR EXISTING ASSETS,0.9303423848878394,"â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
666"
LICENSES FOR EXISTING ASSETS,0.9315230224321134,"â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of
667"
LICENSES FOR EXISTING ASSETS,0.9327036599763873,"service of that source should be provided.
668"
LICENSES FOR EXISTING ASSETS,0.9338842975206612,"â€¢ If assets are released, the license, copyright information, and terms of use in the
669"
LICENSES FOR EXISTING ASSETS,0.935064935064935,"package should be provided. For popular datasets, paperswithcode.com/datasets
670"
LICENSES FOR EXISTING ASSETS,0.9362455726092089,"has curated licenses for some datasets. Their licensing guide can help determine the
671"
LICENSES FOR EXISTING ASSETS,0.9374262101534829,"license of a dataset.
672"
LICENSES FOR EXISTING ASSETS,0.9386068476977568,"â€¢ For existing datasets that are re-packaged, both the original license and the license of
673"
LICENSES FOR EXISTING ASSETS,0.9397874852420307,"the derived asset (if it has changed) should be provided.
674"
LICENSES FOR EXISTING ASSETS,0.9409681227863046,"â€¢ If this information is not available online, the authors are encouraged to reach out to
675"
LICENSES FOR EXISTING ASSETS,0.9421487603305785,"the assetâ€™s creators.
676"
NEW ASSETS,0.9433293978748524,"13. New Assets
677"
NEW ASSETS,0.9445100354191264,"Question: Are new assets introduced in the paper well documented and is the documentation
678"
NEW ASSETS,0.9456906729634003,"provided alongside the assets?
679"
NEW ASSETS,0.9468713105076741,"Answer: [Yes]
680"
NEW ASSETS,0.948051948051948,"Justification: See the Appendix.
681"
NEW ASSETS,0.9492325855962219,"Guidelines:
682"
NEW ASSETS,0.9504132231404959,"â€¢ The answer NA means that the paper does not release new assets.
683"
NEW ASSETS,0.9515938606847698,"â€¢ Researchers should communicate the details of the dataset/code/model as part of their
684"
NEW ASSETS,0.9527744982290437,"submissions via structured templates. This includes details about training, license,
685"
NEW ASSETS,0.9539551357733176,"limitations, etc.
686"
NEW ASSETS,0.9551357733175915,"â€¢ The paper should discuss whether and how consent was obtained from people whose
687"
NEW ASSETS,0.9563164108618654,"asset is used.
688"
NEW ASSETS,0.9574970484061394,"â€¢ At submission time, remember to anonymize your assets (if applicable). You can either
689"
NEW ASSETS,0.9586776859504132,"create an anonymized URL or include an anonymized zip file.
690"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9598583234946871,"14. Crowdsourcing and Research with Human Subjects
691"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.961038961038961,"Question: For crowdsourcing experiments and research with human subjects, does the paper
692"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9622195985832349,"include the full text of instructions given to participants and screenshots, if applicable, as
693"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9634002361275088,"well as details about compensation (if any)?
694"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9645808736717828,"Answer: [NA]
695"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9657615112160567,"Justification:
696"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9669421487603306,"Guidelines:
697"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681227863046045,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
698"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693034238488784,"human subjects.
699"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704840613931524,"â€¢ Including this information in the supplemental material is fine, but if the main contribu-
700"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716646989374262,"tion of the paper involves human subjects, then as much detail as possible should be
701"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728453364817001,"included in the main paper.
702"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974025974025974,"â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
703"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752066115702479,"or other labor should be paid at least the minimum wage in the country of the data
704"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9763872491145218,"collector.
705"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775678866587958,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
706"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787485242030697,"Subjects
707"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9799291617473436,"Question: Does the paper describe potential risks incurred by study participants, whether
708"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811097992916175,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
709"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9822904368358913,"approvals (or an equivalent approval/review based on the requirements of your country or
710"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9834710743801653,"institution) were obtained?
711"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9846517119244392,"Answer: [NA]
712"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858323494687131,"Justification:
713"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987012987012987,"Guidelines:
714"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881936245572609,"â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with
715"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893742621015348,"human subjects.
716"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905548996458088,"â€¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
717"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917355371900827,"may be required for any human subjects research. If you obtained IRB approval, you
718"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929161747343566,"should clearly state this in the paper.
719"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9940968122786304,"â€¢ We recognize that the procedures for this may vary significantly between institutions
720"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952774498229043,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
721"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9964580873671782,"guidelines for their institution.
722"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976387249114522,"â€¢ For initial submissions, do not include any information that would break anonymity (if
723"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988193624557261,"applicable), such as the institution conducting the review.
724"
