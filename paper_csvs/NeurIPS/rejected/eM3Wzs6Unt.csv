Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010298661174047373,"The option framework in hierarchical reinforcement learning has notably advanced
1"
ABSTRACT,0.0020597322348094747,"the automatic discovery of temporally-extended actions from long-horizon tasks.
2"
ABSTRACT,0.003089598352214212,"However, existing methods often struggle with ineffective exploration and unstable
3"
ABSTRACT,0.004119464469618949,"updates when learning action and option policies simultaneously. Addressing these
4"
ABSTRACT,0.005149330587023687,"challenges, we introduce the Variational Markovian Option Critic (VMOC), an
5"
ABSTRACT,0.006179196704428424,"off-policy algorithm with provable convergence that employs variational inference
6"
ABSTRACT,0.007209062821833162,"to stabilize updates. VMOC naturally integrates maximum entropy as intrinsic re-
7"
ABSTRACT,0.008238928939237899,"wards to promote the exploration of diverse and effective options. Furthermore, we
8"
ABSTRACT,0.009268795056642637,"adopt low-cost option embeddings instead of traditional, computationally expensive
9"
ABSTRACT,0.010298661174047374,"option triples, enhancing scalability and expressiveness. Extensive experiments in
10"
ABSTRACT,0.01132852729145211,"challenging Mujoco environments validate VMOC’s superior performance over ex-
11"
ABSTRACT,0.012358393408856848,"isting on-policy and off-policy methods, demonstrating its effectiveness in learning
12"
ABSTRACT,0.013388259526261586,"coherent and diverse option sets suitable for complex tasks.
13"
INTRODUCTION,0.014418125643666324,"1
Introduction
14"
INTRODUCTION,0.015447991761071062,"Recent advancements in deep reinforcement learning (DRL) have demonstrated significant successes
15"
INTRODUCTION,0.016477857878475798,"across a variety of complex domains, such as mastering the human level of atari [36] and Go [44]
16"
INTRODUCTION,0.017507723995880537,"games. These achievements underscore the potential of combining reinforcement learning (RL)
17"
INTRODUCTION,0.018537590113285273,"with powerful function approximators like neural networks [5] to tackle intricate tasks that require
18"
INTRODUCTION,0.01956745623069001,"nuanced control over extended periods. Despite these breakthroughs, Deep RL still faces substantial
19"
INTRODUCTION,0.02059732234809475,"challenges, such as insufficient exploration in dynamic environments [18, 13, 42], inefficient learning
20"
INTRODUCTION,0.021627188465499485,"associated with temporally extended actions [6, 9] and long horizon tasks [30, 4], and vast amounts
21"
INTRODUCTION,0.02265705458290422,"of samples required for training proficient behaviors [16, 40, 15].
22"
INTRODUCTION,0.02368692070030896,"One promising area for addressing these challenges is the utilization of hierarchical reinforcement
23"
INTRODUCTION,0.024716786817713696,"learning (HRL) [11, 2, 12], a diverse set of strategies that decompose complex tasks into simpler, hier-
24"
INTRODUCTION,0.025746652935118436,"archical structures for more manageable learning. Among these strategies, the option framework [47],
25"
INTRODUCTION,0.026776519052523172,"developed on the Semi-Markov Decision Process (SMDP), is particularly effective at segmenting
26"
INTRODUCTION,0.027806385169927908,"non-stationary task stages into temporally-extended actions known as options. Options are typically
27"
INTRODUCTION,0.028836251287332648,"learned through a maximum likelihood approach that aims to maximize the expected rewards across
28"
INTRODUCTION,0.029866117404737384,"trajectories. In this framework, options act as temporally abstracted actions executed over variable
29"
INTRODUCTION,0.030895983522142123,"time steps, controlled by a master policy that decides when each option should execute and terminate.
30"
INTRODUCTION,0.03192584963954686,"This structuring not only simplifies the management of complex environments but also enables the
31"
INTRODUCTION,0.032955715756951595,"systematic discovery and execution of temporal abstractions over long-horizon tasks [24, 23].
32"
INTRODUCTION,0.03398558187435633,"However, the underlying SMDP framework is frequently undermined by three key challenges:
33"
INTRODUCTION,0.035015447991761074,"1) Insufficient exploration and degradation [20, 37, 23]. As options are unevenly updated using
34"
INTRODUCTION,0.03604531410916581,"conventional maximum likelihood methods [4, 10, 45, 25, 26], the policy is quickly saturated with
35"
INTRODUCTION,0.037075180226570546,"early rewarding observations. This typically results in focusing on only low-entropy options that lead
36"
INTRODUCTION,0.03810504634397528,"to local optima rewards, causing a single option to either dominate the entire policy or switch every
37"
INTRODUCTION,0.03913491246138002,"timestep. Such premature convergence limits option diversity significantly. 2) Sample Inefficiency.
38"
INTRODUCTION,0.04016477857878476,"The semi-Markovian nature inherently leads to sample inefficiency [47, 29]: each policy update
39"
INTRODUCTION,0.0411946446961895,"at the master level extends over multiple time steps, thus consuming a considerable volume of
40"
INTRODUCTION,0.042224510813594233,"experience samples with relatively low informational gain. This inefficiency is further exacerbated
41"
INTRODUCTION,0.04325437693099897,"by the prevalence of on-policy option learning algorithms [4, 52], which require new samples to be
42"
INTRODUCTION,0.044284243048403706,"collected simultaneously from both high-level master policies and low-level action policies at each
43"
INTRODUCTION,0.04531410916580844,"gradient step, and thus sample expensive. 3) Computationally expensive. Options are conventionally
44"
INTRODUCTION,0.046343975283213185,"defined as triples [4] with intra-option policies and termination functions, often modeled using neural
45"
INTRODUCTION,0.04737384140061792,"networks which are expensive to optimize. These challenges collectively limit the broader adoption
46"
INTRODUCTION,0.04840370751802266,"and effectiveness of the option framework in real-world scenarios, particularly in complex continuous
47"
INTRODUCTION,0.04943357363542739,"environments where scalability and stability are critical [14, 34, 26].
48"
INTRODUCTION,0.05046343975283213,"To address these challenges, we introduce the Variational Markovian Option Critic (VMOC), a
49"
INTRODUCTION,0.05149330587023687,"novel off-policy algorithm that integrates the variational inference framework on option-induced
50"
INTRODUCTION,0.05252317198764161,"MDPs [35]. We first formulate the optimal option-induced SMDP trajectory as a probabilistic
51"
INTRODUCTION,0.053553038105046344,"inference problem, presenting a theoretical convergence proof of the variational distribution under
52"
INTRODUCTION,0.05458290422245108,"the soft policy iteration framework [19]. Similar to prior variational methods [31], policy entropy
53"
INTRODUCTION,0.055612770339855816,"terms naturally arise as intrinsic rewards during the inference procedure. As a result, VMOC not
54"
INTRODUCTION,0.05664263645726056,"only seeks high-reward options but also maximizes entropy across the space, promoting extensive
55"
INTRODUCTION,0.057672502574665295,"exploration and maintaining high diversity. We implements this inference procedure as an off-policy
56"
INTRODUCTION,0.05870236869207003,"soft actor critic [19] algorithm, which allows reusing samples from replay buffer and enhances sample
57"
INTRODUCTION,0.05973223480947477,"efficiency. Furthermore, to address the computational inefficiencies associated with conventional
58"
INTRODUCTION,0.0607621009268795,"option triples, we follow [35] and employ low-cost option embeddings rather than complex neural
59"
INTRODUCTION,0.061791967044284246,"network models. This not only simplifies the training process but also enhances the expressiveness of
60"
INTRODUCTION,0.06282183316168898,"the model by allowing the agent to capture a more diverse set of environmental dynamics.
61"
INTRODUCTION,0.06385169927909372,"Our contributions can be summarized as follows:
62"
INTRODUCTION,0.06488156539649846,"• We propose a variational inference approach within the maximum entropy framework to
63"
INTRODUCTION,0.06591143151390319,"enhance diverse and robust exploration of options.
64"
INTRODUCTION,0.06694129763130793,"• We implement an off-policy algorithm that improves sample efficiency.
65"
INTRODUCTION,0.06797116374871266,"• We introduce option embeddings into latent variable policies and enhance expressiveness
66"
INTRODUCTION,0.0690010298661174,"and computational cost-effectiveness of option representations.
67"
INTRODUCTION,0.07003089598352215,"• We conduct extensive experiments in OpenAI Gym Mujoco [49] environments, demonstrat-
68"
INTRODUCTION,0.07106076210092688,"ing that VMOC significantly outperforms other option-based variants in terms of exploration
69"
INTRODUCTION,0.07209062821833162,"capabilities, sample efficiency, and computational efficiency.
70"
PRELIMINARY,0.07312049433573635,"2
Preliminary
71"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.07415036045314109,"2.1
Control as Structured Variational Inference
72"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.07518022657054584,"Conventionally, the control as inference framework [19, 31, 19, 53] is derived using the maximum
73"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.07621009268795056,"entropy objective. In this section, we present an alternative derivation from the perspective of
74"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.07723995880535531,"structured variational inference. We demonstrate that this approach provides a more concise and
75"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.07826982492276004,"intuitive pathway to the same theoretical results, where the maximum entropy principle naturally
76"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.07929969104016478,"emerges through the direct application of variational inference techniques.
77"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08032955715756952,"Traditional control methods focus on directly maximizing rewards, often resulting in suboptimal trade-
78"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08135942327497425,"offs between exploration and exploitation. By reinterpreting the control problem as a probabilistic
79"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.082389289392379,"inference problem, the control as inference framework incorporates both the reward structure and
80"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08341915550978372,"environmental uncertainty into decision-making, providing a more robust and flexible approach
81"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08444902162718847,"to policy optimization. In this framework, optimality is represented by a binary random variable
82"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.0854788877445932,"E ∈{0, 1}1. The probability of optimality given a state-action pair (s, a) is denoted as P(E =
83"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08650875386199794,"1 | s, a) = exp(r(s, a)), which is an exponential function of the conventional reward function
84"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08753861997940268,"r(s, a) that measures the desirability of an action in a specific state. Focusing on E = 1 captures the
85"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08856848609680741,"occurrence of optimal events. For simplicity, we will use E instead of E = 1 in the following text
86"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.08959835221421215,"1Conventionally, the optimality variable is denoted by O. However, in this context, we use E to avoid conflict
with notation used in the option framework."
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09062821833161688,"to avoid cluttered notations. The joint distribution over trajectories τ = (s1, a1, . . . , sT , aT ) given
87"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09165808444902163,"optimality is expressed as:
88"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09268795056642637,"P(τ|E1:T ) ∝P(τ, E1:T ) = P(s1)"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.0937178166838311,"T −1
Y"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09474768280123584,"t=1
P(st+1|st, at)P(Et|st, at)"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09577754891864057,"where P(s1) is the initial state distribution, P(st+1|st, at) is the dynamics model. As explained
89"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09680741503604531,"in [19, 31], direct optimization of P(τ | E1:T ) can result in an optimistic policy that assumes a degree
90"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09783728115345006,"of control over the dynamics. One way to correct this risk-seeking behavior [31] is through structured
91"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09886714727085479,"variational inference. In our case, the goal is to approximate the optimal trajectory P(τ) with the
92"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.09989701338825953,"variational distribution:
93"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10092687950566426,q(τ) = P(s1)
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.101956745623069,"T −1
Y"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10298661174047374,"t=1
P(st+1 | st, at)q(at | st)"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10401647785787847,"where the initial distribution P(s1) and transition distribution P(st+1 | st, at) is set to be the true
94"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10504634397528322,"environment dynamics from P(τ). The only variational term is the variational policy q(at | st),
95"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10607621009268794,"which is used to approximate the optimal policy P(at | st, E1:T ). Under this setting, the environment
96"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10710607621009269,"dynamics will be canceled out from the optimization objective between P(τ | E) and q(τ), thus
97"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10813594232749743,"explicitly disallowing the agent to influence its dynamics and correcting the risk-seeking behavior.
98"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.10916580844490216,"With the variational distribution at hand, the conventional maximum entropy framework can be
99"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.1101956745623069,"recovered through a direct application of standard structural variational inference [28]:
100"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11122554067971163,"log P(E1:T ) = L(q(τ), P(τ, E1:T )) + DKL(q(τ) ∥P(τ|E1:T ))"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11225540679711637,"= Eτ∼q(τ)[ T
X"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11328527291452112,"t
r(st, at) + H(q(·|st))]"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11431513903192585,"|
{z
}
maximum entropy objective"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11534500514933059,"+DKL(q(at|st) ∥P(at|st, E1:T ))"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11637487126673532,"where L(q, P) = Eq[log P"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11740473738414006,"q ] is the Evidence Lower Bound (ELBO) [28]. The maximum entropy
101"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.1184346035015448,"objective arises naturally as the environment dynamics in P(τ, E) and q(τ) cancel out. Under this
102"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.11946446961894953,"formulation, the soft policy iteration theorem [19] has an elegant Expectation-Maximization (EM)
103"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12049433573635428,"algorithm [28] interpretation: the E-step corresponds to the policy evaluation of the maximum
104"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.121524201853759,"entropy objective L(q[k], P); while the M-step corresponds to the policy improvement of the DKL
105"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12255406797116375,"term q[k+1] = arg maxq DKL(q[k](τ) ∥P(τ | E)). Thus, soft policy iteration is an exact inference if
106"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12358393408856849,"both EM steps can be performed exactly.
107"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12461380020597322,"Theorem 1 (Convergence Theorem for Soft Policy Iteration). Let τ be the latent variable and E
108"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12564366632337795,"be the observed variable. Define the variational distribution q(τ) and the log-likelihood log P(E).
109"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.1266735324407827,"Let M : q[k] →q[k+1] represent the mapping defined by the EM steps inference update, so that
110"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12770339855818744,"q[k+1] = M(q[k]). The likelihood function increases at each iteration of the variational inference
111"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12873326467559218,"algorithm until convergence conditions are satisfied.
112"
CONTROL AS STRUCTURED VARIATIONAL INFERENCE,0.12976313079299692,"Proof. See Appendix A.1.
113"
THE OPTION FRAMEWORK,0.13079299691040164,"2.2
The Option Framework
114"
THE OPTION FRAMEWORK,0.13182286302780638,"In conventional SMDP-based Option Framework [47], an option is a triple (Io, πo, βo) ∈O, where O
115"
THE OPTION FRAMEWORK,0.13285272914521112,"denotes the option set; o ∈O = {1, 2, . . . , K} is a positive integer index which denotes the o-th triple
116"
THE OPTION FRAMEWORK,0.13388259526261587,"where K is the number of options; Io is an initiation set indicating where the option can be initiated;
117"
THE OPTION FRAMEWORK,0.1349124613800206,"πo = Po(a|s) : A × S →[0, 1] is the action policy of the oth option; βo = Po(b = 1|s) : S →[0, 1]
118"
THE OPTION FRAMEWORK,0.13594232749742532,"where b ∈{0, 1} is a termination function. For clarity, we use Po(b = 1|s) instead of βo which is
119"
THE OPTION FRAMEWORK,0.13697219361483007,"widely used in previous option literatures (e.g., Sutton et al. [47], Bacon et al. [4]). A master policy
120"
THE OPTION FRAMEWORK,0.1380020597322348,"π(o|s) = P(o|s) where o ∈O is used to sample which option will be executed. Therefore, the
121"
THE OPTION FRAMEWORK,0.13903192584963955,"dynamics (stochastic process) of the option framework is written as:
122"
THE OPTION FRAMEWORK,0.1400617919670443,"P(τ) = P(s0, o0) ∞
Y"
THE OPTION FRAMEWORK,0.141091658084449,"t=1
P(st|st−1, at−1)Pot(at|st)"
THE OPTION FRAMEWORK,0.14212152420185376,"[Pot−1(bt = 0|st)1ot=ot−1 + Pot−1(bt = 1|st)P(ot|st)],
(1)"
THE OPTION FRAMEWORK,0.1431513903192585,"where τ = {s0, o0, a0, s1, o1, a1, . . .} denotes the trajectory of the option framework. 1 is an
123"
THE OPTION FRAMEWORK,0.14418125643666324,"indicator function and is only true when ot = ot−1 (notice that ot−1 is the realization at ot−1).
124"
THE OPTION FRAMEWORK,0.14521112255406798,"Therefore, under this formulation the option framework is defined as a Semi-Markov process since
125"
THE OPTION FRAMEWORK,0.1462409886714727,"the dependency on an activated option o can cross a variable amount of time [47]. Due to the nature
126"
THE OPTION FRAMEWORK,0.14727085478887744,"of SMDP assumption, conventional option framework is unstable and computationally expensive to
127"
THE OPTION FRAMEWORK,0.14830072090628219,"optimize. Li et al. [34, 35] proposed the Hidden Temporal Markovian Decision Process (HiT-MDP):
128"
THE OPTION FRAMEWORK,0.14933058702368693,"P(τ) = P(s0, o0) ∞
Y"
THE OPTION FRAMEWORK,0.15036045314109167,"t=1
P(st|st−1, at−1)P(at|st, ot)P(ot|st, ot−1)
(2)"
THE OPTION FRAMEWORK,0.1513903192584964,"and theoretically proved that the option-induced HiT-MDP is homomorphically equivalent to the
129"
THE OPTION FRAMEWORK,0.15242018537590113,"conventional SMDP-based option framework. Following RL conventions, we use πA = P(at|st, ot)
130"
THE OPTION FRAMEWORK,0.15345005149330587,"to denote the action policy and πO = P(ot|st, ot−1) to denote the option policy respectively. In
131"
THE OPTION FRAMEWORK,0.15447991761071062,"HiT-MDPs, options can be viewed as latent variables with a temporal structure P(ot|st, ot−1),
132"
THE OPTION FRAMEWORK,0.15550978372811536,"enabling options to be represented as dense latent embeddings rather than traditional option triples.
133"
THE OPTION FRAMEWORK,0.15653964984552007,"They demonstrated that learning options as embeddings on HiT-MDPs offers significant advantages
134"
THE OPTION FRAMEWORK,0.15756951596292482,"in performance, scalability, and stability by reducing variance. However, their work only derived an
135"
THE OPTION FRAMEWORK,0.15859938208032956,"on-policy policy gradient algorithm for learning options on HiT-MDPs. In this work, we extend their
136"
THE OPTION FRAMEWORK,0.1596292481977343,"approach to an off-policy algorithm under the variational inference framework, enhancing exploration
137"
THE OPTION FRAMEWORK,0.16065911431513905,"and sample efficiency.
138"
METHODOLOGY,0.16168898043254376,"3
Methodology
139"
METHODOLOGY,0.1627188465499485,"In this section, we introduce the Variational Markovian Option Critic (VMOC) algorithm by extending
140"
METHODOLOGY,0.16374871266735325,"the variational policy iteration (Theorem 1) to the option framework. In Section 3.1, we reformulate
141"
METHODOLOGY,0.164778578784758,"the optimal option trajectory and the variational distribution as probabilistic graphical models (PGMs),
142"
METHODOLOGY,0.16580844490216273,"propose the corresponding variational objective, and present a provable exact inference procedure for
143"
METHODOLOGY,0.16683831101956745,"these objectives in tabular settings. Section 3.2 extends this result by introducing VMOC, a practical
144"
METHODOLOGY,0.1678681771369722,"off-policy option learning algorithm that uses neural networks as function approximators and proves
145"
METHODOLOGY,0.16889804325437693,"the convergence of VMOC under approximate inference settings. Our approach differs from previous
146"
METHODOLOGY,0.16992790937178168,"works [19, 33, 34] by leveraging structured variational inference directly, providing a more concise
147"
METHODOLOGY,0.1709577754891864,"pathway to both theoretical results and practical algorithms.
148"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.17198764160659114,"3.1
PGM Formulations of The Option Framework
149"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.17301750772399588,"Formulating complex problems as probabilistic graphical models (PGMs) offers a consistent and
150"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.17404737384140062,"flexible framework for deriving principled objectives, analyzing convergence, and devising practical
151"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.17507723995880536,"algorithms. In this section, we first formulate the optimal trajectory of the conventional SMDP-based
152"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.17610710607621008,"option framework (Eq. 1) as a PGM. We then use the HiT-MDPs as the variational distribution to
153"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.17713697219361482,"approximate this optimal trajectory. With these PGMs, we can straightforwardly derive the variational
154"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.17816683831101957,"objective, where maximum entropy terms arise naturally. This approach allows us to develop a stable
155"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.1791967044284243,"algorithm for learning diversified options and preventing degeneracy. Specifically, we follow [31, 28]"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.18022657054582905,Figure 1: PGMs of the option framework. 156
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.18125643666323377,"by introducing the concept of ""Optimality"" [48] into the conventional SMDP-based option framework
157"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.1822863027806385,"(Equation equation 1). This allows us to define the probability of an option trajectory being optimal
158"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.18331616889804325,"as a probabilistic graphical model (PGM), as illustrated in Figure 1 (a):
159"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.184346035015448,"P(τ, EA
1:T , EO
1:T ) = P(s0, o0) T
Y"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.18537590113285274,"t=1
P(st+1|st, at)P(EA
t = 1|st, at)P(EO
t = 1|st, at, ot, ot−1)P(ot)P(at)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.18640576725025745,"∝P(s0) T
Y"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.1874356333676622,"t=1
P(st+1|st, at)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.18846549948506694,"|
{z
}
Environment Dynamics T
Y"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.18949536560247168,"t=1
P(EA
t = 1|st, at)P(EO
t = 1|st, at, ot, ot−1)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19052523171987643,"|
{z
}
Optimality Likelihood ,
(3)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19155509783728114,"where E ∈{0, 1} are observable binary “optimal random variables” [31], τ = {s0, o0, a0, s1 . . .}
160"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19258496395468588,"denotes the trajectory of the option framework. The agent is optimal at time step t when P(EA
t =
161"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19361483007209063,"1|st, at) and P(EO
t
= 1|st, at, ot, ot−1). We will use E instead of E = 1 in the following text to
162"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19464469618949537,"avoid cluttered notations. To simplify the derivation, priors P(o) and P(a) can be assumed to be
163"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.1956745623069001,"uniform distributions without loss of generality [31]. Note that Eq. 3 shares the same environment
164"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19670442842430483,"dynamics with Eq. 1 and Eq. 2. With the optimal random variables EO and EA, the likelihood of a
165"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19773429454170957,"state-action {st, at} pair that is optimal is defined as:
166"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19876416065911431,"P(EA
t |st, at) = exp(r(st, at)),
(4)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.19979402677651906,"as this specific design facilitates recovering the value function at the latter structural variational infer-
167"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2008238928939238,"ence stage. Based on the same motivation, the likelihood of an option-state-action {ot, st, at, ot−1}
168"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.20185375901132852,"pair that is optimal is defined as,
169"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.20288362512873326,"P(EO
t |st, at, ot, ot−1) = exp(f(ot, st, at, ot−1)),
(5)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.203913491246138,"where f(·) is an arbitrary non-positive function which measures the preferable of selecting an option
170"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.20494335736354274,"given state-action pair [st, at] and the previous executed option ot−1. In this work, we choose f to
171"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2059732234809475,"be the mutual-information f = I[ot|st, at, ot−1] as a fact that when the uniform prior assumption of
172"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2070030895983522,"P(o) is relaxed the optimization introduces a mutual-information as a regularizer [35].
173"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.20803295571575695,"As explained in Section 2.1, direct optimization of Eq. 3 results in optimistic policies that assumes a
174"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2090628218331617,"degree of control over the dynamics. We correct this risk-seeking behavior [31] through approximating
175"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21009268795056643,"the optimal trajectory P(τ) with the variational distribution:
176"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21112255406797117,"q(τ) = P(s0, o0)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2121524201853759,"T −1
Y"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21318228630278063,"t=1
P(st+1|st, at)q(at|st, ot)q(ot|st, ot−1)
(6)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21421215242018538,"where the initial distribution P(s0, o0) and transition distribution P(st+1 | st, at) is set to be the true
177"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21524201853759012,"environment dynamics from P(τ). The variational distribution turns out to be the HiT-MDP, where
178"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21627188465499486,"the action policy q(at | st) and the option policy q(ot|st, ot−1) are used to approximate the optimal
179"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21730175077239958,"policy P(at|st, ot, EA
1:T ) and P(ot|st, ot−1, EO
1:T ). The Evidence Lower Bound (ELBO) [28] of the
180"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21833161688980432,"log-likelihood optimal trajectory (Eq. 3) can be derived as (see Appendix A.3):
181"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.21936148300720906,"L(q(τ), P(τ, EA
1:T , EO
1:T )) = Eq(τ)[log P(τ, EA
1:T , EO
1:T ) −log q(τ)]"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2203913491246138,"= Eq(τ)[r(st, at) + f(·) −log q(at|st, ot) −log q(ot|st, ot−1)]"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.22142121524201855,"= Eq(τ)

r(st, at) + f(·) + H[πA] + H[πO]

(7)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.22245108135942326,"where line 2 is substituting Eq. 3 and Eq. 6 into the ELBO. As a result, the maximum entropy
182"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.223480947476828,"objective naturally arises in Eq. 7. Optimizing the ELBO not only seeks high-reward options but also
183"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.22451081359423275,"maximizes entropy across the space, promoting extensive exploration and maintaining high diversity.
184"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2255406797116375,"Given the ELBO, we now define soft value functions of the option framework following the Bellman
185"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.22657054582904224,"Backup Functions along the trajectory q(τ) as bellow:
186"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.22760041194644695,"Qsoft
O
[st, ot] = f(·) + Eat∼πA
h
Qsoft
A
[st, ot, at]
i
+ H[πA],
(8)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2286302780638517,"Qsoft
A
[st, ot, at] = r(s, a) + Est+1∼P (st+1|st,at)
h
Eot+1∼πO
h
Qsoft
O
[st+1, ot+1]
i
+ H[πO]
i
(9)"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.22966014418125644,"Assuming policies πA, πO ∈Π where Π is an arbitrary feasible set, under a tabular setting where the
187"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23069001029866118,"inference on L can be done exactly, we have the following theorem holds:
188"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23171987641606592,"Theorem 2 (Soft Option Policy Iteration Theorem). Repeated optimizing L and DKL defined in
189"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23274974253347064,"Eq. 10 from any πA
0 , πO
0 ∈Π converges to optimal policies πA∗, πO∗such that Qsoft∗
O
[st, ot] ≥
190"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23377960865087538,"Qsoft
O
[st, ot] and Qsoft∗
A
[st, ot, at] ≥Qsoft
A
[st, ot, at], for all πA
0 , πO
0
∈Π and (st, at, ot) ∈
191"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23480947476828012,"S × A × O, assuming under tabular settings where |S| < ∞, |O| < ∞, |A| < ∞.
192"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23583934088568487,"Proof. See Appendix A.2.
193"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2368692070030896,"Theorem 2 guarantees finding the optimal solution only when the inference can be done exactly
194"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23789907312049433,"under tabular settings. However, real-world applications often involve large continuous domains and
195"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.23892893923789907,"employ neural networks as function approximators. In these cases, inference procedures can only be
196"
PGM FORMULATIONS OF THE OPTION FRAMEWORK,0.2399588053553038,"done approximately. This necessitate a practical approximation algorithm which we present below.
197"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.24098867147270855,"3.2
Variational Markovian Option Critic Algorithm
198"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2420185375901133,"Formulating complex problems as probabilistic graphical models (PGMs) allowing us to leverage
199"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.243048403707518,"established methods from PGM literature to address the associated inference and learning challenges
200"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.24407826982492276,"in real-world applications. To this end, we utilizes the structured variational inference treatment for
201"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2451081359423275,"optimizing the log-likelihood of optimal trajectory and prove its convergence under approximate
202"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.24613800205973224,"inference settings. Specifically, using the variational distribution q(τ) (Eq. 6) as an approximator, the
203"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.24716786817713698,"ELBO can be derived as (see Appendix A.3):
204"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2481977342945417,"L(q(τ), P(τ, EA
1:T , EO
1:T )) = −DKL(q(τ)||P(τ|EA
1:T , EO
1:T )) + log P(EA
1:T , EO
1:T )
(10)"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.24922760041194644,"where DKL is the KL-Divergence between the trajectory following variational policies q(τ) and
205"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.25025746652935116,"optimal policies P(τ|EA
1:T , EO
1:T ). Under the structural variational inference [28] perspective, con-
206"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2512873326467559,"vergence to the optimal policy can be achieved by optimizing the ELBO with respect to the the
207"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.25231719876416064,"variational policy repeatedly:
208"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2533470648815654,"Theorem 3 (Convergence Theorem for Variational Markovian Option Policy Iteration). Let τ be
209"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.25437693099897013,"the latent variable and EA, EO be the ground-truth optimality variables. Define the variational
210"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2554067971163749,"distribution q(τ) and the true log-likelihood of optimality log P(EA, EO). iterates according to the
211"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2564366632337796,"update rule qk+1 = arg maxq L(q(τ), P(τ, EA
1:T , EO
1:T )) converges to the maximum value bounded
212"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.25746652935118436,"by the true log-likelihood of optimality.
213"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2584963954685891,"Proof. See Appendix A.4.
214"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.25952626158599384,"We further implements a practical algorithm, the Variational Markovian Option Critic (VMOC)
215"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.26055612770339853,"algorithm, which is suitable for complex continuous domains. Specifically, we employ parameterized
216"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2615859938208033,"neural networks as function approximators for both the Q-functions (Qsoft
ψA , Qsoft
ψO ) and the policies
217"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.262615859938208,"(πθA, πθO). Instead of running evaluation and improvement to full convergence using Theorem 2, we
218"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.26364572605561276,"can optimize the variational distribution by taking stochastic gradient descent following Theorem 3
219"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2646755921730175,"with respect to the ELBO (Eq. 7) directly. Share the same motivation with Haarnoja et al. [19]
220"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.26570545829042225,"of reducing the variance during the optimization procedure, we derive an option critic framework
221"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.266735324407827,"by optimizing the maximum entropy objectives between the action Eq. 9 and the option Eq. 8
222"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.26776519052523173,"alternatively. The Bellman residual for the action critic is:
223"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2687950566426365,"JQA(ψA
i ) = E(st,ot,at,st+1)∼D"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2698249227600412,"
min
i=1,2 QψA
i (st, ot, at)−"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2708547888774459," 
r(st, at) + Eot+1∼πO
h
Qsoft
O
[st+1, ot+1]
i
+ αOH[πO]
2"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.27188465499485065,"where αO is the temperature hyper-parameter and the expectation over option random variable
224"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2729145211122554,"Eot+1∼πO can be evaluated exactly since πO is a discrete distribution. The Bellman residual for the
225"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.27394438722966014,"option critic is:
226"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2749742533470649,"JQO(ψO
i ) = E(st,ot)∼D"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2760041194644696,"
min
i=1,2 QO
ψO
i (st, ot)−"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.27703398558187436," 
f(·) + Eat∼πA
h
Qsoft
A
[st, ot, at] −αA log q(at|st, ot)
i 2"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2780638516992791,"αA is the temperature hyper-parameter. Unlike Eot+1∼πO can be trivially evaluated, evaluating
227"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.27909371781668385,"Eat∼πA is typically intractable. Therefore, in implementation we use at sampled from the replay
228"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2801235839340886,"buffer to estimate the expectation over πA.
229"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2811534500514933,"Following Theorem 3, the policy gradients can be derived by directly taking gradient with respect to
230"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.282183316168898,"the ELBOs defined for the action Eq. 9 and the option Eq. 8 policies respectively. The action policy
231"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.28321318228630277,"objective is given by:
232"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2842430484037075,"JπA(θA) = −E(st,ot)∼D"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.28527291452111225,"
min
i=1,2 QψA
i (st, ot, ˜at) −αA log q(˜at|st, ot)

, ˜at ∼q(·|st, ot)"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.286302780638517,"where in practice the action policy is often sampled by using the re-parameterization trick introduced
233"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.28733264675592174,"in [19]. The option objective is given by:
234"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2883625128733265,"JπO(θO) = −E(st,ot−1)∼D"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2893923789907312,"
min
i=1,2 QψO
i (st, ot) + αOH[πO]
"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.29042224510813597,"The variational distribution q(τ) defined in Eq. 6 allows us to learn options as embeddings [34, 35]
235"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.29145211122554066,"with a learnable embedding matrix W ∈Rnum_options×embedding_dim. Under this setting, the embedding
236"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2924819773429454,"matrix W can be absorbed into the parameter vector θO. This integration into VMOC ensures that
237"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.29351184346035014,"options are represented as embeddings without any additional complications, thereby enhancing the
238"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2945417095777549,"expressiveness and scalability of the model.
239"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2955715756951596,"The temperature hyper-parameters can also be adjusted by minimizing the following objective:
240"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.29660144181256437,"J(αA) = −E˜at∼πA

αA(log πA(˜at | st, ot) + H)
"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2976313079299691,"for the action policy temperature αA, where H is a target entropy. Similarly, the option policy
241"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.29866117404737386,"temperature αO can be adjusted by:
242"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.2996910401647786,"J(αO) = −Eot∼πO

αO(log πO(ot | st, ot−1) + H)
"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.30072090628218334,"where H is also a target entropy for the option policy. In both cases, the temperatures αA and αO
243"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.30175077239958803,"are updated using gradient descent, ensuring that the entropy regularization terms dynamically adapt
244"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.3027806385169928,"to maintain a desired level of exploration. This approach aligns with the methodology proposed
245"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.3038105046343975,"in SAC [19]. By adjusting the temperature parameters, the VMOC algorithm ensures a balanced
246"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.30484037075180226,"trade-off between exploration and exploitation, which is crucial for achieving optimal performance in
247"
VARIATIONAL MARKOVIAN OPTION CRITIC ALGORITHM,0.305870236869207,"complex continuous control tasks. We summarize the VMOC algorithm in Appendix B.
248"
EXPERIMENTS,0.30690010298661174,"4
Experiments
249"
EXPERIMENTS,0.3079299691040165,"In this section, we design experiments on the challenging single task OpenAI Gym MuJoCo [7]
250"
EXPERIMENTS,0.30895983522142123,"environments (10 environments) to test Variational Markovian Option Critic (VMOC)’s performance
251"
EXPERIMENTS,0.309989701338826,"over other option variants and non-option baselines.
252"
EXPERIMENTS,0.3110195674562307,"For VMOC in all environments, we fix the temperature rate for both αO and αA to 0.05; we add an
253"
EXPERIMENTS,0.3120494335736354,"exploration noise N(µ = 0, σ = 0.2) during exploration. For all baselines, we follow DAC [52]’s
254"
EXPERIMENTS,0.31307929969104015,"open source implementations and compare our algorithm with six baselines, five of which are option
255"
EXPERIMENTS,0.3141091658084449,"variants, i.e., MOPG [35], DAC+PPO, AHP+PPO [32], IOPG [45], PPOC [27], OC [4] and PPO
256"
EXPERIMENTS,0.31513903192584963,"[41]. All baselines’ parameters used by DAC remain unchanged over 1 million environment steps
257"
EXPERIMENTS,0.3161688980432544,"to converge. Figures are plotted following DAC’s style: curves are averaged over 10 independent
258"
EXPERIMENTS,0.3171987641606591,"runs and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations.
259"
EXPERIMENTS,0.31822863027806386,"All experiments are run on an Intel® Core™i9-9900X CPU @ 3.50GHz with a single thread and
260"
EXPERIMENTS,0.3192584963954686,"process. Our implementation details are summarized in Appendix C. For a fair comparison, we follow
261"
EXPERIMENTS,0.32028836251287335,"option literature conventions and use four options in all implementations. Our code is available in
262"
EXPERIMENTS,0.3213182286302781,"supplemental materials.
263"
EXPERIMENTS,0.3223480947476828,"5
Experiments
264"
EXPERIMENTS,0.3233779608650875,"We evaluate the performance of VMOC against six option-based baselines (MOPG [35],
265"
EXPERIMENTS,0.32440782698249226,"DAC+PPO [52], AHP+PPO [32], IOPG [45], PPOC [27], and OC [4]) as well as the hierarchy-free
266"
EXPERIMENTS,0.325437693099897,"PPO algorithm [41]. Previous studies [27, 45, 20, 52] have suggested that option-based algorithms
267"
EXPERIMENTS,0.32646755921730175,"do not exhibit significant advantages over hierarchy-free algorithms in single-task environments.
268"
EXPERIMENTS,0.3274974253347065,"Nonetheless, our results demonstrate that VMOC significantly outperforms all baselines in terms
269"
EXPERIMENTS,0.32852729145211124,"of episodic return, convergence speed, step variance, and variance across 10 runs, as illustrated in
270"
EXPERIMENTS,0.329557157569516,"Figure 2. The only exception is the relatively simple InvertedDoublePendulum environment, which
271"
EXPERIMENTS,0.3305870236869207,"we suspect is due to hyper-parameter tuning issues and will be addressed in future work.
272"
EXPERIMENTS,0.33161688980432547,"Figure 2: Experiments on Mujoco Environments. Curves are averaged over 10 independent runs with different
random seeds and smoothed by a sliding window of size 20. Shaded regions indicate standard deviations.
Notably, VMOC exhibits superior performance on the Humanoid-v2 and HumanoidStandup-v2
273"
EXPERIMENTS,0.33264675592173015,"environments. These environments are characterized by a large state space (S ∈R376) and action
274"
EXPERIMENTS,0.3336766220391349,"space (A ∈R17), whereas other environments typically have state dimensions less than 20 and
275"
EXPERIMENTS,0.33470648815653964,"action dimensions less than 5. The enhanced performance of VMOC in these environments can be
276"
EXPERIMENTS,0.3357363542739444,"attributed to its maximum entropy capability: in large state-action spaces, the agent must maximize
277"
EXPERIMENTS,0.3367662203913491,"rewards while exploring a diverse set of state-action pairs. Maximum likelihood methods tend to
278"
EXPERIMENTS,0.33779608650875387,"quickly saturate with early rewarding observations, leading to the selection of low-entropy options
279"
EXPERIMENTS,0.3388259526261586,"that converge to local optima.
280"
EXPERIMENTS,0.33985581874356335,"A particularly relevant comparison is with the Markovian Option Policy Gradient (MOPG) [35],
281"
EXPERIMENTS,0.3408856848609681,"as both VMOC and MOPG are developed based on HiT-MDPs and employ option embeddings.
282"
EXPERIMENTS,0.3419155509783728,"Despite being derived under the maximum entropy framework, MOPG utilizes an on-policy gradient
283"
EXPERIMENTS,0.3429454170957775,"descent approach. Our experimental results show that VMOC’s performance surpasses that of MOPG,
284"
EXPERIMENTS,0.34397528321318227,"highlighting the limitations of on-policy methods, which suffer from shortsighted rollout lengths
285"
EXPERIMENTS,0.345005149330587,"and quickly saturate to early high-reward observations. In contrast, VMOC’s variational off-policy
286"
EXPERIMENTS,0.34603501544799176,"approach effectively utilizes the maximum entropy framework by ensuring better exploration and
287"
EXPERIMENTS,0.3470648815653965,"stability across the learning process. Additionally, the off-policy nature of VMOC allows it to reuse
288"
EXPERIMENTS,0.34809474768280124,"samples from a replay buffer, enhancing sample efficiency and promoting greater diversity in the
289"
EXPERIMENTS,0.349124613800206,"learned policies. This capability leads to more robust learning, as the algorithm can leverage a broader
290"
EXPERIMENTS,0.35015447991761073,"range of experiences to improve policy optimization.
291"
RELATED WORK,0.35118434603501547,"6
Related Work
292"
RELATED WORK,0.35221421215242016,"The VMOC incorporates three key ingredients: the option framework, a structural variational in-
293"
RELATED WORK,0.3532440782698249,"ference based off-policy algorithm and latent variable policies. We review prior works that draw
294"
RELATED WORK,0.35427394438722964,"on some of these ideas in this section. The options framework [47] offers a promising approach
295"
RELATED WORK,0.3553038105046344,"for discovering and reusing temporal abstractions, with options representing temporally abstract
296"
RELATED WORK,0.35633367662203913,"skills. Conventional option frameworks [39], typically developed under the maximum likelihood
297"
RELATED WORK,0.3573635427394439,"(MLE) framework with few constraints on options behavior, often suffer from the option degra-
298"
RELATED WORK,0.3583934088568486,"dation problem [32, 4]. This problem occurs when options quickly saturate with early rewarding
299"
RELATED WORK,0.35942327497425336,"observations, causing a single option to dominate the entire policy, or when options switch every
300"
RELATED WORK,0.3604531410916581,"timestep, maximizing policy at the expense of skill reuse across tasks. On-policy option learning
301"
RELATED WORK,0.36148300720906285,"algorithms [4, 3, 52, 34, 35] aim to maximize expected return by adjusting policy parameters to in-
302"
RELATED WORK,0.36251287332646753,"crease the likelihood of high-reward option trajectories, which often leads to focusing on low-entropy
303"
RELATED WORK,0.3635427394438723,"options. Several techniques [20, 21, 23] have been proposed to enhance on-policy algorithms with
304"
RELATED WORK,0.364572605561277,"entropy-like extrinsic rewards as regularizers, but these often result in biased optimal trajectories. In
305"
RELATED WORK,0.36560247167868176,"contrast, the maximum entropy term in VMOC arises naturally within the variational framework and
306"
RELATED WORK,0.3666323377960865,"provably converges to the optimal trajectory.
307"
RELATED WORK,0.36766220391349125,"Although several off-policy option learning algorithms have been proposed [10, 43, 45, 50], these
308"
RELATED WORK,0.368692070030896,"typically focus on improving sample efficiency by leveraging the control as inference framework.
309"
RELATED WORK,0.36972193614830073,"Recent works [45] aim to enhance sample efficiency by inferring and marginalizing over options,
310"
RELATED WORK,0.3707518022657055,"allowing all options to be learned simultaneously. Wulfmeier et al. [50] propose off-policy learning of
311"
RELATED WORK,0.3717816683831102,"all options across every experience in hindsight, further boosting sample efficiency. However, these
312"
RELATED WORK,0.3728115345005149,"approaches generally lack constraints on options behavior. A closely related work [33] also derives
313"
RELATED WORK,0.37384140061791965,"a variational approach under the option framework; however, it is based on probabilistic graphical
314"
RELATED WORK,0.3748712667353244,"model that we believe are incorrect, potentially leading to convergence issues. Additionally, our
315"
RELATED WORK,0.37590113285272914,"algorithm enables learning options as latent embeddings, a feature not present in their approach.
316"
RELATED WORK,0.3769309989701339,"Recently, several studies have extended the maximum entropy reinforcement learning framework to
317"
RELATED WORK,0.3779608650875386,"discover skills by incorporating additional latent variables. One class of methods [22, 17] maintains
318"
RELATED WORK,0.37899073120494337,"latent variables constant over the duration of an episode, providing a time-correlated exploration
319"
RELATED WORK,0.3800205973223481,"signal. Other works [19, 51] focus on discovering multi-level action abstractions that are suitable for
320"
RELATED WORK,0.38105046343975285,"repurposing by promoting skill distinguishability, but they do not incorporate temporal abstractions.
321"
RELATED WORK,0.3820803295571576,"Studies such as [38, 1, 8] aim to discover temporally abstract skills essential for exploration, but they
322"
RELATED WORK,0.3831101956745623,"predefine their temporal resolution. In contrast, VMOC learns temporal abstractions as embeddings
323"
RELATED WORK,0.384140061791967,"in an end-to-end data-driven approach with minimal prior knowledge encoded in the framework.
324"
CONCLUSION,0.38516992790937177,"7
Conclusion
325"
CONCLUSION,0.3861997940267765,"In this paper, we have introduced the Variational Markovian Option Critic (VMOC), a novel off-policy
326"
CONCLUSION,0.38722966014418125,"algorithm designed to address the challenges of ineffective exploration, sample inefficiency, and com-
327"
CONCLUSION,0.388259526261586,"putational complexity inherent in the conventional option framework for hierarchical reinforcement
328"
CONCLUSION,0.38928939237899074,"learning. By integrating a variational inference framework, VMOC leverages maximum entropy
329"
CONCLUSION,0.3903192584963955,"as intrinsic rewards to promote the discovery of diverse and effective options. Additionally, by
330"
CONCLUSION,0.3913491246138002,"employing low-cost option embeddings instead of traditional, computationally expensive option
331"
CONCLUSION,0.39237899073120497,"triples, VMOC enhances both scalability and expressiveness. Extensive experiments in challenging
332"
CONCLUSION,0.39340885684860966,"Mujoco environments demonstrate that VMOC significantly outperforms existing on-policy and
333"
CONCLUSION,0.3944387229660144,"off-policy option variants, validating its effectiveness in learning coherent and diverse option sets
334"
CONCLUSION,0.39546858908341914,"suitable for complex tasks. This work advances the field of hierarchical reinforcement learning by
335"
CONCLUSION,0.3964984552008239,"providing a robust, scalable, and efficient method for learning temporally extended actions.
336"
LIMITATIONS,0.39752832131822863,"8
Limitations
337"
LIMITATIONS,0.39855818743563337,"Due to limited computing resources, we did not conduct an ablation study of VMOC. Additionally,
338"
LIMITATIONS,0.3995880535530381,"the temperature parameter was fixed in our experiments, whereas an automatically tuned parameter
339"
LIMITATIONS,0.40061791967044286,"could potentially enhance performance (see SAC [19]). While our baselines focus on option variants,
340"
LIMITATIONS,0.4016477857878476,"a thorough comparison to other off-policy algorithms is also worth investigating. It is particularly
341"
LIMITATIONS,0.40267765190525234,"important to explore whether VMOC exhibits performance improvements in scalability when the
342"
LIMITATIONS,0.40370751802265703,"number of option embeddings is significantly increased. These investigations are left for future work.
343"
REFERENCES,0.4047373841400618,"References
344"
REFERENCES,0.4057672502574665,"[1] Ajay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum, O. Opal: Offline primitive discovery
345"
REFERENCES,0.40679711637487126,"for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.
346"
REFERENCES,0.407826982492276,"[2] Araujo, E. G. and Grupen, R. A. Learning control composition in a complex environment. In
347"
REFERENCES,0.40885684860968075,"Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, pp.
348"
REFERENCES,0.4098867147270855,"333–342, 1996.
349"
REFERENCES,0.41091658084449023,"[3] Bacon, P.-L. Temporal Representation Learning. PhD thesis, McGill University Libraries, 2018.
350"
REFERENCES,0.411946446961895,"[4] Bacon, P.-L., Harb, J., and Precup, D. The option-critic architecture. In Thirty-First AAAI
351"
REFERENCES,0.4129763130792997,"Conference on Artificial Intelligence, 2017.
352"
REFERENCES,0.4140061791967044,"[5] Bertsekas, D. and Tsitsiklis, J. N. Neuro-dynamic programming. Athena Scientific, 1996.
353"
REFERENCES,0.41503604531410915,"[6] Brockett, R. W. Hybrid models for motion control systems. In Essays on Control: Perspectives
354"
REFERENCES,0.4160659114315139,"in the Theory and its Applications, pp. 29–53. Springer, 1993.
355"
REFERENCES,0.41709577754891863,"[7] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba,
356"
REFERENCES,0.4181256436663234,"W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
357"
REFERENCES,0.4191555097837281,"[8] Co-Reyes, J., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S. Self-consistent
358"
REFERENCES,0.42018537590113286,"trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings. In
359"
REFERENCES,0.4212152420185376,"International conference on machine learning, pp. 1009–1018. PMLR, 2018.
360"
REFERENCES,0.42224510813594235,"[9] Colombetti, M., Dorigo, M., and Borghi, G. Behavior analysis and training-a methodology
361"
REFERENCES,0.4232749742533471,"for behavior engineering. IEEE Transactions on Systems, Man, and Cybernetics, Part B
362"
REFERENCES,0.4243048403707518,"(Cybernetics), 26(3):365–380, 1996.
363"
REFERENCES,0.4253347064881565,"[10] Daniel, C., Van Hoof, H., Peters, J., and Neumann, G. Probabilistic inference for determining
364"
REFERENCES,0.42636457260556127,"options in reinforcement learning. Machine Learning, 104(2-3):337–357, 2016.
365"
REFERENCES,0.427394438722966,"[11] Dayan, P. and Hinton, G. E. Feudal reinforcement learning. Advances in Neural Information
366"
REFERENCES,0.42842430484037075,"Processing Systems, pp. 271–278, 1993.
367"
REFERENCES,0.4294541709577755,"[12] Dietterich, T. G. Hierarchical reinforcement learning with the maxq value function decomposi-
368"
REFERENCES,0.43048403707518024,"tion. Journal of Artificial Intelligence Research, 13:227–303, 2000.
369"
REFERENCES,0.431513903192585,"[13] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills
370"
REFERENCES,0.4325437693099897,"without a reward function. arXiv preprint arXiv:1802.06070, 2018.
371"
REFERENCES,0.43357363542739447,"[14] Fujimoto, S., Van Hoof, H., and Meger, D. Addressing function approximation error in
372"
REFERENCES,0.43460350154479915,"actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
373"
REFERENCES,0.4356333676622039,"[15] Goyal, A., Islam, R., Strouse, D., Ahmed, Z., Botvinick, M., Larochelle, H., Bengio, Y., and
374"
REFERENCES,0.43666323377960864,"Levine, S. Infobot: Transfer and exploration via the information bottleneck. arXiv preprint
375"
REFERENCES,0.4376930998970134,"arXiv:1901.10902, 2019.
376"
REFERENCES,0.4387229660144181,"[16] Guo, Z., Thomas, P. S., and Brunskill, E. Using options and covariance testing for long
377"
REFERENCES,0.43975283213182287,"horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems,
378"
REFERENCES,0.4407826982492276,"pp. 2492–2501, 2017.
379"
REFERENCES,0.44181256436663235,"[17] Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K. Relay policy learning: Solving
380"
REFERENCES,0.4428424304840371,"long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956,
381"
REFERENCES,0.44387229660144184,"2019.
382"
REFERENCES,0.44490216271884653,"[18] Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Reinforcement learning with deep energy-
383"
REFERENCES,0.44593202883625127,"based policies. In International Conference on Machine Learning, pp. 1352–1361. PMLR,
384"
REFERENCES,0.446961894953656,"2017.
385"
REFERENCES,0.44799176107106076,"[19] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum
386"
REFERENCES,0.4490216271884655,"entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
387"
REFERENCES,0.45005149330587024,"2018.
388"
REFERENCES,0.451081359423275,"[20] Harb, J., Bacon, P.-L., Klissarov, M., and Precup, D. When waiting is not an option: Learning
389"
REFERENCES,0.45211122554067973,"options with a deliberation cost. In Thirty-Second AAAI Conference on Artificial Intelligence,
390"
REFERENCES,0.45314109165808447,"2018.
391"
REFERENCES,0.45417095777548916,"[21] Harutyunyan, A., Dabney, W., Borsa, D., Heess, N., Munos, R., and Precup, D. The termination
392"
REFERENCES,0.4552008238928939,"critic. arXiv preprint arXiv:1902.09996, 2019.
393"
REFERENCES,0.45623069001029865,"[22] Hausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. Learning an
394"
REFERENCES,0.4572605561277034,"embedding space for transferable robot skills. In International Conference on Learning Repre-
395"
REFERENCES,0.45829042224510813,"sentations, 2018.
396"
REFERENCES,0.4593202883625129,"[23] Kamat, A. and Precup, D. Diversity-enriched option-critic. arXiv, 2020.
397"
REFERENCES,0.4603501544799176,"[24] Khetarpal, K. and Precup, D. Learning options with interest functions. In Proceedings of the
398"
REFERENCES,0.46138002059732236,"32nd AAAI Conference on Artificial Intelligence, pp. 1–2, 2019.
399"
REFERENCES,0.4624098867147271,"[25] Khetarpal, K., Klissarov, M., Chevalier-Boisvert, M., Bacon, P.-L., and Precup, D. Options of
400"
REFERENCES,0.46343975283213185,"interest: Temporal abstraction with interest functions. In Proceedings of the AAAI Conference
401"
REFERENCES,0.46446961894953653,"on Artificial Intelligence, volume 34, pp. 4,444–4,451, 2020.
402"
REFERENCES,0.4654994850669413,"[26] Klissarov, M. and Precup, D. Flexible option learning. In Ranzato, M., Beygelzimer, A.,
403"
REFERENCES,0.466529351184346,"Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing
404"
REFERENCES,0.46755921730175076,"Systems, volume 34, pp. 4632–4646. Curran Associates, 2021.
405"
REFERENCES,0.4685890834191555,"[27] Klissarov, M., Bacon, P.-L., Harb, J., and Precup, D. Learnings options end-to-end for continu-
406"
REFERENCES,0.46961894953656025,"ous action tasks. arXiv preprint arXiv:1712.00004, 2017.
407"
REFERENCES,0.470648815653965,"[28] Koller, D. and Friedman, N. Probabilistic graphical models: principles and techniques. MIT
408"
REFERENCES,0.47167868177136973,"press, 2009.
409"
REFERENCES,0.4727085478887745,"[29] Kolobov, A., Weld, D. S., et al. Discovering hidden structure in factored mdps. Artificial
410"
REFERENCES,0.4737384140061792,"Intelligence, 189:19–47, 2012.
411"
REFERENCES,0.4747682801235839,"[30] Konidaris, G. and Barto, A. G. Skill discovery in continuous reinforcement learning domains
412"
REFERENCES,0.47579814624098865,"using skill chaining. In Advances in neural information processing systems, pp. 1015–1023,
413"
REFERENCES,0.4768280123583934,"2009.
414"
REFERENCES,0.47785787847579814,"[31] Levine, S. Reinforcement learning and control as probabilistic inference: Tutorial and review.
415"
REFERENCES,0.4788877445932029,"arXiv preprint arXiv:1805.00909, 2018.
416"
REFERENCES,0.4799176107106076,"[32] Levy, K. Y. and Shimkin, N. Unified inter and intra options learning using policy gradient
417"
REFERENCES,0.48094747682801237,"methods. In European Workshop on Reinforcement Learning, pp. 153–164. Springer, 2011.
418"
REFERENCES,0.4819773429454171,"[33] Li, C., Ma, X., Zhang, C., Yang, J., Xia, L., and Zhao, Q. Soac: The soft option actor-critic
419"
REFERENCES,0.48300720906282185,"architecture. arXiv preprint arXiv:2006.14363, 2020.
420"
REFERENCES,0.4840370751802266,"[34] Li, C., Song, D., and Tao, D. The skill-action architecture: Learning abstract action embeddings
421"
REFERENCES,0.4850669412976313,"for reinforcement learning. 2020.
422"
REFERENCES,0.486096807415036,"[35] Li, C., Song, D., and Tao, D. Hit-mdp: learning the smdp option framework on mdps with hidden
423"
REFERENCES,0.48712667353244077,"temporal embeddings. In The Eleventh International Conference on Learning Representations,
424"
REFERENCES,0.4881565396498455,"2022.
425"
REFERENCES,0.48918640576725025,"[36] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,
426"
REFERENCES,0.490216271884655,"A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep
427"
REFERENCES,0.49124613800205974,"reinforcement learning. Nature, 518(7540):529–533, 2015.
428"
REFERENCES,0.4922760041194645,"[37] Osa, T., Tangkaratt, V., and Sugiyama, M. Hierarchical reinforcement learning via advantage-
429"
REFERENCES,0.4933058702368692,"weighted information maximization. arXiv preprint arXiv:1901.01365, 2019.
430"
REFERENCES,0.49433573635427397,"[38] Pertsch, K., Rybkin, O., Ebert, F., Finn, C., Jayaraman, D., and Levine, S. Long-horizon visual
431"
REFERENCES,0.49536560247167866,"planning with goal-conditioned hierarchical predictors. NeurIPS, 2020.
432"
REFERENCES,0.4963954685890834,"[39] Precup, D. Temporal abstraction in reinforcement learning. University of Massachusetts
433"
REFERENCES,0.49742533470648814,"Amherst, 2000.
434"
REFERENCES,0.4984552008238929,"[40] Schulman, J., Chen, X., and Abbeel, P. Equivalence between policy gradients and soft q-learning.
435"
REFERENCES,0.49948506694129763,"arXiv preprint arXiv:1704.06440, 2017.
436"
REFERENCES,0.5005149330587023,"[41] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimiza-
437"
REFERENCES,0.5015447991761071,"tion algorithms. arXiv preprint arXiv:1707.06347, 2017.
438"
REFERENCES,0.5025746652935118,"[42] Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. Dynamics-aware unsupervised
439"
REFERENCES,0.5036045314109165,"discovery of skills. arXiv preprint arXiv:1907.01657, 2019.
440"
REFERENCES,0.5046343975283213,"[43] Shiarlis, K., Wulfmeier, M., Salter, S., Whiteson, S., and Posner, I. Taco: Learning task
441"
REFERENCES,0.505664263645726,"decomposition via temporal alignment for control. In International Conference on Machine
442"
REFERENCES,0.5066941297631308,"Learning, pp. 4654–4663. PMLR, 2018.
443"
REFERENCES,0.5077239958805355,"[44] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser,
444"
REFERENCES,0.5087538619979403,"J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep
445"
REFERENCES,0.509783728115345,"neural networks and tree search. Nature, 529(7587):484–489, 2016.
446"
REFERENCES,0.5108135942327497,"[45] Smith, M., Hoof, H., and Pineau, J. An inference-based policy gradient method for learning
447"
REFERENCES,0.5118434603501545,"options. In International Conference on Machine Learning, pp. 4,703–4,712, 2018.
448"
REFERENCES,0.5128733264675592,"[46] Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018.
449"
REFERENCES,0.513903192584964,"[47] Sutton, R. S., Precup, D., and Singh, S. Between mdps and semi-mdps: A framework for
450"
REFERENCES,0.5149330587023687,"temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181–211, 1999.
451"
REFERENCES,0.5159629248197735,"[48] Todorov, E. Linearly-solvable markov decision problems. Advances in neural information
452"
REFERENCES,0.5169927909371782,"processing systems, 19, 2006.
453"
REFERENCES,0.518022657054583,"[49] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012
454"
REFERENCES,0.5190525231719877,"IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE,
455"
REFERENCES,0.5200823892893924,"2012.
456"
REFERENCES,0.5211122554067971,"[50] Wulfmeier, M., Rao, D., Hafner, R., Lampe, T., Abdolmaleki, A., Hertweck, T., Neunert, M.,
457"
REFERENCES,0.5221421215242018,"Tirumala, D., Siegel, N., Heess, N., et al. Data-efficient hindsight off-policy option learning.
458"
REFERENCES,0.5231719876416066,"arXiv preprint arXiv:2007.15588, 2020.
459"
REFERENCES,0.5242018537590113,"[51] Zhang, D., Courville, A., Bengio, Y., Zheng, Q., Zhang, A., and Chen, R. T.
Latent
460"
REFERENCES,0.525231719876416,"state marginalization as a low-cost approach for improving exploration.
arXiv preprint
461"
REFERENCES,0.5262615859938208,"arXiv:2210.00999, 2022.
462"
REFERENCES,0.5272914521112255,"[52] Zhang, S. and Whiteson, S. DAC: The double actor-critic architecture for learning options. In
463"
REFERENCES,0.5283213182286303,"Advances in Neural Information Processing Systems, pp. 2,012–2,022, 2019.
464"
REFERENCES,0.529351184346035,"[53] Ziebart, B. D., Bagnell, J. A., and Dey, A. K. Modeling interaction via the principle of maximum
465"
REFERENCES,0.5303810504634398,"causal entropy. In ICML, 2010.
466"
REFERENCES,0.5314109165808445,"A
Proofs
467"
REFERENCES,0.5324407826982492,"A.1
Theorem 1
468"
REFERENCES,0.533470648815654,"Theorem 1 (Convergence Theorem for Structured Variational Policy Iteration).
Let τ be the
469"
REFERENCES,0.5345005149330587,"latent variable and E be the observed variable. Define the variational distribution q(τ) and the
470"
REFERENCES,0.5355303810504635,"log-likelihood log P(E). Let M : q[k] →q[k+1] represent the mapping defined by the EM steps
471"
REFERENCES,0.5365602471678682,"inference update, so that q[k+1] = M(q[k]). The likelihood function increases at each iteration of the
472"
REFERENCES,0.537590113285273,"variational inference algorithm until the conditions for equality are satisfied and a fixed point of the
473"
REFERENCES,0.5386199794026777,"iteration is reached:
474"
REFERENCES,0.5396498455200824,"log P(E | q[k+1]) ≥log P(E | q[k]), with equality if and only if 475"
REFERENCES,0.5406797116374872,"L(q[k+1], P) = L(q[k], P)
and
476"
REFERENCES,0.5417095777548918,DKL(q[k+1](τ) ∥P(τ | E)) = DKL(q[k](τ) ∥P(τ | E)).
REFERENCES,0.5427394438722966,"Proof. Let τ be the latent variable and E be the observed variable. Define the evidence lower bound
477"
REFERENCES,0.5437693099897013,"(ELBO) as L(q, P) and the Kullback-Leibler divergence as DKL(q ∥P), where q(τ) approximates
478"
REFERENCES,0.544799176107106,"the posterior distribution and P(E | τ) is the likelihood.
479"
REFERENCES,0.5458290422245108,"The log-likelihood function log P(E) can be decomposed as:
480"
REFERENCES,0.5468589083419155,"log P(E) = L(q, P) + DKL(q(τ) ∥P(τ | E)),"
REFERENCES,0.5478887744593203,"where
481"
REFERENCES,0.548918640576725,"L(q, P) = Eq(τ) [log P(E, τ) −log q(τ)]"
REFERENCES,0.5499485066941298,"and
482"
REFERENCES,0.5509783728115345,DKL(q(τ) ∥P(τ | E)) = Eq(τ)
REFERENCES,0.5520082389289392,"
log
q(τ)
P(τ | E) 
."
REFERENCES,0.553038105046344,"Let M : q[k] →q[k+1] represent the mapping defined by the variational inference update, so that
483"
REFERENCES,0.5540679711637487,"q[k+1] = M(q[k]). If q∗is a variational distribution that maximizes the ELBO, so that log P(E |
484"
REFERENCES,0.5550978372811535,"q∗) ≥log P(E | q) for all q, then log P(E | M(q∗)) = log P(E | q∗). In other words, the
485"
REFERENCES,0.5561277033985582,"maximizing distributions are fixed points of the variational inference algorithm. Since the likelihood
486"
REFERENCES,0.557157569515963,"function is bounded (for distributions of practical interest), the sequence of variational distributions
487"
REFERENCES,0.5581874356333677,"q[0], q[1], . . . , q[k] yields a bounded nondecreasing sequence log P(E | q[0]) ≤log P(E | q[1]) ≤
488"
REFERENCES,0.5592173017507724,"· · · ≤log P(E | q[k]) ≤log P(E | q[k]) which must converge as k →∞.
489 490"
REFERENCES,0.5602471678681772,"A.2
Theorem 2
491"
REFERENCES,0.5612770339855818,"Theorem 2 (Soft Option Policy Iteration Theorem). Repeated optimizing L and DKL defined in
492"
REFERENCES,0.5623069001029866,"Eq. 10 from any πA
0 , πO
0 ∈Π converges to optimal policies πA∗, πO∗such that Qsoft∗
O
[st, ot] ≥
493"
REFERENCES,0.5633367662203913,"Qsoft
O
[st, ot] and Qsoft∗
A
[st, ot, at] ≥Qsoft
A
[st, ot, at], for all πA
0 , πO
0
∈Π and (st, at, ot) ∈
494"
REFERENCES,0.564366632337796,"S × A × O, assuming |S| < ∞, |O| < ∞, |A| < ∞.
495"
REFERENCES,0.5653964984552008,"Proof. Define the entropy augmented reward as rsoft(st, at)
=
r(st, at) + H[πA] and
496"
REFERENCES,0.5664263645726055,"f soft(ot, st, at, ot−1) = f(ot, st, at, ot−1) + H[πO] and rewrite Bellman Backup functions as,
497"
REFERENCES,0.5674562306900103,"QO[st, ot] = f soft(·) + Eat∼πA [QA[st, ot, at]] ,"
REFERENCES,0.568486096807415,"QA[st, ot, at] = rsoft(s, a) + Est+1∼P (st+1|st,at)

Eot+1∼πO [QO[st+1, ot+1]]
"
REFERENCES,0.5695159629248198,"We start with proving the convergence of soft option policy evaluation. As with the standard Q-
498"
REFERENCES,0.5705458290422245,"function and value function, we can relate the Q-function at a future state via a Bellman Operator
499"
REFERENCES,0.5715756951596292,"T soft. The option-action value function satisfies the Bellman Operator T soft
500"
REFERENCES,0.572605561277034,"T softQA[st, ot, at] = E[Gt|st, ot, at]"
REFERENCES,0.5736354273944387,"= rsoft(s, a) + γ
X"
REFERENCES,0.5746652935118435,"st+1
P(st+1|st, at)QO[st+1, ot],"
REFERENCES,0.5756951596292482,"As with the standard convergence results for policy evaluation [46], by the definition of T soft (Eq. 11)
501"
REFERENCES,0.576725025746653,"the option-action value function QπA
A is a fixed point.
502"
REFERENCES,0.5777548918640577,"To prove the T soft is a contraction, define a norm on V -values functions V and U
503"
REFERENCES,0.5787847579814624,"∥V −U∥∞≜max
¯s∈¯S |V (¯s) −U(¯s)|.
(11)"
REFERENCES,0.5798146240988672,"where ¯s = {s, o}.
504"
REFERENCES,0.5808444902162719,"By recurssively apply the Hidden Temporal Bellman Operator T soft, we have:
505"
REFERENCES,0.5818743563336766,"QO[st, ot−1] = E[Gt|st, ot−1] =
X"
REFERENCES,0.5829042224510813,"ot
P(ot|st, ot−1)QO[st, ot] =
X"
REFERENCES,0.583934088568486,"ot
P(ot|st, ot−1)
X"
REFERENCES,0.5849639546858908,"at
P(at|st, ot)

r(s, a) + γ
X"
REFERENCES,0.5859938208032955,"st+1
P(st+1|st, at)QO[st+1, ot]
"
REFERENCES,0.5870236869207003,"= r(s, a) + γ
X"
REFERENCES,0.588053553038105,"ot
P(ot|st, ot−1)
X"
REFERENCES,0.5890834191555098,"at
P(at|st, ot)
X"
REFERENCES,0.5901132852729145,"st+1
P(st+1|st, at)QO[st+1, ot]"
REFERENCES,0.5911431513903193,"= r(s, a) + γ
X"
REFERENCES,0.592173017507724,"ot,st+1
P(st+1, ot|st, ot−1)QO[st+1, ot]"
REFERENCES,0.5932028836251287,"= r(s, a) + γEst+1,ot"
REFERENCES,0.5942327497425335,"
QO[st+1, ot]

(12)"
REFERENCES,0.5952626158599382,"Therefore, by applying Eq. 12 to V and U we have:
506"
REFERENCES,0.596292481977343,∥T πV −T πU∥∞
REFERENCES,0.5973223480947477,"= max
¯s∈¯S"
REFERENCES,0.5983522142121525,"γEst+1,ot"
REFERENCES,0.5993820803295572,"
QO[st+1, ot]

−γEst+1,ot"
REFERENCES,0.6004119464469619,"
U[st+1, ot]
"
REFERENCES,0.6014418125643667,"= γ max
¯s∈¯S Est+1,ot"
REFERENCES,0.6024716786817713,"QO[st+1, ot] −U[st+1, ot] "
REFERENCES,0.6035015447991761,"≤γ max
¯s∈¯S Est+1,ot"
REFERENCES,0.6045314109165808,"
γ max
¯s∈¯S"
REFERENCES,0.6055612770339855,"QO[st+1, ot] −U[st+1, ot] "
REFERENCES,0.6065911431513903,"≤γ max
¯s∈¯S |V [¯s] −U[¯s]|"
REFERENCES,0.607621009268795,"= γ∥V −U∥∞
(13)"
REFERENCES,0.6086508753861998,"Therefore, T soft is a contraction. By the fixed point theorem, assuming that throughout our computa-
507"
REFERENCES,0.6096807415036045,"tion the QA[·, ·] and QO[·] are bounded and A < ∞, the sequence Qk
A defined by Qk+1
A
= T softQk
A
508"
REFERENCES,0.6107106076210093,"will converge to the option-action value function QπA
A as k →∞.
509"
REFERENCES,0.611740473738414,"The convergence results of and the Soft Option Policy Improvement Theorem then follows conven-
510"
REFERENCES,0.6127703398558187,"tional Soft Policy Improvement Theorem Theorem 1. Consequently, the Soft Option Policy Iteration
511"
REFERENCES,0.6138002059732235,"Theorem follows directly from these results.
512 513"
REFERENCES,0.6148300720906282,"A.3
Derivation of Eq. 10
514"
REFERENCES,0.615859938208033,"L(q(τ), P(τ, EA
1:T , EO
1:T )) = Eq(τ)[log P(τ, EA
1:T , EO
1:T ) −log q(τ)]"
REFERENCES,0.6168898043254377,"= Eq(τ)[log P(τ|EA
1:T , EO
1:T ) + log P(EA
1:T , EO
1:T ) −log q(τ)]"
REFERENCES,0.6179196704428425,"= Eq(τ)[log P(τ|EA
1:T , EO
1:T ) −log q(τ)] + Eq(τ) log P(EA
1:T , EO
1:T )"
REFERENCES,0.6189495365602472,"= Eq(τ)[log P(τ|EA
1:T , EO
1:T )
log q(τ)
] + log P(EA
1:T , EO
1:T )"
REFERENCES,0.619979402677652,"= −DKL(log q(τ) ∥log P(τ|EA
1:T , EO
1:T )) + log P(EA
1:T , EO
1:T )"
REFERENCES,0.6210092687950567,"A.4
Theorem 3
515"
REFERENCES,0.6220391349124614,"Theorem 3 (Convergence Theorem for Variational Markovian Option Policy Iteration). Let τ be
516"
REFERENCES,0.6230690010298661,"the latent variable and EA, EO be the ground-truth optimality variables. Define the variational
517"
REFERENCES,0.6240988671472708,"distribution q(τ) and the true log-likelihood of optimality log P(EA, EO). iterates according to the
518"
REFERENCES,0.6251287332646756,"update rule qk+1 = arg maxq L(q(τ), P(τ, EA
1:T , EO
1:T )) converges to the maximum value bounded
519"
REFERENCES,0.6261585993820803,"by the data log-likelihood.
520"
REFERENCES,0.627188465499485,"Proof. The objective is to maximize the ELBO with respect to the policy q. Formally, this can be
521"
REFERENCES,0.6282183316168898,"written as:
522"
REFERENCES,0.6292481977342945,"qk+1 = arg max
q
L(q, P)."
REFERENCES,0.6302780638516993,"Suppose we q is a neural network function approximator, assuming the continuity and differentiability
523"
REFERENCES,0.631307929969104,"of q with respect to its parameters. Using stochastic gradient descent (SGD) to optimize the parameters
524"
REFERENCES,0.6323377960865088,"guarantees that the ELBO increases, such that L(qk+1, P) ≥L(qk, P).
525"
REFERENCES,0.6333676622039135,"Rearranging Eq. 10 we get:
526"
REFERENCES,0.6343975283213182,"DKL(qk+1(τ)||P(τ|EA
1:T , EO
1:T )) = −L(qk+1(τ), P(τ, EA
1:T , EO
1:T )) + log P(EA
1:T , EO
1:T )"
REFERENCES,0.635427394438723,"≤−L(qk(τ), P(τ, EA
1:T , EO
1:T )) + log P(EA
1:T , EO
1:T )"
REFERENCES,0.6364572605561277,"= DKL(qk(τ)||P(τ|EA
1:T , EO
1:T ))"
REFERENCES,0.6374871266735325,"Thus, each SGD update not only potentially increases the ELBO but also decreases the KL divergence,
527"
REFERENCES,0.6385169927909372,"moving q closer to P. Given the properties of SGD and assuming appropriate learning rates and
528"
REFERENCES,0.639546858908342,"sufficiently expressive neural network architectures, the sequence {qk} converges to a policy q∗that
529"
REFERENCES,0.6405767250257467,"minimizes the KL divergence to the true posterior.
530"
REFERENCES,0.6416065911431514,"B
VMOC Algorithm
531"
REFERENCES,0.6426364572605562,Algorithm 1 VMOC Algorithm
REFERENCES,0.6436663233779608,"1: Initialize parameter vectors ψA, ψO, θO, θA"
REFERENCES,0.6446961894953656,"2: for each epoch do
3:
Collect trajectories {ot−1, st, at, ot} into the replay buffer
4:
for each gradient step do
5:
Update Qsoft
ψA
i : ψA
i ←ψA
i −ηQA∇JQsoft
ψA
i
for i ∈{1, 2}"
REFERENCES,0.6457260556127703,"6:
Update Qsoft
ψO
i : ψO
i ←ψO
i −ηQO∇JQsoft
ψO
i
for i ∈{1, 2}"
REFERENCES,0.646755921730175,"7:
Update πO
θO: θO ←θO −ηπO∇JπO
8:
Update πA
θA: θA ←θA −ηπA∇JπA
9:
Update target networks: ¯ψA ←σψA + (1 −σ) ¯ψA, ¯ψO ←σψO + (1 −σ) ¯ψO"
REFERENCES,0.6477857878475798,"10:
Update temperature factors: αO ←αO −ηαO∇JαO, αA ←αA −ηαA∇JαA
11:
end for
12: end for"
REFERENCES,0.6488156539649845,"C
Implementation Details
532"
REFERENCES,0.6498455200823893,"C.1
Hyperparameters
533"
REFERENCES,0.650875386199794,"In this section we summarize our implementation details. For a fair comparison, all baselines:
534"
REFERENCES,0.6519052523171988,"MOPG [35], DAC+PPO [52], AHP+PPO [32], PPOC [27], OC [4] and PPO [41] are from DAC’s
535"
REFERENCES,0.6529351184346035,"open source Github repo: https://github.com/ShangtongZhang/DeepRL/tree/DAC. Hyper-
536"
REFERENCES,0.6539649845520082,"parameters used in DAC [52] for all these baselines are kept unchanged.
537"
REFERENCES,0.654994850669413,"VMOC Network Architecture: We use Pytorch to build neural networks. Specifically, for option
538"
REFERENCES,0.6560247167868177,"embeddings, we use an embedding matrix WS ∈R4×40 which has 4 options (4 rows) and an
539"
REFERENCES,0.6570545829042225,"embedding size of 40 (40 columns). For layer normalization we use Pytorch’s built-in function
540"
REFERENCES,0.6580844490216272,"LayerNorm 2. For Feed Forward Networks (FNN), we use a 2 layer FNN with ReLu function as
541"
REFERENCES,0.659114315139032,"activation function with input size of state-size, hidden size of [256, 256], and output size of action-
542"
REFERENCES,0.6601441812564367,"dim neurons. For Linear layer, we use built-in Linear function3 to map FFN’s outputs to 4 dimension.
543"
REFERENCES,0.6611740473738414,"2https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html
3https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
REFERENCES,0.6622039134912462,"Each dimension acts like a logit for each skill and is used as density in Categorical distribution4. For
544"
REFERENCES,0.6632337796086509,"both action policy and critic module, FFNs are of the same size as the one used in the skill policy.
545"
REFERENCES,0.6642636457260556,"Preprocessing: States are normalized by a running estimation of mean and std.
546"
REFERENCES,0.6652935118434603,"Hyperparameters for all on-policy option variants: For a fair comparison, we use exactly the same
547"
REFERENCES,0.666323377960865,"parameters of PPO as DAC . Specifically:
548"
REFERENCES,0.6673532440782698,"• Optimizer: Adam with ϵ = 10−5 and an initial learning rate 3 × 10−4
549"
REFERENCES,0.6683831101956745,"• Discount ratio γ: 0.99
550"
REFERENCES,0.6694129763130793,"• GAE coefficient: 0.95
551"
REFERENCES,0.670442842430484,"• Gradient clip by norm: 0.5
552"
REFERENCES,0.6714727085478888,"• Rollout length: 2048 environment steps
553"
REFERENCES,0.6725025746652935,"• Optimization epochs: 10
554"
REFERENCES,0.6735324407826982,"• Optimization batch size: 64
555"
REFERENCES,0.674562306900103,"• Action probability ratio clip: 0.2
556"
REFERENCES,0.6755921730175077,"Computing Infrastructure: We conducted our experiments on an Intel® Core™i9-9900X CPU @
557"
REFERENCES,0.6766220391349125,"3.50GHz with a single thread and process with PyTorch.
558"
REFERENCES,0.6776519052523172,4https://github.com/pytorch/pytorch/blob/master/torch/distributions/categorical.py
REFERENCES,0.678681771369722,"NeurIPS Paper Checklist
559"
CLAIMS,0.6797116374871267,"1. Claims
560"
CLAIMS,0.6807415036045315,"Question: Do the main claims made in the abstract and introduction accurately reflect the
561"
CLAIMS,0.6817713697219362,"paper’s contributions and scope?
562"
CLAIMS,0.6828012358393409,"Answer: [Yes]
563"
CLAIMS,0.6838311019567456,"Justification: The abstract and introduction accurately reflect the claims and findings of the
564"
CLAIMS,0.6848609680741503,"paper.
565"
CLAIMS,0.685890834191555,"Guidelines:
566"
CLAIMS,0.6869207003089598,"• The answer NA means that the abstract and introduction do not include the claims
567"
CLAIMS,0.6879505664263645,"made in the paper.
568"
CLAIMS,0.6889804325437693,"• The abstract and/or introduction should clearly state the claims made, including the
569"
CLAIMS,0.690010298661174,"contributions made in the paper and important assumptions and limitations. A No or
570"
CLAIMS,0.6910401647785788,"NA answer to this question will not be perceived well by the reviewers.
571"
CLAIMS,0.6920700308959835,"• The claims made should match theoretical and experimental results, and reflect how
572"
CLAIMS,0.6930998970133883,"much the results can be expected to generalize to other settings.
573"
CLAIMS,0.694129763130793,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
574"
CLAIMS,0.6951596292481977,"are not attained by the paper.
575"
LIMITATIONS,0.6961894953656025,"2. Limitations
576"
LIMITATIONS,0.6972193614830072,"Question: Does the paper discuss the limitations of the work performed by the authors?
577"
LIMITATIONS,0.698249227600412,"Answer: [Yes]
578"
LIMITATIONS,0.6992790937178167,"Justification: Limitations of the study are discussed in the discussion section.
579"
LIMITATIONS,0.7003089598352215,"Guidelines:
580"
LIMITATIONS,0.7013388259526262,"• The answer NA means that the paper has no limitation while the answer No means that
581"
LIMITATIONS,0.7023686920700309,"the paper has limitations, but those are not discussed in the paper.
582"
LIMITATIONS,0.7033985581874357,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
583"
LIMITATIONS,0.7044284243048403,"• The paper should point out any strong assumptions and how robust the results are to
584"
LIMITATIONS,0.7054582904222451,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
585"
LIMITATIONS,0.7064881565396498,"model well-specification, asymptotic approximations only holding locally). The authors
586"
LIMITATIONS,0.7075180226570545,"should reflect on how these assumptions might be violated in practice and what the
587"
LIMITATIONS,0.7085478887744593,"implications would be.
588"
LIMITATIONS,0.709577754891864,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
589"
LIMITATIONS,0.7106076210092688,"only tested on a few datasets or with a few runs. In general, empirical results often
590"
LIMITATIONS,0.7116374871266735,"depend on implicit assumptions, which should be articulated.
591"
LIMITATIONS,0.7126673532440783,"• The authors should reflect on the factors that influence the performance of the approach.
592"
LIMITATIONS,0.713697219361483,"For example, a facial recognition algorithm may perform poorly when image resolution
593"
LIMITATIONS,0.7147270854788877,"is low or images are taken in low lighting. Or a speech-to-text system might not be
594"
LIMITATIONS,0.7157569515962925,"used reliably to provide closed captions for online lectures because it fails to handle
595"
LIMITATIONS,0.7167868177136972,"technical jargon.
596"
LIMITATIONS,0.717816683831102,"• The authors should discuss the computational efficiency of the proposed algorithms
597"
LIMITATIONS,0.7188465499485067,"and how they scale with dataset size.
598"
LIMITATIONS,0.7198764160659115,"• If applicable, the authors should discuss possible limitations of their approach to
599"
LIMITATIONS,0.7209062821833162,"address problems of privacy and fairness.
600"
LIMITATIONS,0.721936148300721,"• While the authors might fear that complete honesty about limitations might be used by
601"
LIMITATIONS,0.7229660144181257,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
602"
LIMITATIONS,0.7239958805355304,"limitations that aren’t acknowledged in the paper. The authors should use their best
603"
LIMITATIONS,0.7250257466529351,"judgment and recognize that individual actions in favor of transparency play an impor-
604"
LIMITATIONS,0.7260556127703398,"tant role in developing norms that preserve the integrity of the community. Reviewers
605"
LIMITATIONS,0.7270854788877446,"will be specifically instructed to not penalize honesty concerning limitations.
606"
THEORY ASSUMPTIONS AND PROOFS,0.7281153450051493,"3. Theory Assumptions and Proofs
607"
THEORY ASSUMPTIONS AND PROOFS,0.729145211122554,"Question: For each theoretical result, does the paper provide the full set of assumptions and
608"
THEORY ASSUMPTIONS AND PROOFS,0.7301750772399588,"a complete (and correct) proof?
609"
THEORY ASSUMPTIONS AND PROOFS,0.7312049433573635,"Answer: [Yes]
610"
THEORY ASSUMPTIONS AND PROOFS,0.7322348094747683,"Justification: The paper provides a full derivation of assumptions and proofs of the theoretical
611"
THEORY ASSUMPTIONS AND PROOFS,0.733264675592173,"result (convergence of the evidence lower bound)
612"
THEORY ASSUMPTIONS AND PROOFS,0.7342945417095778,"Guidelines:
613"
THEORY ASSUMPTIONS AND PROOFS,0.7353244078269825,"• The answer NA means that the paper does not include theoretical results.
614"
THEORY ASSUMPTIONS AND PROOFS,0.7363542739443872,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
615"
THEORY ASSUMPTIONS AND PROOFS,0.737384140061792,"referenced.
616"
THEORY ASSUMPTIONS AND PROOFS,0.7384140061791967,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
617"
THEORY ASSUMPTIONS AND PROOFS,0.7394438722966015,"• The proofs can either appear in the main paper or the supplemental material, but if
618"
THEORY ASSUMPTIONS AND PROOFS,0.7404737384140062,"they appear in the supplemental material, the authors are encouraged to provide a short
619"
THEORY ASSUMPTIONS AND PROOFS,0.741503604531411,"proof sketch to provide intuition.
620"
THEORY ASSUMPTIONS AND PROOFS,0.7425334706488157,"• Inversely, any informal proof provided in the core of the paper should be complemented
621"
THEORY ASSUMPTIONS AND PROOFS,0.7435633367662204,"by formal proofs provided in appendix or supplemental material.
622"
THEORY ASSUMPTIONS AND PROOFS,0.7445932028836252,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7456230690010298,"4. Experimental Result Reproducibility
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7466529351184346,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7476828012358393,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.748712667353244,"of the paper (regardless of whether the code and data are provided or not)?
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497425334706488,"Answer: [Yes]
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7507723995880535,"Justification: Yes. Our code is provided in supplementary materials. Full details of the
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7518022657054583,"experimental setup, model architectures are provided.
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.752832131822863,"Guidelines:
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7538619979402678,"• The answer NA means that the paper does not include experiments.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7548918640576725,"• If the paper includes experiments, a No answer to this question will not be perceived
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7559217301750772,"well by the reviewers: Making the paper reproducible is important, regardless of
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756951596292482,"whether the code and data are provided or not.
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7579814624098867,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7590113285272915,"to make their results reproducible or verifiable.
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7600411946446962,"• Depending on the contribution, reproducibility can be accomplished in various ways.
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.761071060762101,"For example, if the contribution is a novel architecture, describing the architecture fully
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7621009268795057,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7631307929969104,"be necessary to either make it possible for others to replicate the model with the same
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7641606591143152,"dataset, or provide access to the model. In general. releasing code and data is often
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7651905252317199,"one good way to accomplish this, but reproducibility can also be provided via detailed
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7662203913491246,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7672502574665293,"of a large language model), releasing of a model checkpoint, or other means that are
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.768280123583934,"appropriate to the research performed.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693099897013388,"• While NeurIPS does not require releasing code, the conference does require all submis-
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7703398558187435,"sions to provide some reasonable avenue for reproducibility, which may depend on the
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7713697219361483,"nature of the contribution. For example
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.772399588053553,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7734294541709578,"to reproduce that algorithm.
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7744593202883625,"(b) If the contribution is primarily a new model architecture, the paper should describe
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7754891864057673,"the architecture clearly and fully.
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.776519052523172,"(c) If the contribution is a new model (e.g., a large language model), then there should
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7775489186405767,"either be a way to access this model for reproducing the results or a way to reproduce
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7785787847579815,"the model (e.g., with an open-source dataset or instructions for how to construct
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7796086508753862,"the dataset).
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.780638516992791,"(d) We recognize that reproducibility may be tricky in some cases, in which case
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7816683831101957,"authors are welcome to describe the particular way they provide for reproducibility.
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7826982492276005,"In the case of closed-source models, it may be that access to the model is limited in
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7837281153450052,"some way (e.g., to registered users), but it should be possible for other researchers
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7847579814624099,"to have some path to reproducing or verifying the results.
662"
OPEN ACCESS TO DATA AND CODE,0.7857878475798146,"5. Open access to data and code
663"
OPEN ACCESS TO DATA AND CODE,0.7868177136972193,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
664"
OPEN ACCESS TO DATA AND CODE,0.787847579814624,"tions to faithfully reproduce the main experimental results, as described in supplemental
665"
OPEN ACCESS TO DATA AND CODE,0.7888774459320288,"material?
666"
OPEN ACCESS TO DATA AND CODE,0.7899073120494335,"Answer: [Yes]
667"
OPEN ACCESS TO DATA AND CODE,0.7909371781668383,"Justification: The paper provides open access to the code and data.
668"
OPEN ACCESS TO DATA AND CODE,0.791967044284243,"Guidelines:
669"
OPEN ACCESS TO DATA AND CODE,0.7929969104016478,"• The answer NA means that paper does not include experiments requiring code.
670"
OPEN ACCESS TO DATA AND CODE,0.7940267765190525,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
671"
OPEN ACCESS TO DATA AND CODE,0.7950566426364573,"public/guides/CodeSubmissionPolicy) for more details.
672"
OPEN ACCESS TO DATA AND CODE,0.796086508753862,"• While we encourage the release of code and data, we understand that this might not be
673"
OPEN ACCESS TO DATA AND CODE,0.7971163748712667,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
674"
OPEN ACCESS TO DATA AND CODE,0.7981462409886715,"including code, unless this is central to the contribution (e.g., for a new open-source
675"
OPEN ACCESS TO DATA AND CODE,0.7991761071060762,"benchmark).
676"
OPEN ACCESS TO DATA AND CODE,0.800205973223481,"• The instructions should contain the exact command and environment needed to run to
677"
OPEN ACCESS TO DATA AND CODE,0.8012358393408857,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
678"
OPEN ACCESS TO DATA AND CODE,0.8022657054582905,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
679"
OPEN ACCESS TO DATA AND CODE,0.8032955715756952,"• The authors should provide instructions on data access and preparation, including how
680"
OPEN ACCESS TO DATA AND CODE,0.8043254376930999,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
681"
OPEN ACCESS TO DATA AND CODE,0.8053553038105047,"• The authors should provide scripts to reproduce all experimental results for the new
682"
OPEN ACCESS TO DATA AND CODE,0.8063851699279093,"proposed method and baselines. If only a subset of experiments are reproducible, they
683"
OPEN ACCESS TO DATA AND CODE,0.8074150360453141,"should state which ones are omitted from the script and why.
684"
OPEN ACCESS TO DATA AND CODE,0.8084449021627188,"• At submission time, to preserve anonymity, the authors should release anonymized
685"
OPEN ACCESS TO DATA AND CODE,0.8094747682801235,"versions (if applicable).
686"
OPEN ACCESS TO DATA AND CODE,0.8105046343975283,"• Providing as much information as possible in supplemental material (appended to the
687"
OPEN ACCESS TO DATA AND CODE,0.811534500514933,"paper) is recommended, but including URLs to data and code is permitted.
688"
OPEN ACCESS TO DATA AND CODE,0.8125643666323378,"6. Experimental Setting/Details
689"
OPEN ACCESS TO DATA AND CODE,0.8135942327497425,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
690"
OPEN ACCESS TO DATA AND CODE,0.8146240988671473,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
691"
OPEN ACCESS TO DATA AND CODE,0.815653964984552,"results?
692"
OPEN ACCESS TO DATA AND CODE,0.8166838311019567,"Answer: [Yes]
693"
OPEN ACCESS TO DATA AND CODE,0.8177136972193615,"Justification: Justification: All details are provided in the main content and the appendix.
694"
OPEN ACCESS TO DATA AND CODE,0.8187435633367662,"Guidelines:
695"
OPEN ACCESS TO DATA AND CODE,0.819773429454171,"• The answer NA means that the paper does not include experiments.
696"
OPEN ACCESS TO DATA AND CODE,0.8208032955715757,"• The experimental setting should be presented in the core of the paper to a level of detail
697"
OPEN ACCESS TO DATA AND CODE,0.8218331616889805,"that is necessary to appreciate the results and make sense of them.
698"
OPEN ACCESS TO DATA AND CODE,0.8228630278063852,"• The full details can be provided either with the code, in appendix, or as supplemental
699"
OPEN ACCESS TO DATA AND CODE,0.82389289392379,"material.
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8249227600411947,"7. Experiment Statistical Significance
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8259526261585994,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8269824922760041,"information about the statistical significance of the experiments?
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8280123583934088,"Answer: [Yes]
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8290422245108136,"Justification: All gym env experiments are run with 10 different random seeds. Performance
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8300720906282183,"are reported by 1 sigma shaded area over all 10 runs.
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.831101956745623,"Guidelines:
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8321318228630278,"• The answer NA means that the paper does not include experiments.
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8331616889804325,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8341915550978373,"dence intervals, or statistical significance tests, at least for the experiments that support
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.835221421215242,"the main claims of the paper.
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8362512873326468,"• The factors of variability that the error bars are capturing should be clearly stated (for
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8372811534500515,"example, train/test split, initialization, random drawing of some parameter, or overall
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8383110195674562,"run with given experimental conditions).
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.839340885684861,"• The method for calculating the error bars should be explained (closed form formula,
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8403707518022657,"call to a library function, bootstrap, etc.)
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8414006179196705,"• The assumptions made should be given (e.g., Normally distributed errors).
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8424304840370752,"• It should be clear whether the error bar is the standard deviation or the standard error
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.84346035015448,"of the mean.
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8444902162718847,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8455200823892894,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465499485066942,"of Normality of errors is not verified.
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8475798146240988,"• For asymmetric distributions, the authors should be careful not to show in tables or
723"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8486096807415036,"figures symmetric error bars that would yield results that are out of range (e.g. negative
724"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8496395468589083,"error rates).
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.850669412976313,"• If error bars are reported in tables or plots, The authors should explain in the text how
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8516992790937178,"they were calculated and reference the corresponding figures or tables in the text.
727"
EXPERIMENTS COMPUTE RESOURCES,0.8527291452111225,"8. Experiments Compute Resources
728"
EXPERIMENTS COMPUTE RESOURCES,0.8537590113285273,"Question: For each experiment, does the paper provide sufficient information on the com-
729"
EXPERIMENTS COMPUTE RESOURCES,0.854788877445932,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
730"
EXPERIMENTS COMPUTE RESOURCES,0.8558187435633368,"the experiments?
731"
EXPERIMENTS COMPUTE RESOURCES,0.8568486096807415,"Answer: [Yes]
732"
EXPERIMENTS COMPUTE RESOURCES,0.8578784757981462,"Justification: Computational details are provided in the Appendix.
733"
EXPERIMENTS COMPUTE RESOURCES,0.858908341915551,"Guidelines:
734"
EXPERIMENTS COMPUTE RESOURCES,0.8599382080329557,"• The answer NA means that the paper does not include experiments.
735"
EXPERIMENTS COMPUTE RESOURCES,0.8609680741503605,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
736"
EXPERIMENTS COMPUTE RESOURCES,0.8619979402677652,"or cloud provider, including relevant memory and storage.
737"
EXPERIMENTS COMPUTE RESOURCES,0.86302780638517,"• The paper should provide the amount of compute required for each of the individual
738"
EXPERIMENTS COMPUTE RESOURCES,0.8640576725025747,"experimental runs as well as estimate the total compute.
739"
EXPERIMENTS COMPUTE RESOURCES,0.8650875386199794,"• The paper should disclose whether the full research project required more compute
740"
EXPERIMENTS COMPUTE RESOURCES,0.8661174047373842,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
741"
EXPERIMENTS COMPUTE RESOURCES,0.8671472708547889,"didn’t make it into the paper).
742"
CODE OF ETHICS,0.8681771369721936,"9. Code Of Ethics
743"
CODE OF ETHICS,0.8692070030895983,"Question: Does the research conducted in the paper conform, in every respect, with the
744"
CODE OF ETHICS,0.870236869207003,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
745"
CODE OF ETHICS,0.8712667353244078,"Answer: [Yes]
746"
CODE OF ETHICS,0.8722966014418125,"Justification: The research was conducted in accordance with the NeurIPs Code of Ethics.
747"
CODE OF ETHICS,0.8733264675592173,"Guidelines:
748"
CODE OF ETHICS,0.874356333676622,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
749"
CODE OF ETHICS,0.8753861997940268,"• If the authors answer No, they should explain the special circumstances that require a
750"
CODE OF ETHICS,0.8764160659114315,"deviation from the Code of Ethics.
751"
CODE OF ETHICS,0.8774459320288363,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
752"
CODE OF ETHICS,0.878475798146241,"eration due to laws or regulations in their jurisdiction).
753"
BROADER IMPACTS,0.8795056642636457,"10. Broader Impacts
754"
BROADER IMPACTS,0.8805355303810505,"Question: Does the paper discuss both potential positive societal impacts and negative
755"
BROADER IMPACTS,0.8815653964984552,"societal impacts of the work performed?
756"
BROADER IMPACTS,0.88259526261586,"Answer: [NA]
757"
BROADER IMPACTS,0.8836251287332647,"Justification: The work in the paper has no potential for societal impacts.
758"
BROADER IMPACTS,0.8846549948506695,"Guidelines:
759"
BROADER IMPACTS,0.8856848609680742,"• The answer NA means that there is no societal impact of the work performed.
760"
BROADER IMPACTS,0.8867147270854789,"• If the authors answer NA or No, they should explain why their work has no societal
761"
BROADER IMPACTS,0.8877445932028837,"impact or why the paper does not address societal impact.
762"
BROADER IMPACTS,0.8887744593202883,"• Examples of negative societal impacts include potential malicious or unintended uses
763"
BROADER IMPACTS,0.8898043254376931,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
764"
BROADER IMPACTS,0.8908341915550978,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
765"
BROADER IMPACTS,0.8918640576725025,"groups), privacy considerations, and security considerations.
766"
BROADER IMPACTS,0.8928939237899073,"• The conference expects that many papers will be foundational research and not tied
767"
BROADER IMPACTS,0.893923789907312,"to particular applications, let alone deployments. However, if there is a direct path to
768"
BROADER IMPACTS,0.8949536560247168,"any negative applications, the authors should point it out. For example, it is legitimate
769"
BROADER IMPACTS,0.8959835221421215,"to point out that an improvement in the quality of generative models could be used to
770"
BROADER IMPACTS,0.8970133882595263,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
771"
BROADER IMPACTS,0.898043254376931,"that a generic algorithm for optimizing neural networks could enable people to train
772"
BROADER IMPACTS,0.8990731204943357,"models that generate Deepfakes faster.
773"
BROADER IMPACTS,0.9001029866117405,"• The authors should consider possible harms that could arise when the technology is
774"
BROADER IMPACTS,0.9011328527291452,"being used as intended and functioning correctly, harms that could arise when the
775"
BROADER IMPACTS,0.90216271884655,"technology is being used as intended but gives incorrect results, and harms following
776"
BROADER IMPACTS,0.9031925849639547,"from (intentional or unintentional) misuse of the technology.
777"
BROADER IMPACTS,0.9042224510813595,"• If there are negative societal impacts, the authors could also discuss possible mitigation
778"
BROADER IMPACTS,0.9052523171987642,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
779"
BROADER IMPACTS,0.9062821833161689,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
780"
BROADER IMPACTS,0.9073120494335737,"feedback over time, improving the efficiency and accessibility of ML).
781"
SAFEGUARDS,0.9083419155509783,"11. Safeguards
782"
SAFEGUARDS,0.9093717816683831,"Question: Does the paper describe safeguards that have been put in place for responsible
783"
SAFEGUARDS,0.9104016477857878,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
784"
SAFEGUARDS,0.9114315139031925,"image generators, or scraped datasets)?
785"
SAFEGUARDS,0.9124613800205973,"Answer: [NA]
786"
SAFEGUARDS,0.913491246138002,"Justification: The paper poses no such risks.
787"
SAFEGUARDS,0.9145211122554068,"Guidelines:
788"
SAFEGUARDS,0.9155509783728115,"• The answer NA means that the paper poses no such risks.
789"
SAFEGUARDS,0.9165808444902163,"• Released models that have a high risk for misuse or dual-use should be released with
790"
SAFEGUARDS,0.917610710607621,"necessary safeguards to allow for controlled use of the model, for example by requiring
791"
SAFEGUARDS,0.9186405767250257,"that users adhere to usage guidelines or restrictions to access the model or implementing
792"
SAFEGUARDS,0.9196704428424305,"safety filters.
793"
SAFEGUARDS,0.9207003089598352,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
794"
SAFEGUARDS,0.92173017507724,"should describe how they avoided releasing unsafe images.
795"
SAFEGUARDS,0.9227600411946447,"• We recognize that providing effective safeguards is challenging, and many papers do
796"
SAFEGUARDS,0.9237899073120495,"not require this, but we encourage authors to take this into account and make a best
797"
SAFEGUARDS,0.9248197734294542,"faith effort.
798"
LICENSES FOR EXISTING ASSETS,0.925849639546859,"12. Licenses for existing assets
799"
LICENSES FOR EXISTING ASSETS,0.9268795056642637,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
800"
LICENSES FOR EXISTING ASSETS,0.9279093717816684,"the paper, properly credited and are the license and terms of use explicitly mentioned and
801"
LICENSES FOR EXISTING ASSETS,0.9289392378990731,"properly respected?
802"
LICENSES FOR EXISTING ASSETS,0.9299691040164778,"Answer: [Yes]
803"
LICENSES FOR EXISTING ASSETS,0.9309989701338826,"Justification: The only applicable assets are the code which are credited and distributed
804"
LICENSES FOR EXISTING ASSETS,0.9320288362512873,"under a Creative Commons Attribution License.
805"
LICENSES FOR EXISTING ASSETS,0.933058702368692,"Guidelines:
806"
LICENSES FOR EXISTING ASSETS,0.9340885684860968,"• The answer NA means that the paper does not use existing assets.
807"
LICENSES FOR EXISTING ASSETS,0.9351184346035015,"• The authors should cite the original paper that produced the code package or dataset.
808"
LICENSES FOR EXISTING ASSETS,0.9361483007209063,"• The authors should state which version of the asset is used and, if possible, include a
809"
LICENSES FOR EXISTING ASSETS,0.937178166838311,"URL.
810"
LICENSES FOR EXISTING ASSETS,0.9382080329557158,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
811"
LICENSES FOR EXISTING ASSETS,0.9392378990731205,"• For scraped data from a particular source (e.g., website), the copyright and terms of
812"
LICENSES FOR EXISTING ASSETS,0.9402677651905252,"service of that source should be provided.
813"
LICENSES FOR EXISTING ASSETS,0.94129763130793,"• If assets are released, the license, copyright information, and terms of use in the
814"
LICENSES FOR EXISTING ASSETS,0.9423274974253347,"package should be provided. For popular datasets, paperswithcode.com/datasets
815"
LICENSES FOR EXISTING ASSETS,0.9433573635427395,"has curated licenses for some datasets. Their licensing guide can help determine the
816"
LICENSES FOR EXISTING ASSETS,0.9443872296601442,"license of a dataset.
817"
LICENSES FOR EXISTING ASSETS,0.945417095777549,"• For existing datasets that are re-packaged, both the original license and the license of
818"
LICENSES FOR EXISTING ASSETS,0.9464469618949537,"the derived asset (if it has changed) should be provided.
819"
LICENSES FOR EXISTING ASSETS,0.9474768280123584,"• If this information is not available online, the authors are encouraged to reach out to
820"
LICENSES FOR EXISTING ASSETS,0.9485066941297632,"the asset’s creators.
821"
NEW ASSETS,0.9495365602471678,"13. New Assets
822"
NEW ASSETS,0.9505664263645726,"Question: Are new assets introduced in the paper well documented and is the documentation
823"
NEW ASSETS,0.9515962924819773,"provided alongside the assets?
824"
NEW ASSETS,0.952626158599382,"Answer: [Yes]
825"
NEW ASSETS,0.9536560247167868,"Justification: New assets include the code required to run the experiments described in the
826"
NEW ASSETS,0.9546858908341915,"paper. Documentation is provided along with the code.
827"
NEW ASSETS,0.9557157569515963,"Guidelines:
828"
NEW ASSETS,0.956745623069001,"• The answer NA means that the paper does not release new assets.
829"
NEW ASSETS,0.9577754891864058,"• Researchers should communicate the details of the dataset/code/model as part of their
830"
NEW ASSETS,0.9588053553038105,"submissions via structured templates. This includes details about training, license,
831"
NEW ASSETS,0.9598352214212152,"limitations, etc.
832"
NEW ASSETS,0.96086508753862,"• The paper should discuss whether and how consent was obtained from people whose
833"
NEW ASSETS,0.9618949536560247,"asset is used.
834"
NEW ASSETS,0.9629248197734295,"• At submission time, remember to anonymize your assets (if applicable). You can either
835"
NEW ASSETS,0.9639546858908342,"create an anonymized URL or include an anonymized zip file.
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.964984552008239,"14. Crowdsourcing and Research with Human Subjects
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9660144181256437,"Question: For crowdsourcing experiments and research with human subjects, does the paper
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670442842430484,"include the full text of instructions given to participants and screenshots, if applicable, as
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9680741503604532,"well as details about compensation (if any)?
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9691040164778579,"Answer: [NA]
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9701338825952626,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9711637487126673,"Guidelines:
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.972193614830072,"• The answer NA means that the paper does not involve crowdsourcing nor research with
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732234809474768,"human subjects.
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742533470648815,"• Including this information in the supplemental material is fine, but if the main contribu-
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752832131822863,"tion of the paper involves human subjects, then as much detail as possible should be
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976313079299691,"included in the main paper.
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9773429454170958,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783728115345005,"or other labor should be paid at least the minimum wage in the country of the data
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794026776519053,"collector.
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.98043254376931,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9814624098867147,"Subjects
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824922760041195,"Question: Does the paper describe potential risks incurred by study participants, whether
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9835221421215242,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984552008238929,"approvals (or an equivalent approval/review based on the requirements of your country or
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9855818743563337,"institution) were obtained?
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9866117404737385,"Answer: [NA]
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876416065911432,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886714727085479,"Guidelines:
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897013388259527,"• The answer NA means that the paper does not involve crowdsourcing nor research with
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9907312049433573,"human subjects.
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9917610710607621,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9927909371781668,"may be required for any human subjects research. If you obtained IRB approval, you
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9938208032955715,"should clearly state this in the paper.
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948506694129763,"• We recognize that the procedures for this may vary significantly between institutions
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995880535530381,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969104016477858,"guidelines for their institution.
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979402677651905,"• For initial submissions, do not include any information that would break anonymity (if
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989701338825953,"applicable), such as the institution conducting the review.
870"
