Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0040650406504065045,"Despite the recent success of Graph Neural Networks (GNNs), training GNNs on
large graphs remains challenging. The limited resource capacities of the existing
servers, the dependency between nodes in a graph, and the privacy concern due
to the centralized storage and model learning have spurred the need to design an
effective distributed algorithm for GNN training. However, existing distributed
GNN training methods impose either excessive communication costs or large
memory overheads that hinders their scalability. To overcome these issues, we
propose a communication-efﬁcient distributed GNN training technique named
Learn Locally, Correct Globally (LLCG). To reduce the communication and mem-
ory overhead, each local machine in LLCG ﬁrst trains a GNN on its local data by
ignoring the dependency between nodes among different machines, then sends the
locally trained model to the server for periodic model averaging. However, ignoring
node dependency could result in signiﬁcant performance degradation. To solve the
performance degradation, we propose to apply Global Server Corrections on the
server to reﬁne the locally learned models. We rigorously analyze the convergence
of distributed methods with periodic model averaging for training GNNs and show
that naively applying periodic model averaging but ignoring the dependency be-
tween nodes will suffer from an irreducible residual error. However, this residual
error can be eliminated by utilizing the proposed global corrections to entail fast
convergence rate. Extensive experiments on real-world datasets show that LLCG
can signiﬁcantly improve the efﬁciency without hurting the performance."
INTRODUCTION,0.008130081300813009,"1
INTRODUCTION"
INTRODUCTION,0.012195121951219513,"In recent years, Graph Neural Networks (GNNs) have achieved impressive results across numerous
graph-based applications, including social networks (Hamilton et al., 2017; Deng et al., 2019), 0 5 10"
INTRODUCTION,0.016260162601626018,Memory (GB)
INTRODUCTION,0.02032520325203252,"Speedup
Memory"
INTRODUCTION,0.024390243902439025,"1
2
4
No. Machines 2 4"
INTRODUCTION,0.028455284552845527,Speedup
INTRODUCTION,0.032520325203252036,"Figure 1: Comparison of the
speedup and the memory con-
sumption of distributed multi-
machine training and central-
ized single machine training
on the Reddit dataset."
INTRODUCTION,0.036585365853658534,"recommendation systems (Ying et al., 2018; Wang et al., 2018),
and drug discovery (Fout et al., 2017; Do et al., 2019; Ghorbani
et al., 2022; Faez et al., 2021). Despite their recent success, effective
training of GNNs on large-scale real-world graphs, such as Face-
book social network (Boldi & Vigna, 2004), remains challenging.
Although several attempts have been made to scale GNN training by
sampling techniques (Hamilton et al., 2017; Zou et al., 2019; Zeng
et al., 2020; Chiang et al., 2019; Chen et al., 2018; Zhang et al.,
2021; Ramezani et al., 2020), they are still inefﬁcient for training
on extremely large graphs, due to the unique structure of GNNs
and the limited memory capacity/bandwidth of current servers. One
potential solution to tackle these limitations is employing distributed
training with data parallelism, which have become almost a de facto
standard for fast and accurate training for natural language processing (Lin et al., 2021; Hard et al.,"
INTRODUCTION,0.04065040650406504,∗Equal Contribution.
INTRODUCTION,0.044715447154471545,Published as a conference paper at ICLR 2022
INTRODUCTION,0.04878048780487805,"2018) and computer vision (Bonawitz et al., 2019; Koneˇcn`y et al., 2018). For example, as shown in
Figure 1, moving from single machine to multiple machines reduces the training time and alleviates
the memory burden on each machine. Besides, scaling the training of GNNs with sampling techniques
can result in privacy concerns: existing sampling-based methods require centralized data storage and
model learning, which could result in privacy concerns in real-world scenarios (Shin et al., 2018; Wu
et al., 2021). Fortunately, the privacy in distributed learning can be preserved by avoiding mutual
access to data between different local machines, and using only a trusted third party server to access
the entire data."
INTRODUCTION,0.052845528455284556,"Nonetheless, generalizing the existing data parallelism techniques of classical distributed train-
ing to the graph domain is non-trivial, which is mainly due to the dependency between nodes
in a graph. For example, unlike solving image classiﬁcation problems where images are mu-
tually independent, such that we can divide the image dataset into several partitions without
worrying about the dependency between images; GNNs are heavily relying on the information
inherent to a node and its neighboring nodes. As a result, partitioning the graph leads to sub-
graphs with edges spanning subgraphs (cut-edges), which will cause information loss and hin-
der the performance of the model (Angerd et al., 2020). To cope with this problem, (Md et al.,
2021; Jiang & Rumi, 2021; Angerd et al., 2020) propose to transfer node features and (Zheng
et al., 2020; Tripathy et al., 2020; Scardapane et al., 2020) propose to transfer both the node
feature and its hidden embeddings between local machines, both of which can cause signiﬁ-
cant storage/communication overhead and privacy concerns (Shin et al., 2018; Wu et al., 2021)."
INTRODUCTION,0.056910569105691054,"0
20
40
60
Number of Communication 0.00 0.25 0.50 0.75"
INTRODUCTION,0.06097560975609756,Global Validation
INTRODUCTION,0.06504065040650407,F1 Score
INTRODUCTION,0.06910569105691057,"PSGA-PA
GGS
Baseline Score (a)"
INTRODUCTION,0.07317073170731707,"PSGD-PA
GGS
foo 108 109"
INTRODUCTION,0.07723577235772358,Average Number of
INTRODUCTION,0.08130081300813008,Bytes Communicated
INTRODUCTION,0.08536585365853659,"(b)
Figure 2: Comparison of (a) the validation F1-
score and (b) the average data communicated
per round (in bytes and log-scale) for two differ-
ent distributed GNN training settings, including
Parallel SGD with Periodic Averaging (PSGD-
PA) where the cut-edges are ignored and only
the model parameters are transferred and Global
Graph Sampling (GGS), where the cut-edges are
considered and the node features of the cut-edges
are transferred to the corresponding local ma-
chine, on the Reddit dataset using 8 machines."
INTRODUCTION,0.08943089430894309,"To better understand the challenge of distributed
GNN training, we compare the validation F1-
score in Figure 2 (a) and the average data com-
municated per round in Figure 2 (b) for two dif-
ferent distributed GNN training methods on the
Reddit dataset. On the one hand, we can ob-
serve that when ignoring the cut-edges, Parallel
SGD with Periodic Averaging (PSGD-PA (Dean
et al., 2012; Li et al., 2020b)) suffers from sig-
niﬁcant accuracy drop and cannot achieve the
same accuracy as the single machine training,
even by increasing the number of communica-
tion. However, Global Graph Sampling (GGS)
can successfully reach the baseline by consider-
ing the cut-edges and allowing feature transfer, at
the cost of signiﬁcant communication overhead,
and potential violation of privacy."
INTRODUCTION,0.09349593495934959,"In this paper, we propose a communication-efﬁcient distributed GNN training method, called Learn
Locally, Correct Globally (LLCG). To reduce the communication overhead, inspired by the recent
success of the distributed optimization with periodic averaging (Stich, 2019; Yu et al., 2019), we
propose Local Training with Periodic Averaging: where each local machine ﬁrst locally trains a
GNN model by ignoring the cut-edges, then sends the trained model to the server for periodic model
averaging, and receive the averaged model from server to continue the training. By doing so we
eliminate the features exchange phase between server and local machines, but it can result in a
signiﬁcant performance degradation due to the lack of the global graph structure and the dependency
between nodes among different machines. To compensate for this error, we propose a Global Server
Correction scheme to take advantage of the available global graph structure on the server and reﬁne
the averaged locally learned models before sending it back to each local machine. Notice that without
Global Server Correction, LLCG is similar to PSGD-PA as introduced in Figure 2."
INTRODUCTION,0.0975609756097561,"To get a deeper understanding on the necessity of Global Server Correction, we provide the ﬁrst
theoretical analysis on the convergence of distributed training for GNNs with periodic averaging. In
particular, we show that solely averaging the local machine models and ignoring the global graph
structure will suffer from an irreducible residual error, which provides sufﬁcient explanation on
why Parallel SGD with Periodic Averaging can never achieve the same performance as the model
trained on a single machine in Figure 2 (a). Then, we theoretically analyze the convergence of
our proposal LLCG . We show that by carefully choosing the number of global correction steps,"
INTRODUCTION,0.1016260162601626,Published as a conference paper at ICLR 2022
INTRODUCTION,0.10569105691056911,"LLCG can overcome the aforementioned residual error and enjoys O
 
1/
√"
INTRODUCTION,0.10975609756097561,"PT

convergence rate
with P local machines and T iterations of gradient updates, which matches the rate of (Yu et al.,
2019) on a general (not speciﬁc for GNN training) non-convex optimization setting. Finally, we
conduct comprehensive evaluations on real-world graph datasets with ablation study to validate the
effectiveness of LLCG and its improvements over the existing distributed methods."
INTRODUCTION,0.11382113821138211,"Related works. Recently, several attempts have been made on distributed GNN training. According
to how they deal with the input/hidden feature of nodes that are associated with the cut-edges (i.e.,
the edges spanning subgraphs of each local machine), existing methods can be classiﬁed into two
main categories: (1) Input feature only communication-based methods: In these methods, each local
machine receives the input features of all nodes required for the gradient computation from other
machines, and trains individually. However, since the number of required nodes grows exponentially
with the number of layers, these methods suffer from a signiﬁcant communication and storage
overhead. To alleviate these issues, (Md et al., 2021) proposes to split the original graph using a
min-cut graph partition algorithm that can minimize the number of cut-edges. (Jiang & Rumi, 2021)
proposes to use importance sampling to assign nodes on the local machine with a higher probability.
(Angerd et al., 2020) proposes to sample and save a small subgraph from other local machines as
an approximation of the original graph structure. Nonetheless, these methods are limited to a very
shallow GNN structure and suffer from signiﬁcant performance degradation when the original graph
is dense. (2) Input and hidden feature communication-based methods: These methods propose to
communicate hidden features in addition to the input node features. Although these methods reduce
the number of transferred bytes during each communication round (due to the smaller size of hidden
embedding and less required nodes features), the number of communication rounds grows linearly
as the number of layers, and are prone to more communication delay. To address these issues, in
addition to optimal partitioning of the graph, (Zheng et al., 2020) proposes to use sparse embedding
to reduce the number of bytes to communicate and (Tripathy et al., 2020) proposes several graph
partitioning techniques to diminish the communication overhead."
BACKGROUND AND PROBLEM FORMULATION,0.11788617886178862,"2
BACKGROUND AND PROBLEM FORMULATION"
BACKGROUND AND PROBLEM FORMULATION,0.12195121951219512,"In this section, we start by describing Graph Convolutional Network (GCN) and its training algorithm
on a single machine, then formulate the problem of distributed GCN training. Note that we use
GCN with mean aggregation for simplicity, however, our discussion is also applicable to other GNN
architectures, such as SAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2018), ResGCN (Li
et al., 2019) and APPNP (Klicpera et al., 2019)."
BACKGROUND AND PROBLEM FORMULATION,0.12601626016260162,"Training GCN on a single machine. Here, we consider the semi-supervised node classiﬁcation in
an undirected graph G(V, E) with N = |V| nodes and |E| edges. Each node vi ∈V is associated with
a pair (xi, yi), where xi ∈Rd is the input feature vector, yi ∈R|C| is the ground truth label, and C is
the candidate labels in the multi-class classiﬁcations. Besides, let X = [x1, . . . , xN] ∈RN×d denote
the input node feature matrix. Our goal is to ﬁnd a set of parameters θ = {W(ℓ)}L
ℓ=1 by minimizing
the empirical loss L(θ) over all nodes in the training set, i.e.,"
BACKGROUND AND PROBLEM FORMULATION,0.13008130081300814,L(θ) = 1 N X
BACKGROUND AND PROBLEM FORMULATION,0.13414634146341464,"i∈V φ(h(L)
i
, yi),
h(ℓ)
i
= σ

1
|N(vi)| X"
BACKGROUND AND PROBLEM FORMULATION,0.13821138211382114,"j∈N(vi) h(ℓ−1)
j
W(ℓ)
,
(1)"
BACKGROUND AND PROBLEM FORMULATION,0.14227642276422764,"where φ(·, ·) is the loss function (e.g., cross entropy loss), σ(·) is the activation function (e.g., ReLU),
and N(vi) is the neighborhood of node vi. In practice, we can update the model parameters by the
stochastic gradient computed on a sampled mini-batch (using full-neighbors) by
˜∇L(θ, ξ) = 1 B X"
BACKGROUND AND PROBLEM FORMULATION,0.14634146341463414,"i∈ξ ∇φ(h(L)
i
, yi),
(2)"
BACKGROUND AND PROBLEM FORMULATION,0.15040650406504066,"where ξ denotes an i.i.d. sampled mini-batch of size B and we have E[ ˜∇L(θ, ξ)] = ∇L(θ)."
BACKGROUND AND PROBLEM FORMULATION,0.15447154471544716,"Distributed GCN training with periodic averaging.
In this paper, we consider the distributed
learning setting with P local machines and a single parameter server. The original input graph G
is partitioned into P subgraphs, where Gp(Vp, Ep) denotes the subgraph on the p-th local machine
with Np = |Vp| nodes, and Xp ∈RNp×d as the input feature of all nodes in Vp located on the p-th
machine. Then, the full-batch local gradient ∇Llocal
p
(θp) is computed as"
BACKGROUND AND PROBLEM FORMULATION,0.15853658536585366,"∇Llocal
p
(θp) = 1 Np X"
BACKGROUND AND PROBLEM FORMULATION,0.16260162601626016,"i∈Vp ∇φ(h(L)
i
, yi), h(ℓ)
i
= σ

1
|Np(vi)| X"
BACKGROUND AND PROBLEM FORMULATION,0.16666666666666666,"j∈Np(vi) h(ℓ−1)
j
W(ℓ)
p

,
(3)"
BACKGROUND AND PROBLEM FORMULATION,0.17073170731707318,Published as a conference paper at ICLR 2022
BACKGROUND AND PROBLEM FORMULATION,0.17479674796747968,Algorithm 1 Distributed GCN training with “Parallel SGD with Periodic Averaging”
BACKGROUND AND PROBLEM FORMULATION,0.17886178861788618,"Input: Global parameters ¯θ0, local parameters θ0
p = ¯θ0, time-step t = 0, learning rate η.
1: for r ←1 to R do
2:
for p ←1 to P do in parallel
▷Parallel training on local machines
3:
Local machine p receives the global parameters θt
p ←¯θt.
▷Communication
4:
for k ←1 to K do
5:
t ←t + 1.
6:
Local machine p constructs the mini-batch ξt
p with neighbor sampling.
7:
Local machine p computes the stochastic gradients ˜∇Llocal
p
(θt
p, ξt
p).
8:
Local machine p updates the local parameter by θt+1
p
= θt
p −η ˜∇Llocal
p
(θt
p, ξt
p).
9:
end for
10:
Local machine p sends the local parameters θt+1
p
to the server. ▷Communication
11:
end for
12:
Server updates the global parameters by parameter averaging ¯θt+1 = 1"
BACKGROUND AND PROBLEM FORMULATION,0.18292682926829268,"P
PP
p=1 θt+1
p
.
13: end for
Output: Server returns trained GCN model with mint E[∥∇L(¯θt)∥2]."
BACKGROUND AND PROBLEM FORMULATION,0.18699186991869918,"where θp = {W(ℓ)
p }L
ℓ=1 is the model parameters on the p-th local machine, Np(vi) = {vj|(vi, vj) ∈
Ep} is the local neighbors of node vi on the p-th local machine. When the graph is large, the
computational complexity of forward and backward propagation could be very high. One practical
solution is to compute the stochastic gradient on a sampled mini-batch with neighbor sampling, i.e.,
˜∇Llocal
p
(θp, ξp) = 1 Bp X"
BACKGROUND AND PROBLEM FORMULATION,0.1910569105691057,"i∈ξp ∇φ(˜h(L)
i
, yi), ˜h(ℓ)
i
= σ

1
| ˜
Np(vi)| X"
BACKGROUND AND PROBLEM FORMULATION,0.1951219512195122,"j∈˜
Np(vi)
˜h(ℓ−1)
j
W(ℓ)
p

, (4)"
BACKGROUND AND PROBLEM FORMULATION,0.1991869918699187,"where ξp is an i.i.d. sampled mini-batch of Bp nodes, ˜
Np(vi) ⊂N(vi) is the sampled neighbors."
BACKGROUND AND PROBLEM FORMULATION,0.2032520325203252,"An illustration of distributed GCN training with Parallel SGD with Periodic Averaging (PSGD-PA) is
summarized in Algorithm 1. Before training, the server maintains a global model ¯θ0 and each local
machine keeps a local copy of the same model θ0
p. During training, the local machine ﬁrst updates
the local model θt
p using the stochastic gradient ˜∇Llocal
p
(θt
p, ξt
p) computed by Eq. 4 for K iterations
(line 8), then sends the local model θt
p to the server (line 10). At each communication step, the server
collects and averages the model parameters from the local machines (line 12) and send the averaged
model θt+1
p
back to each local machine."
BACKGROUND AND PROBLEM FORMULATION,0.2073170731707317,"Limitations. Although PSGD-PA can signiﬁcantly reduce the communication overhead by transfer-
ring the locally trained models instead of node feature/embeddings (refer to Figure 2 (b)), it suffers
from performance degeneration due to ignorance of the cut-edges (refer to Figure 2 (a)). In the next
section, we introduce a communication-efﬁcient algorithm LLCG that does not suffer from this
issue, and can achieve almost the same performance as training the model on a single machine."
BACKGROUND AND PROBLEM FORMULATION,0.21138211382113822,"3
PROPOSED ALGORITHM: LEARN LOCALLY CORRECT GLOBALLY"
BACKGROUND AND PROBLEM FORMULATION,0.21544715447154472,"In this section, we describe Learn Locally, Correct Globally (LLCG) for distributed GNN training.
LLCG includes two main phases, local training with periodic model averaging and global server
correction, to help reduce both the number of required communications and size of transferred data,
without compromising the predictive accuracy. We summarize the details of LLCG in Algorithm 2."
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.21951219512195122,"3.1
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING"
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.22357723577235772,"At the beginning of a local epoch, each local machine receives the latest global model parameters
from the server (line 3). Next, each local machine runs Kρr iterations to update the local model
(line 4 to 9), where K and ρ are the hyper-parameters that control the local epoch size. Note that
instead of using a ﬁxed local epoch size as Algorithm 1, we choose to use exponentially increasing
local epoch size in LLCG with ρ > 1. The reasons are as follows."
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.22764227642276422,"At the beginning of the training phase, all local models θt
p are far from the optimal solution and
will receive a gradient ˜∇Llocal
p
(θt
p, ξt
p) computed by Eq. 4. Using a smaller local update step at
the early stage guarantees each local model does not diverge too much from each other before the
model averaging step at the server side (line 12). However, towards the end of the training, all local"
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.23170731707317074,Published as a conference paper at ICLR 2022
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.23577235772357724,"Algorithm 2 Distributed GCN training by “Learn Locally, Correct Globally”"
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.23983739837398374,"Input: Global parameters ¯θ0, local parameters θ0
p, time-step t = 0, local step size hyper-
parameters K, ρ, and learning rate γ, η
1: for r ←1 to R do
2:
for p ←1 to P do in parallel
▷Parallel training on local machine
3:
Local machine p receives the global parameters θt
p ←¯θt
▷Communication
4:
for k ←1 to Kρr do
5:
t ←t + 1
6:
Local machine p constructs the mini-batch ξt
p with neighbor sampling
7:
Local machine p computes stochastic gradients ˜∇Llocal
p
(θt
p, ξt
p)
8:
Local machine p updates model parameter by θt+1
p
= θt
p −η ˜∇Llocal
p
(θt
p, ξt
p)
9:
end for
10:
Local machine p sends the local parameters θt+1
p
to the server
▷Communication
11:
end for
12:
Server updates the global parameters using parameter averaging ¯θt+1 = 1"
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.24390243902439024,"P
PP
p=1 θt+1
p
13:
for s ←1 to S do
▷Server Correction
14:
t ←t + 1
15:
Server constructs a mini-batch ξt with full-neighbors
16:
Server computes the stochastic gradient ˜∇L(¯θt, ξt)
17:
Server updates the global parameters by ¯θt+1 = ¯θt −γ ˜∇L(¯θt, ξt)
18:
end for
19: end for
Output: Server return GCN model with trained mint E[∥∇L(¯θt)∥2]"
LOCAL TRAINING WITH PERIODIC MODEL AVERAGING,0.24796747967479674,"models θt
p will receive relatively smaller gradient ˜∇Llocal
p
(θt
p, ξt
p), such that we can chose a larger
local epoch size to reduce the number of communications, without worrying about the divergence of
local models. By doing so, after total number of T = PR
r=1 Kρr iterations, LLCG only requires
R = logρ
T
K rounds of communications. Therefore, compared to the fully-synchronous method, we
can signiﬁcantly reduce the total number of communications from O(T) to O(logρ
T
K )."
GLOBAL SERVER CORRECTION,0.25203252032520324,"3.2
GLOBAL SERVER CORRECTION"
GLOBAL SERVER CORRECTION,0.25609756097560976,"The design of the global server correction is to ensure that the trained model not only learns from
the data on each local machine, but also learns the global structure of the graph, thus reducing
the information loss caused by graph partitioning and avoiding cut-edges. Before the correction,
the server receives the locally trained models from all local machines (line 10) and applies model
parameter averaging (line 12). Next, S server correction steps are applied on top of the averaged
model (line 13 to 18). During the correction, the server ﬁrst constructs a mini-batch ξt using full-
neighbors1(line 15), compute the stochastic gradient ˜∇L(¯θt, ξt) on the constructed mini-batch by
Eq. 2 (line 16) and update the averaged model ¯θt for S iterations (line 17). The number of correction
steps S 2 depends on the heterogeneity among the subgraphs on each local machine: the more
heterogeneous the subgraphs are, the more correction steps are required to better reﬁne the averaged
model and reduce the divergence across the local models. Note that, the heterogeneity is minimized
when employing GGS (Figure 2) with the local machines having access to the full graph, as a result.
However, GGS requires sampling from the global graph and communication at every iteration, which
results in additional overhead and lower efﬁciency. Instead, in LLCG we are trading computation on
the server for the costly feature communication, and only requires periodic communication."
THEORETICAL ANALYSIS,0.2601626016260163,"4
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.26422764227642276,"In this section, we provide the convergence analysis on the distributed training of GCN under two
different settings, i.e., with and without server correction. We ﬁrst introduce the notations and
assumptions for the analysis (Section 4.1). Then, we show that periodic averaging of local machine
models alone and ignoring the global graph structure will suffer from an irreducible residual error
(Section 4.2). Finally, we show that this residual error can be eliminated by running server correction
steps after each periodic averaging step on the server (Section 4.3)."
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.2682926829268293,"1Note that using full neighbors is required for the server correction but not the local machines
2In practice, we found S = 1 or S = 2 works well on most datasets."
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.27235772357723576,Published as a conference paper at ICLR 2022 ℓ=0 ℓ=1 ℓ=2
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.2764227642276423,"(c) 
Without cross-device communication 
With neighbor sampling"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.2804878048780488,"(b) 
Without cross-device communication 
Without neighbor sampling"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.2845528455284553,"(a) 
With cross-device communication 
Without neighbor sampling"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.2886178861788618,∇L(θ) = ∇Lfull
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.2926829268292683,1 (θ) + ∇Lfull
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.2967479674796748,"2 (θ)
2
∇L(θ) ̸= ∇Llocal"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3008130081300813,"1
(θ) + ∇Llocal"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3048780487804878,"2
(θ)
2 E"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3089430894308943,! ˜∇Llocal
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3130081300813008,"1
(θ, ξ1) """
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3170731707317073,̸= ∇Llocal
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.32113821138211385,"1
(θ), E"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3252032520325203,! ˜∇Llocal
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.32926829268292684,"1
(θ, ξ2) """
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3333333333333333,"̸= ∇Llocal 2
(θ)"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.33739837398373984,˜∇Llocal
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.34146341463414637,"2
(θ, ξ2)
˜∇Llocal"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.34552845528455284,"1
(θ, ξ1)
∇Llocal"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.34959349593495936,"1
(θ)
∇Llocal"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.35365853658536583,"2
(θ)
∇Lfull"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.35772357723577236,"2 (θ)
∇Lfull 1 (θ)"
NOTE THAT USING FULL NEIGHBORS IS REQUIRED FOR THE SERVER CORRECTION BUT NOT THE LOCAL MACHINES,0.3617886178861789,"Figure 3: Comparison of notations ∇Llocal
p
(θ), ˜∇Llocal
p
(θ, ξp), and ∇Lfull
p (θ) on two local machines,
where the blue node and green circles represent nodes on different local machines."
NOTATIONS AND ASSUMPTIONS,0.36585365853658536,"4.1
NOTATIONS AND ASSUMPTIONS"
NOTATIONS AND ASSUMPTIONS,0.3699186991869919,"Let us ﬁrst recall the notations deﬁned in Section 2, where L(θ) denotes the global objective function
computed using the all node features X and the original graph G, Lp(θ) denotes the local objective
function computed using the local node features Xp and local graph Gp, θt
p denotes the model
parameters on the p-th local machine at the t-th step, and ¯θt = 1"
NOTATIONS AND ASSUMPTIONS,0.37398373983739835,"P
PP
p=1 θt
p denotes the virtual
averaged model at the t-th step. In the non-convex optimization, our goal is to show the expected
gradient of the global objective on the virtual averaged model parameters E[∥∇L(¯θt)∥2] decreases as
the number of local machines P and the number of training steps T increase. Besides, we introduce
∇Lfull
p (θ) as the gradient computed on the p-th local machine but have access the full node features
X and the original graph structure G as"
NOTATIONS AND ASSUMPTIONS,0.3780487804878049,"∇Lfull
p (θ) =
1
|Vp| X"
NOTATIONS AND ASSUMPTIONS,0.3821138211382114,"i∈Vp ∇φ(h(L)
i
, yi),
h(ℓ)
i
= σ

1
|N(vi)| X"
NOTATIONS AND ASSUMPTIONS,0.3861788617886179,"j∈N(vi) h(ℓ−1)
j
W(ℓ)
p

.
(5)"
NOTATIONS AND ASSUMPTIONS,0.3902439024390244,"Please refer to Figure 3 for an illustration of different gradient computations. Besides, we introduce
local-global gradient discrepancy as κ2 = κ2
A + κ2
X, where κ2
A = maxp∈[P ]{∥∇Llocal
p
(θ) −
∇Lfull
p (θ)∥2} is the maximum difference between the gradient computed on the local machine with
and without having access to the global graph structure, which is mainly due to fact that the local
machines are oblivious to the full graph information; and κ2
X = maxp∈[P ]{∥∇Lfull
p (θ) −∇L(θ)∥2}
is the maximum difference between the gradient computed using the local node and all nodes, which
is mainly due to the heterogeneity of the node features on each local machine, and we have κ2
X = 0
if the nodes are i.i.d. sampled to each local machine. Notice that local-global gradient discrepancy
κ2 plays an important role in our theoretical results."
NOTATIONS AND ASSUMPTIONS,0.3943089430894309,"For the convergence analysis, we make the following standard assumptions."
NOTATIONS AND ASSUMPTIONS,0.3983739837398374,"Assumption 1 The stochastic gradient on the p-th local machine (with neighbor sampling) has
stochastic gradient variance bounded by σ2
var and stochastic gradient bias bounded by σ2
bias, i.e.,
E[∥˜∇Llocal
p
(θ; ξ) −E[ ˜∇Llocal
p
(θ; ξ)]∥2] ≤σ2
var, E[∥E[ ˜∇Llocal
p
(θ; ξ)] −∇Llocal
p
(θ)∥2] ≤σ2
bias."
NOTATIONS AND ASSUMPTIONS,0.4024390243902439,"Assumption 2 The stochastic gradient for global server correction (with full neighbors) has stochas-
tic gradient variance bounded by σ2
global, i.e., E[∥˜∇Lfull
p (θ; ξ) −∇Lfull
p (θ)]∥2] ≤σ2
global."
NOTATIONS AND ASSUMPTIONS,0.4065040650406504,"The existence of stochastic gradient bias and variance in sampling-based GNN training have been
studied in (Cong et al., 2020; 2021), where (Cong et al., 2021) further quantify the stochastic gradient
bias and variance as a function of the number of GCN layers. In particular, they show that the
existence of σ2
bias is due to neighbor sampling and non-linear activation, and we have σ2
bias = 0 if all
neighbors are used or the non-linear activation is removed. The existence of σ2
var is because we are
sampling mini-batches to compute the stochastic gradient on each local machine during training. As
the mini-batch size increases, σ2
var will be decreasing, and we have σ2
var = 0 when using full-batch."
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.4105691056910569,"4.2
DISTRIBUTED GNN VIA PARAMETER AVERAGING"
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.4146341463414634,"In the following, we provide the ﬁrst convergence analysis on distributed training of GCN. We show
that solely periodic averaging of the local machine models and ignoring the global graph structure
suffers from an upper bound that is irreducible with the number of training steps. Comparing to the
traditional distributed training (e.g., distributed training Convolutional Neural Network for image
classiﬁcation (Dean et al., 2012; Li et al., 2020b)), the key challenges in the distributed GCN training
is the two different types of gradient bias: (1) The expectation of the local full-batch gradient is"
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.4186991869918699,Published as a conference paper at ICLR 2022
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.42276422764227645,"a biased estimation of the global full-batch gradient, i.e.,
1
P
PP
p=1 ∇Llocal
p
(θ) ̸= ∇L(θ). This is
because each local machine does not have access to the original input graph and full node feature
matrix. Note that the aforementioned equivalence is important for the classifcal distributed training
analysis Dean et al. (2012); Yu et al. (2019). (2) The expectation of the local stochastic gradient is a
biased estimation of the local full-batch gradient i.e., E[ ˜∇Llocal
p
(θ, ξp)] ̸= ∇Llocal
p
(θ). This is because
the stochastic gradient on each local machine is computed by using neighbor sampling, which has
been studied in (Cong et al., 2021)."
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.4268292682926829,"Theorem 1 (Distributed GCN via Parameter Averaging) Consider applying model averaging for
GNN training under Assumption 1 and 2. If we choose learning rate η =
√ P
√"
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.43089430894308944,"T and the local step size
K ≤
√"
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.4349593495934959,2T 1/4
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.43902439024390244,"8LP 3/4 , then for any T ≥L2P steps of gradient updates we have
1
T XT −1"
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.44308943089430897,"t=0 E[∥∇L(¯θt)∥2] = O

1
√ PT"
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.44715447154471544,"
+ O(κ2 + σ2
bias)."
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.45121951219512196,"Theorem 1 implies that, by carefully choosing the learning rate η and the local step size K, the gradient
norm computed on the virtual averaged model is bounded by O(1/
√"
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.45528455284552843,PT) after R = T/K = O( P 3/4
DISTRIBUTED GNN VIA PARAMETER AVERAGING,0.45934959349593496,"T 3/4 )
communication rounds, but suffers from an irreducible residual error upper bound O(κ2 + σ2
bias). In
the next section, we show that this residual error can be eliminated by applying server correction."
DISTRIBUTED GCN VIA SERVER CORRECTION,0.4634146341463415,"4.3
DISTRIBUTED GCN VIA SERVER CORRECTION"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.46747967479674796,"Before proceeding to our result, in order to simplify the presentation, let us ﬁrst deﬁne the nota-
tion Gr
global = mint∈Tglobal(r) E[∥∇L(¯θt)∥2] and Gr
local = mint∈Tlocal(r) E
 1"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.4715447154471545,"P
PP
p=1 ∇Llocal
p
(θt
p)
2"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.47560975609756095,"as the minimum gradient computed at the r-th round global and local step, where Tglobal(r) and
Tlocal(r) are the number of iteration run after the r-th communication round on server and local
machine, respectively. Please refer to Eq. 42 in Appendix C.2 for a formal deﬁnition."
DISTRIBUTED GCN VIA SERVER CORRECTION,0.4796747967479675,"Theorem 2 Consider applying model averaging for GCN training under Assumption 1 and 2. If we
choose learning rate γ = η =
√ P
√"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.483739837398374,"T , the local step size K, ρ such that PR
r=1 K2ρ2r ≤
RT 1/2"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.4878048780487805,"32L2P 3/2 , and"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.491869918699187,"server correction step size S = maxr∈[R]
 
κ2+2σ2
bias
1−L(√"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.4959349593495935,"P/T ) −Gr
local
 Kρr"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.5,"Gr
local , then for any T ≥L2P steps"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.5040650406504065,of gradient updates we have: 1
DISTRIBUTED GCN VIA SERVER CORRECTION,0.508130081300813,"T
PT
t=1 E[∥∇L(¯θt)∥2] = O
 
1
√"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.5121951219512195,"P T

."
DISTRIBUTED GCN VIA SERVER CORRECTION,0.516260162601626,"Theorem 2 implies that, by carefully choosing the learning rates γ and η, the local step size hyper-
parameters K, ρ, and the number of global correction steps S, after T steps (R rounds of communica-
tion), employing parameter averaging with Global Server Correction, we have the norm of gradient
bounded by O(1/
√"
DISTRIBUTED GCN VIA SERVER CORRECTION,0.5203252032520326,"PT), without suffering the residual error that exists in the naive parameter
averaging (in Theorem 1). Besides, the server correction step size is proportional to the scale of κ2
and local stochastic gradient bias σ2
bias. The larger κ2 and σ2
bias, the more corrections are required to
eliminate the residual error. However, in practice, we observe that a very small number of correction
steps (e.g., S = 1) performs well, which minimizes the computation overhead on the server."
EXPERIMENTS,0.524390243902439,"5
EXPERIMENTS"
EXPERIMENTS,0.5284552845528455,"Real-world simulation. In a real-world distributed setting, the server and local machines are located
on different machines, connected through the network (Li et al., 2020a). However, for our experiments,
we only have access to a single machine with multiple GPUs. As a result, we simulate a real-world
distributed learning scenario, such that each GPU is responsible for the computation of two local
machines (8 in total) and the CPU acts as the server. For these reasons, in our evaluations, we opted
to report the communication size and number of communication rounds, instead of the wall-clock
time, which can show the beneﬁt of distributed training. We argue that these are acceptable measures
in real-world scenarios as well since the two main factors in distributed training are initializing
connection overhead and bandwidth (Tripathy et al., 2020).
Baselines. To illustrate the effectiveness of LLCG, we setup two general synchronized distributed
training techniques as the our baseline methods, namely “Parallel SGD with Parameter Averaging”
(PSGD-PA) and “Global Graph Sampling” (GGS), as introduced in Figure 2, where the cut-edges
in PSGD-PA are ignored and only the model parameters are transferred, but the cut-edges in GGS
are considered and the node features of the cut-edges are transferred to the corresponding machine."
EXPERIMENTS,0.532520325203252,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.5365853658536586,"0
10
20
30
No. Communications 0.40 0.45 0.50"
EXPERIMENTS,0.540650406504065,Global Val. F1 Score
EXPERIMENTS,0.5447154471544715,Flickr
EXPERIMENTS,0.5487804878048781,"PSGD-PA
GGS
LLCG (a)"
EXPERIMENTS,0.5528455284552846,"0
20
40
No. Communications 0.6 0.7"
EXPERIMENTS,0.556910569105691,Global Val. F1 Score
EXPERIMENTS,0.5609756097560976,Proteins
EXPERIMENTS,0.5650406504065041,"PSGD-PA
GGS
LLCG (b)"
EXPERIMENTS,0.5691056910569106,"0
20
40
No. Communications 0.60 0.65 0.70"
EXPERIMENTS,0.573170731707317,Global Val. F1 Score Arxiv
EXPERIMENTS,0.5772357723577236,"PSGD-PA
GGS
LLCG (c)"
EXPERIMENTS,0.5813008130081301,"0
20
40
60
No. Communications 0.2 0.4 0.6 0.8"
EXPERIMENTS,0.5853658536585366,Global Val. F1 Score
EXPERIMENTS,0.5894308943089431,Reddit
EXPERIMENTS,0.5934959349593496,"PSGD-PA
GGS
LLCG (d)"
EXPERIMENTS,0.5975609756097561,"0
20
40
No. Communications 0.8 1.0 1.2 1.4"
EXPERIMENTS,0.6016260162601627,Global Train Loss Arxiv
EXPERIMENTS,0.6056910569105691,"PSGD-PA
GGS
LLCG (e)"
EXPERIMENTS,0.6097560975609756,"0
20
40
60
No. Communications 0 1 2 3"
EXPERIMENTS,0.6138211382113821,Global Train Loss
EXPERIMENTS,0.6178861788617886,Reddit
EXPERIMENTS,0.6219512195121951,"PSGD-PA
GGS
LLCG (f)"
EXPERIMENTS,0.6260162601626016,"108
1010"
EXPERIMENTS,0.6300813008130082,Size of Comm. in Bytes (Log Scale) 0.4 0.5 0.6 0.7
EXPERIMENTS,0.6341463414634146,Global Val. F1 Score Arxiv
EXPERIMENTS,0.6382113821138211,"PSGD-PA
GGS
LLCG (g)"
EXPERIMENTS,0.6422764227642277,"108
1010"
EXPERIMENTS,0.6463414634146342,Size of Comm. in Bytes (Log Scale) 0.0 0.5
EXPERIMENTS,0.6504065040650406,Global Val. F1 Score
EXPERIMENTS,0.6544715447154471,Reddit
EXPERIMENTS,0.6585365853658537,"(h)
Figure 4: Comparing LLCG against PSGD-PA and GGS on real-world datasets. We show the global
validation score in terms of the number of communications in (a,b,c,d), the training loss per round of
communications in (e,f), and the global validation score per bytes of exchanged data in (g,h)."
EXPERIMENTS,0.6626016260162602,"Table 1: Comparison of performance and the average Megabytes of node representation/feature
communicated per round on various datasets."
EXPERIMENTS,0.6666666666666666,"Method
No.
Comm.
GCN / SAGE
GAT
APPNP
Performance
Avg. MB
Performance
Avg. MB
Performance
Avg. MB"
EXPERIMENTS,0.6707317073170732,"Flickr
(F1-score)"
EXPERIMENTS,0.6747967479674797,"PSGD-PA
50
49.08±0.27
12.57
51.56±0.28
4.24
50.81±0.48
8.40
GGS
51.22±0.13
1849.32
52.41±0.29
1895.61
51.33±0.33
1897.82
LLCG
50.38±0.20
12.57
52.01±0.33
4.24
51.15±0.25
8.40"
EXPERIMENTS,0.6788617886178862,"OGB-Proteins
(ROC-AUC)"
EXPERIMENTS,0.6829268292682927,"PSGD-PA
100
72.85±0.70
6.20
64.95±1.01
3.14
71.10±0.79
7.31
GGS
74.78±0.36
922.42
68.11±0.60
912.79
71.29±0.31
917.20
LLCG
73.92±0.45
6.20
67.62±0.58
3.14
71.18±0.43
7.31"
EXPERIMENTS,0.6869918699186992,"OGB-Arxiv
(F1-score)"
EXPERIMENTS,0.6910569105691057,"PSGD-PA
100
69.43±0.21
3.55
69.88±0.18
3.59
68.48±0.17
7.71
GGS
70.51±0.26
3391.03
70.82±0.23
3396.79
69.01±0.10
3394.33
LLCG
70.21±0.13
3.55
70.58±0.37
3.59
68.73±0.29
7.71"
EXPERIMENTS,0.6951219512195121,"Reddit
(F1-score)"
EXPERIMENTS,0.6991869918699187,"PSGD-PA
75
71.17±1.06
14.83
70.57±1.24
7.48
83.48±0.81
11.63
GGS
94.77±0.20
3798.81
95.03±0.48
3805.28
95.23±0.22
3770.46
LLCG
94.67±0.15
14.83
94.73±0.23
7.48
94.64±0.17
11.63"
EXPERIMENTS,0.7032520325203252,"Note that we choose GGS as a reasonable representation for most existing proposals (Md et al., 2021;
Zheng et al., 2020; Tripathy et al., 2020) for distributed GNN training, since these methods have very
close communication cost and also require a large cluster of machines to truly show their performance
improvement. We also use PSGD-PA as a lower bound for communication size, which is widely
used in traditional distributed training and similar to the one used in (Angerd et al., 2020; Jiang &
Rumi, 2021). However, we did not speciﬁcally include these methods in our results since we could
not reproduce their results in our settings. Please refer to Appendix A for a detailed description of
implementation, hardware speciﬁcation and link to our source code.
Datasets and evaluation metric.
We compare LLCG and other baselines on real-world semi-
supervised node classiﬁcation datasets, details of which are summarized in Table 2 in the Appendix.
The input graphs are splitted into multiple subgraphs using METIS before training, then the same
set of subgraphs are used for all baselines. For training, we use neighborhood sampling (Hamilton
et al., 2017) with 10 neighbors sampled per node and ρ = 1.1 for LLCG. For a fair comparison, we
chose the base local update step K such that LLCG has the same number of local update steps as
PSGD-PA. During evaluation, we use full-batch without sampling, and report the performance on
the full graph using AUC ROC and F1 Micro as the evaluation metric. Unless otherwise stated, we
conduct each experiment ﬁve times and report the mean and standard deviation."
PRIMARY RESULTS,0.7073170731707317,"5.1
PRIMARY RESULTS"
PRIMARY RESULTS,0.7113821138211383,"In this section, we compare our proposed LLCG algorithm with baselines on four datasets. Due to
space limitations we defer the detailed discussion on additional datasets to the Appendix A.4."
PRIMARY RESULTS,0.7154471544715447,"LLCG requires same number of communications.
Figure 4 (a) through 4 (d) illustrate the
validation accuracy per communication rounds on four different datasets. We run a ﬁxed number of
communication rounds and plot the global validation score (the validation score computed using the
full-graph on the server) at the end of each communication step. For PSGD-PA and GGS, the score is
calculated on the averaged model, whereas for LLCG the validation is calculated after the correction"
PRIMARY RESULTS,0.7195121951219512,Published as a conference paper at ICLR 2022
PRIMARY RESULTS,0.7235772357723578,"0
20
40
Number of Communication 0.5 0.6 0.7 0.8"
PRIMARY RESULTS,0.7276422764227642,Global Validation F1 Score Arxiv
PRIMARY RESULTS,0.7317073170731707,"K=1
K=4"
PRIMARY RESULTS,0.7357723577235772,"K=16
K=64 K=128"
PRIMARY RESULTS,0.7398373983739838,"Figure 5:
Effect of local
epoch size (K)"
PRIMARY RESULTS,0.7439024390243902,"20
40
60
Number of Communications 0.0 0.5"
PRIMARY RESULTS,0.7479674796747967,Global Val. F1 Score
PRIMARY RESULTS,0.7520325203252033,Reddit
PRIMARY RESULTS,0.7560975609756098,"w/o sampling   ,S=1
w/ sampling 20%,S=1
w/ sampling 5%, S=1
w/ sampling 5%, S=2"
PRIMARY RESULTS,0.7601626016260162,"20
40
60
Number of Communications 0.5 0.6 0.7"
PRIMARY RESULTS,0.7642276422764228,Global Val. F1 Score Arxiv
PRIMARY RESULTS,0.7682926829268293,"w/o sampling   ,S=1
w/ sampling 20%,S=1
w/ sampling 5%, S=1
w/ sampling 5%, S=4"
PRIMARY RESULTS,0.7723577235772358,"Figure 6: Effect of sampling on local machine and num-
ber of correction steps on the server"
PRIMARY RESULTS,0.7764227642276422,"step. It can be seen that PSGD-PA suffers from performance drop compared to other two methods,
due to the residual error we discussed in Section 4, while both GGS and LLCG perform well and
can achieve the expected accuracy. Note that the performance drop of PSGD-PA can vary across
different datasets; in some cases such as Reddit, PSGD-PA can signiﬁcantly hurt the accuracy,
while on other datasets the gap is smaller. Nevertheless, LLCG can always close the gap between
PSGD-PA and GGS with minimal overhead.
LLCG convergences as fast as GGS. To represent the effect of communication on the real-time
convergence, in Figure 4 (e) and 4 (f), we plot the global training loss (training loss computed on the
full-graph on the server) after each communication round. Similar to the accuracy score, the training
loss is also computed on the server averaged (and corrected, in case of LLCG) global model. These
results clearly indicate that LLCG can improve the convergence over PSGD-PA, while it shows a
similar convergence speed to GGS.
LLCG exchanges data as little as PSGD-PA. Figure 4 (g) and 4 (h) show the relation between
global validation accuracy with the average size (volume) of communication in bytes. As expected,
this ﬁgure clearly shows the effectiveness of LLCG . On the one hand, LLCG has a similar amount
of communication volume as PSGD-PA but can achieve a higher accuracy. On the other hand,
LLCG requires signiﬁcantly less amount of communication volume than GGS to achieve the same
accuracy, which leads to slower training time in real world settings.
LLCG works with various GNN models and aggregations.
We evaluate four popular GNN
models, used in recent graph learning literature: GCN Kipf & Welling (2017), SAGE Hamilton et al.
(2017), GAT Velickovic et al. (2018) and APPNP Klicpera et al. (2019). In Table 1, we summarize
the test score and average communication size (in MB) on different datasets for a ﬁxed number of
communication rounds. Note that we only include the results for the aggregation methods (GCN
or SAGE) that have higher accuracy for the speciﬁc datasets, details of which can be found in
Appendix A.2. As shown here, LLCG can consistently improve the test accuracy for all different
models compared to PSGD-PA, while the communication size is signiﬁcantly lower than GGS, since
LLCG only needs to exchange the model parameters.
Effect of local epoch size.
Figure 5 compares the effect of various values of local epoch size
K ∈{1, 4, 16, 64, 128} for ﬁxed ρ and S on the OGB-Arxiv dataset. When using fully synchronous
with K = 1, the model suffers from very slow convergence and needs more communications. Further
increasing the K to larger values can speed up the training; however, we found a diminishing return
point for K > 128 in this dataset and extremely large K in general.
Effect of sampling in local machines. In Figure 6, we report the validation scores per round of
communication to compare the effect of neighborhood sampling at local machines. We can observe
that when the neighborhood sampling size is reasonably large (i.e., 20%), the performance is very
similar to full neighborhood training. However, reducing the neighbor sampling ratio to 5% could
result in a larger local stochastic gradient bias σ2
bias, which requires using more correction steps (S)."
CONCLUDING REMARKS,0.7804878048780488,"6
CONCLUDING REMARKS"
CONCLUDING REMARKS,0.7845528455284553,"In this paper, we propose a novel distributed algorithm for training Graph Neural Networks (GNNs).
We theoretically analyze various GNN models and discover that, unlike the traditional deep neural
networks, due to inherent data samples dependency in GNNs, naively applying periodic parameter
averaging leads to a residual error and current solutions to this issue impose huge communication
overheads. Instead, our proposal tackles these problems by applying correction on top of locally
learned models, to infuse the global structure of the graph back into the network and avoid any costly
communication. In addition, through extensive empirical analysis, we support our theoretical ﬁndings
and demonstrate that LLCG can achieve high accuracy without additional communication costs."
CONCLUDING REMARKS,0.7886178861788617,Published as a conference paper at ICLR 2022
CONCLUDING REMARKS,0.7926829268292683,ACKNOWLEDGEMENTS
CONCLUDING REMARKS,0.7967479674796748,"This work was supported in part by CRISP, one of six centers in JUMP, a Semiconductor Research
Corporation (SRC) program sponsored by DARPA and NSF grants 1909004, 1714389, 1912495,
1629915, 1629129, 1763681, 2008398."
REPRODUCIBILITY STATEMENT,0.8008130081300813,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.8048780487804879,"We provide a GitHub repository in Appendix A including all code and scripts used in our ex-
perimental studies. This repository includes a README.md ﬁle, explaining how to install and
prepare the code and required packages.
Detailed instruction on how to use the partitioning
scripts is provided for various datasets. In addition, we provide several conﬁguration ﬁles (un-
der scripts/configs) folder for different hyper-parameters on each individual dataset, and a
general script (scripts/run-config.py) to run and reproduce the results with these conﬁgura-
tions. Details of various models and parameters used in our evaluation studies can also be found in
Appendix A."
REFERENCES,0.8089430894308943,REFERENCES
REFERENCES,0.8130081300813008,"Alexandra Angerd, Keshav Balasubramanian, and Murali Annavaram. Distributed training of graph
convolutional networks using subgraph approximation. arXiv preprint arXiv:2012.04930, 2020."
REFERENCES,0.8170731707317073,"Paolo Boldi and Sebastiano Vigna. The webgraph framework I: compression techniques. In Proceed-
ings of the 13th international conference on World Wide Web, WWW 2004, New York, NY, USA,
May 17-20, 2004, pp. 595–602. ACM, 2004. doi: 10.1145/988672.988752."
REFERENCES,0.8211382113821138,"Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir
Ivanov, Chloé Kiddon, Jakub Konecný, Stefano Mazzocchi, Brendan McMahan, Timon Van
Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning
at scale: System design. In Proceedings of Machine Learning and Systems 2019, MLSys 2019,
Stanford, CA, USA, March 31 - April 2, 2019. mlsys.org, 2019."
REFERENCES,0.8252032520325203,"Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 941–949. PMLR, 2018."
REFERENCES,0.8292682926829268,"Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019,
Anchorage, AK, USA, August 4-8, 2019, pp. 257–266. ACM, 2019. doi: 10.1145/3292500.3330925."
REFERENCES,0.8333333333333334,"Weilin Cong, Rana Forsati, Mahmut T. Kandemir, and Mehrdad Mahdavi. Minimal variance sampling
with provable guarantees for fast training of graph neural networks. In KDD ’20: The 26th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August
23-27, 2020, pp. 1393–1403. ACM, 2020."
REFERENCES,0.8373983739837398,"Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On the importance of sampling in learning
graph convolutional networks. arXiv preprint arXiv:2103.02696, 2021."
REFERENCES,0.8414634146341463,"Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. Large
scale distributed deep networks. In Advances in Neural Information Processing Systems 25: 26th
Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting
held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1232–1240, 2012."
REFERENCES,0.8455284552845529,"Songgaojun Deng, Huzefa Rangwala, and Yue Ning. Learning dynamic context graphs for predicting
social events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 1007–1016.
ACM, 2019. doi: 10.1145/3292500.3330919."
REFERENCES,0.8495934959349594,Published as a conference paper at ICLR 2022
REFERENCES,0.8536585365853658,"Kien Do, Truyen Tran, and Svetha Venkatesh. Graph transformation policy network for chemical
reaction prediction. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp.
750–760. ACM, 2019. doi: 10.1145/3292500.3330958."
REFERENCES,0.8577235772357723,"Faezeh Faez, Yassaman Ommi, Mahdieh Soleymani Baghshah, and Hamid R Rabiee. Deep graph
generators: A survey. IEEE Access, 9:106675–106702, 2021."
REFERENCES,0.8617886178861789,"Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
CA, USA, pp. 6530–6539, 2017."
REFERENCES,0.8658536585365854,"Mahsa Ghorbani, Anees Kazi, Mahdieh Soleymani Baghshah, Hamid R Rabiee, and Nassir Navab.
Ra-gcn: Graph convolutional network for disease prediction problems with imbalanced data.
Medical Image Analysis, 75:102272, 2022."
REFERENCES,0.8699186991869918,"William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024–1034,
2017."
REFERENCES,0.8739837398373984,"Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018."
REFERENCES,0.8780487804878049,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec.
Open graph benchmark: Datasets for machine learning on graphs.
In
Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.8821138211382114,"Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A
large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021."
REFERENCES,0.8861788617886179,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and
Conference Proceedings, pp. 448–456. JMLR.org, 2015."
REFERENCES,0.8902439024390244,"Peng Jiang and Masuma Akter Rumi. Communication-efﬁcient sampling for distributed training of
graph convolutional networks. arXiv preprint arXiv:2101.07706, 2021."
REFERENCES,0.8943089430894309,"Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.
In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017."
REFERENCES,0.8983739837398373,"Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019."
REFERENCES,0.9024390243902439,"Jakub Koneˇcn`y, H Brendan McMahan, X Yu Felix, Ananda Theertha Suresh, Dave Bacon, and Peter
Richtárik. Federated learning: Strategies for improving communication efﬁciency. 2018."
REFERENCES,0.9065040650406504,"Guohao Li, Matthias Müller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep
as cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
Korea (South), October 27 - November 2, 2019, pp. 9266–9275. IEEE, 2019. doi: 10.1109/ICCV.
2019.00936."
REFERENCES,0.9105691056910569,"Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff
Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating
data parallel training. arXiv preprint arXiv:2006.15704, 2020a."
REFERENCES,0.9146341463414634,Published as a conference paper at ICLR 2022
REFERENCES,0.9186991869918699,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020b."
REFERENCES,0.9227642276422764,"Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Mahdi Soltanolkotabi,
Xiang Ren, and Salman Avestimehr. Fednlp: A research platform for federated learning in natural
language processing. arXiv preprint arXiv:2104.08815, 2021."
REFERENCES,0.926829268292683,"Vasimuddin Md, Sanchit Misra, Guixiang Ma, Ramanarayan Mohanty, Evangelos Georganas, Alexan-
der Heinecke, Dhiraj Kalamkar, Nesreen K Ahmed, and Sasikanth Avancha. Distgnn: Scalable
distributed training for large-scale graph neural networks. arXiv preprint arXiv:2104.06700, 2021."
REFERENCES,0.9308943089430894,"Morteza Ramezani, Weilin Cong, Mehrdad Mahdavi, Anand Sivasubramaniam, and Mahmut Kan-
demir. Gcn meets gpu: Decoupling “when to sample” from “how to sample”. Advances in Neural
Information Processing Systems, 33, 2020."
REFERENCES,0.9349593495934959,"Simone Scardapane, Indro Spinelli, and Paolo Di Lorenzo. Distributed graph convolutional networks.
arXiv preprint arXiv:2007.06281, 2020."
REFERENCES,0.9390243902439024,"Hyejin Shin, Sungwook Kim, Junbum Shin, and Xiaokui Xiao. Privacy enhanced matrix factorization
for recommendation with local differential privacy. IEEE Transactions on Knowledge and Data
Engineering, 30(9):1770–1782, 2018."
REFERENCES,0.943089430894309,"Sebastian U. Stich.
Local SGD converges fast and communicates little.
In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net, 2019."
REFERENCES,0.9471544715447154,"Chuxiong Sun and Guoshi Wu. Adaptive graph diffusion networks with hop-wise attention. arXiv
preprint arXiv:2012.15024, 2020."
REFERENCES,0.9512195121951219,"Chuxiong Sun and Guoshi Wu. Scalable and adaptive graph neural networks with self-label-enhanced
training. arXiv preprint arXiv:2104.09376, 2021."
REFERENCES,0.9552845528455285,"Alok Tripathy, Katherine Yelick, and Aydin Buluc. Reducing communication in graph neural network
training. arXiv preprint arXiv:2005.03300, 2020."
REFERENCES,0.959349593495935,"Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018."
REFERENCES,0.9634146341463414,"Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. Billion-scale
commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018,
London, UK, August 19-23, 2018, pp. 839–848. ACM, 2018. doi: 10.1145/3219819.3219869."
REFERENCES,0.967479674796748,"Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. Fedgnn: Federated graph
neural network for privacy-preserving recommendation. arXiv preprint arXiv:2102.04925, 2021."
REFERENCES,0.9715447154471545,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018,
London, UK, August 19-23, 2018, pp. 974–983. ACM, 2018. doi: 10.1145/3219819.3219890."
REFERENCES,0.975609756097561,"Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 5693–5700, 2019."
REFERENCES,0.9796747967479674,"Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna.
Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020."
REFERENCES,0.983739837398374,Published as a conference paper at ICLR 2022
REFERENCES,0.9878048780487805,"Qingru Zhang, David Wipf, Quan Gan, and Le Song. A biased graph neural network sampler with
near-optimal regret. arXiv preprint arXiv:2103.01089, 2021."
REFERENCES,0.991869918699187,"Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan Gan, Zheng Zhang,
and George Karypis. Distdgl: Distributed graph neural network training for billion-scale graphs.
arXiv preprint arXiv:2010.05337, 2020."
REFERENCES,0.9959349593495935,"Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent
importance sampling for training deep and large graph convolutional networks. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11247–11256,
2019."
