Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007204610951008645,"We consider the overfitting behavior of minimum norm interpolating solutions
of Gaussian kernel ridge regression (i.e. kernel ridgeless regression), when the
bandwidth or input dimension varies with the sample size. For fixed dimensions,
we show that even with varying or tuned bandwidth, the ridgeless solution is
never consistent and, at least with large enough noise, always worse than the null
predictor. For increasing dimension, we give a generic characterization of the
overfitting behavior for any scaling of the dimension with sample size. We use
this to provide the first example of benign overfitting using the Gaussian kernel
with sub-polynomial scaling dimension. All our results are under the Gaussian
universality ansatz and the (non-rigorous) risk predictions in terms of the kernel
eigenstructure."
INTRODUCTION,0.001440922190201729,"1
Introduction"
INTRODUCTION,0.002161383285302594,"A central question in learning theory is how learning algorithms can generalize well even when
returning models that perfectly fit (i.e. interpolate) noisy training data. This phenomenon was
observed empirically by Zhang et al. [63], and does not align with the traditional belief from
statistical learning theory that overfitting to noise leads to poor generalization. Consequently, it
attracted significant interest in recent years, and there has been much effort to understand the
overfitting behavior of linear models, kernel methods, and neural networks."
INTRODUCTION,0.002881844380403458,"In this paper, we study the overfitting behavior of Kernel Ridge Regression (KRR) with Gaussian
kernel, namely, the behavior of the limiting test error when training on noisy data as the number
of samples tends to infinity by insisting on interpolation (achieving zero training error). When the
input dimension and bandwidth are fixed, the overfitting behavior is known to be “catastrophic” [37],
i.e. for any nonzero noise, the test risk tends to infinity as the sample size increases. However,
this is not how Gaussian Kernel Ridge Regression is typically used in practice. In fixed dimension,
the bandwidth is tuned, that is decreased, when the sample size increases [51, 19]. Additionally, it
makes sense to study the behaviour when the input dimension increases with sample size (as in, e.g.
linear models with proportional scaling [42]). This could be because when more data is available,
more input features are used, even with a kernel; because as more resources are available we scale
up both the input dimension and amount of data used; or to capture the fact that very large scale
problems typically involve both more samples and higher input dimension. But unlike with linear
models, where the dimension must scale linearly with the number of samples in order to allow for
interpolation, when a kernel is used we can study the behaviour even when the input dimensionality
increases much slower, and ask how slowly it could increase without catastrophic overfitting. Previous
studies on kernel ridgeless regression considered polynomial increasing dimension (i.e. dimension
∝sample-sizea, for 0 < a ≤1) [4, 64, 24, 38, 40], but not subpolynomial scaling."
INTRODUCTION,0.0036023054755043226,"We aim to provide a more comprehensive picture of overfitting with Gaussian KRR by studying
the overfitting behavior with varying bandwidth or with arbitrarily varying dimension, including"
INTRODUCTION,0.004322766570605188,"sub-polynomially. In particular, we show that for fixed dimension, even with varying bandwidth,
the interpolation learning is never consistent and generally not better than the null predictor (either
the test error tends to infinity or is finite but it is almost always not better than the null predictor).
For increasing dimension, we give an upper and lower bound on the test risk for any scaling of the
dimension with sample size, which indicates in many cases whether the overfitting is catastrophic
(test error tends to infinity), tempered (test error tends to a constant), or benign (consistent). Our
result agrees with the polynomial scaling of the dimension with sample size, showing tempered
overfitting for an exponent that is a reciprocal of an integer and benign overfitting for any other
exponent [4, 64]. Moreover, our result goes further, as we show the first example of sub-polynomially
scaling dimension that achieves benign overfitting for the Gaussian kernel. Additionally, we show that
a class of dot-product kernels on the sphere is inconsistent when the dimension scales logarithmically
with sample size. All our results are under the Gaussian universality ansatz and the non-rigorous but
well-established risk predictions in terms of the kernel eigenstructure [50, 66, 8, 57]."
INTRODUCTION,0.005043227665706052,Related work
INTRODUCTION,0.005763688760806916,"The test performance of overfitting models has been extensively studied for linear regression [27, 6,
3, 43, 45, 16, 32, 58, 53, 65, 30, 54, 13, 2, 49, 25], linear classification [12, 55, 10, 44, 42, 49, 36, 52,
56, 18], neural networks [20, 21, 11, 34, 60, 61, 39, 33, 23, 31, 29], and kernel methods. Below we
focus on overfitting in kernel ridge regression."
INTRODUCTION,0.006484149855907781,"Overfitting in fixed dimension with fixed kernel.
In Mallinar et al. [37], the authors show
that the minimum norm interpolating solution for Gaussian kernel with fixed bandwidth overfits
catastrophically. In [15, 4, 64], the authors derive bounds on the test risk of minimum norm interpolant
with a fixed kernel under various assumptions. Our result for fixed dimension will only apply to the
Gaussian kernel, but it allows for any varying or adaptively chosen bandwidth."
INTRODUCTION,0.007204610951008645,"Inconsistency of Kernel Ridge Regression.
The case of varying bandwidth has been considered
in Beaglehole et al. [5], Rakhlin and Zhai [47], Haas et al. [26]. In Beaglehole et al. [5], the authors
show that there exists a specific data distribution for which the minimum norm interpolanting solution
for a particular set of translation invariant kernels is not consistent. In Rakhlin and Zhai [47], the
authors show that for input distributions on the unit ball, the Laplace kernel is inconsistent, even with
varying bandwidth. In Haas et al. [26], the authors show that under different assumptions on the data
distribution, for a general class of (potentially varying) kernels in fixed dimension, any differentiable
function that overfits the data and is not much different from the minimum norm interpolant is
inconsistent. All of these works only consider whether we can achieve consistency. None of these
results apply in the case of data distributions that we are considering, and even if the predictor is
not consistent, we ask how bad is it by comparing it to the null predictor and whether it might be
tempered."
INTRODUCTION,0.00792507204610951,"Overfitting in increasing dimensions.
Many papers have studied the setup where the dimension
increases with sample size [4, 64, 66, 38, 40, 28, 59], in particular when the dimension is a function
of sample size (or vice versa), but they all consider only the case of a polynomial scaling of dimension
and sample size. It was shown that in this case the minimum norm interpolating solution of dot
product kernels on the sphere can be benign, depending on if the exponent is not an integer. We
generalize these results to any scaling of the dimension and sample size. Our results recover the
existing results in the case of polynomially scaling dimension and show benign overfitting in a certain
sub-polynomial scaling. Having sub-polynomimal scaling of the dimension allows us to expand
the set of possible target functions from only polynomials of a bounded degree, as in the case of
polynomial scaling of dimension [4, 64, 66, 38, 40], to, in our case of sub-polynomially increasing
dimension, polynomials of any degree and even non-polynomial functions."
PROBLEM FORMULATION AND ASSUMPTIONS,0.008645533141210375,"2
Problem formulation and assumptions"
PROBLEM FORMULATION AND ASSUMPTIONS,0.009365994236311239,"Kernel ridge(less) regression and the Gaussian kernel.
Let D be an unknown distribution over
X × Y ⊆Rd × R and let {(xi, yi)}m
i=1 ∼Dm be a dataset consisting of m samples. For simplicity,
we will assume that the distribution of the target is given by a target function f ∗of the input x ∈X"
PROBLEM FORMULATION AND ASSUMPTIONS,0.010086455331412104,"with zero mean independent noise ξ with variance σ2, that is y ∼f ∗(x) + ξ. We note that our results
can be extended to a distribution agnostic setting, as analyzed by Zhou et al. [66]."
PROBLEM FORMULATION AND ASSUMPTIONS,0.010806916426512969,"Let K : X × X →R be a positive semi-definite kernel function. Let ∥f∥K be the norm of f in the
RKHS HK corresponding to K. For a predictor f, let R(f) and ˆR(f) be the test and training risk of
f,"
PROBLEM FORMULATION AND ASSUMPTIONS,0.011527377521613832,"ˆR(f) = 1 m m
X"
PROBLEM FORMULATION AND ASSUMPTIONS,0.012247838616714697,"i=1
(f(xi) −yi)2 and R(f) = ED

(f(x) −y)2
."
PROBLEM FORMULATION AND ASSUMPTIONS,0.012968299711815562,"Two important risks to consider are the risk of the null predictor, f ≡0, which we will denote by
R(0) = ED[(y)2], and the Bayes (or irreducible) risk, which we will denote σ2 or R(f ∗). Using this
notation, the risk of the null predictor is R(0) = σ2 + ED[(y)2]. Bayes risk represents the minimum
possible risk that can be achieved by any predictor. For a regularization parameter δ, the regularized
ridge solution ˆfδ is given by"
PROBLEM FORMULATION AND ASSUMPTIONS,0.013688760806916427,ˆfδ = argminf∈HK ˆR(f) + δ
PROBLEM FORMULATION AND ASSUMPTIONS,0.01440922190201729,"m∥f∥2
K."
PROBLEM FORMULATION AND ASSUMPTIONS,0.015129682997118156,"We are interested in the minimum norm interpolating (ridgeless) solution ˆf0 = limδ→0+ ˆfδ, namely"
PROBLEM FORMULATION AND ASSUMPTIONS,0.01585014409221902,"ˆf0 = argmin ˆ
R(f)=0;f∈HK∥f∥2
K."
PROBLEM FORMULATION AND ASSUMPTIONS,0.016570605187319884,"We will focus on the Gaussian kernel, which is given by K(x1, x2) = exp

−∥x1−x2∥2
2
τ 2
m"
PROBLEM FORMULATION AND ASSUMPTIONS,0.01729106628242075,"
, where m
is the sample size and τm is a predetermined bandwidth parameter that can vary with sample size.
The Gaussian kernel is widely used and achieves good error rates for a variety of learning tasks
[51, 19]. Gaussian KRR achieves optimal convergence to the best possible (Bayes) error for learning
any function in a Besov space of high enough order (essentially bounded and twice differentiable in
the weak sense) under very mild assumptions on the distribution of the input and target X × Y [19].
For ridge regression with the Gaussian kernel and under a standard data distribution assumption, the
minimum distance between samples decreases with sample size so it makes sense to also decrease τm.
Additionally, decreasing τm with sample size helps to achieve good convergence rates theoretically
[19]."
PROBLEM FORMULATION AND ASSUMPTIONS,0.018011527377521614,"Main question.
We will consider the problem of learning using the minimum norm interpolating
solution ˆf0 of KRR. We want to understand the limiting behavior of test risk R( ˆf0) as the sample
size increases m →∞, that is limm→∞R( ˆf0). It suffices to understand lim supm→∞R( ˆf0) and
lim infm→∞R( ˆf0) and this way we do not assume the existence of the limit. In this work, we use
the taxonomy of benign, tempered, and catastrophic overfitting from Mallinar et al. [37], which
indicates whether limm→∞R( ˆf0) is the Bayes (optimal) error, a non-optimal but constant error, or
infinity. Note that in this taxonomy, the null predictor can be classified as tempered. Therefore, we
will compare the limiting risk to the risk of the null predictor R(0) in order to understand whether the
performance of the interpolating solution is non-trivial."
PROBLEM FORMULATION AND ASSUMPTIONS,0.018731988472622477,"Main tool: Eigenframework and a closed form of the test risk.
Our main tool will be the closed
form of the test risk predicted by the eigenframwork [50]. Under the eigenframework, we can write
down the closed form of the test risk using Mercer’s theorem decomposition of a kernel function K."
PROBLEM FORMULATION AND ASSUMPTIONS,0.019452449567723344,"Given a positive semi-definite kernel function K : X × X →R, we can decompose it as"
PROBLEM FORMULATION AND ASSUMPTIONS,0.020172910662824207,"K(x1, x2) = ∞
X"
PROBLEM FORMULATION AND ASSUMPTIONS,0.02089337175792507,"k=1
λkϕk(x1)ϕk(x2),
(1)"
PROBLEM FORMULATION AND ASSUMPTIONS,0.021613832853025938,"where λk and ϕk are the eigenvalues and eigenfunctions of the integral operator associated to K. The
eigenfunctions {ϕk} are an orthonormal basis of L2
DX (X), where DX is the marginal distribution of
X. We denote the Bayes optimal target function by f ∗, and expand it in the kernel basis {ϕi}∞
i=1 as
f ∗(x) = P∞
i=1 βiϕi(x). To state the close form of the test risk we will introduce a few quantities. Let
effective regularization, κδ, be the solution to P∞
i=1
λi
λi+κδ + δ"
PROBLEM FORMULATION AND ASSUMPTIONS,0.0223342939481268,"κδ = m. Furthermore, let Li,δ =
λi
λi+κδ"
PROBLEM FORMULATION AND ASSUMPTIONS,0.023054755043227664,"and Eδ =
m
m−P∞
i=1 L2
i,δ . Then, the predicted risk, i.e. the predicted closed form of the test risk of ˆfδ,
is given by"
PROBLEM FORMULATION AND ASSUMPTIONS,0.02377521613832853,"˜R( ˆfδ) = Eδ ∞
X"
PROBLEM FORMULATION AND ASSUMPTIONS,0.024495677233429394,"i=1
(1 −Li,δ)2 β2
i + σ2
! ,
(2)"
PROBLEM FORMULATION AND ASSUMPTIONS,0.02521613832853026,"where σ2 is the Bayes error of D. Equation (2) was initially heuristically derived using the replica
method or continuous approximations to the learning curves inspired by the Gaussian process
literature [7]. In [50], it is dervied using a conservation law. Note that [66] shows that the predicted
closed form of the test risk from [50] extends to general target distributions. There is strong evidence
that the predicted risk is a good estimate of the true test risk, namely R( ˆfδ) ≈˜R( ˆfδ). Indeed, a
number of works use the predicted risk closed form to estimate the test risk of KRR [58, 8, 66]. The
following assumption on Gaussian design ansatz is used by all of these works.
Assumption 1 (Gaussian design ansatz, cf. Zhou et al. [66]). When sampling (x, ·) ∼D, we have that
the Gaussian universality holds for the eigenfunctions in the sense that the expected risk is unchanged
if we replace ϕ with ˜ϕ, where ˜ϕ is Gaussian with appropriate parameters, i.e. ˜ϕ ∼N(0, diag{λi})."
PROBLEM FORMULATION AND ASSUMPTIONS,0.025936599423631124,"This assumption appears to hold for real datasets as well, namely, the predictions computed for
Gaussian design agree well with the experiments on kernel regression using real data [8, 50, 57].
As discussed in Zhou et al. [66], under this assumption, the equivalence R( ˆfδ) ≈˜R( ˆfδ) holds in a
few ways. First, in an appropriate asymptotic limit in which the sample size m and the number of
eigenmodes in a given eigenvalue grow proportionally, the equivalence holds [27, 1]. Second, if the
eigenstructure of the task is fixed, the error between the two can be bounded by a decaying function
of m [14]. Finally, various numerical experiments show that the error between the two is small even
for a small sample size m [8, 50]. Specifically, Canatar et al. [8] (see Figure 5 there) gives empirical
evidence that the predicted risk closely approximates the true risk for the Gaussian kernel with data
uniform on a sphere, which is the setting that we consider in some of our results."
PROBLEM FORMULATION AND ASSUMPTIONS,0.026657060518731988,"There has been some recent progress in bounding the error between R( ˆfδ) and ˜R( ˆfδ) unconditionally.
Misiakiewicz and Saeed [41] shows that the error will tend to zero if the dimension d grows fast
enough with the sample size. Additionally, they provide strong empirical evidence that the predicted
risk is close to the test risk for a real dataset (MNIST) and Gaussian kernel, see Figure 1 in [41]."
PROBLEM FORMULATION AND ASSUMPTIONS,0.027377521613832854,"Formally, we will prove results about the predicted risk ˜R( ˆfδ), but as previously presented evidence
suggests, treating R( ˆfδ) ≈˜R( ˆfδ) as equivalence is sufficient for understanding the behavior of KRR."
PROBLEM FORMULATION AND ASSUMPTIONS,0.028097982708933718,"We note that using the eigenframework might introduce restrictions for which kernels some of our
results apply, as concurrent work showed that the eigenframework prediction might not hold for the
NTK in fixed dimension [4, 15]. Our two main results concern the Gaussian kernel, for which we
described ample empirical evidence that the eigenlearning predictions hold [8, 41]. Understanding
the limitations of the eigenframework is an important future research direction."
PROBLEM FORMULATION AND ASSUMPTIONS,0.02881844380403458,"3
Fixed dimension: Gaussian kernel with varying bandwidth"
PROBLEM FORMULATION AND ASSUMPTIONS,0.029538904899135448,"We will assume that the source distribution is uniform on a d dimensional sphere, that the target
function is square integrable, and that the target distribution is given by the target function with an
independent noise.
Assumption 2 (Target function and data distribution). Let D be the distribution over X × Y =
Sd−1 × R, such that the X marginal, denoted by DX , is Unif(Sd−1). We will assume that for a target
function f ∗∈L2
DX (Sd−1), the marginal Y distribution is given by y ∼f ∗(x) + ξ, where ξ has
mean zero and variance σ2 > 0. We write f ∗= P"
PROBLEM FORMULATION AND ASSUMPTIONS,0.03025936599423631,"i βiϕi, where {ϕi} is the L2
DX (Sd−1) eigenbasis
corresponding to the kernel K (Equation (1)). If we write β = (β1, β2, . . . ), then we have that
∥β∥2
2 = EDX
 
(f ∗(x))2
. We will use the notation ∥f ∗∥2 = ∥β∥2
2."
PROBLEM FORMULATION AND ASSUMPTIONS,0.030979827089337175,"The assumption on the distribution on X is common in the literature on KRR [38, 4, 64, 22]. The
assumption that x ∼Unif
 
Sd−1
can be relaxed to a more general setting where x is uniformly
distributed on other manifolds that are diffeomorphic to the sphere using the results of Li et al. [35],
although that will not be the focus of this paper."
PROBLEM FORMULATION AND ASSUMPTIONS,0.03170028818443804,"Note that here we vary the bandwidth τm so both ˆf0 and R( ˆf0) will depend on the bandwidth τm
as well as m. We will identify three different regimes of bandwidth scaling. We will show that the
minimum norm interpolating solution exhibits either tempered or catastrophic overfitting, and we
will argue that it is almost always worse than the risk of the null predictor."
PROBLEM FORMULATION AND ASSUMPTIONS,0.032420749279538905,"Theorem 3 (Overfitting behavior of Gaussian kernel in fixed dimension). Under Assumption 2, the
following bounds hold for the predicted risk ˜R( ˆf0) of the minimum norm interpolating solution of
Gaussian KRR:"
PROBLEM FORMULATION AND ASSUMPTIONS,0.03314121037463977,"1. If τm = o(m−
1
d−1 ), then ˜
R(0) ≤lim inf m→∞˜
R( ˆ
f0) ≤lim supm→∞˜
R( ˆ
f0) < ∞.
More precisely, if τm ≤m−
1
d−1 t(m), where t(m) →0 as m →∞, then there is a scalar cd
that depends only on the dimension and m0 that depends on t(m) such that for all m > m0
we have ˜R( ˆf0) > σ2 + (1 −cdt(m)
d−1"
PROBLEM FORMULATION AND ASSUMPTIONS,0.03386167146974063,"2 )∥f ∗∥2.
2. If τm = ω(m−
1
d−1 ), then limm→∞˜
R( ˆ
f0) = ∞. Hence, for large enough m we have
˜R( ˆf0) > ˜R(0).
3. If τm = Θ(m−
1
d−1 ), then lim supm→∞R( ˆ
f0) < ∞.
Moreover, Suppose that
C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 for some constants C1 and C2, then there exist η, µ > 0 that
depend only on d, C1, and C2, such that for all m we have ˜
R( ˆ
f0) > µ∥f ∗∥2+(1+η)σ2.
Consequently, ˜
R( ˆ
f0) > ˜
R(0) as long as σ2 > 1−µ"
PROBLEM FORMULATION AND ASSUMPTIONS,0.0345821325648415,η ∥f ∗∥2.
PROBLEM FORMULATION AND ASSUMPTIONS,0.035302593659942365,"Theorem 3 shows that the minimum norm interpolating solution of Gaussian KRR cannot be consistent
when data is distributed uniformly on the sphere, even with varying or adaptively chosen bandwidth.
Additionally, in the first two modes of bandwidth change, the minimum norm interpolating solution is
never better than the null predictor. In the third case, the interpolating solution is worse than null for
noise that is not too small. This shows that even though the minimum norm interpolating predictor is
classified as tempered in the first and third cases of scaling of the bandwidth, it is still worse than the
trivial null predictor. Note that our analysis does not exclude the possibility that for τm = Θ(m
1
d−1 )
there exists small enough σ2 for which the interpolating solution is better than the null predictor.
We leave this as an open question. In Appendix A, we provide further empirical justification for
Theorem 3."
INCREASING DIMENSION,0.03602305475504323,"4
Increasing dimension"
INCREASING DIMENSION,0.03674351585014409,"For the case of increasing dimension, we consider the problem of learning a sequence of distributions
D(d) over X × Y = Rd × R given by y ∼f ∗
d (x) + ξd using a sequence of kernels K(d). Here, ξd is
independent noise with mean 0 and variance σ2 > 0. Formally, the kernel and the target function
can change with the dimension d, but we will think of it as the same kernel and target with higher
dimensional input. Furthermore, d will increase with sample size m, i.e. d = d(m) (or analogously
m will increase with d). A common assumption, which we also adopt, is that the projections of the
target function f ∗
d onto the eigenfunctions ϕ(d)
k
of the kernels K(d) are uniformly bounded [4, 64]."
INCREASING DIMENSION,0.037463976945244955,"Assumption 4 (Target function and distribution in increasing dimension). Consider learning a
sequence of target functions f ∗
d with a sequence of kernels K(d). Let the target function f ∗
d have only
Sd nonzero coefficients (where Sd can change with d), so f ∗
d = PSd
i=1 β(d)
i
ϕ(d)
i
where ϕ(d)
i
are from
Equation (1) and |β(d)
i
| ≤B, i.e. ∥β∥∞≤B, for B that is independent of d."
INCREASING DIMENSION,0.03818443804034582,"The functions that can be represented in this form depend on the number of nonzero coefficients, Sd,
and the kernel that we are using. In particular, for dot-product kernels on the sphere it includes all
polynomials of degree k ≤kd where kd is such that the multiplicities of first kd eigenfunctions are at
most Sd. If Sd grows with d, then this set will include much more general functions. See Remark 14
for further discussion."
INCREASING DIMENSION,0.03890489913544669,"First, we will consider a general kernel K since Theorem 7 and Theorem 9 will apply more generally.
Then, we will apply these results to the cases of dot-product and Gaussian kernels."
INCREASING DIMENSION,0.03962536023054755,"The multiplicities of eigenvalues will play an important role in bounding the test risk, both from
above and below, so we will introduce the following related notation."
INCREASING DIMENSION,0.040345821325648415,"Definition 5 (Lower and upper index). Let ˜λk be the k-th non-repeating eigenvalue of a kernel
K and let N(k) be its multiplicity. Let Nk = N(1) + · · · + N(k). Let m be the sample size.
Let km be defined as the maximal k such that there is less than m eigenvalues with index k, i.e.
km = max{k ∈N|N(1) + · · · + N(k) < m}. Define the lower index Lm and the upper index Um
as follows Lm = N(1) + · · · + N(km) and Um = N(1) + · · · + N(km + 1). When the dimension
changes with sample size, we sometimes denote N(k) by N(d, k)."
INCREASING DIMENSION,0.04106628242074928,"We will first state a generic bound on the test risk for any data distribution, kernel with a bounded
sum of eigenvalues, sample size, and dimension. This bound will be informative when we scale the
dimension d with sample size m, but it holds for any kernel that satisfies the following assumption.
Assumption 6 (Bounded sum of eigenvalues). Assume that the kernel K has a bounded sum of
eigenvalues, i.e. there is a constant A such that P∞
i=1 N(i)˜λi ≤A. For a sequence of kernels K(d),
assume that all such A(d) are bounded by some constant A.1"
INCREASING DIMENSION,0.04178674351585014,"This is a reasonable assumption for most dot-product kernels, as we show in Appendix C.4. It also
implicitly sets the scale of the kernel.
Theorem 7 (Test risk upper bound for kernel ridgeless regression). Let d and m be any dimension
and sample size. Define Lm, Um, km, N(i), Nl, and ˜λk as in Definition 5. Consider KRR with a
kernel K satisfying Assumption 6 for some A. Assume that for some integer l, the target function f ∗
satisfies Assumption 4 with at most Nl nonzero coefficients. Then, the predicted risk of the minimum
norm interpolating solution is bounded by the following:"
INCREASING DIMENSION,0.04250720461095101,"˜R( ˆf0) ≤

1 −Lm m"
INCREASING DIMENSION,0.043227665706051875,"−1 
1 −m Um"
INCREASING DIMENSION,0.04394812680115274,"−1
σ2
(3)"
INCREASING DIMENSION,0.0446685878962536,"+ B2

1 −Lm m"
INCREASING DIMENSION,0.045389048991354465,"−1 
1 −m Um"
INCREASING DIMENSION,0.04610951008645533,"−1 A2 m2 l
X"
INCREASING DIMENSION,0.0468299711815562,"i=1
N(i) 1 ˜λ2
i ! .
(4)"
INCREASING DIMENSION,0.04755043227665706,"Alternatively, we can bound the risk using
Pl
i=1 N(i) 1 ˜λi"
INCREASING DIMENSION,0.048270893371757925,"
instead of
Pl
i=1 N(i) 1 ˜λ2
i"
INCREASING DIMENSION,0.04899135446685879,"
(see Theo-
rem 20 in the appendix)."
INCREASING DIMENSION,0.04971181556195965,"We also establish a generic inconsistency result for any data distribution, kernel with a bounded sum
of eigenvalues, sample size, and dimension based on the upper and lower indices from Definition 5.
We will further need to assume that the eigenvalues are bounded away from zero. Similarly, this will
be useful when scaling dimension d with sample size, but it holds generally.
Assumption 8 (Lower bound on eigenvalues). Assumme that the kernel K has eigenvalues that
are not too small, i.e. there is a constant b such that maxi≤km

1
˜λi"
INCREASING DIMENSION,0.05043227665706052,"
< m−Lm"
INCREASING DIMENSION,0.051152737752161385,"b
. For a sequence of"
INCREASING DIMENSION,0.05187319884726225,"kernels K(d), assume that for the corresponding m = m(d) (since d = d(m), we can also ""invert""
the dependence) all such b(d) are bounded below by some b."
INCREASING DIMENSION,0.05259365994236311,"This assumption will hold for most dot-product kernels and we will show it for Gaussian kernel in
Appendix C.4.
Theorem 9 (Test risk lower bound for any kernel ridgeless regression). Let km and Lm be as
in Definition 5. Consider learning a target function f ∗, with some sample size m. Let K be a
kernel satisfying Assumption 6 and Assumption 8 for some A and b. Consider the minimum norm
interpolating solution of KRR (with any data distribution) with kernel K. Then, for the predicted risk
of minimum norm interpolating solution, the following lower bound holds:"
INCREASING DIMENSION,0.053314121037463975,˜R( ˆf0) > 
INCREASING DIMENSION,0.05403458213256484,"1 −

b
b + 1 2 Lm m !−1 σ2."
INCREASING DIMENSION,0.05475504322766571,"To apply Theorem 7 for varying dimension d, we would additionally require that A is uniformly
bounded for all d and kernel K(d) and also that l = l(d) changes with d such that Assumption 4 holds
with Sd = Nl(d). For dot-product kernels K(d) on the sphere, if we let K(d)(x, y) = h(d)(∥x −y∥),"
INCREASING DIMENSION,0.05547550432276657,1Note that this assumption implicitly sets the scale of the kernel.
INCREASING DIMENSION,0.056195965417867436,"we will have A = supd h(d)(0), so if h(d) does not change with d we can take A = h(0) (see
Appendix C.4 for more details). Specifically, this holds for the Gaussian kernel on the sphere with
A = 1."
INCREASING DIMENSION,0.0569164265129683,"To apply Theorem 9 to the case of increasing dimension, we would require that the bounds
P
i N(d, i)˜λi ≤A and maxi≤km

1
˜λi"
INCREASING DIMENSION,0.05763688760806916,"
< m−Lm"
INCREASING DIMENSION,0.058357348703170026,"b
hold for all d and kernels K(d). Usually, the"
INCREASING DIMENSION,0.059077809798270896,"condition maxi≤km

1
˜λi"
INCREASING DIMENSION,0.05979827089337176,"
< m−Lm"
INCREASING DIMENSION,0.06051873198847262,"b
will be satisfied for b = 1. We will show it for the two cases of
sub-polynomially scaling dimensions with a Gaussian kernel. For the polynomial scaling dimension,
it is reasonable to assume it for general dot-product kernels, as discussed in Appendix C.4."
INCREASING DIMENSION,0.061239193083573486,"Now, we will show that using Theorem 7 and Theorem 9 we can recover the behavior of the minimum
norm interpolating solution for polynomially increasing dimension [4, 64], i.e. tempered overfitting
for integer exponent and benign for non-integer exponent. Here, we will need to additionally assume
that the eigenvalue decay is not too fast.
Assumption 10 (Eigenvalue decay). The eigenvalues do not decrease too quickly, i.e. for km as in
Definition 5, we have that there is a constant c such that maxi≤km

1
˜λi"
INCREASING DIMENSION,0.06195965417867435,"
≤cN(km). For increasing"
INCREASING DIMENSION,0.06268011527377522,"dimension, we require that maxi≤km

1
˜λi"
INCREASING DIMENSION,0.06340057636887608,"
≤cN(d, km) for all m (i.e. all d, as d and m both
increase)."
INCREASING DIMENSION,0.06412103746397695,"This assumption is stronger than Assumption 8, but as we show in Appendix C.4, it is reasonable for
dot-product kernels on the sphere and even the NTK.
Corollary 11 (Dot-product kernels with polynomially increasing dimension, recovering the results of
[24, 38, 40, 4, 64]). Consider the problem of learning a sequence of target functions f ∗
d satisfying
Assumption 4 with Sd ≤Θ(d⌊α⌋) with a dot-product kernel K(x, y) = h(∥x−y∥) with h(0) = 1 on
the sphere Sd−1 (where h does not depend on d, i.e. A = 1 from Assumption 6)) that further satisfies
Assumption 10. Let dα"
INCREASING DIMENSION,0.06484149855907781,"m = Θ(1) for α ∈(0, ∞). Then the overfitting behavior of the minimum norm
interpolating solution is benign if α is not an integer and tempered if α is an integer."
INCREASING DIMENSION,0.06556195965417867,"Additionally, we will show that for d = log m, we cannot get benign overfitting, i.e. consistency with
a class of dot-product kernels on the sphere. Similarly, as in the previous corollary, this will hold for
any sequence where d = log m even only asymptotically.
Corollary 12 (Inconsistency with dot-product kernels in logarithmically scaling dimension). Let
K(d) be a sequence of dot-product kernels on Sd−1 that satisfy Assumption 8. Let the dimension d
grows with sample size as d = log2 m (i.e. m = 2d). Then, the minimum norm interpolant cannot
exhibit benign overfitting for any such sequence K(d), i.e. there exists an absolute constant η > 0
such that for all d, m, ˜R( ˆf0) > (1 + η)σ2."
INCREASING DIMENSION,0.06628242074927954,"On the other hand, using Theorem 7, we will establish the first case of sub-polynomial scaling
dimension with benign overfitting using the Gaussian kernel and data on the sphere. We will use
d = exp(√log m).
Corollary 13 (Benign overfitting with Gaussian kernel and sub-polynomial dimension). Let K be
the Gaussian kernel on the sphere Sd−1 with a fixed bandwidth, and take a sequence of dimensions d
and sample sizes m that scale as d = exp
 √log m

(in particular, we take l ∈N such that d = 22l"
INCREASING DIMENSION,0.0670028818443804,"and m = 222l with l = 1, 2, 3 . . . ). Consider learning a sequence of target functions f ∗
d as in
Assumption 4 with Sd ≤m
1
4 . Then, we have that the minimum norm interpolating solution achieves
the Bayes error in the limit (m, d) →∞. In particular, for d ≥4 and m ≥16 we have"
INCREASING DIMENSION,0.06772334293948126,"˜R( ˆf0) ≤

1 −
1
log m"
INCREASING DIMENSION,0.06844380403458213,"−1 
1 −exp

−0.89
p"
INCREASING DIMENSION,0.069164265129683,"log m
−1
σ2 + 2B2 1 m."
INCREASING DIMENSION,0.06988472622478387,"Remark 14 (Allowed target functions). The set of allowed target functions f ∗
d in Corollary 13, i.e.
with sub-polynomial scaling dimension, is strictly larger than the set of allowed target functions for
polynomially scaling dimension, as in Corollary 11 and [38, 4, 64]. In particular, for polynomially
scaling dimension dα"
INCREASING DIMENSION,0.07060518731988473,"m = Θ(1), the result holds only if the target function is a polynomial of degree at
most ⌊α⌋. On the other hand, Corollary 13 shows that sub-polynomially scaling dimension allows for
the target function to be a polynomial of arbitrary degree, as well as non-polynomial functions. In
particular, in dimension d, we can represent polynomials of degree up to Θ(log2 d)."
PROOFS OUTLINE,0.07132564841498559,"5
Proofs outline"
PROOFS OUTLINE,0.07204610951008646,"In this section, we discuss the main proof ideas. All formal proofs are provided in the appendix."
PROOFS OUTLINE,0.07276657060518732,"By Equation (2), to understand how the test risk of the minimum norm interpolating solution behaves,
it suffices to understand how the eigenvalues corresponding to the kernel K and thus the quantities
E0, Li,0 behave. In Zhou et al. [66], the authors show that E0 is bounded both above and below in"
PROOFS OUTLINE,0.07348703170028818,"terms of the effective rank of the systems of eigenvalue {λ}∞
i=1, defined by rk :="
PROOFS OUTLINE,0.07420749279538905,"P∞
i=k+1 λi"
PROOFS OUTLINE,0.07492795389048991,"λk+1
. We will
use this, along with directly bounding Li,0."
PROOFS OUTLINE,0.07564841498559077,"If K is a dot-product kernel on the sphere (such as the Gaussian kernel), we can take the eigen-
functions ϕi to be the spherical harmonics Yks, where k ≥0 and s ∈[1, N(d, k)].
Here
N(d, k) =
(2k+d−2)(k+d−3)!"
PROOFS OUTLINE,0.07636887608069164,"k!(d−2)!
is the multiplicity of the k-th spherical harmonic. All Yks for the"
PROOFS OUTLINE,0.07708933717579251,"same k will have the same eigenvalue, which we will denote ˜λk. In this case, we can write a
closed-form expression for the eigenvalues of Gaussian kernel, ˜λk, in terms of the bandwidth τm and
modified Bessel functions of the first kind Iv(x) [46] (see Appendix D). Using the closed form of
˜λi and the multiplicities of eigenvalues, we can understand how the test risk of the minimum norm
interpolating solution R( ˆf0) behaves as m →∞, which tells us its overfitting behavior."
PROOFS OUTLINE,0.07780979827089338,"Additionally, E0 appearing in Equation (2), will be informative. For example, if limm→∞E0 > 1
then the overfitting cannot be benign. The following bound using the effective rank rk holds [66]:
For k < m such that rk + k > m we have"
PROOFS OUTLINE,0.07853025936599424,"E0 ≤

1 −k m"
PROOFS OUTLINE,0.0792507204610951,"−1 
1 −
m
k + rk"
PROOFS OUTLINE,0.07997118155619597,"−1
.
(5)"
PROOFS OUTLINE,0.08069164265129683,For k ≥m it holds that
PROOFS OUTLINE,0.0814121037463977,"E0 ≥
1 1 −m"
PROOFS OUTLINE,0.08213256484149856,"k

k−m
k−m+rk"
PROOFS OUTLINE,0.08285302593659942,".
(6)"
PROOFS OUTLINE,0.08357348703170028,"Proof sketch of Theorem 3.
We will focus on the lower bounds in this case, as the result is
negative. The key elements to understanding the effective rank rk and the test risk Equation (2) is
to understand how the ratios of eigenvalues
˜λk+1"
PROOFS OUTLINE,0.08429394812680115,"˜λk
and the multiplicities N(d, k) behave. Using the
closed form of the eigenvalues of Gaussian kernel and the properties of modified Bessel functions
[46, 48], with some computation (see Theorem 28 in the appendix for the computations), we can
derive the following bounds on ratios of eigenvalues

2
τ 2
m "
PROOFS OUTLINE,0.08501440922190202,"2
 
k + d"
PROOFS OUTLINE,0.08573487031700289,"2

+

2
τ 2
m"
PROOFS OUTLINE,0.08645533141210375," <
˜λk+1 ˜λk
<"
PROOFS OUTLINE,0.08717579250720461,"
2
τ 2
m "
PROOFS OUTLINE,0.08789625360230548," 
k + d 2 −1"
PROOFS OUTLINE,0.08861671469740634,"2

+

2
τ 2
m"
PROOFS OUTLINE,0.0893371757925072," .
(7)"
PROOFS OUTLINE,0.09005763688760807,"From these bounds, we can derive tight bounds on
˜λk+j"
PROOFS OUTLINE,0.09077809798270893,"˜λk
for indices k and j using simple but long"
PROOFS OUTLINE,0.0914985590778098,"calculations (see Theorem 28 in the appendix). If k = o

1
τm"
PROOFS OUTLINE,0.09221902017291066,"
and j = o

1
τm"
PROOFS OUTLINE,0.09293948126801153,"
, then
˜λk+j"
PROOFS OUTLINE,0.0936599423631124,"˜λk
≈1. If"
PROOFS OUTLINE,0.09438040345821326,"k ≤Θ

1
τm"
PROOFS OUTLINE,0.09510086455331412,"
and j = Θ

1
τm"
PROOFS OUTLINE,0.09582132564841499,"
, then
˜λk+j"
PROOFS OUTLINE,0.09654178674351585,"˜λk
= Θ(1). If k = ω

1
τm"
PROOFS OUTLINE,0.09726224783861671,"
, then
˜λk+j"
PROOFS OUTLINE,0.09798270893371758,"˜λk
= o

1
jn

for any"
PROOFS OUTLINE,0.09870317002881844,"integer n ∈N (i.e. it deceases super-polynomialy). For N(d, l) it holds that N(d, l) = Θ(ld−2) and
Nl := N(d, 1) + · · · + N(d, l) = Θ(ld−1). Therefore if ˜l is the index such that ˜λ˜l = λl, we have
that ˜l = Θ(l
1
d−1 )."
PROOFS OUTLINE,0.0994236311239193,"For τm ≥ω(m−
1
d−1 ), we will take l = (1 +
1
√m)m and show that rl = o(m). Then, the bound in"
PROOFS OUTLINE,0.10014409221902017,"Equation (6) will imply that E0 ≥1 + √m, so from Equation (2), ˜R( ˆf0) > E0σ2 = √mσ2. Note"
PROOFS OUTLINE,0.10086455331412104,"that for l = (1 +
1
√m)m, we have that ˜l = Θ(m
1
d−1 ) = ω

1
τm"
PROOFS OUTLINE,0.10158501440922191,"
. Note that for rl−1, we have that"
PROOFS OUTLINE,0.10230547550432277,"rl−1 = ∞
X i=0 λl+i"
PROOFS OUTLINE,0.10302593659942363,"λl
< N(d, ˜l) + ∞
X"
PROOFS OUTLINE,0.1037463976945245,"i=1
N(d, ˜l + i)
˜λ˜l+i"
PROOFS OUTLINE,0.10446685878962536,"˜λ˜l
< Θ(m
d−2
d−1 ) (1 + Θ(1)) ,"
PROOFS OUTLINE,0.10518731988472622,"since N(d, ˜l + i) < N(d, j)id−2 and
˜λ˜l+i"
PROOFS OUTLINE,0.10590778097982709,"˜λ˜l
< 1"
PROOFS OUTLINE,0.10662824207492795,id . So indeed rl−1 = o(m) which implies rl = o(m).
PROOFS OUTLINE,0.10734870317002881,"For τm = o(m−
1
d−1 ) and τm = Θ(m−
1
d−1 ), we will directly analyze Equation (2). For k = Θ( 1 τm ),"
PROOFS OUTLINE,0.10806916426512968,"we have that
˜λi
˜λ1 > 1"
PROOFS OUTLINE,0.10878962536023054,"2 for all i ≤k. Let Li =
˜λi
˜λi+κ0 . Note that Li > 1"
PROOFS OUTLINE,0.10951008645533142,"2L1 for i ≤k. Note that m =
X"
PROOFS OUTLINE,0.11023054755043228,"i
N(d, i)Li > 1"
PROOFS OUTLINE,0.11095100864553314,"2L1 (N(d, 1) + · · · + N(d, k)) > Θ  1 τm d−1! L1."
PROOFS OUTLINE,0.11167146974063401,"So, we have that for all i that Li < L1 <
m
Θ

(
1
τm )
d−1. From Equation (2), we have that"
PROOFS OUTLINE,0.11239193083573487,"˜R( ˆf0) = E0
X"
PROOFS OUTLINE,0.11311239193083573,"i
N(d, i)(1 −Li)2β2
i + E0σ2 > E0 "
PROOFS OUTLINE,0.1138328530259366,"

1 −
m"
PROOFS OUTLINE,0.11455331412103746,"Θ

1
τm d−1  

 2"
PROOFS OUTLINE,0.11527377521613832,∥f ∗∥2 + E0σ2.
PROOFS OUTLINE,0.11599423631123919,"For τm = o(m
1
d−1 ) this is sufficient. Additionally, by a similar computation as above, in this
case, r1 = ω(m), so E0 is bounded by Equation (5). For τm = Θ(m
1
d−1 ), using Equation (5) and
Equation (6), by showing that for l = Θ(m), rl = Θ(m), we have Θ(1) > E0 > 1 + Θ(1)."
PROOFS OUTLINE,0.11671469740634005,"Proof sketch of Theorem 7.
Let Li =
˜λi
˜λi+κ0 . Note that (1 −Li)2 =
κ2
0
(κ0+˜λi)2 . Then, Equation (2)
can be rewritten and bounded as (with an abuse of notation for βi)"
PROOFS OUTLINE,0.11743515850144093,"˜R( ˆf0) = E0
X"
PROOFS OUTLINE,0.11815561959654179,"i
N(d, i)(1 −Li)2β2
i + E0σ2 ≤E0B2
l
X"
PROOFS OUTLINE,0.11887608069164265,"i=1
N(d, i)
κ2
0
(κ0 + ˜λi)2 + E0σ2."
PROOFS OUTLINE,0.11959654178674352,"Note that
κ2
0
(κ0+˜λi)2
<
κ2
0
˜λ2
i
and also Li
≤
˜λi
κ0 .
Therefore, we have that
˜R( ˆf0)
≤"
PROOFS OUTLINE,0.12031700288184438,"E0B2κ2
0
Pl
i=1 N(d, i) 1 ˜λ2
i"
PROOFS OUTLINE,0.12103746397694524,"
+ E0σ2. Note that m = P"
PROOFS OUTLINE,0.12175792507204611,"i N(d, i)Li < P"
PROOFS OUTLINE,0.12247838616714697,"i N(d, i)
˜λi
κ0 <
A
κ0 , so"
PROOFS OUTLINE,0.12319884726224783,"κ0 <
A
m. Finally, to bound E0, note that in Equation (5) we can choose k = Lm < m, then"
PROOFS OUTLINE,0.1239193083573487,"rk + k > Um > m, so E0 ≤
 
1 −Lm"
PROOFS OUTLINE,0.12463976945244956,"m
−1 
1 −
m
Um"
PROOFS OUTLINE,0.12536023054755044,"−1
. Combining these with κ0 < A"
PROOFS OUTLINE,0.1260806916426513,"m gives the
claim of Theorem 7. The proof of the alternative bound is harder and will be delayed to the appendix."
PROOFS OUTLINE,0.12680115273775217,"Proof sketch of Theorem 9.
By Theorem 3 from Zhou et al. [66], if k is the first k < m such"
PROOFS OUTLINE,0.12752161383285301,"that k + brk ≥m, then E0 ≥

1 −

b
b+1
2 k m"
PROOFS OUTLINE,0.1282420749279539,"−1
. Since maxi≤km

1
˜λi"
PROOFS OUTLINE,0.12896253602305474,"
<
m−Lm"
PROOFS OUTLINE,0.12968299711815562,"b
, for l <"
PROOFS OUTLINE,0.1304034582132565,"N(d, 1)+· · ·+N(d, km), we have that brl+l ≤N(d, 1)+· · ·+N(d, km)−1+b maxi≤km

1
˜λi 
<"
PROOFS OUTLINE,0.13112391930835735,"Lm + m −Lm ≤m, so the first l for which rl + l > m is l = Lm = N(d, 1) + · · · + N(d, km)."
PROOFS OUTLINE,0.13184438040345822,"Plugging in k = Lm we get that E0 ≥

1 −

b
b+1

Lm"
PROOFS OUTLINE,0.13256484149855907,"m
−1
, so from Equation (6), we have"
PROOFS OUTLINE,0.13328530259365995,"˜R( ˆf0) ≥

1 −

b
b+1
2 Lm m"
PROOFS OUTLINE,0.1340057636887608,"−1
σ2."
PROOFS OUTLINE,0.13472622478386168,"Proof sketch of Corollary 11.
Note that an analogous proof holds for any dα"
PROOFS OUTLINE,0.13544668587896252,"m →c, for a constant c,
but we take equality for simplicity. If k is a constant, i.e. it does not change with the dimension, we
have that N(d, k) = Θ(dk). Therefore, km = ⌊α⌋if α is a non-integer and α −1 if α is an integer.
If α is a non-integer, then Lm = N(d, km) + · · · + N(d, 1) = Θ(d⌊α⌋) and Um = N(d, km + 1) +
· · · + N(d, 1) = Θ(d⌊α⌋+1). So we have that Lm"
PROOFS OUTLINE,0.1361671469740634,"m = d⌊α⌋−α,
m
Lm = dα−⌊α⌋−1, N(d, ⌊α⌋) = d⌊α⌋.
Note that km = ⌊α⌋. We have from Assumption 10 that maxi≤km
1
˜λi = O(N(d, km)). Note"
PROOFS OUTLINE,0.13688760806916425,"that Theorem 7 holds with
Pl
i=1 N(d, i) 1 ˜λi"
PROOFS OUTLINE,0.13760806916426513,"
instead of
Pl
i=1 N(d, i) 1 ˜λ2
i"
PROOFS OUTLINE,0.138328530259366,"
. Therefore, we have"
PROOFS OUTLINE,0.13904899135446686,"1
m2 maxl≤km

1
˜λi N(d, i)

≤O(d(2⌊α⌋−2α)). This gives"
PROOFS OUTLINE,0.13976945244956773,"˜R( ˆf0) ≤

1 −d⌊α⌋−α−1 
1 −dα−⌊α⌋−1−1
σ2"
PROOFS OUTLINE,0.14048991354466858,"+ O

B2 
1 −d⌊α⌋−α−1 
1 −dα−⌊α⌋−1−1
d(2⌊α⌋−2α)

."
PROOFS OUTLINE,0.14121037463976946,"Therefore, limm→∞˜R( ˆf0) = σ2. So, if α is not an integer, we get benign overfitting. If α is an
integer, note that N(d, 1) + · · · + N(d, α −1) = o (dα), so km = α. Then there are cu, cl ∈(0, 1)
such that cl < Lm"
PROOFS OUTLINE,0.1419308357348703,"m ≤cu < 1. So, Theorem 9 holds for b = 1"
PROOFS OUTLINE,0.14265129682997119,"2, so ˜R( ˆf0) >
1
1−c1"
PROOFS OUTLINE,0.14337175792507204,9 σ2. This shows
PROOFS OUTLINE,0.1440922190201729,"that lim infm→∞˜R( ˆf0) ≥
1
1−c1"
PROOFS OUTLINE,0.14481268011527376,9 σ2 > σ2. The upper bound follows as above. Since we assumed
PROOFS OUTLINE,0.14553314121037464,"that maxi≤km
1
˜λi = O(N(d, km)), and maxi≤km N(d, i) = N(d, km) = Θ(dα), we have that
Pkm
i=1 N(d, i) 1"
PROOFS OUTLINE,0.14625360230547552,"˜λi = Θ(d2α), so"
PROOFS OUTLINE,0.14697406340057637,"˜R( ˆf0) ≤(1 −cl)−1  
1 −d−1−1 σ2 + Θ

B2  
1 −d−1−1
."
PROOFS OUTLINE,0.14769452449567724,"So, we conclude that for integer α we have tempered overfitting and for non-integer α we have benign
overfitting. This recovers results of Ghorbani et al. [24], Mei et al. [38], Misiakiewicz [40], Barzilai
and Shamir [4], Zhang et al. [64]."
PROOFS OUTLINE,0.1484149855907781,"Proof sketch of Corollary 12.
Note that this holds for any scaling of d and m where d = Θ(log m),
but we take this particular one for concreteness. As we show in the appendix (Theorem 21), it
is not hard to see that Lm ≈αlm for some constant αl < 1 and Lm ≈αum for some constant
αu > 1. Theorem 9 implies that we cannot get benign overfitting in this case, i.e. for all m,
˜R( ˆf0) >
1
1−(
b
b+1)
2αl σ2. This shows that lim infm→∞˜R( ˆf0) ≥
1
1−(
b
b+1)
2αl σ2 > σ2."
PROOFS OUTLINE,0.14913544668587897,"Proof sketch of Corollary 13.
We have that for the Gaussian kernel on the sphere, Theorem 7
holds with A = 1 (see Appendix C.4 for further details). First, we will compute km and hence Lm
and Um. After some tedious calculation (Theorem 23 in the appendix), we see that for k ≤2l + l −1
we have N(d, 1) + · · · + N(d, k) = o(m), but for k = 2l + l we have N(d, 1) + · · · + N(d, k) >
m. This shows that km = 2l + l −1 and so again after long calculations Lm = Θ(
m
log m) and"
PROOFS OUTLINE,0.14985590778097982,"Um > Θ(md0.89). To estimate Pk
i=1 N(d, i)˜λi, note that eigenvalues are decreasing and iN(d, i) =
iN(d, i+1) 2i+d−2"
PROOFS OUTLINE,0.1505763688760807,"2i+d
i+1
i+d−2 = o(N(d, i+1)) (take k ≪d), so it suffices to estimate N(d, k) 1"
PROOFS OUTLINE,0.15129682997118155,"˜λk . Now,"
PROOFS OUTLINE,0.15201729106628242,from Equation (7) and an estimate on the size of the first eigenvalue (Corollary 31) ˜λ1 = Θ( 1
PROOFS OUTLINE,0.15273775216138327,"dα ), for
fixed α > 0, we can estimate the size ˜λk. We have that"
PROOFS OUTLINE,0.15345821325648415,"1
˜λk
< 1"
PROOFS OUTLINE,0.15417867435158503,"˜λ1
(τm)k
d"
PROOFS OUTLINE,0.15489913544668588,2 + k + 2 τ 2m
PROOFS OUTLINE,0.15561959654178675,"k
< dk(1+ε),"
PROOFS OUTLINE,0.1563400576368876,for arbitrarily small ε. For k = 7km
PROOFS OUTLINE,0.15706051873198848,"24 , from additional calculation (Theorem 23 in the appendix), we
have that m
1
4 ≤N(d, k) ≤m
1
3 and dk < m
7
24 . So Pkm
i=1 N(d, i) 1"
PROOFS OUTLINE,0.15778097982708933,"˜λ2
i < m. Plugging these back into
Theorem 7 gives the desired result."
SUMMARY,0.1585014409221902,"6
Summary"
SUMMARY,0.15922190201729106,"In this paper, we considered the minimum norm interpolating solution of kernel ridge regression in
fixed dimension with Gaussian kernel and varying or adaptively chosen bandwidth, and in increasing
dimension with various kernels. In fixed dimension, we showed that if the source distribution is
uniform on the sphere, then the minimum norm interpolating solution is inconsistent for any choice
of bandwidth, and usually worse than the null predictor, except possibly in one particular scaling of
bandwidth and with small noise. Furthermore, we showed a general upper and lower bound on the
test risk, which we applied in the case of increasing dimension to recover the currently known results
about polynomially increasing dimension and show two new results: We showed that no dot-product
kernel on the sphere can achieve consistency for logarithmic scaling of the dimension, and obtained
the first case of sub-polynomially scaling dimension where the minimum norm interpolating solution
exhibits benign overfitting, namely with Gaussian kernel and dimension scaling as d = exp(√log m)."
SUMMARY,0.15994236311239193,Acknowledgments and Disclosure of Funding
SUMMARY,0.16066282420749278,We would like to thank Theodor Misiakiewicz and Sam Buchanan for useful discussions.
SUMMARY,0.16138328530259366,"This work was done as part of the NSF-Simons funded collaboration on the Theoretical Foundations
of Deep Learning (https://deepfoundations.ai), and primarily while GV was at TTIC. GV is supported
by research grants from the Center for New Scientists at the Weizmann Institute of Science, and the
Shimon and Golde Picker – Weizmann Annual Grant."
REFERENCES,0.16210374639769454,References
REFERENCES,0.1628242074927954,"[1] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections.
arXiv preprint arXiv:2303.01372, 2023."
REFERENCES,0.16354466858789626,"[2] Peter L. Bartlett and Philip M. Long. Failures of model-dependent generalization bounds for least-norm
interpolation. The Journal of Machine Learning Research, 22(1):9297–9311, 2021."
REFERENCES,0.1642651296829971,"[3] Peter L. Bartlett, Philip M. Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020."
REFERENCES,0.164985590778098,"[4] Daniel Barzilai and Ohad Shamir. Generalization in kernel regression under realistic assumptions. arXiv
preprint arXiv:2312.15995, 2023."
REFERENCES,0.16570605187319884,"[5] Daniel Beaglehole, Mikhail Belkin, and Parthe Pandit. On the inconsistency of kernel ridgeless regression
in fixed dimensions. arXiv preprint arXiv:2205.13525, 2023."
REFERENCES,0.16642651296829972,"[6] Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on
Mathematics of Data Science, 2(4):1167–1180, 2020."
REFERENCES,0.16714697406340057,"[7] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learn- ing curves in
kernel regression and wide neural networks. International Conference on Machine Learning, PMLR, pages
1024–1034, 2020."
REFERENCES,0.16786743515850144,"[8] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain
generalization in kernel regression and infinitely wide neural networks. Nature Communications, 12(1):
1–12, 2021."
REFERENCES,0.1685878962536023,"[9] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019."
REFERENCES,0.16930835734870317,"[10] Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin
classification on sub-gaussian mixtures. In Advances in Neural Information Processing Systems (NeurIPS),
2021."
REFERENCES,0.17002881844380405,"[11] Yuan Cao, Zixiang Chen, Mikhail Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional
neural networks. arXiv preprint arXiv:2202.06526, 2022."
REFERENCES,0.1707492795389049,"[12] Niladri S. Chatterji and Philip M. Long. Finite-sample analysis of interpolating linear classifiers in the
overparameterized regime. Journal of Machine Learning Research, 22(129):1–30, 2021."
REFERENCES,0.17146974063400577,"[13] Niladri S. Chatterji, Philip M. Long, and Peter L Bartlett. The interplay between implicit bias and benign
overfitting in two-layer linear networks. arXiv preprint arXiv:2108.11489, 2021."
REFERENCES,0.17219020172910662,"[14] Chen Cheng and Andrea Montanari. Dimension free ridge regression. arXiv preprint arXiv:2210.08571,
2022."
REFERENCES,0.1729106628242075,"[15] Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, and David Belius. Characterizing overfitting in
kernel ridgeless regression through the eigenspectrum. arXiv preprint arXiv:2402.01297, 2024."
REFERENCES,0.17363112391930835,"[16] Geoffrey Chinot and Matthieu Lerasle. On the robustness of the minimum ℓ2 interpolator. arXiv preprint
arXiv:2003.05838, 2020."
REFERENCES,0.17435158501440923,"[17] W. J. Conover. Practical Nonparametric Statistics. Wiley Series in Probability and Statistics, 1999."
REFERENCES,0.17507204610951008,"[18] Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates for noisy interpo-
lation require rethinking the effect of inductive bias. In International Conference on Machine Learning
(ICML), 2022."
REFERENCES,0.17579250720461095,"[19] Mona Eberts and Ingo Steinwart. Optimal regression rates for svms using gaussian kernels. Electronic
Journal of Statistics, page Vol. 7 1–42, 2013."
REFERENCES,0.1765129682997118,"[20] Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett. Benign overfitting without linearity: Neural
network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory
(COLT), 2022."
REFERENCES,0.17723342939481268,"[21] Spencer Frei, Gal Vardi, Peter L Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and
leaky relu networks from kkt conditions for margin maximization. arXiv preprint arXiv:2303.01462, 2023."
REFERENCES,0.17795389048991356,"[22] Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On the
similarity between the laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580, 2020."
REFERENCES,0.1786743515850144,"[23] Erin George, Michael Murray, William Swartworth, and Deanna Needell. Training shallow relu networks
on noisy data using hinge loss: when do we overfit and is it benign? Advances in Neural Information
Processing Systems, 36, 2024."
REFERENCES,0.17939481268011528,"[24] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural
networks in high dimension. The Annals of Statistics, 49(2):1029–1054, 2021."
REFERENCES,0.18011527377521613,"[25] Nikhil Ghosh and Mikhail Belkin. A universal trade-off between the model size, test loss, and training loss
of linear predictors. arXiv preprint arXiv:2207.11621, 2022."
REFERENCES,0.180835734870317,"[26] Moritz Haas, David Holzmüller, Ulrike von Luxburg, and Ingo Steinwart. Mind the spikes: Benign
overfitting of kernels and neural networks in fixed dimension. 37th Conference on Neural Information
Processing Systems (NeurIPS 2023), 2023."
REFERENCES,0.18155619596541786,"[27] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional
ridgeless least squares interpolation. arXiv peprint arXiv:1903.08560, 2020."
REFERENCES,0.18227665706051874,"[28] Hong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of random feature regression beyond the
linear scaling regime. arXiv preprint arXiv:2403.08160v1, 2023."
REFERENCES,0.1829971181556196,"[29] Nirmit Joshi, Gal Vardi, and Nathan Srebro. Noisy interpolation learning with shallow univariate relu
networks. arXiv preprint arXiv:2307.15396, 2023."
REFERENCES,0.18371757925072046,"[30] Peizhong Ju, Xiaojun Lin, and Jia Liu. Overfitting can be harmless for basis pursuit, but only to a degree.
Advances in Neural Information Processing Systems, 33:7956–7967, 2020."
REFERENCES,0.1844380403458213,"[31] Kedar Karhadkar, Erin George, Michael Murray, Guido Montúfar, and Deanna Needell. Benign overfitting
in leaky relu networks with moderate input dimension. arXiv preprint arXiv:2403.06903, 2024."
REFERENCES,0.1851585014409222,"[32] Frederic Koehler, Lijia Zhou, Danica J Sutherland, and Nathan Srebro. Uniform convergence of in-
terpolators: Gaussian width, norm bounds, and benign overfitting. arXiv preprint arXiv:2106.09276,
2021."
REFERENCES,0.18587896253602307,"[33] Guy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in relu neural
networks. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.18659942363112392,"[34] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting for two-layer relu
networks. arXiv preprint arXiv:2303.04145, 2023."
REFERENCES,0.1873198847262248,"[35] Yicheng Li, Zixiong Yu, Guhan Chen, and Qian Lin. Statistical optimality of deep wide neural networks.
arXiv preprint arXiv:2305.02657, 2023."
REFERENCES,0.18804034582132564,"[36] Tengyuan Liang and Benjamin Recht. Interpolating classifiers make few mistakes. arXiv preprint
arXiv:2101.11815, 2021."
REFERENCES,0.18876080691642652,"[37] Neil Mallinar, James B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakki-
ran. Benign, tempered, or catastrophic: A taxonomy of overfitting. arXiv preprint arXiv:2207.06569v2,
2022."
REFERENCES,0.18948126801152737,"[38] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random feature and
kernel methods: hypercontractivity and kernel matrix concentration. Applied and Computational Harmonic
Analysis, 59:3–84, 2022."
REFERENCES,0.19020172910662825,"[39] Xuran Meng, Difan Zou, and Yuan Cao. Benign overfitting in two-layer relu convolutional neural networks
for xor data. arXiv preprint arXiv:2310.01975, 2023."
REFERENCES,0.1909221902017291,"[40] Theodor Misiakiewicz. Spectrum of inner-product kernel matrices in the polynomial regime and multiple
descent phenomenon in kernel ridge regression. arXiv:2204.10425, 2022."
REFERENCES,0.19164265129682997,"[41] Theodor Misiakiewicz and Basil Saeed. A non-asymptotic theory of kernel ridge regression: deterministic
equivalents, test error, and gcv estimator. arXiv preprint arXiv:2403.08938, 2024."
REFERENCES,0.19236311239193082,"[42] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin
linear classifiers: Benign overfitting and high dimensional asymptotics in the overparametrized regime.
Preprint arXiv:1911.01544v3, 2023."
REFERENCES,0.1930835734870317,"[43] Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of
noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 2020."
REFERENCES,0.19380403458213258,"[44] Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant
Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? Journal
of Machine Learning Research, 22(222):1–69, 2021."
REFERENCES,0.19452449567723343,"[45] Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel Roy. In defense of uniform convergence: General-
ization via derandomization with an application to interpolating predictors. In International Conference on
Machine Learning, pages 7263–7272, 2020."
REFERENCES,0.1952449567723343,"[46] Minh Ha Quang and Yuan Yao. Mercer’s theorem, feature maps, and smoothing. Conference: Learning
Theory, 19th Annual Conference on Learning Theory, COLT, 2006."
REFERENCES,0.19596541786743515,"[47] Alexander Rakhlin and Xiyu Zhai. Consistency of interpolation with laplace kernels is a high-dimensional
phenomenon. arXiv preprint arXiv:1812.11167, 2018."
REFERENCES,0.19668587896253603,"[48] Javier Segura. Simple bounds with best possible accuracy for ratios of modified bessel functions. arXiv
preprint arXiv:2207.02713v3, 2023."
REFERENCES,0.19740634005763688,"[49] Ohad Shamir. The implicit bias of benign overfitting. In Conference on Learning Theory, pages 448–478.
PMLR, 2022."
REFERENCES,0.19812680115273776,"[50] James B. Simon, Madeline Dickens, Dhruva Karkada, and Michael R. DeWeese. The eigenlearning
framework: A conservation law perspective on kernel regression and wide neural networks. arXiv preprint
arXiv:2110.03922, 2021."
REFERENCES,0.1988472622478386,"[51] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media,
2008."
REFERENCES,0.19956772334293948,"[52] Christos Thrampoulidis, Samet Oymak, and Mahdi Soltanolkotabi. Theoretical insights into multiclass
classification: A high-dimensional asymptotic view. Advances in Neural Information Processing Systems,
33:8907–8920, 2020."
REFERENCES,0.20028818443804033,"[53] A. Tsigler and P. L. Bartlett. Benign overfitting in ridge regression. arXiv peprint arXiv:2009.14286, 2020."
REFERENCES,0.2010086455331412,"[54] Guillaume Wang, Konstantin Donhauser, and Fanny Yang. Tight bounds for minimum l1-norm interpolation
of noisy data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022."
REFERENCES,0.2017291066282421,"[55] Ke Wang and Christos Thrampoulidis. Binary classification of gaussian mixtures: Abundance of support
vectors, benign overfitting and regularization. arXiv peprint arXiv:2011.09148, 2021."
REFERENCES,0.20244956772334294,"[56] Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overfitting in multiclass classification:
All roads lead to interpolation. In Advances in Neural Information Processing Systems (NeurIPS), 2021."
REFERENCES,0.20317002881844382,"[57] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how real-
world neural representations generalize. In International Conference on Machine Learning, Proceedings of
Machine Learning Research, 2022."
REFERENCES,0.20389048991354466,"[58] Denny Wu and Ji Xu. On the optimal weighted ℓ2 regularization in overparameterized linear regression.
Advances in Neural Information Processing Systems, 33:10112–10123, 2020."
REFERENCES,0.20461095100864554,"[59] Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue M. Lu, and Jeffrey Pennington. Precise learning curves
and higher-order scaling limits for dot product kernel regression. arXiv preprint: arXiv:2205.14846v3,
2023."
REFERENCES,0.2053314121037464,"[60] Xingyu Xu and Yuantao Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In
International Conference on Artificial Intelligence and Statistics, pages 11094–11117. PMLR, 2023."
REFERENCES,0.20605187319884727,"[61] Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in relu
networks for xor cluster data. arXiv preprint arXiv:2310.02541, 2023."
REFERENCES,0.20677233429394812,"[62] Zhen-Hang Yang and Yu-Ming Chu. On approximating the modified bessel function of the first kind and
toader-qi mean. Journal of Inequalities and Applications, 2016."
REFERENCES,0.207492795389049,"[63] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning requires rethinking generalization. In International Conference on Learning Representations,
2017."
REFERENCES,0.20821325648414984,"[64] Haobo Zhang, Weihao Lu, and Qian Lin. The phase diagram of kernel interpolation in large dimensions.
arXiv preprint arXiv:2404.12597v1, 2024."
REFERENCES,0.20893371757925072,"[65] Lijia Zhou, Frederic Koehler, Pragya Sur, Danica J Sutherland, and Nathan Srebro. A non-asymptotic
moreau envelope theory for high-dimensional generalized linear models. arXiv preprint arXiv:2210.12082,
2022."
REFERENCES,0.20965417867435157,"[66] Lijia Zhou, James B Simon, Gal Vardi, and Nathan Srebro. An agnostic view on the cost of overfitting in
(kernel) ridge regression. arXiv preprint arXiv:2306.13185, 2023."
REFERENCES,0.21037463976945245,"A
Empirical Justification"
REFERENCES,0.21109510086455333,"(a) τm = o(m−
1
d−1 ) with σ2 = 1 and d = 6. We see that the test error tends to something that is equal to or
larger than the test risk of the null predictor."
REFERENCES,0.21181556195965417,"(b) τm = ω(m−
1
d−1 ) with σ2 = 10 and d = 4. We see that the test error is increasing with the sample size m,
suggesting that the test error diverges to infinity."
REFERENCES,0.21253602305475505,"(c) τm = Θ(m−
1
d−1 ) with σ2 = 10000 and d = 6. We see that the test error is well above the risk of the null
predictor."
REFERENCES,0.2132564841498559,"Figure 1: We plot the dependence of the test error of the Gaussian kernel ridgeless predictor for
y = f ∗(x) + ξ, with ξ ∼N(0, σ2), f ∗= 10, x ∼Unif(Sd−1), and σ2 and d as above, and sample
size m. Light blue lines represent the test error for 100 different runs of the experiment and dark blue
is the mean of all those runs. The red dotted line is the noise level σ2 and the yellow dotted line is
the error the null predictor achieves in this setting. We see that the test error concentrates around its
mean and that the behavior is as predicted by Theorem 3."
REFERENCES,0.21397694524495678,"In this section, we provide empirical justification for the predictions of Theorem 3, which deals with
the case of fixed dimension. Figure 1 shows the dependence of the test error on the sample size for
the Gaussian Kernel Ridgeless prediction with varying bandwidth, as in the setup of Theorem 3.
Specifically, we consider y = f ∗(x) + ξ where ξ ∼N(0, σ2), f ∗= 10, σ2 is the noise level, and"
REFERENCES,0.21469740634005763,"x ∼Unif(Sd−1). We vary the values of d and σ2 and the bandwidth scaling τm as follows: for
τm = o(m−
1
d−1 ) we take σ2 = 1 and d = 6, for τm = ω(m−
1
d−1 ) we take σ2 = 10 and d = 4, and
for τm = Θ(m−
1
d−1 ) we take σ2 = 10000 and d = 6. We compare the test error (light blue) and
the mean of the test error for 100 different runs of the experiment (dark blue) with the noise level
(the Bayes risk; red) and the risk of the null predictor (yellow). We see that for all three cases, our
predictions agree with the experiments, that is, in the case τm = o(m−
1
d−1 ) the test error is eventually
above the null predictor error but finite, in the case τm = ω(m−
1
d−1 ) the test error increases with
sample size, and in the case τm = Θ(m−
1
d−1 ) the test error is above null predictor error. The code to
reproduce these experiments can be found at"
REFERENCES,0.2154178674351585,https://github.com/marko-medvedev/overfitting-behavior-of-gaussian-kernel-ridgeless-regression.
REFERENCES,0.21613832853025935,We ran the experiments on one A6000 GPU.
REFERENCES,0.21685878962536023,"Statistical significance of experimental results: In Figure 2 and Figure 3, we present evidence for
the statistical significance of our experimental results. We examine whether a significant mass of the
distribution of the mean of the test loss, L, exhibits the same behavior as L. We use 50 of the 100
experiments to estimate an interval [Llow, Lhigh] and the other 50 to test whether this interval contains
p = 0.8 of the total mass of the distribution of L at α = 0.05 significance level. In Figure 2, we plot
Llow and Lhigh, the dark blue lines. We perform one lower-tailed and one upper-tailed test [17]. The
null hypotheses are: 1 −1−p"
REFERENCES,0.21757925072046108,"2
population quantile is no greater than Lhigh and 1−p"
POPULATION QUANTILE,0.21829971181556196,"2
population quantile
is at least as great as Llow. Indeed, both Llow and Lhigh have the desired behavior. Furthermore, in
Figure 3, we report the relevant test statistics, T1 which is the number of draws of L no greater than
Lhigh and T2 which is the number of draws of L less than Llow. t1 and t2 are the minimum numbers
such that P(Y ≤t1) = α and P(Y ≤t2) = 1 −α, where Y is a binomially distributed random
variables with parameters p and n = 50. The leftmost bar plot compares T1 and t1, the middle bar
plot compares T2 and t2, and the rightmost bar plot shows the p-value and the significance α. To
accept the null hypothesis we need T1 > t1, T2 ≤t2, and high p-value. We report an aggregated
p-value. We accept the null hypothesis in both cases - the test statistics T1 and T2 satisfy the required
conditions and the p-values are high."
POPULATION QUANTILE,0.21902017291066284,"Overall, we can conclude that the prediction of Theorem 3 agrees with the empirical observations
that are statistically significant."
POPULATION QUANTILE,0.21974063400576369,"(a) τm = o(m−
1
d−1 ) with σ2 = 1 and d = 6."
POPULATION QUANTILE,0.22046109510086456,"(b) τm = ω(m−
1
d−1 ) with σ2 = 10 and d = 4."
POPULATION QUANTILE,0.2211815561959654,"(c) τm = Θ(m−
1
d−1 ) with σ2 = 10000 and d = 6."
POPULATION QUANTILE,0.2219020172910663,"Figure 2: For each of the setups of Theorem 3, we plot the dependence of the estimates for the
minimum and maximum of the test error of the Gaussian kernel ridgeless predictor Llow and Lhigh
for y = f ∗(x) + ξ, with ξ ∼N(0, σ2), f ∗= 10, x ∼Unif(Sd−1), and σ2 and d as above, on the
sample size m. The red dotted line is the noise level σ2 and the yellow dotted line is the error the null
predictor achieves in this setting. In dark blue, we plot the estimates for the minimum and maximum
of mean test error Llow and Lhigh for a given sample size. We test whether [Llow, Lhigh] contains
p = 0.8 of the total mass of the mean test error L."
POPULATION QUANTILE,0.22262247838616714,"(a) τm = o(m−
1
d−1 ) with σ2 = 1 and d = 6."
POPULATION QUANTILE,0.22334293948126802,"(b) τm = ω(m−
1
d−1 ) with σ2 = 10 and d = 4."
POPULATION QUANTILE,0.22406340057636887,"(c) τm = Θ(m−
1
d−1 ) with σ2 = 10000 and d = 6."
POPULATION QUANTILE,0.22478386167146974,"Figure 3: For each of the setups of Theorem 3, we test the statistical significance that p = 0.8 of
the probability mass of L, the test error, is inside [Llow, Lhigh] at α = 0.05 significance level. In the
leftmost plots, we plot the dependence of the test statistics T1 and t1 on sample size m. T1 is the
number of draws of the test error L no greater than Lhigh and t1 is the minimal number such that
P(Y ≤t1) = α. In the middle plots, we plot the dependence of test statistics T2 and t2 on sample
size m. T2 is the number of draws of the test error L less than Llow and t2 is the minimal number such
that P(Y ≤t2) = 1 −α. Here Y is a binomially distributed random variable with parameters p and
n = 50. On the rightmost plots, we show the dependence of the aggregated p-value on the sample
size m. We accept the null hypothesis in both cases because T1 > t1, T2 ≤t2, and the p-values are
high."
POPULATION QUANTILE,0.2255043227665706,"B
The case of fixed dimension"
POPULATION QUANTILE,0.22622478386167147,"Cost of overfitting
The cost of overfitting, introduced in [66], is defined as the ratio of the test risk
of an interpolating solution and the test risk of the optimally regularized solution. It measures how
worse off are we interpolating rather than regularizing."
POPULATION QUANTILE,0.22694524495677235,"Definition 15 (Cost of overfitting). Let ˆfδ∗be the optimally regularized solution, i.e."
POPULATION QUANTILE,0.2276657060518732,"R( ˆfδ∗) = inf
δ≥0 R( ˆfδ)."
POPULATION QUANTILE,0.22838616714697407,"For a distribution D over X × R, we will define the cost of overfitting C(D, m) as"
POPULATION QUANTILE,0.22910662824207492,"C(D, m) = R( ˆf0)"
POPULATION QUANTILE,0.2298270893371758,"R( ˆfδ∗)
."
POPULATION QUANTILE,0.23054755043227665,"Zhou et al. [66] show that agnostically to the distribution D, the behavior of the cost of overfitting is
tightly controlled by the effective ranks of systems of eigenvalue {λi}∞
i=1 of a given kernel (coming
from Mercer’s theorem) rk :="
POPULATION QUANTILE,0.23126801152737753,"P∞
i=k+1 λi"
POPULATION QUANTILE,0.23198847262247838,"λk+1
and Rk :="
POPULATION QUANTILE,0.23270893371757925," P∞
i=k+1 λi
2
P∞
i=k+1 λ2
i
."
POPULATION QUANTILE,0.2334293948126801,"We will assume additionally that the eigenvalues are nonzero, so rk and Rk are well defined on N.
Since we chose that the eigenvalues are sorted and since they square summable, we have that
rk ≤Rk ≤r2
k."
POPULATION QUANTILE,0.23414985590778098,"The following two results of [66] summarize how we will use effective ranks to control the cost of
overfitting. Proposition 16 gives a necessary and sufficient condition for the overfitting to benign in
terms of the cost of overfitting, i.e. that C(D, m) →1, in terms of effective ranks.
Proposition 16 (Necessary and sufficient condition for benign overfitting [66]). Let kn be the
smallest integer k ∈N ∪{0} for which n < k + rk. Then E0 →1 if and only if"
POPULATION QUANTILE,0.23487031700288186,"lim
n→∞
kn"
POPULATION QUANTILE,0.2355907780979827,"n = 0 and
lim
n→∞
n
Rkn
= 0."
POPULATION QUANTILE,0.23631123919308358,"Proposition 17 gives a sufficient condition for ˜R( ˆf0) →∞in terms of effective ranks.
Proposition 17 (Sufficient condition for catastrophic overfitting [66]). Let ε > 0 and let k = (1+ε)m.
If limk→∞
rk"
POPULATION QUANTILE,0.23703170028818443,"k = 0 then E0 →∞, i.e. the overfitting is catastrophic."
POPULATION QUANTILE,0.2377521613832853,"Using sharp bounds on eigenvalues {λi}∞
i=1, derived in Theorem 28, we can derive bounds on
effective ranks so that we can apply the results of [66] and Equation (2) to bound the test risk of
minimum norm interpolant."
POPULATION QUANTILE,0.23847262247838616,"When the input is distributed uniformly on the sphere, the eigenfunctions {ϕk} in the Mercer
decomposition of the kernel function can be taken to be the spherical harmonics Yks. Using the Funk-
Hecke formula we can find the closed form of eigenvalues for the Gaussian kernel [46], which are
given in terms of bandwidth τm and modified Bessel function of the first kind Iv(x). In Appendix D,
we will derive bounds on the sizes of the eigenvalues using the properties of Bessel functions and
their multiplicities."
POPULATION QUANTILE,0.23919308357348704,"We need to take a particular input distribution, in order to able to compute the eigenvalues explicitly
and to know their multiplicities. The result of Cao et al. [9] offers one possible way of generalizing
our result to more general different distributions"
POPULATION QUANTILE,0.23991354466858789,"We split the proof into two parts. First, we will bound the cost of overfitting. Then, we will provide
lower bounds on the test risk of Gaussian KRR."
POPULATION QUANTILE,0.24063400576368876,"Bounds on cost of overfitting
The bounds on cost of overfitting will be used to show whether the
test risk is bounded above or away from Bayes risk.
Lemma 18 (Cost of overfitting for Gaussian KRR). Let X ∼Unif(Sd−1). Then the following
bounds hold for the minimum norm interpolating solution of KRR and its cost of overfitting C(D, m)
and E0:"
POPULATION QUANTILE,0.2413544668587896,"1. If τm ≤m−
1
d−1 t(m), where t(m) →0 as m →∞then there is a constant CI that depends
on d and C, such that"
POPULATION QUANTILE,0.2420749279538905,"1 ≤C(D, m) ≤E0 ≤

1 −1 m"
POPULATION QUANTILE,0.24279538904899137,"−2 
1 −CIt(m)
d−1"
POPULATION QUANTILE,0.24351585014409222,"2
−1
."
POPULATION QUANTILE,0.2442363112391931,"2. If τm ≥m−
1
d−1 t(m), where t(m) →∞as m →∞then there is a integer m0 depending
on d and t(m) such that for all m ≥m0"
POPULATION QUANTILE,0.24495677233429394,E0 ≥√m.
POPULATION QUANTILE,0.24567723342939482,"3. If C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 then there exist constants CI > 0 and CII depending on
C1, C2 and d such that"
POPULATION QUANTILE,0.24639769452449567,"C(D, m) ≤E0 ≤CI
E0 ≥1 + CII."
POPULATION QUANTILE,0.24711815561959655,Proof. We will break down the proof into the proof of the three parts.
POPULATION QUANTILE,0.2478386167146974,"Part 1:
We will use Proposition 16 to prove this that E0 →1. We can see the upper bound on E0
from Theorem 2 [66]. We have that for k < m"
POPULATION QUANTILE,0.24855907780979827,"E0 ≤

1 −k m"
POPULATION QUANTILE,0.24927953890489912,"−2 
1 −m Rk −1
."
POPULATION QUANTILE,0.25,"Now we will show that for k = 1, we have that k + rk > m and limm→∞m"
POPULATION QUANTILE,0.2507204610951009,R1 = 0. This implies that
POPULATION QUANTILE,0.25144092219020175,conditions of Proposition 16 hold so E0 →1. We will rewrite rk = P
POPULATION QUANTILE,0.2521613832853026,"i>k λk
λk+1
using ˜λi. We have that"
POPULATION QUANTILE,0.25288184438040345,"r1 =
P"
POPULATION QUANTILE,0.25360230547550433,i>1 λi
POPULATION QUANTILE,0.2543227665706052,"λ2
= ˜N(d, ˜1) + ∞
X"
POPULATION QUANTILE,0.25504322766570603,"j=1
N(d, ˜1 + j)
˜λ˜1+j"
POPULATION QUANTILE,0.2557636887608069,"˜λi1
,"
POPULATION QUANTILE,0.2564841498559078,"where N(d, 1) = d, so ˜1 = 1 and ˜N(d, ˜1) = d −1. Therefore, we have"
POPULATION QUANTILE,0.25720461095100866,"r1 = (d −1) + ∞
X"
POPULATION QUANTILE,0.2579250720461095,"j=1
N(d, 1 + j)
˜λ1+j ˜λ1
."
POPULATION QUANTILE,0.25864553314121036,"Let T =
2
τ 2
m . Taking √"
POPULATION QUANTILE,0.25936599423631124,T g(m)
POPULATION QUANTILE,0.2600864553314121,"2
≤j ≤
√"
POPULATION QUANTILE,0.260806916426513,"Tg(m), k = 1, we can apply the bound Theorem 28 on
˜λ1+j"
POPULATION QUANTILE,0.2615273775216138,"˜λ1 .
Therefore, we have that"
POPULATION QUANTILE,0.2622478386167147,"exp(−5g(m)) <
˜λ1+j"
POPULATION QUANTILE,0.26296829971181557,"˜λ1
< 1."
POPULATION QUANTILE,0.26368876080691644,"In particular, we have that for m large enough, for all such j it holds that
˜λ1+j"
POPULATION QUANTILE,0.26440922190201727,"˜λ1
> 1"
GOING BACK TO,0.26512968299711814,"2. Going back to
r1, this means that"
GOING BACK TO,0.265850144092219,"r1 = (d −1) + ∞
X"
GOING BACK TO,0.2665706051873199,"j=1
N(d, 1 + j)
˜λ1+j ˜λ1 > √"
GOING BACK TO,0.2672910662824208,"T g(m)
X j= 1 2
√"
GOING BACK TO,0.2680115273775216,"T g(m)
N(d, 1 + j)
˜λ1+j ˜λ1
> √"
GOING BACK TO,0.2687319884726225,"T g(m)
X j= 1 2
√"
GOING BACK TO,0.26945244956772335,"T g(m)
(1 + j)d−2 1 2 > √"
GOING BACK TO,0.27017291066282423,"T g(m)
X j= 1 2
√"
GOING BACK TO,0.27089337175792505,T g(m)
GOING BACK TO,0.2716138328530259,"1
2d+1 (
√"
GOING BACK TO,0.2723342939481268,Tg(m))d−2 > 1 2 √
GOING BACK TO,0.2730547550432277,"Tg(m)
1
2d+1 (
√"
GOING BACK TO,0.2737752161383285,"Tg(m))d−2 > 1 2(
√"
GOING BACK TO,0.2744956772334294,"Tg(m))d−1. If
√"
GOING BACK TO,0.27521613832853026,"Tg(m) is not an integer, we can take its integer part and add a small constant since
√"
GOING BACK TO,0.27593659942363113,"Tg(m)
will be large. Note that τm ≤m−
1
d−1 t(m), so"
GOING BACK TO,0.276657060518732,"T ≥m
2
d−1
1
t(m)2 ."
GOING BACK TO,0.27737752161383283,Therefore
GOING BACK TO,0.2780979827089337,"r1 > 1 2(
√"
GOING BACK TO,0.2788184438040346,Tg(m))d−1 > 1 2
GOING BACK TO,0.27953890489913547,"
m
1
d−1 g(m) t(m)"
GOING BACK TO,0.2802593659942363,"d−1
= m1
g(m) t(m)"
GOING BACK TO,0.28097982708933716,"d−1
,"
GOING BACK TO,0.28170028818443804,so as long as g(m)
GOING BACK TO,0.2824207492795389,"t(m) →∞, r1 > m for m large enough. We can take g(m) =
p"
GOING BACK TO,0.2831412103746398,"t(m). Then, in
Proposition 16, we can select km = 1, since 1 + r1 > m for large enough m. Note also that R1 ≥r1,"
GOING BACK TO,0.2838616714697406,"so we also have that R1 ≥r1 > m1 
g(m)"
GOING BACK TO,0.2845821325648415,"t(m)
d−1
, so"
GOING BACK TO,0.28530259365994237,"lim
m→∞
1
m = 0 and
lim
m→∞
m
R1
= 0."
GOING BACK TO,0.28602305475504325,This finishes the proof of the first claim.
GOING BACK TO,0.28674351585014407,"Part 2:
Let τm ≥m−
1
d−1 t(m), where t(m) →∞as m →∞. We will use Proposition 17 to prove
this claim. Using Theorem 5 from [66] we know that for any ε > 0 if rk = o(m) for k = (1 + ε)m
then the following bound holds"
GOING BACK TO,0.28746397694524495,"E0 ≥
1 1 −m"
GOING BACK TO,0.2881844380403458,"k

k−m
k−m+rk"
GOING BACK TO,0.2889048991354467, > 1 + 1 ε.
GOING BACK TO,0.2896253602305475,"We will show that there is a constant U > 0 such that for any k ≥m, we have that"
GOING BACK TO,0.2903458213256484,"rk ≤Um

1
t(m)  d−1 2
."
GOING BACK TO,0.2910662824207493,"By Theorem 28, we have that for j ≥
√"
GOING BACK TO,0.29178674351585016,Tg(m) where g(m) →∞as m →∞that
GOING BACK TO,0.29250720461095103,˜λ˜k+j
GOING BACK TO,0.29322766570605185,"˜k
< exp

−g(m)2 4 
."
GOING BACK TO,0.29394812680115273,Let k = (1 + ε)m. We have that
GOING BACK TO,0.2946685878962536,"rk = ˜N(d, ˜k) + ∞
X"
GOING BACK TO,0.2953890489913545,"j=1
N(d, ˜k + j)
˜λ˜k+j"
GOING BACK TO,0.2961095100864553,"˜λ˜k
."
GOING BACK TO,0.2968299711815562,"Now, if we take j ≥j0 =
√"
GOING BACK TO,0.29755043227665706,"Tg(m), it holds that"
GOING BACK TO,0.29827089337175794,˜λ˜k+j
GOING BACK TO,0.2989913544668588,"˜λ˜k
< exp(−g(m)2 4
)."
GOING BACK TO,0.29971181556195964,"Therefore, we have that"
GOING BACK TO,0.3004322766570605,"rk = ˜N(d, ˜k) + ∞
X"
GOING BACK TO,0.3011527377521614,"j=1
N(d, ˜k + j)
˜λ˜k+j ˜λ˜k"
GOING BACK TO,0.30187319884726227,"= ˜N(d, ˜k) + j−1
X"
GOING BACK TO,0.3025936599423631,"j=1
N(d, ˜k + j)
˜λ˜k+j"
GOING BACK TO,0.30331412103746397,"˜λ˜k
+ ∞
X"
GOING BACK TO,0.30403458213256485,"j=j
N(d, ˜k + j)
˜λ˜k+j ˜λ˜k"
GOING BACK TO,0.3047550432276657,"≤nujd−1 + ∞
X"
GOING BACK TO,0.30547550432276654,"j=j
N(d, ˜k + j)
˜λ˜k+j"
GOING BACK TO,0.3061959654178674,"˜λ˜k
."
GOING BACK TO,0.3069164265129683,"Note that by Theorem 28, for j ≥l
√"
GOING BACK TO,0.3076368876080692,Tg(m) we have that
GOING BACK TO,0.30835734870317005,˜λ˜k+j
GOING BACK TO,0.3090778097982709,"˜k
< exp

−l2g(m)2 4 
."
GOING BACK TO,0.30979827089337175,"Note also that this bounds holds for all l simultaneously. Therefore, we can write"
GOING BACK TO,0.31051873198847263,"rk ≤nujd−1 + ∞
X"
GOING BACK TO,0.3112391930835735,"j=j
N(d, ˜k + j)
˜λ˜k+j ˜λ˜k"
GOING BACK TO,0.31195965417867433,"= nujd−1 + ∞
X l=1 j
X"
GOING BACK TO,0.3126801152737752,"s=1
N(d, lj + s)
˜λ˜k+lj+s ˜λ˜k"
GOING BACK TO,0.3134005763688761,"≤nujd−1 + ∞
X l=1 j
X"
GOING BACK TO,0.31412103746397696,"s=1
N(d, (l + 1)j)
˜λ˜k+lj ˜λ˜k"
GOING BACK TO,0.31484149855907784,"≤nujd−1 + ∞
X"
GOING BACK TO,0.31556195965417866,"l=1
(l + 1)d−1jd−1 exp(−l2g(m)2 4
)"
GOING BACK TO,0.31628242074927954,"≤m
g(m) t(m) d−1 nu + ∞
X"
GOING BACK TO,0.3170028818443804,"l=1
(l + 1)d−1 exp(−l2 4 ) !"
GOING BACK TO,0.3177233429394813,"≤Um
g(m) t(m)"
GOING BACK TO,0.3184438040345821,"d−1
= o(m)."
GOING BACK TO,0.319164265129683,"Second to last inequality is true because g(m) →∞as m →∞. The last inequality follows from
the fact that P∞
l=1(l + 1)d−1 exp(−l2"
GOING BACK TO,0.31988472622478387,"4 ) is bounded by a constant. Note that the bound is independent
of k, so we can vary ε with m as well. If we choose ε =
1
√m, the desired result follows."
GOING BACK TO,0.32060518731988474,"Part 3:
Let C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 . Note that the following two bounds hold for E0 [66]:"
GOING BACK TO,0.32132564841498557,1. For k < m such that rk + k > m
GOING BACK TO,0.32204610951008644,"E0 ≤

1 −k m"
GOING BACK TO,0.3227665706051873,"−1 
1 −
m
k + rk −1
."
GOING BACK TO,0.3234870317002882,2. For k ≥m it holds that
M,0.3242074927953891,"1
m X"
M,0.3249279538904899,"i
Li,δ ≥m k"
M,0.3256484149855908,"
k −m
k −m + rk 
."
M,0.32636887608069165,"Note that in this case c′
1m
1
d−1 ≤
√ T =
q"
M,0.3270893371757925,"2
τ 2
m ≤c′
2m
1
d−1 , where c′
2 =
q"
M,0.32780979827089335,"2
C2
1 (and similarly for"
M,0.3285302593659942,"indices 1 and 2 swapped with inequalities reversed).
Theorem 28 shows that for i is such that i ≤(k
√"
M,0.3292507204610951,"T), we have that for ˜λi it holds that"
M,0.329971181556196,"˜λi
˜λ1
≥"
M,0.33069164265129686,"1
1 +
k
√ T !k
√ T"
M,0.3314121037463977,≥exp(−k2).
M,0.33213256484149856,"Take k = (1 + ε)m. We want to show that there is a constant B > 0 that depends on the dimension
d and the constant C such that"
M,0.33285302593659943,rk ≤Bm.
M,0.3335734870317003,"Let ˜k = ((1 + ε)m)
1
d−1 . Note that by an analogous proof to Theorem 28, we have that for the size of
eigenvalues it holds"
M,0.33429394812680113,˜λ˜k+j
M,0.335014409221902,"˜λ˜k
≤"
M,0.3357348703170029,"T
T + (˜k + j −1 + d 2 −1 2) ! j−1 2
."
M,0.33645533141210376,"For l˜k ≤j ≤(l + 1)˜k, we have that"
M,0.3371757925072046,˜λ˜k+j
M,0.33789625360230546,"˜λ˜k
≤"
M,0.33861671469740634,"T
T + (˜k + j −1 + d 2 −1 2) ! j−1"
M,0.3393371757925072,"2
<

T
T + ((l + 1)˜k)"
M,0.3400576368876081, l˜k−1
M,0.3407780979827089,"2
<

T
T + ((l + 1)˜k)"
M,0.3414985590778098," (l+1)˜k 4
."
M,0.34221902017291067,We have that ˜k = 1
M,0.34293948126801155,"c′ (1+ε)
1
d−1 c′m
1
d−1 = 1"
M,0.34365994236311237,"c′ (1+ε)
1
d−1 √"
M,0.34438040345821325,"T = α
√"
M,0.3451008645533141,T for α = 1
M,0.345821325648415,"c′ (1+ε)
1
d−1 . Therefore,
we have that"
M,0.3465417867435158,˜λ˜k+j
M,0.3472622478386167,"˜λ˜k
<

T
T + ((l + 1)˜k)"
M,0.3479827089337176, (l+1)˜k
M,0.34870317002881845,"4
=

T
T + ((l + 1)α
√ T)"
M,0.34942363112391933," (l+1)α
√"
M,0.35014409221902015,"T
4
→exp

−(l + 1)2α2 4 
,"
M,0.35086455331412103,"as m →∞i.e.
√"
M,0.3515850144092219,"T →∞. Therefore, for m large enough, for all l we have that if l˜k ≤j ≤(l + 1)˜k
then
˜λ˜k+j"
M,0.3523054755043228,"˜λ˜k
≤
1
(l + 1)d+2 ."
M,0.3530259365994236,"The last inequality is true because we can take m large enough so that α
√"
M,0.3537463976945245,"T and
√"
M,0.35446685878962536,"T
α are both greater
than 400d. Then we have that for all l"
M,0.35518731988472624,˜λ˜k+j
M,0.3559077809798271,"˜λ˜k
<

T
T + ((l + 1)α
√ T)"
M,0.35662824207492794," (l+1)α
√ T
4
<"
M,0.3573487031700288,"1
1 + l+1"
D,0.3580691642651297,100d
D,0.35878962536023057,!(l+1)100d
D,0.3595100864553314,"<
1
(l + 1)d+2 ."
D,0.36023054755043227,"The last inequality holds because of the following

1 + l + 1"
D,0.36095100864553314,100d
D,0.361671469740634,"(l+1)100d
>
l + 1"
D,0.36239193083573484,100d
D,0.3631123919308357,"d+2 (l + 1)100d
d + 2"
D,0.3638328530259366,"
> (l + 1)d+2
1
(100d)d+2 (100d)d+2 > (l + 1)d+2."
D,0.3645533141210375,"Therefore, we can bound rk as follows"
D,0.36527377521613835,"rk =
P"
D,0.3659942363112392,i>k λk λk
D,0.36671469740634005,"= ˜N(d, ˜k) + ∞
X"
D,0.36743515850144093,"j=1
N(d, ˜k + j)
˜λ˜k+j ˜λ˜k"
D,0.3681556195965418,"< N(d, ˜k) + ∞
X"
D,0.3688760806916426,"j=1
N(d, ˜k + j)
˜λ˜k+j"
D,0.3695965417867435,"˜λ˜k
< (˜k)d−2 + ∞
X"
D,0.3703170028818444,"l=1
((l + 1)˜k)d−2(˜k)
1
(l + 1)d+2"
D,0.37103746397694526,"< ˜kd−1
 ∞
X l=1"
D,0.37175792507204614,"1
(l + 1)4 !"
D,0.37247838616714696,< ˜kd−1C = C(1 + ε)m.
D,0.37319884726224783,"Therefore, applying claim 1, we have that k −m + rk < (C(1 + ε) + ε)m, so we have that
k−m
k−m+rk >
ε
(C(1+ε)+ε), so it follows that E0 > (1 −
(1+ε)ε
(C(1+ε)+ε))−1 > 1."
D,0.3739193083573487,"Note that the above proof actually shows that for any k1 ≤k, we have that rk1 ≤Cm. Now we want
to show that rk is lower bounded and that we can take k < m such that k + rk > m. Again apply
Theorem 28, we can generalize"
D,0.3746397694524496,"˜λi
˜λ1
≥"
D,0.3753602305475504,"1
1 +
k
√ T !k
√ T"
D,0.3760806916426513,≥exp(−k2).
D,0.37680115273775217,"to other ratios as well. Let j ≤k
√"
D,0.37752161383285304,"T and let k
√"
D,0.37824207492795386,"T ≤i ≤(k + 1)
√"
D,0.37896253602305474,T. Then we have that
D,0.3796829971181556,"˜λi
˜λj
≥"
D,0.3804034582132565,"1
1 + 2k
√ T"
D,0.3811239193083574,"!(k+1)
√ T"
D,0.3818443804034582,≥exp(−2k(k + 1)).
D,0.38256484149855907,"Note that c′
1m
1
d−1 ≤
√ T =
q"
D,0.38328530259365995,"2
τ 2
m ≤c′
2m
1
d−1 , where c′
2 =
q"
D,0.3840057636887608,"2
C2
1 (and similarly for indices 1 and 2"
D,0.38472622478386165,"swapped with inequalities reversed). Therefore, we have that the following bound holds on rk for any
k < m:"
D,0.3854466858789625,"rk =
P"
D,0.3861671469740634,"i>k λi
λk+1
= ˜N(d,
˜
(k + 1)) + ∞
X"
D,0.3868876080691643,"j=1
N(d,
˜
(k + 1) + j)
˜λ
˜
(k+1)+j
˜λ
˜
(k+1)
."
D,0.38760806916426516,"Note that since
√"
D,0.388328530259366,"T ≥c′
1m
1
d−1 then N√"
D,0.38904899135446686,"T ≥nl(c′
1)d−1m, so ˜m ≤
1
c′
1
1 n"
D,0.38976945244956773,"1
d−1
l
. Therefore, if we take"
D,0.3904899135446686,"j = ˜k, l =
1
c′
1
1 n"
D,0.39121037463976943,"1
d−1
l
, and any l
√"
D,0.3919308357348703,"T ≤i ≤(l + 1)
√"
D,0.3926512968299712,"T, we have that (since exp(−2l(l + 1)) <"
D,0.39337175792507206,exp(−l2))
D,0.3940922190201729,"˜λi
˜λj
≥exp(−2l(l + 1))."
D,0.39481268011527376,"Note that there is at least N(l+1)
√"
D,0.39553314121037464,"T −Nl
√"
D,0.3962536023054755,"T ≥nl
 
(l + 1)d−1 −ld−1
m such indices i, we have
that"
D,0.3969740634005764,"rk =
P"
D,0.3976945244956772,"i>k λi
λk+1
= ˜N(d,
˜
(k + 1)) + ∞
X"
D,0.3984149855907781,"j=1
N(d,
˜
(k + 1) + j)
˜λ
˜
(k+1)+j
˜λ
˜
(k+1)
. >
X l
√"
D,0.39913544668587897,"T ≤i≤(l+1)
√"
D,0.39985590778097985,"T
N(d, i)
˜λi
˜λk"
D,0.40057636887608067,"> nl
 
(l + 1)d−1 −ld−1
m exp(−2l(l + 1)) > βm,"
D,0.40129682997118155,"where β = nl
 
(l + 1)d−1 −ld−1
exp(−2l(l + 1)). Note that β is independent of k, so in particular
for k = (1 −β/2)m, we will have rk + k > m. Therefore we can use the upper bound on E0 to see
that"
D,0.4020172910662824,E0 ≤22
D,0.4027377521613833,β2 (1 + β/2).
D,0.4034582132564842,"Since β only depends on d and C1 and C2, this finishes the proof."
D,0.404178674351585,"In particular, Lemma 18 shows that when τm
=
o(m−
1
d−1 ) or τm
=
ω(m−
1
d−1 ), then
lim sup ˜R( ˆf0) ≤∞."
D,0.4048991354466859,"Lower bound for the test risk of Gaussian KRR
Now we will show lower bounds for the test risk
of Gaussian KRR."
D,0.40561959654178675,"Lemma 19 (Lower bound for the test risk). Under Assumption 2, the following bounds hold for the
risk ˜R( ˆf0) of the minimum norm interpolating solution of Gaussian KRR:"
D,0.40634005763688763,"1. If τm = o(m−
1
d−1 ), then ˜
R(0) ≤lim inf ˜
R( ˆ
f0) ≤lim supm→∞˜
R( ˆ
f0) < ∞. More
precisely, if τm ≤m−
1
d−1 t(m), where t(m) →0 as m →∞, then there is a scalar cd that
depends only on the dimension so that for all such t(m) there is m0 that depends on t(m)
such that for all m > m0 we have ˜R( ˆf0) > σ2 + (1 −cdt(m)
d−1"
D,0.40706051873198845,2 )∥f ∗∥2.
D,0.40778097982708933,"2. If τm = ω(m−
1
d−1 ), then limm→∞˜
R( ˆ
f0) = ∞. More precisely, if τm ≥m−
1
d−1 t(m),
where t(m) →∞as m →∞then there is an integer m0 that depends on the dimension
and t(m) such that for m > m0, we have that
tR( ˆf0) →√m(σ2) (note that ˜R(0) is bounded). Hence, for large enough m we have
˜R( ˆf0) > ˜R(0)."
D,0.4085014409221902,"3. If τm = Θ(m−
1
d−1 ), then lim supm→∞R( ˆ
f0) < ∞.
Moreover, suppose that
C1m−
1
d−1 ≤τm ≤C2m−
1
d−1 for some constants C1 and C2, then there exist η, µ > 0 that
depend only on d, C1, and C2, such that for all m we have ˜
R( ˆ
f0) > µ∥f ∗∥2+(1+η)σ2.
Consequently, ˜
R( ˆ
f0) > ˜
R(0) as long as σ2 > 1−µ"
D,0.4092219020172911,η ∥f ∗∥2.
D,0.4099423631123919,Proof. All three proofs will similarly follow by directly analyzing Equation (2).
D,0.4106628242074928,"Part 1:
Note that for any i, we have that ˜λi + κ0 ≤˜λ1 + κ0, so we have that Li ≥
˜λi
˜λ1 L1. From"
D,0.41138328530259366,"Theorem 28 we know that for j ≤
√"
D,0.41210374639769454,"Tt(m)
1
2 , we have that"
D,0.4128242074927954,"˜λj
˜λ1
≥exp(−t(m)),"
D,0.41354466858789624,so there is m0 that depends on t(m) such that for all m > m0
D,0.4142651296829971,˜λj ≥1
D,0.414985590778098,"2
˜λ1,"
D,0.41570605187319887,"for all j ≤
√"
D,0.4164265129682997,"Tt(m)
1
2 . Therefore we have that m =
X"
D,0.41714697406340057,"i
N(d, i)Li >

N(d, 1) + . . . N(d,
√"
D,0.41786743515850144,"Tt(m))
 1 2L1."
D,0.4185878962536023,"Note that

N(d, 1) + . . . N(d,
√"
D,0.41930835734870314,"Tt(m))

> cl
√"
D,0.420028818443804,"Tt(m)
1
2
d−1
= c′
lm(t(m))−d−1"
D,0.4207492795389049,"2 , where c′
l =
√"
D,0.4214697406340058,"2cl. Therefore, we have that L1 < 2"
D,0.42219020172910665,"c′
l (t(m))
d−1"
D,0.4229106628242075,"2 . Therefore, we have that"
D,0.42363112391930835,"(1 −Li)2 ≥(1 −L1)2 >

1 −2"
D,0.4243515850144092,"c′
l
(t(m))
d−1 2

."
D,0.4250720461095101,"From Equation (2), we have that then"
D,0.4257925072046109,"˜R( ˆf0) ≥E0σ2 + E0(1 −cdt(m)
d−1"
D,0.4265129682997118,2 )∥β∥2.
D,0.4272334293948127,This shows the first claim.
D,0.42795389048991356,"For the second part, note the following property of Li,δ =
λi
λi+κδ : if for j > i, we have that"
D,0.42867435158501443,"Li,δ > Lj,δ. Let λj"
D,0.42939481268011526,"λi > c for some fixed c > 0 and j > i, then we have that"
D,0.43011527377521613,"Lj,δ > cLi,δ."
D,0.430835734870317,"By Theorem 28, we have that for j ≤
√"
D,0.4315561959654179,Tg(m) with g(m) →0 as m →∞
D,0.4322766570605187,˜λ˜k+j
D,0.4329971181556196,"˜λ˜k
> exp (−5g(m)) ."
D,0.43371757925072046,"So there is m > m0 such that for m > m0
˜λ˜k+j"
D,0.43443804034582134,"˜λ˜k
> exp (−5g(m)) > 1 2."
D,0.43515850144092216,"Note that
m
m −P∞
i=1 L2
i,δ
(1 −Lj,δ)2 > 1 if ∞
X"
D,0.43587896253602304,"i=1
L2
i,δ > mLj,δ (2 −Lj,δ)"
D,0.4365994236311239,"For this it suffices to have
∞
X"
D,0.4373198847262248,"i=1
L2
i,δ > mL1,δ (2 −L1,δ) ,"
D,0.43804034582132567,"since Li,δ are decreasing.
Note that there is at least N(d, 1) + · · · + N(d,
√"
D,0.4387608069164265,"Tg(m)) > nl(
√"
D,0.43948126801152737,"Tg(m))d−1 > cum

g(m)"
D,0.44020172910662825,"t(m)
d−1
of indices i such that"
D,0.4409221902017291,"λi
λ1
=
˜λ1+j"
D,0.44164265129682995,"˜λ1
> 1 2."
D,0.4423631123919308,"Therefore, if we select g(m) =
p"
D,0.4430835734870317,"t(m), then we have that ∞
X"
D,0.4438040345821326,"i=1
L2
i,δ > 1"
CUM,0.44452449567723346,"2cum

1
t(m)  d−1"
CUM,0.4452449567723343,"2
> mL1,δ (2 −L1,δ) ."
CUM,0.44596541786743515,"This implies that for all j simultaneously
m
m −P∞
i=1 L2
i,δ
(1 −Lj,δ)2 > 1."
CUM,0.44668587896253603,This translates to
CUM,0.4474063400576369,˜R( ˆf0) > ˜R(0) +
CUM,0.44812680115273773,"m
m −P∞
i=1 L2
i,δ
−1 ! σ2."
CUM,0.4488472622478386,This finishes the proof of the first part.
CUM,0.4495677233429395,"Part 2:
Note that we showed that for m large enough, E0 > √m. This shows that"
CUM,0.45028818443804036,˜R( ˆf0) = E0 X
CUM,0.4510086455331412,"i
(1 −L0)2v2
i + σ2
!"
CUM,0.45172910662824206,> √mσ2.
CUM,0.45244956772334294,"So, there is m0 that depends only on t(m) such that for m > m0 we have that ˜R( ˆf0) > √mσ2."
CUM,0.4531700288184438,"Part 3:
Note first that Li ≥Lj for i ≤j. Let T =
2
τ 2
m . From Lemma 26 it follows that"
CUM,0.4538904899135447,"nl(α
√"
CUM,0.4546109510086455,"T)d−1 ≤N(d, 1) + · · · + N(d, α
√"
CUM,0.4553314121037464,"T) ≤nu(α
√"
CUM,0.45605187319884727,T)d−1.
CUM,0.45677233429394815,"Take α such that N(d, 1) + · · · + N(d, α
√"
CUM,0.45749279538904897,"T) > 2m. Then, we have that m =
X"
CUM,0.45821325648414984,"i
N(d, i)Li > (N(d, 1) + · · · + N(d, α
√"
CUM,0.4589337175792507,"T))Lα
√"
CUM,0.4596541786743516,"T > 2mLα
√ T Lα
√ T ≤1"
CUM,0.4603746397694525,"2 =⇒λα
√"
CUM,0.4610951008645533,T ≤κ0.
CUM,0.4618155619596542,Note that (1 −L1)2 < (1 −Li)2 and
CUM,0.46253602305475505,"(1 −L1)2 =
κ2
0
(λ1 + κ0)2 ."
CUM,0.46325648414985593,"If κ0 > λ1, then 2κ0 > λ1 + κ0, so
1
2κ0 <
1
λ1+κ0 . This shows that
κ2
0
(λ1+κ0)2 > 1"
OTHERWISE,0.46397694524495675,"4. Otherwise
κ0 ≤λ1, so κ0 + λ1 ≤2λ1. This implies that"
OTHERWISE,0.46469740634005763,"κ2
0
(κ0 + λ1)2 > κ2
0
4λ2
1
> λα
√"
OTHERWISE,0.4654178674351585,"T
4λ2
1
."
OTHERWISE,0.4661383285302594,"From Theorem 28 it follows that
λα
√"
OTHERWISE,0.4668587896253602,"T
4λ2
1
> exp(−α2). Since

1
nu
2
C2
1"
OTHERWISE,0.4675792507204611,"
1
d−1
< α <

1
nl
2
C2
2"
OTHERWISE,0.46829971181556196,"
1
d−1
,"
OTHERWISE,0.46902017291066284,"we know that exp
 
−α2
> exp "
OTHERWISE,0.4697406340057637,"−

1
nl
2
C2
2"
OTHERWISE,0.47046109510086453,"
2
d−1 !"
OTHERWISE,0.4711815561959654,", so µ = exp "
OTHERWISE,0.4719020172910663,"−

1
nl
2
C2
2"
OTHERWISE,0.47262247838616717,"
2
d−1 !"
OTHERWISE,0.473342939481268,. Combining
OTHERWISE,0.47406340057636887,"Lemma 18 and Lemma 19, we get Theorem 3."
OTHERWISE,0.47478386167146974,"C
The case of increasing dimension"
OTHERWISE,0.4755043227665706,"In Appendix C.1 we will prove Theorem 7 and Theorem 9. In Appendix C.2, we will prove related
claims about the eigenvalue multiplicities for relevant scalings of the dimension Corollary 12 and
Corollary 13. In Appendix C.3, we will give a detailed proof of Corollary 13. In Appendix C.4, we
will discuss when the conditions posed on kernels are satisfied."
OTHERWISE,0.4762247838616715,"C.1
Proofs of Theorem 7 and Theorem 9"
OTHERWISE,0.4769452449567723,"Theorem 20 (Error scaling for any kernel ridge regression). Let d and m be any dimension and
sample size. Define Lm, Um, km, N(i), Nl, and ˜λk as in Definition 5. Consider KRR with a kernel
K satisfying Assumption 6 for some A. Assume that for some integer l, the target function f ∗
satisfies Assumption 4 with at most Nl nonzero coefficients. Then, the risk of the minimum norm
interpolating solution is bounded by the following:"
OTHERWISE,0.4776657060518732,"˜R( ˆf0) ≤

1 −Lm m"
OTHERWISE,0.4783861671469741,"−1 
1 −m Um"
OTHERWISE,0.47910662824207495,"−1
σ2
(8)"
OTHERWISE,0.47982708933717577,"+ B2

1 −Lm m"
OTHERWISE,0.48054755043227665,"−1 
1 −m Um"
OTHERWISE,0.4812680115273775,"−1 A2 m2 l
X"
OTHERWISE,0.4819884726224784,"i=1
N(i) 1 ˜λ2
i ! .
(9)"
OTHERWISE,0.4827089337175792,"Additionally,
we
can
get
an
alternative
bound
if
we
swap
Pl
i=1 N(d, i) 1 ˜λ2
i"
OTHERWISE,0.4834293948126801,"
with
Pl
i=1 N(d, i) 1 ˜λi"
OTHERWISE,0.484149855907781,"
, i.e."
OTHERWISE,0.48487031700288186,"˜R( ˆf0) ≤

1 −Lm m"
OTHERWISE,0.48559077809798273,"−1 
1 −m Lm"
OTHERWISE,0.48631123919308356,"−1
σ2
+B2

1 −Lm m"
OTHERWISE,0.48703170028818443,"−1 
1 −m Lm −1 A m2 Lm
X"
OTHERWISE,0.4877521613832853,"i=1
N(d, i) 1 ˜λi ! ."
OTHERWISE,0.4884726224783862,"Proofs of Theorem 7 and Theorem 20:
Note first that Equation (2) implies that"
OTHERWISE,0.489193083573487,"˜R( ˆf0) =
X"
OTHERWISE,0.4899135446685879,"i
E0 (1 −Li,0)2 β2
i + E0σ2."
OTHERWISE,0.49063400576368876,"Let Li =
˜λi
˜λi+κ0 . Then, we can rewrite it as"
OTHERWISE,0.49135446685878964,"˜R( ˆf0) =
X"
OTHERWISE,0.4920749279538905,"i
E0N(d, i) (1 −Li)2 β2
i + E0σ2,"
OTHERWISE,0.49279538904899134,"with a slight of abuse of notation for βi. From Equation (5), we have that for k < m"
OTHERWISE,0.4935158501440922,"E0 ≤

1 −k m"
OTHERWISE,0.4942363112391931,"−1 
1 −
m
k + rk −1
."
OTHERWISE,0.49495677233429397,"Take k = Lm. Note that by Definition 5, Lm < m Then λk+1 = ˜λkm+1 and it repeats N(d, km + 1)
times. Therefore, we have that"
OTHERWISE,0.4956772334293948,"rk = N(d, km + 1) + ∞
X"
OTHERWISE,0.49639769452449567,"i=1
N(d, km + i + 1)
˜λkm+i+1"
OTHERWISE,0.49711815561959655,"˜λkm+1
."
OTHERWISE,0.4978386167146974,"Therefore, rk + k > N(d, km + 1) + Lm = Lm > m. Therefore, we can apply Equation (5). We
get that"
OTHERWISE,0.49855907780979825,"E0 ≤

1 −k m"
OTHERWISE,0.4992795389048991,"−1 
1 −
m
k + rk"
OTHERWISE,0.5,"−1
=

1 −Lm m"
OTHERWISE,0.5007204610951008,"−1 
1 −m Lm −1
."
OTHERWISE,0.5014409221902018,"Note now that m =
X"
OTHERWISE,0.5021613832853026,"i
N(d, i)Li."
OTHERWISE,0.5028818443804035,"Note that ˜λi + κ0 > κ0 so we have that Li <
˜λi
κ0 . Therefore, we have that m =
X"
OTHERWISE,0.5036023054755043,"i
N(d, i)Li < 1 κ0 X"
OTHERWISE,0.5043227665706052,"i
N(d, i)˜λi ! < A κ0
."
OTHERWISE,0.5050432276657061,"Therefore, we have that κ0 < A"
OTHERWISE,0.5057636887608069,"m. Since (1 −Li)2 =
κ2
0
(κ0+˜λ)2 < κ2
0
˜λ2 . Since there are only ld nonzero
βi, we have that
X"
OTHERWISE,0.5064841498559077,"i
N(d, i)(1 −Li)2β2
i ≤B2 A2 m2 l
X"
OTHERWISE,0.5072046109510087,"i=1
N(d, i) 1 ˜λ2
i ! ."
OTHERWISE,0.5079250720461095,"Going back to Equation (2), we have
˜R( ˆf0) = E0
X"
OTHERWISE,0.5086455331412104,"i
(1 −Li)2N(d, i)β2
i + E0σ2"
OTHERWISE,0.5093659942363112,"≤E0B2 A2 m2 l
X"
OTHERWISE,0.5100864553314121,"i=1
N(d, i) 1 ˜λ2
i !"
OTHERWISE,0.510806916426513,+ E0σ2
OTHERWISE,0.5115273775216138,"≤

1 −Lm m"
OTHERWISE,0.5122478386167147,"−1 
1 −m Lm"
OTHERWISE,0.5129682997118156,"−1
σ2"
OTHERWISE,0.5136887608069164,"+

1 −Lm m"
OTHERWISE,0.5144092219020173,"−1 
1 −m Lm"
OTHERWISE,0.5151296829971181,"−1
B2 A2 m2 l
X"
OTHERWISE,0.515850144092219,"i=1
N(d, i) 1 ˜λ2
i !"
OTHERWISE,0.5165706051873199,"This shows the first bound. For the second bound, note that ∞
X i=1"
OTHERWISE,0.5172910662824207,"κ2
0
(κ0 + λi)2 β2
i ≤ ∞
X"
OTHERWISE,0.5180115273775217,"i=l+1
β2
i + l
X"
OTHERWISE,0.5187319884726225,"i=1
β2
i
1
P i λi 1
λi"
OTHERWISE,0.5194524495677233,"P
j>l λj m 2
."
OTHERWISE,0.5201729106628242,"So if we take β to have only l nonzero terms, we get l
X i=1"
OTHERWISE,0.520893371757925,"κ2
0
(κ0 + λi)2 β2
i ≤ l
X"
OTHERWISE,0.521613832853026,"i=1
β2
i
1
P i λi 1
λi P"
OTHERWISE,0.5223342939481268,"j>l λj m 2
."
OTHERWISE,0.5230547550432276,"From this, we have that l
X i=1"
OTHERWISE,0.5237752161383286,"κ2
0
(κ0 + λi)2 β2
i ≤A λi"
OTHERWISE,0.5244956772334294,"1
m2 ."
OTHERWISE,0.5252161383285303,"Therefore, we have that"
OTHERWISE,0.5259365994236311,"N(d, i)
κ2
0
(κ0 + ˜λi)2 β2
i ≤N(d, i)β2
i
A
λi"
OTHERWISE,0.526657060518732,"1
m2 ."
OTHERWISE,0.5273775216138329,"Therefore, we have the improved inequality from l
X"
OTHERWISE,0.5280979827089337,"i=1
N(d, i)(1 −Li)2β2
i ≤ l
X"
OTHERWISE,0.5288184438040345,"i=1
β2
i N(d, i) A λi"
OTHERWISE,0.5295389048991355,"1
m2 ."
OTHERWISE,0.5302593659942363,"This gives the second bound on the test risk, as we can bound l
X"
OTHERWISE,0.5309798270893372,"i=1
N(d, i)(1 −Li)2β2
i ≤B2A m2 l
X"
OTHERWISE,0.531700288184438,"i=1
N(d, i) 1 λi ! ."
OTHERWISE,0.5324207492795389,"[Lower bound for test risk in increasing dimension] Let km and Lm be as in Definition 5. Consider
learning a target function f ∗, with some sample size m. Let K be a kernel satisfying Assumption 8 for
some A and b. Consider the minimum norm interpolating solution of KRR (with any data distribution)
with kernel K. Then, for the risk of minimum norm interpolating solution, the following lower bound
holds:"
OTHERWISE,0.5331412103746398,˜R( ˆf0) > 
OTHERWISE,0.5338616714697406,"1 −

b
b + 1 2 Lm m !−1 σ2."
OTHERWISE,0.5345821325648416,"Proof of Theorem 9:
By Theorem 3 from Zhou et al. [66], if k is the first k < m such that
k + brk ≥m, then E0 ≥ "
OTHERWISE,0.5353025936599424,"1 −

b
b + 1 2 k m !−1 ."
OTHERWISE,0.5360230547550432,"Therefore, it suffices to show that the first such k is actually Lm = N(d, 1) + · · · + N(d, km). Note
that"
OTHERWISE,0.5367435158501441,"rk =
P"
OTHERWISE,0.537463976945245,"i>k λi
λk+1
< max
i≤k  1 λi 
(
X"
OTHERWISE,0.5381844380403458,"i>k
λi) < A max
i≤k  1 λi 
."
OTHERWISE,0.5389048991354467,"Since maxi≤km

1
˜λi"
OTHERWISE,0.5396253602305475,"
<
m−Lm"
OTHERWISE,0.5403458213256485,"b
, for l < N(d, 1) + · · · + N(d, km), we have that brl + l ≤"
OTHERWISE,0.5410662824207493,"N(d, 1) + · · · + N(d, km) −1 + b maxi≤km

1
˜λi"
OTHERWISE,0.5417867435158501,"
< Lm + m −Lm ≤m, so the first l for
which rl + l > m is l = Lm = N(d, 1) + · · · + N(d, km). Plugging in k = Lm we get that"
OTHERWISE,0.542507204610951,"E0 ≥

1 −

b
b+1

Lm"
OTHERWISE,0.5432276657060519,"m
−1
, so from Equation (6), we have ˜R( ˆf0) ≥

1 −

b
b+1
2 Lm m"
OTHERWISE,0.5439481268011528,"−1
σ2."
OTHERWISE,0.5446685878962536,"C.2
Dot-product kernels on the sphere"
OTHERWISE,0.5453890489913544,"First we will prove results about the eigenvalue multiplicites Corollary 12 and Corollary 13.
Theorem 21 (Log-scaling multiplicity). Let d = log2 m, i.e. m = 2d, and let kd be an index for
which the following holds:"
OTHERWISE,0.5461095100864554,"N(d, 1) + · · · + N(d, km) < m
N(d, 1) + · · · + N(d, km + 1) ≥m."
OTHERWISE,0.5468299711815562,Then the following hold:
OTHERWISE,0.547550432276657,"1.
dN(d, d"
OTHERWISE,0.5482708933717579,"5 )
2d
is decreasing and has limd→∞
dN(d, d"
OTHERWISE,0.5489913544668588,"5 )
2d
→0. This also holds for dN(d,k)"
"D
WITH",0.5497118155619597,"2d
with
k = f(d) ≤d"
"D
WITH",0.5504322766570605,5 for all d.
"D
WITH",0.5511527377521613,"2.
N(d, d"
"D
WITH",0.5518731988472623,"2 )
2d
is increasing and has limd→∞
N(d, d"
"D
WITH",0.5525936599423631,"2 )
2d
→∞.This also holds for N(d,k)"
D,0.553314121037464,"2d
with k =
f(d) ≥d"
D,0.5540345821325648,2 for all d.
D,0.5547550432276657,"3. There is an absolutet constant d0 such that for d > d0, we have that d"
D,0.5554755043227666,5 ≤km ≤d 2.
D,0.5561959654178674,"4. There are absolute constants cl−1 =
1
54, cl = 1"
D,0.5569164265129684,"9, and cl+1 = 2"
D,0.5576368876080692,"3 such that N(d, km + i) >
cl+im, for all i ∈{±1, 0}. Additionally, there are constants cu−1 = 1"
D,0.55835734870317,"3, cu = 1, cu+1 = 6,
such that N(d, km + i) < cu+im for all i ∈{±1, 0}."
D,0.5590778097982709,"Furthermore, if
d
log m = Θ(1), then we will also have Lm = Θ(m) and Lm = Θ(m)."
D,0.5597982708933718,"Proof. We will split the proof into three parts. Note first that N(d, k) increases as k increases. This
can be seen from the ratio of consecutive multiplicities"
D,0.5605187319884726,"N(d, k + 1)"
D,0.5612391930835735,"N(d, k)
=
2k + d
2k + d −2
k + d −2"
D,0.5619596541786743,"k + 1
."
D,0.5626801152737753,"Part 1:
We will use Stirling’s approximation to estimate N(d, k). It states that n! ≈
  n"
D,0.5634005763688761,"e
n √"
D,0.5641210374639769,"2πn.
Therefore, we have that"
D,0.5648414985590778,"N(d, k) = (2k + d −2)(k + d −3)!"
D,0.5655619596541787,"k!(d −2)!
≈ p"
D,0.5662824207492796,"2π(k + d −3)
  k+d−3"
D,0.5670028818443804,"e
k+d−3 (2k + d −2)
  k"
D,0.5677233429394812,"e
k   d−2"
D,0.5684438040345822,"e
d−2 √ k
√"
D,0.569164265129683,"d −2
."
D,0.5698847262247838,"Since N(d, k) is increasing in k and k ≤d"
D,0.5706051873198847,"5, so we can just plug in k = d"
D,0.5713256484149856,5 in the expression. Therefore
D,0.5720461095100865,"N(d, d 5) ≈ p"
D,0.5727665706051873,"2π(k + d −3)
  k+d−3"
D,0.5734870317002881,"e
k+d−3 (2k + d −2)
  k"
D,0.5742074927953891,"e
k   d−2"
D,0.5749279538904899,"e
d−2 √ k
√ d −2 = q 2π( d"
D,0.5756484149855908,"5 + d −3)
 d"
D,0.5763688760806917,"5 +d−3 e
 d"
D,0.5770893371757925,"5 +d−3
(2 d"
D,0.5778097982708934,"5 + d −2)
 d"
E,0.5785302593659942,"5
e
 d"
E,0.579250720461095,5   d−2
E,0.579971181556196,"e
d−2 q d
5
√ d −2 = C √"
E,0.5806916426512968,"ddd
d
5 +d−3  6 5 −3"
E,0.5814121037463977,"d
e
 6 5 d−3 √ d
  d"
E,0.5821325648414986,"5e
 d 5   d"
E,0.5828530259365994,"e
d−2 √ d
√ d"
E,0.5835734870317003,"= C 5
d
5   6"
E,0.5842939481268011,"5
 6d 5
√ d
="
E,0.5850144092219021," 
51/5( 6"
E,0.5857348703170029,"5)6/5d
√ d
."
E,0.5864553314121037,"Note that
 
51/5( 6"
E,0.5871757925072046,"5)6/5
< 1.8, so (51/5( 6"
E,0.5878962536023055,"5 )6/5)
d
√"
E,0.5886167146974063,"d
< 1.8d √"
E,0.5893371757925072,"d . In particular, we have that"
E,0.590057636887608,"dN(d, d"
E,0.590778097982709,"5)
2d
< √"
E,0.5914985590778098,d(1.8)d
D,0.5922190201729106,"2d
→0."
D,0.5929394812680115,"Part 2:
The same calculation in this case gives"
D,0.5936599423631124,"N(d, d 2) ≈C"
D,0.5943804034582133,"
2
1
2 ( 3"
D,0.5951008645533141,"2)
3
2
d √ d"
D,0.595821325648415,"but note that

2
1
2 ( 3"
D,0.5965417867435159,"2)
3
2

> 2.5, so N(d, d"
D,0.5972622478386167,"2 )
2d
>
  2.5"
D,0.5979827089337176,"2
d
1
√ d →∞."
D,0.5987031700288185,"Part 3:
Note that if km ≤d"
THEN FOR D LARGE ENOUGH,0.5994236311239193,5 then for d large enough
THEN FOR D LARGE ENOUGH,0.6001440922190202,"N(d, 1) + · · · + N(d, km) ≤kmN(d, km) ≤dN(d, km) < d."
THEN FOR D LARGE ENOUGH,0.600864553314121,"Similarly, note that if km > d"
THEN FOR D LARGE ENOUGH,0.6015850144092219,"2, then N(d, km) > d for d large enough. Both of these imply hat
d
5 ≤km ≤d"
THEN FOR D LARGE ENOUGH,0.6023054755043228,2 when d > d0 (d0 is determined by when the inequalities above start holding).
THEN FOR D LARGE ENOUGH,0.6030259365994236,"Part 4:
Note that when d"
THEN FOR D LARGE ENOUGH,0.6037463976945245,5 ≤k ≤d
THEN FOR D LARGE ENOUGH,0.6044668587896254,"2, we have that 6 > N(d,k+1)"
THEN FOR D LARGE ENOUGH,0.6051873198847262,"N(d,k)
=
2k+d
2k+d−2
k+d−2"
THEN FOR D LARGE ENOUGH,0.6059077809798271,"k+1
> 3 for d large"
THEN FOR D LARGE ENOUGH,0.6066282420749279,"enough, since the firs ratio is close to 1 and the second is 1 + d−2"
THEN FOR D LARGE ENOUGH,0.6073487031700289,"k+1. This implies that
N(d,k)
N(d,k+1) < 1"
THEN FOR D LARGE ENOUGH,0.6080691642651297,"3
whenever d is large enough (larger than an absolute constant) and k ≤d"
NOTE NOW THAT,0.6087896253602305,2. Note now that
NOTE NOW THAT,0.6095100864553314,"m < N(d, 1) + · · · + N(d, km + 1) < N(d, km + 1)

1 + 1"
NOTE NOW THAT,0.6102305475504323,3 + (1
NOTE NOW THAT,0.6109510086455331,"3)2 + . . .

< N(d, km + 1)3"
NOTE NOW THAT,0.611671469740634,"2
2
3m < N(d, km + 1)."
NOTE NOW THAT,0.6123919308357348,"This now implies that N(d, km) > 1"
NOTE NOW THAT,0.6131123919308358,"6N(d, km + 1) > 1"
NOTE NOW THAT,0.6138328530259366,"9m, N(d, km −1) >
1
54m. Similarly we
have that N(d, km) < m, so N(d, km + 1) < 6m and N(d, km −1) < 1 3m."
NOTE NOW THAT,0.6145533141210374,"Note that the same proof works even if d is not exactly equal to log2 m. This shows that whenever
d
log m = Θ(1), we have that Lm = Θ(m) and Lm = Θ(m)."
NOTE NOW THAT,0.6152737752161384,"Remark 22. Note that this does not say that we will always have tempered overfitting for
d
log m =
Θ(1) as sometimes it can happen that Lm →m, so we cannot apply Equation (5) and Equation (6).
In this case we expect the overfitting to be catastrophic."
NOTE NOW THAT,0.6159942363112392,"Theorem 23 (Sub-polynomial scaling multiplicity). Let K be any dot product kernel on the sphere
Sd−1 (with nonzero eigenvalues). Let l ∈N and let m = 222l and let d = 22l. This corresponds to
the case d = exp
 √log m

, which is sub-polynomial. Then, for the upper and lower index, Lm and
Lm, we have the following Lm"
NOTE NOW THAT,0.6167146974063401,"m ≤
3
2 log m
and
m
Lm
≤
1
d0.89 ."
NOTE NOW THAT,0.6174351585014409,"Additionally, for k = αkm, where α is a constant, we have that N(d, k) ≈mα, where ≈means up
to sub-polynomial factors."
NOTE NOW THAT,0.6181556195965417,Proof. Let km be the maximum k for which
NOTE NOW THAT,0.6188760806916427,"N(d, 1) + · · · + N(d, km) < m
N(d, 1) + · · · + N(d, km + 1) ≥m."
NOTE NOW THAT,0.6195965417867435,"We want to show that N(d, km) = o(m) and N(d, km + 1) = Ω(m). Then we can take k =
N(d, 1) + · · · + N(d, km) and so k"
NOTE NOW THAT,0.6203170028818443,m →0 and
NOTE NOW THAT,0.6210374639769453,"rk = N(d, km + 1) +
X"
NOTE NOW THAT,0.6217579250720461,"j=2
N(d, km + j)
˜λkm+j
˜λkm+1
= Ω(m)"
NOTE NOW THAT,0.622478386167147,"m
Rk
→0."
NOTE NOW THAT,0.6231988472622478,"Note that we just need to estimate N(d, km) precisely. Let k = 2l + l. We want to show that
N(d, k) > m. Note that by Stirling’s approximation n! ≈
  n"
NOTE NOW THAT,0.6239193083573487,"e
n √ 2πn"
NOTE NOW THAT,0.6246397694524496,"N(d, k) ≈ p"
NOTE NOW THAT,0.6253602305475504,"2π(k + d −3)
  k+d−3"
NOTE NOW THAT,0.6260806916426513,"e
k+d−3 (2k + d −2)
  k"
NOTE NOW THAT,0.6268011527377522,"e
k   d−2"
NOTE NOW THAT,0.627521613832853,"e
d−2 √ 2πk
p"
NOTE NOW THAT,0.6282420749279539,"2π(d −2) =
e
√ 2π"
NOTE NOW THAT,0.6289625360230547,"dk+d−2√ d
q"
NOTE NOW THAT,0.6296829971181557,2π(1 + k d −3
NOTE NOW THAT,0.6304034582132565,"d)
 
1 + k d −3"
NOTE NOW THAT,0.6311239193083573,"d
k+d−3  
1 + 2k d −2 d
"
NOTE NOW THAT,0.6318443804034583,"kk(d −2)d−2√ k
√ d −2 =
e
√ 2π"
NOTE NOW THAT,0.6325648414985591,"dk+d−2√ d
q"
NOTE NOW THAT,0.6332853025936599,2π(1 + 2l+l
NOTE NOW THAT,0.6340057636887608,"22l −
3
22l )

1 + 2l+l"
NOTE NOW THAT,0.6347262247838616,"22l −
3
22l
k+d−3  
1 + 2k d −2 d
"
NOTE NOW THAT,0.6354466858789626,kkdd−2(1 −2
NOTE NOW THAT,0.6361671469740634,"d)d−2√ k
√ d
q 1 −2 d =
e
√ 2π dk
q"
NOTE NOW THAT,0.6368876080691642,2π(1 + k d −3
NOTE NOW THAT,0.6376080691642652,"d)
 
1 + k d −3"
NOTE NOW THAT,0.638328530259366,"d
k+d−3  
1 + 2k d −2 d
 (1 −2"
NOTE NOW THAT,0.6390489913544669,"d)d−2
q 1 −2 d =
e
√ 2π q"
NOTE NOW THAT,0.6397694524495677,2π(1 + k d −3
NOTE NOW THAT,0.6404899135446686,"d)
 
1 + 2k d −2 d
 (1 −2"
NOTE NOW THAT,0.6412103746397695,"d)d−2
q 1 −2 d dk kk√ k"
NOTE NOW THAT,0.6419308357348703,"
1 + k d −3 d"
NOTE NOW THAT,0.6426512968299711,"k+d−3
."
NOTE NOW THAT,0.6433717579250721,"Only the term
dk kk√"
NOTE NOW THAT,0.6440922190201729,"k
 
1 + k d −3"
NOTE NOW THAT,0.6448126801152738,"d
k+d−3 is asymptotic here, as the rest tends to a constant as d →∞.
Note that since for all l > 5 (i.e. d)"
NOTE NOW THAT,0.6455331412103746,"2 >

1 + k d −3 d  d"
NOTE NOW THAT,0.6462536023054755,"k
> 1.95,"
NOTE NOW THAT,0.6469740634005764,"we have that

1 + k d −3 d"
NOTE NOW THAT,0.6476945244956772,"k+d−3
> (1.95)
k2"
NOTE NOW THAT,0.6484149855907781,d +k−3 k
NOTE NOW THAT,0.649135446685879,d > (1.95)k > 2(0.9k)
NOTE NOW THAT,0.6498559077809798,"
1 + k d −3 d"
NOTE NOW THAT,0.6505763688760807,"k+d−3
< 2
k2"
NOTE NOW THAT,0.6512968299711815,d +k−3 k d .
NOTE NOW THAT,0.6520172910662824,"Note that we have that then
dk kk√ k"
NOTE NOW THAT,0.6527377521613833,"
1 + k d −3 d"
NOTE NOW THAT,0.6534582132564841,"k+d−3
<
dk kk√"
NOTE NOW THAT,0.654178674351585,"k
2
k2"
NOTE NOW THAT,0.6548991354466859,d +k−3 k
NOTE NOW THAT,0.6556195965417867,"d
= 2k log d−k log k−1"
NOTE NOW THAT,0.6563400576368876,2 log k+ k2
NOTE NOW THAT,0.6570605187319885,d +k−3k d .
NOTE NOW THAT,0.6577809798270894,Consider now the expression k log d −k log k −1
NOTE NOW THAT,0.6585014409221902,2 log k + k2
NOTE NOW THAT,0.659221902017291,d + k −3k
NOTE NOW THAT,0.659942363112392,"d . For k = 2l + l −1, we have
that"
NOTE NOW THAT,0.6606628242074928,k log d −k log k −1
NOTE NOW THAT,0.6613832853025937,2 log k + k2
NOTE NOW THAT,0.6621037463976945,d + k −3k
NOTE NOW THAT,0.6628242074927954,d = k log d −k log k + k −1
NOTE NOW THAT,0.6635446685878963,2 log k −3k
NOTE NOW THAT,0.6642651296829971,d + k2 d
NOTE NOW THAT,0.6649855907780979,"=
 
2l + l −1

2l −
 
2l + (l −1)
 
l + l −1"
L,0.6657060518731989,"2l
+ o( 1"
L,0.6664265129682997,"22l )

−1"
L,0.6671469740634006,2(l) + o( 1
L,0.6678674351585014,2l ) + 2l + l −1 + o( 1 2l )
L,0.6685878962536023,= 22l + 2l(l −1) −2l(l) + 2l −l + 1 −1
L,0.6693083573487032,2l + l −1 + o(1) = 22l −1
L,0.670028818443804,2l = log2 m −log2 log2 m.
L,0.670749279538905,"In particular, this implies that for k ≤2l + (l −1), we have that"
L,0.6714697406340058,"N(d, k) < m2−1"
L,0.6721902017291066,2 l = o(m).
L,0.6729106628242075,"Now repeating the same computation for k = 2l + l, we have that dk kk√ k"
L,0.6736311239193083,"
1 + k d −3 d"
L,0.6743515850144092,"k+d−3
>
dk kk√"
L,0.6750720461095101,"k
20.9k
= 2k log d−k log k−1"
L,0.6757925072046109,2 log k+0.9k.
L,0.6765129682997119,k log d −k log k −1
L,0.6772334293948127,2 log k + k2
L,0.6779538904899135,d + 0.9k −3k
L,0.6786743515850144,d = k log d −k log k + k −1
L,0.6793948126801153,2 log k −3k
L,0.6801152737752162,d + k2 d
L,0.680835734870317,"=
 
2l + l

2l −
 
2l + l
 
l + l"
L,0.6815561959654178,2l + o( 1
L,0.6822766570605188,"22l )

−1"
L,0.6829971181556196,2(l) + o( 1
L,0.6837175792507204,2l ) + 0.9 · 2l + 0.9l + o( 1 2l )
L,0.6844380403458213,= 22l + 2l(l) −2l(l) + 0.9 · 2l −l −1
L,0.6851585014409222,2l + l + o(1) = 22l + 0.9 · 2l −1
L,0.6858789625360231,2l
L,0.6865994236311239,"= log2 m + 0.9 ·
p"
L,0.6873198847262247,log2 m −log2 log2 m.
L,0.6880403458213257,"This shows that for k ≥2l + l,"
L,0.6887608069164265,"N(d, k) > m20.9·2l−1"
L,0.6894812680115274,2 l = Ω(m).
L,0.6902017291066282,"To imply now that km = k = 2l + l −1, it suffices to show that N(d, 1) + · · · + N(d, k) = o(m).
But note that we have k ≪d, so"
L,0.6909221902017291,"N(d, k −1) = 2k + d −2"
L,0.69164265129683,"2k + d
k + 1
k + d −2N(d, k) < 1"
L,0.6923631123919308,"dN(d, k)"
L,0.6930835734870316,"N(d, 1) + · · · + N(d, k −1) < k −1"
L,0.6938040345821326,"d
N(d, k)"
L,0.6945244956772334,"N(d, 1) + · · · + N(d, k) = o(m)."
L,0.6952449567723343,"Therefore, we have computed with proof that km = 2l + l −1. So we can now compute Lm and Lm
with Lm = N(d, 1) + · · · + N(d, km). Note that
N(d, k + 1)"
L,0.6959654178674352,"N(d, k)
=
2k + d
2k + d −2
k + d −2"
L,0.696685878962536,"k + 1
."
L,0.6974063400576369,"Since k << d, note that N(d,k+1)"
L,0.6981268011527377,"N(d,k)
> 2, so we have that
N(d,k)
N(d,k+1) < 1"
L,0.6988472622478387,"2, which implies that"
L,0.6995677233429395,"N(d, 1) + N(d, 2) + · · · + N(d, km) < 3"
L,0.7002881844380403,"2N(d, km) = o(m)."
L,0.7010086455331412,"Therefore, we have that Lm"
L,0.7017291066282421,"m = N(d,1)+N(d,2)+···+N(d,km) m
< 3"
L,0.702449567723343,"2
N(d,km)"
L,0.7031700288184438,"m
≤
3
2 log m. From what we
computed previously, we have that"
L,0.7038904899135446,"Lm > N(d, k) > m20.9·2l−1"
L,0.7046109510086456,2 l
L,0.7053314121037464,"m
Lm
<
1"
L,0.7060518731988472,d0.9−1
L,0.7067723342939481,"2 log log dlog d <
1
d0.89 ."
L,0.707492795389049,"Note finally that for k = α2l, the leading term in the exponent of N(d, k) as derived above is α22l, all
other terms are at most l2l, which is sub-polynomial in m, so this shows that N(d, αk) ≈mα."
L,0.7082132564841499,"Remark 24. Note that for the polynomially increasing d, there are sequences (m, d) for which we do
not get benign overfitting, i.e. when d = mk, for integer k. Similarly in this case, there are sequences
of (m, d) for which we do not get benign overfitting, but is much harder to identify them."
L,0.7089337175792507,"C.3
Proof of Corollary 13"
L,0.7096541786743515,"Corollary 3 (Benign overfitting with Gaussian kernel and sub-polynomial dimension). Let K be the
Gaussian kernel on the sphere Sd−1 with a fixed bandwidth, and suppose that for l ∈N the dimension
and sample size scale as d = 22l, m = 222l, i.e. d = exp
 √log m

. Consider learning a sequence
of target functions f ∗
d as in Assumption 4 with Sd ≤m
1
4 . Then, we have that the minimum norm
interpolating solution achieves the Bayes error in the limit (m, d) →∞. In particular, for d ≥4 and
m ≥16 we have"
L,0.7103746397694525,"˜R( ˆf0) ≤

1 −
1
log m"
L,0.7110951008645533,"−1 
1 −exp

−0.89
p"
L,0.7118155619596542,"log m
−1
σ2 + 2B2 1 m."
L,0.712536023054755,"If we take Sd = poly(d), then we can improve the 1"
L,0.7132564841498559,"m error dependence to
1
m2−ε . Furthermore, we
can improve the bound on Sd by reducing the rate of convergence. Let sd be an integer such that
Sd ≤Nsd. Then error dependence is
1
m2 sdN(d, sd)d(1+ε)sd <
1
m2 d2(1+ε)sd."
L,0.7139769452449568,"Proof:
We have that from Theorem 23 that in this case km = 2l + l −1 = log d + log log d −1,
Lm = Θ(
m
log m) and Lm ≥Θ(md0.89). Therefore, we have that"
L,0.7146974063400576,"E0 ≤

1 −
1
log m"
L,0.7154178674351584,"−1 
1 −exp

−0.89
p"
L,0.7161383285302594,"log m
−1
."
L,0.7168587896253602,"To finish the proof, we need to estimate N(d, 1) 1"
L,0.7175792507204611,"˜λ2
1 +. . . N(d, km)
1
˜λ2
km
. Note that the eigenvalues are"
L,0.718299711815562,"sorted and N(d, k −1) = o(N(d, k + 1)) since km = o(d). This implies that it suffices to estimate
N(d, km)
1
˜λ2
km
. Note that for the first eigenvalue, we have from Corollary 31 for α = 1 +
2
τ 2
m ."
L,0.7190201729106628,"1
dα <˜λ1"
L,0.7197406340057637,"So, for ˜λk, it holds that
˜λk
˜λ1
> k
Y i=1"
L,0.7204610951008645,"2
τ 2
m
2
τ 2
m + 2(i + d"
L,0.7211815561959655,2 −1).
L,0.7219020172910663,"Therefore, we have that"
L,0.7226224783861671,"1
˜λk
< 1 ˜λ1 k
Y i=1"
L,0.723342939481268,"2
τ 2
m + 2(i + d 2 −1)"
L,0.7240634005763689,"2
τ 2
m
= dα
τ 2
m
2"
L,0.7247838616714697,"k Γ(d + 2km +
2
τ 2
m ) Γ( d"
L,0.7255043227665706,"2)Γ(d +
2
τ 2
m )
< dα  
τ 2
m
k d"
L,0.7262247838616714,"2 + k + 2 τ 2m k
."
L,0.7269452449567724,"Note also that if k = βkm for some constant β then
N(d, k) ≈mβ,"
L,0.7276657060518732,"in the sense of Theorem 23. So, N(d, km)
1
˜λ2
km
< mβd2k(1+ε)−2α. We want to take k as large as"
L,0.728386167146974,"possible, so we will take it k = βkm. Then d2k ≈m2β. Therefore, as long as 3β < 2, we will have
a polynomially scaling error. So we can take β = 2"
L,0.729106628242075,"3. Note that with Theorem 20, we actually get
even better dependence of the error, like mβdk(1+ε)−α. Plugging the first estimate into Theorem 7,
for 3α = 1, we have that"
L,0.7298270893371758,"˜R( ˆf0) ≤

1 −
1
log m"
L,0.7305475504322767,"−1 
1 −
1
d0.89"
L,0.7312680115273775,"−1
σ2"
L,0.7319884726224783,"+ B2

1 −
1
log m"
L,0.7327089337175793,"−1 
1 −
1
d0.89 −1 1"
L,0.7334293948126801,"m2 m3β
≤

1 −
1
log m"
L,0.734149855907781,"−1 
1 −
1
d0.89"
L,0.7348703170028819,"−1
σ2"
L,0.7355907780979827,"+ B2

1 −
1
log m"
L,0.7363112391930836,"−1 
1 −
1
d0.89 −1 1 m."
L,0.7370317002881844,"If we want to achieve Sd = poly(d), note that we just need to take k to not scale wit d. That is, if we
want deg Sd = n, then we can take k = n. The eigenvalue will be"
L,0.7377521613832853,"1
˜λn
< dα+2n."
L,0.7384726224783862,"The multiplicity will be N(d, n) = Θ(dn). Note that then error scales as d5n"
L,0.739193083573487,"m2 , i.e. since d is sub poly
m, we can take any such n."
L,0.7399135446685879,"C.4
Kernel conditions"
L,0.7406340057636888,"We have three main assumptions on kernels. First, the sum of its eigenvalues is bounded.
Assumption 6 (Bounded sum of eigenvalues). Assume that the kernel K has a bounded sum of
eigenvalues, i.e. there is a constant A such that P∞
i=1 N(i)˜λi ≤A. For a sequence of kernels K(d),
assume that all such A(d) are bounded by some constant A.2"
L,0.7413544668587896,"Second, there is a lower bound on the eigenvalues.
Assumption 8 (Lower bound on eigenvalues). Assumme that the kernel K has eigenvalues that
are not too small, i.e. there is a constant b such that maxi≤km

1
˜λi"
L,0.7420749279538905,"
< m−Lm"
L,0.7427953890489913,"b
. For a sequence of"
L,0.7435158501440923,"kernels K(d), assume that for the corresponding m = m(d) (since d = d(m), we can also ""invert""
the dependence) all such b(d) are bounded below by some b."
L,0.7442363112391931,"And third, the eigenvalues don’t decrease too fast.
Assumption 10 (Eigenvalue decay). The eigenvalues don’t decrease too quickly, i.e. for km as in
Definition 5, we have that there is a constant c such that maxi≤km

1
˜λi"
L,0.7449567723342939,"
≤cN(km). For increasing"
L,0.7456772334293948,"dimension, we require that maxi≤km

1
˜λi"
L,0.7463976945244957,"
≤cN(d, km)."
L,0.7471181556195965,"Note that Assumption 6 will hold quite broadly, in particular at least for dot product kernels on
the sphere (that are effectively the same kernel just in increasing dimensions). Note also that the
Assumption 10 is almost always stronger than Assumption 8. This follows from the definition of
Lm and km, i.e. since Lm = N(1) + · · · + N(km), we have that N(km) < m and as long as
Lm < Θ(m), we can take b large. This is usually the case (we show previously in Appendix C.2 for
dot-product kernels on the sphere). For d = ω(log m), we even have Lm = o(m). So in particular
N(d, km) < Θ(m −Lm) and for d = ω(log m), we have N(d, km) = o(m −Lm), so indeed
Assumption 10 is stronger than Assumption 8."
L,0.7478386167146974,"Assumption 6 and Assumption 10 are also taken to be true in the literature on the polynomially
scaling dimension [4, 64]."
L,0.7485590778097982,"First assumption:
For dot-product kernels on the sphere, we know that the eigenfunction ϕi are
actually the spherical harmonics Yks. Let K(x, y) = h(∥x −y∥). Note that for x ∈Sd−1, it holds
that"
L,0.7492795389048992,"N(d,k)
X"
L,0.75,"s=1
Yks(x)2 = N(d, k)."
L,0.7507204610951008,"Therefore, since"
L,0.7514409221902018,"K(x, y) =
X"
L,0.7521613832853026,"i
λiϕi"
L,0.7528818443804035,"h(0) = K(x, x) =
X k"
L,0.7536023054755043,"N(d,k)
X"
L,0.7543227665706052,"s=1
˜λkYks(x)2 =
X"
L,0.7550432276657061,"k
N(d, k)˜λk."
L,0.7557636887608069,"So the assumption in Theorem 7 holds with A = h(0). For the Gaussian kernel, this is A = 1.
Similarly, A = 1 for Laplace kernel."
L,0.7564841498559077,2Note that this assumption implicitly sets the scale of the kernel.
L,0.7572046109510087,"Second Assumption:
For the eigenvalue assumption, maxi≤km

1
˜λi"
L,0.7579250720461095,"
< m−Lm"
L,0.7586455331412104,"Ab
, note the following."
L,0.7593659942363112,"We will usually have Lm = o(m). In particular, whenever d = ω(log m) for a dot-product kernel on
the sphere this will hold."
L,0.7600864553314121,"The eigegnvalue assumption will hold for the Gaussian kernel in the non-integer polynomial regime,
dα = m. In Appendix C.3, we showed that
1
˜λk < dk, so since km = ⌊α⌋, we will have
1
˜λk < dα −"
L,0.760806916426513,"Θ(d⌊α⌋). Similarly, we can show this for the Gaussian kernel for d = log m and d = exp(√log m)
regime. Note that in Corollary 31, we showed that ˜λ1 >
1
dα and ˜λk >
1
dα+k , so then
1
˜λk Additionally,"
L,0.7615273775216138,"in the d = exp(√log m) regime, we showed that Lm = o(m) and that
1
˜λk = Θ(Lm) for any
k = Θ(km) < km. The same proof shows that this assumption holds for any sub-polynomially
scaling dimension and Gaussian kernel."
L,0.7622478386167147,"Furthermore, this assumption can be weakened to the following: there exists b > 0 such for all
k ≤Lm, b P"
L,0.7629682997118156,"i>k λi < λk+1(m −k). Additionally, since the assumption is specific to our approach,
it seems to be possible to weaken it even further."
L,0.7636887608069164,"Third assumption:
This assumption holds for dot-product kernels on the sphere. It was shown in
Zhang et al. [64] (Lemma 5.2.1) under another assumption similar to (1) that it holds for polynomially
scaling dimension. It is also assumed in Barzilai and Shamir [4] (page 11) for polynomially scaling
dimension. Furthermore, Cao et al. [9] (Theorem 4.3) shows that this holds for Neural Tangent Kernel
even more broadly, i.e. for all i and not only i ≤km."
L,0.7644092219020173,"D
Sharp bounds on the eigenvalues of the Gaussian kernel"
L,0.7651296829971181,"In this section, we will summarize and prove the results about eigenvalues of Gaussian and Laplace
kernels when X ∼Unif(Sd−1).
Given a positive semi-definite kernel function K : X × X →R, we can decompose it as"
L,0.765850144092219,"K(x, t) = ∞
X"
L,0.7665706051873199,"i=1
λkϕk(x)ϕk(t),"
L,0.7672910662824207,"where λk and ϕk are the eigenvalues and eigenfunctions of the operator associated to K, LK :
L2
DX (Sd−1) →L2
DX (Sd−1)"
L,0.7680115273775217,"LK(f)(x) =
Z"
L,0.7687319884726225,"X
K(x, t)f(t)dµ(t)."
L,0.7694524495677233,"LK is a Hilbert-Schmidt operator and has a countable system of non-negative eigenvalues λk
satisfying P∞
k=1 λ2
k < ∞. The corresponding L2
DX (Sd−1)-normalized eigenfunctions {ϕk(x)}∞
k=1
form an orthonormal basis of L2
DX (Sd−1). The eigenfunctions in this case are given by spherical
harmonics, Yi,s. The eigenvalues corresponding to all Yi,s for fixed i have the same eigenvalue, ˜λi.
Eigenvalues ˜λi are reach repeated N(d, i) = (2i+d−2)(i+d−3)!"
L,0.7701729106628242,"i!(d−2)!
times. So the sequence {λi}∞
i=1 =
˜λ1, . . . , ˜λ1, ˜λ2, . . . , ˜λ2, . . . . Using Funk-Hecke formula, we can compute ˜λk explicitly, both for
the case of Gaussian [46] kernel. For Gaussian kernel, we have the following theorem about the
eigenvalues ˜λk.
Theorem 25 (Eigenvalues of the Gaussian kernel [46]). Let X ∼Unif(Sd−1), with d ∈N and d ≥2."
L,0.770893371757925,"For K(x, t) = exp

−∥x−t∥2 τ 2
m"
L,0.771613832853026,"
, τm > 0, and k ∈N0 we have that"
L,0.7723342939481268,"˜λk = e
−
2
τ2m τ d−2
m
Ik+ d 2 −1  2 τ 2m"
L,0.7730547550432276,"
Γ
d 2 
."
L,0.7737752161383286,"Each ˜λk occurs with multiplicity N(d, k) = (2k+d−2)(k+d−3)!"
L,0.7744956772334294,"k!(d−2)!
(we use ˜λ notation to indicate that it
has multiplicity) and its corresponding eigenfunction are the spherical harmonics of order k on Sd−1.
Here, Iv(z), v, z ∈C is the modified Bessel function of the first kind"
L,0.7752161383285303,"Iv(z) = ∞
X j=0"
L,0.7759365994236311,"1
j!Γ(v + j + 1) z 2"
L,0.776657060518732,"v+2j
."
L,0.7773775216138329,"It will be useful to know the size of N(d, i). Also, the size of the sum of the first k multiplicities will
be useful. Let Nk = Pk
i=1 N(d, i)."
L,0.7780979827089337,"Lemma 26 (Size of multiplicity). For N(d, i) = (2k+d−2)(k+d−3)!"
L,0.7788184438040345,"k!(d−2)!
, it holds that"
L,0.7795389048991355,"1
(d −2)!kd−2 ≤N(d, i) ≤2d−1kd−2."
L,0.7802593659942363,"Additionally, there exist constants nl, nu > 0 depending on the dimension d such that Nk is bounded
below and above by the following"
L,0.7809798270893372,nlkd−1 ≤Nk ≤nukd−1.
L,0.781700288184438,"Proof. Since (2k + j) ≤(2k)j for j ≥2 and (k + 1) ≤2k, we have that"
L,0.7824207492795389,"N(d, k) = (2k + d −2)(k + d −3) . . . (k + 1)"
L,0.7831412103746398,"(d −2)!
≤(d −2)!(2k)d−3(2k)"
L,0.7838616714697406,"(d −2)!
≤2d−1kd−2 and"
L,0.7845821325648416,"N(d, k) = (2k + d −2)(k + d −3) . . . (k + 1)"
L,0.7853025936599424,"(d −2)!
≥
1
(d −2)!kd−2."
L,0.7860230547550432,"Note that by Bernoulli’s formula k
X"
L,0.7867435158501441,"l=1
ld−2 =
1
d −1
 
kd−1 + o(kd−1)

."
L,0.787463976945245,"Therefore, we have that"
L,0.7881844380403458,"nlkd−1 ≤
1
(d −1)(d −2)!
 
kd−1 + o(kd−1)

≤Nk ≤2d−1"
L,0.7889048991354467,"d −1
 
kd−1 + o(kd−1)

≤nukd−1."
L,0.7896253602305475,"Lemma 27 (Inverting the index). If the index of an eigenvalue ˆλj is such that Nk−1 ≤j ≤Nk −1,
then ˆλj = λk. We will denote such j with ˜k, i.e. ˜k is an index such that ˆλ˜k = λk."
L,0.7903458213256485,"Proof. Immediate from the fact that {λi}∞
i=1 is a sequence with ˜λi repeating N(d, i) times, in that
order."
L,0.7910662824207493,"An interesting property of eigenvalues of the Gaussian kernel is that they are sorted because the
Modified Bessel functions are [48]. In particular, Iv+1(x) < Iv(x) for all v, x > 0, so ˜λi+1 < ˜λi.
This is not the case for the Laplace kernel."
L,0.7917867435158501,Theorem 28 (Size of Ratios of Eigenvalues for Gaussian Kernel). Let T = ( 2
L,0.792507204610951,"τ 2
m ). For j ≤
√"
L,0.7932276657060519,"Tt(m), k ≤
√"
L,0.7939481268011528,"Tt(m), where t(m) →0 as m →∞, we have that"
L,0.7946685878962536,"exp(−6t(m)) <
˜λk+j"
L,0.7953890489913544,"˜λk
< 1."
L,0.7961095100864554,"For j ≥
√"
L,0.7968299711815562,"Tt(m), where t(m) →∞as m →∞we have that for any k ˜λk+j"
L,0.797550432276657,"˜λk
< exp

−t(m)2 4 
."
L,0.7982708933717579,"Proof. First of all, note that from [48], we have that for v ≥1 2"
L,0.7989913544668588,"(v) +
p"
L,0.7997118155619597,(v)2 + x2
L,0.8004322766570605,"x
> Iv−1(x)"
L,0.8011527377521613,"Iv(x)
>
(v −1"
L,0.8018731988472623,"2) +
q (v −1"
L,0.8025936599423631,"2)2 + x2 x
x"
L,0.803314121037464,"(v) +
p"
L,0.8040345821325648,(v)2 + x2 < Iv(x)
L,0.8047550432276657,"Iv−1(x) <
x (v −1"
L,0.8054755043227666,"2) +
q (v −1"
L,0.8061959654178674,2)2 + x2
L,0.8069164265129684,"In particular, this means that for the eigenvalues ˜λk, we have ( 2"
L,0.8076368876080692,"τ 2
m )"
L,0.80835734870317,(k + d
L,0.8090778097982709,"2) +
q"
L,0.8097982708933718,(k + d
L,0.8105187319884726,2)2 + ( 2
L,0.8112391930835735,"τ 2
m )2 <
˜λk+1"
L,0.8119596541786743,"˜λk
=
Ik+1+ d"
L,0.8126801152737753,2 −1( 2
L,0.8134005763688761,"τ 2
m ) Ik+ d"
L,0.8141210374639769,2 −1( 2
L,0.8148414985590778,"τ 2
m )
<
( 2"
L,0.8155619596541787,"τ 2
m )"
L,0.8162824207492796,(k + d 2 −1
L,0.8170028818443804,"2) +
q"
L,0.8177233429394812,(k + d 2 −1
L,0.8184438040345822,2)2 + ( 2
L,0.819164265129683,"τ 2
m )2 ."
L,0.8198847262247838,This can be bounded with a simpler expression as follows ( 2
L,0.8206051873198847,"τ 2
m )"
L,0.8213256484149856,2(k + d
L,0.8220461095100865,2) + ( 2
L,0.8227665706051873,"τ 2
m ) <
˜λk+1"
L,0.8234870317002881,"˜λk
=
Ik+1+ d"
L,0.8242074927953891,2 −1( 2
L,0.8249279538904899,"τ 2
m ) Ik+ d"
L,0.8256484149855908,2 −1( 2
L,0.8263688760806917,"τ 2
m )
<
( 2"
L,0.8270893371757925,"τ 2
m )"
L,0.8278097982708934,(k + d 2 −1
L,0.8285302593659942,2) + ( 2
L,0.829250720461095,"τ 2
m )."
L,0.829971181556196,"We will use these bounds to derive tight bounds for the ratios
˜λk+j"
L,0.8306916426512968,˜λk . Note the following jY i=1 ( 2
L,0.8314121037463977,"τ 2
m )"
L,0.8321325648414986,2(k + i −1 + d
L,0.8328530259365994,2) + ( 2
L,0.8335734870317003,"τ 2
m ) <
˜λk+j ˜λk
= jY i=1"
L,0.8342939481268011,"˜λk+i
˜λk+i−1
< jY i=1 ( 2"
L,0.8350144092219021,"τ 2
m )"
L,0.8357348703170029,(k + i −1 + d 2 −1
L,0.8364553314121037,2) + ( 2
L,0.8371757925072046,"τ 2
m )."
L,0.8378962536023055,"Note now that
 
( 2"
L,0.8386167146974063,"τ 2
m )"
L,0.8393371757925072,2(k + j −1 + d
L,0.840057636887608,2) + ( 2
L,0.840778097982709,"τ 2
m ) !j < jY i=1 ( 2"
L,0.8414985590778098,"τ 2
m )"
L,0.8422190201729106,2(k + i −1 + d
L,0.8429394812680115,2) + ( 2
L,0.8436599423631124,"τ 2
m ) <
˜λk+j ˜λk
."
L,0.8443804034582133,"Note that since (x + j −i)(x + i) = x2 + ix + (j −i)i, we have that (x + j −i)(x + i) ≥(x + j)x,
therefore jY i=1 ( 2"
L,0.8451008645533141,"τ 2
m )"
L,0.845821325648415,(k + i −1 + d 2 −1
L,0.8465417867435159,2) + ( 2
L,0.8472622478386167,"τ 2
m ) < ( 2"
L,0.8479827089337176,"τ 2
m )"
L,0.8487031700288185,(k + j −1 + d 2 −1
L,0.8494236311239193,2) + ( 2
L,0.8501440922190202,"τ 2
m ) ! j−1"
L,0.850864553314121,"2  
( 2"
L,0.8515850144092219,"τ 2
m )"
L,0.8523054755043228,(k + d 2 −1
L,0.8530259365994236,2) + ( 2
L,0.8537463976945245,"τ 2
m ) ! j+1 2
."
L,0.8544668587896254,"We use j + 1 and j −1 to account for the fact that j might be odd when we split into (j −1)/2 pairs.
When j is even, we split into j"
L,0.8551873198847262,2 pairs and use the fact that the term with exponent j+1
L,0.8559077809798271,"2
is larger."
L,0.8566282420749279,Let T = ( 2
L,0.8573487031700289,"τ 2
m ). We can bound the ratio
˜λk+j"
L,0.8580691642651297,"˜λk
tightly now."
L,0.8587896253602305,"When j ≤
√"
L,0.8595100864553314,"Tm−δ, k ≤
√"
L,0.8602305475504323,"Tm−δ, we have that"
L,0.8609510086455331,"exp(−6m−δ) <
˜λk+j"
L,0.861671469740634,"˜λk
< 1."
L,0.8623919308357348,"When j ≥
√"
L,0.8631123919308358,"Tmδ, we have that for any k ˜λk+j"
L,0.8638328530259366,"λk
< exp(−m2δ 4 )."
L,0.8645533141210374,"To see why this is true, note first that
 
( 2"
L,0.8652737752161384,"τ 2
m )"
L,0.8659942363112392,2(k + j −1 + d
L,0.8667146974063401,2) + ( 2
L,0.8674351585014409,"τ 2
m ) !j ="
L,0.8681556195965417,"T
2(k + j −1 + d"
L,0.8688760806916427,2) + T !j >
L,0.8695965417867435,"T
2(k + j −1 + d"
L,0.8703170028818443,2) + T !√ T
L,0.8710374639769453,">

T
5(
√"
L,0.8717579250720461,"Tm−δ) + T √ T
="
L,0.872478386167147,"1
5( 1
√"
L,0.8731988472622478,T m−δ) + 1 !√ T
L,0.8739193083573487,"→exp(−5m−δ),"
L,0.8746397694524496,"as m →∞. For the second inequality, note that ( 2"
L,0.8753602305475504,"τ 2
m )"
L,0.8760806916426513,(k + d 2 −1
L,0.8768011527377522,2) + ( 2
L,0.877521613832853,"τ 2
m ) ! j+1"
L,0.8782420749279539,"2
< 1."
L,0.8789625360230547,"We also have that
 
( 2"
L,0.8796829971181557,"τ 2
m )"
L,0.8804034582132565,(k + j −1 + d 2 −1
L,0.8811239193083573,2) + ( 2
L,0.8818443804034583,"τ 2
m ) ! j−1 2
="
L,0.8825648414985591,"T
(k + j −1 + d 2 −1"
L,0.8832853025936599,2) + T ! j−1
L,0.8840057636887608,"2
<

T
(
√"
L,0.8847262247838616,"Tmδ) + T 
√"
L,0.8854466858789626,"T mδ−1 2 = 1 (
√ T mδ) T
+ 1 !
√ T mδ 3
< 1 mδ
√ T + 1 !
√ T mδ 3
=  
 1 mδ
√ T + 1 !
√"
L,0.8861671469740634,"T
mδ
 
 m2δ 3"
L,0.8868876080691642,→exp(−m2δ
L,0.8876080691642652,"3 ) →0,"
L,0.888328530259366,"as m →∞. Note that we can turn the limits am →t(m) into inequalities of the form (1 −ε)t(m) <
am < (1 + ε)t(m) for some ε and all m since the convergence is uniform."
L,0.8890489913544669,The following is a simple corollary.
L,0.8897694524495677,Corollary 29. Let T = ( 2
L,0.8904899135446686,"τ 2
m ). If T > 1 and the index of the eigenvalue i is such that i ≤(k
√"
L,0.8912103746397695,"T)d−1,
we have that for λi it holds that"
L,0.8919308357348703,"λi
λ1
≥"
L,0.8926512968299711,"1
1 +
k
√ T !k
√ T"
L,0.8933717579250721,≥exp(−k2).
L,0.8940922190201729,"Proof. Note that

1
1+
k
√ T k
√"
L,0.8948126801152738,"T
is increasing in
√"
L,0.8955331412103746,"T and note that
√"
L,0.8962536023054755,T increases as m increases. Note
L,0.8969740634005764,"that for
√"
L,0.8976945244956772,"T = 1 it suffices to show e(k) > 1 + k which is true for all k as long as
√"
L,0.8984149855907781,T > 1.
L,0.899135446685879,The following simple bound also holds for eigenvalues of Gaussian kernel.
L,0.8998559077809798,"Proposition 30 (Ratio of eigenvalues bounded above [46]). For the eigenvalues associated to the
Gaussian Kernel ˜λk of bandwidth τm we have that ˜λk+1"
L,0.9005763688760807,"˜λk
<
1
τ 2m(k + d 2)."
L,0.9012968299711815,"It is straightforward to convert the bounds on ratios of eigenvalues Theorem 25 to bounds on the sizes
of the actual eigenvalues.
Corollary 31 (Sizes of the eigenvalues of Gaussian kernel). The following bounds hold for the
largest eigenvalue of the Gaussian kernel"
L,0.9020172910662824,"1
τ 2m + 4 Γ(( 2"
L,0.9027377521613833,"τ 2
m + 1"
L,0.9034582132564841,2))Γ( d 2) Γ( 2
L,0.904178674351585,"τ 2
m + d"
L,0.9048991354466859,"2 + 2)
< ˜λ1 <
1
p"
L,0.9056195965417867,τ 4m + 4τ 2m Γ(( 2
L,0.9063400576368876,"τ 2
m + 1"
L,0.9070605187319885,2))Γ( d 2) Γ( 2
L,0.9077809798270894,"τ 2
m + d 2 + 3 2) ."
L,0.9085014409221902,"Therefore, for ˜λk, we have that"
L,0.909221902017291,"1
τ 2k
m Γ(( 2"
L,0.909942363112392,"τ 2
m + k + 1 2)) Γ( 2"
L,0.9106628242074928,"τ 2
m + k + d"
L,0.9113832853025937,"2 + 2)
˜λ1 < ˜λk"
L,0.9121037463976945,˜λk < 2k 1
L,0.9128242074927954,"τ 2k
m Γ(( 2"
L,0.9135446685878963,"τ 2
m + k + 1 2)) Γ( 2"
L,0.9142651296829971,"τ 2
m + k + d 2 + 3"
L,0.9149855907780979,"2)
˜λ1."
L,0.9157060518731989,"Furthermore, for τm fixed we have that"
L,0.9164265129682997,"˜λ1 >
1
τ 2m + 4
1 (( d"
L,0.9171469740634006,"2 +
2
τ 2
m )
1+
2
τ2m )
. and"
L,0.9178674351585014,"˜λk >
1
τ 2m + 4
1 (( d"
L,0.9185878962536023,"2 +
2
τ 2
m )
1+
2
τ2m )"
L,0.9193083573487032,"1
τ 2k
m"
L,0.920028818443804,"1
((k + d"
L,0.920749279538905,"2 +
2
τ 2
m )k)."
L,0.9214697406340058,Proof. Note that from [62] we have
L,0.9221902017291066,"1
1 +
4
τ 2
m
exp
 2 τ 2m"
L,0.9229106628242075,"
< I0  2 τ 2m"
L,0.9236311239193083,"
<
1
q"
L,0.9243515850144092,"1 +
4
τ 2
m"
L,0.9250720461095101,"exp
 2 τ 2m "
L,0.9257925072046109,We also have from [48] ( 2
L,0.9265129682997119,"τ 2
m )"
L,0.9272334293948127,2(i + 1) + ( 2
L,0.9279538904899135,"τ 2
m ) <
Ii+1( 2"
L,0.9286743515850144,"τ 2
m ) Ii( 2"
L,0.9293948126801153,"τ 2
m )
<
( 2"
L,0.9301152737752162,"τ 2
m )"
L,0.930835734870317,2(i + 1 2). Then
L,0.9315561959654178,"˜λk = e
−
2
τ2m τ d−2
m
Ik+ d 2 −1  2 τ 2m"
L,0.9322766570605188,"
Γ
d 2 
."
L,0.9329971181556196,So then
L,0.9337175792507204,"1
τ dm Γ(( 2"
L,0.9344380403458213,"τ 2
m + 1 2)) Γ( 2"
L,0.9351585014409222,"τ 2
m + d"
L,0.9358789625360231,2 + 2) <
L,0.9365994236311239,"d
2 −1
Y i=0 ( 2"
L,0.9373198847262247,"τ 2
m )"
L,0.9380403458213257,2(i + 1) + ( 2
L,0.9387608069164265,"τ 2
m ) <
I1+ d"
L,0.9394812680115274,"2 −1

2
τ 2
m "
L,0.9402017291066282,"I0

2
τ 2
m 
<"
L,0.9409221902017291,"d
2 −1
Y i=0 ( 2"
L,0.94164265129683,"τ 2
m )"
L,0.9423631123919308,2(i + 1
L,0.9430835734870316,2) = 1 τ dm Γ( 1 2) Γ( d 2).
L,0.9438040345821326,So then
L,0.9445244956772334,"1
τ 2m + 4 Γ(( 2"
L,0.9452449567723343,"τ 2
m + 1"
L,0.9459654178674352,2))Γ( d 2) Γ( 2
L,0.946685878962536,"τ 2
m + d"
L,0.9474063400576369,"2 + 2)
< ˜λ1 <
1
p"
L,0.9481268011527377,τ 4m + 4τ 2m Γ(( 2
L,0.9488472622478387,"τ 2
m + 1"
L,0.9495677233429395,2))Γ( d 2) Γ( 2
L,0.9502881844380403,"τ 2
m + d 2 + 3 2) ."
L,0.9510086455331412,"By repeating the same argument, the claim about ˜λk follows."
L,0.9517291066282421,Note that
L,0.952449567723343,"˜λ1 >
1
τ 2m + 4Γ(d 2)"
L,0.9531700288184438,"d
2 −1
Y i=0"
L,0.9538904899135446,"1
(i + 1) + ( 2"
L,0.9546109510086456,"τ 2
m ) >
1
τ 2m + 4
1 (( d"
L,0.9553314121037464,"2 +
2
τ 2
m )
1+
2
τ2m )
."
L,0.9560518731988472,"Therefore, we have that"
L,0.9567723342939481,"˜λk > ˜λ1 k−1
Y i=0"
L,0.957492795389049,"2
τ 2
m
k + d"
L,0.9582132564841499,2 + ( 2
L,0.9589337175792507,"τ 2
m ) > ˜λ1
1
τ 2k
m"
L,0.9596541786743515,"1
((k + d"
L,0.9603746397694525,"2 +
2
τ 2
m )k)."
L,0.9610951008645533,NeurIPS Paper Checklist
CLAIMS,0.9618155619596542,1. Claims
CLAIMS,0.962536023054755,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.9632564841498559,Answer: [Yes]
CLAIMS,0.9639769452449568,"Justification: All the contributions are clearly stated in the introduction, particularly the third
paragraph of the introduction. The assumptions and limitations are also clearly stated in that
paragraph."
CLAIMS,0.9646974063400576,Guidelines:
CLAIMS,0.9654178674351584,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.9661383285302594,2. Limitations
LIMITATIONS,0.9668587896253602,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.9675792507204611,Answer: [Yes]
LIMITATIONS,0.968299711815562,"Justification: All assumptions are clearly and concisely stated. We also provide detailed
reasoning for each one of our assumptions. The assumptions are further discussed in the
appendix."
LIMITATIONS,0.9690201729106628,Guidelines:
LIMITATIONS,0.9697406340057637,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.9704610951008645,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.9711815561959655,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions are stated clearly before each result is presented. We give an
outline of proofs in the main body and complete proofs in the appendix.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.9719020172910663,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
THEORY ASSUMPTIONS AND PROOFS,0.9726224783861671,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The description of the experiments that are provided along with the figures is
sufficient to reproduce the experiments.
Guidelines:"
THEORY ASSUMPTIONS AND PROOFS,0.973342939481268,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9740634005763689,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.9747838616714697,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.9755043227665706,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9762247838616714,"Justification: We provide code and instructions required to reproduce our experimental
results."
OPEN ACCESS TO DATA AND CODE,0.9769452449567724,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9776657060518732,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.978386167146974,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.979106628242075,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9798270893371758,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9805475504322767,Justification: All the necessary details are included in the descriptions of the figures.
OPEN ACCESS TO DATA AND CODE,0.9812680115273775,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9819884726224783,"• . The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9827089337175793,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9834293948126801,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.984149855907781,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9848703170028819,"Justification: We report appropriate information about the statistical significance of the
experiments."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9855907780979827,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9863112391930836,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9870317002881844,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9877521613832853,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The compute resources are stated along the explanation of the experiments.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9884726224783862,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.989193083573487,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: Yes, the paper conforms to all aspects of the NeurIPS Code of Ethics.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9899135446685879,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9906340057636888,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: [NA]
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9913544668587896,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9920749279538905,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9927953890489913,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9935158501440923,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9942363112391931,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9949567723342939,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9956772334293948,"• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9963976945244957,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9971181556195965,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9978386167146974,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9985590778097982,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9992795389048992,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
