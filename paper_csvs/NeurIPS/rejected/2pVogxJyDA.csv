Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002150537634408602,"Diffusion-based generative models have exhibited remarkable capability in the
1"
ABSTRACT,0.004301075268817204,"production of high-fidelity visual content such as images and videos. However,
2"
ABSTRACT,0.0064516129032258064,"their performance is significantly contingent upon the quality of textual inputs,
3"
ABSTRACT,0.008602150537634409,"commonly referred to as ""prompts"". The process of traditional prompt engineering,
4"
ABSTRACT,0.010752688172043012,"while effective, necessitates empirical expertise and poses challenges for inexpe-
5"
ABSTRACT,0.012903225806451613,"rienced users. In this paper, we introduce PromptCoT, an innovative enhancer
6"
ABSTRACT,0.015053763440860216,"that autonomously refines prompts for users. The design of PromptCoT is based
7"
ABSTRACT,0.017204301075268817,"on the observation that, prompts resembling textual information corresponding to
8"
ABSTRACT,0.01935483870967742,"high-quality images within the training set tend to yield superior generation perfor-
9"
ABSTRACT,0.021505376344086023,"mance. As such, we fine-tune the pre-trained Large Language Models (LLM) using
10"
ABSTRACT,0.023655913978494623,"a curated text dataset comprising solely of high-quality visual content descriptions.
11"
ABSTRACT,0.025806451612903226,"By doing so, the LLM becomes capable of capturing the distribution of high-quality
12"
ABSTRACT,0.02795698924731183,"training texts, enabling it to generate aligned continuations and revisions to boost
13"
ABSTRACT,0.030107526881720432,"the original texts. Nonetheless, one drawback of pre-trained LLMs is their tendency
14"
ABSTRACT,0.03225806451612903,"to generate extraneous or irrelevant information. To enhance the alignment between
15"
ABSTRACT,0.034408602150537634,"the original text prompts and the refined counterparts, we leverage the Chain-of-
16"
ABSTRACT,0.03655913978494624,"Thought (CoT) mechanism. CoT can extract and amalgamate crucial information
17"
ABSTRACT,0.03870967741935484,"from the aligned continuation and revision, enabling reasonable inferences based
18"
ABSTRACT,0.04086021505376344,"on the contextual cues to produce a more comprehensive and nuanced final output.
19"
ABSTRACT,0.043010752688172046,"Considering computational efficiency, instead of allocating a dedicated LLM for
20"
ABSTRACT,0.04516129032258064,"prompt enhancement to each individual model or dataset, we integrate adapters
21"
ABSTRACT,0.047311827956989246,"that facilitate dataset-specific adaptation, leveraging a shared pre-trained LLM as
22"
ABSTRACT,0.04946236559139785,"the foundation for this process. By fine-tuning these adapters independently, we
23"
ABSTRACT,0.05161290322580645,"can adapt PromptCoT to new datasets with minimal increase in training cost and
24"
ABSTRACT,0.053763440860215055,"memory usage. We assess the performance of PromptCoT on widely-used latent
25"
ABSTRACT,0.05591397849462366,"diffusion models for image and video generation to validate the effectiveness. The
26"
ABSTRACT,0.05806451612903226,"results demonstrate significant improvements in key performance metrics.
27"
INTRODUCTION,0.060215053763440864,"1
Introduction
28"
INTRODUCTION,0.06236559139784946,"In recent years, deep generative models have made notable advancements, specifically with the
29"
INTRODUCTION,0.06451612903225806,"introduction of diffusion probabilistic models (DPMs). These models have exhibited exceptional
30"
INTRODUCTION,0.06666666666666667,"capabilities in generating a wide range of visually compelling and high-fidelity visual contents, such
31"
INTRODUCTION,0.06881720430107527,"as images and videos, as evidenced by notable contributions in the literature [37, 12, 38, 36, 7, 28,
32"
INTRODUCTION,0.07096774193548387,"32, 30].
33"
INTRODUCTION,0.07311827956989247,"By harnessing textual inputs as conditional guidance, diffusion models have the ability to generate
34"
INTRODUCTION,0.07526881720430108,"visual outputs that align with the corresponding input text, utilizing an iterative denoising procedure.
35"
INTRODUCTION,0.07741935483870968,"This technological advancement has paved the way for revolutionary applications, including notable
36"
INTRODUCTION,0.07956989247311828,"examples such as DALL-E 2 [28], Stable Diffusion [30], MagicVideo [50], among others.
37"
INTRODUCTION,0.08172043010752689,"(a)
(b)
(c)
(d)"
INTRODUCTION,0.08387096774193549,"Figure 1: Impacts of PromptCoT. (a) and (c) shows the images generated with the original text
prompts, and (b) and (d) show the images generated with the text prompts refined by PromptCoT.
The text prompt for (a), (b), (c) and (d) are: 1) ""highly detailed portrait of a hopeful pretty astronaut
lady with a wavy blonde hair, by Jamini Roy , 4k resolution, nier:automata inspired, bravely default
inspired, vibrant but dreary but uplifting red, black and white color scheme!!! ((Space nebula
background))"" ; 2) ""Astronaut portrait of Silica from the game Bravely Default II by Jamini Roy"",
and 3) ""highly detailed portrait of a hopeful pretty astronaut lady with a wavy blonde hair, by Pablo
Picasso, 4k resolution, nier:automata inspired, bravely default inspired, vibrant but dreary but uplifting
red, black and white color scheme!!! ((Space nebula background))"",and 4)""Portrait Of A Beautiful
Astronaut Girl Canvas Art Print"" respectively."
INTRODUCTION,0.08602150537634409,"Nevertheless, the quality of the generated content is intricately tied to the caliber of the textual
38"
INTRODUCTION,0.08817204301075268,"prompts provided to the generative model. Human inputs tend to be informal and straightforward,
39"
INTRODUCTION,0.09032258064516129,"which may impede the expression of the desired scene with the desired level of depth. Additionally,
40"
INTRODUCTION,0.09247311827956989,"the text encoder within the generative model may not fully comprehend the semantic nuances present
41"
INTRODUCTION,0.09462365591397849,"in the human-generated text, resulting in notable disparities between the encoded textual guidance
42"
INTRODUCTION,0.0967741935483871,"and the user’s intended meaning. Diffusion probabilistic models (DPMs) are commonly trained on
43"
INTRODUCTION,0.0989247311827957,"extensive text-vision pairs acquired through web-scraping techniques [35]. Our observation reveals
44"
INTRODUCTION,0.1010752688172043,"that the distribution of the text dataset might not be congruent with the linguistic style employed by
45"
INTRODUCTION,0.1032258064516129,"layman users. Furthermore, even in cases where the training text data aligns with the desired style,
46"
INTRODUCTION,0.1053763440860215,"the quality can exhibit substantial variations due to the presence of meaningless words or extraneous
47"
INTRODUCTION,0.10752688172043011,"information within the text data. This intricacy further complicates the establishment of a clear and
48"
INTRODUCTION,0.10967741935483871,"unambiguous mapping between the text and the corresponding image.
49"
INTRODUCTION,0.11182795698924732,"As a result, there is an immediate imperative to develop a methodology that can effectively align
50"
INTRODUCTION,0.11397849462365592,"prompts, consequently augmenting the image generation performance in generative models. Although
51"
INTRODUCTION,0.11612903225806452,"data cleaning and model fine-tuning have been considered potential solutions, these methods often
52"
INTRODUCTION,0.11827956989247312,"entail drawbacks such as high costs, instability, and time intensiveness. Another alternative is manual
53"
INTRODUCTION,0.12043010752688173,"prompt engineering, which involves refining prompts to optimize generation performance. However,
54"
INTRODUCTION,0.12258064516129032,"this empirical task traditionally demands the expertise of experienced professionals, thereby posing a
55"
INTRODUCTION,0.12473118279569892,"significant challenge for individuals lacking relevant experience.
56"
INTRODUCTION,0.12688172043010754,"In our study, we observe a noticeable trend that prompts, which resemble those found in the training
57"
INTRODUCTION,0.12903225806451613,"set, usually lead to superior generative performance. Stemming from this observation, we propose
58"
INTRODUCTION,0.13118279569892474,"PromptCoT, a novel prompt booster that leverages the power of pre-trained Large Language Models
59"
INTRODUCTION,0.13333333333333333,"(LLMs) and incorporates the Chain-of-Thought (CoT) mechanism to learn high-quality prompt
60"
INTRODUCTION,0.13548387096774195,"expressions from the training texts of generative models. Specifically, we carry out the fine-tuning
61"
INTRODUCTION,0.13763440860215054,"of LLaMA [40], a widely-used pre-trained Large Language Model, on two distinct datasets we’ve
62"
INTRODUCTION,0.13978494623655913,"prepared. With a text-continuation dataset that appends aligned details to original prompts, and a
63"
INTRODUCTION,0.14193548387096774,"text-revision dataset that rewrites original prompts to aligned prompts, we enable LLaMA to refine
64"
INTRODUCTION,0.14408602150537633,"prompts that better match the distribution of the text data used for training the diffusion models. To
65"
INTRODUCTION,0.14623655913978495,"further enhance the performance of LLMs by combining the advantages of both text-continuation
66"
INTRODUCTION,0.14838709677419354,"and text-revision, we construct a dataset using the CoT mechanism assisted by ChatGPT. This CoT
67"
INTRODUCTION,0.15053763440860216,"dataset is designed to enable LLMs to reason and generate text that follows a logical and coherent
68"
INTRODUCTION,0.15268817204301074,"flow. By fine-tuning LLMs on this CoT dataset, we can enhance their reasoning ability and augments
69"
INTRODUCTION,0.15483870967741936,"their capacity to generate high-quality text that is both contextually relevant and logically coherent.
70"
INTRODUCTION,0.15698924731182795,"To accommodate the varying training sets of different generative models, we incorporate a parameter-
71"
INTRODUCTION,0.15913978494623657,"efficient adaptation design into the training pipeline of PromptCoT, augmenting a pre-trained base
72"
INTRODUCTION,0.16129032258064516,"booster with specific lightweight adapters that are capable of aligning text distributions for various
73"
INTRODUCTION,0.16344086021505377,"generative models across multiple tasks. We demonstrate the effectiveness of PromptCoT through
74"
INTRODUCTION,0.16559139784946236,"extensive experiments on widely-used latent diffusion models for image and video generation,
75"
INTRODUCTION,0.16774193548387098,"showing significant improvements in key performance metrics such as Fréchet Inception Distance,
76"
INTRODUCTION,0.16989247311827957,"aesthetic score, and CLIP-similarity.
77"
INTRODUCTION,0.17204301075268819,"Our main contributions are:
78"
INTRODUCTION,0.17419354838709677,"• We propose PromptCoT, an innovative prompt refiner that aligns input prompts with the text
79"
INTRODUCTION,0.17634408602150536,"distribution employed during the training of diffusion models. By accomplishing this alignment,
80"
INTRODUCTION,0.17849462365591398,"PromptCoT effectively activates generative models and enhances their performance.
81"
INTRODUCTION,0.18064516129032257,"• We explore a new optimization scheme for improving prompt quality by leveraging the power
82"
INTRODUCTION,0.1827956989247312,"of pre-trained LLMs and CoT mechanisms. And we construct datasets to facilitate the learning of
83"
INTRODUCTION,0.18494623655913978,"high-quality prompt distribution from the training texts of generative models.
84"
INTRODUCTION,0.1870967741935484,"• We demonstrate that allocating a dedicated Large Language Model (LLM) for each diffusion
85"
INTRODUCTION,0.18924731182795698,"model is not a requirement. Instead, we propose an innovative scheme where a set of lightweight
86"
INTRODUCTION,0.1913978494623656,"adapter weights suffices for each dedicated diffusion model. These adapters can share a shared base
87"
INTRODUCTION,0.1935483870967742,"pre-trained LLM, resulting in a considerable reduction in memory footprint.
88"
INTRODUCTION,0.1956989247311828,"• We show the effectiveness of PromptCoT through extensive experiments on widely-used latent dif-
89"
INTRODUCTION,0.1978494623655914,"fusion models for image and video generation, showing significant improvements in key performance
90"
INTRODUCTION,0.2,"metrics.
91"
RELATED WORK,0.2021505376344086,"2
Related Work
92"
TEXT-TO-IMAGE GENERATIVE MODELS,0.20430107526881722,"2.1
Text-to-Image Generative Models
93"
TEXT-TO-IMAGE GENERATIVE MODELS,0.2064516129032258,"Text-to-Image Generative Models operate by taking natural language descriptions as input and
94"
TEXT-TO-IMAGE GENERATIVE MODELS,0.2086021505376344,"generating corresponding images as output. One of the recent popular model is DALL·E 2 [29].
95"
TEXT-TO-IMAGE GENERATIVE MODELS,0.210752688172043,"It utilize CLIP [26] to align the text and image embeddings. By conditioning the diffusion prob-
96"
TEXT-TO-IMAGE GENERATIVE MODELS,0.2129032258064516,"abilistic generator on the textual embedding, DALL·E 2 is able to produce photorealistic images
97"
TEXT-TO-IMAGE GENERATIVE MODELS,0.21505376344086022,"that correspond to the given textual description. Later, Google’s Imagen [32] and Parti [46] were
98"
TEXT-TO-IMAGE GENERATIVE MODELS,0.2172043010752688,"proposed by gradually simulating the spread of noise into the original image to reveal the desired
99"
TEXT-TO-IMAGE GENERATIVE MODELS,0.21935483870967742,"image. Specifically, both Parti and Imagen combine autoregressive and diffusion. The application
100"
TEXT-TO-IMAGE GENERATIVE MODELS,0.221505376344086,"of diffusion probabilistic models has also been extended to the domain of video generation. The
101"
TEXT-TO-IMAGE GENERATIVE MODELS,0.22365591397849463,"Video Diffusion Model [13], built upon the foundations of diffusion models, enables the sequential
102"
TEXT-TO-IMAGE GENERATIVE MODELS,0.22580645161290322,"generation of high-quality video frames. To address the substantial computational requirements
103"
TEXT-TO-IMAGE GENERATIVE MODELS,0.22795698924731184,"associated with video generation, MagicVideo [51] was introduced, combining latent diffusion and
104"
TEXT-TO-IMAGE GENERATIVE MODELS,0.23010752688172043,"attention models. MagicVideo utilizes a frame-wise lightweight adapter and an attention module to
105"
TEXT-TO-IMAGE GENERATIVE MODELS,0.23225806451612904,"effectively adjust the image-to-video distribution and capture temporal dependencies across frames.
106"
LARGE LANGUAGE MODELS,0.23440860215053763,"2.2
Large Language Models
107"
LARGE LANGUAGE MODELS,0.23655913978494625,"Large Language Models (LLMs) are powerful deep learning models for various natural language
108"
LARGE LANGUAGE MODELS,0.23870967741935484,"processing tasks. The most popular LLMs are the GPT [27, 5] series models developed by OpenAI,
109"
LARGE LANGUAGE MODELS,0.24086021505376345,"which are based on the decoder component of the transformer architecture. Another LLM is Meta’s
110"
LARGE LANGUAGE MODELS,0.24301075268817204,"OPT [49], which is open-sourced and performs similarly in performance to GPT-3. However, GPT-3’s
111"
LARGE LANGUAGE MODELS,0.24516129032258063,"massive size of 175B parameters requires significant computing power and resources, which makes
112"
LARGE LANGUAGE MODELS,0.24731182795698925,"it challenging for researchers to explore. In contrast, LLaMA [40, 41], StableLM [2], as well as
113"
LARGE LANGUAGE MODELS,0.24946236559139784,"the instruction-following Alpaca model [39] are smaller and more performant, achieve comparable
114"
LARGE LANGUAGE MODELS,0.25161290322580643,"results to ChatGPT with far fewer parameters (7B). For specific tasks like conversational applications,
115"
LARGE LANGUAGE MODELS,0.2537634408602151,"ChatGLM [47, 9] can generate coherent and contextually relevant responses in dialogue systems.
116"
PARAMETER-EFFICIENT FINE-TUNING,0.25591397849462366,"2.3
Parameter-Efficient Fine-Tuning
117"
PARAMETER-EFFICIENT FINE-TUNING,0.25806451612903225,"The goal of parameter-efficient fine-tuning is to attain comparable performance to fine-tuning on a
118"
PARAMETER-EFFICIENT FINE-TUNING,0.26021505376344084,"specific downstream task while using the fewest trainable parameters possible. According to [1],
119"
PARAMETER-EFFICIENT FINE-TUNING,0.2623655913978495,"common pre-trained models generally have a very low intrinsic dimension, and LoRA [15] learns
120"
PARAMETER-EFFICIENT FINE-TUNING,0.2645161290322581,"low-rank parameterizations to enhance tuning efficiency based on that. Except reducing the number
121"
PARAMETER-EFFICIENT FINE-TUNING,0.26666666666666666,"of parameters needed for fine-tuning, other approaches try to attach pre-trained parameters to reduce
122"
PARAMETER-EFFICIENT FINE-TUNING,0.26881720430107525,"training time. Adapter training [14, 24] utilizes dynamic pre-trained adapters for different tasks and
123"
PARAMETER-EFFICIENT FINE-TUNING,0.2709677419354839,"languages to reduce adaptation time. Compacter [21] combines both concepts and builds on top of
124"
PARAMETER-EFFICIENT FINE-TUNING,0.2731182795698925,"adapters, low-rank optimization, and parameterized hypercomplex multiplication layers.
125"
PROMPT ENGINEERING,0.2752688172043011,"2.4
Prompt Engineering
126"
PROMPT ENGINEERING,0.27741935483870966,"Prompt Engineering is to optimize the outputs of language models with specific input prompts
127"
PROMPT ENGINEERING,0.27956989247311825,"[4, 33, 20, 8]. Discrete text prompts [16] serve as starting points for the model’s language generation,
128"
PROMPT ENGINEERING,0.2817204301075269,"and are used to generate responses in dialogue systems. Beyond discrete prompts, [17, 43] explores
129"
PROMPT ENGINEERING,0.2838709677419355,"prompt tuning to learn soft prompts to perform specific downstream tasks, which provide more
130"
PROMPT ENGINEERING,0.2860215053763441,"context-aware guidance to the model. [25] extends the idea of learning soft prompts and demonstrates
131"
PROMPT ENGINEERING,0.28817204301075267,"that the implicit factual knowledge in language models was underestimated. Given that manually
132"
PROMPT ENGINEERING,0.2903225806451613,"designing prompts can be cumbersome, automatically generating prompts gives a chance avoid
133"
PROMPT ENGINEERING,0.2924731182795699,"intensive labor and enhance efficiency [33, 34]. [10] proposes to generate all prompt candidates
134"
PROMPT ENGINEERING,0.2946236559139785,"and selectively incorporate them into each context using a refined strategy. [11] introduces a more
135"
PROMPT ENGINEERING,0.2967741935483871,"efficient method to construct prompts with several sub-prompts that employs prompt tuning with
136"
PROMPT ENGINEERING,0.2989247311827957,"rules without searching. Overall, prompt engineering is an efficient approach that helps bridge the
137"
PROMPT ENGINEERING,0.3010752688172043,"gap between pre-training and fine-tuning.
138"
CHAIN-OF-THOUGHT,0.3032258064516129,"2.5
Chain-of-Thought
139"
CHAIN-OF-THOUGHT,0.3053763440860215,"Chain-of-Thought is a specialized tool designed for the task of multi-step reasoning and decision-
140"
CHAIN-OF-THOUGHT,0.30752688172043013,"making [44]. The traditional prompting method [4] performs poorly when it comes to tasks that
141"
CHAIN-OF-THOUGHT,0.3096774193548387,"require reasoning abilities. Inspired by the concept of using intermediate steps to solve reasoning
142"
CHAIN-OF-THOUGHT,0.3118279569892473,"problems [19, 6], the chain of thought method mimics a step-by-step thinking process and breaks
143"
CHAIN-OF-THOUGHT,0.3139784946236559,"down multi-step problems into intermediate steps, enabling the model to deduce more accurate
144"
CHAIN-OF-THOUGHT,0.3161290322580645,"results [23]. Additionally, [52] address the challenge of dealing with tasks that are more complex
145"
CHAIN-OF-THOUGHT,0.31827956989247314,"than example prompts, and proposes the least-to-most prompting approach which breaks down
146"
CHAIN-OF-THOUGHT,0.3204301075268817,"complex problems into smaller and easier subproblems. Moreover, [42] introduces self-consistency
147"
CHAIN-OF-THOUGHT,0.3225806451612903,"as a replacement for the greedy decoding algorithm, which samples and selects the most consistent
148"
CHAIN-OF-THOUGHT,0.3247311827956989,"reasoning paths to replace the greedy set.
149"
METHOD,0.32688172043010755,"3
Method
150"
METHOD,0.32903225806451614,"Figure 2: Pipeline of PromptCoT. (Left) We build three types of instruction patterns for training.
(Middle) We utilize adapters for multi-task adaptation. (Right) Results of t-continue, t2t booster and
PromptCoT."
OVERVIEW,0.3311827956989247,"3.1
Overview
151"
OVERVIEW,0.3333333333333333,"Text-to-image diffusion models serve as an illustrative example for showcasing the functionality of
152"
OVERVIEW,0.33548387096774196,"PromptCoT. However, it is important to note that the same methodology can be extended and applied
153"
OVERVIEW,0.33763440860215055,"to other diffusion-based generative models, including text-to-video and various other domains. In
154"
OVERVIEW,0.33978494623655914,"the context of training text-to-image diffusion-based models, which involve image-text pairs and
155"
OVERVIEW,0.3419354838709677,"employ an iterative denoising process to reconstruct images based on corresponding prompts, our
156"
OVERVIEW,0.34408602150537637,"hypothesis posits that prompts aligned with high-quality images within the training set are more
157"
OVERVIEW,0.34623655913978496,"inclined to yield visually superior outputs. We randomly select 5 sets of 50 prompts corresponding to
158"
OVERVIEW,0.34838709677419355,"images with varying levels of quality from the Stable Diffusion training set, LAION [35], for image
159"
OVERVIEW,0.35053763440860214,"generation. The aesthetic score, an image quality metric introduced by [31], is used to represent
160"
OVERVIEW,0.35268817204301073,"the quality of individual images. As shown in Table 1, the generation performance is highly related
161"
OVERVIEW,0.3548387096774194,"to the prompts corresponding to the original image quality. For convenience, we refer to them as
162"
OVERVIEW,0.35698924731182796,"“high-quality prompts”. In the following sections, we explain the key components of PromptCoT,"
OVERVIEW,0.35913978494623655,"Table 1: Comparison of Aesthetic Scores between Generated Images and Corresponding Training
Images."
OVERVIEW,0.36129032258064514,"Aesthetic Score
Training images
4-5
5-6
6-7
7-8
Generated images
5.2
5.5
6.1
6.3 163"
OVERVIEW,0.3634408602150538,"which is a prompt booster that can align input prompts with high-quality prompts in the training set,
164"
OVERVIEW,0.3655913978494624,"and in turn, improve generation performance.
165"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.36774193548387096,"3.2
Aligning Prompt Distribution with LLM
166"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.36989247311827955,"LLMs are extremely powerful tools that are capable of generating human-like language and complet-
167"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3720430107526882,"ing tasks such as translation, summarization, question answering, etc. They are trained on massive
168"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3741935483870968,"amounts of text data and can learn from unstructured data to generalize to new tasks and domains.
169"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3763440860215054,"LLMs can also be fine-tuned on specific tasks with relatively small amounts of task-specific data,
170"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.37849462365591396,"making them highly versatile. In this paper, we leverage this ability to align the distribution of
171"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.38064516129032255,"high-quality prompts via fine-tuning a popular LLM LLaMA [40], on text continuation and revision
172"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3827956989247312,"tasks. To fine-tune LLaMA on text continuation, we use an instruction tuning template that includes
173"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3849462365591398,"incomplete text descriptions and a goal to provide a compelling continuation. The instruction tuning
174"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3870967741935484,"template is shown in Figure 3. We feed truncated text prompts placed in the input field to the LLM,
175"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.38924731182795697,"supervised by the complete prompts. This enables the LLM to generate continuations containing
176"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3913978494623656,"more details.
177"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3935483870967742,Figure 3: Template of text-continuation dataset (Up) and corresponding output (Bottom).
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3956989247311828,"For text revision, we train the LLM to map human-like input texts to high-quality prompts. However,
178"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.3978494623655914,"acquiring a large amount of human-written input text can be costly. Therefore, we leverage image
179"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.4,"captions from BLIP as a low-cost source of ""human-like"" input texts. The details of collecting
180"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.4021505376344086,"and filtering data pairs are described in the later section. For training, we construct the instruction
181"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.4043010752688172,"tuning template in Figure 4. The training pipeline is similar to continuation, but with the input being
182"
ALIGNING PROMPT DISTRIBUTION WITH LLM,0.4064516129032258,"human-like prompts. As a result, we obtain a booster capable of performing revision tasks.
183"
ENHANCEMENT WITH COT,0.40860215053763443,"3.3
Enhancement with CoT
184"
ENHANCEMENT WITH COT,0.410752688172043,"Instruction tuning enables the LLM to add details and align text distribution, however, it tends to
185"
ENHANCEMENT WITH COT,0.4129032258064516,"generate extraneous information that degrades performance. As such, we introduce the Chain-of-
186"
ENHANCEMENT WITH COT,0.4150537634408602,"Thought (CoT) mechanism in the pipeline to address this issue. We set up five steps to make the
187"
ENHANCEMENT WITH COT,0.4172043010752688,Figure 4: Template of text-revision dataset (Up) and corresponding output (Bottom).
ENHANCEMENT WITH COT,0.41935483870967744,"LLM yield the expected production: (i) Extract key information from the original prompt, such as
188"
ENHANCEMENT WITH COT,0.421505376344086,"visual medium and main elements, (ii) Leverage the text-continuation model to append reasonable
189"
ENHANCEMENT WITH COT,0.4236559139784946,"details, (iii) Extract additional concepts (for example, the color scheme) from the extended prompt
190"
ENHANCEMENT WITH COT,0.4258064516129032,"and emphasize crucial concepts, (iv) With improved key information and crucial concepts, the LLM
191"
ENHANCEMENT WITH COT,0.42795698924731185,"can generate a fluent prompt, remaining to be aligned, (v) Leverage the text-revision model to align
192"
ENHANCEMENT WITH COT,0.43010752688172044,"prompts to the specific distribution. This mechanism extracts and amalgamates crucial information
193"
ENHANCEMENT WITH COT,0.432258064516129,"from the aligned continuation and revision, enabling reasonable inferences based on the contextual
194"
ENHANCEMENT WITH COT,0.4344086021505376,"cues. As a result, a more comprehensive and nuanced final output is produced.
195"
MULTI-TASK ADAPTATION,0.43655913978494626,"3.4
Multi-task Adaptation
196"
MULTI-TASK ADAPTATION,0.43870967741935485,"As the training set of different generative models can vary greatly, one approach to adapt to these
197"
MULTI-TASK ADAPTATION,0.44086021505376344,"new datasets is to fine-tune the entire LLM on the task-specific dataset. However, LLMs are typically
198"
MULTI-TASK ADAPTATION,0.443010752688172,"models with billions of parameters, and allocating a dedicated LLM to each individual model proves
199"
MULTI-TASK ADAPTATION,0.44516129032258067,"impractical due to computational constraints. Moreover, there are plenty of text-to-image generative
200"
MULTI-TASK ADAPTATION,0.44731182795698926,"models trained on different datasets, and a single LLM cannot cover a diverse distribution of these
201"
MULTI-TASK ADAPTATION,0.44946236559139785,"datasets. As an alternative, we integrate adapters that facilitate dataset-specific adaptation, leveraging
202"
MULTI-TASK ADAPTATION,0.45161290322580644,"a shared pre-trained LLM as the foundation for this process. Adapters are lightweight modules that
203"
MULTI-TASK ADAPTATION,0.45376344086021503,"can be independently fine-tuned and subsequently added to the base model. Keeping adapters instead
204"
MULTI-TASK ADAPTATION,0.4559139784946237,"of the whole model significantly reduces memory usage, while enabling the adaptation of the LLM to
205"
MULTI-TASK ADAPTATION,0.45806451612903226,"different datasets.
206"
MULTI-TASK ADAPTATION,0.46021505376344085,"Figure 5: Composition of fine-tuning tasks including text-continuation, text-revision, text-CoT, and
self-instruction of Alpaca."
DATASET PREPARATION,0.46236559139784944,"3.5
Dataset Preparation
207"
DATASET PREPARATION,0.4645161290322581,"We build three types of datasets: text-continuation, text-revision, and text-CoT.
208"
DATASET PREPARATION,0.4666666666666667,"Text-continuation dataset. To create this dataset, we filter high-quality prompts from the training
209"
DATASET PREPARATION,0.46881720430107526,"data of existing generative models, using criteria such as high CLIP similarity and proper length. In
210"
DATASET PREPARATION,0.47096774193548385,"the case of the LAION dataset, we also consider aesthetic scores to ensure a higher quality of prompts.
211"
DATASET PREPARATION,0.4731182795698925,"Once high-quality prompts are identified, we truncate a portion of the text, with the remaining front
212"
DATASET PREPARATION,0.4752688172043011,"part assigned as input data. The LLM is then trained to generate the missing information and complete
213"
DATASET PREPARATION,0.4774193548387097,"the text. This process enables the LLM to learn how to effectively continue text prompts in a manner
214"
DATASET PREPARATION,0.47956989247311826,"that is consistent with the style and context of the original text.
215"
DATASET PREPARATION,0.4817204301075269,"Text-revision dataset. The dataset consists of human-like texts and corresponding high-quality
216"
DATASET PREPARATION,0.4838709677419355,"prompts which are described in the text-continuation dataset. To acquire human-like prompts, we
217"
DATASET PREPARATION,0.4860215053763441,"leverage BLIP and CLIP-interrogator for image captioning. Furthermore, we calculate the text
218"
DATASET PREPARATION,0.4881720430107527,"distance with the text encoder of CLIP, ensuring a score greater than 0.4 to guarantee semantic
219"
DATASET PREPARATION,0.49032258064516127,"relevance between the two prompts.
220"
DATASET PREPARATION,0.4924731182795699,"Text-CoT dataset. We use GPT-3.5-Turbo to build a task-specific dataset. Initially, we design a
221"
DATASET PREPARATION,0.4946236559139785,"step-by-step interaction with GPT-3.5-Turbo to extract and guide the prompt booster to finish the
222"
DATASET PREPARATION,0.4967741935483871,"alignment task, due to the fact that CoT is still difficult for alpaca with a simple finetuning on datasets
223"
DATASET PREPARATION,0.4989247311827957,"above. Following the alpaca’s thought, 52k pairs are all generated from gpt-3.5-turbo.
224"
EXPERIMENTAL RESULTS,0.5010752688172043,"4
Experimental Results
225"
EXPERIMENTAL RESULTS,0.5032258064516129,"In this section, we first introduce the details on the datasets, pre-trained models, and the training
226"
EXPERIMENTAL RESULTS,0.5053763440860215,"hyperparameters used for all our experiments in Section 4.1. Then we demonstrate the results of
227"
EXPERIMENTAL RESULTS,0.5075268817204301,"applying PromptCoT to text-to-image and text-to-video pre-trained generative models in Section 4.2
228"
EXPERIMENTAL RESULTS,0.5096774193548387,"and Section 4.3 respectively.
229"
EXPERIMENTAL RESULTS,0.5118279569892473,"(a)
(b)
(c)
(d)
(e)
(f)
(g)"
EXPERIMENTAL RESULTS,0.513978494623656,"(h)
(i)
(j)
(k)
(l)
(m)
(n)"
EXPERIMENTAL RESULTS,0.5161290322580645,"Figure 6: Generated images from prompts refined by different aligners. (a) and (h) show the images
generated with the original text prompts. (b-g) and (i-n) denote the images generated with text
prompts refine by ‘t-continue’, ‘t2t-blip’,‘t2t-inter’,‘davinci’,‘CoT_d’, and ‘CoT’ respectively."
SETUP,0.5182795698924731,"4.1
Setup
230"
SETUP,0.5204301075268817,"Dataset.
For training, we build Text-revision and Text-continuation dataset from LAION-
231"
SETUP,0.5225806451612903,"aes6plus [35], and Text-CoT dataset with the help of GPT-3.5-turbo. LAION-aes6plus is the subset
232"
SETUP,0.524731182795699,"of LAION, containing 12M image-text pairs with predicted aesthetics scores of 6 or higher. As a
233"
SETUP,0.5268817204301075,"supplement, we also train with Text-revision, Text-continuation, and Text-CoT datasets from the
234"
SETUP,0.5290322580645161,"WebVid-10M dataset [3] for video generation. For evaluation, we conduct experiments on COCO [18]
235"
SETUP,0.5311827956989247,"validation set and MSR-VTT [45] for FID, FVD, aesthetic score, CLIP score, and PickScore.
236"
SETUP,0.5333333333333333,"Models. The pre-trained LLaMA-7B is used as the base model and we employ the adapter design
237"
SETUP,0.535483870967742,"outlined in [48] to facilitate multi-task adaptation. Two versions of Stable Diffusion [31], v1.4 and
238"
SETUP,0.5376344086021505,"v2.1, are used for image generation. MagicVideo [50] is used for video generation.
239"
SETUP,0.5397849462365591,"Implementation Details. We finetune the LLaMA following alpaca’s [39] strategy and instruction
240"
SETUP,0.5419354838709678,"pattern, which has been verified powerful for text generation tasks. We validate the viability of
241"
SETUP,0.5440860215053763,"our two initial ideas by finetuning three task-specific LLaMA for prompt refining works shown in
242"
SETUP,0.546236559139785,"experiments 2. One is trained on the self-constructed text-continuation dataset while the other two
243"
SETUP,0.5483870967741935,"are trained on two types of text-revision dataset. While combining such basic methods by CoT, we
244"
SETUP,0.5505376344086022,"include a dataset from alpaca, a subset of the text-continuation dataset, and the text-revision dataset
245"
SETUP,0.5526881720430108,"with higher text similarity and the CoT dataset as a whole. We evaluate our alignment work on three
246"
SETUP,0.5548387096774193,"diffusion models and on different parameters. Furthermore, we evaluate the portability of promptCoT
247"
SETUP,0.556989247311828,"through an adapter by comparing its performance with the fully-finetuned model.
248"
SETUP,0.5591397849462365,"Table 2: Text-to-image generation performance. We evaluate the generation performance on Stable
Diffusion v1.4 and v2.1 on key metrics including aesthetic score, FID, IS, CLIP score and PickScore."
SETUP,0.5612903225806452,"Generation
Model
Booster
Aesthetic
Score
FID
IS
CLIP
Score
PickScore
(avg/recall)"
SETUP,0.5634408602150538,"baseline
5.40
59.15
39.13 ± 0.84
0.268
27.3%/35.7%
SD v1.4
t-continue
5.54
44.66
35.81 ± 0.96
0.290
39.5%/61.5%
ddim step=50
t2t-blip
5.62
40.77
38.56 ± 0.77
0.293
51.4%/77.5%
scale=7.0
t2t-inter
5.44
55.76
41.00 ± 1.17
0.271
34.3%/49.0%
cot_d
5.64
49.58
37.43 ± 0.94
0.289
40.6%/62.2%"
SETUP,0.5655913978494623,"baseline
5.60
58.02
37.51 ± 1.00
0.266
29.4%/41.7%
SD v2.1
t-continue
5.70
45.62
34.44 ± 0.71
0.287
44.3%/69.9%
ddim step=50
t2t-blip
5.79
40.59
37.38 ± 1.08
0.292
56.3%/82.5%
scale=7.0
t2t-inter
5.64
54.93
38.60 ± 0.85
0.269
37.1%/55.6%
cot_d
5.78
50.41
34.88 ± 0.95
0.290
42.9%/66.2%"
SETUP,0.567741935483871,"baseline
5.60
58.17
36.37 ± 0.81
0.267
-
SD v2.1
t-continue
5.64
46.59
33.29 ± 0.68
0.287
-
ddim step=250
t2t-blip
5.76
40.89
36.16 ± 0.84
0.292
-
scale=12.0
t2t-inter
5.64
55.37
38.10 ± 1.16
0.269
-
cot_d
5.75
50.41
34.88 ± 0.94
0.290
-"
SETUP,0.5698924731182796,"Table 3: Text-to-image generation performance with adapters. We fine-tune adapters by 5 epochs
and compare them with fully fine-tuned Alpaca. Model with adapters achieves comparable results."
SETUP,0.5720430107526882,"Model
Booster
Aesthetic
Score
FID
IS
CLIP
Score
PickScore"
SETUP,0.5741935483870968,"t-continue
5.70
45.62
34.44 ± 0.71
0.287
44.3%/69.9%
Alpaca
t2t-blip
5.79
40.59
37.38 ± 1.08
0.292
56.3%/82.5%
epochs = 3
t2t-inter
5.64
54.93
38.60 ± 0.852
0.269
37.1%/55.6%
cot_d
5.78
50.41
34.88 ± 0.95
0.290
42.9%/66.2%"
SETUP,0.5763440860215053,"t-continue
5.69
48.00
35.8 ± 0.57
0.283
-
Adapter
t2t-blip
5.70
46.86
38.0 ± 0.66
0.289
-
epochs = 5
t2t-inter
5.64
56.28
39.0 ± 0.64
0.269
-
cot_d
5.85
51.06
31.8 ± 0.65
0.251
-"
TEXT-TO-IMAGE EVALUATION,0.578494623655914,"4.2
Text-to-image Evaluation
249"
TEXT-TO-IMAGE EVALUATION,0.5806451612903226,"The COCO [18] validation set is the standard benchmark for evaluating text-to-image models. The
250"
TEXT-TO-IMAGE EVALUATION,0.5827956989247312,"key automated performance metrics used are FID to measure image fidelity, CLIP score, PickScore to
251"
TEXT-TO-IMAGE EVALUATION,0.5849462365591398,"measure image-text alignment, aesthetic score [22] to predict the aesthetic quality, and Inception Score
252"
TEXT-TO-IMAGE EVALUATION,0.5870967741935483,"(IS) to evaluate the diversity. We utilize two versions of Stable Diffusion for image generation with
253"
TEXT-TO-IMAGE EVALUATION,0.589247311827957,"prompts from COCO and our PromptCoT. Table 2 presents the evaluation results for each metric with
254"
TEXT-TO-IMAGE EVALUATION,0.5913978494623656,"different single-function boosters including t-continue, t2t-blip, and t2t-inter, as well as a baseline.
255"
TEXT-TO-IMAGE EVALUATION,0.5935483870967742,"The results show that incorporating the alignment method proposed in our paper consistently improved
256"
TEXT-TO-IMAGE EVALUATION,0.5956989247311828,"the generated image quality across all metrics compared to the baseline. Among the single-function
257"
TEXT-TO-IMAGE EVALUATION,0.5978494623655914,"boosters, the t2t-blip booster demonstrates the best performance, as it is able to achieve alignment
258"
TEXT-TO-IMAGE EVALUATION,0.6,"to a greater extent. For example, it transfers “Boxes of fruit displayed at an open-air market” to “A
259"
TEXT-TO-IMAGE EVALUATION,0.6021505376344086,"view of stalls selling fruit at the Harare International Market in Harare, Zimbabwe” by rephrasing
260"
TEXT-TO-IMAGE EVALUATION,0.6043010752688172,"Table 4: Text-to-image generation performance. We compare finetuned CoT aligner and davinci-
003 model from OpenAI. All metrics are evaluated on a subset of the COCO validation dataset which
contains 1k images."
TEXT-TO-IMAGE EVALUATION,0.6064516129032258,"Booster
Aesthetic
Score
CLIP
Score
PickScore"
TEXT-TO-IMAGE EVALUATION,0.6086021505376344,"baseline
5.62
0.231
16.8%/26.1%
tcontinue
5.72
0.285
37.8%/66.2%
t2t_blip
5.80
0.293
50.6%/81.5%
t2t_inter
5.66
0.269
30.7%/52.5%
cot_d
5.79
0.291
34.9%/59.5%
cot
5.80
0.293
36.4%/59.0%
davinci
5.69
0.277
26.0%/47.5%"
TEXT-TO-IMAGE EVALUATION,0.610752688172043,"the expression and adding reasonable details. In contrast, the t2t-inter booster, which has a similar
261"
TEXT-TO-IMAGE EVALUATION,0.6129032258064516,"function to t2t-blip, shows inferior performance, although it still outperforms the baseline. This could
262"
TEXT-TO-IMAGE EVALUATION,0.6150537634408603,"be due to the CLIP-interrogator used to create the text-revision dataset introducing irrelevant entities.
263"
TEXT-TO-IMAGE EVALUATION,0.6172043010752688,"Furthermore, we test with different factors of classifier-free guidance to prove the generality of our
264"
TEXT-TO-IMAGE EVALUATION,0.6193548387096774,"PromptCoT. Varying the scale of classifier-free guidance results in consistent performance.
265"
TEXT-TO-VIDEO EVALUATION,0.621505376344086,"4.3
Text-to-video Evaluation
266"
TEXT-TO-VIDEO EVALUATION,0.6236559139784946,"In addition, we experiment with the text-to-video evaluation task to demonstrate the effectiveness of
267"
TEXT-TO-VIDEO EVALUATION,0.6258064516129033,"our approach. We employ two single-function boosters, t-continue, and t2t-blip on the WebVid-10M
268"
TEXT-TO-VIDEO EVALUATION,0.6279569892473118,"dataset [3]. For t2t-blip, we uniformly sample the video and randomly select five frames, which
269"
TEXT-TO-VIDEO EVALUATION,0.6301075268817204,"serve as input for the blip model and be used to generate the revision result. Then, we finetune
270"
TEXT-TO-VIDEO EVALUATION,0.632258064516129,"the LLaMA model following alpaca’s [39] strategy and build prompts from MSR-VTT with the
271"
TEXT-TO-VIDEO EVALUATION,0.6344086021505376,"fine-tuned model. We use MagicVideo [50] as the base model to test the effectiveness of our prompts.
272"
TEXT-TO-VIDEO EVALUATION,0.6365591397849463,"The results are shown in Table 5. The results indicate that the boosters are effective in enhancing
273"
TEXT-TO-VIDEO EVALUATION,0.6387096774193548,"the quality of the generated videos compared to the baseline, at least they ""do no harm"". Among
274"
TEXT-TO-VIDEO EVALUATION,0.6408602150537634,"the boosters, the booster better aligns the prompts and achieves the best performance overall. For
275"
TEXT-TO-VIDEO EVALUATION,0.6430107526881721,"cot_d, we generate 21k data with the help of GPT-3.5-turbo. Similar to text, we utilize a chain of five
276"
TEXT-TO-VIDEO EVALUATION,0.6451612903225806,"questions to generate the expected production, but with subtle differences to encourage GPT-3.5-turbo
277"
TEXT-TO-VIDEO EVALUATION,0.6473118279569893,"to generate more video-related features, e.g., movement. Similar to text generation, we adopt a chain
278"
TEXT-TO-VIDEO EVALUATION,0.6494623655913978,"of five questions to generate the expected production for video prompts. However, there are subtle
279"
TEXT-TO-VIDEO EVALUATION,0.6516129032258065,"differences in the question prompts to encourage GPT-3.5-turbo to incorporate more video-related
280"
TEXT-TO-VIDEO EVALUATION,0.6537634408602151,"features, such as movement, into its generated content. For example, ""a large passenger jet flying
281"
TEXT-TO-VIDEO EVALUATION,0.6559139784946236,"in the sky at sunset"" can be refined to ""Boeing 747 flying across a vibrant sunset backdrop in a
282"
TEXT-TO-VIDEO EVALUATION,0.6580645161290323,"captivating, cinematic 4K video. Slowly gaining altitude with wings tilting slightly, this footage
283"
TEXT-TO-VIDEO EVALUATION,0.6602150537634408,"captures the plane’s majesty"". The scores of cot_d will be included in the supplementary material.
284"
TEXT-TO-VIDEO EVALUATION,0.6623655913978495,"Table 5: Text-to-video generation performance. We evaluate the generation performance on
MagicVideo on key metrics including FID, FVD, and CLIP score."
TEXT-TO-VIDEO EVALUATION,0.6645161290322581,"Model
Dataset
Booster
FID
FVD
CLIP Score"
TEXT-TO-VIDEO EVALUATION,0.6666666666666666,"MagicVideo
MSR-VTT
baseline
36.5
998
0.284
t-continue
33.2
951
0.296"
CONCLUSION,0.6688172043010753,"5
Conclusion
285"
CONCLUSION,0.6709677419354839,"In this paper, we present PromptCoT, an innovative system designed to autonomously enhance the
286"
CONCLUSION,0.6731182795698925,"quality of prompts used in diffusion-based generative models, which are critical for high-fidelity
287"
CONCLUSION,0.6752688172043011,"visual content generation. PromptCoT leverages pre-trained Large Language Models (LLMs) and
288"
CONCLUSION,0.6774193548387096,"a unique Chain-of-Thought (CoT) mechanism to refine prompts, thereby improving the alignment
289"
CONCLUSION,0.6795698924731183,"between the original and refined prompts. To balance computational efficiency, we employ adapters to
290"
CONCLUSION,0.6817204301075269,"allow for efficient adaptation to new datasets or models. Our evaluations demonstrate that PromptCoT
291"
CONCLUSION,0.6838709677419355,"can achieve superior performance compared to the baselines.
292"
REFERENCES,0.6860215053763441,"References
293"
REFERENCES,0.6881720430107527,"[1] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness
294"
REFERENCES,0.6903225806451613,"of language model fine-tuning, 2020.
295"
REFERENCES,0.6924731182795699,"[2] Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan,
296"
REFERENCES,0.6946236559139785,"Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri
297"
REFERENCES,0.6967741935483871,"Songz, Wang Phil, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in
298"
REFERENCES,0.6989247311827957,"PyTorch, 8 2021.
299"
REFERENCES,0.7010752688172043,"[3] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image
300"
REFERENCES,0.7032258064516129,"encoder for end-to-end retrieval, 2022.
301"
REFERENCES,0.7053763440860215,"[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
302"
REFERENCES,0.7075268817204301,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
303"
REFERENCES,0.7096774193548387,"Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens
304"
REFERENCES,0.7118279569892473,"Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
305"
REFERENCES,0.7139784946236559,"Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
306"
REFERENCES,0.7161290322580646,"models are few-shot learners. ArXiv, abs/2005.14165, 2020.
307"
REFERENCES,0.7182795698924731,"[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
308"
REFERENCES,0.7204301075268817,"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
309"
REFERENCES,0.7225806451612903,"Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
310"
REFERENCES,0.7247311827956989,"Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
311"
REFERENCES,0.7268817204301076,"Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
312"
REFERENCES,0.7290322580645161,"models are few-shot learners. 2020.
313"
REFERENCES,0.7311827956989247,"[6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
314"
REFERENCES,0.7333333333333333,"Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training
315"
REFERENCES,0.7354838709677419,"verifiers to solve math word problems, 2021.
316"
REFERENCES,0.7376344086021506,"[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in
317"
REFERENCES,0.7397849462365591,"Neural Information Processing Systems, 34, 2021.
318"
REFERENCES,0.7419354838709677,"[8] Ning Ding, Yujia Qin, Guang Yang, Fu Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen,
319"
REFERENCES,0.7440860215053764,"Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Haitao Zheng, Jianfei
320"
REFERENCES,0.7462365591397849,"Chen, Yang Liu, Jie Tang, Juan Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter
321"
REFERENCES,0.7483870967741936,"efficient methods for pre-trained language models. ArXiv, abs/2203.06904, 2022.
322"
REFERENCES,0.7505376344086021,"[9] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General
323"
REFERENCES,0.7526881720430108,"language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting
324"
REFERENCES,0.7548387096774194,"of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320–335, 2022.
325"
REFERENCES,0.7569892473118279,"[10] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.
326"
REFERENCES,0.7591397849462366,"ArXiv, abs/2012.15723, 2021.
327"
REFERENCES,0.7612903225806451,"[11] Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. Ptr: Prompt tuning with rules for text
328"
REFERENCES,0.7634408602150538,"classification. ArXiv, abs/2105.11259, 2021.
329"
REFERENCES,0.7655913978494624,"[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
330"
REFERENCES,0.7677419354838709,"Information Processing Systems, 33:6840–6851, 2020.
331"
REFERENCES,0.7698924731182796,"[13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet.
332"
REFERENCES,0.7720430107526882,"Video diffusion models, 2022.
333"
REFERENCES,0.7741935483870968,"[14] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea
334"
REFERENCES,0.7763440860215054,"Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika
335"
REFERENCES,0.7784946236559139,"Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on
336"
REFERENCES,0.7806451612903226,"Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR,
337"
REFERENCES,0.7827956989247312,"09–15 Jun 2019.
338"
REFERENCES,0.7849462365591398,"[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
339"
REFERENCES,0.7870967741935484,"Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
340"
REFERENCES,0.789247311827957,"[16] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Toward controlled
341"
REFERENCES,0.7913978494623656,"generation of text. In International Conference on Machine Learning, 2017.
342"
REFERENCES,0.7935483870967742,"[17] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning,
343"
REFERENCES,0.7956989247311828,"2021.
344"
REFERENCES,0.7978494623655914,"[18] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
345"
REFERENCES,0.8,"and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on
346"
REFERENCES,0.8021505376344086,"Computer Vision, 2014.
347"
REFERENCES,0.8043010752688172,"[19] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:
348"
REFERENCES,0.8064516129032258,"Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
349"
REFERENCES,0.8086021505376344,"[20] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,
350"
REFERENCES,0.810752688172043,"prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM
351"
REFERENCES,0.8129032258064516,"Computing Surveys, 55:1 – 35, 2021.
352"
REFERENCES,0.8150537634408602,"[21] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank
353"
REFERENCES,0.8172043010752689,"hypercomplex adapter layers, 2021.
354"
REFERENCES,0.8193548387096774,"[22] Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: A large-scale database for aesthetic visual
355"
REFERENCES,0.821505376344086,"analysis. In 2012 IEEE conference on computer vision and pattern recognition, pages 2408–2415. IEEE,
356"
REFERENCES,0.8236559139784946,"2012.
357"
REFERENCES,0.8258064516129032,"[23] Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. Wt5?!
358"
REFERENCES,0.8279569892473119,"training text-to-text models to explain their predictions, 2020.
359"
REFERENCES,0.8301075268817204,"[24] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun
360"
REFERENCES,0.832258064516129,"Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers, 2020.
361"
REFERENCES,0.8344086021505376,"[25] Guanghui Qin and Jas’ Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. ArXiv,
362"
REFERENCES,0.8365591397849462,"abs/2104.06599, 2021.
363"
REFERENCES,0.8387096774193549,"[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
364"
REFERENCES,0.8408602150537634,"Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
365"
REFERENCES,0.843010752688172,"natural language supervision. 2021.
366"
REFERENCES,0.8451612903225807,"[27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models
367"
REFERENCES,0.8473118279569892,"are unsupervised multitask learners. 2019.
368"
REFERENCES,0.8494623655913979,"[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
369"
REFERENCES,0.8516129032258064,"image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
370"
REFERENCES,0.853763440860215,"[29] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and
371"
REFERENCES,0.8559139784946237,"Ilya Sutskever. Dall·e 2: Exploring cross-modal transformers for image generation. OpenAI Blog, 2021.
372"
REFERENCES,0.8580645161290322,"[30] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
373"
REFERENCES,0.8602150537634409,"synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern
374"
REFERENCES,0.8623655913978494,"Recognition (CVPR), pages 10674–10685, 2021.
375"
REFERENCES,0.864516129032258,"[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
376"
REFERENCES,0.8666666666666667,"image synthesis with latent diffusion models, 2022.
377"
REFERENCES,0.8688172043010752,"[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
378"
REFERENCES,0.8709677419354839,"Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,
379"
REFERENCES,0.8731182795698925,"David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language
380"
REFERENCES,0.875268817204301,"understanding, 2022.
381"
REFERENCES,0.8774193548387097,"[33] Timo Schick, Helmut Schmid, and Hinrich Schütze. Automatically identifying words that can serve as
382"
REFERENCES,0.8795698924731182,"labels for few-shot text classification. In International Conference on Computational Linguistics, 2020.
383"
REFERENCES,0.8817204301075269,"[34] Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural
384"
REFERENCES,0.8838709677419355,"language inference.
In Conference of the European Chapter of the Association for Computational
385"
REFERENCES,0.886021505376344,"Linguistics, 2020.
386"
REFERENCES,0.8881720430107527,"[35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
387"
REFERENCES,0.8903225806451613,"Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
388"
REFERENCES,0.8924731182795699,"dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.
389"
REFERENCES,0.8946236559139785,"[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint
390"
REFERENCES,0.896774193548387,"arXiv:2010.02502, 2020.
391"
REFERENCES,0.8989247311827957,"[37] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
392"
REFERENCES,0.9010752688172043,"Advances in Neural Information Processing Systems, 32, 2019.
393"
REFERENCES,0.9032258064516129,"[38] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
394"
REFERENCES,0.9053763440860215,"Poole.
Score-based generative modeling through stochastic differential equations.
arXiv preprint
395"
REFERENCES,0.9075268817204301,"arXiv:2011.13456, 2020.
396"
REFERENCES,0.9096774193548387,"[39] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
397"
REFERENCES,0.9118279569892473,"and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.
398"
REFERENCES,0.9139784946236559,"com/tatsu-lab/stanford_alpaca, 2023.
399"
REFERENCES,0.9161290322580645,"[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
400"
REFERENCES,0.9182795698924732,"Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
401"
REFERENCES,0.9204301075268817,"Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
402"
REFERENCES,0.9225806451612903,"[41] Leandro von Werra, Alex Havrilla, Max reciprocated, Jonathan Tow, Aman cat state, Duy V. Phung,
403"
REFERENCES,0.9247311827956989,"Louis Castricato, Shahbuland Matiana, Alan, Ayush Thakur, Alexey Bukhtiyarov, aaronrmm, Fabrizio
404"
REFERENCES,0.9268817204301075,"Milo, Daniel, Daniel King, Dong Shin, Ethan Kim, Justin Wei, Manuel Romero, Nicky Pochinkov, Omar
405"
REFERENCES,0.9290322580645162,"Sanseviero, Reshinth Adithyan, Sherman Siu, Thomas Simonini, Vladimir Blagojevic, Xu Song, Zack
406"
REFERENCES,0.9311827956989247,"Witten, alexandremuzio, and crumb. CarperAI/trlx: v0.6.0: LLaMa (Alpaca), Benchmark Util, T5 ILQL,
407"
REFERENCES,0.9333333333333333,"Tests, March 2023.
408"
REFERENCES,0.9354838709677419,"[42] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-consistency
409"
REFERENCES,0.9376344086021505,"improves chain of thought reasoning in language models. ArXiv, abs/2203.11171, 2022.
410"
REFERENCES,0.9397849462365592,"[43] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
411"
REFERENCES,0.9419354838709677,"Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021.
412"
REFERENCES,0.9440860215053763,"[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
413"
REFERENCES,0.946236559139785,"Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
414"
REFERENCES,0.9483870967741935,"[45] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video
415"
REFERENCES,0.9505376344086022,"and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
416"
REFERENCES,0.9526881720430107,"5288–5296, 2016.
417"
REFERENCES,0.9548387096774194,"[46] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
418"
REFERENCES,0.956989247311828,"Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han
419"
REFERENCES,0.9591397849462365,"Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image
420"
REFERENCES,0.9612903225806452,"generation, 2022.
421"
REFERENCES,0.9634408602150538,"[47] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi
422"
REFERENCES,0.9655913978494624,"Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414,
423"
REFERENCES,0.967741935483871,"2022.
424"
REFERENCES,0.9698924731182795,"[48] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and
425"
REFERENCES,0.9720430107526882,"Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint
426"
REFERENCES,0.9741935483870968,"arXiv:2303.16199, 2023.
427"
REFERENCES,0.9763440860215054,"[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
428"
REFERENCES,0.978494623655914,"Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster,
429"
REFERENCES,0.9806451612903225,"Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open
430"
REFERENCES,0.9827956989247312,"pre-trained transformer language models, 2022.
431"
REFERENCES,0.9849462365591398,"[50] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient
432"
REFERENCES,0.9870967741935484,"video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.
433"
REFERENCES,0.989247311827957,"[51] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient
434"
REFERENCES,0.9913978494623656,"video generation with latent diffusion models, 2023.
435"
REFERENCES,0.9935483870967742,"[52] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,
436"
REFERENCES,0.9956989247311828,"Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables complex reasoning in
437"
REFERENCES,0.9978494623655914,"large language models. ArXiv, abs/2205.10625, 2022.
438"
