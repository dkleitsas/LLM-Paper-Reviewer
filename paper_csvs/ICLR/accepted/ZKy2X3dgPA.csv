Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002777777777777778,"Metric learning involves learning a discriminative representation such that em-
beddings of similar classes are encouraged to be close, while embeddings of dis-
similar classes are pushed far apart. State-of-the-art methods focus mostly on
sophisticated loss functions or mining strategies. On the one hand, metric learn-
ing losses consider two or more examples at a time. On the other hand, modern
data augmentation methods for classiﬁcation consider two or more examples at a
time. The combination of the two ideas is under-studied.
In this work, we aim to bridge this gap and improve representations using mixup,
which is a powerful data augmentation approach interpolating two or more exam-
ples and corresponding target labels at a time. This task is challenging because
unlike classiﬁcation, the loss functions used in metric learning are not additive
over examples, so the idea of interpolating target labels is not straightforward. To
the best of our knowledge, we are the ﬁrst to investigate mixing both examples
and target labels for deep metric learning. We develop a generalized formulation
that encompasses existing metric learning loss functions and modify it to accom-
modate for mixup, introducing Metric Mix, or Metrix. We also introduce a new
metric—utilization—to demonstrate that by mixing examples during training, we
are exploring areas of the embedding space beyond the training classes, thereby
improving representations. To validate the effect of improved representations, we
show that mixing inputs, intermediate representations or embeddings along with
target labels signiﬁcantly outperforms state-of-the-art metric learning methods on
four benchmark deep metric learning datasets."
INTRODUCTION,0.005555555555555556,"1
INTRODUCTION"
INTRODUCTION,0.008333333333333333,"anchor
positive
negative
mixed"
INTRODUCTION,0.011111111111111112,"class
label
interpolation"
INTRODUCTION,0.013888888888888888,"Figure 1: Metrix (= Metric Mix) allows an an-
chor to interact with positive (same class), neg-
ative (different class) and interpolated examples,
which also have interpolated labels."
INTRODUCTION,0.016666666666666666,"Classiﬁcation is one of the most studied tasks
in machine learning and deep learning.
It
is a common source of pre-trained models
for transfer learning to other tasks (Donahue
et al., 2014; Kolesnikov et al., 2020). It has
been studied under different supervision set-
tings (Caron et al., 2018; Sohn et al., 2020),
knowledge transfer (Hinton et al., 2015) and
data augmentation (Cubuk et al., 2018), in-
cluding the recent research on mixup (Zhang
et al., 2018; Verma et al., 2019), where em-
beddings and labels are interpolated."
INTRODUCTION,0.019444444444444445,"Deep metric learning is about learning from
pairwise interactions such that inference re-
lies on instance embeddings, e.g. for near-
est neighbor classiﬁcation (Oh Song et al.,"
INTRODUCTION,0.022222222222222223,∗equal contribution
INTRODUCTION,0.025,Published as a conference paper at ICLR 2022
INTRODUCTION,0.027777777777777776,"2016), instance-level retrieval (Gordo et al., 2016), few-shot learning (Vinyals et al., 2016), face
recognition (Schroff et al., 2015) and semantic textual similarity (Reimers & Gurevych, 2019)."
INTRODUCTION,0.030555555555555555,"Following (Xing et al., 2003), it is most often fully supervised by one class label per example,
like classiﬁcation. The two most studied problems are loss functions (Musgrave et al., 2020) and
hard example mining (Wu et al., 2017; Robinson et al., 2021). Tuple-based losses with example
weighting (Wang et al., 2019) can play the role of both."
INTRODUCTION,0.03333333333333333,"Unlike classiﬁcation, classes (and distributions) at training and inference are different in metric
learning. Thus, one might expect interpolation-based data augmentation like mixup to be even more
important in metric learning than in classiﬁcation. Yet, recent attempts are mostly limited to special
cases of embedding interpolation and have trouble with label interpolation (Ko & Gu, 2020). This
raises the question: what is a proper way to deﬁne and interpolate labels for metric learning?"
INTRODUCTION,0.03611111111111111,"In this work, we observe that metric learning is not different from classiﬁcation, where examples
are replaced by pairs of examples and class labels by “positive” or “negative”, according to whether
class labels of individual examples are the same or not. The positive or negative label of an example,
or a pair, is determined in relation to a given example which is called an anchor. Then, as shown in
Figure 1, a straightforward way is to use a binary (two class) label per pair and interpolate it linearly
as in standard mixup. We call our method Metric Mix, or Metrix for short."
INTRODUCTION,0.03888888888888889,"To show that mixing examples improves representation learning, we quantitatively measure the
properties of the test distributions using alignment and uniformity (Wang & Isola, 2020). Align-
ment measures the clustering quality and uniformity measures its distribution over the embedding
space; a well clustered and uniformly spread distribution indicates higher representation quality. We
also introduce a new metric, utilization, to measure the extent to which a test example, seen as a
query, lies near any of the training examples, clean or mixed. By quantitatively measuring these
three metrics, we show that interpolation-based data augmentation like mixup is very important in
metric learning, given the difference between distributions at training and inference."
INTRODUCTION,0.041666666666666664,"In summary, we make the following contributions:"
INTRODUCTION,0.044444444444444446,"1. We deﬁne a generic way of representing and interpolating labels, which allows straightforward
extension of any kind of mixup to deep metric learning for a large class of loss functions. We
develop our method on a generic formulation that encapsulates these functions (section 3)."
INTRODUCTION,0.04722222222222222,"2. We deﬁne the “positivity” of a mixed example and we study precisely how it increases as a
function of the interpolation factor, both in theory and empirically (subsection 3.6)."
INTRODUCTION,0.05,"3. We systematically evaluate mixup for deep metric learning under different settings, including
mixup at different representation levels (input/manifold), mixup of different pairs of examples
(anchors/positives/negatives), loss functions and hard example mining (subsection 4.2)."
INTRODUCTION,0.05277777777777778,"4. We introduce a new evaluation metric, utilization, validating that a representation more appro-
priate for test classes is implicitly learned during exploration of the embedding space in the
presence of mixup (subsection 4.3)."
INTRODUCTION,0.05555555555555555,5. We improve the state of the art on four common metric learning benchmarks (subsection 4.2).
RELATED WORK,0.058333333333333334,"2
RELATED WORK"
RELATED WORK,0.06111111111111111,"Metric learning
Metric learning aims to learn a metric such that positive pairs of examples are
nearby and negative ones are far away. In deep metric learning, we learn an explicit non-linear
mapping from raw input to a low-dimensional embedding space (Oh Song et al., 2016), where
the Euclidean distance has the desired properties. Although learning can be unsupervised (Hadsell
et al., 2006), deep metric learning has mostly followed the supervised approach, where positive and
negative pairs are deﬁned as having the same or different class label, respectively (Xing et al., 2003)."
RELATED WORK,0.06388888888888888,"Loss functions can be distinguished into pair-based and proxy-based (Musgrave et al., 2020). Pair-
based losses use pairs of examples (Wu et al., 2017; Hadsell et al., 2006), which can be deﬁned
over triplets (Wang et al., 2014; Schroff et al., 2015; Weinberger & Saul, 2009; Hermans et al.,
2017), quadruples (Chen et al., 2017) or tuples (Sohn, 2016; Oh Song et al., 2016; Wang et al.,
2019). Proxy-based losses use one or more proxies per class, which are learnable parameters in the
embedding space (Movshovitz-Attias et al., 2017; Qian et al., 2019; Kim et al., 2020c; Teh et al.,"
RELATED WORK,0.06666666666666667,Published as a conference paper at ICLR 2022
RELATED WORK,0.06944444444444445,"2020; Zhu et al., 2020b). Pair-based losses capture data-to-data relations, but they are sensitive to
noisy labels and outliers. They often involve terms where given constraints are satisﬁed, which
produce zero gradients and do not contribute to training. This necessitates mining of hard examples
that violate the constraints, like semi-hard (Schroff et al., 2015) and distance weighted (Wu et al.,
2017). By contrast, proxy-based losses use data-to-proxy relations, assuming proxies can capture
the global structure of the embedding space. They involve less computations that are more likely to
produce nonzero gradient, hence have less or no dependence on mining and converge faster."
RELATED WORK,0.07222222222222222,"Mixup
Input mixup (Zhang et al., 2018) linearly interpolates between two or more examples in
the input space for data augmentation. Numerous variants take advantage of the structure of the
input space to interpolate non-linearly, e.g. for images (Yun et al., 2019; Kim et al., 2020a; 2021;
Hendrycks et al., 2020; DeVries & Taylor, 2017; Qin et al., 2020; Uddin et al., 2021). Manifold
mixup (Verma et al., 2019) interpolates intermediate representations instead, where the structure
is learned. This can be applied to or assisted by decoding back to the input space (Berthelot et al.,
2018; Liu et al., 2018; Beckham et al., 2019; Zhu et al., 2020a; Venkataramanan et al., 2021). In both
cases, corresponding labels are linearly interpolated too. Most studies are limited to cross-entropy
loss for classiﬁcation. Pairwise loss functions have been under-studied, as discussed below."
RELATED WORK,0.075,"Interpolation for pairwise loss functions
As discussed in subsection 3.3, interpolating target
labels is not straightforward in pairwise loss functions. In deep metric learning, embedding expan-
sion (Ko & Gu, 2020), HDML (Zheng et al., 2019) and symmetrical synthesis (Gu & Ko, 2020)
interpolate pairs of embeddings in a deterministic way within the same class, applying to pair-based
losses, while proxy synthesis (Gu et al., 2021) interpolates between classes, applying to proxy-based
losses. None performs label interpolation, which means that (Gu et al., 2021) risks synthesizing
false negatives when the interpolation factor λ is close to 0 or 1."
RELATED WORK,0.07777777777777778,"In contrastive representation learning, MoCHi (Kalantidis et al., 2020) interpolates anchor with
negative embeddings but not labels and chooses λ ∈[0, 0.5] to avoid false negatives. This resembles
thresholding of λ at 0.5 in OptTransMix (Zhu et al., 2020a). Finally, i-mix (Lee et al., 2021) and
MixCo (Kim et al., 2020b) interpolate pairs of anchor embeddings as well as their (virtual) class
labels linearly. There is only one positive, while all negatives are clean, so it cannot take advantage
of interpolation for relative weighting of positives/negatives per anchor (Wang et al., 2019)."
RELATED WORK,0.08055555555555556,"By contrast, Metrix is developed for deep metric learning and applies to a large class of both pair-
based and proxy-based losses. It can interpolate inputs, intermediate features or embeddings of
anchors, (multiple) positives or negatives and the corresponding two-class (positive/negative) labels
per anchor, such that relative weighting of positives/negatives depends on interpolation."
MIXUP FOR METRIC LEARNING,0.08333333333333333,"3
MIXUP FOR METRIC LEARNING"
PRELIMINARIES,0.08611111111111111,"3.1
PRELIMINARIES"
PRELIMINARIES,0.08888888888888889,"Problem formulation
We are given a training set X ⊂X, where X is the input space. For each
anchor a ∈X, we are also given a set P(a) ⊂X of positives and a set N(a) ⊂X of negatives.
The positives are typically examples that belong to the same class as the anchor, while negatives
belong to a different class. The objective is to train the parameters θ of a model f : X →Rd
that maps input examples to a d-dimensional embedding, such that positives are close to the anchor
and negatives are far away in the embedding space. Given two examples x, x′ ∈X, we denote
by s(x, x′) the similarity between x, x′ in the embedding space, typically a decreasing function of
Euclidean distance. It is common to ℓ2-normalize embeddings and deﬁne s(x, x′) := ⟨f(x), f(x′)⟩,
which is the cosine similarity. To simplify notation, we drop the dependence of f, s on θ."
PRELIMINARIES,0.09166666666666666,"Pair-based losses (Hadsell et al., 2006; Wang et al., 2014; Oh Song et al., 2016; Wang et al., 2019)
use both anchors and positives/negatives in X, as discussed above. Proxy-based losses deﬁne one
or more learnable proxies ∈Rd per class, and only use proxies as anchors (Kim et al., 2020c) or
as positives/negatives (Movshovitz-Attias et al., 2017; Qian et al., 2019; Teh et al., 2020). To ac-
commodate for uniform exposition, we extend the deﬁnition of similarity as s(v, x) := ⟨v, f(x)⟩
for v ∈Rd, x ∈X (proxy anchors) and s(x, v) := ⟨f(x), v⟩for x ∈X, v ∈Rd (proxy pos-
itives/negatives).
Finally, to accommodate for mixed embeddings in subsection 3.5, we deﬁne
s(v, v′) := ⟨v, v′⟩for v, v′ ∈Rd. Thus, we deﬁne s : (X ∪Rd)2 →R over pairs of either"
PRELIMINARIES,0.09444444444444444,Published as a conference paper at ICLR 2022
PRELIMINARIES,0.09722222222222222,"inputs in X or embeddings in Rd. We discuss a few representative loss functions below, before
deriving a generic form."
PRELIMINARIES,0.1,"Contrastive
The contrastive loss (Hadsell et al., 2006) encourages positive examples to be pulled
towards the anchor and negative examples to be pushed away by a margin m ∈R. This loss is
additive over positives and negatives, deﬁned as:"
PRELIMINARIES,0.10277777777777777,"ℓcont(a; θ) :=
X"
PRELIMINARIES,0.10555555555555556,"p∈P (a)
−s(a, p) +
X"
PRELIMINARIES,0.10833333333333334,"n∈N(a)
[s(a, n) −m]+.
(1)"
PRELIMINARIES,0.1111111111111111,"Multi-Similarity
The multi-similarity loss (Wang et al., 2019) introduces relative weighting to
encourage positives (negatives) that are farthest from (closest to) the anchor to be pulled towards
(pushed away from) the anchor by a higher weight. This loss is not additive over positives and
negatives:"
PRELIMINARIES,0.11388888888888889,ℓMS(a; θ) := 1 β log 
PRELIMINARIES,0.11666666666666667,"1 +
X"
PRELIMINARIES,0.11944444444444445,"p∈P (a)
e−β(s(a,p)−m)  + 1 γ log "
PRELIMINARIES,0.12222222222222222,"1 +
X"
PRELIMINARIES,0.125,"n∈N(a)
eγ(s(a,n)−m) "
PRELIMINARIES,0.12777777777777777,".
(2)"
PRELIMINARIES,0.13055555555555556,"Here, β, γ ∈R are scaling factors for positives, negatives respectively."
PRELIMINARIES,0.13333333333333333,"Proxy Anchor
The proxy anchor loss (Kim et al., 2020c) deﬁnes a learnable proxy in Rd for each
class and only uses proxies as anchors. For a given anchor (proxy) a ∈Rd, the loss has the same
form as (2), although similarity s is evaluated on Rd × X."
GENERIC LOSS FORMULATION,0.1361111111111111,"3.2
GENERIC LOSS FORMULATION"
GENERIC LOSS FORMULATION,0.1388888888888889,"We observe that both additive (1) and non-additive (2) loss functions involve a sum over positives
P(a) and a sum over negatives N(a). They also involve a decreasing function of similarity s(a, p)
for each positive p ∈P(a) and an increasing function of similarity s(a, n) for each negative n ∈
N(a). Let us denote by ρ+, ρ−this function for positives, negatives respectively. Then, non-additive
functions differ from additive by the use of a nonlinear function σ+, σ−on positive and negative
terms respectively, as well as possibly another nonlinear function τ on their sum:"
GENERIC LOSS FORMULATION,0.14166666666666666,ℓ(a; θ) := τ  σ+  X
GENERIC LOSS FORMULATION,0.14444444444444443,"p∈P (a)
ρ+(s(a, p))  + σ−  X"
GENERIC LOSS FORMULATION,0.14722222222222223,"n∈N(a)
ρ−(s(a, n))   "
GENERIC LOSS FORMULATION,0.15,".
(3)"
GENERIC LOSS FORMULATION,0.1527777777777778,"With the appropriate choice for τ, σ+, σ−, ρ+, ρ−, this deﬁnition encompasses contrastive (1),
multi-similarity (2) or proxy-anchor as well as many pair-based or proxy-based loss functions, as
shown in Table 1. It does not encompass the triplet loss (Wang et al., 2014), which operates on pairs
of positives and negatives, forming triplets with the anchor. The triplet loss is the most challenging
in terms of mining because there is a very large number of pairs and only few contribute to the loss.
We only use function τ to accommodate for lifted structure (Oh Song et al., 2016; Hermans et al.,
2017), where τ(x) := [x]+ is reminiscent of the triplet loss. We observe that multi-similarity (Wang
et al., 2019) differs from binomial deviance (Yi et al., 2014) only in the weights of the positive and
negative terms. Proxy anchor (Kim et al., 2020c) is a proxy version of multi-similarity (Wang et al.,
2019) on anchors and ProxyNCA (Movshovitz-Attias et al., 2017) is a proxy version of NCA (Gold-
berger et al., 2005) on positives/negatives."
GENERIC LOSS FORMULATION,0.15555555555555556,"This generic formulation highlights the components of the loss functions that are additive over pos-
itives/negatives and paves the way towards incorporating mixup."
IMPROVING REPRESENTATIONS USING MIXUP,0.15833333333333333,"3.3
IMPROVING REPRESENTATIONS USING MIXUP"
IMPROVING REPRESENTATIONS USING MIXUP,0.16111111111111112,"To improve the learned representations, we follow (Zhang et al., 2018; Verma et al., 2019) in mixing
inputs and features from intermediate network layers, respectively. Both are developed for classiﬁ-
cation."
IMPROVING REPRESENTATIONS USING MIXUP,0.1638888888888889,"Input mixup (Zhang et al., 2018) augments data by linear interpolation between a pair of input
examples. Given two examples x, x′ ∈X we draw λ ∼Beta(α, α) as interpolation factor and mix
x with x′ using the standard mixup operation mixλ(x, x′) := λx + (1 −λ)x′."
IMPROVING REPRESENTATIONS USING MIXUP,0.16666666666666666,Published as a conference paper at ICLR 2022
IMPROVING REPRESENTATIONS USING MIXUP,0.16944444444444445,"LOSS
ANCHOR
POS/NEG
τ(x)
σ+(x)
σ−(x)
ρ+(x)
ρ−(x)"
IMPROVING REPRESENTATIONS USING MIXUP,0.17222222222222222,"Contrastive (Hadsell et al., 2006)
X
X
x
x
x
−x
[x −m]+
Lifted structure (Hermans et al., 2017)
X
X
[x]+
log(x)
log(x)
e−x
ex−m"
IMPROVING REPRESENTATIONS USING MIXUP,0.175,"Binomial deviance (Yi et al., 2014)
X
X
x
log(1 + x)
log(1 + x)
e−β(x−m)
eγ(x−m)"
IMPROVING REPRESENTATIONS USING MIXUP,0.17777777777777778,"Multi-similarity (Wang et al., 2019)
X
X
x
1
β log(1 + x)
1
γ log(1 + x)
e−β(x−m)
eγ(x−m)"
IMPROVING REPRESENTATIONS USING MIXUP,0.18055555555555555,"Proxy anchor (Kim et al., 2020c)
proxy
X
x
1
β log(1 + x)
1
γ log(1 + x)
e−β(x−m)
eγ(x−m)"
IMPROVING REPRESENTATIONS USING MIXUP,0.18333333333333332,"NCA (Goldberger et al., 2005)
X
X
x
−log(x)
log(x)
ex
ex"
IMPROVING REPRESENTATIONS USING MIXUP,0.18611111111111112,"ProxyNCA (Movshovitz-Attias et al., 2017)
X
proxy
x
−log(x)
log(x)
ex
ex"
IMPROVING REPRESENTATIONS USING MIXUP,0.18888888888888888,"ProxyNCA++ (Teh et al., 2020)
X
proxy
x
−log(x)
log(x)
ex/T
ex/T"
IMPROVING REPRESENTATIONS USING MIXUP,0.19166666666666668,"Table 1: Loss functions. Anchor/positive/negative: X: embedding of input example from training
set X by f; proxy: learnable parameter in Rd ; T : temperature. All loss functions are encompassed
by (3) using the appropriate deﬁnition of functions τ, σ+, σ−, ρ+, ρ−as given here."
IMPROVING REPRESENTATIONS USING MIXUP,0.19444444444444445,"Manifold mixup (Verma et al., 2019) linearly interpolates between intermediate representations (fea-
tures) of the network instead. Referring to 2D images, we deﬁne gm : X →Rc×w×h as the mapping
from the input to intermediate layer m of the network and fm : Rc×w×h →Rd as the mapping from
intermediate layer m to the embedding, where c is the number of channels (feature dimensions) and
w×h is the spatial resolution. Thus, our model f can be expressed as the composition f = fm ◦gm."
IMPROVING REPRESENTATIONS USING MIXUP,0.19722222222222222,"For manifold mixup, we follow (Venkataramanan et al., 2021) and mix either features of intermedi-
ate layer m or the ﬁnal embeddings. Thus, we deﬁne three mixup types in total:"
IMPROVING REPRESENTATIONS USING MIXUP,0.2,"fλ(x, x′) := 
 "
IMPROVING REPRESENTATIONS USING MIXUP,0.20277777777777778,"f(mixλ(x, x′)),
input mixup
fm(mixλ(gm(x), gm(x′))),
feature mixup
mixλ(f(x), f(x′)),
embedding mixup.
(4)"
IMPROVING REPRESENTATIONS USING MIXUP,0.20555555555555555,"Function fλ : X 2 →Rd performs both mixup and embedding. We explore different mixup types in
subsection B.4 of the Appendix."
LABEL REPRESENTATION,0.20833333333333334,"3.4
LABEL REPRESENTATION"
LABEL REPRESENTATION,0.2111111111111111,"Classiﬁcation
In supervised classiﬁcation, each example x ∈X is assigned an one-hot encoded
label y ∈{0, 1}C, where C is the number of classes. Label vectors are also linearly interpolated:
given two labeled examples (x, y), (x′, y′), the interpolated label is mixλ(y, y′). The loss (cross-
entropy) is a continuous function of the label vector. We extend this idea to metric learning."
LABEL REPRESENTATION,0.21388888888888888,"Metric learning
Positives P(a) and negatives N(a) of anchor a are deﬁned as having the same
or different class label as the anchor, respectively. To every example in P(a) ∪N(a), we assign a
binary (two-class) label y ∈{0, 1}, such that y = 1 for positives and y = 0 for negatives:"
LABEL REPRESENTATION,0.21666666666666667,"U +(a) := {(p, 1) : p ∈P(a)}
(5)"
LABEL REPRESENTATION,0.21944444444444444,"U −(a) := {(n, 0) : n ∈N(a)}
(6)"
LABEL REPRESENTATION,0.2222222222222222,"Thus, we represent both positives and negatives by U(a) := U +(a) ∪U −(a). We now rewrite the
generic loss function (3) as:"
LABEL REPRESENTATION,0.225,"ℓ(a; θ) := τ  σ+  
X"
LABEL REPRESENTATION,0.22777777777777777,"(x,y)∈U(a)
yρ+(s(a, x))  + σ−  
X"
LABEL REPRESENTATION,0.23055555555555557,"(x,y)∈U(a)
(1 −y)ρ−(s(a, x))   "
LABEL REPRESENTATION,0.23333333333333334,".
(7)"
LABEL REPRESENTATION,0.2361111111111111,"Here, every labeled example (x, y) in U(a) appears in both positive and negative terms. However,
because label y is binary, only one of the two contributions is nonzero. Now, in the presence of
mixup, we can linearly interpolate labels exactly as in classiﬁcation."
MIXED LOSS FUNCTION,0.2388888888888889,"3.5
MIXED LOSS FUNCTION"
MIXED LOSS FUNCTION,0.24166666666666667,"Mixup
For every anchor a, we are given a set M(a) of pairs of examples to mix. This is a subset of
(S(a) ∪U(a)) × U(a) where S(a) := (a, 1). That is, we allow mixing between positive-negative,
positive-positive and negative-negative pairs, where the anchor itself is also seen as positive. We"
MIXED LOSS FUNCTION,0.24444444444444444,Published as a conference paper at ICLR 2022
MIXED LOSS FUNCTION,0.24722222222222223,"deﬁne the possible choices of mixing pairs M(a) in subsection 4.1 and we assess them in subsec-
tion B.4 of the Appendix. Let V (a) be the set of corresponding labeled mixed embeddings"
MIXED LOSS FUNCTION,0.25,"V (a) := {(fλ(x, x′), mixλ(y, y′)) : ((x, y), (x′, y′)) ∈M(a), λ ∼Beta(α, α)},
(8)"
MIXED LOSS FUNCTION,0.25277777777777777,"where fλ is deﬁned by (4). With these deﬁnitions in place, the generic loss function eℓover mixed
examples takes exactly the same form as (7), with only U(a) replaced by V (a):"
MIXED LOSS FUNCTION,0.25555555555555554,"eℓ(a; θ) := τ  σ+  
X"
MIXED LOSS FUNCTION,0.25833333333333336,"(v,y)∈V (a)
yρ+(s(a, v))  + σ−  
X"
MIXED LOSS FUNCTION,0.2611111111111111,"(v,y)∈V (a)
(1 −y)ρ−(s(a, v))   "
MIXED LOSS FUNCTION,0.2638888888888889,",
(9)"
MIXED LOSS FUNCTION,0.26666666666666666,"where similarity s is evaluated on X × Rd for pair-based losses and on Rd × Rd for proxy anchor.
Now, every labeled embedding (v, y) in V (a) appears in both positive and negative terms and both
contributions are nonzero for positive-negative pairs, because after interpolation, y ∈[0, 1]."
MIXED LOSS FUNCTION,0.26944444444444443,"Error function
Parameters θ are learned by minimizing the error function, which is a linear com-
bination of the clean loss (3) and the mixed loss (9), averaged over all anchors"
MIXED LOSS FUNCTION,0.2722222222222222,"E(X; θ) :=
1
|X| X"
MIXED LOSS FUNCTION,0.275,"a∈X
ℓ(a; θ) + weℓ(a; θ),
(10)"
MIXED LOSS FUNCTION,0.2777777777777778,"where w ≥0 is the mixing strength. At least for manifold mixup, this combination comes at little
additional cost, since clean embeddings are readily available."
MIXED LOSS FUNCTION,0.28055555555555556,"3.6
ANALYSIS: MIXED EMBEDDINGS AND POSITIVITY"
MIXED LOSS FUNCTION,0.2833333333333333,"Let Pos(a, v) be the event that a mixed embedding v behaves as “positive” for anchor a, i.e., min-
imizing the loss eℓ(a; θ) will increase the similarity s(a, v). In subsection A.2 of the Appendix, we
explain that this “positivity” is equivalent to ∂eℓ(a; θ)/∂s(a, v) ≤0. Under positive-negative mix-
ing, i.e., M(a) ⊂U +(a) × U −(a), we then estimate the probability of Pos(a, v) as a function of λ
in the case of multi-similarity (2) with a single mixed embedding v:"
MIXED LOSS FUNCTION,0.2861111111111111,"P(Pos(a, v)) = Fλ"
MIXED LOSS FUNCTION,0.28888888888888886,"
1
β + γ ln

λ
1 −λ"
MIXED LOSS FUNCTION,0.2916666666666667,"
+ m

,
(11)"
MIXED LOSS FUNCTION,0.29444444444444445,"where Fλ is the CDF of similarities s(a, v) between anchors a and mixed embeddings v with in-
terpolation factor λ. In Figure 2, we measure the probability of Pos(a, v) as a function of λ in two
ways, both purely empirically and theoretically by (11). Both measurements are increasing func-
tions of λ of sigmoidal shape, where a mixed embedding is mostly positive for λ close to 1 and
mostly negative for λ close to 0."
EXPERIMENTS,0.2972222222222222,"4
EXPERIMENTS"
SETUP,0.3,"4.1
SETUP"
SETUP,0.30277777777777776,"Datasets
We experiment on Caltech-UCSD Birds (CUB200) (Wah et al., 2011), Stanford Cars
(Cars196) (Krause et al., 2013), Stanford Online Products (SOP) (Oh Song et al., 2016) and In-Shop
Clothing retrieval (In-Shop) (Liu et al., 2016) image datasets. More details are in subsection B.1."
SETUP,0.3055555555555556,"Network, features and embeddings
We use Resnet-50 (He et al., 2016) (R-50) pretrained on Im-
ageNet (Russakovsky et al., 2015) as a backbone network. We obtain the intermediate representation
(feature), a 7×7×2048 tensor, from the last convolutional layer. Following (Kim et al., 2020c), we
combine adaptive average pooling with max pooling, followed by a fully-connected layer to obtain
the embedding of d = 512 dimensions."
SETUP,0.30833333333333335,"Loss functions
We reproduce contrastive (Cont) (Hadsell et al., 2006),
multi-similarity
(MS) (Wang et al., 2019), proxy anchor (PA) (Kim et al., 2020c) and ProxyNCA++ (Teh et al.,
2020) and we evaluate them under different mixup types. For MS (2), following Musgrave et al.
(2020), we use β = 18, γ = 75 and m = 0.77. For PA, we use β = γ = 32 and m = 0.1, as
reported by the authors. Details on training are in subsection B.1 of the Appendix."
SETUP,0.3111111111111111,Published as a conference paper at ICLR 2022
SETUP,0.3138888888888889,"Methods
We compare our method, Metrix, with proxy synthesis (PS) (Gu et al., 2021), i-mix (Lee
et al., 2021) and MoCHi (Kalantidis et al., 2020). For PS, we adapt the ofﬁcial code1 to PA on all
datasets, and use it with PA only, because it is designed for proxy-based losses. PS has been shown
superior to (Ko & Gu, 2020; Gu & Ko, 2020), although in different networks. MoCHi and i-mix are
meant for contrastive representation learning. We evaluate using Recall@K (Oh Song et al., 2016):
For each test example taken as a query, we ﬁnd its K-nearest neighbors in the test set excluding
itself in the embedding space. We assign a score of 1 if an example of the same class is contained in
the neighbors and 0 otherwise. Recall@K is the average of this score over the test set."
SETUP,0.31666666666666665,"0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 λ"
SETUP,0.3194444444444444,"P(Pos(a, v))"
SETUP,0.32222222222222224,"empirical
theoretical"
SETUP,0.325,"Figure 2: “Positivity” of mixed embeddings vs.
λ.
We measure P(Pos(a, v)) empirically as
P(∂eℓMS(a; θ)/∂s(a, v) ≤0) and theoretically
by (11), where Fλ is again measured from data.
We use embedding mixup on MS (2) on CUB200
at epoch 0, based on the setup of subsection 4.1."
SETUP,0.3277777777777778,"Mixup settings
For input mixup, we use the
k hardest negative examples for each anchor
(each example in the batch) and mix them
with positives or with the anchor.
We use
k = 3 by default. For manifold mixup, we
focus on the last few layers instead, where
features and embeddings are compact, and we
mix all pairs. We use feature mixup by de-
fault and call it Metrix/feature or just Metrix,
while input and embedding mixup are called
Metrix/input and Metrix/embed, respectively.
For all mixup types, we use clean exam-
ples as anchors and we deﬁne a set M(a) of
pairs of examples to mix for each anchor a,
with their labels (positive or negative).
By
default, we mix positive-negative or anchor-
negative pairs, by choosing uniformly at ran-
dom between M(a) := U +(a) × U −(a) and
M(a) := S(a) × U −(a), respectively, where
U −(a) is replaced by hard negatives only for input mixup. More details are in subsection B.2."
RESULTS,0.33055555555555555,"4.2
RESULTS"
RESULTS,0.3333333333333333,"Improving the state of the art
As shown in Table 2, Metrix consistently improves the perfor-
mance of all baseline losses (Cont, MS, PA, ProxyNCA++) across all datasets. More results in
subsection B.3 of the Appendix reveal that the same is true for Metrix/input and Metrix/embed too.
Surprisingly, MS outperforms PA and ProxyNCA++ under mixup on all datasets but SOP, where the
three losses are on par. This is despite the fact that baseline PA outperforms MS on CUB200 and
Cars-196, while ProxyNCA++ outperforms MS on SOP and In-Shop. Both contrastive and MS are
signiﬁcantly improved by mixup. By contrast, improvements on PA and ProxyNCA++ are marginal,
which may be due to the already strong performance of PA, or further improvement is possible by
employing different mixup methods that take advantage of the image structure."
RESULTS,0.33611111111111114,"In terms of Recall@1, our MS+Metrix is best overall, improving by 3.6% (67.8 →71.4) on
CUB200, 1.8% (87.8 →89.6) on Cars196, 4.1% (76.9 →81.0) on SOP and 2.1% (90.1 →92.2)
on In-Shop. The same solution sets new state of the art, outperforming the previously best PA by
1.7% (69.7 →71.4) on CUB200, MS by 1.8% (87.8 →89.6) on Cars196, ProxyNCA++ by 0.3%
(80.7 →81.0) on SOP and SoftTriple by 1.2% (91.0 →92.2) on In-Shop. Importantly, while the
previous state of the art comes from a different loss per dataset, MS+Metrix is almost consistently
best across all datasets."
RESULTS,0.3388888888888889,"Alternative mixing methods
In Table 3, we compare Metrix/input with i-Mix (Lee et al., 2021)
and Metrix/embed with MoCHi (Kalantidis et al., 2020) using contrastive loss, and with PS (Gu
et al., 2021) using PA. MoCHi and PS mix embeddings only, while labels are always negative. For
i-Mix, we mix anchor-negative pairs (S(a) × U −(a)). For MoCHi, the anchor is clean and we mix
negative-negative (U −(a)2) and anchor-negative (S(a)×U −(a)) pairs, where U −(a) is replaced by
k = 100 hardest negatives and λ ∈(0, 0.5) for anchor-negative. PS mixes embeddings of different
classes and treats them as new classes. For clean anchors, this corresponds to positive-negative
(U +(a) × U −(a)) and negative-negative (U −(a)2) pairs, but PS also supports mixed anchors."
RESULTS,0.3416666666666667,1https://github.com/navervision/proxy-synthesis
RESULTS,0.34444444444444444,Published as a conference paper at ICLR 2022
RESULTS,0.3472222222222222,"CUB200
CARS196
SOP
IN-SHOP"
RESULTS,0.35,"METHOD
R@1
R@2
R@4
R@1
R@2
R@4
R@1
R@10
R@100
R@1
R@10
R@20"
RESULTS,0.3527777777777778,"Triplet (Weinberger & Saul, 2009)
63.5
75.6
84.4
77.3
85.4
90.8
70.5
85.6
94.3
85.3
96.6
97.8
LiftedStructure (Oh Song et al., 2016)
65.9
75.8
84.5
81.4
88.3
92.4
76.1
88.6
95.2
88.6
97.6
98.4
ProxyNCA (Movshovitz-Attias et al., 2017)
65.2
75.6
83.8
81.2
87.9
92.6
73.2
87.0
94.4
86.2
95.9
97.0
Margin (Wu et al., 2017)
65.0
76.2
84.6
82.1
88.7
92.7
74.8
87.8
94.8
88.6
97.0
97.8
SoftTriple (Qian et al., 2019)
67.3
77.7
86.2
86.5
91.9
95.3
79.8
91.2
96.3
91.0
97.6
98.3
D&C (Sanakoyeu et al., 2019)∗
65.9
76.6
84.4
84.6
90.7
94.1
75.9
88.4
94.9
85.7
95.5
96.9
EPSHN (Xuan et al., 2020)∗
64.9
75.3
83.5
82.7
89.3
93.0
78.3
90.7
96.3
87.8
95.7
96.8"
RESULTS,0.35555555555555557,"Cont (Hadsell et al., 2006)
64.7
75.9
84.6
81.6
88.2
92.7
74.9
87.0
93.9
86.4
94.7
96.2
+Metrix
67.4
77.9
85.7
85.1
91.1
94.6
77.5
89.1
95.5
89.1
95.7
97.1
+2.7
+2.0
+1.1
+3.5
+2.9
+1.9
+2.6
+2.1
+1.5
+2.7
+1.0
+0.9"
RESULTS,0.35833333333333334,"MS (Wang et al., 2019)
67.8
77.8
85.6
87.8
92.7
95.3
76.9
89.8
95.9
90.1
97.6
98.4
+Metrix
71.4
80.6
86.8
89.6
94.2
96.0
81.0
92.0
97.2
92.2
98.5
98.6
+3.6
+2.8
+1.2
+1.8
+1.5
+0.7
+4.1
+2.2
+1.3
+2.1
+0.9
+0.2"
RESULTS,0.3611111111111111,"PA (Kim et al., 2020c)∗
69.7
80.0
87.0
87.7
92.9
95.8
–
–
–
–
–
–
PA (Kim et al., 2020c)
69.5
79.3
87.0
87.6
92.3
95.5
79.1
90.8
96.2
90.0
97.4
98.2
+Metrix
71.0
81.8
88.2
89.1
93.6
96.7
81.3
91.7
96.9
91.9
98.2
98.8
+1.3
+1.8
+1.2
+1.4
+0.7
+0.9
+2.2
+0.9
+0.7
+1.9
+0.8
+0.6"
RESULTS,0.3638888888888889,"ProxyNCA++ (Teh et al., 2020)∗
69.0
79.8
87.3
86.5
92.5
95.7
80.7
92.0
96.7
90.4
98.1
98.8
ProxyNCA++ (Teh et al., 2020)
69.1
79.5
87.7
86.6
92.1
95.4
80.4
91.7
96.7
90.2
97.6
98.4
+Metrix
70.4
80.6
88.7
88.5
93.4
96.5
81.3
92.7
97.1
91.9
98.1
98.8
+1.3
+0.8
+1.0
+1.9
+0.9
+0.8
+0.6
+0.7
+0.4
+1.5
+0.0
+0.0"
RESULTS,0.36666666666666664,"Gain over SOTA
+1.7
+1.8
+0.5
+1.8
+1.3
+0.9
+0.6
+0.7
+0.5
+1.2
+0.4
+0.0"
RESULTS,0.36944444444444446,"Table 2: Improving the SOTA with our Metrix (Metrix/feature) using Resnet-50 with embedding size
d = 512. R@K (%): Recall@K; higher is better. ∗: reported by authors. Bold black: best baseline
(previous SOTA, one per column). Red: Our new SOTA. Gain over SOTA is over best baseline.
MS: Multi-Similarity, PA: Proxy Anchor. Additional results are in subsection B.3 of the Appendix."
RESULTS,0.37222222222222223,"In terms of Recall@1, Metrix/input outperforms i-Mix with anchor-negative pairs by 0.5% (65.8 →
66.3) on CUB200, 0.9% (82.0 →82.9) on Cars196, 0.6% (75.2 →75.8) and 0.6% (87.1 →87.7)
on In-Shop. Metrix/embed outperforms MoCHI with anchor-negative pairs by 1.2% (65.2 →66.4)
on CUB200, 1.4% (82.5 →83.9) on Cars196, 0.9% (75.8 →76.7) and 1.2% (87.2 →88.4) on
In-Shop. The gain over MoCHi with negative-negative pairs is signiﬁcantly higher. Metrix/embed
also outperforms PS by 0.4% (70.0 →70.4) on CUB200, 1% (87.9 →88.9) on Cars196, 1%
(79.6 →80.6) on SOP and 1.3% (90.3 →91.6) on In-Shop."
RESULTS,0.375,"Computational complexity
We study the computational complexity of Metrix in subsection B.3."
RESULTS,0.37777777777777777,"Ablation study
We study the effect of the number k of hard negatives, mixup types (input, feature
and embedding), mixing pairs and mixup strength w in subsection B.4 of the Appendix."
RESULTS,0.38055555555555554,"4.3
HOW DOES MIXUP IMPROVE REPRESENTATIONS?"
RESULTS,0.38333333333333336,"We analyze how Metrix improves representation learning, given the difference between distributions
at training and inference. As discussed in section 1, since the classes at inference are unseen at train-
ing, one might expect interpolation-based data augmentation like mixup to be even more important
than in classiﬁcation. This is so because, by mixing examples during training, we are exploring
areas of the embedding space beyond the training classes. We hope that this exploration would pos-
sibly lead the model to implicitly learn a representation more appropriate for the test classes, if the
distribution of the test classes lies near these areas."
RESULTS,0.3861111111111111,"Alignment and Uniformity
In terms of quantitative measures of properties of the training and
test distributions, we follow Wang & Isola (2020). This work introduces two measures – alignment
and uniformity (the lower the better) to be used both as loss functions (on the training set) and as
evaluation metrics (on the test set). Alignment measures the expected pairwise distance between
positive examples in the embedding space. A small value of alignment indicates that the positive
examples are clustered together. Uniformity measures the (log of the) expected pairwise similarity
between all examples regardless of class, using a Gaussian kernel as similarity. A small value
of uniformity indicates that the distribution is more uniform over the embedding space, which is
particularly relevant to our problem. Meant for contrastive learning, (Wang & Isola, 2020) use the
same training and test classes, while in our case they are different."
RESULTS,0.3888888888888889,Published as a conference paper at ICLR 2022
RESULTS,0.39166666666666666,"CUB200
CARS196
SOP
IN-SHOP"
RESULTS,0.39444444444444443,"METHOD
MIXING PAIRS
R@1 R@2 R@4 R@1 R@2 R@4 R@1 R@10 R@100 R@1 R@10 R@20"
RESULTS,0.3972222222222222,"Cont (Hadsell et al., 2006)
–
64.7 75.9 84.6
81.6 88.2 92.7
74.9
87.0
93.9
86.4
94.7
96.3
+ i-Mix (Lee et al., 2021)
anc-neg
65.8 76.2 84.9
82.0 88.5 93.2
75.2
87.3
94.2
87.1
95.4
96.1
+ Metrix/input
pos-neg / anc-neg 66.3 77.1 85.2
82.9 89.3 93.7
75.8
87.8
94.6
87.7
95.9
96.5"
RESULTS,0.4,"+MoCHi (Kalantidis et al., 2020)
neg-neg
63.1 74.3 83.8
76.3 84.0 89.3
68.9
83.1
91.8
81.8
91.9
93.9
+MoCHi (Kalantidis et al., 2020)
anc-neg
65.2 75.8 84.2
82.5 88.0 92.9
75.8
87.1
94.8
87.2
92.8
94.9
+Metrix/embed
pos-neg / anc-neg 66.4 77.6 85.4
83.9 90.3 94.1
76.7
88.6
95.2
88.4
95.4
96.9"
RESULTS,0.4027777777777778,"PA (Kim et al., 2020c)
–
69.7 80.0 87.0
87.6 92.3 95.5
79.1
90.8
96.2
90.0
97.4
98.2
+PS (Gu et al., 2021)
pos-neg / neg-neg 70.0 79.8 87.2
87.9 92.8 95.6
79.6
90.9
96.4
90.3
97.4
98.0
+Metrix/embed
pos-neg / anc-neg 70.4 81.1 87.9
88.9 93.3 96.4
80.6
91.7
96.6
91.6
98.3
98.3"
RESULTS,0.40555555555555556,"Table 3: Comparison of our Metrix/embed with other mixing methods using R-50 with embedding
size d = 512. R@K (%): Recall@K; higher is better. PA: Proxy Anchor, PS: Proxy Synthesis."
RESULTS,0.4083333333333333,"By training with contrastive loss on CUB200 and then measuring on the test set, we achieve an
alignment (lower the better) of 0.28 for contrastive loss, 0.28 for i-Mix (Lee et al., 2021) and 0.19
for Metrix/input. MoCHi (Kalantidis et al., 2020) and Metrix/embed achieve an alignment of 0.19
and 0.17, respectively. We also obtain a uniformity (lower the better) of −2.71 for contrastive loss,
−2.13 for i-Mix and −3.13 for Metrix/input. The uniformity of MoCHi and Metrix/embed is −3.18
and −3.25, respectively. This indicates that Metrix helps obtain a test distribution that is more
uniform over the embedding space, where classes are better clustered and better separated."
RESULTS,0.4111111111111111,"Utilization
The measures proposed by (Wang & Isola, 2020) are limited to a single distribution
or dataset, either the training set (as loss functions) or the test set (as evaluation metrics). It is more
interesting to measure the extent to which a test example, seen as a query, lies near any of the training
examples, clean or mixed. For this, we introduce the measure of utilization u(Q, X) of the training
set X by the test set Q as
u(Q, X) =
1
|Q| X"
RESULTS,0.41388888888888886,"q∈Q
min
x∈X ∥f(q) −f(x)∥2
(12)"
RESULTS,0.4166666666666667,"Utilization measures the average, over the test set Q, of the minimum distance of a query q to
a training example x ∈X in the embedding space of the trained model f (lower is better). A
low value of utilization indicates that there are examples in the training set that are similar to test
examples. When using mixup, we measure utilization as u(Q, ˆX), where ˆX is the augmented
training set including clean and mixed examples over a number of epochs and f remains ﬁxed.
Because X ⊂ˆX, we expect u(Q, ˆX) < u(Q, X), that is, the embedding space is better explored in
the presence of mixup."
RESULTS,0.41944444444444445,"By using contrastive loss on CUB200, utilization drops from 0.41 to 0.32 when using Metrix. This
indicates that test samples are indeed closer to mixed examples than clean in the embedding space.
This validates our hypothesis that a representation more appropriate for test classes is implicitly
learned during exploration of the embedding space in the presence of mixup."
CONCLUSION,0.4222222222222222,"5
CONCLUSION"
CONCLUSION,0.425,"Based on the argument that metric learning is binary classiﬁcation of pairs of examples into “pos-
itive” and “negative”, we have introduced a direct extension of mixup from classiﬁcation to metric
learning. Our formulation is generic, applying to a large class of loss functions that separate pos-
itives from negatives per anchor and involve component functions that are additive over examples.
Those are exactly loss functions that require less mining. We contribute a principled way of in-
terpolating labels, such that the interpolation factor affects the relative weighting of positives and
negatives. Other than that, our approach is completely agnostic with respect to the mixup method,
opening the way to using more advanced mixup methods for metric learning."
CONCLUSION,0.42777777777777776,"We consistently outperform baselines using a number of loss functions on a number of bench-
marks and we improve the state of the art using a single loss function on all benchmarks, while
previous state of the art was not consistent in this respect. Surprisingly, this loss function, multi-
similarity Wang et al. (2019), is not the state of the art without mixup. Because metric learning is
about generalizing to unseen classes and distributions, our work may have applications to other such
problems, including transfer learning, few-shot learning and continual learning."
CONCLUSION,0.4305555555555556,Published as a conference paper at ICLR 2022
ACKNOWLEDGEMENT,0.43333333333333335,"6
ACKNOWLEDGEMENT"
ACKNOWLEDGEMENT,0.4361111111111111,"Shashanka’s work was supported by the ANR-19-CE23-0028 MEERQAT project and was per-
formed using the HPC resources from GENCI-IDRIS Grant 2021 AD011011709R1. Bill’s work
was partially supported by the EU RAMONES project grant No. 101017808 and was performed
using the HPC resources from GRNET S.A. project pr011028. This work was partially done when
Yannis was at Inria."
REFERENCES,0.4388888888888889,REFERENCES
REFERENCES,0.44166666666666665,"Christopher Beckham, Sina Honari, Vikas Verma, Alex Lamb, Farnoosh Ghadiri, R Devon Hjelm,
Yoshua Bengio, and Christopher Pal.
On adversarial mixup resynthesis.
arXiv preprint
arXiv:1903.02709, 2019."
REFERENCES,0.4444444444444444,"David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow.
Understanding and improving
interpolation in autoencoders via an adversarial regularizer. arXiv preprint arXiv:1807.07543,
2018."
REFERENCES,0.44722222222222224,"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In ECCV, 2018."
REFERENCES,0.45,"Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: a deep quadru-
plet network for person re-identiﬁcation. In CVPR, 2017."
REFERENCES,0.4527777777777778,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018."
REFERENCES,0.45555555555555555,"Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017."
REFERENCES,0.4583333333333333,"Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML,
2014."
REFERENCES,0.46111111111111114,"Jacob Goldberger, Sam Roweis, Geoffrey Hinton, and Ruslan Salakhutdinov. Neighbourhood com-
ponents analysis. In NIPS, 2005."
REFERENCES,0.4638888888888889,"Albert Gordo, Jon Almaz´an, Jerome Revaud, and Diane Larlus. Deep image retrieval: Learning
global representations for image search. In ECCV, 2016."
REFERENCES,0.4666666666666667,"Geonmo Gu and Byungsoo Ko. Symmetrical synthesis for deep metric learning. In AAAI, 2020."
REFERENCES,0.46944444444444444,"Geonmo Gu, Byungsoo Ko, and Han-Gyu Kim. Proxy synthesis: Learning with synthetic classes
for deep metric learning. In AAAI, 2021."
REFERENCES,0.4722222222222222,"Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In CVPR, 2006."
REFERENCES,0.475,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016."
REFERENCES,0.4777777777777778,"Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. Augmix: A simple data processing method to improve robustness and uncertainty.
ICLR, 2020."
REFERENCES,0.48055555555555557,"Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-
identiﬁcation. arXiv preprint arXiv:1703.07737, 2017."
REFERENCES,0.48333333333333334,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015."
REFERENCES,0.4861111111111111,"Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard
negative mixing for contrastive learning. NeurIPS, 2020."
REFERENCES,0.4888888888888889,Published as a conference paper at ICLR 2022
REFERENCES,0.49166666666666664,"Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. In ICML, 2020a."
REFERENCES,0.49444444444444446,"Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint
mixup with supermodular diversity. In ICLR, 2021."
REFERENCES,0.49722222222222223,"Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun. Mixco: Mix-up contrastive learning
for visual representation. NeurIPS Workshop on Self-Supervised Learning, 2020b."
REFERENCES,0.5,"Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric
learning. In CVPR, 2020c."
REFERENCES,0.5027777777777778,"Byungsoo Ko and Geonmo Gu. Embedding expansion: Augmentation in embedding space for deep
metric learning. In CVPR, 2020."
REFERENCES,0.5055555555555555,"Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. In ECCV, 2020."
REFERENCES,0.5083333333333333,"Jonathan Krause, Michael Stark, Jia Deng, and Fei-Fei Li. 3d object representations for ﬁne-grained
categorization. ICCVW, 2013."
REFERENCES,0.5111111111111111,"Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. I-mix: A
domain-agnostic strategy for contrastive representation learning. In ICLR, 2021."
REFERENCES,0.5138888888888888,"Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia,
and Jane You. Data augmentation via latent space interpolation for image classiﬁcation. In ICPR,
2018."
REFERENCES,0.5166666666666667,"Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust
clothes recognition and retrieval with rich annotations. In CVPR, 2016."
REFERENCES,0.5194444444444445,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019."
REFERENCES,0.5222222222222223,"Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No
fuss distance metric learning using proxies. In ICCV, 2017."
REFERENCES,0.525,"Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. In ECCV,
2020."
REFERENCES,0.5277777777777778,"Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted
structured feature embedding. In CVPR, 2016."
REFERENCES,0.5305555555555556,"Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric
learning without triplet sampling. In ICCV, 2019."
REFERENCES,0.5333333333333333,"Jie Qin, Jiemin Fang, Qian Zhang, Wenyu Liu, Xingang Wang, and Xinggang Wang. Resizemix:
Mixing data with preserved object information and true labels. arXiv preprint arXiv:2012.11101,
2020."
REFERENCES,0.5361111111111111,"Nils Reimers and Iryna Gurevych.
Sentence-bert: Sentence embeddings using siamese bert-
networks. In EMNLP, 2019."
REFERENCES,0.5388888888888889,"Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with
hard negative samples. ICLR, 2021."
REFERENCES,0.5416666666666666,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. IJCV, 2015."
REFERENCES,0.5444444444444444,"Artsiom Sanakoyeu, Vadim Tschernezki, Uta Buchler, and Bjorn Ommer. Divide and conquer the
embedding space for metric learning. In CVPR, 2019."
REFERENCES,0.5472222222222223,"Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face
recognition and clustering. In CVPR, 2015."
REFERENCES,0.55,Published as a conference paper at ICLR 2022
REFERENCES,0.5527777777777778,"Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NIPS, 2016."
REFERENCES,0.5555555555555556,"Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and conﬁdence. arXiv preprint arXiv:2001.07685, 2020."
REFERENCES,0.5583333333333333,"Eu Wern Teh, Terrance DeVries, and Graham W Taylor. Proxynca++: Revisiting and revitalizing
proxy neighborhood component analysis. In ECCV, 2020."
REFERENCES,0.5611111111111111,"A. F. M. Shahab Uddin, Sirazam Monira Mst., Wheemyung Shin, TaeChoong Chung, and Sung-Ho
Bae. Saliencymix: A saliency guided data augmentation strategy for better regularization. In
ICLR, 2021."
REFERENCES,0.5638888888888889,"Shashanka Venkataramanan, Yannis Avrithis, Ewa Kijak, and Laurent Amsaleg. Alignmix: Improv-
ing representation by interpolating aligned features. arXiv preprint arXiv:2103.15375, 2021."
REFERENCES,0.5666666666666667,"Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, David Lopez-
Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.
In ICML, 2019."
REFERENCES,0.5694444444444444,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. arXiv preprint arXiv:1606.04080, 2016."
REFERENCES,0.5722222222222222,"Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-
UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Institute of
Technology, 2011."
REFERENCES,0.575,"Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen,
and Ying Wu. Learning ﬁne-grained image similarity with deep ranking. In CVPR, 2014."
REFERENCES,0.5777777777777777,"Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In ICML, 2020."
REFERENCES,0.5805555555555556,"Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss
with general pair weighting for deep metric learning. In CVPR, 2019."
REFERENCES,0.5833333333333334,"Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neigh-
bor classiﬁcation. JMLR, 2009."
REFERENCES,0.5861111111111111,"Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Kr¨ahenb¨uhl. Sampling matters in
deep embedding learning. In ICCV, 2017."
REFERENCES,0.5888888888888889,"Eric P Xing, Michael I Jordan, Stuart J Russell, and Andrew Y Ng. Distance metric learning with
application to clustering with side-information. In NIPS, 2003."
REFERENCES,0.5916666666666667,"Hong Xuan, Abby Stylianou, and Robert Pless. Improved embeddings with easy positive triplet
mining. In WACV, 2020."
REFERENCES,0.5944444444444444,"Dong Yi, Zhen Lei, and Stan Z. Li. Deep metric learning for practical person re-identiﬁcation. arXiv
preprint arXiv:1703.07737, 2014."
REFERENCES,0.5972222222222222,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In ICCV,
2019."
REFERENCES,0.6,"Andrew Zhai and Hao-Yu Wu. Classiﬁcation is a strong baseline for deep metric learning. arXiv
preprint arXiv:1811.12649, 2018."
REFERENCES,0.6027777777777777,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In ICLR, 2018."
REFERENCES,0.6055555555555555,"Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, and Jie Zhou. Hardness-aware deep metric learning.
In CVPR, 2019."
REFERENCES,0.6083333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.6111111111111112,"Jianchao Zhu, Liangliang Shi, Junchi Yan, and Hongyuan Zha. Automix: Mixup networks for
sample interpolation via cooperative barycenter learning. In ECCV, 2020a."
REFERENCES,0.6138888888888889,"Yuehua Zhu, Muli Yang, Cheng Deng, and Wei Liu. Fewer is more: A deep graph metric learning
perspective using fewer proxies. NeurIPS, 2020b."
REFERENCES,0.6166666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.6194444444444445,"A
MORE ON THE METHOD"
REFERENCES,0.6222222222222222,"A.1
MIXED LOSS FUNCTION"
REFERENCES,0.625,"Interpretation
To better understand the two contributions of a labeled embedding (v, y) in V (a)
to the positive and negative terms of (9), consider the case of positive-negative mixing pairs,
M(a) ⊂U +(a) × U −(a). Then, for ((x, y), (x′, y′)) ∈M(a), the mixed label is mixλ(y, y′) =
mixλ(1, 0) = λ and (9) becomes"
REFERENCES,0.6277777777777778,"eℓ(a; θ) = τ  σ+  
X"
REFERENCES,0.6305555555555555,"(v,λ)∈V (a)
λρ+(s(a, v))  + σ−  
X"
REFERENCES,0.6333333333333333,"(v,λ)∈V (a)
(1 −λ)ρ−(s(a, v))   "
REFERENCES,0.6361111111111111,".
(13)"
REFERENCES,0.6388888888888888,"Thus, the mixed embedding v is both positive (with weight λ) and negative (with weight 1 −λ).
Whereas for positive-positive mixing, that is, for M(a) ⊂U +(a)2, the mixed label is 1 and the
negative term vanishes. Similarly, for negative-negative mixing, that is, for M(a) ⊂U −(a)2, the
mixed label is 0 and the positive term vanishes."
REFERENCES,0.6416666666666667,"In the particular case of contrastive (1) loss, positive-negative mixing (13) becomes"
REFERENCES,0.6444444444444445,"eℓcont(a; θ) :=
X"
REFERENCES,0.6472222222222223,"(v,λ)∈V (a)
−λs(a, v) +
X"
REFERENCES,0.65,"(v,λ)∈V (a)
(1 −λ)[s(a, v) −m]+.
(14)"
REFERENCES,0.6527777777777778,"Similarly, for multi-similarity (2),"
REFERENCES,0.6555555555555556,eℓMS(a; θ) := 1 β log 
REFERENCES,0.6583333333333333,"1 +
X"
REFERENCES,0.6611111111111111,"(v,λ)∈V (a)
λe−β(s(a,v)−m)  +"
REFERENCES,0.6638888888888889,"1
γ log "
REFERENCES,0.6666666666666666,"1 +
X"
REFERENCES,0.6694444444444444,"(v,λ)∈V (a)
(1 −λ)eγ(s(a,v)−m)  . (15)"
REFERENCES,0.6722222222222223,"A.2
ANALYSIS: MIXED EMBEDDINGS AND POSITIVITY"
REFERENCES,0.675,"Positivity
Under positive-negative mixing, (13) shows that a mixed embedding v with interpo-
lation factor λ behaves as both positive and negative to different extents, depending on λ: mostly
positive for λ close to 1, mostly negative for λ close to 0. The net effect depends on the derivative of
the loss with respect to the similarity ∂eℓ(a; θ)/∂s(a, v): if the derivative is negative, then v behaves
as positive and vice versa. This is clear from the chain rule"
REFERENCES,0.6777777777777778,∂eℓ(a; θ)
REFERENCES,0.6805555555555556,"∂v
= ∂eℓ(a; θ)"
REFERENCES,0.6833333333333333,"∂s(a, v) · ∂s(a, v)"
REFERENCES,0.6861111111111111,"∂v
,
(16)"
REFERENCES,0.6888888888888889,"because ∂s(a, v)/∂v is a vector pointing in a direction that makes a, v more similar and the loss is
being minimized. Let Pos(a, v) be the event that v behaves as “positive”, i.e., ∂eℓ(a; θ)/∂s(a, v) ≤0
and minimizing the loss will increase the similarity s(a, v)."
REFERENCES,0.6916666666666667,"Multi-similarity
We estimate the probability of Pos(a, v) as a function of λ in the case of multi-
similarity with a single embedding v obtained by mixing a positive with a negative:"
REFERENCES,0.6944444444444444,eℓMS(a; θ) = 1
REFERENCES,0.6972222222222222,"β log

1 + λe−β(s(a,v)−m)
+ 1"
REFERENCES,0.7,"γ log

1 + (1 −λ)eγ(s(a,v)−m)
.
(17)"
REFERENCES,0.7027777777777777,"In this case, Pos(a, v) occurs if and only if"
REFERENCES,0.7055555555555556,∂eℓMS(a; θ)
REFERENCES,0.7083333333333334,"∂s(a, v)
=
−λe−β(s(a,v)−m)"
REFERENCES,0.7111111111111111,"(1 + λe−β(s(a,v)−m)) +
(1 −λ)eγ(s(a,v)−m)"
REFERENCES,0.7138888888888889,"(1 + (1 −λ)eγ(s(a,v)−m)) ≤0.
(18)"
REFERENCES,0.7166666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.7194444444444444,Clean train examples
REFERENCES,0.7222222222222222,Mixed train examples
REFERENCES,0.725,Test examples
REFERENCES,0.7277777777777777,"(a)
(b) 𝑑
𝑑 𝑞
𝑞"
REFERENCES,0.7305555555555555,"Figure 3: Exploring the embedding space when using (a) only clean examples (b) clean and mixed
examples. Given a query q, the distance d to its nearest training embedding (clean or mixed) is
smaller with mixup (b) than without (a)."
REFERENCES,0.7333333333333333,"By letting t := s(a, v) −m, this condition is equivalent to"
REFERENCES,0.7361111111111112,(1 −λ)eγt
REFERENCES,0.7388888888888889,"(1 + (1 −λ)eγt) ≤
λe−βt"
REFERENCES,0.7416666666666667,"(1 + λe−βt)
(19)"
REFERENCES,0.7444444444444445,"(1 −λ)eγt(1 + λe−βt) ≤λe−βt(1 + (1 −λ)eγt)
(20)"
REFERENCES,0.7472222222222222,"(1 −λ)eγt + λ(1 −λ)e(γ−β)t ≤λe−βt + λ(1 −λ)e(γ−β)t
(21)"
REFERENCES,0.75,"e(β+γ)t ≤
λ
1 −λ
(22)"
REFERENCES,0.7527777777777778,"(β + γ)(s(a, v) −m) ≤ln

λ
1 −λ"
REFERENCES,0.7555555555555555,"
(23)"
REFERENCES,0.7583333333333333,"s(a, v) ≤
1
β + γ ln

λ
1 −λ"
REFERENCES,0.7611111111111111,"
+ m.
(24)"
REFERENCES,0.7638888888888888,"Finally, the probability of Pos(a, v) as a function of λ is"
REFERENCES,0.7666666666666667,"P(Pos(a, v)) = Fλ"
REFERENCES,0.7694444444444445,"
1
β + γ ln

λ
1 −λ"
REFERENCES,0.7722222222222223,"
+ m

,
(25)"
REFERENCES,0.775,"where Fλ is the CDF of similarities s(a, v) between anchors a and mixed embeddings v with inter-
polation factor λ."
REFERENCES,0.7777777777777778,"In Figure 2, we measure the probability of Pos(a, v) as a function of λ in two ways. First, we
measure the derivative ∂eℓMS(a; θ)/∂s(a, v) for anchors a and mixed embeddings v over the en-
tire dataset and we report the empirical probability of this derivative being non-positive versus λ.
Second, we measure P(Pos(a, v)) theoretically using (25), where the CDF of similarities s(a, v) is
again measured empirically for a and v over the dataset, as a function of λ. Despite the simplify-
ing assumption of a single positive and a single negative in deriving (25), we observe that the two
measurements agree in general. They are both increasing functions of λ of sigmoidal shape, they
roughly yield P(Pos(a, v)) ≥0.5 for λ ≥0.5 and they conﬁrm that a mixed embedding is mostly
positive for λ close to 1 and mostly negative for λ close to 0."
REFERENCES,0.7805555555555556,"A.3
MORE ON UTILIZATION"
REFERENCES,0.7833333333333333,"In subsection 4.3, we discuss that a representation more appropriate for test classes is implicitly
learned during exploration of the embedding space in the presence of mixup. We provide an illus-
tration of this exploration in Figure 3, where we visualize the embedding space using (a) only clean
train examples and (b) clean and mixed train examples. In case (a), the model is trained using only
clean examples, exploring a smaller area of the embedding space. In case (b), it is trained using both"
REFERENCES,0.7861111111111111,Published as a conference paper at ICLR 2022
REFERENCES,0.7888888888888889,"DATASET
CUB200 (Wah et al., 2011) CARS196 (Krause et al., 2013) SOP (Oh Song et al., 2016) IN-SHOP (Liu et al., 2016)"
REFERENCES,0.7916666666666666,"Objects
birds
cars
household furniture
clothes
# classes
200
196
22, 634
7, 982
# training images
5, 894
8, 092
60, 026
26, 356
# testing images
5, 894
8, 093
60, 027
26, 356
# training classes
100
98
11, 318
3991
# testing classes
100
98
11, 318
3991"
REFERENCES,0.7944444444444444,"sampling
random
random
balanced
balanced
samples per class
–
–
5
5
classes per batch
65†
70†
20
20"
REFERENCES,0.7972222222222223,"learning rate
1 × 10−4
1 × 10−4
3 × 10−5
1 × 10−4"
REFERENCES,0.8,Table 4: Statistics and settings for the four datasets we use in our experiments. †: average.
REFERENCES,0.8027777777777778,"mixed and clean examples, exploring a larger area. It is clear that the distance between a query and
its nearest training example (clean or mixup) is smaller in the presence of mixup. Utilization is the
average of this distance over the test set. This shows that the model implicitly learns a representation
closer the test example in the presence of mixup during training and it partially explains why mixup
leads to better performance."
REFERENCES,0.8055555555555556,"B
MORE ON EXPERIMENTS"
REFERENCES,0.8083333333333333,"B.1
SETUP"
REFERENCES,0.8111111111111111,"Datasets and sampling
Dataset statistics are summarized in Table 4. Since the number of classes
is large compared to the batch size in SOP and In-Shop, batches would rarely contain a positive pair
when sampled uniformly at random. Hence, we use balanced sampling (Zhai & Wu, 2018), i.e.,
a ﬁxed number of classes and examples per class, as shown in Table 4. For fair comparison with
baseline methods, images are randomly ﬂipped and cropped to 224 × 224 at training. At inference,
we resize to 256 × 256 and then center-crop to 224 × 224."
REFERENCES,0.8138888888888889,"Training
We train R-50 using AdamW (Loshchilov & Hutter, 2019) optimizer for 100 epochs
with a batch size 100. The initial learning rate per dataset is shown in Table 4. The learning rate is
decayed by 0.1 for Cont and by 0.5 for MS and PA on CUB200 and Cars196. For SOP and In-Shop,
we decay the learning rate by 0.25 for all losses. The weight decay is set to 0.0001."
REFERENCES,0.8166666666666667,"B.2
MIXUP SETTINGS"
REFERENCES,0.8194444444444444,"In mixup for classiﬁcation, given a batch of n examples, it is standard to form n pairs of examples
by pairing the batch with a random permutation of itself, resulting in n mixed examples, either for
input or manifold mixup. In metric learning, it is common to obtain n embeddings and then use all
1
2n(n −1) pairs of embeddings in computing the loss. We thus treat mixup types differently."
REFERENCES,0.8222222222222222,"Input mixup
Mixing all pairs would be computationally expensive in this case, because we would
compute 1"
REFERENCES,0.825,"2n(n −1) embeddings. A random permutation would not produce as many hard examples
as can be found in all pairs. Thus, for each anchor (each example in the batch), we use the k hardest
negative examples and mix them with positives or with the anchor. We use k = 3 by default."
REFERENCES,0.8277777777777777,"Manifold mixup
Originally, manifold mixup (Verma et al., 2019) focuses on the ﬁrst few layers
of the network. Mixing all pairs would then be even more expensive than input mixup, because
intermediate features (tensors) are even larger than input examples. Hence, we focus on the last few
layers instead, where features and embeddings are compact, and we mix all pairs. We use feature
mixup by default and call it Metrix/feature or just Metrix, while input and embedding mixup are
called Metrix/input and Metrix/embed, respectively. All options are studied in subsection B.4."
REFERENCES,0.8305555555555556,"Mixing pairs
Whatever the mixup type, we use clean examples as anchors and we deﬁne a set
M(a) of pairs of examples to mix for each anchor a, with their labels (positive or negative). By
default, we mix positive-negative or anchor-negative pairs, according to M(a) := U +(a) × U −(a)"
REFERENCES,0.8333333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.8361111111111111,"CUB200
CARS196
SOP
IN-SHOP"
REFERENCES,0.8388888888888889,"Method
1
2
4
1
2
4
1
10
100
1
10
20"
REFERENCES,0.8416666666666667,"Triplet (Weinberger & Saul, 2009)
63.5
75.6
84.4
77.3
85.4
90.8
70.5
85.6
94.3
85.3
96.6
97.8
LiftedStructure (Oh Song et al., 2016)
65.9
75.8
84.5
81.4
88.3
92.4
76.1
88.6
95.2
88.6
97.6
98.4
ProxyNCA (Movshovitz-Attias et al., 2017)
65.2
75.6
83.8
81.2
87.9
92.6
73.2
87.0
94.4
86.2
95.9
97.0
Margin (Wu et al., 2017)
65.0
76.2
84.6
82.1
88.7
92.7
74.8
87.8
94.8
88.6
97.0
97.8
SoftTriple (Qian et al., 2019)
67.3
77.7
86.2
86.5
91.9
95.3
79.8
91.2
96.3
91.0
97.6
98.3
D&C (Sanakoyeu et al., 2019)∗
65.9
76.6
84.4
84.6
90.7
94.1
75.9
88.4
94.9
85.7
95.5
96.9
EPSHN (Xuan et al., 2020)∗
64.9
75.3
83.5
82.7
89.3
93.0
78.3
90.7
96.3
87.8
95.7
96.8
ProxyNCA++ (Teh et al., 2020)∗
69.0
79.8
87.3
86.5
92.5
95.7
80.7
92.0
96.7
90.4
98.1
98.8"
REFERENCES,0.8444444444444444,"Cont (Hadsell et al., 2006)
64.7
75.9
84.6
81.6
88.2
92.7
74.9
87.0
93.9
86.4
94.7
96.2
+Metrix/input
66.3
77.1
85.2
82.9
89.3
93.7
75.8
87.8
94.6
87.7
95.9
96.5
+1.6
+1.2
+0.6
+1.3
+1.1
+1.0
+0.9
+0.8
+0.7
+1.3
+1.2
+0.3
+Metrix
67.4
77.9
85.7
85.1
91.1
94.6
77.5
89.1
95.5
89.1
95.7
97.1
+2.7
+2.0
+1.1
+3.5
+2.9
+1.9
+2.6
+2.1
+1.5
+2.7
+1.0
+0.9
+Metrix/embed
66.4
77.6
85.4
83.9
90.3
94.1
76.7
88.6
95.2
88.4
95.4
96.8
+1.7
+1.7
+0.8
+2.3
+2.1
+1.4
+1.8
+1.6
+1.3
+2.0
+0.7
+0.6"
REFERENCES,0.8472222222222222,"MS (Wang et al., 2019)
67.8
77.8
85.6
87.8
92.7
95.3
76.9
89.8
95.9
90.1
97.6
98.4
+Metrix/input
69.0
79.1
86.0
89.0
93.4
96.0
77.9
90.6
95.9
91.8
98.0
98.9
+1.2
+1.3
+0.4
+1.2
+0.7
+0.7
+1.0
+0.8
+0.0
+1.7
+0.4
+0.5
+Metrix
71.4
80.6
86.8
89.6
94.2
96.0
81.0
92.0
97.2
92.2
98.5
98.6
+3.6
+2.8
+1.2
+1.8
+1.5
+0.7
+4.1
+2.2
+1.3
+2.1
+0.9
+0.2
+Metrix/embed
70.2
80.4
86.7
88.8
92.9
95.6
78.5
91.3
96.7
91.9
98.3
98.7
+2.4
+2.6
+1.1
+1.0
+0.2
+0.3
+1.6
+1.5
+0.8
+1.8
+0.7
+0.3"
REFERENCES,0.85,"PA (Kim et al., 2020c)∗
69.7
80.0
87.0
87.7
92.9
95.8
–
–
–
–
–
–
PA (Kim et al., 2020c)
69.5
79.3
87.0
87.6
92.3
95.5
79.1
90.8
96.2
90.0
97.4
98.2
+Metrix/input
70.5
81.2
87.8
88.2
93.2
96.2
79.8
91.4
96.5
90.9
98.1
98.4
+0.8
+1.2
+0.8
+0.5
+0.3
+0.4
+0.7
+0.6
+0.3
+0.9
+0.7
+0.2
+Metrix
71.0
81.8
88.2
89.1
93.6
96.7
81.3
91.7
96.9
91.9
98.2
98.8
+1.3
+1.8
+1.2
+1.4
+0.7
+0.9
+2.2
+0.9
+0.7
+1.9
+0.8
+0.6
+Metrix/embed
70.4
81.1
87.9
88.9
93.3
96.4
80.6
91.7
96.6
91.6
98.3
98.3
+0.7
+1.1
+0.9
+1.2
+0.4
+0.6
+1.5
+0.9
+0.4
+1.6
+0.9
+0.1"
REFERENCES,0.8527777777777777,"ProxyNCA++ (Teh et al., 2020)∗
69.0
79.8
87.3
86.5
92.5
95.7
80.7
92.0
96.7
90.4
98.1
98.8
ProxyNCA++ (Teh et al., 2020)
69.1
79.5
87.7
86.6
92.1
95.4
80.4
91.7
96.7
90.2
97.6
98.4
+Metrix/input
69.7
79.9
88.3
87.5
92.9
96.0
80.9
92.2
96.9
91.4
98.1
98.8
+0.6
+0.1
+0.6
+0.9
+0.4
+0.3
+0.2
+0.2
+0.2
+1.0
+0.0
+0.0
+Metrix
70.4
80.6
88.7
88.5
93.4
96.5
81.3
92.7
97.1
91.9
98.1
98.8
+1.3
+0.8
+1.0
+1.9
+0.9
+0.8
+0.6
+0.7
+0.4
+1.5
+0.0
+0.0
+Metrix/ embed
70.2
80.2
88.2
88.1
93.0
96.2
81.1
92.4
97.0
91.6
98.1
98.8
+1.1
+0.4
+0.5
+1.5
+0.5
+0.5
+0.4
+0.4
+0.3
+1.2
+0.0
+0.0"
REFERENCES,0.8555555555555555,"Gain over SOTA
+1.7
+1.8
+0.5
+1.8
+1.3
+0.9
+0.6
+0.0
+0.5
+1.2
+0.4
+0.0"
REFERENCES,0.8583333333333333,"Table 5: Improving the SOTA with our Metrix (Metrix/feature) using Resnet-50 with embedding size
d = 512. R@K (%): Recall@K; higher is better. ∗: reported by authors. Bold black: best baseline
(previous SOTA, one per column). Red: Our new SOTA. Gain over SOTA is over best baseline.
MS: Multi-Similarity, PA: Proxy Anchor"
REFERENCES,0.8611111111111112,"and M(a) := S(a) × U −(a), respectively, where U −(a) is replaced by hard negatives only for
input mixup. The two options are combined by choosing uniformly at random in each iteration.
More options are studied in subsection B.4."
REFERENCES,0.8638888888888889,"Hyper-parameters
For any given mixup type or set of mixup pairs, the interpolation factor λ is
drawn from Beta(α, α) with α = 2. We empirically set the mixup strength (10) to w = 0.4 for
positive-negative pairs and anchor-negative pairs."
REFERENCES,0.8666666666666667,"B.3
MORE RESULTS"
REFERENCES,0.8694444444444445,"Computational complexity
On CUB200 dataset, using a batch size of 100 on an NVIDIA RTX
2080 Ti GPU, the average training time in ms/batch is 586 for MS and 817 for MS+Metrix. The 39%
increase in complexity is reasonable for 3.6% increase in R@1. Furthermore, the average training
time in ms/batch is 483 for baseline PA, 965 for PA+Metrix and 1563 for PS (Gu et al., 2021). While
the computation cost of PS is higher than Metrix by 62%, Metrix outperform PS by 0.4% and 1.3%
in terms of R@1 and R@2 respectively (Table 3). At inference, the computational cost is equal for
all methods."
REFERENCES,0.8722222222222222,Published as a conference paper at ICLR 2022
REFERENCES,0.875,"Figure 4: Retrieval results on CUB200 using Contrastive loss, with and without mixup. For each
query, the top-5 retrieved images are shown. Images highlighted in green (red) are correctly (incor-
rectly) retrieved images."
REFERENCES,0.8777777777777778,"Improving the state of the art
Table 5 is an extension of Table 2 that includes all three mixup
types (input, feature, embedding). It shows that not just feature mixup but all mixup types con-
sistently improve the performance of all baseline losses (Cont, MS, PA, ProxyNCA++) across all
datasets. It also shows that across all baseline losses and all datasets, feature mixup works best,
followed by embedding and input mixup. This result conﬁrms the ﬁndings of Table 6 on Cars196."
REFERENCES,0.8805555555555555,"Qualitative results of retrieval
Figure 4 shows qualitative results of retrieval on CUB200 using
Contrastive loss, with and without mixup. This dataset has large intra-class variations such as pose
variation and background clutter. Baseline Contrastive loss may fail to retrieve the correct images
due to these challenges. The ranking is improved in the presence of mixup."
REFERENCES,0.8833333333333333,"Visualization of embedding space
We visualize CUB200 test examples for 10, 15 and 20 classes
in the embedding space using Contrastive loss, with and without mixup in Figure 5. We observe that
in the presence of mixup, the embeddings are more tightly clustered and more uniformly spread,
despite the variations in pose and background in the test set. This ﬁnding validates our quantitative
analysis of alignment and uniformity in subsection 4.3."
REFERENCES,0.8861111111111111,"B.4
ABLATIONS"
REFERENCES,0.8888888888888888,"We perform ablations on Cars196 using R-50 with d = 512, applying mixup on contrastive loss."
REFERENCES,0.8916666666666667,"Hard negatives
We study the effect of the number k of hard negatives using different mixup types.
The set of mixing pairs is chosen from (positive-negative, anchor-negative) uniformly at random per
iteration. We choose k = 3 for input mixup. For feature/embedding mixup, we mix all pairs in a
batch by default, but also study k ∈{20, 40}. As shown in Table 6, k = 3 for input and all pairs for
feature/embedding mixup works best. Still, using few hard negatives for feature/embedding mixup
is on par or outperforms input mixup. All choices signiﬁcantly outperform the baseline."
REFERENCES,0.8944444444444445,Published as a conference paper at ICLR 2022
CLASSES,0.8972222222222223,"10 classes
15 classes
20 classes Cont. Cont."
CLASSES,0.9,"+
Metrix"
CLASSES,0.9027777777777778,"Figure 5: Embedding space visualization of CUB200 test examples of a given number of classes
using Contrastive loss, with and without mixup."
CLASSES,0.9055555555555556,"Mixing pairs
We study the effect of mixing pairs M(a), in particular, U +(a)2 (positive-positive),
U +(a) × U −(a) (positive-negative) and S(a) × U −(a) (anchor-negative), again using different
mixup types. As shown in Table 6, when using a single set of mixing pairs during training, positive-
negative and anchor-negative consistently outperform the baseline, while positive-positive is actually
outperformed by the baseline. This may be due to the lack of negatives in the mixed loss (9), despite
the presence of negatives in the clean loss (3). Hence, we only use positive-negative and anchor-
negative by default, combined by choosing uniformly at random in each iteration."
CLASSES,0.9083333333333333,"Mixup types
We study the effect of mixup type (input, feature, embedding), when used alone.
The set of mixing pairs is chosen from (positive-negative, anchor-negative) uniformly at random per
iteration. As shown in both “hard negatives” and “mixing pairs” parts of Table 6, our default feature
mixup works best, followed by embedding and input mixup."
CLASSES,0.9111111111111111,"Mixup type combinations
We study the effect of using more than one mixup type (input, feature,
embedding), chosen uniformly at random per iteration. The set of mixing pairs is also chosen from
(positive-negative, anchor-negative) uniformly at random per iteration. As shown in Table 6, mixing
inputs, features and embeddings works best. Although this solution outperforms feature mixup alone
by 0.2% Recall@1 (85.1 →85.3), it is computationally expensive because of using input mixup.
The next best efﬁcient choice is mixing features and embeddings, which however is worse than
mixing features alone (84.7 vs. 85.1). This is why we chose feature mixup by default."
CLASSES,0.9138888888888889,"Mixup strength w
We study the effect of the mixup strength w in the combination of the clean
and mixed loss (10) for different mixup types. As shown in Figure 6, mixup consistently improves
the baseline and the effect of w is small, especially for input and embedding mixup. Feature mixup
works best and is slightly more sensitive."
CLASSES,0.9166666666666666,"Ablation on CUB200
We perform additional ablations on CUB200 using R-50 with d = 128 by
applying contrastive loss. All results are shown in Table 7. One may draw the same conclusions as
from Table 6 on Cars196 with d = 512, which conﬁrms that our choice of hard negatives and mixup
pairs is generalizable across different datasets and embedding sizes."
CLASSES,0.9194444444444444,"In particular, following the settings of subsection B.4, we observe in Table 7 that using k = 3 hard
negatives for input mixup and all pairs for feature/embedding mixup achieves the best performance
in terms of Recall@1. Similarly, using a single set of mixing pairs, positive-negative and anchor-
negative consistently outperform the baseline, whereas positive-positive is inferior than the baseline.
Furthermore, combining positive-negative and anchor-negative pairs by choosing uniformly at ran-
dom in each iteration achieves the best overall performance."
CLASSES,0.9222222222222223,Published as a conference paper at ICLR 2022
CLASSES,0.925,"STUDY
HARD NEGATIVES k
MIXING PAIRS
MIXUP TYPE
R@1
R@2
R@4
R@8"
CLASSES,0.9277777777777778,"baseline
81.6
88.2
92.7
95.8"
CLASSES,0.9305555555555556,"1
pos-neg / anc-neg
input
82.0
89.1
93.1
96.1
2
pos-neg / anc-neg
input
82.5
89.2
93.4
96.2
3
pos-neg / anc-neg
input
82.9
89.3
93.7
95.5"
CLASSES,0.9333333333333333,"20
pos-neg / anc-neg
feature
83.5
90.1
94.0
96.5
hard negatives
40
pos-neg / anc-neg
feature
84.0
90.4
94.2
96.8
all
pos-neg / anc-neg
feature
85.1
91.1
94.6
97.0"
CLASSES,0.9361111111111111,"20
pos-neg / anc-neg
embed
82.7
89.2
93.4
96.1
40
pos-neg / anc-neg
embed
83.0
90.0
93.8
96.4
all
pos-neg / anc-neg
embed
83.4
89.9
94.1
96.4"
CLASSES,0.9388888888888889,"–
pos-pos
input
81.0
88.2
92.6
95.6
3
pos-neg
input
82.4
89.1
93.3
95.6
3
anc-neg
input
81.8
89.0
93.6
95.4"
CLASSES,0.9416666666666667,"–
pos-pos
feature
81.1
88.3
92.9
95.8
mixing pairs
all
pos-neg
feature
84.0
90.2
94.2
96.6
all
anc-neg
feature
83.7
90.1
94.4
96.7"
CLASSES,0.9444444444444444,"–
pos-pos
embed
78.3
85.7
90.8
94.4
all
pos-neg
embed
83.1
90.0
93.9
96.6
all
anc-neg
embed
82.7
89.5
93.5
96.3"
CLASSES,0.9472222222222222,"{1, all}
pos-neg / anc-neg
{input, feature}
83.7
94.2
95.9
96.7
mixup type
{3, all}
pos-neg / anc-neg
{input, embed}
83.0
90.9
94.1
96.4
combinations
{all, all}
pos-neg / anc-neg
{feature, embed}
84.7
90.6
94.4
96.9
{1, all, all}
pos-neg / anc-neg
{input, feature, embed}
85.3
94.9
96.2
97.1"
CLASSES,0.95,"Table 6: Ablation study of our Metrix using contrastive loss and R-50 with embedding size d = 512
on Cars196. R@K (%): Recall@K; higher is better."
CLASSES,0.9527777777777777,"0.2
0.4
0.6
0.8
1 80 82 84 86 88 90"
CLASSES,0.9555555555555556,mixup strength w
CLASSES,0.9583333333333334,Recall@1
CLASSES,0.9611111111111111,"baseline
input
embedding
feature"
CLASSES,0.9638888888888889,"Figure 6: Effect of mixup strength for different mixup types using contrastive loss and R-50 with
embedding size d = 512 on Cars196. Recall@K (%): higher is better."
CLASSES,0.9666666666666667,Published as a conference paper at ICLR 2022
CLASSES,0.9694444444444444,"STUDY
HARD NEGATIVES k
MIXING PAIRS
MIXUP TYPE
R@1
R@2
R@4
R@8"
CLASSES,0.9722222222222222,"baseline
61.6
73.7
83.6
90.1"
CLASSES,0.975,"1
pos-neg / anc-neg
input
62.4
73.9
83.0
89.7
2
pos-neg / anc-neg
input
62.7
74.2
83.6
90.0
3
pos-neg / anc-neg
input
63.1
74.5
83.5
90.3"
CLASSES,0.9777777777777777,"20
pos-neg / anc-neg
feature
63.9
75.0
83.9
89.9
hard negatives
40
pos-neg / anc-neg
feature
63.5
75.2
83.5
89.8
all
pos-neg / anc-neg
feature
64.5
75.4
84.3
90.6"
CLASSES,0.9805555555555555,"20
pos-neg / anc-neg
embed
63.1
74.3
83.1
90.0
40
pos-neg / anc-neg
embed
63.5
74.7
83.6
90.1
all
pos-neg / anc-neg
embed
64.0
75.1
84.8
90.9"
CLASSES,0.9833333333333333,"–
pos-pos
input
58.7
70.7
80.1
87.1
3
pos-neg
input
62.9
75.1
83.4
90.6
3
anc-neg
input
62.8
74.7
83.6
90.1"
CLASSES,0.9861111111111112,"–
pos-pos
feature
61.0
73.1
82.5
89.7
mixing pairs
all
pos-neg
feature
63.9
75.0
83.9
89.9
all
anc-neg
feature
63.8
74.8
83.6
90.2"
CLASSES,0.9888888888888889,"–
pos-pos
embed
59.7
72.2
82.7
89.5
all
pos-neg
embed
63.8
75.1
83.3
90.5
all
anc-neg
embed
63.5
75.0
83.9
90.5"
CLASSES,0.9916666666666667,"{1, all}
pos-neg / anc-neg
{input, feature}
63.9
75.1
84.9
90.5
mixup type
{3, all}
pos-neg / anc-neg
{input, embed}
63.4
74.9
84.5
90.1
combinations
{all, all}
pos-neg / anc-neg
{feature, embed}
64.2
75.2
84.1
90.7
{1, all, all}
pos-neg / anc-neg
{input, feature, embed}
65.3
76.2
84.4
91.2"
CLASSES,0.9944444444444445,"Table 7: Ablation study of our Metrix using contrastive loss and R-50 with embedding size d = 128
on CUB200. R@K (%): Recall@K; higher is better."
CLASSES,0.9972222222222222,"We also study the effect of using more than one mixup type (input, feature,embedding), chosen
uniformly at random per iteration. The set of mixing pairs is also chosen from (positive-negative,
anchor-negative) uniformly at random per iteration in this study. From Table 7, we observe that al-
though mixing input, features and embedding works best with an improvement of 0.8% over feature
mixup alone (64.5 →65.3), it is computationally expensive due to using input mixup. The next best
choice is mixing features and embeddings, which is worse than using feature mixup alone (64.2 vs.
64.5). This conﬁrms our choice of using feature mixup as default."
