Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0038022813688212928,"We propose stiffness-aware neural network (SANN), a new method for learning
Hamiltonian dynamical systems from data. SANN identiﬁes and splits the train-
ing data into stiff and nonstiff portions based on a stiffness-aware index, a simple,
yet effective metric we introduce to quantify the stiffness of the dynamical system.
This classiﬁcation along with a resampling technique allows us to apply different
time integration strategies such as step size adaptation to better capture the dy-
namical characteristics of the Hamiltonian vector ﬁelds. We evaluate SANN on
complex physical systems including a three-body problem and billiard model. We
show that SANN is more stable and can better preserve energy when compared
with the state-of-the-art methods, leading to signiﬁcant improvement in accuracy."
INTRODUCTION,0.0076045627376425855,"1
INTRODUCTION"
INTRODUCTION,0.011406844106463879,"Data-driven modeling of dynamical systems provides a computationally inexpensive approach for
exploiting the scientiﬁc law of the physical processes and predicting future phenomena (Giannakis
& Majda, 2014; Harlim et al., 2021; Gu et al., 2021; Mou et al., 2021). With the superior approx-
imation and generalization capacity (Lu et al., 2020; Shen et al., 2021) of neural networks (NNs),
important advances have been made in learning Hamiltonian systems (Greydanus et al., 2019; Finzi
et al., 2020; Tong et al., 2021). Based on the physical property that the total energy of the system
(also called Hamiltonian) must be conserved, many studies model the Hamiltonian system by learn-
ing this conserved quantity from data (Greydanus et al., 2019; DiPietro et al., 2020). Even though
some successes have been achieved, it remains challenging to learn the estimated system to capture
the exact physical law, because of the elusive and chaotic characteristics of the systems (Choudhary
et al., 2020; Marchal, 1990), especially for complex systems that are intrinsically stiff. When learn-
ing stiff dynamics, the NN optimization easily leads to an unstable solution or biased estimation due
to the lack of constraint on parameters (Kim et al., 2021; Wang et al., 2020)."
INTRODUCTION,0.015209125475285171,"To illustrate the difﬁculty in learning stiff Hamiltonian dynamics, let us consider a three-body prob-
lem that describes the interactions of three particles under gravitational force. According to phys-
ical law, the repulsive force of two particles increases dramatically when they get close to each
other. Hence, the close three-body interaction intensiﬁes stiffness phenomena (Huang & Leimkuh-
ler, 1997). Fig. 1 shows the reference orbits of three particles and the orbits learned by the Hamilto-
nian neural network (HNN) (Greydanus et al., 2019), a famous approach that directly approximates
the Hamiltonian with a neural network. In Fig. 1(a, e), the HNN orbits coincide with the reference
when the three particles are far from each other, which is consistent with the results in (Greydanus
et al., 2019). However, the particles deviate from the reference orbits rapidly after close encounter,
and the energy becomes nonconserved (Fig. 1(c, f)). This reveals although the NN is trained with
the data containing close encounter, the NN does not fully capture the dynamics and easily diverges
when stiffness grows. Two primary factors may affect the accuracy of the learned Hamiltonian."
INTRODUCTION,0.019011406844106463,"Implicit bias. NN-based optimization has an implicit bias toward ﬁtting a smooth function with the
fast decay in the frequency domain (Xu et al., 2020; Cao et al., 2019). This implicit bias impedes
the NN from capturing high-frequency components, such as singularities in an N-body problem."
INTRODUCTION,0.022813688212927757,∗Correspondence should be addressed to hongzhang@anl.gov
INTRODUCTION,0.026615969581749048,Published as a conference paper at ICLR 2022
INTRODUCTION,0.030418250950570342,"Figure 1: First four columns show a comparison of the reference orbits and the Hamiltonian neural
network (HNN) orbits of three particles. The orbits of the different particles are displayed with dif-
ferent colors. The rightmost ﬁgures show the energy comparison. The reference orbits are simulated
by the RKF45 solver with the ground truth Hamiltonian, and the energy is conserved. The energy of
the HNN changes dramatically at the close interaction of two particles (see c, f), and eventually the
HNN orbits diverge from the reference."
INTRODUCTION,0.034220532319391636,"Imbalanced stiffness proportion. The stiffness of the dynamical system changes with time and
varies across different trajectories (e.g., the same system with different initial conditions). It is not
uncommon that only a small proportion of trajectories corresponds to stiff dynamics. As a statistical
example, in the 1, 000 independent simulations of three-body trajectories following (Chen et al.,
2019), 91.4% of the trajectories contain close encounter, but on average only 4.2% of the time
intervals within the trajectories contain close encounter."
INTRODUCTION,0.03802281368821293,"To mitigate the implicit bias caused by NN-based optimization and the imbalanced stiffness propor-
tion problem, we propose a new method called the stiffness-aware neural network (SANN) based on
the stiffness classiﬁcation of the training data. In our approach, we introduce a stiffness-aware index
(SAI) as a simple, yet effective metric to classify the time intervals into stiff and nonstiff portions.
For training efﬁciency, we integrate the Hamiltonian dynamics over different intervals with different
step sizes based on their classiﬁcation. To balance the ratio between the stiff group and the nonstiff
group and avoid biased training, we resample the stiff intervals. Our contributions are as follows."
WE IDENTIFY THE IMPORTANCE OF THE STIFFNESS CONCEPT IN LEARNING HAMILTONIAN SYSTEMS FROM THE,0.04182509505703422,"1. We identify the importance of the stiffness concept in learning Hamiltonian systems from the
time series data. We show that SAI is easy to calculate and can be used to effectively determine
stiffness intervals of the data.
2. We validate the SANN method with complex Hamiltonian dynamics including a three-body prob-
lem and billiard model. Extensive numerical results show that SANN can accurately predict the
the stiff dynamics and signiﬁcantly outperform the existing methods."
PRELIMINARIES,0.045627376425855515,"2
PRELIMINARIES"
HAMILTONIAN SYSTEM,0.049429657794676805,"2.1
HAMILTONIAN SYSTEM"
HAMILTONIAN SYSTEM,0.053231939163498096,"The Hamiltonian system describes the continuous-time evolution of states in the phase space (p, q),
where p ∈Rn represents the generalized momentum and q ∈Rn denotes position coordinates.
The Hamiltonian H(p, q) : R2n →R1 is denoted as the total energy of the system at (p, q). The
dynamics can be described with H(p, q) by
dq"
HAMILTONIAN SYSTEM,0.057034220532319393,dt = ∂H
HAMILTONIAN SYSTEM,0.060836501901140684,"∂p ,
dp"
HAMILTONIAN SYSTEM,0.06463878326996197,dt = −∂H
HAMILTONIAN SYSTEM,0.06844106463878327,"∂q .
(1)"
HAMILTONIAN SYSTEM,0.07224334600760456,"With Eqn. (1), H(p, q) is conservative during the evolution as dH"
HAMILTONIAN SYSTEM,0.07604562737642585,dt = ∂H
HAMILTONIAN SYSTEM,0.07984790874524715,∂p · dp
HAMILTONIAN SYSTEM,0.08365019011406843,dt + ∂H
HAMILTONIAN SYSTEM,0.08745247148288973,∂q · dq
HAMILTONIAN SYSTEM,0.09125475285171103,"dt = 0. In
classical mechanics, the Hamiltonian is usually expressed as the sum of the kinetic and potential"
HAMILTONIAN SYSTEM,0.09505703422053231,Published as a conference paper at ICLR 2022
HAMILTONIAN SYSTEM,0.09885931558935361,Figure 2: Workﬂow of SANN.
HAMILTONIAN SYSTEM,0.10266159695817491,"energies. We are interested in learning the separable Hamiltonian, namely, H(p, q) = T(p) +
V (q), where T(p) is kinetic energy while V (q) is potential energy. In this paper our goal is to
learn the Hamiltonian of a dynamical system. What is available is the time series of observations
{(pti, qti)}N
i=1. The Hamiltonian is parameterized by a neural network as follows,"
HAMILTONIAN SYSTEM,0.10646387832699619,"H(p, q) ≈Hθ(p, q) ≜p⊤Mp + φ(q; W ),
(2)"
HAMILTONIAN SYSTEM,0.11026615969581749,"where φ : Rn →R1 is a fully connected NN, M ∈Rn×n is a trainable matrix, and θ ≜{M, W }
is a set of all trainable parameters. Once we obtain Hθ, the trajectories of the learned Hamiltonian
system can be simulated by Eqn. (1) with an ordinary differential equation (ODE) solver."
ODE SOLVER FOR THE HAMILTONIAN SYSTEM,0.11406844106463879,"2.2
ODE SOLVER FOR THE HAMILTONIAN SYSTEM"
ODE SOLVER FOR THE HAMILTONIAN SYSTEM,0.11787072243346007,"We use Forward((p0, q0), H, ∆t) to denote a one-step integration of Eqn. (1) from (p0, q0) over
the step size ∆t. The Euler method is a ﬁrst-order Runge–Kutta method adopted in (Greydanus
et al., 2019). When the Euler method is used, the integration becomes"
ODE SOLVER FOR THE HAMILTONIAN SYSTEM,0.12167300380228137,"(pti+1, qti+1) = Forward((pti, qti), H, ti+1 −ti) ≜(pti, qti) + (−∂H"
ODE SOLVER FOR THE HAMILTONIAN SYSTEM,0.12547528517110265,"∂qti , ∂H"
ODE SOLVER FOR THE HAMILTONIAN SYSTEM,0.12927756653992395,"∂pti )(ti+1 −ti).
(3)"
ODE SOLVER FOR THE HAMILTONIAN SYSTEM,0.13307984790874525,"The Euler method is usually not suitable for stiff dynamics because of its poor stability property.
Another popular numerical method (Fehlberg, 1969) for the Hamiltonian system is Runge—Kutta-
–Fehlberg (RKF45). RKF45 allows the step size to be changed adaptively based on an estimate
of the local truncation error. This error-based step size control has great potential for improving
computational efﬁciency. However, its effectiveness may be hampered by the use of minibatches,
which are crucial for training. Minibatching adds an additional dimension to Eqn. (1). Controlling
the step size requires considering a combined ODE system and estimating the error on all batch
elements, as noted in Chen et al. (2018). Moreover, the step size is limited by the stiffest element in
a batch, making it difﬁcult to use a large step size especially when the batch size is large."
ODE SOLVER FOR THE HAMILTONIAN SYSTEM,0.13688212927756654,"A symplectic integrator, a numerical method that conserves the energy quantity, is widely used
in learning Hamiltonian system (Chen et al., 2019; Zhong et al., 2020). Leapfrog is a second-order
symplectic integrator designed for a separable Hamiltonian. The Leapfrog scheme is in Appendix C."
STIFFNESS-AWARE NEURAL NETWORK,0.14068441064638784,"3
STIFFNESS-AWARE NEURAL NETWORK"
STIFFNESS-AWARE NEURAL NETWORK,0.1444866920152091,"In this section we introduce our method, called SANN (stiffness-aware neural network), with its
workﬂow shown in Fig. 2. First, we propose SAI (stiffness-aware index) to classify the time interval
as either stiff or nonstiff. During training, we integrate the NN-parameterized Hamiltonian dynamics
over the interval with different step sizes based on their classiﬁcation. Next, we resample the stiff
intervals to balance their ratio and avoid biased training."
IDENTIFYING THE STIFF INTERVAL,0.1482889733840304,"3.1
IDENTIFYING THE STIFF INTERVAL"
IDENTIFYING THE STIFF INTERVAL,0.1520912547528517,"In this part we ﬁrst discuss the stiffness index (SI) that is used to characterize stiffness for an ODE
and propose the stiffness-aware index (SAI) to characterize the stiffness for the time series data."
IDENTIFYING THE STIFF INTERVAL,0.155893536121673,"SI reveals the fastest rate of change of state. We consider the dynamics of the state u, du"
IDENTIFYING THE STIFF INTERVAL,0.1596958174904943,"dt = f(u).
(4)"
IDENTIFYING THE STIFF INTERVAL,0.1634980988593156,Published as a conference paper at ICLR 2022
IDENTIFYING THE STIFF INTERVAL,0.16730038022813687,"SI at the state u(t) is deﬁned by max{|Re(λi)|}, where λi is the eigenvalue of the Jacobian matrix
of Eqn. (4) (Aiken, 1985). To illustrate that SI reveals the fastest rate of change of state, we consider
that Eqn. (4) is a linear system with constant coefﬁcients, namely, du"
IDENTIFYING THE STIFF INTERVAL,0.17110266159695817,"dt = Au, and A ∈Rn×n is a
diagonalizable matrix with eigenvalues {λi}n
i=1 and corresponding eigenvectors {vi}n
i=1. Then the
solution of the linear system is u(t) = Pn
i=1 civieλit. Let us suppose that Re(λi) < 0, i = 1, ..., n.
We have eλit →0 as t →∞. Hence, max{|Re(λi)|} reveals the fastest speed of decaying to 0. If
max{|Re(λi)|} is large, the integrator needs a small step size to reduce the local truncation error."
IDENTIFYING THE STIFF INTERVAL,0.17490494296577946,"Deﬁnition of SAI. Given the observations {uti}N
t=1 from the system (4), we deﬁne SAI at uti by"
IDENTIFYING THE STIFF INTERVAL,0.17870722433460076,"1
∥uti∥2"
IDENTIFYING THE STIFF INTERVAL,0.18250950570342206,uti+1 −uti
IDENTIFYING THE STIFF INTERVAL,0.18631178707224336,ti+1 −ti
IDENTIFYING THE STIFF INTERVAL,0.19011406844106463,"2.
(5)"
IDENTIFYING THE STIFF INTERVAL,0.19391634980988592,"Intuitively, SAI characterizes the relative changing speed of the state u from time ti to ti+1. The
norm of ﬁnite difference
 uti+1−uti"
IDENTIFYING THE STIFF INTERVAL,0.19771863117870722,"ti+1−ti

2 approximates
f(uti)∥2, which is used in time parameter-
ization of the adaptive time step method (Huang & Leimkuhler, 1997). In our data-driven setting,
we cannot compute SI as the analytic expression of f is unavailable. But SAI can serve as a proxy
of SI in time series data, which will be demonstrated in Section 5."
IDENTIFYING THE STIFF INTERVAL,0.20152091254752852,"Next, we split the training data into stiff and nonstiff portions by classifying the time interval as
either stiff or nonstiff based on SAI. For a time series observation {uti}N
i=1, we ﬁrst compute SAI
for each time interval. Let SAIi =
1
∥uti∥2
 uti+1−uti"
IDENTIFYING THE STIFF INTERVAL,0.20532319391634982,"ti+1−ti

2. We then rank {SAIi}N−1
i=1 . The interval
(ti, ti+1) with a larger SAIi is supposed to be a stiffer part. We use γ ∈(0, 1) to denote the stiff
ratio, a hyperparameter to determine the ratio of the stiff portion. The intervals belonging to the
top-γ of {SAIi}N−1
i=1 are identiﬁed as stiff intervals, and the others are classiﬁed as nonstiff. In other
words, the intervals in the (1 −γ) × 100-th percentile of SAI are identiﬁed as nonstiff intervals."
TRAINING THE ESTIMATED HAMILTONIAN,0.20912547528517111,"3.2
TRAINING THE ESTIMATED HAMILTONIAN"
TRAINING THE ESTIMATED HAMILTONIAN,0.21292775665399238,"Loss function. Let {(pti, qti)}N
i=1 be the given trajectory from Hamiltonian system. Given a pair of
consecutive states, (pti, qti) and (pti+1, qti+1), we integrate Eqn. (1) from ti to obtain the estimated
solution (ˆpti+1, ˆqti+1) at ti+1. To achieve a more accurate solution, we conduct the integration over
[ti, ti+1] with a small time step ti+1−ti"
TRAINING THE ESTIMATED HAMILTONIAN,0.21673003802281368,"S
, where S is an integer hyperparameter called partition.
Speciﬁcally, we proceed as follows:"
TRAINING THE ESTIMATED HAMILTONIAN,0.22053231939163498,"(ˆpti+s/S, ˆqti+s/S) = Forward((ˆpti+(s−1)/S, ˆqti+(s−1)/S), Hθ, ti+1 −ti"
TRAINING THE ESTIMATED HAMILTONIAN,0.22433460076045628,"S
), s = 1, · · · , S,
(6)"
TRAINING THE ESTIMATED HAMILTONIAN,0.22813688212927757,"where (ˆpti, ˆqti) ≜(pti, qti). For the stiff equation, the step size of the numerical integration must
be small enough for the stable solution. Therefore, the large partition S should be chosen for the
stiff portion while the small partition S is used for the nonstiff portion. The parameter θ in Hθ can
be optimized by minimizing the mean squared error between the prediction and the ground truth:"
TRAINING THE ESTIMATED HAMILTONIAN,0.23193916349809887,"1
N −1 N−1
X"
TRAINING THE ESTIMATED HAMILTONIAN,0.23574144486692014,"i=1
∥pti+1 −ˆpti+1∥2
2 + ∥qti+1 −ˆqti+1∥2
2,
(7)"
TRAINING THE ESTIMATED HAMILTONIAN,0.23954372623574144,"with stochastic optimization algorithms such as Adam (Kingma & Ba, 2014)."
TRAINING THE ESTIMATED HAMILTONIAN,0.24334600760456274,"Resampling. As demonstrated in Section 1, the stiff phenomena may comprise only a small pro-
portion in the trajectory of the Hamiltonian system. We usually pick the stiff ratio γ with a value
less than 0.5. To avoid biased regression and unstable training caused by imbalanced categories of
intervals, we adopt a random resampling technique that balances the data by replicating the stiff in-
tervals K times. A typical choice for replication K is 1−γ"
TRAINING THE ESTIMATED HAMILTONIAN,0.24714828897338403,"γ
such that the number of the stiff intervals
is comparable to that of the nonstiff intervals."
EXPERIMENTS,0.2509505703422053,"4
EXPERIMENTS"
EXPERIMENTS,0.25475285171102663,"To evaluate the performance of SANN, we use two complex Hamiltonian systems: the billiard
model and the three-body problem. We compare SANN with two famous approaches, including"
EXPERIMENTS,0.2585551330798479,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2623574144486692,"Figure 3: (a) Proﬁle of the potential function for the billiard model; q = (qx, qy) ∈R2 denotes the
position. Comparison of (b) relative energy error and (c) MSE for billiard model."
EXPERIMENTS,0.2661596958174905,"Reference
SANN (Ours)
SRNN
HNN"
EXPERIMENTS,0.26996197718631176,"Reference
SANN (Ours)
SRNN
HNN"
EXPERIMENTS,0.2737642585551331,"Reference
SANN (Ours)
SRNN
HNN"
EXPERIMENTS,0.27756653992395436,"Figure 4: Comparison of billiard orbits simulated using the Hamiltonian learned by different meth-
ods. We can see that SANN produces orbits that are almost identical to those of the reference. We
provide more results in Appendix E and dynamic graphs on website."
EXPERIMENTS,0.2813688212927757,"HNN (Greydanus et al., 2019) and SRNN (Chen et al., 2019). All approaches aim to learn Hθ(p, q)
in Eqn. (1) as a representation of the dynamics. HNN trains Hθ(p, q) by minimizing the error
between the partial derivatives of Hθ(p, q) and time derivatives approximated by data. SRNN
utilizes symplectic solvers to conduct multistep integration and trains Hθ(p, q) by minimizing the
error between the prediction and the data. SANN uses a similar loss function but with stiffness
awareness as shown in Section 3.2. For a fair comparison, we use the Leapfrog solver for both
SRNN and SANN. To evaluate the accuracy of the learned Hθ(p, q) for different methods, we
simulate the dynamics using the Leapfrog solver with the same settings during testing. We provide a
guidance for hyperparameter selection in Appendix G and additional experiments on the Pendulum-
N problem which involves inseparable Hamiltonian in Appendix H."
BILLIARD MODEL,0.28517110266159695,"4.1
BILLIARD MODEL"
BILLIARD MODEL,0.2889733840304182,"The billiard model has wide application in real-world physical systems, spanning quantum-classical
correspondence (St¨ockmann & Stein, 1990), lasers (Stone, 2010), quantum dots (Ponomarenko
et al., 2008), and nanodevices (Chen et al., 2016). The billiard model that we consider describes
a billiard bouncing between a ring with soft boundaries (Choudhary et al., 2020). The Hamiltonian
is deﬁned as"
BILLIARD MODEL,0.29277566539923955,"H(p, q) = 1"
BILLIARD MODEL,0.2965779467680608,"2∥p∥2
2 +

1 + exp
 rα −∥q∥2"
BILLIARD MODEL,0.30038022813688214,"s
−1
−

1 + exp
 rβ −∥q −(q0, 0)∥2"
BILLIARD MODEL,0.3041825095057034,"s
−1
,
(8)"
BILLIARD MODEL,0.30798479087452474,Published as a conference paper at ICLR 2022
BILLIARD MODEL,0.311787072243346,"where p ∈R2 and q ∈R2 are the momentum and the position of a billiard on a 2D plane, respec-
tively; rα, rβ are the radius of the outer circle and the inner circle, respectively; s is the softness
of the boundary; and (q0, 0) is a shift of the inner circle from the center. As shown in Fig. 3(a),
the potential function becomes sharp near the boundaries of the ring, causing a rapid change of the
momenta of the billiard ball when it gets close to the boundaries."
BILLIARD MODEL,0.3155893536121673,"Experimental setup. The parameters rα, rβ, s, and q0 are set to be 1.0, 0.5, 0.05, and 0.1, respec-
tively. We use the physics-informed model Hθ(p, q) deﬁned in Eqn. (2); φ is a 3-hidden-layer fully
connected NN with width 256 per layer and a rational activation function (Boull´e et al., 2020). We
simulate the training set with 100 trajectories and the testing set with 30 trajectories by RKF45.
Each trajectory contains 1,000 time steps with step size of 0.1. We set the stiff ratio γ to be 10% and
the partition S to be 10 for the stiff interval and 2 for the nonstiff interval. The loss function (7) is
optimized by Adam with a batch size of 1,024, and we use an initial learning rate of 0.001 for 500
epochs. The learning rate follows cosine decay with the increasing training epoch. During testing,
we integrate ODE (1) using the Leapfrog integrator with a ﬁxed step size of 0.005."
BILLIARD MODEL,0.3193916349809886,"Results. SANN can predict the dynamics more accurately than HNN and SRNN can. Fig. 3(b,
c) shows the mean square error (MSE) with time and relative energy error, which is deﬁned as the
ratio of difference between the current energy and the initial energy to the initial energy. We can
see that SANN maintains a much smaller energy error compared with HNN or SRNN. On average,
SANN reduces the relative energy error by 56.07% compared with SRNN and 92.13% compared
with HNN. The gap in terms of MSE enlarges as the number of time steps increases. At the last
time step, the MSE of SANN is 56.91% lower than that of SRNN and 83.44% lower than that of
HNN. Fig. 4 gives a comparison of the predicted trajectories. SANN recovers visually the same
trajectories as the reference, while the trajectories obtained with SRNN and HNN clearly diverge
from the reference. Additional results for noisy data are given in Appendix F. A discussion on
implicit bias is given in Appendix A."
BILLIARD MODEL,0.3231939163498099,"Figure 5: The ﬁrst two ﬁgures show the comparison of relative energy error and MSE with varied
time for the three-body problem. The last two ﬁgures show the comparison of performance with and
without resampling."
THREE-BODY PROBLEM,0.3269961977186312,"4.2
THREE-BODY PROBLEM"
THREE-BODY PROBLEM,0.33079847908745247,"We consider a three-body problem that describes the motion of three particles under Newtonian
gravitational force. The Hamiltonian of the three-body system is given by"
THREE-BODY PROBLEM,0.33460076045627374,"H(p, q) = 1 2"
X,0.33840304182509506,"3
X i=1"
X,0.34220532319391633,"∥pi∥2
2
mi
+
X"
X,0.34600760456273766,"1≤i<j≤3
−Gmimj"
X,0.34980988593155893,"∥qi −qj∥2
,
(9)"
X,0.35361216730038025,"where pi ∈R2, qi ∈R2 and mi are the momentum, position, and mass of the ith particle, i =
1, 2, 3, p ≜(p1, p2, p3) and q ≜(q1, q2, q3), and G is the gravitational constant. One can see that
the potential function incurs singularities when the distance between any two particles is small."
X,0.3574144486692015,"Experiment setup. We set m1 = m2 = m3 = 1 and G = 1 in our experiments. To approximate
the Hamiltonian of three-body dynamics, we use a physics-informed Hθ as follows,"
X,0.3612167300380228,"Hθ(p, q) ≜p⊤Mp + φ(∥q1 −q2∥2, ∥q1 −q3∥2, ∥q2 −q3∥2; W ),
(10)"
X,0.3650190114068441,"where φ is deﬁned as in Section 4.1, and we incorporate the physics information where the potential
is a function of pairwise distances. The training data comprises 1,000 trajectories, and the length"
X,0.3688212927756654,Published as a conference paper at ICLR 2022
X,0.3726235741444867,"of each trajectory N is 60 with the step size 0.1. The random initialization of initial states follows
(Chen et al., 2019). The testing data set consists of 300 trajectories with length N of 100 and step
size of 0.1. We set the stiff ratio γ to be 10% and the partition S to be 50 for the stiff interval and 10
for the nonstiff interval. When training, we minimize the loss function (7) by Adam with the batch
size of 1,024. Hθ is trained with an initial learning rate of 0.001; the learning rate decays for 3,000
epochs. During evaluation, the Leapfrog integrator with ﬁxed step size of 0.01 is used."
X,0.376425855513308,"Results. SANN learns a more accurate Hamiltonian than HNN and SRNN do on the three-body
problem. Fig. 6 shows the orbits of three particles simulated by different methods and a system
energy comparison. The relative energy error and MSE of orbits simulated by different methods are
presented in Fig. 5. In Fig. 6, in the beginning the orbits of all methods coincide with the reference.
However, we see that the orbits of HNN and SRNN rapidly drift away from the reference, and the
energy changes dramatically whenever two orbits meet. On the contrary, SANN produces orbits that
are nearly identical to the orbits of the reference, and the energy remains roughly constant. From
Fig. 5, we see that SANN achieves lower energy error and MSE compared with HNN and SRNN.
On average, SANN reduces the relative energy error by 67.80% compared with SRNN and 96.30%
compared with HNN. Also, at the last time step SANN’s MSE is 63.84% lower than SRNN’s and
92.65% lower than HNN’s."
X,0.38022813688212925,"Figure 6: The ﬁrst three columns show the comparison of the reference orbits (solid curves) and the
orbits learned by different method (dotted curves). • is the initial position, and ▶is the direction.
The orbit of the different particles is presented by different colors. The rightmost ﬁgures show the
energy comparison. We provide more results on Appendix E and dynamic graphs in website."
STIFFNESS METRIC,0.3840304182509506,"5
STIFFNESS METRIC"
STIFFNESS METRIC,0.38783269961977185,"In Section 3.1 SAI was introduced in order to characterize the stiffness of a trajectory. In this section
we show theoretically and numerically that SAI can serve as a proxy of SI in a data-driven scenario."
STIFFNESS METRIC,0.3916349809885932,"SAI keeps the same trend as SI. With the analytic expression of dynamics in three-body problem
and a given trajectory, we compute the SI and SAI for each time step of the trajectory, respectively.
As a representative example, Fig. 7 shows the orbits of three particles and the corresponding curves
of SAI and SI with time. We can see that SI remains roughly a small constant when three particles
are far away from each other but rises sharply when two particles get close. In three-body dynam-"
STIFFNESS METRIC,0.39543726235741444,Published as a conference paper at ICLR 2022
STIFFNESS METRIC,0.39923954372623577,"Figure 7: Comparison of SAI and SI with time. The color markers refer to the occurrence of the
stiffness phenomena; the same color corresponds to the same time period. The trends of SAI and SI
are consistent. We provide more results on Appendix E and dynamic graphs in website."
STIFFNESS METRIC,0.40304182509505704,"ics, the stiff phenomena are known to occur at the close interaction of particles, which is in good
correspondence to large SI values. More important, SAI keeps the same trend with SI as time goes
by, and SAI reaches the peak at the same time interval as SI does. Through extensive numerical
validation like this, we ﬁnd that SAI can characterize the stiffness of a trajectory very well."
STIFFNESS METRIC,0.4068441064638783,"SAI is a generalized SI. For a theoretical analysis, we follow the standard technique that linearizes
the ODE locally (Arrowsmith & Place, 1992). In our method, SAI is computed by adjacent states
with small step size, and the adjacent states satisfy the linearized ODE. By deﬁnition, SI depends on
the eigenvalue λm of the linear ODE with the maximum absolute real part. We show in Theorem 1
(with proof in Appendix D) that SAI depends on all the eigenvalues. In an ideal setting where
ℓm = 1, ℓi = 0, ∀i ̸= m, SAI is roughly equivalent to SI.
Theorem 1. Let A be a n × n symmetric real matrix and {λi}n
i=1 be its n distinct eigenvalues.
Then for the linear system dx"
STIFFNESS METRIC,0.41064638783269963,"dt = Ax with nonzero initial condition x(0) = u0, we have"
STIFFNESS METRIC,0.4144486692015209,"1
∥u0∥2"
STIFFNESS METRIC,0.41825095057034223,u1 −u0 ∆t
STIFFNESS METRIC,0.4220532319391635,"2 =
 
n
X"
STIFFNESS METRIC,0.42585551330798477,"i=1
ℓiλ2
i
1/2 + O(∆t)
(11)"
STIFFNESS METRIC,0.4296577946768061,"where u1 = x(∆t) and ℓi ≥0, i = 1, ..., n with Pn
i=1 ℓi = 1."
STIFFNESS METRIC,0.43346007604562736,"One limitation of our stiffness metric is that the value of SAI may change with the coordinate system.
For example, a coordinate translation does not change the norm of the ﬁnite difference term in
Eqn. (5), but may change the norm of the state. However, the stiffness classiﬁcation in our method is
not sensitive to coordinate translation because the classiﬁcation relies only on the ranking (not their
values) of the SAIs for the time steps in a trajectory. See Appendix B for a detailed analysis."
ABLATION STUDY,0.4372623574144487,"6
ABLATION STUDY"
ABLATION STUDY,0.44106463878326996,"In this section we explore the effect of varied resampling replication K introduced in Section 3.2 and
activation functions used in Hθ(p, q). The experiments are conducted on the three-body problem.
Table 1 shows the training MSE, testing MSE, relative energy error, and training time (in seconds)
for each epoch. Fig. 5 shows the performance with and without resampling. Fig. 8 displays the
performance of different activation functions used in the NN."
ABLATION STUDY,0.4448669201520912,"Resampling. From Exp. 2–Exp. 6, we see that with the increasing resampling replication for stiff
portion, the learned Hamiltonian dynamics can achieve smaller errors, but the training time grows
accordingly. For example, when the replication K changes from 6 to 8, the testing MSE decreases
by 47.61% but the training time increases by 22.72%."
ABLATION STUDY,0.44866920152091255,"Efﬁciency. In Exp. 7 where all training intervals are integrated with partition 50, we achieve a
more accurate solution compared with Exp. 2 where some portions of intervals are integrated with
partition 10. However, the training time increases signiﬁcantly. On the other hand, despite the time
increment caused by resampling, Exp. 4 achieves comparable performance to Exp. 7 but reduces
over 40% of training time. To trade off between the accuracy and time cost, we can choose an
appropriate resampling replication."
ABLATION STUDY,0.4524714828897338,"Activation functions. We compare the performance of ReLU, Tanh, and Rational (Boull´e et al.,
2020) used in the NN to approximate the Hamiltonian. Even though theoretically ReLU NN has a"
ABLATION STUDY,0.45627376425855515,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.4600760456273764,"good approximation property, it has worse performance in our problem where the dynamics involve
a ﬁrst-order derivative. Rational NN is slightly better than Tanh NN. According to Occam’s Ra-
zor (Lattimore & Hutter, 2011), we intuitively prefer to formulate the physical system with a simple
expression and avoid a complicated one such as Tanh NN, a highly nonlinear function. Rational NN,
which has the form of a rational fraction, is more suitable for modeling the Hamiltonian."
ABLATION STUDY,0.46387832699619774,"Exp. ID
Stiff Ratio γ
Partition (nonstiff)
Partition (stiff)
Replication
Train MSE
Test MSE
Energy Error
Training Time"
ABLATION STUDY,0.467680608365019,"Exp. 1
0.1
10
10
9
2.0e-6
0.0513
0.0611
13.52
Exp. 2
0.1
10
50
1
2.0e-7
0.2173
0.7884
10.08
Exp. 3
0.1
10
50
4
1.5e-7
0.0460
0.1026
19.82
Exp. 4
0.1
10
50
6
7.4e-8
0.0294
0.0581
26.36
Exp. 5
0.1
10
50
8
5.5e-8
0.0154
0.0228
32.35
Exp. 6
0.1
10
50
9
4.3e-8
0.0234
0.0457
35.35
Exp. 7
0.5
50
50
1
6.2e-7
0.0554
0.0900
34.75"
ABLATION STUDY,0.4714828897338403,Table 1: Performance comparison under different parameter settings. “Exp.” is short for experiment.
ABLATION STUDY,0.4752851711026616,"Figure 8: Performance comparison of different activation functions on the three-body problem and
billiard model."
RELATED WORK,0.4790874524714829,"7
RELATED WORK"
RELATED WORK,0.4828897338403042,"Learning Hamiltonian system from data. Enforcing the conservative law into the system structure
becomes a powerful and popular tool to learn the Hamiltonian system from data (Willard et al.,
2020; Cheriﬁ, 2020; Zhong et al., 2021). Greydanus et al. (2019); Choudhary et al. (2020) use an
NN to approximate the H(p, q) instead of learning the dynamics directly. This idea also applies
to learning the conserved quantities from images (Toth et al., 2020). To improve the accuracy of
integration, Chen et al. (2019) conduct multi-step integration with a symplectic solver. Finzi et al.
(2020) simplify the learning process by coordinate transformation and enforcing the constraints of
new coordinates. To learn the Hamiltonian dynamics directly, Tong et al. (2021); Jin et al. (2020)
design NNs with a symplectic structure to characterize the system, while Chen & Tao (2021) learn
a symplectic map of the Hamiltonian dynamics. Zhong et al. (2020) incorporate the physical prior
into the dynamics parameterization."
RELATED WORK,0.4866920152091255,"Learning stiff dynamics. It has been shown that stiffness may lead to failures in data-driven mod-
elling (Wang et al., 2020; Kim et al., 2021; Parmar et al., 2021). Kim et al. (2021) propose a scaling
strategy that mitigates the stiffness of the dynamics to stabilize the gradient calculation. This ap-
proach, however, is applicable only to problems where stiffness is caused by widely separated time
scales and endures during the whole trajectory."
CONCLUSION,0.49049429657794674,"8
CONCLUSION"
CONCLUSION,0.49429657794676807,"We propose SANN (stiffness-aware neural network) to learn the stiff Hamiltonian system. We also
propose a new metric SAI (stiffness-aware index) to classify the training data into stiff and nonstiff
portions. This classiﬁcation along with a resampling technique allows us to apply step size adapta-
tion strategies to better capture the dynamical characteristics of the Hamiltonian vector ﬁelds. On
complex physical systems including the three-body problem and the billiard model, our method out-
performs the state-of-art approaches with a signiﬁcant margin. Our method has potential to extend
to other types of stiff dynamical systems, not limited to learning the Hamiltonian; such an extension
is left as future work."
CONCLUSION,0.49809885931558934,Published as a conference paper at ICLR 2022
CONCLUSION,0.5019011406844106,ACKNOWLEDGEMENT
CONCLUSION,0.5057034220532319,"This material is based upon work supported by the U.S. Department of Energy, Ofﬁce of Science,
Ofﬁce of Advanced Scientiﬁc Computing Research, Scientiﬁc Discovery through Advanced Com-
puting (SciDAC) program through the FASTMath Institute under contract DE-AC02-06CH11357 at
Argonne National Laboratory. S. L. acknowledges the support of the Ross-Lynn fellowship from
Purdue University."
REFERENCES,0.5095057034220533,REFERENCES
REFERENCES,0.5133079847908745,"Richard C Aiken. Stiff computation, volume 169. Oxford University Press Oxford, 1985."
REFERENCES,0.5171102661596958,"David Arrowsmith and Colin M Place. Dynamical systems: differential equations, maps, and chaotic
behaviour, volume 5. CRC Press, 1992."
REFERENCES,0.5209125475285171,"Nicolas Boull´e, Yuji Nakatsukasa, and Alex Townsend. Rational neural networks. arXiv preprint
arXiv:2004.01902, 2020."
REFERENCES,0.5247148288973384,"Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019."
REFERENCES,0.5285171102661597,"Renyi Chen and Molei Tao. Data-driven prediction of general hamiltonian dynamics via learning
exactly-symplectic maps. In International Conference on Machine Learning, pp. 1717–1727.
PMLR, 2021."
REFERENCES,0.532319391634981,"Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural Ordinary Dif-
ferential Equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
69386f6bb1dfed68692a24c8686939b9-Paper.pdf."
REFERENCES,0.5361216730038023,"Shaowen Chen, Zheng Han, Mirza M. Elahi, K. M. Masum Habib, Lei Wang, Bo Wen, Yuanda Gao,
Takashi Taniguchi, Kenji Watanabe, James Hone, Avik W. Ghosh, and Cory R. Dean. Electron
optics with p-n junctions in ballistic graphene. Science, 353(6307):1522–1525, 2016. doi: 10.
1126/science.aaf5481."
REFERENCES,0.5399239543726235,"Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and L´eon Bottou. Symplectic recurrent neural
networks. arXiv preprint arXiv:1909.13334, 2019."
REFERENCES,0.5437262357414449,"Karim Cheriﬁ. An overview on recent machine learning techniques for port hamiltonian systems.
Physica D: Nonlinear Phenomena, 411:132620, 2020."
REFERENCES,0.5475285171102662,"Anshul Choudhary, John F Lindner, Elliott G Holliday, Scott T Miller, Sudeshna Sinha, and
William L Ditto. Physics-enhanced neural networks learn order and chaos. Physical Review
E, 101(6):062207, 2020."
REFERENCES,0.5513307984790875,"Daniel DiPietro, Shiying Xiong, and Bo Zhu. Sparse Symplectically Integrated Neural Networks.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 6074–6085. Curran Associates, Inc., 2020."
REFERENCES,0.5551330798479087,"Erwin Fehlberg. Low-order classical Runge-Kutta formulas with stepsize control and their applica-
tion to some heat transfer problems, volume 315. National aeronautics and space administration,
1969."
REFERENCES,0.55893536121673,"Marc Finzi, Ke Alexander Wang, and Andrew G Wilson. Simplifying Hamiltonian and Lagrangian
Neural Networks via Explicit Constraints. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 13880–
13889. Curran Associates, Inc., 2020."
REFERENCES,0.5627376425855514,"Dimitrios Giannakis and Andrew J Majda. Data-driven methods for dynamical systems: Quantifying
predictability and extracting spatiotemporal patterns. Wiley & Sons, 2014."
REFERENCES,0.5665399239543726,Published as a conference paper at ICLR 2022
REFERENCES,0.5703422053231939,"Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances
in Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.5741444866920152,"Yiqi Gu, John Harlim, Senwei Liang, and Haizhao Yang. Stationary Density Estimation of It\ˆ o
Diffusions Using Deep Learning. arXiv preprint arXiv:2109.03992, 2021."
REFERENCES,0.5779467680608364,"John Harlim, Shixiao W Jiang, Senwei Liang, and Haizhao Yang. Machine learning for prediction
with missing dynamics. Journal of Computational Physics, 428:109922, 2021."
REFERENCES,0.5817490494296578,"Weizhang Huang and Benedict Leimkuhler. The adaptive Verlet method. SIAM Journal on Scientiﬁc
Computing, 18(1):239–256, 1997."
REFERENCES,0.5855513307984791,"Pengzhan Jin, Zhen Zhang, Aiqing Zhu, Yifa Tang, and George Em Karniadakis. SympNets: In-
trinsic structure-preserving symplectic networks for identifying Hamiltonian systems. Neural
Networks, 132:166–179, 2020."
REFERENCES,0.5893536121673004,"Suyong Kim, Weiqi Ji, Sili Deng, and Christopher Rackauckas. Stiff neural ordinary differential
equations. arXiv preprint arXiv:2103.15341, 2021."
REFERENCES,0.5931558935361216,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.596958174904943,"Tor Lattimore and Marcus Hutter. No Free Lunch versus Occam’s Razor in Supervised Learning. In
Algorithmic Probability and Friends, 2011."
REFERENCES,0.6007604562737643,"J. Lu, Z. Shen, H. Yang, and S. Zhang. Deep Network Approximation for Smooth Functions. arXiv
e-prints, arXiv:2001.03040, 2020."
REFERENCES,0.6045627376425855,"C. Marchal. The Three-body Problem. Advances in Industrial Engineering. Elsevier, 1990. ISBN
9780444874405."
REFERENCES,0.6083650190114068,"Changhong Mou, Birgul Koc, Omer San, Leo G Rebholz, and Traian Iliescu. Data-driven variational
multiscale reduced order models. Computer Methods in Applied Mechanics and Engineering,
373:113470, 2021."
REFERENCES,0.6121673003802282,"Mihir Parmar, Mathew Halm, and Michael Posa. Fundamental challenges in deep learning for stiff
contact dynamics. arXiv preprint arXiv:2103.15406, 2021."
REFERENCES,0.6159695817490495,"L. A. Ponomarenko, F. Schedin, M. I. Katsnelson, R. Yang, E. W. Hill, K. S. Novoselov, and A. K.
Geim. Chaotic Dirac Billiard in Graphene Quantum Dots. Science, 320(5874):356–358, 2008.
doi: 10.1126/science.1154663."
REFERENCES,0.6197718631178707,"Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural net-
works for learned functions of different frequencies. Advances in Neural Information Processing
Systems, 32:4761–4771, 2019."
REFERENCES,0.623574144486692,"Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep Network with Approximation Error Being
Reciprocal of Width to Power of Square Root of Depth. Neural Computation, 2021."
REFERENCES,0.6273764258555133,"H-J St¨ockmann and J Stein. Quantum chaos in billiards studied by microwave absorption. Physical
review letters, 64(19):2215, 1990."
REFERENCES,0.6311787072243346,"A Douglas Stone. Chaotic billiard lasers. Nature, 465(7299):696–697, 2010."
REFERENCES,0.6349809885931559,"Yunjin Tong, Shiying Xiong, Xingzhe He, Guanghan Pan, and Bo Zhu. Symplectic neural networks
in Taylor series form for Hamiltonian systems. Journal of Computational Physics, 437:110325,
2021. ISSN 0021-9991."
REFERENCES,0.6387832699619772,"Peter Toth, Danilo J. Rezende, Andrew Jaegle, S´ebastien Racani`ere, Aleksandar Botev, and Irina
Higgins. Hamiltonian generative networks. In International Conference on Learning Represen-
tations, 2020. URL https://openreview.net/forum?id=HJenn6VFvB."
REFERENCES,0.6425855513307985,"Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies
in physics-informed neural networks. arXiv preprint arXiv:2001.04536, 2020."
REFERENCES,0.6463878326996197,Published as a conference paper at ICLR 2022
REFERENCES,0.6501901140684411,"Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar.
Integrating
physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919,
1(1):1–34, 2020."
REFERENCES,0.6539923954372624,"Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency Principle:
Fourier Analysis Sheds Light on Deep Neural Networks. Communications in Computational
Physics, 28(5):1746–1767, 2020. ISSN 1991-7120."
REFERENCES,0.6577946768060836,"Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ODE-Net: Learning
Hamiltonian Dynamics with Control. In International Conference on Learning Representations,
2020."
REFERENCES,0.6615969581749049,"Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Benchmarking energy-conserving
neural networks for learning dynamics from data. In Learning for Dynamics and Control, pp.
1218–1229. PMLR, 2021."
REFERENCES,0.6653992395437263,Published as a conference paper at ICLR 2022
REFERENCES,0.6692015209125475,"A
MITIGATING IMPLICIT BIAS"
REFERENCES,0.6730038022813688,"In this section, we empirically demonstrate that our SANN can mitigate the implicit bias of NN-
based optimization. First, the potential functions learned by different methods are compared, and
we can see that SANN can learn the potential function more accurately than other methods. Then,
we show the performance for excessive number of epochs to demonstrate that the baseline methods
cannot capture the dynamics well even when a large number of training epochs is used."
REFERENCES,0.6768060836501901,"A.1
LEARNED POTENTIAL FUNCTION"
REFERENCES,0.6806083650190115,"The stiffness of the billiard model is mainly caused by the sharp boundary of the potential function.
Hence, the learned potential function can reﬂect the accuracy of the simulated dynamics. Fig. 9
shows the top view and side view of the potential function of the billiard model from different
methods. The reference potential function becomes sharp near the boundaries of the ring. From
the top view, compared with SRNN and HNN, SANN learns a potential function that is close to the
reference. From the side view, the potential functions of SRNN and HNN tend to learn a smooth
inner circle boundary, which indicates these methods suffer from the implicit bias mentioned in
Section 1. In contrast, the edge and corner of the SANN potential function appear much sharper."
REFERENCES,0.6844106463878327,"A.2
INFLUENCE OF NUMBER OF EPOCHS"
REFERENCES,0.688212927756654,"Ronen et al. (2019) points out that it requires O(k2) time to learn a function of frequency k for
NN-based optimization. However, we observe that the extending the training time cannot mitigate
the implicit bias for SRNN or HNN when learning the stiff Hamiltonian systems."
REFERENCES,0.6920152091254753,"We increase the number of epochs for training SRNN and HNN from 3, 000 to 6, 000, 10, 000, and
15, 000. Fig. 10 shows the training error versus different numbers of epochs, as well as MSE and
the energy error of the simulated trajectories. We can see that the training loss of SRNN converges
to the same value even when the number of epochs becomes extremely large, and the MSE and the
energy error do not improve much as the number of epochs grows. Similar behaviours are observed
for HNN."
REFERENCES,0.6958174904942965,Figure 9: The learned potential function of the billiard model.
REFERENCES,0.6996197718631179,Published as a conference paper at ICLR 2022
REFERENCES,0.7034220532319392,"Figure 10: Comparison of different methods in training loss, MSE, relative energy error for different
numbers of training epochs."
REFERENCES,0.7072243346007605,Published as a conference paper at ICLR 2022
REFERENCES,0.7110266159695817,"B
GENERALITY OF SAI-BASED STIFFNESS CLASSIFICATION"
REFERENCES,0.714828897338403,"In this section, we empirically demonstrate the SAI-based stiffness classiﬁcation is not sensitive to
the translation of the coordinate. The purpose is to show that this classiﬁcation strategy can identify
the stiff part of the trajectory under different kinds of coordinate translation."
REFERENCES,0.7186311787072244,"Speciﬁcally, we translate the position coordinates of the three-body trajectories with two kinds of
translations, ﬁxed direction and random direction. For ﬁxed direction, the position coordinates are
added with a ﬁxed vector v = (0, · · · , 0, v, · · · , v), e.g., (pti, qti) + v. For a random direction,
the position coordinates are added to with a random vector v = (0, · · · , 0, v1, · · · , vn) and {vi} are
i.i.d random variables uniformly distributed on [0, 1]."
REFERENCES,0.7224334600760456,"Fig. 11(a) and 11(b) highlight the stiff intervals (marked in yellow) that our method identiﬁes using
the SAIs calculated in different coordinate systems. Three examples (one in each column) are chosen
for illustration. Fig. 11(a) shows the classiﬁcation results (γ = 0.1) for the ﬁxed translation using 7
widely different values of v (v = 0.1, 10, · · · , 106). Fig. 11(b) shows the classiﬁcation results for 7
occurrences of the random vector v uniformly sampled from [0, v] (v = 0.1, 10, · · · , 106). Fig. 12
illustrates the same analysis for three additional examples."
REFERENCES,0.7262357414448669,"We can see that (1) the SAI-based classiﬁcation captures the stiff parts of the trajectory (those with
large SI values) successfully for all scenarios, and (2) coordinate translation has almost no im-
pact on the classiﬁcation results. This is expected by construction. The second term of Eqn. (5)
 uti+1−uti"
REFERENCES,0.7300380228136882,"ti+1−ti

2 is a translation-invariant quantity. Coordinate translation changes only the norm of
the states, so it causes the SAIs to be scaled for all the time intervals. Therefore, when sorting the
intervals based on SAI, we can maintain a similar order for these intervals and identify the stiff parts
accurately."
REFERENCES,0.7338403041825095,"Figure 11: (a) SAI-based classiﬁcation for a trajectory translated by a ﬁxed vector for 7 different
values of v (v = 0.1, 10, · · · , 106). Stiff intervals are marked in yellow. (b) SAI-based classiﬁca-
tion for a trajectory translated by a uniform random vector v on [0, v] (v = 0.1, 10, · · · , 106). (c)
Comparison of SI, SAI of the original trajectory, and SAI of the translated trajectory."
REFERENCES,0.7376425855513308,Published as a conference paper at ICLR 2022
REFERENCES,0.7414448669201521,"Figure 12: (a) SAI-based classiﬁcation for a trajectory translated by a ﬁxed vector for 7 different
values of v (v = 0.1, 10, · · · , 106). Stiff intervals are marked in yellow. (b) SAI-based classiﬁca-
tion for a trajectory translated by a uniform random vector v on [0, v] (v = 0.1, 10, · · · , 106). (c)
Comparison of SI, SAI of the original trajectory, and SAI of the translated trajectory."
REFERENCES,0.7452471482889734,Published as a conference paper at ICLR 2022
REFERENCES,0.7490494296577946,"C
LEAPFROG INTEGRATION METHOD"
REFERENCES,0.752851711026616,"The forward step of Leapfrog on separable Hamiltonian H(p, q) = T(p) + V (q) is given as fol-
lows (Chen et al., 2019),"
REFERENCES,0.7566539923954373,"p
ti+ 1"
REFERENCES,0.7604562737642585,2 = pti −1
REFERENCES,0.7642585551330798,"2(ti+1 −ti)∇qV (qti),"
REFERENCES,0.7680608365019012,"qti+1 = qti + (ti+1 −ti)∇pT(p
ti+ 1 2 ),"
REFERENCES,0.7718631178707225,"pti+1 = p
ti+ 1 2 −1"
REFERENCES,0.7756653992395437,2(ti+1 −ti)∇pV (qti+1).
REFERENCES,0.779467680608365,"D
PROOF OF THEOREM 1"
REFERENCES,0.7832699619771863,"Theorem 1. Let A be an n × n symmetric real matrix and {λi}n
i=1 be its n distinct eigenvalues.
Then for the linear system dx"
REFERENCES,0.7870722433460076,"dt = Ax with nonzero initial condition x(0) = u0, we have"
REFERENCES,0.7908745247148289,"1
∥u0∥2"
REFERENCES,0.7946768060836502,u1 −u0 ∆t
REFERENCES,0.7984790874524715,"2 =
 
n
X"
REFERENCES,0.8022813688212928,"i=1
ℓiλ2
i
1/2 + O(∆t),
(12)"
REFERENCES,0.8060836501901141,"where u1 = x(∆t) and ℓi ≥0, i = 1, ..., n with Pn
i=1 ℓi = 1."
REFERENCES,0.8098859315589354,"Proof. Let {vi}n
i=1 be the eigenvectors corresponding to the eigenvalues {λi}n
i=1. Since A is a
symmetric real matrix, {vi}n
i=1 form a set of orthogonal vectors. Without loss of generalization, we
assume {vi}n
i=1 is standard orthogonal basis. Let x(0) = u0 = Pn
i=1 civi; then ∥(c1, ..., cn)∥2 =
∥u0∥2. The solution of the linear system is x(t) = Pn
i=1 civieλit. Therefore,"
REFERENCES,0.8136882129277566,"lim
∆t→0
1
∥u0∥2"
REFERENCES,0.8174904942965779,u1 −u0 ∆t
REFERENCES,0.8212927756653993,"2 = lim
∆t→0
1
∥u0∥2"
REFERENCES,0.8250950570342205,"Pn
i=1 civieλi∆t −Pn
i=1 civi
∆t 2"
REFERENCES,0.8288973384030418,"=
1
∥u0∥2 n
X"
REFERENCES,0.8326996197718631,"i=1
ciλivi

2"
REFERENCES,0.8365019011406845,"=
 
n
X i=1"
REFERENCES,0.8403041825095057,"c2
i
Pn
s=1 c2s
λ2
i
1/2. (13)"
REFERENCES,0.844106463878327,"We let ℓi ≜
c2
i
Pn
s=1 c2s ≥0 and have Pn
i=1 ℓi = 1."
REFERENCES,0.8479087452471483,Published as a conference paper at ICLR 2022
REFERENCES,0.8517110266159695,"E
SUPPLEMENTARY RESULTS"
REFERENCES,0.8555133079847909,In this section we provide some supplementary results to the experiments of the main text.
REFERENCES,0.8593155893536122,"Fig. 13 gives a comparison of the predicted trajectories of the billiard model, and this is a supplement
to Fig. 4. Again, we see that SANN recovers visually the same trajectories as the reference. For
the three-body problem, Fig. 14 and Fig. 15 show the orbits of three particles learned by different
methods, and these results are a supplement to Fig. 6. We can see that HNN and SRNN diverge from
the orbits after a period of time while SANN produces orbits that are nearly identical to the orbits of
the reference and the energy is roughly conserved."
REFERENCES,0.8631178707224335,"Fig. 16 displays the comparison of SAI and SI with time, and these results are a supplement to Fig. 7.
These four examples in Fig. 16 support that the SAI keeps the same trend as SI and achieves the
peak at the same time as SI does."
REFERENCES,0.8669201520912547,"Reference
SANN (Ours)
SRNN
HNN"
REFERENCES,0.870722433460076,"Reference
SANN (Ours)
SRNN
HNN"
REFERENCES,0.8745247148288974,"Reference
SANN (Ours)
SRNN
HNN"
REFERENCES,0.8783269961977186,"Reference
SANN (Ours)
SRNN
HNN"
REFERENCES,0.8821292775665399,"Figure 13: (Supplement to Fig. 4) Comparison of billiard orbits simulated using the Hamiltonian
learned by different methods."
REFERENCES,0.8859315589353612,Published as a conference paper at ICLR 2022
REFERENCES,0.8897338403041825,"Figure 14: (Supplement to Fig. 6) The ﬁrst three columns show the comparison of the reference
orbits (solid curves) and the orbits learned by different method (dotted curves). • is the initial
position, and ▶is the direction."
REFERENCES,0.8935361216730038,Published as a conference paper at ICLR 2022
REFERENCES,0.8973384030418251,"Figure 15: (Supplement to Fig. 6) The ﬁrst three columns show the comparison of the reference
orbits (solid curves) and the orbits learned by different methods (dotted curves). • is the initial
position, and ▶is the direction."
REFERENCES,0.9011406844106464,"Figure 16: (Supplement to Fig. 7) Comparison of SAI and SI with time. The color markers refers to
the occurrence of the stiffness phenomena, and the same color corresponds to the same time period.
The trends of SAI and SI are consistent."
REFERENCES,0.9049429657794676,Published as a conference paper at ICLR 2022
REFERENCES,0.908745247148289,"F
NOISY DATA"
REFERENCES,0.9125475285171103,"In this section we evaluate our method on the noisy training data on Billiard model. We add the
Gaussian noise with different noise levels (standard deviation) to the clean training data. Figures 17
and 18 respectively show the MSE and relative energy error with time under noisy data with different
noise level. We can see that all the learned dynamical systems become unstable and the energy gets
nonconserved. We see that HNN, SRNN, and SANN have difﬁculty in learning the chaotic dynamics
when the data is disturbed by noise."
REFERENCES,0.9163498098859315,"Figure 17: Comparison of MSE of the learned dynamical systems under noisy data with different
noise levels."
REFERENCES,0.9201520912547528,"Figure 18: Comparison of relative energy error of the learned dynamical systems under noisy data
with different noise levels."
REFERENCES,0.9239543726235742,Published as a conference paper at ICLR 2022
REFERENCES,0.9277566539923955,"G
HYPERPARAMETER SELECTION"
REFERENCES,0.9315589353612167,"We determine the stiff ratio γ based on the validation dataset. We investigate the distribution about
the SAI over a trajectory and estimate the ratio of the stiff portion for the dataset. Take the dataset
of the three-body problem as an example. We normalize the SAIs of a trajectory to [0, 1] and show
the increasing order of SAIs in Fig. 19. One can see that large SAIs mainly concentrate around the
order index from 90 to 100. Hence, we choose 90-percentile (γ = 0.1) as the threshold."
REFERENCES,0.935361216730038,"Figure 19: The increasing order of the normalized SAIs over the three-body dataset of 1,000 trajec-
tories. The shaded area represents the 95% conﬁdence interval."
REFERENCES,0.9391634980988594,"For the partition S for nonstiff and stiff portions, we perform the grid search for the pairs of pa-
rameters as did in Choudhary et al. (2020). We use the validation loss as a proxy of the ﬁnal
performance. In particular, we ﬁrst conduct 10-epoch training and sort the pairs of parameters based
on the validation loss. Then, we choose the parameters with the smallest validation loss."
REFERENCES,0.9429657794676806,Published as a conference paper at ICLR 2022
REFERENCES,0.9467680608365019,"H
INSEPARABLE HAMILTONIAN"
REFERENCES,0.9505703422053232,"SANN can also be applied to inseparable Hamiltonian. Like HNN, we use an NN that takes p and q
as an input to approximate the Hamiltonian directly on two problems: a chain of N pendulums(Finzi
et al., 2020) and the three-body problem."
REFERENCES,0.9543726235741445,"In the Pendulum-N problem, let mi be the mass of the i-th pendulum, ℓi be the rigid rod between
(i −1)-th and i-th pendulum, pi be the generalized momentum and qi be the angle between the i-th
pendulum and the y-axis. The Hamiltonian can be deﬁned as"
REFERENCES,0.9581749049429658,"H(p, q) = 1"
REFERENCES,0.9619771863117871,"2p⊤M(q)−1p − N
X i=1 i
X"
REFERENCES,0.9657794676806084,"k=1
miℓk cos qk,
(14)"
REFERENCES,0.9695817490494296,"where the mass matrix has a complicated form M(q)i,j = cos(qi −qj)ℓiℓj
PN
k=max{i,j} mk. We
consider N = 4 and N = 6, and set g = mi = ℓi = 1, i = 1, 2, · · · , N. We simulate the training
set with 200 trajectories and the testing set with 100 trajectories by RKF45. The initial states are
uniformly and randomly sampled from [−0.25, 0.25]. Each trajectory contains 100 time steps with
a step size of 0.03."
REFERENCES,0.973384030418251,"The three-body problem is described in Section 4.2. It has a separable Hamiltonian, but we treat it
as an inseparable Hamiltonian in this experiment."
REFERENCES,0.9771863117870723,"We compare SANN with HNN using these two problems. For both methods, the Euler method is
used for training while RKF45 is used for predicting. For SANN, we set the partition parameter S
to be 50 and 10 for the N-chain pendulum and the three-body problem, respectively. Table 2 shows
the train MSE and test MSE for SANN and HNN. We can see that SANN outperforms HNN in all
the tests."
REFERENCES,0.9809885931558935,"Dataset
Method
Train MSE
Test MSE"
REFERENCES,0.9847908745247148,"Pendulum-4
HNN
3.80E-08
8.40E-05
SANN
1.80E-08
4.60E-05"
REFERENCES,0.9885931558935361,"Pendulum-6
HNN
1.60E-07
4.80E-04
SANN
1.00E-07
3.90E-04"
REFERENCES,0.9923954372623575,"Three-body
HNN
4.20E-06
1.60E+00
SANN
8.60E-07
7.00E-01"
REFERENCES,0.9961977186311787,Table 2: Train MSE and test MSE for HNN and SANN.
