Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002544529262086514,"To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely
used but often requires a large number of negative samples. Without negative
samples yet achieving competitive performance, a recent work (Chen & He, 2021)
has attracted signiﬁcant attention for providing a minimalist simple Siamese (Sim-
Siam) method to avoid collapse. However, the reason for how it avoids collapse
without negative samples remains not fully clear and our investigation starts by
revisiting the explanatory claims in the original SimSiam. After refuting their
claims, we introduce vector decomposition for analyzing the collapse based on
the gradient analysis of the l2-normalized representation vector. This yields a uni-
ﬁed perspective on how negative samples and SimSiam alleviate collapse. Such a
uniﬁed perspective comes timely for understanding the recent progress in SSL."
INTRODUCTION,0.005089058524173028,"1
INTRODUCTION"
INTRODUCTION,0.007633587786259542,"Beyond the success of NLP (Lan et al., 2020; Radford et al., 2019; Devlin et al., 2019; Su et al.,
2020; Nie et al., 2020), self-supervised learning (SSL) has also shown its potential in the ﬁeld of
vision tasks (Li et al., 2021; Chen et al., 2021; El-Nouby et al., 2021). Without the ground-truth
label, the core of most SSL methods lies in learning an encoder with augmentation-invariant repre-
sentation (Bachman et al., 2019; He et al., 2020; Chen et al., 2020a; Caron et al., 2020; Grill et al.,
2020). Speciﬁcally, they often minimize the representation distance between two positive samples,
i.e. two augmented views of the same image, based on a Siamese network architecture (Bromley
et al., 1993). It is widely known that for such Siamese networks there exists a degenerate solution,
i.e. all outputs “collapsing” to an undesired constant (Chen et al., 2020a; Chen & He, 2021). Early
works have attributed the collapse to lacking a repulsive component in the optimization goal and
adopted contrastive learning (CL) with negative samples, i.e. views of different samples, to alleviate
this problem. Introducing momentum into the target encoder, BYOL shows that Siamese architec-
tures can be trained with only positive pairs. More recently, SimSiam (Chen & He, 2021) has caught
great attention by further simplifying BYOL by removing the momentum encoder, which has been
seen as a major milestone achievement in SSL for providing a minimalist method for achieving
competitive performance. However, more investigation is required for the following question:
How does SimSiam avoid collapse without negative samples?"
INTRODUCTION,0.010178117048346057,"Our investigation starts with revisiting the explanatory claims in the original SimSiam paper (Chen
& He, 2021). Notably, two components, i.e. stop gradient and predictor, are essential for the success
of SimSiam (Chen & He, 2021). The reason has been mainly attributed to the stop gradient (Chen
& He, 2021) by hypothesizing that it implicitly involves two sets of variables and SimSiam behaves
like alternating between optimizing each set. Chen & He argue that the predictor h is helpful in
SimSiam because h ﬁlls the gap to approximate expectation over augmentations (EOA)."
INTRODUCTION,0.01272264631043257,"Unfortunately, the above explanatory claims are found to be ﬂawed due to reversing the two paths
with and without gradient (see Sec. 2.2). This motivates us to ﬁnd an alternative explanation, for
which we introduce a simple yet intuitive framework for facilitating the analysis of collapse in SSL."
INTRODUCTION,0.015267175572519083,∗equal contribution
INTRODUCTION,0.017811704834605598,Published as a conference paper at ICLR 2022
INTRODUCTION,0.020356234096692113,"Speciﬁcally, we propose to decompose a representation vector into center and residual components.
This decomposition facilitates understanding which gradient component is beneﬁcial for avoiding
collapse. Under this framework, we show that a basic Siamese architecture cannot prevent collapse,
for which an extra gradient component needs to be introduced. With SimSiam interpreted as pro-
cessing the optimization target with an inverse predictor, the analysis of its extra gradient shows that
(a) its center vector helps prevent collapse via the de-centering effect; (b) its residual vector achieves
dimensional de-correlation which also alleviates collapse."
INTRODUCTION,0.022900763358778626,"Moreover, under the same gradient decomposition, we ﬁnd that the extra gradient caused by neg-
ative samples in InfoNCE (He et al., 2019; Chen et al., 2020b;a; Tian et al., 2019; Khosla et al.,
2020) also achieves de-centering and de-correlation in the same manner. It contributes to a uniﬁed
understanding on various frameworks in SSL, which also inspires the investigation of hardness-
awareness Wang & Liu (2021) from the inter-anchor perspective Zhang et al. (2022) for further
bridging the gap between CL and non-CL frameworks in SSL. Finally, simplifying the predictor for
more explainable SimSiam, we show that a single bias layer is sufﬁcient for preventing collapse."
INTRODUCTION,0.02544529262086514,"The basic experimental settings for our analysis are detailed in Appendix A.1 with a more speciﬁc
setup discussed in the context. Overall, our work is the ﬁrst attempt for performing a comprehensive
study on how SimSiam avoids collapse without negative samples. Several works, however, have
attempted to demystify the success of BYOL (Grill et al., 2020), a close variant of SimSiam. A
technical report (Fetterman & Albrecht, 2020) has suggested the importance of batch normalization
(BN) in BYOL for its success, however, a recent work (Richemond et al., 2020) refutes their claim
by showing BYOL works without BN, which is discussed in Appendix B."
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS,0.027989821882951654,"2
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS"
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS,0.030534351145038167,"l2-normalized vector and optimization goal. SSL trains an encoder f for learning discriminative
representation and we denote such representation as a vector z, i.e. f(x) = z where x is a certain
input. For the augmentation-invariant representation, a straightforward goal is to minimize the dis-
tance between the representations of two positive samples, i.e. augmented views of the same image,
for which mean squared error (MSE) is a default choice. To avoid scale ambiguity, the vectors are
often l2-normalized, i.e. Z = z/||z|| (Chen & He, 2021), before calculating the MSE:"
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS,0.03307888040712468,"LMSE = (Za −Zb)2/2 −1 = −Za · Zb = Lcosine,
(1)"
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS,0.035623409669211195,"which shows the equivalence of a normalized MSE loss to the cosine loss (Grill et al., 2020)."
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS,0.03816793893129771,"Collapse in SSL and solution of SimSiam. Based on a Siamese architecture, the loss in Eq 1
causes the collapse, i.e. f always outputs a constant regardless of the input variance. We refer to
this Siamese architecture with loss Eq 1 as Naive Siamese in the remainder of paper. Contrastive
loss with negative samples is a widely used solution (Chen et al., 2020a). Without using negative
samples, SimSiam solves the collapse problem via predictor and stop gradient, based on which the
encoder is optimized with a symmetric loss:"
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS,0.04071246819338423,"LSimSiam = −(Pa · sg(Zb) + Pb · sg(Za)),
(2)"
REVISITING SIMSIAM AND ITS EXPLANATORY CLAIMS,0.043256997455470736,"where sg(·) is stop gradient and P is the output of predictor h, i.e. p = h(z) and P = p/||p||."
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.04580152671755725,"2.1
REVISING EXPLANATORY CLAIMS IN SIMSIAM"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.04834605597964377,"Interpreting stop gradient as AO. Chen & He hypothesize that the stop gradient in Eq 2 is an
implementation of Alternating between the Optimization of two sub-problems, which is denoted
as AO. Speciﬁcally, with the loss considered as L(θ, η) = Ex,T
hFθ(T (x)) −ηx
2i
, the op-"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.05089058524173028,"timization objective minθ,η L(θ, η) can be solved by alternating ηt ←arg minη L(θt, η) and
θt+1 ←arg minθ L(θ, ηt). It is acknowledged that this hypothesis does not fully explain why
the collapse is prevented (Chen & He, 2021). Nonetheless, they mainly attribute SimSiam success
to the stop gradient with the interpretation that AO might make it difﬁcult to approach a constant ∀x."
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.05343511450381679,"Interpreting predictor as EOA. The AO problem (Chen & He, 2021) is formulated independent of
predictor h, for which they believe that the usage of predictor h is related to approximating EOA for
ﬁlling the gap of ignoring ET [·] in a sub-problem of AO. The approximation of ET [·] is summarized"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.05597964376590331,Published as a conference paper at ICLR 2022
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.058524173027989825,"in Appendix A.2. Chen & He support their interpretation by proof-of-concept experiments. Specif-
ically, they show that updating ηx with a moving-average ηt
x ←m ∗ηt
x + (1 −m) ∗Fθt(T ′(x))
can help prevent collapse without predictor (see Fig. 1 (b)). Given that the training completely fails
when the predictor and moving average are both removed, at ﬁrst sight, their reasoning seems valid."
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.061068702290076333,"2.2
DOES THE PREDICTOR FILL THE GAP TO APPROXIMATE EOA?"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.06361323155216285,"Reasoning ﬂaw. Considering the stop gradient, we divide the framework into two sub-models with
different paths and term them Gradient Path (GP) and Stop Gradient Path (SGP). For SimSiam, only
the sub-model with GP includes the predictor (see Fig. 1 (a)). We point out that their reasoning ﬂaw
of predictor analysis lies in the reverse of GP and SGP. By default, the moving-average sub-model,
as shown in Fig. 1 (b), is on the same side as SGP. Note that Fig. 1 (b) is conceptually similar to
Fig. 1 (c) instead of Fig. 1 (a). It is worth mentioning that the Mirror SimSiam in Fig. 1 (c) is what
stop gradient in the original SimSiam avoids. Therefore, it is problematic to perceive h as EOA."
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.06615776081424936,"GP
GP
SGP
GP zb"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.06870229007633588,"encoder f 
encoder f"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.07124681933842239,predictor h
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.0737913486005089,similarity
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.07633587786259542,image x
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.07888040712468193,(a) SimSiam
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.08142493638676845,"encoder f 
encoder f"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.08396946564885496,similarity
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.08651399491094147,image x SGP
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.089058524173028,(c) Mirror SimSiam
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.0916030534351145,predictor h
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.09414758269720101,"encoder f 
encoder f"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.09669211195928754,similarity
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.09923664122137404,image x SGP
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.10178117048346055,(b) Moving Average
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.10432569974554708,"moving 
average zb pa"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.10687022900763359,"za
zb
za
za pb"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.10941475826972011,"Figure 1: Reasoning Flaw in SimSiam. (a) Standard SimSiam architecture. (b) Moving-Average
Model proposed in the proof-of-concept experiment (Chen & He, 2021). (c) Mirror SimSiam, which
has the same model architecture as SimSiam but with the reverse of GP and SGP."
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.11195928753180662,"encoder f 
encoder f"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.11450381679389313,predictor h
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.11704834605597965,similarity
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.11959287531806616,image x GP SGP
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.12213740458015267,(c) SimSiam with Inverse Predictor
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.12468193384223919,predictor h
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.1272264631043257,"inverse 
predictor h-1"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.1297709923664122,"encoder f 
encoder f"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.13231552162849872,predictor h
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.13486005089058525,similarity
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.13740458015267176,image x
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.13994910941475827,"GP
SGP"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.14249363867684478,(b) SimSiam with Symmetric Predictor
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.1450381679389313,predictor h
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.1475826972010178,"encoder f 
encoder f"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.15012722646310434,similarity
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.15267175572519084,image x GP
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.15521628498727735,(a) Naive Siamese
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.15776081424936386,"za
zb
za
zb
za
zb"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.16030534351145037,"pa
pb
pa
pb SGP"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.1628498727735369,"Figure 2: Different architectures of Siamese model. When it is trained experimentally, the inverse
predictor in (c) has the same architecture as predictor h."
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.16539440203562342,"Method
# aug
Collapse
Std
Top-1 (%)
Moving average
2
×
0.0108
46.57
Same batch
10
✓
0
1
Same batch
25
✓
0
1"
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.16793893129770993,Table 1: Inﬂuence of Explicit EOA. Detailed setup is reported in Appendix A.3
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.17048346055979643,"Explicit EOA does not prevent collapse. (Chen & He, 2021) points out that “in practice, it would
be unrealistic to actually compute the expectation ET [·]. But it may be possible for a neural net-
work (e.g., the preditor h) to learn to predict the expectation, while the sampling of T is implicitly
distributed across multiple epochs.” If implicitly sampling across multiple epochs is beneﬁcial, ex-
plicitly sampling sufﬁcient large N augmentations in a batch with the latest model would be more
beneﬁcial to approximate ET [·]. However, Table 1 shows that the collapse still occurs and suggests
that the equivalence between predictor and EOA does not hold."
REVISING EXPLANATORY CLAIMS IN SIMSIAM,0.17302798982188294,Published as a conference paper at ICLR 2022
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.17557251908396945,"2.3
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM"
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.178117048346056,"Symmetric Predictor does not prevent collapse. The difference between Naive Siamese and Sim-
siam lies in whether the gradient in backward propagation ﬂows through a predictor, however, we
show that this propagation helps avoid collapse only when the predictor is not included in the SGP
path. With h being trained the same as Eq 2, we optimize the encoder f through replacing the Z
in Eq 2 with P . The results in Table. 2 show that it still leads to collapse. Actually, this is well
expected by perceiving h to be part of the new encoder F, i.e. p = F(x) = h(f(x)). In other
words, the symmetric architectures with and without predictor h both lead to collapse."
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.1806615776081425,"Predictor with stop gradient is asymmetric. Clearly, how SimSiam avoids collapse lies in its
asymmetric architecture, i.e. one path with h and the other without h. Under this asymmetric archi-
tecture, the role of stop gradient is to only allow the path with predictor to be optimized with the
encoder output as the target, not vice versa. In other words, the SimSiam avoids collapse by exclud-
ing Mirror SimSiam (Fig. 1 (c)) which has a loss (mirror-like Eq 2) as LMirror = −(Pa·Zb+Pb·Za),
where stop gradient is put on the input of h, i.e. pa = h(sg[za]) and pb = h(sg[zb])."
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.183206106870229,"Method
Collapse
Top-1 (%)
Simsiam
×
66.62
Mirror SimSiam
✓
1
Naive Siamese
✓
1
Symmetric Predictor
✓
1"
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.18575063613231552,"Table 2: Results of various Siamese archi-
tectures. Detailed trend and setup are re-
ported in Appendix A.4"
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.18829516539440203,"Predictor vs. inverse predictor. We interpret h as a
function mapping from z to p, and introduce a con-
ceptual inverse mapping h−1, i.e. z = h−1(p). Here,
as shown in Table 2, SimSiam with symmetric predic-
tor (Fig. 2 (b)) leads to collapse, while SimSiam (Fig. 1
(a)) avoids collapse. With the conceptual h−1, we in-
terpret Fig. 1 (a) the same as Fig. 2 (c) which differs
from Fig. 2 (b) via changing the optimization target
from pb to zb, i.e. zb = h−1(pb). This interpretation
suggests that the collapse can be avoided by processing the optimization target with h−1. By con-
trast, Fig. 1 (c) and Fig. 2 (a) both lead to collapse, suggesting that processing the optimization
target with h is not beneﬁcial for preventing collapse. Overall, asymmetry alone does not guarantee
collapse avoidance, which requires the optimization target to be processed by h−1 not h."
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.19083969465648856,"0
200
400
600
800
1000
Epoch 10 20 30 40 50 60 70"
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.19338422391857507,Top-1 (%)
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.19592875318066158,"(I) SimSiam
(II) Inverse Predictor"
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.1984732824427481,"Figure 3: Comparison of original SimSiam
and SimSiam with Inverse Predictor."
ASYMMETRIC INTERPRETATION OF PREDICTOR WITH STOP GRADIENT IN SIMSIAM,0.2010178117048346,"Trainable inverse predictor and its implication on
EOA. In the above, we propose a conceptual inverse
predictor h−1 in Fig. 2 (c), however, it remains yet
unknown whether such an inverse predictor is experi-
mentally trainable. A detailed setup for this investiga-
tion is reported in Appendix A.5. The results in Fig. 3
show that a learnable h−1 leads to slightly inferior
performance, which is expected because h−1 cannot
make the trainable inverse predictor output z∗
b com-
pletely the same as zb. Note that it would be equiva-
lent to SimSiam if z∗
b = zb. Despite a slight perfor-
mance drop, the results conﬁrm that h−1 is trainable.
The fact that h−1 is trainable provides additional ev-
idence that the role h plays in SimSiam is not EOA
because theoretically h−1 cannot restore a random augmentation T ′ from an expectation p, where
p = h(z) = ET
h
Fθt(T (x))
i
."
VECTOR DECOMPOSITION FOR UNDERSTANDING COLLAPSE,0.2035623409669211,"3
VECTOR DECOMPOSITION FOR UNDERSTANDING COLLAPSE"
VECTOR DECOMPOSITION FOR UNDERSTANDING COLLAPSE,0.20610687022900764,"By default, InfoNCE (Chen et al., 2020a) and SimSiam (Chen & He, 2021) both adopt l2-
normalization in their loss for avoiding scale ambiguity. We treat the l2-normalized vector, i.e. Z,
as the encoder output, which signiﬁcantly simpliﬁes gradient derivation and the following analysis."
VECTOR DECOMPOSITION FOR UNDERSTANDING COLLAPSE,0.20865139949109415,"Vector decomposition. For the purpose of analysis, we propose to decompose Z into two parts,
Z = o + r, where o, r denote center vector and residual vector respectively. Speciﬁcally, the
center vector o is deﬁned as an average of Z over the whole representation space oz = E[Z].
However, we approximate it with all vectors in current mini-batch, i.e. oz =
1
M
PM
m=1 Zm, where
M is the mini-batch size. We deﬁne the residual vector r as the residual part of Z, i.e. r = Z −oz."
VECTOR DECOMPOSITION FOR UNDERSTANDING COLLAPSE,0.21119592875318066,Published as a conference paper at ICLR 2022
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.21374045801526717,"3.1
COLLAPSE FROM THE VECTOR PERSPECTIVE"
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.21628498727735368,"Collapse: from result to cause. A Naive Siamese is well expected to collapse since the loss is
designed to minimize the distance between positive samples, for which a constant constitutes an
optimal solution to minimize such loss. When the collapse occurs, ∀i, Zi =
1
M
PM
m=1 Zm = oz,
where i denotes a random sample index, which shows the constant vector is oz in this case. This
interpretation only suggests a possibility that a dominant o can be one of the viable solutions, while
the optimization, such as SimSiam, might still lead to a non-collapse solution. This merely describes
o as the consequence of the collapse, and our work investigates the cause of such collapse through
analyzing the inﬂuence of individual gradient components, i.e. o and r during training."
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.21882951653944022,"Competition between o and r. Complementary to the Standard Deviation (Std) (Chen & He,
2021), for indicating collapse, we introduce the ratio of o in z, i.e. mo = ||o||/||z||, where || ∗|| is
the L2 norm. Similarly, the ratio of r in z is deﬁned as mr = ||r||/||z||. When collapse happens,
i.e. all vectors Z are close to the center vector o, mo approaches 1 and mr approaches 0, which
is not desirable for SSL. A desirable case would be a relatively small mo and a relatively large
mr, suggesting a relatively small (large) contribution of o (r) in each Z. We interpret the cause of
collapse as a competition between o and r where o dominates over r, i.e. mo ≫mr. For Eq 1, the
derived negative gradient on Za (ignoring Zb for simplicity due to symmetry) is shown as:"
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.22137404580152673,Gcosine = −∂LMSE
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.22391857506361323,"∂Za
= Zb −Za ⇐⇒−∂Lcosine"
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.22646310432569974,"∂Za
= Zb,
(3)"
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.22900763358778625,"where the gradient component Za is a dummy term because the loss −Za · Za = −1 is a constant
having zero gradient on the encoder f. 0 1 mr 0 1 mo"
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.23155216284987276,"0
1000
2000
3000
4000
5000
6000
Iteration Step oz"
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.2340966921119593,random ra 
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.2366412213740458,"Figure 4: Inﬂuence of various gradient com-
ponents on mr and mo."
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.23918575063613232,"Conjecture1. With Za = oz + ra, we conjecture
that the gradient component of oz is expected to up-
date the encoder to boost the center vector thus in-
crease mo, while the gradient component of ra is ex-
pected to behave in the opposite direction to increase
mr. A random gradient component is expected to
have a relatively small inﬂuence."
COLLAPSE FROM THE VECTOR PERSPECTIVE,0.24173027989821882,"To verify the above conjecture, we revisit the dummy
gradient term Za. We design loss −Za · sg(oz) and
−Za · sg(Za −oz) to show the inﬂuence of gradi-
ent component o and ra respectively. The results in
Fig. 4 show that the gradient component oz has the
effect of increasing mo while decreasing mr. On the
contrary, ra helps increase mr while decreasing mo. Overall, the results verify Conjecture1."
EXTRA GRADIENT COMPONENT FOR ALLEVIATING COLLAPSE,0.24427480916030533,"3.2
EXTRA GRADIENT COMPONENT FOR ALLEVIATING COLLAPSE"
EXTRA GRADIENT COMPONENT FOR ALLEVIATING COLLAPSE,0.24681933842239187,"Revisit collapse in a symmetric architecture. Based on Conjecture1, here, we provide an intuitive
interpretation on why a symmetric Siamese architecture, such as Fig. 2 (a) and (b), cannot be trained
without collapse. Take Fig. 2 (a) as example, the gradient in Eq 3 can be interpreted as two equivalent
forms, from which we choose Zb−Za = (oz +rb)−(oz +ra) = rb−ra. Since rb comes from the
same positive sample as ra, it is expected that rb also increases mr, however, this effect is expected
to be smaller than that of ra, thus causing collapse."
EXTRA GRADIENT COMPONENT FOR ALLEVIATING COLLAPSE,0.24936386768447838,"Basic gradient and Extra gradient components. The negative gradient on Za in Fig. 2 (a) is
derived as Zb, while that on Pa in Fig. 2 (b) is derived as Pb. We perceive Zb and Pb in these
basic Siamese architectures as the Basic Gradient. Our above interpretation shows that such basic
components cannot prevent collapse, for which an Extra Gradient component, denoted as Ge, needs
to be introduced to break the symmetry. As the term suggests, Ge is deﬁned as a gradient term that
is relative to the basic gradient in a basic Siamese architecture. For example, negative samples can
be introduced to Naive Siamese (Fig. 2 (a)) for preventing collapse, where the extra gradient caused
by negative samples can thus be perceived as Ge with Zb as the basic gradient. Similarly, we can
also disentangle the negative gradient on Pa in SimSiam (Fig. 1 (a)), i.e. Zb, into a basic gradient
(which is Pb) and Ge which is derived as Zb −Pb (note that Zb = Pb + Ge). We analyze how Ge
prevents collapse via studying the independent roles of its center vector oe and residual vector re."
EXTRA GRADIENT COMPONENT FOR ALLEVIATING COLLAPSE,0.25190839694656486,Published as a conference paper at ICLR 2022
A TOY EXAMPLE EXPERIMENT WITH NEGATIVE SAMPLE,0.2544529262086514,"3.3
A TOY EXAMPLE EXPERIMENT WITH NEGATIVE SAMPLE"
A TOY EXAMPLE EXPERIMENT WITH NEGATIVE SAMPLE,0.25699745547073793,"Which repulsive component helps avoid collapse? Existing works often attribute the collapse in
Naive Siamese to lacking a repulsive part during the optimization. This explanation has motivated
previous works to adopt contrastive learning, i.e. attracting the positive samples while repulsing the
negative samples. We experiment with a simple triplet loss1, Ltri = −Za·sg(Zb −Zn), where Zn
indicates the representation of a Negative sample. The derived negative gradient on Za is Zb −Zn,
where Zb is the basic gradient component and thus Ge = −Zn in this setup. For a sample represen-
tation, what determines it as a positive sample for attracting or a negative sample for repulsing is the
residual component, thus it might be tempting to interpret that re is the key component of repulsive
part that avoids the collapse. However, the results in Table 3 show that the component beneﬁcial for
preventing collapse inside Ge is oe instead of re. Speciﬁcally, to explore the individual inﬂuence
of oe and re in the Ge, we design two experiments by removing one component while keeping the
other one. In the ﬁrst experiment, we remove the re in Ge while keeping the oe. By contrast, the
oe is removed while keeping the re in the second experiment. In contrast to what existing explana-
tions may expect, we ﬁnd that the residual component oe prevents collapses. With Conjecture1, a
gradient component alleviates collapse if it has negative center vector. In this setup, oe = −oz, thus
oe has the de-centering role for preventing collapse. On the contrary, re does not prevent collapse
and keeping re even decreases the performance (36.21% < 47.41%). Since the negative sample is
randomly chosen, re just behaves like a random noise on the optimization to decrease performance."
A TOY EXAMPLE EXPERIMENT WITH NEGATIVE SAMPLE,0.2595419847328244,"Method
Ltriplet
Std
mo
mr
Collapse
Top-1 (%)
Baseline
−Za · sg(Zb + Ge)
0.020
0.026
0.99
×
36.21
Removing re
−Za · sg(Zb + oe)
0.02005
0.026
0.99
×
47.41
Removing oe
−Za · sg(Zb + re)
0
1
0
✓
1"
A TOY EXAMPLE EXPERIMENT WITH NEGATIVE SAMPLE,0.26208651399491095,Table 3: Gradient component analysis with a random negative sample.
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.26463104325699743,"3.4
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.26717557251908397,"It is challenging to derive the gradient on the encoder output in SimSiam due to a nonlinear MLP
module in h. The negative gradient on Pa for LSimSiam in Eq 2 can be derived as"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.2697201017811705,GSimSiam = −∂LSimSiam
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.272264631043257,"∂Pa
= Zb = Pb + (Zb −Pb) = Pb + Ge,
(4)"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.2748091603053435,"oe
re
Collapse
Top-1 (%)
✓
✓
×
66.62
✓
×
×
48.08
×
✓
×
66.15
×
×
✓
1"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.27735368956743,"Table 4: Gradient component analysis
for SimSiam."
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.27989821882951654,"where Ge indicates the aforementioned extra gradient
component. To investigate the inﬂuence of oe and re on
the collapse, similar to the analysis with the toy exam-
ple experiment in Sec. 3.3, we design the experiment by
removing one component while keeping the other. The
results are reported in Table 4. As expected, the model
collapses when both components in Ge are removed and
the best performance is achieved when both components
are kept. Interestingly, the model does not collapse when
either oe or re is kept. To start, we analyze how oe affects the collapse based on Conjecture1."
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.2824427480916031,"How oe alleviates collapse in SimSiam. Here, op is used to denote the center vector of P to
differentiate from the above introduced oz for denoting that of Z. In this setup Ge = Zb −Pb,
thus the residual gradient component is derived to be oe = oz −op. With Conjecture1, it is well
expected that oe helps prevent collapse if oe contains negative op since the analyzed vector is Pa. To
determine the amount of component of op existing in oe, we measure the cosine similarity between
oe −ηpop and op for a wide range of ηp. The results in Fig. 5 (a) show that their cosine similarity
is zero when ηp is around −0.5, suggesting oe has ≈−0.5op. With Conjecture1, this negative ηp
explains why SimSiam avoids collapse from the perspective of de-centering."
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.28498727735368956,"How oe causes collapse in Mirror SimSiam. As mentioned above, the collapse occurs in Mirror
SimSiam, which can also be explained by analyzing its oe. Here, oe = op −oz, for which we
evaluate the amount of component oz existing in oe via reporting the similarity between oe −ηzoz"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.2875318066157761,1Note that the triplet loss here does not have clipping form as in Schroff et al. (2015) for simplicity.
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.2900763358778626,Published as a conference paper at ICLR 2022
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.2926208651399491,"and oz. The results in Fig. 5 (a) show that their cosine similarity is zero when ηz is set to around
0.2. This positive ηz explains why Fig. 1(c) causes collapse from the perspective of de-centering."
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.2951653944020356,"Overall, we ﬁnd that processing the optimization target with h−1, as in Fig. 2 (c), alleviates collapse
(ηp ≈−0.5), while processing it with h, as in Fig. 1(c), actually strengthens the collapse (ηz ≈0.2).
In other words, via the analysis of oe, our results help explain how SimSiam avoids collapse as well
as how Mirror SimSiam causes collapse from a straightforward de-centering perspective."
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.29770992366412213,"1.0
0.5
0.0
0.5
1.0
1.0 0.5 0.0 0.5"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.30025445292620867,Cosine similarity p z (a) 0.25 0.50 0.75 1.00 mr
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.30279898218829515,"0
1000
2000
3000
Iteration Step 0.00 0.25 0.50 0.75 1.00 mo (b)"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.3053435114503817,"0.0
0.2
0.4
0.6
0.8
1.0
Tempture 0.0 0.2 0.4 0.6 0.8"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.30788804071246817,"Cosine similarity oe
re (c)"
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.3104325699745547,"Figure 5: (a) Investigating the amount of op existing in oz −op and the amount of oz existing in
op −oz. (b) Normally train the model as SimSiam for 5 epochs, then using collapsing loss for 1
epoch to reduce mr, followed by a correlation regularization loss. (c) Cosine similarity between re
(oe) and gradient on Za induced by a correlation regularization loss."
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.31297709923664124,"Relation to prior works. Motivated from preventing the collapse to a constant, multiple prior
works, such as W-MSE (Ermolov et al., 2021), Barlow-twins (Zbontar et al., 2021), DINO (Caron
et al., 2021), explicitly adopt de-centering to prevent collapse. Despite various motivations, we ﬁnd
that they all implicitly introduce an oe that contains a negative center vector. The success of their
approaches aligns well with our Conjecture1 as well as our above empirical results. Based on our
ﬁndings, we argue that the effect of de-centering can be perceived as oe having a negative center
vector. With this interpretation, we are the ﬁrst to demonstrate that how SimSiam with predictor and
stop gradient avoids collapse can be explained from the perspective of de-centering."
DECOMPOSED GRADIENT ANALYSIS IN SIMSIAM,0.3155216284987277,"Beyond de-centering for avoiding collapse. In the toy example experiment in Sec. 3.3, re is
found to be not beneﬁcial for preventing collapse and keeping re even decreases the performance.
Interestingly, as shown in Table 4, we ﬁnd that re alone is sufﬁcient for preventing collapse and
achieves comparable performance as Ge. This can be explained from the perspective of dimensional
de-correlation, which will be discussed in Sec. 3.5."
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.31806615776081426,"3.5
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE"
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.32061068702290074,"Conjecture2 and motivation. We conjecture that dimensional de-correlation increases mr for pre-
venting collapse. The motivation is straightforward as follows. The dimensional correlation would
be minimum if only a single dimension has a very high value for every individual class and the
dimension changes for different classes. In another extreme case, when all the dimensions have the
same values, equivalent to having a single dimension, which already collapses by itself in the sense
of losing representation capacity. Conceptually, re has no direct inﬂuence on the center vector, thus
we interpret that re prevents collapse through increasing mr."
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3231552162849873,"To verify the above conjecture, we train SimSiam normally with the loss in Eq 2 and train for several
epochs with the loss in Eq 1 for intentionally decreasing the mr to close to zero. Then, we train the
loss with only a correlation regularization term, which is detailed in Appendix A.6. The results in
Fig. 5 (b) show that this regularization term increases mr at a very fast rate."
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3256997455470738,"Dimensional de-correlation in SimSiam. Assuming h only has a single FC layer to exclude the
inﬂuence of oe, the weights in FC are expected to learn the correlation between different dimensions
for the encoder output. This interpretation echos well with the ﬁnding that the eigenspace of h weight
aligns well with that of correlation matrix (Tian et al., 2021). In essence, the h is trained to minimize
the cosine similarity between h(za) and I(zb), where I is identity mapping. Thus, h that learns the
correlation is optimized close to I, which is conceptually equivalent to optimizing with the goal of
de-correlation for Z. As shown in Table 4, for SimSiam, re alone also prevents collapse, which"
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3282442748091603,Published as a conference paper at ICLR 2022
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.33078880407124683,"0
50
100
150
200
Epoch 0.0 0.2 0.4 0.6 0.8 1.0 mo"
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3333333333333333,"SimSiam
InfoNCE
re removal"
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.33587786259541985,"0
50
100
150
200
Epoch 0.0 0.2 0.4 0.6 0.8 1.0 mr"
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3384223918575064,"0
50
100
150
200
Epoch 0.000 0.001 0.002 0.003 0.004"
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.34096692111959287,covariance
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3435114503816794,"0
50
100
150
200
Epoch 20 40"
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3460559796437659,Top-1 (%)
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3486005089058524,Figure 6: Inﬂuence of various gradient components on mr and mo.
DIMENSIONAL DE-CORRELATION HELPS PREVENT COLLAPSE,0.3511450381679389,"is attributed to the de-correlation effect since re has no de-centering effect. We observe from Fig.
6 that except in the ﬁrst few epochs, SimSiam decreases the covariance during the whole training.
Fig. 6 also reports the results for InfoNCE which will be discussed in Sec. 4."
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.35368956743002544,"4
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.356234096692112,"De-centering and de-correlation in InfoNCE. InfoNCE loss is a default choice in multiple seminal
contrastive learning frameworks (Sohn, 2016; Wu et al., 2018; Oord et al., 2018; Wang & Liu, 2021).
The derived negative gradient of InfoNCE on Za is proportional to Zb + PN
i=0 −λiZi, where
λi =
exp(Za·Zi/τ)
PN
i=0 exp(Za·Zi/τ), and Z0 = Zb for notation simplicity. See Appendix A.7 for the detailed"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.35877862595419846,"derivation. The extra gradient component Ge = PN
i=0 −λiZi = −oz −PN
i=0 λiri, for which
oe = −oz and re = −PN
i=0 λiri. Clearly, oe contains negative oz as de-centering for avoiding
collapse, which is equivalent to the toy example in Sec. 3.3 when the re is removed. Regarding
re, the main difference between Ltri in the toy example and InfoNCE is that the latter exploits a
batch of negative samples instead of a random one. λi is proportional to exp(Za·Zi), indicating
that a large weight is put on the negative sample when it is more similar to the anchor Za, for which,
intuitively, its dimensional values tend to have a high correlation with Za. Thus, re containing
such negative representation with a high weight tends to decrease dimensional correlation. To verify
this intuition, we measure the cosine similarity between re and the gradient on Za induced by a
correlation regularization loss. The results in Fig. 5 (c) show that their gradient similarity is high for
a wide range of temperature values, especially when τ is around 0.1 or 0.2, suggesting re achieves
similar role as an explicit regularization loss for performing de-correlation. Replacing re with oe
leads to a low cosine similarity, which is expected because oe has no de-correlation effect."
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.361323155216285,"The results of InfoNCE in Fig. 6 resembles that of SimSiam in terms of the overall trend. For ex-
ample, InfoNCE also decreases the covariance value during training. Moreover, we also report the
results of InfoNCE where re is removed for excluding the de-correlation effect. Removing re from
the InfoNCE loss leads to a high covariance value during the whole training. Removing re also
leads to a signiﬁcant performance drop, which echos with the ﬁnding in (Bardes et al., 2021) that
dimensional de-correlation is essential for competitive performance. Regarding how re in InfoNCE
achieves de-correlation, formally, we hypothesize that the de-correlation effect in InfoNCE arises
from the biased weights (λi) on negative samples. This hypothesis is corroborated by the tempera-
ture analysis in Fig. 7. We ﬁnd that a higher temperature makes the weight distribution of λi more
balanced indicated a higher entropy of λi, which echos with the ﬁnding in (Wang & Liu, 2021).
Moreover, we observe that a higher temperature also tends to increase the covariance value. Over-
all, with temperature as the control variable, we ﬁnd that more balanced weights among negative
samples decrease the de-correlation effect, which constitutes an evidence for our hypothesis."
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.3638676844783715,"Unifying SimSiam and InfoNCE. At ﬁrst sight, there is no conceptual similarity between SimSiam
and InfoNCE, and this is why the community is intrigued by the success of SimSiam without nega-
tive samples. Through decomposing the Ge into oe and re, we ﬁnd that for both, their oe plays the
role of de-centering and their re behaves like de-correlation. In this sense, we bring two seemingly
irrelevant frameworks into a uniﬁed perspective with disentangled de-centering and de-correlation."
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.366412213740458,"Beyond SimSiam and InfoNCE. In SSL, there is a trend of performing explicit manipulation of
de-centering and de-correlation, for which W-MSE (Ermolov et al., 2021), Barlow-twins (Zbontar
et al., 2021), DINO (Caron et al., 2021) are three representative works. They often achieve perfor-
mance comparable to those with InfoNCE or SimSiam. Towards a uniﬁed understanding of recent
progress in SSL, our work is most similar to a concurrent work (Bardes et al., 2021). Their work
is mainly inspired by Barlow-twins (Zbontar et al., 2021) but decomposes its loss into three explicit
components. By contrast, our work is motivated to answer the question of how SimSiam prevents"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.36895674300254455,Published as a conference paper at ICLR 2022
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.37150127226463103,"0.2
0.4
0.6
0.8
1.0
Temperature 0 1 2 3 4 5"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.37404580152671757,Entropy (a)
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.37659033078880405,"0
50
100
150
200
Epoch 10 20 30 40 50"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.3791348600508906,Top-1 (%)
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.3816793893129771,"= 0.1
= 0.4
= 0.8
= 1.0 (b)"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.3842239185750636,"0
50
100
150
200
Epoch 0.0 0.5 1.0 1.5"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.38676844783715014,"Covariance (×10
3) (c)"
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.3893129770992366,"Figure 7: Inﬂuence of temperature. (a) Entropy of λi with regard to temperature; (b) Top-1 accuracy
trend with various temperature; (c) Covariance trend with various temperature."
TOWARDS A UNIFIED UNDERSTANDING OF RECENT PROGRESS IN SSL,0.39185750636132316,"collapse without negative samples. Their work claims that variance component (equivalent to de-
centering) is an indispensable component for preventing collapse, while we ﬁnd that de-correlation
itself alleviates collapse. Overall, our work helps understand various frameworks in SSL from an
uniﬁed perspective, which also inspires an investigation of inter-anchor hardness-awareness Zhang
et al. (2022) for further bridging the gap between CL and non-CL frameworks in SSL."
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.3944020356234097,"5
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM"
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.3969465648854962,"Based on our understanding of how SimSiam prevents collapse, we demonstrate that simple com-
ponents (instead of a non-linear MLP in SimSiam) in the predictor are sufﬁcient for preventing
collapse. For example, to achieve dimensional de-correlation, a single FC layer might be sufﬁcient
because a single FC layer can realize the interaction among various dimensions. On the other hand,
to achieve de-centering, a single bias layer might be sufﬁcient because a bias vector can represent the
center vector. Attaching an l2-normalization layer at the end of the encoder, i.e. before the predictor,
is found to be critical for achieving the above goal."
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.3994910941475827,"Method
Predictor
Top-1 (%)
SimSiam
Non-linear MLP
66.9
Two FC
FC+FC+Bias
66.7
One FC
Tanh(FC)
64.82
One bias
Bias
49.82"
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.4020356234096692,Table 5: Linear evaluation on CIFAR100.
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.40458015267175573,"Pridictor with FC layers. To learn the dimensional
correlation, an FC layer is sufﬁcient theoretically but
can be difﬁcult to train in practice.
Inspired by the
property that Multiple FC layers make the training more
stable even though they can be mathematically equiva-
lent to a single FC layer (Bell-Kligler et al., 2019), we
adopt two consecutive FC layers which are equivalent
to removing the BN and ReLU in the original predictor.
The training can be made more stable if a Tanh layer is applied on the adopted single FC after every
iteration. Table 5 shows that they achieve performance comparable to that with a non-linear MLP."
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.4071246819338422,"Bias
(1) single bias
(2) bias in MLP
Similarity
0.99
0.89"
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.40966921119592875,"Table 6: Similarity between center vector
and (1) single bias layer (bp), (2) the last
bias layer of MLP in the predictor."
TOWARDS SIMPLIFYING THE PREDICTOR IN SIMSIAM,0.4122137404580153,"Predictor with a bias layer. A predictor with a single
bias layer can be utilized for preventing collapse (see
Table 5) and the trained bias vector is found to have
a cosine similarity of 0.99 with the center vector (see
Table 6). A bias in the MLP predictor also has a high
cosine similarity of 0.89, suggesting that it is not a co-
incidence. A theoretical derivation for justifying such a
high similarity as well as how this single bias layer prevents collapse are discussed in Appendix A.8."
CONCLUSION,0.41475826972010177,"6
CONCLUSION"
CONCLUSION,0.4173027989821883,"We point out a hidden ﬂaw in prior works for explaining the success of SimSiam and propose to
decompose the representation vector and analyze the decomposed components of extra gradient. We
ﬁnd that its center vector gradient helps prevent collapse via the de-centering effect and its residual
gradient achieves de-correlation which also alleviates collapse. Our further analysis reveals that
InfoNCE achieve the two effects in a similar manner, which bridges the gap between SimSiam and
InfoNCE and contributes to a uniﬁed understanding of recent progress in SSL. Towards simplifying
the predictor we have also found that a single bias layer is sufﬁcient for preventing collapse."
CONCLUSION,0.4198473282442748,Published as a conference paper at ICLR 2022
CONCLUSION,0.4223918575063613,ACKNOWLEDGEMENT
CONCLUSION,0.42493638676844786,"This work was partly supported by Institute for Information & communications Technology Plan-
ning & Evaluation (IITP) grant funded by the Korea government (MSIT) under grant No.2019-0-
01396 (Development of framework for analyzing, detecting, mitigating of bias in AI model and
training data), No.2021-0-01381 (Development of Causal AI through Video Understanding and Re-
inforcement Learning, and Its Applications to Real Environments) and No.2021-0-02068 (Artiﬁcial
Intelligence Innovation Hub). During the rebuttal, multiple anonymous reviewers provide valuable
advice to signiﬁcantly improve the quality of this work. Thank you all."
REFERENCES,0.42748091603053434,REFERENCES
REFERENCES,0.4300254452926209,"Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019."
REFERENCES,0.43256997455470736,"Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization
for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021."
REFERENCES,0.4351145038167939,"SeﬁBell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using
an internal-gan. NeurIPS, 2019."
REFERENCES,0.43765903307888043,"Jane Bromley, James W Bentz, L´eon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard
S¨ackinger, and Roopak Shah. Signature veriﬁcation using a “siamese” time delay neural network.
International Journal of Pattern Recognition and Artiﬁcial Intelligence, 1993."
REFERENCES,0.4402035623409669,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments.
arXiv preprint
arXiv:2006.09882, 2020."
REFERENCES,0.44274809160305345,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin.
Emerging properties in self-supervised vision transformers.
arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.44529262086513993,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In ICML, 2020a."
REFERENCES,0.44783715012722647,"Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021."
REFERENCES,0.45038167938931295,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b."
REFERENCES,0.4529262086513995,"Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision
transformers. ICCV, 2021."
REFERENCES,0.455470737913486,"Victor G. Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. Solo-learn: A library
of self-supervised methods for visual representation learning, 2021."
REFERENCES,0.4580152671755725,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), 2019."
REFERENCES,0.46055979643765904,"Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand
Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-
covariance image transformers. arXiv preprint arXiv:2106.09681, 2021."
REFERENCES,0.4631043256997455,"Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-
supervised representation learning. In ICML. PMLR, 2021."
REFERENCES,0.46564885496183206,"Abe Fetterman and Josh Albrecht. Understanding self-supervised and contrastive learning with
”bootstrap your own latent” (byol), 2020.
https://untitled-ai.github.io/
understanding-self-supervised-
contrastive-learning.html."
REFERENCES,0.4681933842239186,Published as a conference paper at ICLR 2022
REFERENCES,0.4707379134860051,"Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural
Information Processing Systems, 2020."
REFERENCES,0.4732824427480916,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019."
REFERENCES,0.4758269720101781,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020."
REFERENCES,0.47837150127226463,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan.
Supervised contrastive learning.
arXiv preprint
arXiv:2004.11362, 2020."
REFERENCES,0.48091603053435117,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. In ICLR, 2020."
REFERENCES,0.48346055979643765,"Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and
Jianfeng Gao. Efﬁcient self-supervised vision transformers for representation learning. arXiv
preprint arXiv:2106.09785, 2021."
REFERENCES,0.4860050890585242,"Ping Nie, Yuyu Zhang, Xiubo Geng, Arun Ramamurthy, Le Song, and Daxin Jiang. Dc-bert: De-
coupling question and document for efﬁcient contextual encoding. In Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in Information Retrieval,
2020."
REFERENCES,0.48854961832061067,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.4910941475826972,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 2019."
REFERENCES,0.49363867684478374,"Pierre H Richemond, Jean-Bastien Grill, Florent Altch´e, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, et al. Byol works even without
batch statistics. arXiv preprint arXiv:2010.10241, 2020."
REFERENCES,0.4961832061068702,"Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face
recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2015."
REFERENCES,0.49872773536895676,"Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS,
2016."
REFERENCES,0.5012722646310432,"Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. {VL}-{bert}:
Pre-training of generic visual-linguistic representations. In ICLR, 2020."
REFERENCES,0.5038167938931297,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019."
REFERENCES,0.5063613231552163,"Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021."
REFERENCES,0.5089058524173028,"Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In CVPR, 2021."
REFERENCES,0.5114503816793893,"Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In CVPR, 2018."
REFERENCES,0.5139949109414759,"Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. ICML, 2021."
REFERENCES,0.5165394402035624,"Chaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. Towards un-
derstanding and simplifying moco: Dual temperature helps contrastive learning without many
negative samples. In CVPR, 2022."
REFERENCES,0.5190839694656488,Published as a conference paper at ICLR 2022
REFERENCES,0.5216284987277354,"A
APPENDIX"
REFERENCES,0.5241730279898219,"A.1
EXPERIMENTAL SETTINGS"
REFERENCES,0.5267175572519084,"Self-supervised encoder training: Below are the settings for self-supervised encoder training. For
simplicity, we mainly use the default settings in a popular open library termed solo-learn (da Costa
et al., 2021)."
REFERENCES,0.5292620865139949,"Data augmentation and normalization:
We use a series of transformations including Ran-
domResizedCrop with scale [0.2, 1.0], bicubic interpolation. ColorJitter (brightness (0.4), contrast
(0.4), saturation (0.4), hue (0.1)) is randomly applied with the probability of 0.8. Random gray
scale RandomGrayscale is applied with p = 0.2 Horizontal ﬂip is applied with p = 0.5 The images
are normalized with the mean (0.4914, 0.4822, 0.4465) and Std (0.247, 0.243, 0.261)."
REFERENCES,0.5318066157760815,"Network architecture and initialization: The backbone architecture is ResNet-18. The projection
head contains three fully-connected (FC) layers followed by Batch Norm (BN) and ReLU, for which
ReLU in the ﬁnal FC layer is removed, i.e. FC1+BN +ReLU +FC2+BN +ReLU +FC3+BN.
All projection FC layers have 2048 neurons for input, output as well as the hidden dimensions. The
predictor head includes two FC layers as follows: FC1 + BN + ReLU + FC2. Input and output
of the predictor both have the dimension of 2048, while the hidden dimension is 512. All layers of
the network are by default initialized in Pytorch."
REFERENCES,0.5343511450381679,"Optimizer: SGD optimizer is used for the encoder training.
The batch size M is 256 and
the learning rate is linearly scaled by the formula lr × M/256 with the base learning rate lr set
to 0.5. The schedule for learning rate adopts the cosine decay as SimSiam. Momentum 0.9 and
weight decay 1.0 × 10−5 are used for SGD. We use one GPU for each pre-training experiment.
Following the practice of SimSiam, the learning rate of the predictor is ﬁxed during the training.
We use warmup training for the ﬁrst 10 epochs. If not speciﬁed, by default we train the model for
1000 epochs."
REFERENCES,0.5368956743002544,"Online linear evaluation:
For the online linear revaluation, we also follow the practice in
the solo-learn library (da Costa et al., 2021). The frozen features (2048 dimensions) from the
training set are extracted (from the self-supervised pre-trained model) to feed into a linear classiﬁer
(1 FC layer with the input 2048 and output of 100). The test is performed on the validation set. The
learning rate for the linear classiﬁer is 0.1. Overall, we report Top-1 accuracy with the online linear
evaluation in this work."
REFERENCES,0.539440203562341,"A.2
TWO SUB-PROBLEMS IN AO OF SIMSIAM"
REFERENCES,0.5419847328244275,"In the sub-problem ηt ←arg minη L(θt, η), ηt indicating latent representation of images at step t"
REFERENCES,0.544529262086514,"is actually obtained through ηt
x ←ET
h
Fθt(T (x))
i
, where they in practice ignore ET [·] and sample"
REFERENCES,0.5470737913486005,"only one augmentation T ′, i.e. ηt
x ←Fθt(T ′(x)). Conceptually, Chen & He equate the role of
predictor to EOA."
REFERENCES,0.549618320610687,"A.3
EXPERIMENTAL DETAILS FOR EXPLICIT EOA IN TABLE 1"
REFERENCES,0.5521628498727735,"In the Moving average experiment, we follow the setting in SimSiam (Chen & He, 2021) without
predictor. In the Same batch experiment, multiple augmentations, 10 augmentations for instance,
are applied on the same image. With multi augmentations, we get the corresponding encoded rep-
resentation, i.e. zi, i ∈[1, 10]. We minimize the cosine distance between the ﬁrst representation z1
and the average of the remaining vectors, i.e. ¯z = 1"
REFERENCES,0.55470737913486,"9
P10
i=2 zi. The gradient stop is put on the aver-
aged vector. We also experimented with letting the gradient backward through more augmentations,
however, they consistently led to collapse."
REFERENCES,0.5572519083969466,Published as a conference paper at ICLR 2022
REFERENCES,0.5597964376590331,"A.4
EXPERIMENTAL SETUP AND RESULT TREND FOR TABLE 2."
REFERENCES,0.5623409669211196,"Mirror SimSiam. Here we provide the pseudocode for Mirror SimSiam. In the Mirror SimSiam ex-
periment which relates to Fig. 1 (c). Without taking symmetric loss into account, the pseudocode is
shown in Algorithm 1. Taking symmetric loss into account, the pseudocode is shown in Algorithm 2."
REFERENCES,0.5648854961832062,Algorithm 1 Pytorch-like Pseudocode: Mirror SimSiam
REFERENCES,0.5674300254452926,"# f: encoder (backbone + projector)
# h: predictor"
REFERENCES,0.5699745547073791,for x in loader: # load a minibatch x with n samples
REFERENCES,0.5725190839694656,"x_a, x_b = aug(x), aug(x) # augmentation
z_a, z_b = f(x_a), f(x_b) # projections"
REFERENCES,0.5750636132315522,p_b = h(z_b.detach()) # detach z_b but still allowing gradient p_b
REFERENCES,0.5776081424936387,"L = D_cosine(z_a, p_b) # loss"
REFERENCES,0.5801526717557252,"L.backward() # back-propagate
update(f, h) # SGD update"
REFERENCES,0.5826972010178118,"def D_cosine(z, p): # negative cosine similarity"
REFERENCES,0.5852417302798982,"z = normalize(z, dim=1) # l2-normalize
p = normalize(p, dim=1) # l2-normalize
return -(z*p).sum(dim=1).mean()"
REFERENCES,0.5877862595419847,Algorithm 2 Pytorch-like Pseudocode: Mirror SimSiam
REFERENCES,0.5903307888040712,"# f: encoder (backbone + projector)
# h: predictor"
REFERENCES,0.5928753180661578,for x in loader: # load a minibatch x with n samples
REFERENCES,0.5954198473282443,"x_a, x_b = aug(x), aug(x) # augmentation
z_a, z_b = f(x_a), f(x_b) # projections"
REFERENCES,0.5979643765903307,"p_b = h(z_b.detach()) # detach z_b but still allowing gradient p_b
p_a = h(z_a.detach()) # detach z_a but still allowing gradient p_a"
REFERENCES,0.6005089058524173,"L = D_cosine(z_a, p_b)/2 + D_cosine(z_b, p_a)/2 # loss"
REFERENCES,0.6030534351145038,"L.backward() # back-propagate
update(f, h) # SGD update"
REFERENCES,0.6055979643765903,"def D_cosine(z, p): # negative cosine similarity"
REFERENCES,0.6081424936386769,"z = normalize(z, dim=1) # l2-normalize
p = normalize(p, dim=1) # l2-normalize
return -(z*p).sum(dim=1).mean()"
REFERENCES,0.6106870229007634,"Symmetric Predictor. To implement the SimSiam with Symmetric Predictor as in Fig. 2 (b), we
can just perceive the predictor as part of the new encoder, for which the pseudocode is provided in
Algorithm 3. Alternatively, we can additionally train the predictor similarly as that in SimSiam, for
which the training involves two losses, one for training the predictor and another for training the
new encoder (the corresponding pseudocode is provided in Algorithm 4). Moreover, for the second
implementation, we also experiment with another variant that ﬁxes the predictor while optimizing
the new encoder and then train the predictor alternatingly. All of them lead to collapse with a similar
trend as long as the symmetric predictor is used for training the encoder. For avoiding redundancy,
in Fig. 8 we only report the result of the second implementation."
REFERENCES,0.6132315521628499,"Result trend. The result trend of SimSiam, Naive Siamese, Mirror SimSiam, Symmetric Predictor
are shown in Fig. 8. We observe that all architectures lead to collapse except for SimSiam. Mirroe
SimSiam was stopped in the middle because a NaN value was returned from the loss."
REFERENCES,0.6157760814249363,"A.5
EXPERIMENTAL DETAILS FOR INVERSE PREDICTOR."
REFERENCES,0.6183206106870229,"In the inverse predictor experiment which relates to Fig. 2 (c), we introduce a new predictor which
has the same structure as that of the original predictor. The training loss consists of 3 parts: predictor
training loss, inverse predictor training and new encoder (old encoder+predictor) training. The new"
REFERENCES,0.6208651399491094,Published as a conference paper at ICLR 2022
REFERENCES,0.6234096692111959,Algorithm 3 Pytorch-like Pseudocode: Symmetric Predictor
REFERENCES,0.6259541984732825,"# f: encoder (backbone + projector)
# h: predictor"
REFERENCES,0.628498727735369,for x in loader: # load a minibatch x with n samples
REFERENCES,0.6310432569974554,"x_a, x_b = aug(x), aug(x) # augmentation
z_a, z_b = f(x_a), f(x_b) # projections
p_a, p_b = h(z_a), h(z_b) # predictions"
REFERENCES,0.6335877862595419,"L = D(p_a, p_b)/2 + D(p_b, p_a)/2 # loss"
REFERENCES,0.6361323155216285,"L.backward() # back-propagate
update(f, h) # SGD update"
REFERENCES,0.638676844783715,"def D(p, z): # negative cosine similarity"
REFERENCES,0.6412213740458015,"z = z.detach() # stop gradient
p = normalize(p, dim=1) # l2-normalize
z = normalize(z, dim=1) # l2-normalize
return -(p*z).sum(dim=1).mean()"
REFERENCES,0.6437659033078881,Algorithm 4 Pytorch-like Pseudocode: Symmetric Predictor (with additional training on predictor)
REFERENCES,0.6463104325699746,"# f: encoder (backbone + projector)
# h: predictor"
REFERENCES,0.648854961832061,for x in loader: # load a minibatch x with n samples
REFERENCES,0.6513994910941476,"x_a, x_b = aug(x), aug(x) # augmentation
z_a, z_b = f(x_a), f(x_b) # projections
p_a, p_b = h(z_a), h(z_b) # predictions"
REFERENCES,0.6539440203562341,"d_p_a, d_p_b = h(z_a.detach()), h(z_b.detach()) # detached predictor output"
REFERENCES,0.6564885496183206,"# predictor training loss
L_pred = D(d_p_a, z_b)/2 + D(d_p_b, z_a)/2"
REFERENCES,0.6590330788804071,"# encoder training loss
L_enc = D(p_a, d_p_b)/2 + D(p_b, d_p_a)/2"
REFERENCES,0.6615776081424937,L = L_pred + L_enc
REFERENCES,0.6641221374045801,"L.backward() # back-propagate
update(f, h) # SGD update"
REFERENCES,0.6666666666666666,"def D(p, z): # negative cosine similarity with detach on z"
REFERENCES,0.6692111959287532,"z = z.detach() # stop gradient
p = normalize(p, dim=1) # l2-normalize
z = normalize(z, dim=1) # l2-normalize
return -(p*z).sum(dim=1).mean()"
REFERENCES,0.6717557251908397,"0
200
400
600
800
1000
Epoch 0 10 20 30 40 50 60"
REFERENCES,0.6743002544529262,Top-1 (%)
REFERENCES,0.6768447837150128,"SimSiam
Symmetric Predictor
Mirror SimSiam
Naive Siamese"
REFERENCES,0.6793893129770993,"Figure 8: Result trend of Naive Siamese, Mirror SimSiam, Symmetric Predictor."
REFERENCES,0.6819338422391857,"encoder F consists of the old encoder f + predictor h. The practice of gradient stop needs to be
considered in the implementation. We provide the pseudocode in Algorithm 5."
REFERENCES,0.6844783715012722,Published as a conference paper at ICLR 2022
REFERENCES,0.6870229007633588,Algorithm 5 Pytorch-like Pseudocode: Trainable Inverse Predictor
REFERENCES,0.6895674300254453,"# f: encoder (backbone + projector)
# h: predictor
# h_inv: inverse predictor"
REFERENCES,0.6921119592875318,for x in loader: # load a minibatch x with n samples
REFERENCES,0.6946564885496184,"x_a, x_b = aug(x), aug(x) # augmentation
z_a, z_b = f(x_a), f(x_b) # projections
p_a, p_b = h(z_a), h(z_b) # predictions"
REFERENCES,0.6972010178117048,"d_p_a, d_p_b = h(z_a.detach()), h(z_b.detach()) # detached predictor output
# predictor training loss
L_pred = D(d_p_a, z_b)/2 + D(d_p_b, z_a)/2 # to train h"
REFERENCES,0.6997455470737913,"inv_p_a, inv_p_b = h_inv(p_a.detach()), h_inv(p_b.detach()) # to train h_inv
# inverse predictor training loss
L_inv_pred = D(inv_p_a, z_a)/2 + D(inv_p_b, z_b)/2"
REFERENCES,0.7022900763358778,"# encoder training loss
L_enc = D(p_a, h_inv(p_b))/2 + D(p_b, h_inv(p_a))"
REFERENCES,0.7048346055979644,L = L_pred + L_inv_pred + L_enc
REFERENCES,0.7073791348600509,"L.backward() # back-propagate
update(f, h, h_inv) # SGD update"
REFERENCES,0.7099236641221374,"def D(p, z): # negative cosine similarity with detach on z"
REFERENCES,0.712468193384224,"z = z.detach() # stop gradient
p = normalize(p, dim=1) # l2-normalize
z = normalize(z, dim=1) # l2-normalize
return -(p*z).sum(dim=1).mean()"
REFERENCES,0.7150127226463104,"A.6
REGULARIZATION LOSS"
REFERENCES,0.7175572519083969,"Following Zbontar et al. (2021), we compute covariance regularization loss of encoder output along
the mini-batch. The pseudocode for de-correlation loss calculation is put in Algorithm 6."
REFERENCES,0.7201017811704835,Algorithm 6 Pytorch-like Pseudocode: De-correlation loss
REFERENCES,0.72264631043257,"# Z_a: representation vector
# N: batch size
# D: the number of dimension for representation vector"
REFERENCES,0.7251908396946565,Z_a = Z_a - Z_a.mean(dim=0)
REFERENCES,0.727735368956743,"cov = Z_a.T @ Z_a / (N-1)
diag = torch.eye(D)"
REFERENCES,0.7302798982188295,loss = cov[˜diag.bool()].pow_(2).sum() / D
REFERENCES,0.732824427480916,"A.7
GRADIENT DERIVATION AND TEMPERATURE ANALYSIS FOR INFONCE"
REFERENCES,0.7353689567430025,"With · indicating the cosine similarity between vectors, the InfoNCE loss can be expressed as"
REFERENCES,0.7379134860050891,"LInfoNCE = −log
exp(Za · Zb/τ)"
REFERENCES,0.7404580152671756,"exp(Za · Zb/τ) + PN
i=1 exp(Za · Zi/τ)"
REFERENCES,0.7430025445292621,"= −log
exp(Za · Zb/τ)
PN
i=0 exp(Za · Zi/τ)
,
(5)"
REFERENCES,0.7455470737913485,"where N indicates the number of negative samples and Z0 = Zb for simplifying the notation. By
treating Za · Zi as the logit in a normal CE loss, we have the corresponding probability for each
negative sample as λi =
exp(Za·Zi/τ)
PN
i=0 exp(Za·Zi/τ), where i = 0, 1, 2, ..., N and we have PN
i=0 λi = 1."
REFERENCES,0.7480916030534351,Published as a conference paper at ICLR 2022
REFERENCES,0.7506361323155216,The negative gradient of the InfoNCE on the representation Za is shown as
REFERENCES,0.7531806615776081,−∂LInfoNCE
REFERENCES,0.7557251908396947,"∂Za
= 1"
REFERENCES,0.7582697201017812,"τ (1 −λ0)Zb −1 τ N
X"
REFERENCES,0.7608142493638677,"i=1
λiZi = 1"
REFERENCES,0.7633587786259542,"τ (Zb − N
X"
REFERENCES,0.7659033078880407,"i=0
λiZi) = 1"
REFERENCES,0.7684478371501272,"τ (Zb − N
X"
REFERENCES,0.7709923664122137,"i=0
λi(oz + ri)) = 1"
REFERENCES,0.7735368956743003,"τ (Zb + (−oz − N
X"
REFERENCES,0.7760814249363868,"i=0
λiri)"
REFERENCES,0.7786259541984732,"∝Zb + (−oz − N
X"
REFERENCES,0.7811704834605598,"i=0
λiri) (6)"
REFERENCES,0.7837150127226463,where 1
REFERENCES,0.7862595419847328,"τ can be adjusted through learning rate and is omitted for simple discussion. With Zb as the
basic gradient, Ge = −oz −PN
i=0 λiri, for which oe = −oz and re = −PN
i=0 λiri."
REFERENCES,0.7888040712468194,"When the temperature is set to a large value, λi =
exp(Za·Zi/τ)
PN
i=0 exp(Za·Zi/τ), approaches
1
N+1, indicated
by a high entropy value (see Fig. 7). InfoNCE will degenerate to a simple contrastive loss, i.e.
Lsimple = −Za · Zb +
1
N+1
PN
i=0 Za · Zi , which repulses every negative sample with an equal
force. In contrast, a relative smaller temperature will give more relative weight, i.e. larger λ, to
negative samples that are more similar to the anchor (Za)."
REFERENCES,0.7913486005089059,"The inﬂuence of the temperature on the covariance and accuracy is shown in Fig. 7 (b) and (c).
We observe that a higher temperature tends to decrease the effect of de-correlation, indicated by
a higher covariance value, which also leads to a performance drop. This veriﬁes our hypothesis
regarding on how re in InfoNCE achieves de-correlation because a large temperature causes more
balanced weights λi, which is found to alleviate the effect of de-correlation. For the setup, we note
that the encoder is trained for 200 epochs with the default setting in Solo-learn for the SimCLR
framework."
REFERENCES,0.7938931297709924,"A.8
THEORETICAL DERIVATION FOR A SINGLE BIAS LAYER"
REFERENCES,0.7964376590330788,With the cosine similarity loss deﬁned as Eq 7 Eq 8:
REFERENCES,0.7989821882951654,"cossim(a, b) =
a · b
√"
REFERENCES,0.8015267175572519,"a2 · b2 ,
(7)"
REFERENCES,0.8040712468193384,for which the derived gradient on the vector a is shown as
REFERENCES,0.806615776081425,"∂
∂acossim(a, b) =
b1
|a| · |b| −cossim(a, b) · a1"
REFERENCES,0.8091603053435115,"|a|2 .
(8)"
REFERENCES,0.811704834605598,"The above equation is used as a prior for our following derivations. As indicated in the main
manuscript, the encoder output za is l2-normalized before feeding into the predictor, thus pa =
Za + bp, bp denotes the bias layer in the predictor. The cosine similarity loss (ignoring the symme-
try for simplicity) is shown as"
REFERENCES,0.8142493638676844,Lcosine = −Pa · Zb = −pa
REFERENCES,0.816793893129771,"||pa|| ·
zb
∥zb∥
(9)"
REFERENCES,0.8193384223918575,Published as a conference paper at ICLR 2022
REFERENCES,0.821882951653944,The gradient on pa is derived as
REFERENCES,0.8244274809160306,−∂Lcosine
REFERENCES,0.8269720101781171,"∂pa
=
zb
∥zb∥· ∥pa∥−cossim(Za, Zb) ·
pa
||pa||2"
REFERENCES,0.8295165394402035,"=
1
∥pa∥  zb"
REFERENCES,0.8320610687022901,"∥zb∥−cossim(Za, Zb) · Pa "
REFERENCES,0.8346055979643766,"=
1
∥pa∥"
REFERENCES,0.8371501272264631,"
Zb −cossim(Za, Zb) · Za + bp ∥pa∥ "
REFERENCES,0.8396946564885496,"=
1
∥pa∥"
REFERENCES,0.8422391857506362,"
(oz + rb) −cossim(Za, Zb)"
REFERENCES,0.8447837150127226,"∥pa∥
· (oz + ra + bp)
"
REFERENCES,0.8473282442748091,"=
1
∥pa∥((oz + rb) −m · (oz + ra + bp))"
REFERENCES,0.8498727735368957,"=
1
∥pa∥((1 −m)oz −mbp + rb −m · ra) , (10)"
REFERENCES,0.8524173027989822,"where m = cossim(Za,Zb)"
REFERENCES,0.8549618320610687,"∥pa∥
."
REFERENCES,0.8575063613231552,"Given that pa = Za + bp, the negative gradient on bp is the same as that on pa as"
REFERENCES,0.8600508905852418,−∂Lcosine
REFERENCES,0.8625954198473282,"∂bp
= −∂Lcosine ∂pa"
REFERENCES,0.8651399491094147,"=
1
∥pa∥((1 −m)oz −mbp + rb −m · ra) .
(11)"
REFERENCES,0.8676844783715013,"We assume that the training is stable and the bias layer converges to a certain value when
−∂cossim(Za,Zb)"
REFERENCES,0.8702290076335878,"∂bp
= 0. Thus, the converged bp satisﬁes the following constraint:"
REFERENCES,0.8727735368956743,"1
∥pa∥((1 −m)oz −mbp + rb −mra)) = 0"
REFERENCES,0.8753180661577609,bp = 1 −m
REFERENCES,0.8778625954198473,"m
oz + 1"
REFERENCES,0.8804071246819338,"mrb −ra.
(12)"
REFERENCES,0.8829516539440203,"With a batch of samples, the average of 1"
REFERENCES,0.8854961832061069,"mrb and ra is expected to be close to 0 by the deﬁnition of
residual vector. Thus, the bias layer vector is expected to converge to:"
REFERENCES,0.8880407124681934,bp = 1 −m
REFERENCES,0.8905852417302799,"m
oz.
(13)"
REFERENCES,0.8931297709923665,"Rational behind the high similarity between bp and oz. The above theoretical derivation shows
that the parameters in the bias layer are excepted to converge to a vector 1−m"
REFERENCES,0.8956743002544529,"m oz. This theoretical
derivation justiﬁes why the empirically observed cosine similarity between bp and oz is as high as
0.99. Ideally, it should be 1, however, such a small deviation is expected with the training dynamics
taken into account."
REFERENCES,0.8982188295165394,"Rational behind how a single bias layer prevents collapse. Given that pa = Za +bp, the negative
gradient on Za is shown as"
REFERENCES,0.9007633587786259,−∂Lcosine
REFERENCES,0.9033078880407125,"∂Za
= −∂Lcosine ∂pa"
REFERENCES,0.905852417302799,"=
1
∥pa∥"
REFERENCES,0.9083969465648855,"
Zb −cossim(Za, Zb) · Za + bp ∥pa∥ "
REFERENCES,0.910941475826972,"=
1
∥pa∥Zb −cossim(Za, Zb)"
REFERENCES,0.9134860050890585,"∥pa∥2
Za −cossim(Za, Zb)"
REFERENCES,0.916030534351145,"∥pa∥2
bp. (14)"
REFERENCES,0.9185750636132316,"Here, we highlight that since the loss −Za · Za = −1 is a constant having zero gradients on the en-
coder, −cossim(Za,Zb)"
REFERENCES,0.9211195928753181,"∥pa∥2
Za can be seen as a dummy term. Considering Eq 13 and m = cossim(Za,Zb)"
REFERENCES,0.9236641221374046,"∥pa∥
,"
REFERENCES,0.926208651399491,Published as a conference paper at ICLR 2022
REFERENCES,0.9287531806615776,"we have b = (
∥pa∥
cossim(Za,Zb) −1)oz. The above equation is equivalent to"
REFERENCES,0.9312977099236641,−∂Lcosine
REFERENCES,0.9338422391857506,"∂Za
=
1
∥pa∥Zb −cossim(Za, Zb)"
REFERENCES,0.9363867684478372,"∥pa∥2
bp"
REFERENCES,0.9389312977099237,"=
1
∥pa∥Zb −cossim(Za, Zb)"
REFERENCES,0.9414758269720102,"∥pa∥2
(
∥pa∥
cossim(Za, Zb) −1)oz"
REFERENCES,0.9440203562340967,"=
1
∥pa∥Zb −
1
∥pa∥(1 −cossim(Za, Zb)"
REFERENCES,0.9465648854961832,"∥pa∥
)oz"
REFERENCES,0.9491094147582697,"∝Zb −(1 −cossim(Za, Zb)"
REFERENCES,0.9516539440203562,"∥pa∥
)oz. (15)"
REFERENCES,0.9541984732824428,"With Zb as the basic gradient, the extra gradient component Ge = −(1 −cossim(Za,Zb)"
REFERENCES,0.9567430025445293,"∥pa∥
)oz. Given
that pa = Za + bp and ∥Za∥= 1, thus ∥pa∥< 1 only when Za is negatively correlated with bp. In
practice, however, Za and bp are often positively correlated to some extent due to their shared center
vector component. In other words, ∥pa∥> 1. Moreover, cossim(Za, Zb) is smaller than 1, thus
−(1−cossim(Za,Zb)"
REFERENCES,0.9592875318066157,"∥pa∥
) < 0, suggesting Ge consists of negative oz with the effect of de-centerization.
This above derivation justiﬁes the rationale why a single bias layer can help alleviate collapse."
REFERENCES,0.9618320610687023,"B
DISCUSSION: DOES BN HELP AVOID COLLAPSE?"
REFERENCES,0.9643765903307888,"0
50
100
150
200
Epoch 0.0 0.2 0.4 0.6 0.8 1.0 mo"
REFERENCES,0.9669211195928753,"MSE
SimSiam"
REFERENCES,0.9694656488549618,"0
50
100
150
200
Epoch 0.0 0.2 0.4 0.6 0.8 1.0 mr"
REFERENCES,0.9720101781170484,"0
50
100
150
200
Epoch"
REFERENCES,0.9745547073791349,0.00000
REFERENCES,0.9770992366412213,0.00025
REFERENCES,0.9796437659033079,0.00050
REFERENCES,0.9821882951653944,0.00075
REFERENCES,0.9847328244274809,0.00100
REFERENCES,0.9872773536895675,covariance
REFERENCES,0.989821882951654,"0
50
100
150
200
Epoch 20 40"
REFERENCES,0.9923664122137404,Top-1 (%)
REFERENCES,0.9949109414758269,"Figure 9: BN with MSE helps prevent collapse without predictor or stop gradient. Its performance,
however, is inferior to the cosine loss-based SimSiam (with predictor and stop gradient)."
REFERENCES,0.9974554707379135,"To our knowledge, our work is the ﬁrst to revisit and refute the explanatory claims in (Chen & He,
2021). Several works, however, have attempted to demystify the success of BYOL (Grill et al.,
2020), a close variant of SimSiam. The success has been ascribed to BN in (Fetterman & Albrecht,
2020), however, (Richemond et al., 2020) refutes their claim. Since the role of intermediate BNs is
ascribed to stabilize training (Richemond et al., 2020; Chen & He, 2021), we only discuss the ﬁnal
BN in the SimSiam encoder. Note that with our Conjecture1, the ﬁnal BN that removes the mean
of representation vector is supposed to have de-centering effect. BY default SimSiam has such a
BN at the end of its encoder, however, it still collapses with the predictor and stop gradient. Why
would such a BN not prevent collapse in this case? Interestingly, we observe that such BN can help
alleviate collapse with a simple MSE loss (see Fig. 9), however, its performance is is inferior to the
cosine loss-based SimSiam (with predictor and stop gradient) due to the lack of the de-correlation
effect in SimSiam. Note that the cosine loss is in essence equivalent to a MSE loss on the l2-
normalized vectors. This phenomenon can be interpreted as that the l2-normalization causes another
mean after the BN removes it. Thus, with such l2-normalization in the MSE loss, i.e. adopting the
default cosine loss, it is important to remove the oe from the optimization target. The results with
the loss of −Za · sg(Zb + oe) in Table 3 show that this indeed prevents collapse and veriﬁes the
above interpretation."
