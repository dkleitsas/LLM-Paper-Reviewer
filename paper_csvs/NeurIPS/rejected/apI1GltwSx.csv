Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011210762331838565,"Recent advances in dataset distillation have led to solutions in two main directions.
1"
ABSTRACT,0.002242152466367713,"The conventional batch-to-batch matching mechanism is ideal for small-scale
2"
ABSTRACT,0.0033632286995515697,"datasets and includes bi-level optimization methods on models and syntheses,
3"
ABSTRACT,0.004484304932735426,"such as FRePo, RCIG, and RaT-BPTT, as well as other methods like distribution
4"
ABSTRACT,0.005605381165919282,"matching, gradient matching, and weight trajectory matching. Conversely, batch-to-
5"
ABSTRACT,0.006726457399103139,"global matching typifies decoupled methods, which are particularly advantageous
6"
ABSTRACT,0.007847533632286996,"for large-scale datasets. This approach has garnered substantial interest within the
7"
ABSTRACT,0.008968609865470852,"community, as seen in SRe2L, G-VBSM, WMDD, and CDA. A primary challenge
8"
ABSTRACT,0.010089686098654708,"with the second approach is the lack of diversity among syntheses within each
9"
ABSTRACT,0.011210762331838564,"class since samples are optimized independently and the same global supervision
10"
ABSTRACT,0.01233183856502242,"signals are reused across different synthetic images. In this study, we propose a
11"
ABSTRACT,0.013452914798206279,"new EarlyLate training scheme to enhance the diversity of images in batch-to-
12"
ABSTRACT,0.014573991031390135,"global matching with less computation. Our approach is conceptually simple yet
13"
ABSTRACT,0.01569506726457399,"effective, it partitions predefined IPC samples into smaller subtasks and employs
14"
ABSTRACT,0.016816143497757848,"local optimizations to distill each subset into distributions from distinct phases,
15"
ABSTRACT,0.017937219730941704,"reducing the uniformity induced by the unified optimization process. These distilled
16"
ABSTRACT,0.01905829596412556,"images from the subtasks demonstrate effective generalization when applied to
17"
ABSTRACT,0.020179372197309416,"the entire task. We conducted extensive experiments on CIFAR, Tiny-ImageNet,
18"
ABSTRACT,0.021300448430493273,"ImageNet-1K, and its sub-datasets. Our empirical results demonstrate that the
19"
ABSTRACT,0.02242152466367713,"proposed approach significantly improves over previous state-of-the-art methods
20"
ABSTRACT,0.023542600896860985,"under various IPCs1.
21"
INTRODUCTION,0.02466367713004484,"1
Introduction
22"
INTRODUCTION,0.0257847533632287,"Prior Dataset Distillation
Our DELT Distillation"
INTRODUCTION,0.026905829596412557,"Prior
Ours"
INTRODUCTION,0.028026905829596414,"Computation
!√ó#
# + # ‚àí./ ‚Ä¶ + ./"
INTRODUCTION,0.02914798206278027,"Early-optimized image
Late-optimized image
IPC1
IPCN"
INTRODUCTION,0.030269058295964126,Our DELT generated images
INTRODUCTION,0.03139013452914798,"IPC1:N
IPC1:N
batch-to-global"
INTRODUCTION,0.032511210762331835,matching
INTRODUCTION,0.033632286995515695,batch-to-global
INTRODUCTION,0.034753363228699555,matching ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶
INTRODUCTION,0.03587443946188341,"Full
Full IPC1 IPC2 IPCN IPC1 IPC2 IPCN #! #"" # # # ##"
INTRODUCTION,0.03699551569506727,"bird
bird"
INTRODUCTION,0.03811659192825112,"Figure 1: Distill datasets to IPCùëÅrequires ùëÅ‚àóùëá
iterations in traditional distillation processes
(left) but fewer iteration processes (right)."
INTRODUCTION,0.03923766816143498,"In the era of large models and large datasets, dataset
23"
INTRODUCTION,0.04035874439461883,"distillation has emerged as a crucial strategy to en-
24"
INTRODUCTION,0.04147982062780269,"hance training efficiency and make AI technologies
25"
INTRODUCTION,0.042600896860986545,"more accessible and affordable for the general public.
26"
INTRODUCTION,0.043721973094170405,"Previous approaches [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
27"
INTRODUCTION,0.04484304932735426,"primarily employ a batch-to-batch matching tech-
28"
INTRODUCTION,0.04596412556053812,"nique, where information like features, gradients,
29"
INTRODUCTION,0.04708520179372197,"and trajectories from a local original data batch are
30"
INTRODUCTION,0.04820627802690583,"used to supervise and train a corresponding batch
31"
INTRODUCTION,0.04932735426008968,"of generated data. This method‚Äôs strength lies in its
32"
INTRODUCTION,0.05044843049327354,"ability to capture fine-grained information from the
33"
INTRODUCTION,0.0515695067264574,"original data, as each batch‚Äôs supervision signals vary.
34"
INTRODUCTION,0.052690582959641255,"However, the downside is the necessity to repeatedly
35"
INTRODUCTION,0.053811659192825115,1represents ùëõImages Per Class for the distilled dataset.
INTRODUCTION,0.05493273542600897,Class Index
INTRODUCTION,0.05605381165919283,(1) SRe2L
INTRODUCTION,0.05717488789237668,(3) Ours [1√ó1 initialized]
INTRODUCTION,0.05829596412556054,(2) CDA
INTRODUCTION,0.05941704035874439,"Figure 2: Left: Intra-class semantic cosine similarity after a pretrained ResNet-18 model on ImageNet-
1K dataset, lower values are better. Right: Synthetic images from SRe2L, CDA and our DELT."
INTRODUCTION,0.06053811659192825,"input both original and generated data for each training iteration, which significantly increases memory
36"
INTRODUCTION,0.061659192825112105,"usage and computational costs. Recently, a new decoupled method [11, 12, 13] has been proposed
37"
INTRODUCTION,0.06278026905829596,"to separate the model training and data synthesis, also it leverages the batch-to-global matching to
38"
INTRODUCTION,0.06390134529147982,"avoid inputting original data during distilled data generation. This solution has demonstrated great
39"
INTRODUCTION,0.06502242152466367,"advantage on large-scale datasets like ImageNet-1K [11, 14] and ImageNet-21K [12]. However, as
40"
INTRODUCTION,0.06614349775784753,"shown in Fig. 2 right subfigure, a significant limitation of this method is its strategy of synthesizing
41"
INTRODUCTION,0.06726457399103139,"each data point individually, where supervision is repetitively applied across various synthetic images.
42"
INTRODUCTION,0.06838565022421525,"For instance, SRe2L[11] utilizes globally-counted layer-wise running means and variances from the
43"
INTRODUCTION,0.06950672645739911,"pre-trained model for supervising different intra-class image synthesis. This methodology results in a
44"
INTRODUCTION,0.07062780269058296,"pronounced lack of diversity within the same category of generated images.
45"
INTRODUCTION,0.07174887892376682,"To address this issue, previous studies such as G-VBSM [14] and RDED [15] have been conducted.
46"
INTRODUCTION,0.07286995515695067,"Specifically, G-VBSM [14] introduces a framework that utilizes a diverse set of local-match-global
47"
INTRODUCTION,0.07399103139013453,"matching signals derived from multiple backbones and statistical metrics, offering more precise and
48"
INTRODUCTION,0.07511210762331838,"effective matching than the singular model. However, as the diversity of matching models grows, the
49"
INTRODUCTION,0.07623318385650224,"overall complexity of the framework also increases, thus diminishing its conciseness. RDED [15]
50"
INTRODUCTION,0.0773542600896861,"crops each original image into multiple patches and ranks these using realism scores generated by an
51"
INTRODUCTION,0.07847533632286996,"observer model. Then it amalgamates every four chosen patches from previous stage into a single new
52"
INTRODUCTION,0.0795964125560538,"image, maintaining the resolution of the original images, and produce IPC-numbered distilled images
53"
INTRODUCTION,0.08071748878923767,"for each class. While RDED is effective for selecting and combining data, it does not enhance or
54"
INTRODUCTION,0.08183856502242152,"optimize the visual content within the distilled dataset. Thus, the diversity and richness of information
55"
INTRODUCTION,0.08295964125560538,"it encapsulates largely dependent on the distribution of the original dataset.
56"
INTRODUCTION,0.08408071748878924,"Our solution, termed the EarlyLate training scheme, is straightforward and also orthogonal to
57"
INTRODUCTION,0.08520179372197309,"these prior methods: by initializing each image in the same category at a different starting point for
58"
INTRODUCTION,0.08632286995515695,"optimization, we ensure that the final optimized results vary across images. We also use teacher-ranked
59"
INTRODUCTION,0.08744394618834081,"real image patches to initialize the synthetic images. This prevents some images from being short-
60"
INTRODUCTION,0.08856502242152467,"optimized and ensures they provide sufficient information. As shown in Fig. 1 of the computation
61"
INTRODUCTION,0.08968609865470852,"comparison, our approach not only enhances intra-class diversity but also significantly reduces the
62"
INTRODUCTION,0.09080717488789238,"computational load of the training process. Specifically, while conventional training requires ùëá
63"
INTRODUCTION,0.09192825112107623,"optimization iterations per image or batch, in our EarlyLate scheme, the first image undergoes ùëá1
64"
INTRODUCTION,0.0930493273542601,"iterations (where ùëá1 = ùëá). Subsequent batches are processed with progressively fewer iterations, such
65"
INTRODUCTION,0.09417040358744394,"as ùëá2 (ùëá2 = ùëá1 ‚àíRI2) for the next set, and so forth. The iterations for the final batch are reduced
66"
INTRODUCTION,0.0952914798206278,"to RI which is 1/ùëóof the standard count (where typically ùëó= 4 or 8), meaning the total number of
67"
INTRODUCTION,0.09641255605381166,"optimization iterations required is just about 2/3 of prior batch-to-global matching methods, such as
68"
INTRODUCTION,0.09753363228699552,"SRe2L and CDA. We further visualize the average cosine similarity between each sample of 50 IPCs
69"
INTRODUCTION,0.09865470852017937,"with the associated cluster centroid within the same class on ImageNet-1K, as shown in Fig. 2 left
70"
INTRODUCTION,0.09977578475336323,"subfigure, DELT shows significantly better diversity than other counterpart methods across all classes.
71"
INTRODUCTION,0.10089686098654709,"We perform extensive experiments on datasets of CIFAR-10, Tiny-ImageNet, ImageNet-1K and its
72"
INTRODUCTION,0.10201793721973094,"subsets. On ImageNet-1K, our proposed approach achieves 66.1% under IPC 50 with ResNet-101,
73"
INTRODUCTION,0.1031390134529148,"outperforming previous state-of-the-art RDED by 4.9%. On small-scale datasets of CIFAR-10, our
74"
INTRODUCTION,0.10426008968609865,"approach also obtains 2.5% and 19.2% improvement over RDED and SRe2L using ResNet-101.
75"
INTRODUCTION,0.10538116591928251,"Our main contributions in this work are as follows:
76"
INTRODUCTION,0.10650224215246637,"‚Ä¢ We propose a simple yet effective EarlyLate training scheme for dataset distillation to
77"
INTRODUCTION,0.10762331838565023,"enhance the intra-class diversity of synthetic images from batch-to-global matching.
78"
INTRODUCTION,0.10874439461883408,2RI is the number of round iterations and will be introduced in Sec. 4.3.
INTRODUCTION,0.10986547085201794,"‚Ä¢ We demonstrate empirically that the proposed method can generate optimized images at
79"
INTRODUCTION,0.1109865470852018,"different distances from their initializations, to enlarge informativeness among generations.
80"
INTRODUCTION,0.11210762331838565,"‚Ä¢ We conducted extensive experiments and ablations on various datasets across different scales
81"
INTRODUCTION,0.1132286995515695,"to prove the effectiveness of the proposed approach3.
82"
RELATED WORK,0.11434977578475336,"2
Related Work
83"
RELATED WORK,0.11547085201793722,"Dataset Distillation. Dataset distillation or condensation [1] focuses on creating a compact yet
84"
RELATED WORK,0.11659192825112108,"representative subset from a large original dataset. This enables more efficient model training while
85"
RELATED WORK,0.11771300448430494,"maintaining the ability to evaluate on the original test data distribution and achieve satisfactory
86"
RELATED WORK,0.11883408071748879,"performance. Previous works [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] mainly designed how to better match
87"
RELATED WORK,0.11995515695067265,"the distribution between original data and generated data in a batch-to-batch manner, such as the
88"
RELATED WORK,0.1210762331838565,"distribution of features [6], gradients [2], or the model weight trajectories [4, 8]. The primary
89"
RELATED WORK,0.12219730941704036,"optimization method used is bi-level optimization [16, 17], which involves optimizing model
90"
RELATED WORK,0.12331838565022421,"parameters and updating images simultaneously. For instance, using gradient matching, the process
91"
RELATED WORK,0.12443946188340807,"can be formulated as to minimize the gradient distance:
92"
RELATED WORK,0.12556053811659193,"min
S‚ààRùëÅ√óùëëùê∑(‚àáùúÉ‚Ñì(S; ùúÉ), ‚àáùúÉ‚Ñì(T; ùúÉ)) = ùê∑(S, T; ùúÉ)
(1)"
RELATED WORK,0.1266816143497758,"where the function ùê∑(¬∑, ¬∑) is defined as a distance metric such as MSE [18], ùúÉdenotes the model
93"
RELATED WORK,0.12780269058295965,"parameters, and ‚àáùúÉ‚Ñì(¬∑; ùúÉ) represents the gradient, utilizing either the original dataset T or its synthetic
94"
RELATED WORK,0.1289237668161435,"version S. ùëÅis the number of ùëë-dimensional synthetic data. During distillation, the synthetic dataset
95"
RELATED WORK,0.13004484304932734,"S and model ùúÉare updated alternatively,
96"
RELATED WORK,0.1311659192825112,"S ‚ÜêS ‚àíùúÜ‚àáSùê∑(S, T; ùúÉ),
ùúÉ‚ÜêùúÉ‚àíùúÇ‚àáùúÉ‚Ñì(ùúÉ; S),
(2)"
RELATED WORK,0.13228699551569506,"where ùúÜand ùúÇare learning rates designated for S and ùúÉ, respectively.
97"
RELATED WORK,0.13340807174887892,"Batch-to-global matching used in [11, 14, 12, 13] tracks the distribution of BN statistics derived from
98"
RELATED WORK,0.13452914798206278,"the original dataset for the local batch synthetic data, the formulation can be:
99"
RELATED WORK,0.13565022421524664,"min
S‚ààRùëÅ√óùëë(
‚àëÔ∏Å ùëô"
RELATED WORK,0.1367713004484305,"ùúáùëô(S) ‚àíBNRM
ùëô

2 +
‚àëÔ∏Å ùëô"
RELATED WORK,0.13789237668161436,"ùúé2
ùëô(S) ‚àíBNRV
ùëô

2)
(3)"
RELATED WORK,0.13901345291479822,"where ùëôis the index of BN layer, ùúáùëô(S) and ùúé2
ùëô(S) are mean and variance. BNRM
ùëô
and BNRV
ùëô
are
100"
RELATED WORK,0.14013452914798205,"running mean and running variance in the pre-trained model at ùëô-th layer, which are globally counted.
101"
RELATED WORK,0.1412556053811659,"Fig. 3 illustrates the difference of batch-to-batch and batch-to-global matching mechanisms, where ùëè
102"
RELATED WORK,0.14237668161434977,"represents a local batch in data T and S.
103"
RELATED WORK,0.14349775784753363,(1) Batch‚Äìto-Batch Matching
RELATED WORK,0.1446188340807175,(2) Batch‚Äìto-Global Matching
RELATED WORK,0.14573991031390135,"Global mean,"
RELATED WORK,0.1468609865470852,var statistics
RELATED WORK,0.14798206278026907,"Batch mean, var"
RELATED WORK,0.1491031390134529,statistics
RELATED WORK,0.15022421524663676,weight pretraining
RELATED WORK,0.15134529147982062,weight sharing
RELATED WORK,0.15246636771300448,"13
Gradient/Trajectory/Feature, etc."
RELATED WORK,0.15358744394618834,"Gradient/Trajectory/Feature, etc. ‚Ñí2 ‚Ñí2 ‚Ñí+, 03 03"
RELATED WORK,0.1547085201793722,"Original
dataset . . .1"
RELATED WORK,0.15582959641255606,! represents a local
RELATED WORK,0.15695067264573992,batch in # and $
RELATED WORK,0.15807174887892378,"Figure 3: Batch‚Äìto-batch vs. batch-to-global
matching in dataset distillation. ùúÉùëìindicates
weights are pretrained and frozen in this stage."
RELATED WORK,0.1591928251121076,"Moreover, for the recent advances of multi-stage
104"
RELATED WORK,0.16031390134529147,"dataset distillation methods, MDC [10] proposes to
105"
RELATED WORK,0.16143497757847533,"compress multiple condensation processes into a
106"
RELATED WORK,0.1625560538116592,"single one by including an adaptive subset loss on
107"
RELATED WORK,0.16367713004484305,"top of the basic condensation loss, so that to obtain
108"
RELATED WORK,0.1647982062780269,"datasets with multiple sizes. PDD [9] generates mul-
109"
RELATED WORK,0.16591928251121077,"tiple small batches of synthetic images, each batch is
110"
RELATED WORK,0.16704035874439463,"conditioned on the accumulated data from previous
111"
RELATED WORK,0.1681614349775785,"batches. Unlike PDD, our current synthetic batch
112"
RELATED WORK,0.16928251121076232,"is independent with different operation iterations
113"
RELATED WORK,0.17040358744394618,"and not relevant to any previous batches. D3 [19]
114"
RELATED WORK,0.17152466367713004,"partitions large datasets into smaller subtasks and
115"
RELATED WORK,0.1726457399103139,"employs locally trained experts to distill each subset
116"
RELATED WORK,0.17376681614349776,"into distributions. These distilled distributions from
117"
RELATED WORK,0.17488789237668162,"the subtasks demonstrate effective generalization
118"
RELATED WORK,0.17600896860986548,"when applied to the entire task.
119"
RELATED WORK,0.17713004484304934,"Initialization. Weight initialization [20, 21, 22, 23] is pivotal in training neural networks, significantly
120"
RELATED WORK,0.17825112107623317,"influencing their optimization process. Proper initialization is essential for ensuring model convergence
121"
RELATED WORK,0.17937219730941703,"and mitigating issues such as gradient vanishing. Recently, weight selection [24] introduces a strategy
122"
RELATED WORK,0.1804932735426009,"for initializing smaller models by selecting a subset of weights from a pretrained larger model. This
123"
RELATED WORK,0.18161434977578475,3Our synthetic images on ImageNet-1K are available anonymously at link.
RELATED WORK,0.1827354260089686,"Scheduler
+
+"
RELATED WORK,0.18385650224215247,"+
Concatenation node"
RELATED WORK,0.18497757847533633,Images Per Class whose rank
RELATED WORK,0.1860986547085202,"CDA Gradient updates for 
 iterations"
RELATED WORK,0.18721973094170405,Figure 4: The proposed DELT learning procedure via a multi-round EarlyLate scheme.
RELATED WORK,0.18834080717488788,"method facilitates the transfer of learned attributes from the pretrained weights, enhancing the smaller
124"
RELATED WORK,0.18946188340807174,"model‚Äôs performance. Weight subcloning [25] involves manipulating the pretrained model to derive a
125"
RELATED WORK,0.1905829596412556,"correspondingly scaled-down version with equivalent initialization. This involves two main steps:
126"
RELATED WORK,0.19170403587443946,"initially, it applies a neuron importance ranking to reduce the embedding dimension per layer within
127"
RELATED WORK,0.19282511210762332,"the pretrained model. Subsequently, it eliminates blocks from the transformer model to align with the
128"
RELATED WORK,0.19394618834080718,"layer count of the scaled-down network.
129"
RELATED WORK,0.19506726457399104,"This work focuses on data initialization for generation processes. Few studies have examined this
130"
RELATED WORK,0.1961883408071749,"angle. While, PCA-K [26] appears to be the most relevant. It employs an initialization method that
131"
RELATED WORK,0.19730941704035873,"involves drawing samples from a distribution that accurately mirrors and is easily sampled from the
132"
RELATED WORK,0.1984304932735426,"training distribution. During training, it is possible to retrieve some details from the original image
133"
RELATED WORK,0.19955156950672645,"using the initial noisy sample, which at best provides a blurred representation of the original image.
134"
OUR APPROACH,0.2006726457399103,"3
Our Approach
135"
OUR APPROACH,0.20179372197309417,"Preliminaries. The objective of a regular dataset distillation task is to generate a compact synthetic
136"
OUR APPROACH,0.20291479820627803,"dataset S = {( ÀÜùíô1, ÀÜùíö1) , . . . ,   ÀÜùíô|S|, ÀÜùíö|S|
} as a student dataset that captures a substantial amount of
137"
OUR APPROACH,0.2040358744394619,"the information from a larger labeled dataset T = {(ùíô1, ùíö1) , . . . ,  ùíô| T|, ùíö| T|
}, which serves as the
138"
OUR APPROACH,0.20515695067264575,"teacher dataset. Here, ÀÜùíörepresents the soft label for the synthetic sample ÀÜùíô, and the size of S is much
139"
OUR APPROACH,0.2062780269058296,"smaller than T, yet it retains the essential information of the original dataset T. The learning goal
140"
OUR APPROACH,0.20739910313901344,"using this distilled dataset is to train a post-validation model with parameters ùúΩ:
141"
OUR APPROACH,0.2085201793721973,"ùúΩS = arg min
ùúΩ
LS(ùúΩ),
(4) 142"
OUR APPROACH,0.20964125560538116,"LS(ùúΩ) = E( ÀÜùíô, ÀÜùíö)‚ààS

‚Ñì(ùúôùúΩS ( ÀÜùíô), ÀÜùíö; ùúΩ)

,
(5)
where ‚Ñìis a standard loss function such as soft cross-entropy and ùúôùúΩS represents the model.
143"
OUR APPROACH,0.21076233183856502,"The primary aim of dataset distillation is to produce synthetic data that ensures minimal performance
144"
OUR APPROACH,0.21188340807174888,"difference between models trained on the synthetic dataset S and those trained on the original dataset
145"
OUR APPROACH,0.21300448430493274,"T using validation data ùëâ. The optimization procedure for generating S is given by:
146"
OUR APPROACH,0.2141255605381166,"arg min
S,|S|"
OUR APPROACH,0.21524663677130046,"
sup
‚Ñì ùúôùúΩT (ùíôùë£ùëéùëô), ùíöùë£ùëéùëô
 ‚àí‚Ñì ùúôùúΩS (ùíôùë£ùëéùëô), ùíöùë£ùëéùëô
"
OUR APPROACH,0.2163677130044843,"(ùíôùë£ùëéùëô,ùíöùë£ùëéùëô)‚àºùëâ

.
(6)"
OUR APPROACH,0.21748878923766815,"where (ùíôùë£ùëéùëô, ùíöùë£ùëéùëô) are the sample and label pairs in the validation set of the real dataset T. The
147"
OUR APPROACH,0.218609865470852,"learning task then focuses on the <data, label> pairs within S, maintaining a balanced representation
148"
OUR APPROACH,0.21973094170403587,"of distilled data across each class.
149 2 3 5 1 4"
OUR APPROACH,0.22085201793721973,Teacher
OUR APPROACH,0.2219730941704036,Ranker 1 2 3 4 5 3 2 4
OUR APPROACH,0.22309417040358745,IPC initialization
OUR APPROACH,0.2242152466367713,"Figure 5: Selection criteria with a
teach ranker."
OUR APPROACH,0.22533632286995517,"Initialization. Previous dataset distillation methods [11, 14,
150"
OUR APPROACH,0.226457399103139,"12] on large-scale datasets like ImageNet-1K and 21K employ
151"
OUR APPROACH,0.22757847533632286,"Gaussian noise by default for data initialization in the synthesis
152"
OUR APPROACH,0.22869955156950672,"phase. However, Gaussian noise is random and lacks any
153"
OUR APPROACH,0.22982062780269058,"semantic information. Intuitively, using real images provide a
154"
OUR APPROACH,0.23094170403587444,"more meaningful and structured starting point, and this struc-
155"
OUR APPROACH,0.2320627802690583,"tured start can lead to quicker convergence during optimization
156"
OUR APPROACH,0.23318385650224216,"because the initial data already contains useful features and
157"
OUR APPROACH,0.23430493273542602,"patterns that are closer to the target distribution, which further
158"
OUR APPROACH,0.23542600896860988,"enhances realism, quality, and generalization of the synthesized images. As shown in Fig. 2 right
159"
OUR APPROACH,0.2365470852017937,"subfigure, our generated images exhibit both diversity and a high degree of realism in some cases.
160"
OUR APPROACH,0.23766816143497757,"Selection Criteria. Here, we introduce how to select real image patches to initialize the synthetic
161"
OUR APPROACH,0.23878923766816143,"images. In our final syntheses, a significant fraction of our data has been subject to limited optimization
162"
OUR APPROACH,0.2399103139013453,"iterations, making effective initialization crucial. A proper initialization also dramatically minimizes
163"
OUR APPROACH,0.24103139013452915,"the overall computational load required for the updating on data. Prior approach [15] has demonstrated
164"
OUR APPROACH,0.242152466367713,"that choosing representative data patches from the original dataset without training can yield favorable
165"
OUR APPROACH,0.24327354260089687,"performance without any additional training. Our observation, however, underscores that applying
166"
OUR APPROACH,0.24439461883408073,"iterative refinement to original patches can lead to markedly improved results. As illustrated in Fig. 1,
167"
OUR APPROACH,0.24551569506726456,"our selection criterion is based on a pretrained teacher model as a ranker, we calculate all patches‚Äô
168"
OUR APPROACH,0.24663677130044842,"probabilities and sort them as the initialization pool. Then, we choose lowest, medium, or highest
169"
OUR APPROACH,0.24775784753363228,"probability patches as the initialization for our optimization.
170"
OUR APPROACH,0.24887892376681614,"Diversity-driven IPC Concatenation Training. As shown in Fig. 4, to further emphasize diversity
171"
OUR APPROACH,0.25,"and avoid potential distribution bias from initialization, we optimize the initialized images starting
172"
OUR APPROACH,0.25112107623318386,"from different points. The motivation behind this design is that different data samples require varying
173"
OUR APPROACH,0.2522421524663677,"numbers of iterations to converge which is similar to the early stopping idea [27]. Importantly, as
174"
OUR APPROACH,0.2533632286995516,"images become easier to predict with more updates by class labels, training primarily on easy data
175"
OUR APPROACH,0.25448430493273544,"points can hinder model generalization. Therefore, our method enhances generalization by generating
176"
OUR APPROACH,0.2556053811659193,"data samples with varying difficulty levels, acting as a regularizer by limiting the optimization process
177"
OUR APPROACH,0.25672645739910316,"to a smaller volume of image pixel space. Previous work [28] studies how to perform early stopping
178"
OUR APPROACH,0.257847533632287,"training on different layers‚Äô weights of the model with progressive retraining to mitigate noisy labels.
179"
OUR APPROACH,0.2589686098654709,"We are pioneering to study how to leverage early-late training when optimizing data. Moreover, we
180"
OUR APPROACH,0.2600896860986547,"improve the efficiency of our approach by performing gradient updates in a single scan. Initially, we
181"
OUR APPROACH,0.26121076233183854,"conduct a single gradient loop, continually introducing new data for distillation by concatenating them
182"
OUR APPROACH,0.2623318385650224,"at different time stamps. Consequently, the ùëÄbatch receives the synthetic images of all preceding
183"
OUR APPROACH,0.26345291479820626,"batches, IPC0:ùëÄùëò‚àí1, as final generations. This process can be simplified as follows:
184"
OUR APPROACH,0.2645739910313901,"IPC0:ùëÄùëò‚àí1 = [ ÀÜùíô0, ÀÜùíô1, . . . , ÀÜùíôùëò‚àí1
|              {z              }"
OUR APPROACH,0.265695067264574,IPC0:ùëò‚àí1
OUR APPROACH,0.26681614349775784,", . . ."
OUR APPROACH,0.2679372197309417,|                    {z                    } ...
OUR APPROACH,0.26905829596412556,", ÀÜùíôùëÄùëò‚àí1"
OUR APPROACH,0.2701793721973094,|                               {z                               }
OUR APPROACH,0.2713004484304933,"IPC0:ùëÄùëò‚àí1 ]
(7)"
OUR APPROACH,0.27242152466367714,"where [ ÀÜùíô0, ÀÜùíô1, . . . , ÀÜùíôùëÄùëò‚àí1] refers to the concatenation of the generated image. ùëÄis the number
185"
OUR APPROACH,0.273542600896861,"of batches, ùëòis the number of generated images in each batch. We train these different batches at
186"
OUR APPROACH,0.27466367713004486,"different starting points, each batch goes through a completed learning phase, but the total number of
187"
OUR APPROACH,0.2757847533632287,"iterations varies. Then, the multiple IPCs of ÀÜùíôare concatenated into a simple batch. Because of its
188"
OUR APPROACH,0.2769058295964126,"early-late training property, we refer to this simple training scheme as EarlyLate training.
189"
OUR APPROACH,0.27802690582959644,"Training Procedure. As illustrated in Fig. 4, our learning procedure is extremely simple using an
190"
OUR APPROACH,0.27914798206278024,"incremental learning process: We split the total IPCs to be learned into multiple batches. The training
191"
OUR APPROACH,0.2802690582959641,"begins with the first batch. Following a predefined number of iterations, the second batch commences
192"
OUR APPROACH,0.28139013452914796,"its iterative training, and this process continues sequentially with subsequent batches. Batch-to-global
193"
OUR APPROACH,0.2825112107623318,"matching algorithm [12] of Eq. 3 has been utilized between each round.
194"
EXPERIMENTS,0.2836322869955157,"4
Experiments
195"
DATASETS AND RESULTS DETAILS,0.28475336322869954,"4.1
Datasets and Results Details
196"
DATASETS AND RESULTS DETAILS,0.2858744394618834,"We first run DELT on five standard benchmark tests including CIFAR-10 (10 classes) [29], Tiny-
197"
DATASETS AND RESULTS DETAILS,0.28699551569506726,"ImageNet (200 classes) [30], ImageNet-1K (1,000 classes) [31] and it variants of ImageNette (10
198"
DATASETS AND RESULTS DETAILS,0.2881165919282511,"classes) [32], and ImageNet-100 (100 classes) [33] with performances reported in Table 1 and Table 2.
199"
DATASETS AND RESULTS DETAILS,0.289237668161435,"The evaluation protocol is following prior works [15, 11]. We compare DELT to six baseline dataset
200"
DATASETS AND RESULTS DETAILS,0.29035874439461884,"distillation algorithms including Matching Training Trajectories (MTT) [4], Improved Distribution
201"
DATASETS AND RESULTS DETAILS,0.2914798206278027,"Matching (IDM) [34], TrajEctory Matching with Constant Memory (TESLA) [8], Squeeze-Recover-
202"
DATASETS AND RESULTS DETAILS,0.29260089686098656,"Relabel (SRe2L) [11], Difficulty-Aligned Trajectory-Matching (DATM) [35], Realistic-Diverse-
203"
DATASETS AND RESULTS DETAILS,0.2937219730941704,"Efficient Dataset Distillation (RDED) [15]. Following previous dataset distillation methods [2, 15,
204"
DATASETS AND RESULTS DETAILS,0.2948430493273543,"11], we use ConvNet [36], ResNet-18/ResNet-101 [37], EfficientNet-B0 [38], MobileNet-V2 [39],
205"
DATASETS AND RESULTS DETAILS,0.29596412556053814,"MnasNet1_3 [40], and RegNet-Y-8GF [41], as our backbone for training or post-validation. All our
206"
DATASETS AND RESULTS DETAILS,0.297085201793722,"experiments are conducted on 4 NVIDIA RTX 4090 GPUs.
207"
DATASETS AND RESULTS DETAILS,0.2982062780269058,"ResNet-18
ResNet-101
MobileNet-v2"
DATASETS AND RESULTS DETAILS,0.29932735426008966,"Dataset
IPC
SRe2L [11]
RDED [15]
Ours
SRe2L [11]
RDED [15]
Ours
Ours"
DATASETS AND RESULTS DETAILS,0.3004484304932735,"1
16.6 ¬± 0.9
22.9 ¬± 0.4
24.0 ¬± 0.8
13.7 ¬± 0.2
18.7 ¬± 0.1
20.4 ¬± 1.0 20.2 ¬± 0.4
CIFAR-10
10
29.3 ¬± 0.5
37.1 ¬± 0.3
43.0 ¬± 0.9
24.3 ¬± 0.6
33.7 ¬± 0.3
37.4 ¬± 1.2 29.3 ¬± 0.3
50
45.0 ¬± 0.7
62.1 ¬± 0.1
64.9 ¬± 0.9
34.9 ¬± 0.1
51.6 ¬± 0.4
54.1 ¬± 0.8 42.9 ¬± 2.2"
DATASETS AND RESULTS DETAILS,0.3015695067264574,"1
19.1 ¬± 1.1
35.8 ¬± 1.0
24.1 ¬± 1.8
15.8 ¬± 0.6
25.1 ¬± 2.7
19.4 ¬± 1.7
19.1 ¬± 1.0
ImageNette
10
29.4 ¬± 3.0
61.4 ¬± 0.4
66.0 ¬± 1.4
23.4 ¬± 0.8
54.0 ¬± 0.4
55.4 ¬± 6.2 64.7 ¬± 1.4
50
40.9 ¬± 0.3
80.4 ¬± 0.4
88.2 ¬± 1.2
36.5 ¬± 0.7
75.0 ¬± 1.2
83.3 ¬± 1.1 85.7 ¬± 0.4"
DATASETS AND RESULTS DETAILS,0.30269058295964124,"Tiny-ImageNet
1
2.62 ¬± 0.1
9.7 ¬± 0.4
9.3 ¬± 0.5
1.9 ¬± 0.1
3.8 ¬± 0.1
5.6 ¬± 1.0
3.5 ¬± 0.5
10
16.1 ¬± 0.2
41.9 ¬± 0.2
43.0 ¬± 0.1
14.6 ¬± 1.1
22.9 ¬± 3.3
42.8 ¬± 0.9 26.5 ¬± 0.5
50
41.1 ¬± 0.4
58.2 ¬± 0.1
55.7 ¬± 0.5
42.5 ¬± 0.2
41.2 ¬± 0.4
58.5 ¬± 0.3 51.3 ¬± 0.5"
DATASETS AND RESULTS DETAILS,0.3038116591928251,"ImageNet-100
10
9.5 ¬± 0.4
36.0 ¬± 0.3
28.2 ¬± 1.5
6.4 ¬± 0.1
33.9 ¬± 0.1
22.4 ¬± 3.3
15.8 ¬± 0.2
50
27.0 ¬± 0.4
61.6 ¬± 0.1
67.9 ¬± 0.6
25.7 ¬± 0.3
66.0 ¬± 0.6
70.8 ¬± 2.3
55.0 ¬± 1.8
100
-
74.5 ¬± 0.4
75.1 ¬± 0.2
-
73.5 ¬± 0.8
77.6 ¬± 1.8
76.7 ¬± 0.3"
DATASETS AND RESULTS DETAILS,0.30493273542600896,"ImageNet-1K
10
21.3 ¬± 0.6
42.0 ¬± 0.1
45.8 ¬± 0.1
30.9 ¬± 0.1
48.3 ¬± 1.0
48.5 ¬± 1.6
35.1 ¬± 0.5
50
46.8 ¬± 0.2
56.5 ¬± 0.1
59.2 ¬± 0.4
60.8 ¬± 0.5
61.2 ¬± 0.4
66.1 ¬± 0.5
56.2 ¬± 0.3
100
52.8 ¬± 0.3
59.8 ¬± 0.1
62.4 ¬± 0.2
62.8 ¬± 0.2
-
67.6 ¬± 0.3
58.9 ¬± 0.3
Table 1: Comparison with SOTA dataset distillation methods using relatively large-scale backbones
on five benchmarks across different scales. MobileNet-v2 is modified to match the low resolutions of
CIFAR-10 and Tiny-ImageNet following [42]. Due to the table space limitation, some other methods
that are weaker than RDED are not listed, such as CDA and G-VBSM. Since IPC 1 is not applicable
to use EarlyLate strategy and the single image in each class is optimized with a constant iteration."
DATASETS AND RESULTS DETAILS,0.3060538116591928,ConvNet
DATASETS AND RESULTS DETAILS,0.3071748878923767,"Dataset
IPC
MTT [4]
IDM [34]
TESLA [8]
DATM [35]
RDED [15]
Ours"
DATASETS AND RESULTS DETAILS,0.30829596412556054,"1
47.7 ¬± 0.9
-
-
-
33.8 ¬± 0.8
29.8 ¬± 1.4
ImageNette
10
63.0 ¬± 1.3
-
-
-
63.2 ¬± 0.7
51.7 ¬± 1.2
50
-
-
-
-
83.8 ¬± 0.2
84.5 ¬± 0.4"
DATASETS AND RESULTS DETAILS,0.3094170403587444,"Tiny-ImageNet
1
8.8 ¬± 0.3
10.1 ¬± 0.2
-
17.1 ¬± 0.3
12.0 ¬± 0.1
12.4 ¬± 0.8
10
23.2 ¬± 0.2
21.9 ¬± 0.3
-
31.1 ¬± 0.3
39.6 ¬± 0.1
40.0 ¬± 0.4
50
28.0 ¬± 0.3
27.7 ¬± 0.3
-
39.7 ¬± 0.3
47.6 ¬± 0.2
48.6 ¬± 0.2"
DATASETS AND RESULTS DETAILS,0.31053811659192826,"ImageNet-100
10
-
17.1 ¬± 0.6
-
-
29.6 ¬± 0.1
24.7 ¬± 1.5
50
-
26.3 ¬± 0.4
-
-
50.2 ¬± 0.2
51.9 ¬± 1.1
100
-
-
-
-
58.6 ¬± 0.4
61.5 ¬± 0.5"
DATASETS AND RESULTS DETAILS,0.3116591928251121,"ImageNet-1K
1
-
-
7.7 ¬± 0.2
-
6.4 ¬± 0.1
8.8 ¬± 0.5
10
-
-
17.8 ¬± 1.3
-
20.4 ¬± 0.1
31.3 ¬± 0.8
50
-
-
27.9 ¬± 1.2
-
38.4 ¬± 0.2
41.7 ¬± 0.1
Table 2: Comparison with SOTA dataset distillation methods using small-scale backbone architecture
on four benchmark datasets. Following [4, 34, 15], Conv-3 is used for CIFAR-10, Conv-4 for Tiny-
ImageNet and ImageNet-1K, Conv-5 for ImageNette, and Conv-6 for ImageNet-100 and ImageNet-1K.
Entries marked with ‚Äú-‚Äù are missing due to scalability issue."
DATASETS AND RESULTS DETAILS,0.312780269058296,"As shown in Table 1, our approach establishes the new state-of-the-art accuracy in 13 out of 15 of
208"
DATASETS AND RESULTS DETAILS,0.31390134529147984,"the configurations on five datasets from small-scale CIFAR-10 to large-scale ImageNet-1K using
209"
DATASETS AND RESULTS DETAILS,0.3150224215246637,"relatively large backbone architecture of ResNet-101, in many cases with significant margins of
210"
DATASETS AND RESULTS DETAILS,0.31614349775784756,"improvement. The results using small-scale architecture ConvNet are shown in Table 2, our approach
211"
DATASETS AND RESULTS DETAILS,0.3172645739910314,"also achieves the state-of-the-art accuracy in 8 out of 12 of the configurations on four datasets.
212"
CROSS-ARCHITECTURE GENERALIZATION,0.3183856502242152,"4.2
Cross-architecture generalization
213"
CROSS-ARCHITECTURE GENERALIZATION,0.3195067264573991,"An important characteristic of distilled datasets is their effectiveness in generalizing to novel training
214"
CROSS-ARCHITECTURE GENERALIZATION,0.32062780269058294,"architectures. In this context, we assess the transferability of DELT‚Äôs distilled datasets tailored for
215"
CROSS-ARCHITECTURE GENERALIZATION,0.3217488789237668,"ImageNet-1K with 10 images per class. Following previous studies [11, 15], we test our models
216"
CROSS-ARCHITECTURE GENERALIZATION,0.32286995515695066,"using five distinct architectures: ResNet-18 [37], MobileNet-V2 [39], MnasNet1_3 [40], EfficientNet-
217"
CROSS-ARCHITECTURE GENERALIZATION,0.3239910313901345,"B0 [38], and RegNet-Y-8GF [41]. As shown in Table 4, our proposed approach demonstrates
218"
CROSS-ARCHITECTURE GENERALIZATION,0.3251121076233184,"significant better performance than other competitive methods on all these architectures.
219"
ABLATION STUDY,0.32623318385650224,"4.3
Ablation Study
220"
ABLATION STUDY,0.3273542600896861,"Mosaic splicing pattern. Mosaic stitching method [43] in RDED selects four crops from the train set
221"
ABLATION STUDY,0.32847533632286996,"as the optimal hyper-parameter, and puts the contents of the four crops into a synthetic image that is
222"
ABLATION STUDY,0.3295964125560538,"directly used for post-validation. In this work, considering that we use different difficulty levels of
223"
ABLATION STUDY,0.3307174887892377,"3√ó3
4√ó4
5√ó5
6√ó6
Figure 6: Mosaic splicing patterns on ImageNet-1K using real image patches as the initialization.
In each block, the left column is the starting real image initialized samples and right is the final
optimized syntheses. From top to bottom are images generated by early training and late training."
ABLATION STUDY,0.33183856502242154,"selection for initialization, we examine different strategies of the Mosaic splicing patterns, including
224"
ABLATION STUDY,0.3329596412556054,"1 √ó 1, 2 √ó 2, 3 √ó 3, 4 √ó 4, and 5 √ó 5 patches, as illustrated in Fig. 11. The ablation results are shown in
225"
ABLATION STUDY,0.33408071748878926,"Table 3, it can be observed that 1 √ó 1 achieves the best accuracy.
226"
ABLATION STUDY,0.3352017937219731,"Initialization. We examine how different initialization strategies affect final performance, including:
227"
ABLATION STUDY,0.336322869955157,"choosing lowest probability crops, medium probability crops and highest probability crops. Our results
228"
ABLATION STUDY,0.3374439461883408,"are shown in Table 3. Overall, the performance gap between different strategies is not significant, and
229"
ABLATION STUDY,0.33856502242152464,"selecting the medium probability crops as the initialization achieves the best accuracy.
230"
ABLATION STUDY,0.3396860986547085,"Optimization iterations. We examine two types of optimization iterations: maximum iteration
231"
ABLATION STUDY,0.34080717488789236,"(MI) for the earliest batch training and round iteration (RI). MI presents the number of optimization
232"
ABLATION STUDY,0.3419282511210762,"iterations that the earliest batch goes through. RI represents the number of iterations used for each
233"
ABLATION STUDY,0.3430493273542601,"round in Fig. 4. It essentially indicates the iteration gap between the optimization of two adjacent
234"
ABLATION STUDY,0.34417040358744394,"batches. As shown in Table 3, we test MI values of 1K, 2K, and 4K, using 500 and 1K iterations for
235"
ABLATION STUDY,0.3452914798206278,"each RI. Note that when MI is set to 1K, it is not feasible to use 1K as RI. The results show that 4K
236"
ABLATION STUDY,0.34641255605381166,"(same as [11, 12]) MI and 500 RI achieves the best accuracy.
237"
ABLATION STUDY,0.3475336322869955,"Early-only vs. EarlyLate. Early-only is equivalent to using constant MI to optimize each image. The
238"
ABLATION STUDY,0.3486547085201794,"method will transform to baseline batch-to-global matching of CDA [12] + real image initialization.
239"
ABLATION STUDY,0.34977578475336324,"Our results in Table 3 clearly show that the EarlyLate training bring a significant improvement on
240"
ABLATION STUDY,0.3508968609865471,"final performance. More importantly, this strategy is the key factor in enhancing generation diversity.
241"
ABLATION STUDY,0.35201793721973096,"Real image stitching vs. Minimax diffusion vs. Ours. We further compare the performance of our
242"
ABLATION STUDY,0.3531390134529148,"approach with real image stitching [15] and diffusion generation [44]. The results are presented in
243"
ABLATION STUDY,0.3542600896860987,"Table 3d. While the first two methods produce more realistic images, each image contains limited
244"
ABLATION STUDY,0.35538116591928254,"information. In contrast, our method achieves the best final performance.
245"
COMPUTATIONAL ANALYSIS,0.35650224215246634,"4.4
Computational Analysis
246"
COMPUTATIONAL ANALYSIS,0.3576233183856502,"For image optimization-based methods like SRe2L and CDA, the total computational cost is calculated
247"
COMPUTATIONAL ANALYSIS,0.35874439461883406,"as ùëÅ√ó ùëá, where ùëÅis the MI. In our EarlyLate scheme, the first batch images undergo ùëá1 iterations
248"
COMPUTATIONAL ANALYSIS,0.3598654708520179,"(where ùëá1 = ùëá). Subsequent batches are processed with progressively fewer iterations, such as ùëá2
249"
COMPUTATIONAL ANALYSIS,0.3609865470852018,"(ùëá2 = ùëá1 ‚àíRI) for the next set, and so forth. The iterations for the final batch are reduced to RI which
250"
COMPUTATIONAL ANALYSIS,0.36210762331838564,"is 1/ùëóof the standard count (where ùëó= 4 or 8 in our ablation), the total number of our optimization
251"
COMPUTATIONAL ANALYSIS,0.3632286995515695,iterations required is ùëÅ√ó ùëá‚àíùëó( ùëó‚àí1)
COMPUTATIONAL ANALYSIS,0.36434977578475336,"2
RI, which is roughly 2/3 of prior batch-to-global matching
252"
COMPUTATIONAL ANALYSIS,0.3654708520179372,"methods. Our real time consumptions for data generation are shown in Table 5, note that the smaller
253"
COMPUTATIONAL ANALYSIS,0.3665919282511211,"the dataset like CIFAR, the more time is spent on loading and processing the data, rather than training.
254"
VISUALIZATION OF DELT,0.36771300448430494,"4.5
Visualization of DELT
255"
VISUALIZATION OF DELT,0.3688340807174888,"Fig. 7 illustrates a comprehensive visual comparison between randomly selected synthetic images from
256"
VISUALIZATION OF DELT,0.36995515695067266,"our distilled dataset and those from the real image patches [15], MinimaxDiffusion [44], MTT [4],
257"
VISUALIZATION OF DELT,0.3710762331838565,"IDC [45], SRe2L [11], SCDD [46], CDA [12] and G-VBSM [14] distilled data. It can be observed that
258"
VISUALIZATION OF DELT,0.3721973094170404,Table 3: Ablation experiments on various aspects of our framework with ResNet-18 on ImageNet-1K.
VISUALIZATION OF DELT,0.37331838565022424,"# Patches
Top 1 acc"
VISUALIZATION OF DELT,0.3744394618834081,"1 √ó 1
57.57
2 √ó 2
56.92
3 √ó 3
56.62
4 √ó 4
56.71
5 √ó 5
56.51"
VISUALIZATION OF DELT,0.3755605381165919,"(a) Number of patches. Ablation on initializing
different numbers of scoring patches. Results
are from ResNet-18 on ImageNet-1K for 500
iterations to synthesize 50 IPCs."
VISUALIZATION OF DELT,0.37668161434977576,"Selection criteria
Top 1 acc"
VISUALIZATION OF DELT,0.3778026905829596,"Lowest probability
57.55
Medium probability
57.67
Highest probability
57.03"
VISUALIZATION OF DELT,0.3789237668161435,"(b) Selection criteria. Initializing 1 √ó 1 images
selected according to teacher model‚Äôs probability"
VISUALIZATION OF DELT,0.38004484304932734,"Iterations
Round Iterations
500
1K"
K,0.3811659192825112,"1K
44.87
n/a
2K
45.61
44.40
4K
46.42
44.66"
K,0.38228699551569506,"(c) Round Iterations. Top-1 acc. of our method for IPC
10 using different round iterations with ResNet-18."
K,0.3834080717488789,"Dataset
CDA [12] + Our init.
Ours"
K,0.3845291479820628,"ImageNet-1K
43.5
45.8
Tiny-ImageNet
42.2
43.0
CIFAR-10
39.4
43.0
(d) Ablation on init. and EarlyLate under IPC 10."
K,0.38565022421524664,"IPC
RDED [15]
MinimaxDiffusion [44]
Ours"
K,0.3867713004484305,"10
42.0
44.3
45.8
50
56.5
58.6
59.2
(e) Comparison with real and diffusion generated data."
K,0.38789237668161436,Table 4: Cross-architecture generalization. Results are evaluated on IPC 10.
K,0.3890134529147982,"Recover \Validation
ResNet-18
EfficientNet-B0
MobileNet-V2
MnasNet1_3
RegNet-Y-8GF"
K,0.3901345291479821,ResNet-18
K,0.39125560538116594,"SRe2L [11]
41.9
41.9
33.1
39.3
51.5
CDA [12]
42.2
43.9
34.2
39.7
52.9
G-VBSM [14]
41.4
42.6
33.5
40.1
52.2
RDED [15]
42.3
42.8
34.4
40.0
54.8
Ours
46.4(+4.1)
47.1(+4.3)
36.1(+1.7)
40.7(+0.7)
57.5(+2.7)"
K,0.3923766816143498,"Table 5: Actual computational consumption and analysis (hours under IPC 50) in data synthesis
with image optimization-based methods on a single NVIDIA 4090 GPU. ‚ÄúRI‚Äù represents round
iterations. A total 4K iterations are used for all methods and datasets to ensure fair comparisons."
K,0.39349775784753366,Dataset (hours)
K,0.39461883408071746,"Method
ImageNet-1K
Tiny-ImageNet
CIFAR-10"
K,0.3957399103139013,"G-VBSM [14]
114.1
5.5
0.195
SRe2L [11]
29.0
5.0
0.084
CDA [12]
29.0
5.0
0.084
Ours (RI = 500)
17.6(‚Üì39.3%)
3.4(‚Üì32.0%)
0.083(‚Üì1.1%)
Ours (RI = 1K)
18.8(‚Üì35.2%)
3.6(‚Üì28.0%)
0.084(‚Üì0.0%)"
K,0.3968609865470852,"the images generated by each method have their own characteristics. MinimaxDiffusion leverages the
259"
K,0.39798206278026904,"diffusion model to synthesize images which is close to the real ones. However, as in our above ablation,
260"
K,0.3991031390134529,"both real and diffusion-generated data are inferior to ours. MTT results show noticeable artifacts
261"
K,0.40022421524663676,"and distortions, the objects in all images are located in the middle of the generations, the diversity is
262"
K,0.4013452914798206,"limited. IDC results also show distorted and less recognizable dog images, but diversity is increased.
263"
K,0.4024663677130045,"SRe2L exhibits some dog features but with significant distortions and similar simple background.
264"
K,0.40358744394618834,"SCDD shows more recognizable dog features but still the color is simple and monochromatic, the
265"
K,0.4047085201793722,"same situation happens in CDA. G-VBSM shows more colorful patterns, possibly due to recovery
266"
K,0.40582959641255606,"from multiple different networks, but all generations are in the same pattern and the diversity is
267"
K,0.4069506726457399,"not large. Our approach‚Äôs synthetic images exhibit a higher degree of diversity, including both
268"
K,0.4080717488789238,"compressed distorted images from long-optimized initializations and clear, recognizable dog images
269"
K,0.40919282511210764,"from short-optimized initializations, a unique capability not present in other methods.
270"
K,0.4103139013452915,"4.6
Application I: Data-free Network Pruning
271"
K,0.41143497757847536,"Our distilled dataset acts as a multifunctional training tool and boosts the adaptability for diverse
272"
K,0.4125560538116592,"downstream applications. We validate its utility in the scenario of data-free network pruning [47].
273"
K,0.413677130044843,"Table 6 shows the applicability of our dataset in this task when pruning 50% weights, where it
274"
K,0.4147982062780269,"significantly surpasses previous methods such as SRe2L and RDED under IPC 10 and 50.
275"
K,0.41591928251121074,"Ours
G-VBSM
CDA
SCDD
SRe2L
MTT
IDC
Real
M-Diffusion
Figure 7: Distilled dataset visualization compared with other image optimization-based methods."
K,0.4170403587443946,Table 6: Accuracy of data-free network pruning using slimming [48] on VGG11-BN [49].
K,0.41816143497757846,"SRe2L [11]
RDED [15]
Ours"
K,0.4192825112107623,"IPC 10
12.5
13.2
17.9(+4.7)
IPC 50
31.7
42.8
44.8(+2.0)"
K,0.4204035874439462,"4.7
Application II: Continual Learning
276"
K,0.42152466367713004,"100
200
300
400
500
600
700
800
900
Class Number 10 15 20 25 30 35 40"
K,0.4226457399103139,Top-1 Accuracy (%)
K,0.42376681614349776,"SRe2L
G-VBSM
Ours"
K,0.4248878923766816,Figure 8: Continual learning results.
K,0.4260089686098655,"We examine the effectiveness of DELT generated images
277"
K,0.42713004484304934,"in the continual learning scenario. Following the setup in
278"
K,0.4282511210762332,"prior studies [11, 6], we perform 100-step class-incremental
279"
K,0.42937219730941706,"experiments on ImageNet-1K, comparing our results with
280"
K,0.4304932735426009,"the baselines G-VBSM and SRe2L. As shown in Fig. 8,
281"
K,0.4316143497757848,"our DELT distilled dataset significantly outperforms G-
282"
K,0.4327354260089686,"VBSM, with an average improvement of about 10% in
283"
K,0.43385650224215244,"100-step class-incremental learning task. This highlights
284"
K,0.4349775784753363,"the significant benefits of deploying DELT, particularly in
285"
K,0.43609865470852016,"mitigating the challenges of continual learning.
286"
CONCLUSION,0.437219730941704,"5
Conclusion
287"
CONCLUSION,0.4383408071748879,"We have introduced a new training strategy, EarlyLate, to improve image diversity in batch-to-global
288"
CONCLUSION,0.43946188340807174,"matching scenarios for dataset distillation. The proposed approach organizes predefined IPC samples
289"
CONCLUSION,0.4405829596412556,"into smaller, manageable subtasks and utilizes local optimizations. This strategy helps in refining
290"
CONCLUSION,0.44170403587443946,"each subset into distributions characteristic of different phases, thereby mitigating the homogeneity
291"
CONCLUSION,0.4428251121076233,"typically caused by a singular optimization process. The images refined through this method exhibit
292"
CONCLUSION,0.4439461883408072,"robust generalization across the entire task. We have extensively evaluated this approach on CIFAR-10
293"
CONCLUSION,0.44506726457399104,"and 100, Tiny-ImageNet, ImageNet-1K, and its variants. Our empirical findings indicate that our
294"
CONCLUSION,0.4461883408071749,"approach significantly outperforms prior state-of-the-art methods across various IPC configurations.
295"
CONCLUSION,0.44730941704035876,"Limitations. Our method effectively avoids the issue of insufficient data diversity generated by
296"
CONCLUSION,0.4484304932735426,"batch-to-global methods and reduces the computational cost of the generation process. However,
297"
CONCLUSION,0.4495515695067265,"there is still a performance gap when training the model on our generated data compared to training
298"
CONCLUSION,0.45067264573991034,"on the original dataset. Additionally, our short-optimized data exhibits similar semantic information
299"
CONCLUSION,0.4517937219730942,"to the original images, which may potentially leak the privacy of the original dataset.
300"
REFERENCES,0.452914798206278,"References
301"
REFERENCES,0.45403587443946186,"[1] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint
302"
REFERENCES,0.4551569506726457,"arXiv:1811.10959, 2018.
303"
REFERENCES,0.4562780269058296,"[2] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. arXiv
304"
REFERENCES,0.45739910313901344,"preprint arXiv:2006.05929, 2020.
305"
REFERENCES,0.4585201793721973,"[3] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression.
306"
REFERENCES,0.45964125560538116,"Advances in Neural Information Processing Systems, 35:9813‚Äì9827, 2022.
307"
REFERENCES,0.460762331838565,"[4] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset
308"
REFERENCES,0.4618834080717489,"distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer
309"
REFERENCES,0.46300448430493274,"Vision and Pattern Recognition, pages 4750‚Äì4759, 2022.
310"
REFERENCES,0.4641255605381166,"[5] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon. Dataset condensation
311"
REFERENCES,0.46524663677130046,"with contrastive signals. In International Conference on Machine Learning, pages 12352‚Äì12364. PMLR,
312"
REFERENCES,0.4663677130044843,"2022.
313"
REFERENCES,0.4674887892376682,"[6] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In IEEE/CVF Winter
314"
REFERENCES,0.46860986547085204,"Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023,
315"
REFERENCES,0.4697309417040359,"2023.
316"
REFERENCES,0.47085201793721976,"[7] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xinchao Wang. Dataset distillation via factorization.
317"
REFERENCES,0.47197309417040356,"Advances in Neural Information Processing Systems, 35:1100‚Äì1113, 2022.
318"
REFERENCES,0.4730941704035874,"[8] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-1k with
319"
REFERENCES,0.4742152466367713,"constant memory. In International Conference on Machine Learning, pages 6565‚Äì6590. PMLR, 2023.
320"
REFERENCES,0.47533632286995514,"[9] Xuxi Chen, Yu Yang, Zhangyang Wang, and Baharan Mirzasoleiman. Data distillation can be like
321"
REFERENCES,0.476457399103139,"vodka: Distilling more times for better quality. In The Twelfth International Conference on Learning
322"
REFERENCES,0.47757847533632286,"Representations, 2024.
323"
REFERENCES,0.4786995515695067,"[10] Yang He, Lingao Xiao, Joey Tianyi Zhou, and Ivor Tsang. Multisize dataset condensation. ICLR, 2024.
324"
REFERENCES,0.4798206278026906,"[11] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet
325"
REFERENCES,0.48094170403587444,"scale from a new perspective. In NeurIPS, 2023.
326"
REFERENCES,0.4820627802690583,"[12] Zeyuan Yin and Zhiqiang Shen. Dataset distillation in large data era. arXiv preprint arXiv:2311.18838,
327"
REFERENCES,0.48318385650224216,"2023.
328"
REFERENCES,0.484304932735426,"[13] Haoyang Liu, Tiancheng Xing, Luwei Li, Vibhu Dalal, Jingrui He, and Haohan Wang. Dataset distillation
329"
REFERENCES,0.4854260089686099,"via the wasserstein metric. arXiv preprint arXiv:2311.18531, 2023.
330"
REFERENCES,0.48654708520179374,"[14] Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen. Generalized large-scale data
331"
REFERENCES,0.4876681614349776,"condensation via various backbone and statistical matching. In CVPR, 2024.
332"
REFERENCES,0.48878923766816146,"[15] Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficient
333"
REFERENCES,0.4899103139013453,"dataset distillation paradigm. In CVPR, 2024.
334"
REFERENCES,0.4910313901345291,"[16] Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization
335"
REFERENCES,0.492152466367713,"for learning and vision from a unified perspective: A survey and beyond. IEEE Transactions on Pattern
336"
REFERENCES,0.49327354260089684,"Analysis and Machine Intelligence, 44(12):10045‚Äì10067, 2021.
337"
REFERENCES,0.4943946188340807,"[17] Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, and Sƒ≥ia Liu. An
338"
REFERENCES,0.49551569506726456,"introduction to bi-level optimization: Foundations and applications in signal processing and machine
339"
REFERENCES,0.4966367713004484,"learning. arXiv preprint arXiv:2308.00788, 2023.
340"
REFERENCES,0.4977578475336323,"[18] Zhou Wang and Alan C Bovik. Mean squared error: Love it or leave it? a new look at signal fidelity
341"
REFERENCES,0.49887892376681614,"measures. IEEE signal processing magazine, 26(1):98‚Äì117, 2009.
342"
REFERENCES,0.5,"[19] Tian Qin, Zhiwei Deng, and David Alvarez-Melis.
Distributional dataset distillation with subtask
343"
REFERENCES,0.5011210762331838,"decomposition. arXiv preprint arXiv:2403.00999, 2024.
344"
REFERENCES,0.5022421524663677,"[20] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
345"
REFERENCES,0.5033632286995515,"networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics,
346"
REFERENCES,0.5044843049327354,"pages 249‚Äì256. JMLR Workshop and Conference Proceedings, 2010.
347"
REFERENCES,0.5056053811659192,"[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
348"
REFERENCES,0.5067264573991032,"human-level performance on imagenet classification. In Proceedings of the IEEE international conference
349"
REFERENCES,0.507847533632287,"on computer vision, pages 1026‚Äì1034, 2015.
350"
REFERENCES,0.5089686098654709,"[22] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.
351"
REFERENCES,0.5100896860986547,"[23] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selfie: Self-supervised pretraining for image
352"
REFERENCES,0.5112107623318386,"embedding. arXiv preprint arXiv:1906.02940, 2019.
353"
REFERENCES,0.5123318385650224,"[24] Zhiqiu Xu, Yanjie Chen, Kirill Vishniakov, Yida Yin, Zhiqiang Shen, Trevor Darrell, Lingjie Liu, and
354"
REFERENCES,0.5134529147982063,"Zhuang Liu. Initializing models with larger ones. In The Twelfth International Conference on Learning
355"
REFERENCES,0.5145739910313901,"Representations, 2024.
356"
REFERENCES,0.515695067264574,"[25] Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash Faghri, Devang
357"
REFERENCES,0.5168161434977578,"Naik, Oncel Tuzel, and Mohammad Rastegari. Weight subcloning: direct initialization of transformers
358"
REFERENCES,0.5179372197309418,"using larger pretrained ones. arXiv preprint arXiv:2312.09299, 2023.
359"
REFERENCES,0.5190582959641256,"[26] Jeffrey Zhang, Shao-Yu Chang, Kedan Li, and David Forsyth. Preserving image properties through
360"
REFERENCES,0.5201793721973094,"initializations in diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of
361"
REFERENCES,0.5213004484304933,"Computer Vision, pages 5242‚Äì5250, 2024.
362"
REFERENCES,0.5224215246636771,"[27] Lutz Prechelt. Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55‚Äì69. Springer,
363"
REFERENCES,0.523542600896861,"2002.
364"
REFERENCES,0.5246636771300448,"[28] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu.
365"
REFERENCES,0.5257847533632287,"Understanding and improving early stopping for learning with noisy labels. Advances in Neural Information
366"
REFERENCES,0.5269058295964125,"Processing Systems, 34:24392‚Äì24403, 2021.
367"
REFERENCES,0.5280269058295964,"[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
368"
REFERENCES,0.5291479820627802,"[30] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.
369"
REFERENCES,0.5302690582959642,"[31] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
370"
REFERENCES,0.531390134529148,"image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255.
371"
REFERENCES,0.5325112107623319,"Ieee, 2009.
372"
REFERENCES,0.5336322869955157,"[32] Fastai. Fastai/imagenette: A smaller subset of 10 easily classified classes from imagenet, and a little more
373"
REFERENCES,0.5347533632286996,"french.
374"
REFERENCES,0.5358744394618834,"[33] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision‚ÄìECCV
375"
REFERENCES,0.5369955156950673,"2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XI 16, pages
376"
REFERENCES,0.5381165919282511,"776‚Äì794. Springer, 2020.
377"
REFERENCES,0.5392376681614349,"[34] Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Improved distribution matching for dataset
378"
REFERENCES,0.5403587443946188,"condensation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
379"
REFERENCES,0.5414798206278026,"pages 7856‚Äì7865, 2023.
380"
REFERENCES,0.5426008968609866,"[35] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards lossless
381"
REFERENCES,0.5437219730941704,"dataset distillation via difficulty-aligned trajectory matching. In The Twelfth International Conference on
382"
REFERENCES,0.5448430493273543,"Learning Representations, 2024.
383"
REFERENCES,0.5459641255605381,"[36] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings
384"
REFERENCES,0.547085201793722,"of the IEEE conference on computer vision and pattern recognition, pages 4367‚Äì4375, 2018.
385"
REFERENCES,0.5482062780269058,"[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
386"
REFERENCES,0.5493273542600897,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì778, 2016.
387"
REFERENCES,0.5504484304932735,"[38] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In
388"
REFERENCES,0.5515695067264574,"International conference on machine learning, pages 6105‚Äì6114. PMLR, 2019.
389"
REFERENCES,0.5526905829596412,"[39] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
390"
REFERENCES,0.5538116591928252,"Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and
391"
REFERENCES,0.554932735426009,"pattern recognition, pages 4510‚Äì4520, 2018.
392"
REFERENCES,0.5560538116591929,"[40] Mingxing Tan, Bo Chen, Ruoming Pang, Vƒ≥ay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V
393"
REFERENCES,0.5571748878923767,"Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF
394"
REFERENCES,0.5582959641255605,"conference on computer vision and pattern recognition, pages 2820‚Äì2828, 2019.
395"
REFERENCES,0.5594170403587444,"[41] Ilƒ≥a Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll√°r. Designing network
396"
REFERENCES,0.5605381165919282,"design spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
397"
REFERENCES,0.5616591928251121,"pages 10428‚Äì10436, 2020.
398"
REFERENCES,0.5627802690582959,"[42] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In
399"
REFERENCES,0.5639013452914798,"Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11953‚Äì11962,
400"
REFERENCES,0.5650224215246636,"2022.
401"
REFERENCES,0.5661434977578476,"[43] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy
402"
REFERENCES,0.5672645739910314,"of object detection. arXiv preprint arXiv:2004.10934, 2020.
403"
REFERENCES,0.5683856502242153,"[44] Jianyang Gu, Saeed Vahidian, Vyacheslav Kungurtsev, Haonan Wang, Wei Jiang, Yang You, and Yiran
404"
REFERENCES,0.5695067264573991,"Chen. Efficient dataset distillation via minimax diffusion. In CVPR, 2024.
405"
REFERENCES,0.570627802690583,"[45] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo
406"
REFERENCES,0.5717488789237668,"Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In Proceedings
407"
REFERENCES,0.5728699551569507,"of the 39th International Conference on Machine Learning, 2022.
408"
REFERENCES,0.5739910313901345,"[46] Muxin Zhou, Zeyuan Yin, Shitong Shao, and Zhiqiang Shen. Self-supervised dataset distillation: A good
409"
REFERENCES,0.5751121076233184,"compression is all you need. arXiv preprint arXiv:2404.07976, 2024.
410"
REFERENCES,0.5762331838565022,"[47] Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv
411"
REFERENCES,0.577354260089686,"preprint arXiv:1507.06149, 2015.
412"
REFERENCES,0.57847533632287,"[48] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning
413"
REFERENCES,0.5795964125560538,"efficient convolutional networks through network slimming. In Proceedings of the IEEE international
414"
REFERENCES,0.5807174887892377,"conference on computer vision, pages 2736‚Äì2744, 2017.
415"
REFERENCES,0.5818385650224215,"[49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
416"
REFERENCES,0.5829596412556054,"recognition. arXiv preprint arXiv:1409.1556, 2014.
417"
REFERENCES,0.5840807174887892,"[50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
418"
REFERENCES,0.5852017937219731,"Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
419"
REFERENCES,0.5863228699551569,"learning library. Advances in neural information processing systems, 32, 2019.
420"
REFERENCES,0.5874439461883408,"[51] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data
421"
REFERENCES,0.5885650224215246,"augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision
422"
REFERENCES,0.5896860986547086,"and pattern recognition workshops, pages 702‚Äì703, 2020.
423"
REFERENCES,0.5908071748878924,"Appendix
424"
REFERENCES,0.5919282511210763,"A
Broader Impacts
425"
REFERENCES,0.5930493273542601,"Our dataset distillation framework can significantly reduce the computational resources required for
426"
REFERENCES,0.594170403587444,"training machine learning models. This leads to lower energy consumption and cost, making AI
427"
REFERENCES,0.5952914798206278,"more accessible and sustainable. By generating smaller, more manageable datasets, researchers and
428"
REFERENCES,0.5964125560538116,"developers can iterate and experiment more quickly, accelerating the pace of innovation in various
429"
REFERENCES,0.5975336322869955,"AI applications. However, condensed datasets might inadvertently amplify biases present in the
430"
REFERENCES,0.5986547085201793,"original data. If the distillation process does not adequately address bias, it could lead to unfair
431"
REFERENCES,0.5997757847533632,"or discriminatory AI systems. Also, simplifying datasets may lead to a loss of important nuances
432"
REFERENCES,0.600896860986547,"and context, potentially degrading the performance of models in real-world applications where such
433"
REFERENCES,0.602017937219731,"details are crucial. Moreover, the models may overfit to condensed data, indicating that models trained
434"
REFERENCES,0.6031390134529148,"on distilled datasets might perform well on the condensed data but poorly on more diverse real-world
435"
REFERENCES,0.6042600896860987,"data, limiting their generalizability and robustness.
436"
REFERENCES,0.6053811659192825,"B
Training Details
437"
REFERENCES,0.6065022421524664,Table 7: Hyper-parameter settings.
REFERENCES,0.6076233183856502,(a) Validation settings
REFERENCES,0.6087443946188341,"config
value"
REFERENCES,0.6098654708520179,"optimizer
AdamW"
REFERENCES,0.6109865470852018,"base learning rate
0.001 (all)
0.0025 (MobileNet-v2)
weight decay
0.01"
REFERENCES,0.6121076233183856,batch size
REFERENCES,0.6132286995515696,"100 (IPC50)
50 (IPC10)
10 (IPC1)
learning rate schedule
cosine decay
training epoch
300"
REFERENCES,0.6143497757847534,augmentation
REFERENCES,0.6154708520179372,"RandAugment
RandomResizedCrop
RandomHorizontalFlip"
REFERENCES,0.6165919282511211,(b) Recovery settings
REFERENCES,0.6177130044843049,"config
value"
REFERENCES,0.6188340807174888,"ùõºBN
0.01
optimizer
Adam
base learning rate
0.25
momentum
ùõΩ1, ùõΩ2 = 0.5, 0.9
batch size
100
learning rate schedule
cosine decay
recovery iteration
4,000
round iteration
500 [IPC 10, 50, 100]
initialization
top medium
augmentation
RandomResizedCrop"
REFERENCES,0.6199551569506726,(c) Dataset-specific settings in recovery
REFERENCES,0.6210762331838565,"config
CIFAR10
Tiny-ImageNet
ImageNette
ImageNet-100
ImageNet-1K"
REFERENCES,0.6221973094170403,"RandAugment (m)
5
4
6
6
6
RandAugment (n)
4
3
2
2
2
RandAugment (mstd)
1.0
1.0
1.0
1.0
1.0"
REFERENCES,0.6233183856502242,IPC1 Recovery Iterations
REFERENCES,0.624439461883408,"2K (R18)
500 (R18)
1K (R18)
-
3K (Conv4)
3K (R101)
500 (R101)
1K (R101)
-
-
2K (MobileNet)
500 (MobileNet)
2K (MobileNet)
-
-
-
1K (Conv4)
4K (Conv5)
-
-"
REFERENCES,0.625560538116592,"For reproducibility, we provide all our hyper-parameter settings used in our experiments in Table 7,
438"
REFERENCES,0.6266816143497758,"we outline such details below.
439"
REFERENCES,0.6278026905829597,"Squeezing and Pre-trained models. Following the previous works [11, 12, 15], we use the official
440"
REFERENCES,0.6289237668161435,"PyTorch [50] pre-trained ResNet-18 model for ImageNet-1K, and we use the same official Torchvision
441"
REFERENCES,0.6300448430493274,"[50] code to produce our pre-trained models, ResNet-18 and ConvNet, for the other datasets.
442"
REFERENCES,0.6311659192825112,"Ranking. A crucial part of our method is initialization, we simply use ResNet-18 pre-trained models
443"
REFERENCES,0.6322869955156951,"to rank and select the top-medium images as initialization for all our datasets, except for ImageNet-100
444"
REFERENCES,0.6334080717488789,"where we simply extracted the top-medium images based on the rankings of the original ImageNet-1K.
445"
REFERENCES,0.6345291479820628,"Recovery. For our synthesis, we provide the details of the general hyper-parameters used for different
446"
REFERENCES,0.6356502242152466,"datasets, including ImageNet-1K, ImageNet-100, ImageNette, Tiny-ImageNet, and CIFAR10, in
447"
REFERENCES,0.6367713004484304,"Table 7b. Because synthesizing a single image per class, i.e., IPC 1, is quite special as we cannot use
448"
REFERENCES,0.6378923766816144,Figure 9: Synthetic image visualizations on Tiny-ImageNet generated by our DELT.
REFERENCES,0.6390134529147982,"rounds, we apply different numbers of iterations based on both the dataset scale and the validation
449"
REFERENCES,0.6401345291479821,"teacher model as outlined in Table 7c.
450"
REFERENCES,0.6412556053811659,"Validation. This includes both the soft-label generation, Relabel in SRe2L, and evaluation, or
451"
REFERENCES,0.6423766816143498,"post-training. We outline such details in Table 7a. We use timm‚Äôs version of RandAugment [51] with
452"
REFERENCES,0.6434977578475336,"different settings depending on the synthesized dataset being validated as outlined in Table 7c.
453"
REFERENCES,0.6446188340807175,"C
More Visualizations
454"
REFERENCES,0.6457399103139013,"We provide more visualizations on synthetic Tiny-ImageNet, ImageNette and CIFAR-10 datasets. In
455"
REFERENCES,0.6468609865470852,"each figure, each column represents a different class, with images progressing from long optimization
456"
REFERENCES,0.647982062780269,"at the top to short optimization at the bottom.
457"
REFERENCES,0.649103139013453,Figure 10: Synthetic image visualizations on ImageNette generated by our DELT.
REFERENCES,0.6502242152466368,Figure 11: Synthetic image visualizations on CIFAR-10 generated by our DELT.
REFERENCES,0.6513452914798207,"NeurIPS Paper Checklist
458"
CLAIMS,0.6524663677130045,"1. Claims
459"
CLAIMS,0.6535874439461884,"Question: Do the main claims made in the abstract and introduction accurately reflect the
460"
CLAIMS,0.6547085201793722,"paper‚Äôs contributions and scope?
461"
CLAIMS,0.655829596412556,"Answer: [Yes]
462"
CLAIMS,0.6569506726457399,"Justification: We have clearly stated the contributions and scope of this paper.
463"
CLAIMS,0.6580717488789237,"Guidelines:
464"
CLAIMS,0.6591928251121076,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims made
465"
CLAIMS,0.6603139013452914,"in the paper.
466"
CLAIMS,0.6614349775784754,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
467"
CLAIMS,0.6625560538116592,"contributions made in the paper and important assumptions and limitations. A No or
468"
CLAIMS,0.6636771300448431,"NA answer to this question will not be perceived well by the reviewers.
469"
CLAIMS,0.6647982062780269,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
470"
CLAIMS,0.6659192825112108,"much the results can be expected to generalize to other settings.
471"
CLAIMS,0.6670403587443946,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
472"
CLAIMS,0.6681614349775785,"are not attained by the paper.
473"
LIMITATIONS,0.6692825112107623,"2. Limitations
474"
LIMITATIONS,0.6704035874439462,"Question: Does the paper discuss the limitations of the work performed by the authors?
475"
LIMITATIONS,0.67152466367713,"Answer: [Yes]
476"
LIMITATIONS,0.672645739910314,"Justification: The limitations have been discussed in the conclusion section.
477"
LIMITATIONS,0.6737668161434978,"Guidelines:
478"
LIMITATIONS,0.6748878923766816,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
479"
LIMITATIONS,0.6760089686098655,"the paper has limitations, but those are not discussed in the paper.
480"
LIMITATIONS,0.6771300448430493,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
481"
LIMITATIONS,0.6782511210762332,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
482"
LIMITATIONS,0.679372197309417,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
483"
LIMITATIONS,0.6804932735426009,"model well-specification, asymptotic approximations only holding locally). The authors
484"
LIMITATIONS,0.6816143497757847,"should reflect on how these assumptions might be violated in practice and what the
485"
LIMITATIONS,0.6827354260089686,"implications would be.
486"
LIMITATIONS,0.6838565022421524,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
487"
LIMITATIONS,0.6849775784753364,"only tested on a few datasets or with a few runs. In general, empirical results often
488"
LIMITATIONS,0.6860986547085202,"depend on implicit assumptions, which should be articulated.
489"
LIMITATIONS,0.6872197309417041,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
490"
LIMITATIONS,0.6883408071748879,"For example, a facial recognition algorithm may perform poorly when image resolution
491"
LIMITATIONS,0.6894618834080718,"is low or images are taken in low lighting. Or a speech-to-text system might not be
492"
LIMITATIONS,0.6905829596412556,"used reliably to provide closed captions for online lectures because it fails to handle
493"
LIMITATIONS,0.6917040358744395,"technical jargon.
494"
LIMITATIONS,0.6928251121076233,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
495"
LIMITATIONS,0.6939461883408071,"and how they scale with dataset size.
496"
LIMITATIONS,0.695067264573991,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to address
497"
LIMITATIONS,0.6961883408071748,"problems of privacy and fairness.
498"
LIMITATIONS,0.6973094170403588,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
499"
LIMITATIONS,0.6984304932735426,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
500"
LIMITATIONS,0.6995515695067265,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
501"
LIMITATIONS,0.7006726457399103,"judgment and recognize that individual actions in favor of transparency play an important
502"
LIMITATIONS,0.7017937219730942,"role in developing norms that preserve the integrity of the community. Reviewers will
503"
LIMITATIONS,0.702914798206278,"be specifically instructed to not penalize honesty concerning limitations.
504"
THEORY ASSUMPTIONS AND PROOFS,0.7040358744394619,"3. Theory Assumptions and Proofs
505"
THEORY ASSUMPTIONS AND PROOFS,0.7051569506726457,"Question: For each theoretical result, does the paper provide the full set of assumptions and
506"
THEORY ASSUMPTIONS AND PROOFS,0.7062780269058296,"a complete (and correct) proof?
507"
THEORY ASSUMPTIONS AND PROOFS,0.7073991031390134,"Answer: [NA]
508"
THEORY ASSUMPTIONS AND PROOFS,0.7085201793721974,"Justification: The paper does not include theoretical assumptions.
509"
THEORY ASSUMPTIONS AND PROOFS,0.7096412556053812,"Guidelines:
510"
THEORY ASSUMPTIONS AND PROOFS,0.7107623318385651,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
511"
THEORY ASSUMPTIONS AND PROOFS,0.7118834080717489,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
512"
THEORY ASSUMPTIONS AND PROOFS,0.7130044843049327,"referenced.
513"
THEORY ASSUMPTIONS AND PROOFS,0.7141255605381166,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
514"
THEORY ASSUMPTIONS AND PROOFS,0.7152466367713004,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
515"
THEORY ASSUMPTIONS AND PROOFS,0.7163677130044843,"they appear in the supplemental material, the authors are encouraged to provide a short
516"
THEORY ASSUMPTIONS AND PROOFS,0.7174887892376681,"proof sketch to provide intuition.
517"
THEORY ASSUMPTIONS AND PROOFS,0.718609865470852,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
518"
THEORY ASSUMPTIONS AND PROOFS,0.7197309417040358,"by formal proofs provided in appendix or supplemental material.
519"
THEORY ASSUMPTIONS AND PROOFS,0.7208520179372198,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7219730941704036,"4. Experimental Result Reproducibility
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7230941704035875,"Question: Does the paper fully disclose all the information needed to reproduce the main
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7242152466367713,"experimental results of the paper to the extent that it affects the main claims and/or conclusions
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7253363228699552,"of the paper (regardless of whether the code and data are provided or not)?
524"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.726457399103139,"Answer: [Yes]
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7275784753363229,"Justification: We have provided all the experimental details to reproduce the results. Code is
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7286995515695067,"also available in the supplemental materials.
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7298206278026906,"Guidelines:
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7309417040358744,"‚Ä¢ The answer NA means that the paper does not include experiments.
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7320627802690582,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived well
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7331838565022422,"by the reviewers: Making the paper reproducible is important, regardless of whether
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.734304932735426,"the code and data are provided or not.
532"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7354260089686099,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
533"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7365470852017937,"to make their results reproducible or verifiable.
534"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7376681614349776,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
535"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7387892376681614,"For example, if the contribution is a novel architecture, describing the architecture fully
536"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7399103139013453,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
537"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7410313901345291,"be necessary to either make it possible for others to replicate the model with the same
538"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.742152466367713,"dataset, or provide access to the model. In general. releasing code and data is often
539"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7432735426008968,"one good way to accomplish this, but reproducibility can also be provided via detailed
540"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7443946188340808,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
541"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7455156950672646,"of a large language model), releasing of a model checkpoint, or other means that are
542"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7466367713004485,"appropriate to the research performed.
543"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7477578475336323,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7488789237668162,"submissions to provide some reasonable avenue for reproducibility, which may depend
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.75,"on the nature of the contribution. For example
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7511210762331838,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7522421524663677,"to reproduce that algorithm.
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7533632286995515,"(b) If the contribution is primarily a new model architecture, the paper should describe
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7544843049327354,"the architecture clearly and fully.
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7556053811659192,"(c) If the contribution is a new model (e.g., a large language model), then there should
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7567264573991032,"either be a way to access this model for reproducing the results or a way to reproduce
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.757847533632287,"the model (e.g., with an open-source dataset or instructions for how to construct the
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7589686098654709,"dataset).
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7600896860986547,"(d) We recognize that reproducibility may be tricky in some cases, in which case authors
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7612107623318386,"are welcome to describe the particular way they provide for reproducibility. In the
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7623318385650224,"case of closed-source models, it may be that access to the model is limited in some
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7634529147982063,"way (e.g., to registered users), but it should be possible for other researchers to have
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7645739910313901,"some path to reproducing or verifying the results.
559"
OPEN ACCESS TO DATA AND CODE,0.765695067264574,"5. Open access to data and code
560"
OPEN ACCESS TO DATA AND CODE,0.7668161434977578,"Question: Does the paper provide open access to the data and code, with sufficient instructions
561"
OPEN ACCESS TO DATA AND CODE,0.7679372197309418,"to faithfully reproduce the main experimental results, as described in supplemental material?
562"
OPEN ACCESS TO DATA AND CODE,0.7690582959641256,"Answer: [Yes]
563"
OPEN ACCESS TO DATA AND CODE,0.7701793721973094,"Justification: We have included the code in the supplemental materials and shared the data
564"
OPEN ACCESS TO DATA AND CODE,0.7713004484304933,"link anonymously in the main paper.
565"
OPEN ACCESS TO DATA AND CODE,0.7724215246636771,"Guidelines:
566"
OPEN ACCESS TO DATA AND CODE,0.773542600896861,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
567"
OPEN ACCESS TO DATA AND CODE,0.7746636771300448,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
568"
OPEN ACCESS TO DATA AND CODE,0.7757847533632287,"public/guides/CodeSubmissionPolicy) for more details.
569"
OPEN ACCESS TO DATA AND CODE,0.7769058295964125,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
570"
OPEN ACCESS TO DATA AND CODE,0.7780269058295964,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
571"
OPEN ACCESS TO DATA AND CODE,0.7791479820627802,"including code, unless this is central to the contribution (e.g., for a new open-source
572"
OPEN ACCESS TO DATA AND CODE,0.7802690582959642,"benchmark).
573"
OPEN ACCESS TO DATA AND CODE,0.781390134529148,"‚Ä¢ The instructions should contain the exact command and environment needed to run
574"
OPEN ACCESS TO DATA AND CODE,0.7825112107623319,"to reproduce the results.
See the NeurIPS code and data submission guidelines
575"
OPEN ACCESS TO DATA AND CODE,0.7836322869955157,"(https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
576"
OPEN ACCESS TO DATA AND CODE,0.7847533632286996,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
577"
OPEN ACCESS TO DATA AND CODE,0.7858744394618834,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
578"
OPEN ACCESS TO DATA AND CODE,0.7869955156950673,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
579"
OPEN ACCESS TO DATA AND CODE,0.7881165919282511,"proposed method and baselines. If only a subset of experiments are reproducible, they
580"
OPEN ACCESS TO DATA AND CODE,0.7892376681614349,"should state which ones are omitted from the script and why.
581"
OPEN ACCESS TO DATA AND CODE,0.7903587443946188,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
582"
OPEN ACCESS TO DATA AND CODE,0.7914798206278026,"versions (if applicable).
583"
OPEN ACCESS TO DATA AND CODE,0.7926008968609866,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
584"
OPEN ACCESS TO DATA AND CODE,0.7937219730941704,"paper) is recommended, but including URLs to data and code is permitted.
585"
OPEN ACCESS TO DATA AND CODE,0.7948430493273543,"6. Experimental Setting/Details
586"
OPEN ACCESS TO DATA AND CODE,0.7959641255605381,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
587"
OPEN ACCESS TO DATA AND CODE,0.797085201793722,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
588"
OPEN ACCESS TO DATA AND CODE,0.7982062780269058,"results?
589"
OPEN ACCESS TO DATA AND CODE,0.7993273542600897,"Answer: [Yes]
590"
OPEN ACCESS TO DATA AND CODE,0.8004484304932735,"Justification: We have specified all the training and test details to understand the results.
591"
OPEN ACCESS TO DATA AND CODE,0.8015695067264574,"Guidelines:
592"
OPEN ACCESS TO DATA AND CODE,0.8026905829596412,"‚Ä¢ The answer NA means that the paper does not include experiments.
593"
OPEN ACCESS TO DATA AND CODE,0.8038116591928252,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
594"
OPEN ACCESS TO DATA AND CODE,0.804932735426009,"that is necessary to appreciate the results and make sense of them.
595"
OPEN ACCESS TO DATA AND CODE,0.8060538116591929,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
596"
OPEN ACCESS TO DATA AND CODE,0.8071748878923767,"material.
597"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8082959641255605,"7. Experiment Statistical Significance
598"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8094170403587444,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
599"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8105381165919282,"information about the statistical significance of the experiments?
600"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8116591928251121,"Answer: [Yes]
601"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8127802690582959,"Justification: We have performed our experiments three times for each to provide the mean
602"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8139013452914798,"and variance accuracy suitably and correctly in our tables.
603"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8150224215246636,"Guidelines:
604"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8161434977578476,"‚Ä¢ The answer NA means that the paper does not include experiments.
605"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8172645739910314,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confidence
606"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8183856502242153,"intervals, or statistical significance tests, at least for the experiments that support the
607"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8195067264573991,"main claims of the paper.
608"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.820627802690583,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
609"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8217488789237668,"example, train/test split, initialization, random drawing of some parameter, or overall
610"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8228699551569507,"run with given experimental conditions).
611"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8239910313901345,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
612"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8251121076233184,"call to a library function, bootstrap, etc.)
613"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8262331838565022,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
614"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.827354260089686,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error of
615"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.82847533632287,"the mean.
616"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8295964125560538,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
617"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8307174887892377,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
618"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8318385650224215,"of Normality of errors is not verified.
619"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8329596412556054,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
620"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8340807174887892,"figures symmetric error bars that would yield results that are out of range (e.g. negative
621"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8352017937219731,"error rates).
622"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8363228699551569,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
623"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8374439461883408,"they were calculated and reference the corresponding figures or tables in the text.
624"
EXPERIMENTS COMPUTE RESOURCES,0.8385650224215246,"8. Experiments Compute Resources
625"
EXPERIMENTS COMPUTE RESOURCES,0.8396860986547086,"Question: For each experiment, does the paper provide sufficient information on the computer
626"
EXPERIMENTS COMPUTE RESOURCES,0.8408071748878924,"resources (type of compute workers, memory, time of execution) needed to reproduce the
627"
EXPERIMENTS COMPUTE RESOURCES,0.8419282511210763,"experiments?
628"
EXPERIMENTS COMPUTE RESOURCES,0.8430493273542601,"Answer: [Yes]
629"
EXPERIMENTS COMPUTE RESOURCES,0.844170403587444,"Justification: We have provided the details of computer resources in the experimental section.
630"
EXPERIMENTS COMPUTE RESOURCES,0.8452914798206278,"Guidelines:
631"
EXPERIMENTS COMPUTE RESOURCES,0.8464125560538116,"‚Ä¢ The answer NA means that the paper does not include experiments.
632"
EXPERIMENTS COMPUTE RESOURCES,0.8475336322869955,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
633"
EXPERIMENTS COMPUTE RESOURCES,0.8486547085201793,"or cloud provider, including relevant memory and storage.
634"
EXPERIMENTS COMPUTE RESOURCES,0.8497757847533632,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
635"
EXPERIMENTS COMPUTE RESOURCES,0.850896860986547,"experimental runs as well as estimate the total compute.
636"
EXPERIMENTS COMPUTE RESOURCES,0.852017937219731,"‚Ä¢ The paper should disclose whether the full research project required more compute
637"
EXPERIMENTS COMPUTE RESOURCES,0.8531390134529148,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
638"
EXPERIMENTS COMPUTE RESOURCES,0.8542600896860987,"didn‚Äôt make it into the paper).
639"
CODE OF ETHICS,0.8553811659192825,"9. Code Of Ethics
640"
CODE OF ETHICS,0.8565022421524664,"Question: Does the research conducted in the paper conform, in every respect, with the
641"
CODE OF ETHICS,0.8576233183856502,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
642"
CODE OF ETHICS,0.8587443946188341,"Answer: [Yes]
643"
CODE OF ETHICS,0.8598654708520179,"Justification: This research conducted in the paper conforms in every respect with the
644"
CODE OF ETHICS,0.8609865470852018,"NeurIPS Code of Ethics.
645"
CODE OF ETHICS,0.8621076233183856,"Guidelines:
646"
CODE OF ETHICS,0.8632286995515696,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
647"
CODE OF ETHICS,0.8643497757847534,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
648"
CODE OF ETHICS,0.8654708520179372,"deviation from the Code of Ethics.
649"
CODE OF ETHICS,0.8665919282511211,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special
650"
CODE OF ETHICS,0.8677130044843049,"consideration due to laws or regulations in their jurisdiction).
651"
BROADER IMPACTS,0.8688340807174888,"10. Broader Impacts
652"
BROADER IMPACTS,0.8699551569506726,"Question: Does the paper discuss both potential positive societal impacts and negative
653"
BROADER IMPACTS,0.8710762331838565,"societal impacts of the work performed?
654"
BROADER IMPACTS,0.8721973094170403,"Answer: [Yes]
655"
BROADER IMPACTS,0.8733183856502242,"Justification: We have discussed both potential positive societal impacts and negative societal
656"
BROADER IMPACTS,0.874439461883408,"impacts in Sec. A.
657"
BROADER IMPACTS,0.875560538116592,"Guidelines:
658"
BROADER IMPACTS,0.8766816143497758,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
659"
BROADER IMPACTS,0.8778026905829597,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
660"
BROADER IMPACTS,0.8789237668161435,"impact or why the paper does not address societal impact.
661"
BROADER IMPACTS,0.8800448430493274,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
662"
BROADER IMPACTS,0.8811659192825112,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
663"
BROADER IMPACTS,0.8822869955156951,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
664"
BROADER IMPACTS,0.8834080717488789,"groups), privacy considerations, and security considerations.
665"
BROADER IMPACTS,0.8845291479820628,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
666"
BROADER IMPACTS,0.8856502242152466,"to particular applications, let alone deployments. However, if there is a direct path to
667"
BROADER IMPACTS,0.8867713004484304,"any negative applications, the authors should point it out. For example, it is legitimate
668"
BROADER IMPACTS,0.8878923766816144,"to point out that an improvement in the quality of generative models could be used to
669"
BROADER IMPACTS,0.8890134529147982,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
670"
BROADER IMPACTS,0.8901345291479821,"that a generic algorithm for optimizing neural networks could enable people to train
671"
BROADER IMPACTS,0.8912556053811659,"models that generate Deepfakes faster.
672"
BROADER IMPACTS,0.8923766816143498,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
673"
BROADER IMPACTS,0.8934977578475336,"being used as intended and functioning correctly, harms that could arise when the
674"
BROADER IMPACTS,0.8946188340807175,"technology is being used as intended but gives incorrect results, and harms following
675"
BROADER IMPACTS,0.8957399103139013,"from (intentional or unintentional) misuse of the technology.
676"
BROADER IMPACTS,0.8968609865470852,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
677"
BROADER IMPACTS,0.897982062780269,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
678"
BROADER IMPACTS,0.899103139013453,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
679"
BROADER IMPACTS,0.9002242152466368,"feedback over time, improving the efficiency and accessibility of ML).
680"
SAFEGUARDS,0.9013452914798207,"11. Safeguards
681"
SAFEGUARDS,0.9024663677130045,"Question: Does the paper describe safeguards that have been put in place for responsible
682"
SAFEGUARDS,0.9035874439461884,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
683"
SAFEGUARDS,0.9047085201793722,"image generators, or scraped datasets)?
684"
SAFEGUARDS,0.905829596412556,"Answer: [NA]
685"
SAFEGUARDS,0.9069506726457399,"Justification: We believe this paper poses no such risks.
686"
SAFEGUARDS,0.9080717488789237,"Guidelines:
687"
SAFEGUARDS,0.9091928251121076,"‚Ä¢ The answer NA means that the paper poses no such risks.
688"
SAFEGUARDS,0.9103139013452914,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
689"
SAFEGUARDS,0.9114349775784754,"necessary safeguards to allow for controlled use of the model, for example by requiring
690"
SAFEGUARDS,0.9125560538116592,"that users adhere to usage guidelines or restrictions to access the model or implementing
691"
SAFEGUARDS,0.9136771300448431,"safety filters.
692"
SAFEGUARDS,0.9147982062780269,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
693"
SAFEGUARDS,0.9159192825112108,"should describe how they avoided releasing unsafe images.
694"
SAFEGUARDS,0.9170403587443946,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
695"
SAFEGUARDS,0.9181614349775785,"not require this, but we encourage authors to take this into account and make a best
696"
SAFEGUARDS,0.9192825112107623,"faith effort.
697"
LICENSES FOR EXISTING ASSETS,0.9204035874439462,"12. Licenses for existing assets
698"
LICENSES FOR EXISTING ASSETS,0.92152466367713,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
699"
LICENSES FOR EXISTING ASSETS,0.922645739910314,"the paper, properly credited and are the license and terms of use explicitly mentioned and
700"
LICENSES FOR EXISTING ASSETS,0.9237668161434978,"properly respected?
701"
LICENSES FOR EXISTING ASSETS,0.9248878923766816,"Answer: [Yes]
702"
LICENSES FOR EXISTING ASSETS,0.9260089686098655,"Justification: We have cited all papers and credited all code we utilized in this work.
703"
LICENSES FOR EXISTING ASSETS,0.9271300448430493,"Guidelines:
704"
LICENSES FOR EXISTING ASSETS,0.9282511210762332,"‚Ä¢ The answer NA means that the paper does not use existing assets.
705"
LICENSES FOR EXISTING ASSETS,0.929372197309417,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
706"
LICENSES FOR EXISTING ASSETS,0.9304932735426009,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
707"
LICENSES FOR EXISTING ASSETS,0.9316143497757847,"URL.
708"
LICENSES FOR EXISTING ASSETS,0.9327354260089686,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
709"
LICENSES FOR EXISTING ASSETS,0.9338565022421524,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
710"
LICENSES FOR EXISTING ASSETS,0.9349775784753364,"service of that source should be provided.
711"
LICENSES FOR EXISTING ASSETS,0.9360986547085202,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
712"
LICENSES FOR EXISTING ASSETS,0.9372197309417041,"package should be provided. For popular datasets, paperswithcode.com/datasets
713"
LICENSES FOR EXISTING ASSETS,0.9383408071748879,"has curated licenses for some datasets. Their licensing guide can help determine the
714"
LICENSES FOR EXISTING ASSETS,0.9394618834080718,"license of a dataset.
715"
LICENSES FOR EXISTING ASSETS,0.9405829596412556,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
716"
LICENSES FOR EXISTING ASSETS,0.9417040358744395,"the derived asset (if it has changed) should be provided.
717"
LICENSES FOR EXISTING ASSETS,0.9428251121076233,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to the
718"
LICENSES FOR EXISTING ASSETS,0.9439461883408071,"asset‚Äôs creators.
719"
NEW ASSETS,0.945067264573991,"13. New Assets
720"
NEW ASSETS,0.9461883408071748,"Question: Are new assets introduced in the paper well documented and is the documentation
721"
NEW ASSETS,0.9473094170403588,"provided alongside the assets?
722"
NEW ASSETS,0.9484304932735426,"Answer: [Yes]
723"
NEW ASSETS,0.9495515695067265,"Justification: Our code has been included in the supplemental materials and is well
724"
NEW ASSETS,0.9506726457399103,"documented, we have also shared the synthetic data in the main paper.
725"
NEW ASSETS,0.9517937219730942,"Guidelines:
726"
NEW ASSETS,0.952914798206278,"‚Ä¢ The answer NA means that the paper does not release new assets.
727"
NEW ASSETS,0.9540358744394619,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
728"
NEW ASSETS,0.9551569506726457,"submissions via structured templates. This includes details about training, license,
729"
NEW ASSETS,0.9562780269058296,"limitations, etc.
730"
NEW ASSETS,0.9573991031390134,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
731"
NEW ASSETS,0.9585201793721974,"asset is used.
732"
NEW ASSETS,0.9596412556053812,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
733"
NEW ASSETS,0.9607623318385651,"create an anonymized URL or include an anonymized zip file.
734"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9618834080717489,"14. Crowdsourcing and Research with Human Subjects
735"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9630044843049327,"Question: For crowdsourcing experiments and research with human subjects, does the paper
736"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9641255605381166,"include the full text of instructions given to participants and screenshots, if applicable, as
737"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9652466367713004,"well as details about compensation (if any)?
738"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9663677130044843,"Answer: [NA]
739"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9674887892376681,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.968609865470852,"Guidelines:
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9697309417040358,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9708520179372198,"human subjects.
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9719730941704036,"‚Ä¢ Including this information in the supplemental material is fine, but if the main
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9730941704035875,"contribution of the paper involves human subjects, then as much detail as possible
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742152466367713,"should be included in the main paper.
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9753363228699552,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.976457399103139,"or other labor should be paid at least the minimum wage in the country of the data
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775784753363229,"collector.
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786995515695067,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9798206278026906,"Subjects
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9809417040358744,"Question: Does the paper describe potential risks incurred by study participants, whether
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820627802690582,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831838565022422,"approvals (or an equivalent approval/review based on the requirements of your country or
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.984304932735426,"institution) were obtained?
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854260089686099,"Answer: [NA]
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865470852017937,"Justification: This paper does not involve crowdsourcing nor research with human subjects.
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9876681614349776,"Guidelines:
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9887892376681614,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9899103139013453,"human subjects.
760"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910313901345291,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992152466367713,"may be required for any human subjects research. If you obtained IRB approval, you
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932735426008968,"should clearly state this in the paper.
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9943946188340808,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955156950672646,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966367713004485,"guidelines for their institution.
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977578475336323,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988789237668162,"applicable), such as the institution conducting the review.
768"
