Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006844626967830253,"Simulators are a pervasive tool in reinforcement learning, but most existing algo-
rithms cannot efficiently exploit simulator access—particularly in high-dimensional
domains that require general function approximation. We explore the power of
simulators through online reinforcement learning with local simulator access (or,
local planning), an RL protocol where the agent is allowed to reset to previously
observed states and follow their dynamics during training. We use local simulator
access to unlock new statistical guarantees that were previously out of reach:"
ABSTRACT,0.0013689253935660506,"1. We show that MDPs with low coverability [63]—a general structural con-
dition that subsumes Block MDPs and Low-Rank MDPs—can be learned
in a sample-efficient fashion with only Q⋆-realizability (realizability of
the optimal state-value function); existing online RL algorithms require
significantly stronger representation conditions."
ABSTRACT,0.002053388090349076,"2. As a consequence, we show that the notorious Exogenous Block MDP
problem [22] is tractable under local simulator access."
ABSTRACT,0.0027378507871321013,"The results above are achieved through a computationally-inefficient algorithm.
We complement them with a more computationally efficient algorithm, RVFS
(Recursive Value Function Search), which achieves provable sample complexity
guarantees under strengthened statistical assumption known as pushforward
coverability.
RVFS can be viewed as a principled, provable counterpart to a
successful empirical paradigm that combines recursive search (e.g., MCTS) with
value function approximation."
INTRODUCTION,0.0034223134839151265,"1
Introduction"
INTRODUCTION,0.004106776180698152,"Simulators are a widely used tool in reinforcement learning. Many of the most well-known bench-
marks for reinforcement learning research make use of simulators (Atari [9], MuJoCo [55], OpenAI
Gym [11], DeepMind Control Suite [53]), and high-quality simulators are available for a wide range of
real-world control tasks, including robotic control [45, 2], autonomous vehicles [10, 6], and game play-
ing [51, 52]. Simulators also provide a useful abstraction for planning with a known or learned model,
an important building block for many RL techniques [48]. Yet, in spite of the ubiquity of simulators,
almost all existing research into algorithm design—empirical and theoretical—has focused on the
online reinforcement learning (where only trajectory-based feedback is available), and does not take
advantage of the extra information available through the simulator. Relatively little is known about the
full power of RL with simulator access, either in terms of algorithmic principles or fundamental limits."
INTRODUCTION,0.004791238877481177,"We explore the power of simulators through online reinforcement learning with local simulator access
(RLLS for short), also known as local planning [57, 40, 3, 59, 65, 66]. Here, the agent learns by
repeatedly executing policies and observing the resulting trajectories (as in online RL), but is allowed
to reset to previously observed states and follow their dynamics during training."
INTRODUCTION,0.0054757015742642025,"Empirically, algorithms based on local simulators have received limited investigation, but with
promising results. Notably, the Go-Explore algorithm [19, 20] uses local simulator access to achieve
state-of-the-art performance for Montezuma’s Revenge (a difficult Atari game that requires systematic"
INTRODUCTION,0.006160164271047228,"exploration), beating the performance of the best agents trained with online RL [7, 27] by a significant
margin that has yet to be closed. The successful line of research on AlphaGo and successors
[51, 52, 48] also uses local simulator access, albeit at test time in addition to training time."
INTRODUCTION,0.006844626967830253,"These results suggest that developing improved algorithm design principles for RL with local
simulator access could have significant practical implications, but current theoretical understanding of
local simulators is limited. Recent work has shown that local simulator access has provable benefits
for reinforcement learning with various types of linear function approximation [57, 40, 3, 65, 59],
but essentially nothing is known for RL problems in large state spaces that demand general, potential
neural function approximation. This leads us to ask: Can we develop algorithms for reinforcement
learning with general function approximation that provably benefit from local simulator access?"
INTRODUCTION,0.007529089664613279,"From an algorithm design perspective, perhaps the greatest challenge in using local simulators to
speed up learning is to understand which states are “informative” in the sense that we should prioritize
revisiting them. Here, we are faced with a chicken-and-egg problem: to understand which states
to prioritize, we must explore and gather information, but it is unclear how to do so efficiently
unless we already have a way to understand which states are informative. It is natural to let function
approximation guide us; to this end, recent research [40, 65, 59] on linearly-parameterized RL with
local simulators makes use of core-sets: small, adaptively chosen sets of informative state-action pairs
designed to cover the feature space and enable efficient value function learning. Core-sets facilitate
sample complexity guarantees for linear models that are not possible without local simulator access
(e.g., [40]). Yet, for general function classes—particularly rich models like neural networks that do
not readily support extrapolation—defining a suitable notion of core-set is challenging. Consequently,
existing techniques have yet to meaningfully leverage local simulator access beyond the linear regime."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.008213552361396304,"1.1
Contributions
We show that local simulator access unlocks new guarantees for online RL with general value
function approximation—statistical and computational—that were previously out of reach."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.00889801505817933,"Sample-efficient learning. We show that MDPs with low coverability [63]—a general structural
condition that subsumes Block MDPs and Low-Rank MDPs—can be learned in a sample-efficient
fashion with only Q⋆-realizability (that is, realizability for the optimal state-action value function).
This is achieved through a new algorithm, SimGolf, that augments the principle of global optimism
with local simulator access, and improves upon the best existing guarantees for the fully online
RL setting, which require significantly stronger representation conditions. As a consequence, we
show for the first time that the notoriously challenging Exogenous Block MDP (ExBMDP) problem
[22, 21] is tractable in its most general form under local simulator access."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.009582477754962354,"Practical, computationally efficient learning.
Our results above are achieved through a
computationally inefficient algorithm. We complement them with a practical and computationally
efficient algorithm, RVFS (“Recursive Value Function Search”), which achieves sample-efficient
learning guarantees with general value function approximation under a strengthened, yet novel,
statistical assumption known as pushforward coverability [62]. Assuming either i) realizability
of the optimal state-value function V ⋆and a state-action gap or ii) realizability of V π for all π,
RVFS achieves polynomial sample complexity in a computationally efficient fashion, and leads to
guarantees for a new class of Exogenous Block MDPs with weakly correlated exogenous noise.
RVFS explores by building core-sets with a novel value function-guided scheme, and can be viewed
as a principled counterpart to algorithms including MCTS and AlphaZero [51, 52, 19, 20, 66], that
combine recursive search with value function approximation. Compared to these approaches, RVFS
is designed to provably address stochastic environments and distribution shift."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.01026694045174538,"Paper organization. Section 2 introduces the local simulator framework. Section 3 presents our
main sample complexity guarantees, and Section 4 gives computationally efficient algorithms. All
proofs are deferred to the appendix."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.010951403148528405,"2
Setup: Reinforcement Learning with Local Simulator Access"
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.01163586584531143,"We consider an episodic reinforcement learning setting. A Markov Decision Process (MDP) is a tuple
M = (X,A,T,R,H), where X is a (large/potentially infinite) state space, A is the action space (we
abbreviate A = ∣A∣), H ∈N is the horizon, R = {Rh}H
h=1 is the reward function (where Rh ∶X × A →
[0,1]) and T = {Th}H
h=0 is the transition distribution (where Th ∶X × A →∆(X)), with the conven-
tion that T0(⋅∣∅) is the initial state distribution. A policy is a sequence of functions π = {πh ∶X →"
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.012320328542094456,"∆(A)}H
h=1; we use ΠS to denote the set of all such functions. When a policy is executed, it generates
a trajectory (x1,a1,r1),...,(xH,aH,rh) via the process ah ∼πh(xh),rh ∼Rh(xh,ah),xh+1 ∼
Th(⋅∣xh,ah), initialized from x1 ∼T0(⋅∣∅) (we use xH+1 to denote a terminal state with zero
reward). We write Pπ[⋅] and Eπ[⋅] to denote the law and expectation under this process."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.013004791238877482,"For a policy π, J(π) ∶= Eπ[∑H
h=1 rh] denotes expected reward, and the value functions are given by
V π
h (x) ∶= Eπ[∑H
h′=h rh′ ∣xh = x], and Qπ
h(x,a) ∶= Eπ[∑H
h′=h rh′ ∣xh = x,ah = a]. We denote by
π⋆the optimal deterministic policy that maximizes Qπ⋆, and write Q⋆∶= Qπ⋆and V ⋆∶= V π⋆."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.013689253935660506,"Online reinforcement learning with a local simulator. In the standard online reinforcement
learning framework, the learner repeatedly interacts with an (unknown) MDP by executing a policy
and observing the resulting trajectory, with the goal of maximizing the total reward. Formally, for each
episode τ ∈[Nepisodes], the learner selects a policy π(τ) = {π
(τ)
h }H
h=1, executes it in the underlying
MDP M⋆and observes the trajectory {(x
(τ)
h ,a
(τ)
h ,r
(τ)
h )}H
h=1. After all Nepisodes episodes conclude,
the learner produces a policy ̂π ∈ΠS with the goal of minimizing the risk given by E[J(π⋆) −J(̂π)]."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.014373716632443531,"In online RL with local simulator access, or RLLS, [57, 40, 65, 59, 66], we augment the online
RL protocol as follows: At each episode τ ∈[N], instead of starting from a random initial state
x1 ∼T0(⋅∣∅), the agent can reset the MDP to any layer h ∈[H] and any state xh previously
encountered, and proceed with a new episode starting from xh. As in the online RL protocol, the goal
is to produce a policy ̂π ∈ΠS such that E[J(π⋆) −J(̂π)] ≤ε with as few episodes of interaction as
possible; our main results take Nepisodes = poly(C,ε−1) for a suitable problem parameter C."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.015058179329226557,"Executable versus non-executable policies. We focus on learning policies that can be executed
without access to a local simulator (in other words, the local simulator used at train time, but not
test time). Some recent work using local simulators for RL with linear function approximation
[57] considers a more permissive setting where the final policy π produced by the learner can be
non-executable; our function approximation requirements can be slightly relaxed in this case."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.01574264202600958,"Definition 2.1 (Non-executable policy). We refer to a policy π for which computing π(x) ∈∆(A) for
any x ∈X requires n local simulator queries as a non-executable policy with sample complexity n."
"CONTRIBUTIONS
WE SHOW THAT LOCAL SIMULATOR ACCESS UNLOCKS NEW GUARANTEES FOR ONLINE RL WITH GENERAL VALUE",0.01642710472279261,"Additional notation.
For any m,n ∈N, we denote by [m..n] the integer interval {m,...,n}.
We also let [n] ∶= [1..n]. We refer to a scalar c > 0 as an absolute constant to indicate that
it is independent of all problem parameters and use ̃O(⋅) to denote a bound up to factors poly-
logarithmic in parameters appearing in the expression.
We define πunif ∈ΠS as the random
policy that selects actions in A uniformly. We define the occupancy measure for policy π via
dπ
h(x,a) ∶= Pπ[xh = x,ah = a].
For functions g ∶X × A →R and f ∶X →R, we de-
fine Bellman backup operators by Th[g](x,a) = E[rh + maxa′∈A g(xh+1,a′) ∣xh = x,ah = a] and
Ph[f](x,a) = E[rh + f(xh+1) ∣xh = x,ah = a]. For a stochastic policy π ∈ΠS, we will occasion-
ally use the bold notation πh(x) as shorthand for the random variable ah ∼πh(x) ∈∆(A). For a
function f ∶A →R, we write a′ ∈arg maxa∈A f(a) to denote the action that maximizes f. If there
are ties, we break them by picking the action with the smallest index; we assume without loss of
generality that actions in A are index from 1,...,∣A∣."
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.017111567419575632,"3
New Sample-Efficient Learning Guarantees via Local Simulators"
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.01779603011635866,"This section presents our most powerful results for RLLS. We present a new algorithm for learning
with local simulator access, SimGolf (Section 3.1), and show that it enables sample-efficient RL for
MDPs with low coverability [63] using only Q⋆-realizability (Section 3.2). We then give implications
for the Exogenous Block MDP problem (Section 3.3)."
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.018480492813141684,"Function approximation setup and coverability. To achieve sample complexity guarantees for
online reinforcement learning that are suitable for large, high-dimensional state spaces, we appeal to
value function approximation. We assume access to a function class Q ⊂(X × A × [H] →[0,H])
that contains the optimal state-action value function Q⋆; we define Qh = {Qh ∣Q ∈Q}."
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.019164955509924708,"Assumption 3.1 (Q⋆-realizability). For all h ∈[H], we have Q⋆
h ∈Qh."
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.019849418206707735,"Q⋆-realizability is widely viewed as a minimal representation condition for online RL [61, 17, 16, 39,
58, 56]. The class Q encodes the learner’s prior knowledge about the MDP, and can be parameterized
by rich function approximators like neural networks. We assume for simplicity of exposition that Q"
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.02053388090349076,"and Π are finite, and aim for sample complexity guarantees scaling with log∣Q∣and log∣Π∣; extending
our results to infinite classes via standard uniform convergence arguments is straightforward."
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.021218343600273786,"Coverability. Beyond representation conditions like realizability, online RL algorithms require
structural conditions that limit the extent to which deliberately designed algorithms can be surprised
by substantially new state distributions. We focus on a structural condition known as coverability
[63], which is inspired by connections between online and offline RL."
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.02190280629705681,"Assumption 3.2. The coverability coefficient is Ccov ∶= maxh∈[H] infµh∈∆(X×A) supπ∈ΠS ∥dπ
h
µh ∥
∞."
NEW SAMPLE-EFFICIENT LEARNING GUARANTEES VIA LOCAL SIMULATORS,0.022587268993839837,"Coverability is an intrinsic strutural property of the underlying MDP. Examples of MDP families with
low coverability include (Exogenous) Block MDPs, which have Ccov ≤∣S∣∣A∣, where S is the latent
state space [63], and Low-Rank MDPs, which have Ccov ≤d∣A∣, where d is the feature dimension
[29]; importantly, these settings exhibit high-dimensional state spaces and require nonlinear function
approximation. As in prior work [63, 4], our algorithms require no prior knowledge of the distribution
µh that achieves the minimum in Assumption 3.2."
ALGORITHM,0.02327173169062286,"3.1
Algorithm
Our main algorithm, SimGolf, is displayed in Algorithm 1. The algorithm is a variant of the GOLF
method of Jin et al. [32], Xie et al. [63] with novel adaptations to exploit the availability of a local
simulator. Like GOLF, SimGolf explores using the principle of global optimisim: At each iteration
t ∈[N], it maintains a confidence set (or, version space) Q(t) ⊂Q of candidate value functions with
low squared Bellman error under the data collected so far, and chooses a new exploration policy π(t)
by picking the most “optimistic” value function in this set. As the algorithm gathers more data, the
confidence set shrinks, leaving only near-optimal policies."
ALGORITHM,0.023956194387405885,"The main novelty in SimGolf arises in the data collection strategy and design of confidence sets.
Like GOLF, SimGolf algorithm constructs the confidence set Q(t) ⊂Q such that all value functions
g ∈Q(t) have small squared Bellman error:"
ALGORITHM,0.024640657084188913,"∑
i<t
Eπ(i)[(gh(xh,ah) −Th[gh+1](xh,ah))2] ≲log∣Q∣,
∀h ∈[H].
(1)"
ALGORITHM,0.025325119780971937,"Due to the presence of the Bellman backup Th[gh+1] in Eq. (1), naively estimating squared Bellman
error leads to the notorious double sampling problem. To avoid this, the approach taken with GOLF
and related work [67, 32] is to adapt a certain de-biasing technique to remove double sampling bias,
but this requires access to a value function class that satisfies Bellman completeness, a representation
significantly more restrictive than realizability (e.g., Foster et al. [26])."
ALGORITHM,0.026009582477754964,"The idea behind SimGolf is to use local simulator access to directly produce high-quality estimates
for the Bellman backup function Th[gh+1] in Eq. (1).
In particular, for a given state-action
pair (x,a) ∈X × A, we can estimate the Bellman backup Th[gh+1](x,a) for all functions
g ∈Q simultaneously by collecting K next-state transitions ̃x
(1)
h+1,..., ̃x
(K)
h+1
i.i.d.
∼
Th(⋅∣x,a)"
ALGORITHM,0.026694045174537988,"and K rewards ̃r
(1)
h ,..., ̃r
(K)
h
i.i.d.
∼
Rh(x,a), then taking the empirical mean: Th[gh+1](x,a) ≈
1
K ∑K
k=1(̃r
(k)
h + maxa′∈A gh+1(̃x
(k)
h+1,a′)). Line 8 of SimGolf uses this technique to directly estimate
the Bellman residual backup under a trajectory gathered with π(t), sidestepping the double sampling
problem and removing the need for Bellman completeness. We suspect this technique (estimation
with respect to squared Bellman error using local simulator access) may find broader use."
MAIN RESULT,0.02737850787132101,"3.2
Main Result
We now state the main guarantee for SimGolf and discuss some of its implications."
MAIN RESULT,0.02806297056810404,"Theorem 3.1 (Main guarantee for SimGolf). Let ε,δ ∈(0,1) be given and suppose Assumption 3.1
(Q⋆-realizability) and Assumption 3.2 (coverability) hold with Ccov > 0. Then the policy ̂π produced
by SimGolf(Q,Ccov,ε,δ) (Algorithm 1) has J(π⋆) −E[J(̂π)] ≤ε with probability at least 1 −δ.
The total sample complexity in the RLLS framework is bounded by ̃O(H5C2
cov log(∣Q∣/δ) ⋅ε−4)."
MAIN RESULT,0.028747433264887063,"This result (whose proof is in Appendix E) shows that under only Q⋆-realizability and coverability,
SimGolf learns an ε-optimal policy with polynomial sample complexity, significantly relaxing the
representation assumptions (Bellman completeness, weight function realizability) required by prior
algorithms for coverability [63, 4]. This is the first instance we are aware of where local simulator"
MAIN RESULT,0.02943189596167009,Algorithm 1 SimGolf: Global Optimism via Local Simulator Access
MAIN RESULT,0.030116358658453114,"1: input: Value function class Q, coverability Ccov > 0, suboptimality ε > 0, and confidence δ > 0.
2: Set N ←̃Θ(H2Ccovβ/ε2), βstat ←16log(2HN∣Q∣δ−1), β ←2βstat, and K ←
8N
βstat ."
MAIN RESULT,0.030800821355236138,"3: initialize: Q(1) ←Q.
4: for iteration t = 1,2,...,N do
5:
Select g(t) = arg maxg∈Q(t) ∑t−1
s=1 maxa∈A g1(x
(s)
1 ,a).
6:
For each h ∈[H] and x ∈X, define π
(t)
h (x) ∈arg maxa∈A g
(t)
h (x,a).
7:
Execute π(t) for an episode and observe τ (t) ∶= (x
(t)
1 ,a
(t)
1 ),...,(x
(t)
H ,a
(t)
H ).
8:
For h ∈[H], draw K independent samples x
(t,k)
h+1 ∼Th(⋅∣x
(t)
h ,a
(t)
h ), r
(t,k)
h
∼Rh(x
(t)
h ,a
(t)
h ).
9:
Compute confidence set:"
MAIN RESULT,0.03148528405201916,"Q
(t+1) ←
⎧⎪⎪⎨⎪⎪⎩
g ∈Q ∶∑
s≤t
(gh(x
(s)
h , a
(s)
h ) −1 K"
MAIN RESULT,0.03216974674880219,"K
∑
k=1
(r
(s,k)
h
+ max
a∈A gh+1(x
(s,k)
h+1 , a)))"
MAIN RESULT,0.03285420944558522,"2
≤β, ∀h ∈[H]
⎫⎪⎪⎬⎪⎪⎭
."
MAIN RESULT,0.03353867214236824,"10: return: ̂π = unif(π(1),...,π(N))."
MAIN RESULT,0.034223134839151265,"access unlocks sample complexity guarantees for reinforcement learning with nonlinear function
approximation that were previously out of reach; perhaps the most important technical idea here
is our approach to combining global optimism with local simulator access, in contrast to greedy
layer-by-layer schemes used in prior work on local simulators (with the exception of Weisz et al.
[57]). In particular, we suspect that the idea of performing estimation with respect to squared Bellman
error directly using local simulator access may find broader use beyond coverability. Improving the
polynomial dependence on problem parameters is an interesting question for future work."
MAIN RESULT,0.03490759753593429,"A conjecture. By analogy to results in offline reinforcement learning, where Q⋆-realizability
and concentrability (the offline counterpart to coverability) alone are known to be insufficient for
sample-efficient learning [12, 26], we conjecture that Q⋆-realizability and coverability alone are
not sufficient for polynomial sample complexity in vanilla online RL. If true, this would imply a
new separation between online RL with and without local simulators."
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.03559206023271732,"3.3
Implications for Exogenous Block MDPs"
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.03627652292950034,"We now apply SimGolf and Theorem 3.1 to the Exogenous Block MDP (ExBMDP) problem [22, 21,
38, 30], a challenging rich-observation reinforcement learning setting in which the observed states
xh are high-dimensional, while the underlying dynamics of the system are low-dimensional, yet
confounded by temporally correlated exogenous noise."
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.03696098562628337,"Formally, an Exogenous Block MDP M = (X,S,Ξ,A,H,T,R,g) is defined by a latent state space
and an observation space. We begin with the latent state space. Starting from an initial endogenous
state s1 ∈S and exogenous state ξ1 ∈Ξ, the latent state zh = (sh,ξh) evolves for h ∈[H] via
sh+1 ∼T endo
h
(⋅∣sh,ah) and ξh+1 ∼T exo
h (⋅∣ξh), where ah ∈A is the agent’s action at layer h; we
adopt the convention that s1 ∼T endo
0
(⋅∣∅) and ξ1 ∼T exo
0
(⋅∣∅). Note that only the endogenous
state is causally influenced by the action. The latent state is not observed; instead, at each step h, the
agent receives an observation xh ∈X generated via1 xh = gobs
h (sh,ξh), where gobs
h
∶S × Ξ →X is
the emission function. We assume the endogenous latent space S and action space A are finite, and
define S ∶= ∣S∣and A ∶= ∣A∣. However, the exogenous state space Ξ and observation space X may be
arbitrarily large or infinite, with ∣Ξ∣,∣X∣≫∣S∣.2"
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.03764544832306639,"The final property of the ExBMDP model is decodability, which asserts the existence of a decoder
such that ϕ⋆∶X →S such that ϕ⋆(xh) = sh a.s. for all h ∈[H] with xh = gobs
h (sh,ξh),. Informally,
decodability ensures the existence of an (unknown to the learner) mapping that allows one to
perfectly recover the endogenous latent state from observations. In addition to decodability, we
assume the rewards in the ExBMDP are endogenous; that is, the reward distribution Rh(xh,ah) only"
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.038329911019849415,"1A more standard formulation [22, 21, 38, 30] assumes that observations are generated via xh ∼qh(sh, ξh),
where qh(⋅, ⋅) is a conditional distribution with the decodability property. This is equivalent to xh = gobs
h (sh, ξh),
as randomness in the emission process can be included in the exogenous state w.l.o.g.
2To simplify presentation, we assume that Ξ and X are countable; our results trivially extend to the case
where the corresponding variables are continuous with an appropriate measure-theoretic treatment."
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.039014373716632446,"depends on the observations (xh) through the corresponding latent states (ϕ⋆(xh) = sh). To enable
sample-efficient learning, we assume access to a decoder class Φ that contains ϕ⋆, as in prior work."
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.03969883641341547,Assumption 3.3 (Decoder realizability). We have access to a decoder class Φ such that ϕ⋆∈Φ.
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.040383299110198494,"Applying SimGolf and Theorem 3.1. To apply Theorem 3.1 to the ExBMDP problem, we need
to verify that Q⋆-realizability and coverability hold. Realizability is a straightforward consequence
of decodability (Lemma D.1 in Part II of the appendix). For coverability, Xie et al. [63] show that
ExBMDPs have Ccov ≤SA under decodability, in spite of the time-correlated exogenous noise
process (ξh) and potentially infinite observation space X (interestingly, coverability is essentially the
only useful structural property that ExBMDPs are known to satisfy, which is our primary motivation
for studying it). This leads to the following corollary of Theorem 3.1."
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.04106776180698152,"Corollary 3.1 (SimGolf for ExBMDPs). Consider the ExBMDP setting. Suppose that Assumption 3.3
holds, and let Q be constructed as in Lemma D.1 of Part II. Then for any ε,δ ∈(0,1), the policy
̂π = SimGolf(Q,SA,ε,δ) has J(π⋆) −J(̂π) ≤ε with probability at least 1 −δ. The total sample
complexity in the RLLS framework is N = ̃O(H5S3A3 log∣Φ∣⋅ε−4)."
IMPLICATIONS FOR EXOGENOUS BLOCK MDPS,0.04175222450376454,"This shows for the first time that general ExBMDPs are learnable with local simulator access. Prior
to this work, online RL algorithms for ExBMDPs required either (i) deterministic latent dynamics
[22], or (ii) factored emission structure [21]. Xie et al. [63] observed that ExBMDPs admit low
coverability, but their algorithm requires Bellman completeness, which is not satisfied by ExBMDPs
(see Islam et al. [30]). See Appendix A for more discussion."
COMPUTATIONALLY EFFICIENT LEARNING WITH LOCAL SIMULATORS,0.04243668720054757,"4
Computationally Efficient Learning with Local Simulators"
COMPUTATIONALLY EFFICIENT LEARNING WITH LOCAL SIMULATORS,0.043121149897330596,"Our result in Section 3 show that local simulator access facilitates sample-efficient learning in MDPs
with low coverability, a challenging setting that was previously out of reach. However, our algorithm
SimGolf is computationally-inefficient because it relies on global optimism, a drawback found in
most prior work on RL with general function approximation [31, 32, 18]. It remains an open question
whether any form of global optimism can be implemented efficiently, and some variants have provable
barriers to efficient implementation [14]."
COMPUTATIONALLY EFFICIENT LEARNING WITH LOCAL SIMULATORS,0.04380561259411362,"To address this drawback, in this section we present a new algorithm, RVFS (Recursive Value
Function Search; Algorithm 5), which requires stronger versions of the coverability and realizability
assumptions in Section 3, but is computationally efficient in the sense that it reduces to convex
optimization over the state-value function class V. RVFS makes use of a sophisticated recursive
exploration scheme based on core-sets, sidestepping the need for global optimism."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.044490075290896644,"4.1
Function Approximation and Statistical Assumptions
To begin, we require the following strengthening of the coverability assumption in Assumption 3.2."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.045174537987679675,"Assumption 4.1 (Pushforward coverability). The pushforward coverability coefficient Cpush > 0 is
given by Cpush = maxh∈[H] infµh∈∆(X) sup(xh−1,ah−1,xh)∈Xh−1×A×X
Th−1(xh∣xh−1,ah−1)"
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.0458590006844627,"µh(xh)
."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.04654346338124572,"Pushforward coverability is inspired by the pushforward concentrability condition used in offline RL
by [62, 26]. Concrete examples include, (i) Block MDPs with latent space S, which admit Cpush ≤∣S∣,
(ii) Low-Rank MDPs in dimension d, which admit Cpush ≤d [62], and (iii) Exogenous Block MDPs
for which the exogenous noise process satisfies a weak correlation condition that we introduce in
Appendix B. Note that Ccov ≤Cpush∣A∣, but the converse is not true in general."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.04722792607802875,"Instead of state-action value function approximation as in SimGolf, in this section we make use of a
state value function class V ⊂(X × [H] →[0,H]), but require somewhat stronger representation
conditions than in Section 3. We consider two complementary setups:"
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.04791238877481177,• Setup I: Assumptions 4.2 and 4.3 (V ⋆/π⋆-realizability) and Assumption 4.4 (∆-gap) hold.
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.0485968514715948,• Setup II: Assumption 4.5 (V π-realizability) and Assumption 4.6 (π-realizability) hold.
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.049281314168377825,We describe these assumptions in more detail below.
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.04996577686516085,"Function approximation setup I. First, instead of Q⋆-realizability, we consider the weaker V ⋆-
realizability [31, 57, 3]."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.05065023956194387,"Assumption 4.2 (V ⋆-realizability). For all h ∈[H], we have V ⋆
h ∈Vh."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.0513347022587269,"Under V ⋆-realizability, our algorithm learns a near-optimal policy, but the policy is non-executable
(cf. Definition 2.1); this property is shared by prior work on local simulator access with value function
realizability [57] . To produce executable policies, we additionally require access to a policy class
Π ⊂ΠS containing π⋆; we define Πh = {πh ∣π ∈Π}."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.05201916495550993,Assumption 4.3 (π⋆-realizability). The policy class Π contains the optimal policy π⋆.
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.05270362765229295,"V ⋆-realizability (Assumption 4.2) and π⋆-realizability (Assumption 4.3) are both implied by
Q⋆-realizability, and hence are weaker. However, we also assume the optimal Q-function admits
constant gap (this makes the representation conditions for Setup I incomparable to Assumption 3.1)."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.053388090349075976,"Assumption 4.4 (∆-Gap). The optimal action π⋆
h(x) is unique, and there exists ∆> 0 such that for
all h ∈[H], x ∈X, and a ∈A ∖{π⋆
h(x)}, Q⋆
h(x,π⋆
h(x)) > Q⋆
h(x,a) + ∆."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.054072553045859,"This condition has been used in a many prior works on computationally efficient RL with function
approximation [16, 17, 24, 56]."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.05475701574264202,"Function approximation setup II. We also provide guarantees under the assumption that the class
V satisfies all-policy realizability [59, 65, 60] in the sense that V π ∈V for all π ∈ΠS."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.055441478439425054,"Assumption 4.5 (V π-realizability). The class V = V1∶H has V π
h ∈Vh for all π ∈ΠS and h ∈[H]."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.05612594113620808,"This assumption will be sufficient to learn a non-executable policy, but to learn executable policies
we require an analogous strengthening of Assumption 4.5."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.0568104038329911,"Assumption 4.6 (π-realizability). For all π ∈ΠS, we have that x ↦arg maxa∈A Ph[V π
h+1](⋅,a) ∈Π."
FUNCTION APPROXIMATION AND STATISTICAL ASSUMPTIONS,0.057494866529774126,"This assumption has been used by a number of prior works on computationally efficient RL
[8, 44]. Assumptions 4.5 and 4.6 are both implied by the slightly simpler-to-state assumption of
Qπ-realizability [59, 65, 60], which asserts access to a class Q that contains Qπ for all π ∈ΠS."
ALGORITHM,0.05817932922655715,"4.2
Algorithm
For ease of exposition, we defer the full version of our algorithm, RVFS (Algorithm 5), to Appendix F
and present a simplified version here (Algorithm 2). The algorithms are nearly identical, except
that the simplified version assumes that certain quantities of interest (e.g., Bellman backups) can be
computed exactly, while the full version (provably) approximates them from samples."
ALGORITHM,0.05886379192334018,"RVFS maintains a value function estimator ̂V = ̂V1∶H that aims to approximate the optimal value
function V ⋆
1∶H, as well as core sets C1,...,CH of state-action pairs that are used to perform estimation
and guide exploration. At a high level, RVFS alternates between (i) fitting the value function ̂Vh for a
given layer h ∈[H] based on Monte-Carlo rollouts, and (ii) using the core-sets to test whether the
current value function estimates ̂Vh+1∶H remain accurate as the roll-in policy induced by ̂Vh changes."
ALGORITHM,0.059548254620123205,"In more detail, RVFS is based on recursion across the layers h ∈[H]. When invoked for layer h with
value function estimates ̂Vh+1∶H and core-sets Ch,...,CH, RVFSh performs two steps:"
ALGORITHM,0.06023271731690623,"1. For each state-action pair (xh−1,ah−1) ∈Ch,4 the algorithm gathers Ntest trajectories by
rolling out from (xh−1,ah−1) with the greedy policy ̂πℓ(x) ∈arg maxa∈A Pℓ[̂Vℓ+1](x,a)
that optimizes the estimated value function; in the full version of RVFS (see Algorithm 5),
we estimate the bellman backup Pℓ[̂Vℓ+1](x,a) using the local simulator.
For all states
xℓ−1 ∈{xh,...,xH−1} encountered during this process, the algorithm checks whether
∣E[̂Vℓ(xℓ) −V ⋆
ℓ(xℓ) ∣xℓ−1 = xℓ−1,aℓ−1 = aℓ−1]∣≲ε for all aℓ−1 ∈A using a test based on (im-
plicitly maintained) confidence sets. If the test fails, this indicates that distribution shift has
occurred, and the algorithm adds the pair (xℓ−1,aℓ−1) to Cℓand recurses on layer ℓvia RVFSℓ."
ALGORITHM,0.06091718001368925,"2. If all tests above pass, this means that ̂Vh+1,..., ̂VH are accurate, and no distribution shift has
occurred. In this case, the algorithm fits ̂Vh by collecting Monte-Carlo rollouts from all state-action
pairs in the core-set Ch with ̂πℓ(x) ∈arg maxa∈A Pℓ[̂Vℓ+1](x,a) (cf. Line 16), and returns."
ALGORITHM,0.061601642710472276,"When the tests in Item 1 succeed for all h ∈[H], the algorithm returns the estimated value functions
̂V1∶H; in this case, the greedy policy ̂πℓ(x) ∈arg maxa∈A Pℓ[̂Vℓ+1](x,a) is guaranteed to be near"
ALGORITHM,0.06228610540725531,"4Informally, Ch represents a collection of state-action pairs (xh−1, ah−1) at layer h −1 for which we want
E[∣̂Vh(xh) −V ⋆
h (xh)∣∣xh−1 = xh−1, ah−1 = ah−1] ≤ε for some small ε > 0."
ALGORITHM,0.06297056810403832,Algorithm 2 RVFSh: Recursive Value Function Search (Informal version of Algorithm 5)
ALGORITHM,0.06365503080082136,"1: parameters: Value function class V, suboptimality ε ∈(0,1), confidence δ ∈(0,1).
2:
input: Level h ∈[0..H], value functions ̂Vh+1∶H, confidence sets ̂Vh+1∶H, core-sets Ch∶H."
ALGORITHM,0.06433949349760439,"3: Initialize parameters M, Ntest, Nreg, ε2
reg, and β (see Algorithm 5 for parameter settings).
/* Test the fit for the estimated value functions ̂Vh+1∶H at future layers. */
4: for (xh−1,ah−1) ∈Ch and ℓ= H,...,h + 1 do
5:
for n = 1,...,Ntest do
6:
Draw xh ∼Th−1(⋅∣xh−1,ah−1), then draw xℓ−1 by rolling out with ̂πh∶H, where3"
ALGORITHM,0.0650239561943874,"∀τ ∈[H],
̂πτ(⋅) ∈arg maxa∈A Pτ[̂Vτ+1](⋅,a).
(2)"
ALGORITHM,0.06570841889117043,"7:
for aℓ−1 ∈A do"
ALGORITHM,0.06639288158795345,"/* Test fit; if test fails, re-fit value functions ̂Vh+1∶ℓup to layer ℓ. */
8:
if supf∈̂Vℓ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(xℓ−1,aℓ−1)∣> ε + ε ⋅β then
9:
Cℓ←Cℓ∪{(xℓ−1,aℓ−1)}.
10:
for τ = ℓ,...,h + 1 do
11:
(̂Vτ∶H, ̂Vh∶H,Cτ∶H) ←RVFSτ(̂Vτ+1∶H, ̂Vh+1∶H,Cτ∶H;V,ε,δ).
12:
go to line 4.
13: if h = 0 then return: (̂V1∶H,⋅,⋅,⋅)."
ALGORITHM,0.06707734428473648,"/* Re-fit ̂Vh and build a new confidence set. */
14: for (xh−1,ah−1) ∈Ch do
// Êπh∶H [∑H
ℓ=h rℓ∣xh] can be estimated using local simulator.
15:
Set Dh(xh−1,ah−1) ←∅. For i = 1,...,Nreg, sample xh ∼Th−1(⋅∣xh−1,ah−1) and update
Dh(xh−1,ah−1) ←Dh(xh−1,ah−1) ∪{(xh,Êπh∶H[∑H
ℓ=h rℓ∣xh])}."
ALGORITHM,0.06776180698151951,"16: Let ̂Vh ∶= arg minf∈̂V ∑(xh−1,ah−1)∈Ch ∑(xh,vh)∈Dh(xh−1,ah−1)(f(xh) −vh)2.
17: Compute value function confidence set:"
ALGORITHM,0.06844626967830253,"̂Vh ∶=
⎧⎪⎪⎨⎪⎪⎩
f ∈V
RRRRRRRRRRRR
∑
(xh−1,ah−1)∈Ch"
NREG,0.06913073237508556,"1
Nreg
∑
(xh,-)∈Dh(xh−1,ah−1)
(̂Vh(xh) −f(xh))
2 ≤ε2
reg"
NREG,0.06981519507186858,"⎫⎪⎪⎬⎪⎪⎭
."
NREG,0.07049965776865161,"18: return (̂Vh∶H, ̂Vh∶H,Ch∶H)."
NREG,0.07118412046543464,"optimal. The full version of RVFS in Algorithm 5 uses local simulator access to estimate the Bellman
backups Ph[̂Vh+1](x,a) for different state-action pairs (x,a). These backups are used to (i) compute
actions of the greedy policy that maximizes ̂V1∶H via (e.g., Eq. (2)); (ii) generate trajectories by rolling
out from state-action pairs in the core-sets (Line 6); and (iii) perform the test in Item 1 (Line 8)."
NREG,0.07186858316221766,"RVFS is inspired by the DMQ algorithm [16, 56] originally introduced in the context of online re-
inforcement learning with linearly realizable Q⋆. RVFS incorporates local simulator access (most
critically, via core-set construction) to allow for more general nonlinear function approximation with-
out restrictive statistical assumptions. Prior algorithms for RLLS have used core-sets of state-action
pairs in a similar fashion [40, 65, 59], but in a way that is tailored to linear function approximation."
NREG,0.07255304585900069,"In what follows, we discuss various features of the algorithm in greater detail."
NREG,0.0732375085557837,"Bellman backup policies.
Since RVFS works with state value functions instead of state-action
value functions, we need a way to extract policies from the former. The most natural way to
extract a policy from estimated value functions ̂V1∶H ∈V is as follows: for all h ∈[H], define
̂πh(x) ∈arg maxa∈A Ph[̂Vh+1](x,a). In reality, we do not have access to Ph[̂Vh+1](x,a) directly,
so the full version of RVFS (Algorithm 5) estimates this quantity on the fly using the local simu-
lator using the following scheme (Algorithm 7 in Appendix F): Given a state x, for each a, we
sample K rewards rh ∼Rh(x,a) and next-state transitions xh+1 ∼Th(⋅∣x,a), then approximate
Ph[̂Vh+1](x,a) by the empirical mean. We remark that the use of these Bellman backup policies
is actually crucial in the analysis for RVFS; even if we were to work with estimated state-action
value functions ̂Q1∶H instead, our analysis would require executing the Bellman backup policies
̂πh(x) ∈arg maxa∈A Th[ ̂Qh+1](x,a) (instead of naively using ̂πh(x) ∈arg maxa∈A ̂Qh(x,a))."
NREG,0.07392197125256673,Invoking the algorithm. The base invocation of RVFS takes the form
NREG,0.07460643394934977,"̂V1∶H ←RVFS0(̂V1∶H = arbitrary, ̂V1∶H = {Vh}H
h=1,C0∶H = {∅}H
h=0,;V,ε,δ)."
NREG,0.07529089664613278,"Whenever this call returns, the greedy policy induced by ̂V1∶H is guaranteed to be near-optimal.
Naively, the approximate Bellman backup policy induced by ̂V1∶H (described above) is non-executable,
and must be computed by invoking the local simulator. To provide an end-to-end guarantee to
learn an executable policy, we give an outer-level algorithm, RVFS.bc (Algorithm 6, deferred to
Appendix F for space), which invokes RVFS0, then extracts an executable policy from ̂V1∶H us-
ing behavior cloning. Subsequent recursive calls to RVFS take the form (̂Vh∶H, ̂Vh∶H,Ch∶H) ←
RVFSh(̂Vh+1∶H, ̂Vh+1∶H,Ch∶H;V,ε,δ).
The arguments here are: Importantly, the confidence sets
̂Vh+1∶H do not need to be explicitly maintained, and can be used implicitly whenever a regression
oracle for the value function class is available (discussed below)."
NREG,0.07597535934291581,"Remark 4.1 (Oracle-efficiency). RVFS is computationally efficient in the sense that it reduces to
convex optimization over the value function class V. In particular, the only computationally intensive
steps in the algorithm are (i) the regression step in Line 16, and (ii) the test in Line 8 involving
the confidence set ̂Vℓ. For the latter, we do not explicitly need to maintain ̂Vℓ, as the optimization
problem over this set in Line 8 (for the full version of RVFS in Algorithm 5) reduces to solving
arg maxV ∈V{±∑n
i=1 V (̃x(i)) ∣∑n
i=1(V (x(i)) −y(i))2 ≤β2} for a dataset {(x(i), ̃x(i),y(i))}n
i=1. This
is convex optimization problem in function space, and in particular can be implemented in a provably
efficient fashion whenever V is linearly parameterized. We expect that the problem can also be
reduced to a square loss regression by adapting the techniques in Krishnamurthy et al. [37], Foster
et al. [23], but we do not pursue this here."
MAIN RESULT,0.07665982203969883,"4.3
Main Result
We present the main guarantee for RVFS under the function approximation assumptions in Section 4.1."
MAIN RESULT,0.07734428473648186,"Theorem 4.1 (Main guarantee for RVFS). Let ε,δ ∈(0,1) be given, and suppose that Assumption 4.1
(pushforward coverability) holds with Cpush > 0. Further, suppose that one the following holds:"
MAIN RESULT,0.07802874743326489,"• Setup I: Assumptions 4.2 and 4.3 (V ⋆/π⋆-realizability) and Assumption 4.4 (∆-gap) hold,
and ε ≤6H ⋅∆."
MAIN RESULT,0.07871321013004791,• Setup II: Assumption 4.5 (V π-realizability) and Assumption 4.6 (π-realizability) hold.
MAIN RESULT,0.07939767282683094,"Then, RVFS.bc(Π,V,ε,δ) (Algorithm 6) returns a policy ̂π1∶H such that J(π⋆) −J(̂π1∶H) ≤2ε with
probability at least 1 −δ, and has total sample complexity bounded by"
MAIN RESULT,0.08008213552361396,"̃O (C8
pushH23A ⋅ε−13)."
MAIN RESULT,0.08076659822039699,"Furthermore, the algorithm makes at most poly(Cpush,H,A,ε−1) calls to the convex optimization
oracle over value function space described in Remark 4.1."
MAIN RESULT,0.08145106091718002,"Theorem 4.1 shows for the first time that sample- and computationally-efficient RL with local simula-
tor access is possible under pushforward coverability. In particular, RVFS is the first computationally
efficient algorithm for RL with local simulator access that supports nonlinear function approximation.
The assumptions in Theorem 4.1, while stronger than those in Section 3, are not known to enable
sample-efficient RL without simulator access. Nonetheless, understanding whether RVFS can be
strengthened to support general coverability or weaker function approximation is an important open
problem.
See Appendix H.1 for an overview of the analysis; we remark (Appendix I.1) that the
result is actually proven under slightly weaker assumptions than those in Setup I/Setup II."
MAIN RESULT,0.08213552361396304,"Connection to empirical algorithms. RVFS bears some similarity to Monte-Carlo Tree Search
(MCTS) [13, 35] and AlphaZero [52], which perform planning with local simulator. Informally,
MCTS can be viewed as a form of breadth-first search over the state space (where each node represents
a state at a given layer), and AlphaZero is a particular instantiation of a MCTS that leverages V −value
function approximation to allow for generalization across states. Compared to RVFS, MCTS and
AlphaZero perform exploration via simple bandit-style heuristics, and are not explicitly designed
to handle distribution shifts that arise in settings where actions have long-term downstream effects.
What is more, MCTS requires finite states to iterate over all possible child nodes of each state, making
it inapplicable in environments with continuous states. RVFS may be viewed as a provable counterpart"
MAIN RESULT,0.08281998631074607,"that can handle continuous states and uses function approximation to address distribution shift in a
principled fashion (in particular, through the use of confidence sets and the test in Line 14).5"
MAIN RESULT,0.08350444900752908,"Applying RVFS to Exogenous Block MDPs. ExBMDPs satisfy coverability (Assumption 3.2), but
do not satisfy the pushforward coverability assumption (Assumption 4.1) in general. However, it
turns out that ExBMDPs do satisfy pushforward coverability when the exogenous noise process is
weakly correlated across time, a new statistical assumption we refer to the weak correlation condition.
In Appendix B (Theorem B.1), we give a variant of RVFS for ExBMDPs that succeeds under (i) weak
correlation, and (ii) decoder realizability, sidestepping the need for the ∆-gap or V π-realizability."
DISCUSSION AND FUTURE WORK,0.08418891170431211,"5
Discussion and Future Work"
DISCUSSION AND FUTURE WORK,0.08487337440109514,"In this paper, we demonstrated that resets can substantially expand the range of reinforcement learning
(RL) settings that are tractable, both statistically and computationally. Our practical algorithm, RVFS,
provides a principled counterpart to MCTS by supporting continuous state spaces and offering
provable guarantees, setting it apart from traditional MCTS. Statistically, our results extend to MDPs
with a finite Sequential Estimation Coefficient (SEC) [63], capturing a broader class of MDPs
beyond those with finite coverability—encompassing low Bellman Eluder MDPs [32] and MDPs
with finite bilinear rank [18]. Although not formally developed here, it is possible to generalize
push-forward coverability and our analysis to encompass a range of linear function approximation
settings [57, 40, 3, 59, 65, 66], thereby recovering known positive results under local access in these
settings."
DISCUSSION AND FUTURE WORK,0.08555783709787816,"While our focus has been on theoretical contributions—analyzing the sample and computational
complexity of RL with access to a local simulator—this work also raises promising empirical
questions. We are particularly interested in exploring these questions in future work, aiming to bridge
these theoretical advances with empirical validation in practical RL settings."
DISCUSSION AND FUTURE WORK,0.08624229979466119,"5We note in passing that in the context of tree search, the pushforward coverability assumption (Assump-
tion 4.1) may be viewed as the stochastic analogue of branching factor."
REFERENCES,0.08692676249144421,References
REFERENCES,0.08761122518822724,"[1] A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. Schapire. Taming the monster: A fast
and simple algorithm for contextual bandits. In International Conference on Machine Learning,
pages 1638–1646, 2014."
REFERENCES,0.08829568788501027,"[2] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino,
M. Plappert, G. Powell, R. Ribas, et al. Solving rubik’s cube with a robot hand. arXiv preprint
arXiv:1910.07113, 2019."
REFERENCES,0.08898015058179329,"[3] P. Amortila, N. Jiang, D. Madeka, and D. P. Foster. A few expert queries suffices for sample-
efficient rl with resets and linear value approximation. Advances in Neural Information Process-
ing Systems, 35:29637–29648, 2022."
REFERENCES,0.08966461327857632,"[4] P. Amortila, D. J. Foster, N. Jiang, A. Sekhari, and T. Xie. Harnessing density ratios for online
reinforcement learning. International Conference on Learning Representations (ICLR), 2024."
REFERENCES,0.09034907597535935,"[5] P. Amortila, D. J. Foster, and A. Krishnamurthy. Scalable online exploration via coverability.
arXiv preprint arXiv:2403.06571, 2024."
REFERENCES,0.09103353867214237,"[6] S. Aradi. Survey of deep reinforcement learning for motion planning of autonomous vehicles.
IEEE Transactions on Intelligent Transportation Systems, 23(2):740–759, 2020."
REFERENCES,0.0917180013689254,"[7] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, Z. D. Guo, and C. Blundell.
Agent57: Outperforming the atari human benchmark. In International conference on machine
learning, pages 507–517. PMLR, 2020."
REFERENCES,0.09240246406570841,"[8] J. Bagnell, S. M. Kakade, J. Schneider, and A. Ng. Policy search by dynamic programming.
Advances in neural information processing systems, 16, 2003."
REFERENCES,0.09308692676249145,"[9] M. Bellemare, J. Veness, and M. Bowling. Investigating contingency awareness using atari 2600
games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pages
864–871, 2012."
REFERENCES,0.09377138945927448,"[10] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel,
M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316, 2016."
REFERENCES,0.0944558521560575,"[11] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.
Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.09514031485284052,"[12] J. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning, pages 1042–1051. PMLR, 2019."
REFERENCES,0.09582477754962354,"[13] R. Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In Interna-
tional conference on computers and games, pages 72–83. Springer, 2006."
REFERENCES,0.09650924024640657,"[14] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracle-
efficient PAC RL with rich observations. In Advances in neural information processing systems,
pages 1422–1432, 2018."
REFERENCES,0.0971937029431896,"[15] S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient
RL with rich observations via latent state decoding. In International Conference on Machine
Learning, pages 1665–1674. PMLR, 2019."
REFERENCES,0.09787816563997262,"[16] S. S. Du, Y. Luo, R. Wang, and H. Zhang. Provably efficient Q-learning with function ap-
proximation via distribution shift error checking oracle. In Advances in Neural Information
Processing Systems, pages 8060–8070, 2019."
REFERENCES,0.09856262833675565,"[17] S. S. Du, S. M. Kakade, R. Wang, and L. F. Yang. Is a good representation sufficient for sample
efficient reinforcement learning? In International Conference on Learning Representations,
2020."
REFERENCES,0.09924709103353867,"[18] S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear
classes: A structural framework for provable generalization in RL. International Conference on
Machine Learning, 2021."
REFERENCES,0.0999315537303217,"[19] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach
for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019."
REFERENCES,0.10061601642710473,"[20] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. First return, then explore.
Nature, 590(7847):580–586, 2021."
REFERENCES,0.10130047912388775,"[21] Y. Efroni, D. J. Foster, D. Misra, A. Krishnamurthy, and J. Langford. Sample-efficient reinforce-
ment learning in the presence of exogenous information. In Conference on Learning Theory,
pages 5062–5127. PMLR, 2022."
REFERENCES,0.10198494182067078,"[22] Y. Efroni, D. Misra, A. Krishnamurthy, A. Agarwal, and J. Langford. Provably filtering
exogenous distractors using multistep inverse dynamics.
In International Conference on
Learning Representations, 2022."
REFERENCES,0.1026694045174538,"[23] D. J. Foster, A. Agarwal, M. Dudík, H. Luo, and R. E. Schapire. Practical contextual bandits
with regression oracles. International Conference on Machine Learning, 2018."
REFERENCES,0.10335386721423682,"[24] D. J. Foster, A. Rakhlin, D. Simchi-Levi, and Y. Xu. Instance-dependent complexity of
contextual bandits and reinforcement learning: A disagreement-based perspective. Conference
on Learning Theory (COLT), 2020."
REFERENCES,0.10403832991101986,"[25] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive
decision making. arXiv preprint arXiv:2112.13487, 2021."
REFERENCES,0.10472279260780287,"[26] D. J. Foster, A. Krishnamurthy, D. Simchi-Levi, and Y. Xu. Offline reinforcement learning:
Fundamental barriers for value function approximation. In Conference on Learning Theory,
pages 3489–3489. PMLR, 2022."
REFERENCES,0.1054072553045859,"[27] Z. Guo, S. Thakoor, M. Pîslar, B. Avila Pires, F. Altché, C. Tallec, A. Saade, D. Calandriello,
J.-B. Grill, Y. Tang, et al. Byol-explore: Exploration by bootstrapped prediction. Advances in
neural information processing systems, 35:31855–31870, 2022."
REFERENCES,0.10609171800136892,"[28] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13–30, 1963."
REFERENCES,0.10677618069815195,"[29] A. Huang, J. Chen, and N. Jiang. Reinforcement learning in low-rank mdps with density
features. International Conference on Machine Learning (ICML), 2023."
REFERENCES,0.10746064339493498,"[30] R. Islam, M. Tomar, A. Lamb, Y. Efroni, H. Zang, A. Didolkar, D. Misra, X. Li, H. van
Seijen, R. T. d. Combes, et al. Agent-controller representations: Principled offline rl with rich
exogenous information. International Conference on Machine Learning (ICML), 2023."
REFERENCES,0.108145106091718,"[31] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision
processes with low Bellman rank are PAC-learnable. In International Conference on Machine
Learning, pages 1704–1713, 2017."
REFERENCES,0.10882956878850103,"[32] C. Jin, Q. Liu, and S. Miryoosefi. Bellman eluder dimension: New rich classes of RL problems,
and sample-efficient algorithms. Neural Information Processing Systems, 2021."
REFERENCES,0.10951403148528405,"[33] S. M. Kakade. On the sample complexity of reinforcement learning. University of London,
University College London (United Kingdom), 2003."
REFERENCES,0.11019849418206708,"[34] M. Kearns and S. Singh. Finite-sample convergence rates for q-learning and indirect algorithms.
Advances in neural information processing systems, 11, 1998."
REFERENCES,0.11088295687885011,"[35] L. Kocsis and C. Szepesvári. Bandit based monte-carlo planning. In European conference on
machine learning, pages 282–293. Springer, 2006."
REFERENCES,0.11156741957563313,"[36] A. Krishnamurthy, A. Agarwal, and J. Langford. PAC reinforcement learning with rich observa-
tions. In Advances in Neural Information Processing Systems, pages 1840–1848, 2016."
REFERENCES,0.11225188227241616,"[37] A. Krishnamurthy, A. Agarwal, T.-K. Huang, H. Daumé III, and J. Langford. Active learning
for cost-sensitive classification. In International Conference on Machine Learning, pages
1915–1924, 2017."
REFERENCES,0.11293634496919917,"[38] A. Lamb, R. Islam, Y. Efroni, A. R. Didolkar, D. Misra, D. J. Foster, L. P. Molu, R. Chari,
A. Krishnamurthy, and J. Langford. Guaranteed discovery of control-endogenous latent states
with multi-step inverse models. Transactions on Machine Learning Research, 2023."
REFERENCES,0.1136208076659822,"[39] T. Lattimore, C. Szepesvari, and G. Weisz. Learning with good feature representations in
bandits and in rl with a generative model. In International Conference on Machine Learning,
pages 5662–5670. PMLR, 2020."
REFERENCES,0.11430527036276524,"[40] G. Li, Y. Chen, Y. Chi, Y. Gu, and Y. Wei. Sample-efficient reinforcement learning is feasible
for linearly realizable mdps with limited revisiting. Advances in Neural Information Processing
Systems, 34:16671–16685, 2021."
REFERENCES,0.11498973305954825,"[41] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic mle: A generic model-based
algorithm for partially observable sequential decision making. In Proceedings of the 55th
Annual ACM Symposium on Theory of Computing, pages 363–376, 2023."
REFERENCES,0.11567419575633128,"[42] Z. Mhammedi, D. J. Foster, and A. Rakhlin. Representation learning with multi-step inverse
kinematics: An efficient and optimal approach to rich-observation rl. International Conference
on Machine Learning (ICML), 2023."
REFERENCES,0.1163586584531143,"[43] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and
provably efficient rich-observation reinforcement learning. arXiv preprint arXiv:1911.05815,
2019."
REFERENCES,0.11704312114989733,"[44] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and
provably efficient rich-observation reinforcement learning. In International conference on
machine learning, pages 6961–6971. PMLR, 2020."
REFERENCES,0.11772758384668036,"[45] M. A. Qassem, I. Abuhadrous, and H. Elaydi. Modeling and simulation of 5 dof educational
robot arm. In 2010 2nd International Conference on Advanced Computer Control, volume 5,
pages 569–574. IEEE, 2010."
REFERENCES,0.11841204654346338,"[46] S. Ross and D. Bagnell. Efficient reductions for imitation learning. In Proceedings of the
thirteenth international conference on artificial intelligence and statistics, pages 661–668.
JMLR Workshop and Conference Proceedings, 2010."
REFERENCES,0.11909650924024641,"[47] T. Salimans and R. Chen. Learning montezuma’s revenge from a single demonstration. arXiv
preprint arXiv:1812.03381, 2018."
REFERENCES,0.11978097193702943,"[48] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock-
hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a
learned model. Nature, 588(7839):604–609, 2020."
REFERENCES,0.12046543463381246,"[49] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization.
In International conference on machine learning, pages 1889–1897. PMLR, 2015."
REFERENCES,0.12114989733059549,"[50] A. Sidford, M. Wang, X. Wu, L. Yang, and Y. Ye. Near-optimal time and sample complexities
for solving markov decision processes with a generative model. Advances in Neural Information
Processing Systems, 31, 2018."
REFERENCES,0.1218343600273785,"[51] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.12251882272416154,"[52] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess,
shogi, and go through self-play. Science, 362(6419):1140–1144, 2018."
REFERENCES,0.12320328542094455,"[53] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki,
J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018."
REFERENCES,0.12388774811772758,"[54] A. Tavakoli, V. Levdik, R. Islam, C. M. Smith, and P. Kormushev. Exploring restart distributions.
arXiv preprint arXiv:1811.11298, 2018."
REFERENCES,0.12457221081451061,"[55] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ international conference on intelligent robots and systems, pages 5026–5033. IEEE,
2012."
REFERENCES,0.12525667351129363,"[56] Y. Wang, R. Wang, and S. M. Kakade. An exponential lower bound for linearly-realizable
MDPs with constant suboptimality gap. Neural Information Processing Systems (NeurIPS),
2021."
REFERENCES,0.12594113620807665,"[57] G. Weisz, P. Amortila, B. Janzer, Y. Abbasi-Yadkori, N. Jiang, and C. Szepesvári. On query-
efficient planning in mdps under linear realizability of the optimal state-value function. In
Conference on Learning Theory, pages 4355–4385. PMLR, 2021."
REFERENCES,0.1266255989048597,"[58] G. Weisz, P. Amortila, and C. Szepesvári. Exponential lower bounds for planning in MDPs
with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory, pages
1237–1264. PMLR, 2021."
REFERENCES,0.1273100616016427,"[59] G. Weisz, A. György, T. Kozuno, and C. Szepesvári. Confident approximate policy iteration
for efficient local planning in qπ-realizable mdps. Advances in Neural Information Processing
Systems, 35:25547–25559, 2022."
REFERENCES,0.12799452429842573,"[60] G. Weisz, A. György, and C. Szepesvári. Online rl in linearly qπ-realizable mdps is as easy as
in linear mdps if you learn what to ignore. arXiv preprint arXiv:2310.07811, 2023."
REFERENCES,0.12867898699520877,"[61] Z. Wen and B. Van Roy. Efficient reinforcement learning in deterministic systems with value
function generalization. Mathematics of Operations Research, 42(3):762–782, 2017."
REFERENCES,0.1293634496919918,"[62] T. Xie and N. Jiang. Batch value-function approximation with only realizability. In International
Conference on Machine Learning, pages 11404–11413. PMLR, 2021."
REFERENCES,0.1300479123887748,"[63] T. Xie, D. J. Foster, Y. Bai, N. Jiang, and S. M. Kakade. The role of coverage in online
reinforcement learning. In The Eleventh International Conference on Learning Representations,
2023."
REFERENCES,0.13073237508555785,"[64] L. Yang and M. Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning, pages 6995–7004. PMLR, 2019."
REFERENCES,0.13141683778234087,"[65] D. Yin, B. Hao, Y. Abbasi-Yadkori, N. Lazi´c, and C. Szepesvári. Efficient local planning with
linear function approximation. In International Conference on Algorithmic Learning Theory,
pages 1165–1192. PMLR, 2022."
REFERENCES,0.13210130047912388,"[66] D. Yin, S. Thiagarajan, N. Lazic, N. Rajaraman, B. Hao, and C. Szepesvari. Sample efficient
deep reinforcement learning via local planning. arXiv preprint arXiv:2301.12579, 2023."
REFERENCES,0.1327857631759069,"[67] A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies
with low inherent bellman error. In International Conference on Machine Learning, pages
10978–10989. PMLR, 2020."
REFERENCES,0.13347022587268995,"[68] X. Zhang, Y. Song, M. Uehara, M. Wang, A. Agarwal, and W. Sun. Efficient reinforcement
learning in block mdps: A model-free representation learning approach. In International
Conference on Machine Learning, pages 26517–26547. PMLR, 2022."
REFERENCES,0.13415468856947296,NeurIPS Paper Checklist
CLAIMS,0.13483915126625598,1. Claims
CLAIMS,0.13552361396303902,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?"
CLAIMS,0.13620807665982204,Answer: [Yes]
CLAIMS,0.13689253935660506,Justification: The main claims do accurately reflect the paper’s contribution.
CLAIMS,0.1375770020533881,Guidelines:
CLAIMS,0.13826146475017112,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper."
CLAIMS,0.13894592744695414,"• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers."
CLAIMS,0.13963039014373715,"• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings."
CLAIMS,0.1403148528405202,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper."
LIMITATIONS,0.14099931553730322,2. Limitations
LIMITATIONS,0.14168377823408623,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.14236824093086928,Answer: [Yes]
LIMITATIONS,0.1430527036276523,"Justification: Yes, see the discussion section."
LIMITATIONS,0.1437371663244353,Guidelines:
LIMITATIONS,0.14442162902121836,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper."
LIMITATIONS,0.14510609171800137,"• The authors are encouraged to create a separate ""Limitations"" section in their paper."
LIMITATIONS,0.1457905544147844,"• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be."
LIMITATIONS,0.1464750171115674,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated."
LIMITATIONS,0.14715947980835045,"• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon."
LIMITATIONS,0.14784394250513347,"• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size."
LIMITATIONS,0.14852840520191649,"• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness."
LIMITATIONS,0.14921286789869953,"• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.14989733059548255,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.15058179329226556,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.1512662559890486,Answer: [Yes]
THEORY ASSUMPTIONS AND PROOFS,0.15195071868583163,"Justification: We provide a proof-sketch in the main paper and detailed proofs in the
appendix."
THEORY ASSUMPTIONS AND PROOFS,0.15263518138261464,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.15331964407939766,• The answer NA means that the paper does not include theoretical results.
THEORY ASSUMPTIONS AND PROOFS,0.1540041067761807,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced."
THEORY ASSUMPTIONS AND PROOFS,0.15468856947296372,• All assumptions should be clearly stated or referenced in the statement of any theorems.
THEORY ASSUMPTIONS AND PROOFS,0.15537303216974674,"• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition."
THEORY ASSUMPTIONS AND PROOFS,0.15605749486652978,"• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material."
THEORY ASSUMPTIONS AND PROOFS,0.1567419575633128,• Theorems and Lemmas that the proof relies upon should be properly referenced.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.15742642026009582,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.15811088295687886,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.15879534565366188,Answer: [NA]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.1594798083504449,Justification: [NA]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.1601642710472279,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16084873374401096,• The answer NA means that the paper does not include experiments.
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16153319644079397,"• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.162217659137577,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16290212183436004,"• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16358658453114305,"• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16427104722792607,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16495550992470911,"(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16563997262149213,"(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.16632443531827515,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.16700889801505817,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.1676933607118412,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.16837782340862423,Answer: [NA]
OPEN ACCESS TO DATA AND CODE,0.16906228610540724,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
OPEN ACCESS TO DATA AND CODE,0.1697467488021903,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.1704312114989733,• The answer NA means that paper does not include experiments requiring code.
OPEN ACCESS TO DATA AND CODE,0.17111567419575632,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details."
OPEN ACCESS TO DATA AND CODE,0.17180013689253937,"• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark)."
OPEN ACCESS TO DATA AND CODE,0.17248459958932238,"• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details."
OPEN ACCESS TO DATA AND CODE,0.1731690622861054,"• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc."
OPEN ACCESS TO DATA AND CODE,0.17385352498288842,"• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why."
OPEN ACCESS TO DATA AND CODE,0.17453798767967146,"• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable)."
OPEN ACCESS TO DATA AND CODE,0.17522245037645448,"• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.1759069130732375,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.17659137577002054,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.17727583846680356,Answer: [NA]
OPEN ACCESS TO DATA AND CODE,0.17796030116358658,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
OPEN ACCESS TO DATA AND CODE,0.17864476386036962,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.17932922655715264,• The answer NA means that the paper does not include experiments.
OPEN ACCESS TO DATA AND CODE,0.18001368925393565,"• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them."
OPEN ACCESS TO DATA AND CODE,0.1806981519507187,"• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.18138261464750172,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.18206707734428473,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.18275154004106775,Answer: [NA]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.1834360027378508,Justification: [NA]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.1841204654346338,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.18480492813141683,• The answer NA means that the paper does not include experiments.
EXPERIMENT STATISTICAL SIGNIFICANCE,0.18548939082819987,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.1861738535249829,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.1868583162217659,"• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.18754277891854895,"• The assumptions made should be given (e.g., Normally distributed errors)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.18822724161533197,"• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.188911704312115,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.189596167008898,"• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates)."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.19028062970568105,"• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.19096509240246407,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.19164955509924708,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.19233401779603013,Answer: [NA]
EXPERIMENTS COMPUTE RESOURCES,0.19301848049281314,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
EXPERIMENTS COMPUTE RESOURCES,0.19370294318959616,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.1943874058863792,• The answer NA means that the paper does not include experiments.
EXPERIMENTS COMPUTE RESOURCES,0.19507186858316222,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage."
EXPERIMENTS COMPUTE RESOURCES,0.19575633127994524,"• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute."
EXPERIMENTS COMPUTE RESOURCES,0.19644079397672826,"• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.1971252566735113,9. Code Of Ethics
CODE OF ETHICS,0.19780971937029432,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.19849418206707733,Answer: [Yes]
CODE OF ETHICS,0.19917864476386038,Justification: This paper does conform to the code of ethics.
CODE OF ETHICS,0.1998631074606434,Guidelines:
CODE OF ETHICS,0.2005475701574264,• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
CODE OF ETHICS,0.20123203285420946,"• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics."
CODE OF ETHICS,0.20191649555099248,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.2026009582477755,10. Broader Impacts
BROADER IMPACTS,0.2032854209445585,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.20396988364134155,Answer: [No]
BROADER IMPACTS,0.20465434633812457,"Justification: This paper provides a purely mathematical contribution. As such, it is subject
to the standard ethical concerns present for all mathematical papers, but no further ones."
BROADER IMPACTS,0.2053388090349076,Guidelines:
BROADER IMPACTS,0.20602327173169063,• The answer NA means that there is no societal impact of the work performed.
BROADER IMPACTS,0.20670773442847365,"• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact."
BROADER IMPACTS,0.20739219712525667,"• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations."
BROADER IMPACTS,0.2080766598220397,"• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster."
BROADER IMPACTS,0.20876112251882273,"• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology."
BROADER IMPACTS,0.20944558521560575,"• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.21013004791238876,11. Safeguards
SAFEGUARDS,0.2108145106091718,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.21149897330595482,Answer: [NA]
SAFEGUARDS,0.21218343600273784,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
SAFEGUARDS,0.21286789869952089,Guidelines:
SAFEGUARDS,0.2135523613963039,• The answer NA means that the paper poses no such risks.
SAFEGUARDS,0.21423682409308692,"• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters."
SAFEGUARDS,0.21492128678986996,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images."
SAFEGUARDS,0.21560574948665298,"• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.216290212183436,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.21697467488021902,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.21765913757700206,Answer: [NA]
LICENSES FOR EXISTING ASSETS,0.21834360027378508,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
LICENSES FOR EXISTING ASSETS,0.2190280629705681,Guidelines:
LICENSES FOR EXISTING ASSETS,0.21971252566735114,• The answer NA means that the paper does not use existing assets.
LICENSES FOR EXISTING ASSETS,0.22039698836413416,• The authors should cite the original paper that produced the code package or dataset.
LICENSES FOR EXISTING ASSETS,0.22108145106091717,"• The authors should state which version of the asset is used and, if possible, include a
URL."
LICENSES FOR EXISTING ASSETS,0.22176591375770022,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset."
LICENSES FOR EXISTING ASSETS,0.22245037645448323,"• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided."
LICENSES FOR EXISTING ASSETS,0.22313483915126625,"• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset."
LICENSES FOR EXISTING ASSETS,0.22381930184804927,"• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided."
LICENSES FOR EXISTING ASSETS,0.2245037645448323,"• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.22518822724161533,13. New Assets
NEW ASSETS,0.22587268993839835,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.2265571526351814,Answer: [NA]
NEW ASSETS,0.2272416153319644,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
NEW ASSETS,0.22792607802874743,Guidelines:
NEW ASSETS,0.22861054072553047,• The answer NA means that the paper does not release new assets.
NEW ASSETS,0.2292950034223135,"• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc."
NEW ASSETS,0.2299794661190965,"• The paper should discuss whether and how consent was obtained from people whose
asset is used."
NEW ASSETS,0.23066392881587952,"• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23134839151266257,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23203285420944558,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2327173169062286,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23340177960301164,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23408624229979466,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23477070499657768,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23545516769336072,"• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23613963039014374,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23682409308692676,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2375085557837098,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23819301848049282,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23887748117727584,"Justification: This paper has only mathematical congtent. There are no experiments in this
paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.23956194387405885,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2402464065708419,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24093086926762491,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24161533196440793,"• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24229979466119098,"• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.242984257357974,Contents of Appendix
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.243668720054757,"A Additional Related Work
23"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24435318275154005,"I
Additional Results
25"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24503764544832307,"B Applying RVFS to Exogenous Block MDPs
25"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2457221081451061,"C Helper Lemmas
28
C.1
Concentration and Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2
Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.3
Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2464065708418891,"II
Proofs for SimGolf (Section 3)
33"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24709103353867215,"D Preliminary Lemmas for Proof of Theorem 3.1
33"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24777549623545517,"E
Proof of Theorem 3.1
35"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24845995893223818,"III
Proofs for RVFS (Section 4)
37"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24914442162902123,"F
Full Version of RVFS
37
F.1
RVFS Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.2
RVFSexo Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.24982888432580425,"G Organization
43"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.25051334702258726,"H Overview of Analysis and Preliminaries
43
H.1
Overview of Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
H.2
Benchmark Policy Class and Randomized Policies . . . . . . . . . . . . . . . . . . . . 44
H.3 Additional Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2511978097193703,"I
Guarantee under V π-Realizability (Proof of Theorem 4.1, Setup II)
46
I.1
Analysis: Proof of Theorem 4.1 (Setup II)
. . . . . . . . . . . . . . . . . . . . . . . . 46
I.2
Proof of Lemma I.1 (Number of Test Failures) . . . . . . . . . . . . . . . . . . . . . . 48
I.3
Proof of Lemma I.2 (Consequence of Passing the Tests) . . . . . . . . . . . . . . . . . 50
I.4
Proof of Lemma I.3 (Value Function Regression Guarantee)
. . . . . . . . . . . . . . 52
I.5
Proof of Lemma I.4 (Guarantee for Confidence Sets) . . . . . . . . . . . . . . . . . . . 54
I.6
Proof of Theorem I.1 (Main Guarantee of RVFS)
. . . . . . . . . . . . . . . . . . . . . 57
I.7
Proof of Theorem I.2 (Guarantee of RVFS.bc) . . . . . . . . . . . . . . . . . . . . . . . 58"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2518822724161533,"J
Guarantee under V ⋆-Realizability (Proof of Theorem 4.1, Setup I)
59
J.1
Analysis: Proof of Theorem 4.1 (Setup I) . . . . . . . . . . . . . . . . . . . . . . . . . 59
J.2
Proof of Lemma J.1 (Relaxed V π-Realizability under Gap) . . . . . . . . . . . . . . . 60"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.25256673511293637,"K Guarantee for Weakly Correlated ExBMDPs (Proof of Theorem B.1)
61
K.1 Analysis: Proof of Theorem B.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
K.2
Proof of Lemma K.1 (Endogenous Benchmark Policies) . . . . . . . . . . . . . . . . . 65
K.3
Proof of Lemma K.2 (Snapping Probability) . . . . . . . . . . . . . . . . . . . . . . . . 65
K.4
Proof of Lemma K.3 (Coverability in Weakly Correlated ExBMDP) . . . . . . . . . . 67
K.5
Proof of Lemma K.6 (Confidence Sets)
. . . . . . . . . . . . . . . . . . . . . . . . . . 67
K.6
Proof of Lemma K.7 (Main Guarantee of RVFSexo) . . . . . . . . . . . . . . . . . . . . 69"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2532511978097194,"L Additional Technical Lemmas
70"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2539356605065024,"M BehaviorCloning Algorithm and Analysis
73"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2546201232032854,"A
Additional Related Work"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.25530458590006844,"Local simulators: Theoretical research.
RL with local simulators has received extensive interest in
the context of linear function approximation. Most notably, Weisz et al. [57] show that reinforcement
learning with linear V ⋆is tractable with local simulator access, and Li et al. [40] show that RL with
linear Q⋆and a state-action gap is tractable; online RL is known to be intractable under the same
assumptions [57, 56]. Amortila et al. [3] show that the gap assumption can be removed if a small
number of expert queries are available. Also of note are the works of Yin et al. [65], Weisz et al. [59],
which give computationally efficient algorithms under linear Qπ-realizability for all π; this setting
is known to be tractable in the online RL model [60], but computationally efficient algorithms are
currently only known for RLLS."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.25598904859685145,"Global simulators—in which the agent can query arbitrary state-action pairs and observe next state
transitions—have also received theoretical investigation, but like local simulators, results are largely
restricted to tabular reinforcement learning and linear models [34, 33, 50, 17, 64, 39]."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.25667351129363447,"Local simulators: Empirical research.
The Go-Explore algorithm [19, 20] uses local simulator
access to achieve state-of-the-art performance for the Atari games Montezuma’s Revenge and Pitfall—
both notoriously difficult games that require systematic exploration. To the best of our knowledge, the
performance of Go-Explore on these tasks has yet to be matched by online reinforcement learning;
the performing agents [7, 27] are roughly a factor of four worse in terms of cumulative reward.
Interestingly, like RVFS, Go-Explore makes use of core sets of informative state-action pairs to guide
exploration. However, Go-Explore uses an ad-hoc, domain specific approach to designing the core
set, and does not use function approximation to drive exploration."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.25735797399041754,"Recent work of Yin et al. [66] provides an empirical framework for online RL with local planning
that can take advantage of deep neural function approximation, and is inspired by the theoretical
works in Weisz et al. [57], Li et al. [40], Yin et al. [65], Weisz et al. [59]. This approach does not
have provable guarantees, but achieves super-human performance at Montezuma’s Revenge."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.25804243668720056,"Other notable empirical works that incorporate local simulator access, as highlighted by Yin et al.
[66], include Schulman et al. [49], Salimans and Chen [47], Tavakoli et al. [54]."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2587268993839836,"Planning.
RL with local simulator access is a convenient abstraction for the problem of planning:
Given a known (e.g., learned) model, compute an optimal policy. Planning with a learned model
is an important task in theory [25, 41] and practice (e.g., MuZero [48]). Since the model is known,
computing an optimal policy is a purely computational problem, not a statistical problem. Nonethe-
less, for planning problems in large state spaces, where enumerating over all states is undesirable,
algorithms for online RL with local simulator access can be directly applied, treating the model as
if it were the environment the agent is interacting with. Here, any computationally efficient RLLS
algorithm immediately yields an efficient algorithm for planning."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2594113620807666,"Empirically, Monte-Carlo Tree Search [13, 35] is a successful paradigm for planning, acting as a
key component in AlphaGo [51] and AlphaZero [52].6 Viewed as a planning algorithm, a potential
advantage of RVFS is that it is well suited to stochastic environments, and provides a principled way
to use estimated (neural) value function estimates to guide exploration."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2600958247775496,"Coverability.
Xie et al. [63] introduced coverability as a structural parameter for online reinforce-
ment leanring, inspired by connections between online and offline RL. Existing guarantees for the
online RL framework based on coverability require either Bellman completeness [63], model-based
realizability [5], or weight function realizability [4, 5]), and it is not currently known whether value
function realizability is sufficient in this framework."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.26078028747433263,"Exogenous Block MDPs.
Our results in Section 3.3 (Corollary 3.1) show that general Exogenous
Block MDPs are learnable with local simulator access. Prior work, on learning EXBMDPs in the
online RL model requires additional assumptions:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2614647501711157,"• Deterministic ExBMDP [22]. In this setting, the latent transition distribution T endo is
assumed to be deterministic. In this case, it suffices to learn open-loop policies (i.e., policies
that play a deterministic sequence of actions). This avoids compounding errors due to"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2621492128678987,"6Compare to our work, a small difference is that these works are not concerned with producing executable
policies, c.f. Definition 2.1."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.26283367556468173,"learning imperfect decoders that depend on the exogenous noise, making this setting much
less challenging than the general ExBMDP setting."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.26351813826146475,"• Factored ExMDP [21]. This is an ExBMDP setting with a restrictive structure in which
the observation is a d-dimensional vector and the latent state is a k-dimensional subset of
the observed coordinates. This structure prevents the setting from subsuming the basic
(non-exogenous) Block MDP framework, and makes it possible to learn decoders that act
only on the endogenous state, preventing compounding errors."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.26420260095824777,"• Bellman completeness. Xie et al. [63] observed that ExBMDPs admit low coverability, but
their algorithm requires Bellman completeness, which is not satisfied by ExBMDPs (see
Efroni et al. [22], Islam et al. [30])."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2648870636550308,"Part I
Additional Results"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2655715263518138,"This section of the appendix contains additional results omitted from the main body due to space
constraints."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2662559890485969,"B
Applying RVFS to Exogenous Block MDPs"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2669404517453799,"We now apply RVFS to the Exogenous Block MDP (ExBMDP) model introduced in Section 3.3.
ExBMDPs satisfy coverability (Assumption 3.2), but do not satisfy the pushforward coverability
assumption (Assumption 4.1) required by RVFS in general. However, it turns out that ExBMDPs do
satisfy pushforward coverability when the exogenous noise process is weakly correlated across time;
we refer to this new statistical assumption as the weak correlation condition."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2676249144421629,"Assumption B.1 (Weak correlation condition). For the underlying ExBMDP M, there is a constant
Cexo ≥1 such that for all h ∈[H −1] and (ξ,ξ′) ∈Ξh−1 × Ξh, we have7"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2683093771389459,"P[ξh = ξ,ξh+1 = ξ′] ≤Cexo ⋅P[ξh = ξ] ⋅P[ξh+1 = ξ′]."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.26899383983572894,"The weak correlation property asserts that the joint law for the exogenous noise variables ξh and ξh+1
is at most a multiplicative factor Cexo ≥1 larger than the corresponding product distribution obtained
by sampling ξh and ξh+1 independently from their marginals. This setting strictly generalizes the
(non-exogenous) Block MDP model [36, 15, 43, 68, 42], by allowing for arbitrary stochastic dynamics
for the endogenous state and an arbitrary emission process, but requires that temporal correlations in
the exogenous noise decay over time."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.26967830253251196,"We show that under Assumption B.1, pushforward coverability is satisfied with Cpush ≤Cexo ⋅SA
(Lemma K.3 in Appendix K.1). In addition, V ⋆-realizability is implied by decoder realizability
(Lemma D.1). Thus, by applying Theorem 4.1 (Setup I), we conclude that RVFS efficiently learns
a near-optimal policy for any weakly correlated ExBMDP for which the optimal value function has
∆-gap."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.270362765229295,"An improved algorithm for ExBMDPs: RVFSexo.
At first glance, removing the gap assumption for
RVFS in ExBMDPs seems difficult: The V π-realizability assumption required to invoke Theorem 4.1
(Setup II) is not satisfied by ExBMDPs, as decoder realizability only implies V π realizability for
endogenous policies π.8 In spite of this, we now show that with a slight modification, RVFS can
efficiently learn any weakly correlated ExBMDP under decoder realizability alone (without gap or
V π-realizability)."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.27104722792607805,"Our new variant of RVFS, RVFSexo, is presented in Algorithm 8 (deferred to Appendix F for space).
The algorithm is almost identical to RVFS (Algorithm 5), with the main difference being that we
use an additional randomized rounding step to compute the policies ̂π1∶H from the learned value
functions ̂V1∶H. In particular, instead of directly defining the policies ̂π1∶H based on the bellman
backups Ph[̂Vh+1] as in Eq. (14), RVFSexo targets a “rounded” version of the backup given by"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.27173169062286107,"ε ⋅⌈Ph[̂Vh+1](x,a)/ε + ζh⌉,
(3)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2724161533196441,"where ε ∈(0,1) is a rounding parameter and ζ1,...,ζH are i.i.d. random variables sampled uniformly
at random from the interval [0,1/2] (at the beginning of the algorithm’s execution). Concretely,
RVFSexo estimates the bellman backup Ph[̂Vh+1](x,a) in Eq. (3) using the local simulator (as in
Eq. (14) of Algorithm 5), and defines its policies via"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2731006160164271,"̂πh(⋅) ∈arg max
a∈A
⌈Ph[̂Vh+1](⋅,a)/ε + ζh⌉.
(4)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2737850787132101,"7Throughout this paper, when considering the law for the exogenous variables ξ1, . . . ,ξH, we write P[⋅]
instead of Pπ[⋅] to emphasize that the law is independent of the agent’s policy.
8We say that a policy π is endogenous if it does not depend on exogenous noise, in the sense that π(xh) is a
measurable function of ϕ⋆(xh)."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.27446954140999313,"This rounding scheme, which quantizes the Bellman backup into ε−1 bins with a random offset, is de-
signed to emulate certain properties implied by the ∆-gap assumption (Assumption 4.4). Specifically,
we show that with constant probability over the draw of ζ1∶H, the policy ̂π in (4) “snaps” on to an
endogenous policy π. This means that for RVFSexo to succeed (with constant probability), it suffices to
pass it a class V that realizes the value functions (V π
h ) for endogenous policies π ∈ΠS. Fortunately,
such a function class can be constructed explicitly under decoder realizability (Assumption 3.3)."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2751540041067762,"Lemma B.1 ([21]). For the ExBMDP setting, under Assumption 3.3, the function class Vh ∶= {x ↦
f(ϕ(x)) ∶f ∈[0,H]S,ϕ ∈Φ} is such that V π
h ∈Vh for all endogenous policies π. Furthermore, the
policy class Πh ∶= {π(⋅) ∈arg maxa∈A f(ϕ(⋅),a) ∶f ∈[0,H]S×A,ϕ ∈Φ} contains all endogenous
policies."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2758384668035592,"A small technical challenge with the scheme above is that it is only guaranteed to succeed with
constant probability over the draw of the rounding parameters ζ1,...,ζH. To address this, we
provide an outer-level algorithm, RVFSexo.bc (Algorithm 9, deferred to Appendix F for space), which
performs confidence boosting by invoking RVFSexo multiple times independently, and extracts a
high-quality executable policy using behavior cloning."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.27652292950034224,"Main result.
We now state the main guarantee for RVFSexo (the proof is in Appendix K)."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.27720739219712526,"Theorem B.1 (Main guarantee of RVFSexo for EXBMDPs). Consider the ExBMDP setting. Suppose
the decoder class Φ satisfies Assumption 3.3, and that Assumption B.1 holds with Cexo > 0. Let
ε,δ ∈(0,1) be given, and let Vh and Πh be as in Lemma B.1. Then RVFSexo.bc(Π,V1∶H,ε,ζ1∶H,δ)
(Algorithm 9) produces a policy ̂π1∶H such that J(π⋆)−J(̂π1∶H) ≤ε, and has total sample complexity"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2778918548939083,"̃O (C8
exoS8H36A9 ⋅ε−26)."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2785763175906913,"This result shows for the first time that sample- and computationally-efficient learning is possible for
ExBMDPs beyond deterministic or factored settings [22, 21]."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2792607802874743,"We mention in passing that our use of randomized rounding to emulate certain consequences of the
∆-gap assumption leverages the fact that ExBMDPs have a finite number of (endogenous) latent
states. It is unclear if this technique can be used when the (latent) state space is large or infinite."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2799452429842574,"Algorithm 3 RVFSexo
h : Recursive Value Function Search for Exogenous Block MDPs"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2806297056810404,"1: parameters: Value function class V, suboptimality ε ∈(0,1), seeds ζ1∶H ∈(0,1), confidence
δ ∈(0,1).
2: input: Level h ∈[0..H], value function estimates ̂Vh+1∶H, confidence sets ̂Vh+1∶H, state-
action collections Ch∶H, and buffers Bh∶H, and counters th∶H."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2813141683778234,"/* Initialize parameters. */
3: Set M ←⌈8ε−2CexoSAH⌉.
4: Set Ntest ←28M 2Hε−2 log(8M 6H8ε−2δ−1), Nreg ←28M 2ε−2 log(8∣V∣HM 2δ−1).
5: Set Nest(k) ←2N 2
reg log(8ANregHk3/δ) and δ′ ←δ/(4M 7N 2
testH8∣V∣)."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.28199863107460643,"6: Set ε2
reg ←9MH2 log(8M 2H∣V∣/δ)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.28268309377138945,"Nreg
+ 34MH3 log(8M 6N 2
testH8/δ)
Ntest
."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.28336755646817247,"7: Set β(t) ←
√"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2840520191649555,log1/δ′(8MA∣V∣t2/δ).
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.28473648186173856,"/* Test the fit for the estimated value functions ̂Vh+1∶H at future layers. */
8: for (xh−1,ah−1) ∈Ch do
9:
for layer ℓ= H,...,h + 1 do
10:
for n = 1,...,Ntest do
11:
Draw xh ∼Th−1(⋅∣xh−1,ah−1), then draw xℓ−1 by rolling out with ̂πh+1∶H, where"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.28542094455852157,"∀τ ∈[H], ̂πτ(⋅) ∈arg max
a∈A
⌈̂Pτ,ε2,δ′[̂Vτ+1](⋅,a) ⋅ε−1 + ζτ⌉,
with
̂VH+1 ≡0."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2861054072553046,"12:
for aℓ−1 ∈A do
13:
Update tℓ←tℓ+ 1.
/* Test fit; if test fails, re-fit value functions ̂Vh+1∶ℓup to layer
ℓ. */
14:
if supf∈̂Vℓ∣(̂Pℓ−1,ε2,δ′[̂Vℓ] −̂Pℓ−1,ε2,δ′[fℓ])(xℓ−1,aℓ−1)∣> ε2 + ε2 ⋅β(tℓ) then"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2867898699520876,"15:
Cℓ←Cℓ∪{(xℓ−1,aℓ−1)} and Bℓ←Bℓ∪{(xℓ−1,aℓ−1, ̂Vℓ, ̂Vℓ,tℓ)}.
16:
for τ = ℓ,...,h + 1 do
17:
(̂Vτ∶H, ̂Vτ∶H,Cτ∶H,Bτ∶H,tτ∶H) ←RVFSexo
τ (̂Vτ+1∶H, ̂Vτ+1∶H,Cτ∶H,Bτ∶H,tτ∶H;V,ε,ζ1∶H,δ).
18:
go to line 8.
19: if h = 0 then return (̂V1∶H,⋅,⋅,⋅,⋅)."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2874743326488706,"/* Re-fit ̂Vh and build a new confidence set. */
20: for (xh−1,ah−1) ∈Ch do
21:
Set Dh(xh−1,ah−1) ←∅.
22:
for i = 1,...,Nreg do
23:
Sample xh ∼Th−1(⋅∣xh−1,ah−1).
24:
For each a ∈A, let ̂Vh(xh) be a Monte-Carlo estimate for Êπh∶H[∑H
ℓ=h rℓ∣xh] computed by
collecting Nest(∣Ch∣) trajectories starting from xh and rolling out with ̂πh∶H.
25:
Update D(xh−1,ah−1) ←D(xh−1,ah−1) ∪{(xh, ̂Vh(xh))}.
26: Let ̂Vh ∶= arg minf∈Vh ∑(xh−1,ah−1)∈Ch ∑(xh,vh)∈Dh(xh−1,ah−1)(f(xh) −vh)2.
27: Compute value function confidence set"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.28815879534565364,"̂Vh ∶=
⎧⎪⎪⎨⎪⎪⎩
f ∈Vh"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.2888432580424367,"RRRRRRRRRRRR
∑
(xh−1,ah−1)∈Ch"
NREG,0.28952772073921973,"1
Nreg
∑
(xh,-)∈Dh(xh−1,ah−1)
(̂Vh(xh) −f(xh))
2 ≤ε2
reg"
NREG,0.29021218343600275,"⎫⎪⎪⎬⎪⎪⎭
.
(5)"
NREG,0.29089664613278576,"28: return (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H)."
NREG,0.2915811088295688,Algorithm 4 RVFSexo.bc: Learn an executable policy with RVFSexo via imitation learning.
NREG,0.2922655715263518,"1: input: Decoder class Φ, suboptimality ε ∈(0,1), confidence δ ∈(0,1)."
NREG,0.2929500342231348,"/* Set parameters for RVFS and define the value function and policy classes. */
2: Set εRVFS ←εH−1/48.
3: Set V = V1∶H, where Vh = {x ↦f(ϕ(x)) ∶f ∈[0,H]S,ϕ ∈Φ}, ∀h ∈[H].
4: Set Π = Π1∶H, where Πh = {π(⋅) ∈arg maxa∈A f(ϕ(⋅),a) ∶f ∈[0,H]S×A,ϕ ∈Φ}, ∀h ∈[H]."
NREG,0.2936344969199179,"/* Set parameters for BehaviorCloning. */
5: Set Nbc ←8H2 log(4H∣Π∣/δ)/ε, Nboost ←log(1/δ)/log(24SAHε), Neval ←162ε−2 log(2Nboost/δ).
6: Set M ←⌈8ε−1
RVFSSACcovH⌉, Ntest ←28M 2Hε−1
RVFS log(80M 6H8Nboostε−2
RVFSδ−1), and δ′ =
δ
40M 7N2H8∣V∣Nboost ."
NREG,0.2943189596167009,"7: Set Nreg ←28M 2ε−1
RVFS log(80∣Φ∣2HM 2Nboostδ−1).
8: Set ̂V1∶H ←arbitrary, ̂V1∶H ←V, C0∶H ←∅, B0∶H ←∅, iopt = 1, and Jmax = 0."
NREG,0.2950034223134839,"/*
Repeatedly
invoke
RVFSexo
and
extract
policy
with
BehaviorCloning
to
boost
confidence. */
9: for i = 1,...,Nboost do"
NREG,0.29568788501026694,"/* Invoke RVFSexo. */
10:
(̂V
(i)
1∶H,⋅,⋅,⋅,⋅) ←RVFSexo
0 (̂V1∶H, ̂V1∶H,C0∶H,B0∶H;V,Nreg,Ntest,εRVFS,δ/(10Nboost)).
/* Imitation learning with BehaviorCloning. */
11:
Define ̂πRVFS
h
(⋅) ∈arg maxa∈A ̂Ph,εRVFS,δ′[̂V
(i)
h+1](⋅,a).
12:
Compute ̂π
(i)
1∶H ←BehaviorCloning(Π,ε,̂πRVFS
1∶H ,δ/(2Nboost)).
/* Evaluate current policy. */
13:
v = 0.
14:
for = 1,...,Neval do
15:
Sample trajectory (x1,a1,r1,...,xH,aH,rH) by executing ̂π
(i)
1∶H.
16:
Set v ←v + ∑H
h=1 rh.
17:
Set ̂J(̂π
(i)
1∶H) ←v/Neval.
18:
if ̂J(̂π
(i)
1∶H) > Jmax then
19:
Set iopt = i.
20:
Set Jmax = ̂J(̂π
(i)
1∶H)."
NREG,0.29637234770704995,"21: return: ̂π1∶H = ̂π
(iopt)
1∶H ."
NREG,0.29705681040383297,"C
Helper Lemmas"
NREG,0.29774127310061604,This section of the appendix contains supporting lemmas used within the proofs of our main results.
NREG,0.29842573579739906,"C.1
Concentration and Probability"
NREG,0.2991101984941821,"Lemma C.1. Let δ ∈(0,1) and H ≥1 be given. If a sequence of events E1,...,EH satisfies
P[Eh ∣E1,...,Eh−1] ≥1 −δ/H for all h ∈[H], then"
NREG,0.2997946611909651,P[E1∶H] ≥1 −δ.
NREG,0.3004791238877481,"Proof of Lemma C.1. By the chain rule, we have"
NREG,0.30116358658453113,"P[E1∶H] = ∏
h∈[H]
P[Eh ∣E1,...,Eh−1] ≥∏
h∈[H]
(1 −δ/H) = (1 −δ/H)H ≥1 −δ."
NREG,0.30184804928131415,"We make use of the following version of Freedman’s inequality, due to Agarwal et al. [1, Lemma 9]:"
NREG,0.3025325119780972,"Lemma C.2. Let R > 0 be given and let w1,...wn be a sequence of real-valued random variables
adapted to filtration H1,⋯,Hn. Assume that for all t ∈[n], wi ≤R and E[wi ∣Hi−1] = 0. Define
Sn ∶= ∑n
t=1 wi and Vn ∶= ∑n
t=1 E[w2
i ∣Hi−1]. Then, for any δ ∈(0,1) and λ ∈[0,1/R], with
probability at least 1 −δ,"
NREG,0.30321697467488024,Sn ≤λVn + log(1/δ)/λ.
NREG,0.30390143737166325,"We will also use the following lemma, which is a standard consequence of Freedman’s inequality."
NREG,0.30458590006844627,"Lemma C.3 (e.g., Foster et al. [25]). Let (wt)t≤T be a sequence of random variables adapted to a
filtration (Ht)t≤T . If 0 ≤wt ≤R almost surely, then with probability at least 1 −δ,"
NREG,0.3052703627652293,"T
∑
t=1
wt ≤3 2"
NREG,0.3059548254620123,"T
∑
t=1
Et−1[wt] + 4R log(2δ−1), and"
NREG,0.3066392881587953,"T
∑
t=1
Et−1[wt] ≤2
T
∑
t=1
wt + 8R log(2δ−1)."
NREG,0.3073237508555784,"C.2
Regression
Using Lemmas C.2 and C.3, we obtain the following concentration lemma, which will be used to
prove guarantees for square loss regression within our algorithms."
NREG,0.3080082135523614,"Lemma C.4. Let B > 0 and n ∈N be given, and let Y be an abstract set. Further, let Q ⊆{g ∶Y →
[0,B]} be a finite function class and y1,...,yn be a sequence of random variables in Y adapted to
filtration a H1,⋯,Hn. Then, for any δ ∈(0,1), with probability at least 1 −δ, we have"
NREG,0.3086926762491444,"∀g ∈Q,
1
2∥g∥2 −2B2 log(2∣Q∣/δ) ≤∥g∥2
n ≤2∥g∥2 + 2B2 log(2∣Q∣/δ),"
NREG,0.30937713894592744,"where ∥g∥2 ∶= ∑i∈[n] E[g(yi)2 ∣Hi−1] and ∥g∥2
n ∶= ∑n
i=1 g(yi)2."
NREG,0.31006160164271046,"Proof of Lemma C.4. Fix g ∈Q. Applying Lemma C.2 with wi = g(yi)2 −E[g(yi)2 ∣Hi−1], for
all i ∈[n], and (R,λ) = (B2,1/B2), we get that with probability at least 1 −δ/(2∣Q∣):"
NREG,0.3107460643394935,"∥g∥2
n −∥g∥2 ≤λB2∥g∥2 + log(2∣Q∣/δ)/λ."
NREG,0.31143052703627655,"By substituting λ = B−2 and rearranging, we get"
NREG,0.31211498973305957,"∥g∥2
n ≤2∥g∥2 + B2 log(2∣Q∣/δ).
(6)"
NREG,0.3127994524298426,"Similarly, applying Lemma C.2 with wi = E[g(yi)2 ∣Hi−1] −g(yi)2, for all i ∈[n], and (R,λ) =
(B2,1/(2B2)), we get that with probability at least 1 −δ/(2∣Q∣):"
NREG,0.3134839151266256,"∥g∥2 −∥g∥2
n ≤λB2∥g∥2 + log(2∣Q∣/δ)/λ."
NREG,0.3141683778234086,"By substituting λ = 2−1B−2 and rearranging, we get"
NREG,0.31485284052019163,"∥g∥2
n ≥1"
NREG,0.31553730321697465,2∥g∥2 −2B2 log(2∣Q∣/δ).
NREG,0.3162217659137577,"Combining this with (6) and the union bound, we get the desired result."
NREG,0.31690622861054074,"With this lemma, we now prove the following key result for square loss regression."
NREG,0.31759069130732376,"Lemma C.5 (Generic regression guarantee). Let B > 0 and n ∈N be given and Y be an abstract
set. Further, let F ⊆{f ∶Y →[0,B]} be a finite function class, and suppose that there is a function
f⋆∈F and a sequence of random variables (y1,x1),...,(yn,xn) ∈Y × R such that for all i ∈[n]:"
NREG,0.3182751540041068,• xi = f⋆(yi) + εi + bi;
NREG,0.3189596167008898,• ∣bi∣≤ξ;
NREG,0.3196440793976728,"• εi ∈[−B,B]; and"
NREG,0.3203285420944558,"• E[εi ∣Fi] = 0, where Fi ∶= σ(y1∶i,ε1∶i−1,x1∶i−1,b1∶i−1)."
NREG,0.3210130047912389,"Then, for ̂f ∈arg minf∈F ∑n
i=1(f(yi) −xi)2 and any δ ∈(0,1), with probability at least 1 −δ/2,"
NREG,0.3216974674880219,"∥̂f −f⋆∥2
n ≤4B2 log(2∣F∣/δ) + 4B
n
∑
i=1
∣bi∣,"
NREG,0.32238193018480493,"where ∥̂f −f⋆∥2
n ∶= ∑n
i=1( ̂f(yi) −f ⋆(yi))2."
NREG,0.32306639288158795,"Proof of Lemma C.5. Fix δ ∈(0,1) and let ̂Ln(f) ∶= ∑n
i=1(f(yi) −xi)2, for f ∈F, and note that
since ̂f ∈arg minf∈F ̂Ln(f), we have"
NREG,0.32375085557837097,"0 ≥̂Ln( ̂f) −̂Ln(f⋆) = ∇̂Ln(f⋆)[ ̂f −f⋆] + ∥̂f −f⋆∥2
n,"
NREG,0.324435318275154,"where ∇denotes directional derivative. Rearranging, we get that"
NREG,0.32511978097193706,"∥̂f −f⋆∥2
n ≤−2∇̂Ln(f⋆)[ ̂f −f⋆] −∥̂f −f⋆∥2
n,"
NREG,0.3258042436687201,"= 4
n
∑
i=1
(xi −f⋆(yi))( ̂f(yi) −f⋆(yi)) −∥̂f −f⋆∥2
n,"
NREG,0.3264887063655031,"≤4
n
∑
i=1
(εi + bi)( ̂f(yi) −f⋆(yi)) −∥̂f −f⋆∥2
n,"
NREG,0.3271731690622861,"≤4
n
∑
i=1
εi ⋅( ̂f(yi) −f⋆(yi)) −∥̂f −f⋆∥2
n"
NREG,0.3278576317590691,"´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
I"
NREG,0.32854209445585214,"+4
n
∑
i=1
bi ⋅( ̂f(yi) −f⋆(yi))"
NREG,0.32922655715263516,"´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
II .
(7)"
NREG,0.32991101984941823,"Bounding Term I. To bound Term I, we apply Lemma C.2 with wi = εi ⋅( ̂f(yi) −f⋆(yi)), R = B2,
λ = 1/(8B2), and Hi = F−
i+1, and use"
NREG,0.33059548254620125,1. the union bound over f ∈F; and
NREG,0.33127994524298426,"2. the facts that E[yi ∣F−
i ] = yi and E[εi ∣F−
i ] = 0,"
NREG,0.3319644079397673,"to get that with probability at least 1 −δ/2,"
N,0.3326488706365503,"4
n
∑
i=1
εi ⋅( ̂f(yi) −f⋆(yi)) ≤∥̂f −f⋆∥2
n + 4B2 log(2∣F∣/δ)."
N,0.3333333333333333,"By rearranging, we get that with probability at least 1 −δ/2,"
N,0.33401779603011633,Term I ≤4B2 log(2∣F∣/δ).
N,0.3347022587268994,"Bounding Term II. We now bound the second term in (7). For this, note that since ∥̂f −f⋆∥∞≤B, we
have"
N,0.3353867214236824,"Term II ≤4B
n
∑
i=1
∣bi∣."
N,0.33607118412046544,This completes the proof.
N,0.33675564681724846,"C.3
Reinforcement Learning"
N,0.33744010951403147,"Lemma C.6 (Performance Difference Lemma (e.g., Kakade [33])). For any two policies ̂π,π ∈ΠS
and t ∈[H], we have"
N,0.3381245722108145,"Eπ [V π
t (xt) −V ̂π
t (xt)] = Eπ [
H
∑
h=t
Q̂π
h(xh,πh(xh)) −Q̂π
h(xh, ̂πh(xh))]."
N,0.33880903490759756,"In particular, applying this for t = 1 gives"
N,0.3394934976043806,"J(π) −J(̂π) = Eπ [
H
∑
h=1
Q̂π
t (xh,πh(xh)) −Q̂π
h(xh, ̂πh(xh))]."
N,0.3401779603011636,"Lemma C.7 (Potential lemma [63]). Fix h ∈[H]. Suppose we have a sequence of functions
g(1),...,g(T ) ∈[0,B] and policies π(1),...,π(T ) such that"
N,0.3408624229979466,"∀t ∈[T],
∑
i<t
Eπ(i)[(g
(t)(xh))2] ≤β2"
N,0.34154688569472963,"for some β ≥0. Then under Assumption 4.1, we have"
N,0.34223134839151265,"T
∑
t=1
Eπ(t)[g
(t)(xh)] ≤2
√"
N,0.34291581108829566,"β2CpushT log(2T) + 2BCpush,"
N,0.34360027378507874,and consequently
N,0.34428473648186175,"min
t∈[T ]Eπ(t)[g
(t)(xh)] ≤2 √"
N,0.34496919917864477,β2Cpush log(2T)
N,0.3456536618754278,"T
+ 2BCpush T
."
N,0.3463381245722108,"Proof of Lemma C.8. See proof of [63, Theorem 1]."
N,0.3470225872689938,The following result is a variant of the coverability-based potential argument given in Xie et al. [63].
N,0.34770704996577684,"Lemma C.8 (Pushforward coverability potential lemma). Fix h ∈[H]. Suppose we have a sequence
of functions g(1),...,g(T ) ∈[0,B] and state-action pairs (x(1),a(1)),...,(x(T ),a(T )) such that"
N,0.3483915126625599,"∀t ∈[T],
∑
i<t
E[(g
(t)(xh))2 ∣xh−1 = x
(i),ah−1 = a
(i)] ≤β2"
N,0.3490759753593429,"for some β ≥0. Then under Assumption 4.1, we have"
N,0.34976043805612594,"T
∑
t=1
E[g
(t)(xh) ∣xh−1 = x
(t),ah−1 = a
(t)] ≤2
√"
N,0.35044490075290896,"β2CpushT log(2T) + 2BCpush,"
N,0.351129363449692,and consequently
N,0.351813826146475,"min
t∈[T ]E[g
(t)(xh) ∣xh−1 = x
(t),ah−1 = a
(t)] ≤2 √"
N,0.35249828884325807,β2Cpush log(2T)
N,0.3531827515400411,"T
+ 2BCpush T
."
N,0.3538672142368241,"Proof of Lemma C.8. Define d
(t)
h (x) ∶= P[xh = x ∣xh−1 = x(t),ah−1 = a(t)], and let ̃d
(t)
h ∶= ∑i<t d(i).
Let
τh(x) ∶= min{t ∣̃d
(t)
h (x) ≥Cpush ⋅µh(x)}.
We have"
N,0.3545516769336071,"T
∑
t=1
E[g
(t)(xh) ∣xh−1 = x
(t),ah−1 = a
(t)]"
N,0.35523613963039014,"=
T
∑
t=1
∑
x∈X
d
(t)
h (x)g
(t)(x)"
N,0.35592060232717315,"≤
T
∑
t=1
∑
x∈X
d
(t)
h (x)g
(t)(x)I{t ≥τh(x)} + B
T
∑
t=1
∑
x∈X
d
(t)
h (x)I{t < τh(x)}."
N,0.35660506502395617,"From the definition of pushforward coverability, we can bound"
N,0.35728952772073924,"T
∑
t=1
∑
x∈X
d
(t)
h (x)I{t < τh(x)} = ∑
x∈X
̃d
(τh(x))
h
(x) ≤2Cpush ∑
x∈X
µh(x) = 2Cpush."
N,0.35797399041752226,"For the other term, we bound"
N,0.3586584531143053,"T
∑
t=1
∑
x∈X
d
(t)
h (x)g
(t)(x)I{t ≥τh(x)}"
N,0.3593429158110883,"≤(
T
∑
t=1
∑
x∈X"
N,0.3600273785078713,"(d
(t)
h (x))2"
N,0.3607118412046543,"̃d
(t)
h (x)
I{t ≥τh(x)})"
N,0.3613963039014374,"1/2
⋅(
T
∑
t=1
∑
x∈X
̃d
(t)
h (x)(g
(t)
h (x))2) 1/2"
N,0.3620807665982204,"= (
T
∑
t=1
∑
x∈X"
N,0.36276522929500343,"(d
(t)
h (x))2"
N,0.36344969199178645,"̃d
(t)
h (x)
I{t ≥τh(x)})"
N,0.36413415468856947,"1/2
⋅(
T
∑
t=1
∑
i<t
E[(g
(t)(xh))2 ∣xh−1 = x
(i),ah−1 = a
(i)]) 1/2"
N,0.3648186173853525,"≤(
T
∑
t=1
∑
x∈X"
N,0.3655030800821355,"(d
(t)
h (x))2"
N,0.3661875427789186,"̃d
(t)
h (x)
I{t ≥τh(x)})"
N,0.3668720054757016,"1/2
⋅
√ β2T."
N,0.3675564681724846,"Finally, we have"
N,0.3682409308692676,"T
∑
t=1
∑
x∈X"
N,0.36892539356605064,"(d
(t)
h (x))2"
N,0.36960985626283366,"̃d
(t)
h (x)
I{t ≥τh(x)} ≤2
T
∑
t=1
∑
x∈X"
N,0.3702943189596167,"(d
(t)
h (x))2"
N,0.37097878165639975,"̃d
(t)
h (x) + Cpushµh(x)"
N,0.37166324435318276,≤2Cpush
N,0.3723477070499658,"T
∑
t=1
∑
x∈X
µh(x)
d
(t)
h (x)
̃d
(t)
h (x) + Cpushµh(x)"
N,0.3730321697467488,"= 2Cpush ∑
x∈X
µh(x)
T
∑
t=1"
N,0.3737166324435318,"d
(t)
h (x)
̃d
(t)
h (x) + Cpushµh(x)
= 4Cpush log(T + 1),"
N,0.37440109514031483,where the last line uses Lemma 4 of Xie et al. [63].
N,0.3750855578370979,"Part II
Proofs for SimGolf (Section 3)"
N,0.3757700205338809,"As described in Section 3.1, the main difference between SimGolf and GOLF lies in the construction
of the confidence sets. The most important new step in the proof of Theorem 3.1 is to show that the
local simulator-based confidence set construction in Line 9 is valid in the sense that the property
Eq. (1) holds with high probability. From here, the sample complexity bound follows by adapting the
change-of-measure argument based on coverability from Xie et al. [63]."
N,0.37645448323066394,"To this end, this part of the appendix is organized as follows. We first state and prove technical
lemmas concerning realizability (Lemma D.1) and the confidence set construction (Lemma D.2 and
Lemma D.3) in Appendix D. Then, in Appendix E, we prove Theorem 3.1 as a consequence."
N,0.37713894592744696,"D
Preliminary Lemmas for Proof of Theorem 3.1"
N,0.37782340862423,"For this section, we define"
N,0.378507871321013,"ℓ
(t)
h (g) ∶= (gh(x
(t)
h ,a
(t)
h ) −1 K"
N,0.379192334017796,"K
∑
k=1
(r
(t,k)
h
+ max
a∈A gh+1(x
(t,k)
h+1 ,a))) 2 and"
N,0.3798767967145791,"ℓ
(t)
h (g) ∶= Eπ(t)[(gh(xh,ah) −Th[gh+1](xh,ah))2],"
N,0.3805612594113621,"where (x
(t)
h ,a
(t)
h ,r
(t,k)
h
,x
(t,k)
h+1 ) are as in Algorithm 1."
N,0.3812457221081451,"Lemma D.1 ([21]). For the ExBMDP setting, under Assumption 3.3, the function class Qh ∶=
{(x,a) ↦g(ϕ(x),a) ∶g ∈[0,H]SA,ϕ ∈Φ} satisfies Assumption 3.1 and has log∣Qh∣= log∣Πh∣=
̃O(SA + log∣Φ∣).9"
N,0.38193018480492813,"Lemma D.2. With probability at least 1 −δ, for all h ∈[H], t ∈[N], and g ∈Q,"
N,0.38261464750171115,"∑
i<t
ℓ
(i)
h (g) ≤3∑
i<t
Eπ(i)[(gh(xh,ah) −Th[gh+1](xh,ah))2] + 8N"
N,0.38329911019849416,"K + 16log(2HN∣Q∣δ−1), and"
N,0.3839835728952772,"∑
i<t
Eπ(i)[(gh(xh,ah) −Th[gh+1](xh,ah))2] ≤4∑
i<t
ℓ
(i)
h (g) + 8N"
N,0.38466803559206025,K + 64log(2HN∣Q∣δ−1).
N,0.38535249828884327,"Proof of Lemma D.2. Let t ∈[N] and h ∈[H] be fixed. Let us denote z
(t)
h = {(r
(t,k)
h
,x
(t,k)
h+1 )}k∈[K].
Define a filtration"
N,0.3860369609856263,"H
(t) = σ(τ
(1),z
(1)
1 ,...,z
(1)
H ,...,τ
(t),z
(t)
1 ,...,z
(t)
H ),"
N,0.3867214236824093,"where τ (i) is the trajectory generated in the ith iteration of Algorithm 1 (see Line 7). Fix g ∈Q.
Observe that ℓ
(i)
h (g) ∈[0,4], so Lemma C.3 ensures that with probability at least 1 −δ,"
N,0.3874058863791923,"∑
i<t
ℓ
(i)
h (g) ≤3"
N,0.38809034907597534,"2 ∑
i<t
E[ℓ
(i)
h (g) ∣H
(i−1)] + 16log(2δ−1), and"
N,0.3887748117727584,"∑
i<t
E[ℓ
(i)
h (g) ∣H
(i−1)] ≤2∑
i<t
ℓ
(i)
h (g) + 32log(2δ−1).
(8)"
N,0.3894592744695414,"9Formally, this requires a standard covering number argument; we omit the details."
N,0.39014373716632444,"By the AM-GM inequality, for all i < t, we can bound"
N,0.39082819986310746,"E[ℓ
(i)
h (g) ∣H
(i−1)]"
N,0.3915126625598905,"= E
⎡⎢⎢⎢⎢⎣
(gh(x
(i)
h ,a
(i)
h ) −1 K"
N,0.3921971252566735,"K
∑
k=1
(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a)))"
N,0.3928815879534565,"2
∣H
(i−1)
⎤⎥⎥⎥⎥⎦"
N,0.3935660506502396,"≤2E[(gh(x
(i)
h ,a
(i)
h ) −Th[gh+1](x
(i)
h ,a
(i)
h ))
2 ∣H
(i−1)]"
N,0.3942505133470226,"+ 2E
⎡⎢⎢⎢⎢⎣
(Th[gh+1](x
(i)
h ,a
(i)
h ) −1 K"
N,0.3949349760438056,"K
∑
k=1
(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a)))"
N,0.39561943874058864,"2
∣H
(i−1)
⎤⎥⎥⎥⎥⎦
. and"
N,0.39630390143737165,"E[ℓ
(i)
h (g) ∣H
(i−1)] ≥1"
N,0.39698836413415467,"2 E[(gh(x
(i)
h ,a
(i)
h ) −Th[gh+1](x
(i)
h ,a
(i)
h ))
2 ∣H
(i−1)]"
N,0.3976728268309377,"−E
⎡⎢⎢⎢⎢⎣
(Th[gh+1](x
(i)
h ,a
(i)
h ) −1 K"
N,0.39835728952772076,"K
∑
k=1
(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a)))"
N,0.3990417522245038,"2
∣H
(i−1)
⎤⎥⎥⎥⎥⎦
."
N,0.3997262149212868,We have
N,0.4004106776180698,"E[(gh(x
(i)
h ,a
(i)
h ) −Th[gh+1](x
(i)
h ,a
(i)
h ))
2 ∣H
(i−1)] = Eπ(i)[(gh(xh,ah) −Th[gh+1](xh,ah))2] and"
N,0.4010951403148528,"E
⎡⎢⎢⎢⎢⎣
(Th[gh+1](x
(i)
h ,a
(i)
h ) −1 K"
N,0.40177960301163584,"K
∑
k=1
(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a)))"
N,0.4024640657084189,"2
∣H
(i−1)
⎤⎥⎥⎥⎥⎦"
N,0.40314852840520193,"= E
⎡⎢⎢⎢⎢⎣
E
⎡⎢⎢⎢⎢⎣
(Th[gh+1](xh,ah) −1 K"
N,0.40383299110198495,"K
∑
k=1
(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a)))"
N,0.40451745379876797,"2
∣xh = x
(i)
h ,ah = a
(i)
h"
N,0.405201916495551,"⎤⎥⎥⎥⎥⎦
∣H
(i−1)
⎤⎥⎥⎥⎥⎦
."
N,0.405886379192334,"Since
E[r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a) ∣xh = x
(i)
h ,ah = a
(i)
h ] = Th[gh+1](x
(i)
h ,a
(i)
h )"
N,0.406570841889117,"and {(r
(i,k)
h
,x
(i,k)
h+1 )}k∈[K] are i.i.d. conditioned on (xh,ah) = (x
(i)
h ,a
(i)
h ), we have,"
N,0.4072553045859001,"E
⎡⎢⎢⎢⎢⎣
(Th[gh+1](xh,ah) −1 K"
N,0.4079397672826831,"K
∑
k=1
(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a)))"
N,0.4086242299794661,"2
∣xh = x
(i)
h ,ah = a
(i)
h"
N,0.40930869267624914,⎤⎥⎥⎥⎥⎦ = 1
N,0.40999315537303216,"K E[(Th[gh+1](xh,ah) −(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h
,a)))"
N,0.4106776180698152,"2
∣xh = x
(i)
h ,ah = a
(i)
h ] ≤4 K ,"
N,0.4113620807665982,so that
N,0.41204654346338127,"E
⎡⎢⎢⎢⎢⎣
(Th[gh+1](x
(i)
h ,a
(i)
h ) −1 K"
N,0.4127310061601643,"K
∑
k=1
(r
(i,k)
h
+ max
a∈A gh+1(x
(i,k)
h+1 ,a)))"
N,0.4134154688569473,"2
∣H
(i−1)
⎤⎥⎥⎥⎥⎦
≤4 K ."
N,0.4140999315537303,Combining these bounds with (8) and rearranging thus gives
N,0.41478439425051333,"∑
i<t
ℓ
(i)
h (g) ≤3∑
i<t
Eπ(i)[(gh(xh,ah) −Th[gh+1](xh,ah))2] + 8N"
N,0.41546885694729635,"K + 16log(2δ−1), and"
N,0.4161533196440794,"∑
i<t
Eπ(i)[(gh(xh,ah) −Th[gh+1](xh,ah))2] ≤4∑
i<t
ℓ
(i)
h (g) + 8N"
N,0.41683778234086244,K + 64log(2δ−1).
N,0.41752224503764546,Taking a union bound yields the result.
N,0.4182067077344285,"Lemma D.3. Define βstat = 16log(2HN∣Q∣δ−1). Suppose we set K ≥
8N
βstat and β ≥2βstat. Then
with probability at least 1 −δ, for all t ∈[N] and h ∈H:"
N,0.4188911704312115,• Q⋆∈Q(t).
N,0.4195756331279945,• All g ∈Q(t) satisfy
N,0.4202600958247775,"∑
i<t
Eπ(i)[(gh(xh,ah) −Th[gh+1](xh,ah))2] ≤9β."
N,0.4209445585215606,"Proof of Lemma D.3. Condition on the event in Lemma D.2. For any fixed t ∈[N] and h ∈[H],
we have that"
N,0.4216290212183436,"∑
i<t
ℓ
(i)
h (Q⋆) ≤3∑
i<t
Eπ(i)[(Q⋆
h(xh,ah) −Th[Q⋆
h+1](xh,ah))2] + 8N"
N,0.42231348391512663,K + 16log(2HN∣Q∣δ−1) ≤8N
N,0.42299794661190965,"K + 16log(2HN∣Q∣δ−1) ≤2βstat,"
N,0.42368240930869266,"where the first inequality uses that Q⋆
h = Th[Q⋆
h+1] and the second inequality uses our choice for K.
It follows that Q⋆∈Q(t) as long as β ≥2βstat."
N,0.4243668720054757,"To prove the second claim, we note that for all g ∈Q(t), by construction,"
N,0.42505133470225875,"∑
i<t
Eπ(i)[(gh(xh) −Th[gh+1](xh,ah))2] ≤4∑
i<t
ℓ
(i)
h (g) + 8N"
N,0.42573579739904177,K + 64log(2HN∣Q∣δ−1)
N,0.4264202600958248,"≤4∑
i<t
ℓ
(i)
h (g) + 5βstat ≤9β."
N,0.4271047227926078,"E
Proof of Theorem 3.1"
N,0.4277891854893908,"Proof of Theorem 3.1. From Lemma D.3, the parameter setting in the theorem statement ensures
that with probability at least 1 −δ, for all t ∈[2..N], Q⋆∈Q(t), and all g ∈Q(t) satisfy"
N,0.42847364818617384,"∑
i<t
Eπ(i)[(gh(xh) −Th[gh+1](xh,ah))2] ≤9β.
(9)"
N,0.42915811088295686,"for all h. Let us condition on this event going forward. First, note that since Q⋆∈Q(t) for all
t ∈[2..N], we have that"
N,0.42984257357973993,"J(π⋆) ≤E[max
a∈A Q⋆
1(x1,a)] ≤sup
g∈Q(t) E[max
a∈A g1(x1,a)].
(10)"
N,0.43052703627652295,"On the other hand, we have g(t) ∈arg maxg∈Q(t) ∑s<t maxa∈A g1(x
(s)
1 ,a), and so since x
(1)
1 ,x
(2)
1 ,...
are i.i.d. and any g ∈Q(t) take values in [0,H], we have that by Hoeffding’s inequality, there is an
event E of probability at least 1 −δ under which"
N,0.43121149897330596,"∀t ∈[2..N],∀g ∈Q,
∣E[max
a∈A g1(x1,a)] −
1
t −1 ∑
s<t
max
a∈A g1(x
(s)
1 ,a)∣≤
√"
N,0.431895961670089,(t −1)−1 log(2N∣Q∣/δ). (11)
N,0.432580424366872,"This implies that under E, we have"
N,0.433264887063655,"∀t ∈[2..N],
sup
g∈Q(t) E[max
a∈A g1(x1,a)] ≤sup
g∈Q(t)
1
t −1 ∑
s<t
max
a∈A g1(x
(s)
1 ,a) +
√"
N,0.43394934976043803,"(t −1)−1 log(2N∣Q∣/δ),"
N,0.4346338124572211,"=
1
t −1 ∑
s<t
max
a∈A g
(t)
1 (x
(s)
1 ,a) +
√"
N,0.4353182751540041,"(t −1)−1 log(2N∣Q∣/δ),"
N,0.43600273785078714,"≤E[max
a∈A g
(t)
1 (x1,a)] + 2
√"
N,0.43668720054757015,"(t −1)−1 log(2N∣Q∣/δ), (12)"
N,0.43737166324435317,"where in the last inequality we have used (11) with f = g(t). Thus, summing (12) for t = 2,...N and
using (10) gives that under E:"
N,0.4380561259411362,"N
∑
t=2
J(π⋆) ≤
N
∑
t=2
E[g
(t)
1 (x1,a1)] + 4
√"
N,0.43874058863791926,"N log(2N∣Q∣/δ),"
N,0.4394250513347023,"and so since J(π⋆) ≤H,"
N,0.4401095140314853,"N
∑
t=1
J(π⋆) ≤
N
∑
t=1
E[g
(t)
1 (x1,a1)] + 4
√"
N,0.4407939767282683,"N log(2N∣Q∣/δ) + H.
(13)"
N,0.4414784394250513,"On the other hand, using that g
(t)
H+1 ≡0, we get"
N,0.44216290212183434,"N
∑
t=1
(E[g
(t)
1 (x1,a1)] −J(π
(t)))"
N,0.44284736481861736,"≤
N
∑
t=1"
N,0.44353182751540043,"H
∑
h=1
Eπ(t) [g
(t)
h (xh,ah) −rh −max
a∈A g
(t)
h+1(xh+1,a)],"
N,0.44421629021218345,"=
N
∑
t=1"
N,0.44490075290896647,"H
∑
h=1
Eπ(t) [g
(t)
h (xh,ah) −E[rh + max
a∈A g
(t)
h+1(xh+1,a) ∣xh,ah]],
(law of total expectation)"
N,0.4455852156057495,"=
N
∑
t=1"
N,0.4462696783025325,"H
∑
h=1
Eπ(t) [g
(t)
h (xh,ah) −Th[g
(t)
h+1](xh,ah)]."
N,0.4469541409993155,"and so, by the potential lemma (Lemma C.7) and (9), we have ≤6H
√"
N,0.44763860369609854,CcovβN log(2N) + 2H2Ccov.
N,0.4483230663928816,"Combining this with (13), we obtain that with probability at least 1 −2δ,"
N,0.4490075290896646,"N
∑
t=1
(J(π⋆) −J(π
(t))) ≤6H
√"
N,0.44969199178644764,"CcovβN log(2N) + 4
√"
N,0.45037645448323066,N log(2N∣Q∣/δ) + 3H2Ccov.
N,0.4510609171800137,"It follows that if N = ̃O(H2Ccovβ/ε2), then the policy"
N,0.4517453798767967,"̂π ∈unif(π
(1),...,π
(N))"
N,0.45242984257357977,"returned by SimGolf satisfies, with probability at least 1 −δ:"
N,0.4531143052703628,J(π⋆) −E[J(̂π)] ≤ε.
N,0.4537987679671458,"Sample complexity.
We now bound the number of episodes. Note that that within an iteration
t of SimGolf, the local simulator is called KH times to update the confidence set, where K ≤
N/log(2HN∣Q∣/δ)). Consequently, the total sample complexity is bounded by"
N,0.4544832306639288,"HNK ≤̃O(H5C2
cov log(∣Q∣/δ)/ε4)."
N,0.45516769336071183,"Part III
Proofs for RVFS (Section 4)"
N,0.45585215605749485,"F
Full Version of RVFS"
N,0.45653661875427787,"Algorithm 5 displays the full version of RVFS. Algorithm 6 contains an “outer-level” wrapper for
RVFS, RVFS.bc, which invokes RVFS and extracts an executable policy with imitation learning, and
Algorithm 7 contains the subroutine used within Algorithm 5 to approximate Bellman backups
for value functions using local simulator access. Additionally, we display the variant of RVFS for
Exogenous Block MDPs, described in Appendix B, in Algorithms 8 and 9. Before diving into
the proof, we first describe how the full version of the algorithm differs from the informal version
presented in the main body in greater detail."
N,0.45722108145106094,"Differences between full version (Algorithm 5) and informal version (Algorithm 2) of RVFS.
The main difference between Algorithm 2 and its full version in Algorithm 5 is that in the former we
simply assume access to quantities involving conditional expectations such as:"
N,0.45790554414784396,"• The bellman backups Ph[̂Vh+1], which are required to evaluate the actions of RVFS’s policies
(see (2)), and to perform the tests in Line 8; and"
N,0.458590006844627,"• The value functions Êπh+1∶H[∑H
ℓ=h rτ ∣xh = x,ah = a] in Line 15, which are needed in the
regression problem in Line 16."
N,0.45927446954141,"These quantities are not available to the algorithm directly, but they can be estimated using the local
simulator. This is reflected in the full version of RVFS in Algorithm 5."
N,0.459958932238193,"Extracting policies from value functions.
Let us briefly comment in more detail on how Algo-
rithm 5 extracts the policy π(t) from the optimistic value function f (t) ∈V at iteration t. From the
Bellman equation, the ideal choice would be to set π
(t)
h (x) = arg maxa∈A Ph[f
(t)
h+1](x,a), but this
requires knowledge of the transition distribution. Instead, given parameters ε,δ ∈(0,1), SimGolf
invokes Algorithm 7 via π
(t)
h (x) ∈arg maxa∈A ̂Ph,ε,δ[f
(t)
h+1](x,a). The operator ̂Ph,ε,δ[f] (Al-
gorithm 7), when given input (x,a) ∈X × A and fh+1 ∶X →R, uses the local simulator to
generate Nsim ≥1 next states x
(1)
h+1,...,x
(Nsim)
h+1
i.i.d.
∼Th(⋅∣x,a) to estimate the bellman back-up"
N,0.460643394934976,"Ph[fh+1] via
1
Nsim ∑Nsim
i=1 (r
(i)
h + fh+1(x
(i)
h+1)), where r
(1)
h ,...,r
(Nsim)
h
i.i.d.
∼Rh(x,a). The number of
samples Nsim in Algorithm 7 is set as a function of (ε,δ) such that with probability at least 1 −δ,
∣̂Ph,ε,δ[fh+1](x,a) −Ph[fh+1](x,a)∣≤ε."
N,0.46132785763175904,"Invoking the algorithm.
The base invocation of RVFS takes the form"
N,0.4620123203285421,"̂V1∶H ←RVFS0(̂V1∶H = arbitrary, ̂V1∶H = {V}H
h=1,C0∶H = {∅}H
h=0,B0∶H = {∅}H
h=0,t1∶H = {1}H
h=1;⋯)."
N,0.46269678302532513,"Whenever this call returns, the greedy policy induced by ̂V1∶H is guaranteed to be near-optimal.
Naively, the policy induced by ̂V1∶H is non-executable, and must be computed by invoking the local
simulator through Line 14. To provide an end-to-end guarantee to learn an executable policy, the
outer-level algorithm, RVFS.bc (Algorithm 6, invokes RVFS0, then extracts an executable policy from
̂V1∶H using imitation learning."
N,0.46338124572210815,Subsequent recursive calls take the form
N,0.46406570841889117,"(̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H) ←RVFSh(̂Vh+1∶H, ̂Vh+1∶H,Ch∶H,Bh∶H,th∶H;V,ε,δ)."
N,0.4647501711156742,"For such a call, the arguments above are:"
N,0.4654346338124572,• ̂Vh+1∶H: Value function estimates for subsequent layers.
N,0.46611909650924027,"• ̂Vh+1∶H: Value function confidence sets ̂Vh+1∶H ⊂Vh+1∶H, which are used in the test on"
N,0.4668035592060233,"Line 14 to quantify uncertainty on new state-action pairs and decide whether to expand the
core-sets."
N,0.4674880219028063,• Ch∶H: Core-sets for current and subsequent layers.
N,0.4681724845995893,"• Bh∶H: Buffers of tuples (xh−1,ah−1, ̂Vh, ̂Vh,th), which record relevant features of the
algorithm’s state whenever the test on Line 14 fails and a recursive call is performed."
N,0.46885694729637234,"• th∶H: Counters that track the number of times Algorithm 7 is called in the test on Line 14,
which facilitate tuning of confidence parameters."
N,0.46954140999315536,"Importantly, the confidence sets ̂Vh+1∶H do not need to be explicitly maintained, and can be invoked
implicitly whenever a regression oracle for the value function class is available (cf. discussion in
Section 4). Likewise, the buffers Bh∶H are only used in our analysis, and do not need to be explicitly
maintained."
N,0.4702258726899384,"F.1
RVFS Pseudocode"
N,0.47091033538672145,Algorithm 5 RVFSh: Recursive Value Function Search
N,0.47159479808350446,"1: parameters: Value function class V, suboptimality ε ∈(0,1), confidence δ ∈(0,1).
2:
input:"
N,0.4722792607802875,"• Level h ∈{0,...,H}.
• Value function estimates ̂Vh+1∶H, confidence sets ̂Vh+1∶H, state-action collections Ch∶H,
buffers Bh∶H, and counters th∶H.
/* Initialize parameters. */
3: Set M ←⌈8ε−1CpushH⌉.
4: Set Ntest ←28M 2Hε−1 log(8M 6H8ε−2δ−1), Nreg ←28M 2ε−1 log(8∣V∣2HM 2δ−1),
5: Set Nest(k) ←2N 2
reg log(8ANregHk3/δ) and δ′ ←δ/(8M 7N 2
testH8∣V∣)."
N,0.4729637234770705,"6: Set ε2
reg ←9MH2 log(8M 2H∣V∣2/δ)"
N,0.4736481861738535,"Nreg
+ 34MH3 log(8M 6N2
testH8/δ)
Ntest
."
N,0.47433264887063653,"7: Set β(t) ←
√"
N,0.4750171115674196,2log1/δ′(8AM∣V∣t2/δ).
N,0.4757015742642026,"/* Test the fit for the estimated value functions ̂Vh+1∶H at future layers. */
8: for (xh−1,ah−1) ∈Ch do
9:
for layer ℓ= H,...,h + 1 do
10:
for n = 1,...,Ntest do
11:
Draw xh ∼Th−1(⋅∣xh−1,ah−1), then draw xℓ−1 by rolling out with ̂πh∶H, where10"
N,0.47638603696098564,"∀τ ∈[H],
̂πτ(⋅) ∈arg max
a∈A
̂Pτ,ε,δ′[̂Vτ+1](⋅,a),
with
̂VH+1 ≡0.
(14)"
N,0.47707049965776865,"12:
for aℓ−1 ∈A do"
N,0.47775496235455167,"/* Number of times ̂Pℓ−1,ε,δ′ (Algorithm 7) is called in the test on Line
14. */
13:
Update tℓ←tℓ+ 1.
/* Test fit; if test fails, re-fit value functions ̂Vh+1∶ℓup to layer
ℓ. */
14:
if supf∈̂Vℓ∣(̂Pℓ−1,ε,δ′[̂Vℓ] −̂Pℓ−1,ε,δ′[fℓ])(xℓ−1,aℓ−1)∣> ε + ε ⋅β(tℓ) then"
N,0.4784394250513347,"15:
Cℓ←Cℓ∪{(xℓ−1,aℓ−1)} and Bℓ←Bℓ∪{(xℓ−1,aℓ−1, ̂Vℓ, ̂Vℓ,tℓ)}.
16:
for τ = ℓ,...,h + 1 do
17:
(̂Vτ∶H, ̂Vτ∶H,Cτ∶H,Bτ∶H,tτ∶H) ←RVFSτ(̂Vτ+1∶H, ̂Vτ+1∶H,Cτ∶H,Bτ∶H,tτ∶H;V,ε,δ).
18:
go to line 8.
19: if h = 0 then return: (̂V1∶H,⋅,⋅,⋅,⋅)."
N,0.4791238877481177,"/* Re-fit ̂Vh and build a new confidence set. */
20: for (xh−1,ah−1) ∈Ch do
21:
Set Dh(xh−1,ah−1) ←∅.
22:
for i = 1,...,Nreg do
23:
Sample xh ∼Th−1(⋅∣xh−1,ah−1).
24:
Let ̂Vh(xh) be a Monte-Carlo estimate for Êπh∶H[∑H
ℓ=h rℓ∣xh] computed by collecting
Nest(∣Ch∣) trajectories starting from xh and rolling out with ̂πh∶H.
25:
Update Dh(xh−1,ah−1) ←Dh(xh−1,ah−1) ∪{(xh, ̂Vh(xh))}.
26: Let ̂Vh ∶= arg minf∈̂V ∑(xh−1,ah−1)∈Ch ∑(xh,vh)∈Dh(xh−1,ah−1)(f(xh) −vh)2.
27: Compute value function confidence set"
N,0.4798083504449008,"̂Vh ∶=
⎧⎪⎪⎨⎪⎪⎩
f ∈V
RRRRRRRRRRRR
∑
(xh−1,ah−1)∈Ch"
NREG,0.4804928131416838,"1
Nreg
∑
(xh,-)∈Dh(xh−1,ah−1)
(̂Vh(xh) −f(xh))
2 ≤ε2
reg"
NREG,0.4811772758384668,"⎫⎪⎪⎬⎪⎪⎭
.
(15)"
NREG,0.48186173853524983,"28: return (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H)."
NREG,0.48254620123203285,Algorithm 6 RVFS.bc: Learn an executable policy with RVFS via behavior cloning.
NREG,0.48323066392881586,"1: input: Value function class V, policy class Π, suboptimality ε ∈(0,1), confidence δ ∈(0,1)."
NREG,0.4839151266255989,"/* Set parameters for RVFS. */
2: Set εRVFS ←εH−1/48.
3: Set ̂V1∶H ←arbitrary, ̂V1∶H ←V, C0∶H ←∅, B0∶H ←∅, and ti ←0, for all i ∈[0..H]."
NREG,0.48459958932238195,"/* Set parameters for BehaviorCloning. */
4: Set M ←⌈8ε−1
RVFSCpushH⌉and Ntest ←28M 2Hε−1
RVFS log(80M 6H8ε−2
RVFSδ−1).
5: Nreg ←28M 2ε−1
RVFS log(80∣V∣2HM 2δ−1) and δ′ =
δ
40M 7N 2H8∣V∣.
/* Get value functions from RVFS */
6: (̂V1∶H,⋅,⋅,⋅,⋅) ←RVFS0(̂V1∶H, ̂V1∶H,C0∶H,B0∶H,t0∶H;V,Nreg,Ntest,εRVFS,δ/10)."
NREG,0.48528405201916497,"/* Extract executable policy via BehaviorCloning algorithm for imitation learning. */
7: Define ̂πRVFS
h
(⋅) ∈arg maxa∈A ̂Ph,εRVFS,δ′[̂Vh+1](⋅,a).
8: Compute ̂π1∶H ←BehaviorCloning(Π,ε,̂πRVFS
1∶H ,δ/2)
9: return: ̂π1∶H."
NREG,0.485968514715948,"Algorithm 7 ̂Ph,ε,δ[f]: Estimate conditional expectation E[rh + f(xh+1) ∣xh = ⋅,ah = ⋅]."
NREG,0.486652977412731,"1: parameters: Layer h, suboptimality ε ∈(0,1), confidence δ ∈(0,1), target function f.
2: input: (x,a) ∈X × A.
3: Set Nsim ∶= 2log(1/δ)/ε2.
4: Set D ←∅
5: for i = 1,...,Nsim do
6:
Sample rh ∼Rh(x,a) and xh+1 ∼Th(⋅∣x,a).
// Uses local simulator access.
7:
Update D ←D ∪{(rh,xh+1)}.
8: return: N −1
sim ⋅∑(r,x)∈D(r + f(x))."
NREG,0.487337440109514,"F.2
RVFSexo Pseudocode"
NREG,0.48802190280629704,"Algorithm 8 RVFSexo
h : Recursive Value Function Search for Exogenous Block MDPs"
NREG,0.4887063655030801,"1: parameters: Value function class V, suboptimality ε ∈(0,1), seeds ζ1∶H ∈(0,1), confidence
δ ∈(0,1).
2: input: Level h ∈[0..H], value function estimates ̂Vh+1∶H, confidence sets ̂Vh+1∶H, state-
action collections Ch∶H, and buffers Bh∶H, and counters th∶H."
NREG,0.4893908281998631,"/* Initialize parameters. */
3: Set M ←⌈8ε−2CexoSAH⌉.
4: Set Ntest ←28M 2Hε−2 log(8M 6H8ε−2δ−1), Nreg ←28M 2ε−2 log(8∣V∣HM 2δ−1).
5: Set Nest(k) ←2N 2
reg log(8ANregHk3/δ) and δ′ ←δ/(4M 7N 2
testH8∣V∣)."
NREG,0.49007529089664614,"6: Set ε2
reg ←9MH2 log(8M 2H∣V∣/δ)"
NREG,0.49075975359342916,"Nreg
+ 34MH3 log(8M 6N 2
testH8/δ)
Ntest
."
NREG,0.4914442162902122,"7: Set β(t) ←
√"
NREG,0.4921286789869952,log1/δ′(8MA∣V∣t2/δ).
NREG,0.4928131416837782,"/* Test the fit for the estimated value functions ̂Vh+1∶H at future layers. */
8: for (xh−1,ah−1) ∈Ch do
9:
for layer ℓ= H,...,h + 1 do
10:
for n = 1,...,Ntest do
11:
Draw xh ∼Th−1(⋅∣xh−1,ah−1), then draw xℓ−1 by rolling out with ̂πh+1∶H, where"
NREG,0.4934976043805613,"∀τ ∈[H], ̂πτ(⋅) ∈arg max
a∈A
⌈̂Pτ,ε2,δ′[̂Vτ+1](⋅,a) ⋅ε−1 + ζτ⌉,
with
̂VH+1 ≡0."
NREG,0.4941820670773443,"12:
for aℓ−1 ∈A do
13:
Update tℓ←tℓ+ 1.
/* Test fit; if test fails, re-fit value functions ̂Vh+1∶ℓup to layer
ℓ. */
14:
if supf∈̂Vℓ∣(̂Pℓ−1,ε2,δ′[̂Vℓ] −̂Pℓ−1,ε2,δ′[fℓ])(xℓ−1,aℓ−1)∣> ε2 + ε2 ⋅β(tℓ) then"
NREG,0.4948665297741273,"15:
Cℓ←Cℓ∪{(xℓ−1,aℓ−1)} and Bℓ←Bℓ∪{(xℓ−1,aℓ−1, ̂Vℓ, ̂Vℓ,tℓ)}.
16:
for τ = ℓ,...,h + 1 do
17:
(̂Vτ∶H, ̂Vτ∶H,Cτ∶H,Bτ∶H,tτ∶H) ←RVFSexo
τ (̂Vτ+1∶H, ̂Vτ+1∶H,Cτ∶H,Bτ∶H,tτ∶H;V,ε,ζ1∶H,δ).
18:
go to line 8.
19: if h = 0 then return (̂V1∶H,⋅,⋅,⋅,⋅)."
NREG,0.49555099247091033,"/* Re-fit ̂Vh and build a new confidence set. */
20: for (xh−1,ah−1) ∈Ch do
21:
Set Dh(xh−1,ah−1) ←∅.
22:
for i = 1,...,Nreg do
23:
Sample xh ∼Th−1(⋅∣xh−1,ah−1).
24:
For each a ∈A, let ̂Vh(xh) be a Monte-Carlo estimate for Êπh∶H[∑H
ℓ=h rℓ∣xh] computed by
collecting Nest(∣Ch∣) trajectories starting from xh and rolling out with ̂πh∶H.
25:
Update D(xh−1,ah−1) ←D(xh−1,ah−1) ∪{(xh, ̂Vh(xh))}.
26: Let ̂Vh ∶= arg minf∈Vh ∑(xh−1,ah−1)∈Ch ∑(xh,vh)∈Dh(xh−1,ah−1)(f(xh) −vh)2.
27: Compute value function confidence set"
NREG,0.49623545516769335,"̂Vh ∶=
⎧⎪⎪⎨⎪⎪⎩
f ∈Vh"
NREG,0.49691991786447637,"RRRRRRRRRRRR
∑
(xh−1,ah−1)∈Ch"
NREG,0.4976043805612594,"1
Nreg
∑
(xh,-)∈Dh(xh−1,ah−1)
(̂Vh(xh) −f(xh))
2 ≤ε2
reg"
NREG,0.49828884325804246,"⎫⎪⎪⎬⎪⎪⎭
.
(16)"
NREG,0.4989733059548255,"28: return (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H)."
NREG,0.4996577686516085,Algorithm 9 RVFSexo.bc: Learn an executable policy with RVFSexo via imitation learning.
NREG,0.5003422313483915,"1: input: Decoder class Φ, suboptimality ε ∈(0,1), confidence δ ∈(0,1)."
NREG,0.5010266940451745,"/* Set parameters for RVFS and define the value function and policy classes. */
2: Set εRVFS ←εH−1/48.
3: Set V = V1∶H, where Vh = {x ↦f(ϕ(x)) ∶f ∈[0,H]S,ϕ ∈Φ}, ∀h ∈[H].
4: Set Π = Π1∶H, where Πh = {π(⋅) ∈arg maxa∈A f(ϕ(⋅),a) ∶f ∈[0,H]S×A,ϕ ∈Φ}, ∀h ∈[H]."
NREG,0.5017111567419575,"/* Set parameters for BehaviorCloning. */
5: Set Nbc ←8H2 log(4H∣Π∣/δ)/ε, Nboost ←log(1/δ)/log(24SAHε), Neval ←162ε−2 log(2Nboost/δ).
6: Set M ←⌈8ε−1
RVFSSACcovH⌉, Ntest ←28M 2Hε−1
RVFS log(80M 6H8Nboostε−2
RVFSδ−1), and δ′ =
δ
40M 7N2H8∣V∣Nboost ."
NREG,0.5023956194387406,"7: Set Nreg ←28M 2ε−1
RVFS log(80∣Φ∣2HM 2Nboostδ−1).
8: Set ̂V1∶H ←arbitrary, ̂V1∶H ←V, C0∶H ←∅, B0∶H ←∅, iopt = 1, and Jmax = 0."
NREG,0.5030800821355236,"/*
Repeatedly
invoke
RVFSexo
and
extract
policy
with
BehaviorCloning
to
boost
confidence. */
9: for i = 1,...,Nboost do"
NREG,0.5037645448323066,"/* Invoke RVFSexo. */
10:
(̂V
(i)
1∶H,⋅,⋅,⋅,⋅) ←RVFSexo
0 (̂V1∶H, ̂V1∶H,C0∶H,B0∶H;V,Nreg,Ntest,εRVFS,δ/(10Nboost)).
/* Imitation learning with BehaviorCloning. */
11:
Define ̂πRVFS
h
(⋅) ∈arg maxa∈A ̂Ph,εRVFS,δ′[̂V
(i)
h+1](⋅,a).
12:
Compute ̂π
(i)
1∶H ←BehaviorCloning(Π,ε,̂πRVFS
1∶H ,δ/(2Nboost)).
/* Evaluate current policy. */
13:
v = 0.
14:
for = 1,...,Neval do
15:
Sample trajectory (x1,a1,r1,...,xH,aH,rH) by executing ̂π
(i)
1∶H.
16:
Set v ←v + ∑H
h=1 rh.
17:
Set ̂J(̂π
(i)
1∶H) ←v/Neval.
18:
if ̂J(̂π
(i)
1∶H) > Jmax then
19:
Set iopt = i.
20:
Set Jmax = ̂J(̂π
(i)
1∶H)."
NREG,0.5044490075290896,"21: return: ̂π1∶H = ̂π
(iopt)
1∶H ."
NREG,0.5051334702258727,"G
Organization"
NREG,0.5058179329226558,"This remainder of Part III of the appendix contains the proofs for the main results concerning the
RVFS algorithm (Theorem 4.1 and Theorem B.1)."
NREG,0.5065023956194388,"• First, in Appendix H we give a brief overview of the analysis and introduce a restricted
set of benchmark policies which will be used throughout the proofs for Theorem 4.1 and
Theorem B.1. The benchmark policy class is central to the regret decomposition for RVFS,
and facilitates an analysis that does not require optimism."
NREG,0.5071868583162218,"• In Appendix I, we prove Theorem 4.1 under Setup II (V π-realizability). This constitutes
the main technical development for Theorem 4.1. The central technical results proven here
are Theorem I.1, Theorem I.2 and which generalize Theorem 4.1."
NREG,0.5078713210130048,"• In Appendix J, we prove Theorem 4.1 under Setup I (V ⋆-realizability), as a straightforward
consequence of the tools developed in Appendix I (Theorem I.1 and Theorem I.2)."
NREG,0.5085557837097878,"• Finally, in Appendix K, we prove Theorem B.1 (analysis of RVFSexo for the ExBMDP
problem). This analysis has a similar structure to the proof of Theorem 4.1 under Setup II in
Appendix I, and builds on the same analysis techniques, but requires specialized arguments
due to extra technical challenges in the ExBMDP setting."
NREG,0.5092402464065708,"• Appendix M gives a self-contained presentation of the BehaviorCloning algorithm for
imitation learning, which is used within RVFS.bc and RVFSexo.bc."
NREG,0.5099247091033539,"H
Overview of Analysis and Preliminaries"
NREG,0.5106091718001369,"In this section, we present some notation, technical tools, and preliminary results we require for the
analysis of RVFS in the settings we described in Section 4. We start by defining a set of restricted
benchmark policies used in the regret decomposition for RVFS."
NREG,0.5112936344969199,"H.1
Overview of Analysis
In this section we give a brief overview of the analysis techniques behind Theorem 4.1 and Theo-
rem B.1. We focus on Theorem 4.1 to begin."
NREG,0.5119780971937029,"Recall that RVFS is recursive in the sense that whenever the test in Line 14 fails for a layer h ∈[H],
an recursive call RVFSh is initiated. Throughout the recursion, via the steps in Item 1 and Item 2,
RVFS maintains the following invariant: whenever a call to RVFSh (an instance of RVFS initiated at
layer h) terminates, the confidence sets ̂Vh+1∶H that it outputs satisfy, with high probability:"
NREG,0.5126625598904859,"∀ℓ∈[h + 1..H],
V ⋆
ℓ∈̂Vℓ.
(Inv1)"
NREG,0.5133470225872689,"In addition, RVFSh can only return if the value function tests in Line 14 (which involve the confidence
sets ̂Vh+1∶H) all succeed. From the invariant in (Inv1), it can be shown that the tests only succeed if
the estimated value functions ̂Vh+1∶H satisfy:"
NREG,0.5140314852840521,"∀ℓ∈[h + 1..H],
P̂π [sup
a∈A
∣(Pℓ[̂Vℓ+1] −Pℓ[V ⋆
ℓ+1])(xh,a)∣≥3ε] ≤εtest,
(Inv2)"
NREG,0.5147159479808351,"where εtest > 0 is a parameter set by the algorithm. We use pushforward coverability to show
that RVFS can only expand the core-sets C1∶H a polynomial number of times before the algorithm
terminates and (Inv2) is satisfied."
NREG,0.5154004106776181,The invariant in (Inv2) is useful because it ensures that the greedy policies
NREG,0.5160848733744011,"̂πh(x) ≈arg max
a∈A
Ph[̂Vh+1](x,a)"
NREG,0.5167693360711841,"induced by the learned value functions ̂V1∶H are near-optimal. To make this precise, recall that given
parameters ε,δ ∈(0,1), the action ̂πh(x) of RVFS’s policy at layer h and state x ∈X, is given by"
NREG,0.5174537987679672,"̂πh(x) ∈arg max
a∈A
̂Ph,ε,δ[̂Vh+1](x,a),
(17)"
NREG,0.5181382614647502,"where ̂Vh+1 is the estimated value function at layer h + 1. The operator ̂Ph,ε,δ[̂Vh+1] (Algorithm 7),
when given input (x,a) ∈X × A, ensures that probability at least 1 −δ, ∣̂Ph,ε,δ[̂Vh+1](x,a) −"
NREG,0.5188227241615332,"Ph[̂Vh+1](x,a)∣≤ε. Combining this with the invariant in (Inv2) and the fact that V ⋆
h ≡Ph[V ⋆
h+1],
one can see that with high probability (over the randomness in xh ∼P̂π and ̂P), we have:"
NREG,0.5195071868583162,"max
a∈A ∣̂Ph,ε,δ[̂Vh+1](xh,a) −V ⋆
h (xh,a)∣≤4ε.
(18)"
NREG,0.5201916495550992,"Analysis under Setup I.
For Theorem 4.1 (Setup I), Eq. (18) together with the definition of ̂πh in
Eq. (17) and the gap assumption (Assumption 4.4) implies that if ε ≤∆/8, we have that with high
probability (over the randomness in xh ∼P̂π and ̂P),"
NREG,0.5208761122518822,"̂πh(xh) ∈arg max
a∈A
̂Ph,ε,δ[̂Vh+1](xh,a) = arg max
a∈A
V ⋆
h (x,a) = π⋆
h(xh)."
NREG,0.5215605749486653,"This suffices to show that ̂π is near-optimal, since by the performance lemma [33], the suboptimality
of ̂π can be bounded as"
NREG,0.5222450376454483,"J(π⋆) −J(̂π) ≤
H
∑
h=1
P̂π[̂πh(xh) ≠π⋆
h(xh)].
(19)"
NREG,0.5229295003422314,This suffices to prove the performance bound in Theorem 4.1 under Setup I.
NREG,0.5236139630390144,"Analysis under Setup II.
For Theorem 4.1 (Setup II), an immediate challenge in applying a
similar analysis to Setup I is the lack of suboptimality gap ∆, which makes it impossible to directly
bound the probability that ̂π ≠π⋆in Eq. (19). To address this, we introduce a restricted benchmark
policy class Πε ⊂ΠS in Appendix H.2 below. The class Πε ⊂ΠS is constructed such that that there
exists a policy π ∈Πε that (i) is O(ε)-suboptimal, and (ii) emulates certain properties of a gap.
Together, these properties facilitate analysis similar to Setup I. Overall, this argument is similar to
the “virtual policy iteration” analysis in Yin et al. [65]."
NREG,0.5242984257357974,"Analysis of RVFSexo.
The analysis of RVFSexo for ExBMDPs (Theorem B.1) uses the same idea as
the analysis for Setup II, except that we can only realize V π for endogenous policies that act on
ϕ⋆(xh). To address this, we use the randomized rounding scheme in RVFSexo, and the crux of the
proof is to show that with high probability, the rounded policies in RVFSexo “snap” onto endogenous
policies, facilitating an argument similar to Setup II."
NREG,0.5249828884325805,"Generalizing the analysis.
We mention in passing that RVFS can be slightly modified to recover
other existing sample complexity guarantees for RL with linear function approximation and local
simulator access that do not require pushforward coverability, including linear-Q⋆realizability with
gap [40] and Qπ-realizability [65]; we leave a more general treatment for future work."
NREG,0.5256673511293635,"H.2
Benchmark Policy Class and Randomized Policies
As described above, central to our analysis is a set of O(ε)-suboptimal policies against which we
benchmark the policies returned RVFS, which emulate certain consequences of the ∆-gap assumption
(Assumption 4.4). Before introducing this concept formally, we first define the notion of a randomized
policy."
NREG,0.5263518138261465,"Induced stochastic policies.
Given an arbitrary collection of independent random variables ̃
Q =
( ̃
Qh(x,a))(h,x,a)∈[H]×X×A, we say that a policy π is induced by ̃
Q if π satisfies"
NREG,0.5270362765229295,"∀h ∈[H],∀x ∈X,
πh(x) ∈arg max
a′∈A
̃
Qh(x,a′),
(20)"
NREG,0.5277207392197125,"where we use the bold notation πh(x) as shorthand for the random variable ah ∼πh(x) ∈∆(A);
in other words, for each x ∈X, πh(x) ∈∆(A) is the distribution induced by sampling Qh(x,⋅)
and playing the action πh(x) ∈arg maxa′∈A ̃
Qh(x,a′). If there are ties in (20), we break them by
picking the action with the smallest index; we assume without loss of generality that actions in A are
index from 1,...,∣A∣."
NREG,0.5284052019164955,"Benchmark policy class.
We now define the benchmark policy class as follows."
NREG,0.5290896646132786,"Definition H.1 (Benchmark policy class). For ε ∈(0,1), let Πε ⊆ΠS be the set of stochastic
policies such that π ∈Πε if and only if there exists a collection of independent random variables
̃
Q = ( ̃
Qh(x,a))(h,x,a)∈[H]×X×A in [0,H] such that:"
NREG,0.5297741273100616,"• π is induced by ̃
Q (i.e. Eq. (20) is satisfied); and"
NREG,0.5304585900068446,"• For all (h,x,a) ∈[H] × X × A, we have ∣( ̃
Qh −Qπ
h)(x,a)∣≤ε, almost surely under the
draw of ̃
Q."
NREG,0.5311430527036276,"Intuitively, the set Πε contains the set of all (stochastic) policies corresponding to (randomized)
state-action value functions that are point-wise O(ε) close to Q⋆. We formalize this claim in the next
lemma."
NREG,0.5318275154004107,"Lemma H.1 (Suboptimality of benchmark policies). Let ε ∈(0,1) be given. Let ˜π ∈Πε be a
stochastic policy induced by a collection of (independent) random state-action value functions
( ̃
Qh(x,a))(h,x,a)∈[H]×X×A ⊂[0,H] such that for all h ∈[H] and all (x,a) ∈X × A:"
NREG,0.5325119780971937,"∣̃
Qh(x,a) −Q˜π
h(x,a)∣≤ε almost surely, and
˜πh(x) ∈arg max
a′∈A
̃
Qh(x,a′)."
NREG,0.5331964407939768,"Then, for all h ∈[H],"
NREG,0.5338809034907598,"∀x ∈X,
V ⋆
h (x) ≤V ˜π
h (x) + 3Hε.
(21)"
NREG,0.5345653661875428,"Proof of Lemma H.1. Using backward induction over ℓ= H,...,1, we start by showing that all ℓ:"
NREG,0.5352498288843258,"∀(x,a) ∈X × A,
Q⋆
ℓ(x,a) ≤̃
Qℓ(x,a) + 2ε ⋅(H −ℓ+ 1).
(22)"
NREG,0.5359342915811088,"almost surely. We then instantiate this with ℓ= 1 and use the fact that ∥̃
Qh −Q˜π
h∥∞≤ε to get the
desired result."
NREG,0.5366187542778919,"Base case [ℓ= H].
By definition of the state-action value function, we have, for all π ∈ΠS,
Q⋆
H ≡Qπ
H. Thus, since sup(x,a)∈X×A ∣( ̃
QH −Q˜π
H)(x,a)∣≤ε almost surely (by definition of ̃
Q1∶H),
we get that"
NREG,0.5373032169746749,"∀(x,a) ∈X × A,
∣̃
QH(x,a) −Q⋆
H(x,a)∣≤ε,
almost surely. This implies (21) for ℓ= H."
NREG,0.5379876796714579,"General case [ℓ< h].
Fix h ∈[H −1] and suppose that (22) holds for all ℓ∈[h + 1,...,H] almost
surely. We show that it holds for ℓ= h almost surely. Fix (x,a) ∈X × A. We have"
NREG,0.5386721423682409,"Q⋆
h(x,a) −̃
Qh(x,a) = Th[Q⋆
h+1](x,a) −Th[ ̃
Qh+1](x,a) + Th[ ̃
Qh+1](x,a) −̃
Qh(x,a),"
NREG,0.5393566050650239,"≤2ε ⋅(H −h) + Th[ ̃
Qh+1](x,a) −̃
Qh(x,a),
(23)
almost surely, where the last step follows by the induction hypothesis.
We now bound
Th[ ̃
Qh+1](x,a) −̃
Qh(x,a). We have, almost surely, that"
NREG,0.5400410677618069,"Th[ ̃
Qh+1](x,a) −̃
Qh(x,a) = Th[ ̃
Qh+1](x,a) −Ph[V ˜π
h+1](x,a) + Ph[V ˜π
h+1](x,a) −̃
Qh(x,a),"
NREG,0.54072553045859,"= Th[ ̃
Qh+1](x,a) −Ph[V ˜π
h+1](x,a) + Q˜π
h(x,a) −̃
Qh(x,a),"
NREG,0.5414099931553731,"= Th[ ̃
Qh+1](x,a) −Ph[V ˜π
h+1](x,a) + ε,
(by the assumption on ̃
Qh)"
NREG,0.5420944558521561,"= E[max
a′∈A
̃
Qh+1(xh+1,a′) −Q˜π
h+1(xh+1, ˜πh+1(xh+1)) ∣xh = x,ah = a] + ε,"
NREG,0.5427789185489391,"= E[ ̃
Qh+1(xh+1, ˜πh+1(xh+1)) −Q˜π
h+1(xh+1, ˜πh+1(xh+1)) ∣xh = x,ah = a] + ε,"
NREG,0.5434633812457221,"≤2ε,
(24)
where the penultimate inequality follows by the definition of ˜πh+1, and the last inequality follows by
the fact that ∥̃
Qh+1 −Q˜π
h+1∥∞≤ε almost surely, by assumption. Plugging (24) into (23) completes
the induction, and so we have that
∀(x,a) ∈X × A,
Q⋆
1(x,a) ≤̃
Q1(x,a) ≤2Hε.
In particular, taking the max over a on both sides and using the definition of ˜π, we get that"
NREG,0.5441478439425051,"∀x ∈X,
V ⋆
1 (x) ≤̃
Q1(x, ˜π1(x)) ≤2Hε,"
NREG,0.5448323066392882,"almost surely. Combining this with the fact that ̃
Q1(x, ˜π1(x)) ≤Q˜π
1(x, ˜π1(x)) + ε, almost surely
(since ∥̃
Q1 −Q˜π
1∥∞≤ε almost surely by assumption) implies that
V ⋆
1 (x) ≤Qπ
1(x, ˜π1(x)) + 2Hε + ε.
Taking expectation over ˜π1(x) and bounding 2Hε + ε by 3Hε leads to the desired result."
NREG,0.5455167693360712,"H.3
Additional Preliminaries
The following lemma gives a guarantee for the Bellman backup approximation algorithm ̂P (Algo-
rithm 7) that is tailored to the analysis of RVFS."
NREG,0.5462012320328542,"Lemma H.2. Let ε,δ,δ′ ∈(0,1), B > 0, and h ∈[H], be given and let V be a finite function
class. For any sequence (xi)i≥1 ⊂X of state action pairs, the outputs (̂Ph,ε,δ′[f](xi,a))i≥1,a∈A of
Algorithm 7 satisfy, with probability at least 1 −δ,"
NREG,0.5468856947296372,"∀i ≥1,∀f ∈V,∀a ∈A,
∣̂Ph,ε,δ′[f](xi,a) −Ph[f](xi,a)∣≤ε ⋅
√"
NREG,0.5475701574264202,2log1/δ′(2Ai2∣V∣/δ).
NREG,0.5482546201232033,"Proof of Lemma H.2. By Hoeffding’s inequality [28] and the union bound over a ∈A and f ∈V,
we have that for any i ≥1, with probability at least 1 −δ/(2i2),"
NREG,0.5489390828199863,"∀f ∈V,∀a ∈A,
∣̂Ph,ε,δ′[f](xi,a) −Ph[f](xi,a)∣≤ε ⋅
√"
NREG,0.5496235455167693,2log1/δ′(2Ai2∣V∣/δ).
NREG,0.5503080082135524,The desired result follows by the union bound over i ≥1 and the fact that ∑i≥1 1/i2 = π2/6 ≤2.
NREG,0.5509924709103354,"I
Guarantee under V π-Realizability (Proof of Theorem 4.1, Setup II)"
NREG,0.5516769336071184,"In this section, we prove Theorem 4.1 under Setup II. First, in Appendix I.1 we state a number
of supporting technical lemmas, then use them to prove a more general version of Theorem 4.1,
Theorem I.2, which holds under a weaker realizability assumption (informally, V π-realizability only
for near-optimal policies π); Theorem 4.1 follows as an immediate consequence. The remainder of
the section (Appendix I.2 through to Appendix I.6) contains the proofs for the intermediate results."
NREG,0.5523613963039015,"I.1
Analysis: Proof of Theorem 4.1 (Setup II)
We analyze RVFS in the setting of Theorem 4.1 (Setup II), where we have a function class V satisfying
V π-realizability (Assumption 4.5). We will actually show that the conclusion of Theorem 4.1 holds
under a weaker function approximation setup we refer to as relaxed V π-realizability: instead of
requiring V π-realizability for all π ∈ΠS, we only require it for policies π in the set of near-optimal
policies corresponding to the benchmark policy class Πεreal for some εreal > 0 (Πε is defined in
Appendix H)."
NREG,0.5530458590006845,"Assumption I.1 (Relaxed V π-realizability). For εreal > 0 and all π ∈Πεreal and h ∈[H], we have
V π
h (x) ∈V ⊆{f ∶X →[0,H]}."
NREG,0.5537303216974675,"We will analyze RVFS under Assumption I.1 and Assumption 4.1. However, it turns out that all of
the main results for RVFS can be derived under this assumption: As we will see in Appendix J in the
sequel, when the ∆-gap assumption (Assumption 4.4) is satisfied, then Πεreal = {π⋆} for all εreal < ∆,
allowing us to prove Theorem 4.1 under Setup I as a special case of relaxed V π-realizability. Our
analysis for the ExBMDP setting in Appendix K requires more work, but uses that for ExBMDPs,
Assumption I.1 is satisfied for a subset of Πεreal corresponding to endogenous policies."
NREG,0.5544147843942505,"We begin with our analysis under Setup II by bounding the number of times the test in Line 14 fails.
Since the sizes of the core sets C1∶H in RVFS are directly related to the number of test failures, the
next result, which bounds ∣Ch∣for h ∈[H], allows us to show that RVFS terminates in polynomial
iterations with high probability. The proof is in Appendix I.2."
NREG,0.5550992470910335,"Lemma I.1 (Bounding the number of test failures). Let δ,ε ∈(0,1) be given, and suppose that
Assumption 4.1 (pushforward coverability) holds with parameter Cpush > 0. Further, let f ∈V, be
given, where V is an arbitrary function class. Then there is an event E of probability at least 1 −δ
under which any call RVFS0(f,V1∶H,∅,∅,0;V,ε,δ) (Algorithm 5) terminates, and throughout the
execution of RVFS0, we have"
NREG,0.5557837097878165,"∀h ∈[H],
∣Ch∣≤⌈8ε−1CpushH⌉.
(25)"
NREG,0.5564681724845996,"In particular, Lemma I.1 ensures that with high probability, every call to RVFSh (made recursively
via the call to RVFS0) terminates in polynomial iterations. When RVFSh terminates, all the tests in
Line 14 must have passed for all ℓ> h. Using this and a standard concentration argument, we get the
following guarantee for the estimated value functions and confidence sets returned by each call to
RVFSh. The proof is in Appendix I.3."
NREG,0.5571526351813826,"Lemma I.2 (Consequence of passing the tests). Let h ∈[0..H] and ε,δ ∈(0,1) be given and
consider a call to RVFS0 in the setting of Lemma I.1. Further, let E be the event of Lemma I.1.
There exists an event E′
h of probability at least 1 −δ/H such that under E ∩E′
h, if a call to RVFSh
within the execution of RVFS0 terminates and returns (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H), then for any
(xh−1,ah−1) ∈Ch and ℓ∈[h + 1..H + 1]:"
NREG,0.5578370978781656,"P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣Pℓ−1[̂Vℓ] −Pℓ−1[fℓ](xℓ−1,a)∣> 3ε ∣xh−1 = xh−1,ah−1 = ah−1
⎤⎥⎥⎥⎥⎦
≤4log(8M 6N 2
testH8/δ)
Ntest
, (26)"
NREG,0.5585215605749486,"where (̂πτ)τ≥h is the stochastic policy induced by ̂Vh∶H and M and Ntest are defined as in Algo-
rithm 5. Furthermore, under the event E, the total number of times the operator ̂P is called in the
test of Line 14 of Algorithm 5 is at most O(CpushNtestH4ε−1)."
NREG,0.5592060232717317,"We now give a guarantee for the estimated value functions ̂V1∶H computed within RVFS in Line 26
(the proof is in Appendix I.4)."
NREG,0.5598904859685148,"Lemma I.3 (Value function regression guarantee). Let h ∈[0..H] and δ,ε′ ∈(0,1) be given, and
consider a call to RVFS0 in the setting of Lemma I.1. Further, let Π′ ⊆ΠS be a finite policy class such
that the class V realizes the value functions V π for π ∈Π′ (i.e. V satisfies Assumption I.1 with Πεreal
replaced by Π′). Then, there is an event E′′
h of probability at least 1 −δ/H under which for all k ≥1,
if"
NREG,0.5605749486652978,1. RVFSh gets called for the kth time during the execution of RVFS0; and
NREG,0.5612594113620808,"2. this kth call terminates and returns (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H),"
NREG,0.5619438740588638,"then if (̂πτ)τ≥h is the policy induced by ̂Vh∶H and Nreg is set as in Algorithm 5, we have that for all
π ∈Π′,"
NREG,0.5626283367556468,"∑
(xh−1,ah−1)∈Ch"
NREG,0.5633127994524298,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V π
h (xh))
2"
NREG,0.5639972621492129,≤9kH2 log(8k2H∣Π′∣∣V∣/δ)
NREG,0.5646817248459959,"Nreg
+ 8H2
∑
(xh−1,ah−1)∈Ch"
NREG,0.5653661875427789,"H
∑
τ=h
Êπ [Dtv(̂πτ(xτ),πτ(xτ)) ∣xh−1 = xh−1,ah−1 = ah−1],"
NREG,0.5660506502395619,"where the datasets {Dh(x,a) ∶(x,a) ∈Ch} are as in the definition of ̂Vh in (15)."
NREG,0.5667351129363449,"Next, we use this result to show that the confidence sets ̂V1∶H returned by RVFS0 are “valid” in the
sense that they contain a value function (V ˜π
h ) corresponding to a near-optimal stochastic policy ˜π in
the benchmark class Π4ε. In the sequel, we use this fact to substitute V ˜π
ℓfor fℓin Eq. (26) and bound
the suboptimality of the learned policy ̂π."
NREG,0.567419575633128,"Lemma I.4 (Confidence sets). Let ε,δ ∈(0,1) be given and suppose that Assumption 4.1 (push-
forward coverability) holds with parameter Cpush > 0 . Let f ∈V be arbitrary, and suppose that
V satisfies Assumption I.1 with εreal = 4ε. Then, there is an event E′′′ of probability at least
1 −3δ under which a call to RVFS0(f,V,∅,∅,0;V,ε,δ) (Algorithm 5) terminates and returns tuple
(̂V1∶H, ̂V1∶H,C1∶H,B1∶H,t1∶H) such that"
NREG,0.568104038329911,"∀h ∈[H],
V ˜π
h ∈̂Vh,"
NREG,0.5687885010266941,where ˜π1∶H ∈ΠS is the stochastic policy defined recursively via
NREG,0.5694729637234771,"∀x ∈X, ˜πτ(x) ∈arg max
a∈A
{
̂
Qτ(x,a),
if ∥̂
Qτ(x,⋅) −Pτ[V ˜π
τ+1](x,⋅)∥∞≤4ε,
Pτ[V ˜π
τ+1](x,a),
otherwise,
for τ = H,...,1, (27)"
NREG,0.5701574264202601,"where ̂
Qτ(x,a) ∶= ̂Pτ,ε,δ′[̂Vτ+1](x,a) is a realization of the stochastic output of the ̂P operator in
Algorithm 7 given input (x,a), and δ′ is as in Algorithm 5. Furtherore, we have ˜π ∈Π4ε."
NREG,0.5708418891170431,The proof of the lemma is in Appendix I.5.
NREG,0.5715263518138262,"Equipped with the preceding lemmas, we now state the main technical result of this section, Theo-
rem I.1, a generalization of Theorem 4.1 which holds under relaxed V π-realizability (Assumption I.1).
The proof is in Appendix I.6."
NREG,0.5722108145106092,"Theorem I.1 (Guarantee for RVFS under relaxed V π-realizability). Let δ,ε ∈(0,1) be given, and
suppose that Assumption 4.1 (pushforward coverability) holds with parameter Cpush > 0. Let f ∈V
be arbitrary, and assume that V that satisfies Assumption I.1 with εreal = 4ε. Then, with probability
at least 1 −5δ, RVFS0(f,V,∅,∅,0;V,ε,δ) (Algorithm 5) terminates and returns value functions
̂V1∶H that satisfy"
NREG,0.5728952772073922,"∀h ∈[H],
Êπ[Dtv(̂πh(xh), ˜πh(xh))] ≤
ε
4H3Cpush
,"
NREG,0.5735797399041752,"where ̂πh(x) ∈arg maxa∈A ̂Ph,ε,δ′[̂Vh+1](x,a) for all h ∈[H], with ˜π ∈Π4ε defined as in
Lemma I.4 and δ′ defined as in Algorithm 5. Furthermore, the number of episodes is bounded
by"
NREG,0.5742642026009582,"̃O(C8
pushH10A ⋅ε−13)."
NREG,0.5749486652977412,"Next, we state a guarantee for the outer-level algorithm, RVFS.bc, under relaxed V π-realizability.
Recall that RVFS.bc invokes RVFS0, then extracts an executable policy by applying the
BehaviorCloning algorithm (see Appendix M), with the “expert” policy set to be the output of RVFS."
NREG,0.5756331279945243,"Theorem I.2 (Main guarantee of RVFS.bc). Let δ,ε ∈(0,1) be given, and define εRVFS = εH−1/48.
Suppose that"
NREG,0.5763175906913073,• Assumption 4.1 (pushforward coverability) holds with parameter Cpush > 0;
NREG,0.5770020533880903,• the function class V satisfies Assumption I.1 with εreal = 1 (i.e. all π-realizability); and
NREG,0.5776865160848734,• the policy class Π satisfies Assumption 4.3.
NREG,0.5783709787816564,"Then, with probability at least 1 −δ, ̂π1∶H = RVFS.bc(Π,V,ε,δ) (Algorithm 6) satisfies"
NREG,0.5790554414784395,"J(π⋆) −J(̂π1∶H) ≤ε.
(28)"
NREG,0.5797399041752225,"Furthermore, the total number sample complexity in the RLLS framework is bounded by"
NREG,0.5804243668720055,"̃O (C8
pushH23Aϵ−13)."
NREG,0.5811088295687885,"The proof is in Appendix I.7. Note that Theorem I.2 is a restatement of Theorem 4.1 in Setup II
(restated for convenience). As a result, Theorem 4.1 is an immediate corollary."
NREG,0.5817932922655715,"Proof of Theorem 4.1. The result follows from Theorem I.2, since Assumption 4.5 is stronger than
Assumption I.1."
NREG,0.5824777549623545,"I.2
Proof of Lemma I.1 (Number of Test Failures)
Proof of Lemma I.1. Fix h ∈[H]. We note that the size of Ch corresponds to the number of times
the test in Line 14 fails for ℓ= h throughout the execution of RVFS0(f,V,∅,∅;V,ε,δ)."
NREG,0.5831622176591376,"Let M ∶= ⌈8ε−1CpushH⌉denote the desired upper bound on ∣Ch∣. Suppose that the test in Line 14
fails at least twice for ℓ= h (if the test fails at most twice, then ∣Ch∣≤2 and so (25) holds for ℓ= h
trivially), and let"
NREG,0.5838466803559206,"(x
(1)
h−1,a
(1)
h−1, ̂V
(1)
h , ̂V
(1)
h ,t
(1)
h ),(x
(2)
h−1,a
(2)
h−1, ̂V
(2)
h , ̂V
(2)
h ,t
(2)
h ),..."
NREG,0.5845311430527036,"denote the elements of the set Bh in the order at which they are added to the latter in Line 15 of
Algorithm 5. Note that ∣Bh∣= ∣Ch∣. Note also that t
(i)
h represents the number of times the subroutine
̂Ph−1,ε,δ′ has been called in the test of Line 14 throughout the execution of RVFS0 and up to the time
the test failed for (x
(i)
h−1,a
(i)
h−1). We will use this fact in a concentration argument in the sequel."
NREG,0.5852156057494866,"By definition of (̂V
(i)
h ) and Lemma C.4 (Freedman’s inequality) instantiated with"
NREG,0.5859000684462696,"• Q = {̂V
(i)
h
−fh ∶f ∈̂V
(i)
h };"
NREG,0.5865845311430528,• yh = xh;
NREG,0.5872689938398358,• B = H; and
NREG,0.5879534565366188,• n = Nreg ⋅i;
NREG,0.5886379192334018,"and the union bound over i ∈[M ∧∣Ch∣], we get that there is an event Eh of probability at least
1 −δ/(2H) under which"
NREG,0.5893223819301848,"∀i ∈[M ∧∣Ch∣],∀f ∈̂V
(i)
h ,
∑
j<i
E[(̂V
(i)
h (xh) −fh(xh))2 ∣xh−1 = x
(j)
h−1,ah−1 = a
(j)
h−1]"
NREG,0.5900068446269678,"≤˜ε2
reg ∶= 2ε2
reg + 4H2 log(4MH∣V∣/δ)"
NREG,0.5906913073237509,"Nreg
.
(29)"
NREG,0.5913757700205339,"Now, define f
(i)
h
∈arg maxf∈̂V(i)
h ∣E[̂V
(i)
h (xh) −fh(xh) ∣xh−1 = x
(i)
h−1,ah−1 = a
(i)
h−1]∣. From (29),
we have that under Eh:"
NREG,0.5920602327173169,"∀i ∈[M ∧∣Ch∣],
∑
j<i
E[(̂V
(i)
h (xh) −f
(i)
h (xh))2 ∣xh−1 = x
(j)
h−1,ah−1 = a
(j)
h−1] ≤˜ε2
reg.
(30)"
NREG,0.5927446954140999,"We now use this to bound the number of times the test in Line 14 fails for ℓ= h. Suppose for the sake
of contradiction that the test fails at least N times for some N ≥M (i.e. ∣Ch∣= N ≥M). Conditioned
on Eh, we have by Lemma C.8 and Eq. (30),"
NREG,0.5934291581108829,"min
i∈[M] sup"
NREG,0.5941136208076659,"f∈̂V(i)
h
∣E[̂V
(i)
h (xh) −fh(xh) ∣xh−1 = x
(i)
h−1,ah−1 = a
(i)
h−1]∣"
NREG,0.594798083504449,"= min
i∈[M]∣E[̂V
(i)
h (xh) −f
(i)
h (xh) ∣xh−1 = x
(i)
h−1,ah−1 = a
(i)
h−1]∣,"
NREG,0.5954825462012321,≤2(Cpush
NREG,0.5961670088980151,"M 2 M ˜ε2
reg log(2M))"
NREG,0.5968514715947981,"1/2
+ 2CpushH M
."
NREG,0.5975359342915811,"Now, substituting the expression of ˜ε2
reg in (29) and using the definition of ε2
reg in Line 6 of Algo-
rithm 5, we get"
NREG,0.5982203969883642,= 2(Cpush
NREG,0.5989048596851472,"M
⋅(2ε2
reg + 4MH2 log(4MH∣V∣/δ)"
NREG,0.5995893223819302,"Nreg
))"
NREG,0.6002737850787132,"1/2
+ 2CpushH M
,"
NREG,0.6009582477754962,≤2(Cpush
NREG,0.6016427104722792,"M
⋅(22MH2 log(8M 2H∣V∣2/δ)"
NREG,0.6023271731690623,"Nreg
+ 68MH3 log(8M 6N 2
testH8/δ)
Ntest
))"
NREG,0.6030116358658453,"1/2
+ 2CpushH M
,"
NREG,0.6036960985626283,"≤ε,
(31)"
NREG,0.6043805612594113,where the last inequality uses that M = ⌈8ε−1CpushH⌉and
NREG,0.6050650239561944,Nreg = 28M 2ε−1 log(8∣V∣2HM 2δ−1) and Ntest = 28M 2Hε−1 log(8M 6H8ε−2δ−1);
NREG,0.6057494866529775,see Line 5 of Algorithm 5.
NREG,0.6064339493497605,"On the other hand, by Lemma H.2, there is an event E′
h of probability at least 1 −δ/(2MH) under
which for all f ∈V, all i ∈[M], and δ′ as in Algorithm 5:"
NREG,0.6071184120465435,"∣̂Ph−1,ε,δ′[̂V
(i)
h ](x
(i)
h−1,a
(i)
h−1) −̂Ph−1,ε,δ′[fh](x
(i)
h−1,a
(i)
h−1)∣"
NREG,0.6078028747433265,"≤∣Ph−1[̂V
(i)
h
−fh](x
(i)
h−1,a
(i)
h−1)∣+ ε ⋅
√"
NREG,0.6084873374401095,"2log1/δ′(4MAH∣V∣(t
(i)
h )2/δ),"
NREG,0.6091718001368925,"= ∣Ph−1[̂V
(i)
h
−fh](x
(i)
h−1,a
(i)
h−1)∣+ ε ⋅β(t
(i)
h ),
(32)"
NREG,0.6098562628336756,"where β(t
(i)
h ) is as in Algorithm 5. Thus, under E′
h, the test in Line 14 fails for ℓ= h at least M times
only if"
NREG,0.6105407255304586,"∀i ∈[M],
ε < sup"
NREG,0.6112251882272416,"f∈̂V(i)
h
∣(̂Ph−1,ε,δ′[̂V
(i)
h ] −̂Ph−1,ε,δ′[fh])(x
(i)
h−1,a
(i)
h−1)∣"
NREG,0.6119096509240246,"−ε ⋅β(t
(i)
h ), ≤sup"
NREG,0.6125941136208076,"f∈̂V(i)
h
∣E[̂V
(i)
h (xh) −fh(xh) ∣xh−1 = x
(i)
h−1,ah−1 = a
(i)
h−1]∣
(by (32)), < sup"
NREG,0.6132785763175906,"f∈̂V(i)
h
∣E[̂V
(i)
h (xh) −fh(xh) ∣xh−1 = x
(i)
h−1,ah−1 = a
(i)
h−1]∣."
NREG,0.6139630390143738,"Unless N < M, this is a contradiction to Eq. (31). We conclude that under the event Eh ∩E′
h, the test
in Line 14 fails at most N < M = ⌈8ε−1CpushH⌉times for ℓ= h, and so under E1 ∩E′
1 ∩⋅⋅⋅∩EH ∩EH,
we have
∀h ∈[H],
∣Ch∣≤⌈8ε−1CpushH⌉.
By the union bound, we have P[E1 ∩E′
1 ∩⋅⋅⋅∩EH ∩E′
H] ≥1 −δ, which completes the proof."
NREG,0.6146475017111568,"I.3
Proof of Lemma I.2 (Consequence of Passing the Tests)
Proof of Lemma I.2. Let h ∈[H] be given. Fix ℓ∈[h + 1..H] and let x
(1)
ℓ−1,x
(2)
ℓ−1,... denote the
sequence of states used in the tests of Line 14 throughout the execution of RVFS0; we assume that the
sequence is ordered in the sense that if i < j, then x
(i)
ℓ−1 is used in the test of Line 14 before x
(j)
ℓ−1. Let
Tℓ∈N ∪{+∞} be the random variable representing the total number of times the operator ̂Pℓ−1,ε,δ′
is invoked in Line 14 throughout the execution of RVFS0 (Tℓis also the random length of the sequence
x
(1)
ℓ−1,x
(2)
ℓ−1,...; if RVFS0 terminates, then Tℓis finite. The first step of the proof will be to show that
under the event E of Lemma I.1, Tℓis no larger than M 3NtestH3 at any point during the execution
of RVFS0. This will help us establish key concentration results, leading to the desired inequality (26)."
NREG,0.6153319644079398,"Bounding Tℓunder E.
First, note that under the event E of Lemma I.1, we have that for any
τ ∈[H],"
NREG,0.6160164271047228,"∣Cτ∣≤M ∶= ⌈8ε−1CpushH⌉,
(33)
and so RVFSτ gets called at most M times throughout the execution of RVFS0. For the rest of this
paragraph, we condition on E and fix τ ∈[0..H]. Within any given call to RVFSτ (throughout the
execution of RVFS0), the operator ̂Pℓ−1,ε,δ′ is invoked at most"
NREG,0.6167008898015058,"∣Cτ∣NtestH
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
Due to the for-loops in Line 8, Line 9, & Line 10"
NREG,0.6173853524982889,"×
HM
±
Number of times RVFSτ returns to Line 8 (see below)
≤M 2NtestH2"
NREG,0.6180698151950719,"times. This is because the for-loop in Line 8 of RVFSτ resumes whenever a test in Line 14 fails for
one of the layers τ + 1,...,H (see Line 18) once the recursive calls return, and the total number of
test failures across all these layers is bounded by HM (by (33)). Now, since RVFSτ gets called at
most M times throughout the execution of RVFS0 (as argued in the prequel), the total number of times
the operator ̂Pℓ−1,ε,δ′ is invoked in Line 14 within RVFSτ is at most"
NREG,0.6187542778918549,M 3NtestH2.
NREG,0.6194387405886379,"Finally, the total number of times the operator ̂Pℓ−1,ε,δ′ is called in Line 14 throughout the execution
of RVFS0 is at most H times larger (accounting for the contributions from RVFSτ for all τ ∈[H]); that
is, it is at most M 3NtestH3. We conclude that the random variable Tℓsatisfies
Tℓ≤M 3NtestH3
(34)
under E."
NREG,0.6201232032854209,"Specifying E′
h.
In this paragraph, we no longer condition on E. We will specify the event E′
h in the
lemma statement. Let δ′ be defined as in Algorithm 5. By Lemma H.2, we have that there is an event
E′
h,ℓof probability at least 1 −δ/(2H2) under which:"
NREG,0.6208076659822039,"∀i ∈[Tℓ],∀aℓ−1 ∈A ∶sup
f∈̂Vℓ
∣(̂Pℓ−1,ε,δ′[̂Vℓ] −̂Pℓ−1,ε,δ′[fℓ])(x
(i)
ℓ−1,aℓ−1)∣−ε −ε ⋅β(Tℓ)"
NREG,0.621492128678987,"= sup
f∈̂Vℓ
∣(̂Pℓ−1,ε,δ′[̂Vℓ] −̂Pℓ−1,ε,δ′[fℓ])(x
(i)
ℓ−1,aℓ−1)∣−ε −ε ⋅
√"
NREG,0.62217659137577,"2log1/δ′(8AH2M∣V∣T 2
ℓ/δ),"
NREG,0.6228610540725531,"≥sup
f∈̂Vℓ
∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(x
(i)
ℓ−1,aℓ−1)∣−ε −ε ⋅
√"
NREG,0.6235455167693361,"2log1/δ′(8AH2M∣V∣T 2
ℓ/δ)"
NREG,0.6242299794661191,"−ε ⋅
√"
NREG,0.6249144421629022,"2log1/δ′(4AH2M∣V∣i2/δ),
(Lemma H.2)"
NREG,0.6255989048596852,"≥sup
f∈̂Vℓ
∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(x
(i)
ℓ−1,aℓ−1)∣−ε −2ε ⋅
√"
NREG,0.6262833675564682,"2log1/δ′(8AH2M∣V∣T 2
ℓ/δ)."
NREG,0.6269678302532512,"(35)
On the other hand, for k ∈[Tℓ−Ntest + 1], we have by Lemma C.4 (Freedman’s inequality)
instantiated with"
NREG,0.6276522929500342,"• n = Ntest and yi = I{supf∈̂Vℓmaxa∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1)[fℓ](x
(k+i)
ℓ−1 ,a)∣> 3ε}, for all
i ∈[Ntest];"
NREG,0.6283367556468172,• Q = {id};
NREG,0.6290212183436003,• B = 1; and
NREG,0.6297056810403833,• λ = 1;
NREG,0.6303901437371663,"that there is an event E′′
h,ℓ,k of probability at least 1 −δ/(4k2H2) under which"
NREG,0.6310746064339493,"∑
0≤i<Ntest
P
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(x
(k+i)
ℓ−1 ,a)∣> 3ε
⎤⎥⎥⎥⎥⎦"
NREG,0.6317590691307323,"≤4log(8H2T 2
ℓ/δ) +
∑
0≤i<Ntest
I
⎧⎪⎪⎨⎪⎪⎩
sup
f∈̂Vℓ,a∈A
∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(x
(k+i)
ℓ−1 ,a)∣> 3ϵ
⎫⎪⎪⎬⎪⎪⎭
."
NREG,0.6324435318275154,"Now, let E′′
h,ℓ∶= ⋂k∈[Tℓ−Ntest+1] E′′
h,ℓ,k. By the union bound and the fact that ∑k≥1 1/k2 = π2/6 ≤2,
we have that P[E′′
h,ℓ] ≥1 −δ/(2H2). Furthermore, under E′′
h,ℓ, we have"
NREG,0.6331279945242985,"∀k ∈[Tℓ−Ntest + 1],"
NREG,0.6338124572210815,"∑
0≤i<Ntest
P
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(x
(k+i)
ℓ−1 ,a)∣> 3ε
⎤⎥⎥⎥⎥⎦"
NREG,0.6344969199178645,"≤4log(8H2T 2
ℓ/δ) +
∑
0≤i<Ntest
I
⎧⎪⎪⎨⎪⎪⎩
sup
f∈̂Vℓ,a∈A
∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(x
(k+i)
ℓ−1 ,a)∣> 3ϵ
⎫⎪⎪⎬⎪⎪⎭
.
(36)"
NREG,0.6351813826146475,"We define E′
h ∶= E′
h,1 ∩E′′
h,1 ∩⋅⋅⋅∩E′
h,H ∩E′′
h,H. Note that by the union bound, we have P[E′
h] ≥1−δ"
NREG,0.6358658453114305,"H
as desired."
NREG,0.6365503080082136,"Termination of RVFSh under E ∩E′
h.
We now show that under E ∩E′
h, if RVFSh terminates, its
output satisfies (26). For the rest of the proof, we condition on E ∩E′
h. Suppose that RVFSh terminates
and returns (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H). In this case, the value function ̂Vℓmust have passed the
tests in Line 14 for all (xh−1,ah−1) ∈Ch, n ∈Ntest, and aℓ−1 ∈A. Fix (xh−1,ah−1) ∈Ch and let
k ∈[Tℓ−Ntest ⋅A + 1] be such that (xk+j
ℓ−1)j∈[0..Ntest−1] represents a subsequence of states that pass
the tests in Line 14 at layer ℓfor (xh−1,ah−1) within the call to RVFSh. The fact that the sequence
(x
(i)
ℓ−1)i≥1 is ordered (see definition in the first paragraph of this proof) and that (xk+j
ℓ−1)j∈[0..Ntest−1]
pass the tests imply that"
NREG,0.6372347707049966,"1. The states (x
(k+i)
ℓ−1 )i∈[0..Ntest−1] at layer ℓ−1 are i.i.d., and are obtained by rolling out with
̂πh∶H starting from (xh−1,ah−1); and"
NREG,0.6379192334017796,"2. The test in Line 14 succeeds for all (x
(k+j)
ℓ−1 )j∈[0..Ntest−1]; that is"
NREG,0.6386036960985626,"∀j ∈[0..Ntest −1],∀aℓ−1 ∈A,
sup
f∈̂Vℓ
∣(̂Pℓ−1,ε,δ′[̂Vℓ] −̂Pℓ−1,ε,δ′[fℓ])(x
(k+j)
ℓ−1 ,aℓ−1)∣"
NREG,0.6392881587953456,"≤ε + ε ⋅β(k + j),"
NREG,0.6399726214921286,"≤ε + ε ⋅
√"
NREG,0.6406570841889117,"2log1/δ′(8AM∣V∣(k + j)2/δ),"
NREG,0.6413415468856948,"≤ε + ε ⋅
√"
NREG,0.6420260095824778,"2log1/δ′(8AM∣V∣T 2
ℓ/δ)."
NREG,0.6427104722792608,This implies that
NREG,0.6433949349760438,"∀i ∈[0..Ntest −1],∀aℓ−1 ∈A ∶"
NREG,0.6440793976728268,"sup
f∈̂Vℓ
∣Pℓ−1[̂Vℓ−fℓ](x
(k+i)
ℓ−1 ,aℓ−1)∣−3ε"
NREG,0.6447638603696099,"≤sup
f∈̂Vℓ
∣Pℓ−1[̂Vℓ−fℓ](x
(k+i)
ℓ−1 ,aℓ−1)∣−ε −2ε ⋅
√"
NREG,0.6454483230663929,"2log1/δ′(4AMH2∣V∣T 2
ℓδ),
(37) (35)"
NREG,0.6461327857631759,"≤sup
f∈̂Vℓ
∣(̂Pℓ−1,ε,δ′[̂Vℓ] −̂Pℓ−1,ε,δ′[fℓ])(x
(k+i)
ℓ−1 ,aℓ−1)∣−ε −ε ⋅
√"
NREG,0.6468172484599589,"2log1/δ′(4AMH2∣V∣T 2
ℓ/δ),"
NREG,0.6475017111567419,"≤0.
(by Item 2)
(38)"
NREG,0.648186173853525,where (37) follows by (34) and the choice of δ′ in Algorithm 5.
NREG,0.648870636550308,"Now, by Item 1, we have that x
(k+i)
ℓ−1 has probability law P̂πh∶H[⋅∣xh−1 = xh−1,ah−1 = ah−1] for all
i ∈[0..Ntest −1], and so by (36), we have:"
NREG,0.649555099247091,"P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(xℓ−1,a)∣> 3ε ∣xh−1 = xh−1,ah−1 = ah−1
⎤⎥⎥⎥⎥⎦"
NREG,0.6502395619438741,"≤4log(8H2T 2
ℓ/δ)
Ntest
+
1
Ntest
∑
0≤i<Ntest
I
⎧⎪⎪⎨⎪⎪⎩
sup
f∈̂Vℓ,a∈A
∣Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(x
(k+i)
ℓ−1 ,a)∣> 3ϵ
⎫⎪⎪⎬⎪⎪⎭
,"
NREG,0.6509240246406571,"≤4log(8M 6N 2
testH8/δ)
Ntest
(using (34) and the fact that all the tests pass, i.e. (38))."
NREG,0.6516084873374401,"Concluding.
We have established that under E ∩E′
h, we have for all ℓ∈[h + 1..H] and all
(xh−1,ah−1) ∈Ch:"
NREG,0.6522929500342232,"P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(xℓ−1,a)∣> 3ε ∣xh−1 = xh−1,ah−1 = ah−1
⎤⎥⎥⎥⎥⎦
≤4log(8M 6N 2
testH8/δ)
Ntest
,"
NREG,0.6529774127310062,"Furthermore, we have P[E′
h] ≥1 −δ/H. This completes the proof."
NREG,0.6536618754277892,"I.4
Proof of Lemma I.3 (Value Function Regression Guarantee)"
NREG,0.6543463381245722,"Proof of Lemma I.3. Fix π ∈Π′ ⊆ΠS and k ≥1, and consider the kth call to RVFSh as per the lemma
statement, and let Sk be the state of RVFS0 during the kth call to RVFSh and immediately before Line
20, i.e. immediately before gathering data for the regression step in RVFSh."
NREG,0.6550308008213552,"Relating the regression targets to V ̂π
h .
Observe that ̂Vh is the least-squares solution of the objective
in Line 26, where the targets are empirical estimates of V ̂π
h . In particular, if we let {Dh(x,a) ∶
(x,a) ∈Ch} be the datasets in the definition of ̂Vh in (15), then for any (xh−1,ah−1) ∈Ch and
(xh,vh) ∈Dh(xh−1,ah−1), the target vh satisfies"
NREG,0.6557152635181382,"vh = ̂Vh(xh),"
NREG,0.6563997262149213,"where ̂Vh(xh) is an empirical estimate of V ̂π
h (xh) obtained by sampling Nest(∣Ch∣) = Nest(k)
episodes (for Nest(⋅) defined as in Algorithm 5) by rolling out with ̂π after starting from xh and
playing action a at layer h; note that ∣Ch∣= k because we are considering the kth call to RVFSh. Thus,
by Hoeffding’s inequality and the union bound over (xh−1,ah−1) ∈Ch and (xh,−) ∈Dh(xh−1,ah−1),
there is an event E′′
h,k(Sk) of probability at least 1 −δ/(8k2H) under which"
NREG,0.6570841889117043,"∀(xh−1,ah−1) ∈Ch,∀(xh,−) ∈Dh(xh−1,ah−1) ∶"
NREG,0.6577686516084873,"∣V ̂π
h (xh) −̂Vh(xh)∣≤H"
NREG,0.6584531143052703,"¿
Á
Á
À2log(8∣Ch∣NregHk2/δ)"
NREG,0.6591375770020534,"Nest(k)
≤H"
NREG,0.6598220396988365,"¿
Á
Á
À2log(8NregHk3/δ)"
NREG,0.6605065023956195,"Nest(k)
,
(39)"
NREG,0.6611909650924025,"where Nreg is as in Line 5, and the last inequality follows by ∣Ch∣≤k since we are considering the
kth call to RVFSh. Thus, under E′′
h,k, we have"
NREG,0.6618754277891855,"∀(xh−1,ah−1) ∈Ch,∀(xh,vh) ∈Dh(xh−1,ah−1) ∶"
NREG,0.6625598904859685,"∣V ̂π
h (xh) −vh∣= ∣V ̂π
h (xh) −̂Vh(xh)∣≤H"
NREG,0.6632443531827515,"¿
Á
Á
À2log(8NregHk3/δ)"
NREG,0.6639288158795346,"Nest(k)
≤
H
Nreg
,
(40)"
NREG,0.6646132785763176,"where the second-to-last inequality is by (39) and the last inequality follows by the choice of Nest in
Algorithm 5."
NREG,0.6652977412731006,"Bounding the discrepancy V ̂π
h −V π
h .
On the other hand, by the performance difference lemma,
the value function V ̂π
h satisfies:"
NREG,0.6659822039698836,"∀x ∈X,
∣V ̂π
h (x) −V π
h (x)∣≤
H
∑
τ=h
Êπ[∣Qπ
τ (xτ,πτ(xτ)) −Qπ
τ (xτ, ̂πτ(xτ))∣∣xh = x],"
NREG,0.6666666666666666,"≤H
H
∑
τ=h
Êπ[Dtv(̂πτ(xτ),πτ(xτ)) ∣xh = x].
(41)"
NREG,0.6673511293634496,"Now, let (x
(1)
h−1,a
(1)
h−1),(x
(2)
h−1,a
(2)
h−1),... denote the elements of Ch in the order in which they are
added to the latter in Line 15. By Lemma C.2 (Freedman’s inequality) instantiated with"
NREG,0.6680355920602327,• n = Nreg ⋅k.
NREG,0.6687200547570158,"• wi = ∣V π
h (x
(i)
h ) −V ̂π
h (x
(i)
h )∣−E[∣V π
h (xh) −V ̂π
h (xh)∣∣xh−1 = x
(j)
h−1,ah−1 = a
(j)
h−1], for"
NREG,0.6694045174537988,"all i ∈[n] and j = ⌊i/Nreg⌋+ 1, where x
(Nreg⋅j)
h
,...,x
(Nreg⋅j+Nreg−1)
h
i.i.d.
∼
Th(⋅∣xh−1 =
x
(j)
h−1,ah−1 = a
(j)
h−1);"
NREG,0.6700889801505818,"• Hi = σ(x
(1)
h ,...x
(i−1)
h
), for all i ∈[n];"
NREG,0.6707734428473648,• R = H; and
NREG,0.6714579055441479,• λ = 1/H;
NREG,0.6721423682409309,"we get that there is an event ̃E′′
h,k,π(Sk) of probability at least 1 −δ/(8k2H∣Π′∣) under which:"
NREG,0.6728268309377139,"∑
(xh−1,ah−1)∈Dh
∑
(xh,−)∈Dh(xh−1,ah−1)
∣V π
h (xh) −V ̂π
h (xh)∣"
NREG,0.6735112936344969,"= 2Nreg
∑
(xh−1,ah−1)∈Dh
E[∣V π
h (xh) −V ̂π
h (xh)∣∣xh−1 = xh−1,ah−1 = ah−1] + H log(8k2∣Π′∣H/δ),"
NREG,0.6741957563312799,"≤2HNreg
∑
(xh−1,ah−1)∈Dh"
NREG,0.6748802190280629,"H
∑
τ=h
Êπ [Dtv(̂πτ(xτ),πτ(xτ)) ∣xh−1 = xh−1,ah−1 = ah−1] + H log(8k2∣Π′∣H/δ), (42)"
NREG,0.675564681724846,where the last inequality follows by (41) and the law of total expectation.
NREG,0.676249144421629,"Regression guarantee.
Since π ∈Π′ ⊆ΠS and Assumption I.1 holds, Lemma C.5 (regression
guarantee) instantiated with"
NREG,0.676933607118412,"• f⋆(x) = V π
h (x);"
NREG,0.6776180698151951,• B = H;
NREG,0.6783025325119781,"• bi = vh −V π
h (xh) (where vh ∶= maxa∈A ̂Qh(xh,a)); and"
NREG,0.6789869952087612,• ξ = H;
NREG,0.6796714579055442,"implies that there is an event ˘E′′
h,k,π(Sk) of probability at least 1 −δ/(4k2H∣Π′∣) under which we
have:"
NREG,0.6803559206023272,"∑
(xh−1,ah−1)∈Ch"
NREG,0.6810403832991102,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V π
h (xh))
2"
NREG,0.6817248459958932,≤4kH2 log(4k2H∣Π′∣∣V∣/δ)
NREG,0.6824093086926762,"Nreg
+ 4H"
NREG,0.6830937713894593,"Nreg
∑
(xh−1,ah−1)∈Ch
∑
(xh,vh)∈Dh(xh−1,ah−1)
∣V π
h (xh) −vh∣,"
NREG,0.6837782340862423,≤4kH2 log(4k2H∣Π′∣∣V∣/δ)
NREG,0.6844626967830253,"Nreg
+ 4H"
NREG,0.6851471594798083,"Nreg
∑
(xh−1,ah−1)∈Ch
∑
(xh,vh)∈Dh(xh−1,ah−1)
∣V ̂π
h (xh) −vh∣ + 4H"
NREG,0.6858316221765913,"Nreg
∑
(xh−1,ah−1)∈Ch
∑
(xh,vh)∈Dh(xh−1,ah−1)
∣V π
h (xh) −V ̂π
h (xh)∣,
(43)"
NREG,0.6865160848733745,"where the last step follows by the triangle inequality. Thus, by plugging (42) and (40) into (43), we
get that under E′′
h,k(Sk) ∩̃E′′
h,k,π(Sk) ∩˘E′′
h,k,π(Sk):"
NREG,0.6872005475701575,"∑
(xh−1,ah−1)∈Ch"
NREG,0.6878850102669405,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V π
h (xh))
2"
NREG,0.6885694729637235,≤9kH2 log(8k2H∣Π′∣∣V∣/δ)
NREG,0.6892539356605065,"Nreg
+ 8H2
∑
(xh−1,ah−1)∈Ch"
NREG,0.6899383983572895,"H
∑
τ=h
Êπ [Dtv(̂πτ(xτ),πτ(xτ)) ∣xh−1 = xh−1,ah−1 = ah−1]. (44)"
NREG,0.6906228610540726,"Applying the union bound to conclude.
Let Sk be the random state of RVFS0 during the kth call to
RVFSh and immediately before Line 20, i.e. immediately before gathering data for the regression step
in RVFSh. Further, let S+
k be the random state of RVFS0 during the kth call to RVFSh and immediately
before Line 26, i.e. immediately before the regression step in RVFSh. If RVFS0 terminates before the
kth call to RVFSh, we use the convention that Sk = S+
k = t, where t denotes a terminal state, and
define E′′
h,k(t) = ̃E′′
h,k,π(t) = ˘E′′
h,k,π(t) = {t}. Further, we define"
NREG,0.6913073237508556,"E′′
h ∶=
⎧⎪⎪⎨⎪⎪⎩
∏
k∈N,π∈Π′ I{S+
k ∈E′′
h,k(Sk) ∩̃E′′
h,k,π(Sk) ∩˘E′′
h,k,π(Sk)} = 1
⎫⎪⎪⎬⎪⎪⎭
."
NREG,0.6919917864476386,"Note that by the argument in the sequel and the union bound, we have that"
NREG,0.6926762491444216,"∀k ≥1,∀Sk,
P[S+
k ∈E′′
h,k(Sk) ∩̃E′′
h,k,π(Sk) ∩˘E′′
h,k,π(Sk)] ≥1 −
δ
2k2H ,
(45)"
NREG,0.6933607118412046,"where Sk denotes the state of RVFS0 during the kth call to RVFSh and immediately before Line 20. By
letting S′
1,S′
2,... denote an identical, independent copy of the sequence S1,S2,..., we have by
the chain rule:"
NREG,0.6940451745379876,"P[E′′
h] = ES′
1,S′
2,... [∏
k≥1
P[S+
k ∈E′′
h,k(Sk) ∩̃E′′
h,k,π(Sk) ∩˘E′′
h,k,π(Sk) ∣Sk = S′
k]],"
NREG,0.6947296372347707,"≥∏
k≥1
(1 −
δ
2k2H ),
(by (45)) ≥1 −δ"
NREG,0.6954140999315537,"H ,
(46)"
NREG,0.6960985626283368,"where the last inequality follows from the fact that for any sequence x1,x2,⋅⋅⋅∈(0,1), we have
∏k≥1(1 −xk) ≥1 −∑k≥1 xk. Combining (46) with (44) implies that E′′
h gives the desired result."
NREG,0.6967830253251198,"I.5
Proof of Lemma I.4 (Guarantee for Confidence Sets)
To prove Lemma I.4, we need one additional result pertaining to the order in which the instances
(RVFSh)h∈[H] are called."
NREG,0.6974674880219028,"Lemma I.5. Let h ∈[0..H] be given, and consider the setting of Lemma I.4. Further, consider a
call to RVFS0(f,V,∅,∅;V,ε,δ) that terminates, and let h ∈[H] be any layer such that RVFSh is
called during the execution of RVFS0. Then, after the last call to RVFSh terminates, no instance of
RVFS in (RVFSτ)τ>h is called before RVFS0 terminates."
NREG,0.6981519507186859,"Proof of Lemma I.5. Suppose there is an instance of RVFS in (RVFSτ)τ>h that is called after the last
call to RVFSh terminates. Let τ > h be the lowest layer where RVFSτ is called after the last call to
RVFSh terminates. Let RVFSlast
τ
denote the corresponding instance of RVFSτ. Further, let ℓ< τ be
such that RVFSℓis the parent instance of RVFSlast
τ
(i.e. the instance that called RVFSlast
τ
). Note that
we cannot have ℓ= h as this would imply that an instance of RVFSh terminates after RVFSlast
τ
, and we
have assumed that RVFSlast
τ
terminates after the last call RVFSh. It is also not possible to have ℓ> h
as this would imply that τ is not the lowest layer where RVFSτ is called after the last call to RVFSh
terminates. Thus, we must have that ℓ< h. Now, the for-loop in Line 16 ensures that that there is an
instance of RVFSh that is called after RVFSlast
τ
terminates and before RVFSℓdoes. This contradicts the
assumption that RVFSlast
τ
is called after the last call to RVFSh."
NREG,0.6988364134154689,"Proof of Lemma I.4. We start by showing that ˜π ∈Π4ε by constructing the corresponding collection
of random state-action value functions ( ̃
Qh(x,a))(h,x,a)∈[H]×X×A ⊂[0,H] in the definition of Π4ε.
In particular, for (h,x,a) ∈[H] × X × A, we define"
NREG,0.6995208761122519,"̃
Qh(x,a) = {
̂
Qh(x,a),
if ∥̂
Qh(x,⋅) −Ph[V ˜π
h+1](x,⋅)∥∞≤4ε,
Ph[V ˜π
h+1](x,a),
otherwise,
for h = H,...,1."
NREG,0.7002053388090349,"where ̂
Qτ(x,a) ∶= ̂Pτ,ε,δ′[̂Vτ+1](x,a). Note that ̃
Qh(x,a) only depends on the randomness of
̂Ph,ε,δ′[̂Vh+1](x,a), and so ( ̃
Qh(x,a))(h,x,a)∈[H]×X×A are independent random variables. Further-
more, since Ph[V ˜π
h+1] ≡Q˜π
h, we have that"
NREG,0.7008898015058179,"∀(x,a) ∈X × A,
∥̃
Qh(x,a) −Q˜π
h(x,a)∥≤4ε."
NREG,0.7015742642026009,"Finally, since ˜πh(⋅) ∈arg maxa∈A ̃
Qh(⋅,a), we have that ˜π ∈Π4ε."
NREG,0.702258726899384,"We show V ˜π
h ∈̂Vh.
We prove that V ˜π
h ∈̂Vh, for all h ∈[H], under the event E′′′ ∶= E ∩E′
1∩E′′
1 ∩⋅⋅⋅∩
E′
H ∩E′′
H, where E, (E′
h), and (E′′
h) are the events defined in Lemma I.1, Lemma I.2, and Lemma I.3,
respectively. Throughout, we condition on E′′′. First, note that by Lemma I.1, RVFS0 terminates. Let
(̂V1∶H, ̂V1∶H,C1∶H,B1∶H,t1∶H) be the tuple it returns."
NREG,0.702943189596167,"We will show via backwards induction over ℓ= H + 1,...,1, that"
NREG,0.70362765229295,"V ˜π
ℓ∈̂Vℓ,
(47)"
NREG,0.704312114989733,where ˜π1∶H is the stochastic policy defined recursively via
NREG,0.7049965776865161,"˜πτ(x) ∈arg max
a∈A
{
̂
Qτ(x,a) ∶= ̂Pτ,ε,δ′[̂Vτ+1](x,a),
if ∥̂
Qτ(x,⋅) −Pτ[V ˜π
τ+1](x,⋅)∥∞≤4ε,
Pτ[V ˜π
τ+1](x,a),
otherwise,
for τ = H,...,1,"
NREG,0.7056810403832992,"where ̂
Qτ(x,a) ∶= ̂Pτ,ε,δ′[̂Vτ+1](x,a)."
NREG,0.7063655030800822,"Base case [ℓ= H + 1].
This holds trivially because V π
H+1 ≡0 for any π ∈ΠS by convention."
NREG,0.7070499657768652,"General case [ℓ≤H].
Fix h ∈[H] and suppose that (47) holds for all ℓ∈[h + 1..H + 1]. We
show as a consequence that (47) holds for ℓ= h. First, note that if RVFSh is never called during the
execution of RVFS0, then ̂Vh = V, and so (47) trivially holds for ℓ= h under Assumption I.1 with
εreal = 4ε."
NREG,0.7077344284736482,"Now, suppose that RVFSh is called at least once, and let (̂V +
h∶H, ̂V+
h∶H,C+
h∶H,B+
h∶H,t+
h∶H) be the output
of the last call to RVFSh during the execution of RVFS0. We claim that"
NREG,0.7084188911704312,"(̂V +
h∶H, ̂V+
h∶H,C+
h∶H) = (̂Vh∶H, ̂Vh∶H,Ch∶H).
(48)"
NREG,0.7091033538672142,"To see this, first note that the for-loop in Line 16 ensures that no instance of (RVFSτ)τ>h can be called
after the last call to RVFSh (by Lemma I.5). Thus, the estimated value functions, confidence sets, and
core sets for layers h + 1,...,H remain unchanged after the last call to RVFSh; that is, (48) holds."
NREG,0.7097878165639973,"Thus, by Lemma I.2, and since we are conditioning on E′
h+1∶H, we have that for all (xh−1,ah−1) ∈Ch
and ℓ∈[h + 1..H + 1]:"
NREG,0.7104722792607803,"P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(xℓ−1,a)∣> 3ε ∣xh−1 = xh−1,ah−1 = ah−1
⎤⎥⎥⎥⎥⎦
≤4log(8M 6N 2
testH8/δ)
Ntest
, (49)"
NREG,0.7111567419575633,"where M = ⌈8ε−1CpushH⌉. Now, by the induction hypothesis, we have V ˜π
ℓ∈̂Vℓ, and so substituting
V ˜π
ℓfor fℓin (49), we get that for all (xh−1,ah−1) ∈Ch and ℓ∈[h + 1..H + 1]:"
NREG,0.7118412046543463,"P̂π[max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[V ˜π
ℓ])(xℓ−1,a)∣> 3ε ∣xh−1 = xh−1,ah−1 = ah−1] ≤4log(8M 6N 2
testH8/δ)
Ntest
."
NREG,0.7125256673511293,"Therefore, by Lemma L.1 (instantiated with µ[⋅] = P̂π[⋅∣xh−1 = xh−1,ah−1 = ah−1], τ = ℓ−1, and
Vτ+1 = V ˜π
ℓ), we have that (xh−1,ah−1) ∈Ch and ℓ∈[h + 1..H + 1]:"
NREG,0.7132101300479123,"Êπ[Dtv(̂πℓ−1(xℓ−1), ˜πℓ−1(xℓ−1)) ∣xh−1 = xh−1,ah−1 = ah−1] ≤4log(8M 6N 2
testH8/δ)
Ntest
+ δ′, (50)"
NREG,0.7138945927446955,where δ′ is as in Algorithm 5.
NREG,0.7145790554414785,"Applying the regression guarantee to conclude the induction.
Note that ˜π ∈Π′, where Π′ ⊂ΠS
is the set of stochastic policies such that π ∈Π′ if and only if there exists V1∶H ∈V such that π is
defined recursively as"
NREG,0.7152635181382615,"πτ(x) ∈arg max
a∈A
{ Qτ(x,a) ∶= ̂Pτ,ε,δ′[Vτ+1](x,a),
if ∥Qτ(x,⋅) −Pτ[V π
τ+1](x,⋅)∥∞≤4ε,
Pτ[V π
τ+1](x,a),
otherwise,"
NREG,0.7159479808350445,"for τ = H,...,1, where Qτ(x,a) ∶= ̂Pτ,ε,δ′[Vτ+1](x,a). The policy class Π′ is finite and ∣Π′∣≤∣V∣.
Furthermore, we have Π′ ⊆Π4ε as shown at the beginning of this proof. Therefore, if we let
{Dh(x,a) ∶(x,a) ∈Ch} be the datasets in the definition of ̂Vh in (15), we have by Lemma I.3 (under
Assumption I.1 with εreal = 4ε) and the conditioning on E′′
h+1∶H and E:"
NREG,0.7166324435318275,"∑
(xh−1,ah−1)∈Ch"
NREG,0.7173169062286106,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V ˜π
h (xh))
2"
NREG,0.7180013689253936,≤9∣Ch∣H2 log(8∣Ch∣2H∣V∣2/δ)
NREG,0.7186858316221766,"Nreg
+ 8H2
∑
(xh−1,ah−1)∈Ch"
NREG,0.7193702943189596,"H
∑
τ=h
Êπ [Dtv(̂πτ(xτ), ˜πτ(xτ)) ∣xh−1 = xh−1,ah−1ah−1],"
NREG,0.7200547570157426,≤9MH2 log(8M 2H∣V∣2/δ)
NREG,0.7207392197125256,"Nreg
+ 8H2
∑
(xh−1,ah−1)∈Ch"
NREG,0.7214236824093087,"H
∑
τ=h
Êπ [Dtv(̂πτ(xτ), ˜πτ(xτ)) ∣xh−1 = xh−1,ah−1 = ah−1], (51)"
NREG,0.7221081451060917,"where the last inequality follows by the fact that ∣Ch∣≤M under E. Combining (51) with (50), implies
that"
NREG,0.7227926078028748,"∑
(xh−1,ah−1)∈Ch"
NREG,0.7234770704996578,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V ˜π
h (xh))
2"
NREG,0.7241615331964408,≤9MH2 log(8M 2H∣V∣2/δ)
NREG,0.7248459958932238,"Nreg
+ 8MH3 ⋅4log(8M 6N 2
testH8/δ)
Ntest
+ 8MH3δ′,"
NREG,0.7255304585900069,= 9MH2 log(8M 2H∣V∣2/δ)
NREG,0.7262149212867899,"Nreg
+ 8MH3 ⋅4log(8M 6N 2
testH8/δ)
Ntest
+ 8MH3
δ
4M 7N 2
testH8∣V∣,"
NREG,0.7268993839835729,"≤ε2
reg,
(52)"
NREG,0.7275838466803559,"where the last inequality follows by the fact that δ ∈(0,1) and the definition of ε2
reg in Algo-
rithm 5. By the definition of ̂Vh in (15), (52) implies that V ˜π
h ∈̂Vh, which completes the induction."
NREG,0.7282683093771389,"I.6
Proof of Theorem I.1 (Main Guarantee of RVFS)"
NREG,0.728952772073922,"Proof of Theorem I.1. We condition on the event ̃E ∶= E ∩E′′′ ∩E′
1 ∩⋅⋅⋅∩E′
H, where E, E′′′, and
(E′
h) are the events in Lemma I.1, Lemma I.4, and Lemma I.2, respectively. Note that by the union
bound, we have P[̃E] ≥1 −5δ. By Lemma I.2, we have that"
NREG,0.729637234770705,"∀h ∈[H],
P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vh
max
a∈A ∣(Ph−1[̂Vh] −Ph−1[fh])(xh−1,a)∣> 3ε
⎤⎥⎥⎥⎥⎦
≤4log(8M 6N 2
testH8/δ)
Ntest
, (53)"
NREG,0.730321697467488,"where M = ⌈8ε−1CpushH⌉and Ntest = 28M 2Hε−1 log(8M 6H8ε−2δ−1). On the other hand, by
Lemma I.4, we have"
NREG,0.731006160164271,"∀h ∈[H],
V ˜π
h ∈̂Vh."
NREG,0.731690622861054,"Thus, substituting V ˜π
h for fh in (53) we get that for all h ∈[H + 1]."
NREG,0.7323750855578371,"P̂π[max
a∈A ∣(Ph−1[̂Vh] −Ph−1[V ˜π
h ])(xh−1,a)∣> 3ε] ≤4log(8M 6N 2
testH8/δ)
Ntest
."
NREG,0.7330595482546202,"This together with Lemma L.1, instantiated with µ[⋅] = P̂π[⋅]; τ = h −1; Vτ+1 = V ˜π
h ; and δ = δ′ (with
δ′ as in Algorithm 5), translates to:"
NREG,0.7337440109514032,"∀h ∈[H],
Êπ[Dtv(̂πh(xh), ˜πh(xh))] ≤4log(8M 6N 2
testH8/δ)
Ntest
+ δ′,"
NREG,0.7344284736481862,"= 4log(8M 6N 2
testH8/δ)
Ntest
+
δ
4M 7N 2
testH8∣V∣,"
NREG,0.7351129363449692,"≤
ε
4H3Cpush
,"
NREG,0.7357973990417522,"where the last step follows from the fact that Ntest = 28M 2Hε−1 log(8M 6H8ε−2δ−1) (with M as
in Line 3)."
NREG,0.7364818617385352,"Bounding the sample complexity.
We now bound the number of episodes used by Algorithm 5
under the event ̃E. First, we fix h ∈[H], and focus on the number of episodes used within a
to call RVFSh, excluding any episodes used by any subsequent calls to RVFSτ for τ > h. We
start by counting the number of episodes used to test the fit of the estimated value functions
̂Vh+1∶H.
Starting from Line 8, there are for-loops over (xh−1,ah−1) ∈Ch, ℓ= H,...,h + 1,
and n ∈[Ntest] to collect partial episodes using the learned policy ̂π in Algorithm 5, where
Ntest = 28M 2Hε−1 log(8M 6H8ε−2δ−1) and M = ⌈8ε−1CpushH⌉.
Note that executing ̂π re-
quires the local simulator and uses Nsim = 2log(4M 7N 2
testH2∣V∣/δ)/ε2 local simulator queries
to output an action at each layer (since Algorithm 5 calls Algorithm 7 with confidence level
δ′ = δ/(8M 7N 2
testH8∣V∣)). Also, note that whenever a test fails in Line 14 and the recursive
RVFS calls return, the for-loop in Line 8 resumes. We also know (by Lemma I.1) that the number of
times the test fails in Line 14 is at most M. Thus, the number of times the for-loop in Line 8 resumes
is bounded by HM; here, H accounts for possible test failures across all layers τ ∈[h + 1..H].
Thus, the total sample complexity required to generate episodes between lines Line 8 and Line 11 is
bounded by"
NREG,0.7371663244353183,"# episodes for roll-outs ≤
MH
±
# of times Line 8 resumes
⋅
MH2NtestNsim
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
Sample complexity in case of no test failures"
NREG,0.7378507871321013,".
(54)"
NREG,0.7385352498288843,"Note that the test in Line 14 also uses episodes because it calls the operator ̂P for every a ∈A. Thus,
the number of episodes used for the test in Line 14 is bounded by"
NREG,0.7392197125256673,"# episodes for the tests ≤
MH
±
# of times Line 8 resumes
⋅
MHANtestNsim
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
Sample complexity for Line 14"
NREG,0.7399041752224503,".
(55)"
NREG,0.7405886379192334,"We now count the number of episodes used to re-fit the value function in Line 16 and onwards. Note
that starting from Line 16, there are for-loops over (xh−1,ah−1) ∈Ch and i ∈[Nreg] to generate
A ⋅Nest(∣Ch∣) ≤A ⋅Nest(M) partial episodes using ̂π, where Nest(k) = 2N 2
reg log(8ANregHk3/δ)"
NREG,0.7412731006160165,"is defined as in Algorithm 5. Since ̂π uses the local simulator and requires Nest samples (see
Algorithm 7) to output an action at each layer, the number of episodes used to refit the value function
is bounded by
# episodes for V -refitting ≤MNregANest(M)HNsim.
(56)
Therefore, by (54), (55), and (56), the number of episodes used within a single call to RVFSh (not
accounting for episodes used by recursive calls to RVFSτ, for τ > h) is bounded by"
NREG,0.7419575633127995,"# episodes used locally within RVFSh ≤M 2H(H + A)NtestNsim + MNregANest(M)HNsim.
(57)
Finally, by Lemma I.1, RVFSh may be called at most M times throughout the execution of RVFS0.
Using this together with (57) and accounting for the number of episodes from all layers h ∈[H], we
get that the total number of episodes is bounded by"
NREG,0.7426420260095825,"M 3H2(H + A)NtestNsim + M 2H2NregANest(M)Nsim.
Substituting the expressions of M, Ntest, Nest, Nsim, and Nreg from Algorithm 5 and Algorithm 7,
we obtain the desired number of episodes, which concludes the proof."
NREG,0.7433264887063655,"I.7
Proof of Theorem I.2 (Guarantee of RVFS.bc)"
NREG,0.7440109514031485,"Proof of Theorem I.2.
Let ̂V1∶H be the value function estimates produced by RVFS0 within Al-
gorithm 6, and let ̂πRVFS
h
(⋅) ∈arg maxa∈A ̂Ph,εRVFS,δ′[̂Vh+1](⋅,a), for all h ∈[H] with ̂VH+1 ≡0
with εRVFS and δ′ as in Algorithm 6. Further, let and let ˜π1∶H ∈ΠS be the stochastic policy defined
recursively via"
NREG,0.7446954140999316,"∀x ∈X, ˜πτ(x) ∈arg max
a∈A
{
̂
Qτ(x,a),
if ∥̂
Qτ(x,⋅) −Pτ[V ˜π
τ+1](x,⋅)∥∞≤4εRVFS,
Pτ[V ˜π
τ+1](x,a),
otherwise,
(58)"
NREG,0.7453798767967146,"for τ = H,...,1, where ̂
Qτ(x,a) ∶= ̂Pτ,εRVFS,δ′[̂Vτ+1](x,a). By Theorem I.1, there is an event ̃E of
probability at least 1 −δ/2 under which:
˜π ∈Π4εRVFS,
(59)
and"
NREG,0.7460643394934976,"∀h ∈[H],
ÊπRVFS[Dtv(̂πRVFS
h
(xh), ˜πh(xh))] ≤
εRVFS
4H3Cpush
≤
ε
4H2 ,
(60)"
NREG,0.7467488021902806,where the last inequality follows by the choice of εRVFS in Algorithm 6.
NREG,0.7474332648870636,"For the rest of the proof, we condition on ̃E. By (59), (63), and Proposition M.1 instantiated with
εmis = 0 (due to all π-realizability), we have that there is an event ̃E′ of probability at least 1 −δ/2
under which the policy ̂π1∶H produced by BehaviorCloning ensures that"
NREG,0.7481177275838466,"J(̂πRVFS
1∶H ) −J(̂π1∶H) ≤ε"
NREG,0.7488021902806297,"2.
(61)"
NREG,0.7494866529774127,"Now, by Lemma C.6 (the performance difference lemma), we have for ˜π as in (58):"
NREG,0.7501711156741958,"J(˜π) −J(̂πRVFS
1∶H ) =
H
∑
h=1
ÊπRVFS[Q˜π
h(xh, ˜πh(xh)) −Q˜π
h(xh, ̂πRVFS
h
(xh))],"
NREG,0.7508555783709788,"≤H
H
∑
h=1
ÊπRVFS[Dtv(̂πRVFS
h
(xh), ˜πh(xh))],"
NREG,0.7515400410677618,"and so by (63), we have
J(˜π) −J(̂πRVFS
1∶H ) ≤ε/4.
(62)
Finally, since ˜π ∈Π4εRVFS (see (59)), we have by Lemma H.1,
J(π⋆) −J(˜π) ≤12HεRVFS ≤ε/4,
where the last inequality follows by the choice εRVFS in Algorithm 6. Combining this with (61) and
(62), we conclude that under ̃E ∩̃E′:
J(π⋆) −J(̂π1∶H) ≤ε."
NREG,0.7522245037645449,"By the union bound, we have P[̃E ∩̃E′] ≥1 −δ, and so the desired suboptimality guarantee in (28)
holds with probability at least 1 −δ."
NREG,0.7529089664613279,"Bounding the sample complexity.
The sample complexity is dominated by the call to RVFS0 within
RVFS.bc (Algorithm 6). Since RVFS.bc calls RVFS0 with ε = εRVFS = εH−1/48, we conclude from
Theorem I.1 that the total sample complexity is bounded by"
NREG,0.7535934291581109,"̃O (C8
pushH23A ⋅ε−13)."
NREG,0.7542778918548939,"J
Guarantee under V ⋆-Realizability (Proof of Theorem 4.1, Setup I)"
NREG,0.7549623545516769,"In this section, we prove Theorem 4.1 under Setup I (V ⋆/π⋆-realizability (Assumptions 4.2 and 4.3)
and ∆-gap (Assumption 4.4)). We prove this result as a consequence of the more general results
(Theorem I.2) in Appendix I by appealing to the relaxed V π-realizability condition in Assumption I.1."
NREG,0.75564681724846,"J.1
Analysis: Proof of Theorem 4.1 (Setup I)
We begin by showing that Assumption 4.2 and Assumption 4.4 together imply that Assumption I.1
holds for any εreal ≤∆/2; we prove this by showing that the benchmark policy class Πε′ (Ap-
pendix H.2) reduces to {π⋆} when ε′ ≤∆/2."
NREG,0.756331279945243,"Lemma J.1. Assume that V satisfies Assumption 4.2 (V ⋆-realizability), and that Assumption 4.4
(gap) holds with ∆> 0. Then, for all ε′ ≤∆/2, we have Πε′ = {π⋆} and V satisfies Assumption I.1"
NREG,0.757015742642026,with εreal = ε′.
NREG,0.757700205338809,"Informally Lemma J.1, whose proof is in Appendix J.2, states that under Assumption 4.2, Assump-
tion 4.4 with ∆> 0, and Assumption 4.1 (pushforward coverability) with Cpush > 0, we are essentially
in the setting of Theorem I.1 (guarantee of RVFS under relaxed V π-realizability), as long as we choose
εreal ≤∆/2. With this, we now state and prove a central guarantee for RVFS under V ⋆-realizability
with a gap."
NREG,0.758384668035592,"Lemma J.2 (Intermediate guarantee for RVFS under Setup I). Let δ ∈(0,1) be given, and suppose
that:"
NREG,0.759069130732375,• Assumption 4.1 (pushforward coverability) holds with parameter Cpush > 0;
NREG,0.7597535934291582,• Assumption 4.4 (gap) holds with parameter ∆> 0;
NREG,0.7604380561259412,• The function class V satisfies Assumption 4.2 (V ⋆-realizability).
NREG,0.7611225188227242,"Then, for any f ∈V and ε ∈(0,∆/8), with probability at least 1 −δ, RVFS0(f,V,∅,∅;V,ε,δ)
(Algorithm 5) terminates and returns value functions ̂V1∶H that satisfy"
NREG,0.7618069815195072,"∀h ∈[H],
P̂π[̂πh(xh) ≠π⋆
h(xh)] ≤
ε
4CpushH3 ,"
NREG,0.7624914442162902,"where ̂πh(x) ∈arg maxa∈A ̂Pτ,ε,δ′[̂Vh+1](x,a), for all h ∈[H], with δ′ is defined as in Algorithm 5."
NREG,0.7631759069130732,"Proof of Lemma J.2. From Lemma J.1, we have that Π4ε = {π⋆}, and so Theorem I.1 implies that
with probability at least 1 −δ,"
NREG,0.7638603696098563,"∀h ∈[H],
ε
4CpushH3 ≥Êπ[Dtv(̂πh(xh),π⋆
h(xh)] = P̂π[̂πh(xh) ≠π⋆
h(xh)],"
NREG,0.7645448323066393,where the equality follows by the fact that π⋆is deterministic.
NREG,0.7652292950034223,"From here, Theorem 4.1 follows swiftly as a consequence."
NREG,0.7659137577002053,"Proof of Theorem 4.1 (Setup I). Let ̂V1∶H be the value function estimates produced by RVFS0 within
Algorithm 6, and let ̂πRVFS
h
(⋅) ∈arg maxa∈A ̂Ph,εRVFS,δ′[̂Vh+1](⋅,a), for all h ∈[H] with ̂VH+1 ≡0
with εRVFS and δ′ as in Algorithm 6. By Lemma J.2, there is an event ̃E of probability at least 1 −δ/2
under which:"
NREG,0.7665982203969883,"P̂π[̂πh(xh) ≠π⋆
h(xh)] ≤
εRVFS
4H3Cpush
≤
ε
4H2 ,
(63)"
NREG,0.7672826830937713,where the last inequality follows by the choice of εRVFS in Algorithm 6.
NREG,0.7679671457905544,"For the rest of the proof, we condition on ̃E. By (63) and Assumption 4.3 (π⋆-realizability), the policy
̂πRVFS
1∶H returned by RVFS0 satisfies the condition in Proposition M.1 with εmis = ε/(4CpushH3). Thus,
by Proposition M.1, there is an event ̃E′ of probability at least 1 −δ/2 under which the policies ̂π1∶H
produced by RVFS.bc satisfy"
NREG,0.7686516084873375,"J(̂πRVFS
1∶H ) −J(̂π1∶H) ≤ε H + ε 2 ≤3ε 2 ."
NREG,0.7693360711841205,"We now condition on ̃E ∩̃E′. By Lemma C.6 (performance difference lemma), we have"
NREG,0.7700205338809035,"J(π⋆) −J(̂πRVFS
1∶H ) =
H
∑
h=1
ÊπRVFS[Qπ⋆
h (xh,π⋆
h(xh)) −Qπ⋆
h (xh, ̂πRVFS
h
(xh))],"
NREG,0.7707049965776865,"≤H
H
∑
h=1
P̂π[̂πh(xh) ≠π⋆
h(xh)],"
NREG,0.7713894592744696,"≤ε/(4H),"
NREG,0.7720739219712526,where the last inequality follows by (63).
NREG,0.7727583846680356,"Finally, by the union bound, we have P[̃E ∩̃E′] ≥1 −δ, and so the desired suboptimality guarantee in
(28) holds with probability at least 1 −δ."
NREG,0.7734428473648186,"Bounding the sample complexity.
The sample complexity is dominated by the call to RVFS0 within
RVFS.bc (Algorithm 6). Since RVFS.bc calls RVFS0 with ε = εRVFS = εH−1/48, we conclude from
Theorem I.1 that the total number of episodes is bounded by
̃O (C8
pushH23A ⋅ε−13)."
NREG,0.7741273100616016,"Bounding the number of oracle calls.
We now bound the number of calls to the oracle described
in Remark 4.1 that RVFS makes. To do this, note that bounding the number of calls to the oracle is
equivalent to bounding the number of executions of Line 14 in Algorithm 5, or, equivalently, the
number of times the operator ̂P in Line 14 is invoked during the entire execution of RVFS, including
all subsequent recursive calls to RVFS. The proof of Lemma I.2 establishes a direct bound on the
number of times the operator ̂P is called, giving an upper limit of O(CpushNtestH4ε−1), where
Ntest is a polynomial in the problem parameters (see Algorithm 5) and does not depend on X."
NREG,0.7748117727583846,"J.2
Proof of Lemma J.1 (Relaxed V π-Realizability under Gap)"
NREG,0.7754962354551677,"Proof of Lemma J.1. Fix ε′ ∈(0,1) and π ∈Πε′. Let ( ̃
Qh(x,a))(h,x,a)∈[H]×X×A be independent
random variables such that"
NREG,0.7761806981519507,"∀h ∈[H],
πh(⋅) ∈arg max
a′∈A
̃
Qh(⋅,a′)
and
∥̃
Qh −Qπ
h∥∞≤ε′, almost surely."
NREG,0.7768651608487337,"Such a collection of random state-action value functions ( ̃
Qh(x,a))(h,x,a)∈[H]×X×A is guaranteed to
exist for pi by the definition of Πε′. We will show via backward induction over ℓ= H + 1,...,1 that"
NREG,0.7775496235455168,"∀x ∈Xℓ,
πℓ(x) = π⋆
ℓ(x)
(64)"
NREG,0.7782340862422998,"almost surely, with the convention that πH+1 ≡π⋆
H+1 ≡πunif. This convention makes the base case,
ℓ= H + 1, hold trivially."
NREG,0.7789185489390829,"Now, we consider the general case. Fix h ∈[H] and suppose that (64) holds for all ℓ∈[h+1..H +1].
We will show that it holds for ℓ= h."
NREG,0.7796030116358659,"Thanks to the induction hypothesis, we have for all x ∈Xh+1 and a ∈A:"
NREG,0.7802874743326489,"V π
h+1(x) = V ⋆
h+1(x),"
NREG,0.7809719370294319,and so
NREG,0.7816563997262149,"Qπ
h ≡Ph[V π
h+1] ≡Ph[V ⋆
h+1] = V ⋆
h .
(65)"
NREG,0.7823408624229979,"Fix x ∈X. We will show that πh(x) = π⋆
h(x) almost surely. Note that thanks to (65), the fact that
∥̃
Qh −Th[Qπ
h+1]∥∞≤ε′ almost surely, implies that"
NREG,0.783025325119781,"∥̃
Qh −Q⋆
h∥∞≤ε′,"
NREG,0.783709787816564,"almost surely. Using this, we have, almost surely"
NREG,0.784394250513347,"Q⋆
h(x,πh(x)) ≥̃
Qh(x,πh(x)) −ε′,"
NREG,0.78507871321013,"≥̃
Qh(x,π⋆
h(x)) −ε′,"
NREG,0.785763175906913,"≥Q⋆
h(x,π⋆
h(x)) −2ε′ = Q⋆
h(x,π⋆
h(x)) −∆.
(66)"
NREG,0.7864476386036962,"On the other hand, if πh(x) ≠π⋆
h(x), then"
NREG,0.7871321013004792,"Q⋆
h(x,πh(x)) < Q⋆
h(x,π⋆
h(x)) −∆,"
NREG,0.7878165639972622,"which would contradict (66). Thus, πh(x) = π⋆
h(x), which concludes the induction and shows that
π ≡π⋆. We conclude that Πε′ = {π⋆}."
NREG,0.7885010266940452,"K
Guarantee for Weakly Correlated ExBMDPs (Proof of Theorem B.1)"
NREG,0.7891854893908282,"In this section, we prove Theorem B.1, the main guarantee for RVFSexo. First, in Appendix K.1 we
state a number of supporting technical lemmas and use them to prove Theorem B.1. The remainder
of the section (Appendix K.2 through Appendix K.6) contains the proofs for the intermediate results."
NREG,0.7898699520876112,"K.1
Analysis: Proof of Theorem B.1
Recall that the the V π-realizability assumption required by RVFS for Theorem 4.1 is not satisfied
in ExBMDPs, as the value functions for policies that act on the exogenous noise variables cannot
be realized as a function of the true decoder ϕ⋆. In RVFSexo, we address this issue by applying the
randomized rounding technique in Line 11 to the learned value functions. The crux of the analysis
will be to show that for an appropriate choice of the rounding parameters ζ1∶H, the policies produced
by RVFSexo are endogenous in the sense that we can write π(x) = π(ϕ⋆(x)) for all x ∈X. This will
allow us to leverage the decoder realizability (Assumption 3.3), which implies that the function class
V = V1∶H given by"
NREG,0.7905544147843943,"Vh ∶= {x ↦f(ϕ(x)) ∶f ∈[0,H]S,ϕ ∈Φ},
(67)"
NREG,0.7912388774811773,satisfies V π-realizability for all endogenous policies π.
NREG,0.7919233401779603,"In what follows, we first motivate the randomized rounding approach in RVFSexo in detail and prove
that it succeeds, then use this result to proceed with an analysis similar to that of Theorem 4.1 (Setup
II), re-using many of the technical tools developed for Theorem 4.1."
NREG,0.7926078028747433,"K.1.1
Randomized Rounding for Endogeneity
Naively, to ensure that the policies we execute are endogenous, it would seem that we require
knowledge of the true decoder ϕ⋆. Alas, knowing ϕ⋆trivializes the ExBMDP problem by reducing it
to the tabular setting. To avoid requiring knowledge of ϕ⋆, we apply a randomized rounding to the
policies learned by RVFSexo to ensure their endogeneity."
NREG,0.7932922655715263,"Let ε > 0 be fixed going forward. Recall that compared to RVFS, RVFSexo (Algorithm 8) takes an
additional input ζ1∶H ⊂(0,1/2) and executes the following coarsened policies:"
NREG,0.7939767282683093,"̂πh(⋅) ∈arg max
a∈A
⌈̂Ph,ε,δ[̂Vh+1](⋅,a)/ε + ζh⌉."
NREG,0.7946611909650924,"The rounding parameters ζ1∶H, which can be thought of as an offset, are chosen randomly; this will
be elucidated in the sequel."
NREG,0.7953456536618754,"Following a similar analysis to Appendix I (Setup II), we can associate a near-optimal benchmark
policy ˜π ∈Π2ε with ̂π in order to emulate certain properties of the ∆-gap assumption. In particular,
generalizing the construction in Eq. (27), we define a near-optimal benchmark policy ˜π recursively
via:"
NREG,0.7960301163586585,"∀x ∈X, ˜πτ(x;ζ1∶H,ε,δ) ∈arg max
a∈A
{ ⌈̂
Qτ(x,a)/ε + ζτ⌉,
if ∥̂
Qτ(x,⋅) −Pτ[V ˜π
τ+1](x,⋅)∥∞≤4ε2,
⌈Pτ[V ˜π
τ+1](x,a)/ε + ζτ⌉,
otherwise,
(68)"
NREG,0.7967145790554415,"for τ = H,...,1, where ̂
Qτ(⋅,a) ∶= ̂Pτ,ε,δ[̂Vτ+1](⋅,a)."
NREG,0.7973990417522245,"Naively, to use the benchmark policy ˜π within the analysis based on relaxed V π-realizability (As-
sumption I.1) in Appendix I , we would require the function class V to realize (V ˜π
h ). However, as
argued earlier, this is not feasible unless ˜π is an endogenous policy. Fortunately, it turns out that if
ζ1∶H (the additional input to RVFSexo) are drawn randomly from uniform distribution over [0,1/2],
then with constant probability, ˜π is indeed endogenous. What’s more, under such an event, and
for all possible choices of (̂Vh) in (68) uniformly, ˜π “snaps” onto the stochastic endogenous policy
¯π(⋅;ζ1∶H,ε) defined recursively as follows:"
NREG,0.7980835044490076,"¯πh(⋅;ζ1∶H,ε) ∈arg max
a∈A
⌈Ph[V ¯π
h+1](x,a)/ε + ζh⌉,
(69)"
NREG,0.7987679671457906,"for h = H,...,1. Informally, this happens because, as long as ζ1∶H ⊂(0,1/2) avoid certain
pathological locations in (0,1/2), the coarsened state-action value functions ε ⋅⌈Pτ[V ¯π
τ+1](x,a)/ε +
ζh⌉defining ¯π exhibit a “gap” of order Θ(ε2) separating optimal actions from the rest. This “snapping”
behavior is analogous to what happens in Setup I with V ⋆-realizability and ∆-gap, where Πε reduces
to {π⋆} for all ε < ∆/2 (see Lemma J.1). We formalize these claims in the next two lemmas. We
start by showing that ¯π is endogenous and that ¯π ∈Π2ε. The proof is in Appendix K.2."
NREG,0.7994524298425736,"Lemma K.1 (Endogenous Benchmark policies). For any δ ∈(0,1), ε ∈(0,1/2), and ζ1∶H ⊂(0,1/2),
the stochastic policy ¯π(⋅;ζ1∶H,ε) defined in Eq. (69) is endogenous, and we have ¯π(⋅;ζ1∶H,ε) ∈Π2ε."
NREG,0.8001368925393566,"Next, we show that ˜π “snaps” onto ¯π for the certain choices of ζ1∶H. The proof is in Appendix K.3."
NREG,0.8008213552361396,"Lemma K.2 (Snapping probability). Let δ ∈(0,1), ε ∈(0,1/2) be given, and Pζ denote the
probability law of ζ1,...,ζH ∼unif([0,1/2]). Then, there is an event Erand of probability at least
1 −24SAHε under ζ1∶H ∼Pζ such that for all ̃V ∈(X × [H] →[0,H]) simultaneously,"
NREG,0.8015058179329226,"∀h ∈[H],
˜πh(⋅; ̃V ,ζ1∶H,ε,δ) = ¯πh(⋅;ζ1∶H,ε),"
NREG,0.8021902806297057,"where ˜πh(⋅; ̃V ,ζ1∶H,ε,δ) is defined as in (68) with ̂V = ̃V , and ¯π is defined as in (69)."
NREG,0.8028747433264887,"The lemma together, with Lemma K.1, implies that with constant probability under Pζ, the benchmark
policies (˜πh) used in the analysis of RVFSexo are endogenous and satisfy ˜π ∈Π2ε."
NREG,0.8035592060232717,"K.1.2
Pushforward Coverability
In order to proceed with the analysis strategy in Appendix I, we need to verify that pushforward
coverability is satisfied for ExBMDPs under the weak correlation assumption. We do so in the next
lemma; see Appendix K.4 for a proof."
NREG,0.8042436687200547,"Lemma K.3 (Pushforward coverability). A weakly correlated ExBMDP with constant Cexo (see
Assumption B.1) satisfies Cpush-pushforward coverability (Assumption 4.1) with Cpush = Cexo ⋅SA,
where S ∈N is the number of endogenous states."
NREG,0.8049281314168378,"Equipped with the preceding lemmas, we proceed with an analysis similar to the approach for
Theorem 4.1 (Setup II) in Appendix I. In what follows, we state a number of technical lemmas that
apply the relevant results from Appendix I to the ExBMDP setting we consider here."
NREG,0.8056125941136209,"K.1.3
Bounding the Number of Test Failures
Since the size of the core sets C1∶H in RVFSexo is directly proportional to the number of test failures,
the next result, which bounds ∣Ch∣for all h ∈[H], allows us to show that RVFSexo (Algorithm 8)
terminates in a polynomial number of iterations."
NREG,0.8062970568104039,"Lemma K.4 (Bounding the number of test failures). Let δ,ε ∈(0,1) and ζ1∶H ∈[0,1/2] be given,
and suppose that Assumption B.1 (weak correlation) holds with Cexo > 0. Let f ∈V, be given, where
V is an arbitrary function class. Then, there is an event E of probability at least 1 −δ under which the
call to RVFSexo
0 (f,VH,∅,∅,0;V,ε,ζ1∶H,δ) (Algorithm 8) terminates, and throughout the execution
of RVFSexo
0 , we have"
NREG,0.8069815195071869,∣Ch∣≤⌈8ε−2CexoSAH⌉.
NREG,0.8076659822039699,Proof of Lemma K.4. The results follows from Lemma K.3 and Lemma I.1.
NREG,0.8083504449007529,"K.1.4
Value Function Regression Guarantee
We next give a guarantee for the estimated value functions ̂V1∶H computed within RVFSexo in Line 26
of Algorithm 8."
NREG,0.8090349075975359,"Lemma K.5 (Value function regression guarantee). Let h ∈[0..H], δ,ε ∈(0,1), and ζ1∶H ∈[0,1/2]
be given, and consider a call to RVFSexo
0
in the setting of Lemma K.4. Further, let V be defined as in
Eq. (67), and assume that Φ satisfies Assumption 3.3. Then, for any endogenous policy π in ΠS, there
is an event E′′
h of probability at least 1 −δ/H under which for any k ≥1, if"
NREG,0.809719370294319,"• RVFSexo
h
gets called for the kth time during the execution of RVFSexo
0 ; and"
NREG,0.810403832991102,"• this kth call terminates and returns (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H),"
NREG,0.811088295687885,"then if (̂πτ)τ≥h is the policy induced by ̂Vh∶H and Nreg is set as in Algorithm 8, we have"
NREG,0.811772758384668,"∑
(xh−1,ah−1)∈Ch"
NREG,0.812457221081451,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V π
h (xh))
2"
NREG,0.813141683778234,≤9kH2 log(8k2H∣V∣/δ)
NREG,0.8138261464750172,"Nreg
+ 8H2
∑
(xh−1,ah−1)∈Ch"
NREG,0.8145106091718002,"H
∑
τ=h
Êπ [Dtv(̂πτ(xτ),πτ(xτ)) ∣xh−1 = xh−1,ah−1 = ah−1],"
NREG,0.8151950718685832,"where the datasets {Dh(x,a) ∶(x,a) ∈Ch} are as in the definition of ̂Vh in (16)."
NREG,0.8158795345653662,"Proof of Lemma K.5.
Since Φ satisfies Assumption 3.3, the function class V = V1∶H satisfies
V π-realizability for all endogenous policies π (see Lemma D.1). Thus, the proof of Lemma K.5
follows from that of Lemma I.3 (see Appendix I.4)."
NREG,0.8165639972621492,"K.1.5
Confidence Sets
We now state a version of the confidence set validity lemma (Lemma I.4) that supports the ExBMDP
setting."
NREG,0.8172484599589322,"Lemma K.6 (Confidence sets). Let ε ∈(0,1/2) and ζ1∶H ⊂[0,1/2] be given, and suppose that"
NREG,0.8179329226557153,• Assumption B.1 holds with Cexo > 0;
NREG,0.8186173853524983,• The decoder class Φ satisfies Assumption 3.3;
NREG,0.8193018480492813,"• ζ1∶H ∈Erand, where Erand is the event in Lemma K.2."
NREG,0.8199863107460643,"Let f ∈V be arbitrary. There is an event E′′′ of probability at least 1 −3δ under which a call to
RVFSexo
0 (f,V,∅,∅,0;V,ε,ζ1∶H,δ) terminates and returns tuple (̂V1∶H, ̂V1∶H,C1∶H,B1∶H,t1∶H) such
that"
NREG,0.8206707734428473,"∀h ∈[H],
V ¯π
h ∈̂Vh,"
NREG,0.8213552361396304,where ¯π1∶H is the policy defined recursively via
NREG,0.8220396988364134,"¯πτ(x) ∈arg max
a∈A
⌈Pτ[V ¯π
τ+1](x,a)/ε + ζh⌉,
for τ = H,...,1.
(70)"
NREG,0.8227241615331964,"While the proof of this lemma is very similar to that of Lemma I.4, we need a dedicated treatment to
handle the rounding in RVFSexo. The fully proof of Lemma K.8 is in Appendix I.5."
NREG,0.8234086242299795,"K.1.6
Main Guarantee for RVFSexo"
NREG,0.8240930869267625,"We now state the central technical guarantee for RVFSexo, Lemma K.7, which shows that the base
invocation of the algorithm returns a set of value functions ̂V1∶H that induce a near-optimal policy ̂π,
as long as the randomized rounding parameters ζ1∶H satisfy ζ1∶H ∈Erand, where Erand is the success
event in Lemma K.2. The proof of the theorem is in Appendix K.6."
NREG,0.8247775496235455,"Lemma K.7 (Main guarantee for RVFSexo). Let δ,ε ∈(0,1) and ζ1∶H ⊂[0, 1"
NREG,0.8254620123203286,"2] be given, and suppose
that"
NREG,0.8261464750171116,• Assumption B.1 holds with Cexo > 0;
NREG,0.8268309377138946,• The decoder class Φ satisfies Assumption 3.3;
NREG,0.8275154004106776,"• ζ1∶H ∈Erand, where Erand is the event in Lemma K.2."
NREG,0.8281998631074606,"Then, for any f ∈V, with probability at least 1 −5δ, a call to RVFSexo
0 (f,VH,∅,∅,0;V,ε,ζ1∶H,δ)
(Algorithm 8) terminates and returns value functions ̂V1∶H such that"
NREG,0.8288843258042436,"∀h ∈[H],
P̂π[̂πh(xh) ≠¯πh(xh)] ≤
ε2"
NREG,0.8295687885010267,"4H3SACexo
,"
NREG,0.8302532511978097,"where ̂πh(x) ∈arg maxa∈A⌈̂Ph,ε,δ′[̂Vh+1](x,a)/ε + ζh⌉, for all h ∈[H], ¯π is defined as in Eq. (69),
and δ′ is defined as in Algorithm 8. Furthermore, the number of episodes used by RVFSexo
0
is bounded
by
̃O (C8
exoS8H10A9 ⋅ε−26)."
NREG,0.8309377138945927,"K.1.7
Concluding: Main Guarantee for RVFSexo.bc
To conclude, we prove Theorem B.1, which shows that RVFSexo.bc succeeds with high probability.
Recall that RVFSexo.bc (i) invokes RVFSexo multiple times for random samples ζ1∶H to ensure that the
success event for Lemma K.7 occurs for at least one invocation, and (ii) extracts an executable policy
using behavior cloning. Regarding the former point, note that the probability of the success event
of Lemma K.2 can be boosted by sampling i.i.d. ζ
(1)
1∶H,...,ζ
(n)
1∶H ∼Pζ inputs to RVFSexo for n ≥1;
as long as n is polynomially large, with high probability at least one of the inputs ζ
(1)
1∶H,...,ζ
(n)
1∶H
will satisfy the conclusion of Lemma K.2. Thus, it suffices to pick the policy with the highest value
function among the different calls to RVFSexo. Using this, we prove Theorem B.1."
NREG,0.8316221765913757,"Proof of Theorem B.1. Recall that Algorithm 9 picks the final policy ̂π
(iopt)
1∶H based on empirical
value function estimates. In particular, for every i ∈[Nboost] (with Nboost as in Algorithm 9), the
estimate ̂J(̂π
(i)
1∶H) for J(̂π
(i)
1∶H) is computed using Neval episodes. Thus, by Hoeffding’s inequality
and the union bound, we have that there is an event ˘E of probability at least 1 −δ/4 under which"
NREG,0.8323066392881588,"∀i ∈[Nboost],
∣J(̂π
(i)
1∶H) −̂J(̂π
(i)
1∶H)∣≤
√"
NREG,0.8329911019849419,2log(2Nboost/δ)/Neval.
NREG,0.8336755646817249,"Therefore, by definition of iopt in Algorithm 9, we have that under ˘E:"
NREG,0.8343600273785079,"∀i ∈[Nboost],
J(̂π
(i)
1∶H) ≤J(̂π
(iopt)
1∶H ) +
√"
NREG,0.8350444900752909,"2log(2Nboost/δ)/Neval,"
NREG,0.8357289527720739,"≤J(̂π
(iopt)
1∶H ) + ε/8,
(71)
where the last inequality follows by the choice of Neval in Algorithm 9. On the other hand, by
Lemma K.2, there is an event Esuccess of Pζ-probability at least
1 −(24SAHε)Nboost ≥1 −δ/4
(by the choice of Nboost in Algorithm 9)
under which there exists j ∈[Nboost] such that ζ
(j)
1∶H ∈Erand, where Erand is defined as in Lemma K.2.
In what follows, we condition on the event Esuccess and let j ∈[Nboost] be such that ζ
(j)
1∶H ∈Erand.
Further, we use ̂πRVFS
1∶H to denote the policy returned by the instance of RVFSexo that is used by
RVFSexo.bc to learn ̂π
(j)
1∶H."
NREG,0.836413415468857,"By Proposition M.1 (instantiated with εmis = 0), there is an event ̃E′ of probability at least 1 −δ/4
under which the policy ̂π(j) produced by BehaviorCloning satisfies"
NREG,0.83709787816564,"J(̂πRVFS
1∶H ) −J(̂π
(j)
1∶H) ≤ε"
NREG,0.837782340862423,"2.
(72)"
NREG,0.838466803559206,"By Lemma K.7 and the fact that ζ
(j)
1∶H ∈Erand, there is an event ̃E of probability at least 1 −δ/2 under
which:"
NREG,0.839151266255989,"∀h ∈[H],
∀h ∈[H],
P̂π[̂πh(xh) ≠¯πh(xh)] ≤
ε2
RVFS
4H3SACexo
≤
ε
4H2 ,
(73)"
NREG,0.839835728952772,"where ¯π(⋅) ∶= ¯π(⋅;ζ
(j)
1∶H,εRVFS) which is defined in (69)."
NREG,0.840520191649555,"Moving forward, we condition on ˘E ∩̃E ∩̃E′. By Lemma C.6 (the performance difference lemma),
we have"
NREG,0.8412046543463382,"J(¯π) −J(̂πRVFS
1∶H ) =
H
∑
h=1
ÊπRVFS[Q˜π
h(xh, ¯πh(xh)) −Q¯π
h(xh, ̂πRVFS
h
(xh))],"
NREG,0.8418891170431212,"≤H
H
∑
h=1
P̂π[̂πh(xh) ≠¯πh(xh)],"
NREG,0.8425735797399042,"≤ε/4,
(74)"
NREG,0.8432580424366872,"where the last inequality follows by (73). Now, by Lemma K.1, we have ¯π ∈Π2εRVFS, and so by
Lemma H.1,"
NREG,0.8439425051334702,"J(π⋆) −J(¯π) ≤6HεRVFS ≤ε/8,
(75)"
NREG,0.8446269678302533,"where the last inequality follows by the choice of εRVFS in Algorithm 9. Combining (75) with (71),
(72), and (74), we get that"
NREG,0.8453114305270363,"J(π⋆) −J(̂π1∶H) = J(π⋆) −J(̂π
(iopt)
1∶H ) ≤ε."
NREG,0.8459958932238193,"Finally, by the union bound, we have P[ ˘E ∩̃E ∩̃E′] ≥1−δ, and so the desired suboptimality guarantee
holds with probability at least 1 −δ."
NREG,0.8466803559206023,"Bounding the sample complexity.
The sample complexity is dominated by the calls to RVFSexo
0
within RVFSexo.bc (Algorithm 9). Since RVFSexo.bc calls RVFSexo
0
with suboptimality parameter
εRVFS = εH−1/48, we get by Lemma K.7 that the total sample complexity is bounded by"
NREG,0.8473648186173853,"̃O (C8
exoS8H36A9 ⋅ε−26)."
NREG,0.8480492813141683,"K.2
Proof of Lemma K.1 (Endogenous Benchmark Policies)
Proof of Lemma K.1. Fix δ ∈(0,1), ε ∈(0,1/2), and ζ′
1∶H ⊂[0,1/2]. We show via backward
induction over ℓ= H + 1,...,1 that ¯πτ(⋅;ζ′
1∶H,ε) is endogenous for all τ ∈[ℓ..H + 1], with the
convention that ¯πH+1 = πunif. The base case holds trivially by convention."
NREG,0.8487337440109514,"Fix h ∈[H] and suppose that the induction hypothesis holds for all ℓ∈[h+1..H +1]. We show that it
holds for ℓ= h. First, by the induction hypothesis, ¯πℓ(⋅;ζ′
1∶H,ε) is endogenous for all ℓ∈[h + 1..H].
Thus, there exists a function fh+1 ∶S →[0,H −h] such that"
NREG,0.8494182067077344,"V ¯π
h+1(x′) = fh+1(ϕ⋆(x′)),
∀x′ ∈X."
NREG,0.8501026694045175,"Therefore, we have for all (x,a) ∈X × A:"
NREG,0.8507871321013005,"Ph[V ¯π
h+1](x,a) = rh(x,a) + E[fh+1(ϕ⋆(xh+1)) ∣xh = x,ah = a],
= rh(x,a) + E[fh+1(sh+1) ∣xh = x,ah = a],"
NREG,0.8514715947980835,"= rh(x,a) + E[fh+1(sh+1) ∣sh = ϕ⋆(x),ah = a],
(76)"
NREG,0.8521560574948666,"where the last equality follows by the ExBMDP transition structure. Eq. (76) together with the fact
that the rewards are endogenous (by assumption) implies that there exists gh ∶S × A →[0,H −h + 1]
such that"
NREG,0.8528405201916496,"∀(x,a) ∈X × A,
Ph[V ¯π
h+1](x,a) = gh(ϕ⋆(x),a),"
NREG,0.8535249828884326,"which in turn implies that x ↦⌈Ph[V ¯π
h+1](x,a)/ε + ζ′
h⌉is only a function of x through ϕ⋆(x) for all
a ∈A. Thus, ¯πh is an endogenous policy and the induction is completed."
NREG,0.8542094455852156,"For the second claim, observe that for the functions ̃Q1,⋯, ̃QH ∈[0,H]X×A defined as"
NREG,0.8548939082819986,"∀h ∈[H],∀(x,a) ∈X × A,
̃Qh(x,a) = ε ⋅⌈Ph[V ¯π
h+1](x,a)/ε + ζ′
h⌉,"
NREG,0.8555783709787816,we have
NREG,0.8562628336755647,"∀h ∈[H],
¯πh(⋅;ζ′
1∶H,ε) ∈arg max
a∈A
̃Qh(⋅,a)
and
∥̃Qh −Q¯π
h∥∞≤2ε,"
NREG,0.8569472963723477,"which implies that ¯π(⋅;ζ′
1∶H,ε) ∈Π2ε."
NREG,0.8576317590691307,"K.3
Proof of Lemma K.2 (Snapping Probability)"
NREG,0.8583162217659137,"Proof of Lemma K.2.
Fix ε ∈(0,1) and δ ∈(0,1/2). For τ ≤ℓ∈[H], let Pζ
τ∶ℓdenote the
probability law of ζτ,...,ζℓ. We also use the shorthand Pζ
τ for Pζ
τ∶τ, for all τ ∈[H]. We show via
backward induction over ℓ= H + 1,...,1 that there exists an event Eℓof Pζ
ℓ∶H-probability at least
1 −24SA(H −ℓ+ 1)ε under which for all ̃V ∈(X × [H] →[0,H]):"
NREG,0.8590006844626967,"∀τ ∈[ℓ..H],
˜πτ(⋅; ̃V ,ζ1∶H,ε,δ) = ¯πτ(⋅;ζ1∶H,ε),"
NREG,0.8596851471594799,with the convention that ¯πH+1 ≡˜πH+1 ≡πunif. We then set Erand = E1.
NREG,0.8603696098562629,The base case follows trivially by convention.
NREG,0.8610540725530459,"We now proceed with the inductive step. Fix h ∈[H] and suppose that the induction hypothesis
holds for all ℓ∈[h + 1..H]. We show that it holds for ℓ= h. Throughout, we condition on Eh+1. By
definition of Eh+1, we have for all ̃V ∈(X × [H] →[0,H]):"
NREG,0.8617385352498289,"∀ℓ∈[h + 1..H],
¯πℓ(⋅;ζ1∶H,ε) = ˜πℓ(⋅; ̃V ,ζ1∶H,ε,δ)."
NREG,0.8624229979466119,"This implies that for all ̃V ∈(X × [H] →[0,H]):"
NREG,0.8631074606433949,"∀x ∈X, ˜πh(x; ̃V ,ζ1∶H,ε,δ) ∈arg max
a∈A
{ ⌈̂
Qh(x,a)/ε + ζh⌉,
if ∥̂
Qh(x,⋅) −Ph[V ¯π
h+1](x,⋅)∥∞≤4ε2,
⌈Ph[V ¯π
h+1](x,a)/ε + ζh⌉,
otherwise,"
NREG,0.863791923340178,"by the definition of ˜πh in (68), where ̂
Qh(⋅,a) ∶= ̂Ph,ε,δ[̃Vh+1](⋅,a). From this, we see that to prove
˜πh(⋅; ̃V ,ζ1∶H,ε,δ) = ¯πh(⋅;ζ1∶H,ε) for all ̃V , it suffices to show that for all x ∈X and ̃V ,"
NREG,0.864476386036961,"arg max
a∈A
⌈̂
Qh(x,a)/ε + ζh⌉= arg max
a∈A
⌈Ph[V ¯π
h+1](x,a)/ε + ζh⌉, whenever ∣̂
Qh(x,a) −Ph[V ¯π
h+1](x,a)∣≤4ε2."
NREG,0.865160848733744,Observe that a sufficient condition for this to hold is that
NREG,0.865845311430527,"∀x ∈X,∀a ∈A,∀δ ∈[−4ε2,4ε2],
⌈(Ph[V ¯π
h+1](x,a) + δ) ⋅ε−1 + ζh⌉= ⌈Ph[V ¯π
h+1](x,a) ⋅ε−1 + ζh⌉,
(77)"
NREG,0.86652977412731,"where δ represents all the possible values that the difference ̂
Qh(x,a) −Ph[V ¯π
h+1](x,a) is allowed
to take. By Lemma K.1, we know that ¯π is endogenous, and so there exists a function gh ∶S × A →
[0,H −h + 1] such that"
NREG,0.867214236824093,"∀x ∈X,a ∈A,
Ph[V ¯π
h+1](x,a) = gh(ϕ⋆(x),a)."
NREG,0.8678986995208761,"Toward proving Eq. (77), observe that for any (s,a) ∈S ∈A, if ζh is such that"
NREG,0.8685831622176592,"gh(s,a)/ε + ζh + 4ε ≤⌈gh(s,a)/ε + ζh⌉,
and
gh(s,a)/ε + ζh −4ε > ⌈gh(s,a)/ε + ζh⌉−1,
(78)"
NREG,0.8692676249144422,"then, for all δ ∈[−4ε2,4ε2] and all x ∈X such that ϕ⋆(x) = s, we have"
NREG,0.8699520876112252,"⌈(Ph[V ¯π
h+1](x,a) + δ)/ε + ζh⌉= ⌈(gh(s,a) + δ)/ε + ζh⌉= ⌈gh(s,a)/ε + ζh⌉= ⌈Ph[V ¯π
h+1](x,a)/ε + ζh⌉."
NREG,0.8706365503080082,"Therefore, if we let Eh(s,a) denote the event in (78), then under ⋂(s,a)∈S×A Eh(s,a), the desired
condition in (77) holds. At this point, setting Eh = (⋂(s,a)∈S×A Eh(s,a)) ∩Eh+1 would complete the
induction as long as Pζ
h∶H[Eh] ≥1 −24SA(H −h + 1)ε. We now show that this is indeed the case by
bounding the probability of the event ⋂(s,a)∈S×A Eh(s,a). By the union bound, we have"
NREG,0.8713210130047913,"Pζ
h∶H"
NREG,0.8720054757015743,"⎡⎢⎢⎢⎢⎣
⋂
(s,a)∈S×A
Eh(s,a) ∣Eh+1
⎤⎥⎥⎥⎥⎦
≥1 −
∑
(s,a)∈S×A
Pζ
h∶H [Eh(s,a)c ∣Eh+1],
(79)"
NREG,0.8726899383983573,"where Eh(s,a)c denotes the complement of Eh(s,a). We now bound the probability"
NREG,0.8733744010951403,"Pζ
h∶H [Eh(s,a)c ∣Eh+1]."
NREG,0.8740588637919233,"Fix (s,a) ∈S × A. We have that ζh ∈E(s,a)c if and only if"
NREG,0.8747433264887063,"gh(s,a)/ε + ζh + 4ε > ⌈gh(s,a)/ε + ζh⌉,
or
gh(s,a)/ε + ζh −4ε ≤⌈gh(s,a)/ε + ζh⌉−1.
(80)"
NREG,0.8754277891854894,"Now, since ζh ∈[0,1/2], Lemma L.3 (instantiated with (x,ζ,ν) = (gh(s,a)/ε,ζh,4ε)) implies that
(80) holds only if"
NREG,0.8761122518822724,"⌈gh(s,a)/ε⌉−4ε ≤gh(s,a)/ε + ζh ≤⌈gh(s,a)/ε⌉+ 4ε
or
0 ≤ζh ≤4ε.
(81)"
NREG,0.8767967145790554,"Further, note that since ζh is uniformly distributed over [0,1/2], the Pζ
h-probability of the event in
(81) is at most the sum of the lengths of the intervals"
NREG,0.8774811772758385,"[⌈gh(s,a)/ε⌉−gh(s,a)/ε −4ε,⌈gh(s,a)/ε⌉−gh(s,a)/ε + 4ε]
and
[0,4ε],"
NREG,0.8781656399726215,"multiplied by 2, which is equal to 24ε. Therefore, we have"
NREG,0.8788501026694046,"Pζ
h∶H [Eh(s,a)c ∣Eh+1]"
NREG,0.8795345653661876,"≤Pζ
h [⌈gh(s,a)/ε⌉−4ε ≤gh(s,a)/ε + ζh ≤⌈gh(s,a)/ε⌉+ 4ε or 0 ≤ζh ≤4ε] ≤24ε.
Combining this with (79), we obtain"
NREG,0.8802190280629706,"Pζ
h∶H"
NREG,0.8809034907597536,"⎡⎢⎢⎢⎢⎣
⋂
(s,a)∈S×A
Eh(s,a) ∣Eh+1
⎤⎥⎥⎥⎥⎦
≥1 −
∑
(s,a)∈S×A
Pζ
h∶H [Eh(s,a)c ∣Eh+1] ≥1 −24SAε."
NREG,0.8815879534565366,"Thus, by setting Eh = (⋂(s,a)∈S×A Eh(s,a)) ∩Eh+1, we get that"
NREG,0.8822724161533196,"Pζ
h∶H[Eh] ≥Pζ
h+1∶H[Eh+1] ⋅Pζ
h∶H[Eh ∣Eh+1] ≥(1 −24SA(H −h)ε)(1 −24SAε),
≥1 −24SA(H −h + 1)ε,
which completes the induction."
NREG,0.8829568788501027,"K.4
Proof of Lemma K.3 (Coverability in Weakly Correlated ExBMDP)
Proof of Lemma K.3. Fix h ∈[2..H] and define the measure µ as
µ(x) ∶= ∑
ξ′∈Ξ
q(x′ ∣(ϕ⋆
h(x′),ξ′)) ⋅P[ξh = ξ′] ⋅P[sh = ϕ⋆
h(x′) ∣sh−1 = ϕ⋆
h−1(x),ah−1 = a],"
NREG,0.8836413415468857,"for all h ∈[H] and x ∈X. We show that µ satisfies Assumption 4.1 with Cpush = Cexo ⋅SA. First,
note that µ is indeed a probability measure over X. Fix (x,a,x′) ∈X × A × X. We have
P[xh = x′ ∣xh−1 = x,ah−1 = a]"
NREG,0.8843258042436687,"= P[xh = x′,xh−1 = x ∣ah−1 = a]"
NREG,0.8850102669404517,"P[xh−1 = x ∣ah−1 = a]
,"
NREG,0.8856947296372347,"= P[xh = x′,xh−1 = x ∣ah−1 = a]"
NREG,0.8863791923340179,"P[xh−1 = x]
,"
NREG,0.8870636550308009,"= ∑ξ,ξ′∈Ξ q(x′ ∣(ϕ⋆
h(x′),ξ′)) ⋅q(x ∣(ϕ⋆
h−1(x),ξ)) ⋅P[sh = ϕ⋆
h(x′),ξh = ξ′,sh−1 = ϕ⋆
h−1(x),ξh−1 = ξ ∣ah−1 = a]"
NREG,0.8877481177275839,"∑ξ∈Ξ q(x ∣(ϕ⋆
h−1(x),ξ)) ⋅P[sh−1 = ϕ⋆
h−1(x),ξh−1 = ξ]
,"
NREG,0.8884325804243669,and so by the ExBMDP structure:
NREG,0.8891170431211499,"= ∑ξ,ξ′∈Ξ q(x′ ∣(ϕ⋆
h(x′),ξ′)) ⋅q(x ∣(ϕ⋆
h−1(x),ξ)) ⋅P[ξh = ξ′,ξh−1 = ξ] ⋅P[sh = ϕ⋆
h(x′) ∣sh−1 = ϕ⋆
h−1(x),ah−1 = a]"
NREG,0.8898015058179329,"∑ξ∈Ξ q(x ∣(ϕ⋆
h(x),ξ)) ⋅P[ξh−1 = ξ]
,"
NREG,0.890485968514716,and by Assumption B.1
NREG,0.891170431211499,"≤Cexo
∑ξ,ξ′∈Ξ q(x′ ∣(ϕ⋆
h(x′),ξ′)) ⋅q(x ∣(ϕ⋆
h−1(x),ξ)) ⋅P[ξh = ξ′] ⋅P[ξh−1 = ξ] ⋅P[sh = ϕ⋆
h(x′) ∣sh−1 = ϕ⋆
h−1(x),ah−1 = a]"
NREG,0.891854893908282,"∑ξ∈Ξ q(x ∣(ϕ⋆
h(x),ξ)) ⋅P[ξh−1 = ξ]"
NREG,0.892539356605065,"= Cexo ∑
ξ′∈Ξ
q(x′ ∣(ϕ⋆
h(x′),ξ′)) ⋅P[ξh = ξ′] ⋅P[sh = ϕ⋆
h(x′) ∣sh−1 = ϕ⋆
h−1(x),ah−1 = a],"
NREG,0.893223819301848,"= CexoSA ⋅µ(x′),
This completes the proof."
NREG,0.893908281998631,"K.5
Proof of Lemma K.6 (Confidence Sets)
To prove Lemma K.6, we need the following consequence of tests in Line 14 passing for all
ℓ∈[h + 1..H]."
NREG,0.894592744695414,"Lemma K.8 (Consequence of passed tests). Let h ∈[0..H], ε > 0, and ζ1∶H ∈[0,1/2] be given and
consider a call to RVFSexo
0
(Algorithm 8) in the setting of Lemma K.4. Further, let E be the event of
Lemma K.4. There exists an event E′
h of probability at least 1 −δ/H such that under E ∩E′
h, if a call
to RVFSexo
h
during the execution of RVFSexo
0
terminates and returns (̂Vh∶H, ̂Vh∶H,Ch∶H,Bh∶H,th∶H),
then for any (xh−1,ah−1) ∈Ch and ℓ∈[h + 1..H + 1]:"
NREG,0.8952772073921971,"P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(xℓ−1,a)∣> 3ε2 ∣xh−1 = xh−1,ah−1 = ah−1
⎤⎥⎥⎥⎥⎦
≤4log(8M 6N 2
testH8/δ)
Ntest
,"
NREG,0.8959616700889802,"where (̂πτ)τ≥h ⊂ΠS, M, and Ntest are as in RVFSexo
h
(Algorithm 8)."
NREG,0.8966461327857632,"Proof of Lemma K.8. This is just a restatement of Lemma I.2, and the proof is exactly the same as
the latter."
NREG,0.8973305954825462,"We will also use Lemma I.5; even though this result is stated in section for the V π-realizable setting,
it is also applicable to the ExBMDP variant of RVFS as it merely says something about the order in
which the (RVFSexo
h ) instances are called. With this, we now prove Lemma K.6."
NREG,0.8980150581793293,"Proof of Lemma K.6. The proof is very similar to that of Lemma I.2, with differences to account
for the “coarsening” of the learned and benchmark policies."
NREG,0.8986995208761123,"We prove the desired result for E′′′ ∶= E ∩E′
1 ∩E′′
1 ∩⋅⋅⋅∩E′
H ∩E′′
H, where E, (E′
h), and (E′′
h) are the
events in Lemma K.4, Lemma K.8, and Lemma K.5, respectively. Throughout, we condition on E′′′.
First, note that by Lemma K.4, RVFSexo
0
terminates. Let (̂V1∶H, ̂V1∶H,C1∶H,B1∶H,t1∶H) be its returned
tuple."
NREG,0.8993839835728953,"We show via backward induction over ℓ= H + 1,...,1, that"
NREG,0.9000684462696783,"V ˜π
ℓ∈̂Vℓ,
(82)"
NREG,0.9007529089664613,where ˜π1∶H is the stochastic policy defined recursively via
NREG,0.9014373716632443,"∀x ∈X, ˜πτ(x;ζ1∶H,ε,δ) ∈arg max
a∈A
{ ⌈̂
Qτ(x,a)/ε + ζτ⌉,
if ∥̂
Qτ(x,⋅) −Pτ[V ˜π
τ+1](x,⋅)∥∞≤4ε2,
⌈Pτ[V ˜π
τ+1](x,a)/ε + ζτ⌉,
otherwise,"
NREG,0.9021218343600274,"for τ = H,...,1, where ̂
Qτ(⋅,a) ∶= ̂Pτ,ε,δ[̂Vτ+1](⋅,a). Note that since ζ1∶H ∈Erand (for Erand is
defined in Lemma K.2), we have ˜π ≡¯π, where ¯π is as in (70). Thus, instantiating the induction
hypothesis with ℓ= h and using the definition of the confidence sets (̂Vℓ) in (16) together with
V ˜π
h ≡V ¯π
h (since ˜π ≡¯π) implies the desired result."
NREG,0.9028062970568104,"Base case [ℓ= H + 1].
Holds trivially since V π
H+1 ≡0 for any π ∈ΠS by convention."
NREG,0.9034907597535934,"General case [ℓ≤H].
Fix h ∈[H] and suppose that (82) holds for all ℓ∈[h + 1..H + 1]. We
show that this remains true for ℓ= h. First, note that if RVFSexo
h
is never called during the execution
of RVFSexo
0 , then ̂Vh = Vh, and so (82) holds for ℓ= h, since ˜π = ¯π is endogenous under ζ1∶H ∈Erand,
where Erand is the event in Lemma K.2."
NREG,0.9041752224503764,"Now, suppose that RVFSexo
h
is called at least once, and let (̂V +
h∶H, ̂V+
h∶H,C+
h∶H,B+
h∶H,t+
h∶H) be the output
of the last call to RVFSexo
h
throughout the execution of RVFSexo
0 . Next, we show that"
NREG,0.9048596851471595,"(̂V +
h∶H, ̂V+
h∶H,C+
h∶H) = (̂Vh∶H, ̂Vh∶H,Ch∶H).
(83)"
NREG,0.9055441478439425,"The for-loop in Line 16 ensures that no instance of (RVFSexo
τ )τ>h can be called after the last call to
RVFSexo
h
(see Lemma I.5). Thus, the estimated value functions, confidence sets, and core sets for
layers h + 1,...,H remain unchanged after the last call to RVFSexo
h ; that is, (83) holds. Thus, by
Lemma K.8, and since we are conditioning on E′
h+1∶H, we have that for all (xh−1,ah−1) ∈Ch and
ℓ∈[h + 1..H + 1]:"
NREG,0.9062286105407256,"P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vℓ
max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[fℓ])(xℓ−1,a)∣> 3ε2 ∣xh−1 = xh−1,ah−1 = ah−1
⎤⎥⎥⎥⎥⎦
≤4log(8M 6N 2
testH8/δ)
Ntest
. (84)"
NREG,0.9069130732375086,"Now, by the induction hypothesis, we have V ˜π
ℓ∈̂Vℓ, and so substituting V ˜π
ℓfor fℓin (84), we get
that for all (xh−1,ah−1) ∈Ch and ℓ∈[h + 1..H + 1]:"
NREG,0.9075975359342916,"P̂π[max
a∈A ∣(Pℓ−1[̂Vℓ] −Pℓ−1[V ˜π
ℓ])(xℓ−1,a)∣> 3ε2 ∣xh−1 = xh−1,ah−1 = ah−1] ≤4log(8M 6N 2
testH8/δ)
Ntest
."
NREG,0.9082819986310746,"Therefore, by Lemma L.2 (instantiated with µ[⋅] = P̂π[⋅∣xh−1 = xh−1,ah−1 = ah−1], τ = ℓ−1,
ε′ = ε2, and Vτ+1 = V ˜π
ℓ), we have that for all (xh−1,ah−1) ∈Ch and ℓ∈[h + 1..H + 1]:"
NREG,0.9089664613278576,"Êπ[Dtv(̂πℓ−1(xℓ−1), ˜πℓ−1(xℓ−1)) ∣xh−1 = xh−1,ah−1 = ah−1] ≤4log(8M 6N 2
testH8/δ)
Ntest
+ δ′. (85)"
NREG,0.9096509240246407,"Now, since ¯π(⋅;ζ1∶H,ε) is endogenous and ˜π ≡¯π (thanks to ζ1∶H ∈Erand), Lemma K.5 (applied with
π = ¯π) and the conditioning on E′′
h+1∶H imply that:"
NREG,0.9103353867214237,"∑
(xh−1,ah−1)∈Ch"
NREG,0.9110198494182067,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V ˜π
h (xh))
2"
NREG,0.9117043121149897,≤9kH2 log(8k2H∣V∣/δ)
NREG,0.9123887748117727,"Nreg
+ 8H2
∑
(xh−1,ah−1)∈Ch"
NREG,0.9130732375085557,"H
∑
τ=h
Êπ [Dtv(̂πτ(xτ), ˜πτ(xτ)) ∣xh−1 = xh−1,ah−1 = ah−1], (86)"
NREG,0.9137577002053389,"where the datasets {Dh(x,a) ∶(x,a) ∈Ch} are as in the definition of ̂Vh in (16). Combining (86)
with (85), we conclude that"
NREG,0.9144421629021219,"∑
(xh−1,ah−1)∈Ch"
NREG,0.9151266255989049,"1
Nreg
∑
(xh,−)∈Dh(xh−1,ah−1)
(̂Vh(xh) −V ˜π
h (xh))
2"
NREG,0.9158110882956879,≤9MH2 log(8M 2H∣V∣/δ)
NREG,0.9164955509924709,"Nreg
+ 8MH3 ⋅4log(8M 6N 2
testH8/δ)
Ntest
+ 8MH3δ′,"
NREG,0.917180013689254,= 9MH2 log(8M 2H∣V∣/δ)
NREG,0.917864476386037,"Nreg
+ 8MH3 ⋅4log(8M 6N 2
testH8/δ)
Ntest
+ 8MH3
δ
4M 7N 2
testH8∣V∣,"
NREG,0.91854893908282,"≤ε2
reg,
(87)"
NREG,0.919233401779603,"where we have used that ∣Ch∣≤M. By the definition of ̂Vh in (16), (87) implies that V ˜π
h ∈̂Vh, which
completes the induction."
NREG,0.919917864476386,"K.6
Proof of Lemma K.7 (Main Guarantee of RVFSexo)"
NREG,0.920602327173169,"Proof of Lemma K.7. We condition on the event ̃E ∶= E ∩E′′′ ∩E′
1 ∩⋅⋅⋅∩E′
H, where E, E′′′, and
(E′
h) are the events in Lemma K.4, Lemma K.6, and Lemma K.8, respectively. Note that by the union
bound, we have P[̃E] ≥1 −5δ. By Lemma K.6, we have that"
NREG,0.921286789869952,"∀h ∈[H],
P̂π
⎡⎢⎢⎢⎢⎣
sup
f∈̂Vh
max
a∈A ∣(Ph−1[̂Vh] −Ph−1[fh])(xh−1,a)∣> 3ε2
⎤⎥⎥⎥⎥⎦
≤4log(8M 6N 2
testH8/δ)
Ntest
, (88)"
NREG,0.9219712525667351,"where M = ⌈8ε−2CexoSAH⌉and Ntest = 28M 2Hε−2 log(8M 6H8ε−2δ−1). On the other hand, by
Lemma K.6, we have
∀h ∈[H],
V ˜π
h ∈̂Vh."
NREG,0.9226557152635181,"Thus, substituting V ˜π
h for fh in (88) we get that for all h ∈[H + 1]."
NREG,0.9233401779603012,"P̂π[max
a∈A ∣(Ph−1[̂Vh] −Ph−1[V ˜π
h ])(xh−1,a)∣> 3ε2] ≤4log(8M 6N 2
testH8/δ)
Ntest
."
NREG,0.9240246406570842,"This together with Lemma L.2, instantiated with µ[⋅] = P̂π[⋅]; τ = h −1; Vτ+1 = V ˜π
h ; and δ = δ′ (with
δ′ as in Algorithm 5), translates to:"
NREG,0.9247091033538672,"∀h ∈[H],
Êπ[Dtv(˜πh(xh),̂πh(xh))] ≤4log(8M 6N 2
testH8/δ)
Ntest
+ δ′,"
NREG,0.9253935660506503,"= 4log(8M 6N 2
testH8/δ)
Ntest
+
δ
4M 7N 2
testH8∣V∣, ≤
ε2"
NREG,0.9260780287474333,"4H3SACexo
,
(89)"
NREG,0.9267624914442163,"where the last step follows from the fact that Ntest = 28M 2Hε−2 log(8M 6H8ε−2δ−1) (with M as
in Line 3). Now, since ζ1∶H ∈Erand (by assumption), Lemma K.2 implies that ˜π ≡¯π, where the latter
is the deterministic policy defined in (69). Thus, by (89), we have"
NREG,0.9274469541409993,"∀h ∈[H],
P̂π[¯πh(xh) ≠̂πh(xh)] = Êπ[Dtv(˜πh(xh),̂πh(xh))] ≤
ε2"
NREG,0.9281314168377823,"4H3SACexo
,"
NREG,0.9288158795345653,"where the first equality follows by the fact that P[¯πh(x) ≠̂πh(x)] = Dtv(˜πh(x),̂πh(x)), for all
x ∈X, since ¯πh is deterministic."
NREG,0.9295003422313484,"Bounding the sample complexity.
We now bound the number of episodes used by Algo-
rithm 8 under ̃E. First, we fix h ∈[H], and focus on the number of episodes used within a
to call RVFSexo
h ; excluding any episodes used by any recursive calls to RVFSexo
τ
for τ > h. We
start by counting the number of episodes used to test the fit of the estimated value functions
̂Vh+1∶H.
Starting from Line 8, there are for-loops over (xh−1,ah−1) ∈Ch, ℓ= H,...,h + 1,
and n ∈[Ntest] to collected partial episodes using the learned policy ̂π in Algorithm 8, where
Ntest = 28M 2Hε−2 log(8M 6H8ε−2δ−1) and M = ⌈8ε−2CexoSAH⌉. Note that ̂π uses the local
simulator and requires Nsim = 2log(4M 7N 2
testH2∣V∣/δ)/ε2 samples to output an action at each layer
(since Algorithm 8 calls Algorithm 7 with confidence level δ′ = δ/(8M 7N 2
testH8∣V∣)). Also, note
that whenever a test fails in Line 14, the for-loop in Line 8 resumes. We also know (by Lemma K.4)
that the number of times the test fails in Line 14 is at most M. Thus, the number of times the for-loop
in Line 8 resumes is bounded by HM; here, H accounts for test failures for all layers τ ∈[h+1..H].
Thus, the number of episodes required to between lines Line 8 and Line 11 is bounded by"
NREG,0.9301848049281314,"# episodes for roll-outs ≤
MH
±
# of times Line 8 resumes
⋅
MH2NtestNsim
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
Number of episodes in case of no test failures"
NREG,0.9308692676249144,".
(90)"
NREG,0.9315537303216974,"Note that the test in Line 14 also uses local simulator access because it calls the operator ̂P for every
a ∈A. Thus, the number of episodes used for the test in Line 14 is bounded by"
NREG,0.9322381930184805,"# episodes for the tests ≤
MH
±
# of times Line 8 resumes
⋅
MHANtestNsim
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
Number of episodes used in Line 14"
NREG,0.9329226557152636,".
(91)"
NREG,0.9336071184120466,"We now count the number of episodes used to re-fit the value function; Line 16 onwards. Note
that starting from Line 16, there are for-loops over (xh−1,ah−1) ∈Ch and i ∈[Nreg] to generate
A ⋅Nest(∣Ch∣) ≤A ⋅Nest(M) partial episodes using ̂π, where Nest(k) = 2N 2
reg log(8ANregHk3/δ)
is as in Algorithm 8. And, since ̂π uses local simulator access and requires Nest samples (see
Algorithm 7) to output an action at each layer, the number of episodes used to refit the value function
is bounded by"
NREG,0.9342915811088296,"# episodes for V -refitting ≤MNregANest(M)HNsim.
(92)"
NREG,0.9349760438056126,"Therefore, by (90), (91), and (92), the number of episodes used within a single call to RVFSexo
h
(not
accounting for episodes used by recursive calls to RVFSexo
τ , for τ > h) is bounded by"
NREG,0.9356605065023956,"# episodes used locally within RVFSexo
h
≤M 2H(H + A)NtestNsim + MNregANest(M)HNsim.
(93)"
NREG,0.9363449691991786,"Finally, by Lemma K.4, RVFSexo
h
may be called at most M times throughout the execution of RVFSexo
0 .
Using this together with (93) and accounting for the number of episodes from all layers h ∈[H], we
get that the total number of episodes is bounded by"
NREG,0.9370294318959617,M 3H2(H + A)NtestNsim + M 2H2NregANest(M)Nsim.
NREG,0.9377138945927447,"Substituting the expressions of M, Ntest, Nest, Nsim, and Nreg from Algorithm 8 and Algorithm 7,
we obtain the desired number of episodes, which concludes the proof."
NREG,0.9383983572895277,"L
Additional Technical Lemmas"
NREG,0.9390828199863107,"Lemma L.1. Let τ ∈[H] and ε,δ,ν ∈(0,1) be given. Consider two value functions Vτ+1, ̂Vτ+1 ∈
[0,H] and a measure µ ∈∆(X) such that"
NREG,0.9397672826830937,"Pxτ ∼µ[I{max
a∈A ∣(Pτ[̂Vτ+1] −Pτ[Vτ+1])(xτ,a)∣> 3ε}] ≤ν.
(94)"
NREG,0.9404517453798767,"Further, for x ∈X, let ̂πτ(x) ∈arg maxa∈A ̂
Qτ(x,a) ∶= ̂Pτ,ε,δ[̂Vτ+1](x,a) and inductively define a
randomized policy ˜π via"
NREG,0.9411362080766599,"˜πτ(x) ∈arg max
a∈A
{ ̂
Qτ(x,a),
if ∥̂
Qτ(x,⋅) −Pτ[Vτ+1](x,⋅)∥∞≤4ε,
Pτ[Vτ+1](x,a),
otherwise."
NREG,0.9418206707734429,"Then, we have"
NREG,0.9425051334702259,"Exτ ∼µ[Dtv(̂πτ(xτ), ˜πτ(xτ))] ≤ν + δ."
NREG,0.9431895961670089,"Proof of Lemma L.1.
In this proof, we let Pµ denote the probability law of xτ and
PP denote the probability law of ̂Pτ,ε,δ.
Denote by E the Pµ-measurable event that
maxa∈A ∣(Pτ[̂Vτ+1] −Pτ[Vτ+1])(xτ,a)∣≤3ε. Fix x ∈E, and let Ex be the PP-measurable event
that maxa∈A ∣̂Pτ,ε,δ[̂Vτ+1](x,a) −Pτ[Vτ+1](x,a)∣≤4ε. From the definition of ˜πτ, we have that"
NREG,0.9438740588637919,"Dtv(̂πτ(x), ˜πτ(x)) = 1"
NREG,0.944558521560575,"2 ∑
a∈A
∣PP [̂πτ(x) = a] −PP [˜πτ(x) = a]∣, = 1"
NREG,0.945242984257358,"2 ∑
a∈A
∣PP[Ex]PP [̂πτ(x) = a ∣Ex] + PP[Ec
x]PP [̂πτ(x) = a ∣Ec
x] −PP[Ex]PP [˜πτ(x) = a ∣Ex] −PP[Ec
x]PP [˜πτ(x) = a ∣E ≤1"
NREG,0.945927446954141,"2 ∑
a∈A
PP[Ex] ⋅∣PP [̂πτ(x) = a ∣Ex] −PP [˜πτ(x) = a ∣Ex]∣"
NREG,0.946611909650924,"+ ∑
a∈A
PP[Ec
x] ⋅∣PP [̂πτ(x) = a ∣Ec
x] −PP [˜πτ(x) = a ∣Ec
x]∣,
(Jensen’s inequality)"
NREG,0.947296372347707,"and since PP [̂πτ(x) = a ∣Ex] = PP [˜πτ(x) = a ∣Ex] ∀a ∈A, we have that = 1"
NREG,0.94798083504449,"2 ∑
a∈A
PP[Ec
x] ⋅∣PP [̂πτ(x) = a ∣Ec
x] −PP [˜πτ(x) = a ∣Ec
x]∣,,"
NREG,0.9486652977412731,"≤PP [Ec
x],"
NREG,0.9493497604380561,"= PP [max
a∈A ∣̂Pτ,ε,δ[̂Vτ+1](x,a) −Pτ[Vτ+1](x,a)∣> 4ε],"
NREG,0.9500342231348392,"≤PP [max
a∈A ∣̂Pτ,ε,δ[̂Vτ+1](x,a) −Pτ[̂Vτ+1](x,a)∣> ε],
(see below)
(95)"
NREG,0.9507186858316222,"≤δ,
(96)"
NREG,0.9514031485284052,"where (95) follows from x ∈E and the last inequality follows from Lemma H.2. Therefore, we have"
NREG,0.9520876112251883,"Eµ[Dtv(̂πτ(xτ), ˜πτ(xτ))] ≤Pµ[E] ⋅Eµ[Dtv(̂πτ(xτ), ˜πτ(xτ)) ∣E] + Pµ[Ec],
≤δ + ν,"
NREG,0.9527720739219713,"where the first inequality follows by the fact that the total variation distance is bounded by 1, and the
last inequality follows by (94) and (96)."
NREG,0.9534565366187543,"Lemma L.2. Let τ ∈[H] and ε′,δ,ν ∈(0,1), and ζ1∶H ∈[0,1/2] be given. Further, consider two
value functions Vτ+1, ̂Vτ+1 ∈[0,H] and measure µ ∈∆(X) such that"
NREG,0.9541409993155373,"Pxτ ∼µ[I{max
a∈A ∣(Pτ[̂Vτ+1] −Pτ[Vτ+1])(xτ,a)∣> 3ε′}] ≤ν.
(97)"
NREG,0.9548254620123203,"Further, for x
∈
Xτ, let ̂πτ(x)
∈
arg maxa∈A⌈̂
Qτ(x,a)/ε′ + ζτ⌉, where
̂
Qτ(x,a)
∶=
̂Pτ,ε′,δ[̂Vτ+1](x,a), and inductively define"
NREG,0.9555099247091033,"˜πτ(x) ∈arg max
a∈A
{ ⌈̂
Qτ(x,a)/ε′ + ζτ⌉,
if ∥̂
Qτ(x,⋅) −Pτ[Vτ+1](x,⋅)∥∞≤4ε′,
⌈Pτ[Vτ+1](x,a)/ε′ + ζτ⌉,
otherwise."
NREG,0.9561943874058864,"Then, we have"
NREG,0.9568788501026694,"Exτ ∼µ[Dtv(̂πτ(xτ), ˜πτ(xτ))] ≤ν + δ."
NREG,0.9575633127994524,"Proof of Lemma L.2.
In this proof, we let Pµ denote the probability law of xτ and
PP denote the probability law of ̂Pτ,ε,δ.
Denote by E be the Pµ-measurable event that
maxa∈A ∣(Pτ[̂Vτ+1] −Pτ[Vτ+1])(xτ,a)∣≤3ε′. Fix x ∈E, and let Ex be the PP-measurable event"
NREG,0.9582477754962354,"that maxa∈A ∣̂Pτ,ε′,δ[̂Vτ+1](x,a) −Pτ[Vτ+1](x,a)∣≤4ε′. From the definition of ˜πτ, we have that"
NREG,0.9589322381930184,"Dtv(̂πτ(x), ˜πτ(x)) = 1"
NREG,0.9596167008898016,"2 ∑
a∈A
∣PP [̂πτ(x) = a] −PP [˜πτ(x) = a]∣, = 1"
NREG,0.9603011635865846,"2 ∑
a∈A
∣PP[Ex]PP [̂πτ(x) = a ∣Ex] + PP[Ec
x]PP [̂πτ(x) = a ∣Ec
x] −PP[Ex]PP [˜πτ(x) = a ∣Ex] −PP[Ec
x]PP [˜πτ(x) = a ∣E ≤1"
NREG,0.9609856262833676,"2 ∑
a∈A
PP[Ex] ⋅∣PP [̂πτ(x) = a ∣Ex] −PP [˜πτ(x) = a ∣Ex]∣"
NREG,0.9616700889801506,"+ ∑
a∈A
PP[Ec
x] ⋅∣PP [̂πτ(x) = a ∣Ec
x] −PP [˜πτ(x) = a ∣Ec
x]∣,
(Jensen’s inequality)"
NREG,0.9623545516769336,"and since PP [̂πτ(x) = a ∣Ex] = PP [˜πτ(x) = a ∣Ex] ∀a ∈A, = 1"
NREG,0.9630390143737166,"2 ∑
a∈A
PP[Ec
x] ⋅∣PP [̂πτ(x) = a ∣Ec
x] −PP [˜πτ(x) = a ∣Ec
x]∣,"
NREG,0.9637234770704997,"≤PP [Ec
x],"
NREG,0.9644079397672827,"= PP [max
a∈A ∣̂Pτ,ε′,δ[̂Vτ+1](x,a) −Pτ[Vτ+1](x,a)∣> 4ε′],"
NREG,0.9650924024640657,"≤PP [max
a∈A ∣̂Pτ,ε′,δ[̂Vτ+1](x,a) −Pτ[̂Vτ+1](x,a)∣> ε′],
(see below)
(98)"
NREG,0.9657768651608487,"≤δ,
(99)"
NREG,0.9664613278576317,"where (98) follows from x ∈E and the last inequality follows from Lemma H.2. Therefore, we have"
NREG,0.9671457905544147,"Exτ ∼µ[Dtv(̂πτ(xτ), ˜πτ(xτ))] ≤Pµ[E] ⋅Exτ ∼µ[Dtv(̂πτ(xτ), ˜πτ(xτ)) ∣E] + Pµ[Ec],
≤δ + ν,"
NREG,0.9678302532511978,"where the first inequality follows by the fact that the total variation is bounded by 1, and the last
inequality follows by (97) and (99)."
NREG,0.9685147159479809,"Lemma L.3. Let x ∈R and ν ∈(0,1/2) be given. Further, let ζ ∈(0,1/2). Then,"
NREG,0.9691991786447639,"x + ζ + ν > ⌈x + ζ⌉
or
x + ζ −ν ≤⌈x + ζ⌉−1,
only if
⌈x⌉−ν ≤x + ζ ≤⌈x⌉+ ν
or
ζ ≤ν."
NREG,0.9698836413415469,"Proof of Lemma L.3. To prove the claim, it suffices to show the following items:"
NREG,0.9705681040383299,1. x + ζ + ν > ⌈x + ζ⌉only if ⌈x⌉≥x + ζ > ⌈x⌉−ν; and
NREG,0.971252566735113,2. x + ζ −ν ≤⌈x + ζ⌉−1 only if ⌈x⌉< x + ζ ≤⌈x⌉+ ν or ζ ≤ν.
NREG,0.971937029431896,"We start by showing the first item. We proceed by showing the contrapositive; that is, we will show
that if x + ζ ≤⌈x⌉−ν or x + ζ > ⌈x⌉, then x + ζ + ν ≤⌈x + ζ⌉. Suppose that x + ζ ≤⌈x⌉−ν. This,
together with the fact that ζ ≥0, implies that"
NREG,0.972621492128679,⌈x + ζ⌉= ⌈x⌉≥x + ζ + ν.
NREG,0.973305954825462,"Now, suppose that x + ζ > ⌈x⌉. Then, we have"
NREG,0.973990417522245,"⌈x + ζ⌉≥⌈x⌉+ 1 ≥⌈x⌉+ ζ + ν ≥x + ζ + ν,"
NREG,0.974674880219028,"where the penultimate inequality follows by ζ,ν ∈(0,1/2)."
NREG,0.9753593429158111,"We now prove the second claim. Again, we proceed by showing the contrapositive; that is, we will
show that if {⌈x⌉+ ν < x + ζ or ⌈x⌉≥x + ζ} and ζ > ν, then x + ζ −ν > ⌈x + ζ⌉−1."
NREG,0.9760438056125941,"Suppose that ⌈x⌉+ ν < x + ζ and ζ > ν. The first inequality together with ν ≥0 implies that
⌈x + ζ⌉> ⌈x⌉. On the other hand, since ζ ≤1/2, we have ⌈x + ζ⌉≤⌈x⌉+ 1, and so"
NREG,0.9767282683093771,"⌈x + ζ⌉−1 = ⌈x⌉< x + ζ −ν,"
NREG,0.9774127310061602,where the last inequality follows by the current assumption that ⌈x⌉+ ν < x + ζ.
NREG,0.9780971937029432,"Now, suppose that ⌈x⌉≥x + ζ and that ζ > ν. Then, we have"
NREG,0.9787816563997263,"⌈x + ζ⌉≤⌈x⌉≤x + 1 < x + ζ −ν + 1,
(100)"
NREG,0.9794661190965093,where the last inequality follows by ζ > ν. Rearranging (100) completes the proof.
NREG,0.9801505817932923,"M
BehaviorCloning Algorithm and Analysis"
NREG,0.9808350444900753,"In this section, we give a self-contained presentation and analysis for the standard behavior cloning
algorithm for imitation learning (e.g., Ross and Bagnell [46]), displayed in Algorithm 10. Given
access to trajectories from an expert policy ̂π1∶H (which may be non-executable in the sense of
Definition 2.1) the algorithm learns an executable policy πbc with similar performance. We use this
scheme within RVFS.bc and RVFSexo.bc."
NREG,0.9815195071868583,Algorithm 10 BehaviorCloning: Imitation Learning Algorithm.
NREG,0.9822039698836413,"1: input: Policy class Π ⊆ΠS, expert policy ̂π1∶H, suboptimality ε ∈(0,1), and confidence
δ ∈(0,1).
2: Set Nbc = 16H2 log(∣Π∣/δ)/ε.
3: Set D ←∅.
4: for i = 1,...,Nbc do
5:
Generate trajectory τ = ((x1,a1),...,(xH,aH)) ∼P̂π.
6:
Update D ←D ∪{τ}.
7: Compute πbc ∈arg minπ∈Π ∑((x1,a1),...,(xH,aH))∈D ∑h∈[H] I{πh(xh) ≠ah}.
8: Return πbc."
NREG,0.9828884325804244,"Proposition M.1. Let ε,δ ∈(0,1) be given and let Π ⊆ΠS and ̂π1∶H be an expert policy such that"
NREG,0.9835728952772074,"inf
π∈Π"
NREG,0.9842573579739904,"H
∑
h=1
P̂π[̂πh(xh) ≠πh(xh)] ≤εmis.
(101)"
NREG,0.9849418206707734,"Then, the policy πbc
1∶H = BehaviorCloning(Π,ε,̂π1∶H,δ) returned by Algorithm 10 satisfies, with
probability at least 1 −δ,"
NREG,0.9856262833675564,J(̂π) −J(πbc) ≤4Hεmis + ε/2.
NREG,0.9863107460643394,"Proof of Proposition M.1. First, by the performance difference lemma, we have"
NREG,0.9869952087611226,"E[V ̂π
1 (x1)] −E[V πbc
1
(x1)] =
H
∑
h=1
Êπ[Qπbc
h (xh, ̂πh(xh)) −Qπbc
h (xh,πbc
h (xh))],"
NREG,0.9876796714579056,"≤H
H
∑
h=1
P̂π[̂πh(xh) ≠πbc
h (xh)].
(102)"
NREG,0.9883641341546886,"We now bound the probability terms on the right-hand side. Fix h ∈[H] and let D be the dataset
in Algorithm 10, which consists of Nbc i.i.d. trajectories ((x1,a1),...,(xH,aH)) generated by
rolling with ̂π1∶H. By Lemma C.4 (with i.i.d. data, B = H, and Q = Π), we have that, with probability
at least 1 −δ,"
NREG,0.9890485968514716,"∀π ∈Π,
∑
((x1,aH),...,(xH,aH))∈D
∑
h∈[H]
I{πh(xh) ≠̂π(xh)} ≤2 ∑
h∈[H]
P̂π[πh(xh) ≠̂πh(xh)]"
NREG,0.9897330595482546,+ 2H log(2∣Π∣/δ)
NREG,0.9904175222450377,"Nbc
,
(103) and"
NREG,0.9911019849418207,"∀π ∈Π,
∑
h∈[H]
P̂π[πh(xh) ≠̂πh(xh)] ≤2
∑
((x1,aH),...,(xH,aH))∈D
∑
h∈[H]
I{πh(xh) ≠̂π(xh)}"
NREG,0.9917864476386037,+ 4H log(2∣Π∣/δ)
NREG,0.9924709103353867,"Nbc
.
(104)"
NREG,0.9931553730321697,"Taking the infimum over π on both sides of (103) and using the definition of πbc
h in Algorithm 10
gives:"
NREG,0.9938398357289527,"∑
((x1,aH),...,(xH,aH))∈D
∑
h∈[H]
I{πbc
h (xh) ≠̂π(xh)} ≤2 inf
π∈Π ∑
h∈[H]
P̂π[πh(xh) ≠̂πh(xh)]"
NREG,0.9945242984257358,"+ 2H log(2∣Π∣/δ) Nbc
,"
NREG,0.9952087611225188,"≤2εmis + 2H log(2∣Π∣/δ) Nbc
,"
NREG,0.9958932238193019,"where the last inequality follows from (101). Using this together with (104), instantiated with π ≡πbc,
we get that with probability at least 1 −δ:"
NREG,0.9965776865160849,"∑
h∈[H]
P̂π[πbc
h (xh) ≠̂πh(xh)] ≤4εmis + 8H log(2∣Π∣/δ) Nbc
."
NREG,0.9972621492128679,"Plugging this into (102), we get that with probability at least 1 −δ:"
NREG,0.997946611909651,"E[V ̂π
1 (x1)] −E[V πbc
1
(x1)] ≤4Hεmis + 8H2 log(2∣Π∣/δ)"
NREG,0.998631074606434,"Nbc
≤4Hεmis + ε/2,"
NREG,0.999315537303217,"where the last inequality follows by the fact that Nbc = 16H2 log(2∣Π∣/δ)/ε. This completes the
proof."
