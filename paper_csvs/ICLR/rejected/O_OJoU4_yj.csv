Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00546448087431694,"Graph neural networks (GNNs) are designed for semi-supervised node classiﬁca-
tion on graphs where only a small subset of nodes have class labels. However,
under extreme cases when very few labels are available (e.g., 1 labeled node per
class), GNNs suffer from severe result quality degradation.
Speciﬁcally, we observe that existing GNNs suffer from unstable training process
on few-labeled graph data, resulting to inferior performance on node classiﬁca-
tion. Therefore, we propose an effective framework, Stabilized self-training with
Negative sampling (SN), which is applicable to existing GNNs to stabilize the
training process and enhance the training data, and consequently, boost classiﬁca-
tion accuracy on graphs with few labeled data. In experiments, we apply our SN
framework to two existing GNN base models (GCN and DAGNN) to get SNGCN
and SNDAGNN, and evaluate the two methods against 13 existing solutions over 4
benchmarking datasets. Extensive experiments show that the proposed SN frame-
work is highly effective compared with existing solutions, especially under set-
tings with very few labeled data. In particular, on a benchmark dataset Cora with
only 1 labeled node per class, while GCN only has 44.6% accuracy, SNGCN
achieves 62.5% accuracy, improving GCN by 17.9%; SNDAGNN has accuracy
66.4%, improving that of the base model DAGNN (59.8%) by 6.6%."
INTRODUCTION,0.01092896174863388,"1
INTRODUCTION"
INTRODUCTION,0.01639344262295082,"Graph is an expressive data model, representing objects and the relationships between objects as
nodes and edges respectively. Graph data are ubiquitous with a wide range of real-world appli-
cations, e.g., social network analysis (Qiu et al., 2018; Li & Goldwasser, 2019), trafﬁc network
prediction (Guo et al., 2019; Li et al., 2019), protein interface prediction (Fout et al., 2017), recom-
mendation systems (Fan et al., 2019; Yang et al., 2020a). Among these applications, an important
task is to classify the nodes in a graph into various classes. However, one tough situation commonly
existing is the lack of sufﬁcient labeled data, which are also expensive to collect."
INTRODUCTION,0.02185792349726776,"To ease the situation, semi-supervised node classiﬁcation on graphs has attracted much attention
from both industry (Qiu et al., 2018; Li & Goldwasser, 2019) and academia (Defferrard et al., 2016;
Hamilton et al., 2017; Velickovic et al., 2018; Liu et al., 2020; Li et al., 2018; Klicpera et al., 2019).
It aims to leverage a small amount of labeled nodes and additionally a large amount of unlabeled
nodes in a graph to train an accurate classiﬁer. There exists a collection of graph neural networks
for semi-supervised node classiﬁcation (Kipf & Welling, 2017; Velickovic et al., 2018; Monti et al.,
2017; Hamilton et al., 2017; Klicpera et al., 2019; Liu et al., 2020). For instance, Graph convolution
networks (GCNs) rely on a message passing scheme called graph convolution that aggregates the
neighborhood information of a node, including node features and graph topology, to learn node
representations, which can then be used in downstream classiﬁcation tasks (Kipf & Welling, 2017)."
INTRODUCTION,0.0273224043715847,"Despite the great success of GCNs, under the extreme cases when very few labels are given (e.g.,
only one labeled node per class), the shallow GCN architecture, typically with two layers (Kipf
& Welling, 2017), cannot effectively propagate the training labels over the input graph, leading to
inferior performance. In particular, as shown in our experiments, on a benchmark dataset Cora with
1 labeled node per class (Cora-1), GCN is even less accurate than some unsupervised methods, such
as DGI (Velickovic et al., 2019) and G2G (Bojchevski & G¨unnemann, 2018). Recently, several latest
studies try to improve classiﬁcation accuracy by designing deeper GNN architectures, e.g., DAGNN"
INTRODUCTION,0.03278688524590164,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03825136612021858,"0
100
200
300
Epoch 0 10 20 30 40"
INTRODUCTION,0.04371584699453552,Percentage(%)
INTRODUCTION,0.04918032786885246,"0
1
2
3
4
5
6"
INTRODUCTION,0.0546448087431694,(a) GCN
INTRODUCTION,0.060109289617486336,"0
100
200
300
Epoch 0 10 20 30 40"
INTRODUCTION,0.06557377049180328,Percentage(%)
INTRODUCTION,0.07103825136612021,(b) DAGNN
INTRODUCTION,0.07650273224043716,"0
100
200
300
Epoch 0 10 20 30 40"
INTRODUCTION,0.08196721311475409,Percentage(%)
INTRODUCTION,0.08743169398907104,(c) SNGCN
INTRODUCTION,0.09289617486338798,"0
100
200
300
Epoch 0 10 20 30 40"
INTRODUCTION,0.09836065573770492,Percentage(%)
INTRODUCTION,0.10382513661202186,(d) SNDAGNN
INTRODUCTION,0.1092896174863388,Figure 1: The distribution of predicted labels in different classes in Cora-1.
INTRODUCTION,0.11475409836065574,"(Liu et al., 2020), which also address the over-smoothing issue identiﬁed in (Xu et al., 2018; Li et al.,
2018; Chen et al., 2020a). However, these deep GNNs are still not directly designed to tackle the
scarcity of labeled data, especially when only very few labels are available."
INTRODUCTION,0.12021857923497267,"After conducting an in-depth study, we have an important ﬁnding that existing GNNs suffer from
unstable training process, when labeled nodes are few. In particular, on Cora dataset with 7 classes,
for each run, we randomly select 1 labeled node per class as the training data for both GCN and
DAGNN, and repeat 100 runs with 300 epochs per run, to get the average number of predicted labels
in percentage per class at each epoch and also the standard deviation. The statistical results of GCN
and DAGNN are shown in Figures 1(a) and 1(b) respectively. x-axis is the epoch from 0 to 300, and
y-axis is the percentage of a class in the predicted node labels. There are 7 colored lines representing
the average percentage of the predicted labels of the respective classes, as the epoch increases. The
dashed lines are the ground-truth percentage of each class in the Cora dataset. The shaded areas
in colors represent the standard deviation. Observe that in Figure 1(a), GCN has high variance at
different runs when predicting node labels, and the variance keeps large at late epochs, e.g., 300,
which indicates that GCN is quite unstable at different runs with 1 training label per class sampled
randomly, leading to inferior classiﬁcation accuracy as illustrated in our experiments. Moreover,
as shown in Figure 1(b), DAGNN also suffers from unstable training process. The variance of
DAGNN is relatively smaller than that of GCN, which provides a hint about why DAGNN performs
better than GCN. Nevertheless, both GCN and DAGNN yield unstable training process with large
variance. Since there is only 1 labeled node per class in Cora-1, at different runs, the randomly
sampled training nodes can heavily inﬂuence the message passing process in GCN and DAGNN,
depending on the connectivity of the training nodes to their neighborhoods over the graph topology,
which result to the unstable training process observed above."
INTRODUCTION,0.12568306010928962,"To address the unstable training process of existing GNNs when only very few labeled data are
available, we propose a framework, Stabilized self-training with Negative sampling (SN), which is
readily applicable to existing GNNs to improve classiﬁcation accuracy via stabilized training pro-
cess. In the proposed SN framework, at each epoch, we select a set of nodes with predicted labels
of high conﬁdence as pseudo labels and add such pseudo labels into training data to enhance the
training of next epoch. To tackle the unstable issue of existing GNNs, we develop a stabilizing
technique in self-training to balance the training. We then design a negative sampling regularization
technique over pseudo labels to further improve node classiﬁcation accuracy. In experiments, we
apply our SN framework to GCN and DAGNN, denoted as SNGCN and SNDAGNN respectively."
INTRODUCTION,0.13114754098360656,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.1366120218579235,"Figures 1(c) and 1(d) report the average percentage and standard variance of the predicted labels
per class per epoch of SNGCN and SNDAGNN on Cora-1 respectively. With our stabilized self-
training technique, obviously, the variance of SNGCN in Figure 1(c) decreases quickly and becomes
stable as epoch increases, compared with Figure 1(a) of GCN. SNDAGNN is also more stable than
DAGNN as shown in Figures 1(d) and 1(b) respectively. As reported later in experiments, with the
proposed SN framework, SNGCN achieves 62.5% node classiﬁcation accuracy on Cora-1, signiﬁ-
cantly improving GCN (44.6%) by 17.9%, and SNDAGNN obtains 66.4% accuracy on Cora-1 and
outperforms DAGNN (59.8%) by a substantial margin of 6.6%. We conduct extensive experiments
on 4 benchmarking datasets, and compare with 13 existing solutions, to evaluate the performance
of the proposed SN framework. Experimental results demonstrate that our SN framework is able to
signiﬁcantly improve classiﬁcation accuracy of existing GNNs when only few labels are available,
and is also effective when training labels are sufﬁcient."
RELATED WORK,0.14207650273224043,"2
RELATED WORK"
RELATED WORK,0.14754098360655737,"In literature, there are two directions to address the scarcity of labeled data for semi-supervised
node classiﬁcation: (i) explore multi-hop graph topological features to propagate the labels in L
over the input graph, e.g., GCN (Kipf & Welling, 2017) and DAGNN (Liu et al., 2020); (ii) enhance
the training data by pseudo labels (self-training) (Li et al., 2018) or augmenting graph data by new
edges and features (Kong et al., 2020; Zhao et al., 2021). Note that these two directions are not
mutually exclusive, but can work together on few-labeled graph data. Here we review the existing
studies that are most relevant to this paper."
RELATED WORK,0.15300546448087432,"GNNs. There exist a large collection of GNNs, such as GCN, DAGNN, GAT, MoNet, and APPNP
(Kipf & Welling, 2017; Liu et al., 2020; Bruna et al., 2014; Henaff et al., 2015; Defferrard et al.,
2016; Velickovic et al., 2018; Monti et al., 2017; Chen et al., 2020b; Klicpera et al., 2019). We
introduce the details of GCN (Kipf & Welling, 2017) and DAGNN (Liu et al., 2020) here. GCN
learns the representation of each node by iteratively aggregating the representations of its neighbors.
Speciﬁcally, GCN consists of k > 0 layers, each with the same propagation rule deﬁned as follows.
At the ℓ-th layer, the representations H(ℓ−1) of previous layer are aggregated to get H(ℓ)."
RELATED WORK,0.15846994535519127,"H(ℓ) = σ( ˆAH(ℓ−1)W(ℓ)), ℓ= 1, 2, ..., k.
(1)"
RELATED WORK,0.16393442622950818,ˆA = ˜D−1
RELATED WORK,0.16939890710382513,2 ˜A ˜D−1
RELATED WORK,0.17486338797814208,"2 is the graph laplacian, where ˜A = A + I is the adjacency matrix of G after
adding self-loops (I is the identity matrix) and ˜D is a diagonal matrix with ˜Dii = P"
RELATED WORK,0.18032786885245902,j ˜Aij. W(ℓ)
RELATED WORK,0.18579234972677597,"is a trainable weight matrix of the ℓ-th layer, and σ is a nonlinear activation function. Initially,
H(0) = X. Note that GCN usually achieves superior performance with 1-layer or 2-layer models
(Kipf & Welling, 2017). When applying multiple layers to leverage large receptive ﬁelds, the per-
formance degrades severely, due to the over-smoothing issue identiﬁed in (Xu et al., 2018; Li et al.,
2018; Chen et al., 2020a). A recent deep GNN architecture, DAGNN, tackles the over-smoothing
issue and achieves state-of-the-art results by decoupling representation transformation and propaga-
tion in GNNs (Liu et al., 2020). Then it utilizes an adaptive adjustment mechanism to balance the
information from local and global neighborhoods of each node. Speciﬁcally, the mathematical ex-
pression of DAGNN is as follows. DAGNN uses a learnable parameter s ∈Rc×1 to adjust the weight
of embeddings at different propagation level (from 1 to k). It processes data in the following way.
Z = MLP(X) ∈Rn×c, Hℓ= ˆAℓ· Z ∈Rn×c, ℓ= 1, 2, ..., k , Sℓ= Hℓ· s ∈Rn×1, ℓ= 1, 2, ..., k,
ˆSℓ= [Sℓ, Sℓ, ..., Sℓ] ∈Rn×c, ℓ= 1, 2, ..., k, Xout = softmax(Pk
ℓ=1 Hℓ⊙ˆSℓ), where ˆAℓis the ℓ-th
power of matrix ˆA, ⊙is the Hadamard product, · is dot product, MLP is the Multilayer Perceptron
and softmax operation is on the second dimension."
RELATED WORK,0.1912568306010929,"Data Augmentation. Another way to address the situation of limited labeled data is to add pseudo
labels to training dataset by self-training (Li et al., 2018), or enhance the graph data by adding new
edges and features (Zhao et al., 2021; Kong et al., 2020). Self-training itself is a general methodology
(Scudder, 1965) and is used in various domains in addition. It is used in word-sense disambiguation
(Yarowsky, 1995; Hearst, 1991), bootstrap for information extraction and learning subjective nouns
(Riloff & Jones, 1999), and text classiﬁcation (Nigam et al., 2000). In (Zhou et al., 2012), it suggests
that selecting informative unlabeled data using a guided search algorithm can signiﬁcantly improve
performance over standard self-training framework. Buchnik & Cohen (2018) mainly consider self-
training for diffusion-based techniques. Recently, self-training has been adopted for semi-supervised"
RELATED WORK,0.19672131147540983,Under review as a conference paper at ICLR 2022
RELATED WORK,0.20218579234972678,"tasks on graphs. For instance, Li et al. (2018) propose self-training and co-training techniques for
GCN. This self-training work selects the top-k conﬁdent predicted labels as pseudo labels. The
co-training technique co-trains a GCN with a random walk model to handle few-labeled data. Com-
pared with existing self-training work, our framework are different as shown later. In particular, our
framework has a different strategy to select pseudo labels and also has a stabilizer to address the
deﬁciencies of existing GNNs; moreover, we propose a negative sampling regularization technique
to further boost accuracy. Besides, in existing work, if a node is selected as a pseudo label, it will
never be moved out even if the pseudo label becomes obviously wrong in later epochs. On the other
hand, in our framework, we update pseudo labels in each epoch to avoid such an issue. There also
exist studies to augment the original graph data, which is different from self-training. For instance,
Zhao et al. (2021) utilize link prediction to promote intra-class edges and demote inter-class edges in
a given graph. Kong et al. (2020) iteratively augment node features with gradient-based adversarial
perturbations to enhance the performance of GNNs."
THE FRAMEWORK,0.20765027322404372,"3
THE FRAMEWORK"
PROBLEM FORMULATION,0.21311475409836064,"3.1
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.2185792349726776,"Let G = (V, E, X) be a graph consisting of a node set V with cardinality n, a set of edges E of
size m, each connecting two nodes in V, a feature matrix X ∈Rn×d, where d is the number of
features in G. For every node vi ∈V, it has a feature vector Xi ∈Rd, where Xi is the i-th row of
X. Let c be the number of classes in G. We use L to denote the set of labeled nodes, and obviously
L ⊆V. Let U be the set of unlabeled nodes and U = V \L. Each labeled node vi ∈L has a one-hot
vector Yi ∈{0, 1}c, indicating the class label of vi. Under the few-labeled setting, |L| ≪|U|. A
high-level deﬁnition of the semi-supervised node classiﬁcation problem is as follows."
PROBLEM FORMULATION,0.22404371584699453,"Deﬁnition 1. Given a graph G = (V, E, X), a set of labeled nodes L ⊆V, and a groundtruth
class label Yi ∈{0, 1}c per node vi ∈L, assuming that each node belongs to exactly one class,
Semi-Supervised Node Classiﬁcation predicts the labels of the unlabeled nodes."
PROBLEM FORMULATION,0.22950819672131148,"In particular, the aim is to leverage the graph G with the labeled nodes in L, and to train a forward
predicting classiﬁcation model/function f(G, θ) that takes as input the graph G and a set of trainable
parameters θ. The output of f is a matrix F ∈Rn×c, with each i-th row Fi ∈[0, 1]c representing
the output probability vector of node vi ∈V (the 1-norm of Fi is normalized to 1)."
PROBLEM FORMULATION,0.23497267759562843,"We adopt the widely used cross-entropy loss. For a node vi, its loss of Fi with respect to its true
class label Yi, L(Yi, Fi), is deﬁned as follows."
PROBLEM FORMULATION,0.24043715846994534,"L(Yi, Fi) = − c
X"
PROBLEM FORMULATION,0.2459016393442623,"j=1
Yi,j ln(Fi,j)"
PROBLEM FORMULATION,0.25136612021857924,"where Yi,j is the j-th value in Yi and Fi,j is the j-th value in Fi."
STABILIZED SELF-TRAINING,0.2568306010928962,"3.2
STABILIZED SELF-TRAINING"
STABILIZED SELF-TRAINING,0.26229508196721313,"Recall that existing GNNs suffer from unstable training process as shown in Figure 1. In this section,
we present the stabilized self-training technique that not only augments training data with pseudo
labels but also stabilizes the training process. We ﬁrst explain how to choose pseudo labels and then
introduce loss function of stabilized self-training."
STABILIZED SELF-TRAINING,0.2677595628415301,"At a certain epoch, given the matrix F ∈Rn×c, with each i-th row Fi ∈[0, 1]c representing the
output probability vector of node vi ∈V. Let ˜Yi,j satisfying the following Eq. (2) be the predicted
label of node vi. We say that, with conﬁdence Fi,j, node vi has class label Cj; i.e., the largest
element Fi,j in vector Fi is called the conﬁdence of node vi."
STABILIZED SELF-TRAINING,0.273224043715847,"˜Yi,j =
 1
if j = arg maxj′ Fi,j′,
0
otherwise,
(2)"
STABILIZED SELF-TRAINING,0.2786885245901639,"Then for every unlabeled node vi ∈U, we can get Ni, the number of nodes with the same predicted
label as vi, in Eq. (3). Recall that in Figure 1, in existing GCN and DAGNN, the distribution of"
STABILIZED SELF-TRAINING,0.28415300546448086,Under review as a conference paper at ICLR 2022
STABILIZED SELF-TRAINING,0.2896174863387978,"the predicted labels is unstable with large variance during the training process; we also observe that
most predicted labels are in the same class, especially at the early epochs. To reduce such unstable
situation of existing GNNs, we use Ni as a stabilizer in our framework to be explained shortly."
STABILIZED SELF-TRAINING,0.29508196721311475,"Ni =

n
vj ∈U
 ˜Yi = ˜Yj
o
(3)"
STABILIZED SELF-TRAINING,0.3005464480874317,"For every unlabeled nodes vi in U, it can have a predicted label.
However, the conﬁdence
arg maxj′ Fi,j′ might be low. We do not want to add such low-conﬁdence labels into the training
of the next epoch. Therefore, we only choose those unlabeled nodes with high-conﬁdence predicted
labels as pseudo labels to be augmented into the training data of next epoch. In particular, an unla-
beled node vi is selected to be a node with pseudo label in next epoch, if its conﬁdence satisﬁes a
threshold β, as shown below. We use U′ to denote all unlabeled nodes selected with pseudo labels."
STABILIZED SELF-TRAINING,0.30601092896174864,"U′ =
n
vi ∈U
 max
j
Fi,j > β
o
(4)"
STABILIZED SELF-TRAINING,0.3114754098360656,"where β ∈[0, 1] is a threshold controlling the extent of cautious selection for self-training. A bigger
threshold means stricter selection of the pseudo labels."
STABILIZED SELF-TRAINING,0.31693989071038253,"After explaining how to choose pseudo labels above, we present the loss of our stabilized self-
training technique in Eq. (5). In particular, we design
1
Ni+1 as the stabilizer of the training process,
to overcome the deﬁciencies of existing GNNs illustrated in Figure 1. The intuition is that, if an
unlabeled node vi is in a pseudo label class with many nodes, its importance in the loss function is
reduced. In other words, our stabilized self-training loss reduces the impact of classes with many
pseudo labeled nodes, which is especially useful to rectify the training process when the predictions
in the early epochs are incorrect or less conﬁdent, compared with ground truth."
STABILIZED SELF-TRAINING,0.3224043715846995,"Lsst =
X"
STABILIZED SELF-TRAINING,0.32786885245901637,∀vi∈U′
STABILIZED SELF-TRAINING,0.3333333333333333,"1
Ni + 1 · L( ˜Yi, Fi)
(5)"
STABILIZED SELF-TRAINING,0.33879781420765026,"Compared with existing self-training techniques (Li et al., 2018), our stabilized self-training tech-
nique has major differences. First, we develop the stabilizer to re-weight the importance of pseudo
labels in the loss function, so as to address the unstable issue of existing GNNs. Second, we select
only those nodes with high-conﬁdence pseudo labels satisfying β threshold, and adaptively update
the pseudo labels per epoch, meaning that a pseudo label in previous epoch will be removed in the
next epoch if its conﬁdence becomes low. On the other hand, existing methods keep a pseudo label
once it is selected and never remove it in later epochs (Li et al., 2018), which may harm the training
quality if the pseudo label is wrong compared with ground truth."
NEGATIVE SAMPLING REGULARIZATION,0.3442622950819672,"3.3
NEGATIVE SAMPLING REGULARIZATION"
NEGATIVE SAMPLING REGULARIZATION,0.34972677595628415,"Under extreme cases with very few labeled nodes (e.g., 1 labeled node per class), we further design
a negative sampling regularization technique for better performance. In existing studies, negative
sampling is used as an unsupervised technique over node embeddings in network embedding meth-
ods (Yang et al., 2020b; Velickovic et al., 2019; Yang et al., 2020b). Here we customize it to the
semi-supervised node classiﬁcation task, and apply negative sampling over labels instead of embed-
dings."
NEGATIVE SAMPLING REGULARIZATION,0.3551912568306011,"Intuitively, the label of a node v should be distant to the label of another node u if these two nodes
are faraway on the input graph G. Speciﬁcally, a positive sample is a node vi in L or U′. We sample
a set I of positive samples from L ∪U′ uniformly at random. The negative samples of a positive
sample vi are the nodes that are not directly connected to vi in graph G. For each positive sample vi
in I, we sample a ﬁxed-size set Ji of negative samples uniformly at random."
NEGATIVE SAMPLING REGULARIZATION,0.36065573770491804,"For a positive-negative pair (vi, vj), compared with the ˜Yi of vi ∈L ∪U′, the intention is to let
the output vector Fj of vj to be as different as possible. Here we use the symbol ˜Yi to represent
the pseudo label ˜Yi for node vi in U′ or ground-truth label Yi of node vi in L to avoid ambiguity.
Denote 1 as the all-one vector in Rc. Then we have the following loss of all positive-negative pairs."
NEGATIVE SAMPLING REGULARIZATION,0.366120218579235,"Lneg =
X ∀vi∈I X"
NEGATIVE SAMPLING REGULARIZATION,0.37158469945355194,∀vj∈Ji
NEGATIVE SAMPLING REGULARIZATION,0.3770491803278688,"1
|I| · |Ji|L( ˜Yi, 1 −Fj)
(6)"
NEGATIVE SAMPLING REGULARIZATION,0.3825136612021858,Under review as a conference paper at ICLR 2022
NEGATIVE SAMPLING REGULARIZATION,0.3879781420765027,Algorithm 1: SN Framework Over GNNs
NEGATIVE SAMPLING REGULARIZATION,0.39344262295081966,"1 Input: Graph G = (V, E, X) with labeled node set L and unlabeled node set U"
NEGATIVE SAMPLING REGULARIZATION,0.3989071038251366,"2 Output: the learned classiﬁer f(·, θ)."
NEGATIVE SAMPLING REGULARIZATION,0.40437158469945356,"3 Generate initial parameter θ for model f(·, ·)."
NEGATIVE SAMPLING REGULARIZATION,0.4098360655737705,"4 for each epoch t = 0, 1, 2, ..., T do"
NEGATIVE SAMPLING REGULARIZATION,0.41530054644808745,"5
Use GNN to compute prediction F ←f(G, θ)"
NEGATIVE SAMPLING REGULARIZATION,0.4207650273224044,"6
Get high conﬁdence set U′ and its stabilizing factor
1
Ni+1 per node vi(Section 3.2)"
NEGATIVE SAMPLING REGULARIZATION,0.4262295081967213,"7
Get positive samples and corresponding negative samples using L ∪U′ and G (Section 3.3)"
NEGATIVE SAMPLING REGULARIZATION,0.43169398907103823,"8
Get Ltotal of current epoch by Eq. (7) (Section 3.4)"
NEGATIVE SAMPLING REGULARIZATION,0.4371584699453552,"9
Update model parameters by θ ←Adam Optimizer(θ, gradient = ∇θLtotal)."
IF CONVERGENCE THEN,0.4426229508196721,"10
if Convergence then"
BREAK,0.44808743169398907,"11
Break"
END,0.453551912568306,"12
end"
END,0.45901639344262296,13 end
OBJECTIVE FUNCTION AND ALGORITHM,0.4644808743169399,"3.4
OBJECTIVE FUNCTION AND ALGORITHM"
OBJECTIVE FUNCTION AND ALGORITHM,0.46994535519125685,"Our ﬁnal loss function is as follows, and it combines the stabilized self-training loss and negative
sampling loss in Eq. (5) and Eq. (6) respectively."
OBJECTIVE FUNCTION AND ALGORITHM,0.47540983606557374,Ltotal = 1
OBJECTIVE FUNCTION AND ALGORITHM,0.4808743169398907,"|L| ·
X"
OBJECTIVE FUNCTION AND ALGORITHM,0.48633879781420764,"∀vi∈L
L(Yi, Fi) + λ1Lsst + λ2Lneg,
(7)"
OBJECTIVE FUNCTION AND ALGORITHM,0.4918032786885246,where λ1 and λ2 are factors controlling the impact of these two losses.
OBJECTIVE FUNCTION AND ALGORITHM,0.4972677595628415,"Algorithm 1 shows the pseudo-code of our SN framework over GNNs, and it takes as input a graph
G with labeled nodes L and unlabeled nodes U. Note that SN can be instantiated over either a
shallow or a deep GNN, e.g., GCN and DAGNN introduced in Section 2. The output of Algorithm 1
is the learned classiﬁcation model f with trainable parameters θ. At Line 3, SN initializes the
trainable parameters θ by Xavier (Glorot & Bengio, 2010). Then from Lines 4 to 12, SN trains the
classiﬁcation model per epoch t iteratively, until convergence or the max number T of iterations is
reached. Speciﬁcally, at Line 5, SN ﬁrst use a GNN to obtain the forward prediction output F. Then
at Line 6, SN detects the pseudo-labeled set U′ and obtain the stabilizer
1
Ni+1 of each node vi in
U′, after which, at Line 7 we perform negative sampling to obtain positive samples I and negative
samples Ji. At Line 8, SN computes loss Ltotal of current epoch according to Eq. (7). And at Line
9, SN updates model parameters θ for next epoch by Adam optimizer (Kingma & Ba, 2015)."
EXPERIMENTS,0.5027322404371585,"4
EXPERIMENTS"
EXPERIMENTS,0.5081967213114754,"We evaluate SN against 13 competitors for semi-supervised node classiﬁcation on 4 benchmark
graph datasets. All experiments are conducted on a machine powered by an Intel(R) Xeon(R) E5-
2603 v4 @ 1.70GHz CPU, 131GB RAM, 16.04.1-Ubuntu, and 4 Nvidia Geforce 1080ti Cards with
Cuda version 10.2. Source codes of all competitors are obtained from the respective authors. Our
SN framework is implemented in Python, using libraries including PyTorch (Paszke et al., 2019) and
PyTorch Geometric (Fey & Lenssen, 2019). An anonymous link1 of our source code is provided."
DATASETS AND COMPETITORS,0.5136612021857924,"4.1
DATASETS AND COMPETITORS"
DATASETS AND COMPETITORS,0.5191256830601093,"Datasets. Table 1 shows the statistics of the 4 real-world graphs used in our experiments. We list
the number of nodes, edges, features and classes in each graph dataset respectively. Speciﬁcally, the
4 datasets are Cora (Sen et al., 2008), Citeseer (Sen et al., 2008), Pubmed (Sen et al., 2008), and
Core-full (Bojchevski & G¨unnemann, 2018), all of which are widely used for benchmarking node
classiﬁcation performance in existing studies (Sun et al., 2020; Li et al., 2018; Liu et al., 2020).
Notice that every node in these graphs has a ground-truth class label."
DATASETS AND COMPETITORS,0.5245901639344263,1https://anonymous.4open.science/r/e7aca211-0d8d-4564-8f3f-0ef24b01941e/
DATASETS AND COMPETITORS,0.5300546448087432,Under review as a conference paper at ICLR 2022
DATASETS AND COMPETITORS,0.5355191256830601,Table 1: Datasets
DATASETS AND COMPETITORS,0.5409836065573771,"Cora
Citeseer
Pubmed
Cora-full"
DATASETS AND COMPETITORS,0.546448087431694,"# of Nodes
2708
3327
19717
19793
# of Edges
5429
4732
44338
65311
# of Features
1433
3703
500
8710
# of Classes
7
6
3
67"
DATASETS AND COMPETITORS,0.5519125683060109,"Competitors. We compare with 13 existing solutions, including LP (Label Propagation) (Wu et al.,
2012), DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015), G2G (Bojchevski & G¨unnemann,
2018), DGI (Velickovic et al., 2019), GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018),
MoNet (Monti et al., 2017), APPNP (Klicpera et al., 2019), DAGNN (Liu et al., 2020), STs (Li
et al., 2018), LCGCN and LCGAT in (Xu et al., 2020). In particular, GCN, GAT, MoNet, APPNP,
and DAGNN are GNNs. DeepWalk, DGI, LINE, and G2G are unsupervised network embedding
methods. STs represents the four variants in (Li et al., 2018), including Self-Training, Co-Training,
Union, and Intersection; we summarize the best results among them as the results of STs."
EXPERIMENTAL SETTINGS,0.5573770491803278,"4.2
EXPERIMENTAL SETTINGS"
EXPERIMENTAL SETTINGS,0.5628415300546448,"We evaluate our framework and the competitors on semi-supervised node classiﬁcation tasks with
various settings. In particular, for each graph dataset, we repeat experiments on 100 random data
splits as suggested in (Liu et al., 2020; Li et al., 2018) and report the average performance. For
each graph dataset, we vary the number of labeled nodes per class in {1, 3, 5, 10, 20}, where 1, 3, 5
represent the very few-labeled settings. Following convention in existing work (Liu et al., 2020),
we explain what a random data split is, as follows. For example, when the number of labeled nodes
per class on Cora is 3 (denoted as Cora-3), since Cora has 7 classes, we randomly pick 3 nodes per
class, combining together as a training set of size 21 (i.e., the labeled node set L), and then, among
the remaining nodes, we randomly select 500 nodes as a validation set, and 1000 nodes as a test set.
Each data split consists of a training set, a validation set, and a test set as mentioned above. We use
the classiﬁcation accuracy on test set as evaluation metric. Speciﬁcally, accuracy is deﬁned as the
fraction of the testing nodes whose class labels are correctly predicted by the learned classiﬁer."
IMPLEMENTATION DETAILS,0.5683060109289617,"4.3
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.5737704918032787,"We instantiate SN framework over the classic GCN model with 2 layers and a recent deep GNN
architecture DAGNN to demonstrate the effectiveness and applicability of SN. The instantiation of
SN over GCN and DAGNN are dubbed as SNGCN and SNDAGNN respectively. SNGCN and
SNDAGNN has parameters (i) inherited from GCN and DAGNN and (ii) developed in SN. Hence,
we ﬁrst tune the best parameters of the base models under each classiﬁcation task setting on each
dataset and report this result for them for a fair comparison."
IMPLEMENTATION DETAILS,0.5792349726775956,"Base models (GCN and DAGNN). In base models, we tune the parameters: a L2 regularization
rate with search space in {1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 0}, a dropout rate in {0.5, 0.8}. For
DAGNN, the level k of propagation after MLP is searched in {10, 15, 20}. We have the follow-
ing parameters for base models in experiments: the number of hidden units of GCN and MLP (in
DAGNN) is 64 units without bias; the number of layers of GCN and MLP (in DAGNN) is 2 layers;
the learning rate of Adam Optimizer is 0.01; the activation function is RELU; the maximum number
of training epochs is 1000. Moreover, early stopping is triggered when the validation loss is smaller
than the average validation loss of previous 100 epochs, and the current epoch is beyond 500 epochs."
IMPLEMENTATION DETAILS,0.5846994535519126,"SN over GCN and DAGNN (SNGCN and SNDAGNN). After ﬁnding the best hyper parameters
of the base models, we then tune the parameters in SN. λ1 is searched in {0.1, 1} and λ2 is searched
in {0, 0.1, 1}. Stabilizing enabler searches in {True, False}. The number of positive and negative
samples (|I|, |Ji|) is searched in {(1, 10), (2, 5), (5, 2), (10, 1)}. For instance, (2, 5) means that we
sample 2 positive nodes and then for each positive node, we sample 5 negative nodes."
IMPLEMENTATION DETAILS,0.5901639344262295,"Competitors. We use the parameters suggested in the original papers of the competitors to tune
their models, and report the best results of the competitors. Notice that for unsupervised network
embedding methods, including DeepWalk, DGI, LINE, and G2G, after obtaining the embeddings,
we use logistic regression to train a node classiﬁer over the embedding (Velickovic et al., 2019)."
IMPLEMENTATION DETAILS,0.5956284153005464,Under review as a conference paper at ICLR 2022
IMPLEMENTATION DETAILS,0.6010928961748634,"Table 2: Accuracy results (in percentage) on Cora and CiteSeer respectively, averaged over
100 random data splits. (The best accuracy is in bold.)"
IMPLEMENTATION DETAILS,0.6065573770491803,"# of Labels
Cora
CiteSeer
per class
1
3
5
10
20
1
3
5
10
20"
IMPLEMENTATION DETAILS,0.6120218579234973,"GCN
44.6 63.8 71.3 77.2 81.4 40.4 53.5 61.0 65.8 69.5
SNGCN (ours)
62.5 72.8 75.8 80.7 82.5 56.2 66.4 68.0 70.2 72.1"
IMPLEMENTATION DETAILS,0.6174863387978142,"DAGNN
59.8 72.4 76.7 80.8 83.7 46.5 58.8 63.6 67.9 71.2
SNDAGNN (ours) 66.4 77.6 79.8 82.2 84.1 48.5 65.9 67.9 69.8 72.1"
IMPLEMENTATION DETAILS,0.6229508196721312,"LP
51.5 60.5 62.5 64.2 67.3 30.1 37.0 39.3 41.9 44.8
DeepWalk
40.4 53.8 59.4 65.4 69.9 28.3 34.7 38.1 42.0 45.6
LINE
49.4 62.6 63.4 71.1 74.0 28.0 34.7 38.0 43.1 48.5
G2G
54.5 68.1 70.9 73.8 75.8 45.1 56.4 60.3 63.1 65.7
DGI
55.3 70.9 72.6 76.4 77.9 46.1 59.2 64.1 67.6 68.7
STs
53.1 67.3 72.5 76.2 79.8 37.2 51.8 60.7 67.4 70.2
GAT
41.8 61.7 71.1 76.0 79.6 32.8 48.6 54.9 60.8 68.2
MoNet
43.4 61.2 70.9 76.1 79.3 38.8 52.9 59.7 64.6 66.9
APPNP
44.7 66.3 74.1 79.0 81.9 34.6 52.2 59.4 66.0 71.8
LCGCN
63.6 74.4 77.5 80.4 82.4 55.3 59.0 68.4 70.3 72.1
LCGAT
58.7 74.5 77.5 79.7 82.6 50.9 66.3 68.5 70.9 71.5"
OVERALL RESULTS,0.6284153005464481,"4.4
OVERALL RESULTS"
OVERALL RESULTS,0.6338797814207651,"Table 2 reports the classiﬁcation accuracy (in percentage) of all methods on Cora and CiteSeer,
when varying the number of labeled nodes per class in {1,3,5,10,20}. The ﬁrst and second rows
report the performance of GCN and SNGCN. The third and fourth rows report the performance of
DAGNN and SNDAGNN. Observe that SNGCN (resp. SNDAGNN) enhanced by our SN frame-
work signiﬁcantly outperforms GCN (resp. DAGNN) under all task settings, and the performance
gain of SNGCN is especially signiﬁcant when the labels per class are few. For instance, on Cora-1,
SNGCN has accuracy 62.5%, while the accuracy of GCN is 44.6%, indicating that the proposed
framework improves GCN by 17.9%. This demonstrates the power of the proposed SN framework
to boost classiﬁcation performance. Compared with other competitors, observe that SNDAGNN has
the best performance under all settings of Cora (in bold), and SNGCN has the best performance on
CiteSeer-1 and 3 and 20, while achieving similar performance compared with LCGCN and LCGAT
on CiteSeer-5 and 10. Apart from the superior performance of our SN framework over GCN and
DAGNN, in Table 2, we can observe two interesting ﬁndings. First, under extremely-few-labels
settings (e.g., Cora-1), unsupervised methods G2G (54.5%) and DGI (55.3%) achieve better perfor-
mance than GCN (44.6%). One reason is that the unsupervised methods are good at cases when no
labeled data are available, while GCNs still require a sufﬁcient amount of labeled data. This ﬁnding
demonstrates the intuition of the negative sampling regularization in Section 3.3, and also sheds
light on possible future research to use unsupervised techniques to further enhance the performance
of semi-supervised learning. Second, the performance gap between our methods and competitors
enlarges as the number of labels per class decreases, which further illustrates the effectiveness of the
proposed SN framework under extreme settings on graphs with very few-labeled nodes per class."
OVERALL RESULTS,0.639344262295082,"Table 3 presents the classiﬁcation accuracy of all methods under all settings in {1,3,5,10,20} on
PubMed and Cora-full datasets. We exclude from this table, the inaccurate competitors (e.g., Deep-
Walk and LINE) that are obviously outperformed by other competitors. Observe that on PubMed and
Cora-full, SNDAGNN and SNGCN achieve the higher accuracy consistently than their respective
base models GCN and DAGNN. For instance, on Cora-full-1, SNGCN achieves 30.8% accuracy,
6.3% better than GCN. Moreover, observe that on PubMed, SNDAGNN consistently outperforms
all other competitors, e.g., LCGCN; on Cora-full, SNGCN outperforms all competitors on 1 and 3
settings, and SNDAGNN has the best accuracy on 5, 10, and 20 settings."
OVERALL RESULTS,0.644808743169399,"In summary, the experimental results presented in Tables 2 and 3 validate the effectiveness of the
proposed SN framework over GCN and DAGNN to boost classiﬁcation performance, especially
when only very few labeled nodes are available."
OVERALL RESULTS,0.6502732240437158,Under review as a conference paper at ICLR 2022
OVERALL RESULTS,0.6557377049180327,"Table 3: Accuracy results (in percentage) on PubMed and Cora-full respectively, averaged
over 100 random data splits. ( The best accuracy is in bold.)"
OVERALL RESULTS,0.6612021857923497,"# of Labels
PubMed
Cora-full
per class
1
3
5
10
20
1
3
5
10
20"
OVERALL RESULTS,0.6666666666666666,"GCN
55.5 66.0 70.4 74.6 78.7 24.5 41.4 48.1 55.8 60.2
SNGCN (ours)
60.8 67.8 71.6 76.1 79.4 30.8 44.9 49.4 56.6 60.9"
OVERALL RESULTS,0.6721311475409836,"DAGNN
59.4 69.5 72.0 76.8 80.1 27.3 43.2 49.8 55.8 60.4
SNDAGNN (ours) 61.0 72.1 74.9 78.2 80.6 27.6 44.4 51.1 56.8 61.2"
OVERALL RESULTS,0.6775956284153005,"LP
55.7 61.9 63.5 65.2 66.4 26.3 32.4 35.1 38.0 41.0
G2G
55.2 64.5 67.4 72.0 74.3 25.8 36.4 43.3 49.3 54.3
DGI
55.1 63.4 65.3 71.8 73.9 26.2 37.9 46.5 55.3 59.8
STs
55.1 65.4 69.7 74.0 78.5 29.2 43.6 48.9 53.4 60.8
APPNP
54.8 66.9 70.8 76.0 79.4 24.3 41.5 48.5 55.3 60.1
GAT
52.7 64.4 69.4 73.7 73.5 24.8 41.0 47.5 54.7 59.9
LCGCN
56.6 69.2 72.6 74.6 80.0 26.7 43.9 49.2 55.9 60.5
LCGAT
49.5 59.2 62.3 70.2 65.3 27.4 43.2 48.4 55.0 60.1"
OVERALL RESULTS,0.6830601092896175,"baseline
baseline+S
baseline+N
baseline+SN"
OVERALL RESULTS,0.6885245901639344,"1
3
5
10
20 50 60 70 80"
OVERALL RESULTS,0.6939890710382514,Number of labels per class
OVERALL RESULTS,0.6994535519125683,Accuracy(%)
OVERALL RESULTS,0.7049180327868853,(a) ablation on GCN
OVERALL RESULTS,0.7103825136612022,"1
3
5
10
20
60 70 80"
OVERALL RESULTS,0.7158469945355191,Number of labels per class
OVERALL RESULTS,0.7213114754098361,Accuracy(%)
OVERALL RESULTS,0.726775956284153,"(b) ablation on DAGNN
Figure 2: Ablation study of SN on Cora."
ABLATION STUDY,0.73224043715847,"4.5
ABLATION STUDY"
ABLATION STUDY,0.7377049180327869,"We conduct ablation study to evaluate the contributions of the techniques of SN presented in Sec-
tion 3. Denote baseline+SN as the method with the whole SN framework enabled, baseline+S as
the method with only stabilized self-training loss in Eq. (5) enabled, and baseline+N as the method
with only negative sampling loss in Eq. (6) enabled. Figures 2a and 2b report the ablation results on
baselines GCN and DAGNN respectively, on Cora when varying the number of labels per class in
{1, 3, 5, 10, 20}. Observe that the accuracies of baseline+S and baseline+N are always better than
the baseline model, i.e., GCN and DAGNN. Also the accuracy of baseline+SN is almost always the
highest under all settings. The ablation study demonstrates the power of our proposed techniques to
improve classiﬁcation accuracy."
CONCLUSION,0.7431693989071039,"5
CONCLUSION"
CONCLUSION,0.7486338797814208,"This paper presents Stabilized self-training with Negative sampling (SN), an effective framework for
semi-supervised node classiﬁcation on few-labeled graph data. SN achieves superior performance
on graphs with extremely few labeled nodes, through two main designs: a stabilized self-training
technique that adaptively selects and re-weights high-conﬁdence pseudo labels based on the conﬁ-
dence distribution of current epoch to enhance the training process, and a negative sampling regu-
larization that further fully utilizes unlabeled data to train a high-quality classiﬁer. The effectiveness
of SN is extensively evaluated on 4 real graphs, compared against 13 existing solutions. Regard-
ing future work, we plan to enhance SN by investigating other unsupervised techniques, and also
implement SN on top of more GNN architectures to further demonstrate its applicability."
CONCLUSION,0.7540983606557377,Under review as a conference paper at ICLR 2022
REFERENCES,0.7595628415300546,REFERENCES
REFERENCES,0.7650273224043715,"Aleksandar Bojchevski and Stephan G¨unnemann. Deep gaussian embedding of graphs: Unsuper-
vised inductive learning via ranking. In ICLR, 2018."
REFERENCES,0.7704918032786885,"Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In ICLR, 2014."
REFERENCES,0.7759562841530054,"Eliav Buchnik and Edith Cohen. Bootstrapped graph diffusions: Exposing the power of nonlinear-
ity. In Abstracts of the 2018 ACM International Conference on Measurement and Modeling of
Computer Systems, pp. 8–10, 2018."
REFERENCES,0.7814207650273224,"Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-
smoothing problem for graph neural networks from the topological view. In AAAI, pp. 3438–3445,
2020a."
REFERENCES,0.7868852459016393,"Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In ICML, pp. 1725–1735, 2020b."
REFERENCES,0.7923497267759563,"Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral ﬁltering. In NeurIPS, pp. 3837–3845, 2016."
REFERENCES,0.7978142076502732,"Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph
neural networks for social recommendation. In WWW, pp. 417–426, 2019."
REFERENCES,0.8032786885245902,"Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
arXiv preprint arXiv:1903.02428, 2019."
REFERENCES,0.8087431693989071,"Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In NeurIPS, pp. 6530–6539, 2017."
REFERENCES,0.8142076502732241,"Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In AISTATS, pp. 249–256, 2010."
REFERENCES,0.819672131147541,"Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. Attention based spatial-
temporal graph convolutional networks for trafﬁc ﬂow forecasting. In AAAI, pp. 922–929, 2019."
REFERENCES,0.825136612021858,"William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NeurIPS, pp. 1024–1034, 2017."
REFERENCES,0.8306010928961749,"Marti Hearst. Noun homograph disambiguation using local context in large text corpora. Using
Corpora, pp. 185–188, 1991."
REFERENCES,0.8360655737704918,"Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015."
REFERENCES,0.8415300546448088,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."
REFERENCES,0.8469945355191257,"Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. In ICLR, 2017."
REFERENCES,0.8524590163934426,"Johannes Klicpera, Aleksandar Bojchevski, and Stephan G¨unnemann.
Predict then propagate:
Graph neural networks meet personalized pagerank. In ICLR, 2019."
REFERENCES,0.8579234972677595,"Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor,
and Tom Goldstein. FLAG: adversarial data augmentation for graph neural networks. CoRR,
2020."
REFERENCES,0.8633879781420765,"Chang Li and Dan Goldwasser. Encoding social information with graph convolutional networks
forpolitical perspective detection in news media. In ACL, pp. 2594–2604, 2019."
REFERENCES,0.8688524590163934,"Jia Li, Zhichao Han, Hong Cheng, Jiao Su, Pengyun Wang, Jianfeng Zhang, and Lujia Pan. Predict-
ing path failure in time-evolving graphs. In SIGKDD, pp. 1279–1289, 2019."
REFERENCES,0.8743169398907104,"Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In AAAI, pp. 3538–3545, 2018."
REFERENCES,0.8797814207650273,Under review as a conference paper at ICLR 2022
REFERENCES,0.8852459016393442,"Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD,
pp. 338–348, 2020."
REFERENCES,0.8907103825136612,"Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodol`a, Jan Svoboda, and Michael M.
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR,
pp. 5425–5434, 2017."
REFERENCES,0.8961748633879781,"Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom M. Mitchell. Text classiﬁcation from
labeled and unlabeled documents using EM. Mach. Learn., pp. 103–134, 2000."
REFERENCES,0.9016393442622951,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In NeurIPS, pp. 8024–8035, 2019."
REFERENCES,0.907103825136612,"Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representa-
tions. In SIGKDD, pp. 701–710, 2014."
REFERENCES,0.912568306010929,"Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf: Social
inﬂuence prediction with deep learning. In SIGKDD, pp. 2110–2119, 2018."
REFERENCES,0.9180327868852459,"Ellen Riloff and Rosie Jones. Learning dictionaries for information extraction by multi-level boot-
strapping. In AAAI, pp. 474–479, 1999."
REFERENCES,0.9234972677595629,"H. J. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Trans. Inf.
Theory, pp. 363–371, 1965."
REFERENCES,0.9289617486338798,"Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
Collective classiﬁcation in network data. AI Mag., pp. 93–106, 2008."
REFERENCES,0.9344262295081968,"Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Multi-stage self-supervised learning for graph convolu-
tional networks on graphs with few labeled nodes. In AAAI, pp. 5892–5899, 2020."
REFERENCES,0.9398907103825137,"Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale
information network embedding. In WWW, pp. 1067–1077, 2015."
REFERENCES,0.9453551912568307,"Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018."
REFERENCES,0.9508196721311475,"Petar Velickovic, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R. Devon
Hjelm. Deep graph infomax. In ICLR, 2019."
REFERENCES,0.9562841530054644,"Xiao-Ming Wu, Zhenguo Li, Anthony Man-Cho So, John Wright, and Shih-Fu Chang. Learning
with partially absorbing random walks. In NeurIPS, pp. 3086–3094, 2012."
REFERENCES,0.9617486338797814,"Bingbing Xu, Junjie Huang, Liang Hou, Huawei Shen, Jinhua Gao, and Xueqi Cheng.
Label-
consistency based graph neural networks for semi-supervised node classiﬁcation. In SIGIR, 2020."
REFERENCES,0.9672131147540983,"Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, pp.
5449–5458, 2018."
REFERENCES,0.9726775956284153,"Gang Yang, Xiaofeng Zhang, and Yueping Li. Session-based recommendation with graph neural
networks for repeat consumption. In ICCPR, pp. 519–524, 2020a."
REFERENCES,0.9781420765027322,"Zhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. Understanding
negative sampling in graph representation learning. In SIGKDD, pp. 1666–1676, 2020b."
REFERENCES,0.9836065573770492,"David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In ACL,
pp. 189–196, 1995."
REFERENCES,0.9890710382513661,"Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data
augmentation for graph neural networks. In AAAI, 2021."
REFERENCES,0.994535519125683,"Yan Zhou, Murat Kantarcioglu, and Bhavani M. Thuraisingham. Self-training with selection-by-
rejection. In ICDM, pp. 795–803, 2012."
