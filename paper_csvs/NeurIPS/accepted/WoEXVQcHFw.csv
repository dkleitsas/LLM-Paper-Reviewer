Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017761989342806395,"Multiobjective optimization (MOO) plays a critical role in various real-world
domains. A major challenge therein is generating K uniform Pareto-optimal
solutions to approximate the entire Pareto front. To address this issue, this paper
firstly introduces fill distance to evaluate the K design points, which provides
a quantitative metric for the representativeness of the design. However, directly
specifying the optimal design that minimizes the fill distance is nearly intractable
due to the involved nested min −max −min problem structure. To address this,
we propose a surrogate “max-packing” design for the fill distance design, which
is easier to optimize and leads to a rate-optimal design with a fill distance at
most 4× the minimum value. Extensive experiments on synthetic and real-world
benchmarks demonstrate that our proposed paradigm efficiently produces high-
quality, representative solutions and outperforms baseline MOO methods."
INTRODUCTION,0.003552397868561279,"1
Introduction"
INTRODUCTION,0.0053285968028419185,"0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
f1 0.0 0.2 0.4 0.6 0.8 1.0 f2"
INTRODUCTION,0.007104795737122558,"Solutions
Covering
PF"
INTRODUCTION,0.008880994671403197,"Figure 1: Covering of a Pareto
Front (PF). Eight diverse Pareto
objectives are used to cover the
entire PF with a small covering
radius."
INTRODUCTION,0.010657193605683837,"Multiobjective optimization (MOO) is widely used to inform real-
world decision-making across various fields, including materials
science [20, 46, 5, 30], recommendation systems [29, 61, 31], and
industrial design [44, 52, 50, 57]. An MOO problem (MOP) involves
optimizing multiple conflicting objectives, which can be (informally)
formulated as:"
INTRODUCTION,0.012433392539964476,"min
x∈X f(x) = (f1(x), . . . , fm(x)),
(1)"
INTRODUCTION,0.014209591474245116,"where m is the number of objectives. Equation (1) is a vector
optimization problem and it does not admit a total ordering, therefore,
to compare solutions, the concept of Pareto optimality is introduced.
A solution is called Pareto optimal if no other solution x′ ∈X
dominates it. Domination occurs when fi(x′) ≤fi(x) for all
i = 1, . . . , m, with at least one strict inequality [37, 19]. In this
paper, f(x) denotes the Pareto objective of a Pareto optimal solution.
The set of all Pareto optimal solutions is the Pareto set (PS), and their objectives form the Pareto front
(PF)."
INTRODUCTION,0.015985790408525755,"Under mild conditions, a PS or PF forms a continuous (m-1)-dim manifold containing infinitely
many solutions [25]. For a general MOP, it is intractable to precisely depict the entire PS or PF with
a closed-form expression. Researchers thus turns to use a small number K of diverse Pareto optimal
objectives to “represent” the entire PF. Several prior works have been focused on generating diverse"
INTRODUCTION,0.017761989342806393,"∗Corresponding to Qingfu Zhang. Contact: {xzhang2523-c@my.,qingfu.zhang@}cityu.edu.hk. The source
code is integrated into the LibMOON library, available at https://github.com/xzhang2523/libmoon."
INTRODUCTION,0.019538188277087035,"solutions (see Section 2), but they lack formal definitions of representability and uniformity. In this
paper, we define representability as the covering radius of a size-K solution set that covers the entire
PF. An illustrative example is provided in Figure 1, where eight uniformly distributed solutions cover
the entire PF with a small radius."
INTRODUCTION,0.021314387211367674,"In this paper, we first introduce the fill distance (FD) as the minimal covering radius of the Pareto
objectives, which measures how well a set of discrete solutions represents the true PF. A design
with a smaller covering radius is considered a better representation of the PF. Next, we demonstrate
that optimizing FD is challenging due to its nested min −max −min structure (Equation (4)). To
overcome this, we maximize the minimal (max −min) pairwise distances between Pareto objectives,
which bounds the minimal covering radius up to a constant. Finally, we propose a bi-level optimization
framework with a neural network to efficiently solve this max −min problem."
INTRODUCTION,0.023090586145648313,"Our method, UMOD (Uniform Multi-Objective optimization based on Decomposition) is within
the decomposition-based MOO paradigm [58]. To show its effectiveness, we conduct comparative
evaluations against methods on complex multiobjective problems with numerous local optimas and
on fairness classification problems with thousands of decision variables. The contribution of this
paper can be summarized as:"
INTRODUCTION,0.02486678507992895,"1. We introduce the fill distance as a metric for a set of Pareto solutions in MOO. We prove that
the optimal fill distance design serves as an upper bound for the optimal Inverted Generational
Distance (IGD) design, and that the max-packing design is rate-optimal with respect to the fill
distance. Therefore, the max-packing design provides an effective surrogate for the optimal fill
distance design.
2. We present a practical algorithm for identifying the maximum-packing design of Pareto objectives,
and formulate it as a bi-level optimization problem. To expedite the optimization process, we
introduce a neural network to avoid the frequent solving of the inner-loop optimization prob-
lem. Additionally, we analyze the optimization bounds of the neural network-based approach in
comparison to solving the original optimization problem directly.
3. Finally, we evaluate our approach against leading MOO methods, including evolutionary algorithms
and gradient-based algorithms, on both synthetic and real-world problems. UMOD outperforms
these methods in terms of both uniformity and efficiency, based on commonly used metrics in
MOO."
INTRODUCTION,0.02664298401420959,"Notations. In this paper, ρ(y(a), y(b)) represents the Euclidean distance between vectors y(a) and
y(b), with bold letters for vectors. Superscripts denote vectors, and subscripts (e.g., yi) indicate
elements of a vector. A PF is denoted by T . The objective space is Y = {y | f(x), x ∈X}. ∆m is
the m-D preference simplex; ∆m = {y | Pm
i=1 yi = 1, yi ≥0, i ∈[m]}, where [m] = {1, . . . , m}.
Y denotes a set."
RELATED WORKS,0.028419182948490232,"2
Related works"
RELATED WORKS,0.03019538188277087,"In this section, we review three lines of works to generate uniform or diverse Pareto objectives. We
focus our discussions on uniform/diverse Pareto objectives rather than Pareto solutions because our
goal is to produce uniform Pareto solutions in the objective space (Y), not the decision space (X)."
METHODS TO GENERATE DIVERSE PARETO OBJECTIVES,0.03197158081705151,"2.1
Methods to generate diverse Pareto objectives"
METHODS TO GENERATE DIVERSE PARETO OBJECTIVES,0.03374777975133215,"Various MOO methods can effectively generate diverse Pareto objectives, for both gradient-based
and evolution-computation (EC)-based frameworks. In the line of gradient-based methods, Pareto
MultiTask Learning (PMTL) [32] generates Pareto objectives which are constrained in specific regions
(sectors); MOO with Stein Variational Gradient Descent (MOO-SVGD) models objective vectors as
particles, updating them through repulsive forces with a kernel function to maximize their separation;
Exact Pareto Optimization (EPO) [36] aligns solutions with user-specific preference vectors, fostering
a diverse distribution of Pareto objectives by specifying diverse preferences. For multiobjective
evolutionary algorithms (MOEAs), NSGA2 [14] introduces the crowding distance and Pareto rank to
achieve a diverse distribution of Pareto objectives; MOEA/D [58] and its variants generate diverse
Pareto objectives by leveraging the positional relationship between preference vectors and Pareto
objectives; Hypervolume-based methods (e.g., SMS-MOEA [6]) maximize the set of solutions with
the largest hypervolume both to enhance convergence and diversity. A distinction of the proposed"
METHODS TO GENERATE DIVERSE PARETO OBJECTIVES,0.035523978685612786,"UMOD method with the previously mentioned methods is that, for general MOPs, the distribution of
the achieved Pareto objectives remains unknown, whereas the objective distribution of the proposed
method has desirable properties."
SUBSET SELECTION FOR MULTIOBJECTIVE OPTIMIZATION,0.037300177619893425,"2.2
Subset selection for multiobjective optimization"
SUBSET SELECTION FOR MULTIOBJECTIVE OPTIMIZATION,0.03907637655417407,"Another approach to generating a size-K diverse Pareto set is subset selection. This method first
generates a large number of Pareto solutions, then selects K solutions to maximize hypervolume
or minimize the IGD indicator [23, 53, 9, 45]. Solving the subset selection problem, a discrete
optimization problem, is generally inefficient compared with continuous optimization problems.
Recently, some approaches employ greedy algorithms [9, 33, 28] to obtain approximate solutions,
with naive greedy methods typically providing a (1 −1/e) guarantee. In contrast, our method
addresses a continuous optimization problem on the PF, using gradient-based techniques to solve the
established optimization problem to improve both accuracy and efficiency."
PREFERENCE ADJUSTMENT METHODS IN THE DECOMPOSITION-BASED MOO PARADIGM,0.04085257548845471,"2.3
Preference adjustment methods in the decomposition-based MOO paradigm"
PREFERENCE ADJUSTMENT METHODS IN THE DECOMPOSITION-BASED MOO PARADIGM,0.04262877442273535,"Since the proposed method can also be classified under the preference adjustment category, we discuss
the relationship between the proposed UMOD and preference/weight2 adjustment methods. The study
of preference adjustment methods start from MOEA/D-AWA [40], where its strategy is to remove the
preference corresponding to the most crowded objective and add a preference corresponding to the
most sparse one. Subsequently, several preference adjustment methods have been introduced [35],
including DEA-GNG [34] and MOEA/D-SOM [22], which utilize neural gas networks to guide
the selection of preference vectors. W-MOEA/D [21], tw-MOEA/D [38], paλ-MOEA/D [48], and
MOEA/D-AWG [55] use mathematical models to shape the non-dominated solutions and adjust
preference vectors. The proposed method differs from other preference adjustment methods in
two ways: (1) it models the PF with a neural network for better accuracy and efficiency, and (2)
it offers a rigorous theoretical analysis for selecting preference vectors yielding uniformity and
representativeness."
PARETO SOLUTIONS WITH UNIFORM DESIGNS,0.04440497335701599,"3
Pareto solutions with uniform designs"
FD AS AN UPPER BOUND OF IGD,0.046181172291296625,"3.1
FD as an upper bound of IGD"
FD AS AN UPPER BOUND OF IGD,0.047957371225577264,"We first define fill distance (FD) [11] of a set Y (Y = [y(1), . . . , y(K)]) and establish its relationship
with the inverted generational distance (IGD) indicator [49] of a set Y, a famous metric in MOO.
Formally, FD and IGD are defined as follows:
Definition 1 (FD & IGD)."
FD AS AN UPPER BOUND OF IGD,0.0497335701598579,"FD(Y) = max
y∈T min
y′∈Y ρ(y, y′) = max
y∈T dist(y, Y),
IGD(Y) =
Z"
FD AS AN UPPER BOUND OF IGD,0.05150976909413854,"T
min
y′∈Y ρ(y, y′)dy,
(2)"
FD AS AN UPPER BOUND OF IGD,0.05328596802841918,"where ρ(·, ·) represents the Euclidean distance. The term miny′∈Y ρ(y, y′) represents the nearest dis-
tance from a point y on the PF to the reference set Y. Therefore, FD(Y) = maxy∈T miny′∈Y ρ(y, y′)
denotes the covering radius of Y, i.e., the largest radius within which at least one solution in Y covers
the entire PF. However, IGD(Y), which represents the average distance from a point on the PF to
the set Y, lacks the clear geometric interpretation that fill distance offers. For MOO, the goal is
to minimize a set of Pareto objectives, i.e., Y ⊂T , by optimizing either the FD or IGD indicator:
minY⊂T FD(Y) or minY⊂T IGD(Y) to reach a diverse distribution. Let the optimal sets be YFD
and YIGD respectively. The following theorem compares FD and IGD.
Theorem 2 (FD as an upper bound of IGD)."
FD AS AN UPPER BOUND OF IGD,0.055062166962699825,"IGD(YIGD) ≤IGD(YFD) ≤FD(YFD)
(3)"
FD AS AN UPPER BOUND OF IGD,0.056838365896980464,"The first inequality follows because YIGD minimizes IGD, and the second holds because the average
distance (IGD(YFD)) is no greater than the maximum distance (FD(YFD)). Theorem 2 shows that
YFD, the optimal FD configuration, sets an upper bound for the IGD value. Since the optimal IGD
configuration does not similarly bound FD, we focus on FD in this paper."
FD AS AN UPPER BOUND OF IGD,0.0586145648312611,"2In this paper, “preference vector” and “weight vector” are used interchangeably."
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.06039076376554174,"3.2
Max-packing design as a surrogate of FD design"
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.06216696269982238,The minimization of FD involves solving the following nested min −max −min problem:
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.06394316163410302,"dFD = min
Y⊂T max
y∈T min
y′∈Y ρ(y, y′).
(4)"
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.06571936056838366,"A small dFD suggests that the optimal configuration YFD effectively covers the PF with a low
covering radius. However, this triply nested structure is challenging to optimize [54, 39], so we
propose using a max-packing design as a surrogate for solving Equation (4)."
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.0674955595026643,"dPack = max
Y⊂T δ = max
Y⊂T"
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.06927175843694494,"
min
1≤i<j≤K ρ(y(i), y(j))

,
(5)"
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.07104795737122557,"where δ represents the separation distance between two vectors. The optimal design YPack, which
solves the optimization problem (Equation (5)), is known as the max-packing design [8]. In this paper,
we show that YPack effectively optimizes FD when the decision space is a PF, as dFD is bounded by
YPack up to a constant factor, independent of size K in Theorem 3.
Theorem 3 (Surrogate for minimal FD). Consider a connected, compact 3 PF T . The minimal fill
distance dFD between T and a size-K design YPack/YFD will then be bounded as:"
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.07282415630550622,"1
4dPack ≤dFD ≤FD(YPack) ≤dPack,
(6)"
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.07460035523978685,"Furthermore, the fill distance between T and the optimal design dPack induced by YPack is upper
bounded by dPack, which is guaranteed to be upper bounded by 4dFD. YPack is thus considered a
quality, rate-optimal representative for the whole PF T ."
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.0763765541740675,"The second inequality dFD ≤dPack is proved by Auffray et al. [3], Pronzato [39], and we provide
a tighter lower bound dPack/4, utilizing the topological property of a PF. The complete proof of
Theorem 3 is left in Appendix A.1. Furthermore, under an additional strict inequality condition
dPack
K
< dPack
K+1 on the PF, the max-packing design YPack
K
serves as a dPack
K
-covering of T , which is
established by Theorem 4. The subscript “K” specifically denotes the max-packing distance dPack
K
for a size-K design, similarly used for YPack
K
to represent a size-K design."
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.07815275310834814,"Theorem 4. Consider a connected, compact PF (T ), with the property dPack
K
< dPack
K+1, the max-
packing design YPack
K
covers T with radius of at most dPack
K
."
MAX-PACKING DESIGN AS A SURROGATE OF FD DESIGN,0.07992895204262877,"This theorem suggests that YPack
K
can represent T well since the maximal distance between any
vector y ∈T and YPack
K
is bounded by dPack
K
."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.08170515097690942,"3.3
Characterizations of a max-packing design"
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.08348134991119005,"This section discusses key properties of the max-packing design on a PF. For bi-objective problems,
YPack shows favorable properties when YPack ⊂T , as formalized in Theorem 5, with the proof
in Appendix A.2."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.0852575488454707,"Theorem 5 (Characterization of YPack for biobjective problems). Let YPack = [y(1), . . . , y(K)] be
sorted by the first component, such that y(1)
1
≤. . . ≤y(K)
1
. For a compact, connected T , YPack is
characterized as follows:"
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.08703374777975133,"1. Equal spacing: ρ(y(1), y(2)) = . . . = ρ(y(K−1), y(K)), for K ≥3."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.08880994671403197,"2. Endpoint alignment: y(1) and y(K) are two endpoints (p(1), p(2)) of T , i.e., y(1) = p(1) =
[infy∈T y1, supy∈T y2] and y(K) = p(2) = [supy∈T y1, infy∈T y2], for K ≥2."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.0905861456483126,"Remark. Firstly, YPack, including both the starting and ending points, spans the maximum range
among all designs, which is a desirable property. Secondly, equal pairwise distances between Pareto
objectives yields an intuitive interpretation of uniformity. Maximizing hypervolume ensures the
“equal spacing” property only for bi-objective linear PFs [4][Theorem 4], while the max-packing
design only requires the PF to be compact and connected."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.09236234458259325,"3A connected, compact set is also called a rectifiable set."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.0941385435168739,"Besides this non-asymptotical bi-objective results, we examine the asymptotic properties of YPack
with Theorem 6.
Theorem 6 (Asymptotic uniformity [8](Theorem. 2.1)). As the number of set size K →∞, the
set sequence {YPack
K
} weakly converge to a uniform distribution over a compact, connected T .
Specifically, for any subset B ⊂T with measure-zero boundary, the proportion of points in YPack
K
lying within B converges to the proportion of T occupied by B:"
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.09591474245115453,"lim
K→∞
#(YPack
K
∩B)
#(YPack
K
)
= Vol(B)"
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.09769094138543517,"Vol(T ) = P(y ∈B | y ∈T ),
(7)"
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.0994671403197158,"where # denotes the number of points in a set, and “Vol” denotes the volume of a set."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.10124333925399645,"Theorem 6 shows that as the solution set size K grows, YPack
K
approaches a uniform distribution."
CHARACTERIZATIONS OF A MAX-PACKING DESIGN,0.10301953818827708,"Specifically, random variable Y Pack
K
d−→Unif(T ), meaning Y Pack
K
, the categorical distribution where
each y ∈YPack
K
has probability 1/K, converges in distribution to Unif(T )."
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.10479573712255773,"4
Efficient optimization of a size-K uniform set"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.10657193605683836,"The original max-packing problem (Equation (5)) maximizes the minimal pairwise distance among
Pareto objectives and can be reformulated as the following constrained optimization problem on the
PF:"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.108348134991119,"max

min
1≤i<j≤K ρ(y(i), y(j))

s.t. y(i), y(j) ∈T .
(8)"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.11012433392539965,"To constrain y(i), y(j) pairs as Pareto objectives, we solve decision variables of y(i)’s as the optimal
solution of the following modified Tchebycheff (mTche) aggregation function (Equation (9)),"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.11190053285968028,"y = ˜h(λ) = arg min
y′∈Y"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.11367673179396093,yi −zi λi
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.11545293072824156,"
:
h
0, π 2"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.1172291296625222,"im−1
7→Rm,
(9)"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.11900532859680284,"where z is a reference point (zi ≤yi, ∀y ∈Y, ∀i ∈[m]). This substitution is equivalent when the
optimal solution of Equation (9) is unique, as for any Pareto objective y, there exists a preference
λ ∈∆m such that the optimal value of Equation (9) matches y (explained in Appendix A.4).
Substituting Equation (9) into Equation (8) yields the following bi-level optimization problems:




"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.12078152753108348,"


"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.12255772646536411,"dPack =
max
ϑϑϑ(1),...,ϑϑϑ(K)
min
1≤i<j≤K ρ(y(i), y(j))"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.12433392539964476,"y(k) = arg min
y(k)∈Y"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.1261101243339254,"(
y(k)
i
−zi
λi(ϑϑϑ(k)) )"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.12788632326820604,", i ∈[m].
⇒ 

 
"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.12966252220248667,"dPack =
max
ϑϑϑ(1),...,ϑϑϑ(K)
min
1≤i<j≤K ρ(h(ϑϑϑ(i)), h(ϑϑϑ(j)))"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.13143872113676733,"s.t.
ϑϑϑ(k) ∈
h
0, π 2"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.13321492007104796,"im−1
. (10)"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.1349911190053286,"Exact Pareto optimal solutions can also be obtained using PMGDA [59], which achieves faster
convergence with a suitable control parameter σ. The function λ(ϑϑϑ) converts a “preference angle”
from an angle space

0, π"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.13676731793960922,"2
m−1 into a preference vector. The conversion relationship is detailed
in Appendix C.3. We use λ(ϑϑϑ) as decision variables for easier optimization since λ(ϑϑϑ) is constrained
in a box. In the right equation, the Pareto objective is denoted as y = h(ϑϑϑ) = ˜h(λ(ϑϑϑ)). Various bi-
level optimization methods [47, 60] can be used to solve the problem (Equation (10)). For efficiency
consideration, we use a gradient-based approach. Define δ = min(i,j) ρ(y(i), y(j)), where (i∗, j∗) is
the optimal pair from arg min(i,j) ρ(y(i), y(j)). After some basic algebraic calculations, ∂δ"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.13854351687388988,"∂ϑϑϑ can be
calculated by the following two equations:"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.14031971580817051,"∂δ
∂ϑϑϑ(i∗) = ∂h(ϑϑϑ(i∗))"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.14209591474245115,"∂ϑϑϑ(i∗)
|
{z
}
B(n×m)"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.1438721136767318,"h(ϑϑϑ(i∗)) −h(ϑϑϑ(j∗))
ρ(h(ϑϑϑ(i∗)), h(ϑϑϑ(j∗)))
|
{z
}
A(m×1)"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.14564831261101244,"⊤
,
∂δ
∂ϑϑϑ(j∗)
= −∂h(ϑϑϑ(j∗))"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.14742451154529307,"∂ϑϑϑ(j∗)
|
{z
}
C(n×m)"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.1492007104795737,"h(ϑϑϑ(i∗))h(ϑϑϑ(j∗))
ρ(h(ϑϑϑ(i∗)), h(ϑϑϑ(j∗)))
|
{z
}
A(m×1) ⊤
."
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.15097690941385436,"For k ̸= i∗, j∗,
∂δ
∂ϑϑϑ(k) is zero. While computing the gradient vector A is straightforward, B
and C are challenging because h(ϑϑϑ) is a black-box function representing optimal values. Finite-
difference estimation of B and C is impractical, requiring solving Equation (9) at least n × m times."
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.152753108348135,"Instead, we use a neural model hϕ trained on historical data (ϑϑϑ, h(ϑϑϑ)) to approximate h, enabling
efficient estimation via ∂hϕ"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.15452930728241562,"∂ϑϑϑ . We analyze the neural model’s error in Theorem 7, with full details
in Appendix A.3.
Theorem 7 (Optimization error ϵnn introduced by using a network). Let hϕ(ϑϑϑ) be an approximator
of h(ϑϑϑ) such that ||hϕ(ϑϑϑ) −h(ϑϑϑ)|| ≤ϵ, for every ϑϑϑ ∈[0, π"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.15630550621669628,"2 ]m−1, as commonly assumed in bi-level
optimization, e.g., [18], Eq. (10). ϵnn is the difference between the maximum of the minimal distances
calculated using the approximate function hϕ and the true function h. Then, ϵnn is bounded by 2ϵ:"
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.15808170515097691,"ϵnn =

max
ϑϑϑ(1),...,ϑϑϑ(K)
min
1≤i<j≤K ρ(hϕ(ϑϑϑ(i)), hϕ(ϑϑϑ(j))) −dPack
 ≤2ϵ."
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.15985790408525755,"Remark. The error ϵ, defined as ||hϕ(ϑϑϑ) −h(ϑϑϑ)|| ≤ϵ, is both influenced by the covering
radius R of the estimated solutions hϕ(ϑϑϑ(1)), . . . , hϕ(ϑϑϑ(K)) and the fitting error ϵfit, where
ϵfit = maxk∈[K] ||hϕ(ϑϑϑ(k)) −h(ϑϑϑ(k))||.
For any ϑϑϑ, the error satisfies ||hϕ(ϑϑϑ) −h(ϑϑϑ)|| ≤
L||ϑϑϑ −ϑϑϑ(i′)|| + ϵfit ≤L · R + ϵfit, where L is the Lipschitz constant of function (hϕ(ϑϑϑ) −h(ϑϑϑ)),
ϑϑϑ(i′) is the nearest solution to ϑϑϑ among the estimated solutions, and R is the covering radius, which
can be further reduced by adding more training pairs. For overparameterized networks, ϵfit can be
considered as very small."
EFFICIENT OPTIMIZATION OF A SIZE-K UNIFORM SET,0.16163410301953818,"Practical algorithms. Due to the space limit, the pseudo-codes of UMOD are provided in Algorithm 1
and Algorithm 2 in Appendix C.1. Initially, K diverse preferences are generated by the Das-Dennis
algorithm [12]. Then either a decomposition-based multiobjective evolutionary algorithm or a
gradient-based MOO with mTche aggregation function is employed for producing preference angle
and Pareto objective pairs (ϑϑϑ, y). Evolutionary algorithms are preferred for problems with multiple
local optimas, while gradient-based MOO methods are preferred for multiobjective machine learning
(MOML) problems. Given (ϑϑϑ, y) pairs, a PF model hϕ(ϑϑϑ) is fitted by optimizing the mean square
estimation error. Finally, preference angles are updated to maximize the minimal pairwise distances
of Pareto objectives. These two steps are repeated alternatively until convergence."
EXPERIMENTS,0.16341030195381884,"5
Experiments"
EXPERIMENTS,0.16518650088809947,"The experiments compare UMOD with other methods on two types of MOPs: (1) those with multiple
local optimas which can be solved by MOEAs efficiently, and (2) multiobjective fairness classification
neural networks as decision variables. MOEAs and multiobjective fairness problems are executed
with 31/5 random seeds, separately. We employ seven indicators to assess performance of different
algorithms: (1) Hypervolume (↑) [24], (2) IGD (↓) [26], (3) Sparsity (↓) [56], (4) Spacing (↓) [43],
(5) Uniformity (↑), (6) Smooth Uniformity (↑), and (7) Fill distance (↓). Indicators 1-2 focus on
convergence and diversity for multi-objective optimization (MOO), while indicators 3-7 evaluate
solution uniformity. See Appendix B.1 for more detailed expression for these indicators."
COMPARISON WITH MOEAS,0.1669626998223801,"5.1
Comparison with MOEAs f1 0.0 0.2 0.4 0.6 0.8 1.0 f2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f3"
COMPARISON WITH MOEAS,0.16873889875666073,"0.0
0.2
0.4
0.6
0.8 1.0"
COMPARISON WITH MOEAS,0.1705150976909414,"UMOD
MOEA/D
PF"
COMPARISON WITH MOEAS,0.17229129662522202,(a) UMOD and MOEA/D on RE37 f1
COMPARISON WITH MOEAS,0.17406749555950266,"0.0
0.2
0.4
0.6
0.8 1.0 f2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f3"
COMPARISON WITH MOEAS,0.17584369449378331,"0.0
0.2
0.4
0.6
0.8
1.0"
COMPARISON WITH MOEAS,0.17761989342806395,"UMOD
Mapping
Preferences"
COMPARISON WITH MOEAS,0.17939609236234458,(b) Visualization on RE37
COMPARISON WITH MOEAS,0.1811722912966252,"Figure 2: (a): UMOD solutions are more
uniform. (b): A PF model can be trained with
a small number of solutions."
COMPARISON WITH MOEAS,0.18294849023090587,"We demonstrate the effectiveness of our proposed method
across a diverse set of MOEA benchmark problems, in-
cluding the ZDT4 [16], DTLZ [15]5, and real-world testing
problems [50]. Real-world testing problems include: four-
bar truss design (RE21), reinforced concrete beam design
(RE22), disc brake design (RE33), rocket injector design
(RE37), car side impact design (RE41), and conceptual ma-
rine design (RE42). Notably, RE41 and RE42 are complex
four-objective problem having a large objective spaces.
The prefix “RE” denotes this problem is a real-world one."
COMPARISON WITH MOEAS,0.1847246891651865,"Baseline
MOEAs
include:
MOEA/D
[58],
a
decomposition-based approach; (2) MOEA/D-AWA [40],"
COMPARISON WITH MOEAS,0.18650088809946713,"4ZDT5 is a discrete optimization problem and is commonly excluded in MOEA studies.
5DTLZ 7 is excluded because our theoretical results only apply to compact and connected PFs, which DTLZ
7 does not satisfy. Results on DTLZ 5-6 are demonstrated in Appendix B.6."
COMPARISON WITH MOEAS,0.1882770870337478,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.19005328596802842,"Solutions
Covering"
COMPARISON WITH MOEAS,0.19182948490230906,(a) LMPFE
COMPARISON WITH MOEAS,0.1936056838365897,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.19538188277087035,"Solutions
Covering"
COMPARISON WITH MOEAS,0.19715808170515098,(b) DEAGNG
COMPARISON WITH MOEAS,0.1989342806394316,"0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
f1 0.0 0.2 0.4 0.6 0.8 1.0 f2"
COMPARISON WITH MOEAS,0.20071047957371227,"Solutions
Covering"
COMPARISON WITH MOEAS,0.2024866785079929,(c) Subset selection
COMPARISON WITH MOEAS,0.20426287744227353,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.20603907637655416,"Solutions
Covering"
COMPARISON WITH MOEAS,0.20781527531083482,(d) SMS-EMOA
COMPARISON WITH MOEAS,0.20959147424511546,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.2113676731793961,"Solutions
Covering"
COMPARISON WITH MOEAS,0.21314387211367672,(e) NSGA3
COMPARISON WITH MOEAS,0.21492007104795738,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.216696269982238,"Solutions
Covering"
COMPARISON WITH MOEAS,0.21847246891651864,(f) MOEA/D
COMPARISON WITH MOEAS,0.2202486678507993,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.22202486678507993,"Solutions
Covering"
COMPARISON WITH MOEAS,0.22380106571936056,(g) MOEA/D-AWA
COMPARISON WITH MOEAS,0.2255772646536412,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.22735346358792186,"Solutions
Covering"
COMPARISON WITH MOEAS,0.2291296625222025,(h) UMOD
COMPARISON WITH MOEAS,0.23090586145648312,Figure 3: Result comparison by different methods on ZDT1.
COMPARISON WITH MOEAS,0.23268206039076378,Table 1: Partial results for biobjective problems (full results are in Table 7).
COMPARISON WITH MOEAS,0.2344582593250444,"Indicator
DEA-GNG [34] LMPFE [51]
Subset [9, 45]
NSGA3 [13, 27] SMS-EMOA [6] MOEA/D [58]
MOEA/D-AWA [40] UMOD ZDT1"
COMPARISON WITH MOEAS,0.23623445825932504,"HV
1.03 (0.00) (6)
1.03 (0.00) (2) 1.02 (0.00) (7)
1.03 (0.00) (5)
1.04 (0.00) (1)
1.03 (0.00) (4)
1.03 (0.00) (3)
1.04 (0.00) (0)
IGD
5.68 (0.29) (7)
5.42 (0.21) (6) 5.21 (0.40) (1)
5.28 (0.00) (3)
5.22 (0.07) (2)
5.30 (0.00) (4)
5.42 (0.02) (5)
5.19 (0.00) (0)
Spacing
6.35 (1.74) (7)
2.44 (1.82) (2) 3.51 (0.47) (3)
5.90 (0.00) (5)
1.91 (0.46) (1)
5.92 (0.01) (6)
3.69 (0.04) (4)
0.12 (0.06) (0)
Sparsity
4.80 (0.19) (7)
4.55 (0.12) (4) 2.59 (0.18) (0)
4.60 (0.00) (5)
4.48 (0.01) (2)
4.61 (0.00) (6)
4.51 (0.01) (3)
4.39 (0.00) (1)
Uniform
1.33 (0.11) (6)
1.67 (0.32) (2) 0.89 (0.10) (7)
1.53 (0.00) (3)
1.85 (0.03) (1)
1.51 (0.00) (4)
1.51 (0.00) (5)
2.07 (0.01) (0)
SUniform
0.47 (0.12) (6)
0.68 (0.13) (2) 0.09 (0.09) (7)
0.49 (0.00) (4)
0.74 (0.01) (1)
0.48 (0.00) (5)
0.53 (0.00) (3)
0.77 (0.00) (0)
Fill Distance 1.60 (0.23) (6)
1.23 (0.09) (2) 2.00 (0.18) (7)
1.55 (0.00) (5)
1.16 (0.05) (1)
1.50 (0.00) (4)
1.43 (0.03) (3)
1.04 (0.01) (0) RE21"
COMPARISON WITH MOEAS,0.23801065719360567,"HV
1.24 (0.00) (5)
1.23 (0.01) (7) 1.24 (0.00) (4)
1.24 (0.00) (2)
1.24 (0.00) (1)
1.24 (0.00) (6)
1.24 (0.00) (3)
1.24 (0.00) (0)
IGD
4.63 (0.28) (5)
4.61 (0.13) (4) 5.15 (0.01) (6)
4.44 (0.00) (3)
4.23 (0.02) (1)
5.40 (0.02) (7)
4.33 (0.04) (2)
4.12 (0.00) (0)
Spacing
6.63 (1.16) (6)
3.62 (0.93) (4) 1.38 (0.00) (1)
5.71 (0.01) (5)
3.19 (0.37) (2)
10.49 (0.04) (7) 3.47 (0.41) (3)
0.12 (0.05) (0)
Sparsity
3.10 (0.21) (6)
2.90 (0.10) (4) 1.74 (0.00) (0)
3.02 (0.00) (5)
2.82 (0.02) (2)
3.87 (0.02) (7)
2.84 (0.01) (3)
2.70 (0.00) (1)
Uniform
0.81 (0.12) (7)
0.95 (0.16) (5) 0.95 (0.00) (4)
1.16 (0.00) (2)
1.26 (0.05) (1)
0.93 (0.00) (6)
1.11 (0.03) (3)
1.62 (0.01) (0)
SUniform
-0.03 (0.07) (5)
0.12 (0.08) (3) -0.17 (0.00) (7) 0.08 (0.00) (4)
0.20 (0.01) (1)
-0.17 (0.00) (6) 0.13 (0.01) (2)
0.31 (0.00) (0)
Fill Distance 1.45 (0.20) (4)
1.21 (0.12) (3) 2.62 (0.01) (7)
1.47 (0.00) (5)
1.09 (0.04) (1)
2.11 (0.01) (6)
1.15 (0.02) (2)
0.83 (0.00) (0) Rank"
COMPARISON WITH MOEAS,0.23978685612788633,"HV
5.14
4.14
5.57
3.86
0.57
3.86
3.57
1.29
IGD
5
4.86
4
2.43
2.29
4.43
4
1
Spacing
5.71
2.71
3.86
3.86
3
5.14
3.57
0.14
Sparsity
5.29
5.71
1.43
3.71
2.71
4.86
3.43
0.86
Uniform
5
3.86
6.43
3
1.71
4.29
3.57
0.14
SUniform
5.14
3.57
6.86
3.14
2
4
3.14
0.14
Fill Distance 5.43
3.57
5.57
3.43
2.29
3.57
3.29
0.86"
COMPARISON WITH MOEAS,0.24156305506216696,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.2433392539964476,"Solutions
Covering"
COMPARISON WITH MOEAS,0.24511545293072823,(a) UMOD on RE21
COMPARISON WITH MOEAS,0.2468916518650089,0.250.00 0.25 0.50 0.75 1.00 1.25 f1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f2
COMPARISON WITH MOEAS,0.24866785079928952,"Solutions
Covering"
COMPARISON WITH MOEAS,0.25044404973357015,(b) MOEA/D on RE21 f1
COMPARISON WITH MOEAS,0.2522202486678508,"0.0
0.2
0.4
0.6
0.8
1.0 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3"
COMPARISON WITH MOEAS,0.2539964476021314,"0.0
0.2
0.4
0.6
0.8
1.0"
COMPARISON WITH MOEAS,0.2557726465364121,"UMOD
MOEA/D"
COMPARISON WITH MOEAS,0.25754884547069273,"(c) UMOD and MOEA/D
(d) Minimal distances to
other objectives"
COMPARISON WITH MOEAS,0.25932504440497334,Figure 4: Results on RE21 and DTLZ2. f1
COMPARISON WITH MOEAS,0.261101243339254,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 f4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.26287744227353466,"DEA-GNG
PF"
COMPARISON WITH MOEAS,0.26465364120781526,(a) DEAGNG f1
COMPARISON WITH MOEAS,0.2664298401420959,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 f4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.2682060390763766,"LMPFE
PF"
COMPARISON WITH MOEAS,0.2699822380106572,(b) LMPFE f1
COMPARISON WITH MOEAS,0.27175843694493784,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 f4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.27353463587921845,"Subset
PF"
COMPARISON WITH MOEAS,0.2753108348134991,(c) Subset selection f1
COMPARISON WITH MOEAS,0.27708703374777977,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 f4 0.0 0.2 0.4 0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.27886323268206037,"UMOD
PF"
COMPARISON WITH MOEAS,0.28063943161634103,(d) UMOD
COMPARISON WITH MOEAS,0.2824156305506217,Figure 5: Results on RE41 by different methods (full results are in Figure 8). f1
COMPARISON WITH MOEAS,0.2841918294849023,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f4"
COMPARISON WITH MOEAS,0.28596802841918295,"0.0
0.2
0.4 0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.2877442273534636,"DEA-GNG
PF"
COMPARISON WITH MOEAS,0.2895204262877442,(a) DEAGNG f1
COMPARISON WITH MOEAS,0.2912966252220249,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f4"
COMPARISON WITH MOEAS,0.29307282415630553,"0.0
0.2
0.4
0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.29484902309058614,"LMPFE
PF"
COMPARISON WITH MOEAS,0.2966252220248668,(b) LMPFE on RE41 f1
COMPARISON WITH MOEAS,0.2984014209591474,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f4"
COMPARISON WITH MOEAS,0.30017761989342806,"0.0
0.2
0.4 0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.3019538188277087,"Subset
PF"
COMPARISON WITH MOEAS,0.3037300177619893,(c) Subset selection f1
COMPARISON WITH MOEAS,0.30550621669627,"0.0
0.2
0.4
0.6
0.8
1.0
1.2 f2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 f4"
COMPARISON WITH MOEAS,0.30728241563055064,"0.0
0.2
0.4 0.6 0.8 1.0"
COMPARISON WITH MOEAS,0.30905861456483125,"UMOD
PF"
COMPARISON WITH MOEAS,0.3108348134991119,(d) UMOD
COMPARISON WITH MOEAS,0.31261101243339257,Figure 6: Results on RE42 by different methods (full results are in Figure 9).
COMPARISON WITH MOEAS,0.31438721136767317,"integrating adaptive weight adjustment; (3) NSGA3 [13, 27], generating diverse Pareto objectives
through crowding distance; (4) SMS-EMOA [6], maximizing hypervolume for diverse solutions; (5)
LMPFE [51], estimating the PF using multiple local models; (6) Subset selection [9, 45], choosing a
solution set by hypervolume maximization6; and (7) DEA-GNG [34], a preference adjustment method
based on growing neural gas network. Methods (1)-(4) are classical MOEA methods implemented by
Pymoo [7], while methods (5)-(7) are more recent methods. Full name of these methods are provided
in Table 4."
COMPARISON WITH MOEAS,0.31616341030195383,Table 2: Partial results on three-objective problems (full results are in Table 8).
COMPARISON WITH MOEAS,0.31793960923623443,"Indicator
DEA-GNG [34] LMPFE [51]
Subset [45]
NSGA3 [13]
SMS-EMOA [6] MOEA/D [58]
MOEA/D-AWA [40] UMOD DTLZ2"
COMPARISON WITH MOEAS,0.3197158081705151,"HV
1.01 (0.01) (7)
1.05 (0.00) (6)
1.08 (0.00) (1)
1.06 (0.00) (5)
1.08 (0.00) (0)
1.06 (0.00) (3)
1.06 (0.00) (4)
1.07 (0.00) (2)
IGD
12.52 (0.64) (2) 12.46 (0.09) (1) 15.14 (0.00) (6) 12.54 (0.00) (3) 15.45 (0.25) (7)
12.55 (0.00) (5) 12.55 (0.00) (4)
12.19 (0.04) (0)
Spacing
5.29 (1.72) (2)
2.64 (0.39) (1)
8.97 (0.00) (7)
5.45 (0.00) (3)
7.15 (0.51) (6)
5.45 (0.01) (4)
5.45 (0.01) (5)
0.52 (0.21) (0)
Sparsity
1.50 (0.13) (0)
2.18 (0.13) (2)
2.42 (0.00) (3)
2.43 (0.00) (4)
2.75 (0.14) (7)
2.43 (0.00) (6)
2.43 (0.00) (5)
1.61 (0.13) (1)
Uniform
1.58 (0.19) (5)
2.45 (0.13) (1)
0.96 (0.00) (7)
2.43 (0.00) (4)
1.51 (0.15) (6)
2.43 (0.00) (3)
2.43 (0.00) (2)
3.17 (0.09) (0)
SUniform
0.36 (0.23) (5)
0.97 (0.04) (1)
-0.04 (0.00) (7) 0.95 (0.00) (4)
0.18 (0.07) (6)
0.95 (0.00) (2)
0.95 (0.00) (3)
1.18 (0.02) (0)
Fill Distance 3.20 (0.60) (5)
2.69 (0.10) (4)
3.54 (0.00) (7)
2.59 (0.00) (1)
3.42 (0.16) (6)
2.59 (0.00) (3)
2.59 (0.00) (2)
2.37 (0.08) (0) Rank"
COMPARISON WITH MOEAS,0.32149200710479575,"HV
6.8
4.6
1.8
4.4
0.2
3.6
4.2
2.4
IGD
6
3
4.6
1.8
4.6
3.4
4
0.6
Spacing
4.4
2.8
5.6
3.6
4
3.4
3.6
0.6
Sparsity
1.4
3.6
1.6
4.6
4.8
5.6
4.6
1.8
Uniform
5.8
2
5.8
2.8
4
3.4
3.4
0.8
SUniform
6
2
5.8
3.4
4.4
3
3.2
0.2
Fill Distance 6.6
4.2
5.2
1
4.2
3.2
3.2
0.4"
COMPARISON WITH MOEAS,0.32326820603907636,"We present the results for biobjective problems in Figure 3 and Table 17. By directly minimizing
maximal pairwise distances, the uniform indicator (which corresponds to maximal pairwise distances,
see Appendix B.1, metric 5) is optimized effectively and ranks best among all methods. The
fill distance, a surrogate for maximal pairwise distance up to constant, also performs best among
all methods. This indicates that solutions found by UMOD cover the true PF with the minimal
covering radius among all methods. Figure 3 further confirms that the covering radius of UMOD
is significantly smaller than other methods. We also observe that the IGD indicator (Appendix B.1,
metric 2), representing the mean Euclidean distance between the true PF and the found size-K
solution set, is significantly improved. The significant improvement over IGD, a well-established
indicator of uniformity and convergence of Pareto solutions, suggests that our method finds high-"
COMPARISON WITH MOEAS,0.325044404973357,"6Code: https://github.com/HisaoLabSUSTC/GAHSS.
7For a better illustration, the unit of IGD, Spacing, Sparsity, Uniform, SUniform, and Fill distance are scaled
by 0.01,0.01,0.01,0.1,0.1, and 0.1."
COMPARISON WITH MOEAS,0.3268206039076377,"Table 3: Statistical results on fairness classification problems. Results are averaged on five random
seeds. Results are presented in the the format of “(mean)/(std)”."
COMPARISON WITH MOEAS,0.3285968028419183,"Indicator
UMOD
Agg-mTche [37] Agg-LS [37]
Agg-PBI [58]
EPO [36]
PMGDA [59]
HVGrad [17] Adult"
COMPARISON WITH MOEAS,0.33037300177619894,"δUnif(↑)
0.1049 (0.0000)
0.0151 (0.0000)
0.0006 (0.0000)
0.0232 (0.0000)
0.0161 (0.0000)
0.0201 (0.0000)
0.0421 (0.0000)
δSUnif(↑) 0.0088 (0.0000)
-0.0849 (0.0000) -0.1482 (0.0000) -0.0764 (0.0000) -0.0838 (0.0000) -0.0738 (0.0000) -0.0510 (0.0000)
HV(↑)
0.6800 (0.0000)
0.6769 (0.0000)
0.6422 (0.0000)
0.6679 (0.0000)
0.6755 (0.0000)
0.6764 (0.0000)
0.6865 (0.0000)
Span(↑)
0.0773 (0.0000)
0.0615 (0.0000)
0.0012 (0.0000)
0.0735 (0.0000)
0.0594 (0.0000)
0.0688 (0.0000)
0.0766 (0.0000)
Spacing(↓) 0.8677 (0.0979)
2.9454 (0.0194)
0.0199 (0.0000)
2.3149 (0.0105)
2.9316 (0.0055)
5.6512 (0.0058)
1.9228 (0.0146)
Sparsity(↓) 1.3914 (0.0057)
0.3067 (0.0004)
0.0002 (0.0000)
0.3013 (0.0001)
0.3124 (0.0002)
0.8369 (0.0003)
0.4888 (0.0002)"
COMPARISON WITH MOEAS,0.3321492007104796,Compass
COMPARISON WITH MOEAS,0.3339253996447602,"δUnif(↑)
0.3801 (0.0027)
0.0131 (0.0000)
0.0014 (0.0000)
0.0203 (0.0000)
0.0191 (0.0000)
0.0176 (0.0000)
0.2199 (0.0000)
δSUnif(↑) 0.3148 (0.0014)
-0.0429 (0.0000) -0.1126 (0.0000) -0.0332 (0.0000) -0.0358 (0.0000) -0.0360 (0.0000) 0.1635 (0.0000)
HV(↑)
0.7262 (0.0012)
0.7394 (0.0000)
0.7256 (0.0000)
0.7359 (0.0000)
0.7448 (0.0000)
0.7428 (0.0000)
0.7625 (0.0000)
Span(↑)
0.2156 (0.0006)
0.1950 (0.0000)
0.1783 (0.0000)
0.1954 (0.0000)
0.1979 (0.0000)
0.1986 (0.0000)
0.2089 (0.0000)
Spacing(↓) 4.4137 (6.1916)
25.4158 (0.1208) 27.1222 (0.0185) 22.8444 (0.2152) 25.7459 (0.2039) 26.4816 (0.1548) 7.6151 (0.0198)
Sparsity(↓) 19.7485 (1.5139) 12.5492 (0.0864) 12.4902 (2.1463) 10.8758 (0.0985) 13.1004 (0.1647) 13.8356 (0.0960) 10.0604 (0.0233)"
COMPARISON WITH MOEAS,0.33570159857904086,"quality Pareto solutions and also implies an inherent theoretical connection between IGD and minimal
pairwise distance maximization, warranting further investigation."
COMPARISON WITH MOEAS,0.33747779751332146,"We would like to mention another advantage of UMOD is it can handle PFs of different scales,
as demonstrated in Figure 4(a)/(b). Unlike using fixed preference vectors, which can result in
non-uniform Pareto objectives, UMOD ensures uniformity in the objective space, remaining the
uniformity of the achieved distribution unaffected by the scale of a PF."
COMPARISON WITH MOEAS,0.3392539964476021,"For three-objective problems, DTLZ1 owns a simplex-like PF, making DTLZ1 the only problem
where uniform preferences result in uniform Pareto objectives. For DTLZ2 problem with a sphere-like
PF, using uniform preferences on the simplex fails to produce uniform Pareto objectives by MOEA/D.
As shown in Figure 4(c), objectives solved by MOEA/D around the center of the PF are sparse, while
those around the margin are dense. In contrast, UMOD produces uniform Pareto objectives on the
PF. The minimal distances from one Pareto objective to the rest of the solutions, sorted by index, are
shown in Figure 4(d), indicating that only UMOD achieves the maximal minimal pairwise distance.
The results for the real-world RE37 problem are shown in Figure 2(a), indicating MOEA/D produces
inefficient duplicated solutions on the boundary of the PF. In contrast, UMOD effectively reduces
duplicated solutions by maximizing the minimal pairwise distance on the PF, as duplicated solutions
have a minimal pairwise distance of zero. A visualization of the learned PF is shown in Figure 2(b),
indicating that a PF model can be learned efficiently by using only a small number of solutions. Most
indicators related with uniformity, such as IGD, uniform distance, and FD outperforms other methods
significantly, indicating UMOD finds much more uniform and representative solutions. The HV
indicator of UMOD is the 3-rd best and comparable to SMS-EMOA, a method directly optimizes the
HV indicator."
COMPARISON WITH MOEAS,0.3410301953818828,"Finally, we discuss the results for four-objective problems, shown in Figures 8 and 9 and Table 9.
UMOD outperforms DEAGNG and LMPFE by discovering a broader PF (Figure 5), as it directly
maximizes the minimal pairwise distance on the PF. Although subset selection performs similarly to
UMOD on the RE41 problem, it relies on an inefficient two-phase optimization to select a subset
from a larger solution set, making it less efficient for four-objective problems. On RE41, UMOD
significantly outperforms other methods in IGD, finding more representative solutions. On RE42,
UMOD ranks highly (best or second best) in uniformity indicators like spacing, sparsity, and smooth
uniformity. Due to a challenging PF region, IGD indicators for UMOD, NSGA3, and SMS-EMOA
are similar, with these three methods outperforming the others."
RESULTS ON FAIRNESS CLASSIFICATION,0.3428063943161634,"5.2
Results on fairness classification"
RESULTS ON FAIRNESS CLASSIFICATION,0.34458259325044405,"We compare our method against other gradient-based MOO methods on multiobjective fairness
tasks involving the Adult [2] and Compass [1] datasets. The decision variables are neural network
parameters optimized for classification accuracy and fairness. The first objective is binary cross-
entropy classification loss, and the second is a hyperbolic tangent relaxation of the difference of
equality of opportunity (DEO) loss [41][Eq. 6]. Data and network architecture details are provided in
Table 6 in Appendix B.3."
RESULTS ON FAIRNESS CLASSIFICATION,0.3463587921847247,"Results with five solutions are illustrated in Figure 7 compared with PMGDA [59], Hypervolume-
gradient method (HVGrad), EPO [36], modified Tchebycheff aggregation function method (Agg-
mTche) [58]. For the Adult dataset, all methods run for 30 epochs, and for the Compass dataset, 20
epochs. A detailed description of these methods is in Appendix C.4. In real-world problems, the
true PF range is often unknown, making it difficult for preference-based methods to select uniform
preference vectors. If these vectors exclude PF endpoints, parts of the PF can not be recovered. In
contrast, HV-Grad and the proposed UMOD method do not rely on preferences and can automatically
identify a large PF. Figure 7 validates Theorem 5, showing that UMOD identifies both endpoints of
the true PF, while HVGrad’s endpoints cannot be determined."
RESULTS ON FAIRNESS CLASSIFICATION,0.3481349911190053,"0.30
0.35
0.40
0.45
0.50
0.55
0.60
f1 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 f2"
RESULTS ON FAIRNESS CLASSIFICATION,0.34991119005328597,"UMOD
Agg-mTche
PMGDA
HVGrad
EPO"
RESULTS ON FAIRNESS CLASSIFICATION,0.35168738898756663,(a) Adult
RESULTS ON FAIRNESS CLASSIFICATION,0.35346358792184723,"0.25
0.50
0.75
1.00
1.25
1.50
1.75
f1 0.00 0.05 0.10 0.15 0.20 f2"
RESULTS ON FAIRNESS CLASSIFICATION,0.3552397868561279,"UMOD
Agg-mTche
PMGDA
HVGrad
EPO"
RESULTS ON FAIRNESS CLASSIFICATION,0.35701598579040855,(b) Compass
RESULTS ON FAIRNESS CLASSIFICATION,0.35879218472468916,Figure 7: Result on fairness classification.
RESULTS ON FAIRNESS CLASSIFICATION,0.3605683836589698,"Numerical results are presented in Table 3.
In real-world problems, objectives often vary
in scale, making uniform preference vectors
non-equivalent to uniform objective vectors.
UMOD, however, is designed to generate uni-
form objective vectors independent of scale,
as evidenced by its superior performance in
uniformity and soft uniformity indicators. Ad-
ditionally, UMOD’s spacing indicator, mea-
suring neighborhood distance deviation, is sig-
nificantly lower than other methods (except
Agg-LS, which produces duplicate solutions in the Adult dataset, resulting in the lowest spacing).
UMOD’s Span indicators outperform other methods a lot, demonstrating its ability to recover a
more complete PF, a desirable feature. Many methods show similar HV indicators, but their solution
configurations vary, suggesting HV optimization is a coarse measure of uniformity. UMOD’s signifi-
cantly better uniformity performance indicates that directly optimizing uniformity, as in UMOD, is a
promising approach for generating diverse Pareto solutions."
CONCLUSIONS AND FURTHER WORKS,0.3623445825932504,"6
Conclusions and further works"
CONCLUSIONS AND FURTHER WORKS,0.3641207815275311,"Conclusions. In this paper, we have proposed a new understanding of a longstanding problem in
MOO, generating K uniform and representative Pareto objectives, through searching for a max-
packing design on the PF. We provide rigorous analysis of the resulting objective design, and in
particular, we show this design will asymptotically converge to the uniform measure over Pareto front.
With this new paradigm, we also empirically demonstrate how the space-filling design we obtain can
benefit downstream performance with both synthetic and real-world MOO tasks. Overall, we believe
that we pave a new way for (rate-)optimally configuring the Pareto objectives."
CONCLUSIONS AND FURTHER WORKS,0.36589698046181174,"Future works include applying UMOD to large-scale real-world multiobjective problems, such as
material design, vaccine design, and recommendation systems. The broader impacts of this work is
discussed in Appendix D.1."
CONCLUSIONS AND FURTHER WORKS,0.36767317939609234,Acknowledge
CONCLUSIONS AND FURTHER WORKS,0.369449378330373,"GH Li offers guidance on evolutionary computation, while YF Chen proves Theorem 3. The work
described in this paper was supported by the Research Grants Council of the Hong Kong Special
Administrative Region, China [GRF Project No. CityU 11215622]."
REFERENCES,0.37122557726465366,References
REFERENCES,0.37300177619893427,"[1] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. In Ethics of data
and analytics, pages 254–264. Auerbach Publications, 2022."
REFERENCES,0.3747779751332149,"[2] Arthur Asuncion and David Newman. Uci machine learning repository, 2007."
REFERENCES,0.3765541740674956,"[3] Yves Auffray, Pierre Barbillon, and Jean-Michel Marin. Maximin design on non hypercube
domains and kernel interpolation. Statistics and Computing, 22:703–712, 2012."
REFERENCES,0.3783303730017762,"[4] Anne Auger, Johannes Bader, Dimo Brockhoff, and Eckart Zitzler. Theory of the hypervolume
indicator: optimal µ-distributions and the choice of the reference point. In Proceedings of the
tenth ACM SIGEVO workshop on Foundations of genetic algorithms, pages 87–102, 2009."
REFERENCES,0.38010657193605685,"[5] Sterling G Baird, Ramsey Issa, and Taylor D Sparks. Materials science optimization benchmark
dataset for multi-objective, multi-fidelity optimization of hard-sphere packing simulations. Data
in Brief, 50:109487, 2023."
REFERENCES,0.38188277087033745,"[6] Nicola Beume, Boris Naujoks, and Michael Emmerich. Sms-emoa: Multiobjective selection
based on dominated hypervolume. European Journal of Operational Research, 181(3):1653–
1669, 2007."
REFERENCES,0.3836589698046181,"[7] J. Blank and K. Deb. pymoo: Multi-objective optimization in python. IEEE Access, 8:89497–
89509, 2020."
REFERENCES,0.38543516873889877,"[8] S Borodachov, D Hardin, and E Saff. Asymptotics of best-packing on rectifiable sets. Proceed-
ings of the American Mathematical Society, 135(8):2369–2380, 2007."
REFERENCES,0.3872113676731794,"[9] Weiyu Chen, Hisao Ishibuchi, and Ke Shang. Fast greedy subset selection from large candidate
solution sets in evolutionary multiobjective optimization. IEEE Transactions on Evolutionary
Computation, 26(4):750–764, 2021."
REFERENCES,0.38898756660746003,"[10] Eng Ung Choo and Derek R Atkins. Proper efficiency in nonconvex multicriteria programming.
Mathematics of Operations Research, 8(3):467–470, 1983."
REFERENCES,0.3907637655417407,"[11] Paolo Climaco and Jochen Garcke. On minimizing the training set fill distance in machine
learning regression, 2024. URL https://arxiv.org/abs/2307.10988."
REFERENCES,0.3925399644760213,"[12] Indraneel Das and John E Dennis. Normal-boundary intersection: A new method for gener-
ating the pareto surface in nonlinear multicriteria optimization problems. SIAM journal on
optimization, 8(3):631–657, 1998."
REFERENCES,0.39431616341030196,"[13] Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm
using reference-point-based nondominated sorting approach, part i: Solving problems with
box constraints. IEEE Transactions on Evolutionary Computation, 18(4):577–601, 2014. doi:
10.1109/TEVC.2013.2281535."
REFERENCES,0.3960923623445826,"[14] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist
multiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6
(2):182–197, 2002."
REFERENCES,0.3978685612788632,"[15] Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable multi-objective
optimization test problems. In Proceedings of the 2002 Congress on Evolutionary Computation.
CEC’02 (Cat. No. 02TH8600), volume 1, pages 825–830. IEEE, 2002."
REFERENCES,0.3996447602131439,"[16] Kalyanmoy Deb, Ankur Sinha, and Saku Kukkonen. Multi-objective test problems, linkages,
and evolutionary methodologies. In Proceedings of the 8th annual conference on Genetic and
evolutionary computation, pages 1141–1148, 2006."
REFERENCES,0.40142095914742454,"[17] Timo M Deist, Monika Grewal, Frank JWM Dankers, Tanja Alderliesten, and Peter AN Bosman.
Multi-objective learning to predict pareto fronts using hypervolume maximization. arXiv
preprint arXiv:2102.04523, 2021."
REFERENCES,0.40319715808170514,"[18] Justin Dumouchelle, Esther Julien, Jannis Kurtz, and Elias B Khalil. Neur2bilo: Neural bilevel
optimization. arXiv preprint arXiv:2402.02552, 2024."
REFERENCES,0.4049733570159858,"[19] Matthias Ehrgott. Multicriteria optimization, volume 491. Springer Science & Business Media,
2005."
REFERENCES,0.4067495559502664,"[20] Abhijith M Gopakumar, Prasanna V Balachandran, Dezhen Xue, James E Gubernatis, and Turab
Lookman. Multi-objective optimization for materials discovery via adaptive design. Scientific
reports, 8(1):3738, 2018."
REFERENCES,0.40852575488454707,"[21] Fang-Qing Gu and Hai-Lin Liu. A novel weight design in multi-objective evolutionary algorithm.
In 2010 International Conference on Computational Intelligence and Security, pages 137–141.
IEEE, 2010."
REFERENCES,0.4103019538188277,"[22] Fangqing Gu and Yiu-Ming Cheung.
Self-organizing map-based weight design for
decomposition-based many-objective evolutionary algorithm. IEEE Transactions on Evo-
lutionary Computation, 22(2):211–225, 2017."
REFERENCES,0.41207815275310833,"[23] Yu-Ran Gu, Chao Bian, Miqing Li, and Chao Qian. Subset selection for evolutionary multi-
objective optimization. IEEE Transactions on Evolutionary Computation, 2023."
REFERENCES,0.413854351687389,"[24] Andreia P Guerreiro, Carlos M Fonseca, and Luís Paquete.
The hypervolume indicator:
Problems and algorithms. arXiv preprint arXiv:2005.00515, 2020."
REFERENCES,0.41563055062166965,"[25] Claus Hillermeier. Generalized homotopy approach to multiobjective optimization. Journal of
Optimization Theory and Applications, 110(3):557–583, 2001."
REFERENCES,0.41740674955595025,"[26] Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance
calculation in generational distance and inverted generational distance. In Evolutionary Multi-
Criterion Optimization: 8th International Conference, EMO 2015, Guimarães, Portugal, March
29–April 1, 2015. Proceedings, Part II 8, pages 110–125. Springer, 2015."
REFERENCES,0.4191829484902309,"[27] Himanshu Jain and Kalyanmoy Deb. An evolutionary many-objective optimization algorithm
using reference-point based nondominated sorting approach, part ii: Handling constraints and
extending to an adaptive approach. IEEE Transactions on Evolutionary Computation, 18(4):
602–622, 2014. doi: 10.1109/TEVC.2013.2281534."
REFERENCES,0.42095914742451157,"[28] Vishal Kaushal, Ganesh Ramakrishnan, and Rishabh Iyer. Submodlib: A submodular optimiza-
tion library. arXiv preprint arXiv:2202.10680, 2022."
REFERENCES,0.4227353463587922,"[29] Naime Ranjbar Kermany. Towards Fairness-aware Multi-Objective Recommendation Systems.
PhD thesis, Macquarie University, 2024."
REFERENCES,0.42451154529307283,"[30] Beichen Li, Bolei Deng, Wan Shou, Tae-Hyun Oh, Yuanming Hu, Yiyue Luo, Liang Shi,
and Wojciech Matusik. Computational discovery of microstructured composites with optimal
stiffness-toughness trade-offs, 2024."
REFERENCES,0.42628774422735344,"[31] Qiuzhen Lin, Xiaozhou Wang, Bishan Hu, Lijia Ma, Fei Chen, Jianqiang Li, and Carlos A
Coello Coello. Multiobjective personalized recommendation algorithm using extreme point
guided evolutionary computation. Complexity, 2018:1–18, 2018."
REFERENCES,0.4280639431616341,"[32] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task
learning. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.42984014209591476,"[33] Yajing Liu, Edwin KP Chong, Ali Pezeshki, and Zhenliang Zhang. Submodular optimization
problems and greedy strategies: A survey. Discrete Event Dynamic Systems, 30(3):381–412,
2020."
REFERENCES,0.43161634103019536,"[34] Yiping Liu, Hisao Ishibuchi, Naoki Masuyama, and Yusuke Nojima. Adapting reference
vectors and scalarizing functions by growing neural gas to handle irregular pareto fronts. IEEE
Transactions on Evolutionary Computation, 24(3):439–453, 2019."
REFERENCES,0.433392539964476,"[35] Xiaoliang Ma, Yanan Yu, Xiaodong Li, Yutao Qi, and Zexuan Zhu. A survey of weight vector
adjustment methods for decomposition-based multiobjective evolutionary algorithms. IEEE
Transactions on Evolutionary Computation, 24(4):634–649, 2020."
REFERENCES,0.4351687388987567,"[36] Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient
descent with controlled ascent in pareto optimization. In International Conference on Machine
Learning, pages 6597–6607. PMLR, 2020."
REFERENCES,0.4369449378330373,"[37] Kaisa Miettinen. Nonlinear multiobjective optimization, volume 12. Springer Science &
Business Media, 1999."
REFERENCES,0.43872113676731794,"[38] Martin Pilát and Roman Neruda. General tuning of weights in moea/d. In 2016 IEEE Congress
on Evolutionary Computation (CEC), pages 965–972. IEEE, 2016."
REFERENCES,0.4404973357015986,"[39] Luc Pronzato. Minimax and maximin space-filling designs: some properties and methods for
construction. Journal de la Société Française de Statistique, 158(1):7–36, 2017."
REFERENCES,0.4422735346358792,"[40] Yutao Qi, Xiaoliang Ma, Fang Liu, Licheng Jiao, Jianyong Sun, and Jianshe Wu. Moea/d with
adaptive weight adjustment. Evolutionary computation, 22(2):231–264, 2014."
REFERENCES,0.44404973357015987,"[41] Michael Ruchte and Josif Grabocka. Scalable pareto front approximation for deep multi-
objective learning. In 2021 IEEE international conference on data mining (ICDM), pages
1306–1311. IEEE, 2021."
REFERENCES,0.44582593250444047,"[42] Serpil Sayın. Measuring the quality of discrete representations of efficient sets in multiple
objective mathematical programming. Mathematical Programming, 87:543–560, 2000."
REFERENCES,0.44760213143872113,"[43] Jason R Schott. Fault tolerant design using single and multicriteria genetic algorithm optimiza-
tion. 1995."
REFERENCES,0.4493783303730018,"[44] Adriana Schulz, Jie Xu, Bo Zhu, Changxi Zheng, Eitan Grinspun, and Wojciech Matusik.
Interactive design space exploration and optimization for cad models. ACM Transactions on
Graphics (TOG), 36(4):1–14, 2017."
REFERENCES,0.4511545293072824,"[45] Ke Shang, Tianye Shu, Hisao Ishibuchi, Yang Nan, and Lie Meng Pang. Benchmarking large-
scale subset selection in evolutionary multi-objective optimization. Information Sciences, 622:
755–770, 2023."
REFERENCES,0.45293072824156305,"[46] Bofeng Shi, Turab Lookman, and Dezhen Xue. Multi-objective optimization and its application
in materials science. Materials Genome Engineering Advances, 1(2):e14, 2023."
REFERENCES,0.4547069271758437,"[47] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: From
classical to evolutionary approaches and applications. IEEE transactions on evolutionary
computation, 22(2):276–295, 2017."
REFERENCES,0.4564831261101243,"[48] Jiang Siwei, Cai Zhihua, Zhang Jie, and Ong Yew-Soon. Multiobjective optimization by
decomposition with pareto-adaptive weight vectors. In 2011 Seventh international conference
on natural computation, volume 3, pages 1260–1264. IEEE, 2011."
REFERENCES,0.458259325044405,"[49] Yanan Sun, Gary G Yen, and Zhang Yi. Igd indicator-based evolutionary algorithm for many-
objective optimization problems. IEEE Transactions on Evolutionary Computation, 23(2):
173–187, 2018."
REFERENCES,0.46003552397868563,"[50] Ryoji Tanabe and Hisao Ishibuchi. An easy-to-use real-world multi-objective optimization
problem suite. Applied Soft Computing, 89:106078, 2020."
REFERENCES,0.46181172291296624,"[51] Ye Tian, Langchun Si, Xingyi Zhang, Kay Chen Tan, and Yaochu Jin. Local model-based
pareto front estimation for multiobjective optimization. IEEE Transactions on Systems, Man,
and Cybernetics: Systems, 53(1):623–634, 2022."
REFERENCES,0.4635879218472469,"[52] Lihui Wang, Amos HC Ng, and Kalyanmoy Deb. Multi-objective evolutionary optimisation for
product design and manufacturing. Springer, 2011."
REFERENCES,0.46536412078152756,"[53] Zihan Wang, Bochao Mao, Hao Hao, Wenjing Hong, Chunyun Xiao, and Aimin Zhou. En-
hancing diversity by local subset selection in evolutionary multiobjective optimization. IEEE
Transactions on Evolutionary Computation, 2022."
REFERENCES,0.46714031971580816,"[54] Jeff Wu. Space-filling designs, 2016. URL https://www2.isye.gatech.edu/~jeffwu/
isye8813/spacefilling_designs.pdf."
REFERENCES,0.4689165186500888,"[55] Mengyuan Wu, Sam Kwong, Yuheng Jia, Ke Li, and Qingfu Zhang. Adaptive weights generation
for decomposition-based multi-objective optimization using gaussian process regression. In
Proceedings of the Genetic and Evolutionary Computation Conference, pages 641–648, 2017."
REFERENCES,0.4706927175843694,"[56] Jie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech Matusik.
Prediction-guided multi-objective reinforcement learning for continuous robot control. In
International conference on machine learning, pages 10607–10616. PMLR, 2020."
REFERENCES,0.4724689165186501,"[57] Jie Xu, Andrew Spielberg, Allan Zhao, Daniela Rus, and Wojciech Matusik. Multi-objective
graph heuristic search for terrestrial robot design. In 2021 IEEE international conference on
robotics and automation (ICRA), pages 9863–9869. IEEE, 2021."
REFERENCES,0.47424511545293074,"[58] Qingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decom-
position. IEEE Transactions on evolutionary computation, 11(6):712–731, 2007."
REFERENCES,0.47602131438721135,"[59] Xiaoyuan Zhang, Xi Lin, and Qingfu Zhang. Pmgda: A preference-based multiple gradient
descent algorithm. IEEE Transactions on Emerging Topics in Computational Intelligence, 2024."
REFERENCES,0.477797513321492,"[60] Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, and Sijia Liu.
An introduction to bi-level optimization: Foundations and applications in signal processing and
machine learning. arXiv preprint arXiv:2308.00788, 2023."
REFERENCES,0.47957371225577267,"[61] Yong Zheng and David Xuejun Wang. A survey of recommender systems with multi-objective
optimization. Neurocomputing, 474:141–153, 2022."
REFERENCES,0.48134991119005327,Supplementary Material
REFERENCES,0.48312611012433393,Table of Contents
REFERENCES,0.4849023090586146,"A Complete proofs of theoretical results
16
A.1
Upper and lower bounds for space filling design
. . . . . . . . . . . . . . . . .
16
A.2
Configuration of dPack for bi-objective problems . . . . . . . . . . . . . . . . .
16
A.3 Theoretical results for optimization bounds . . . . . . . . . . . . . . . . . . . .
18
A.4
Proof of the “equivalent conversion” argument . . . . . . . . . . . . . . . . . .
18"
REFERENCES,0.4866785079928952,"B
Experiment details
19
B.1
Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
B.2
Full name of multiobjective methods
. . . . . . . . . . . . . . . . . . . . . . .
21
B.3
Detailed hyperparameters and licences
. . . . . . . . . . . . . . . . . . . . . .
21
B.4
Visualization for four objective problems . . . . . . . . . . . . . . . . . . . . .
22
B.5
Full numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
B.6
Results on DTLZ5 and DTLZ6 . . . . . . . . . . . . . . . . . . . . . . . . . .
25"
REFERENCES,0.48845470692717585,"C Method details
26
C.1
Practical algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
C.2
Problem formulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
C.3
Conversion between a preference and a preference angle . . . . . . . . . . . . .
27
C.4
Baseline methods used in fairness classification problem . . . . . . . . . . . . .
28
C.5
Duplicated solutions issues caused by the mTche aggregation function . . . . . .
28"
REFERENCES,0.49023090586145646,"D Miscellanies
29
D.1
Broader impacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
D.2
Limitations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29"
REFERENCES,0.4920071047957371,The appendix comprises four sections:
REFERENCES,0.4937833037300178,1. Appendix A: Detailed proofs for the main paper’s conclusions.
REFERENCES,0.4955595026642984,2. Appendix B: Additional experimental details.
REFERENCES,0.49733570159857904,3. Appendix C: Method details omitted from the main paper.
REFERENCES,0.4991119005328597,4. Appendix D: Discussion on the broader impact of our method.
REFERENCES,0.5008880994671403,"A
Complete proofs of theoretical results"
REFERENCES,0.5026642984014209,"This section provides complete proofs for the theoretical results. We first provide the properties for
max-packing and space-filling designs in Appendices A.1 and A.2. Lastly, in Appendix A.3, we
prove for the bi-level optimization bound by using neural network as an approximation for the inner
loop optimization problem."
REFERENCES,0.5044404973357016,"A.1
Upper and lower bounds for space filling design"
REFERENCES,0.5062166962699822,"In the following content, we proves for Theorem 3 in the main paper, i.e.,"
REFERENCES,0.5079928952042628,"1
4dPack ≤dFD ≤FD(YPack) ≤dPack,"
REFERENCES,0.5097690941385435,"Proof. Following the derivation to attain Equation (3) in Pronzato [39], we can prove dFD ≤dPack
as well as the claim that the fill distance between T and the optimal design YPack induced by dPack
is upper bounded by dPack."
REFERENCES,0.5115452930728241,"Next, we prove dFD ≥1"
REFERENCES,0.5133214920071048,"4dPack by contradiction. Consider YFD is the design that induces dFD, we
have that each point in YPack must be within a dFD-ball centered at a point in YFD, and the condition
dFD < 1"
REFERENCES,0.5150976909413855,4dPack requires that a specific ball will only contain a single y(k) ∈YPack otherwise YPack
REFERENCES,0.5168738898756661,will not be a dPack-packing.
REFERENCES,0.5186500888099467,Since dFD < 1
REFERENCES,0.5204262877442274,"4dPack, for all k ∈[K] the corresponding dFD-ball is completely contained in the
larger (dPack/2)-ball centered at y(k). However, we note T is connected, and thus there exists a
certain y ∈T outside all the K (dPack/2)-balls; this certain point will not be covered by all the
dFD-ball centered at points in YFD as well. Finally, the existence of the certain point contradicts the
claim that YFD is a dFD-covering."
REFERENCES,0.522202486678508,"Lastly, we prove for Theorem 4 in the main paper by a contradiction."
REFERENCES,0.5239786856127886,"Proof. Since dPack
K+1 > dPack
K
, K is the maximal packing number under distance dPack
K
."
REFERENCES,0.5257548845470693,"Assume YPack
K
is not a dPack
K
-covering. Then there exists y(K+1) such that ρ(y(K+1), y(i)) < dPack
K
for all i ∈[K]. This contradicts the assumption, as YPack
K
∪{y(K+1)} forms a dPack
K
-covering,
implying a packing number of K + 1 when K was maximal. Thus, YPack
K
is a dPack
K
-covering."
REFERENCES,0.5275310834813499,"A.2
Configuration of dPack for bi-objective problems"
REFERENCES,0.5293072824156305,"In the following, we prove Theorem 5 in the main paper. Before that, we prove for Lemma 8 to
describe a property of ρ(y(1), y(2)), the distance between y(1) and y(2) when y(1), y(2) ∈T ."
REFERENCES,0.5310834813499112,"Lemma 8. For biobjective problem function g is strictly decreasing with respect to the first element
(y(1)
1 ) of y(1) and strictly increasing with the first element (y(2)
1 ) of y(2)."
REFERENCES,0.5328596802841918,"Proof. We consider a new vector ˜y(1) ∈T such that ˜y(1)
1
< y(1)
1 . Since ˜y(1) ∈T , y(1) and ˜y(1)"
REFERENCES,0.5346358792184724,"cannot dominate each other, implying ˜y(1)
2
> y(1)
2 . The distance between this new solution ˜y(1) and
y(2) is:"
REFERENCES,0.5364120781527532,"ρ(˜y(1), y(2)) =
q"
REFERENCES,0.5381882770870338,"(˜y(1)
1
−y(2)
1 )2 + (˜y(1)
2
−y(2)
2 )2 =
q"
REFERENCES,0.5399644760213144,"(y(1)
1
−δ1 −y(2)
1 )2 + (y(1)
2
+ δ2 −y(2)
2 )2
(δ1, δ2 > 0) =
q"
REFERENCES,0.5417406749555951,"(y(1)
1
−y(2)
1 )2 + δ2
1 −2δ1(y(1)
1
−y(2)
1 ) + (y(1)
2
−y(2)
2 )2 + δ2
2 + 2δ2(y(1)
2
−y(2)
2 ) ≥
q"
REFERENCES,0.5435168738898757,"(y(1)
1
−y(2)
1 )2 + (y(1)
2
−y(2)
2 )2 + C
(C > 0)"
REFERENCES,0.5452930728241563,"> ρ(y(1), y(2)).
(11)"
REFERENCES,0.5470692717584369,"The previous equations show that ρ(y(1), y(2)) is strictly decreasing with respect to the first element
of y(1). Similarly, considering a new vector ˜y(2) ∈T , ˜y(2)
1
> y(2)
1 (˜y(2)
2
< y(2)
2 ), using the same
calculation method used in Equation (11), it is proved that ρ(y(1), ˜y(2)) > ρ(y(1), y(2)). This
indicates that ρ(y(1), y(2)) is strictly increasing with respect to the first element of y(2)."
REFERENCES,0.5488454706927176,"With this established Lemma, we are now geared up for the complete proof."
REFERENCES,0.5506216696269982,Proof. The proof consists of two parts.
REFERENCES,0.5523978685612788,"Part 1 proves that the neighboring distances ρ(y(i), y(i+1)) are equal for all i ∈[K −1]."
REFERENCES,0.5541740674955595,Part 2 proves that y(1) = p(1) and y(K) = p(2). p(1) and p(2) are the two endpoints of a PF.
REFERENCES,0.5559502664298401,"Part 1.
We prove ρ(y(1), y(2)) = . . . = ρ(y(K−1), y(K)) by contradiction. We denote d(i) the
distance between y(i) and y(i+1) and i′ and j′ are two indices such that
(
d(i′) > d(i′+1), . . . , d(j′),"
REFERENCES,0.5577264653641207,"d(i′+1), . . . , d(j′−1) > d(j′).
(12)"
REFERENCES,0.5595026642984015,"Without loss of generality, assume i′ < j′. We now aim to derive a contradiction under the
condition given by Equation (12). The approach is to iteratively decrease the first element of
y(i′+1) by a small margin ε > 0, yielding ˜y(i′+1). By Lemma 8, this adjustment ensures that
ρ(y(i′), ˜y(i′+1)) < ρ(y(i′), y(i′+1)) while ρ(˜y(i+1), y(i+2)) > ρ(y(i+1), y(i+2)). Specifically, for
each k such that i′ + 1 ≤k ≤j′ −2, y(k) is updated to ˜y(k) according to the following rules:"
REFERENCES,0.5612788632326821,"


 

"
REFERENCES,0.5630550621669627,"ρ(y(i′), y(i′+1)) −ε = ρ(˜y(i′), ˜y(i′+1)),"
REFERENCES,0.5648312611012434,"ρ(y(k), y(k+1)) = ρ(˜y(k), ˜y(k+1)),
i′ + 1 ≤k ≤j′ −2,"
REFERENCES,0.566607460035524,"ρ(y(j′−1), y(j′)) + ϵ = ρ(˜y(j′−1), ˜y(j′)). (13)"
REFERENCES,0.5683836589698046,"When ε is sufficiently small, ˜j′ = arg min{ρ(˜y(i′), ˜y(i′+1)), . . . , ρ(˜y(j′−1), ˜y(j′))} = j′, meaning
the minimal index before and after adjustment remains unchanged. However, the value of the adjusted
distance ρ(˜y(j′), ˜y(j′+1)) is increased from ρ(y(j′), y(j′+1)) by ε, showing the original design is not
a max-packing design, leading to a contradiction. If conditions
(
d(i′) < d(i′+1), . . . , d(j′),"
REFERENCES,0.5701598579040853,"d(i′+1), . . . , d(j′−1) < d(j′),
(14)"
REFERENCES,0.5719360568383659,"hold, a similar argument as above can be used to derive a contradiction. Since the selection of i′ and j′
is arbitrary, this implies that for any interval between i′ and j′, both Equation (12) and Equation (14)
cannot hold simultaneously. This leads to the following equality, which aligns with our intended
design goal:
ρ(y(1), y(2)) = . . . = ρ(y(K−1), y(K)).
(15)"
REFERENCES,0.5737122557726465,"Remark. The key component of our proof is the validity of Lemma 8, which applies to a two-dim
PF only. This leads to a contradiction with both Equation (12) and Equation (14), forming the core of
our argument."
REFERENCES,0.5754884547069272,"Part 2.
We prove y(1)
= p(1) by contradiction.
The proof for y(K)
= p(2) follows
similarly.
Assuming y(1) ̸= p(1), we replace y(1) with p(1), forming a new configuration
[˜y(1)(p(1)), y(2), . . . , y(K)]. Based on Part 1, where we showed equal neighboring distances between
vectors, the condition"
REFERENCES,0.5772646536412078,"ρ(˜y(1), y(2)) > ρ(y(2), y(3)) = . . . = ρ(y(K−1), y(K))"
REFERENCES,0.5790408525754884,holds.
REFERENCES,0.5808170515097691,"Next, we replace y(2), . . . , y(K−1) with ˜y(2), . . . , ˜y(K−1) on the PF, such that ˜y(i)
1
= y(i)
1
−ϵ(i), for
2 ≤i ≤K −1, ensuring"
REFERENCES,0.5825932504440497,"ρ(˜y(i), ˜y(i+1)) > ρ(y(i), y(i+1)),
i ∈[K −1]."
REFERENCES,0.5843694493783304,"By Lemma 8, moving y(1) to ˜y(1) results in ρ(˜y(1), y(2)) > ρ(y(1), y(2)). Iteratively shifting
y(2), . . . , y(K−1) ensures ˜y(2)
1
< y(2)
1
and so on, until ˜y(K−1)
1
< y(K−1)
1
, completing the process."
REFERENCES,0.5861456483126111,"This leads to a contradiction, proving that y(1) = p(1) and y(K) = p(2)."
REFERENCES,0.5879218472468917,"A.3
Theoretical results for optimization bounds"
REFERENCES,0.5896980461811723,"In this part, we prove for Theorem 7, which bounds the optimization error caused by neural network
in the bi-level optimization problem (Equation (10))."
REFERENCES,0.5914742451154529,"Proof. Consider the function ρ(·, ·), which measures the distance between two vectors. Given our
assumption, we derive the error between distances computed under h and hϕ. For any two points
y(i), y(j) in the image of h, the error in their distances compared to hϕ can be bounded as follows:"
REFERENCES,0.5932504440497336,"|ρ(y(i), y(j)) −ρ(hϕ(ϑϑϑ(i)), hϕ(ϑϑϑ(j)))| = |∥y(i) −y(j)∥−∥hϕ(ϑϑϑ(i)) −hϕ(ϑϑϑ(j))∥|"
REFERENCES,0.5950266429840142,≤∥y(i) −hϕ(ϑϑϑ(i)) + hϕ(ϑϑϑ(j)) −y(j)∥
REFERENCES,0.5968028419182948,"≤∥y(i) −hϕ(ϑϑϑ(i))∥+ ∥hϕ(ϑϑϑ(j)) −y(j)∥
≤ϵ + ϵ = 2ϵ. (16)"
REFERENCES,0.5985790408525755,This follows from the triangle inequality and the assumption |hϕ(ϑϑϑ) −h(ϑϑϑ)| ≤ϵ.
REFERENCES,0.6003552397868561,"Given this pairwise bound, the overall configuration of points is such that the minimization of
distances among points under hϕ will either match or exceed the minimization under h within the
bounds of 2ϵ:"
REFERENCES,0.6021314387211367,"min
1≤i<j≤K ρ(hϕ(ϑϑϑ(i)), hϕ(ϑϑϑ(j))) ≤
min
1≤i<j≤K ρ(y(i), y(j)) + 2ϵ.
(17)"
REFERENCES,0.6039076376554174,"Considering the maximization of these minimum distances across all configurations ϑϑϑ(1), . . . ,ϑϑϑ(K),
we have:"
REFERENCES,0.605683836589698,"max
ϑϑϑ(1),...,ϑϑϑ(K)
min
1≤i<j≤K ρ(hϕ(ϑϑϑ(i)), hϕ(ϑϑϑ(j))) −
max
ϑϑϑ(1),...,ϑϑϑ(K)
min
1≤i<j≤K ρ(y(i), y(j))"
REFERENCES,0.6074600355239786,"≤
max
ϑϑϑ(1),...,ϑϑϑ(K) 2ϵ = 2ϵ.
(18)"
REFERENCES,0.6092362344582594,"Thus, the error in the optimal maximal minimal distance under the model transformation can be
bounded by 2ϵ, completing the proof."
REFERENCES,0.61101243339254,"A.4
Proof of the “equivalent conversion” argument"
REFERENCES,0.6127886323268206,This argument is actually a direct corollary the following two lemmas.
REFERENCES,0.6145648312611013,"Lemma 9 (Adapted from [10], Theorem 3.1). A solution x is weakly Pareto optimal iff there exists a
weight vector λ such that x is (one of) an optimal solution of the modified Tchebycheff function."
REFERENCES,0.6163410301953819,Figure 8: Results on RE41.
REFERENCES,0.6181172291296625,"Lemma 10 (Modified from [37], Theorem 2.6.2). If an aggregation function is decreasing w.r.t.
vector f(x) (i.e., gλ(f(x)) ≤gλ(f(x′)) when fi(x) ≤fi(x′), ∀i ∈[m] and at least one index j
fj(x) < fj(x′), then one of the optimal solution x∗of gλ(f(x)) is a weakly Pareto optimal solution
for the original MOP. In addition, if the optimality is unique, x∗is Pareto optimal."
REFERENCES,0.6198934280639432,"Based on the first Lemma, we know that for any weakly Pareto objective, there is a corresponding
preference vector that solving the modified Tchebycheff function can recover this vector. Furthermore,
since we assume the uniqueness of the optimality of the modified Tchebycheff function, thus according
to the second lemma, solving the modified Tchebycheff function only yields Pareto optimal solutions.
Combining these two arguments, we achieve that for any Pareto optimal objective, there exists a
preference vector such that solving the corresponding modified Tchebycheff function yield this Pareto
optimal vector."
REFERENCES,0.6216696269982238,"B
Experiment details"
REFERENCES,0.6234458259325044,"This section has four parts. In Appendix B.1, we explain the metrics used in the experiments in details.
In Appendix B.3, we list the necessary hyperparameters and license. In Appendix B.4, we visualize
the results on four-objective problems by projection. Lastly, Appendix B.5 list for all numerical
results for all experiments."
REFERENCES,0.6252220248667851,"B.1
Metrics"
REFERENCES,0.6269982238010657,"To evaluate the uniformity and quality of these solutions, we use various performance indicators,
detailed and mathematically expressed below."
REFERENCES,0.6287744227353463,"1. Hypervolume (HV) (↑) [24] both assesses convergence to a PF and solution diversity. Low
HV values suggest poor convergence to the PF, while comparisons are significant when HVs are
substantially high. The hypervolume indicator measures the dominated volume by at least one"
REFERENCES,0.6305506216696269,Figure 9: Results on RE42.
REFERENCES,0.6323268206039077,objective belongs to the set Y with a reference point r.
REFERENCES,0.6341030195381883,"HVr = Vol({y|∃y′ ∈Y, y′ ⪯y ⪯r})."
REFERENCES,0.6358792184724689,2. IGD [26] indicator of a set A with a reference set Z is defined as:
REFERENCES,0.6376554174067496,"IGD(A) = 1 |Z| |Z|
X"
REFERENCES,0.6394316163410302,"i=1
di,"
REFERENCES,0.6412078152753108,where di represents the Euclidean distance from zi to the nearest distance in the set of A.
REFERENCES,0.6429840142095915,"3. Sparsity (↓) [56] is a measure calculated from the squared distances among solution vectors that
are sorted according to their non-dominance levels [14]. The mathematical definition is given by:"
REFERENCES,0.6447602131438721,"Sparsity =
1
N −1 m
X j=1 N−1
X i=1"
REFERENCES,0.6465364120781527,"
˜y(i)
j
−˜y(i+1)
j
2
,"
REFERENCES,0.6483126110124334,"where ˜y(i) are the objective vectors arranged in a non-dominated sorting order from the set
{y(1), . . . , y(N)}. Here, m represents the number of objectives, and N is the number of solu-
tions. A lower Sparsity value indicates a more uniformly distributed set of solutions along the Pareto
front. Specifically, for a Pareto front with a two-dimensional linear shape, the sparsity indicator
reaches its minimum when the objectives are spaced equidistantly."
REFERENCES,0.650088809946714,"4. Spacing (↓) [43]: This metric assesses solution distribution uniformity by calculating the standard
deviation of ˜d(i), the minimal distance between a solution y(i) and its nearest neighbor, where d(i) =
minj∈[m],j̸=i ρ(y(i), y(j)). A lower spacing indicator implies evenly spaced solutions, reflecting
uniform distribution. A spacing indicator of zero indicates that all Pareto objectives have equally
minimal neighborhood distances."
REFERENCES,0.6518650088809946,"Table 4: Full name table, which has three parts. The first part is related with evolutionary algorithms,
the second part is related with gradient-based methods, the third part is related to indicators, while
the last part is related to multiobjective optimization concepts."
REFERENCES,0.6536412078152753,"Short Name
Full name"
REFERENCES,0.655417406749556,"DEA-GNG
Decomposition based Evolutionary Algorithm guided by Growing Neural Gas
LMPFE
Evolutionary algorithm with Local Model based Pareto Front Estimation
NSGA3
Nondominated Sorting Genetic Algorithm 3
SMS-EMOA
S Metric Selection based Evolutionary Multiobjective Optimization Algorithm
MOEA/D
MultiObjective Evolutionary Algorithm based on Decomposition
MOEA/D-AWA
MOEA/D with Adaptive Weight Adjustment
UMOD
Uniform Multiobjective Optimization based on Decomposition"
REFERENCES,0.6571936056838366,"MOO-SVGD
MultiObjective Optimization Stein Variational Gradient Descent
EPO
Exact Pareto Optimization
PMGDA
Preference based Multiple Gradient Descent Algorithm
Agg-LS
Aggregation function based on Linear Scalarization
Agg-PBI
Aggregation function based on Penalty Based Intersection
Agg-Tche
Aggregation function based Tchebycheff Scalarization"
REFERENCES,0.6589698046181173,"IGD
Inverted General Distance
HV
HyperVolume"
REFERENCES,0.6607460035523979,"MOO
MultiObjective Optimization
MOP
Multiobjective Optimization Problem"
REFERENCES,0.6625222024866785,"5. Uniformity (↑) and Smooth Uniformity (↑) indicators, as introduced by [42], evaluate the
distribution of solutions. The Uniformity indicator, δUnif, is defined as the minimum distance
between any two solutions:"
REFERENCES,0.6642984014209592,"δUnif =
min
1≤i<j≤K ρ(y(i), y(j))."
REFERENCES,0.6660746003552398,"On the other hand, the Smooth Uniformity indicator, ˜δUnif, incorporates a logarithmic sum exponential
function to average distances among solutions. It introduces a sensitivity parameter η (set to 20 in
this study) to highlight the overall distribution of distances:"
REFERENCES,0.6678507992895204,"˜δUnif = −
2
ηK(K −1) log
X"
REFERENCES,0.6696269982238011,"1≤i<j≤K
exp(η · ρ(y(i), y(j)))."
REFERENCES,0.6714031971580817,"B.2
Full name of multiobjective methods"
REFERENCES,0.6731793960923623,"To avoid confusion, we provide the full names of the baseline MOO methods in Table 4. For
SMS-MOEA (MOEA using S-Metric Selection), the term “S-metric” refers to the “hypervolume”
metric."
REFERENCES,0.6749555950266429,"B.3
Detailed hyperparameters and licences"
REFERENCES,0.6767317939609236,"Table 5 lists the hyperparameters for implementing MOPs solved by evolutionary algorithms. The
system used features an Intel Core i7-10700 CPU and a NVIDIA RTX 3080 GPU."
REFERENCES,0.6785079928952042,"Our method is implemented in the MOEA/D framework using Pymoo [7], without modifying
the MOEA/D hyperparameters. The main difference is the use of a PF model (a fully-connected
neural network) to map preference angles to Pareto objectives and update preference vectors. The
hyperparameters for the PF model are listed in Table 5."
REFERENCES,0.6802841918294849,"For fairness classification problems, we use an additional fully connected network to classify input
features into corresponding classes. The parameters of this network serve as the decision variables
(x) for a multiobjective problem. Details of the fairness classification problems are shown in Table 6."
REFERENCES,0.6820603907637656,"Table 6: Fairness classification problem details and network architectures. Act. is the short name for
activation."
REFERENCES,0.6838365896980462,"Dataset
Features
Architecture
Act. function
# Params
Samples
Sensitivity"
REFERENCES,0.6856127886323268,"Adult
88
88-60-25-1
ReLU
6891
34188
Sex
Compass
20
20-60-25-1
ReLU
2811
4319
Sex"
REFERENCES,0.6873889875666075,Table 5: Hyper-parameters used in UMOD-MOEA
REFERENCES,0.6891651865008881,"Hyper-parameters
Values
Hyper-parameters
Values"
REFERENCES,0.6909413854351687,"Crossover
SBX (Simulated Binary Crossover)
Mutation
PM (Polynomial mutation)
SBX mating threshold
0.5
SBX offsprings
2
PM mutation probability
0.9
PFL optimizer
SGD
PFL network
(m-1)-128-128-128 →m
PFL activation function
ReLU
PFL Learning rate
1e-3
PFL training epoch
1000
Number of preferences (m = 2)
8
Number of preferences (m = 3)
21
Number of preferences (m = 4)
35
Preference initialization
Das-Dennis [12]
Number of neighborhood in MOEA/D
3
Probability of mating
0.3
Number of experiments (MOEA)
31
Number of experiments (MOML)
5
Number of fitness value (2 obj)
40,000
Number of fitness value (3 obj)
126,000
Number of fitness value (4 obj)
350,000"
REFERENCES,0.6927175843694494,"(License) To close this subsection, we would like to mention that the license used for Adult follows
Creative Commons Attribution 4.0 International (CC BY 4.0) license and Compas is supported by
Database Contents License (DbCL) v1.0 license."
REFERENCES,0.69449378330373,"B.4
Visualization for four objective problems"
REFERENCES,0.6962699822380106,"This section presents the visualization results for four-objective problems (see Figures 8 and 9). Due
to the difficulty of visualizing four-dimensional space, we project the Pareto objectives into 3-D
spaces: (f1, f2, f3), (f1, f2, f4), (f1, f3, f4), and (f2, f3, f4), labeled P-1 to P-4 in Figures 8 and 9."
REFERENCES,0.6980461811722913,"Despite losing some information in the projections, meaningful conclusions can still be drawn from
these figures:"
REFERENCES,0.6998223801065719,"1. DEAGNG and LMPFE can find partial parts of the true Pareto front. For the P1, P2, and P4
projections, DEAGNG typically finds only a small portion in the upper right of the 3-D Pareto front,
while LMPFE misses a small part of this region. In contrast, the proposed method captures a more
extensive span of the Pareto front, covering the largest area."
REFERENCES,0.7015985790408525,"2. For a four-objective problem, MOEA/D finds many duplicate Pareto objectives on the PF boundary
by using fixed preference vectors. This highlights the importance of finding optimal preference
vectors to achieve a more uniform PF."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7033747779751333,"3. The proposed UMOD method finds more uniform Pareto objectives on the Pareto front compared
to NSGA3, SMS-EMOA, and the Subset selection method. This aligns with Table 9 in the main
paper, showing UMOD has a much lower IGD indicator than these three methods."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7051509769094139,"B.5
Full numerical results"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7069271758436945,"We report the full numerical results for bi-objective, tri-objective, and four-objective problems
in Tables 7 to 9, respectively. Each experiment was conducted with 31 random seeds. For each data
point, X(Y )(C), X is the mean value, (Y ) is the standard deviation, and (C) is the rank among all
eight methods."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7087033747779752,"The hypervolume, Uniform, and Smooth Uniform (SUniform) are preferred larger, while IGD,
spacing, sparsity, and fill distances are preferred smaller. The optimal result averaged across all seeds
is marked in bold. In the last row of each table, we calculate the mean rank of each indicator, with
the highest one in bold."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7104795737122558,"The tables show that, with a small solution budget, the uniformity indicators IGD, Spacing, Uni-
form, SUniform, and FD achieve the best results, outperforming previous methods significantly."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7122557726465364,"SMS-EMOA achieves the highest HV value among all methods, but HV is only a rough mea-
sure of uniformity, validating that optimizing hypervolume alone does not ensure the best uniform
distribution."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7140319715808171,"These tables also validate the effectiveness of maximizing pairwise distances, a proper surrogate for
fill distance. This suggests that in practice, maximal packing distance can more tightly bound the
minimal fill distance. An interesting finding is that the IGD indicator, an important measure in MOO
serving as the average covering radius of the size-K optimized set, is optimized by maximizing the
pairwise distance. This interesting empirical finding is worthy of further investigation."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7158081705150977,"Table 7: Full results for biobjective problems, based on 31 random seeds, include the standard
deviation and rankings across all methods. The ranking values in the last row are averaged across all
problems."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7175843694493783,"Indicator
DEA-GNG
LMPFE
Subset
NSGA3
SMS-EMOA
MOEA/D
MOEA/D-AWA UMOD ZDT1"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7193605683836589,"HV
1.03 (0.00) (6)
1.03 (0.00) (2)
1.02 (0.00) (7)
1.03 (0.00) (5)
1.04 (0.00) (1)
1.03 (0.00) (4)
1.03 (0.00) (3)
1.04 (0.00) (0)
IGD
5.68 (0.29) (7)
5.42 (0.21) (6)
5.21 (0.40) (1)
5.28 (0.00) (3)
5.22 (0.07) (2)
5.30 (0.00) (4)
5.42 (0.02) (5)
5.19 (0.00) (0)
Spacing
6.35 (1.74) (7)
2.44 (1.82) (2)
3.51 (0.47) (3)
5.90 (0.00) (5)
1.91 (0.46) (1)
5.92 (0.01) (6)
3.69 (0.04) (4)
0.12 (0.06) (0)
Sparsity
4.80 (0.19) (7)
4.55 (0.12) (4)
2.59 (0.18) (0)
4.60 (0.00) (5)
4.48 (0.01) (2)
4.61 (0.00) (6)
4.51 (0.01) (3)
4.39 (0.00) (1)
Uniform
1.33 (0.11) (6)
1.67 (0.32) (2)
0.89 (0.10) (7)
1.53 (0.00) (3)
1.85 (0.03) (1)
1.51 (0.00) (4)
1.51 (0.00) (5)
2.07 (0.01) (0)
SUniform
0.47 (0.12) (6)
0.68 (0.13) (2)
0.09 (0.09) (7)
0.49 (0.00) (4)
0.74 (0.01) (1)
0.48 (0.00) (5)
0.53 (0.00) (3)
0.77 (0.00) (0)
Fill Distance 1.60 (0.23) (6)
1.23 (0.09) (2)
2.00 (0.18) (7)
1.55 (0.00) (5)
1.16 (0.05) (1)
1.50 (0.00) (4)
1.43 (0.03) (3)
1.04 (0.01) (0) ZDT2"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7211367673179396,"HV
0.71 (0.00) (5)
0.65 (0.07) (6)
0.63 (0.00) (7)
0.71 (0.00) (4)
0.71 (0.00) (0)
0.71 (0.00) (3)
0.71 (0.00) (2)
0.71 (0.00) (1)
IGD
5.43 (0.13) (4)
10.80 (6.71) (7) 6.69 (0.00) (6)
5.31 (0.00) (2)
5.84 (0.14) (5)
5.31 (0.00) (3)
5.29 (0.00) (1)
5.23 (0.01) (0)
Spacing
3.92 (1.97) (6)
2.11 (0.88) (1)
3.02 (0.00) (5)
3.01 (0.00) (4)
4.87 (0.53) (7)
3.01 (0.01) (3)
2.30 (0.03) (2)
0.25 (0.23) (0)
Sparsity
4.66 (0.09) (5)
8.87 (5.33) (7)
2.26 (0.00) (0)
4.54 (0.00) (3)
4.69 (0.06) (6)
4.54 (0.00) (4)
4.52 (0.00) (2)
4.45 (0.00) (1)
Uniform
1.41 (0.27) (5)
1.25 (0.64) (6)
0.85 (0.00) (7)
1.63 (0.00) (4)
1.71 (0.11) (2)
1.63 (0.00) (3)
1.83 (0.02) (1)
2.05 (0.06) (0)
SUniform
0.57 (0.10) (5)
0.16 (0.68) (6)
-0.04 (0.00) (7) 0.68 (0.00) (3)
0.63 (0.02) (4)
0.68 (0.00) (2)
0.71 (0.00) (1)
0.78 (0.00) (0)
Fill Distance 1.36 (0.11) (4)
2.82 (1.88) (7)
2.39 (0.00) (6)
1.24 (0.00) (3)
1.63 (0.09) (5)
1.24 (0.00) (2)
1.23 (0.00) (1)
1.07 (0.03) (0) ZDT3"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7229129662522202,"HV
0.91 (0.00) (0)
0.90 (0.01) (2)
0.90 (0.00) (4)
0.89 (0.00) (7)
0.91 (0.02) (1)
0.90 (0.00) (3)
0.89 (0.02) (5)
0.89 (0.02) (6)
IGD
38.60 (0.25) (1)
38.78 (0.31) (2) 38.58 (0.00) (0) 39.45 (0.03) (5) 39.23 (2.25) (4) 39.09 (0.00) (3) 39.94 (2.08) (7) 39.83 (2.10) (6)
Spacing
6.60 (1.62) (4)
3.08 (1.42) (0)
12.38 (0.00) (7) 5.12 (2.21) (3)
7.85 (0.88) (5)
5.02 (0.00) (2)
8.07 (0.98) (6)
3.57 (1.66) (1)
Sparsity
3.83 (0.16) (5)
3.83 (0.12) (6)
7.27 (0.00) (7)
3.82 (0.21) (4)
3.59 (0.50) (1)
3.65 (0.00) (2)
3.70 (0.54) (3)
3.45 (0.40) (0)
Uniform
0.89 (0.50) (2)
1.42 (0.26) (0)
0.00 (0.00) (7)
0.69 (0.54) (4)
0.75 (0.07) (3)
0.66 (0.00) (5)
0.28 (0.29) (6)
1.20 (0.19) (1)
SUniform
0.06 (0.28) (3)
0.39 (0.10) (0)
-0.91 (0.00) (7) 0.04 (0.33) (4)
-0.12 (0.09) (5) 0.09 (0.00) (2)
-0.32 (0.12) (6) 0.30 (0.04) (1)
Fill Distance 8.88 (0.02) (4)
8.87 (0.00) (0)
8.87 (0.00) (1)
8.88 (0.00) (3)
9.22 (0.71) (5)
8.88 (0.00) (2)
9.23 (0.71) (7)
9.22 (0.71) (6) ZDT4"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7246891651865008,"HV
0.75 (0.17) (7)
1.03 (0.01) (3)
1.02 (0.00) (6)
1.03 (0.00) (2)
1.04 (0.00) (0)
1.03 (0.00) (4)
1.03 (0.00) (5)
1.04 (0.00) (1)
IGD
38.29 (19.75) (7) 5.65 (0.19) (6)
5.49 (0.07) (5)
5.28 (0.00) (2)
5.19 (0.03) (0)
5.31 (0.01) (3)
5.45 (0.05) (4)
5.21 (0.01) (1)
Spacing
5.47 (2.97) (5)
4.69 (1.93) (4)
2.72 (0.22) (2)
5.91 (0.00) (6)
1.60 (0.58) (1)
5.93 (0.01) (7)
3.66 (0.03) (3)
0.22 (0.06) (0)
Sparsity
1.72 (1.93) (0)
4.71 (0.16) (7)
2.52 (0.00) (1)
4.60 (0.00) (5)
4.47 (0.01) (3)
4.62 (0.00) (6)
4.52 (0.02) (4)
4.39 (0.01) (2)
Uniform
0.41 (0.55) (7)
1.18 (0.46) (5)
1.06 (0.01) (6)
1.53 (0.00) (2)
1.90 (0.08) (1)
1.52 (0.00) (3)
1.50 (0.02) (4)
2.05 (0.01) (0)
SUniform
-0.75 (0.64) (7)
0.45 (0.22) (5)
0.10 (0.01) (6)
0.49 (0.00) (3)
0.75 (0.02) (1)
0.49 (0.00) (4)
0.53 (0.00) (2)
0.77 (0.00) (0)
Fill Distance 8.54 (3.59) (7)
1.33 (0.09) (2)
2.03 (0.02) (6)
1.54 (0.00) (5)
1.10 (0.03) (1)
1.52 (0.02) (4)
1.45 (0.01) (3)
1.06 (0.01) (0) ZDT6"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7264653641207816,"HV
0.66 (0.00) (6)
0.66 (0.00) (3)
0.44 (0.00) (7)
0.66 (0.00) (2)
0.66 (0.00) (1)
0.66 (0.00) (4)
0.66 (0.00) (5)
0.66 (0.00) (0)
IGD
4.81 (0.60) (6)
4.25 (0.09) (3)
15.28 (0.00) (7) 4.19 (0.00) (1)
4.20 (0.03) (2)
4.32 (0.00) (4)
4.37 (0.00) (5)
4.18 (0.00) (0)
Spacing
5.04 (2.57) (7)
2.52 (0.99) (5)
2.12 (0.00) (3)
1.94 (0.00) (2)
1.30 (0.37) (1)
2.48 (0.00) (4)
3.50 (0.04) (6)
0.12 (0.01) (0)
Sparsity
3.24 (0.28) (7)
2.95 (0.06) (6)
1.83 (0.00) (0)
2.89 (0.00) (3)
2.88 (0.02) (2)
2.91 (0.00) (4)
2.95 (0.00) (5)
2.86 (0.00) (1)
Uniform
0.88 (0.21) (6)
1.23 (0.19) (4)
0.79 (0.00) (7)
1.32 (0.00) (2)
1.48 (0.05) (1)
1.23 (0.00) (3)
0.97 (0.01) (5)
1.67 (0.00) (0)
SUniform
0.06 (0.15) (6)
0.26 (0.06) (4)
-0.29 (0.00) (7) 0.31 (0.00) (2)
0.33 (0.02) (1)
0.28 (0.00) (3)
0.22 (0.00) (5)
0.35 (0.00) (0)
Fill Distance 1.27 (0.30) (6)
1.05 (0.08) (5)
3.89 (0.00) (7)
0.92 (0.00) (1)
0.94 (0.05) (2)
0.97 (0.00) (3)
0.97 (0.00) (4)
0.81 (0.01) (0) RE21"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7282415630550622,"HV
1.24 (0.00) (5)
1.23 (0.01) (7)
1.24 (0.00) (4)
1.24 (0.00) (2)
1.24 (0.00) (1)
1.24 (0.00) (6)
1.24 (0.00) (3)
1.24 (0.00) (0)
IGD
4.63 (0.28) (5)
4.61 (0.13) (4)
5.15 (0.01) (6)
4.44 (0.00) (3)
4.23 (0.02) (1)
5.40 (0.02) (7)
4.33 (0.04) (2)
4.12 (0.00) (0)
Spacing
6.63 (1.16) (6)
3.62 (0.93) (4)
1.38 (0.00) (1)
5.71 (0.01) (5)
3.19 (0.37) (2)
10.49 (0.04) (7) 3.47 (0.41) (3)
0.12 (0.05) (0)
Sparsity
3.10 (0.21) (6)
2.90 (0.10) (4)
1.74 (0.00) (0)
3.02 (0.00) (5)
2.82 (0.02) (2)
3.87 (0.02) (7)
2.84 (0.01) (3)
2.70 (0.00) (1)
Uniform
0.81 (0.12) (7)
0.95 (0.16) (5)
0.95 (0.00) (4)
1.16 (0.00) (2)
1.26 (0.05) (1)
0.93 (0.00) (6)
1.11 (0.03) (3)
1.62 (0.01) (0)
SUniform
-0.03 (0.07) (5)
0.12 (0.08) (3)
-0.17 (0.00) (7) 0.08 (0.00) (4)
0.20 (0.01) (1)
-0.17 (0.00) (6) 0.13 (0.01) (2)
0.31 (0.00) (0)
Fill Distance 1.45 (0.20) (4)
1.21 (0.12) (3)
2.62 (0.01) (7)
1.47 (0.00) (5)
1.09 (0.04) (1)
2.11 (0.01) (6)
1.15 (0.02) (2)
0.83 (0.00) (0) RE22"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7300177619893428,"HV
1.17 (0.00) (7)
1.18 (0.00) (6)
1.18 (0.00) (4)
1.18 (0.00) (5)
1.18 (0.00) (0)
1.18 (0.00) (3)
1.18 (0.00) (2)
1.18 (0.00) (1)
IGD
4.63 (0.40) (5)
4.93 (0.25) (7)
4.48 (0.00) (4)
4.35 (0.00) (1)
4.42 (0.04) (2)
4.92 (0.00) (6)
4.47 (0.02) (3)
4.25 (0.02) (0)
Spacing
2.78 (0.95) (2)
3.11 (1.00) (5)
6.57 (0.00) (6)
2.90 (0.00) (3)
3.08 (0.12) (4)
6.78 (0.01) (7)
2.25 (0.64) (1)
0.34 (0.02) (0)
Sparsity
3.01 (0.34) (5)
3.26 (0.22) (7)
2.84 (0.00) (2)
2.77 (0.00) (1)
2.90 (0.03) (3)
3.12 (0.00) (6)
2.93 (0.03) (4)
2.70 (0.01) (0)
Uniform
1.08 (0.16) (2)
0.96 (0.01) (5)
0.49 (0.00) (7)
1.06 (0.00) (3)
1.03 (0.01) (4)
0.83 (0.00) (6)
1.09 (0.16) (1)
1.56 (0.01) (0)
SUniform
0.08 (0.01) (4)
0.02 (0.05) (5)
-0.12 (0.00) (7) 0.19 (0.00) (2)
0.20 (0.02) (1)
-0.06 (0.00) (6) 0.14 (0.03) (3)
0.30 (0.01) (0)
Fill Distance 1.39 (0.32) (6)
1.54 (0.14) (7)
1.33 (0.00) (5)
1.06 (0.00) (2)
1.04 (0.06) (1)
1.33 (0.00) (4)
1.29 (0.00) (3)
0.90 (0.01) (0) Rank"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7317939609236235,"HV
5.14
4.14
5.57
3.86
0.57
3.86
3.57
1.29
IGD
5
4.86
4
2.43
2.29
4.43
4
1
Spacing
5.71
2.71
3.86
3.86
3
5.14
3.57
0.14
Sparsity
5.29
5.71
1.43
3.71
2.71
4.86
3.43
0.86
Uniform
5
3.86
6.43
3
1.71
4.29
3.57
0.14
SUniform
5.14
3.57
6.86
3.14
2
4
3.14
0.14
Fill Distance 5.43
3.57
5.57
3.43
2.29
3.57
3.29
0.86"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7335701598579041,Table 8: Full numerical and ranking results on three-objective problems.
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7353463587921847,"Indicator
DEA-GNG
LMPFE
Subset
NSGA3
SMS-EMOA
MOEA/D
MOEA/D-AWA UMOD DTLZ1"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7371225577264654,"HV
1.51 (0.11) (7)
1.69 (0.00) (2)
1.69 (0.00) (6)
1.69 (0.00) (4)
1.69 (0.00) (0)
1.69 (0.00) (3)
1.69 (0.00) (5)
1.69 (0.00) (1)
IGD
14.62 (3.52) (7)
4.91 (0.10) (4)
4.94 (0.00) (6)
4.85 (0.00) (0)
4.86 (0.05) (1)
4.86 (0.01) (2)
4.93 (0.09) (5)
4.87 (0.00) (3)
Spacing
3.12 (0.87) (7)
1.14 (0.10) (3)
1.94 (0.00) (6)
0.01 (0.00) (0)
1.43 (0.35) (4)
0.02 (0.00) (1)
1.77 (1.75) (5)
0.09 (0.02) (2)
Sparsity
0.30 (0.22) (0)
0.48 (0.02) (3)
0.36 (0.00) (1)
0.74 (0.00) (6)
0.41 (0.02) (2)
0.75 (0.00) (7)
0.73 (0.01) (4)
0.73 (0.00) (5)
Uniform
0.02 (0.03) (7)
1.05 (0.05) (3)
0.68 (0.00) (6)
1.41 (0.00) (1)
0.86 (0.14) (4)
1.41 (0.00) (0)
0.81 (0.60) (5)
1.39 (0.01) (2)
SUniform
-1.99 (0.20) (7)
-0.94 (0.01) (3)
-0.99 (0.00) (6) -0.90 (0.00) (2) -0.95 (0.01) (5) -0.90 (0.00) (1) -0.95 (0.05) (4) -0.90 (0.00) (0)
Fill Distance 4.59 (0.64) (7)
1.03 (0.13) (6)
1.01 (0.00) (4)
0.77 (0.00) (0)
0.96 (0.05) (3)
0.77 (0.00) (1)
1.01 (0.28) (5)
0.78 (0.00) (2) DTLZ2"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.738898756660746,"HV
1.01 (0.01) (7)
1.05 (0.00) (6)
1.08 (0.00) (1)
1.06 (0.00) (5)
1.08 (0.00) (0)
1.06 (0.00) (3)
1.06 (0.00) (4)
1.07 (0.00) (2)
IGD
12.52 (0.64) (2)
12.46 (0.09) (1)
15.14 (0.00) (6) 12.54 (0.00) (3) 15.45 (0.25) (7) 12.55 (0.00) (5) 12.55 (0.00) (4) 12.19 (0.04) (0)
Spacing
5.29 (1.72) (2)
2.64 (0.39) (1)
8.97 (0.00) (7)
5.45 (0.00) (3)
7.15 (0.51) (6)
5.45 (0.01) (4)
5.45 (0.01) (5)
0.52 (0.21) (0)
Sparsity
1.50 (0.13) (0)
2.18 (0.13) (2)
2.42 (0.00) (3)
2.43 (0.00) (4)
2.75 (0.14) (7)
2.43 (0.00) (6)
2.43 (0.00) (5)
1.61 (0.13) (1)
Uniform
1.58 (0.19) (5)
2.45 (0.13) (1)
0.96 (0.00) (7)
2.43 (0.00) (4)
1.51 (0.15) (6)
2.43 (0.00) (3)
2.43 (0.00) (2)
3.17 (0.09) (0)
SUniform
0.36 (0.23) (5)
0.97 (0.04) (1)
-0.04 (0.00) (7) 0.95 (0.00) (4)
0.18 (0.07) (6)
0.95 (0.00) (2)
0.95 (0.00) (3)
1.18 (0.02) (0)
Fill Distance 3.20 (0.60) (5)
2.69 (0.10) (4)
3.54 (0.00) (7)
2.59 (0.00) (1)
3.42 (0.16) (6)
2.59 (0.00) (3)
2.59 (0.00) (2)
2.37 (0.08) (0) DTLZ3"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7406749555950266,"HV
0.80 (0.20) (7)
1.02 (0.02) (6)
1.08 (0.00) (1)
1.06 (0.00) (4)
1.08 (0.00) (0)
1.06 (0.00) (2)
1.06 (0.00) (3)
1.06 (0.00) (5)
IGD
31.92 (16.92) (7) 14.80 (1.03) (4)
15.15 (0.00) (5) 12.55 (0.01) (1) 15.41 (0.43) (6) 12.56 (0.01) (2) 12.57 (0.01) (3) 12.31 (0.09) (0)
Spacing
13.23 (12.33) (6) 422.58 (557.38) (7)
8.98 (0.00) (5)
5.46 (0.01) (3)
7.49 (0.50) (4)
5.45 (0.01) (2)
5.34 (0.09) (1)
1.93 (1.43) (0)
Sparsity
5.86 (5.81) (6)
5433.68 (8409.98) (7) 2.42 (0.00) (1)
2.43 (0.01) (4)
2.70 (0.17) (5)
2.43 (0.00) (3)
2.43 (0.01) (2)
1.70 (0.09) (0)
Uniform
0.23 (0.28) (7)
1.20 (0.75) (5)
0.97 (0.00) (6)
2.44 (0.00) (1)
1.48 (0.24) (4)
2.44 (0.00) (3)
2.44 (0.00) (2)
2.72 (0.54) (0)
SUniform
-1.27 (0.68) (7)
0.02 (0.57) (5)
-0.04 (0.00) (6) 0.95 (0.00) (3)
0.17 (0.09) (4)
0.95 (0.00) (2)
0.96 (0.01) (1)
1.13 (0.12) (0)
Fill Distance 7.83 (2.94) (7)
3.84 (0.55) (6)
3.54 (0.00) (5)
2.59 (0.00) (1)
3.33 (0.23) (4)
2.59 (0.00) (3)
2.59 (0.00) (2)
2.39 (0.10) (0) DTLZ4"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7424511545293073,"HV
0.96 (0.12) (7)
1.06 (0.01) (6)
1.08 (0.00) (1)
1.06 (0.00) (4)
1.08 (0.00) (0)
1.06 (0.00) (3)
1.06 (0.00) (5)
1.07 (0.00) (2)
IGD
19.84 (15.30) (7) 12.78 (0.16) (4)
15.14 (0.00) (5) 12.54 (0.00) (1) 15.48 (0.30) (6) 12.55 (0.00) (2) 12.60 (0.10) (3) 12.19 (0.07) (0)
Spacing
5.44 (0.81) (3)
2.78 (0.34) (1)
8.97 (0.00) (7)
5.45 (0.00) (5)
7.67 (0.60) (6)
5.45 (0.00) (4)
5.36 (0.17) (2)
0.83 (0.54) (0)
Sparsity
1.55 (0.22) (0)
2.20 (0.18) (2)
2.42 (0.00) (3)
2.43 (0.00) (4)
2.81 (0.13) (7)
2.43 (0.00) (5)
2.45 (0.05) (6)
1.77 (0.11) (1)
Uniform
1.26 (0.64) (6)
2.55 (0.09) (1)
0.96 (0.00) (7)
2.43 (0.00) (2)
1.38 (0.16) (5)
2.43 (0.00) (4)
2.43 (0.00) (3)
3.07 (0.15) (0)
SUniform
0.06 (0.78) (6)
1.00 (0.06) (1)
-0.04 (0.00) (7) 0.95 (0.00) (4)
0.15 (0.08) (5)
0.95 (0.00) (3)
0.97 (0.05) (2)
1.18 (0.02) (0)
Fill Distance 5.13 (4.21) (7)
2.83 (0.24) (4)
3.54 (0.00) (6)
2.59 (0.00) (1)
3.41 (0.23) (5)
2.59 (0.00) (3)
2.59 (0.00) (2)
2.41 (0.14) (0) RE37"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7442273534635879,"HV
0.98 (0.07) (6)
1.03 (0.02) (3)
1.10 (0.00) (0)
1.01 (0.01) (5)
1.07 (0.00) (1)
0.98 (0.00) (7)
1.02 (0.02) (4)
1.07 (0.01) (2)
IGD
17.20 (4.58) (7)
12.24 (0.82) (2)
12.17 (0.00) (1) 13.31 (0.27) (4) 12.86 (0.11) (3) 16.54 (0.05) (6) 14.15 (1.35) (5) 10.74 (0.20) (0)
Spacing
7.47 (2.16) (4)
5.69 (1.56) (2)
5.74 (0.01) (3)
11.92 (1.21) (7) 4.89 (1.90) (0)
8.30 (0.47) (6)
8.22 (0.95) (5)
5.25 (1.32) (1)
Sparsity
1.05 (0.50) (1)
1.52 (0.31) (4)
0.86 (0.01) (0)
1.74 (0.16) (5)
1.45 (0.10) (3)
3.08 (0.05) (7)
2.47 (0.40) (6)
1.39 (0.07) (2)
Uniform
0.23 (0.30) (4)
1.17 (0.56) (0)
0.59 (0.00) (3)
0.01 (0.02) (6)
0.99 (0.09) (1)
0.00 (0.00) (7)
0.04 (0.05) (5)
0.94 (0.53) (2)
SUniform
-1.04 (0.38) (5)
0.08 (0.33) (0)
-0.66 (0.00) (3) -0.94 (0.12) (4) -0.37 (0.04) (2) -1.55 (0.01) (7) -1.19 (0.28) (6) 0.07 (0.24) (1)
Fill Distance 5.33 (1.46) (7)
3.36 (0.93) (1)
4.56 (0.00) (4)
3.44 (0.34) (2)
3.73 (0.18) (3)
4.82 (0.04) (6)
4.80 (0.03) (5)
2.93 (0.32) (0) Rank"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7460035523978685,"HV
6.8
4.6
1.8
4.4
0.2
3.6
4.2
2.4
IGD
6
3
4.6
1.8
4.6
3.4
4
0.6
Spacing
4.4
2.8
5.6
3.6
4
3.4
3.6
0.6
Sparsity
1.4
3.6
1.6
4.6
4.8
5.6
4.6
1.8
Uniform
5.8
2
5.8
2.8
4
3.4
3.4
0.8
SUniform
6
2
5.8
3.4
4.4
3
3.2
0.2
Fill Distance 6.6
4.2
5.2
1
4.2
3.2
3.2
0.4"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7477797513321492,Table 9: Numerical results on four-objective problems.
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7495559502664298,"Indicator
DEA-GNG
LMPFE
Subset
NSGA3
SMS-EMOA
MOEA/D
MOEA/D-AWA UMOD RE41"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7513321492007105,"HV
0.63 (0.16) (7)
1.07 (0.01) (3)
1.14 (0.00) (0)
1.08 (0.00) (2)
1.00 (0.00) (5)
1.00 (0.00) (6)
1.02 (0.00) (4)
1.11 (0.00) (1)
IGD
41.21 (12.46) (7) 17.56 (1.51) (2) 15.15 (0.06) (1) 19.47 (0.76) (3)
19.57 (1.34) (4) 24.95 (0.26) (6) 23.96 (0.67) (5) 14.36 (0.34) (0)
Spacing
4.75 (0.94) (1)
4.11 (0.74) (0)
7.45 (0.07) (2)
10.81 (0.36) (7)
9.67 (0.04) (5)
9.25 (0.04) (4)
9.86 (1.66) (6)
8.16 (0.49) (3)
Sparsity
0.40 (0.11) (0)
1.06 (0.09) (4)
0.75 (0.00) (1)
0.99 (0.03) (3)
1.28 (0.19) (5)
2.24 (0.06) (7)
2.03 (0.00) (6)
0.77 (0.01) (2)
Uniform
0.07 (0.07) (4)
2.04 (0.01) (0)
1.10 (0.02) (1)
0.01 (0.01) (5)
0.16 (0.08) (3)
0.00 (0.00) (7)
0.00 (0.00) (6)
0.54 (0.34) (2)
SUniform
-1.79 (0.32) (5)
0.41 (0.03) (0)
-0.24 (0.02) (1) -1.21 (0.04) (3)
-1.31 (0.15) (4) -2.00 (0.02) (7) -1.88 (0.08) (6) -0.41 (0.17) (2)
Fill Distance 11.41 (0.12) (7)
5.74 (1.07) (5)
3.39 (0.00) (0)
5.27 (0.74) (3)
4.28 (0.52) (2)
5.71 (0.07) (4)
5.88 (0.02) (6)
4.15 (0.02) (1) RE42"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7531083481349912,"HV
0.55 (0.02) (3)
0.60 (0.01) (2)
0.61 (0.00) (1)
0.52 (0.02) (5)
0.55 (0.02) (4)
0.44 (0.00) (7)
0.51 (0.04) (6)
0.62 (0.01) (0)
IGD
28.16 (8.45) (6)
23.75 (3.02) (3) 25.33 (0.09) (4) 21.01 (2.74) (0)
22.33 (2.39) (1) 30.31 (0.70) (7) 26.16 (0.62) (5) 23.73 (1.05) (2)
Spacing
10.82 (5.18) (5)
8.51 (9.05) (3)
12.28 (0.06) (7) 11.81 (10.46) (6) 8.31 (1.18) (2)
6.52 (0.47) (0)
9.48 (1.32) (4)
8.01 (0.64) (1)
Sparsity
5.96 (7.42) (7)
2.43 (3.44) (4)
2.00 (0.01) (3)
3.86 (4.58) (6)
1.18 (0.28) (1)
3.45 (0.27) (5)
1.96 (0.51) (2)
0.77 (0.04) (0)
Uniform
0.03 (0.02) (4)
1.03 (0.34) (0)
0.00 (0.00) (6)
0.04 (0.03) (3)
0.05 (0.05) (2)
0.00 (0.00) (7)
0.00 (0.00) (5)
0.07 (0.05) (1)
SUniform
-1.47 (0.12) (4)
-0.61 (0.18) (0) -2.53 (0.00) (7) -1.45 (0.12) (3)
-1.31 (0.18) (2) -2.28 (0.04) (6) -2.12 (0.16) (5) -1.25 (0.14) (1)
MaxGD
29.05 (8.88) (1)
35.21 (5.92) (5) 35.59 (0.01) (6) 28.04 (7.95) (0)
31.62 (2.71) (2) 34.41 (1.41) (3) 34.75 (2.23) (4) 37.36 (0.61) (7)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7548845470692718,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f1"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7566607460035524,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7584369449378331,(a) UMOD
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7602131438721137,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f1"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7619893428063943,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7637655417406749,(b) AWA
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7655417406749556,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f1"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7673179396092362,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7690941385435168,(c) MOEA/D
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7708703374777975,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f1"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7726465364120781,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7744227353463587,(d) NSGA3
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7761989342806395,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f1"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7779751332149201,"0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7797513321492007,(e) SMS-EMOA
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7815275310834814,Figure 10: Comparison of using different methods on DTLZ5. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.783303730017762,(a) UMOD 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7850799289520426,(b) AWA 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7868561278863233,(c) MOEA/D 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 f1 0.0 0.2 0.4 0.6 0.8 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7886323268206039,(d) NSGA3 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 f2 0.0 0.2 0.4 0.6 0.8 1.0 f3
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7904085257548845,(e) SMS-EMOA
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7921847246891652,Figure 11: Comparison of using different methods on DTLZ6.
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7939609236234458,"B.6
Results on DTLZ5 and DTLZ6"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7957371225577264,"The PFs of DTLZ5 and DTLZ6 are identical, which are a degenerate 1-dimensional hyper-curve
within the three-objective space. The visualization results are shown in Figures 10 and 11 and
numerical results are shown in Table 10."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7975133214920072,"Table 10: Numerical results on DTLZ5 and DTLZ6 problems.
Spacing
Sparsity
HV
Uniform
Smooth Uniform
IGD
FD"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.7992895204262878,"UMOD
0.0073
0.0131
0.6995
0.0867
-0.0634
0.0294
0.085
MOEAD
0.0871
0.0645
0.6352
0
-0.1978
0.143
0.3002
AWA
0.0909
0.0312
0.6697
0
-0.1802
0.0712
0.1697
SMS-MOEA
0.0454
0.0147
0.7035
0.0783
-0.0753
0.0307
0.1099
NSGA3
0.0599
0.0289
0.6623
0.0005
-0.1417
0.0683
0.1802"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8010657193605684,"UMOD
0.0125
0.0128
0.7011
0.0738
-0.0618
0.0285
0.0731
MOEAD
0.0843
0.0599
0.6352
0
-0.2032
0.143
0.3002
AWA
0.0908
0.0312
0.6697
0
-0.1694
0.0712
0.1697
SMS-MOEA
0.0426
0.0143
0.7036
0.072
-0.0752
0.0305
0.1099
NSGA3
0.0524
0.0415
0.6623
0
-0.1357
0.0931
0.2972"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8028419182948491,"Table 10 highlights that UMOD significantly outperforms other methods in ensuring evenly dis-
tributed solutions. In the decomposition-based framework, for such degenerated problems, different
preferences may correspond to the same Pareto objective. The reason behind generating duplicate
solutions is explained in Appendix C.5. And therefore, MOEA/D and MOEA/D-AWA tend to produce
duplicate solutions. NSGA3 is also found to produce duplicate solutions easily. SMS-MOEA avoids
duplicate solutions by maximizing the hypervolume. SMS-MOEA surpasses UMOD in hypervolume,
but UMOD considerably outperforms SMS-MOEA in IGD and FD, which are of interests in this
paper."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8046181172291297,"C
Method details"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8063943161634103,"C.1
Practical algorithms"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8081705150976909,"In this section, we present practical algorithms to solve the maximal packing problem in the Pareto
front (Equation (10)). We start by generating an initial uniform distribution of preference vectors.
Then, we use either multiobjective evolutionary algorithms (MOEAs) or gradient-based MOO to
solve for the preference angle and Pareto objective pairs. MOEAs are suitable for problems with
local optimas, while gradient-based MOO is efficient for neural network problems with millions of
decision variables. Next, we fit a Pareto front model to learn the expression of hϕ and re-determine
the preference angles by maximizing the pairwise distances. These two steps are repeated alternately."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8099467140319716,Algorithm 1 Uniform Multiobjective Optimization (UMOD)
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8117229129662522,"1: Input: Initial K uniform preferences {λ(1), . . . , λ(K)} by Das-Dennis method [12]. Initial
solutions {x(1), . . . , x(K)}.
2: for n = 1 to N do
3:
Run MOEA/D with mTche aggregation function or gradient-based MOO using
{x(1), . . . , x(K)} as initial solutions under preferences {λ(1), . . . , λ(K)}.
4:
Train a model hϕ to predict Pareto objectives by the preference angles using mean square
estimation with angle-objective pairs (ϑϑϑ(i), y(i)).
5:
Update (ϑϑϑ(1), . . . ,ϑϑϑ(K)) by Algorithm 2.
6:
Recalculate preference vectors, λ(1), . . . , λ(K).
7:
Update the initial solutions {x(1), . . . , x(K)} by MOEA/D or gradient-based mTche using the
last generation of solutions as a warm start.
8: end for"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8134991119005328,Algorithm 2 Recalculate Preference Angles (ALG_Update)
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8152753108348135,"1: Input: The initial configuration {ϑϑϑ(1), . . . ,ϑϑϑ(K)} and hϕ.
2: for i = 1 to Nopt do
3:
Calculate the indexes for the minimal pairwise objectives:"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8170515097690941,"(i∗, j∗) = arg
min
1≤i<j≤K hϕ(ϑϑϑ(i),ϑϑϑ(j))."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8188277087033747,"4:
Update the positions of (ϑϑϑ(i∗),ϑϑϑ(j∗)):"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8206039076376554,"






"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.822380106571936,"





"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8241563055062167,ϑϑϑ(i∗) ←clip 
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8259325044404974,"ϑϑϑ(i∗) + η
∂hϕ(ϑϑϑ(i∗))"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.827708703374778,"∂ϑϑϑ(i∗)
)Aϕ, 0,
π 2  "
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8294849023090586,ϑϑϑ(j∗) ←clip 
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8312611012433393,"ϑϑϑ(j∗) −η
∂hϕ(ϑϑϑ(j∗))"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8330373001776199,"∂ϑϑϑ(j∗)
Aϕ, 0,
π 2  ,"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8348134991119005,"where Aϕ =
hϕ(ϑϑϑ(i∗))−hϕ(ϑϑϑ(j∗))
ρ(hϕ(ϑϑϑ(i∗)),hϕ(ϑϑϑ(j∗))) ⊤
."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8365896980461812,"5: end for
6: Output: The updated preference angles {ϑϑϑ(1), . . . ,ϑϑϑ(K)}."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8383658969804618,"C.2
Problem formulations"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8401420959147424,"For completeness, ZDT1, ZDT2, and DTLZ1 problems are described as follows."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8419182948490231,"ZDT1.










"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8436944937833037,"








"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8454706927175843,"f1(x) = x1,
f2(x) = g(x) · h(f1(x), g(x)),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8472468916518651,"g(x) = 1 +
9
n −1 n
X"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8490230905861457,"i=2
xi,"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8507992895204263,"h(f1(x), g(x)) = 1 −
p"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8525754884547069,"f1(x)/g(x),
0 ≤xi ≤1,
i ∈[n]. (19)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8543516873889876,"The PF of ZDT1 is f2 = 1 −√f1, 0 ≤f1 ≤1."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8561278863232682,"ZDT2.









"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8579040852575488,"







"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8596802841918295,"f1(x) = x1,
f2(x) = g(x) · h(f1(x), g(x)),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8614564831261101,"g(x) = 1 +
9
n −1 n
X"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8632326820603907,"i=2
xi,"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8650088809946714,"h(f1(x), g(x)) = 1 −f1(x)/g(x)2,
0 ≤xi ≤1,
i ∈[n]. (20)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.866785079928952,"The PF of ZDT2 is f2 = 1 −f 2
1 , 0 ≤f1 ≤1."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8685612788632326,"DTLZ1.














"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8703374777975134,"












"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.872113676731794,f1(x) = 1
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8738898756660746,"2x1x2(1 + g(x)),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8756660746003553,f2(x) = 1
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8774422735346359,"2x1(1 −x2)(1 + g(x)),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8792184724689165,f3(x) = 1
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8809946714031972,"2(1 −x1)(1 + g(x)),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8827708703374778,"g(x) = 100((n −2) + n
X"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8845470692717584,"i=3
(xi −0.5)2 + n
X"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8863232682060391,"i=3
cos(20π(xi −0.5))),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8880994671403197,"0 ≤xi ≤1,
i ∈[n]. (21)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8898756660746003,The PF of DTLZ1 is 0.5∆3 (3-dim simplex).
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8916518650088809,"C.3
Conversion between a preference and a preference angle"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8934280639431617,"The preference vector λ and preference angles ϑϑϑ are easily inter-convertible via the following
equations. This one-to-one, differentiable mapping allows conversion between ϑϑϑ and λ. While λ
belongs to the m-D simplex, ϑϑϑ lies within the box constraint [0, π"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8952042628774423,"2 ]m−1. For optimization purposes,
ϑϑϑ is more manageable because it can be easily projected onto [0, π"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8969804618117229,"2 ]m−1, whereas projecting λ onto
the m-D simplex is more complex."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.8987566607460036,"















"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9005328596802842,"














"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9023090586145648,"ϑ1 = arg cos(
p λ1),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9040852575488455,"ϑ2 = arg cos
 √λ2"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9058614564831261,"sin ϑ1 
,"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9076376554174067,"ϑ3 = arg cos

√λ3
sin ϑ1 sin ϑ2 
, ..."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9094138543516874,ϑm−1 = arg cos p
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.911190053285968,"λm−1
Qm−2
i=1 sin ϑi ! ,"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9129662522202486,"








"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9147424511545293,"







"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.91651865008881,"λ2 = sin2(ϑ1) cos2(ϑ2),"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9182948490230906,"λ3 = sin2(ϑ1) sin2(ϑ2) cos2(ϑ3),
... λm = m−1
Y"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9200710479573713,"i=1
sin2(ϑi). (22)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9218472468916519,"C.4
Baseline methods used in fairness classification problem"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9236234458259325,"This subsection describes baseline gradient methods for fairness classification problems (Section 5.2).
We introduce three aggregation functions:"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9253996447602132,1. Agg-LS (Linear Scalarization):
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9271758436944938,"gLS
λ (x) = m
X"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9289520426287744,"i=1
λifi(x)."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9307282415630551,2. Agg-Tche (Tchebycheff):
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9325044404973357,"gTche
λ
(x) = max
i∈[m]{λi(fi(x) −zi)},"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9342806394316163,"where z is a reference point (e.g., ideal point) which dominates the entire PF, i.e., z ⪯y, for all
y ∈T . For the modified Tchebycheff (Agg-mTche) function, the modification involves replacing λi
with
1
λi in the equation above."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9360568383658969,3. Agg-PBI (Penalty-Based Intersection):
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9378330373001776,"gPBI
λ
(x) = d1 + µd2 ="
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9396092362344582,(z −f(x))⊤λ
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9413854351687388,"∥λ∥
+ µ ∥f(x) −(z −d1λ)∥,"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9431616341030196,"where z is the same reference point as introduced. For gradient-based multiobjective optimization
methods, solution x is updated as x ←x −η ∂gλ(x)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9449378330373002,"∂x
, where η is a small positive learning rate, and
the upperscript agg denotes PBI, LS, or Tche. We also would like to briefly introduce the other
three gradient based method, PMGDA (Preference-based Multiple Gradient Descent Algorithm)
[59], EPO (Exact Pareto optimization) [36], and HVGrad (Gradient-based HV maximization method)
[17]. PMGDA and EPO employ gradient-based techniques to precisely identify Pareto solutions
at the intersection points where the Pareto objectives converge with the PF. On the other hand,
HVGrad leverages gradient ascent on the hypervolume to maximize the hypervolume metric, thereby
optimizing the overall dominance of the solution set."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9467140319715808,"C.5
Duplicated solutions issues caused by the mTche aggregation function 𝒇𝟏 𝒇𝟐 O"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9484902309058615,Pareto front
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9502664298401421,"𝒚∗
𝝀(𝟏) 𝝀(𝟐)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9520426287744227,"Figure 12: Duplicated solutions generated by Agg-mTche. Different preference vectors λ(1) and λ(2)
correspond to the same optimal objective vector y∗."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9538188277087034,"In this section, we discuss when Tchebycheff aggregation methods yield duplicated Pareto objectives.
Appendix C.5 illustrates this, showing a preference vector λ(1) intersecting the PF at the optimal
solution y∗, where
y∗
i
λi
= . . . = y∗
m
λm
.
(23)"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.955595026642984,"If the preference vector does not intersect the Pareto front (λ(2)), y∗is the Pareto front endpoint with
the highest Tchebycheff value. In Appendix C.5, y∗is the optimal value, so preferences λ(1) and
λ(2) correspond to the duplicated Pareto objective y∗."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9573712255772646,"D
Miscellanies"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9591474245115453,"D.1
Broader impacts"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9609236234458259,"By optimizing multiple conflicting objectives, these algorithms enhance decision-making in fields
such as healthcare, product design, and trustworthy machine learning. In product design, multiobjec-
tive optimization balances trade-offs between capacity and cost, facilitating the effective release of
products that represent the entire Pareto front. In trustworthy machine learning, the UMOD method
designs a series of classifiers that balance fairness and accuracy, improving our understanding of
different Pareto-optimal classifiers."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9626998223801065,"UMOD is a foundational algorithm, and its broader impact depends on its downstream applications.
We believe UMOD itself does not have a direct negative social impact."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9644760213143873,"D.2
Limitations"
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9662522202486679,"While we have illustrated UMOD’s success, we acknowledge some limitations. First, UMOD does
not address disconnected Pareto fronts such as DTLZ7, as Theorems 3 and 4 assume a connected
Pareto front to ensure uniform distribution. Second, we have not considered problems with discrete,
binary decision variables, or constrained MOPs, as our approach requires a continuous Pareto front.
Adapting UMOD to these complex MOO problems is our next goal. Finally, UMOD requires a neural
model to estimate the Pareto front shape. Although training and preference updating with neural
networks are efficient, this adds extra operations compared to traditional multiobjective algorithms,
which brings inconvenience."
THE PROPOSED UMOD METHOD FINDS MORE UNIFORM PARETO OBJECTIVES ON THE PARETO FRONT COMPARED,0.9680284191829485,NeurIPS Paper Checklist
CLAIMS,0.9698046181172292,1. Claims
CLAIMS,0.9715808170515098,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes] .
Justification: We explicitly enumerate our contributions in the end of the introduction part.
2. Limitations"
CLAIMS,0.9733570159857904,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We list the limitations in the last section (Section 6).
3. Theory Assumptions and Proofs"
CLAIMS,0.9751332149200711,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The assumptions are included in theorems and the full proofs are provided in
Appendices A.1 to A.3 respectively.
4. Experimental Result Reproducibility"
CLAIMS,0.9769094138543517,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide experiment details in Appendix B.3.
5. Open access to data and code"
CLAIMS,0.9786856127886323,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes] ."
CLAIMS,0.9804618117229129,"Justification: The model implementation and the code are attached as supplementary materi-
als. Source codes have been open to the public.
6. Experimental Setting/Details"
CLAIMS,0.9822380106571936,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes] .
Justification: The details experiment settings are provided in Appendix B.3.
7. Experiment Statistical Significance"
CLAIMS,0.9840142095914742,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes] .
Justification: All numerical results are averaged on 31 random seeds.
8. Experiments Compute Resources"
CLAIMS,0.9857904085257548,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes] .
Justification: Details are provided in Section 5 (Experiment settings).
9. Code Of Ethics"
CLAIMS,0.9875666074600356,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes] .
10. Broader Impacts"
CLAIMS,0.9893428063943162,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes] .
Social impact is discussed in Appendix D.1.
11. Safeguards"
CLAIMS,0.9911190053285968,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper is not related to LLMs and image generators.
12. Licenses for existing assets"
CLAIMS,0.9928952042628775,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes] .
Justification: Licenses are provided in Appendix B.3.
13. New Assets"
CLAIMS,0.9946714031971581,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes] .
Justification: The paper proposes a new model, and we choose the CC BY 4.0 license.
14. Crowdsourcing and Research with Human Subjects"
CLAIMS,0.9964476021314387,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA] .
Justification: This paper is not related with human subjects.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA] ."
CLAIMS,0.9982238010657194,"Justification: The proposed method is a fundamental algorithm and is not directly related to
human subjects."
