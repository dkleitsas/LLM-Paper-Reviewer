Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0024154589371980675,"Power consumption is a major obstacle in the deployment of deep neural networks
(DNNs) on end devices. Existing approaches for reducing power consumption
rely on quite general principles, including avoidance of multiplication operations
and aggressive quantization of weights and activations. However, these methods
do not take into account the precise power consumed by each module in the
network, and are therefore far from optimal. In this paper we develop accurate
power consumption models for all arithmetic operations in the DNN, under various
working conditions. Surprisingly, we reveal several important factors that have been
overlooked to date. Based on our analysis, we present PANN (power-aware neural
network), a simple approach for approximating any full-precision network by a low-
power ﬁxed-precision variant. Our method can be applied to a pre-trained network,
and can also be used during training to achieve improved performance. In contrast
to previous methods, PANN incurs only a minor degradation in accuracy w.r.t. the
full-precision version of the network, even when working at the power-budget of
a 2-bit quantized variant. In addition, our scheme enables to seamlessly traverse
the power-accuracy trade-off at deployment time, which is a major advantage over
existing quantization methods that are constrained to speciﬁc bit widths."
INTRODUCTION,0.004830917874396135,"1
INTRODUCTION"
INTRODUCTION,0.007246376811594203,"With the ever increasing popularity of deep neural networks (DNNs) for tasks like face detection,
voice recognition, and image enhancement, power consumption has become one of the major
considerations in the design of DNNs for resource-limited end-devices. Over the last several years, a
plethora of approaches have been introduced for achieving power efﬁciency in DNNs. These range
from specialized architectures (Sandler et al., 2018; Huang et al., 2019; Tan et al., 2019; Radosavovic
et al., 2020), to hardware oriented methods like multiplier-free designs and low-precision arithmetic."
INTRODUCTION,0.00966183574879227,"Multiplier aware methods attempt to reduce power consumption by avoiding the costly multiplication
operations, which dominate the computations in a DNN. Several works replaced multiplications
by additions (Courbariaux et al., 2015; Li et al., 2016; Chen et al., 2020) or by bit shift operations
(Elhoushi et al., 2019) or both (You et al., 2020). Others employed efﬁcient matrix multiplication
operators (Tschannen et al., 2018; Lavin & Gray, 2016). However, most methods in this category
introduce dedicated architectures, which require training the network from scratch. This poses a
severe limitation, as different variants of the network need to be trained for different power constraints."
INTRODUCTION,0.012077294685990338,"Low-precision DNNs reduce power consumption by using low-precision arithmetic. This is done
either via quantization-aware training (QAT) or with post-training quantization techniques. The latter
avoid the need for retraining the network but often still require access to a small number of calibration
samples in order to adapt the network’s weights. Such techniques include approaches like re-training,
ﬁne-tuning, calibration and optimization (Banner et al., 2019; Jacob et al., 2018; Nahshan et al.,
2019; Li et al., 2021). All existing methods in this category suffer from a large drop in accuracy
with respect to the full-precision version of the network, especially when working at very low bit
widths. Moreover, similarly to the multiplier-free approaches, they do not provide a mechanism for
traversing the power-accuracy trade-off without actually changing the hardware (e.g., replacing an
8-bit multiplier by a 4-bit one). In this work, we introduce a power-aware neural network (PANN)
approach that allows to dramatically cut down the power consumption of any model. Our method can
be applied at post-training to improve the power efﬁciency of a pre-trained model, or in a QAT setting"
INTRODUCTION,0.014492753623188406,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016908212560386472,"ZeroQ
PANN"
INTRODUCTION,0.01932367149758454,"ZeroQ
Unsigned PANN"
INTRODUCTION,0.021739130434782608,Unsigned PANN PANN
INTRODUCTION,0.024154589371980676,Unsigned ZeroQ ZeroQ
INTRODUCTION,0.026570048309178744,Unsigned
INTRODUCTION,0.028985507246376812,(a) Improving upon ZeroQ (4 bits) BRECQ
INTRODUCTION,0.03140096618357488,"PANN
BRECQ
Unsigned PANN"
INTRODUCTION,0.033816425120772944,Unsigned PANN PANN
INTRODUCTION,0.036231884057971016,Unsigned
INTRODUCTION,0.03864734299516908,"Unsigned
BRECQ BRECQ"
INTRODUCTION,0.04106280193236715,(b) Improving upon BRECQ (2 bits)
INTRODUCTION,0.043478260869565216,"Figure 1: Power-accuracy trade-off at post training. For each pre-trained full-precision model,
we used (a) ZeroQ (Cai et al., 2020) and (b) BRECQ (Li et al., 2021) to quantize the weights and
activations at post-training. In (a) we quantize to 4 bits and in (b) to 2 bits. Then, we convert
the quantized models to work with unsigned arithmetic (←), which already cuts down 33% of
the power consumption in (a) and 58% in (b) (assuming a 32 bit accumulator). Using our PANN
approach to quantize the weights (at post-training) and remove the multiplier, further decreases power
consumption and allows achieving higher accuracy for the same power level (↑). See more examples
in Appendix A.5.1"
INTRODUCTION,0.04589371980676329,"to obtain even improved results. Our approach is based on careful analysis of the power consumed by
additions and multiplications, as functions of several factors. We rely on bit toggling activity, which
is known to be the main factor affecting dynamic power consumption, and support our theoretical
analysis with accurate gate-level simulations on a 5nm process."
INTRODUCTION,0.04830917874396135,"Our ﬁrst important observation is that a major portion of the power consumed by a DNN is due to the
use of signed integers. We therefore present a simple method for converting any pre-trained model to
use unsigned arithmetic. This conversion does not change the functionality of the model and, as can
be seen in Fig. 1, dramatically reduces power consumption on common hardware conﬁgurations."
INTRODUCTION,0.050724637681159424,"Our second observation is that the multiplier’s power consumption is dominated by the larger bit
width among its two inputs. Therefore, although high accuracy can often be achieved with quite
drastic quantization of only the weights, this common practice turns out to be ineffective in terms of
power consumption. To be able to take advantage of drastic weight quantization, here we introduce a
method that allows removing the multiplier altogether. Our approach can work in combination with
any activation quantization method. We show theoretically and experimentally that this method is far
advantageous over existing quantization methods at low power budgets, both at post-training and in
QAT settings (see Fig. 1 and Sec. 6)."
INTRODUCTION,0.05314009661835749,"Our method allows working under any power constraint by tuning the number of additions used to
approximate each mutltiply-accumulate (MAC) operation. This is in contrast to regular quantization
methods, which are limited to particular values. We can thus traverse the power-accuracy trade-off
without changing the architecture (e.g., bit width of the multiplier), as required by existing methods."
RELATED WORK,0.05555555555555555,"2
RELATED WORK"
RELATED WORK,0.057971014492753624,"Avoiding multiplications
In ﬁxed point (integer) representation, additions are typically much more
power-efﬁcient than multiplications (Horowitz, 2014b;a). Some works suggested to binarize or
ternarize the weights to enable working with additions only (Courbariaux et al., 2015; Lin et al.,
2015; Li et al., 2016). However, this often severely impairs the network’s accuracy. Recent works
suggested to replace multiplications by bit shifts (Elhoushi et al., 2019) or additions (Chen et al.,
2020) or both (You et al., 2020). Other methods reduce the number of multiplications by inducing"
RELATED WORK,0.06038647342995169,Under review as a conference paper at ICLR 2022
RELATED WORK,0.06280193236714976,Accumulator FF
RELATED WORK,0.06521739130434782,Multiplier
RELATED WORK,0.06763285024154589,Converter
RELATED WORK,0.07004830917874397,"Figure 2: Multiply-accumulate. The multiplier
accepts two b-bit inputs. The product is then
summed with the previous B bit sum, which
awaits in the ﬂip-ﬂop (FF) register."
RELATED WORK,0.07246376811594203,"Element
toggles"
RELATED WORK,0.0748792270531401,"Multiplier inputs
0.5b+0.5b
Multiplier’s internal units
0.5b2
Accumulator input
0.5B
Accumulator sum & FF
0.5bacc+0.5bacc"
RELATED WORK,0.07729468599033816,"Table 1: Average number of bit ﬂips per signed
MAC. The b-bit multiplier inputs are drawn uni-
formly from [−2b−1, 2b−1) and its bacc = 2b bit
output is summed with the B-bit number in the FF."
RELATED WORK,0.07971014492753623,"sparsity (Venkatesh et al., 2016; Mahmoud et al., 2020), decomposition into smaller intermediate
products (Kim et al., 2016), Winograd based convolutions (Lavin & Gray, 2016), or Strassen’s matrix
multiplication algorithm (Tschannen et al., 2018). Some of these methods require internal changes in
the model, a dedicated backpropagation scheme, or other modiﬁcations to the training process.
Quantization
DNN quantization approaches include post-training quantization (PTQ), which is
applied to a pre-trained model, and quantization-aware training (QAT), where the network’s weights
are adapted to the quantization during training (Gupta et al., 2015; Louizos et al., 2018; Achterhold
et al., 2018; Esser et al., 2019). PTQ methods are more ﬂexible in that they do not require access
to the training set. These methods show optimal results for 8-bit quantization, but tend to incur a
large drop in accuracy at low bit widths. To battle this effect, some PTQ methods minimize the
quantization errors of each layer individually by optimizing the parameters over a calibration set
(Nahshan et al., 2019; Nagel et al., 2020; Hubara et al., 2020). Others use nonuniform quantization
(Liu et al., 2021; Fang et al., 2020). Effort is also invested in avoiding the need of any data sample for
calibration (Cai et al., 2020; Shoukai et al., 2020; Nagel et al., 2019; Haroush et al., 2020). These
methods, however, still show a signiﬁcant drop in accuracy at the lower bit widths, while frequently
requiring additional computational resources. Common to all quantization works is that they lack
analysis of the power consumed by each arithmetic operation as a function of bit-width, and thus
cannot strive for optimal power-accuracy trade-offs."
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.0821256038647343,"3
POWER CONSUMPTION OF A CONVENTIONAL DNN"
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.08454106280193237,"The total amount of power consumed by a logic circuit can be attributed to two main sources: a
static power component and a dynamic one. The static power is due to a constant leakage current. It
does not depend on the circuit’s activity and is typically the smaller component among the two. The
dynamic power consumed by each node in the circuit is given by P = CV 2fα, where C is the node
capacitance, V is the supply voltage, f is the operating frequency, and α is the switching activity
factor (the average number of bit ﬂips per clock) (Nasser et al., 2017). Here we focus on dynamic
power, which is a major contributor to the overall power consumption (see Appendix A.1 and (Karimi
et al., 2019; Kim et al., 2020)). Also, this is the only factor affected by the DNN architecture."
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.08695652173913043,"Most of the computation in a forward pass of a DNN can be attributed to MAC operations. As shown
in Fig. 2, MACs involve a multiplier that accepts two b-bit numbers and outputs a bacc-bit result
(bacc = 2b to account for the largest possible product), and an accumulator with a large bit width B
to which the multiplier’s output is added repeatedly."
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.0893719806763285,"To understand how much power each of these components consumes, we simulated them in Python.
For the multiplier, we used the Booth-encoding architecture, which is considered efﬁcient in terms
of bit toggling (Asif & Kong, 2015). For the accumulator, we simulated a serial adder. Our Python
simulation allows measuring the total number of bit ﬂips in each MAC operation, including at the
inputs, at the outputs, in the ﬂip-ﬂop (FF) register holding the previous sum, and within each of the
internal components (e.g., the full-adders) of the multiplier. We also veriﬁed our analysis with an
accurate physical gate-level simulation on a 5nm process and found good agreement in terms of the
dependence of power consumption on the bit widths (see details in Appendix A.1)."
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.09178743961352658,"Table 1 shows the average number of bit ﬂips per MAC when both inputs to the multiplier are
drawn uniformly at random from [−2b−1, 2b−1) (Gaussian inputs lead to similar results; please see"
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.09420289855072464,Under review as a conference paper at ICLR 2022
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.0966183574879227,"Appendix Figs 7-8). As can be seen, the power consumed by the multiplier is given by1"
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.09903381642512077,"Pmult = 0.5b2 + b,
(1)"
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.10144927536231885,"where 0.5b2 is due to the bit toggling in the internal units, and 0.5b is contributed by the bit ﬂips in
each input. The power consumed by the accumulator is given by"
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.10386473429951691,"Pacc = 0.5B + 2b,
(2)"
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.10628019323671498,"where 0.5B is due to the bit toggling in its input coming from the multiplier, 0.5bacc = b (recall
bacc = 2b) to the bit ﬂips at the output, and an additional 0.5bacc = b to the bit ﬂips in the FF. These
results lead us to our ﬁrst important observation.
Observation 1. A dominant source of power consumption is the bit toggling at the input of the
accumulator (0.5B)."
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.10869565217391304,"Suppose, for example, we use b = 4 bits for representing the weights and activations and employ a
B = 32 bit accumulator, as common in modern architectures (Kalamkar et al., 2019; Rodriguez et al.,
2018). Then the toggling at the input of the accumulator (0.5B = 16) is responsible for 44.4% of the
total power consumption (Pmult + Pacc = 36). At lower bit widths, this percentage is even larger."
POWER CONSUMPTION OF A CONVENTIONAL DNN,0.1111111111111111,"Unfortunately, existing quantization methods and multiplier-free designs do not battle this source of
power consumption. Ni et al. (2021) have recently shown that the bit-width B of the accumulator
can be somewhat reduced by explicitly accounting for overﬂows. However, this approach requires
dedicated training, and degrades the network’s classiﬁcation accuracy at low values of B. As we now
show, it is possible to drastically reduce the bit toggles at the input of the accumulator at post-training
without changing the model’s functionality (thus retaining the same classiﬁcation accuracy)."
SWITCHING TO UNSIGNED ARITHMETIC,0.11352657004830918,"4
SWITCHING TO UNSIGNED ARITHMETIC"
SWITCHING TO UNSIGNED ARITHMETIC,0.11594202898550725,"Since the output of the multiplier has only 2b bits, one could expect to experience no more than
b bit ﬂips on average at the accumulator’s input. Why do we have 0.5B bit ﬂips instead? The
reason is rooted in the use of signed arithmetic. Speciﬁcally, negative numbers are represented
using two’s complement, and thus switching between positive and negative numbers results in
ﬂipping of many of the higher bits. For example, when using a 32 bit accumulator, if the output of
the multiplier switches from +2 to −2, then the bits at the input of the accumulator switch from
00000000000000000000000000000010 to 11111111111111111111111111111110. Note that this
effect is dominant only at the accumulator’s input simply because sign changes at the output are rare."
SWITCHING TO UNSIGNED ARITHMETIC,0.11835748792270531,"If we could work with unsigned integers, then the higher bits at the accumulator’s input would
always remain zero, which would lead to a substantial reduction in power consumption without any
performance degradation. To quantify this, we repeated the experiment of Sec. 3, but with the b-bit
inputs to the multiplier now drawn uniformly from [0, 2b−1) (see Appendix A.2 for details). In this
case, the average number of bit ﬂips at the input of the accumulator reduced from 0.5B to 0.5bacc = b.
Speciﬁcally, the average power consumption of an unsigned MAC operation was measured to be"
SWITCHING TO UNSIGNED ARITHMETIC,0.12077294685990338,"P u
mult = 0.5b2 + b
(3)"
SWITCHING TO UNSIGNED ARITHMETIC,0.12318840579710146,"due to the multiplier and
P u
acc = 3b
(4)
due to the accumulator. In (4), 2b bit ﬂips occur at the accumulator’s output and the FF, and b bit ﬂips
occur at the accumulator’s input coming from the multiplier. Thus, although the mutliplier’s power (3)
turns out to be the same as in the signed setting (1), the accumulator’s power (4) is substantially
reduced w.r.t. the signed case (2)."
SWITCHING TO UNSIGNED ARITHMETIC,0.12560386473429952,"Converting a pre-trained network with ReLU activation functions to work with unsigned integers is
simple. Speciﬁcally, consider a layer performing y = Wx + b. The elements of x are non-negative
because of the preceding ReLU2. Therefore, we can split any such layer into two parallel layers as"
SWITCHING TO UNSIGNED ARITHMETIC,0.1280193236714976,"y+ = W +x + b+,
y−= W −x + b−,
(5)"
SWITCHING TO UNSIGNED ARITHMETIC,0.13043478260869565,"1The amount of power consumed by a single bit ﬂip may vary across platforms (e.g., between a 5nm and a
45nm fabrication), but the number of bit ﬂips per MAC does not change. We therefore report power in units of
bit-ﬂips, which allows comparing between implementations while disregarding the platform.
2Batch-norm layers should ﬁrst be absorbed into the weights and biases."
SWITCHING TO UNSIGNED ARITHMETIC,0.13285024154589373,Under review as a conference paper at ICLR 2022
SWITCHING TO UNSIGNED ARITHMETIC,0.13526570048309178,"where W + = ReLU(W), b+ = ReLU(b), W −= ReLU(−W), b−= ReLU(−b), and compute
y = y+ −y−.
(6)
This way, all MACs are converted to unsigned ones in (5), and only a single subtraction per output
element is needed in (6). This one subtraction is negligible w.r.t. the MACs, whose number is usually
in the thousands. Please see Fig. 11(b) in the Appendix for a schematic illustration."
SWITCHING TO UNSIGNED ARITHMETIC,0.13768115942028986,"Figure 1 shows the effect that this approach has on the power consumption of several pretrained
networks for ImageNet classiﬁcation. Figures 1(a) and 1(b) show 4 bit and 2 bit quantized networks,
respectively. With a 32 bit accumulator, merely switching to unsigned arithmetic cuts 33% and 58%
of the power consumption of these networks (see Appendix A.3.1 for other accumulator bit widths)."
REMOVING THE MULTIPLIER,0.14009661835748793,"5
REMOVING THE MULTIPLIER"
REMOVING THE MULTIPLIER,0.14251207729468598,"Having reduced the power consumed by the accumulator, we now turn to treat the multiplier. Quanti-
zation methods often use different bit widths for the weights and activations. This ﬂexibility allows
achieving good classiﬁcation accuracy with quite aggressive quantization of one of them (typically
the weights), but a ﬁner quantization of the other. An interesting question is whether this approach is
beneﬁcial in terms of power consumption."
REMOVING THE MULTIPLIER,0.14492753623188406,"We repeated the experiment of Sec. 3, this time with the multiplier inputs having different bit widths,
bw and bx. We focused on the standard setting of signed numbers, which we drew uniformly from
[−2bw, 2bw−1) and [−2bx, 2bx−1). Interestingly, we found that the average number of bit ﬂips in the
multiplier’s internal units is affected only by the larger among bw and bx. Accounting also for the bit
ﬂips at the inputs, we obtained that the multiplier’s total average power is"
REMOVING THE MULTIPLIER,0.1473429951690821,"Pmult = 0.5 (max{bw, bx})2 + 0.5(bw + bx).
(7)
We found this surprising behavior to be characteristic of both the Booth multiplier and the simple
serial multiplier, and veriﬁed it also with accurate simulations on a 5nm silicon process gate level
synthesis (see Appendix Figs. 9,10). This leads us to our second important observation.
Observation 2. There is marginal beneﬁt in the common practice of decreasing the bit width of only
the weights or only the activations, at least in terms of the power consumed by the multiplier."
REMOVING THE MULTIPLIER,0.1497584541062802,"It should be noted that in the case of unsigned numbers, where inputs are drawn uniformly from
[0, 2bw−1) and [0, 2bx−1), there exists some power save when reducing one of the bit widths, es-
pecially for the serial multiplier (see Appendix Fig. 10). This highlights again the importance of
unsigned arithmetic. However, in all our experiments we do not take this extra beneﬁt of our approach
into account when computing power consumption, so that our reports are conservative."
REMOVING THE MULTIPLIER,0.15217391304347827,"To beneﬁt from the ability to achieve high precision with drastic quantization of only the weights, we
now explore a solution that allows removing the multiplier altogether. Unlike other multiplier-free
designs, our method allows converting any full-precision pre-trained model into a low-precision
power-efﬁcient one without any changes to the architecture."
POWER AWARE WEIGHT QUANTIZATION,0.15458937198067632,"5.1
POWER AWARE WEIGHT QUANTIZATION"
POWER AWARE WEIGHT QUANTIZATION,0.1570048309178744,"Consider the computation y = d
X"
POWER AWARE WEIGHT QUANTIZATION,0.15942028985507245,"i=1
wi · xi,
(8)"
POWER AWARE WEIGHT QUANTIZATION,0.16183574879227053,"which involves d MACs. Here, {wi, xi} are the weights and activations of a convolution or a fully-
connected layer. Given {wi, xi} in full precision, our goal is to accurately approximate (8) in a
power-efﬁcient manner. When quantizing the weights and activations we obtain the approximation y ≈ d
X"
POWER AWARE WEIGHT QUANTIZATION,0.1642512077294686,"i=1
γwQw(wi) · γxQx(xi),
(9)"
POWER AWARE WEIGHT QUANTIZATION,0.16666666666666666,"where the quantizers Qw(·) and Qx(·) map R to Z, and γw and γx are their quantization steps3. To
make the computation (9) power efﬁcient, we propose to implement multiplications via additions."
POWER AWARE WEIGHT QUANTIZATION,0.16908212560386474,3In quantized models MAC operations are always performed on integers and rescaling is applied at the end.
POWER AWARE WEIGHT QUANTIZATION,0.17149758454106281,Under review as a conference paper at ICLR 2022
POWER AWARE WEIGHT QUANTIZATION,0.17391304347826086,"Power of 2-bit 
width unsigned MAC"
-BIT,0.17632850241545894,3-bit
-BIT,0.178743961352657,4-bit
-BIT,0.18115942028985507,5-bit
-BIT,0.18357487922705315,"6-bit 
8-bit 
7-bit"
-BIT,0.1859903381642512,"(a) Number of additions vs. bit width in PANN
(b) The ratio between MSERUQ and MSEPANN"
-BIT,0.18840579710144928,"Figure 3: (a) Each color represents the power of an unsigned bx-bit MAC for some value of bx. In
PANN, we can move on a constant power curve by modifying the number of additions per element R
(vertical axis) on the expense of the activation bit width ˜bx (horizontal axis). (b) We plot the ratio
between the quantization errors of a regular quantizer (RUQ) and a PANN tuned to work at the same
power budget. As can be seen, PANN outperforms RUQ at the low bit widths (where the MSE ratio
is above 1). It should be noted that at the high bit widths, both approaches achieve low errors in
absolute terms, but RUQ is relatively better. In the Gaussian setting, which is closer to the distribution
of DNN weights and activations, the range over which PANN outperforms RUQ is a bit larger."
-BIT,0.19082125603864733,"Speciﬁcally, assume Qw(wi) is a non-negative integer (as in Sec. 4). Then we can implement the
term Qw(wi) · Qx(xi) as"
-BIT,0.1932367149758454,"Qw(wi) · Qx(xi) = Qx(xi) + · · · + Qx(xi)
|
{z
}
Qw(wi) times"
-BIT,0.1956521739130435,",
(10)"
-BIT,0.19806763285024154,"so that (9) is computed as
y ≈γwγx d
X i=1"
-BIT,0.20048309178743962,"Qw(wi)
X"
-BIT,0.2028985507246377,"j=1
Qx(xi).
(11)"
-BIT,0.20531400966183574,This is the basis for our power-aware neural network (PANN) design.
-BIT,0.20772946859903382,"Let w = (w1, . . . , wd)T and x = (x1, . . . , xd)T denote the full precision weights and ac-
tivations, and denote their quantized versions by wq = (Qw(w1), . . . , Qw(wd))T and xq =
(Qx(x1), . . . , Qx(xd))T , respectively. Note that as opposed to conventional quantization meth-
ods, our approach does not necessitate that the quantized weights be conﬁned to any particular range
of the form [0, 2bw). Indeed, what controls our approximation accuracy is not the largest possible
entry in wq, but rather the number of additions per input element, which is given by ∥wq∥1/d.
Therefore, given a budget of R additions per input element, we propose to use a quantization step of
γw = ∥w∥1/(Rd) in (9), so that"
-BIT,0.21014492753623187,"Q(wi) = round(wi/γw).
(12)"
-BIT,0.21256038647342995,"This quantization ensures that the number of additions per input element is indeed as close as possible
to the prescribed R. We remark that although we assumed unsigned weights, this quantization
procedure can also be used for signed weights (after quantization, the positive and negative weights
can be treated separately in order to save power, as in Sec. 4)."
POWER CONSUMPTION,0.21497584541062803,"5.2
POWER CONSUMPTION"
POWER CONSUMPTION,0.21739130434782608,"We emphasize that in PANN, we would not necessarily want to use the same bit width for the
activations as in regular quantization. We therefore denote the activation bit width in PANN by ˜bx
to distuingish it from the bx bits we would use with the regular quantizer. To estimate the power
consumed by our approach, note that we have approximately ∥w∥1 additions of ˜bx bit numbers. On"
POWER CONSUMPTION,0.21980676328502416,Under review as a conference paper at ICLR 2022
POWER CONSUMPTION,0.2222222222222222,"average, each such addition leads to 0.5˜bx bit ﬂips at the accumulator’s output and 0.5˜bx bit ﬂips in
the FF register (see Table 1). The input to the accumulator, however, remains ﬁxed for Qw(wi) times
when approximating the ith MAC and therefore changes a total of only d times throughout the entire
computation in (11), each time with 0.5˜bx bit ﬂips on average. Thus, overall, the average power per
element consumed by PANN is"
POWER CONSUMPTION,0.2246376811594203,PPANN = ∥w∥1˜bx + 0.5˜bxd
POWER CONSUMPTION,0.22705314009661837,"d
= (R + 0.5)˜bx.
(13)"
POWER CONSUMPTION,0.22946859903381642,"This implies that to comply with a prescribed power budget, we can either increase the activation bit
width ˜bx on the expense of the number of additions R, or vice versa."
POWER CONSUMPTION,0.2318840579710145,"Figure 3(a) depicts the combinations of ˜bx and R that lead to the same power consumption as that
of a bx bit unsigned MAC, P u
MAC = 0.5b2
x + 4bx (see (3),(4)), for several values of bx (different
colors). When we traverse such an equal-power curve, we also change the quantization error. Thus,
the question is whether there exist points along each curve, which lead to lower errors than those
obtained with regular quantization at the bit-width corresponding to that curve."
QUANTIZATION ERROR,0.23429951690821257,"5.3
QUANTIZATION ERROR"
QUANTIZATION ERROR,0.23671497584541062,"Let us compute the quantization error incurred by PANN and compare it to that of a regular uniform
quantizer (RUQ). Obviously, the error incurred by the approximation (9) is contributed by both the
quantization of the weights and the quantization of the activations. To see how their errors interact, let
us assume that w and x are statistically independent random vectors, each with iid components. In
this case, if the quantization errors εw = w −γwQ(w) and εx = x −γxQ(x) satisfy E[εw|w] = 0
and E[εx|x] = 0, then we can show (see Appendix A.9) that the mean squared error (MSE) between
the full precision operation (8) and its quantized version (9) is given by"
QUANTIZATION ERROR,0.2391304347826087,"MSE = E
h 
wT x −wT
q xq
2i
≈d
 
σ2
wσ2
εx + σ2
xσ2
εw

.
(14)"
QUANTIZATION ERROR,0.24154589371980675,"Here σ2
w, σ2
x, σ2
εw, and σ2
εx denote the second-order moments of the elements of w, x, εw, and εx,
respectively, and the expression on the right results from neglecting second-order terms."
QUANTIZATION ERROR,0.24396135265700483,"Consider a simplistic scenario where the activations are uniformly distributed in [0, Ma] (recall that
activations are non-negative due to the preceding ReLU) and the weights are uniformly distributed in
[−1"
QUANTIZATION ERROR,0.2463768115942029,"2Mw, 1"
QUANTIZATION ERROR,0.24879227053140096,"2Mw] . If we use a RUQ with bx bits to quantize the activations and a RUQ with bw bits to
quantize the weights, then we have that"
QUANTIZATION ERROR,0.25120772946859904,"σ2
x = M 2
x
3 ,
σ2
εx =
M 2
x
12 · 22bx ,
σ2
w = M 2
w
12 ,
σ2
εw =
M 2
w
12 · 22bw .
(15)"
QUANTIZATION ERROR,0.2536231884057971,"This is because the quantization errors are uniformly distributed as εx ∼U[−2−(bx−1), 2−(bx−1)]
and εw ∼U[−2−(bw−1), 2−(bw−1)]. Substituting (15) into (14), we obtain that the MSE of a RUQ is"
QUANTIZATION ERROR,0.2560386473429952,"MSERUQ = dM 2
xM 2
w
122
 
2−2bx + 4 · 2−2bw
.
(16)"
QUANTIZATION ERROR,0.2584541062801932,"In PANN, we have that (εw)i|w ∼U[−∥w∥1"
QUANTIZATION ERROR,0.2608695652173913,"2Rd , ∥w∥1"
QUANTIZATION ERROR,0.2632850241545894,"2Rd ], so that E[(εw)2
i |w] = ∥w∥2
1
12Rd . Therefore,"
QUANTIZATION ERROR,0.26570048309178745,"σ2
εw ≈E[∥w∥2
1]
12(Rd)2 = d2(0.25Mw)2"
QUANTIZATION ERROR,0.26811594202898553,"12(Rd)2
=
M 2
w
192R2 ,
(17)"
QUANTIZATION ERROR,0.27053140096618356,"where we used the fact that |wi| ∼U[0, 0.5Mw]. Substituting this expression in (14), we ﬁnd that
using PANN together with a ˜bx bit RUQ for the activations, we achieve"
QUANTIZATION ERROR,0.27294685990338163,"MSEPANN = dM 2
xM 2
w
122"
QUANTIZATION ERROR,0.2753623188405797,"
2−2˜bx +
1
4R2"
QUANTIZATION ERROR,0.2777777777777778,"
.
(18)"
QUANTIZATION ERROR,0.28019323671497587,"To compare between PANN and RUQ, we need to ﬁx a power budget P. Given such a budget, (13)
dictates that the number of additions in PANN should be set to R = P/˜bx −0.5. Substituting this
into (19), we obtain that"
QUANTIZATION ERROR,0.2826086956521739,"MSEPANN = dM 2
xM 2
w
122 "
QUANTIZATION ERROR,0.28502415458937197,"2−2˜bx +
˜b2
x
(2P −˜bx)2 !"
QUANTIZATION ERROR,0.28743961352657005,".
(19)"
QUANTIZATION ERROR,0.2898550724637681,Under review as a conference paper at ICLR 2022
QUANTIZATION ERROR,0.2922705314009662,"The optimal bit-width for the activations can therefore be found numerically by minimizing MSEPANN
over ˜bx ∈Z+. This typically requires evaluating (19) for a small number of candidate bit widths,
e.g., ˜bx ∈{2, . . . , 8}. See Appendix A.8 for a thorough analysis."
QUANTIZATION ERROR,0.2946859903381642,"Figure 3(b) shows the ratio between MSERUQ and MSEPANN (with the optimal ˜bx) as a function of
the bit width of the RUQ, where PANN is tuned to the same power. For the RUQ, we use bx = bw
as its power consumption (7) is anyway dominated by the larger of them. It can be seen that for
low bit widths, PANN has a signiﬁcant advantage over RUQ (ratio larger than 1). In the Gaussian
setting, which is closer to the distribution of DNN weights and activations, the range over which
PANN outperforms RUQ is even larger. As we show in Appendix A.8, this behavior is very similar to
that observed in deep networks for image classiﬁcation."
QUANTIZATION ERROR,0.2971014492753623,"We emphasize that (19) is valid for uniformly distributed weights and activations, which is often not
an accurate enough assumption for DNNs. Thus, in practice the best way to determine the optimal bit
width ˜bx is by running the quantized network on a validation set, as summarized in Algorithm 1."
QUANTIZATION ERROR,0.2995169082125604,Algorithm 1: Determining the optimal parameters for PANN
QUANTIZATION ERROR,0.30193236714975846,"1: Input: Power budget P
2: Output: Optimal ˜bx, R
3: for each ˜bx ∈[˜bmin
x
,˜bmax
x
] do
4:
Set R = P/˜bx −0.5 (Eq. (13))
5:
Quantize the weights using Eq. (12) with γw = ∥w∥/(Rd)
6:
Quantize the activations to ˜bx bits using any quantization method
7:
Run the network on a validation set, with multiplications replaced by additions using Eq. (10)
8:
Save the accuracy to Acc(˜bx).
9: end for
10: set ˜bx ←arg max˜bx Acc(˜bx),
R ←P/˜bx −0.5"
EXPERIMENTS,0.30434782608695654,"6
EXPERIMENTS"
EXPERIMENTS,0.30676328502415456,"We now examine PANN in DNN classiﬁcation experiments. We start by examining its performance
at post training, and then move on to employ it during training. Here we focus only on the effect of
removing the multiplier (vertical arrows in Fig. 1). Namely, we assume all models have already been
converted to unsigned arithmetic (recall this by itself reduces a lot of the power consumption)."
EXPERIMENTS,0.30917874396135264,"PANN at post training
We illustrate PANN’s performance in conjunction with a variety of post
training quantization methods, including the data free approaches GDFQ (Shoukai et al., 2020)
and ZeroQ (Cai et al., 2020), the small calibration set method ACIQ (Banner et al., 2019), and the
optimization based approach BRECQ (Li et al., 2021), which is currently the state-of-the-art for post
training quantization at low bit widths. Table 2 reports results with ResNet-50 on ImageNet (See
Appendix A.5.1 for results with other models). For the baseline methods, we always use equal bit
widths for the weights and activations. Each row also shows our PANN variant, which works at the
precise same power budget, where we choose the optimal ˜bx and R using Alg. 1. As can be seen,
PANN exhibits only a minor degradation w.r.t. the full-precision model, even when working at the
power budget of 2 bit networks. This is while all existing methods completely fail in this regime."
EXPERIMENTS,0.3115942028985507,"PANN for quantization aware training
To use PANN during training, we employ a straight-
through estimator for backpropagation through the quantizers. Table 3 compares our method to
LSQ (Esser et al., 2019), which is a state-of-the-art QAT approach, where in PANN we use LSQ for
quantizing the activations. As can be seen, PANN outperforms LSQ for various models and power
budgets. In Table 4 we compare our method to the multiplication-free approaches AdderNet (Chen
et al., 2020) and ShiftAddNet (You et al., 2020), which are also training-based techniques. For each
method, we report the addition factor, which is the ratio between its number of additions per layer
and a regular layer. For example, AdderNet uses no multiplications but twice as many additions, so
that its addition factor is 2. ShiftAddNet, on the other hand, uses one addition and one shift operation.
According to You et al. (2020), a shift operation costs between 0.2 (on FPGA) and 0.8 (on a 45nm
ASIC) an addition operation. Therefore ShiftAddNet’s addition factor is between 1.2 and 1.8, and for
simplicity we regard it as 1.5. In PANN, we can choose any addition factor R, and therefore examine"
EXPERIMENTS,0.3140096618357488,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3164251207729469,"POWER
(BITS)"
EXPERIMENTS,0.3188405797101449,"ACIQ
ZEROQ
GDFQ
BRECQ"
EXPERIMENTS,0.321256038647343,"MEM.
LATENCY
BASE.
OUR
BASE.
OUR
BASE.
OUR
BASE.
OUR"
EXPERIMENTS,0.32367149758454106,"265 (8)
1×
7.5×
76.02
76.10
75.90
75.77
76.17
76.05
76.10
76.05
217 (6)
1.3×
4.7×
75.41
76.05
73.57
74.65
76.05
76.02
75.86
76.01
134 (5)
2×
3.5×
74.02
75.50
58.62
74.32
71.40
75.96
75.75
75.96
99 (4)
2.3×
2.9×
66.12
75.10
3.53
68.24
50.81
75.20
75.42
75.80
68 (3)
2×
2.2×
7.73
74.16
1.51
68.12
0.24
74.85
68.12
74.62
41 (2)
3×
1.1×
0.20
71.55
0.10
62.96
0.13
74.32
18.80
73.21"
EXPERIMENTS,0.32608695652173914,"Table 2: PTQ: Classiﬁcation accuracy [%] of ResNet-50 on ImageNet (FP 76.11%). The base-
lines (Base.) use equal bit widths for weights and activations. This bit width determines the power P,
reported in ﬁrst column in units of Giga bit-ﬂips. The power is calculated as P u
mult +P u
acc (Eqs. (3),(4))
times the number of MACs in the network. In each row, our variant PANN is tuned to work at the
same power budget, for which we choose the optimal ˜bx and R using Alg. 1."
EXPERIMENTS,0.3285024154589372,"Bits (Power), Net
LSQ
PANN"
EXPERIMENTS,0.3309178743961353,"18 (2), ResNet-18
67.32
70.83
30 (3), ResNet-18
69.81
71.12
41 (2), ResNet-50
71.36
76.65
68 (3), ResNet-50
73.54
76.78
155 (2), VGG-16bn
71.15
73.30"
EXPERIMENTS,0.3333333333333333,"Table 3: QAT: Comparison with LSQ.
Imagenet classiﬁcation accuracy [%] of
various models. We report the bit width
of LSQ and power in Giga bit-ﬂips."
EXPERIMENTS,0.3357487922705314,"Method
6/6
5/5
4/4
3/3"
EXPERIMENTS,0.33816425120772947,"Our (1×)
91.15
91.05
89.93
85.62
Our (1.5×)
91.52
91.50
90.05
86.12
Our (2×)
91.63
91.61
90.10
86.84
ShiftAddNet (1.5×)
87.72
87.61
86.76
85.10
AdderNet (2×)
67.39
65.53
64.31
63.50"
EXPERIMENTS,0.34057971014492755,"Table 4: QAT: Comparison with multiplier-free meth-
ods. Classiﬁcation accuracy [%] of ResNet-20 on CIFAR-
10. The top row speciﬁes weight/activation bit widths,
and the addition factor is speciﬁed in parentheses."
EXPERIMENTS,0.34299516908212563,"our method for R = 1, 1.5, 2. We can see in the table that PANN outperforms both AdderNet and
ShiftAddNet for all bit widths, even when using a smaller addition factor. Please see more QAT
comparisons in Appendix A.5.2"
EXPERIMENTS,0.34541062801932365,"Runtime memory footprint and latency of PANN
We now analyze the effect of PANN on other
inference aspects besides power. One important aspect is runtime memory footprint. When working
with batches of image, the runtime memory consumption is dominated by the activations (Mishra
et al., 2017) (see discussion on the memory footprint of the weights in Appendix A.7). The optimal
number of bits ˜bx we use for the activations is typically larger than the bit width bx used in regular
quantization. The second column of Table 2 reports the factor ˜bx/bx by which the runtime memory
of PANN exceeds that of the baseline model. As can be seen, this factor never exceeds 3. In the
comparisons with the multiplier-free methods (Table 4), we keep the same bit width for the activations
and therefore there is no change in the memory footprint. A second important factor is latency. Recall
we remove the multiplier and remain only with the accumulator. Since addition is faster than
multiplication, one could potentially use a higher clock-rate and thus gain speed. However, if we
conservatively assume the original clock-rate, then the latency is increased by R (each multiplication
is replaced by R additions). As can be seen in Table 2, the increase in latency is quite small at the
lower power budgets. For the multiplier-free methods, we obtain improvement in accuracy even for
R = 1. In that case, our latency is smaller than that of AdderNet (2×) and ShiftAddNet (1.5×).
Please refer to Appendix A.7 for more analyses."
CONCLUSION,0.34782608695652173,"7
CONCLUSION"
CONCLUSION,0.3502415458937198,"We presented an approach for reducing the power consumption of DNNs. Our technique relies
on a detailed analysis of the power consumption of each arithmetic module in the network, and
makes use of two key principles: switching to unsigned arithemtic, and employing a new weight
quantization method that allows removing the multiplier. Our method substantially improves over
existing approaches, both at post-training and when used during training, and allows achieving a
signiﬁcantly higher accuracy at any given power consumption budget."
CONCLUSION,0.3526570048309179,Under review as a conference paper at ICLR 2022
REFERENCES,0.35507246376811596,REFERENCES
REFERENCES,0.357487922705314,"Jan Achterhold, Jan Mathias Koehler, Anke Schmeink, and Tim Genewein. Variational network
quantization. In International Conference on Learning Representations, 2018."
REFERENCES,0.35990338164251207,"Shahzad Asif and Yinan Kong. Performance analysis of wallace and radix-4 booth-wallace multipliers.
In 2015 Electronic System Level Synthesis Conference (ESLsyn), pp. 17–22. IEEE, 2015."
REFERENCES,0.36231884057971014,"Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional
networks for rapid-deployment. In Advances in Neural Information Processing Systems, pp.
7950–7958, 2019."
REFERENCES,0.3647342995169082,"Oresti Banos, Rafael Garcia, Juan A Holgado-Terriza, Miguel Damas, Hector Pomares, Ignacio Rojas,
Alejandro Saez, and Claudia Villalonga. mhealthdroid: a novel framework for agile development
of mobile health applications. In International workshop on ambient assisted living, pp. 91–98.
Springer, 2014."
REFERENCES,0.3671497584541063,"Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq:
A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 13169–13178, 2020."
REFERENCES,0.3695652173913043,"Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. Addernet:
Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 1468–1477, 2020."
REFERENCES,0.3719806763285024,"Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. arXiv preprint arXiv:1511.00363, 2015."
REFERENCES,0.3743961352657005,"Mostafa Elhoushi, Zihao Chen, Farhan Shaﬁq, Ye Henry Tian, and Joey Yiwei Li. Deepshift: Towards
multiplication-less neural networks. arXiv preprint arXiv:1905.13298, 2019."
REFERENCES,0.37681159420289856,"Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar-
mendra S Modha. Learned step size quantization. In International Conference on Learning
Representations, 2019."
REFERENCES,0.37922705314009664,"Jun Fang, Ali Shaﬁee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H
Hassoun. Post-training piecewise linear quantization for deep neural networks. In European
Conference on Computer Vision, pp. 69–86. Springer, 2020."
REFERENCES,0.38164251207729466,"Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746,
2015."
REFERENCES,0.38405797101449274,"Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry. The knowledge within: Methods for
data-free model compression. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 8494–8502, 2020."
REFERENCES,0.3864734299516908,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.3888888888888889,"M Horowitz. Computing’s energy problem (and what we can do about it). 2014 ieee int. In Solid-State
Circuits Conference Digest of Technical Papers (ISSCC), pp. 10–14, 2014a."
REFERENCES,0.391304347826087,Mark Horowitz. Energy table for 45nm process. In Stanford VLSI wiki. 2014b.
REFERENCES,0.39371980676328505,"Ning-Chi Huang, Huan-Jan Chou, and Kai-Chiang Wu. Efﬁcient systolic array based on decompos-
able mac for quantized deep neural networks. 2019."
REFERENCES,0.3961352657004831,"Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
Improving post
training neural quantization: Layer-wise calibration and integer programming. arXiv preprint
arXiv:2006.10518, 2020."
REFERENCES,0.39855072463768115,Under review as a conference paper at ICLR 2022
REFERENCES,0.40096618357487923,"Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁcient
integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2704–2713, 2018."
REFERENCES,0.4033816425120773,"Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,
Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,
et al. A study of bﬂoat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019."
REFERENCES,0.4057971014492754,"Naghmeh Karimi, Thorben Moos, and Amir Moradi. Exploring the effect of device aging on static
power analysis attacks. UMBC Faculty Collection, 2019."
REFERENCES,0.4082125603864734,"Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-
pression of deep convolutional neural networks for fast and low power mobile applications. In
Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL
http://arxiv.org/abs/1511.06530."
REFERENCES,0.4106280193236715,"Youngbae Kim, Heekyung Kim, Nandakishor Yadav, Shuai Li, and Kyuwon Ken Choi. Low-power
rtl code generation for advanced cnn algorithms toward object detection in autonomous vehicles.
Electronics, 9(3):478, 2020."
REFERENCES,0.41304347826086957,"Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4013–4021, 2016."
REFERENCES,0.41545893719806765,"Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711,
2016."
REFERENCES,0.4178743961352657,"Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi
Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=POWv6hDd9XH."
REFERENCES,0.42028985507246375,"Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural networks with
few multiplications. arXiv preprint arXiv:1510.03009, 2015."
REFERENCES,0.4227053140096618,"Xingchao Liu, Mao Ye, Dengyong Zhou, and Qiang Liu. Post-training quantization with multiple
points: Mixed precision without mixed precision. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 35, pp. 8697–8705, 2021."
REFERENCES,0.4251207729468599,"Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling.
Relaxed quantization for discretized neural networks. arXiv preprint arXiv:1810.01875, 2018."
REFERENCES,0.427536231884058,"Mostafa Mahmoud, Isak Edo, Ali Hadi Zadeh, Omar Mohamed Awad, Gennady Pekhimenko, Jorge
Albericio, and Andreas Moshovos. Tensordash: Exploiting sparsity to accelerate deep neural
network training and inference, 2020."
REFERENCES,0.42995169082125606,"Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Debbie Marr. Wrpn: Wide reduced-precision
networks. arXiv preprint arXiv:1709.01134, 2017."
REFERENCES,0.4323671497584541,"Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization
through weight equalization and bias correction.
In Proceedings of the IEEE International
Conference on Computer Vision, pp. 1325–1334, 2019."
REFERENCES,0.43478260869565216,"Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or
down? adaptive rounding for post-training quantization. arXiv preprint arXiv:2004.10568, 2020."
REFERENCES,0.43719806763285024,"Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein,
and Avi Mendelson. Loss aware post-training quantization. arXiv preprint arXiv:1911.07190,
2019."
REFERENCES,0.4396135265700483,"Yehya Nasser, J-C Pr´evotet, Maryline H´elard, and Jordane Lorandel. Dynamic power estimation based
on switching activity propagation. In 2017 27th International Conference on Field Programmable
Logic and Applications (FPL), pp. 1–2. IEEE, 2017."
REFERENCES,0.4420289855072464,Under review as a conference paper at ICLR 2022
REFERENCES,0.4444444444444444,"Renkun Ni, Hong-min Chu, Oscar Castaneda Fernandez, Ping-yeh Chiang, Christoph Studer, and Tom
Goldstein. Wrapnet: Neural net inference with ultra-low-precision arithmetic. In 9th International
Conference on Learning Representations (ICLR 2021), 2021."
REFERENCES,0.4468599033816425,"Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll´ar. Designing
network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 10428–10436, 2020."
REFERENCES,0.4492753623188406,"Andres Rodriguez, Eden Segal, Etay Meiri, Evarist Fomenko, Y Jim Kim, Haihao Shen, and Barukh
Ziv. Lower numerical precision deep learning inference and training. Intel White Paper, 3:1–19,
2018."
REFERENCES,0.45169082125603865,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510–4520, 2018."
REFERENCES,0.45410628019323673,"Xu Shoukai, Li Haokun, Zhuang Bohan, Liu Jing, Cao Jiezhang, Liang Chuangrun, and Tan Mingkui.
Generative low-bitwidth data free quantization. In The European Conference on Computer Vision,
2020."
REFERENCES,0.45652173913043476,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019."
REFERENCES,0.45893719806763283,"Michael Tschannen, Aran Khanna, and Animashree Anandkumar. Strassennets: Deep learning with a
multiplication budget. In International Conference on Machine Learning, pp. 4985–4994. PMLR,
2018."
REFERENCES,0.4613526570048309,"Ganesh Venkatesh, Eriko Nurvitadhi, and Debbie Marr. Accelerating deep convolutional networks
using low-precision and sparsity, 2016."
REFERENCES,0.463768115942029,"Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu, Zhangyang Wang,
and Yingyan Lin. Shiftaddnet: A hardware-inspired deep network. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.46618357487922707,"A
APPENDIX"
REFERENCES,0.46859903381642515,"A.1
POWER SIMULATION ON 5NM PROCESS"
REFERENCES,0.47101449275362317,"Using the Synopsys DesignWare Library4, we built a Verilog RTL (Register Transfer Logic) simula-
tion which instantiates signed multipliers and signed adders of 2-8 bit widths. For the multipliers,
we used a Radix-4 Booth-encoder implementation and for the adders, we used the Ripple Carry
implementation. We synthesized these adders and multipliers, with a 5nm cells library at a clock
frequency of 1.6GHz."
REFERENCES,0.47342995169082125,"In order to analyze the power consumption of each module individually, we used a hierarchical
gate-level synthesis where each module is a different utility that does not share logic with any
other module. The synthesis result is the gate-level netlist with the logic gates, which is the actual
implementation of the multipliers and adders. Then, using Synopsys PrimeTime PX5 (PTPX), which
accurately reﬂects ASIC power consumption, we ran a simulation with uniformly distributed random
inputs on the gate-level netlist and measured the power of multiplication and addition instructions."
REFERENCES,0.4758454106280193,"It should be noted that, as opposed to papers experimenting with FPGAs, our primary interest is in
modern integrated chips (ASICs), like CPUs and GPUs. Gate-level simulations of the type we use
here are the only practical (and most accurate) way to estimate ASIC power consumption. This is
because even if we had fabricated this netlist as part of a real ASIC (which would cost millions of
dollars), it would still be impossible to measure the power of a small portion of the chip accurately."
REFERENCES,0.4782608695652174,"4https://www.synopsys.com/silicon-design.html
5https://www.synopsys.com/support/training/signoff/primetimepx-fcd.html"
REFERENCES,0.4806763285024155,Under review as a conference paper at ICLR 2022
REFERENCES,0.4830917874396135,"In ﬁgures 4(a) and 4(b) we depict the average power consumed by a multiplication of two b-bit
numbers and by an addition of two b-bit numbers, respectively. We can see that these power
measurements agree with our Python simulation, which we discuss in the next section. A remark
is in place about the slight deviation between the 5nm simulation and our Python simulation seen
in Fig. 4(a). This deviation implies that the advantage of PANN over a regular DNN is slightly
more substantial than that reported in the paper. Speciﬁcally, since our theoretical model slightly
underestimates the power consumed by a multiplier at high bit-widths, we actually underestimate the
beneﬁt of our method, which avoids multiplications. The conﬁguration ﬁles will be published online
to enable easy reproduction of the results."
REFERENCES,0.4855072463768116,"In Table 5, we report the amount in [%] of the dynamic power and the static power as measured using
our 5nm simulation."
REFERENCES,0.48792270531400966,"MEASURED
2-BIT
3-BIT
4-BIT
5-BIT
6-BIT
7-BIT
8-BIT
32-BIT"
REFERENCES,0.49033816425120774,"DYNAMIC POWER (MULTIPLIER)
59
57
55
51
50
51
51
–
STATIC POWER (MULTIPLIER)
41
43
45
49
50
49
49
–
DYNAMIC POWER (ADDER)
61
60
59
58
58
55
56
60
STATIC POWER (ADDER)
39
40
41
42
42
45
44
40"
REFERENCES,0.4927536231884058,"Table 5: Static power vs. dynamic power in [%]. We can see that overall the dynamic power
constitutes a major portion of the total power."
REFERENCES,0.49516908212560384,"A.2
POWER SIMULATION IN PYTHON"
REFERENCES,0.4975845410628019,"Using Python, we implemented a simple serial adder and two types of multipliers: a simple serial
multiplier and a Radix-2 Booth encoding multiplier. A serial multiplier follows the long multiplication
concept in which each bit of the multiplicand multiplies the multiplier word. This results in a word of
length b + 1 bits at most, called a partial product. Going over all bits in the multiplicand results in b
partial products that need to be summed. The Booth encoder is more efﬁcient in terms of the number
of partial products that need to be summed. It comprises an encoder that follows a lookup table and
directs whether to perform shift, addition or subtraction, basing on consecutive pairs of bits of the
multiplicand. For example, suppose we want to multiply a number x by 15, which is 1111 in binary
representation. The serial multiplier performs the computation x ×
 
23 + 22 + 21 + 20
while the
Booth encoder multiplier computes x ×
 
24 −20
, and saves two sums. Those partial products are
summed by half and full adders that are the major area and power consumers of a multiplier."
REFERENCES,0.5,"We are focusing on the dynamic power, which is a prominent source of power consumption and
linearly depends on the switching activity. Therefore, in order to estimate the power, we measured
the average number of bit toggles per instruction (e.g., multiplication and addition). We counted
the toggles at the inputs of each half or full 1-bit adder component, both for the b-bit multiplier and
for the b-bit adder. Please see an illustrative example in Fig. 6. In ﬁgures 7,8 we show the average
number of toggles per instruction that were counted in the multiplier and in the adder using signed and
unsigned numbers, respectively. We ran our simulation with data drawn from a uniform distribution
and a Gaussian one. We took the uniform distribution to be over the range [−2b−1, 2b−1). As for
the Gaussian, we ﬁrst drew N full precision numbers from N(0, 1). Then, we divided them by their
maximum (in absolute value), multiplied by 2b−1, and rounded to the closest integer. We clipped the
values to the range [−2b−1, 2b−1) in order to eliminate outliers (speciﬁcally the number 2b−1). In all
our experiments, we took N = 36000. Please see an example histogram for the Gaussian distributed
numbers in Fig. 5(b), where b = 8."
REFERENCES,0.5024154589371981,"A.3
OBSERVATION-1"
REFERENCES,0.5048309178743962,"When working with an accumulator having a large bit width B (e.g. B = 32), a dominant source of
power consumption is the bit toggling in its inputs, which is 0.5B per instruction on average. This
comes from the 2’s complement representation. Hence, a signiﬁcant amount of power can be saved
when switching to unsigned numbers. For example, in the right plot of Fig. 8 we show that the power
in the accumulator inputs is reduced from 16 (assuming a 32-bit accumulator) to 0.5bacc where bacc
is the bit width at the input to the accumulator."
REFERENCES,0.5072463768115942,Under review as a conference paper at ICLR 2022
REFERENCES,0.5096618357487923,"(a) Measuring the power of a multiplication operation.
(b) Measuring power of an addition operation."
REFERENCES,0.5120772946859904,"Figure 4: (a) In red we plot the power consumed by a multiplication operation as measured in our
Python simulation. This curve agrees with the theoretical power model in Eq. (3) in the paper, i.e.
Pmult = 0.5b2 + b (see left side of Fig. 7). In blue, we plot the power measurements on a 5-nm silicon
process. In order to ignore the different power units and set both measurements on the same axis, we
scaled the results of the 5nm power simulation so that the curves intersect at bit width of 4. (b) In
this experiment, we measured the power consumed by a b-bit accumulator without the FF (thus, here
bacc = b). In red we can observe the power measured in our Python simulation, which is very close
to our theoretical model Pacc = 0.5b + 0.5b = b (see right side of Fig. 7). We can see that the power
measurements for the 5nm silicon process (blue) nicely agree with our Python simulation. Here we
scaled the results of the 5nm simulation using the same factor we found in Fig. 4(a)."
REFERENCES,0.5144927536231884,"(a) Switching to unsigned numbers does not affect the
multiplier."
REFERENCES,0.5169082125603864,(b) An example quantized Gaussian distribution.
REFERENCES,0.5193236714975845,"Figure 5: (a) Using our Python simulation, We measured the average number of toggles between
unsigned multiplication and signed multiplication for bit widths of 4-8. As can be seen, we obtained
an average ratio of 92% (red curve). This observation is aligned with the power measured in our
5nm silicon process (blue curve). (b) Unlike the uniform distribution, we can see that the majority of
values occupies roughly half of the allowed interval and therefore on average, we observe a bit less
toggles than with the uniform distribution (here the bit width is b = 8)."
REFERENCES,0.5217391304347826,"As for the multiplier, switching to unsigned values turns out to have a negligible effect in terms of
power consumption (left plot of Fig. 8). In Fig. 5(a) we show the ratio between the power consumed
by multiplication of unsigned numbers and multiplication of signed numbers, as measured in our
Python simulation and in the 5nm silicon process. As can be seen, this ratio is close to 1 for all bit"
REFERENCES,0.5241545893719807,Under review as a conference paper at ICLR 2022
-BIT,0.5265700483091788,1-bit
-BIT,0.5289855072463768,"full 
adder"
-BIT,0.5314009661835749,1-bit
-BIT,0.533816425120773,"full 
adder"
-BIT,0.5362318840579711,1-bit
-BIT,0.538647342995169,"full 
adder"
-BIT,0.5410628019323671,1-bit
-BIT,0.5434782608695652,"half 
adder"
-BIT,0.5458937198067633,"1
0
1
0
1
0
1
1"
-BIT,0.5483091787439613,"1
1
1
1-bit"
-BIT,0.5507246376811594,"full 
adder"
-BIT,0.5531400966183575,1-bit
-BIT,0.5555555555555556,"full 
adder"
-BIT,0.5579710144927537,1-bit
-BIT,0.5603864734299517,"full 
adder"
-BIT,0.5628019323671497,1-bit
-BIT,0.5652173913043478,"half 
adder"
-BIT,0.5676328502415459,"1
0
1
1
1
0
1
0 0
0
1"
-BIT,0.5700483091787439,"(a) State 𝑖−1
(b) State 𝑖"
-BIT,0.572463768115942,"Figure 6: Counting bit toggles in the multiplier’s internal adders. We depict a snapshot of the
multiplier’s internal components at two consecutive addition instructions. At state i −1 we sum 1111
and 0001 and at state i we sum 1111 and 0100. In our python simulation, we compare between the
bit status of consecutive operations therefore in this example, we will count four toggles (two in the
input words and two in the internal carry outputs)."
-BIT,0.5748792270531401,"Figure 7: Python simulation for signed integers. On the left, we plot the power consumed by the
multiplier. We counted the toggles at the inputs of the multiplier (row 1 in Table 1 of the paper) as
well as the toggles inside its internal units (row 2 in Table 1). We can see that the power measured in
our simulations closely agrees with power model in Eq. (1), 0.5b2 +b. On the right, we plot the power
consumed by the accumulator, where the label “acc inputs” refers to the power due to the bit ﬂips at
its input (row 3 in Table 1). In this case B = 32 and therefore we observe a constant power of 16.
The label “acc sum” refers to the power consumed due to the toggles at the output of the accumulator
(row 4 in Table 1) which also toggles the bits in the FF. Again, the simulation agrees with our model."
-BIT,0.5772946859903382,"widths. Therefore, we adopt the same power model for the unsigned multiplier case as in the signed
setting."
-BIT,0.5797101449275363,"A.3.1
SWITCHING TO UNSIGNED ARITHMETIC"
-BIT,0.5821256038647343,"Figure 11(a) compares the average power consumption of a signed MAC to that of an unsigned
MAC for a 32 bit accumulator. Speciﬁcally, we are dividing P u
mult + P u
acc by Pmult + Pacc. In this
setting, it can be seen for example that when working with b = 4 bits for the weights and activations,
unsigned MACs are 33% cheaper in power. The approach we suggest for switching a linear layer
(e.g., convolution, fully connected) to work with unsigned arithmetic is illustrated schematically in
Fig. 11(b)."
-BIT,0.5845410628019324,"Accumulator bit width
The bit width of the accumulator is commonly chosen to be 32. One of the
main reasons for that is this allows ﬂexibility in changing the bit widths of the activations and weights"
-BIT,0.5869565217391305,Under review as a conference paper at ICLR 2022
-BIT,0.5893719806763285,"Figure 8: Python simulation for unsigned integers. Here we repeat the experiment of Fig. 7,
but with numbers drawn only from the interval [0, 2b−1) for both the uniform and the Gaussian
distributions. On the left, we can see that the overall power of the multiplier has not changed much
and is aligned with Eq. (3) in the paper. However, on the right we can see that due to the use of
unsigned values, the power consumed by the toggling at the accumulator inputs is dramatically
reduced (0.5bacc instead of 0.5B). The rest of the power contributors did not change much."
-BIT,0.5917874396135265,"Figure 9: Working with bw < bx in a Booth encoder multiplier. On the right, we uniformly
drew bx-bit numbers from [−2bx−1, 2bx−1) and bw-bit numbers from [−2bw−1, 2bw−1), for various
bw ≤bx. We can see that the power is affected only by the larger bit width (bx), and remains
nearly constant when reducing only bw. On the left, we repeat the same experiment, however with
unsigned values, where the bx-bit input is uniformly drawn from [0, 2bx−1) and the bw-bit input from
[0, 2bw−1). Here, there is a slight beneﬁt in decreasing only bw. The black dashed curve connects the
power measurements for the cases where bw = bx, and follows the parabolic behaviour."
-BIT,0.5942028985507246,"(e.g., from 4-bit to 8-bit and vice versa). Nevertheless, if we are not concerned with ﬂexibility, then
when quantizing the activations and weights to less than 8 bits, we can use an accumulator with less
than 32 bits. The required accumulator bit width B can be calculated by"
-BIT,0.5966183574879227,"B = bx + bw + 1 + log
 
k2Cin

,
(20)"
-BIT,0.5990338164251208,"where k is the convolution kernel size, and Cin is the number of input channels. In Table 6 we analyze
the case of ResNet networks. We choose the layer with the largest value of k2Cin, which is 3x3x512
(Table 1 in He et al. (2016)). We calculate the required bit width for the accumulator (e.g., B) when
the activations and weights are quantized to 2-6 bits. In addition, we calculate the power save in
[%] when switching to unsigned arithmetic. As can be seen, even with smaller accumulator bit
widths, switching to unsigned arithmetic leads to a signiﬁcant saving in power. This is also visually"
-BIT,0.6014492753623188,Under review as a conference paper at ICLR 2022
-BIT,0.6038647342995169,"Figure 10: Working with bw < bx in a simple serial multiplier. On the right, we uniformly drew
bx-bit numbers from [−2bx−1, 2bx−1) and bw-bit numbers from [−2bw−1, 2bw−1) such that bw ≤bx.
We can see that the power is affected by the larger bit width (bx). On the left, we repeat the experiment
however with unsigned values, where the bx-bit input is uniformly drawn from [0, 2bx−1) and the
bw-bit input from [0, 2bw−1). Here, there is more beneﬁt in decreasing only bw. The black dashed
curve connect the power measurements for the cases where bw = bx, and follows the parabolic
behaviour. 33% 58% 44% 25% 14%"
-BIT,0.606280193236715,(a) Switching to unsigned arithmetic ++ -
-BIT,0.6086956521739131,(b) Unsigned arithmetic layer
-BIT,0.6111111111111112,"Figure 11: (a) Based on our power model, we show that a signiﬁcant amount of power can be
saved with minimal effort, by switching to work with unsigned numbers. Here we assume a 32 bit
accumulator. (b) Any weight matrix W can be split into its positive and negative parts (See Sec. 4).
Assuming the elements of its input x are non-negative (due to the preceding ReLU), this makes all
MACs unsigned and thus substantially reduces power consumption."
-BIT,0.6135265700483091,"illustrated in Fig. 12, where we repeat the experiment of Fig. 1 bit with a 17 bit accumulator for the
2-bit networks, and with a 21 bit accumulator for the 4-bit networks."
-BIT,0.6159420289855072,"A.4
OBSERVATION-2"
-BIT,0.6183574879227053,"We now analyze the case where the inputs of the multiplier have different bit widths, bx and bw. In
Fig. 9 we show the average bit toggles in signed and unsigned Booth encoder multiplication, for
bw ≤bx. In Fig. 10 we show the same analysis for the simple serial multiplier. We observe that when
working with signed numbers (the common setting), the power is mostly affected by the larger bit
width (bx in this case)."
-BIT,0.6207729468599034,"In the case of unsigned numbers, there is some power save when reducing one of the bit widths. In
other words, Eq. (7) in the paper is accurate for the popular signed case and behaves as an upper
bound for the unsigned case. The difference between Eq. (7) and the actual power consumption in
the unsigned setting is more dominant for the simple serial multiplier. Therefore, in certain settings,"
-BIT,0.6231884057971014,Under review as a conference paper at ICLR 2022
-BIT,0.6256038647342995,"2-BIT
3-BIT
4-BIT
5-BIT
6-BIT"
-BIT,0.6280193236714976,"REQUIRED BIT WIDTH B
17
19
21
23
25
POWER SAVE FOR A B BIT ACCUMULATOR
39%
28%
21%
16%
13%
POWER SAVE FOR A 32 BIT ACCUMULATOR
58%
44%
33%
25%
19%"
-BIT,0.6304347826086957,"Table 6: Required accumulator bit width. Here we compute the bit width required for the accumu-
lator, according the largest linear layer in ResNets (which is 3x3x512). For example, in case of 2 bit
width activations and weights, we might use an accumulator with 16 bits and not 32 bits. In the last
two rows we report the power save in [%] when switching to unsigned arithmetic. Just like in the 32
bit accumulator case, when working with lower bit width accumulators, we can obtain a signiﬁcant
reduction in power by switching to unsigned arithmetic."
-BIT,0.6328502415458938,"ZeroQ
PANN"
-BIT,0.6352657004830918,"ZeroQ
Unsigned PANN"
-BIT,0.6376811594202898,Unsigned PANN PANN
-BIT,0.6400966183574879,Unsigned ZeroQ ZeroQ
-BIT,0.642512077294686,Unsigned
-BIT,0.644927536231884,(a) Improving upon ZeroQ (4 bits) BRECQ
-BIT,0.6473429951690821,"PANN
BRECQ
Unsigned PANN"
-BIT,0.6497584541062802,Unsigned PANN PANN
-BIT,0.6521739130434783,Unsigned
-BIT,0.6545893719806763,"Unsigned
BRECQ BRECQ"
-BIT,0.6570048309178744,(b) Improving upon BRECQ (2 bits)
-BIT,0.6594202898550725,"Figure 12: Power-accuracy trade-off at post training with different bit width accumulators. We
repeat the experiment of Fig. 1, however this time we assume a 21 bits accumulator in (a) and a 17
bit accumulator in (b). Theretofore, when converting the quantized models to work with unsigned
arithmetic (←), it cuts down 21% of the power consumption in (a) and 39% in (b)."
-BIT,0.6618357487922706,"there is an additional beneﬁt of switching to unsigned arithmetic, which we did not report in the
experiments in the paper (i.e. the horizontal arrows in Fig. 1 should actually be slightly longer in some
cases). Yet, this effect is relatively small compared to the reduction in bit ﬂips in the accumulator."
-BIT,0.6642512077294686,"We validated our observation on the 5-nm silicon process setup. We used 8 × 8 multiplier and
measured the power when one of the inputs was drawn uniformly from [0, 27) and the other from
[0, 23). We got 95% of the power that was measured when both inputs were drawn from [0, 27). In
case of signed values, when one of the inputs was drawn uniformly from [−27, 27) and the other
from [−23, 23) we observed 100% of the power that was measured when both inputs were drawn
from [−27, 27)."
-BIT,0.6666666666666666,"Note that in order to avoid changing the multiplier’s architecture, when switching to unsigned
numbers, we use only half the range allowed by the bit width bx, i.e. [0, 2bx−1). Therefore, we
obtain a representation with half of the 2bx levels of the signed case (note that we also need to
replace −2bx−1 with −2bx−1 + 1). If we permit architectural changes, then a better way to represent
unsigned numbers would be to replace the signed multiplier by a (bx + 1) × (bx + 1) multiplier that
can support unsigned and signed multiplications but consumes much more power, or to work with
a bx × bx unsigned multiplier that allow representation in the full interval of [0, 2bx) and then will
follow Eq. (7) again."
-BIT,0.6690821256038647,Under review as a conference paper at ICLR 2022
-BIT,0.6714975845410628,"A.5
ADDITIONAL RESULTS"
-BIT,0.6739130434782609,"A.5.1
POST TRAINING QUANTIZATION"
-BIT,0.6763285024154589,"Similarly to Table 2, we now examine PANN’s performance on additional networks. We use the
same methods for activations quantization as in Table 2: ACIQ6 (Banner et al., 2019), ZeroQ7 (Cai
et al., 2020), GDFQ8 (Shoukai et al., 2020) and BRECQ9 (Li et al., 2021). We also add Dynamic
Quantization, which quantizes the activation and weights on the ﬂy at inference time, according to
their dynamic ranges. The results are reported in Tables 7-9."
-BIT,0.678743961352657,"We ﬁrst change all networks to work with unsigned arithmetic and measure the classiﬁcation accuracy,
which serves as a baseline (see left side of each method ‘Base.’). As mentioned, this step already
saves a signiﬁcant amount of power, without any change in classiﬁcation accuracy, compared to
signed MAC arithmetic. We use the unsigned MAC power consumption as the power budget P and
follow Alg. 1. We report the classiﬁcation accuracy on the right side of each columns (see ‘Our’)."
-BIT,0.6811594202898551,"POWER
(BITS)"
-BIT,0.6835748792270532,"DYNAMIC
ACIQ
ZEROQ
GDFQ
BRECQ"
-BIT,0.6859903381642513,"BASE.
OUR
BASE.
OUR
BASE.
OUR
BASE.
OUR
BASE.
OUR"
-BIT,0.6884057971014492,"116 (8)
69.77
69.78
69.61
69.67
69.67
69.68
69.75
69.71
69.73
69.72
95 (6)
66.56
69.50
69.05
69.60
67.51
69.55
69.20
69.35
69.70
69.71
76 (5)
55.52
69.12
67.18
69.53
54.76
69.50
68.60
69.12
69.50
69.56
43 (4)
0.33
68.88
55.00
69.26
26.50
69.10
60.61
69.01
68.69
68.29
30 (3)
0.11
68.28
1.50
68.43
0.23
68.20
19.88
68.52
65.20
67.34
18 (2)
0.09
63.62
0.11
66.68
0.10
66.12
0.12
68.11
43.67
66.73"
-BIT,0.6908212560386473,"Table 7: Classiﬁcation accuracy [%] of ResNet-18 on ImageNet (FP: 69.77%). The baselines
(Base.) use equal bit widths for weights and activations (leftmost column). The bit width determines
the power P, which we specify in units of Giga bit-ﬂips. The power is calculated as P u
mult + P u
acc
(Eqs. (3),(4)) times the number of MACs in the network. (1.82 × 109 in ResNet-18). In each row,
our variant is tuned to work at the same power budget, for which we choose the optimal ˜bx and R
using Alg. 1."
-BIT,0.6932367149758454,"POWER
(BITS)"
-BIT,0.6956521739130435,"DYNAMIC
ACIQ
ZEROQ
GDFQ
BRECQ"
-BIT,0.6980676328502415,"BASE.
OUR
BASE.
OUR
BASE.
OUR
BASE.
OUR
BASE.
OUR"
-BIT,0.7004830917874396,"21 (8)
71.82
71.79
69.73
69.71
71.79
71.53
71.88
71.76
71.95
71.85
19 (6)
62.13
64.13
66.16
67.18
69.35
69.58
70.48
70.52
71.36
71.55
11 (5)
13.11
59.55
27.06
61.14
60.49
64.33
65.32
68.31
70.30
70.98
8 (4)
3.56
51.25
2.32
55.13
13.92
62.14
50.96
66.02
65.12
69.12
5 (3)
0.05
49.33
0.09
50.23
0.06
61.11
31.19
64.13
55.14
67.85
3 (2)
0.01
23.26
0.07
35.55
0.03
48.12
1.55
51.12
25.91
61.08"
-BIT,0.7028985507246377,"Table 8: Classiﬁcation accuracy [%] of Mobilenet-V2 on ImageNet (FP: 71.91%). The baselines
(Base.) use equal bit widths for weights and activations (leftmost column). The bit width determines
the power P, which we specify in units of Giga bit-ﬂips. The power is calculated as P u
mult + P u
acc
(Eqs. (3),(4)) times the number of MACs in the network. (0.33 × 109 in MobileNet-V2). In each row,
our variant is tuned to work at the same power budget, for which we choose the optimal ˜bx and R
using Alg. 1."
-BIT,0.7053140096618358,"In ﬁgures 13-14, we demonstrate PANN at post training for different networks under power constrains
of 4-bit and 2-bit unsigned MAC. In each ﬁgure, we start by running the speciﬁed approach to"
-BIT,0.7077294685990339,"6https://github.com/submission2019/cnn-quantization
7https://github.com/amirgholami/ZeroQ
8https://github.com/xushoukai/GDFQ
9https://github.com/yhhhli/BRECQ"
-BIT,0.7101449275362319,Under review as a conference paper at ICLR 2022
-BIT,0.7125603864734299,"POWER
(BITS)"
-BIT,0.714975845410628,"DYNAMIC
ACIQ
ZEROQ
GDFQ
BRECQ"
-BIT,0.717391304347826,"BASE.
OUR
BASE.
OUR
BASE.
OUR
BASE.
OUR
BASE.
OUR"
-BIT,0.7198067632850241,"994 (8)
73.28
73.31
73.24
73.13
73.29
73.30
73.34
73.25
-
-
652 (6)
72.15
73.23
73.02
73.05
73.18
73.12
73.31
73.15
-
-
505 (5)
64.05
72.88
72.31
73.02
71.11
71.92
72.25
73.02
-
-
373 (4)
51.13
72.06
66.20
72.03
64.19
70.19
67.05
71.66
-
-
256 (3)
2.15
70.55
31.22
71.18
20.88
69.95
51.16
71.12
-
-
155 (2)
0.56
69.95
0.13
71.02
0.18
66.62
3.63
67.96
-
-"
-BIT,0.7222222222222222,"Table 9: Classiﬁcation accuracy [%] of VGG-16bn on ImageNet (FP: 73.35%). The baselines
(Base.) use equal bit widths for weights and activations (leftmost column). The bit width determines
the power P, which we specify in parentheses in units of Giga bit-ﬂips. The power is calculated as
P u
mult + P u
acc (Eqs. (3),(4)) times the number of MACs in the network. (15.53 × 109 in VGG-16bn).
In each row, our variant is tuned to work at the same power budget, for which we choose the optimal
˜bx and R using Alg. 1. We failed to run BRECQ due to a CUDA ‘out of memory’ error."
-BIT,0.7246376811594203,"quantize the bits and the activation to 4 or 2 bits (see caption). We measure the power in bit ﬂips.
Speciﬁcally, the power of each signed MAC is calculated by Pmult + Pacc (Eqs. (1),(2)) times the
number of MACs in the network. Then, we switch to unsigned arithmetic (←). In this case the power
is calculated by P u
mult + P u
acc (Eqs. (3),(4)) times the number of MACs in the network. For simplicity,
let us denote the total power as P u. As can be seen, this stage save power but does not change the
accuracy (points marked as ‘Unsigned’). Constraining to the same power budget P u, we apply PANN
(↑). Using Alg. 1, we calculate the optimal activation bit width and the corresponding addition factor
(points marked as ‘PANN’). We can see a dramatic improvement in classiﬁcation accuracy without
any change in the power consumption. ACIQ PANN PANN"
-BIT,0.7270531400966184,Unsigned PANN PANN
-BIT,0.7294685990338164,Unsigned ACIQ
-BIT,0.7318840579710145,Unsigned
-BIT,0.7342995169082126,"ACIQ
Unsigned ACIQ"
-BIT,0.7367149758454107,(a) Improving upon ACIQ (4-bit MAC) GDFQ PANN
-BIT,0.7391304347826086,"GDFQ
Unsigned PANN PANN PANN"
-BIT,0.7415458937198067,Unsigned
-BIT,0.7439613526570048,Unsigned GDFQ GDFQ
-BIT,0.7463768115942029,(b) Improving upon GDFQ (4-bit MAC)
-BIT,0.748792270531401,"Figure 13: Power-accuracy trade-off at post training. For each pre-trained full-precision model,
we used (a) ACIQ (Banner et al., 2019) and (b) GDFQ (Shoukai et al., 2020) to quantize the weights
and activations to 4 bits at post-training. Converting the quantized models to work with unsigned
arithmetic (←), already cuts down 33% of the power consumption (assuming a 32 bit accumulator).
Using our PANN approach to quantize the weights (at post-training) and remove the multiplier (↑),
further improves model accuracy for the same power level."
-BIT,0.751207729468599,"A.5.2
QUANTIZATION AWARE TRAINING"
-BIT,0.7536231884057971,"In Table 10, we report the classiﬁcation accuracy of different networks on ImageNet, when we use
PANN during training. Each row deﬁnes a speciﬁc power budget, corresponding to 2-bit, 3-bit and"
-BIT,0.7560386473429952,Under review as a conference paper at ICLR 2022 ZeroQ PANN PANN PANN ZeroQ ZeroQ PANN
-BIT,0.7584541062801933,"Unsigned
Unsigned
Unsigned"
-BIT,0.7608695652173914,(a) Improving upon ZeroQ (2-bit MAC) PANN GDFQ
-BIT,0.7632850241545893,Unsigned PANN PANN
-BIT,0.7657004830917874,Unsigned
-BIT,0.7681159420289855,"GDFQ
Unsigned GDFQ PANN"
-BIT,0.7705314009661836,(b) Improving upon GDFQ (2-bit MAC)
-BIT,0.7729468599033816,"Figure 14: Power-accuracy trade-off at post training. For each pre-trained full-precision model,
we used (a) ZeroQ (Cai et al., 2020) and (b) GDFQ (Shoukai et al., 2020) to quantize the weights
and activations to 2 bits at post-training. Converting the quantized models to work with unsigned
arithmetic (←), already cuts down 58% of the power consumption (assuming a 32 bit accumulator).
Using our PANN approach to quantize the weights (at post-training) and remove the multiplier (↑),
further improves model accuracy for the same power level."
-BIT,0.7753623188405797,"4-bit unsigned MACs (weights and activations have equal bit widths). We compare our results to
LSQ (Esser et al., 2019), whose accuracy is reported in parentheses. Note that the total number of
bit ﬂips differs between the networks, because each has a different number of MACS in its forward
pass. Therefore, instead of specifying results as a function of the total number of bit ﬂips, we report
results as a function of the bit-width. Each bit-width deﬁnes a power budget for which we tune PANN
(Alg. 1), where we use LSQ to quantize the activations. We can see that PANN outperforms LSQ at
all power budgets."
-BIT,0.7777777777777778,"Power (bit-width)
ResNet-18
ResNet-34
ResNet-50
ResNet-101
VGG-16bn"
-BIT,0.7801932367149759,"FP
70.13
73.88
76.87
77.55
73.33
2
70.03 (67.32)
72.54 (71.21)
76.65 (71.36)
77.13 (75.21)
73.30 (71.15)
3
70.12 (69.81)
73.87 (72.88)
76.78 (73.54)
77.24 (76.62)
73.31 (73.26)
4
70.10 (70.13)
73.96 (73.90)
76.81 (76.89)
77.33 (77.52)
73.46 (73.51)"
-BIT,0.782608695652174,"Table 10: PANN for QAT. Here we report more results of PANN for quantization aware training. In
parentheses we report the classiﬁcation accuracy [%] of LSQ on Imagenet, where both activations
and weights are quantized to 2,3 or 4 bits. As for PANN, we follow Alg. 1 to calculate the optimal
activation bit width and addition factor."
-BIT,0.785024154589372,"A.5.3
ADDITIONAL COMPARISONS TO MULTIPLICATION-FREE METHODS"
-BIT,0.7874396135265701,"In tables 11-12 we report additional comparisons with the recent multiplication free methods Shif-
tAddNet (You et al., 2020) and AdderNet (Chen et al., 2020), this time on the CIFAR100 and
MHEALTH Banos et al. (2014) datasets. Here again PANN is used during training, like the compet-
ing methods."
-BIT,0.7898550724637681,"A.6
HYPER PARAMETERS FOR PANN IN QAT"
-BIT,0.7922705314009661,"A.6.1
LSQ"
-BIT,0.7946859903381642,"In all experiments we used the SGD optimizer with momentum of 0.9 and weight decay of 10−4.
We used the softmax cross entropy loss. Unlike the original paper, we started the training from"
-BIT,0.7971014492753623,Under review as a conference paper at ICLR 2022
-BIT,0.7995169082125604,"METHOD
6/6
5/5
4/4
3/3"
-BIT,0.8019323671497585,"OUR (1×)
66.16
64.50
62.80
55.51
OUR (1.5×)
66.23
65.8
63.58
56.85
OUR (2×)
66.90
66.50
63.99
57.51
SHIFTADDNET (1.5×)
64.08
64.05
63.23
61.31
ADDERNET (2×)
41.57
35.20
29.19
21.50"
-BIT,0.8043478260869565,"Table 11: QAT: Comparison with multiplier-free methods.
Classiﬁcation accuracy [%] of
multiplier-free methods on CIFAR-100. The top row speciﬁes weight/activation bit widths, and the
addition factor is speciﬁed in parentheses."
-BIT,0.8067632850241546,"METHOD
6/6
5/5
4/4
3/3"
-BIT,0.8091787439613527,"OUR (1×)
95.01
84.13
65.36
59.9
OUR (1.5×)
95.05
85.91
68.96
62.32
OUR (2×)
95.34
87.36
70.82
62.51
SHIFTADDNET YOU ET AL. (2020) (1.5×)
85.61
63.34
35.77
18.19
ADDERNET CHEN ET AL. (2020) (2×)
89.31
68.21
26.77
10.56"
-BIT,0.8115942028985508,"Table 12: QAT: Comparison with multiplier-free methods.
Classiﬁcation accuracy [%] of
multiplier-free methods on the MHEALTH dataset Banos et al. (2014). The top row speciﬁes
weight/activation bit widths, and the addition factor is speciﬁed in parentheses."
-BIT,0.8140096618357487,"pre-trained networks and with a smaller initial learning rate of 10−3. Please refer to Table 13 for
PANN details, scheduling and number of epochs used for the training."
-BIT,0.8164251207729468,"ARCH.
QAT (bx/bw)
P
˜bx
R
LR SCHEDULE
Bs
EPOCHS"
-BIT,0.8188405797101449,"RESNET-18
LSQ (2/2)
18
3
2.83
×0.1 EVERY 25 EPOCHS
128
75
RESNET-18
LSQ (3/3)
30
6
2.5
×0.1 EVERY 25 EPOCHS
128
75
RESNET-18
LSQ (4/4)
43
6
3.5
×0.1 EVERY 25 EPOCHS
128
75
RESNET-34
LSQ (2/2)
36
3
2.83
×0.1 EVERY 20 EPOCHS
64
60
RESNET-34
LSQ (3/3)
61
6
2.5
×0.1 EVERY 20 EPOCHS
64
60
RESNET-34
LSQ (4/4)
88
6
3.5
×0.1 EVERY 20 EPOCHS
64
60
RESNET-50
LSQ (2/2)
41
3
2.83
×0.1 EVERY 20 EPOCHS
64
60
RESNET-50
LSQ (3/3)
68
6
2.5
×0.1 EVERY 20 EPOCHS
64
60
RESNET-50
LSQ (4/4)
99
6
3.5
×0.1 EVERY 20 EPOCHS
64
60
RESNET-101
LSQ (2/2)
78
3
2.83
×0.1 EVERY 20 EPOCHS
64
60
RESNET-101
LSQ (3/3)
128
6
2.5
×0.1 EVERY 20 EPOCHS
64
60
RESNET-101
LSQ (4/4)
187
6
3.5
×0.1 EVERY 20 EPOCHS
64
60
VGG-16BN
LSQ (2/2)
155
3
2.83
×0.1 EVERY 20 EPOCHS
64
60
VGG-16BN
LSQ (3/3)
279
6
2.5
×0.1 EVERY 20 EPOCHS
64
60
VGG-16BN
LSQ (4/4)
372
6
3.5
×0.1 EVERY 20 EPOCHS
64
60"
-BIT,0.821256038647343,"Table 13: Hyper-parameters used in LSQ. When using pure LSQ as the baseline approach, we
quantize both the weights and the activations to the same bit width as speciﬁed in the second column
(bx/bw). Then, when applying PANN, we keep the exact training regime and the quantized activations,
and only change the quantized weights to be calculated by PANN. Here we report the optimal bit
width for the activations and the corresponding addition factor (Alg. 1)."
-BIT,0.8236714975845411,"A.6.2
MULTIPLIER FREE APPROACHES"
-BIT,0.8260869565217391,"In all experiments we followed the training regime described in (You et al., 2020). Speciﬁcally, for
CIFAR10 or CIFAR100 we used a batch size of 256, and 160 epochs. The initial learning rate was
0.1 and then divided by 10 at the 80-th and the 120-th epoch. We used the SGD optimizer with
momentum of 0.9 and weight decay of 1e −4. For the MHEALTH dataset, we used only 40 epochs
to train. The initial learning rate was 0.01 and then divided by 10 at the 20-th and the 30-th epochs."
-BIT,0.8285024154589372,Under review as a conference paper at ICLR 2022
-BIT,0.8309178743961353,"Similarly to the CIFAR experiments, we used SGD optimizer with momentum of 0.9 and weight
decay of 1e −4."
-BIT,0.8333333333333334,"A.7
HARDWARE-ACCURACY TRADE-OFF"
-BIT,0.8357487922705314,"When operating on a single image at inference time (rather than on a large batch), the memory
footprint of the weights is not negligible anymore (Mishra et al., 2017). Therefore, we need to also
account for the bit-width required for storing the quantized weights. In Table 14 we report the optimal
activation bit width and addition factor for each power constraint in a certain setting. Speciﬁcally, we
use ZeroQ to quantize the activations of a pre-trained full-precision ResNet-50. We then measure the
maximal addition factor per neuron, which deﬁnes the bit width bR required to store the weights. We
can observe that overall, the increase in the runtime memory footprint of the weights is relatively low,
especially in the low power regimes."
-BIT,0.8381642512077294,"Up to now, we have only shown results with the bit width ˜bx (and corresponding additions factor R)
that is optimal in terms of classiﬁcation accuracy. However, for a given power budget P, choosing
˜bx (and R) can be done while also accounting for other factors, like latency and memory footprint.
We illustrate this in Table 15 for the case of a power constraing corresponding to 2-bit MAC. Here,
we report results for all options for ˜bx and R that conform to that power budget. While ˜bx = 6
and R = 1.16 is optimal in terms of classiﬁcation accuracy, the user can choose other options,
e.g., according to latency or memory constraints."
-BIT,0.8405797101449275,"POWER (bx/bw)
˜bx
LATENCY(= R)
bR
ACTIVATIONS MEMORY
WEIGHTS MEMORY"
-BIT,0.8429951690821256,"2/2
6
1.16×
3
3×
1.5×
3/3
6
2.25×
3
2×
1×
4/4
7
2.9×
3
1.75×
0.75×
5/5
8
3.5×
4
1.6×
0.8×
6/6
8
4.75×
5
1.33×
0.83×
7/7
8
6.06×
5
1.14×
0.714×
8/8
8
7.5×
5
1×
0.625×"
-BIT,0.8454106280193237,"Table 14: Runtime memory footprint of PANN. We report the increase in the memory required
to store the weights and activations, when using PANN. Each row speciﬁes a power budget, corre-
sponding to a bx bit width unsigned MAC. We follow Alg. 1 to ﬁnd the optimal bit width ˜bx and
the additions factor R, which is equal to the latency increase. We measure the maximal value of
additions per neuron which deﬁnes the required number of bits for storing the weights (bR). Then,
we calculate the increase in weights memory footprint as bR/bx."
-BIT,0.8478260869565217,"˜bx
LATENCY(= R)
bR
ACTIVATIONS MEMORY
WEIGHTS MEMORY
ACCURACY [%]"
-BIT,0.8502415458937198,"2
4.5×
5
1×
2.5×
0.9
3
2.83×
3
1.5×
1×
6.55
4
2.0×
3
2×
0.75×
61.15
5
1.5×
2
2.5×
0.4×
65.69
6
1.16×
2
3×
0.33×
71.55
7
0.92×
2
3.5×
0.28×
70.18
8
0.75×
2
4×
0.25×
60.01"
-BIT,0.8526570048309179,"Table 15: Hardware-accuracy trade-off. Here we analyze the run-time memory footprint and
latency increase for all different values of ˜bx and R that lead to the same power of a 2-bit unsigned
MAC (blue curve in Figure 3(a)). For each setting we measure the classiﬁcation accuracy of ResNet-
50 on ImageNet. Here we use ACIQ (Banner et al., 2019) for quantizing the activations. The baseline
(pure ACIQ) accuracy is 0.20% (Table 2, third column, last row)."
-BIT,0.855072463768116,Under review as a conference paper at ICLR 2022
-BIT,0.857487922705314,"Bit Width
Bit Width
Bit Width
Bit Width"
-BIT,0.8599033816425121,"Gaussian
Uniform 
ImageNet"
-BIT,0.8623188405797102,"Bit Width
Bit Width
Bit Width
Bit Width"
-BIT,0.8647342995169082,"Bit Width
Bit Width
Bit Width
Bit Width"
-BIT,0.8671497584541062,Power of unsigned
-BIT MAC,0.8695652173913043,2-bit MAC
-BIT MAC,0.8719806763285024,Power of unsigned
-BIT MAC,0.8743961352657005,3-bit MAC
-BIT MAC,0.8768115942028986,Power of unsigned
-BIT MAC,0.8792270531400966,4-bit MAC
-BIT MAC,0.8816425120772947,Power of unsigned
-BIT MAC,0.8840579710144928,5-bit MAC
-BIT MAC,0.8864734299516909,"MSE
MSE
Error [%]"
-BIT MAC,0.8888888888888888,"MSE
MSE
Error [%]"
-BIT MAC,0.8913043478260869,"MSE
MSE
Error [%]"
-BIT MAC,0.893719806763285,"MSE
MSE
Error [%]"
-BIT MAC,0.8961352657004831,"Figure 15: Optimal bit width analysis for PANN. The ﬁrst two rows depict the MSE as a function
of the activation bit width ˜bx for the cases where the weights and the activations are uniformly
distributed and for the setting in which they are Gaussian (the activations are further subjected to a
ReLU function in this case). The third row shows the classiﬁcation error of a ResNet18 model on
ImageNet. Although the precise value of the optimal ˜bx is a bit different than the ﬁrst two rows, the
qualitative behavior is similar. For both for the Gaussian simulation and the ImageNet results, we
used ACIQ Banner et al. (2019) to quantize the activations."
-BIT MAC,0.8985507246376812,"A.8
QUANTIZATION ERROR ANALYSIS"
-BIT MAC,0.9009661835748792,"Figure 15 (ﬁrst row) depicts MSEPANN (Eq. (19)) as a function of ˜bx for several power budgets P.
First, it can be seen that our theoretical analysis agrees well with simulations. Second, it shows that
the optimal ˜bx (where the minimum MSE is attained) increases with the power budget. This implies
that at higher power budgets, it is preferable to increase the bit width of the activations on the expense
of reducing the number of additions R. The second row of the ﬁgure illustrates that the qualitative
conclusions drawn from our uniform distribution analysis also hold when the weights are Gaussian
and the activations are Gaussian numbers after a ReLU function (here we used the ACIQ quantizer
(Banner et al., 2019)). In the third row, we show that similar behaviors characterise the error rates of
a ResNet-18 model for ImageNet classiﬁcation when using PANN to quantize its weights and ACIQ
to quantize the activations."
-BIT MAC,0.9033816425120773,Under review as a conference paper at ICLR 2022
-BIT MAC,0.9057971014492754,"A.9
PROOF OF EQ. (14)"
-BIT MAC,0.9082125603864735,"Let w and x be statistically independent random vectors, each with iid components. Recall that wq
and xq are obtained by applying a scalar function on each of the elements of w and x, respectively.
Therefore wq and xq also have iid components. We assume that w = wq + εw and x = xq + εx,
where E[εw|w] = 0 and E[εx|x] = 0. Then we have that"
-BIT MAC,0.9106280193236715,"MSE = E
h 
wT x −wT
q xq
2i"
-BIT MAC,0.9130434782608695,"= E
h 
wT x −(w + εw)T (x + εx)
2i"
-BIT MAC,0.9154589371980676,"= E
h 
wT x −(wT x + wT εx + εT
wx + εT
wεx)
2i"
-BIT MAC,0.9178743961352657,"= E
h 
wT εx + εT
wx + εT
wεx
2i
(21)"
-BIT MAC,0.9202898550724637,"and therefore,"
-BIT MAC,0.9227053140096618,"MSE = E
h 
wT εx
2i
+ E
h 
εT
wx
2i
+ E
h 
εT
wεx
2i"
-BIT MAC,0.9251207729468599,"+ 2E

wT εxεT
wx

+ 2E

wT εxεT
wεx

+ 2E

εT
wxεT
wεx

.
(22)"
-BIT MAC,0.927536231884058,"We now turn to show that the last three terms equal zero. For the ﬁrst of those terms, we have"
-BIT MAC,0.9299516908212561,"E

wT εxεT
wx

= E

wT εxxT εw
"
-BIT MAC,0.9323671497584541,"= E

E

wT εxxT εw | w, εw
"
-BIT MAC,0.9347826086956522,"= E

wT E

εxxT | w, εw

εw
"
-BIT MAC,0.9371980676328503,"= E

wT E

εxxT 
εw
"
-BIT MAC,0.9396135265700483,"= 0,
(23)"
-BIT MAC,0.9420289855072463,"where in the second line we used the law of total expectations, in the fourth line we used the fact that
the pair {w, εw} is statistically independent of the pair {x, εx}, and in the ﬁfth line we used the fact
that E[εxxT ] = E[E[εxxT | x]] = E[E[εx|x]xT ] = 0 because of our assumption that E[εx|x] = 0."
-BIT MAC,0.9444444444444444,"For the second among the last three terms in (22), we have that"
-BIT MAC,0.9468599033816425,"E

wT εxεT
wεx

= E

εT
xwεT
wεx
"
-BIT MAC,0.9492753623188406,"= E

εT
xwεT
wεx | εx
"
-BIT MAC,0.9516908212560387,"= E

εT
x

wεT
w | εx

εx
"
-BIT MAC,0.9541062801932367,"= E

εT
x

wεT
w

εx
"
-BIT MAC,0.9565217391304348,"= 0,
(24)"
-BIT MAC,0.9589371980676329,"where we used the fact that the pair {w, εw} is independent of εx, and E[wεT
w] = E[E[wεT
w|w]] =
E[wE[εw|w]T ] = 0 because of our assumption that E[εw|w] = 0."
-BIT MAC,0.961352657004831,"For the the last term in (22), we have that"
-BIT MAC,0.9637681159420289,"E

εT
wxεT
wεx

= E

εT
wxεT
xεw
"
-BIT MAC,0.966183574879227,"= E

E

εT
wxεT
xεw | εw
"
-BIT MAC,0.9685990338164251,"= E

εT
wE

xεT
x | εw

εw
"
-BIT MAC,0.9710144927536232,"= E

εT
wE

xεT
x

εw
"
-BIT MAC,0.9734299516908212,"= 0,
(25)"
-BIT MAC,0.9758454106280193,Under review as a conference paper at ICLR 2022
-BIT MAC,0.9782608695652174,"where we used the fact that the pair {x, εx} is independent of εw, and E[xεT
x] = 0, as in (23). We
thus remain only with the ﬁrst three terms of (22), so that"
-BIT MAC,0.9806763285024155,"MSE = E
h 
wT εx
2i
+ E
h 
εT
wx
2i
+ E
h 
εT
wεx
2i"
-BIT MAC,0.9830917874396136,"= E

wT εxεT
xw

+ E

xT εwεT
wx

+ E

εT
wεxεT
xεw
"
-BIT MAC,0.9855072463768116,"= E

E

wT εxεT
xw | w

+ E

E

xT εwεT
wx | x

+ E

E

εT
wεxεT
xεw | εw
"
-BIT MAC,0.9879227053140096,"= E

wT E

εxεT
x | w

w

+ E

xT E

εwεT
w | x

x

+ E

εT
wE

εxεT
x | εw

εw
"
-BIT MAC,0.9903381642512077,"= E

wT E

εxεT
x

w

+ E

xT E

εwεT
w

x

+ E

εT
wE

εxεT
x

εw
"
-BIT MAC,0.9927536231884058,"= E

wT  
σ2
εxI

w

+ E

xT  
σ2
εwI

x

+ E

εT
w
 
σ2
εxI

εw
"
-BIT MAC,0.9951690821256038,"= d
 
σ2
wσ2
εx + σ2
xσ2
εw + σ2
εxσ2
εw

,
(26)"
-BIT MAC,0.9975845410628019,"where σ2
w, σ2
x, σ2
εw, and σ2
εx denote the second-order moments of the elements of w, x, εw, and εx,
respectively, and I is the d × d identity matrix. Here, in the ﬁfth equality we used the fact that w
is independent of εx, x is independent of εw, and εw is independent of εx. In the sixth equality
we used the fact that εx and εw are iid vectors with zero mean, since E[εx] = E[E[εx|x]] = 0 and
E[εw] = E[E[εw|w]] = 0. This completes the proof of Eq. (14) in the main text."
