Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0007974481658692185,"Metriplectic systems are learned from data in a way that scales quadratically in both
1"
ABSTRACT,0.001594896331738437,"the size of the state and the rank of the metriplectic data. Besides being provably
2"
ABSTRACT,0.0023923444976076554,"energy conserving and entropy stable, the proposed approach comes with approxi-
3"
ABSTRACT,0.003189792663476874,"mation results demonstrating its ability to accurately learn metriplectic dynamics
4"
ABSTRACT,0.003987240829346092,"from data as well as an error estimate indicating its potential for generalization to
5"
ABSTRACT,0.004784688995215311,"unseen timescales when approximation error is low. Examples are provided which
6"
ABSTRACT,0.005582137161084529,"illustrate performance in the presence of both full state information as well as when
7"
ABSTRACT,0.006379585326953748,"entropic variables are unknown, confirming that the proposed approach exhibits
8"
ABSTRACT,0.007177033492822967,"superior accuracy and scalability without compromising on model expressivity.
9"
INTRODUCTION,0.007974481658692184,"1
Introduction
10"
INTRODUCTION,0.008771929824561403,"The theory of metriplectic, also called GENERIC, systems [1, 2] provides a principled formalism
11"
INTRODUCTION,0.009569377990430622,"for encoding dissipative dynamics in terms of complete thermodynamical systems that conserve
12"
INTRODUCTION,0.01036682615629984,"energy and produce entropy. By formally expressing the reversible and irreversible parts of state
13"
INTRODUCTION,0.011164274322169059,"evolution with separate algebraic brackets, the metriplectic formalism provides a general mechanism
14"
INTRODUCTION,0.011961722488038277,"for maintaining essential conservation laws while simultaneously respecting dissipative effects.
15"
INTRODUCTION,0.012759170653907496,"Thermodynamic completeness implies that any dissipation is caught within a metriplectic system
16"
INTRODUCTION,0.013556618819776715,"through the generation of entropy, allowing for a holistic treatment which has already found use in
17"
INTRODUCTION,0.014354066985645933,"modeling fluids [3, 4], plasmas [5, 6], and kinetic theories [7, 8].
18"
INTRODUCTION,0.015151515151515152,"From a machine learning point of view, the formal separation of conservative and dissipative effects
19"
INTRODUCTION,0.01594896331738437,"makes metriplectic systems highly appealing for the development of phenomenological models. Given
20"
INTRODUCTION,0.01674641148325359,"data which is physics-constrained or exhibits some believed properties, a metriplectic system can be
21"
INTRODUCTION,0.017543859649122806,"learned to exhibit the same properties with clearly identifiable conservative and dissipative parts. This
22"
INTRODUCTION,0.018341307814992026,"allows for a more nuanced understanding of the governing dynamics via an evolution equation which
23"
INTRODUCTION,0.019138755980861243,"reduces to an idealized Hamiltonian system as the dissipation is taken to zero. Moreover, elements
24"
INTRODUCTION,0.019936204146730464,"in the kernel of the learned conservative part are immediately understood as Casimir invariants,
25"
INTRODUCTION,0.02073365231259968,"which are special conservation laws inherent to the phase space of solutions, and are often useful
26"
INTRODUCTION,0.0215311004784689,"for understanding and exerting control on low-dimensional structure in the system. On the other
27"
INTRODUCTION,0.022328548644338118,"hand, the same benefit of metriplectic structure as a “direct sum” of reversible and irreversible parts
28"
INTRODUCTION,0.023125996810207338,"makes it challenging to parameterize in an efficient way, since delicate degeneracy conditions must
29"
INTRODUCTION,0.023923444976076555,"be enforced in the system for all time. In fact, there are no methods at present for learning general
30"
INTRODUCTION,0.024720893141945772,"metriplectic systems which scale optimally with both dimension and the rank of metriplectic data—an
31"
INTRODUCTION,0.025518341307814992,"issue which this work directly addresses.
32"
INTRODUCTION,0.02631578947368421,"Precisely, metriplectic dynamics on the finite or infinite dimensional phase space P are generated by a
33"
INTRODUCTION,0.02711323763955343,"free energy function(al) F : P →R, F = E+S defined in terms of a pair E, S : P →R representing
34"
INTRODUCTION,0.027910685805422646,"energy and entropy, respectively, along with two algebraic brackets {·, ·}, [·, ·] : C∞(P)×C∞(P) →
35"
INTRODUCTION,0.028708133971291867,"C∞(P) which are bilinear derivations on C∞(P) with prescribed symmetries and degeneracies
36"
INTRODUCTION,0.029505582137161084,"{S, ·} = [E, ·] = 0. Here {·, ·} is an antisymmetric Poisson bracket, which is a Lie algebra
37"
INTRODUCTION,0.030303030303030304,"realization on functions, and [·, ·] is a degenerate metric bracket which is symmetric and positive
38"
INTRODUCTION,0.03110047846889952,"semi-definite. When P ⊂Rn for some n > 0, these brackets can be identified with symmetric
39"
INTRODUCTION,0.03189792663476874,"matrix fields L : P →Skewn(R), M : P →Symn(R) satisfying {F, G} = ∇F · L∇G and
40"
INTRODUCTION,0.03269537480063796,"[F, G] = ∇F · M∇G for all functions F, G ∈C∞(P) and all states x ∈P. Using the degeneracy
41"
INTRODUCTION,0.03349282296650718,"conditions along with ∇x = I and abusing notation slightly then leads the standard equations for
42"
INTRODUCTION,0.0342902711323764,"metriplectic dynamics,
43"
INTRODUCTION,0.03508771929824561,"˙x = {x, F} + [x, F] = {x, E} + [x, S] = L∇E + M∇S,"
INTRODUCTION,0.03588516746411483,"which are provably energy conserving and entropy producing. To see why this is the case, recall that
44"
INTRODUCTION,0.03668261562998405,"L⊺= −L. It follows that the infinitesimal change in energy satisfies
45"
INTRODUCTION,0.037480063795853266,"˙E = ˙x · ∇E = L∇E · ∇E + M∇S · ∇E = −L∇E · ∇E + ∇S · M∇E = 0,"
INTRODUCTION,0.03827751196172249,"and therefore energy is conserved along the trajectory of x. Similarly, the fact that M ⊺= M is
46"
INTRODUCTION,0.03907496012759171,"positive semi-definite implies that
47"
INTRODUCTION,0.03987240829346093,"˙S = ˙x · ∇S = L∇E · ∇S + M∇S · ∇S = −∇E · L∇S + M∇S · ∇S = |∇S|2
M ≥0,"
INTRODUCTION,0.04066985645933014,"so that entropy is nondecreasing along x as well. Geometrically, this means that the motion of a
48"
INTRODUCTION,0.04146730462519936,"trajectory x is everywhere tangent to the level sets of energy and transverse to those of entropy,
49"
INTRODUCTION,0.04226475279106858,"reflecting the fact that metriplectic dynamics are a combination of noncanonical Hamiltonian (M =
50"
INTRODUCTION,0.0430622009569378,"0) and generalized gradient (L = 0) motions. Note that these considerations also imply the
51"
INTRODUCTION,0.043859649122807015,"Lyapunov stability of metriplectic trajectories, as can be seen by taking E as a Lyapunov function.
52"
INTRODUCTION,0.044657097288676235,"Importantly, this also implies that metriplectic trajectories which start in the (often compact) set
53"
INTRODUCTION,0.045454545454545456,"K = {x | E(x) ≤E(x0)} remain there for all time.
54"
INTRODUCTION,0.046251993620414676,"In phenomenological modeling, the entropy S is typically chosen from Casimirs of the Poisson
55"
INTRODUCTION,0.04704944178628389,"bracket generated by L, i.e. those quantities C ∈C∞(P) such that L∇C = 0. On the other hand,
56"
INTRODUCTION,0.04784688995215311,"the method which will be presented here, termed neural metriplectic systems (NMS), allows for all of
57"
INTRODUCTION,0.04864433811802233,"the metriplectic data L, M, E, S to be approximated simultaneously, removing the need for Casimir
58"
INTRODUCTION,0.049441786283891544,"invariants to be known or assumed ahead of time. The only restriction inherent to NMS is that the
59"
INTRODUCTION,0.050239234449760764,"metriplectic system being approximated is nondegenerate (c.f. Definition 3.1), a mild condition
60"
INTRODUCTION,0.051036682615629984,"meaning that the gradients of energy and entropy cannot vanish at any point x ∈P in the phase space.
61"
INTRODUCTION,0.051834130781499205,"It will be shown that NMS alleviates known issues with previous methods for metriplectic learning,
62"
INTRODUCTION,0.05263157894736842,"leading to easier training, superior parametric efficiency, and better generalization performance.
63"
INTRODUCTION,0.05342902711323764,"Contributions.
The proposed NMS method for learning metriplectic models offers the following
64"
INTRODUCTION,0.05422647527910686,"advantages over previous state-of-the-art: (1) It approximates arbitrary nondegenerate metriplectic
65"
INTRODUCTION,0.05502392344497608,"dynamics with optimal quadratic scaling in both the problem dimension n and the rank r of the
66"
INTRODUCTION,0.05582137161084529,"irreversible dynamics. (2) It produces realistic, thermodynamically consistent entropic dynamics
67"
INTRODUCTION,0.05661881977671451,"from unobserved entropy data. (3) It admits universal approximation and error accumulation results
68"
INTRODUCTION,0.05741626794258373,"given in Proposition 3.7 and Theorem 3.9. (4) It yields exact energy conservation and entropy stability
69"
INTRODUCTION,0.058213716108452954,"by construction, allowing for effective generalization to unseen timescales.
70"
PREVIOUS AND RELATED WORK,0.05901116427432217,"2
Previous and Related Work
71"
PREVIOUS AND RELATED WORK,0.05980861244019139,"Previous attempts to learn metriplectic systems from data separate into “hard” and “soft” constrained
72"
PREVIOUS AND RELATED WORK,0.06060606060606061,"methods. Hard constrained methods enforce metriplectic structure by construction, so that the
73"
PREVIOUS AND RELATED WORK,0.06140350877192982,"defining properties of metriplecticity cannot be violated. Conversely, methods with soft constraints
74"
PREVIOUS AND RELATED WORK,0.06220095693779904,"relax some aspects of metriplectic structure in order to produce a wider model class which is easier to
75"
PREVIOUS AND RELATED WORK,0.06299840510366826,"parameterize. While hard constraints are the only way to truly guarantee appropriate generalization
76"
PREVIOUS AND RELATED WORK,0.06379585326953748,"in the learned surrogate, the hope of soft constrained methods is that the resulting model is “close
77"
PREVIOUS AND RELATED WORK,0.0645933014354067,"enough” to metriplectic that it will exhibit some of the favorable characteristics of metriplectic
78"
PREVIOUS AND RELATED WORK,0.06539074960127592,"systems, such as energy and entropy stability. Some properties of the methods compared in this work
79"
PREVIOUS AND RELATED WORK,0.06618819776714513,"are summarized in Table 1.
80"
PREVIOUS AND RELATED WORK,0.06698564593301436,"Soft constrained methods.
Attempts to learn metriplectic systems using soft constraints rely on
81"
PREVIOUS AND RELATED WORK,0.06778309409888357,"relaxing the degeneracy conditions L∇S = M∇E = 0. This is the approach taken in [9], termed
82"
PREVIOUS AND RELATED WORK,0.0685805422647528,"SPNN, which learns an almost-metriplectic model parameterized with generic neural networks
83"
PREVIOUS AND RELATED WORK,0.06937799043062201,"through a simple L2 penalty term in the training loss, Lpen = |L∇E|2 + |M∇S|2. This widens
84"
PREVIOUS AND RELATED WORK,0.07017543859649122,"the space of allowable network parameterizations for the approximate operators L, M. While
85"
PREVIOUS AND RELATED WORK,0.07097288676236045,"the resulting model violates the first and second laws of thermodynamics, the authors show that
86"
PREVIOUS AND RELATED WORK,0.07177033492822966,"reasonable trajectories are still obtained, at least when applied within the range of timescales used
87"
PREVIOUS AND RELATED WORK,0.07256778309409888,"for training. A similar approach is taken in [10], which targets larger problems and develops an
88"
PREVIOUS AND RELATED WORK,0.0733652312599681,"almost-metriplectic model reduction strategy based on the same core idea.
89"
PREVIOUS AND RELATED WORK,0.07416267942583732,"Hard constrained methods.
Perhaps the first example of learning metriplectic systems from data
90"
PREVIOUS AND RELATED WORK,0.07496012759170653,"was given in [11] in the context of system identification. Here, training data is assumed to come from
91"
PREVIOUS AND RELATED WORK,0.07575757575757576,"a finite element simulation, so that the discrete gradients of energy and entropy can be approximated
92"
PREVIOUS AND RELATED WORK,0.07655502392344497,"as ∇E(x) = Ax, ∇S(x) = Bx. Assuming a fixed form for L produces a constrained learning
93"
PREVIOUS AND RELATED WORK,0.0773524720893142,"problem for the constant matrices M, A, B which is solved to yield a provably metriplectic surrogate
94"
PREVIOUS AND RELATED WORK,0.07814992025518341,"model. Similarly, the work [12] learns M, E given L, S by considering a fixed block-wise decoupled
95"
PREVIOUS AND RELATED WORK,0.07894736842105263,"form which trivializes the degeneracy conditions, i.e. L = [⋆0; 0 0] and M = [0 0; 0 ⋆]. This
96"
PREVIOUS AND RELATED WORK,0.07974481658692185,"line of thought is continued in [13] and [14], both of which learn metriplectic systems with neural
97"
PREVIOUS AND RELATED WORK,0.08054226475279107,"network parameterizations by assuming this decoupled block structure. A somewhat broader class
98"
PREVIOUS AND RELATED WORK,0.08133971291866028,"of metriplectic systems are considered in [15] using tools from exterior calculus, with the goal of
99"
PREVIOUS AND RELATED WORK,0.08213716108452951,"learning metriplectic dynamics on graph data. This leads to a structure-preserving network surrogate
100"
PREVIOUS AND RELATED WORK,0.08293460925039872,"which scales linearly in the size of the graph domain, but also cannot express arbitrary metriplectic
101"
PREVIOUS AND RELATED WORK,0.08373205741626795,"dynamics due to the specific choices of model form for L, M.
102"
PREVIOUS AND RELATED WORK,0.08452950558213716,"A particularly inspirational method for learning general metriplectic systems was given in [16] and
103"
PREVIOUS AND RELATED WORK,0.08532695374800638,"termed GNODE, building on parameterizations of metriplectic operators developed in [17]. GNODE
104"
PREVIOUS AND RELATED WORK,0.0861244019138756,"parameterizes learnable reversible and irreversible bracket generating matrices L, M in terms of state-
105"
PREVIOUS AND RELATED WORK,0.08692185007974482,"independent tensors ξ ∈(Rn)⊗3 and ζ ∈(Rn)⊗4: for 1 ≤α, β, γ, µ, ν ≤n, the authors choose
106"
PREVIOUS AND RELATED WORK,0.08771929824561403,Lαβ(x) = P
PREVIOUS AND RELATED WORK,0.08851674641148326,γ ξαβγ∂γS and Mαβ(x) = P
PREVIOUS AND RELATED WORK,0.08931419457735247,"µ,ν ζαµ,βν∂µE∂νE, where ∂αF = ∂F/∂xα, ξ is to-
107"
PREVIOUS AND RELATED WORK,0.09011164274322168,"tally antisymmetric, and ζ is symmetric between the pairs (α, µ) and (β, ν) but antisymmetric within
108"
PREVIOUS AND RELATED WORK,0.09090909090909091,"each of these pairs. The key idea here is to exchange the problem of enforcing degeneracy conditions
109"
PREVIOUS AND RELATED WORK,0.09170653907496013,"L∇E = M∇S = 0 in matrix fields L, M with the problem of enforcing symmetry conditions in
110"
PREVIOUS AND RELATED WORK,0.09250398724082935,"tensor fields ξ, ζ, which is comparatively easier but comes at the expense of underdetermining the
111"
PREVIOUS AND RELATED WORK,0.09330143540669857,"problem. In GNODE, enforcement of these symmetries is handled by a generic learnable 3-tensor
112"
PREVIOUS AND RELATED WORK,0.09409888357256778,"˜ξ ∈(Rn)⊗3 along with learnable matrices D ∈Symr(R) and Λs ∈Skewn(R) for 1 ≤s ≤r ≤n,
113"
PREVIOUS AND RELATED WORK,0.094896331738437,leading to the final parameterizations ξαβγ = 1
PREVIOUS AND RELATED WORK,0.09569377990430622,"3!

˜ξαβγ −˜ξαγβ + ˜ξβγα −˜ξβαγ + ˜ξγαβ −˜ξγβα

and
114"
PREVIOUS AND RELATED WORK,0.09649122807017543,"ζαµ,βν = P"
PREVIOUS AND RELATED WORK,0.09728867623604466,"s,t Λs
αµDstΛt
βν. Along with learnable energy and entropy functions E, S parameterized
115"
PREVIOUS AND RELATED WORK,0.09808612440191387,"by multi-layer perceptrons (MLPs), the data L, M learned by GNODE guarantees metriplectic
116"
PREVIOUS AND RELATED WORK,0.09888357256778309,"structure in the surrogate model and leads to successful learning of metriplectic systems in some
117"
PREVIOUS AND RELATED WORK,0.09968102073365231,"simple cases of interest. However, note that this is a highly redundant parameterization requiring
118
 n
3

+ r
 n
2

+
 r+1
2

+ 2 learnable scalar functions, which exhibits O(n3 + rn2) scaling in the
119"
PREVIOUS AND RELATED WORK,0.10047846889952153,"problem size because of the necessity to compute and store
 n
3

entries of ξ and r
 n
2

entries of Λ.
120"
PREVIOUS AND RELATED WORK,0.10127591706539076,"Additionally, the assumption of state-independence in the bracket generating tensors ξ, ζ is somewhat
121"
PREVIOUS AND RELATED WORK,0.10207336523125997,"restrictive, limiting the class of problems to which GNODE can be applied.
122"
PREVIOUS AND RELATED WORK,0.10287081339712918,"A related approach to learning metriplectic dynamics with hard constraints was seen in [18], which
123"
PREVIOUS AND RELATED WORK,0.10366826156299841,"proposed a series of GFINN architectures depending on how much of the information L, M, E, S
124"
PREVIOUS AND RELATED WORK,0.10446570972886762,"is assumed to be known. In the case that L, M are known, the GFINN energy and entropy are
125"
PREVIOUS AND RELATED WORK,0.10526315789473684,"parameterized with scalar-valued functions f ◦PkerA where f : Rn →R (E or S) is learnable and
126"
PREVIOUS AND RELATED WORK,0.10606060606060606,"PkerA : Rn →Rn is orthogonal projection onto the kernel of the (known) operator A (L or M).
127"
PREVIOUS AND RELATED WORK,0.10685805422647528,"It follows that the gradient ∇(f ◦PkerA) = PkerA∇f(PkerA) lies in the kernel of A, so that the
128"
PREVIOUS AND RELATED WORK,0.1076555023923445,"degeneracy conditions are guaranteed at the expense of constraining the model class of potential ener-
129"
PREVIOUS AND RELATED WORK,0.10845295055821372,"gies/entropies. Alternatively, in the case that all of L, M, E, S are unknown, GFINNs use learnable
130"
PREVIOUS AND RELATED WORK,0.10925039872408293,"scalar functions f for E, S parameterized by MLPs as well as two matrix fields QE, QS ∈Rr×n
131"
PREVIOUS AND RELATED WORK,0.11004784688995216,"with rows given by qf
s =
 
Sf
s ∇f
⊺for learnable skew-symmetric matrices Sf
s , 1 ≤s ≤r,
132"
PREVIOUS AND RELATED WORK,0.11084529505582137,"f = E, S. Along with two triangular (r × r) matrix fields TL, TM, this yields the parameterizations
133"
PREVIOUS AND RELATED WORK,0.11164274322169059,"L(x) = QS(x)⊺(TL(x)⊺−TL(x))QS(x) and M(x) = QE(x)⊺(TM(x)⊺TM(x))QE(x),
134"
PREVIOUS AND RELATED WORK,0.11244019138755981,"which necessarily satisfy the symmetries and degeneracy conditions required for metriplectic struc-
135"
PREVIOUS AND RELATED WORK,0.11323763955342903,"ture. GFINNs are shown to both increase expressivity over the GNODE method as well as decrease
136"
PREVIOUS AND RELATED WORK,0.11403508771929824,"redundancy, since the need for an explicit order-3 tensor field is removed and the reversible and
137"
PREVIOUS AND RELATED WORK,0.11483253588516747,"irreversible brackets are allowed to depend explicitly on the state x. However, GFINNs still exhibit
138"
PREVIOUS AND RELATED WORK,0.11562998405103668,"cubic scaling through the requirement of rn(n −1) + r2 + 2 = O
 
rn2
learnable functions, which
139"
PREVIOUS AND RELATED WORK,0.11642743221690591,"is well above the theoretical minimum required to express a general metriplectic system and leads to
140"
PREVIOUS AND RELATED WORK,0.11722488038277512,"difficulties in training the resulting models.
141"
PREVIOUS AND RELATED WORK,0.11802232854864433,"Model reduction.
Finally, it is worth mentioning the closely related line of work involving model
142"
PREVIOUS AND RELATED WORK,0.11881977671451356,"reduction for metriplectic systems, which began with [19]. As remarked there, preserving metriplec-
143"
PREVIOUS AND RELATED WORK,0.11961722488038277,"ticity in reduced-order models (ROMs) exhibits many challenges due to its delicate requirements on
144"
PREVIOUS AND RELATED WORK,0.12041467304625199,"the kernels of the involved operators. There are also hard and soft constrained approaches: the already
145"
PREVIOUS AND RELATED WORK,0.12121212121212122,"mentioned [10] aims to learn a close-to-metriplectic data-driven ROM by enforcing degeneracies by
146"
PREVIOUS AND RELATED WORK,0.12200956937799043,"penalty, while [20] directly enforces metriplectic structure in projection-based ROMs using exterior
147"
PREVIOUS AND RELATED WORK,0.12280701754385964,"algebraic factorizations. The parameterizations of metriplectic data presented here are related to those
148"
PREVIOUS AND RELATED WORK,0.12360446570972887,"presented in [20], although NMS does not require access to nonzero components of ∇E, ∇S.
149"
FORMULATION AND ANALYSIS,0.12440191387559808,"3
Formulation and Analysis
150"
FORMULATION AND ANALYSIS,0.1251993620414673,"The proposed formulation of the metriplectic bracket-generating operators L, M used by NMS is
151"
FORMULATION AND ANALYSIS,0.12599681020733652,"based on the idea of exploiting structure in the tensor fields ξ, ζ to reduce the necessary number
152"
FORMULATION AND ANALYSIS,0.12679425837320574,"of degrees of freedom. In particular, it will be shown that the degeneracy conditions L∇S =
153"
FORMULATION AND ANALYSIS,0.12759170653907495,"M∇E = 0 imply more than just symmetry constraints on these fields, and that taking these additional
154"
FORMULATION AND ANALYSIS,0.1283891547049442,"constraints into account allows for a more compact representation of metriplectic data. Following
155"
FORMULATION AND ANALYSIS,0.1291866028708134,"this, results are presented which show that the proposed formulation is universally approximating on
156"
FORMULATION AND ANALYSIS,0.12998405103668262,"nondegenerate systems (c.f. Definition 3.1) and admits a generalization error bound in time.
157"
EXTERIOR ALGEBRA,0.13078149920255183,"3.1
Exterior algebra
158"
EXTERIOR ALGEBRA,0.13157894736842105,"Developing these metriplectic expressions will require some basic facts from exterior algebra, of
159"
EXTERIOR ALGEBRA,0.13237639553429026,"which more details can be found in, e.g., [21, Chapter 19]. The basic objects in the exterior algebra
160
VV over the vector space V are multivectors, which are formal linear combinations of totally
161"
EXTERIOR ALGEBRA,0.1331738437001595,"antisymmetric tensors on V . More precisely, if I(V ) denotes the two-sided ideal of the free tensor
162"
EXTERIOR ALGEBRA,0.1339712918660287,"algebra T(V ) generated by elements of the form v ⊗v (v ∈V ), then the exterior algebra is the
163"
EXTERIOR ALGEBRA,0.13476874003189793,"quotient space VV ≃T(V )/I(V ) equipped with the antisymmetric wedge product operation ∧.
164"
EXTERIOR ALGEBRA,0.13556618819776714,"This graded algebra is equipped with natural projection operators P k : VV →VkV which map
165"
EXTERIOR ALGEBRA,0.13636363636363635,"between the full exterior algebra and the kth exterior power of V , denoted VkV , whose elements
166"
EXTERIOR ALGEBRA,0.1371610845295056,"are homogeneous k-vectors. More generally, given an n-manifold M with tangent bundle TM, the
167"
EXTERIOR ALGEBRA,0.1379585326953748,"exterior algebra V(TM) is the algebra of multivector fields whose fiber over x ∈M is given by
168
VTxM.
169"
EXTERIOR ALGEBRA,0.13875598086124402,"For the present purposes, it will be useful to develop a correspondence between bivectors B ∈V2(Rn)
170"
EXTERIOR ALGEBRA,0.13955342902711323,"and skew-symmetric matrices B ∈Skewn(R), which follows directly from Riesz representation in
171"
EXTERIOR ALGEBRA,0.14035087719298245,"terms of the usual Euclidean dot product. More precisely, supposing that e1, ..., en are the standard
172"
EXTERIOR ALGEBRA,0.14114832535885166,"basis vectors for Rn, any bivector B ∈V2TRn can be represented as B = P"
EXTERIOR ALGEBRA,0.1419457735247209,"i<j Bijei ∧ej where
173"
EXTERIOR ALGEBRA,0.14274322169059012,"Bij ∈R denote the components of B. Define a grade-lowering action of bivectors on vectors through
174"
EXTERIOR ALGEBRA,0.14354066985645933,"right contraction (see e.g. Section 3.4 of [22]), expressed for any vector v and basis bivector ei ∧ej
175"
EXTERIOR ALGEBRA,0.14433811802232854,"as (ei ∧ej) · v = (ej · v)ei −(ei · v)ej. It follows that the action of B is equivalent to
176"
EXTERIOR ALGEBRA,0.14513556618819776,"B · v =
X"
EXTERIOR ALGEBRA,0.145933014354067,"i<j
Bij((ej · v)ei −(ei · v)ej) =
X"
EXTERIOR ALGEBRA,0.1467304625199362,"i<j
Bijvjei −
X"
EXTERIOR ALGEBRA,0.14752791068580542,"j<i
Bjivjei =
X"
EXTERIOR ALGEBRA,0.14832535885167464,"i,j
Bijvjei = Bv,"
EXTERIOR ALGEBRA,0.14912280701754385,"where B⊺= −B ∈Rn×n is a skew-symmetric matrix representing B, and we have re-indexed
177"
EXTERIOR ALGEBRA,0.14992025518341306,"under the second sum and applied that Bij = −Bji for all i, j. Since the kernel of this action is
178"
EXTERIOR ALGEBRA,0.1507177033492823,"the zero bivector, it is straightforward to check that this string of equalities defines an isomorphism
179"
EXTERIOR ALGEBRA,0.15151515151515152,"M : V2Rn →Skewn(R) from the 2nd exterior power of Rn to the space of skew-symmetric
180"
EXTERIOR ALGEBRA,0.15231259968102073,"(n×n)-matrices over R: in what follows, we will write B ≃B rather than B = M(B) for notational
181"
EXTERIOR ALGEBRA,0.15311004784688995,"convenience. Note that a correspondence in the more general case of bivector/matrix fields follows in
182"
EXTERIOR ALGEBRA,0.15390749601275916,"the usual way via the fiber-wise extension of M.
183"
LEARNABLE METRIPLECTIC OPERATORS,0.1547049441786284,"3.2
Learnable metriplectic operators
184"
LEARNABLE METRIPLECTIC OPERATORS,0.15550239234449761,"It is now possible to explain the proposed NMS formulation. First, note the following key definition
185"
LEARNABLE METRIPLECTIC OPERATORS,0.15629984051036683,"which prevents the consideration of unphysical examples.
186"
LEARNABLE METRIPLECTIC OPERATORS,0.15709728867623604,"Definition 3.1. A metriplectic system on K ⊂Rn generated by the data L, M, E, S will be called
187"
LEARNABLE METRIPLECTIC OPERATORS,0.15789473684210525,"nondegenerate provided ∇E, ∇S ̸= 0 for all x ∈K.
188"
LEARNABLE METRIPLECTIC OPERATORS,0.15869218500797447,"With this, the NMS parameterizations for metriplectic operators are predicated on an algebraic result
189"
LEARNABLE METRIPLECTIC OPERATORS,0.1594896331738437,"proven in Appendix A.
190"
LEARNABLE METRIPLECTIC OPERATORS,0.16028708133971292,"Lemma 3.2. Let K ⊂Rn. For all x ∈K, the operator L : K →Rn×n satisfies L⊺= −L
191"
LEARNABLE METRIPLECTIC OPERATORS,0.16108452950558214,"and L∇S = 0 for some S : K →R, ∇S ̸= 0, provided there exists a non-unique bivector field
192"
LEARNABLE METRIPLECTIC OPERATORS,0.16188197767145135,"A : U →V2Rn and equivalent matrix field A ≃A such that
193 L ≃ "
LEARNABLE METRIPLECTIC OPERATORS,0.16267942583732056,"A ∧
∇S |∇S|2 !"
LEARNABLE METRIPLECTIC OPERATORS,0.1634768740031898,"· ∇S = A −
1"
LEARNABLE METRIPLECTIC OPERATORS,0.16427432216905902,|∇S|2 A∇S ∧∇S.
LEARNABLE METRIPLECTIC OPERATORS,0.16507177033492823,"Similarly, for all x ∈K a positive semi-definite operator M : K →Rn×n satisfies M ⊺= M
194"
LEARNABLE METRIPLECTIC OPERATORS,0.16586921850079744,"and M∇E = 0 for some E : K →R, ∇E ̸= 0, provided there exists a non-unique matrix-valued
195"
LEARNABLE METRIPLECTIC OPERATORS,0.16666666666666666,"B : K →Rn×r and symmetric matrix-valued D : K →Rr×r such that r ≤n and
196 M =
X"
LEARNABLE METRIPLECTIC OPERATORS,0.1674641148325359,"s,t
Dst "
LEARNABLE METRIPLECTIC OPERATORS,0.1682615629984051,"bs ∧
∇E |∇E|2 !"
LEARNABLE METRIPLECTIC OPERATORS,0.16905901116427433,· ∇E ⊗ 
LEARNABLE METRIPLECTIC OPERATORS,0.16985645933014354,"bt ∧
∇E |∇E|2 ! · ∇E =
X"
LEARNABLE METRIPLECTIC OPERATORS,0.17065390749601275,"s,t
Dst "
LEARNABLE METRIPLECTIC OPERATORS,0.17145135566188197,bs −bs · ∇E
LEARNABLE METRIPLECTIC OPERATORS,0.1722488038277512,|∇E|2 ∇E !
LEARNABLE METRIPLECTIC OPERATORS,0.17304625199362042,bt −bt · ∇E
LEARNABLE METRIPLECTIC OPERATORS,0.17384370015948963,"|∇E|2 ∇E !⊺ ,"
LEARNABLE METRIPLECTIC OPERATORS,0.17464114832535885,"where bs denotes the sth column of B. Moreover, using P ⊥
f
=

I −∇f ∇f ⊺"
LEARNABLE METRIPLECTIC OPERATORS,0.17543859649122806,"|∇f|2

to denote the
197"
LEARNABLE METRIPLECTIC OPERATORS,0.1762360446570973,"orthogonal projector onto Span(∇f)⊥, these parameterizations of L, M are equivalent to the
198"
LEARNABLE METRIPLECTIC OPERATORS,0.17703349282296652,"matricized expressions L = P ⊥
S AP ⊥
S and M = P ⊥
E BDB⊺P ⊥
E .
199"
LEARNABLE METRIPLECTIC OPERATORS,0.17783094098883573,"Remark 3.3. Observe that the projections appearing in these expressions are the minimum necessary
200"
LEARNABLE METRIPLECTIC OPERATORS,0.17862838915470494,"for guaranteeing the symmetries and degeneracy conditions necessary for metriplectic structure. In
201"
LEARNABLE METRIPLECTIC OPERATORS,0.17942583732057416,"particular, conjugation by P ⊥
f respects symmetry and ensures that both the input and output to the
202"
LEARNABLE METRIPLECTIC OPERATORS,0.18022328548644337,"conjugated matrix field lie in Span(∇f)⊥.
203"
LEARNABLE METRIPLECTIC OPERATORS,0.1810207336523126,"Lemma 3.2 demonstrates specific parameterizations for L, M that hold for any nondegenerate
204"
LEARNABLE METRIPLECTIC OPERATORS,0.18181818181818182,"metriplectic data and are core to the NMS method for learning metriplectic dynamics. While
205"
LEARNABLE METRIPLECTIC OPERATORS,0.18261562998405104,"generally underdetermined, these expressions are in a sense maximally specific given no additional
206"
LEARNABLE METRIPLECTIC OPERATORS,0.18341307814992025,"information, since there is nothing available in the general metriplectic formalism to determine the
207"
LEARNABLE METRIPLECTIC OPERATORS,0.18421052631578946,"matrix fields A, BDB⊺on Span(∇S), Span(∇E), respectively. The following Theorem, also
208"
LEARNABLE METRIPLECTIC OPERATORS,0.1850079744816587,"proven in Appendix A, provides a rigorous correspondence between metriplectic systems and these
209"
LEARNABLE METRIPLECTIC OPERATORS,0.18580542264752792,"particular parameterizations.
210"
LEARNABLE METRIPLECTIC OPERATORS,0.18660287081339713,"Theorem 3.4. The data L, M, E, S form a nondegenerate metriplectic system in the state variable
211"
LEARNABLE METRIPLECTIC OPERATORS,0.18740031897926634,"x ∈K if and only if there exist a skew-symmetric A : K →Skewn(R), symmetric postive
212"
LEARNABLE METRIPLECTIC OPERATORS,0.18819776714513556,"semidefinite D : K →Symr(R), and generic B : K →Rn×r such that
213"
LEARNABLE METRIPLECTIC OPERATORS,0.18899521531100477,"˙x = L∇E + M∇S = P ⊥
S AP ⊥
S ∇E + P ⊥
E BDB⊺P ⊥
E ∇S."
LEARNABLE METRIPLECTIC OPERATORS,0.189792663476874,"Remark 3.5. Note that the proposed parameterizations for L, M are not one-to-one but properly
214"
LEARNABLE METRIPLECTIC OPERATORS,0.19059011164274323,"contain the set of valid nondegenerate metriplectic systems in E, S, since the Jacobi identity on L
215"
LEARNABLE METRIPLECTIC OPERATORS,0.19138755980861244,"necessary for a true Poisson manifold structure is not strictly enforced. For 1 ≤i, j, k ≤n, the
216"
LEARNABLE METRIPLECTIC OPERATORS,0.19218500797448165,"Jacobi identity is given in components as P
ℓLiℓ∂ℓLjk + Ljℓ∂ℓLki + Lkℓ∂ℓLij = 0. However, this
217"
LEARNABLE METRIPLECTIC OPERATORS,0.19298245614035087,"requirement is not often enforced in algorithms for learning general metriplectic (or even symplectic)
218"
LEARNABLE METRIPLECTIC OPERATORS,0.1937799043062201,"systems, since it is considered subordinate to energy conservation and it is well known that both
219"
LEARNABLE METRIPLECTIC OPERATORS,0.19457735247208932,"qualities cannot hold simultaneously in general [23].
220"
SPECIFIC PARAMETERIZATIONS,0.19537480063795853,"3.3
Specific parameterizations
221"
SPECIFIC PARAMETERIZATIONS,0.19617224880382775,"Now that Theorem 3.4 has provided a model class which is rich enough to express any desired
222"
SPECIFIC PARAMETERIZATIONS,0.19696969696969696,"metriplectic system, it remains to discuss what NMS actually learns. First, note that it is unlikely to
223"
SPECIFIC PARAMETERIZATIONS,0.19776714513556617,"be the case that E, S are known a priori, so it is beneficial to allow these functions to be learnable
224"
SPECIFIC PARAMETERIZATIONS,0.19856459330143542,"alongside the governing operators L, M. For simplicity, energy and entropy E, S are parameterized
225"
SPECIFIC PARAMETERIZATIONS,0.19936204146730463,"as scalar-valued MLPs with tanh activation, although any desired architecture could be chosen for
226"
SPECIFIC PARAMETERIZATIONS,0.20015948963317384,"this task. The skew-symmetric matrix field A = Atri −A⊺
tri used to build L is parameterized
227"
SPECIFIC PARAMETERIZATIONS,0.20095693779904306,"through its strictly lower-triangular part Atri using a vector-valued MLP with output dimension
 n
2

,
228"
SPECIFIC PARAMETERIZATIONS,0.20175438596491227,"which guarantees that the mapping Atri 7→A above is bijective. Similarly, the symmetric matrix
229"
SPECIFIC PARAMETERIZATIONS,0.2025518341307815,"field D = KcholK⊺
chol is parameterized through its lower-triangular Cholesky factor Kchol, which
230"
SPECIFIC PARAMETERIZATIONS,0.20334928229665072,"is a vector-valued MLP with output dimension
 r+1
2

. While this choice does not yield a bijective
231"
SPECIFIC PARAMETERIZATIONS,0.20414673046251994,"mapping Kchol 7→D unless, e.g., D is assumed to be positive definite with diagonal entries of fixed
232"
SPECIFIC PARAMETERIZATIONS,0.20494417862838915,"sign, this does not hinder the method in practice. In fact, D should not be positive definite in general,
233"
SPECIFIC PARAMETERIZATIONS,0.20574162679425836,"as this is equivalent to claiming that M is positive definite on vectors tangent to the level sets of E.
234"
SPECIFIC PARAMETERIZATIONS,0.2065390749601276,"Finally, the generic matrix field B is parameterized as a vector-valued MLP with output dimension
235"
SPECIFIC PARAMETERIZATIONS,0.20733652312599682,"nr. Remarkably, the exterior algebraic expressions in Lemma 3.2 require less redundant operations
236"
SPECIFIC PARAMETERIZATIONS,0.20813397129186603,"than the corresponding matricized expressions from Theorem 3.4, and therefore the expressions from
237"
SPECIFIC PARAMETERIZATIONS,0.20893141945773525,"Lemma 3.2 are used when implementing NMS. Figure 1 summarizes this information.
238"
SPECIFIC PARAMETERIZATIONS,0.20972886762360446,"Remark 3.6. With these choices, the NMS parameterization of metriplectic systems requires only
239"
SPECIFIC PARAMETERIZATIONS,0.21052631578947367,"(1/2)
 
(n + r)2 −(n −r)

+ 2 learnable scalar functions, in contrast to
 n
3

+ r
 n
2

+
 r+1
2

+ 2 for
240"
SPECIFIC PARAMETERIZATIONS,0.2113237639553429,"the GNODE approach in [16] and rn(n −1) + r2 + 2 for the GFINN approach in [18]. In particular,
241"
SPECIFIC PARAMETERIZATIONS,0.21212121212121213,"NMS is quadratic in both n, r with no decrease in model expressivity, in contrast to the cubic scaling
242"
SPECIFIC PARAMETERIZATIONS,0.21291866028708134,"of previous methods.
243"
SPECIFIC PARAMETERIZATIONS,0.21371610845295055,"Table 1: Properties of the metriplectic
architectures compared."
SPECIFIC PARAMETERIZATIONS,0.21451355661881977,"Name
Physics Bias Restrictive
Scale"
SPECIFIC PARAMETERIZATIONS,0.215311004784689,"NODE
None
No
Linear
SPNN
Soft
No
Quadratic
GNODE
Hard
Yes
Cubic
GFINN
Hard
No
Cubic
NMS
Hard
No
Quadratic  "
SPECIFIC PARAMETERIZATIONS,0.21610845295055822,"Input/Output
Learnable
Calculated"
SPECIFIC PARAMETERIZATIONS,0.21690590111642744,Integrate
SPECIFIC PARAMETERIZATIONS,0.21770334928229665,Figure 1: A visual depiction of the NMS architecture. 244
APPROXIMATION AND ERROR,0.21850079744816586,"3.4
Approximation and error
245"
APPROXIMATION AND ERROR,0.21929824561403508,"Besides offering a compact parameterization of metriplectic dynamics, the expressions used in NMS
246"
APPROXIMATION AND ERROR,0.22009569377990432,"also exhibit desirable approximation properties which guarantee a reasonable bound on state error
247"
APPROXIMATION AND ERROR,0.22089314194577353,"over time. To state this precisely, first note the following universal approximation result proven in
248"
APPROXIMATION AND ERROR,0.22169059011164274,"Appendix A.
249"
APPROXIMATION AND ERROR,0.22248803827751196,"Proposition 3.7. Let K ⊂Rn be compact and E, S : K →R be continuous such that L∇S =
250"
APPROXIMATION AND ERROR,0.22328548644338117,"M∇E = 0 and ∇E, ∇S ̸= 0 for all x ∈K. For any ε > 0, there exist two-layer neural network
251"
APPROXIMATION AND ERROR,0.2240829346092504,"functions ˜E, ˜S : K →R, ˜L : K →Skewn(R) and ˜
M : K →Symn(R) such that ∇˜E, ∇˜S ̸= 0 on
252"
APPROXIMATION AND ERROR,0.22488038277511962,"K, ˜
M is positive semi-definite, ˜L∇˜S = ˜
M∇˜E = 0 for all x ∈K, and each approximate function
253"
APPROXIMATION AND ERROR,0.22567783094098884,"is ε-close to its given counterpart on K. Moreover, if L, M have k ≥0 continuous derivatives on K
254"
APPROXIMATION AND ERROR,0.22647527910685805,"then so do ˜L, ˜
M.
255"
APPROXIMATION AND ERROR,0.22727272727272727,"Remark 3.8. The assumption x ∈K of the state remaining in a compact set V is not restrictive when
256"
APPROXIMATION AND ERROR,0.22807017543859648,"at least one of E, −S : Rn →R, say E, has bounded sublevel sets. In this case, letting x0 = x(0) it
257"
APPROXIMATION AND ERROR,0.22886762360446572,"follows from ˙E ≤0 that E(x(t)) = E(x0) +
R t
0 ˙E(x(τ)) dτ ≤E(x0), so that the entire trajectory
258"
APPROXIMATION AND ERROR,0.22966507177033493,"x(t) lies in the (closed and bounded) compact set K = {x | E(x) ≤E(x0)} ⊂Rn.
259"
APPROXIMATION AND ERROR,0.23046251993620415,"Leaning on Proposition 3.7 and classical universal approximation results in [24], it is further possible
260"
APPROXIMATION AND ERROR,0.23125996810207336,"to establish the following error estimate also proven in Appendix A which gives an idea of the error
261"
APPROXIMATION AND ERROR,0.23205741626794257,"accumulation rate that can be expected from this method.
262"
APPROXIMATION AND ERROR,0.23285486443381181,"Theorem 3.9. Suppose L, M, E, S are nondegenerate metriplectic data such that L, M have at
263"
APPROXIMATION AND ERROR,0.23365231259968103,"least one continuous derivative, E, S have Lipschitz continuous gradients, and at least one of E, −S
264"
APPROXIMATION AND ERROR,0.23444976076555024,"have bounded sublevel sets. For any ε > 0, there exist nondegenerate metriplectic data ˜L, ˜
M, ˜E, ˜S
265"
APPROXIMATION AND ERROR,0.23524720893141945,"defined by two-layer neural networks such that, for all T > 0,
266 Z T"
APPROXIMATION AND ERROR,0.23604465709728867,"0
|x −˜x|2 dt ! 1"
APPROXIMATION AND ERROR,0.23684210526315788,"2
≤ε

b
a p"
APPROXIMATION AND ERROR,0.23763955342902712,"e2aT −2eaT + T + 1,"
APPROXIMATION AND ERROR,0.23843700159489634,"where a, b ∈R are constants depending on both sets of metriplectic data and ˙˜x = ˜L(˜x)∇˜E(˜x) +
267"
APPROXIMATION AND ERROR,0.23923444976076555,"˜
M(˜x)∇˜S(˜x).
268"
APPROXIMATION AND ERROR,0.24003189792663476,"Remark 3.10. Theorem 3.9 provides a bound on state error over time under the assumption that the
269"
APPROXIMATION AND ERROR,0.24082934609250398,"approximation error in the metriplectic networks can be controlled. On the other hand, notice that
270"
APPROXIMATION AND ERROR,0.24162679425837322,"Theorem 3.9 can also be understood as a generic error bound on any trained metriplectic networks
271"
APPROXIMATION AND ERROR,0.24242424242424243,"˜L, ˜
M, ˜E, ˜S provided universal approximation results are not invoked in the estimation leading to εb.
272"
APPROXIMATION AND ERROR,0.24322169059011164,"This result confirms that the error in the state x for a fixed final time T tends to zero with the
273"
APPROXIMATION AND ERROR,0.24401913875598086,"approximation error in the networks ˜L, ˜
M, ˜E, ˜S, as one would hope based on the approximation
274"
APPROXIMATION AND ERROR,0.24481658692185007,"capabilities of neural networks. More importantly, Theorem 3.9 also bounds the generalization error
275"
APPROXIMATION AND ERROR,0.24561403508771928,"of any trained metriplectic network for an appropriate (and possibly large) ε equal to the maximum
276"
APPROXIMATION AND ERROR,0.24641148325358853,"approximation error on K, where the learned metriplectic trajectories are confined for all time.
277"
APPROXIMATION AND ERROR,0.24720893141945774,"With this theoretical guidance, the remaining goal of this work is to demonstrate that NMS is also
278"
APPROXIMATION AND ERROR,0.24800637958532695,"practically effective at learning metriplectic systems from data and exhibits reasonable generalization
279"
APPROXIMATION AND ERROR,0.24880382775119617,"to unseen timescales.
280"
ALGORITHM,0.24960127591706538,"4
Algorithm
281"
ALGORITHM,0.2503987240829346,"Similar to previous approaches in [16] and [18], the learnable parameters in NMS are calibrated
282"
ALGORITHM,0.2511961722488038,"using data along solution trajectories to a given dynamical system. This brings up an important
283"
ALGORITHM,0.25199362041467305,"question regarding how much information about the system in question is realistically present in
284"
ALGORITHM,0.2527910685805423,"the training data. While many systems have a known metriplectic form, it is not always the case
285"
ALGORITHM,0.2535885167464115,"that one will know metriplectic governing equations for a given set of training data. Therefore, two
286"
ALGORITHM,0.2543859649122807,"approaches are considered in the experiments below corresponding to whether full or partial state
287"
ALGORITHM,0.2551834130781499,"information is assumed available during NMS training. More precisely, the state x = (xo, xu) will
288"
ALGORITHM,0.25598086124401914,"be partitioned into “observable” and “unobservable” variables, where xu may be empty in the case
289"
ALGORITHM,0.2567783094098884,"that full state information is available. In a partially observable system xo typically contains positions
290"
ALGORITHM,0.25757575757575757,"and momenta while xu contains entropy or other configuration variables which are more difficult
291"
ALGORITHM,0.2583732057416268,"to observe during physical experiments. In both cases, NMS will learn a metriplectic system in x
292"
ALGORITHM,0.259170653907496,"according to the procedure described in Algorithm 1.
293"
ALGORITHM,0.25996810207336524,Algorithm 1 Training neural metriplectic systems
ALGORITHM,0.2607655502392344,"1: Input: snapshot data X ∈Rn×ns, each column xs = x(ts, µs), target rank r ≥1, batch size
nb ≥1.
2: Initialize networks Atri, B, Kchol, E, S, and loss L = 0
3: for step in Nsteps do
4:
Randomly draw batch P = {(tsk, xsk)}nb
k=1
5:
for (t, x) in P do
6:
Evaluate Atri(x), B(x), Kchol(x), E(x), S(x)
7:
Automatically differentiate E, S to obtain ∇E(x), ∇S(x)
8:
Form A(x) = Atri(x) −Atri(x)⊺and D(x) = Kchol(x)Kchol(x)⊺"
ALGORITHM,0.26156299840510366,"9:
Build L(x), M(x) according to Lemma 3.2
10:
Evaluate ˙x = L(x)∇E(x) + M(x)∇S(x)
11:
Randomly draw n1, ..., nl and form tj = t + nj∆t for all j
12:
˜x1, ..., ˜xl = ODEsolve( ˙x, t1, ..., tl)
13:
L += l−1 P"
ALGORITHM,0.2623604465709729,"j Loss(xj, ˜xj)
14:
end for
15:
Rescale L = |P|−1L
16:
Update Atri, B, Kchol, E, S through gradient descent on L.
17: end for"
ALGORITHM,0.2631578947368421,"Note that the batch-wise training strategy in Algorithm 1 requires initial conditions for xu in the
294"
ALGORITHM,0.26395534290271133,"partially observed case. There are several options for this, and two specific strategies will be
295"
ALGORITHM,0.2647527910685805,"considered here. Suppose the data are drawn from the training interval [0, T] with initial state x0
296"
ALGORITHM,0.26555023923444976,"and final state xT . The first strategy sets xu
0 = 0, xu
T = 1 (where 1 is the all ones vector), and
297"
ALGORITHM,0.266347687400319,"xu
s = 1/T, 0 < s < T, so that the unobserved states are initially assumed to lie on a straight line.
298"
ALGORITHM,0.2671451355661882,"The second strategy is more sophisticated, and involves training a diffusion model to predict the
299"
ALGORITHM,0.2679425837320574,"distribution of xu given xo. Specific details of this procedure are given in Appendix E.
300"
EXAMPLES,0.2687400318979266,"5
Examples
301"
EXAMPLES,0.26953748006379585,"The goal of the following experiments is to show that NMS is effective even when entropic information
302"
EXAMPLES,0.2703349282296651,"cannot be observed during training, yielding superior performance when compared to previous
303"
EXAMPLES,0.2711323763955343,"methods including GNODE, GFINN, and SPNN discussed in Section 2. The metrics considered
304"
EXAMPLES,0.2719298245614035,"for this purpose will be mean absolute error (MAE) and mean squared error (MSE) defined in the
305"
EXAMPLES,0.2727272727272727,"usual way as the average ℓ1 (resp. squared ℓ2) error between the discrete states x, ˜x ∈Rn×ns. For
306"
EXAMPLES,0.27352472089314195,"brevity, many implementation details have been omitted here and can be found in Appendix B. An
307"
EXAMPLES,0.2743221690590112,"additional experiment showing the effectiveness of NMS in the presence of both full and partial state
308"
EXAMPLES,0.2751196172248804,"information can be found in Appendix C.
309"
EXAMPLES,0.2759170653907496,"Remark 5.1. To facilitate a more equal parameter count between the compared metriplectic meth-
310"
EXAMPLES,0.2767145135566188,"ods, the results of the experiments below were generated using the alternative parameterization
311"
EXAMPLES,0.27751196172248804,"D = KK⊺where K : K →Rr×r′ is full and r′ ≥r. Of course, this change does not affect
312"
EXAMPLES,0.2783094098883573,"metriplecticity since D is still positive semi-definite for each x ∈K.
313"
TWO GAS CONTAINERS,0.27910685805422647,"5.1
Two gas containers
314"
TWO GAS CONTAINERS,0.2799043062200957,"The first test of NMS involves two ideal gas containers separated by movable wall which is removed
315"
TWO GAS CONTAINERS,0.2807017543859649,"at time t0, allowing for the exchange of heat and volume. In this example, the motion of the state
316"
TWO GAS CONTAINERS,0.28149920255183414,"x = (q
p
S1
S2)⊺is governed by the metriplectic equations:
317"
TWO GAS CONTAINERS,0.2822966507177033,˙q = p
TWO GAS CONTAINERS,0.28309409888357256,"m,
˙p = 2 3"
TWO GAS CONTAINERS,0.2838915470494418,E1(x)
TWO GAS CONTAINERS,0.284688995215311,"q
−E2(x) 2L −q 
,"
TWO GAS CONTAINERS,0.28548644338118023,"˙S1 = 9N 2k2
Bα
4E1(x)"
TWO GAS CONTAINERS,0.2862838915470494,"
1
E1(x) −
1
E2(x)"
TWO GAS CONTAINERS,0.28708133971291866,"
,
˙S2 = −9N 2k2
Bα
4E1(x)"
TWO GAS CONTAINERS,0.2878787878787879,"
1
E1(x) −
1
E2(x) 
,"
TWO GAS CONTAINERS,0.2886762360446571,"where (q, p) are the position and momentum of the separating wall, S1, S2 are the entropies of the
318"
TWO GAS CONTAINERS,0.2894736842105263,"two subsystems, and the internal energies E1, E2 are determined from the Sackur-Tetrode equation
319"
TWO GAS CONTAINERS,0.2902711323763955,"for ideal gases, Si/NkB = ln

ˆcViE3/2
i

, 1 ≤i ≤2. Here, m denotes the mass of the wall, 2L is
320"
TWO GAS CONTAINERS,0.29106858054226475,"the total length of the system, and Vi is the volume of the ith container. As in [16, 25] NkB = 1 and
321"
TWO GAS CONTAINERS,0.291866028708134,"α = 0.5 fix the characteristic macroscopic unit of entropy while ˆc = 102.25 ensures the argument of
322"
TWO GAS CONTAINERS,0.2926634768740032,"the logarithm defining Ei is dimensionless. This leads to the total entropy S(x) = S1 + S2 and the
323"
TWO GAS CONTAINERS,0.2934609250398724,"total energy E(x) = (1/2m)p2 + E1(x) + E2(x), which are guaranteed to be nondecreasing and
324"
TWO GAS CONTAINERS,0.2942583732057416,"constant, respectively.
325"
TWO GAS CONTAINERS,0.29505582137161085,"The primary goal here is to verify that NMS can accurately and stably predict gas container dynamics
326"
TWO GAS CONTAINERS,0.2958532695374801,"without the need to observe the entropic variables S1, S2. To that end, NMS has been compared to
327"
TWO GAS CONTAINERS,0.2966507177033493,"GNODE, SPNN, and GFINN on the task of predicting the trajectories of this metriplectic system
328"
TWO GAS CONTAINERS,0.2974481658692185,"over time, with results displayed in Table 2. More precisely, given an intial condition x0 and an
329"
TWO GAS CONTAINERS,0.2982456140350877,"interval 0 < ttrain < tvalid < ttest, each method is trained on partial state information (in the case of
330"
TWO GAS CONTAINERS,0.29904306220095694,"NMS) or full state information (in the case of the others) from the interval [0, ttrain] and validated on
331"
TWO GAS CONTAINERS,0.29984051036682613,"(ttrain, tvalid] before state errors in q, p only are calculated on the whole interval [0, ttest]. As can be
332"
TWO GAS CONTAINERS,0.30063795853269537,"seen from Table 2 and Figure 2, NMS is remarkably accurate over unseen timescales even in this
333"
TWO GAS CONTAINERS,0.3014354066985646,"unfair comparison, avoiding the unphysical behavior which often hinders soft-constrained methods
334"
TWO GAS CONTAINERS,0.3022328548644338,"like SPNN. The energy and instantaneous entropy plots in Figure 2 further confirm that the strong
335"
TWO GAS CONTAINERS,0.30303030303030304,"enforcement of metriplectic structure guaranteed by NMS leads to correct energetic and entropic
336"
TWO GAS CONTAINERS,0.3038277511961722,"dynamics for all time.
337"
THERMOELASTIC DOUBLE PENDULUM,0.30462519936204147,"5.2
Thermoelastic double pendulum
338"
THERMOELASTIC DOUBLE PENDULUM,0.3054226475279107,"Next, consider the thermoelastic double pendulum from [26] with 10-dimensional state variable x =
339"
THERMOELASTIC DOUBLE PENDULUM,0.3062200956937799,"(q1
q2
p1
p2
S1
S2)⊺, which represents a highly challenging benchmark for metriplectic
340"
THERMOELASTIC DOUBLE PENDULUM,0.30701754385964913,"methods. The equations of motion in this case are given for 1 ≤i ≤2 as
341"
THERMOELASTIC DOUBLE PENDULUM,0.3078149920255183,˙qi = pi
THERMOELASTIC DOUBLE PENDULUM,0.30861244019138756,"mi
,
˙pi = −∂qi(E1(x) + E2(x)),
˙S1 = κ
 
T −1
1
T2 −1

,
˙S2 = κ
 
T1T −1
2
−1

,"
THERMOELASTIC DOUBLE PENDULUM,0.3094098883572568,"where κ > 0 is a thermal conductivity constant (set to 1), mi is the mass of the ith spring (also set to
342"
THERMOELASTIC DOUBLE PENDULUM,0.310207336523126,"1) and Ti = ∂SiEi is its absolute temperature. In this case, qi, pi ∈R2 represent the position and
343"
THERMOELASTIC DOUBLE PENDULUM,0.31100478468899523,"(a) Hamiltonian H =
p2"
M,0.3118022328548644,"2m
(b) Position q"
M,0.31259968102073366,"(c) Momentum p
(d) Energy E = H + P"
M,0.3133971291866029,"i Ei
(e) Instantaneous Entropy ˙S"
M,0.3141945773524721,"Figure 2: The ground-truth and predicted position, momentum, instantaneous entropy, and energies
for the two gas containers example in the training (white), validation (yellow), and testing (red)
regimes."
M,0.3149920255183413,"Table 2: Prediction errors for xo measured in MSE and MAE on the interval [0, ttest] in the two gas
containers example (left) and on the test set in the thermoelastic double pendulum example (right)."
M,0.3157894736842105,"NODE
SPNN
GNODE
GFINN
NMS"
M,0.31658692185007975,"MSE
.12 ± .04
.13 ± .10
.16 ± .10
.07 ± .03
.01 ± .02
MAE
.25 ± .10
.26 ± .14
.25 ± .13
.13 ±.03
.08 ± .06"
M,0.31738437001594894,"NODE
SPNN
GNODE
GFINN
NMS"
M,0.3181818181818182,"MSE
.41 ± .01
.42 ± .01
.42 ± .01
.40 ± .03
.38 ± .03
MAE
.48 ± .04
.47 ± .03
.46 ± .04
.43 ± .07
.42 ± .07"
M,0.3189792663476874,"momentum of the ith mass, while Si represents the entropy of the ith pendulum. As before, the total
344"
M,0.3197767145135566,"entropy S(x) = S1 + S2 is the sum of the entropies of the two springs, while defining the internal
345"
M,0.32057416267942584,"energies Ei(x) = (1/2)(ln λi)2 + ln λi + eSi−ln λi −1, λ1 = |qi|, λ2 = |q2 −q1|, leads to the total
346"
M,0.32137161084529503,"energy E(x) = (1/2m1)|p1|2 + (1/2m2)|p2|2 + E1(x) + E2(x).
347"
M,0.32216905901116427,"The task in this case is prediction across initial conditions. As in [18], 100 trajectories are drawn from
348"
M,0.3229665071770335,"the ranges in Appendix B and integrated over the interval [0, 40] with ∆t = 0.1, with an 80/10/10
349"
M,0.3237639553429027,"split for training/validation/testing. Here all compared models are trained using full state information.
350"
M,0.32456140350877194,"As seen in Table 2, NMS is again the most performant, although all models struggle to approximate
351"
M,0.3253588516746411,"the dynamics over the entire training interval. It is also notable that the training time of NMS is greatly
352"
M,0.32615629984051037,"decreased relative to GNODE and GFINN due to its improved quadratic scaling; a representative
353"
M,0.3269537480063796,"study to this effect is given in Appendix D.
354"
CONCLUSION,0.3277511961722488,"6
Conclusion
355"
CONCLUSION,0.32854864433811803,"Neural metriplectic systems (NMS) have been considered for learning finite-dimensional metriplectic
356"
CONCLUSION,0.3293460925039872,"dynamics from data. Making use of novel non-redundant parameterizations for metriplectic operators,
357"
CONCLUSION,0.33014354066985646,"NMS provably approximates arbitrary nondegenerate metriplectic systems with generalization error
358"
CONCLUSION,0.3309409888357257,"bounded in terms of the operator approximation quality. Benchmark examples have shown that
359"
CONCLUSION,0.3317384370015949,"NMS is both more scalable and more accurate than previous methods, including when only partial
360"
CONCLUSION,0.33253588516746413,"state information is observed. Future work will consider extensions of NMS to infinite-dimensional
361"
CONCLUSION,0.3333333333333333,"metriplectic systems with the aim of addressing its main limitation: the difficulty of scaling NMS
362"
CONCLUSION,0.33413078149920256,"(among all present methods for metriplectic learning) to realistic, 3-D problems of the size that would
363"
CONCLUSION,0.3349282296650718,"be considered in practice. A promising direction is to consider the use of NMS in model reduction,
364"
CONCLUSION,0.335725677830941,"where sparse, large-scale systems are converted to small, dense systems through a clever choice of
365"
CONCLUSION,0.3365231259968102,"encoding/decoding.
366"
REFERENCES,0.3373205741626794,"References
367"
REFERENCES,0.33811802232854865,"[1] Philip J. Morrison. A paradigm for joined hamiltonian and dissipative systems. Physica D: Nonlinear
368"
REFERENCES,0.33891547049441784,"Phenomena, 18(1):410–419, 1986.
369"
REFERENCES,0.3397129186602871,"[2] Miroslav Grmela and Hans Christian Öttinger. Dynamics and thermodynamics of complex fluids. i.
370"
REFERENCES,0.3405103668261563,"development of a general formalism. Phys. Rev. E, 56:6620–6632, Dec 1997.
371"
REFERENCES,0.3413078149920255,"[3] P. J. Morrison. Some observations regarding brackets and dissipation. Technical Report PAM-228, Center
372"
REFERENCES,0.34210526315789475,"for Pure and Applied Mathematics, University of California, Berkeley, 1984.
373"
REFERENCES,0.34290271132376393,"[4] PJ Morrison. Thoughts on brackets and dissipation: old and new. In Journal of Physics: Conference Series,
374"
REFERENCES,0.3437001594896332,"volume 169, page 012006. IOP Publishing, 2009.
375"
REFERENCES,0.3444976076555024,"[5] Allan N. Kaufman and Philip J. Morrison. Algebraic structure of the plasma quasilinear equations. Physics
376"
REFERENCES,0.3452950558213716,"Letters A, 88(8):405–406, 1982.
377"
REFERENCES,0.34609250398724084,"[6] Emmanuele Materassi, M.; Tassi. Metriplectic framework for dissipative magneto-hydrodynamics. Physica
378"
REFERENCES,0.34688995215311,"D: Nonlinear Phenomena, 2012.
379"
REFERENCES,0.34768740031897927,"[7] Allan N. Kaufman. Dissipative hamiltonian systems: A unifying principle. Physics Letters A, 100(8):419–
380"
REFERENCES,0.3484848484848485,"422, 1984.
381"
REFERENCES,0.3492822966507177,"[8] Darryl D Holm, Vakhtang Putkaradze, and Cesare Tronci. Kinetic models of oriented self-assembly.
382"
REFERENCES,0.35007974481658694,"Journal of Physics A: Mathematical and Theoretical, 41(34):344010, aug 2008.
383"
REFERENCES,0.3508771929824561,"[9] Quercus Hernández, Alberto Badías, David González, Francisco Chinesta, and Elías Cueto. Structure-
384"
REFERENCES,0.35167464114832536,"preserving neural networks. Journal of Computational Physics, 426:109950, 2021.
385"
REFERENCES,0.3524720893141946,"[10] Quercus Hernández, Alberto Badías, David González, Francisco Chinesta, and Elías Cueto. Deep learning
386"
REFERENCES,0.3532695374800638,"of thermodynamics-aware reduced-order models from data. Computer Methods in Applied Mechanics and
387"
REFERENCES,0.35406698564593303,"Engineering, 379:113763, 2021.
388"
REFERENCES,0.3548644338118022,"[11] David González, Francisco Chinesta, and Elías Cueto. Thermodynamically consistent data-driven compu-
389"
REFERENCES,0.35566188197767146,"tational mechanics. Continuum Mechanics and Thermodynamics, 31(1):239–253, 2019.
390"
REFERENCES,0.35645933014354064,"[12] D. Ruiz, D. Portillo, and I. Romero. A data-driven method for dissipative thermomechanics. IFAC-
391"
REFERENCES,0.3572567783094099,"PapersOnLine, 54(19):315–320, 2021.
392"
REFERENCES,0.3580542264752791,"[13] Baige Xu, Yuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Learning generic systems using neural
393"
REFERENCES,0.3588516746411483,"symplectic forms. In International Symposium on Nonlinear Theory and Its Applications, number A2L-D-
394"
REFERENCES,0.35964912280701755,"03 in IEICE Proceeding Series, pages 29–32. The Institute of Electronics, Information, and Communication
395"
REFERENCES,0.36044657097288674,"Engineers (IEICE), 2022.
396"
REFERENCES,0.361244019138756,"[14] Baige Xu, Yuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Equivalence class learning for
397"
REFERENCES,0.3620414673046252,"GENERIC systems. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems,
398"
REFERENCES,0.3628389154704944,"2023.
399"
REFERENCES,0.36363636363636365,"[15] Anthony Gruber, Kookjin Lee, and Nathaniel Trask. Reversible and irreversible bracket-based dynamics
400"
REFERENCES,0.36443381180223283,"for deep graph neural networks. In Thirty-seventh Conference on Neural Information Processing Systems,
401"
REFERENCES,0.3652312599681021,"2023.
402"
REFERENCES,0.3660287081339713,"[16] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Machine learning structure preserving brackets for
403"
REFERENCES,0.3668261562998405,"forecasting irreversible processes. Advances in Neural Information Processing Systems, 34:5696–5707,
404"
REFERENCES,0.36762360446570974,"2021.
405"
REFERENCES,0.3684210526315789,"[17] Hans Christian Öttinger. Irreversible dynamics, onsager-casimir symmetry, and an application to turbulence.
406"
REFERENCES,0.36921850079744817,"Phys. Rev. E, 90:042121, Oct 2014.
407"
REFERENCES,0.3700159489633174,"[18] Zhen Zhang, Yeonjong Shin, and George Em Karniadakis. Gfinns: Generic formalism informed neural
408"
REFERENCES,0.3708133971291866,"networks for deterministic and stochastic dynamical systems. Philosophical Transactions of the Royal
409"
REFERENCES,0.37161084529505584,"Society A: Mathematical, Physical and Engineering Sciences, 380(2229):20210207, 2022.
410"
REFERENCES,0.372408293460925,"[19] Hans Christian Öttinger. Preservation of thermodynamic structure in model reduction. Phys. Rev. E,
411"
REFERENCES,0.37320574162679426,"91:032147, Mar 2015.
412"
REFERENCES,0.3740031897926635,"[20] Anthony Gruber, Max Gunzburger, Lili Ju, and Zhu Wang. Energetically consistent model reduction for
413"
REFERENCES,0.3748006379585327,"metriplectic systems. Computer Methods in Applied Mechanics and Engineering, 404:115709, 2023.
414"
REFERENCES,0.37559808612440193,"[21] Loring W. Tu. Differential Geometry: Connections, Curvature, and Characteristic Classes. Springer
415"
REFERENCES,0.3763955342902711,"International Publishing, 2017.
416"
REFERENCES,0.37719298245614036,"[22] Leo Dorst, Daniel Fontijne, and Stephen Mann. Geometric Algebra for Computer Science: An Object-
417"
REFERENCES,0.37799043062200954,"oriented Approach to Geometry. Morgan Kaufmann, Amsterdam, 2007.
418"
REFERENCES,0.3787878787878788,"[23] Ge Zhong and Jerrold E. Marsden. Lie-poisson hamilton-jacobi theory and lie-poisson integrators. Physics
419"
REFERENCES,0.379585326953748,"Letters A, 133(3):134–139, 1988.
420"
REFERENCES,0.3803827751196172,"[24] Xin Li. Simultaneous approximations of multivariate functions and their derivatives by neural networks
421"
REFERENCES,0.38118022328548645,"with one hidden layer. Neurocomputing, 12(4):327–343, 1996.
422"
REFERENCES,0.38197767145135564,"[25] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Structure-preserving sparse identification of nonlinear
423"
REFERENCES,0.3827751196172249,"dynamics for data-driven modeling. In Mathematical and Scientific Machine Learning, pages 65–80.
424"
REFERENCES,0.3835725677830941,"PMLR, 2022.
425"
REFERENCES,0.3843700159489633,"[26] Ignacio Romero. Thermodynamically consistent time-stepping algorithms for non-linear thermomechanical
426"
REFERENCES,0.38516746411483255,"systems. International Journal for Numerical Methods in Engineering, 79(6):706–732, 2023/05/14 2009.
427"
REFERENCES,0.38596491228070173,"[27] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational
428"
REFERENCES,0.386762360446571,"and applied mathematics, 6(1):19–26, 1980.
429"
REFERENCES,0.3875598086124402,"[28] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
430"
REFERENCES,0.3883572567783094,"arXiv:1412.6980, 2014.
431"
REFERENCES,0.38915470494417864,"[29] Xiaocheng Shang and Hans Christian Öttinger. Structure-preserving integrators for dissipative systems
432"
REFERENCES,0.38995215311004783,"based on reversible–irreversible splitting. Proceedings of the Royal Society A, 476(2234):20190446, 2020.
433"
REFERENCES,0.39074960127591707,"[30] Haksoo Lim, Minjung Kim, Sewon Park, and Noseong Park. Regular time-series generation using sgm.
434"
REFERENCES,0.3915470494417863,"arXiv preprint arXiv:2301.08518, 2023.
435"
REFERENCES,0.3923444976076555,"[31] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation,
436"
REFERENCES,0.39314194577352474,"23(7):1661–1674, 2011.
437"
REFERENCES,0.3939393939393939,"[32] Simo Särkkä and Arno Solin, editors. Applied stochastic differential equations, volume 10. Cambridge
438"
REFERENCES,0.39473684210526316,"University Press, 2019.
439"
REFERENCES,0.39553429027113235,"[33] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
440"
REFERENCES,0.3963317384370016,"Score-based generative modeling through stochastic differential equations. CoRR, abs/2011.13456, 2020.
441"
REFERENCES,0.39712918660287083,"A
Proof of Theoretical Results
442"
REFERENCES,0.39792663476874,"This Appendix provides proof of the analytical results in Section 3 of the body. First, the parameteri-
443"
REFERENCES,0.39872408293460926,"zations of L, M in terms of exterior algebra are established.
444"
REFERENCES,0.39952153110047844,"Proof of Lemma 3.2. First, it is necessary to check that the operators L, M parameterized this way
445"
REFERENCES,0.4003189792663477,"satisfy the symmetries and degeneracy conditions claimed in the statement. To that end, recall that
446"
REFERENCES,0.4011164274322169,"a ∧b ≃ab⊺−ba⊺, meaning that (ab⊺−ba⊺)⊺≃b ∧a = −a ∧b. It follows that A⊺≃˜A = −A
447"
REFERENCES,0.4019138755980861,"where ˜A denotes the reversion of A, i.e., ˜A = P"
REFERENCES,0.40271132376395535,"i<j Aijej ∧ei. Therefore, we may write
448"
REFERENCES,0.40350877192982454,"L⊺≃˜A −
1"
REFERENCES,0.4043062200956938,"|∇S|2 A∇S ∧∇S
: = −A +
1"
REFERENCES,0.405103668261563,"|∇S|2 A∇S ∧∇S ≃−L,"
REFERENCES,0.4059011164274322,"showing that L⊺= −L. Moreover, using that
449"
REFERENCES,0.40669856459330145,"(b ∧c) · a = −a · (b ∧c) = (a · c)b −(a · b)c,
it follows that
450"
REFERENCES,0.40749601275917063,"L∇S = A · ∇S −
1"
REFERENCES,0.4082934609250399,"|∇S|2 (A∇S ∧∇S) · ∇S = A∇S −A∇S = 0,"
REFERENCES,0.4090909090909091,"since ∇S · A∇S = −∇S · A∇S = 0. Moving to the case of M, notice that M = Dstvs ⊗vt for
451"
REFERENCES,0.4098883572567783,"a particular choice of v, meaning that
452"
REFERENCES,0.41068580542264754,"M ⊺=
X"
REFERENCES,0.41148325358851673,"s,t
Dst
 
vs ⊗vt⊺=
X"
REFERENCES,0.41228070175438597,"s,t
Dstvt ⊗vs =
X"
REFERENCES,0.4130781499202552,"t,s
Dtsvs ⊗vt =
X"
REFERENCES,0.4138755980861244,"s,t
Dstvs ⊗vt = M,"
REFERENCES,0.41467304625199364,"since D is a symmetric matrix. Additionally, it is straightforward to check that, for any 1 ≤s ≤r,
453"
REFERENCES,0.4154704944178628,vs · ∇E = 
REFERENCES,0.41626794258373206,bs −bs · ∇E
REFERENCES,0.41706539074960125,|∇E|2 ∇E !
REFERENCES,0.4178628389154705,· ∇E = bs · ∇E −bs · ∇E = 0.
REFERENCES,0.41866028708133973,"So, it follows immediately that
454"
REFERENCES,0.4194577352472089,"M∇E =
X"
REFERENCES,0.42025518341307816,"s,t
Dst
 
vs ⊗vt
· ∇E =
X"
REFERENCES,0.42105263157894735,"s,t
Dst
 
vt · ∇E

vs = 0."
REFERENCES,0.4218500797448166,"Now, observe that
455"
REFERENCES,0.4226475279106858,"L = A −
1"
REFERENCES,0.423444976076555,|∇S|2 (A∇S(∇S)⊺−∇S(A∇S)⊺)
REFERENCES,0.42424242424242425,"= A −
1"
REFERENCES,0.42503987240829344,|∇S|2 (A∇S(∇S⊺) + ∇S(∇S)⊺A) = 
REFERENCES,0.4258373205741627,I −∇S(∇S)⊺ |∇S|2 ! A 
REFERENCES,0.4266347687400319,I −∇S(∇S)⊺ |∇S|2 !
REFERENCES,0.4274322169059011,"= P ⊥
S AP ⊥
S ,"
REFERENCES,0.42822966507177035,"since A⊺= −A and hence v⊺Av = 0 for all v ∈Rn. Similarly, it follows that for every 1 ≤s ≤r,
456"
REFERENCES,0.42902711323763953,"P ⊥
E bs = bs −bs · ∇E"
REFERENCES,0.4298245614035088,"|∇E|2 ∇E,"
REFERENCES,0.430622009569378,"and therefore M is expressible as
457 M =
X"
REFERENCES,0.4314194577352472,"s,t
Dst
 
P ⊥
E bs 
P ⊥
E bt⊺= P ⊥
E BDB⊺P ⊥
E ."
REFERENCES,0.43221690590111644,"With Lemma 3.2 established, the proof of Theorem 3.4 is straightforward.
458"
REFERENCES,0.43301435406698563,"Proof of Theorem 3.4. The “if” direction follows immediately from Lemma 3.2. Now, suppose that
459"
REFERENCES,0.43381180223285487,"L and M define a metriplectic system, meaning that the mentioned symmetries and degeneracy
460"
REFERENCES,0.43460925039872406,"conditions hold. Then, it follows from L∇S = 0 that the projection P ⊥
S LP ⊥
S
= L leaves L
461"
REFERENCES,0.4354066985645933,"invariant, so that choosing A = L yields P ⊥
S AP ⊥
S = L. Similarly, from positive semi-definiteness
462"
REFERENCES,0.43620414673046254,"and M∇E = 0 it follows that M = UΛU ⊺= P ⊥
E UΛU ⊺P ⊥
E for some column-orthonormal
463"
REFERENCES,0.4370015948963317,"U ∈RN×r and positive diagonal Λ ∈Rr×r. Therefore, choosing B = U and D = Λ yields
464"
REFERENCES,0.43779904306220097,"M = P ⊥
E BDB⊺P ⊥, as desired.
465"
REFERENCES,0.43859649122807015,"Looking toward the proof of Proposition 3.7, we also need to establish the following Lemmata which
466"
REFERENCES,0.4393939393939394,"give control over the orthogonal projectors P ⊥
˜
E , P ⊥
˜S . First, we recall how control over the L∞norm
467"
REFERENCES,0.44019138755980863,"|·|∞of a matrix field gives control over its spectral norm |·|.
468"
REFERENCES,0.4409888357256778,"Lemma A.1. Let A : K →Rn×n be a matrix field defined on the compact set K ⊂Rn with m
469"
REFERENCES,0.44178628389154706,"continuous derivatives. Then, for any ε > 0 there exists a two-layer neural network ˜
A : K →Rn×n
470"
REFERENCES,0.44258373205741625,"such that supx∈K
A −˜
A
 < ε and supx∈K
∇kA −∇k ˜
A

∞< ε for 1 ≤k ≤m where ∇k is the
471"
REFERENCES,0.4433811802232855,"(total) derivative operator of order k.
472"
REFERENCES,0.44417862838915473,"Proof. This will be a direct consequence of Corollary 2.2 in [24] provided we show that |A| ≤c|A|∞
473"
REFERENCES,0.4449760765550239,"for some c > 0. To that end, if σ1 ≥... ≥σr > 0 (r ≤n) denote the nonzero singular values of
474"
REFERENCES,0.44577352472089316,"A −˜
A, it follows that for each x ∈K,
475"
REFERENCES,0.44657097288676234,"A −˜
A
 = σ1 ≤
q"
REFERENCES,0.4473684210526316,"σ2
1 + ... + σ2r = sX i,j"
REFERENCES,0.4481658692185008,"Aij −˜Aij

2
=
A −˜
A

F ."
REFERENCES,0.44896331738437,"On the other hand, it also follows that
476"
REFERENCES,0.44976076555023925,"A −˜
A

F = sX i,j"
REFERENCES,0.45055821371610844,"Aij −˜Aij

2
≤
sX"
REFERENCES,0.4513556618819777,"i,j
max
i,j"
REFERENCES,0.45215311004784686,"Aij −˜Aij
 = n
r"
REFERENCES,0.4529505582137161,"max
i,j"
REFERENCES,0.45374800637958534,"Aij −˜Aij
 = n
A −˜
A

∞,"
REFERENCES,0.45454545454545453,"and therefore the desired inequality holds with c = n. Now, for any ε > 0 it follows from [24] that
477"
REFERENCES,0.45534290271132377,"there exists a two layer network ˜
A with m continuous derivatives such that supx∈K
A −˜
A

∞< ε/n
478"
REFERENCES,0.45614035087719296,"and supx∈K
∇kA −∇k ˜
A

∞< ε/n < ε for all 1 ≤k ≤m. Therefore, it follows that
479"
REFERENCES,0.4569377990430622,"sup
x∈K"
REFERENCES,0.45773524720893144,"A −˜
A
 ≤n sup
x∈K"
REFERENCES,0.4585326953748006,"A −˜
A

∞< n ε"
REFERENCES,0.45933014354066987,"n = ε,"
REFERENCES,0.46012759170653905,"completing the argument.
480"
REFERENCES,0.4609250398724083,"Next, we bound the deviation in the orthogonal projectors P ⊥
˜
E , P ⊥
˜S .
481"
REFERENCES,0.46172248803827753,"Lemma A.2. Let f : Rn →R be such that ∇f ̸= 0 on the compact set K ⊂Rn. For any ε > 0,
482"
REFERENCES,0.4625199362041467,"there exists a two-layer neural network ˜f : K →R such that ∇˜f ̸= 0 on K, supx∈K
f −˜f
 <
483"
REFERENCES,0.46331738437001596,"ε, supx∈K
∇f −∇˜f
 < ε, and supx∈K
P ⊥
f −P ⊥
˜
f"
REFERENCES,0.46411483253588515,"< ε.
484"
REFERENCES,0.4649122807017544,"Proof. Denote ∇f = v and consider any ˜v : K →R. Since |v| ≤|˜v| + |v −˜v|, it follows for all
485"
REFERENCES,0.46570972886762363,"x ∈K that whenever |v −˜v| < (1/2) infx∈K|v|,
486"
REFERENCES,0.4665071770334928,|˜v| ≥|v| −|v −˜v| > |v| −1
INF,0.46730462519936206,"2 inf
x∈K|v| > 0,"
INF,0.46810207336523124,"so that ˜v ̸= 0 in K, and since the square function is monotonic,
487"
INF,0.4688995215311005,"inf
x∈K|˜v|2 ≥inf
x∈K"
INF,0.4696969696969697,"
|v| −1"
INF,0.4704944178628389,"2 inf
x∈K|v|
2
= 1"
INF,0.47129186602870815,"4 inf
x∈K|v|2."
INF,0.47208931419457734,"On the other hand, we also have |˜v| ≤|v| + |˜v −v| < |v| + (1/2) infx∈K|v|, so that, adding and
488"
INF,0.4728867623604466,"subtracting ˜vv⊺and applying Cauchy-Schwarz, it follows that for all x ∈K,
489"
INF,0.47368421052631576,"|vv⊺−˜v˜v⊺| ≤|v −˜v||v| + |˜v||v −˜v| ≤2 max{|v|, |˜v|}|v −˜v| <

2|v| + inf
x∈K|v|

|v −˜v|."
INF,0.474481658692185,"Now, by Corollary 2.2 in [24], for any ε > 0 there exists a two-layer neural network ˜f : K →R such
490"
INF,0.47527910685805425,"that
491"
INF,0.47607655502392343,"sup
x∈K"
INF,0.4768740031897927,"v −∇˜f
 < min"
INF,0.47767145135566186,"(
1
2 inf
x∈K|v|,
infx∈K|v|2"
INF,0.4784688995215311,"2 supx∈K|v| + infx∈K|v|
ε
4, ε ) ≤ε,"
INF,0.47926634768740034,"and also supx∈K
f −˜f
 < ε. Letting ˜v = ∇˜f, it follows that for all x ∈K,
492"
INF,0.4800637958532695,"P ⊥
f −P ⊥
˜
f = vv⊺"
INF,0.48086124401913877,|v|2 −˜v˜v⊺ |˜v|2
INF,0.48165869218500795,"≤
|vv⊺−˜v˜v⊺|"
INF,0.4824561403508772,"min
n
|v|2, |˜v|2o ≤2|v| + infx∈K|v|"
INF,0.48325358851674644,"min
n
|v|2, |˜v|2o |v −˜v|,"
INF,0.4840510366826156,"and therefore, taking the supremum of both sides and applying the previous work yields the desired
493"
INF,0.48484848484848486,"estimate,
494"
INF,0.48564593301435405,"sup
x∈K"
INF,0.4864433811802233,"P ⊥
f −P ⊥
˜
f"
INF,0.48724082934609253,≤42 supx∈K|v| + infx∈K|v|
INF,0.4880382775119617,"infx∈K|v|2
sup
x∈K
|v −˜v| < ε."
INF,0.48883572567783096,"With these intermediate results established, the proof of the approximation result Proposition 3.7
495"
INF,0.48963317384370014,"proceeds as follows.
496"
INF,0.4904306220095694,"Proof of Proposition 3.7. Recall from Theorem 3.4 that we can write L = P ⊥
S (Atri −A⊺
tri)P ⊥
S
497"
INF,0.49122807017543857,"and similarly for ˜L. Notice that, by adding and subtracting P ⊥
˜S AtriP ⊥
S and P ⊥
˜S ˜
AtriP ⊥
S , it follows
498"
INF,0.4920255183413078,"that for all x ∈K,
499
P ⊥
S AtriP ⊥
S −P ⊥
˜S ˜
AtriP ⊥
˜S "
INF,0.49282296650717705,"=

 
P ⊥
S −P ⊥
˜S

AtriP ⊥
S + P ⊥
˜S"
INF,0.49362041467304624,"
Atri −˜
Atri

P ⊥
S + P ⊥
˜S ˜
Atri
 
P ⊥
S −P ⊥
˜S
"
INF,0.4944178628389155,"≤
P ⊥
S −P ⊥
˜S
|Atri| +
Atri −˜
Atri
 +
 ˜
Atri

P ⊥
S −P ⊥
˜S"
INF,0.49521531100478466,"≤2 max
n
|Atri|,
 ˜
Atri

oP ⊥
S −P ⊥
˜S
 +
Atri −˜
Atri"
INF,0.4960127591706539,"where we have used that P ⊥
S , P ⊥
˜S have unit spectral norm. By Lemma A.1, for any ε > 0 there exists
500"
INF,0.49681020733652315,"a two layer neural network ˜
Atri such that supx∈K
Atri −˜
Atri
 < ε"
INF,0.49760765550239233,"4, and by Lemma A.2 there exists
501"
INF,0.4984051036682616,"a two-layer network ˜S with ∇˜S ̸= 0 on K such that
502"
INF,0.49920255183413076,"sup
x∈K"
INF,0.5,"P ⊥
S −P ⊥
˜S
 < min ("
INF,0.5007974481658692,"ε, max

sup
x∈K
|Atri|, sup
x∈K"
INF,0.5015948963317385,"˜
Atri

−1 ε 8 ) ."
INF,0.5023923444976076,"It follows that ˜S, ∇˜S are ε-close to S, ∇S on K and
503"
INF,0.5031897926634769,"sup
x∈K"
INF,0.5039872408293461,"
2 max
n
|Atri|,
 ˜
Atri

oP ⊥
S −P ⊥
˜S


< ε 4."
INF,0.5047846889952153,"Therefore, the estimate
504"
INF,0.5055821371610846,"sup
x∈K"
INF,0.5063795853269537,"L −˜L
 ≤2 sup
x∈K"
INF,0.507177033492823,"P ⊥
S AtriP ⊥
S −P ⊥
˜S ˜
AtriP ⊥
˜S"
INF,0.5079744816586922,"< 2
ε 4 + ε 4"
INF,0.5087719298245614,"
= ε,"
INF,0.5095693779904307,"implies that ˜L is ε-close to L on K as well.
505"
INF,0.5103668261562998,"Moving to the case of M, we see that for all x ∈K, by writing M = UΛU ⊺= KcholK⊺
chol for
506"
INF,0.511164274322169,"Kchol = UΛ1/2 and repeating the first calculation with Kchol in place of Atri and P ⊥
E in place of
507"
INF,0.5119617224880383,"P ⊥
S ,
508"
INF,0.5127591706539075,"P ⊥
E KcholK⊺
cholP ⊥
E −P ⊥
˜
E ˜
Kchol ˜
K⊺
cholP ⊥
˜
E "
INF,0.5135566188197768,"≤2 max
n
|Kchol|,
 ˜
Kchol

oP ⊥
E −P ⊥
˜
E
 +
KcholK⊺
chol −˜
Kchol ˜
K⊺
chol
."
INF,0.5143540669856459,"Moreover, if
Kchol −˜
Kchol
 < (1/2) infx∈K|Kchol| for all x ∈K then similar arguments as used
509"
INF,0.5151515151515151,"in the proof of Lemma A.2 yield the following estimate for all x ∈K,
510
KcholK⊺
chol −˜
Kchol ˜
K⊺
chol
 ≤2 max
n
|Kchol|,
 ˜
Kchol

oKchol −˜
Kchol"
INF,0.5159489633173844,"≤

2|Kchol| + inf
x∈K|Kchol|
Kchol −˜
Kchol
."
INF,0.5167464114832536,"As before, we now invoke Lemma A.1 to construct a two-layer lower-triangular network ˜
Kchol such
511"
INF,0.5175438596491229,"that
512"
INF,0.518341307814992,"sup
x∈K"
INF,0.5191387559808612,"Kchol −˜
Kchol
 < min
1"
INF,0.5199362041467305,"2 inf
x∈K|Kchol|,

2 sup
x∈K
|Kchol| + inf
x∈K|Kchol|
−1 ε 2 
,"
INF,0.5207336523125997,"as well as (using Lemma A.2) a network ˜E satisfying ∇˜E ̸= 0 on K and
513"
INF,0.5215311004784688,"sup
x∈K"
INF,0.5223285486443381,"P ⊥
E −P ⊥
˜
E
 < min ("
INF,0.5231259968102073,"ε, max

sup
x∈K
|Kchol|, sup
x∈K"
INF,0.5239234449760766,"˜
Kchol

−1 ε 4 ) ."
INF,0.5247208931419458,"Again, it follows that ˜E, ∇˜E are ε-close to E, ∇E on K, and by the work above we conclude
514"
INF,0.5255183413078149,"sup
x∈K"
INF,0.5263157894736842,"M −˜
M
 = sup
x∈K"
INF,0.5271132376395534,"P ⊥
E KcholK⊺
cholP ⊥
E −P ⊥
˜
E ˜
Kchol ˜
K⊺
cholP ⊥
˜
E < ε 2 + ε"
INF,0.5279106858054227,"2 = ε,"
INF,0.5287081339712919,"as desired.
515"
INF,0.529505582137161,"It is now possible to give a proof of the error bound in Theorem 3.9. Recall the L2([0, T]) error
516"
INF,0.5303030303030303,"metric ∥x∥and Lipschitz constant Lf, defined for all x, y ∈Rn and Lipschitz continuous functions
517"
INF,0.5311004784688995,"f as
518"
INF,0.5318979266347688,"∥x∥2 =
Z T"
INF,0.532695374800638,"0
|x|2 dt,
|f(x) −f(y)| ≤Lf|x −y|."
INF,0.5334928229665071,"Proof of Theorem 3.9. First, note that the assumption that one of E, −S (without loss of generality,
519"
INF,0.5342902711323764,"say E) has bounded sublevel sets implies bounded trajectories for the state x as in Remark 3.8,
520"
INF,0.5350877192982456,"so we may assume x ∈K for some compact K ⊂Rn. Moreover, for any ε > 0 it follows
521"
INF,0.5358851674641149,"from Proposition 3.7 that there are approximate networks ˜E, ˜S which are ε-close to E, S on K.
522"
INF,0.5366826156299841,"Additionally, it follows that ˜E, ˜S have nonzero gradients ∇˜E, ∇˜S which are also ε-close to the true
523"
INF,0.5374800637958532,"gradients ∇E, ∇S on K. This implies that for each x ∈K, E = ˜E + (E −˜E) ≤˜E + ε, so it
524"
INF,0.5382775119617225,"follows that the sublevel sets {x | ˜E(x) ≤m} ⊆{x | E(x) ≤m + ε} are also bounded. Therefore,
525"
INF,0.5390749601275917,"we may assume (by potentially enlarging K) that both x, ˜x ∈K lie in the compact set K for all time.
526"
INF,0.539872408293461,"Now, let y = x −˜x. The next goal is to bound the following quantity:
527"
INF,0.5406698564593302,"| ˙y| =
L(x)∇E(x) + M(x)∇S(x) −˜L(˜x)∇˜E(˜x) −˜
M(˜x)∇˜S(˜x)"
INF,0.5414673046251993,"=


L(x)∇E(x) −˜L(˜x)∇E(˜x)

+

M(x)∇S(x) −˜
M(˜x)∇S(˜x)
 =: | ˙yE + ˙yS|."
INF,0.5422647527910686,"To that end, notice that by adding and subtracting L(x)∇E(˜x), ˜L(x)∇E(˜x), ˜L(˜x)∇E(˜x), it fol-
528"
INF,0.5430622009569378,"lows that
529"
INF,0.543859649122807,"˙yE = L(x)(∇E(x) −∇E(˜x)) +

L(x) −˜L(x)

∇E(˜x)"
INF,0.5446570972886763,"+

˜L(x) −˜L(˜x)

∇E(˜x) + ˜L(˜x)

∇E(˜x) −∇˜E(˜x)

."
INF,0.5454545454545454,"By Proposition 3.7 there exists a two-layer neural network ˜L with one continuous derivative such
530"
INF,0.5462519936204147,"that supx∈K
L −˜L
 < ε, which implies that ˜L is Lipschitz continuous with (uniformly well-
531"
INF,0.5470494417862839,"approximated) Lipschitz constant. Using this fact along with the assumed Lipschitz continuity of
532"
INF,0.5478468899521531,"∇E and the approximation properties of the network ˜E already constructed then yields
533"
INF,0.5486443381180224,"| ˙yE| ≤

L∇E sup
x∈K
|L| + L ˜L sup
x∈K
|∇E|

|y| + ε

sup
x∈K"
INF,0.5494417862838915,"˜L
 + sup
x∈K
|∇E|

=: aE|y| + ε bE."
INF,0.5502392344497608,"Similarly, by adding and subtracting M(x)∇S(˜x), ˜
M(x)∇S(˜x), ˜
M(˜x)∇S(˜x), it follows that
534"
INF,0.55103668261563,"˙yS = M(x)(∇S(x) −∇S(˜x)) +

M(x) −˜
M(x)

∇S(˜x)"
INF,0.5518341307814992,"+

˜
M(x) −˜
M(˜x)

∇S(˜x) + ˜
M(˜x)

∇S(˜x) −∇˜S(˜x)

."
INF,0.5526315789473685,"By Proposition 3.7, there exists a two-layer network ˜
M with one continuous derivative such that
535"
INF,0.5534290271132376,"supx∈K
M −˜
M
 < ε, with ˜
M Lipschitz continuous for the same reason as before. It follows from
536"
INF,0.5542264752791068,"this and supx∈K
∇S −∇˜S
 < ε that
537"
INF,0.5550239234449761,"| ˙yS| ≤

L∇S sup
x∈K
|M| + L ˜
M sup
x∈K
|∇S|

|y| + ε

sup
x∈K"
INF,0.5558213716108453,"˜
M
 + sup
x∈K
|∇S|

=: aS|y| + ε bS."
INF,0.5566188197767146,"Now, recall that ∂t|y| = |y|−1( ˙y · y) ≤| ˙y| by Cauchy-Schwarz, and therefore the time derivative of
538"
INF,0.5574162679425837,"|y| is bounded by
539"
INF,0.5582137161084529,∂t|y| ≤| ˙yE| + | ˙yS| = (aE + aS)|y| + ε(bE + bS) =: a|y| + b.
INF,0.5590111642743222,"This implies that ∂t|y| −a|y| ≤b, so multiplying by the integrating factor e−at and integrating in
540"
INF,0.5598086124401914,"time yields
541"
INF,0.5606060606060606,"|y(t)| ≤εb
Z t"
INF,0.5614035087719298,"0
ea(t−τ) dτ = ε b"
INF,0.562200956937799,"a
 
eat −1

,"
INF,0.5629984051036683,"where we used that y(0) = 0 since the initial condition of the trajectories is shared. Therefore, the
542"
INF,0.5637958532695375,"L2 error in time can be approximated by
543"
INF,0.5645933014354066,"∥y∥2 =
Z T"
INF,0.5653907496012759,"0
|y|2 dt ≤ε2 b2"
INF,0.5661881977671451,"a2
 
e2aT −2eaT + T + 1

,"
INF,0.5669856459330144,"establishing the conclusion.
544"
INF,0.5677830940988836,"B
Experimental and Implementation Details
545"
INF,0.5685805422647527,"This Appendix records additional details related to the numerical experiments in Section 5. For each
546"
INF,0.569377990430622,"benchmark problem, a set of trajectories is manufactured given initial conditions by simulating ODEs
547"
INF,0.5701754385964912,"with known metriplectic structure. For the experiments in Table 2, only the observable variables
548"
INF,0.5709728867623605,"are used to construct datasets, since entropic information is assumed to be unknown. Algorithm 2
549"
INF,0.5717703349282297,"summarizes the training of the dynamics models used for comparison with NMS.
550"
INF,0.5725677830940988,Algorithm 2 Training dynamics models
INF,0.5733652312599681,"1: Input: snapshot data X ∈Rn×ns, each column xs = x(ts, µs), target rank r ≥1
2: Initialize loss L = 0 and networks with parameters Θ
3: for step in Nsteps do
4:
Randomly draw an initial condition (t0k, x0k) where k ∈ns
5:
˜x1, ..., ˜xl = ODEsolve(x0k, ˙x, t1, ..., tl)
6:
Compute the loss L((xo
1, . . . , xo
l), (˜xo
0, . . . , ˜xo
l))
7:
Update the model parameters Θ via SGD
8: end for"
INF,0.5741626794258373,"For each compared method, integrating the ODEs is done via the Dormand–Prince method (do-
551"
INF,0.5749601275917066,"pri5) [27] with relative tolerance 10−7 and absolute tolerance 10−9. The loss is evaluated by
552"
INF,0.5757575757575758,"measuring the discrepancy between the ground truth observable states xo and the approximate observ-
553"
INF,0.5765550239234449,"able states ˜xo in the mean absolute error (MAE) metric. The model parameters Θ (i.e., the weights
554"
INF,0.5773524720893142,"and biases) are updated by using Adamax [28] with an initial learning rate of 0.01. The number of
555"
INF,0.5781499202551834,"training steps is set as 30,000, and the model parameters resulting in the best performance for the
556"
INF,0.5789473684210527,"validation set are chosen for testing. Specific information related to the experiments in Section 5 is
557"
INF,0.5797448165869219,"given in the subsections below.
558"
INF,0.580542264752791,"For generating the results reported in Table 2, we implemented the proposed algorithm in Python
559"
INF,0.5813397129186603,"3.9.12 and PyTorch 2.0.0. Other required information is provided with the accompanying code. All
560"
INF,0.5821371610845295,"experiments are conducted on Apple M2 Max chips with 96 GB memory. To provide the mean
561"
INF,0.5829346092503987,"and the standard deviation, experiments are repeated three times with varying random seeds for all
562"
INF,0.583732057416268,"considered methods.
563"
INF,0.5845295055821371,"B.1
Two gas containers
564"
INF,0.5853269537480064,"As mentioned in the body, the two gas container (TGC) problem tests models’ predictive capability
565"
INF,0.5861244019138756,"(i.e., extrapolation in time). To this end, one simulated trajectory is obtained by solving an IVP with
566"
INF,0.5869218500797448,"a known TGC system and an initial condition, and the trajectory of the observable variables is split
567"
INF,0.5877192982456141,"into three subsequences, [0, ttrain], (ttrain, tval], and (tval, ttest] for training, validation, and test with
568"
INF,0.5885167464114832,"0 < ttrain < tval < ttest.
569"
INF,0.5893141945773525,"In the experiment, a sequence of 100,000 timesteps is generated using the Runge–Kutta 4th-
570"
INF,0.5901116427432217,"order (RK4) time integrator with a step size 0.001.
The initial condition is given as x =
571"
INF,0.5909090909090909,"(1, 2, 103.2874, 103.2874) following [29]. The training/validation/test split is defined by ttrain = 20,
572"
INF,0.5917065390749602,"tval = 30, and ttest = 100. For a fair comparison, all considered models are set to have a similar
573"
INF,0.5925039872408293,"number of model parameters, ∼2,000. The specifications of the network architectures are:
574"
INF,0.5933014354066986,"• NMS: The total number of model parameters is 1959. The functions Atri, B, Kchol, E, S
575"
INF,0.5940988835725678,"are parameterized as MLPs with the Tanh nonlinear activation function. The MLPs pa-
576"
INF,0.594896331738437,"rameterizing Atri, B, Kchol, E are specified as 1 hidden layer with 10 neurons, and the on
577"
INF,0.5956937799043063,"parameterizing S is specified as 3 hidden layers with 25 neurons.
578"
INF,0.5964912280701754,"• NODE: The total number of model parameters is 2179. The black-box NODE is param-
579"
INF,0.5972886762360446,"eterized as an MLP with the Tanh nonlinear activation function, 4 hidden layers and 25
580"
INF,0.5980861244019139,"neurons.
581"
INF,0.5988835725677831,"• SPNN: The total number of model parameters is 1954. The functions E and S are parame-
582"
INF,0.5996810207336523,"terized as MLPs with the Tanh nonlinear activation function; each MLP is specified as 3
583"
INF,0.6004784688995215,"hidden layers and 20 neurons. The two 2-tensors defining L and M are defined as learnable
584"
INF,0.6012759170653907,"3 × 3 matrices.
585"
INF,0.60207336523126,"• GNODE: The total number of model parameters is 2343. The functions E and S are
586"
INF,0.6028708133971292,"parameterized as MLPs with the Tanh nonlinear activaton function; each MLP is specified
587"
INF,0.6036682615629984,"as 2 hidden layers and 30 neurons. The matrices and 3-tensors required to learn L and M
588"
INF,0.6044657097288676,"are defined as learnable 3 × 3 matrices and 3 × 3 × 3 tensor.
589"
INF,0.6052631578947368,"• GFINN: The total number of model parameters is 2065. The functions E and S are
590"
INF,0.6060606060606061,"parameterized as MLPs with Tanh nonlinear activation function; each MLP is specified as 2
591"
INF,0.6068580542264753,"hidden layers and 20 neurons. The matrices to required to learn L and M are defined as K
592"
INF,0.6076555023923444,"learnable 3 × 3 matrices, where K is set to 2.
593"
INF,0.6084529505582137,"B.2
Thermoelastic double pendulum
594"
INF,0.6092503987240829,"The equations of motion in this case are given for 1 ≤i ≤2 as
595"
INF,0.6100478468899522,˙qi = pi
INF,0.6108452950558214,"mi
,
˙pi = −∂qi(E1(x) + E2(x)),
˙S1 = κ
 
T −1
1
T2 −1

,
˙S2 = κ
 
T1T −1
2
−1

,"
INF,0.6116427432216905,"where κ > 0 is a thermal conductivity constant (set to 1), mi is the mass of the ith spring (also set to
596"
INF,0.6124401913875598,"1) and Ti = ∂SiEi is its absolute temperature. In this case, qi, pi ∈R2 represent the position and
597"
INF,0.613237639553429,"momentum of the ith mass, while Si represents the entropy of the ith pendulum. As before, the total
598"
INF,0.6140350877192983,"entropy S(x) = S1 + S2 is the sum of the entropies of the two springs, while defining the internal
599"
INF,0.6148325358851675,"energies
600"
INF,0.6156299840510366,Ei(x) = 1
INF,0.6164274322169059,"2(ln λi)2 + ln λi + eSi−ln λi −1,
λ1 = |qi|,
λ2 = |q2 −q1|,"
INF,0.6172248803827751,"leads to the total energy E(x) = (1/2m1)|p1|2 + (1/2m2)|p2|2 + E1(x) + E2(x).
601"
INF,0.6180223285486444,"The thermoelastic double pendulum experiment tests model prediction across initial conditions. In
602"
INF,0.6188197767145136,"this case, 100 trajectories are generated by varying initial conditions that are randomly sampled from
603"
INF,0.6196172248803827,"[0.1,1.1] × [-0.1,0.1] × [2.1, 2.3] × [-0.1,0.1] × [-1.9,2.1] × [0.9,1.1] × [-0.1, 0.1] × [0.9,1.1] ×
604"
INF,0.620414673046252,"[0.1,0.3] ⊂R10. Each trajectory is obtained from the numerical integration of the ODEs using an
605"
INF,0.6212121212121212,"RK4 time integrator with step size 0.02 and the final time T = 40, resulting in the trajectories of
606"
INF,0.6220095693779905,"length 2,000. The resulting 100 trajectories are split into 80/10/10 for training/validation/test sets. For
607"
INF,0.6228070175438597,"a fair comparison, all considered models are again set to have similar number of model parameters,
608"
INF,0.6236044657097288,"∼2,000. The specifications of the network architectures are:
609"
INF,0.6244019138755981,"• NMS: The total number of model parameters is 2201. The functions A, B, K, E, S are pa-
610"
INF,0.6251993620414673,"rameterized as MLPs with the Tanh nonlinear activation function. The MLPs parameterizing
611"
INF,0.6259968102073366,"are specified as 1 hidden layer with 15 neurons.
612"
INF,0.6267942583732058,"• NODE: The total number of model parameters is 2005. The black-box NODE is param-
613"
INF,0.6275917065390749,"eterized as an MLP with the Tanh nonlinear activation function, 2 hidden layers and 35
614"
INF,0.6283891547049442,"neurons.
615"
INF,0.6291866028708134,"• SPNN: The total number of model parameters is 2362. The functions E and S are parame-
616"
INF,0.6299840510366826,"terized as MLPs with the Tanh nonlinear activation function; each MLP is specified as 3
617"
INF,0.6307814992025519,"hidden layers and 20 neurons. The two 2-tensors defining L and M are defined as learnable
618"
INF,0.631578947368421,"3 × 3 matrices.
619"
INF,0.6323763955342903,"• GNODE: The total number of model parameters is 2151. The functions E and S are
620"
INF,0.6331738437001595,"parameterized as MLPs with the Tanh nonlinear activaton function; each MLP is specified
621"
INF,0.6339712918660287,"as 2 hidden layers and 15 neurons. The matrices and 3-tensors required to learn L and M
622"
INF,0.6347687400318979,"are defined as learnable 3 × 3 matrices and 3 × 3 × 3 tensor.
623"
INF,0.6355661881977671,"• GFINN: The total number of model parameters is 2180. The functions E and S are
624"
INF,0.6363636363636364,"parameterized as MLPs with Tanh nonlinear activation function; each MLP is specified as 2
625"
INF,0.6371610845295056,"hidden layers and 15 neurons. The matrices to required to learn L and M are defined as K
626"
INF,0.6379585326953748,"learnable 3 × 3 matrices, where K is set to 2.
627"
INF,0.638755980861244,"C
Additional experiment: Damped nonlinear oscillator
628"
INF,0.6395534290271132,"Consider a damped nonlinear oscillator of variable dimension with state x = (q
p
S)⊺, whose
629"
INF,0.6403508771929824,"motion is governed by the metriplectic system
630"
INF,0.6411483253588517,˙q = p
INF,0.6419457735247209,"m,
˙p = k sin q −γp,
˙S = γ|q|2 mT ."
INF,0.6427432216905901,"Here q, p ∈Rn denote the position and momentum of the oscillator, S is the entropy of a surround-
631"
INF,0.6435406698564593,"ing thermal bath, and the constant parameters m, γ, T are the mass, damping rate, and (constant)
632"
INF,0.6443381180223285,"temperature. This leads to the total energy E(x) = (1/2m)|p|2 −k cos q + TS, which is readily
633"
INF,0.6451355661881978,"seen to be constant along solutions x(t).
634"
INF,0.645933014354067,"It is now verified that NMS can accurately and stably predict the dynamics of a nonlinear oscillator
635"
INF,0.6467304625199362,"x = (q
p
S)⊺in the case that n = 1, 2, both when the entropy S is observable as well as when it
636"
INF,0.6475279106858054,"is not. As before, the task considered is prediction in time, although all compared methods NODE,
637"
INF,0.6483253588516746,"GNODE, and NMSknown are now trained on full state information from the training interval, and test
638"
INF,0.6491228070175439,"errors are computed over the full state x on the extrapolation interval (tvalid, ttest], which is 150%
639"
INF,0.6499202551834131,"longer than the training interval. In addition, another NMS model, NMSdiff, was trained using only
640"
INF,0.6507177033492823,"the partial state information xo = (q, p)⊺and tested under the same conditions, with the initial guess
641"
INF,0.6515151515151515,"for xu generated as in Appendix E. As can be seen in Table 3, NMS is more accurate than GNODE
642"
INF,0.6523125996810207,"or NODE in both the 1-D and 2-D nonlinear oscillator experiments, improving on previous results by
643"
INF,0.65311004784689,"up to two orders of magnitude. Remarkably, NMS produces more accurate entropic dynamics even
644"
INF,0.6539074960127592,"in the case where the entropic variable S is unobserved during NMS training and observed during
645"
INF,0.6547049441786283,"the training of other methods. This illustrates another advantage of the NMS approach: because of
646"
INF,0.6555023923444976,"the reasonable initial data for S produced by the diffusion model, the learned metriplectic system
647"
INF,0.6562998405103668,"produced by NMS remains performant even when metriplectic governing equations are unknown and
648"
INF,0.6570972886762361,"only partial state information is observed.
649"
INF,0.6578947368421053,"To describe the experimental setup precisely, data is collected from a single trajectory with initial
650"
INF,0.6586921850079744,"condition as x = (2, 0, 0) following [16]. The path is calculated at 180,000 steps with a time interval
651"
INF,0.6594896331738437,"of 0.001, and is then split into training/validation/test sets as before using ttrain = 60, tval = 90 and
652"
INF,0.6602870813397129,"ttest = 180. Specifications of the networks used for the experiments in Table 3 are:
653"
INF,0.6610845295055822,"• NMS: The total number of parameters is 154. The number of layers for Atri, B, Kchol, E, S
654"
INF,0.6618819776714514,"is selected from {1,2,3} and the number of neurons per layer from {5,10,15}. The best
655"
INF,0.6626794258373205,"hyperparameters are 1 hidden layer with 5 neurons for each network function.
656"
INF,0.6634768740031898,"• GNODE: The total number of model parameters is 203. The number of layers and num-
657"
INF,0.664274322169059,"ber of neurons for each network is chosen from the same ranges as for NMS. The best
658"
INF,0.6650717703349283,"hyperparameters are 1 layer with 10 neurons for each network function.
659"
INF,0.6658692185007975,"Table 3: Experimental results for the benchmark problems with respect to MSE and MAE. The best
scores are in boldface."
INF,0.6666666666666666,"1-D D.N.O.
T.G.C.
2-D D.N.O.
MSE
MAE
MSE
MAE
MSE
MAE"
INF,0.6674641148325359,"NMSdiff
.0170
.1132
.0045
.0548
.0275
.1456
NMSknown
.0239
.1011
.0012
.0276
.0018
.0357"
INF,0.6682615629984051,"NODE
.0631
.2236
.0860
.2551
.0661
.2096
GNODE
.0607
.1976
.0071
.0732
.2272
.4267"
INF,0.6690590111642744,"• NODE: The total number of model paramters is 3003. The NODE architecture is formed by
660"
INF,0.6698564593301436,"stacking MLPs with Tanh activation functions. The number of blocks is chosen from {3,4,5}
661"
INF,0.6706539074960127,"and the number of neurons of each MLP from {30,40,50}. The best hyperparameters are 4
662"
INF,0.671451355661882,"and 30 for the number of blocks and number of neurons, respectively.
663"
INF,0.6722488038277512,"D
Scaling study
664"
INF,0.6730462519936204,"To compare the scalability of the proposed NMS architecture design with existing architectures, dif-
665"
INF,0.6738437001594896,"ferent realizations of GNODE, GFINN, and NMS are generated by varying the dimension of the state
666"
INF,0.6746411483253588,"variables, n = {1, 5, 10, 15, 20, 30, 50}. The specifications of these models (i.e., hyperparameters)
667"
INF,0.6754385964912281,"are set so that the number of model parameters is kept similar between each method for smaller values
668"
INF,0.6762360446570973,"of n. For example, for n = 1, 5 the number of model parameters is ∼20,000 for each architecture.
669"
INF,0.6770334928229665,"The results in Figure 3(a) confirm that GNODE scales cubically in n while both GFINN and NMS
670"
INF,0.6778309409888357,"scale quadratically. Note that only a constant scaling advantage of NMS over GFINN can be seen
671"
INF,0.6786283891547049,"from this plot, since r is fixed during this study.
672"
INF,0.6794258373205742,"It is also worthwhile to investigate the computational timings of these three models. Consider-
673"
INF,0.6802232854864434,"ing the same realizations of the models listed above, i.e., the model instances for varying n =
674"
INF,0.6810207336523126,"{1, 5, 10, 15, 20, 30, 50}, 1,000 random samples of states {x(i)}1,000
i=1
are generated. These samples
675"
INF,0.6818181818181818,"are then fed to the dynamics function L(x(i))∇E(x(i)) + M(x(i))∇S(x(i)) for i = 1, . . . , 1000,
676"
INF,0.682615629984051,"and the computational wall time of the function evaluation via PyTorch’s profiler API is measured.
677"
INF,0.6834130781499203,"The results of this procedure are displayed in Figure 3(b). Again, it is seen that the proposed NMSs
678"
INF,0.6842105263157895,"require less computational resources than GNODEs and GFINNs.
679"
INF,0.6850079744816587,"(a) State dimension n versus number
of model parameters"
INF,0.6858054226475279,"(b) Model parameters versus wall
time in microseconds"
INF,0.6866028708133971,"Figure 3: A study of the scaling behavior of GNODE, GFINN, and NMS."
INF,0.6874003189792663,"E
Diffusion model for unobserved variables
680"
INF,0.6881977671451356,"Recent work in [30] suggests the benefits of performing time-series generation using a diffusion
681"
INF,0.6889952153110048,"model. This Appendix describes how this technology is used to generate initial conditions for the
682"
INF,0.689792663476874,"unobserved NMS variables in the experiments corresponding to Table 3. More precisely, we describe
683"
INF,0.6905901116427432,"how to train a conditional diffusion model which generates values for unobserved variables xu given
684"
INF,0.6913875598086124,"values for the observed variables xo.
685"
INF,0.6921850079744817,"Training and sampling:
Recall that diffusion models add noise with the following stochastic
686"
INF,0.6929824561403509,"differential equation (SDE):
687"
INF,0.69377990430622,"dx(t) = f(t, x(t))dt + g(t)dw,
t ∈[0, 1],
where w ∈Rdim(x) is a multi-dimensional Brownian motion, f(t, ·) : Rdim(x) →Rdim(x) is a
688"
INF,0.6945773524720893,"vector-valued drift term, and g : [0, 1] →R is a scalar-valued diffusion function.
689"
INF,0.6953748006379585,"For the forward SDE, there exists a corresponding reverse SDE:
690"
INF,0.6961722488038278,"dx(t) = [f(t, x(t)) −g2(t)∇x(t)log p(x(t))]dt + g(t)d¯w,
which produces samples from the initial distribution at t = 0. This formula suggests that if the score
691"
INF,0.696969696969697,"function, ∇x(t)log p(x(t)), is known, then real samples from the prior distribution p(x) ∼N(µ, σ2)
692"
INF,0.6977671451355661,"can be recovered, where µ, σ vary depending on the forward SDE type.
693"
INF,0.6985645933014354,"In order for a model Mθ to learn the score function, it has to optimize the following loss:
694"
INF,0.6993620414673046,"L(θ) = Et{λ(t)Ex(t)[
Mθ(t, x(t)) −∇x(t) log p(x(t))
2
2]},
where t is uniformly sampled over [0, 1] with an appropriate weight function λ(t) : [0, 1] →R.
695"
INF,0.7001594896331739,"However, using the above formula is computationally prohibitive. Thanks to [31], this loss can be
696"
INF,0.7009569377990431,"substituted with the following denoising score matching loss:
697"
INF,0.7017543859649122,"L∗(θ) = Et{λ(t)Ex(0)Ex(t)|x(0)[
Mθ(t, x(t)) −∇x(t) log p(x(t)|x(0))
2
2]}."
INF,0.7025518341307815,"Since score-based generative models use an affine drift term, the transition kernel p(x(t)|x(0)) follows
698"
INF,0.7033492822966507,"a certain Gaussian distribution [32], and therefore the gradient term ∇x(t) log p(x(t)|x(0)) can be
699"
INF,0.70414673046252,"analytically calculated.
700"
INF,0.7049441786283892,"Experimental details
On the other hand, the present goal is to generate unobserved variables xu
701"
INF,0.7057416267942583,"given values for the observed variables xo = (q, p), i.e., conditional generation. Therefore, our model
702"
INF,0.7065390749601276,"has to learn the conditional score function, ∇xu(t) log p(xu(t)|xo). For example, in the damped
703"
INF,0.7073365231259968,"nonlinear oscillator case, S(t) is initialized as a perturbed t ∈[0, 1], from which the model takes the
704"
INF,0.7081339712918661,"concatenation of q, p, S(t) as inputs and learns conditional the score function ∇S(t) log(S(t)|q, p).
705"
INF,0.7089314194577353,"For the experiments in Table 3, diffusion models are trained to generate xu variables on three
706"
INF,0.7097288676236044,"benchmark problems: the damped nonlinear oscillator, two gas containers, and thermolastic double
707"
INF,0.7105263157894737,"pendulum. On each problem, representative parameters such as mass or thermal conductivity are
708"
INF,0.7113237639553429,"varied, with the total number of cases denoted by N. Full trajectory data of length T is then generated
709"
INF,0.7121212121212122,"using a standard numerical integrator (e.g., dopri5), before it is evenly cut into ⌊T/L⌋pieces of
710"
INF,0.7129186602870813,"length L. Let V, U denote the total number of variables and the number of unobserved variables,
711"
INF,0.7137161084529505,"respectively. It follows that the goal is to generate U unobserved variables given V −U observed
712"
INF,0.7145135566188198,"ones, i.e., the objective is to generate data of shape (NT/L, L, U) conditioned on data of shape
713"
INF,0.715311004784689,"(NT/L, L, V −U). After the diffusion model has been trained for this task, the output data is
714"
INF,0.7161084529505582,"reshaped into size (N, T, U), which is used to initialize the NMS model. Note that the NODE and
715"
INF,0.7169059011164274,"GNODE methods compared to NMS in Table 3 use full state information for their training, i.e.,
716"
INF,0.7177033492822966,"xu = ∅in these cases, making it comparatively easier for these methods to learn system dynamics.
717"
INF,0.7185007974481659,"As in other diffusion models e.g. [33], a U-net architecture is used, modifying 2-D convolutions to
718"
INF,0.7192982456140351,"1-D ones and following the detailed hyperparameters described in [33]. Note the following probability
719"
INF,0.7200956937799043,"flow ODE seen in [33]:
720"
INF,0.7208931419457735,"dx(t) =

f(t, x(t)) −1"
INF,0.7216905901116427,"2g2(t)∇x(t)log p(x(t))

dt,"
INF,0.722488038277512,"Although models trained to mimic the probability flow ODE do not match the perofrmance of the
721"
INF,0.7232854864433812,"forward SDE’s result in the image domain, the authors of [30] observe that the probability flow ODE
722"
INF,0.7240829346092504,"outperforms the forward SDE in the time-series domain. Therefore, the probability flow ODE is used
723"
INF,0.7248803827751196,"with the default hyperparameters of [33].
724"
INF,0.7256778309409888,"NeurIPS Paper Checklist
725"
INF,0.726475279106858,"The checklist is designed to encourage best practices for responsible machine learning research,
726"
INF,0.7272727272727273,"addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
727"
INF,0.7280701754385965,"the checklist: The papers not including the checklist will be desk rejected. The checklist should
728"
INF,0.7288676236044657,"follow the references and follow the (optional) supplemental material. The checklist does NOT count
729"
INF,0.7296650717703349,"towards the page limit.
730"
INF,0.7304625199362041,"Please read the checklist guidelines carefully for information on how to answer these questions. For
731"
INF,0.7312599681020734,"each question in the checklist:
732"
INF,0.7320574162679426,"• You should answer [Yes] , [No] , or [NA] .
733"
INF,0.7328548644338118,"• [NA] means either that the question is Not Applicable for that particular paper or the
734"
INF,0.733652312599681,"relevant information is Not Available.
735"
INF,0.7344497607655502,"• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
736"
INF,0.7352472089314195,"The checklist answers are an integral part of your paper submission. They are visible to the
737"
INF,0.7360446570972887,"reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
738"
INF,0.7368421052631579,"(after eventual revisions) with the final version of your paper, and its final version will be published
739"
INF,0.7376395534290271,"with the paper.
740"
INF,0.7384370015948963,"The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
741"
INF,0.7392344497607656,"While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
742"
INF,0.7400318979266348,"proper justification is given (e.g., ""error bars are not reported because it would be too computationally
743"
INF,0.740829346092504,"expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
744"
INF,0.7416267942583732,"""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way, we
745"
INF,0.7424242424242424,"acknowledge that the true answer is often more nuanced, so please just use your best judgment and
746"
INF,0.7432216905901117,"write a justification to elaborate. All supporting evidence can appear either in the main paper or the
747"
INF,0.7440191387559809,"supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
748"
INF,0.74481658692185,"please point to the section(s) where related material for the question can be found.
749"
INF,0.7456140350877193,"IMPORTANT, please:
750"
INF,0.7464114832535885,"• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
751"
INF,0.7472089314194578,"• Keep the checklist subsection headings, questions/answers and guidelines below.
752"
INF,0.748006379585327,"• Do not modify the questions and only use the provided macros for your answers.
753"
CLAIMS,0.7488038277511961,"1. Claims
754"
CLAIMS,0.7496012759170654,"Question: Do the main claims made in the abstract and introduction accurately reflect the
755"
CLAIMS,0.7503987240829346,"paper’s contributions and scope?
756"
CLAIMS,0.7511961722488039,"Answer: [Yes]
757"
CLAIMS,0.751993620414673,"Justification: The claims made in the abstract and contributions paragraph at the end of the
758"
CLAIMS,0.7527910685805422,"introduction are justified in detail throughout the rest of the paper.
759"
CLAIMS,0.7535885167464115,"Guidelines:
760"
CLAIMS,0.7543859649122807,"• The answer NA means that the abstract and introduction do not include the claims
761"
CLAIMS,0.75518341307815,"made in the paper.
762"
CLAIMS,0.7559808612440191,"• The abstract and/or introduction should clearly state the claims made, including the
763"
CLAIMS,0.7567783094098883,"contributions made in the paper and important assumptions and limitations. A No or
764"
CLAIMS,0.7575757575757576,"NA answer to this question will not be perceived well by the reviewers.
765"
CLAIMS,0.7583732057416268,"• The claims made should match theoretical and experimental results, and reflect how
766"
CLAIMS,0.759170653907496,"much the results can be expected to generalize to other settings.
767"
CLAIMS,0.7599681020733652,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
768"
CLAIMS,0.7607655502392344,"are not attained by the paper.
769"
LIMITATIONS,0.7615629984051037,"2. Limitations
770"
LIMITATIONS,0.7623604465709729,"Question: Does the paper discuss the limitations of the work performed by the authors?
771"
LIMITATIONS,0.7631578947368421,"Answer: [Yes]
772"
LIMITATIONS,0.7639553429027113,"Justification: Limitations are discussed in the Conclusion section.
773"
LIMITATIONS,0.7647527910685805,"Guidelines:
774"
LIMITATIONS,0.7655502392344498,"• The answer NA means that the paper has no limitation while the answer No means that
775"
LIMITATIONS,0.766347687400319,"the paper has limitations, but those are not discussed in the paper.
776"
LIMITATIONS,0.7671451355661882,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
777"
LIMITATIONS,0.7679425837320574,"• The paper should point out any strong assumptions and how robust the results are to
778"
LIMITATIONS,0.7687400318979266,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
779"
LIMITATIONS,0.7695374800637959,"model well-specification, asymptotic approximations only holding locally). The authors
780"
LIMITATIONS,0.7703349282296651,"should reflect on how these assumptions might be violated in practice and what the
781"
LIMITATIONS,0.7711323763955343,"implications would be.
782"
LIMITATIONS,0.7719298245614035,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
783"
LIMITATIONS,0.7727272727272727,"only tested on a few datasets or with a few runs. In general, empirical results often
784"
LIMITATIONS,0.773524720893142,"depend on implicit assumptions, which should be articulated.
785"
LIMITATIONS,0.7743221690590112,"• The authors should reflect on the factors that influence the performance of the approach.
786"
LIMITATIONS,0.7751196172248804,"For example, a facial recognition algorithm may perform poorly when image resolution
787"
LIMITATIONS,0.7759170653907496,"is low or images are taken in low lighting. Or a speech-to-text system might not be
788"
LIMITATIONS,0.7767145135566188,"used reliably to provide closed captions for online lectures because it fails to handle
789"
LIMITATIONS,0.777511961722488,"technical jargon.
790"
LIMITATIONS,0.7783094098883573,"• The authors should discuss the computational efficiency of the proposed algorithms
791"
LIMITATIONS,0.7791068580542265,"and how they scale with dataset size.
792"
LIMITATIONS,0.7799043062200957,"• If applicable, the authors should discuss possible limitations of their approach to
793"
LIMITATIONS,0.7807017543859649,"address problems of privacy and fairness.
794"
LIMITATIONS,0.7814992025518341,"• While the authors might fear that complete honesty about limitations might be used by
795"
LIMITATIONS,0.7822966507177034,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
796"
LIMITATIONS,0.7830940988835726,"limitations that aren’t acknowledged in the paper. The authors should use their best
797"
LIMITATIONS,0.7838915470494418,"judgment and recognize that individual actions in favor of transparency play an impor-
798"
LIMITATIONS,0.784688995215311,"tant role in developing norms that preserve the integrity of the community. Reviewers
799"
LIMITATIONS,0.7854864433811802,"will be specifically instructed to not penalize honesty concerning limitations.
800"
THEORY ASSUMPTIONS AND PROOFS,0.7862838915470495,"3. Theory Assumptions and Proofs
801"
THEORY ASSUMPTIONS AND PROOFS,0.7870813397129187,"Question: For each theoretical result, does the paper provide the full set of assumptions and
802"
THEORY ASSUMPTIONS AND PROOFS,0.7878787878787878,"a complete (and correct) proof?
803"
THEORY ASSUMPTIONS AND PROOFS,0.7886762360446571,"Answer: [Yes]
804"
THEORY ASSUMPTIONS AND PROOFS,0.7894736842105263,"Justification: All theoretical results are clearly stated along with the necessary assumptions.
805"
THEORY ASSUMPTIONS AND PROOFS,0.7902711323763956,"All formal arguments are complete and contained in the Appendix.
806"
THEORY ASSUMPTIONS AND PROOFS,0.7910685805422647,"Guidelines:
807"
THEORY ASSUMPTIONS AND PROOFS,0.7918660287081339,"• The answer NA means that the paper does not include theoretical results.
808"
THEORY ASSUMPTIONS AND PROOFS,0.7926634768740032,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
809"
THEORY ASSUMPTIONS AND PROOFS,0.7934609250398724,"referenced.
810"
THEORY ASSUMPTIONS AND PROOFS,0.7942583732057417,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
811"
THEORY ASSUMPTIONS AND PROOFS,0.7950558213716108,"• The proofs can either appear in the main paper or the supplemental material, but if
812"
THEORY ASSUMPTIONS AND PROOFS,0.79585326953748,"they appear in the supplemental material, the authors are encouraged to provide a short
813"
THEORY ASSUMPTIONS AND PROOFS,0.7966507177033493,"proof sketch to provide intuition.
814"
THEORY ASSUMPTIONS AND PROOFS,0.7974481658692185,"• Inversely, any informal proof provided in the core of the paper should be complemented
815"
THEORY ASSUMPTIONS AND PROOFS,0.7982456140350878,"by formal proofs provided in appendix or supplemental material.
816"
THEORY ASSUMPTIONS AND PROOFS,0.7990430622009569,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
817"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998405103668261,"4. Experimental Result Reproducibility
818"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8006379585326954,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
819"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8014354066985646,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
820"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8022328548644339,"of the paper (regardless of whether the code and data are provided or not)?
821"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.803030303030303,"Answer: [Yes]
822"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8038277511961722,"Justification: All information necessary to implement the proposed architecture is included
823"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8046251993620415,"in the body of the manuscript. In addition, all relevant experimental details are included in
824"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8054226475279107,"the Appendix.
825"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.80622009569378,"Guidelines:
826"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8070175438596491,"• The answer NA means that the paper does not include experiments.
827"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8078149920255183,"• If the paper includes experiments, a No answer to this question will not be perceived
828"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8086124401913876,"well by the reviewers: Making the paper reproducible is important, regardless of
829"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8094098883572568,"whether the code and data are provided or not.
830"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.810207336523126,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
831"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8110047846889952,"to make their results reproducible or verifiable.
832"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8118022328548644,"• Depending on the contribution, reproducibility can be accomplished in various ways.
833"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8125996810207337,"For example, if the contribution is a novel architecture, describing the architecture fully
834"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8133971291866029,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
835"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8141945773524721,"be necessary to either make it possible for others to replicate the model with the same
836"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8149920255183413,"dataset, or provide access to the model. In general. releasing code and data is often
837"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8157894736842105,"one good way to accomplish this, but reproducibility can also be provided via detailed
838"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8165869218500797,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
839"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.817384370015949,"of a large language model), releasing of a model checkpoint, or other means that are
840"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181818181818182,"appropriate to the research performed.
841"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8189792663476874,"• While NeurIPS does not require releasing code, the conference does require all submis-
842"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8197767145135566,"sions to provide some reasonable avenue for reproducibility, which may depend on the
843"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8205741626794258,"nature of the contribution. For example
844"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8213716108452951,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
845"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8221690590111643,"to reproduce that algorithm.
846"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8229665071770335,"(b) If the contribution is primarily a new model architecture, the paper should describe
847"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8237639553429027,"the architecture clearly and fully.
848"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8245614035087719,"(c) If the contribution is a new model (e.g., a large language model), then there should
849"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8253588516746412,"either be a way to access this model for reproducing the results or a way to reproduce
850"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8261562998405104,"the model (e.g., with an open-source dataset or instructions for how to construct
851"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8269537480063796,"the dataset).
852"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8277511961722488,"(d) We recognize that reproducibility may be tricky in some cases, in which case
853"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.828548644338118,"authors are welcome to describe the particular way they provide for reproducibility.
854"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8293460925039873,"In the case of closed-source models, it may be that access to the model is limited in
855"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8301435406698564,"some way (e.g., to registered users), but it should be possible for other researchers
856"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8309409888357256,"to have some path to reproducing or verifying the results.
857"
OPEN ACCESS TO DATA AND CODE,0.8317384370015949,"5. Open access to data and code
858"
OPEN ACCESS TO DATA AND CODE,0.8325358851674641,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
859"
OPEN ACCESS TO DATA AND CODE,0.8333333333333334,"tions to faithfully reproduce the main experimental results, as described in supplemental
860"
OPEN ACCESS TO DATA AND CODE,0.8341307814992025,"material?
861"
OPEN ACCESS TO DATA AND CODE,0.8349282296650717,"Answer: [Yes]
862"
OPEN ACCESS TO DATA AND CODE,0.835725677830941,"Justification: Code for running the proposed algorithm is included in the supplemental
863"
OPEN ACCESS TO DATA AND CODE,0.8365231259968102,"material and will be released publicly upon publication.
864"
OPEN ACCESS TO DATA AND CODE,0.8373205741626795,"Guidelines:
865"
OPEN ACCESS TO DATA AND CODE,0.8381180223285486,"• The answer NA means that paper does not include experiments requiring code.
866"
OPEN ACCESS TO DATA AND CODE,0.8389154704944178,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
867"
OPEN ACCESS TO DATA AND CODE,0.8397129186602871,"public/guides/CodeSubmissionPolicy) for more details.
868"
OPEN ACCESS TO DATA AND CODE,0.8405103668261563,"• While we encourage the release of code and data, we understand that this might not be
869"
OPEN ACCESS TO DATA AND CODE,0.8413078149920256,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
870"
OPEN ACCESS TO DATA AND CODE,0.8421052631578947,"including code, unless this is central to the contribution (e.g., for a new open-source
871"
OPEN ACCESS TO DATA AND CODE,0.8429027113237639,"benchmark).
872"
OPEN ACCESS TO DATA AND CODE,0.8437001594896332,"• The instructions should contain the exact command and environment needed to run to
873"
OPEN ACCESS TO DATA AND CODE,0.8444976076555024,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
874"
OPEN ACCESS TO DATA AND CODE,0.8452950558213717,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
875"
OPEN ACCESS TO DATA AND CODE,0.8460925039872408,"• The authors should provide instructions on data access and preparation, including how
876"
OPEN ACCESS TO DATA AND CODE,0.84688995215311,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
877"
OPEN ACCESS TO DATA AND CODE,0.8476874003189793,"• The authors should provide scripts to reproduce all experimental results for the new
878"
OPEN ACCESS TO DATA AND CODE,0.8484848484848485,"proposed method and baselines. If only a subset of experiments are reproducible, they
879"
OPEN ACCESS TO DATA AND CODE,0.8492822966507177,"should state which ones are omitted from the script and why.
880"
OPEN ACCESS TO DATA AND CODE,0.8500797448165869,"• At submission time, to preserve anonymity, the authors should release anonymized
881"
OPEN ACCESS TO DATA AND CODE,0.8508771929824561,"versions (if applicable).
882"
OPEN ACCESS TO DATA AND CODE,0.8516746411483254,"• Providing as much information as possible in supplemental material (appended to the
883"
OPEN ACCESS TO DATA AND CODE,0.8524720893141946,"paper) is recommended, but including URLs to data and code is permitted.
884"
OPEN ACCESS TO DATA AND CODE,0.8532695374800638,"6. Experimental Setting/Details
885"
OPEN ACCESS TO DATA AND CODE,0.854066985645933,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
886"
OPEN ACCESS TO DATA AND CODE,0.8548644338118022,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
887"
OPEN ACCESS TO DATA AND CODE,0.8556618819776715,"results?
888"
OPEN ACCESS TO DATA AND CODE,0.8564593301435407,"Answer: [Yes]
889"
OPEN ACCESS TO DATA AND CODE,0.8572567783094099,"Justification: All relevant experimental details are presented in the Appendix at an appropri-
890"
OPEN ACCESS TO DATA AND CODE,0.8580542264752791,"ate level of detail.
891"
OPEN ACCESS TO DATA AND CODE,0.8588516746411483,"Guidelines:
892"
OPEN ACCESS TO DATA AND CODE,0.8596491228070176,"• The answer NA means that the paper does not include experiments.
893"
OPEN ACCESS TO DATA AND CODE,0.8604465709728868,"• The experimental setting should be presented in the core of the paper to a level of detail
894"
OPEN ACCESS TO DATA AND CODE,0.861244019138756,"that is necessary to appreciate the results and make sense of them.
895"
OPEN ACCESS TO DATA AND CODE,0.8620414673046252,"• The full details can be provided either with the code, in appendix, or as supplemental
896"
OPEN ACCESS TO DATA AND CODE,0.8628389154704944,"material.
897"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8636363636363636,"7. Experiment Statistical Significance
898"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8644338118022329,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
899"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8652312599681021,"information about the statistical significance of the experiments?
900"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8660287081339713,"Answer: [Yes]
901"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8668261562998405,"Justification: All experiments in the body contain means and standard deviations as the
902"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8676236044657097,"initialization is varied.
903"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.868421052631579,"Guidelines:
904"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8692185007974481,"• The answer NA means that the paper does not include experiments.
905"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8700159489633174,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
906"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8708133971291866,"dence intervals, or statistical significance tests, at least for the experiments that support
907"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8716108452950558,"the main claims of the paper.
908"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8724082934609251,"• The factors of variability that the error bars are capturing should be clearly stated (for
909"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8732057416267942,"example, train/test split, initialization, random drawing of some parameter, or overall
910"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8740031897926634,"run with given experimental conditions).
911"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8748006379585327,"• The method for calculating the error bars should be explained (closed form formula,
912"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8755980861244019,"call to a library function, bootstrap, etc.)
913"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8763955342902712,"• The assumptions made should be given (e.g., Normally distributed errors).
914"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8771929824561403,"• It should be clear whether the error bar is the standard deviation or the standard error
915"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8779904306220095,"of the mean.
916"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8787878787878788,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
917"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.879585326953748,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
918"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8803827751196173,"of Normality of errors is not verified.
919"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8811802232854864,"• For asymmetric distributions, the authors should be careful not to show in tables or
920"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8819776714513556,"figures symmetric error bars that would yield results that are out of range (e.g. negative
921"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8827751196172249,"error rates).
922"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8835725677830941,"• If error bars are reported in tables or plots, The authors should explain in the text how
923"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8843700159489634,"they were calculated and reference the corresponding figures or tables in the text.
924"
EXPERIMENTS COMPUTE RESOURCES,0.8851674641148325,"8. Experiments Compute Resources
925"
EXPERIMENTS COMPUTE RESOURCES,0.8859649122807017,"Question: For each experiment, does the paper provide sufficient information on the com-
926"
EXPERIMENTS COMPUTE RESOURCES,0.886762360446571,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
927"
EXPERIMENTS COMPUTE RESOURCES,0.8875598086124402,"the experiments?
928"
EXPERIMENTS COMPUTE RESOURCES,0.8883572567783095,"Answer: [Yes]
929"
EXPERIMENTS COMPUTE RESOURCES,0.8891547049441786,"Justification: All necessary information is included in the Appendix.
930"
EXPERIMENTS COMPUTE RESOURCES,0.8899521531100478,"Guidelines:
931"
EXPERIMENTS COMPUTE RESOURCES,0.8907496012759171,"• The answer NA means that the paper does not include experiments.
932"
EXPERIMENTS COMPUTE RESOURCES,0.8915470494417863,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
933"
EXPERIMENTS COMPUTE RESOURCES,0.8923444976076556,"or cloud provider, including relevant memory and storage.
934"
EXPERIMENTS COMPUTE RESOURCES,0.8931419457735247,"• The paper should provide the amount of compute required for each of the individual
935"
EXPERIMENTS COMPUTE RESOURCES,0.8939393939393939,"experimental runs as well as estimate the total compute.
936"
EXPERIMENTS COMPUTE RESOURCES,0.8947368421052632,"• The paper should disclose whether the full research project required more compute
937"
EXPERIMENTS COMPUTE RESOURCES,0.8955342902711324,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
938"
EXPERIMENTS COMPUTE RESOURCES,0.8963317384370016,"didn’t make it into the paper).
939"
CODE OF ETHICS,0.8971291866028708,"9. Code Of Ethics
940"
CODE OF ETHICS,0.89792663476874,"Question: Does the research conducted in the paper conform, in every respect, with the
941"
CODE OF ETHICS,0.8987240829346093,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
942"
CODE OF ETHICS,0.8995215311004785,"Answer: [Yes]
943"
CODE OF ETHICS,0.9003189792663477,"Justification: This is explained in the ""broader impacts"" section.
944"
CODE OF ETHICS,0.9011164274322169,"Guidelines:
945"
CODE OF ETHICS,0.9019138755980861,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
946"
CODE OF ETHICS,0.9027113237639554,"• If the authors answer No, they should explain the special circumstances that require a
947"
CODE OF ETHICS,0.9035087719298246,"deviation from the Code of Ethics.
948"
CODE OF ETHICS,0.9043062200956937,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
949"
CODE OF ETHICS,0.905103668261563,"eration due to laws or regulations in their jurisdiction).
950"
BROADER IMPACTS,0.9059011164274322,"10. Broader Impacts
951"
BROADER IMPACTS,0.9066985645933014,"Question: Does the paper discuss both potential positive societal impacts and negative
952"
BROADER IMPACTS,0.9074960127591707,"societal impacts of the work performed?
953"
BROADER IMPACTS,0.9082934609250398,"Answer: [Yes]
954"
BROADER IMPACTS,0.9090909090909091,"Justification: This paper investigates a novel machine learning method tailored to physics-
955"
BROADER IMPACTS,0.9098883572567783,"based simulations and fundamental science. While subsequent applications of this work may
956"
BROADER IMPACTS,0.9106858054226475,"have societal impact, the research presented here is strictly foundational and only serves to
957"
BROADER IMPACTS,0.9114832535885168,"improve the production of physically realistic dynamics from data.
958"
BROADER IMPACTS,0.9122807017543859,"Guidelines:
959"
BROADER IMPACTS,0.9130781499202552,"• The answer NA means that there is no societal impact of the work performed.
960"
BROADER IMPACTS,0.9138755980861244,"• If the authors answer NA or No, they should explain why their work has no societal
961"
BROADER IMPACTS,0.9146730462519936,"impact or why the paper does not address societal impact.
962"
BROADER IMPACTS,0.9154704944178629,"• Examples of negative societal impacts include potential malicious or unintended uses
963"
BROADER IMPACTS,0.916267942583732,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
964"
BROADER IMPACTS,0.9170653907496013,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
965"
BROADER IMPACTS,0.9178628389154705,"groups), privacy considerations, and security considerations.
966"
BROADER IMPACTS,0.9186602870813397,"• The conference expects that many papers will be foundational research and not tied
967"
BROADER IMPACTS,0.919457735247209,"to particular applications, let alone deployments. However, if there is a direct path to
968"
BROADER IMPACTS,0.9202551834130781,"any negative applications, the authors should point it out. For example, it is legitimate
969"
BROADER IMPACTS,0.9210526315789473,"to point out that an improvement in the quality of generative models could be used to
970"
BROADER IMPACTS,0.9218500797448166,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
971"
BROADER IMPACTS,0.9226475279106858,"that a generic algorithm for optimizing neural networks could enable people to train
972"
BROADER IMPACTS,0.9234449760765551,"models that generate Deepfakes faster.
973"
BROADER IMPACTS,0.9242424242424242,"• The authors should consider possible harms that could arise when the technology is
974"
BROADER IMPACTS,0.9250398724082934,"being used as intended and functioning correctly, harms that could arise when the
975"
BROADER IMPACTS,0.9258373205741627,"technology is being used as intended but gives incorrect results, and harms following
976"
BROADER IMPACTS,0.9266347687400319,"from (intentional or unintentional) misuse of the technology.
977"
BROADER IMPACTS,0.9274322169059012,"• If there are negative societal impacts, the authors could also discuss possible mitigation
978"
BROADER IMPACTS,0.9282296650717703,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
979"
BROADER IMPACTS,0.9290271132376395,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
980"
BROADER IMPACTS,0.9298245614035088,"feedback over time, improving the efficiency and accessibility of ML).
981"
SAFEGUARDS,0.930622009569378,"11. Safeguards
982"
SAFEGUARDS,0.9314194577352473,"Question: Does the paper describe safeguards that have been put in place for responsible
983"
SAFEGUARDS,0.9322169059011164,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
984"
SAFEGUARDS,0.9330143540669856,"image generators, or scraped datasets)?
985"
SAFEGUARDS,0.9338118022328549,"Answer: [NA]
986"
SAFEGUARDS,0.9346092503987241,"Justification: N/A
987"
SAFEGUARDS,0.9354066985645934,"Guidelines:
988"
SAFEGUARDS,0.9362041467304625,"• The answer NA means that the paper poses no such risks.
989"
SAFEGUARDS,0.9370015948963317,"• Released models that have a high risk for misuse or dual-use should be released with
990"
SAFEGUARDS,0.937799043062201,"necessary safeguards to allow for controlled use of the model, for example by requiring
991"
SAFEGUARDS,0.9385964912280702,"that users adhere to usage guidelines or restrictions to access the model or implementing
992"
SAFEGUARDS,0.9393939393939394,"safety filters.
993"
SAFEGUARDS,0.9401913875598086,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
994"
SAFEGUARDS,0.9409888357256778,"should describe how they avoided releasing unsafe images.
995"
SAFEGUARDS,0.9417862838915471,"• We recognize that providing effective safeguards is challenging, and many papers do
996"
SAFEGUARDS,0.9425837320574163,"not require this, but we encourage authors to take this into account and make a best
997"
SAFEGUARDS,0.9433811802232854,"faith effort.
998"
LICENSES FOR EXISTING ASSETS,0.9441786283891547,"12. Licenses for existing assets
999"
LICENSES FOR EXISTING ASSETS,0.9449760765550239,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
1000"
LICENSES FOR EXISTING ASSETS,0.9457735247208932,"the paper, properly credited and are the license and terms of use explicitly mentioned and
1001"
LICENSES FOR EXISTING ASSETS,0.9465709728867624,"properly respected?
1002"
LICENSES FOR EXISTING ASSETS,0.9473684210526315,"Answer: [NA]
1003"
LICENSES FOR EXISTING ASSETS,0.9481658692185008,"Justification: N/A
1004"
LICENSES FOR EXISTING ASSETS,0.94896331738437,"Guidelines:
1005"
LICENSES FOR EXISTING ASSETS,0.9497607655502392,"• The answer NA means that the paper does not use existing assets.
1006"
LICENSES FOR EXISTING ASSETS,0.9505582137161085,"• The authors should cite the original paper that produced the code package or dataset.
1007"
LICENSES FOR EXISTING ASSETS,0.9513556618819776,"• The authors should state which version of the asset is used and, if possible, include a
1008"
LICENSES FOR EXISTING ASSETS,0.9521531100478469,"URL.
1009"
LICENSES FOR EXISTING ASSETS,0.9529505582137161,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
1010"
LICENSES FOR EXISTING ASSETS,0.9537480063795853,"• For scraped data from a particular source (e.g., website), the copyright and terms of
1011"
LICENSES FOR EXISTING ASSETS,0.9545454545454546,"service of that source should be provided.
1012"
LICENSES FOR EXISTING ASSETS,0.9553429027113237,"• If assets are released, the license, copyright information, and terms of use in the
1013"
LICENSES FOR EXISTING ASSETS,0.956140350877193,"package should be provided. For popular datasets, paperswithcode.com/datasets
1014"
LICENSES FOR EXISTING ASSETS,0.9569377990430622,"has curated licenses for some datasets. Their licensing guide can help determine the
1015"
LICENSES FOR EXISTING ASSETS,0.9577352472089314,"license of a dataset.
1016"
LICENSES FOR EXISTING ASSETS,0.9585326953748007,"• For existing datasets that are re-packaged, both the original license and the license of
1017"
LICENSES FOR EXISTING ASSETS,0.9593301435406698,"the derived asset (if it has changed) should be provided.
1018"
LICENSES FOR EXISTING ASSETS,0.960127591706539,"• If this information is not available online, the authors are encouraged to reach out to
1019"
LICENSES FOR EXISTING ASSETS,0.9609250398724083,"the asset’s creators.
1020"
NEW ASSETS,0.9617224880382775,"13. New Assets
1021"
NEW ASSETS,0.9625199362041468,"Question: Are new assets introduced in the paper well documented and is the documentation
1022"
NEW ASSETS,0.9633173843700159,"provided alongside the assets?
1023"
NEW ASSETS,0.9641148325358851,"Answer: [NA]
1024"
NEW ASSETS,0.9649122807017544,"Justification: N/A
1025"
NEW ASSETS,0.9657097288676236,"Guidelines:
1026"
NEW ASSETS,0.9665071770334929,"• The answer NA means that the paper does not release new assets.
1027"
NEW ASSETS,0.967304625199362,"• Researchers should communicate the details of the dataset/code/model as part of their
1028"
NEW ASSETS,0.9681020733652312,"submissions via structured templates. This includes details about training, license,
1029"
NEW ASSETS,0.9688995215311005,"limitations, etc.
1030"
NEW ASSETS,0.9696969696969697,"• The paper should discuss whether and how consent was obtained from people whose
1031"
NEW ASSETS,0.970494417862839,"asset is used.
1032"
NEW ASSETS,0.9712918660287081,"• At submission time, remember to anonymize your assets (if applicable). You can either
1033"
NEW ASSETS,0.9720893141945773,"create an anonymized URL or include an anonymized zip file.
1034"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728867623604466,"14. Crowdsourcing and Research with Human Subjects
1035"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9736842105263158,"Question: For crowdsourcing experiments and research with human subjects, does the paper
1036"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9744816586921851,"include the full text of instructions given to participants and screenshots, if applicable, as
1037"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752791068580542,"well as details about compensation (if any)?
1038"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9760765550239234,"Answer: [NA]
1039"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768740031897927,"Justification: N/A
1040"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776714513556619,"Guidelines:
1041"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9784688995215312,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1042"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792663476874003,"human subjects.
1043"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800637958532695,"• Including this information in the supplemental material is fine, but if the main contribu-
1044"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808612440191388,"tion of the paper involves human subjects, then as much detail as possible should be
1045"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981658692185008,"included in the main paper.
1046"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9824561403508771,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
1047"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832535885167464,"or other labor should be paid at least the minimum wage in the country of the data
1048"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840510366826156,"collector.
1049"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848484848484849,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
1050"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9856459330143541,"Subjects
1051"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864433811802232,"Question: Does the paper describe potential risks incurred by study participants, whether
1052"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9872408293460925,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
1053"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9880382775119617,"approvals (or an equivalent approval/review based on the requirements of your country or
1054"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.988835725677831,"institution) were obtained?
1055"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9896331738437002,"Answer: [NA]
1056"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9904306220095693,"Justification: N/A
1057"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912280701754386,"Guidelines:
1058"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920255183413078,"• The answer NA means that the paper does not involve crowdsourcing nor research with
1059"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.992822966507177,"human subjects.
1060"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9936204146730463,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
1061"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9944178628389154,"may be required for any human subjects research. If you obtained IRB approval, you
1062"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9952153110047847,"should clearly state this in the paper.
1063"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9960127591706539,"• We recognize that the procedures for this may vary significantly between institutions
1064"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968102073365231,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
1065"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9976076555023924,"guidelines for their institution.
1066"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9984051036682615,"• For initial submissions, do not include any information that would break anonymity (if
1067"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9992025518341308,"applicable), such as the institution conducting the review.
1068"
