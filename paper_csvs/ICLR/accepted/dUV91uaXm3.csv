Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004,"Recently over-smoothing phenomenon of Transformer-based models is observed
in both vision and language Ô¨Åelds. However, no existing work has delved deeper
to further investigate the main cause of this phenomenon. In this work, we make
the attempt to analyze the over-smoothing problem from the perspective of graph,
where such problem was Ô¨Årst discovered and explored. Intuitively, the self-attention
matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based
on the above connection, we provide some theoretical analysis and Ô¨Ånd that layer
normalization plays a key role in the over-smoothing issue of Transformer-based
models. SpeciÔ¨Åcally, if the standard deviation of layer normalization is sufÔ¨Åciently
large, the output of Transformer stacks will converge to a speciÔ¨Åc low-rank subspace
and result in over-smoothing. To alleviate the over-smoothing problem, we consider
hierarchical fusion strategies, which combine the representations from different
layers adaptively to make the output more diverse. Extensive experiment results on
various data sets illustrate the effect of our fusion method."
INTRODUCTION,0.008,"1
INTRODUCTION"
INTRODUCTION,0.012,"Over the past few years, Transformer (Vaswani et al., 2017) has been widely used in various natural
language processing (NLP) tasks, including text classiÔ¨Åcation (Wang et al., 2018a), text translation
(Ott et al., 2018), question answering (Rajpurkar et al., 2016; 2018) and text generation (Brown
et al., 2020). The recent application of Transformer in computer vision (CV) Ô¨Åeld also demonstrate
the potential capacity of Transformer architecture. For instance, Transformer variants have been
successfully used for image classiÔ¨Åcation (Dosovitskiy et al., 2021), object detection (Carion et al.,
2020) and semantic segmentation (Strudel et al., 2021). Three fundamental descendants from
Transformer include BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan
et al., 2020), which achieve state-of-the-art performance on a wide range of NLP tasks."
INTRODUCTION,0.016,"Recently, Dong et al. (2021) observes the ‚Äútoken uniformity‚Äù problem, which reduces the capacity
of Transformer-based architectures by making all token representations identical. They claim that
pure self-attention (SAN) modules cause token uniformity, but they do not discuss whether the token
uniformity problem still exists in Transformer blocks. On the other hand, Gong et al. (2021) observe
the ‚Äúover-smoothing‚Äù problem for ViT (Dosovitskiy et al., 2021), in that different input patches are
mapped to a similar latent representation. To prevent loss of information, they introduce additional
loss functions to encourage diversity and successfully improve model performance by suppressing
over-smoothing. Moreover, ‚Äúoverthinking‚Äù phenomenon, indicating that shallow representations
are better than deep representations, also be observed in (Zhou et al., 2020; Kaya et al., 2019). As
discussed in Section 3, this phenomenon has some inherent connection with over-smoothing. In this
paper, we use ‚Äúover-smoothing‚Äù to unify the above issues, and refer this as the phenomenon that the
model performance is deteriorated because different inputs are mapped to a similar representation."
INTRODUCTION,0.02,"As the over-smoothing problem is Ô¨Årst studied in the graph neural network (GNN) literature (Li et al.,
2018; Xu et al., 2018; Zhao & Akoglu, 2020), in this paper, we attempt to explore the cause of such"
INTRODUCTION,0.024,‚àóEqual contribution.
INTRODUCTION,0.028,Published as a conference paper at ICLR 2022
INTRODUCTION,0.032,"problem by building a relationship between Transformer blocks and graphs. SpeciÔ¨Åcally, we consider
the self-attention matrix as the normalized adjacency matrix of a weighted graph, whose nodes are
the tokens in a sentence. Furthermore, we consider the inherent connection between BERT and graph
convolutional networks (Kipf & Welling, 2017). Inspired by the over-smoothing problem in GNN,
we study over-smoothing in BERT from a theoretical view via matrix projection. As opposed to Dong
et al. (2021), where the authors claim that layer normalization is irrelevant to over-smoothing, we
Ô¨Ånd that layer normalization (Ba et al., 2016) plays an important role in over-smoothing. SpeciÔ¨Åcally,
we theoretically prove that, if the standard deviation in layer normalization is sufÔ¨Åciently large, the
outputs of the Transformer stacks will converge to a low-rank subspace, resulting in over-smoothing.
Empirically, we verify that the conditions hold for a certain number of samples for a pre-trained and
Ô¨Åne-tuned BERT model (Devlin et al., 2019), which is consistent with our above observations."
INTRODUCTION,0.036,"To alleviate the over-smoothing problem, we propose a hierarchical fusion strategy that adaptively
fuses representations from different layers. Three fusion approaches are used: (i) Concat Fusion,
(ii) Max Fusion, and (iii) Gate Fusion. The proposed method reduces the similarity between tokens
and outperforms BERT baseline on the GLUE (Wang et al., 2018a), SWAG (Zellers et al., 2018) and
SQuAD (Rajpurkar et al., 2016; 2018) data sets."
INTRODUCTION,0.04,"In summary, the contributions of this paper are as follows: (i) We develop the relationship between
self-attention and graph for a better understanding of over-smoothing in BERT. (ii) We provide
theoretical analysis on over-smoothing in the BERT model, and empirically verify the theoretical
results. (iii) We propose hierarchical fusion strategies that adaptively combine different layers to
alleviate over-smoothing. Extensive experimental results verify our methods‚Äô effectiveness."
RELATED WORK,0.044,"2
RELATED WORK"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.048,"2.1
TRANSFORMER BLOCK AND SELF-ATTENTION"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.052,"Transformer block is a basic component in Transformer model (Vaswani et al., 2017). Each Trans-
former block consists of a self-attention layer and a feed-forward layer. Let X ‚ààRn√ód be the input
to a Transformer block, where n is the number of input tokens and d is the embedding size. The
self-attention layer output can be written as:"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.056,"Attn(X) = X + h
X"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.06,"k=1
œÉ(XW Q
k (XW K
k )‚ä§)XW V
k W O‚ä§
k
= X + h
X"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.064,"k=1
ÀÜ
AkXW V O
k
,
(1)"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.068,"where h is the number of heads, œÉ is the softmax function, and W Q
k , W K
k , W V
k , W O
k ‚ààRd√ódh
(where dh = d/h is the dimension of a single-head output) are weight matrices for the query, key,
value, and output, respectively of the kth head. In particular, the self-attention matrix"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.072,"ÀÜ
A = œÉ(XW Q(XW K)‚ä§) = œÉ(QK‚ä§)
(2)"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.076,"in (1) plays a key role in the self-attention layer (Park et al., 2019; Gong et al., 2019; Kovaleva et al.,
2019). As in (Yun et al., 2020; Shi et al., 2021; Dong et al., 2021), we drop the scale product 1/‚àödh
to simplify analysis."
TRANSFORMER BLOCK AND SELF-ATTENTION,0.08,The feed-forward layer usually has two fully-connected (FC) layers with residual connection:
TRANSFORMER BLOCK AND SELF-ATTENTION,0.084,"FF(X) = Attn(X) + ReLU(Attn(X)W1 + b1)W2 + b2,"
TRANSFORMER BLOCK AND SELF-ATTENTION,0.088,"where W1 ‚ààRd√ódff, W2 ‚ààRdff√ód (dff is the size of the intermediate layer) are the weight matrices,
and b1, b2 are the biases. Two layer normalization (Ba et al., 2016) operations are performed after
the self-attention layer and fully-connected layer, respectively."
OVER-SMOOTHING,0.092,"2.2
OVER-SMOOTHING"
OVER-SMOOTHING,0.096,"In graph neural networks, over-smoothing refers to the problem that the performance deteriorates
as representations of all the nodes become similar (Li et al., 2018; Xu et al., 2018; Huang et al.,
2020). Its main cause is the stacked aggregation layer using the same adjacency matrix. Recently,
several approaches have been proposed to alleviate the over-smoothing problem. Xu et al. (2018)
propose a jumping knowledge network for better structure-aware representation, which Ô¨Çexibly"
OVER-SMOOTHING,0.1,Published as a conference paper at ICLR 2022
OVER-SMOOTHING,0.104,"(a) Token-wise cosine similarity.
(b) Token similarity and error.
(c) Hierarchical fusion."
OVER-SMOOTHING,0.108,Figure 1: Over-smoothing in BERT models.
OVER-SMOOTHING,0.112,"leverages different neighborhood ranges. ResGCN (Li et al., 2019) adapts the residual connection and
dilated convolution in the graph convolutional network (GCN), and successfully scales the GCN to
56 layers. Zhao & Akoglu (2020) propose PairNorm, a novel normalization layer, that prevents node
embeddings from becoming too similar. DropEdge (Rong et al., 2020; Huang et al., 2020) randomly
removes edges from the input graph at each training epoch, and reduces the effect of over-smoothing."
OVER-SMOOTHING,0.116,"Unlike graph neural networks, over-smoothing in Transformer-based architectures has not been
discussed in detail. Dong et al. (2021) introduce the ‚Äútoken-uniformity‚Äù problem for self-attention,
and show that skip connections and multi-layer perceptron can mitigate this problem. However, Gong
et al. (2021) still observe over-smoothing on the Vision Transformers (Dosovitskiy et al., 2021)."
OVER-SMOOTHING,0.12,"3
DOES OVER-SMOOTHING EXIST IN BERT?"
OVER-SMOOTHING,0.124,"In this section, we Ô¨Årst explore the existence of over-smoothing in BERT, by measuring the similarity
between tokens in each Transformer layer. SpeciÔ¨Åcally, we use the token-wise cosine similarity
(Gong et al., 2021) as our similarity measure:"
OVER-SMOOTHING,0.128,"CosSim =
1
n(n ‚àí1) X iÃ∏=j"
OVER-SMOOTHING,0.132,"h‚ä§
i hj
‚à•hi‚à•2‚à•hj‚à•2
,"
OVER-SMOOTHING,0.136,"where n is the number of tokens, hi and hj are two representations of different tokens, and ‚à•¬∑ ‚à•2 is
the Euclidean norm. Following Dong et al. (2021), we use WikiBio (Lebret et al., 2016) as input to
the following Transformer-based models Ô¨Åne-tuned on the SQuAD data set (Rajpurkar et al., 2018):
(i) BERT (Devlin et al., 2019), (ii) RoBERTa (Liu et al., 2019) and (iii) ALBERT (Lan et al., 2020).1
For comparison, all three models are stacked with 12 blocks. We calculate each CosSim for each data
sample and show the average and standard derivation of CosSim values over all WikiBio data."
OVER-SMOOTHING,0.14,"In the Ô¨Ågures, layer 0 represents original input token representation, and layer 1-12 represents the
corresponding transformer layers. As shown in Figure 1(a), the original token representations are
different from each other, while token similarities are high in the last layer. For instance, the average
token-wise cosine similarity of the last layer of ALBERT and RoBERTa are both larger than 90%."
OVER-SMOOTHING,0.144,"To illustrate the relationship between ‚Äúover-thinking‚Äù and ‚Äúover-smoothing‚Äù, we compare the token-
wise cosine similarity at each layer with the corresponding error rate. As for the corresponding error
rate of layer i, we use the representations from layer i as the Ô¨Ånal output and Ô¨Åne-tune the classiÔ¨Åer.
Following Zhou et al. (2020), we experiment with ALBERT (Lan et al., 2020) Ô¨Åne-tuned on the
MRPC data set (Dolan & Brockett, 2005) and use their error rate results for convenience. As shown
in Figure 1(b), layer 10 has the lowest cosine similarity and error rate. At layers 11 and 12, the tokens
have larger cosine similarities, making them harder to distinguish and resulting in the performance
drop. Thus, ‚Äúover-thinking‚Äù can be explained by ‚Äúover-smoothing‚Äù."
OVER-SMOOTHING,0.148,"A direct consequence of over-smoothing is that the performance cannot be improved when the model
gets deeper, since the individual tokens are no longer distinguishable. To illustrate this, we increase
the number of layers in BERT to 24 while keeping the other settings. As shown in Figure 1(c), the"
OVER-SMOOTHING,0.152,"1Our implementation is based on the HuggingFace‚Äôs Transformers library (Wolf et al., 2020)."
OVER-SMOOTHING,0.156,Published as a conference paper at ICLR 2022
OVER-SMOOTHING,0.16,"[CLS]
worth the"
OVER-SMOOTHING,0.164,effort
OVER-SMOOTHING,0.168,"to
watch . [SEP]"
OVER-SMOOTHING,0.172,"(a) Graph G.
(b) Adjacency matrix A.
(c) Normalized adjacency matrix ÀÜ
A."
OVER-SMOOTHING,0.176,"Figure 2: Illustration of self-attention and the corresponding graph G. For simplicity, we drop the
self-loops in G."
OVER-SMOOTHING,0.18,"performance of vanilla BERT cannot improve as the model gets deeper. In contrast, the proposed
hierarchical fusion (as will be discussed in Section 6) consistently outperforms the baseline, and has
better and better performance as the model gets deeper. Based on these observations, we conclude
that the over-smoothing problem still exists in BERT."
RELATIONSHIP BETWEEN SELF-ATTENTION AND GRAPH,0.184,"4
RELATIONSHIP BETWEEN SELF-ATTENTION AND GRAPH"
RELATIONSHIP BETWEEN SELF-ATTENTION AND GRAPH,0.188,"Since over-smoothing is Ô¨Årst discussed in the graph neural network literature (Li et al., 2018; Zhao &
Akoglu, 2020), we attempt to understand its cause from a graph perspective in this section."
SELF-ATTENTION VS RESGCN,0.192,"4.1
SELF-ATTENTION VS RESGCN"
SELF-ATTENTION VS RESGCN,0.196,"Given a Transformer block, construct a weighted graph G with the input tokens as nodes and
exp(Q‚ä§
i Kj) as the (i, j)th entry of its adjacency matrix A. By rewriting the self-attention matrix
ÀÜ
A in (2) as ÀÜAi,j = œÉ(QK‚ä§)i,j = exp(Q‚ä§
i Kj)/ P"
SELF-ATTENTION VS RESGCN,0.2,"l exp(Q‚ä§
i Kl), ÀÜ
A can thus be viewed as G‚Äôs
normalized adjacency matrix (Von Luxburg, 2007). In other words, ÀÜ
A = D‚àí1A, where D =
diag(d1, d2, . . . , dn) and di = P"
SELF-ATTENTION VS RESGCN,0.204,"j Ai,j. Figure 2 shows an example for the sentence ‚Äúworth the
effort to watch."" from the SST-2 data set (Socher et al., 2013) processed by BERT."
SELF-ATTENTION VS RESGCN,0.208,"Note that graph convolutional network combined with residual connections (ResGCN) (Kipf &
Welling, 2017) can be expressed as follows."
SELF-ATTENTION VS RESGCN,0.212,"ResGCN(X) = X + ReLU(D‚àí1/2AD‚àí1/2XW ) = X + ReLU( ÀÜ
AXW ),
(3)"
SELF-ATTENTION VS RESGCN,0.216,"which has the similar form with the self-attention layer in Eq. (1). By comparing self-attention module
with ResGCN, we have the following observations: (i) Since Ai,j Ã∏= Aj,i in general, G in self-attention
is a directed graph; (ii) ÀÜ
A = D‚àí1A in self-attention is the random walk normalization (Chung &
Graham, 1997), while GCN usually uses the symmetric normalization version ÀÜ
A = D‚àí1/2AD‚àí1/2;
(iii) The attention matrices constructed at different Transformer layers are different, while in typical
graphs, the adjacency matrices are usually static."
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.22,"4.2
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX"
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.224,"As discussed in Section 2.2, over-smoothing in graph neural networks is mainly due to the repeated
aggregation operations using the same adjacency matrix. To compare the self-attention matrices
( ÀÜ
A‚Äôs) at different Transformer layers, we Ô¨Årst Ô¨Çatten the multi-head attention and then measure the
cosine similarity between ÀÜ
A‚Äôs at successive layers. Experiment is performed with BERT (Devlin
et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2020) on the WikiBio data set
(Lebret et al., 2016)."
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.228,Published as a conference paper at ICLR 2022
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.232,"Table 1: Performance (%) on the GLUE development set by the original BERT (top row) and various
BERT variants with different degrees of self-attention matrix sharing. Numbers in parentheses are
the layers that share the self-attention matrix (e.g., BERT (1-12) means that the ÀÜ
A‚Äôs from layers 1-12
are shared). The last column shows the FLOPs in the self-attention modules."
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.236,"MNLI (m/mm)
QQP
QNLI
SST-2
COLA
STS-B
MRPC
RTE
Average
FLOPs
BERT
85.4/85.8
88.2
91.5
92.9
62.1
88.8
90.4
69.0
83.8
2.7G
BERT (11-12)
84.9/85.0
88.1
91.0
93.0
62.3
89.7
91.1
70.8
84.0
2.4G
BERT (9-12)
85.3/85.1
88.1
90.1
92.9
62.6
89.3
91.2
68.5
83.7
2.1G
BERT (7-12)
84.2/84.8
88.0
90.6
92.1
62.7
89.2
90.5
68.2
83.4
1.8G
BERT (5-12)
84.0/84.3
88.0
89.7
92.8
64.1
89.0
90.3
68.2
83.4
1.5G
BERT (3-12)
82.5/82.4
87.5
88.6
91.6
57.0
87.9
88.4
65.7
81.3
1.2G
BERT (1-12)
81.3/81.7
87.3
88.5
92.0
57.7
87.4
87.5
65.0
80.9
1.1G"
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.24,"Figure 3 shows the cosine similarities obtained. As can be seen, the similarities at the last few layers
are high,2 while those at the Ô¨Årst few layers are different from each other. In other words, the attention
patterns at the Ô¨Årst few layers are changing, and become stable at the upper layers."
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.244,"Figure 3: Consine similarity between the attention
matrices ÀÜ
A‚Äôs at layer i and its next higher layer."
UNSHARED ATTENTION MATRIX VS SHARED ATTENTION MATRIX,0.248,"In the following, we focus on BERT and ex-
plore how many layers can share the same self-
attention matrix. Note that this is different from
ALBERT, which shares model parameters in-
stead of attention matrices. Results are shown
in Table 1. As can be seen, sharing attention
matrices among the last 8 layers (i.e., layers 5-
12) does not harm model performance. This
is consistent with the observation in Figure 3.
Note that sharing attention matrices not only
reduces the number of parameters in the self-
attention module, but also makes the model
more efÔ¨Åcient by reducing the computations dur-
ing training and inference. As shown in Table 1,
BERT (5-12) reduces 44.4% FLOPs in the self-
attention modules compared with the vanilla
BERT, while still achieving comparable average
GLUE scores."
OVER-SMOOTHING IN BERT,0.252,"5
OVER-SMOOTHING IN BERT"
OVER-SMOOTHING IN BERT,0.256,"In this section, we analyze the over-smoothing problem in BERT theoretically, and then verify the
result empirically."
THEORETICAL ANALYSIS,0.26,"5.1
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.264,"Our analysis is based on matrix projection. We deÔ¨Åne a subspace M, in which each row vector of the
element in this subspace is identical."
THEORETICAL ANALYSIS,0.268,"DeÔ¨Ånition 1. DeÔ¨Åne M := {Y ‚ààRn√ód|Y = eC, C ‚ààR1√ód} as a subspace in Rn√ód, where
e = [1, 1, . . . , 1]‚ä§‚ààRn√ó1, n is the number of tokens and d is the dimension of token representation."
THEORETICAL ANALYSIS,0.272,"Each Y in subspace M suffers from the over-smoothing issue since the representation of each token
is C, which is the same with each other. We deÔ¨Åne the distance between matrix H ‚ààRn√ód and M
as dM(H) := minY ‚ààM ‚à•H ‚àíY ‚à•F , where ‚à•¬∑ ‚à•F is the Frobenius norm. Next, we investigate the
distance between the output of layer l and subspace M. We have the following Lemma."
THEORETICAL ANALYSIS,0.276,"2For example, in BERT, the attention matrices ÀÜ
A‚Äôs for the last 8 layers are very similar."
THEORETICAL ANALYSIS,0.28,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.284,"ùëØ!
ùëØ""
ùëØ# ‚Ä¶ ‚Ñ≥ ùëØ! ùëØ#"
THEORETICAL ANALYSIS,0.288,"Figure 4: The illustration of over-smoothing problem. Recursively, Hl will converge to subspace M
where representation of each token is identical."
THEORETICAL ANALYSIS,0.292,"Lemma 1. For self-attention matrix ÀÜ
A, any H, B ‚ààRn√ód and Œ±1, Œ±2 ‚â•0, we have:"
THEORETICAL ANALYSIS,0.296,"dM(HW ) ‚â§sdM(H),
(4)
dM(ReLU(H)) ‚â§dM(H),
(5)
dM(Œ±1H + Œ±2B) ‚â§Œ±1dM(H) + Œ±2dM(B),
(6)"
THEORETICAL ANALYSIS,0.3,"dM( ÀÜ
AH) ‚â§
p"
THEORETICAL ANALYSIS,0.304,"ŒªmaxdM(H),
(7)"
THEORETICAL ANALYSIS,0.308,"where Œªmax is the largest eigenvalue of ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A and s is the largest singular value of W ."
THEORETICAL ANALYSIS,0.312,"Using Lemma 1, we have the following Theorem.
Theorem 2. For a BERT block with h heads, we have"
THEORETICAL ANALYSIS,0.316,"dM(Hl+1) ‚â§vdM(Hl),
(8)"
THEORETICAL ANALYSIS,0.32,"where v = (1 + s2)(1 +
‚àö"
THEORETICAL ANALYSIS,0.324,"Œªhs)/(œÉ1œÉ2), s > 0 is the largest element of all singular values of all
Wl, Œª is the largest eigenvalue of all ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A for each self-attention matrix ÀÜ
A, and œÉ1, œÉ2
are the minimum standard deviation for two layer normalization operations."
THEORETICAL ANALYSIS,0.328,"Proof is in Appendix A. Theorem 2 shows that if v < 1 (i.e., œÉ1œÉ2 > (1+s2)(1+
‚àö"
THEORETICAL ANALYSIS,0.332,"Œªhs)), the output
of layer l + 1 will be closer to M than the output of layer l. An illustration of Theorem 2 is shown in
Figure 4. H0 is the graph corresponding to the input layer. Initially, the token representations are
very different (indicated by the different colors of the nodes). Recursively, Hl will converge towards
to subspace M if v < 1 and all representations are the same, resulting in over-smoothing."
THEORETICAL ANALYSIS,0.336,"Remark Though we only focus on the case v < 1, over-smoothing may still exist if v ‚â•1."
THEORETICAL ANALYSIS,0.34,"As can be seen, layer normalization plays an important role for the convergence rate v. Interestingly,
Dong et al. (2021) claim that layer normalization plays no roles for token uniformity, which seems
to conÔ¨Çict with the conclusion in Theorem 2. However, note that the matrix rank cannot indicate
similarity between tokens completely because matrix rank is discrete while similarity is continuous.
For instance, given two token embeddings hi and hj, the matrix [hi, hj]‚ä§has rank 2 only if hi Ã∏= hj."
THEORETICAL ANALYSIS,0.344,"In contrast, the consine similarity between tokens is
h‚ä§
i hj
‚à•hi‚à•2‚à•hj‚à•2 ."
THEORETICAL ANALYSIS,0.348,"As discussed in Section 4.1, GCN use the symmetric normalization version ÀÜ
A = D‚àí1/2AD‚àí1/2,
resulting in the target subspace M‚Ä≤ := {Y ‚ààRn√ód|Y = D1/2eC, C ‚ààR1√ód} is dependent with
adjacent matrix (Huang et al., 2020). In contrast, our subspace M is independent of ÀÜ
A thanks to its
random walk normalization. Thus, Theorem 2 can be applied to the vanilla BERT even though its
attention matrix ÀÜ
A is not similar."
EMPIRICAL VERIFICATION,0.352,"5.2
EMPIRICAL VERIFICATION"
EMPIRICAL VERIFICATION,0.356,"Theorem 2 illustrates that the magnitude of œÉ1œÉ2 is important for over-smoothing issue. If œÉ1œÉ2 >
(1+s2)(1+
‚àö"
EMPIRICAL VERIFICATION,0.36,"Œªhs), the output will be closer to subspace M suffered from over-smoothing. Since s is
usually small due to the ‚Ñì2-penalty during training (Huang et al., 2020), we neglect the effect of s and
compare œÉ1œÉ2 with 1 for simplicity. To verify the theoretical results, we visualize œÉ1œÉ2 in different
Ô¨Åne-tuned BERT models. SpeciÔ¨Åcally, we take the development set data of STS-B (Cer et al., 2017),"
EMPIRICAL VERIFICATION,0.364,Published as a conference paper at ICLR 2022
EMPIRICAL VERIFICATION,0.368,"(a) STS-B.
(b) CoLA.
(c) SQuAD."
EMPIRICAL VERIFICATION,0.372,Figure 5: The estimated distribution of œÉ1œÉ2 for different Ô¨Åne-tuned models.
EMPIRICAL VERIFICATION,0.376,"CoLA (Warstadt et al., 2019), SQuAD (Rajpurkar et al., 2016) as input to the Ô¨Åne-tuned models and
visualize the distribution of œÉ1œÉ2 at the last layer using kernel density estimation (Rosenblatt, 1956)."
EMPIRICAL VERIFICATION,0.38,"Results are shown in Figure 5. As can be seen, the distributions of œÉ1œÉ2 can be very different
across data sets. For STS-B (Cer et al., 2017), œÉ1œÉ2 of all data is larger than 1, which means that
over-smoothing is serious for this data set. For CoLA (Warstadt et al., 2019) and SQuAD (Rajpurkar
et al., 2016), there also exists a fraction of samples satisfying œÉ1œÉ2 > 1."
METHOD,0.384,"6
METHOD"
METHOD,0.388,"From our proof in Appendix A, we Ô¨Ågure out that the main reason is the post-normalization scheme
in BERT. In comparison, to train a 1000-layer GCN, Li et al. (2021) instead apply pre-normalization
with skip connections to ensure v > 1. However, the performance of pre-normalization is not better
than post-normalization for layer normalization empirically (He et al., 2021). In this section, we
preserve the post-normalization scheme and propose a hierarchical fusion strategy to alleviate the
over-smoothing issue. SpeciÔ¨Åcally, since only deep layers suffer from the over-smoothing issue, we
allow the model select representations from both shallow layers and deep layers as Ô¨Ånal output."
HIERARCHICAL FUSION STRATEGY,0.392,"6.1
HIERARCHICAL FUSION STRATEGY"
HIERARCHICAL FUSION STRATEGY,0.396,"Concat Fusion We Ô¨Årst consider a simple and direct layer-wise Concat Fusion approach. Considering
a L-layer model, we Ô¨Årst concatenate the representations Hk from each layer k to generate a matrix
[H1, H2, . . . , HL] and then apply a linear mapping to generate the Ô¨Ånal representation PL
k=1 Œ±kHk.
Here {Œ±k} are model parameters independent with inputs. Since this scheme requires preserving
feature maps from all layers, the memory cost will be huge as the model gets deep."
HIERARCHICAL FUSION STRATEGY,0.4,"Max Fusion Inspired by the idea of the widely adopted max-pooling mechanism, we construct the
Ô¨Ånal output by taking the maximum value across all layers for each dimension of the representation.
Max Fusion is an adaptive fusion mechanism since the model can dynamically decide the important
layer for each element in the representation. Max Fusion is the most Ô¨Çexible strategy, since it does
not require learning any additional parameters and is more efÔ¨Åcient in terms of speed and memory."
HIERARCHICAL FUSION STRATEGY,0.404,"Gate Fusion Gate mechanism is commonly used for information propagation in natural language
processing Ô¨Åeld (Cho et al., 2014). To exploit the advantages from different semantic levels, we
propose a vertical gate fusion module, which predicts the respective importance of token-wise
representations from different layers and aggregate them adaptively. Given token representations
{Ht
k}, where t denotes the token index and k denotes the layer index, the Ô¨Ånal representation for
token t is calculated by PL
k=1 It
k ¬∑Ht
k, where It
1, It
2, . . . , It
L = softmax(g(Ht
1), g(Ht
2), . . . , g(Ht
L)).
Here L is the number of layers and the gate function g(¬∑) is a fully-connected (FC) layer, which relies
on the word representation itself in respective layers to predict its importance scores. The weights of
the gate function g(¬∑) are shared across different layers."
HIERARCHICAL FUSION STRATEGY,0.408,"Even though Concat Fusion and Max Fusion have been investigated in the graph Ô¨Åeld (Xu et al.,
2018), their effectiveness for pre-trained language model have not yet been explored. Besides,
since the layer-wise Concat Fusion and element-wise Max Fusion lack the ability to generate token
representations according to each token‚Äôs speciÔ¨Åcity, we further propose the token-wise Gate Fusion
for adapting fusion to the language scenario."
HIERARCHICAL FUSION STRATEGY,0.412,Published as a conference paper at ICLR 2022
HIERARCHICAL FUSION STRATEGY,0.416,Table 2: Performance (in %) of the various BERT variants on the GLUE development data set.
HIERARCHICAL FUSION STRATEGY,0.42,"MNLI (m/mm)
QQP
QNLI
SST-2
COLA
STS-B
MRPC
RTE
Average
BERT
85.4/85.8
88.2
91.5
92.9
62.1
88.8
90.4
69.0
83.8
BERT (concat)
85.3/85.4
87.8
91.8
93.8
65.1
89.8
91.3
71.1
84.6
BERT (max)
85.3/85.6
88.5
92.0
93.7
64.6
90.3
91.7
71.5
84.7
BERT (gate)
85.4/85.7
88.4
92.3
93.9
64.0
90.3
92.0
73.9
85.1
ALBERT
81.6/82.2
85.6
90.7
90.3
50.8
89.4
91.3
75.5
81.8
ALBERT (concat)
82.8/82.8
86.7
90.9
90.7
48.7
89.7
91.5
76.5
82.3
ALBERT (max)
82.5/82.8
86.9
91.1
90.7
50.5
89.6
92.6
77.3
82.6
ALBERT (gate)
83.0/83.7
87.0
90.9
90.4
51.3
90.0
92.4
76.2
82.7"
EXPERIMENT RESULTS,0.424,"6.2
EXPERIMENT RESULTS"
EXPERIMENT RESULTS,0.428,"The BERT model is stacked with 12 Transformer blocks (Section 2.1) with the following hyper-
parameters: number of tokens n = 128, number of self-attention heads h = 12, and hidden layer size
d = 768. As for the feed-forward layer, we set the Ô¨Ålter size dff to 3072 as in Devlin et al. (2019).
All experiments are performed on NVIDIA Tesla V100 GPUs."
DATA AND SETTINGS,0.432,"6.2.1
DATA AND SETTINGS"
DATA AND SETTINGS,0.436,"Pre-training For the setting in pre-training phase, we mainly follows BERT paper (Devlin et al.,
2019). Our pre-training tasks are vanilla masked language modeling (MLM) and next sentence
prediction (NSP). The pre-training datasets are English BooksCorpus (Zhu et al., 2015) and Wikipedia
(Devlin et al., 2019) (16G in total). The WordPiece embedding (Wu et al., 2016) and the dictionary
containing 30, 000 tokens in (Devlin et al., 2019) are still used in our paper. To pre-process text, we
use the special token [CLS] as the Ô¨Årst token of each sequence and [SEP] to separate sentences in
a sequence. The pre-training is performed for 40 epochs."
DATA AND SETTINGS,0.44,"Fine-tuning In the Ô¨Åne-tuning phase, we perform downstream experiments on the GLUE (Wang
et al., 2018a), SWAG (Zellers et al., 2018) and SQuAD (Rajpurkar et al., 2016; 2018) benchmarks.
GLUE is a natural language understanding benchmark, which includes three categories tasks: (i)
single-sentence tasks (CoLA and SST-2); (ii) similarity and paraphrase tasks (MRPC, QQP and
STS-B); (iii) inference tasks (MNLI, QNLI and RTE). For MNLI task, we experiment on both the
matched (MNLI-m) and mismatched (MNLI-mm) versions. The SWAG data set is for grounded
commonsense inference, while SQuAD is a task for question answering. In SQuAD v1.1 (Rajpurkar
et al., 2016), the answers are included in the context. SQuAD v2.0 (Rajpurkar et al., 2018) is more
challenge than SQuAD v1.0, in which some answers are not included in the context. Following BERT
(Devlin et al., 2019), we report accuracy for MNLI, QNLI, RTE, SST-2 tasks, F1 score for QQP and
MRPC, Spearman correlation for STS-B, and Matthews correlation for CoLA. For SWAG task, we
use accuracy for evaluation. For SQuAD v1.1 and v2.0, we report the Exact Match (EM) and F1
scores. Descriptions of the data sets and details of other hyper-parameter settings are in Appendix B
and in Appendix C, respectively."
RESULTS AND ANALYSIS,0.444,"6.2.2
RESULTS AND ANALYSIS"
RESULTS AND ANALYSIS,0.448,"Table 3: Performance (in %) on the SWAG and
SQuAD development sets."
RESULTS AND ANALYSIS,0.452,"SWAG
SQuAD v1.1
SQuAD v2.0
acc
EM
F1
EM
F1
BERT
81.6
79.7
87.1
72.9
75.5
BERT (concat)
82.0
80.2
87.8
74.1
77.0
BERT (max)
81.9
80.1
87.6
73.6
76.6
BERT (gate)
82.1
80.7
88.0
73.9
77.3"
RESULTS AND ANALYSIS,0.456,"Since BERT (Devlin et al., 2019) and RoBERTa
(Liu et al., 2019) share the same architecture and
the only difference is data resource and training
steps, here we mainly evaluate our proposed
method on BERT and ALBERT (Lan et al.,
2020). Results on the GLUE benchmark are
shown in Table 2, while results on SWAG and
SQuAD are illustrated in Table 3. For SQuAD
task, in contrast to BERT which (Devlin et al.,
2019) utilize the augmented training data during Ô¨Åne-tuning phase, we only Ô¨Åne-tune our model on
the standard SQuAD data set. As can be seen, our proposed fusion strategies also perform better than
baselines on various tasks consistently."
RESULTS AND ANALYSIS,0.46,"Following the previous over-smoothing measure, we visualize the token-wise cosine similarity in
each layer. Here we perform visualization on the same data sets as Section 5.2 and the results are
shown in Figure 6. For all three data sets, the cosine similarity has a drop in the last layer compared"
RESULTS AND ANALYSIS,0.464,Published as a conference paper at ICLR 2022
RESULTS AND ANALYSIS,0.468,"(a) STS-B.
(b) CoLA.
(c) SQuAD."
RESULTS AND ANALYSIS,0.472,"Figure 6: The token-wise similarity comparison between BERT and BERT with gate fusion. Here F
means the Ô¨Ånal output, which is the fusion results for our approach."
RESULTS AND ANALYSIS,0.476,Figure 7: Visualization of importance weights of gate fusion on different layers.
RESULTS AND ANALYSIS,0.48,"with baseline. It‚Äôs remarkable that the similarity drop is the most obvious in STS-B (Cer et al., 2017),
which is consistent with our empirical veriÔ¨Åcation that STS-B‚Äôs œÉ1œÉ2 is the largest in Section 5.2.
Since the representation of tokens from prior layers is not similar with each other, our fusion method
alleviates the over-smoothing issue and improve the model performance at the same time."
RESULTS AND ANALYSIS,0.484,"To study the dynamic weights of fusion gate strategy, we visualize the importance weight It
k for each
token t and for each layer k. We randomly select three samples and the visualization results are
illustrated in Figure 7. Note that our gate strategy will reduce to vanilla model if representation from
the last layer is selected for each token. As can be seen, the weight distribution of different tokens
is adaptively decided, illustrating that the vanilla BERT stacks model is not the best choice for all
tokens. The keywords which highly affect meaning of sentences (i.e. ‚Äúwomen‚Äù, ‚Äúwater‚Äù, ‚ÄúÔ¨Åsh‚Äù) are
willing to obtain more semantic representations from the deep layer, while for some simple words
which appear frequently (i.e. ‚Äúa‚Äù, ‚Äúis‚Äù), the features in shallow layers are preferred."
CONCLUSION,0.488,"7
CONCLUSION"
CONCLUSION,0.492,"In this paper, we revisit the over-smoothing problem in BERT models. Since this issue has been
detailed discuss in graph learning Ô¨Åeld, we Ô¨Årstly establish the relationship between BERT and graph
for inspiration, and Ô¨Ånd out that self-attention matrix can be shared among last few blocks without
performance drop. Inspired by over-smoothing discussion in graph convolutional network, we provide
some theoretical analysis for BERT models and Ô¨Ågure out the importance of layer normalization.
SpeciÔ¨Åcally, if the standard derivation of layer normalization is sufÔ¨Åciently large, the output will
converge towards to a low-rank subspace. To alleviate the over-smoothing problem, we also propose
a hierarchical fusion strategy to combine representations from different layers adaptively. Extensive
experiment results on various data sets illustrate the effect of our fusion methods."
CONCLUSION,0.496,Published as a conference paper at ICLR 2022
REFERENCES,0.5,REFERENCES
REFERENCES,0.504,"J. Ba, J. Kiros, and G. Hinton. Layer normalization. Preprint arXiv:1607.06450, 2016."
REFERENCES,0.508,"L. Bentivogli, I. Dagan, D. Hoa, D. Giampiccolo, and B. Magnini. The Fifth PASCAL Recognizing
Textual Entailment Challenge. In TAC 2009 Workshop, 2009."
REFERENCES,0.512,"T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language Models are Few-Shot Learners. Preprint arXiv:2005.14165,
2020."
REFERENCES,0.516,"N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-End Object
Detection with Transformers. In European Conference on Computer Vision, 2020."
REFERENCES,0.52,"D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. SemEval-2017 Task 1: Semantic
Textual Similarity Multilingual and Crosslingual Focused Evaluation. In International Workshop
on Semantic Evaluation, 2017."
REFERENCES,0.524,"Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora Question Pairs. University of Waterloo, 2018."
REFERENCES,0.528,"K. Cho, B. van Merri√´nboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio.
Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation.
In Empirical Methods in Natural Language Processing, 2014."
REFERENCES,0.532,"F. Chung and F. Graham. Spectral Graph Theory. American Mathematical Soc., 1997."
REFERENCES,0.536,"J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transform-
ers for Language Understanding. In North American Chapter of the Association for Computational
Linguistics, 2019."
REFERENCES,0.54,"W. Dolan and C. Brockett. Automatically Constructing a Corpus of Sentential Paraphrases. In
International Workshop on Paraphrasing, 2005."
REFERENCES,0.544,"Y. Dong, J. Cordonnier, and A. Loukas. Attention is Not All You Need: Pure Attention Loses Rank
Doubly Exponentially with Depth. In International Conference on Machine Learning, 2021."
REFERENCES,0.548,"A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, et al. An Image is Worth 16x16 Words: Transformers for
Image Recognition at Scale. In International Conference on Learning Representations, 2021."
REFERENCES,0.552,"F. Gantmakher. The Theory of Matrices, Volume 2. American Mathematical Soc., 2000."
REFERENCES,0.556,"C. Gong, D. Wang, M. Li, V. Chandra, and Q. Liu. Improve Vision Transformers Training by
Suppressing Over-smoothing. Preprint arXiv:2104.12753, 2021."
REFERENCES,0.56,"L. Gong, D. He, Z. Li, T. Qin, L. Wang, and T. Liu. EfÔ¨Åcient Training of BERT by Progressively
Stacking. In International Conference on Machine Learning, 2019."
REFERENCES,0.564,"R. He, A. Ravula, B. Kanagal, and J. Ainslie. Realformer: Transformer likes residual attention. In
Findings of Annual Meeting of the Association for Computational Linguistics, 2021."
REFERENCES,0.568,"W. Huang, Y. Rong, T. Xu, F. Sun, and J. Huang. Tackling Over-Smoothing for General Graph
Convolutional Networks. Preprint arXiv:2008.09864, 2020."
REFERENCES,0.572,"Y. Kaya, S. Hong, and T. Dumitras. Shallow-Deep Networks: Understanding and Mitigating Network
Overthinking. In International Conference on Machine Learning, 2019."
REFERENCES,0.576,"T. Kipf and M. Welling. Semi-Supervised ClassiÔ¨Åcation with Graph Convolutional Networks. In
International Conference on Learning Representations, 2017."
REFERENCES,0.58,"O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky. Revealing the Dark Secrets of BERT. In
Empirical Methods in Natural Language Processing, 2019."
REFERENCES,0.584,"Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. ALBERT: A Lite BERT for
Self-supervised Learning of Language Representations. In International Conference on Learning
Representations, 2020."
REFERENCES,0.588,Published as a conference paper at ICLR 2022
REFERENCES,0.592,"R. Lebret, D. Grangier, and M. Auli. Neural Text Generation from Structured Data with Application
to the Biography Domain. In Empirical Methods in Natural Language Processing, 2016."
REFERENCES,0.596,"G. Li, M. M√ºller, A. Thabet, and B. Ghanem. DeepGCNs: Can GCNs Go as Deep as CNNs? In
International Conference on Computer Vision, 2019."
REFERENCES,0.6,"G. Li, M. M√ºller, B. Ghanem, and V. Koltun. Training Graph Neural Networks with 1000 Layers. In
International Conference on Machine Learning, 2021."
REFERENCES,0.604,"Q. Li, Z. Han, and X. Wu. Deeper Insights into Graph Convolutional Networks for Semi-Supervised
Learning. In AAAI conference on artiÔ¨Åcial intelligence, 2018."
REFERENCES,0.608,"Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoy-
anov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Preprint arXiv:1907.11692,
2019."
REFERENCES,0.612,"K. Oono and T. Suzuki. Graph Neural Networks Exponentially Lose Expressive Power for Node
ClassiÔ¨Åcation. In International Conference on Learning Representations, 2020."
REFERENCES,0.616,"M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling Neural Machine Translation. In Machine
Translation, 2018."
REFERENCES,0.62,"C. Park, I. Na, Y. Jo, S. Shin, J. Yoo, B. Kwon, J. Zhao, H. Noh, Y. Lee, and J. Choo. SANVis: Visual
Analytics for Understanding Self-Attention Networks. In IEEE Visualization Conference, 2019."
REFERENCES,0.624,"P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ Questions for Machine
Comprehension of Text. In Empirical Methods in Natural Language Processing, 2016."
REFERENCES,0.628,"P. Rajpurkar, R. Jia, and P. Liang. Know What You Don‚Äôt Know: Unanswerable Questions for
SQuAD. In Annual Meeting of the Association for Computational Linguistics, 2018."
REFERENCES,0.632,"Y. Rong, W. Huang, T. Xu, and J. Huang. DropEdge: Towards Deep Graph Convolutional Networks
on Node ClassiÔ¨Åcation. In International Conference on Learning Representations, 2020."
REFERENCES,0.636,"M. Rosenblatt. Remarks on Some Nonparametric Estimates of a Density Function. Annals of
Mathematical Statistics, 1956."
REFERENCES,0.64,"H. Shi, J. Gao, X. Ren, H. Xu, X. Liang, Z. Li, and J. Kwok. SparseBERT: Rethinking the Importance
Analysis in Self-attention. In International Conference on Machine Learning, 2021."
REFERENCES,0.644,"R. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning, A. Ng, and C. Potts. Recursive Deep Models
for Semantic Compositionality Over a Sentiment Treebank. In Empirical Methods in Natural
Language Processing, 2013."
REFERENCES,0.648,"R. Strudel, R. Garcia, I. Laptev, and C. Schmid. Segmenter: Transformer for Semantic Segmentation.
In International Conference on Computer Vision, 2021."
REFERENCES,0.652,"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, ≈Å. Kaiser, and I. Polosukhin.
Attention Is All You Need. In Neural Information Processing Systems, 2017."
REFERENCES,0.656,"U. Von Luxburg. A Tutorial on Spectral Clustering. Statistics and computing, 2007."
REFERENCES,0.66,"A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. GLUE: A Multi-Task Benchmark
and Analysis Platform for Natural Language Understanding. In EMNLP Workshop BlackboxNLP,
2018a."
REFERENCES,0.664,"W. Wang, M. Yan, and C. Wu. Multi-Granularity Hierarchical Attention Fusion Networks for Reading
Comprehension and Question Answering. In Annual Meeting of the Association for Computational
Linguistics, 2018b."
REFERENCES,0.668,"A. Warstadt, A. Singh, and S. Bowman. Neural Network Acceptability Judgments. Transactions of
the Association for Computational Linguistics, 2019."
REFERENCES,0.672,Published as a conference paper at ICLR 2022
REFERENCES,0.676,"A. Williams, N. Nangia, and S. Bowman. A Broad-Coverage Challenge Corpus for Sentence
Understanding through Inference. In North American Chapter of the Association for Computational
Linguistics, 2018."
REFERENCES,0.68,"T. Wolf, J. Chaumond, L. Debut, V. Sanh, C. Delangue, A. Moi, P. Cistac, M. Funtowicz, J. Davison,
S. Shleifer, et al. Transformers: State-of-the-Art Natural Language Processing. In Empirical
Methods in Natural Language Processing: System Demonstrations, 2020."
REFERENCES,0.684,"Y. Wu, M. Schuster, Z. Chen, Q. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, et al. Google‚Äôs Neural Machine Translation System: Bridging the Gap between
Human and Machine Translation. Preprint arXiv:1609.08144, 2016."
REFERENCES,0.688,"K. Xu, C. Li, Y. Tian, T. Sonobe, K. Kawarab, and S. Jegelka. Representation Learning on Graphs
with Jumping Knowledge Networks. In International Conference on Machine Learning, 2018."
REFERENCES,0.692,"C. Yun, Y. Chang, S. Bhojanapalli, A. Rawat, S. Reddi, and S. Kumar. O(n) Connections are
Expressive Enough: Universal Approximability of Sparse Transformers. In Neural Information
Processing Systems, 2020."
REFERENCES,0.696,"R. Zellers, Y. Bisk, R. Schwartz, and Y. Choi. SWAG: A Large-Scale Adversarial Dataset for
Grounded Commonsense Inference. In Empirical Methods in Natural Language Processing, 2018."
REFERENCES,0.7,"L. Zhao and L. Akoglu. PairNorm: Tackling Oversmoothing in GNNs. In International Conference
on Learning Representations, 2020."
REFERENCES,0.704,"W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei. BERT Loses Patience: Fast and Robust
Inference with Early Exit. In Neural Information Processing Systems, 2020."
REFERENCES,0.708,"Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning Books
and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books. In
International Conference on Computer Vision, 2015."
REFERENCES,0.712,Published as a conference paper at ICLR 2022
REFERENCES,0.716,"A
PROOF"
REFERENCES,0.72,"Lemma 1. For self-attention matrix ÀÜ
A, any H, B ‚ààRn√ód and Œ±1, Œ±2 ‚â•0, we have:
dM(HW ) ‚â§sdM(H),
(4)
dM(ReLU(H)) ‚â§dM(H),
(5)
dM(Œ±1H + Œ±2B) ‚â§Œ±1dM(H) + Œ±2dM(B),
(6)"
REFERENCES,0.724,"dM( ÀÜ
AH) ‚â§
p"
REFERENCES,0.728,"ŒªmaxdM(H),
(7)"
REFERENCES,0.732,"where Œªmax is the largest eigenvalue of ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A and s is the largest singular value of W ."
REFERENCES,0.736,"Proof. Here we only prove the last inequality (7), as the inequity is different from the theories in
GCN since ÀÜ
A is not symmetric and shared in Transformer architecture. For the Ô¨Årst three inequalities,
we refer to Oono & Suzuki (2020) and Huang et al. (2020)."
REFERENCES,0.74,"Write HH‚ä§= Q‚Ñ¶Q‚ä§for the eigin-decomposition of HH‚ä§, where Q = [q1, q2, . . . , qn] is the
orthogonal and ‚Ñ¶= diag(œâ1, . . . , œân) with all œâi ‚â•0. Recall e = n‚àí1/2[1, 1, . . . , 1]‚ä§‚ààRn√ó1."
REFERENCES,0.744,"Note that
dM( ÀÜ
AH)2 = ‚à•(I ‚àíee‚ä§) ÀÜ
AH‚à•2
F
= tr{(I ‚àíee‚ä§) ÀÜ
AHH‚ä§ÀÜ
A‚ä§(I ‚àíee‚ä§)} = n
X"
REFERENCES,0.748,"i=1
œâiq‚ä§
i ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
Aqi."
REFERENCES,0.752,"Since matrix ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A is positive semideÔ¨Ånite, its all eigenvalues are non-negative. Let Œªmax
be the largest eigenvalue of ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A. Consider"
REFERENCES,0.756,"ŒªmaxdM(H)2 ‚àídM( ÀÜ
AH)2 = n
X"
REFERENCES,0.76,"i=1
œâiq‚ä§
i {Œªmax(I ‚àíee‚ä§) ‚àíÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A}qi."
REFERENCES,0.764,"Let Œ£ = Œªmax(I ‚àíee‚ä§) ‚àíÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A."
REFERENCES,0.768,"Note that ÀÜ
A = D‚àí1A is a stochastic matrix, we have ÀÜ
Ae = e. Thus, ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A has an
eigenvalue 0 and corresponding eigenvecter e. Let fi be a normalised eigenvector of ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A
orthogonal to e, and Œª be its corresponding eigenvalue. Then we have
e‚ä§Œ£e = 0,"
REFERENCES,0.772,"f ‚ä§
i Œ£fi = Œªmax ‚àíŒª ‚â•0."
REFERENCES,0.776,"It follows that dM( ÀÜ
AH)2 ‚â§ŒªmaxdM(H)2."
REFERENCES,0.78,"Discussion Assume further that ÀÜ
A is doubly stochastic (so that ÀÜ
A‚ä§e = e) with positive entries.
Then by Perron‚ÄìFrobenius theorem (Gantmakher, 2000), ÀÜ
A‚ä§ÀÜ
A has a maximum eigenvalue 1 with
associated eigenvector e as well. In this case, the matrix ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A = ÀÜ
A‚ä§ÀÜ
A ‚àíee‚ä§has a
maximum eigenvalue Œªmax < 1.
Theorem 2. For a BERT block with h heads, we have
dM(Hl+1) ‚â§vdM(Hl),
(8)"
REFERENCES,0.784,"where v = (1 + s2)(1 +
‚àö"
REFERENCES,0.788,"Œªhs)/(œÉ1œÉ2), s > 0 is the largest element of all singular values of all
Wl, Œª is the largest eigenvalue of all ÀÜ
A‚ä§(I ‚àíee‚ä§) ÀÜ
A for each self-attention matrix ÀÜ
A, and œÉ1, œÉ2
are the minimum standard deviation for two layer normalization operations."
REFERENCES,0.792,"Proof. From the deÔ¨Ånition of self-attention and feed-forward modules, we have"
REFERENCES,0.796,"Attn(X) = LayerNorm(X + H
X"
REFERENCES,0.8,"k=1
ÀÜ
AkXW k + 1b‚ä§) = (X + H
X"
REFERENCES,0.804,"k=1
ÀÜ
AkXW k + 1b‚ä§‚àí1b‚ä§
LN)D‚àí1
LN"
REFERENCES,0.808,"FF(X) = LayerNorm(X + ReLU(XW1 + 1b‚ä§
1 )W2 + 1b‚ä§
2 )"
REFERENCES,0.812,"= (X + ReLU(XW1 + 1b‚ä§
1 )W2 + 1b‚ä§
2 ‚àí1b‚ä§
LN)D‚àí1
LN"
REFERENCES,0.816,Published as a conference paper at ICLR 2022
REFERENCES,0.82,"Based on the Lemma 1, we have"
REFERENCES,0.824,"dM(Attn(X)) = dM((X + h
X"
REFERENCES,0.828,"k=1
ÀÜ
AkXW k + 1b‚ä§‚àí1b‚ä§
LN)D‚àí1
LN)"
REFERENCES,0.832,"‚â§dM(XD‚àí1
LN) + dM( h
X"
REFERENCES,0.836,"k=1
ÀÜ
AkXW kD‚àí1
LN) + dM(1(b ‚àíbLN)‚ä§)"
REFERENCES,0.84,"‚â§œÉ‚àí1
1 dM(X) + h
X"
REFERENCES,0.844,"k=1
dM( ÀÜ
AkXW kD‚àí1
LN)"
REFERENCES,0.848,"‚â§œÉ‚àí1
1 dM(X) +
‚àö"
REFERENCES,0.852,"ŒªhsœÉ‚àí1
1 dM(X)"
REFERENCES,0.856,"= (1 +
‚àö"
REFERENCES,0.86,"Œªhs)œÉ‚àí1
1 dM(X)."
REFERENCES,0.864,"dM(FF(X)) = dM((X + ReLU(XW1 + 1b‚ä§
1 )W2 + 1b‚ä§
2 ‚àí1b‚ä§
LN)D‚àí1
LN)"
REFERENCES,0.868,"‚â§dM(XD‚àí1
LN) + dM(ReLU(XW1 + 1b‚ä§
1 )W2D‚àí1
LN) + dM(1(b‚ä§
2 ‚àíb‚ä§
LN)D‚àí1
LN)"
REFERENCES,0.872,"‚â§dM(XD‚àí1
LN) + dM(XW1W2D‚àí1
LN) + dM(1b‚ä§
1 W2D‚àí1
LN)"
REFERENCES,0.876,"‚â§œÉ‚àí1
2 dM(X) + s2œÉ‚àí1
2 dM(X)"
REFERENCES,0.88,"= (1 + s2)œÉ‚àí1
2 dM(X).
It follows that
dM(Hl+1) ‚â§(1 + s2)(1 +
‚àö"
REFERENCES,0.884,"Œªhs)œÉ‚àí1
1 œÉ‚àí1
2 dM(Hl)."
REFERENCES,0.888,"B
DATA SET"
REFERENCES,0.892,"B.1
MNLI"
REFERENCES,0.896,"The Multi-Genre Natural Language Inference (Williams et al., 2018) is a crowdsourced ternary
classiÔ¨Åcation task. Given a premise sentence and a hypothesis sentence, the target is to predict
whether the last sentence is an [entailment], [contradiction], or [neutral] relationships with respect to
the Ô¨Årst one."
REFERENCES,0.9,"B.2
QQP"
REFERENCES,0.904,"The Quora Question Pairs (Chen et al., 2018) is a binary classiÔ¨Åcation task. Given two questions on
Quora, the target is to determine whether these two asked questions are semantically equivalent or
not."
REFERENCES,0.908,"B.3
QNLI"
REFERENCES,0.912,"The Question Natural Language Inference (Wang et al., 2018b) is a binary classiÔ¨Åcation task derived
from the Stanford Question Answering Dataset (Rajpurkar et al., 2016). Given sentence pairs
(question, sentence), the target is to predict whether the last sentence contains the correct answer to
the question."
REFERENCES,0.916,"B.4
SST-2"
REFERENCES,0.92,"The Stanford Sentiment Treebank (Socher et al., 2013) is a binary sentiment classiÔ¨Åcation task for
a single sentence. All sentences are extracted from movie reviews with human annotations of their
sentiment."
REFERENCES,0.924,"B.5
COLA"
REFERENCES,0.928,"The Corpus of Linguistic Acceptability (Warstadt et al., 2019) is a binary classiÔ¨Åcation task consisting
of English acceptability judgments extracted from books and journal articles. Given a single sentence,
the target is to determine whether the sentence is linguistically acceptable or not."
REFERENCES,0.932,Published as a conference paper at ICLR 2022
REFERENCES,0.936,"B.6
STS-B"
REFERENCES,0.94,"The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a regression task for predicting the
similarity score (from 1 to 5) between a given sentence pair, whose sentence pairs are drawn from
news headlines and other sources."
REFERENCES,0.944,"B.7
MRPC"
REFERENCES,0.948,"The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a binary classiÔ¨Åcation task.
Given a sentence pair extracted from online news sources, the target is to determine whether the
sentences in the pair are semantically equivalent."
REFERENCES,0.952,"B.8
RTE"
REFERENCES,0.956,"The Recognizing Textual Entailment (Bentivogli et al., 2009) is a binary entailment classiÔ¨Åcation task
similar to MNLI, where [neutral] and [contradiction] relationships are classiÔ¨Åed into [not entailment]."
REFERENCES,0.96,"B.9
SWAG"
REFERENCES,0.964,"The Situations with Adversarial Generations (Zellers et al., 2018) is a multiple-choice task consisting
of 113K questions about grounded situations. Given a source sentence, the task is to select the most
possible one among four choices for sentence continuity."
REFERENCES,0.968,"B.10
SQUAD V1.1"
REFERENCES,0.972,"The Stanford Question Answering Dataset (SQuAD v1.1) (Rajpurkar et al., 2016) is a large-scale
question and answer task consisting of 100K question and answer pairs from more than 500 articles.
Given a passage and the question from Wikipedia, the goal is to determine the start and the end token
of the answer text."
REFERENCES,0.976,"B.11
SQUAD V2.0"
REFERENCES,0.98,"The SQuAD v2.0 task (Rajpurkar et al., 2018) is the extension of above SQuAD v1.1, which contains
the 100K questions in SQuAD v1.1 and 50K unanswerable questions. The existence of unanswerable
question makes this task more realistic and challenging."
REFERENCES,0.984,"C
IMPLEMENTATION DETAILS"
REFERENCES,0.988,The hyper-parameters of various downstream tasks are shown in Table 4.
REFERENCES,0.992,Table 4: Hyper-parameters for different downstream tasks.
REFERENCES,0.996,"GLUE
SWAG
SQuAD v1.1
SQuAD v2.0
Batch size
32
16
32
48
Weight decay
[0.1, 0.01]
[0.1, 0.01]
[0.1, 0.01]
[0.1, 0.01]
Warmup proportion
0.1
0.1
0.1
0.1
Learning rate decay
linear
linear
linear
linear
Training Epochs
3
3
3
2
Learning rate
[2e-5, 1e-5, 1.5e-5, 3e-5, 4e-5, 5e-5]"
