Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.005714285714285714,"Self-supervised learning on graphs aims to learn graph representations in an unsu-
pervised manner. While graph contrastive learning (GCL - relying on graph
augmentation for creating perturbation views of anchor graphs and maximiz-
ing/minimizing similarity for positive/negative pairs) is a popular self-supervised
method, it faces challenges in finding label-invariant augmented graphs and deter-
mining the exact extent of similarity between sample pairs to be achieved. In this
work, we propose an alternative self-supervised solution that (i) goes beyond the
label invariance assumption without distinguishing between positive/negative sam-
ples, (ii) can calibrate the encoder for preserving not only the structural information
inside the graph, but the matching information between different graphs, (iii) learns
isometric embeddings that preserve the distance between graphs, a by-product
of our objective. Motivated by optimal transport theory, this scheme relies on an
observation that the optimal transport plans between node representations at the
output space, which measure the matching probability between two distributions,
should be consistent with the plans between the corresponding graphs at the input
space. The experimental findings include: (i) The plan alignment strategy signifi-
cantly outperforms the counterpart using the transport distance; (ii) The proposed
model shows superior performance using only node attributes as calibration signals,
without relying on edge information; (iii) Our model maintains robust results even
under high perturbation rates; (iv) Extensive experiments on various benchmarks
validate the effectiveness of the proposed method."
INTRODUCTION,0.011428571428571429,"1
Introduction"
INTRODUCTION,0.017142857142857144,"Self-supervised graph learning involves learning representations of real-world graph data without
the need for human supervision. Graph contrastive learning (GCL) [6, 16, 71] has been identified
as one of the most successful graph self-supervised learning approaches, with its key components
consisting of graph augmentation and contrastive learning. The former creates perturbation views
of anchor graphs via various augmenting techniques, while the latter maximizes the similarity for
two augmentations (positive pairs) of the same anchor and minimizes the similarity for those of
two different anchors (negative pairs). The effectiveness of contrastive learning is dependent on
the assumption that the augmented operations preserve the nature of data points and ensure that the
augmented samples have consistent labels with the original ones."
INTRODUCTION,0.022857142857142857,† Corresponding author.
INTRODUCTION,0.02857142857142857,"However, graph data structures are discrete and their properties may vary significantly even with
slight perturbations, which makes it much more challenging to design reasonable augmentations that
guarantee the label-invariant assumption for graphs, in contrast to images or text. Additionally, the
concept of “maximum similarity” in contrastive learning is difficult to measure since it is vague and
lacks a clear indication of how much similarity should be maximized (or minimized) for a given pair
of positive (or negative) views."
INTRODUCTION,0.03428571428571429,"To address these challenges, an intuitive solution is to introduce the concept of distance from the input
space, whereby the distance between the learned embeddings is forced to be equal to the distance
between the corresponding input graphs. However, this requirement is challenging to achieve as
the input objects (graphs) and output representations (vectors) are two distinct concepts, making it
difficult to agree on their distance metrics. For instance, comparing the graph edit distance between
graphs [15] and the Euclidean distance between vectors directly is inconceivable."
INTRODUCTION,0.04,"Figure 1: An illustra-
tion of discrete opti-
mal transport plan."
INTRODUCTION,0.045714285714285714,"In response to the aforementioned challenges, in this paper, we propose a
novel self-supervised learning method that seeks to align optimal transport
plans [59] from graph space to node representation space, instead of transport
distance [44]. This method is referred to as Graph Transport Learning (GTL),
and it exploits the key concept in optimal transport (OT) theory, which
aims to identify an optimal match (i.e., transport plan) between two data
distributions (e.g., node sets of graphs) that minimizes the matching cost
(i.e., transport distance). As shown in Figure 1, the transportation plan π
explicitly determines how to match particles from the source distribution
µ to the target distribution ν. Our approach involves several key steps.
First, we obtain two graph views by augmenting a graph and generating
node embeddings using a backbone model, such as Graph Neural Network
(GNNs) [25]. Two optimal transport plans can be computed from the graph
space and the representation space, respectively. To accurately capture the
matching relationships in the original graph space, we enforce the backbone
to learn representations that exhibit consistency with the optimal transport
plans between the corresponding source graphs in the input space. This is
achieved by minimizing the discrepancy between the two plans. Unlike distance-based approaches,
the transport plan alignment shows several advantages: (i) Direct comparability: Plans with the same
dimension can be compared directly, regardless of differences between the graph and representation
spaces. The value of the transport plan is dimensionless, representing the joint probability of two
particles; (ii) Accurate match relationship: For discrete objects, the transport plan retains more
accurate matching relation between data compared to distance; (iii) Label-variant: Importantly, our
self-supervised model does not require differentiating between positive and negative samples after
graph augmentation. This is due to the availability of corrective information from the input space,
which eliminates the need for a label-invariant assumption. Notably, our experimental findings in
Section 7 demonstrate that our model maintains robust performance even under high perturbation
rates, such as when 80% of both edges and node attributes are destroyed during graph augmentation.
In summary, we make the following contributions:
• We propose a novel paradigm for self-supervised graph learning based on optimal plan alignment,
GALOPA, which offers a distinct objective compared to contrastive learning methods. This approach
eliminates the need for a label-invariant assumption.
• By constraining the discrepancy between the transport plans, we introduce a new loss to enable the
sharing of the exact matching information from the graphs space to the representation space.
• Multiple comprehensive experiments, including distance v.s plan, node feature v.s edge, robustness
test and comparison with state-of-the-art methods demonstrate remarkable performance of GALOPA."
RELATED WORK,0.05142857142857143,"2
Related Work"
RELATED WORK,0.05714285714285714,"Self-supervised Learning on Graphs. Graph self-supervised learning has been a promising paradigm
for learning useful representations of unlabeled graph data. The node embedding methods aim to
learn a low-dimensional vector embedding of vertex preserving different kinds of order proximity via
factorization [3], random walk [17] or deep learning [8, 61]. Recently, graph contrastive learning
has achieved unprecedented success [6, 28, 55, 64], where the goal is to ensure representations
have high similarity between positive views of a graph and high dissimilarity between negative
views. A common way of contrastive objective is contrasting views at the node level. For the"
RELATED WORK,0.06285714285714286,"representations (positive views) h1
i and h2
i of same node i in two augmented graphs G1 and G2, the
pairwise contrastive loss can be defined as"
RELATED WORK,0.06857142857142857,"Lcontrast =
X i
log"
RELATED WORK,0.07428571428571429,"eD(h1
i ,h2
i )/τ"
RELATED WORK,0.08,"eD(h1
i ,h2
i )/τ + P"
RELATED WORK,0.08571428571428572,"k̸=i eD(h1
i ,h2
k)/τ ! (1)"
RELATED WORK,0.09142857142857143,"where D is a discriminator that maps two views to an agreement (similarity) score, such as the
inner product. τ denotes the temperature. Obviously, this type of loss strongly relies on the label
invariance assumption. In other words, it needs to know (or assume) beforehand that the two views are
positive/negative samples, which is challenging for discrete graph structures. For example, there exist
some graphs, such as molecular graphs, whose labels are very sensitive to perturbation/corruption.
Some studies have explored the label-invariant augmentation [29, 31, 74], but such augmentations
require very careful design and adjustment, and sometimes limit the power of graph augmentation.
More recently, [22] utilizes the graph edit distance to train graph discriminator to predict whether a
graph is an original graph or a perturbed one. However, this model still requires positive and negative
samples and relies on the assumption of label invariance."
RELATED WORK,0.09714285714285714,"Optimal Transport. Optimal transport (OT) [59] is a mathematical tool for aligning probability
distributions has received much attention in the machine learning community in many applications,
e.g., computer vision [4, 51], generative adversarial network [2, 7, 30], domain adaptation [18]. OT
aims to derive a transport plan between a source and a target distribution, such that the transport cost
is minimized. [34] proposes the Gromov-Wasserstein (GW) distance between metrics defined within
each space rather than between samples across different spaces, which has been used as a distance
between graphs in several applications [56, 60, 67]. However, these studies can only perform graph
classification on datasets with multiple graphs and cannot be applied to network analysis, such as node
representation classification, where only one network is available. In this work, we combine optimal
transport problem with graph neural network to form a novel self-supervised learning paradigm that
allows both graph and network learning. See Appendix A for more details."
BACKGROUND,0.10285714285714286,"3
Background"
BACKGROUND,0.10857142857142857,"Notations.
Let G = (V, A, µ) be a graph of n nodes where V denotes the set of nodes, A ∈
{0, 1}n×n is the adjacency matrix. µ ∈Rn is the empirical distribution of nodes in G. Generally,
µ is a uniform discrete probability distribution. When the node label (or attribute) is available, we
represent the node feature matrix as X ∈Rn×d, where d denotes the dimension of node attributes.
Xi represents the i-row of X. Let [[n]] = {1, · · · , n}, [[n]]2 = [[n]] × [[n]] and ⟨·, ·⟩denotes the inner
product for matrices. We denote ⊗the tensor-matrix multiplication. 1n represents the vector with
ones as all the n elements. | · | denotes absolute value."
BACKGROUND,0.11428571428571428,"Plan and Optimal Transport.
The optimal transport (OT) problem is pioneered by Monge [35] in
order to seek the most cost-effective transport plan that transforms the mass of a pile of sand into
another one. In particular, it studies how to find an optimal coupling or optimal plan π for transforming
the distribution µ to ν with minimum total transport cost (i.e., optimal transport distance), where the
element of π describes the probability of moving mass from one position to another. In this work, we
mainly focus on the discrete case. Given two sets of features X1 = {Xi
1}n
i=1 and X2 = {Xj
2}m
j=1,
where n and m are are the number of features, respectively. µ ∈Rn and ν ∈Rm are the probability
distributions of the entities in the two sets, respectively. The formulation of the OT distance is"
BACKGROUND,0.12,"W(X1, X2) =
min
π∈Π(µ,ν) X"
BACKGROUND,0.12571428571428572,i∈[[n]] X
BACKGROUND,0.13142857142857142,"j∈[[m]]
cX (Xi
1, Xj
2) · πij =
min
π∈Π(µ,ν) ⟨K(X1, X2), π⟩
(2)"
BACKGROUND,0.13714285714285715,"where
Π(µ, ν) = {π ∈Rn×m | π1m = µ, 1nπ = ν}
(3)"
BACKGROUND,0.14285714285714285,"denotes all the joint distributions π with marginals µ and ν. K(X1, X2)ij = cX (Xi
1, Xj
2) is the
cost (work) of moving Xi
1 to Xj
2, the cosine distance between Xi
1 and Xi
2 is a popular choice. The
π ∈Rn×m is called as transport plan. This distance is also known as the Wasserstein distance."
BACKGROUND,0.14857142857142858,"Optimal Transport for Graphs.
The Wasserstein problem requires the two distributions of point
sets to lie in the same space. But for graphs, it is difficult to measure the cost between two nodes on"
BACKGROUND,0.15428571428571428,"different graphs without node label (attribute). Even if the cost between nodes could be calculated,
the Wasserstein distance cannot take the edge information into account. To compare distributions that
are not necessarily in the same space, [34] defines Gromov-Wasserstein distance between two graphs
without node label G1 = (A1, µ) and G2 = (A2, ν) as follow"
BACKGROUND,0.16,"WGW(G1, G2) =
min
πG∈Π(µ,ν) X"
BACKGROUND,0.1657142857142857,"i,k∈[[n]]2 X"
BACKGROUND,0.17142857142857143,"j,l∈[[m]]2
cA(Aik
1 , Ajl
2 ) · πikπjl =
min
π∈Π(µ,ν)⟨L(A1, A2) ⊗π, π⟩"
BACKGROUND,0.17714285714285713,"(4)
where L(A1, A2) is 4-D tensor and L(A1, A2)ijkl = cA(Aik
1 , Ajl
2 ). The cost function cA is
commonly defined as cA(Aik
1 , Ajl
2 ) = |Aik
1 −Ajl
2 |."
BACKGROUND,0.18285714285714286,"Consider two graphs with node attributes G1 = (A1, X1, µ) and G2 = (A2, X2, ν) where A1 ∈
Rn×n and A2 ∈Rm×m denote their adjacency matrices, X1 ∈Rn×d and X2 ∈Rm×d are feature
matrices. The fused Gromov-Wasserstein distance [56] between graphs G1 and G2 can be defined as"
BACKGROUND,0.18857142857142858,"WFGW(G1, G2) =
min
πG∈Π(µ,ν) σ
X"
BACKGROUND,0.19428571428571428,"ij
cX (Xi
1, Xj
2) · πG
ij + (1 −σ)
X"
BACKGROUND,0.2,"ijkl
cA(Aik
1 , Ajl
2 ) · πG
ijπG
kl
(5)"
BACKGROUND,0.2057142857142857,which is equivalent to
BACKGROUND,0.21142857142857144,"min
πG∈Π(µ,ν) ⟨σK(X1, X2) + (1 −σ)L(A1, A2) ⊗πG, πG⟩
(6)"
BACKGROUND,0.21714285714285714,"where K(X1, X2)ij = cX (Xi
1, Xj
2), L(A1, A2)ijkl = cA(Aik
1 , Ajl
2 ) and σ ∈[0; 1] denotes a
trade-off parameter. Obviously, this definition is a fusion of Equations (2) and (4)."
GRAPH TRANSPORT ALIGNMENT,0.22285714285714286,"4
Graph Transport Alignment"
GRAPH TRANSPORT ALIGNMENT,0.22857142857142856,Encoder
GRAPH TRANSPORT ALIGNMENT,0.2342857142857143,Encoder
GRAPH TRANSPORT ALIGNMENT,0.24,Aligning
GRAPH TRANSPORT ALIGNMENT,0.24571428571428572,"Figure 2:
A framework of graph transport self-
supervised learning."
GRAPH TRANSPORT ALIGNMENT,0.25142857142857145,"As analyzed in the previous section, the re-
cent graph contrastive techniques are deeply
plagued by positive and negative sample
generation, since graph properties could be-
come completely different even with slight
perturbations.
To design a universal self-
supervision scheme, we are motivated to cal-
ibrate the similarity between different repre-
sentations in the output space using the match-
ing signal between corresponding graphs
from the input space. In particular, we first
perturb the given graph to obtain two differ-
ent graphs (e.g., a perturbed graph and the
original one) and generate the node embed-
dings of the two graphs using the backbone
model (e.g., GNNs). After computing the op-
timal transport plans for the graphs and the
sets of node representations respectively (Sec-
tion 4.1), we take the discrepancy between the two plans as the loss to calibrate the backbone for
obtaining representation with rich geometry awareness and interpretable correspondences (Section
4.2). We compare the proposed graph self-supervised learning paradigm with graph contrastive
learning in Section 4.3. The framework can be found in Figure 2."
OPTIMAL TRANSPORT PLAN,0.2571428571428571,"4.1
Optimal Transport Plan"
OPTIMAL TRANSPORT PLAN,0.26285714285714284,"In general, it is challenging to define two similarity metrics (e.g., distances), which can be directly
compared, in two different spaces. This is especially the case for graphs and vectors, two distinct
objects by nature. Fortunately, optimal transport theory offers a glimmer of hope, transportation plan,
for such comparison. In this section, we present objectives that aim at finding the optimal plan for
two graphs (or sets of vectors) to minimize the transport cost. For the graph, a natural idea is to
jointly take into account both node attributes and explicit topology information (i.e., edges) in the
transportation plan. In this work, we leverage [56] to present an objective function for calculating the
optimal transport plan which integrates the edge structure and the feature information on nodes."
OPTIMAL TRANSPORT PLAN,0.26857142857142857,"Specifically, the fused optimal transport plan π∗
G ∈Rn×m between two node-attribute graphs G1 and
G2 can be defined as
π∗
G = argmin
πG∈Π(µ,ν)
⟨σK(X1, X2) + (1 −σ)L(A1, A2) ⊗πG, πG⟩
(7)"
OPTIMAL TRANSPORT PLAN,0.2742857142857143,"with the fused Gromov-Wasserstein distance (6). By tuning the parameter σ we can control the bias
of the learned optimal plan between node attributes and edge structure. Intuitively we might think
that combining more edge information could greatly benefit the expressiveness power of the model.
However, we observe that the performance of the proposed method does not degrade significantly in
the absence of edge information, and even increases rather than decreases on some datasets. It will be
explained in detail later."
OPTIMAL TRANSPORT PLAN,0.28,"For the node representations of graph encoded by the backbone model (e.g., GNNs), we can either
use Equation (2) directly to calculate the optimal plan π∗
Z ∈Rn×m or set σ = 1 in Equation (7) as
π∗
Z(Z1, Z2) = argmin
πZ∈Π(µ,ν)
⟨J(Z1, Z2), πZ⟩
(8)"
OPTIMAL TRANSPORT PLAN,0.2857142857142857,"where J(Z1, Z2)ij = cZ(Zi
1, Zj
2), Z1 and Z2 denote the node representations corresponding to G1
and G2, respectively."
OPTIMAL TRANSPORT ALIGNMENT,0.2914285714285714,"4.2
Optimal Transport Alignment"
OPTIMAL TRANSPORT ALIGNMENT,0.29714285714285715,"The previous section establishes the foundation that comparing the similarity metrics defined on
graph space and vector space can be reduced into comparing the two optimal transport plans from
these spaces, each of which can be solved using Equations (7) or (2). This is a valid comparison
because the optimal transport plan π acts as a probabilistic matching of two distributions, while the
two plan matrices have the same dimensions (i.e., n × m)."
OPTIMAL TRANSPORT ALIGNMENT,0.3028571428571429,"Naturally, the encoder may succeed in obtaining a good representations for a graph if it learns the
node embeddings that not only retain its structural information inside the graph, but also capture
matching information with other graphs. This motivates us to force the encoder to preserve the
matching relationship in the graph space by aligning the plan between the two graphs with the plan
of their corresponding node representations. We define a match alignment loss by minimizing the
discrepancy between the two transport plans as follows"
OPTIMAL TRANSPORT ALIGNMENT,0.30857142857142855,"Lmatch = ∆

π∗
G, π∗
Z
 
Z1, Z2

(9)"
OPTIMAL TRANSPORT ALIGNMENT,0.3142857142857143,"where the discrepancy function ∆(·, ·) can be any commonly used metric, e.g., the Frobenius-norm
∥· −· ∥F or the divergence D(·∥·)."
OPTIMAL TRANSPORT ALIGNMENT,0.32,"In addition, to guide the encoder to learn a representation retaining structural information inside the
graph, we also calibrate the cost matrix J(Z1, Z2), which implies the implicit structure relationships
between nodes, in the representation space as follow"
OPTIMAL TRANSPORT ALIGNMENT,0.32571428571428573,"L(im)strc = ∆

σK
 
X1, X2

+ (1 −σ)L
 
A1, A2

⊗π∗
G, J
 
Z1, Z2

(10)"
OPTIMAL TRANSPORT ALIGNMENT,0.3314285714285714,Figure 3: An illustration of implicit structure in one-dimension.
OPTIMAL TRANSPORT ALIGNMENT,0.33714285714285713,"To understand the concept of ‘im-
plicit structure’ intuitively, here we
consider the one-dimensional case.
As shown in Figure 3, given a point
set P = {Z1
1, · · · , Zn
1 }, the loca-
tion of each of its points is fixed. If
the position of a point (yellow) Zj
2
is unknown but the transportation
cost from this point to all points (blue) in the point set P is known, then the location of this point is
determined with respect to the entire point set P. The same holds true for another point Zl
2. Thus the
relative position relationship between points Zj
2 and Zl
2 can be captured implicitly by the transport
costs matrix J(Z1, Z2)."
OPTIMAL TRANSPORT ALIGNMENT,0.34285714285714286,"To this end, we define the overall graph transport alignment loss as
LGALOPA = Lmatch + ρL(im)strc
(11)
where ρ is the trade-off parameter."
COMPARE WITH GRAPH CONTRASTIVE LEARNING,0.3485714285714286,"4.3
Compare with Graph Contrastive Learning"
COMPARE WITH GRAPH CONTRASTIVE LEARNING,0.35428571428571426,"Although our objective function is different from the contrastive loss as shown in Equation (1), we
find that the algorithmic philosophy of both is very similar. Here we analyze at the node level. Given
a graph G, the perturbation graphs G1 and G2 are obtained by augmenting G. If the attribute and
context of node i in G are corrupted in a similar way and obtained two node views in G1 and G2. The
cost between these two views is quite small (almost zero) and thus the optimal transport plan yields a
high probability of matching between these two node views. With Equations (9) and (10), the model
GALOPA calibrates the matching probability and cost of the corresponding node representations in
the output space, which is actually making the representations of two similar nodes similar enough.
And for two different nodes i and k of G, the cost between their corresponding node views in graphs
G1 and G2 is relatively larger. This leads to the opposite correction, i.e., making the representation of
two dissimilar nodes sufficiently dissimilar."
COMPARE WITH GRAPH CONTRASTIVE LEARNING,0.36,"From the contrastive learning perspective, the above process is not inconsistent with its goals. But
there is a fundamental difference between graph transport alignment and graph contrastive learning:
GALOPA directly utilizes calibration signal from the graph space. Precisely because of this signal, we
do not have to distinguish between positive and negative samples, like Maxwell’s demon."
COMPLEXITY,0.3657142857142857,"4.4
Complexity"
COMPLEXITY,0.37142857142857144,"The time complexity of the model GALOPA is mainly influenced by the optimization process of
Equations (7) and (8). To optimize Equation (7), which contains the fused Gromov-Wasserstein term,
we utilize a conditional gradient (CG) solver [21]. This solver necessitates the computation of a
gradient with a nearly cubic time complexity at each iteration concerning the size of the graph, i.e., the
number of nodes. On the other hand, Equation (8) with the Wasserstein term can be optimized using
the Sinkhorn-Knopp algorithm [12], which is highly time-efficient with a nearly square complexity."
COMPLEXITY,0.37714285714285717,"5
Plan or Distance? 0.842 0.743 0.769 0.911 0.795 0.689 0.733 0.851 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95"
COMPLEXITY,0.38285714285714284,"CORA
CITESEER
PROTEINS
MUTAG"
COMPLEXITY,0.38857142857142857,ACCURACY plan dist
COMPLEXITY,0.3942857142857143,"Figure 4: Plan versus distance.
Comparing mean
graph/node classification accuracy between transport
alignment loss and distance loss on 4 datasets."
COMPLEXITY,0.4,"Thanks to the alignment of the plan and
cost for the representation, as a byprod-
uct, we find that the OT distance between
the optimal node representations Z∗in
Equation (11) is equal to the distance be-
tween its corresponding graphs. This is
due to the two losses (10) and (9) con-
strain the optimal node representation to
satisfy J(Z∗
1, Z∗
2) = σK(X1, X2)+(1−
σ)L(A1, A2) ⊗π∗
G and π∗
Z(Z∗
1, Z∗
2) =
π∗
G, respectively. This means that our losses
can prompt the encoder to learn an isomet-
ric embedding that preserves the distance
between graphs, which is one of the pur-
suits of the general representation model.
Hence, we became interested in the ques-
tion of who is more important, distance or
plan? How would the model perform if we
drop the alignment of the plan and cost but instead optimize the distance directly? In this section, we
assess and rationalize the role of the plan for graph structure data in our self-supervised framework.
To compare the possible performance gap between distance and plan, instead of directly optimizing
the plan, we construct a new loss with distance as following"
COMPLEXITY,0.4057142857142857,"Ldist = |WG(G1, G2) −W(Z1, Z2)|
(12)"
COMPLEXITY,0.4114285714285714,"where |·| denotes absolute value, and WG(G1, G2) = minπ∈Π(h1,h2) σ P"
COMPLEXITY,0.41714285714285715,"ij cX (Xi
1, Xj
2)·πG
ij +(1− σ) P"
COMPLEXITY,0.4228571428571429,"ijkl cA(Aik
1 , Ajl
2 ) · πG
ikπG
jl."
COMPLEXITY,0.42857142857142855,"We evaluate the performance of using the pretraining representations on 2 social network datasets,
CORA and CITESEER [25] for node classification, and 2 graph classification data PROTEINS and"
COMPLEXITY,0.4342857142857143,"MUTAG from TUDataset [36] for graph classification. See Section 8 for detailed experimental
configurations. Figure 4 reports the averaged node/graph classification accuracy results over the
node/graph-level datasets. The results suggest that the model using the plan as an objective signif-
icantly outperforms the counterpart models using the distance. Although the experimental result
may lead to ‘surprise’, it demonstrates that the plan is closer to the essence than the distance, for
discrete structured data. The optimal transport formulation Equation (5) contains both matching and
implicit structural information. If only the final distance is retained instead of capturing the two
types of information separately, the learned representation may fail to align properly with the input
element. Because the optimal transport plan for the discrete OT problem is not unique in general and
the optimal distance may correspond to several plans."
COMPLEXITY,0.44,"6
Node Attributes or Edges? 0.776 0.647 0.751 0.856 0.842 0.743 0.769 0.91 0.835 0.738 0.768 0.911 0.823 0.735 0.759 0.868"
COMPLEXITY,0.44571428571428573,"CORA
CITESEER
PROTEINS
MUTAG"
COMPLEXITY,0.4514285714285714,ACCURACY
COMPLEXITY,0.45714285714285713,"ρ=0, σ=1
ρ≠0, σ=1
ρ≠0, σ=0.5
ρ≠0, σ=0"
COMPLEXITY,0.46285714285714286,"Figure 5: The mean graph/node classification accuracy
on 4 datasets under different values of parameter σ."
COMPLEXITY,0.4685714285714286,"Among the information encoded in a graph,
the structure and node attributes are two
crucial elements for representation learning.
The basic requirement of the encoder is to
preserve the topology structures and capture
the vertex feature of graphs. Thus a prob-
lem is encountered in our self-supervised
learning paradigm: if we try to calibrate
the representations learned by the backbone
encoder, which is more important, the edge
structure or the node attributes? In other
words, if the calibration signal from the in-
put space contains only node attribute in-
formation and completely ignores the ex-
plicit edge connectivity, will the perfor-
mance of the proposed model deteriorate
significantly? The answer seems obvious—it should be. But the case seems to be different. Let’s take
a look at the experiment below."
COMPLEXITY,0.4742857142857143,"As in the previous section, we performed the comparative experiments on four datasets CORA,
CITESEER, PROTEINS, and MUTAG. Here we first consider the normal case of our loss function
(i.e., ρ ̸= 0). We set the value of the parameter σ to adjust the bias of node attributes or edge
connections for the plan in the graph space. If σ = 1, the model takes into account only node
attributes in the transportation plans. When σ = 0, it integrates explicit edge information while
completely ignoring the node feature. Figure 5 reports the results with different σ and shows a
surprising outcome: With only node attributes for the calculation of the plan, the model achieves
outstanding performance on all the datasets, even optimal on some data. However, if we set ρ = 0
to remove the implicit structure constraint term L(im)strc, the performance of the model suddenly
deteriorates dramatically."
COMPLEXITY,0.48,"This verifies that the constraint L(im)strc is necessary and the implicit structural information it captures
does calibrate the encoder even in the case of missing explicit edge connection. It therefore inspires
self-supervised graph learning: it may not be able to tell that edge information does not contribute to
correction for significant performance gains, but it is perfectly feasible to use only node attributes as
a calibration supervisory signal for backbone model. This is a practical and valuable finding since the
number of edges in a real-world graph dataset is much more than the number of nodes. The time
and space complexity of the model can be reduced greatly if only the node information is used in the
calculation of the correction signal while ignoring the edge connections."
COMPLEXITY,0.4857142857142857,"7
Are the Transport Alignment Free from Positive/Negative Samples?"
COMPLEXITY,0.49142857142857144,"As shown in Equation (11), the proposed objective function does not distinguish whether the two
different graphs/nodes are positive (negative) samples or not. Here, we want to show that the graph
transport alignment strategy can be independent of the label invariance assumption. We conduct
experiments below to see how different levels of perturbation affect the performance of GALOPA.
When augmenting the graph, we fixed one of the augmentations as NoAug and the other augmentation"
COMPLEXITY,0.49714285714285716,"0.1
0.2
0.4
0.6
0.8
Edge Perturbation Rate"
COMPLEXITY,0.5028571428571429,"0.8
0.6
0.4
0.2
0.1
Feature Masking Rate"
COMPLEXITY,0.5085714285714286,"0.766
0.764
0.756
0.762
0.755"
COMPLEXITY,0.5142857142857142,"0.768
0.763
0.761
0.763
0.758"
COMPLEXITY,0.52,"0.761
0.76
0.768
0.759
0.755"
COMPLEXITY,0.5257142857142857,"0.769
0.767
0.762
0.76
0.763"
COMPLEXITY,0.5314285714285715,"0.768
0.763
0.761
0.757
0.759"
COMPLEXITY,0.5371428571428571,PROTEINS
COMPLEXITY,0.5428571428571428,"0.1
0.2
0.4
0.6
0.8
Edge Perturbation Rate"
COMPLEXITY,0.5485714285714286,"0.8
0.6
0.4
0.2
0.1"
COMPLEXITY,0.5542857142857143,"0.903
0.901
0.905
0.902
0.901"
COMPLEXITY,0.56,"0.91
0.904
0.906
0.908
0.897"
COMPLEXITY,0.5657142857142857,"0.898
0.91
0.907
0.903
0.911"
COMPLEXITY,0.5714285714285714,"0.909
0.907
0.911
0.899
0.903"
COMPLEXITY,0.5771428571428572,"0.906
0.91
0.908
0.91
0.904 MUTAG"
COMPLEXITY,0.5828571428571429,"0.1
0.2
0.4
0.6
0.8
Edge Perturbation Rate"
COMPLEXITY,0.5885714285714285,"0.8
0.6
0.4
0.2
0.1"
COMPLEXITY,0.5942857142857143,"0.739
0.737
0.741
0.74
0.742"
COMPLEXITY,0.6,"0.74
0.743
0.736
0.737
0.741"
COMPLEXITY,0.6057142857142858,"0.742
0.739
0.74
0.732
0.739"
COMPLEXITY,0.6114285714285714,"0.743
0.741
0.742
0.738
0.739"
COMPLEXITY,0.6171428571428571,"0.743
0.74
0.741
0.738
0.74"
COMPLEXITY,0.6228571428571429,CITESEER
COMPLEXITY,0.6285714285714286,"0.1
0.2
0.4
0.6
0.8
Edge Perturbation Rate"
COMPLEXITY,0.6342857142857142,"0.8
0.6
0.4
0.2
0.1"
COMPLEXITY,0.64,"0.837
0.833
0.827
0.827
0.82"
COMPLEXITY,0.6457142857142857,"0.838
0.839
0.836
0.837
0.826"
COMPLEXITY,0.6514285714285715,"0.84
0.842
0.838
0.832
0.831"
COMPLEXITY,0.6571428571428571,"0.842
0.84
0.837
0.841
0.832"
COMPLEXITY,0.6628571428571428,"0.84
0.839
0.841
0.839
0.824 CORA"
COMPLEXITY,0.6685714285714286,"Figure 6: The mean graph/node classification accuracy when contrasting with different perturbation
rates under 4 datasets. Fix one of the augmentations as NoAug and the other augmentation be the
combination of edge perturbation and feature masking. Darker colors indicate better performance."
COMPLEXITY,0.6742857142857143,"requires a hyper-parameter “aug ratio” that controls the portion of node attributes/edge that are
selected for perturbing. Note that different augmentation strategies can be combined. Since the
computation of the optimal transport plan in this paper involves node attributes and edge, we perform
two augmentation policies, edge perturbation and feature masking, with different augmentation rates
on four datasets (i.e., CORA, CITESEER, MUTAG, and PROTEINS) as shown in Figure 6."
COMPLEXITY,0.68,"From Figure 6 we find that the performance of our model does not change much even when the
original graph is perturbed heavily. For example, even if eighty percent of the edges are removed
while eighty percent of the node attributes are destroyed, the model’s performance on the dataset
CITESEER (0.742) is almost equal to the optimal result (0.743). Fluctuations of only zero point
five to two percent are also observed on other data sets, such as one point three in PROTEINS data.
Hence, it validates that the alignment of optimal transport between the source space and target space
is indeed free from the label-invariant assumption."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.6857142857142857,"8
Comparison with the State-of-the-art Methods"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.6914285714285714,"In this section, we compare our proposed self-supervised pre-training framework, GALOPA, with
state-of-the-art methods in the settings of unsupervised learning on graph/node classification. More
results can be found in the Appendix."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.6971428571428572,"Table 1: Mean node classification accuracy (%) for supervised and unsupervised models. The highest
performance of unsupervised models is highlighted in boldface. OOM indicates Out-Of-Memory."
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.7028571428571428,"Model
CORA
CITESEER
PUBMED
WikiCS
Amz-Comp.
Amz-Photo
Coauthor-CS
Average"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.7085714285714285,"MLP
47.92 ± 0.41
49.31 ± 0.26
69.14 ± 0.34
71.98 ± 0.42
73.81 ± 0.21
78.53 ± 0.32
90.37 ± 0.19
68.72 ± 0.31
GCN
81.54 ± 0.68
70.73 ± 0.65
79.16 ± 0.25
93.02 ± 0.11
86.51 ± 0.54
92.42 ± 0.22
93.03 ± 0.31
85.20 ± 0.39"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.7142857142857143,"DEEPWALK
70.72 ± 0.63
51.39 ± 0.41
73.27 ± 0.86
74.42 ± 0.13
85.68 ± 0.07
89.40 ± 0.11
84.61 ± 0.22
75.64 ± 0.35
NODE2VEC
71.08 ± 0.91
47.34 ± 0.84
66.23 ± 0.95
71.76 ± 0.14
84.41 ± 0.14
89.68 ± 0.19
85.16 ± 0.04
73.67 ± 0.46"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.72,"GAE
71.49 ± 0.41
65.83 ± 0.40
72.23 ± 0.71
73.97 ± 0.16
85.27 ± 0.19
91.62 ± 0.13
90.01 ± 0.71
78.63 ± 0.39
VGAE
77.31 ± 1.02
67.41 ± 0.24
75.85 ± 0.62
75.56 ± 0.28
86.40 ± 0.22
92.16 ± 0.12
92.13 ± 0.16
80.97 ± 0.38
DGI
82.34 ± 0.71
71.83 ± 0.54
76.78 ± 0.31
75.37 ± 0.13
84.01 ± 0.52
91.62 ± 0.42
92.16 ± 0.62
82.02 ± 0.46
GMI
82.39 ± 0.65
71.72 ± 0.15
79.34 ± 1.04
74.87 ± 0.13
82.18 ± 0.27
90.68 ± 0.18
OOM
–
MVGRL
83.45 ± 0.68
73.28 ± 0.48
80.09 ± 0.62
77.51 ± 0.06
87.53 ± 0.12
91.74 ± 0.08
92.11 ± 0.14
83.67 ± 0.31
GRACE
81.92 ± 0.89
71.21 ± 0.64
80.54 ± 0.36
78.19 ± 0.10
86.35 ± 0.44
92.15 ± 0.25
92.91 ± 0.20
83.32 ± 0.41
GCA
82.38 ± 0.47
71.51 ± 0.32
80.89 ± 0.28
78.29 ± 0.36
87.88 ± 0.26
92.33 ± 0.68
92.64 ± 0.34
83.70 ± 0.39
BGRL
81.30 ± 0.54
72.06 ± 0.63
80.52 ± 0.30
76.13 ± 0.18
89.09 ± 0.51
92.15 ± 0.32
92.33 ± 0.39
83.37 ± 0.41"
COMPARISON WITH THE STATE-OF-THE-ART METHODS,0.7257142857142858,"GALOPA
84.21 ± 0.30
74.34 ± 0.18
84.57 ± 0.34
81.23 ± 0.19
88.65 ± 0.11
92.77 ± 0.40
93.04 ± 0.25
85.54 ± 0.25"
EXPERIMENTAL SETUP,0.7314285714285714,"8.1
Experimental Setup"
EXPERIMENTAL SETUP,0.7371428571428571,"Datasets. We analyze the quality of representations learned by GALOPA on node and graph classifi-
cation benchmarks. For node classification, we evaluate the performance of using the pretraining
representations on 7 benchmark graph datasets, namely, CORA, CITESEER, PUBMED [25] and
Wiki-CS, Amazon-Computers, Amazon-Photo, and Coauthor-CS [47]. For graph classification, we
follow GRAPHCL [72] to perform evaluations on 6 graph classification data NCI1, PROTEINS, DD,
MUTAG, COLLAB, and IMDB-B from TUDataset [36]."
EXPERIMENTAL SETUP,0.7428571428571429,"Baselines. For node-level tasks, we adopt three types of baselines: 1) Supervised learning methods,
including MLP and GCN [25]; 2) Graph embedding methods, including DEEPWALK [42] and"
EXPERIMENTAL SETUP,0.7485714285714286,"NODE2VEC [17]; 3) Graph contrastive learning methods, including GAE, VGAE [24] , DGI [58],
GMI [41], MVGRL [19], GRACE [77], GCA [78], and BGRL [55]. For graph-level task, we evaluate
the performance of GALOPA in terms of the linear classification accuracy and compare it with 1)
two supervised learning methods, including GCN [25] and GIN [69]; 2) seven kernel-based methods,
including SP [5], GK [49], WL [50], WLPM [39], FGW [56], DGK [70], and MLG [26]; 3) three
unsupervised methods, including NODE2VEC [17], SUB2VEC [1], GRAPH2VEC [37]; 4) five recent
SOTA self-supervised learning methods based on contrastive learning, including INFOGRAPH [52],
GRAPHCL [72], AD-GCL [53], JOAOv2 [73], RGCL [28] and SIMGRACE [66]."
EXPERIMENTAL SETUP,0.7542857142857143,"Protocol. We follow the standard evaluation protocol of previous state-of-the-art graph self-supervised
learning approaches at the graph and node levels, respectively. Specifically, for graph classification,
we report the mean 10-fold cross-validation accuracy after 5 runs followed by a linear SVM. The
linear SVM is trained by applying cross-validation on training data folds and the best mean accuracy
is reported. For node classification, we report the mean accuracy on the test set after 50 runs of
training followed by a linear neural network model. For the graphs (nodes) datasets, we randomly
split the data, where 80%/10%/10% (10%/10%/80%) of graphs (nodes) are selected for the training,
validation, and test set, respectively."
EXPERIMENTAL SETUP,0.76,"Implementation Details. In the experiments, we use the Adam optimizer [23] with learning
rate is tuned in {0.0001, 0.001, 0.01}. The optimization routine and the convergence analysis are
summarized in Appendix B. We conduct the experiment with the trade-off parameter ρ and σ,
the parameter C of SVM, batch size in the sets {10−3, 10−2, . . . , 102, 103}, {0, 0.1, . . . , 0.9, 1},
{10−3, . . . , 103}, {16, 64, 128, 256, 512}, respectively. To perform graph augmentation, we use 4
types of operations: Edge Perturbation, Feature Masking, Node Dropping, and Graph Sampling. Our
model is implemented with Pytorch Geometric [13] and Deep Graph Library [63]."
EXPERIMENTAL SETUP,0.7657142857142857,"Table 2: Supervised and unsupervised representation learning classification accuracy (%) along
with average accuracy of the algorithms on TU datasets. Bold indicates the best performance for
unsupervised methods on each dataset. ‘–’ means that the results are unavailable."
EXPERIMENTAL SETUP,0.7714285714285715,"Model
PROTEINS
DD
MUTAG
NCI1
COLLAB
IMDB-B
Average"
EXPERIMENTAL SETUP,0.7771428571428571,"GCN
74.92 ± 0.33
76.24 ± 0.14
85.63 ± 0.24
80.20 ± 0.14
79.01 ± 0.18
70.45 ± 0.37
77.74 ± 0.23
GIN
76.28 ± 0.28
78.91 ± 0.13
89.47 ± 0.16
82.75 ± 0.19
80.23 ± 0.19
73.70 ± 0.60
80.22 ± 0.25"
EXPERIMENTAL SETUP,0.7828571428571428,"SP
75.07 ± 0.54
>1d
85.25 ± 0.24
73.53 ± 0.16
–
55.62 ± 0.02
–
GK
71.67 ± 0.55
78.53 ± 0.03
81.71 ± 0.21
66.06 ± 0.12
71.81 ± 0.31
65.93 ± 0.10
72.61 ± 0.22
WL
72.92 ± 0.56
79.78 ± 0.36
80.76 ± 0.30
80.01 ± 0.50
69.30 ± 0.42
72.30 ± 0.44
75.84 ± 0.43
WLPM
–
78.79 ± 0.38
87.13 ± 0.42
86.32 ± 0.19
–
–
–
FGW
74.50 ± 0.23
–
88.34 ± 0.12
86.24 ± 0.31
–
62.97 ± 0.24
–
DGK
73.21 ± 0.61
74.79 ± 0.32
87.51 ± 0.65
79.98 ± 0.36
64.43 ± 0.48
67.09 ± 0.37
74.50 ± 0.46
MLG
41.23 ± 0.27
>1d
87.94 ± 0.16
>1d
>1d
66.67 ± 0.30
–"
EXPERIMENTAL SETUP,0.7885714285714286,"NODE2VEC
57.58 ± 0.36
–
72.62 ± 1.02
54.93 ± 0.16
56.12 ± 0.02
50.25 ± 0.09
–
SUB2VEC
53.06 ± 0.56
54.33 ± 0.24
61.17 ± 1.59
52.82 ± 0.15
55.26 ± 0.15
55.34 ± 0.15
55.33 ± 0.47
GRAPH2VEC
73.33 ± 0.21
79.32 ± 0.29
83.28 ± 0.93
73.21 ± 0.18
71.10 ± 0.54
71.16 ± 0.05
75.23 ± 0.36"
EXPERIMENTAL SETUP,0.7942857142857143,"INFOGRAPH
74.44 ± 0.31
72.85 ± 1.78
89.01 ± 1.13
76.20 ± 1.06
70.05 ± 1.13
73.03 ± 0.87
75.93 ± 1.04
GRAPHCL
74.39 ± 0.45
78.62 ± 0.40
86.80 ± 1.34
77.87 ± 0.41
71.36 ± 1.15
71.14 ± 0.44
76.69 ± 0.69
AD-GCL
73.28 ± 0.46
75.79 ± 0.87
88.74 ± 1.85
73.91 ± 0.77
72.02 ± 0.56
70.21 ± 0.68
75.65 ± 0.86
JOAOV2
74.13 ± 0.51
77.32 ± 0.29
87.17 ± 1.09
78.40 ± 0.17
69.19 ± 0.16
70.37 ± 0.37
76.09 ± 0.43
RGCL
75.03 ± 0.43
78.86 ± 0.48
87.66 ± 1.01
78.14 ± 1.08
70.92 ± 0.65
71.85 ± 0.84
77.07 ± 0.74
SIMGRACE
75.23 ± 0.19
77.45 ± 1.03
89.27 ± 1.39
79.10 ± 0.25
71.37 ± 0.44
71.45 ± 0.29
77.31 ± 0.59"
EXPERIMENTAL SETUP,0.8,"GALOPA
76.93 ± 0.18
83.87 ± 0.42
91.11 ± 1.27
77.86 ± 0.36
73.20 ± 0.37
70.72 ± 0.48
78.94 ± 0.51"
PERFORMANCE COMPARISON,0.8057142857142857,"8.2
Performance Comparison"
PERFORMANCE COMPARISON,0.8114285714285714,"Performance under Node-level. Table 1 reports the averaged results over the node-level datasets.
Comparing the results in Table 1, we have the following major observations. The proposed method
outperforms the state-of-the-art self-supervised models significantly and even exceeds the supervised
models on several datasets. For example, on PUBMED, GALOPA achieves 84.57% accuracy, which
is a 3.68% relative improvement over previous state-of-the-art unsupervised algorithms. When
compared to supervised baselines, it outperforms strong supervised baselines: On CORA, CITESEER
and PUBMED benchmarks we observe 2.67%, 3.61% and 5.41% relative improvement over GCN,
respectively. On Coauthor-CS, the proposed unsupervised method shows competitive performance
compared to the supervised models. Tabel 1 lists the average accuracy of 7 benchmark datasets,
from which GALOPA achieves the best performance as well. For example, our proposed GALOPA"
PERFORMANCE COMPARISON,0.8171428571428572,"outperforms the unsupervised SOTA baseline SIMGRACE by 1.84% on average, and even outperforms
supervised GCN by 0.34%. These results further validate that calibrating the backbone model by
optimal transport alignment can produce expressive and generalizable representations."
PERFORMANCE COMPARISON,0.8228571428571428,"Performance under Graph-level. In this section, we examine whether the proposed GALOPA
performs better than state-of-the-art methods at graph-level datasets. The results of supervised
learning baselines and unsupervised methods are reported in Table 2. The results shown in Table 2
suggest that GALOPA achieves state-of-the-art results with respect to unsupervised models. For
example, on DD it achieves 83.87% accuracy, a 4.09% relative improvement over the previous
state-of-the-art baselines. For kernel methods, our approach achieves better performance on most
datasets. When compared to supervised baselines individually, our model outperforms GCN in 4
out of 6 datasets and outperforms GIN in 3 out of 6 datasets, e.g., a 1.64% relative improvement on
GIN for the NCI1 dataset. Our approach outperforms the state-of-the-art graph contrastive learning
approaches. For example, compared to SIMGRACE, which is one of the best SOTA methods, GALOPA
has a relative improvement of 1.63% on average across all datasets. GALOPA outperforms GRAPHCL
and INFOGRAPH with a relative improvement of 2.25% and 3.01% on average, respectively. To
summarize, our newly proposed GALOPA for graph self-supervised has achieved SOTA performance."
PERFORMANCE COMPARISON,0.8285714285714286,Discussion
PERFORMANCE COMPARISON,0.8342857142857143,"Conclusion. In this paper, we investigated the self-supervised graph learning problem, addressing the
challenges posed by label-invariant issues in contrasting graph learning. Unlike existing methods that
adopt contrastive or distance-based regularization approaches, we propose a novel paradigm based
on optimal transport for self-supervised graph learning. Our approach focuses on aligning optimal
transport plans between the graph space and the representation space. By aligning the transport plans,
our method enforces the backbone model to learn representations that precisely preserve the matching
relationships in the original graph space. Our observations reveal several noteworthy findings: (i) The
optimal transport plan serves as a more informative calibration signal for the encoder compared to the
transport distance, capturing essential characteristics; (ii) It is feasible to utilize only node attributes
as a correction signal for the backbone model, without relying on edge information; (iii) Our proposed
graph self-supervised model eliminates the need to distinguish between positive and negative samples
and overcomes the label-invariant assumption; Furthermore, extensive experiments conducted on
multiple benchmark datasets demonstrate the state-of-the-art performance of our proposed framework
in terms of generalizability and robustness."
PERFORMANCE COMPARISON,0.84,"Limitations and Future Work. Although the transport plan opens the door for direct communication
between the input graph space and the output representation space, it also becomes a computational
bottleneck for the model to some extent due to the limitation of optimal transport computation
complexity. To reduce the time complexity, we can utilize the properties of the proposed model
and/or the scaling optimal transport techniques that can reduce the time complexity from cubic to
square or even to linear, we provide 4 ways to do this below: (i) Unlike general OT settings, where
the two graphs are typically quite different and the matching relationship between them is completely
unknown, the difference between the original and augmented graphs in GALOPA is quite small and
the matching relations for subgraph components except with different part (i.e., complementary
set of difference part) is known. This means that we can utilize the matching prior to reduce the
computational cost. Hence, we can split the difference part with its neighborhood from the two graphs
and compute the optimal transport plan only for that part. Since the percentage of that part is very
small, it can greatly reduce the time complexity; (ii) According to the observation in Section 6, we
can avoid the cubic complexity of optimizing GW by using only the node attributes for computing the
optimal plan in graph space, while retaining similar performance with near-square time complexity;
(iii) Alternatively, we can reduce the computational cost by utilizing sparsity [27] or graph partitioning
[10, 67]. In particular, we can employ the most recent work on linear optimal transport [9, 38], which
computes FGW term and/or Wasserstein term in linear time; (iv) We have the option to combine the
aforementioned methods. For instance, by merging insights from the first point, a significant portion
of subgraph pairs acquired via graph partitioning in the third point turns out to be identical. This
realization can further pare down the complexity of partitioning methods."
PERFORMANCE COMPARISON,0.8457142857142858,"As the main goal of this paper is to propose an alternative self-supervised graph learning paradigm
beyond the label-invariant assumption that accurately links/communicates the input and output spaces,
we leave the scalability issue as our future work."
PERFORMANCE COMPARISON,0.8514285714285714,Acknowledgments and Disclosure of Funding
PERFORMANCE COMPARISON,0.8571428571428571,"We thank Edouard Pauwels and Samuel Vaiter for their valuable work and for proving the convergence
of the derivatives of Sinkhorn–Knopp. We also thank the anonymous reviewers for their constructive
suggestions. This project was in part supported by the following projects: the National Natural
Science Foundation of China (No.62032013, No.92267206); Singapore Institute of Technology
Ignition Grant (No.R-IE2-A405-0001)."
REFERENCES,0.8628571428571429,References
REFERENCES,0.8685714285714285,"[1] Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B Aditya Prakash. Sub2vec: Feature
learning for subgraphs. In Pacific-Asia Conference on KDDM, pages 170–182. Springer, 2018.
[2] Jonas Adler and Sebastian Lunz. Banach wasserstein gan. Advances in Neural Information
Processing Systems, 31, 2018.
[3] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding
and clustering. Advances in Neural Information Processing Systems, 14, 2001.
[4] Nicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement
interpolation using lagrangian mass transport. In Proceedings of the 2011 SIGGRAPH Asia
Conference, pages 1–12, 2011.
[5] Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In IEEE
International Conference on Data Mining, pages 8 pp.–. IEEE, 2005.
[6] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. LightGCL: Simple yet effective
graph contrastive learning for recommendation. In International Conference on Learning
Representations, 2023.
[7] Jiezhang Cao, Langyuan Mo, Yifan Zhang, Kui Jia, Chunhua Shen, and Mingkui Tan. Multi-
marginal wasserstein gan. Advances in Neural Information Processing Systems, 32, 2019.
[8] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph rep-
resentations. In Proceedings of the AAAI Conference on Artificial Intelligence, number 1,
2016.
[9] Yidong Chen, Chen Li, and Zhonghua Lu. Computing wasserstein-p distance between images
with linear cost. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 519–528, 2022.
[10] Samir Chowdhury, David Miller, and Tom Needham. Quantized gromov-wasserstein. In
Machine Learning and Knowledge Discovery in Databases, pages 811–827. Springer, 2021.
[11] Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. Advances in Neural Information Processing
Systems, 30, 2017.
[12] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in
Neural Information Processing Systems, 26, 2013.
[13] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
arXiv preprint arXiv:1903.02428, 2019.
[14] Ji Gao, Xiao Huang, and Jundong Li. Unsupervised graph alignment with wasserstein distance
discriminator. In Proceedings of the ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 426–435, 2021.
[15] Xinbo Gao, Bing Xiao, Dacheng Tao, and Xuelong Li. A survey of graph edit distance. Pattern
Analysis and Applications, 13:113–129, 2010.
[16] Amur Ghose, Yingxue Zhang, Jianye Hao, and Mark Coates. Spectral augmentations for graph
contrastive learning. arXiv:2302.02909, 2023.
[17] Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 855–864, 2016.
[18] Xiang Gu, Yucheng Yang, Wei Zeng, Jian Sun, and Zongben Xu. Keypoint-guided optimal
transport with applications in heterogeneous domain adaptation. Advances in Neural Information
Processing Systems, 35:14972–14985, 2022."
REFERENCES,0.8742857142857143,"[19] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
graphs. In Proceedings of the International Conference on Machine Learning, pages 4116–4126.
PMLR, 2020.
[20] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele
Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.
Advances in Neural Information Processing Systems, 33:22118–22133, 2020.
[21] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In Proceed-
ings of the International Conference on Machine Learning, pages 427–435. PMLR, 2013.
[22] Dongki Kim, Jinheon Baek, and Sung Ju Hwang. Graph self-supervised learning with accurate
discrepancy learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
editors, Advances in Neural Information Processing Systems, 2022.
[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[24] Thomas Kipf and Max Welling. Variational graph auto-encoders. arXiv:1611.07308, 2016.
[25] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. arXiv:1609.02907, 2016.
[26] Risi Kondor and Horace Pan. The multiscale laplacian graph kernel. Advances in Neural
Information Processing Systems, 29, 2016.
[27] Mengyu Li, Jun Yu, Hongteng Xu, and Cheng Meng. Efficient approximation of gromov-
wasserstein distance using importance sparsification. Journal of Computational and Graphical
Statistics, pages 1–12, 2023.
[28] Sihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant
rationale discovery inspire graph contrastive learning. In Proceedings of the International
Conference on Machine Learning, pages 13052–13065, 2022.
[29] Lu Lin, Jinghui Chen, and Hongning Wang. Spectral augmentation for self-supervised learning
on graphs. arXiv preprint arXiv:2210.00643, 2022.
[30] Huidong Liu, Xianfeng Gu, and Dimitris Samaras. Wasserstein gan with quadratic transport
cost. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
4832–4841, 2019.
[31] Nian Liu, Xiao Wang, Deyu Bo, Chuan Shi, and Jian Pei. Revisiting graph contrastive learning
from the perspective of graph spectrum. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[32] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen
Chang, and Doina Precup. Revisiting heterophily for graph neural networks. Advances in
Neural Information Processing Systems, 35:1362–1375, 2022.
[33] Facundo Mémoli. Spectral gromov-wasserstein distances for shape matching. In Proceedings
of the IEEE/CVF International Conference on Computer Vision Workshops, pages 256–263.
IEEE, 2009.
[34] Facundo Mémoli. Gromov–wasserstein distances and the metric approach to object matching.
Foundations of Computational Mathematics, 11:417–487, 2011.
[35] Gaspard Monge. Mémoire sur la théorie des déblais et des remblais. Mem. Math. Phys. Acad.
Royale Sci., pages 666–704, 1781.
[36] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and
Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs.
arXiv:2007.08663, 2020.
[37] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang
Liu, and Shantanu Jaiswal.
Graph2vec: Learning distributed representations of graphs.
arXiv:1707.05005, 2017.
[38] Dai Hai Nguyen and Koji Tsuda. On a linear fused gromov-wasserstein distance for graph
structured data. Pattern Recognition, 138:109351, 2023.
[39] Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching node em-
beddings for graph similarity. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 31, 2017."
REFERENCES,0.88,"[40] Edouard Pauwels and Samuel Vaiter. The derivatives of sinkhorn–knopp converge. SIAM
Journal on Optimization, 33(3):1494–1517, 2023."
REFERENCES,0.8857142857142857,"[41] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and
Junzhou Huang. Graph representation learning via graphical mutual information maximization.
In Proceedings of The Web Conference 2020, pages 259–270, 2020."
REFERENCES,0.8914285714285715,"[42] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social
representations. In Proceedings of the ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 701–710, 2014."
REFERENCES,0.8971428571428571,"[43] Gabriel Peyré, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and
distance matrices. In Proceedings of the International Conference on Machine Learning, pages
2664–2672. PMLR, 2016."
REFERENCES,0.9028571428571428,"[44] Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019."
REFERENCES,0.9085714285714286,"[45] Meyer Scetbon and Marco Cuturi. Linear time sinkhorn divergences using positive features.
Advances in Neural Information Processing Systems, 33:13468–13480, 2020."
REFERENCES,0.9142857142857143,"[46] Thibault Séjourné, François-Xavier Vialard, and Gabriel Peyré. The unbalanced gromov
wasserstein distance: Conic formulation and relaxation. Advances in Neural Information
Processing Systems, 34:8766–8779, 2021."
REFERENCES,0.92,"[47] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann.
Pitfalls of graph neural network evaluation. arXiv:1811.05868, 2018."
REFERENCES,0.9257142857142857,"[48] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation
learning for domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018."
REFERENCES,0.9314285714285714,"[49] Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt.
Efficient graphlet kernels for large graph comparison. In Artificial Intelligence and Statistics,
pages 488–495. PMLR, 2009."
REFERENCES,0.9371428571428572,"[50] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9),
2011."
REFERENCES,0.9428571428571428,"[51] Justin Solomon, Fernando De Goes, Gabriel Peyré, Marco Cuturi, Adrian Butscher, Andy
Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal
transportation on geometric domains. ACM Transactions on Graphics, 34(4):1–11, 2015."
REFERENCES,0.9485714285714286,"[52] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and
semi-supervised graph-level representation learning via mutual information maximization.
arXiv:1908.01000, 2019."
REFERENCES,0.9542857142857143,"[53] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to
improve graph contrastive learning. Advances in Neural Information Processing Systems, 34:
15920–15933, 2021."
REFERENCES,0.96,"[54] Jianheng Tang, Weiqi Zhang, Jiajin Li, Kangfei Zhao, Fugee Tsung, and Jia Li. Robust
attributed graph alignment via joint structure learning and optimal transport. arXiv preprint
arXiv:2301.12721, 2023."
REFERENCES,0.9657142857142857,"[55] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer,
Remi Munos, Petar Veliˇckovi´c, and Michal Valko. Large-scale representation learning on graphs
via bootstrapping. In International Conference on Learning Representations, 2022."
REFERENCES,0.9714285714285714,"[56] Vayer Titouan, Nicolas Courty, Romain Tavenard, and Rémi Flamary. Optimal transport for
structured data with application on graphs. In Proceedings of the International Conference on
Machine Learning, pages 6275–6284. PMLR, 2019."
REFERENCES,0.9771428571428571,"[57] Vayer Titouan, Ievgen Redko, Rémi Flamary, and Nicolas Courty. Co-optimal transport.
Advances in Neural Information Processing Systems, 33:17559–17570, 2020."
REFERENCES,0.9828571428571429,"[58] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. International Conference on Learning Representations, 2019."
REFERENCES,0.9885714285714285,"[59] Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009."
REFERENCES,0.9942857142857143,"[60] Cédric Vincent-Cuaz, Rémi Flamary, Marco Corneli, Titouan Vayer, and Nicolas Courty. Semi-
relaxed gromov-wasserstein divergence and applications on graphs. In International Conference
on Learning Representations, 2022.
[61] Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pages 1225–1234, 2016.
[62] Haonan Wang, Jieyu Zhang, Qi Zhu, and Wei Huang. Can single-pass contrastive learning work
for both homophilic and heterophilic graph? arXiv preprint arXiv:2211.10890, 2022.
[63] Minjie Yu Wang. Deep graph library: Towards efficient and scalable deep learning on graphs. In
International Conference on Learning Representations Workshop on Representation Learning
on Graphs and Manifolds, 2019.
[64] Yejiang Wang, Yuhai Zhao, Zhengkui Wang, and Meixia Wang. Robust self-supervised multi-
instance learning with structure awareness. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 37, pages 10218–10225, 2023.
[65] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger.
Simplifying graph convolutional networks. In International Conference on Machine Learning,
pages 6861–6871. PMLR, 2019.
[66] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. Simgrace: A simple framework
for graph contrastive learning without data augmentation. In Proceedings of the ACM Web
Conference 2022, pages 1070–1079, 2022.
[67] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph
partitioning and matching. Advances in Neural Information Processing Systems, 32, 2019.
[68] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learn-
ing for graph matching and node embedding. In Proceedings of the International Conference
on Machine Learning, pages 6932–6941. PMLR, 2019.
[69] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv:1810.00826, 2018.
[70] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 1365–1374, 2015.
[71] Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogcl: Auto-
mated graph contrastive learning via learnable view generators. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 36, pages 8892–8900, 2022.
[72] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen.
Graph contrastive learning with augmentations. Advances in Neural Information Processing
Systems, 33:5812–5823, 2020.
[73] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning auto-
mated. In Marina Meila and Tong Zhang, editors, Proceedings of the International Conference
on Machine Learning, pages 12121–12132. PMLR, 18–24 Jul 2021.
[74] Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. Label-invariant augmentation for
semi-supervised graph classification. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[75] Yuhai Zhao, Yejiang Wang, Zhengkui Wang, and Chengqi Zhang. Multi-graph multi-label
learning with dual-granularity labeling. In Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining, pages 2327–2337, 2021.
[76] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks
for graphs with heterophily: A survey. arXiv preprint arXiv:2202.07082, 2022.
[77] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive
representation learning. arXiv:2006.04131, 2020.
[78] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive
learning with adaptive augmentation. In Proceedings of the Web Conference 2021, pages
2069–2080, 2021."
