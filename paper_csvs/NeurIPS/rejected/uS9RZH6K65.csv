Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009900990099009901,"As one of the fundamental video tasks in computer vision, Open-Vocabulary Action
1"
ABSTRACT,0.0019801980198019802,"Recognition (OVAR) has recently gained increasing attention, with the develop-
2"
ABSTRACT,0.0029702970297029703,"ment of vision-language pre-trainings. To enable open-vocabulary generalization,
3"
ABSTRACT,0.0039603960396039604,"existing methods formulate vanilla OVAR to evaluate the embedding similarity
4"
ABSTRACT,0.0049504950495049506,"between visual samples and text descriptions. However, one crucial issue is com-
5"
ABSTRACT,0.005940594059405941,"pletely ignored: the text descriptions given by users may be noisy, e.g., misspellings
6"
ABSTRACT,0.006930693069306931,"and typos, limiting the real-world practicality. To fill the research gap, this paper
7"
ABSTRACT,0.007920792079207921,"analyzes the noise rate/type in text descriptions by full statistics of manual spelling;
8"
ABSTRACT,0.00891089108910891,"then reveals the poor robustness of existing methods; and finally rethinks to study
9"
ABSTRACT,0.009900990099009901,"a practical task: noisy OVAR. One novel DENOISER framework, covering two
10"
ABSTRACT,0.01089108910891089,"parts: generation and discrimination, is further proposed for solution. Concretely,
11"
ABSTRACT,0.011881188118811881,"the generative part denoises noisy text descriptions via a decoding process, i.e.,
12"
ABSTRACT,0.01287128712871287,"proposes text candidates, then utilizes inter-modal and intra-modal information to
13"
ABSTRACT,0.013861386138613862,"vote for the best. At the discriminative part, we use vanilla OVAR models to assign
14"
ABSTRACT,0.01485148514851485,"visual samples to text descriptions, injecting more semantics. For optimization, we
15"
ABSTRACT,0.015841584158415842,"alternately iterate between generative-discriminative parts for progressive refine-
16"
ABSTRACT,0.016831683168316833,"ments. The denoised text descriptions help OVAR models classify visual samples
17"
ABSTRACT,0.01782178217821782,"more accurately; in return, assigned visual samples help better denoising. We carry
18"
ABSTRACT,0.01881188118811881,"out extensive experiments to show our superior robustness, and thorough ablations
19"
ABSTRACT,0.019801980198019802,"to dissect the effectiveness of each component.
20"
INTRODUCTION,0.020792079207920793,"1
Introduction
21"
INTRODUCTION,0.02178217821782178,"Action recognition is one of the fundamental tasks in computer vision that involves classifying videos
22"
INTRODUCTION,0.02277227722772277,"into meaningful semantics. Despite huge progress that has been made, existing researches focus more
23"
INTRODUCTION,0.023762376237623763,"on closed-set scenarios, where action classes remain constant during training and inference. Such
24"
INTRODUCTION,0.024752475247524754,"scenarios are an oversimplification of real life, and thus limiting their practical application. Recently,
25"
INTRODUCTION,0.02574257425742574,"another line of research considers one more challenging scenario, namely open-vocabulary action
26"
INTRODUCTION,0.026732673267326732,"recognition (OVAR), and receives increasing attention.
27"
INTRODUCTION,0.027722772277227723,"OVAR allows users to give free texts to describe action classes, and the model needs to match novel
28"
INTRODUCTION,0.028712871287128714,"(unseen) text descriptions to videos with similar semantics. To tackle OVAR task, Vision-Language
29"
INTRODUCTION,0.0297029702970297,"Alignment (VLA) paradigm [41, 14, 57] provides one preliminary but popular idea, i.e., measuring
30"
INTRODUCTION,0.030693069306930693,"the embedding similarity between text descriptions and video embeddings. Following this paradigm,
31"
INTRODUCTION,0.031683168316831684,"recent works focus on minor improvements, e.g., better align vision-language modalities [16, 49, 62].
32"
INTRODUCTION,0.032673267326732675,"Although promising, these works all maintain one unrealistic assumption in real-world scenarios, i.e.,
33"
INTRODUCTION,0.033663366336633666,"the given text descriptions are absolutely clean/accurate. The concrete form is that they evaluate open-
34"
INTRODUCTION,0.034653465346534656,"vocabulary performance by re-partitioning closed-set datasets in which text descriptions of classes are
35"
INTRODUCTION,0.03564356435643564,"fully human-checked. But in fact, under real-world OVAR, novel text descriptions provided by users
36"
INTRODUCTION,0.03663366336633663,"are sometimes noisy. Character misspellings (typos, missing, tense error) are inevitable [43, 25] in
37"
INTRODUCTION,0.03762376237623762,Clean Text Descriptions
INTRODUCTION,0.03861386138613861,acting in play
INTRODUCTION,0.039603960396039604,adjusting glasses ‚Ä¶
INTRODUCTION,0.040594059405940595,yarn spinning
INTRODUCTION,0.041584158415841586,Existing Models for
INTRODUCTION,0.04257425742574258,Open-Vocabulary Action Recognition
INTRODUCTION,0.04356435643564356,"acting in play
yarn spinning
‚àö
√ó"
INTRODUCTION,0.04455445544554455,Noisy Text Descriptions
INTRODUCTION,0.04554455445544554,avting in play
INTRODUCTION,0.046534653465346534,adjusting hlasses ‚Ä¶
INTRODUCTION,0.047524752475247525,yarn spinning
INTRODUCTION,0.048514851485148516,"Figure 1: Left: For open-vocabulary action recognition (OVAR), existing researches neglect an
essential aspect: the text descriptions provided by users may be noisy (e.g., misspelling and typos),
resulting in potential classification errors and limiting the real-world practicality. Right: Rethinking
the robustness for popular OVAR methods [49, 62]. On various datasets, they exhibit high sensitivity
to text noises. Besides, as the noise level increases, the performance degrades significantly."
INTRODUCTION,0.04950495049504951,"thousands of descriptions, since users often don‚Äôt double-check, as well as differences in user habits
38"
INTRODUCTION,0.0504950495049505,"and diversity of scenarios (Fig. 1 Left).
39"
INTRODUCTION,0.05148514851485148,"We are hence motivated to fill the research gap of noisy text descriptions in OVAR. We analyze the
40"
INTRODUCTION,0.05247524752475247,"noise rate/type in real-world corpora [26, 45, 3]. We also make comprehensive simulations of text
41"
INTRODUCTION,0.053465346534653464,"noises, following NLP literature [42, 47]. Fig. 1 Right empirically evaluates noise hazards for existing
42"
INTRODUCTION,0.054455445544554455,"OVAR methods [16, 49, 62]. One can find that just a small amount of noise lowers recognition
43"
INTRODUCTION,0.055445544554455446,"accuracy by a large margin, implying quite poor robustness.
44"
INTRODUCTION,0.05643564356435644,"To spur the community to deal with the noisy OVAR task, being necessary and practical, this paper
45"
INTRODUCTION,0.05742574257425743,"bravely faces the challenges. One vanilla idea is using a separate language model (e.g., GPT [1]) to
46"
INTRODUCTION,0.05841584158415842,"correct noisy class descriptions, and then adapt the off-the-shelf vision-language paradigm [41, 14, 57].
47"
INTRODUCTION,0.0594059405940594,"However, there exist two nettlesome issues. 1) Textual Ambiguity. One text description is usually a few
48"
INTRODUCTION,0.060396039603960394,"compact words, with vague semantics, e.g., for the noisy text ‚Äúboird‚Äù, there could be multiple cleaned
49"
INTRODUCTION,0.061386138613861385,"candidates in terms of spelling, such as ‚Äúbird‚Äù and ‚Äúboard‚Äù. This short text lacks context, making
50"
INTRODUCTION,0.062376237623762376,"phrase correction difficult for uni-modal language models. 2) Cascaded Errors. Text correction and
51"
INTRODUCTION,0.06336633663366337,"action recognition are independently completed, without sharing knowledge. The noisy output of
52"
INTRODUCTION,0.06435643564356436,"text correction is cascaded to the input of action recognition, resulting in continuous propagation of
53"
INTRODUCTION,0.06534653465346535,"errors. To address these issues, we design one multi-modal robust framework: DENOISER.
54"
INTRODUCTION,0.06633663366336634,"Our first insight is to treat denoising of text descriptions as one generative task: given noisy text
55"
INTRODUCTION,0.06732673267326733,"descriptions, decode the clean ones, by considering text-vision information to help denoising. Specif-
56"
INTRODUCTION,0.06831683168316832,"ically, it consists of three components: text proposals, inter-modal weighting, and intra-modal
57"
INTRODUCTION,0.06930693069306931,"weighting. We first propose potential text candidates based on spelling similarity to limit the decoding
58"
INTRODUCTION,0.0702970297029703,"space. Then, two types of weighting are combined to decide the best candidate, that is, inter-modal
59"
INTRODUCTION,0.07128712871287128,"weighting uses assigned visual samples to vote; while intra-modal weighting relies solely on text
60"
INTRODUCTION,0.07227722772277227,"information. Our other insight is employing existing OVAR models as off-the-shelf tools to assign
61"
INTRODUCTION,0.07326732673267326,"visual samples at discriminative step. Such tools have been proven to handle clean OVAR tasks well,
62"
INTRODUCTION,0.07425742574257425,"also making our framework easier to adapt to previous models. For full usage of information in
63"
INTRODUCTION,0.07524752475247524,"the same semantics, we then assign detail-rich visual samples to clarify the semantic ambiguity of
64"
INTRODUCTION,0.07623762376237624,"compact text descriptions. To further avoid cascaded errors, we propose a solution of alternating
65"
INTRODUCTION,0.07722772277227723,"iterations, to connect generative and discriminative steps. By progressive refinement, denoised text
66"
INTRODUCTION,0.07821782178217822,"descriptions help OVAR models to match visual samples more accurately; assigned visual samples
67"
INTRODUCTION,0.07920792079207921,"help better denoising. Under multiple iterations, denoising results and OVAR are both better.
68"
INTRODUCTION,0.0801980198019802,"Our main contributions are summarized as follows:
69"
INTRODUCTION,0.08118811881188119,"‚Ä¢ We pioneer to explore noisy text descriptions for open-vocabulary action recognition (OVAR): first
70"
INTRODUCTION,0.08217821782178218,"fully analyze the noise rate/type in text descriptions by extensive statistics in real-world corpora; then
71"
INTRODUCTION,0.08316831683168317,"evaluate the robustness for existing methods; finally rethink to study one practical task: noisy OVAR.
72"
INTRODUCTION,0.08415841584158416,"‚Ä¢ We propose a novel DENOISER framework to tackle the noisy OVAR task, by alternately optimizing
73"
INTRODUCTION,0.08514851485148515,"generative-discriminative steps. The generative step leverages knowledge of vision-text alignment to
74"
INTRODUCTION,0.08613861386138613,"denoises noisy text descriptions, in the form of progressive decoding; while the discriminative step
75"
INTRODUCTION,0.08712871287128712,"assigns visual samples to text descriptions for open-vocabulary action recognition.
76"
INTRODUCTION,0.08811881188118811,"‚Ä¢ We carry out extensive experiments to show the superior robustness of DENOISER against noisy
77"
INTRODUCTION,0.0891089108910891,"text descriptions, under various noises and datasets. Great performance improvements are achieved
78"
INTRODUCTION,0.0900990099009901,"over existing competitors. Thorough ablations are studied to show effectiveness of every design.
79"
RELATED WORK,0.09108910891089109,"2
Related Work
80"
RELATED WORK,0.09207920792079208,"Vision-Language-Audio Pre-training (VLP) aims to jointly optimize multi-modal embeddings with
81"
RELATED WORK,0.09306930693069307,"large-scale web data, e.g., CLIP [41], ALIGN [14], Florence [57], FILIP [55], VideoCLIP [52], and
82"
RELATED WORK,0.09405940594059406,"LiT [58]. In architectures, VLP uses independent encoders for vision, text, and audio, followed by
83"
RELATED WORK,0.09504950495049505,"cross-modal fusion. For optimization, contrastive learning [5, 61] and cross-modal matching [7, 29]
84"
RELATED WORK,0.09603960396039604,"are mainstream, covering self supervision [32, 34], weak supervision [28, 8] and partial supervi-
85"
RELATED WORK,0.09702970297029703,"sion [19, 33]. VLP benefits various applications: image-text retrieval [6, 18], video understand-
86"
RELATED WORK,0.09801980198019802,"ing [23, 20, 22, 21], action recognition [16, 60], visual grounding [32, 56, 31], AIGC [4, 36].
87"
RELATED WORK,0.09900990099009901,"Open-Vocabulary Concept Learning aims to understand vision, where conceptual semantics are
88"
RELATED WORK,0.1,"described by free/arbitrary text descriptions. It is characterized by using vision-language pre-trainings
89"
RELATED WORK,0.100990099009901,"to match text descriptions and visual samples in semantic space. Its typical evaluation metric is
90"
RELATED WORK,0.10198019801980197,"the downstream zero-shot performance, i.e., classify unseen classes [49, 62, 17, 38, 54, 48, 37]. To
91"
RELATED WORK,0.10297029702970296,"achieve the evaluation, most methods re-partition closed-set datasets.[49] Although there is some
92"
RELATED WORK,0.10396039603960396,"plausibility, such re-partition implicitly makes an unrealistic assumption: text descriptions of unseen
93"
RELATED WORK,0.10495049504950495,"classes are human-checked, and thus absolutely clean, limiting real-world application. We pioneer
94"
RELATED WORK,0.10594059405940594,"taking noises from text descriptions (misspellings and typos) into consideration. By adding real-world
95"
RELATED WORK,0.10693069306930693,"noise for the above methods, we reveal their poor robustness, and design DENOISER for solution.
96"
RELATED WORK,0.10792079207920792,"Robustness of Language Models is extensively studied by adversarial attack-defense techniques [50,
97"
RELATED WORK,0.10891089108910891,"59]. When text inputs are facing noises, defense methods correct the outputs, dividing into: detection-
98"
RELATED WORK,0.1099009900990099,"purification [63, 39], as well as adversarial training [53, 9, 35, 30, 51]. The former methods detect
99"
RELATED WORK,0.11089108910891089,"and correct the corrupted part of a text phrase. The latter trains a model on adversarial samples to
100"
RELATED WORK,0.11188118811881188,"increase its direct noise-against ability. Overall, all these methods employ solely textual information
101"
RELATED WORK,0.11287128712871287,"for robustness in pure NLP tasks. We differ from them by considering robustness in the context of
102"
RELATED WORK,0.11386138613861387,"multi-modal scenarios and by employing multi-modal information to better assist text denoising.
103"
METHOD,0.11485148514851486,"3
Method
104"
METHOD,0.11584158415841585,"We explore noisy text descriptions for open-vocabulary action recognition. In Sec 3.1, we introduce
105"
METHOD,0.11683168316831684,"noisy open-vocabulary setting; in Sec 3.2, we detail our DENOISER framework, covering generative
106"
METHOD,0.11782178217821782,"- discriminative sub-parts; in Sec 3.3, we report the accompanying optimization strategy.
107"
PRELIMINARY & RETHINKING,0.1188118811881188,"3.1
Preliminary & Rethinking
108"
PRELIMINARY & RETHINKING,0.1198019801980198,"Open-Vocabulary Action Recognition (OVAR). For a video dataset V = (vj ‚ààRT √óH√óW √ó3)N
j ,
109"
PRELIMINARY & RETHINKING,0.12079207920792079,"OVAR aims to train one model Œ¶OVAR that matches target videos with arbitrary text description T .
110"
PRELIMINARY & RETHINKING,0.12178217821782178,"\mat h cal {Y}^{\ma t h rm {tr a in}} = 
\Phi _ {\mathrm {O V A R}}(\ m athcal {V}^{\mathrm {train}}\, ,\mathcal {T}^{\mathrm {train}}) \in \mathbb {R}^{C_{\text {base}}}, \quad \mathcal {Y}^{\mathrm {test}} = \Phi _{\mathrm {OVAR}}(\mathcal {V}^{\mathrm {test}}\, , \mathcal {T}^{\mathrm {test}}) \in \mathbb {R}^{C_{\text {novel}}}, 
(1)"
PRELIMINARY & RETHINKING,0.12277227722772277,"where Y refers to the matching label between V and T . During training, (video, text, matching label)
111"
PRELIMINARY & RETHINKING,0.12376237623762376,"triplets from the base semantic-classes are provided; while during testing, the model is evaluated
112"
PRELIMINARY & RETHINKING,0.12475247524752475,"on the novel semantic-classes. Note that, the semantic-classes between training (Cbase) and testing
113"
PRELIMINARY & RETHINKING,0.12574257425742574,"(Cnovel) are disjoint, i.e., Cbase ‚à©Cnovel = ‚àÖ.
114"
PRELIMINARY & RETHINKING,0.12673267326732673,"Vision-Language Alignment (VLA). To enable open-vocabulary capability, recent OVAR stud-
115"
PRELIMINARY & RETHINKING,0.12772277227722773,"ies [16, 49, 62, 40] embrace vision-language pre-trainings (VLPs), for their notable ability in cross-
116"
PRELIMINARY & RETHINKING,0.12871287128712872,"modal alignment. Specifically, OVAR could be achieved by measuring the embedding similarity
117"
PRELIMINARY & RETHINKING,0.1297029702970297,"between text descriptions T and video samples V, which is formally formulated as:
118"
PRELIMINARY & RETHINKING,0.1306930693069307,"\mat h cal 
{Y }  = \sigma (\ma t hcal 
{F } _v * \ m a thcal {F}_t), \quad \mathcal {F}_v = \Phi _{\mathrm {pool}}(\Phi _{\mathrm {vis}}(\mathcal {V})) \in \mathbb {R}^{N \times D}, \quad \mathcal {F}_t = \Phi _{\mathrm {txt}}(\mathcal {T}) \in \mathbb {R}^{C \times D}. 
(2)"
PRELIMINARY & RETHINKING,0.1316831683168317,"where œÉ refers to the softmax activation, Œ¶pool is the spatio-temporal pooling, Œ¶vis and Œ¶txt are
119"
PRELIMINARY & RETHINKING,0.13267326732673268,"visual and textual encoders of VLPs, D is the embedding dimension.
120"
PRELIMINARY & RETHINKING,0.13366336633663367,"Noisy Text Descriptions in OVAR. Although great progress has been made, the VLA paradigm
121"
PRELIMINARY & RETHINKING,0.13465346534653466,"suffers from an unrealistic assumption, i.e., that text descriptions are absolutely clean/accurate,
122"
PRELIMINARY & RETHINKING,0.13564356435643565,"Generative Step
Discriminative Step"
PRELIMINARY & RETHINKING,0.13663366336633664,"Better Texts,
Better Results 0.7"
PRELIMINARY & RETHINKING,0.13762376237623763,"0.2
0.1 0.2 0.6 0.2"
PRELIMINARY & RETHINKING,0.13861386138613863,"weight
distance 1 2 2"
PRELIMINARY & RETHINKING,0.13960396039603962,Text Candidates
PRELIMINARY & RETHINKING,0.1405940594059406,adjoining hlasses
PRELIMINARY & RETHINKING,0.14158415841584157,adjusting hlasses
PRELIMINARY & RETHINKING,0.14257425742574256,assisting hlasses ‚Üí
PRELIMINARY & RETHINKING,0.14356435643564355,"Visual
Encoder"
PRELIMINARY & RETHINKING,0.14455445544554454,"Textual
Encoder
Visual
Encoder"
PRELIMINARY & RETHINKING,0.14554455445544554,"Textual
Encoder"
PRELIMINARY & RETHINKING,0.14653465346534653,avting in plat
PRELIMINARY & RETHINKING,0.14752475247524752,abseiling
PRELIMINARY & RETHINKING,0.1485148514851485,air drumming
PRELIMINARY & RETHINKING,0.1495049504950495,adjisting hlasses
PRELIMINARY & RETHINKING,0.1504950495049505,"ùíØùëñ‚àí1: 
Decoded Texts"
PRELIMINARY & RETHINKING,0.15148514851485148,at ùëñ‚àí1 Step
PRELIMINARY & RETHINKING,0.15247524752475247,"Visual
Samples
:"
PRELIMINARY & RETHINKING,0.15346534653465346,avting in plat
PRELIMINARY & RETHINKING,0.15445544554455445,abseiling
PRELIMINARY & RETHINKING,0.15544554455445544,air drumming
PRELIMINARY & RETHINKING,0.15643564356435644,adjisting hlasses
PRELIMINARY & RETHINKING,0.15742574257425743,"ùíØùëñ‚àí1: Decoded
Texts at ùëñ‚àí1 Step"
PRELIMINARY & RETHINKING,0.15841584158415842,"ùíØùëñ: Decoded
Texts at ùëñStep"
PRELIMINARY & RETHINKING,0.1594059405940594,acting in plat
PRELIMINARY & RETHINKING,0.1603960396039604,abseiling
PRELIMINARY & RETHINKING,0.1613861386138614,air drumming
PRELIMINARY & RETHINKING,0.16237623762376238,adjusting hlasses
PRELIMINARY & RETHINKING,0.16336633663366337,Embedding Space
PRELIMINARY & RETHINKING,0.16435643564356436,"Inter-modal
Weighting"
PRELIMINARY & RETHINKING,0.16534653465346535,"Intra-modal
Weighting"
PRELIMINARY & RETHINKING,0.16633663366336635,Embedding Space Using
PRELIMINARY & RETHINKING,0.16732673267326734,Update
PRELIMINARY & RETHINKING,0.16831683168316833,"Using
Text embeddings
of text candidates"
PRELIMINARY & RETHINKING,0.16930693069306932,"Visual 
Embeddings"
PRELIMINARY & RETHINKING,0.1702970297029703,Text Embeddings of ùíØùëñ
PRELIMINARY & RETHINKING,0.1712871287128713,Text Embeddings
PRELIMINARY & RETHINKING,0.17227722772277226,of ùíØùëñ‚àí1
PRELIMINARY & RETHINKING,0.17326732673267325,Fusion & Vote for
PRELIMINARY & RETHINKING,0.17425742574257425,Text Candidates
PRELIMINARY & RETHINKING,0.17524752475247524,"Visual
Samples
:"
PRELIMINARY & RETHINKING,0.17623762376237623,"Use the Best,"
PRELIMINARY & RETHINKING,0.17722772277227722,to Update |
PRELIMINARY & RETHINKING,0.1782178217821782,"Figure 2: Framework Overview. DENOISER is composed of one generative part Œ®gene and one
discriminative part Œ®disc. Œ®gene views denoising text descriptions as a decoding process Ti‚àí1 ‚ÜíTi.
We first propose text candidates Œ¶prop for Ti‚àí1 based on spelling similarity; then choose the best
candidate by inter-modal weighting Œ¶inter and intra-modal weighting Œ¶intra. Œ¶inter uses vision-text
information, while Œ¶intra relies solely on texts. Œ®disc assigns text semantics to visual samples, then
only visual samples with the same semantics can vote for text candidates. We optimize alternatively
between generative and discriminative steps to tackle noisy OVAR."
PRELIMINARY & RETHINKING,0.1792079207920792,"limiting the practicality in reality. Actually, the diversity of users and scenarios can easily cause
123"
PRELIMINARY & RETHINKING,0.1801980198019802,"text descriptions given to be somewhat noisy, especially for unseen semantic-classes, due to their
124"
PRELIMINARY & RETHINKING,0.18118811881188118,"enormous degree of freedom. Formally, for one text description with n words, the clean/noisy
125"
PRELIMINARY & RETHINKING,0.18217821782178217,"versions T /T ‚Ä≤ are:
126"
PRELIMINARY & RETHINKING,0.18316831683168316,"\ lab
el  { e q :n
oi s e} \math c al 
{ T }^\p r i m e  =(t_{1}^\prime ,\cdots ,t_{n}^\prime ) = \Psi _{\mathrm {noise}}(\mathcal {T}\,; p), \quad \mathcal {T}=(t_{1},\cdots ,t_{n}). 
(3)"
PRELIMINARY & RETHINKING,0.18415841584158416,"where ti is the i-th word of T .Œ®noise refers to noise contamination in reality, e.g., inserting, substitut-
127"
PRELIMINARY & RETHINKING,0.18514851485148515,"ing and deleting characters with probability p, following [42, 47]. Since these three atomic operations
128"
PRELIMINARY & RETHINKING,0.18613861386138614,"defined in Levenshtein edit distance D are of distance 1, noise rate p can also be deduced by:
129"
PRELIMINARY & RETHINKING,0.18712871287128713,"\la b e l 
{eq:noise ra t e } p = \f r ac {\mathcal {D}(\mathcal {T}, \mathcal {T}^\prime )}{\max (\text {length of } \mathcal {T}, \text {length of } \mathcal {T}^\prime )} 
(4)"
PRELIMINARY & RETHINKING,0.18811881188118812,"As a result, the noisy OVAR task can be formulated as: given V and T ‚Ä≤, the model is expected to
130"
PRELIMINARY & RETHINKING,0.1891089108910891,"maximize the accuracy of action recognition, and even recovering T ‚Ä≤ to T .
131"
PRELIMINARY & RETHINKING,0.1900990099009901,"Robustness of Existing Methods. Fig. 1 evaluates for typical OVAR studies [49, 62], across three
132"
PRELIMINARY & RETHINKING,0.1910891089108911,"public datasets. In terms of Top-1 classification accuracy, existing methods are rather sensitive to
133"
PRELIMINARY & RETHINKING,0.19207920792079208,"noise and show one trend: the larger the noise, the more significant the performance degradation
134"
PRELIMINARY & RETHINKING,0.19306930693069307,"(please see quantitative experiments in Tab. 2). Such poor robustness to the noisy OVAR task, proves
135"
PRELIMINARY & RETHINKING,0.19405940594059407,"excessive idealization of existing studies and also motivates us to fill the research gap.
136"
PRELIMINARY & RETHINKING,0.19504950495049506,"3.2
DENOISER: One Robust OVAR Framework
137"
PRELIMINARY & RETHINKING,0.19603960396039605,"Motivation. Given the complexity of noisy OVAR, we here divide it into two sub-steps: denoising of
138"
PRELIMINARY & RETHINKING,0.19702970297029704,"text descriptions, and then vanilla OVAR. The former is viewed as one generative decoding form, by
139"
PRELIMINARY & RETHINKING,0.19801980198019803,"considering both vision-text information for progressive denoising. While the latter is in one natural
140"
PRELIMINARY & RETHINKING,0.19900990099009902,"discriminative form, by assigning text descriptions to video samples. For the joint optimization of
141"
PRELIMINARY & RETHINKING,0.2,"these two sub-steps, we iterate alternately between generative and discriminative forms. As a result,
142"
PRELIMINARY & RETHINKING,0.200990099009901,"our DENOISER framework progressively tackles the noisy OVAR task.
143"
PRELIMINARY & RETHINKING,0.201980198019802,"Framework. As shown in Fig. 2, our DENOISER framework covers two components: generative
144"
PRELIMINARY & RETHINKING,0.20297029702970298,"sub-step Œ®gene and discriminative sub-step Œ®disc. For Œ®gene, we iteratively refine text descriptions
145"
PRELIMINARY & RETHINKING,0.20396039603960395,"by one decoding process, that is, (T0, T1, ¬∑ ¬∑ ¬∑ , Tn), where n is the index of decoding steps. Upon
146"
PRELIMINARY & RETHINKING,0.20495049504950494,"finishing step i, we will have Ti = (t1, ¬∑ ¬∑ ¬∑ , ti, t‚Ä≤
i+1, ¬∑ ¬∑ ¬∑ , t‚Ä≤
n), where t refers to the decoded version
147"
PRELIMINARY & RETHINKING,0.20594059405940593,"of t, meaning that the i-th word of text descriptions is decoded at step i. We start with T0 = T ‚Ä≤, and
148"
PRELIMINARY & RETHINKING,0.20693069306930692,"finish at Tn to ensure that all words are denoised. While for Œ®disc, we find it identical to vanilla OVAR
149"
PRELIMINARY & RETHINKING,0.2079207920792079,"task and thus leveraging the VLA pipeline [16, 49] for help, which is off-the-shelf and well-studied.
150"
PRELIMINARY & RETHINKING,0.2089108910891089,"Formally, our DENOISER framework tackles noisy OVAR as follows:
  \"
PRELIMINARY & RETHINKING,0.2099009900990099,"ma t hcal {T}_{i } = \ Psi
 _{\ m athrm {gene }} ( \mathcal {T }_{i-1}, \mathcal {Y}_{i-1}, \mathcal {V}), \quad \mathcal {Y}_{i-1} = \Psi _{\mathrm {disc}}(\mathcal {T}_{i-1}, \mathcal {V}) = \Phi _{\mathrm {OVAR}}(\mathcal {T}_{i-1}, \mathcal {V}).
(5)"
PRELIMINARY & RETHINKING,0.21089108910891088,"At the discriminative step, we calculate the matching label Yi‚àí1 to make coarse semantic classification
152"
PRELIMINARY & RETHINKING,0.21188118811881188,"of visual samples, i.e., assign Ti‚àí1 to V. At the generative step, we first propose K text candidates
153"
PRELIMINARY & RETHINKING,0.21287128712871287,"Œ¶prop(Ti‚àí1) for Ti base on Ti‚àí1 to limit the decoding space. Then, to vote for the best candidate, we
154"
PRELIMINARY & RETHINKING,0.21386138613861386,"design two novel modules, namely inter-modal weighting Œ¶inter and intra-modal weighting Œ¶intra.
155"
PRELIMINARY & RETHINKING,0.21485148514851485,"Here, Œ¶inter uses vision information V, while Œ¶intra relies on text information Ti‚àí1.
156"
PRELIMINARY & RETHINKING,0.21584158415841584,"We alternate between the generative and discriminative steps to optimize the decoding result step by
157"
PRELIMINARY & RETHINKING,0.21683168316831683,"step. Please find in Algorithm 1 for comprehensive details.
158"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.21782178217821782,"3.3
Optimization for the DENOISER Framework
159"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2188118811881188,"Discriminative Step consists in calculating cross-modal matching labels Y using Œ®disc. Intuitively,
160"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2198019801980198,"visual samples Vc whose labels Y are assigned to semantic-class c, i.e. argmax Y = c, are those who
161"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2207920792079208,"could help decode Tc,i most efficiently. On the contrary, visual samples from other semantic-classes
162"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22178217821782178,"may have few connections with the current class and thus provide no meaningful aid. Here, we find
163"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22277227722772278,"this process is identical to vanilla OVAR, and hence employs Œ¶OVAR as Œ®disc. We theoretically
164"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22376237623762377,"prove in the Appendix that, Vc is the best set of visual samples to choose from. With Vc defined and
165"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22475247524752476,"argmax Y = c, Œ®gene decodes text descriptions Tc,i for each semantic-class c:
  \"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22574257425742575,"Psi _{\mathrm  { ge n e}}(\mathcal {T} _ {c,i-1
}, \
mathcal {Y}, \ mathcal {V}) = \Psi _{\mathrm {gene}}(\mathcal {T}_{c,i-1}, \mathcal {V}_c) = \argmax _{\mathcal {T}_{c,i}} p(\mathcal {T}_{c,i}|\mathcal {T}_{c,i-1},\mathcal {V}_c).
(6)"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22673267326732674,"Recall tc,i is the i-th word to be decoded, and Tc,i‚àí1 is from last decoding, with the first i ‚àí1
167"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22772277227722773,"words decoded. As we decode word-by-word, choosing the best Tc,i is exactly choosing the best tc,i,
168"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.22871287128712872,"i.e. argmaxTc,i p(Tc,i|Tc,i‚àí1, Vc) = argmaxtc,i p(tc,i|Tc,i‚àí1, Vc), as we do in generative step.
169"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2297029702970297,"Generative Step here consists in, for each semantic-class c, choosing the best tc,i that maximizes
170"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2306930693069307,"p(tc,i|Tc,i‚àí1, Vc). With p(Tc,i‚àí1, Vc) and p(Vc) same for all possible tc,i, we make detailed deriva-
171"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2316831683168317,"tions in the Appendix to show that:
  p"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.23267326732673269,"(t_{c,i} | \ma thc a l {T}_{ c,i-1},  \m a
t"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.23366336633663368,"hcal 
{V}_c) \varpropto p(t_{c ,i}, \mathcal {T}_{c,i-1}, \mathcal {V}_c) \varpropto \prod _{v_j\in \mathcal {V}_c} p(t_{c,i}|v_j) p(\mathcal {T}_{c, i-1}|t_{c,i}, v_j).
(7)"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.23465346534653464,"Here, the error model p(Tc,i‚àí1|tc,i, vj) evaluates how tc,i may be misspelled as t‚Ä≤
c,i, since the i-th
173"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.23564356435643563,"word in Tc,i‚àí1 is still noisy and not decoded. Knowing that errors in text descriptions are independent
174"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.23663366336633662,"of visual samples, it reduces to uni-modal p(Tc,i‚àí1|tc,i). As the error that one may make given the
175"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2376237623762376,"correct text is harder to model while the reverse is much easier, we let p(Tc,i‚àí1|tc,i) ‚àùp(tc,i|Tc,i‚àí1).
176"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2386138613861386,"Please refer to detailed derivations in the Appendix. As a result, our final objective is:
  p"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2396039603960396,"(t_{c,i}|\math
c"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24059405940594059,"al {T
}_{c, i-1} )  \prod"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24158415841584158,"_{v_j
\in \mathcal {V}_c} p(t_{c,i}|v_j) = \Phi _{\mathrm {intra}} \prod _{v_j\in \mathcal {V}_c} \Phi _{\mathrm {inter}} .
(8)"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24257425742574257,"Text Proposals consists in proposing K candidates {tk
i }k for ti with the lowest Levenshtein Edit
178"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24356435643564356,"Distance D(¬∑, t‚Ä≤
i) (a metric of spelling similarity). By replacing original noisy word t‚Ä≤
i in T k
i‚àí1 with
179"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24455445544554455,"{tk
i }k, they form Œ¶prop(Ti‚àí1) = T k
i
= (t1, ¬∑ ¬∑ ¬∑ , ti‚àí1, tk
i , t‚Ä≤
i+1, ¬∑ ¬∑ ¬∑ , t‚Ä≤
n), the K candidates for Ti.
180"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24554455445544554,"The benefit of text proposals is to reduce computing complexity. Since text embeddings are quantized
181"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24653465346534653,"in the semantic space, the search is limited to proposed candidates, rather than in the entire space.
182"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.24752475247524752,"Inter-modal Weighting Œ¶inter = p(tc,i|vj), vj ‚ààVc relies on vision samples from semantic-class c
183"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2485148514851485,"to determine the best tc,i for the next iteration. Concretely, we model the probability of being chosen
184"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2495049504950495,Algorithm 1 DENOISER: Robust Open-Vocabulary Action Recognition
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2504950495049505,"Require: noisy text descriptions T ‚Ä≤, visual samples V, iteration number n, temperature Œª, candidate
number K, edit distance D, open-vocabulary model Œ¶OVAR
T0 ‚ÜêT ‚Ä≤
for i = 1, 2, ¬∑ ¬∑ ¬∑ , n do"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2514851485148515,"for c = 1, 2, ¬∑ ¬∑ ¬∑ , C do
\triangleright Text Proposals
t‚Ä≤
c,i is the i-th word of Tc,i‚àí1, which is noisy and not yet decoded
Select from corpus, K candidates {tk
c,i}k with the smallest D with t‚Ä≤
c,i
Replace t‚Ä≤
c,i with {tk
c,i}k, forming {T k
c,i}k
end for
for j = 1, 2, ¬∑ ¬∑ ¬∑ , |V| do
\triangleright Discriminative Step"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2524752475247525,"c ‚Üêargmax
c
max
k"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.25346534653465347,"exp(S(vj,T k
c,i))
P"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.25445544554455446,"k‚Ä≤ exp(S(vj,T k‚Ä≤
c,i))
Assign vj to class c, vj ‚ààVc
end for
for c = 1, 2, ¬∑ ¬∑ ¬∑ , C do
\triangleright Generative Step"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.25544554455445545,"Œ¶k
intra ‚Üê
exp(‚àíD(tk
c,i,t‚Ä≤
c,i)/Œª)
P"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.25643564356435644,"k‚Ä≤ exp(‚àíD(tk‚Ä≤
c,i,t‚Ä≤
c,i)/Œª)
\triangleright Intra-Modal Weighting"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.25742574257425743,"Œ¶k
inter ‚ÜêQ"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2584158415841584,"vj‚ààVc
exp(S(vj,T k
c,i))
P"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2594059405940594,"k‚Ä≤ exp(S(vj,T k‚Ä≤
c,i))
\triangleright Inter-Modal Weighting"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2603960396039604,"Tc,i ‚ÜêT k
c,i, k = argmaxk Œ¶k
intra √ó Œ¶k
inter
end for
end for"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2613861386138614,"for each proposed candidate to be:
  \"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2623762376237624,"mathbb  {P
}(t_{c, i } = t^ k _ {
c,i}|v_ j
) = \math b b
 {P}(
\"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2633663366336634,"ma thcal {T} _ {c
,i} = \m a thcal {T}^k_{c,i}|v_j) = \frac { \text {exp}(\mathcal {S}(v_j,\mathcal {T}^k_{c, i}))}{\sum _{k^\prime } \text {exp}(\mathcal {S}(v_j,\mathcal {T}^{k^\prime }_{c, i}))}, ~ v_j \in \mathcal {V}_{c}.
(9)"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.26435643564356437,"where S(¬∑, ¬∑) is the cosine similarity between video-text embeddings, both encoded by Œ¶OVAR. The
186"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.26534653465346536,"intuition is that the more unanimously visual samples agree on candidate T k
c,i, the more likely it is the
187"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.26633663366336635,"text descriptions corresponding to semantic-class c. Besides, by letting visual samples vote on T k
c,i
188"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.26732673267326734,"instead of tk
c,i, we take into consideration not only the current word tc,i but also context implicitly.
189"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.26831683168316833,"Intra-modal Weighting Œ¶intra = p(tc,i|Tc,i‚àí1) relies solely on text information to decide the best tc,i
190"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2693069306930693,"for next iteration. Although Œ¶intra may be solved by uni-modal spell-checkers [15] or large language
191"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2702970297029703,"models [1], we here design a simple model by considering only spelling similarity (ignore contexts),
192"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2712871287128713,"to save computing costs. That is, choose tc,i depending solely on t‚Ä≤
c,i instead of on entire Tc,i‚àí1:
  \"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2722772277227723,"mathbb  {P
}(t_{c,i} =  t_{c,i } ^k
|\math
cal {
T}_{c,i-1
}) =  \
mathbb 
{"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2732673267326733,"P} (t_{c,i} =
 t_{ c,
i}^k|t^\prime _{c,i}) = \frac {\text {exp} (-\mathcal {D}(t^k_{c,i}, t^\prime _{c,i})/\lambda )}{\sum _{k^\prime } \text {exp} (-\mathcal {D}(t^{k^\prime }_{c,i}, t^\prime _{c,i})/\lambda )}.
(10)"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.2742574257425743,"The intuition is that, the more similar a word candidate tk
c,i is, compared to the noisy word t‚Ä≤
c,i, the
194"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.27524752475247527,"more likely it is the corresponding denoised word. Here, we introduce one temperature parameter Œª to
195"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.27623762376237626,"balance Œ¶intra and Œ¶inter. A larger Œª indicates that different edit distance gives similar probabilities,
196"
OPTIMIZATION FOR THE DENOISER FRAMEWORK,0.27722772277227725,"meaning that we rely more on visual samples for decision, and vice versa.
197"
EXPERIMENTS,0.27821782178217824,"4
Experiments
198"
EXPERIMENTS,0.27920792079207923,"Typical Models for Vanilla OVAR. To illustrate the generalizability of our framework, we leverage
199"
EXPERIMENTS,0.2801980198019802,"two typical models from the VLA pipeline as Œ¶OVAR, that is, ActionCLIP [49] and XCLIP [62].
200"
EXPERIMENTS,0.2811881188118812,"These two models adopt hand-crafted prompts and visual-conditioned prompt tuning, respectively.
201"
EXPERIMENTS,0.28217821782178215,"Under both models, we choose ViT-B/16-32F as the network backbones, for simplicity.
202"
EXPERIMENTS,0.28316831683168314,"Datasets. HMDB51 [26] contains 7k videos covering 51 action categories. UCF101 [45] contains
203"
EXPERIMENTS,0.28415841584158413,"13k videos spanning 101 action categories. Kinetics700 [3] (K700) is simply an extension of K400,
204"
EXPERIMENTS,0.2851485148514851,"with around 650k video clips sourced from YouTube. To partition these datasets for open-vocabulary
205"
EXPERIMENTS,0.2861386138613861,"action recognition, this paper follows the standard consensus [49, 62], for the sake of fairness.
206"
EXPERIMENTS,0.2871287128712871,"Figure 3:
Statistics for Noises in Reality.
Text noises may be classified into 4 types: in-
serting, substituting, swapping, and deleting
characters.[2] In terms of edit distance, based on
TOEFL-Spell dataset[10], most of the text noises
have an edit distance = 1 compared to the clean
version. Nevertheless, the distribution tends to
be positively skewed towards larger noise."
EXPERIMENTS,0.2881188118811881,Insertion 14%
EXPERIMENTS,0.2891089108910891,Substitution
EXPERIMENTS,0.2900990099009901,"37%
Deletion 33% Swap 16%"
EXPERIMENTS,0.29108910891089107,NOISE TYPE
EXPERIMENTS,0.29207920792079206,"=1
83%"
EXPERIMENTS,0.29306930693069305,"=2
13% =3
3% >3
1%"
EXPERIMENTS,0.29405940594059404,EDIT DISTANCE
EXPERIMENTS,0.29504950495049503,"Table 1: Comparisons between Various Competitors. Using ActionCLIP [49] as Œ¶OVAR while
evaluating on UCF101, we compare with statistical text spell-checkers (PySpellChecker [15]), neural
based ones (Bert from NeuSpell) [13], and GPT 3.5 [1]. Our method remarkably outperforms others
in terms of Top-1 classification accuracy, and semantic similarity of recovered text descriptions."
EXPERIMENTS,0.296039603960396,"Noise Type
Noise Rate
Competitors
Top-1 Acc
Label Acc
Semantic Similarity"
EXPERIMENTS,0.297029702970297,"‚Äì
0%
Upper Bound
66.3
100
100"
EXPERIMENTS,0.298019801980198,"Real
‚àº5.52%"
EXPERIMENTS,0.299009900990099,"GPT 3.5 [1]
61.2¬±1.4
74.7¬±1.9
97.1¬±0.4
Bert (NeuSpell) [13]
56.0¬±1.1
64.7¬±2.0
94.5¬±0.4
PySpellChecker [15]
59.9¬±1.2
79.6¬±1.6
96.7¬±0.3
Ours
61.5¬±0.7
82.3¬±1.6
97.2¬±0.3"
EXPERIMENTS,0.3,Simulated 5%
EXPERIMENTS,0.300990099009901,"GPT 3.5 [1]
59.7¬±1.2
47.6¬±3.1
95.9¬±0.4
Bert (NeuSpell) [13]
56.6¬±0.5
66.2¬±2.3
94.6¬±0.4
PySpellChecker [15]
60.9¬±1.1
82.5¬±2.9
97.1¬±0.4
Ours
63.8¬±0.7
86.4¬±2.3
97.7¬±0.2 10%"
EXPERIMENTS,0.30198019801980197,"GPT 3.5 [1]
58.5¬±1.3
51.6¬±2.3
95.8¬±0.3
Bert (NeuSpell) [13]
51.0¬±0.5
50.4¬±3.6
91.6¬±0.6
PySpellChecker [15]
55.7¬±1.1
69.3¬±1.5
94.8¬±0.3
Ours
61.2¬±0.8
75.9¬±1.9
96.4¬±0.3"
EXPERIMENTS,0.30297029702970296,"Metric. We use three metrics for full evaluations from multiple perspectives. Top-1 Acc refers to
207"
EXPERIMENTS,0.30396039603960395,"the top-1 classification accuracy of noisy open-vocabulary action recognition. Label Acc counts the
208"
EXPERIMENTS,0.30495049504950494,"percentage of denoised text descriptions that match exactly with ground truth. Semantic Similarity
209"
EXPERIMENTS,0.30594059405940593,"calculates the cosine similarity of embeddings, between denoised and clean text descriptions. Label
210"
EXPERIMENTS,0.3069306930693069,"Acc and Semantic Similarity measure how well noisy text descriptions are recovered.
211"
EXPERIMENTS,0.3079207920792079,"Implementations. We set the proposal number K = 10. Intra-modal weighting and inter-modal
212"
EXPERIMENTS,0.3089108910891089,"weighting are both used to determine the best candidate. Temperature Œª follows a linear schedule
213"
EXPERIMENTS,0.3099009900990099,"from 0.01 to 1. We use the same corpus as in PySpellChecker, which contains 70317 English words,
214"
EXPERIMENTS,0.3108910891089109,"for text proposals. For typical OVAR methods [49, 62], we choose the ViT-B/16-32F checkpoint
215"
EXPERIMENTS,0.3118811881188119,"pretrained on K400 [24] to evaluate their zero-shot robustness on HMDB51 [27], UCF101 [46] and
216"
EXPERIMENTS,0.31287128712871287,"K700 [44]. Since K700 and K400 have overlapped categories, we exclude them when evaluating on
217"
EXPERIMENTS,0.31386138613861386,"K700. For UCF101, we use the separated lowercase text label. All ablation studies are conducted on
218"
EXPERIMENTS,0.31485148514851485,"UCF101 under 20% noise. For statistical significance, We do each simulation 10 times and report the
219"
EXPERIMENTS,0.31584158415841584,"mean and confidence interval of 95%. All experiments are done using a single RTX 3090.
220"
EXPERIMENTS,0.31683168316831684,"4.1
Statistics on Noise Type/Rate for Text Descriptions
221"
EXPERIMENTS,0.3178217821782178,"Real Noise. We adopt two large-scale corpora [11, 10] of misspellings to analyze noise type in text
222"
EXPERIMENTS,0.3188118811881188,"descriptions. As shown in Fig. 3, the conclusion is similar to the NLP community [42, 47], i.e., three
223"
EXPERIMENTS,0.3198019801980198,"atomic types of noise are inserting, substituting, and deleting text characters. More complicated noise
224"
EXPERIMENTS,0.3207920792079208,"patterns, e.g. swaping, can be constructed by mixing atomic noise types. Then, following previous
225"
EXPERIMENTS,0.3217821782178218,"literature, we quantify noise rate through Levenshtein Edit Distance, a generally accepted metric,
226"
EXPERIMENTS,0.3227722772277228,"to calculate the occurrence number of atomic noise types. Specifically, GitHub Typo Corpus [11]
227"
EXPERIMENTS,0.3237623762376238,"contains over 350k edits of typos from GitHub. The average noise rate (per sentence) is 3.3%.
228"
EXPERIMENTS,0.32475247524752476,"Nevertheless, the distribution is highly positively skewed (skewness = 2.9). For the worst 5% cases,
229"
EXPERIMENTS,0.32574257425742575,"the noise rate (per sentence) is larger than 9.4%. TOEFL-Spell Corpus [10] samples essays written
230"
EXPERIMENTS,0.32673267326732675,"by candidates from various language backgrounds in TOEFL¬Æ iBT test. There are, on average, 6.9
231"
EXPERIMENTS,0.32772277227722774,"spelling mistakes per essay. For misspelled words, the noise rate (per word) is on average 16.0%.
232"
EXPERIMENTS,0.3287128712871287,"Table 2: Comparison Across Datasets and Models. On three standard datasets, facing multiple
noise types (real or simulated), and under various noise rates, our DENOISER consistently improves
the performance for noisy OVAR, regardless of underlying OVAR methods Œ¶OVAR."
EXPERIMENTS,0.3297029702970297,"Dataset
Noise Type
Noise Rate"
EXPERIMENTS,0.3306930693069307,"Œ¶OVAR: Typical Models for Vanilla OVAR task
ActionCLIP [49]
XCLIP [62]
w/o Ours
w Ours
w/o Ours
w Ours"
EXPERIMENTS,0.3316831683168317,UCF101
EXPERIMENTS,0.3326732673267327,"Upper Bound
66.3
68.6
Real
‚àº5.52%
54.0¬±2.3
61.5¬±0.7
53.8¬±2.7
63.4¬±0.9"
EXPERIMENTS,0.3336633663366337,"Simulated
5%
54.9¬±1.8
63.2¬±0.7
55.6¬±2.2
64.2¬±1.4
10%
47.3¬±1.4
61.2¬±1.2
46.4¬±1.3
62.9¬±2.3"
EXPERIMENTS,0.3346534653465347,HMDB51
EXPERIMENTS,0.33564356435643566,"Upper Bound
46.2
45.0
Real
‚àº6.71%
37.6¬±1.6
40.0¬±1.4
35.3¬±1.5
38.4¬±1.4"
EXPERIMENTS,0.33663366336633666,"Simulated
5%
39.4¬±1.4
41.3¬±1.4
37.5¬±1.8
39.7¬±1.0
10%
35.2¬±2.3
39.6¬±1.4
31.8¬±2.2
37.3¬±1.5 K700"
EXPERIMENTS,0.33762376237623765,"Upper Bound
40.2
49.3
Real
‚àº5.47%
30.8¬±0.51
35.9¬±0.4
35.6¬±0.6
43.5¬±0.7"
EXPERIMENTS,0.33861386138613864,"Simulated
5%
31.5¬±0.5
36.8¬±0.3
36.7¬±0.9
44.1¬±0.6
10%
25.4¬±0.8
35.3¬±0.5
27.5¬±0.7
41.8¬±0.9"
EXPERIMENTS,0.33960396039603963,"Noise Scenarios. In the ""Simulated"" noise type, we mix three atomic noises: insertion, substitution,
233"
EXPERIMENTS,0.3405940594059406,"and deletion. Concretely, for each character, we perturb it with probability p. For each perturbation,
234"
EXPERIMENTS,0.3415841584158416,"it will be insertion, substitution, and deletion with equal probability. To further ensure real-world
235"
EXPERIMENTS,0.3425742574257426,"generalizability, we ask GPT3.5 to give examples of perturbation according to real-world scenarios.
236"
EXPERIMENTS,0.3435643564356436,"We mix them into simulated noises. Noise rate p of the ""Real"" noise type is estimated with Eq. (3).
237"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3445544554455445,"4.2
Comparison with State-of-the-art Methods
238"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3455445544554455,"Comparison to Competitors. Tab. 1 compares from three axes: Top-1 Acc of Œ¶OVAR after correction,
239"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3465346534653465,"Label Acc and Semantic Similarity. PySpellChecker is a uni-modal statistical model that corrects
240"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3475247524752475,"each word by edit distance and appearance frequency. Bert (NeuSpell) [13] employs a uni-modal
241"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3485148514851485,"Bert-based model to translate noisy text descriptions into clean ones. We also ask GPT 3.5 to denoise
242"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3495049504950495,"text descriptions using the prompt ‚ÄúThe following words may contain spelling errors by deleting,
243"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3504950495049505,"inserting, and substituting letters. You are a corrector of spelling errors. Give only the answer
244"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.35148514851485146,"without explication. What is the correct spelling of the action of <noisy text description>?‚Äù. Our
245"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.35247524752475246,"method outperforms all competitors by large margins, which is impressive because our method is
246"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.35346534653465345,"unsupervised without prior knowledge other than those contained in the OVAR model. Note that the
247"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.35445544554455444,"output of GPT 3.5 tends to be unstable depending on prompts, which requires manual cleaning to
248"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.35544554455445543,"remove irrelevant parts contained in the output, thus impeding real-world usage.
249"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3564356435643564,"Comparisons Across Datasets/Models. Tab. 2 compares Top-1 Acc to further reveal our solution is
250"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3574257425742574,"scalable/generalizable. Under various noise rates, our model is robust to achieve huge improvements.
251"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3584158415841584,"In terms of scalability across models, our method is not only applicable to hand-crafted prompts as in
252"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3594059405940594,"ActionCLIP but also to learnable visual-conditioned prompts as in XCLIP. Furthermore, we notice
253"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3603960396039604,"that, whenever XCLIP outperforms ActionCLIP, our method also yields a better result. A better
254"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.3613861386138614,"visual encoder and well-tuned prompt may significantly increase our performance, showing that our
255"
COMPARISON WITH STATE-OF-THE-ART METHODS,0.36237623762376237,"method‚Äôs upper limit could become higher, as the community continues to train better OVAR models.
256"
ABLATION STUDY,0.36336633663366336,"4.3
Ablation Study
257"
ABLATION STUDY,0.36435643564356435,"Inter-modal Weighting Œ¶inter & Intra-modal Weighting Œ¶intra. Tab. 3 shows that, both Œ¶inter
258"
ABLATION STUDY,0.36534653465346534,"and Œ¶intra contribute to denoising text descriptions and to improving the robustness of underlying
259"
ABLATION STUDY,0.36633663366336633,"Œ¶OVAR. In terms of Top-1 Acc and Semantic Similarity, Œ¶inter performs better than Œ¶intra, since
260"
ABLATION STUDY,0.3673267326732673,"Œ¶inter uses visual information as one direct optimization guideline to improve video recognition.
261"
ABLATION STUDY,0.3683168316831683,"While Œ¶intra performs better in terms of Label Acc, which focuses more on spelling correctness.
262"
ABLATION STUDY,0.3693069306930693,"Besides, Œ¶inter and Œ¶intra turn out to be complementary: visual information helps to understand
263"
ABLATION STUDY,0.3702970297029703,"noisy text descriptions; while textual information prevents the model from being misled by visual
264"
ABLATION STUDY,0.3712871287128713,"samples. We achieve the best performance when combining these two weightings.
265"
ABLATION STUDY,0.3722772277227723,"Table 3: Ablations for Inter-modal Weighting Œ¶Inter, Intra-modal Weighting Œ¶Inter, Schedule of
Temperature Œª. Œ¶Inter alone outperforms Œ¶Intra. Both contribute to correcting class texts, and give
the best results when combined. Linear schedule of balancing factor Œª outperforms the constant one,
meaning that it helps to rely more on Œ¶Intra at first, and then gradually switch to Œ¶Inter."
ABLATION STUDY,0.37326732673267327,"Œ¶Inter
Œ¶Intra
Schedule Œª
Top-1 Acc
Label Acc
Semantic Similarity
A1
‚úì
/
48.1¬±2.2
38.2¬±2.5
88.9¬±0.4
A2
‚úì
/
52.9¬±1.4
34.1¬±2.4
89.1¬±0.6
A3
‚úì
‚úì
Constant
54.5¬±2.5
54.9¬±4.5
92.4¬±0.8
A4
‚úì
‚úì
Linear
55.2¬±1.5
55.1¬±3.0
92.9¬±0.6"
ABLATION STUDY,0.37425742574257426,"Figure 4: We evaluate on UCF101 by using ActionCLIP as Œ¶OVAR. Left: Ablation Study on Noise
Type. ‚ÄúMixed‚Äù means that all types of text noises: ‚ÄúSubstitute‚Äù, ‚ÄúInsert‚Äù, ‚ÄúDelete‚Äù take place with
equal probability. Our DENOISER shows good resilience, especially against noises of inserting or
substituting. Right: Ablation Study on Proposal Number K. As K increases, Top-1 Acc increases
and converges gradually towards the upper bound, but it also brings heavier computing costs."
ABLATION STUDY,0.37524752475247525,"Temperature Schedule Œª balances intra-modal weighting and inter-modal weighting. One larger Œª
266"
ABLATION STUDY,0.37623762376237624,"indicates more reliance on inter-modal weighting. ‚ÄúLinear‚Äù means that Œª augments from 0.01 to 1
267"
ABLATION STUDY,0.37722772277227723,"linearly. Tab. 3 reports that it is beneficial to rely more on intra-modal at the beginning of decoding,
268"
ABLATION STUDY,0.3782178217821782,"and then gradually turn to inter-modal for more help. This indicates that, when text noises are high,
269"
ABLATION STUDY,0.3792079207920792,"Œ¶intra offers more help; when text noises are slight, Œ¶inter could help more.
270"
ABLATION STUDY,0.3801980198019802,"Noise Type. Fig. 4 Left reports our robustness under various noise types/rates. ‚ÄúMixed‚Äù means that
271"
ABLATION STUDY,0.3811881188118812,"three noise types: ‚ÄúSubstitute‚Äù, ‚ÄúInsert‚Äù, ‚ÄúDelete‚Äù are equally possible to appear. Our method shows
272"
ABLATION STUDY,0.3821782178217822,"remarkable resilience when texts are perturbed by inserting or substituting characters. Performance
273"
ABLATION STUDY,0.3831683168316832,"degradation is observed when texts are perturbed by deleting characters. It is reasonable, as deleting
274"
ABLATION STUDY,0.38415841584158417,"characters causes huge information loss, making the model difficult to recover clean text descriptions.
275"
ABLATION STUDY,0.38514851485148516,"Number of Candidates K. Fig. 4 Right shows as K increases, inter-modal weighting can reveal
276"
ABLATION STUDY,0.38613861386138615,"its full power, hence improving performance. Otherwise, if a good candidate is excluded from the
277"
ABLATION STUDY,0.38712871287128714,"proposal stage due to a small K, it can be selected by neither of the inter- or intra-modal weighting,
278"
ABLATION STUDY,0.38811881188118813,"thus decreasing performance. Moreover, the performance tends towards one plateau, showing a
279"
ABLATION STUDY,0.3891089108910891,"decreasing marginal contribution of more proposals to performance. Since a larger K means more
280"
ABLATION STUDY,0.3900990099009901,"computing costs for text encoding, we select K = 10 by default to make reasonable trade-offs.
281"
CONCLUSION,0.3910891089108911,"5
Conclusion
282"
CONCLUSION,0.3920792079207921,"This paper investigates how noises in class-text descriptions negatively interference OVAR; and
283"
CONCLUSION,0.3930693069306931,"one novel framework DENOISER is proposed for solutions. By incorporating visual information
284"
CONCLUSION,0.3940594059405941,"during denoising, we clarify the ambiguity induced by short and context-lacking text descriptions; by
285"
CONCLUSION,0.39504950495049507,"iteratively refining the denoised output through one generative-discriminative process, we mitigate
286"
CONCLUSION,0.39603960396039606,"cascaded errors which may propagate from spell-checking models to outputs of OVAR model. We
287"
CONCLUSION,0.39702970297029705,"conduct extensive experiments to demonstrate the generalizability of DENOISER across multiple
288"
CONCLUSION,0.39801980198019804,"models and datasets, and also show our superiority over uni-modal spell-checking solutions.
289"
CONCLUSION,0.39900990099009903,"Limitations. 1) We focus more on spelling noises; while in the real world, text noises can be more
290"
CONCLUSION,0.4,"complex, involving semantic ambiguity. Equipping DENOISER with large language models may
291"
CONCLUSION,0.400990099009901,"be a feasible solution. 2) Using more text candidates or visual samples brings better results for
292"
CONCLUSION,0.401980198019802,"DENOISER, but also costs more. There is a trade-off between performance and computational cost.
293"
REFERENCES,0.402970297029703,"References
294"
REFERENCES,0.403960396039604,"[1] Gpt-3.5 turbo, https://platform.openai.com/docs/models/gpt-3-5-turbo/
295"
REFERENCES,0.404950495049505,"[2] Al-Oudat, A.: Spelling errors in english writing committed by english-major students at bau.
296"
REFERENCES,0.40594059405940597,"Journal of Literature, Languages and Linguistics 32(2) (2017)
297"
REFERENCES,0.4069306930693069,"[3] Carreira, J., Noland, E., Hillier, C., Zisserman, A.: A short note on the kinetics-700 human
298"
REFERENCES,0.4079207920792079,"action dataset. arXiv preprint arXiv:1907.06987 (2019)
299"
REFERENCES,0.4089108910891089,"[4] Chen, M., Chen, X., Zhai, Z., Ju, C., Hong, X., Lan, J., Xiao, S.: Wear-any-way: Manipulable
300"
REFERENCES,0.4099009900990099,"virtual try-on via sparse correspondence alignment. arXiv preprint arXiv:2403.12965 (2024)
301"
REFERENCES,0.41089108910891087,"[5] Chen, X., Chen, S., Yao, J., Zheng, H., Zhang, Y., Tsang, I.W.: Learning on attribute-missing
302"
REFERENCES,0.41188118811881186,"graphs. IEEE transactions on pattern analysis and machine intelligence (2020)
303"
REFERENCES,0.41287128712871285,"[6] Chen, X., Cheng, Z., Yao, J., Ju, C., Huang, W., Lan, J., Zeng, X., Xiao, S.: Enhancing
304"
REFERENCES,0.41386138613861384,"cross-domain click-through rate prediction via explicit feature augmentation. arXiv preprint
305"
REFERENCES,0.41485148514851483,"arXiv:2312.00078 (2023)
306"
REFERENCES,0.4158415841584158,"[7] Cheng, F., Wang, X., Lei, J., Crandall, D., Bansal, M., Bertasius, G.: Vindlu: A recipe for
307"
REFERENCES,0.4168316831683168,"effective video-and-language pretraining. In: Proceedings of the IEEE Conference on Computer
308"
REFERENCES,0.4178217821782178,"Vision and Pattern Recognition (2023)
309"
REFERENCES,0.4188118811881188,"[8] Cheng, Z., Xiao, S., Zhai, Z., Zeng, X., Huang, W.: Mixer: Image to multi-modal retrieval
310"
REFERENCES,0.4198019801980198,"learning for industrial application. arXiv preprint arXiv:2305.03972 (2023)
311"
REFERENCES,0.4207920792079208,"[9] Dinan, E., Humeau, S., Chintagunta, B., Weston, J.: Build it break it fix it for dialogue safety:
312"
REFERENCES,0.42178217821782177,"Robustness from adversarial human attack. arXiv preprint arXiv:1908.06083 (2019)
313"
REFERENCES,0.42277227722772276,"[10] Flor, M., Fried, M., Rozovskaya, A.: A benchmark corpus of english misspellings and a
314"
REFERENCES,0.42376237623762375,"minimally-supervised model for spelling correction. In: Proceedings of the Fourteenth Workshop
315"
REFERENCES,0.42475247524752474,"on Innovative Use of NLP for Building Educational Applications. pp. 76‚Äì86 (2019)
316"
REFERENCES,0.42574257425742573,"[11] Hagiwara, M., Mita, M.: Github typo corpus: A large-scale multilingual dataset of misspellings
317"
REFERENCES,0.4267326732673267,"and grammatical errors. arXiv preprint arXiv:1911.12893 (2019)
318"
REFERENCES,0.4277227722772277,"[12] Hu, X., Zhang, K., Xia, L., Chen, A., Luo, J., Sun, Y., Wang, K., Qiao, N., Zeng, X., Sun,
319"
REFERENCES,0.4287128712871287,"M., et al.: Reclip: Refine contrastive language image pre-training with source free domain
320"
REFERENCES,0.4297029702970297,"adaptation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
321"
REFERENCES,0.4306930693069307,"Vision. pp. 2994‚Äì3003 (2024)
322"
REFERENCES,0.4316831683168317,"[13] Jayanthi, S.M., Pruthi, D., Neubig, G.: Neuspell: A neural spelling correction toolkit. arXiv
323"
REFERENCES,0.43267326732673267,"preprint arXiv:2010.11085 (2020)
324"
REFERENCES,0.43366336633663366,"[14] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V., Sung, Y., Li, Z., Duerig,
325"
REFERENCES,0.43465346534653465,"T.: Scaling up visual and vision-language representation learning with noisy text supervision.
326"
REFERENCES,0.43564356435643564,"In: Proceedings of the International Conference on Machine Learning (2021)
327"
REFERENCES,0.43663366336633663,"[15] Jiang, Y.G., Liu, J., Zamir, A.R., Toderici, G., Laptev, I., Shah, M., Sukthankar, R.:
328"
REFERENCES,0.4376237623762376,"pyspellchecker: Action recognition with a large number of classes, https://github.com/
329"
REFERENCES,0.4386138613861386,"barrust/pyspellchecker/
330"
REFERENCES,0.4396039603960396,"[16] Ju, C., Han, T., Zheng, K., Zhang, Y., Xie, W.: Prompting visual-language models for efficient
331"
REFERENCES,0.4405940594059406,"video understanding. In: Proceedings of the European Conference on Computer Vision. Springer
332"
REFERENCES,0.4415841584158416,"(2022)
333"
REFERENCES,0.4425742574257426,"[17] Ju, C., Li, Z., Zhao, P., Zhang, Y., Zhang, X., Tian, Q., Wang, Y., Xie, W.: Multi-modal
334"
REFERENCES,0.44356435643564357,"prompting for low-shot temporal action localization. arXiv preprint arXiv:2303.11732 (2023)
335"
REFERENCES,0.44455445544554456,"[18] Ju, C., Wang, H., Li, Z., Chen, X., Zhai, Z., Huang, W., Xiao, S.: Turbo: Informativity-driven
336"
REFERENCES,0.44554455445544555,"acceleration plug-in for vision-language models. arXiv preprint arXiv:2312.07408 (2023)
337"
REFERENCES,0.44653465346534654,"[19] Ju, C., Wang, H., Liu, J., Ma, C., Zhang, Y., Zhao, P., Chang, J., Tian, Q.: Constraint and union
338"
REFERENCES,0.44752475247524753,"for partially-supervised temporal sentence grounding. arXiv preprint arXiv:2302.09850 (2023)
339"
REFERENCES,0.4485148514851485,"[20] Ju, C., Zhao, P., Chen, S., Zhang, Y., Wang, Y., Tian, Q.: Divide and conquer for single-frame
340"
REFERENCES,0.4495049504950495,"temporal action localization. In: Proceedings of the International Conference on Computer
341"
REFERENCES,0.4504950495049505,"Vision (2021)
342"
REFERENCES,0.4514851485148515,"[21] Ju, C., Zhao, P., Chen, S., Zhang, Y., Zhang, X., Wang, Y., Tian, Q.: Adaptive mutual supervision
343"
REFERENCES,0.4524752475247525,"for weakly-supervised temporal action localization. IEEE Transactions on Multimedia (2022)
344"
REFERENCES,0.4534653465346535,"[22] Ju, C., Zhao, P., Zhang, Y., Wang, Y., Tian, Q.: Point-level temporal action localization: Bridging
345"
REFERENCES,0.45445544554455447,"fully-supervised proposals to weakly-supervised losses. arXiv preprint arXiv:2012.08236 (2020)
346"
REFERENCES,0.45544554455445546,"[23] Ju, C., Zheng, K., Liu, J., Zhao, P., Zhang, Y., Chang, J., Tian, Q., Wang, Y.: Distilling vision-
347"
REFERENCES,0.45643564356435645,"language pre-training to collaborate with weakly-supervised temporal action localization. In:
348"
REFERENCES,0.45742574257425744,"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2023)
349"
REFERENCES,0.45841584158415843,"[24] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F.,
350"
REFERENCES,0.4594059405940594,"Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint
351"
REFERENCES,0.4603960396039604,"arXiv:1705.06950 (2017)
352"
REFERENCES,0.4613861386138614,"[25] Keller, Y., Mackensen, J., Eger, S.: Bert-defense: A probabilistic model based on bert to combat
353"
REFERENCES,0.4623762376237624,"cognitively inspired orthographic adversarial attacks. arXiv preprint arXiv:2106.01452 (2021)
354"
REFERENCES,0.4633663366336634,"[26] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: HMDB: A large video database
355"
REFERENCES,0.4643564356435644,"for human motion recognition. In: Proceedings of the International Conference on Computer
356"
REFERENCES,0.46534653465346537,"Vision (2011)
357"
REFERENCES,0.46633663366336636,"[27] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: HMDB: a large video database for
358"
REFERENCES,0.46732673267326735,"human motion recognition. In: Proceedings of the International Conference on Computer Vision
359"
REFERENCES,0.46831683168316834,"(ICCV) (2011)
360"
REFERENCES,0.4693069306930693,"[28] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with
361"
REFERENCES,0.47029702970297027,"frozen image encoders and large language models. In: International conference on machine
362"
REFERENCES,0.47128712871287126,"learning. PMLR (2023)
363"
REFERENCES,0.47227722772277225,"[29] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified
364"
REFERENCES,0.47326732673267324,"vision-language understanding and generation. In: International conference on machine learning.
365"
REFERENCES,0.47425742574257423,"pp. 12888‚Äì12900. PMLR (2022)
366"
REFERENCES,0.4752475247524752,"[30] Liu, H., Zhang, Y., Wang, Y., Lin, Z., Chen, Y.: Joint character-level word embedding and
367"
REFERENCES,0.4762376237623762,"adversarial stability training to defend adversarial text. In: Proceedings of the AAAI Conference
368"
REFERENCES,0.4772277227722772,"on Artificial Intelligence (2020)
369"
REFERENCES,0.4782178217821782,"[31] Liu, J., Ju, C., Ma, C., Wang, Y., Wang, Y., Zhang, Y.: Audio-aware query-enhanced transformer
370"
REFERENCES,0.4792079207920792,"for audio-visual segmentation. arXiv preprint arXiv:2307.13236 (2023)
371"
REFERENCES,0.4801980198019802,"[32] Liu, J., Ju, C., Xie, W., Zhang, Y.: Exploiting transformation invariance and equivariance
372"
REFERENCES,0.48118811881188117,"for self-supervised sound localisation. In: Proceedings of ACM International Conference on
373"
REFERENCES,0.48217821782178216,"Multimedia (2022)
374"
REFERENCES,0.48316831683168315,"[33] Liu, J., Liu, Y., Zhang, F., Ju, C., Zhang, Y., Wang, Y.: Audio-visual segmentation via unlabeled
375"
REFERENCES,0.48415841584158414,"frame exploitation. arXiv preprint arXiv:2403.11074 (2024)
376"
REFERENCES,0.48514851485148514,"[34] Liu, J., Wang, Y., Ju, C., Ma, C., Zhang, Y., Xie, W.: Annotation-free audio-visual segmentation.
377"
REFERENCES,0.4861386138613861,"In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
378"
REFERENCES,0.4871287128712871,"(2024)
379"
REFERENCES,0.4881188118811881,"[35] Liu, K., Liu, X., Yang, A., Liu, J., Su, J., Li, S., She, Q.: A robust adversarial training approach
380"
REFERENCES,0.4891089108910891,"to machine reading comprehension. In: Proceedings of the AAAI Conference on Artificial
381"
REFERENCES,0.4900990099009901,"Intelligence (2020)
382"
REFERENCES,0.4910891089108911,"[36] Ma, C., Yang, Y., Ju, C., Zhang, F., Liu, J., Wang, Y., Zhang, Y., Wang, Y.: Diffusionseg:
383"
REFERENCES,0.49207920792079207,"Adapting diffusion towards unsupervised object discovery. arXiv preprint arXiv:2303.09813
384"
REFERENCES,0.49306930693069306,"(2023)
385"
REFERENCES,0.49405940594059405,"[37] Ma, C., Yang, Y., Ju, C., Zhang, F., Zhang, Y., Wang, Y.: Open-vocabulary semantic segmen-
386"
REFERENCES,0.49504950495049505,"tation via attribute decomposition-aggregation. Advances in Neural Information Processing
387"
REFERENCES,0.49603960396039604,"Systems (2024)
388"
REFERENCES,0.497029702970297,"[38] Nag, S., Zhu, X., Song, Y.Z., Xiang, T.: Zero-shot temporal action detection via vision-language
389"
REFERENCES,0.498019801980198,"prompting. In: Proceedings of the European Conference on Computer Vision. Springer (2022)
390"
REFERENCES,0.499009900990099,"[39] Pruthi, D., Dhingra, B., Lipton, Z.C.: Combating adversarial misspellings with robust word
391"
REFERENCES,0.5,"recognition. arXiv preprint arXiv:1905.11268 (2019)
392"
REFERENCES,0.500990099009901,"[40] Qian, R., Li, Y., Xu, Z., Yang, M.H., Belongie, S., Cui, Y.: Multimodal open-vocabulary video
393"
REFERENCES,0.501980198019802,"classification via pre-trained vision and language models. arXiv preprint arXiv:2207.07646
394"
REFERENCES,0.502970297029703,"(2022)
395"
REFERENCES,0.503960396039604,"[41] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,
396"
REFERENCES,0.504950495049505,"A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language
397"
REFERENCES,0.505940594059406,"supervision. In: Proceedings of the International Conference on Machine Learning. PMLR
398"
REFERENCES,0.5069306930693069,"(2021)
399"
REFERENCES,0.5079207920792079,"[42] Rychalska, B., Basaj, D., Gosiewska, A., Biecek, P.: Models in the wild: On corruption robust-
400"
REFERENCES,0.5089108910891089,"ness of neural nlp systems. In: Neural Information Processing: 26th International Conference,
401"
REFERENCES,0.5099009900990099,"ICONIP 2019, Sydney, NSW, Australia, December 12‚Äì15, 2019, Proceedings, Part III 26.
402"
REFERENCES,0.5108910891089109,"Springer (2019)
403"
REFERENCES,0.5118811881188119,"[43] Sakaguchi, K., Duh, K., Post, M., Van Durme, B.: Robsut wrod reocginiton via semi-character
404"
REFERENCES,0.5128712871287129,"recurrent neural network. In: Proceedings of the AAAI Conference on Artificial Intelligence
405"
REFERENCES,0.5138613861386139,"(2017)
406"
REFERENCES,0.5148514851485149,"[44] Smaira, L., Carreira, J., Noland, E., Clancy, E., Wu, A., Zisserman, A.: A short note on the
407"
REFERENCES,0.5158415841584159,"kinetics-700-2020 human action dataset. arXiv preprint arXiv:2010.10864 (2020)
408"
REFERENCES,0.5168316831683168,"[45] Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions classes from
409"
REFERENCES,0.5178217821782178,"videos in the wild. arXiv preprint arXiv:1212.0402 (2012)
410"
REFERENCES,0.5188118811881188,"[46] Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from
411"
REFERENCES,0.5198019801980198,"videos in the wild. arXiv preprint arXiv:1212.0402 (2012)
412"
REFERENCES,0.5207920792079208,"[47] Sun, S., Gu, J., Gong, S.: Benchmarking robustness of text-image composed retrieval. arXiv
413"
REFERENCES,0.5217821782178218,"preprint arXiv:2311.14837 (2023)
414"
REFERENCES,0.5227722772277228,"[48] Wang, H., Yan, C., Wang, S., Jiang, X., Tang, X., Hu, Y., Xie, W., Gavves, E.: Towards
415"
REFERENCES,0.5237623762376238,"open-vocabulary video instance segmentation. In: Proceedings of the International Conference
416"
REFERENCES,0.5247524752475248,"on Computer Vision (2023)
417"
REFERENCES,0.5257425742574258,"[49] Wang, M., Xing, J., Liu, Y.: Actionclip: A new paradigm for video action recognition. arXiv
418"
REFERENCES,0.5267326732673268,"preprint arXiv:2109.08472 (2021)
419"
REFERENCES,0.5277227722772277,"[50] Wang, W., Wang, R., Wang, L., Wang, Z., Ye, A.: Towards a robust deep neural network in
420"
REFERENCES,0.5287128712871287,"texts: A survey. arXiv preprint arXiv:1902.07285 (2019)
421"
REFERENCES,0.5297029702970297,"[51] Wang, Z., Wang, H.: Defense of word-level adversarial attacks via random substitution encoding.
422"
REFERENCES,0.5306930693069307,"In: Knowledge Science, Engineering and Management: 13th International Conference, KSEM
423"
REFERENCES,0.5316831683168317,"2020, Hangzhou, China, August 28‚Äì30, 2020, Proceedings, Part II 13. Springer (2020)
424"
REFERENCES,0.5326732673267327,"[52] Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L.,
425"
REFERENCES,0.5336633663366337,"Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot video-text understanding.
426"
REFERENCES,0.5346534653465347,"arXiv preprint arXiv:2109.14084 (2021)
427"
REFERENCES,0.5356435643564357,"[53] Xu, J., Zhao, L., Yan, H., Zeng, Q., Liang, Y., Sun, X.: Lexicalat: Lexical-based adversarial re-
428"
REFERENCES,0.5366336633663367,"inforcement training for robust sentiment classification. In: Proceedings of the 2019 conference
429"
REFERENCES,0.5376237623762377,"on empirical methods in natural language processing and the 9th international joint conference
430"
REFERENCES,0.5386138613861386,"on natural language processing (EMNLP-IJCNLP). pp. 5518‚Äì5527 (2019)
431"
REFERENCES,0.5396039603960396,"[54] Yang, Y., Ma, C., Ju, C., Zhang, Y., Wang, Y.: Multi-modal prototypes for open-set semantic
432"
REFERENCES,0.5405940594059406,"segmentation. arXiv preprint arXiv:2307.02003 (2023)
433"
REFERENCES,0.5415841584158416,"[55] Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., Xu, C.:
434"
REFERENCES,0.5425742574257426,"Filip: Fine-grained interactive language-image pre-training. In: Proceedings of the International
435"
REFERENCES,0.5435643564356436,"Conference on Learning Representations (2022)
436"
REFERENCES,0.5445544554455446,"[56] Ye, Z., Ju, C., Ma, C., Zhang, X.: Unsupervised domain adaption via similarity-based prototypes
437"
REFERENCES,0.5455445544554456,"for cross-modality segmentation. In: Domain Adaptation and Representation Transfer, and
438"
REFERENCES,0.5465346534653466,"Affordable Healthcare and AI for Resource Diverse Global Health: Third MICCAI Workshop,
439"
REFERENCES,0.5475247524752476,"DART 2021, and First MICCAI Workshop, FAIR 2021, Held in Conjunction with MICCAI
440"
REFERENCES,0.5485148514851486,"2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings 3 (2021)
441"
REFERENCES,0.5495049504950495,"[57] Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C.,
442"
REFERENCES,0.5504950495049505,"et al.: Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432
443"
REFERENCES,0.5514851485148515,"(2021)
444"
REFERENCES,0.5524752475247525,"[58] Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., Beyer, L.: Lit:
445"
REFERENCES,0.5534653465346535,"Zero-shot transfer with locked-image text tuning. In: Proceedings of the IEEE Conference on
446"
REFERENCES,0.5544554455445545,"Computer Vision and Pattern Recognition (2022)
447"
REFERENCES,0.5554455445544555,"[59] Zhang, W.E., Sheng, Q.Z., Alhazmi, A., Li, C.: Adversarial attacks on deep-learning models
448"
REFERENCES,0.5564356435643565,"in natural language processing: A survey. ACM Transactions on Intelligent Systems and
449"
REFERENCES,0.5574257425742575,"Technology (TIST) (2020)
450"
REFERENCES,0.5584158415841585,"[60] Zhao, P., Xie, L., Ju, C., Zhang, Y., Wang, Y., Tian, Q.: Bottom-up temporal action localization
451"
REFERENCES,0.5594059405940595,"with mutual regularization. In: Proceedings of the European Conference on Computer Vision
452"
REFERENCES,0.5603960396039604,"(2020)
453"
REFERENCES,0.5613861386138614,"[61] Zheng, H., Chen, X., Yao, J., Yang, H., Li, C., Zhang, Y., Zhang, H., Tsang, I., Zhou, J., Zhou,
454"
REFERENCES,0.5623762376237624,"M.: Contrastive attraction and contrastive repulsion for representation learning. arXiv preprint
455"
REFERENCES,0.5633663366336633,"arXiv:2105.03746 (2021)
456"
REFERENCES,0.5643564356435643,"[62] Zhou, J., Dong, L., Gan, Z., Wang, L., Wei, F.: Non-contrastive learning meets language-
457"
REFERENCES,0.5653465346534653,"image pre-training. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
458"
REFERENCES,0.5663366336633663,"Recognition (2023)
459"
REFERENCES,0.5673267326732673,"[63] Zhou, Y., Jiang, J.Y., Chang, K.W., Wang, W.: Learning to discriminate perturbations for
460"
REFERENCES,0.5683168316831683,"blocking adversarial attacks in text classification. arXiv preprint arXiv:1909.03084 (2019)
461"
REFERENCES,0.5693069306930693,"A
Theoretical Analysis
462"
REFERENCES,0.5702970297029702,"A.1
Decoding Objective
463"
REFERENCES,0.5712871287128712,"At each step i, the decoding objective to find argmaxti p(ti|Ti‚àí1, V). Note that, p(Ti‚àí1, V) is same
464"
REFERENCES,0.5722772277227722,"for all possible ti. As a result, our objective is written as:
  \"
REFERENCES,0.5732673267326732,"argmax
 _
{t_i} p(t_ {i }  |\mat
hc
al {T}_{i- 1}, \math ca
l {V"
REFERENCES,0.5742574257425742,"} ) &= \
ar
gmax _{t_i } 
p(t_"
REFERENCES,0.5752475247524752,"{ i} |\m
at
hca l {T} _{i-1 }, \mathcal {V})p(\mathcal {T}_{i-1}, \mathcal {V}) \\ &= \argmax _{t_i} p(t_{i} , \mathcal {T}_{i-1}, \mathcal {V}) \\ &= \argmax _{t_i} \log p(t_{i} , \mathcal {T}_{i-1}, \mathcal {V})
(13)"
REFERENCES,0.5762376237623762,"A.2
Discriminative Step
466"
REFERENCES,0.5772277227722772,"At the discriminative step, we choose the best set of V that helps decode tc,i for each semantic-class
467"
REFERENCES,0.5782178217821782,"c. To understand why Vc, the set of visual samples vj whose labels Yj are assigned to semantic-class
468"
REFERENCES,0.5792079207920792,"c are those who help decode most efficiently, we first introduce a hidden discrete random variable
469"
REFERENCES,0.5801980198019802,"zj ‚àºQj for each vj, indicating the index of class assignment. zj = c means that argmax Yj = c.
470"
REFERENCES,0.5811881188118811,"Knowing that all visual samples are independent and using Jensen inequality:
  \"
REFERENCES,0.5821782178217821,"log  p(t_ i, \m at h
c"
REFERENCES,0.5831683168316831,"a
l { T}_{i -1}, \ma
thca l {
V})"
REFERENCES,0.5841584158415841,"& 
= \su m _j \lo g p
(t_i ,"
REFERENCES,0.5851485148514851,"\
mat
h"
REFERENCES,0.5861386138613861,"ca
l {T}_{i-1} , v_j ) \ \ &"
REFERENCES,0.5871287128712871,"= \su
m _j \ l o"
REFERENCES,0.5881188118811881,"g 
\sum _ {z_ j} p( t_i, \ma thc"
REFERENCES,0.5891089108910891,"al {T}_{i-1}, v_j, z_j) \\ & = \sum _j \log \sum _{z_j} Q_{j}(z_j) \frac {p(t_i, \mathcal {T}_{i-1}, v_j, z_j)}{Q_{j}(z_j)} \\ & \geq \sum _j \sum _{z_j} Q_{j}(z_j) \log \frac {p(t_i, \mathcal {T}_{i-1}, v_j, z_j)}{Q_{j}(z_j)}
(17)"
REFERENCES,0.5900990099009901,"Equality is attained at Qj(zj) ‚àùp(ti, Ti‚àí1, vj, zj). Since P"
REFERENCES,0.5910891089108911,"zj Qj(zj) = 1, to maximize the lower
472"
REFERENCES,0.592079207920792,"bound, we have:
  Q"
REFERENCES,0.593069306930693,"_j(z_j )
 & = \frac  {p (t_
i"
REFERENCES,0.594059405940594,", \math cal { T}_ {i-
1},"
REFERENCES,0.595049504950495,"v _j, z _j)}{ \su m _"
REFERENCES,0.596039603960396,"{z_j}  p(t_ i, 
\mat"
REFERENCES,0.597029702970297,"h cal {T}_ {i-1} , v
_j, 
z _j)} \\ & = \frac {p(t_i, \mathcal {T}_{i-1}, v_j, z_j)}{p(t_i, \mathcal {T}_{i-1}, v_j)} \\ & = p(z_j | t_i, \mathcal {T}_{i-1}, v_j) \\ & = p(z_j | \mathcal {T}_{i}, v_j)
(21)"
REFERENCES,0.598019801980198,"Given class texts and visual samples, the best estimation is:
  \"
REFERENCES,0.599009900990099,math b b {P} (z_ j =
REFERENCES,0.6,"c
| \ mathca
l
 {T
}"
REFERENCES,0.600990099009901,"_i,v_j) = \
begin"
REFERENCES,0.601980198019802,"{c ases} 1 \q qu
ad & 
c
 = \argmax \limits _c \max \limits _k \frac {\text {exp}(\mathcal {S}(v_j,\mathcal {T}^k_{c, i}))}{\sum _{k^\prime } \text {exp}(\mathcal {S}( v_j,\mathcal {T}^{k^\prime }_{c, i}))} \\ 0 \qquad & \text {otherwise} \\ \end {cases} (22)"
REFERENCES,0.602970297029703,"Note that, Qj is well defined because:
  \"
REFERENCES,0.6039603960396039,"lim
 _{Q_{j}( z_j)\t o 0 ^+} Q _{j}( z_j ) \"
REFERENCES,0.6049504950495049,"log \f
r ac {p(t_i, \mathcal {T}_{i-1}, v_j, z_j)}{Q_{j}(z_j)} = 0
(23)"
REFERENCES,0.6059405940594059,"With Qj defined in this way, we find the discriminative step to be identical to how Œ¶OVAR assigns
476"
REFERENCES,0.6069306930693069,"labels. We have Qj(c) = 1 only for {j|vj ‚ààVc}:
  \"
REFERENCES,0.6079207920792079,"log  p(t_ i, \m at h
c a l"
REFERENCES,0.6089108910891089,"{
T}_{i- 1},  \mat hcal {V} ) &"
REFERENCES,0.6099009900990099,"\geq 
\sum _ j "
REFERENCES,0.6108910891089109,\sum _{ z
REFERENCES,0.6118811881188119,"_j
} Q_{j }(z _j) \ log \ fra c {"
REFERENCES,0.6128712871287129,"p(t_i,
 \ma t
h c a"
REFERENCES,0.6138613861386139,"l {T}_{
i-1 }, v_ j, z_ j)} {Q _ {j
}(z_ j
)"
REFERENCES,0.6148514851485148,"}
 \\  & = \s um _c \ sum
 _{j, v_j\in \mathcal {V}_c} \sum _{z_j} Q_{j}(z_j) \log \frac {p(t_i, \mathcal {T}_{i-1}, v_j, z_j)}{Q_{j}(z_j)} \\ & = \sum _c \sum _{j, v_j\in \mathcal {V}_c} \log p(t_i, \mathcal {T}_{i-1}, v_j, z_j = c) \\ & = \sum _c \log p(t_{c,i}, \mathcal {T}_{c,i-1}, \mathcal {V}_c) \\ (28)"
REFERENCES,0.6158415841584158,"A.3
Generative Step
478"
REFERENCES,0.6168316831683168,"We optimize tc,i for each semantic-class:
  \"
REFERENCES,0.6178217821782178,"argmax
 _{t
_{c ,i}} \l og p(t_ {c, i } , \m
athc
al {T}_ {c,i-1} , \
math"
REFERENCES,0.6188118811881188,"c al {V}
_c) &"
REFERENCES,0.6198019801980198,"= \a
rgmax _ {t_{c,i }} 
p(t_"
REFERENCES,0.6207920792079208,"{ c,i}, 
\mat h"
REFERENCES,0.6217821782178218,"cal {
T}_{c,i-1}, \m athcal {V}_c) \\ &
 = \"
REFERENCES,0.6227722772277228,"a rgmax 
_{t_ {"
REFERENCES,0.6237623762376238,"c,i}}
 \prod _{v_j\i n \mathcal {V}_c} p(t_{c,i}, \mathcal {T}_{c,i-1}, v_j) \\ &= \argmax _{t_{c,i}} \prod _{v_j\in \mathcal {V}_c} p(\mathcal {T}_{c, i-1}|t_{c,i}, v_j) p(t_{c,i}|v_j) p(v_j) \\ &= \argmax _{t_{c,i}} \prod _{v_j\in \mathcal {V}_c} p(\mathcal {T}_{c, i-1}|t_{c,i}, v_j) p(t_{c,i}|v_j)
(32)"
REFERENCES,0.6247524752475248,"Noting that p(Tc,i‚àí1) is the same for any possible tc,i:
  \"
REFERENCES,0.6257425742574257,"argmax
 _{t
_{c,i}} p(\mat hca l  {T}_{
c, i
-1}|t_{c,i}, v
_j)"
REFERENCES,0.6267326732673267,"& = \arg
max"
REFERENCES,0.6277227722772277,"_{t_{c,i}} p(\mathcal {"
REFERENCES,0.6287128712871287,"T}_{c, 
i-1}"
REFERENCES,0.6297029702970297,"| t_{c,i
}) \"
REFERENCES,0.6306930693069307,\ &= \argmax _
REFERENCES,0.6316831683168317,"{t_{c,i}}\frac {p(t_{c,i}|\mathcal {T}_{c, i-1})p(\mathcal {T}_{c, i-1})}{p(t_{c,i})}\\ &= \argmax _{t_{c,i}} \frac {p(t_{c,i}|\mathcal {T}_{c, i-1})}{p(t_{c,i})}
(35)"
REFERENCES,0.6326732673267327,"It is possible to optimize with prior p(tc,i) by considering that the more a word is frequent, the less it
481"
REFERENCES,0.6336633663366337,"is likely to be misspelled in real-world scenarios. In this paper, for simplicity, we assume the tc,i to
482"
REFERENCES,0.6346534653465347,"be uniform:
  \"
REFERENCES,0.6356435643564357,"argmax
 _{t
_{c,i}} p(\mat hca l  {T}_{
c, i
-1}|t_{c,i}, v_j) &= \argmax _{t_{c,i}} p(t_{c,i}|\mathcal {T}_{c, i-1})
(36)"
REFERENCES,0.6366336633663366,"B
Additional Experiments
484"
REFERENCES,0.6376237623762376,"B.1
DENOISER vs. Adversarial Training
485"
REFERENCES,0.6386138613861386,"Fig. 5 studies how adversarial training might mitigate the noise in text descriptions. We first train
486"
REFERENCES,0.6396039603960396,"ActionCLIP ViT-B/32-8F from scratch on K400 by randomly injecting noise in its text labels, then
487"
REFERENCES,0.6405940594059406,"test the model‚Äôs zero-shot performance on UCF101 under different noise rate scenarios. We find that
488"
REFERENCES,0.6415841584158416,"adversarial training, though promising under closed-set scenarios in previous studies, is relatively
489"
REFERENCES,0.6425742574257426,"ineffective under open-vocabulary settings. Specifically, training with more noise lowers significantly
490"
REFERENCES,0.6435643564356436,"the model‚Äôs performance under low noise rate. Additionally, its added value is limited under heavy
491"
REFERENCES,0.6445544554455446,"noise rate. These phenomena are probably related to the domain gap between datasets. By training
492"
REFERENCES,0.6455445544554456,"on noisy text descriptions, the model tends to overfit the noise pattern, jeopardizing its zero-shot
493"
REFERENCES,0.6465346534653466,"performance. We conclude that noisy text descriptions are better solved in testing time rather than
494"
REFERENCES,0.6475247524752475,"during training stage. Our DENOISER framework shows a significant advantage over the adversarial
495"
REFERENCES,0.6485148514851485,"training.
496"
REFERENCES,0.6495049504950495,"Figure 5: Comparison to Adversarial Training. Adversarial training is not efficient, especially in
low-noise scenarios, even leading to a lower performance compared to the original model. It also
falls behind our method by a significant margin."
REFERENCES,0.6504950495049505,"Figure 6: Ablation Study on the Number of Visual Samples. When fewer visual samples are used
in Œ¶inter, our method shows a drop in performance. The bigger the noise rate, the larger the drop,
showing that Œ¶inter plays a role of increasing importance when the noise is larger."
REFERENCES,0.6514851485148515,"B.2
Ablation Study on the Number of Visual Samples
497"
REFERENCES,0.6524752475247525,"Fig. 6 ablates on the number of visual samples in Œ¶inter. Our method shows a drop in performance
498"
REFERENCES,0.6534653465346535,"when fewer visual samples are used in Œ¶inter. The performance tends to converge towards that
499"
REFERENCES,0.6544554455445545,"when solely Œ¶intra is used. We hypothesize that fewer visual samples make Œ¶inter harder to extract
500"
REFERENCES,0.6554455445544555,"added value to Œ¶intra. With the noise rate increasing, we find an increasingly large drop in perfor-
501"
REFERENCES,0.6564356435643565,"mance, which shows conversely that Œ¶inter is more important under large noise scenarios as textual
502"
REFERENCES,0.6574257425742575,"information becomes more ambiguous and less informative.
503"
REFERENCES,0.6584158415841584,"B.3
Qualitative Results
504"
REFERENCES,0.6594059405940594,"Fig. 7 visualizes the embedding of (visual samples, text descriptions) from three semantic-classes:
505"
REFERENCES,0.6603960396039604,"bird (green), ship (yellow), truck (blue) in CIFAR-10 using T-SNE. The first principal component of
506"
REFERENCES,0.6613861386138614,"textual embedding is removed following ReCLIP[12] to prevent them from clustering at the same
507"
REFERENCES,0.6623762376237624,"place. The Left shows that classification accuracy is low when text descriptions are noisy. Almost
508"
REFERENCES,0.6633663366336634,"all visual samples are recognized as ‚Äúbird‚Äù. The Middle shows the embeddings of proposed text
509"
REFERENCES,0.6643564356435644,"candidates. Some of them remain at the same place, because they move perpendicular to this 2D space
510"
REFERENCES,0.6653465346534654,"in the real semantic space. We assign the best set of visual samples for each semantic-class to help
511"
REFERENCES,0.6663366336633664,"denoise, e.g., the blue dots are used to vote on the two candidates ‚Äútrump‚Äù (red) and ‚Äútruck‚Äù (purple)
512"
REFERENCES,0.6673267326732674,"of ‚Äútrumk‚Äù. The Right shows that the denoised text descriptions improve the OVAR performance.
513"
REFERENCES,0.6683168316831684,"Tab. 4 quantifies some good/bad cases. We find GPT 3.5 is better at understanding semantics of
514"
REFERENCES,0.6693069306930693,"noisy text descriptions, e.g., ‚Äúwal4ingm with a dog‚Äù ‚Üí‚Äúdogwalking‚Äù. However, its output is highly
515"
REFERENCES,0.6702970297029703,"Table 4: Cases of Denoised Text Descriptions for GPT 3.5 and DENOISER. The output from
GPT 3.5 [1] tends to be unstable, and sometimes it‚Äôs a relatively high-level understanding of noisy
text descriptions. Our DENOISER ensures a relatively faithful output in terms of spelling but could
be slightly mistaken when two words are similar in terms of both semantics and spelling."
REFERENCES,0.6712871287128713,"Ground Truth
Noisy Text Descriptions
GPT 3.5 [1]
Ours"
REFERENCES,0.6722772277227723,Good Case
REFERENCES,0.6732673267326733,"walking with a dog
wal4ingm with a dog
dogwalking
walking with a dog
baby crawling
babty crawling
baby crying
baby crawling
cutting in kitchen
cutting i
aitnchen
cutting
cutting in kitchen
Bad Case
juggling balls
juggling ball
juggling
juggling ball"
REFERENCES,0.6742574257425743,"Figure 7: Denoising Visualization. Left: result with noisy text descriptions (crosses w black border).
Middle: text candidates (crosses w/o black border), the visual samples (in dots) that are used to vote
for candidates. Right: denoised class texts (crosses w black border) help for better classification."
REFERENCES,0.6752475247524753,"affected by input prompts, and thus tends to be unstable: important text parts are sometimes omitted
516"
REFERENCES,0.6762376237623763,"or misinterpreted, e.g., ‚Äúbabty crawling‚Äù ‚Üí‚Äúbaby crying‚Äù. Such unstable outputs require manual
517"
REFERENCES,0.6772277227722773,"cleaning, limiting its applications in reality. Our DENOISER remains faithful in terms of spelling,
518"
REFERENCES,0.6782178217821783,"e.g., ‚Äúwal4ingm with a dog‚Äù ‚Üí‚Äúwalking with a dog‚Äù instead of ‚Äúdogwalking‚Äù. While it may be
519"
REFERENCES,0.6792079207920793,"mistaken when two words are similar in semantics and spelling (rare cases), e.g., ‚Äúball‚Äù and ‚Äúballs‚Äù.
520"
REFERENCES,0.6801980198019802,"C
On the efficiency of DENOISER
521"
REFERENCES,0.6811881188118812,"Our model requires a trade-off between computational cost and performance. As shown in Fig. 4
522"
REFERENCES,0.6821782178217822,"and Fig. 6, the performance of our DENOISER increases as the number of proposals K and the
523"
REFERENCES,0.6831683168316832,"percentage of the visual samples used. Since the theoretical complexity of DENOISER increases
524"
REFERENCES,0.6841584158415842,"linearly with K and the percentage of visual samples used, while the marginal contribution of a larger
525"
REFERENCES,0.6851485148514852,"K or percentage is decreasing, a trade-off between computational cost and performance is necessary.
526"
REFERENCES,0.6861386138613862,"DENOISER requires only simple operations for each iteration. After having extracted the embedding
527"
REFERENCES,0.6871287128712872,"of visual samples, DENOISER only requires recomputing the text embedding and doing a dot product
528"
REFERENCES,0.6881188118811881,"with visual embeddings, which is extremely fast. Compared to other approaches that intend to align
529"
REFERENCES,0.689108910891089,"noisy text-image pairs or to train spell-checking models, DENOISER that denoises at evaluation time
530"
REFERENCES,0.69009900990099,"is extremely time-saving.
531"
REFERENCES,0.691089108910891,"NeurIPS Paper Checklist
532"
CLAIMS,0.692079207920792,"1. Claims
533"
CLAIMS,0.693069306930693,"Question: Do the main claims made in the abstract and introduction accurately reflect the
534"
CLAIMS,0.694059405940594,"paper‚Äôs contributions and scope?
535"
CLAIMS,0.695049504950495,"Answer: [Yes]
536"
CLAIMS,0.696039603960396,"Justification: The main claims made in the abstract and introduction accurately reflect the
537"
CLAIMS,0.697029702970297,"paper‚Äôs contributions and scope.
538"
CLAIMS,0.698019801980198,"Guidelines:
539"
CLAIMS,0.699009900990099,"‚Ä¢ The answer NA means that the abstract and introduction do not include the claims
540"
CLAIMS,0.7,"made in the paper.
541"
CLAIMS,0.700990099009901,"‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the
542"
CLAIMS,0.7019801980198019,"contributions made in the paper and important assumptions and limitations. A No or
543"
CLAIMS,0.7029702970297029,"NA answer to this question will not be perceived well by the reviewers.
544"
CLAIMS,0.7039603960396039,"‚Ä¢ The claims made should match theoretical and experimental results, and reflect how
545"
CLAIMS,0.7049504950495049,"much the results can be expected to generalize to other settings.
546"
CLAIMS,0.7059405940594059,"‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals
547"
CLAIMS,0.7069306930693069,"are not attained by the paper.
548"
LIMITATIONS,0.7079207920792079,"2. Limitations
549"
LIMITATIONS,0.7089108910891089,"Question: Does the paper discuss the limitations of the work performed by the authors?
550"
LIMITATIONS,0.7099009900990099,"Answer: [Yes]
551"
LIMITATIONS,0.7108910891089109,"Justification: We discuss the limitation of our method at the end of the paper, and in the
552"
LIMITATIONS,0.7118811881188118,"appendix.
553"
LIMITATIONS,0.7128712871287128,"Guidelines:
554"
LIMITATIONS,0.7138613861386138,"‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that
555"
LIMITATIONS,0.7148514851485148,"the paper has limitations, but those are not discussed in the paper.
556"
LIMITATIONS,0.7158415841584158,"‚Ä¢ The authors are encouraged to create a separate ""Limitations"" section in their paper.
557"
LIMITATIONS,0.7168316831683168,"‚Ä¢ The paper should point out any strong assumptions and how robust the results are to
558"
LIMITATIONS,0.7178217821782178,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
559"
LIMITATIONS,0.7188118811881188,"model well-specification, asymptotic approximations only holding locally). The authors
560"
LIMITATIONS,0.7198019801980198,"should reflect on how these assumptions might be violated in practice and what the
561"
LIMITATIONS,0.7207920792079208,"implications would be.
562"
LIMITATIONS,0.7217821782178218,"‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
563"
LIMITATIONS,0.7227722772277227,"only tested on a few datasets or with a few runs. In general, empirical results often
564"
LIMITATIONS,0.7237623762376237,"depend on implicit assumptions, which should be articulated.
565"
LIMITATIONS,0.7247524752475247,"‚Ä¢ The authors should reflect on the factors that influence the performance of the approach.
566"
LIMITATIONS,0.7257425742574257,"For example, a facial recognition algorithm may perform poorly when image resolution
567"
LIMITATIONS,0.7267326732673267,"is low or images are taken in low lighting. Or a speech-to-text system might not be
568"
LIMITATIONS,0.7277227722772277,"used reliably to provide closed captions for online lectures because it fails to handle
569"
LIMITATIONS,0.7287128712871287,"technical jargon.
570"
LIMITATIONS,0.7297029702970297,"‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms
571"
LIMITATIONS,0.7306930693069307,"and how they scale with dataset size.
572"
LIMITATIONS,0.7316831683168317,"‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to
573"
LIMITATIONS,0.7326732673267327,"address problems of privacy and fairness.
574"
LIMITATIONS,0.7336633663366336,"‚Ä¢ While the authors might fear that complete honesty about limitations might be used by
575"
LIMITATIONS,0.7346534653465346,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
576"
LIMITATIONS,0.7356435643564356,"limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
577"
LIMITATIONS,0.7366336633663366,"judgment and recognize that individual actions in favor of transparency play an impor-
578"
LIMITATIONS,0.7376237623762376,"tant role in developing norms that preserve the integrity of the community. Reviewers
579"
LIMITATIONS,0.7386138613861386,"will be specifically instructed to not penalize honesty concerning limitations.
580"
THEORY ASSUMPTIONS AND PROOFS,0.7396039603960396,"3. Theory Assumptions and Proofs
581"
THEORY ASSUMPTIONS AND PROOFS,0.7405940594059406,"Question: For each theoretical result, does the paper provide the full set of assumptions and
582"
THEORY ASSUMPTIONS AND PROOFS,0.7415841584158416,"a complete (and correct) proof?
583"
THEORY ASSUMPTIONS AND PROOFS,0.7425742574257426,"Answer: [Yes]
584"
THEORY ASSUMPTIONS AND PROOFS,0.7435643564356436,"Justification: We provide detailed derivation in the appendix.
585"
THEORY ASSUMPTIONS AND PROOFS,0.7445544554455445,"Guidelines:
586"
THEORY ASSUMPTIONS AND PROOFS,0.7455445544554455,"‚Ä¢ The answer NA means that the paper does not include theoretical results.
587"
THEORY ASSUMPTIONS AND PROOFS,0.7465346534653465,"‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
588"
THEORY ASSUMPTIONS AND PROOFS,0.7475247524752475,"referenced.
589"
THEORY ASSUMPTIONS AND PROOFS,0.7485148514851485,"‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
590"
THEORY ASSUMPTIONS AND PROOFS,0.7495049504950495,"‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if
591"
THEORY ASSUMPTIONS AND PROOFS,0.7504950495049505,"they appear in the supplemental material, the authors are encouraged to provide a short
592"
THEORY ASSUMPTIONS AND PROOFS,0.7514851485148515,"proof sketch to provide intuition.
593"
THEORY ASSUMPTIONS AND PROOFS,0.7524752475247525,"‚Ä¢ Inversely, any informal proof provided in the core of the paper should be complemented
594"
THEORY ASSUMPTIONS AND PROOFS,0.7534653465346535,"by formal proofs provided in appendix or supplemental material.
595"
THEORY ASSUMPTIONS AND PROOFS,0.7544554455445545,"‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7554455445544555,"4. Experimental Result Reproducibility
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7564356435643564,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7574257425742574,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
599"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7584158415841584,"of the paper (regardless of whether the code and data are provided or not)?
600"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7594059405940594,"Answer: [Yes]
601"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7603960396039604,"Justification: We detail the proposed algorithm and the setting of experiments. Additionally,
602"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613861386138614,"we provide source code.
603"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7623762376237624,"Guidelines:
604"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7633663366336634,"‚Ä¢ The answer NA means that the paper does not include experiments.
605"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7643564356435644,"‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived
606"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7653465346534654,"well by the reviewers: Making the paper reproducible is important, regardless of
607"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7663366336633664,"whether the code and data are provided or not.
608"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7673267326732673,"‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken
609"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7683168316831683,"to make their results reproducible or verifiable.
610"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7693069306930693,"‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways.
611"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7702970297029703,"For example, if the contribution is a novel architecture, describing the architecture fully
612"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7712871287128713,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
613"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7722772277227723,"be necessary to either make it possible for others to replicate the model with the same
614"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7732673267326733,"dataset, or provide access to the model. In general. releasing code and data is often
615"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7742574257425743,"one good way to accomplish this, but reproducibility can also be provided via detailed
616"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7752475247524753,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
617"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7762376237623763,"of a large language model), releasing of a model checkpoint, or other means that are
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7772277227722773,"appropriate to the research performed.
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7782178217821782,"‚Ä¢ While NeurIPS does not require releasing code, the conference does require all submis-
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7792079207920792,"sions to provide some reasonable avenue for reproducibility, which may depend on the
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7801980198019802,"nature of the contribution. For example
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7811881188118812,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7821782178217822,"to reproduce that algorithm.
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7831683168316832,"(b) If the contribution is primarily a new model architecture, the paper should describe
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7841584158415842,"the architecture clearly and fully.
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7851485148514852,"(c) If the contribution is a new model (e.g., a large language model), then there should
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7861386138613862,"either be a way to access this model for reproducing the results or a way to reproduce
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7871287128712872,"the model (e.g., with an open-source dataset or instructions for how to construct
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7881188118811882,"the dataset).
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7891089108910891,"(d) We recognize that reproducibility may be tricky in some cases, in which case
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7900990099009901,"authors are welcome to describe the particular way they provide for reproducibility.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7910891089108911,"In the case of closed-source models, it may be that access to the model is limited in
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7920792079207921,"some way (e.g., to registered users), but it should be possible for other researchers
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7930693069306931,"to have some path to reproducing or verifying the results.
635"
OPEN ACCESS TO DATA AND CODE,0.7940594059405941,"5. Open access to data and code
636"
OPEN ACCESS TO DATA AND CODE,0.7950495049504951,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
637"
OPEN ACCESS TO DATA AND CODE,0.7960396039603961,"tions to faithfully reproduce the main experimental results, as described in supplemental
638"
OPEN ACCESS TO DATA AND CODE,0.7970297029702971,"material?
639"
OPEN ACCESS TO DATA AND CODE,0.7980198019801981,"Answer: [Yes]
640"
OPEN ACCESS TO DATA AND CODE,0.799009900990099,"Justification: We provide source code. Datasets are publicly accessible.
641"
OPEN ACCESS TO DATA AND CODE,0.8,"Guidelines:
642"
OPEN ACCESS TO DATA AND CODE,0.800990099009901,"‚Ä¢ The answer NA means that paper does not include experiments requiring code.
643"
OPEN ACCESS TO DATA AND CODE,0.801980198019802,"‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/
644"
OPEN ACCESS TO DATA AND CODE,0.802970297029703,"public/guides/CodeSubmissionPolicy) for more details.
645"
OPEN ACCESS TO DATA AND CODE,0.803960396039604,"‚Ä¢ While we encourage the release of code and data, we understand that this might not be
646"
OPEN ACCESS TO DATA AND CODE,0.804950495049505,"possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
647"
OPEN ACCESS TO DATA AND CODE,0.805940594059406,"including code, unless this is central to the contribution (e.g., for a new open-source
648"
OPEN ACCESS TO DATA AND CODE,0.806930693069307,"benchmark).
649"
OPEN ACCESS TO DATA AND CODE,0.807920792079208,"‚Ä¢ The instructions should contain the exact command and environment needed to run to
650"
OPEN ACCESS TO DATA AND CODE,0.808910891089109,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
651"
OPEN ACCESS TO DATA AND CODE,0.80990099009901,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
652"
OPEN ACCESS TO DATA AND CODE,0.810891089108911,"‚Ä¢ The authors should provide instructions on data access and preparation, including how
653"
OPEN ACCESS TO DATA AND CODE,0.8118811881188119,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
654"
OPEN ACCESS TO DATA AND CODE,0.8128712871287128,"‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new
655"
OPEN ACCESS TO DATA AND CODE,0.8138613861386138,"proposed method and baselines. If only a subset of experiments are reproducible, they
656"
OPEN ACCESS TO DATA AND CODE,0.8148514851485148,"should state which ones are omitted from the script and why.
657"
OPEN ACCESS TO DATA AND CODE,0.8158415841584158,"‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized
658"
OPEN ACCESS TO DATA AND CODE,0.8168316831683168,"versions (if applicable).
659"
OPEN ACCESS TO DATA AND CODE,0.8178217821782178,"‚Ä¢ Providing as much information as possible in supplemental material (appended to the
660"
OPEN ACCESS TO DATA AND CODE,0.8188118811881188,"paper) is recommended, but including URLs to data and code is permitted.
661"
OPEN ACCESS TO DATA AND CODE,0.8198019801980198,"6. Experimental Setting/Details
662"
OPEN ACCESS TO DATA AND CODE,0.8207920792079207,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
663"
OPEN ACCESS TO DATA AND CODE,0.8217821782178217,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
664"
OPEN ACCESS TO DATA AND CODE,0.8227722772277227,"results?
665"
OPEN ACCESS TO DATA AND CODE,0.8237623762376237,"Answer: [Yes]
666"
OPEN ACCESS TO DATA AND CODE,0.8247524752475247,"Justification: We specify all settings of experiments in the experiments section.
667"
OPEN ACCESS TO DATA AND CODE,0.8257425742574257,"Guidelines:
668"
OPEN ACCESS TO DATA AND CODE,0.8267326732673267,"‚Ä¢ The answer NA means that the paper does not include experiments.
669"
OPEN ACCESS TO DATA AND CODE,0.8277227722772277,"‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail
670"
OPEN ACCESS TO DATA AND CODE,0.8287128712871287,"that is necessary to appreciate the results and make sense of them.
671"
OPEN ACCESS TO DATA AND CODE,0.8297029702970297,"‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental
672"
OPEN ACCESS TO DATA AND CODE,0.8306930693069307,"material.
673"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8316831683168316,"7. Experiment Statistical Significance
674"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326732673267326,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
675"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336633663366336,"information about the statistical significance of the experiments?
676"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8346534653465346,"Answer: [Yes]
677"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8356435643564356,"Justification: We report confidence intervals.
678"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8366336633663366,"Guidelines:
679"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8376237623762376,"‚Ä¢ The answer NA means that the paper does not include experiments.
680"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8386138613861386,"‚Ä¢ The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
681"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8396039603960396,"dence intervals, or statistical significance tests, at least for the experiments that support
682"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8405940594059406,"the main claims of the paper.
683"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8415841584158416,"‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for
684"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8425742574257425,"example, train/test split, initialization, random drawing of some parameter, or overall
685"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8435643564356435,"run with given experimental conditions).
686"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8445544554455445,"‚Ä¢ The method for calculating the error bars should be explained (closed form formula,
687"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8455445544554455,"call to a library function, bootstrap, etc.)
688"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8465346534653465,"‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
689"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8475247524752475,"‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error
690"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8485148514851485,"of the mean.
691"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8495049504950495,"‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should
692"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8504950495049505,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
693"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8514851485148515,"of Normality of errors is not verified.
694"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8524752475247525,"‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or
695"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8534653465346534,"figures symmetric error bars that would yield results that are out of range (e.g. negative
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8544554455445544,"error rates).
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8554455445544554,"‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8564356435643564,"they were calculated and reference the corresponding figures or tables in the text.
699"
EXPERIMENTS COMPUTE RESOURCES,0.8574257425742574,"8. Experiments Compute Resources
700"
EXPERIMENTS COMPUTE RESOURCES,0.8584158415841584,"Question: For each experiment, does the paper provide sufficient information on the com-
701"
EXPERIMENTS COMPUTE RESOURCES,0.8594059405940594,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
702"
EXPERIMENTS COMPUTE RESOURCES,0.8603960396039604,"the experiments?
703"
EXPERIMENTS COMPUTE RESOURCES,0.8613861386138614,"Answer: [Yes]
704"
EXPERIMENTS COMPUTE RESOURCES,0.8623762376237624,"Justification: We report information of computer resources.
705"
EXPERIMENTS COMPUTE RESOURCES,0.8633663366336634,"Guidelines:
706"
EXPERIMENTS COMPUTE RESOURCES,0.8643564356435643,"‚Ä¢ The answer NA means that the paper does not include experiments.
707"
EXPERIMENTS COMPUTE RESOURCES,0.8653465346534653,"‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
708"
EXPERIMENTS COMPUTE RESOURCES,0.8663366336633663,"or cloud provider, including relevant memory and storage.
709"
EXPERIMENTS COMPUTE RESOURCES,0.8673267326732673,"‚Ä¢ The paper should provide the amount of compute required for each of the individual
710"
EXPERIMENTS COMPUTE RESOURCES,0.8683168316831683,"experimental runs as well as estimate the total compute.
711"
EXPERIMENTS COMPUTE RESOURCES,0.8693069306930693,"‚Ä¢ The paper should disclose whether the full research project required more compute
712"
EXPERIMENTS COMPUTE RESOURCES,0.8702970297029703,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
713"
EXPERIMENTS COMPUTE RESOURCES,0.8712871287128713,"didn‚Äôt make it into the paper).
714"
CODE OF ETHICS,0.8722772277227723,"9. Code Of Ethics
715"
CODE OF ETHICS,0.8732673267326733,"Question: Does the research conducted in the paper conform, in every respect, with the
716"
CODE OF ETHICS,0.8742574257425743,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
717"
CODE OF ETHICS,0.8752475247524752,"Answer: [Yes]
718"
CODE OF ETHICS,0.8762376237623762,"Justification: We conduct in the paper conform, in every respect, with the NeurIPS Code of
719"
CODE OF ETHICS,0.8772277227722772,"Ethics.
720"
CODE OF ETHICS,0.8782178217821782,"Guidelines:
721"
CODE OF ETHICS,0.8792079207920792,"‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
722"
CODE OF ETHICS,0.8801980198019802,"‚Ä¢ If the authors answer No, they should explain the special circumstances that require a
723"
CODE OF ETHICS,0.8811881188118812,"deviation from the Code of Ethics.
724"
CODE OF ETHICS,0.8821782178217822,"‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
725"
CODE OF ETHICS,0.8831683168316832,"eration due to laws or regulations in their jurisdiction).
726"
BROADER IMPACTS,0.8841584158415842,"10. Broader Impacts
727"
BROADER IMPACTS,0.8851485148514852,"Question: Does the paper discuss both potential positive societal impacts and negative
728"
BROADER IMPACTS,0.8861386138613861,"societal impacts of the work performed?
729"
BROADER IMPACTS,0.8871287128712871,"Answer: [Yes]
730"
BROADER IMPACTS,0.8881188118811881,"Justification: Our model helps users better leverage the existing Open-Vocabulary models in
731"
BROADER IMPACTS,0.8891089108910891,"a more robust way.
732"
BROADER IMPACTS,0.8900990099009901,"Guidelines:
733"
BROADER IMPACTS,0.8910891089108911,"‚Ä¢ The answer NA means that there is no societal impact of the work performed.
734"
BROADER IMPACTS,0.8920792079207921,"‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal
735"
BROADER IMPACTS,0.8930693069306931,"impact or why the paper does not address societal impact.
736"
BROADER IMPACTS,0.8940594059405941,"‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses
737"
BROADER IMPACTS,0.8950495049504951,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
738"
BROADER IMPACTS,0.8960396039603961,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
739"
BROADER IMPACTS,0.897029702970297,"groups), privacy considerations, and security considerations.
740"
BROADER IMPACTS,0.898019801980198,"‚Ä¢ The conference expects that many papers will be foundational research and not tied
741"
BROADER IMPACTS,0.899009900990099,"to particular applications, let alone deployments. However, if there is a direct path to
742"
BROADER IMPACTS,0.9,"any negative applications, the authors should point it out. For example, it is legitimate
743"
BROADER IMPACTS,0.900990099009901,"to point out that an improvement in the quality of generative models could be used to
744"
BROADER IMPACTS,0.901980198019802,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
745"
BROADER IMPACTS,0.902970297029703,"that a generic algorithm for optimizing neural networks could enable people to train
746"
BROADER IMPACTS,0.903960396039604,"models that generate Deepfakes faster.
747"
BROADER IMPACTS,0.904950495049505,"‚Ä¢ The authors should consider possible harms that could arise when the technology is
748"
BROADER IMPACTS,0.905940594059406,"being used as intended and functioning correctly, harms that could arise when the
749"
BROADER IMPACTS,0.906930693069307,"technology is being used as intended but gives incorrect results, and harms following
750"
BROADER IMPACTS,0.907920792079208,"from (intentional or unintentional) misuse of the technology.
751"
BROADER IMPACTS,0.9089108910891089,"‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitigation
752"
BROADER IMPACTS,0.9099009900990099,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
753"
BROADER IMPACTS,0.9108910891089109,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
754"
BROADER IMPACTS,0.9118811881188119,"feedback over time, improving the efficiency and accessibility of ML).
755"
SAFEGUARDS,0.9128712871287129,"11. Safeguards
756"
SAFEGUARDS,0.9138613861386139,"Question: Does the paper describe safeguards that have been put in place for responsible
757"
SAFEGUARDS,0.9148514851485149,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
758"
SAFEGUARDS,0.9158415841584159,"image generators, or scraped datasets)?
759"
SAFEGUARDS,0.9168316831683169,"Answer: [NA]
760"
SAFEGUARDS,0.9178217821782179,"Justification: The paper poses no such risks.
761"
SAFEGUARDS,0.9188118811881189,"Guidelines:
762"
SAFEGUARDS,0.9198019801980198,"‚Ä¢ The answer NA means that the paper poses no such risks.
763"
SAFEGUARDS,0.9207920792079208,"‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with
764"
SAFEGUARDS,0.9217821782178218,"necessary safeguards to allow for controlled use of the model, for example by requiring
765"
SAFEGUARDS,0.9227722772277228,"that users adhere to usage guidelines or restrictions to access the model or implementing
766"
SAFEGUARDS,0.9237623762376238,"safety filters.
767"
SAFEGUARDS,0.9247524752475248,"‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
768"
SAFEGUARDS,0.9257425742574258,"should describe how they avoided releasing unsafe images.
769"
SAFEGUARDS,0.9267326732673268,"‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do
770"
SAFEGUARDS,0.9277227722772278,"not require this, but we encourage authors to take this into account and make a best
771"
SAFEGUARDS,0.9287128712871288,"faith effort.
772"
LICENSES FOR EXISTING ASSETS,0.9297029702970298,"12. Licenses for existing assets
773"
LICENSES FOR EXISTING ASSETS,0.9306930693069307,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
774"
LICENSES FOR EXISTING ASSETS,0.9316831683168317,"the paper, properly credited and are the license and terms of use explicitly mentioned and
775"
LICENSES FOR EXISTING ASSETS,0.9326732673267327,"properly respected?
776"
LICENSES FOR EXISTING ASSETS,0.9336633663366337,"Answer: [Yes]
777"
LICENSES FOR EXISTING ASSETS,0.9346534653465347,"Justification: All the assets are properly cited. License and terms of use are properly
778"
LICENSES FOR EXISTING ASSETS,0.9356435643564357,"respected.
779"
LICENSES FOR EXISTING ASSETS,0.9366336633663367,"Guidelines:
780"
LICENSES FOR EXISTING ASSETS,0.9376237623762376,"‚Ä¢ The answer NA means that the paper does not use existing assets.
781"
LICENSES FOR EXISTING ASSETS,0.9386138613861386,"‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
782"
LICENSES FOR EXISTING ASSETS,0.9396039603960396,"‚Ä¢ The authors should state which version of the asset is used and, if possible, include a
783"
LICENSES FOR EXISTING ASSETS,0.9405940594059405,"URL.
784"
LICENSES FOR EXISTING ASSETS,0.9415841584158415,"‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
785"
LICENSES FOR EXISTING ASSETS,0.9425742574257425,"‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of
786"
LICENSES FOR EXISTING ASSETS,0.9435643564356435,"service of that source should be provided.
787"
LICENSES FOR EXISTING ASSETS,0.9445544554455445,"‚Ä¢ If assets are released, the license, copyright information, and terms of use in the
788"
LICENSES FOR EXISTING ASSETS,0.9455445544554455,"package should be provided. For popular datasets, paperswithcode.com/datasets
789"
LICENSES FOR EXISTING ASSETS,0.9465346534653465,"has curated licenses for some datasets. Their licensing guide can help determine the
790"
LICENSES FOR EXISTING ASSETS,0.9475247524752475,"license of a dataset.
791"
LICENSES FOR EXISTING ASSETS,0.9485148514851485,"‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of
792"
LICENSES FOR EXISTING ASSETS,0.9495049504950495,"the derived asset (if it has changed) should be provided.
793"
LICENSES FOR EXISTING ASSETS,0.9504950495049505,"‚Ä¢ If this information is not available online, the authors are encouraged to reach out to
794"
LICENSES FOR EXISTING ASSETS,0.9514851485148514,"the asset‚Äôs creators.
795"
NEW ASSETS,0.9524752475247524,"13. New Assets
796"
NEW ASSETS,0.9534653465346534,"Question: Are new assets introduced in the paper well documented and is the documentation
797"
NEW ASSETS,0.9544554455445544,"provided alongside the assets?
798"
NEW ASSETS,0.9554455445544554,"Answer: [Yes]
799"
NEW ASSETS,0.9564356435643564,"Justification: We provided well-documented source code.
800"
NEW ASSETS,0.9574257425742574,"Guidelines:
801"
NEW ASSETS,0.9584158415841584,"‚Ä¢ The answer NA means that the paper does not release new assets.
802"
NEW ASSETS,0.9594059405940594,"‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their
803"
NEW ASSETS,0.9603960396039604,"submissions via structured templates. This includes details about training, license,
804"
NEW ASSETS,0.9613861386138614,"limitations, etc.
805"
NEW ASSETS,0.9623762376237623,"‚Ä¢ The paper should discuss whether and how consent was obtained from people whose
806"
NEW ASSETS,0.9633663366336633,"asset is used.
807"
NEW ASSETS,0.9643564356435643,"‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either
808"
NEW ASSETS,0.9653465346534653,"create an anonymized URL or include an anonymized zip file.
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9663366336633663,"14. Crowdsourcing and Research with Human Subjects
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9673267326732673,"Question: For crowdsourcing experiments and research with human subjects, does the paper
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9683168316831683,"include the full text of instructions given to participants and screenshots, if applicable, as
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693069306930693,"well as details about compensation (if any)?
813"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9702970297029703,"Answer: [NA]
814"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9712871287128713,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
815"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9722772277227723,"Guidelines:
816"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9732673267326732,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
817"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742574257425742,"human subjects.
818"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9752475247524752,"‚Ä¢ Including this information in the supplemental material is fine, but if the main contribu-
819"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9762376237623762,"tion of the paper involves human subjects, then as much detail as possible should be
820"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772277227722772,"included in the main paper.
821"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9782178217821782,"‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
822"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9792079207920792,"or other labor should be paid at least the minimum wage in the country of the data
823"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9801980198019802,"collector.
824"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811881188118812,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
825"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821782178217822,"Subjects
826"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9831683168316832,"Question: Does the paper describe potential risks incurred by study participants, whether
827"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9841584158415841,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
828"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9851485148514851,"approvals (or an equivalent approval/review based on the requirements of your country or
829"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9861386138613861,"institution) were obtained?
830"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9871287128712871,"Answer: [NA]
831"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9881188118811881,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
832"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9891089108910891,"Guidelines:
833"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900990099009901,"‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with
834"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910891089108911,"human subjects.
835"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920792079207921,"‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equivalent)
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9930693069306931,"may be required for any human subjects research. If you obtained IRB approval, you
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.994059405940594,"should clearly state this in the paper.
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.995049504950495,"‚Ä¢ We recognize that the procedures for this may vary significantly between institutions
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996039603960396,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.997029702970297,"guidelines for their institution.
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.998019801980198,"‚Ä¢ For initial submissions, do not include any information that would break anonymity (if
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.999009900990099,"applicable), such as the institution conducting the review.
843"
