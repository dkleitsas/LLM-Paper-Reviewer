Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001610305958132045,"We study an important variant of the stochastic multi-armed bandit (MAB) prob-
lem, which takes fairness into consideration. Instead of directly maximizing cu-
mulative expected reward, we need to balance between the total reward and fair-
ness level. In this paper, we present a new insight in MAB with fairness and
formulate the problem in the penalization framework, where rigorous penalized
regret can be well deﬁned and more sophisticated regret analysis is possible. Un-
der such a framework, we propose a hard-threshold UCB-like algorithm, which
enjoys many merits including asymptotic fairness, nearly optimal regret, better
tradeoff between reward and fairness. Both gap-dependent and gap-independent
upper bounds have been established. Lower bounds are also given to illustrate the
tightness of our theoretical analysis. Numerous experimental results corroborate
the theory and show the superiority of our method over other existing methods."
INTRODUCTION,0.00322061191626409,"1
INTRODUCTION"
INTRODUCTION,0.004830917874396135,"The multi-armed bandit (MAB) problem is a classical framework for sequential decision-making in
uncertain environments. Starting with the seminal work of Robbins (1952), over the years, a signiﬁ-
cant body of work has been developed to address both theoretical aspects and practical applications
of this problem. In a traditional stochastic multi-armed bandit (MAB) problem (Lai & Robbins,
1985; Auer et al., 2002; Vermorel & Mohri, 2005; Bubeck & Cesa-Bianchi, 2012), a learner has ac-
cess to K arms and pulling arm k generates a stochastic reward for the principal from an unknown
distribution Fk with an unknown expected reward µk. If the mean rewards were known as prior in-
formation, the learner could just repeatedly pull the best arm given by k∗= arg maxk µk. However,
the learner has no such knowledge of the reward of each arm. Hence, one should use some learning
algorithm π which operates in rounds, pulls arm πt ∈{1, . . . , K} in round t, observes the stochastic
reward generated from reward distribution Fπt, and uses that information to learn the best arm over
time. The performance of learning algorithm π is evaluated based on its cumulative regret over time
horizon T, deﬁned as"
INTRODUCTION,0.00644122383252818,"¯Rπ(T) = µk∗T − T
X"
INTRODUCTION,0.008051529790660225,"t=1
Eµπt.
(1)"
INTRODUCTION,0.00966183574879227,"To achieve the minimum regret, a good learner should make a balance between exploration (pulling
different arms to get more information of reward distribution of each arm) and exploitation (pulling
the arm currently believed to have the highest reward)."
INTRODUCTION,0.011272141706924315,"In addition to the above classical MAB problem, many variations of the MAB framework have been
extensively studied in the literature recently. Various papers study MAB problems with additional
constraints which include bandits with knapsack constraints (Badanidiyuru et al., 2013), bandits
with budget constraints (Xia et al., 2015), sleeping bandits (Kleinberg et al., 2010; Chatterjee et al.,
2017), etc. Except these, there is a huge research interest in fairness within machine learning ﬁeld.
Fairness has been a hot topic of many recent application tasks, including classiﬁcation (Zafar et al.,
2017a;b; Agarwal et al., 2018; Roh et al., 2021), regression (Berk et al., 2017; Rezaei et al., 2019),
recommendation (Celis et al., 2017; Singh & Joachims, 2018; Beutel et al., 2019; Wang et al., 2021),
resource allocation (Baruah et al., 1996; Talebi & Proutiere, 2018; Li et al., 2020), Markov decision
process (Khan & Goodridge, 2019), etc. There are two popular deﬁnitions of fairness in the MAB
literature. 1). The fairness is introduced into the bandit learning framework by saying that it is unfair"
INTRODUCTION,0.01288244766505636,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.014492753623188406,"to preferentially choose one arm over another if the chosen arm has lower expected reward than the
unchosen arm (Joseph et al., 2016). In other words, the learning algorithm cannot favor low-reward
arms. 2). The fairness is introduced such that the algorithm needs to ensure that uniformly (i.e.,
at the end of every round) each arm is pulled at least a pre-speciﬁed fraction of times (Patil et al.,
2020). In other words, it imposes an additional constraint to prevent the algorithm from playing
low-reward arms too few times."
INTRODUCTION,0.01610305958132045,"In this paper, we adopt a new perspective, e.g., in addition to maximizing the cumulative expected
reward, it also allows the user to specify how “hard” or how “soft” the fairness requirement on each
arm is. In this view, it is not always easy even to formulate the problem and to introduce an appro-
priate notion of regret. We thus propose a new formulation of fairness MAB by introducing penalty
term Ak max(τkT −Nk(T), 0), where Ak, τk are the penalty rate and fairness fraction for arm k and
Nk(T) is the number of times pulling arm k. Hence it gives penalization when the algorithm fails
to meet the fairness constraint and penalty term is proportional to the gap between pulling number
and its required level. To solve this regularized MAB problem, we also propose a hard-threshold
upper conﬁdence bound (UCB) algorithm. It is similar to the classical UCB algorithm but adds an
additional term to encourage the learner to favor those arms whose pulling numbers are below the
required level at each round. The advantage of our approach is that it allows the user to distinguish ,
if desired, between arms for which is more important to sample an arm with required frequency and
those arms for which it is less important to do so."
INTRODUCTION,0.017713365539452495,"To the best of our knowledge, there is no work on mathematical framework of fairness MAB with
regularization term in the literature. In this paper, we provide a relatively complete theory for the
fairness MAB. We rigorously formalize the penalized regret which can be used for evaluating the
performance of learning algorithm under fairness constraints. On theoretical side, the hard-threshold
UCB algorithm is proved to achieve asymptotic fairness when a large penalty rate is chosen. The al-
gorithm is shown to obtain O(log T) regret when the sub-optimality gap is assumed to be ﬁxed. Ad-
ditionally, the characterization of ﬂuctuation of non-fairness level, max1≤t≤T max(τkt −Nk(t), 0)
is also given. Its magnitude is also shown to be O(log T). Moreover, we establish a sub-optimal
gap-free regret bound of proposed method and provide insights on how hard-threshold based UCB
index works. We also point out that the analysis of proposed hard-threshold UCB algorithm is much
harder than the classical UCB due to the existence of interventions between different sub-optimal
arms. On numerical side, the experimental results conﬁrm our theory and show that the perfor-
mance of the proposed algorithm is better than other popular methods. Our method achieves a better
trade-off between reward and fairness."
INTRODUCTION,0.01932367149758454,"Notations. For real number x, (x)+ stands for max{0, x}; ⌊x⌋is the largest integer smaller or equal
to x. For integer n, we use [n] to represent the set {1, . . . , n}. We say a = O(b); a = Ω(b) if there
exists a constant C such that a ≤Cb; a ≥b/C. The symbols E and P(·) denote generic expectation
and probability under a probability measure that may be determined from the context. We let π be a
generic policy / learning algorithm."
ACHIEVING FAIRNESS VIA PENALIZATION,0.020933977455716585,"2
ACHIEVING FAIRNESS VIA PENALIZATION"
ACHIEVING FAIRNESS VIA PENALIZATION,0.02254428341384863,"Consider a stochastic multi-armed bandit problem with K arms and unknown expected rewards
µ1, . . . , µK associated with these arms. The notion of fairness we introduce consists of proportions
τk ≥0, k = 1, . . . , K with τ1 + · · · + τK < 1. We use T ∈{1, 2, . . . , } to denote the time horizon
and Nk,π(t) to denote the number of times that arm k has been pulled by time t ∈[T] using policy
π. For notational simplicity, we may write Nk,π(t) as Nk(t). It is desired to pull arm k at least at
the uniform rate of τk, k = 1, . . . , K. In other words, the learner should obey the constraint that
Nk(t) ≥τkt for any t ∈[T]. Thus a good policy aims to solve the following optimization problem,"
ACHIEVING FAIRNESS VIA PENALIZATION,0.024154589371980676,"arg max
π
E
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.02576489533011272,"k
µkNk,π(T), subject to Nk,π(t) ≥τkt for all k and t.
(2)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.027375201288244767,"Instead of directly working with such a constrained bandit problem, we consider a penalization
problem. That is, one gets penalized if the arm is not pulled sufﬁciently often. To reﬂect this, we
introduce the following design problem. Let Sπ(T) be the sum of the rewards obtained by time
t under policy π, i.e., Sπ(T) = PT
t=1 rπt where πt is the arm index chosen by policy π at time"
ACHIEVING FAIRNESS VIA PENALIZATION,0.028985507246376812,Under review as a conference paper at ICLR 2022
ACHIEVING FAIRNESS VIA PENALIZATION,0.030595813204508857,t ∈[T] and rπt is the corresponding reward. Then the penalized total reward is deﬁned as
ACHIEVING FAIRNESS VIA PENALIZATION,0.0322061191626409,"Spen,π(T) = Sπ(T) − K
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.033816425120772944,"k=1
Ak
 
τkT −Nk,π(T)
"
ACHIEVING FAIRNESS VIA PENALIZATION,0.03542673107890499,"+,
(3)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.037037037037037035,"where A1, . . . , AK are known nonnegative penalty rates. Our goal is to design a learning algorithm
to make the expectation of Spen,π(T) as large as possible. By taking the expectation, we have"
ACHIEVING FAIRNESS VIA PENALIZATION,0.03864734299516908,"E[Spen,π(T)] = K
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.040257648953301126,"k=1
µkE[Nk,π(t)] − K
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.04186795491143317,"k=1
AkE[
 
τkT −Nk,π(T)
"
ACHIEVING FAIRNESS VIA PENALIZATION,0.043478260869565216,"+],
(4)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.04508856682769726,"which is the penalized reward achieved by policy π and we would like to maximize it over π. Now
we are ready to introduce the penalized regret function, which is the core for the regret analysis."
ACHIEVING FAIRNESS VIA PENALIZATION,0.04669887278582931,"To derive the new regret, we ﬁrst note that maximizing E[Spen,π(T)] is the same as minimizing the
following loss function,"
ACHIEVING FAIRNESS VIA PENALIZATION,0.04830917874396135,"L(T) = µ∗T −E[Spen,π(T)] = K
X k=1"
ACHIEVING FAIRNESS VIA PENALIZATION,0.0499194847020934,"h
∆kE[Nk(t)] + AkE[
 
τkT −Nk(T)
"
ACHIEVING FAIRNESS VIA PENALIZATION,0.05152979066022544,"+]
i
,
(5)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.05314009661835749,"where we denote
µ∗=
max
k=1,...,K µk, ∆k = µ∗−µk, k = 1, . . . , K."
ACHIEVING FAIRNESS VIA PENALIZATION,0.05475040257648953,"In order to ﬁnd the minimum possible value of L(T), let us understand what a prophet (who knows
the expected rewards µ1, . . . , µK) would do. Clearly, a prophet (who, in addition, is not constrained
by integer value) would solve the following optimization problem,"
ACHIEVING FAIRNESS VIA PENALIZATION,0.05636070853462158,"min
x1,...,xK K
X k=1"
ACHIEVING FAIRNESS VIA PENALIZATION,0.057971014492753624,"h
∆kxk + AkE
 
τkT −xk
 +"
ACHIEVING FAIRNESS VIA PENALIZATION,0.05958132045088567,"i
subject to K
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.061191626409017714,"k=1
xk = T, xk ≥0, k = 1, . . . , K,"
ACHIEVING FAIRNESS VIA PENALIZATION,0.06280193236714976,"and pull arm k for xk times (k = 1, . . . , K). By denoting yk = xk/T, k ∈[K], we transform this
problem into"
ACHIEVING FAIRNESS VIA PENALIZATION,0.0644122383252818,"min
y1,...,yK K
X k=1"
ACHIEVING FAIRNESS VIA PENALIZATION,0.06602254428341385,"h
∆kyk + Ak
 
τk −yk
 +"
ACHIEVING FAIRNESS VIA PENALIZATION,0.06763285024154589,"i
subject to K
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.06924315619967794,"k=1
yk = 1, yk ≥0, k = 1, . . . , K.
(6)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.07085346215780998,"We will solve the problem (6) by ﬁnding y1, . . . , yK that satisfy the constraints and that minimize
simultaneously each term in the objective function. It is not hard to observe the following facts."
ACHIEVING FAIRNESS VIA PENALIZATION,0.07246376811594203,"1. For A ≥0, function y 7→A(τ −y)+ achieves its minimum value of 0 for y ≥τ.
2. For A ≥∆> 0, function y 7→∆y + A(τ −y)+ achieves its minimum of ∆τ at y = τ.
3. For ∆> A ≥0, function y 7→∆y + A(τ −y)+ achieves its minimum of Aτ at y = 0."
ACHIEVING FAIRNESS VIA PENALIZATION,0.07407407407407407,"As a result, we introduce the following three sets"
ACHIEVING FAIRNESS VIA PENALIZATION,0.07568438003220612,"Aopt =

k ∈[K] : µk = µ∗	
, Acr =

k ∈[K] : Ak ≥∆k > 0
	
, Anon−cr =

k ∈[K] : ∆k > Ak
	
,"
ACHIEVING FAIRNESS VIA PENALIZATION,0.07729468599033816,"where Aopt consists of all optimal arms, Acr consists of sub-optimal arms with penalty rate larger
than (or equal to) the sub-optimal gap and Anon−cr includes sub-optimal arms with penalty rate
smaller than the sub-optimal gap. Therefore an optimal solution to the problem (6) can be con-
structed as follows. Let k∗be an arbitrary arm in Aopt, and choose yk = 
 "
ACHIEVING FAIRNESS VIA PENALIZATION,0.07890499194847021,"1 −P
j∈Acr∪(Aopt\{k∗}) τj,
k = k∗,
τk,
k ∈Acr ∪(Aopt \ {k∗}),
0,
k ∈Anon−cr.
(7)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.08051529790660225,"Therefore, a prophet would choose (modulo rounding) in (5)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.0821256038647343,"Nk(T) = 

 
"
ACHIEVING FAIRNESS VIA PENALIZATION,0.08373590982286634,"
1 −P"
ACHIEVING FAIRNESS VIA PENALIZATION,0.0853462157809984,"j∈Acr∪(Aopt\{k∗}) τj

T,
k = k∗,
τkT,
k ∈Acr ∪(Aopt \ {k∗}),
0,
k ∈Anon−cr,
(8)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.08695652173913043,Under review as a conference paper at ICLR 2022
ACHIEVING FAIRNESS VIA PENALIZATION,0.08856682769726248,"leading to the following optimal value of L(T),"
ACHIEVING FAIRNESS VIA PENALIZATION,0.09017713365539452,"L∗(T) =
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.09178743961352658,"k∈Acr
∆kτkT +
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.09339774557165861,"k∈Anon−cr
AkτkT = K
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.09500805152979067,"k=1
min(∆k, Ak)τk !"
ACHIEVING FAIRNESS VIA PENALIZATION,0.0966183574879227,"T.
(9)"
ACHIEVING FAIRNESS VIA PENALIZATION,0.09822866344605476,"Given an arbitrary algorithm π, we can therefore deﬁne the penalized regret as"
ACHIEVING FAIRNESS VIA PENALIZATION,0.0998389694041868,"Rπ(T) = Lπ(T) −L∗(T) =
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.10144927536231885,"k∈Aopt
AkE
 
τkT −Nk,π(T)
"
ACHIEVING FAIRNESS VIA PENALIZATION,0.10305958132045089,"+
(10) +
X k∈Acr"
ACHIEVING FAIRNESS VIA PENALIZATION,0.10466988727858294,"h
∆kE
 
Nk,π(T) −τkT

+ AkE
 
τkT −Nk,π(T)
 + i +
X"
ACHIEVING FAIRNESS VIA PENALIZATION,0.10628019323671498,k∈Anon−cr
ACHIEVING FAIRNESS VIA PENALIZATION,0.10789049919484701,"h
∆kENk,π(T) + Ak

E
 
τkT −Nk,π(T)
"
ACHIEVING FAIRNESS VIA PENALIZATION,0.10950080515297907,"+ −τkT
i
."
A HARD-THRESHOLD UCB ALGORITHM,0.1111111111111111,"3
A HARD-THRESHOLD UCB ALGORITHM"
A HARD-THRESHOLD UCB ALGORITHM,0.11272141706924316,"We now introduce a UCB-like algorithm which aims to achieve the minimum penalized regret de-
scribed in the previous section. We assume that all rewards take values in the interval [0, 1]. We
denote by X(k)
n
the reward obtained after pulling arm k for the nth time, k ∈[K], n = 1, 2, . . .. Let"
A HARD-THRESHOLD UCB ALGORITHM,0.1143317230273752,"ˆmk(n) =
1
Nn(k)"
A HARD-THRESHOLD UCB ALGORITHM,0.11594202898550725,"Nn(k)
X"
A HARD-THRESHOLD UCB ALGORITHM,0.11755233494363929,"i=1
X(k)
i
, k = 1, . . . , K, n = 1, 2, . . .
(11)"
A HARD-THRESHOLD UCB ALGORITHM,0.11916264090177134,"and introduce the following index: for k = 1, . . . , K, n = 1, 2, . . . set"
A HARD-THRESHOLD UCB ALGORITHM,0.12077294685990338,"ik(n) = ˆmk(n −1) + Ak1
 
Nk(n −1) < τkn

+ s"
LOG N,0.12238325281803543,"2 log n
Nk(n −1).
(12)"
LOG N,0.12399355877616747,"The algorithm proceeds as follows. It starts by pulling each arm once. Then at each subsequent step,
we pull an arm with the highest value of the index ik(n). In equation 12, there is an additional term
Ak1(Nk(n −1) < τkn) compared with classical UCB algorithm. It takes the hard threshold form.
Once the number of times that arm k has been pulled before time n is less than the fairness level
(τkn) at round n, penalty rate Ak will be added to the UCB index. In other words, the proposed algo-
rithm favors those arms which does not meet the fairness requirement. The detailed implementation
is given in Algorithm 1."
THEORETICAL ANALYSIS OF THE HARD-THRESHOLD UCB ALGORITHM,0.12560386473429952,"4
THEORETICAL ANALYSIS OF THE HARD-THRESHOLD UCB ALGORITHM"
THEORETICAL ANALYSIS OF THE HARD-THRESHOLD UCB ALGORITHM,0.12721417069243157,"In this section, we present theoretical results for the hard-threshold UCB algorithm introduced in
Section 3. Throughout this section, we need to introduce additional notation and concepts. We say
τk = ˜Ω(1) if it is a positive constant which is independent of T. We assume that there exists a
positive constant c0 such that P
k τk ≤1 −c0 and T is much larger than K. The penalty rates Ak’s
are assumed to be known ﬁxed constants. The expected reward µk (k ∈[K]) is assumed between 0
and 1. Hence sub-optimality gap ∆k is between 0 and 1 as well."
THEORETICAL ANALYSIS OF THE HARD-THRESHOLD UCB ALGORITHM,0.1288244766505636,"Asymptotic Fairness. Given the large penalty rates, the proposed algorithm can guarantee the
asymptotic fairness for any arm k ∈[K]. In other words, the algorithm can guarantee that the
number of times that arm k has been pulled up to time T is at least ⌊τkT⌋with high probability."
THEORETICAL ANALYSIS OF THE HARD-THRESHOLD UCB ALGORITHM,0.13043478260869565,"Theorem 1 If Ak −∆k ≥min{
q"
LOG T,0.1320450885668277,32 log T
LOG T,0.13365539452495975,"τkT
, 1} and τk = ˜Ω(1) for all k, we have Nk(T) ≥⌊τkT⌋
for any k with probability going to 1 as T →∞."
LOG T,0.13526570048309178,"Theorem 1 tells us that the proposed algorithm treats every arm fairly when the penalty rate domi-
nates the sub-optimality gap."
LOG T,0.13687600644122383,Under review as a conference paper at ICLR 2022
LOG T,0.13848631239935588,Algorithm 1 Hard-Threshold UCB Algorithm.
LOG T,0.14009661835748793,"1: Input. Number of arms K, fairness proportions τk’s, penalty rates Ak’s, time horizon T.
2: Output. Cumulative reward, the number of times that each arm is played (Nk(T), k ∈
{1, . . . , K}.)
3: Initialization.
For each k ∈{1, . . . , K}, we set initial count Nk = 0 and arm-speciﬁc cumulative reward
Rk = 0.
4: while n ≤T do
5:
If n ≤K, we choose kn = n.
6:
If n > K, we choose kn = arg maxk ik(n).
7:
We observe reward rn. We update count Nkn = Nkn +1 and update reward Rkn = Rkn +rn."
LOG T,0.14170692431561996,"8:
We update hard-threshold index for each arm k ∈{1, . . . , K} by calculating"
LOG T,0.143317230273752,ik(n + 1) = Rk/Nk + Ak1(Nk < τk(n + 1)) + r
LOG N,0.14492753623188406,"2 log n Nk
."
LOG N,0.14653784219001612,"9:
Increase time index n by 1.
10: end while
11: Return vector (Nk)."
LOG N,0.14814814814814814,"4.1
REGRET ANALYSIS: UPPER BOUNDS
In this section, we provide upper bounds on the penalized regret deﬁned in equation 10 under two
scenarios. (1) We establish the gap-dependent bound when the sub-optimality ∆k’s are ﬁxed con-
stants. (2) We prove the gap-independent bound when ∆k’s vary within the interval [0, 1]."
LOG N,0.1497584541062802,"Theorem 2 (Gap-dependent Upper bound.) Assume that Ak −∆k ≥ca (ca is a positive constant)
holds for any arm k ∈Aopt ∪Acr and ∆k −Ak ≥
q"
K LOG T,0.15136876006441224,8K log T
K LOG T,0.1529790660225443,"c2
0T
holds for any k ∈Anon-cr. We then
have the following results."
K LOG T,0.15458937198067632,"For any k ∈Aopt ∪Acr, it holds"
K LOG T,0.15619967793880837,E[(τkT −Nk(T))+] = O(1).
K LOG T,0.15780998389694043,"For any k ∈Acr, it holds"
K LOG T,0.15942028985507245,E[Nk(T)] ≤max{8 log T
K LOG T,0.1610305958132045,"∆2
k
, τkT} + O(1)."
K LOG T,0.16264090177133655,"For any arm kj ∈Anon-cr, it holds"
K LOG T,0.1642512077294686,"E[Nk(T)] ≤max{min{
8 log T
(∆k −Ak)2 , τkT}, 8 log T"
K LOG T,0.16586151368760063,"∆2
k
} + O(1)."
K LOG T,0.16747181964573268,"Therefore, we have"
K LOG T,0.16908212560386474,"Rπ(T) ≤
X"
K LOG T,0.1706924315619968,"k∈Acr
(8 log T"
K LOG T,0.1723027375201288,"∆k
−τkT)+ +
X"
K LOG T,0.17391304347826086,"k∈Anon-cr
max{min{ 8 log T"
K LOG T,0.17552334943639292,"∆k −Ak
, (∆k −Ak)τkT}, 8 log T"
K LOG T,0.17713365539452497,"∆k
} + O(K). (13)"
K LOG T,0.178743961352657,"Theorem 2 tells us that the number of times that each arm k in critical set Acr is played is at least
around fairness requirement τkT when the the penalty rate is larger than the sub-optimality gap by
some constant. On the other hand, for each arm k in non-critical set Anon-cr, it could be played less
than fairness requirement when sub-optimality gap substantially dominates the penalty rate. The
total penalized regret has order of log T and is hence nearly not improvable. In addition, when
Ak ≡0 and it degenerates to the classical settings, then all arms become non-critical arms and our
bound reduces to O(P"
K LOG T,0.18035426731078905,"k
8 log T"
K LOG T,0.1819645732689211,"∆k ) which matches the existing result (Auer et al., 2002)."
K LOG T,0.18357487922705315,"Maximal Inequality. In Theorem 2 above we have shown that E[(τkT −Nk(T))+] = O(1) for any
k ∈Aopt ∪Acr under mild conditions on ∆k’s. In the result below, we derive a maximal inequality
for the non-fairness level, (τkt −Nk(t))+, t ∈[T]."
K LOG T,0.18518518518518517,Under review as a conference paper at ICLR 2022
K LOG T,0.18679549114331723,Theorem 3 Order the K arms in such a way that
K LOG T,0.18840579710144928,Ak1 + µk1 ≥. . . ≥Akj + µkj ≥. . . ≥AkK + µkK.
K LOG T,0.19001610305958133,"Then for any arm kj ∈Aopt ∪Acr, we have"
K LOG T,0.19162640901771336,"E[ max
1≤t≤T(τkjt −Nkj(t))+] ≤aj log T + O(1),"
K LOG T,0.1932367149758454,where the coefﬁcient aj is deﬁned as
K LOG T,0.19484702093397746,"aj = 8 j
X"
K LOG T,0.1964573268921095,"d=1
(j −d + 1) d−1
X m=1"
K LOG T,0.19806763285024154,"1
(µkd + Akd −µkm)2 + K
X m=d+1"
K LOG T,0.1996779388083736,"1
(µkd + Akd −µkm −Akm)2 ! . (14)"
K LOG T,0.20128824476650564,"Theorem 3 nearly guarantees the ANY-ROUND fairness for all arms k ∈[K] up to a O(log T)
difference."
K LOG T,0.2028985507246377,"Gap-independent Upper bound. We now switch to establishing a gap-independent upper bound. It
relies on the following observations. The key challenge is how to bound the term E[(τkT −Nk(T))+]
for k ∈Aopt ∪Acr."
K LOG T,0.20450885668276972,"(Observation 1)
If Ak −∆k
≤
4
q"
LOG T,0.20611916264090177,2 log T
LOG T,0.20772946859903382,"T 2/3 , then (Ak −∆k)E[(τkT −Nk(T))+]
≤"
LOG T,0.20933977455716588,4τkT 2/3(2 log T)1/2.
LOG T,0.2109500805152979,"Lemma 1 (Observation 2) If arm k satisﬁes that Ak −∆k ≥4
q"
LOG T,0.21256038647342995,2 log T
LOG T,0.214170692431562,"T 2/3 and τk = ˜Ω(1), then we"
LOG T,0.21578099838969403,have E[(τkT −Nk(T))+] = O(τkKT 2/3).
LOG T,0.21739130434782608,"Based on above observations, we have the following regret bound."
LOG T,0.21900161030595813,"Theorem 4 When τk = ˜Ω(1), it holds that"
LOG T,0.22061191626409019,"Rπ(T) ≤8
p"
LOG T,0.2222222222222222,"T log T(
X k"
LOG T,0.22383252818035426,"√τk) + 8
p"
LOG T,0.22544283413848631,"(1 −τmin)KT log T + AmaxKT 2/3(2 log T)1/2,
(15)"
LOG T,0.22705314009661837,where τmin = mink τk and Amax = maxk Ak.
LOG T,0.2286634460547504,"The ﬁrst term in (15) is for Ak(E(τkT −Nk(T))+) with k ∈Anon-cr. The second term gives a bound
for ∆kE(Nk(T) −τkT) for k ∈[K]. The third term in (15) is for bounding AkE(τkT −Nk(T))+
for k ∈Aopt ∪Acr."
LOG T,0.23027375201288244,"4.2
REGRET ANALYSIS: LOWER BOUNDS"
LOG T,0.2318840579710145,"Gap-dependent Lower Bound.
In this part, we ﬁrst show that the bound given in inequality (13)
is tight. To see this, the results are stated in the following theorems."
LOG T,0.23349436392914655,"Theorem 5 There exists a bandit setting for which the regret of proposed algorithm has the follow-
ing lower bound,"
LOG T,0.23510466988727857,"Rπ(T) ≥
X"
LOG T,0.23671497584541062,"k∈Anon-cr,τk>0"
LOG T,0.23832528180354268,"log T
∆k −Ak
.
(16)"
LOG T,0.23993558776167473,"Theorem 6 There exists a bandit setting for which the regret of proposed algorithm has the follow-
ing lower bound,"
LOG T,0.24154589371980675,"Rπ(T) ≥
X"
LOG T,0.2431561996779388,"k∈Acr
∆k(log T"
LOG T,0.24476650563607086,"∆2
k
−τkT).
(17)"
LOG T,0.2463768115942029,"Theorem 5 says that the term log T/(∆k −Ak) is nearly optimal up a multiplicative constant 8 for
any arm in the non-critical set. Similarly, Theorem 6 tells us that ( 8 log T"
LOG T,0.24798711755233493,"∆k
−τkT)+ is also nearly"
LOG T,0.249597423510467,Under review as a conference paper at ICLR 2022
LOG T,0.25120772946859904,"optimal for arms in the critical set. Therefore, Theorem 2 gives a relatively sharp gap-dependent
upper bound. It is almost impossible to improve the regret bound analysis for our proposed hard-
threshold UCB algorithm in the instance-dependent scenario."
LOG T,0.2528180354267311,"Gap-independent Lower Bound.
We also obtain a gap-independent lower bound as follows."
LOG T,0.25442834138486314,"Theorem 7 Let K > 1 and T be a large integer. Penalty rates A1, A2, . . . , AK are ﬁxed positive
constants. Assume that the fairness parameters τ1, . . . , τK ∈[0, 1] with P"
LOG T,0.2560386473429952,"k τk < 1. Then, for any
policy π, there exists a mean vector µ = (µ1, . . . , µK) such that"
LOG T,0.2576489533011272,"Rπ(T) ≥C(1 −2 max
k
τk)
p"
LOG T,0.25925925925925924,"(K −1)T,"
LOG T,0.2608695652173913,"where C is a universal constant which does not depend on Ak, τk’s."
LOG T,0.26247987117552335,"By comparing Theorems 4 and 7, we can see that there is a substantial gap. This is because term
AkE[(τkT −Nk(T))+] is very hard to handle. This term can be trivially lower bounded below by
zero for any algorithm. However, this term is proved to be O(T 2/3) (ignoring log T factor) by our
current techniques under the proposed algorithm. Whether we can improve the gap-independent
upper bound to be O(T 1/2) is an open question in the future work."
COMMENTS ON THE HARD-THRESHOLD UCB ALGORITHM,0.2640901771336554,"5
COMMENTS ON THE HARD-THRESHOLD UCB ALGORITHM"
COMMENTS ON THE HARD-THRESHOLD UCB ALGORITHM,0.26570048309178745,"On hard threshold. In the proposed algorithm, we use a hard-threshold term Ak1(Nk(n −1) <
τkn) in constructing a UCB-like index ik(n). A natural question is whether we can use a soft-
threshold index by deﬁning"
COMMENTS ON THE HARD-THRESHOLD UCB ALGORITHM,0.2673107890499195,"˜ik(n) = ˆmk(n −1) + Ak
max(τkn −Nk(n −1), 0) τkn
+ s"
LOG N,0.2689210950080515,"2 log n
Nk(n −1)?"
LOG N,0.27053140096618356,"The answer is negative in the sense that ˜ik(n) becomes a continuous function of Nk and does not
have a jump point at the critical value τkn. Hence it does not give sufﬁcient penalization to those
arms k which are below the fairness proportion τk. Hence, a soft-threshold UCB-like index fails to
guarantee the asymptotic fairness and nearly-optimal penalized regret."
LOG N,0.2721417069243156,"When τk is not constant. In our theoretical analysis, we only consider the case that τk = ˜Ω(1) for
ease of presentation. The current results could also apply when threshold τk is dependent on time
horizon T with τk(T) = 1/T b(0 < b < 1)."
LOG N,0.27375201288244766,"Technical Challenges. Since the index ik(n) is a discontinuous function of Nk, this brings addi-
tional difﬁculties in analyzing the regret bound. The most distinguished feature from the classical
regret analysis is that we cannot analyze term Nk(T) separately for each sub-optimal gap k. In fact,
the optimal arm (arg maxk µk) is ﬁxed for all rounds in the classical setting. In contrast, the “opti-
mal arm” (arg maxk µk + Ak1{Nk < τkn}) varies as the algorithm progresses in our framework.
Due to such interventions among different arms, term (τkT −Nk(T))+ should be treated carefully."
LOG N,0.2753623188405797,"Connections to LASSO problems. We would like to point out that our current framework shares
similarities with LASSO problem (Tibshirani, 1996; Zhao & Yu, 2006; Zou, 2006) in linear regres-
sion models. Both of them introduces the penalization terms to enforce the solution to obey fairness
constraints / sparsity to some degree. In our penalized MAB framework, whether an arm k is played
at least τkT times or not depends on the penalty rate Ak and the sub-optimality gap ∆k. Similarly,
in the LASSO framework, whether a coefﬁcient is to be estimated as zero depends on the penalty
parameter and its true coefﬁcient value."
LOG N,0.27697262479871176,Comparison with Baselines. We compare the proposed methods with related existing methods.
LOG N,0.2785829307568438,"Learning with Fairness Guarantee (LFG, Li et al. (2019)). It is implemented via following steps."
LOG N,0.28019323671497587,"• For each round n, we compute the index for each arm, ¯ik(n) = min{ ˆmk(n −1) +
q"
LOG N,0.28180354267310787,"2 log n
Nk(n−1), 1} and compute queue length for each arm, Qk(n) = max{Qk(n −1) +
τk −1{arm k is pulled}, 0}."
LOG N,0.2834138486312399,Under review as a conference paper at ICLR 2022
LOG N,0.28502415458937197,"• The learner plays the arm which maximizes Qk(n) + η0wkik(n) and receive the corre-
sponding reward, where η0 is the tuning parameter and wk is the known weight. Without
loss of generality, we assume wk ≡1 by treating each arm equally when we have no
additional information."
LOG N,0.286634460547504,"Fair-Learn (Flearn, Patil et al. (2020)). Its main procedure is given as below."
LOG N,0.2882447665056361,"• For each round n, we compute set A(n), A(n) := {k : τk(n −1) −Nk(n −1) > α},
which contains those arms which are not fair at round n at level.
• If A(n) ̸= ∅, we play arm which maximizes τk(n −1) −Nk(n −1). Otherwise, we play"
LOG N,0.2898550724637681,"arm which maximizes ˆmk(n −1) +
q"
LOG N,0.2914653784219002,"2 log n
Nk(n−1)."
LOG N,0.29307568438003223,"Fair-learn method can enforce each arm k should be played at proportion level τk only when α = 0.
LFG method fails to guarantee the asymptotic fairness when η0 > 0. Neither of these methods can
well balance between total rewards and fairness constraint as our method does."
EXPERIMENT RESULTS,0.2946859903381642,"6
EXPERIMENT RESULTS"
EXPERIMENT RESULTS,0.2962962962962963,"A In this experimental setting, we examine the relationship between number of times that non-
critical arm k has been pulled at T (=20000) rounds and the inverse gap 1/(∆k −Ak)2. In
particular, we construct the following three parameter settings (τk ≡1/20).
Case 1: K = 9; µ = (0.9, 0.8, 0.7, 0.6, 0.6, 0.4, 0.3, 0.2, 0.1); Ak ≡0.45.
Case 2: K = 9; µ = (0.95, 0.8, 0.7, 0.6, 0.6, 0.4, 0.3, 0.2, 0.1); Ak ≡0.41.
Case 3: K = 9; µ = (0.9, 0.8, 0.7, 0.6, 0.6, 0.425, 0.4, 0.375, 0.35); A ≡0.45.
B Similarly, we examine the relationship between number of times that critical arm k has been
pulled at T (=20000) rounds and the inverse gap 1/∆2
k. We set τk ≡1/20, Ak ≡0.45.
Case 1: K = 9; µ = (0.9, 0.86, 0.84, 0.82, 0.6, 0.4, 0.3, 0.2, 0.1).
Case 2: K = 9; µ = (0.95, 0.85, 0.84, 0.83, 0.82, 0.4, 0.3, 0.2, 0.1).
Case 3: K = 9; µ = (0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1)."
EXPERIMENT RESULTS,0.29790660225442833,"C We investigate the relationship between cumulative penalized regret and total time hori-
zon (T) under three algorithms (proposed method, LFG, and Flearn). The parameters
are constructed as follows.
The number of arms (K) is set to be 5 or 20.
The total
time horizon (T) varies from 500 to 16000. The fairness proportion τk of each arm is
set to be τk = τ/K with τ ∈{0.2, 0.4, 0.8}. The penalty rate Ak is constructed as
Ak ≡(maxk µk −mink µk)/2. Each entry of the mean reward vector (µk) is randomly
generated between [0, 1]. The reward distribution of each arm is a Gaussian distribution,
e.g., N(µk,
1
K2 ). For Flearn algorithm, we take tuning parameter α = 0. For LFG algo-
rithm, we take η0 =
√"
EXPERIMENT RESULTS,0.2995169082125604,T. Each case is replicated for 50 times.
EXPERIMENT RESULTS,0.30112721417069244,"From Figure 1, we can see that the pulling number Nk(T) is proportional to 1/(∆k −Ak)2 for
k ∈Anon-cr when Nk(T) does not reach fairness level τkT. We also see that Nk(T) is proportional
to 1/∆2
k for k ∈Acr when the pulling number is larger than fairness level τkT. These phenomena
match the results in Theorem 2. From Figure 2, we observe that the proposed method achieves
smaller penalized regret compared with LFG and Flearn. This conﬁrms that our method is indeed a
good learning algorithm under penalization framework."
EXPERIMENT RESULTS,0.3027375201288245,"In the appendix, we also study the paths of unfairness level ((τkT −Nk(T))+) when tuning parame-
ter varies and investigate the relationship between total expected reward (PT
t=1 µπt) and unfairness
level (P"
EXPERIMENT RESULTS,0.30434782608695654,"k∈[K](τkT −Nk(T))+) for three algorithms. From Figure 3 (See Appendix A), the paths
of unfairness level show different behaviors under three algorithms. For our method, with scale
parameter decreasing, each arm becomes unfair one by one. By contrast, all arms under both Flearn
and LFG methods suddenly become unfair once scale parameter decreases from 1. This suggests
that our method has sparsity feature as LASSO does, e.g., making arms with small sub-optimality
gap fair. From Figure 4 (See Appendix A), we can tell that the proposed method always achieves
the highest reward given the same unfairness level under different parameter settings. This gives
evidence that hard-threshold UCB algorithm makes better balance between total reward and fairness
constraints compared with other competing methods."
EXPERIMENT RESULTS,0.3059581320450886,Under review as a conference paper at ICLR 2022
EXPERIMENT RESULTS,0.3075684380032206,"Figure 1: Upper row: Nk(T) vs 1/(∆k −Ak)2 for arm k ∈Anon-cr. Bottom row: Nk(T) vs 1/∆2
k
for arm k ∈Acr. In all plots, the blue horizontal line stands for fairness level τkT."
EXPERIMENT RESULTS,0.30917874396135264,Figure 2: Penalized Regret (Rπ(T)) vs Different Time Horizon (T) under different settings.
CONCLUSION,0.3107890499194847,"7
CONCLUSION"
CONCLUSION,0.31239935587761675,"In this paper, we provide a new framework of fairness MAB problem by introducing regularization
terms. The advantage of our new approach is that it allows the user to distinguish between arms for
which is more important to sample an arm with required frequency level and arms for which it is
less important to do so. A hard-threshold UCB algorithm is proposed and is shown to have good
performance under this framework. Unlike other existing algorithms, the proposed algorithm not
only achieves the asymptotic fairness but also handles well in balance between reward and fairness
constraints. A relatively complete theory, including both gap-dependent / independent bounds, has
been established. The new theoretical results contribute to the fairness in machine learning ﬁeld and
bring better insights in how to play smartly in the exploitation and exploration games."
CONCLUSION,0.3140096618357488,Under review as a conference paper at ICLR 2022
REFERENCES,0.31561996779388085,REFERENCES
REFERENCES,0.3172302737520129,"Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John Langford, and Hanna Wallach. A re-
ductions approach to fair classiﬁcation. In International Conference on Machine Learning, pp.
60–69. PMLR, 2018."
REFERENCES,0.3188405797101449,"Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2):235–256, 2002."
REFERENCES,0.32045088566827695,"Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.
In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pp. 207–216. IEEE,
2013."
REFERENCES,0.322061191626409,"Sanjoy K Baruah, Neil K Cohen, C Greg Plaxton, and Donald A Varvel. Proportionate progress: A
notion of fairness in resource allocation. Algorithmica, 15(6):600–625, 1996."
REFERENCES,0.32367149758454106,"Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgen-
stern, Seth Neel, and Aaron Roth.
A convex framework for fair regression.
arXiv preprint
arXiv:1706.02409, 2017."
REFERENCES,0.3252818035426731,"Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan
Hong, Ed H Chi, et al. Fairness in recommendation ranking through pairwise comparisons. In
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 2212–2220, 2019."
REFERENCES,0.32689210950080516,"S´ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. arXiv preprint arXiv:1204.5721, 2012."
REFERENCES,0.3285024154589372,"L Elisa Celis, Damian Straszak, and Nisheeth K Vishnoi. Ranking with fairness constraints. arXiv
preprint arXiv:1704.06840, 2017."
REFERENCES,0.33011272141706927,"Aritra Chatterjee, Ganesh Ghalme, Shweta Jain, Rohit Vaish, and Y Narahari. Analysis of thompson
sampling for stochastic sleeping bandits. In UAI, 2017."
REFERENCES,0.33172302737520126,"Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
Classic and contextual bandits. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.,
2016."
REFERENCES,0.3333333333333333,"Koffka Khan and Wayne Goodridge. S-mdp: Streaming with markov decision processes. IEEE
Transactions on Multimedia, 21(8):2012–2025, 2019."
REFERENCES,0.33494363929146537,"Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping
experts and bandits. Machine learning, 80(2):245–272, 2010."
REFERENCES,0.3365539452495974,"Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances
in applied mathematics, 6(1):4–22, 1985."
REFERENCES,0.33816425120772947,"Fengjiao Li, Jia Liu, and Bo Ji. Combinatorial sleeping bandits with fairness constraints. IEEE
Transactions on Network Science and Engineering, 7(3):1799–1813, 2019."
REFERENCES,0.3397745571658615,"Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. In 8th International Conference on Learning Representations, 2020."
REFERENCES,0.3413848631239936,"Vishakha Patil, Ganesh Ghalme, Vineet Nair, and Y Narahari. Achieving fairness in the stochastic
multi-armed bandit problem. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pp. 5379–5386, 2020."
REFERENCES,0.34299516908212563,"Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian D Ziebart. Fair logistic regression: An
adversarial perspective. 2019."
REFERENCES,0.3446054750402576,"Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American
Mathematical Society, 58(5):527–535, 1952."
REFERENCES,0.3462157809983897,Under review as a conference paper at ICLR 2022
REFERENCES,0.34782608695652173,"Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fairbatch: Batch selection for
model fairness. In 9th International Conference on Learning Representations, 2021."
REFERENCES,0.3494363929146538,"Ashudeep Singh and Thorsten Joachims.
Fairness of exposure in rankings.
In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
2219–2228, 2018."
REFERENCES,0.35104669887278583,"Mohammad Sadegh Talebi and Alexandre Proutiere. Learning proportionally fair allocations with
low regret. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 2(2):
1–31, 2018."
REFERENCES,0.3526570048309179,"Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267–288, 1996."
REFERENCES,0.35426731078904994,"Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical evaluation. In
European conference on machine learning, pp. 437–448. Springer, 2005."
REFERENCES,0.355877616747182,"Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims.
Fairness of exposure in stochastic
bandits. In Proceedings of the 38th International Conference on Machine Learning, volume 139,
pp. 10686–10696. PMLR, 2021."
REFERENCES,0.357487922705314,"Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Thompson sampling for budgeted
multi-armed bandits. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence,
2015."
REFERENCES,0.35909822866344604,"Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171–
1180, 2017a."
REFERENCES,0.3607085346215781,"Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classiﬁcation. In Artiﬁcial Intelligence and Statistics, pp.
962–970. PMLR, 2017b."
REFERENCES,0.36231884057971014,"Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine Learning
Research, 7:2541–2563, 2006."
REFERENCES,0.3639291465378422,"Hui Zou. The adaptive lasso and its oracle properties. Journal of the American statistical associa-
tion, 101(476):1418–1429, 2006."
REFERENCES,0.36553945249597425,Under review as a conference paper at ICLR 2022
REFERENCES,0.3671497584541063,APPENDICES
REFERENCES,0.3687600644122383,"In this appendix, the ﬁrst section is dedicated for experimental results of Experiments D and E. In
the rest, we collect all technical proofs. Speciﬁcally, the proofs of gap-dependent upper and lower
bounds are given in Section B and C. The proofs of gap independent upper and lower bounds are
given in Section E and F, respectively. The proof of E[max1≤t≤T (τkt−Nk(t))]+ is given in Section
D."
REFERENCES,0.37037037037037035,"A
THE PLOTS FOR EXPERIMENTS D AND E"
REFERENCES,0.3719806763285024,"D We investigate the path of unfairness level ((τkT −Nk(T))+) of each arm when the tuning
parameter varies. The parameters of two settings are constructed as follows.
Setting 1: K = 8, T = 10000; (µ1, . . . , µ8) = (0.9, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1); τ1 =
. . . = τ8 =
1
2K . The reward distribution of each arm is a Gaussian distribution, e.g.,
N(µk,
1
K2 ).
Setting 2: K = 8, T = 10000; (µ1, . . . , µ8) = (0.95, 0.7, 0.65, 0.6, 0.2, 0.15, 0.1, 0.05);
τ1 = . . . = τ4 = 0.8 1"
REFERENCES,0.37359098228663445,"K and τ5 = . . . = τ8 = 0.4τ1. Again, the reward distribution of each
arm is a Gaussian distribution, e.g., N(µk,
1
K2 ).
The penalty rates Ak ≡η, where we call η is the scale parameter which takes value between
0 and 1. For Flearn algorithm, the tuning parameter α = (1 −η)τ1T with η varying from
0 to 1. For LFG algorithm, the tuning parameter η0 = (1 −η)T with η ∈(0, 1].
When scale parameter η →1, three algorithms will prefer to exploit the arm with highest
reward and pay less attention to the fairness. On the other hand, η →0, three algorithms
tend to treat the fairness as the priority. (Due to space limit, the results are in Appendix A)"
REFERENCES,0.3752012882447665,"E We investigate the relationship between total expected reward (PT
t=1 µπt) and unfairness
level (P"
REFERENCES,0.37681159420289856,"k∈[K](τkT −Nk(T))+) for three algorithms. The parameters are given as follows.
We set K ∈{5, 20} and τk ≡τ/K with τ ∈{0, 20.5}. Each element in mean reward
vector (µk) is generated between 0 and 1. Moreover, we generate the reward from three
different distributions, (1) Gaussian N(µk,
1
K2 ), (2) Beta Beta(µk, 1 −µk), (3) Bernoulli
Bern(1, µk)."
REFERENCES,0.3784219001610306,"Figure 3: Unfairness path ((τkT −Nk(T))+, k ∈[K]) for three algorithms under two settings
described in Experiment D. (Upper row is for Setting 1 and bottom row is for Setting 2.) For
sub-optimal arms, the proposed method can guarantee the fairness with a wider range of tuning
parameter. By contrast, Flearn and LFG can break the fairness easily."
REFERENCES,0.38003220611916266,Under review as a conference paper at ICLR 2022
REFERENCES,0.38164251207729466,"Figure 4: Total regret vs unfairness level for three algorithms under different settings described in
Experiment E. (The ﬁrst row is for 5 arms with required fraction of times τ = 0.2; The second row
is for 5 arms with required fraction of times τ = 0.5; The third row is for 20 arms with required
fraction of times τ = 0.2; The fourth row is for 20 arms with required fraction of times τ = 0.5.
The ﬁrst column is for Gaussian reward distribution; the second column is for Beta distribution; and
the third column is for Bernoulli distribution.) Given the ﬁxed unfairness level, the proposed method
can have larger total reward than other two methods consistently over all experimental settings."
REFERENCES,0.3832528180354267,Under review as a conference paper at ICLR 2022
REFERENCES,0.38486312399355876,"Highlight of the proof. The technical challenge lies in handling the term (τkT −Nk(T))+. (1)
Our main task in proving upper bound is to show that, for any k ∈Acr, (τkT −Nk(T))+ is O(1)
for in gap-dependent setting and it is ˜O(T 2/3) for gap-independent setting. Unlike the classical
UCB algorithm analysis, we cannot bound Nk(T) separately for each arm k. Instead, we need to
study the relationship between any pair of critical arms and the relationship between critical arm
and non-critical arm. A key step is to ﬁnd a stopping time n1 such that any arm k ∈Acr satisﬁes
Nk(n1) ≥τkn1. Therefore, between rounds n1 and T, the behavior of (τkT −Nk(T))+ can
be well controlled. (2) In proving maximal inequality, we need to order K arms according to the
values of µk + Ak. Then the bound of max1≤t≤T (τkt −Nk(T))+ can be obtained by a recursive
formula (see equation 30) starting from k = k1 to k = kK, where k1 := arg max{µk + Ak} and
kK := arg min{µk + Ak}."
REFERENCES,0.3864734299516908,"B
PROOF OF GAP-DEPENDENT UPPER BOUNDS"
REFERENCES,0.38808373590982287,"Proof of Theorem 1 Its proof is essentially same as the proof of ﬁrst part in Theorem 2 by treating
the non-critical set Anon-cr empty."
REFERENCES,0.3896940418679549,Proof of Theorem 2 We ﬁrst prove the ﬁrst part: E[(τkT −Nk(T))+] = O(1) for any k ∈Aopt∪Acr.
REFERENCES,0.391304347826087,"Suppose at time n that a critical arm k is played less than τkn. We can prove that the algorithm
pulls critical arm k′ at time n such that Nk′(n) ≥8 log T/c2
a and Nk′(n) > τkn with vanishing
probability. This is because"
REFERENCES,0.392914653784219,P(Ak + mk(n) + s
LOG N,0.394524959742351,"2 log n
Nk(n) ≤mk′(n) + s"
LOG N,0.3961352657004831,"2 log n
Nk′(n))"
LOG N,0.3977455716586151,"≤
P(Ak + µk ≤+µ′
k + 2 s"
LOG N,0.3993558776167472,"2 log n
Nk′(n)) + 2 n2"
LOG N,0.40096618357487923,"≤
P(Ak −∆k ≤−∆k′ + 2 s"
LOG N,0.4025764895330113,"2 log n
Nk′(n)) + 2 n2"
LOG N,0.40418679549114334,"=
2/n2.
(18)"
LOG N,0.4057971014492754,"By the same reason, the algorithm pulls non-critical arm k′′ at time n when Nk′′(n) ≥8 log T/c2
a
with vanishing probability."
LOG N,0.4074074074074074,"(Observation 3) In other words, it holds with high probability that once a critical arm k is played
with proportion less than required level τk’s, it must be pulled in next round when all other arms is
played with proportion greater than level τk’s and is played more than 8 log T/c2
a times."
LOG N,0.40901771336553944,"(Observation 4) It also holds with high probability that once a non-critical arm is played more than
8 log T/c2
a, it can be only played when all critical arms are played with frequency more than the
required level τk’s."
LOG N,0.4106280193236715,"Moreover, we can show that Nk′(n) ≥8 log T/c2
a at time n = c0T/2 for each critical arm k′. If
not, note that 8 log T/c2
a ≤τk′c0T/4, then N ′
k(n) < τ ′
kn for any n ∈{⌈c0T/4⌉, . . . , ⌊c0T/2⌋}.
Hence, for any critical arm k′ can be played at most max{τk′c0T/4, 8 log T/c2
a} times between
rounds c0T/2 and c0T; every non-critical arm k′′ can be played at most 8 log T/c2
a times. Then, we
must have
c0T/2 −c0T/4 ≤
X"
LOG N,0.41223832528180354,"k
τkc0T/4 +
X"
LOG N,0.4138486312399356,"k
8 log T/c2
a."
LOG N,0.41545893719806765,"However, the above inequality fails to hold when T/ log T ≥16c2
0K/c2
a. This leads to the contra-
diction. Thus, we have Nk′(n) ≥8 log T/c2
a for any critical arm k′ at time n = c0T/2."
LOG N,0.4170692431561997,"Actually, this further gives us that we must have Nk′(n) ≥⌊τk′n⌋for all very critical arms at some
time n ∈[c0T/2, T]. To see this, we observe the fact that for any arm ¯k, it will be played with prob-
ability less than
2
T 2 at time n once N¯k(n) ≥max{τkn, 8 log T/c2
a} and one critical arm k′ is played
less than τk′n. (In other words, this tells us that once arm ¯k has been played max{τ¯kn, 8 log T/c2
a}
times, then it can only be played at time when all very critical arms k′s have been played for τk′n
times or ⌊τ¯kn⌋jumps by one with probability greater than 1 −2/T 2.)"
LOG N,0.41867954911433175,Under review as a conference paper at ICLR 2022
LOG N,0.42028985507246375,"Let n1(≥c0T/2) be the ﬁrst ever time such that Nk′(n1) ≥⌊τk′n1⌋for all critical arms k′s. By
straightforward calculation, it gives that n1 must be bounded by"
LOG N,0.4219001610305958,"n1 ≤c0T/2 +
X"
LOG N,0.42351046698872785,"k:non-critical
8 log T/c2
a + (
X"
LOG N,0.4251207729468599,"k:critical
τk)T"
LOG N,0.42673107890499196,"with probability greater than 1 −2K/T. That is, n1 is well deﬁned between c0T and T. At time n1,
we have all critical arms k′ such that Nk′(n1) ≥τk′n1."
LOG N,0.428341384863124,"Moreover, we consider the ﬁrst time n2(> n1) such that every non-critical arm k′′ has been played"
LOG N,0.42995169082125606,"for at least 8 log T/c2
a times when ∆k′′ ≤
q"
LOG N,0.43156199677938806,"ca
log(c0T/2)"
LOG T,0.4331723027375201,"16 log T
(If ca > 4, it automatically holds for
any ∆k′′). We claim that n2 ≤n1 + c0T/2. This is because, between rounds n1 and n2, the
algorithm will choose non-critical arm k′′ when Nk′(n) ≥τk′(n) for all critical arms k′s and
Nk′′(n) ≤log(c0T/2)/2∆2
k′′. To see this, we know that"
LOG T,0.43478260869565216,P(mk′(n) + s
LOG N,0.4363929146537842,"2 log n
Nk′(n) ≥mk′′(n) + s"
LOG N,0.43800322061191627,"2 log n
Nk′′(n))"
LOG N,0.4396135265700483,"≤
P(µ′
k + 2 s"
LOG N,0.44122383252818037,"2 log n
Nk′(n) ≥µk′′ + s"
LOG N,0.4428341384863124,"log n
Nk′′(n))"
LOG N,0.4444444444444444,"≤
P(µ′
k + 2 s"
LOG T,0.44605475040257647,"2 log T
τk′c0T/2 ≥µk′′ + s"
LOG T,0.4476650563607085,log(c0T/2)
LOG T,0.4492753623188406,"Nk′′(n)
)"
LOG T,0.45088566827697263,"≤
2/c0T.
(19)"
LOG T,0.4524959742351047,"That is, index of arm k′′ is larger than k′ with high probability."
LOG T,0.45410628019323673,"In other words, for each round between n1 and n2, each critical arm k′ can be only
pulled at most τk′(n2 −n1) before every non-critical arm k′′
has been played for
min{8 log T/c2
a, log(c0T/2)/2∆2
k′′}. Additionally, each non-critical arm k′′ can be only played
for at most 8 log T/(∆k −Ak)2 with high-probability. Therefore, it must hold that"
LOG T,0.4557165861513688,"n2 −n1 ≤(
X"
LOG T,0.4573268921095008,"k
τk)(n2 −n1) +
X"
LOG T,0.45893719806763283,"k
8 log T/(∆k −Ak)2."
LOG T,0.4605475040257649,"However, the above inequality fails to hold when n2−n1 ≥c0T/2 under assumption that ∆k−Ak ≥
q"
K LOG T,0.46215780998389694,8K log T
K LOG T,0.463768115942029,"c2
0T
. This validates the claim n2 ≤n1 + c0T/2."
K LOG T,0.46537842190016104,"Starting from time n2, by the observations 3 and 4, it can be seen that the maximum values of
(τk′n −Nk′(n))+ for any critical arm k′ is always bounded by 1 with probability 1 −2K/T (n ∈
[n2, T]). This completes the proof of the ﬁrst part."
K LOG T,0.4669887278582931,"For the second part, we need to prove E[Nk(T)] ≤max{ 8 log T"
K LOG T,0.46859903381642515,"∆2
k , τkT} + O(1) for k ∈Acr."
K LOG T,0.47020933977455714,When 8 log T
K LOG T,0.4718196457326892,"∆2
k
> τkT, we can calculate the probability"
K LOG T,0.47342995169082125,P(arm k is pulled at round n + 1 |Nk(n) ≥8 log T
K LOG T,0.4750402576489533,"∆2
k
)"
K LOG T,0.47665056360708535,"≤
P(ik(n + 1) ≥ik∗(n + 1))"
K LOG T,0.4782608695652174,"≤
P( ˆmk(n + 1) + s"
K LOG T,0.47987117552334946,2 log(n + 1)
K LOG T,0.48148148148148145,"Nk(n)
≥ˆmk∗(n + 1) + s"
K LOG T,0.4830917874396135,2 log(n + 1)
K LOG T,0.48470209339774556,"Nk(n)
)"
K LOG T,0.4863123993558776,"≤
1/n2 ≤1/(8 log T/∆k)2 ≤1/(τkT)2.
(20)"
K LOG T,0.48792270531400966,Under review as a conference paper at ICLR 2022
K LOG T,0.4895330112721417,When 8 log T
K LOG T,0.49114331723027377,"∆2
k
≤τkT, we can similarly calculate the probability"
K LOG T,0.4927536231884058,"P(arm k is pulled at round n + 1 |Nk(n) ≥τkT)
≤
P(ik(n + 1) ≥ik∗(n + 1))"
K LOG T,0.4943639291465378,"≤
P( ˆmk(n + 1) + s"
K LOG T,0.49597423510466987,2 log(n + 1)
K LOG T,0.4975845410628019,"Nk(n)
≥ˆmk∗(n + 1) + s"
K LOG T,0.499194847020934,2 log(n + 1)
K LOG T,0.500805152979066,"Nk(n)
)"
K LOG T,0.5024154589371981,"≤
1/n2 ≤1/(τkT)2.
(21)"
K LOG T,0.5040257648953301,Hence we can easily obtain that E[Nk(T)] ≤max{ 8 log T
K LOG T,0.5056360708534622,"∆2
k , τkT} + O(1) by union bound."
K LOG T,0.5072463768115942,"For the third part that E[Nk(T)] ≤min{
8 log T
(∆k−Ak)2 , τkT} + O(1) (kj ∈Anon-cr), it follows from the
fact that we can treat µk + Ak as new expected reward for arm k ∈Anon-cr. Thus the corresponding
sub-optimality gap is ∆k −Ak. The result follow by using standard technique in the classical UCB
algorithm. Hence we omit the details here."
K LOG T,0.5088566827697263,"Finally, by combining three parts and straightforward calculation, we obtain the desired gap-
dependent upper bounds. This concludes the proof."
K LOG T,0.5104669887278583,"C
PROOF OF GAP-DEPENDENT LOWER BOUNDS"
K LOG T,0.5120772946859904,"Proof of Theorem 5.
We consider the following setting, where arm 1 is the optimal arm with a
deterministic reward ∆and arms k, (k ≥2) are sub-optimal arms with reward zero. Let penalty
rate Ak = A for all k ∈[K] with ∆> A. Assuming that
8 log T
(∆−A)2 ≤τkT/2, we construct a lower
bound as follows."
K LOG T,0.5136876006441223,"We claim that each arm k ≥2 will be played at least n1 :=
log T
(∆−A)2 times. If there exists an arm k0
has not been played for n1 times, we then consider the time index na = T/2+1+(K −2) 8 log T"
K LOG T,0.5152979066022544,"(∆−A)2 +
n1. At this time, we have that arm 1 is the arm with largest index since that for each sub-optimal
arm k ̸= k0, its index will never exceeds ∆once it has been played
8 log T
(∆−A)2 times. According to
assumption that arm k0 has been played less than n1 times, thus arm 1 is the arm with largest index
at time na."
K LOG T,0.5169082125603864,"However, the index of arm 1 at time na is never larger than
q"
LOG T,0.5185185185185185,2 log T
LOG T,0.5201288244766505,"T/2
+ ∆. The index of arm k0 at"
LOG T,0.5217391304347826,"time na is always larger than A +
q"
LOG T,0.5233494363929146,2 log(T/2)
LOG T,0.5249597423510467,"n1
. It gives"
LOG T,0.5265700483091788,i1(na) ≤ s
LOG T,0.5281803542673108,2 log T
LOG T,0.5297906602254429,"T/2
+ ∆< A + s"
LOG T,0.5314009661835749,2 log(T/2)
LOG T,0.533011272141707,"n1
≤ik0(na),
(22)"
LOG T,0.534621578099839,"which leads to the contradiction of the mechanism of the proposed algorithm. Hence, we have that
each sub-optimal should have been played for at least
log T
(∆−A)2 times."
LOG T,0.5362318840579711,"Proof of Theorem 6. We consider the another setting, where where arm 1 is the optimal arm with
deterministic reward ∆1 + ∆2, arm k’s (k ∈Acr) are sub-optimal arms with reward being ∆1 and
arm k’s (k ∈Anon−cr) ar sub-optimal arms with reward being ∆2. Let penalty rate Ak = A2 for
all k ∈Acr with ∆2 < A2 and penalty rate Ak = A1 for all k ∈Anon−cr with ∆1 > A1. Assume
that P"
LOG T,0.537842190016103,"k∈Anon−cr
8 log T
(∆1−A1)2 + P"
LOG T,0.5394524959742351,"k∈Acr
8 log T"
LOG T,0.5410628019323671,"∆2
2
< T/2 and τkT ≤log T"
LOG T,0.5426731078904992,"∆2
2 for k ∈Acr, we then have
the following lower bound."
LOG T,0.5442834138486312,We claim that for each arm k ∈Acr will be played for at least n2 := log T
LOG T,0.5458937198067633,"∆2
2 times. If not, there will
be at least one arm k1 ∈Acr has been played for less than n2 times. We consider the time stamp,
nb = T/2+1+P
k∈Anon−cr
8 log T
(∆1−A1)2 +P
k∈Acr;k̸=k1
8 log T"
LOG T,0.5475040257648953,"∆2
2
+n2. At this time, we have that arm
1 is the arm with the largest index since that for each arm in Anon−cr, its index is always smaller
than ∆1 +∆2 once it has been played for
8 log T
(∆1−A1)2 times. For each arm k ∈Acr (k ̸= k1), its index"
LOG T,0.5491143317230274,is also smaller than ∆1 +∆2 once it has been played for 8 log T
LOG T,0.5507246376811594,"∆2
2
times. According to assumption that
arm k1 has been played less than n2 times, thus arm 1 is the arm with largest index at time nb."
LOG T,0.5523349436392915,Under review as a conference paper at ICLR 2022
LOG T,0.5539452495974235,"However, on other hand, the index of arm 1 at time nb is never larger than
q"
LOG T,0.5555555555555556,2 log T
LOG T,0.5571658615136876,T/2 +∆1 +∆2. The
LOG T,0.5587761674718197,"index of arm k1 is not smaller than ∆1 +
q"
LOG T,0.5603864734299517,2 log(T/2)
LOG T,0.5619967793880838,"n2
. It leads to"
LOG T,0.5636070853462157,i1(nb) ≤ s
LOG T,0.5652173913043478,2 log T
LOG T,0.5668276972624798,"T/2
+ ∆1 + ∆2 ≤∆1 + s"
LOG T,0.5684380032206119,2 log(T/2)
LOG T,0.5700483091787439,"n2
≤i2(nb),"
LOG T,0.571658615136876,"this contradicts with arm 1 is arm with largest index at time nb. Hence, any arm in Acr should be
played at least log T"
LOG T,0.573268921095008,"∆2
2 times."
LOG T,0.5748792270531401,"D
PROOF OF MAXIMAL INEQUALITY (PROOF OF THEOREM 3)"
LOG T,0.5764895330112721,"We can order K arms according to the sums µk + Ak’s. Speciﬁcally, let the order k1, k2, . . . , kK be
deﬁned by
µk1 + Ak1 > µk2 + Ak2 > · · · > µkK + AkK.
(23)
For simplicity we assume no ties in equation 23. We also assume that Ak > ∆k for all k ∈
Aopt ∪Acr."
LOG T,0.5780998389694042,"We now aim to bound expectations of the E maxt∈[T ]
 
τkt −Nk(t)
"
LOG T,0.5797101449275363,"+ for k ∈Aopt ∪Acr. We will
use the ordering of the arms k1, k2, . . . , kK deﬁned in equation 23. Take any arbitrary t ∈[T] and
let kj ∈Aopt ∪Acr,
m(j)
t
= sup

n = 1, . . . , t : τkjn ≤Nkj(n)
	
.
(24)"
LOG T,0.5813204508856683,"Suppose for a moment that m(j)
t
< t. We have
 
τkjt −Nkj(t)
"
LOG T,0.5829307568438004,"+ ≤τkj
(25)"
LOG T,0.5845410628019324,"+τkj#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1) for some d = 1, . . . , j −1"
LOG T,0.5861513687600645,"+τkj#

n = m(j)
t
+ 1, . . . , t : τkdn ≤Nkd(n −1) for all d = 1, . . . , j −1,"
LOG T,0.5877616747181964,arm kj not pulled at time n
LOG T,0.5893719806763285,"−(1 −τkj)#

n = m(j)
t
+ 1, . . . , t : τkdn ≤Nkd(n −1) for all d = 1, . . . , j −1,"
LOG T,0.5909822866344605,arm kj pulled at time n
LOG T,0.5925925925925926,"=τkj + τkj#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1) for some d = 1, . . . , j −1"
LOG T,0.5942028985507246,"−(1 −τkj)#

n = m(j)
t
+ 1, . . . , t : τkdn ≤Nkd(n −1) for all d = 1, . . . , j −1"
LOG T,0.5958132045088567,"+#

n = m(j)
t
+ 1, . . . , t : τkdn ≤Nkd(n −1) for all d = 1, . . . , j −1,"
LOG T,0.5974235104669887,arm kj not pulled at time n
LOG T,0.5990338164251208,"=τkj + #

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1) for some d = 1, . . . , j −1"
LOG T,0.6006441223832528,"+#

n = m(j)
t
+ 1, . . . , t : τkdn ≤Nkd(n −1) for all d = 1, . . . , j −1,"
LOG T,0.6022544283413849,arm kj not pulled at time n
LOG T,0.6038647342995169,"−(1 −τkj)(t −m(j)
t )."
LOG T,0.605475040257649,"The ﬁnal bound is, clearly, also valid in the case m(j)
t
= t. Next,"
LOG T,0.607085346215781,"#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1) for some d = 1, . . . , j −1
	
(26) = j−1
X"
LOG T,0.6086956521739131,"d=1
#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), m = 1, . . . , d −1
	
."
LOG T,0.6103059581320451,"For d = 1, . . . , j −1 denote"
LOG T,0.6119162640901772,"m(j,d)
t
= sup

n = m(j)
t , . . . , t : τkdn > Nkd(n −1)
	
.
(27)"
LOG T,0.6135265700483091,Under review as a conference paper at ICLR 2022
LOG T,0.6151368760064412,"Suppose, for a moment, that m(j,d)
t
> m(j)
t . Then"
LOG T,0.6167471819645732,"0 < τkdm(j,d)
t
−Nkd
 
m(j,d)
t
−1

= τkdm(j)
t
−Nkd
 
m(j)
t
−1
"
LOG T,0.6183574879227053,"+τkd

m(j,d)
t
−m(j)
t
−#

n = m(j)
t
+ 1, . . . , m(j,d)
t
: arm kd pulled
	"
LOG T,0.6199677938808373,"−(1 −τkd)#

n = m(j)
t
+ 1, . . . , m(j,d)
t
: arm kd pulled"
LOG T,0.6215780998389694,"=τkdm(j)
t
−Nkd
 
m(j)
t
−1

+ τkd
 
m(j,d)
t
−m(j)
t
"
LOG T,0.6231884057971014,"−#

n = m(j)
t
+ 1, . . . , m(j,d)
t
: arm kd pulled
	
.
We conclude that
#

n = m(j)
t
+ 1, . . . , m(j,d)
t
: arm kd pulled"
LOG T,0.6247987117552335,"≤max
n=1,...,t
 
τkdn −Nkd(n)
"
LOG T,0.6264090177133655,"+ + τkd
 
m(j,d)
t
−m(j)
t

."
LOG T,0.6280193236714976,"Therefore,"
LOG T,0.6296296296296297,"#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), m = 1, . . . , d −1"
LOG T,0.6312399355877617,"=#

n = m(j)
t
+ 1, . . . , m(j,d)
t
: τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), m = 1, . . . , d −1"
LOG T,0.6328502415458938,"≤#

n = m(j)
t
+ 1, . . . , m(j,d)
t
: arm kd pulled"
LOG T,0.6344605475040258,"+#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), m = 1, . . . , d −1,"
LOG T,0.6360708534621579,arm kd not pulled
LOG T,0.6376811594202898,"≤max
n=1,...,t
 
τkdn −Nkd(n)
"
LOG T,0.6392914653784219,"+ + τkd
 
t −m(j)
t
"
LOG T,0.6409017713365539,"+#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), m = 1, . . . , d −1,"
LOG T,0.642512077294686,"arm kd not pulled
	
,"
LOG T,0.644122383252818,"and the ﬁnal bound is clearly valid even if m(j,d)
T
= m(j)
T . Substituting this bound into equation 26
we obtain
#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1) for some d = 1, . . . , j −1"
LOG T,0.6457326892109501,"≤
 
t −m(j)
t
 j−1
X"
LOG T,0.6473429951690821,"d=1
τkd + j−1
X"
LOG T,0.6489533011272142,"d=1
max
t′=1,...,t
 
τkdt′ −Nkd(t′)
 + + j−1
X"
LOG T,0.6505636070853462,"d=1
#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), m = 1, . . . , d −1,"
LOG T,0.6521739130434783,"arm kd not pulled
	
,
Substituting this bound into equation 25 gives us
 
τkjt −Nkj(t)
"
LOG T,0.6537842190016103,"+ ≤τkj
(28) + j−1
X"
LOG T,0.6553945249597424,"d=1
max
t′=1,...,t
 
τkdt′ −Nkd(t′)
"
LOG T,0.6570048309178744,"+ −
 
t −m(j)
t
 1 − j
X"
LOG T,0.6586151368760065,"d=1
τkd ! + j
X"
LOG T,0.6602254428341385,"d=1
#

n = m(j)
t
+ 1, . . . , t : τkdn > Nkd(n −1),"
LOG T,0.6618357487922706,"τkmn ≤Nkm(n −1), m = 1, . . . , d −1, arm kd not pulled"
LOG T,0.6634460547504025,"≤τkj + j−1
X"
LOG T,0.6650563607085346,"d=1
max
t′=1,...,t
 
τkdt′ −Nkd(t′)
 + + j
X"
LOG T,0.6666666666666666,"d=1
#

n = 1, . . . , t : τkdn > Nkd(n −1),"
LOG T,0.6682769726247987,"τkmn ≤Nkm(n −1), m = 1, . . . , d −1, arm kd not pulled
	
."
LOG T,0.6698872785829307,Under review as a conference paper at ICLR 2022
LOG T,0.6714975845410628,"Taking the maximum over t on both sides of above inequality, we then have"
LOG T,0.6731078904991948,"max
t=1,...,T
 
τkjt −Nkj(t)
"
LOG T,0.6747181964573269,"+ ≤τkj + j−1
X"
LOG T,0.6763285024154589,"d=1
max
t=1,...,T
 
τkdt −Nkd(t)
"
LOG T,0.677938808373591,"+
(29) + j
X"
LOG T,0.679549114331723,"d=1
#

n = 1, . . . , T : τkdn > Nkd(n −1),"
LOG T,0.6811594202898551,"τkmn ≤Nkm(n −1), m = 1, . . . , d −1, arm kd not pulled
	
."
LOG T,0.6827697262479872,"Therefore, we arrive at"
LOG T,0.6843800322061192,"E

max
t=1,...,T
 
τkjt −Nkj(t)
 +"
LOG T,0.6859903381642513,"
(30)"
LOG T,0.6876006441223832,"≤τkj + E j−1
X"
LOG T,0.6892109500805152,"d=1
max
t=1,...,T
 
τkdt −Nkd(t)
 + ! + j
X"
LOG T,0.6908212560386473,"d=1
E
 T
X"
LOG T,0.6924315619967794,"n=1
1
 
τkdn > Nkd(n −1),"
LOG T,0.6940418679549114,"τkmn ≤Nkm(n −1), m = 1, . . . , d −1, arm kd not pulled

."
LOG T,0.6956521739130435,We will prove that for kd ∈Aopt ∪Acr
LOG T,0.6972624798711755,"E
 T
X"
LOG T,0.6988727858293076,"n=1
1
 
τkdn > Nkd(n −1),
(31)"
LOG T,0.7004830917874396,"τkmn ≤Nkm(n −1), m = 1, . . . , d −1, arm kd not pulled
"
LOG T,0.7020933977455717,≤bd log T + O(1)
LOG T,0.7037037037037037,"for bd > 0 that we will compute. It is elementary that kj ∈Aopt ∪Acr implies kd ∈Aopt ∪Acr
for d = 1, . . . , j −1. Therefore, it will follow from equation 31, equation 30 and a simple inductive
argument that for any kj ∈Aopt ∪Acr,"
LOG T,0.7053140096618358,"E

max
t=1,...,T
 
τkjt −Nkj(t)
 +"
LOG T,0.7069243156199678,"
≤aj log T + O(1)
(32)"
LOG T,0.7085346215780999,"with a1 = b1 and for j > 1, aj = j−1
X"
LOG T,0.7101449275362319,"d=1
ad + j
X"
LOG T,0.711755233494364,"d=1
bd,"
LOG T,0.7133655394524959,"which means that aj = j
X"
LOG T,0.714975845410628,"d=1
(j −d + 1)bd.
(33)"
LOG T,0.71658615136876,"We now prove equation 31. We have Eπ  T
X"
LOG T,0.7181964573268921,"n=1
1
 
τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), m = 1, . . . , d −1, arm kd not pulled
 = d−1
X"
LOG T,0.7198067632850241,"m=1
Eπ  T
X"
LOG T,0.7214170692431562,"n=1
1
 
τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), arm km is pulled at time n
 + K
X"
LOG T,0.7230273752012882,"m=d+1
Eπ  T
X"
LOG T,0.7246376811594203,"n=1
1
 
τkdn > Nkd(n −1), arm km is pulled at time n

."
LOG T,0.7262479871175523,Under review as a conference paper at ICLR 2022
LOG T,0.7278582930756844,Observe that a “no-tie” assumption imposed at the beginning of the section implies that
LOG T,0.7294685990338164,µkd + Akd > µ∗≥µkm.
LOG T,0.7310789049919485,"Therefore, we can use once again the usual UCB-type argument to see that for any m = 1, . . . , d−1,
for any B > 0, Eπ  T
X"
LOG T,0.7326892109500805,"n=1
1
 
τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), arm km is pulled at time n
"
LOG T,0.7342995169082126,"≤B log T + T
X"
LOG T,0.7359098228663447,"n=1
Pπ
 
Nkm(n −1) > B log T,"
LOG T,0.7375201288244766,"τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), arm km is pulled at time n
"
LOG T,0.7391304347826086,"≤B log T + T
X"
LOG T,0.7407407407407407,"n=1
Pπ
 
Nkm(n −1) > B log T,"
LOG T,0.7423510466988728,"τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), ikm(n) ≥ikd(n)
"
LOG T,0.7439613526570048,"≤B log T + T
X"
LOG T,0.7455716586151369,"n=1
Pπ "
LOG T,0.7471819645732689,"Nkm(n −1) > B log T, ˆmkm(n −1) + s"
LOG N,0.748792270531401,"2 log n
Nkm(n −1)"
LOG N,0.750402576489533,≥ˆmkd(n −1) + Akd + s
LOG N,0.7520128824476651,"2 log n
Nkd(n −1) ! ."
LOG N,0.7536231884057971,By carefully choosing
LOG N,0.7552334943639292,"B =
8
(µkd + Akd −µkm)2 ,"
LOG N,0.7568438003220612,"we obtain the bound Eπ  T
X"
LOG N,0.7584541062801933,"n=1
1
 
τkdn > Nkd(n −1), τkmn ≤Nkm(n −1), arm km is pulled at time n

(34)"
LOG N,0.7600644122383253,"≤
8
(µkd + Akd −µkm)2 log T + O(1),"
LOG N,0.7616747181964574,"m = 1, . . . , d −1. The same argument shows that for every m = d + 1, . . . , K, Eπ  T
X"
LOG N,0.7632850241545893,"n=1
1
 
τkdn > Nkd(n −1), arm km is pulled at time n

(35)"
LOG N,0.7648953301127214,"≤
8
(µkd + Akd −µkm −Akm)2 log T + O(1)."
LOG N,0.7665056360708534,"Now equation 34 and equation 35 imply equation 31 with bd = d−1
X m=1"
LOG N,0.7681159420289855,"8
(µkd + Akd −µkm)2 + K
X m=d+1"
LOG N,0.7697262479871175,"8
(µkd + Akd −µkm −Akm)2 .
(36)"
LOG N,0.7713365539452496,"Now it follows from equation 36 and equation 33 that for every j such that kj ∈Aopt ∪Acr,"
LOG N,0.7729468599033816,"aj = 8 j
X"
LOG N,0.7745571658615137,"d=1
(j −d + 1) d−1
X m=1"
LOG N,0.7761674718196457,"1
(µkd + Akd −µkm)2 + K
X m=d+1"
LOG N,0.7777777777777778,"1
(µkd + Akd −µkm −Akm)2 ! ."
LOG N,0.7793880837359098,"(37)
We conclude by equation 32 that every j such that kj ∈Aopt ∪Acr,"
LOG N,0.7809983896940419,"Eπ
 
τkjT −Nkj(T)
"
LOG N,0.782608695652174,"+ ≤aj log T + O(1),
(38)"
LOG N,0.784219001610306,with aj given in equation 37.
LOG N,0.785829307568438,"Remark. In the proof, we assume that there is no tie, i.e., Akj1 + µkj1 ̸= Akj2 + µkj2 for any
j1 ̸= j2 ∈[K]. This assumption is not restrictive since the probability that event “Akj1 + µkj1 ̸=
Akj2 + µkj2 for some j1 ̸= j2 ∈[K].” is zero when we pick penalty rates Ak’s uniformly randomly."
LOG N,0.7874396135265701,Under review as a conference paper at ICLR 2022
LOG N,0.789049919484702,"E
PROOF OF GAP-INDEPENDENT UPPER BOUNDS"
LOG N,0.7906602254428341,"Proof of Lemma 1 We ﬁrst prove that the algorithm pulls arm k′ with Ak′ −∆k′ ≤2
q"
LOG T,0.7922705314009661,2 log T
LOG T,0.7938808373590982,T 2/3 at
LOG T,0.7954911433172303,time n when Nk′(n) ≥T 2/3 and Nk(n) < τkn with vanishing probability. This is because
LOG T,0.7971014492753623,P(Ak + mk(n) + s
LOG N,0.7987117552334944,"2 log n
Nk(n) ≤Ak′ + mk′(n) + s"
LOG N,0.8003220611916264,"2 log n
Nk′(n))"
LOG N,0.8019323671497585,"≤
P(Ak + µk ≤A′
k + µ′
k + 2 s"
LOG N,0.8035426731078905,"2 log n
Nk′(n)) + 2 n2"
LOG N,0.8051529790660226,"≤
P(Ak −∆k ≤A′
k −∆k′ + 2 s"
LOG N,0.8067632850241546,"2 log n
Nk′(n)) + 2 n2"
LOG N,0.8083735909822867,"=
2/n2.
(39)"
LOG N,0.8099838969404187,"Next, we say arm k is a very critical arm if arm k satisﬁes Ak −∆k ≥2
q"
LOG T,0.8115942028985508,2 log T
LOG T,0.8132045088566827,"T 2/3 . Otherwise k
is a non-very critical arm. In other words, each non-very critical arm can be only played at most
O(T 2/3) times with high probability."
LOG T,0.8148148148148148,"Furthermore, we can show that Nk′(n) ≥T 2/3 at time n = c0T/2 for each very critical arm k′.
If not, note that T 2/3 ≤τk′c2
0T/4, then N ′
k(n) < τ ′
kn for any n ∈{⌈c2
0T/4⌉, . . . , ⌊c0T/2⌋}.
Hence, for any arm k′′ can be played at most max{τk′′c0T/2, T 2/3} times between rounds c2
0T/4
and c0T/2. Then, we must have"
LOG T,0.8164251207729468,"c0T/2 −c2
0T/4 ≤
X"
LOG T,0.8180354267310789,"k
τkc0T/2 +
X"
LOG T,0.8196457326892109,"k
T 2/3."
LOG T,0.821256038647343,"However, the above inequality fails to hold when T ≥(4K/c2
0)3. This leads to the contradiction.
Thus, we have Nk′(n) ≥T 2/3 for any very critical arm k′ at time n = c0T/2."
LOG T,0.822866344605475,"This further gives us that we must have Nk′(n) ≥⌊τk′n⌋for all very critical arms at some time
n ∈[c0T, T]. To prove this, we observe the fact that for any arm ¯k, it will be played with probability
less than
2
T 2 at time n once N¯k(n) ≥max{τkn, T 2/3} and one critical arm k′ is played less than
τk′n. (In other words, this tells us that once arm ¯k has been played max{τ¯kn, T 2/3} times, then it
can only be played at time when all very critical arms k′s have been played for τk′n times or ⌊τ¯kn⌋
jumps by one with probability greater than 1 −2/n2.)"
LOG T,0.8244766505636071,"Let n1(≥c0T/2) be the ﬁrst ever time such that Nk′(n1) ≥⌊τk′n1⌋. By straightforward calcula-
tion, it gives that n1 must be bounded by"
LOG T,0.8260869565217391,"n1 ≤c0T/2 +
X"
LOG T,0.8276972624798712,"k′′:non-very critical
T 2/3 + (
X"
LOG T,0.8293075684380032,"k′
τk′)T ≤(c0 +
X"
LOG T,0.8309178743961353,"k′
τk′)T"
LOG T,0.8325281803542673,with probability greater than 1 −2K/T.
LOG T,0.8341384863123994,"That is, n1 is well deﬁned between c0T/2 and T. At time n1, we have all very critical arms k′
such that Nk′(n1) ≥τk′n1. Therefore, starting from time n1, the maximum difference between any
non-fairness level (τk′n −Nk′(n))+’s with k′ in the set of very-critical arms is always bounded by
1 with probability 1 −2K/T for all n ∈[n1, T]."
LOG T,0.8357487922705314,"Lastly, suppose n2 be the last time that arm k is above fairness level. We know at time n = n2, each
very critical arm k′ is played for at least τk′n2 −1. by previous argument. Then in the remaining
T −n2 rounds, we know that each very critical arm is played at most τk′T −τk′n2 + 1. Then we
must have
T −n2 ≤(
X"
LOG T,0.8373590982286635,"k′:very critical
τk′)(T −n2) + K +
X"
LOG T,0.8389694041867954,"k:non-very critical
T 2/3,"
LOG T,0.8405797101449275,"which implies T −n2 ≤(KT 2/3 + K)/c0. This ﬁnally implies that Nk(T) ≥Nk(n2) ≥τkT −
τk(KT 2/3 + K)/c0 −1 with probability at least 1 −2K/T. That is, E[(τkT −Nk(T))+] =
τk(KT 2/3 + K)/c0 + 1 = O(τkKT 2/3)."
LOG T,0.8421900161030595,Under review as a conference paper at ICLR 2022
LOG T,0.8438003220611916,We prove the gap-independent upper bound (Theorem 4) by considering the following situations.
LOG T,0.8454106280193237,"Situation 1.a For arm k ∈Anon-cr and ∆k ≤4
q log T"
LOG T,0.8470209339774557,"T
, the regret on arm k is upper bounded by"
LOG T,0.8486312399355878,"(∆k −Ak)(τkT −Nk(T))
(40)"
LOG T,0.8502415458937198,if 0 ≤Nk(T) ≤τkT; or bounded by
LOG T,0.8518518518518519,"∆k(Nk(T) −τkT) + (∆k −Ak)τkT
(41)"
LOG T,0.8534621578099839,if Nk(T) ≥τkT.
LOG T,0.855072463768116,"Situation 1.b For arm k ∈Anon-cr and ∆k > 4
q log T T
,"
LOG T,0.856682769726248,"• if ∆k −Ak > 4
p"
LOG T,0.8582930756843801,log T/τkT the regret on arm k is upper bounded by
LOG T,0.8599033816425121,"(∆k −Ak)(
8 log T
(∆k −Ak)2 + O(1)).
(42)"
LOG T,0.8615136876006442,"• if ∆k −Ak ≤4
p"
LOG T,0.8631239935587761,log T/τkT the regret on arm k is upper bounded by
LOG T,0.8647342995169082,(∆k −Ak)τkT + ∆k[(8 log T
LOG T,0.8663446054750402,"∆2
k
−τkT)+ + O(1)].
(43)"
LOG T,0.8679549114331723,"In other words, for any arm k ∈Anon-cr, its regret is always bounded by 4 r log T"
LOG T,0.8695652173913043,"T
τkT + 4
p"
LOG T,0.8711755233494364,τkT log T + 4 r log T
LOG T,0.8727858293075684,"T
(Nk(T) −τkT)+.
(44)"
LOG T,0.8743961352657005,"Situation 2 We then split set Aopt ∪Acr into two subsets, Acr, large and Acr,small, where"
LOG T,0.8760064412238325,"Acr, large := {k : Ak −∆k > 4 r"
LOG T,0.8776167471819646,2 log T
LOG T,0.8792270531400966,T 2/3 } and
LOG T,0.8808373590982287,"Acr, small := {k : Ak −∆k ≤4 r"
LOG T,0.8824476650563607,2 log T
LOG T,0.8840579710144928,T 2/3 }.
LOG T,0.8856682769726248,"For arm k ∈Acr,large, we have E[(τkT −Nk(T))+] = O(τkKT 2/3) by Lemma 1. The regret on
arm k is then bounded by"
LOG T,0.8872785829307569,∆kE[Nk(T) −τkT] + O(AkτkKT 2/3)
LOG T,0.8888888888888888,"≤
∆k min{8 log T"
LOG T,0.8904991948470209,"∆2
k
−τkT, Nk(T) −τkT} + O(AkτkKT 2/3).
(45)"
LOG T,0.8921095008051529,"For arm k ∈Acr,small, the regret on arm k is then bounded by"
LOG T,0.893719806763285,"(Ak −∆k)(τkT −Nk(T)) ≤4τkT 2/3(log T)1/2
(46)"
LOG T,0.895330112721417,"if 0 ≤Nk(T) ≤τkT, or"
LOG T,0.8969404186795491,∆k min{8 log T
LOG T,0.8985507246376812,"∆2
k
−τkT + O(1), Nk(T) −τkT}
(47)"
LOG T,0.9001610305958132,if Nk(T) ≥τkT.
LOG T,0.9017713365539453,"In summary, for any arm k ∈Aopt ∪Acr,"
LOG T,0.9033816425120773,∆k min{8 log T
LOG T,0.9049919484702094,"∆2
k
−τkT, Nk(T) −τkT} + O(max{AkτkKT 2/3, 4τkT 2/3(log T)1/2}).
(48)"
LOG T,0.9066022544283414,Under review as a conference paper at ICLR 2022
LOG T,0.9082125603864735,"Combining above situations, the total regret is upper bounded by X"
LOG T,0.9098228663446055,"k∈Anon-cr
max{8
p"
LOG T,0.9114331723027376,τkT log T + 4 r log T
LOG T,0.9130434782608695,"T
(Nk(T) −τkT)+} +
X"
LOG T,0.9146537842190016,"k∈Aopt∪Acr
∆k min{8 log T"
LOG T,0.9162640901771336,"∆2
k
−τkT, Nk(T) −τkT} + O(max{AkτkKT 2/3, 4τkT 2/3(log T)1/2}) ≤
8
p"
LOG T,0.9178743961352657,"T log T(
X"
LOG T,0.9194847020933977,k∈Anon-cr
LOG T,0.9210950080515298,"√τk) + AmaxKT 2/3(log T)1/2(
X"
LOG T,0.9227053140096618,"k∈Acr∪Aopt
τk) + 4 r log T T X"
LOG T,0.9243156199677939,"k∈Anon-cr
(Nk(T) −τkT)+ +
X"
LOG T,0.9259259259259259,k∈Aopt∪Acr p
LOG T,0.927536231884058,"8(Nk(T) −τkT)+ log T ≤
8
p"
LOG T,0.92914653784219,"T log T(
X k"
LOG T,0.9307568438003221,√τk) + AmaxKT 2/3(log T)1/2 + 4 r log T
LOG T,0.9323671497584541,"T
(1 −τmin)T +
p"
"LOG T
P",0.9339774557165862,"8 log T
p"
"LOG T
P",0.9355877616747182,KT(1 −τmin)
"LOG T
P",0.9371980676328503,",
(49)"
"LOG T
P",0.9388083735909822,where 49 uses the fact that P
"LOG T
P",0.9404186795491143,k∈Acr∪Aopt τk ≤P
"LOG T
P",0.9420289855072463,k τk ≤1; P
"LOG T
P",0.9436392914653784,"k∈Anon-cr(Nk(T) −τkT)+ ≤T(1 −
τmin) and X"
"LOG T
P",0.9452495974235104,k∈Aopt∪Acr p
"LOG T
P",0.9468599033816425,"(Nk(T) −τkT)+ ≤
X k p"
"LOG T
P",0.9484702093397746,"(Nk(T) −τkT)+ ≤
s K
X"
"LOG T
P",0.9500805152979066,"k
(Nk(T) −τkT)+ ≤
p"
"LOG T
P",0.9516908212560387,KT(1 −τmin)
"LOG T
P",0.9533011272141707,by Jenson’s inequality.
"LOG T
P",0.9549114331723028,"F
PROOF OF GAP-INDEPENDENT LOWER BOUNDS"
"LOG T
P",0.9565217391304348,"Consider a K-arm setting with µ2 = µ3 = . . . = µK = 0, µ1 = ∆(0 < ∆< 1/2),
A1, A2, . . . , AK > 0, ∆< Ak for k ∈[K], τ1, τ2, . . . , τK ∈[0, 1]."
"LOG T
P",0.9581320450885669,"Since PT
k=2 Nk(T)
≤
T,
then it holds Eπ[Nk1(T)]
≤
T/(K −1) with k1
=
arg mink>1 Eπ[Nk(T)] for any policy π. We then construct another K-arm setting with µk1 = 2∆
and all other parameters remain the same."
"LOG T
P",0.9597423510466989,"For policy π, the regret of the ﬁrst setting is"
"LOG T
P",0.961352657004831,"R1,π(T) ≥AE[(τ1T −N1(T))+] + {∆E[Nk1(T) −τk1T] + AE(τk1T −Nk1(T))+}"
"LOG T
P",0.9629629629629629,and the regret of the second setting is
"LOG T
P",0.964573268921095,"R2,π(T) ≥AE[(τk1T −Nk1(T))+] + {∆E[N1(T) −τ1T] + AE(τ1T −N1(T))+}"
"LOG T
P",0.966183574879227,"If N1(T) < (1 + τ1 −τk1)T/2, then R1,π(T) ≥∆
1−τ1−τk1"
"LOG T
P",0.9677938808373591,"2
T. While N1(T) > (1 + τ1 −τk1)/2,
then R2,π(T) ≥∆
1−τ1−τk1"
"LOG T
P",0.9694041867954911,"2
T. In other words, for policy π,"
"LOG T
P",0.9710144927536232,"worst regret
≥
1
2(R1,π(T) + R2,π(T))"
"LOG T
P",0.9726247987117552,"≥
1
2(∆T 1 −τ1 −τk1"
"LOG T
P",0.9742351046698873,"2
P(N1(T) < 1 + τ1 −τk1"
"LOG T
P",0.9758454106280193,"2
) + ∆T 1 −τ1 −τk1"
"LOG T
P",0.9774557165861514,"2
P(N1(T) ≥1 + τ1 −τk1 2
))"
"LOG T
P",0.9790660225442834,"≥
(1 −τ1 −τk1)∆T"
"LOG T
P",0.9806763285024155,"8
exp{−KL(P1∥P2)}
(50)"
"LOG T
P",0.9822866344605475,"≥
(1 −τ1 −τk1)∆T"
"LOG T
P",0.9838969404186796,"8
exp{−CT∆2/(K −1)},
(51)"
"LOG T
P",0.9855072463768116,"where P1 and P2 are two probability distributions under two settings associated with policy
π; 50 follows from the Bretagnolle–Huber inequality. Inequality 51 holds since KL-divergence
KL(P1∥P2) ≤CT∆2/(K −1) for many probability distributions. (E.g. C = 1/2 if the reward of
each arm follows Gaussian distribution with variance 1.)"
"LOG T
P",0.9871175523349437,Under review as a conference paper at ICLR 2022
"LOG T
P",0.9887278582930756,"Taking ∆=
q K−1"
"LOG T
P",0.9903381642512077,"CT , we have"
"LOG T
P",0.9919484702093397,"worst regret
≥
(1 −τ1 −maxk̸=1 τk)∆T"
"LOG T
P",0.9935587761674718,"8
exp{−CT∆2/(K −1)}"
"LOG T
P",0.9951690821256038,"≥
(1 −2 maxk τk)
p"
"LOG T
P",0.9967793880837359,"(K −1)T/C
8e
,
(52)"
"LOG T
P",0.998389694041868,where e = exp{1}. This completes the proof of Theorem 7.
