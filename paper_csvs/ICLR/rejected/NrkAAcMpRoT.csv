Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0008347245409015025,"Minwise hashing (MinHash) is an important and practical algorithm for generat-
ing random hashes to approximate the Jaccard (resemblance) similarity in mas-
sive binary (0/1) data. The basic theory of MinHash requires applying hundreds
or even thousands of independent random permutations to each data vector in the
dataset, in order to obtain reliable results for (e.g.,) building large-scale learning
models or approximate near neighbor search in massive data. In this paper, we
propose Circulant MinHash (C-MinHash) and provide the surprising theoret-
ical results that using only two independent random permutations in a circulant
manner leads to uniformly smaller Jaccard estimation variance than that of the
classical MinHash with K independent permutations. Experiments are conducted
to show the effectiveness of the proposed method. We also analyze a more conve-
nient C-MinHash variant which reduces two permutations to just one, with exten-
sive numerical results to validate that it achieves essentially the same estimation
accuracy as using two permutations with rigorous theory."
INTRODUCTION,0.001669449081803005,"1
INTRODUCTION"
INTRODUCTION,0.0025041736227045075,"Given two D-dimensional binary vectors v, w ∈{0, 1}D, the Jaccard similarity is deﬁned as"
INTRODUCTION,0.00333889816360601,"J(v, w) =
PD
i=1 1{vi = wi = 1}
PD
i=1 1{vi + wi ≥1}
,
(1)"
INTRODUCTION,0.004173622704507512,"which is a commonly used similarity metric in machine learning and web search applications. The
vectors v and w can also be viewed as two sets of items (which represent the locations of non-zero
entries), the Jaccard similarity can be equivalently viewed as the size of set intersection over the size
of set union. The well-known method of “minwise hashing” (MinHash) (Broder, 1997; Broder et al.,
1997; 1998; Li & Church, 2005; Li & König, 2011) is a standard technique for computing/estimating
the Jaccard similarity in massive binary datasets, with numerous applications such as near neighbor
search, duplicate detection, malware detection, web search, clustering, large-scale learning, social
networks, computer vision, etc. (Charikar, 2002; Fetterly et al., 2003; Henzinger, 2006; Das et al.,
2007; Buehrer & Chellapilla, 2008; Bendersky & Croft, 2009; Chierichetti et al., 2009; Lee et al.,
2010; Li et al., 2011; Deng et al., 2012; Chum & Matas, 2012; Shrivastava & Li, 2012; He et al.,
2013; Tamersoy et al., 2014; Shrivastava & Li, 2014; Zamora et al., 2016)."
INTRODUCTION,0.005008347245409015,"1.1
A REVIEW OF MINWISE HASHING (MINHASH)"
INTRODUCTION,0.005843071786310518,"Algorithm 1 Minwise-hashing (MinHash)
Input: Binary data vector v ∈{0, 1}D,
K independent permutations π1, ..., πK: [D] →[D]."
INTRODUCTION,0.00667779632721202,"Output: K hash values h1(v), ..., hK(v).
For k = 1 to K
hk(v) ←mini:vi̸=0 πk(i)
End For"
INTRODUCTION,0.007512520868113523,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.008347245409015025,"For simplicity, Algorithm 1 considers just one vector v ∈{0, 1}D. In order to generate K hash val-
ues for v, we assume K independent permutations: π1, ..., πK : [D] 7→[D]. For each permutation,
the hash value is the ﬁrst non-zero location in the permuted vector, i.e., hk(v) = mini:vi̸=0 πk(i),
∀k = 1, ..., K. Similarly, for another binary vector w ∈{0, 1}D, using the same K permutations,
we can also obtain K hash values, hk(w). The estimator of J(v, w) is simply"
INTRODUCTION,0.009181969949916527,"ˆJMH(v, w) = 1 K K
X"
INTRODUCTION,0.01001669449081803,"k=1
1{hk(v) = hk(w)},
(2)"
INTRODUCTION,0.010851419031719533,"where 1{·} is the indicator function. By fundamental probability and the independence among the
permutations, it is easy to show that"
INTRODUCTION,0.011686143572621035,"E[ ˆJMH] = J,
V ar[ ˆJMH] = J(1 −J)"
INTRODUCTION,0.012520868113522538,"K
.
(3)"
INTRODUCTION,0.01335559265442404,"How large is K? The answer depends on the application domains. For example, for training large-
scale machine learning models, it appears that K = 512 or K = 1024 might be sufﬁcient (Li et al.,
2011). However, for approximate near neighbor search using many hash tables (Indyk & Motwani,
1998), it is likely that K might have to be much larger than 1024 (Shrivastava & Li, 2012; 2014)."
INTRODUCTION,0.014190317195325543,"In the early work of MinHash (Broder, 1997; Broder et al., 1997), actually only one permutation
was used by storing the ﬁrst K non-zero locations after the permutation. Later Li & Church (2005)
proposed better estimators to improve the estimation accuracy. The major drawback of the original
scheme was that the hashed values did not form a metric space (e.g., satisfying the triangle inequal-
ity) and hence could not be used in many algorithms/applications. We believe this was the main
reason why the original authors moved to using K permutations (Broder et al., 1998)."
OUTLINE OF MAIN RESULTS,0.015025041736227046,"1.2
OUTLINE OF MAIN RESULTS"
OUTLINE OF MAIN RESULTS,0.015859766277128547,"From K Permutations to two. The main idea of this work, is to replace the independent per-
mutations in MinHash with “circulant” permutations. Thus, we name the proposed framework C-
MinHash (circulant MinHash). The “circulant” trick was used in the literature of random projec-
tions. For example, Yu et al. (2017) showed that using circulant projections hurts the estimation
accuracy, but not by too much when the data are sparse. In Section 3, we present some (perhaps
surprising) theoretical ﬁndings that we just need 2 permutations in MinHash and the results (esti-
mation variances) are even more accurate. Basically, with the initial permutation (denoted by σ),
we randomly shufﬂe the data to break whatever structure which might exist in the original data,
and then the second permutation (denoted by π) is applied and re-used K times to generate K
hash values, via circulation. This method is called C-MinHash-(σ, π). Before that, in Section 2, we
analyze a simpler variant C-MinHash-(0, π) without initial permutation σ. Although it is not our
recommended method, our analysis for C-MinHash-(0, π) provides the necessary preparation for
later methods and the intuition to understand the need for the initial permutation."
OUTLINE OF MAIN RESULTS,0.01669449081803005,"From Two Permutations to one. In Section 5, we provide a more convenient variant, C-MinHash-
(π, π), that only needs one permutation π for both pre-processing and hashing. Although the theo-
retical analysis becomes very complicated, we are able to explicitly write down the expectation of
the estimator, which is no longer unbiased but the bias is extremely small and has essentially no im-
pact on the estimation accuracy (mean square errors). Extensive experiments are provided to verify
that this variant has same estimation accuracy as C-MinHash-(σ, π) using two permutations."
OUTLINE OF MAIN RESULTS,0.017529215358931552,"In this paper, we mainly focus on presenting the fundamental idea of C-MinHash and the theoretical
analysis on its uniform variance reduction with non-trivial efforts. Besides improving the Jaccard
estimation accuracy when directly implemented in place of MinHash, the proposed C-MinHash
mechanism can also be conveniently used as a tool to improve more MinHash based algorithms,
for example, the One Permutation Hashing (OPH, Li et al. (2012)). When combined with OPH, C-
MinHash can reduce the required one permutation to effectively 1/K permutation only. Since such
combination is a research direction which requires independent introduction and efforts, we present
the work in a separate manuscript (which could be anonymously shared with Referees if needed)."
OUTLINE OF MAIN RESULTS,0.018363939899833055,Under review as a conference paper at ICLR 2022
OUTLINE OF MAIN RESULTS,0.019198664440734557,"Algorithm 2 C-MinHash-(0, π)"
OUTLINE OF MAIN RESULTS,0.02003338898163606,"Input: Binary data vector v ∈{0, 1}D,
Permutation vector π: [D] →[D]
Output: Hash values h1(v), ..., hK(v)"
OUTLINE OF MAIN RESULTS,0.020868113522537562,"For k = 1 to K
Shift π circulantly rightwards by k units: πk = π→k
hk(v) ←mini:vi̸=0 π→k(i)
End For 𝟑"
OUTLINE OF MAIN RESULTS,0.021702838063439065,Circular
OUTLINE OF MAIN RESULTS,0.022537562604340568,"Shift
𝟏
𝟔"
OUTLINE OF MAIN RESULTS,0.02337228714524207,"𝝅→𝒌= {𝟓, 𝟖, 𝟏, 𝟕, 𝟑, 𝟒, 𝟔, 𝟐} 𝟏 𝟖 𝒗𝟖
𝟔 𝟕
𝟑 𝟓"
OUTLINE OF MAIN RESULTS,0.024207011686143573,"𝝅→𝒌+𝟏= {𝟐, 𝟓, 𝟖, 𝟏, 𝟕, 𝟑, 𝟒, 𝟔} 𝟒"
OUTLINE OF MAIN RESULTS,0.025041736227045076,"𝟐
𝒗𝟏
𝒗𝟐 𝒗𝟑"
OUTLINE OF MAIN RESULTS,0.02587646076794658,"𝒗𝟒
𝒗𝟓
𝒗𝟔 𝒗𝟕 𝒗𝟖 𝒗𝟏
𝒗𝟐 𝒗𝟑"
OUTLINE OF MAIN RESULTS,0.02671118530884808,"𝒗𝟒
𝒗𝟓
𝒗𝟔 𝒗𝟕 𝟓 𝟖 𝟕
𝟒 𝟐"
OUTLINE OF MAIN RESULTS,0.027545909849749584,"Figure 1: An illustration of the idea of C-MinHash. The data vector has three non-zeros, v2 = v4 =
v5 = 1. In this example, hk(v) = 3, hk+1(v) = 1."
OUTLINE OF MAIN RESULTS,0.028380634390651086,"2
C-MINHASH-(0, π) WITHOUT INITIAL PERMUTATION"
OUTLINE OF MAIN RESULTS,0.02921535893155259,"As shown in Algorithm 2, the C-MinHash algorithm has similar operations as MinHash. The differ-
ence lies in the permutations used in the hashing process. To generate each hash hk(v), we permute
the data vector using π→k, which is the permutation shifted k units circulantly towards right based
on π. For example, π = [3, 1, 2, 4], π→1 = [4, 3, 1, 2], π→2 = [2, 4, 3, 1], etc. Conceptually, we may
think of circulation as concatenating the ﬁrst and last elements of a vector to form a circle; see Fig-
ure 1. We set the hash value hk(v) as the position of the ﬁrst non-zero after being permuted by π→k.
Analogously, we deﬁne the C-MinHash-(0, π) estimator of the Jaccard similarity J(v, w) as"
OUTLINE OF MAIN RESULTS,0.03005008347245409,"ˆJ0,π = 1 K K
X"
OUTLINE OF MAIN RESULTS,0.030884808013355594,"k=1
1{hk(v) = hk(w)},
(4)"
OUTLINE OF MAIN RESULTS,0.03171953255425709,"where h is the hash value output by Algorithm 2. In this paper, for simplicity, we assume K ≤D."
OUTLINE OF MAIN RESULTS,0.0325542570951586,"Next, we present the theoretical analysis for Algorithm 2, in terms of the expectation (mean) and the
variance of the estimator ˆJ0,π. Our results reveal that the estimation accuracy depends on the initial
data distribution, which may lead to undesirable performance behaviors when real-world datasets
exhibit various structures. On the other hand, while it is not our recommended method, the analysis
serves a preparation (and insight) for the C-MinHash-(σ, π) which will soon be described."
OUTLINE OF MAIN RESULTS,0.0333889816360601,"First, we introduce some notations and deﬁnitions. Given v, w ∈{0, 1}D, we deﬁne a and f as a = D
X"
OUTLINE OF MAIN RESULTS,0.034223706176961605,"i=1
1{vi = 1 and wi = 1},
f = D
X"
OUTLINE OF MAIN RESULTS,0.035058430717863104,"i=1
1{vi = 1 or wi = 1}.
(5)"
OUTLINE OF MAIN RESULTS,0.03589315525876461,"We say that (v, w) is a (D, f, a)-data pair, whose Jaccard similarity can also be written as J = a/f.
Deﬁnition 2.1. Consider v, w ∈{0, 1}D. Deﬁne the location vector as x ∈{O, ×, −}D, with xi
being “O”, “×”, “−”, when vi = wi = 1, vi + wi = 1 and vi = wi = 0, respectively."
OUTLINE OF MAIN RESULTS,0.03672787979966611,"The location vector x can fully characterize a hash collision. When a permutation π→k is applied,
the hashes hk(v) and hk(w) would collide if after permutation, the ﬁrst “O” is placed before the
ﬁrst “×” (counting from small to large). This observation will be the key in our theoretical analysis."
OUTLINE OF MAIN RESULTS,0.037562604340567615,Under review as a conference paper at ICLR 2022
OUTLINE OF MAIN RESULTS,0.038397328881469114,"Deﬁnition 2.2. For A, B ∈{O, ×, −}, let {(A, B)|△} denote the set {(i, j) : (xi, xj) =
(A, B), j −i = △}. For each 1 ≤△≤K −1, deﬁne
L0(△) = {(O, O)|△}, L1(△) = {(O, ×)}, L2(△) = {(O, −)},
G0(△) = {(−, O)|△}, G1(△) = {(−, ×)} , G2(△) = {(−, −)},
H0(△) = {(×, O)|△}, H1(△) = {(×, ×)}, H2(△) = {(×, −)}.
Remark 2.1. For the ease of notation, by circulation we write xj = xj−D when D < j < 2D."
OUTLINE OF MAIN RESULTS,0.03923205342237062,"Deﬁnition 2.2 measures the relative location of different types of points in the location vector, for a
speciﬁc pair of data vectors. One can easily verify that for ∀1 ≤△≤K −1,
|L0| + |L1| + |L2| = |L0| + |G0| + |H0| = a,
|G0| + |G1| + |G2| = |L2| + |G2| + |H2| = D −f,
|H0| + |H1| + |H2| = |L1| + |G1| + |H1| = f −a,
(6)"
OUTLINE OF MAIN RESULTS,0.04006677796327212,"which is the intrinsic constraints on the size of above sets. We are now ready to analyze the expecta-
tion and variance of ˆJ0,π. It is easy to see that ˆJ0,π is still unbiased, i.e., E[ ˆJ0,π] = J, by linearity of
expectation. Lemma 2.1 provides an important quantity that leads to V ar[ ˆJ0,π] as in Theorem 2.2.
Lemma 2.1. For any 1 ≤s < t ≤K with t −s = △, we have"
OUTLINE OF MAIN RESULTS,0.040901502504173626,"Eπ

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

= |L0(△)| + (|G0(△)| + |L2(△)|)J"
OUTLINE OF MAIN RESULTS,0.041736227045075125,"f + |G0(△)| + |G1(△)|
,"
OUTLINE OF MAIN RESULTS,0.04257095158597663,"where the sets are deﬁned in Deﬁnition 2.2 and hs, ht are the hash values as in Algorithm 2.
Theorem 2.2. Under the same setting as in Lemma 2.1, the variance of ˆJ0,π is"
OUTLINE OF MAIN RESULTS,0.04340567612687813,"V ar[ ˆJ0,π] = J"
OUTLINE OF MAIN RESULTS,0.04424040066777963,"K + 2 PK
s=2(s −1)ΘK−s+1"
OUTLINE OF MAIN RESULTS,0.045075125208681135,"K2
−J2,"
OUTLINE OF MAIN RESULTS,0.045909849749582635,"where Θ△≜Eπ

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

as in Lemma 2.1 with any t −s = △."
OUTLINE OF MAIN RESULTS,0.04674457429048414,"From Theorem 2.2, we see that the variance of ˆJ0,π depends on a, f, and the size of sets L’s and
G’s as in Deﬁnition 2.1, which is determined by the location vector x. Since we use the original
data vectors without randomly permuting the entries beforehand, V ar[ ˆJ0,π] is called “location-
dependent” as it is dependent on the location of non-zero entries of the original data."
OUTLINE OF MAIN RESULTS,0.04757929883138564,"3
C-MINHASH-(σ, π) WITH INDEPENDENT INITIAL PERMUTATION"
OUTLINE OF MAIN RESULTS,0.048414023372287146,"Algorithm 3 C-MinHash-(σ, π)"
OUTLINE OF MAIN RESULTS,0.049248747913188645,"Input: Binary data vector v ∈{0, 1}D,
Permutation vectors π and σ: [D] →[D]
Output: Hash values h1(v), ..., hK(v)"
OUTLINE OF MAIN RESULTS,0.05008347245409015,"Initial permutation: v′ = σ(v)
For k = 1 to K
Shift π circulantly rightwards by k units: πk = π→k
hk(v) ←mini:v′
i̸=0 π→k(i)
End For"
OUTLINE OF MAIN RESULTS,0.05091819699499165,"The method C-MinHash-(σ, π) is summarized in Algorithm 3, which is very similar to Algorithm 2.
This time, as pre-processing, we apply an initial permutation σ |="
OUTLINE OF MAIN RESULTS,0.05175292153589316,"π on the data to break whatever
structures which might exist. Similarly, we deﬁne the C-MinHash-(σ, π) estimator of J as"
OUTLINE OF MAIN RESULTS,0.052587646076794656,"ˆJσ,π = 1 K K
X"
OUTLINE OF MAIN RESULTS,0.05342237061769616,"k=1
1{hk(v) = hk(w)},
(7)"
OUTLINE OF MAIN RESULTS,0.05425709515859766,"where hk’s are the hash values output by Algorithm 3. We will present our detailed theoretical
analysis and the main result (Theorem 3.5). First, by linearity of expectation and that σ and π are
independent, ˆJσ,π is still an unbiased estimator E[ ˆJσ,π] = E[1{hi(v) = hi(w)}] = J. To provide
an immediate intuition, the following proposition emphasizes that the collision indicator variables in
(7) are pairwise negatively correlated, which intuitively explains the source of variance reduction."
OUTLINE OF MAIN RESULTS,0.05509181969949917,Under review as a conference paper at ICLR 2022
OUTLINE OF MAIN RESULTS,0.055926544240400666,"Proposition 3.1. In (7), Cov(1{hi(v) = hi(w)}, 1{hj(v) = hj(w)}) < 0, ∀i ̸= j."
OUTLINE OF MAIN RESULTS,0.05676126878130217,"Proof.
Denote ˜E = Ei̸=j[1{hi(v) = hi(w)}1{hj(v) = hj(w)}], for i ̸= j. The task is then
to show that Cov(1{hi(v) = hi(w)}, 1{hj(v) = hj(w)}) = ˜E −J2 < 0. By Theorem 3.5,
V ar[ ˆJMH] < V ar[ ˆJσ,π], where V ar[ ˆJMH] = J(1−J) K
and"
OUTLINE OF MAIN RESULTS,0.05759599332220367,"V ar[ ˆJσ,π] = E

ˆJσ,π
2
−J2 = E  "
K,0.05843071786310518,"1
K K
X"
K,0.05926544240400668,"k=1
1{hk(v) = hk(w)} !2 −J2 =
P"
K,0.06010016694490818,"i E[1{hi(v) = hi(w)}] K2
+ P"
K,0.06093489148580968,i̸=j E[1{hi(v) = hi(w)}1{hj(v) = hj(w)}]
K,0.06176961602671119,"K2
−J2 = J"
K,0.06260434056761269,K + K(K −1)E[1{hi(v) = hi(w)}1{hj(v) = hj(w)}]
K,0.06343906510851419,"K2
−J2,
where i ̸= j = J"
K,0.0642737896494157,K + (K −1) ˜E
K,0.0651085141903172,"K
−J2 < J(1 −J)"
K,0.0659432387312187,"K
=⇒

˜E −J2 
1 −1 K"
K,0.0667779632721202,"
< 0 =⇒˜E −J2 < 0.
□"
K,0.0676126878130217,"Next, we formally present the path to our main result. The next Theorem gives the variance of ˆJσ,π.
Theorem 3.2. Let a, f be deﬁned as in (5). When 0 < a < f ≤D (J /∈{0, 1}), we have"
K,0.06844741235392321,"V ar[ ˆJσ,π] = J"
K,0.06928213689482471,K + (K −1) ˜E
K,0.07011686143572621,"K
−J2,
(8)"
K,0.0709515859766277,"where with l = max(0, D −2f + a), and"
K,0.07178631051752922,"˜E =Ei̸=j[1{hi(v) = hi(w)}1{hj(v) = hj(w)}] =
X"
K,0.07262103505843072,"{l0,l2,g0,g1}"
K,0.07345575959933222,"( 
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f  ×"
K,0.07429048414023372,"D−f−1
X s=l"
K,0.07512520868113523," D−f
s
"
K,0.07595993322203673," D−a−1
D−f−1
"
K,0.07679465776293823," 
f−a−1
D−f−s−1
  s
n1
 D−f−s
n2
 D−f−s
n3
 f−a−(D−f−s)
n4
 
a−1
a−l1−l2
"
K,0.07762938230383973," D−1
a
 ) . (9)"
K,0.07846410684474124,"The feasible set {l0, l2, g0, g1} satisﬁes the intrinsic constraints (6), and"
K,0.07929883138564274,"n1 = g0 −(D −f −s −g1),
n2 = D −f −s −g1,
n3 = l2 −g0 + (D −f −s −g1), n4 = l1 −(D −f −s −g1)."
K,0.08013355592654424,"Also, when a = 0 or f = a (J = 0 or J = 1), we have V ar[ ˆJσ,π] = 0."
K,0.08096828046744574,"Since the original locational structure of the data is broken by the initial permutation σ, V ar[ ˆJσ,π]
only depends on the value of (D, f, a), i.e., it is “location-independent”. This would make the
performance of C-MinHash-(σ, π) consistent in different tasks. Same as MinHash, Proposition 3.3
states that given D and f, the variance of ˆJσ,π is symmetric about J = 0.5, as illustrated in Figure 2,
which also shows that the variance of ˆJσ,π is smaller than the variance of the original MinHash."
K,0.08180300500834725,"Proposition 3.3 (Symmetry). V ar[ ˆJσ,π] is the same for the (D, f, a)-data pair and the (D, f, f −
a)-data pair, ∀0 ≤a ≤f ≤D."
K,0.08263772954924875,"0
0.2
0.4
0.6
0.8
1
J 1 2 3"
K,0.08347245409015025,"4
5
10-4"
K,0.08430717863105175,"MinHash
 0.2 0.8"
K,0.08514190317195326,f/D = 1
K,0.08597662771285476,D = 1000  K = 500
K,0.08681135225375626,"0
0.2
0.4
0.6
0.8
1
J 1 2 3 10-4"
K,0.08764607679465776,MinHash
K,0.08848080133555926,"0.2
 0.4
 0.6 0.8"
K,0.08931552587646077,f/D = 1
K,0.09015025041736227,D = 1000  K = 800
K,0.09098497495826377,"Figure 2: V ar[ ˆJσ,π] versus J, with D = 1000 and varying f. Left: K = 500. Right: K = 800."
K,0.09181969949916527,Under review as a conference paper at ICLR 2022
K,0.09265442404006678,"101
102
103 D 10-3 10-2 10-1 100"
K,0.09348914858096828,J = 0.1
K,0.09432387312186978,J = 0.3
K,0.09515859766277128,J = 0.5
K,0.09599332220367279,J = 0.9
K,0.09682804674457429,f = 10
K,0.09766277128547579,"0
0.2
0.4
0.6
0.8
1
J 2 3 4 5"
K,0.09849749582637729,Variance Ratio
K,0.0993322203672788,"0.2
 0.4
 0.6 0.8"
K,0.1001669449081803,f/D = 1
K,0.1010016694490818,"0
0.2
0.4
0.6
0.8
1
K/D 100 101 102"
K,0.1018363939899833,Variance Ratio
K,0.10267111853088481,f/D = 0.1
K,0.10350584307178631,"0.7
0.9"
K,0.10434056761268781,"f/D = 1
D = 1000"
K,0.10517529215358931,"Figure 3: Left: Theoretical ˜E, f = 10. Each dash line represents the corresponding J2. Mid:
Variance ratio V ar[ ˆ
JMH]
V ar[ ˆ
Jσ,π] , D = 1000, K = 800. Right: Variance ratio V ar[ ˆ
JMH]
V ar[ ˆ
Jσ,π] , D = 1000."
K,0.10601001669449082,"A rigorous comparison of V ar[ ˆJσ,π] and V ar[ ˆJMH] appears to be a challenging task given the
complicated combinatorial form of V ar[ ˆJσ,π]. The following lemma characterizes an important
property of ˜E in (9), that it is monotone in D when a and f are ﬁxed, as illustrated in Figure 3 (left)."
K,0.10684474123539232,"Lemma 3.4 (Increasing Increment). Assume f > a > 0 are arbitrary and ﬁxed. Denote ˜ED as in
(9) in Theorem 3.2, with D treated as a parameter. Then we have ˜ED+1 > ˜ED for ∀D ≥f."
K,0.10767946577629382,"Equipped with Lemma 3.4, we arrive at the following main theoretical result of this work, on the
uniform variance reduction of C-MinHash-(σ, π).
Theorem 3.5 (Uniform Superiority). For any two binary vectors v, w ∈{0, 1}D with J ̸= 0 or 1,
it holds that V ar[ ˆJσ,π(v, w)] < V ar[ ˆJMH(v, w)]."
K,0.10851419031719532,"That is, using merely two permutations as C-MinHash-(σ, π) improves the Jaccard estimation vari-
ance of classical MinHash, in all cases. Next, in Figure 3 (mid) and Theorem 3.6, we show that
interestingly, the relative improvement is actually the same for any J, for given D, f and K.
Proposition 3.6 (Consistent Improvement). Suppose f is ﬁxed. In terms of a, the variance ratio
V ar[ ˆ
JMH(v,w)]
V ar[ ˆ
Jσ,π(v,w)] is constant for any 0 < a < f."
K,0.10934891485809682,"How is the improvement affected by the sparsity (i.e., f) and the number of hashes K? In Figure 3
(right), we plot the variance ratio V ar[ ˆ
JMH]
V ar[ ˆ
Jσ,π] with different f and K, given ﬁxed D. We see that the
improvement in variance increases with K (more hashes) and f (more non-zero entries). Note that,
by Proposition 3.6, here we do not need to consider a which does not affect the variance ratio. The
results in Figure 3 again verify Theorem 3.5, as the variance ratio is always greater than 1."
NUMERICAL EXPERIMENTS,0.11018363939899833,"4
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.11101836393989983,"In this section, we provide numerical experiments to validate our theoretical ﬁndings and demon-
strate that C-MinHash can indeed lead to smaller Jaccard estimation errors."
NUMERICAL EXPERIMENTS,0.11185308848080133,"4.1
SANITY CHECK: A SIMULATION STUDY"
NUMERICAL EXPERIMENTS,0.11268781302170283,"A simulation study is conducted on synthetic data to verify the theoretical variances given by The-
orem 2.2 and Theorem 3.2. We simulate D = 128 dimensional binary vector pairs (v, w) with
different f and a, which have a special locational structure that the location vector x is such that a
“O”’s are followed by (f −a) “×”’s and then followed by (D −f) “−”’s sequentially. We plot the
empirical and theoretical mean square errors (MSE = variance + bias2) in Figure 4:"
NUMERICAL EXPERIMENTS,0.11352253756260434,"• The theoretical variance agrees with the empirical observation, as the curves overlap, con-
ﬁrming Theorem 2.2 and Theorem 3.2. The variance reduction increases with larger K."
NUMERICAL EXPERIMENTS,0.11435726210350584,"• V ar[ ˆJσ,π] (C-MinHash-(σ, π)) is always smaller than V ar[ ˆJMH], as stated by Theo-
rem 3.5. In contrast, V ar[ ˆJ0,π] (C-MinHash-(0, π)) varies signiﬁcantly depending on dif-
ferent data structures, as mentioned in Section 2."
NUMERICAL EXPERIMENTS,0.11519198664440734,Under review as a conference paper at ICLR 2022
NUMERICAL EXPERIMENTS,0.11602671118530884,"1
20
40
60
80
100 120
K 10-4 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.11686143572621036,"(D,f,a) = (128,8,1)"
NUMERICAL EXPERIMENTS,0.11769616026711185,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.11853088480801335,"1
20
40
60
80
100 120
K 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.11936560934891485,"(D,f,a) = (128,8,2)"
NUMERICAL EXPERIMENTS,0.12020033388981637,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.12103505843071787,"1
20
40
60
80
100 120
K 10-3 10-2 10-1 100 MSE"
NUMERICAL EXPERIMENTS,0.12186978297161936,"(D,f,a) = (128,8,4)"
NUMERICAL EXPERIMENTS,0.12270450751252086,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.12353923205342238,"1
20
40
60
80
100 120
K 10-4 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.12437395659432388,"(D,f,a) = (128,64,2)"
NUMERICAL EXPERIMENTS,0.12520868113522537,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.12604340567612687,"1
20
40
60
80
100 120
K 10-4 10-3 10-2 10-1 MSE"
NUMERICAL EXPERIMENTS,0.12687813021702837,"(D,f,a) = (128,64,8)"
NUMERICAL EXPERIMENTS,0.12771285475792987,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.1285475792988314,"1
20
40
60
80
100 120
K 10-6 10-4 10-2 100 MSE"
NUMERICAL EXPERIMENTS,0.1293823038397329,"(D,f,a) = (128,128,16)"
NUMERICAL EXPERIMENTS,0.1302170283806344,"Simulation
(0, ) Theory
( , ) Theory
MinHash"
NUMERICAL EXPERIMENTS,0.1310517529215359,"Figure 4: Empirical vs. theoretical variance of ˆJ0,π (C-MinHash-(0, π)) and ˆJσ,π (C-MinHash-
(σ, π)), on synthetic binary data vector pairs with different data patterns."
NUMERICAL EXPERIMENTS,0.1318864774624374,"28
 29
210
211
212 K 0.4 0.6 0.8"
NUMERICAL EXPERIMENTS,0.1327212020033389,Mean Absolute Error 10-2 BBC
NUMERICAL EXPERIMENTS,0.1335559265442404,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.1343906510851419,"28
 29
210
211
212 K 0.5 1 1.5"
NUMERICAL EXPERIMENTS,0.1352253756260434,Mean Absolute Error 10-2 NIPS
NUMERICAL EXPERIMENTS,0.13606010016694492,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.13689482470784642,"25
26
27
28
29 K 10-2 10-1"
NUMERICAL EXPERIMENTS,0.13772954924874792,Mean Absolute Error MNIST
NUMERICAL EXPERIMENTS,0.13856427378964942,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.13939899833055092,"26
 27
 28
 29
210 K 10-2 10-1"
NUMERICAL EXPERIMENTS,0.14023372287145242,Mean Absolute Error CIFAR
NUMERICAL EXPERIMENTS,0.14106844741235391,"MinHash
C-MinHash-(0, )
C-MinHash-( , )"
NUMERICAL EXPERIMENTS,0.1419031719532554,Figure 5: Mean Absolute Error ( MAE) of MinHash and C-MinHash on real-world datasets.
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14273789649415694,"4.2
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14357262103505844,"We test C-MinHash on four datasets, including two text datasets: the NIPS full paper dataset from
UCI repository (Dua & Graff, 2017), and the BBC News dataset (Greene & Cunningham, 2006),
and two popular image datasets: the MNIST dataset (LeCun et al., 1998) with hand-written digits,
and the CIFAR dataset (Krizhevsky, 2009) containing natural images. All the datasets are processed
to be binary. For each dataset with n data vectors, there are in total n(n −1)/2 data vector pairs.
We estimate the Jaccard similarities for all the pairs and report the mean absolute errors (MAE). The
results are averaged over 10 independent repetitions, for each dataset, as shown in Figure 5:"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14440734557595994,"• The MAE of C-MinHash-(σ, π) is consistently smaller than that of MinHash, again con-
ﬁrming Theorem 3.5 on the beneﬁt of the improved variance. The improvements become
more substantial with larger K, which is consistent with the trend in Figure 3.
• Without the initial permutation σ, the accuracy of C-MinHash-(0, π) is affected by the
distribution of the original data, and it is worse than C-MinHash-(σ, π) on all these four
datasets. Also, the performance of C-MinHash-(0, π) on image data seems much worse
than that on text data, which we believe is because the image datasets contain more struc-
tural patterns. This again suggests that the initial permutation σ might be needed in practice."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14524207011686144,"In summary, the simulation study has veriﬁed the correctness of our theoretical ﬁndings in Theo-
rem 2.2 and Theorem 3.2. The experiments with Jaccard estimation on four datasets conﬁrm that
C-MinHash is more accurate than the original MinHash. The initial permutation σ is recommended."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14607679465776294,"5
C-MINHASH-(π, π): PRACTICALLY REDUCING TO ONE PERMUTATION"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14691151919866444,"We propose a more convenient variant, C-MinHash-(π, π), which only requires one permutation.
That is, π is used for both pre-processing and circulant hashing. The procedure is the same as Al-
gorithm 3, except that the initial permutation σ is replaced by π. Denote the Jaccard estimator ˆJπ,π."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14774624373956594,Under review as a conference paper at ICLR 2022
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14858096828046743,"The complicated dependency between π (initial permutation) and π→k (hashing) makes the theoret-
ical analysis challenging. In the following, we provide the mean of each hash collision indicator.
Theorem 5.1. Assume K ≤D and let a, f be deﬁned as (5). The location vector x is deﬁned in
Deﬁnition 2.1. Denote B1 = {i : xi = O}, B2 = {i : xi = ×} and B3 = {i : xi = −}. For
a ≤j ≤D and 1 ≤k ≤K, deﬁne"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.14941569282136896,"A−(j) = {xi : (i + k −1 mod D) + 1 ≤j},
A+(j) = {xi : (i + k −1 mod D) + 1 > j}."
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15025041736227046,"Let n−,1(j) = |{xi = O : i ∈A−(j)}| be the number of “O” points in A−(j). Analogously let
n−,2(j), n−,3(j) be the number of “×” and “−” points in A−(j), and n+,1(j), n+,2(j), n+,3(j)
be the number of “O”, “×” and “−” points in A+(j). Then, for the k-th C-MinHash-(π, π) hash,"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15108514190317196,"E[1{hk(v) = hk(w)}] = D
X j=1 X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15191986644407346,"Z∈Θj
Pj(Z) ·
n
3
X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15275459098497496,"i=1
Ψi(j) +
X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15358931552587646,"i∈B1
(1 −
z−,1
n−,1(i∗)) ˜P1
o
,"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15442404006677796,"where Pj(Z) is the density function of Z
= (z−,k|3
1, z+,k|3
1) which follows hyper(D, D −
f, n−,k(j)|3
1, n+,k(j)|3
1), with domain Θj. For q = 1, 2, 3, denote 1#
q = 1{j# ∈Bq}, and"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15525876460767946,"Ψq(j) =
X"
JACCARD ESTIMATION ON TEXT AND IMAGE DATASETS,0.15609348914858096,"i∈Bq,j<i∗"
X,0.15692821368948248,"3
X"
X,0.15776293823038398,"p=1
1#
p
 
1 −
z−,p
n−,p(j) + 1{q = 3}(2
z−,p
n−,p(j) −1)

(1 −
z+,q
n+,q(j)) ˜Pq ¯Jq +
X"
X,0.15859766277128548,"i∈Bq,j>i∗"
X,0.15943238731218698,"h
1#
q (1 −
z−,q
n−,q(j))(1 −
z−,q
n−,q(j) −1) +"
X,0.16026711185308848,"3
X"
X,0.16110183639398998,"p̸=q
1#
p (1 −
z−,q
n−,q(j))(1 −
z−,p
n−,p(j))
i
˜PqJ∗,"
X,0.16193656093489148,"where i∗= (i + k −1 mod D) + 1, i# = (i −k −1 mod D) + 1, ∀i. Deﬁne J∗=
a−r1
f−r1−r2 , and"
X,0.16277128547579298,"˜P1 = ˜P2 =
1
r1 + r2"
X,0.1636060100166945," b0
r3
 D−j−b0
r1+r2−1
"
X,0.164440734557596," D−f
r3
 
f
r1+r2
,
˜P3 = 1 r3"
X,0.1652754590984975,"  b0
r3−1
 D−j−b0
r1+r2
"
X,0.166110183639399," D−f
r3
 
f
r1+r2
 ,"
X,0.1669449081803005,¯Jq = r1 −1{q = 1}
X,0.167779632721202,"D −j −b0
+ (1 −r1 + r2 −1{q ̸= 3}"
X,0.1686143572621035,"D −j −b0
)J∗, q = 1, 2, 3,"
X,0.169449081803005,"where b0 = P3
k=1 z+,k, r1 = a−z−,1−z+,1, r2 = f−a−z−,2−z+,2 and r3 = D−f−z−,3−z+,3."
X,0.17028380634390652,"Theorem 5.1 says that E[1{hk(v) = hk(w)}] would be different for different k. Figure 6 presents
numerical examples to validate the theory and demonstrate the magnitude of bias2 (recall MSE =
bias2 + variance), where the simulations match perfectly Theorem 5.1. As the bias of E[1{hk(v) ="
X,0.17111853088480802,"0
20
40
60
K 0 1 2 3 Bias2 10-6"
X,0.17195325542570952,"(64,4,1)"
X,0.17278797996661102,"Simulation
Theory"
X,0.17362270450751252,"0
20
40
60
K 0 1 2 3 4 Bias2 10-6"
X,0.17445742904841402,"(64,4,2)"
X,0.17529215358931552,"Simulation
Theory"
X,0.17612687813021702,"0
20
40
60
K 0 0.5 1 Bias2 10-5"
X,0.17696160267111852,"(64,32,16)"
X,0.17779632721202004,"Simulation
Theory"
X,0.17863105175292154,"0
20
40
60
k 0.24 0.245 0.25 0.255 0.26"
X,0.17946577629382304,Mean Estimate
X,0.18030050083472454,"(D,f,a) = (64,4,1)
Simulation
Theory
True J"
X,0.18113522537562604,"0
20
40
60
k 0.49 0.495 0.5 0.505 0.51"
X,0.18196994991652754,Mean Estimate
X,0.18280467445742904,"(64,4,2)"
X,0.18363939899833054,"Simulation
Theory
True J"
X,0.18447412353923207,"0
20
40
60
k 0.49 0.495 0.5 0.505 0.51"
X,0.18530884808013356,Mean Estimate
X,0.18614357262103506,"(64,32,16)"
X,0.18697829716193656,"Simulation
Theory
True J"
X,0.18781302170283806,"Figure 6: 1st row: bias2 = (E[ ˆJπ,π] −J)2 vs. number of hashes K on simulated (D, f, a)-data
pairs, D = 64. 2nd row: mean of each collision indicator."
X,0.18864774624373956,Under review as a conference paper at ICLR 2022
X,0.18948247078464106,"hk(w)}] can be positive or negative, the overall bias of ˆJπ,π would approach 0 as K increases. From
the 2nd row, we see that bias2 is very small (10−5 or even smaller) and indeed converges to 0 with
more hashes, i.e., the averaging effect. Also, from the proof (see Appendix A.8) we can know that
when a and f are ﬁxed, as D increases, E[ ˆJπ,π] would converge to J."
X,0.19031719532554256,"We found through extensive numerical experiments that, the MSE of ˆJπ,π is essentially the same
as ˆJσ,π. Figure 7 (1st row) compares the empirical of C-MinHash-(π, π) with the theoretical vari-
ances of C-MinHash-(σ, π), where the simulated data has the same special locational structure as in
Section 4.1. In the 2nd row, we present the MAE comparison on real datasets, where we see that
the curves for these two estimators ( ˆJσ,π and ˆJπ,π) match well. In all ﬁgures, the overlapping MSE
curves verify our claim that we just need one permutation π. Due to the space limitation, we provide
more empirical justiﬁcations in Appendix B."
X,0.1911519198664441,"100
101
102 K 10-3 10-2 10-1 MSE"
X,0.19198664440734559,"(128,8,2)"
PERM,0.19282136894824708,"1 Perm
2 Perm Theo."
PERM,0.19365609348914858,"100
101
102 K 10-3 10-2 10-1 100 MSE"
PERM,0.19449081803005008,"(128,8,4)"
PERM,0.19532554257095158,"1 Perm
2 Perm Theo."
PERM,0.19616026711185308,"100
101
102 K 10-4 10-3 10-2 10-1 MSE"
PERM,0.19699499165275458,"(128,64,8)"
PERM,0.19782971619365608,"1 Perm
2 Perm Theo."
PERM,0.1986644407345576,"100
101
102 K 10-6 10-4 10-2 100 MSE"
PERM,0.1994991652754591,"(128,128,16)"
PERM,0.2003338898163606,"1 Perm
2 Perm Theo."
PERM,0.2011686143572621,"26
 27
 28
 29
210
211
212 K 0.5 1 1.5"
PERM,0.2020033388981636,Mean Absolute Error 10-2 BBC
PERM,0.2028380634390651,"2 Perm
1 Perm"
PERM,0.2036727879799666,"28
 29
210
211
212 K 0.5 1 1.5"
PERM,0.2045075125208681,Mean Absolute Error 10-2 NIPS
PERM,0.20534223706176963,"2 Perm
1 Perm"
PERM,0.20617696160267113,"25
26
27
28
29 K 10-2 10-1"
PERM,0.20701168614357263,Mean Absolute Error MNIST
PERM,0.20784641068447413,"1 Perm
2 Perm"
PERM,0.20868113522537562,"26
 27
 28
 29
210 K 10-2 10-1"
PERM,0.20951585976627712,Mean Absolute Error CIFAR
PERM,0.21035058430717862,"2 Perm
1 Perm"
PERM,0.21118530884808012,"Figure 7: 1st row: estimator MSE vs. K on simulated data pairs. 2nd row: MAE of Jaccard
estimation on four datasets. “1 Perm” is C-MinHash-(π, π), and “2 Perm” is C-MinHash-(σ, π)."
CONCLUSION,0.21202003338898165,"6
CONCLUSION"
CONCLUSION,0.21285475792988315,"The method of minwise hashing (MinHash), from the seminal works of Broder and his colleagues,
has become standard in industrial practice. One fundamental reason for its wide applicability is
that the binary (0/1) high-dimensional representation is convenient and suitable for a wide range of
practical scenarios. The so-called Jaccard similarity is a popular measure of similarity in binary data."
CONCLUSION,0.21368948247078465,"To estimate the Jaccard similarity J, standard MinHash requires to use K independent permuta-
tions, where K, the number of hashes, can be several hundreds or even thousands in practice. In this
paper, we proposed Circulant MinHash (C-MinHash) present the surprising theoretical results that,
with merely 2 permutations, we still obtain an unbiased estimate of the Jaccard similarity with the
variance strictly smaller than that of the original MinHash, as conﬁrmed by numerical experiments
on simulated and real datasets. The initial permutation is applied to break whatever structure the
original data may exhibit. The second permutation is re-used K times in a circulant shifting fash-
ion. Moreover, we analyze a more convenient C-MinHash variance which uses only 1 permutation
for both pre-processing and circulant hashing. We derive the complicated mean estimation, and
validate through extensive experiments that it has the same Jaccard estimation accuracy as using 2
permutations with strict theoretical guarantee."
CONCLUSION,0.21452420701168615,"Practically speaking, our theoretical results may reveal a useful direction for designing hashing
methods. For example, in many applications, using permutation vectors of length (e.g.,) 230 might
be sufﬁcient. While it is perhaps unrealistic to store (e.g.,) K = 1024 such permutation vectors in
the memory, one can afford to store two such permutations (even in GPU memory). Using perfectly
random permutations in lieu of approximate permutations would simplify the design and analysis of
randomized algorithms and ensure that the practical performance strictly matches the theory."
CONCLUSION,0.21535893155258765,"Finally, we mention that our work can be used as a building block to improve Li et al. (2012),
reducing the required one permutation to effectively 1/K permutation only. If needed, we would be
happy to anonymously share the technical manuscript with the related results."
CONCLUSION,0.21619365609348914,Under review as a conference paper at ICLR 2022
REFERENCES,0.21702838063439064,REFERENCES
REFERENCES,0.21786310517529214,"Michael Bendersky and W. Bruce Croft. Finding text reuse on the web. In Proceedings of the
Second International Conference on Web Search and Web Data Mining (WSDM), pp. 262–271,
Barcelona, Spain, 2009."
REFERENCES,0.21869782971619364,"Andrei Z. Broder. On the resemblance and containment of documents. In Proceedings of the Con-
ference on Compression and Complexity of SEQUENCES, pp. 21–29, Positano, Amalﬁtan Coast,
Salerno, Italy, 1997."
REFERENCES,0.21953255425709517,"Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. Syntactic clustering
of the web. Comput. Networks, 29(8-13):1157–1166, 1997."
REFERENCES,0.22036727879799667,"Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael Mitzenmacher. Min-wise inde-
pendent permutations. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing (STOC), pp. 327–336, Dallas, TX, 1998."
REFERENCES,0.22120200333889817,"Gregory Buehrer and Kumar Chellapilla. A scalable pattern mining approach to web graph com-
pression with communities. In Proceedings of the International Conference on Web Search and
Web Data Mining (WSDM), pp. 95–106, Stanford, CA, 2008."
REFERENCES,0.22203672787979967,"Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings on
34th Annual ACM Symposium on Theory of Computing (STOC), pp. 380–388, Montreal, Canada,
2002."
REFERENCES,0.22287145242070117,"Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Michael Mitzenmacher, Alessandro Panconesi, and
Prabhakar Raghavan. On compressing social networks. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD), pp. 219–228, Paris,
France, 2009."
REFERENCES,0.22370617696160267,"Ondrej Chum and Jiri Matas. Fast computation of min-hash signatures for image collections. In
Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 3077–3084, Providence, RI, 2012."
REFERENCES,0.22454090150250416,"Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyamsundar Rajaram.
Google news per-
sonalization: scalable online collaborative ﬁltering. In Proceedings of the 16th International
Conference on World Wide Web (WWW), pp. 271–280, Banff, Alberta, Canada, 2007."
REFERENCES,0.22537562604340566,"Fan Deng, Stefan Siersdorfer, and Sergej Zerr. Efﬁcient jaccard-based diversity analysis of large
document collections. In Proceedings of the 21st ACM International Conference on Information
and Knowledge Management (CIKM), pp. 1402–1411, Maui, HI, 2012."
REFERENCES,0.2262103505843072,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.2270450751252087,"Dennis Fetterly, Mark Manasse, Marc Najork, and Janet L. Wiener. A large-scale study of the
evolution of web pages. In Proceedings of the Twelfth International World Wide Web Conference
(WWW), pp. 669–678, Budapest, Hungary, 2003."
REFERENCES,0.2278797996661102,"Derek Greene and Padraig Cunningham. Practical solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the Twenty-Third International Conference on
Machine Learning (ICML), pp. 377–384, Pittsburgh, PA, 2006."
REFERENCES,0.2287145242070117,"Kaiming He, Fang Wen, and Jian Sun.
K-means hashing: An afﬁnity-preserving quantization
method for learning binary compact codes. In Proceedings of the 2013 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 2938–2945, Portland, OR, 2013."
REFERENCES,0.2295492487479132,"Monika Rauch Henzinger. Finding near-duplicate web pages: a large-scale evaluation of algorithms.
In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR), pp. 284–291, Seattle, WA, 2006."
REFERENCES,0.2303839732888147,"Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse
of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing (STOC), pp. 604–613, Dallas, TX, 1998."
REFERENCES,0.23121869782971619,Under review as a conference paper at ICLR 2022
REFERENCES,0.23205342237061768,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
REFERENCES,0.2328881469115192,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.2337228714524207,"David C. Lee, Qifa Ke, and Michael Isard. Partition min-hash for partial duplicate image discovery.
In Proceedings of the 11th European Conference on Computer Vision (ECCV), Part I, pp. 648–
662, Heraklion, Crete, Greece, 2010."
REFERENCES,0.2345575959933222,"Ping Li and Kenneth Ward Church. Using sketches to estimate associations. In Proceedings of
the Conference on Human Language Technology and the Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP), pp. 708–715, Vancouver, Canada, 2005."
REFERENCES,0.2353923205342237,"Ping Li and Arnd Christian König. Theory and applications of b-bit minwise hashing. Commun.
ACM, 54(8):101–109, 2011."
REFERENCES,0.2362270450751252,"Ping Li, Anshumali Shrivastava, Joshua Moore, and Arnd Christian König. Hashing algorithms for
large-scale learning. In Advances in Neural Information Processing Systems (NIPS), pp. 2672–
2680, Granada, Spain, 2011."
REFERENCES,0.2370617696160267,"Ping Li, Art B Owen, and Cun-Hui Zhang.
One permutation hashing.
In Advances in Neural
Information Processing Systems (NIPS), pp. 3122–3130, Lake Tahoe, NV, 2012."
REFERENCES,0.2378964941569282,"Anshumali Shrivastava and Ping Li. Fast near neighbor search in high-dimensional binary data.
In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in
Databases (ECML-PKDD), pp. 474–489, Bristol, UK, 2012."
REFERENCES,0.2387312186978297,"Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. In Proceedings of the
Seventeenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), pp. 886–
894, Reykjavik, Iceland, 2014."
REFERENCES,0.2395659432387312,"Acar Tamersoy, Kevin A. Roundy, and Duen Horng Chau. Guilt by association: large scale malware
detection by mining ﬁle-relation graphs. In Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD), pp. 1524–1533, New York, NY,
2014."
REFERENCES,0.24040066777963273,"Felix X. Yu, Aditya Bhaskara, Sanjiv Kumar, Yunchao Gong, and Shih-Fu Chang. On binary em-
bedding using circulant matrices. J. Mach. Learn. Res., 18:150:1–150:30, 2017."
REFERENCES,0.24123539232053423,"Juan Zamora, Marcelo Mendoza, and Héctor Allende. Hashing-based clustering in high dimensional
data. Expert Syst. Appl., 62:202–211, 2016."
REFERENCES,0.24207011686143573,Under review as a conference paper at ICLR 2022
REFERENCES,0.24290484140233723,"A
PROOFS OF TECHNICAL RESULTS"
REFERENCES,0.24373956594323873,"Notations. In our analysis, we will use 1s to denote 1{hs(v) = hs(w)} in for ∀1 ≤s ≤K, where
h is the hash value. Given two data vectors v, w ∈{0, 1}D. Recall in (5): a = PD
i=1 1{vi =
1 and wi = 1}, f = PD
i=1 1{vi = 1 or wi = 1}. Thus, the Jaccard similarity J = a/f. We also
deﬁne ˜J = (a −1)/(f −1)."
REFERENCES,0.24457429048414023,"Deﬁnition A.1. Let v, w ∈{0, 1}D. Deﬁne the location vector as x ∈{O, ×, −}D, with xi being
“O”, “×”, “−” when vi = wi = 1, vi + wi = 1 and vi = wi = 0, respectively."
REFERENCES,0.24540901502504173,"Deﬁnition A.2. For A, B ∈{O, ×, −}, let {(i, j) : (A, B)|△} denote a pair of indices {(i, j) :
(xi, xj) = (A, B), j −i = △}. Deﬁne"
REFERENCES,0.24624373956594323,"L0(△) = {(i, j) : (O, O)|△},
L1(△) = {(i, j) : (O, ×)|△},
L2(△) = {(i, j) : (O, −)|△},
G0(△) = {(i, j) : (−, O)|△},
G1(△) = {(i, j) : (−, ×)|△} , G2(△) = {(i, j) : (−, −)|△},
H0(△) = {(i, j) : (×, O)|△},
H1(△) = {(i, j) : (×, ×)|△},
H2(△) = {(i, j) : (×, −)|△}."
REFERENCES,0.24707846410684475,"Remark A.1. For the ease of notation, by circulation we write xj = xj−D when D < j < 2D."
REFERENCES,0.24791318864774625,"One can easily verify that given ﬁxed a, f, D, it holds that for ∀1 ≤△≤K −1,"
REFERENCES,0.24874791318864775,"|L0(△)| + |L1(△)| + |L2(△)| = |L0(△)| + |G0(△)| + |H0(△)| = a,
|G0(△)| + |G1(△)| + |G2(△)| = |L2(△)| + |G2(△)| + |H2(△)| = D −f,
|H0(△)| + |H1(△)| + |H2(△)| = |L1(△)| + |G1(△)| + |H1(△)| = f −a.
(10)"
REFERENCES,0.24958263772954925,We will refer this as the intrinsic constraints on the size of above sets.
REFERENCES,0.25041736227045075,"A.1
PROOF OF LEMMA 2.1"
REFERENCES,0.25125208681135225,"Lemma 2.1. For any 1 ≤s < t ≤K with t −s = △, we have"
REFERENCES,0.25208681135225375,"Eπ

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

= |L0(△)| + (|G0(△)| + |L2(△)|)J"
REFERENCES,0.25292153589315525,"f + |G0(△)| + |G1(△)|
,"
REFERENCES,0.25375626043405675,"where the sets are deﬁned in Deﬁnition 2.2 and hs, ht are the hash values as in Algorithm 2."
REFERENCES,0.25459098497495825,"Proof. To check whether a hash sample generated by MinHash collides (under some permutation
π), it sufﬁces to look at the permuted location vector x. If a collision happens, after permuted by π,
type “O” point must appear before the ﬁrst “×” point. That said, the minimal permutation index of
“O” elements must be smaller than that of “×” elements. If the hash sample does not collide, then
the ﬁrst “×” must appear before the ﬁrst “O”. Note that “−” points does not affect the collision."
REFERENCES,0.25542570951585974,"To compute the variance of the estimator, it sufﬁces to compute E[1s1t]. Let L, G and H be the
union of L’s, G’s and H’s, respectively. In the following, we say that an index i belongs to a set if i
is the ﬁrst term of an element in that set. We have"
REFERENCES,0.25626043405676124,"|L| = a,
|H| = f −a,
|G| = D −f."
REFERENCES,0.2570951585976628,"One key observation is that, for a pair (i, j) in above sets, the hash index πs(i) will be the hash index
of πt(j). We begin by decomposing the expectation into"
REFERENCES,0.2579298831385643,"E[1s1t] = P[collision s, collision t] =
X"
REFERENCES,0.2587646076794658,"i∗s∈L
P[collision s at i∗
s, collision t] ="
X,0.2595993322203673,"2
X p=0 X"
X,0.2604340567612688,"i∗
s∈Lp
P[collision s at i∗
s, collision t].
(11)"
X,0.2612687813021703,"where i∗
s is the location of the original “O” in vector x that collides for s-th hash sample. It is
different from the exact location of collision in x(πs). Note that the permutation is totally random,
so the location of collision is independent of 1s, and uniformly distributed among all type “O” pairs."
X,0.2621035058430718,Under review as a conference paper at ICLR 2022
X,0.2629382303839733,"1) When i∗
s ∈L0. In this case, the minimum index of the type “O” pair in x(πs), πs(i∗
s), is shifted
to another type “O” pair in x(πt). Therefore, the indices of pairs with the ﬁrst element being “O”
or “×” originally in x(πs) will still be greater than πt(i∗
s). If sample s collides at i∗
s, hash sample t
will collide when"
X,0.2637729549248748,"1. All the points in G1, after permutation πs, is greater than πs(i∗
s). In this case, regardless of
the permuted G0, hash t will always collide."
X,0.2646076794657763,"2. There exist points in G1 after permutation πs smaller than πs(i∗
s), and also points in G0 that
is smaller than the minimum of permuted G1."
X,0.2654424040066778,"Consequently, we have for i∗
s ∈L0,"
X,0.2662771285475793,"P[collision s at i∗
s, collision t]
= P[πs(i∗
s) < πs(i), ∀i ∈H ∪L/i∗
s, and min
j∈G1 πs(j) > πs(i∗
s)]"
X,0.2671118530884808,"+ P[πs(i∗
s) < πs(i), ∀i ∈H ∪L/i∗
s, and min
j∈G0 πs(j) < min
j∈G1 πs(j) < πs(i∗
s)] = 1"
X,0.2679465776293823,"a ·
a
f + |G1| +
|G0|
f + |G0| + |G1| ·
|G1|
f + |G1| · a f · 1 a"
X,0.2687813021702838,"=
1
f + |G1| +
|G0| · |G1|
(f + |G0| + |G1|)(f + |G1|)f .
(12)"
X,0.2696160267111853,"This probability holds for ∀i∗
s ∈L0."
X,0.2704507512520868,"2) When i∗
s ∈L1. Similarly, we consider the condition where i∗
s ∈L1, and both hash samples
collide. In this case, πs(i∗
s) would be shifted to a “×” pair in x(πt). That is, the indices of pairs with
the ﬁrst element being “O” or “×” originally in x(πs) will all become greater than πs(i∗
s), which
now is the location of a “×” pair in x(πt). Thus, to make hash t collide, we will need:"
X,0.27128547579298834,"• At least one point from G0 is smaller than any other points in H ∪L ∪G1 after permutation
πs."
X,0.27212020033388984,"Therefore, for any i∗
s ∈L1,"
X,0.27295492487479134,"P[collision s at i∗
s, collision t]
= P[πs(i∗
s) < πs(i), ∀i ∈H ∪L/i∗
s, and min
j∈G0 πs(j) < min{πs(i∗
s), min
j∈G1 πs(j)}]"
X,0.27378964941569284,"=
|G0|
f + |G0| + |G1| · a f · 1 a"
X,0.27462437395659434,"=
|G0|
(f + |G0| + |G1|)f ,
(13)"
X,0.27545909849749584,"which is true for ∀i∗
s ∈L1."
X,0.27629382303839733,"3) When i∗
s ∈L2."
X,0.27712854757929883,"In this scenario, πs(i∗
s) would be shifted to a “−” pair in x(πt). Therefore, if hash s collides, hash t
will also collide when:"
X,0.27796327212020033,"• After applying πs, the minimum of L0 ∪H0 ∪G0 is smaller than the minimum of L1 ∪
H1 ∪G1."
X,0.27879799666110183,"Thus, we obtain that for any i∗
s ∈L2,"
X,0.27963272120200333,"P[collision s at i∗
s, collision t]
= P[πs(i∗
s) < πs(i), ∀i ∈H ∪L/i∗
s, and
min
j∈L0∪G0∪H0 πs(j) <
min
j∈L1∪G1∪H1 πs(j)]"
X,0.28046744574290483,≜P[Ω].
X,0.28130217028380633,Under review as a conference paper at ICLR 2022
X,0.28213689482470783,"Let 1s,i∗s denote the event {πs(i∗
s) < πs(i), ∀i ∈H ∪L/i∗
s}. Then Ωcan be separated into the
following several cases:"
X,0.28297161936560933,"1. Ω1:
1s,i∗s, minj∈L0∪H0 πs(j)
<
minj∈L1∪H1 πs(j), and minj∈L0∪H0 πs(j)
<
minj∈G1 πs(j)."
X,0.2838063439065108,"2. Ω2:
1s,i∗s, minj∈L0∪H0 πs(j)
<
minj∈L1∪H1 πs(j), and minj∈L0∪H0 πs(j)
>
minj∈G1 πs(j) > minj∈G0 πs(j) > πs(i∗
s)."
X,0.2846410684474123,"3. Ω3:
1s,i∗s, minj∈L0∪H0 πs(j)
<
minj∈L1∪H1 πs(j), and minj∈L0∪H0 πs(j)
>
minj∈G1 πs(j) > πs(i∗
s) > minj∈G0 πs(j)."
X,0.2854757929883139,"4. Ω4: 1s,i∗s, minj∈L0∪H0 πs(j) < minj∈L1∪H1 πs(j), and minj∈L0∪H0 πs(j) > πs(i∗
s) >
minj∈G1 πs(j) > minj∈G0 πs(j)."
X,0.2863105175292154,"5. Ω5: 1s,i∗s, minj∈L0∪H0 πs(j) > minj∈L1∪H1 πs(j), and πs(i∗
s) < minj∈G0 πs(j) <
minj∈L1∪H1∪G1 πs(j)."
X,0.2871452420701169,"6. Ω6: 1s,i∗s, minj∈L0∪H0 πs(j) > minj∈L1∪H1 πs(j), and minj∈G0 πs(j) < πs(i∗
s) <
minj∈L1∪H1∪G1 πs(j)."
X,0.2879799666110184,We can compute the probability of each event as
X,0.2888146911519199,P[Ω1] = 1
X,0.2896494156928214,"a ·
a
f + |G1| ·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1| + |G1|,"
X,0.2904841402337229,"=
a −|G0|
(f −|G0|)(f + |G1|),"
X,0.2913188647746244,P[Ω2] = 1
X,0.2921535893155259,"a ·
a
f + |G0| + |G1| ·
|G0|
|G0| + |G1| + |L0| + |H0| + |L1| + |H1|"
X,0.2929883138564274,"·
|G1|
|L0| + |H0| + |L1| + |H1| + |G1| ·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1|"
X,0.2938230383973289,"=
1
f + |G0| + |G1| · |G0|"
X,0.29465776293823037,"f
·
|G1|
f −|G0| ·
a −|G0|
f −|G0| −|G1|"
X,0.29549248747913187,"=
|G0| · |G1| · (a −|G0|)
(f + |G0| + |G1|)(f −|G0|)(f −|G0| −|G1|)f ,"
X,0.29632721202003337,"P[Ω3] =
|G0|
f + |G0| + |G1| ·
1
f + |G1| ·
|G1|
|L0| + |H0| + |L1| + |H1| + |G1| ·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1|"
X,0.29716193656093487,"=
|G0| · |G1| · (a −|G0|)
(f + |G0| + |G1|)(f + |G1|)(f −|G0|)(f −|G0| −|G1|),"
X,0.29799666110183637,"P[Ω4] =
|G0|
f + |G0| + |G1| ·
|G1|
f + |G1| · 1"
X,0.2988313856427379,"f ·
|L0| + |H0|
|L0| + |H0| + |L1| + |H1|"
X,0.2996661101836394,"=
|G0| · |G1| · (a −|G0|)
(f + |G0| + |G1|)(f + |G1|)(f −|G0| −|G1|)f ,"
X,0.3005008347245409,"P[Ω5] =
1
f + |G0| + |G1| ·
|G0|
|G0| + |G1| + |L0| + |H0| + |L1| + |H1| ·
|L1| + |H1|
|L0| + |H0| + |L1| + |H1|"
X,0.3013355592654424,"=
|G0| · (f −a −|G1|)
(f + |G0| + |G1|)(f −|G0| −|G1|)f ,"
X,0.3021702838063439,"P[Ω6] =
|G0|
f + |G0| + |G1| · 1"
X,0.3030050083472454,"f ·
|L1| + |H1|
|L0| + |H0| + |L1| + |H1|"
X,0.3038397328881469,"=
|G0| · (f −a −|G1|)
(f + |G0| + |G1|)(f −|G0| −|G1|)f ."
X,0.3046744574290484,Under review as a conference paper at ICLR 2022
X,0.3055091819699499,Note that
X,0.3063439065108514,P[Ω2] + P[Ω3] + P[Ω4]
X,0.3071786310517529,"=
|G0| · |G1| · (a −|G0|)
(f + |G0| + |G1|)(f −|G0| −|G1|)"
X,0.3080133555926544,"
1
(f −|G0|)f +
1
(f −|G0|)(f + |G1|) +
1
f(f + |G1|) "
X,0.3088480801335559,"=
|G0| · |G1| · (a −|G0|)(3f −|G0| + |G1|)
(f + |G0| + |G1|)(f −|G0| −|G1|)f(f −|G0|)(f + |G1|)."
X,0.3096828046744574,"Summing up all the terms together, we obtain P[Ω] as"
X,0.3105175292153589,"6
X"
X,0.3113522537562604,"n=1
P[Ωn] = f(f + |G0| + |G1|)(f −|G0| −|G1|)(a −|G0|) + |G0||G1|(a −|G0|)(3f −|G0| + |G1|)"
X,0.3121869782971619,(f + |G0| + |G1|)(f −|G0| −|G1|)(f −|G0|)(f + |G1|)f
X,0.31302170283806346,"+
2|G0|(f −a −|G1|)(f −|G0|)(f + |G1|)
(f + |G0| + |G1|)(f −|G0| −|G1|)(f −|G0|)(f + |G1|)f"
X,0.31385642737896496,= (a −|G0|)(f + |G0| −|G1|)(f −|G0|)(f + |G1|) + 2|G0|(f −a −|G1|)(f −|G0|)(f + |G1|)
X,0.31469115191986646,(f + |G0| + |G1|)(f −|G0| −|G1|)(f −|G0|)(f + |G1|)f
X,0.31552587646076796,"=
(a + |G0|)(f −|G0| −|G1|)(f −|G0|)(f + |G1|)
(f + |G0| + |G1|)(f −|G0| −|G1|)(f −|G0|)(f + |G1|)f"
X,0.31636060100166946,"=
a + |G0|
(f + |G0| + |G1|)f ,
(14)"
X,0.31719532554257096,"which holds for ∀i∗
s ∈L2. Now combining (12), (13), (14) with (11), we obtain"
X,0.31803005008347246,"E[1s1t] =
|L0|
f + |G1| +
|G0||G1||L0|
(f + |G0| + |G1|)(f + |G1|)f +
|G0||L1|
(f + |G0| + |G1|)f +
(a + |G0|)|L2|
(f + |G0| + |G1|)f . (15)"
X,0.31886477462437396,"Here, recall that the sets are associated with all 1 ≤s < t ≤K such that △= t −s. Using the
intrinsic constraints (10), after some calculation we can simplify (15) as"
X,0.31969949916527546,"Eπ[1s1t] =
|L0|
f + |G0| + |G1| +
a(|G0| + |L2|)
(f + |G0| + |G1|)f ,"
X,0.32053422370617696,which completes the proof.
X,0.32136894824707846,"A.2
PROOF OF THEOREM 2.2"
X,0.32220367278797996,"Theorem 2.2. Under the same setting as in Lemma 2.1, the variance of ˆJ0,π is"
X,0.32303839732888145,"V ar[ ˆJ0,π] = J"
X,0.32387312186978295,"K + 2 PK
s=2(s −1)ΘK−s+1"
X,0.32470784641068445,"K2
−J2,"
X,0.32554257095158595,"where Θ△≜Eπ

1{hs(v) = hs(w)}1{ht(v) = ht(w)}

as in Lemma 2.1 with any t −s = △."
X,0.32637729549248745,"Proof. By the expansion of variance formula, since E[12
s] = E[1s] = J, we have"
X,0.327212020033389,"V ar[ ˆJ0,π] = J K +"
X,0.3280467445742905,"PK
s=1
PK
t̸=s E[1s1t]"
X,0.328881469115192,"K2
−J2.
(16)"
X,0.3297161936560935,"Note here that for ∀t > s, the t-th hash sample uses πt as the permutation, which is shifted right-
wards by △= t −s from πs. Thus, we have E[1s1t] = E[1s−i1t−i] for ∀0 < i < s ∧t, which
implies E[1s1t] = E[111t−s+1], ∀s < t. Since by assumption K ≤D, we have K
X s K
X"
X,0.330550918196995,"t̸=s
E[1s1t] = 2E

(1112 + 1113 + ... + 111K) + (1213 + ... + 121K) + ... + 1K−11K
"
X,0.3313856427378965,"= 2E

(1112 + 1113 + ... + 111K) + (1112 + ... + 111K−1) + ... + 1112
"
X,0.332220367278798,"Under review as a conference paper at ICLR 2022 = 2 K
X"
X,0.3330550918196995,"s=2
(s −1)E[111K−s+2] ≜2 K
X"
X,0.333889816360601,"s=2
(s −1)ΘK−s+1.
(17)"
X,0.3347245409015025,"Finally, integrating (16), (17) and Lemma 2.1 completes the proof."
X,0.335559265442404,"A.3
PROOF OF THEOREM 3.2"
X,0.3363939899833055,"Theorem 3.2. Let a, f be deﬁned as in (5). When 0 < a < f ≤D (J /∈{0, 1}), we have"
X,0.337228714524207,"V ar[ ˆJσ,π] = J"
X,0.3380634390651085,K + (K −1) ˜E
X,0.33889816360601,"K
−J2,
(18)"
X,0.3397328881469115,"where with l = max(0, D −2f + a), and"
X,0.34056761268781305,"˜E =
X"
X,0.34140233722871455,"{l0,l2,g0,g1}"
X,0.34223706176961605,"( 
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f  ×"
X,0.34307178631051755,"D−f−1
X s=l"
X,0.34390651085141904," D−f
s
"
X,0.34474123539232054," D−a−1
D−f−1
"
X,0.34557595993322204," 
f−a−1
D−f−s−1
  s
n1
 D−f−s
n2
 D−f−s
n3
 f−a−(D−f−s)
n4
 
a−1
a−l1−l2
"
X,0.34641068447412354," D−1
a
 ) . (19)"
X,0.34724540901502504,"The feasible set {l0, l2, g0, g1} satisﬁes the intrinsic constraints (6), and"
X,0.34808013355592654,"n1 = g0 −(D −f −s −g1),
n2 = D −f −s −g1,
n3 = l2 −g0 + (D −f −s −g1), n4 = l1 −(D −f −s −g1)."
X,0.34891485809682804,"Also, when a = 0 or f = a (J = 0 or J = 1), we have V ar[ ˆJσ,π] = 0."
X,0.34974958263772954,"Proof. Similar to the proof of Theorem 2.2, we denote Θ△= Eσ,π[1s1t] with |t −s| = △. Note
that now the expectation is taken w.r.t. both two independent permutations σ and π. Since σ is
random, we know that Θ1 = Θ2 = · · · = ΘK−1. Then by the variance formula, we have"
X,0.35058430717863104,"V ar[ ˆJσ,π] = J2"
X,0.35141903171953254,K −(K −1)Θ1
X,0.35225375626043404,"K
−J2
(20)"
X,0.35308848080133554,"Hence, it sufﬁces to consider Θ1. In this proof, we will set △= 1 and drop the notation △for
conciseness, and denote ˜E = Θ1 from now on. First, we note that Lemma 2.1 gives the desired
quantity conditional on σ. By the law of total probability, we have"
X,0.35392320534223703,˜E = Eσ
X,0.3547579298831386,"
|L0|
f + |G0| + |G1| +
a(|G0| + |L2|)
(f + |G0| + |G1|)f"
X,0.3555926544240401,"
,
(21)"
X,0.3564273789649416,"where the sizes of sets are random depending on the initial permutation σ (i.e.
counted after
permuting by σ). As a result, the problem turns into deriving the distribution of |L0|, |L1|, |L2|, |G0|
and |G1| under random permutation σ, and then taking expectation of (21) with respect to this
additional randomness."
X,0.3572621035058431,"When a = 0, we know that |L0| = |L2| = |G0| = 0, hence the expectation ˜E is trivially 0. Thus, the
V ar[ ˆJσ,π] = 0. When f = a, |G1| = 0, and the constraint on the sets becomes"
X,0.3580968280467446,"|L0| + |G0| = |L0| + |L2| = f,
|L2| + |G2| = |G0| + |G2| = D −f."
X,0.3589315525876461,Then (21) becomes
X,0.3597662771285476,˜E = Eσ
X,0.3606010016694491,"
|L0|
f + |G0| + |G0| + |L2|"
X,0.3614357262103506,f + |G0| 
X,0.3622704507512521,Under review as a conference paper at ICLR 2022 = Eσ
X,0.3631051752921536,|L0| + |G0| + |L2|
X,0.3639398998330551,"f + |G0| 
≡1."
X,0.3647746243739566,"Therefore, when f = a, we also have V ar[ ˆJσ,π] = 0."
X,0.3656093489148581,"Next, we will consider the general case where 0 < a < f ≤D. This can be considered as a
combinatorial problem where we randomly arrange a type “O”, (f −a) type “×” and (D −f)
type “−” points in a circle. We are interested in the distribution of the number of {O, O}, {O, ×},
{O, −}, {−, O} and {−, ×} pairs of consecutive points in clockwise direction. We consider this
procedure in two steps, where we ﬁrst place “×” and “−” points, and then place “O” points."
X,0.3664440734557596,Step 1. Randomly place “×” and “−” points on the circle.
X,0.3672787979966611,"In this step, four types of pairs may appear: {−, −}, {−, ×}, {×, ×} and {×, −}. Denote C1, C2,
C3 and C4 as the collections of above pairs. Since"
X,0.36811352253756263,"|C1| + |C4| = |C1| + |C2| = D −f,
|C2| + |C3| = |C2| + |C4| = f −a,"
X,0.36894824707846413,"knowing the size of one set gives information on the size of all the sets. Thus, we can characterize the
joint distribution by analyzing the distribution of |C1|. First, placing (D −f) “−” points on a circle
leads to (D−f) number of {−, −} pairs. This (D−f) elements can be regarded as the borders that
split the circle into (D −f) bins. Now, we randomly throw (f −a) number of “×” points into these
bins. If at least one “×” falls into one bin, then the number of {−, −} pairs (|C1|) would reduce by
1, while |C2| and |C4| would increase by 1. If z “×” points fall into one bin, then the number of
{×, ×} (|C3|) would increase by (z −1). Notice that since s ≤D −f and D −f −s ≤f −a, we
have max(0, D −2f + a) ≤s ≤D −f. Consequently, for s in this range, we have"
X,0.36978297161936563,"P
n
|C1| = s
o
= P
n
|C1| = s, |C3| = f −a −(D −f −s)
o ="
X,0.37061769616026713,"  D−f
D−f−s
 
f−a−1
D−f−s−1
"
X,0.37145242070116863," D−a−1
D−f−1
 ="
X,0.3722871452420701," D−f
s
 
f−a−1
D−f−s−1
"
X,0.3731218697829716," D−a−1
D−f−1

.
(22)"
X,0.3739565943238731,"The second line is due to the stars and bars problem that the number of ways to place n unlabeled
balls in m distinct bins such that each bin has at least one ball is
  n−1
m−1

. For |C1| = s, we need
n = f −a (number of “×”) and m = |C2| = D −f −s. Moreover, the number of ways to
place n balls in m distinct bins is
 n+m−1
m−1

. When counting the total number of possibilities, we
have n = f −a and m = D −f. This gives the denominator. We notice that (22) is actually a
hyper-geometric distribution."
X,0.3747913188647746,Step 2. Randomly place “O” points on the circle.
X,0.3756260434056761,We have the probability mass function
X,0.3764607679465776,"P[Ψ] ≜P
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
o ="
X,0.3772954924874791,"D−f−1
X"
X,0.3781302170283806,"s=D−2f+a
P
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
|C1| = s
o
P
n
|C1| = s
o
.
(23)"
X,0.3789649415692821,"Now it remains to compute the distribution conditional on |C1|. Here we drop |L0| because it is
intrinsically determined by |L1| and |L2|. Again, given a placement of all “×” and “−” points, each
consecutive pair can be regarded as a distinct bin. Therefore, now the problem is to randomly throw
a type “O” points into that (D −a) bins, given that we have placed type “×” and “−” points on the
circle with |C1| = s (and thus |C2| = |C3| = D −f −s and |C4| = f −a −(D −f −s) are also
determined correspondingly). In the following, we count the number of “O” points that fall in Ci,
i = 1, 2, 3, 4, to make the event Ψ happen. Note that"
X,0.3797996661101836,Under review as a conference paper at ICLR 2022
X,0.3806343906510851,"• When at least one “O” point falls into C1 (between {−, −}), |L2| and |G0| increase by 1."
X,0.3814691151919866,"• When at least one “O” point falls into C2 (between {−, ×}), |L1| and |G0| increase by 1,
while |G1| decreases by 1."
X,0.3823038397328882,"• When at least one “O” point falls into C3 (between {×, −}), |L2| increases by 1."
X,0.3831385642737897,"• When at least one “O” point falls into C4 (between {×, ×}), |L1| increases by 1."
X,0.38397328881469117,"We denote the number of bins in Ci, i = 1, 2, 3, 4 that contain at least one “O” point as n1, n2, n3, n4,
respectively. As a result of above reasoning, in the event Ψ, we have



 

"
X,0.38480801335559267,"n1 + n3 = l2,
n2 + n4 = l1,
n1 + n2 = g0,
D −f −s −n2 = g1."
X,0.38564273789649417,"Solving the equations gives



 

"
X,0.38647746243739567,"n1 = g0 −(D −f −s −g1),
n2 = D −f −s −g1,
n3 = l2 −g0 + (D −f −s −g1),
n4 = l1 −(D −f −s −g1)."
X,0.38731218697829717,"Note that P4
i=1 ni = l1 + l2. Therefore, event Ψ is equivalent to randomly pick n1, n2, n3 and
n4 bins in C1,...,C4, and then distribute a type “O” points in these (l1 + l2) bins such that each bin
contains at least one “O”. Hence, we obtain"
X,0.38814691151919867,"P
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
|C1| = s
o
="
X,0.38898163606010017,"  s
n1
 D−f−s
n2
 D−f−s
n3
 f−a−(D−f−s)
n4
 
a−1
l1+l2−1
"
X,0.38981636060100167,"  D−1
D−a−1
 ="
X,0.39065108514190316,"  s
n1
 D−f−s
n2
 D−f−s
n3
 f−a−(D−f−s)
n4
 
a−1
a−l1−l2
"
X,0.39148580968280466," D−1
a

, (24)"
X,0.39232053422370616,"which is also multi-variate hyper-geometric distributed. Now combining (22), (23) and (24), we
obtain the joint distribution of |L0|, |L1|, |L2|, |G0| and |G1| as"
X,0.39315525876460766,"P
n
|L1| = l1, |L2| = l2, |G0| = g0, |G1| = g1
o ="
X,0.39398998330550916,"D−f−1
X"
X,0.39482470784641066,"s=max(0,D−2f+a)"
X,0.39565943238731216,"  s
n1
 D−f−s
n2
 D−f−s
n3
 f−a−(D−f−s)
n4
 
a−1
a−l1−l2
"
X,0.3964941569282137," D−1
a

·"
X,0.3973288814691152," D−f
s
 
f−a−1
D−f−s−1
"
X,0.3981636060100167," D−a−1
D−f−1

. (25)"
X,0.3989983305509182,"Now let Ξ be the feasible set of (l0, l1, g0, g1, g2) that satisﬁes the intrinsic constraints (10). The
desired expectation w.r.t. both π and σ can thus be written as"
X,0.3998330550918197,"˜E =
X Ξ"
X,0.4006677796327212,"
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f 
·  "
X,0.4015025041736227,"D−f−1
X"
X,0.4023372287145242,"s=max(0,D−2f+a)"
X,0.4031719532554257,"  s
n1
 D−f−s
n2
 D−f−s
n3
 f−a−(D−f−s)
n4
 
a−1
a−l1−l2
"
X,0.4040066777963272," D−1
a

·"
X,0.4048414023372287," D−f
s
 
f−a−1
D−f−s−1
"
X,0.4056761268781302," D−a−1
D−f−1
  ."
X,0.4065108514190317,The desired result can then follows by (20).
X,0.4073455759599332,"A.4
PROOF OF PROPOSITION 3.3"
X,0.4081803005008347,"Proposition 3.3 (Symmetry). V ar[ ˆJσ,π] is the same for the (D, f, a)-data pair and the (D, f, f −
a)-data pair, ∀0 ≤a ≤f ≤D."
X,0.4090150250417362,Under review as a conference paper at ICLR 2022
X,0.40984974958263776,"Proof. For ﬁxed a, f, D, let ˜E1 be the expectation deﬁned in Theorem 3.2 for (v1, w1), and ˜E2 be
that for (v2, w2). From Theorem 3.2 we know that"
X,0.41068447412353926,"˜E1 = E(l0,l2,g0,g1)
h
l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f i
,"
X,0.41151919866444076,"where (l0, l2, g0, g1) follows the distribution of (|L0|, |L2|, |G0|, |G1|) associated with the location
vector x1 of (v1, v2). For data pair (v2, w2), we can consider its location vector x2 as swapping
the “O” and “×” entries of x1. Now we denote the size of the corresponding sets (Deﬁnition 2.2)
of x2 as l′
is, g′
is, h′
is, for i = 0, 1, 2. Since σ is applied before hashing, by symmetry there is a
one-to-one correspondence between the two location vectors. More speciﬁcally, l′
0 corresponds to
h1, g′
0 corresponds to g1, g′
1 corresponds to g0, and l′
2 corresponds to h2. Therefore, in probability
we can write"
X,0.41235392320534225,"˜E2 = E(l′
0,l′
2,g′
0,g′
1)
h
l′
0
f + g′
0 + g′
1
+
a(g′
0 + l′
2)
(f + g′
0 + g′
1)f i"
X,0.41318864774624375,"= E(h1,h2,g0,g1)
h
h1
f + g0 + g1
+ (f −a)(g1 + h2)"
X,0.41402337228714525,"(f + g0 + g1)f i
."
X,0.41485809682804675,"Consequently, we have"
X,0.41569282136894825,"˜E1 −˜E2 = E(l0,l2,h1,h2,g0,g1)
h
l0 −h1
f + g0 + g1
+ a(g0 + l2) −(f −a)(g1 + h2)"
X,0.41652754590984975,"(f + g0 + g1)f i
."
X,0.41736227045075125,"In the sequel, the subscript of expectation is suppressed for conciseness. Exploiting the constraints
(10), we deduce that h1 = (f −a) −l1 −g1, h2 = l0 + g0 + l1 + g1 −a and l0 + l1 = a −l2.
Using these facts we obtain"
X,0.41819699499165275,"˜E1 −˜E2 = E
h(l0 −(f −a) + l1 + g1)f + a(g0 + l2) −(f −a)(l0 + g0 + l1 + 2g1 −a)"
X,0.41903171953255425,(f + g0 + g1)f i
X,0.41986644407345575,"= E
h(2a −f + g1 −l2)f + a(g0 + l2) −(f −a)(2g1 + g0 −l2)"
X,0.42070116861435725,(f + g0 + g1)f i
X,0.42153589315525875,"= E
h2(f + g0 + g1)a −(f + g0 + g1)f"
X,0.42237061769616024,(f + g0 + g1)f i
X,0.42320534223706174,= 2J −1.
X,0.4240400667779633,"Comparing the variances of ˆJσ,π(v1, w1) and ˆJσ,π(v2, w2), we derive"
X,0.4248747913188648,"V ar[ ˆJσ,π(v1, w1)] −V ar[ ˆJσ,π(v2, w2)] = ( J"
X,0.4257095158597663,K + (K −1) ˜E1
X,0.4265442404006678,"K
−J2) −(1 −J"
X,0.4273789649415693,"K
+ (K −1) ˜E2"
X,0.4282136894824708,"K
−(1 −J)2)"
X,0.4290484140233723,= −K −1
X,0.4298831385642738,"K
(2J −1) + K −1"
X,0.4307178631051753,"K
( ˜E1 −˜E2) = 0."
X,0.4315525876460768,This completes the proof.
X,0.4323873121869783,"A.5
PROOF OF LEMMA 3.4"
X,0.4332220367278798,"Lemma 3.4 (Increasing Increment). Assume a > 0 and f > a are arbitrary and ﬁxed. Denote ˜ED
as in (19) in Theorem 3.2, with D treated as a parameter. Then we have ˜ED+1 > ˜ED for ∀D ≥f."
X,0.4340567612687813,"Proof. Let the probability mass function (25) with a, f and dimension D be Pa,f,D(l0, l2, g0, g1).
Conditional on l0, l2, g0, g1 with D elements, the possible values l′
0, l′
2, g′
0, g′
1 when adding a “−”
are"
X,0.4348914858096828,"• g′
0 = g0 + 1, l′
0 = l0, l′
2 = l2, g′
1 = g1. This is true when the new elements falls between a
pair of (×, O), with probability l1+l2−g0 D
."
X,0.4357262103505843,Under review as a conference paper at ICLR 2022
X,0.4365609348914858,"• g′
1 = g1 + 1, l′
0 = l0, l′
2 = l2, g′
0 = g0, when the new elements falls between a pair of
(×, ×), with probability f−a−l1−g1 D
."
X,0.4373956594323873,"• g′
1 = g1 + 1, l′
2 = l2 + 1, l′
0 = l0, g′
0 = g0, when the new elements falls between a pair of
(O, ×), with probability l1 D."
X,0.43823038397328884,"• l′
0 = l0 −1, l′
2 = l2 + 1, g′
0 = g0 + 1, g′
1 = g1, when the new elements falls between a pair
of (O, O), with probability l0 D."
X,0.43906510851419034,"• All values unchanged, when the “−” falls between other types of pairs, with probability
D−f+g0+g1 D
."
X,0.43989983305509184,"Denote ΞD as the feasible set satisfying (10) with dimension D ≥f. Above reasoning builds a
correspondence between ΞD and ΞD+1. More precisely, we have"
X,0.44073455759599334,"˜ED+1 =
X ΞD+1"
X,0.44156928213689484,"
l′
0
f + g′
0 + g′
1
+
a(g′
0 + l′
2)
(f + g′
0 + g′
1)f"
X,0.44240400667779634,"
Pa,f,D+1(l′
0, l′
2, g′
0, g′
1) =
X ΞD"
X,0.44323873121869783,"
l0
f + g0 + g1 + 1 +
a(g0 + l2 + 1)
(f + g0 + g1 + 1)f"
X,0.44407345575959933,l1 + l2 −g0
X,0.44490818030050083,"D
Pa,f,D(l0, l2, g0, g1)"
X,0.44574290484140233,"+

l0
f + g0 + g1 + 1 +
a(g0 + l2)
(f + g0 + g1 + 1)f"
X,0.44657762938230383,f −a −l1 −g1
X,0.44741235392320533,"D
Pa,f,D(l0, l2, g0, g1)"
X,0.44824707846410683,"+

l0
f + g0 + g1 + 1 +
a(g0 + l2 + 1)
(f + g0 + g1 + 1)f  l1"
X,0.44908180300500833,"DPa,f,D(l0, l2, g0, g1)"
X,0.44991652754590983,"+

l0 −1
f + g0 + g1 + 1 +
a(g0 + l2 + 2)
(f + g0 + g1 + 1)f  l0"
X,0.4507512520868113,"DPa,f,D(l0, l2, g0, g1)"
X,0.4515859766277129,"+

l0
f + g0 + g1
+
a(g0 + l2)
(f + g0 + g1)f"
X,0.4524207011686144,D −f + g0 + g1
X,0.4532554257095159,"D
Pa,f,D(l0, l2, g0, g1)

."
X,0.4540901502504174,The increment can be computed as
X,0.4549248747913189,"˜δD ≜˜ED+1 −˜ED =
X ΞD"
X,0.4557595993322204,f −g0 −g1 D
X,0.4565943238731219,"h 
l0
f + g0 + g1 + 1 −
l0
f + g0 + g1"
X,0.4574290484140234,"
+
  a(g0 + l2 + 1)"
X,0.4582637729549249,f + g0 + g1 + 1 −a(g0 + l2)
X,0.4590984974958264,f + g0 + g1 i
X,0.4599332220367279,"−
l0
D(f + g0 + g1 + 1) −a(f −a −l1 −g1) −al0"
X,0.4607679465776294,Df(f + g0 + g1 + 1)
X,0.46160267111853087,"
Pa,f,D(l0, l2, g0, g1) =
X ΞD"
X,0.46243739565943237,(f −g0 −g1)[a(f + g1 −l2) −fl0]
X,0.46327212020033387,Df(f + g0 + g1)(f + g0 + g1 + 1) −(f −a)l0 + a(f −a −l1 −g1)
X,0.46410684474123537,Df(f + g0 + g1 + 1)
X,0.46494156928213687,"
Pa,f,D(l0, l2, g0, g1) =
X ΞD"
X,0.4657762938230384,2af(l1 + g1) −2f(f −a)l0 −2a(f −a)(g0 + g1)
X,0.4666110183639399,"Df(f + g0 + g1)(f + g0 + g1 + 1)
Pa,f,D(l0, l2, g0, g1)"
X,0.4674457429048414,"= E
h2af(l1 + g1) −2f(f −a)l0 −2a(f −a)(g0 + g1)"
X,0.4682804674457429,Df(f + g0 + g1)(f + g0 + g1 + 1) i
X,0.4691151919866444,"= E
h2af(f −a −h1) −2f(f −a)l0 −2a(f −a)(g0 + g1 + f −f)"
X,0.4699499165275459,Df(f + g0 + g1)(f + g0 + g1 + 1) i
X,0.4707846410684474,"= E
h
4a(f −a)
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4716193656093489,"i
−E
h
2ah1 + 2(f −a)l0
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4724540901502504,"i
−E
h
2a(f −a)
Df(f + g0 + g1 + 1) i"
X,0.4732888146911519,"≜4a(f −a)E0 −2aE1 −2(f −a)E2 −2a(f −a)E3,
(26) where"
X,0.4741235392320534,"E0 = E
h
1
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4749582637729549,"i
, E1 = E
h
h1
D(f + g0 + g1)(f + g0 + g1 + 1) i
,"
X,0.4757929883138564,Under review as a conference paper at ICLR 2022
X,0.4766277128547579,"E2 = E
h
l0
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.4774624373956594,"i
, E3 = E
h
g2
Df(f + g0 + g1 + 1) i
."
X,0.4782971619365609,"Note that here the expectations are taken w.r.t. the set size distribution with a, f, D. We can expand
the terms of density function (25) to derive"
X,0.4791318864774624,"Pa,f,D(l0, l2, g0, g1) ="
X,0.47996661101836396,"D−f−1
X"
X,0.48080133555926546,"s=max(0,D−2f+a)"
X,0.48163606010016696,"(D −f −s)(D −f)!(f −a −1)!
[D −(f + g0 + g1)]![(f + g0 + g1) −D + s]!g1!(D −f −s −g1)!"
X,0.48247078464106846,"(a −1)!
(g0 + g1 −l2)![D −s + l2 −(f + g0 + g1)]!(f −a −l1 −g1)!(f + g1 + l1 −D + s)!l0!(a −l0 −1)!
a!(f −a)!(D −f −1)!"
X,0.48330550918196996,"(D −1)!
."
X,0.48414023372287146,"Denote a′ = a −1, f ′ = f −1, D′ = D −1 and l′
0 = l0 −1. We have"
X,0.48497495826377296,"E2 =
X ΞD"
X,0.48580968280467446,"l0
D(f + g0 + g1)(f + g0 + g1 + 1)Pa,f,D(l0, l2, g0, g1) =
X ΞD"
X,0.48664440734557596,a(a −1)
X,0.48747913188647746,"D −1
·
1
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.48831385642737896,"D′−f ′−1
X"
X,0.48914858096828046,"s=max(0,D′−2f ′+a′)"
X,0.48998330550918195,"(D′ −f ′ −s)(D′ −f ′)!(f ′ −a′ −1)!
[D′ −(f ′ + g0 + g1)]![(f ′ + g0 + g1) −D′ + s]!g1!(D′ −f ′ −s −g1)!"
X,0.49081803005008345,"(a′ −1)!
(g0 + g1 −l2)![D′ −s + l2 −(f ′ + g0 + g1)]!(f ′ −a′ −l1 −g1)!(f ′ + g1 + l1 −D′ + s)!l′
0!(a′ −l′
0 −1)!
a′!(f ′ −a′)!(D′ −f ′ −1)!"
X,0.49165275459098495,"(D′ −1)! =
X ΞD−1"
X,0.49248747913188645,a(a −1)
X,0.493322203672788,"D −1
1
D(f + g0 + g1)(f + g0 + g1 + 1)Pa−1,f−1,D−1(l0, l2, g0, g1)"
X,0.4941569282136895,= a(a −1)
X,0.494991652754591,"D −1 Ea−1,f−1,D−1
h
1
D(f + g0 + g1)(f + g0 + g1 + 1) i"
X,0.4958263772954925,≜a(a −1)
X,0.496661101836394,"D −1
¯E."
X,0.4974958263772955,"Here the subscript means that we are taking expectation w.r.t the set sizes when the number of “O”,
“×” and “−” points is (a −1, f −1, D −1). By symmetry, it can be shown similarly that"
X,0.498330550918197,E1 = (f −a)(f −a −1)
X,0.4991652754590985,"D −1
Ea,f−1,D−1
h
1
D(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.5,"i
= (f −a)(f −a −1)"
X,0.5008347245409015,"D −1
¯E."
X,0.501669449081803,"Substituting above results into (26), we obtain"
X,0.5025041736227045,˜δD = 2a(f −a)[2E0 −f −2
X,0.503338898163606,"D −1
¯E −E3]."
X,0.5041736227045075,"To compute E0, note that with a, f and D, variable g2 is distributed as hyper(D−1, D−f, D−f−1).
For ¯E, the distribution becomes hyper(D −2, D −f, D −f −1). Since f + g0 + g1 = D −g2, we
deduce E0 ="
X,0.505008347245409,"D−f−1
X"
X,0.5058430717863105,"s=max(0,D−2f)"
X,0.506677796327212,"1
D(D −s)(D −s + 1)"
X,0.5075125208681135," D−f−1
s
 
f
D−f−s
"
X,0.508347245409015," D−1
D−f
 ="
X,0.5091819699499165,"D−f−1
X"
X,0.510016694490818,"s=max(0,D−2f)"
X,0.5108514190317195,"1
D(D −s)(D −s + 1)
(D −f −1)!f!
s!(D −f −s −1)!(D −f −s)!(−D + 2f + s)!
(D −f)!(f −1)!"
X,0.511686143572621,"(D −1)!
,"
X,0.5125208681135225,Under review as a conference paper at ICLR 2022 and ¯E =
X,0.513355592654424,"D−f−1
X"
X,0.5141903171953256,"s=max(0,D−2f+1)"
X,0.5150250417362271,"1
D(D −s)(D −s + 1)"
X,0.5158597662771286," D−f−1
s
 
f−1
D−f−s
"
X,0.5166944908180301," D−2
D−f
 ="
X,0.5175292153589316,"D−f−1
X"
X,0.5183639398998331,"s=max(0,D−2f+1)"
X,0.5191986644407346,"1
D(D −s)(D −s + 1)
(D −f)!(f −2)!"
X,0.5200333889816361,"(D −2)!
·"
X,0.5208681135225376,"(D −f −1)!(f −1)!
s!(D −f −s −1)!(D −f −s)!(−D + 2f + s −1)!."
X,0.5217028380634391,"For ∀D ≥f, we have"
X,0.5225375626043406,"f −2
D −1
¯E ≤"
X,0.5233722871452421,"D−f−1
X"
X,0.5242070116861436,"s=max(0,D−2f)"
X,0.5250417362270451,"(f −2)(D −1)(−D + 2f + s)
D(D −1)f(f −1)(D −s)(D −s + 1)"
X,0.5258764607679466,"(D −f −1)!f!
s!(D −f −s −1)!(D −f −s)!(−D + 2f + s)!
(D −f)!(f −1)!"
X,0.5267111853088481,(D −1)!
X,0.5275459098497496,"≤E
h
(f −2)(f −(D −f −g2))
Df(f −1)(D −g2)(D −g2 + 1) i"
X,0.5283806343906511,"< E
h
(f −g0 −g1)
Df(f + g0 + g1)(f + g0 + g1 + 1) i
."
X,0.5292153589315526,"Consequently, we have"
X,0.5300500834724541,"˜δD > 2a(f −a)E
h
2
D(f + g0 + g1)(f + g0 + g1 + 1) −
f −g0 −g1
Df(f + g0 + g1)(f + g0 + g1 + 1)"
X,0.5308848080133556,"−
f + g0 + g1
Df(f + g0 + g1)(f + g0 + g1 + 1) i = 0,"
X,0.5317195325542571,and note that this holds for ∀D ≥K. The proof is complete.
X,0.5325542570951586,"A.6
PROOF OF THEOREM 3.5"
X,0.5333889816360601,"Theorem 3.5 (Uniform Superiority). For any two binary vectors v, w ∈{0, 1}D with J ̸= 0 or 1,
it holds that V ar[ ˆJσ,π(v, w)] < V ar[ ˆJMH(v, w)]."
X,0.5342237061769616,"Proof. By assumption we have 0 < a < f. To compare V ar[ ˆJσ,π] with V ar[ ˆJMH] = J(1−J) K
="
X,0.5350584307178631,"J
K + (K−1)J2"
X,0.5358931552587646,"K
−J2, it sufﬁces to compare ˜E with J2. When D = f, we know that the location vector
x of (v, w) contains no “−” elements. It is easy to verify that in this case, |G0| = |G1| = |L2| = 0,
and |L0| follows hyper(f −1, a, a −1). By Theorem 3.2, it follows that when D = f,"
X,0.5367278797996661,˜ED = 1
X,0.5375626043405676,f E[|L0|] = a(a −1)
X,0.5383973288814691,f(f −1) = J ˜J < J2.
X,0.5392320534223706,Recall the deﬁnition ˜J = a−1
X,0.5400667779632721,"f−1, which is always less than J. On the other hand, as D →∞, we have
|L0| →0, |L2| →a, |G0| →a and |G1| →f −a. We can easily show that"
X,0.5409015025041736,"˜ED →J2,
as D →∞."
X,0.5417362270450752,"By Lemma 3.4, the sequence ( ˜Ef, ˜Ef+1, ˜Ef+2, ...) is strictly increasing. Since it is convergent with
limit J2, by the Monotone Convergence Theorem we know that ˜ED < J2, ∀D ≥f."
X,0.5425709515859767,"A.7
PROOF OF PROPOSITION 3.6"
X,0.5434056761268782,"Proposition 3.6 (Consistent Improvement). Suppose f is ﬁxed. In terms of a, the variance ratio
ρ(a) = V ar[ ˆ
JMH(v,w)]
V ar[ ˆ
Jσ,π(v,w)] is constant for any 0 < a < f."
X,0.5442404006677797,Under review as a conference paper at ICLR 2022
X,0.5450751252086812,"Proof. Let ˜E be deﬁned as in Theorem 3.2. Assume that D and f are ﬁxed and a is variable. Firstly,
we can write the variance ratio explicitly as"
X,0.5459098497495827,ρ(a) = J−J2
X,0.5467445742904842,"K
J
K + (K+1) ˜E"
X,0.5475792988313857,"K
−J2 =
1 −J"
X,0.5484140233722872,"1 −J −(K −1)(J −
˜E
J )
."
X,0.5492487479131887,"We now show that the term J −
˜E
J = C(1 −J), where C is some constant independent of J (i.e.,
a). Then, for ﬁxed D and f, by cancellation ρ(a) would be constant for all 0 < a < f. We have"
X,0.5500834724540902,"J −
˜E
J = a"
X,0.5509181969949917,"f −Ea,f,D
h
fl0
a(f + g0 + g1) +
g0 + l2
f + g0 + g1 i"
X,0.5517529215358932,"= E
ha2(f + g0 + g1) −f 2l0 −af(g0 + l2)"
X,0.5525876460767947,af(f + g0 + g1) i
X,0.5534223706176962,"= E
ha(a −f)(g0 + g1) + a2f + afg1 −f 2l0 −afl2"
X,0.5542570951585977,af(f + g0 + g1) i
X,0.5550918196994992,"= E
ha(a −f)(g0 + g1) + af(l0 + l1) + afg1 −f 2l0"
X,0.5559265442404007,af(f + g0 + g1) i
X,0.5567612687813022,"= E
ha(a −f)(g0 + g1) + f(a −f)l0 + af(f −a −h1)"
X,0.5575959933222037,af(f + g0 + g1)
X,0.5584307178631052,"i
,
(27)"
X,0.5592654424040067,"where we use the constraints (10) that l0 + l1 + l2 = a and l1 + g1 + h1 = f −a. We now study the
three terms respectively. We have"
X,0.5601001669449082,"E
ha(a −f)(g0 + g1)"
X,0.5609348914858097,af(f + g0 + g1)
X,0.5617696160267112,"i
= −(1 −J)E
h
g0 + g1
f + g0 + g1"
X,0.5626043405676127,"i
≜−E′(1 −J)."
X,0.5634390651085142,We have shown in the proof of Lemma 3.4 that
X,0.5642737896494157,"Ea,f,D
h
l0
f + g0 + g1"
X,0.5651085141903172,"i
= a(a −1)"
X,0.5659432387312187,"D −1 Ea−1,f−1,D−1
h
1
f + g0 + g1"
X,0.5667779632721202,"i
≜a(a −1)"
X,0.5676126878130217,"D −1 E∗,"
X,0.5684474123539232,and by symmetry it holds that
X,0.5692821368948247,"Ea,f,D
h
h1
f + g0 + g1"
X,0.5701168614357263,"i
= (f −a)(f −a −1)"
X,0.5709515859766278,"D −1
E∗."
X,0.5717863105175293,"Note that Since f is ﬁxed, (|G0| + |G1|) is distributed independent of a. Consequently, E′ and E∗
are both independent of a. Next, we obtain"
X,0.5726210350584308,"E
h
f(a −f)l0
af(f + g0 + g1)"
X,0.5734557595993323,"i
= −(1 −J)f(a −1)"
X,0.5742904841402338,"D −1 E∗, and"
X,0.5751252086811353,"E
h af(f −a −h1)"
X,0.5759599332220368,af(f + g0 + g1)
X,0.5767946577629383,"i
= (1 −J)fE∗−(1 −J)f(f −a −1)"
X,0.5776293823038398,"D −1
E∗."
X,0.5784641068447413,"Summing up the terms and substituting into (27), we derive"
X,0.5792988313856428,"J −
˜E
J = C(1 −J),"
X,0.5801335559265443,where C = −E′ + (f −f(f−2)
X,0.5809682804674458,"D−1 )E∗, which is independent of a. Taking into ρ(a), we get"
X,0.5818030050083473,"ρ(a) =
1 −J
1 −J −(K −1)C(1 −J) =
1
1 −(K −1)C ,"
X,0.5826377295492488,"which is a constant only depending on f, D and K. This completes the proof."
X,0.5834724540901502,Under review as a conference paper at ICLR 2022
X,0.5843071786310517,"A.8
PROOF OF THEOREM 5.1"
X,0.5851419031719532,"Proof. Denote E[1k] := E[1{hk(v) = hk(w)}] for any 1 ≤k ≤K. We ﬁrst recall some notations.
We have v, w ∈{0, 1}D, and a and f are deﬁned in (5). Denote B1 = {i : xi = O}, B2 = {i :
xi = ×} and B3 = {i : xi = −} as the sets of three types of points, respectively. For a ≤j ≤D
and 1 ≤k ≤K, deﬁne"
X,0.5859766277128547,"A−(j) = {xi : (i + k −1 mod D) + 1 ≤j},
A+(j) = {xi : (i + k −1 mod D) + 1 > j}."
X,0.5868113522537562,"Let n−,1(j) = |{xi = O : i ∈A−(j)}| be the number of “O” points in A−(j). Analogously let
n−,2(j), n−,3(j) be the number of “×” and “−” points in A−(j), and n+,1(j), n+,2(j), n+,3(j) be
the number of “O”, “×” and “−” points in A+(j). For any i, denote i∗= (i + k −1 mod D) + 1,
i# = (i −k −1 mod D) + 1."
X,0.5876460767946577,"Our analysis starts with the decomposition of hash collision probability,"
X,0.5884808013355592,"E[1k] = P
h
hk(v) = hk(w)
i
= D
X"
X,0.5893155258764607,"j=1
P
h
hk(v) = hk(w) = j
i
,
(28)"
X,0.5901502504173622,"where recall h(·) is the hash sample. Consider the process for generating the hash. As before, we
look at the location vector x. In Method 2, we ﬁrst permute x by π to get π(x). Then the k-th
hash samples collide if the minimum of π→k(π(x)) is “O”. One key observation is that, when
applying π→k, the random index for the i-th element in π(x) is exactly the one used for xi# (shifted
backwards) in the initial permutation. A toy example in provided in Figure 8 to help understand the
reasoning."
X,0.5909849749582637,"Figure 8: Illustration of C-MinHash-(π, π) hash collision, with k = 2. Here, “circulant right” means
“circulant down”. Small indices correspond to upper elements."
X,0.5918196994991652,"Further denote set M = {π(i) : π(i) /∈B3} be the collection of indices of initially permuted vector
π(x) that are not “−” points, and M←k = (M −k −1 mod D) + 1 be the corresponding indices
shifted backwards. In Figure 8, M = {2, 5, 8, 11}. Also, π(6) = 4, and the permutation maps are
described by the red arrows. Consequently, in π→k(π(x)), the 8-th element (“O”) in π(x) will be
permuted to the same index of the 8 −2 = 6-th element in x, which equals to 4. It is important to
notice that, when considering the k-th collision, only points in M←k matters. Hence, we deduct:"
X,0.5926544240400667,"• (Collision condition) Denote i = arg mint∈M←k π(xt) be the location of minimal permu-
tation index in M←k. The k-th collision occurs at j i.f.f. π(i) = j, and the i∗-th element
in π(x) must be a “O” point. Recall the deﬁnition i∗= (i + k −1 mod D) + 1."
X,0.5934891485809682,Under review as a conference paper at ICLR 2022
X,0.5943238731218697,"In Figure 8, consider i = 6 and j = 4 for example. Above condition means that the i = 6-th element
in x (“−” in red bold border) is permuted to the j = 4-th position, and it is above all other permuted
elements with red bold borders. Meanwhile, the i + k = 6 + 2 = 8-th element in π(x) must be a
“O”. Figure 8 exactly satisﬁes the condition, so it depicts a collision. Mathematically, we have"
X,0.5951585976627712,"E[1k] = D
X"
X,0.5959933222036727,"j=1
P
h
hk(v) = hk(w) = j
i = D
X j=1 D
X"
X,0.5968280467445742,"i=1
P
h
π(i) = j, π−1(i∗) ∈B1
i
,
(29)"
X,0.5976627712854758,"where i = arg mint∈M←k π(xt). In this expression, everything is random of π, except for the set
B1 which is ﬁxed given the data."
X,0.5984974958263773,"Now we will focus on deriving the probability for a ﬁxed i and j in (29). Our analysis will be
conditional on the collection of variables Z which is deﬁned as follows. Let z−,1, z−,2 and z−,3 be
the number of “O”, “×” and “−” points in A−(j) ∩Mc
←k, and z+,1, z+,2 and z+,3 be the number
of “O”, “×” and “−” points in A+(j)∩Mc
←k, respectively. Here Mc
←k represents the complement
of M←k. Notice that Z (and its density function) depends on different j since A−(j) and A+(j)
depends on j. For the ease of notation we suppress the information of j in Z (and z’s). It is easy to
see that Z = (z−,k|3
1, z+,k|3
1) follows hyper(D, D −f, n−,k(j)|3
1, n+,k(j)|3
1). Denote the domain of
Z and Θj. Conditional on Z, we obtain"
X,0.5993322203672788,"E[1k] = D
X j=1 X Z∈Θj D
X"
X,0.6001669449081803,"i=1
P
h
π(i) = j, π−1(i∗) ∈B1|Z
i
Pj
h
Z
i = D
X j=1 X Z∈Θj D
X"
X,0.6010016694490818,"i=1
P
h
π−1(i∗) ∈B1|π(i) = j, Z
i
P
h
π(i) = j|Z
i
Pj
h
Z
i ≜ D
X j=1 X Z∈Θj D
X"
X,0.6018363939899833,"i=1
Γ(i, j)Pj
h
Z
i
,
(30)"
X,0.6026711185308848,"with i = arg mint∈M←k π(xt). We will carefully compute the probabilities in the summation.
Basically, the key is that elements in M need to be controlled, i.e. smaller than j, and other positions
can be arbitrary. Given Z, this means that we need to put r1 = a −z−,1 −z+,1 type “O” points,
r2 = f −a−z−,2 −z+,2 type “×” points and r3 = D −f −z−,3 −z+,3 type “−” points no smaller
than j, with π(i) = j exactly. Also note that there are ﬁxed b0 = P3
k=1 z+,k type “−” points no
smaller than j."
X,0.6035058430717863,"With all these deﬁnitions and reasoning, we are ready to proceed with the proof. Based on xi, we
have three general cases."
X,0.6043405676126878,1) xi ∈B1. The ﬁrst case is that xi = O.
X,0.6051752921535893,"Case 1a) j < i∗. Firstly, we consider the case where j < i∗. By combinatorial theory we have"
X,0.6060100166944908,"P
h
π(i) = j|Z
i
= P
h
π(i) = j|π−1(j) /∈B3, Z
i
P
h
π−1(j) /∈B3|Z
i"
X,0.6068447412353923,"=
1
r1 + r2"
X,0.6076794657762938," b0
r3
 D−j−b0
r1+r2−1
"
X,0.6085141903171953," D−f
r3
 
f
r1+r2
P
h
π−1(j) /∈B3|Z
i"
X,0.6093489148580968,"≜˜P1 · P
h
π−1(j) /∈B3|Z
i
,
(31)"
X,0.6101836393989983,"where the second probability is that the j-th element in π(x) is not “−”. Conditional on Z, the
probability is dependent on j#:"
X,0.6110183639398998,"P
h
π−1(j) /∈B3|Z
i
="
X,0.6118530884808013,"3
X"
X,0.6126878130217028,"p=1
1{j# ∈Bp}(1 −
z−,p
n−,p(j))."
X,0.6135225375626043,Under review as a conference paper at ICLR 2022
X,0.6143572621035058,Combining with (31) we obtain
X,0.6151919866444073,"P
h
π(i) = j|Z
i
= ˜P1"
X,0.6160267111853088,"3
X"
X,0.6168614357262103,"p=1
1{j# ∈Bp}(1 −
z−,p
n−,p(j)).
(32)"
X,0.6176961602671118,"Next we compute P[π−1(i∗) ∈B1|π(i) = j, Z]. Note that given the conditions, π−1(i∗) has two
cases: 1) it comes from M←k (i.e. it is one of the elements with red bold border); 2) Otherwise. We
then can write"
X,0.6185308848080133,"P
h
π−1(i∗) ∈B1|π(i) = j, Z
i"
X,0.6193656093489148,"= P
h
π−1(i∗) ∈B1|π−1(i∗) ∈M←k, π(i) = j, Z
i
P
h
π−1(i∗) ∈M←k|π(i) = j, Z
i"
X,0.6202003338898163,"+ P
h
π−1(i∗) ∈B1|π−1(i∗) /∈M←k, π(i) = j, Z
i
P
h
π−1(i∗) /∈M←k|π(i) = j, Z
i"
X,0.6210350584307178,"= (1 −
z+,1
n+,1(j))
r1 + r2 −1"
X,0.6218697829716193,D −j −b0
X,0.6227045075125208,"r1 −1
r1 + r2 −1 + (1 −r1 + r2 −1"
X,0.6235392320534223,"D −j −b0
)
a −r1
f −r1 −r2 "
X,0.6243739565943238,"≜(1 −
z+,1
n+,1(j))

r1 −1
D −j −b0
+ (1 −r1 + r2 −1"
X,0.6252086811352254,"D −j −b0
)J∗
"
X,0.6260434056761269,"≜(1 −
z+,1
n+,1(j)) ¯J1.
(33)"
X,0.6268781302170284,"Combining (32) and (33), we obtain when i ∈B1 and j < i∗,"
X,0.6277128547579299,"Γ(i, j) ="
X,0.6285475792988314,"3
X"
X,0.6293823038397329,"p=1
1{j# ∈Bp}(1 −
z−,p
n−,p(j))(1 −
z+,1
n+,1(j)) ˜P1 ¯J1.
(34)"
X,0.6302170283806344,"Case 1b) j = i∗. Similarly approach also applies to the situation with j = i∗. In this case,"
X,0.6310517529215359,"P
h
π−1(j) /∈B3|Z
i
= (1 −
z−,1
n−,1(j)) ˜P1,
P[π−1(i∗) ∈B1|π(i) = j, Z] = 1.
(35)"
X,0.6318864774624374,"The equations are because (i∗)# = i ∈B1, and equivalently, π−1(i∗) = π−1(j) = i ∈B1."
X,0.6327212020033389,"Case 1c) j > i∗. I this case, we still have"
X,0.6335559265442404,"P
h
π(i) = j|Z
i
= ˜P1"
X,0.6343906510851419,"3
X"
X,0.6352253756260434,"p=1
1{j# ∈Bp}(1 −
z−,p
n−,p(j)),"
X,0.6360601001669449,"but the probability of π−1(i∗) being “O” is different. Since j > i∗, this event now depends on z−,p,
p = 1, 2, 3. More speciﬁcally,"
X,0.6368948247078464,"P[π−1(i∗) ∈B1|π(i) = j, Z] = """
X,0.6377295492487479,"1{j# ∈B1}(1 −
z−,1
n−,1(j) −1) +
X"
X,0.6385642737896494,"p=2,3
1{j# ∈Bp}(1 −
z−,p
n−,p(j)) # J∗."
X,0.6393989983305509,"Therefore, when i ∈B1 and j > j∗, it holds that"
X,0.6402337228714524,"Γ(i, j) = (1 −
z−,1
n−,1(j)) """
X,0.6410684474123539,"1{j# ∈B1}(1 −
z−,1
n−,1(j) −1) +
X"
X,0.6419031719532554,"p=2,3
1{j# ∈Bp}(1 −
z−,p
n−,p(j)) # J∗. (36)"
X,0.6427378964941569,"2) xi ∈B2. The case where xi ∈B2 can be analyzed using similar arguments. For conciseness, we
mainly present the ﬁnal results."
X,0.6435726210350584,Case 2a) j < i∗. The calculation ends up in the same form. We have
X,0.6444073455759599,"P
h
π(i) = j|Z
i
= ˜P2"
X,0.6452420701168614,"3
X"
X,0.6460767946577629,"p=1
1{j# ∈Bp}(1 −
z−,p
n−,p(j)),"
X,0.6469115191986644,Under review as a conference paper at ICLR 2022
X,0.6477462437395659,"with ˜P2 = ˜P1. In addition,"
X,0.6485809682804674,"P
h
π−1(i∗) ∈B1|π(i) = j, Z
i
= (1 −
z+,2
n+,2(j)) ¯J2,"
X,0.6494156928213689,"where ¯J2 =
r1
D−j−b0 + (1 −r1+r2−1"
X,0.6502504173622704,"D−j−b0 )J∗. Hence, when xi ∈B2 and j < i∗, we have"
X,0.6510851419031719,"Γ(i, j) ="
X,0.6519198664440734,"3
X"
X,0.6527545909849749,"p=1
1{j# ∈Bp}(1 −
z−,p
n−,p(j))(1 −
z+,2
n+,2(j)) ˜P2 ¯J2.
(37)"
X,0.6535893155258765,"Case 2b) j = i∗. In this case, Γ(i, j) simply equals to 0, since π−1(i∗) = π−1(j) ∈B2. The
probability of π−1(i∗) being “O” is 0."
X,0.654424040066778,"Case 2c) j > i∗. Omitting the details, we have"
X,0.6552587646076795,"Γ(i, j) = (1 −
z−,2
n−,2(j)) """
X,0.656093489148581,"1{j# ∈B2}(1 −
z−,2
n−,2(j) −1) +
X"
X,0.6569282136894825,"p=1,3
1{j# ∈Bp}(1 −
z−,p
n−,p(j)) # J∗. (38)"
X,0.657762938230384,2) xi ∈B3.
X,0.6585976627712855,"Case 3a) j < i∗. The expression is different from previous two, in that we need π−1(j) ∈B3."
X,0.659432387312187,"P
h
π(i) = j|Z
i
= P
h
π(i) = j|π−1(j) ∈B3, Z
i
P
h
π−1(j) ∈B3|Z
i = ˜P3"
X,0.6602671118530885,"3
X"
X,0.66110183639399,"p=1
1{j# ∈Bp}
z−,p
n−,p(j), where"
X,0.6619365609348915,˜P3 = 1 r3
X,0.662771285475793,"  b0
r3−1
 D−j−b0
r1+r2
"
X,0.6636060100166945," D−f
r3
 
f
r1+r2
 ."
X,0.664440734557596,"Moreover, we have"
X,0.6652754590984975,"P
h
π−1(i∗) ∈B1|π(i) = j, Z
i
= (1 −
z+,3
n+,3(j)) ¯J3,"
X,0.666110183639399,"with ¯J3 =
r1
D−j−b0 + (1 −
r1+r2
D−j−b0 )J∗. Combining parts together we obtain"
X,0.6669449081803005,"Γ(i, j) ="
X,0.667779632721202,"3
X"
X,0.6686143572621035,"p=1
1{j# ∈Bp}
z−,p
n−,p(j)(1 −
z+,3
n+,3(j)) ˜P3 ¯J3.
(39)"
X,0.669449081803005,"Case 3b) j = i∗. By same reasoning as Case 2a), Γ(i, j) = 0."
X,0.6702838063439065,Case 3c) j > i∗. We have in this case
X,0.671118530884808,"Γ(i, j) = """
X,0.6719532554257095,"1{j# ∈B3}
z−,3
n−,3(j)
n−,3(j) −z−,3"
X,0.672787979966611,"n−,3(j) −1
+ (1 −
z−,3
n−,3(j))
X"
X,0.6736227045075125,"p=1,2
1{j# ∈Bp}
z−,p
n−,p(j) # J∗ ="
X,0.674457429048414,"3
X"
X,0.6752921535893155,"p=1
1{j# ∈Bp}
z−,p
n−,p(j)(1 −z−,3 −1{p = 3}"
X,0.676126878130217,"n−,3(j)
−1{p = 3})J∗.
(40)"
X,0.6769616026711185,"Finally, combining (30), (34), (35), (36), (37), (38), (39) and (40) and re-organizing terms, the proof
is complete."
X,0.67779632721202,Under review as a conference paper at ICLR 2022
X,0.6786310517529215,"B
MORE NUMERICAL JUSTIFICATION ON C-MINHASH-(π, π)"
X,0.679465776293823,"The “Words” dataset (Li & Church, 2005) (which is publicly available) contains a large number of
word vectors, with the i-th entry indicating whether this word appears in the i-th document, for a
total of D = 216 documents. The key statistics of the 120 selected word pairs are presented in
Table 1. Those 120 pairs of words are more or less randomly selected except that we make sure
they cover a wide spectrum of data distributions. Denote d as the number of non-zero entries in the
vector. Table 1 reports the density ˜d = d/D for each word vector, ranging from 0.0006 to 0.6. The
Jaccard similarity J ranges from 0.002 to 0.95."
X,0.6803005008347245,"In Figures 9 - 16, we plot the empirical MSE along with the empirical bias2 for ˆJπ,π, as well as
the empirical MSE for ˆJσ,π. Note that for D this large, it is numerically difﬁcult to evaluate the
theoretical variance formulas. From the results in the Figures, we can observe"
X,0.6811352253756261,"• For all the data pairs, the MSE of C-MinHash-(π, π) estimator overlaps with the empirical
MSE of C-MinHash-(σ, π) estimator for all K from 1 up to 4096."
X,0.6819699499165276,"• The bias2 is several orders of magnitudes smaller than the MSE, in all data pairs. This
veriﬁes that the bias of ˆJπ,π is extremely small in practice and can be safely neglected."
X,0.6828046744574291,"We have many more plots on more data pairs. Nevertheless, we believe the current set of experiments
on this “Words” dataset should be sufﬁcient to verify that, the proposed C-MinHash-(π, π) could
give indistinguishable Jaccard estimation accuracy in practice compared with C-MinHash-(σ, π)."
X,0.6836393989983306,Under review as a conference paper at ICLR 2022
X,0.6844741235392321,"Table 1: 120 selected word pairs from the Words dataset (Li & Church, 2005). For each pair, we
report the density ˜d (number of non-zero entries divided by D = 216) for each word as well as the
Jaccard similarity J. Both ˜d and J cover a wide range of values."
X,0.6853088480801336,"˜
d1
˜
d2
J
˜
d1
˜
d2
J
ABOUT - INTO
0.302
0.125
0.258
NEW - WEB
0.291
0.194
0.224
ABOUT - LIKE
0.302
0.140
0.281
NEWS - LIKE
0.168
0.140
0.172
ACTUAL - DEVELOPED
0.017
0.030
0.071
NO - WELL
0.220
0.120
0.244
ACTUAL - GRABBED
0.017
0.002
0.016
NOT - IT
0.281
0.295
0.437
AFTER - OR
0.103
0.356
0.220
NOTORIOUSLY - LOCK
0.0006
0.006
0.004
AND - PROBLEM
0.554
0.044
0.070
OF - THEN
0.570
0.104
0.168
AS - NAME
0.280
0.144
0.204
OF - WE
0.570
0.226
0.361
AT - CUT
0.374
0.242
0.052
OPPORTUNITY - COUNTRIES
0.029
0.024
0.066
BE - ONE
0.323
0.221
0.403
OUR - THAN
0.244
0.125
0.245
BEST - AND
0.136
0.554
0.228
OVER - BACK
0.148
0.160
0.233
BRAZIL - OH
0.010
0.031
0.019
OVER - TWO
0.148
0.121
0.289
BUT - MANY
0.167
0.116
0.340
PEAK - SHOWS
0.006
0.033
0.026
CALLED - BUSINESSES
0.016
0.018
0.043
PEOPLE - BY
0.121
0.425
0.228
CALORIES - MICROSOFT
0.002
0.045
0.0003
PEOPLE - INFO
0.121
0.138
0.117
CAN - FROM
0.243
0.326
0.444
PICKS - BOOST
0.007
0.005
0.007
CAN - SEARCH
0.243
0.214
0.237
PLANET - REWARD
0.013
0.003
0.018
COMMITTED - PRODUCTIVE
0.013
0.004
0.029
PLEASE - MAKE
0.168
0.141
0.195
CONTEMPORARY - FLASH
0.011
0.021
0.013
PREFER - PUEDE
0.010
0.003
0.0001
CONVENIENTLY - INDUSTRIES
0.003
0.011
0.009
PRIVACY - FOUND
0.126
0.136
0.053
COPYRIGHT - AN
0.218
0.290
0.209
PROSECUTION - MAXIMIZE
0.002
0.003
0.006
CREDIT - CARD
0.046
0.041
0.285
RECENTLY - INT
0.028
0.007
0.014
DE - WEB
0.117
0.194
0.091
REPLY - ACHIEVE
0.013
0.012
0.023
DO - GOOD
0.174
0.102
0.276
RESERVED - BEEN
0.172
0.141
0.108
EARTH - GROUPS
0.021
0.035
0.056
RIGHTS - FIND
0.187
0.144
0.166
EXPRESSED - FRUSTRATED
0.010
0.002
0.024
RIGHTS - RESERVED
0.187
0.172
0.877
FIND - HAS
0.144
0.228
0.214
SCENE - ABOUT
0.012
0.301
0.029
FIND - SITE
0.144
0.275
0.212
SEE - ALSO
0.138
0.166
0.291
FIXED - SPECIFIC
0.011
0.039
0.054
SEIZE - ANYTHING
0.0007
0.037
0.012
FLIGHT - TRANSPORTATION
0.011
0.018
0.040
SHOULDERS - GORGEOUS
0.003
0.004
0.028
FOUND - DE
0.136
0.117
0.039
SICK - FELL
0.008
0.008
0.085
FRANCISCO - SAN
0.025
0.049
0.476
SITE - CELLULAR
0.275
0.006
0.010
GOOD - BACK
0.102
0.160
0.220
SOLD - LIVE
0.018
0.064
0.055
GROUPS - ORDERED
0.035
0.011
0.034
SOLO - CLAIMS
0.010
0.012
0.007
HAPPY - CONCEPT
0.029
0.013
0.054
SOON - ADVANCE
0.040
0.017
0.057
HAVE - FIRST
0.267
0.151
0.320
SPECIALIZES - ACTUAL
0.003
0.017
0.008
HAVE - US
0.267
0.284
0.349
STATE - OF
0.101
0.570
0.165
HILL - ASSURED
0.020
0.004
0.011
STATES - UNITED
0.061
0.062
0.591
HOME - SYNTHESIS
0.365
0.002
0.003
TATTOO - JEANS
0.002
0.004
0.035
HONG - KONG
0.014
0.014
0.925
THAT - ALSO
0.301
0.166
0.376
HOSTED - DRUGS
0.016
0.013
0.013
THIS - CITY
0.423
0.123
0.132
INTERVIEWS - FOURTH
0.012
0.011
0.031
THEIR - SUPPORT
0.165
0.117
0.189
KANSAS - PROPERTY
0.017
0.045
0.052
THEIR - VIEW
0.165
0.103
0.151
KIRIBATI - GAMBIA
0.003
0.003
0.712
THEM - OF
0.112
0.570
0.187
LAST - THIS
0.135
0.423
0.221
THEN - NEW
0.104
0.291
0.192
LEAST - ROMANCE
0.046
0.007
0.019
THINKS - LOT
0.007
0.040
0.079
LIME - REGISTERED
0.002
0.030
0.004
TIME - OUT
0.189
0.191
0.366
LINKS - TAKE
0.191
0.105
0.134
TIME - WELL
0.189
0.120
0.299
LINKS - THAN
0.191
0.125
0.141
TOP - AS
0.140
0.280
0.217
MAIL - AND
0.160
0.554
0.192
TOP - COPYRIGHT
0.140
0.218
0.149
MAIL - BACK
0.160
0.160
0.132
TOP - NEWS
0.140
0.168
0.192
MAKE - LIKE
0.141
0.140
0.297
UP - AND
0.200
0.554
0.334
MANAGING - LOCK
0.010
0.006
0.010
UP - HAS
0.200
0.228
0.312
MANY - US
0.116
0.284
0.210
US - BE
0.284
0.323
0.335
MASS - DREAM
0.016
0.017
0.048
VIEW - IN
0.103
0.540
0.153
MAY - HELP
0.184
0.156
0.206
VIEW - PEOPLE
0.103
0.121
0.138
MOST - HOME
0.141
0.365
0.207
WALKED - ANTIVIRUS
0.006
0.002
0.002
NAME - IN
0.144
0.540
0.207
WEB - GO
0.194
0.111
0.138
NEITHER - FIGURE
0.011
0.016
0.085
WELL - INFO
0.120
0.138
0.110
NET - SO
0.101
0.154
0.112
WELL - NEWS
0.120
0.168
0.161
NEW - PLEASE
0.291
0.168
0.205
WEEKS - LONDON
0.028
0.032
0.050"
X,0.6861435726210351,Under review as a conference paper at ICLR 2022
X,0.6869782971619366,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
X,0.6878130217028381,WELL - INFO
PERM,0.6886477462437396,"2 Perm
1 Perm Bias2"
PERM,0.6894824707846411,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6903171953255426,ABOUT - INTO
PERM,0.6911519198664441,"2 Perm
1 Perm Bias2"
PERM,0.6919866444073456,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6928213689482471,ABOUT - LIKE
PERM,0.6936560934891486,"2 Perm
1 Perm Bias2"
PERM,0.6944908180300501,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6953255425709516,WELL - NEWS
PERM,0.6961602671118531,"2 Perm
1 Perm Bias2"
PERM,0.6969949916527546,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.6978297161936561,ACTUAL - DEVELOPED
PERM,0.6986644407345576,"2 Perm
1 Perm Bias2"
PERM,0.6994991652754591,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7003338898163606,ACTUAL - GRABBED
PERM,0.7011686143572621,"2 Perm
1 Perm Bias2"
PERM,0.7020033388981636,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7028380634390651,AFTER - OR
PERM,0.7036727879799666,"2 Perm
1 Perm Bias2"
PERM,0.7045075125208681,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7053422370617696,AND - PROBLEM
PERM,0.7061769616026711,"2 Perm
1 Perm Bias2"
PERM,0.7070116861435726,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7078464106844741,AS - NAME
PERM,0.7086811352253757,"2 Perm
1 Perm Bias2"
PERM,0.7095158597662772,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7103505843071787,AT - CUT
PERM,0.7111853088480802,"2 Perm
1 Perm Bias2"
PERM,0.7120200333889817,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7128547579298832,BE - ONE
PERM,0.7136894824707847,"2 Perm
1 Perm Bias2"
PERM,0.7145242070116862,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7153589315525877,BEST - AND
PERM,0.7161936560934892,"2 Perm
1 Perm Bias2"
PERM,0.7170283806343907,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7178631051752922,BRAZIL - OH
PERM,0.7186978297161937,"2 Perm
1 Perm Bias2"
PERM,0.7195325542570952,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7203672787979967,BUT - MANY
PERM,0.7212020033388982,"2 Perm
1 Perm Bias2"
PERM,0.7220367278797997,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7228714524207012,CALLED - BUSINESSES
PERM,0.7237061769616027,"2 Perm
1 Perm Bias2"
PERM,0.7245409015025042,"Figure 9: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.7253756260434057,Under review as a conference paper at ICLR 2022
PERM,0.7262103505843072,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7270450751252087,CALORIES - MICROSOFT
PERM,0.7278797996661102,"2 Perm
1 Perm Bias2"
PERM,0.7287145242070117,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7295492487479132,CAN - FROM
PERM,0.7303839732888147,"2 Perm
1 Perm Bias2"
PERM,0.7312186978297162,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7320534223706177,CAN - SEARCH
PERM,0.7328881469115192,"2 Perm
1 Perm Bias2"
PERM,0.7337228714524207,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7345575959933222,COMMITTED - PRODUCTIVE
PERM,0.7353923205342237,"2 Perm
1 Perm Bias2"
PERM,0.7362270450751253,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7370617696160268,CONTEMPORARY - FLASH
PERM,0.7378964941569283,"2 Perm
1 Perm Bias2"
PERM,0.7387312186978298,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7395659432387313,CONVENIENTLY - INDUSTRIES
PERM,0.7404006677796328,"2 Perm
1 Perm Bias2"
PERM,0.7412353923205343,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7420701168614358,COPYRIGHT - AN
PERM,0.7429048414023373,"2 Perm
1 Perm Bias2"
PERM,0.7437395659432388,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7445742904841403,CREDIT - CARD
PERM,0.7454090150250418,"2 Perm
1 Perm Bias2"
PERM,0.7462437395659433,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7470784641068448,DE - WEB
PERM,0.7479131886477463,"2 Perm
1 Perm Bias2"
PERM,0.7487479131886478,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7495826377295493,DO - GOOD
PERM,0.7504173622704507,"2 Perm
1 Perm Bias2"
PERM,0.7512520868113522,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7520868113522537,EARTH - GROUPS
PERM,0.7529215358931552,"2 Perm
1 Perm Bias2"
PERM,0.7537562604340567,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7545909849749582,EXPRESSED - FRUSTRATED
PERM,0.7554257095158597,"2 Perm
1 Perm Bias2"
PERM,0.7562604340567612,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7570951585976627,FIND - HAS
PERM,0.7579298831385642,"2 Perm
1 Perm Bias2"
PERM,0.7587646076794657,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7595993322203672,FIND - SITE
PERM,0.7604340567612687,"2 Perm
1 Perm Bias2"
PERM,0.7612687813021702,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7621035058430717,FIXED - SPECIFIC
PERM,0.7629382303839732,"2 Perm
1 Perm Bias2"
PERM,0.7637729549248747,"Figure 10: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.7646076794657763,Under review as a conference paper at ICLR 2022
PERM,0.7654424040066778,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7662771285475793,FLIGHT - TRANSPORTATION
PERM,0.7671118530884808,"2 Perm
1 Perm Bias2"
PERM,0.7679465776293823,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7687813021702838,FOUND - DE
PERM,0.7696160267111853,"2 Perm
1 Perm Bias2"
PERM,0.7704507512520868,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7712854757929883,FRANCISCO - SAN
PERM,0.7721202003338898,"2 Perm
1 Perm Bias2"
PERM,0.7729549248747913,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7737896494156928,GOOD - BACK
PERM,0.7746243739565943,"2 Perm
1 Perm Bias2"
PERM,0.7754590984974958,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7762938230383973,GROUPS - ORDERED
PERM,0.7771285475792988,"2 Perm
1 Perm Bias2"
PERM,0.7779632721202003,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7787979966611018,HAPPY - CONCEPT
PERM,0.7796327212020033,"2 Perm
1 Perm Bias2"
PERM,0.7804674457429048,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7813021702838063,HAVE - FIRST
PERM,0.7821368948247078,"2 Perm
1 Perm Bias2"
PERM,0.7829716193656093,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7838063439065108,HAVE - US
PERM,0.7846410684474123,"2 Perm
1 Perm Bias2"
PERM,0.7854757929883138,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7863105175292153,HILL - ASSURED
PERM,0.7871452420701168,"2 Perm
1 Perm Bias2"
PERM,0.7879799666110183,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7888146911519198,HOME - SYNTHESIS
PERM,0.7896494156928213,"2 Perm
1 Perm Bias2"
PERM,0.7904841402337228,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7913188647746243,HONG - KONG
PERM,0.7921535893155259,"2 Perm
1 Perm Bias2"
PERM,0.7929883138564274,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7938230383973289,HOSTED - DRUGS
PERM,0.7946577629382304,"2 Perm
1 Perm Bias2"
PERM,0.7954924874791319,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7963272120200334,INTERVIEWS - FOURTH
PERM,0.7971619365609349,"2 Perm
1 Perm Bias2"
PERM,0.7979966611018364,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.7988313856427379,KANSAS - PROPERTY
PERM,0.7996661101836394,"2 Perm
1 Perm Bias2"
PERM,0.8005008347245409,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8013355592654424,KIRIBATI - GAMBIA
PERM,0.8021702838063439,"2 Perm
1 Perm Bias2"
PERM,0.8030050083472454,"Figure 11: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.8038397328881469,Under review as a conference paper at ICLR 2022
PERM,0.8046744574290484,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8055091819699499,LAST - THIS
PERM,0.8063439065108514,"2 Perm
1 Perm Bias2"
PERM,0.8071786310517529,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8080133555926544,LEAST - ROMANCE
PERM,0.8088480801335559,"2 Perm
1 Perm Bias2"
PERM,0.8096828046744574,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8105175292153589,LIME - REGISTERED
PERM,0.8113522537562604,"2 Perm
1 Perm Bias2"
PERM,0.8121869782971619,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8130217028380634,LINKS - TAKE
PERM,0.8138564273789649,"2 Perm
1 Perm Bias2"
PERM,0.8146911519198664,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8155258764607679,LINKS - THAN
PERM,0.8163606010016694,"2 Perm
1 Perm Bias2"
PERM,0.8171953255425709,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8180300500834724,MAIL - AND
PERM,0.8188647746243739,"2 Perm
1 Perm Bias2"
PERM,0.8196994991652755,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.820534223706177,MAIL - BACK
PERM,0.8213689482470785,"2 Perm
1 Perm Bias2"
PERM,0.82220367278798,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8230383973288815,MAKE - LIKE
PERM,0.823873121869783,"2 Perm
1 Perm Bias2"
PERM,0.8247078464106845,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.825542570951586,MANAGING - LOCK
PERM,0.8263772954924875,"2 Perm
1 Perm Bias2"
PERM,0.827212020033389,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8280467445742905,MANY - US
PERM,0.828881469115192,"2 Perm
1 Perm Bias2"
PERM,0.8297161936560935,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.830550918196995,MASS - DREAM
PERM,0.8313856427378965,"2 Perm
1 Perm Bias2"
PERM,0.832220367278798,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8330550918196995,MAY - HELP
PERM,0.833889816360601,"2 Perm
1 Perm Bias2"
PERM,0.8347245409015025,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.835559265442404,MOST - HOME
PERM,0.8363939899833055,"2 Perm
1 Perm Bias2"
PERM,0.837228714524207,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8380634390651085,NAME - IN
PERM,0.83889816360601,"2 Perm
1 Perm Bias2"
PERM,0.8397328881469115,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.840567612687813,NEITHER - FIGURE
PERM,0.8414023372287145,"2 Perm
1 Perm Bias2"
PERM,0.842237061769616,"Figure 12: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.8430717863105175,Under review as a conference paper at ICLR 2022
PERM,0.843906510851419,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8447412353923205,NET - SO
PERM,0.845575959933222,"2 Perm
1 Perm Bias2"
PERM,0.8464106844741235,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8472454090150251,NEW - PLEASE
PERM,0.8480801335559266,"2 Perm
1 Perm Bias2"
PERM,0.8489148580968281,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8497495826377296,NEW - WEB
PERM,0.8505843071786311,"2 Perm
1 Perm Bias2"
PERM,0.8514190317195326,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8522537562604341,NEWS - LIKE
PERM,0.8530884808013356,"2 Perm
1 Perm Bias2"
PERM,0.8539232053422371,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8547579298831386,NO - WELL
PERM,0.8555926544240401,"2 Perm
1 Perm Bias2"
PERM,0.8564273789649416,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8572621035058431,NOT - IT
PERM,0.8580968280467446,"2 Perm
1 Perm Bias2"
PERM,0.8589315525876461,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8597662771285476,NOTORIOUSLY - LOCK
PERM,0.8606010016694491,"2 Perm
1 Perm Bias2"
PERM,0.8614357262103506,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8622704507512521,OF - THEN
PERM,0.8631051752921536,"2 Perm
1 Perm Bias2"
PERM,0.8639398998330551,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8647746243739566,OF - WE
PERM,0.8656093489148581,"2 Perm
1 Perm Bias2"
PERM,0.8664440734557596,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8672787979966611,OPPORTUNITY - COUNTRIES
PERM,0.8681135225375626,"2 Perm
1 Perm Bias2"
PERM,0.8689482470784641,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8697829716193656,WALKED - ANTIVIRUS
PERM,0.8706176961602671,"2 Perm
1 Perm Bias2"
PERM,0.8714524207011686,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8722871452420701,OUR - THAN
PERM,0.8731218697829716,"2 Perm
1 Perm Bias2"
PERM,0.8739565943238731,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8747913188647746,OVER - BACK
PERM,0.8756260434056762,"2 Perm
1 Perm Bias2"
PERM,0.8764607679465777,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8772954924874792,OVER - TWO
PERM,0.8781302170283807,"2 Perm
1 Perm Bias2"
PERM,0.8789649415692822,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8797996661101837,PEAK - SHOWS
PERM,0.8806343906510852,"2 Perm
1 Perm Bias2"
PERM,0.8814691151919867,"Figure 13: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.8823038397328882,Under review as a conference paper at ICLR 2022
PERM,0.8831385642737897,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8839732888146912,PEOPLE - BY
PERM,0.8848080133555927,"2 Perm
1 Perm Bias2"
PERM,0.8856427378964942,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8864774624373957,PEOPLE - INFO
PERM,0.8873121869782972,"2 Perm
1 Perm Bias2"
PERM,0.8881469115191987,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8889816360601002,PICKS - BOOST
PERM,0.8898163606010017,"2 Perm
1 Perm Bias2"
PERM,0.8906510851419032,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8914858096828047,PLANET - REWARD
PERM,0.8923205342237062,"2 Perm
1 Perm Bias2"
PERM,0.8931552587646077,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8939899833055092,PLEASE - MAKE
PERM,0.8948247078464107,"2 Perm
1 Perm Bias2"
PERM,0.8956594323873122,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8964941569282137,PREFER - PUEDE
PERM,0.8973288814691152,"2 Perm
1 Perm Bias2"
PERM,0.8981636060100167,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.8989983305509182,PRIVACY - FOUND
PERM,0.8998330550918197,"2 Perm
1 Perm Bias2"
PERM,0.9006677796327212,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9015025041736227,PROSECUTION - MAXIMIZE
PERM,0.9023372287145242,"2 Perm
1 Perm Bias2"
PERM,0.9031719532554258,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9040066777963273,RECENTLY - INT
PERM,0.9048414023372288,"2 Perm
1 Perm Bias2"
PERM,0.9056761268781303,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9065108514190318,REPLY - ACHIEVE
PERM,0.9073455759599333,"2 Perm
1 Perm Bias2"
PERM,0.9081803005008348,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9090150250417363,RESERVED - BEEN
PERM,0.9098497495826378,"2 Perm
1 Perm Bias2"
PERM,0.9106844741235393,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9115191986644408,RIGHTS - FIND
PERM,0.9123539232053423,"2 Perm
1 Perm Bias2"
PERM,0.9131886477462438,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9140233722871453,RIGHTS - RESERVED
PERM,0.9148580968280468,"2 Perm
1 Perm Bias2"
PERM,0.9156928213689483,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9165275459098498,SCENE - ABOUT
PERM,0.9173622704507512,"2 Perm
1 Perm Bias2"
PERM,0.9181969949916527,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9190317195325542,SEE - ALSO
PERM,0.9198664440734557,"2 Perm
1 Perm Bias2"
PERM,0.9207011686143572,"Figure 14: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.9215358931552587,Under review as a conference paper at ICLR 2022
PERM,0.9223706176961602,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9232053422370617,SEIZE - ANYTHING
PERM,0.9240400667779632,"2 Perm
1 Perm Bias2"
PERM,0.9248747913188647,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9257095158597662,SHOULDERS - GORGEOUS
PERM,0.9265442404006677,"2 Perm
1 Perm Bias2"
PERM,0.9273789649415692,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9282136894824707,SICK - FELL
PERM,0.9290484140233722,"2 Perm
1 Perm Bias2"
PERM,0.9298831385642737,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9307178631051753,SITE - CELLULAR
PERM,0.9315525876460768,"2 Perm
1 Perm Bias2"
PERM,0.9323873121869783,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9332220367278798,SOLD - LIVE
PERM,0.9340567612687813,"2 Perm
1 Perm Bias2"
PERM,0.9348914858096828,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9357262103505843,SOLO - CLAIMS
PERM,0.9365609348914858,"2 Perm
1 Perm Bias2"
PERM,0.9373956594323873,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9382303839732888,SOON - ADVANCE
PERM,0.9390651085141903,"2 Perm
1 Perm Bias2"
PERM,0.9398998330550918,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9407345575959933,SPECIALIZES - ACTUAL
PERM,0.9415692821368948,"2 Perm
1 Perm Bias2"
PERM,0.9424040066777963,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9432387312186978,STATE - OF
PERM,0.9440734557595993,"2 Perm
1 Perm Bias2"
PERM,0.9449081803005008,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9457429048414023,STATES - UNITED
PERM,0.9465776293823038,"2 Perm
1 Perm Bias2"
PERM,0.9474123539232053,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9482470784641068,TATTOO - JEANS
PERM,0.9490818030050083,"2 Perm
1 Perm Bias2"
PERM,0.9499165275459098,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9507512520868113,THAT - ALSO
PERM,0.9515859766277128,"2 Perm
1 Perm Bias2"
PERM,0.9524207011686143,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9532554257095158,THIS - CITY
PERM,0.9540901502504173,"2 Perm
1 Perm Bias2"
PERM,0.9549248747913188,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9557595993322203,THEIR - SUPPORT
PERM,0.9565943238731218,"2 Perm
1 Perm Bias2"
PERM,0.9574290484140233,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9582637729549248,THEIR - VIEW
PERM,0.9590984974958264,"2 Perm
1 Perm Bias2"
PERM,0.9599332220367279,"Figure 15: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
PERM,0.9607679465776294,Under review as a conference paper at ICLR 2022
PERM,0.9616026711185309,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9624373956594324,THEM - OF
PERM,0.9632721202003339,"2 Perm
1 Perm Bias2"
PERM,0.9641068447412354,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9649415692821369,THEN - NEW
PERM,0.9657762938230384,"2 Perm
1 Perm Bias2"
PERM,0.9666110183639399,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9674457429048414,THINKS - LOT
PERM,0.9682804674457429,"2 Perm
1 Perm Bias2"
PERM,0.9691151919866444,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9699499165275459,TIME - OUT
PERM,0.9707846410684474,"2 Perm
1 Perm Bias2"
PERM,0.9716193656093489,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9724540901502504,TIME - WELL
PERM,0.9732888146911519,"2 Perm
1 Perm Bias2"
PERM,0.9741235392320534,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9749582637729549,TOP - AS
PERM,0.9757929883138564,"2 Perm
1 Perm Bias2"
PERM,0.9766277128547579,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9774624373956594,TOP - COPYRIGHT
PERM,0.9782971619365609,"2 Perm
1 Perm Bias2"
PERM,0.9791318864774624,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9799666110183639,TOP - NEWS
PERM,0.9808013355592654,"2 Perm
1 Perm Bias2"
PERM,0.9816360601001669,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9824707846410684,UP - AND
PERM,0.9833055091819699,"2 Perm
1 Perm Bias2"
PERM,0.9841402337228714,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9849749582637729,UP - HAS
PERM,0.9858096828046744,"2 Perm
1 Perm Bias2"
PERM,0.986644407345576,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9874791318864775,US - BE
PERM,0.988313856427379,"2 Perm
1 Perm Bias2"
PERM,0.9891485809682805,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.989983305509182,VIEW - IN
PERM,0.9908180300500835,"2 Perm
1 Perm Bias2"
PERM,0.991652754590985,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9924874791318865,VIEW - PEOPLE
PERM,0.993322203672788,"2 Perm
1 Perm Bias2"
PERM,0.9941569282136895,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.994991652754591,WEB - GO
PERM,0.9958263772954925,"2 Perm
1 Perm Bias2"
PERM,0.996661101836394,"100
101
102
103
104 K 10-8 10-6 10-4 10-2 100 MSE"
PERM,0.9974958263772955,WEEKS - LONDON
PERM,0.998330550918197,"2 Perm
1 Perm Bias2"
PERM,0.9991652754590985,"Figure 16: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs."
