Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.006329113924050633,"Deep learning (DL) has emerged as a powerful tool for accelerated MRI recon-
struction, but often necessitates a database of fully-sampled measurements for
training. Recent self-supervised and unsupervised learning approaches enable
training without fully-sampled data. However, a database of undersampled mea-
surements may not be available in many scenarios, especially for scans involving
contrast or translational acquisitions in development. Moreover, recent studies
show that database-trained models may not generalize well when the unseen mea-
surements differ in terms of sampling pattern, acceleration rate, SNR, image con-
trast, and anatomy. Such challenges necessitate a new methodology to enable
subject-speciﬁc DL MRI reconstruction without external training datasets, since
it is clinically imperative to provide high-quality reconstructions that can be used
to identify lesions/disease for every individual. In this work, we propose a zero-
shot self-supervised learning approach to perform subject-speciﬁc accelerated DL
MRI reconstruction to tackle these issues. The proposed approach partitions the
available measurements from a single scan into three disjoint sets. Two of these
sets are used to enforce data consistency and deﬁne loss during training for self-
supervision, while the last set serves to self-validate, establishing an early stopping
criterion. In the presence of models pre-trained on a database with different image
characteristics, we show that the proposed approach can be combined with transfer
learning for faster convergence time and reduced computational complexity."
INTRODUCTION,0.012658227848101266,"1
INTRODUCTION"
INTRODUCTION,0.0189873417721519,"Magnetic resonance imaging (MRI) is a non-invasive, radiation-free medical imaging modality that
provides excellent soft tissue contrast for diagnostic purposes. However, lengthy acquisition times
in MRI remain a limitation. Accelerated MRI techniques acquire fewer measurements at a sub-
Nyquist rate, and use redundancies in the acquisition system or the images to remove the resulting
aliasing artifacts during reconstruction. In clinical MRI systems, multi-coil receivers are used during
data acquisition. Parallel imaging (PI) is the most clinically used method for accelerated MRI, and
exploits the redundancies between these coils for reconstruction (Pruessmann et al., 1999; Griswold
et al., 2002). Compressed sensing (CS) is another conventional accelerated MRI technique that
exploits the compressibility of images in sparsifying transform domains (Lustig et al., 2007), and
is commonly used in combination with PI. However, PI and CS may suffer from noise and residual
artifacts at high acceleration rates (Robson et al., 2008; Sandino et al., 2020)."
INTRODUCTION,0.02531645569620253,"Recently, deep learning (DL) methods have emerged as an alternative for accelerated MRI due
to their improved reconstruction quality compared to conventional approaches (Hammernik et al.,
2018; Knoll et al., 2020b; Akc¸akaya et al., 2022). Particularly, physics-guided deep learning recon-
struction (PG-DLR) approaches have gained interest due to their robustness and improved perfor-
mance (Hammernik et al., 2018; Hosseini et al., 2020b). PG-DLR explicitly incorporates the physics
of the data acquisition system into the neural network via a procedure known as algorithm unrolling
(Monga et al., 2021). This is done by unrolling iterative optimization algorithms that alternate be-
tween data consistency (DC) and regularization steps for a ﬁxed number of iterations. Subsequently,
PG-DLR methods are trained in a supervised manner using large databases of fully-sampled mea-
surements (Hammernik et al., 2018; Aggarwal et al., 2019). More recently, self-supervised learning"
INTRODUCTION,0.03164556962025317,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0379746835443038,"has shown that reconstruction quality similar to supervised PG-DLR can be achieved while training
on a database of only undersampled measurements (Yaman et al., 2020)."
INTRODUCTION,0.04430379746835443,"While such database learning strategies offer improved reconstruction quality, acquisition of large
datasets may often be infeasible. In some MRI applications involving time-varying physiological
processes, dynamic information such as time courses of signal changes, contrast uptake or breath-
ing patterns may differ substantially between subjects, making it difﬁcult to generate high-quality
databases of sufﬁcient size for the aforementioned strategies. Furthermore, database training, in
general, brings along concerns about robustness and generalization (Eldar et al., 2017; Knoll et al.,
2020c). In MRI reconstruction, this may exhibit itself when there are mismatches between training
and test datasets in terms of image contrast, sampling pattern, SNR, vendor, and anatomy. While it is
imperative to have high-quality reconstructions that can be used to correctly identify lesions/disease
for every individual, the fastMRI transfer track challenge shows that pretrained models fail to gen-
eralize when applied to patients/scans with different distribution or acquisition parameters, with po-
tential for misdiagnosis (Muckley et al., 2021). Finally, training datasets may lack examples of rare
and/or subtle pathologies, increasing the risk of generalization failure (Knoll et al., 2019; 2020c)."
INTRODUCTION,0.05063291139240506,"In this work, we tackle these challenges associated with database training, and propose a zero-shot
self-supervised learning (ZS-SSL) approach, which performs subject-speciﬁc training of PG-DLR
without any external training database. Succinctly, ZS-SSL partitions the acquired measurements
into three types of disjoint sets, which are respectively used only in the PG-DLR neural network,
in deﬁning the training loss, and in establishing a stopping strategy to avoid overﬁtting. Thus, our
training is both self-supervised and self-validated. In cases where a database-pretrained network is
available, ZS-SSL leverages transfer learning (TL) for improved reconstruction quality and reduced
computational complexity."
INTRODUCTION,0.056962025316455694,Our contributions can be summarized as follows:
INTRODUCTION,0.06329113924050633,"• We propose a zero-shot self-supervised method for learning subject-speciﬁc DL MRI re-
construction from a single undersampled dataset without any external training database.
• We provide a well-deﬁned methodology for determining stopping criterion to avoid over-
ﬁtting in contrast to other single-image training approaches (Ulyanov et al., 2018).
• We apply the proposed zero-shot learning approach to knee and brain MRI datasets, and
show its efﬁcacy in removing residual aliasing and banding artifacts compared to super-
vised database learning.
• We show our ZS-SSL can be combined with with TL in cases when a database-pretrained
model is available to reduce computational costs.
• We show that our zero-shot learning strategies address robustness and generalizability is-
sues of trained supervised models in terms of changes in sampling pattern, acceleration
rate, contrast, SNR, and anatomy at inference time."
BACKGROUND AND RELATED WORK,0.06962025316455696,"2
BACKGROUND AND RELATED WORK"
ACCELERATED MRI ACQUISITION MODEL,0.0759493670886076,"2.1
ACCELERATED MRI ACQUISITION MODEL"
ACCELERATED MRI ACQUISITION MODEL,0.08227848101265822,"In MRI, raw measurement data is acquired in the frequency domain, also known as k-space. In
current clinical MRI systems, multiple receiver coils are used, where each is sensitive to different
parts of the volume. In practice, MRI is accelerated by taking fewer measurements, which are
characterized by an undersampling mask that speciﬁes the acquired locations in k-space. For a
multi-coil MRI acquisition, the forward model is given as"
ACCELERATED MRI ACQUISITION MODEL,0.08860759493670886,"yi = PΩFCix + ni, i ∈{1, . . . , nc},
(1)"
ACCELERATED MRI ACQUISITION MODEL,0.0949367088607595,"where x is the underlying image, yi is the acquired data for the ith coil, PΩis the masking operator
for undersampling pattern Ω, F is the Fourier transform, Ci is a diagonal matrix characterizing the
ith coil sensitivity, ni is measurement noise for ith coil, and nc is the number of coils (Pruessmann
et al., 1999). This system can be concatenated across the coil dimension for a compact representation"
ACCELERATED MRI ACQUISITION MODEL,0.10126582278481013,"yΩ= EΩx + n,
(2)"
ACCELERATED MRI ACQUISITION MODEL,0.10759493670886076,"where yΩis the acquired undersampled measurements across all coils, EΩis the forward encod-
ing operator that concatenates PΩFCi across i ∈{1, . . . , nc}. The general inverse problem for"
ACCELERATED MRI ACQUISITION MODEL,0.11392405063291139,Published as a conference paper at ICLR 2022
ACCELERATED MRI ACQUISITION MODEL,0.12025316455696203,"accelerated MRI is given as
arg min
x ∥yΩ−EΩx∥2
2 + R(x),
(3)"
ACCELERATED MRI ACQUISITION MODEL,0.12658227848101267,"where the ∥yΩ−EΩx∥2
2 term enforces consistency with acquired data (DC) and R(·) is a regularizer."
PG-DLR WITH ALGORITHM UNROLLING,0.13291139240506328,"2.2
PG-DLR WITH ALGORITHM UNROLLING"
PG-DLR WITH ALGORITHM UNROLLING,0.13924050632911392,"Several optimization methods are available for solving the inverse problem in (3) (Fessler, 2020).
Variable-splitting via quadratic penalty is one of the approaches that can be employed to cast Eq. (3)
into two sub-problems as"
PG-DLR WITH ALGORITHM UNROLLING,0.14556962025316456,"z(i) = arg min
z µ∥x(i−1) −z∥2
2 + R(z),
(4a)"
PG-DLR WITH ALGORITHM UNROLLING,0.1518987341772152,"x(i) = arg min
x ∥yΩ−EΩx∥2
2 + µ∥x −z(i)∥2
2,
(4b)"
PG-DLR WITH ALGORITHM UNROLLING,0.15822784810126583,"where µ is the penalty parameter, z(i) is an intermediate variable and x(i) is the desired image at
iteration i. In PG-DLR, an iterative algorithm, as in (4a) and (4b) is unrolled for a ﬁxed number
of iterations (Liang et al., 2020). The regularizer sub-problem in Eq. (4a) is implicitly solved with
neural networks and the DC sub-problem in Eq. (4b) is solved via linear methods such as gradient
descent (Hammernik et al., 2018) or conjugate gradient (CG) (Aggarwal et al., 2019)."
PG-DLR WITH ALGORITHM UNROLLING,0.16455696202531644,"There have been numerous works on PG-DLR for accelerated MRI (Schlemper et al., 2018; Ham-
mernik et al., 2018; Aggarwal et al., 2019; Liang et al., 2020; Yaman et al., 2020). Most of these
works vary from each other on the algorithms used for DC and neural networks employed in the
regularizer units. However, all these works require a large database of training samples."
SUPERVISED LEARNING FOR PG-DLR,0.17088607594936708,"2.3
SUPERVISED LEARNING FOR PG-DLR"
SUPERVISED LEARNING FOR PG-DLR,0.17721518987341772,"In supervised PG-DLR, training is performed using a database of fully-sampled reference data. Let
yn
ref be the fully-sampled k-space for subject n and f(yn
Ω, En
Ω; θ) be the output of the unrolled net-
work for under-sampled k-space yn
Ω, where the network is parameterized by θ. End-to-end training
minimizes (Knoll et al., 2020b; Yaman et al., 2020)"
SUPERVISED LEARNING FOR PG-DLR,0.18354430379746836,"min
θ
1
N N
X"
SUPERVISED LEARNING FOR PG-DLR,0.189873417721519,"n=1
L(yn
ref, En
fullf(yn
Ω, En
Ω; θ)),
(5)"
SUPERVISED LEARNING FOR PG-DLR,0.1962025316455696,"where N is the number of samples in the training database, En
full is the fully-sampled encoding
operator that transform network output to k-space and L(·, ·) is a loss function."
SELF-SUPERVISED LEARNING FOR PG-DLR,0.20253164556962025,"2.4
SELF-SUPERVISED LEARNING FOR PG-DLR"
SELF-SUPERVISED LEARNING FOR PG-DLR,0.2088607594936709,"Unlike supervised learning, self-supervised learning enables training without fully-sampled data by
only utilizing acquired undersampled measurements (Yaman et al., 2020). A masking approach is
used for self-supervision in this setting, where a subset Λ ⊂Ωis set aside for checking prediction
performance/loss calculation, while the remainder of points Θ = Ω\Λ are used in the DC units of
the PG-DLR network. End-to-end training is performed using the loss function"
SELF-SUPERVISED LEARNING FOR PG-DLR,0.21518987341772153,"min
θ
1
N N
X"
SELF-SUPERVISED LEARNING FOR PG-DLR,0.22151898734177214,"n=1
L

yn
Λ, En
Λ
 
f(yn
Θ, En
Θ; θ)

.
(6)"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.22784810126582278,"3
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.23417721518987342,"As discussed in Section 1, lack of large datasets in numerous MRI applications, as well as robustness
and generalizability issues of pretrained models pose a challenge for the clinical translation of DL
reconstruction methods. Hence, subject-speciﬁc reconstruction is desirable in clinical practice, since
it is critical to achieve a reconstruction quality that can be used for correctly diagnosing every patient.
While the conventional self-supervised masking strategy, as in (Yaman et al., 2020) can be applied
for subject-speciﬁc learning, it leads to overﬁtting unless the training is stopped early (Hosseini et al.,
2020a). This is similar to other single-image learning strategies, such as the deep image prior (DIP)"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.24050632911392406,Published as a conference paper at ICLR 2022
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.2468354430379747,"Figure 1: An overview of the proposed zero-shot self-supervised learning approach. a) Acquired
measurements for the single scan are partitioned into three sets: a training (Θ) and loss mask (Λ) for
self-supervision, and a self-validation mask for automated early stopping (Γ). b) The parameters,
θ, of the unrolled MRI reconstruction network are updated using Θ and Λ in the data consistency
(DC) units of the unrolled network and for deﬁning loss, respectively. c) Concurrently, a k-space
validation procedure is used to establish the stopping criterion by using Ω\Γ in the DC units and Γ
to measure a validation loss. d) Once the network training has been stopped due to an increasing
trend in the k-space validation loss, the ﬁnal reconstruction is performed using the relevant learned
network parameters and all the acquired measurements in the DC unit."
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.25316455696202533,"or zero-shot super-resolution (Ulyanov et al., 2018; Shocher et al., 2018). DIP-type approaches
shows that an untrained neural network can successfully perform instance-speciﬁc image restoration
tasks such as denoising, super-resolution, inpainting without any training data. However, such DIP-
type techniques requires an early stopping for avoiding over-ﬁtting, which is typically done with
a manual heuristic selection (Ulyanov et al., 2018; Hosseini et al., 2020a; Darestani et al., 2021).
While this may work in a research setting, having a well-deﬁned automated early stopping criterion
is critical to fully harness the potential of subject-speciﬁc DL MRI reconstruction in practice."
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.25949367088607594,"Early stopping regularization in database-trained setting is conventionally motivated through the
bias-variance trade-off, in which a validation set is used as a proxy for the generalization error to
identify the stopping criterion. Using the same bias-variance trade-off motivation, having a valida-
tion set can aid in devising a stopping criterion, but this has not been feasible in existing zero-shot
learning approaches, which either use all acquired measurements (Ulyanov et al., 2018; Senouf et al.,
2019) or partition them into two sets for training and deﬁning loss (Hosseini et al., 2020a). Hence,
existing zero-shot learning techniques lack a validation set to identify the stopping criterion."
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.26582278481012656,"ZS-SSL Formulation and Training: We propose a new ZS-SSL partitioning framework to enable
subject-speciﬁc self-supervised training and validation with a well-deﬁned stopping criterion. We
deﬁne the following partition for the available measurement locations from a single scan, Ω:"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.2721518987341772,"Ω= Θ ⊔Λ ⊔Γ,
(7)"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.27848101265822783,"where ⊔denotes a disjoint union, i.e. Θ, Λ and Γ are pairwise disjoint (Figure 1). Similar to Section
2.4, Θ is used in the DC units of the unrolled network, and Λ is used to deﬁne a k-space loss for the
self-supervision of the network. The third partition Γ is a set of acquired k-space indices set aside for
deﬁning a k-space validation loss. Thus, ZS-SSL training is both self-supervised and self-validated."
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.2848101265822785,"In general, since zero-shot learning approaches perform training using a single dataset, generation of
multiple data pairs from this single dataset is necessary to self-supervise the neural network (Quan
et al., 2020). Hence, we generate multiple (Θ, Λ) pairs from the acquired locations Ωof the single
scan. In ZS-SSL, this is achieved by ﬁxing the k-space validation partition Γ ⊂Ω, and performing
the retrospective masking on Ω\Γ multiple times. Formally, Ω\Γ is partitioned K times such that"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.2911392405063291,"Ω\Γ = Θk ⊔Λk,
k ∈{1, . . . , K},
(8)"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.2974683544303797,Published as a conference paper at ICLR 2022
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.3037974683544304,"where Λk, Θk and Γ are pairwise disjoint, i.e. Ω= Γ ⊔Θk ⊔Λk, ∀k. ZS-SSL training minimizes"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.310126582278481,"min
θ
1
K K
X"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.31645569620253167,"k=1
L

yΛk, EΛk
 
f(yΘk, EΘk; θ)
"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.3227848101265823,"In the proposed ZS-SSL, this is supplemented by a k-space self-validation loss, which tests the
generalization performance of the trained network on the k-space validation partition Γ. For the lth"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.3291139240506329,"epoch, where the learned network weights are speciﬁed by θ(l), this validation loss is given by:"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.33544303797468356,"L

yΓ, EΓ
 
f(yΩ\Γ, EΩ\Γ; θ(l))

.
(9)"
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.34177215189873417,"Note that in (9), the network output is calculated by applying the DC units on Ω\Γ = Θ ⊔Λ,
i.e. all acquired points outside of Γ, to better assess its generalizability performance. Our key
motivation is that while the training loss will decrease over epochs, the k-space validation loss will
start increasing once overﬁtting is observed. Thus, we monitor the loss in (9) during training to
deﬁne an early stopping criterion to avoid overﬁtting. Let L be the epoch in which training needs
to be stopped. Then at inference time, the network output is calculated as f(yΩ, EΩ; θ(L)), i.e. all
acquired points are used to calculate the network output."
ZERO-SHOT SELF-SUPERVISED LEARNING FOR PG-DLR,0.34810126582278483,"ZS-SSL with Transfer Learning (TL): While pretrained models are very efﬁcient in reconstruct-
ing new unseen measurements from similar MRI scan protocols, their performance degrades signif-
icantly when acquisition parameters vary (Muckley et al., 2021). Moreover, retraining a new model
on a large database for each acquisition parameter, sampling/contrast/anatomy/acceleration, may be
very computationally expensive (Knoll et al., 2019). Hence, TL has been used for re-training DL
models pre-trained on large databases to reconstruct MRI data with different characteristics (Knoll
et al., 2019). However, such transfer still requires another, often smaller, database for re-training. In
contrast, in the presence of pre-trained models, ZS-SSL can be combined with TL, referred to as ZS-
SSL-TL, to reconstruct a single slice/instance with different characteristics by using weights of the
pre-trained model for initialization. Thus, ZS-SSL-TL ensures that the pretrained model is adapted
for each patient/subject, while facilitating faster convergence time and reduced reconstruction time."
EXPERIMENTS,0.35443037974683544,"4
EXPERIMENTS"
DATASETS,0.36075949367088606,"4.1
DATASETS"
DATASETS,0.3670886075949367,"We performed experiments on publicly available fully-sampled multi-coil knee and brain MRI from
fastMRI database (Knoll et al., 2020a). Knee and brain MRI datasets contained data from 15 and 16
receiver coils, respectively. Fully-sampled datasets were retrospectively undersampled by keeping
24 lines of autocalibrated signal (ACS) from center of k-space. FastMRI database contains different
contrast weightings. For knee MRI, we used coronal proton density (Cor-PD) and coronal proton
density with fat suppression (Cor-PDFS), and for brain MRI, axial FLAIR (Ax-FLAIR) and axial
T2 (Ax-T2). Different types of datasets and undersampling masks used in this study are provided in
Figure 7 in the Appendix."
IMPLEMENTATION DETAILS,0.37341772151898733,"4.2
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.379746835443038,"All PG-DLR approaches were trained end-to-end using 10 unrolled iterations. CG method and a
ResNet structure (Timofte et al., 2017) were employed in the DC and regularizer units of the unrolled
network, respectively (Yaman et al., 2020). The ResNet is comprised of a layer of input and output
convolution layers, and 15 residual blocks (RB) each containing two convolutional layers, where
the ﬁrst layer is followed by ReLU and the second layer is followed by a constant multiplication
(Timofte et al., 2017). All layers had a kernel size of 3 × 3, 64 channels. The real and imaginary
parts of the complex MR images were concatenated prior to being input to the ResNet as 2-channel
images. The unrolled network, which shares parameters across the unrolled iterations had a total of
592,129 trainable parameters. Coil sensitivity maps were generated from the central 24×24 ACS
using ESPIRiT (Uecker et al., 2014). End-to-end training was performed with a normalized ℓ1-ℓ2
loss (Adam optimizer, LR = 5·10−4, batch size = 1) (Yaman et al., 2020). Peak signal-to-noise ratio
(PSNR) and structural similarity index (SSIM) were used for quantitative evaluation."
IMPLEMENTATION DETAILS,0.3860759493670886,Published as a conference paper at ICLR 2022
RECONSTRUCTION METHOD COMPARISONS,0.3924050632911392,"4.3
RECONSTRUCTION METHOD COMPARISONS"
RECONSTRUCTION METHOD COMPARISONS,0.3987341772151899,"In this work, we focus on comparing training strategies for accelerated MRI reconstruction. Thus,
we use the same network architecture from Section 4.2 for all training methods in all experiments.
We note that the proposed ZS-SSL strategy is agnostic to the speciﬁcs of the neural network ar-
chitecture. In fact, the number of network parameters is higher than the number of undersampled
measurements available on a single slice, i.e. dimension of yΩ. As such, different neural networks
may be used for the regularizer unit in the unrolled network, but this is not the focus of our study."
RECONSTRUCTION METHOD COMPARISONS,0.4050632911392405,"Supervised PG-DLR: Supervised PG-DLR models for knee and brain MRI were trained on 300
slices from 15 and 30 different subjects, respectively. For each knee and brain contrast weighting,
two networks were trained separately using random and uniform masks (Hammernik et al., 2018) at
an acceleration rate (R) of 4 (Knoll et al., 2020c). Trained networks were used for comparison and
TL purposes. We note that random undersampling results in incoherent artifacts, whereas uniform
undersampling leads to coherent artifacts that are harder to remove (Figure 7 in Appendix) (Knoll
et al., 2019). Hence, we focus on the more difﬁcult problem of uniform undersampling, while
presenting random undersampling results in the Appendix."
RECONSTRUCTION METHOD COMPARISONS,0.41139240506329117,"Self-Supervision via Data Undersampling (SSDU) PG-DLR: SSDU (Yaman et al., 2020) PG-
DLR was trained using the same database approach as supervised PG-DLR, with the exception that
SSDU performed training only using the undersampled data (Sec. 2.4)."
RECONSTRUCTION METHOD COMPARISONS,0.4177215189873418,"DIP-Recon: We employ a DIP-type subject-speciﬁc MRI reconstruction that uses all acquired mea-
surements in both DC and deﬁning loss (Senouf et al., 2019; Jafari et al., 2021)"
RECONSTRUCTION METHOD COMPARISONS,0.4240506329113924,"L

yΩ, EΩ
 
f(yΩ, EΩ; θ)

.
(10)"
RECONSTRUCTION METHOD COMPARISONS,0.43037974683544306,"We refer to the reconstruction from this training mechanism as DIP-Recon. DIP-Recon-TL refers to
combining (10) with TL. As mentioned, DIP-Recon does not have a stopping criterion, hence early
stopping was heuristically determined (Figure 8 in the Appendix)."
RECONSTRUCTION METHOD COMPARISONS,0.43670886075949367,"Parallel Imaging: We include CG-SENSE, which is a commonly used subject-speciﬁc conventional
PI method (Pruessmann et al., 1999; 2001), as the clinical baseline quality for comparison purposes."
AUTOMATED STOPPING AND ABLATION STUDY,0.4430379746835443,"4.4
AUTOMATED STOPPING AND ABLATION STUDY"
AUTOMATED STOPPING AND ABLATION STUDY,0.44936708860759494,"The stopping criterion for the proposed ZS-SSL was investigated on slices from the knee dataset.
The k-space self-validation set Γ was selected from the acquired measurements Ωusing a uniformly
random selection with |Γ|/|Ω| = 0.2. The remaining acquired measurements Ω\Γ were retrospec-
tively partitioned into disjoint 2-tuples multiple times based on uniformly random selection with the
ratio ρ = |Λk|/|Ω\Γ| = 0.4 ∀k ∈{1, . . . , K} (Yaman et al., 2020)."
AUTOMATED STOPPING AND ABLATION STUDY,0.45569620253164556,"Figure 2a shows representative subject-speciﬁc training and validation loss curves at R = 4 for K ∈
{1, 10, 25, 50, 100}. As expected, training loss decreases with increasing epochs for all K. The
k-space validation loss for K = 1 decreases without showing a clear breaking point for stopping.
For K > 1, the validation loss forms an L-curve, and the breaking point of the L-curve is used as
the stopping criterion. K = 10 is used for the rest of the study, while noting K = 25, 50 and 100"
AUTOMATED STOPPING AND ABLATION STUDY,0.4620253164556962,"Figure 2:
a) Representative training and k-space validation loss curves for ZS-SSL with multiple
K ∈{1, 10, 25, 50, 100} masks on Cor-PD knee MRI using uniform undersampling at R = 4. For
K > 1 the validation loss forms an L-curve, whose breaking point (red arrows) dictates the auto-
mated early stopping criterion for training. b) Loss curves for ZS-SSL with/without TL for K = 10
on a Cor-PD knee MRI slice. ZS-SSL with TL converges faster compared to ZS-SSL (red arrows)."
AUTOMATED STOPPING AND ABLATION STUDY,0.46835443037974683,Published as a conference paper at ICLR 2022
AUTOMATED STOPPING AND ABLATION STUDY,0.47468354430379744,"Figure 3: Reconstruction results on a representative test slice from a) Cor-PD knee MRI and b) Ax-
FLAIR brain MRI at R = 4 with uniform undersampling. CG-SENSE, DIP-Recon, DIP-Recon-TL
suffer from noise ampliﬁcation and residual artifacts shown with red arrows, especially in knee MRI
due to the unfavorable coil geometry. Subject-speciﬁc ZS-SSL and ZS-SSL-TL achieve artifact-free
and improved reconstruction quality, similar to the database-trained SSDU and supervised PG-DLR."
AUTOMATED STOPPING AND ABLATION STUDY,0.4810126582278481,"also show similar performance. Figure 2b shows loss curves on a Cor-PD slice with and without
transfer learning. ZS-SSL-TL, which uses pre-trained supervised PG-DLR parameters as initial
starting parameters, converges faster in time compared to ZS-SSL, substantially reducing the total
training time. Average computation times for single-instance reconstruction methods are presented
in Table 2 in the Appendix. Similarly, corresponding reconstruction results for the loss curves in
Figure 2a and b are provided in Figure 9 in the Appendix."
RECONSTRUCTION RESULTS,0.4873417721518987,"4.5
RECONSTRUCTION RESULTS"
RECONSTRUCTION RESULTS,0.4936708860759494,"In the ﬁrst set of experiments, we compare all methods for the case when the testing and training
data belong to the same knee/brain MRI contrast weighting with the same acceleration rate and
undersampling mask. These experiments aim to show the efﬁcacy of the proposed approach in
performing subject-speciﬁc MRI reconstruction, while removing residual aliasing artifacts. We also
note that this is the most favorable setup for database-trained supervised PG-DLR."
RECONSTRUCTION RESULTS,0.5,"In the subsequent experiments, we focus on the reported generalization and robustness issues with
database-trained PG-DLR methods (Knoll et al., 2019; 2020c; Defazio et al., 2020; Muckley et al.,
2021). We investigate banding artifacts, as well as in-domain and cross-domain transfer cases. For
these experiments, we concentrate on ZS-SSL-TL, since ZS-SSL has no prior domain information,
and is inherently not susceptible to such generalizability issues."
RECONSTRUCTION RESULTS,0.5063291139240507,"Comparison of Reconstruction Methods: In these experiments, supervised and SSDU PG-DLR
are trained and tested using uniform undersampling at R = 4, representing a perfect match for training
and testing conditions. Figure 3a and b show reconstruction results for Cor-PD knee and Ax-FLAIR
brain MRI datasets in this setting. CG-SENSE reconstruction suffers from signiﬁcant residual arti-
facts and noise ampliﬁcation in Cor-PD knee and Ax-FLAIR brain MRIs, respectively. Similarly,
both DIP-Recon and DIP-Recon-TL suffer from residual artifacts and noise ampliﬁcation. Super-
vised PG-DLR achieves artifact-free reconstruction. Both ZS-SSL and ZS-SSL-TL also perform
artifact-free reconstruction with similar image quality. Table 1 shows the average SSIM and PSNR
values on 30 test slices. Similar observations apply when random undersampling is employed (Fig-
ure 10 in the Appendix). For the remaining experiments, we investigate the generalizability of
database-pretrained models using supervised PG-DLR as baseline due to its higher performance,"
RECONSTRUCTION RESULTS,0.5126582278481012,"Table 1: Average PSNR and SSIM values on 30 test slices.
Metrics
CG-SENSE
Supervised PG-DLR
SSDU PG-DLR
DIP-Recon
DIP-Recon-TL
ZS-SSL
ZS-SSL-TL"
RECONSTRUCTION RESULTS,0.5189873417721519,"Cor-PD
SSIM
0.862
0.952
0.949
0.793
0.819
0.948
0.951
PSNR
34.521
39.966
39.545
32.668
33.583
39.550
40.102"
RECONSTRUCTION RESULTS,0.5253164556962026,"Ax-FLAIR
SSIM
0.836
0.934
0.929
0.799
0.818
0.935
0.937
PSNR
31.969
37.375
36.761
30.637
31.249
36.861
37.250"
RECONSTRUCTION RESULTS,0.5316455696202531,Published as a conference paper at ICLR 2022
RECONSTRUCTION RESULTS,0.5379746835443038,"Figure 4:
Supervised PG-DLR suffers from banding artifacts (yellow arrows), while ZS-SSL-TL
signiﬁcantly alleviates these artifacts. DIP-Recon-TL suffers from clear noise ampliﬁcation."
RECONSTRUCTION RESULTS,0.5443037974683544,"while noting SSDU PG-DLR, which is a self-supervised database-trained model, may also be used
as a baseline if needed."
RECONSTRUCTION RESULTS,0.5506329113924051,"Banding Artifacts: Banding artifacts appear in the form of streaking horizontal lines, and occur due
to high acceleration rates and anisotropic sampling (Defazio et al., 2020). These hinder radiological
evaluation and are regarded as a barrier for the translation of DL reconstruction methods into clinical
practice (Defazio et al., 2020). This set of experiments explored training and testing on Cor-PDFS
data, where database-trained PG-DLR reconstruction has been reported to show such artifacts (De-
fazio et al., 2020; Muckley et al., 2021). Figure 4 shows reconstructions for a Cor-PDFS test slice.
While DIP-Recon-TL suffers from clearly visible noise ampliﬁcation, supervised PG-DLR suffers
from banding artifacts shown with yellow arrows. ZS-SSL-TL signiﬁcantly alleviates these banding
artifacts in the reconstruction. While supervised PG-DLR achieves slightly better SSIM and PSNR
(Table 3 in the Appendix), we note that the banding artifacts do not necessarily correlate with such
metrics, and are usually picked up in expert readings (Defazio et al., 2020; Knoll et al., 2020c)."
RECONSTRUCTION RESULTS,0.5569620253164557,"In-Domain Transfer:
In these experiments, we compared the in-domain generalizability of
database-trained PG-DLR and subject-speciﬁc PG-DLR. For in-domain transfer, training and test
datasets are of the same type of data, but may differ from each other in terms of acceleration and un-
dersampling pattern (Figure 11 in the Appendix). In Figure 5a, supervised PG-DLR was trained with
random undersampling and tested on uniform undersampling, both at R = 4. Supervised PG-DLR
fails to generalize and suffers from residual aliasing artifacts (red arrows), consistent with previous
reports (Knoll et al., 2019; Muckley et al., 2021). Similarly, DIP-Recon-TL suffers from artifacts
and noise ampliﬁcation. Proposed ZS-SSL-TL achieves an artifact-free and improved reconstruc-
tion quality. In Figure 5b, supervised PG-DLR was trained with uniform undersampling at R = 4
and tested on uniform undersampling at R = 6. While both supervised PG-DLR and DIP-Recon-TL
suffers from aliasing artifacts, ZS-SSL-TL successfully removes these artifacts. Average PSNR and
SSIM values align with the observations (Table 3 in the Appendix)."
RECONSTRUCTION RESULTS,0.5632911392405063,"Figure 5: Supervised PG-DLR was trained with a) random mask and tested on uniform mask, both
R = 4; b) uniform mask at R = 4 and tested on R = 6 uniform mask. Supervised PG-DLR and DIP-
Recon-TL suffer from visible artifacts (red arrows). ZS-SSL-TL yields artifact-free reconstruction."
RECONSTRUCTION RESULTS,0.569620253164557,Published as a conference paper at ICLR 2022
RECONSTRUCTION RESULTS,0.5759493670886076,"Figure 6:
Using pre-trained a) Cor-PDFS (low-SNR) and b) Ax-FLAIR (brain MRI) models for
Cor-PD. Supervised PG-DLR fails to generalize for both contrast/SNR and anatomy changes, suf-
fering from residual artifacts (red arrows). DIP-Recon-TL also shows artifacts. ZS-SSL-TL suc-
cessfully removes noise and artifacts for both cases."
RECONSTRUCTION RESULTS,0.5822784810126582,"Cross-Domain Transfer: In the last set of experiments, we investigated the cross-domain gen-
eralizability of database-trained PG-DLR compared to subject-speciﬁc trained PG-DLR. For cross-
domain transfer, training and test datasets are of the different data characteristics and generally differ
in terms of contrast, SNR, and anatomy (Figure 11 in the Appendix). Figure 6 shows results for the
case when the testing contrast/SNR and anatomy differs from training contrast/SNR and anatomy,
even though the same R = 4 uniform undersampling is used for both training and testing. In Fig-
ure 6a, supervised PG-DLR was trained on Cor-PDFS (low-SNR), but tested on Cor-PD (high-SNR
and different contrast). In Figure 6b, supervised PG-DLR was trained on Ax-FLAIR (brain MRI)
and tested on Cor-PD (knee MRI). In both cases, supervised PG-DLR fails to generalize and has
residual artifacts (red arrows). Similarly, DIP-Recon-TL suffers from artifacts and noise. ZS-SSL-
TL achieves an artifact-free improved reconstruction. For both cross-domain transfer experiments,
similar results were observed for brain MRI (Figure 12 in the Appendix). Average PSNR and SSIM
values match these observations (Table 3 in the Appendix)"
CONCLUSIONS,0.5886075949367089,"5
CONCLUSIONS"
CONCLUSIONS,0.5949367088607594,"We proposed a zero-shot self-supervised deep learning method, ZS-SSL, for subject-speciﬁc accel-
erated DL MRI reconstruction from a single undersampled dataset. The proposed ZS-SSL partitions
the acquired measurements from a single scan into three types of disjoint sets, which are used only
in the PG-DLR network, in deﬁning the training loss, and in establishing a validation strategy for
early stopping to avoid overﬁtting. In particular, we showed that with our training methodology
and automated stopping criterion, subject-speciﬁc zero-shot learning of PG-DLR for MRI can be
achieved even when the number of tunable network parameters is higher than the number of avail-
able measurements. Finally, we also combined ZS-SSL with transfer learning, in cases where a
pre-trained model may be available, for faster convergence time and reduced reconstruction time.
Our results showed that ZS-SSL methods perform similarly to database-trained supervised PG-DLR
when training and testing data are matched, and they signiﬁcantly outperform database-trained meth-
ods in terms of artifact reduction and generalizability when the training and testing data differ in
terms of image characteristics and acquisition parameters. In fact, the subject-speciﬁc nature of
ZS-SSL ensures that it is agnostic to such changes in acquisition parameters. As such, the proposed
work is able to provide good reconstruction quality for each subject, and may have signiﬁcant im-
plications in the integration of DL reconstruction to clinical studies. We note that hyperparameters,
such as learning rate may be adjusted based on the domain for further improvements. It is also
noteworthy that the subject-speciﬁc ZS-SSL eliminates the requirement for large training sets. This
may also facilitate the use and appeal of DL reconstruction for recently developed acquisitions, as
well as pilot studies that are often performed to determine the acquisition parameters/acceleration
rates of large-scale imaging studies, such as the Human Connectome Project (HCP) (Ugurbil et al.,
2013). Finally, while we concentrated on physics-guided models in MRI reconstruction, our ideas
and results may inspire further work in related image restoration problems, as well as for generative
models or data-driven problems without a data consistency term."
CONCLUSIONS,0.6012658227848101,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.6075949367088608,ACKNOWLEDGEMENT
CONCLUSIONS,0.6139240506329114,"This work was partially supported by NIH R01HL153146, P41EB027061, U01EB025144; NSF
CAREER CCF-1651825. There is no conﬂict of interest for the authors."
REFERENCES,0.620253164556962,REFERENCES
REFERENCES,0.6265822784810127,"Hemant Kumar Aggarwal, Merry P. Mani, and Mathews Jacob. Modl: Model-based deep learning
architecture for inverse problems. IEEE Trans. Medical Imaging, 38(2):394–405, 2019."
REFERENCES,0.6329113924050633,"Mehmet Akc¸akaya, Burhaneddin Yaman, Hyungjin Chung, and Jong Chul Ye. Unsupervised deep
learning methods for biological image reconstruction and enhancement: An overview from a
signal processing perspective. IEEE Sig Proc Mag, 39(2):28–44, 2022."
REFERENCES,0.6392405063291139,"Jesper LR Andersson and Sotiropoulos Stamatios N. An integrated approach to correction for off-
resonance effects and subject movement in diffusion mr imaging. Neuroimage, 125:1063–1078,
2016."
REFERENCES,0.6455696202531646,"Tamer A Basha, Mehmet Akcakaya, Charlene Liew, Connie W Tsao, Francesca N Delling, Gifty
Addae, Long Ngo, Warren J Manning, and Reza Nezafat. Clinical performance of high-resolution
late gadolinium enhancement imaging with compressed sensing. Journal of Magnetic Resonance
Imaging, 46(6):1829–1838, 2017."
REFERENCES,0.6518987341772152,"Mohammad Zalbagi Darestani, Akshay S. Chaudhari, and Reinhard Heckel. Measuring robustness
in deep learning based compressive sensing. In ICML, volume 139 of Proceedings of Machine
Learning Research, pp. 2433–2444. PMLR, 2021."
REFERENCES,0.6582278481012658,"Aaron Defazio, Tullie Murrell, and Michael Recht. MRI banding removal via adversarial training.
In Advances in Neural Information Processing Systems, volume 33, pp. 7660–7670, 2020."
REFERENCES,0.6645569620253164,"Yonina C. Eldar, Alfred O. Hero III, Li Deng, Jeffrey A. Fessler, Jelena Kovacevic, H. Vincent
Poor, and Steve J. Young. Challenges and open problems in signal processing: Panel discussion
summary from ICASSP. IEEE Signal Process. Mag., 34(6):8–23, 2017."
REFERENCES,0.6708860759493671,"Jeffrey A. Fessler. Optimization methods for magnetic resonance image reconstruction: Key models
and optimization algorithms. IEEE Signal Processing Magazine, 37(1):33–40, 2020."
REFERENCES,0.6772151898734177,"Mark A. Griswold, Peter M. Jakob, Robin M. Heidemann, Mathias Nittka, Vladimir Jellus, Jianmin
Wang, Berthold Kiefer, and Axel Haase. Generalized autocalibrating partially parallel acquisi-
tions (GRAPPA). Magn Reson Med, 47:1202–1210, 2002."
REFERENCES,0.6835443037974683,"Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P. Recht, Daniel K. Sodickson, Thomas
Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI
data. Magn Reson Med, 79:3055–3071, 2018."
REFERENCES,0.689873417721519,"Seyed Amir Hossein Hosseini, Burhaneddin Yaman, Steen Moeller, and Mehmet Akc¸akaya. High-
ﬁdelity accelerated MRI reconstruction by scan-speciﬁc ﬁne-tuning of physics-based neural net-
works. In EMBC, pp. 1481–1484. IEEE, 2020a."
REFERENCES,0.6962025316455697,"Seyed Amir Hossein Hosseini, Burhaneddin Yaman, Steen Moeller, Mingyi Hong, and Mehmet
Akc¸akaya. Dense recurrent neural networks for accelerated MRI: history-cognizant unrolling of
optimization algorithms. IEEE J. Sel. Top. Signal Process., 14(6):1280–1291, 2020b."
REFERENCES,0.7025316455696202,"Ramin Jafari, Pascal Spincemaille, Jinwei Zhang, Thanh D. Nguyen, Martin R. Prince, Xianfu Luo,
Junghun Cho, Daniel Margolis, and Yi Wang. Deep neural network for water/fat separation:
Supervised training, unsupervised training, and no training. Magn Reson Med, 85(4):2263–2277,
04 2021."
REFERENCES,0.7088607594936709,"F. Knoll, J. Zbontar, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio, M. Parente, K. J. Geras, J. Kat-
snelson, H. Chandarana, Z. Zhang, M. Drozdzalv, A. Romero, M. Rabbat, P. Vincent, J. Pinkerton,
D. Wang, N. Yakubova, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui.
fastMRI: A publicly available raw k-space and DICOM dataset of knee images for accelerated
MR image reconstruction using machine learning. Radiology AI, pp. e190007, 2020a."
REFERENCES,0.7151898734177216,Published as a conference paper at ICLR 2022
REFERENCES,0.7215189873417721,"Florian Knoll, Kerstin Hammernik, Erich Kobler, Thomas Pock, Michael P. Recht, and Daniel K.
Sodickson. Assessment of the generalization of learned image reconstruction and the potential
for transfer learning. Magn Reson Med, 81(1):116–128, 01 2019."
REFERENCES,0.7278481012658228,"Florian Knoll, Kerstin Hammernik, Chi Zhang, Steen Moeller, Thomas Pock, Daniel K. Sodick-
son, and Mehmet Akc¸akaya. Deep-learning methods for parallel magnetic resonance imaging
reconstruction. IEEE Signal Processing Magazine, 37(1):128–140, 2020b."
REFERENCES,0.7341772151898734,"Florian Knoll, Tullie Murrell, Anuroop Sriram, Naﬁssa Yakubova, Jure Zbontar, Michael G. Rabbat,
Aaron Defazio, Matthew J. Muckley, Daniel K. Sodickson, C. Lawrence Zitnick, and Michael P.
Recht. Advancing machine learning for MR image reconstruction with an open competition:
Overview of the 2019 fastMRI challenge. Magn Reson Med, Jun 2020c."
REFERENCES,0.740506329113924,"Dong Liang, Jing Cheng, Ziwen Ke, and Leslie Ying. Deep magnetic resonance image reconstruc-
tion: Inverse problems meet neural networks. IEEE Sig Proc Mag, 37(1):141–151, 2020."
REFERENCES,0.7468354430379747,"Michael Lustig, David L. Donoho, Juan M. Santos, and John M. Pauly. Sparse MRI: The application
of compressed sensing for rapid MR imaging. Magn Reson Med, 58:1182–1195, 2007."
REFERENCES,0.7531645569620253,"Vishal Monga, Yuelong Li, and Yonina C. Eldar. Algorithm unrolling: Interpretable, efﬁcient deep
learning for signal and image processing. IEEE Signal Process. Mag., 38(2):18–44, 2021."
REFERENCES,0.759493670886076,"Matthew J. Muckley, Bruno Riemenschneider, Alireza Radmanesh, Sunwoo Kim, Geunu Jeong,
Jingyu Ko, Yohan Jun, Hyungseob Shin, Dosik Hwang, Mahmoud Mostapha, Simon Arberet,
Dominik Nickel, Zaccharie Ramzi, Philippe Ciuciu, Jean-Luc Starck, Jonas Teuwen, Dim-
itrios Karkalousos, Chaoping Zhang, Anuroop Sriram, Zhengnan Huang, Naﬁssa Yakubova,
Yvonne W. Lui, and Florian Knoll. Results of the 2020 fastmri challenge for machine learn-
ing MR image reconstruction. IEEE Trans. Medical Imaging, 40(9):2306–2317, 2021."
REFERENCES,0.7658227848101266,"Klaas P. Pruessmann, Markus Weiger, Markus B. Scheidegger, and Peter Boesiger. SENSE: Sensi-
tivity encoding for fast MRI. Magn Reson Med, 42:952–962, 1999."
REFERENCES,0.7721518987341772,"Klaas P. Pruessmann, Markus Weiger, Peter Bornert, and Peter Boesiger. Advances in sensitivity
encoding with arbitrary k-space trajectories. Magn Reson Med, 46:638–651, 2001."
REFERENCES,0.7784810126582279,"Yuhui Quan, Mingqin Chen, Tongyao Pang, and Hui Ji. Self2self with dropout: Learning self-
supervised denoising from single image. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, June 2020."
REFERENCES,0.7848101265822784,"Philip M. Robson, Aaron K. Grant, nanth J. Madhuranthakam, Riccardo Lattanzi, Daniel K. Sodick-
son, and Charles A. McKenzie. Comprehensive quantiﬁcation of signal-to-noise ratio and g-factor
for image-based and k-space-based parallel imaging reconstructions. Magn Reson Med, 60(4):
895–907, Oct 2008."
REFERENCES,0.7911392405063291,"Christopher M Sandino, Joseph Y Cheng, Feiyu Chen, Morteza Mardani, John M Pauly, and
Shreyas S Vasanawala. Compressed sensing: From research to clinical practice with deep neu-
ral networks: Shortening scan times for magnetic resonance imaging. IEEE Signal Processing
Magazine, 37(1):117–127, 2020."
REFERENCES,0.7974683544303798,"Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony N. Price, and Daniel Rueckert. A deep
cascade of convolutional neural networks for dynamic MR image reconstruction. IEEE Trans.
Medical Imaging, 37(2):491–503, 2018."
REFERENCES,0.8037974683544303,"Ortal Senouf, Sanketh Vedula, Tomer Weiss, Alex Bronstein, Oleg Michailovich, and Michael
Zibulevsky. Self-supervised learning of inverse problem solvers in medical imaging. In Do-
main Adaptation and Representation Transfer and Medical Image Learning with Less Labels and
Imperfect Data, pp. 111–119. Springer, 2019."
REFERENCES,0.810126582278481,"Assaf Shocher, Nadav Cohen, and Michal Irani. “Zero-shot” super-resolution using deep internal
learning. In Proc IEEE CVPR, June 2018."
REFERENCES,0.8164556962025317,Published as a conference paper at ICLR 2022
REFERENCES,0.8227848101265823,"Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun
Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, Xintao Wang, Yapeng Tian, Ke Yu, Yulun
Zhang, Shixiang Wu, Chao Dong, Liang Lin, Yu Qiao, Chen Change Loy, Woong Bae, Jae Jun
Yoo, Yoseob Han, Jong Chul Ye, Jae-Seok Choi, Munchurl Kim, Yuchen Fan, Jiahui Yu, Wei
Han, Ding Liu, Haichao Yu, Zhangyang Wang, Honghui Shi, Xinchao Wang, Thomas S. Huang,
Yunjin Chen, Kai Zhang, Wangmeng Zuo, Zhimin Tang, Linkai Luo, Shaohui Li, Min Fu, Lei
Cao, Wen Heng, Giang Bui, Truc Le, Ye Duan, Dacheng Tao, Ruxin Wang, Xu Lin, Jianxin
Pang, Jinchang Xu, Yu Zhao, Xiangyu Xu, Jin-shan Pan, Deqing Sun, Yujin Zhang, Xibin Song,
Yuchao Dai, Xueying Qin, Xuan-Phung Huynh, Tiantong Guo, Hojjat Seyed Mousavi, Tiep Huu
Vu, Vishal Monga, Crist´ov˜ao Cruz, Karen O. Egiazarian, Vladimir Katkovnik, Rakesh Mehta,
Arnav Kumar Jain, Abhinav Agarwalla, Ch V. Sai Praveen, Ruofan Zhou, Hongdiao Wen, Che
Zhu, Zhiqiang Xia, Zhengtao Wang, and Qi Guo. NTIRE 2017 challenge on single image super-
resolution: Methods and results. In CVPR Workshops, pp. 1110–1121. IEEE Computer Society,
2017."
REFERENCES,0.8291139240506329,"Martin Uecker, Peng Lai, Mark J. Murphy, Patrick Virtue, Michael Elad, John Pauly, Shreyas S.
Vasanawala, and Michael Lustig. ESPIRiT–an eigenvalue approach to autocalibrating parallel
MRI: where SENSE meets GRAPPA. Magn Reson Med, 71(3):990–1001, Mar 2014."
REFERENCES,0.8354430379746836,"Kamil Ugurbil, Junqian Xu, Edward J. Auerbach, Steen Moeller, An T. Vu, Julio Martin Duarte-
Carvajalino, Christophe Lenglet, Xiaoping Wu, Sebastian Schmitter, Pierre-Franc¸ois van de
Moortele, John P. Strupp, Guillermo Sapiro, Federico De Martino, Dingxin Wang, Noam Harel,
Michael Garwood, Liyong Chen, David A. Feinberg, Stephen M. Smith, Karla L. Miller, Sta-
matios N. Sotiropoulos, Saˆad Jbabdi, Jesper L. R. Andersson, Timothy Edward John Behrens,
Matthew F. Glasser, David C. Van Essen, and Essa Yacoub. Pushing spatial and temporal resolu-
tion for functional and diffusion MRI in the human connectome project. NeuroImage, 80:80–104,
2013."
REFERENCES,0.8417721518987342,"Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proc. IEEE CVPR,
June 2018."
REFERENCES,0.8481012658227848,"Luca Vizioli, Steen Moeller, Logan Dowdle, Mehmet Akc¸akaya, Federico De Martino, Essa Yacoub,
and Kamil U˘gurbil. Lowering the thermal noise barrier in functional brain mapping with magnetic
resonance imaging. Nature communications, 12(1):1–15, 2021."
REFERENCES,0.8544303797468354,"Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta Ellermann, Kˆamil Ugur-
bil, and Mehmet Akc¸akaya. Self-supervised learning of physics-guided reconstruction neural
networks without fully-sampled reference data. Magn Reson Med, 84:3172–3191, Dec 2020."
REFERENCES,0.8607594936708861,Published as a conference paper at ICLR 2022
REFERENCES,0.8670886075949367,"A
APPENDIX"
REFERENCES,0.8734177215189873,"Figure 7: Different contrast weightings and anatomies used in this study: a) Cor-PD, b) Cor-PDFS,
c) Ax-FLAIR, d) Ax-T2, as well as undersampling patterns: e) Uniform, f) Random mask. Zero-
ﬁlled images generated by uniform and random undersampling masks have coherent and incoherent
aliasing artifacts, respectively. Coherent aliasing artifacts are generally harder to remove than inco-
herent artifacts."
REFERENCES,0.879746835443038,"Figure 8:
Cor-PD Knee MRI reconstruction results across different epochs for DIP-Recon using
uniform undersampling at R = 4. At the 25th epoch, the reconstruction suffers from artifacts, with
the zoom-in area showing texture that does not resemble the ground truth. With more epochs, this
aspect of the reconstruction improves, but the reconstruction starts to suffer from noise ampliﬁcation
as the number of epochs increases. Hence, the 50th epoch was used in the experiments."
REFERENCES,0.8860759493670886,Published as a conference paper at ICLR 2022
REFERENCES,0.8924050632911392,"Figure 9: a) and b) show reconstruction results corresponding to the loss curves in Figure 2a and b,
respectively."
REFERENCES,0.8987341772151899,"Figure 10:
Reconstruction results from R = 4 with random undersampling on representative test
slices from a) Cor-PD knee MRI and b) Ax-FLAIR brain MRI. CG-SENSE, DIP-Recon and DIP-
Recon-TL suffer from noise ampliﬁcation. Supervised PG-DLR, SSDU PG-DLR, ZS-SSL and ZS-
SSL-TL all show artifact-free reconstruction quality, with similar quantitative metrics."
REFERENCES,0.9050632911392406,"Table 2: Average reconstruction times for single-instance reconstruction methods. The computation
times were measured on the machines equipped with 4 NVIDIA V100 GPUs (each with 32 GB
memory). While CG-SENSE and DIP methods have lower computational times, their reconstruc-
tion quality is severely degraded hindering clinical usage. ZS-SSL-TL (K = 10) provides an 8-fold
faster convergence time compared to ZS-SSL (K = 10). We note that ZS-SSL methods recon-
struction times may further be reduced by means of more compact architectures. Additionally, the
increased computational times may be tolerable within the workﬂow, for instance in clinical settings
where image readings are done the next day (Basha et al., 2017), or scans such as high-resolution
functional or diffusion MRI, where it is challenging to have high-quality high-resolution data, while
post-reconstruction analyses readily take hours to days (Andersson & Stamatios N, 2016; Vizioli
et al., 2021)."
REFERENCES,0.9113924050632911,"CG-SENSE
DIP-Recon
DIP-Recon-TL
ZS-SSL
ZS-SSL-TL
Average Time (sec)
<<1
75
75
640
85"
REFERENCES,0.9177215189873418,Published as a conference paper at ICLR 2022
REFERENCES,0.9240506329113924,"Figure 11:
Test datasets may differ from the training datasets in terms of sampling pattern, SNR,
contrast and anatomy. Such differences lead to sub-optimal reconstructions in the test datasets,
raising robustness and generalizability concerns for translation of trained MRI reconstruction models
to clinical practice."
REFERENCES,0.930379746835443,"Figure 12: a) Using pre-trained Ax-Flair for Ax-T2 reconstruction. b) Using a pre-trained Cor-PD
(knee MRI) for Ax-Flair (brain MRI) reconstructions. Supervised PG-DLR fails to generalize when
contrast, SNR and anatomy changes, with residual artifacts (red arrows). DIP-Recon-TL also shows
artifacts. ZS-SSL-TL successfully removes noise and artifacts."
REFERENCES,0.9367088607594937,Published as a conference paper at ICLR 2022
REFERENCES,0.9430379746835443,"Table 3: Average PSNR and SSIM values on 30 test slices for the experiments associated with
Figures 4-6 (in the main text) and 12. We note that ZS-SSL-TL successfully ﬁne-tunes the network
for each new dataset/instance regardless of the starting pretrained model. Interestingly, there are
cases where out-of-domain transfer has slightly higher metrics than in-domain transfer. In these
cases, since the metrics are already high, the slight quantitative differences do not affect the overall
quality. However, the main difference in these cases is in the convergence/stopping time, where in-
domain transfer is typically converges/stops in ∼2-fold fewer iterations than out-of-domain transfer."
REFERENCES,0.9493670886075949,"Metrics
Supervised PG-DLR
DIP-Recon-TL
ZS-SSL-TL"
REFERENCES,0.9556962025316456,"Figure 4: Banding Artifacts
SSIM
0.873
0.530
0.861
PSNR
36.365
26.924
36.121
Figure 5a: In-Domain Transfer -
Different Mask"
REFERENCES,0.9620253164556962,"SSIM
0.949
0.836
0.951
PSNR
39.167
34.093
40.088
Figure 5b: In-Domain Transfer -
Different Rates"
REFERENCES,0.9683544303797469,"SSIM
0.937
0.792
0.940
PSNR
38.262
32.658
38.301
Figure 6a: Cross-Domain Transfer -
Knee-Different Contrast"
REFERENCES,0.9746835443037974,"SSIM
0.931
0.859
0.949
PSNR
37.566
34.855
39.855
Figure 6b: Anatomy Change -
Trained on Brain & Tested on Knee"
REFERENCES,0.9810126582278481,"SSIM
0.936
0.890
0.957
PSNR
37.494
35.458
40.407
Figure 12a: Cross-Domain Transfer -
Brain-Different Contrast"
REFERENCES,0.9873417721518988,"SSIM
0.929
0.834
0.950
PSNR
35.578
32.655
38.767
Figure 12b: Anatomy Change -
Trained on Knee & Tested on Brain"
REFERENCES,0.9936708860759493,"SSIM
0.929
0.806
0.936
PSNR
36.242
30.849
37.134"
