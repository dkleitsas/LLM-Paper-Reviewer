Section,Section Appearance Order,Paragraph
DEPARTMENT OF CHEMICAL ENGINEERING,0.0,"1Department of Chemical Engineering
2Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology, Cambridge, MA 02139, USA
{keir,lagnajit,ccoley}@mit.edu"
ABSTRACT,0.0022471910112359553,ABSTRACT
ABSTRACT,0.0044943820224719105,"Molecular chirality, a form of stereochemistry most often describing relative spa-
tial arrangements of bonded neighbors around tetrahedral carbon centers, influ-
ences the set of 3D conformers accessible to the molecule without changing its
2D graph connectivity. Chirality can strongly alter (bio)chemical interactions,
particularly protein-drug binding. Most 2D graph neural networks (GNNs) de-
signed for molecular property prediction at best use atomic labels to na¨ıvely treat
chirality, while E(3)-invariant 3D GNNs are invariant to chirality altogether. To
enable representation learning on molecules with defined stereochemistry, we de-
sign an SE(3)-invariant model that processes torsion angles of a 3D molecular
conformer. We explicitly model conformational flexibility by integrating a novel
type of invariance to rotations about internal molecular bonds into the architec-
ture, mitigating the need for multi-conformer data augmentation. We test our
model on four benchmarks: contrastive learning to distinguish conformers of dif-
ferent stereoisomers in a learned latent space, classification of chiral centers as
R/S, prediction of how enantiomers rotate circularly polarized light, and ranking
enantiomers by their docking scores in an enantiosensitive protein pocket. We
compare our model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D
and 3D GNNs to demonstrate that our model achieves state of the art performance
when learning chiral-sensitive functions from molecular structures."
INTRODUCTION,0.006741573033707865,"1
INTRODUCTION"
INTRODUCTION,0.008988764044943821,"Advances in graph neural networks (GNNs) have revolutionized molecular representation learning
for (bio)chemical applications such as high-fidelity property prediction (Huang et al., 2021; Chuang
et al., 2020), accelerated conformer generation (Ganea et al., 2021; Mansimov et al., 2019; Simm
& Hernandez-Lobato, 2020; Xu et al., 2021; Pattanaik et al., 2020b), and molecular optimization
(Elton et al., 2019; Brown et al., 2019). Fueling recent developments have been efforts to model
shape-dependent physio-chemical properties by learning directly from molecular conformers (snap-
shots of 3D molecular structures) or from 4D conformer ensembles, which better capture molecular
flexibility. For instance, recent state-of-the-art (SOTA) GNNs feature message updates informed by
bond distances, bond angles, and torsion angles of the conformer (Sch¨utt et al., 2017; Klicpera et al.,
2020b; Liu et al., 2021; Klicpera et al., 2021). However, few studies have considered the expressiv-
ity of GNNs when tasked with learning the nuanced effects of stereochemistry, which describes how
the relative arrangement of atoms in space differ for molecules with equivalent graph connectivity."
INTRODUCTION,0.011235955056179775,"Tetrahedral (point) chirality is a prevalent form of stereochemistry, and describes the spatial ar-
rangement of chemical substituents around the vertices of a tetrahedron centered on a chiral center,
typically a carbon atom with four non-equivalent bonded neighbors. Two molecules differing only
in the relative arrangements around their chiral centers are called stereoisomers, or enantiomers if
they can be interconverted through reflection across a plane. Enantiomers are distinguished in chem-
ical line drawings by a dashed or bold wedge indicating whether a bond to a chiral center is directed
into or out of the page (Figure 1A). Although enantiomers share many properties such as boiling
points, electronic energies, and solubility in most solvents, enantiomers can display strikingly dif-
ferent behavior when interacting with external chiral environments. Notably, chirality is critical for"
INTRODUCTION,0.01348314606741573,Published as a conference paper at ICLR 2022
INTRODUCTION,0.015730337078651686,"Conformers
(bond rotation)"
INTRODUCTION,0.017977528089887642,"O
O
Enantiomers
(mirror images)"
INTRODUCTION,0.020224719101123594,"SE(3)-Invariance
(SE(3) + InterRoto)-Invariance
E(3)-Invariance
2D Achiral Graph A B"
INTRODUCTION,0.02247191011235955,"Separates
Enantiomers?"
INTRODUCTION,0.024719101123595506,"InterRoto
Invariant?"
INTRODUCTION,0.02696629213483146,"Conformers
(bond rotation)"
INTRODUCTION,0.029213483146067417,"Figure 1: Schematic of how different conformers of two enantiomers (panel A) separate in a learned
latent space (panel B) under a 2D (achiral) graph model, E(3)- and SE(3)-invariant 3D models, and
ChIRo, which is invariant to rotations of internal molecular bonds (InterRoto-invariance)."
INTRODUCTION,0.03146067415730337,"drug design (Nguyen et al., 2006; Jamali et al., 1989), where protein-ligand interactions are influ-
enced by ligand chirality, as well as for designing structure-directing agents for zeolite growth (Luis
& Beatriz, 2018) and for optimizing asymmetric catalysts (Pfaltz & Drury, 2004; Liao et al., 2018)."
INTRODUCTION,0.033707865168539325,"Chiral centers are inverted upon reflection through a mirror plane. Consequently, E(3)-invariant 3D
GNNs that only consider pairwise atomic distances or bond angles in their message updates, such
as SchNet (Sch¨utt et al., 2017) and DimeNet/DimeNet++ (Klicpera et al., 2020a;b), are inherently
limited in their ability to distinguish enantiomers (Figure 1B). Although SE(3)-invariant 3D GNNs,
such as the recently proposed SphereNet (Liu et al., 2021) and GemNet (Klicpera et al., 2021), can
in theory learn chirality, their expressivity in this setting has not been explored."
INTRODUCTION,0.035955056179775284,"Alongside the development of 3D GNNs, which process individual 3D conformers, there have been
efforts to better represent conformational flexibility by encoding multiple conformers in a 4D en-
semble for property prediction (Zankov et al., 2021; 2019; Axelrod & Gomez-Bombarelli, 2021),
identifying important conformer poses (Chuang & Keiser, 2020), and predicting solvation energies
(Weinreich et al., 2021). Unless in the solid state, molecules are not rigid objects or static 2D graphs,
but are flexible structures that rapidly interconvert through rotations of chemical bonds as well as
through smaller perturbations such as bond stretching, bending, and wagging. Explicitly modeling
this probability distribution over accessible conformer space has the potential to improve the mod-
eling of protein-drug interactions, where the most relevant active pose of the ligand is not known a
priori, as well as in the prediction of Boltzmann-averages, which depend on a distribution of con-
formers. One challenge with these methods is selecting which conformers to include in an ensemble:
the space of accessible conformations combinatorially explodes with the number of rotatable bonds,
and important poses are not known a priori. Modeling flexibility with multi-instance methods thus
requires explicit conformer enumeration, increasing the cost of training/inference without guaran-
teeing performance gains. Apart from 2D methods, which ignore 3D information altogether, no
studies have explicitly modeled conformational flexibility directly within a model architecture."
INTRODUCTION,0.038202247191011236,"To explicitly model tetrahedral chirality and conformational flexibility, we design a neural frame-
work to augment 2D GNNs with processing of the SE(3)-invariant internal coordinates of a con-
former, namely bond distances, bond angles, and torsion angles. Our specific contributions are:"
INTRODUCTION,0.04044943820224719,"• We design a method for graph neural networks to learn the relative orientations of sub-
stituents around tetrahedral chiral centers directly from 3D torsion angles
• We introduce a novel invariance to internal rotations of rotatable bonds directly into a model
architecture, potentially mitigating the need for 4D ensemble methods or conformer-based
data augmentation to treat conformational flexibility of molecules
• We propose a contrastive learning framework to probe the ability of SE(3)-invariant 3D
graph neural networks to differentiate stereoisomers in a learned latent space"
INTRODUCTION,0.04269662921348315,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0449438202247191,"• Through our ablation study, we demonstrate that a global node aggregation scheme,
adapted from Winter et al. (2021), which exploits subgraphs based on internal coordinate
connectivity can provide a simple way to improve GNNs for chiral property prediction"
INTRODUCTION,0.04719101123595506,"We explore multiple tasks to benchmark the ability of our model to learn the effects of chirality.
We do not consider common MoleculeNet (Wu et al., 2018) benchmarks, as our focus is on tasks
where the effects of chirality are more distinguishable from experimental noise. Our self-supervised
contrastive learning task is the first of its kind applied to clustering multiple 3D conformers of dif-
ferent stereoisomers in a latent space. Following Pattanaik et al. (2020a), we also employ a toy R/S
labeling task as a necessary but not sufficient test of chiral recognition. For a harder classification
task, we follow Mamede et al. (2021) in predicting how enantiomers experimentally rotate circularly
polarized light. Lastly, we create a dataset of simulated docking scores to rank small enantiomeric
ligands by their binding affinities in a chirality-sensitive protein pocket. We make our datasets for
the contrastive learning, R/S classification, and docking tasks available to the public. Comparisons
with 2D baselines and the SE(3)-invariant SphereNet demonstrate that our model, Chiral InterRoto-
Invariant Neural Network (ChIRo), achieves SOTA in 3D chiral molecular representation learning."
RELATED WORK,0.04943820224719101,"2
RELATED WORK"
RELATED WORK,0.051685393258426963,"Message passing neural networks. Gilmer et al. (2017) introduced a framework for using GNNs to
embed molecules into a continuous latent space for property prediction. In the typical 2D message
passing scheme, a molecule is modeled as a discrete 2D graph G with atoms as nodes and bonds as
edges. Nodes and edges are initialized with features xi and eij to embed initial node states:"
RELATED WORK,0.05393258426966292,"h0
i = U0(xi, {eij}j∈N(i))
(1)"
RELATED WORK,0.056179775280898875,"where N(i) denotes the neighboring atoms of node i. In each layer t of message passing, node
states are updated with aggregated messages from neighboring nodes. After T layers, graph feature
vectors are constructed from some (potentially learnable) aggregation over the learned node states.
A readout phase then uses this graph embedding for downstream property prediction:"
RELATED WORK,0.058426966292134834,"ht+1
i
= Ut(ht
i, mt+1
i
),
mt+1
i
=
X"
RELATED WORK,0.060674157303370786,"j∈N(i)
Mt(ht
i, ht
j, eij)
(2)"
RELATED WORK,0.06292134831460675,"ˆy = Readout(g), g = Agg({hT
i |i ∈G})
(3)
There exist many variations on this basic message passing framework (Duvenaud et al., 2015;
Kearnes et al., 2016). In particular, Yang et al. (2019)’s directed message passing neural network
(DMPNN, or ChemProp) based on Dai et al. (2016) learns edge-based messages mt
ij and updates
edge embeddings ht
ij rather than node embeddings. The Graph Attention Network (Veliˇckovi´c et al.,
2018) constructs message updates mt
i using attention pooling over local node states."
RELATED WORK,0.0651685393258427,"3D Message Passing and Euclidean Invariances.
3D GNNs differ in their message passing
schemes by using molecular internal coordinates (distances, angles, torsions) to pass geometry-
informed messages between nodes. It is important to use 3D information that is at least SE(3)-
invariant, as molecular properties are invariant to global rotations or translations of the conformer."
RELATED WORK,0.06741573033707865,"SchNet (Sch¨utt et al., 2017), a well-established network for learning quantum mechanical properties
of 3D conformers, updates node states using messages informed by radial basis function expansions
of interatomic distances between neighboring nodes. DimeNet (Klicpera et al., 2020b) and its newer
variant DimeNet++ (Klicpera et al., 2020a) exploit additional molecular geometry by using spherical
Bessel functions to embed angles ϕijk between the edges formed between nodes i and j and nodes
j and k ̸= i in the directed message updates to node i."
RELATED WORK,0.0696629213483146,"SchNet and DimeNet are E(3)-invariant, as pairwise distances and angles formed between two edges
are unchanged upon global rotations, translations, and reflections. Since enantiomers are mirror
images, SchNet and DimeNet are therefore invariant to this form of chirality. To be SE(3)-invariant,
3D GNNs must consider torsion angles, denoted ψixyj, between the planes defined by angles ϕixy
and ϕxyj, where i, x, y, j are four sequential nodes along a simple path. Torsion angles are negated
upon reflection, and thus models considering all torsion angles should be implicitly sensitive to
chirality. Flam-Shepherd et al. (2021), Liu et al. (2021) (SphereNet), and Klicpera et al. (2021)
(GemNet) introduce 3D GNNs that all embed torsions in their message updates."
RELATED WORK,0.07191011235955057,Published as a conference paper at ICLR 2022
RELATED WORK,0.07415730337078652,"Using a complete set of torsion angles provides access to the full geometric information present
in the conformer but does not guarantee expressivity when learning chiral-dependent functions.
Torsions are negated upon reflection, but any given torsion can also be changed via simple rotations
of a rotatable bond–which changes the conformation, but not the molecular identity (i.e., chirality
does not change). Reflecting a non-chiral conformer will also negate its torsions, but the reflected
conformer can be reverted to its original structure via rotations about internal bonds. To model
chirality, neural networks must learn how coupled torsions, the set of torsions {ψixyj}(i,j) that share
a bond between nodes x and y (with x or y being chiral), collectively differ between enantiomers."
RELATED WORK,0.07640449438202247,"E(3)- and SE(3)-Equivariant Neural Networks. Recent work has introduced equivariant layers
into graph neural network architectures to explicitly model how global rotations, translations, and
(in some cases) reflections of a 3D structure transform tensor properties, such as molecular dipoles
or force vectors. SE(3)-equivariant models (Fuchs et al., 2020; Thomas et al., 2018) should be
sensitive to chirality, while E(3)-equivariant models (Satorras et al., 2021) will only be sensitive if
the output layer is not E(3)-invariant. Since we use SE(3)-invariant internal coordinates as our 3D
representation, we only compare our model to other SE(3)- or E(3)-invariant 3D GNNs."
RELATED WORK,0.07865168539325842,"Explicit representations of chirality in machine learning models. A number of machine learning
studies account for chirality through hand-crafted molecular descriptors (Schneider et al., 2018;
Golbraikh et al., 2001; Kovatcheva et al., 2007; Vald´es-Martin´ı et al., 2017; Mamede et al., 2021).
A na¨ıve but common method for making 2D GNNs sensitive to chirality is through the inclusion of
chiral tags as node features. Local chiral tags describe the orientation of substituents around chiral
centers (CW or CCW) given an ordered list of neighbors. Global chiral tags use the Cahn-Ingold-
Prelog (CIP) rules for labeling the handedness of chiral centers as R (“rectus”) or S (“sinister”). It
is unclear whether (and how) models can suitably learn chiral-dependent functions when exposed to
these tags as the only indication of chirality. Pattanaik et al. (2020a) propose changing the symmetric
message aggregation function in 2D GNNs (sum/max/mean) to an asymmetric function tailored to
tetrahedral chiral centers, but this method does not learn chirality from 3D molecular geometries."
RELATED WORK,0.08089887640449438,"Chirality in 2D Vision and 3D Pose Estimation. Outside of molecular chirality, there has been
work in the deep learning community to develop neural methods that learn chiral representations
for 2D image recognition (Lin et al., 2020) and 3D human pose estimation (Yeh et al., 2019). In
particular, Yeh et al. (2019) consider integrating equivariance to chiral transforms directly into neural
architectures including feed forward layers, LSTMs/GRUs, and convolutional layers."
RELATED WORK,0.08314606741573034,"3
CHIRAL INTERROTO-INVARIANT NEURAL NETWORK (CHIRO)"
RELATED WORK,0.0853932584269663,"Our model uses a 2D GNN to embed node states based on graph connectivity, two feed-forward
networks based on Winter et al. (2021) to encode 3D bond distances and bond angles, a specially de-
signed torsion encoder inspired by Ganea et al. (2021) to explicitly encode chirality with invariance
to internal bond rotations (InterRoto-invariance), and a readout phase for property prediction. Figure
2 visualizes how we encode torsion angles to achieve an InterRoto-invariant representation of chi-
rality. The model is visualized in appendix A.4. We implement our network with Pytorch Geometric
(Fey & Lenssen, 2019). Our code is available at https://github.com/keiradams/ChIRo."
RELATED WORK,0.08764044943820225,"2D Graph Encoder. Given a molecular conformer, we initialize a 2D graph G where nodes are
atoms initialized with features xi and edges are bonds initialized with features eij. We treat all
hydrogens implicitly, use undirected edges, and omit chiral atom tags and bond stereochemistry tags.
xi include the atomic mass and one-hot encodings of atom type, formal charge, degree, number of
hydrogens, and hybridization state. eij include one-hot encodings of the bond type, whether the
bond is conjugated, and whether the bond is in a ring system. In select experiments, we include
global and local chiral atom tags and bond stereochemistry tags for performance comparisons."
RELATED WORK,0.0898876404494382,"Any 2D GNN suffices to embed node states. Following Winter et al. (2021), we use Simonovsky &
Komodakis (2017)’s edge convolution (EConv) to embed the node features with the edge features:"
RELATED WORK,0.09213483146067415,"h0
i = Θxi +
X"
RELATED WORK,0.09438202247191012,"j∈N(i)
xj · fe(eij)
(4)"
RELATED WORK,0.09662921348314607,"where Θ is a learnable weight matrix and fe is a multi-layer perceptron (MLP). Node states ht
i ∈
Rht are then updated through T sequential Graph Attention Layers (GAT) (Veliˇckovi´c et al., 2018):"
RELATED WORK,0.09887640449438202,"ht
i = GAT(t)(ht−1
i
, {ht−1
j
}j∈N(i)), t = 1, ..., T
(5)"
RELATED WORK,0.10112359550561797,"Published as a conference paper at ICLR 2022 A C
B R A B
C R A C B H H H H A C B R R r r"
RELATED WORK,0.10337078651685393,"Figure 2: Geometric visualization of how ChIRo learns chiral representations with InterRoto-
invariance. Given two enantiomers with three coupled torsions ψa, ψb, and ψc involving a chiral
carbon center, rotating the shared carbon-carbon bond by R ∈[0, 2π) rotates each torsion together,
forming three periodic sinusoids (solid RGB curves). Learning phase shifts φi for each torsion
produces shifted waves (dashed RGB curves) that differ between enantiomers. A weighted sum of
the shifted sinusoids results in different degrees of wave interference between enantiomers, yielding
different amplitudes of the net waves. These amplitudes, visualized here as the different radii (r)
of the circles formed by plotting the summed sines against the summed cosines, are invariant to the
rotation of the internal carbon-carbon bond. See appendices A.1, A.2 for further details."
RELATED WORK,0.10561797752808989,"Bond Distance and Bond Angle Encoders. To embed 3D bond distance and bond angle informa-
tion, we follow Winter et al. (2021) by individually encoding each bond distance dij (in Angstroms)
and angle ϕijk into learned latent vectors. We then sum-pool these vectors to get conformer-level
latent embeddings zd ∈Rz and zϕ ∈Rz:"
RELATED WORK,0.10786516853932585,"zd =
X"
RELATED WORK,0.1101123595505618,"(i,j)∈G
fd(hi, hj, dij) + fd(hj, hi, dij)
(6)"
RELATED WORK,0.11235955056179775,"zϕ =
X"
RELATED WORK,0.1146067415730337,"(i,j,k)∈G
fϕ(hi, hj, hk, cos ϕijk, sin ϕijk) + fϕ(hk, hj, hi, cos ϕijk, sin ϕijk)
(7)"
RELATED WORK,0.11685393258426967,"where fd and fϕ are MLPs and (., .) denotes concatenation. We maintain invariance to node ordering
by encoding both (i, j) and (j, i) for each distance dij, and both (i, j, k) and (k, j, i) for each angle
ϕijk. For experiments that mask all internal coordinates (i.e., set all bond lengths and angles to 0),
zd and zϕ represent aggregations of 2D node states based on subgraphs of local atomic connectivity."
RELATED WORK,0.11910112359550562,"Torsion Encoder. We specially encode torsion angles to achieve two desired properties: 1) an in-
variance to rotations about internal molecular bonds, and 2) the ability to learn molecular chirality.
Both of these properties depend critically on how the model treats coupled torsion angles–torsions
that cannot be independently rotated in the molecular conformer. For instance, torsions ψa, ψb, and
ψc in Figure 2 are coupled, as rotating the internal carbon-carbon bond rotates each torsion simulta-
neously. Symmetrically aggregating every redundant torsion does not respect this inherent coupling.
However, encoding a set of non-redundant torsions–a minimal set that completely define the struc-
ture of the conformer–would make the model sensitive to which arbitrary subset was selected."
RELATED WORK,0.12134831460674157,"Ganea et al. (2021) provide a partial solution to this problem in their molecular conformer generator,
which we add to below. Rather than encoding each torsion individually, they compute a weighted
sum of the cosines and sines of each coupled torsion (which are redundant), where a neural network
predicts the weights1. Formally, given a non-terminal (internal) bond between nodes x and y that
yields a set of coupled torsions {ψixyj}(i,j) for i ∈N(x) \ {y} and j ∈N(y) \ {x}, they compute:"
RELATED WORK,0.12359550561797752,"(α(xy)
cos , α(xy)
sin ) =
X"
RELATED WORK,0.1258426966292135,i∈N(x)\{y} X
RELATED WORK,0.12808988764044943,"j∈N(y)\{x}
c(xy)
ij
·

cos(ψixyj), sin(ψixyj)

(8)"
RELATED WORK,0.1303370786516854,"1Ganea et al. (2021) use an untrained network with randomized parameters to predict the weighting coeffi-
cients. Instead, we learn the parameters in fc and compare these approaches in Appendix A.8."
RELATED WORK,0.13258426966292136,Published as a conference paper at ICLR 2022
RELATED WORK,0.1348314606741573,"c(xy)
ij
= σ

fc(hi, hx, hy, hj) + fc(hj, hy, hx, hi)

(9)"
RELATED WORK,0.13707865168539327,"where fc is an MLP that maps to R. The sigmoid activation σ, which keeps each c(xy)
ij
bounded to
(0, 1), is our addition. The following formulation is our novel addition to Ganea et al. (2021)."
RELATED WORK,0.1393258426966292,"Because
rotating
the
bond
(x, y)
rotates
the
torsions
{ψixyj}
together,
the
sinusoids
{sin(ψixyj)}(i,j) and {cos(ψixyj)}(i,j) have the same frequency. Therefore, the weighted sums
of these cosines and sines are also sinusoids, which when plotted against each other as a function
of rotating the bond (x, y), form a perfect circle (appendix A.1). Critically, the radius of this circle,
||α(xy)
cos , α(xy)
sin ||, is invariant to the rotation of the bond (x, y) (Figure 2). We therefore encode these
radii in order to achieve invariance to how any bond in the conformer is rotated."
RELATED WORK,0.14157303370786517,"The above formulation, as presented, is also invariant to chirality (appendix A.2). To break this
symmetry, we add a learned phase shift, φixyj, to each torsion ψixyj:"
RELATED WORK,0.14382022471910114,"(cos φixyj, sin φixyj) = ℓ2
norm

fφ(hi, hx, hy, hj) + fφ(hj, hy, hx, hi)

(10)"
RELATED WORK,0.14606741573033707,"(α(xy)
cos , α(xy)
sin ) =
X"
RELATED WORK,0.14831460674157304,i∈N(x)\{y} X
RELATED WORK,0.15056179775280898,"j∈N(y)\{x}
c(xy)
ij
·

cos(ψixyj + φixyj), sin(ψixyj + φixyj)

(11)"
RELATED WORK,0.15280898876404495,"where fφ is an MLP that maps to R2 and ℓ2
norm indicates L2-normalization. Because enantiomers
have the same 2D graph, the learned coefficients c(xy)
ij
and phase shifts φixyj are identical between
enantiomers (if xi is initialized without chiral tags). However, because enantiomers have different
spatial orientations of atoms around chiral centers, the relative values of coupled torsions around
bonds involving those chiral centers also differ (Figure 2). As a result, learning phase shifts creates
different degrees of wave-interference when summing the weighted cosines and sines for inverted
chiral centers. The amplitudes of the net waves (corresponding to radius ||α(xy)
cos , α(xy)
sin ||) will differ
between enantiomers, allowing our model to encode different radii for different enantiomers."
RELATED WORK,0.1550561797752809,"With this torsion aggregation scheme, we complete our torsion encoder by individually encoding
each internal molecular bond, along with its learned radius, into a latent vector and sum-pooling:"
RELATED WORK,0.15730337078651685,"zα =
X"
RELATED WORK,0.15955056179775282,"(x,y)∈G
zαxy,
zαxy = fα"
RELATED WORK,0.16179775280898875,"
hx, hy,
α(xy)
cos , α(xy)
sin


+ fα"
RELATED WORK,0.16404494382022472,"
hy, hx,
α(xy)
cos , α(xy)
sin


(12)"
RELATED WORK,0.1662921348314607,"Readout. For property prediction, we concatenate the sum-pooled node states with the conformer
embedding components zd, zϕ, and zα, capturing embeddings of bond lengths, angles, and torsions,
respectively. This concatenated representation is then fed through a feed-forward neural network:"
RELATED WORK,0.16853932584269662,ˆy = fout X
RELATED WORK,0.1707865168539326,"i∈G
hi, zd, zϕ, zα ! (13)"
RELATED WORK,0.17303370786516853,"Appendix A.3 explores the option of propagating the learned 3D representations of chirality to node
states prior to sum-pooling via additional message passing, treating each zαxy as a vector of edge-
attributes. This stage of chiral message passing (CMP) is designed to help propagate local chiral
representations across the molecular graph, and to provide an alternative method of augmenting
2D GNNs which include chiral tags as atom features. However, CMP does not significantly affect
ChIRo’s performance across the tasks considered and is thus not included in our default model."
EXPERIMENTS,0.1752808988764045,"4
EXPERIMENTS"
EXPERIMENTS,0.17752808988764046,"We evaluate the ability of our model to learn chirality with four distinct tasks: contrastive learning to
distinguish between 3D conformers of different stereoisomers in a learned latent space, classifying
enantiomer chiral centers as R/S, predicting how enantiomers rotate circularly polarized light, and
ranking enantiomers by their docking scores in a chiral protein environment. We compare our model
with 2D baselines including DMPNN with chiral atom tags (Yang et al., 2019), and Tetra-DMPNN
with and without chiral atom tags (Pattanaik et al., 2020a). We also compare our model to 3D GNN
baselines, including the E(3)-invariant SchNet and DimeNet++, and the SE(3)-invariant SphereNet."
EXPERIMENTS,0.1797752808988764,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.18202247191011237,"Figure 3: Visualization of how conformers of two enantiomers in the contrastive learning test set
cluster in the learned latent space for ChIRo, SchNet, DimeNet++, and SphereNet (top row) using
the original OMEGA-generated conformers; (middle row) upon reflecting the conformers, which
adds points with inverted chirality (opposite color); and (bottom row) upon rotating internal bonds
involving the chiral center. Unlike SchNet, DimeNet++, and even the SE(3)-invariant SphereNet,
ChIRo maintains perfect separation between stereoisomers across these conformer transformations."
EXPERIMENTS,0.1842696629213483,"Contrastive Learning to Distinguish Stereoisomers. For contrastive learning, we use a subset of
the PubChem3D dataset, which consists of multiple OMEGA-generated conformations of organic
molecules with up to 50 heavy atoms and 15 rotatable bonds (Bolton et al., 2011; Hawkins et al.,
2010). Our subset consists of 2.98M conformers of 598K stereoisomers of 257K 2D graphs. Each
stereoisomer has at least two conformers, and each graph has at least two stereoisomers. We create
70/15/15 training/validation/test splits, keeping conformers corresponding to the same 2D graphs in
the same data partition. See appendix A.5 for full training details."
EXPERIMENTS,0.18651685393258427,"We formulate this task to answer the following question: can the model learn to cluster conformers
sharing the same (chiral) molecular identity in a learned latent space, while distinguishing clusters
belonging to different stereoisomers of a shared 2D graph? We train each model with a triplet margin
loss (Balntas et al., 2016) with a normalized Euclidean distance metric:"
EXPERIMENTS,0.18876404494382024,"Ltriplet = max(0, d(za, zp) −d(za, zn) + 1), d(z1, z2) =

z1
∥z1∥−
z2
∥z2∥ (14)"
EXPERIMENTS,0.19101123595505617,"where za, zp, zn are learned latent vectors of anchor, positive, and negative examples. For each
triplet, we randomly sample a conformer a of stereoisomer i as the anchor, a conformer p ̸= a of
stereoisomer i as the positive, and a conformer n of stereoisomer j ̸= i as the negative, where i and
j share the same 2D graph. For ChIRo, we use z = zα. For SchNet, DimeNet++, and SphereNet,
we use aggregations of node states (out channels) as z. We set z ∈R2 to visualize the latent space."
EXPERIMENTS,0.19325842696629214,"Figure 3 visualizes how conformers of two enantiomers in the test set separate in the latent space
for ChIRo, SchNet, DimeNet++, and SphereNet. We explore how this separation is affected upon
reflecting the conformers (which inverts chirality) and rotating bonds around the chiral center. By
design, ChIRo maintains perfect separation across these transforms. While SchNet and DimeNet++
may seem to superficially separate the original conformers, the separation collapses upon reflection
and rotation due to their E(3)-invariance. SphereNet learns good separation that persists through
reflection, but the clusters overlap upon rotation of internal bonds. This emphasizes that 3D GNNs
have limited chiral expressivity when not explicitly considering coupled torsions in message updates."
EXPERIMENTS,0.19550561797752808,"Classifying Tetrahedral Chiral Centers as R/S. As a toy test of chiral perception for SchNet,
DimeNet++, SphereNet and ChIRo, we test whether these 3D models can classify tetrahedral chiral
centers as R/S, a simple indication of chirality that can be easily encoded into the initial node fea-
tures. We use a subset of PubChem3D containing 466K conformers of 78K enantiomers with one
tetrahedral chiral center. We create 70/15/15 train/validation/test splits, keeping conformers corre-
sponding to the same 2D graphs in the same partition. We train with a cross-entropy loss and sample
one conformer per enantiomer in each batch. See Appendix A.5 for full training details."
EXPERIMENTS,0.19775280898876405,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.2,"Table 1: R / S classification accuracy on
the test set for ChIRo and 3D GNN base-
lines. Mean and standard deviations are
reported across three trials."
EXPERIMENTS,0.20224719101123595,"Model
R / S Accuracy (%) ↑"
EXPERIMENTS,0.20449438202247192,"ChIRo
98.5 ± 0.2
SchNet
54.5 ± 0.2
DimeNet++
65.7 ± 2.9
SphereNet
98.2 ± 0.2"
EXPERIMENTS,0.20674157303370785,"Table 1 reports classification accuracies when evaluat-
ing on all conformers in the test set, without conformer-
based averaging.
The SE(3)-invariant ChIRo and
SphereNet both surpass 98% accuracy, whereas the
E(3)-invariant SchNet and DimeNet++ fail to learn this
simplest indication of chirality. We emphasize that this
classification task, which RDKit trivially solves, is nec-
essary but not sufficient to demonstrate that a model can
meaningfully learn chiral representations of molecules."
EXPERIMENTS,0.20898876404494382,"Predicting Enantiomers’ Signs of Optical Rotation.
Enantiomers rotate circularly polarized light in differ-
ent directions, and the sign of rotation designates the
enantiomer as l (levorotatory, −) or d (dextrorotatory, +). Optical activity is an experimental prop-
erty, and has no correlation with R/S classifications (Table 15, appendix A.7.2). Following Mamede
et al. (2021), who used hand-crafted descriptors to predict l / d labels, we extract 30K enantiomers
(15K pairs) with their optical activity in a chloroform solvent from the commercial Reaxys database
(Reaxys, Accessed Aug. 6-9, 2021), and generate 5 conformers per enantiomer using the ETKDG
algorithm in RDKit (Landrum, 2010). Appendix A.7 describes the filtering procedure. We eval-
uate with 5-fold cross validation, where each pair of enantiomers are randomly assigned to one
of the five folds for testing. In each fold, we randomly split the remaining dataset into 87.5/12.5
training/validation splits, assigning enantiomers to the same data partition. We train with a cross-
entropy loss. To characterize how conformer data augmentation affects the performance of ChIRo
and SphereNet, we employ two training schemes: the first randomly samples a conformer per enan-
tiomer in each batch; the second uses one pre-sampled conformer per enantiomer. In both schemes,
we evaluate on all conformers in the test splits. Appendix A.5 specifies full training details."
EXPERIMENTS,0.21123595505617979,"Table 2 reports the classification accuracies for each model. Notably, ChIRo outperforms SphereNet
by a significant 14%. Also, whereas ChIRo’s performance does not suffer when only one conformer
per enantiomer is used, SphereNet’s performance drops by 10%. This may be due to SphereNet
confusing differences in conformational structure versus chiral identities when evaluating pairs of
enantiomers (appendix A.10). The consistent performance of ChIRo emphasizes its inherent model-
ing of conformational flexibility without need for data augmentation. ChIRo also offers significant
improvement over DMPNN, demonstrating that our torsion encoder yields a more expressive repre-
sentation of chirality than simply including chiral tags in node features. The (smaller) improvements
over Tetra-DMPNN, which was specially designed to treat tetrahedral chirality in 2D GNNs, suggest
that using 3D information to encode chirality is more powerful than 2D representations of chirality."
EXPERIMENTS,0.21348314606741572,"Ranking Enantiomers by Binding Affinity in a Chiral Protein Pocket. In silico docking sim-
ulations are widely used to screen drug-ligand interactions, especially in high-throughput virtual
screens. However, docking scores can be highly sensitive to the conformation of the docked lig-"
EXPERIMENTS,0.2157303370786517,"Table 2: Comparisons of ChIRo with baselines when classifying enantiomers as l / d and ranking
enantiomers by their docking scores. Mean accuracies and standard deviations are reported on the
test sets across 5 folds (l / d) or three trials (enantiomer ranking)."
EXPERIMENTS,0.21797752808988763,"Model
Accuracy (%) ↑"
EXPERIMENTS,0.2202247191011236,"l / d
Enantiomer Ranking"
EXPERIMENTS,0.22247191011235956,"ChIRo
79.3 ± 0.4
72.0 ± 0.5
ChIRo (1-Conformer)
79.1 ± 0.5
71.9 ± 0.3
SphereNet
65.5 ± 2.4
68.6 ± 0.3
SphereNet (1-Conformer)
55.2 ± 0.3
63.0 ± 0.1
DMPNN with Chiral Tags
74.4 ± 0.8
65.9 ± 0.6
Tetra-DMPNN (permute)
70.2 ± 0.7
66.1 ± 0.3
Tetra-DMPNN (concatenate)
72.6 ± 0.7
68.5 ± 0.3
Tetra-DMPNN (permute) with Chiral Tags
75.6 ± 1.3
67.6 ± 0.6
Tetra-DMPNN (concatenate) with Chiral Tags
76.5 ± 0.8
70.1 ± 0.5"
EXPERIMENTS,0.2247191011235955,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.22696629213483147,"Table 3: Effects of ablating components of ChIRo on test-set accuracies for the l / d classification
and enantiomer docking score ranking tasks. Mean and standard deviations are reported across 5
folds (l / d) or 3 repeated trials (enantiomer ranking). The first row indicates the original ChIRo."
EXPERIMENTS,0.2292134831460674,"Model Components
Accuracy (%) ↑"
EXPERIMENTS,0.23146067415730337,"Tags
(dij, ϕijk, ψixyj)
(zd, zϕ, zα)
l / d
Enantiomer Ranking"
EXPERIMENTS,0.23370786516853934,"X
✓
✓
79.3 ± 0.4
72.0 ± 0.5
✓
✓
✓
77.7 ± 0.8
71.3 ± 0.7
✓
X
✓
76.2 ± 0.6
68.4 ± 0.9
✓
X
X
70.0 ± 0.6
63.7 ± 0.2
X
X
✓
50.0 ± 0.0
49.6 ± 0.3"
EXPERIMENTS,0.23595505617977527,"and, which creates a noisy background from which to extract meaningful differences in docking
scores between enantiomers. To partially control for this stochasticity, we source conformers of
200K pairs of small (MW ≤225, # of rotatable bonds ≤5) enantiomers with only 1 chiral center
from PubChem3D (Bolton et al., 2011). We use AutoDock Vina (Trott & Olson, 2010) to dock
each enantiomer in a small docking box (PDB ID: 4JBV) three times, and select pairs for which
both enantiomers have a range of docking scores ≤0.1 kcal/mol across the three trials. Finally, we
select pairs of enantiomers whose difference in (best) scores is ≥0.3 kcal/mol to form a dataset
of enantiosensitive docking scores. Each conformer for an enantiomer is assigned the same (best)
score. This treats docking score as a stereoisomer-level property. Our final dataset includes 335K
conformers of 69K enantiomers (34.5K pairs), which we split into 70/15/15 training/validation/test
splits, keeping enantiomers in the same data partition. We train with a mean squared error loss and
either randomly sample a conformer per enantiomer in each batch or use one pre-selected conformer
per enantiomer. Appendix A.5 specifies full training details. Note that we evaluate the ranking ac-
curacy of the predicted conformer-averaged docking scores between enantiomers in the test set."
EXPERIMENTS,0.23820224719101124,"ChIRo outperforms SphereNet in ranking enantiomer docking scores, achieving an accuracy of 72%
(Table 2). This performance is fairly consistent when evaluating ChIRo on subsets of enantiomers
which have various differences in their ground-truth scores (Figure 12, appendix A.9). SphereNet
once again suffers a drop in performance without conformer data augmentation, whereas ChIRo’s
performance remains high without such augmentation. ChIRo also outperforms the 2D baselines,
with performance gains similar to those in the l / d classification task."
EXPERIMENTS,0.24044943820224718,"Ablation Study. To investigate how the components of ChIRo contribute to its performance on the l
/ d classification and ranking tasks, we ablate various components including whether stereochemical
tags are included in node/edge intialization, whether internal coordinates are masked-out (set to
zero), and whether the conformer latent vectors zd, zϕ, and zα are used in readout (Table 3). Overall,
when internal coordinates are masked, including (zd, zϕ, zα) in readout improves performance by
∼5% on both tasks. This suggests that if chiral tags are used as the only indication of chirality
(i.e., 3D geometry is excluded), using the learned node aggregation scheme provides a simple way
to improve the ability of 2D GNNs to learn chiral functions. Notably, adding chiral tags to the
unablated ChIRo does not improve performance, which is expected given ChIRo’s improvements
over 2D baselines (Table 2). Omitting chiral tags and 3D geometry yields an achiral 2D GNN."
CONCLUSION,0.24269662921348314,"5
CONCLUSION"
CONCLUSION,0.2449438202247191,"In this work we design a method for improving representations of molecular stereochemistry in
graph neural networks through encoding 3D bond distances, angles, and especially torsions. Our
torsion encoder learns expressive representations of tetrahedral chirality by processing coupled tor-
sion angles that share a common bond while simultaneously achieving invariance to rotations about
internal molecular bonds, diminishing the need for conformer-based data augmentation to cap-
ture conformational flexibility. Comparisons to the E(3)-invariant SchNet and DimeNet++ and the
SE(3)-invariant SphereNet on a variety of tasks (one self-supervised, three supervised) dependent on
molecular chirality demonstrate that ChIRo achieves state-of-the-art in learning chiral representa-
tions from 3D molecular structures, while also outperforming 2D GNNs. We leave consideration of
other types of stereoisomerism, particularly cis/trans isomerism and atropisomerism, to future work."
CONCLUSION,0.24719101123595505,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.24943820224719102,ETHICS STATEMENT
ETHICS STATEMENT,0.251685393258427,"Advancing representations of molecular chirality and conformational flexibility has the potential
to accelerate pharmaceutical drug design, renewable energy development, and progress towards
energy-efficient and waste-reducing catalysts, among other areas of scientific research. However,
in principle, the same advances could also be used for harmful biological, chemical, or materials
research and applications thereof."
REPRODUCIBILITY STATEMENT,0.2539325842696629,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.25617977528089886,"Care has been taken to ensure reproducibility of all models and experiments in this paper. In addition
to detailing exact model architectures and training details in the appendix, we make source code with
experimental setups, model implementations, and random seeds available at https://github.
com/keiradams/ChIRo. Our GitHub site also contains links to the exact datasets and splits
used in each experiment for the contrastive learning, R/S classification, and ranking enantiomers by
docking score tasks. Although copyright restrictions prevent us from releasing the dataset and splits
for the l / d classification task, we detail our data filtering and processing steps in appendix A.7."
REPRODUCIBILITY STATEMENT,0.25842696629213485,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.2606741573033708,"This research was supported by the Office of Naval Research under grant number N00014-21-1-
2195. L.P. thanks the MIT-Takeda fellowship program for financial support. The authors acknowl-
edge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC re-
sources that have contributed to the research results reported within this paper. The authors thank
Octavian Ganea and John Bradshaw for providing helpful suggestions regarding the content and
organization of this paper."
REFERENCES,0.26292134831460673,REFERENCES
REFERENCES,0.2651685393258427,"Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019."
REFERENCES,0.26741573033707866,"Simon Axelrod and Rafael Gomez-Bombarelli. Molecular machine learning with conformer ensem-
bles. arXiv preprint arXiv:2012.08452, 2021."
REFERENCES,0.2696629213483146,"Vassileios Balntas, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Learning local feature
descriptors with triplets and shallow convolutional neural networks. In Edwin R. Hancock Richard
C. Wilson and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
(BMVC), pp. 119.1–119.11. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/
C.30.119. URL https://dx.doi.org/10.5244/C.30.119."
REFERENCES,0.27191011235955054,"Evan E. Bolton, Jie Chen, Sunghwan Kim, Lianyi Han, Siqian He, Wenyao Shi, Vahan Simonyan,
Yan Sun, Paul A. Thiessen, Jiyao Wang, Bo Yu, Jian Zhang, and Stephen H. Bryant. PubChem3D:
a new resource for scientists. Journal of Cheminformatics, 3, 2011."
REFERENCES,0.27415730337078653,"Nathan Brown, Marco Fiscato, Marwin H.S. Segler, and Alain C. Vaucher. GuacaMol: Benchmark-
ing models for de novo molecular design. Journal of Chemical Information and Modeling, 59(3):
1096–1108, 2019."
REFERENCES,0.27640449438202247,"Kangway V. Chuang and Michael J. Keiser. Attention-based learning on molecular ensembles. arXiv
preprint arXiv:2011.12820, 2020."
REFERENCES,0.2786516853932584,"Kangway V. Chuang, Laura M. Gunsalus, and Michael J. Keiser. Learning molecular representations
for medicinal chemistry. Journal of Medicinal Chemistry, 63:8705–8722, 2020."
REFERENCES,0.2808988764044944,"Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc-
tured data. In Proceedings of the 33rd International Conference on Machine Learning, JMLR:
WCP, volume 48, 2016."
REFERENCES,0.28314606741573034,Published as a conference paper at ICLR 2022
REFERENCES,0.2853932584269663,"David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G´omez-Bombarelli, Tim-
othy Hirzel, Al´an Aspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for
learning molecular fingerprints. In Proceedings of Advances in Neural Information Processing
Systems, volume 28, 2015."
REFERENCES,0.2876404494382023,"Daniel C. Elton, Zois Boukouvalas, Mark D. Fuge, and Peter W. Chung. Deep learning for molecular
design–a review of the state of the art. Molecular Systems Design & Engineering, 4:828–849,
2019."
REFERENCES,0.2898876404494382,"Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with PyTorch Geometric.
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019."
REFERENCES,0.29213483146067415,"Daniel Flam-Shepherd, Tony Wu, Pascal Friederich, and Alan Aspuru-Guzik. Neural message pass-
ing on high order paths. Machine Learning: Science and Technology, 2(4), 2021."
REFERENCES,0.2943820224719101,"Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-transformers: 3D roto-
translation equivariant attention networks. 34th Conference on Neural Information Processing
Systems (NeurIPS), 2020."
REFERENCES,0.2966292134831461,"Octavian-Eugen Ganea, Lagnajit Pattanaik, Connor W. Coley, Regina Barzilay, Klavs F. Jensen,
William H. Green, and Tommi S. Jaakola. GeoMol: Torsional geometric generation of molecular
3D conformer ensembles. 35th Conference on Neural Information Processing Systems (NeurIPS),
2021."
REFERENCES,0.298876404494382,"Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, volume 70, pp. 1263–1272, 2017."
REFERENCES,0.30112359550561796,"Alexander Golbraikh, Danail Bonchev, and Alexander Tropsha. Novel chirality descriptors derived
from molecular topology. J. Chem. Inf. Comput. Sci., 41:147–158, 2001."
REFERENCES,0.30337078651685395,"Paul C. D. Hawkins, A. Geoffrey Skillman, Gregory L. Warren, Benjamin A. Ellingson, and
Matthew T. Stahl. Conformer generation with OMEGA: algorithm and validation using high
quality structures from the protein databank and cambridge structural database. Journal of Chem-
ical Information and Modeling, 50(4):572–584, 2010."
REFERENCES,0.3056179775280899,"Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W. Co-
ley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics Data Commons: Machine learning
datasets and tasks for drug discovery and development. In Proceedings of Neural Information
Processing Systems, NeurIPS Datasets and Benchmarks, 2021."
REFERENCES,0.30786516853932583,"F. Jamali, R. Mehvar, and F.M. Pasutto. Enantioselective aspects of drug action and disposition:
therapeutic pitfalls. Journal of Pharmaceutical Sciences, 78(9):695–715, 1989."
REFERENCES,0.3101123595505618,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, Alex Bridgland,
Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-
Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman,
Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Se-
bastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Push-
meet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold.
Nature, 596:583–589, 2021."
REFERENCES,0.31235955056179776,"Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: Moving beyond fingerprints. Journal of Computer-Aided Molecular Design, 30:
595–608, 2016."
REFERENCES,0.3146067415730337,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.31685393258426964,"Johannes Klicpera, Shankari Giri, Johannes T. Margraf, and Stephan G¨unnemann.
Fast and
uncertainty-aware directional message passing for non-equilibrium molecules. Machine Learning
for Molecules Workshop at NeurIPS, 2020a."
REFERENCES,0.31910112359550563,Published as a conference paper at ICLR 2022
REFERENCES,0.32134831460674157,"Johannes Klicpera, Janek Groß, and Stephan G¨unnemann. Directional message passing for molec-
ular graphs. International Conference on Learning Representations (ICLR), 2020b."
REFERENCES,0.3235955056179775,"Johannes Klicpera, Florian Becker, and Stephan G¨unnemann. GemNet: Universal directional graph
neural networks for molecules.
35th Conference on Neural Information Processing Systems
(NeurIPS), 2021."
REFERENCES,0.3258426966292135,"Assia Kovatcheva, Alexander Golbraikh, S. Oloff, J. Feng, W. Zheng, and Alexander Tropsha.
QSAR modeling of datasets with enantioselective compounds using chirality sensitive molecu-
lar descriptors. SAR and QSAR in Environmental Research, 16(1-2):93–102, 2007."
REFERENCES,0.32808988764044944,"Greg Landrum. RDKit: Open-source cheminformatics. 2010. URL https://www.rdkit.
org/."
REFERENCES,0.3303370786516854,"Kuangbiao Liao, Yun-Fang Yang, Yingzi Li, Jacob N. Sanders, K.N. Houk, Djamaladdin G. Musaev,
and Huw M.L. Davies. Design of catalysts for site-selective and enantioselective functionalization
of non-activated primary C–H bonds. Nature Chemistry, 10:1048–1055, 2018."
REFERENCES,0.3325842696629214,"Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E. Gonzalez, and Ion Sto-
ica.
Tune: A research platform for distributed model selection and training.
arXiv preprint
arXiv:1807.05118, 2018."
REFERENCES,0.3348314606741573,"Zhiqiu Lin, Jin Sun, Abe Davis, and Noah Snavely. Visual chirality. Computer Vision and Pattern
Recognition (CVPR), 2020."
REFERENCES,0.33707865168539325,"Yi Liu, Limei Wang, Meng Liu, Xuan Zhang, Bora Oztekin, and Ji Shuiwang. Spherical message
passing for 3D graph networks. arXiv preprint arXiv:2102.05013, 2021."
REFERENCES,0.3393258426966292,"G´omez-Hortig¨uela Luis and Bernardo-Maestro Beatriz. Chiral Organic Structure-Directing Agents.
In: G´omez-Hortig¨uela, Luis (eds) Insights into the Chemistry of Organic Structure-Directing
Agents in the Synthesis of Zeolitic Materials. Structure and Bonding, volume 175. Springer Inter-
national Publishing, 2018."
REFERENCES,0.3415730337078652,"Rafael Mamede, Bruno Sim˜oes de Almeida, Mengyao Chen, Qingyou Zhang, and Joao Aires-de
Sousa. Machine learning classification of one-chiral-center organic molecules according to optical
rotation. Journal of Chemical Information and Modeling, 61(1):67–75, 2021."
REFERENCES,0.3438202247191011,"Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry pre-
diction using a deep generative graph neural network. Scientific Reports, 9, 2019."
REFERENCES,0.34606741573033706,"Lien Ai Nguyen, Hua He, and Chuong Pham-Huy. Chiral drugs: An overview. International Journal
of Biomedical Science, 2(2):85–100, 2006."
REFERENCES,0.34831460674157305,"Lagnajit Pattanaik, Octavian-Eugen Ganea, Ian Coley, Klavs F. Jensen, William H. Green, and Con-
nor W. Coley. Message passing networks for molecules with tetrahedral chirality. arXiv preprint
arXiv:2012.00094, 2020a."
REFERENCES,0.350561797752809,"Lagnajit Pattanaik, John B Ingraham, Colin A Grambow, and William H Green. Generating transi-
tion states of isomerization reactions with deep learning. Physical Chemistry Chemical Physics,
22(41):23618–23626, 2020b."
REFERENCES,0.35280898876404493,"Andreas Pfaltz and William J. III Drury. Design of chiral ligands for asymmetric catalysis: From
C2-symmetric P,P- and N,N-ligands to sterically and electronically nonsymmetrical P,N-ligands.
PNAS, 101(16):5723–5726, 2004."
REFERENCES,0.3550561797752809,"Reaxys. Reaxys and the Reaxys trademark are owned and protected by Reed Elsevier Properties SA
and used under license. Accessed Aug. 6-9, 2021. URL https://www.reaxys.com/."
REFERENCES,0.35730337078651686,"Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural net-
works. In Proceedings of the 38th International Conference on Machine Learning, PMLR, volume
139, pp. 9323–9332, 2021."
REFERENCES,0.3595505617977528,"Nadine Schneider, Richard A. Lewis, Nikolas Fechner, and Peter Ertl. Chiral cliffs: Investigating
the influence of chirality on binding affinity. ChemMedChem, 13(13):1315–1324, 2018."
REFERENCES,0.36179775280898874,Published as a conference paper at ICLR 2022
REFERENCES,0.36404494382022473,"Kristof T. Sch¨utt, Pieter-Jan Kindermans, Huziel E. Sauceda, Stefan Chmiela, Alexandre
Tkatchenko, and KLaus-Robert M¨uller. SchNet: A continuous-filter convolutional neural net-
work for modeling quantum interactions. In Proceedings of Advances in Neural Information
Processing Systems, volume 30, pp. 992–1002, 2017."
REFERENCES,0.36629213483146067,"Gregor Simm and Jose Miguel Hernandez-Lobato. A generative model for molecular distance geom-
etry. In Proceedings of the 37th International Conference on Machine Learning, PMLR, volume
119, pp. 8949–8958, 2020."
REFERENCES,0.3685393258426966,"Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neu-
ral networks on graphs. Computer Vision and Pattern Recognition (CVPR), 2017."
REFERENCES,0.3707865168539326,"Nathaniel Thomas, Tess Smidt, Steven M. Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point
clouds. arXiv preprint arXiv:1802.08219, 2018."
REFERENCES,0.37303370786516854,"Oleg Trott and Arthur J. Olson. AutoDock Vina: improving the speed and accuracy of docking
with a new scoring function, efficient optimization and multithreading. Journal of Computational
Chemistry, 31(2):455–461, 2010."
REFERENCES,0.3752808988764045,"Jos´e R. Vald´es-Martin´ı, Yovani Marrero-Ponce, C´esar R. Garc´ıa-Jacas, Karina Martinez-Mayorga,
Stephen J. Barigye, Yasser Silveira Vaz d’Almeida, Facundo P´erez-Gim´enez, and Carlos A.
Morell. QuBiLS-MAS, open source multi-platform software for atom-and bond-based topological
(2D) and chiral (2.5 D) algebraic molecular descriptors computations. Journal of Cheminformat-
ics, 9(1):1–26, 2017."
REFERENCES,0.3775280898876405,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio.
Graph attention networks.
International Conference on Learning Representations
(ICLR), 2018."
REFERENCES,0.3797752808988764,"Jan Weinreich, Nicholas J. Browning, and O. Anatole von Lilienfeld. Machine learning of free
energies in chemical compound space using ensemble representations: Reaching experimental
uncertainty for solvation. The Journal of Chemical Physics, 154(13), 2021."
REFERENCES,0.38202247191011235,"Robin Winter, Frank No´e, and Djork-Arn´e Clevert. Auto-encoding molecular conformations. arXiv
preprint arXiv:2101.01618, 2021."
REFERENCES,0.3842696629213483,"Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S.
Pappu, Karl Leswing, and Vijay Pande.
MoleculeNet: A benchmark for molecular machine
learning. Chem Sci., 9(2):513–530, 2018."
REFERENCES,0.3865168539325843,"Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative
dynamics for molecular conformation generation. International Conference on Learning Repre-
sentations (ICLR), 2021."
REFERENCES,0.3887640449438202,"Kevin Yang, Kyle Swanson, Wengong Jin, Connor W. Coley, Philipp Eiden, Hua Gao, Angel
Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels,
Tommi Jaakola, Klavs F. Jensen, and Regina Barzilay. Analyzing learned molecular representa-
tions for property prediction. Journal of Chemical Information and Modeling, 59(8):3370–3388,
2019."
REFERENCES,0.39101123595505616,"Raymond A. Yeh, Yuan-Ting Hu, and Alexander G. Schwing. Chirality nets for human pose regres-
sion. 33rd Conference on Neural Information Processing Systems (NeurIPS), 2019."
REFERENCES,0.39325842696629215,"Dmitry V. Zankov, Maxim D. Shevelev, Aleksandra V. Nikoenko, Pavel G. Polishchuk, Asima I.
Rakhimbekova, and Timur I. Madzhidov. Multi-instance learning for structure-activity modeling
for molecular properties. International Conference on Analysis of Images, Social Networks and
Texts, pp. 62–71, 2019."
REFERENCES,0.3955056179775281,"Dmitry V. Zankov, Mariia Matveieva, Aleksandra Nikoenko, Ramil Nugmanov, Igor I. Baskin,
Alexandre Varnek, Pavel Polishchuk, and Timur Madzhidov. QSAR modeling based on con-
formation ensembles using a multi-instance learning approach. Journal of Chemical Information
and Modeling, 2021."
REFERENCES,0.39775280898876403,Published as a conference paper at ICLR 2022
REFERENCES,0.4,"A
APPENDIX"
REFERENCES,0.40224719101123596,CONTENTS
REFERENCES,0.4044943820224719,"A Appendix
14"
REFERENCES,0.4067415730337079,"A.1
Why does plotting α(xy)
sin
vs. α(xy)
cos
yield a perfect circle?
. . . . . . . . . . . . . .
15"
REFERENCES,0.40898876404494383,"A.2
Why do we need phase shifts?
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
16"
REFERENCES,0.41123595505617977,"A.3
Chiral Message Passing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.4134831460674157,"A.4
Model Architectures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.4157303370786517,"A.4.1
ChIRo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17"
REFERENCES,0.41797752808988764,"A.4.2
3D Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20"
REFERENCES,0.4202247191011236,"A.4.3
2D Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22"
REFERENCES,0.42247191011235957,"A.5
Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
REFERENCES,0.4247191011235955,"A.5.1
DimeNet++ Initializations . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
REFERENCES,0.42696629213483145,"A.5.2
SphereNet Data Processing Errors . . . . . . . . . . . . . . . . . . . . . .
23"
REFERENCES,0.42921348314606744,"A.5.3
Auxiliary Torsion Loss when Training ChIRo . . . . . . . . . . . . . . . .
23"
REFERENCES,0.4314606741573034,"A.5.4
Contrastive Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23"
REFERENCES,0.4337078651685393,"A.5.5
R / S Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.43595505617977526,"A.5.6
l / d Classification
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.43820224719101125,"A.5.7
Ranking Enantiomers by Docking Scores . . . . . . . . . . . . . . . . . .
24"
REFERENCES,0.4404494382022472,"A.6
Hyperparameter Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
REFERENCES,0.44269662921348313,"A.6.1
ChIRo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26"
REFERENCES,0.4449438202247191,"A.6.2
SphereNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27"
REFERENCES,0.44719101123595506,"A.6.3
DMPNN
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27"
REFERENCES,0.449438202247191,"A.7
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28"
REFERENCES,0.451685393258427,"A.7.1
Contrastive Learning and R/S Classification Datasets . . . . . . . . . . . .
28"
REFERENCES,0.45393258426966293,"A.7.2
l / d Classification Dataset . . . . . . . . . . . . . . . . . . . . . . . . . .
30"
REFERENCES,0.45617977528089887,"A.7.3
Ranking Enantiomers by Docking Scores . . . . . . . . . . . . . . . . . .
32"
REFERENCES,0.4584269662921348,"A.8
Additional Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34"
REFERENCES,0.4606741573033708,"A.9
Additional Evaluation of Ranking Enantiomers by Docking Scores . . . . . . . . .
34"
REFERENCES,0.46292134831460674,"A.10 Analyzing SE(3)-Invariant 3D GNNs without InterRoto-Invariance . . . . . . . . .
36"
REFERENCES,0.4651685393258427,Published as a conference paper at ICLR 2022
REFERENCES,0.46741573033707867,"A.1
WHY DOES PLOTTING α(xy)
SIN
VS. α(xy)
COS
YIELD A PERFECT CIRCLE?"
REFERENCES,0.4696629213483146,"It is not immediately obvious that plotting α(xy)
sin
versus α(xy)
cos
forms a perfect circle with a constant
radius, as opposed to an ellipse or other shape with non-constant radii. Yet, this result follows from
simple trigonometry."
REFERENCES,0.47191011235955055,"From Equation 11, we have:"
REFERENCES,0.47415730337078654,"α(xy)
cos =
X"
REFERENCES,0.4764044943820225,i∈N(x)\{y} X
REFERENCES,0.4786516853932584,"j∈N(y)\{x}
c(xy)
ij
cos(ψixyj + φixyj)"
REFERENCES,0.48089887640449436,"α(xy)
sin
=
X"
REFERENCES,0.48314606741573035,i∈N(x)\{y} X
REFERENCES,0.4853932584269663,"j∈N(y)\{x}
c(xy)
ij
sin(ψixyj + φixyj)"
REFERENCES,0.48764044943820223,"for a internal molecular bond between nodes (x, y) that forms a set of coupled torsions {ψixyj}(i,j)
where i ∈N(x) \ {y} and j ∈N(y) \ {x}. For clarity, we drop the (xy) notation and index each
torsion in this set of n coupled torsions by a single index k such that {ψixyj}(i,j) = {ψk}n
k=1."
REFERENCES,0.4898876404494382,"We want to show that plotting Pn
k=1 ck sin(R+ψk +φk) versus Pn
k=1 ck cos(R+ψk +φk), where
R ∈[0, 2π) is a rotation of the bond (x, y), yields a circle with a constant radius invariant to the
rotation R. As a simple case, consider n = 2. We then have:"
REFERENCES,0.49213483146067416,αcos = c1 cos(R + ψ1 + φ1) + c2 cos(R + ψ2 + φ2) = c1 cos(R + θ1) + c2 cos(R + θ2)
REFERENCES,0.4943820224719101,αsin = c1 sin(R + ψ1 + φ1) + c2 sin(R + ψ2 + φ2) = c1 sin(R + θ1) + c2 sin(R + θ2)
REFERENCES,0.4966292134831461,αcos can be expanded and simplified as:
REFERENCES,0.49887640449438203,c1 cos(R + θ1) + c2 cos(R + θ2) = c1
REFERENCES,0.501123595505618,"
cos(R) cos(θ1) −sin(R) sin(θ1)

+ c2"
REFERENCES,0.503370786516854,"
cos(R) cos(θ2) −sin(R) sin(θ2)

="
REFERENCES,0.5056179775280899,a cos(R) −b sin(R) = d cos(R + e)
REFERENCES,0.5078651685393258,"where
a = d cos e = c1 cos(θ1) + c2 cos(θ2)
b = d sin e = c1 sin(θ1) + c2 sin(θ2)"
REFERENCES,0.5101123595505618,"Similarly, αsin can be expanded and simplified as:"
REFERENCES,0.5123595505617977,c1 sin(R + θ1) + c2 sin(R + θ2) = c1
REFERENCES,0.5146067415730337,"
sin(R) cos(θ1) + cos(R) sin(θ1)

+ c2"
REFERENCES,0.5168539325842697,"
sin(R) cos(θ2) + cos(R) sin(θ2)

="
REFERENCES,0.5191011235955056,a′ sin(R) + b′ cos(R) = d′ sin(R + e′)
REFERENCES,0.5213483146067416,"where
a′ = d′ cos e′ = c1 cos(θ1) + c2 cos(θ2)
b′ = d′ sin e′ = c1 sin(θ1) + c2 sin(θ2)"
REFERENCES,0.5235955056179775,"Since d = d′ and e = e′, we have that αcos = d cos(R + e) and αsin = d sin(R + e). It follows that
in the general case (n ≥2):"
REFERENCES,0.5258426966292135,"αcos = n
X"
REFERENCES,0.5280898876404494,"k=1
ck cos(ωt + ψk + φk) = r cos(R + s)"
REFERENCES,0.5303370786516854,"αsin = n
X"
REFERENCES,0.5325842696629214,"k=1
ck sin(ωt + ψk + φk) = r sin(R + s)"
REFERENCES,0.5348314606741573,"for some coefficients r and s, where R ∈[0, 2π) represents some rotation about the internal bond. It
is immediately obvious that plotting αsin versus αcos for all R ∈[0, 2π) yields a perfect circle with
radius r."
REFERENCES,0.5370786516853933,Published as a conference paper at ICLR 2022
REFERENCES,0.5393258426966292,"A.2
WHY DO WE NEED PHASE SHIFTS?"
REFERENCES,0.5415730337078651,"At first glance, it is unclear as to why Equations 8-9 are insufficient to learn chirality, e.g. why we
need to add phase shifts to distinguish enantiomers. To understand why this is the case, consider an
arbitrary stereoisomer containing an internal molecular bond between nodes x and y (with x or y
being chiral) that forms a set of n coupled torsions {ψixyj}(i,j) = {ψ(k)
xy }n
k=1 indexed by k. Direct"
REFERENCES,0.5438202247191011,"computation of the norm ||α(xy)
cos , α(xy)
sin || = ||αcos, αsin|| yields:"
REFERENCES,0.5460674157303371,"||αcos, αsin|| =
p"
REFERENCES,0.5483146067415731,αcos2 + αsin2
REFERENCES,0.550561797752809,where we have dropped the (xy) notation for clarity. Expanding and squaring Equation 8 gives:
REFERENCES,0.5528089887640449,"αcos
2 = n
X"
REFERENCES,0.5550561797752809,"k=1
c2
k cos2 ψk + n
X k=1 n
X"
REFERENCES,0.5573033707865168,"l=k+1
2ckcl cos ψk cos ψl"
REFERENCES,0.5595505617977528,"αsin
2 = n
X"
REFERENCES,0.5617977528089888,"k=1
c2
k sin2 ψk + n
X k=1 n
X"
REFERENCES,0.5640449438202247,"l=k+1
2ckcl sin ψk sin ψl"
REFERENCES,0.5662921348314607,"Now consider reflecting this stereoisomer across a plane to generate its enantiomer with inverted
chiral centers. Reflecting a conformer negates all its torsions, and thus the reflected enantiomer’s in-
ternal bond between (reflected) nodes x′ and y′ will form a set of n negated torsions {ψ′
ixyj}(i,j) ="
REFERENCES,0.5685393258426966,"{ψ′(k)
xy }n
k=1 where ψ′(k)
xy
= −ψ(k)
xy for each k. Because enantiomers have the same 2D graph con-
nectivity, the learned set of coefficients {ck} (which depend only on the permutation invariant node
features, see Equation 5) will be identical between enantiomers. It immediately follows from the
identities sin(−x) = −sin(x) and cos(−x) = cos(x) that α′
cos
2 = α2
cos and α′
sin
2 = α2
sin. Because
||αcos, αsin|| is invariant to any rotation of the bond (x, y), no matter how we rotate (x, y) or (x′, y′),
||αcos, αsin|| = ||α′
cos, α′
sin||."
REFERENCES,0.5707865168539326,Published as a conference paper at ICLR 2022
REFERENCES,0.5730337078651685,"Table 4: Effects of ablating components of ChIRo on test-set accuracies for the l / d classification and
enantiomer docking score ranking tasks, when chiral message passing is included prior to readout.
Mean and standard deviations are reported across 5 folds (l / d) or 3 repeated trials (enantiomer
ranking). The first row indicates the original ChIRo without CMP."
REFERENCES,0.5752808988764045,"Model Components
Accuracy (%) ↑"
REFERENCES,0.5775280898876405,"CMP
Tags
(dij, ϕijk, ψixyj)
(zd, zϕ, zα)
l / d
Enantiomer Ranking"
REFERENCES,0.5797752808988764,"X
X
✓
✓
79.3 ± 0.4
72.0 ± 0.5"
REFERENCES,0.5820224719101124,"✓
X
✓
✓
78.5 ± 0.5
71.5 ± 0.5
✓
✓
✓
✓
77.0 ± 0.5
71.7 ± 0.6
✓
✓
X
✓
75.1 ± 0.3
69.8 ± 0.6
✓
✓
X
X
75.0 ± 0.9
68.9 ± 0.4
✓
X
X
✓
50.0 ± 0.0
49.9 ± 0.3"
REFERENCES,0.5842696629213483,"A.3
CHIRAL MESSAGE PASSING"
REFERENCES,0.5865168539325842,"Tetrahedral chirality is fundamentally a node-level property. Yet, we have treated chirality through
the lens of torsions and internal bonds involving chiral centers. To propagate the learned representa-
tions of chirality to node states, we may perform an (optional) additional phase of message passing,
treating each zαxy as a vector of edge-attributes. This might allow the model to propagate chiral
information across the graph, i.e., help learn the global effects of local chirality. This phase may
be included prior to readout, and uses a single EConv layer followed by a sequence of TCMP GAT
layers:"
REFERENCES,0.5887640449438202,"hCMP
i
= GATTCMP
CMP ◦... ◦GAT1
CMP"
REFERENCES,0.5910112359550562,"
ΘCMP hT
i +
X"
REFERENCES,0.5932584269662922,"j∈N(i)
hT
j · fCMP(zαij)

(15)"
REFERENCES,0.5955056179775281,"During readout, these updated node states hCMP
i
are summed rather than the (non-chiral) node states."
REFERENCES,0.597752808988764,"Table 4 reports the effects of including chiral message passing on ChIRo’s performance on the l /
d classification task and the ranking enantiomers by docking scores task, along with the effects of
ablating other components of ChIRo. Although CMP does not improve performance (and marginally
hurts) the unablated ChIRo, using CMP in place of (zd, zϕ, zα) during readout provides another
option of augmenting 2D GNNs that solely use chiral tags as the only indication of chirality."
REFERENCES,0.6,"A.4
MODEL ARCHITECTURES"
REFERENCES,0.6022471910112359,"A.4.1
CHIRO"
REFERENCES,0.604494382022472,"Figure 4 visualizes the full architecture of ChIRo. Table 5 specifies the hyperparameters chosen for
each task. See appendix A.6 for details on hyperparameter optimizations."
REFERENCES,0.6067415730337079,Published as a conference paper at ICLR 2022
REFERENCES,0.6089887640449438,Edge Convolution
D MESSAGE PASSING,0.6112359550561798,2D Message Passing
D MESSAGE PASSING,0.6134831460674157,Bond Distance Encoder
D MESSAGE PASSING,0.6157303370786517,Sum-Pooling
D MESSAGE PASSING,0.6179775280898876,Bond Angle Encoder
D MESSAGE PASSING,0.6202247191011236,Torsion Encoder
D MESSAGE PASSING,0.6224719101123596,L2-normalization
D MESSAGE PASSING,0.6247191011235955,"Sum-Pooling
Edge Convolution"
D MESSAGE PASSING,0.6269662921348315,GAT Layers
D MESSAGE PASSING,0.6292134831460674,Readout
D MESSAGE PASSING,0.6314606741573033,"Internal Coordinates
O
O
O"
D MESSAGE PASSING,0.6337078651685393,Sum-Pooling
D MESSAGE PASSING,0.6359550561797753,GAT Layers
D MESSAGE PASSING,0.6382022471910113,Edge/Node Featurization
D MESSAGE PASSING,0.6404494382022472,Figure 4: Architecture of ChIRo
D MESSAGE PASSING,0.6426966292134831,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.6449438202247191,"Table 5: Hyperparameters optimized for ChIRo on each task. Parameters for chiral message passing
are shown, although chiral message passing is omitted in the default version of ChIRo."
D MESSAGE PASSING,0.647191011235955,"Hyperparameters
Task"
D MESSAGE PASSING,0.6494382022471911,"Contrastive
R / S
l / d
Ranking"
D MESSAGE PASSING,0.651685393258427,"Node Features Dimension
52
Edge Features Dimension
14
All MLP Hidden Activations
LeakyReLU
All MLP Output Activations
Identity
EConv MLP Hidden Layer Size
32
64
64
32
EConv MLP # Hidden Layers
2
1
1
2
h0, hT , hCMP
t
Dimension
64
32
32
64
ht, t = 1, ..., T −1 Dimension
64
64
32
64
# GAT Layers
2
3
2
2
# GAT Heads
4
4
2
4
fd Hidden Layer Size
–
128
32
64
fd # Hidden Layers
–
2
2
2
fϕ Hidden Layer Size
–
128
32
64
fϕ # Hidden Layers
–
2
2
2
fα Hidden Layer Size
64
128
32
64
fα # Hidden Layers
2
2
2
2
fc Hidden Layer Size
64
128
32
64
fc # Hidden Layers
2
2
2
2
fφ Hidden Layer Size
64
256
256
256
fφ # Hidden Layers
2
2
3
2
fout Hidden Layer Size
–
64
64
128
fout # Hidden Layers
–
2
2
2
zd, zϕ, zα Dimension
2
64
8
8
CMP EConv Hidden Layer Size
–
32
256
256
CMP EConv # Hidden Layers
–
1
3
2
# CMP GAT Layers
–
3
3
3
# CMP GAT Heads
–
2
2
2
γaux
8.25e-4
6.86e-3
1.86e-3
8.25e-4
Learning Rate
6.06e-4
5.69e-4
1.28e-4
6.06e-4
Batch Size
32
16
16
32
# Epochs
50
100
100
150"
D MESSAGE PASSING,0.6539325842696629,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.6561797752808989,"Table 6: Hyperparameters selected for SphereNet on each task. See appendix A.6 for hyperparame-
ter optimizations."
D MESSAGE PASSING,0.6584269662921348,"Hyperparameters
Task"
D MESSAGE PASSING,0.6606741573033708,"Contrastive
R / S
l / d
Ranking"
D MESSAGE PASSING,0.6629213483146067,"Readout MLP Hidden Activations
LeakyReLU
Readout MLP Output Activation
Identity
Readout MLP Hidden Size
–
256
64
64
Readout MLP # of Hidden Layers
–
4
2
2
hidden channels
128
256
64
256
out channels
2
64
64
32
cutoff
5.0
5.0
5.0
5.0
num layers
4
4
4
5
int emb size
64
32
128
64
basis emb size dist
8
8
8
8
basis emb size angle
8
8
8
8
basis emb size torsion
8
8
8
8
out emb channels
256
128
64
32
num spherical
7
7
7
7
num radial
6
6
6
6
envelope exponent
5
5
5
5
num before skip
1
1
1
1
num after skip
2
2
2
2
num output layers
3
3
3
3
Learning Rate
1e-4
1.54e-4
4.79e-4
1.40e-4
Batch Size
32
64
16
32"
D MESSAGE PASSING,0.6651685393258427,"A.4.2
3D BASELINES"
D MESSAGE PASSING,0.6674157303370787,"To adapt SchNet, DimeNet++, and SphereNet for our tasks, we increase the dimensionality of their
respective aggregated node embeddings (“out channels”) and use this aggregation as a latent vector
either for direct use in Equation 14 (for self-supervised contrastive learning), or for input to a feed-
forward readout MLP for downstream regression/classification. For all three 3D GNNs, we use their
default node featurizations, using the atomic number as the only node feature."
D MESSAGE PASSING,0.6696629213483146,"Table 6 lists the hyperparameters used for SphereNet on all four tasks. Tables 7 and 8 list the
hyperparameters used for SchNet and DimeNet++ on the contrastive learning and R / S classification
tasks."
D MESSAGE PASSING,0.6719101123595506,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.6741573033707865,"Table 7: Hyperparameters selected for SchNet on each task. Apart from the number of out channels
and the inclusion of a readout MLP, the default SchNet architecture was used. Note that we used the
same readout MLP architecture as in the optimized SphereNet."
D MESSAGE PASSING,0.6764044943820224,"Hyperparameters
Task"
D MESSAGE PASSING,0.6786516853932584,"Contrastive
R / S"
D MESSAGE PASSING,0.6808988764044944,"Readout MLP Hidden Activations
–
LeakyReLU
Readout MLP Output Activation
–
Identity
Readout MLP Hidden Size
–
64
Readout MLP # of Hidden Layers
–
2
hidden channels
128
128
out channels
2
32
num filters
128
128
num interactions
6
6
num gaussians
50
50
cutoff
10.0
10.0
max num neighbors
32
32
Learning Rate
1e-4
1e-4
Batch Size
32
32"
D MESSAGE PASSING,0.6831460674157304,"Table 8: Hyperparameters selected for DimeNet++ on each task. Apart from the number of out
channels and the inclusion of a readout MLP, the default DimeNet++ architecture was used. Note
that we used the same readout MLP architecture as in the optimized SphereNet."
D MESSAGE PASSING,0.6853932584269663,"Hyperparameters
Task"
D MESSAGE PASSING,0.6876404494382022,"Contrastive
R / S"
D MESSAGE PASSING,0.6898876404494382,"Readout MLP Hidden Activations
–
LeakyReLU
Readout MLP Output Activation
–
Identity
Readout MLP Hidden Size
–
64
Readout MLP # of Hidden Layers
–
2
hidden channels
128
128
out channels
2
32
num blocks
4
4
cutoff
5.0
5.0
int emb size
64
64
basis emb size
8
8
out emb channels
256
256
num spherical
7
7
num radial
6
6
envelope exponent
5
5
num before skip
1
1
num after skip
2
2
num output layers
3
3
Learning Rate
1e-4
1e-4
Batch Size
32
32"
D MESSAGE PASSING,0.6921348314606741,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.6943820224719102,"Table 9: Hyperparameters selected for DMPNN on each task. Following Pattanaik et al., we only
optimize hyperparameters for the baseline sum aggregator and extend these hyperparameters to the
TetraDMPNN models."
D MESSAGE PASSING,0.6966292134831461,"Hyperparameters
Task"
D MESSAGE PASSING,0.698876404494382,"Contrastive
R / S"
D MESSAGE PASSING,0.701123595505618,"hidden size
300
300
depth
3
3
dropout
0.2
0.2
Max Learning Rate
1e-4
1e-4
Batch Size
50
50"
D MESSAGE PASSING,0.7033707865168539,"A.4.3
2D BASELINES"
D MESSAGE PASSING,0.7056179775280899,"To optimize hyperparameters for all 2D baselines, we directy use the code provided by Pattanaik
et al.. We do not make any modifications to the architectures, since we train on similar tasks as the
original work."
D MESSAGE PASSING,0.7078651685393258,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.7101123595505618,"A.5
TRAINING DETAILS"
D MESSAGE PASSING,0.7123595505617978,This section describes the full training protocols for each task considered in this paper.
D MESSAGE PASSING,0.7146067415730337,"A.5.1
DIMENET++ INITIALIZATIONS"
D MESSAGE PASSING,0.7168539325842697,"The default parameter initializations for the output blocks of DimeNet++ make DimeNet++ unable
to break symmetry between different output channels when the number of output channels is set
> 1. Thus, we remove the default initializations for the output blocks when training DimeNet++ on
the contrastive learning task (out channels = 2) and the R/S classification task (out channels = 32)."
D MESSAGE PASSING,0.7191011235955056,"A.5.2
SPHERENET DATA PROCESSING ERRORS"
D MESSAGE PASSING,0.7213483146067415,"When training SphereNet, occasionally the publicly-available implementation of SphereNet failed
to process select conformers. Because this occurred for only a tiny fraction of the conformers in
the overall datasets, we removed 2D graphs whose stereoisomers contained problematic conformers
from the R/S and l / d datasets for SphereNet only. We emphasize that this filtering step did not
meaningfully change the size of the datasets: only 40 2D graphs (out of 39,256) were removed for
the R/S task, and only 6 2D graphs (out of 15,038) were removed for the l / d task. No molecules had
to be removed for the ranking enantiomers by docking scores task. For the contrastive learning task,
we simply skipped the rare batch during training/inference that contained problematic conformers.
Only 452 2D graphs out of 257,743 caused processing errors in the contrastive learning dataset."
D MESSAGE PASSING,0.7235955056179775,"A.5.3
AUXILIARY TORSION LOSS WHEN TRAINING CHIRO"
D MESSAGE PASSING,0.7258426966292135,"In Equation 10, we predict an angular phase shift φ by using fφ to predict cos φ and sin φ separately
with a linear output activation, and then use ℓ2 normalization on the vector [cos φ, sin φ] to ensure
that these sin and cos (and thus the angle φ) have correct circular properties, namely that φ ∈
[0, 2π). We have chosen to predict angles using sin and cos rather than directly predicting a scalar
φ ∈[0, 2π) (e.g., through a scaled sigmoid activation) in order to avoid biasing the predicted phase
shifts toward 0. However, the ℓ2 normalization can also cause the predicted phase shift to be biased
toward 0, π/2, π, and 3π/2 if one of the unnormalized cos φ or sin φ blows up. To prevent this, we
follow Jumper et al. (2021) in adding an auxiliary loss during training to encourage the unnormalized
[cos φ, sin φ] to fall close to the unit circle:"
D MESSAGE PASSING,0.7280898876404495,"Laux = γaux(1 −|| cos φ, sin φ||)
(16)"
D MESSAGE PASSING,0.7303370786516854,where γaux is a small scalar that is tuned during hyperparameter optimization.
D MESSAGE PASSING,0.7325842696629213,"A.5.4
CONTRASTIVE LEARNING"
D MESSAGE PASSING,0.7348314606741573,"For contrastive learning, we train with a triplet margin loss with a normalized Euclidean distance
metric and a margin of 1 (Equation 14). In each training epoch, we randomly partition the train-
ing data into N/b minibatches, where N is the number of distinct stereoisomers (not conformers)
in the training set and b is the batch size. Before triplet sampling, each minibatch therefore con-
tains b stereoisomers. For each stereoisomer i = (1, 2, ..., b) in the minibatch, we then generate a
triplet (a, p, n), where a (anchor) and p (positive) are two randomly selected (without replacement)
conformers of stereoisomer i, and n (negative) is a randomly selected conformer of stereoisomer
j ̸= i, where i and j share the same 2D graph connectivity. If stereoisomer i has multiple differ-
ent stereoisomers j, k, ... (all sharing the same 2D graph) present in the training set, one of these
stereoisomers is randomly chosen. Each anchor, positive, and negative in each triplet are processed
independently by the network to generate b triplets of latent vectors (za, zp, zn), which are then fed
into the triplet margin loss function. Loss contributions from each triplet are averaged to form a
loss for the entire batch. In the case of ChIRo, we add the mean auxiliary torsion loss across all
conformers in the batch to the batch triplet loss."
D MESSAGE PASSING,0.7370786516853932,"We use the Adam optimizer (Kingma & Ba, 2014) with a flat learning rate throughout training,
but employ gradient clipping with a maximum gradient L2-norm of 10. We train each model for a
maximum of 50 epochs, and use the batch-averaged triplet loss (without the auxiliary torsion loss
contributions) on the validation set to select the model with the best estimated generalization perfor-
mance across the 50 epochs. We do not employ dropout or other forms of explicit regularization."
D MESSAGE PASSING,0.7393258426966293,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.7415730337078652,"A.5.5
R / S CLASSIFICATION"
D MESSAGE PASSING,0.7438202247191011,"For R / S classification, we train each 3D model with a binary cross-entropy loss. In each training
epoch, we randomly partition the training data into N/b minibatches, where N is the number of
enantiomers in the training set and b is the batch size. For each enantiomer in the batch, we randomly
sample a conformer for that enantiomer and a conformer for its opposite enantiomer with the same
2D graph. This ensures that minibatches contain both enantiomers of the enantiomeric pair such
that the stochastic gradient steps consider contributions from both enantiomers. We average the loss
contributions for all conformers in the batch. In the case of ChIRo, the mean auxiliary torsion loss
across all conformers in the batch is added to the batch cross-entropy loss."
D MESSAGE PASSING,0.7460674157303371,"We use the Adam optimizer with a flat learning rate throughout training, but employ gradient clip-
ping with a maximum gradient L2-norm of 10. We train each model for a maximum of 100 epochs,
and use the batch-averaged classification accuracy on the validation set to select the model with
the best estimated generalization performance across the 100 epochs. We do not employ dropout or
other forms of explicit regularization."
D MESSAGE PASSING,0.748314606741573,"For testing, we evaluate the classification accuracy on all conformers in the test set, and do not use
any form of conformer-based averaging or voting."
D MESSAGE PASSING,0.750561797752809,"A.5.6
l / d CLASSIFICATION"
D MESSAGE PASSING,0.7528089887640449,"For l / d classification, we train each model with a binary cross-entropy loss. In each training
epoch, we randomly partition the training data into N/b minibatches, where N is the number of
enantiomers in the training set and b is the batch size. For each enantiomer in the batch, we sample
either 1) a random conformer or 2) a pre-selected conformer for that enantiomer, and either 1) a
random conformer or 2) a pre-selected conformer conformer for its opposite enantiomer with the
same 2D graph. This ensures that minibatches contain both enantiomers of the enantiomeric pair
such that the stochastic gradient steps consider contributions from both enantiomers. In both cases,
we average the loss contributions for all conformers in the batch. In the case of ChIRo, we add the
mean auxiliary torsion loss across all conformers in the batch to the batch cross-entropy loss."
D MESSAGE PASSING,0.755056179775281,"We use the Adam optimizer with a flat learning rate throughout training, but employ gradient clip-
ping with a maximum gradient L2-norm of 10. We train each model for a maximum of 100 epochs,
and use the batch-averaged classification accuracy on the validation set to select the model with
the best estimated generalization performance across the 100 epochs. We do not employ dropout or
other forms of explicit regularization."
D MESSAGE PASSING,0.7573033707865169,"For testing, we evaluate the classification accuracy on all conformers in the test set, and do not use
any form of conformer-based averaging or voting."
D MESSAGE PASSING,0.7595505617977528,"A.5.7
RANKING ENANTIOMERS BY DOCKING SCORES"
D MESSAGE PASSING,0.7617977528089888,"For ranking enantiomers by their docking scores, we train each model with a mean squared error
(MSE) loss, with the target values being the ground-truth docking scores for each enantiomer. Note
that although we train the models with an MSE loss, we are more concerned with the ability of each
model to correctly rank enantiomers by their docking scores than with the absolute performance of
each model as a surrogate model for docking score prediction. Thus, we evaluate this task using the
ranking accuracy, defined as whether or not the model correctly predicts a lower docking score for
the enantiomer with the lower ground-truth score."
D MESSAGE PASSING,0.7640449438202247,"In each training epoch, we randomly partition the training data into N/b minibatches, where N is the
number of enantiomers in the training set and b is the batch size. For each enantiomer in the batch,
we sample either 1) a random conformer or 2) a pre-selected conformer for that enantiomer, and
either 1) a random conformer or 2) a pre-selected conformer conformer for its opposite enantiomer
with the same 2D graph. This ensures that minibatches contain both enantiomers of the enantiomeric
pair such that the stochastic gradient steps consider contributions from both enantiomers. In both
cases, we average the loss contributions for all conformers in the batch. In the case of ChIRo, we
add the mean auxiliary torsion loss across all conformers in the batch to the batch MSE loss."
D MESSAGE PASSING,0.7662921348314606,"We use the Adam optimizer with a flat learning rate throughout training, but employ gradient clip-
ping with a maximum gradient L2-norm of 10. We train each model for a maximum of 150 epochs,"
D MESSAGE PASSING,0.7685393258426966,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.7707865168539326,"and use the batch-averaged ranking accuracy on the validation set to select the model with the best
estimated generalization performance across the 150 epochs. We do not employ dropout or other
forms of regularization."
D MESSAGE PASSING,0.7730337078651686,"For testing, we first predict the docking score for all conformers in the test set. We then average the
predicted docking scores for each conformer of each enantiomer, yielding a mean predicted score
for each enantiomer. Finally, we compute the ranking accuracy using these mean predicted scores."
D MESSAGE PASSING,0.7752808988764045,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.7775280898876404,Table 10: Hyperparameter search space for ChIRo
D MESSAGE PASSING,0.7797752808988764,"Hyperparameter
Search Space"
D MESSAGE PASSING,0.7820224719101123,"EConv MLP Hidden Layer Size
[32, 64, 128, 256]
EConv MLP # Hidden Layers
[1,2]
h0, hT , hCMP
t
Dimension
[8, 16, 32, 64]
ht, t = 1, ..., T −1 Dimension
[16, 32, 64]
# GAT Layers
[2,3,4]
# GAT Heads
[1,2,4,8]
fd, fϕ, fα, fc Hidden Layer Size
[32, 64, 128, 256]
fd, fϕ, fα, fc # Hidden Layers
[1,2,3,4]
fφ Hidden Layer Size
[32, 64, 128, 256]
fφ # Hidden Layers
[1,2,3,4]
fout Hidden Layer Size
[32, 64, 128, 256]
fout # Hidden Layers
[1,2,3,4]
zd, zϕ, zα Dimension
[8, 16, 32, 64]
CMP EConv Hidden Layer Size
[32, 64, 128, 256]
CMP EConv # Hidden Layers
[1,2,3,4]
# CMP GAT Layers
[1, 2, 3, 4]
# CMP GAT Heads
[1, 2, 4, 8]
γaux
log uniform (1e-4, 1e-2)
Learning Rate
log uniform (5e-5, 5e-3)
Batch Size
[16, 32, 64, 128, 256]"
D MESSAGE PASSING,0.7842696629213484,"A.6
HYPERPARAMETER OPTIMIZATIONS"
D MESSAGE PASSING,0.7865168539325843,"A.6.1
CHIRO"
D MESSAGE PASSING,0.7887640449438202,"Hyperparameters were tuned for ChIRo on the R / S, l / d, and ranking enantiomers by docking
score tasks, using the Raytune (Liaw et al., 2018) Python package with the HyperOpt plug-in. For
the R / S classification and ranking enantiomers by docking score tasks, we tuned hyperparameters
by training with the training set and evaluating model accuracy on the validation set. For the l / d
classification task, we used the training and validation splits of the first cross-validation fold to tune
hyperparameters, evaluating model accuracy on the validation split. The optimal hyperparameters
were then held constant for the remaining four folds."
D MESSAGE PASSING,0.7910112359550562,"For each task below, we used the HyperOptSearch search algorithm in Raytune, which employs
Tree-structured Parzen Estimators, to search for optimal hyperparameters over the search space
specified in Table 10."
D MESSAGE PASSING,0.7932584269662921,"R / S Classification. We trained a total of 100 models with different hyperparameter combinations
for a maximum of 50 epochs, using the Async Hyperband Scheduler (grace period = 5, reduction
factor = 3, brackets = 1) for aggressive early stopping."
D MESSAGE PASSING,0.7955056179775281,"l / d Classification We performed two stages of optimization. We first trained 100 models with
different hyperparameter combinations for a maximum of 50 epochs, using the Async Hyperband
Scheduler (grace period = 5, reduction factor = 3, brackets = 1) for aggressive early stopping. We
then re-ran the optimization for 50 parameter combinations over a maximum of 100 epochs, using
the best five models from the first optimization as starting (seed) configurations. In this second stage,
we again used HyperOptSearch but changed the Async Hyperband Scheduler to use parameters
(grace period = 10, reduction factor = 4, brackets = 1)."
D MESSAGE PASSING,0.797752808988764,"Ranking enantiomers by docking score. We trained a total of 100 models with different hyperpa-
rameter combinations for a maximum of 50 epochs, using the Async Hyperband Scheduler (grace
period = 5, reduction factor = 3, brackets = 1) for aggressive early stopping."
D MESSAGE PASSING,0.8,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.802247191011236,Table 11: Hyperparameter search space for SphereNet
D MESSAGE PASSING,0.8044943820224719,"Hyperparameter
Search Space"
D MESSAGE PASSING,0.8067415730337079,"hidden channels
[64, 128, 256]
out channels
[16, 32, 64, 128, 256]
cutoff
[5.0, 10.0]
num layers
[3,4,5]
int emb size
[32, 64, 128]
basis emb size dist
8
basis emb size angle
8
basis emb size torsion
8
out emb channels
[64, 128, 256]
num spherical
7
num radial
6
envelope exponent
5
num before skip
1
num after skip
2
num output layers
3
Readout MLP Hidden Size
[32, 64, 128, 256]
Readout MLP # of Hidden Layers
[1,2,3,4]
Learning Rate
log uniform (5e-5, 1e-2)
Batch Size
[16, 32, 64, 128, 256]"
D MESSAGE PASSING,0.8089887640449438,Table 12: Hyperparameter search space for DMPNN
D MESSAGE PASSING,0.8112359550561797,"Hyperparameter
Search Space"
D MESSAGE PASSING,0.8134831460674158,"hidden size
[300, 600, 900, 1200]
depth
[2, 3, 4, 5, 6]
dropout
[0, 0.2, 0.4, 0.6, 0.8, 1]
Max Learning Rate
log uniform (1e-5, 1e-3)
Batch Size
[25, 50, 100]"
D MESSAGE PASSING,0.8157303370786517,"A.6.2
SPHERENET"
D MESSAGE PASSING,0.8179775280898877,"Hyperparameters were also tuned for SphereNet on the R / S, l / d, and ranking enantiomers by
docking score tasks, using the same optimization methodologies as when tuning ChIRo. The hy-
perparameter search space is specified in Table 11. In each of these tasks, we trained a total of 50
models with different hyperparameter combinations for a maximum of 50 epochs, using the Hyper-
OptSearch algorithm and the Async Hyperband Scheduler (grace period = 5, reduction factor = 3,
brackets = 1) for aggressive early stopping."
D MESSAGE PASSING,0.8202247191011236,"A.6.3
DMPNN"
D MESSAGE PASSING,0.8224719101123595,"We tune hyperparameters for the DMPNN on the l / d classification and ranking enantiomers by
docking score tasks using the original code provided by Pattanaik et al., which employs the Optuna
hyperoptimization framework (Akiba et al., 2019). We train a total of 100 models using the Hyper-
bandPruner algorithm to prune unpromising trials and the CmaEsSampler to sample new trials. Note
that we only optimize hyperparameters for the sum aggregation baseline model, and we extend these
hyperparameters to the TetraDMPNN models, following the work of Pattanaik et al.. Because the
TetraDMPNN models use a permutation-based aggregation function, the runtime of these models is
much slower, which renders a full hyperparameter optimization infeasible."
D MESSAGE PASSING,0.8247191011235955,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.8269662921348314,"A.7
DATASETS"
D MESSAGE PASSING,0.8292134831460675,"A.7.1
CONTRASTIVE LEARNING AND R/S CLASSIFICATION DATASETS"
D MESSAGE PASSING,0.8314606741573034,"To create the datasets for the contrastive learning and R/S classification tasks, we first randomly
selected 50 sdf files from the “10 conformers per compound” directory from the PubChem3D FTP
site ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound_3D/, out of 6199 available.
After extracting the conformers in each sdf file, we filtered the data to only include conformers of 2D
graphs for which at least 2 stereoisomers appear in the data, and where each of these stereoisomers
has at least 2 conformers in the data. We further filtered the data to only include 2D graphs whose
conformers contain at least 1 torsion, as recognized by RDKit. This dataset was separated into
80/20 partitions, with conformers corresponding to the same 2D graph being assigned to the same
data partition. The larger (80%) subset was used as-is for contrastive learning. The smaller (20%)
subset was further filtered to only include pairs of enantiomers with only 1 chiral center, excluding
2D graphs that contained other (cis/trans) stereoisomers of these enantiomer pairs in the dataset.
Note that this extra step ensures that there are only two stereoisomers (which are enantiomers) per
2D graph. The resultant subset was used as the R/S classification dataset."
D MESSAGE PASSING,0.8337078651685393,"Figures 5 and 6 show the distributions of the number of conformers per stereoisomer and the number
of stereoisomers per 2D graph in the full contrastive learning dataset. Figure 7 shows the distribution
of the number of conformers per enantiomer in the full R/S classification dataset. Table 13 indicates
the distribution of R/S labels in the R/S dataset."
D MESSAGE PASSING,0.8359550561797753,"The full contrastive learning dataset was split into 70/15/15 sets for training, validation, and testing,
respectively, with conformers corresponding to the same 2D graphs being assigned to the same
data partition. The training set contains 2,088,008 conformers of 418,922 stereoisomers of 180,426
distinct 2D graphs. The validation set contains 450,726 conformers of 89,786 stereoisomers of
38,658 distinct 2D graphs. The test set contains 448,017 conformers of 89,914 stereoisomers of
38,659 distinct 2D graphs."
D MESSAGE PASSING,0.8382022471910112,"The full R/S dataset was similarly separated into 70/15/15 sets, with pairs of enantiomers be-
ing assigned to the same data partition. The training set contains 326,865 conformers of 55,084
stereoisomers (27,542 pairs of enantiomers). The validation set contains 70,099 conformers of
11,748 stereoisomers (5874 pairs of enantiomers). The test set contains 69,719 conformers of 11,680
stereoisomers (5840 pairs of enantiomers)."
D MESSAGE PASSING,0.8404494382022472,"0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26"
D MESSAGE PASSING,0.8426966292134831,# of Conformers 100 101 102 103 104 105
D MESSAGE PASSING,0.8449438202247191,# of Stereoisomers
D MESSAGE PASSING,0.8471910112359551,"Figure 5: Histogram of the number of conformers per stereoisomer in the contrastive learning
dataset."
D MESSAGE PASSING,0.849438202247191,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.851685393258427,"0
1
2
3
4
5
6
7
8
9
10
# of Stereoisomers 101 102 103 104 105"
D MESSAGE PASSING,0.8539325842696629,# of 2D Graphs
D MESSAGE PASSING,0.8561797752808988,Figure 6: Histogram of the number of stereoisomers per 2D graph in the contrastive learning dataset.
D MESSAGE PASSING,0.8584269662921349,"0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
# of Conformers 101 102 103 104"
D MESSAGE PASSING,0.8606741573033708,# of Stereoisomers
D MESSAGE PASSING,0.8629213483146068,Figure 7: Histogram of the number of conformers per enantiomer in the R/S classification dataset.
D MESSAGE PASSING,0.8651685393258427,Table 13: Balance of R/S labels in the R/S classification dataset.
D MESSAGE PASSING,0.8674157303370786,"R / S Label
# of Conformers
# of Enantiomers"
D MESSAGE PASSING,0.8696629213483146,"R
236222
39256
S
230461
39256"
D MESSAGE PASSING,0.8719101123595505,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.8741573033707866,"A.7.2
l / d CLASSIFICATION DATASET"
D MESSAGE PASSING,0.8764044943820225,"Figure 8 enumerates the data filtering steps used to extract and filter the l / d classification dataset
from the experimental Reaxys database. In Reaxys, we queried non-fragmented molecules with
molecular weights ≤564 which had optical rotatory power reported at 18-30 oC and a wavelength
of 589 nm. We further filtered these molecules to include those with SMILES strings that were val-
idated by RDKit, and those which contained only 1 chiral center. Of these molecules, we removed
duplicate entries if they were reported with different signs of optical rotation (which indicates an
experimental measurement error). If duplicates had consistent signs of optical rotation, we ran-
domly selected one entry. We then removed entries that did not have full stereochemistry specified
in their SMILES strings, since we would later need to generate conformers for each molecule. We
also removed molecules whose non-enantiomeric (e.g., cis/trans) stereoisomers also appeared in the
dataset. We then checked if pairs of enantiomers were both reported in the dataset, and excluded
such pairs if they were reported with the same sign of optical rotation (which indicates an experi-
mental measurement error). For molecules whose opposite enantiomers were not in the dataset, we
artificially generated the opposite enantiomer and assigned it the opposite sign of optical rotation.
This ensures a balanced dataset. We then selected pairs of enantiomers which were reported with ≥
95% enantiomeric excess in order to reduce the risk of experimental noise causing labeling errors.
Lastly, we used RDKit to generate 5 conformers for each enantiomer in the filtered dataset. If RD-
Kit failed to generate a conformer for any pair of enantiomers, both enantiomers in the pair were
removed from the dataset."
D MESSAGE PASSING,0.8786516853932584,"Table 14 reports the distribution of l / d labels in the final dataset, which contains 150,380 conformers
of 30,076 enantiomers (15,038 pairs of enantiomers). We split this dataset into 5 folds for cross-
validation, randomly assigning each pair of enantiomers (with their conformers) to a test set in
one of the five folds. For each fold, we randomly split the remaining (i.e., non-testing) pairs of
enantiomers into 87.5/12.5 training/validation sets. Note that this ensures each fold has 70/10/20
training/validation/test splits, with pairs of enantiomers being assigned to the same data partition
within each fold, and that each pair of enantiomers only appears in one test set across the five folds."
D MESSAGE PASSING,0.8808988764044944,"Select (valid) molecules with 1 chiral center only, MW    564,
containing organic atoms only, with optical rotatory power
reported at 18-30 oC and 589 nm in a pure chloroform solvent."
D MESSAGE PASSING,0.8831460674157303,Reaxys Database
D MESSAGE PASSING,0.8853932584269663,"Remove duplicate entries if reported with different signs of 
optical rotation. Among valid duplicates, randomly select one entry."
D MESSAGE PASSING,0.8876404494382022,"Remove entries that do not have full stereochemistry 
(e.g., cis/trans stereoisomerism) specified. Remove molecules 
whose non-enantiomeric stereoisomers also appear in the dataset."
D MESSAGE PASSING,0.8898876404494382,"If enantiomer pairs both appear in the dataset but are reported
with the same sign of rotation, remove both enantiomers. If a molecule’s 
enantiomer does not appear in the dataset, artificially generate the 
opposite enantiomer and assign it the opposite sign of optical rotation."
D MESSAGE PASSING,0.8921348314606742,"Generate 5 conformers for each enantiomer with RDKit. If RDKit 
cannot generate a conformer, remove the pair of enantiomers."
D MESSAGE PASSING,0.8943820224719101,"Select enantiomer pairs that are reported with               
enantiomeric excess. S H
N S H
N"
D MESSAGE PASSING,0.8966292134831461,"Figure 8: Data extraction, filtering, and generation steps for the l / d classification task."
D MESSAGE PASSING,0.898876404494382,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.9011235955056179,Table 14: Balance of l / d labels in dataset.
D MESSAGE PASSING,0.903370786516854,"l / d Label
# of Conformers
# of Enantiomers"
D MESSAGE PASSING,0.9056179775280899,"l
75190
15038
d
75190
15038"
D MESSAGE PASSING,0.9078651685393259,"Table 15: Frequency of R/S labels amongst enantiomers in the l / d dataset that rotate light in the
positive (d) or negative (l) direction. The balance in R/S labels indicates the lack of empirical
correlation between these two classification schemes. l
d"
D MESSAGE PASSING,0.9101123595505618,"R
7316
7722
S
7722
7316"
D MESSAGE PASSING,0.9123595505617977,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.9146067415730337,"A.7.3
RANKING ENANTIOMERS BY DOCKING SCORES"
D MESSAGE PASSING,0.9168539325842696,"To create the dataset for the ranking enantiomers by their docking scores task, we randomly se-
lected 750 sdf files from the “10 conformers per compound” directory from the PubChem3D FTP
site ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound_3D/, out of the 6199 avail-
able, but excluding the 50 sdf files used previously for the contrastive learning and R/S classification
datasets. After extracting the conformers in each sdf file, we filtered the data to only include con-
formers of 2D graphs for which only 2 stereoisomers (corresponding to pairs of enantiomers with
1 chiral center) appeared in the dataset. As before, we only included enantiomers whose conform-
ers contain at least 1 torsion, as recognized by RDKit. We then filtered enantiomers which have a
molecular weight ≤225 and ≤5 rotatable bonds (as computed by RDKit). This step was designed
to intentionally select small enantiomers which had few rotational degrees of freedom such that the
docking simulations would be less stochastic. We then docked each pair of enantiomers three times
against the protein (PDB ID: 4JBV) in the docking box centered at [10, 16, 61] with (x,y,z) radii of
[10, 14, 12]. In each docking simulation, we increased the exhaustiveness parameter to 24 to help
reduce noise in the resultant docking scores. As a final control for stochasticity in the docking simu-
lations, we also removed pairs of enantiomers for which one or both of the enantiomers had a range
of (top) docking scores > 0.1 kcal/mol across the three simulation trials. Finally, in order to select
pairs of enantiomers which exhibit meaningful differences in docking scores, we filtered the dataset
to only include pairs of enantiomers which had a difference in their best docking scores (across the
three trials) of at least 0.3 kcal/mol. These enantiomers and their PubChem3D conformers were
used as the final dataset. Figure 9 visualizes the full filtering procedure."
D MESSAGE PASSING,0.9191011235955057,"Figure 10 plots the distribution of the number of conformers per enantiomer in the docking dataset.
Figure 11 plots the distribution of the difference in docking scores between pairs of enantiomers in
the docking dataset. We split the full dataset into 70/15/15 training/validation/test sets, assigning
pairs of enantiomers (with their conformers) to the same data partition. The training set contains
234,622 conformers of 48,384 enantiomers (24,192 pairs of enantiomers). The validation set con-
tains 49,878 conformers of 10,368 enantiomers (5184 pairs of enantiomers). The test set contains
50,571 conformers of 10,368 enantiomers (5184 pairs of enantiomers)."
D MESSAGE PASSING,0.9213483146067416,"Select pairs of enantiomers
with only 1 chiral center and which 
have    2 conformers each"
D MESSAGE PASSING,0.9235955056179775,"Select enantiomers with
MW    225 , Nrot     5"
D MESSAGE PASSING,0.9258426966292135,"Dock each enantiomer 3x, and 
select pairs where both enantiomers 
have a range of scores    0.1 kcal/mol"
D MESSAGE PASSING,0.9280898876404494,"Select enantiomer pairs which
have a difference in best 
docking scores    0.3 kcal/mol Br H
N
S N"
D MESSAGE PASSING,0.9303370786516854,"S
H
N
N
H O O O
O F N
Cl S N
N H
N S N
N H
N"
D MESSAGE PASSING,0.9325842696629213,PubChem3D Conformers
D MESSAGE PASSING,0.9348314606741573,Figure 9: Data generation and filtering steps for the ranking enantiomers by docking scores task.
D MESSAGE PASSING,0.9370786516853933,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.9393258426966292,"0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26"
D MESSAGE PASSING,0.9415730337078652,# of Conformers 100 101 102 103 104
D MESSAGE PASSING,0.9438202247191011,# of Stereoisomers
D MESSAGE PASSING,0.946067415730337,Figure 10: Histogram of the number of conformers per enantiomer in the docking dataset.
D MESSAGE PASSING,0.9483146067415731,0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0
D MESSAGE PASSING,0.950561797752809,Difference in Docking Scores (kcal/mol) 0 2000 4000 6000 8000 10000 12000
D MESSAGE PASSING,0.952808988764045,# of Pairs of Enantiomers
D MESSAGE PASSING,0.9550561797752809,"Figure 11: Distribution of the differences in (best) docking scores between pairs of enantiomers in
the docking dataset."
D MESSAGE PASSING,0.9573033707865168,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.9595505617977528,"A.8
ADDITIONAL ABLATIONS"
D MESSAGE PASSING,0.9617977528089887,"It is not strictly necessary that ChIRo learn the weight coefficients c(xy)
ij
in order to distinguish
enantiomers. ChIRo can still learn chiral representations and preserve invariance to internal bond
rotations if each c(xy)
ij
is generated by a network with random, untrained parameters. However, this
is not the case for the phase shifts φixyj, which must be learned for good performance. To demon-
strate this, we perform additional ablations where the feed-forward network fc is not trained, but
rather keep its initial random parameters. We perform the same analysis for fφ to also evaluate
whether ChIRo needs to learn the phase shifts φixyj, or if these can also be generated by a random
network. Table 16 reports the results of not training these MLPs on the otherwise unchanged ChIRo
on the l / d classification task and ranking enantiomers by docking scores task. Overall, learning the
parameters in fc leads to only small (if any) performance gains versus keeping the randomly initial-
ized parameters. On the other hand, learning the parameters in fφ leads to considerable performance
gains versus not learning fφ. This emphasizes that in our case, relying on random noise to break
symmetry between embeddings of enantiomers is insufficient to learn expressive chiral representa-
tions. Rather, ChIRo best models the effects of chirality by learning to break the symmetry through
learned phase shifts in a task-specific manner."
D MESSAGE PASSING,0.9640449438202248,"Table 16: Effects of not learning the parameters in fc and fφ on ChIRo’s performance on the l /
d classification and the ranking enantiomers by docking score tasks. For the original, unablated
ChIRo (first row), mean accuracy and standard deviations on the test sets are reported across 5 folds
(l / d) or three repeated trials (enantiomer ranking). To better account for the impact of random
network initializations, for the ablated models we report the mean accuracy and standard deviations
on the test sets across 3 repeated trials (enantiomer ranking) or the mean, fold-averaged accuracy
(e.g., mean of means) and standard error of this mean across three repeated 5-fold CV trials (l / d
classification)."
D MESSAGE PASSING,0.9662921348314607,"Model Components
Accuracy (%) ↑"
D MESSAGE PASSING,0.9685393258426966,"Learned fc
Learned fφ
l / d
Enantiomer Ranking"
D MESSAGE PASSING,0.9707865168539326,"✓
✓
79.3 ± 0.4
72.0 ± 0.5"
D MESSAGE PASSING,0.9730337078651685,"X
✓
79.4 ± 0.3
71.7 ± 0.2
✓
X
53.7 ± 1.2
68.4 ± 0.7
X
X
50.8 ± 0.8
66.8 ± 0.3"
D MESSAGE PASSING,0.9752808988764045,"A.9
ADDITIONAL EVALUATION OF RANKING ENANTIOMERS BY DOCKING SCORES"
D MESSAGE PASSING,0.9775280898876404,"Because some enantiomers have larger differences in their ground truth docking scores than other
enantiomers, a single ranking accuracy metric may not fully describe the ability of the models to
learn the enantioselectivity of the protein pocket. To evaluate model performance more thoroughly,
we compute ranking accuracies on various subsets of the test set. Figure 12 plots the ranking accu-
racies of ChIRo, SphereNet, Tetra-DMPNN (concatenate) with chiral tags, and DMPNN with chiral
tags when evaluated on subsets of the test data where the difference in ground truth docking scores
are (upper left) greater or equal to a margin, (upper right) less than or equal to a margin, or (bottom)
exactly equal to a margin. The plots suggest that while ChIRo is superior in correctly ranking enan-
tiomers which have relatively small differences in their true docking scores, the differences between
each model become less distinct when ranking enantiomers with larger differences in their ground-
truth docking scores. This may be due to the fact that the docking dataset is imbalanced, skewed
heavily toward enantiomers that have smaller differences in their true docking scores (Figure 11)."
D MESSAGE PASSING,0.9797752808988764,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.9820224719101124,"Figure 12: Ranking accuracy of the predicted conformer-averaged docking scores when evaluated
on subsections of the test set in which the ground truth difference in best docking scores between
enantiomers is (upper left) greater than or equal to the specified margin, (upper right) less than or
equal to the specified margin, or (bottom) equal to the specified margin. Error bars for the models
represent standard deviations in ranking accuracy across three random training/inference trials. Error
bars for the random baseline correspond to the standard deviation of a binomial distribution B(p =
0.5, N), where N is the number of enantiomer pairs in the subset."
D MESSAGE PASSING,0.9842696629213483,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.9865168539325843,"A.10
ANALYZING SE(3)-INVARIANT 3D GNNS WITHOUT INTERROTO-INVARIANCE"
D MESSAGE PASSING,0.9887640449438202,"The qualitative results shown in Figure 3 suggest that an SE(3)-invariant 3D GNN without InterRoto-
invariance, such as SphereNet, will get confused between differences in molecular chirality (e.g.,
inverted chiral centers) with differences in conformational structure (e.g., bond rotations) when
learning chiral-dependent functions. To support this hypothesis with quantitative evidence, Fig-
ure 13 shows the distribution of the fraction of conformers per enantiomer in the test sets of the
l/d classification task which were predicted to have the same sign of optical rotation by SphereNet
and ChIRo, irrespective of the actual accuracy of the predicted class labels. SphereNet predicts the
same label for each of the 5 RDKit-generated conformers per enantiomer for only ∼32% of the
enantiomers in the test sets, compared to ∼93% for ChIRo. When only trained on 1 conformer
per enantiomer, SphereNet’s labeling consistency drops to 14%, whereas ChIRo’s labeling consis-
tency remains roughly the same at ∼92%. ChIRo’s dramatically increased consistency in labeling
conformers sharing the same chiral molecular identity compared to SphereNet is a direct result of
ChIRo’s InterRoto-invariance. Note that ChIRo’s consistency is not perfectly 100% because the
RDKit-generated conformers have conformational differences not associated with simple bond ro-
tations. For instance, two conformers can slightly differ in their bond distances and bond angles,
perturbations to which ChIRo is not invariant."
D MESSAGE PASSING,0.9910112359550561,"Moreover, manually rotating bonds near the chiral center (similar to the bottom row in Figure 3)
causes SphereNet to get confused when predicting signs of optical rotation for these rotated con-
formers, even for enantiomers (in the test-set) whose conformers were all originally classified cor-
rectly by SphereNet (Figure 14). Since ChIRo is invariant to such bond rotations (Figure 3), it is not
susceptible to this particular type of confusion, and therefore has increased ability to generalize to
unseen chiral conformers."
D MESSAGE PASSING,0.9932584269662922,"Figure 13: Distribution of the fraction of conformers per enantiomer predicted to have the same class
label for the l/d classification task across the test sets in each of the five folds, for both SphereNet
and ChIRo when trained on all conformers in the training set (top row) or on a single conformer per
enantiomer (bottom row). Note that for the l/d classification task, there are exactly five conformers
per enantiomer in the test splits of each fold."
D MESSAGE PASSING,0.9955056179775281,Published as a conference paper at ICLR 2022
D MESSAGE PASSING,0.9977528089887641,"Figure 14: Confusion matrices indicating SphereNet’s (mis)classification of conformers of three
pairs of enantiomers in the l/d classification task’s test set (of the first fold) whose highlighted
bonds are each rotated in increments of 12◦(left and center) or 30◦(right). These three pairs
of enantiomers were specifically chosen for this confusion analysis because SphereNet correctly
classifies all of their original (non-rotated) conformers."
