Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0032679738562091504,"Conditional diffusion models have shown remarkable success in visual content
generation, producing high-quality samples across various domains, largely due to
classifier-free guidance (CFG). Recent attempts to extend guidance to unconditional
models have relied on heuristic techniques, resulting in suboptimal generation qual-
ity and unintended effects. In this work, we propose Smoothed Energy Guidance
(SEG), a novel training- and condition-free approach that leverages the energy-
based perspective of the self-attention mechanism to enhance image generation.
By defining the energy of self-attention, we introduce a method to reduce the
curvature of the energy landscape of attention and use the output as the uncondi-
tional prediction. Practically, we control the curvature of the energy landscape
by adjusting the Gaussian kernel parameter while keeping the guidance scale pa-
rameter fixed. Additionally, we present a query blurring method that is equivalent
to blurring the entire attention weights without incurring quadratic complexity
in the number of tokens. In our experiments, SEG achieves a Pareto improve-
ment in both quality and the reduction of side effects. The code is available at
https://github.com/SusungHong/SEG-SDXL."
INTRODUCTION,0.006535947712418301,"1
Introduction"
INTRODUCTION,0.00980392156862745,"Diffusion models [12, 45, 46] have emerged as a promising tool for visual content generation,
producing high-quality and diverse samples across various domains, including image [38, 40, 42,
8, 13, 30, 2, 24, 9, 29, 34, 33, 4, 41, 5, 20, 22], video [11, 50, 23, 18, 15, 3, 19, 44], and 3D
generation [36, 27, 6, 26, 49, 43, 48, 16]. The success of these models can be largely attributed to
the use of classifier-free guidance (CFG) [14], which enables sampling from a sharper distribution,
resulting in improved sample quality. However, CFG is not applicable to unconditional image
generation, where no specific conditions are provided, creating a disparity between the capabilities
of text-conditioned sampling and sampling without text. This disparity results in a restriction in
application, e.g., synthesizing images with ControlNet[51] without a text prompt (see the last two
columns of Fig. 1)."
INTRODUCTION,0.013071895424836602,"Recent literature [17, 1] has attempted to decouple CFG and image quality by extending guidance
to general diffusion models, leveraging their inherent representations [25, 32, 17]. Self-attention
guidance (SAG) [17] proposes leveraging the intermediate self-attention map of diffusion models to
blur the input pixels and provide guidance, while perturbed attention guidance (PAG) [1] perturbs the
attention map itself by replacing it with an identity attention map. Despite these efforts, these methods
rely on heuristics to make perturbed predictions, resulting in unintended effects such as smoothed-out
details, saturation, color shifts, and significant changes in the image structure when given a large
guidance scale. Notably, the mathematical underpinnings of these unconditional guidance approaches
are not well elucidated."
INTRODUCTION,0.016339869281045753,∗This work was mostly done at Korea University.
INTRODUCTION,0.0196078431372549,"Figure 1: Teaser. (a) Images sampled from vanilla SDXL [35] without any guidance. (b) Images
sampled with Smoothed Energy Guidance (Ours). ∅denotes that there is no condition given. With
various input conditions, and even without any, SEG supports the diffusion model in generating
plausible and high-quality images without any training."
INTRODUCTION,0.02287581699346405,"In this work, we approach the objective from an energy-based perspective of the self-attention
mechanism, which has been previously explored based on its close connection to the Hopfield
energy [39, 31, 7]. Specifically, we start from the definition of the energy of self-attention, where
performing a self-attention operation is equivalent to taking a gradient step. In light of this, we
propose a tuning- and condition-free method that reduces the curvature of the underlying energy
function by directly blurring the attention weights, and then leverages the output as the negative
prediction. We call this method Smoothed Energy Guidance (SEG)."
INTRODUCTION,0.026143790849673203,"SEG does not merely rely on the guidance scale parameter that cause side effects when its value
becomes large. Instead, we can continuously control the original and maximally attenuated curvature
of the energy landscape behind the self-attention by simply adjusting the parameter of the Gaussian
kernel, with the guidance scale parameter fixed. Additionally, we introduce a novel query blurring
technique, which is equivalent to blurring the entire attention weights without incurring quadratic
cost in the number of tokens."
INTRODUCTION,0.029411764705882353,"We validate the effectiveness of SEG throughout the various experiments without and with text
conditions, and ControlNet [51] trained on canny and depth maps. Based on the attention modulation,
SEG results in less structural change from the original prediction compared to previous approaches [17,
1], while achieving better sample quality."
PRELIMINARIES,0.032679738562091505,"2
Preliminaries"
DIFFUSION MODELS,0.03594771241830065,"2.1
Diffusion models"
DIFFUSION MODELS,0.0392156862745098,"Diffusion models [12, 45, 46] are a class of generative models that generate data through an iterative
denoising process. The process of adding noise to an image x over time t ∈[0, T] is governed by the
forward stochastic differential equation (SDE):"
DIFFUSION MODELS,0.042483660130718956,"dx = f(x, t)dt + g(t)dw,
(1)"
DIFFUSION MODELS,0.0457516339869281,"where f and g are predefined functions that determine the manner in which the noise is added, and
dw denotes a standard Wiener process."
DIFFUSION MODELS,0.049019607843137254,"Correspondingly, the denoising process can be described by the reverse SDE:"
DIFFUSION MODELS,0.05228758169934641,"dx = [f(x, t) −g(t)2∇x log pt(x)]dt + g(t)d ¯w,
(2)"
DIFFUSION MODELS,0.05555555555555555,"where ∇x log pt(x) represents the score of the noisy data distribution and d ¯w denotes the standard
Wiener process for the reversed time."
DIFFUSION MODELS,0.058823529411764705,"Diffusion models are trained to approximate the score function with sθ(x, t) ≈∇x log pt(x). To
generate an image based on a condition c, e.g., a class label or text, one simply needs to train diffusion
models to approximate the conditional score function with sθ(x, t, c) ≈∇x log pt(x|c) and replace
∇x log pt(x) with it in the denoising process. To enhance the quality and faithfulness of the generated
samples, classifier-free guidance (CFG) [14] is widely adopted. Accordingly, the reverse process
becomes:"
DIFFUSION MODELS,0.06209150326797386,"dx = [f(x, t) −g(t)2(γcfgsθ(x, t, c) −(γcfg −1)sθ(x, t))]dt + g(t)d ¯w.
(3)"
DIFFUSION MODELS,0.06535947712418301,"Here, sθ(x, t) is learned by dropping the label by a certain proportion, and γcfg is a hyperparameter
that controls the strength of the guidance. Intuitively, CFG helps us to sample from sharper distribution
by conditioning on a class label or text."
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.06862745098039216,"2.2
Energy-based view of attention mechanism"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.0718954248366013,"The attention mechanism [47], which has been widely adopted in diffusion models [12], has been
interpreted through the lens of energy-based models (EBMs) [31, 39, 7], especially through its close
connection with the Hopfield energy [7, 39]. In the modern (continuous) Hopfield network, the
attention operation can be derived based on the concave-convex procedure (CCCP) from the following
energy function [39]:"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.07516339869281045,E(ξ) = −lse(Xξ⊤) + 1
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.0784313725490196,"2ξξ⊤,
(4)"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.08169934640522876,"where ξ ∈R1×d, X ∈RN×d, and lse stands for the log-sum-exp function, defined as lse(v) :="
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.08496732026143791,"log
PN
i=1 evi

. The quadratic term acts as a regularizer to prevent ξ from exploding [39], while"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.08823529411764706,−lse(Xξ⊤) penalizes misalignment between X and ξ.
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.0915032679738562,"Mathematically, it turns out that the attention mechanism is equivalent to the update rule of the
modern Hopfield network [7, 39]. Specifically, inspired by the Hopfield energy in (4), and noticing
that the first term depends on the attention weights, we propose the following energy function for
entire self-attention weights in diffusion models:"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.09477124183006536,"Definition 2.1 (Energy Function for Self-Attention). Let Q ∈R(HW )×d be a matrix of query vectors
and K ∈R(HW )×d be a matrix of key vectors, where H, W, and d represent the height, width, and
dimension, respectively. Let A ∈R(HW )×(HW ) := QK⊤. The energy function with respect to entire
self-attention weights in diffusion models is defined as:"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.09803921568627451,"E(A) := H
X i=1 W
X"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.10130718954248366,"j=1
E′(a:(i,j)),
E′(a) := −lse (a) = −log H
X k=1 W
X"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.10457516339869281,"l=1
ea(k,l)
! .
(5)"
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.10784313725490197,"Note that to explicitly denote the spatial dimension, we use the subscript (x, y) to represent the index
of a row or column of the matrices. Despite using the definition in (5) for the rest of the paper for
simplicity, we additionally discuss the dual case, where we use the swapped indexing, in Appendix B."
ENERGY-BASED VIEW OF ATTENTION MECHANISM,0.1111111111111111,"This view leads us to an important intuition: the attention operation can be seen as a minimization
step on the energy landscape, considering that the first derivative represents the softmax operation
which also appears in the attention operation. Building upon this intuition, we argue that Gaussian
blurring on the attention weights modulates the underlying landscape to have less curvature, and we
demonstrate this in the following sections by analyzing the second derivatives."
METHOD,0.11437908496732026,"3
Method"
METHOD,0.11764705882352941,"Our aim is to theoretically derive the effect of Gaussian blur applied on the attention weights, which
in the end attenuates the curvature of the underlying energy function. Then, utilizing this fact, we
develop attention-based drop-in diffusion guidance that enhances the quality of the generated samples,
regardless of whether an explicit condition is given. In Section 3.1, we claim some useful properties
of Gaussian blur: that it preserves mean, reduces variance, and thus decreases the lse value. In"
METHOD,0.12091503267973856,"Section 3.2, we find that the curvature of the energy landscape is attenuated by the attention blur
operation, leading naturally to a blunter prediction for guidance. And finally, in Section 3.3, built upon
this fact, we define Smoothed Energy Guidance (SEG) and propose the equivalent query blurring
method, which can perform attention blurring while avoiding quadratic complexity in the number of
tokens."
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.12418300653594772,"3.1
Gaussian blur to attention weights"
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.12745098039215685,"In this section, we derive some important properties of the Gaussian blur with the aim of figuring out
the variation of the energy landscape. To this end, we start from some mathematical underpinnings
on applying Gaussian blur to attention weights."
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.13071895424836602,"A 2D Gaussian filter is a convolution kernel that uses a 2D Gaussian function to assign weights to
neighboring pixels. The 2D Gaussian function is defined as:"
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.13398692810457516,"G(x, y) =
1
2πσ2 e−
(x−µx)2+(y−µy)2 2σ2"
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.13725490196078433,"where µx and µy are the means in the x and y directions, and σ is the standard deviation. The
2D Gaussian filter possesses symmetry, i.e., G(x, y) = G(−x, −y), and normalization, i.e.,
RR
G(x, y)dxdy = 1. In practice, we use a discretized version of the Gaussian filter with a fi-
nite kernel size depending on σ, normalized to sum to 1.
Lemma 3.1. Spatially applying a 2D Gaussian blur to the attention weights a := Qk⊤preserves
the average Ei,j[a(i,j)]. In addition, the variance monotonically decreases every time we apply the
Gaussian blur."
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.14052287581699346,"Proof sketch. Applying a 2D Gaussian filter to the attention weights a(i,j) yields the blurred values
˜a(i,j):"
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.1437908496732026,"˜a(i,j) = k
X m=−k k
X"
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.14705882352941177,"n=−k
G(m, n) · a(i+m,j+n)"
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.1503267973856209,"where k is the filter size, G(m, n) is the Gaussian filter value at position (m, n), and a(i+m,j+n) is
the attention weight at position (i + m, j + n). Since the Gaussian filter is symmetric and normalized,
it can be shown that the mean of the blurred attention weights is equal to the mean of the original
attention weights. Similarly, we can show that the variance monotonically decreases when we apply
a 2D Gaussian filter. See Appendix A.1 for the complete proof."
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.15359477124183007,"Note that this fact also implies that blurring with a Gaussian filter with a larger standard deviation
causes a greater decrease in the variance of attention weights. This is because a Gaussian filter with
a larger standard deviation can always be represented as a convolution of two filters with smaller
standard deviations, due to the associativity of the convolution operation."
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.1568627450980392,"Finally, we show that applying a 2D Gaussian blur to attention weights increases the lse value in (5),
i.e., increases the energy in (5). This provides a bit of intuition about the underlying energy landscape,
yet it is more prominently utilized in the claims in the following sections.
Lemma 3.2. Applying a 2D Gaussian blur to attention weights a := Qk⊤increases the lse term
when we consider the second-order Taylor series approximation of the exponential function around
the mean µ := Ei,j[a(i,j)]. Consequently, the maximum is achieved when the attention is uniform,
i.e., a(i,j) = a(k,l) ∀i, j, k, l. This corresponds to the case when we apply the Gaussian blur with
σ →∞."
GAUSSIAN BLUR TO ATTENTION WEIGHTS,0.16013071895424835,"Proof sketch. Applying the second-order Taylor series approximation around the mean µ, and using
Proposition 3.1, we show that the second-order approximation of lse(a) is larger than or equal to that
of lse(˜a). Subsequently, we introduce Lagrange multipliers to find the maximum, which gives us the
result, a(i,j) = a(k,l) ∀i, j, k, l. We leave the full proof in Appendix A.2."
ANALYSIS OF THE ENERGY LANDSCAPE,0.16339869281045752,"3.2
Analysis of the energy landscape"
ANALYSIS OF THE ENERGY LANDSCAPE,0.16666666666666666,"In this section, we demonstrate that applying a 2D Gaussian blur to the attention weights before the
softmax operation results in computing the updated value with reduced curvature of the underlying
energy function. To this end, we analyze the Gaussian curvature before and after blurring the attention
weights. This is closely related to the Hessian of the energy function."
ANALYSIS OF THE ENERGY LANDSCAPE,0.16993464052287582,"Theorem 3.1. Let the attention weights be defined as a := Qk⊤. Consider the energy function in
(5). Then, applying a Gaussian blur to the attention weights a before the softmax operation results in
the attenuation of the Gaussian curvature of the underlying energy function where gradient descent
is performed."
ANALYSIS OF THE ENERGY LANDSCAPE,0.17320261437908496,"Proof sketch. Let H denote the Hessian of the original energy function, i.e., the derivative of the
negative softmax, and ˜H denote the Hessian of the new energy function associated with blurred
attention weights. Furthermore, let bij denote the i-th row, j-th column entry in the Toeplitz matrix
B representing the Gaussian blur. Calculating the derivatives, we have the elements of the Hessians,
hij = (ξ(a)i −δij)ξ(a)j and ˜hij = (ξ(˜a)i −δij)ξ(˜a)jbij. Using Lemmas 3.1 and 3.2 and under
reasonable assumptions, we observe that | det(H)| > | det( ˜H)|, which implies that the minimization
step is performed on a smoother energy landscape with attenuated Gaussian curvature. The full proof
is in Appendix A.3."
ANALYSIS OF THE ENERGY LANDSCAPE,0.17647058823529413,"To provide more intuition about what is actually happening and how we utilize this property in
the later section, it is intriguing to consider the attenuating effect on the curvature in analogy to
classifier-free guidance (CFG). CFG uses the difference between the prediction based on the sharper
conditional distribution and the prediction based on the smoother unconditional distribution to guide
the sampling process. By analogy, we propose a method to make the landscape of the energy function
smoother to guide the sampling process, as opposed to the original (sharper) energy landscape."
ANALYSIS OF THE ENERGY LANDSCAPE,0.17973856209150327,"From a probabilistic perspective, the energy is associated with the likelihood of the attention weights
in terms of the Boltzmann distribution conditioned on a given configuration, i.e., the feature map.
Blurring the attention weights diminishes this likelihood as shown in Lemma 3.2, and also reduces
the curvature of the distribution as shown in Theorem 3.1."
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.1830065359477124,"3.3
Smoothed energy guidance for diffusion models"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.18627450980392157,"Based on the above observation that the Gaussian blur on attention weights attenuates the curvature
of the energy function, we propose Smoothed Energy Guidance (SEG) in this section. For brevity, we
redefine the unconditional score prediction as sθ(x, t), and the unconditional score prediction with
the energy curvature reduced as ˜sθ(x, t). Specifically, ˜sθ(x, t) is the prediction with the attention
weights blurred using a 2D Gaussian filter G with the standard deviation σ. We formulate the process
as:
(QK⊤)seg = G ∗(QK⊤),
(6)"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.1895424836601307,"where ∗denotes the 2D convolution operator. Then, we replace the original attention weights with
(QK⊤)seg and compute the final value as in ordinary self-attention."
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.19281045751633988,"For practical purposes when the number of tokens is large, we propose an efficient computation of
(6) using the property of a linear map, since the convolution operation is linear. Concretely, blurring
queries is exactly the same as blurring the entire attention weights, and we propose the following
proposition to justify our claim.
Proposition 3.1. Let Q and K be the query and key matrices in self-attention, and let G be a 2D
Gaussian filter. Blurring the attention weights with G is equivalent to blurring the query matrix Q
with G and then computing the attention weights."
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.19607843137254902,"Proof. Since the convolution operation is linear, we can always find a Toeplitz matrix B such that:"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.19934640522875818,"G ∗(QK⊤) = B(QK⊤),
(7)"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.20261437908496732,"where ∗denotes the 2D convolution operation. Using the properties of matrix multiplication, we can
rewrite (7) as:
B(QK⊤) = (BQ)K⊤= (G ∗Q)K⊤.
(8)"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.20588235294117646,"Finally, SEG is formulated as follows:"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.20915032679738563,"dx = [f(x, t) −g(t)2(γsegsθ(x, t) −(γseg −1)˜sθ(x, t))]dt + g(t)d ¯w,
(9)"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.21241830065359477,where γseg denotes the guidance scale of SEG.
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.21568627450980393,Figure 2: Unconditional generation using SEG.
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.21895424836601307,Figure 3: Text-conditional generation using SEG.
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.2222222222222222,"In a straightforward manner, as SEG does not rely on external conditions, it can be used for conditional
sampling strategies such as CFG [14] and ControlNet [51]. For the combinatorial sampling with
CFG, following [17], we simply extend (9) for improved conditional sampling with both SEG and
CFG as follows:"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.22549019607843138,"dx = [f(x, t) −g(t)2((1 −γcfg + γseg)sθ(x, t) + γcfgsθ(x, t, c) −γseg˜sθ(x, t))]dt + g(t)d ¯w, (10)"
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.22875816993464052,"which is an intuitive result, as the update rule moves x towards the conditional prediction while
keeping it far from the prediction with blurred attention weights."
SMOOTHED ENERGY GUIDANCE FOR DIFFUSION MODELS,0.23202614379084968,"We are likely to get a result with saturation when using a large guidance scale, such as with classifier-
free guidance (CFG) [14], self-attention guidance (SAG) [17], and perturbed attention guidance
(PAG) [1]. This is a significant caveat since we need to increase the scale to achieve a maximum
effect with these methods. Contrary to this, we can fix the scale of SEG as justified in Sec. 5.5 and
control its maximum effect through σ of the Gaussian blur, making the choice more flexible. For σ,
two extreme cases are recognized. If σ →0, the blurred attention weights remain the same as the
original, while when σ →∞, the attention weights merely adopt a single mean value across spatial
axes. We find that even the latter extreme case results in a high-quality outcome, corroborating that
we can control the quality to the limit without saturation."
DISCUSSION ON RELATED WORK,0.23529411764705882,"4
Discussion on related work"
DISCUSSION ON RELATED WORK,0.238562091503268,"Classifier-free guidance (CFG) [14], first proposed as a replacement for classifier guidance (CG) [8]
is controlled by a scale parameter. The higher we set classifier-free guidance, the more we get faithful,
high-quality images. However, it requires external labels, such as text [30] or class [8] labels, making
it impossible to apply to unconditional diffusion models. Also, it requires specific traning procedure
with label dropping and it is known that high CFG causes saturation [42]."
DISCUSSION ON RELATED WORK,0.24183006535947713,"Figure 4: Conditional generation using ControlNet [51] and SEG.
Table 1: Quantitative comparison of SEG with vanilla SDXL [35], SAG [17], and PAG [1] for
unconditional generation."
DISCUSSION ON RELATED WORK,0.24509803921568626,"Metric
Vanilla SDXL [35]
SAG [17]
PAG [1]
SEG
SEG
σ = 10
σ →∞
FID↓
129.496
106.683
105.271
95.316
88.215
LPIPSvgg ↓
-
0.706
0.542
0.522
0.536
LPIPSalex ↓
-
0.644
0.472
0.454
0.472"
DISCUSSION ON RELATED WORK,0.24836601307189543,"Tackling the caveats of CFG, unconditional approaches such as self-attention guidance (SAG) [17]
and perturbed attention guidance (PAG) [1] have been proposed. SAG selectively blurs images with
the mask obtained from the attention map and guides the generation process given the prediction.
This indirect approach causes saturation and noisy images when given a large guidance scale, leading
to the selection of a guidance scale less than or equal to 1. PAG guides images using prediction with
identity attention, where the attention map is an identity matrix. However, the reliance on heuristics
to make perturbed predictions results in unintended side effects. As an example of the side effects
of replacing the attention map with identity attention, PAG changes the visual structure and color
distribution of an image, as evidenced in Figs. 5, 8, and 9."
DISCUSSION ON RELATED WORK,0.25163398692810457,"Contrary to these, we control the effect of SEG through the standard deviation of the Gaussian filter,
σ. Moreover, while being theory-inspired, SEG is relatively free from unintended effects. In the
following section, we corroborate our claim with extensive experiments."
EXPERIMENTS,0.2549019607843137,"5
Experiments"
IMPLEMENTATION DETAILS,0.2581699346405229,"5.1
Implementation details"
IMPLEMENTATION DETAILS,0.26143790849673204,"We build upon the current open-source state-of-the-art diffusion model, Stable Diffusion XL
(SDXL) [35], as our baseline, and do not change the configuration. To sample with SEG, we
choose the same attention layers (mid-blocks) and guidance scale as PAG [1]. For SEG and PAG
sampling, we use the Euler discrete scheduler [21], while for SAG [17], we instead use the DDIM
scheduler [45] since the current implementation of SAG does not support the Euler discrete sampler.
For SAG and PAG, we use the same configurations they used in the experiments with the previous
version of Stable Diffusion, with guidance scales of 1.0 and 3.0, respectively. We set γseg to 3.0,
except in the ablation study."
IMPLEMENTATION DETAILS,0.2647058823529412,"Figure 5: Qualitative comparison of SEG with vanilla SDXL [35], SAG [17], and PAG [1]."
IMPLEMENTATION DETAILS,0.2679738562091503,Table 2: Text-conditional sampling with different σ.
IMPLEMENTATION DETAILS,0.27124183006535946,"Metric
Vanilla SDXL [35]
SEG
1
2
5
10
∞
FID↓
53.423
48.284
41.784
33.819
29.325
26.169
CLIP Score↑
0.271
0.273
0.278
0.285
0.290
0.292
LPIPSvgg ↓
-
0.361
0.410
0.449
0.472
0.493
LPIPSalex ↓
-
0.295
0.347
0.390
0.416
0.440"
METRICS,0.27450980392156865,"5.2
Metrics"
METRICS,0.2777777777777778,"We use various metrics to evaluate quality (FID [10] and CLIP score [37], calculated with 30k
references from the MS-COCO 2014 validation set [28]) and to assess the extent of change due to
applied guidance (LPIPSvgg, alex [52]). The latter metric, calculated using the outputs of vanilla SDXL,
measures the extent of side effects by comparing guided images to their unguided counterparts."
CONTROLLING IMAGE GENERATION WITH THE STANDARD DEVIATION,0.28104575163398693,"5.3
Controlling image generation with the standard deviation"
CONTROLLING IMAGE GENERATION WITH THE STANDARD DEVIATION,0.28431372549019607,"In this section, our aim is to demonstrate that with SEG, we can sample plausible images using vanilla
SDXL [35] under various conditions and even without any conditions, as demonstrated in Fig. 1.
Furthermore, without the risk of saturation, we can control the quality and plausibility of the samples.
For the results, we use σ ∈{1, 2, 5, 10}. Additionally, as mentioned in Sec. 3.3, we present two
extreme cases, σ →0 (vanilla SDXL) and σ →∞(uniform queries)."
CONTROLLING IMAGE GENERATION WITH THE STANDARD DEVIATION,0.2875816993464052,"Unconditional generation
In this section, our aim is to demonstrate that with SEG, we can sample
plausible images from the unconditional mode of the vanilla SDXL, which was originally trained
on a large-scale text-to-image dataset. The results are presented in Fig. 1, Fig. 2, and Table 1. The
results show a clear tendency to draw higher quality samples by utilizing the differences between the
two energy landscapes with different curvatures derived from self-attention mechanisms."
CONTROLLING IMAGE GENERATION WITH THE STANDARD DEVIATION,0.2908496732026144,"In Fig. 2 and Fig. 13, we show the effectiveness of generating more plausible images, while vanilla
SDXL is unable to generate high-quality images without any conditions. The results show a clear ten-
dency to draw higher quality samples by utilizing the differences between the two energy landscapes
with different curvatures derived from self-attention mechanisms. When σ is larger, the definition
and expression of the samples improve, as the difference in curvature becomes more pronounced."
CONTROLLING IMAGE GENERATION WITH THE STANDARD DEVIATION,0.29411764705882354,"Conditional generation
In Figs. 3, 4, 10, 11, and 14, we display sampling results conditioned
on text, Canny, and depth map. Using text (Fig. 3), the vanilla SDXL without CFG is unable to
generate high-quality images and produces noisy results. Canny and depth map conditioning on
SDXL (Fig. 4, 10, and 11) is achieved through ControlNet [51], trained on such maps. The results
show that SEG enhances the quality and fidelity of the generated images while preserving the textual
and structural information provided by the conditioning inputs. Notably, as σ increases, the generated
images exhibit improved definition and quality without introducing significant artifacts or deviations
from the original condition. The combination with higher CFG scales is shown in Figs. 15–19."
CONTROLLING IMAGE GENERATION WITH THE STANDARD DEVIATION,0.2973856209150327,"In Table 2, we show the quantitative results for text-conditional generation in terms of σ. We observe
a clear trade-off between image quality (represented by FID and CLIP score) and the deviation from
the original sample (represented by LPIPS). We sample 30k images for each σ to compute the metrics."
COMPARISON WITH PREVIOUS METHODS,0.3006535947712418,"5.4
Comparison with previous methods"
COMPARISON WITH PREVIOUS METHODS,0.30392156862745096,"Since the results are visually favorable when we use σ = 10 and σ →∞, and they are the best
in terms of CLIP score and FID, respectively, we adopt those configurations for comparison of
unconditional guidance methods. The results are presented in Figs. 5, 8, 9, and Table 1. Notably, our
method achieves better image quality in terms of FID, while remaining similar to the original output
of vanilla SDXL as measured by LPIPS, implying a Pareto improvement."
ABLATION STUDY,0.30718954248366015,"5.5
Ablation study"
ABLATION STUDY,0.3104575163398693,"1
10
25 30 35 40 45 50 55 FID"
ABLATION STUDY,0.3137254901960784,FID Scores
ABLATION STUDY,0.31699346405228757,seg = 1
ABLATION STUDY,0.3202614379084967,seg = 3
ABLATION STUDY,0.3235294117647059,seg = 5
ABLATION STUDY,0.32679738562091504,"1
10
0.270 0.275 0.280 0.285 0.290"
ABLATION STUDY,0.3300653594771242,CLIP Score
ABLATION STUDY,0.3333333333333333,CLIP Scores
ABLATION STUDY,0.3366013071895425,seg = 1
ABLATION STUDY,0.33986928104575165,seg = 3
ABLATION STUDY,0.3431372549019608,seg = 5
ABLATION STUDY,0.3464052287581699,Figure 6: Ablation study on γseg and σ.
ABLATION STUDY,0.34967320261437906,"In this section, we address two parameters, γseg and
σ, and justify that fixing γseg is a reasonable choice.
In Fig. 6, we present the results from our testing.
The results reveal that increasing γseg does not gen-
erally lead to improved sample quality in terms of
FID and CLIP score, due to various issues such as
saturation. In contrast, increasing σ tends to im-
prove sample quality and plausibility. This supports
the claim that image quality should be controlled by
σ, instead of the guidance scale parameter. We sam-
ple 30k images for each combination to calculate
the metrics."
ABLATION STUDY,0.35294117647058826,"6
Conclusion, limitations and societal impacts"
ABLATION STUDY,0.3562091503267974,"Conclusion
We introduce Smoothed Energy Guidance (SEG), a novel training- and condition-free
guidance method for image generation with diffusion models. The key advantages of SEG lie in
its flexibility and the theoretical foundation, allowing us to significantly enhance sample quality
without side effects by adjusting the standard deviation of the Gaussian filter. We hope our method
inspires further research on improving generative models, and extending the approach beyond image
generation, for example, to video or natural language processing."
ABLATION STUDY,0.35947712418300654,"Limitations and societal impacts
The paper proposes guidance to enhance quality outcomes.
Consequently, the attainable quality of our approach is contingent upon the baseline model employed.
Furthermore, the application of SEG to temporal attention mechanisms in video or multi-view
diffusion models is not addressed, remaining a promising avenue for future research. It is important to
note that the improvements achieved through this method may potentially lead to unintended negative
societal consequences by inadvertently amplifying existing stereotypes or harmful biases."
ABLATION STUDY,0.3627450980392157,Acknowledgements
ABLATION STUDY,0.3660130718954248,"I would like to express my gratitude to Yong-Hyun Park, Junha Hyung, and Donghoon Ahn for their
valuable feedback and insights. Their thoughtful comments and suggestions have been instrumental
in improving this work."
REFERENCES,0.369281045751634,References
REFERENCES,0.37254901960784315,"[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee
Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention
guidance. arXiv preprint arXiv:2403.17377, 2024."
REFERENCES,0.3758169934640523,"[2] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, and Christian Etmann. Conditional image
generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021."
REFERENCES,0.3790849673202614,"[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and
Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563–
22575, 2023."
REFERENCES,0.38235294117647056,"[4] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Retrieval-augmented
diffusion models. Advances in Neural Information Processing Systems, 35:15309–15324, 2022."
REFERENCES,0.38562091503267976,"[5] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting,
and Apolinário Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8861–8870, 2024."
REFERENCES,0.3888888888888889,"[6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance
for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 22246–22256, 2023."
REFERENCES,0.39215686274509803,"[7] Mete Demircigil, Judith Heusel, Matthias Löwe, Sven Upgang, and Franck Vermet. On a model of
associative memory with huge storage capacity. Journal of Statistical Physics, 168:288–299, 2017."
REFERENCES,0.3954248366013072,"[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in
neural information processing systems, 34:8780–8794, 2021."
REFERENCES,0.39869281045751637,"[9] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for
controllable image generation. Advances in Neural Information Processing Systems, 36:16222–16239,
2023."
REFERENCES,0.4019607843137255,"[10] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.40522875816993464,"[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video
generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022."
REFERENCES,0.4084967320261438,"[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural
information processing systems, 33:6840–6851, 2020."
REFERENCES,0.4117647058823529,"[13] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.
Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research,
23(47):1–33, 2022."
REFERENCES,0.4150326797385621,"[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,
2022."
REFERENCES,0.41830065359477125,"[15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.
Video diffusion models. Advances in Neural Information Processing Systems, 35:8633–8646, 2022."
REFERENCES,0.4215686274509804,"[16] Susung Hong, Donghoon Ahn, and Seungryong Kim. Debiasing scores and prompts of 2d diffusion for
view-consistent text-to-3d generation. Advances in Neural Information Processing Systems, 36, 2024."
REFERENCES,0.42483660130718953,"[17] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion
models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 7462–7471, 2023."
REFERENCES,0.42810457516339867,"[18] Susung Hong, Junyoung Seo, Heeseong Shin, Sunghwan Hong, and Seungryong Kim.
Direct2v:
Large language models are frame-level directors for zero-shot text-to-video generation. arXiv preprint
arXiv:2305.14330, 2023."
REFERENCES,0.43137254901960786,"[19] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, and Sibei Yang. Free-bloom: Zero-shot
text-to-video generator with llm director and ldm animator. Advances in Neural Information Processing
Systems, 36, 2024."
REFERENCES,0.434640522875817,"[20] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face
generation and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6080–6090, 2023."
REFERENCES,0.43790849673202614,"[21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based
generative models. Advances in Neural Information Processing Systems, 35:26565–26577, 2022."
REFERENCES,0.4411764705882353,"[22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal
Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 6007–6017, 2023."
REFERENCES,0.4444444444444444,"[23] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang,
Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot
video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
15954–15964, 2023."
REFERENCES,0.4477124183006536,"[24] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for
robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 2426–2435, 2022."
REFERENCES,0.45098039215686275,"[25] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space.
arXiv preprint arXiv:2210.10960, 2022."
REFERENCES,0.4542483660130719,"[26] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion
for consistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023."
REFERENCES,0.45751633986928103,"[27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,
Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300–309,
2023."
REFERENCES,0.46078431372549017,"[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:
13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages
740–755. Springer, 2014."
REFERENCES,0.46405228758169936,"[29] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual
generation with composable diffusion models. In European Conference on Computer Vision, pages
423–439. Springer, 2022."
REFERENCES,0.4673202614379085,"[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided
diffusion models. arXiv preprint arXiv:2112.10741, 2021."
REFERENCES,0.47058823529411764,"[31] Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, and Jong Chul Ye. Energy-based cross
attention for bayesian context update in text-to-image diffusion models. Advances in Neural Information
Processing Systems, 36, 2024."
REFERENCES,0.4738562091503268,"[32] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent
space of diffusion models through the lens of riemannian geometry. Advances in Neural Information
Processing Systems, 36:24129–24142, 2023."
REFERENCES,0.477124183006536,"[33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023."
REFERENCES,0.4803921568627451,"[34] Hao Phung, Quan Dao, and Anh Tran. Wavelet diffusion models are fast and scalable image generators.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10199–
10208, 2023."
REFERENCES,0.48366013071895425,"[35] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,
and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv
preprint arXiv:2307.01952, 2023."
REFERENCES,0.4869281045751634,"[36] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.
arXiv preprint arXiv:2209.14988, 2022."
REFERENCES,0.49019607843137253,"[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR,
2021."
REFERENCES,0.4934640522875817,"[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022."
REFERENCES,0.49673202614379086,"[39] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler,
Lukas Gruber, Markus Holzleitner, Milena Pavlovi´c, Geir Kjetil Sandve, et al. Hopfield networks is all
you need. arXiv preprint arXiv:2008.02217, 2020."
REFERENCES,0.5,"[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 10684–10695, 2022."
REFERENCES,0.5032679738562091,"[41] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and
Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference
proceedings, pages 1–10, 2022."
REFERENCES,0.5065359477124183,"[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-
image diffusion models with deep language understanding. Advances in neural information processing
systems, 35:36479–36494, 2022."
REFERENCES,0.5098039215686274,"[43] Junyoung Seo, Susung Hong, Wooseok Jang, Inès Hyeonsu Kim, Minseop Kwak, Doyup Lee, and Seungry-
ong Kim. Retrieval-augmented score distillation for text-to-3d generation. arXiv preprint arXiv:2402.02972,
2024."
REFERENCES,0.5130718954248366,"[44] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,
Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv
preprint arXiv:2209.14792, 2022."
REFERENCES,0.5163398692810458,"[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502, 2020."
REFERENCES,0.5196078431372549,"[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole.
Score-based generative modeling through stochastic differential equations.
arXiv preprint
arXiv:2011.13456, 2020."
REFERENCES,0.5228758169934641,"[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017."
REFERENCES,0.5261437908496732,"[48] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian
chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 12619–12629, 2023."
REFERENCES,0.5294117647058824,"[49] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer:
High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural
Information Processing Systems, 36, 2024."
REFERENCES,0.5326797385620915,"[50] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected
latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 18456–18466, 2023."
REFERENCES,0.5359477124183006,"[51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion
models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847,
2023."
REFERENCES,0.5392156862745098,"[52] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 586–595, 2018."
REFERENCES,0.5424836601307189,"A
Full proofs"
REFERENCES,0.545751633986928,"A.1
Proof of Lemma 3.1"
REFERENCES,0.5490196078431373,"Let a(i,j) denote the original attention weights and ˜a(i,j) denote the blurred attention weights, as in
the main paper. Assume that the original attention weights are properly padded to maintain consistent
statistics. Then, the following shows that the mean of the blurred attention weights remains the same."
REFERENCES,0.5522875816993464,"Ei,j[˜a(i,j)] =
1
HW H
X i=1 W
X"
REFERENCES,0.5555555555555556,"j=1
˜a(i,j) =
1
HW H
X i=1 W
X j=1 k
X m=−k k
X"
REFERENCES,0.5588235294117647,"n=−k
G(m, n) · a(i+m,j+n) = k
X m=−k k
X"
REFERENCES,0.5620915032679739,"n=−k
G(m, n) · "
REFERENCES,0.565359477124183,"
1
HW H
X i=1 W
X"
REFERENCES,0.5686274509803921,"j=1
a(i+m,j+n)   = k
X m=−k k
X"
REFERENCES,0.5718954248366013,"n=−k
G(m, n) · Ei,j[a(i,j)] = Ei,j[a(i,j)] · k
X m=−k k
X"
REFERENCES,0.5751633986928104,"n=−k
G(m, n) = Ei,j[a(i,j)]"
REFERENCES,0.5784313725490197,"In addition, the variance of the blurred attention weights is smaller than or equal to the variance of
the original attention weights."
REFERENCES,0.5816993464052288,"Vari,j[˜a(i,j)] =
1
HW H
X i=1 W
X"
REFERENCES,0.5849673202614379,"j=1
(˜a(i,j) −Ei,j[˜a(i,j)])2"
REFERENCES,0.5882352941176471,"=
1
HW H
X i=1 W
X j=1 k
X m=−k k
X"
REFERENCES,0.5915032679738562,"n=−k
G(m, n) · (a(i+m,j+n) −Ei,j[a(i,j)]) !2 = k
X m=−k k
X n=−k k
X r=−k k
X"
REFERENCES,0.5947712418300654,"s=−k
G(m, n) · G(r, s) · Cov[a(i+m,j+n), a(i+r,j+s)]"
REFERENCES,0.5980392156862745,"Using the Cauchy-Schwarz inequality and the normalization property of the 2D Gaussian filter, we
can show that the variance monotonically decreases when we apply Gaussian blur."
REFERENCES,0.6013071895424836,"Vari,j[˜a(i,j)] ≤ k
X m=−k k
X n=−k k
X r=−k k
X"
REFERENCES,0.6045751633986928,"s=−k
G(m, n) · G(r, s) ·
q"
REFERENCES,0.6078431372549019,"Var[a(i+m,j+n)] · Var[a(i+r,j+s)] = k
X m=−k k
X"
REFERENCES,0.6111111111111112,"n=−k
G(m, n) ·
q"
REFERENCES,0.6143790849673203,"Var[a(i+m,j+n)] !2 ≤ k
X m=−k k
X"
REFERENCES,0.6176470588235294,"n=−k
G(m, n) ! · k
X m=−k k
X"
REFERENCES,0.6209150326797386,"n=−k
G(m, n) · Var[a(i+m,j+n)] ! = k
X m=−k k
X"
REFERENCES,0.6241830065359477,"n=−k
G(m, n) · Var[a(i+m,j+n)]"
REFERENCES,0.6274509803921569,"= Vari,j[a(i,j)]"
REFERENCES,0.630718954248366,"A.2
Proof of Lemma 3.2"
REFERENCES,0.6339869281045751,"Applying the second-order Taylor series approximation of ex to our function f around the mean µ,
we get: H
X i=1 W
X"
REFERENCES,0.6372549019607843,"j=1
ea(i,j) ≈ H
X i=1 W
X j=1"
REFERENCES,0.6405228758169934,"
eµ + eµ(a(i,j) −µ) + 1"
REFERENCES,0.6437908496732027,"2eµ(a(i,j) −µ)2

(11)"
REFERENCES,0.6470588235294118,= HW · eµ + 1
REFERENCES,0.6503267973856209,"2eµ
H
X i=1 W
X"
REFERENCES,0.6535947712418301,"j=1
(a(i,j) −µ)2
(12)"
REFERENCES,0.6568627450980392,"In the last step, we used the fact that PH
i=1
PW
j=1(a(i,j) −µ) = 0 because µ is the mean."
REFERENCES,0.6601307189542484,"Similarly, H
X i=1 W
X"
REFERENCES,0.6633986928104575,"j=1
e˜a(i,j) ≈HW · eµ + 1"
REFERENCES,0.6666666666666666,"2eµ
H
X i=1 W
X"
REFERENCES,0.6699346405228758,"j=1
(˜a(i,j) −µ)2
(13)"
REFERENCES,0.673202614379085,"Since Var[a] > Var[˜a], we have: H
X i=1 W
X"
REFERENCES,0.6764705882352942,"j=1
(a(i,j) −µ)2 ≥ H
X i=1 W
X"
REFERENCES,0.6797385620915033,"j=1
(˜a(i,j) −µ)2
(14)"
REFERENCES,0.6830065359477124,"Therefore, the second-order approximation of lse(a) is larger than that of lse(˜a)."
REFERENCES,0.6862745098039216,"Note that this fact also implies blurring with a Gaussian filter with a bigger variance causes more
decrease in the variance of attention weights, because Gaussian filter with a larger variance can
always be represented as a convolution of two filters with smaller variances, and the convolution
operation is associative."
REFERENCES,0.6895424836601307,"To find the maximum value subject to the constraint a(1,1) + a(1,2) + . . . + a(H,W ) = c for some
constant c, we introduce Lagrange multipliers. Let g(a(1,1), a(1,2), . . . , a(H,W )) = a(1,1) + a(1,2) +
. . . + a(H,W ). The Lagrangian function is defined as:"
REFERENCES,0.6928104575163399,"L(a(1,1), a(1,2), . . . , a(H,W ), λ) = ea(1,1)+ea(1,2)+. . .+ea(H,W )−λ(a(1,1)+a(1,2)+. . .+a(H,W )−c)
(15)
Taking partial derivatives and setting them to zero yields:"
REFERENCES,0.696078431372549,"∂L
∂a(i,j)
= ea(i,j) −λ = 0
(16)"
REFERENCES,0.6993464052287581,"Solving for a(i,j), we obtain a(i,j) = ln(λ) for all i = 1, 2, . . . , H and j = 1, 2, . . . , W Summing
these equations results in:"
REFERENCES,0.7026143790849673,"λ = e
c
HW
(17)"
REFERENCES,0.7058823529411765,"Substituting λ back into a(i,j) = ln(λ) gives a(1,1) = a(1,2) = . . . = a(H,W ) =
c
HW . Therefore, the
minimum value of PH
i=1
PW
j=1 ea(i,j) is achieved when a(1,1) = a(1,2) = . . . = a(H,W )."
REFERENCES,0.7091503267973857,"A.3
Proof of Theorem 3.1"
REFERENCES,0.7124183006535948,"Let a = (a1, . . . , an) denote the attention values before the softmax operation, and let ˜a =
(˜a1, . . . , ˜an) denote the attention values after applying the 2D Gaussian blur. Let H denote the
Hessian of the original energy, i.e., the derivative of the negative softmax, and ˜H denote the Hessian
of the underlying energy associated with the blurred weights."
REFERENCES,0.7156862745098039,The elements in the i-th row and j-th column of the Hessian matrices are given by:
REFERENCES,0.7189542483660131,"hij = (ξ(a)i −δij)ξ(a)j,
(18)"
REFERENCES,0.7222222222222222,"˜hij = (ξ(˜a)i −δij)ξ(˜a)jbij,
(19)"
REFERENCES,0.7254901960784313,"respectively, where bij are the elements of the Toeplitz matrix corresponding to the Gaussian blur
kernel, and δij denotes the Kronecker delta."
REFERENCES,0.7287581699346405,"Assuming ξ(˜a)iξ(˜a)j ≈0 and ξ(a)iξ(a)j ≈0 for all i and j, which is a reasonable assumption
when the number of token is large and the softmax values get small, the non-diagonal elements of the
Hessians approximate to 0 and the diagonal elements dominate. Therefore, the determinants of the
Hessian matrices are approximated as the product of the dominant terms:"
REFERENCES,0.7320261437908496,"| det(H)| ≈ n
Y"
REFERENCES,0.7352941176470589,"i=1
ξ(a)i,
| det( ˜H)| ≈ n
Y"
REFERENCES,0.738562091503268,"i=1
ξ(˜a)ibii
(20)"
REFERENCES,0.7418300653594772,"We have the following inequality: n
Y"
REFERENCES,0.7450980392156863,"i=1
ξ(˜a)ibii < n
Y"
REFERENCES,0.7483660130718954,"i=1
ξ(˜a)i =
e
Pn
j=1 ˜aj"
REFERENCES,0.7516339869281046,"(Pn
j=1 e˜aj)n
(21)"
REFERENCES,0.7549019607843137,"≤
e
Pn
j=1 aj"
REFERENCES,0.7581699346405228,"(Pn
j=1 eaj)n = n
Y"
REFERENCES,0.761437908496732,"i=1
ξ(a)i,
(22)"
REFERENCES,0.7647058823529411,"where the first inequality follows from the property of the Gaussian blur kernel, 0 ≤bii < 1, and
the second inequality is derived from Lemmas 3.1 and 3.2, which demonstrate the mean-preserving
property and the decrease in the lse value when applying a blur. The monotonicity of the logarithm
function implies that the denominator involving the blurred attention weights is smaller. Eventually,
we obtain the following inequality:"
REFERENCES,0.7679738562091504,"| det( ˜H)| < | det(H)|.
(23)"
REFERENCES,0.7712418300653595,"This implies that the updated value is derived with attenuated curvature of the energy function
underlying the blurred softmax operation compared to that of the original softmax operation."
REFERENCES,0.7745098039215687,"B
Dual definition"
REFERENCES,0.7777777777777778,"As we previously stated in Section 2.2, we have the dual definition regarding (5), where we use
swapped indexing. Importantly, the swapped indices can be interpreted as altering the definition of
attention weights to A := KQ⊤."
REFERENCES,0.7810457516339869,"A similar conclusion can be drawn as in the main paper, except that query blurring becomes key
blurring with this definition. To see this, Eq. 7 changes slightly with this definition, using the
symmetry of the Toeplitz matrix B:"
REFERENCES,0.7843137254901961,"G ∗(KQ⊤) = B(KQ⊤)
(24)"
REFERENCES,0.7875816993464052,"= ((KQ⊤)⊤B⊤)⊤
(25)"
REFERENCES,0.7908496732026143,"= (QK⊤B⊤)⊤
(26)"
REFERENCES,0.7941176470588235,"= (Q(BK)⊤)⊤
(27)"
REFERENCES,0.7973856209150327,"= (Q(G ∗K)⊤)⊤
(28)"
REFERENCES,0.8006535947712419,"= (G ∗K)Q⊤,
(29)"
REFERENCES,0.803921568627451,"where ∗denotes the 2D convolution operation. Empirically, this altered definition does not introduce
a significant difference in the overall image quality, as shown in Fig. 12."
REFERENCES,0.8071895424836601,"C
Additional qualitative results"
REFERENCES,0.8104575163398693,"In this section, we present further qualitative results to demonstrate the effectiveness and versatility
of our Smoothed Energy Guidance (SEG) method across various generation tasks and in comparison
with other approaches."
REFERENCES,0.8137254901960784,"Comparison with previous methods
Figs. 8 and 9 provide a qualitative comparison of SEG
against vanilla SDXL [35], Self-Attention Guidance (SAG) [17], and Perturbed Attention Guidance
(PAG) [1]. These comparisons highlight the superior performance of SEG in terms of image quality,
coherence, and adherence to the given prompts. SEG consistently produces sharper details, more
realistic textures, and better overall composition compared to the other methods."
REFERENCES,0.8169934640522876,"Conditional generation with ControlNet
Figs. 10 and 11 showcase the application of SEG in
conjunction with ControlNet [51] for conditional image generation. These results illustrate how SEG
can enhance the quality and coherence of generated images while maintaining fidelity to the provided
control signals. The images demonstrate improved detail, texture, and overall visual appeal compared
to standard ControlNet outputs without prompts."
REFERENCES,0.8202614379084967,"Figure 7: Pipeline of SEG. (a) Original sampling process, self-attention weights, and the correspond-
ing energy landscape. (b) Our modified sampling process with blurred queries where σ ∈(0, ∞),
inducing blurred attention weights and the corresponding smoothed energy landscape. (c) A concep-
tual figure of γseg. Note that since the guidance linearly extrapolates predictions from (a) and (b), a
high guidance scale causes samples to be out of the manifold."
REFERENCES,0.8235294117647058,"Unconditional and text-conditional generation
Fig. 13 demonstrates the capability of SEG in
unconditional image generation, showcasing its ability to produce high-quality, diverse images
without text prompts. Fig. 14 exhibits text-conditional generation results using SEG, illustrating its
effectiveness in translating textual descriptions into visually appealing and accurate images."
REFERENCES,0.826797385620915,"Interaction with classifier-free guidance
Figs. 15–19 present a series of experiments exploring
the combination of SEG with CFG. In these experiments, the SEG guidance scale (γseg) is fixed at
3.0, while the CFG scale is varied. The results demonstrate that SEG consistently improves image
quality across different CFG scales without causing saturation or significant changes in the general
structure of the images."
REFERENCES,0.8300653594771242,"Ablation study
Fig. 20 displays a visual example of unconditional generation with controlled γseg
and σ. Consistent with results in Sec. 5.5, controlling image quality with σ has fewer side effects
than controlling with γseg."
REFERENCES,0.8333333333333334,"D
Pipeline figure"
REFERENCES,0.8366013071895425,"The overall pipeline and conceptual framework of SEG are presented in Fig. 7. Fig. 7 (a) and Fig. 7
(b) depict the original sampling process and the modified sampling process with smoothed energy,
respectively. Fig. 7 (c) illustrates the the final prediction (the red arrow) with the guidance scale."
REFERENCES,0.8398692810457516,"Figure 8: Qualitative comparison of SEG with vanilla SDXL [35], SAG [17], and PAG [1]."
REFERENCES,0.8431372549019608,"Figure 9: Qualitative comparison of SEG with vanilla SDXL [35], SAG [17], and PAG [1]."
REFERENCES,0.8464052287581699,Figure 10: Conditional generation using ControlNet [51] and SEG.
REFERENCES,0.8496732026143791,Figure 11: Conditional generation using ControlNet [51] and SEG.
REFERENCES,0.8529411764705882,Figure 12: Comparison between query and key blur across different values of σ.
REFERENCES,0.8562091503267973,Figure 13: Unconditional generation using SEG.
REFERENCES,0.8594771241830066,Figure 14: Text-conditional generation using SEG.
REFERENCES,0.8627450980392157,"Figure 15: Experiment on the combination of SEG and CFG. γseg is fixed to 3.0. The prompt is ""a
friendly robot helping an old lady cross the street."" Without causing saturation or significant changes
in the general structure, SEG improves the image quality."
REFERENCES,0.8660130718954249,"Figure 16: Experiment on the combination of SEG and CFG. γseg is fixed to 3.0. The prompt is ""a
skateboarding turtle zooming through a mini city made of Legos."""
REFERENCES,0.869281045751634,"Figure 17: Experiment on the combination of SEG and CFG. γseg is fixed to 3.0. The prompt is ""a
group of puppies playing soccer with a ball of yarn."""
REFERENCES,0.8725490196078431,"Figure 18: Experiment on the combination of SEG and CFG. γseg is fixed to 3.0. The prompt is ""a
family of teddy bears having a barbecue in their backyard."""
REFERENCES,0.8758169934640523,"Figure 19: Experiment on the combination of SEG and CFG. γseg is fixed to 3.0. The prompt is ""a
baby elephant learning to paint with its trunk in an art studio."""
REFERENCES,0.8790849673202614,Figure 20: Unconditional generation result with controlled γseg and σ.
REFERENCES,0.8823529411764706,NeurIPS Paper Checklist
CLAIMS,0.8856209150326797,1. Claims
CLAIMS,0.8888888888888888,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction state the main claims and contributions of the
paper.
Guidelines:"
CLAIMS,0.8921568627450981,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8954248366013072,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations are discussed the paper.
Guidelines:"
CLAIMS,0.8986928104575164,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9019607843137255,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]"
CLAIMS,0.9052287581699346,"Justification: The paper provide the full set of assumptions and a complete proof.
Guidelines:"
CLAIMS,0.9084967320261438,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility"
CLAIMS,0.9117647058823529,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The paper fully disclose all the information needed to reproduce the results.
Guidelines:"
CLAIMS,0.9150326797385621,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code"
CLAIMS,0.9183006535947712,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
CLAIMS,0.9215686274509803,"Answer: [Yes]
Justification: The paper provide open access to the data and code.
Guidelines:"
CLAIMS,0.9248366013071896,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details"
CLAIMS,0.9281045751633987,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper specify all the training and test details.
Guidelines:"
CLAIMS,0.9313725490196079,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance"
CLAIMS,0.934640522875817,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The paper reports statistical information.
Guidelines:"
CLAIMS,0.9379084967320261,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean."
CLAIMS,0.9411764705882353,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
CLAIMS,0.9444444444444444,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The paper provide sufficient information on the computer resources.
Guidelines:"
CLAIMS,0.9477124183006536,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
CLAIMS,0.9509803921568627,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]"
CLAIMS,0.954248366013072,"Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics.
Guidelines:"
CLAIMS,0.9575163398692811,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts"
CLAIMS,0.9607843137254902,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The paper discuss potential societal impacts.
Guidelines:"
CLAIMS,0.9640522875816994,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to"
CLAIMS,0.9673202614379085,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
CLAIMS,0.9705882352941176,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper proposes a guidance method for the current model; therefore, the
paper itself poses no such risks.
Guidelines:"
CLAIMS,0.9738562091503268,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
CLAIMS,0.9771241830065359,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The creators or original owners of assets are properly credited and the license
and terms of use are explicitly mentioned and properly respected.
Guidelines:"
CLAIMS,0.9803921568627451,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
CLAIMS,0.9836601307189542,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:"
CLAIMS,0.9869281045751634,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
CLAIMS,0.9901960784313726,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CLAIMS,0.9934640522875817,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:"
CLAIMS,0.9967320261437909,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
