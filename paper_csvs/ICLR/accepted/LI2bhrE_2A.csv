Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004016064257028112,"Antibodies are versatile proteins that bind to pathogens like viruses and stimulate
the adaptive immune system. The speciﬁcity of antibody binding is determined
by complementarity-determining regions (CDRs) at the tips of these Y-shaped
proteins. In this paper, we propose a generative model to automatically design
the CDRs of antibodies with enhanced binding speciﬁcity or neutralization capa-
bilities. Previous generative approaches formulate protein design as a structure-
conditioned sequence generation task, assuming the desired 3D structure is given
a priori. In contrast, we propose to co-design the sequence and 3D structure of
CDRs as graphs. Our model unravels a sequence autoregressively while itera-
tively reﬁning its predicted global structure. The inferred structure in turn guides
subsequent residue choices. For efﬁciency, we model the conditional dependence
between residues inside and outside of a CDR in a coarse-grained manner. Our
method achieves superior log-likelihood on the test set and outperforms previous
baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus1."
INTRODUCTION,0.008032128514056224,"1
INTRODUCTION"
INTRODUCTION,0.012048192771084338,"Monoclonal antibodies are increasingly adopted as therapeutics targeting a wide range of pathogens
such as SARS-CoV-2 (Pinto et al., 2020). Since the binding speciﬁcity of these Y-shaped proteins is
largely determined by their complementarity-determining regions (CDRs), the main goal of compu-
tational antibody design is to automate the creation of CDR subsequences with desired properties.
This problem is particularly challenging due to the combinatorial search space of over 2060 possi-
ble CDR sequences and the small solution space which satisﬁes the desired constraints of binding
afﬁnity, stability, and synthesizability (Raybould et al., 2019)."
INTRODUCTION,0.01606425702811245,"There are three key modeling questions in CDR generation. The ﬁrst is how to model the relation
between a sequence and its underlying 3D structure. Generating sequences without the correspond-
ing structure (Alley et al., 2019; Shin et al., 2021) can lead to sub-optimal performance (Ingraham
et al., 2019), while generating from a predeﬁned 3D structure (Ingraham et al., 2019) is not suit-
able for antibodies since the desired structure is rarely known a priori (Fischman & Ofran, 2018).
Therefore, it is crucial to develop models that co-design the sequence and structure. The second
question is how to model the conditional distribution of CDRs given the remainder of a sequence
(context). Attention-based methods only model the conditional dependence at the sequence level,
but the structural interaction between the CDR and its context is crucial for generation. The last
question relates to the model’s ability to optimize for various properties. Traditional physics-based
methods (Lapidoth et al., 2015; Adolf-Bryfogle et al., 2018) focus on binding energy minimization,
but in practice, our objective can be much more involved than binding energies (Liu et al., 2020)."
INTRODUCTION,0.020080321285140562,"In this paper, we represent a sequence-structure pair as a graph and formulate the co-design task as
a graph generation problem. The graph representation allows us to model the conditional depen-
dence between a CDR and its context at both the sequence and structure levels. Antibody graph
generation poses unique challenges because the global structure is expected to change when new
nodes are inserted. Previous autoregressive models (You et al., 2018; Gebauer et al., 2019) cannot"
INTRODUCTION,0.024096385542168676,1Our code is available at https://github.com/wengong-jin/RefineGNN
INTRODUCTION,0.028112449799196786,Published as a conference paper at ICLR 2022
INTRODUCTION,0.0321285140562249,"modify a generated structure because they are trained under teacher forcing. Thus errors made in
the previous steps can lead to a cascade of errors in subsequent generation steps. To address these
problems, we propose a novel architecture which interleaves the generation of amino acid nodes
with the prediction of 3D structures. The structure generation is based on an iterative reﬁnement of
a global graph rather than a sequential expansion of a partial graph with teacher forcing. Since the
context sequence is long, we further introduce a coarsened graph representation by grouping nodes
into blocks. We apply graph convolution at a coarser level to efﬁciently propagate the contextual
information to the CDR residues. After pretraining our model on antibodies with known structures,
we ﬁnetune it using a predeﬁned property predictor to generate antibodies with speciﬁc properties."
INTRODUCTION,0.03614457831325301,"We evaluate our method on three generation tasks, ranging from language modeling to SARS-CoV-2
neutralization optimization and antigen-binding antibody design. Our method is compared with a
standard sequence model (Saka et al., 2021; Akbar et al., 2021) and a state-of-the-art graph genera-
tion method (You et al., 2018) tailored to antibodies. Our method not only achieves lower perplexity
on test sequences but also outperforms previous baselines in property-guided antibody design tasks."
RELATED WORK,0.040160642570281124,"2
RELATED WORK"
RELATED WORK,0.04417670682730924,"Antibody/protein design. Current methods for computational antibody design roughly fall into two
categories. The ﬁrst class is based on energy function optimization (Pantazes & Maranas, 2010; Li
et al., 2014; Lapidoth et al., 2015; Adolf-Bryfogle et al., 2018), which use Monte Carlo simulation
to iteratively modify a sequence and its structure until reaching a local energy minimum. Similar
approaches are used in protein design (Leaver-Fay et al., 2011; Tischer et al., 2020). Nevertheless,
these physics-based methods are computationally expensive (Ingraham et al., 2019) and our desired
objective can be much more complicated than low binding energy (Liu et al., 2020)."
RELATED WORK,0.04819277108433735,"The second class is based on generative models. For antibodies, they are mostly sequence-based
(Alley et al., 2019; Shin et al., 2021; Saka et al., 2021; Akbar et al., 2021). For proteins, O’Connell
et al. (2018); Ingraham et al. (2019); Strokach et al. (2020); Karimi et al. (2020); Cao et al. (2021)
further developed models conditioned on a backbone structure or protein fold. Our model also seeks
to incorporate 3D structure information for antibody generation. Since the best CDR structures are
often unknown for new pathogens, we co-design sequences and structures for speciﬁc properties."
RELATED WORK,0.05220883534136546,"Generative models for graphs. Our work is related to autoregressive models for graph generation
(You et al., 2018; Li et al., 2018; Liu et al., 2018; Liao et al., 2019; Jin et al., 2020a). In particular,
Gebauer et al. (2019) developed G-SchNet for molecular graph and conformation co-design. Unlike
our method, they generate edges sequentially and cannot modify a previously generated subgraph
when new nodes arrive. While Graphite (Grover et al., 2019) also uses iterative reﬁnement to predict
the adjacency matrix of a graph, it assumes all the node labels are given and predicts edges only. In
contrast, our work combines autoregressive models with iterative reﬁnement to generate a full graph
with node and edge labels, including node labels and coordinates."
RELATED WORK,0.05622489959839357,"3D structure prediction. Our approach is closely related to protein folding (Ingraham et al., 2018;
Yang et al., 2020a; Baek et al., 2021; Jumper et al., 2021). Inputs to the state-of-the-art models
like AlphaFold require a complete protein sequence, its multi-sequence alignment (MSA), and its
template features. These models are not directly applicable because we need to predict the structure
of an incomplete sequence and the MSA is not speciﬁed in advance."
RELATED WORK,0.060240963855421686,"Our iterative reﬁnement model is also related to score matching methods for molecular conformation
prediction (Shi et al., 2021) and diffusion-based methods for point clouds (Luo & Hu, 2021). These
algorithms also iteratively reﬁne a predicted 3D structure, but only for a complete molecule or
point cloud. In contrast, our approach learns to predict the 3D structure for incomplete graphs and
interleaves 3D structure reﬁnement with graph generation."
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.0642570281124498,"3
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN"
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.06827309236947791,"Overview. The role of an antibody is to bind to an antigen (e.g. a virus), present it to the immune
system, and stimulate an immune response. A subset of antibodies known as neutralizing antibodies
not only bind to an antigen but can also suppress its activity. An antibody consists of a heavy chain
and a light chain, each composed of one variable domain (VH/VL) and some constant domains. The"
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.07228915662650602,Published as a conference paper at ICLR 2022
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.07630522088353414,"Heavy chain
CDR-H1
CDR-H2
CDR-H3"
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.08032128514056225,"CDR-L1
CDR-L2
CDR-L3
Light chain variable region (VL)"
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.08433734939759036,Heavy chain variable region (VH)
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.08835341365461848,"Antibody 
structure"
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.09236947791164658,Antibody sequence
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.0963855421686747,Figure 1: Schematic structure of an antibody (ﬁgure modiﬁed from Wikipedia).
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.10040160642570281,"variable domain is further divided into a framework region and three complementarity determining
regions (CDRs). The three CDRs on the heavy chain are labeled as CDR-H1, CDR-H2, CDR-H3,
each occupying a contiguous subsequence (Figure 1). As the most variable part of an antibody,
CDRs are the main determinants of binding and neutralization (Abbas et al., 2014)."
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.10441767068273092,"Following Shin et al. (2021); Akbar et al. (2021), we formulate antibody design as a CDR generation
task, conditioned on the framework region. Speciﬁcally, we represent an antibody as a graph, which
encodes both its sequence and 3D structure. We propose a new graph generation approach called
ReﬁneGNN and extend it to handle conditional generation given a ﬁxed framework region. Lastly,
we describe how to apply ReﬁneGNN to property-guided optimization to design new antibodies with
better neutralization properties. For simplicity, we focus on the generation of heavy chain CDRs,
though our method can be easily extended to model light chains CDRs."
ANTIBODY SEQUENCE AND STRUCTURE CO-DESIGN,0.10843373493975904,"Notations. An antibody VH domain is represented as a sequence of amino acids s = s1s2 · · · sn.
Each token si in the sequence is called a residue, whose value can be either one of the 20 amino acids
or a special token ⟨MASK⟩, meaning that its amino acid type is unknown and needs to be predicted.
The VH sequence folds into a 3D structure and each residue si is labeled with three backbone
coordinates: xi,α for its alpha carbon atom, xi,c for its carbon atom, and xi,n for its nitrogen atom."
GRAPH REPRESENTATION,0.11244979919678715,"3.1
GRAPH REPRESENTATION"
GRAPH REPRESENTATION,0.11646586345381527,"We represent an antibody (VH) as a graph G(s) = (V, E) with node features V = {v1, · · · , vn} and
edge features E = {eij}i̸=j. Each node feature vi encodes three dihedral angles (φi, ψi, ωi) related
to three backbone coordinates of residue i. For each residue i, we compute an orientation matrix
Oi representing its local coordinate frame (Ingraham et al., 2019) (deﬁned in the appendix). This
allows us to compute edge features describing the spatial relationship between two residues i and j:"
GRAPH REPRESENTATION,0.12048192771084337,"eij =
 
Epos(i −j),
RBF(∥xi,α −xj,α∥),
O⊤
i
xj,α −xi,α
∥xi,,α −xj,,α∥,
q(O⊤
i Oj)

.
(1)"
GRAPH REPRESENTATION,0.12449799196787148,"The edge feature eij contains four parts. The positional encoding Epos(i −j) encodes the relative
distance between two residues in an antibody sequence. The second term RBF(·) is a distance
encoding lifted into radial basis. The third term in eij is a direction encoding that corresponds to
the relative direction of xj in the local frame of residue i. The last term q(O⊤
i Oj) is the orientation
encoding of the quaternion representation q(·) of the spatial rotation matrix O⊤
i Oj. We only include
edges in the K-nearest neighbors graph of G(s) with K = 8. For notation convenience, we use G
as a shorthand for G(s) when there is no ambiguity."
GRAPH REPRESENTATION,0.1285140562248996,"3.2
ITERATIVE REFINEMENT GRAPH NEURAL NETWORK (REFINEGNN)"
GRAPH REPRESENTATION,0.13253012048192772,"We propose to generate an antibody graph via an iterative reﬁnement process. Let G(0) be the initial
guess of the true antibody graph. Each residue is initialized as a special token ⟨MASK⟩and each edge
(i, j) is initialized to be of distance 3|i −j| since the average distance between consecutive residues
is around three. The direction and orientation features are set to zero. In each generation step t, the
model learns to revise a current antibody graph G(t) and predict the label of the next residue t + 1.
Speciﬁcally, it ﬁrst encodes G(t) with a message passing network (MPN) with parameter θ"
GRAPH REPRESENTATION,0.13654618473895583,"{h(t)
1 , · · · , h(t)
n } = MPNθ(G(t)),
(2)"
GRAPH REPRESENTATION,0.14056224899598393,"where h(t)
i
is a learned representation of residue i under the current graph G(t). Our MPN consists
of L message passing layers with the following architecture"
GRAPH REPRESENTATION,0.14457831325301204,"h(t,l+1)
i
= LayerNorm
 X"
GRAPH REPRESENTATION,0.14859437751004015,"j
FFN
 
h(t,l)
i
, h(t,l)
j
, E(sj), ei,j

,
0 ≤l ≤L −1,
(3)"
GRAPH REPRESENTATION,0.15261044176706828,Published as a conference paper at ICLR 2022
GRAPH REPRESENTATION,0.1566265060240964,"where h(t,0)
i
= vi and h(t)
i
= h(t,L)
i
. FFN is a two-layer feed-forward network (FFN) with ReLU
activation function. E(sj) is a learned embedding of amino acid type sj. Based on the learned
residue representations, we predict the amino acid type of the next residue t + 1 (Figure 2A)."
GRAPH REPRESENTATION,0.1606425702811245,"pt+1 = softmax(Wah(t)
t+1)
(4)"
GRAPH REPRESENTATION,0.1646586345381526,"This prediction gives us a new graph G(t+0.5) with the same edges as G(t) but the node label of t+1
is changed (Figure 2B). Next, we need to update the structure to accommodate the new residue t+1.
To this end, we encode graph G(t+0.5) by another MPN with a different parameter ˜θ and predict the
coordinate of all residues."
GRAPH REPRESENTATION,0.1686746987951807,"{h(t+0.5)
1
, · · · , h(t+0.5)
n
}
=
MPN˜θ(G(t+0.5))
(5)"
GRAPH REPRESENTATION,0.17269076305220885,"x(t+1)
i,e
=
W e
xh(t+0.5)
i
,
1 ≤i ≤n, e ∈{α, c, n}.
(6)"
GRAPH REPRESENTATION,0.17670682730923695,"The new coordinates x(t+1)
i
deﬁne a new antibody graph G(t+1) for the next iteration (Figure 2C).
We explicitly realize the coordinates of each residue because we need to calculate the spatial edge
features for G(t+1). The structure prediction (coordinates xi) and sequence prediction (amino acid
types pt+1) are carried out by two different MPNs, namely the structure network ˜θ and sequence
network θ. This disentanglement allows the two networks to focus on two distinct tasks."
GRAPH REPRESENTATION,0.18072289156626506,"Training. During training, we only apply teacher forcing to the discrete amino acid type prediction.
Speciﬁcally, in each generation step t, residues 1 to t are set to their ground truth amino acid types
s1, · · · , st, while all future residues t + 1, · · · , n are set to a padding token. In contrast, the contin-
uous structure prediction is carried out without teacher forcing. In each iteration, the model reﬁnes
the entire structure predicted in the previous step and constructs a new K-nearest neighbors graph
G(t+1) of all residues based on the predicted coordinates {x(t+1)
i,e
| 1 ≤i ≤n, e ∈{α, c, n}}."
GRAPH REPRESENTATION,0.18473895582329317,"Loss function. Our model remains rotation and translation invariant because the loss function is
computed over pairwise distance and angles rather than coordinates. The loss function for antibody
structure prediction consists of three parts."
GRAPH REPRESENTATION,0.18875502008032127,"• Distance loss: For each residue pair i, j, we compute its pairwise distance between the predicted
alpha carbons x(t)
i,α, x(t)
j,α. We deﬁne the distance loss as the Huber loss between the predicted and
true pairwise distances"
GRAPH REPRESENTATION,0.1927710843373494,"L(t)
d
=
X"
GRAPH REPRESENTATION,0.19678714859437751,"i,j ℓhuber(∥x(t)
i,α −x(t)
j,α∥2, ∥xi,α −xj,α∥2),
(7)"
GRAPH REPRESENTATION,0.20080321285140562,where distance is squared to avoid the square root operation which causes numerical instability.
GRAPH REPRESENTATION,0.20481927710843373,"• Dihedral angle loss: For each residue, we calculate its dihedral angle
 
φ(t)
i , ψ(t)
i , ω(t)
i

based on
the predicted atom coordinates x(t)
i,α, x(t)
i,c, x(t)
i,n and x(t)
i+1,α, x(t)
i+1,c, x(t)
i+1,n. We deﬁne the dihedral
angle loss as the mean square error between the predicted and true dihedral angles"
GRAPH REPRESENTATION,0.20883534136546184,"L(t)
a
=
X i X"
GRAPH REPRESENTATION,0.21285140562248997,"a∈{φ,ψ,ω}(cos a(t)
i
−cos ai)2 + (sin a(t)
i
−sin ai)2
(8)"
GRAPH REPRESENTATION,0.21686746987951808,"• Cα angle loss: We calculate angles γ(t)
i
between two vectors x(t)
i−1,α −x(t)
i,α and x(t)
i,α −x(t)
i+1,α as"
GRAPH REPRESENTATION,0.22088353413654618,"well as dihedral angles β(t)
i
between two planes deﬁned by x(t)
i−2,α, x(t)
i−1,α, x(t)
i,α, x(t)
i+1,α."
GRAPH REPRESENTATION,0.2248995983935743,"L(t)
c
=
X"
GRAPH REPRESENTATION,0.2289156626506024,"i(cos γ(t)
i
−cos γi)2 + (cos β(t)
i
−cos βi)2
(9)"
GRAPH REPRESENTATION,0.23293172690763053,"In summary, the overall graph generation loss is deﬁned as L = Lseq + Lstruct, where"
GRAPH REPRESENTATION,0.23694779116465864,"Lstruct =
X"
GRAPH REPRESENTATION,0.24096385542168675,"t L(t)
d + L(t)
a + L(t)
c
Lseq =
X"
GRAPH REPRESENTATION,0.24497991967871485,"t Lce(pt, st).
(10)"
GRAPH REPRESENTATION,0.24899598393574296,The sequence prediction loss Lseq is the cross entropy Lce between predicted and true residue types.
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.25301204819277107,"3.3
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.2570281124497992,"The model architecture described so far is designed for unconditional generation — it generates an
entire antibody graph without any constraints. In practice, we usually ﬁx the framework region of an"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.26104417670682734,Published as a conference paper at ICLR 2022 A R G D
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.26506024096385544,"Y
Predict 
amino acid A R G D"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.26907630522088355,"VYYC
…SA"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.27309236947791166,"…VT
WGQG"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.27710843373493976,Message
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.28112449799196787,"passing
A R G D Y"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.285140562248996,"Coarsened 
context sequence"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.2891566265060241,"Update 
structure
Update 
sequence"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.2931726907630522,"VYYC
…SA"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.2971887550200803,"…VT
WGQG"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.30120481927710846,"VYYC
…SA"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.30522088353413657,"…VT
WGQG"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3092369477911647,"Predict all 
coordinates"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3132530120481928,"𝒢(t)
𝒢(t+0.5)
𝒢(t+1)
s<l s>r A
B
C"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3172690763052209,"…TIYPGDGDTGYAQKFQGKATLTADKSSKTVYMHLSSLASEDSAVYYCARGDYYGSNSLDYWGQGTSVT…
D"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.321285140562249,"WGQG TSVT
VYYC"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3253012048192771,Original sequence :s
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3293172690763052,"Context sequence 
:
bl,r(s)
SLAS
MHLS
KTVY
EDSA
DKSS
…
TLTA
QGKA
AQKF
…
CDR-H3
l
r"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3333333333333333,"Figure 2: (A-C) One generation step of ReﬁneGNN. Each circle represents a CDR residue and each
square represents a residue block in a coarsened context sequence. (D) Sequence coarsening."
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3373493975903614,"antibody and design the CDR sequence only. Therefore, we need to extend the model architecture to
learn the conditional distribution P(s′|s<l, s>r), where s<l = s1 · · · sl−1 and s>r = sr+1 · · · sn
are residues outside of the CDR sl · · · sr."
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3413654618473896,"Conditioning via attention. A simple extension of ReﬁneGNN is to encode the non-CDR se-
quence using a recurrent neural network and propagate information to the CDR through an at-
tention layer.
To be speciﬁc, we ﬁrst concatenate s<l and s>r into a context sequence ˜s =
s<l ⊕⟨MASK⟩· · · ⟨MASK⟩⊕s>r, where ⊕means string concatenation and ⟨MASK⟩is repeated n
times. We then encode this context sequence by a Gated Recurrent Unit (GRU) (Cho et al., 2014)
and modify the structure and sequence prediction step (Equation 4 and 6) as"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3453815261044177,"{c1, · · · , cn}
=
c1:n = GRU(˜s)
(11)"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3493975903614458,"pt+1
=
softmax
 
Wah(t)
t+1 + U ⊤
a attention(c1:n, h(t)
t+1)
 (12)"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3534136546184739,"x(t+1)
i,e
=
W e
xh(t+0.5)
i
+ U e
x
⊤attention(c1:n, h(t+0.5)
i
)
(13)"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.357429718875502,"Multi-resolution modeling. The attention-based approach alone is not sufﬁcient because it does
not model the structure of the context sequence, thus ignoring how its residues structurally interact
with the CDR’s. While this information is not available for new antibodies at test time, we can learn
to predict this interaction using antibodies in the training set with known structures."
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3614457831325301,"A naive solution is to iteratively reﬁne the entire antibody structure (more than 100 residues) while
generating CDR residues. This approach is computationally expensive because we need to recom-
pute the MPN encoding for all residues in each generation step. Importantly, we cannot predict the
context residue coordinates at the outset and ﬁx them because they need to be adjusted accordingly
when the coordinates of CDR residues are updated in each generation step."
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3654618473895582,"For computational efﬁciency, we propose a coarse-grained model that reduces the context sequence
length by clustering it into residue blocks. Speciﬁcally, we construct a coarsened context sequence
bl,r(s) by clustering every b context residues into a block (Figure 2D). The new sequence bl,r(s)
deﬁnes a coarsened graph G(bl,r(s)) over the residue blocks, whose edges are deﬁned based on
block coordinates. The coordinate of each block xbi,e is deﬁned as the mean coordinate of residues
within the block. The embedding of each block E(bi) is the mean of its residue embeddings."
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.36947791164658633,"E(bi) =
X"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.37349397590361444,"sj∈bi E(sj)/b,
xbi,e =
X"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.37751004016064255,"sj∈bi xj,e/b,
e ∈{α, c, n}.
(14)"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3815261044176707,"Now we can apply ReﬁneGNN to generate the CDR residues while iteratively reﬁning the global
graph G(bl,r(s)) by predicting the coordinates of all blocks. The only change is that the structure
prediction loss is deﬁned over block coordinates xbi,e. Lastly, we combine both the attention mech-
anism and coarse-grained modeling to keep both ﬁne-grained and coarse-grained information. The
decoding process of this conditional ReﬁneGNN is illustrated in Algorithm 1."
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3855421686746988,Published as a conference paper at ICLR 2022
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.3895582329317269,Algorithm 1 ReﬁneGNN decoding
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.39357429718875503,"Require: Context sequence s<l, s>r"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.39759036144578314,"1: Predict the CDR length n
2: Coarsen the context sequence into bl,r(s)
3: Construct the initial graph G(0)"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.40160642570281124,"4: for t = 0 to n −1 do
5:
Encode G(t) using the sequence MPN
6:
Predict distribution of the next residue
pt+1
7:
Sample st+1 ∼categorical(pt+1)
8:
Encode G(t+0.5) with the structure MPN
9:
Predict all residue coordinates x(t+1)
i,e
10:
Update G(t+1) using the new coordinates"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.40562248995983935,Algorithm 2 ITA-based sequence optimization
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.40963855421686746,"Require: A set of antibodies D to be optimized
Require: A neutralization predictor f.
Require: A set of neutralizing antibodies Q"
CONDITIONAL GENERATION GIVEN THE FRAMEWORK REGION,0.41365461847389556,"1: for each iteration do
2:
Sample an antibody s from D, remove its
CDR and get a context sequence bl,r(s)
3:
for i = 1 to M do
4:
Sample s′
i ∼PΘ(s′|bl,r(s))
5:
if f(s′
i) > max(f(s), 0.5) then
6:
Q ←Q ∪{s′
i}
7:
Sample a batch of new antibodies from Q
8:
Update model parameter Θ by minimizing
the sequence prediction loss Lseq."
PROPERTY-GUIDED SEQUENCE OPTIMIZATION,0.41767068273092367,"3.4
PROPERTY-GUIDED SEQUENCE OPTIMIZATION"
PROPERTY-GUIDED SEQUENCE OPTIMIZATION,0.42168674698795183,"Our ultimate goal is to generate new antibodies with desired properties such as neutralizing a par-
ticular virus. This task can be formulated as an optimization problem. Let Y be a binary indicator
variable for neutralization. Our goal is to learn a conditional generative model PΘ(s′|bl,r(s)) that
maximizes the probability of neutralization for a training set of antibodies D, i.e.
X"
PROPERTY-GUIDED SEQUENCE OPTIMIZATION,0.42570281124497994,"s∈D log P(Y = 1|bl,r(s)) =
X"
PROPERTY-GUIDED SEQUENCE OPTIMIZATION,0.42971887550200805,"s∈D log
X"
PROPERTY-GUIDED SEQUENCE OPTIMIZATION,0.43373493975903615,"s′ f(s′)PΘ(s′|bl,r(s))
(15)"
PROPERTY-GUIDED SEQUENCE OPTIMIZATION,0.43775100401606426,"where f(s′) is a predictor for P(Y = 1|s′). Assuming f is given, this problem can be solved
by iterative target augmentation (ITA) (Yang et al., 2020b). Before ITA optimization starts, we
ﬁrst pretrain our model on a set of real antibody structures to learn a prior distribution over CDR
sequences and structures. In each ITA ﬁnetuning step, we ﬁrst randomly sample a sequence s from
D, a set of antibodies whose CDRs need to be redesigned. Next, we generate M new sequences
given its context bl,r(s). A generated sequence s′
i is added to our training set Q if it is predicted as
neutralizing. Initially, the training set Q contains antibodies that are known to be neutralizing (Y =
1). Lastly, we sample a batch of neutralizing antibodies from Q and update the model parameter
by minimizing their sequence prediction loss Lseq (Eq.(10)). The structure prediction loss Lstruct is
excluded in ITA ﬁnetuning phase because the structure of a generated sequence is unknown."
EXPERIMENTS,0.44176706827309237,"4
EXPERIMENTS"
EXPERIMENTS,0.4457831325301205,"Setup. We construct three evaluation setups to quantify the performance of our approach. Following
standard practice in generative model evaluation, we ﬁrst measure the perplexity of different models
on new antibodies in a test set created based on sequence similarity split. We also measure structure
prediction error by comparing generated and ground truth CDR structures recorded in the Structural
Antibody Database (Dunbar et al., 2014). Results for this task are shown in section 4.1."
EXPERIMENTS,0.4497991967871486,"Second, we evaluate our method on an existing antibody design benchmark of 60 antibody-antigen
complexes from Adolf-Bryfogle et al. (2018). The goal is to design the CDR-H3 of an antibody so
that it binds to a given antigen. Results for this task are shown in section 4.2."
EXPERIMENTS,0.4538152610441767,"Lastly, we propose an antibody optimization task which aims to redesign CDR-H3 of antibodies in
the Coronavirus Antibody Database (Raybould et al., 2021) to improve their neutralization against
SARS-CoV-2. CDR-H3 design with a ﬁxed framework is a common practice in the antibody engi-
neering community (Adolf-Bryfogle et al., 2018; Liu et al., 2020). Following works in molecular
design (Jin et al., 2020b), we use a predictor to evaluate the neutralization of generated antibodies
since we cannot experimentally test them in wet labs. Results for this task are reported in section 4.3."
EXPERIMENTS,0.4578313253012048,"Baselines. We consider three baselines for comparison (details in the appendix). The ﬁrst baseline
is a sequence-based LSTM model used in Saka et al. (2021); Akbar et al. (2021). This model does
not utilize any 3D structure information. It consists of an encoder that learns to encode a context
sequence ˜s, a decoder that decodes a CDR sequence, and an attention layer connecting the two."
EXPERIMENTS,0.46184738955823296,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.46586345381526106,"Table 1: Left: Language modeling results. We report perplexity (PPL) and root mean square de-
viation (RMSD) for each CDR in the heavy chain. Right: Results on the antigen-binding antibody
design task. We report the amino acid recovery (AAR) for all methods."
EXPERIMENTS,0.46987951807228917,"CDR-H1
CDR-H2
CDR-H3
Model
PPL
RMSD
PPL
RMSD
PPL
RMSD
LSTM
6.79
-
7.21
-
9.70
-
AR-GNN
6.44
2.97
6.86
2.27
9.44
3.63
ReﬁneGNN
6.09
1.18
6.58
0.87
8.54
2.50"
EXPERIMENTS,0.4738955823293173,"Model
AAR
RAbD
28.53%
LSTM
22.53%
AR-GNN
23.86%
ReﬁneGNN
34.14%"
EXPERIMENTS,0.4779116465863454,"The second baseline is an autoregressive graph generation model (AR-GNN) whose architecture is
similar to You et al. (2018); Jin et al. (2020b) but tailored for antibodies. AR-GNN generates an
antibody graph residue by residue. In each step t, it ﬁrst predicts the amino acid type of residue t
and then generates edges between t and previous residues. Importantly, AR-GNN cannot modify a
partially generated 3D structure of residues s1 · · · st−1 because it is trained by teacher forcing."
EXPERIMENTS,0.4819277108433735,"On the antigen-binding task, we include an additional physics-based baseline called RosettaAnti-
bodyDesign (RAbD) (Adolf-Bryfogle et al., 2018). We apply their de novo design protocol com-
posed of graft design followed by 250 iterations of sequence design and energy minimization. We
cannot afford to run more iterations because it takes more than 10 hours per antibody. We also
could not apply RAbD to the SARS-CoV-2 task because it requires 3D structures to be given. This
information is unavailable for antibodies in CoVAbDab."
EXPERIMENTS,0.4859437751004016,"Hyperparameters. We performed hyperparameter tuning to ﬁnd the best setting for each method.
For ReﬁneGNN, both its structure and sequence MPN have four message passing layers, with a
hidden dimension of 256 and block size b = 4. All models are trained by the Adam optimizer with
a learning rate of 0.001. More details are provided in the appendix."
EXPERIMENTS,0.4899598393574297,"4.1
LANGUAGE MODELING AND 3D STRUCTURE PREDICTION"
EXPERIMENTS,0.4939759036144578,"Data. The Structural Antibody Database (SAbDab) consists of 4994 antibody structures renumbered
according to the IMGT numbering scheme (Lefranc et al., 2003). To measure a model’s ability to
generalize to novel CDR sequences, we divide the heavy chains into training, validation, and test sets
based on CDR cluster split. We illustrate our cluster split process using CDR-H3 as an example.
First, we use MMseqs2 (Steinegger & S¨oding, 2017) to cluster all the CDR-H3 sequences. The
sequence identity is calculated under the BLOSUM62 substitution matrix (Henikoff & Henikoff,
1992). Two antibodies are put into the same cluster if their CDR-H3 sequence identity is above
40%. We then randomly split the clusters into training, validation, and test set with 8:1:1 ratio. We
repeat the same procedure for creating CDR-H1 and CDR-H2 splits. In total, there are 1266, 1564,
and 2325 clusters for CDR-H1, H2, and H3. The size of training, validation, and test sets for each
CDR is shown in the appendix."
EXPERIMENTS,0.4979919678714859,"Metrics. For each method, we report the perplexity (PPL) of test sequences and the root mean
square deviation (RMSD) between a predicted structure and its ground truth structure reported in
SAbDab. RMSD is calculated by the Kabsch algorithm (Kabsch, 1976) based on Cα coordinate of
CDR residues. Since the mapping between sequences and structures is deterministic in ReﬁneGNN,
we can calculate perplexity in the same way as standard sequence models."
EXPERIMENTS,0.5020080321285141,"Results. Since the LSTM baseline does not involve structure prediction, we report RMSD for graph-
based methods only. As shown in Table 1, ReﬁneGNN signiﬁcantly outperforms all baselines in
both metrics. For CDR-H3, our model gives 13% PPL reduction (8.54 v.s. 9.70) over sequence only
model and 10% PPL reduction over AR-GNN (8.54 v.s. 9.44). ReﬁneGNN also predicts the struc-
ture more accurately, with 30% relative RMSD reduction over AR-GNN. In Figure 3, we provide
examples of predicted 3D structures of CDR-H3 loops."
EXPERIMENTS,0.5060240963855421,"Ablation studies. We further conduct ablation experiments on the CDR-H3 generation task to study
the importance of different modeling choices. First, when we remove the attention mechanism and
context coarsening step in section 3.3, the PPL increases from 8.54 to 8.86 (Figure 3C, row 2) and
9.01 (Figure 3C, row 3) respectively. We also tried to remove both the attention and coarsening"
EXPERIMENTS,0.5100401606425703,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.5140562248995983,"CDR-H3
CDR-H3"
EXPERIMENTS,0.5180722891566265,RefineGNN
EXPERIMENTS,0.5220883534136547,No attention
EXPERIMENTS,0.5261044176706827,No coarsening
EXPERIMENTS,0.5301204819277109,Unconditional
EXPERIMENTS,0.5341365461847389,"Structure- 
conditioned"
EXPERIMENTS,0.5381526104417671,CDR-H3 Perplexity
EXPERIMENTS,0.5421686746987951,"5
6
7
8
9 7.39 8.95 9.01 8.86"
"A
B
C",0.5461847389558233,"8.54
A
B
C"
"A
B
C",0.5502008032128514,Ground truth
"A
B
C",0.5542168674698795,RefineGNN
"A
B
C",0.5582329317269076,Ground truth
"A
B
C",0.5622489959839357,AR-GNN
"A
B
C",0.5662650602409639,"Figure 3: (A) CDR-H3 structure predicted by ReﬁneGNN (PDB: 4bkl, RMSD = 0.57). The pre-
dicted structure (cyan) is aligned to the true structure (green) using the Kabsch algorithm. (B) CDR-
H3 structure predicted by AR-GNN (PDB: 4bkl, RMSD = 2.16). (C) Ablation studies of different
modeling choices in ReﬁneGNN in the CDR-H3 perplexity evaluation task."
"A
B
C",0.570281124497992,"modules and trained the model without conditioning on the context sequence. The PPL of this
unconditional variant is much worse than our conditional model (Figure 3C, row 4). Lastly, we
train a structure-conditioned model by feeding the ground truth structure to ReﬁneGNN at every
generation step (Figure 3C, row 5). While this structure-conditioned model gives a lower PPL as
expected (7.39 v.s. 8.54), it is not too far away from the sequence only model (PPL = 9.70). This
suggests that ReﬁneGNN is able to extract a decent amount of information from the partial structure
co-evolving with the sequence."
ANTIGEN-BINDING ANTIBODY DESIGN,0.5742971887550201,"4.2
ANTIGEN-BINDING ANTIBODY DESIGN"
ANTIGEN-BINDING ANTIBODY DESIGN,0.5783132530120482,"Data. Adolf-Bryfogle et al. (2018) selected 60 antibody-antigen complexes as an antibody design
benchmark. Given the framework of an antibody, the goal is to design its CDR-H3 that binds to its
corresponding antigen. For simplicity, none of the methods is conditioned on the antigen structure
during CDR-H3 generation. We leave antigen-conditioned CDR generation for future work."
ANTIGEN-BINDING ANTIBODY DESIGN,0.5823293172690763,"Metric. Following Adolf-Bryfogle et al. (2018), we use amino acid recovery (AAR) as the evalu-
ation metric. For any generated sequence, we deﬁne its AAR as the percentage of residues having
the same amino acid as the corresponding residue in the original antibody."
ANTIGEN-BINDING ANTIBODY DESIGN,0.5863453815261044,"Results. For LSTM, AR-GNN, and ReﬁneGNN, the training set in this setup is the entire SAbDab
except antibodies in the same cluster as any of the test antibodies. At test time, we generate 10000
CDR-H3 sequences for each antibody and select the top 100 candidates with the lowest perplexity.
For simplicity, all methods are conﬁgured to generate CDRs of the same length as the original
CDR. As shown in Table 1, our model achieves the highest AAR score, with around 7% absolute
improvement over the best baseline. In Figure 4A, we show an example of a generated CDR-
H3 sequence and highlight residues that are different from the original antibody. We also found
that sequences with lower perplexity tend to have a lower AA recovery error (Pearson R = 0.427,
Figure 4B). This suggests that we can use perplexity as the ranking criterion for antibody design."
ANTIGEN-BINDING ANTIBODY DESIGN,0.5903614457831325,"4.3
SARS-COV-2 NEUTRALIZATION OPTIMIZATION"
ANTIGEN-BINDING ANTIBODY DESIGN,0.5943775100401606,"Data. The Coronavirus Antibody Database (CoVAbDab) contains 2411 antibodies, each associ-
ated with multiple binary labels indicating whether it neutralizes a coronavirus (SARS-CoV-1 or
SARS-CoV-2) at a certain epitope. Similar to the previous experiment, we divide the antibodies into
training, validation, and test sets based on CDR-H3 cluster split with 8:1:1 ratio."
ANTIGEN-BINDING ANTIBODY DESIGN,0.5983935742971888,"Neutralization predictor. The predictor takes as input the VH sequence of an antibody and out-
puts a neutralization probability for the SARS-CoV-1 and SARS-CoV-2 viruses. Each residue is
embedded into a 64 dimensional vector, which is fed to a SRU encoder (Lei, 2021) followed by
average-pooling and a two-layer feed forward network. The ﬁnal outputs are the probabilities p1"
ANTIGEN-BINDING ANTIBODY DESIGN,0.6024096385542169,Published as a conference paper at ICLR 2022
ANTIGEN-BINDING ANTIBODY DESIGN,0.606425702811245,"Figure 4: (A) Visualization of a generated CDR-H3 sequence and its structure in complex with an
antigen (PDB: 4cmh). The predicted structure is aligned and grafted onto the original antibody using
the Kabsch algorithm. Residues different from the original antibody are highlighted in red. (B) The
correlation between the perplexity of a generated sequence and AA recovery error."
ANTIGEN-BINDING ANTIBODY DESIGN,0.6104417670682731,"Table 2: SARS-CoV-2 neutralization optimization results. For each method, we report the PPL on
CoVAbDab after pretraining on SAbDab and then report the average neutralization score after ITA
ﬁnetuning. The average neutralization probability of original CoVAbDab antibodies is 69.3%."
ANTIGEN-BINDING ANTIBODY DESIGN,0.6144578313253012,"Original
LSTM
AR-GNN
ReﬁneGNN
CoVAbDab PPL (↓)
-
9.40
8.67
7.86
Neutralization (↑)
69.3%
72.0%
70.4%
75.2%"
ANTIGEN-BINDING ANTIBODY DESIGN,0.6184738955823293,"and p2 of neutralizing SARS-CoV-1 and SARS-CoV-2 and our scoring function is f(s) = p2. The
predictor achieved 0.81 test AUROC for SARS-CoV-2 neutralization prediction."
ANTIGEN-BINDING ANTIBODY DESIGN,0.6224899598393574,"CDR sequence constraints. Therapeutic antibodies must be free from developability issues such
as glycosylation and high charges (Raybould et al., 2019). Thus, we include four constraints on a
CDR-H3 sequence s: 1) Its net charge must be between -2.0 and 2.0 (Raybould et al., 2019). The
deﬁnition of net charge is given in the appendix. 2) It must not contain the N-X-S/T motif which is
prone to glycosylation. 3) Any amino acid should not repeat more than ﬁve times (e.g. SSSSS). 4)
Perplexity of a generated sequence given by LSTM, AR-GNN, and ReﬁneGNN should be all less
than 10. The last two constraints force generated sequences to be realistic. We use all three models
in the perplexity constraint to ensure a fair comparison for all methods."
ANTIGEN-BINDING ANTIBODY DESIGN,0.6265060240963856,"Metric. For each antibody in the test set, we generate 100 new CDR-H3 sequences, concatenate
them with its context sequence to form 100 full VH sequences, and feed them into the neutralization
predictor f. We report the average neutralization score of antibodies in the test set. Neutralization
score of a generated sequence s′ equals f(s′) if it satisﬁes all the CDR sequence constraints. Oth-
erwise the score is the same as the original sequence. In addition, we pretrain each model on the
SAbDab CDR-H3 sequences and evaluate its PPL on the CoVAbDab CDR-H3 sequences."
ANTIGEN-BINDING ANTIBODY DESIGN,0.6305220883534136,"Results. All methods are pretrained on SAbDab antibodies and ﬁnetuned on CoVAbDab using the
ITA algorithm to generate neutralizing antibodies. Our model outperforms the best baseline by a 3%
increase in terms of average neutralization score (Table 2). Our pretrained ReﬁneGNN also achieves
a much lower perplexity on CoVAbDab antibodies (7.86 v.s. 8.67). Examples of generated CDR-H3
sequences and their predicted neutralization scores are shown in the appendix."
CONCLUSION,0.6345381526104418,"5
CONCLUSION"
CONCLUSION,0.6385542168674698,"In this paper, we developed a ReﬁneGNN model for antibody sequence and structure co-design.
The advantage of our model over previous graph generation methods is its ability to revise a gen-
erated subgraph to accommodate addition of new residues. Our approach signiﬁcantly outperforms
sequence-based and graph-based approaches on three antibody generation tasks."
CONCLUSION,0.642570281124498,Published as a conference paper at ICLR 2022
CONCLUSION,0.6465863453815262,ACKNOWLEDGEMENT
CONCLUSION,0.6506024096385542,"We would like to thank Rachel Wu, Xiang Fu, Jason Yim, and Peter Mikhael for their valuable
feedback on the manuscript. We also want to thank Nitan Shalon, Nicholas Webb, Jae Hyeon Lee,
Qiu Yu, and Galit Alter for their suggestions on method development. We are grateful for the
generous support of Mark and Lisa Schwartz, funding in a form of research grant from Sanoﬁ,
Defense Threat Reduction Agency (DTRA), C3.ai Digital Transformation Institute, Eric and Wendy
Schmidt Center at the Broad Institute, Abdul Latif Jameel Clinic for Machine Learning in Health,
DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats
program, and DARPA Accelerated Molecular Discovery program."
REFERENCES,0.6546184738955824,REFERENCES
REFERENCES,0.6586345381526104,"Abul K Abbas, Andrew H Lichtman, and Shiv Pillai. Cellular and molecular immunology E-book.
Elsevier Health Sciences, 2014."
REFERENCES,0.6626506024096386,"Jared Adolf-Bryfogle, Oleks Kalyuzhniy, Michael Kubitz, Brian D Weitzner, Xiaozhen Hu, Yumiko
Adachi, William R Schief, and Roland L Dunbrack Jr. Rosettaantibodydesign (rabd): A general
framework for computational antibody design. PLoS computational biology, 14(4):e1006112,
2018."
REFERENCES,0.6666666666666666,"Rahmad Akbar, Philippe A Robert, C´edric R Weber, Michael Widrich, Robert Frank, Milena
Pavlovi´c, Lonneke Scheffer, Maria Chernigovskaya, Igor Snapkov, Andrei Slabodkin, et al. In sil-
ico proof of principle of machine learning-based antibody design at unconstrained scale. BioRxiv,
2021."
REFERENCES,0.6706827309236948,"Mohammed M Al Qaraghuli, Karina Kubiak-Ossowska, Valerie A Ferro, and Paul A Mulheran.
Antibody-protein binding and conformational changes: identifying allosteric signalling pathways
to engineer a better effector response. Scientiﬁc reports, 10(1):1–10, 2020."
REFERENCES,0.6746987951807228,"Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M Church.
Uniﬁed rational protein engineering with sequence-based deep representation learning. Nature
methods, 16(12):1315–1322, 2019."
REFERENCES,0.678714859437751,"Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie
Lee, Jue Wang, Qian Cong, Lisa N Kinch, R Dustin Schaeffer, et al. Accurate prediction of
protein structures and interactions using a three-track neural network. Science, 373(6557):871–
876, 2021."
REFERENCES,0.6827309236947792,"Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, and Yang Shen. Fold2seq:
A joint sequence (1d)-fold (3d) embedding-based generative model for protein design. In Inter-
national Conference on Machine Learning, pp. 1261–1271. PMLR, 2021."
REFERENCES,0.6867469879518072,"Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014."
REFERENCES,0.6907630522088354,"James Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye
Shi, and Charlotte M Deane. Sabdab: the structural antibody database. Nucleic acids research,
42(D1):D1140–D1146, 2014."
REFERENCES,0.6947791164658634,"Sharon Fischman and Yanay Ofran. Computational design of antibodies. Current opinion in struc-
tural biology, 51:156–162, 2018."
REFERENCES,0.6987951807228916,"Niklas WA Gebauer, Michael Gastegger, and Kristof T Sch¨utt. Symmetry-adapted generation of
3d point sets for the targeted discovery of molecules. In Proceedings of the 33rd International
Conference on Neural Information Processing Systems, pp. 7566–7578, 2019."
REFERENCES,0.7028112449799196,"Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs.
In International conference on machine learning, pp. 2434–2444. PMLR, 2019."
REFERENCES,0.7068273092369478,"Steven Henikoff and Jorja G Henikoff. Amino acid substitution matrices from protein blocks. Pro-
ceedings of the National Academy of Sciences, 89(22):10915–10919, 1992."
REFERENCES,0.7108433734939759,Published as a conference paper at ICLR 2022
REFERENCES,0.714859437751004,"John Ingraham, Adam Riesselman, Chris Sander, and Debora Marks. Learning protein structure
with a differentiable simulator. In International Conference on Learning Representations, 2018."
REFERENCES,0.7188755020080321,"John Ingraham, Vikas K Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-
based protein design. Neural Information Processing Systems, 2019."
REFERENCES,0.7228915662650602,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs
using structural motifs. In Proceedings of the 37th International Conference on Machine Learn-
ing, volume 119, pp. 4839–4848. PMLR, 2020a."
REFERENCES,0.7269076305220884,"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using
interpretable substructures. In Proceedings of the 37th International Conference on Machine
Learning, volume 119, pp. 4849–4859. PMLR, 2020b."
REFERENCES,0.7309236947791165,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021."
REFERENCES,0.7349397590361446,"Wolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallo-
graphica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32
(5):922–923, 1976."
REFERENCES,0.7389558232931727,"Mostafa Karimi, Shaowen Zhu, Yue Cao, and Yang Shen. De novo protein design for novel folds
using guided conditional wasserstein generative adversarial networks. Journal of Chemical Infor-
mation and Modeling, 60(12):5667–5681, 2020."
REFERENCES,0.7429718875502008,"Gideon D Lapidoth, Dror Baran, Gabriele M Pszolla, Christoffer Norn, Assaf Alon, Michael D
Tyka, and Sarel J Fleishman. Abdesign: A n algorithm for combinatorial backbone design guided
by natural conformations and sequences. Proteins: Structure, Function, and Bioinformatics, 83
(8):1385–1406, 2015."
REFERENCES,0.7469879518072289,"Andrew Leaver-Fay, Michael Tyka, Steven M Lewis, Oliver F Lange, James Thompson, Ron Jacak,
Kristian W Kaufman, P Douglas Renfrew, Colin A Smith, Will Shefﬂer, et al. Rosetta3: an object-
oriented software suite for the simulation and design of macromolecules. Methods in enzymology,
487:545–574, 2011."
REFERENCES,0.751004016064257,"Marie-Paule Lefranc, Christelle Pommi´e, Manuel Ruiz, V´eronique Giudicelli, Elodie Foulquier,
Lisa Truong, Val´erie Thouvenin-Contet, and G´erard Lefranc. Imgt unique numbering for im-
munoglobulin and t cell receptor variable domains and ig superfamily v-like domains. Develop-
mental & Comparative Immunology, 27(1):55–77, 2003."
REFERENCES,0.7550200803212851,"Tao Lei. When attention meets fast recurrence: Training language models with reduced compute.
arXiv preprint arXiv:2102.12459, 2021."
REFERENCES,0.7590361445783133,"Tong Li, Robert J Pantazes, and Costas D Maranas. Optmaven–a new framework for the de novo
design of antibody variable region models targeting speciﬁc antigen epitopes. PloS one, 9(8):
e105954, 2014."
REFERENCES,0.7630522088353414,"Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. arXiv preprint arXiv:1803.03324, 2018."
REFERENCES,0.7670682730923695,"Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel
Urtasun, and Richard Zemel. Efﬁcient graph generation with graph recurrent attention networks.
Advances in Neural Information Processing Systems, 32:4255–4265, 2019."
REFERENCES,0.7710843373493976,"Ge Liu, Haoyang Zeng, Jonas Mueller, Brandon Carter, Ziheng Wang, Jonas Schilz, Geraldine
Horny, Michael E Birnbaum, Stefan Ewert, and David K Gifford. Antibody complementarity
determining region design using high-capacity machine learning. Bioinformatics, 36(7):2126–
2133, 2020."
REFERENCES,0.7751004016064257,"Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L Gaunt. Constrained graph vari-
ational autoencoders for molecule design. Neural Information Processing Systems, 2018."
REFERENCES,0.7791164658634538,Published as a conference paper at ICLR 2022
REFERENCES,0.7831325301204819,"Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2837–2845,
2021."
REFERENCES,0.7871485943775101,"James O’Connell, Zhixiu Li, Jack Hanson, Rhys Heffernan, James Lyons, Kuldip Paliwal, Abdollah
Dehzangi, Yuedong Yang, and Yaoqi Zhou. Spin2: Predicting sequence proﬁles from protein
structures using deep neural networks. Proteins: Structure, Function, and Bioinformatics, 86(6):
629–633, 2018."
REFERENCES,0.7911646586345381,"RJ Pantazes and Costas D Maranas. Optcdr: a general computational method for the design of
antibody complementarity determining regions for targeted epitope binding. Protein Engineering,
Design & Selection, 23(11):849–858, 2010."
REFERENCES,0.7951807228915663,"Dora Pinto, Young-Jun Park, Martina Beltramello, Alexandra C Walls, M Alejandra Tortorici, Siro
Bianchi, Stefano Jaconi, Katja Culap, Fabrizia Zatta, Anna De Marco, et al. Cross-neutralization
of sars-cov-2 by a human monoclonal sars-cov antibody. Nature, 583(7815):290–295, 2020."
REFERENCES,0.7991967871485943,"Matthew IJ Raybould, Claire Marks, Konrad Krawczyk, Bruck Taddese, Jaroslaw Nowak, Alan P
Lewis, Alexander Bujotzek, Jiye Shi, and Charlotte M Deane. Five computational developability
guidelines for therapeutic antibody proﬁling. Proceedings of the National Academy of Sciences,
116(10):4025–4030, 2019."
REFERENCES,0.8032128514056225,"Matthew IJ Raybould, Aleksandr Kovaltsuk, Claire Marks, and Charlotte M Deane. Cov-abdab: the
coronavirus antibody database. Bioinformatics, 37(5):734–735, 2021."
REFERENCES,0.8072289156626506,"Koichiro Saka, Taro Kakuzaki, Shoichi Metsugi, Daiki Kashiwagi, Kenji Yoshida, Manabu Wada,
Hiroyuki Tsunoda, and Reiji Teramoto. Antibody design using lstm based deep generative model
from phage display library for afﬁnity maturation. Scientiﬁc reports, 11(1):1–13, 2021."
REFERENCES,0.8112449799196787,"Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient ﬁelds for molecular confor-
mation generation. International Conference on Machine Learning, 2021."
REFERENCES,0.8152610441767069,"Jung-Eun Shin, Adam J Riesselman, Aaron W Kollasch, Conor McMahon, Elana Simon, Chris
Sander, Aashish Manglik, Andrew C Kruse, and Debora S Marks. Protein design and variant
prediction using autoregressive generative models. Nature communications, 12(1):1–11, 2021."
REFERENCES,0.8192771084337349,"Martin Steinegger and Johannes S¨oding. Mmseqs2 enables sensitive protein sequence searching for
the analysis of massive data sets. Nature biotechnology, 35(11):1026–1028, 2017."
REFERENCES,0.8232931726907631,"Alexey Strokach, David Becerra, Carles Corbi-Verge, Albert Perez-Riba, and Philip M Kim. Fast
and ﬂexible design of novel proteins using graph neural networks. BioRxiv, pp. 868935, 2020."
REFERENCES,0.8273092369477911,"Doug Tischer, Sidney Lisanza, Jue Wang, Runze Dong, Ivan Anishchenko, Lukas F Milles, Sergey
Ovchinnikov, and David Baker. Design of proteins presenting discontinuous functional sites using
deep learning. bioRxiv, 2020."
REFERENCES,0.8313253012048193,"Jianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and David
Baker. Improved protein structure prediction using predicted interresidue orientations. Proceed-
ings of the National Academy of Sciences, 117(3):1496–1503, 2020a."
REFERENCES,0.8353413654618473,"Kevin Yang, Wengong Jin, Kyle Swanson, Regina Barzilay, and Tommi Jaakkola. Improving molec-
ular design by stochastic iterative target augmentation. In International Conference on Machine
Learning, pp. 10716–10726. PMLR, 2020b."
REFERENCES,0.8393574297188755,"Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton, and Jure Leskovec. Graphrnn: A deep
generative model for graphs. International Conference on Machine Learning, 2018."
REFERENCES,0.8433734939759037,Published as a conference paper at ICLR 2022
REFERENCES,0.8473895582329317,"A
MODEL ARCHITECTURE DETAILS"
REFERENCES,0.8514056224899599,"A.1
REFINEGNN"
REFERENCES,0.8554216867469879,Node features. Each node feature vi encodes three dihedral angles as follows.
REFERENCES,0.8594377510040161,"vi = (cos φi, cos ψi, cos ωi, sin φi, sin ψi, sin ωi)
(16)"
REFERENCES,0.8634538152610441,"Edge features. The orientation matrix Oi = [bi, ni, bi × ni] deﬁnes a local coordinate system for
each residue i (Ingraham et al., 2019), which is calculated as"
REFERENCES,0.8674698795180723,"ui =
xi −xi−1
∥xi −xi−1∥,
bi =
ui −ui+1
∥ui −ui+1∥,
ni =
ui × ui+1
∥ui × ui+1∥
(17)"
REFERENCES,0.8714859437751004,Attention mechanism. The attention layer used in Eq.(13) is a standard bilinear attention:
REFERENCES,0.8755020080321285,"attention(c1:n, ht) =
X"
REFERENCES,0.8795180722891566,"i
αi,tci,
αi,t =
exp(c⊤
i W ht)
P"
REFERENCES,0.8835341365461847,"j exp(c⊤
j W ht)
(18)"
REFERENCES,0.8875502008032129,"A.2
AR-GNN"
REFERENCES,0.891566265060241,"AR-GNN generates an antibody graph autoregressively. In each generation step t, AR-GNN learns
to encode the current subgraph G1:t induced from residues {s1, · · · , st} into a list of vectors"
REFERENCES,0.8955823293172691,"{h1, · · · , ht} = MPNθ(G1:t).
(19)"
REFERENCES,0.8995983935742972,"For fair comparison, we use the same MPN architecture for both ReﬁneGNN and AR-GNN. In
terms of structure prediction, AR-GNN ﬁrst predicts the node feature ˆvt+1 of the next residue t + 1,
namely the dihedral angle between its three atoms Cα, C, N."
REFERENCES,0.9036144578313253,"ˆvt+1 = Wvht
(20)"
REFERENCES,0.9076305220883534,"In addition, AR-GNN predicts the pairwise distance between st+1 and previous residues s1, · · · , st.
ˆdi,t+1 = FFN(Wdhi + Udht + VdEpos(t + 1 −i)),
(21)"
REFERENCES,0.9116465863453815,"where FFN is a feed-forward network with one hidden layer and Epos is the positional encoding of
t + 1 −i, the gap between residue st+1 and si in the sequence. Lastly, AR-GNN predicts the amino
acid type of residue st+1 by"
REFERENCES,0.9156626506024096,"ˆpt+1 = softmax(Wagt+1),
{g1, · · · , gt+1} = MPNθ′(G1:t+1)
(22)"
REFERENCES,0.9196787148594378,"Note that AR-GNN also uses two separate MPNs for structure and sequence prediction. However,
unlike ReﬁneGNN, AR-GNN is trained under teacher forcing — we need to feed it the ground
truth structure and sequence in each generation step. In particular, we ﬁnd data augmentation to
be crucial for AR-GNN performance. Data augmentation is essential because of the discrepancy
between training and testing. The model is trained under teacher forcing, but it needs to decode a
graph without teacher forcing at test time. We ﬁnd mistakes made in previous steps have a great
impact on subsequent predictions during decoding."
REFERENCES,0.9236947791164659,"Speciﬁcally, for every antibody s, we create a corrupted graph eG by adding independent random
Gaussian noise to every coordinate: ˜xi = xi + 3ϵ, ϵ ∼N(0, I). In each generation step, we apply
MPN over the corrupted graph instead."
REFERENCES,0.927710843373494,"{eh1, · · · , eht} = MPNθ( eG1:t),
{eg1, · · · , egt+1} = MPNθ′( eG1:t+1)
(23)"
REFERENCES,0.9317269076305221,"The node and edge labels are still deﬁned by the ground truth structure. Speciﬁcally, let vt and di,j
be the ground truth dihedral angle and pairwise distance calculated from the original, uncorrupted
graph G. AR-GNN loss function is deﬁned as the following."
REFERENCES,0.9357429718875502,"LAR =
X"
REFERENCES,0.9397590361445783,"i,j
∥ˆdi,j −di,j∥2 +
X"
REFERENCES,0.9437751004016064,"t
∥ˆvt −vt∥2 + Lce(ˆpt, st)
(24)"
REFERENCES,0.9477911646586346,"Similar to ReﬁneGNN, AR-GNN also uses attention mechanism for conditional generation. Specif-
ically, we concatenate the residue representations eht, egt from MPN with context vectors learned
from an attention layer.
eht ←eht ⊕attention(c1:n, eht)
egt ←egt ⊕attention(c1:n, egt)
(25)"
REFERENCES,0.9518072289156626,Published as a conference paper at ICLR 2022
REFERENCES,0.9558232931726908,"Table 3: SARS-CoV-2 neutralization optimization results. Here we show examples of new CDR-
H3 sequences generated by our model and their predicted neutralization improvement over original
antibodies S1D7 and C694 in the CoVAbDab database."
REFERENCES,0.9598393574297188,"Antibody:
S1D7
C694
Old CDR-H3
TRGHSDY
ARDRGYDSSGPDAFDI
New CDR-H3
ARWWMDV
ARERIIIVSISAWMDV
Improvement
63% →73%
82% →91%"
REFERENCES,0.963855421686747,"B
EXPERIMENTAL DETAILS"
REFERENCES,0.9678714859437751,"Hyperparameters. For AR-GNN and ReﬁneGNN, we tried hidden dimension dh ∈{128, 256}
and number of message passing layers L ∈{1, 2, 3, 4, 5}. We found dh = 256, L = 4 worked
the best for ReﬁneGNN and dh = 256, L = 3 worked the best for AR-GNN. For LSTM, we tried
dh ∈{128, 256, 512, 1024}. We found dh = 256 worked the best. All models are trained by an
Adam optimizer with a dropout of 0.1 and a learning rate of 0.001."
REFERENCES,0.9718875502008032,"SAbDab data. The dataset statistics of SAbDab is the following (after deduplication). For CDR-
H1, the train/validation/test size is 4050, 359, and 326. For CDR-H2, the train/validation/test size is
3876, 483, and 376. For CDR-H3, the train/validation/test size is 3896, 403, and 437."
REFERENCES,0.9759036144578314,"Since SAbDab includes both bound and unbound structures, we removed all antigens and used the
bound antibody structure for training. Speciﬁcally, 65% of our training data came from bound
state structures. We included all data in our training set because the mismatch between bound and
unbound structures is relatively small. In fact, Al Qaraghuli et al. (2020) studied eight antibodies
and found that the RMSD between bound and unbound structures over VH domains is less than 0.7
on average."
REFERENCES,0.9799196787148594,"RAbD conﬁguration. We provided details of the de novo design setup of RosettaAntibodyDesign
(RAbD) here. For each antibody in the test set, RAbD starts by randomly selecting a CDR from
RAbD’s internal database of known CDR structures. The chosen CDR-H3 sequence is required to
have same length as the original sequence, but it cannot be exactly the same as the original CDR-H3
sequence. After the initial CDR structure is chosen, RAbD grafts it onto the antibody and performs
energy minimization to stabilize its structure. Next, RAbD runs 100 iterations of sequence design
to modify the grafted CDR-H3 structure by randomly substituting amino acids. In each sequence
design iteration, it performs energy minimization to adjust the structure according to the changed
amino acid. Lastly, the model returns the generated CDR-H3 sequence with the lowest energy."
REFERENCES,0.9839357429718876,"SARS-CoV-2 neutralization. Each generative model is pretrained on the SAbDab data to learn a
prior distribution over CDR-H3 structures. Given a ﬁxed predictor f, we use the ITA algorithm to
ﬁnetune our pretrained models to generate neutralizing antibodies. Each model is trained for 3000
ITA steps with M = 100. Generated CDR-H3 sequences from our model are visualized in Table 3."
REFERENCES,0.9879518072289156,"Our neutralization predictor f is trained on the CoVAbDab database. For simplicity, we only con-
sider two viruses, SARS-CoV-1 and SARS-CoV-2 since other coronavirus have very little training
data. For the same reason, we only consider the spike protein receptor binding domain as our target
epitope. The predictor is trained in a multi-task fashion to predict both SARS-CoV-1 and SARS-
CoV-2 neutralization labels. The SRU encoder has a hidden dimension of 256. The model was
trained with a dropout of 0.2, a learning rate of 0.0005, and batch size of 16."
REFERENCES,0.9919678714859438,"The charge of a residue is deﬁned as C(si) = I[si ∈{R, K}] + 0.1 · I[si = H] −I[si ∈{D, E}]
(Raybould et al., 2019). The net charge of a sequence s1 · · · sn is deﬁned as P"
REFERENCES,0.9959839357429718,i C(si).
