Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0030959752321981426,"Fair representation learning (FRL) is a popular class of methods aiming to produce
fair classiﬁers via data preprocessing. However, recent work has shown that prior
methods achieve worse accuracy-fairness tradeoffs than originally suggested by
their results. This dictates the need for FRL methods that provide provable upper
bounds on unfairness of any downstream classiﬁer, a challenge yet unsolved. In
this work we address this challenge and propose Fairness with Restricted Encoders
(FARE), the ﬁrst FRL method with provable fairness guarantees. Our key in-
sight is that restricting the representation space of the encoder enables us to derive
suitable fairness guarantees, while allowing empirical accuracy-fairness tradeoffs
comparable to prior work. FARE instantiates this idea with a tree-based encoder,
a choice motivated by inherent advantages of decision trees when applied in our
setting. Crucially, we develop and apply a practical statistical procedure that com-
putes a high-conﬁdence upper bound on the unfairness of any downstream clas-
siﬁer. In our experimental evaluation on several datasets and settings we demon-
strate that FARE produces tight upper bounds, often comparable with empirical
results of prior methods, which establishes the practical value of our approach."
INTRODUCTION,0.006191950464396285,"1
INTRODUCTION"
INTRODUCTION,0.009287925696594427,"It has been repeatedly shown that machine learning systems deployed in real-world applications
propagate training data biases, producing discriminatory predictions (Buolamwini & Gebru, 2018;
Corbett-Davies et al., 2017; Kleinberg et al., 2017; Tatman & Kasten, 2017). This is especially con-
cerning in decision-making applications on data that represents humans (e.g., ﬁnancial or medical),
and can lead to unfavorable treatment that negatively affects certain subgroups of the population
(Brennan et al., 2009; Khandani et al., 2010; Barocas & Selbst, 2016). For instance, a loan predic-
tion system deployed by a ﬁnancial institution might recommend loan rejection based on a sensitive
attribute of a client, such as race or gender. These observations have forced regulators into action,
leading to directives (FTC, 2021; EU, 2021) which demand parties aiming to deploy such systems
to ensure fairness (Dwork et al., 2012) of their predictions. Mitigation of unfairness has become
a key concern for organizations, with the highest increase in perceived relevance over the previous
year, out of all potential risks of artiﬁcial intelligence (Chui et al., 2021; Benaich & Hogarth, 2021)."
INTRODUCTION,0.01238390092879257,"Fair representation learning
A promising approach that attempts to address this issue is fair
representation learning (FRL) (Zemel et al., 2013; Moyer et al., 2018; Madras et al., 2018; Gupta
et al., 2021; Kim et al., 2022; Shui et al., 2022; Balunovi´c et al., 2022)—a long line of work that
preprocesses the data using an encoder f, transforming each datapoint x ∈X into a debiased rep-
resentation z. The key promise of FRL is that these debiased representations can be given to other
parties, who want to solve a prediction task without being aware of fairness (or potentially even
being ﬁne with discriminating), while ensuring that any downstream classiﬁer they train on these
representations has favorable fairness. However, recent work (Xu et al., 2020; Song & Shmatikov,
2020; Gupta et al., 2021) has demonstrated that for some FRL methods it is possible to train sig-
niﬁcantly more unfair classiﬁers than originally claimed. This illuminates a major drawback of all
existing work—their claim about fairness of the downstream classiﬁers holds only for the models
they considered during the evaluation, and does not guarantee favorable fairness of other down-
stream classiﬁers trained on z. This is insufﬁcient for critical applications where fairness must be
guaranteed or is enforced by regulations, leading to our key question:"
INTRODUCTION,0.015479876160990712,Can we create an FRL method that provably bounds the unfairness of any downstream classiﬁer?
INTRODUCTION,0.018575851393188854,Under review as a conference paper at ICLR 2023
INTRODUCTION,0.021671826625386997,Restricted encoder
INTRODUCTION,0.02476780185758514,"Input dataset
Representations"
INTRODUCTION,0.02786377708978328,"Figure 1: Overview of our provably fair representation learning method, FARE. The input dataset is
transformed into fair representations using a restricted encoder. Our method can compute a provable
upper bound T on unfairness of any classiﬁer g ∈G trained on these representations."
INTRODUCTION,0.030959752321981424,"The most prominent prior attempt to tackle this question, and the work most closely related to ours,
is FNF (Balunovi´c et al., 2022); we discuss other related work in Section 2. Assuming two groups
s = 0 and s = 1 based on the sensitive attribute s, FNF shows that knowing the input distribution for
each group can lead to an upper bound on unfairness of any downstream classiﬁer. While this work is
an important step towards provable fairness, the required assumption is unrealistic for most machine
learning settings, and represents an obstacle to applying the approach in practice. Thus, the original
problem of creating FRL methods that provide fairness guarantees remains largely unsolved."
INTRODUCTION,0.034055727554179564,"This work: provably fair representation learning
We propose FARE (Fairness with Restricted
Encoders, Fig. 1)—the ﬁrst FRL method that offers provable upper bounds on the unfairness of any
downstream classiﬁer g trained on its representations, without unrealistic prior assumptions. Our key
insight is that using an encoder with restricted representations, i.e., limiting possible representations
to a ﬁnite set {z1, . . . , zk}, allows us to derive a practical statistical procedure that computes a high-
conﬁdence upper bound on the unfairness of any g, detailed in Section 4. FARE instantiates this idea
with a suitable encoder based on fair decision trees (see Section 5), leading to a practical end-to-end
FRL method which produces debiased representations augmented with strong fairness guarantees."
INTRODUCTION,0.03715170278637771,"More concretely, FARE takes as input the set of samples {x(1), . . . , x(n)} from the input distribution
X (left of Fig. 1), and partitions the input space into k cells (middle plane, k = 3 in this example)
using the decision tree encoder. Finally, all samples from the same cell i are transformed into the
same representation zi (right). As usual in FRL, training a downstream classiﬁer on representations
leads to lower empirical unfairness, while slightly sacriﬁcing accuracy on the prediction task."
INTRODUCTION,0.04024767801857585,"However, the main advantage of FARE comes from the fact that using a restricted set of repre-
sentations allows us to, using the given samples, estimate the distribution of two sensitive groups
in each cell, i.e., compute an empirical estimate of the conditional probabilities P(s = 0|zi) and
P(s = 1|zi) (solid color orange bars) for all zi. Further, we can use conﬁdence intervals to obtain
upper bounds on these values that hold with high probability (transparent bars). As noted above, this
in turn leads to the key feature of our method: a tight upper bound T on the unfairness of any g ∈G,
where G is the set of all downstream classiﬁers that can be trained on the resulting representations.
As we later elaborate on, increasing the number of samples n makes the bounds tighter. Given the
current trend of rapidly growing datasets, this further illustrates the practical value of FARE."
INTRODUCTION,0.043343653250773995,"In our experimental evaluation in Section 6 we empirically demonstrate that on real datasets FARE
produces tight upper bounds, i.e., the unfairness of any downstream classiﬁer trained on FARE
representations is tightly upper-bounded, which was not possible for any of the previously proposed
FRL methods. Moreover, these downstream classiﬁers are able to achieve comparable empirical
accuracy-fairness tradeoffs to methods from prior work. We believe this work represents a major
step towards solving the important problem of preventing discriminatory machine learning models."
INTRODUCTION,0.04643962848297214,Under review as a conference paper at ICLR 2023
INTRODUCTION,0.04953560371517028,"Main contributions
The key contributions of our work are:"
INTRODUCTION,0.05263157894736842,"• A practical statistical procedure that, for restricted encoders, upper-bounds the unfairness
of any downstream classiﬁer trained on their representations (Section 4)."
INTRODUCTION,0.05572755417956656,"• An end-to-end FRL method FARE, that instantiates this approach with a fair decision tree
encoder (Section 5), and applies the said statistical procedure to augment the representa-
tions with a tight provable upper bound on unfairness of any downstream classiﬁer."
INTRODUCTION,0.058823529411764705,"• An extensive experimental evaluation in several settings, demonstrating favorable empirical
fairness results, as well as tight upper bounds on unfairness (which were out of reach for
prior work), often comparable to empirical results of existing FRL methods (Section 6)."
LIT REVIEW,0.06191950464396285,"2
RELATED WORK"
LIT REVIEW,0.06501547987616099,"We discuss related work on FRL, prior attempts to obtain fairness guarantees, and fair decision trees."
LIT REVIEW,0.06811145510835913,"FRL for group fairness
Following Zemel et al. (2013) which originally introduced FRL, a
plethora of different methods have been proposed based on optimization (Calmon et al., 2017; Shui
et al., 2022), adversarial training (Edwards & Storkey, 2016; Xie et al., 2017; Madras et al., 2018;
Song et al., 2019; Feng et al., 2019; Roy & Boddeti, 2019; Jaiswal et al., 2020; Kairouz et al., 2022;
Kim et al., 2022), variational approaches (Louizos et al., 2016; Moyer et al., 2018; Oh et al., 2022;
Liu et al., 2022), disentanglement (Sarhan et al., 2020), mutual information (Gupta et al., 2021;
Gitiaux & Rangwala, 2022), and normalizing ﬂows (Balunovi´c et al., 2022; Cerrato et al., 2022).
Notably, no prior method restricts representations as deﬁned in Section 4, which is a key step in our
work. While Zemel et al. (2013) map data to a set of prototypes, this mapping is probabilistic, and
thus fundamentally incompatible with our bounding procedure (see Section 4 for further details)."
LIT REVIEW,0.07120743034055728,"Towards fairness guarantees
The key issue is that most of these methods produce representations
that have no provable guarantees of fairness, meaning a model trained on their representations could
have arbitrarily bad fairness. In fact, prior work (Elazar & Goldberg, 2018; Xu et al., 2020; Gupta
et al., 2021) has shown that methods based on adversarial training often signiﬁcantly overestimate
the fairness of their representations. Some of these works (McNamara et al., 2017; Zhao et al., 2020;
Gupta et al., 2021; Shen et al., 2021; Kairouz et al., 2022) propose theoretically-principled FRL, but
do not provide provable fairness guarantees with ﬁnite samples (we discuss the difference in Ap-
pendix E). Closest to our work is FNF (Balunovi´c et al., 2022) that can compute high-conﬁdence
bounds, but critically, assumes knowledge of the input probability distribution, which is rarely the
case in practice. Our work makes no such assumption, which makes it signiﬁcantly more practical."
LIT REVIEW,0.07430340557275542,"Provable fairness in other settings
Numerous related works on provable fairness provide a dif-
ferent kind of guarantee or assume a different setting than ours. First, several FRL methods have
proposed approaches for learning individually fair representations (Lahoti et al., 2019; Ruoss et al.,
2020; Peychev et al., 2021), a different notion of fairness than group fairness which we focus on.
Prior work has also examined provable fairness guarantees in various problem settings such as rank-
ing (Konstantinov & Lampert, 2021), distribution shifting (Kang et al., 2022; Jin et al., 2022), fair
classiﬁcation with in-processing (Feldman et al., 2015; Donini et al., 2018; Celis et al., 2019), indi-
vidually fair classiﬁcation with post-processing (Petersen et al., 2021), and fair meta-learning (Oneto
et al., 2020). These are all different from our setting, which is FRL for group fairness."
LIT REVIEW,0.07739938080495357,"Fair decision trees
The line of work focusing on adapting decision trees to fairness concerns
includes a wide range of methods which differ mainly in the branching criterion. Common choices
include variations of Gini impurity (Kamiran et al., 2010; Raff et al., 2018; Zhang & Ntoutsi, 2019),
mixed-integer programming (Aghaei et al., 2019; Wang et al., 2022) or AUC (Barata & Veenman,
2021), while some apply adversarial training (Grari et al., 2020; Ranzato et al., 2021). Further,
some works operate in a different setting such as online learning (Zhang & Ntoutsi, 2019) or post-
processing (Abebe et al., 2022). The only works in this area that offer provable fairness guarantees
are Ranzato et al. (2021), which certiﬁes individual fairness for post-processing, and Meyer et al.
(2021), which certiﬁes that predictions will not be affected by data changes. This fundamentally
differs from our FRL setting where the goal is to certify fairness of any downstream classiﬁer."
LIT REVIEW,0.0804953560371517,Under review as a conference paper at ICLR 2023
LIT REVIEW,0.08359133126934984,"3
PRELIMINARIES"
LIT REVIEW,0.08668730650154799,We now set up the notation and provide the background necessary to understand our contributions.
LIT REVIEW,0.08978328173374613,"Fair representation learning
Assume data (x, s) ∈Rd ×{0, 1} from a joint probability distribu-
tion X, where each datapoint belongs to a group with respect to a sensitive attribute s. While in this
work we focus on the case of binary s, our results can be directly extended to other settings. Further,
we focus on binary classiﬁcation, i.e., given a label y ∈{0, 1} associated with each datapoint, the
goal is to build a model g: Rd →{0, 1} that predicts y from x. Besides maximizing accuracy
of g, we aim to maximize its fairness with respect to sensitive groups, according to some fairness
deﬁnition, which often implies a slight loss in accuracy as these two goals are generally at odds."
LIT REVIEW,0.09287925696594428,"A large class of methods aims to directly produce g with satisfactory fairness properties. A different
group of methods, our focus here, preprocesses data by applying an encoder f : Rd →Rd′ to obtain
a new representation z = f(x, s) of each datapoint. This induces a joint distribution Z of (z, s).
The downstream classiﬁer g is now trained to predict y from z, i.e., now we have g: Rd′ →{0, 1}.
The main advantage of these methods is that by ensuring fairness properties of representations z,
we can limit the unfairness of any g trained on data from Z."
LIT REVIEW,0.09597523219814241,"Fairness metric
Let Z0 and Z1 denote conditional distributions of z where s = 0 and s = 1,
respectively. In this work, we aim to minimize the demographic parity distance of g, reﬂecting the
goal of equally likely assigning positive outcomes to inputs from both sensitive groups:"
LIT REVIEW,0.09907120743034056,"∆DP
Z0,Z1(g) := |Ez∼Z0[g(z)] −Ez∼Z1[g(z)]| ."
LIT REVIEW,0.1021671826625387,"Our choice of metric is primarily motivated by consistency with prior work—other deﬁnitions (e.g.,
equalized odds) may be more suitable for a particular use-case (Hardt et al., 2016), and our method
can be easily adapted to support them, following the corresponding results of Madras et al. (2018)."
LIT REVIEW,0.10526315789473684,"In the remainder of this work, we will use p0 and p1 to denote the PDFs of Z0 and Z1 respectively,
i.e., p0(zi) = P(zi|s = 0) and p1(zi) = P(zi|s = 1) and p to denote the PDF of the marginal
distribution of z. Similarly, we will use q for the marginal distribution of s, and qi for the conditional
distribution of s for z = zi, i.e., qi(0) = P(s = 0|z = zi) and qi(1) = P(s = 1|z = zi)."
LIT REVIEW,0.10835913312693499,"Classiﬁcation trees
Starting from the training set Droot of examples (x, y) ∈Rd × {0, 1}, a
binary classiﬁcation tree f repeatedly splits some leaf node P with assigned DP , i.e., picks a split
feature j ∈{1, . . . , d} and a split threshold v, and adds two nodes L and R as children of P, such
that DL = {(x, y) ∈DP | xj ≤v} and DR = DP \ DL. j and v are picked to minimize a chosen
criterion, weighted by |DL| and |DR|, aiming to produce y-homogeneous leaves. We focus on Gini
impurity, computed as Giniy(D) = 2py(1 −py) ∈[0, 0.5] where py = P"
LIT REVIEW,0.11145510835913312,"(x,y)∈D 1{y = 1}/|D|.
At inference, a test example x is propagated to a leaf l, and we predict the majority class of Dl."
IMPLEMENTATION/METHODS,0.11455108359133127,"4
PROVABLE FAIRNESS BOUNDS FOR RESTRICTED ENCODERS"
IMPLEMENTATION/METHODS,0.11764705882352941,"In the following we describe our primary contribution, the derivation of provable fairness bounds
on unfairness of downstream classiﬁers, under the assumption of restricted encoders (i.e., encoders
with restricted representations, explained in more detail shortly). In Section 5 we demonstrate the
feasibility of our approach, by instantiating it with a particular encoder based on decision trees."
IMPLEMENTATION/METHODS,0.12074303405572756,"Optimal adversary
Consider the adversary h: Rd′ →{0, 1} predicting group membership s,
which aims to maximize the following balanced accuracy objective:"
IMPLEMENTATION/METHODS,0.1238390092879257,"BAZ0,Z1(h) := 1"
IMPLEMENTATION/METHODS,0.12693498452012383,"2 (Ez∼Z0[1 −h(z)] + Ez∼Z1[h(z)]) .
(1)"
IMPLEMENTATION/METHODS,0.13003095975232198,"Let h⋆, such that for all h, BAZ0,Z1(h⋆) ≥BAZ0,Z1(h), denote the optimal adversary. Intuitively,
the optimal adversary predicts the group s for which the likelihood of z under the corresponding
distribution (Z0 or Z1) is larger. More formally, h⋆(z) = 1{p1(z) ≥p0(z)}, where 1{φ} = 1 if φ
holds, and 0 otherwise (see Balunovi´c et al. (2022) for a proof). As shown in Madras et al. (2018),"
IMPLEMENTATION/METHODS,0.13312693498452013,"∆DP
Z0,Z1(g) ≤2 · BAZ0,Z1(h⋆) −1
(2)"
IMPLEMENTATION/METHODS,0.13622291021671826,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.1393188854489164,"holds for any g, i.e., we can upper-bound the unfairness of any downstream classiﬁer trained on data
from Z by computing the balanced accuracy of the optimal adversary h⋆. Shen et al. (2021) also
discuss connection between the balanced accuracy and total variation. h3 h12 · · · h7"
IMPLEMENTATION/METHODS,0.14241486068111456,h∗= hi S
IMPLEMENTATION/METHODS,0.14551083591331268,"BAZ0,Z1 CI"
IMPLEMENTATION/METHODS,0.14860681114551083,"Figure 2:
Restricted rep-
resentations enable upper-
bounding of BAZ0,Z1(h⋆)."
IMPLEMENTATION/METHODS,0.15170278637770898,"Restricted encoders
Prior work is unable to utilize Eq. (2) to ob-
tain a fairness guarantee, as using unconstrained neural network en-
coders generally makes it intractable to compute the densities p0(z)
and p1(z) that deﬁne the optimal adversary h∗. Notably, Balunovi´c
et al. (2022) use normalizing ﬂows, allowing computation of p0(z)
and p1(z) under the assumption of knowing corresponding densities
in the original distribution X. In contrast, we propose a class of en-
coders for which we can derive a procedure that can upper-bound the
RHS of Eq. (2), and thus the unfairness of g, without imposing any
assumption in terms of knowledge of X. We rely only on a set of
samples (z, s) ∼Z, obtained by applying f to samples (x, s) ∼X,
which are readily available in the form of a given dataset."
IMPLEMENTATION/METHODS,0.15479876160990713,"Namely, we hypothesize that restricting the space of representations
can still lead to favorable fairness-accuracy tradeoffs. Based on this,
we propose restricted encoders f : Rd →{z1, . . . , zk}, i.e., encoders
that map each x to one of k possible values (cells) zi ∈Rd′. As now
there is a ﬁnite number of possible values for a representation, we
can use samples from Z to analyze the optimal adversary h∗on each
possible z. Moreover, we can upper-bound its balanced accuracy on the whole distribution Z with
some value S with high probability, using conﬁdence intervals (CI) (as illustrated in Fig. 2). Finally,
we can apply Eq. (2) to obtain the bound ∆DP
Z0,Z1(g) ≤2S −1 = T."
IMPLEMENTATION/METHODS,0.15789473684210525,"A detailed presentation of our upper-bounding procedure follows. In Sections 5 and 6 we demon-
strate how to design practical and efﬁcient restricted encoders, by describing and evaluating our
instantiation based on decision trees, which, as we later discuss, are naturally suited to this problem."
IMPLEMENTATION/METHODS,0.1609907120743034,"Upper-bounding the balanced accuracy
Starting from Eq. (1), we reformulate the balanced ac-
curacy of the optimal adversary as follows:"
IMPLEMENTATION/METHODS,0.16408668730650156,"BAZ0,Z1(h∗) = 1 2 k
X"
IMPLEMENTATION/METHODS,0.16718266253869968,"i=1
p0(zi) · [1 −h∗(zi)] + k
X"
IMPLEMENTATION/METHODS,0.17027863777089783,"i=1
p1(zi) · [h∗(zi)] !"
IMPLEMENTATION/METHODS,0.17337461300309598,"(E of discrete RV) = 1 2 k
X"
IMPLEMENTATION/METHODS,0.17647058823529413,"i=1
max (p0(zi), p1(zi)) !"
IMPLEMENTATION/METHODS,0.17956656346749225,"(Optimal adversary) = k
X"
IMPLEMENTATION/METHODS,0.1826625386996904,"i=1
p(zi) · max "
IMPLEMENTATION/METHODS,0.18575851393188855,"(1/2q(0))
|
{z
}
α0"
IMPLEMENTATION/METHODS,0.18885448916408668,"·qi(0), (1/2q(1))
|
{z
}
α1"
IMPLEMENTATION/METHODS,0.19195046439628483,·qi(1) 
IMPLEMENTATION/METHODS,0.19504643962848298,",
(Bayes’ rule)"
IMPLEMENTATION/METHODS,0.19814241486068113,"where applications of Bayes’ rule are p0(zi) = qi(0)p(zi)/q(0) and p1(zi) = qi(1)p(zi)/q(1). As
we do not know Z, but instead have access to n samples (z(j), s(j)) ∼Z, we will aim to upper-
bound BAZ0,Z1(h∗) with high probability. In particular, we focus on the ﬁnal expression above, the
prior-weighted (i.e., weighted by p(zi)) per-cell balanced accuracy (i.e., max(α0qi(0), α1qi(1)) for
each cell i), where we deﬁne α0 = 1/2q(0) and α1 = 1/2q(1)."
IMPLEMENTATION/METHODS,0.20123839009287925,"Next, we introduce 3 lemmas, and later combine them to obtain the desired upper bound. We use
B(p; v, w) to denote the p-th quantile of a beta distribution with parameters v and w. Note that for
Lemma 1 we do not use the values z(j) in the proof, but still introduce them for consistency."
IMPLEMENTATION/METHODS,0.2043343653250774,"Lemma 1 (Bounding base rates). Given n independent samples {(z(1), s(1)), . . . , (z(n), s(n))} ∼Z
and a parameter ϵb, for α0 and α1 as deﬁned above, it holds that"
IMPLEMENTATION/METHODS,0.20743034055727555,"α0 <
1
2B( ϵb"
IMPLEMENTATION/METHODS,0.21052631578947367,"2 ; m, n −m + 1),
and
α1 <
1
2(1 −B(1 −ϵb"
IMPLEMENTATION/METHODS,0.21362229102167182,"2 ; m + 1, n −m)),"
IMPLEMENTATION/METHODS,0.21671826625386997,"with conﬁdence 1 −ϵb, where m = Pn
j=1 1{s(j) = 0}."
IMPLEMENTATION/METHODS,0.21981424148606812,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.22291021671826625,"Proof. We deﬁne n independent Bernoulli random variables X(j) := 1{s(j) = 0} with same un-
known success probability q(0). Using the Clopper-Pearson binomial CI (Clopper & Pearson, 1934)
(Appendix A) to estimate the probability q(0) we get P(q(0) ≤B( ϵb"
IMPLEMENTATION/METHODS,0.2260061919504644,"2 ; m, n −m + 1)) ≤ϵb/2 and
P(q(0) ≥B(1 −ϵb"
IMPLEMENTATION/METHODS,0.22910216718266255,"2 ; m + 1, n −m)) ≤ϵb/2. Substituting q(0) = 1 −q(1) in the latter, as well as
the deﬁnitions of α0 and α1 in both inequalities, produces the inequalities from the lemma statement,
which per union bound simultaneously hold with conﬁdence 1 −ϵb."
IMPLEMENTATION/METHODS,0.23219814241486067,"Lemma 2 (Bounding balanced accuracy for each cell).
Given n independent samples
{(z(1), s(1)), . . . , (z(n), s(n))} ∼Z, parameter ϵc, and constants ¯α0 and ¯α1 such that α0 < ¯α0
and α1 < ¯α1, it holds for each cell i ∈{1, . . . , k}, with total conﬁdence 1 −ϵc, that
max(α0 · qi(0), α1 · qi(1)) ≤ti,
(3)
where ti = max

¯α0B( ϵc"
IMPLEMENTATION/METHODS,0.23529411764705882,"2k; mi, ni −mi + 1), ¯α1(1 −B(1 −ϵc"
IMPLEMENTATION/METHODS,0.23839009287925697,"2k; mi + 1, ni −mi))

. In this ex-
pression, ni = |Zi|, and mi = P"
IMPLEMENTATION/METHODS,0.24148606811145512,"j∈Zi 1{s(j) = 0}, where we denote Zi = |{j|z(j) = zi}|."
IMPLEMENTATION/METHODS,0.24458204334365324,"Proof. As in Lemma 1, for each cell we use the Clopper-Pearson CI to estimate qi(0) with samples
indexed by Zi and conﬁdence 1 −ϵc/k. As before, we apply qi(0) = 1 −qi(1) to arrive at a set of
k inequalities of the form Eq. (3), which per union bound jointly hold with conﬁdence 1 −ϵc."
IMPLEMENTATION/METHODS,0.2476780185758514,"Lemma 3 (Bounding the sum). Given n independent samples {(z(1), s(1)), . . . , (z(n), s(n))} ∼Z,
where for each j ∈{1, . . . , n} we deﬁne a function idx(z(j)) = i such that z(j) = zi (cell index),
parameter ϵs, and a set of real-valued constants {t1, . . . , tk}, it holds that P k
X"
IMPLEMENTATION/METHODS,0.25077399380804954,"i=1
p(zi)ti ≤S !"
IMPLEMENTATION/METHODS,0.25386996904024767,"≥1 −ϵs, where S = 1 n n
X"
IMPLEMENTATION/METHODS,0.25696594427244585,"j=1
tidx(z(j)) + (b −a) r"
IMPLEMENTATION/METHODS,0.26006191950464397,−log ϵs
IMPLEMENTATION/METHODS,0.2631578947368421,"2n
,
(4)"
IMPLEMENTATION/METHODS,0.26625386996904027,"and we denote a = min{t1, . . . , tk} and b = max{t1, . . . , tk}."
IMPLEMENTATION/METHODS,0.2693498452012384,"Proof. For each j let X(j) := tidx(z(j)) denote a random variable. As for all j, X(j) ∈[a, b] with
probability 1 and X(j) are independent, we can apply Hoeffding’s inequality (Hoeffding, 1963) (re-
stated in Appendix A) to upper-bound the difference between the population mean Pk
i=1 p(zi)ti =
Ez∼Ztidx(z) and its empirical estimate 1"
IMPLEMENTATION/METHODS,0.2724458204334365,"n
Pn
j=1 X(j). Setting the upper bound such that the error is
below ϵs directly recovers S and the statement of the lemma."
IMPLEMENTATION/METHODS,0.2755417956656347,"Applying the lemmas
Finally, we describe how we apply the lemmas in practice to upper-bound
BAZ0,Z1(h⋆), and in turn upper-bound ∆DP
Z0,Z1(g) for any downstream classiﬁer g trained on repre-
sentations learned by a restricted encoder. We assume a standard setting, where a set D of datapoints
{(x(j), s(j))} from X is split into a training set Dtrain, used to train f, validation set Dval, held-out
for the upper-bounding procedure (and not used in training of f in any capacity), and a test set Dtest,
used to evaluate the empirical accuracy and fairness of downstream classiﬁers."
IMPLEMENTATION/METHODS,0.2786377708978328,"After training the encoder and applying it to produce representations (z(j), s(j)) ∼Z for all three
data subsets, we aim to derive an upper bound on ∆DP
Z0,Z1(g) for any g, that holds with conﬁdence
at least 1 −ϵ, where ϵ is the hyperparameter of the procedure (we use ϵ = 0.05). To this end, we
heuristically choose some decomposition ϵ = ϵb + ϵc + ϵs, and apply Lemma 1 on Dtrain to obtain
upper bounds α0 < ¯α0 and α1 < ¯α1 with error probability ϵb. As mentioned above, using Dtrain in
this step is sound as estimated probabilities q(0) and q(1) are independent of the encoder f. Next,
we use ¯α0, ¯α1 and Dval in Lemma 2, to obtain upper bounds t1, . . . , tk on per-cell accuracy that
jointly hold with error probability ϵc. Finally, we upper-bound the sum Pk
i=1 p(zi)ti ≤S with error
probability ϵs using Lemma 3 on Dtest with previously computed t1, . . . , tk. Combining this with
Eq. (2) ﬁnally gives the desired upper bound
∆DP
Z0,Z1(g) ≤2 · BAZ0,Z1(h⋆) −1 ≤2S −1 = T,
(5)
which per union bound holds with desired error probability ϵ, with respect to the sampling process."
IMPLEMENTATION/METHODS,0.28173374613003094,"This completes the upper-bounding procedure, enabling provable fair representation learning with
no restrictive assumptions, which addresses a major limitation of prior work. Our procedure can
be applied to representations produced by any restricted encoder. In the following sections, we
will describe a particular instantiation based on decision trees, and experimentally demonstrate that
applying the above procedure produces tight unfairness upper bounds on real datasets."
IMPLEMENTATION/METHODS,0.2848297213622291,Under review as a conference paper at ICLR 2023
IMPLEMENTATION/METHODS,0.28792569659442724,"5
RESTRICTED REPRESENTATIONS WITH FAIR DECISION TREES"
IMPLEMENTATION/METHODS,0.29102167182662536,"Next, we describe a practical restricted encoder, enabling a favorable accuracy-fairness tradeoff
(similar to prior work), while allowing application of our results from Section 4 to provably bound
the fairness of downstream classiﬁers trained on the representations (unique to our work)."
IMPLEMENTATION/METHODS,0.29411764705882354,"Our encoder is based on decision trees, a choice motivated by strong results of tree-based models on
tabular data (Borisov et al., 2021), as well as their feature space splitting procedure, whose discrete
behavior is inherently suitable for our requirement of restricted representations. In particular, we
train a classiﬁcation tree f with k leaves, and encode all examples that end up in leaf i to the same
representation zi. We construct zi based on the set of training examples in leaf i, taking the median
value for continuous, and the most common value for categorical features (thus in our case, d′ = d)."
IMPLEMENTATION/METHODS,0.29721362229102166,"Fairness-aware criterion
Common splitting criteria are aimed at maximizing accuracy by making
the distribution of y in each leaf highly unbalanced, e.g., Giniy(D) = 2py(1 −py) ∈[0, 0.5] where
py = P"
IMPLEMENTATION/METHODS,0.30030959752321984,"(x,y)∈D 1{y = 1}/|D|. Aiming to use such a tree as an encoder generally leads to high
unfairness, making it necessary to introduce a direct way to prioritize more fair tree structures."
IMPLEMENTATION/METHODS,0.30340557275541796,"To this end, similar to Kamiran et al. (2010) and others (see discussion of related work in Section 2),
we use the criterion FairGini(D) = (1 −γ)Giniy(D) + γ(0.5 −Ginis(D)) ∈[0, 0.5], where
Ginis is deﬁned analogously to Giniy. The second term aims to maximize Ginis(D), i.e., make the
distribution of s in each leaf i as close to uniform (making it challenging for the adversary to infer
the value of s based on zi), while the hyperparameter γ controls the accuracy-fairness tradeoff."
IMPLEMENTATION/METHODS,0.3065015479876161,"Fairness-aware categorical splits
While usual splits of the form xj ≤v (see Section 3) are suit-
able for continuous, they are inefﬁcient for categorical (usually one-hot) variables, as only 1 category
can be isolated. Consequently, this increases the number of cells and makes our fairness bounds
loose. Instead, we represent nj categories for feature j as integers c ∈{1, 2, ..., nj}. To avoid
evaluating all 2nj −1 possible partitions, we sort the values by py(c) = P"
IMPLEMENTATION/METHODS,0.30959752321981426,"(x,y)∈Dc 1{y = 1}/|Dc|
where Dc = {x ∈D | xj = c}, and consider all preﬁx-sufﬁx partitions (Breiman shortcut)."
IMPLEMENTATION/METHODS,0.3126934984520124,"This ordering focuses on accuracy and is provably optimal for FairGini(D) with γ = 0 (Breiman
et al., 1984). However, as it ignores fairness, it is inefﬁcient for γ > 0. To alleviate this, we gener-
alize the Breiman shortcut, and explore all preﬁx-sufﬁx partitions under several orderings. Namely,
for several values of the parameter q, we split the set of categories {1, 2, . . . , nj} in q-quantiles with
respect to ps(c) (deﬁned analogous to py(c)), and sort each quantile by py(c) as before, interspersing
q obtained arrays to obtain the ﬁnal ordering. Note that while this offers no optimality guarantees, it
is an efﬁcient way to consider both objectives, complementing our fairness-aware criterion."
IMPLEMENTATION/METHODS,0.3157894736842105,We defer the discussion of the hyperparameters of our encoder to Appendix B.
RESULTS/EXPERIMENTS,0.3188854489164087,"6
EXPERIMENTAL EVALUATION"
RESULTS/EXPERIMENTS,0.3219814241486068,"We evaluate our method, FARE, on several common fairness datasets, demonstrating that it pro-
duces representations with fairness-accuracy tradeoffs comparable to prior work, while for the ﬁrst
time offering provable fairness bounds. We further provide more insights into FARE, discuss the
interpretability of the representations, and provide additional experiments on transfer learning."
RESULTS/EXPERIMENTS,0.32507739938080493,"Experimental setup
We consider common fairness datasets: Health (Kaggle, 2012) and two vari-
ants of ACSIncome (Ding et al., 2021), ACSIncome-CA (only California), and ACSIncome-US
(US-wide, larger but more difﬁcult due to distribution shift). The sensitive attributes are age and
sex, respectively. We compare our method with the following recent FRL baselines (described
in Section 2): LAFTR (Madras et al., 2018), CVIB (Moyer et al., 2018), FCRL (Gupta et al., 2021),
FNF (Balunovi´c et al., 2022), sIPM-LFR (Kim et al., 2022), and FairPath (Shui et al., 2022). We
provide all omitted details regarding datasets, baselines, and our experimental setup, in Appendix B."
RESULTS/EXPERIMENTS,0.3281733746130031,"Main experiments
We explore the fairness-accuracy tradeoff of each method by running it with
various hyperparameters. Each run produces representations, used to train a 1-hidden-layer neural"
RESULTS/EXPERIMENTS,0.33126934984520123,Under review as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.33436532507739936,"0.65
0.70
0.75
0.80
Accuracy 0.00 0.05 0.10 0.15 0.20"
RESULTS/EXPERIMENTS,0.33746130030959753,Demographic Parity Distance
RESULTS/EXPERIMENTS,0.34055727554179566,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FARE (Upper Bound)
FARE (Empirical)"
RESULTS/EXPERIMENTS,0.34365325077399383,"0.70
0.73
0.76
0.79
0.82
Accuracy 0.00 0.05 0.10 0.15 0.20"
RESULTS/EXPERIMENTS,0.34674922600619196,Demographic Parity Distance
RESULTS/EXPERIMENTS,0.3498452012383901,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FARE (Upper Bound)
FARE (Empirical)"
RESULTS/EXPERIMENTS,0.35294117647058826,Figure 3: Evaluation of FRL methods on ACSIncome-CA (left) and ACSIncome-US (right).
RESULTS/EXPERIMENTS,0.3560371517027864,"0.74
0.76
0.78
0.80
0.82
Accuracy 0.0 0.1 0.2 0.3 0.4 0.5"
RESULTS/EXPERIMENTS,0.3591331269349845,Demographic Parity Distance
RESULTS/EXPERIMENTS,0.3622291021671827,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FARE (Upper Bound)
FARE (Empirical)"
RESULTS/EXPERIMENTS,0.3653250773993808,Figure 4: Evaluation on Health.
RESULTS/EXPERIMENTS,0.3684210526315789,"network (1-NN) for the prediction task using a standard
training procedure (same for each method), and plot its de-
mographic parity (DP) distance and prediction accuracy."
RESULTS/EXPERIMENTS,0.3715170278637771,"Following Kim et al. (2022), we show a test set Pareto front
for each method. Further, for FARE we independently show
a Pareto front of a 95% conﬁdence provable upper bound
on DP distance (following Section 4), which is a key fea-
ture of our approach and cannot be produced by any other
method. Finally, we include an Unfair Baseline, which uses
an identity encoder. The results are shown in Figs. 3 and 4.
We omit FairPath and LAFTR from the main plots (see ex-
tended results in Appendix C), as LAFTR has stability and
convergence issues (Gupta et al., 2021; Kim et al., 2022),
and FairPath uses a different metric to us (Shui et al., 2022)."
RESULTS/EXPERIMENTS,0.3746130030959752,"We observe that FARE can achieve a better (Fig. 3) or comparable (Fig. 4) accuracy-fairness tradeoff
compared to baselines. Crucially, other methods cannot guarantee that there is no classiﬁer with a
worse DP distance when trained on their representations. This cannot happen for our method—we
produce a provable upper bound on DP distance of any classiﬁer trained on our representations. The
results indicate that our provable upper bound is often comparable to empirical values of baselines.
Note that there is a small gap (up to 1.5%) between the maximum accuracy of FARE and the unfair
baseline, indicating a tradeoff of restricted encoders—FARE obtains provable bounds, but loses
some information, limiting the predictive power of downstream classiﬁers. This is rarely an issue
in practice, as obtaining meaningful fairness improvements generally requires a non-trivial accuracy
loss, especially when s is correlated with the task label. Finally, another advantage of FARE is its
efﬁciency, with runtime of several seconds, as opposed to minutes or hours for baselines."
RESULTS/EXPERIMENTS,0.37770897832817335,"Exploring downstream classiﬁers
In Fig. 5 (left), we show a representative point from Fig. 4, its
fairness guarantee, and 24 diverse downstream classiﬁers (see Appendix B) trained on same repre-
sentations, where half are trained to maximize accuracy, and half to maximize unfairness. The latter
(left cluster) can reach higher unfairness than initially suggested, reafﬁrming a known limitation of
prior work (Xu et al., 2020; Gupta et al., 2021): evaluating representations with some model class
(here, a 1-NN) does not reliably estimate unfairness, as other classiﬁers (perhaps intentionally cre-
ated by a malicious actor) might be more unfair. This highlights the value of FARE which provides
a provable unfairness upper bound—all unfairness values still remain below a known upper bound."
RESULTS/EXPERIMENTS,0.38080495356037153,"Similarly, we explore a point from Fig. 3 (right), with accuracy 75.1% and DP distance of 0.005.
As here k = 6, i.e., the possible representations are {z1, . . . , z6}, the previous investigation of
downstream classiﬁers simpliﬁes. Instead of choosing a model class, we can enumerate all 26 = 64
possible classiﬁers, and directly conﬁrm that each DP distance is below the upper bound, as shown
in Fig. 5 (middle). Note that in the original experiment, all baseline methods have DP distance
≥0.04 at similar accuracy of ≈75%, implying that the FARE bound is in this case very tight."
RESULTS/EXPERIMENTS,0.38390092879256965,Under review as a conference paper at ICLR 2023
RESULTS/EXPERIMENTS,0.38699690402476783,"0.25
0.50
0.75
Accuracy 0.25 0.30 0.35 0.40"
RESULTS/EXPERIMENTS,0.39009287925696595,Demographic Parity Distance
RESULTS/EXPERIMENTS,0.3931888544891641,"FARE (Upper Bound)
Downstream Classiﬁers"
RESULTS/EXPERIMENTS,0.39628482972136225,"0.25
0.50
0.75
Accuracy 0.00 0.01 0.02"
RESULTS/EXPERIMENTS,0.3993808049535604,Demographic Parity Distance
RESULTS/EXPERIMENTS,0.4024767801857585,"FARE (Upper Bound)
Downstream Classiﬁers"
RESULTS/EXPERIMENTS,0.4055727554179567,"0.70
0.73
0.76
0.79
0.82
Accuracy 0.00 0.05 0.10 0.15 0.20"
RESULTS/EXPERIMENTS,0.4086687306501548,Demographic Parity Distance
RESULTS/EXPERIMENTS,0.4117647058823529,"FARE (Upper Bound)
Upper Bound with M=2
Upper Bound with M=4
Upper Bound with M=8
Upper Bound with M=16
Upper Bound with M=32
FARE (Empirical)"
RESULTS/EXPERIMENTS,0.4148606811145511,"Figure 5: Comparing trained downstream classiﬁers with the FARE upper bound (left and middle).
The impact of increasing the dataset size M times on the fairness upper bound tightness (right)."
RESULTS/EXPERIMENTS,0.4179566563467492,"Data improves bounds
In Fig. 3 we see that the FARE bounds are tighter for ACSIncome-US,
as using more samples improves the bounding procedure. To investigate this further, we choose a
representative set of FARE points from Fig. 3 (left), and repeat the upper-bounding procedure with
the dataset repeated M times, showing the resulting upper bounds for M ∈{2, 4, 8, 16, 32} in Fig. 5
(right). We can observe a signiﬁcant improvement in the provable upper bound for larger dataset
sizes, indicating that FARE is well-suited for large datasets and will beneﬁt from ever-increasing
amounts of data used in real-world machine learning deployments (Villalobos & Ho, 2022)."
RESULTS/EXPERIMENTS,0.42105263157894735,"Interpretability
Another advantage of FARE is that its tree-based encoder enables direct inter-
pretation of representations. To illustrate this, for representations with k = 6 analyzed in Fig. 5
(middle) we can easily ﬁnd that, for example, the representation z6 is assigned to each person older
that 24, with at least a Bachelor’s degree, and an occupation in management, business or science."
RESULTS/EXPERIMENTS,0.4241486068111455,"y
∆DP
Z0,Z1
T
FARE
FCRL
FNF
sIPM"
RESULTS/EXPERIMENTS,0.42724458204334365,"MIS
≤0.20
0.64
79.3
78.6
78.9
79.8
≤0.05
0.54
78.7
78.6
78.7
78.6"
RESULTS/EXPERIMENTS,0.43034055727554177,"NEU
≤0.20
0.64
73.2
72.4
71.9
76.6
≤0.05
0.42
72.1
71.4
71.7
/"
RESULTS/EXPERIMENTS,0.43343653250773995,"ART
≤0.20
0.41
74.4
70.7
68.9
78.3
≤0.05
0.23
69.5
69.5
68.5
/"
RESULTS/EXPERIMENTS,0.43653250773993807,"MET
≤0.20
0.46
69.8
69.2
75.0
/
≤0.05
0.12
66.1
65.3
/
/"
RESULTS/EXPERIMENTS,0.43962848297213625,"MSC
≤0.20
0.53
67.2
70.5
73.0
/
≤0.05
0.12
63.0
/
/
/"
RESULTS/EXPERIMENTS,0.44272445820433437,Table 1: Transfer learning on Health.
RESULTS/EXPERIMENTS,0.4458204334365325,"Transfer learning
Finally, we analyze the transfer-
ability of learned representations across tasks. We pro-
duce a diverse set of representations on the Health
dataset with each method, and following the procedure
from prior work (Madras et al., 2018; Balunovi´c et al.,
2022; Kim et al., 2022) evaluate them on ﬁve unseen
tasks y (see Appendix B for details), where for each
the goal is to predict a certain primary condition group.
For each task and each method, we identify the high-
est accuracy obtained while keeping ∆DP
Z0,Z1 not above
0.20 (or 0.05). Moreover, we show T, the provable DP
distance upper bound of FARE."
RESULTS/EXPERIMENTS,0.44891640866873067,"The results are shown in Table 1. First, we observe that some methods are unable to reduce ∆DP
Z0,Z1
below the given threshold. Our method can always reduce the ∆DP
Z0,Z1 sufﬁciently, but due to our
restriction on representations which enables provable upper bounds, we often lose more accuracy
than other methods for high ∆DP
Z0,Z1 thresholds. Future work could focus on investigating alternative
restricted encoders with better fairness-accuracy tradeoffs in the transfer learning setting."
CONCLUSION/DISCUSSION ,0.4520123839009288,"7
CONCLUSION"
CONCLUSION/DISCUSSION ,0.4551083591331269,"We introduced FARE, a method for provably fair representation learning. The key idea was that
using restricted encoders enables a practical statistical procedure for computing a provable upper
bound on unfairness of downstream classiﬁers trained on these representations. We instantiated this
idea with a tree-based encoder, and experimentally demonstrated that FARE can for the ﬁrst time
obtain tight fairness bounds on several datasets, while simultaneously producing empirical fairness-
accuracy tradeoffs similar to prior work which offers no guarantees."
CONCLUSION/DISCUSSION ,0.4582043343653251,Under review as a conference paper at ICLR 2023
CONCLUSION/DISCUSSION ,0.4613003095975232,REPRODUCIBILITY STATEMENT
CONCLUSION/DISCUSSION ,0.46439628482972134,"To foster reproducibility, all of our code, datasets and scripts are provided in the Openreview dis-
cussion. All of our experiments presented in Section 6 can be run using the instructions we provide
in the Readme ﬁle that accompanies our code. The hyperparameters used for our runs are described
in Appendix B and further detailed in the Readme."
REFERENCES,0.4674922600619195,REFERENCES
REFERENCES,0.47058823529411764,"Seyum Assefa Abebe, Claudio Lucchese, and Salvatore Orlando.
Eifffel: enforcing fairness in
forests by ﬂipping leaves. In SAC, 2022."
REFERENCES,0.47368421052631576,"Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair decision trees
for non-discriminative decision-making. In AAAI, 2019."
REFERENCES,0.47678018575851394,"Mislav Balunovi´c, Anian Ruoss, and Martin T. Vechev. Fair normalizing ﬂows. In ICLR, 2022."
REFERENCES,0.47987616099071206,"Ant´onio Pereira Barata and Cor J. Veenman. Fair tree learning. CoRR, 2021."
REFERENCES,0.48297213622291024,"Solon Barocas and Andrew D. Selbst. Big data’s disparate impact. California Law Review, 2016."
REFERENCES,0.48606811145510836,"Nathan Benaich and Ian Hogarth. State of ai report 2021. 2021. https://www.stateof.ai/
2021, accessed: 2022-09-26."
REFERENCES,0.4891640866873065,"Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji
Kasneci. Deep neural networks and tabular data: A survey. arXiv, 2021."
REFERENCES,0.49226006191950467,"Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression Trees.
Wadsworth, 1984."
REFERENCES,0.4953560371517028,"Tim Brennan, William Dieterich, and Beate Ehret. Evaluating the predictive validity of the compas
risk and needs assessment system. Criminal Justice and Behavior, 2009."
REFERENCES,0.4984520123839009,"Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commer-
cial gender classiﬁcation. In FAccT, 2018."
REFERENCES,0.5015479876160991,"Fl´avio P. Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
Kush R. Varshney. Optimized pre-processing for discrimination prevention. In NeurIPS, 2017."
REFERENCES,0.5046439628482973,"L. Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K. Vishnoi. Classiﬁcation with fair-
ness constraints: A meta-algorithm with provable guarantees. In FAT, 2019."
REFERENCES,0.5077399380804953,"Mattia Cerrato, Marius K¨oppel, Alexander Segner, and Stefan Kramer. Fair group-shared represen-
tations with normalizing ﬂows. CoRR, 2022."
REFERENCES,0.5108359133126935,"Michael Chui,
Bryce Hall,
Alex Singla,
and Alex Sukharevsky.
The state of ai in
2021.
2021.
https://www.mckinsey.com/capabilities/quantumblack/
our-insights/global-survey-the-state-of-ai-in-2021, accessed: 2022-09-
26."
REFERENCES,0.5139318885448917,"C. J. Clopper and E. S. Pearson. The use of conﬁdence or ﬁducial limits illustrated in the case of the
binomial. Biometrika, 1934."
REFERENCES,0.5170278637770898,"Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision
making and the cost of fairness. In ACM SIGKDD, 2017."
REFERENCES,0.5201238390092879,"Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair
machine learning. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.5232198142414861,"Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil. Em-
pirical risk minimization under fairness constraints. In NeurIPS, pp. 2796–2806, 2018."
REFERENCES,0.5263157894736842,"Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness
through awareness. In ITCS, 2012."
REFERENCES,0.5294117647058824,Under review as a conference paper at ICLR 2023
REFERENCES,0.5325077399380805,"Harrison Edwards and Amos J. Storkey. Censoring representations with an adversary. In ICLR,
2016."
REFERENCES,0.5356037151702786,"Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In
EMNLP, 2018."
REFERENCES,0.5386996904024768,"EU. Proposal for a regulation laying down harmonised rules on artiﬁcial intelligence, 2021."
REFERENCES,0.541795665634675,"Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In KDD, 2015."
REFERENCES,0.544891640866873,"Rui Feng, Yang Yang, Yuehan Lyu, Chenhao Tan, Yizhou Sun, and Chunping Wang. Learning fair
representations via an adversarial framework. CoRR, 2019."
REFERENCES,0.5479876160990712,"FTC.
Aiming for truth,
fairness,
and equity in your companys use of ai,
2021.
https://www.ftc.gov/news-events/blogs/business-blog/2021/04/
aiming-truth-fairness-equity-your-companys-use-ai, accessed: 2022-09-
26."
REFERENCES,0.5510835913312694,"Xavier Gitiaux and Huzefa Rangwala. Sofair: Single shot fair representation learning. In IJCAI,
2022."
REFERENCES,0.5541795665634675,"Vincent Grari, Boris Ruf, Sylvain Lamprier, and Marcin Detyniecki. Achieving fairness with deci-
sion trees: An adversarial approach. Data Sci. Eng., 2020."
REFERENCES,0.5572755417956656,"Steffen Gr¨unew¨alder and Azadeh Khaleghi. Oblivious data for fairness with kernels. J. Mach. Learn.
Res., 22:208:1–208:36, 2021."
REFERENCES,0.5603715170278638,"Umang Gupta, Aaron M. Ferber, Bistra Dilkina, and Greg Ver Steeg. Controllable guarantees for
fair outcomes via contrastive information estimation. In AAAI, 2021."
REFERENCES,0.5634674922600619,"Moritz Hardt, Eric Price, and Nati Srebro.
Equality of opportunity in supervised learning.
In
NeurIPS, 2016."
REFERENCES,0.56656346749226,"Wassily Hoeffding. Probability inequalities for sums of bounded random variables. JSTOR, (301),
1963."
REFERENCES,0.5696594427244582,"Ayush Jaiswal, Daniel Moyer, Greg Ver Steeg, Wael AbdAlmageed, and Premkumar Natarajan.
Invariant representations through adversarial forgetting. In AAAI, 2020."
REFERENCES,0.5727554179566563,"Jiayin Jin, Zeru Zhang, Yang Zhou, and Lingfei Wu. Input-agnostic certiﬁed group fairness via
gaussian parameter smoothing. In ICML, 2022."
REFERENCES,0.5758513931888545,"Kaggle. Health heritage prize, 2012. URL https://www.kaggle.com/c/hhp."
REFERENCES,0.5789473684210527,"Peter Kairouz, Jiachun Liao, Chong Huang, Maunil Vyas, Monica Welfert, and Lalitha Sankar.
Generating fair universal representations using adversarial models. IEEE TIFS, 2022."
REFERENCES,0.5820433436532507,"Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree learn-
ing. In ICDM, 2010."
REFERENCES,0.5851393188854489,"Mintong Kang, Linyi Li, Maurice Weber, Yang Liu, Ce Zhang, and Bo Li. Certifying some distri-
butional fairness with subpopulation decomposition. CoRR, 2022."
REFERENCES,0.5882352941176471,"Amir E Khandani, Adlar J Kim, and Andrew W Lo. Consumer credit-risk models via machine-
learning algorithms. Journal of Banking & Finance, 2010."
REFERENCES,0.5913312693498453,"Dongha Kim, Kunwoong Kim, Insung Kong, Ilsang Ohn, and Yongdai Kim. Learning fair repre-
sentation with a parametric integral probability metric. In ICML, 2022."
REFERENCES,0.5944272445820433,"Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determi-
nation of risk scores. In ITCS, 2017."
REFERENCES,0.5975232198142415,"Nikola Konstantinov and Christoph H. Lampert. Fairness through regularization for learning to rank.
CoRR, 2021."
REFERENCES,0.6006191950464397,Under review as a conference paper at ICLR 2023
REFERENCES,0.6037151702786377,"Preethi Lahoti, Krishna P. Gummadi, and Gerhard Weikum. ifair: Learning individually fair data
representations for algorithmic decision making. In ICDE, 2019."
REFERENCES,0.6068111455108359,"Ji Liu, Zenan Li, Yuan Yao, Feng Xu, Xiaoxing Ma, Miao Xu, and Hanghang Tong. Fair represen-
tation learning: An alternative to mutual information. In KDD, 2022."
REFERENCES,0.6099071207430341,"Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S. Zemel. The variational
fair autoencoder. In ICLR, 2016."
REFERENCES,0.6130030959752322,"David Madras, Elliot Creager, Toniann Pitassi, and Richard S. Zemel. Learning adversarially fair
and transferable representations. In ICML, 2018."
REFERENCES,0.6160990712074303,"Daniel McNamara, Cheng Soon Ong, and Robert C. Williamson. Provably fair representations.
CoRR, abs/1710.04394, 2017."
REFERENCES,0.6191950464396285,"Anna P. Meyer, Aws Albarghouthi, and Loris D’Antoni. Certifying robustness to programmable
data bias in decision trees. In NeurIPS, 2021."
REFERENCES,0.6222910216718266,"Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg.
Invariant
representations without adversarial training. In NeurIPS, 2018."
REFERENCES,0.6253869969040248,"Changdae Oh, Heeji Won, Junhyuk So, Taero Kim, Yewon Kim, Hosik Choi, and Kyungwoo Song.
Learning fair representation via distributional contrastive disentanglement. In KDD, 2022."
REFERENCES,0.628482972136223,"Luca Oneto, Michele Donini, Massimiliano Pontil, and Andreas Maurer. Learning fair and transfer-
able representations with theoretical guarantees. In DSAA, 2020."
REFERENCES,0.631578947368421,"Felix Petersen, Debarghya Mukherjee, Yuekai Sun, and Mikhail Yurochkin. Post-processing for
individual fairness. In NeurIPS, 2021."
REFERENCES,0.6346749226006192,"Momchil Peychev, Anian Ruoss, Mislav Balunovic, Maximilian Baader, and Martin T. Vechev.
Latent space smoothing for individually fair representations. CoRR, 2021."
REFERENCES,0.6377708978328174,"Edward Raff, Jared Sylvester, and Steven Mills. Fair forests: Regularized tree induction to minimize
model bias. In AIES, 2018."
REFERENCES,0.6408668730650154,"Francesco Ranzato, Caterina Urban, and Marco Zanella. Fair training of decision tree classiﬁers.
CoRR, 2021."
REFERENCES,0.6439628482972136,"Proteek Chandan Roy and Vishnu Naresh Boddeti. Mitigating information leakage in image repre-
sentations: A maximum entropy approach. In CVPR, 2019."
REFERENCES,0.6470588235294118,"Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin T. Vechev. Learning certiﬁed individually
fair representations. In NeurIPS, 2020."
REFERENCES,0.6501547987616099,"Mhd Hasan Sarhan, Nassir Navab, Abouzar Eslami, and Shadi Albarqouni. Fairness by learning
orthogonal disentangled representations. In ECCV, 2020."
REFERENCES,0.653250773993808,"Xudong Shen, Yongkang Wong, and Mohan S. Kankanhalli. Fair representation: Guaranteeing
approximate multiple group fairness for unknown tasks. CoRR, abs/2109.00545, 2021."
REFERENCES,0.6563467492260062,"Changjian Shui, Qi Chen, Jiaqi Li, Boyu Wang, and Christian Gagn´e. Fair representation learning
through implicit path alignment. In ICML, 2022."
REFERENCES,0.6594427244582043,"Congzheng Song and Vitaly Shmatikov. Overlearning reveals sensitive attributes. In ICLR, 2020."
REFERENCES,0.6625386996904025,"Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning
controllable fair representations. In AISTATS, 2019."
REFERENCES,0.6656346749226006,"Zilong Tan, Samuel Yeom, Matt Fredrikson, and Ameet Talwalkar. Learning fair representations
for kernel models. In AISTATS, volume 108 of Proceedings of Machine Learning Research, pp.
155–166. PMLR, 2020."
REFERENCES,0.6687306501547987,"Rachael Tatman and Conner Kasten. Effects of talker dialect, gender & race on accuracy of bing
speech and youtube automatic captions. In INTERSPEECH, 2017."
REFERENCES,0.6718266253869969,Under review as a conference paper at ICLR 2023
REFERENCES,0.6749226006191951,"Pablo Villalobos and Anson Ho. Trends in training dataset sizes. https://epochai.org/
blog/trends-in-training-dataset-sizes, 2022. Accessed: 2022-09-28."
REFERENCES,0.6780185758513931,"Jingbo Wang, Yannan Li, and Chao Wang. Synthesizing fair decision trees via iterative constraint
solving. In CAV, 2022."
REFERENCES,0.6811145510835913,"Qizhe Xie, Zihang Dai, Yulun Du, Eduard H. Hovy, and Graham Neubig. Controllable invariance
through adversarial feature learning. In NIPS, pp. 585–596, 2017."
REFERENCES,0.6842105263157895,"Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable
information under computational constraints. In ICLR, 2020."
REFERENCES,0.6873065015479877,"Richard S. Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. Learning fair
representations. In ICML, 2013."
REFERENCES,0.6904024767801857,"Wenbin Zhang and Eirini Ntoutsi. FAHT: an adaptive fairness-aware decision tree classiﬁer. In
IJCAI, 2019."
REFERENCES,0.6934984520123839,"Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J. Gordon. Conditional learning of fair
representations. In ICLR. OpenReview.net, 2020."
OTHER,0.6965944272445821,Under review as a conference paper at ICLR 2023
OTHER,0.6996904024767802,"Dataset
Training size
Test size
Base rate (s)
Base rate (y)"
OTHER,0.7027863777089783,"ACSIncome-CA
165 546
18 395
0.46
0.64
ACSIncome-US
1 429 070
158 786
0.48
0.68
Health
174 732
43 683
0.35
0.68"
OTHER,0.7058823529411765,Table 2: Statistics of evaluated datasets.
OTHER,0.7089783281733746,"A
MATHEMATICAL TOOLS"
OTHER,0.7120743034055728,"We ﬁrst derive Eq. (2). More details can be found in Madras et al. (2018), and here we provide an
overview:"
OTHER,0.7151702786377709,"∆DP
Z0,Z1(g) = |Ez∼Z0[g(z)] −Ez∼Z1[g(z)]|"
OTHER,0.718266253869969,"= |Ez∼Z0[−g(z)] + Ez∼Z1[g(z)]|
= |Ez∼Z0[1 −g(z)] + Ez∼Z1[g(z)] −1|
= |2BAZ0,Z1(g) −1|"
OTHER,0.7213622291021672,"From this, we can argue that we can drop the absolute value and bound the balanced accuracy of g
with the balanced accuracy of h∗, ﬁnally arriving at Equation 2."
OTHER,0.7244582043343654,"Then, we formally state the Hoeffding’s inequality and the Clopper-Pearson binomial conﬁdence
intervals, used in our upper-bounding procedure in Section 4."
OTHER,0.7275541795665634,"Hoeffding’s inequality (Hoeffding, 1963): Let X(1), . . . , X(n) be independent random variables such
that P(X(j) ∈[a(j), b(j)]) = 1. Let ˆµ = X(1)+...X(n)"
OTHER,0.7306501547987616,"n
and µ = E[ˆµ]. It holds that:"
OTHER,0.7337461300309598,"P(µ −ˆµ ≥t) ≤exp

−2n2t2
Pn
i=1(b(i) −a(i))2 
."
OTHER,0.7368421052631579,"Clopper-Pearson binomial proportion conﬁdence intervals (Clopper & Pearson, 1934): Assume a
binomial distribution with an unknown success probability θ. Given m successes out of n experi-
ments, it holds that: B(α"
OTHER,0.739938080495356,"2 ; m, n −m + 1) < θ < B(1 −α"
OTHER,0.7430340557275542,"2 ; m + 1, n −m)
(6)"
OTHER,0.7461300309597523,"with conﬁdence at least 1−α over the sampling process, where B(p; v, w) denotes the p-th quantile
of a beta distribution with parameters v and w."
OTHER,0.7492260061919505,"B
DETAILS OF EXPERIMENTAL EVALUATION"
OTHER,0.7523219814241486,In this section we provide details of our experimental evaluation omitted from the main text.
OTHER,0.7554179566563467,"Datasets
As mentioned in Section 6, we perform our experiments on ACSIncome (Ding et al.,
2021) and Health (Kaggle, 2012) datasets. In Table 2 we show some general statistics about the
datasets: size of the training and test set, base rate for the sensitive attribute s (percentage of the
majority group out of the total population), and base rate for the label y (accuracy of the majority
class predictor)."
OTHER,0.7585139318885449,"ACSIncome is a dataset recently proposed by Ding et al. (2021) as an improved version of UCI
Adult, with comprehensive data from US Census collected across all states and several years (we
use 2014). The task is to predict whether an individual’s income is above $50,000, and we consider
sex as a sensitive attribute. We evaluate our method on two variants of the dataset: ACSIncome-CA,
which contains only data from California, and ACSIncome-US, which merges data from all states
and is thus signiﬁcantly larger but also more difﬁcult, due to distribution shift. 10% of the total
dataset is used as the test set. We also use the Health dataset (Kaggle, 2012), where the goal is
to predict the Charlson Comorbidity Index, and we consider age as a sensitive attribute (binarized
by thresholding at 60 years). For this dataset perform the same preprocessing as Balunovi´c et al.
(2022), and use 20% of the total dataset as the test set."
OTHER,0.7616099071207431,Under review as a conference paper at ICLR 2023
OTHER,0.7647058823529411,"0.65
0.70
0.75
0.80
Accuracy 0.00 0.05 0.10 0.15 0.20"
OTHER,0.7678018575851393,Demographic Parity Distance
OTHER,0.7708978328173375,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FairPath
LAFTR
FARE (Upper Bound)
FARE (Empirical)"
OTHER,0.7739938080495357,"0.70
0.73
0.76
0.79
0.82
Accuracy 0.00 0.05 0.10 0.15 0.20"
OTHER,0.7770897832817337,Demographic Parity Distance
OTHER,0.7801857585139319,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FairPath
LAFTR
FARE (Upper Bound)
FARE (Empirical)"
OTHER,0.7832817337461301,Figure 6: Extended evaluation on ACSIncome-CA (left) and ACSIncome-US (right).
OTHER,0.7863777089783281,"Evaluation procedure
For our main experiments, as a downstream classiﬁer we use a 1-hidden-
layer neural network with hidden layer size 50, trained until convergence on representations normal-
ized such that their mean is approximately 0 and standard deviation approximately 1. We train the
classiﬁer 5 times and in our main ﬁgures report the average test set accuracy, and the maximal DP
distance obtained, following the procedure of Gupta et al. (2021)."
OTHER,0.7894736842105263,"Hyperparameters
For baselines, we follow the instructions in respective writeups, as well as
Gupta et al. (2021) to densely explore an appropriate parameter range for each value (linearly, or
exponentially where appropriate), aiming to obtain different points on the accuracy-fairness curve.
For CVIB, we explore λ ∈[0.01, 1] and β ∈[0.001, 0.1]. For FCRL on ACSIncome we explore
λ = β ∈[0.01, 2], and for Health λ ∈[0.01, 2] and β = 0.5λ. For FNF, we explore γ ∈[0, 1].
For sIPM-LFR, we use λ ∈[0.0001, 1.0] and λF ∈[0.0001, 100.0], extending the suggested ranges.
For FairPath we set the parameter κ ∈[0, 100]. Finally, for LAFTR we use g ∈[0.1, 50], extending
the range of [0, 4] suggested by (Gupta et al., 2021). We adjust the parameters for transfer learning
whenever supported by the method."
OTHER,0.7925696594427245,"For FARE, there are four hyperparameters: γ (used for the criterion, where larger γ puts more focus
on fairness), ¯k (upper bound for the number of leaves), ni (lower bound for the number of examples
in a leaf), and v (the ratio of the training set to be used as a validation set). Note that all parameters
affect accuracy, empirical fairness, and the tightness of the fairness bound. For example, larger ni is
likely to improve the bound by making Lemma 2 tighter, as more samples can be used for estimation.
For the same reason, increasing v improves the tightness of the bound, but may slightly reduce the
accuracy as fewer samples remain in the training set used to train the tree. In our experiments we
investigate γ ∈[0, 1], k ∈[2, 200], ni ∈[50, 1000], v ∈{0.1, 0.2, 0.3, 0.5}. For the upper-bounding
procedure, we always set ϵ = 0.05, ϵb = ϵs = 0.005, and thus ϵc = 0.04. Finally, when sorting
categorical features as described in Section 5, we use q ∈{1, 2, 4} in all cases."
OTHER,0.7956656346749226,"0.74
0.76
0.78
0.80
0.82
Accuracy 0.0 0.1 0.2 0.3 0.4 0.5"
OTHER,0.7987616099071208,Demographic Parity Distance
OTHER,0.8018575851393189,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FairPath
LAFTR
FARE (Upper Bound)
FARE (Empirical)"
OTHER,0.804953560371517,"Figure 7: Extended evaluation of
FRL methods on Health."
OTHER,0.8080495356037152,"Omitted details of additional experiments
For the exper-
iment with alternative classiﬁers (Fig. 5, left) we explore the
following classiﬁers: (i) 1-hidden-layer neural network (1-
NN) with hidden layer sizes 50 and 200, (ii) 2-NN with hid-
den layers of size (50, 50), as well as (200, 100), (iii) logistic
regression, (iv) random forest classiﬁer with 100 and 1000 es-
timators, (v) decision tree with 100 and an unlimited number
of leaf nodes. We train all these classiﬁers with a standard-
ization preprocessing step as described above. We further
train one variant of 1-NN, 2-NN, random forest, and logis-
tic regression, on unnormalized data. All described models
are trained both to predict the task label y, and to maximize
unfairness, i.e., predict s, leading to 24 evaluated models."
OTHER,0.8111455108359134,Under review as a conference paper at ICLR 2023
OTHER,0.8142414860681114,"y
∆DP
Z0,Z1
T
FARE
FCRL
FNF
sIPM"
OTHER,0.8173374613003096,"MIS
≤0.30
0.64
79.3
78.6
79.2
79.8
≤0.20
0.64
79.3
78.6
78.9
79.8
≤0.15
0.64
79.3
78.6
78.9
79.6
≤0.10
0.48
78.8
78.6
78.9
79.0
≤0.05
0.54
78.7
78.6
78.7
78.6"
OTHER,0.8204334365325078,"NEU
≤0.30
0.64
73.2
72.4
71.9
78.8
≤0.20
0.64
73.2
72.4
71.9
76.6
≤0.15
0.64
73.2
72.4
71.8
73.2
≤0.10
0.64
73.2
72.2
71.8
/
≤0.05
0.42
72.1
71.4
71.7
/"
OTHER,0.8235294117647058,"ART
≤0.30
0.41
74.4
70.7
68.9
78.3
≤0.20
0.41
74.4
70.7
68.9
78.3
≤0.15
0.46
74.2
70.1
68.9
/
≤0.10
0.23
69.5
69.6
68.7
/
≤0.05
0.23
69.5
69.5
68.5
/"
OTHER,0.826625386996904,"MET
≤0.30
0.47
74.0
72.5
76.2
/
≤0.20
0.46
69.8
69.2
75.0
/
≤0.15
0.33
68.7
67.9
73.2
/
≤0.10
0.12
66.1
66.7
73.2
/
≤0.05
0.12
66.1
65.3
/
/"
OTHER,0.8297213622291022,"MSC
≤0.30
0.56
71.3
70.5
73.5
77.6
≤0.20
0.53
67.2
70.5
73.0
/
≤0.15
0.12
63.0
69.7
/
/
≤0.10
0.12
63.0
69.0
/
/
≤0.05
0.12
63.0
/
/
/"
OTHER,0.8328173374613003,Table 3: Extended results of transfer learning experiments on Health.
OTHER,0.8359133126934984,"For transfer learning (Table 1), the ﬁve transfer tasks repre-
sent prediction of the following attributes from the Health
dataset: MISCHRT (MIS), NEUMENT (NEU), ARTH-
SPIN (ART), METAB3 (MET), MSC2A3 (MSC)."
OTHER,0.8390092879256966,"C
EXTENDED RESULTS"
OTHER,0.8421052631578947,"We provide the extended results of our main experiments, including two originally excluded meth-
ods, LAFTR and FairPath in Fig. 6 and Fig. 7, corresponding to Fig. 3 and Fig. 4."
OTHER,0.8452012383900929,"Additionally, in Table 3 we provide extended results of our transfer learning experiments, showing
the accuracy values for thresholds ∆DP
Z0,Z1 ∈{0.30, 0.20, 0.15, 0.10, 0.05}. We can observe similar
trends as shown in Table 1 in the main paper."
OTHER,0.848297213622291,"D
ADDITIONAL EXPERIMENTS"
OTHER,0.8513931888544891,"In this section, we provide additional experimental results showing the effects which imbalance in
sensitive attribute, different downstream classiﬁers, and dataset size have on FARE."
OTHER,0.8544891640866873,"D.1
EFFECT OF SENSITIVE ATTRIBUTE IMBALANCE"
OTHER,0.8575851393188855,"In this section, we demonstrate empirically what effect an imbalance in the sensitive attribute has
on the resulting fairness and accuracy of our proposed method. Let c denote the level of imbalance
of each training set (i.e., the number of data points in the larger of the two sensitive classes divided
by the total number of data points in the set). For each value of c we are interested in, we sample
a random subset of size 49 053 from the original ACSIncome-CA training dataset (out of 165 546
data points in total) and ensuring that the level of imbalance is exactly c. We use 49 053 samples, as
this is the largest number for which we can have same dataset size for each c, thus ensuring the fair
comparison."
OTHER,0.8606811145510835,Under review as a conference paper at ICLR 2023
OTHER,0.8637770897832817,"0.72
0.75
0.78
0.81
Accuracy 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
OTHER,0.8668730650154799,Demographic Parity Distance
OTHER,0.8699690402476781,"FARE c=0.9 (Upper Bound)
FARE c=0.9 (Empirical)
FARE c=0.8 (Upper Bound)
FARE c=0.8 (Empirical)
FARE c=0.7 (Upper Bound)
FARE c=0.7 (Empirical)
FARE c=0.6 (Upper Bound)
FARE c=0.6 (Empirical)
FARE c=0.5 (Upper Bound)
FARE c=0.5 (Empirical)"
OTHER,0.8730650154798761,"Figure 8: Evaluation of FARE at different levels of imbalance in the sensitive attribute (denoted by
c) on randomly sampled subsets of ACSIncome-CA of the same size."
OTHER,0.8761609907120743,"We train FARE on each subset separately and show Pareto plots, similar to those in Fig. 3, in Fig. 8.
We observe that FARE is very robust to the level of imbalance, as even for very high levels, such as
c = 0.9, we only see small difference in our Pareto curves (only at the low-accuracy regime)."
OTHER,0.8792569659442725,"D.2
EXPERIMENTS WITH DIFFERENT DOWNSTREAM CLASSIFIERS"
OTHER,0.8823529411764706,"In this section, we compare different FRL methods on ACSIncome-CA, similarly to Fig. 3 (left),
in the case when different downstream classiﬁers are used. In Fig. 9, we show the results on 4
additional downstream classiﬁers:"
OTHER,0.8854489164086687,"• Decision Tree with maximum 2500 leaves
• Random Forest using 100 trees
• Logistic Regression
• Two-layer neural network with 50 neurons per layer"
OTHER,0.8885448916408669,"We observe that the general trends observed in Fig. 3 (left) for the different FRL methods hold re-
gardless of the downstream classiﬁer choice. We also see that the gap of our method to the maximum
achievable accuracy is the smallest when the downstream classiﬁer is a tree. This is unsurprising
given that FARE’s own representations are based on trees. Further, we see that the more complex
feature extraction of FCRL and sIPM-LFR allow them to gain better accuracy over the unfair base-
line when using a more simple classiﬁer such as a decision tree."
OTHER,0.891640866873065,"D.3
PERFORMANCE GAP ON LARGER DATASETS"
OTHER,0.8947368421052632,"In this section, we explore whether the small performance gap we observed in Fig. 3 between
FARE’s most accurate model and the unfair baseline widens for larger datasets. To this end, we
merge two ACSIncome-US datasets from two consecutive years (2014 and 2015) and compare the
results to the single year dataset from 2014, shown in Fig. 3 (right). We note that the merged
dataset has roughly 2x the number of data points. The comparison between the merged and single-
year datasets is shown in Fig. 10. We observe almost no difference between the results on the two
datasets for the unfair baseline as well as the empirical and provable fairness of our method. This
suggests that the complexity of the dataset is a more important factor than the data volume for the
observed performance gap."
OTHER,0.8978328173374613,Under review as a conference paper at ICLR 2023
OTHER,0.9009287925696594,"0.65
0.70
0.75
0.80
Accuracy 0.00 0.05 0.10 0.15 0.20"
OTHER,0.9040247678018576,Demographic Parity Distance
OTHER,0.9071207430340558,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FARE (Upper Bound)
FARE (Empirical)"
OTHER,0.9102167182662538,(a) Decision Tree
OTHER,0.913312693498452,"0.65
0.70
0.75
0.80
Accuracy 0.00 0.05 0.10 0.15 0.20"
OTHER,0.9164086687306502,Demographic Parity Distance
OTHER,0.9195046439628483,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FARE (Upper Bound)
FARE (Empirical)"
OTHER,0.9226006191950464,(b) Random Forest
OTHER,0.9256965944272446,"0.65
0.70
0.75
0.80
Accuracy 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40"
OTHER,0.9287925696594427,Demographic Parity Distance
OTHER,0.9318885448916409,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FARE (Upper Bound)
FARE (Empirical)"
OTHER,0.934984520123839,(c) Logistic Regression
OTHER,0.9380804953560371,"0.65
0.70
0.75
0.80
Accuracy 0.00 0.05 0.10 0.15 0.20"
OTHER,0.9411764705882353,Demographic Parity Distance
OTHER,0.9442724458204335,"Unfair Baseline
CVIB
FCRL
FNF
sIPM-LFR
FARE (Upper Bound)
FARE (Empirical)"
OTHER,0.9473684210526315,(d) Two-Layer NN
OTHER,0.9504643962848297,"Figure 9: Comparison between different downstream classiﬁers on different FRL methods on
ACSIncome-CA."
OTHER,0.9535603715170279,"E
DIFFERENCES BETWEEN THEORETICALLY-PRINCIPLED FRL AND
PROVABLY FAIR REPRESENTATION LEARNING"
OTHER,0.9566563467492261,"In this section we discuss the differences between theoretically-principled fair representation learn-
ing, which is most of the prior work, and provably fair representation learning, which is our method."
OTHER,0.9597523219814241,"Provably fair representation learning
Here, we ﬁrst restate what kind of provable guarantee we
want to obtain for our learned representations."
OTHER,0.9628482972136223,"We use a ﬁnite set of n datapoints {(x(j), s(j))} to learn representations z(j) = f(x(j), s(j)) such
that the DP distance of any classiﬁer g trained on these representations is bounded by some value
T with conﬁdence 1 −ϵ. We consider practical scenarios, requiring 95% conﬁdence, and n ranging
from 100 000 to roughly 1 000 000."
OTHER,0.9659442724458205,"Note that this is the only type of guarantee that can provide an assurance to a practitioner that
their representations are indeed fair. The practitioner can only beneﬁt from a provable fairness
guarantee that is obtained using a ﬁnite number of samples, and bounds that are obtained under
stronger assumptions such as perfect convergence of the training procedure are less useful."
OTHER,0.9690402476780186,Under review as a conference paper at ICLR 2023
OTHER,0.9721362229102167,"0.74
0.76
0.78
0.80
0.82
Accuracy 0.00 0.05 0.10 0.15 0.20 0.25"
OTHER,0.9752321981424149,Demographic Parity Distance
OTHER,0.978328173374613,"Unfair Baseline ACSIncome 2014
Unfair Baseline ACSIncome 2014+2015
FARE ACSIncome 2014 (Upper Bound)
FARE ACSIncome 2014 (Empirical)
FARE ACSIncome 2014+2015 (Upper Bound)
FARE ACSIncome 2014+2015 (Empirical)"
OTHER,0.9814241486068112,"Figure 10:
Comparison of the performance gap between FARE and the Unfair Baseline on
ACSIncome-US for a single year and two years."
OTHER,0.9845201238390093,"Theoretically-principled FRL
We now discuss theoretically-principled FRL methods from prior
work and, as argued in Section 2, explain why they cannot achieve the guarantee of provable fairness
using a ﬁnite number of samples, described above. These are the methods which use sound theory as
a basis for their representation learning algorithms. There are several works that compute bounds on
DP distance using total variation, mutual information, and other techniques. However, to make the
training practical they replace these bounds with approximations that cannot yield high-probability
certiﬁcates with a ﬁnite number of samples, which is what we call provably fair representation
learning above."
OTHER,0.9876160990712074,"For example, Madras et al. (2018) bound demographic parity using the accuracy of the optimal ad-
versary for predicting a sensitive attribute from the learned representations (which is also closely
connected to the TV distance between the representations). However, even in the case of known
input distribution it is not possible to compute the optimal adversary for the learned representa-
tions when using standard feedforward neural network encoders (this is discussed in more detail
in Balunovi´c et al. (2022)). In this direction, Feng et al. (2019) bound the accuracy of the optimal
Lipschitz continuous adversary (though the general optimal adversary does not have to be Lipschitz
continuous) using Wasserstein distance, which again cannot be provably bounded for ﬁnite number
of samples. Kairouz et al. (2022) formulate the optimal adversary for each type of the reconstruction
loss, but then explain that it is not possible to compute those in practice (except for the restricted case
when input distribution is e.g., a mixture of Gaussians), and they simply use it as a motivation for
min-max optimization in training. Perhaps the most advanced work in this direction is Balunovi´c
et al. (2022) which proposes a new architecture based on normalizing ﬂows that essentially allows
pushing the input distribution through the encoder to obtain the distribution of the latent represen-
tations which can then be used to estimate TV distance. However, in practice the input distribution
is unknown and their certiﬁcate only holds for the estimated input distribution. Zhao et al. (2020)
also show that the TV distance can bound the DP distance, but they provide no ﬁnite sample certiﬁ-
cate. They rely on solving the min-max optimization problem where the objective is approximated
and then approximately optimized using SGD. Thus, it is impossible to know whether the resulting
representations are indeed fair for any downstream classiﬁer."
OTHER,0.9907120743034056,"Another idea is based on the fact that the DP distance can be bounded with an expression involving
mutual information between the representations and sensitive attribute. For example, Gupta et al.
(2021) provide one such bound, but they use it as a motivation for their training algorithm and do
not provide a way to compute this bound on ﬁnite sample dataset. Kim et al. (2022) bound the"
OTHER,0.9938080495356038,Under review as a conference paper at ICLR 2023
OTHER,0.9969040247678018,"DP distance using IPM (integral probability metric), which again cannot be computed with high
probability using ﬁnite sample dataset. Tan et al. (2020) use classical sufﬁcient dimension reduction
framework to construct representations as subspaces of reproducing kernel Hilbert space (RKHS),
and apply this to obtain fair Gaussian processes, but they assume existence of a fair subspace which
might not generally hold. Similarly, Gr¨unew¨alder & Khaleghi (2021) generate features oblivious to
sensitive attributes using Hilbert-space-valued conditional expectation and relaxed optimization of
MMD criterion, which cannot provably guarantee fairness. All of these works fall into the category
of theoretically-principled FRL, and they cannot provide a high-probability certiﬁcate of fairness
using a ﬁnite number of samples."
