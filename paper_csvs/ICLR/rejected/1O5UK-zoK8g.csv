Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003236245954692557,"Out-of-distribution robustness remains a salient weakness of current state-of-the-
art models for semantic segmentation. Until recently, research on generalization
followed a restrictive assumption that the model parameters remain fixed after
the training process. In this work, we empirically study an adaptive inference
strategy for semantic segmentation that adjusts the model to the test sample before
producing the final prediction. We achieve this with two complementary techniques.
Using Instance-adaptive Batch Normalization (IaBN), we modify normalization
layers by combining the feature statistics acquired at training time with those
of the test sample. We next introduce a test-time training (TTT) approach for
semantic segmentation, Seg-TTT, which adapts the model parameters to the test
sample using a self-supervised loss. Relying on a more rigorous evaluation protocol
compared to previous work on generalization in semantic segmentation, our study
shows that these techniques consistently and significantly outperform the baseline
and attain a new state of the art, substantially improving in accuracy over previous
generalization methods."
INTRODUCTION,0.006472491909385114,"1
INTRODUCTION"
INTRODUCTION,0.009708737864077669,"The current state of the art for semantic segmentation (Long et al., 2015; Chen et al., 2018b) lacks
direly in out-of-distribution robustness, i. e. when the training and testing distributions are different.
Numerous studies have investigated this issue, with a primary focus on image classification (Arjovsky
et al., 2019; Bickel et al., 2009; Li et al., 2017a; Torralba and Efros, 2011; Volpi et al., 2018). However,
the conclusion from a recent study of domain generalization methods (Gulrajani and Lopez-Paz,
2020), spanning more than three years of research, is disillusioning: Empirical Risk Minimization
(ERM), which is based on the i. i. d. assumption of the training and testing distributions, is still highly
competitive. This is in stark contrast to the evident advances in the area of domain adaptation, both
for image classification (Ben-David et al., 2010; Ganin et al., 2016; Long et al., 2016; Xie et al., 2018)
and semantic segmentation (Araslanov and Roth, 2021; Yang and Soatto, 2020; Vu et al., 2019). This
setup, however, assumes access to an unlabelled test distribution at training time, whereas in the
generalization setting considered here, only one test sample is accessible at inference time, and no
knowledge between the subsequent test samples must be shared."
INTRODUCTION,0.012944983818770227,"In this work, we study the generalization problem of semantic segmentation from synthetic data
(Richter et al., 2016; Ros et al., 2016) through the lens of adaptation. Instead of modifying the model
architecture (Pan et al., 2018) or the training process (Chen et al., 2020; 2021; Yue et al., 2019),
we enhance the inference procedure with two orthogonal techniques inspired by domain adaptation
methods (Araslanov and Roth, 2021; Li et al., 2017b). The first mechanism leverages normalization
layers (Ioffe and Szegedy, 2015) in modern convolutional neural networks (CNNs) (He et al., 2016)
by integrating feature statistics of the test sample. Expanding upon previously attained conclusions
for the image classification task (Schneider et al., 2020), we find that this strategy not only improves
the segmentation accuracy, but also the calibration quality of the prediction confidence. Our second
contribution is a self-supervised loss, allowing the model to adapt to a single test sample with a few
parameter updates. Thirdly, we set up an extensive empirical study following a rigorous evaluation
protocol, allowing us to establish that the two techniques are complementary. Our study of this
one-sample adaptation process reveals a consistent improvement in out-of-distribution robustness over
the baseline in all benchmark scenarios, and yields a new state of the art in segmentation accuracy,
substantially surpassing that of previous work in virtually all considered settings."
INTRODUCTION,0.016181229773462782,Under review as a conference paper at ICLR 2022
RELATED WORK,0.019417475728155338,"2
RELATED WORK"
RELATED WORK,0.022653721682847898,"Our work contributes to recent research on generalization of semantic segmentation models, and
relates to the studies on feature normalization (Pan et al., 2018; Schneider et al., 2020) and test-time
training (Sun et al., 2020). While the focus in previous investigations was the training strategy (Yue
et al., 2019) and model design (Pan et al., 2018), we exclusively study the test-time inference process
here. Yue et al. (2019) augmented the synthetic training data by transferring style from real images.
Assuming access to a classification model trained on real images, Chen et al. (2020) regularize
the training on synthetic data by ensuring feature proximity of the two models via distillation, and
seek layer-specific learning rates for improved generalization. Advancing the distillation technique,
Chen et al. (2021) devise a contrastive loss that facilitates model invariance to standard image
augmentations. Pan et al. (2018) heuristically add instance normalization (IN) layers to the network.
We remark on two limitations of these works that we address here: First, these methods assume access
to a distribution of real images during training (Chen et al., 2020; 2021; Yue et al., 2019) (as opposed
to only for pre-training of the backbone), or require a modification of the network architecture (Pan
et al., 2018). Our work requires neither, hence the presented techniques apply even post-hoc to the
already (pre-)trained models to improve their generalization. Second, as we discuss and address in
Sec. 5, the evaluation protocol used by previous studies exhibits a number of shortcomings."
RELATED WORK,0.025889967637540454,"Normalization.
Batch Normalization (BN; Ioffe and Szegedy, 2015) and other normalization
techniques have been increasingly linked to model robustness (Deecke et al., 2019; Huang et al.,
2019; Schneider et al., 2020; Wang et al., 2018; Wu and He, 2018). The most commonly used BN,
Layer Normalization (LN; Ba et al., 2016), and Instance Normalization (IN; Ulyanov et al., 2016)
also affect the model’s expressive power, which can be further enhanced by their arrangements (Nam
and Kim, 2018; Luo et al., 2019). In a domain adaptation setting, Li et al. (2017b) use source-domain
statistics during training while replacing them with target-domain statistics during inference. More
recently, Schneider et al. (2020) combine the source and target statistics during inference, but the
statistics are weighted depending on the number of samples that these statistics aggregate. Nado et al.
(2020) propose using batch statistics during inference from the target domain instead of the training
statistics acquired from the source domain. Our comprehensive empirical study complements these
results by demonstrating improved generalization of semantic segmentation models."
RELATED WORK,0.02912621359223301,"Test-time training.
Test-time training (TTT) refers to updating the model parameters also at
inference time with a self-supervised loss incurred on a single unlabelled test sample. This technique
has been recently applied with success for improving the robustness of image classification models
(Sun et al., 2020; Wang et al., 2021a). The design of the self-supervised task is crucial, and the
techniques developed for image classification are unsuitable for dense prediction tasks, such as se-
mantic segmentation. Nevertheless, recent work explored such losses in domain adaptation scenarios
(Araslanov and Roth, 2021), and a number of other works exploit domain-specific knowledge from
medical imaging (Varsavsky et al., 2020) or first-person vision (Cai et al., 2020)."
RELATED WORK,0.032362459546925564,"Setup comparison.
Most of these technically related works (Schneider et al., 2020; Sun et al.,
2020; Wang et al., 2021a) focus on the problem of domain adaptation in the context of image
classification. They typically assume access to a number of samples (or even all test images) from
the target distribution at training time. Our work instead addresses semantic segmentation in the
domain generalization setting, which is fundamentally different as it only necessitates a single datum
from the test set. In this scenario, simple objectives, such as entropy minimization employed by Tent
(Wang et al., 2021a), improve the baseline accuracy only moderately. By contrast, our one-sample
adaptation with pseudo labels accounts for the inherent uncertainty in the predictions, which proves
substantially more effective, as the comparison to Tent in Sec. 5.3 reveals. Our task is also different
from few-shot learning (e. g., (Finn et al., 2017)), where the model may adapt at test time using a
small annotated set of image samples. No such annotation is available in our setup; our model adjusts
to the test sample in a completely unsupervised fashion, requires neither proxy tasks to update the
parameters (Sun et al., 2020) nor any knowledge of the test domain."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.03559870550161812,"3
INSTANCE-ADAPTIVE BATCH NORMALIZATION"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.038834951456310676,"Batch Normalization (BN; Ioffe and Szegedy, 2015) has become an inextricable component of
modern CNNs (He et al., 2016). Although BN was originally designed for improving training"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.042071197411003236,Under review as a conference paper at ICLR 2022
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.045307443365695796,"convergence, there is now substantial evidence that it plays an important role in model robustness
(Nado et al., 2020), including domain generalization (Pan et al., 2018). Let z ∈RB,C,H,W denote a
feature tensor, with C channels and batch size B, produced by a convolutional layer at resolution
H × W. We omit the layer indexing, as the following presentation applies to all layers in which BN
is applied. At training time, BN first computes the mean and the standard deviation for each of the C
channels batch-wise, i. e."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.04854368932038835,"µc =
1
BHW
X"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.05177993527508091,"i,j,k
zi,c,j,k ,
σ2
c =
1
BHW
X"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.05501618122977346,"i,j,k
(zi,c,j,k −µc)2.
(1)"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.05825242718446602,"The normalized features ˆz follow from applying these statistics,"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.061488673139158574,"ˆzi,c,j,k = (zi,c,j,k−µc)/√"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.06472491909385113,"σ2c+ϵ.
(2)"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.06796116504854369,"Notably, this process differs from the normalization used at inference time. At training time, every
BN layer maintains a running estimate of µc and σc across the training batches, which we denote
here as ˆµc and ˆσc. At test time, it is an established practice to normalize w. r. t. ˆµc and ˆσc in Eq. (2),
instead of the test-batch statistics, a scheme we refer to as train BN (t-BN)."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.07119741100323625,"BN and generalization.
In the context of out-of-distribution generalization, the running statistics
ˆµc and ˆσc derive from the source data and can be substantially different had they been computed
using the target images. This discrepancy is generally known as the covariate shift problem. Domain
adaptation methods, which assume access to the (unlabelled) target distribution, often alleviate this
issue with a technique typically referred to as Adaptive Batch Normalization (AdaBN; Li et al.,
2017b). The key idea behind the method is to simply replace the source running statistics with
those of the target. Instead of alternating BN layers between the training and testing modes, recent
work (Nado et al., 2020) studies prediction-time BN (p-BN), which maintains BN layers in ªtraining
modeº also at inference time. That is, p-BN replaces the running statistics from the training time, ˆµc
and ˆσc, with the statistics µc and σc of the current test batch. Such a seemingly innocuous change is
shown to benefit model robustness for image classification (Nado et al., 2020; Schneider et al., 2020)."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.0744336569579288,"In contrast to AdaBN and p-BN, which utilize either the whole target distribution or a number of
samples, our study of model generalization assumes only a single target example to be available Ð
the one that our model receives as the input at inference time. A viable alternative to AdaBN and
p-BN is to compute the statistics per sample, which amounts to replacing BN layers with Instance
Normalization (IN) layers (Ulyanov et al., 2016) after model training. However, this may cause
another extreme scenario for covariate shift, since such replacement may significantly interfere
with the statistics of the activations in the intermediate layers with which the network was trained.
Moreover, previous work showed that IN layers hurt the discriminative power of the model and can
improve model robustness only in combination with BN (Pan et al., 2018), thereby requiring changes
in the model architecture."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.07766990291262135,"Instance-adaptive batch normalization.
Following Schneider et al. (2020), we compute a weighted
average of the running statistics from training on source data and the target statistics, which in contrast
to Schneider et al. (2020) always derives from a single sample in our work. Let ˆµ(s)
c
and ˆσ(s)
c
denote
the running estimate of the mean and the standard deviation at training time on the source data (for
feature channel c). Similarly, define µ(t)
c
and σ(t)
c
as the mean and the standard deviation computed
from a feature tensor z(t) ∈RC,H,W , coming from a single target sample:"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.08090614886731391,"µ(t)
c
=
1
HW
X"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.08414239482200647,"j,k
z(t)
c,j,k ,
σ(t)2
c
=
1
HW
X"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.08737864077669903,"j,k
(z(t)
c,j,k −µ(t)
c )2.
(3)"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.09061488673139159,"At inference time, we compute the new mean and standard deviation, µ(∗)
c
and σ(∗)
c , as follows:"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.09385113268608414,"µ(∗)
c
= αµ(t)
c
+ (1 −α)ˆµ(s)
c
,
σ(∗)2
c
= ασ(t)2
c
+ (1 −α)ˆσ(s)
c .
(4)"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.0970873786407767,"We then use µ(∗)
c
and σ(∗)
c
in place of µc and σc in Eq. (2) to normalize the features. Note that
this does not affect the behaviour of the BN layers at training time and applies only at test time.
Since this approach combines the inductive bias coming in the form of the running statistics from
the source domain with statistics extracted from a single test instance, we refer to this technique as
Instance-adaptive Batch Normalization (IaBN)."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.10032362459546926,Under review as a conference paper at ICLR 2022
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.10355987055016182,"Augmented Images
CNN"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.10679611650485436,Original Image
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.11003236245954692,"Softmax
Probabilities
pixelwise mean
per class"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.11326860841423948,"Ignore Mask
pixelwise threshold
per class
Pseudo
Ground Truth Loss"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.11650485436893204,"Softmax
Probabilities
per input
Backpropagation"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.11974110032362459,"Semantic
Logits
Afine Inverse
Transformation"
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.12297734627831715,"Figure 1: Overview of the one-sample adaptation process. Based on a single test sample, we create a
batch of images by augmenting the original input with multi-scale versions. Furthermore, we add a
horizontally flipped and grayscaled version for every scale. In order to project the resulting output
from every version back to the original image, we apply the respective inverse affine transformation
to every prediction. Afterwards, we average the predicted softmax probabilities and create a pseudo
label using a class-dependent confidence threshold. We update the model parameters by minimizing
the cross-entropy loss w. r. t. the pseudo label and repeat this process for a small number of iterations,
Nt, before producing the final prediction. The updated model is then discarded."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.1262135922330097,"Discussion.
Setting α = 0 in Eq. (4) defaults to the established procedure, t-BN, and uses only
the running statistics from training on the source domain at inference time. Conversely, α = 1
corresponds to Instance Normalization (Ulyanov et al., 2016) or, equivalently, to the p-BN strategy
(Nado et al., 2020) with a batch size of 1. This is in contrast to (Nado et al., 2020), where µ(t)
c
and
σ(t)
c
are averaged over a batch of target images, whereas we rely on a single test sample in this work.
Our experiments in Sec. 5.1 expand upon previous analyses (Schneider et al., 2020) with an extensive
emperical study of this normalization strategy for semantic segmentation."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.12944983818770225,"Note that Batch Instance Normalization (Nam and Kim, 2018) used a similar weighting approach
for adaptive stylization. However, α was a training parameter, whereas we use α only for model
selection using a validation set."
INSTANCE-ADAPTIVE BATCH NORMALIZATION,0.13268608414239483,"In Sec. 5 we empirically verify that IaBN yields a consistent boost of the segmentation accuracy in
the out-of-distribution scenario. Perhaps more surprisingly, we also find significant improvements in
model calibration (in terms of the expected calibration error, ECE (Naeini et al., 2015)). We leverage
this further in devising a self-supervised approach for test-time training of the segmentation model."
LEARNING FROM A SINGLE SAMPLE,0.13592233009708737,"4
LEARNING FROM A SINGLE SAMPLE"
LEARNING FROM A SINGLE SAMPLE,0.13915857605177995,"So far, we assumed that the parameters of the segmentation model remain fixed at inference time.
However, both from the perspective of adaptive systems and their biological counterparts, this
assumption seems implausible (Thrun, 1998; Widmer and Kubat, 1996). Here, we allow the model to
update its parameters. Note that our setup is distinct from the domain adaptation scenario, in contrast
to Wang et al. (2021a), since we discard the updated parameters when processing the next sample."
LEARNING FROM A SINGLE SAMPLE,0.1423948220064725,"Our approach, visualized in Fig. 1, uses data augmentation as a method to create mini-batches of
images for each test sample. Based on the original test image, we first create a set of augmented
images by multi-scaling, horizontal flipping, and grayscaling. These augmented images are used to
form a mini-batch, which is fed through the CNN. We project the produced softmax probabilities from
the model back to the original pixels using the inverse affine transformations, and denote the result
as mi,:,:,: for every sample i in the mini-batch. This allows the model to have multiple predictions
for one pixel. We then compute the mean ¯m of these softmax probabilities along the mini-batch
dimension i for class c, as"
LEARNING FROM A SINGLE SAMPLE,0.14563106796116504,"¯mc,j,k = 1 N X"
LEARNING FROM A SINGLE SAMPLE,0.1488673139158576,"i
mi,c,j,k.
(5)"
LEARNING FROM A SINGLE SAMPLE,0.15210355987055016,"Using hyperparameter ψ ∈(0, 1), we compute a threshold value tc from the maximum probability of
every class to yield a class-dependent threshold tc:"
LEARNING FROM A SINGLE SAMPLE,0.1553398058252427,"tc = ψ · max( ¯mc,:,:).
(6)"
LEARNING FROM A SINGLE SAMPLE,0.15857605177993528,Under review as a conference paper at ICLR 2022
LEARNING FROM A SINGLE SAMPLE,0.16181229773462782,"We finally extract the class c∗
j,k with the highest probability for every pixel by a simple arg max
operation:
c∗
j,k = arg max( ¯m:,j,k).
(7)
We ignore low-confidence predictions using our class-dependent threshold tc. Specifically, all pixels
with a softmax probability below the threshold are set to an ignore label, while the remaining pixels
use the dominant class c∗
j,k as the pseudo label uj,k,"
LEARNING FROM A SINGLE SAMPLE,0.1650485436893204,"uj,k ="
LEARNING FROM A SINGLE SAMPLE,0.16828478964401294,"(
c∗
j,k,
if max( ¯m:,j,k) ≥tc∗
j,k
ignore,
else.
(8)"
LEARNING FROM A SINGLE SAMPLE,0.1715210355987055,"This generated pseudo ground truth u for the original test image is used to fine-tune the model for
Nt iterations using the cross-entropy loss and gradient descent. We determine all hyperparameters,
i. e. resolution of the scales, threshold ψ, number of iterations Nt, and learning rate η, based on a
development dataset. After the one-sample adaptation process, we produce a single final prediction
using the updated model weights. To process the next test sample, we reset these weights to their
initial value, hence the model obtains no knowledge about the complete target data distribution."
EXPERIMENTS,0.17475728155339806,"5
EXPERIMENTS"
EXPERIMENTS,0.1779935275080906,"Previous studies (Chen et al., 2020; 2021; Pan et al., 2018; Yue et al., 2019) on domain generalization
for semantic segmentation used a number of evaluation schemes. For example, Chen et al. (2020;
2021); Pan et al. (2018) used only a single target domain, Cityscapes (Cordts et al., 2016), for testing.
However, like other research datasets, Cityscapes (Cordts et al., 2016) is carefully curated (e. g., the
same camera hardware and country was used for the image capture), hence only partially represents
the visual diversity of the world. A more comprehensive approach by Yue et al. (2019) considered
a number of target domains and used the average accuracy across those as a generalization metric.
However, the model was separately selected for every target domain based on a different validation
set. As a result, the average accuracy across the tested domains represents the expected accuracy of a
model ensemble. In this work, in contrast, we aim to improve the accuracy of a single model."
EXPERIMENTS,0.18122977346278318,"To that end, we propose a revised evaluation protocol to quantify the generalization ability of a single
model. We consider a practical scenario in which a supplier prepares a model for a consumer without
a-priori knowledge on where this model may be deployed. On the supplier’s side, we assume access
to two data distributions for model training and validation, the source data and the development
set. We assess the generalization ability of the model yielded by the validation process on three
qualitatively distinct target sets. The average accuracy across these sets provides an estimate of the
expected model accuracy for its out-of-distribution deployment on the consumer’s side. Next, we
concretize the datasets used in this study, limited to traffic scenes for compatibility with previous work
(Chen et al., 2021; Pan et al., 2018; Yue et al., 2019), and detail the dataset specifics in Appendix F."
EXPERIMENTS,0.18446601941747573,"Source data.
We train our model on the training split of two synthetic datasets (mutually exclusive)
with low-cost ground truth annotation: GTA (Richter et al., 2016) and SYNTHIA (Ros et al., 2016).
Importantly, these datasets exhibit visual discrepancy (i. e. domain shift) w. r. t. the real imagery, from
which our model needs to generalize."
EXPERIMENTS,0.18770226537216828,"Development set.
For model selection and hyperparameter tuning, we use the validation set of
WildDash (Zendel et al., 2018). In our scenario, the development set is understood to be of limited
quantity, owing to its more costly annotation compared to the source data. In contrast to the training
set, however, it bears closer visual resemblance to the potential target domains."
EXPERIMENTS,0.19093851132686085,"Multi-target evaluation.
Following model selection, we evaluate the single model on three target
domains comprising the validation sets from three datasets of street scenes: Cityscapes (Cordts et al.,
2016), BDD (Yu et al., 2020), and IDD (Varma et al., 2019). The choice of these test domains
stems from a number of considerations, such as the geographic origin of the scenes (Cityscapes,
BDD, and IDD were collected in Germany, North America, and India, respectively). Geographic
distinction as well as substantial differences in data acquisition (e. g., camera properties) of these
datasets bring together an assortment of challenges for the segmentation model at test time. Since the
deployment site of our model is unknown, we assume a uniform prior over the target domains as our
test distribution. Under this assumption, a simple average of the mean segmentation accuracy across
our target domains yields the expected model accuracy."
EXPERIMENTS,0.1941747572815534,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.19741100323624594,"Table 1: (a) Segmentation accuracy using IaBN. We report the mean IoU (%) on three target domains
(Cityscapes, BDD, IDD) across both backbones. t-BN denotes train BN (Ioffe and Szegedy, 2015),
while p-BN refers to prediction-time BN (Nado et al., 2020). (b) ECE (%) for IaBN and MC-Dropout
(Gal and Ghahramani, 2016). We report scores for three target domains (Cityscapes, BDD, IDD)
across both backbones. We trained the networks on GTA in both cases (cf. supplemental material for
results with SYNTHIA training).
(a)"
EXPERIMENTS,0.20064724919093851,"Method
IoU (%, ↑)"
EXPERIMENTS,0.20388349514563106,"CS
BDD
IDD
Mean"
EXPERIMENTS,0.20711974110032363,"ResNet-50
w/ t-BN
30.95
28.52
32.78
30.75
w/ p-BN
37.71
31.67
30.85
33.41
w/ IaBN (ours)
37.54
32.79
34.21
34.85"
EXPERIMENTS,0.21035598705501618,"ResNet-101
w/ t-BN
32.90
32.54
30.36
31.93
w/ p-BN
39.88
34.30
33.05
35.74
w/ IaBN (ours)
42.17
35.40
33.52
37.03 (b)"
EXPERIMENTS,0.21359223300970873,"Method
ECE (%, ↓)"
EXPERIMENTS,0.2168284789644013,"CS
BDD
IDD"
EXPERIMENTS,0.22006472491909385,"ResNet-50 (Baseline)
37.28
35.61
27.73
w/ IaBN
30.57
30.94
26.90
w/ MC-Dropout
30.29
29.80
24.17
w/ MC-Dropout + IaBN
25.50
27.36
22.62"
EXPERIMENTS,0.22330097087378642,"ResNet-101 (Baseline)
35.24
33.74
27.28
w/ IaBN
26.12
28.89
23.98
w/ MC-Dropout
31.30
29.95
25.15
w/ MC-Dropout + IaBN
24.44
28.68
23.32"
EXPERIMENTS,0.22653721682847897,"To compare to previous works, we also evaluate on Mapillary (Neuhold et al., 2017). Mapillary does
not publicly disclose the geographic origins of individual samples, hence is unsuitable to identify a
potential location bias acquired by the model from the training data. This is possible in our proposed
evaluation protocol, since the geographic locations from Cityscapes, BDD, and IDD do not overlap."
EXPERIMENTS,0.2297734627831715,"Implementation details.
We implement our framework1 in PyTorch (Paszke et al., 2019). Following
(Pan et al., 2018), our baseline model is DeepLabv1 (Chen et al., 2015) without CRF post-processing,
but the reported results also generalize to more advanced architectures (see Appendix D). We use
ResNet-50 and ResNet-101 (He et al., 2016) pre-trained on ImageNet (Deng et al., 2009) as backbone.
We minimize the cross-entropy loss with an SGD optimizer and a learning rate of 0.005, decayed
polynomially with the power set to 0.9. All models are trained on the source domains for 50 epochs
with batch size, momentum, and weight decay set to 4, 0.9, and 0.0001, respectively. For data
augmentation, we compute crops of random size (0.08 to 1.0) of the original image size, apply a
random aspect ratio (3/4 to 4/3) to the crop, and then resize the result to 512 × 512 pixels. Furthermore,
we use random horizontal flipping, color jitter, random blur, and grayscaling. We train our models
with SyncBN (Paszke et al., 2019) on two NVIDIA GeForce RTX 2080 GPUs."
EVALUATING IABN,0.23300970873786409,"5.1
EVALUATING IABN"
EVALUATING IABN,0.23624595469255663,"For both source domains (GTA, SYNTHIA) in combination with all main target domains (Cityscapes,
BDD, IDD), we investigate the influence of α on the IoU. Setting an optimal α for every target domain
is infeasible in domain generalization as the target domain during inference is unknown. Instead, we
choose the optimal α in steps of 0.1 based on the IoU on the development set of WildDash. For the
ResNet-50 backbone, we attain the highest validation IoU for both training datasets with α = 0.1
(see Fig. 4 in Appendix B). Fixing this optimal α, we proceed with evaluating our model on the target
domains. Table 1 reports the segmentation accuracy with the optimal α. For comparison, we also
test our models with the two boundary values of α = 0 and α = 1, corresponding to t-BN and p-BN
(cf. Sec. 3), respectively. In Table 1(a) we report IoU scores for both backbones on generalization
from GTA to Cityscapes, BDD, and IDD and compare the accuracy of the target domains with t-BN
and p-BN. Remarkably, IaBN improves the mean IoU not only of the t-BN baseline (e. g., by 4.1%
IoU with ResNet-50), which represents an established evaluation mode, but also over the more recent
p-BN (Nado et al., 2020). This improvement is consistent across the board, i. e. irrespective of the
backbone architecture and the target domain tested. Furthermore, we found that the calibration of
our models, in terms of the expected calibration error (ECE; Naeini et al., 2015), also improves. As
shown in Table 1(b), not only does IaBN substantially enhance the baseline, but is even competitive
with the commonly used MC-Dropout method (Gal and Ghahramani, 2016). Rather surprisingly,
IaBN exhibits a complementary effect with MC-Dropout: the calibration of the predictions improves
even further when both methods are used jointly."
EVALUATING IABN,0.23948220064724918,1Our code and pre-trained models will be made publicly available under the Apache License.
EVALUATING IABN,0.24271844660194175,Under review as a conference paper at ICLR 2022
EVALUATING IABN,0.2459546925566343,"Table 3: Mean IoU (%) with TTA (Simonyan and Zisserman, 2015) and our Seg-TTT. We report
scores across both source domains (GTA, SYNTHIA) and three target domains (Cityscapes, BDD,
IDD) for both backbones."
EVALUATING IABN,0.24919093851132687,"Method
Source: GTA
Source: SYNTHIA"
EVALUATING IABN,0.2524271844660194,"CS
BDD
IDD
Mean
CS
BDD
IDD
Mean"
EVALUATING IABN,0.255663430420712,"ResNet-50 (w/ IaBN)
37.54
32.79
34.21
34.85
36.14
26.66
26.37
29.72
w/ TTA
42.56
37.72
37.98
39.42
39.67
32.10
30.46
34.08
w/ Seg-TTT (ours)
45.13
39.61
40.32
41.69
41.60
33.35
31.22
35.39"
EVALUATING IABN,0.2588996763754045,"ResNet-101 (w/ IaBN)
42.17
35.40
33.52
37.03
38.01
28.66
27.28
31.32
w/ TTA
44.37
38.49
38.35
40.40
39.91
32.68
30.04
34.21
w/ Seg-TTT (ours)
46.99
40.21
40.56
42.59
42.32
33.27
31.40
35.66"
EVALUATING IABN,0.2621359223300971,"Table 2: Mean IoU (%) comparison of IaBN to
alternative normalization strategies: SN (Luo et al.,
2019) and BIN (Nam and Kim, 2018)."
EVALUATING IABN,0.26537216828478966,"Method
CS
BDD
IDD
Mean"
EVALUATING IABN,0.2686084142394822,"SN
31.75
33.60
31.60
32.32
BIN
34.57
32.68
30.22
32.49
IaBN (ours)
37.54
32.79
34.21
34.85"
EVALUATING IABN,0.27184466019417475,"Comparison to other related work.
We addi-
tionally compare IaBN to alternative normaliza-
tion strategies proposed in the literature: Batch-
Instance Normalization (BIN; Nam and Kim,
2018) and Switchable Normalization (SN; Luo
et al., 2019). Although both of these techniques
may appear technically similar to our IaBN,
these approaches were developed for different
purposes. SN was only shown to improve in-
domain accuracy, while BIN tackles domain
adaptation for image classification. Our work studies domain generalization. Furthermore, both
methods modify the model architecture before training, while IaBN works with any pretrained seman-
tic segmentation model. We implemented both in our segmentation model based on the ResNet-50
backbone. We trained these approaches on GTA in an identical setup as IaBN. From results in Table 2,
we observe that IaBN outperforms both BIN and SN by a significant margin in terms of mean IoU of
the target domains."
EVALUATING SEG-TTT,0.2750809061488673,"5.2
EVALUATING SEG-TTT"
EVALUATING SEG-TTT,0.2783171521035599,"In addition to comparing our Seg-TTT to standard inference, we test our models against Test-Time
Augmentation (TTA) (Simonyan and Zisserman, 2015) as a stronger baseline. TTA augments the test
samples with their flipped and grayscaled version on multiple scales and averages the predictions
as the final result. For Seg-TTT, we use horizontal flipping and grayscaling with factor scales of
(0.25, 0.5, 0.75) w. r. t. the original image resolution. We study the relative importance of these
augmentation types in Appendix C. Based on the validation set WildDash, we set threshold ψ = 0.7,
use Nt = 10 iterations and a learning rate η = 0.05. We only train the layers conv4_x, conv5_x,
and the classification head as we did not observe any benefits from updating all model parameters.
Furthermore, this reduces runtime due to not backpropagating through the whole network. We
investigate this choice as part of the runtime-accuracy trade-off in Sec. 5.3. In Table 3, we show
IoU scores for both source domains (GTA, SYNTHIA) and three target domains (Cityscapes, BDD,
IDD) across both backbones. Even though TTA improves the baseline (e. g., by 3.37% IoU with
ResNet-101 using GTA), our proposed Seg-TTT still outperforms it by a clear and consistent margin
of 2.19% IoU on average. This observation aligns well with our reported ECE scores in Table 1(b)
to demonstrate that Seg-TTT further exploits the calibrated confidence of our predictions to yield
reliable pseudo labels for adapting the model for a particular test sample."
COMBINING IABN AND SEG-TTT,0.2815533980582524,"5.3
COMBINING IABN AND SEG-TTT"
COMBINING IABN AND SEG-TTT,0.284789644012945,"Both IaBN and Seg-TTT are orthogonal techniques and can be used jointly. Surprisingly, we find the
techniques mutually complementary. We combine IaBN and Seg-TTT in our model and compare
with state-of-the-art domain generalization methods in Table 4. While most of the other methods
report their results on weaker baselines, we show consistent improvements even over a substantially
stronger baseline. Our single model with IaBN and Seg-TTT even outperforms the model ensemble
approach of Yue et al. (2019) on most benchmarks (e. g., by 13.37% and 9.44% on GTA to Mapillary"
COMBINING IABN AND SEG-TTT,0.28802588996763756,Under review as a conference paper at ICLR 2022
COMBINING IABN AND SEG-TTT,0.2912621359223301,"Table 4: Mean IoU (%) comparison to state-of-the-art domain generalization methods for both
source domains (GTA, SYNTHIA) as well as three target domains (Cityscapes, Mapillary, BDD). We
compare to IBN-Net (Pan et al., 2018), Yue et al. (2019), ASG (Chen et al., 2020) and CSG (Chen
et al., 2021). In-domain training to obtain the upper bounds uses our baseline DeepLabv1 and follows
the same schedule as with the synthetic datasets. (‡) and (†) denote the use of FCN (Long et al.,
2015) and DeepLabv2 (Chen et al., 2018a) architectures, respectively."
COMBINING IABN AND SEG-TTT,0.29449838187702265,"Method
Backbone: ResNet-50
Backbone: ResNet-101"
COMBINING IABN AND SEG-TTT,0.2977346278317152,"CS
Mapillary
BDD
CS
Mapillary
BDD"
COMBINING IABN AND SEG-TTT,0.30097087378640774,"In-domain Bound
71.23
58.39
58.53
73.84
62.81
61.19 GTA"
COMBINING IABN AND SEG-TTT,0.3042071197411003,"No Adapt
22.17↑7.47
±
±
±
±
±
IBN-Net
29.64"
COMBINING IABN AND SEG-TTT,0.3074433656957929,"No Adapt
32.45↑4.97
25.66↑8.46
26.73↑5.41
33.56↑8.97
28.33↑9.72
27.76↑10.96
Yue et al. (2019)‡
37.42
34.12
32.14
42.53
38.05
38.72"
COMBINING IABN AND SEG-TTT,0.3106796116504854,"No Adapt
25.88↑3.77
±
±
29.63↑3.16
±
±
ASG†
29.65
32.79"
COMBINING IABN AND SEG-TTT,0.313915857605178,"No Adapt
25.88↑9.39
±
±
29.63↑9.25
±
±
CSG†
35.27
38.88"
COMBINING IABN AND SEG-TTT,0.31715210355987056,"No Adapt
30.95↑14.18
34.56↑12.93
28.52↑11.09 32.90↑14.09
36.00↑11.49
32.54↑7.67
Ours
45.13
47.49
39.61
46.99
47.49
40.21"
COMBINING IABN AND SEG-TTT,0.32038834951456313,SYNTHIA
COMBINING IABN AND SEG-TTT,0.32362459546925565,"No Adapt
28.36↑7.29
27.24↑5.50
25.16↑6.37
29.67↑7.91
28.73↑5.39
25.64↑8.70
Yue et al. (2019)‡
35.65
32.74
31.53
37.58
34.12
34.34"
COMBINING IABN AND SEG-TTT,0.3268608414239482,"No Adapt
31.83↑9.77
33.41↑7.80
24.30↑9.05
37.25↑5.07
36.84↑4.36
29.32↑3.95
Ours
41.60
41.21
33.35
42.32
41.20
33.27"
COMBINING IABN AND SEG-TTT,0.3300970873786408,"0
1
2
3
4
5
6
7
8
9
10
11
Runtime (s) 30 35 40 45"
COMBINING IABN AND SEG-TTT,0.3333333333333333,IoU (%)
COMBINING IABN AND SEG-TTT,0.3365695792880259,Ensemble (Baseline)
COMBINING IABN AND SEG-TTT,0.33980582524271846,Ensemble (IaBN)
COMBINING IABN AND SEG-TTT,0.343042071197411,Ensemble (TTA)
COMBINING IABN AND SEG-TTT,0.34627831715210355,Baseline IaBN TTA
COMBINING IABN AND SEG-TTT,0.34951456310679613,"Seg-TTT (conv4_x + conv5_x + Head)
Seg-TTT (conv5_x + Head)
Seg-TTT (Head)"
COMBINING IABN AND SEG-TTT,0.35275080906148865,"Figure 2: Runtime-accuracy comparison on GTA →Cityscapes generalization. The curves trace
Seg-TTT iterations, i. e. the first point corresponds to Nt = 1, while the last shows Nt = 10. While
Seg-TTT increases the inference time of the baseline and TTA for the sake of improved accuracy, it is
still more efficient and accurate than model ensembles of 10 networks. The choice of the layers for
Seg-TTT updates (the naming follows He et al. (2016)) further provides a favorable runtime-accuracy
trade-off. Runtimes are computed on a single NVIDIA GeForce RTX 2080 GPU."
COMBINING IABN AND SEG-TTT,0.3559870550161812,"with ResNet-50 and ResNet-101, respectively). Recall that ASG (Chen et al., 2020) and CSG (Chen
et al., 2021) (as well as Yue et al. (2019)) require access to a distribution of real images for training,
while IBN-Net (Pan et al., 2018) modifies the model architecture. Our approach requires neither,
alters only the inference procedure, yet outperforms these methods on most benchmarks substantially.
As in the previous discussion, the improvement over the baseline is consistent, regardless of backbone
architecture or source data."
COMBINING IABN AND SEG-TTT,0.3592233009708738,"Runtime-accuracy trade-off.
We investigate the influence of the number of iterations required to
adapt to a single sample during Seg-TTT. Fig. 2 plots IoU scores for Cityscapes using the ResNet-50
backbone trained on GTA (Appendix C provides further numerical comparison). As a widely adopted
baseline, we also train a model ensemble comprising 10 DeepLabv1 networks (as in Seg-TTT),
initialized with a random seed (Hansen and Salamon, 1990). Note that TTA, Seg-TTT, and the
ensemble use IaBN for a fair comparison, and we also test the ensemble with TTA. We observe that
although Seg-TTT increases the accuracy of the baselines at the expense of higher test-time latency,"
COMBINING IABN AND SEG-TTT,0.36245954692556637,Under review as a conference paper at ICLR 2022
COMBINING IABN AND SEG-TTT,0.3656957928802589,"GTA →Cityscapes
GTA →BDD
GTA →Mapillary
GTA →IDD"
COMBINING IABN AND SEG-TTT,0.36893203883495146,"Image
Ground Truth
Baseline
Ours"
COMBINING IABN AND SEG-TTT,0.37216828478964403,"Figure 3: Qualitative semantic segmentation results for the generalization from GTA to Cityscapes,
BDD, Mapillary, and IDD for the ResNet-50 backbone. The input image, ground truth, prediction of
the baseline model, and prediction of the proposed combination of IaBN and Seg-TTT are shown."
COMBINING IABN AND SEG-TTT,0.37540453074433655,"it is still more efficient and more accurate than the model ensembles. Furthermore, Seg-TTT offers a
practical accuracy-runtime trade-off by means of using fewer update iterations, or updating fewer
upper network layers. While the top-accuracy variant of Seg-TTT may not be suitable for real-time
applications yet, it may still be valuable in other important domains, such as medical imaging, where
high accuracy is desirable even at the cost of increased latency. For real-time needs, IaBN alone
boosts the accuracy of the baseline significantly without any computational overhead."
COMBINING IABN AND SEG-TTT,0.3786407766990291,"Comparison to Tent (Wang et al., 2021a).
Like Seg-TTT, Tent also relies on test-time training.
However, different from constructing the pseudo labels based on well-calibrated predictions in
our Seg-TTT, Tent simply minimizes the entropy of a single-scale prediction. Tent also limits
the adaptation to updating only the BN parameters, whereas our Seg-TTT extends this process to
convolutional layers. To demonstrate these advantages, we trained HRNet-W18 (Wang et al., 2021b)
on GTA and compare the IoU on Cityscapes to the equivalent configuration of Tent. While Tent
reaches 36.4% with 10 update iterations by adapting the model to a single image, IaBN alone already
outperforms it substantially with a single forward pass (40.0%), and reaches 44.1% with 10 Seg-TTT
update iterations."
COMBINING IABN AND SEG-TTT,0.3818770226537217,"Qualitative results.
In Fig. 3 we visualize qualitative segmentation results from our joint model
with IaBN and Seg-TTT for generalization from GTA to Cityscapes, BDD, Mapillary, and IDD. We
observe a clearly perceivable improvement over the baseline, especially in terms of consistency w. r. t.
the image boundaries. Appendix G illustrates further results and offers a discussion of failure cases."
CONCLUSION,0.3851132686084142,"6
CONCLUSION"
CONCLUSION,0.3883495145631068,"We presented a study of an adaptive inference process for improving out-of-distribution robustness of
semantic segmentation models. The accuracy improvement demonstrated in the study is surprisingly
substantial, despite no changes to the training process or the model architecture, unlike in previous
works (Chen et al., 2020; Yue et al., 2019). Considering the simplicity of the studied approach, yet its
significant empirical benefits, we hope that such test-time adaptive strategies can inspire follow-up
work on improving out-of-distribution generalization of models in other research subfields, potentially
extending to other application domains and dense prediction tasks, such as panoptic segmentation, or
monocular depth prediction. In future work, we are excited to explore more effective self-supervised
loss functions, as well as more efficient test-time adjustments to the model parameters."
CONCLUSION,0.39158576051779936,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3948220064724919,"Ethics Statement.
Although increased out-of-distribution robustness can be potentially life-saving
in self-driving scenarios, semantic segmentation models can be also deployed with malicious intent,
such as unauthorized surveillance. However, semantic segmentation models are still subject to
fundamental research with generally low levels of technology readiness (1 to 3) to date and cannot
discriminate on the object level (in contrast to, e. g., instance segmentation). These reasons make
such unwelcome deployment unlikely without any expert knowledge that we commit not to provide.
Moreover, the accuracy of semantic segmentation approaches, while impressive, has not yet reached
levels where these methods should be uncritically deployed."
REPRODUCIBILITY STATEMENT,0.39805825242718446,"Reproducibility Statement.
To facilitate reproducibility, we provide our implementation to the
reviewers, and commit to releasing it publicly upon acceptance. We have also discussed even trivial,
but crucially useful training details of our baseline in-depth in Appendix A and at the start of Sec. 5."
REFERENCES,0.40129449838187703,REFERENCES
REFERENCES,0.4045307443365696,"N. Araslanov and S. Roth. Self-supervised augmentation consistency for adapting semantic segmentation. In
CVPR, pages 15384±15394, 2021."
REFERENCES,0.4077669902912621,"M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv:1907.02893
[stat.ML], 2019."
REFERENCES,0.4110032362459547,"J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016."
REFERENCES,0.41423948220064727,"S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from
different domains. Mach. Learn., 79(1-2):151±175, 2010."
REFERENCES,0.4174757281553398,"S. Bickel, M. Brückner, and T. Scheffer. Discriminative learning under covariate shift. JMLR, 10:2137±2155,
2009."
REFERENCES,0.42071197411003236,"M. Cai, F. Lu, and Y. Sato. Generalizing hand segmentation in egocentric videos with uncertainty-guided model
adaptation. In CVPR, pages 14392±14401, 2020."
REFERENCES,0.42394822006472493,"L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation
with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Trans. Pattern Anal. Mach.
Intell., 40(4):834±848, 2018a."
REFERENCES,0.42718446601941745,"L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep
convolutional nets and fully connected CRFs. In ICLR, 2015."
REFERENCES,0.43042071197411,"L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-decoder with atrous separable convolution
for semantic image segmentation. In ECCV, volume 7, pages 833±851, 2018b."
REFERENCES,0.4336569579288026,"W. Chen, Z. Yu, Z. Wang, and A. Anandkumar. Automated synthetic-to-real generalization. In ICML, pages
1746±1756, 2020."
REFERENCES,0.4368932038834951,"W. Chen, Z. Yu, S. D. Mello, S. Liu, J. M. Alvarez, Z. Wang, and A. Anandkumar. Contrastive syn-to-real
generalization. In ICLR, 2021."
REFERENCES,0.4401294498381877,"S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, and J. Choo. Robustnet: Improving domain generalization in
urban-scene segmentation via instance selective whitening. In CVPR, pages 11580±11590, 2021."
REFERENCES,0.44336569579288027,"M. Cordts, M. Omran, S. Ramos, T. Scharwächter, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and
B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR, pages 3213±3223,
2016."
REFERENCES,0.44660194174757284,"L. Deecke, I. Murray, and H. Bilen. Mode normalization. In ICLR, 2019."
REFERENCES,0.44983818770226536,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li. ImageNet: A large-scale hierarchical image database.
In CVPR, pages 248±255, 2009."
REFERENCES,0.45307443365695793,"C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML,
volume 70, pages 1126±1135, 2017."
REFERENCES,0.4563106796116505,"Y. Gal and Z. Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep
learning. In ICML, pages 1050±1059, 2016."
REFERENCES,0.459546925566343,"Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. S. Lempitsky.
Domain-adversarial training of neural networks. JMLR, 17:59:1±59:35, 2016."
REFERENCES,0.4627831715210356,Under review as a conference paper at ICLR 2022
REFERENCES,0.46601941747572817,"I. Gulrajani and D. Lopez-Paz. In search of lost domain generalization. arXiv:2007.01434 [cs.LG], 2020."
REFERENCES,0.4692556634304207,"L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Trans. Pattern Anal. Mach. Intell., 12(10):
993±1001, 1990."
REFERENCES,0.47249190938511326,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770±778,
2016."
REFERENCES,0.47572815533980584,"J. Huang, D. Guan, A. Xiao, and S. Lu. Fsdr: Frequency space domain randomization for domain generalization.
In CVPR, pages 6891±6902, 2021."
REFERENCES,0.47896440129449835,"L. Huang, Y. Zhou, F. Zhu, L. Liu, and L. Shao. Iterative normalization: Beyond standardization towards
efficient whitening. In CVPR, pages 4874±4883, 2019."
REFERENCES,0.48220064724919093,"S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate
shift. In ICML, pages 448±456, 2015."
REFERENCES,0.4854368932038835,"D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In ICCV,
pages 5542±5550, 2017a."
REFERENCES,0.4886731391585761,"Y. Li, N. Wang, J. Shi, J. Liu, and X. Hou. Revisiting batch normalization for practical domain adaptation. In
ICLR, 2017b."
REFERENCES,0.4919093851132686,"J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages
3431±3440, 2015."
REFERENCES,0.49514563106796117,"M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks.
In NeurIPS, pages 136±144, 2016."
REFERENCES,0.49838187702265374,"P. Luo, J. Ren, Z. Peng, R. Zhang, and J. Li. Differentiable learning-to-normalize via switchable normalization.
In ICLR, 2019."
REFERENCES,0.5016181229773463,"Z. Nado, S. Padhy, D. Sculley, A. D’Amour, B. Lakshminarayanan, and J. Snoek. Evaluating prediction-time
batch normalization for robustness under covariate shift. arXiv:2006.10963 [cs.LG], 2020."
REFERENCES,0.5048543689320388,"M. P. Naeini, G. F. Cooper, and M. Hauskrecht. Obtaining well calibrated probabilities using bayesian binning.
In AAAI, pages 2901±2907, 2015."
REFERENCES,0.5080906148867314,"H. Nam and H. Kim. Batch-instance normalization for adaptively style-invariant neural networks. In NeurIPS,
pages 2563±2572, 2018."
REFERENCES,0.511326860841424,"G. Neuhold, T. Ollmann, S. Rota Bulò, and P. Kontschieder. The Mapillary Vistas dataset for semantic
understanding of street scenes. In ICCV, pages 4990±4999, 2017."
REFERENCES,0.5145631067961165,"X. Pan, P. Luo, J. Shi, and X. Tang. Two at once: Enhancing learning and generalization capacities via IBN-Net.
In ECCV, volume 4, pages 484±500, 2018."
REFERENCES,0.517799352750809,"A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024±8035,
2019."
REFERENCES,0.5210355987055016,"S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV,
volume 2, pages 102±118, 2016."
REFERENCES,0.5242718446601942,"G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The SYNTHIA dataset: A large collection of
synthetic images for semantic segmentation of urban scenes. In CVPR, pages 3234±3243, 2016."
REFERENCES,0.5275080906148867,"S. Schneider, E. Rusak, L. Eck, O. Bringmann, W. Brendel, and M. Bethge. Improving robustness against
common corruptions by covariate shift adaptation. In NeurIPS, volume 33, pages 11539±11551, 2020."
REFERENCES,0.5307443365695793,"K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR,
2015."
REFERENCES,0.5339805825242718,"Y. Sun, X. Wang, Z. Liu, J. Miller, A. A. Efros, and M. Hardt. Test-time training with self-supervision for
generalization under distribution shifts. In ICML, pages 9229±9248, 2020."
REFERENCES,0.5372168284789643,"S. Thrun. Lifelong learning algorithms. In Learning to Learn, pages 181±209. Springer, 1998."
REFERENCES,0.540453074433657,"A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR, pages 1521±1528, 2011."
REFERENCES,0.5436893203883495,Under review as a conference paper at ICLR 2022
REFERENCES,0.5469255663430421,"D. Ulyanov, A. Vedaldi, and V. S. Lempitsky. Instance normalization: The missing ingredient for fast stylization.
arXiv:1607.08022 [cs.CV], 2016."
REFERENCES,0.5501618122977346,"G. Varma, A. Subramanian, A. M. Namboodiri, M. Chandraker, and C. V. Jawahar. IDD: A dataset for exploring
problems of autonomous navigation in unconstrained environments. In WACV, pages 1743±1751, 2019."
REFERENCES,0.5533980582524272,"T. Varsavsky, M. Orbes-Arteaga, C. H. Sudre, M. S. Graham, P. Nachev, and M. J. Cardoso. Test-time
unsupervised domain adaptation. In MICCAI, pages 428±436, 2020."
REFERENCES,0.5566343042071198,"R. Volpi, H. Namkoong, O. Sener, J. C. Duchi, V. Murino, and S. Savarese. Generalizing to unseen domains via
adversarial data augmentation. In NeurIPS, pages 5339±5349, 2018."
REFERENCES,0.5598705501618123,"T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. Pérez. Advent: Adversarial entropy minimization for domain
adaptation in semantic segmentation. In CVPR, pages 2517±2526, 2019."
REFERENCES,0.5631067961165048,"D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell. Tent: Fully test-time adaptation by entropy
minimization. In ICLR, 2021a."
REFERENCES,0.5663430420711975,"G. Wang, J. Peng, P. Luo, X. Wang, and L. Lin. Kalman normalization: Normalizing internal representations
across network layers. In NeurIPS, pages 21±31, 2018."
REFERENCES,0.56957928802589,"J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang, W. Liu, and B. Xiao.
Deep high-resolution representation learning for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell.,
43(10):3349±3364, 2021b."
REFERENCES,0.5728155339805825,"G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden contexts. Mach. Learn., 23(1):
69±101, 1996."
REFERENCES,0.5760517799352751,"Y. Wu and K. He. Group normalization. In ECCV, pages 3±19, 2018."
REFERENCES,0.5792880258899676,"S. Xie, Z. Zheng, L. Chen, and C. Chen. Learning semantic representations for unsupervised domain adaptation.
In ICML, pages 5423±5432, 2018."
REFERENCES,0.5825242718446602,"Y. Yang and S. Soatto. FDA: Fourier domain adaptation for semantic segmentation. In CVPR, pages 4084±4094,
2020."
REFERENCES,0.5857605177993528,"F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell. BDD100K: A diverse driving
dataset for heterogeneous multitask learning. In CVPR, pages 2633±2642, 2020."
REFERENCES,0.5889967637540453,"X. Yue, Y. Zhang, S. Zhao, A. L. Sangiovanni-Vincentelli, K. Keutzer, and B. Gong. Domain randomization and
pyramid consistency: Simulation-to-real generalization without accessing target domain data. In ICCV, pages
2100±2110, 2019."
REFERENCES,0.5922330097087378,"O. Zendel, K. Honauer, M. Murschitz, D. Steininger, and G. F. Domínguez. WildDash - Creating hazard-aware
benchmarks. In ECCV, volume 6, pages 407±421, 2018."
REFERENCES,0.5954692556634305,Under review as a conference paper at ICLR 2022
REFERENCES,0.598705501618123,"A
BASELINE"
REFERENCES,0.6019417475728155,"We found a number of training details to be crucial for obtaining a highly competitive baseline, i. e. a
model without our IaBN and Seg-TTT components. Among them is using heavy data augmentation.
Recall from Sec. 5 that we used random horizontal flipping, multi-scale cropping with a scale range
of [0.08, 1.0], as well as photometric image perturbations:2 color jitter, random blur, and grayscaling.
Color jitter, applied with probability 0.5, perturbs image brightness, contrast, and saturation using a
factor sampled uniformly from the range [0.7, 1.3]. We use a different range of [0.9, 1.1] for the hue
factor. We randomly blur the image using a Gaussian kernel with the standard deviation sampled from
[0.1, 2.0]. Additionally, we convert the image to grayscale with a probability of 0.1. Furthermore,
we also found that the polynomial decay schedule we used for the learning rate, as well training for
at least 50 epochs (for both GTA and SYNTHIA) are essential to achieve a high baseline accuracy.
Note that we only used WildDash as the development set to tune these training details. We also
experimented with higher input resolution and a larger batch size, but did not observe a significant
improvement, yet a drastic increase in the computational overhead."
REFERENCES,0.6051779935275081,"On importance of the baseline.
We note that the reported accuracy from previous work in Table 4
exhibits an inconsistency w. r. t. the choice of the model architectures. In particular, Yue et al. (2019)
use an FCN, yet outperform other domain generalization approaches with DeepLabv1 (Pan et al.,
2018) and DeepLabv2 (Chen et al., 2020; 2021) considerably, as shown in Table 4. This is a
regrettable consequence of inconsistent training schedules used in previous works that proved difficult
to reproduce. For example, at the time of submission, Yue et al. (2019) have not released their
code despite the promise;3 Pan et al. (2018) did not share parts of the code implementing semantic
segmentation.4 These circumstances make reporting the accuracy of the implementation-specific
baselines indispensable, which has thus become the standard practice in more recent previous (Chen
et al., 2020; 2021) and related works (Gulrajani and Lopez-Paz, 2020)."
REFERENCES,0.6084142394822006,"B
IABN"
REFERENCES,0.6116504854368932,"Selecting α.
Fig. 4 shows a detailed plot of the influence of α on the segmentation accuracy, both
on the development set of WildDash and on the target domains. We observe that the maximum
accuracy on the development set is attained with α = 0.1. Clearly, there is no guarantee that value 0.1
is the optimal one for the target domains. However, choosing α based on the development set is in
line with the established practice in machine learning: Tuning model hyperparameters is not allowed
on the test sets (i. e. Cityscapes, BDD, IDD), but is only possible on the validation set (WildDash). In
general, the hyperparameters found to be optimal on the validation set are not guaranteed to remain so
on the test set. Nevertheless, our empirical results show consistent improvements over the baselines
across all scenarios, despite α having been picked based on the validation dataset."
REFERENCES,0.6148867313915858,"SYNTHIA as the training set.
Due to space constraints, we limited our study of IaBN in the
main paper to the scenario of using GTA as the source data (cf. Table 1). Here, we extend this
study by training our models on SYNTHIA instead. Table 5 reports the segmentation accuracy (in
terms of IoU) and the expected calibration error (ECE) for this case. Notably, IaBN can still provide
benefits for the expected segmentation accuracy if p-BN (i. e. using target instance normalization
statistics) fails to improve over the t-BN baseline, as is the case with ResNet-50 in Table 5; the results
remain on par with the t-BN baseline even when p-BN is significantly worse than t-BN. In regard to
calibration quality, the results are consistent with our model trained on GTA (cf. Table 1): Not only
does IaBN improve prediction calibration of the baseline, it again exhibits a complementary effect
with MC-Dropout. Overall, the combined results from Tables 1 and 5 demonstrate that IaBN improves
both the model accuracy and the calibration quality of the predictions in the out-of-distribution setting
irrespective of the backbone network and specifics of the source data."
REFERENCES,0.6181229773462783,"2We use Pillow library (https://pillow.readthedocs.io) to implement photometric augmentation.
3https://github.com/xyyue/DRPC/issues
4https://github.com/XingangPan/IBN-Net"
REFERENCES,0.6213592233009708,Under review as a conference paper at ICLR 2022
REFERENCES,0.6245954692556634,"0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
10 20 30 40"
REFERENCES,0.627831715210356,IoU (%)
REFERENCES,0.6310679611650486,Source: GTA
REFERENCES,0.6343042071197411,"WildDash
CS"
REFERENCES,0.6375404530744336,"BDD
IDD"
REFERENCES,0.6407766990291263,0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
REFERENCES,0.6440129449838188,Source: SYNTHIA
REFERENCES,0.6472491909385113,"Figure 4: Mean IoU (%, ↑) using IaBN based on the optimal alpha on the development set (WildDash).
We report scores for the target domains (Cityscapes, BDD, IDD) for the ResNet-50 backbone after
training on GTA (left) and SYNTHIA (right)."
REFERENCES,0.6504854368932039,"Table 5: (a) Segmentation accuracy using IaBN. We report the mean IoU (%) on three target domains
(Cityscapes, BDD, IDD) across both backbones. As before, t-BN denotes train BN (Ioffe and Szegedy,
2015), while p-BN refers to prediction-time BN (Nado et al., 2020). (b) ECE (%) for IaBN and
MC-Dropout (Gal and Ghahramani, 2016). We report scores for three target domains (Cityscapes,
BDD, IDD) across both backbones. We trained the networks on SYNTHIA in both cases.
(a)"
REFERENCES,0.6537216828478964,"Method
IoU (%, ↑)"
REFERENCES,0.656957928802589,"CS
BDD
IDD
Mean"
REFERENCES,0.6601941747572816,"ResNet-50
w/ t-BN
31.83
24.30
24.73
26.95
w/ p-BN
33.83
23.36
23.39
26.86
w/ IaBN (ours)
36.14
26.66
26.37
29.72"
REFERENCES,0.6634304207119741,"ResNet-101
w/ t-BN
37.25
29.32
27.19
31.25
w/ p-BN
34.58
24.24
22.32
27.05
w/ IaBN (ours)
38.01
28.66
27.28
31.32 (b)"
REFERENCES,0.6666666666666666,"Method
ECE (%, ↓)"
REFERENCES,0.6699029126213593,"CS
BDD
IDD"
REFERENCES,0.6731391585760518,"ResNet-50 (Baseline)
37.50
43.19
40.11
w/ IaBN
30.96
33.27
36.31
w/ MC-Dropout
34.82
37.30
36.63
w/ MC-Dropout + IaBN
30.66
33.06
35.60"
REFERENCES,0.6763754045307443,"ResNet-101 (Baseline)
31.39
33.77
36.56
w/ IaBN
30.33
31.83
36.26
w/ MC-Dropout
32.73
32.76
34.07
w/ MC-Dropout + IaBN
27.71
30.48
32.67"
REFERENCES,0.6796116504854369,"Table 6: The role of the augmentation type in Seg-TTT. We report mean IoU (%, ↑) and runtime (ms,
↓) for TTA (Simonyan and Zisserman, 2015) and our Seg-TTT for the GTA source domain and the
Cityscapes target domain for the ResNet-50 backbone."
REFERENCES,0.6828478964401294,"Method
TTA
Seg-TTT"
REFERENCES,0.686084142394822,"IoU
Runtime
IoU
Runtime"
REFERENCES,0.6893203883495146,"Baseline
30.95
314
31.47
7135
IaBN
37.54
314
39.04
7135"
REFERENCES,0.6925566343042071,"Multiple scales
42.27
749
44.92
7272
Horizontal flipping
38.01
986
39.33
7593
Grayscaling
37.96
908
39.65
7193"
REFERENCES,0.6957928802588996,"Multiple scales + horizontal flipping
42.48
1316
44.94
7890
Multiple scales + grayscaling
42.28
1202
45.06
7486
Multiple scales + horizontal flipping + grayscaling
42.56
1308
45.13
7862"
REFERENCES,0.6990291262135923,"C
SEG-TTT"
REFERENCES,0.7022653721682848,"Further implementation details.
For Seg-TTT, we only update the model parameters in the
layers conv4_x, conv5_x, and the classification head for the ResNet-50 backbone. Due to the
higher computational cost of the ResNet-101 backbone, we only train the layers conv5_x and the
classification head in this case."
REFERENCES,0.7055016181229773,Under review as a conference paper at ICLR 2022
REFERENCES,0.7087378640776699,"Table 7: Runtime (ms) with TTA (Simonyan and Zisserman, 2015) or Seg-TTT. We report the runtime
for both ResNet-50 and ResNet-101 on three dominant resolutions of 2048 × 1024, 1280 × 720, and
1920 × 1080, corresponding to the target domains Cityscapes, BDD, and IDD, respectively."
REFERENCES,0.7119741100323624,"Method
Input resolution"
REFERENCES,0.7152103559870551,"CS
BDD
IDD
Mean"
REFERENCES,0.7184466019417476,"ResNet-50 (single-scale)
314
136
214
221
w/ TTA
1308
713
766
929
w/ Seg-TTT (ours)
7862
3742
5307
5637"
REFERENCES,0.7216828478964401,"ResNet-101 (single-scale)
458
239
252
316
w/ TTA
1519
766
860
1048
w/ Seg-TTT (ours)
9060
4241
6142
6481"
REFERENCES,0.7249190938511327,"The choice of augmentation strategies.
We verify the influence of the augmentation type used
by Seg-TTT for test-time training. Recall from Sec. 5.2 that we use multiple scales with horizontal
flipping and grayscaling to augment one image sample. We compare a flipping-only, scaling-only,
and grayscaling-only version of our Seg-TTT to the combination of flipping, grayscaling, and scaling,
which we used in the main text. We used a ResNet-50 backbone trained on GTA and report the
test-time accuracy on Cityscapes in Table 6. We observe a significant boost in accuracy in comparison
to our IaBN baseline with no augmentations. Furthermore, we show that using multiple scales
is more important than flipping for Seg-TTT. Note that the augmentations used do not impact
runtime in a significant way, since the batch sizes between these setups vary insignificantly; it is the
backpropagation that dominates the main computational footprint. Varying the number of iterations,
as studied in Sec. 5.3, provides a more flexible mechanism for accuracy-runtime trade-off."
REFERENCES,0.7281553398058253,"Inference time.
Table 7 compares the inference time of our Seg-TTT w. r. t. test-time augmentation
(TTA) and single-scale inference across a range of input resolutions available in the target domains.
We obtain these results by running inference on a single NVIDIA GeForce RTX 2080 GPU. To
improve the runtime estimate, for each dataset we average the inference time over the complete image
set. This is to account for small deviations in the input resolution (e. g., IDD mostly contains images
of resolution 1920 × 1080, but also has images with resolution 1280 × 720). Since our Seg-TTT uses
10 update iterations, the increase in the inference time w. r. t. TTA is expected. Although such cost
may be detrimental for real-time applications, the clear accuracy benefits of Seg-TTT (cf. Table 4)
may potentially appeal to use cases where the importance of the prediction quality outweighs the
overhead in the frame rate. Furthermore, we investigated the influence of using the automatic mixed
precision module in PyTorch and its influence on the inference runtime. While maintaining an
identical IoU on Cityscapes using the ResNet-50 backbone, mixed precision achieves a runtime of
5746 ms compared to 7862 ms using single precision using Nt = 10 for our Seg-TTT. With Nt = 3
we are even able to decrease the runtime of Seg-TTT to 2914 ms."
REFERENCES,0.7313915857605178,"We additionally provide runtime-accuracy plots for GTA →BDD and GTA →IDD generalization in
Fig. 5. The data supports our conclusions drawn on GTA →Cityscapes generalization (cf. Sec. 5.3)
that (i) Seg-TTT provides clear advantages in segmentation accuracy over baselines at a reasonable
increase of the inference time; (ii) it is both more accurate and more efficient than model ensembles;
and (iii) it exhibits a flexible runtime-accuracy trade-off by means of varying the number of update
iterations and the number of the layers to adjust."
REFERENCES,0.7346278317152104,"D
TESTING IABN AND SEG-TTT WITH OTHER ARCHITECTURES"
REFERENCES,0.7378640776699029,"Our approach generalizes to more recent architectures. We trained four state-of-the-art segmentation
models on GTA: DeepLabv3+ (Chen et al., 2018b) with both a ResNet-50 and ResNet-101 backbone
as well as HRNet-W18 and HRNet-W48 (Wang et al., 2021b). Table 8 reports consistent and
substantial improvement of the mean IoU over the baseline, across all these architectures and the
target domains."
REFERENCES,0.7411003236245954,Under review as a conference paper at ICLR 2022
REFERENCES,0.7443365695792881,"0
1
2
3
4
5
6
7
8
9
10
11
Runtime (s) 25 30 35 40"
REFERENCES,0.7475728155339806,IoU (%)
REFERENCES,0.7508090614886731,Ensemble (Baseline)
REFERENCES,0.7540453074433657,Ensemble (IaBN)
REFERENCES,0.7572815533980582,Ensemble (TTA)
REFERENCES,0.7605177993527508,Baseline IaBN TTA
REFERENCES,0.7637540453074434,"Seg-TTT (conv4_x + conv5_x + Head)
Seg-TTT (conv5_x + Head)
Seg-TTT (Head)"
REFERENCES,0.7669902912621359,"0
1
2
3
4
5
6
7
8
9
10
11
Runtime (s) 25 30 35 40"
REFERENCES,0.7702265372168284,IoU (%)
REFERENCES,0.7734627831715211,"Ensemble (Baseline)
Ensemble (IaBN)"
REFERENCES,0.7766990291262136,Ensemble (TTA)
REFERENCES,0.7799352750809061,Baseline IaBN TTA
REFERENCES,0.7831715210355987,"Seg-TTT (conv4_x + conv5_x + Head)
Seg-TTT (conv5_x + Head)
Seg-TTT (Head)"
REFERENCES,0.7864077669902912,"Figure 5: Runtime-accuracy comparison on GTA →BDD (top) and GTA →IDD (bottom) general-
ization. The curves on this plot trace Seg-TTT iterations, i. e. the first point corresponds to Nt = 1,
while the last shows Nt = 10. While Seg-TTT increases the inference time of the baseline and TTA
for the sake of improved accuracy, it is still more efficient and accurate than model ensembles of
10 networks. The choice of the layers for Seg-TTT updates (the naming follows He et al. (2016))
further provides a favorable runtime-accuracy trade-off. Runtimes are computed on a single NVIDIA
GeForce RTX 2080 GPU."
REFERENCES,0.7896440129449838,"Table 8: Mean IoU (%) using our IaBN + Seg-TTT integrated with DeepLabv3+ (Chen et al., 2018b)
based on a ResNet-50 and ResNet-101 backbone as well as with HRNet-W18 and HRNet-W48 (Wang
et al., 2021b). We observe substantial improvements of the segmentation accuracy on all three target
domains (Cityscapes, BDD, and IDD) after training on GTA."
REFERENCES,0.7928802588996764,"Method
Target domains"
REFERENCES,0.7961165048543689,"CS
BDD
IDD
Mean"
REFERENCES,0.7993527508090615,"DeepLabv3+ ResNet-50 (Baseline)
37.51
35.45
37.50
36.82
w/ IaBN + Seg-TTT (ours)
46.56
43.17
44.07
44.60"
REFERENCES,0.8025889967637541,"DeepLabv3+ ResNet-101 (Baseline)
38.19
37.05
38.22
37.82
w/ IaBN + Seg-TTT (ours)
48.14
44.52
45.72
46.13"
REFERENCES,0.8058252427184466,"HRNet-W18 (Baseline)
33.08
29.40
32.97
31.82
w/ IaBN + Seg-TTT (ours)
44.05
38.29
43.78
42.04"
REFERENCES,0.8090614886731392,"HRNet-W48 (Baseline)
34.66
30.85
34.64
33.38
w/ IaBN + Seg-TTT (ours)
48.82
42.79
43.74
45.12"
REFERENCES,0.8122977346278317,"E
COMPARISON TO CONTEMPORANEOUS WORK"
REFERENCES,0.8155339805825242,"In Table 9, we compare our method to the more recent approaches: RobustNet (Choi et al., 2021)
and FSDR (Huang et al., 2021). Choi et al. (2021) disentangle domain-specific and domain-invariant
properties from higher-order statistics of the feature representation by using an instance selective"
REFERENCES,0.8187702265372169,Under review as a conference paper at ICLR 2022
REFERENCES,0.8220064724919094,"Table 9: Mean IoU (%) comparison to state-of-the-art domain generalization methods for both
source domains (GTA, SYNTHIA) as well as three target domains (Cityscapes, Mapillary, BDD). We
compare to RobustNet (Choi et al., 2021) and FSDR (Huang et al., 2021). (‡) and (††) denote the use
of FCN (Long et al., 2015) and DeepLabv3+ (Chen et al., 2018b) architectures, respectively."
REFERENCES,0.8252427184466019,"Method
Backbone: ResNet-50
Backbone: ResNet-101"
REFERENCES,0.8284789644012945,"CS
Mapillary
BDD
CS
Mapillary
BDD GTA"
REFERENCES,0.8317152103559871,"No Adapt
28.95↑7.63
28.18↑12.15
25.14↑10.06
±
±
±
RobustNet††
36.58
40.33
35.20"
REFERENCES,0.8349514563106796,"No Adapt
37.75↑8.36
40.36↑9.17
33.70↑8.05
±
±
±
Ours††
46.11
49.53
41.75 GTA"
REFERENCES,0.8381877022653722,"No Adapt
±
±
±
33.4 ↑11.4
27.9 ↑15.5
27.3 ↑13.9
FSDR‡
44.8
43.4
41.2"
REFERENCES,0.8414239482200647,"No Adapt
30.95↑14.18
34.56↑12.93
28.52↑11.09 32.90↑14.09
36.00↑11.49
32.54↑7.67
Ours
45.13
47.49
39.61
46.99
47.49
40.21"
REFERENCES,0.8446601941747572,SYNTHIA
REFERENCES,0.8478964401294499,"No Adapt
±
±
±
-
-
-
FSDR‡
40.8
39.6
37.4"
REFERENCES,0.8511326860841424,"No Adapt
31.83↑9.77
33.41↑7.80
24.30↑9.05
37.25↑5.07
36.84↑4.36
29.32↑3.95
Ours
41.60
41.21
33.35
42.32
41.20
33.27"
REFERENCES,0.8543689320388349,"whitening loss. FSDR (Huang et al., 2021) randomizes images in the frequency space by keeping
domain-invariant frequency components and varying the domain-sensitive frequency components."
REFERENCES,0.8576051779935275,"For a fair comparison with RobustNet, we trained a DeepLabv3+ model with the output stride of
16 in contrast to the output stride of 8 used by default (e. g., in Table 8). We also remark that
FSDR used the target domains for hyperparameter tuning and model selection, which gives it a clear
advantage.5 Nonetheless, we outperform FSDR in terms of IoU on two out of three target domains
using ResNet-101 backbone. Remarkably, our baseline training schedule and data augmentation
alone already reach the accuracy of RobustNet without any additional regularization or changes to
the model architecture. Our IaBN with Seg-TTT further improve over these results by significant
margins (more than 8% IoU) across all tested target domains."
REFERENCES,0.86084142394822,"F
DATASET DETAILS"
REFERENCES,0.8640776699029126,"GTA.
GTA (Richter et al., 2016) is a street view dataset generated semi-automatically from the
computer game Grand Theft Auto V. The dataset consists of 12,403 training images, 6,382 validation
images, and 6,181 testing images of resolution 1914 × 1052 with 19 different semantic classes."
REFERENCES,0.8673139158576052,"SYNTHIA. We use the SYNTHIA-RAND-CITYSCAPES subset of the synthetic dataset SYNTHIA
(Ros et al., 2016), which contains 9,400 images, and has 16 semantic classes in common with GTA.
Images have a resolution of 1280 × 760 pixels."
REFERENCES,0.8705501618122977,"WildDash.
This dataset was developed to evaluate models regarding their robustness for driving
scenarios under real-world conditions. It comprises 4256 images of real-world scenes with a resolution
of 1920 × 1080 pixels."
REFERENCES,0.8737864077669902,"Cityscapes.
Cityscapes (Cordts et al., 2016) is an ego-centric street-scene dataset and contains
5,000 high-resolution images with 2048 × 1024 pixels. It is split into 2,975 train, 500 val, and 1,525
test images with 19 semantic classes being annotated."
REFERENCES,0.8770226537216829,"BDD. BDD (Yu et al., 2020) is a driving video dataset, which also contains semantic labelings with
the identical 19 classes as in the other datasets. Images have a resolution of 1280 × 720 pixels. The
training, validation, and test sets contain 7,000, 1,000, and 2,000 images, respectively."
REFERENCES,0.8802588996763754,"IDD. IDD (Varma et al., 2019) is a dataset for road scene understanding in unstructured environments.
It contains 10,003 images annotated with 34 classes even though we only evaluate on the 19 classes"
REFERENCES,0.883495145631068,5https://github.com/jxhuang0508/FSDR/issues/2#issuecomment-910089417
REFERENCES,0.8867313915857605,Under review as a conference paper at ICLR 2022
REFERENCES,0.889967637540453,"GTA →Cityscapes
GTA →BDD
GTA →Mapillary
GTA →IDD"
REFERENCES,0.8932038834951457,"Image
Ground Truth
Baseline
Ours"
REFERENCES,0.8964401294498382,"Figure 6: Qualitative semantic segmentation results for generalization from GTA to Cityscapes, BDD,
Mapillary, and IDD for the ResNet-101 backbone."
REFERENCES,0.8996763754045307,"overlapping with the other datasets. IDD is split into 6,993 training images, 981 validation images,
and 2,029 test images."
REFERENCES,0.9029126213592233,"Mapillary. Annotations from Mapillary (Neuhold et al., 2017) contain 66 object classes; analogously
to IDD we only evaluate on the 19 classes overlapping with the other datasets. The dataset is split into
a training set with 18,000 images and a validation set with 2,000 images with a minimum resolution
of 1920 × 1080 pixels."
REFERENCES,0.9061488673139159,"G
QUALITATIVE EXAMPLES"
REFERENCES,0.9093851132686084,"We provide additional qualitative results by running inference on three models: ResNet-101 trained
on GTA in Fig. 6; ResNet-50 and ResNet-101 trained on SYNTHIA in Figs. 7 and 8, respectively.
Similar to our observations in Sec. 6.3 (cf. main text), our approach exhibits more homogenous
semantic masks with visibly fewer jagged-shaped artifacts than the baseline (e. g., ªsidewalkº false
positives in Fig. 6). Models with our IaBN and Seg-TTT may still struggle with cases of mislabeling
regions with an incorrect, but semantically related class. For example, the model often assigns
ªsidewalkº to the road pixels from BDD and IDD in Figs. 7 and 8. This is an expected outcome if
the erroneous labels are already contained in the pseudo labels of the initial prediction, on which
Seg-TTT relies. These failure cases occur more frequently if the domain shift between the train and
the test distributions is more significant, such as between SYNTHIA and BDD, which can lead to
poorly calibrated predictions. Since applying IaBN results in improved calibration (cf. Table 1), it
alleviates this issue, and Seg-TTT can cope well with milder domain shift scenarios, as a result. As
examples on Cityscapes and Mapillary in Figs. 7 and 8 show, despite the baseline model exhibiting
some degree of this failure mode, our inference method visibly rectifies these errors."
REFERENCES,0.912621359223301,"We additionally ran our inference on video sequences and include the results as part of our sup-
plementary material. Confirming our previous analysis of the qualitative results (cf. Sec. 5.3), we
observe that our approach clearly improves the segmentation quality and removes some of the most
pathological failure modes of the baseline (e. g., the lower middle part of the frame area)."
REFERENCES,0.9158576051779935,Under review as a conference paper at ICLR 2022
REFERENCES,0.919093851132686,"SYNTHIA →Cityscapes
SYNTHIA →BDD
SYNTHIA →Mapillary
SYNTHIA →IDD"
REFERENCES,0.9223300970873787,"Image
Ground Truth
Baseline
Ours"
REFERENCES,0.9255663430420712,"Figure 7: Qualitative semantic segmentation results for generalization from SYNTHIA to Cityscapes,
BDD, Mapillary, and IDD for the ResNet-50 backbone."
REFERENCES,0.9288025889967637,"SYNTHIA →Cityscapes
SYNTHIA →BDD
SYNTHIA →Mapillary
SYNTHIA →IDD"
REFERENCES,0.9320388349514563,"Image
Ground Truth
Baseline
Ours"
REFERENCES,0.9352750809061489,"Figure 8: Qualitative semantic segmentation results for generalization from SYNTHIA to Cityscapes,
BDD, Mapillary, and IDD for the ResNet-101 backbone."
REFERENCES,0.9385113268608414,"H
FAILURE CASES"
REFERENCES,0.941747572815534,"Conceptually, if the initial semantic prediction is incorrect and confident (due to miscalibration), it is
likely to end up in the pseudo labels and lead astray the adaptation process. The rightmost columns
in Fig. 7 and Fig. 8 already visualized this failure mode: the baseline largely misclassifies ªroadª as
the ªsidewalkª class. We have further extended these examples with Fig. 9. Misguided by incorrect
pseudo labels, our test-time adaptation may exacerbate this issue, i. e. propagate the incorrect label to
the areas sharing the same appearance. To gain further insights, we investigated this issue statistically."
REFERENCES,0.9449838187702265,Under review as a conference paper at ICLR 2022
REFERENCES,0.948220064724919,"GTA →Cityscapes
GTA →BDD
GTA →Mapillary
GTA →IDD"
REFERENCES,0.9514563106796117,"Image
Ground Truth
Baseline
Ours"
REFERENCES,0.9546925566343042,"Figure 9: Failure cases of semantic segmentation for generalization results from GTA to Cityscapes,
BDD, Mapillary, and IDD for the ResNet-50 backbone."
REFERENCES,0.9579288025889967,"10 0 10
10 20 30 40
0 10 20 30"
CS,0.9611650485436893,"40
CS"
CS,0.9644012944983819,"10 0 10
10 20 30 40 BDD"
CS,0.9676375404530745,"10 0 10
10 20 30 40 IDD"
CS,0.970873786407767,"10 0 10
10 20 30 40"
CS,0.9741100323624595,Mapillary
CS,0.9773462783171522,IoU (%)
CS,0.9805825242718447,Amount of images (%)
CS,0.9838187702265372,"Figure 10: Density distribution of the individual images for generalization from source domain (GTA)
to target domains (Cityscapes, BDD, IDD and Mapillary) for the ResNet-50 backbone. We visualize
the relative improvement of our test-time inference strategy w.r.t. the baseline in terms of accuracy
with ∆IoU (%)."
CS,0.9870550161812298,"The histograms in Fig. 10 show relative improvement of our test-time inference strategy w. r. t. the
baseline in terms of IoU on four validation sets. We conclude that such decrease in segmentation
accuracy is actually rather rare: less than 10% of the images, on average, exhibit lower accuracy. In
most such cases the accuracy reduces only marginally (by less than 5%), and only a fraction of images
(less than 1% on average) deteriorate in accuracy by at most 10%. The overwhelming majority of the
image samples benefit from our test-time adaptive process and increase in accuracy by up to 35%
IoU."
CS,0.9902912621359223,Under review as a conference paper at ICLR 2022
CS,0.9935275080906149,"I
CODE"
CS,0.9967637540453075,"We provide an implementation of our approach as part of the supplementary material. The enclosed
README provides further details on running this code for both training and inference. Upon acceptance,
this code will be released to the community under the Apache License. Furthermore, to reproduce
the state-of-the-art results of our models reported in Table 4, we make their snapshots available via
an anonymous link provided in the README file."
