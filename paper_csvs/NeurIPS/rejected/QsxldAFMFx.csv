Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010141987829614604,"There has been a growing interest in using AI to model human behavior, particularly
1"
ABSTRACT,0.002028397565922921,"in domains where humans interact with this technology. While most existing work
2"
ABSTRACT,0.0030425963488843813,"models human behavior at an aggregate level, our goal is to model behavior at
3"
ABSTRACT,0.004056795131845842,"the individual level. Recent approaches to behavioral stylometry—or the task of
4"
ABSTRACT,0.005070993914807302,"identifying a person from their actions alone—have shown promise in domains
5"
ABSTRACT,0.006085192697768763,"like chess, but these approaches are either not scalable (e.g., fine-tune a model for
6"
ABSTRACT,0.007099391480730223,"each person) or not generative, in that they cannot generate actions in the style of
7"
ABSTRACT,0.008113590263691683,"each person. We address these limitations by casting behavioral stylometry as a
8"
ABSTRACT,0.009127789046653144,"multi-task learning problem—where each task represents a distinct person—and
9"
ABSTRACT,0.010141987829614604,"using parameter-efficient fine-tuning (PEFT) methods to learn an explicit style
10"
ABSTRACT,0.011156186612576065,"vector for each person. Style vectors are generative: they selectively activate
11"
ABSTRACT,0.012170385395537525,"shared ""skill"" parameters to generate actions in the style of each person. They also
12"
ABSTRACT,0.013184584178498986,"induce a latent style space that we can interpret and manipulate algorithmically.
13"
ABSTRACT,0.014198782961460446,"In particular, we develop a general technique for style steering that identifies a
14"
ABSTRACT,0.015212981744421906,"subset of players with a desired style property, and steers a new player towards that
15"
ABSTRACT,0.016227180527383367,"property. We apply our approach to two very different games, at unprecedented
16"
ABSTRACT,0.017241379310344827,"scale: chess (47,864 players) and Rocket League (2,000 players).
17"
INTRODUCTION,0.018255578093306288,"1
Introduction
18"
INTRODUCTION,0.019269776876267748,"The rapid advances in machine learning in recent years has made it increasingly important to find
19"
INTRODUCTION,0.02028397565922921,"constructive ways for humans to interact with this technology. Even in domains where AI has
20"
INTRODUCTION,0.02129817444219067,"achieved proficiency, it is often important to understand how humans approach these tasks. Such an
21"
INTRODUCTION,0.02231237322515213,"understanding can help identify areas for improvement in humans, develop better AI collaborators or
22"
INTRODUCTION,0.02332657200811359,"teachers, create more human-like experiences, and more.
23"
INTRODUCTION,0.02434077079107505,"A common method for capturing human behavior is behavioral cloning (BC), a form of imitation
24"
INTRODUCTION,0.02535496957403651,"learning [Schaal, 1996] that applies supervised learning to fixed demonstrations collected for a given
25"
INTRODUCTION,0.02636916835699797,"task. While traditionally used in domains such as robotics [Florence et al., 2022] and self-driving
26"
INTRODUCTION,0.02738336713995943,"vehicles [Pomerleau, 1988], BC has seen increasing use in gaming, such as in Counter-Strike [Pearce
27"
INTRODUCTION,0.028397565922920892,"and Zhu, 2022], Overcooked [Carroll et al., 2019], Minecraft [Schäfer et al., 2023], Bleeding
28"
INTRODUCTION,0.029411764705882353,"Edge [Jelley et al., 2024], and chess McIlroy-Young et al. [2020].
29"
INTRODUCTION,0.030425963488843813,"The above work focuses on modeling human behavior in aggregate, with the goal of developing better
30"
INTRODUCTION,0.03144016227180527,"AI partners, opponents, and training tools. However, we believe that the most value for such goals can
31"
INTRODUCTION,0.032454361054766734,"be derived by modeling human behavior at the individual level. To that end, recent results in chess
32"
INTRODUCTION,0.033468559837728194,"have shown the most promise. McIlroy-Young et al. [2020] used behavior cloning to create a set of
33"
INTRODUCTION,0.034482758620689655,"models called Maia that match human play at 9 aggregate skill levels. By fine-tuning these models on
34"
INTRODUCTION,0.035496957403651115,"the data of 400 individual players, they created 400 personalized models that achieve 4-5% higher
35"
INTRODUCTION,0.036511156186612576,"move-matching accuracy on average [McIlroy-Young et al., 2022]. The authors use these models to
36"
INTRODUCTION,0.037525354969574036,"perform behavioral stylometry with high accuracy, where the goal is to identify which person played
37"
INTRODUCTION,0.038539553752535496,"a given query set of games; in this case, they simply apply each of the 400 models to the query set
38"
INTRODUCTION,0.03955375253549696,"and output the one with the highest accuracy. McIlroy-Young et al. [2021] propose a more scalable
39"
INTRODUCTION,0.04056795131845842,"approach of training a Transformer-based embedding on the games of each player, and use this to
40"
INTRODUCTION,0.04158215010141988,"perform accurate stylometry across 2,844 players; in this case, they compute the embedding of the
41"
INTRODUCTION,0.04259634888438134,"query set of games and match it to the closest player’s embedding.
42"
INTRODUCTION,0.0436105476673428,"These approaches have different merits. The individual model approach creates a generative model
43"
INTRODUCTION,0.04462474645030426,"for each player, but it is not scalable and shares only initial (base model) knowledge across the
44"
INTRODUCTION,0.04563894523326572,"players; adding a new player requires fine-tuning a separate model. The embedding approach is
45"
INTRODUCTION,0.04665314401622718,"much more scalable: it learns a compact (single-vector) representation of each player in a shared
46"
INTRODUCTION,0.04766734279918864,"style space, and supports few-shot learning to embed a new player in this space. It cannot be used to
47"
INTRODUCTION,0.0486815415821501,"generate moves, however, and hence cannot reason about player behavior in practice.
48"
INTRODUCTION,0.04969574036511156,"An ideal solution would combine these properties: generative, scalable, shared knowledge, compact
49"
INTRODUCTION,0.05070993914807302,"representation. Our key insight for achieving this is to view behavioral stylometry as a multi-task
50"
INTRODUCTION,0.05172413793103448,"learning problem, where each task represents an individual person. The goal here is to generalize
51"
INTRODUCTION,0.05273833671399594,"across an initial set of players (tasks) while supporting few-shot learning of new players (tasks). To
52"
INTRODUCTION,0.0537525354969574,"do this efficiently, we leverage recent advances in parameter-efficient fine-tuning (PEFT) [Ponti et al.,
53"
INTRODUCTION,0.05476673427991886,"2023, Caccia et al., 2022]. Specifically, we augment an existing BC model with a set of Low Rank
54"
INTRODUCTION,0.055780933062880324,"Adapters (LoRAs) as well as a routing matrix that specifies a distribution over these adapters for
55"
INTRODUCTION,0.056795131845841784,"each player. Unlike approaches that train a separate LoRA for each task, this modular design allows
56"
INTRODUCTION,0.057809330628803245,"players to softly share parameters in a fine-grained manner. We apply this adapter framework to two
57"
INTRODUCTION,0.058823529411764705,"very different game models (which we create): a modified version of the Maia model for chess, and a
58"
INTRODUCTION,0.059837728194726165,"Transformer-based BC model for Rocket League, a 3D soccer video game played by cars in a caged
59"
INTRODUCTION,0.060851926977687626,"arena. (Our models scale beyond the prior art and may be of independent interest.) Our methodology
60"
INTRODUCTION,0.061866125760649086,"first trains the BC models to convergence across all player data, and then fine-tunes the adapters
61"
INTRODUCTION,0.06288032454361055,"and routing matrix on per-player data. This encourages the adapters to learn different latent skills
62"
INTRODUCTION,0.06389452332657201,"that explain the variance between players, while each row of the routing matrix induces a weight
63"
INTRODUCTION,0.06490872210953347,"distribution over these skills. We call each row the style vector for the corresponding player.
64"
INTRODUCTION,0.06592292089249494,"Style vectors are versatile and powerful. They support few-shot learning which enables stylometry at
65"
INTRODUCTION,0.06693711967545639,"scale. They induce a generative model for each player that we can run and observe. They induce a
66"
INTRODUCTION,0.06795131845841786,"shared style space that we can interpret and manipulate algorithmically. Leveraging these properties,
67"
INTRODUCTION,0.06896551724137931,"we develop a general technique for style steering that identifies a subset of players who exhibit a
68"
INTRODUCTION,0.06997971602434078,"desired style property, and steers a new player towards that property. Our main results include:
69"
INTRODUCTION,0.07099391480730223,"1. We perform behavioral stylometry at an unprecedented scale for chess (47,864 players, 94.4%
70"
INTRODUCTION,0.0720081135902637,"accuracy) and Rocket League (2,000 players, 86.7% accuracy), using a query set of 100 games.
71"
INTRODUCTION,0.07302231237322515,"2. Our per-player generative models achieve move-matching accuracy in the range 45-69% for
72"
INTRODUCTION,0.07403651115618662,"chess and 44-72% for Rocket League, even for players with very few (e.g., 50) games.
73"
INTRODUCTION,0.07505070993914807,"3. Style vectors capture a wide diversity of playing styles and strengths. They can be combined,
74"
INTRODUCTION,0.07606490872210954,"interpolated, and steered, while reflecting consistent changes to play style and strength.
75"
BACKGROUND AND FRAMING,0.07707910750507099,"2
Background and Framing
76"
BACKGROUND AND FRAMING,0.07809330628803246,"We frame behavioral stylometry and per-player generative modeling as a multitask learning problem,
77"
BACKGROUND AND FRAMING,0.07910750507099391,"to which we apply PEFT methods. In multitask learning [Caruana, 1997, Ruder et al., 2019],
78"
BACKGROUND AND FRAMING,0.08012170385395538,"we are given a collection of tasks T =
 
T1, . . . , T|T |

, each task Ti associated with a dataset
79"
BACKGROUND AND FRAMING,0.08113590263691683,"Di =

(x1, y1), ..., (xni, yni)
	
. Multitask learning exploits the similarities among related training
80"
BACKGROUND AND FRAMING,0.0821501014198783,"tasks by transferring knowledge among them; ideally, this builds representations that are easily
81"
BACKGROUND AND FRAMING,0.08316430020283976,"adaptable to new tasks using potentially few target examples. The premise of this paper is that
82"
BACKGROUND AND FRAMING,0.08417849898580122,"modeling individual human behavior from a pool of players can be interpreted as a multitask learning
83"
BACKGROUND AND FRAMING,0.08519269776876268,"problem. In other words, each task Ti consists of modeling the behavior of a specific player i; and
84"
BACKGROUND AND FRAMING,0.08620689655172414,"dataset Di corresponds to the sequence of game actions taken by player i. Specifically, an (x, y) tuple
85"
BACKGROUND AND FRAMING,0.0872210953346856,"denotes a game state x at a specific point in time during game, along with the action y that player i
86"
BACKGROUND AND FRAMING,0.08823529411764706,"took in this state. For the rest of the paper, we use the notion of tasks and players interchangeably.
87"
PARAMETER-EFFICIENT FINE-TUNING,0.08924949290060852,"2.1
Parameter-efficient fine-tuning
88"
PARAMETER-EFFICIENT FINE-TUNING,0.09026369168356999,"Popularized in NLP, parameter-efficient fine-tuning (PEFT) [Houlsby et al., 2019, Hu et al., 2022, Liu
89"
PARAMETER-EFFICIENT FINE-TUNING,0.09127789046653144,"et al., 2022] approaches have emerged as a scalable solution for adapting Large Language Models to
90"
PARAMETER-EFFICIENT FINE-TUNING,0.0922920892494929,"several downstream tasks. Indeed, standard finetuning of pretrained LLMs requires updating (and
91"
PARAMETER-EFFICIENT FINE-TUNING,0.09330628803245436,"storing) possibly billions of parameters for each task. PEFT methods instead freeze the pretrained
92"
PARAMETER-EFFICIENT FINE-TUNING,0.09432048681541583,"model and inject a small set of trainable task-specific weights, or “adapters"".
93"
PARAMETER-EFFICIENT FINE-TUNING,0.09533468559837728,"One such approach is the use of Low Rank Adapters (LoRA) [Hu et al., 2022], which modify linear
94"
PARAMETER-EFFICIENT FINE-TUNING,0.09634888438133875,"transformations in the network by adding a learnable low rank shift
95"
PARAMETER-EFFICIENT FINE-TUNING,0.0973630831643002,"h =
 
W0 + ∆W

x =
 
W0 + ABT 
x.
(1)"
PARAMETER-EFFICIENT FINE-TUNING,0.09837728194726167,"Here, W0 ∈Rd×d are the (frozen) weights of the pre-trained model, and A, B ∈Rd×r the learnable
96"
PARAMETER-EFFICIENT FINE-TUNING,0.09939148073022312,"low-rank parameters of rank r ≪d. With this approach, practitioners can trade off parameter
97"
PARAMETER-EFFICIENT FINE-TUNING,0.10040567951318459,"efficiency with expressivity by increasing the rank r of the transformation.
98"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10141987829614604,"2.2
Polytropon and Multi-Head Adapter Routing
99"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10243407707910751,"Standard PEFT methods such as LoRA can adapt a pretrained model for a given task. In multitask
settings, training a separate set of adapters for each task is suboptimal, as it does not enable any
sharing of information, or transfer, across similar tasks. On the other hand, using the same set of
adapters for all tasks risks negative interference [Wang et al., 2021] across dissimilar tasks. Polytropon
[Ponti et al., 2019] (Poly) addresses this transfer/interference tradeoff by softly sharing parameters
across tasks. That is, each Poly layer contains 1) an inventory of LoRA adapters"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10344827586206896,"M = {A(1)B(1), . . . , A(m)B(m)},"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10446247464503043,"with m ≪|T |, and 2) a task-routing matrix Z ∈R|T |×m, where Zτ ∈Rm specifies task τ’s
100"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10547667342799188,"distribution over the shared modules. This formulation allows similar tasks to share adapters, while
101"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10649087221095335,"allowing dissimilar tasks to have non-overlapping parameters. The collection of adapters M can be
102"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.1075050709939148,"interpreted as capturing different facets of knowledge, or latent skills, of the full multitask distribution.
103"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10851926977687627,"At each forward pass, Poly LoRA adapters for task τ are constructed as follows:
104"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.10953346855983773,"Aτ =
X"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.1105476673427992,"i
αiA(i); Bτ =
X"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11156186612576065,"i
αiB(i)
(Poly)"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11257606490872211,"where αi = softmax(Z [τ])i denotes the mixing weight of the i-th adapter in the inventory, and
105"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11359026369168357,"A(i), B(i), Aτ, Bτ ∈Rd×r. Here, the τ-th row of the routing matrix Z is effectively selecting
106"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11460446247464504,"which adapter modules to include in the linear combination. In our setting, where each task consists
107"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11561866125760649,"of modeling an individual, Z [τ] specifies which latent skills are activated for user τ; we call this their
108"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11663286004056796,"style vector. As per Eqn 1, the final output of the linear mapping becomes h =
 
W0 + Aτ(Bτ)T 
x.
109"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11764705882352941,"In Poly, the module combination step remains coarse, as only linear combinations of the existing
110"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11866125760649088,"modules can be generated. Caccia et al. [2022] propose a more fine-grained approach, called Multi-
111"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.11967545638945233,"Head Routing (MHR), which is what we use in our work. Similar to Multi-Head Attention [Vaswani
112"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.1206896551724138,"et al., 2017], the input dimension of A (and output dimensions of B) are partitioned into h heads,
113"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.12170385395537525,"where a Poly-style procedure occurs for each head. The resulting parameters from each head are
114"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.12271805273833672,"then concatenated, recovering the full input (and output) dimensions. See A.1 for more details.
115"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.12373225152129817,"Routing-only fine-tuning.
While LoRA adapters can reduce the parameter cost from billions to
116"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.12474645030425964,"millions [Liu et al., 2022], training the adapters for each new task can still be prohibitive when dealing
117"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.1257606490872211,"with thousands of tasks. To this end, Caccia et al. [2022] proposed routing-only finetuning, where
118"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.12677484787018256,"after an initial phase of pretraining, the adapter modules are fixed, and only the routing parameters Z
119"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.12778904665314403,"are learned for a new task. This reduces the parameter cost for each additional task by several orders
120"
POLYTROPON AND MULTI-HEAD ADAPTER ROUTING,0.12880324543610547,"of magnitude, while maintaining similar performance. We use this method for few-shot learning.
121"
ML METHODOLOGY,0.12981744421906694,"3
ML Methodology
122"
ML METHODOLOGY,0.1308316430020284,"In this section, we detail our methodology for creating a generative model of individual behavior that
123"
ML METHODOLOGY,0.13184584178498987,"enables our style analyses. Our methodology applies to any behavior cloning scenario with access to
124"
ML METHODOLOGY,0.1328600405679513,"human demonstrations from multiple individuals. To demonstrate this generality, we apply it to two
125"
ML METHODOLOGY,0.13387423935091278,"very different games: chess and Rocket League. We start with a base model for each and apply the
126"
ML METHODOLOGY,0.13488843813387424,"MHR adapter framework to it, and then discuss model training and evaluation.
127"
ML METHODOLOGY,0.1359026369168357,"game state
p(action|x)"
ML METHODOLOGY,0.13691683569979715,possible actions
ML METHODOLOGY,0.13793103448275862,player style vector
ML METHODOLOGY,0.1389452332657201,skill inventory
ML METHODOLOGY,0.13995943204868155,"+
Wpretrained"
ML METHODOLOGY,0.140973630831643,Base Model Layer LoRA
ML METHODOLOGY,0.14198782961460446,"A
BT
active player’s style vector"
ML METHODOLOGY,0.14300202839756593,players
ML METHODOLOGY,0.1440162271805274,"skills or
…"
ML METHODOLOGY,0.14503042596348883,Layer N
ML METHODOLOGY,0.1460446247464503,skill inventory
ML METHODOLOGY,0.14705882352941177,Layer 1 …
ML METHODOLOGY,0.14807302231237324,Base Model
ML METHODOLOGY,0.14908722109533468,"Figure 1: (left) Our overall architecture. We augment a base model with a set of MHR adapters and
a routing matrix composed of each player’s style vector. (right) Detailed view of an MHR layer,
showing a skill inventory of adapters shared across players. The player’s style vector specifies which
skills are active (in this case, the first and third) to generate the final low-rank weight shift that is
applied to the (frozen) base model layer."
MODEL ARCHITECTURE,0.15010141987829614,"3.1
Model architecture
128"
MODEL ARCHITECTURE,0.1511156186612576,"For chess, we follow McIlroy-Young et al. [2022] and use the Squeeze-and-Excitation (S&E) Residual
129"
MODEL ARCHITECTURE,0.15212981744421908,"Network [Hu et al., 2018] as a base model, but with a deeper and wider configuration (see A.2).
130"
MODEL ARCHITECTURE,0.15314401622718052,"At every residual block, an additional 2-layer MLP rescales the residual output along the channel
131"
MODEL ARCHITECTURE,0.15415821501014199,"dimension to explicitly model channel interdependencies. The input is a 112-channel 8 × 8 image
132"
MODEL ARCHITECTURE,0.15517241379310345,"representation of the chess board; the output is the predicted move represented as a 1858-dimensional
133"
MODEL ARCHITECTURE,0.15618661257606492,"vector. The total parameters is 15.7M. For Rocket League, we use the GPT-2 architecture from
134"
MODEL ARCHITECTURE,0.15720081135902636,"Radford et al. [2019] with a dimensionality of 768, 12 attention heads, and 12 layers. The input is a
135"
MODEL ARCHITECTURE,0.15821501014198783,"49-dimensional vector with game physics information; the output is 8 heads: 5 with 3 bins of [-1, 0,
136"
MODEL ARCHITECTURE,0.1592292089249493,"1] and 3 binary. The model has no embedding layer, as the game data points are passed directly as
137"
MODEL ARCHITECTURE,0.16024340770791076,"tokens after processing. The total parameters is 87.7M.
138"
MODEL ARCHITECTURE,0.1612576064908722,"To enable user-based adaptation, we incorporate the MHR adapters described in §2.2 into our base
139"
MODEL ARCHITECTURE,0.16227180527383367,"models, as illustrated in Fig. 1. In chess, for every linear transformation in the MLP used for channel-
140"
MODEL ARCHITECTURE,0.16328600405679514,"wise rescaling, we add an MHR layer built of LoRA adapters with rank 16, for a total of 12×2=24
141"
MODEL ARCHITECTURE,0.1643002028397566,"MHR layers. We use an adapter inventory of size 32 and a multi-head routing strategy with 8 heads.
142"
MODEL ARCHITECTURE,0.16531440162271804,"Therefore, for each user we must learn 32×8=256 routing parameters as their style vector. This yields
143"
MODEL ARCHITECTURE,0.1663286004056795,"5M additional parameters. For Rocket League, we attach the adapters to the fully connected layer of
144"
MODEL ARCHITECTURE,0.16734279918864098,"each transformer block, resulting in 12 MHR layers of LoRAs with rank 16. We use an inventory size
145"
MODEL ARCHITECTURE,0.16835699797160245,"of 16 and 64 heads. This yields 13.8M additional parameters. To facilitate interpretability and style
146"
MODEL ARCHITECTURE,0.16937119675456389,"analysis, we use the same routing (style vector) across all MHR layers.
147"
DATA COLLECTION AND PARTITIONING,0.17038539553752535,"3.2
Data collection and partitioning
148"
DATA COLLECTION AND PARTITIONING,0.17139959432048682,"We use data from the largest open-source online chess platform, Lichess.org [Duplessis, 2021], which
149"
DATA COLLECTION AND PARTITIONING,0.1724137931034483,"boasts a database of over 4.8 billion games. We collected Blitz games played between 2013 and
150"
DATA COLLECTION AND PARTITIONING,0.17342799188640973,"2020 inclusive—these are games with 3 or 5 minutes per side, optionally with a few seconds of
151"
DATA COLLECTION AND PARTITIONING,0.1744421906693712,"time increment per move—and applied the same player filtering criteria as McIlroy-Young et al.
152"
DATA COLLECTION AND PARTITIONING,0.17545638945233266,"[2022]. The resulting dataset comprises 47,864 unique players and over 244 million games. (See A.2
153"
DATA COLLECTION AND PARTITIONING,0.17647058823529413,"for a discussion on data imbalance.) For Rocket League, we collect data from a large open-source
154"
DATA COLLECTION AND PARTITIONING,0.17748478701825557,"replay database, Ballchasing.com [CantFlyRL, 2024]. We use 2.2 million 1v1 replays from 2015 to
155"
DATA COLLECTION AND PARTITIONING,0.17849898580121704,"mid-2022, totalling several decades of human game play hours at 5 minutes per game. After parsing,
156"
DATA COLLECTION AND PARTITIONING,0.1795131845841785,"each Rocket League game state is a vector holding the player’s 3D position, linear and angular
157"
DATA COLLECTION AND PARTITIONING,0.18052738336713997,"velocity, boost remaining, rotation, and team; we also include the opponent’s state and the position,
158"
DATA COLLECTION AND PARTITIONING,0.1815415821501014,"linear and angular velocity of the ball. Given a game state, we have to predict the user’s throttle, steer
159"
DATA COLLECTION AND PARTITIONING,0.18255578093306288,"(while grounded), pitch, yaw, roll (while aerial), jump, boost, and handbrake. Additional processing
160"
DATA COLLECTION AND PARTITIONING,0.18356997971602435,"was needed to correct for missing aerial controls and inconsistent sampling rates (24-27hz). Our full
161"
DATA COLLECTION AND PARTITIONING,0.1845841784989858,"data processing procedure, including the challenges we faced, are detailed in A.3.
162"
DATA COLLECTION AND PARTITIONING,0.18559837728194725,"We divide the set of players into a few subsets to support our training methodology. The base player
163"
DATA COLLECTION AND PARTITIONING,0.18661257606490872,"set comprises all data and is used to train the base models. The fine-tuning player set is used to
164"
DATA COLLECTION AND PARTITIONING,0.1876267748478702,"fine-tune the MHR architecture shown in Fig. 1. (For both, we split each player’s data into 80/10/10 for
165"
DATA COLLECTION AND PARTITIONING,0.18864097363083165,"train/test/validation.) The few-shot player set is used for few-shot learning based on a reference set of
166"
DATA COLLECTION AND PARTITIONING,0.1896551724137931,"100 games per player. For our chess experiments, to enable a direct comparison with prior work, we
167"
DATA COLLECTION AND PARTITIONING,0.19066937119675456,"create an additional fine-tuning player set consisting of the same 400 players used in those studies.
168"
DATA COLLECTION AND PARTITIONING,0.19168356997971603,"Currently, we treat each player’s data holistically, but in principle one could partition a player’s data
169"
DATA COLLECTION AND PARTITIONING,0.1926977687626775,"in different ways to perform a finer analysis of their playing style. We explore this in A.4.
170"
MODEL TRAINING AND EVALUATION,0.19371196754563894,"3.3
Model training and evaluation
171"
MODEL TRAINING AND EVALUATION,0.1947261663286004,"Base model.
We train our base Maia model for chess using data from a base player set of all 47,864
172"
MODEL TRAINING AND EVALUATION,0.19574036511156187,"players, treating this as a classification task of predicting human move y made in chess position x,
173"
MODEL TRAINING AND EVALUATION,0.19675456389452334,"given a datapoint (x, y). We use the same loss functions and evaluation criteria as the original Maia
174"
MODEL TRAINING AND EVALUATION,0.19776876267748478,"work: Maia’s policy head uses a cross entropy loss while the value head uses MSE; the output of the
175"
MODEL TRAINING AND EVALUATION,0.19878296146044624,"policy head is used to evaluate the model’s move-matching accuracy.
176"
MODEL TRAINING AND EVALUATION,0.1997971602434077,"We train our Rocket League model using a base player set of over 800,000 players, though the vast
177"
MODEL TRAINING AND EVALUATION,0.20081135902636918,"majority of players have 5 games or fewer. We discretize the actions into 3 bins for throttle, steer,
178"
MODEL TRAINING AND EVALUATION,0.20182555780933062,"pitch, yaw, and roll, as most of this data is close to 0, -1, or 1. We use binary outputs for jump, boost,
179"
MODEL TRAINING AND EVALUATION,0.2028397565922921,"and handbrake. A next-move prediction is labelled correct if and only if all of the outputs are correct.
180"
MODEL TRAINING AND EVALUATION,0.20385395537525355,"MHR fine-tuning.
To train the MHR LoRA adapters, we adopt the methodology used in Caccia et al.
181"
MODEL TRAINING AND EVALUATION,0.20486815415821502,"[2022]: namely, we freeze the base model and fine-tune the MHR layers and routing matrix using data
182"
MODEL TRAINING AND EVALUATION,0.20588235294117646,"from a fine-tuning player set. Recall that the routing matrix Z has a row (style vector) for each player
183"
MODEL TRAINING AND EVALUATION,0.20689655172413793,"in the fine-tuning set. Following Ponti et al. [2019], we use a two-speed learning rate, where the style
184"
MODEL TRAINING AND EVALUATION,0.2079107505070994,"vectors’ learning rate is higher than the adapters’, to enable better specialization.
185"
MODEL TRAINING AND EVALUATION,0.20892494929006086,"For chess, we use two fine-tuning player sets in our experiments, creating two separate MHR-Maia
186"
MODEL TRAINING AND EVALUATION,0.2099391480730223,"models. The first set comprises all 47,864 players and is used to evaluate behavioral cloning and
187"
MODEL TRAINING AND EVALUATION,0.21095334685598377,"stylometry at very large scale. The second set is comprised of the same 400 players used by McIlroy-
188"
MODEL TRAINING AND EVALUATION,0.21196754563894524,"Young et al. [2022], which we use to compare few-shot learning and stylometry results. For Rocket
189"
MODEL TRAINING AND EVALUATION,0.2129817444219067,"League, we train an MHR-Rocket model on a fine-tuning set of 2,000 players with 100 games each.
190"
MODEL TRAINING AND EVALUATION,0.21399594320486814,"Few-shot learning.
To perform few-shot learning on our MHR models, we perform the “routing-only
191"
MODEL TRAINING AND EVALUATION,0.2150101419878296,"fine-tuning"" described in section 2.2 that additionally freezes all MHR LoRA adapters. Given a few-
192"
MODEL TRAINING AND EVALUATION,0.21602434077079108,"shot player, we add a (randomly-initialized) new row to Z and fine-tune it on the player’s reference
193"
MODEL TRAINING AND EVALUATION,0.21703853955375255,"set of games, eventually learning a style vector for the player. Using this style vector, we can invoke
194"
MODEL TRAINING AND EVALUATION,0.21805273833671399,"a generative model of the player and use it to evaluate move-matching accuracy, as described above.
195"
MODEL TRAINING AND EVALUATION,0.21906693711967545,"To perform stylometry, if the player is a seen player (i.e., part of the fine-tuning set), then a matching
196"
MODEL TRAINING AND EVALUATION,0.22008113590263692,"style vector already exists in Z, and we can find it using cosine similarity. Otherwise, if the player is
197"
MODEL TRAINING AND EVALUATION,0.2210953346855984,"unseen, then we simply repeat the few-shot learning process on a query set of games (from the same
198"
MODEL TRAINING AND EVALUATION,0.22210953346855983,"player), and compare this new style vector to the entries in Z.
199"
MODEL TRAINING AND EVALUATION,0.2231237322515213,"For chess, (unless stated otherwise), all of our few-shot experiments use the MHR-Maia model fine-
200"
MODEL TRAINING AND EVALUATION,0.22413793103448276,"tuned on the 400-player set from McIlroy-Young et al. [2022]. For Rocket League, the few-shot
201"
MODEL TRAINING AND EVALUATION,0.22515212981744423,"player set consists of 1,000 of the 2,000-player set used to fine-tune MHR-Rocket.
202"
MODEL TRAINING AND EVALUATION,0.22616632860040567,"Evaluation.
We evaluate a fine-tuned MHR model in two ways. First, we measure its move-matching
203"
MODEL TRAINING AND EVALUATION,0.22718052738336714,"accuracy, similar to how we evaluate the base models. However, since our MHR models provide a
204"
MODEL TRAINING AND EVALUATION,0.2281947261663286,"generative model for each player (invoked through their style vector), we can separately evaluate each
205"
MODEL TRAINING AND EVALUATION,0.22920892494929007,"player’s model by applying it to their test set and measuring move-matching accuracy. The overall
206"
MODEL TRAINING AND EVALUATION,0.2302231237322515,"move-matching accuracy for the model is simply the average of these per-player accuracies.
207"
MODEL TRAINING AND EVALUATION,0.23123732251521298,"Our second evaluation method uses the model to perform behavioral stylometry among all players in
208"
MODEL TRAINING AND EVALUATION,0.23225152129817445,"the fine-tuning set. This is done by leveraging our few-shot learning methodology (above). That is,
209"
MODEL TRAINING AND EVALUATION,0.2332657200811359,"given a query set of games from some player, we learn a new style vector in Z for those games via
210"
MODEL TRAINING AND EVALUATION,0.23427991886409735,"few-shot learning, and compare this vector to every other vector in Z. Using cosine similarity as our
211"
MODEL TRAINING AND EVALUATION,0.23529411764705882,"distance metric, we simply output the player with the highest cosine similarity to the query set vector.
212"
STYLE METHODOLOGY,0.2363083164300203,"4
Style methodology
213"
STYLE METHODOLOGY,0.23732251521298176,"The style vectors in Z represent distinct distributions over latent skills that give us a starting point for
214"
STYLE METHODOLOGY,0.2383367139959432,"comparing player styles. For example, our stylometry method above uses the cosine similarity of
215"
STYLE METHODOLOGY,0.23935091277890466,"Method
|Query|
|Universe|
|Query Games|
Random (%)
Acc. (%)"
STYLE METHODOLOGY,0.24036511156186613,"Seen few-shot players
McIlroy-Young et al. [2022]
400
400
100
0.25
98.0
McIlroy-Young et al. [2021]
400
400
100
0.25
99.5
MHR-Maia
400
400
100
0.25
99.8
McIlroy-Young et al. [2022]
400
400
30
0.25
94.0
MHR-Maia
400
400
30
0.25
98.8
MHR-Maia
10000
47864
100
0.002
94.4"
STYLE METHODOLOGY,0.2413793103448276,"Unseen few-shot players
McIlroy-Young et al. [2021]
578
2844
100
0.035
79.1
MHR-Maia (100 games)
10000
10000
100
0.01
87.6
Table 1: Stylometry accuracy results. Seen few-shot players are a subset of the fine-tuning player set,
unlike unseen players. Numbers for McIlroy-Young et al. [2022] and McIlroy-Young et al. [2021] are
borrowed from their respective papers."
STYLE METHODOLOGY,0.24239350912778904,"these vectors to determine how similar or different players are. However, style vectors also enable
216"
STYLE METHODOLOGY,0.2434077079107505,"much more powerful capabilities, such as the ability to synthesize new (human-like) styles.
217"
STYLE METHODOLOGY,0.24442190669371197,"To begin, we measure the intra-player consistency of style vectors by splitting a player’s dataset
218"
STYLE METHODOLOGY,0.24543610547667344,"into disjoint subsets of varying size, and few-shot learning a style vector for each subset. We then
219"
STYLE METHODOLOGY,0.24645030425963488,"investigate inter-player consistency by merging the datasets of two players and seeing if the style
220"
STYLE METHODOLOGY,0.24746450304259635,"vector trained on the merged dataset is similar to the average of the two player’s style vectors.
221"
STYLE METHODOLOGY,0.2484787018255578,"The latter method actually creates a new playing style that is human-like and yet previously unseen
222"
STYLE METHODOLOGY,0.24949290060851928,"in the world. This suggests a more general approach to style synthesis: interpolate between players
223"
STYLE METHODOLOGY,0.25050709939148075,"using a convex combination of their style vectors. To determine the playing strength of these new
224"
STYLE METHODOLOGY,0.2515212981744422,"players, we can simulate games between them and the players they are derived from. The results of
225"
STYLE METHODOLOGY,0.2525354969574036,"these games can be used to calculate a win rate, which can then be converted to a strength rating.
226"
STYLE METHODOLOGY,0.2535496957403651,"Currently, our advanced style synthesis techniques focus on chess, where there is a robust mapping
227"
STYLE METHODOLOGY,0.25456389452332656,"between win rates and playing strength (the Elo rating system), and simulating games is cheap.
228"
STYLE METHODOLOGY,0.25557809330628806,"Rocket League simulations are quite costly at present, but in principle the same methodology should
229"
STYLE METHODOLOGY,0.2565922920892495,"apply and we plan to reduce these costs in future work.
230"
STYLE METHODOLOGY,0.25760649087221094,"In order to make style comparisons more human-understandable, we again exploit the generative
231"
STYLE METHODOLOGY,0.25862068965517243,"nature of our MHR models. Inspired by the concept probing technique used to analyze AlphaZero
232"
STYLE METHODOLOGY,0.25963488843813387,"(a deep RL chess engine) [McGrath et al., 2022], we use a set of human-coded heuristic functions
233"
STYLE METHODOLOGY,0.2606490872210953,"found in Stockfish (a traditional chess engine) to evaluate a player’s model. These functions capture
234"
STYLE METHODOLOGY,0.2616632860040568,"concepts such as: king safety, material imbalance, piece mobility, and so on. By invoking a player’s
235"
STYLE METHODOLOGY,0.26267748478701824,"model on a fixed set of chess positions and seeing which move they select, we can use this to
236"
STYLE METHODOLOGY,0.26369168356997974,"summarize how much emphasis the player places on the corresponding concepts.
237"
STYLE METHODOLOGY,0.2647058823529412,"Finally, we combine the above methods to design a simple but general method for steering a player’s
238"
STYLE METHODOLOGY,0.2657200811359026,"game style towards a specific attribute a, such as increasing their king safety, while limiting the
239"
STYLE METHODOLOGY,0.2667342799188641,"changes on other attributes (so as to preserve their style). To achieve this, we first collect a set players
240"
STYLE METHODOLOGY,0.26774847870182555,"X who exhibit high values for attribute a—determined, for example, by running their generative
241"
STYLE METHODOLOGY,0.268762677484787,"models on a fixed set of game states. We then extract the common direction among these players, by
242"
STYLE METHODOLOGY,0.2697768762677485,"averaging their style vectors and subtracting the population average. This yields a style delta vector
243"
STYLE METHODOLOGY,0.27079107505070993,"that can be added to any player’s style vector to elicit the desired change.
244"
EXPERIMENTS,0.2718052738336714,"5
Experiments
245"
EXPERIMENTS,0.27281947261663286,"In this section, we demonstrate two main findings. First, MHR-Maia performs competitively with
246"
EXPERIMENTS,0.2738336713995943,"prior methods for behavior cloning and stylometry in chess, while achieving unprecedented scale.
247"
EXPERIMENTS,0.2748478701825558,"We also show that our approach can be applied to Rocket League, for both stylometry and move
248"
EXPERIMENTS,0.27586206896551724,"prediction. Second, we show that explicitly capturing style vectors allows us to reason about and
249"
EXPERIMENTS,0.2768762677484787,"perform arithmetic operations on generated behaviors.
250"
BEHAVIORAL STYLOMETRY,0.2778904665314402,"5.1
Behavioral Stylometry
251"
BEHAVIORAL STYLOMETRY,0.2789046653144016,"In this section, we show that our models perform competitively with previous behavioral stylometry
252"
BEHAVIORAL STYLOMETRY,0.2799188640973631,"methods for both seen and unseen players. Here, the goal is to predict the player who produced a given
253"
BEHAVIORAL STYLOMETRY,0.28093306288032455,"set of games. We compare to individual model fine-tuning [McIlroy-Young et al., 2022], fitting a
254"
BEHAVIORAL STYLOMETRY,0.281947261663286,"pre-trained Maia to the data from a single player, and to a Transformer-based method [McIlroy-Young
255"
BEHAVIORAL STYLOMETRY,0.2829614604462475,"et al., 2021], which embeds players in a 512-dimensional style space based on their gameplay. All
256"
BEHAVIORAL STYLOMETRY,0.2839756592292089,"reported accuracies are top-1 unless stated otherwise.
257"
BEHAVIORAL STYLOMETRY,0.28498985801217036,"To perform stylometry on a query set of games, McIlroy-Young et al. [2022] suggest measuring
258"
BEHAVIORAL STYLOMETRY,0.28600405679513186,"the move-matching accuracy of each available fine-tuned model and selecting the best performing
259"
BEHAVIORAL STYLOMETRY,0.2870182555780933,"model. As seen in Table 1, this procedure works well, but is tremendously expensive—requiring
260"
BEHAVIORAL STYLOMETRY,0.2880324543610548,"computationally intensive inference calls on the entire query set for every candidate player.
261"
BEHAVIORAL STYLOMETRY,0.28904665314401623,"In contrast, both the Transformer-based method and MHR-Maia scale much better to large numbers of
262"
BEHAVIORAL STYLOMETRY,0.29006085192697767,"players. The Transformer-based method needs only to condition on these games to produce a vector,
263"
BEHAVIORAL STYLOMETRY,0.29107505070993916,"while MHR-Maia needs only to fit a new vector. In either case, the produced vectors need only be
264"
BEHAVIORAL STYLOMETRY,0.2920892494929006,"matched to those in the player set, e.g., using cosine similarity. Table 1 compares both approaches,
265"
BEHAVIORAL STYLOMETRY,0.29310344827586204,"showing that MHR-Maia performs competitively or better, on a much larger universe. To do this, we
266"
BEHAVIORAL STYLOMETRY,0.29411764705882354,"use few-shot learning to compute style vectors for 10,000 players based on their 100 game reference
267"
BEHAVIORAL STYLOMETRY,0.295131845841785,"sets, then fit new style vectors for each player based on their respective query sets. Note that the
268"
BEHAVIORAL STYLOMETRY,0.2961460446247465,"individual model fine-tuning method is omitted from the larger few-shot study due to scalability
269"
BEHAVIORAL STYLOMETRY,0.2971602434077079,"reasons. The Transformer-based method can scale, but it is not a generative model.
270"
BEHAVIORAL STYLOMETRY,0.29817444219066935,"For Rocket League, to the best of our knowledge, we are the first to attempt stylometry. We report
271"
BEHAVIORAL STYLOMETRY,0.29918864097363085,"player identification results averaged over the few-shot player set. For each prediction, our MHR-
272"
BEHAVIORAL STYLOMETRY,0.3002028397565923,"Rocket approach must correctly identify each of the 1,000 players among a pool of 2,000 players.
273"
BEHAVIORAL STYLOMETRY,0.3012170385395537,"Yet, it reaches an accuracy of 86.7% (random: 0.05%), showcasing the validity of our approach.
274"
MOVE GENERATION,0.3022312373225152,"5.2
Move generation
275"
MOVE GENERATION,0.30324543610547666,"25
50
100
500
1000
5000 10000
Game Count 57 58 59 60 61 62"
MOVE GENERATION,0.30425963488843816,Accuracy (%)
MOVE GENERATION,0.3052738336713996,"Maia Accuracy
MHR-Maia Accuracy"
MOVE GENERATION,0.30628803245436104,"Figure 2: Accuracy at various game counts
of the individual models (Maia) and our
method (MHR-Maia)."
MOVE GENERATION,0.30730223123732253,"Here we compare the efficacy of our method to using
276"
MOVE GENERATION,0.30831643002028397,"individually fine-tuned models for each player. Fine-
277"
MOVE GENERATION,0.3093306288032454,"tuning individual models generally results in superior
278"
MOVE GENERATION,0.3103448275862069,"results compared to PEFT methods, as the increased pa-
279"
MOVE GENERATION,0.31135902636916835,"rameter count produces more expressive models. How-
280"
MOVE GENERATION,0.31237322515212984,"ever, they are also more computationally intensive to
281"
MOVE GENERATION,0.3133874239350913,"train and store. That said, in the domain of modeling in-
282"
MOVE GENERATION,0.3144016227180527,"dividual behavior in chess, MHR-Maia is able to perform
283"
MOVE GENERATION,0.3154158215010142,"comparatively well despite using a much smaller pa-
284"
MOVE GENERATION,0.31643002028397565,"rameter budget. Figure 2 shows that MHR-Maia matches
285"
MOVE GENERATION,0.3174442190669371,"individual model fine-tuning over a wide range of game
286"
MOVE GENERATION,0.3184584178498986,"counts. The base model is frozen for all game counts
287"
MOVE GENERATION,0.31947261663286003,"in MHR-Maia. The model has already learned the set
288"
MOVE GENERATION,0.3204868154158215,"of skills required to differentiate the players, all that is
289"
MOVE GENERATION,0.32150101419878296,"needed with very few-shot learning is to find a proper recombination of the learned skills within the
290"
MOVE GENERATION,0.3225152129817444,"new style vectors. The Transformer-based method is omitted, as it is incapable of generating moves.
291"
MOVE GENERATION,0.3235294117647059,"For Rocket League, we compare the next move prediction of our base model, with MHR-Rocket,
292"
MOVE GENERATION,0.32454361054766734,"to validate that our user-based conditioning generates better predictions. We find that, on average,
293"
MOVE GENERATION,0.3255578093306288,"MHR-Rocket increases the next move prediction from 53.1% to 56.1%.
294"
ANALYSIS OF STYLE VECTORS,0.3265720081135903,"5.3
Analysis of style vectors
295"
ANALYSIS OF STYLE VECTORS,0.3275862068965517,"In this section, we explore the consistency of our style vectors across different players and datasets.
296"
ANALYSIS OF STYLE VECTORS,0.3286004056795132,"Consistency across a single player.
To showcase intra-player consistency, we first partition 50
297"
ANALYSIS OF STYLE VECTORS,0.32961460446247465,"players’ datasets into disjoint subsets. We use 50 splits for chess and 20 for Rocket League. The
298"
ANALYSIS OF STYLE VECTORS,0.3306288032454361,"subsets are sampled across a wide range of dates, opposing players, and playing sessions. Next,
299"
ANALYSIS OF STYLE VECTORS,0.3316430020283976,"we train a style vector for every split across all players. We find that vectors corresponding to the
300"
ANALYSIS OF STYLE VECTORS,0.332657200811359,"same player will be similar to each other, and have low similarity with the other players and general
301"
ANALYSIS OF STYLE VECTORS,0.33367139959432046,"population. This is visualized in Figure 3. This suggests that our neural network is able to find
302"
ANALYSIS OF STYLE VECTORS,0.33468559837728196,"Rocket League
Chess"
ANALYSIS OF STYLE VECTORS,0.3356997971602434,"Figure 3: Cosine similarity between style vectors learned
from different partitions of the same player (red) vs across
different players (blue)."
ANALYSIS OF STYLE VECTORS,0.3367139959432049,bishop pairs
ANALYSIS OF STYLE VECTORS,0.33772819472616633,"threats 
mid game"
ANALYSIS OF STYLE VECTORS,0.33874239350912777,"mobility    
mid game"
ANALYSIS OF STYLE VECTORS,0.33975659229208927,"king danger
passed pawns"
ANALYSIS OF STYLE VECTORS,0.3407707910750507,mid game
ANALYSIS OF STYLE VECTORS,0.34178498985801214,imbalance
ANALYSIS OF STYLE VECTORS,0.34279918864097364,end game eval 0.2 0.4 0.6 0.8
ANALYSIS OF STYLE VECTORS,0.3438133874239351,"Player 1
Player 2
Player 3
Player 4
Player 5"
ANALYSIS OF STYLE VECTORS,0.3448275862068966,"Figure 4: Comparing player styles
using human-interpretable evaluation
metrics."
ANALYSIS OF STYLE VECTORS,0.345841784989858,"Rocket League
Chess"
ANALYSIS OF STYLE VECTORS,0.34685598377281945,bishop pairs
ANALYSIS OF STYLE VECTORS,0.34787018255578095,"threats 
mid game"
ANALYSIS OF STYLE VECTORS,0.3488843813387424,"mobility    
mid game"
ANALYSIS OF STYLE VECTORS,0.34989858012170383,"king danger
passed pawns"
ANALYSIS OF STYLE VECTORS,0.3509127789046653,mid game
ANALYSIS OF STYLE VECTORS,0.35192697768762676,imbalance
ANALYSIS OF STYLE VECTORS,0.35294117647058826,end game eval 0.2 0.4 0.6 0.8
ANALYSIS OF STYLE VECTORS,0.3539553752535497,"Player A
Player B
Interpolated"
ANALYSIS OF STYLE VECTORS,0.35496957403651114,"Figure 5: Cosine similarity between averaged style vectors of two players, and the learned style
vectors on their merged datasets (red) vs across the full population (blue). The style of an intermediate
player (green) is shown along with the two component players (blue and red) on the right."
ANALYSIS OF STYLE VECTORS,0.35598377281947263,"distinct tendencies for each player. To confirm, we sampled 5 random chess players, predicted their
303"
ANALYSIS OF STYLE VECTORS,0.35699797160243407,"preferred move across 217 positions, and measured a series of Stockfish evaluation metrics per player.
304"
ANALYSIS OF STYLE VECTORS,0.3580121703853955,"Figure 4 shows the distribution of these metrics for each player, demonstrating that these vectors
305"
ANALYSIS OF STYLE VECTORS,0.359026369168357,"store a wide diversity of styles.
306"
ANALYSIS OF STYLE VECTORS,0.36004056795131845,"Consistency across merged players.
To parse out whether we can generate new styles using this
307"
ANALYSIS OF STYLE VECTORS,0.36105476673427994,"information, we merged two players’ datasets together to generate a new set with the tendencies
308"
ANALYSIS OF STYLE VECTORS,0.3620689655172414,"of both players, measuring inter-player consistency. We then compared this new set of vectors to a
309"
ANALYSIS OF STYLE VECTORS,0.3630831643002028,"different set of vectors generated by simply averaging the style vectors of the player pair. As seen
310"
ANALYSIS OF STYLE VECTORS,0.3640973630831643,"in Figure 5 (left and center), vectors with the same two source players have very high similarity in
311"
ANALYSIS OF STYLE VECTORS,0.36511156186612576,"both chess and Rocket League. We then sampled a random pair in the merged dataset, created a new
312"
ANALYSIS OF STYLE VECTORS,0.3661257606490872,"player by averaging the two players’ vectors, and recorded their gameplay according to the previous
313"
ANALYSIS OF STYLE VECTORS,0.3671399594320487,"section. The results are visualized in Figure 5 (right), showing that the new player (green) has an
314"
ANALYSIS OF STYLE VECTORS,0.36815415821501013,"intermediate playing style to the source players (red, blue).
315"
SYNTHESIS OF NEW STYLES,0.3691683569979716,"5.4
Synthesis of new styles
316"
SYNTHESIS OF NEW STYLES,0.37018255578093306,"Convex combinations.
We show that interpolating between skill vectors results in a player whose
317"
SYNTHESIS OF NEW STYLES,0.3711967545638945,"level is a weighted average of the interpolated players. Here, we take 100 pairs of learned player
318"
SYNTHESIS OF NEW STYLES,0.372210953346856,"vectors, such that one item in the pair corresponds to a strong player and the other to a weaker player.
319"
SYNTHESIS OF NEW STYLES,0.37322515212981744,"We then gradually interpolate between the weak and strong player as (1 −λ)uw + λus, 0 ≤λ ≤1,
320"
SYNTHESIS OF NEW STYLES,0.3742393509127789,"where uw and us are respectively vectors representing the weak and strong player. For each value of
321"
SYNTHESIS OF NEW STYLES,0.3752535496957404,"λ we simulate 1,000 games between the interpolated vector and us, the stronger player.
322"
SYNTHESIS OF NEW STYLES,0.3762677484787018,"Figure 6 plots the win rate of the interpolated player as a function of λ for each player pair we
323"
SYNTHESIS OF NEW STYLES,0.3772819472616633,"considered. This plot demonstrates that win rate progresses in a roughly linear fashion, starting off
324"
SYNTHESIS OF NEW STYLES,0.37829614604462475,"winning infrequently against the stronger player and eventually winning roughly half the time as the
325"
SYNTHESIS OF NEW STYLES,0.3793103448275862,"interpolated player converges to the stronger player.
326"
SYNTHESIS OF NEW STYLES,0.3803245436105477,"Directly steering player style.
Finally, we directly control the playing style of a player by creating
327"
SYNTHESIS OF NEW STYLES,0.3813387423935091,"skill vectors according to the procedure described in 4. We choose players in our chess dataset with
328"
SYNTHESIS OF NEW STYLES,0.38235294117647056,"high (>2 std) bishop pair utilization, and separately players with high king danger. Figure 7 shows
329"
SYNTHESIS OF NEW STYLES,0.38336713995943206,"0.0
0.2
0.4
0.6
0.8
1.0 0.1 0.2 0.3 0.4 0.5"
SYNTHESIS OF NEW STYLES,0.3843813387423935,Win Rate
SYNTHESIS OF NEW STYLES,0.385395537525355,"Figure 6: Winrate as a weaker player is
interpolated with a stronger player as a
function of λ."
SYNTHESIS OF NEW STYLES,0.38640973630831643,"attack
bishop pair
mobility
king danger pawns eg
Style attributes 0.0 0.5 1.0 1.5 2.0"
SYNTHESIS OF NEW STYLES,0.38742393509127787,Difference in std.
SYNTHESIS OF NEW STYLES,0.38843813387423937,Steerability of game-playing style via delta style vectors
SYNTHESIS OF NEW STYLES,0.3894523326572008,"Manipulating King dager
Manipulating Bishop pair"
SYNTHESIS OF NEW STYLES,0.39046653144016225,"Figure 7: Modifying the a player’s style to-
wards a specific attribute"
SYNTHESIS OF NEW STYLES,0.39148073022312374,"the change in 2,000 randomly sampled player’s stockfish evaluations after adding the skill vector
330"
SYNTHESIS OF NEW STYLES,0.3924949290060852,"corresponding to each heuristic to their style vectors. Indeed, we see that the player’s style is steered
331"
SYNTHESIS OF NEW STYLES,0.3935091277890467,"towards the attribute in question, with model impact on other attributes.
332"
RELATED WORK,0.3945233265720081,"6
Related Work
333"
RELATED WORK,0.39553752535496955,"Stylometry and player style modeling.
Originally referring to performing author attribution via
334"
RELATED WORK,0.39655172413793105,"statistical analysis of text [Tweedie et al., 1996, Neal et al., 2017], stylometry has since come to refer
335"
RELATED WORK,0.3975659229208925,"to the general task of identifying individuals given a set of samples or actions, and has found broad
336"
RELATED WORK,0.39858012170385393,"application for tasks such as handwriting recognition [Bromley et al., 1993], speaker verification
337"
RELATED WORK,0.3995943204868154,"[Wan et al., 2018], identifying programmers from code [Caliskan-Islam et al., 2015], determining
338"
RELATED WORK,0.40060851926977686,"user age and gender from blog posts [Goswami et al., 2009], and identifying characteristics of authors
339"
RELATED WORK,0.40162271805273836,"of scientific articles [Bergsma et al., 2012]. In the context of gaming (covered in the introduction),
340"
RELATED WORK,0.4026369168356998,"stylometry is closely related to playstyle modeling, where the goal is to associate a player with a
341"
RELATED WORK,0.40365111561866124,"reference style, such as by building agents representative of different playstyles and find the closest
342"
RELATED WORK,0.40466531440162273,"behavioral match [Holmgård et al., 2014], or gathering gameplay data and applying methods such
343"
RELATED WORK,0.4056795131845842,"as clustering [Ingram et al., 2022], LDA [Gow et al., 2012], Bayesian approaches [Normoyle and
344"
RELATED WORK,0.4066937119675456,"Jensen, 2015] and sequential models [Valls-Vargas et al., 2015] to identify groups of players with
345"
RELATED WORK,0.4077079107505071,"similar styles. Unlike our work, these approaches focus on aggregate playstyles, and do not learn
346"
RELATED WORK,0.40872210953346855,"generative models that can be conditioned on an individual’s style.
347"
RELATED WORK,0.40973630831643004,"Our method for style synthesis is inspired by earlier work on vector arithmetic with embed-
348"
RELATED WORK,0.4107505070993915,"dings [Church, 2017], as well as recent work on steering multiask models with task vectors [Ilharco
349"
RELATED WORK,0.4117647058823529,"et al., 2023]. Finally, our steering method is reminiscent of Radford et al. [2016], which manipulates
350"
RELATED WORK,0.4127789046653144,"the model’s latent space to generate images containing specific attributes.
351"
RELATED WORK,0.41379310344827586,"Parameter-efficient adaptation
Approaches for efficient adaption of a pretrained model can
352"
RELATED WORK,0.4148073022312373,"be broadly grouped in two categories. Adapter based methods inject new parameters within a
353"
RELATED WORK,0.4158215010141988,"pretrained model, and only updates the newly inserted parameters while keeping the backbone
354"
RELATED WORK,0.41683569979716023,"fixed. Houlsby et al. [2019] defines an adapter as a two-layer feed-forward neural network with a
355"
RELATED WORK,0.4178498985801217,"bottleneck representation, and are inserted before the multi-head attention layer in Transformers.
356"
RELATED WORK,0.41886409736308317,"Similar approaches have been used for cross-lingual transfer [Pfeiffer et al., 2020]. Adapters have
357"
RELATED WORK,0.4198782961460446,"also been used in vision based multitask settings [Rebuffi et al., 2017]. More recently, Ansell et al.
358"
RELATED WORK,0.4208924949290061,"[2022] propose to learn sparse masks, and show that these marks are composable, enabling zero-shot
359"
RELATED WORK,0.42190669371196754,"transfer. Lastly, Hu et al. [2022] learn low-rank shifts on the original weights, and [Liu et al., 2022]
360"
RELATED WORK,0.422920892494929,"learns an elementwise multiplier of the pretrained model’s activations. Adapters have also been used
361"
RELATED WORK,0.4239350912778905,"in multitask settings. Chronopoulou et al. [2023] independently trains adapters for each task. In order
362"
RELATED WORK,0.4249492900608519,"to transfer to new tasks, the authors merge the parameters of the adapters of relevant training tasks.
363"
CONCLUSION,0.4259634888438134,"7
Conclusion
364"
CONCLUSION,0.42697768762677485,"We show that individual player behavior can be modeled at very large scale in games as different as
365"
CONCLUSION,0.4279918864097363,"chess and Rocket League. We cast this problem in the framework of multi-task learning and employ
366"
CONCLUSION,0.4290060851926978,"modular PEFT methods to learn a shared set of skills across players, modulated by a distinct style
367"
CONCLUSION,0.4300202839756592,"vector for each player. We use these style vectors to perform behavioral stylometry, analyze player
368"
CONCLUSION,0.43103448275862066,"styles, and synthesize and steer new styles.
369"
REFERENCES,0.43204868154158216,"References
370"
REFERENCES,0.4330628803245436,"A. Ansell, E. Ponti, A. Korhonen, and I. Vuli´c. Composable sparse fine-tuning for cross-lingual
371"
REFERENCES,0.4340770791075051,"transfer.
In Proceedings of the 60th Annual Meeting of the Association for Computational
372"
REFERENCES,0.43509127789046653,"Linguistics (Volume 1: Long Papers), pages 1778–1796, Dublin, Ireland, May 2022. Asso-
373"
REFERENCES,0.43610547667342797,"ciation for Computational Linguistics.
doi: 10.18653/v1/2022.acl-long.125.
URL https:
374"
REFERENCES,0.43711967545638947,"//aclanthology.org/2022.acl-long.125.
375"
REFERENCES,0.4381338742393509,"S. Bergsma, M. Post, and D. Yarowsky. Stylometric analysis of scientific articles. In Proceedings
376"
REFERENCES,0.43914807302231235,"of the 2012 Conference of the North American Chapter of the Association for Computational
377"
REFERENCES,0.44016227180527384,"Linguistics: Human Language Technologies, pages 327–337, 2012.
378"
REFERENCES,0.4411764705882353,"R.-A. Braaten. Rl-rpt - rocket league replay pre-training. https://github.com/Rolv-Arild/replay-
379"
REFERENCES,0.4421906693711968,"pretraining, 2022.
380"
REFERENCES,0.4432048681541582,"J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah. Signature verification using a"" siamese""
381"
REFERENCES,0.44421906693711966,"time delay neural network. Advances in neural information processing systems, 6, 1993.
382"
REFERENCES,0.44523326572008115,"L. Caccia, E. Ponti, L. Liu, M. Pereira, N. L. Roux, and A. Sordoni. Multi-head adapter routing for
383"
REFERENCES,0.4462474645030426,"data-efficient fine-tuning. arXiv preprint arXiv:2211.03831, 2022.
384"
REFERENCES,0.44726166328600403,"A. Caliskan-Islam, R. Harang, A. Liu, A. Narayanan, C. Voss, F. Yamaguchi, and R. Greenstadt.
385"
REFERENCES,0.4482758620689655,"De-anonymizing programmers via code stylometry. In 24th USENIX security symposium (USENIX
386"
REFERENCES,0.44929006085192696,"Security 15), pages 255–270, 2015.
387"
REFERENCES,0.45030425963488846,"CantFlyRL. Ballchasing.com. https://ballchasing.com/, 2024.
388"
REFERENCES,0.4513184584178499,"M. Carroll, R. Shah, M. K. Ho, T. Griffiths, S. Seshia, P. Abbeel, and A. Dragan. On the utility of
389"
REFERENCES,0.45233265720081134,"learning about humans for human-ai coordination. Advances in neural information processing
390"
REFERENCES,0.45334685598377283,"systems, 32, 2019.
391"
REFERENCES,0.4543610547667343,"R. Caruana. Multitask learning. Machine learning, 28:41–75, 1997.
392"
REFERENCES,0.4553752535496957,"A. Chronopoulou, M. Peters, A. Fraser, and J. Dodge.
AdapterSoup: Weight averaging to
393"
REFERENCES,0.4563894523326572,"improve generalization of pretrained language models.
In Findings of the Association for
394"
REFERENCES,0.45740365111561865,"Computational Linguistics: EACL 2023, pages 2054–2063, Dubrovnik, Croatia, May 2023.
395"
REFERENCES,0.45841784989858014,"Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.153. URL
396"
REFERENCES,0.4594320486815416,"https://aclanthology.org/2023.findings-eacl.153.
397"
REFERENCES,0.460446247464503,"K. W. Church. Word2vec. Natural Language Engineering, 23(1):155–162, 2017.
398"
REFERENCES,0.4614604462474645,"T. Duplessis. Lichess. http://lichess.org, 2021. Accessed: 2021-01-01.
399"
REFERENCES,0.46247464503042596,"L. Emery. Rlgym - the rocket league gym. https://rlgym.org/, 2021.
400"
REFERENCES,0.4634888438133874,"P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch,
401"
REFERENCES,0.4645030425963489,"and J. Tompson. Implicit behavioral cloning. In Conference on Robot Learning, pages 158–168.
402"
REFERENCES,0.46551724137931033,"PMLR, 2022.
403"
REFERENCES,0.4665314401622718,"S. Goswami, S. Sarkar, and M. Rustagi. Stylometric analysis of bloggers’ age and gender. In
404"
REFERENCES,0.46754563894523327,"Proceedings of the International AAAI Conference on Web and Social Media, volume 3, pages
405"
REFERENCES,0.4685598377281947,"214–217, 2009.
406"
REFERENCES,0.4695740365111562,"J. Gow, R. Baumgarten, P. Cairns, S. Colton, and P. Miller. Unsupervised modeling of player style
407"
REFERENCES,0.47058823529411764,"with lda. IEEE Transactions on Computational Intelligence and AI in Games, 4(3):152–166, 2012.
408"
REFERENCES,0.4716024340770791,"C. Holmgård, A. Liapis, J. Togelius, and G. N. Yannakakis. Evolving personas for player decision
409"
REFERENCES,0.4726166328600406,"modeling. In 2014 IEEE Conference on Computational Intelligence and Games, pages 1–8. IEEE,
410"
REFERENCES,0.473630831643002,"2014.
411"
REFERENCES,0.4746450304259635,"N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan,
412"
REFERENCES,0.47565922920892495,"and S. Gelly. Parameter-efficient transfer learning for NLP. In International Conference on
413"
REFERENCES,0.4766734279918864,"Machine Learning, pages 2790–2799, 2019. URL http://proceedings.mlr.press/v97/
414"
REFERENCES,0.4776876267748479,"houlsby19a/houlsby19a.pdf.
415"
REFERENCES,0.4787018255578093,"E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank
416"
REFERENCES,0.47971602434077076,"adaptation of large language models. In International Conference on Learning Representations,
417"
REFERENCES,0.48073022312373226,"2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.
418"
REFERENCES,0.4817444219066937,"J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference
419"
REFERENCES,0.4827586206896552,"on computer vision and pattern recognition, pages 7132–7141, 2018.
420"
REFERENCES,0.48377281947261663,"G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, and A. Farhadi. Editing models
421"
REFERENCES,0.4847870182555781,"with task arithmetic. In The Eleventh International Conference on Learning Representations, 2023.
422"
REFERENCES,0.48580121703853957,"URL https://openreview.net/forum?id=6t0Kwf8-jrj.
423"
REFERENCES,0.486815415821501,"B. Ingram, B. Rosman, C. van Alten, and R. Klein. Play-style identification through deep unsupervised
424"
REFERENCES,0.48782961460446245,"clustering of trajectories. In 2022 IEEE Conference on Games (CoG), pages 393–400. IEEE, 2022.
425"
REFERENCES,0.48884381338742394,"A. Jelley, Y. Cao, D. Bignell, S. Devlin, and T. Rashid. Aligning agents like large language models,
426"
REFERENCES,0.4898580121703854,"2024. URL https://openreview.net/forum?id=kQqZVayz07.
427"
REFERENCES,0.4908722109533469,"H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. Raffel. Few-shot parameter-
428"
REFERENCES,0.4918864097363083,"efficient fine-tuning is better and cheaper than in-context learning, 2022. URL https://arxiv.
429"
REFERENCES,0.49290060851926976,"org/abs/2205.05638.
430"
REFERENCES,0.49391480730223125,"T. McGrath, A. Kapishnikov, N. Tomašev, A. Pearce, M. Wattenberg, D. Hassabis, B. Kim, U. Paquet,
431"
REFERENCES,0.4949290060851927,"and V. Kramnik. Acquisition of chess knowledge in alphazero. Proceedings of the National
432"
REFERENCES,0.49594320486815413,"Academy of Sciences, 119(47):e2206625119, 2022.
433"
REFERENCES,0.4969574036511156,"R. McIlroy-Young, S. Sen, J. Kleinberg, and A. Anderson. Aligning superhuman ai with human
434"
REFERENCES,0.49797160243407707,"behavior: Chess as a model system. In Proceedings of the 26th ACM SIGKDD International
435"
REFERENCES,0.49898580121703856,"Conference on Knowledge Discovery and Data Mining, page 1677–1687, 2020.
436"
REFERENCES,0.5,"R. McIlroy-Young, Y. Wang, S. Sen, J. Kleinberg, and A. Anderson. Detecting individual decision-
437"
REFERENCES,0.5010141987829615,"making style: Exploring behavioral stylometry in chess. Advances in Neural Information Process-
438"
REFERENCES,0.5020283975659229,"ing Systems, 34:24482–24497, 2021.
439"
REFERENCES,0.5030425963488844,"R. McIlroy-Young, R. Wang, S. Sen, J. Kleinberg, and A. Anderson. Learning models of individual
440"
REFERENCES,0.5040567951318459,"behavior in chess. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
441"
REFERENCES,0.5050709939148073,"and Data Mining, page 1253–1263, 2022.
442"
REFERENCES,0.5060851926977687,"T. Neal, K. Sundararajan, A. Fatima, Y. Yan, Y. Xiang, and D. Woodard. Surveying stylometry
443"
REFERENCES,0.5070993914807302,"techniques and applications. ACM Computing Surveys (CSuR), 50(6):1–36, 2017.
444"
REFERENCES,0.5081135902636917,"A. Normoyle and S. Jensen. Bayesian clustering of player styles for multiplayer games. In Pro-
445"
REFERENCES,0.5091277890466531,"ceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,
446"
REFERENCES,0.5101419878296146,"volume 11, pages 163–169, 2015.
447"
REFERENCES,0.5111561866125761,"T. Pearce and J. Zhu. Counter-strike deathmatch with large-scale behavioural cloning. In 2022 IEEE
448"
REFERENCES,0.5121703853955375,"Conference on Games (CoG), pages 104–111. IEEE, 2022.
449"
REFERENCES,0.513184584178499,"J. Pfeiffer, I. Vuli´c, I. Gurevych, and S. Ruder. MAD-X: An Adapter-based framework for multi-task
450"
REFERENCES,0.5141987829614605,"cross-lingual transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural
451"
REFERENCES,0.5152129817444219,"Language Processing (EMNLP), pages 7654–7673, Nov. 2020. URL https://aclanthology.
452"
REFERENCES,0.5162271805273834,"org/2020.emnlp-main.617.
453"
REFERENCES,0.5172413793103449,"D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural
454"
REFERENCES,0.5182555780933062,"information processing systems, 1, 1988.
455"
REFERENCES,0.5192697768762677,"E. M. Ponti, H. O’Horan, Y. Berzak, I. Vuli´c, R. Reichart, T. Poibeau, E. Shutova, and A. Ko-
456"
REFERENCES,0.5202839756592292,"rhonen.
Modeling language variation and universals: A survey on typological linguistics
457"
REFERENCES,0.5212981744421906,"for natural language processing.
Computational Linguistics, 45(3):559–601, 2019.
URL
458"
REFERENCES,0.5223123732251521,"https://watermark.silverchair.com/coli_a_00357.pdf.
459"
REFERENCES,0.5233265720081136,"E. M. Ponti, A. Sordoni, Y. Bengio, and S. Reddy. Combining parameter-efficient modules for task-
460"
REFERENCES,0.5243407707910751,"level generalisation. In Proceedings of the 17th Conference of the European Chapter of the Associ-
461"
REFERENCES,0.5253549695740365,"ation for Computational Linguistics, pages 687–702, Dubrovnik, Croatia, May 2023. Association
462"
REFERENCES,0.526369168356998,"for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.49.
463"
REFERENCES,0.5273833671399595,"A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
464"
REFERENCES,0.5283975659229209,"generative adversarial networks. In International Conference on Learning Representations, 2016.
465"
REFERENCES,0.5294117647058824,"A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised
466"
REFERENCES,0.5304259634888439,"multitask learners. 2019.
467"
REFERENCES,0.5314401622718052,"S.-A. Rebuffi, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters.
468"
REFERENCES,0.5324543610547667,"Advances in neural information processing systems, 30, 2017.
469"
REFERENCES,0.5334685598377282,"RLBot. Rlbot. https://github.com/RLBot/RLBot, 2017.
470"
REFERENCES,0.5344827586206896,"S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf.
Transfer learning in natural language
471"
REFERENCES,0.5354969574036511,"processing.
In Proceedings of the 2019 Conference of the North American Chapter of the
472"
REFERENCES,0.5365111561866126,"Association for Computational Linguistics: Tutorials, pages 15–18, Minneapolis, Minnesota,
473"
REFERENCES,0.537525354969574,"June 2019. Association for Computational Linguistics.
doi: 10.18653/v1/N19-5004.
URL
474"
REFERENCES,0.5385395537525355,"https://aclanthology.org/N19-5004.
475"
REFERENCES,0.539553752535497,"SaltieRL. Carball. https://github.com/SaltieRL/carball, 2024.
476"
REFERENCES,0.5405679513184585,"S. Schaal. Learning from demonstration. Advances in neural information processing systems, 9,
477"
REFERENCES,0.5415821501014199,"1996.
478"
REFERENCES,0.5425963488843814,"L. Schäfer, L. Jones, A. Kanervisto, Y. Cao, T. Rashid, R. Georgescu, D. Bignell, S. Sen, A. T. Gavito,
479"
REFERENCES,0.5436105476673428,"and S. Devlin. Visual encoders for data-efficient imitation learning in modern video games, 2023.
480"
REFERENCES,0.5446247464503042,"F. J. Tweedie, S. Singh, and D. I. Holmes. Neural network applications in stylometry: The federalist
481"
REFERENCES,0.5456389452332657,"papers. Computers and the Humanities, 30:1–10, 1996.
482"
REFERENCES,0.5466531440162272,"J. Valls-Vargas, S. Ontanón, and J. Zhu. Exploring player trace segmentation for dynamic play style
483"
REFERENCES,0.5476673427991886,"prediction. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital
484"
REFERENCES,0.5486815415821501,"Entertainment, volume 11, pages 93–99, 2015.
485"
REFERENCES,0.5496957403651116,"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.
486"
REFERENCES,0.550709939148073,"Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.
487"
REFERENCES,0.5517241379310345,"03762.
488"
REFERENCES,0.552738336713996,"L. Wan, Q. Wang, A. Papir, and I. L. Moreno. Generalized end-to-end loss for speaker verification.
489"
REFERENCES,0.5537525354969574,"In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
490"
REFERENCES,0.5547667342799188,"pages 4879–4883. IEEE, 2018.
491"
REFERENCES,0.5557809330628803,"Z. Wang, Y. Tsvetkov, O. Firat, and Y. Cao. Gradient vaccine: Investigating and improving multi-
492"
REFERENCES,0.5567951318458418,"task optimization in massively multilingual models. In International Conference on Learning
493"
REFERENCES,0.5578093306288032,"Representations, 2021. URL https://openreview.net/forum?id=F1vEjWK-lH_.
494"
REFERENCES,0.5588235294117647,"Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li. On the continuity of rotation representations in neural
495"
REFERENCES,0.5598377281947262,"networks, 2020.
496"
REFERENCES,0.5608519269776876,"A
Appendix
497"
REFERENCES,0.5618661257606491,"A.1
Multi-Head Adapter Routing
498"
REFERENCES,0.5628803245436106,"In Poly, the module combination step remains coarse, as only linear combinations of the existing
499"
REFERENCES,0.563894523326572,"modules can be generated. Caccia et al. [2022] propose a more fine-grained module combination
500"
REFERENCES,0.5649087221095335,"approach, referred to as Multi-Head Routing (MHR). Similar to Multi-Head Attention [Vaswani et al.,
501"
REFERENCES,0.565922920892495,"2017], the input dimension of A (and output dimensions of B) are partitioned into h heads, where
502"
REFERENCES,0.5669371196754563,"a Poly-style procedure occurs for each head. The resulting parameters from each head are then
503"
REFERENCES,0.5679513184584178,"concatenated, recovering the full input (and output) dimensions. This makes the module combination
504"
REFERENCES,0.5689655172413793,"step piecewise linear, with a separate task-routing matrix Z learned for each head.
505"
REFERENCES,0.5699797160243407,"Formally, a MHR layer learns a 3-dimensional task-routing tensor Z ∈R|T |×|M|×h. The 2D slice
506"
REFERENCES,0.5709939148073022,"Z:,:,k ∈R|T |×|M| of the tensor Z denotes the distribution over modules for the k-th head, and
507"
REFERENCES,0.5720081135902637,"W [k] ∈R
d
h ×r the k-th partition along the rows of the matrix W ∈Rd×r. The adapter parameters
508"
REFERENCES,0.5730223123732252,"Aτ ∈Rd×r for task τ, and for each adapter layer, are computed as (similarly for Bτ):
509"
REFERENCES,0.5740365111561866,"Aτ
k =
X"
REFERENCES,0.5750507099391481,"j
αi,k · Aj[k] with Aτ
k ∈R
d
h ×r,
(MHR)"
REFERENCES,0.5760649087221096,"Aτ = concat(Aτ
1, . . . , Aτ
h),"
REFERENCES,0.577079107505071,"where αi,k = softmax(Z[τ,:,k])i. Importantly, the number of LoRA adapter parameters does
510"
REFERENCES,0.5780933062880325,"not increase with the number of heads. Only the task-routing parameters linearly increase with h for
511"
REFERENCES,0.579107505070994,"MHR vs. Poly. However, this cost is negligible as the parameter count of the routing matrices is much
512"
REFERENCES,0.5801217038539553,"smaller than for the LoRA modules themselves.
513"
REFERENCES,0.5811359026369168,"A.2
Maia Architecture/Data
514"
REFERENCES,0.5821501014198783,"Our base Maia architecture follows McIlroy-Young et al. [2022] and uses the Squeeze-and-Excitation
515"
REFERENCES,0.5831643002028397,"(S&E) Residual Network of [Hu et al., 2018]. At every residual block, channel information is
516"
REFERENCES,0.5841784989858012,"aggregated across spatial dimensions via a global pooling operation. The resulting vector is then
517"
REFERENCES,0.5851926977687627,"processed by a 2-layer MLP, with a bottleneck representation compressing the number of channels
518"
REFERENCES,0.5862068965517241,"by r. The output of this MLP is a one-dimensional vector used to scale the output of the residual
519"
REFERENCES,0.5872210953346856,"block along the channel dimension. We use 12 residual blocks containing 256 filters, and a bottleneck
520"
REFERENCES,0.5882352941176471,"compression factor of r = 8. We note that this differs from the base Maia model in McIlroy-Young
521"
REFERENCES,0.5892494929006086,"et al. [2022], which uses 64 filters and 6 residual blocks.
522"
REFERENCES,0.59026369168357,"While our dataset has a median game count of 3,479 games, many players may have as few as 10-50
523"
REFERENCES,0.5912778904665315,"games, implying some degree of data imbalance. Our evaluation of few-shot learning shows that 100
524"
REFERENCES,0.592292089249493,"games is sufficient to learn the style vector of an unseen player. However, one might still ask how
525"
REFERENCES,0.5933062880324543,"accurately such a style vector is given a very small number of games. To explore this, we first split a
526"
REFERENCES,0.5943204868154158,"player into disjoint sets of 10, 25, 50, 100, 500, and 1,000 games. We then train a style vector on
527"
REFERENCES,0.5953346855983773,"each set. As a baseline, we train a style vector on 10,000 games and track the cosine similarity of the
528"
REFERENCES,0.5963488843813387,"smaller-set style vectors relative to this baseline vector. We show the results in Figure 8.
529"
REFERENCES,0.5973630831643002,"A.3
Rocket League Architecture/Data
530"
REFERENCES,0.5983772819472617,"The 1v1 replays dataset was scraped over the course of several weeks from the Ballchasing.com API
531"
REFERENCES,0.5993914807302231,"using the Grand Champion subscription tier, though the API does have a slower free tier. This API
532"
REFERENCES,0.6004056795131846,"yields raw game replays, which are uploaded by users either manually or using a community-made
533"
REFERENCES,0.6014198782961461,"plugin for the game. The replays are in a binary format which must be parsed using community-made
534"
REFERENCES,0.6024340770791075,"projects such as Carball [SaltieRL, 2024].
535"
REFERENCES,0.603448275862069,"The Carball library allows us to convert the binary replay format to a more standard CSV format,
536"
REFERENCES,0.6044624746450304,"which we save to a Cloud binary blob storage. The data present in both is a lossy reconstruction
537"
REFERENCES,0.6054766734279919,"of game states, and requires some processing to be usable. In particular, the data is sampled at an
538"
REFERENCES,0.6064908722109533,"inconsistent rate (varying between 24hz and 27hz), contains repeated physics ticks, and is missing
539"
REFERENCES,0.6075050709939148,"action data for aerial controls (pitch, yaw, roll).
540"
REFERENCES,0.6085192697768763,"We resolve the issue of sampling rate and repeated ticks by removing repeated ticks, and doing a
541"
REFERENCES,0.6095334685598377,"time-weighted resampling and interpolation to a standard 10hz for model training, though we found
542"
REFERENCES,0.6105476673427992,"10
25
50
100
500
1000
Games 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0"
REFERENCES,0.6115618661257607,Cosine Similarity
REFERENCES,0.6125760649087221,"Figure 8: Cosine similarity of style vectors trained with varying game sizes compared to a style
vector trained with 10,000 games, run on 50 players."
REFERENCES,0.6135902636916836,"that 30hz also works well. Note that the actual game physics ticks occur at 120hz, so any value
543"
REFERENCES,0.6146044624746451,"aligned with this should work. Without these changes, the model performs extremely poorly and is
544"
REFERENCES,0.6156186612576064,"unable to navigate the arena.
545"
REFERENCES,0.6166328600405679,"We resolve the issue of missing aerial controls through the physics-based solver present in the Carball
546"
REFERENCES,0.6176470588235294,"library. The estimation of these controls is not perfect, but it is sufficient for our purposes. Some
547"
REFERENCES,0.6186612576064908,"previous community work has used inverse dynamics [Braaten, 2022] trained from rollouts of in-game
548"
REFERENCES,0.6196754563894523,"bots to solve for these actions, though we opted to not use this due to the inconsistency in replay data
549"
REFERENCES,0.6206896551724138,"sampling.
550"
REFERENCES,0.6217038539553753,"The data returned by the CSVs are fairly large, messy, and inconsistent. We apply the following
551"
REFERENCES,0.6227180527383367,"transformations to the dataframe to bring the values closer to 0:
552"
REFERENCES,0.6237322515212982,"• Divide position by 2300
553"
REFERENCES,0.6247464503042597,"• Divide linear velocity by 23000
554"
REFERENCES,0.6257606490872211,"• Divide angular velocity by 5500
555"
REFERENCES,0.6267748478701826,"• Divide boost by 255
556"
REFERENCES,0.6277890466531441,"• Encode rotation Euler angles according to Zhou et al. [2020]
557"
REFERENCES,0.6288032454361054,"Additionally, when turning the data into tokens for use in our model, we add in an extra dimension to
558"
REFERENCES,0.6298174442190669,"represent the team, and concatenate the opponent’s data points along with the position, linear and
559"
REFERENCES,0.6308316430020284,"angular velocity of the ball. We complete all of these transformations at runtime.
560"
REFERENCES,0.6318458417849898,"We also have to align the data returned by the simulators for Rocket League with the data used to
561"
REFERENCES,0.6328600405679513,"train the model, RLBot [RLBot, 2017] and RLGym [Emery, 2021]. Along with including an extra
562"
REFERENCES,0.6338742393509128,"dimension to represent the team, we apply the following transformations to all samples obtained from
563"
REFERENCES,0.6348884381338742,"the game:
564"
REFERENCES,0.6359026369168357,"• Divide position by 2300
565"
REFERENCES,0.6369168356997972,"• Divide linear velocity by 2300
566"
REFERENCES,0.6379310344827587,"• Divide angular velocity by 5.5
567"
REFERENCES,0.6389452332657201,"• Divide boost by 100
568"
REFERENCES,0.6399594320486816,"The skill distribution of the players in our dataset can be found in Figure 9.
569"
REFERENCES,0.640973630831643,"A.4
Implicit Stationarity Assumptions
570"
REFERENCES,0.6419878296146044,"Most of the existing work in chess assumes that a player remains stationary over time and across
571"
REFERENCES,0.6430020283975659,"gameplay situations. However, in reality, a player’s style may depend on the type of opponent they
572"
REFERENCES,0.6440162271805274,"are facing, which opening is used, which stage of the game they are in (opening, middle, endgame),
573"
REFERENCES,0.6450304259634888,"and so on. For instance, McIlroy-Young et al. [2021] observe that stylometry accuracy drops when
574"
REFERENCES,0.6460446247464503,bronze-1
REFERENCES,0.6470588235294118,bronze-2
REFERENCES,0.6480730223123732,bronze-3
REFERENCES,0.6490872210953347,silver-1
REFERENCES,0.6501014198782962,silver-2
REFERENCES,0.6511156186612576,silver-3
REFERENCES,0.652129817444219,gold-1
REFERENCES,0.6531440162271805,gold-2
REFERENCES,0.654158215010142,gold-3
REFERENCES,0.6551724137931034,platinum-1
REFERENCES,0.6561866125760649,platinum-2
REFERENCES,0.6572008113590264,platinum-3
REFERENCES,0.6582150101419878,diamond-1
REFERENCES,0.6592292089249493,diamond-2
REFERENCES,0.6602434077079108,diamond-3
REFERENCES,0.6612576064908722,champion-1
REFERENCES,0.6622718052738337,champion-2
REFERENCES,0.6632860040567952,champion-3
REFERENCES,0.6643002028397565,grand-champion
REFERENCES,0.665314401622718,grand-champion-1
REFERENCES,0.6663286004056795,grand-champion-2
REFERENCES,0.6673427991886409,grand-champion-3
REFERENCES,0.6683569979716024,supersonic-legend Rank 0.00 0.05 0.10 %
REFERENCES,0.6693711967545639,Rocket League Rank Distribution
REFERENCES,0.6703853955375254,Figure 9: Skill distribution of Rocket League players in our dataset.
REFERENCES,0.6713995943204868,"removing the opening (e.g., the first 15 moves) moves, suggesting that the opening has an outsized
575"
REFERENCES,0.6724137931034483,"effect on style identification. Our approach does not rely on these assumptions and can in principle
576"
REFERENCES,0.6734279918864098,"be applied to arbitrary subsets of a player’s data. For instance, one could split a player’s data into
577"
REFERENCES,0.6744421906693712,"opening, middlegame, and endgame moves and train a separate style vector for each. One could
578"
REFERENCES,0.6754563894523327,"further split the data based on which defense the opponent uses, what time of the day it is, etc..
579"
REFERENCES,0.6764705882352942,"Despite treating players holistically and avoiding any splits of their data, we are still able to capture
580"
REFERENCES,0.6774847870182555,"the peculiarities of each individual’s playing style and perform stylometry with high accuracy. This
581"
REFERENCES,0.678498985801217,"also enables us to compare our results to those of prior work, which also treats player data holistically.
582"
REFERENCES,0.6795131845841785,"A.5
Delta Style Vector Computation
583"
REFERENCES,0.6805273833671399,Algorithm 1 Style Delta Vector computation
REFERENCES,0.6815415821501014,"Input:
X : Style vectors of top-k players for attrib. a;
P : Style vectors of all players in population
Output ∆a: Style delta vector for attr. a"
REFERENCES,0.6825557809330629,"Va = mean(X, axis = ‘players’)
VP = mean(P, axis = ‘players’)
∆a = Va −VP
Returns ∆a"
REFERENCES,0.6835699797160243,"NeurIPS Paper Checklist
584"
CLAIMS,0.6845841784989858,"1. Claims
585"
CLAIMS,0.6855983772819473,"Question: Do the main claims made in the abstract and introduction accurately reflect the
586"
CLAIMS,0.6866125760649088,"paper’s contributions and scope?
587"
CLAIMS,0.6876267748478702,"Answer: [Yes]
588"
CLAIMS,0.6886409736308317,"Justification: The abstract directly summarizes the key results of the paper, which focus on
589"
CLAIMS,0.6896551724137931,"performing behavioral stylometry at scale in games (chess and Rocket League)
590"
CLAIMS,0.6906693711967545,"Guidelines:
591"
CLAIMS,0.691683569979716,"• The answer NA means that the abstract and introduction do not include the claims
592"
CLAIMS,0.6926977687626775,"made in the paper.
593"
CLAIMS,0.6937119675456389,"• The abstract and/or introduction should clearly state the claims made, including the
594"
CLAIMS,0.6947261663286004,"contributions made in the paper and important assumptions and limitations. A No or
595"
CLAIMS,0.6957403651115619,"NA answer to this question will not be perceived well by the reviewers.
596"
CLAIMS,0.6967545638945233,"• The claims made should match theoretical and experimental results, and reflect how
597"
CLAIMS,0.6977687626774848,"much the results can be expected to generalize to other settings.
598"
CLAIMS,0.6987829614604463,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
599"
CLAIMS,0.6997971602434077,"are not attained by the paper.
600"
LIMITATIONS,0.7008113590263692,"2. Limitations
601"
LIMITATIONS,0.7018255578093306,"Question: Does the paper discuss the limitations of the work performed by the authors?
602"
LIMITATIONS,0.7028397565922921,"Answer: [Yes]
603"
LIMITATIONS,0.7038539553752535,"Justification: Please see Related work and explanation of our limited style synthesis/steering
604"
LIMITATIONS,0.704868154158215,"results for Rocket League.
605"
LIMITATIONS,0.7058823529411765,"Guidelines:
606"
LIMITATIONS,0.7068965517241379,"• The answer NA means that the paper has no limitation while the answer No means that
607"
LIMITATIONS,0.7079107505070994,"the paper has limitations, but those are not discussed in the paper.
608"
LIMITATIONS,0.7089249492900609,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
609"
LIMITATIONS,0.7099391480730223,"• The paper should point out any strong assumptions and how robust the results are to
610"
LIMITATIONS,0.7109533468559838,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
611"
LIMITATIONS,0.7119675456389453,"model well-specification, asymptotic approximations only holding locally). The authors
612"
LIMITATIONS,0.7129817444219066,"should reflect on how these assumptions might be violated in practice and what the
613"
LIMITATIONS,0.7139959432048681,"implications would be.
614"
LIMITATIONS,0.7150101419878296,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
615"
LIMITATIONS,0.716024340770791,"only tested on a few datasets or with a few runs. In general, empirical results often
616"
LIMITATIONS,0.7170385395537525,"depend on implicit assumptions, which should be articulated.
617"
LIMITATIONS,0.718052738336714,"• The authors should reflect on the factors that influence the performance of the approach.
618"
LIMITATIONS,0.7190669371196755,"For example, a facial recognition algorithm may perform poorly when image resolution
619"
LIMITATIONS,0.7200811359026369,"is low or images are taken in low lighting. Or a speech-to-text system might not be
620"
LIMITATIONS,0.7210953346855984,"used reliably to provide closed captions for online lectures because it fails to handle
621"
LIMITATIONS,0.7221095334685599,"technical jargon.
622"
LIMITATIONS,0.7231237322515213,"• The authors should discuss the computational efficiency of the proposed algorithms
623"
LIMITATIONS,0.7241379310344828,"and how they scale with dataset size.
624"
LIMITATIONS,0.7251521298174443,"• If applicable, the authors should discuss possible limitations of their approach to
625"
LIMITATIONS,0.7261663286004056,"address problems of privacy and fairness.
626"
LIMITATIONS,0.7271805273833671,"• While the authors might fear that complete honesty about limitations might be used by
627"
LIMITATIONS,0.7281947261663286,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
628"
LIMITATIONS,0.72920892494929,"limitations that aren’t acknowledged in the paper. The authors should use their best
629"
LIMITATIONS,0.7302231237322515,"judgment and recognize that individual actions in favor of transparency play an impor-
630"
LIMITATIONS,0.731237322515213,"tant role in developing norms that preserve the integrity of the community. Reviewers
631"
LIMITATIONS,0.7322515212981744,"will be specifically instructed to not penalize honesty concerning limitations.
632"
THEORY ASSUMPTIONS AND PROOFS,0.7332657200811359,"3. Theory Assumptions and Proofs
633"
THEORY ASSUMPTIONS AND PROOFS,0.7342799188640974,"Question: For each theoretical result, does the paper provide the full set of assumptions and
634"
THEORY ASSUMPTIONS AND PROOFS,0.7352941176470589,"a complete (and correct) proof?
635"
THEORY ASSUMPTIONS AND PROOFS,0.7363083164300203,"Answer: [NA]
636"
THEORY ASSUMPTIONS AND PROOFS,0.7373225152129818,"Justification: we dont use proofs as a contribution
637"
THEORY ASSUMPTIONS AND PROOFS,0.7383367139959433,"Guidelines:
638"
THEORY ASSUMPTIONS AND PROOFS,0.7393509127789046,"• The answer NA means that the paper does not include theoretical results.
639"
THEORY ASSUMPTIONS AND PROOFS,0.7403651115618661,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
640"
THEORY ASSUMPTIONS AND PROOFS,0.7413793103448276,"referenced.
641"
THEORY ASSUMPTIONS AND PROOFS,0.742393509127789,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
642"
THEORY ASSUMPTIONS AND PROOFS,0.7434077079107505,"• The proofs can either appear in the main paper or the supplemental material, but if
643"
THEORY ASSUMPTIONS AND PROOFS,0.744421906693712,"they appear in the supplemental material, the authors are encouraged to provide a short
644"
THEORY ASSUMPTIONS AND PROOFS,0.7454361054766734,"proof sketch to provide intuition.
645"
THEORY ASSUMPTIONS AND PROOFS,0.7464503042596349,"• Inversely, any informal proof provided in the core of the paper should be complemented
646"
THEORY ASSUMPTIONS AND PROOFS,0.7474645030425964,"by formal proofs provided in appendix or supplemental material.
647"
THEORY ASSUMPTIONS AND PROOFS,0.7484787018255578,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7494929006085193,"4. Experimental Result Reproducibility
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7505070993914807,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7515212981744422,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7525354969574036,"of the paper (regardless of whether the code and data are provided or not)?
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7535496957403651,"Answer: [Yes]
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7545638945233266,"Justification: We provide thorough implementation details, some of which appear in the
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.755578093306288,"appendix.
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7565922920892495,"Guidelines:
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.757606490872211,"• The answer NA means that the paper does not include experiments.
657"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7586206896551724,"• If the paper includes experiments, a No answer to this question will not be perceived
658"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7596348884381339,"well by the reviewers: Making the paper reproducible is important, regardless of
659"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7606490872210954,"whether the code and data are provided or not.
660"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7616632860040567,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
661"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7626774847870182,"to make their results reproducible or verifiable.
662"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7636916835699797,"• Depending on the contribution, reproducibility can be accomplished in various ways.
663"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7647058823529411,"For example, if the contribution is a novel architecture, describing the architecture fully
664"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7657200811359026,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
665"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7667342799188641,"be necessary to either make it possible for others to replicate the model with the same
666"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7677484787018256,"dataset, or provide access to the model. In general. releasing code and data is often
667"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.768762677484787,"one good way to accomplish this, but reproducibility can also be provided via detailed
668"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7697768762677485,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
669"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.77079107505071,"of a large language model), releasing of a model checkpoint, or other means that are
670"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7718052738336714,"appropriate to the research performed.
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7728194726166329,"• While NeurIPS does not require releasing code, the conference does require all submis-
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7738336713995944,"sions to provide some reasonable avenue for reproducibility, which may depend on the
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7748478701825557,"nature of the contribution. For example
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7758620689655172,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7768762677484787,"to reproduce that algorithm.
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7778904665314401,"(b) If the contribution is primarily a new model architecture, the paper should describe
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7789046653144016,"the architecture clearly and fully.
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7799188640973631,"(c) If the contribution is a new model (e.g., a large language model), then there should
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7809330628803245,"either be a way to access this model for reproducing the results or a way to reproduce
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.781947261663286,"the model (e.g., with an open-source dataset or instructions for how to construct
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7829614604462475,"the dataset).
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.783975659229209,"(d) We recognize that reproducibility may be tricky in some cases, in which case
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7849898580121704,"authors are welcome to describe the particular way they provide for reproducibility.
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7860040567951319,"In the case of closed-source models, it may be that access to the model is limited in
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7870182555780934,"some way (e.g., to registered users), but it should be possible for other researchers
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7880324543610547,"to have some path to reproducing or verifying the results.
687"
OPEN ACCESS TO DATA AND CODE,0.7890466531440162,"5. Open access to data and code
688"
OPEN ACCESS TO DATA AND CODE,0.7900608519269777,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
689"
OPEN ACCESS TO DATA AND CODE,0.7910750507099391,"tions to faithfully reproduce the main experimental results, as described in supplemental
690"
OPEN ACCESS TO DATA AND CODE,0.7920892494929006,"material?
691"
OPEN ACCESS TO DATA AND CODE,0.7931034482758621,"Answer: [Yes]
692"
OPEN ACCESS TO DATA AND CODE,0.7941176470588235,"Justification: We will open source our data and models upon publication.
693"
OPEN ACCESS TO DATA AND CODE,0.795131845841785,"Guidelines:
694"
OPEN ACCESS TO DATA AND CODE,0.7961460446247465,"• The answer NA means that paper does not include experiments requiring code.
695"
OPEN ACCESS TO DATA AND CODE,0.7971602434077079,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
696"
OPEN ACCESS TO DATA AND CODE,0.7981744421906694,"public/guides/CodeSubmissionPolicy) for more details.
697"
OPEN ACCESS TO DATA AND CODE,0.7991886409736308,"• While we encourage the release of code and data, we understand that this might not be
698"
OPEN ACCESS TO DATA AND CODE,0.8002028397565923,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
699"
OPEN ACCESS TO DATA AND CODE,0.8012170385395537,"including code, unless this is central to the contribution (e.g., for a new open-source
700"
OPEN ACCESS TO DATA AND CODE,0.8022312373225152,"benchmark).
701"
OPEN ACCESS TO DATA AND CODE,0.8032454361054767,"• The instructions should contain the exact command and environment needed to run to
702"
OPEN ACCESS TO DATA AND CODE,0.8042596348884381,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
703"
OPEN ACCESS TO DATA AND CODE,0.8052738336713996,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
704"
OPEN ACCESS TO DATA AND CODE,0.8062880324543611,"• The authors should provide instructions on data access and preparation, including how
705"
OPEN ACCESS TO DATA AND CODE,0.8073022312373225,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
706"
OPEN ACCESS TO DATA AND CODE,0.808316430020284,"• The authors should provide scripts to reproduce all experimental results for the new
707"
OPEN ACCESS TO DATA AND CODE,0.8093306288032455,"proposed method and baselines. If only a subset of experiments are reproducible, they
708"
OPEN ACCESS TO DATA AND CODE,0.8103448275862069,"should state which ones are omitted from the script and why.
709"
OPEN ACCESS TO DATA AND CODE,0.8113590263691683,"• At submission time, to preserve anonymity, the authors should release anonymized
710"
OPEN ACCESS TO DATA AND CODE,0.8123732251521298,"versions (if applicable).
711"
OPEN ACCESS TO DATA AND CODE,0.8133874239350912,"• Providing as much information as possible in supplemental material (appended to the
712"
OPEN ACCESS TO DATA AND CODE,0.8144016227180527,"paper) is recommended, but including URLs to data and code is permitted.
713"
OPEN ACCESS TO DATA AND CODE,0.8154158215010142,"6. Experimental Setting/Details
714"
OPEN ACCESS TO DATA AND CODE,0.8164300202839757,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
715"
OPEN ACCESS TO DATA AND CODE,0.8174442190669371,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
716"
OPEN ACCESS TO DATA AND CODE,0.8184584178498986,"results?
717"
OPEN ACCESS TO DATA AND CODE,0.8194726166328601,"Answer: [Yes]
718"
OPEN ACCESS TO DATA AND CODE,0.8204868154158215,"Justification: See Appendix and main paper for full experimental details.
719"
OPEN ACCESS TO DATA AND CODE,0.821501014198783,"Guidelines:
720"
OPEN ACCESS TO DATA AND CODE,0.8225152129817445,"• The answer NA means that the paper does not include experiments.
721"
OPEN ACCESS TO DATA AND CODE,0.8235294117647058,"• The experimental setting should be presented in the core of the paper to a level of detail
722"
OPEN ACCESS TO DATA AND CODE,0.8245436105476673,"that is necessary to appreciate the results and make sense of them.
723"
OPEN ACCESS TO DATA AND CODE,0.8255578093306288,"• The full details can be provided either with the code, in appendix, or as supplemental
724"
OPEN ACCESS TO DATA AND CODE,0.8265720081135902,"material.
725"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8275862068965517,"7. Experiment Statistical Significance
726"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8286004056795132,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
727"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8296146044624746,"information about the statistical significance of the experiments?
728"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8306288032454361,"Answer: [Yes]
729"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8316430020283976,"Justification: While we do not use error bars, our methodology is properly described and
730"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8326572008113591,"clarifies the significance or our results.
731"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8336713995943205,"Guidelines:
732"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.834685598377282,"• The answer NA means that the paper does not include experiments.
733"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8356997971602435,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
734"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8367139959432048,"dence intervals, or statistical significance tests, at least for the experiments that support
735"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8377281947261663,"the main claims of the paper.
736"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8387423935091278,"• The factors of variability that the error bars are capturing should be clearly stated (for
737"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8397565922920892,"example, train/test split, initialization, random drawing of some parameter, or overall
738"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8407707910750507,"run with given experimental conditions).
739"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8417849898580122,"• The method for calculating the error bars should be explained (closed form formula,
740"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8427991886409736,"call to a library function, bootstrap, etc.)
741"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8438133874239351,"• The assumptions made should be given (e.g., Normally distributed errors).
742"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8448275862068966,"• It should be clear whether the error bar is the standard deviation or the standard error
743"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845841784989858,"of the mean.
744"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8468559837728195,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
745"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.847870182555781,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
746"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8488843813387424,"of Normality of errors is not verified.
747"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8498985801217038,"• For asymmetric distributions, the authors should be careful not to show in tables or
748"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8509127789046653,"figures symmetric error bars that would yield results that are out of range (e.g. negative
749"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8519269776876268,"error rates).
750"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8529411764705882,"• If error bars are reported in tables or plots, The authors should explain in the text how
751"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8539553752535497,"they were calculated and reference the corresponding figures or tables in the text.
752"
EXPERIMENTS COMPUTE RESOURCES,0.8549695740365112,"8. Experiments Compute Resources
753"
EXPERIMENTS COMPUTE RESOURCES,0.8559837728194726,"Question: For each experiment, does the paper provide sufficient information on the com-
754"
EXPERIMENTS COMPUTE RESOURCES,0.8569979716024341,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
755"
EXPERIMENTS COMPUTE RESOURCES,0.8580121703853956,"the experiments?
756"
EXPERIMENTS COMPUTE RESOURCES,0.859026369168357,"Answer: [Yes]
757"
EXPERIMENTS COMPUTE RESOURCES,0.8600405679513184,"Justification: We talk about the exact model parameter sizes, and use standard models that
758"
EXPERIMENTS COMPUTE RESOURCES,0.8610547667342799,"are very small. Due to the use of standard base models, information on computational
759"
EXPERIMENTS COMPUTE RESOURCES,0.8620689655172413,"resources required to train them based on token count is readily available.
760"
EXPERIMENTS COMPUTE RESOURCES,0.8630831643002028,"Guidelines:
761"
EXPERIMENTS COMPUTE RESOURCES,0.8640973630831643,"• The answer NA means that the paper does not include experiments.
762"
EXPERIMENTS COMPUTE RESOURCES,0.8651115618661258,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
763"
EXPERIMENTS COMPUTE RESOURCES,0.8661257606490872,"or cloud provider, including relevant memory and storage.
764"
EXPERIMENTS COMPUTE RESOURCES,0.8671399594320487,"• The paper should provide the amount of compute required for each of the individual
765"
EXPERIMENTS COMPUTE RESOURCES,0.8681541582150102,"experimental runs as well as estimate the total compute.
766"
EXPERIMENTS COMPUTE RESOURCES,0.8691683569979716,"• The paper should disclose whether the full research project required more compute
767"
EXPERIMENTS COMPUTE RESOURCES,0.8701825557809331,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
768"
EXPERIMENTS COMPUTE RESOURCES,0.8711967545638946,"didn’t make it into the paper).
769"
CODE OF ETHICS,0.8722109533468559,"9. Code Of Ethics
770"
CODE OF ETHICS,0.8732251521298174,"Question: Does the research conducted in the paper conform, in every respect, with the
771"
CODE OF ETHICS,0.8742393509127789,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
772"
CODE OF ETHICS,0.8752535496957403,"Answer: [Yes]
773"
CODE OF ETHICS,0.8762677484787018,"Justification: Upon reading the code of Ethics, the paper conforms to the code of ethics.
774"
CODE OF ETHICS,0.8772819472616633,"Guidelines:
775"
CODE OF ETHICS,0.8782961460446247,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
776"
CODE OF ETHICS,0.8793103448275862,"• If the authors answer No, they should explain the special circumstances that require a
777"
CODE OF ETHICS,0.8803245436105477,"deviation from the Code of Ethics.
778"
CODE OF ETHICS,0.8813387423935092,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
779"
CODE OF ETHICS,0.8823529411764706,"eration due to laws or regulations in their jurisdiction).
780"
BROADER IMPACTS,0.8833671399594321,"10. Broader Impacts
781"
BROADER IMPACTS,0.8843813387423936,"Question: Does the paper discuss both potential positive societal impacts and negative
782"
BROADER IMPACTS,0.8853955375253549,"societal impacts of the work performed?
783"
BROADER IMPACTS,0.8864097363083164,"Answer: [Yes]
784"
BROADER IMPACTS,0.8874239350912779,"Justification: This paper extends prior work on behavior cloning of individual behavior, but
785"
BROADER IMPACTS,0.8884381338742393,"it is not the first to perform such fine-grained behavior cloning or observe their societal
786"
BROADER IMPACTS,0.8894523326572008,"implications. Prior work by McIlroy-Young et al. discusses the implications of mimicking
787"
BROADER IMPACTS,0.8904665314401623,"individual behavior with high fidelity (see ""Mimetic Models: Ethical Implications of AI that
788"
BROADER IMPACTS,0.8914807302231237,"Acts Like You"" in AIES ’2022).
789"
BROADER IMPACTS,0.8924949290060852,"Guidelines:
790"
BROADER IMPACTS,0.8935091277890467,"• The answer NA means that there is no societal impact of the work performed.
791"
BROADER IMPACTS,0.8945233265720081,"• If the authors answer NA or No, they should explain why their work has no societal
792"
BROADER IMPACTS,0.8955375253549696,"impact or why the paper does not address societal impact.
793"
BROADER IMPACTS,0.896551724137931,"• Examples of negative societal impacts include potential malicious or unintended uses
794"
BROADER IMPACTS,0.8975659229208925,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
795"
BROADER IMPACTS,0.8985801217038539,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
796"
BROADER IMPACTS,0.8995943204868154,"groups), privacy considerations, and security considerations.
797"
BROADER IMPACTS,0.9006085192697769,"• The conference expects that many papers will be foundational research and not tied
798"
BROADER IMPACTS,0.9016227180527383,"to particular applications, let alone deployments. However, if there is a direct path to
799"
BROADER IMPACTS,0.9026369168356998,"any negative applications, the authors should point it out. For example, it is legitimate
800"
BROADER IMPACTS,0.9036511156186613,"to point out that an improvement in the quality of generative models could be used to
801"
BROADER IMPACTS,0.9046653144016227,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
802"
BROADER IMPACTS,0.9056795131845842,"that a generic algorithm for optimizing neural networks could enable people to train
803"
BROADER IMPACTS,0.9066937119675457,"models that generate Deepfakes faster.
804"
BROADER IMPACTS,0.907707910750507,"• The authors should consider possible harms that could arise when the technology is
805"
BROADER IMPACTS,0.9087221095334685,"being used as intended and functioning correctly, harms that could arise when the
806"
BROADER IMPACTS,0.90973630831643,"technology is being used as intended but gives incorrect results, and harms following
807"
BROADER IMPACTS,0.9107505070993914,"from (intentional or unintentional) misuse of the technology.
808"
BROADER IMPACTS,0.9117647058823529,"• If there are negative societal impacts, the authors could also discuss possible mitigation
809"
BROADER IMPACTS,0.9127789046653144,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
810"
BROADER IMPACTS,0.9137931034482759,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
811"
BROADER IMPACTS,0.9148073022312373,"feedback over time, improving the efficiency and accessibility of ML).
812"
SAFEGUARDS,0.9158215010141988,"11. Safeguards
813"
SAFEGUARDS,0.9168356997971603,"Question: Does the paper describe safeguards that have been put in place for responsible
814"
SAFEGUARDS,0.9178498985801217,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
815"
SAFEGUARDS,0.9188640973630832,"image generators, or scraped datasets)?
816"
SAFEGUARDS,0.9198782961460447,"Answer: [NA]
817"
SAFEGUARDS,0.920892494929006,"Guidelines:
818"
SAFEGUARDS,0.9219066937119675,"• The answer NA means that the paper poses no such risks.
819"
SAFEGUARDS,0.922920892494929,"• Released models that have a high risk for misuse or dual-use should be released with
820"
SAFEGUARDS,0.9239350912778904,"necessary safeguards to allow for controlled use of the model, for example by requiring
821"
SAFEGUARDS,0.9249492900608519,"that users adhere to usage guidelines or restrictions to access the model or implementing
822"
SAFEGUARDS,0.9259634888438134,"safety filters.
823"
SAFEGUARDS,0.9269776876267748,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
824"
SAFEGUARDS,0.9279918864097363,"should describe how they avoided releasing unsafe images.
825"
SAFEGUARDS,0.9290060851926978,"• We recognize that providing effective safeguards is challenging, and many papers do
826"
SAFEGUARDS,0.9300202839756593,"not require this, but we encourage authors to take this into account and make a best
827"
SAFEGUARDS,0.9310344827586207,"faith effort.
828"
LICENSES FOR EXISTING ASSETS,0.9320486815415822,"12. Licenses for existing assets
829"
LICENSES FOR EXISTING ASSETS,0.9330628803245437,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
830"
LICENSES FOR EXISTING ASSETS,0.934077079107505,"the paper, properly credited and are the license and terms of use explicitly mentioned and
831"
LICENSES FOR EXISTING ASSETS,0.9350912778904665,"properly respected?
832"
LICENSES FOR EXISTING ASSETS,0.936105476673428,"Answer: [Yes]
833"
LICENSES FOR EXISTING ASSETS,0.9371196754563894,"Justification: Papers and codebases are properly cited.
834"
LICENSES FOR EXISTING ASSETS,0.9381338742393509,"Guidelines:
835"
LICENSES FOR EXISTING ASSETS,0.9391480730223124,"• The answer NA means that the paper does not use existing assets.
836"
LICENSES FOR EXISTING ASSETS,0.9401622718052738,"• The authors should cite the original paper that produced the code package or dataset.
837"
LICENSES FOR EXISTING ASSETS,0.9411764705882353,"• The authors should state which version of the asset is used and, if possible, include a
838"
LICENSES FOR EXISTING ASSETS,0.9421906693711968,"URL.
839"
LICENSES FOR EXISTING ASSETS,0.9432048681541582,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
840"
LICENSES FOR EXISTING ASSETS,0.9442190669371197,"• For scraped data from a particular source (e.g., website), the copyright and terms of
841"
LICENSES FOR EXISTING ASSETS,0.9452332657200812,"service of that source should be provided.
842"
LICENSES FOR EXISTING ASSETS,0.9462474645030426,"• If assets are released, the license, copyright information, and terms of use in the
843"
LICENSES FOR EXISTING ASSETS,0.947261663286004,"package should be provided. For popular datasets, paperswithcode.com/datasets
844"
LICENSES FOR EXISTING ASSETS,0.9482758620689655,"has curated licenses for some datasets. Their licensing guide can help determine the
845"
LICENSES FOR EXISTING ASSETS,0.949290060851927,"license of a dataset.
846"
LICENSES FOR EXISTING ASSETS,0.9503042596348884,"• For existing datasets that are re-packaged, both the original license and the license of
847"
LICENSES FOR EXISTING ASSETS,0.9513184584178499,"the derived asset (if it has changed) should be provided.
848"
LICENSES FOR EXISTING ASSETS,0.9523326572008114,"• If this information is not available online, the authors are encouraged to reach out to
849"
LICENSES FOR EXISTING ASSETS,0.9533468559837728,"the asset’s creators.
850"
NEW ASSETS,0.9543610547667343,"13. New Assets
851"
NEW ASSETS,0.9553752535496958,"Question: Are new assets introduced in the paper well documented and is the documentation
852"
NEW ASSETS,0.9563894523326572,"provided alongside the assets?
853"
NEW ASSETS,0.9574036511156186,"Answer: [NA]
854"
NEW ASSETS,0.9584178498985801,"Guidelines:
855"
NEW ASSETS,0.9594320486815415,"• The answer NA means that the paper does not release new assets.
856"
NEW ASSETS,0.960446247464503,"• Researchers should communicate the details of the dataset/code/model as part of their
857"
NEW ASSETS,0.9614604462474645,"submissions via structured templates. This includes details about training, license,
858"
NEW ASSETS,0.962474645030426,"limitations, etc.
859"
NEW ASSETS,0.9634888438133874,"• The paper should discuss whether and how consent was obtained from people whose
860"
NEW ASSETS,0.9645030425963489,"asset is used.
861"
NEW ASSETS,0.9655172413793104,"• At submission time, remember to anonymize your assets (if applicable). You can either
862"
NEW ASSETS,0.9665314401622718,"create an anonymized URL or include an anonymized zip file.
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9675456389452333,"14. Crowdsourcing and Research with Human Subjects
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9685598377281948,"Question: For crowdsourcing experiments and research with human subjects, does the paper
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9695740365111561,"include the full text of instructions given to participants and screenshots, if applicable, as
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9705882352941176,"well as details about compensation (if any)?
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716024340770791,"Answer: [NA]
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726166328600405,"Guidelines:
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973630831643002,"• The answer NA means that the paper does not involve crowdsourcing nor research with
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9746450304259635,"human subjects.
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9756592292089249,"• Including this information in the supplemental material is fine, but if the main contribu-
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766734279918864,"tion of the paper involves human subjects, then as much detail as possible should be
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776876267748479,"included in the main paper.
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787018255578094,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797160243407708,"or other labor should be paid at least the minimum wage in the country of the data
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9807302231237323,"collector.
877"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817444219066938,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
878"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9827586206896551,"Subjects
879"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9837728194726166,"Question: Does the paper describe potential risks incurred by study participants, whether
880"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9847870182555781,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
881"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858012170385395,"approvals (or an equivalent approval/review based on the requirements of your country or
882"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.986815415821501,"institution) were obtained?
883"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878296146044625,"Answer: [NA]
884"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888438133874239,"Guidelines:
885"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898580121703854,"• The answer NA means that the paper does not involve crowdsourcing nor research with
886"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908722109533469,"human subjects.
887"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9918864097363083,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
888"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929006085192698,"may be required for any human subjects research. If you obtained IRB approval, you
889"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9939148073022313,"should clearly state this in the paper.
890"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949290060851927,"• We recognize that the procedures for this may vary significantly between institutions
891"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959432048681541,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
892"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969574036511156,"guidelines for their institution.
893"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979716024340771,"• For initial submissions, do not include any information that would break anonymity (if
894"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989858012170385,"applicable), such as the institution conducting the review.
895"
