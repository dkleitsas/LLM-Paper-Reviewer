Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0038910505836575876,"In this paper, we study Lipschitz bandit problems with batched feedback, where the
expected reward is Lipschitz and the reward observations are communicated to the
player in batches. We introduce a novel landscape-aware algorithm, called Batched
Lipschitz Narrowing (BLiN), that optimally solves this problem. Specifically,
we show that for a T-step problem with Lipschitz reward of zooming dimension
dz, our algorithm achieves theoretically optimal (up to logarithmic factors) regret
rate e
O

T"
ABSTRACT,0.007782101167315175,"dz+1
dz+2

using only O (log log T) batches. We also provide complexity
analysis for this problem. Our theoretical lower bound implies that Ω(log log T)
batches are necessary for any algorithm to achieve the optimal regret. Thus, BLiN
achieves optimal regret rate using minimal communication."
INTRODUCTION,0.011673151750972763,"1
Introduction"
INTRODUCTION,0.01556420233463035,"Multi-Armed Bandit (MAB) algorithms aim to exploit the good options while explore the decision
space. These algorithms and methodologies find successful applications in artificial intelligence
and reinforcement learning [e.g., 37]. While the classic MAB setting assumes that the rewards are
immediately observed after each arm pull, real-world data often arrives in different patterns. For
example, observations from clinical trials are often be collected in a batched fashion [34]. Another
example is from online advertising, where strategies are tested on multiple customers at the same
time [10]. In such cases, any observation-dependent decision-making should comply with this
data-arriving pattern, including MAB algorithms."
INTRODUCTION,0.019455252918287938,"In this paper, we study the Lipschitz bandit problem with batched feedback – a MAB problem where
the expected reward is Lipschitz and the reward observations are communicated to the player in
batches. In such settings, rewards are communicated only at the end of the batches, and the algorithm
can only make decisions based on information up to the previous batch. Existing Lipschitz bandit
algorithms heavily rely on timely access to the reward samples, since the partition of arm space
may change at any time. Therefore, they can not solve the batched feedback setting. To address
this difficulty, we present a novel adaptive algorithm for Lipschitz bandits with communication
constraints, named Batched Lipschitz Narrowing (BLiN). BLiN learns the landscape of the reward by
adaptively narrowing the arm set, so that regions of high reward are more frequently played. Also,
BLiN determines the data collection procedure adaptively, so that only very few data communications
are needed."
INTRODUCTION,0.023346303501945526,∗Corresponding authors
INTRODUCTION,0.027237354085603113,"The above BLiN procedure achieves optimal regret rate e
O

T"
INTRODUCTION,0.0311284046692607,"dz+1
dz+2

(dz is the zooming dimension
[26, 15]), and can be implemented in a clean and easy-to-implement form. In addition to achieving
the optimal regret rate, BLiN is also optimal in the following senses:"
INTRODUCTION,0.03501945525291829,"• BLiN’s communication complexity is optimal. BLiN only needs O(log log T) rounds of commu-
nications to achieve the optimal regret rate (Theorem 2), and no algorithm can achieve this rate
with fewer than Ω(log log T) rounds of communications (Corollary 2).
• BLiN’s time complexity is optimal (Remark 1): if the arithmetic operations and sampling are of
complexity O(1), then the time complexity of BLiN is O(T), which improve the best known time
complexity O(T log T) for Lipschitz bandit problems [15]."
SETTINGS & PRELIMINARIES,0.038910505836575876,"1.1
Settings & Preliminaries"
SETTINGS & PRELIMINARIES,0.042801556420233464,"For a Lipschitz bandit problem (with communication constraints), the arm set is a compact doubling
metric space (A, dA). The expected reward µ : A →R is 1-Lipschitz with respect to the metric dA,
that is, |µ(x1) −µ(x2)| ≤dA(x1, x2) for any x1, x2 ∈A."
SETTINGS & PRELIMINARIES,0.04669260700389105,"At time t ≤T, the learning agent pulls an arm xt ∈A that yields a reward sample yt = µ(xt) + ϵt,
where ϵt is a mean-zero independent sub-Gaussian noise. Without loss of generality, we assume that
ϵt ∼N(0, 1), since generalizations to other sub-Gaussian noises are not hard."
SETTINGS & PRELIMINARIES,0.05058365758754864,"Similar to most bandit learning problems, the agent seeks to minimize regret in the batched feedback
environment. The regret is defined as R(T) = PT
t=1 (µ∗−µ(xt)), where µ∗denotes maxx∈A µ(x).
For simplicity, we define ∆x = µ∗−µ(x) (called optimality gap of x) for all x ∈A."
SETTINGS & PRELIMINARIES,0.054474708171206226,"1.1.1
Doubling Metric Spaces and the ([0, 1]d, ∥· ∥∞) Metric Space"
SETTINGS & PRELIMINARIES,0.058365758754863814,"By the Assouad’s embedding theorem [6], the (compact) doubling metric space (A, dA) can be
embedded into a Euclidean space with some distortion of the metric; See [43] for more discussions in
a machine learning context. Due to existence of such embedding, the metric space ([0, 1]d, ∥· ∥∞),
where metric balls are hypercubes, is sufficient for the purpose of our paper. For the rest of the paper,
we will use hypercubes in algorithm design for simplicity, while our algorithmic idea generalizes to
other doubling metric spaces."
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.0622568093385214,"1.1.2
Zooming Numbers and Zooming Dimensions"
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.06614785992217899,"An important concept for bandit problems in metric spaces is the zooming number and the zooming
dimension [26, 14, 39], which we discuss now."
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.07003891050583658,"Define the set of r-optimal arms as S(r) = {x ∈A : ∆x ≤r}. For any r = 2−i, the decision space
[0, 1]d can be equally divided into 2di cubes with edge length r, which we call standard cubes (also
referred to as dyadic cubes). The r-zooming number is defined as"
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.07392996108949416,Nr := #{C : C is a standard cube with edge length r and C ⊂S(16r)}.
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.07782101167315175,"In words, Nr is the r-packing number of the set S(16r) in terms of standard cubes. The zooming
dimension is then defined as"
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.08171206225680934,"dz := min{d ≥0 : ∃a > 0, Nr ≤ar−d, ∀r = 2−i for some i ∈N}."
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.08560311284046693,"Moreover, we define the zooming constant Cz as the minimal a to make the above inequality true for
dz, Cz = min{a > 0 : Nr ≤ar−dz, ∀r = 2−i for some i ∈N}."
ZOOMING NUMBERS AND ZOOMING DIMENSIONS,0.08949416342412451,"Zooming dimension dz can be significantly smaller than ambient dimension d and can be zero. For a
simple example, consider a problem with ambient dimension d = 1 and expected reward function
µ(x) = x for 0 ≤x ≤1. Then for any r = 2−i with i ≥4, we have S(16r) = [1 −16r, 1] and
Nr = 16. Therefore, for this problem the zooming dimension equals to 0, with zooming constant
Cz = 16."
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.0933852140077821,"1.2
Batched feedback pattern and our results"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.09727626459143969,"In the batched feedback setting, for a T-step game, the player determines a grid T = {t0, · · · , tB}
adaptively, where 0 = t0 < t1 < · · · < tB = T and B ≪T. During the game, reward observations"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.10116731517509728,"are communicated to the player only at the grid points t1, · · · , tB. As a consequence, for any time
t in the j-th batch, that is, tj−1 < t ≤tj, the reward yt cannot be observed until time tj, and the
decision made at time t depends only on rewards up to time tj−1. The determination of the grid T
is adaptive in the sense that the player chooses each grid point tj ∈T based on the operations and
observations up to the previous point tj−1."
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.10505836575875487,"In this work, we present BLiN algorithm to solve Lipschitz bandits under batched feedback. During
the learning procedure, BLiN detects and eliminates the ‘bad area’ of the arm set in batches and
partition the remaining area according to an approporiate edge-length sequence. Our first theoretical
upper bound is that with simple Doubling Edge-length Sequence rm = 2−m+1, BLiN achieves
optimal regret rate e
O

T"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.10894941634241245,"dz+1
dz+2

by using O(log T) batches."
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.11284046692607004,"Theorem 1. With probability exceeding 1 −
2
T 6 , the T-step total regret R(T) of BLiN with Doubling
Edge-length Sequence (D-BLiN) satisfies"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.11673151750972763,R(T) ≲T
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.12062256809338522,"dz+1
dz+2 · (log T)
1
dz+2 ,"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.1245136186770428,"where dz is the zooming dimension of the problem instance. In addition, D-BLiN only needs no more
than O(log T) rounds of communications to achieve this regret rate. Here and henceforth, ≲only
omits constants."
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.12840466926070038,"While D-BLiN is efficient for batched Lipschitz bandits, its communication complexity is not optimal.
We then propose a new edge-length sequence, which we call Appropriate Combined Edge-length
Sequence (ACE Sequence) to improve the algorithm. The idea behind this sequence is that by
appropriately combining some batches, the algorithm can achieve better communication bound
without incurring increased regret. As we shall see, BLiN with ACE Sequence (A-BLiN) achieves
regret rate e
O

T"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.13229571984435798,"dz+1
dz+2

with only O(log log T) batches."
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.13618677042801555,"Theorem 2. With probability exceeding 1 −
2
T 6 , the T-step total regret R(T) of A-BLiN satisfies"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.14007782101167315,R(T) ≲T
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.14396887159533073,"dz+1
dz+2 · (log T)
1
dz+2 · log log T,"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.14785992217898833,"where dz is the zooming dimension of the problem instance. In addition, Algorithm 1 only needs no
more than O(log log T) rounds of communications to achieve this regret rate."
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.1517509727626459,"As a comparison, seminal works [26, 39, 15] show that the optimal regret bound for Lipschitz bandits
without communications constraints, where the reward observations are immediately observable after
each arm pull, is R(T) ≲T"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.1556420233463035,"dz+1
dz+2 · (log T)
1
dz+2 in terms of zooming dimension, and"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.15953307392996108,"R(T) ≲Rz(T) ≜inf
r0 
"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.16342412451361868,"r0T +
X"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.16731517509727625,"r=2−i,r≥r0 Nr"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.17120622568093385,"r log T 
 
(1)"
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.17509727626459143,"in terms of zooming number. Therefore, A-BLiN achieves optimal regret rate of Lipschitz bandits by
using very few batches."
BATCHED FEEDBACK PATTERN AND OUR RESULTS,0.17898832684824903,"Moreover, our lower-bound analysis shows that Ω(log log T) batches are necessary for any algorithm
to achieve the optimal regret rate. Thus, BLiN is optimal in terms of both regret and communication."
RELATED WORKS,0.1828793774319066,"1.3
Related Works"
RELATED WORKS,0.1867704280155642,"The history of the Multi-Armed Bandit (MAB) problem can date back to Thompson [42]. Solvers for
this problems include the UCB algorithms [28, 4, 7], the arm elimination method [20, 32, 36], the
ϵ-greedy strategy [7, 40], the exponential weights and mirror descent framework [8]."
RELATED WORKS,0.19066147859922178,"Recently, with the prevalence of distributed computing and large-scale field experiments, the setting
of batched feedback has captured attention [e.g., 17]. Perchet et al. [33] mainly consider batched
bandit with two arms, and a matching lower bound for static grid is proved. It was then generalized
by Gao et al. [21] to finite-armed bandit problems. In their work, the authors designed an elimination
method for finite-armed bandit problem and proved matching lower bounds for both static and
adaptive grid. Soon afterwards, Zhang et al. [46] studies inference for batched bandits. Esfandiari et
al. [19] studies batched linear bandits and batched adversarial bandits. Han et al. [22] and Ruan et"
RELATED WORKS,0.19455252918287938,"al. [35] provide solutions for batched contextual linear bandits. Li and Scarlett [29] studies batched
Gaussian process bandits. Batched dueling bandits have also been studied by [2]. Parallel to the
regret control regime, best arm identification with limited number of batches was studied in [1] and
[23]. Top-k arm identification in the collaborative learning framework is also closely related to the
batched setting, where the goal is to minimize the number of iterations (or communication steps)
between agents. In this setting, tight bounds have been obtained in the recent works [41, 24]. Yet the
problem of Lipschitz bandit with communication constraints remains unsolved."
RELATED WORKS,0.19844357976653695,"The Lipschitz bandit problem is important in its own stand. The Lipschitz bandit problem was
introduced as “continuum-armed bandits” [3], where the arm space is a compact interval. Along this
line, bandits that are Lipschitz (or Hölder) continuous have been studied. For this problem, Kleinberg
[25] proves a Ω(T 2/3) lower bound and introduced a matching algorithm. Under extra conditions
on top of Lipschitzness, regret rate of e
O(T 1/2) was achieved [9, 18]. For general (doubling) metric
spaces, the Zooming bandit algorithm [26] and the Hierarchical Optimistic Optimization (HOO)
algorithm [15] were developed. In more recent years, some attention has been focused on Lipschitz
bandit problems with certain extra structures. To name a few, Bubeck et al. [16] study Lipschitz
bandits for differentiable rewards, which enables algorithms to run without explicitly knowing the
Lipschitz constants. Wang et al. [44] studied discretization-based Lipschitz bandit algorithms from
a Gaussian process perspective. Magureanu et al. [31] derive a new concentration inequality and
study discrete Lipschitz bandits. The idea of robust mean estimators [11, 5, 13] was applied to the
Lipschitz bandit problem to cope with heavy-tail rewards, leading to the development of a near-
optimal algorithm for Lipschitz bandit with heavy-tailed rewards [30]. Lipschitz bandits where a
clustering is used to infer the underlying metric, has been studied by [45]. Contextual Lipschitz
bandits have also been studied by [39] and [27]. Yet all of the existing works for Lipschitz bandits
assume that the reward sample is immediately observed after each arm pull, and none of them solve
the Lipschitz bandit problem with communication constraints."
RELATED WORKS,0.20233463035019456,"This paper is organized as follows. In section 2, we introduce the BLiN algorithm and give a visual
illustration of the algorithm procedure. In section 3, we prove that BLiN with ACE Sequence achieves
the optimal regret rate using only O (log log T) rounds of communications. Section 4 provides
information-theoretical lower bounds for Lipschitz bandits with communication constraints, which
shows that BLiN is optimal in terms of both regret and rounds of communications. Experimental
results are presented in Section 5."
ALGORITHM,0.20622568093385213,"2
Algorithm"
ALGORITHM,0.21011673151750973,"With communication constraints, the agent’s knowledge about the environment does not accumulate
within each batch. This characteristic of the problem suggests a ‘uniform’ type algorithm – we
shall treat each step within the same batch equally. Following this intuition, in each batch, we
uniformly play the remaining arms, and then eliminate arms of low reward after the observations are
communicated. Next we describe the uniform play rule and the arm elimination rule."
ALGORITHM,0.2140077821011673,"Uniform Play Rule: At the beginning of each batch m, a collection of subsets of the arm space
Am = {Cm,1, Cm,2, · · · , Cm,|Am|} is constructed. This collection of subset Am consists of standard
cubes, and all cubes in Am have the same edge length rm. We will detail the construction of Am
when we describe the arm elimination rule. We refer to cubes in Am as active cubes of batch m."
ALGORITHM,0.2178988326848249,"During batch m, each cube in Am is played nm := 16 log T"
ALGORITHM,0.22178988326848248,"r2m
times, where T is the total time horizon.
More specifically, within each C ∈Am, arms xC,1, xC,2, · · · , xC,nm ∈C are played.2 The reward
samples {yC,1, yC,2, · · · , yC,nm}C∈Am corresponding to {xC,1, xC,2, · · · , xC,nm}C∈Am will be
collected at the end of the this batch."
ALGORITHM,0.22568093385214008,"Arm Elimination Rule: At the end of batch m, information from the arm pulls is collected, and
we estimate the reward of each C ∈Am by bµm(C) =
1
nm
Pnm
i=1 yC,i. Cubes of low estimated
rewards are then eliminated, according to the following rule: a cube C ∈Am is eliminated if
bµmax
m
−bµm(C) ≥4rm, where bµmax
m
:= maxC∈Am bµm(C). After necessary removal of “bad cubes”,"
ALGORITHM,0.22957198443579765,"each cube in Am that survives the elimination is equally partitioned into

rm
rm+1"
ALGORITHM,0.23346303501945526,"d
subcubes of edge"
ALGORITHM,0.23735408560311283,"2One can arbitrarily play xC,1, xC,2, · · · , xC,nm as long as xC,i ∈C for all i."
ALGORITHM,0.24124513618677043,"length rm+1, where rm+1 is predetermined. These cubes (of edge length rm+1) are collected to
construct Am+1, and the learning process moves on to the next batch. Appropriate rounding may be
required to ensure the ratio
rm
rm+1 is an integer. See Remark 2 for more details."
ALGORITHM,0.245136186770428,The learning process is summarized in Algorithm 1.
ALGORITHM,0.2490272373540856,Algorithm 1 Batched Lipschitz Narrowing (BLiN)
ALGORITHM,0.2529182879377432,"1: Input. Arm set A = [0, 1]d; time horizon T.
2: Initialization Number of batches B; Edge-length sequence {rm}B+1
m=1; The first grid point t0 = 0;
Equally partition A to rd
1 subcubes and define A1 as the collection of these subcubes.
3: Compute nm = 16 log T"
ALGORITHM,0.25680933852140075,"r2m
for m = 1, · · · , B + 1.
4: for m = 1, 2, · · · , B do
5:
For each cube C ∈Am, play arms xC,1, · · · xC,nm from C."
ALGORITHM,0.2607003891050584,"6:
Collect the rewards of all pulls up to tm. Compute the average payoff bµm(C) ="
ALGORITHM,0.26459143968871596,"Pnm
i=1 yC,i"
ALGORITHM,0.26848249027237353,"nm
for
each cube C ∈Am. Find bµmax
m
= maxC∈Am bµ(C).
7:
For each cube C ∈Am, eliminate C if bµmax
m
−bµm(C) > 4rm. Let A+
m be set of cubes not
eliminated.
8:
Compute tm+1 = tm + (rm/rm+1)d · |A+
m| · nm+1. If tm+1 ≥T or m = B then break.
9:
Equally partition each cube in A+
m into (rm/rm+1)d subcubes and define Am+1 as the
collection of these subcubes. /*See Remark 2 for more details on cases where (rm/rm+1)d is
not an integer.*/
10: end for
11: Cleanup: Arbitrarily play the remaining arms until all T steps are used."
ALGORITHM,0.2723735408560311,"The following theorem gives regret and communication upper bound of BLiN with Doubling Edge-
length Sequence rm = 2−m+1 (see Appendix B for proof). Note that this result implies Theorem
1.
Theorem 3. With probability exceeding 1 −
2
T 6 , the T-step total regret R(T) of BLiN with Doubling
Edge-length Sequence (D-BLiN) satisfies"
ALGORITHM,0.27626459143968873,"R(T) ≤528(log T)
1
dz+2 · T"
ALGORITHM,0.2801556420233463,"dz+1
dz+2 ,"
ALGORITHM,0.2840466926070039,"where dz is the zooming dimension of the problem instance. In addition, D-BLiN only needs no more
than log T −log log T"
ALGORITHM,0.28793774319066145,"dz+2
+ 2 rounds of communications to achieve this regret rate."
ALGORITHM,0.2918287937743191,"Although D-BLiN efficiently solves batched Lipschitz bandits, its simple partition strategy leads
to suboptimal communication complexity. Now we show that by approporiately combining some
batches, BLiN achieves the optimal communication bound, without incurring increasing regret.
Specifically, we introduce the following edge-length sequence, which we call ACE Sequence.
Definition 1. For a problem with ambient dimension d, zooming dimension dz and time horizon T,
we denote c1 =
dz+1
(d+2)(dz+2) log
T
log T and ci+1 = ηci for any i ≥1, where η = d+1−dz"
ALGORITHM,0.29571984435797666,"d+2
. Then the"
ALGORITHM,0.29961089494163423,"Appropriate Combined Edge-length Sequence {rm} is defined by rm = 2−Pm
i=1 ci for any m ≥1."
ALGORITHM,0.3035019455252918,"We show that BLiN with ACE Sequence (A-BLiN) obtains an improved communication complexity,
thus proves Theorem 2.
Theorem 4. With probability exceeding 1 −2"
ALGORITHM,0.30739299610894943,"T 6 , the T-step total regret R(T) of Algorithm 1 satisfies"
ALGORITHM,0.311284046692607,R(T) ≤
"CZ
LOG",0.3151750972762646,"128Cz
log
d+2
d+1−dz
· log log T + 8e ! · T"
"CZ
LOG",0.31906614785992216,"dz+1
dz+2 (log T)
1
dz+2 ,
(2)"
"CZ
LOG",0.3229571984435798,"where dz is the zooming dimension of the problem instance. In addition, Algorithm 1 only needs no
more than
log log T
log
d+2
d+1−dz
+ 1 rounds of communications to achieve this regret rate."
"CZ
LOG",0.32684824902723736,"The partition and elimination process of a real A-BLiN run is in Figure 1. In the i-th subgraph, the
white cubes are those remaining after the (i −1)-th batch. In this experiment, we set A = [0, 1]2,
and the optimal arm is x∗= (0.8, 0.7). Note that x∗is not eliminated during the game. More details
of this experiment are in Section 5."
"CZ
LOG",0.33073929961089493,"Figure 1: Partition and elimination process of A-BLiN. The i-th subfigure shows the pattern before
the i-th batch, which is from a real A-BLiN run on the reward function defined in Section 5. Dark gray
cubes are those eliminated in the most recent batch, while the light gray ones are those eliminated in
earlier batches. For the total time horizon T = 80000, A-BLiN needs 4 rounds of communications.
For this experiment, r1 = 1"
"CZ
LOG",0.3346303501945525,"4, r2 = 1"
"CZ
LOG",0.33852140077821014,"8, r3 =
1
16, r4 =
1
32, which is the ACE sequence (rounded as
in Remark 2) for d = 2 and dz = 0."
"CZ
LOG",0.3424124513618677,"Remark 1 (Time and space complexity). The time complexity of our algorithm is O(T), which is
better than the state of the art O(T log T) in [15]. This is because that the running time of a batch j
is of order O(lj), where lj = tj −tj−1 is number of samples in batch j. Since P"
"CZ
LOG",0.3463035019455253,"j lj = T, the time
complexity of BLiN is O(T). Besides, the space complexity of BLiN is also improved, because we do
not need to store information of cubes in previous batches."
REGRET ANALYSIS OF A-BLIN,0.35019455252918286,"3
Regret Analysis of A-BLiN"
REGRET ANALYSIS OF A-BLIN,0.3540856031128405,"In this section, we provide regret analysis for A-BLiN. The highlight of the finding is that O(log log T)"
REGRET ANALYSIS OF A-BLIN,0.35797665369649806,"batches are sufficient to achieve optimal regret rate of e
O

T"
REGRET ANALYSIS OF A-BLIN,0.36186770428015563,"dz+1
dz+2

, as summarized in Theorem 4."
REGRET ANALYSIS OF A-BLIN,0.3657587548638132,"To prove Theorem 4, we first show that the estimator bµ is concentrated to the true expected reward µ
(Lemma 1), and the optimal arm survives all eliminations with high probability (Lemma 2). In the
following analysis, we let Bstop be the total number of batches of the BLiN run.
Lemma 1. Define E := ("
REGRET ANALYSIS OF A-BLIN,0.36964980544747084,|µ(x) −bµm(C)| ≤rm + r
LOG T,0.3735408560311284,16 log T
LOG T,0.377431906614786,"nm
, ∀1 ≤m ≤Bstop −1, ∀C ∈Am, ∀x ∈C ) ."
LOG T,0.38132295719844356,"It holds that P (E) ≥1 −2T −6.
Lemma 2. Under event E, the optimal arm x∗= arg max µ(x) is not eliminated after the first
Bstop −1 batches."
LOG T,0.3852140077821012,"Based on these results, we show the cubes that survive elimination are of high reward.
Lemma 3. Under event E, for any 1 ≤m ≤Bstop, any C ∈Am and any x ∈C, ∆x satisfies"
LOG T,0.38910505836575876,"∆x ≤8rm−1.
(3)"
LOG T,0.39299610894941633,The proofs of Lemma 1-3 are in Appendix A. We are now ready to prove the theorem.
LOG T,0.3968871595330739,"Proof of Theorem 4. Let Rm denote regret of the m-th batch. Fixing any positive number B, the
total regret R(T) can be divided into two parts: R(T) = P"
LOG T,0.40077821011673154,m≤B Rm +P
LOG T,0.4046692607003891,"m>B Rm. In the following,
we bound these two parts separately and then determine B to obtain the upper bound of the total
regret. Moreover, we show A-BLiN uses only O(log log T) rounds of communications to achieve the
optimal regret."
LOG T,0.4085603112840467,"Recall that Am is set of the active cubes in the m-th batch. According to Lemma 3, for any
x ∈∪C∈AmC, we have ∆x ≤8rm−1. Let A+
m be set of cubes not eliminated in batch m. Then any
cube in A+
m−1 is a subset of S(8rm−1), and thus"
LOG T,0.41245136186770426,"|A+
m−1| ≤Nrm−1 ≤Czr−dz
m−1.
(4)"
LOG T,0.4163424124513619,"By definition, rm = rm−12−cm, so"
LOG T,0.42023346303501946,"|Am| = 2dcm|A+
m−1|.
(5)"
LOG T,0.42412451361867703,The total regret of the m-th batch is
LOG T,0.4280155642023346,"Rm =
X C∈Am nm
X"
LOG T,0.43190661478599224,"i=1
∆xC,i ≤|Am| · 16 log T"
LOG T,0.4357976653696498,"r2m
· 8rm−1
(i)
= 2dcm|A+
m−1| · 16 log T"
LOG T,0.4396887159533074,"r2m
· 8rm−1"
LOG T,0.44357976653696496,"(ii)
≤2dcm · Czr−dz+1
m−1
· 128 log T r2m"
LOG T,0.4474708171206226,"(iii)
= 2(Pm−1
i=1
ci)(dz+1)+cm(d+2) · 128Cz log T,"
LOG T,0.45136186770428016,"where (i) follows from (5), (ii) follows from (4), and (iii) follows from the definition of ACE Sequence."
LOG T,0.45525291828793774,"Define Cm = (Pm−1
i=1 ci)(dz + 1) + cm(d + 2). Since cm = cm−1 · d+1−dz"
LOG T,0.4591439688715953,"d+2
, calculation shows that
Cm = (Pm−2
i=1 ci)(dz + 1) + cm−1(d + 2) + cm−1(dz + 1 −d −2) + cm(d + 2) = Cm−1. Thus
for any m, we have Cm = C1 = c1(d + 2). Hence,"
LOG T,0.46303501945525294,Rm ≤2c1(d+2) · 128Cz log T = T
LOG T,0.4669260700389105,"dz+1
dz+2 · 128Cz(log T)
1
dz+2 .
(6)"
LOG T,0.4708171206225681,"The inequality (6) holds even if the m-th batch does not exist (where we let Rm = 0) or is not
completed. Thus we obtain the first upper bound P"
LOG T,0.47470817120622566,m≤B Rm ≤T
LOG T,0.4785992217898833,"dz+1
dz+2 · 128Cz · B(log T)
1
dz+2 .
Lemma 3 implies that any arm x played after the first B batches satisfies ∆x ≤8rB, so the total
regret after B batches is bounded by X"
LOG T,0.48249027237354086,"m>B
Rm ≤8rB · T = 8T · 2−PB
i=1 ci = 8T · 2−c1( 1−ηB 1−η ) =8T"
LOG T,0.48638132295719844,"dz+1
dz+2 (log T)
1
dz+2 · (T/ log T) ηB"
LOG T,0.490272373540856,dz+2 ≤8T
LOG T,0.49416342412451364,"dz+1
dz+2 (log T)
1
dz+2 · T ηB"
LOG T,0.4980544747081712,dz+2 .
LOG T,0.5019455252918288,"Therefore, the total regret R(T) satisfies"
LOG T,0.5058365758754864,"R(T) =
X"
LOG T,0.5097276264591439,"m≤B
Rm +
X"
LOG T,0.5136186770428015,"m>B
Rm ≤

128Cz · B + 8T ηB"
LOG T,0.5175097276264592,"dz+2

· T"
LOG T,0.5214007782101168,"dz+1
dz+2 (log T)
1
dz+2 ."
LOG T,0.5252918287937743,"This inequality holds for any positive B.
Then by choosing B∗=
log log T −log(dz+2) log 1 η
="
LOG T,0.5291828793774319,log log T −log(dz+2)
LOG T,0.5330739299610895,"log
d+2
d+1−dz
, we have
ηB∗"
LOG T,0.5369649805447471,"dz+2 =
1
log T and"
LOG T,0.5408560311284046,R(T) ≤
CZ LOG LOG T,0.5447470817120622,128Cz log log T
CZ LOG LOG T,0.5486381322957199,"log
d+2
d+1−dz
+ 8e ! · T"
CZ LOG LOG T,0.5525291828793775,"dz+1
dz+2 (log T)
1
dz+2 ."
CZ LOG LOG T,0.556420233463035,"The above analysis implies that we can achieve the optimal regret rate e
O

T"
CZ LOG LOG T,0.5603112840466926,"dz+1
dz+2

by letting the
for-loop run B∗times and finishing the remaining rounds in the Cleanup step. In other words, B∗+ 1
rounds of communications are sufficient for A-BLiN to achieve the regret bound (2)."
CZ LOG LOG T,0.5642023346303502,"Remark 2. The quantity
rm
rm+1 in line 9 of Algorithm 1 may not be integers for some m. Thus,
in practice we denote αn = ⌊Pn
i=1 ci⌋, βn = ⌈Pn
i=1 ci⌉, and define rounded ACE Sequence
{erm}m∈N by erm = 2−αk for m = 2k −1 and erm = 2−βk for m = 2k. Then the total regret can
be divided as R(T) = P"
CZ LOG LOG T,0.5680933852140078,1≤k≤B∗R2k−1 + P
CZ LOG LOG T,0.5719844357976653,1≤k≤B∗R2k + P
CZ LOG LOG T,0.5758754863813229,m>2B∗Rm. For the first part we
CZ LOG LOG T,0.5797665369649806,"have er2k−2 ≤rk−1 and er2k−1 ≥rk, while for the second part we have er2k−1"
CZ LOG LOG T,0.5836575875486382,"er2k
= 2. Therefore, by
similar argument to the proof of Theorem 4, we can bound these three parts separately, and conclude
that BLiN with rounded ACE sequence achieves the optimal regret bound e
O(T"
CZ LOG LOG T,0.5875486381322957,"dz+1
dz+2 ) by using only
O(log log T) rounds of communications. The details are in Appendix C."
LOWER BOUNDS,0.5914396887159533,"4
Lower Bounds"
LOWER BOUNDS,0.5953307392996109,"In this section, we present lower bounds for Lipschitz bandits with batched feedback, which in turn
gives communication lower bounds for all Lipschitz bandit algorithms. Our lower bounds depend on
the rounds of communications B. When B is sufficiently large, our results match the lower bound
for the vanilla Lipschitz bandit problem eΘ(Rz(T)) (Rz(T) is defined in Eq. 1). More importantly,
this dependency on B gives the minimal rounds of communications needed to achieve optimal regret
bound for all Lipschitz bandit algorithms, which is summarized in Corollary 2. Since this lower
bound matches the upper bound presented in Theorem 4, BLiN optimally solves Lipschitz bandits
with minimal communication."
PROOF OUTLINE,0.5992217898832685,"4.1
Proof Outline"
PROOF OUTLINE,0.603112840466926,"Similar to most lower bound proofs, we need to construct problem instances that are difficult to
differentiate. What’s different is that we need to carefully integrate batched feedback pattern [33] with
the Lipschitz payoff reward [39, 30]. To capture the adaptivity in grid determination, we construct
“static reference communication grids” to remove the stochasticity in grid selection [1, 21]. Below, we
first consider the static grid case, where the grid is predetermined. This static grid case will provide
intuition for the adaptive and more general case."
PROOF OUTLINE,0.6070038910505836,"The expected reward functions of these instances are constructed as follows: we choose some
‘position’ and ‘height’, such that the expected reward function obtains local maximum of the specified
‘height’ at the specified ‘position’. We will use the word ‘peak’ to refer to the local maxima. The
following theorem presents the lower bound for the static grid case.
Theorem 5 (Lower Bound for Static Grid). Consider Lipschitz bandit problems with time horizon T
such that the grid of reward communication T is static and determined before the game. If B rounds
of communications are allowed, then for any policy π, there exists a problem instance such that"
PROOF OUTLINE,0.6108949416342413,"E[RT (π)] ≥c · (log T) − 1
d+2"
PROOF OUTLINE,0.6147859922178989,"1−(
1
d+2)
B
· Rz(T) 1"
PROOF OUTLINE,0.6186770428015564,"1−(
1
d+2)
B
,"
PROOF OUTLINE,0.622568093385214,"where c > 0 is a numerical constant independent of B, T, π and T , Rz(T) is defined in (1), and d is
the dimension of the arm space."
PROOF OUTLINE,0.6264591439688716,"To prove Theorem 5, we first show that for any k > 1 there exists an instance such that E[RT (π)] ≥
tk t"
PROOF OUTLINE,0.6303501945525292,"1
d+2
k−1
. Fixing k > 1, we let rk =
1 t"
PROOF OUTLINE,0.6342412451361867,"1
d+2
k−1
and Mk := tk−1r2
k =
1
rd
k . Then we construct a set of problem"
PROOF OUTLINE,0.6381322957198443,"instances Ik = {Ik,1, · · · , Ik,Mk}, such that the gap between the highest peak and the second highest
peak is about rk for every instance in Ik."
PROOF OUTLINE,0.642023346303502,"Based on this construction, we prove that no algorithm can distinguish instances in Ik from one
another in the first (k −1) batches, so the worst-case regret is at least rktk, which gives the inequality
we need. For the first batch (0, t1], we can easily construct a set of instances where the worst-case
regret is at least t1, since no information is available during this time. Thus, there exists a problem"
PROOF OUTLINE,0.6459143968871596,instance such that E[RT (π)] ≳max (
PROOF OUTLINE,0.6498054474708171,"t1,
t2 t"
PROOF OUTLINE,0.6536964980544747,"1
d+2
1
, · · · ,
tB t"
PROOF OUTLINE,0.6575875486381323,"1
d+2
B−1 )"
PROOF OUTLINE,0.6614785992217899,". Since 0 < t1 < · · · < tB = T, the"
PROOF OUTLINE,0.6653696498054474,inequality in Theorem 5 follows.
PROOF OUTLINE,0.669260700389105,"As a result of Theorem 5, we can derive the minimum rounds of communications needed to achieve
optimal regret bound for Lipschitz bandit problem, which is stated in Corollary 1.
Corollary 1. Any Lipschitz bandit algorithm needs Ω(log log T) rounds of communications to achieve
the optimal regret rate, for the case that the times of reward communication are predetermined and
static."
PROOF OUTLINE,0.6731517509727627,The detailed proof of Theorem 5 and Corollary 1 are deferred to Appendix D and E.
COMMUNICATION LOWER BOUND FOR BLIN,0.6770428015564203,"4.2
Communication Lower Bound for BLiN"
COMMUNICATION LOWER BOUND FOR BLIN,0.6809338521400778,"So far we have derived lower bounds for the static grid case. Yet there is a gap between the static and
the adaptive case. We will close this gap in the following Theorem."
COMMUNICATION LOWER BOUND FOR BLIN,0.6848249027237354,"Theorem 6 (Lower Bound for Adaptive Grid). Consider Lipschitz bandit problems with time horizon
T such that the grid of reward communication T is adaptively determined by the player. If B rounds
of communications are allowed, then for any policy π, there exists an instance such that"
COMMUNICATION LOWER BOUND FOR BLIN,0.688715953307393,E [RT (π)] ≥c 1
COMMUNICATION LOWER BOUND FOR BLIN,0.6926070038910506,"B2 (log T) − 1
d+2"
COMMUNICATION LOWER BOUND FOR BLIN,0.6964980544747081,"1−(
1
d+2)
B
Rz(T) 1"
COMMUNICATION LOWER BOUND FOR BLIN,0.7003891050583657,"1−(
1
d+2)
B
,"
COMMUNICATION LOWER BOUND FOR BLIN,0.7042801556420234,"where c > 0 is a numerical constant independent of B, T, π and T , Rz(T) is defined in (1), and d is
the dimension of the arm space."
COMMUNICATION LOWER BOUND FOR BLIN,0.708171206225681,"To prove Theorem 6, we consider a reference static grid Tr = {T0, T1, · · · , TB}, where Tj = T 1−εj 1−εB"
COMMUNICATION LOWER BOUND FOR BLIN,0.7120622568093385,"for ε =
1
d+2. We construct a series of ‘worlds’, denoted by I1, · · · , IB. Each world is a set of
problem instances, and each problem instance in world Ij is defined by peak location set Uj and
basic height rj, where the sets Uj and quantities rj for 1 ≤j ≤B are presented in Appendix F."
COMMUNICATION LOWER BOUND FOR BLIN,0.7159533073929961,"Based on these constructions, we first prove that for any adaptive grid and policy, there exists an
index j such that the event Aj = {tj−1 < Tj−1, tj ≥Tj} happens with sufficiently high probability
in world Ij. Then similar to Theorem 5, we prove that in world Ij there exists a set of problem
instances that is difficult to differentiate in the first j −1 batches. In addition, event Aj implies that
tj ≥Tj, so the worst-case regret is at least rjTj, which gives the lower bound we need."
COMMUNICATION LOWER BOUND FOR BLIN,0.7198443579766537,"The proof of Theorem 6 is deferred to Appendix F. Similar to Corollary 1, we can prove that at least
Ω(log log T) rounds of communications are needed to achieve optimal regret bound. This result is
formally summarized in Corollary 2.
Corollary 2. Any Lipschitz bandit algorithm3 needs Ω(log log T) rounds of communications to
achieve the optimal regret rate."
EXPERIMENTS,0.7237354085603113,"5
Experiments"
EXPERIMENTS,0.7276264591439688,"In this section, we present numerical studies of A-BLiN. In the experiments, we use the arm space
A = [0, 1]2 and the expected reward function µ(x) = 1 −1"
EXPERIMENTS,0.7315175097276264,"2∥x −x1∥2 −
3
10∥x −x2∥2, where
x1 = (0.8, 0.7) and x2 = (0.1, 0.1). The landscape of µ and the resulting partition is shown in
Figure 2(a). As can be seen, the partition is finer in the area closer to the optimal arm x∗= (0.8, 0.7)."
EXPERIMENTS,0.7354085603112841,"(a) Partition
(b) Regret"
EXPERIMENTS,0.7392996108949417,"Figure 2: Resulting partition and regret of A-BLiN. In Figure 2(a), we show the resulting partition
of A-BLiN. The background color denotes the true value of expected reward µ, and blue means
high values. The figure shows that the partition is finer for larger values of µ. In Figure 2(b), we
show accumulated regret of A-BLiN and zooming algorithm [26]. In the figure, different background
colors represent different batches of A-BLiN. For the total time horizon T = 80000, A-BLiN needs 4
rounds of communications."
EXPERIMENTS,0.7431906614785992,"We let the time horizon T = 80000, and report the accumulated regret in Figure 2(b). The regret
curve is sublinear, which agrees with the regret bound (2). Besides, different background colors
in Figure 2(b) represent different batches. For the total time horizon T = 80000, A-BLiN only
needs 4 rounds of communications. We also present regret curve of zooming algorithm [26] for"
INCLUDING ALGORITHMS OF WHICH THE NUMBER OF BATCHES CAN BE DETERMINED ADAPTIVELY,0.7470817120622568,3including algorithms of which the number of batches can be determined adaptively
INCLUDING ALGORITHMS OF WHICH THE NUMBER OF BATCHES CAN BE DETERMINED ADAPTIVELY,0.7509727626459144,"comparison. Different from zooming algorithm, regret curve of A-BLiN is approximately piecewise
linear, which is because the strategy of BLiN does not change within each batch. Results of more
repeated experiments are in Appendix G, as well as experimental results of D-BLiN. Our code is
available at https://github.com/FengYasong-fifol/Batched-Lipschitz-Narrowing."
CONCLUSION,0.754863813229572,"6
Conclusion"
CONCLUSION,0.7587548638132295,"In this paper, we study Lipschitz bandits with communication constraints, and propose the BLiN
algorithm as a solution. We prove that BLiN only need O (log log T) rounds of communications
to achieve the optimal regret rate of best previous Lipschitz bandit algorithms [26, 14] that need
T batches. This improvement in number of the batches significantly saves data communication
costs. We also provide complexity analysis for this problem. We show that Ω(log log T) rounds
of communications are necessary for any algorithm to optimally solve Lipschitz bandit problems.
Hence, BLiN algorithm is optimal."
CONCLUSION,0.7626459143968871,Acknowledgments and Disclosure of Funding
CONCLUSION,0.7665369649805448,"This work was partly supported by the National Key Research and Development Program of China
(2020AAA0107600)."
REFERENCES,0.7704280155642024,References
REFERENCES,0.77431906614786,"[1] Arpit Agarwal, Shivani Agarwal, Sepehr Assadi, and Sanjeev Khanna. Learning with limited
rounds of adaptivity: coin tossing, multi-armed bandits, and ranking from pairwise comparisons.
In Conference on Learning Theory, pages 39–75. PMLR, 2017."
REFERENCES,0.7782101167315175,"[2] Arpit Agarwal, Rohan Ghuge, and Viswanath Nagarajan. Batched dueling bandits. arXiv
preprint arXiv:2202.10660, 2022."
REFERENCES,0.7821011673151751,"[3] Rajeev Agrawal.
The continuum-armed bandit problem.
SIAM Journal on Control and
Optimization, 33(6):1926–1951, 1995."
REFERENCES,0.7859922178988327,"[4] Rajeev Agrawal. Sample mean based index policies by O(log n) regret for the multi-armed
bandit problem. Advances in Applied Probability, 27(4):1054–1078, 1995."
REFERENCES,0.7898832684824902,"[5] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the
frequency moments. Journal of Computer and System Sciences, 58(1):137–147, 1999."
REFERENCES,0.7937743190661478,"[6] Patrice Assouad. Plongements Lipschitziens dans Rn. Bulletin de la Société Mathématique de
France, 111:429–448, 1983."
REFERENCES,0.7976653696498055,"[7] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine learning, 47(2):235–256, 2002."
REFERENCES,0.8015564202334631,"[8] Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic
multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002."
REFERENCES,0.8054474708171206,"[9] Peter Auer, Ronald Ortner, and Csaba Szepesvári. Improved rates for the stochastic continuum-
armed bandit problem. In Conference on Computational Learning Theory, pages 454–468.
Springer, 2007."
REFERENCES,0.8093385214007782,"[10] Dimitris Bertsimas and Adam J Mersereau. A learning approach for interactive marketing to a
customer segment. Operations Research, 55(6):1120–1135, 2007."
REFERENCES,0.8132295719844358,"[11] Peter J. Bickel. On some robust estimates of location. The Annals of Mathematical Statistics,
pages 847–858, 1965."
REFERENCES,0.8171206225680934,"[12] Jean Bretagnolle and Catherine Huber. Estimation des densités: risque minimax. Séminaire de
probabilités de Strasbourg, 12:342–363, 1978."
REFERENCES,0.8210116731517509,"[13] Sébastien Bubeck, Nicolo Cesa-Bianchi, and Gábor Lugosi. Bandits with heavy tail. IEEE
Transactions on Information Theory, 59(11):7711–7717, 2013."
REFERENCES,0.8249027237354085,"[14] Sébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. Online optimization in
X-armed bandits. Advances in Neural Information Processing Systems, 22:201–208, 2009."
REFERENCES,0.8287937743190662,"[15] Sébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. X-armed bandits. Journal
of Machine Learning Research, 12(5):1655–1695, 2011."
REFERENCES,0.8326848249027238,"[16] Sébastien Bubeck, Gilles Stoltz, and Jia Yuan Yu. Lipschitz bandits without the Lipschitz
constant. In International Conference on Algorithmic Learning Theory, pages 144–158. Springer,
2011."
REFERENCES,0.8365758754863813,"[17] Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and
other adaptive adversaries. Advances in Neural Information Processing Systems, 26:1160–1168,
2013."
REFERENCES,0.8404669260700389,"[18] Eric W. Cope. Regret and convergence bounds for a class of continuum-armed bandit problems.
IEEE Transactions on Automatic Control, 54(6):1243–1253, 2009."
REFERENCES,0.8443579766536965,"[19] Hossein Esfandiari, Amin Karbasi, Abbas Mehrabian, and Vahab Mirrokni. Regret bounds for
batched bandits. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pages 7340–7348, 2021."
REFERENCES,0.8482490272373541,"[20] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and
stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal
of machine learning research, 7(6):1079–1105, 2006."
REFERENCES,0.8521400778210116,"[21] Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits
problem. Advances in Neural Information Processing Systems, 32:503–513, 2019."
REFERENCES,0.8560311284046692,"[22] Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W Glynn, and Yinyu
Ye.
Sequential batch learning in finite-action linear contextual bandits.
arXiv preprint
arXiv:2004.06321, 2020."
REFERENCES,0.8599221789883269,"[23] Kwang-Sung Jun, Kevin Jamieson, Robert Nowak, and Xiaojin Zhu. Top arm identification
in multi-armed bandits with batch arm pulls. In Artificial Intelligence and Statistics, pages
139–148. PMLR, 2016."
REFERENCES,0.8638132295719845,"[24] Nikolai Karpov, Qin Zhang, and Yuan Zhou. Collaborative top distribution identifications with
limited interaction. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science
(FOCS), pages 160–171. IEEE, 2020."
REFERENCES,0.867704280155642,"[25] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in
Neural Information Processing Systems, 18:697–704, 2005."
REFERENCES,0.8715953307392996,"[26] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In
Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 681–690,
2008."
REFERENCES,0.8754863813229572,"[27] Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual
bandits with continuous actions: smoothing, zooming, and adapting. In Conference on Learning
Theory, pages 2025–2027. PMLR, 2019."
REFERENCES,0.8793774319066148,"[28] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Ad-
vances in Applied Mathematics, 6(1):4–22, 1985."
REFERENCES,0.8832684824902723,"[29] Zihan Li and Jonathan Scarlett. Gaussian process bandit optimization with few batches. In
International Conference on Artificial Intelligence and Statistics, pages 92–107. PMLR, 2022."
REFERENCES,0.8871595330739299,"[30] Shiyin Lu, Guanghui Wang, Yao Hu, and Lijun Zhang. Optimal algorithms for Lipschitz bandits
with heavy-tailed rewards. In International Conference on Machine Learning, pages 4154–4163,
2019."
REFERENCES,0.8910505836575876,"[31] Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower
bound and optimal algorithms. In Conference on Learning Theory, pages 975–999. PMLR,
2014."
REFERENCES,0.8949416342412452,"[32] Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The
Annals of Statistics, 41(2):693–721, 2013."
REFERENCES,0.8988326848249028,"[33] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit
problems. The Annals of Statistics, 44(2):660–681, 2016."
REFERENCES,0.9027237354085603,"[34] Stuart J. Pocock.
Group sequential methods in the design and analysis of clinical trials.
Biometrika, 64(2):191–199, 1977."
REFERENCES,0.9066147859922179,"[35] Yufei Ruan, Jiaqi Yang, and Yuan Zhou. Linear bandits with limited adaptivity and learning
distributional optimal design. In Proceedings of the 53rd Annual ACM SIGACT Symposium on
Theory of Computing, pages 74–87, 2021."
REFERENCES,0.9105058365758755,"[36] Sudeep Salgia, Sattar Vakili, and Qing Zhao. A domain-shrinking based bayesian optimization
algorithm with order-optimal regret performance. In Advances in Neural Information Processing
Systems, volume 34, pages 28836–28847, 2021."
REFERENCES,0.914396887159533,"[37] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016."
REFERENCES,0.9182879377431906,"[38] Sean Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, and Christina Yu. Adaptive dis-
cretization for model-based reinforcement learning. Advances in Neural Information Processing
Systems, 33:3858–3871, 2020."
REFERENCES,0.9221789883268483,"[39] Aleksandrs Slivkins. Contextual bandits with similarity information. Journal of Machine
Learning Research, 15(1):2533–2568, 2014."
REFERENCES,0.9260700389105059,"[40] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press,
2018."
REFERENCES,0.9299610894941635,"[41] Chao Tao, Qin Zhang, and Yuan Zhou. Collaborative learning with limited interaction: tight
bounds for distributed exploration in multi-armed bandits. In 2019 IEEE 60th Annual Symposium
on Foundations of Computer Science (FOCS), pages 126–146. IEEE, 2019."
REFERENCES,0.933852140077821,"[42] William R Thompson. On the likelihood that one unknown probability exceeds another in view
of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933."
REFERENCES,0.9377431906614786,"[43] Tianyu Wang and Cynthia Rudin. Bandits for BMO functions. In International Conference on
Machine Learning, pages 9996–10006. PMLR, 2020."
REFERENCES,0.9416342412451362,"[44] Tianyu Wang, Weicheng Ye, Dawei Geng, and Cynthia Rudin. Towards practical lipschitz
bandits. In Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference,
FODS ’20, page 129–138, New York, NY, USA, 2020. Association for Computing Machinery."
REFERENCES,0.9455252918287937,"[45] Nirandika Wanigasekara and Christina Yu. Nonparametric contextual bandits in an unknown
metric space. In Advances in Neural Information Processing Systems, volume 32, pages
14684–14694, 2019."
REFERENCES,0.9494163424124513,"[46] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in
Neural Information Processing Systems, 33:9818–9829, 2020."
REFERENCES,0.953307392996109,Checklist
REFERENCES,0.9571984435797666,1. For all authors...
REFERENCES,0.9610894941634242,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]"
REFERENCES,0.9649805447470817,(c) Did you discuss any potential negative societal impacts of your work? [N/A]
REFERENCES,0.9688715953307393,(d) Have you read the ethics review guidelines and ensured that your paper conforms to them?
REFERENCES,0.9727626459143969,"[Yes]
2. If you are including theoretical results..."
REFERENCES,0.9766536964980544,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments..."
REFERENCES,0.980544747081712,"(a) Did you include the code, data, and instructions needed to reproduce the main experimental
results (either in the supplemental material or as a URL)? [Yes]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were
chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experiments
multiple times)? [Yes]
(d) Did you include the total amount of compute and the type of resources used (e.g., type of
GPUs, internal cluster, or cloud provider)? [N/A]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9844357976653697,"(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9883268482490273,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9922178988326849,"(a) Did you include the full text of instructions given to participants and screenshots, if applicable?"
REFERENCES,0.9961089494163424,"[N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review Board
(IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on
participant compensation? [N/A]"
