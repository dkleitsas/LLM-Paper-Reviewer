Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0016260162601626016,"Foundation models of time series have not been fully developed due to the limited
availability of time series corpora and the underexploration of scalable pre-training.
Based on the similar sequential formulation of time series and natural language,
increasing research demonstrates the feasibility of leveraging large language mod-
els (LLM) for time series. Nevertheless, the inherent autoregressive property and
decoder-only architecture of LLMs have not been fully considered, resulting in
insufficient utilization of LLM abilities. To fully revitalize the general-purpose
token transition and multi-step generation capability of large language models, we
propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters,
which projects time series into the embedding space of language tokens and autore-
gressively generates future predictions with arbitrary lengths. Compatible with any
decoder-only LLMs, the consequent forecaster exhibits the flexibility of the look-
back length and scalability with larger LLMs. Further, we formulate time series as
prompts, extending the context for prediction beyond the lookback window, termed
in-context forecasting. By introducing LLM-embedded textual timestamps, Auto-
Times can utilize chronological information to align multivariate time series. Em-
pirically, AutoTimes achieves state-of-the-art with 0.1% trainable parameters and
over 5× training/inference speedup compared to advanced LLM-based forecasters.
Code is available at this repository: https://github.com/thuml/AutoTimes."
INTRODUCTION,0.0032520325203252032,"1
Introduction"
INTRODUCTION,0.004878048780487805,"Time series forecasting is of crucial demand in real-world applications, covering various domains
including climate, economics, energy, operations, etc. [22, 43]. The growing challenges of general-
purpose forecasting, where one model is versatile to handle variable-length scenarios [24, 41] and the
prediction is necessarily instructed by auxiliary information in other modalities [40, 44], underscore
the demand for foundation models [3] of time series, which are aimed to exhibit enhanced capabilities,
including multi-step generation, zero-shot generalization [49, 13], in-context learning and multimodal
utilization [15], thereby expanding the scope of time series forecasting to a wider range of situations."
INTRODUCTION,0.0065040650406504065,"Nevertheless, the development of time series foundation models has been hampered by the limited
availability of large-scale pre-training datasets and the technical uncertainty of scalable backbones.
In contrast, rapid progress is witnessed in large language models (LLM), facilitated by extensive
text corpora [50], available pre-trained models [36], and well-established adaptation techniques [14].
Notably, language and time series share basic commonalities in sequence modeling and generation
by learned token transitions, presenting opportunities to adopt off-the-shelf LLMs for time series."
INTRODUCTION,0.008130081300813009,∗Equal Contribution
INTRODUCTION,0.00975609756097561,Autoregressive
INTRODUCTION,0.011382113821138212,"2’
3’
4’ 1
2
3 4’ 1
2
3"
INTRODUCTION,0.013008130081300813,Non-Autoregressive
INTRODUCTION,0.014634146341463415,"1
2
3
+
Concat"
INTRODUCTION,0.016260162601626018,"(a) Forecasting Approach
(b) Prompting Mechanisms 5’
5’ 4’"
INTRODUCTION,0.01788617886178862,Flatten + Project LLM
INTRODUCTION,0.01951219512195122,Next Token
INTRODUCTION,0.02113821138211382,Prediction
INTRODUCTION,0.022764227642276424,"LLM
Lookback Tokens"
INTRODUCTION,0.024390243902439025,Predicted Tokens
INTRODUCTION,0.026016260162601626,Language Tokens …
INTRODUCTION,0.027642276422764227,Time Series
INTRODUCTION,0.02926829268292683,Sequence-wise Prompting
INTRODUCTION,0.030894308943089432,"4’
5’
6’ LLM"
INTRODUCTION,0.032520325203252036,"Language as Prompts                     
Autoregressive (Ours)
Time Series as Prompts (Ours)"
INTRODUCTION,0.03414634146341464,"1
2
3
+
Concat LLM"
INTRODUCTION,0.03577235772357724,Language as Prompts (Previous)
INTRODUCTION,0.03739837398373984,"1
2
3
+
Concat LLM"
INTRODUCTION,0.03902439024390244,Non-Autoregressive
INTRODUCTION,0.04065040650406504,"LLM
LLM"
INTRODUCTION,0.04227642276422764,"Flatten + Project
Next Token"
INTRODUCTION,0.04390243902439024,Prediction …
INTRODUCTION,0.04552845528455285,Time Series
INTRODUCTION,0.04715447154471545,Natural Language
INTRODUCTION,0.04878048780487805,Predicted Series
INTRODUCTION,0.05040650406504065,Lookback Series
INTRODUCTION,0.05203252032520325,Relevant Series
INTRODUCTION,0.05365853658536585,"Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally
flattened representation of lookback series, while large language models inherently predict the next
tokens by autoregression [47]. (b) Previous methods adopt language prompts that may lead to the
modality disparity, while we find time series can be self-prompted, termed in-context forecasting."
INTRODUCTION,0.055284552845528454,"Despite recent studies on large language models for time series (LLM4TS) achieving performance
breakthroughs in current forecasting benchmarks [15], the mechanism by which LLMs are aligned to
the time series modality still remains obscure. The pilot work, FPT [49] leverages LLMs as generic
sequential representation extractors for time series, influencing subsequent LLM4TS methodologies.
As depicted in Figure 1 (a), the non-autoregressive approach, where time series are segmented into
tokens, flattens and projects all lookback tokens for the prediction in a single step. However, it causes
inconsistencies in both model structure and generative approach of LLMs: decoder-only models for
autoregressive generation are converted to encoder-only and non-autoregressive forecasters."
INTRODUCTION,0.056910569105691054,"Given that prior studies [9, 38] reveal that generalization performance of LLMs is largely derived
from the decoder-only structure trained autoregressively, talents of LLMs may not be fully exhibited.
It is also supported by the recent rethinking of previous LLM4TS methods [35], which generally lack
the maintenance of autoregression, the essential characteristic of both large language models and
statistical forecasters [5, 39]. Therefore, autoregressive LLM4TS methods are underexplored, which
can potentially unlock multi-step generation like LLMs, presenting one model for arbitrary lengths."
INTRODUCTION,0.05853658536585366,"Motivated by the reflections, we propose AutoTimes to adapt LLMs as time series forecasters, which
retrieves the consistency of autoregression with revitalized LLM capabilities to produce foundation
models for time series forecasting. Technically, we independently embed time series segments into
the latent space of language models by the consistent training objective: next token prediction [2]. To
fully leverage the inherent token transitions of LLMs and reduce the training cost, we freeze the LLM
and establish token embedding and projection for time series, which only account for up to 0.1% total
parameters. The consequent forecaster adopts autoregressive inference like LLMs, which is no longer
constrained to specific lookback/forecast lengths. Going beyond conventional time series forecasting,
we propose in-context forecasting as shown in Figure 1, where time series can be self-prompted by
relevant contexts. We further adopt LLM-embedded timestamps as the position embedding to utilize
chronological information and align multiple variates. Our contributions are summarized as follows:"
INTRODUCTION,0.06016260162601626,"• By refining the inconsistency of non-autoregressive LLM4TS methods, we propose to inherit
the autoregressive property of LLMs, which frees our method from training respectively on
the lookback length and allows arbitrary-length predictions with chronological awareness."
INTRODUCTION,0.061788617886178863,"• We present AutoTimes, a simple but effective approach to acquire LLM-based forecasters by
lightweight adaptation, which utilizes the inherent token transition as the future extrapolation
of time series. Further, we propose in-context forecasting, which renovates the conventional
paradigm by introducing relevant time series prompts to enhance forecasting."
INTRODUCTION,0.06341463414634146,"• Compared with state-of-the-art methods, our repurposed forecaster achieves superior perfor-
mance while saving over 80% training and inference time, and further exhibits zero-shot
generalizability, in-context forecasting, and scaling behavior empowered by LLMs."
RELATED WORK,0.06504065040650407,"2
Related Work"
AUTOREGRESSIVE MODELS,0.06666666666666667,"2.1
Autoregressive Models"
AUTOREGRESSIVE MODELS,0.06829268292682927,"Autoregression is an essential concept of both language modeling and time series forecasting. De-
spite prevalent deep forecasters [10, 26, 42, 48] adopt a non-autoregressive approach without the"
AUTOREGRESSIVE MODELS,0.06991869918699187,"requirement of iterative forecasting, autoregression, the absent exploration in deep forecasters, serves
as the fundamental principle of statistical methods, which enables variable-length predictions. The
most well-known model, ARIMA [4] is developed by incorporating differencing on AR and MA
models, which are both autoregressive models with learned time-invariant transition from the past to
the future. Incorporated with decomposition and pre-defined transitions, exponential smoothing [39]
and state space models (SSM) [12, 23] also take the same autoregressive formulation."
AUTOREGRESSIVE MODELS,0.07154471544715447,"Autoregressive language models [27, 31] are trained with fine-grained supervision, where the gen-
erated token of each position is independently supervised. Consequently, they are not constrained
by specific input/output lengths and excel at multi-step generation. Furthermore, existing LLMs are
inherently autoregressive models [47], which demonstrate advanced abilities that are not present in
small models, such as the generalization [38], scalability [6], and task generality [31, 32]. Therefore,
it is imperative to adapt off-the-shelf LLMs as autoregressive forecasters, which keeps the consistency
to fully revitalize the model capacity and general-purpose token transitions."
LARGE LANGUAGE MODELS FOR TIME SERIES,0.07317073170731707,"2.2
Large Language Models for Time Series"
LARGE LANGUAGE MODELS FOR TIME SERIES,0.07479674796747968,"With the immense advancement of large language model infrastructure, LLM4TS methods have been
experiencing significant development in recent years. PromptCast [44] reformulates time series as
text pairs and accomplishes forecasting as a sentence-to-sentence task. LLMTime [13] regards time
series as numerical tokens, demonstrating the zero-shot generalizability in time series forecasting.
FPT [49] fine-tunes parameters of the LLM to adapt it as a general representation extractor serving
for multiple time series analysis tasks. UniTime [21] adapts a language model across diverse time
series for a unified forecaster of multiple domains. Based on thriving prompting techniques, deft
language prompts [15, 21] and soft prompting [7] for time series are further investigated."
LARGE LANGUAGE MODELS FOR TIME SERIES,0.07642276422764227,"LLM4TS methods have achieved performance breakthroughs in time series forecasting, but the cost of
training and inference can sometimes be resource-consuming due to the immensity of LLMs. Recent
revisiting of LLM4TS methods has revealed the inefficacy of LLMs adapted in the non-autoregressive
approach [35]. By contrast, AutoTimes frozen LLMs, transfers the general-purpose token transition,
and introduces minimal parameters to realize autoregressive next token prediction, thereby achieving
better model efficiency and consistent utilization of large models. We further provide Table 1 that
categorizes prevalent LLM4TS methods by several essential aspects."
MULTIMODAL LANGUAGE MODELS,0.07804878048780488,"2.3
Multimodal Language Models"
MULTIMODAL LANGUAGE MODELS,0.07967479674796749,"Multimodal models have been well-developed upon LLMs, among which vision language models
(VLM) have experienced rapid growth [1, 27]. The booming pre-trained vision backbones [11, 30],
together with the instruction tuning paradigm, has revealed the potential of LLMs for vision tasks,
where visual tokens and language tokens are concatenated as the input of the LLM [19, 20]. Inspired
by this, previous LLM4TS methods utilize instructive language tokens as prefix-prompts for time
series analysis [15, 34, 44]. Unlike previous works, our proposed method regards time series itself as
the instructive prompt. It avoids the modality gap caused by concatenating time series and language
tokens directly. We incorporate chronological information, the textual timestamp of time series, such
that the language model can effectively perceive date and periodicity as the position embedding, and
align simultaneous events from different time series [22] for multivariate forecasting."
MULTIMODAL LANGUAGE MODELS,0.08130081300813008,"Table 1: Comparison of LLM4TS methods: Autoregressive categories LLM-based forecasters by
whether to conduct autoregression. Freeze LLM enables quick adaptation, which would otherwise
require significant resources for fine-tuning. Multimodal refers to the utilization of information from
other modalities. Prior to AutoTimes, none of the LLM4TS methods achieved all three."
MULTIMODAL LANGUAGE MODELS,0.08292682926829269,"Method
AutoTimes TimeLLM [15] UniTime [21] FPT [49] LLMTime [13] TEST [34] TEMPO [7] PromptCast [44]"
MULTIMODAL LANGUAGE MODELS,0.08455284552845528,"Autoregressive
✓
✗
✗
✗
✓
✗
✗
✗
Freeze LLM
✓
✓
✗
✗
✓
✓
✗
✓
Multimodal
✓
✓
✓
✗
✗
✓
✓
✓"
METHOD,0.08617886178861789,"3
Method"
METHOD,0.08780487804878048,"The proposed AutoTimes adapts large language models for multivariate time series forecasting. Given
lookback observations x1:L = {x1, . . . , xL} ∈RL×C with L time steps and C variates, the objective"
METHOD,0.08943089430894309,"the
quick
brown
fox
jumps
over
the
lazy
dog
Language
Transitions"
METHOD,0.0910569105691057,Time Series
METHOD,0.09268292682926829,Transitions
METHOD,0.0943089430894309,Repurpose LLM
METHOD,0.0959349593495935,Forecaster
METHOD,0.0975609756097561,"Token Perspective
Model Perspective"
METHOD,0.0991869918699187,Token-wise
METHOD,0.1008130081300813,Alignment
METHOD,0.1024390243902439,Figure 2: An example to illustrate how AutoTimes adapts language models for time series forecasting.
METHOD,0.1040650406504065,"is to predict the future F time steps xL+1:L+F = {xL+1, . . . , xL+F } ∈RF ×C. Besides, the textual
timestamp at (e.g. 2016/07/05 00:00:00), as the most common covariate, is adopted for prediction,
which is aligned with time points xt ∈RC at time t. The task is to train an LLM-based forecaster f
that is able to predict with the (varying) lookback length L for the (arbitrary) forecast length F as:"
METHOD,0.10569105691056911,"f : (x1:L, a1:L+F ) 7→ˆxL+1:L+F .
(1)"
MODALITY ALIGNMENT,0.1073170731707317,"3.1
Modality Alignment"
MODALITY ALIGNMENT,0.10894308943089431,"Time series token
To empower the forecaster with the capability to predict time series for arbitrary
lengths, we repurpose autoregressive LLMs as time series forecasters as depicted in Figure 2. Prior
to this, we define time series token as the consecutive and non-overlapping segment of a single
variate. It is regarded as the common token of the LLM-based forecaster, which encompasses series
variations and mitigates excessively long autoregression. To focus on modeling temporal variations,
our forecaster predicts each variate independently. Beyond Channel Independence [26] that implicitly
captures the multivariate correlation [22] by shared parameters, AutoTimes converts timestamps
into position embeddings and explicitly aligns simultaneous segment tokens, which is detailed in
the next paragraph. Therefore, we simplify xt as the time point of specific variate xt ∈R. Given a
single-variate time series of context length NS, the i-th segment of length S is denoted as:"
MODALITY ALIGNMENT,0.11056910569105691,"si = {x(i−1)S+1, . . . , xiS} ∈RS, i = 1, . . . , N.
(2)"
MODALITY ALIGNMENT,0.11219512195121951,"Considering the general-purpose token transition, we freeze the parameters of large language models.
To realize the token-wise alignment between time series tokens and language tokens, we establish
SegmentEmbedding(·) : RS 7→RD that independently embeds segments into the latent space:"
MODALITY ALIGNMENT,0.11382113821138211,"SEi = SegmentEmbedding(si), i = 1, . . . , N,
(3)"
MODALITY ALIGNMENT,0.11544715447154472,where D is consistent with the dimension of the LLM.
MODALITY ALIGNMENT,0.11707317073170732,"Position embedding
Timestamp, an essential covariate indicating the chronological information, is
generally utilized as an extra embedding in previous deep forecasters [43, 48]. However, increasing
models [10, 26, 45] have discarded the embedding and found the performance will not be greatly
affected, implying the improper encoding of timestamps. In contrast, textual timestamps have been
demonstrated as an enhancement in LLM4TS methods, which are always formulated into prefix-
prompts [15, 21]. Nevertheless, it also leads to excessive context length, impeding LLMs from paying
sufficient attention to time series tokens and inducing time-consuming feed-forwarding. Inspired
by the functionality of position embedding, which incorporates information about the relative or
absolute position of the tokens [37]. We adopt LLM-embedded timestamps as position embeddings
to utilize temporal information and align simultaneous events (segments) from different varieties."
MODALITY ALIGNMENT,0.11869918699186992,"Technically, we formulate the starting and end timestamps of corresponding segments by the template
demonstrated in Figure 3. Experimentally, we observe that the simple template without deft design
can consistently boost the forecasting performance in Appendix D.5, aiding the LLM-based forecaster
to comprehend the date and align different variates based on Channel Independence. Since all the
previous language tokens are visible to the special ending token <EOS> of a sentence, we adopt the
embedding of <EOS> as TEi ∈RD as the position embedding from textual timestamps:"
MODALITY ALIGNMENT,0.12032520325203253,"TEi = SelectLast
 
LLM(TimestampTemplate(si))

.
(4)"
MODALITY ALIGNMENT,0.12195121951219512,"Notably, TEi is pre-computed by LLMs such that runtime forwarding for language tokens is not
required during training. Given that the latent space of the LLM locates both time series tokens and"
MODALITY ALIGNMENT,0.12357723577235773,"8’
This is the series 
from 2016/7/2 00:00:00
    to 2016/7/2 15:00:00 <EOS>"
MODALITY ALIGNMENT,0.12520325203252033,Template
MODALITY ALIGNMENT,0.12682926829268293,Ground Truth
MODALITY ALIGNMENT,0.12845528455284552,Segment Projection
MODALITY ALIGNMENT,0.13008130081300814,Large Language Model
MODALITY ALIGNMENT,0.13170731707317074,Segment Embedding
MODALITY ALIGNMENT,0.13333333333333333,"Next Token Prediction 1
2
3 2
3
4"
MODALITY ALIGNMENT,0.13495934959349593,"2’
3’
4’"
MODALITY ALIGNMENT,0.13658536585365855,"as Position
Embedding"
MODALITY ALIGNMENT,0.13821138211382114,Time Series
MODALITY ALIGNMENT,0.13983739837398373,Segments 7 8
MODALITY ALIGNMENT,0.14146341463414633,"MSE
MSE
MSE"
MODALITY ALIGNMENT,0.14308943089430895,Large Language Model
MODALITY ALIGNMENT,0.14471544715447154,"This is the series 
from 2016/7/2 00:00:00
    to 2016/7/2 15:00:00 <EOS>"
MODALITY ALIGNMENT,0.14634146341463414,"This is the series 
from 2016/7/1 00:00:00
    to 2016/7/1 23:00:00"
MODALITY ALIGNMENT,0.14796747967479676,"2016/7/7
2016/7/1
2016/7/2"
MODALITY ALIGNMENT,0.14959349593495935,2016/7/7  00:00:00 ……
MODALITY ALIGNMENT,0.15121951219512195,2016/7/7  23:00:00
MODALITY ALIGNMENT,0.15284552845528454,Timestamp ……
MODALITY ALIGNMENT,0.15447154471544716,Time Series ……
MODALITY ALIGNMENT,0.15609756097560976,(Pre-computed)
MODALITY ALIGNMENT,0.15772357723577235,"This is the series 
from 2016/7/2 00:00:00
    to 2016/7/2 15:00:00 <EOS>"
MODALITY ALIGNMENT,0.15934959349593497,"This is the series 
from 2016/7/2 00:00:00
    to 2016/7/2 15:00:00 <EOS>"
MODALITY ALIGNMENT,0.16097560975609757,Embeddings
MODALITY ALIGNMENT,0.16260162601626016,of Texts s2
MODALITY ALIGNMENT,0.16422764227642275,"SE2
TE2
TE1"
MODALITY ALIGNMENT,0.16585365853658537,"ˆS3
ˆS2 s1 SE1"
MODALITY ALIGNMENT,0.16747967479674797,"ˆE2
ˆE3 s7"
MODALITY ALIGNMENT,0.16910569105691056,TE7 SE7 ˆE8 ˆS8
MODALITY ALIGNMENT,0.17073170731707318,2016/7/2  00:00:00 ……
MODALITY ALIGNMENT,0.17235772357723578,2016/7/2  23:00:00
MODALITY ALIGNMENT,0.17398373983739837,2016/7/1  00:00:00 ……
MODALITY ALIGNMENT,0.17560975609756097,2016/7/1  23:00:00
MODALITY ALIGNMENT,0.1772357723577236,Context Length
MODALITY ALIGNMENT,0.17886178861788618,Segment Length … … … … … …
MODALITY ALIGNMENT,0.18048780487804877,"Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2)
textual timestamps are converted into the position embeddings by the LLM; (3) time series segments
are embedded and projected by next token prediction, where intermediate layers of LLM are frozen."
MODALITY ALIGNMENT,0.1821138211382114,"language tokens, the position embedding can be integrated with the corresponding time span without
increasing the context length. Concretely, the token embedding Ei ∈RD is obtained by:"
MODALITY ALIGNMENT,0.183739837398374,"Ei = SEi + TEi.
(5)"
NEXT TOKEN PREDICTION,0.18536585365853658,"3.2
Next Token Prediction"
NEXT TOKEN PREDICTION,0.18699186991869918,"As shown in Figure 3, prevalent LLMs [6, 36] are endowed with the capability of predicting the next
token si based on the preceding tokens s<i. We reutilize LLMs in a fully consistent approach and
generate prediction of arbitrary lengths iteratively. Given a time series of context length NS, the input
series is segmented and embedded into N token embeddings {E1, . . . , EN}. The training objective
is to independently generate the next tokens {ˆs2, . . . , ˆsN+1}. We feed the token embeddings Ei into
the intermediate layers of the LLM, which inherently parameterize token transitions:"
NEXT TOKEN PREDICTION,0.1886178861788618,"{ˆE2, . . . , ˆEN+1} = LLMLayers({E1, . . . , EN}).
(6)"
NEXT TOKEN PREDICTION,0.1902439024390244,We adopt SegmentProjection(·) : RD 7→RS to independently projects embeddings to segments:
NEXT TOKEN PREDICTION,0.191869918699187,"ˆsi = SegmentProjection(ˆEi), i = 2, . . . , N + 1.
(7)"
NEXT TOKEN PREDICTION,0.19349593495934958,"Finally, each predicted segment is supervised by the token-wise ground truth to optimize the parame-
ters of embedding and projection layers, which are simply implemented by multi-layer perceptrons:"
NEXT TOKEN PREDICTION,0.1951219512195122,"LMSE =
1
NS"
NEXT TOKEN PREDICTION,0.1967479674796748,"X
||si −ˆsi||2
2, i = 2, . . . , N.
(8)"
NEXT TOKEN PREDICTION,0.1983739837398374,"Notably, the context length NS is decided during training, representing the maximum input length
during inference. Therefore, one consequent forecaster is suitable for different input lengths like the
LLM, validated in Appendix D.4. Moreover, AutoTimes can generate predictions of arbitrary lengths
by iterative multi-step forecasting, proven to overcome error accumulation better than state-of-the-art
forecasters in Section 4.1, since autoregressive LLMs inherently excel at multi-step generation:"
NEXT TOKEN PREDICTION,0.2,"ˆsi = LLMForecaster(s<i), i = 1, . . . , F"
NEXT TOKEN PREDICTION,0.2016260162601626,"S .
(9)"
NEXT TOKEN PREDICTION,0.2032520325203252,"Instead of respectively training models on different lookback/forecast lengths, AutoTimes handles
all the scenarios by one model. Surprisingly, with the consistency of autoregression, it also inherits
notable generalizability and scaling behavior of LLMs, which is demonstrated in Sections 4.2 and 4.4."
IN-CONTEXT FORECASTING,0.2048780487804878,"3.3
In-Context Forecasting"
IN-CONTEXT FORECASTING,0.20650406504065041,"Large language models are capable of generating expected outputs based on provided task demonstra-
tions from downstream datasets without gradient updating, known as the in-context learning ability.
The task demonstrations are generally constituted by paired questions and answers [47]. Formally,
the context C = {g(x(1), y(1)), . . . , g(x(m), y(m))} represents a set of demonstrations with m pairs,
where g(·) is the template that transforms each question and answer into natural language."
IN-CONTEXT FORECASTING,0.208130081300813,"In terms of time series forecasting, we propose to constitute the pair by lookback-forecast windows,
which are exactly represented as successive time points from earlier historical observations. Hence, we
use time series in target datasets as prompts, extending the context for prediction beyond consecutive
lookback series. We denote the extended context as C, which contains m time series prompts tsp(j):"
IN-CONTEXT FORECASTING,0.2097560975609756,"C = {tsp(j) = x≤tj| earlier historical time series}, j = 1, . . . , m, tj ≤L.
(10)"
IN-CONTEXT FORECASTING,0.21138211382113822,"During training, we first obtain an LLM-based forecaster on a source dataset and select time series
prompts from the downstream target dataset based on a unified strategy. During inference, we ensure
all the prompts appear before the window to be predicted, such that there is no data leakage from
future information. As shown in Figure 4, we concatenate time series prompts with lookback series
and feed them as the context of the forecaster, termed in-context forecasting:"
IN-CONTEXT FORECASTING,0.21300813008130082,"f : (C, x1:L, a1:L+F ) 7→ˆxL+1:L+F .
(11)"
EXPERIMENTS,0.2146341463414634,"4
Experiments"
EXPERIMENTS,0.216260162601626,"We conduct thorough evaluations of the performance of AutoTimes, including time series forecasting,
zero-shot forecasting, and the proposed in-context forecasting. Additional analyses are included to
evaluate the generality, scaling behavior, and adaptation cost of large language models. Detailed code
implementation for reproduction is provided in our public code repository."
TIME SERIES FORECASTING,0.21788617886178863,"4.1
Time Series Forecasting"
TIME SERIES FORECASTING,0.21951219512195122,"Benchmarks
For long-term time series forecasting, we extensively include real-world datasets,
including ETTh1, ECL, Traffic, Weather [43], and Solar-Energy [22]. For short-term forecasting, we
adopt the well-acknowledged M4 competition [25]. Detailed descriptions are provided in Appendix A."
TIME SERIES FORECASTING,0.22113821138211381,"Baselines
We compare AutoTimes with state-of-the-art models, including advanced LLM4TS
methods: TimeLLM [15], UniTime [21], and FPT [49]; well-acknowledged deep forecasters: iTrans-
former [22], DLinear [45], PatchTST [26], and TimesNet [42]. For the challenging short-term
forecasting, we further include competitive baselines: Koopa [23], N-HiTS [8] and N-BEATS [28].
All baselines are officially implemented or reported. We adopt LLaMA-7B [36] as our base LLM.
Detailed implementations, error bars, and hyperparameter analysis are provided in Appendix B and C."
TIME SERIES FORECASTING,0.22276422764227644,"Setups
For short-term forecasting, we follow the well-acknowledged TimesNet [42], which assesses
the fundamental ability of forecasters in modeling temporal variations. For long-term forecasting, we
establish a novel one-for-all benchmark: a single forecaster is trained on one dataset and subsequently
utilized for all prediction lengths. We highlight that this approach evaluates the basic versatility as
foundation models of time series, which aims to break the prevailing practice of extensive training
across diverse real-world scenarios. To be specific, we evaluate all methods by rolling forecasting: a
model is trained with predetermined input/output lengths, and the predicted values are integrated as
part of the input in subsequent iterations until reaching the desired forecast length. Therefore, the
key to success in this task lies in mitigating multi-step error accumulation. Still, the conventional
one-for-one approach that trains forecasters respectively on each length is also provided in Table 12."
TIME SERIES FORECASTING,0.22439024390243903,"Results
The average results are presented in Table 2- 3, with the best results in bold and the second
best underlined. AutoTimes consistently outperforms all counterparts of short-term forecasting in
Table 2, demonstrating the basic ability of LLM-based forecasters to capture diverse series variations.
Further, in the one-for-all long-term scenarios, AutoTimes surpasses other LLM4TS methods and
deep forecasters in 80% datasets in Table 3, outperforming previous state-of-the-art TimeLLM by"
TIME SERIES FORECASTING,0.22601626016260162,Table 2: Average short-term forecasting results on the M4 [25]. Full results are provided in Table 11.
TIME SERIES FORECASTING,0.22764227642276422,"Models
AutoTimes TimeLLM
FPT
Koopa
N-HiTS DLinear PatchTST TimesNet
FiLM
N-BEATS"
TIME SERIES FORECASTING,0.22926829268292684,Average
TIME SERIES FORECASTING,0.23089430894308943,"sMAPE
11.831
11.983
11.991 11.863 11.960 12.418
13.022
11.930
12.489
11.910
MASE
1.585
1.595
1.600
1.595
1.606
1.656
1.814
1.597
1.690
1.613
OWA
0.850
0.859
0.861
0.858
0.861
0.891
0.954
0.867
0.902
0.862"
TIME SERIES FORECASTING,0.23252032520325203,"Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single
model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}.
AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and
output length F = 96 in other methods. All results are averaged. Full results is provided in Table 10."
TIME SERIES FORECASTING,0.23414634146341465,"Models AutoTimes
TimeLLM [15] UniTime [21]
FPT [49]
iTrans. [22]
DLinear [45]
PatchTST [26] TimesNet [42]"
TIME SERIES FORECASTING,0.23577235772357724,"Metric MSE MAE MSE
MAE
MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE"
TIME SERIES FORECASTING,0.23739837398373984,ETTh1 0.389 0.422 0.412 0.437 0.683 0.596 0.429 0.439 0.421 0.445 0.426 0.444 0.409 0.430 0.495 0.491
TIME SERIES FORECASTING,0.23902439024390243,"ECL
0.159 0.253 0.181 0.288 0.325 0.399 0.184 0.284 0.164 0.258 0.165 0.265 0.169 0.268 0.201 0.303"
TIME SERIES FORECASTING,0.24065040650406505,Weather 0.235 0.273 0.225 0.266 0.461 0.459 0.228 0.266 0.266 0.291 0.239 0.291 0.226 0.268 0.264 0.293
TIME SERIES FORECASTING,0.24227642276422764,"Traffic
0.374 0.264 0.410 0.303 0.584 0.367 0.461 0.326 0.384 0.274 0.423 0.298 0.391 0.275 0.602 0.322"
TIME SERIES FORECASTING,0.24390243902439024,"Solar.
0.197 0.242 0.263 0.335 0.392 0.462 0.236 0.303 0.213 0.291 0.222 0.283 0.202 0.269 0.213 0.295"
TIME SERIES FORECASTING,0.24552845528455283,"9.12% in average. Compared with other forecasters trained in the one-for-one scenario in Table 12,
AutoTimes still achieved state-of-the-art performance in 70% of settings without respective training."
TIME SERIES FORECASTING,0.24715447154471545,"By diving into the proposed one-for-all and the traditional one-for-one benchmarks in Table 3 and 12,
it is notable that prevalent deep forecasters, such as Transformer-based forecasters and DLinear, can
achieve competitive and even better results under rolling forecasting. Nevertheless, the performance
of non-autoregressive LLM4TS methods can degenerate a lot without respective training. Therefore,
it highlights our persistent utilization of autoregression and thorough leveraging of inherent token
transitions of LLMs, thereby mitigating error accumulation during multi-step rolling forecasting."
ZERO-SHOT FORECASTING,0.24878048780487805,"4.2
Zero-Shot Forecasting"
ZERO-SHOT FORECASTING,0.25040650406504067,"Setups
Large language models have exhibited remarkable zero-shot generalization capability [6].
To verify whether our LLM-based forecaster inherits this ability, where no training sample of the
target domain is available, we assess the performance of zero-shot forecasting. Concretely, we adhere
to the benchmark established by FPT [49], where the forecaster is initially trained on a source domain
and subsequently evaluated on an unseen target domain. We conduct the transfer learning between
the M3 and M4 competitions, both of which encompass abundant temporal variation patterns but
follow different data distributions. We compare AutoTimes with deep forecasters and FPT as the only
LLM4TS method, given that only FPT has exhibited zero-shot generalization in this benchmark."
ZERO-SHOT FORECASTING,0.25203252032520324,"Table 4: Zero-shot forecasting results in averaged SMAPE. M4 →M3 trains forecasters on the
datasets of M4 and evaluates on M3, and vice versa. Detailed results are provided in Appendix D.2"
ZERO-SHOT FORECASTING,0.25365853658536586,"Models
AutoTimes
FPT
DLinear PatchTST TimesNet NSFormer FEDFormer Informer Reformer"
ZERO-SHOT FORECASTING,0.2552845528455285,"M4 →M3
12.75
13.06
14.03
13.06
14.17
15.29
13.53
15.82
13.37"
ZERO-SHOT FORECASTING,0.25691056910569104,"M3 →M4
13.036
13.125 15.337
13.228
14.553
14.327
15.047
19.047
14.092"
ZERO-SHOT FORECASTING,0.25853658536585367,"Results
The comprehensive results of zero-shot forecasting are presented in Table 4. AutoTimes
demonstrates superior performance compared to deep forecasters and FPT in both M4 →M3 and
M3 →M4 scenarios. It is evident that LLM4TS methods generally achieve improved performance in
this task due to the enhanced model capacity, leading to a 15% SMAPE reduction compared with the
efficient forecaster DLinear. Despite sharing the same Transformer backbone, LLM4TS methods still
outperform PatchTST due to the transferable knowledge pre-trained on large corpora of sequences.
This underscores the advantage of leveraging LLMs for time series forecasting. Moreover, AutoTimes
inherits general-purpose token transitions, surpassing FPT without tuning intermediate LLM layers."
ZERO-SHOT FORECASTING,0.2601626016260163,Lookback
ZERO-SHOT FORECASTING,0.26178861788617885,Zero-Shot Forecasting
ZERO-SHOT FORECASTING,0.2634146341463415,"S2’
S3’
S4’"
ZERO-SHOT FORECASTING,0.26504065040650404,"S1
S2
S3"
ZERO-SHOT FORECASTING,0.26666666666666666,"S1
S2
S3
S4
Forecast S5’ S4’"
ZERO-SHOT FORECASTING,0.2682926829268293,"…
Forecaster S5"
ZERO-SHOT FORECASTING,0.26991869918699185,In-Context Forecasting
ZERO-SHOT FORECASTING,0.27154471544715447,"S2’
S3’
S4’"
ZERO-SHOT FORECASTING,0.2731707317073171,"S1
S2
S3 S5’ S4’ …"
ZERO-SHOT FORECASTING,0.27479674796747966,"P1
P2
P3
P4"
ZERO-SHOT FORECASTING,0.2764227642276423,"P2’
P3’
P4’
P5’"
ZERO-SHOT FORECASTING,0.2780487804878049,Forecaster …
ZERO-SHOT FORECASTING,0.27967479674796747,"Time Series Prompt
P1
P2
P3
P4"
ZERO-SHOT FORECASTING,0.2813008130081301,"Target Domain
Trained on Source Domain"
ZERO-SHOT FORECASTING,0.28292682926829266,"Figure 4: Demonstration of in-context forecasting and results compared with zero-shot. We uniformly
select the foremost time points from the target domain as prompts and concatenate them with lookback
to obtain the prediction. AutoTimes adapts LLMs on the source domain with a larger context length
to place the additional time series prompt. Supplementary showcases are provided in Figure 12."
IN-CONTEXT FORECASTING,0.2845528455284553,"4.3
In-Context Forecasting"
IN-CONTEXT FORECASTING,0.2861788617886179,"Setups
We conduct in-context forecasting on AutoTimes, which is depicted in Figure 4. Similar to
zero-shot forecasting, the task is to apply a forecaster, trained on a source dataset, to an unseen target
dataset. Additionally, several task demonstrations from the target domain, referred to as time series
prompts in Equation 10, are available during inference. Specifically, we concatenate these prompts
with the lookback window to form the context for prediction."
IN-CONTEXT FORECASTING,0.28780487804878047,"We adopt the aforementioned M4 →M3 scenario. Since the samples of the M3 dataset are univariate
time series with different lengths, we always predict the last F time points of each sample during
inference. We set the lookback length L = F, and thus the length of time series prompts is 2F.
We set the number of prompts m = 1. Therefore, we initially train an LLM-based forecaster on
the source M4 dataset with the context length of 3F. We adopt an intuitive strategy to select the
prompt: uniformly adopting the first 2F time points of the time series as the corresponding prompt.
Supposing the lookback series starts after time t (≥F), in-context forecasting is formulated as:"
IN-CONTEXT FORECASTING,0.2894308943089431,"f : ({x1:2F }, xt+1:t+F , at+1:t+2F ) 7→ˆxt+F +1,t+2F .
(12)"
IN-CONTEXT FORECASTING,0.2910569105691057,"To prevent data leakage of future information, too short samples are discarded to prevent the overlap
between the prompt and the future prediction. The implementation details of in-context forecasting
are provided in Appendix D.6. We further investigate different prompt retrieval strategies. Insightful
results are provided to reveal the influence of using time series prompts for interactive prediction and
take-away instructions of prompt engineering in the time series modality."
IN-CONTEXT FORECASTING,0.2926829268292683,"Results
The quantitative results of in-context forecasting are provided on the right of Figure 4. The
results of zero-shot forecasting, where no downstream demonstration is available, are compared as
the baseline. Benefiting from the time series prompts of the target domain, our LLM-based forecaster
with the proposed in-context forecasting paradigm achieves consistent promotions on all M3 subsets
and the averaged 13.3% SMAPE reduction compared with zero-shot forecasting. In contrast to
previous LLM4TS methods that rely on deft language prompts, LLMs adopted by AutoTimes can
be instructed by time series itself with our intuitive prompting engineering. From the perspective of
the forecasting paradigm, we extend the prediction context beyond the lookback window. To inherit
token transitions of language models parameterized by intermediate layers, AutoTimes takes a crucial
step by establishing a mapping between time series segments and the latent space of language tokens,
which is however absent in non-autoregressive LLM4TS methods. Therefore, ensuring autoregression
consistency enhances the effective utilization of LLMs as foundation models."
METHOD ANALYSIS,0.2943089430894309,"4.4
Method Analysis"
METHOD ANALYSIS,0.2959349593495935,"Generality
Previous LLM4TS [15, 49] methods focus on applying their approach to specific LLMs.
We demonstrate that AutoTimes is compatible with any decoder-only LLMs. By extensively training
LLM-based forecasters by AutoTimes based on prevalent LLMs, including GPT-2 [31], OPT [46],
and LLaMA [36], we present the results in Table 5, highlighting the generality of AutoTimes."
METHOD ANALYSIS,0.2975609756097561,Table 5: Averaged results of alternative language models. Full results are provided in Table 18.
METHOD ANALYSIS,0.2991869918699187,"LLM
GPT-2 (124M)
OPT-350M
OPT-1.3B
OPT-2.7B
OPT-6.7B
LLaMA-7B"
METHOD ANALYSIS,0.3008130081300813,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE"
METHOD ANALYSIS,0.3024390243902439,"ECL
0.173
0.266
0.168
0.263
0.164
0.258
0.164
0.258
0.162
0.256
0.159
0.253"
METHOD ANALYSIS,0.3040650406504065,"ETTh1
0.397
0.425
0.401
0.429
0.396
0.424
0.394
0.424
0.394
0.424
0.389
0.423"
METHOD ANALYSIS,0.3056910569105691,"Traffic
0.406
0.276
0.405
0.277
0.397
0.271
0.394
0.269
0.393
0.270
0.374
0.264"
METHOD ANALYSIS,0.3073170731707317,"Weather
0.242
0.278
0.240
0.275
0.240
0.276
0.243
0.277
0.247
0.282
0.235
0.273"
METHOD ANALYSIS,0.3089430894308943,"Scaling behavior
Scalability is an essential characteristic that emerges from small models to large
foundation models. By investigating the results presented in Table 5, we observe that the prediction
accuracy of the forecaster generally improves with the increase in LLM parameters. This scaling
behavior of LLM-based forecasters introduces a trade-off between performance and adaptation cost.
To provide a comprehensive assessment, we evaluate each adapted forecaster from three perspectives:
performance, training speed, and parameters, as presented in Figure 5. We observe that the largest
LLaMA-7B consistently delivers optimal forecasting performance. As a relatively small language
model, OPT-1.3B exhibits good parameter efficiency as an out-of-the-box forecaster. MSE ECL GPT-2"
METHOD ANALYSIS,0.3105691056910569,OPT-250M
METHOD ANALYSIS,0.3121951219512195,OPT-1.3B OPT-2.7B
METHOD ANALYSIS,0.31382113821138213,OPT-6.7B
METHOD ANALYSIS,0.3154471544715447,LLaMA-7B
METHOD ANALYSIS,0.3170731707317073,Params Count
METHOD ANALYSIS,0.31869918699186994,"125M 1B
3B
7B"
METHOD ANALYSIS,0.3203252032520325,"0
125
250
375
500
625
750
875
Training Time (ms/iter) 0.160 0.170 0.165"
METHOD ANALYSIS,0.32195121951219513,LLaMA-7B
METHOD ANALYSIS,0.3235772357723577,OPT-6.7B
METHOD ANALYSIS,0.3252032520325203,OPT-2.7B
METHOD ANALYSIS,0.32682926829268294,OPT-1.3B
METHOD ANALYSIS,0.3284552845528455,OPT-250M GPT-2
METHOD ANALYSIS,0.3300813008130081,Params Count
METHOD ANALYSIS,0.33170731707317075,"125M 1B
3B
7B"
METHOD ANALYSIS,0.3333333333333333,"0
125
250
375
500
625
750
875
Training Time (ms/iter)"
METHOD ANALYSIS,0.33495934959349594,Traffic 0.380 0.390 0.400 0.370 MSE
METHOD ANALYSIS,0.33658536585365856,"Figure 5: Efficiency comparison of alternative LLMs, evaluated by the same configuration of Table 5."
METHOD ANALYSIS,0.3382113821138211,"Adaptation cost
To mitigate the substantial cost of adapting large language models, AutoTimes
introduces minimal parameters with all intermediate layers of LLM frozen. Additionally, we seam-
lessly integrate the language tokens (e.g. textual timestamps) without excessive context length and
runtime overhead for training, thereby significantly reducing the adaptation cost. Figure 6 presents a
comprehensive efficiency analysis with advanced LLM4TS methods: FPT is applicable on GPT-2 and
TimeLLM is applicable on LLaMA-7B. Not only does AutoTime achieve better results in Table 3,
but its training and reasoning time is also greatly reduced, bringing over 5× speedup on average.
In terms of parameter efficiency, AutoTimes focuses on establishing the embedding for time series
segments, which is simply implemented by the MLP (0.79M) account for 0.1% parameters of the
LLM (7B). Therefore, the results affirm the effectiveness of reutilizing the inherent token transition."
METHOD ANALYSIS,0.33983739837398375,"Figure 6: Comparison of AutoTimes and other LLM4TS methods in terms of training/inference time
and tunable parameters with the same batch size (224) on the ETTh1 dataset."
METHOD ANALYSIS,0.34146341463414637,"Ablation study
Recent research has raised doubts about the validity of previous LLM4TS meth-
ods [35], which predominantly adopt non-autoregression, that is, treating the LLM as a BERT-style
pre-trained backbone and utilize a globally flattening projector on all lookback tokens. Here, we
provide a thorough ablation study to examine our proposed AutoTimes in Table 6. The results under-
score that our method maintains the consistency of the decoder-only architecture and autoregressive
inference, effectively leveraging LLMs and addressing the concerns regarding performance improve-
ment and adaptation cost. Further, we provide a comparison by substituting our token-wise segment
projection (consistent with LLMs) with the flatten linear head [26] (common in non-autoregressive
forecasters). Results of Table 21 in the Appendix reveal that the performance of non-autoregressive
generation is consistently inferior to that of our autoregressive AutoTimes approach."
METHOD ANALYSIS,0.34308943089430893,"Table 6: We follow the protocol of LLM4TS ablation studies [35] to verify whether the LLM is truly
useful in our AutoTimes: (1) w/o LLM replaces the language model entirely and passing input tokens
directly to the last layer; (2) LLM2Attn replaces the language model with a single multi-head attention
layer; (3) LLM2Trsf replaces the language model with a single transformer block."
METHOD ANALYSIS,0.34471544715447155,"Dataset
ETTh1
ECL"
METHOD ANALYSIS,0.3463414634146341,"Type
AutoTimes
w/o LLM
LLM2Attn
LLM2Trsf
AutoTimes
w/o LLM
LLM2Attn
LLM2Trsf"
METHOD ANALYSIS,0.34796747967479674,"Metric
MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE"
METHOD ANALYSIS,0.34959349593495936,Pred-96 0.360 0.400 0.365 0.399 0.383 0.404 0.377 0.401 0.129 0.225 0.171 0.263 0.156 0.255 0.162 0.263
METHOD ANALYSIS,0.35121951219512193,Pred-192 0.388 0.419 0.405 0.425 0.414 0.422 0.406 0.420 0.147 0.241 0.192 0.282 0.178 0.276 0.189 0.287
METHOD ANALYSIS,0.35284552845528455,Pred-336 0.401 0.429 0.429 0.441 0.431 0.432 0.421 0.431 0.162 0.258 0.216 0.304 0.198 0.295 0.216 0.309
METHOD ANALYSIS,0.3544715447154472,Pred-720 0.406 0.440 0.450 0.468 0.456 0.454 0.449 0.452 0.199 0.288 0.264 0.342 0.230 0.320 0.258 0.340
METHOD ANALYSIS,0.35609756097560974,"LoRA adaptation
By incorporating low-rank adaptation technique [14] on the intermediate LLM
layers, the token transition of the large language model can be further fine-tuned to align the future
extrapolation of time series. Table 7 provides the performance comparing the incorporation of LoRA,
which consistently improves the performance of the LLM-based forecaster adapted by AutoTimes."
METHOD ANALYSIS,0.35772357723577236,Table 7: Full long-term forecasting results of AutoTimes and AutoTimes equipped with LoRA [14].
METHOD ANALYSIS,0.359349593495935,"Dataset
ETTh1
ECL
Weather
Traffic
Solar-Energy"
METHOD ANALYSIS,0.36097560975609755,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE"
METHOD ANALYSIS,0.36260162601626017,"Pred-96
AutoTimes
0.360
0.397
0.140
0.236
0.158
0.208
0.369
0.257
0.179
0.220
+ LoRA
0.357
0.397
0.130
0.225
0.151
0.201
0.360
0.256
0.176
0.219"
METHOD ANALYSIS,0.3642276422764228,"Pred-192
AutoTimes
0.391
0.419
0.159
0.253
0.207
0.254
0.394
0.268
0.198
0.236
+ LoRA
0.391
0.420
0.149
0.242
0.197
0.244
0.383
0.267
0.195
0.235"
METHOD ANALYSIS,0.36585365853658536,"Pred-336
AutoTimes
0.408
0.432
0.177
0.270
0.262
0.298
0.413
0.278
0.213
0.252
+ LoRA
0.409
0.433
0.164
0.259
0.251
0.287
0.401
0.277
0.208
0.249"
METHOD ANALYSIS,0.367479674796748,"Pred-720
AutoTimes
0.429
0.452
0.216
0.303
0.342
0.353
0.449
0.299
0.239
0.277
+ LoRA
0.426
0.451
0.202
0.293
0.326
0.339
0.440
0.300
0.225
0.268"
CONCLUSION,0.36910569105691055,"5
Conclusion"
CONCLUSION,0.37073170731707317,"This paper aims to develop foundation models for time series forecasting. We utilize off-the-shelf
LLMs as autoregressive forecasters by transferring the general-purpose and multi-step generation
ability. Different from prior methods, we notice prevalent non-autoregressive LLM4TS methods may
contradict the decoder-only structure and lead to insufficient utilization of LLMs. Experimentally, the
proposed method achieves state-of-the-art performance with remarkable model efficiency. Further
analysis reveals that our forecaster effectively inherits advanced capabilities such as zero-shot and
in-context forecasting, and is able to utilize both instructive times series and timestamps. In the future,
we will further incorporate advanced low-rank adaptation and utilize booming language backbones."
CONCLUSION,0.3723577235772358,Acknowledgments
CONCLUSION,0.37398373983739835,This work was supported by the Ministry of Industry and Information Technology of China.
REFERENCES,0.375609756097561,References
REFERENCES,0.3772357723577236,"[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for
few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022."
REFERENCES,0.37886178861788616,"[2] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances
in neural information processing systems, 13, 2000."
REFERENCES,0.3804878048780488,"[3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of
foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.3821138211382114,"[4] George Box. Box and jenkins: time series analysis, forecasting and control. In A Very British Affair: Six
Britons and the Development of Time Series Analysis During the 20th Century, pages 161–215. Springer,
2013."
REFERENCES,0.383739837398374,"[5] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis:
forecasting and control. John Wiley & Sons, 2015."
REFERENCES,0.3853658536585366,"[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020."
REFERENCES,0.38699186991869916,"[7] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-
based generative pre-trained transformer for time series forecasting. arXiv preprint arXiv:2310.04948,
2023."
REFERENCES,0.3886178861788618,"[8] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco,
and Artur Dubrawski. Nhits: Neural hierarchical interpolation for time series forecasting. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 37, pages 6989–6997, 2023."
REFERENCES,0.3902439024390244,"[9] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context?
language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559,
2022."
REFERENCES,0.39186991869918697,"[10] Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu. Long-term forecasting with tide:
Time-series dense encoder. arXiv preprint arXiv:2304.08424, 2023."
REFERENCES,0.3934959349593496,"[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."
REFERENCES,0.3951219512195122,"[12] James Durbin and Siem Jan Koopman. Time series analysis by state space methods, volume 38. OUP
Oxford, 2012."
REFERENCES,0.3967479674796748,"[13] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot
time series forecasters. arXiv preprint arXiv:2310.07820, 2023."
REFERENCES,0.3983739837398374,"[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
2021."
REFERENCES,0.4,"[15] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan
Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language
models. arXiv preprint arXiv:2310.01728, 2023."
REFERENCES,0.4016260162601626,"[16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361, 2020."
REFERENCES,0.4032520325203252,"[17] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.40487804878048783,"[18] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal
patterns with deep neural networks. In The 41st international ACM SIGIR conference on research &
development in information retrieval, pages 95–104, 2018."
REFERENCES,0.4065040650406504,"[19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In International Conference on Machine
Learning, pages 12888–12900. PMLR, 2022."
REFERENCES,0.408130081300813,"[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint
arXiv:2304.08485, 2023."
REFERENCES,0.4097560975609756,"[21] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and Roger Zimmermann.
Unitime: A language-empowered unified model for cross-domain time series forecasting. arXiv preprint
arXiv:2310.09751, 2023."
REFERENCES,0.4113821138211382,"[22] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itrans-
former: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625,
2023."
REFERENCES,0.41300813008130083,"[23] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series
dynamics with koopman predictors. arXiv preprint arXiv:2305.18803, 2023."
REFERENCES,0.4146341463414634,"[24] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Timer:
Transformers for time series analysis at scale. arXiv preprint arXiv:2402.02368, 2024."
REFERENCES,0.416260162601626,"[25] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: 100,000
time series and 61 forecasting methods. International Journal of Forecasting, 36(1):54–74, 2020."
REFERENCES,0.41788617886178864,"[26] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words:
Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022."
REFERENCES,0.4195121951219512,"[27] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023."
REFERENCES,0.4211382113821138,"[28] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion
analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019."
REFERENCES,0.42276422764227645,"[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems, 32, 2019."
REFERENCES,0.424390243902439,"[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR,
2021."
REFERENCES,0.42601626016260163,"[31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.4276422764227642,"[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
Journal of machine learning research, 21(140):1–67, 2020."
REFERENCES,0.4292682926829268,"[33] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. Neurocomputing, 568:127063, 2024."
REFERENCES,0.43089430894308944,"[34] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to
activate llm’s ability for time series. arXiv preprint arXiv:2308.08241, 2023."
REFERENCES,0.432520325203252,"[35] Mingtian Tan, Mike A Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are language models
actually useful for time series forecasting? arXiv preprint arXiv:2406.16964, 2024."
REFERENCES,0.43414634146341463,"[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023."
REFERENCES,0.43577235772357725,"[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017."
REFERENCES,0.4373983739837398,"[38] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien
Launay, and Colin Raffel. What language model architecture and pretraining objective works best for
zero-shot generalization? In International Conference on Machine Learning, pages 22964–22984. PMLR,
2022."
REFERENCES,0.43902439024390244,"[39] Peter R Winters. Forecasting sales by exponentially weighted moving averages. Management science,
6(3):324–342, 1960."
REFERENCES,0.44065040650406506,"[40] Gerald Woo, Chenghao Liu, Akshat Kumar, and Doyen Sahoo. Pushing the limits of pre-training for time
series forecasting in the cloudops domain. arXiv preprint arXiv:2310.05063, 2023."
REFERENCES,0.44227642276422763,"[41] Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Unified
training of universal time series forecasting transformers. arXiv preprint arXiv:2402.02592, 2024."
REFERENCES,0.44390243902439025,"[42] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal
2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186, 2022."
REFERENCES,0.44552845528455287,"[43] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers
with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems,
34:22419–22430, 2021."
REFERENCES,0.44715447154471544,"[44] Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting.
IEEE Transactions on Knowledge and Data Engineering, 2023."
REFERENCES,0.44878048780487806,"[45] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting?
In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121–11128, 2023."
REFERENCES,0.4504065040650406,"[46] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068, 2022."
REFERENCES,0.45203252032520325,"[47] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
ichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint
arXiv:2303.18223, 2023."
REFERENCES,0.45365853658536587,"[48] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the
AAAI conference on artificial intelligence, volume 35, pages 11106–11115, 2021."
REFERENCES,0.45528455284552843,"[49] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series
analysis by pretrained lm. arXiv preprint arXiv:2302.11939, 2023."
REFERENCES,0.45691056910569106,"[50] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading
books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015."
REFERENCES,0.4585365853658537,"A
Dataset Descriptions"
REFERENCES,0.46016260162601624,"We conduct experiments to evaluate the performance of the proposed AutoTimes on seven real-world datasets
spanning diverse domains: (1) ETTh1 [48] spans from July 2016 to July 2018 and consists of seven factors
related to electricity transformers. (2) Weather [43] encompasses 21 meteorological factors collected every 10
minutes in 2020 from the Weather Station of the Max Planck Biogeochemistry Institute. (3) ECL [43] captures
hourly electricity consumption data from 321 clients. (4) Traffic [43] gathers hourly road occupancy rates from
862 sensors on San Francisco Bay area freeways, covering the period from January 2015 to December 2016. (5)
Solar-Energy [18] records solar power production from 137 PV plants in 2006, sampled every 10 minutes. (6)
M4 is a competition dataset encompassing various time series across different frequencies and domains such as
business and economics. (7) M3, albeit smaller than M4, also contains diverse time series from various domains."
REFERENCES,0.46178861788617886,"We follow the same data processing and train-validation-test set split protocol used in TimesNet [43], where the
train, validation, and test datasets are strictly divided according to chronological order to ensure no data leakage.
As for long-term forecasting settings, we fix the context length of AutoTimes and the lookback length of other
compared methods as 672 in ETT, ECL, Traffic, Weather, and Solar-Energy, and the forecast length varies in {96,
192, 336, 720}. For the short-term forecasting on M4 and M3 datasets, the input length is generally set to twice
the output length according to the official implementation of TimesNet. The details are provided in Table 8."
REFERENCES,0.4634146341463415,"Table 8: Detailed dataset descriptions. Dim denotes the variate number. Dataset Size denotes the total
number of time points in (Train, Validation, Test) splits respectively. Forecast Length denotes the
future time points to be predicted. Frequency denotes the sampling interval of time points."
REFERENCES,0.46504065040650405,"Dataset
Dim
Forecast Length
Dataset Size
Frequency
Information"
REFERENCES,0.4666666666666667,"ETTh1
7
{96, 192, 336, 720}
(8545, 2881, 2881)
Hourly
Electricity"
REFERENCES,0.4682926829268293,"Weather
21
{96, 192, 336, 720}
(36792, 5271, 10540)
10min
Weather"
REFERENCES,0.46991869918699186,"ECL
321
{96, 192, 336, 720}
(18317, 2633, 5261)
Hourly
Electricity"
REFERENCES,0.4715447154471545,"Traffic
862
{96, 192, 336, 720}
(12185, 1757, 3509)
Hourly
Transportation"
REFERENCES,0.47317073170731705,"Solar-Energy
137
{96, 192, 336, 720}
(36601, 5161, 10417)
10min
Energy"
REFERENCES,0.47479674796747967,"M4-Yearly
1
6
(23000, 0, 23000)
Yearly
Demographic"
REFERENCES,0.4764227642276423,"M4-Quarterly
1
8
(24000, 0, 24000)
Quarterly
Finance"
REFERENCES,0.47804878048780486,"M4-Monthly
1
18
(48000, 0, 48000)
Monthly
Industry"
REFERENCES,0.4796747967479675,"M4-Weekly
1
13
(359, 0, 359)
Weekly
Macro"
REFERENCES,0.4813008130081301,"M4-Daily
1
14
(4227, 0, 4227)
Daily
Micro"
REFERENCES,0.48292682926829267,"M4-Hourly
1
48
(414, 0, 414)
Hourly
Other"
REFERENCES,0.4845528455284553,"M3-Yearly
1
6
(645, 0, 645)
Yearly
Demographic"
REFERENCES,0.4861788617886179,"M3-Quarterly
1
8
(756, 0, 756)
Quarterly
Finance"
REFERENCES,0.4878048780487805,"M3-Monthly
1
18
(1428, 0, 1428)
Monthly
Industry"
REFERENCES,0.4894308943089431,"M3-Others
1
8
(174, 0, 174)
Weekly
Macro"
REFERENCES,0.49105691056910566,"B
Implementation Details"
REFERENCES,0.4926829268292683,"AutoTimes processes timestamps in textual form rather than numerical encoding, potentially enabling to handle
other textual data such as news or logs. We utilize LLM to obtain embedding for the special token <EOS> to
capture embedding for the entire sentence. Pseudo-code for this process is depicted in Algorithm 1. It is worth
noting that in the context of multivariate time series forecasting, timestamps are shared across variates. Thus,
timestamps can implicitly express relationships between variates even with channel independence. Further,
assuming there are C variates since the number of timestamps is 1"
REFERENCES,0.4943089430894309,"C of the total time point count, these embeddings
can be efficiently pre-computed by large language models."
REFERENCES,0.4959349593495935,"After obtaining embedding for the timestamps, we repurpose LLM for time series forecasting using Algorithm 2.
At this stage, only the parameters of SegmentEmbedding and SegmentProjection are updated, while the"
REFERENCES,0.4975609756097561,"parameters of LLMs remain entirely frozen. During inference, AutoTimes utilizes the last token generated as its
prediction and then employs this output to create subsequent predictions autoregressively. This approach enables
AutoTimes to predict sequences of variable lengths with just one model dynamically. Such capability is crucial
in real-world application scenarios. The pseudo-code in Algorithm 3-4 illustrates this process."
REFERENCES,0.4991869918699187,"All the experiments are conducted using PyTorch [29] on NVIDIA A100 GPUs. We employ Adam [17] with
an initial learning rate in {10−3, 5 × 10−4, 10−4} and MSE loss for model optimization. We adopt Channel
Independence [26] for multivariate time series and utilize our position embeddings of timestamps to explicitly
align them. The batch size is chosen from {256, 1024, 2048}, and we set the number of training epochs as 10.
As for SegmentEmbedding and SegmentProjection, we implement them by either a linear layer or MLP.
Results of deep forecaster are based on the benchmark provided by the TimesNet [43] repository, which is fairly
built on the same configurations provided by the original paper. LLM4TS methods [15, 21, 49] are implemented
by their official and open-source repository. Unless otherwise specified, we use LLaMA-7B [36] as the default
base LLM. We also present the standard deviation of AutoTimes forecasting performance with three random
seeds in Table 9, demonstrating that the performance of AutoTimes is stable."
REFERENCES,0.5008130081300813,"Algorithm 1 AutoTimes - Generate Text Embedding
Require: Input time series x(i−1)S+1:iS
▷i-th token of length S"
REFERENCES,0.5024390243902439,"1: si = {x(i−1)S+1, . . . , xiS}
▷Model dimension of the LLM D"
REFERENCES,0.5040650406504065,"2: TEi = SelectLast
 
LLM(TextualDuration(si))

▷TEi ∈RD"
REFERENCES,0.5056910569105691,"3: Return TEi
▷Return textual embedding of i-th token"
REFERENCES,0.5073170731707317,"Algorithm 2 AutoTimes - Repurpose LLM
Require: Input time series {x1, . . . , x(N+1)×S}; text embeddings {TE1, . . . , TEN}
▷Token"
REFERENCES,0.5089430894308943,number N
REFERENCES,0.510569105691057,"1: for i in {1, . . . , N}:"
REFERENCES,0.5121951219512195,"2: for si = {x(i−1)S+1, . . . , xiS}
▷si ∈RS"
REFERENCES,0.5138211382113821,"3: for SEi = SegmentEmbedding(si)
▷SEi ∈RD"
REFERENCES,0.5154471544715448,"4: for Ei = SEi + TEi
▷Ei ∈RD"
REFERENCES,0.5170731707317073,"5: {ˆE2, . . . , ˆEN+1} = LLMLayers({E1, . . . , EN})
▷ˆEi ∈RD"
REFERENCES,0.5186991869918699,"6: for i in {2, . . . , N + 1}:"
REFERENCES,0.5203252032520326,"7: for ˆsi = SegmentProjection(ˆEi)
▷ˆsi ∈RS"
REFERENCES,0.5219512195121951,"8: sN+1 = {xNS+1, . . . , xNS+S}"
REFERENCES,0.5235772357723577,"9: LMSE =
1
NS
P ||si −ˆsi||2
2, i ∈{2, . . . , N + 1}
▷Objective of the next token prediction"
REFERENCES,0.5252032520325203,"Algorithm 3 AutoTimes - LLM Forecasting
Require: Input time series {x1, . . . , xN1×S}; text embeddings {TE1, . . . , TEN1}
▷Lookback"
REFERENCES,0.526829268292683,token number N1 ≤N
REFERENCES,0.5284552845528455,"1: for i in {1, . . . , N1}:"
REFERENCES,0.5300813008130081,"2: for si = {x(i−1)S+1, . . . , xiS}
▷si ∈RS"
REFERENCES,0.5317073170731708,"3: for SEi = SegmentEmbedding(si)
▷SEi ∈RD"
REFERENCES,0.5333333333333333,"4: for Ei = SEi + TEi
▷Ei ∈RD"
REFERENCES,0.5349593495934959,"5: {ˆE2, . . . , ˆEN1+1} = LLMLayers({E1, . . . , EN1})
▷ˆEi ∈RD"
REFERENCES,0.5365853658536586,"6: for i in {2, . . . , N + 1}:"
REFERENCES,0.5382113821138211,"7: for ˆsi = SegmentProjection(ˆEi)
▷ˆsi ∈RS"
REFERENCES,0.5398373983739837,"8: Return ˆsN+1
▷Return last token ˆsN+1 ∈RS as the prediction"
REFERENCES,0.5414634146341464,"Algorithm 4 AutoTimes - Autoregressive Generation
Require: Input time series {x1, . . . , xN1×S}; textual embeddings {TE1, . . . , TEN2}
▷Forecast"
REFERENCES,0.5430894308943089,token number N2 −N1
REFERENCES,0.5447154471544715,"1: x = {x1, . . . , xN1×S}"
REFERENCES,0.5463414634146342,2: prediction = {}
REFERENCES,0.5479674796747968,"3: for i in {1, . . . , N2 −N1}:"
REFERENCES,0.5495934959349593,"4: for ˆy = LLMForecaster(x, TE:N1+i−1)
▷Details in Algorithm 3"
REFERENCES,0.551219512195122,"5: for x ←{x, ˆy}
▷Concatenate for the input for next iteration"
REFERENCES,0.5528455284552846,"6: for prediction ←{prediction, ˆy}
▷Record prediction results"
REFERENCES,0.5544715447154471,"7: Return prediction
▷Return result ∈R(N2−N1)×S"
REFERENCES,0.5560975609756098,Table 9: Performance and standard deviations of AutoTimes. Results come from three random seeds.
REFERENCES,0.5577235772357724,"Dataset
ETTh1
ECL
Weather"
REFERENCES,0.5593495934959349,"Horizon
MSE
MAE
MSE
MAE
MSE
MAE"
REFERENCES,0.5609756097560976,"96
0.360±0.002
0.400±0.002
0.129±0.001
0.225±0.001
0.153±0.000
0.203±0.001
192
0.388±0.002
0.419±0.001
0.147±0.002
0.241±0.002
0.201±0.001
0.250±0.001
336
0.401±0.003
0.429±0.002
0.162±0.002
0.258±0.003
0.256±0.002
0.293±0.001
720
0.406±0.004
0.440±0.002
0.199±0.004
0.288±0.002
0.331±0.002
0.345±0.001"
REFERENCES,0.5626016260162602,"Dataset
Traffic
Solar-Energy"
REFERENCES,0.5642276422764227,"Horizon
MSE
MAE
MSE
MAE
96
0.343±0.001
0.248±0.001
0.171±0.001
0.221±0.001
192
0.362±0.001
0.257±0.001
0.190±0.001
0.236±0.002
336
0.379±0.002
0.266±0.001
0.203±0.002
0.248±0.002
720
0.413±0.003
0.284±0.002
0.222±0.002
0.262±0.003"
REFERENCES,0.5658536585365853,"C
Hyperparameter Sensitivity"
REFERENCES,0.567479674796748,"SegmentEmbedding and SegmentProjection are uniformly implemented by MLP. The number of layers is
fixed as 2 and the hidden dimension is selected from {256, 512, 1024} according to the validation loss. The
segment length is set as S = 96 in multivariate datasets and is set as the prediction length S = F in M3
and M4. We verify the robustness of AutoTimes of hyperparameters as follows: the layer number and hidden
dimension of SegmentEmbedding and SegmentProjection, context length, and segment length. We observe
that AutoTimes is insensitive to the configurations of embedding and projection layers. Besides, performance
can be improved by increasing the context length. For long prediction lengths, a larger segment length is favored."
REFERENCES,0.5691056910569106,Figure 7: Hyperparameter sensitivity of AutoTimes. Each curve presents a specific forecast length.
REFERENCES,0.5707317073170731,"D
Supplementary Results"
REFERENCES,0.5723577235772358,"D.1
Time Series Forecasting"
REFERENCES,0.5739837398373984,"We compare the performance of AutoTimes with state-of-the-art LLM4TS methods and well-acknowledged
deep forecasters. Table 11 shows detailed short-term forecast results on M4. Table 10 presents results of the
one-for-all forecasting benchmark across ETTh1, ECL, Traffic, Weather, and Solar-Energy datasets. We evaluate
all methods by rolling forecasting: each model is trained with input length 672 and output length 96, and the
predicted values are integrated as part of the input in the next iteration until reaching the desired forecast horizon."
REFERENCES,0.5756097560975609,"Furthermore, the traditional one-for-one approach, where forecasters are trained individually for each prediction
length, is also presented in Table 12. The results are reproduced using their corresponding official code. For the
sake of rigor, we also provide our reproduced results with the officially reported results in Table 13."
REFERENCES,0.5772357723577236,"Additionally, we evaluate AutoTimes along with other baseline models on recent benchmarks [24]. Results are
presented in Table 15. We also look forward to evaluating on more diverse benchmarks in the future."
REFERENCES,0.5788617886178862,"Table 10: Full long-term forecasting results of one-for-all: we conduct rolling forecasting with a single
model trained on each dataset and accomplish four desired forecast lengths in {96, 192, 336, 720}.
AutoTimes adapt LLMs with the context length C = 672. We set the input length L = 672 and
output length F = 96 in other methods, which are all implemented by their official code."
REFERENCES,0.5804878048780487,Method AutoTimes TimeLLM [15] UniTime [21] FPT [49] iTrans. [22] DLinear [45] PatchTST [26] TimesNet [42]
REFERENCES,0.5821138211382114,Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1
REFERENCES,0.583739837398374,"96
0.360 0.400 0.380 0.412 0.386 0.409 0.377 0.404 0.387 0.418 0.369 0.400 0.374 0.401 0.452 0.463"
REFERENCES,0.5853658536585366,192 0.388 0.419 0.408 0.431 0.639 0.577 0.412 0.426 0.416 0.437 0.405 0.422 0.405 0.422 0.474 0.477
REFERENCES,0.5869918699186992,336 0.401 0.429 0.425 0.443 0.814 0.680 0.440 0.444 0.434 0.450 0.435 0.445 0.423 0.435 0.493 0.489
REFERENCES,0.5886178861788618,720 0.406 0.440 0.434 0.463 0.891 0.719 0.488 0.483 0.447 0.473 0.493 0.508 0.434 0.460 0.560 0.534
REFERENCES,0.5902439024390244,Avg 0.389 0.422 0.412 0.437 0.683 0.596 0.429 0.439 0.421 0.445 0.426 0.444 0.409 0.430 0.495 0.491 ECL
REFERENCES,0.591869918699187,"96
0.129 0.225 0.137 0.244 0.171 0.266 0.137 0.236 0.133 0.229 0.138 0.238 0.132 0.232 0.184 0.288"
REFERENCES,0.5934959349593496,192 0.147 0.241 0.158 0.266 0.293 0.378 0.158 0.258 0.151 0.245 0.152 0.251 0.151 0.250 0.192 0.295
REFERENCES,0.5951219512195122,336 0.162 0.258 0.183 0.292 0.379 0.448 0.181 0.285 0.168 0.262 0.167 0.268 0.171 0.272 0.200 0.303
REFERENCES,0.5967479674796748,720 0.199 0.288 0.247 0.348 0.455 0.502 0.258 0.355 0.205 0.294 0.203 0.302 0.222 0.318 0.228 0.325
REFERENCES,0.5983739837398374,Avg 0.159 0.253 0.181 0.288 0.325 0.399 0.184 0.284 0.164 0.258 0.165 0.265 0.169 0.268 0.201 0.303
REFERENCES,0.6,Weather
REFERENCES,0.6016260162601627,"96
0.153 0.203 0.149 0.200 0.180 0.223 0.154 0.205 0.174 0.225 0.169 0.229 0.149 0.202 0.169 0.228"
REFERENCES,0.6032520325203252,192 0.201 0.250 0.193 0.243 0.450 0.451 0.196 0.243 0.227 0.268 0.211 0.268 0.194 0.245 0.222 0.269
REFERENCES,0.6048780487804878,336 0.256 0.293 0.243 0.284 0.594 0.570 0.244 0.282 0.290 0.309 0.258 0.306 0.244 0.285 0.290 0.310
REFERENCES,0.6065040650406504,720 0.331 0.345 0.315 0.336 0.618 0.593 0.318 0.335 0.374 0.360 0.320 0.362 0.317 0.338 0.376 0.364
REFERENCES,0.608130081300813,Avg 0.235 0.273 0.225 0.266 0.461 0.459 0.228 0.266 0.266 0.291 0.239 0.291 0.226 0.268 0.264 0.293
REFERENCES,0.6097560975609756,Traffic
REFERENCES,0.6113821138211382,"96
0.343 0.248 0.376 0.280 0.438 0.291 0.395 0.283 0.353 0.259 0.399 0.285 0.359 0.255 0.593 0.315"
REFERENCES,0.6130081300813008,192 0.362 0.257 0.397 0.294 0.538 0.353 0.425 0.302 0.373 0.267 0.409 0.290 0.377 0.265 0.596 0.317
REFERENCES,0.6146341463414634,336 0.379 0.266 0.420 0.311 0.621 0.389 0.463 0.328 0.386 0.275 0.422 0.297 0.393 0.276 0.600 0.319
REFERENCES,0.616260162601626,720 0.413 0.284 0.448 0.326 0.737 0.435 0.560 0.392 0.425 0.296 0.461 0.319 0.436 0.305 0.619 0.335
REFERENCES,0.6178861788617886,Avg 0.374 0.264 0.410 0.303 0.584 0.367 0.461 0.326 0.384 0.274 0.423 0.298 0.391 0.275 0.602 0.322
REFERENCES,0.6195121951219512,Solar.
REFERENCES,0.6211382113821138,"96
0.171 0.221 0.224 0.289 0.223 0.274 0.196 0.261 0.183 0.265 0.193 0.258 0.168 0.237 0.180 0.272"
REFERENCES,0.6227642276422765,192 0.190 0.236 0.248 0.315 0.373 0.431 0.219 0.284 0.205 0.283 0.214 0.274 0.189 0.257 0.199 0.286
REFERENCES,0.624390243902439,336 0.203 0.248 0.269 0.338 0.445 0.536 0.245 0.311 0.224 0.299 0.233 0.291 0.212 0.277 0.220 0.301
REFERENCES,0.6260162601626016,720 0.222 0.262 0.310 0.396 0.526 0.608 0.285 0.354 0.239 0.316 0.246 0.307 0.240 0.305 0.251 0.321
REFERENCES,0.6276422764227643,Avg 0.197 0.242 0.263 0.335 0.392 0.462 0.236 0.303 0.213 0.291 0.222 0.283 0.202 0.269 0.213 0.295
REFERENCES,0.6292682926829268,Table 11: Full results of short-term forecasting. We follow the same protocol of TimesNet [42].
REFERENCES,0.6308943089430894,"Method
AutoTimes TimeLLM
FPT
Koopa
N-HiTS N-BEATS PatchTST TimesNet DLinear
FiLM"
REFERENCES,0.6325203252032521,Yearly
REFERENCES,0.6341463414634146,"SMAPE
13.319
13.419
13.531 13.352 13.371
13.866
13.517
13.394
14.012 13.466
MASE
2.993
3.005
3.015
2.997
3.025
3.006
3.031
3.004
3.071
3.059
OWA
0.784
0.789
0.793
0.786
0.790
0.802
0.795
0.787
0.815
0.797"
REFERENCES,0.6357723577235772,Quarterly
REFERENCES,0.6373983739837399,"SMAPE
10.101
10.110
10.177 10.159 10.454
10.689
10.847
10.101
10.758 10.074
MASE
1.182
1.178
1.194
1.189
1.219
1.294
1.315
1.183
1.306
1.163
OWA
0.890
0.889
0.897
0.895
0.919
0.957
0.972
0.890
0.905
0.881"
REFERENCES,0.6390243902439025,Monthly
REFERENCES,0.640650406504065,"SMAPE
12.710
12.980
12.894 12.730 12.794
13.372
14.584
12.866
13.377 12.801
MASE
0.934
0.963
0.956
0.953
0.960
1.014
1.169
0.964
1.021
0.955
OWA
0.880
0.903
0.897
0.901
0.895
0.940
1.055
0.894
0.944
0.893"
REFERENCES,0.6422764227642277,Others
REFERENCES,0.6439024390243903,"SMAPE
4.843
4.795
4.940
4.861
4.696
4.894
6.184
4.982
5.259
5.008
MASE
3.277
3.178
3.228
3.124
3.130
3.358
4.818
3.323
3.608
3.443
OWA
1.026
1.006
1.029
1.004
0.988
1.044
1.140
1.048
1.122
1.070"
REFERENCES,0.6455284552845528,Average
REFERENCES,0.6471544715447154,"SMAPE
11.831
11.983
11.991 11.863 11.960
12.418
13.022
11.930
12.489 11.910
MASE
1.585
1.595
1.600
1.595
1.606
1.656
1.814
1.597
1.690
1.613
OWA
0.850
0.859
0.861
0.858
0.861
0.891
0.954
0.867
0.902
0.862"
REFERENCES,0.6487804878048781,"Table 12: Long-term forecasting results of one-for-one: AutoTimes trains one LLM-based forecaster
to handle all prediction lengths by autoregression, whereas other models are trained respectively on
each prediction length. AutoTimes adapts LLMs with the context length C = 672. The lookback
length is set as L = 672 in others. All results are averaged. Full results is provided in Table 14."
REFERENCES,0.6504065040650406,"Models AutoTimes
TimeLLM [15] UniTime [21]
FPT [49]
iTrans. [22]
DLinear [45]
PatchTST [26] TimesNet [42]"
REFERENCES,0.6520325203252032,"Metric MSE MAE MSE
MAE
MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE"
REFERENCES,0.6536585365853659,ETTh1 0.389 0.422 0.409 0.432 0.438 0.445 0.426 0.438 0.438 0.450 0.423 0.437 0.413 0.431 0.458 0.450
REFERENCES,0.6552845528455284,"ECL
0.159 0.253 0.170 0.275 0.194 0.287 0.167 0.264 0.161 0.256 0.177 0.274 0.159 0.253 0.192 0.295"
REFERENCES,0.656910569105691,Weather 0.235 0.273 0.227 0.266 0.260 0.283 0.231 0.269 0.238 0.272 0.240 0.300 0.226 0.264 0.259 0.287
REFERENCES,0.6585365853658537,"Traffic
0.374 0.264 0.402 0.294 0.460 0.301 0.416 0.295 0.379 0.272 0.434 0.295 0.391 0.264 0.620 0.336"
REFERENCES,0.6601626016260163,"Solar.
0.197 0.242 0.234 0.293 0.254 0.291 0.229 0.296 0.202 0.269 0.217 0.278 0.189 0.257 0.200 0.268"
REFERENCES,0.6617886178861788,Table 13: Results of LLM4TS methods from the original paper and our reproduction by official code.
REFERENCES,0.6634146341463415,"Models AutoTimes
TimeLLM∗[15] TimeLLM [15]
FPT∗[49]
FPT [49]
UniTime∗[21] UniTime [21]"
REFERENCES,0.6650406504065041,"Metric MSE MAE MSE
MAE
MSE
MAE
MSE MAE MSE MAE MSE MAE MSE MAE"
REFERENCES,0.6666666666666666,"ETTh1 0.389 0.422 0.408
0.423
0.409 0.432 0.427 0.426 0.426 0.438 0.442 0.448 0.438 0.445"
REFERENCES,0.6682926829268293,"ECL
0.159 0.253 0.159
0.253
0.170 0.275 0.167 0.263 0.167 0.264 0.216 0.305 0.194 0.287"
REFERENCES,0.6699186991869919,"Weather 0.235 0.273 0.225
0.257
0.227 0.266 0.237 0.270 0.231 0.269 0.253 0.276 0.260 0.283"
REFERENCES,0.6715447154471544,"Traffic
0.374 0.264 0.388
0.264
0.402 0.294 0.414 0.294 0.416 0.295
-
-
0.460 0.301"
REFERENCES,0.6731707317073171,"Solar.
0.197 0.242
-
-
0.234 0.293
-
-
0.229 0.296
-
-
0.254 0.291"
REFERENCES,0.6747967479674797,"1 Methods with ∗means results from the original paper; without ∗means the reproduction.
2 “-” indicates that results are not reported in the original paper."
REFERENCES,0.6764227642276422,"Table 14: Full long-term forecasting results of one-for-one: AutoTimes trains one LLM-based
forecaster to handle all prediction lengths by autoregression, whereas other models are trained
respectively on each prediction length. AutoTimes adapt LLMs with the context length C = 672.
The lookback length is set as L = 672 in others, which are all implemented by their official code."
REFERENCES,0.6780487804878049,"Method One-for-all
Trained respectively on specific lookback/prediction length"
REFERENCES,0.6796747967479675,"AutoTimes
TimeLLM [15] UniTime [21]
FPT [49]
iTrans. [22]
DLinear [45] PatchTST [26] TimesNet [42]"
REFERENCES,0.6813008130081301,Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1
REFERENCES,0.6829268292682927,"96 0.360 0.400 0.380 0.412 0.386 0.409 0.377 0.404 0.386 0.405 0.375 0.399 0.370 0.399 0.384 0.402
192 0.388 0.419 0.405 0.422 0.428 0.436 0.413 0.424 0.422 0.439 0.405 0.416 0.413 0.421 0.557 0.436
336 0.401 0.429 0.422 0.433 0.464 0.456 0.436 0.444 0.444 0.457 0.439 0.443 0.422 0.436 0.491 0.469
720 0.406 0.440 0.430 0.459 0.473 0.479 0.477 0.481 0.500 0.498 0.472 0.490 0.447 0.466 0.521 0.500"
REFERENCES,0.6845528455284553,Avg 0.389 0.422 0.409 0.432 0.438 0.445 0.426 0.438 0.438 0.450 0.423 0.437 0.413 0.431 0.458 0.450 ECL
REFERENCES,0.6861788617886179,"96 0.129 0.225 0.137 0.244 0.171 0.266 0.137 0.236 0.132 0.227 0.153 0.237 0.129 0.222 0.168 0.272
192 0.147 0.241 0.162 0.271 0.178 0.274 0.154 0.251 0.153 0.249 0.152 0.249 0.147 0.240 0.184 0.289
336 0.162 0.258 0.175 0.279 0.194 0.289 0.169 0.267 0.167 0.262 0.169 0.267 0.163 0.259 0.198 0.300
720 0.199 0.288 0.207 0.306 0.232 0.319 0.207 0.300 0.192 0.285 0.233 0.344 0.197 0.290 0.220 0.320"
REFERENCES,0.6878048780487804,Avg 0.159 0.253 0.170 0.275 0.194 0.287 0.167 0.264 0.161 0.256 0.177 0.274 0.159 0.253 0.192 0.295
REFERENCES,0.6894308943089431,Weather
REFERENCES,0.6910569105691057,"96 0.153 0.203 0.149 0.200 0.180 0.223 0.154 0.205 0.163 0.211 0.152 0.237 0.149 0.198 0.172 0.220
192 0.201 0.250 0.195 0.243 0.226 0.261 0.196 0.245 0.205 0.250 0.220 0.282 0.194 0.241 0.219 0.261
336 0.256 0.293 0.245 0.282 0.280 0.300 0.254 0.290 0.254 0.289 0.265 0.319 0.245 0.282 0.280 0.306
720 0.331 0.345 0.318 0.338 0.355 0.348 0.321 0.337 0.329 0.340 0.323 0.362 0.314 0.334 0.365 0.359"
REFERENCES,0.6926829268292682,Avg 0.235 0.273 0.227 0.266 0.260 0.283 0.231 0.269 0.238 0.272 0.240 0.300 0.226 0.264 0.259 0.287
REFERENCES,0.6943089430894309,Traffic
REFERENCES,0.6959349593495935,"96 0.343 0.248 0.373 0.280 0.438 0.291 0.395 0.283 0.351 0.257 0.410 0.282 0.360 0.249 0.593 0.321
192 0.362 0.257 0.390 0.288 0.446 0.293 0.410 0.290 0.364 0.265 0.423 0.287 0.379 0.256 0.617 0.336
336 0.379 0.266 0.407 0.299 0.461 0.300 0.414 0.295 0.382 0.273 0.436 0.296 0.392 0.264 0.629 0.336
720 0.413 0.284 0.438 0.310 0.494 0.318 0.445 0.311 0.420 0.292 0.466 0.315 0.432 0.286 0.640 0.350"
REFERENCES,0.697560975609756,Avg 0.374 0.264 0.402 0.294 0.460 0.301 0.416 0.295 0.379 0.272 0.434 0.295 0.391 0.264 0.620 0.336
REFERENCES,0.6991869918699187,Solar-Energy
REFERENCES,0.7008130081300813,"96 0.171 0.221 0.224 0.289 0.223 0.274 0.196 0.261 0.187 0.255 0.191 0.256 0.168 0.237 0.178 0.256
192 0.190 0.236 0.244 0.289 0.251 0.290 0.224 0.292 0.200 0.270 0.211 0.273 0.187 0.263 0.200 0.268
336 0.203 0.248 0.225 0.291 0.270 0.301 0.240 0.308 0.209 0.276 0.228 0.287 0.196 0.260 0.212 0.274
720 0.222 0.262 0.243 0.301 0.271 0.298 0.256 0.321 0.213 0.276 0.236 0.295 0.205 0.269 0.211 0.273"
REFERENCES,0.7024390243902439,Avg 0.197 0.242 0.234 0.293 0.254 0.291 0.229 0.296 0.202 0.269 0.217 0.278 0.189 0.257 0.200 0.268
REFERENCES,0.7040650406504065,Table 15: Forecasting results on additional benchmark datasets [24] (672-pred-96).
REFERENCES,0.7056910569105691,"Models
AutoTimes
PatchTST
iTransformer
DLinear"
REFERENCES,0.7073170731707317,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE"
REFERENCES,0.7089430894308943,"Australian Electricity
0.150
0.228
0.163
0.242
0.153
0.233
0.167
0.250"
REFERENCES,0.7105691056910569,"Bdg-2 Panther
0.537
0.458
0.565
0.476
0.546
0.462
0.581
0.499"
REFERENCES,0.7121951219512195,"Oikolab Weather
0.603
0.577
0.635
0.603
0.630
0.591
0.663
0.611"
REFERENCES,0.7138211382113822,"D.2
Zero-Shot Forecasting"
REFERENCES,0.7154471544715447,"Following the zero-shot forecasting of FPT [15], each experiment comprises the source and target datasets. We
train a model on the source dataset and apply the model on the target dataset for predictions directly."
REFERENCES,0.7170731707317073,"Notably, the zero-shot scenarios are conducted respectively on the subsets (e.g. M4 Monthly →M3 Monthly)
and the subsets are divided by the sampling frequency but follow different distributions [25]."
REFERENCES,0.71869918699187,"For M4 →M3, which means training on M4 and testing on M3, we directly utilize the same model in the
short-term forecasting experiments reported in Table 11. Considering different horizons in subsets, for M3
Yearly, M3 Quarterly, and M3 Monthly, we directly employ models trained on corresponding subsets of M4 for
testing. As for M3 Others, we test using the model trained on M4 Quarterly to keep the same horizon."
REFERENCES,0.7203252032520325,"For M3 →M4, similarly, for M4 Yearly, M4 Quarterly, and M4 Monthly, we directly employ models trained on
corresponding subsets of M3 for testing. For the remaining subsets, M4 Weekly, M4 Daily, and M4 Hourly, we
perform inference using the model trained on M3 Monthly. Table 16 shows the detailed result."
REFERENCES,0.7219512195121951,"Table 16: Results of zero-shot forecasting. We adopt the same protocol as FPT [49]. M4 →M3 means
training forecasters on M4 datasets and evaluating the performance on M3, and vice versa. Results of
compared baselines are reported from FPT [49]. Lower SMAPE indicates better performance."
REFERENCES,0.7235772357723578,"Method
AutoTimes
FPT
DLinear PatchTST TimesNet NSformer FEDformer Informer Reformer"
REFERENCES,0.7252032520325203,M4 →M3
REFERENCES,0.7268292682926829,"Yearly
15.71
16.42
17.43
15.99
18.75
17.05
16.00
19.70
16.03
Quarterly
9.35
10.13
9.74
9.62
12.26
12.56
9.48
13.00
9.76
Monthly
14.06
14.10
15.65
14.71
14.01
16.82
15.12
15.91
14.80
Others
5.79
4.81
6.81
9.44
6.88
8.13
8.94
13.03
7.53
Average
12.75
13.06
14.03
13.39
14.17
15.29
13.53
15.82
13.37"
REFERENCES,0.7284552845528456,M3 →M4
REFERENCES,0.7300813008130081,"Yearly
13.728
13.740 14.193
13.966
15.655
14.988
13.887
18.542
15.652
Quarterly
10.742
10.787 18.856
10.929
11.877
11.686
11.513
16.907
11.051
Monthly
14.558
14.630 14.765
14.664
16.165
16.098
18.154
23.454
15.604
Others
6.259
7.081
9.194
7.087
6.863
6.977
7.529
7.348
7.001
Average
13.036
13.125 15.337
13.228
14.553
14.327
15.047
19.047
14.092"
REFERENCES,0.7317073170731707,"D.3
Method Generality"
REFERENCES,0.7333333333333333,"Mainstream LLMs predominantly adopt the decoder-only architecture, and AutoTimes can utilize any decoder-
only LLM. We conduct experiments on various types and sizes of LLMs, including GPT-2 [31], multiple sizes
of OPT [46], and LLaMA [36]. Detailed configurations and results are shown in Table 17 and 18, demonstrating
a general trend where performance improves as the model size increases, consistent with the scaling law [16]."
REFERENCES,0.734959349593496,Table 17: Detailed method configurations of AutoTimes for alternative language models.
REFERENCES,0.7365853658536585,"Base LLM
GPT-2 (124M)
OPT-350M
OPT-1.3B
OPT-2.7B
OPT-6.7B
LLaMA-7B"
REFERENCES,0.7382113821138211,"Hidden Dim.
768
1024
2048
2560
4096
4096"
REFERENCES,0.7398373983739838,"Embedding
2-layer MLP
2-layer MLP 2-layer MLP 2-layer MLP 2-layer MLP
Linear"
REFERENCES,0.7414634146341463,"Trainable Param. (M)
0.44
0.58
1.10
1.36
2.15
0.79"
REFERENCES,0.7430894308943089,"D.4
Variable Lookback Length"
REFERENCES,0.7447154471544716,"In the conventional forecasting paradigm, deep forecasters are trained respectively on lookback/forecast lengths,
limiting their applicability to a single lookback length. In contrast, LLMs have the versatility to handle various
input lengths. This capability is derived from Rotary Position Embedding [33] and the next toke prediction
objective, where LLMs are trained with token-wise supervision in Equation 8, that is, the generated token at each
position is supervised. While non-autoregressive LLM4TS methods are typically constrained to a fixed lookback
setting, AutoTimes with a consistent training objective has the flexibility to deal with different lookback lengths.
We present the results in Figure 8, where we adapt the LLM by AutoTimes with the context length of C = 672,
and evaluate the performance with different lookback lengths, which demonstrates the inherited versatility of
LLM-based forecasters. Moreover, the performance is generally improving with increased available lookback
observations, leading to an averaged 9.3% promotion from 384 to 672. By contrast, several works have observed
that the performance of respectively trained deep forecasters does not necessarily improve with the increasing of
lookback length [22, 26, 45]."
REFERENCES,0.7463414634146341,"Table 18: Full Results of alternative LLMs, which are adapted with the context length C = 672."
REFERENCES,0.7479674796747967,"LLM
GPT-2 (124M)
OPT-350M
OPT-1.3B
OPT-2.7B
OPT-6.7B
LLaMA-7B"
REFERENCES,0.7495934959349594,"Metric
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE
MSE
MAE ECL"
REFERENCES,0.751219512195122,"96
0.140
0.236
0.136
0.233
0.132
0.228
0.132
0.227
0.130
0.226
0.129
0.225
192
0.159
0.253
0.154
0.249
0.150
0.245
0.149
0.244
0.148
0.242
0.147
0.241
336
0.177
0.270
0.171
0.267
0.167
0.262
0.167
0.262
0.165
0.260
0.162
0.258
720
0.216
0.303
0.211
0.301
0.206
0.296
0.207
0.297
0.204
0.295
0.199
0.288"
REFERENCES,0.7528455284552845,"Avg
0.173
0.266
0.168
0.263
0.164
0.258
0.164
0.258
0.162
0.256
0.159
0.253 ETTh1"
REFERENCES,0.7544715447154472,"96
0.360
0.397
0.365
0.403
0.357
0.395
0.360
0.398
0.357
0.397
0.360
0.400
192
0.391
0.419
0.395
0.423
0.389
0.417
0.389
0.419
0.386
0.417
0.388
0.419
336
0.408
0.432
0.411
0.434
0.408
0.431
0.404
0.430
0.404
0.429
0.401
0.429
720
0.429
0.452
0.432
0.457
0.430
0.452
0.421
0.447
0.427
0.454
0.406
0.440"
REFERENCES,0.7560975609756098,"Avg
0.397
0.425
0.401
0.429
0.396
0.424
0.394
0.424
0.394
0.424
0.389
0.423"
REFERENCES,0.7577235772357723,Traffic
REFERENCES,0.759349593495935,"96
0.369
0.257
0.371
0.260
0.361
0.253
0.358
0.251
0.357
0.251
0.343
0.248
192
0.394
0.268
0.393
0.270
0.383
0.263
0.380
0.261
0.379
0.261
0.362
0.257
336
0.413
0.278
0.411
0.279
0.402
0.273
0.399
0.271
0.398
0.272
0.379
0.266
720
0.449
0.299
0.446
0.300
0.440
0.295
0.437
0.293
0.437
0.294
0.413
0.284"
REFERENCES,0.7609756097560976,"Avg
0.406
0.276
0.405
0.277
0.397
0.271
0.394
0.269
0.393
0.270
0.374
0.264"
REFERENCES,0.7626016260162601,Weather
REFERENCES,0.7642276422764228,"96
0.158
0.208
0.157
0.208
0.157
0.207
0.157
0.207
0.159
0.209
0.153
0.203
192
0.207
0.254
0.205
0.252
0.207
0.253
0.206
0.253
0.208
0.256
0.201
0.250
336
0.262
0.298
0.261
0.294
0.263
0.296
0.265
0.297
0.268
0.302
0.256
0.293
720
0.342
0.353
0.335
0.346
0.334
0.347
0.344
0.351
0.354
0.360
0.235
0.273"
REFERENCES,0.7658536585365854,"Avg
0.242
0.278
0.240
0.275
0.240
0.276
0.243
0.277
0.247
0.282
0.235
0.273"
REFERENCES,0.767479674796748,"Figure 8: Performance of LLM-based forecasters on the pred-96 scenario, which are adapted by
AutoTimes by the context length C = 672 and directly applied on different lookback lengths."
REFERENCES,0.7691056910569106,"D.5
Timestamps as Position Embeddings"
REFERENCES,0.7707317073170732,"We conduct an ablation study on the proposed position embeddings that integrate timestamps, a prevalent textual
covariate in real-world applications. As shown in Figure 9, the forecasting performance is consistently promoted
across all multivariate datasets and prediction lengths. The steady improvement can be attributed to the fact that
timestamps demote the absolute position of time series segments on the timeline, explicitly aligning different
variates in multivariate scenarios. The increasing promotion with longer prediction length also implies that
chronological information, such as date and periodicity, yields significant benefits for long-term forecasting."
REFERENCES,0.7723577235772358,"Figure 9: Ablation on whether to utilize textual timestamps as the position embedding. Results
of different prediction lengths are provided, where the embedding leads to consistent performance
promotion across all datasets, and the promotion can increase with a longer prediction length."
REFERENCES,0.7739837398373983,"D.6
In-Context Forecasting"
REFERENCES,0.775609756097561,"For in-context forecasting, similar to zero-shot forecasting in Appendix D.2, we train our model using the source
dataset and directly evaluate it on the target dataset. In this task, we first choose M4 as the source dataset and
M3 as the target dataset. It is important to note that the structure of the M3 and M4 datasets differs from typical
datasets used for long-term forecasting. They consist of multiple univariate time sequences of different lengths.
The final part of each sequence serves as the test set, while the preceding part is used for training."
REFERENCES,0.7772357723577236,"Implementation
In zero-shot scenarios, we use a sequence of length F preceding and consecutive to the test
set as input, referred to as the lookback window, where F is the forecast length of each subset. During in-context
forecasting, we concatenate the first 2F time points that belong to the same sequence with the lookback window
as input. We aim to enhance prediction performance by incorporating more contextual information. Too short
sequences (≤4F) are discarded to prevent overlap between the prompt and the lookback window. For a
fair comparison, both zero-shot and in-context forecasting performance are reported on the same remaining
sequences. Figure 12 provides showcases of zero-shot and in-context forecasting."
REFERENCES,0.7788617886178861,"Prompt engineering
Regarding in-context learning, we further delve into the effect of different strategies
to retrieve time series as prompts, which is provided in Table 19. P.1 and P.2 correspond to the zero-shot and
in-context forecasting evaluated in Section 4.3. The prompt of P.3 contains the last 2F time points preceding
the beginning of the lookback window. Another retrieval of prompts as P.4, adopts time series that come from
another uncorrelated time series (out-of-series). We can obtain the following observations:"
REFERENCES,0.7804878048780488,"• P.1 v.s. P.4 indicates that the selected prompt is not suitable, since the prompt does not come from the
earlier observations of the same series to be predicted. Although the context window becomes larger,
the averaged performance will deteriorate because of irrelevant prompts."
REFERENCES,0.7821138211382114,"• P.2 and P.3 indicates that in most cases, selecting the relevant 2F time series from the same series can
provide better contextual information."
REFERENCES,0.7837398373983739,"This highlights the prompt engineering for in-context forecasting. An intuitive suggestion is to utilize consecutive,
inter-periodic, and multiple prompts. To verify this idea, we analyze the periodic effect of time series prompts."
REFERENCES,0.7853658536585366,Table 19: Effects of different strategies to retrieve time series as prompts for in-context forecasting.
REFERENCES,0.7869918699186992,"Context for prediction
Yearly
Quarterly
Monthly
Others
Averaged Err."
REFERENCES,0.7886178861788617,"P.1: Lookback F
21.52
12.03
13.09
8.46
13.61
P.2: Prompt from first 2F + Lookback F
17.03
10.29
12.24
5.33
11.80 ↓
P.3: Prompt from last 2F + Lookback F
16.30
9.59
12.09
6.24
11.48 ↓
P.4: Prompt from 2F of other series + Lookback F
18.95
12.18
14.37
9.46
13.98 ↑"
REFERENCES,0.7902439024390244,"In previous experiments, we adopt M3 and M4 datasets, which are consistent with the zero-shot experiment of
FPT [49], to present the promotion of our in-context paradigm. To provide more rigorous conclusions, we extend
the evaluation to widely recognized datasets. Details of the experiment are as follows: By using a trained model
checkpoint on a source domain (Traffic), we conduct forecasting without gradient update on target ETT datasets.
We evaluate the pred-96 performance on the last variate (OT). For the zero-shot scenario, the input is length-288
lookback series. For in-context forecasting, the input is (length-384 series prompt + length-288 lookback series).
Considering the dataset periodicity, the prompt is uniformly selected as the Ahead-24 (one-day-ahead) series of
the original lookback series. To eliminate the performance boost that comes from extending the input length, we
also provide the results of length-672 lookback series in the zero-shot scenario. Moreover, we further delve into
the effect of different strategies to select time series prompts:"
REFERENCES,0.791869918699187,"• Ahead-Period: The prompt is uniformly selected as the Ahead-24 series of the original lookback
series where 24 is one of the periods (daily period) of ETT."
REFERENCES,0.7934959349593496,• Ahead-Random: The prompt is randomly selected as the previous series of the original series.
REFERENCES,0.7951219512195122,• Fixed Prompt: The prompt is fixed as the first 384 time points in the variate-OT.
REFERENCES,0.7967479674796748,"• Other Variate: The prompt is uniformly selected as Ahead-24 series, but comes from other variates."
REFERENCES,0.7983739837398374,"Results in Table 20 demonstrate the effectiveness of using suitable time series prompts and highlight the influence
of prompt engineering. Using inter-period prompts can outperform simply extending lookback window."
REFERENCES,0.8,"The benefit of the proposed in-context forecasting is to extend the input context of time series forecasting beyond
a continuous lookback window. Since the essence of prompts is to incorporate useful domain-specific knowledge,
here is one use case of in-context forecasting: Considering predicting the weather of one day, one approach is
to extend the lookback length from days to weekends. However, it can also introduce noisy information since"
REFERENCES,0.8016260162601626,Table 20: Strategies to select time series prompts based on periodicity for in-context forecasting.
REFERENCES,0.8032520325203252,"Context for prediction
ETTh1-OT
ETTh2-OT
ETTm1-OT
ETTm2-OT
Average Err."
REFERENCES,0.8048780487804879,"P.0: Zero-Shot (Input-288))
0.0673
0.1637
0.0424
0.1669
0.1101
P.1: Zero-Shot (Input-672)
0.0657
0.1538
0.0415
0.1701
0.1078
P.2: Ahead-Period (Input-672)
0.0645
0.1513
0.0399
0.1629
0.1047
P.3: Ahead-Random (Input-672)
0.0666
0.1621
0.0407
0.1719
0.1103
P.4: Fixed Prompt (Input-672)
0.0769
0.1859
0.0512
0.2104
0.1311
P.5: Other-Variates (Input-672)
0.1263
0.1780
0.0852
0.2297
0.1548"
REFERENCES,0.8065040650406504,"non-stationary meteorological conditions can change with seasons. Another practical way is to consider how
the weather changes on the same day in the last year (or years). Although the input is not continuous, the input
context becomes more relevant based on prior knowledge about the periodicity (yearly). Therefore, in-context
forecasting makes prior knowledge incorporatable and gets performance promotion."
REFERENCES,0.808130081300813,"D.7
Ablation Study"
REFERENCES,0.8097560975609757,"In addition to the ablation study of whether LLMs are useful in AutoTimes (Table 6), we further delve into the
main difference between our method and previous LLM4TS approach and provide a comprehensive ablation
study. The results presented in Table 21 demonstrate that the performance of non-autoregression projection is
consistently inferior to that of our autoregressive AutoTimes approach."
REFERENCES,0.8113821138211382,"Table 21: Ablation study of the autoregression. FlattenHead replaces the segment-wise projection of
AutoTimes by flatten and linear head [26], which is prevalent in non-autoregressive forecasters."
REFERENCES,0.8130081300813008,"Dataset
ETTh1
ECL
Weather
Traffic"
REFERENCES,0.8146341463414634,"Type
AutoTimes
FlattenHead
AutoTimes
FlattenHead
AutoTimes
FlattenHead
AutoTimes
FlattenHead"
REFERENCES,0.816260162601626,"Metric
MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE"
REFERENCES,0.8178861788617886,Pred-96 0.360 0.400 0.385 0.420 0.129 0.225 0.142 0.247 0.153 0.203 0.155 0.209 0.343 0.248 0.367 0.261
REFERENCES,0.8195121951219512,Pred-192 0.388 0.419 0.445 0.463 0.147 0.241 0.157 0.259 0.201 0.250 0.202 0.251 0.362 0.257 0.391 0.282
REFERENCES,0.8211382113821138,Pred-336 0.401 0.429 0.463 0.475 0.162 0.258 0.201 0.311 0.256 0.293 0.257 0.286 0.379 0.266 0.404 0.287
REFERENCES,0.8227642276422764,Pred-720 0.406 0.440 0.574 0.542 0.199 0.288 0.232 0.331 0.331 0.345 0.333 0.261 0.413 0.284 0.432 0.294
REFERENCES,0.824390243902439,"E
Showcases"
REFERENCES,0.8260162601626017,"To facilitate a clear comparison among various models, we present additional prediction showcases for long-term
forecasting and short-term forecasting. These examples are provided by the following models: TimeLLM [15],
FPT [49] and PatchTST [26]. Of all the models, AutoTimes delivers the most accurate future series predictions.
Additionally, we provide the showcases of zero-shot and in-context forecasting in Figure 12."
REFERENCES,0.8276422764227642,"AutoTimes
Time-LLM
FPT
PatchTST"
REFERENCES,0.8292682926829268,Figure 10: Visualization of input-672-predict-96 results on the Traffic dataset.
REFERENCES,0.8308943089430895,"AutoTimes
Time-LLM
FPT
PatchTST"
REFERENCES,0.832520325203252,Figure 11: Visualization of input-36-predict-18 results on the M4 Monthly dataset.
REFERENCES,0.8341463414634146,"Figure 12: Showcases of zero-shot and in-context forecasting. For in-context forecasting, beyond the
lookback window, we uniformly adopt the first 2F time points that belong to the same sequence as the
prompt and concatenate them as the prediction context, which achieves a more accurate prediction."
REFERENCES,0.8357723577235773,"F
Broader Impact"
REFERENCES,0.8373983739837398,"F.1
Impact on Real-world Applications"
REFERENCES,0.8390243902439024,"This paper copes with general-purpose time series forecasting, which is faced with increasing challenges such
as the versatility to handle variable-length scenarios, good generalizability with scarce samples, utilization of
multimodality, and instructive downstream prompts. Since previous studies have demonstrated the feasibility
of leveraging large language models for time series, we propose a simple but effective approach as AutoTimes
to obtain LLM-based forecasters, which keeps the consistency of autoregression. Our model achieves state-of-
the-art performance on forecasting benchmarks and demonstrates remarkable adaptation speed and parameter
efficiency. Besides, advanced capabilities such as multi-step generation and in-context learning are inherited by
the repurposed forecaster. Therefore, the proposed method makes it promising to tackle real-world applications,"
REFERENCES,0.8406504065040651,"which helps our society prevent risks in advance and make better decisions with limited computational budgets.
Our paper mainly focuses on scientific research and has no obvious negative social impact."
REFERENCES,0.8422764227642277,"F.2
Impact on Future Research"
REFERENCES,0.8439024390243902,"In this paper, we find prevalent non-autoregressive LLM4TS methods have inconsistencies in the model structure
and generative approach with LLMs, leading to insufficient utilization of the inherent multi-step token transition.
Given that the generalizability and generative ability of LLMs are largely derived from the autoregressive manner,
the potentials of LLMs may not be fully exhibited in time series forecasting. Therefore, we propose to adapt
LLMs by the consistent training objective, the next token prediction, and accomplish arbitrary-length forecasting
by iterative generation. Beyond the conventional forecasting paradigm, we propose in-context forecasting, where
the context for prediction is extended, and earlier historical time series can be utilized as advantageous prompts.
The compatibility with LLMs and insights from autoregression can be instructive for future LLM4TS research
and the development of foundation time series models."
REFERENCES,0.8455284552845529,"G
Limitation"
REFERENCES,0.8471544715447155,"The proposed method has not supported probabilistic forecasting, since AutoTimes only establishes the mapping
between time series segments to latent embeddings of the LLM, instead of discrete language tokens. Advanced
low-rank adaptation is under exploration in our work, which can further align suitable token transitions as the
future extrapolation of time series. More deftly designed embedding and projection layers are underexplored to
support more compatible tokenization for time series. Besides, it is fascinating to apply AutoTimes on real-world
multimodal time series datasets (such as news-stocks, and logs-measurements), which leaves our future work."
REFERENCES,0.848780487804878,NeurIPS Paper Checklist
CLAIMS,0.8504065040650407,1. Claims
CLAIMS,0.8520325203252033,"Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?"
CLAIMS,0.8536585365853658,Answer: [Yes]
CLAIMS,0.8552845528455284,"Justification: Please refer to Section 1 of the main text, where the claims and contributions are included."
CLAIMS,0.8569105691056911,Guidelines:
CLAIMS,0.8585365853658536,"• The answer NA means that the abstract and introduction do not include the claims made in the
paper.
• The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper."
LIMITATIONS,0.8601626016260162,2. Limitations
LIMITATIONS,0.8617886178861789,Question: Does the paper discuss the limitations of the work performed by the authors?
LIMITATIONS,0.8634146341463415,Answer: [Yes]
LIMITATIONS,0.865040650406504,"Justification: Please refer to Section G of Appendix, where we provide several aspects of limitations."
LIMITATIONS,0.8666666666666667,Guidelines:
LIMITATIONS,0.8682926829268293,"• The answer NA means that the paper has no limitation while the answer No means that the paper
has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these
assumptions might be violated in practice and what the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide
closed captions for online lectures because it fails to handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to address problems
of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
aren’t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms that
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
honesty concerning limitations."
THEORY ASSUMPTIONS AND PROOFS,0.8699186991869918,3. Theory Assumptions and Proofs
THEORY ASSUMPTIONS AND PROOFS,0.8715447154471545,"Question: For each theoretical result, does the paper provide the full set of assumptions and a complete
(and correct) proof?"
THEORY ASSUMPTIONS AND PROOFS,0.8731707317073171,Answer: [NA]
THEORY ASSUMPTIONS AND PROOFS,0.8747967479674796,Justification: The paper does not include theoretical results.
THEORY ASSUMPTIONS AND PROOFS,0.8764227642276423,Guidelines:
THEORY ASSUMPTIONS AND PROOFS,0.8780487804878049,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems."
THEORY ASSUMPTIONS AND PROOFS,0.8796747967479674,"• The proofs can either appear in the main paper or the supplemental material, but if they appear in
the supplemental material, the authors are encouraged to provide a short proof sketch to provide
intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8813008130081301,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8829268292682927,"Question: Does the paper fully disclose all the information needed to reproduce the main experimental
results of the paper to the extent that it affects the main claims and/or conclusions of the paper
(regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8845528455284553,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8861788617886179,"Justification: Please refer to Section B in the main text and code in our public repository, including the
detailed configurations of experiments and the scripts for reproduction."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8878048780487805,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8894308943089431,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data
are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might suffice,
or if the contribution is a specific model and empirical evaluation, it may be necessary to either
make it possible for others to replicate the model with the same dataset, or provide access to
the model. In general. releasing code and data is often one good way to accomplish this, but
reproducibility can also be provided via detailed instructions for how to replicate the results,
access to a hosted model (e.g., in the case of a large language model), releasing of a model
checkpoint, or other means that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should either be
a way to access this model for reproducing the results or a way to reproduce the model (e.g.,
with an open-source dataset or instructions for how to construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5. Open access to data and code"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8910569105691057,"Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8926829268292683,"Answer: [Yes]
Justification: Please refer to Section A of Appendix and code in our public repository, including
dataset descriptions and how to access them."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8943089430894309,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8959349593495934,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be possible,
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
• The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8975609756097561,"• The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
• Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8991869918699187,6. Experimental Setting/Details
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9008130081300812,"Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9024390243902439,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9040650406504065,"Justification: Please refer to Section C and D of Appendix, where we state how hyperparameters are
chosen and the detailed description of experiments."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9056910569105691,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9073170731707317,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9089430894308943,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9105691056910569,"Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-
tion about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9121951219512195,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9138211382113821,"Justification: Please refer to Section B and Table 9 of Appendix, where we report standard deviations
of results with three random seeds."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9154471544715447,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9170731707317074,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main claims
of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
• The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report
a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is
not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9186991869918699,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9203252032520325,"Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9219512195121952,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9235772357723577,"Justification: Please refer to Section B of Appendix, where we provide sufficient information on the
computing resource."
EXPERIMENTS COMPUTE RESOURCES,0.9252032520325203,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.926829268292683,• The answer NA means that the paper does not include experiments.
EXPERIMENTS COMPUTE RESOURCES,0.9284552845528455,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into
the paper)."
CODE OF ETHICS,0.9300813008130081,9. Code Of Ethics
CODE OF ETHICS,0.9317073170731708,"Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code
of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9333333333333333,Answer: [Yes]
CODE OF ETHICS,0.9349593495934959,Justification: We have reviewed and the reasearch conforms with the NeurIPS Code of Ethics.
CODE OF ETHICS,0.9365853658536586,Guidelines:
CODE OF ETHICS,0.9382113821138212,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9398373983739837,10. Broader Impacts
BROADER IMPACTS,0.9414634146341463,"Question: Does the paper discuss both potential positive societal impacts and negative societal impacts
of the work performed?"
BROADER IMPACTS,0.943089430894309,Answer: [Yes]
BROADER IMPACTS,0.9447154471544715,"Justification: Please refer to Section F of Appendix, where we discuss societal impacts and influences
on future research."
BROADER IMPACTS,0.9463414634146341,Guidelines:
BROADER IMPACTS,0.9479674796747968,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied to particular
applications, let alone deployments. However, if there is a direct path to any negative applications,
the authors should point it out. For example, it is legitimate to point out that an improvement in
the quality of generative models could be used to generate deepfakes for disinformation. On the
other hand, it is not needed to point out that a generic algorithm for optimizing neural networks
could enable people to train models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor-
ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the
efficiency and accessibility of ML)."
SAFEGUARDS,0.9495934959349593,11. Safeguards
SAFEGUARDS,0.9512195121951219,"Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or
scraped datasets)?"
SAFEGUARDS,0.9528455284552846,Answer: [NA]
SAFEGUARDS,0.9544715447154472,Justification: The paper poses no such risks.
SAFEGUARDS,0.9560975609756097,Guidelines:
SAFEGUARDS,0.9577235772357724,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere to
usage guidelines or restrictions to access the model or implementing safety filters."
SAFEGUARDS,0.959349593495935,"• Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort."
LICENSES FOR EXISTING ASSETS,0.9609756097560975,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9626016260162602,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
properly credited and are the license and terms of use explicitly mentioned and properly respected?"
LICENSES FOR EXISTING ASSETS,0.9642276422764228,Answer: [Yes]
LICENSES FOR EXISTING ASSETS,0.9658536585365853,Justification: All creators of datasets are properly credited by citations in Section A.
LICENSES FOR EXISTING ASSETS,0.967479674796748,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9691056910569106,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to the asset’s
creators."
NEW ASSETS,0.9707317073170731,13. New Assets
NEW ASSETS,0.9723577235772358,"Question: Are new assets introduced in the paper well documented and is the documentation provided
alongside the assets?"
NEW ASSETS,0.9739837398373984,Answer: [NA]
NEW ASSETS,0.975609756097561,Justification: The paper does not release new assets.
NEW ASSETS,0.9772357723577236,Guidelines:
NEW ASSETS,0.9788617886178862,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
• The paper should discuss whether and how consent was obtained from people whose asset is
used.
• At submission time, remember to anonymize your assets (if applicable). You can either create an
anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9804878048780488,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9821138211382113,"Question: For crowdsourcing experiments and research with human subjects, does the paper include
the full text of instructions given to participants and screenshots, if applicable, as well as details about
compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983739837398374,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9853658536585366,Justification: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9869918699186991,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886178861788618,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
• Including this information in the supplemental material is fine, but if the main contribution of the
paper involves human subjects, then as much detail as possible should be included in the main
paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902439024390244,15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.991869918699187,"Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an
equivalent approval/review based on the requirements of your country or institution) were obtained?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9934959349593496,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951219512195122,Justification: The paper does not involve crowdsourcing nor research with human subjects.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967479674796748,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983739837398374,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly state
this in the paper.
• We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
• For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review."
