Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001422475106685633,"Non-contrastive methods of self-supervised learning (such as BYOL and Sim-
Siam) learn representations by minimizing the distance between two views of the
same image. These approaches have achieved remarkable performance in prac-
tice, but it is not well understood how the representation is learned based on the
augmentation process. Tian et al. (2021) explained why the representation does
not collapse to zero and proposed DirectPred that sets the predictor directly. In
our work, we analyze a generalized version of DirectPred, called DirectSet(Œ±).
We show that in a simple linear network, DirectSet(Œ±) provably learns a desirable
projection matrix and also reduces the sample complexity on downstream tasks.
Our analysis suggests that weight decay acts as an implicit threshold that discard
the features with high variance under augmentation, and keep the features with
low variance. Inspired by our theory, we simplify DirectPred by removing the ex-
pensive eigen-decomposition step. On CIFAR-10, CIFAR-100, STL-10 and Im-
ageNet, DirectCopy, our simpler and more computationally efÔ¨Åcient algorithm,
rivals or even outperforms DirectPred."
INTRODUCTION,0.002844950213371266,"1
INTRODUCTION"
INTRODUCTION,0.004267425320056899,"Self-supervised learning recently emerges as a promising direction to learn representations without
manual labels. While contrastive learning (Oord et al., 2018; Tian et al., 2019; Bachman et al.,
2019; He et al., 2020; Chen et al., 2020a) minimizes the distance of representation between pos-
itive pairs, and maximizes such distances between negative pairs, recently, non-contrastive self-
supervised learning (abbreviated as nc-SSL) is able to learn nontrivial representation with only
positive pairs, using an extra predictor and a stop-gradient operation. Furthermore, the learned
representation shows comparable (or even better) performance for downstream tasks (e.g., image
classiÔ¨Åcation) (Grill et al., 2020; Chen & He, 2020). This brings about two fundamental ques-
tions: (1) why the learned representation does not collapse to trivial (i.e., constant) solutions, and
(2) without negative pairs, what representation nc-SSL learns from the training and how the learned
representation reduces the sample complexity in downstream tasks."
INTRODUCTION,0.005689900426742532,"While many theoretical results on contrastive SSL (Arora et al., 2019; Lee et al., 2020; Tosh et al.,
2020; Wen & Li, 2021) do exist, similar study on nc-SSL has been very rare. As one of the Ô¨Årst
work towards this direction, Tian et al. (2021) show that while the global optimum of the non-
contrastive loss is indeed a trivial one, following gradient direction in nc-SSL, one can Ô¨Ånd a local
optimum that admits a nontrivial representation. Based on their theoretical Ô¨Åndings on gradient-
based methods, they proposed a new approach, DirectPred, that directly sets the predictor using the
eigen-decomposition of the correlation matrix of input before the predictor, rather than updating it
with gradient methods. As a method for nc-SSL, DirectPred shows comparable or better perfor-
mance in multiple datasets, including CIFAR-10 (Krizhevsky et al., 2009), STL-10 (Coates et al.,
2011) and ImageNet (Deng et al., 2009), compared to BYOL (Grill et al., 2020) and SimSiam (Chen
& He, 2020) that optimize the predictor using gradient descent."
INTRODUCTION,0.007112375533428165,"While Tian et al. (2021) address the Ô¨Årst question, i.e., why the learned representation does not
collapse, they do not address the second question, i.e., what representation is learned in nc-SSL and
how the learned representation is related to the data distribution and augmentation process and in
turn whether it reduces the sample complexity in downstream tasks."
INTRODUCTION,0.008534850640113799,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00995732574679943,"Main Contributions.
In this paper, we make a Ô¨Årst attempt towards the second question, by study-
ing a family of algorithms named DirectSet(Œ±), in which the DirectPred algorithm proposed by Tian
et al. (2021) is a special case with Œ± = 1/2. Our contribution is two-folds:"
INTRODUCTION,0.011379800853485065,"First, we perform a theoretical analysis on DirectSet(Œ±) with linear networks. Our analysis shows
that there exists an implicit threshold, determined by weight decay parameter Œ∑, that governs which
features are learned and which are discarded. More speciÔ¨Åcally, the threshold is applied to the vari-
ance of the feature across different data augmentations (or ‚Äúviews‚Äù) of the same instance: nuisance
features (features with high variances under augmentation) are discarded, while invariant features
(i.e., with low variances) are kept. We further make a formal statement on the sample complexity
of the learning process and performance guarantees of the downstream tasks, in the linear setting
similar to Tian et al. (2021). To our knowledge, this is the Ô¨Årst sample complexity result in nc-SSL."
INTRODUCTION,0.012802275960170697,"Second, we show that DirectCopy, a special case of DirectSet(Œ±) when Œ± = 1, performs comparably
with (or even outperforms) DirectPred in downstream tasks in CIFAR-10, CIFAR-100, STL-10
and ImageNet. In DirectCopy, the predictor can be set without the expensive eigen-decomposition
operation, which makes DirectCopy much simpler and more efÔ¨Åcient than DirectPred."
INTRODUCTION,0.01422475106685633,"Related works. In nc-SSL, different techniques are proposed to avoid collapsing. BYOL and Sim-
Siam use an extra predictor and stop gradient operation. Beyond these, BatchNorm (including its
variants (Richemond et al., 2020)), de-correlation (Zbontar et al., 2021; Bardes et al., 2021; Hua
et al., 2021), whitening (Ermolov et al., 2021), centering (Caron et al., 2021), and online clus-
tering (Caron et al., 2020) are all effective ways to enforce implicit contrastive constraints among
samples for collapsing prevention. We study BYOL and SimSiam as representative nc-SSL methods."
INTRODUCTION,0.015647226173541962,"Organization. The paper is organized as follows. Section 2-3 introduce DirectSet(Œ±), prove it
learns a projection matrix onto the invariant features, and the learned representation reduces sample
complexity in downstream tasks. Section 4 demonstrates that DirectCopy achieves comparable or
even better performance than the original DirectPred algorithm in various datasets, and Section 5
shows ablation experiments. Finally, limitation and future works are discussed in Section 6-7."
PRELIMINARIES,0.017069701280227598,"2
PRELIMINARIES"
NOTATIONS,0.01849217638691323,"2.1
NOTATIONS
We use Id to denote the d√ód identity matrix and simply write I when the dimension is clear. For any
linear subspace S in Rd, we use PS ‚ààRd√ód to denote the projection matrix on S. More precisely,
the projection matrix PS equals UU ‚ä§, where the columns of U constitute a set of orthonormal bases
for subspace S. We use N(¬µ, Œ£) to denote the Gaussian distribution with mean ¬µ and covariance Œ£."
NOTATIONS,0.01991465149359886,"We use ‚à•¬∑‚à•to denote spectral norm for a matrix, or ‚Ñì2 norm for a vector and use ‚à•¬∑‚à•F to denote
Frobenius norm for a matrix. For a real symmetric matrix A ‚ààRd√ód whose eigen-decomposition is
Pd
i=1 Œªiuiu‚ä§
i , we use |A| to denote Pd
i=1 |Œªi| uiu‚ä§
i . If A is also positive semi-deÔ¨Ånite, we use AŒ±"
NOTATIONS,0.021337126600284494,"to denote Pd
i=1 ŒªŒ±
i uiu‚ä§
i for any positive Œ± ‚ààR."
NOTATIONS,0.02275960170697013,"2.2
DIRECTSET(Œ±) AND DIRECTCOPY"
NOTATIONS,0.02418207681365576,"In nc-SSL, recent methods as BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020) employ a
dual pair of Siamese networks (Bromley et al., 1994): one side is a composition of an online network
(including a projector) and a predictor network, the other side is a target network (see Figure 1 for
a simple example). The target network has the same architecture as the online network, but has
potentially different weights. Given an input x, two augmented views x1, x2 are generated, and the
network is trained to match the representation of x1 (through the online network and the predictor
network) and the representation of x2 (through the target network). More precisely, suppose the
online network and the target network are two mappings fŒ∏, fŒ∏a : Rd 7‚ÜíRh and the predictor
network is a mapping gŒ∏p : Rh 7‚ÜíRh, the network is trained to minimize the following loss:"
NOTATIONS,0.025604551920341393,"L(Œ∏, Œ∏p, Œ∏a) := 1"
NOTATIONS,0.02702702702702703,"2Ex1,x2"
NOTATIONS,0.02844950213371266,"gŒ∏p (fŒ∏(x1))
gŒ∏p (fŒ∏(x1))
 ‚àíStopGrad
 fŒ∏a(x2)"
NOTATIONS,0.029871977240398292,‚à•fŒ∏a(x2)‚à•  2 .
NOTATIONS,0.031294452347083924,"In BYOL and SimSiam, the online network and the target network are trained by running gradient
methods on L. The target network is not trained by gradient methods; instead, it is directly set with"
NOTATIONS,0.032716927453769556,Under review as a conference paper at ICLR 2022
NOTATIONS,0.034139402560455195,"Online
ùëä(ùúÉ)"
NOTATIONS,0.03556187766714083,"Target
ùëä! (ùúÉ!)"
NOTATIONS,0.03698435277382646,‚Ñì% loss
NOTATIONS,0.03840682788051209,Predictor
NOTATIONS,0.03982930298719772,"ùëä"" (ùúÉ"")"
NOTATIONS,0.041251778093883355,Stop-Gradient ùë•
NOTATIONS,0.04267425320056899,"ùë•#
ùë•$
Augmentation"
NOTATIONS,0.044096728307254626,"ùëì!(ùë•"")
ùëì!!(ùë•#)"
NOTATIONS,0.04551920341394026,"ùëî!""(ùëì! ùë•"" )"
NOTATIONS,0.04694167852062589,"Gradient 
methods"
NOTATIONS,0.04836415362731152,EMA of ùëä
NOTATIONS,0.049786628733997154,"BYOL:
Gradient 
methods"
NOTATIONS,0.051209103840682786,DirectSet(ùõº):
NOTATIONS,0.05263157894736842,"ùëä"" =
ùêπ%"
NOTATIONS,0.05405405405405406,"‚à•ùêπ% ‚à•+ ùúñùêº,"
NOTATIONS,0.05547652916073969,with ùêπ= ùîº&!ùëì' ùë•# ùëì' ùë•# (
NOTATIONS,0.05689900426742532,Figure 1: Problem Setup. Comparison between BYOL and DirectSet(Œ±) on a linear network.
NOTATIONS,0.05832147937411095,"the weights in the online network (Chen & He, 2020) or an exponential moving average (EMA) of
the online network (Grill et al., 2020; He et al., 2020; Chen et al., 2020b)."
NOTATIONS,0.059743954480796585,"The DirectSet(Œ±) algorithm, as shown in Figure 1, directly sets the predictor based on the correlation
matrix F of the predictor inputs:"
NOTATIONS,0.06116642958748222,"Wp =
F Œ±"
NOTATIONS,0.06258890469416785,"‚à•F Œ±‚à•+ œµI,"
NOTATIONS,0.06401137980085349,"where F = Ex1fŒ∏(x1)fŒ∏(x1)‚ä§. In practice, F is estimated by a moving average over batches. That
is, ÀÜF = ¬µ ÀÜF + (1 ‚àí¬µ)EB[fŒ∏(x1)fŒ∏(x1)‚ä§], where EB is the expectation over one batch."
NOTATIONS,0.06543385490753911,"In the original DirectPred proposed by Tian et al. (2021), Œ± is Ô¨Åxed at 1/2. To compute ÀÜF 1/2, one
needs to Ô¨Årst compute the eigen-decomposition of ÀÜF, and then taking the root of each eigenvalue.
This step of eigen-decomposition can be expensive especially when the representation dimension
h is high. To avoid the eigen-decomposition step, we propose DirectCopy (Œ± = 1), in which the
predictor Wp is a direct copy of the ÀÜF (with normalization and regularization)1. As we shall see,
DirectCopy enjoys both theoretical guarantees and strong empirical performance."
NOTATIONS,0.06685633001422475,"3
THEORETICAL ANALYSIS OF DIRECTSET(Œ±)"
NOTATIONS,0.06827880512091039,"Deep linear networks have been widely used as a tractable theoretical model for studying nonconvex
loss landscapes (Kawaguchi, 2016; Du & Hu, 2019; Laurent & Brecht, 2018) and nonlinear learning
dynamics (Saxe et al., 2013; 2019; Lampinen & Ganguli, 2018; Arora et al., 2018). However, most
of them are for supervised learning setting. Tian et al. (2021) analyzed nc-SSL on a linear network,
but did not analyze their proposed approach DirectPred. Here, we analyze the representation learn-
ing process of DirectSet(Œ±) on a minimal setting where the online network fŒ∏ is a single linear layer.
We also verify DirectSet(Œ±) works for practical nonlinear deep models and realistic datasets."
SETUP,0.06970128022759602,"3.1
SETUP"
SETUP,0.07112375533428165,"In this subsection, we deÔ¨Åne the network model, data distribution and simplify DirectSet(Œ±) algo-
rithm for our theoretical analysis. We consider the following network model (see Figure 1),
Assumption 1 (Linear network model). The online, predictor and target network are all single-layer
linear network without bias, with weight matrices denoted as W, Wp, Wa ‚ààRd√ód respectively."
SETUP,0.07254623044096728,"For the data distribution, we assume the input space is a direct sum of a invariant feature subspace
and a nuisance feature subspace. SpeciÔ¨Åcally, we assume
Assumption 2 (Data distribution). The input x is sampled from N(0, Id), and its augmented view
x1, x2 are independently sampled from N(x, œÉ2PB), where B is a (d ‚àír)-dimensional subspace.
We denote S as the orthogonal subspace of B in Rd."
SETUP,0.07396870554765292,"1Computing the spectral norm of ÀÜF is much faster than computing the eigen-decomposition of ÀÜF, because
the former only needs the top eigen-vector of ÀÜF. Table 4 shows that the spectral norm can also be replaced by
Frobenius norm or no normalization, and similar performance can be achieved."
SETUP,0.07539118065433854,Under review as a conference paper at ICLR 2022
SETUP,0.07681365576102418,"In this simple data distribution, subspace S corresponds to the features that are invariant to augmen-
tations and its orthogonal subspace B is the nuisance subspace which the augmentation changes. We
will prove that DirectSet(Œ±) can learn the projection matrix onto S subspace. Note in the previous
work (Tian et al., 2021), they assumed the covariance of the augmentation distribution to be œÉ2I and
did not study what representation is learned."
SETUP,0.07823613086770982,"For the convenience of analysis, we consider a simpliÔ¨Åed version of DirectSet(Œ±). We compute the
loss function without normalizing the two representations, so the population loss is"
SETUP,0.07965860597439545,"L(W, Wa, Wp) := 1"
SETUP,0.08108108108108109,"2Ex1,x2 ‚à•WpWx1 ‚àíStopGrad (Wax2)‚à•2 ,
(1)"
SETUP,0.08250355618776671,and the empirical loss is
SETUP,0.08392603129445235,"ÀÜL(W, Wp, Wa) := 1"
N,0.08534850640113797,"2n n
X i=1"
N,0.08677098150782361,"WpWx(i)
1 ‚àíStopGrad(Wax(i)
2 )

2
,
(2)"
N,0.08819345661450925,"where x(i)‚Äôs are independently sampled from N(0, I), and augmented views x(i)
1
and x(i)
2
are inde-
pendently sampled from N(x(i), œÉ2PB). To train our model, Ô¨Årst, we initialize W as Œ¥I with Œ¥ a
positive real number. We run gradient Ô¨Çow or gradient descent on online network W with weight
decay Œ∑, and set the the target network Wa = W. For clarity of presentation, when training on the
population loss, we set Wp as (WExxx‚ä§W ‚ä§)Œ± = (WW ‚ä§)Œ± instead of (WEx1x1x‚ä§
1 W ‚ä§)Œ± as in
practice; when training on the empirical loss, we set Wp as (W 1"
N,0.08961593172119488,"n
Pn
i=1 x(i)[x(i)]‚ä§W ‚ä§)Œ±. Here, we
set the predictor regularization œµ = 0 and its inÔ¨Çuence will be studied in Section 5."
N,0.09103840682788052,"In the following, DirectSet(Œ±) is shown to recover the projection matrix PS with polynomial number
of samples. Furthermore, given that the learned matrix is close to PS, the sample complexity on
downstream tasks is reduced."
GRADIENT FLOW ON POPULATION LOSS,0.09246088193456614,"3.2
GRADIENT FLOW ON POPULATION LOSS"
GRADIENT FLOW ON POPULATION LOSS,0.09388335704125178,"In this section, we show that DirectSet(Œ±) running on the population loss with inÔ¨Ånitesimal learning
rate and Œ∑ weight decay can learn the projection matrix onto the invariant feature subspace S.
Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-
tion 2, respectively. Suppose we initialize online network W as Œ¥I, and run DirectSet(Œ±) on popu-
lation loss (see Eqn. 1) with inÔ¨Ånitesimal step size and Œ∑ weight decay. If we set the weight decay"
GRADIENT FLOW ON POPULATION LOSS,0.0953058321479374,"coefÔ¨Åcient Œ∑ ‚àà

1
4(1+œÉ2), 1"
GRADIENT FLOW ON POPULATION LOSS,0.09672830725462304,"4

and initialization scale Œ¥ >

1‚àí‚àö1‚àí4Œ∑"
GRADIENT FLOW ON POPULATION LOSS,0.09815078236130868,"2
1/(2Œ±)
, then W converges to

1+‚àö1‚àí4Œ∑"
GRADIENT FLOW ON POPULATION LOSS,0.09957325746799431,"2
1/(2Œ±)
PS when time goes to inÔ¨Ånity."
GRADIENT FLOW ON POPULATION LOSS,0.10099573257467995,"Theorem 1 shows that when the weight decay is in certain range, and when the initialization is
large enough, the online network can converge to the desired projection matrix PS 2. In sequel,
we explain how the dynamics of W leads to a projection matrix and how the weight decay and
initialization scale come into play. We leave the full proof in Appendix B.1. We also consider the
setting when Wp is set as (WEx1x1x‚ä§
1 W ‚ä§)Œ± in Appendix B.4 and extend the result to deep linear
networks in Appendix C."
GRADIENT FLOW ON POPULATION LOSS,0.10241820768136557,"Due to the identity initialization, we can ensure that W is always a real symmetric matrix and is
simultaneously diagonalizable with PB. We can then analyze the evolution of each eigenvalue in W
separately. Under our assumptions, it turns out that all the eigenvalues whose eigenvectors lie in the
B subspace share the same value ŒªB, and all the eigenvalues in the S subspace share the value ŒªS
as shown in the following time dynamics:"
GRADIENT FLOW ON POPULATION LOSS,0.10384068278805121,"ÀôŒªB = ŒªB
h
‚àí(1 + œÉ2) |ŒªB|4Œ± + |ŒªB|2Œ± ‚àíŒ∑
i
,
ÀôŒªS = ŒªS
h
‚àí|ŒªS|4Œ± + |ŒªS|2Œ± ‚àíŒ∑
i
.
(3)"
GRADIENT FLOW ON POPULATION LOSS,0.10526315789473684,"Next, we show ŒªB converges to zero and ŒªS converges to a positive number, which immediately
implies that W converges to some scaling of PS."
GRADIENT FLOW ON POPULATION LOSS,0.10668563300142248,"2Note that Theorem 1 also holds with negative initialization Œ¥ < ‚àí

1‚àí‚àö1‚àí4Œ∑"
GRADIENT FLOW ON POPULATION LOSS,0.10810810810810811,"2
1/(2Œ±)
, in which case W"
GRADIENT FLOW ON POPULATION LOSS,0.10953058321479374,"converges to ‚àí

1+‚àö1‚àí4Œ∑"
GRADIENT FLOW ON POPULATION LOSS,0.11095305832147938,"2
1/(2Œ±)
PS. Our other results can be extended to negative Œ¥ in a similar way."
GRADIENT FLOW ON POPULATION LOSS,0.112375533428165,"Under review as a conference paper at ICLR 2022 O ÃáùúÜ! O ÃáùúÜ"" ùúÜ"" #
ùúÜ"" $"
GRADIENT FLOW ON POPULATION LOSS,0.11379800853485064,"Good
Basin 
Bad
Basin ùúÜ"" ùúÜ!"
GRADIENT FLOW ON POPULATION LOSS,0.11522048364153627,"Figure 2: Left: With appropriate weight decay, ŒªB always converge to zero; ŒªS converges to zero when it‚Äôs
initialized in the bad basin and converges to positive Œª+
S when it‚Äôs initialized in the good basin. Middle: The
evolvement of the eigenvalues of F when it‚Äôs trained by DirectCopy with œµ = 0.2 on STL-10. With weight
decay Œ∑ = 0.0004 (bottom), the eigen-spectrum at epoch 95 has sharp drop; while the drop is much milder
when Œ∑ = 0 (top). Right: Similar phenomenon on CIFAR-10 with œµ = 0.3."
GRADIENT FLOW ON POPULATION LOSS,0.1166429587482219,"Similar as the analysis in Tian et al. (2021), when Œ∑ >
1
4(1+œÉ2), we know ÀôŒªB < 0 for any ŒªB > 0
and ŒªB = 0 is a stable stationary point, as illustrated in Figure 2 (top, left). Therefore, as long as
Œ∑ >
1
4(1+œÉ2), ŒªB must converge to zero. When 0 < Œ∑ < 1"
GRADIENT FLOW ON POPULATION LOSS,0.11806543385490754,"4, there are three non-negative solutions"
GRADIENT FLOW ON POPULATION LOSS,0.11948790896159317,"to ÀôŒªS = 0, which are 0, Œª‚àí
S =

1‚àí‚àö1‚àí4Œ∑"
GRADIENT FLOW ON POPULATION LOSS,0.12091038406827881,"2
1/(2Œ±)
and Œª+
S =

1+‚àö1‚àí4Œ∑"
GRADIENT FLOW ON POPULATION LOSS,0.12233285917496443,"2
1/(2Œ±)
. As illustrated in"
GRADIENT FLOW ON POPULATION LOSS,0.12375533428165007,"Figure 2 (bottom, left), if initialization Œ¥ > Œª‚àí
S (good basin), ŒªS converges to a positive value Œª+
S ;
if 0 < Œ¥ < Œª‚àí
S (bad basin), ŒªS converges to zero."
GRADIENT FLOW ON POPULATION LOSS,0.1251778093883357,"Thresholding role of weight decay in feature learning:
While Tian et al. (2021) shows why nc-
SSL does not collapse, one key question is how nc-SSL learns useful features and how the method
determines which feature is learned. Now it is clear: the weight decay factor Œ∑ makes a call on what
features should be learned. Nuisance features subject to signiÔ¨Åcant change under data augmentation
has larger variance œÉ2 and
1
4(1+œÉ2) < Œ∑, the eigenspace corresponds to this feature goes to zero;
on the other hand, invariant features that are robust to data augmentation has much smaller œÉ2 and
1
4(1+œÉ2) > Œ∑ and these features are kept. In our above analysis, B subspace corresponds to the
nuisance features and collapses to zero; S subspace corresponds to the invariant features (whose
variance was assumed as zero for simplicity) and is kept after training."
GRADIENT FLOW ON POPULATION LOSS,0.12660028449502134,"Figure 2 (middle and right) shows the spectrum of F (which is the correlation matrix of the predictor
inputs) when the network is trained by DirectSet(1) with and without weight decay Œ∑ on STL10
and CIFAR10: when Œ∑ = 0, the eigen-spectrum of F in later epochs does not have a sharp drop
compared with the case of Œ∑ = 0.0004. This means that the nuisance features are not signiÔ¨Åcantly
suppressed when Œ∑ = 0."
GRADIENT FLOW ON POPULATION LOSS,0.12802275960170698,"Therefore, it is crucially important to choose weight decay appropriately: a too small Œ∑ may not be
sufÔ¨Åcient to suppress the nuisance features; a too large Œ∑ can also collapse the invariant features. As
shown in Section 5, both cases lead to worse downstream performance."
GRADIENT DESCENT ON EMPIRICAL LOSS,0.12944523470839261,"3.3
GRADIENT DESCENT ON EMPIRICAL LOSS"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13086770981507823,"In this section, we then proceed to prove that DirectCopy (one special case of DirectSet(Œ±) with
Œ± = 1) successfully learns the projection matrix given polynomial number of samples.
Theorem 2. Suppose network architecture and data distribution are as deÔ¨Åned in Assumption 1
and Assumption 2, respectively. Suppose we initialize online network as Œ¥I, and run DirectCopy
on empirical loss (see Eqn. 2) with Œ≥ step size and Œ∑ weight decay. Suppose the noise scale œÉ2"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13229018492176386,"is a positive constant, the weight decay coefÔ¨Åcient Œ∑ ‚àà

1+œÉ2/4
4(1+œÉ2), 1+3œÉ2/4"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.1337126600284495,"4(1+œÉ2)

and the initialization"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13513513513513514,Under review as a conference paper at ICLR 2022
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13655761024182078,"scale Œ¥ is a constant at least 1/
‚àö"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.1379800853485064,"2. Choose the step size Œ≥ as a small enough constant. For any
accuracy ÀÜœµ > 0, given n ‚â•poly(d, 1/ÀÜœµ) number of samples, with probability at least 0.99 there
exists t = O(log(1/ÀÜœµ)) such that (here f
Wt is the online network weights at the t-th step):

f
Wt ‚àí r"
GRADIENT DESCENT ON EMPIRICAL LOSS,0.13940256045519203,1 + ‚àö1 ‚àí4Œ∑
PS,0.14082503556187767,"2
PS ‚â§ÀÜœµ."
PS,0.1422475106685633,"The proof proceeds by Ô¨Årst proving that gradient descent on the population loss converges in linear
rate and then couples the gradient descent dynamics on empirical loss and that on population loss.
See the detailed proof in Appendix B.2."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14366998577524892,"3.4
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14509246088193456,"In this section, we show that the learned representations can indeed reduce the sample complexity
on the downstream tasks. We consider the following data distribution for the down-stream task:"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.1465149359886202,"Assumption 3 (Downstream data distribution). Each input x(i) is sampled from N(0, Id) and its
label y(i) =

x(i), w‚àó
+ Œæ(i), where w‚àóis the ground truth vector with unit ‚Ñì2 norm and Œæ(i) is
independently sampled from N(0, Œ≤2). We assume the ground truth w‚àólies on an r-dimensional
subspace S and we denote the projection matrix on subspace S simply as P."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14793741109530584,"In practice, usually the semantically relevant features (S subspace here) are invariant to augmenta-
tions and the nuisance features (orthogonal subspace of S) have high variance under augmentations.
Therefore, by previous analysis, we expect DirectSet(Œ±) to learn the projection matrix P."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.14935988620199148,"Suppose {(x(i), y(i))}n
i=1 are n training samples. Each input x(i) is transformed by a matrix ÀÜP ‚àà
Rd√ód (for example the learned online network W) to get its representation ÀÜPx(i). The regularized"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.1507823613086771,"loss is then deÔ¨Åned as ÀÜL(w) :=
1
2n
Pn
i=1

D
ÀÜPx(i), w
E
‚àíy(i)
2
+ œÅ"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15220483641536273,"2 ‚à•w‚à•2 . In the below theorem,"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15362731152204837,"we show that when
P ‚àíÀÜP

F is small, the above ridge regression can recover the ground truth w‚àó"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.155049786628734,given only O(r) number of samples.
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15647226173541964,"Theorem 3. Suppose the downstream data distribution is as deÔ¨Åned in Assumption 3. Suppose
 ÀÜP ‚àíP

F ‚â§ÀÜœµ with ÀÜœµ < 1. Choose the regularizer coefÔ¨Åcient œÅ = ÀÜœµ1/3. For any Œ∂ < 1/2, given"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.15789473684210525,"n ‚â•O(r +log(1/Œ∂)) number of samples, with probability at least 1‚àíŒ∂, the training loss minimizer
ÀÜw satisÔ¨Åes
 ÀÜP ÀÜw ‚àíw‚àó ‚â§O "
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.1593172119487909,"ÀÜœµ1/3 + Œ≤
‚àör +
p"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16073968705547653,"log(1/Œ∂)
‚àön ! ."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16216216216216217,"In the above theorem, when n is at least O

Œ≤2(r+log(1/Œ∂))"
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16358463726884778,"ÀÜœµ2/3

, we have
 ÀÜP ÀÜw ‚àíw‚àó ‚â§O(ÀÜœµ1/3)."
SAMPLE COMPLEXITY ON DOWNSTREAM TASKS,0.16500711237553342,"Note that if we directly estimate ÀÜw without transforming the inputs by ÀÜP, we need ‚Ñ¶(d) number
of samples to ensure that ‚à•ÀÜw ‚àíw‚àó‚à•‚â§o(1) (Wainwright, 2019). The proof of Theorem 3 follows
from bounding the difference between ÀÜP ÀÜw and w‚àóby matrix concentration inequalities and matrix
perturbation bounds. The full proof is in Appendix B.3."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.16642958748221906,"4
EMPIRICAL PERFORMANCE OF DIRECTCOPY"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1678520625889047,"In the previous analysis, we show DirectSet(Œ±), and in particular DirectCopy (DirectSet(Œ±) with
Œ± = 1), could recover the input feature structure with polynomial samples and make the down-
stream task more sample efÔ¨Åcient in a simple linear setting. Compared with the original DirectPred
(DirectSet(Œ±) with Œ± = 1/2), DirectCopy is a simpler and computationally more efÔ¨Åcient algorithm
since it directly set the predictor as the correlation matrix F, without the eigen-decomposition step.
By our analysis in Theorem 1, DirectCopy also learns the projection matrix PS with larger scale 3"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.16927453769559034,"3Recall that in Theorem 1 under DirectSet(Œ±), online matrix W converges to

1+‚àö1‚àí4Œ∑"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17069701280227595,"2
1/(2Œ±)
PS. So
with a larger Œ±, the scalar in front of PS becomes larger."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1721194879089616,Under review as a conference paper at ICLR 2022
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17354196301564723,"compared with DirectPred, which suggests that the invariant features learned by DirectCopy are
stronger and more distinguishable. Next, we show that DirectCopy is on par with (or even outper-
forms) the original DirectPred in various datasets, when coupling with deep nonlinear models on
real datasets."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17496443812233287,"4.1
RESULTS ON STL-10, CIFAR-10 AND CIFAR-100"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1763869132290185,"We use ResNet-18 (He et al., 2016) as the backbone network, a two-layer nonlinear MLP as the
projector, and a linear predictor. Unless speciÔ¨Åed otherwise, SGD is used as the optimizer with
weight decay Œ∑ = 0.0004. To evaluate the quality of the pre-trained representations, we follow
the linear evaluation protocol. Each setting is repeated 5 times to compute the mean and standard
deviation. The accuracy is reported as ‚Äúmean¬±std‚Äù. Unless explicitly speciÔ¨Åed, we use learning rate
Œ≥ = 0.01, regularization œµ = 0.2 on STL-10; Œ≥ = 0.02, œµ = 0.3 on CIFAR-10 and Œ≥ = 0.03, œµ = 0.3
on CIFAR-100. See more detailed experiment settings in Appendix A."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17780938833570412,"Num of epochs
100
300
500
STL-10
DirectCopy
77.83¬±0.56
82.01¬±0.28
82.95¬±0.29
DirectPred
77.86¬±0.16
78.77¬±0.97
78.86¬±1.15
DirectPred (freq=5)
77.54¬±0.11
79.90¬±0.66
80.28¬±0.62
SGD baseline
75.06¬±0.52
75.25¬±0.74
75.25¬±0.74
CIFAR-10
DirectCopy
84.02¬±0.37
89.17¬±0.12
89.62¬±0.10
DirectPred
85.21¬±0.23
88.88¬±0.15
89.52¬±0.04
DirectPred (freq=5)
84.93¬±0.29
88.83¬±0.10
89.56¬±0.13
SGD baseline
84.49¬±0.20
88.57¬±0.15
89.33¬±0.27
CIFAR-100
DirectCopy
55.40¬±0.19
61.06¬±0.14
62.23¬±0.06
DirectPred
56.60¬±0.27
61.65¬±0.18
62.68¬±0.35
DirectPred (freq=5)
56.43¬±0.21
62.01¬±0.22
63.15¬±0.27
SGD baseline
54.94¬±0.50
60.88¬±0.59
61.42¬±0.89"
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.17923186344238975,"Table 1: STL-10/CIFAR-10/CIFAR-100 Top-1 accuracy of DirectCopy.
The numbers for DirectPred, DirectPred (freq=5) and SGD baseline on
STL-10/CIFAR-10 are obtained from Tian et al. (2021)."
EMPIRICAL PERFORMANCE OF DIRECTCOPY,0.1806543385490754,epochs
"IMAGENET
DIRECTCOPY",0.18207681365576103,"100
ImageNet
DirectCopy
68.8
DirectPred
68.5
SGD Baseline
68.6
Table 2: ImageNet Top-1 accu-
racy of DirectCopy, DirectPred
and BYOL baseline."
"IMAGENET
DIRECTCOPY",0.18349928876244664,"STL-10:
We evaluate the quality of the learned representation after each epoch, and report the
best accuracy in the Ô¨Årst 100/300/500 epochs in Table 1. DirectCopy achieves substantially better
performance than the original DirectPred and SGD baseline, especially when trained with longer
epochs. DirectPred (freq=5) means the predictor is set by DirectPred every 5 batchs, and is trained
with gradient updates in other batchs, which outperforms DirectPred in later epochs, but is still much
worse than DirectCopy. The SGD baseline is obtained by training the linear predictor using SGD."
"IMAGENET
DIRECTCOPY",0.18492176386913228,"CIFAR-10/100:
For CIFAR-10, DirectCopy is slighly worse than DirectPred at epoch 100, but
catches up and gets even better performance in epoch 300 and 500 (Table 1). For CIFAR-100, at
earlier epochs, the performance of DirectCopy is not as good as DirectPred, but the gap gradually
diminishes in later epochs. Both DirectCopy and DirectPred outperfoms the SGD baseline. Direct-
Pred (freq=5) achieves even better performance, but at the cost of a more complicated algorithm."
RESULTS ON IMAGENET,0.18634423897581792,"4.2
RESULTS ON IMAGENET"
RESULTS ON IMAGENET,0.18776671408250356,"Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-layer MLP as
the projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs. See
more detailed experiment settings in Appendix A."
RESULTS ON IMAGENET,0.1891891891891892,"For fairness, we compare DirectCopy to the gradient-based baseline which uses the same-sized linear
predictor as ours. As shown in Table 2, at 100-epoch, this baseline achieves 68.6 top-1 accuracy,
which is already signiÔ¨Åcantly higher than BYOL with two-layer predictors reported in the literature
(e.g., Chen & He (2020) reports 66.5 top-1 under 100-epoch training). DirectCopy using normalized
F accumulated with EMA ¬µ = 0.99 on the correlation matrix, regularization parameter œµ = 0.01
achieves 68.8 under the same setting, better than this strong baseline. In contrast, DirectPred (Tian
et al., 2021) achieves 68.5, slightly lower than the linear baseline."
RESULTS ON IMAGENET,0.1906116642958748,"Under review as a conference paper at ICLR 2022 O ÃáùúÜ"" ùúÜ"" #
ùúÜ"" $"
RESULTS ON IMAGENET,0.19203413940256045,"Good
Basin 
Bad
Basin O ÃáùúÜ"""
RESULTS ON IMAGENET,0.1934566145092461,"Good
Basin O ÃáùúÜ"" ùúÜ"" $
ùúÜ"" #"
RESULTS ON IMAGENET,0.19487908961593173,"Bad
Basin"
RESULTS ON IMAGENET,0.19630156472261737,"Increase ùúñ
Increase ùúñ"
RESULTS ON IMAGENET,0.19772403982930298,"ùúÜ""
ùúÜ""
ùúÜ"""
RESULTS ON IMAGENET,0.19914651493598862,"Figure 3: Left: Change of ÀôŒªS when predictor regularization œµ increases. Right: Eigenvalues of F when
trained by DirectCopy under different œµ on CIFAR-10 for 100 epochs."
ABLATION STUDY,0.20056899004267426,"5
ABLATION STUDY"
ABLATION STUDY,0.2019914651493599,"In this section, we study the inÔ¨Çuence of predictor regularization œµ, normalization method, weight
decay and degree Œ± on the performance of DirectCopy."
ABLATION STUDY,0.2034139402560455,"Predictor regularization:
Table 3 shows that when the predictor regularization œµ increases, the
performance of DirectCopy on STL-10 and CIFAR-10 improves at Ô¨Årst and then deteriorates. On
STL-10, DirectCopy with œµ = 1 completely fails. On CIFAR-10, although DirectCopy with œµ = 1
achieved reasonable performance at epoch 300, it‚Äôs still much worse than œµ = 0.3."
ABLATION STUDY,0.20483641536273114,"To better understand the role of œµ, we analyze the simple linear setting as in Section 3.1 while
setting Wp = WW ‚ä§+ œµI. Recall that ŒªB is the eigenvalue of W in B subspace and ŒªS is that in S
subspace. When the weight decay is appropriate, ŒªB still converges to zero. On the other hand, the
dynamics for ŒªS is as follows:"
ABLATION STUDY,0.20625889046941678,ÀôŒªS = ‚àíŒªS
ABLATION STUDY,0.20768136557610242,"
Œª2
S + œµ ‚àí1 ‚àí‚àö1 ‚àí4Œ∑ 2"
ABLATION STUDY,0.20910384068278806," 
Œª2
S + œµ ‚àí1 + ‚àö1 ‚àí4Œ∑ 2 
."
ABLATION STUDY,0.21052631578947367,"Increasing œµ shifts the two positive stationary points Œª‚àí
S , Œª+
S towards zero. As illustrated in Figure 3
(left), as œµ increases, when Œª+
S is still positive, the good attraction basin expands, which means ŒªS
can converge to a positive value from a smaller initialization; when Œª+
S shifts to zero, ŒªS converges
to zero regardless the initialization size. See the full analysis in Appendix D."
ABLATION STUDY,0.2119487908961593,"Intuitively, a reasonable œµ can alleviate representation collapse, but a too large œµ also encourages
representation collapse. As shown in Figure 3 (right), when œµ increases from zero, more eigenvalues
of F becomes large; but when œµ exceeds 0.3, eigenvalues of F begin to collapse."
ABLATION STUDY,0.21337126600284495,"Normalization on F:
In our experiments, we have been normalizing F by its spectral norm
before adding the regularization: Wp = F/ ‚à•F‚à•+ œµI. It turns out that we can also normalize
F by its Frobenius norm or simply skip the normalization step. In Table 4, we see comparable
performance from DirectCopy with Frobenius normalization or no normalization, especially when
trained longer."
ABLATION STUDY,0.2147937411095306,"Number of epochs
100
300
STL-10
œµ = 0
76.57¬±0.66
81.19¬±0.39
œµ = 0.1
78.05¬±0.14
81.60¬±0.15
œµ = 0.2
77.83¬±0.56
82.01¬±0.28
œµ = 1
31.10¬±0.80
31.10¬±0.80
CIFAR-10
œµ = 0
80.53¬±1.14
86.07¬±0.71
œµ = 0.1
83.97¬±0.25
88.58¬±0.11
œµ = 0.3
84.02¬±0.37
89.17¬±0.12
œµ = 1
57.38¬±11.62
83.15¬±4.24
Table 3: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying regularization œµ."
ABLATION STUDY,0.21621621621621623,"Number of epochs
100
300
STL-10
Spectral
77.83¬±0.56
82.01¬±0.28
Frobenius
77.71¬±0.18
82.06¬±0.28
None
77.81¬±0.20
82.00¬±1.24
CIFAR-10
Spectral
84.02¬±0.37
89.17¬±0.12
Frobenius
84.33¬±0.25
89.62¬±0.14
None
81.76¬±0.34
89.21¬±0.17
Table 4: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with F matrix normalized by spectral
norm/Frobenius norm or no normalization."
ABLATION STUDY,0.21763869132290184,"Weight decay:
Table 5 shows that when weight decay Œ∑ increases, the performance of DirectCopy
improves at Ô¨Årst and then deteriorates. This Ô¨Åts our analysis on simple linear networks. Basically,
when the weight decay Œ∑ increases, it can suppress the nuisance features more effectively, but a too
large weight decay also collapses the useful features."
ABLATION STUDY,0.21906116642958748,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.22048364153627312,"Number of epochs
100
300
STL-10
Œ∑ = 0
71.94¬±0.93
78.53¬±0.40
Œ∑ = 0.0004
77.83¬±0.56
82.01¬±0.28
Œ∑ = 0.001
77.65¬±0.16
80.28¬±0.16
Œ∑ = 0.01
58.12¬±0.94
58.53¬±0.76
CIFAR-10
Œ∑ = 0
79.15¬±0.08
85.35¬±0.31
Œ∑ = 0.0004
84.02¬±0.37
89.17¬±0.12
Œ∑ = 0.001
83.91¬±0.33
87.75¬±0.16
Œ∑ = 0.01
65.31¬±1.19
65.63¬±1.30"
ABLATION STUDY,0.22190611664295876,"Table 5: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying weight decay."
ABLATION STUDY,0.2233285917496444,"Number of epochs
100
300
STL-10
Œ± = 2
76.80¬±0.22
80.90¬±0.18
Œ± = 1
77.83¬±0.56
82.01¬±0.28
Œ± = 1/2
77.82¬±0.37
77.83¬±0.37
Œ± = 1/4
76.82¬±0.36
76.82¬±0.36
CIFAR-10
Œ± = 2
82.96¬±0.56
88.60¬±0.11
Œ± = 1
84.02¬±0.37
89.17¬±0.12
Œ± = 1/2
84.88¬±0.21
88.32¬±0.57
Œ± = 1/4
84.78¬±0.21
87.82¬±0.32
Table 6: STL-10/CIFAR-10 Top-1 accuracy of
DirectSet(Œ±) with varying degree Œ±."
ABLATION STUDY,0.22475106685633,"Predictor degree:
We compare DirectCopy against DirectSet(Œ±) with Œ± = 2, 1/2, 1/4. Table 6
shows that DirectCopy outperforms other algorithms on STL-10. On CIFAR-10, DirectCopy is
slightly worse at epoch 100, but catches up in later epochs."
ABLATION STUDY,0.22617354196301565,"6
BEYOND LINEAR MODELS: LIMITATIONS AND DISCUSSION"
ABLATION STUDY,0.22759601706970128,"Figure 4: Eigenvalues of F when trained by DirectCopy, BYOL with linear predictor and BYOL with two-
layer nonlinear predictor on CIFAR-10 for different epochs. Top-1 accuracy at 500 epoch is 89.62 for Direct-
Copy, 88.83 for BYOL with linear predictor and 90.25 for BYOL with two-layer nonlinear predictor."
ABLATION STUDY,0.22901849217638692,"As a linear model used to study the behavior of nc-SSL, our model does not capture all of its
intriguing empirical phenomena. For example, we observed that the discarded nuisance features
gradually come back after training over longer epochs. Moreover, whether it comes back or not is
related to the downstream task performance. In Figure 4 on CIFAR-10 dataset, both DirectCopy
and BYOL with two-layer nonlinear predictor show this resurgence of nuisance features, as well as
strong performance, while BYOL with linear predictor does not seem to learn new features even
when trained longer, which might explain its worse performance."
ABLATION STUDY,0.23044096728307253,"One conjecture is that at the beginning of training, weight decay prioritize the invariant features (i.e.,
low variance under augmentation) over nuisance ones. The invariant features then grow, building
their own supporting low-level features. After that, the nuisance feature, which is also useful, are
gradually picked up in later stage. Since the low-level features are already trained through previous
steps of back-propagation, the nuisance features are encouraged to use them as the supporting fea-
tures, rather than creating their own. In contrast, if we train both the invariant and nuisance features
simultaneously, they will compete over the limited pool of low-level supporting features deÔ¨Åned by
the capacity of the network, leading to worse learned representations. We believe understanding
these phenomena require analysis on the non-linear networks, and we leave it as future work."
CONCLUSION,0.23186344238975817,"7
CONCLUSION"
CONCLUSION,0.2332859174964438,"In this paper, we have proved DirectSet(Œ±) can learn the desirable projection matrix in a linear
network setting and reduce the sample complexity on down-stream tasks. Our analysis sheds light on
the crucial role of weight decay in nc-SSL, which discards the features that have high variance under
augmentations and keep the invariant features. Inspired by the analysis, we also designed a simpler
and more efÔ¨Åcient algorithm DirectCopy, which achieves comparable or even better performance
than the original DirectPred (Tian et al., 2021) on various datasets."
CONCLUSION,0.23470839260312945,"We view our paper as an initial step towards demystifying the representation learning in nc-SSL.
Many mysteries lie beyond the explanation of the current theory and we leave them for future work."
CONCLUSION,0.2361308677098151,Under review as a conference paper at ICLR 2022
REFERENCES,0.2375533428165007,REFERENCES
REFERENCES,0.23897581792318634,"Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In ICML. PMLR, 2018."
REFERENCES,0.24039829302987198,"Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In ICLR, 2019."
REFERENCES,0.24182076813655762,"Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019."
REFERENCES,0.24324324324324326,"Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization
for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021."
REFERENCES,0.24466571834992887,"Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¬®ackinger, and Roopak Shah. Signature veriÔ¨Å-
cation using a‚Äú siamese‚Äù time delay neural network. NeurIPS, 1994."
REFERENCES,0.2460881934566145,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020."
REFERENCES,0.24751066856330015,"Mathilde Caron, Hugo Touvron, Ishan Misra, Herv¬¥e J¬¥egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin.
Emerging properties in self-supervised vision transformers.
arXiv preprint
arXiv:2104.14294, 2021."
REFERENCES,0.24893314366998578,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a."
REFERENCES,0.2503556187766714,"Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint
arXiv:2011.10566, 2020."
REFERENCES,0.25177809388335703,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b."
REFERENCES,0.2532005689900427,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In International conference on artiÔ¨Åcial intelligence and statistics, 2011."
REFERENCES,0.2546230440967283,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009."
REFERENCES,0.25604551920341395,"Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
ICML, 2019."
REFERENCES,0.2574679943100996,"Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-
supervised representation learning. In International Conference on Machine Learning, pp. 3015‚Äì
3024. PMLR, 2021."
REFERENCES,0.25889046941678523,"Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimen-
sions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp.
761‚Äì770. ACM, 2015."
REFERENCES,0.2603129445234708,"Jean-Bastien Grill, Florian Strub, Florent Altch¬¥e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020."
REFERENCES,0.26173541963015645,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016."
REFERENCES,0.2631578947368421,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020."
REFERENCES,0.26458036984352773,"Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature
decorrelation in self-supervised learning. ICCV, 2021."
REFERENCES,0.26600284495021337,"Kenji Kawaguchi. Deep learning without poor local minima. NeurIPS, 2016."
REFERENCES,0.267425320056899,Under review as a conference paper at ICLR 2022
REFERENCES,0.26884779516358465,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.2702702702702703,"Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In ICLR, 2018."
REFERENCES,0.2716927453769559,"Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In ICML, pp. 2902‚Äì2907. PMLR, 2018."
REFERENCES,0.27311522048364156,"Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020."
REFERENCES,0.27453769559032715,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018."
REFERENCES,0.2759601706970128,"Pierre H. Richemond, Jean-Bastien Grill, Florent Altch¬¥e, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. Byol works
even without batch statistics. arXiv, 2020."
REFERENCES,0.2773826458036984,"Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013."
REFERENCES,0.27880512091038406,"Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., 2019."
REFERENCES,0.2802275960170697,"Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM review, 19(4):634‚Äì662, 1977."
REFERENCES,0.28165007112375534,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849, 2019."
REFERENCES,0.283072546230441,"Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021."
REFERENCES,0.2844950213371266,"Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. arXiv preprint arXiv:2008.10150, 2020."
REFERENCES,0.28591749644381226,"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010."
REFERENCES,0.28733997155049784,"Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018."
REFERENCES,0.2887624466571835,"Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019."
REFERENCES,0.2901849217638691,"Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021."
REFERENCES,0.29160739687055476,"Yang You, Igor Gitman, and Boris Ginsburg.
Large batch training of convolutional networks.
arXiv:1708.03888, 2017."
REFERENCES,0.2930298719772404,"Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St¬¥ephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. ICML, 2021."
REFERENCES,0.29445234708392604,Under review as a conference paper at ICLR 2022
REFERENCES,0.2958748221906117,"A
DETAILED EXPERIMENT SETTING"
REFERENCES,0.2972972972972973,"STL-10, CIFAR-10, CIFAR-100
: We use ResNet-18 (He et al., 2016) as the backbone network,
a two-layer nonlinear MLP (with batch normalization, ReLU activation, hidden layer width 512,
output width 128) as the projector, and a linear predictor. Unless speciÔ¨Åed otherwise, SGD is used
as the optimizer with momentum 0.9, weight decay Œ∑ = 0.0004 and batch size 128. The EMA
parameter for the target network is set as 0.996 and the EMA parameter ¬µ of the correlation matrix ÀÜF
is set as 0.5. Our code is adapted from Tian et al. (2021) 4, and we follow the same data augmentation
process."
REFERENCES,0.29871977240398295,"To evaluate the quality of the pre-trained representations, we follow the linear evaluation protocol.
Each setting is repeated 5 times to compute the mean and standard deviation. The accuracy is
reported as ‚Äúmean¬±std‚Äù. Unless explicitly speciÔ¨Åed, we use learning rate Œ≥ = 0.01, regularization
œµ = 0.2 on STL-10; Œ≥ = 0.02, œµ = 0.3 on CIFAR-10 and Œ≥ = 0.03, œµ = 0.3 on CIFAR-100."
REFERENCES,0.30014224751066854,"ImageNet
: Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-
layer MLP (with batch normalization, ReLU, hidden layer width 4096, output width 256) as the
projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs, with a
batch size 4096. The learning rate is 7.2, which is linearly scaled from the base learning rate 0.45
at batch size 256. Other setups such as weight decay (Œ∑ = 1e‚àí6), target EMA (scheduled from 0.99
to 1), augmentation recipe (color jitters, blur, etc.), and linear evaluation protocol are the same as
BYOL."
REFERENCES,0.3015647226173542,"B
PROOFS OF SINGLE-LAYER LINEAR NETWORKS"
REFERENCES,0.3029871977240398,"B.1
GRADIENT FLOW ON POPULATION LOSS"
REFERENCES,0.30440967283072545,"In this section, we give the proof of Theorem 1, which shows that DirectSet(Œ±) running on the
population loss with inÔ¨Ånitesimal learning rate and Œ∑ weight decay can learn the projection matrix
onto subspace S.
Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-
tion 2, respectively. Suppose we initialize online network W as Œ¥I, and run DirectSet(Œ±) on popu-
lation loss (see Eqn. 1) with inÔ¨Ånitesimal step size and Œ∑ weight decay. If we set the weight decay"
REFERENCES,0.3058321479374111,"coefÔ¨Åcient Œ∑ ‚àà

1
4(1+œÉ2), 1"
REFERENCES,0.30725462304409673,"4

and initialization scale Œ¥ >

1‚àí‚àö1‚àí4Œ∑"
REFERENCES,0.30867709815078237,"2
1/(2Œ±)
, then W converges to

1+‚àö1‚àí4Œ∑"
REFERENCES,0.310099573257468,"2
1/(2Œ±)
PS when time goes to inÔ¨Ånity."
REFERENCES,0.31152204836415365,"As we already mentioned in the main text, Theorem 1 is proved by analyzing each eigenvalue of W
separately. We show that the eigenvalues in the B subspace converge to zero, and the eigenvalues in
the S subspace converge to the same positive number, which immediately implies that W converges
to a scaling of the projection matrix PS."
REFERENCES,0.3129445234708393,"Proof of Theorem 1. We can compute the gradient in terms of W as follows,"
REFERENCES,0.31436699857752487,"‚àáL(W) =Ex1,x2W ‚ä§
p (WpWx1 ‚àíWax2) x‚ä§
1
=W ‚ä§
p
 
WpWEx1x1x‚ä§
1 ‚àíWaEx1,x2x2x‚ä§
1

."
REFERENCES,0.3157894736842105,"Note that the two augmented views x1, x2 are sampled by Ô¨Årst sampling input x from N(0, Id), and
then independently sampling x1, x2 from N(x, œÉ2PB). Therefore, we know Ex1x1x‚ä§
1 = I + œÉ2PB
and Ex1,x2x2x‚ä§
1 = I. Recall that we run gradient Ô¨Çow on W with weight decay Œ∑, so the dynamics
on W is as follows:
ÀôW =W ‚ä§
p (‚àíWpW(I + œÉ2PB) + Wa) ‚àíŒ∑W,"
REFERENCES,0.31721194879089615,where the Ô¨Årst term comes from the gradient and the second term is due to weight decay.
REFERENCES,0.3186344238975818,"Since W is initialized as Œ¥I, and Wa = W, Wp = (WW ‚ä§)Œ±, so we know initially W, Wp, Wa, I and
PB are all simultaneously diagonalizable, which then implies ÀôW is simultaneously diagonalizable"
REFERENCES,0.3200568990042674,4Their open source code is at https://github.com/facebookresearch/luckmatters/tree/main/ssl
REFERENCES,0.32147937411095306,Under review as a conference paper at ICLR 2022
REFERENCES,0.3229018492176387,"with W. This argument can continue to show that at any time point, W, Wp, Wa, I and PB are
all simultaneously diagonalizable. Since W is always a real symmetric matrix, we have Wp =
(WW ‚ä§)Œ± = |W|2Œ± . The dynamics on W can then be written as"
REFERENCES,0.32432432432432434,ÀôW = |W|2Œ± (‚àí|W|2Œ± W(I + œÉ2PB) + W) ‚àíŒ∑W
REFERENCES,0.32574679943101,"=W

‚àí(I + œÉ2PB) |W|4Œ± + |W|2Œ± ‚àíŒ∑

."
REFERENCES,0.32716927453769556,"Let the eigenvalue decomposition of W be Pd
i=1 Œªiuiu‚ä§
i , with span({ud‚àír+1, ¬∑ ¬∑ ¬∑ , ud}) equals to
subspace B. We can separately analyze the dynamics of each Œªi. Furthermore, we know Œª1, ¬∑ ¬∑ ¬∑ , Œªr
have the same value ŒªS and Œªd‚àír+1, ¬∑ ¬∑ ¬∑ , Œªd have the same value ŒªB. Next, we separately show that
ŒªB converge to zero and ŒªS converges to a positive value."
REFERENCES,0.3285917496443812,"Dynamics for ŒªB:
We can write down the dynamics for ŒªB as follows:"
REFERENCES,0.33001422475106684,"ÀôŒªB = ŒªB
h
‚àí(1 + œÉ2) |ŒªB|4Œ± + |ŒªB|2Œ± ‚àíŒ∑
i"
REFERENCES,0.3314366998577525,"Similar as the analysis in Tian et al. (2021), when Œ∑ >
1
4(1+œÉ2), we know ÀôŒªB < 0 for any ŒªB > 0
and ŒªB = 0 is a critical point. This means, as long as Œ∑ >
1
4(1+œÉ2), ŒªB must converge to zero."
REFERENCES,0.3328591749644381,"Dynamics for ŒªS:
We can write down the dynamics for ŒªS as follows:"
REFERENCES,0.33428165007112376,"ÀôŒªS = ŒªS
h
‚àí|ŒªS|4Œ± + |ŒªS|2Œ± ‚àíŒ∑
i
."
REFERENCES,0.3357041251778094,When 0 < Œ∑ < 1
REFERENCES,0.33712660028449504,"4, we know ÀôŒªS > 0 for Œª2Œ±
S
‚àà

1‚àí‚àö1‚àí4Œ∑"
REFERENCES,0.3385490753911807,"2
, 1+‚àö1‚àí4Œ∑"
REFERENCES,0.3399715504978663,"2

and ÀôŒªS < 0 for Œª2Œ±
S
‚àà

1+‚àö1‚àí4Œ∑"
REFERENCES,0.3413940256045519,"2
, ‚àû

. Furthermore, we know ÀôŒªS = 0 when Œª2Œ±
S
=
1+‚àö1‚àí4Œ∑"
REFERENCES,0.34281650071123754,"2
. Therefore, as long as"
REFERENCES,0.3442389758179232,0 < Œ∑ < 1
REFERENCES,0.3456614509246088,4 and initialization Œ¥2Œ± > 1‚àí‚àö1‚àí4Œ∑
REFERENCES,0.34708392603129445,"2
, we know Œª2Œ±
S converges to 1+‚àö1‚àí4Œ∑ 2
."
REFERENCES,0.3485064011379801,"Overall, we know when
1
4(1+œÉ2) < Œ∑ < 1"
REFERENCES,0.34992887624466573,"4 and Œ¥ >

1‚àí‚àö1‚àí4Œ∑"
REFERENCES,0.35135135135135137,"2
1/(2Œ±)
, we have ŒªB converge to"
REFERENCES,0.352773826458037,"zero and ŒªS converge to

1+‚àö1‚àí4Œ∑"
REFERENCES,0.3541963015647226,"2
1/(2Œ±)
. That is, matrix W converges to

1+‚àö1‚àí4Œ∑"
REFERENCES,0.35561877667140823,"2
1/(2Œ±)
PS.
‚ñ°"
REFERENCES,0.35704125177809387,"B.2
GRADIENT DESCENT ON EMPIRICAL LOSS"
REFERENCES,0.3584637268847795,"In this section, we prove that DirectCopy successfully learns the projection matrix given polynomial
number of samples.
Theorem 2. Suppose network architecture and data distribution are as deÔ¨Åned in Assumption 1
and Assumption 2, respectively. Suppose we initialize online network as Œ¥I, and run DirectCopy
on empirical loss (see Eqn. 2) with Œ≥ step size and Œ∑ weight decay. Suppose the noise scale œÉ2"
REFERENCES,0.35988620199146515,"is a positive constant, the weight decay coefÔ¨Åcient Œ∑ ‚àà

1+œÉ2/4
4(1+œÉ2), 1+3œÉ2/4"
REFERENCES,0.3613086770981508,"4(1+œÉ2)

and the initialization"
REFERENCES,0.3627311522048364,"scale Œ¥ is a constant at least 1/
‚àö"
REFERENCES,0.36415362731152207,"2. Choose the step size Œ≥ as a small enough constant. For any
accuracy ÀÜœµ > 0, given n ‚â•poly(d, 1/ÀÜœµ) number of samples, with probability at least 0.99 there
exists t = O(log(1/ÀÜœµ)) such that (here f
Wt is the online network weights at the t-th step):

f
Wt ‚àí r"
REFERENCES,0.3655761024182077,1 + ‚àö1 ‚àí4Œ∑
PS,0.3669985775248933,"2
PS ‚â§ÀÜœµ."
PS,0.3684210526315789,"When running gradient descent on the empirical loss, the eigenspace of f
Wt can shift and become no
longer simultaneously diagonalizable with PB. So we cannot independently analyze each eigenvalue
of f
Wt as before, which brings signiÔ¨Åcant challenge into the analysis. Instead of directly analyzing
the dynamics of f
Wt, we Ô¨Årst show that the gradient descent iterates Wt on the population loss
converges to PS in linear rate, and then show that f
Wt stays close to Wt within certain iterations."
PS,0.36984352773826457,Under review as a conference paper at ICLR 2022
PS,0.3712660028449502,"Lemma 1. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population
loss L. Given any accuracy ÀÜœµ > 0, for any t ‚â•C log(1/ÀÜœµ), we have
Wt ‚àí r"
PS,0.37268847795163584,1 + ‚àö1 ‚àí4Œ∑
PS,0.3741109530583215,"2
PS ‚â§ÀÜœµ,"
PS,0.3755334281650071,where C is a positive constant.
PS,0.37695590327169276,"The proof of Lemma 1 is similar as the gradient Ô¨Çow analysis in Section 3.2. Next, we show that the
gradient descent trajectory on the empirical loss stays close to the gradient descent trajectory on the
population loss within O(log(1/ÀÜœµ)) iterations."
PS,0.3783783783783784,"Lemma 2. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population
loss and let f
Wt be the gradient descent iterations on the empirical loss. For any accuracy ÀÜœµ > 0,
given n ‚â•poly(d, 1/ÀÜœµ) number of samples, with probability at least 0.99, for any t ‚â§C log(1/ÀÜœµ),
we have
f
Wt ‚àíWt
 ‚â§ÀÜœµ,"
PS,0.37980085348506404,where the constant C comes from Lemma 1.
PS,0.3812233285917496,Then the proof of Theorem 2 directly follows from Lemma 1 and Lemma 2.
PS,0.38264580369843526,"Proof of Theorem 2. According to Lemma 1, we know given any accuracy ÀÜœµ‚Ä≤, for t = C log(1/ÀÜœµ),
we have
Wt ‚àí r"
PS,0.3840682788051209,1 + ‚àö1 ‚àí4Œ∑
PS,0.38549075391180654,"2
PS ‚â§ÀÜœµ‚Ä≤,"
PS,0.3869132290184922,where C is a positive constant.
PS,0.3883357041251778,"According to Lemma 2, we know given n ‚â•poly(d, 1/ÀÜœµ‚Ä≤) number of samples, with probability at
least 0.99,
f
Wt ‚àíWt
 ‚â§ÀÜœµ‚Ä≤."
PS,0.38975817923186346,"Therefore, we have

f
Wt ‚àí r"
PS,0.3911806543385491,1 + ‚àö1 ‚àí4Œ∑
PS,0.39260312944523473,"2
PS ‚â§ Wt ‚àí r"
PS,0.3940256045519203,1 + ‚àö1 ‚àí4Œ∑
PS,0.39544807965860596,"2
PS"
PS,0.3968705547652916,"+
f
Wt ‚àíWt
 ‚â§2ÀÜœµ‚Ä≤."
PS,0.39829302987197723,"Replacing ÀÜœµ‚Ä≤ by ÀÜœµ/2 Ô¨Ånishes the proof.
‚ñ°"
PS,0.39971550497866287,"In section B.2.1, we give the proof of Lemma 1 and Lemma 2. Proofs of some technical lemmas are
left in Appendix B.5."
PS,0.4011379800853485,"B.2.1
PROOFS FOR LEMMA 1 AND LEMMA 2"
PS,0.40256045519203415,"Proof of Lemma 1. Similar as in Theorem 1, we can show that at any step t, Wt is simultaneously
diagonalizable with Wa,t, Wp,t, I and PB. The update on Wt is as follows,"
PS,0.4039829302987198,"Wt+1 = Wt + Œ≥Wt
 
‚àí(I + œÉ2PB)W 4
t + W 2
t ‚àíŒ∑

."
PS,0.40540540540540543,"Let the eigenvalue decomposition of Wt be Pd
i=1 Œªi,tuiu‚ä§
i , with span({ud‚àír+1, ¬∑ ¬∑ ¬∑ , ud}) equals
to subspace B.
We can separately analyze the dynamics of each Œªi,t. Furthermore, we know
Œª1,t, ¬∑ ¬∑ ¬∑ , Œªr,t have the same value ŒªS,t and Œªd‚àír+1,t, ¬∑ ¬∑ ¬∑ , Œªd,t have the same value ŒªB,t. Next,
we separately show that ŒªB,t converge to zero and ŒªS,t converges to a positive value in linear rate."
PS,0.406827880512091,"Dynamics of ŒªB,t:
We show that"
PS,0.40825035561877665,"0 ‚â§ŒªB,t ‚â§(1 ‚àíŒ≥C1)tŒ¥"
PS,0.4096728307254623,"for any step size Œ≥ ‚â§C2, where C1, C2 are two positive constants."
PS,0.41109530583214793,Under review as a conference paper at ICLR 2022
PS,0.41251778093883357,"According to the gradient update, we have"
PS,0.4139402560455192,"ŒªB,t+1 = ŒªB,t + Œ≥ŒªB,t

‚àí(1 + œÉ2)Œª4
B,t + Œª2
B,t ‚àíŒ∑

."
PS,0.41536273115220484,"We only need to prove that for any ŒªB,t ‚àà[0, Œ¥], we have"
PS,0.4167852062588905,"‚àí(1 + œÉ2)Œª4
B,t + Œª2
B,t ‚àíŒ∑ = ‚àíŒò(1)."
PS,0.4182076813655761,"This is true since Œ∑ ‚àà

1+œÉ2/4
4(1+œÉ2), 1+3œÉ2/4"
PS,0.41963015647226176,"4(1+œÉ2)

and œÉ2, Œ¥ are two positive constants."
PS,0.42105263157894735,"Dynamics of ŒªS:
We show that"
PS,0.422475106685633,"0 ‚â§
Œª2
S,t ‚àí1 + ‚àö1 ‚àí4Œ∑ 2"
PS,0.4238975817923186,"‚â§(1 ‚àíŒ≥C3)t
Œ¥2 ‚àí1 + ‚àö1 ‚àí4Œ∑ 2 "
PS,0.42532005689900426,"for any step size Œ≥ ‚â§C4, where C3, C4 are two positive constants."
PS,0.4267425320056899,"There are two cases to consider: when the initialization scale Œ¥2 ‚àà[1/2, 1+‚àö1‚àí4Œ∑"
PS,0.42816500711237554,"2
], we prove"
PS,0.4295874822190612,0 ‚â§1 + ‚àö1 ‚àí4Œ∑
PS,0.4310099573257468,"2
‚àíŒª2
B,t ‚â§(1 ‚àíŒ≥C3)t
1 + ‚àö1 ‚àí4Œ∑"
PS,0.43243243243243246,"2
‚àíŒ¥2

;"
PS,0.43385490753911804,when the initialization scale Œ¥2 > 1+‚àö1‚àí4Œ∑
PS,0.4352773826458037,"2
, we prove"
PS,0.4366998577524893,"0 ‚â§Œª2
B,t ‚àí1 + ‚àö1 ‚àí4Œ∑"
PS,0.43812233285917496,"2
‚â§(1 ‚àíŒ≥C3)t

Œ¥2 ‚àí1 + ‚àö1 ‚àí4Œ∑ 2 
."
PS,0.4395448079658606,We focus on the second case; the proof for the Ô¨Årst case is similar.
PS,0.44096728307254623,"According to the gradient update, we have"
PS,0.4423897581792319,"ŒªS,t+1 =ŒªS,t + Œ≥ŒªS,t

‚àíŒª4
S,t + Œª2
S,t ‚àíŒ∑
"
PS,0.4438122332859175,"=ŒªS,t ‚àíŒ≥ŒªS,t"
PS,0.44523470839260315,"
Œª2
S,t ‚àí1 ‚àí‚àö1 ‚àí4Œ∑ 2"
PS,0.4466571834992888," 
Œª2
S,t ‚àí1 + ‚àö1 ‚àí4Œ∑ 2 "
PS,0.4480796586059744,"We only need to show that ŒªS,t

Œª2
S,t ‚àí1‚àí‚àö1‚àí4Œ∑"
PS,0.44950213371266,"2

= Œò(1) for any Œª2
S,t ‚àà[ 1+‚àö1‚àí4Œ∑"
PS,0.45092460881934565,"2
, Œ¥]. This is"
PS,0.4523470839260313,"true because Œ∑ ‚àà

1+œÉ2/4
4(1+œÉ2), 1+3œÉ2/4"
PS,0.45376955903271693,"4(1+œÉ2)

and œÉ2, Œ¥ are two positive constants."
PS,0.45519203413940257,"Overall, we know that there exists constant step size such that after t = O(log(1/ÀÜœµ)) steps, we have"
PS,0.4566145092460882,"0 ‚â§ŒªB,t ‚â§ÀÜœµ and"
PS,0.45803698435277385,"ŒªS,t ‚àí r"
PS,0.4594594594594595,1 + ‚àö1 ‚àí4Œ∑ 2 ‚â§ÀÜœµ.
PS,0.46088193456614507,"This then implies,
Wt ‚àí r"
PS,0.4623044096728307,1 + ‚àö1 ‚àí4Œ∑
PS,0.46372688477951635,"2
PS ‚â§ÀÜœµ. ‚ñ°"
PS,0.465149359886202,"Proof of Lemma 2. We know the update on f
Wt is"
PS,0.4665718349928876,"f
Wt+1 ‚àíf
Wt = Œ≥f
W ‚ä§
p,t "
PS,0.46799431009957326,"‚àíf
Wp,tf
Wt"
N,0.4694167852062589,"1
n n
X"
N,0.47083926031294454,"i=1
x(i)
1 [x(i)
1 ]‚ä§
!"
N,0.4722617354196302,"+ f
Wa,t"
N,0.47368421052631576,"1
n n
X"
N,0.4751066856330014,"i=1
x(i)
1 [x(i)
2 ]‚ä§
!!"
N,0.47652916073968704,"‚àíŒ≥Œ∑f
Wt,"
N,0.4779516358463727,and the update on Wt is
N,0.4793741109530583,"Wt+1 ‚àíWt = Œ≥W ‚ä§
p,t
 
‚àíWp,tWt
 
I + œÉ2PB

+ Wa,t

‚àíŒ≥Œ∑Wt."
N,0.48079658605974396,"Next, we bound
f
Wt+1 ‚àíf
Wt ‚àí(Wt+1 ‚àíWt)
 . According to Lemma 3, we know with probability"
N,0.4822190611664296,"at least 1 ‚àíO(d2) exp
 
‚àí‚Ñ¶(ÀÜœµ‚Ä≤2n/d2)

,

1
n n
X"
N,0.48364153627311524,"i=1
x(i)
1 [x(i)
1 ]‚ä§‚àíI ‚àíœÉ2PB ,"
N,0.4850640113798009,"1
n n
X"
N,0.4864864864864865,"i=1
x(i)
1 [x(i)
2 ]‚ä§‚àíI ,"
N,0.4879089615931721,"1
n n
X"
N,0.48933143669985774,"i=1
x(i)[x(i)]‚ä§‚àíI ‚â§ÀÜœµ‚Ä≤."
N,0.4907539118065434,Under review as a conference paper at ICLR 2022
N,0.492176386913229,"Recall that we set f
Wa,t = f
Wt and set Wa,t as Wt, so we have
f
Wa,t ‚àíWa,t
 =
f
Wt ‚àíWt
 ."
N,0.49359886201991465,"Also since we set f
Wp,t = f
Wt
  1"
N,0.4950213371266003,"n
Pn
i=1 x(i)[x(i)]‚ä§ f
W ‚ä§
t
and set Wp,t = WtW ‚ä§
t , we have
f
Wp,t ‚àíWp,t
 = O
f
Wt ‚àíWt
 + ÀÜœµ‚Ä≤
since ‚à•Wt‚à•= O(1)."
N,0.49644381223328593,"Combing the above bounds and recall Œ≥ is a constant, we have
f
Wt+1 ‚àíf
Wt ‚àí(Wt+1 ‚àíWt)
 = O
f
Wt ‚àíWt
 + ÀÜœµ‚Ä≤
."
N,0.49786628733997157,"Therefore,
f
Wt ‚àíWt
 ‚â§Ct
1ÀÜœµ‚Ä≤,"
N,0.4992887624466572,"where C1 is a constant larger than 1. So for any t ‚â§C log(1/ÀÜœµ), we have
f
Wt ‚àíWt
 ‚â§CC log(1/ÀÜœµ)
1
ÀÜœµ‚Ä≤ ‚â§(1/ÀÜœµ)C2ÀÜœµ‚Ä≤,"
N,0.5007112375533428,"for some positive constant C2. Choosing ÀÜœµ‚Ä≤ = ÀÜœµC2+1, we know as long as n ‚â•poly(d, 1/ÀÜœµ), with
probability at least 0.99, for any t ‚â§C log(1/ÀÜœµ), we have
f
Wt ‚àíWt
 ‚â§ÀÜœµ. ‚ñ°"
N,0.5021337126600285,"B.3
SAMPLE COMPLEXITY ON DOWN-STREAM TASKS"
N,0.5035561877667141,"In this section, we give a proof for Theorem 3, which shows that the learned representations can
indeed reduce sample complexity in downstream tasks.
Theorem 3. Suppose the downstream data distribution is as deÔ¨Åned in Assumption 3. Suppose
 ÀÜP ‚àíP

F ‚â§ÀÜœµ with ÀÜœµ < 1. Choose the regularizer coefÔ¨Åcient œÅ = ÀÜœµ1/3. For any Œ∂ < 1/2, given"
N,0.5049786628733998,"n ‚â•O(r +log(1/Œ∂)) number of samples, with probability at least 1‚àíŒ∂, the training loss minimizer
ÀÜw satisÔ¨Åes
 ÀÜP ÀÜw ‚àíw‚àó ‚â§O "
N,0.5064011379800853,"ÀÜœµ1/3 + Œ≤
‚àör +
p"
N,0.5078236130867709,"log(1/Œ∂)
‚àön ! ."
N,0.5092460881934566,"Suppose {(x(i), y(i))}n
i=1 are n training samples in the downstream task, let X ‚ààRn√ód be the data
matrix with its i-th row equal to x(i). Denote y ‚ààRn as the label vector with its i-th entry as y(i).
Each input x(i) is transformed by a matrix ÀÜP ‚ààRd√ód to get its representation ÀÜPx(i). The regularized
loss can be written as
L(w) := 1"
N,0.5106685633001422,2n
N,0.5120910384068279,"X ÀÜPw ‚àíy

2
+ œÅ"
N,0.5135135135135135,2 ‚à•w‚à•2 .
N,0.5149359886201992,"This is the ridge regression problem on inputs {( ÀÜPx(i), y(i))}n
i=1, and the unique global minimizer
ÀÜw has the following close form:"
N,0.5163584637268848,"ÀÜw =
 1"
N,0.5177809388335705,"n
ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.519203413940256,"n
ÀÜP ‚ä§X‚ä§y
(4)"
N,0.5206258890469416,"With the above closed form of ÀÜw, the proof of Theorem 3 follows by bounding the difference be-
tween ÀÜP ÀÜw and w‚àóby matrix concentration inequalities and matrix perturbation bounds. Some proofs
of technical lemmas are left in Appendix B.5."
N,0.5220483641536273,"Proof of Theorem 3. Denoting ÀÜP as P + ‚àÜ, we know ‚à•‚àÜ‚à•F ‚â§ÀÜœµ by assumption. We can also write
y as Xw‚àó+ Œæ where Œæ ‚ààRn is the noise vector with its i-th entry equal to Œæ(i). Then, we can divide
ÀÜw into two terms,"
N,0.5234708392603129,"ÀÜw =
 1"
N,0.5248933143669986,"n
ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.5263157894736842,"n
ÀÜP ‚ä§X‚ä§y =
 1"
N,0.5277382645803699,"n
ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.5291607396870555,"nP ‚ä§X‚ä§(Xw‚àó+ Œæ) +
 1"
N,0.5305832147937412,"n
ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.5320056899004267,n‚àÜ‚ä§X‚ä§(Xw‚àó+ Œæ)
N,0.5334281650071123,Let‚Äôs Ô¨Årst give an upper bound for the second term that comes from the error term ‚àÜ‚ä§.
N,0.534850640113798,Under review as a conference paper at ICLR 2022
N,0.5362731152204836,"Upper bounding


1
n ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.5376955903271693,"n‚àÜ‚ä§X‚ä§(Xw‚àó+ Œæ)

We Ô¨Årst bound the norm of"
N,0.5391180654338549,"1
n‚àÜ‚ä§X‚ä§Xw‚àó. According to Lemma 5, we know with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),
 1
‚àön‚àÜ‚ä§X‚ä§
F ‚â§O(ÀÜœµ). Since Xw‚àóis a standard Gaussian vector with dimension n, according"
N,0.5405405405405406,"to Lemma 8, with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),
 1
‚àönXw‚àó ‚â§O(1). Therefore, we have
 1"
N,0.5419630156472262,"n‚àÜ‚ä§X‚ä§Xw‚àó ‚â§O(ÀÜœµ)."
N,0.5433854907539118,"Then
we
bound
the
norm
of
1
n‚àÜ‚ä§X‚ä§Œæ.
According
to
Lemma
8,
we
know"
N,0.5448079658605974,"with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),
 1
‚àönŒæ

‚â§
O(Œ≤). According to Lemma 6, we"
N,0.5462304409672831,"know with probability at least 1 ‚àíŒ∂/3,
‚àÜ‚ä§X‚ä§¬ØŒæ
 ‚â§O

ÀÜœµ
p"
N,0.5476529160739687,"log(1/Œ∂)

. Therefore, we have
 1"
N,0.5490753911806543,"n‚àÜ‚ä§X‚ä§Œæ
 ‚â§O

Œ≤ÀÜœµ‚àö"
N,0.55049786628734,"log(1/Œ∂)
‚àön 
."
N,0.5519203413940256,"Since Œªmin

1
n ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI

‚â•œÅ, we have


1
n ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 ‚â§1"
N,0.5533428165007113,œÅ. Combining with
N,0.5547652916073968,"above bound on
 1"
N,0.5561877667140825,"n‚àÜ‚ä§X‚ä§(Xw‚àó+ Œæ)
, we know with probability at least 1‚àíexp(‚àí‚Ñ¶(n))‚àíŒ∂/3,  1"
N,0.5576102418207681,"n
ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.5590327169274538,n‚àÜ‚ä§X‚ä§(Xw‚àó+ Œæ) ‚â§O
N,0.5604551920341394,"ÀÜœµ
œÅ + Œ≤ÀÜœµ
p"
N,0.561877667140825,"log(1/Œ∂)
œÅ‚àön ! ."
N,0.5633001422475107,"Analyzing

1
n ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.5647226173541963,"nP ‚ä§X‚ä§(Xw‚àó+ Œæ)
We
can
write
1
n ÀÜP ‚ä§X‚ä§X ÀÜP
as"
N,0.566145092460882,"1
nP ‚ä§X‚ä§XP + E, where E = 1"
N,0.5675675675675675,n‚àÜ‚ä§X‚ä§XP + 1
N,0.5689900426742532,nP ‚ä§X‚ä§X‚àÜ+ 1
N,0.5704125177809388,n‚àÜ‚ä§X‚ä§X‚àÜ.
N,0.5718349928876245,"Let‚Äôs Ô¨Årst bound the spectral norm of XP. Since P is a projection matrix on an r-dimensional
subspace S, we can write P as UU ‚ä§, where U ‚ààRd√ór has columns as an orthonormal basis of
subspace S. According to Lemma 4, we know with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),"
N,0.5732574679943101,‚Ñ¶(1) ‚â§œÉmin
N,0.5746799431009957," 1
‚àönXU

‚â§œÉmax"
N,0.5761024182076814," 1
‚àönXU

‚â§O(1)."
N,0.577524893314367,"Since ‚à•U‚à•‚â§1, we have
 1
‚àönXP
 =
 1
‚àönXUU ‚ä§ ‚â§O(1)."
N,0.5789473684210527,"According to Lemma 5, we know with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),

1
‚àönX‚àÜ

F
‚â§O(ÀÜœµ)."
N,0.5803698435277382,"So overall, we know ‚à•E‚à•‚â§‚à•E‚à•F ‚â§O(ÀÜœµ)."
N,0.5817923186344239,"Then, we can write
 1"
N,0.5832147937411095,"n
ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1
=
 1"
N,0.5846372688477952,"nP ‚ä§X‚ä§XP + œÅI
‚àí1
+ F."
N,0.5860597439544808,"According to the perturbation bound for matrix inverse (Lemma 11), we have ‚à•F‚à•‚â§O( ÀÜœµ"
N,0.5874822190611664,"œÅ2 ). Then,
we have
 1"
N,0.5889046941678521,"n
ÀÜP ‚ä§X‚ä§X ÀÜP + œÅI
‚àí1 1"
N,0.5903271692745377,"nP ‚ä§X‚ä§(Xw‚àó+ Œæ) =
 1"
N,0.5917496443812233,"nP ‚ä§X‚ä§XP + œÅI
‚àí1 1"
N,0.5931721194879089,nP ‚ä§X‚ä§Xw‚àó + F 1
N,0.5945945945945946,nP ‚ä§X‚ä§Xw‚àó +  1
N,0.5960170697012802,"nP ‚ä§X‚ä§XP + œÅI
‚àí1
+ F"
N,0.5974395448079659,"!
1
nP ‚ä§X‚ä§Œæ"
N,0.5988620199146515,Under review as a conference paper at ICLR 2022
N,0.6002844950213371,We Ô¨Årst show that the Ô¨Årst term is close to w‚àó. Let the eigenvalue decomposition of 1
N,0.6017069701280228,"nP ‚ä§X‚ä§XP
be V Œ£V ‚ä§, where V ‚Äôs columns are an orthonormal basis for subspace S. Here Œ£ ‚ààRr√ór is the
diagonal matrix that contains all the eigenvalues of 1"
N,0.6031294452347084,"nP ‚ä§X‚ä§XP. According to Lemma 4, we
know that with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)), all the non-zero eigenvalues of 1"
N,0.604551920341394,"nP ‚ä§X‚ä§XP
are Œò(1)."
N,0.6059743954480796,"Then, it‚Äôs not hard to show that  1"
N,0.6073968705547653,"nP ‚ä§X‚ä§XP + œÅI
‚àí1 1"
N,0.6088193456614509,nP ‚ä§X‚ä§XP ‚àíP
N,0.6102418207681366,‚â§O(œÅ).
N,0.6116642958748222,This immediately implies that  1
N,0.6130867709815079,"nP ‚ä§X‚ä§XP + œÅI
‚àí1 1"
N,0.6145092460881935,"nP ‚ä§X‚ä§Xw‚àó‚àíw‚àó
 ‚â§O(œÅ)"
N,0.615931721194879,"Next, we bound the norm of the second term F 1"
N,0.6173541963015647,"nP ‚ä§X‚ä§Xw‚àó. Similar as before, we know"
N,0.6187766714082503,"with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),
 1
‚àönXw‚àó ‚â§O(1) and
 1
‚àönP ‚ä§X‚ä§ ‚â§O(1). There-
fore, we have
F 1"
N,0.620199146514936,"nP ‚ä§X‚ä§Xw‚àó
 ‚â§‚à•F‚à•

1
‚àönP ‚ä§X‚ä§"
N,0.6216216216216216,"1
‚àönXw‚àó
 ‚â§O
 ÀÜœµ œÅ2 
."
N,0.6230440967283073,"Finally, let‚Äôs bound the third term
  1"
N,0.6244665718349929,"nP ‚ä§X‚ä§XP + œÅI
‚àí1 + F

1
nP ‚ä§X‚ä§Œæ. We Ô¨Årst bound the"
N,0.6258890469416786,norm of 1
N,0.6273115220483642,"nP ‚ä§X‚ä§Œæ. with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)), we know ‚à•Œæ‚à•‚â§2Œ≤‚àön. Therefore,
we know
 1"
N,0.6287339971550497,"nP ‚ä§X‚ä§Œæ
 ‚â§O(Œ≤/‚àön)
P ‚ä§X‚ä§¬ØŒæ
 , where ¬ØŒæ = Œæ/ ‚à•Œæ‚à•. According to Lemma 7,
with probability at least 1 ‚àíŒ∂/3, we have
P ‚ä§X‚ä§¬ØŒæ

‚â§
‚àör + O(
p"
N,0.6301564722617354,"log(1/Œ∂)). Overall,
with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)) ‚àíŒ∂/3,

1
nP ‚ä§X‚ä§Œæ
 ‚â§O"
N,0.631578947368421,"‚àörŒ≤ +
p"
N,0.6330014224751067,"log(1/Œ∂)Œ≤
‚àön ! ."
N,0.6344238975817923,"It‚Äôs not hard to verify that for any vector v
‚àà
Rd
in the subspace S, we have

  1"
N,0.635846372688478,"nP ‚ä§X‚ä§XP + œÅI
‚àí1 + F

v
 ‚â§O(‚à•v‚à•). Since 1"
N,0.6372688477951636,"nP ‚ä§X‚ä§Œæ lies on subspace S, we have   1"
N,0.6386913229018493,"nP ‚ä§X‚ä§XP + œÅI
‚àí1
+ F"
N,0.6401137980085349,"!
1
nP ‚ä§X‚ä§Œæ ‚â§O"
N,0.6415362731152204,"‚àörŒ≤ +
p"
N,0.6429587482219061,"log(1/Œ∂)Œ≤
‚àön ! ."
N,0.6443812233285917,"Combining the above analysis and taking a union bound over all the events, we know
with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)) ‚àí2Œ∂/3,"
N,0.6458036984352774,‚à•ÀÜw ‚àíw‚àó‚à•= O 
N,0.647226173541963,œÅ + ÀÜœµ
N,0.6486486486486487,œÅ + ÀÜœµ
N,0.6500711237553343,"œÅ2 + Œ≤ÀÜœµ
p"
N,0.65149359886202,"log(1/Œ∂)
œÅ‚àön
+
‚àörŒ≤ +
p"
N,0.6529160739687055,"log(1/Œ∂)Œ≤
‚àön !"
N,0.6543385490753911,"Suppose n ‚â•O(log(1/Œ∂)) and setting œÅ = ÀÜœµ1/3, we further have with probability at least 1 ‚àíŒ∂,"
N,0.6557610241820768,‚à•ÀÜw ‚àíw‚àó‚à•=O 
N,0.6571834992887624,ÀÜœµ1/3 + Œ≤ÀÜœµ2/3p
N,0.6586059743954481,"log(1/Œ∂)
‚àön
+
‚àörŒ≤ +
p"
N,0.6600284495021337,"log(1/Œ∂)Œ≤
‚àön ! ‚â§O "
N,0.6614509246088194,"ÀÜœµ1/3 + Œ≤
‚àör +
p"
N,0.662873399715505,"log(1/Œ∂)
‚àön ! ,"
N,0.6642958748221907,where the last inequality assumes ÀÜœµ < 1.
N,0.6657183499288762,Under review as a conference paper at ICLR 2022
N,0.6671408250355618,"We can also bound
 ÀÜP ÀÜw ‚àíw‚àó as follows,
 ÀÜP ÀÜw ‚àíw‚àó =
 ÀÜP ÀÜw ‚àíP ÀÜw + P ÀÜw ‚àíPw‚àó"
N,0.6685633001422475,"‚â§
 ÀÜP ÀÜw ‚àíP ÀÜw
 + ‚à•P ÀÜw ‚àíPw‚àó‚à•"
N,0.6699857752489331,"‚â§
 ÀÜP ‚àíP
 ‚à•ÀÜw‚à•+ ‚à•P‚à•‚à•ÀÜw ‚àíw‚àó‚à• ‚â§ÀÜœµO "
N,0.6714082503556188,"1 + ÀÜœµ1/3 + Œ≤
‚àör +
p"
N,0.6728307254623044,"log(1/Œ∂)
‚àön ! + O "
N,0.6742532005689901,"ÀÜœµ1/3 + Œ≤
‚àör +
p"
N,0.6756756756756757,"log(1/Œ∂)
‚àön ! ‚â§O "
N,0.6770981507823614,"ÀÜœµ1/3 + Œ≤
‚àör +
p"
N,0.6785206258890469,"log(1/Œ∂)
‚àön ! ‚ñ°"
N,0.6799431009957326,"B.4
ANALYSIS WITH Wp := (WEx1x1x‚ä§
1 W ‚ä§)Œ±"
N,0.6813655761024182,"In this section, we prove that DirectSet(Œ±) can also learn the projection matrix when we set Wp :=
(WEx1x1x‚ä§
1 W ‚ä§)Œ±. For the network architecture and data distribution, we follow exactly the same
setting as in Section 3.2. Therefore, we know Wp := (WEx1x1x‚ä§
1 W ‚ä§)Œ± = (W(I +œÉ2PB)W ‚ä§)Œ±.
Theorem 4. Suppose network architecture and data distribution are as deÔ¨Åned in Assumption 1 and
Assumption 2, respectively. Suppose we initialize online network W as Œ¥I, and run DirectPred(Œ±)
on population loss (see Eqn. 1) with inÔ¨Ånitesimal step size and Œ∑ weight decay. Suppose we set Wa =
W and Wp = (WEx1x1x‚ä§
1 W ‚ä§)Œ±. Assuming the weight decay coefÔ¨Åcient Œ∑ ‚àà

1
4(1+œÉ2)1+2Œ± , 1 4
"
N,0.6827880512091038,"and initialization scale Œ¥ >

1‚àí‚àö1‚àí4Œ∑"
N,0.6842105263157895,"2
1/(2Œ±)
, we know W converges to

1+‚àö1‚àí4Œ∑"
N,0.6856330014224751,"2
1/(2Œ±)
PS
when time goes to inÔ¨Ånity."
N,0.6870554765291608,"The only difference from Theorem 4 is that now the initialization Œ¥ is only required to be larger than
1
4(1+œÉ2)1+2Œ± . The proof is almost the same as in Theorem 1."
N,0.6884779516358464,"Proof of Theorem 4. Similar as in the proof of Theorem 1, we can write the dynamics on W is as
follows:
ÀôW =W ‚ä§
p (‚àíWpW(I + œÉ2PB) + Wa) ‚àíŒ∑W"
N,0.689900426742532,"=
W 2(I + œÉ2PB)
Œ± (‚àí
W 2(I + œÉ2PB)
Œ± W(I + œÉ2PB) + W) ‚àíŒ∑W"
N,0.6913229018492176,"=W

‚àí(I + œÉ2PB)1+2Œ± |W|4Œ± + |W|2Œ± ‚àíŒ∑

."
N,0.6927453769559033,"Dynamics for ŒªB:
We can write down the dynamics for ŒªB as follows:"
N,0.6941678520625889,"ÀôŒªB = ŒªB
h
‚àí(1 + œÉ2)1+2Œ± |ŒªB|4Œ± + |ŒªB|2Œ± ‚àíŒ∑
i"
N,0.6955903271692745,"When Œ∑ >
1
4(1+œÉ2)1+2Œ± , we know ÀôŒªB < 0 for any ŒªB > 0 and ŒªB = 0 is a critical point. This
means, as long as Œ∑ >
1
4(1+œÉ2)1+2Œ± , ŒªB must converge to zero."
N,0.6970128022759602,"Dynamics for ŒªS:
The dynamics is same as when setting Wp = (WW ‚ä§)Œ±,"
N,0.6984352773826458,"ÀôŒªS = ŒªS
h
‚àí|ŒªS|4Œ± + |ŒªS|2Œ± ‚àíŒ∑
i
."
N,0.6998577524893315,so when 0 < Œ∑ < 1
N,0.701280227596017,4 and initialization Œ¥2Œ± > 1‚àí‚àö1‚àí4Œ∑
N,0.7027027027027027,"2
, we know Œª2Œ±
S converges to 1+‚àö1‚àí4Œ∑ 2
."
N,0.7041251778093883,"Overall, we know when
1
4(1+œÉ2)1+2Œ± < Œ∑ < 1"
N,0.705547652916074,"4 and Œ¥ >

1‚àí‚àö1‚àí4Œ∑"
N,0.7069701280227596,"2
1/(2Œ±)
, we have ŒªB converge to"
N,0.7083926031294452,"zero and ŒªS converge to

1+‚àö1‚àí4Œ∑"
N,0.7098150782361309,"2
1/(2Œ±)
. That is, matrix W converges to

1+‚àö1‚àí4Œ∑"
N,0.7112375533428165,"2
1/(2Œ±)
PS.
‚ñ°"
N,0.7126600284495022,Under review as a conference paper at ICLR 2022
N,0.7140825035561877,"B.5
TECHNICAL LEMMAS"
N,0.7155049786628734,"Lemma 3. Suppose {x(i), x(i)
1 , x(i)
2 }n
i=1 are sampled as decribed in Section 3.
Suppose n ‚â•
O(d/ÀÜœµ2), with probability at least 1 ‚àíO(d2) exp
 
‚àí‚Ñ¶(ÀÜœµ2n/d2)

, we have

1
n n
X"
N,0.716927453769559,"i=1
x(i)
1 [x(i)
1 ]‚ä§‚àíI ‚àíœÉ2PB ,"
N,0.7183499288762447,"1
n n
X"
N,0.7197724039829303,"i=1
x(i)
1 [x(i)
2 ]‚ä§‚àíI ,"
N,0.7211948790896159,"1
n n
X"
N,0.7226173541963016,"i=1
x(i)[x(i)]‚ä§‚àíI ‚â§ÀÜœµ."
N,0.7240398293029872,"Proof of Lemma 3. For each x(i)
1 , we can write it as x(i) + z(i)
1
where x(i) ‚àºN(0, I) and z(i)
1
‚àº
N(0, œÉ2PB). So we have"
N,0.7254623044096729,"1
n n
X"
N,0.7268847795163584,"i=1
x(i)
1 [x(i)
1 ]‚ä§= 1 n n
X i=1"
N,0.7283072546230441,"
x(i)[x(i)]‚ä§+ z(i)
1 [z(i)
1 ]‚ä§+ x(i)[z(i)
1 ]‚ä§+ z(i)
1 [x(i)]‚ä§
."
N,0.7297297297297297,"According to Lemma 9, we know as long as n ‚â•O(d/ÀÜœµ2), with probability at least 1 ‚àí
exp(‚àí‚Ñ¶(ÀÜœµ2n)),

1
n n
X"
N,0.7311522048364154,"i=1
x(i)[x(i)]‚ä§‚àíI ‚â§ÀÜœµ."
N,0.732574679943101,"Similarly, with probability at least 1 ‚àíexp(‚àí‚Ñ¶(ÀÜœµ2n)),

1
n n
X"
N,0.7339971550497866,"i=1
z(i)
1 [z(i)
1 ]‚ä§‚àíœÉ2PB ‚â§ÀÜœµ."
N,0.7354196301564723,"Next we bound
 1"
N,0.7368421052631579,"n
Pn
i=1 x(i)[z(i)
1 ]‚ä§ . We know each entry in matrix 1"
N,0.7382645803698435,"n
Pn
i=1 x(i)[z(i)
1 ]‚ä§is the"
N,0.7396870554765291,"average of n zero-mean O(1)-subexponential independent random variables. Therefore, according
to the Bernstein‚Äôs inequality, for any Ô¨Åxed entry (k, l), with probability at least 1 ‚àíexp
 
‚àíÀÜœµ2n/d2
, ""
1
n n
X"
N,0.7411095305832148,"i=1
x(i)[z(i)
1 ]‚ä§
# k,l"
N,0.7425320056899004,‚â§ÀÜœµ/d.
N,0.7439544807965861,"Taking a union bound over all the entries, we know with probability at least 1 ‚àíd2 exp
 
‚àíÀÜœµ2n/d2
,

1
n n
X"
N,0.7453769559032717,"i=1
x(i)[z(i)
1 ]‚ä§
 ‚â§"
N,0.7467994310099573,"1
n n
X"
N,0.748221906116643,"i=1
x(i)[z(i)
1 ]‚ä§

F
‚â§ÀÜœµ."
N,0.7496443812233285,"The same analysis also applies to
 1"
N,0.7510668563300142,"n
Pn
i=1 z(i)
1 [x(i)]‚ä§ . Combing all the bounds, we know with"
N,0.7524893314366998,"probability at least 1 ‚àíO(d2) exp
 
‚àí‚Ñ¶(ÀÜœµ2n/d2)

,

1
n n
X"
N,0.7539118065433855,"i=1
x(i)
1 [x(i)
1 ]‚ä§‚àíI ‚àíœÉ2PB ‚â§4ÀÜœµ."
N,0.7553342816500711,"Similarly, we can prove that with probability at least 1 ‚àíO(d2) exp
 
‚àí‚Ñ¶(ÀÜœµ2n/d2)

,

1
n n
X"
N,0.7567567567567568,"i=1
x(i)
1 [x(i)
2 ]‚ä§‚àíI ‚â§4ÀÜœµ."
N,0.7581792318634424,"Changing ÀÜœµ to ÀÜœµ‚Ä≤/4 Ô¨Ånishes the proof.
‚ñ°
Lemma 4. Let X ‚ààRn√ód be a standard Gaussian matrix, and let U ‚ààRd√ór be a matrix with
orthonormal columns. Suppose n ‚â•2r, with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)), we know"
N,0.7596017069701281,‚Ñ¶(1) ‚â§Œªmin  1
N,0.7610241820768137,"nU ‚ä§X‚ä§XU

‚â§Œªmax  1"
N,0.7624466571834992,"nU ‚ä§X‚ä§XU

‚â§O(1)."
N,0.7638691322901849,Under review as a conference paper at ICLR 2022
N,0.7652916073968705,"Proof of Lemma 4. Since U has orthonormal columns, we know XU is a n √ó r matrix with each
entry independently sampled from N(0, 1). According to Lemma 9, we know when n ‚â•2r, with
probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),"
N,0.7667140825035562,‚Ñ¶(1) ‚â§œÉmin
N,0.7681365576102418," 1
‚àönXU

‚â§œÉmax"
N,0.7695590327169275," 1
‚àönXU

‚â§O(1)."
N,0.7709815078236131,This immediately implies that
N,0.7724039829302988,‚Ñ¶(1) ‚â§Œªmin  1
N,0.7738264580369844,"nU ‚ä§X‚ä§XU

‚â§Œªmax  1"
N,0.7752489331436699,"nU ‚ä§X‚ä§XU

‚â§O(1)."
N,0.7766714082503556,"‚ñ°
Lemma 5. Let ‚àÜbe a d√ód matrix with Frobenius norm ÀÜœµ, and let X be a n√ód standard Gaussian
matrix. We know with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),

1
‚àönX‚àÜ

F
‚â§O(ÀÜœµ)."
N,0.7780938833570412,"Proof of Lemma 5. Let the singular value decomposition of ‚àÜbe UŒ£V ‚ä§, where U, V have or-
thonormal columns and Œ£ is a diagonal matrix with diagonals equal to singular values œÉi‚Äôs. Since
‚à•‚àÜ‚à•F = ÀÜœµ, we know Pd
i=1 œÉ2
i = ÀÜœµ2."
N,0.7795163584637269,"Since U is an orthonormal matrix, we know ÀÜX := XU is still an n √ó d standard Gaussian matrix.
Next, we bound the Frobenius norm of e
X := ÀÜXŒ£. It‚Äôs not hard to verify that all the entries in e
X are
independent Gaussian variables and e
Xij ‚àºN(0, œÉ2
j ). According to the Bernstein‚Äôs inequality for
sum of independent and sub-exponential random variables, we have for every t > 0, Pr Ô£Æ Ô£∞  X"
N,0.7809388335704125,"i‚àà[n],j‚àà[d]
e
X2
ij ‚àínÀÜœµ2 ‚â•t Ô£π"
N,0.7823613086770982,"Ô£ª‚â§2 exp """
N,0.7837837837837838,"‚àíc min t2
P"
N,0.7852062588904695,"i‚àà[n],j‚àà[d] œÉ4
j
,
t
maxj‚àà[d] œÉ2
j !# ."
N,0.786628733997155,"Since Pd
j=1 œÉ2
j
= ‚à•‚àÜ‚à•2
F
= ÀÜœµ2, we know maxj‚àà[d] œÉ2
j
‚â§ÀÜœµ2. We also have P"
N,0.7880512091038406,"j‚àà[d] œÉ4
j
‚â§
P"
N,0.7894736842105263,"j‚àà[d] œÉ2
j
2
= ÀÜœµ4. Therefore, we have Pr Ô£Æ Ô£∞  X"
N,0.7908961593172119,"i‚àà[n],j‚àà[d]
e
X2
ij ‚àínÀÜœµ2 ‚â•t Ô£π"
N,0.7923186344238976,"Ô£ª‚â§2 exp

‚àíc min
 t2"
N,0.7937411095305832,"nÀÜœµ4 , t ÀÜœµ2 
."
N,0.7951635846372689,"Replacing t by nÀÜœµ2, we concluded that with probability at least 1 ‚àíexp(‚àí‚Ñ¶(n)),
 e
X

2"
N,0.7965860597439545,F ‚â§2nÀÜœµ2.
N,0.7980085348506402,"Furthermore, since
V ‚ä§ = 1, we have

1
‚àönX‚àÜ

F
=

1
‚àön
e
XV ‚ä§

F
‚â§

1
‚àön
e
X

F
‚à•V ‚à•‚â§O(ÀÜœµ)."
N,0.7994310099573257,"‚ñ°
Lemma 6. Let ‚àÜ‚ä§be a d√ód matrix with Frebenius norm ÀÜœµ and let X‚ä§be a d√ón standard Gaussian
matrix. Let ¬ØŒæ be a unit vector with dimension n. We know with probability at least 1 ‚àíŒ∂/3,
‚àÜ‚ä§X‚ä§¬ØŒæ
 ‚â§O(ÀÜœµ
p"
N,0.8008534850640113,log(1/Œ∂)).
N,0.802275960170697,"Proof of Lemma 6. Let the sigular value decomposition of ‚àÜ‚ä§be UŒ£V ‚ä§. We know X‚ä§¬ØŒæ is a d-
dimensional standard Gaussian vector. Further, we know V ‚ä§X‚ä§¬ØŒæ is also a d-dimensional standard
Gaussian vector. So Œ£V ‚ä§X‚ä§¬ØŒæ has independent Gaussian entries with its i-th entry distributed
as N(0, œÉ2
i ). According to the Bernstein‚Äôs inequality for sum of independent and sub-exponential
random variables, we have for every t > 0,"
N,0.8036984352773826,"Pr
h
Œ£V ‚ä§X‚ä§¬ØŒæ
2 ‚àíÀÜœµ2 ‚â•t
i
‚â§2 exp

‚àíc min
t2"
N,0.8051209103840683,"ÀÜœµ4 , t ÀÜœµ2 
."
N,0.8065433854907539,Under review as a conference paper at ICLR 2022
N,0.8079658605974396,"Choosing t as O(ÀÜœµ2 log(1/Œ∂)), we know with probability at least 1 ‚àíŒ∂/3, we have
Œ£V ‚ä§X‚ä§¬ØŒæ
2 ‚â§O
 
ÀÜœµ2 log(1/Œ∂)

."
N,0.8093883357041252,"Since ‚à•U‚à•= 1, we further have
‚àÜ‚ä§X‚ä§¬ØŒæ
 =
UŒ£V ‚ä§X‚ä§¬ØŒæ
 ‚â§‚à•U‚à•
Œ£V ‚ä§X‚ä§¬ØŒæ
 ‚â§O

ÀÜœµ
p"
N,0.8108108108108109,"log(1/Œ∂)
"
N,0.8122332859174964,"‚ñ°
Lemma 7. Let P ‚ààRd√ód be a projection matrix on a r-dimensional subspace, and let ¬ØŒæ be a unit
vector in Rd. Let X‚ä§be a d √ó n standard Gaussian matrix that is independent with P and Œæ. With
probability at least 1 ‚àíŒ∂/3, we have
P ‚ä§X‚ä§¬ØŒæ
 ‚â§‚àör + O(
p"
N,0.813655761024182,log(1/Œ∂)).
N,0.8150782361308677,"Proof of Lemma 7. Since P is a projection matrix on an r-dimensional subspace, we can write P
as UU ‚ä§, where U ‚ààRd√ór has orthonormal columns. We know U ‚ä§X‚ä§is still a standard Gaussian
matrix with dimension r √ó n. Furthermore, U ‚ä§X‚ä§¬ØŒæ is an r-dimensional standard Gaussian vector.
According to Lemma 8, with probability at least 1 ‚àíŒ∂/3, we have
U ‚ä§X‚ä§¬ØŒæ
 ‚â§‚àör + O(
p"
N,0.8165007112375533,log(1/Œ∂)).
N,0.817923186344239,"Since ‚à•U‚à•= 1, we further have
P ‚ä§X‚ä§¬ØŒæ
 =
UU ‚ä§X‚ä§¬ØŒæ
 ‚â§‚à•U‚à•
U ‚ä§X‚ä§¬ØŒæ
 ‚â§‚àör + O(
p"
N,0.8193456614509246,log(1/Œ∂)). ‚ñ°
N,0.8207681365576103,"C
ANALYSIS OF DEEP LINEAR NETWORKS"
N,0.8221906116642959,"In this section, we extend the analysis in Section 3.2 to deep linear networks. We consider the same
data distribution as deÔ¨Åned in Assumption 2. We consider the following network,
Assumption 4 (Deep linear network). The online network is an l-layer linear networks
WlWl‚àí1 ¬∑ ¬∑ ¬∑ W1 with each Wi ‚ààRd√ód. The target network has the same architecture with weight
matrices Wa,lWa,l‚àí1 ¬∑ ¬∑ ¬∑ Wa,1. For convenience, we denote W as WlWl‚àí1 ¬∑ ¬∑ ¬∑ W1 and denote Wa
as Wa,lWa,l‚àí1 ¬∑ ¬∑ ¬∑ Wa,1."
N,0.8236130867709816,"Training procedure:
At the initialization, we initialize each Wi as Œ¥1/lId. Through the training,
we Ô¨Åx Wp as
 
WW ‚ä§Œ± and Ô¨Åx each Wa,i as Wi. We run gradient Ô¨Çow on every Wi with weight
decay Œ∑. The population loss is"
N,0.8250355618776671,"L({Wi}, Wp, {Wa,i}) := 1"
N,0.8264580369843528,"2Ex1,x2 ‚à•WpWlWl‚àí1 ¬∑ ¬∑ ¬∑ W1x1 ‚àíStopGrad(Wa,lWa,l‚àí1 ¬∑ ¬∑ ¬∑ Wa,1x2)‚à•2 ."
N,0.8278805120910384,"Theorem 5. Suppose the data distribution and network architecture satisÔ¨Åes Assumption 2 and As-
sumption 4, respectively. Suppose we train the network as described above. Assuming the weight
decay coefÔ¨Åcient"
N,0.829302987197724,"Œ∑
‚àà

2Œ±l(2Œ±l+2l‚àí2)1+ 1 Œ± ‚àí1 Œ±l"
N,0.8307254623044097,(4Œ±l+2l‚àí2)2+ 1 Œ± ‚àí1
N,0.8321479374110953,Œ±l (1+œÉ2)1+ 1 Œ± ‚àí1
N,0.833570412517781,"Œ±l , 2Œ±l(2Œ±l+2l‚àí2)1+ 1 Œ± ‚àí1 Œ±l"
N,0.8349928876244666,(4Œ±l+2l‚àí2)2+ 1 Œ± ‚àí1 Œ±l
N,0.8364153627311522,"
, and initialization scale Œ¥
‚â•

2Œ±l+2l‚àí2
4Œ±l+2l‚àí2
 1"
N,0.8378378378378378,"2Œ± , we know W converges to cPS as time goes to inÔ¨Ånity, where c is a positive number"
N,0.8392603129445235,"within

2Œ±l+2l‚àí2
4Œ±l+2l‚àí2
 1"
N,0.8406827880512091,"2Œ± , 1

."
N,0.8421052631578947,"Similar as in the setting of single-layer linear networks, we prove Theorem 5 by analyzing the
dynamics of the eigenvalues of W. Note that with constant Œ±, the upper/lower bounds for Œ∑ and
scalar c in the Theorem are always constants no matter how large l is."
N,0.8435277382645804,"Proof of Theorem 5. For j ‚â•i, we use W[j:i] to denote WjWj‚àí1 ¬∑ ¬∑ ¬∑ Wi and for j < i have
W[j:i] = I. We use similar notations for Wa,[j:i]. For each Wi, we can compute its dynamics as
follows:
ÀôWi = ‚àí
 
WpW[l:i+1]
‚ä§ 
WpW(I + œÉ2PB)
  
W[i‚àí1:1]
‚ä§+
 
WpWa,[l:i+1]
‚ä§Wa
 
Wa,[i‚àí1:1]
‚ä§‚àíŒ∑Wi."
N,0.844950213371266,Under review as a conference paper at ICLR 2022
N,0.8463726884779517,"It‚Äôs clear that through the training all Wi‚Äôs remains the same and they are simultaneously diagonal-
izable with Wp, I and PB. We also have Wa = W and Wp = |W|2Œ± . Since we will ensure that W
is always positive semi-deÔ¨Ånite so Wp = |W|2Œ± = W 2Œ± = W 2Œ±l
i
. So the dynamics for each Wi
can be simpliÔ¨Åed as follows:"
N,0.8477951635846372,"ÀôWi = ‚àíW 4Œ±l+2l‚àí1
i
(I + œÉ2PB) + W 2Œ±l+2l‚àí1
i
‚àíŒ∑Wi."
N,0.8492176386913229,"Let the eigenvalue decomposition of Wi be Pd
i=1 ŒΩiuiu‚ä§
i , with span({ud‚àír+1, ¬∑ ¬∑ ¬∑ , ud}) equals to
subspace B. We can separately analyze the dynamics of each ŒΩi. Furthermore, we know ŒΩ1, ¬∑ ¬∑ ¬∑ , ŒΩr
have the same value ŒΩS and ŒΩd‚àír+1, ¬∑ ¬∑ ¬∑ , ŒΩd have the same value ŒΩB. We can write down the dy-
namics for ŒΩS and ŒΩB as follows,"
N,0.8506401137980085,"ÀôŒΩS = ‚àíŒΩ4Œ±l+2l‚àí1
S
+ ŒΩ2Œ±l+2l‚àí1
S
‚àíŒ∑ŒΩS,"
N,0.8520625889046942,"ÀôŒΩB = ‚àíŒΩ4Œ±l+2l‚àí1
B
(1 + œÉ2) + ŒΩ2Œ±l+2l‚àí1
B
‚àíŒ∑ŒΩB."
N,0.8534850640113798,"Let ŒªS be the eigenvalue of W corresponding to eigen-directions u1, ¬∑ ¬∑ ¬∑ , ur, and let ŒªB be the
eigenvalue of W corresponding to eigen-directions ud‚àír+1, ¬∑ ¬∑ ¬∑ , ud. We know ŒªS = ŒΩl
S and ŒªB =
ŒΩl
B. So we can write down the dynamics for ŒªB as follows,"
N,0.8549075391180654,"ÀôŒªB = lŒΩl‚àí1
B
ÀôŒΩB = ‚àílŒΩ4Œ±l+3l‚àí2
B
(1 + œÉ2) + lŒΩ2Œ±l+3l‚àí2
B
‚àílŒ∑ŒΩl
B"
N,0.8563300142247511,"= ‚àílŒª4Œ±+3‚àí2/l
B
(1 + œÉ2) + lŒª2Œ±+3‚àí2/l
B
‚àílŒ∑ŒªB,"
N,0.8577524893314367,and similarly for ŒªS we have
N,0.8591749644381224,"ÀôŒªS = ‚àílŒª4Œ±+3‚àí2/l
S
+ lŒª2Œ±+3‚àí2/l
S
‚àílŒ∑ŒªS."
N,0.8605974395448079,"Dynamics for ŒªB:
We can write the dynamics on ŒªB as follows,"
N,0.8620199146514936,"ÀôŒªB = lŒªBg(ŒªB),"
N,0.8634423897581792,"where g(ŒªB) := ‚àíŒª4Œ±+2‚àí2/l
B
(1 + œÉ2) + Œª2Œ±+2‚àí2/l
B
‚àíŒ∑. We show that when Œ∑ is large enough,
g(ŒªB) is negative for any positive ŒªB. We compute the maximum value of g(ŒªB) for ŒªB > 0. We
Ô¨Årst compute the derivative of g as follows:"
N,0.8648648648648649,"g‚Ä≤(ŒªB) = ‚àí(4Œ± + 2 ‚àí2/l)(1 + œÉ2)Œª4Œ±+1‚àí2/l
B
+ (2Œ± + 2 ‚àí2/l)Œª2Œ±+1‚àí2/l
B
=Œª2Œ±+1‚àí2/l
B
 
‚àí(4Œ± + 2 ‚àí2/l)(1 + œÉ2)Œª2Œ±
B + (2Œ± + 2 ‚àí2/l)

."
N,0.8662873399715505,"It‚Äôs clear that g‚Ä≤(ŒªB) > 0 for Œª2Œ±
B
‚àà(0,
2Œ±l+2l‚àí2
(4Œ±l+2l‚àí2)(1+œÉ2)) and g‚Ä≤(ŒªB) < 0 for Œª2Œ±
B
‚àà
(
2Œ±l+2l‚àí2
(4Œ±l+2l‚àí2)(1+œÉ2), +‚àû). Therefore, the maximum value of g(ŒªB) for positive ŒªB takes at Œª‚àó
B =

2Œ±l+2l‚àí2
(4Œ±l+2l‚àí2)(1+œÉ2)
 1"
N,0.8677098150782361,2Œ± and
N,0.8691322901849218,"g(Œª‚àó
B) = ‚àí

2Œ±l + 2l ‚àí2
(4Œ±l + 2l ‚àí2)(1 + œÉ2) 2+ 1 Œ± ‚àí1"
N,0.8705547652916074,"Œ±l
(1 + œÉ2) +

2Œ±l + 2l ‚àí2
(4Œ±l + 2l ‚àí2)(1 + œÉ2) 1+ 1 Œ± ‚àí1 Œ±l
‚àíŒ∑"
N,0.871977240398293,"=
2Œ±l(2Œ±l + 2l ‚àí2)1+ 1 Œ± ‚àí1 Œ±l"
N,0.8733997155049786,(4Œ±l + 2l ‚àí2)2+ 1 Œ± ‚àí1
N,0.8748221906116643,Œ±l (1 + œÉ2)1+ 1 Œ± ‚àí1
N,0.8762446657183499,Œ±l ‚àíŒ∑.
N,0.8776671408250356,"As long as Œ∑ >
2Œ±l(2Œ±l+2l‚àí2)1+ 1 Œ± ‚àí1 Œ±l"
N,0.8790896159317212,(4Œ±l+2l‚àí2)2+ 1 Œ± ‚àí1
N,0.8805120910384068,Œ±l (1+œÉ2)1+ 1 Œ± ‚àí1
N,0.8819345661450925,"Œ±l , we know g(ŒªB) < 0 for any ŒªB > 0, which"
N,0.883357041251778,further implies that ÀôŒªB < 0 for any ŒªB > 0. So ŒªB converges to zero.
N,0.8847795163584637,"Dynamics for ŒªS :
We can write down the dynamics on ŒªS as follows,"
N,0.8862019914651493,"ÀôŒªS = lŒªSh(ŒªS),"
N,0.887624466571835,"where h(ŒªS) = ‚àíŒª4Œ±+2‚àí2/l
S
+ Œª2Œ±+2‚àí2/l
S
‚àíŒ∑. We compute the derivative of h as follows:"
N,0.8890469416785206,"h‚Ä≤(ŒªS) = Œª2Œ±+1‚àí2/l
S
 
‚àí(4Œ± + 2 ‚àí2/l)Œª2Œ±
S + (2Œ± + 2 ‚àí2/l)

."
N,0.8904694167852063,Under review as a conference paper at ICLR 2022
N,0.8918918918918919,"So h(ŒªS) is increasing in (0,

2Œ±l+2l‚àí2
4Œ±l+2l‚àí2
 1"
N,0.8933143669985776,"2Œ± ) and is decreasing in (

2Œ±l+2l‚àí2
4Œ±l+2l‚àí2
 1"
N,0.8947368421052632,"2Œ± , ‚àû). The maxi-"
N,0.8961593172119487,"mum value of h for positive ŒªS takes at Œª‚àó
S =

2Œ±l+2l‚àí2
4Œ±l+2l‚àí2
 1"
N,0.8975817923186344,2Œ± and we have
N,0.89900426742532,"h(Œª‚àó
S) = 2Œ±l(2Œ±l + 2l ‚àí2)1+ 1 Œ± ‚àí1 Œ±l"
N,0.9004267425320057,(4Œ±l + 2l ‚àí2)2+ 1 Œ± ‚àí1
N,0.9018492176386913,"Œ±l
‚àíŒ∑."
N,0.903271692745377,As long as Œ∑ < 2Œ±l(2Œ±l+2l‚àí2)1+ 1 Œ± ‚àí1 Œ±l
N,0.9046941678520626,(4Œ±l+2l‚àí2)2+ 1 Œ± ‚àí1
N,0.9061166429587483,"Œ±l
, we have h(Œª‚àó
S) > 0. Furthermore, since h is increasing in"
N,0.9075391180654339,"(0, Œª‚àó
S) and is decreasing in (Œª‚àó
S, ‚àû) and h(0), h(‚àû) < 0, we know there exists Œª‚àí
S ‚àà(0, Œª‚àó
S), Œª+
S ‚àà
(Œª‚àó
S, ‚àû) such that h(ŒªS) < 0 in (0, Œª‚àí
S ), h(ŒªS) > 0 in (Œª‚àí
S , Œª+
S ) and h(ŒªS) < 0 in (Œª+
S , ‚àû).
Therefore, as long as Œ¥ ‚â•Œª‚àó
S > Œª‚àí
S , we have ŒªS converges to Œª+
S . Since h(1) < 0, we know"
N,0.9089615931721194,"Œª+
S ‚àà(

2Œ±l+2l‚àí2
4Œ±l+2l‚àí2
 1"
N,0.9103840682788051,"2Œ± , 1)."
N,0.9118065433854907,"Overall as long as Œ∑ ‚àà

2Œ±l(2Œ±l+2l‚àí2)1+ 1 Œ± ‚àí1 Œ±l"
N,0.9132290184921764,(4Œ±l+2l‚àí2)2+ 1 Œ± ‚àí1
N,0.914651493598862,Œ±l (1+œÉ2)1+ 1 Œ± ‚àí1
N,0.9160739687055477,"Œ±l , 2Œ±l(2Œ±l+2l‚àí2)1+ 1 Œ± ‚àí1 Œ±l"
N,0.9174964438122333,(4Œ±l+2l‚àí2)2+ 1 Œ± ‚àí1 Œ±l
N,0.918918918918919,"
, we know W"
N,0.9203413940256046,"converges to cPS, where c is a positive number within (

2Œ±l+2l‚àí2
4Œ±l+2l‚àí2
 1"
N,0.9217638691322901,"2Œ± , 1).
‚ñ°"
N,0.9231863442389758,"D
ANALYSIS OF PREDICTOR REGULARIZATION."
N,0.9246088193456614,"In this section, we study the inÔ¨Çuence of predictor regularization in a simple linear setting. In
particular, we consider the same setting as in Section 3.2 except that we set Wp := (WW ‚ä§)Œ± + œµI."
N,0.9260312944523471,Theorem 6. In the setting of Theorem 1 except that we set Wp = (WW ‚ä§)Œ± + œµI. We have
N,0.9274537695590327,"‚Ä¢ when œµ ‚àà[0, 1+‚àö1‚àí4Œ∑"
N,0.9288762446657184,"2
), as long as Œ¥ >

max

1‚àí‚àö1‚àí4Œ∑"
N,0.930298719772404,"2
‚àíœµ, 0
 1"
N,0.9317211948790897,"2Œ± , we have W con-"
N,0.9331436699857752,"verges to

1+‚àö1‚àí4Œ∑"
N,0.9345661450924608,"2
‚àíœµ
 1"
N,0.9359886201991465,2Œ± PS;
N,0.9374110953058321,‚Ä¢ when œµ ‚â•1+‚àö1‚àí4Œ∑
N,0.9388335704125178,"2
, W always converges to zero."
N,0.9402560455192034,"Proof of Theorem 6. We can write the dynamics of W as follows,
ÀôW =W ‚ä§
p (‚àíWpW(I + œÉ2PB) + Wa) ‚àíŒ∑W"
N,0.9416785206258891,"=W

‚àí(I + œÉ2PB)

|W|2Œ± + œµI
2
+

|W|2Œ± + œµI

‚àíŒ∑

."
N,0.9431009957325747,"Let the eigenvalue decomposition of W be Pd
i=1 Œªiuiu‚ä§
i , with span({ud‚àír+1, ¬∑ ¬∑ ¬∑ , ud}) equals to
subspace B. We can separately analyze the dynamics of each Œªi. Furthermore, we know Œª1, ¬∑ ¬∑ ¬∑ , Œªr
have the same value ŒªS and Œªd‚àír+1, ¬∑ ¬∑ ¬∑ , Œªd have the same value ŒªB."
N,0.9445234708392604,"Dynamics for ŒªB:
We can write down the dynamics for ŒªB as follows:"
N,0.9459459459459459,ÀôŒªB = ŒªB
N,0.9473684210526315,"
‚àí(1 + œÉ2)

|ŒªB|2Œ± + œµ
2
+

|ŒªB|2Œ± + œµ

‚àíŒ∑
"
N,0.9487908961593172,"When Œ∑ >
1
4(1+œÉ2), we still know ÀôŒªB < 0 for any ŒªB > 0 and ŒªB = 0 is a critical point. So ŒªB
converges to zero."
N,0.9502133712660028,"Dynamics for ŒªS:
We can write down the dynamics for ŒªS as follows:"
N,0.9516358463726885,ÀôŒªS =ŒªS
N,0.9530583214793741,"
‚àí

|ŒªS|2Œ± + œµ
2
+

|ŒªS|2Œ± + œµ

‚àíŒ∑
 = ‚àíŒªS"
N,0.9544807965860598,"
|ŒªS|2Œ± + œµ ‚àí1 ‚àí‚àö1 ‚àí4Œ∑ 2"
N,0.9559032716927454," 
|ŒªS|2Œ± + œµ ‚àí1 + ‚àö1 ‚àí4Œ∑ 2 
,"
N,0.957325746799431,where the second inequality assumes 0 < Œ∑ < 1
WE HAVE,0.9587482219061166,4. We have
WE HAVE,0.9601706970128022,Under review as a conference paper at ICLR 2022
WE HAVE,0.9615931721194879,"‚Ä¢ when œµ ‚àà[0, 1+‚àö1‚àí4Œ∑"
WE HAVE,0.9630156472261735,"2
), as long as Œ¥ >

max

1‚àí‚àö1‚àí4Œ∑"
WE HAVE,0.9644381223328592,"2
‚àíœµ, 0
 1"
WE HAVE,0.9658605974395448,"2Œ± , we have ŒªS con-"
WE HAVE,0.9672830725462305,"verges to

1+‚àö1‚àí4Œ∑"
WE HAVE,0.968705547652916,"2
‚àíœµ
 1"
WE HAVE,0.9701280227596017,2Œ± > 0;
WE HAVE,0.9715504978662873,‚Ä¢ when œµ ‚â•1+‚àö1‚àí4Œ∑
WE HAVE,0.972972972972973,"2
, ŒªS always converges to zero. ‚ñ°"
WE HAVE,0.9743954480796586,"E
TECHNICAL TOOLS"
WE HAVE,0.9758179231863442,"E.1
NORM OF RANDOM VECTORS"
WE HAVE,0.9772403982930299,"The following lemma shows that a standard Gaussian vector with dimension n has ‚Ñì2 norm concen-
trated at ‚àön."
WE HAVE,0.9786628733997155,"Lemma 8 (Theorem 3.1.1 in Vershynin (2018)). Let X = (X1, X2, ¬∑ ¬∑ ¬∑ , Xn) ‚ààRn be a random
vector with each entry independently sampled from N(0, 1). Then"
WE HAVE,0.9800853485064012,"Pr[
‚à•x‚à•‚àí‚àön
 ‚â•t] ‚â§2 exp(‚àít2/C2),"
WE HAVE,0.9815078236130867,where C is an absolute constant.
WE HAVE,0.9829302987197724,"E.2
SINGULAR VALUES OF GAUSSIAN MATRICES"
WE HAVE,0.984352773826458,The following lemma shows a tall random Gaussian matrix is well-conditioned with high probability.
WE HAVE,0.9857752489331437,"Lemma 9 (Corollary 5.35 in Vershynin (2010)). Let A be an N √ó n matrix whose entries are
independent standard normal random variables. Then for every t ‚â•0 with probability at least
1 ‚àí2 exp(‚àít2/2) one has
‚àö"
WE HAVE,0.9871977240398293,"N ‚àí‚àön ‚àít ‚â§smin(A) ‚â§smax(A) ‚â§
‚àö"
WE HAVE,0.9886201991465149,N + ‚àön + t
WE HAVE,0.9900426742532006,"E.3
PERTURBATION BOUND FOR MATRIX PSEUDO-INVERSE"
WE HAVE,0.9914651493598862,"With a lowerbound on œÉmin(A), we can get bounds for the perturbation of pseudo-inverse."
WE HAVE,0.9928876244665719,"Lemma 10 (Theorem 3.4 in Stewart (1977)). Consider the perturbation of a matrix A ‚ààRm√ón :
B = A + E. Assume that rank(A) = rank(B) = n, then
B‚Ä† ‚àíA‚Ä† ‚â§
‚àö"
WE HAVE,0.9943100995732574,"2
A‚Ä† B‚Ä† ‚à•E‚à•."
WE HAVE,0.9957325746799431,The following corollary is particularly useful for us.
WE HAVE,0.9971550497866287,"Lemma 11 (Lemma G.8 in Ge et al. (2015)). Consider the perturbation of a matrix A ‚ààRm√ón :
B = A + E where ‚à•E‚à•‚â§œÉmin(A)/2. Assume that rank(A) = rank(B) = n, then
B‚Ä† ‚àíA‚Ä† ‚â§2
‚àö"
WE HAVE,0.9985775248933144,2 ‚à•E‚à•/œÉmin(A)2.
