Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005434782608695652,"Attention mechanisms have been explored with CNNs, both across the spatial
and channel dimensions. However, all the existing methods devote the attention
modules to capture local interactions from the current feature map only, disre-
garded the valuable previous knowledge that is acquired by the earlier layers. This
paper tackles the following question: Can one incorporate previous knowledge
aggregation while learning channel attention more efﬁciently? To this end, we
propose a Previous Knowledge Channel Attention Module(PKCAM), that cap-
tures channel-wise relations across different layers to model the global context.
Our proposed module PKCAM is easily integrated into any feed-forward CNN
architectures and trained in an end-to-end fashion with a negligible footprint due
to its lightweight property. We validate our novel architecture through extensive
experiments on image classiﬁcation and object detection tasks with different back-
bones. Our experiments show consistent improvements in performances against
their counterparts. We also conduct experiments that probe the robustness of the
learned representations."
INTRODUCTION,0.010869565217391304,"1
INTRODUCTION"
INTRODUCTION,0.016304347826086956,"Over the years, CNN architectures have evolved with many ideas to better deal with spatial image
features. Moreover, their localized nature makes such features lack the global view of the image.
Deeper architectures emerged that stack multiple convolution layers, known with different names;
backbone, bottleneck, feature extractor, or encoder. The main feature of such architectures is their
ability to cover spatial features at multiple scales. As we go deeper, the feature maps get smaller,
while their content represents a wider region in the space, which gets us closer to better semantics
of the image contents Luo et al. (2016). With the emergence of AlexNet Krizhevsky et al. (2012),
many kinds of research investigate to further improve the performance of deep CNNs. Simonyan
& Zisserman (2014) He et al. (2016) Szegedy et al. (2017) Szegedy et al. (2016) Srivastava et al.
(2015) have sought to strengthen the CNNs by making it deeper and deeper as they have shown that
increasing the depth of a network could signiﬁcantly increase the quality of the learned representations.
Many researchers are continuously investigating to further improve the performance of deep CNNs
by incorporating attention mechanisms to exploit its ability to cover the relationship between the
learned spatial features."
INTRODUCTION,0.021739130434782608,"Attention modules, in general, are designed to suppress noise while keeping useful information by
reﬁning the learned features using attention scaling. By quoting from the human perception process
Mnih et al. (2014) where the high-level information is used in guiding the bottom-up learning process
by capturing more sophisticated features while disregarding irrelevant details. Human perception and
visual attention Beck & Kastner (2009) Desimone (1998) Mnih et al. (2014) Desimone & Duncan
(1995) is enhanced by top-down stimuli and non-relevant neurons will be suppressed in feedback
loops. Referencing to human visual system, various different attention mechanisms Cao et al. (2019)
Wang et al. (2018) Zhao et al. (2020) Vaswani et al. (2017) Wang et al. (2017) have been explored
and integrated into deep CNNs. Attention mechanisms were introduced in the context of CNNs to
capture the relations between features, either across the spatial dimension as in Carion et al. (2020)
Hu et al. (2018a) ,or across channel-wise dimension as in Wang et al. (2018) Cao et al. (2019) Hu
et al. (2018b) Lee et al. (2019) Wang et al. (2020) or across both dimensions as in Park et al. (2018)
Woo et al. (2018) Fu et al. (2019) Wang et al. (2017) Linsley et al. (2018) Roy et al. (2018) Chen
et al. (2017). Although these attention methods have achieved higher accuracy than their counterpart"
INTRODUCTION,0.02717391304347826,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03260869565217391,"Table 1: Comparison of existing attention modules in terms of whether previous knowledge cross-
channel interaction (PKCCI), attention dimension, where C indicates channel attention and S indicates
spatial attention, and lightweight or not."
INTRODUCTION,0.03804347826086957,"Methods
SE
ECA
SRM
GSop
GC
GE
CBAM
BAM
DAN
GALA
RAN
PKCAM"
INTRODUCTION,0.043478260869565216,"PKCCI












Attention
C
C
C
C
C
S
C + S
C + S
C + S
C + S
C + S
C
Lightweight
N/A










"
INTRODUCTION,0.04891304347826087,"baselines which do not invoke any attention mechanisms in their architectures, they often bring higher
model complexity and exploit only the current feature map while reﬁning it, that’s why we call it local
attention mechanisms. Exploiting previous knowledge has been applied to image classiﬁcation Huang
et al. (2017) Iandola et al. (2014), image segmentation Ronneberger et al. (2015), tracking Ma et al.
(2015), and human pose estimation Newell et al. (2016) where they obtain enhanced performance.
DenseNets Huang et al. (2017) encourage feature reuse by connecting each layer to every other
layer in a feed-forward fashion. U-Net Ronneberger et al. (2015) consists of two paths, which are
contracting path to capture context and a symmetric expanding path that enables precise localization,
where feature reuse is introduced through using skip connection between two paths. Driven by the
signiﬁcance of employing feature reuse while learning different tasks Szegedy et al. (2015) Huang
et al. (2017) Iandola et al. (2014) Chen et al. (2016) Ma et al. (2015) Newell et al. (2016), a question
arises: How can one incorporate previous knowledge aggregation while learning channel attention
more efﬁciently?"
INTRODUCTION,0.05434782608695652,"To answer this question, we introduce PKCAM, a novel feature recalibration module based on
channel attention, which improves the quality of the representations produced by a network using
the global information to selectively emphasize informative features and suppress less useful ones.
In contrast to the aforementioned attention mechanisms, our global context aware attention block
obtains additional inputs from all preceding attention blocks, that have the same depth, and passes
on its reﬁned feature-maps to all subsequent blocks, creating global awareness from exploiting
previous knowledge aggregation from earlier layers that can capture ﬁne-grained information which is
useful for precise localization while attending to features from earlier layers that can encode abstract
semantic information, which is robust to target appearance changes."
INTRODUCTION,0.059782608695652176,The contributions of this paper are summarized as follows:
INTRODUCTION,0.06521739130434782,"• We propose a simple and effective attention module, PKCAM, which can be integrated
easily with any CNNs and applied across all it’s blocks due to the lightweight computation
of our novel architecture."
INTRODUCTION,0.07065217391304347,"• We verify the effectiveness and robustness of PKCAM throughout extensive experiments
with various baseline architectures on multiple tasks and datasets."
INTRODUCTION,0.07608695652173914,"• Through detailed analysis along with ablation studies, we examine the internal behavior and
validity of our method."
INTRODUCTION,0.08152173913043478,"The rest of the paper is organized as follows, ﬁrst, we discuss the related work, followed by the details
of the proposed model. Then we present detailed ablation studies to settle on the best architectural
design, and ﬁnally, illustrate the experimental setup for the various experiments we conducted for
every contribution."
RELATED WORK,0.08695652173913043,"2
RELATED WORK"
RELATED WORK,0.09239130434782608,"Channel attention
Hu et al. (2018b) proposed SE block, squeeze, and excitation block, which
comprises a lightweight gating mechanism that focuses on enhancing the representational power of
the network by modeling channel-wise relationships using two fully connected layers. ECA-Net
Wang et al. (2020) empirically shows avoiding dimensionality reduction in Hu et al. (2018b) by
using a simple 1-D convolution layer, is important for learning channel attention and appropriate
cross-channel interaction. SRM Lee et al. (2019) proposes a Style-based Recalibration Module,
which adaptively recalibrates intermediate feature maps by exploiting their styles. Zhao et al. (2020)
explore two variations of self-attention, pairwise and patchwise, that produce more powerful reﬁned"
RELATED WORK,0.09782608695652174,Under review as a conference paper at ICLR 2022
RELATED WORK,0.10326086956521739,"Figure 1: Diagram of our Previous Knowledge Channel Attention Module (PKCAM). Given a
R aggregated features, PKCAM generates global aware channel weights by performing a fast 1D
convolution of size R, accompanied by another 1D convolution, which represents global cross channel
interaction, then fused with the standard local cross channel interaction."
RELATED WORK,0.10869565217391304,"features. The basic non-local block (NLB) Wang et al. (2018) aims at strengthening the features
of the query position via aggregating information from other positions. GC-NetCao et al. (2019)
introduces an abstract global context modeling framework, that could be summarized into two blocks:
context modeling and transform block besides proposing a simpliﬁed local network as the context
modeling and use SENet Hu et al. (2018b) as the transform block. GSoP Gao et al. (2019) obtain a
covariance matrix by exploiting holistic image information using global second-order pooling, which
is used for tensor scaling along channel dimension."
RELATED WORK,0.11413043478260869,"Spatial attention
GENet Hu et al. (2018a) consists of two operators that follow also the context
modeling framework Cao et al. (2019), gather and excite operators. GENet Hu et al. (2018a) uses
stridden depth-wise convolution which acts as the gather operator, which applies spatial ﬁlters to
independent channels of the input, and a simple excite operator consists of sigmoid function and
multiplication. Spatial Transformer Networks Jaderberg et al. (2015) tackle the lack of CNN ability
to be spatially invariant to the input, by integrating a learnable module, the Spatial Transformer,
which can be inserted into CNNs, giving neural networks the ability to actively spatially transform
feature maps, conditional on the feature map itself. DETR Carion et al. (2020) stacks a spatial
transformer after the CNN backbone to learn the interaction between each spatial position and its
effect on different vision tasks, object detection, and instance segmentation."
RELATED WORK,0.11956521739130435,"Spatial and channel attention
BAM Park et al. (2018), CBAM Woo et al. (2018), DANet Fu et al.
(2019), Residual attention network Wang et al. (2017), SCA-CNN Chen et al. (2017), scSE Roy
et al. (2018) and GALA Linsley et al. (2018) show that taking the spatial axis into consideration
besides channel axis boost the attention module accuracy. Given an intermediate feature map,
they sequentially infer attention maps along two separate dimensions, channel and spatial, then the
attention maps are multiplied to the input feature map for adaptive feature reﬁnement."
RELATED WORK,0.125,"Table 1 summarizes the existing attention modules in terms of whether previous knowledge cross-
channel interaction, attention type where C means channel attention mechanism is used, S means
spatial attention mechanism is used and C + S indicates that both attention mechanisms are used, and
lightweight model."
METHODOLOGY,0.13043478260869565,"3
METHODOLOGY"
METHODOLOGY,0.1358695652173913,"In this section, we ﬁrst demonstrate an abstracted overview of our PKCAM. Then, we demonstrate
the motivations to adopt the feature reuse concept via exploiting the previous knowledge to create a
global aware attention block (i.e., PKCAM). Finally, dissecting our PKCAM by detailing it main
blocks."
ABSTRACTED OVERVIEW OF OUR PKCAM,0.14130434782608695,"3.1
ABSTRACTED OVERVIEW OF OUR PKCAM"
ABSTRACTED OVERVIEW OF OUR PKCAM,0.14673913043478262,"By scrutinizing the aforementioned channel attention techniques, as presented in Table 1, previous
knowledge aggregation was not explored from the channel attention module perspective. Therefore
we studied the previous knowledge cross-channel interaction by proposing PKCAM. The left part in
Fig.?? demonstrates an abstracted overview of our PKCAM, which exploits both local and global"
ABSTRACTED OVERVIEW OF OUR PKCAM,0.15217391304347827,Under review as a conference paper at ICLR 2022
ABSTRACTED OVERVIEW OF OUR PKCAM,0.15760869565217392,"feature maps while recalibrating the current feature map. PKCAM consists of two stacked modules:
previous knowledge aggregation (PKA) and global cross channel interaction (GCCI). The PKA
module covers the channel interactions across different preceding aggregated feature maps, while
the GCCI module utilizes the reﬁned features produce by the PKA module to model channel-wise
relationships in a computationally efﬁcient manner. GCCI modules could be any one of the on-the-
shelf channel attention techniques which are studied in Section 2. Algorithm 1 demonstrates a pseudo
algorithm for our PKCAM module to show how easily it could be implemented and integrated to any
CNN architecture."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.16304347826086957,"3.2
PREVIOUS KNOWLEDGE AGGREGATION BLOCK"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.16847826086956522,"Algorithm 1 PKCAM module algorithm
Input: X: Current feature map.
P: List of preceding feature maps.
Output: Z1: Learned channel scales"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.17391304347826086,"1: R ←Length(Y )
2: for r in range(R) do
3:
P[r] ←SDA(P[r])
4:
P[r] ←CDA(P[r])
5: end for
6: S ←Stack(X, Y )
7: Y ←1DConv(S, Kernel = R)
8: Z1 ←1DConv(Y, Kernel = 3)
9: return Z1"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.1793478260869565,"In contradiction to the aforementioned channel atten-
tion techniques which relies on the current output
of an arbitrary CNN block, our proposed PKCAM
exploits both the current CNN block output , x0 ∈
RH0×W0×C0, and a range of earlier CNN blocks out-
put , Xp = [x1, x2, ..., xR], where R is the coverage
region that delimits how many previous CNN blocks
output will be consolidated along side the current
CNN block, x1 ∈RH1×W1×C1, x2 ∈RH2×W2×C2,
and xR ∈RHR×WR×CR."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.18478260869565216,"Channel dimension alignment
In general, the ear-
lier features Xp have different channel dimensions,
as the conventional is as we go deeper the depth is
increased. Therefore, the ﬁrst operation in our Previ-
ous Knowledge Aggregation(PKA) block is aligning
the channel dimension among different CNN blocks.
As C0 ≥C1 ≥C2 ≥CR, aligning operation can be
done be learnable upsampling techniques or a simple repeating operation to align with the channel
dimension of the current CNN block C0, producing channel aligned feature maps, x′
0 ∈RH0×W0×C0,
x′
1 ∈RH1×W1×C0, x′
2 ∈RH2×W2×C0, and x′
R ∈RHR×WR×C0."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.19021739130434784,"Spatial dimensions alignment
Analogous to aligning the channel dimensions, the spatial dimen-
sions; H and W, is aligned through squeeze operation by adapting the general global average pooling
equation as follows, e
X =
1
RW H
PR
k=1
PW
i=1
PH
j=1 Xkij, where e
X ∈RR×1×1×C0 and represents
the squeezed feature maps from the channel aligned aggregated feature maps x′
i, where i = 0, 1, ... ,
R, producing e
X = [ex0, ex1, ..., exR], ex0 ∈R1×1×C0, ex1 ∈R1×1×C0 and exR ∈R1×1×C0."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.1956521739130435,"Previous knowledge cross-channel attention
Given the aggregated feature e
X, previous knowl-
edge cross-channel attention can be learned by Y = f( e
X), where Y ∈R1×1×C0 , f( e
X) = W ′ e
X,
and W ′ could take one of the following forms, W
′ ="
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.20108695652173914,"











"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.20652173913043478,"










"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.21195652173913043,"W
′
1 =  "
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.21739130434782608,"W
′
1,1
· · ·
W1,RC0
...
...
...
W
′
RC0,1
· · ·
W
′
RC0,RC0  "
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.22282608695652173,"W
′
2 =  "
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.22826086956521738,"W
′
1,1
0
· · ·
0
0
W
′
2,2
· · ·
0
...
...
...
...
0
0
· · ·
W
′
RC0,RC0   (1)"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.23369565217391305,"where W ′
1 is a RC0 × RC0 parameter matrix which learns previous knowledge interaction in
conjunction with cross-channel interaction. In contrast W ′
2 is a 1 × RC0 parameter matrix which
learns previous knowledge interaction and channel interaction neglecting the cross channel relations.
The key difference between W ′
1 and W ′
2 is that W ′
1 considers previous knowledge cross-channel"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.2391304347826087,Under review as a conference paper at ICLR 2022
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.24456521739130435,"Table 2: Comparison of different previous knowledge Aggregation(PKA) techniques on the Tiny-
ImageNet dataset. Where 1-D Conv., Sum and FC stands for one dimensional convolution layer Eq.3,
summation Eq.2, and fully connected layer Eq.1 respectively."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.25,"PKA
1-D
Conv.
Sum
FC
1-D
Conv.
Sum
FC
1-D
Conv.
Sum
FC"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.2554347826086957,"ResNet-18
ResNet-34
ResNet-50"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.2608695652173913,"Acc.
55.70
55.28
54.63
56.94
56.26
56.52
57.89
56.18
56.41"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.266304347826087,"#.P (M)
10.749 10.749 11.413
20.389 20.389 23.049
22.824 22.824 65.387"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.2717391304347826,"GFLOPs
2.075
2.075
2.076
4.329
4.329
4.331
4.878
4.878
4.878"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.27717391304347827,"interaction while W ′
2 does not, leading W ′
1 to be more complex than W ′
2. Interpreting Eq. 1 to
neural networks W ′
1 and W ′
2 can be regarded as a fully connected layer and depth-wise separable
convolution layer respectively. However, obviously from Eq. 1, W ′
1 and W ′
2 have a tremendous
number of parameters, driving to high model complexity, especially for large channel numbers as
mainly C0 >> R."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.2826086956521739,"Therefore, we divide learning the previous knowledge cross-channel interaction into two sub-modules
as shown in Fig. 1, learning previous knowledge interaction, and exploiting the cross-channel
interaction. Consequently, in contrast to Eq. 1, f( e
X) is splitted into two cascaded functions, f1( e
X)
and f2( e
X), where f1( e
X) is responsible to learn the previous knowledge channel interaction and
f2( e
X) is responsible to learn the cross-channel interaction."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.28804347826086957,"Previous knowledge channel interaction
Previous knowledge channel attention can be learned
by Eq. 2, where for each channel the global information is aggregated using simple summation
operation, where no learnable parameters are invoked."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.29347826086956524,"Y = f1( e
X) = C0
X L=1 R
X"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.29891304347826086,"K=1
e
XLK
(2)"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.30434782608695654,"A possible compromise between Eq. 1 and Eq. 2 is Eq. 3, where a tiny number of parameters are
used whereas W
′ ∈R1×1×R compared to the tremendous number of parameters that are invoked
in Eq. 1 while learning the previous knowledge channel interaction. From the perspective of the
convolution neural network, Eq. 3 could be readily interpreted to a 1-D convolution layer with kernel
k = f
W."
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.30978260869565216,"Y = f1( e
X) = C0
X"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.31521739130434784,"L=1
W
′ e
XL
(3)"
PREVIOUS KNOWLEDGE AGGREGATION BLOCK,0.32065217391304346,"Global cross-channel interaction
Global cross-channel interaction could be learned by adopting
one of the local channel attention modules Hu et al. (2018b) Cao et al. (2019) Wang et al. (2018)
Lee et al. (2019) Wang et al. (2020) Gao et al. (2019) producing Z1 = f(Y ) where Z1 ∈R1×1×C0.
Wang et al. (2020) Gao et al. (2019) Hu et al. (2018b) Cao et al. (2019) Wang et al. (2018) Lee et al.
(2019) refer to the term global as they are taking into consideration the whole spatial dimension from
the fed features using GAP - Global Average Pooling. In contrast we refer to the term global as
previous knowledge aggregation."
EXPERIMENTS,0.32608695652173914,"4
EXPERIMENTS"
EXPERIMENTS,0.33152173913043476,"In this section, we perform controlled ablation experiments to settle on the best design for our
proposed module and assess its sub-modules. Then we evaluate the performance of the proposed
Previous Knowledge Attention module, on a series of benchmark datasets across different tasks
including Tiny-ImageNet Le & Yang (2015), i.e., discussed in the appendix, and ImageNet Deng
et al. (2009) for the classiﬁcation task, and KITTI Geiger et al. (2012) for detection. Finally, We
conduct empirical experiments that probe the robustness of the representations learned by PKCAM,
compared to convolutional baselines and other attention mechanisms."
EXPERIMENTS,0.33695652173913043,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3423913043478261,Table 3: Comparison of the various basic attention modules in our proposed module PKCAM.
EXPERIMENTS,0.34782608695652173,"Basic Attention Module
SE
SRM
ECA
SE
SRM
ECA"
EXPERIMENTS,0.3532608695652174,"Tiny-ImageNet
ImageNet"
EXPERIMENTS,0.358695652173913,"Resnet-18
54.07 55.10 55.70
70.65 70.75 70.83
Resnet-34
56.61 56.52 56.94
73.98 74.48 74.39
Resnet-50
57.42 57.12 57.89
76.86 77.42 77.56"
IMPLEMENTATION DETAILS,0.3641304347826087,"4.1
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.3695652173913043,"For classiﬁcation task two datasets are used, i.e., Tiny-ImageNet dataset Le & Yang (2015) and
ImageNet dataset Deng et al. (2009), to evaluate our proposed module and show its effectiveness,
where the same data augmentation and hyper-parameter settings in Hu et al. (2018b) are adopted.
For the Tiny-ImageNet dataset Le & Yang (2015), input images are randomly cropped to 64 × 64
with random horizontal ﬂipping. For the ImageNet dataset Deng et al. (2009), a 224 × 224 crop is
randomly sampled from an image or its horizontal ﬂip, with the per-pixel RGB mean value subtracted.
All models are trained for 100 epochs from scratch, using the weight initialization strategy described
in He et al. (2015) and the initial learning rate is set to 0.1 and decreased by a factor of 10 every 30
epochs. Stochastic gradient descent (SGD) with weight decay of 1e−4, the momentum of 0.9, and
mini-batch size of 32 is used for Tiny-ImageNet Le & Yang (2015), and 256 for ImageNet Deng et al.
(2009). Our module is implemented in Python using the PyTorch framework using four PCs with
Intel Xeon(R) 4108 1.8GHz CPU, 64G RAM, Nvidia Titan-XP."
ABLATION STUDIES FOR INTERNAL DESIGN,0.375,"4.2
ABLATION STUDIES FOR INTERNAL DESIGN"
ABLATION STUDIES FOR INTERNAL DESIGN,0.3804347826086957,"We have conducted two to settle on the best internal design and analyze the effectiveness of each
component in our PKCAM. As shown in Fig. 1, our proposed module consists of two cascaded
blocks, i.e., previous knowledge aggregation block (PKA) and global cross channel interaction block
(GCCI). Therefore two ablations studies are conducted to settle on the best internal design and analyze
the effectiveness of each one of them. The ﬁrst ablation investigating the different approaches for
previous knowledge channel interaction that are discussed in Section 3.2. Then, we assess the choice
of basic attention modules that are used in the global cross channel interaction modules as shown in
Fig. 1."
PREVIOUS KNOWLEDGE AGGREGATION,0.3858695652173913,"4.2.1
PREVIOUS KNOWLEDGE AGGREGATION"
PREVIOUS KNOWLEDGE AGGREGATION,0.391304347826087,"We have investigated empirically the different global channel interaction techniques that were
described in detail in Section3.2. As shown in Table8 exploiting the previous knowledge channel
interaction through 1-D Conv. layer by following Eq.3 is the best compromise to cope with the
trade-off between performance and complexity, where it shares almost the same model complexity
(i.e., network parameters and FLOPs) with the original ResNet while at the same time it boosts
the accuracy. Based on the aforementioned results in Table 8, our novel approach follows the
compromised combination while exploiting the previous knowledge cross channel interaction by
following Eq.3 to capture the previous knowledge channel interaction."
GLOBAL CROSS CHANNEL INTERACTION,0.3967391304347826,"4.2.2
GLOBAL CROSS CHANNEL INTERACTION"
GLOBAL CROSS CHANNEL INTERACTION,0.40217391304347827,"We next assess the choice of basic attention modules that are used in the global cross channel
interaction module as shown in Fig.1. Three channel attention mechanisms are evaluated on the Tiny-
Imagenet dataset Le & Yang (2015) and ImageNet dataset, including SE-Net Hu et al. (2018b), SRM
Lee et al. (2019), and ECA-Net Wang et al. (2020). As shown in Table 3 ECA-Net achieves the best
accuracy across different ResNet backbones. However, building up our PKCAM using other channel
attention mechanism boost the accuracy as shown in Table 3 compared to their original results that
are mentioned in Table 9 and Table 5."
GLOBAL CROSS CHANNEL INTERACTION,0.4076086956521739,Under review as a conference paper at ICLR 2022
GLOBAL CROSS CHANNEL INTERACTION,0.41304347826086957,Table 4: Comparison of the various ways to integrate PKCAM into CNNs using the ImageNet dataset.
GLOBAL CROSS CHANNEL INTERACTION,0.41847826086956524,"Integration Methodology
All blocks
Last block
LCAM+PKCAM
PKCAM"
GLOBAL CROSS CHANNEL INTERACTION,0.42391304347826086,"Top-1 FPS Top-1 FPS
Top-1
FPS
Top-1 FPS"
GLOBAL CROSS CHANNEL INTERACTION,0.42934782608695654,"Resnet-18
70.78
112
70.78
138
70.78
138
70.83
160
Resnet-34
74.29
69
74.36
83
74.36
83
74.39
93
Resnet-50
77.19
42
77.22
69
77.22
69
77.56
74"
GLOBAL CROSS CHANNEL INTERACTION,0.43478260869565216,"Table 5: Comparisons with state-of-the-art attention modules on ImageNet in terms of the number of
parameters (#P.) in millions, GFLOPs, top-1, and top-5 accuracy. Top-1 relative improvement results
are reported between parentheses w.r.t SENet improvement over Vanilla Resnet."
GLOBAL CROSS CHANNEL INTERACTION,0.44021739130434784,"Methods
#.P.(M) GFLOPs FPS
Top-1
Top-5 #.P.(M) GFLOPs FPS
Top-1
Top-5 #.P.(M) GFLOPs FPS
Top-1
Top-5"
GLOBAL CROSS CHANNEL INTERACTION,0.44565217391304346,"ResNet-18
ResNet-34
ResNet-50"
GLOBAL CROSS CHANNEL INTERACTION,0.45108695652173914,"ResNet
11.14
1.699
247
70.42
89.45
20.78
3.427
139
73.31
91.40
24.37
3.86
110
75.2
92.52
+SENet
11.23
1.700
146
70.59
89.78
20.93
3.428
79
73.87
91.65
26.77
3.87
54
76.71
93.38
+CBAM
11.23
1.700
92 70.73(182%) 89.91
20.94
3.428
48 74.01(125%) 91.76
26.77
3.87
36 77.34(141%) 93.69
+ECANet
11.14
1.699
179 70.75(194%) 89.74
20.78
3.427
100 74.13(146%) 91.68
24.37
3.86
56 77.39(145%) 93.60
+PKCAM
11.14
1.699
160 70.83(241%) 89.96
20.78
3.427
93 74.39(192%) 91.81
24.37
3.86
74 77.56(156%) 93.70"
ABLATION STUDIES FOR INTEGRATING PKCAM,0.45652173913043476,"4.3
ABLATION STUDIES FOR INTEGRATING PKCAM"
ABLATION STUDIES FOR INTEGRATING PKCAM,0.46195652173913043,"4.3.1
DO WE NEED TO INJECT PKCAM AT EACH BLOCK?"
ABLATION STUDIES FOR INTEGRATING PKCAM,0.4673913043478261,"The majority of CNN backbones have the same convention in their structure that consists of cascaded
blocks that form a stage, then the basic structure of the stage is repeated with some modiﬁcations
to form the backbone architecture. Driven by this, we empirically demonstrate that, placing our
PKCAM in the last block of each stage only is sufﬁcient to boost the accuracy by learning more
powerful representations, and there is no need to inject our module after each block to re-calibrate
its channels. Fig. ?? demonstrates the two different approaches. Injecting PKCAM at the last block
will be sufﬁcient as it aggregate the whole previous knowledge within the same stage, exploiting this
feature enables us to overcome all local attention modules in terms of model speed and complexity.
Table 4 shows that injecting PKCAM at the last block only superior injecting it in the whole blocks
in terms of Top-1 accuracy and the inference time."
ABLATION STUDIES FOR INTEGRATING PKCAM,0.47282608695652173,"4.3.2
PKCAM VS. LCAM"
ABLATION STUDIES FOR INTEGRATING PKCAM,0.4782608695652174,"Local channel attention modules (LCAM) showed their ability in learning powerful representations,
however exploiting the previous knowledge reinforcements the learned scales to be more representa-
tive. Table 4 demonstrates that our module is powerful enough to totally replace the known LCAM,
that are discussed at Sec. 2, instead of leveraging both of them. Upper part in Fig. ?? shows the
fusion approach between LCAM and PKCAM, while the right part in Fig.1 shows using PKCAM
only while integrating it to an arbitrary CNN."
IMAGE CLASSIFICATION,0.483695652173913,"4.4
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.4891304347826087,"In this section, we evaluate the performance of proposed PKCAM network on classiﬁcation bench-
mark datasets including ImageNet Deng et al. (2009) and Tiny-ImageNet Le & Yang (2015), that is
mentioned in the appendix. All the classiﬁcation experiments follows the same training procedure
that is discussed in Sec. 4.1. The evaluation metrics incorporate both efﬁciencies (i.e., network
parameters (#P.) in millions, inference frame rate per second (FPS), and ﬂoating-point operations per
second (FLOPs) in Gigas, and effectiveness (i.e., Top-1 accuracy)."
IMAGE CLASSIFICATION,0.4945652173913043,"ImageNet LSVRC 2012 dataset Deng et al. (2009), which contains 103 classes with 1.2 million
training images, 50 × 103 validation images, and 100 × 103 test images. The evaluation is measured
on the non-blacklist images of the ImageNet LSVRC 2012 validation set."
IMAGE CLASSIFICATION,0.5,"We compare our PKCAM module with several state-of-the-art attention methods using ResNet family.
Efﬁciency and effectiveness are measured, and the results are reported in Table 5 from their original
papers besides reproducing ECA-Net results as we notice there are difﬁculties in reproducing and"
IMAGE CLASSIFICATION,0.5054347826086957,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.5108695652173914,Table 6: Zero-shot testing: Analyzing the robustness of trained networks on Tiny-ImageNet.
IMAGE CLASSIFICATION,0.5163043478260869,"Robustness
Vanilla
SE
ECA
PKCAM
Vanilla
SE
ECA
PKCAM
Vanilla
SE
ECA
PKCAM"
IMAGE CLASSIFICATION,0.5217391304347826,"ResNet-18
ResNet-34
ResNet-50"
IMAGE CLASSIFICATION,0.5271739130434783,"No-Rotation
53.33
53.71
53.76
55.70
55.90
56.08
55.66
56.94
56.11
57.78
56.59
57.89
90◦
20.56
20.60
21.80
21.83
21.68
21.75
22.70
22.71
23.04
25.42
25.53
25.68
180◦
25.20
25.85
26.59
27.18
26.60
26.42
27.57
27.57
28.88
32.40
31.74
32.44
270◦
20.71
20.98
21.49
22.26
21.47
21.74
22.48
23.59
22.85
25.69
25.85
26.04"
IMAGE CLASSIFICATION,0.532608695652174,"Table 7: Comparisons with state-of-the-art attention modules on KITTI-RGB in terms of mAP using
YOLOV3 on Resnet-18 and 34 backbones."
IMAGE CLASSIFICATION,0.5380434782608695,"Vanilla
SE
ECA
CBAM
BAM
SRM
PKCAM"
IMAGE CLASSIFICATION,0.5434782608695652,"R-18
57.87
59.32
58.55
57.90
59.61
59.20
59.66
R-50
64.19
65.08
64.34
64.18
65.10
64.82
65.21"
IMAGE CLASSIFICATION,0.5489130434782609,"verifying their results1. We adopt the same training setup as He et al. (2016) Hu et al. (2018b) for
fair comparison as discussed at Section 4.4. Results show that our proposed PKCAM achieves the
best accuracy besides be the lightest model compared to other attention modules. Top-1 relative
improvement results is reported between parentheses in green w.r.t SENet improvement over Vanilla
Resnet."
OBJECT DETECTION,0.5543478260869565,"4.5
OBJECT DETECTION"
OBJECT DETECTION,0.5597826086956522,"We evaluate our proposed PKCAM on object detection task using Faster R-CNN Ren et al. (2015)
on the MS COCO dataset Lin et al. (2014) and using YOLOV3 Redmon & Farhadi (2018) on the
KITTI-RGB dataset. KITTI-RGB Geiger et al. (2012) consists of 7,481 training images and 7,518 test
images, comprising a total of 80,256 labeled objects of eight different classes. Each image has 3 RGB
color channels and pixel dimensions 1242 × 375 which is resized to 224 × 224. We follow the same
training setup as mentioned at Section 4.4. As shown in Table 7, PKCAM considerably improves
the accuracy more than other attention modules compared to the baseline He et al. (2016). The
MMDetection Chen et al. (2019) framework is used to guarantee fair comparison between different
channel attention mechanisms"
ROBUSTNESS,0.5652173913043478,"4.6
ROBUSTNESS"
ROBUSTNESS,0.5706521739130435,"We have conducted experiments to probe the robustness of the representations learned by our proposed
module PKCAM, compared to other channel attention mechanisms on Tiny-ImageNet Le & Yang
(2015), where the testing images are rotated deliberately in one of three ways: clockwise 90◦,
clockwise 180◦, clockwise 270◦, where these rotations were not scrutinized at the training. As shown
in Table 6, PKCAM is less vulnerable than other attention modules."
DISCUSSION,0.5760869565217391,"5
DISCUSSION"
DISCUSSION,0.5815217391304348,"5.1
TOP-1 VALIDATION ACCURACY CURVES"
DISCUSSION,0.5869565217391305,"To provide some insight into inﬂuence of our PKCAM module on the optimisation of these mod-
els, example training curves for runs of the baseline architectures and their respective PKCAM
counterparts are depicted in Fig. 2. We observe that PKCAM blocks yield a steady improvement
throughout the optimisation procedure. Moreover, this trend is fairly consistent across a range of
network architectures considered as baselines."
DISSECTING THE PRODUCED LEARNABLE SCALES,0.592391304347826,"5.2
DISSECTING THE PRODUCED LEARNABLE SCALES"
DISSECTING THE PRODUCED LEARNABLE SCALES,0.5978260869565217,"To further analyze the effect of our PKCAM module on learning channel attention, we visualize
the scales learned by our novel PKCAM modules and compare it against ECA module. For this"
DISSECTING THE PRODUCED LEARNABLE SCALES,0.6032608695652174,"1Referring to issues number 21, 52, 62, 24, 46, and 58 from the ofﬁcial ECA-Net implementation."
DISSECTING THE PRODUCED LEARNABLE SCALES,0.6086956521739131,Under review as a conference paper at ICLR 2022
DISSECTING THE PRODUCED LEARNABLE SCALES,0.6141304347826086,"Table 8: Comparisons with state-of-the-art attention modules on the MS-COCO dataset using Faster
R-CNN detector based on Resnet-50 backbones."
DISSECTING THE PRODUCED LEARNABLE SCALES,0.6195652173913043,"Method
#.P.(M) GFLOPs FPS
AP
AP50 AP75 APS
APM
APL
ResNet-50
41.53
207.07
12
36.4 58.2
39.2
21.8
40.0
46.2
+SENet
44.02
207.18
7
37.7 60.1
40.9
22.9
41.9
48.2
+ECANet
41.53
207.18
8
38.0 60.6
40.9
23.4
42.1
48.0
+PKCAM
41.53
207.18
10
38.3 60.9
41.0
23.9
42.4
48.2"
DISSECTING THE PRODUCED LEARNABLE SCALES,0.625,"Figure 2: Training ResNet, and local channel attention modules (LCA) baseline architectures and
their PKCAM counterparts on ImageNet validation set. PKCAM exhibit improved optimisation
characteristics and produce consistent gains in performance which are sustained throughout the
training process."
DISSECTING THE PRODUCED LEARNABLE SCALES,0.6304347826086957,"experiment, we adopt ResNet-18 as backbone models, and illustrate scales of ﬁrst block only for
each stage as discussed in Sec. 4.2 in the main script. In contrast to ECA setup, where a random
sample consists of four classes only from ImageNet dataset are used, which are hammerhead shark,
ambulance, medicine chest and butternut squash, we have used a more generic and fair way to analyze
the learned scales by averaging them over the whole validation dataset instead of using only four
selected classes. Fig. 3 visualizes the channel learned scales for each ﬁrst block from each stage for
both modules; PKCAM in blue and ECA in orange."
DISSECTING THE PRODUCED LEARNABLE SCALES,0.6358695652173914,"As shown in Fig. 3,ECA scales have larger variance than our learned scales, where ECA module’s
authors claim that it indicates a better discriminative ability which necessarily reﬂect the quality of the
learned scales. Accordingly, a question is arising, is our main focus is to learn a discriminative scales,
or to learn a more representative ones? We argue the claim that says the more discriminative scales
indicates a more useful ones. As the most important a more representative scales not discriminative
ones. This could be shown as all the learned channels is important and we can’t exclude any one of
them unlike the learned ECA scales which indicates that we can totally prune some of the learned
scales, this shows deﬁciency in the learning process where not the full network capacity was exploited.
Our scales are shown empirically that they are more representative as they boost the accuracy in a
consistent manner over different architectures and different tasks."
CONCLUSION,0.6413043478260869,"6
CONCLUSION"
CONCLUSION,0.6467391304347826,"In this paper, we concentrate on determining an effective channel attention module with low model
complexity. To this end, we propose efﬁcient channel attention (PKCAM). Because of the lightweight"
CONCLUSION,0.6521739130434783,"Figure 3: Comparison for the learned channel scales by our novel PKCAM module against ECA
modules. Better view with zooming in."
CONCLUSION,0.657608695652174,Under review as a conference paper at ICLR 2022
CONCLUSION,0.6630434782608695,"computation of the PKCAM block, it can be integrated into all modern CNN architectures across
the whole layers and trained end-to-end. While most previous works utilized uni-scale features,
PKCAM is designed to employ the ability of global information while recalibrating feature maps.
Our experiments demonstrate that simply inserting PKCAM into standard CNN architectures boosts
the performance across different tasks. Furthermore, we verify the robustness of the representations
learned by PKCAM and its generalization ability via zero-shot experiments to rotated images."
AUTHOR CONTRIBUTIONS,0.6684782608695652,AUTHOR CONTRIBUTIONS
AUTHOR CONTRIBUTIONS,0.6739130434782609,"If you’d like to, you may include a section for author contributions as is done in many journals. This
is optional and at the discretion of the authors."
AUTHOR CONTRIBUTIONS,0.6793478260869565,ACKNOWLEDGMENTS
AUTHOR CONTRIBUTIONS,0.6847826086956522,"Use unnumbered third level headings for the acknowledgments. All acknowledgments, including
those to funding agencies, go at the end of the paper."
REFERENCES,0.6902173913043478,REFERENCES
REFERENCES,0.6956521739130435,"Diane M Beck and Sabine Kastner. Top-down and bottom-up mechanisms in biasing competition in
the human brain. Vision research, 49(10):1154–1165, 2009."
REFERENCES,0.7010869565217391,"Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-
excitation networks and beyond. In Proceedings of the IEEE/CVF International Conference on
Computer Vision Workshops, pp. 0–0, 2019."
REFERENCES,0.7065217391304348,"Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer
Vision, pp. 213–229. Springer, 2020."
REFERENCES,0.7119565217391305,"Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen
Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie
Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,
Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark.
arXiv preprint arXiv:1906.07155, 2019."
REFERENCES,0.717391304347826,"Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L Yuille. Attention to scale: Scale-aware
semantic image segmentation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 3640–3649, 2016."
REFERENCES,0.7228260869565217,"Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn:
Spatial and channel-wise attention in convolutional networks for image captioning. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 5659–5667, 2017."
REFERENCES,0.7282608695652174,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.7336956521739131,"Robert Desimone. Visual attention mediated by biased competition in extrastriate visual cortex.
Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 353
(1373):1245–1255, 1998."
REFERENCES,0.7391304347826086,"Robert Desimone and John Duncan. Neural mechanisms of selective visual attention. Annual review
of neuroscience, 18(1):193–222, 1995."
REFERENCES,0.7445652173913043,"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention
network for scene segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 3146–3154, 2019."
REFERENCES,0.75,"Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global second-order pooling convolutional net-
works. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 3024–3033, 2019."
REFERENCES,0.7554347826086957,Under review as a conference paper at ICLR 2022
REFERENCES,0.7608695652173914,"Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti
vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR),
2012."
REFERENCES,0.7663043478260869,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision, pp. 1026–1034, 2015."
REFERENCES,0.7717391304347826,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.7771739130434783,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature
context in convolutional neural networks. arXiv preprint arXiv:1810.12348, 2018a."
REFERENCES,0.782608695652174,"Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132–7141, 2018b."
REFERENCES,0.7880434782608695,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700–4708, 2017."
REFERENCES,0.7934782608695652,"Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, and Kurt Keutzer.
Densenet: Implementing efﬁcient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869,
2014."
REFERENCES,0.7989130434782609,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In
Advances in neural information processing systems, pp. 2017–2025, 2015."
REFERENCES,0.8043478260869565,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.8097826086956522,"Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7, 2015."
REFERENCES,0.8152173913043478,"HyunJae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm: A style-based recalibration module for
convolutional neural networks. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 1854–1862, 2019."
REFERENCES,0.8206521739130435,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740–755. Springer, 2014."
REFERENCES,0.8260869565217391,"Drew Linsley, Dan Shiebler, Sven Eberhardt, and Thomas Serre. Learning what and where to attend.
arXiv preprint arXiv:1805.08819, 2018."
REFERENCES,0.8315217391304348,"Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive
ﬁeld in deep convolutional neural networks. In Proceedings of the 30th International Conference
on Neural Information Processing Systems, pp. 4905–4913, 2016."
REFERENCES,0.8369565217391305,"Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical convolutional features
for visual tracking. In Proceedings of the IEEE international conference on computer vision, pp.
3074–3082, 2015."
REFERENCES,0.842391304347826,"Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual
attention. arXiv preprint arXiv:1406.6247, 2014."
REFERENCES,0.8478260869565217,"Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation.
In European conference on computer vision, pp. 483–499. Springer, 2016."
REFERENCES,0.8532608695652174,"Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Bam: Bottleneck attention
module. arXiv preprint arXiv:1807.06514, 2018."
REFERENCES,0.8586956521739131,"Joseph Redmon and Ali Farhadi.
Yolov3:
An incremental improvement.
arXiv preprint
arXiv:1804.02767, 2018."
REFERENCES,0.8641304347826086,Under review as a conference paper at ICLR 2022
REFERENCES,0.8695652173913043,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015."
REFERENCES,0.875,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234–241. Springer, 2015."
REFERENCES,0.8804347826086957,"Abhijit Guha Roy, Nassir Navab, and Christian Wachinger. Recalibrating fully convolutional networks
with spatial and channel “squeeze and excitation” blocks. IEEE transactions on medical imaging,
38(2):540–549, 2018."
REFERENCES,0.8858695652173914,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.8913043478260869,"Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Training very deep networks. arXiv
preprint arXiv:1507.06228, 2015."
REFERENCES,0.8967391304347826,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015."
REFERENCES,0.9021739130434783,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.907608695652174,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 31, 2017."
REFERENCES,0.9130434782608695,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.9184782608695652,"Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang,
and Xiaoou Tang. Residual attention network for image classiﬁcation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 3156–3164, 2017."
REFERENCES,0.9239130434782609,"Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. Eca-net:
Efﬁcient channel attention for deep convolutional neural networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.9293478260869565,"Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794–7803,
2018."
REFERENCES,0.9347826086956522,"Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block
attention module. In Proceedings of the European conference on computer vision (ECCV), pp.
3–19, 2018."
REFERENCES,0.9402173913043478,"Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
10076–10085, 2020."
REFERENCES,0.9456521739130435,"A
APPENDIX"
REFERENCES,0.9510869565217391,In this section we will demonstrate more experiments on Tiny-ImageNet dataset.
REFERENCES,0.9565217391304348,"Tiny-ImageNet Le & Yang (2015) that is also called Micro-ImageNet, is a classiﬁcation challenge
that is similar to the full ImageNet ILSVRC challenge. Micro-ImageNet comprises 200 classes. Each
class has 500 images for training. The test set includes 10,000 images. A 64 × 64 crop is randomly
sampled from an image or its horizontal ﬂip, with the per-pixel RGB mean value subtracted."
REFERENCES,0.9619565217391305,Under review as a conference paper at ICLR 2022
REFERENCES,0.967391304347826,"Table 9: Comparisons with state-of-the-art attention modules on Tiny-ImageNet in terms of the
number of parameters (#P.) in millions, GFLOPs, FPS, and top-1 accuracy."
REFERENCES,0.9728260869565217,"Methods
#.P (M)
GFLOPs
FPS
Top-1
#.P (M)
GFLOPs
FPS
Top-1
#.P (M)
GFLOPs
FPS
Top-1"
REFERENCES,0.9782608695652174,"ResNet-18
ResNet-34
ResNet-50"
REFERENCES,0.9836956521739131,"Vanilla
10.749
2.074
352
53.33
20.389
4.327
195
55.90
22.802
4.861
150
56.11
SE
10.832
2.075
202
53.71
20.539
4.329
131
56.50
25.201
4.870
95
57.78
ECA
10.749
2.075
242
53.76
20.389
4.329
143
55.66
22.803
4.868
96
56.59
SRM
10.753
2.074
168
53.39
20.396
4.327
107
55.34
22.831
4.861
92
57.02
CBAM
10.835
2.075
132
55.06
20.544
4.329
68
55.27
25.218
4.867
49
56.79
BAM
10.771
2.080
201
55.00
20.411
4.333
145
55.97
23.144
4.962
119
58.37
PKCAM
10.749
2.075
215
55.70
20.389
4.329
136
56.94
22.803
4.868
101
57.89"
REFERENCES,0.9891304347826086,"We compare our PKCAM module with several state-of-the-art attention methods using three variants
of ResNet backbone He et al. (2016) which are ResNet-18, ResNet-34, and ResNet-50 on the Tiny-
Imagenet dataset Le & Yang (2015), including SENetHu et al. (2018b), ECANetWang et al. (2020),
SRM Lee et al. (2019), CBAMWoo et al. (2018), and BAMPark et al. (2018). For a fair comparison,
we ran all experiments by following the same setup that was mentioned above. As shown in Table 9,
our PKCAM module shares almost the same model complexity with the original ResNets variants He
et al. (2016), ResNet-18, ResNet-34, and ResNet-50, while achieving 2%, 0.7%, and 1.78% gains
in Top-1 accuracy, respectively. Comparing with state-of-the-art counterparts (i.e., SENetHu et al.
(2018b), ECANetWang et al. (2020), SRM Lee et al. (2019), CBAMWoo et al. (2018), and BAMPark
et al. (2018)), PKCAM obtains better or competitive results while availing lower model complexity.
Inference time is measured on a PC equipped with TITAN Xp GPU and an Intel(R) Xeon Silver 4112
CPU@2.60GHz."
REFERENCES,0.9945652173913043,"Please note that, Our code will be published once the paper is accepted, also it is attached in the
supplementary material compressed ﬁle."
