Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.003134796238244514,"Causal discovery with time series data remains a challenging yet increasingly
important task across many scientific domains. Convergent cross mapping (CCM)
and related methods have been proposed to study time series that are generated
by dynamical systems, where traditional approaches like Granger causality are
unreliable. However, CCM often yields inaccurate results depending upon the
quality of the data. We propose the Tangent Space Causal Inference (TSCI) method
for detecting causalities in dynamical systems. TSCI works by considering vector
fields as explicit representations of the systems’ dynamics and checks for the degree
of synchronization between the learned vector fields. The TSCI approach is model-
agnostic and can be used as a drop-in replacement for CCM and its generalizations.
We first present a basic version of the TSCI algorithm, which is shown to be more
effective than the basic CCM algorithm with very little additional computation.
We additionally present augmented versions of TSCI that leverage the expressive
power of latent variable models and deep learning. We validate our theory on
standard systems, and we demonstrate improved causal inference performance
across a number of benchmark tasks."
INTRODUCTION,0.006269592476489028,"1
Introduction"
INTRODUCTION,0.009404388714733543,"The discovery of causal relationships is one of the most fundamental goals of scientific work.
When causal relationships are known and understood, we can explain the behavior of a system and
understand how our actions or interventions upon the system will affect its behavior [27]. This kind
of reasoning is fundamental in many problem domains, such as medicine, environmental policy, and
economics. However, it is not always possible to perform interventions and observe their effects. For
example, medical practitioners might only have one chance to prescribe a medication to help a patient.
As another example, an ecologist might find that performing a large experiment is forbiddingly
expensive or otherwise infeasible. Due to these concerns, there remains a great interest in the problem
of observational causal inference, where one infers causation without manipulating the system under
study directly."
INTRODUCTION,0.012539184952978056,"For time series data, the most prominent tool for observational causal inference is Granger causality
[15]. Granger causality operates on the assumption that if one system (the cause) is driving changes
in another system (the effect), then the cause should have unique information about what will happen
to the effect. This is encoded in an assumption called separability, which posits that the information
needed to predict the future behavior of the effect is not contained in the effect itself. However, the
separability condition runs counter to the behavior of coupled dynamical systems: if long histories of
the effect can forecast the cause, then an autoregressive model with the appropriate lag may forecast
the effect from itself. This effect is explored in the supplementary materials of [12, 41], and we give"
INTRODUCTION,0.01567398119122257,∗Equal contribution.
INTRODUCTION,0.018808777429467086,"an example in Appendix B. Since systems with (approximately) dynamical behavior are ubiquitous
in many application domains, alternative methods have become of interest."
INTRODUCTION,0.0219435736677116,"To address the failures of Granger causality in coupled dynamical systems Sugihara et al. [41]
proposed the convergent cross mapping (CCM) method, which directly takes advantage of the
topological properties of dynamical systems. CCM can be seen as an adaption of earlier work that
studied the synchronization of dynamical systems [2, 29, 35] into an algorithmic procedure for
detecting causalities between time series. Using Takens’ theorem, a well-known result in dynamical
systems theory [42], CCM attempts to detect causality by reconstructing the state space of a given
time series, and then learning a time-invariant function that maps between reconstructed state spaces,
called a cross map. The key assumption is that the cross map only exists if the original systems
were dynamically coupled. In this sense, the “causality” of CCM deviates from the popular Pearlian
framework [27], and is better interpreted as identifying which variables “drive” or “force” a dynamical
system based on observational data.2 Building upon this framework, several extensions of CCM have
been proposed to address various technicalities and caveats of the approach and to generalize the
method to new domains. However, the CCM test statistic is difficult to interpret and does not admit a
simple decision rule. CCM also does not explicitly learn a cross map function."
INTRODUCTION,0.025078369905956112,"To improve upon CCM, we propose Tangent Space Causal Inference (TSCI). TSCI detects causation
between dynamical systems by explicitly checking if the dynamics of one system map on to the
other. The proposed method can be seen as a drop-in replacement for the CCM test, providing an
interpretable and principled alternative while remaining compatible with many of the extensions
of CCM. Furthermore, the proposed method is model agnostic, meaning that it can be adapted to
any method used to learn the cross map function, including multilayer perception (MLP) networks,
splines, or Gaussian process regression. The only major assumption of TSCI is that the time series
under study were generated by continuous time dynamical systems (i.e, by systems of differential
equations), which is a standard assumption in a number of physical systems [36]. As a result, TSCI is
applicable to many of the same problems as CCM."
RELATED WORK,0.02821316614420063,"1.1
Related Work"
RELATED WORK,0.03134796238244514,"Causal representation learning. The primary function of Takens’ theorem in the CCM method is to
yield a representation of a system’s latent state so that it may be used for cross mapping. As a result,
numerous generalizations of Takens’ theorem emerged in the following decades [32, 33, 40]. Causal
representation learning aims to learn hidden variables from high dimensional observations [1], and a
principle task is to decipher the causal relationships between many, possibly redundant, observations
of a system [30]. In such cases, methods of dimensionality reduction can be applied to extract causal
variables from the raw observations [31]. This is particularly important in the processing of large,
spatio-temporal data sets [36, 43]. Some authors have proposed that CCM can be improved by
aggregating data from multiple sources into the reconstruction of the latent states [7, 45], which
requires an awareness of how the observation data relate to the causal variables of interest."
RELATED WORK,0.034482758620689655,"Causal discovery with cross maps. CCM has popularized the use of cross maps as a tool for causal
inference, and many variations and improvements of the basic CCM methodology have been proposed.
Some of these works aim to improve the reconstruction of latent states in these models by using a
Bayesian approach to latent state inference [13], where the approach is adapted for spatial/geographic
data [14] or modified for sporadically sampled time series [12]. Additionally, several improvements
have suggested changing the way that a cross map is detected: some authors have recommended
varying the library length [25] or time delaying the cross mapping [44] to yield refined information.
The k-nearest neighbor regression, which is used in the original CCM algorithm, can be swapped
out for a radial basis function network [23], or for a Gaussian process regression model [13]. Other
approaches have not directly learned the cross map at all; instead they have examined other aspects of
the reconstructions such as their dimensionality [3] or used pairwise distance rankings as a signature
of the mapping [5]."
RELATED WORK,0.03761755485893417,"2Although not typically interpreted as such, cross maps have been suggested to be interpretable in a Pearlian
causal framework via the do operator [12]."
PROPOSED METHOD,0.04075235109717868,"2
Proposed Method"
PROPOSED METHOD,0.0438871473354232,"We begin by considering the inference of a causal relationship between two time series, x(t) and
y(t). Our starting assumption is that both time series were generated by latent dynamical systems
with states zx(t) and zy(t), respectively, whose behavior is governed by a set of ordinary differential
equations (ODEs). For motivation, we consider a particular case with a unidirectional coupling
between the latent states:
x(t) = hx(zx(t)),
(1)
y(t) = hy(zy(t)),
(2)
dzx"
PROPOSED METHOD,0.047021943573667714,"dt = fx(zx),
(3) dzy"
PROPOSED METHOD,0.050156739811912224,"dt = fy(zx, zy),
(4)"
PROPOSED METHOD,0.05329153605015674,"where the observation functions hx, hy and the time derivative functions fx, fy are assumed to be
differentiable. A causal relationship between x and y is evidenced by the appearance of zx in the
equation for dzy/dt. The state vectors zx and zy could possibly have redundant information, but by
writing the system in this form, we obtain an asymmetry between x(t) and y(t) which will inform
our inference of causation. In our notation, if zx appears in the equation for dzy/dt, we will write
x →y."
PROPOSED METHOD,0.05642633228840126,"We now explain how the CCM method approaches this problem and how the TSCI method builds
upon the CCM framework."
CONVERGENT CROSS MAPPING,0.05956112852664577,"2.1
Convergent Cross Mapping"
CONVERGENT CROSS MAPPING,0.06269592476489028,"CCM is a technique for inferring causation between time series generated by dynamical systems
[41], as seen in Eqs. (1) to (4). The basic motivation for CCM is that given an observed time series
x(t), one can construct a vector ˜x which acts as a proxy for the latent states zx that generated it.
Given two constructions, ˜x and ˜y, we may detect if there is a mapping between them, which provides
evidence of a causal relationship. From Eq. (4), since zx influences zy unidirectionally, the effect
y(t) contains more information than the cause time series x(t), and as a result ˜y generally contains
enough information to reconstruct ˜x. With this in mind, we can frame CCM as a two-step procedure
[13]. In Step 1, we construct representations ˜x and ˜y that are proxies for zx and zy, respectively. In
Step 2, we detect a mapping between reconstructions; if there is a mapping ˜y 7→˜x, then the reverse
causality holds, x →y."
CONVERGENT CROSS MAPPING,0.06583072100313479,"To reconstruct the latent state space, as in Step 1, multiple approaches could be employed. However,
in the basic CCM methodology, one uses the so-called delay embedding of x(t),"
CONVERGENT CROSS MAPPING,0.06896551724137931,˜x(t) =  
CONVERGENT CROSS MAPPING,0.07210031347962383,"x(t)
x(t −τ)
...
x(t −(Q −1)τ) "
CONVERGENT CROSS MAPPING,0.07523510971786834,",
(5)"
CONVERGENT CROSS MAPPING,0.07836990595611286,"where τ and Q are parameters called the embedding lag and embedding dimension, respectively. The
justification that ˜x is a good proxy for zx is given by Takens’ theorem [42], which states that ˜x and
zx are equivalent up to a nonlinear change of coordinates."
CONVERGENT CROSS MAPPING,0.08150470219435736,"Theorem 2.1 (Takens’ theorem [33]) Let M be a compact manifold of dimension d. Let z ∈M
evolve according to dz/dt = f(z), let ϕτ be the mapping that takes zt to zt+τ, and let x(t) = h(z(t)).
If Q ≥2d + 1, then for almost-every3 triplet (f, h, τ), the map Φf,h,τ"
CONVERGENT CROSS MAPPING,0.08463949843260188,"Φf,h,τ(z) =  "
CONVERGENT CROSS MAPPING,0.0877742946708464,"h(z)
h(ϕ−τ(z))
...
h(ϕ−(Q−1)τ(z)) "
CONVERGENT CROSS MAPPING,0.09090909090909091,"
(6)"
CONVERGENT CROSS MAPPING,0.09404388714733543,"3Several version of Takens’ theorem exist, and they make useful, but mathematically distinct, statements
about how common such embeddings are. The version presented here says that functions that do not produce
embeddings live on a measure zero set, in the sense of prevalence [33, p. 584]."
CONVERGENT CROSS MAPPING,0.09717868338557993,is an embedding of M into RQ.
CONVERGENT CROSS MAPPING,0.10031347962382445,"Since Φf,h,τ(zx(t)) = ˜x(t), Takens’ theorem tells us that if zx lives on a manifold M, then the
points ˜x lie on a manifold Mx, which is called the shadow manifold of x [41]. Despite the number
of assumptions in the statement of Takens’ theorem, many generalizations of the statement exist,
allowing us to justify the use of delay embedding to systems with strange attractors [33] and systems
with noise [40]. If a system satisfies the conditions for Takens’ theorem, in the sense that Mx is a
valid embedding of M, then we say that the system is generic."
CONVERGENT CROSS MAPPING,0.10344827586206896,"Since only zx influences x(t), Takens’ theorem implies that ˜x is equivalent (up to nonlinear coordinate
change) to zx. However, due to the appearance of zx in (4), both zx and zy are responsible
for generating y(t). As a result, Takens’ theorem suggests that ˜y is equivalent to (zx, zy) as a
concatenated vector [39]. Since (zx, zy) clearly can be mapped onto zx, the equivalence between the
delay embeddings and the latent states suggests that there is a mapping ˜y 7→˜x, called a cross map.
Thus, cross maps encode the idea that the effect time series contains information about its cause. This
is encoded in the following corollary to Takens’ theorem."
CONVERGENT CROSS MAPPING,0.10658307210031348,"Corollary 2.1.1 Suppose that x →y for a generic system. Then there exists a function F such that
˜x(t) = F(˜y(t)) for all t."
CONVERGENT CROSS MAPPING,0.109717868338558,The proof of the corollary is provided in the Appendix.
CONVERGENT CROSS MAPPING,0.11285266457680251,"Step 2 of CCM is then to detect if x →y by checking if a cross map ˜y 7→˜x exists. To this end,
Sugihara et. al. [41] propose to check the predictability of the time series x(t) given ˜y(t). They use a
form of k-nearest neighbors regression to produce an estimate ˆx(t) of x(t) given ˜y(t), and then they
define a test statistic
rCCM
X→Y = corr(ˆx(t), x(t)),
(7)"
CONVERGENT CROSS MAPPING,0.11598746081504702,"where corr is the Pearson correlation coefficient. To test the reverse causal direction, x ←y, one
simply performs Step 2 again with the roles of x and y reversed."
TANGENT SPACE CAUSAL INFERENCE,0.11912225705329153,"2.2
Tangent Space Causal Inference"
TANGENT SPACE CAUSAL INFERENCE,0.12225705329153605,"Takens’ theorem and cross maps as a tool for causal inference are rooted in a solid mathematical
foundation, but the CCM test does not exploit all of the properties of shadow manifolds. Namely, it
does not exploit the fact that the shadow manifolds are copies of the latent manifolds. In practice,
there are cases in which CCM learns a cross map that appears to be reasonably predictive, but
results in a false positive. Thus, a more robust algorithm would exploit more subtle properties of the
hypothesized cross map. We propose TSCI as alternative to Step 2 in the CCM algorithm."
TANGENT SPACE CAUSAL INFERENCE,0.12539184952978055,"TSCI operates by checking if the ODE on one manifold, Mx, can be mapped to an ODE on another
manifold My. To understand how this works, we need to reframe our discussion of ODEs in terms
of vector fields. Recall that given an ODE of the form, d˜x"
TANGENT SPACE CAUSAL INFERENCE,0.12852664576802508,"dt = u(˜x),
(8)"
TANGENT SPACE CAUSAL INFERENCE,0.13166144200626959,"we may interpret u to be a velocity vector field on the manifold. When evaluated, u(˜x) is a tangent
vector of the manifold Mx, existing in the tangent space T˜xMx4. From calculus, we know that
tangent vectors in T˜xMx can be mapped to tangent vectors in TF (˜x)My by the Jacobian matrix
JF (˜x) at the point ˜x. By checking if the tangent vectors can be mapped in such a way, TSCI provides
an alternative to the CCM causality test. A visual motivation for the TSCI methods is depicted in
Fig. 1."
TANGENT SPACE CAUSAL INFERENCE,0.13479623824451412,"Before we can map vector fields from one manifold to another, we need to check that meaningful
vector fields exist on the shadow manifolds. As usual, this is also a corollary of Takens’ theorem."
TANGENT SPACE CAUSAL INFERENCE,0.13793103448275862,"Corollary 2.1.2 There exists a vector field u on Mx such that the embedding Φf,h,τ in Takens’
theorem is a time-invariant mapping. If we define γ(0) = Φf,h,τ(zx(0)), and if we let γ(t) to be the
flow of a point γ(0) under the vector field u, then Φf,h,τ(zx(t)) = γ(t), for all times t."
TANGENT SPACE CAUSAL INFERENCE,0.14106583072100312,4See Appendix A for definitions of differential geometric quantities such as the tangent space.
TANGENT SPACE CAUSAL INFERENCE,0.14420062695924765,"Figure 1: Visual motivation for the TSCI method. Observed time series are related to latent states
evolving on some manifolds. Given observed time series x(t) and y(t), we reconstruct latent states
˜x(t) and ˜y(t) that reside in manifolds Mx and My, respectively. If x and y are causally coupled,
there should exist a function F that maps between these manifolds. Using the Jacobian matrix JF ,
we can map the velocity vector field on Mx to a vector field on My, and use the angle between
velocity vectors as a measure of their similarity."
TANGENT SPACE CAUSAL INFERENCE,0.14733542319749215,The proof of the corollary is provided in the Appendix.
TANGENT SPACE CAUSAL INFERENCE,0.15047021943573669,"The corollary guarantees that we can learn ODEs that describe the dynamics of ˜x and ˜y, once we
have obtained valid reconstructions of the latent states. Let u and v be vector fields such that
d˜x"
TANGENT SPACE CAUSAL INFERENCE,0.1536050156739812,"dt = u(˜x),
(9) d˜y"
TANGENT SPACE CAUSAL INFERENCE,0.15673981191222572,"dt = v(˜y).
(10)"
TANGENT SPACE CAUSAL INFERENCE,0.15987460815047022,"Because the cross map F is a mapping between the two shadow manifolds, the Jacobian matrix of
the cross map relates the vectors u and v. We state this formally in the following Lemma."
TANGENT SPACE CAUSAL INFERENCE,0.16300940438871472,"Lemma 2.1.1 Let Mx and My be manifolds with respective vector fields u and v that define their
dynamics. If there exists a cross map F : Mx →My, then for every point ˜x ∈Mx, we have that
v(F(˜x)) = JF (˜x)u(˜x), where JF (˜x) is the Jacobian matrix of F at ˜x."
TANGENT SPACE CAUSAL INFERENCE,0.16614420062695925,The proof of the lemma is in the Appendix.
TANGENT SPACE CAUSAL INFERENCE,0.16927899686520376,"While the cross map F : Mx →My is a mapping between points on the manifolds, the Jacobian
matrix JF (˜x) induces a mapping between the tangent spaces at ˜x and ˜y. We show this visually in
Fig. 1. Since the velocity of ˜x can be represented by a tangent vector u(˜x), the Jacobian matrix JF
allows us to map these vectors to tangent vectors in My. If there is a cross map, then the vector field
v and the push forward vector field JF u should match exactly. On the other hand, there should be no
correlation in the absence of a causal relationship, assuming quality reconstructions and plentiful
data. The degree of alignment between v and JF u can be used as a test statistic for the presence of a
causal link. We therefore propose the TSCI test statistic,"
TANGENT SPACE CAUSAL INFERENCE,0.1724137931034483,"rTSCI
X→Y = corr(u, JF v).
(11)"
TANGENT SPACE CAUSAL INFERENCE,0.1755485893416928,"Because the tangent vectors are centered in their respective tangent planes, we can geometrically
interpret the TSCI test statistic as the expected cosine similarity between the vector field u and the
push forward vector field JF v,"
TANGENT SPACE CAUSAL INFERENCE,0.1786833855799373,"rTSCI
X→Y = E˜y∼My"
TANGENT SPACE CAUSAL INFERENCE,0.18181818181818182,"
u(F(˜y))⊤JF (˜y)v(˜y)
||u(F(˜y))|| ||JF (˜y)v(˜y)||"
TANGENT SPACE CAUSAL INFERENCE,0.18495297805642633,"
.
(12)"
TANGENT SPACE CAUSAL INFERENCE,0.18808777429467086,"In practice, the estimation quality of the cross map also depends on the location in the reconstruction
space, and so the cosine similarity takes on a distribution of values (Fig. 2), and weak-to-moderate
correlation may be empirically observed as an artifact of limited data."
TANGENT SPACE CAUSAL INFERENCE,0.19122257053291536,Algorithm 1 TANGENT SPACE CAUSAL INFERENCE
TANGENT SPACE CAUSAL INFERENCE,0.19435736677115986,"Input: Embedding/vector field matrices X, U ∈RT ×Qx and Y, V ∈RT ×Qy
Output: Score rX→Y"
TANGENT SPACE CAUSAL INFERENCE,0.1974921630094044,"1: Learn a differentiable function F such that Xt = F(Yt) for all t
2: for t = 0, 2, . . . , T −1 do
3:
Compute the Jacobian matrix JF (Yt) ∈RQx×Qy of the function F at location Yt
4:
Push forward the tangent vector by computing ˆUt = VtJ⊤
F (Yt)
5: end for
6: Define rX→Y = corr( ˆU, U)"
TANGENT SPACE CAUSAL INFERENCE,0.2006269592476489,Algorithm 2 TANGENT SPACE CAUSAL INFERENCE WITH K-NEAREST NEIGHBORS
TANGENT SPACE CAUSAL INFERENCE,0.20376175548589343,"Input: Embedding/vector field matrices X, U ∈RT ×Qx and Y, V ∈RT ×Qy, regression parameter
K > max(Qx, Qy)
Output: Score rX→Y"
TANGENT SPACE CAUSAL INFERENCE,0.20689655172413793,"1: for t = 0, 2, . . . , T −1 do
2:
Find the indices τ1, . . . , τK of the K-nearest neighbors of Yt
3:
Compute the local displacements in x-space: ∆Xk = Xτk −Xt
4:
Compute the local displacements in y-space: ∆Yk = Yτk −Yt
5:
Compute the least-squares solution to ∆YkJ = ∆Xk
6:
Compute ˆUt = VtJ
7: end for
8: Define rX→Y = corr( ˆU, U)"
TANGENT SPACE CAUSAL INFERENCE,0.21003134796238246,"Based on these analyses, we propose the TSCI method in Algorithm 1, which learns a cross-map
F and returns the alignment of the vector fields u and JF v. CCM is sensitive to the quality of the
reconstruction of the latent states because an improperly constructed shadow manifold can sabotage
the method, both theoretically and empirically [6]; TSCI similarly requires that the shadow manifold
be properly embedded. Because of this, we assume that the embedding vectors ˜x and ˜y and the
estimates of their vector fields u(˜x) and v(˜y) at each point are supplied as inputs to the algorithm. A
variety of methods could be used to estimate the vector fields, but the simplest approach is to use
finite differences,"
TANGENT SPACE CAUSAL INFERENCE,0.21316614420062696,u(˜x) = d˜x dt =   dx(t)
TANGENT SPACE CAUSAL INFERENCE,0.21630094043887146,"dt
dx(t−τ)"
TANGENT SPACE CAUSAL INFERENCE,0.219435736677116,"dt...
dx(t−(Q−1)τ) dt "
TANGENT SPACE CAUSAL INFERENCE,0.2225705329153605,≈1 ∆t  
TANGENT SPACE CAUSAL INFERENCE,0.22570532915360503,"x(t + 1) −x(t)
x(t + 1 −τ) −x(t −τ)
...
x(t + 1 −(Q −1)τ) −x(t −(Q −1)τ)  ,"
TANGENT SPACE CAUSAL INFERENCE,0.22884012539184953,"where ∆t is the sampling rate of the data. Finite differences are known to be sensitive to noise, so in
real scenarios a more careful approach to obtaining the time derivatives is necessary. For a practical
implementation, we propose using the central finite-differences, which are second-order accurate
[16], or the derivatives interpolated by a Savitsky-Golay filter for noisy data [34]."
TANGENT SPACE CAUSAL INFERENCE,0.23197492163009403,"On methods to learn the cross map function. The TSCI algorithm is notably agnostic to the
regression approach used for the cross map F, with reasonable approaches including multilayer
perceptron networks (MLPs), splines, and Gaussian process regression (GPR). Depending on the
approach, derivatives can then be estimated from the model, either using automatic differentiation or
analytical derivatives. For GPR in particular, analytical derivatives are straightforward to compute
[38]."
TANGENT SPACE CAUSAL INFERENCE,0.23510971786833856,"Since regression can sometimes be a computationally complex procedure, in Algorithm 2 we also
provide a version of TSCI that is based on the k-nearest regression, in analogy to CCM. For clean
and dense data, this simple approach can yield accurate results, but it is generally unsuitable for
noisy or sparse data. In the latter case, it is preferrable to combine the TSCI approach with other
methods of denoising time series, or learning of latent dynamical models of the observed time series.
In particular, the learning of a latent ODE learning [12] can be combined with the TSCI test to yield
accurate causal inference."
TANGENT SPACE CAUSAL INFERENCE,0.23824451410658307,"Figure 2: Shadow manifolds Mx and My from the unidirectionally coupled Rössler-Lorenz system
Eq. (13) with C = 1, and the corresponding histograms of ˆu⊤u and ˆv⊤v. The test statistics, rX→Y
and rX←Y , correspond to the means of these distributions."
TANGENT SPACE CAUSAL INFERENCE,0.2413793103448276,"On the use of correlation coefficient. In relation to CCM, it can be noted that both TSCI and
CCM use a correlation coefficient as their test statistic. However, a critical difference between the
two methods is how the usage of this statistic is justified. The correlation coefficient used in CCM
analysis, called the cross-map skill [41], is used to measure the accuracy of predictions of the cross
map. However, since the points being predicted in CCM live on a manifold, measuring correlation in
the ambient (extrinsic) space is not well-motivated. Furthermore, estimates of this correlation can be
biased by the distribution of observations along the manifold, and as a result, a high correlation can
be achieved with a relatively low-accuracy prediction by guessing the general region in which the
points lie."
TANGENT SPACE CAUSAL INFERENCE,0.2445141065830721,"In contrast, the correlation between tangent vectors (or vector fields) is a more geometrically motivated
quantity: the correlation is a linear measure of similarity and the tangent vectors belong to a (centered)
linear space. Additionally, the shadow manifolds are constructed to be submanifolds of Euclidean
space, so correlations computed in extrinsic coordinates will match correlations computed using the
intrinsic coordinates of the shadow manifold. By Lemma 2.1.1, the correlations will be identically 1
if a cross map exists."
TANGENT SPACE CAUSAL INFERENCE,0.2476489028213166,"One alternative to the cosine similarity is the mutual information (MI). Specialized to the current
text, the MI I(u; JF v) quantifies the reduction in the uncertainty of u given the pushforward vector
JF v [9]. While the information-theoretic underpinning of the MI is attractive, we have two main
reasons to prefer the cosine similarity: First, the MI between two (continuous) distributions can be
difficult to interpret. For example, it is not obvious if I(u; JF v) = 0.5 is a strong dependence or
not, particularly in the case of continuous distributions. The cosine similarity, on the other hand, is
upper bounded by 1, so corr( ˆU, U) = 0.95 is easily interpreted as having near-perfect reconstruction.
Second, accurate estimation of the MI from samples is a notoriously difficult task, particularly in
high dimensions, and no single estimator works consistently well [10]. We provide some experiments
with the MI, and show the differing performance of different estimators, in Appendix B.3."
EXPERIMENTS,0.2507836990595611,"3
Experiments"
EXPERIMENTS,0.25391849529780564,"We validate the performance of TSCI on two datasets that are popular in the literature. The first
arises from a coupled Rössler-Lorenz system, where the ground truth causality is known and the
causal influence can be smoothly modulated. The second example was proposed in [12] and uses
sporadic time series from coupled double pendulums, and illustrates the applicability of TSCI to
extensions of CCM. Code implementing TSCI is available at https://github.com/KurtButler/
tangentspaces. All comparisons to CCM use the skccm module in Python5. Experiments were
run on a 6-Core Intel Core i5 and NVIDIA Titan RTX."
EXPERIMENTS,0.25705329153605017,"5https://github.com/nickc1/skccm, MIT License"
EXPERIMENTS,0.2601880877742947,"0
1
2
3
0.0 0.5"
RX,0.26332288401253917,"1.0
rX
Y (TSCI)
rY
X (TSCI)"
RX,0.2664576802507837,"0
1
2
3
0.0 0.5"
RX,0.26959247648902823,"1.0
rX
Y (CCM)
rY
X (CCM)"
RX,0.2727272727272727,"0.0
0.2
0.4
0.6
0.8
1.0
C 0.0 0.2 0.4 0.6 0.8 1.0"
RX,0.27586206896551724,Correlation Coefficient (a)
RX,0.27899686520376177,"0
2000
4000
6000
0.0 0.5 1.0"
RX,0.28213166144200624,"rX
Y (TSCI)"
RX,0.2852664576802508,"rY
X (TSCI)"
RX,0.2884012539184953,"0
2000
4000
6000
0.0 0.5 1.0"
RX,0.29153605015673983,"rX
Y (CCM)"
RX,0.2946708463949843,"rY
X (CCM)"
RX,0.29780564263322884,"0.0
0.2
0.4
0.6
0.8
1.0
Library Length 0.0 0.2 0.4 0.6 0.8 1.0"
RX,0.30094043887147337,Correlation Coefficient (b)
RX,0.30407523510971785,"Figure 3: Comparison of TSCI with CCM for the Rössler-Lorenz system (true causation X →Y ).
The plotted lines show the median test statistic over 100 trials for both CCM and TSCI, and the
shaded region indicates the 5th and 95th percentiles when (a) C is varied from 0 (no coupling) to 3
(approximate general synchrony), and (b) C is fixed to 1.0 and the library length is varied."
RX,0.3072100313479624,"3.1
Unidirectionally-Coupled Rössler-Lorenz System"
RX,0.3103448275862069,"A common toy system used for studying coupled dynamic systems is a unidirectionally-coupled
Rössler-Lorenz system [28], which we define in Eq. (13). In this system, the first three coordinates
(z1, z2, z3) describe a Rössler system, and they are unidirectionally coupled with a Lorenz-type
system (z4, z5, z6). The strength of the coupling is controlled by the parameter C. When C = 0, the
two systems are disconnected, but for C > 0 there is a causal influence from z1, z2, z3 to z4, z5, z6. dz"
RX,0.31347962382445144,dt = d dt 
RX,0.3166144200626959,
RX,0.31974921630094044,"z1
z2
z3
z4
z5
z6 "
RX,0.322884012539185,"
= "
RX,0.32601880877742945,
RX,0.329153605015674,"−6(z2,t + z3,t)
6(z1,t) + 0.2z2,t
6 (0.2 + z3,t(z1,t −5.7))
10(z5,t −z4,t)
28z4,t −z5,t −z4,tz6,t + Cz2
2,t
z4,tz5,t −8z6,t/3 "
RX,0.3322884012539185,"
(13)"
RX,0.335423197492163,"In Fig. 2, we visualize the TSCI method for x = z2 and y = z4. First, we show the shadow manifolds
Mx and My with a set of tangent vectors on each manifold. The delay embedding dimension
parameters, Qx and Qy, were selected using the false-nearest neighbors algorithm with a tolerance of
0.005 [19].Time lags for the embeddings, τx and τy, were selected by picking the minimal delay such
that the autocorrelation function drops below a threshold [21]. Additionally, we show the histograms
of cos(θ), where θ is the angle between the tangent vectors at each point. For X →Y direction,
which is the true causal direction, the distribution is concentrated near 1. For the Y ←X direction,
the distribution of tangent vectors is centered on 0. The test statistics, rX→Y and rY →X, correspond
to the means of each distribution, and visibly correspond to the correct causality."
RX,0.3385579937304075,"We show the effects of varying the coupling strength C in Fig. 3a, where TSCI clearly shows better
separation across varying C. We see the effect of increasing the library length (i.e., the size of the
training set in nearest neighbors) for C = 1.0 in Fig. 3b. Here, rX→Y increases at similar rates for
TSCI and CCM, suggesting similar data efficiency of the two methods."
DOUBLE PENDULUM SYSTEM,0.34169278996865204,"3.2
Double Pendulum System"
DOUBLE PENDULUM SYSTEM,0.3448275862068966,"To illustrate the generality of TSCI within CCM-like frameworks, we applied the TSCI methodology
to the latent CCM framework, where CCM is applied to a state-space reconstruction obtained via
neural ODEs [12]. One reason to favor this approach is when the observed time series are irregularly
(i.e., with a non-uniform sampling rate) or sporadically (i.e., any given observation only measures a
subset of states) sampled. Notable for TSCI is the fact that ground-truth derivatives are available, as
they are directly learned in the neural ODE reconstruction."
DOUBLE PENDULUM SYSTEM,0.34796238244514105,"Table 1: Double Pendulum Experiments. Entries denote the mean ± one standard deviation across 5
folds. Bolded directions indicate ground truth causality."
DOUBLE PENDULUM SYSTEM,0.3510971786833856,"rX1→X2
Direction
Latent CCM
Latent CCM (MLP)
Latent TSCI (MLP)"
DOUBLE PENDULUM SYSTEM,0.3542319749216301,"X →Y
0.011 ± 0.009
0.015 ± 0.009
0.021 ± 0.010
X →Z
0.044 ± 0.008
0.055 ± 0.011
0.077 ± 0.013
Y →X
−0.003 ± 0.008
−0.005 ± 0.009
−0.010 ± 0.004
Y →Z
0.003 ± 0.006
−0.002 ± 0.004
−0.008 ± 0.013
Z →X
0.737 ± 0.019
0.747 ± 0.015
0.915 ± 0.006
Z →Y
0.475 ± 0.043
0.578 ± 0.030
0.612 ± 0.053"
DOUBLE PENDULUM SYSTEM,0.3573667711598746,"We replicate an experiment in the latent CCM paper where a network of unidirectionally coupled
double pendulums are simulated, and observations sampled irregularly and sporadically. The authors’
source code6 was used to generate data and apply latent CCM. For more information on the data
generation, see Appendix A of [12]."
DOUBLE PENDULUM SYSTEM,0.3605015673981191,"For TSCI, we learn a cross-map using an MLP between the reconstructed state spaces. To avoid
tuning learning rates for every network, the parameter-free COCOB optimizer [26]7 and networks
were trained for 50 epochs. Results can be found in Table 1."
DOUBLE PENDULUM SYSTEM,0.36363636363636365,"When compared to latent CCM, TSCI provided larger correlation coefficients for the true positive
cases, with similarly small coefficients for the true negative cases. The use of MLPs for learning the
cross map partially but not fully explains the difference in performance."
ADDITIONAL EXPERIMENTS,0.3667711598746082,"3.3
Additional Experiments"
ADDITIONAL EXPERIMENTS,0.36990595611285265,"Several additional experiments appear in Appendix B. We briefly describe them here and provide
some commentary on their results."
ADDITIONAL EXPERIMENTS,0.3730407523510972,"Varying the embedding dimension.
In Appendix B.1, we artificially lower quality embeddings
by varying the embedding dimension. We find that the performance of TSCI and CCM similarly
degrade when using an embedding dimension that is very small, and that there is little harm to a
larger embedding dimension. In either case, the hyperparameters from a false-nearest neighbors test
perform well."
ADDITIONAL EXPERIMENTS,0.3761755485893417,"Corrupting signals.
In Appendix B.2, we lower the embedding quality in two different ways: (1)
by injecting additive noise to all observations, and (2) by adding a sine wave to observations. We
observe that TSCI and CCM similarly degrade when the signal-to-noise ratio is altered. However,
because of the larger separation in TSCI, the correct causal relation can be determined at much lower
signal-to-noise ratios."
ADDITIONAL EXPERIMENTS,0.3793103448275862,"Using mutual information.
In Appendix B.3, we experiment with using mutual information instead
of cosine similarity. We find that conclusions made using MI are generally similar to those from
cosine similarity, but with less interpretability and significant difficulties in estimation."
ADDITIONAL EXPERIMENTS,0.3824451410658307,"Comparisons to other causal discovery methods.
As mentioned in the introduction, CCM (and
hence TSCI) are specifically designed to address failures of Granger causality. In Appendix B.4,
we empirically verify the limitations of Granger causality on our Rössler-Lorenz toy system. We
additionally test several other causal discovery methods and show their limitations in our setting."
ADVANTAGES AND LIMITATIONS OF TSCI,0.38557993730407525,"4
Advantages and Limitations of TSCI"
ADVANTAGES AND LIMITATIONS OF TSCI,0.3887147335423197,"Scalability. The TSCI approach, using the K-nearest neighbors algorithm, retains the scalability and
lightweight implementation that CCM enjoys. The only additional computational complexity arises"
ADVANTAGES AND LIMITATIONS OF TSCI,0.39184952978056425,"6https://github.com/edebrouwer/latentCCM, MIT License
7https://github.com/bremen79/parameterfree, MIT License"
ADVANTAGES AND LIMITATIONS OF TSCI,0.3949843260188088,"from solving a local linear system of equations, and from the estimation of derivatives. Since the
linear systems have a fixed size K, and since derivative estimation can be done using a linear filter,
the additional cost is minimal. The general version of TSCI will scale depending on the choice of the
regressor used to learn the cross map, and on other design decisions made to improve the efficacy
of the method. However, this is not unlike the situation in which CCM is augmented with other
customization options that potentially slow down the method."
ADVANTAGES AND LIMITATIONS OF TSCI,0.3981191222570533,"Model agnosticism. Because a TSCI test can be formulated for any differentiable regression model
used to learn the cross map, the approach is highly flexible and model agnostic. Additionally, because
there are many potential ways in which reconstruction of latent states and learning of the velocity
vector fields can be improved, the TSCI method can be incorporated into a wide variety of inference
frameworks."
ADVANTAGES AND LIMITATIONS OF TSCI,0.4012539184952978,"Quality of reconstructed states. In both CCM and TSCI, there are a few assumptions which warrant
some justification before use, since their violation may yield to misapplication of cross map methods.
The first issue is that while Takens’ theorem implies that embeddings are plentiful, not all embeddings
are equal in quality or useful. Shadow manifolds that are sparsely or incompletely sampled, time
series with trends, or otherwise data which do not accurately capture the latent manifold can lead to
dubious results using CCM [6]. While TSCI is less likely to produce a false positive in these cases,
the assumption that a latent manifold is well-represented by a given embedding is nontrivial and
critical to ensuring trustworthy performance of the method."
ADVANTAGES AND LIMITATIONS OF TSCI,0.4043887147335423,"General synchrony. General synchrony is a problem that plagues all cross map-based methods [41].
The issue is that when the causal strength of the relationship x →y is very strong, the influence of
zx dominates the dynamics of zy, and zy cannot exhibit its own independent behavior. As a result,
My will look similar to Mx, and the cross map will become an invertible function. As a result, a
strong unidirectional relationship is detected as a bidirectional causal relationship. In Fig. 3, the
Rössler-Lorenz system enters general synchrony near C = 3 [6]. The TSCI method appears more
resistant to the effects of general synchrony than CCM, but it is a topological fact that as C grows,
synchrony becomes inevitable."
CONCLUSION,0.40752351097178685,"5
Conclusion"
CONCLUSION,0.4106583072100313,"In this paper, we presented the TSCI method for detecting causation in dynamical systems. By
considering how tangent vectors map from one manifold to another, we may achieve more robust
detection of causal relationships in dynamical systems than with standard CCM. Key advantages of
TSCI include that we may use it in many systems in which CCM would be applied, but the method
is far less prone to false positives and spurious causation. We presented both a general form of the
algorithm as well as a k-nearest neighbor version inspired by the original CCM algorithm. Because
TSCI requires us to estimate latent states and their time derivatives, there us much room for the TSCI
method to be further developed in future work."
CONCLUSION,0.41379310344827586,Acknowledgments and Disclosure of Funding
CONCLUSION,0.4169278996865204,"We would like to thank the anonymous reviewers for their suggestions which improved the content
and presentation of this work. This work was supported by the National Science Foundation under
Award 2212506."
REFERENCES,0.4200626959247649,References
REFERENCES,0.4231974921630094,"[1] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal repre-
sentation learning. In International Conference on Machine Learning, pages 372–407. PMLR,
2023."
REFERENCES,0.4263322884012539,"[2] Jochen Arnhold, Peter Grassberger, Klaus Lehnertz, and Christian Erich Elger. A robust
method for detecting interdependences: application to intracranially recorded EEG. Physica D:
Nonlinear Phenomena, 134(4):419–430, 1999."
REFERENCES,0.42946708463949845,"[3] Zsigmond Benk˝o, Ádám Zlatniczki, Marcell Stippinger, Dániel Fabó, András Sólyom, Loránd
Er˝oss, András Telcs, and Zoltán Somogyvári. Bayesian inference of causal relations between
dynamical systems. Chaos, Solitons & Fractals, 185:115142, 2024."
REFERENCES,0.43260188087774293,"[4] Patrick Blöbaum, Dominik Janzing, Takashi Washio, Shohei Shimizu, and Bernhard Schölkopf.
Cause-effect inference by comparing regression errors. In International Conference on Artificial
Intelligence and Statistics, pages 900–909. PMLR, 2018."
REFERENCES,0.43573667711598746,"[5] Leo Breston, Eric J Leonardis, Laleh K Quinn, Michael Tolston, Janet Wiles, and An-
drea A Chiba. Convergent cross sorting for estimating dynamic coupling. Scientific Reports,
11(1):20374, 2021."
REFERENCES,0.438871473354232,"[6] Kurt Butler, Guanchao Feng, and Petar M Djuri´c. On causal discovery with convergent cross
mapping. IEEE Transactions on Signal Processing, 2023."
REFERENCES,0.44200626959247646,"[7] Adam Thomas Clark, Hao Ye, Forest Isbell, Ethan R Deyle, Jane Cowles, G David Tilman, and
George Sugihara. Spatial convergent cross mapping to detect causal relationships from short
time series. Ecology, 96(5):1174–1181, 2015."
REFERENCES,0.445141065830721,"[8] Sarah Cobey and Edward B Baskerville. Limits to causal inference with state-space reconstruc-
tion for infectious disease. PloS one, 11(12):e0169050, 2016."
REFERENCES,0.4482758620689655,"[9] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience,
USA, 2006."
REFERENCES,0.45141065830721006,"[10] Paweł Czy˙z, Frederic Grabowski, Julia E Vogt, Niko Beerenwinkel, and Alexander Marx. Be-
yond normal: On the evaluation of mutual information estimators. In Thirty-seventh Conference
on Neural Information Processing Systems, 2023."
REFERENCES,0.45454545454545453,"[11] Povilas Daniušis, Dominik Janzing, Joris Mooij, Jakob Zscheischler, Bastian Steudel, Kun
Zhang, and Bernhard Schölkopf. Inferring deterministic causal relations. In Proceedings of the
Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, pages 143–150, 2010."
REFERENCES,0.45768025078369906,"[12] Edward De Brouwer, Adam Arany, Jaak Simm, and Yves Moreau. Latent convergent cross
mapping. In International Conference on Learning Representations, 2020."
REFERENCES,0.4608150470219436,"[13] Guanchao Feng, Kezi Yu, Yunlong Wang, Yilian Yuan, and Petar M Djuri´c. Improving conver-
gent cross mapping for causal discovery with Gaussian processes. In 2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3692–3696. IEEE,
2020."
REFERENCES,0.46394984326018807,"[14] Bingbo Gao, Jianyu Yang, Ziyue Chen, George Sugihara, Manchun Li, Alfred Stein, Mei-
Po Kwan, and Jinfeng Wang. Causal inference from cross-sectional earth system data with
geographical convergent cross mapping. Nature Communications, 14(1):5875, 2023."
REFERENCES,0.4670846394984326,"[15] Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral
methods. Econometrica: Journal of the Econometric Society, pages 424–438, 1969."
REFERENCES,0.4702194357366771,"[16] Michael T Heath. Scientific Computing: An Introductory Survey. SIAM, 2018."
REFERENCES,0.47335423197492166,"[17] Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear
causal discovery with additive noise models. Advances in Neural Information Processing
Systems, 21, 2008."
REFERENCES,0.47648902821316613,"[18] Diviyan Kalainathan, Olivier Goudet, and Ritik Dutta. Causal discovery toolbox: Uncovering
causal relationships in Python. Journal of Machine Learning Research, 21(37):1–5, 2020."
REFERENCES,0.47962382445141066,"[19] Matthew B Kennel, Reggie Brown, and Henry DI Abarbanel. Determining embedding di-
mension for phase-space reconstruction using a geometrical construction. Physical Review A,
45(6):3403, 1992."
REFERENCES,0.4827586206896552,"[20] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information.
Physical Review E, 69(6):066138, 2004."
REFERENCES,0.48589341692789967,"[21] Dimitris Kugiumtzis. State space reconstruction parameters in the analysis of chaotic time
series—the role of the time window length. Physica D: Nonlinear Phenomena, 95(1):13–28,
1996."
REFERENCES,0.4890282131661442,"[22] John M Lee. Introduction to Smooth Manifolds, volume 218 of Graduate Texts in Mathematics.
Springer, second edition, 2012."
REFERENCES,0.49216300940438873,"[23] Huanfei Ma, Kazuyuki Aihara, and Luonan Chen. Detecting causality from nonlinear dynamics
with short-term time series. Scientific Reports, 4(1):7464, 2014."
REFERENCES,0.4952978056426332,"[24] A. McInerney. First Steps in Differential Geometry: Riemannian, Contact, Symplectic. Under-
graduate Texts in Mathematics. Springer New York, 2013."
REFERENCES,0.49843260188087773,"[25] Dan Mønster, Riccardo Fusaroli, Kristian Tylén, Andreas Roepstorff, and Jacob F Sherson.
Causal inference from noisy time-series data—testing the convergent cross-mapping algorithm
in the presence of noise and external influence. Future Generation Computer Systems, 73:52–62,
2017."
REFERENCES,0.5015673981191222,"[26] Francesco Orabona and Tatiana Tommasi. Training deep networks without learning rates
through coin betting. Advances in Neural Information Processing Systems, 30, 2017."
REFERENCES,0.5047021943573667,"[27] Judea Pearl. Causality. Cambridge University Press, 2009."
REFERENCES,0.5078369905956113,"[28] R Quian Quiroga, Jochen Arnhold, and Peter Grassberger. Learning driver-response relation-
ships from synchronization patterns. Physical Review E, 61(5):5142, 2000."
REFERENCES,0.5109717868338558,"[29] Nikolai F Rulkov, Mikhail M Sushchik, Lev S Tsimring, and Henry DI Abarbanel. Generalized
synchronization of chaos in directionally coupled chaotic systems. Physical Review E, 51(2):980,
1995."
REFERENCES,0.5141065830721003,"[30] Jakob Runge, Andreas Gerhardus, Gherardo Varando, Veronika Eyring, and Gustau Camps-
Valls. Causal inference for time series. Nature Reviews Earth & Environment, 4(7):487–505,
2023."
REFERENCES,0.5172413793103449,"[31] Jakob Runge, Vladimir Petoukhov, Jonathan F Donges, Jaroslav Hlinka, Nikola Jajcay, Martin
Vejmelka, David Hartman, Norbert Marwan, Milan Paluš, and Jürgen Kurths. Identifying
causal gateways and mediators in complex spatio-temporal systems. Nature Communications,
6(1):8502, 2015."
REFERENCES,0.5203761755485894,"[32] Tim Sauer. Reconstruction of dynamical systems from interspike intervals. Physical Review
Letters, 72(24):3811, 1994."
REFERENCES,0.5235109717868338,"[33] Tim Sauer, James A Yorke, and Martin Casdagli. Embedology. Journal of Statistical Physics,
65:579–616, 1991."
REFERENCES,0.5266457680250783,"[34] Abraham Savitzky and Marcel JE Golay. Smoothing and differentiation of data by simplified
least squares procedures. Analytical Chemistry, 36(8):1627–1639, 1964."
REFERENCES,0.5297805642633229,"[35] Steven J Schiff, Paul So, Taeun Chang, Robert E Burke, and Tim Sauer. Detecting dynamical
interdependence and generalized synchrony through mutual prediction in a neural ensemble.
Physical Review E, 54(6):6708, 1996."
REFERENCES,0.5329153605015674,"[36] Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the
IEEE, 109(5):612–634, 2021."
REFERENCES,0.5360501567398119,"[37] Skipper Seabold and Josef Perktold. statsmodels: Econometric and statistical modeling with
Python. In 9th Python in Science Conference, 2010."
REFERENCES,0.5391849529780565,"[38] Ercan Solak, Roderick Murray-Smith, WE Leithead, Douglas Leith, and Carl Rasmussen.
Derivative observations in Gaussian process models of dynamic systems. Advances in Neural
Information Processing Systems, 15, 2002."
REFERENCES,0.542319749216301,"[39] Jaroslav Stark. Delay embeddings for forced systems. I. Deterministic forcing. Journal of
Nonlinear Science, 9:255–332, 1999."
REFERENCES,0.5454545454545454,"[40] Jaroslav Stark, David S Broomhead, Michael Evan Davies, and J Huke. Delay embeddings for
forced systems. II. Stochastic forcing. Journal of Nonlinear Science, 13:519–577, 2003."
REFERENCES,0.54858934169279,"[41] George Sugihara, Robert May, Hao Ye, Chih-hao Hsieh, Ethan Deyle, Michael Fogarty, and
Stephan Munch. Detecting causality in complex ecosystems. Science, 338(6106):496–500,
2012."
REFERENCES,0.5517241379310345,"[42] Floris Takens. Detecting strange attractors in turbulence. In David Rand and Lai-Sang Young,
editors, Dynamical Systems and Turbulence, Warwick 1980, pages 366–381, Berlin, Heidelberg,
1981. Springer."
REFERENCES,0.554858934169279,"[43] Gherardo Varando, Miguel-Ángel Fernández-Torres, Jordi Muñoz-Marí, and Gustau Camps-
Valls. Learning causal representations with Granger PCA. In UAI 2022 Workshop on Causal
Representation Learning, 2022."
REFERENCES,0.5579937304075235,"[44] Hao Ye, Ethan R Deyle, Luis J Gilarranz, and George Sugihara. Distinguishing time-delayed
causal interactions using convergent cross mapping. Scientific Reports, 5(1):14750, 2015."
REFERENCES,0.5611285266457681,"[45] Hao Ye and George Sugihara. Information leverage in interconnected ecosystems: Overcoming
the curse of dimensionality. Science, 353(6302):922–925, 2016."
REFERENCES,0.5642633228840125,"[46] Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei
Shimizu, Peter Spirtes, and Kun Zhang. Causal-learn: Causal discovery in Python. Journal of
Machine Learning Research, 25(60):1–8, 2024."
REFERENCES,0.567398119122257,"A
Important Facts About Manifolds"
REFERENCES,0.5705329153605015,"In this appendix, we review some important background about manifolds. Much more complete
references include the authoritative [22] (especially chapters 1, 2, 3, and 8), or the more accessible
[24]."
REFERENCES,0.5736677115987461,"We say that a function F is smooth if it has derivatives of all orders. A diffeomorphism is a function
F which is smooth, invertible, and F −1 is also smooth. We say that two subsets M and N of Rn are
diffeomorphic if there exists a diffeomorphism such that F(M) = N."
REFERENCES,0.5768025078369906,"A subset M of Rn is called a d-dimensional manifold if for every point x in M, there exists an open
set Ux such that"
REFERENCES,0.5799373040752351,1. There is a diffeomorphism Fx : Rn →Ux.
REFERENCES,0.5830721003134797,2. The image of the set Hd = {y ∈Rn : yd+1 = · · · = yn = 0} under Fx is given by
REFERENCES,0.5862068965517241,Fx(Hd) = Ux ∩M.
REFERENCES,0.5893416927899686,"Without a loss of generality, we can assume that Fx(0) = x in this definition. This allows us to easily
define tangent vectors. A vector v ∈Rn is called a tangent vector of M at the point x if there exists
a path γ(t) in Hd such that
d
dt (Fx(γ(t))|t=0 = v,"
REFERENCES,0.5924764890282131,"where Fx is the diffeomorphism in the above definition. The tangent space TxM is defined to be the
set of all tangent vectors of M at x. Note that from these definitions, it is clear that Rn is a manifold
and TxRd = Rd."
REFERENCES,0.5956112852664577,"In calculus, we define a vector field u on Rn to be a function u : Rn →Rn. We generalize this to
manifolds in Euclidean space by defining a vector field u on M to be a mapping u : M →Rn such
that u(x) ∈TxM at every point x."
REFERENCES,0.5987460815047022,"An embedding of a manifold M ⊂Rn is a mapping F : Rn →Rm such that F restricted to M is
a diffeomorphism onto its image. If M is a manifold of dimension d and F is an embedding, then
m is necessarily greater than or equal to d. In general, it is an interesting question to ask how much
larger m must be so that embeddings are easy to find. The Whitney embedding theorem says that it
is a generic property that F : Rn →Rm when m ≥2d + 1. Takens’ theorem refines the Whitney
theorem by asserting that such an embedding can be obtained from a scalar observation function, by
time lagging the observations sufficiently many times."
REFERENCES,0.6018808777429467,"B
Additional Experiments"
REFERENCES,0.6050156739811913,"In this appendix, we include experiments and results omitted from the main text."
REFERENCES,0.6081504702194357,"B.1
Varying Embedding Dimension"
REFERENCES,0.6112852664576802,"One of the key parameters in both CCM an TSCI is the embedding dimension parameter Q used
during reconstruction of the shadow manifold. Takens’ theorem suggests that once Q reaches a certain
value, the manifold is embedded and the improvement of the results should saturate in principle. Of
course, in real life there is a curse of dimensionality associated with inference, so it is interesting to
check how well the results behave as the embedding dimension is chosen to be higher than necessary.
We consider the effect of varying the parameter Q using data from the Rössler-Lorenz system in
Fig. 4. Performance of both methods improve as the dimension of the putative effect (if we are testing
x →y, then this is Qy) is increased, until saturating after the manifolds are fully embedded."
REFERENCES,0.6144200626959248,"B.2
Corrupted Signals"
REFERENCES,0.6175548589341693,"Generally, there are many non-trivial ways in which the state reconstruction can be poor, e.g., presence
of periodic trends [8] and underexploration of the shadow manifold [6]. We consider two scenarios
that deteriorate the state reconstruction quality on the Rössler-Lorenz system: additive noise, and a
periodic trend."
REFERENCES,0.6206896551724138,"Figure 4: Comparison of TSCI and CCM for the unidirectionally coupled Rössler-Lorenz system
Eq. (13) with C = 1. Here, x(t) = z2(t) and y(t) = z4(t), and the true causality is x →y. The red
lines indicate the values of Qx and Qy selected by false-nearest neighbors with a tolerance of 0.01."
REFERENCES,0.6238244514106583,"Results for additive noise are presented in Fig. 5. We fixed C = 1.0 and varied the signal-to-noise
ratio (SNR) from 0 dB to 60 dB by adding Gaussian noise to the observed signals. When estimating
derivatives, a Savitzky-Golay filter [34] was used with a window length of 5 and order 2."
REFERENCES,0.6269592476489029,"Overall, we find the performance relative to SNR to be similar between CCM and TSCI, suggesting
that inference of the vector field from noisy data is not a limiting factor in the presence of additive
noise."
REFERENCES,0.6300940438871473,"0
20
40
60
0.0 0.5 1.0"
REFERENCES,0.6332288401253918,"rX
Y (TSCI)"
REFERENCES,0.6363636363636364,"rY
X (TSCI)"
REFERENCES,0.6394984326018809,"0
20
40
60
0.0 0.5 1.0"
REFERENCES,0.6426332288401254,"rX
Y (CCM)"
REFERENCES,0.64576802507837,"rY
X (CCM)"
REFERENCES,0.6489028213166145,"0.0
0.2
0.4
0.6
0.8
1.0
Signal-to-Noise Ratio [dB] 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6520376175548589,Correlation Coefficient
REFERENCES,0.6551724137931034,"Figure 5: Comparison of TSCI with CCM for the Rössler-Lorenz system (true causation X →Y )
with additive noise. The plotted lines show the median test statistic over 100 trials for both CCM and
TSCI, and the shaded region indicates the 5th and 95th percentiles when (a) C is fixed to 1.0 and the
signal-to-noise ratio is varied."
REFERENCES,0.658307210031348,"An alternative way to corrupt signals is with a deterministic periodic trend; we used a sine wave
with a period of 2π, and varied its power with respect to the true signal. Results are presented in
Fig. 6. Our takeaways are relatively similar to the additive noise experiment: TSCI seems to degrade
similarly to CCM, but its increased separation between cause and effect makes this degradation in
performance less problematic. Notably, TSCI seems significantly more robust to false claims of
strong causation when the relative power of the confounder is large."
REFERENCES,0.6614420062695925,"20
10
0
10
20
0.0 0.5 1.0"
REFERENCES,0.664576802507837,"rX
Y (TSCI)"
REFERENCES,0.6677115987460815,"rY
X (TSCI)"
REFERENCES,0.670846394984326,"20
10
0
10
20
0.0 0.5 1.0"
REFERENCES,0.6739811912225705,"rX
Y (CCM)"
REFERENCES,0.677115987460815,"rY
X (CCM)"
REFERENCES,0.6802507836990596,"0.0
0.2
0.4
0.6
0.8
1.0
Relative Signal Power [dB] 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6833855799373041,Correlation Coefficient
REFERENCES,0.6865203761755486,"Figure 6: Results of the Rössler-Lorenz system with C = 1 corrupted by an additive sine signal of
varying amplitude. The amplitude of the signal, relative to the sine wave, is plotted in decibels on the
horizontal axis."
REFERENCES,0.6896551724137931,"B.3
Using the Mutual Information for Scores"
REFERENCES,0.6927899686520376,"One interesting alternative to the cosine similarity score is the mutual information (MI), which
provides a non-linear quantification of how related u and JF v are. One immediate concern with
using MI is that estimates from finite samples are difficult; we use the classical estimator of Kraskov,
Stögbauer, and Grassberger [20], which has shown relevance even in the era of deep learning [10]."
REFERENCES,0.6959247648902821,"Results on the Rössler-Lorenz test system while varying the coupling parameter C are presented in
Fig. 7. We find that MI typically shows less separation than TSCI, which brings into question its
viability when the ground truth is unknown. Another issue is in the interpretation of MI estimates:
it is not entirely clear if an MI estimate of, for example, 1 nat indicates strong influence, and MI
estimates are not easily normalized in the continuous setting."
REFERENCES,0.6990595611285266,"0
1
2
3 0.0 0.5"
RX,0.7021943573667712,"1.0
rX
Y (CS)
rY
X (CS)"
RX,0.7053291536050157,"0
1
2
3
0 2 4"
RX,0.7084639498432602,"rX
Y (MI)
rY
X (MI)"
RX,0.7115987460815048,"0.0
0.2
0.4
0.6
0.8
1.0
C 0.0 0.2 0.4 0.6 0.8 1.0"
RX,0.7147335423197492,TSCI Score
RX,0.7178683385579937,"Figure 7: Results of the Rössler-Lorenz system using the cosine similarity and mutual information
while varying C. Compare to Figure 2a in the main manuscript."
RX,0.7210031347962382,"B.4
Comparisons to Other Causal Discovery Methods"
RX,0.7241379310344828,"As argued in the main manuscript, cross-map-based methods such as TSCI and CCM approach causal
discovery in a rather particular setting, where more mainstream methods do not apply. We empirically
verify these claims on the Rössler-Lorenz test system, using a variety of values of C. We compare
against Granger causality, as well as various bivariate causal discovery methods."
RX,0.7272727272727273,"We first compare against Granger causality in Table 2. As expected from the theory, Granger causality
incorrectly infers bidirectional causality for all C > 0."
RX,0.7304075235109718,"We also compare against various methods from the bivariate or pairwise causal discovery literature.
These methods typically assume the existence of a map y = f(x) which is either deterministic or
subject to additive noise, which does not exist in all but the most trivial dynamical systems. This
includes RECI [4], IGCI [11], and ANM [17]. Results are presented in Tables 3 and 4."
RX,0.7335423197492164,"We find that IGCI consistently fails to detect a causal edge, with weak coefficients. RECI consistently
chooses the direction X →Y , but does so even when C = 0, which suggests that results are not
actually detecting causality, but rather some other anomaly of the data. To this end, the variance
ratios as described by Blobaum et al. [4] are typically large, indicating little confidence. Finally, for
all C > 0, ANM detects bidirectional causality."
RX,0.7366771159874608,"Comparisons with Granger causality use the implementation of statsmodels8 [37]. All compar-
isons to RECI and IGCI used the implementations of cdt9 [18], with a minor bug-fix for RECI.
Comparisons with ANM used the implementation of causal-learn10 [46]."
RX,0.7398119122257053,"Table 2: Granger Causal Results for Rössler-Lorenz System. Entries denote the median, 5th, and 95th
percentile p-values across 50 trials. All p-values are from the F-test implemented in statstools."
RX,0.7429467084639498,"C
p-value X →Y
p-value Y →X"
RX,0.7460815047021944,"0.0
0.0030.345
0.000
0.1680.896
0.000
0.75
0.0000.000
0.000
0.0000.000
0.000
1.5
0.0000.000
0.000
0.0000.000
0.000
2.25
0.0000.000
0.000
0.0000.000
0.000
3.0
0.0000.000
0.000
0.0000.000
0.000"
RX,0.7492163009404389,"Table 3: Bivariate causal discovery results for Rössler-Lorenz System. For all models, a negative score
indicates X →Y and a positive score indicates Y →X. All scores are based on the implementations
of cdt. Entries denote the median, min, and max over 10 trials."
RX,0.7523510971786834,"C
RECI
IGCI"
RX,0.7554858934169278,"0.0
−0.021−0.017
−0.027
0.0350.074
0.018
0.75
−0.025−0.019
−0.048
0.0340.048
−0.017
1.5
−0.028−0.025
−0.039
−0.0160.031
−0.078
2.25
−0.029−0.027
−0.036
0.0400.073
0.004
3.0
−0.036−0.017
−0.041
0.0300.042
0.011"
RX,0.7586206896551724,"C
Proofs"
RX,0.7617554858934169,"In this section, we provide proofs for results mentioned in the main text."
RX,0.7648902821316614,"Proof of Theorem 2.1
There are many different proofs and versions of Takens’ theorem. This
version is proved by Sauer, Yorke and Casdagli [33], and is beyond the scope of this appendix. Other
noteworthy variations of this theorem are proved by Stark [39] and Stark et al. [40].
□"
RX,0.768025078369906,"8https://github.com/statsmodels/statsmodels/, BSD-3-Clause License
9https://github.com/FenTechSolutions/CausalDiscoveryToolbox, MIT License
10https://github.com/py-why/causal-learn, MIT License"
RX,0.7711598746081505,"Table 4: ANM causal discovery results for Rössler-Lorenz System. Entries denote the median, min,
and max p-value over 10 trials. All scores are based on the implementation in causal-learn."
RX,0.774294670846395,"C
X →Y
Y →X"
RX,0.7774294670846394,"0.0
0.5070.777
0.096
0.4500.915
0.003
0.75
0.0000.121
0.000
0.0000.049
0.000
1.5
0.0000.000
0.000
0.0000.000
0.000
2.25
0.0000.000
0.000
0.0000.000
0.000
3.0
0.0000.000
0.000
0.0000.000
0.000"
RX,0.780564263322884,"Proof of Corollary 2.1.1
Let us assume that we have a generic system, where (zx, zy) ∈M for
some manifold M, and x is a function zx and y is a function of zy."
RX,0.7836990595611285,"If x →y and x ←y both hold, then Mx and My are both diffeomorphic to M, and thus,
diffeomorphic to each other. The existence of a cross map is then immediate."
RX,0.786833855799373,"If x →y but not vice-versa, then the dynamics of zx are autonomous, meaning that the behavior of
zx depends only upon itself. As a result, the projection map πx(zx, zy) = zx can be applied directly
to M to yield a new manifold Mx on which zx is an autonomous dynamical system. Since x observes
this system zx, Mx will be diffeomorphic to Mx, since zx only contains information about zx. Let
Φx : Mx →Mx and Φy : M →My denote the diffeomorphisms implied by Takens’ theorem. We
can construct the cross map then as"
RX,0.7899686520376176,"F(˜y) = Φx(πx(Φ−1
y (˜y))). □"
RX,0.7931034482758621,"Proof of Corollary 2.1.2
Since the shadow manifold M produced by Takens’ theorem is a smooth
embedding of the original manifold M, there exists a diffeomorphism Φ : M →M. By Proposition
8.19 of Lee [22, p. 183], for any given vector field u defined on M, there exists a unique vector field
v on M that is the push forward of u under the map Φ. Because the dynamics on the manifold are
derived from these vector fields, the two systems are equivalent as dynamical systems.
□"
RX,0.7962382445141066,"Proof of Lemma 2.1.1
Suppose that F : Mx →My is a cross map. Fix any point ˜x0 ∈Mx and
let ˜y0 = F(˜x0). If we consider flows forwards and backwards with respect to these vector fields,
then there exists some small ε > 0 such that we have curves ˜xt along Mx and ˜yt along My such
that ˜yt = F(˜xt) for each t in the interval (−ε, ε). By the chain rule of calculus, we have that d˜y0"
RX,0.799373040752351,"dt = JF (˜x0)d˜x0 dt ,"
RX,0.8025078369905956,"where JF (˜x) is the Jacobian matrix of the function F at the point ˜x. Since the dynamics of these
spaces are given by vector fields, we can replace the time derivatives in the equation with their vector
fields:
v(˜y0) = JF (˜x0)u(˜x0).
Since the point ˜x0 that we chose was arbitrary, the statement holds for every point in Mx.
□"
RX,0.8056426332288401,NeurIPS Paper Checklist
CLAIMS,0.8087774294670846,1. Claims
CLAIMS,0.8119122257053292,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction have been written to clearly state our claims and
contributions of the paper.
Guidelines:"
CLAIMS,0.8150470219435737,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.8181818181818182,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The paper discusses the limitations and assumptions of the approach, and the
possible failure modes. A section discussing the advantages and limitations of the approach
has also been included in the paper.
Guidelines:"
CLAIMS,0.8213166144200627,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.8244514106583072,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?"
CLAIMS,0.8275862068965517,"Answer: [Yes]
Justification: We have taken care to provide precise mathematical statements and proofs.
For proofs which we do not provide (like the proof of Takens’ theorem), we reference other
sources where the proof may be found."
CLAIMS,0.8307210031347962,Guidelines:
CLAIMS,0.8338557993730408,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8369905956112853,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8401253918495298,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8432601880877743,"Answer: [Yes]
Justification: We have worked to make our results as reproducible as possible. We have
provided a full description of the TSCI algorithm so that a reader should be able to implement
and reproduce our method from the written manuscript. The design of the experiments has
also be described within reason, so that any factor that might determine their outcome has
been detailed. For minor technical details (such as the type of ODE solver used to generate
data) which are less likely affect the results of the experiment, we refer to the code included
with the paper."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8463949843260188,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8495297805642633,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset)."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8526645768025078,"(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.8557993730407524,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.8589341692789969,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.8620689655172413,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8652037617554859,"Justification: Code to reproduce our results is being included with the publication, with MIT
license. The code is reasonably organized and documented, and instructions on how to use
the code have been included with the code itself. When making use of other code bases for
comparison (e.g., with latent CCM), a link to the original code base was provided."
OPEN ACCESS TO DATA AND CODE,0.8683385579937304,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.8714733542319749,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.8746081504702194,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.877742946708464,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.8808777429467085,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.8840125391849529,"Justification: We have included all necessary details to understand and interpret the setup
and results of the experimental section. Code is provided for the Rössler-Lorenz systems,
and details on all networks, optimizers, and training details were provided for latent TSCI."
OPEN ACCESS TO DATA AND CODE,0.8871473354231975,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.890282131661442,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8934169278996865,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.896551724137931,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8996865203761756,Answer: [Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9028213166144201,"Justification: In our experiments, we have provided suitable measures of the statistical
confidence of our methods. In particular, we run over several folds and report either
median/percentiles or mean/standard deviations of all results.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9059561128526645,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9090909090909091,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We state in the experiments section that all code was run on a 6-Core Intel
Core i5 CPU and NVIDIA Titan RTX GPU.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9122257053291536,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9153605015673981,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9184952978056427,"Justification: The research conducted in this paper is in compliance with the NeurIPS Code
of Ethics.
Guidelines:"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9216300940438872,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9247648902821317,10. Broader Impacts
BROADER IMPACTS,0.9278996865203761,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The paper includes discussion of the potential broader impact of the work in
the introduction.
Guidelines:"
BROADER IMPACTS,0.9310344827586207,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards"
BROADER IMPACTS,0.9341692789968652,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not pose any such risks for misuse.
Guidelines:"
BROADER IMPACTS,0.9373040752351097,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets"
BROADER IMPACTS,0.9404388714733543,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have provided in-text mentions and citations for all assets or methods that
we used in this work."
BROADER IMPACTS,0.9435736677115988,Guidelines:
BROADER IMPACTS,0.9467084639498433,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators."
NEW ASSETS,0.9498432601880877,13. New Assets
NEW ASSETS,0.9529780564263323,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?"
NEW ASSETS,0.9561128526645768,Answer: [Yes]
NEW ASSETS,0.9592476489028213,"Justification: We have included code with the publication that we have taken to document,
organize, and make understandable for the future reader. Our code publication includes a
proper license (MIT License) as well as instructions for use."
NEW ASSETS,0.9623824451410659,Guidelines:
NEW ASSETS,0.9655172413793104,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686520376175548,14. Crowdsourcing and Research with Human Subjects
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717868338557993,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749216300940439,Answer: [NA]
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9780564263322884,"Justification: The current paper does not involve crowdsourcing nor research with human
subjects."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9811912225705329,Guidelines:
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9843260188087775,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector."
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987460815047022,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9905956112852664,"Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9937304075235109,"Justification: The current paper does not require IRB approval as it does not contain research
with human or animal subjects.
Guidelines:"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9968652037617555,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
