Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029585798816568047,"We propose a novel and general framework (i.e., SABAL) that formulates batch
active learning as a sparse approximation problem. SABAL aims to ﬁnd a weighted
subset from the unlabeled data pool such that the corresponding training loss func-
tion approximates its full data pool counterpart. We realize the general framework
as a sparsity-constrained discontinuous optimization problem that explicitly bal-
ances uncertainty and representation for large-scale applications, for which we
propose both greedy and iterative hard thresholding schemes. The proposed method
can adapt to various settings, including both Bayesian and non-Bayesian neural
networks. Numerical experiments show that that SABAL achieves state-of-the-art
performance across different settings with lower computational complexity."
INTRODUCTION,0.005917159763313609,"1
INTRODUCTION"
INTRODUCTION,0.008875739644970414,"Over the last decade, deep neural networks have achieved promising results in various learning tasks.
However, obtaining labels for a complex training dataset can be challenging in practice, as the data
annotation is usually a time-consuming process that may require professional knowledge in certain
applications such as in medicine (Hoi et al., 2006; Shen et al., 2021). Active Learning (AL) (Settles,
2009) is commonly employed to mitigate the problem of scarce labeled data – enabling efﬁcient
model training with limited annotation costs. Given a partially labeled dataset, active learning ideally
selects data samples that are the best for learning. Speciﬁcally, it aims to iteratively query the most
helpful data to ask an oracle (human annotator) to annotate. The queried data samples can be added
back to the labeled data pool, and the model is updated. This process is repeated until the model has
achieved the desired performance. Intelligently identifying the most valuable data for annotation,
also known as the query strategy, is the key problem in active learning."
INTRODUCTION,0.011834319526627219,"A common strategy is to take the prediction uncertainty or data representation as the metric for
data query. This uncertainty-based approach (Settles, 2009; Tong & Koller, 2001; Gal et al., 2017;
Beluch et al., 2018) works by querying samples with high uncertainty, but often results in selecting
correlated and redundant data samples in each batch (Kirsch et al., 2019; Ducoffe & Precioso, 2018).
Representation-based approaches (Sener & Savarese, 2017; Yang & Loog, 2019) aim to select a subset
of data that represents the whole unlabeled dataset, but tend to be computationally expensive and
sensitive to batch sizes (Ash et al., 2019; Shui et al., 2020). More recently, several hybrid approaches
that try to take both uncertainty and representation into consideration have shown advantages (Ash
et al., 2019; Shui et al., 2020; Sinha et al., 2019). This paper takes this hybrid view towards an active
learning framework that balances the trade-off between uncertainty and representation."
INTRODUCTION,0.014792899408284023,"Besides hybrid approaches, deep Bayesian active learning has also gained attention due to recent
advances in Bayesian deep learning. Several Bayesian approaches (Gal et al., 2017; Kirsch et al.,
2019) leverage model uncertainty measurements (Gal & Ghahramani, 2015; 2016) determined by
Bayesian neural networks, while other works (Pinsler et al., 2019) leverage progress in Bayesian
Coreset problems (Zhang et al., 2021; Huggins et al., 2016; Campbell & Broderick, 2019). However,
as most existing Bayesian approaches are explicitly designed for Bayesian neural networks, another
goal of this paper is to propose a general method for both Bayesian and non-Bayesian models."
INTRODUCTION,0.01775147928994083,"For deep models, it is reasonable to query a large batch of data simultaneously to reduce model
update frequency. The batch selection approach is known as batch active learning. Taking an
optimization perspective, ﬁnding the best batch is NP-hard in general. Two common approaches
for such combinatorial problems are the greedy and clustering approaches. Greedy algorithms"
INTRODUCTION,0.020710059171597635,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.023668639053254437,"select one data sample in sequence until the batch budget is exhausted (Kirsch et al., 2019; Bıyık
et al., 2019; Chen & Krause, 2013). Here, speciﬁc conditions of the acquisition function such
as submodularity (Nemhauser et al., 1978) are required to guarantee a good optimization result.
Clustering algorithms regard cluster centers as their queried batch (Sener & Savarese, 2017; Ash
et al., 2019), but can be computationally expensive. To our knowledge, except for Pinsler et al.
(2019) that focus on the Bayesian models, so far active learning has rarely been studied from a sparse
approximation perspective. This is despite the ubiquity of sparse approximation in signal processing
for tasks such as dictionary learning (Aharon et al., 2006) and compressed sensing (Donoho, 2006)
due to its performance for discovering a sparse representation while avoiding redundant information.
Here we employ sparse approximation methods for batch active learning tasks."
INTRODUCTION,0.026627218934911243,"Our main contributions are summarized in the following. We propose a novel and ﬂexible Sparse
Approximation-based Batch Active Learning framework, i.e., SABAL. We show how SABAL
generalizes batch active learning as a sparse approximation problem and can adapt to different
settings and models. The central intuition of SABAL is ﬁnding a weighted subset from the unlabeled
data pool so that its corresponding training loss approximates the full-set loss function in a function
space. We realize the SABAL framework as an efﬁcient ﬁnite-dimensional optimization problem:
First, we derive an upper bound to balance the trade-off between uncertainty and representation in a
principled way. Second, we approximate the loss functions using ﬁnite-dimensional approximation.
This results in a sparsity-constrained discontinuous optimization problem, for which we propose
several efﬁcient optimization algorithms. We demonstrate the advantages of SABAL in experiments
for both Bayesian and non-Bayesian batch active learning settings."
INTRODUCTION,0.029585798816568046,"The structure of this manuscript is as follows. In Section 2, we formulate the general framework of
SABAL, and in Section 3, we realize the framework into a ﬁnite-dimensional discontinuous sparse
optimization problem. To solve the resulting optimization problem, we propose two optimization
algorithms in Section 4. Related work are discussed in Section 5 and Appendix Section B. Results of
our experiments are presented in section 6, and all proofs are provided in Appendix Section C."
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.03254437869822485,"2
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.03550295857988166,"This section introduces the preliminaries and the general formulation of batch active learning as a
sparse approximation problem."
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.038461538461538464,"Preliminaries
Vectors are denoted as bold lower case letters, e.g., w ∈Rn. The l0 pseudo-
norm of a vector w is denoted as ∥w∥0, i.e., the number of non-zero elements of w. We denote
R+ := [0, +∞). Distributions are denoted in script, e.g., P, and a random variable is denoted by
tilde, e.g., ˜y ∼P. We denote sets in calligraphy or in uppercase Greek alphabet (e.g., D, Θ), and
additionally we denote [n] := {1, 2, . . . , n}. In supervised learning, given a labeled training dataset
Dl := {(xi, yi)}nl
i=1, where we denote their domain to be x ∈X and y ∈Y, the empirical goal is to
minimize a loss function Ll(θ) := P"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.04142011834319527,"(xi,yi)∈Dl ℓ(xi, yi; θ) formed by the training dataset, where
θ ∈Θ ⊂Rm is the parameter of the model and ℓis a loss function evaluated on individual pairs
of data. Without loss of generality, we assume Θ ⊂Rm is compact and ℓ(x, y; ·) : Θ →R is in a
normed space (L(Θ, R), ∥· ∥†) for all x, y. We further assume the constant function f : Θ →1 is
included in L(Θ, R). The “†” in the norm ∥· ∥† : L(Θ, R) →R+, representing its deﬁnition is a
placeholder that will be discussed later."
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.04437869822485207,"Batch Active Learning
Besides the labeled dataset Dl, there is an unlabeled dataset Du :=
{xj}nu
j=1 where the labels are unknown but could be acquired at a high cost through human labeling.
Combining two datasets, the ideal loss function to minimize w.r.t. θ is
P"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.047337278106508875,"(xi,yi)∈Dl ℓ(xi, yi; θ) + P"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.05029585798816568,"xj∈Du ℓ(xj, y⋆
j ; θ),
(1)"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.05325443786982249,"where y⋆
j is the unknown true label corresponding to the data xj. Since acquiring true labels could
be costly, we have to impose a budget b (b < nu) on the number of label acquisitions. Therefore, the
batch active learning problem is to ﬁnd a subset S ⊂Du such that we can obtain a good model by
optimizing the following loss function w.r.t. θ,
P"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.05621301775147929,"(xi,yi)∈Dl ℓ(xi, yi; θ) + P"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.05917159763313609,"xj∈S ℓ(xj, y⋆
j ; θ),
where |S| = b.
(2)"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.0621301775147929,Under review as a conference paper at ICLR 2022
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.0650887573964497,"Generalized Batch Active Learning
We start our method by generalizing the classical formulation
(equation 2) by considering an importance weight for each unlabeled data. That is, we aim to ﬁnd
a sparse non-negative vector w ∈Rnu
+ such that we can obtain a good model by optimizing the
following loss function w.r.t. θ:
P"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.06804733727810651,"(xi,yi)∈Dl ℓ(xi, yi; θ) + P"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.07100591715976332,"xj∈Du wjℓ(xj, y⋆
j ; θ),
where ∥w∥0 = b.
(3)"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.07396449704142012,"A key question now is—what is the criterion for a good w? Comparing the ideal loss function
(equation 1) and the sparse importance weighted loss (equation 3), the only difference is their
unlabeled data loss functions. Therefore, a straight-forward informal criterion for a good importance
weight w is that the two unlabeled data loss functions are close to each other, i.e.,"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.07692307692307693,"L⋆
w(θ) := 1 b X"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.07988165680473373,"xj∈Du
wjℓ(xj, y⋆
j ; θ)
≈
L⋆(θ) := 1 nu X"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.08284023668639054,"xj∈Du
ℓ(xj, y⋆
j ; θ)."
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.08579881656804733,"However, as the true labels are unknown, we cannot compute L⋆
w and L⋆. Luckily, we can have
an estimator for the true labels, i.e., estimation based on the labeled data p(˜yj | xj, Dl) or an
approximation of it. Denote P(xj) as an estimated distribution, so ˜yj ∼P(xj), then the informal
criterion for a good importance weight w then becomes"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.08875739644970414,˜Lw(θ) := 1 b X
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.09171597633136094,"xj∈Du
wjℓ(xj, ˜yj; θ)
≈
˜L(θ) := 1 nu X"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.09467455621301775,"xj∈Du
ℓ(xj, ˜yj; θ).
(4)"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.09763313609467456,"Thus, we are one step closer to evaluating the quality of a weighted selection. The next question is
how to measure the difference between ˜L and ˜Lw."
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.10059171597633136,"Difference Between Two Loss Functions
Given the two loss functions ˜L, ˜Lw ∈L(Θ, R), where
L(Θ, R) is equipped with the norm ∥· ∥†, a straight-forward measurement of the difference between
them is ∥˜L −˜Lw∥†. However, observing that the optimization of a loss function is shift-invariant, the
difference between two loss functions should also be shift-invariant. For example, for ∀L ∈L(Θ, R)
we have arg minθ∈Θ(L(θ) + c) = arg minθ∈Θ L(θ) for ∀c ∈R, implying that L + c should be
treated the same as L. Therefore, to account for the shift-invariance, we deﬁne q : L(Θ, R) →R+ as"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.10355029585798817,"q(L) := inf
c∈R ∥L + c∥†,
∀L ∈L(Θ, R).
(5)"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.10650887573964497,"Note that we abuse the notation a bit, i.e., the c in L + c should be the constant function that maps
every θ ∈Θ to c. The above deﬁnition has some nice properties that make it a good difference
measurement of two loss functions, as proved in proposition C.1 in the appendix. In particular, q(·)
satisﬁes the triangle inequality, and q(L + c) = q(L) for any constant c. Therefore, we can formulate
the generalized batch active learning problem as the following sparse approximation problem."
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.10946745562130178,"Problem 1 (Sparse Approximation-based Batch Active Learning). Given the shift-invariant seminorm
q induced by the norm ∥· ∥† (equation 5), and a label estimation distribution P, the generalized
batch active learning problem (equation 4) is formally deﬁned as"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.11242603550295859,"arg min
w∈Rnu
+
EP[q(˜L −˜Lw)]
s.t.
∥w∥0 = b,
(6)"
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.11538461538461539,where EP stands for the expectation over ˜yj ∼P(xj) for ∀j ∈[nu].
BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION,0.11834319526627218,"Problem 1 (SABAL) offers a general framework for batch active learning and can be applied with
various settings, i.e., both the norm ∥· ∥† and the individual loss function ℓcan be chosen based on
speciﬁc problems and applications. In the next section, we introduce two practical realizations of
equation 6 for Bayesian and non-Bayesian active learning respectively."
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.12130177514792899,"3
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1242603550295858,"In this section, we transform the sparse approximation problem (equation 6) into a ﬁnite-dimensional
sparse optimization problem. First, we address an issue regarding the sampling of EP. Then, we
discuss some concrete choices of P and ∥· ∥† that lead to a ﬁnite-dimensional sparse optimization."
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.12721893491124261,Under review as a conference paper at ICLR 2022
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1301775147928994,"Addressing the Sampling Issue
In equation 6, the expectation EP is taken over the product
space of (˜y1, . . . , ˜ynu) and each sample has to be remembered for future optimization, which can
be intractable for large datasets. However, it has an upper bound where the complexity of the
optimization is independent of the number of samples from P. First, by the triangle inequality
EP[q(˜L −˜Lw)] = EP[q(˜L −EP[˜L] + EP[˜L] −EP[˜Lw] + EP[˜Lw] −˜Lw)]"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.13313609467455623,"≤EP[q(˜L −EP[˜L])] + EP[q(˜Lw −EP[˜Lw])]
|
{z
}
(i): variance"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.13609467455621302,"+ q(EP[˜L] −EP[˜Lw])
|
{z
}
(ii): approximation bias . (7)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1390532544378698,"We can see that it offers a trade-off between bias and variance, where the bias term is immediately
tractable by expanding ˜L, ˜Lw:
(ii) = q(EP[ 1 nu
P"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.14201183431952663,"xj∈Du ℓ(xj, ˜yj; ·)] −EP[ 1 b
P"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.14497041420118342,"xj∈Du wjℓ(xj, ˜yj; ·)])"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.14792899408284024,= q(( 1
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.15088757396449703,"nu
P
xj∈Du EP(xj)[ℓ(xj, ˜yj; ·)]) −( 1"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.15384615384615385,"b
P
xj∈Du wjEP(xj)[ℓ(xj, ˜yj; ·)])).
(8)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.15680473372781065,"It remains to address the variance term (i). Recall that the more accurate P is, the more accurate our
approximation is. Given the decision wj > 0, if the label of xj is acquired, i.e., the oracle (human
annotator) will offer us its true label y⋆
j , and the labeling distribution would be improved. That being
said, the distribution of ˜yj given xj and wj > 0 will be concentrated on its true label y⋆
j , i.e.,"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.15976331360946747,˜yj ∼Pw(xj) :=
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.16272189349112426,"(
P(xj)
if wj = 0
δy⋆
j
if wj > 0 ,
where w ∈Rnu
+
(9)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.16568047337278108,"where δy⋆
j denotes the distribution that ˜yj can only be y⋆
j . However, the improved distribution Pw
is not known before the acquisition of the true labels y⋆
j for wj > 0. Fortunately, although Pw(xj)
is not known, it is known that the corresponding variance for ˜yj would be zero no matter what its
label is. Applying this trick, we show in the following proposition that the term (i) with the improved
label distribution Pw has an upper bound that does not require to know the true labels.
Proposition 3.1. Let w ∈Rnu
+ and ∥w∥0 = b, by replacing the P by the improved estimation
distribution Pw (equation 9) into (i) in equation 7, we have"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.16863905325443787,"EPw[q(˜L −EPw[˜L])] + EPw[q(˜Lw −EPw[˜Lw])] ≤
X"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.17159763313609466,"xj∈Du
1(wj = 0) · σj,
(10)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.17455621301775148,"where σj :=
1
nu EP(xj)[q(ℓ(xj, ˜yj; ·) −EP(xj)[ℓ(xj, ˜yj; ·)])] is the individual variance, and 1(·)
is the indicator function."
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.17751479289940827,"Therefore, combining equation 8 and equation 10, we have a more tractable form of the sparse
approximation, i.e.,"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1804733727810651,"arg min
w∈Rnu
+
q(EP[˜L] −EP[˜Lw]) +
X"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1834319526627219,"xj∈Du
1(wj = 0) · σj
s.t.
∥w∥0 = b,
(11)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1863905325443787,"Intuitively, such decomposition of bias and variance naturally provides metrics of uncertainty and
representation for active learning, where the variance itself is a metric of uncertainty, meanwhile the
bias term measures how well a subset of selected data can represent the whole unlabeled data. Now,
it remains to specify the choice of ∥· ∥†, i.e., the norm that induces q (equation 5)."
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1893491124260355,"Formulation of the Finite-Dimensional Optimization
We consider two concrete choices of the
∥· ∥† for Bayesian and non-Bayesian settings respectively."
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.19230769230769232,"1. In the Bayesian setting, we can easily sample θi ∼π := p(θ | Dl) from the posterior. Utilizing
the posterior, we make the norm ∥· ∥† more concrete by considering the L2(π)-norm, i.e.,
∥L∥2
π = Eθ∼π[L(θ)2]. Accordingly,
q(L)2 = inf
c∈R ∥L + c∥2
π = inf
c∈R Eθ∼π[(L(θ) + c)2] = Eθ∼π[(L(θ) −Eθ∼π[L(θ)])2].
(12)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.1952662721893491,"The posterior π tells us where and how to evaluate the “magnitude” of L. Noting that equation 12
is in the form of an expectation, we can draw m samples θi ∼π to approximate it. Denote
g :=
1
√m[. . . , (L(θi) −¯L), . . . ]⊤
i=1...m ∈Rm where ¯L := 1"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.19822485207100593,"m
Pm
i=1 L(θi). equation 12 becomes"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.20118343195266272,"q(L)2 ≈
1
m m
X"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.20414201183431951,"i=1
(L(θi) −¯L)2 = ∥g∥2
2,
(13)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.20710059171597633,where ∥g∥2 is simply the Euclidean norm of the m-dimensional vector g.
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.21005917159763313,Under review as a conference paper at ICLR 2022
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.21301775147928995,"2. In the non-Bayesian setting, we evaluate the loss function in a local “window” based on the current
model. We consider the ∥· ∥∞-norm over a Euclidean ball Br(θ0) of radius r centered at the
current model parameter θ0, i.e., ∥L∥∞= maxθ∈Br(θ0) |L(θ)|. Moreover, in the Euclidean ball
we approximate L(θ) ≈L(θ0) + ∇L(θ0)⊤(θ −θ0). Therefore, we have"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.21597633136094674,"q(L) = inf
c∈R ∥L + c∥∞= inf
c∈R
max
θ∈Br(θ0) |L(θ) + c|"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.21893491124260356,"≈inf
c∈R
max
θ∈Br(θ0) |L(θ0) + ∇L(θ0)⊤(θ −θ0) + c| = r∥∇L(θ0)∥2.
(14)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.22189349112426035,Note that ∥∇L(θ0)∥2 is the Euclidean norm of the gradient vector ∇L(θ0) ∈Rm.
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.22485207100591717,"To estimate the label distribution, P(xj) = p(˜yj | xj, Dl) can be directly applied on Bayesian
models by estimating the predictive model posterior. For non-Bayesian models, one could utilize
the calibrated model prediction (Guo et al., 2017) as the label distribution. Finally, plugging either
of the two approximations of q(L) into equation 11, and squaring all of the terms for the ease of
optimization, we can formulate the sparse approximation problem as the following ﬁnite-dimensional
optimization problem, where α > 0 offers a trade-off between bias and variance."
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.22781065088757396,"arg min
w∈Rnu
+
∥v −Φw∥2
2 + α
X"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.23076923076923078,"xj∈Du
1(wj = 0) · σ2
j
s.t.
∥w∥0 = b,
(15)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.23372781065088757,"where we denote v ∈Rm, Φ ∈Rm×nu and σj as"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.23668639053254437,"v := 1 nu nu
X"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.23964497041420119,"j=1
EP(xj)[gj(˜yj)],
Φ := 1"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.24260355029585798,"b (EP(x1)[g1(˜y1)], . . . , EP(xnu)[gnu(˜ynu)]),"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.2455621301775148,σj = 1
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.2485207100591716,"nu
EP(xj)[∥gj(˜yj) −EP(xj)[gj(˜yj)]∥2],"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.2514792899408284,"gj(˜yj) :=
[. . . , (ℓ(xj, ˜yj; θi) −¯ℓ), . . . ]⊤
i=1...m,
¯ℓ:= 1"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.25443786982248523,"m
Pm
i=1 ℓ(xj, ˜yj; θi)
if use (13)
∇ℓ(xj, ˜yj; θ0)
if use (14).
(16)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.257396449704142,"In practice, it is often the case that the number of parameters is less than the number of samples,
i.e., m < nu, even for over-parameterized neural networks where the gradient of the last layer is
commonly used to represent the full-model gradient (Katharopoulos & Fleuret, 2018; Ash et al.,
2019). Therefore, if the batch size is big, i.e., b > m, the approximation bias ∥v −Φw∥2
2 may be
under-determined with inﬁnitely many w to make v = Φw, and the optimization (equation 15)
may be ”overﬁtted”. To make our method more stable, we include a ℓ2 regularizer β∥w −1∥2
2 with
β > 0. Finally, since wj ≥0, minimizing α P"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.2603550295857988,"xj∈Du 1(wj = 0) · σ2
j is equivalent to minimizing
−α P"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.26331360946745563,"xj∈Du 1(wj > 0) · σ2
j . Consequently, we have the following optimization problem."
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.26627218934911245,"Problem 2 (Sparse Approximation as Finite-dimensional Optimization). The ﬁnite-dimensional
optimization for generalized batch active learning is"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.2692307692307692,"arg min
w∈Rnu
+
∥v −Φw∥2
2 −α
X"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.27218934911242604,"xj∈Du
1(wj > 0) · σ2
j + β∥w −1∥2
2
s.t.
∥w∥0 = b.
(17)"
SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION,0.27514792899408286,"While simpliﬁed, the result is a sparse discontinuous optimization problem generally difﬁcult to solve.
In the next section, we propose two optimization algorithms for equation 17 by exploiting its unique
properties. The overall procedure of SABAL in practice is presented in Algorithm 3 Appendix A."
OPTIMIZATION ALGORITHMS,0.2781065088757396,"4
OPTIMIZATION ALGORITHMS"
OPTIMIZATION ALGORITHMS,0.28106508875739644,"This section focuses on optimizing Problem 2. Rewrite equation 17 f(w) := f1(w) + f2(w), where"
OPTIMIZATION ALGORITHMS,0.28402366863905326,"f1(w) := ∥v −Φw∥2
2 + β∥w −1∥2
2,
f2(w) := −α P
xj∈Du 1(wj > 0) · σ2
j ."
OPTIMIZATION ALGORITHMS,0.2869822485207101,"The optimization has two major difﬁculties, i.e., the nonconvex sparsity constraint ∥w∥0 = b and the
discontinuous objective function f2. When it comes to sparsity-constrained optimization, there are"
OPTIMIZATION ALGORITHMS,0.28994082840236685,Under review as a conference paper at ICLR 2022
OPTIMIZATION ALGORITHMS,0.29289940828402367,"two schemes that are widely considered — greedy (Nemhauser et al., 1978; Campbell & Broderick,
2019) and iterative hard thresholding (IHT) (Zhang et al., 2021; Khanna & Kyrillidis, 2018). However,
Problem 2 introduces the new difﬁculty other than the sparsity constraint, i.e., the discontinuous
component f2, which violate the assumptions of many of these methods which require the use of
gradient. Instead, we propose two algorithms (Algorithm 1&2) speciﬁcally for Problem 2 under the
two schemes respectively, while incorporating the discontinuity."
OPTIMIZATION ALGORITHMS,0.2958579881656805,"We introduce some notations used in this section. Given a vector g, we denote [g]+ as g with its
negative elements set to 0. For an index j, we denote gj or (g)j to be its jth element. For an index
set S, we denote [g]S to be the vector where ([g]S)j = (g)j if j ∈S and ([g]S)j = 0 if j /∈S.
Moreover, we denote ej to be the unit vector where (ej)j = 1 and (ej)i = 0 for ∀i ̸= j."
OPTIMIZATION ALGORITHMS,0.2988165680473373,"Although two algorithms use different schemes, they share the same two sub-procedures: a line search
and de-bias step (Algorithm 4 and 5 in Appendix D), which signiﬁcantly improve the optimization
performance (Zhang et al., 2021). The line search sub-procedure optimally solves the problem
arg minµ∈R f1(w −µu), i.e., given a direction u, what is the best step size µ to move the w along
u. The de-bias sub-procedure adjusts a sparse w in its own sparse support for a better solution."
OPTIMIZATION ALGORITHMS,0.30177514792899407,"Algorithm 1: SABAL-Greedy
Parameter: sparsity b; step size τ."
OPTIMIZATION ALGORITHMS,0.3047337278106509,1 w ←0; S ←∅
REPEAT,0.3076923076923077,2 repeat
REPEAT,0.3106508875739645,"3
j ←arg min
j∈[nu]\S
τ(∇f1(w))j −ασ2
j"
REPEAT,0.3136094674556213,"4
S ←S ∪{j}
(update selection)"
REPEAT,0.3165680473372781,"5
µ ←LineSearch(ej, w)"
REPEAT,0.31952662721893493,"6
w ←De-bias(w −µej)"
REPEAT,0.3224852071005917,"7
wj ←0 for ∀wj < 0
(w ∈Rnu
+ )"
REPEAT,0.3254437869822485,"8 until |S| = b;
Return: w"
REPEAT,0.32840236686390534,"Algorithm 2: SABAL-IHT
Parameter: sparsity b; number of iterations T."
REPEAT,0.33136094674556216,1 w ←0; z ←0
REPEAT,0.3343195266272189,2 repeat
REPEAT,0.33727810650887574,"3
w′ ←w
(save previous w)"
REPEAT,0.34023668639053256,"4
µ ←LineSearch(∇f1(z), z)"
REPEAT,0.3431952662721893,"5
s ←z −µ∇f1(z)
(gradient descent)"
REPEAT,0.34615384615384615,"6
w ←
arg min
w∈Rnu
+ ,∥w∥0≤b"
REPEAT,0.34911242603550297,"1
2∥w−s∥2
2 +f2(w)"
REPEAT,0.3520710059171598,"7
w ←De-bias(w)"
REPEAT,0.35502958579881655,"8
wj ←0 for ∀wj < 0
(w ∈Rnu
+ )"
REPEAT,0.35798816568047337,"9
τ ←LineSearch(w −w′, w)"
REPEAT,0.3609467455621302,"10
z ←w −τ(w −w′)
(momentum)"
REPEAT,0.363905325443787,"11 until T iterations;
Return: w"
REPEAT,0.3668639053254438,"Opt. Algorithm: Greedy
The core idea of
the greedy approach is noted in line 3 Algo-
rithm 1, where it chooses an index j to move
a step of size τ that minimizes the objective,
i.e., j ←arg minj∈[nu]\S
(f1(w + τej) −
f1(w)) + (f2(w + τej) −f2(w)). By ap-
proximating f1(w +τej)−f1(w) by its ﬁrst-
order approximation ⟨∇f1(w), τej⟩, and not-
ing that f2(w + τej) −f2(w) = −ασ2
j , we
have the greedy step (line 3) in Algorithm 1.
After choosing the index j to include, line 5
chooses an optimal step to move, followed by
a de-bias step that further improves the solu-
tion in the current sparse support supp(w)."
REPEAT,0.3698224852071006,"Opt. Algorithm: Proximal iterative hard
thresholding
The core idea of the proximal
IHT (Algorithm 2) is noted in line 6, where it
combines both the hard thresholding and the
proximal operator. It minimizes the discontin-
uous f2 in a neighbourhood of the solution s
obtained by minimizing f1, while satisfying
the constraints. As discussed in the section D,
the inner optimization (line 6) can be done op-
timally by simply picking the top-b elements
from nu elements. After this core step, a de-
bias step improves the solution w within its
sparse support, followed by a momentum step."
REPEAT,0.3727810650887574,"Complexity Analysis
We analyze the time
complexity of the proposed algorithms with
respect to the number of data samples n, and the batch size b of batch active learning. Except for line 6
Algorithm 2, all steps are of time complexity O(n). The line 6 Algorithm 2 is ﬁnding the b smallest
elements, which can be done in O(n log(b)). Therefore, the time complexity for SABAL-Greedy is
O(nb), and the time complexity for SABAL-IHT is O(n log(b)). Comparing to the time complexity
O(nb2) of the state-of-the-art method BADGE (Ash et al., 2019), the two proposed algorithms can
be much faster, especially with a large batch size b used in practice."
RELATED WORK,0.3757396449704142,"5
RELATED WORK"
RELATED WORK,0.378698224852071,"Our method has several characteristics: (1) it’s a hybrid active learning approach. (2) it’s a general
framework with building blocks easily adapted to both Bayesian and non-Bayesian settings. (3) it"
RELATED WORK,0.3816568047337278,Under review as a conference paper at ICLR 2022
RELATED WORK,0.38461538461538464,"formulates data acquisition as a sparse approximation problem. This section focuses on discussing
some most relevant works, and explain how they motivate and compare to our work. Other related
works are discussed in Appendix B."
RELATED WORK,0.3875739644970414,"As data acquisitions with the trade-off between uncertainty and representation have attracted attention,
several recent works have proposed hybrid active learning methods. One of the state-of-art methods,
BADGE (Ash et al., 2019), captures uncertainty through the lens of gradients, and samples diverse
batches on the gradient embedding by the k-MEANS++ seeding algorithm. However, one of the
downsides of BADGE is the high run-time complexity, as data acquisition speed is crucial in practice.
Sinha et al. (2019) train a Variational Autoencoder and a discriminator in an adversarial fashion.
The discriminator predicts a sample as unlabeled based on its likelihood of representativeness, and a
batch of samples with the lowest conﬁdence will be queried, but their adversarial method is difﬁcult
to apply to general and Bayesian neural networks. Our proposed method explicitly balances the
trade-offs between uncertainty and representation by bias and variance decomposition."
RELATED WORK,0.3905325443786982,"Coreset selection is a common high-level idea used in active learning, and methods vary in how one
characterizes the closeness of a chosen coreset to the full-set. Sener & Savarese (2017) characterizes
the closeness as how much a coreset covers the full-set in the Euclidean distance in a feature space.
It derives an upper bound for the coreset loss based on the Lipschitz continuity and transforms the
original problem to a KCenter problem. However, their method relies on good feature representation,
which is not always guaranteed in practice. Pinsler et al. (2019) is mainly based on existing Bayesian
inference literature, especially the Bayesian Coreset problem (Campbell & Broderick, 2019). They
characterizes the closeness as how much the core-set log-posterior approximates the full-set log-
posterior, with the log-posterior directly derived from the Bayes’ rule. However, their problem
formulation relies on the Bayesian setting and Bayesian models, and conducting posterior inference
is non-trivial for non-Bayesian models. In constrast, we propose SABAL, which characterizes the
closeness in a more general sense, i.e., through a semi-norm function directly on the difference
between the coreset loss function and the full-set loss function."
EXPERIMENT RESULTS,0.39349112426035504,"6
EXPERIMENT RESULTS"
EXPERIMENT RESULTS,0.39644970414201186,"We demonstrate that SABAL is a ﬂexible batch active learning framework with relatively small time
complexity by evaluating its performance on image classiﬁcation tasks with various models under
different settings. First, using Bayesian neural networks, we show the effectiveness of SABAL on
Bayesian batch active learning. Next, we demonstrate that SABAL can also adapt well to general
batch active learning with general neural networks. Finally, we show that SABAL also has runtime
advantages compared to other state-of-art methods. We also conduct an ablation study to show how
SABAL balances the trade-offs between uncertainty and representation in Appendix E.1."
EXPERIMENT RESULTS,0.3994082840236686,"We have a ﬁxed training, validation, and testing set in each experiment. The model is initially trained
on small amounts of labeled data randomly selected from the training set and then iteratively performs
the data acquisition and annotation. The model is reinitialized and retrained at the beginning of
each active learning iteration. After the model is well trained, its testing accuracy is evaluated on
the testing set as a measure of the performance. All experiments are repeated multiple times using
5 random seeds (3 for the small model LeNet-5 (LeCun et al., 2015)), and the results are reported
as mean and standard deviations. The performance of each iteration are shown in learning curve
plots. To better visualize the overall performance of AL methods, We also measure the area under
curve (AUC) scores of the learning curve of different AL methods across different datasets. The
top two AUC scores are highlighted in bald. We implement SABAL using both proximal IHT and
greedy as two different optimization methods for the sparse approximation, denoted as SABAL-IHT
and SABAL-Greedy, compared with following baselines in literature: (1) Random: A naive baseline
that selects a batch of data uniformly at random. (2) BALD (Houlsby et al., 2011): An uncertainty-
based Bayesian method that selects a batch of data with maximum mutual information between
model parameters and predictions. (3) Entropy (Wang & Shang, 2014): An uncertainty-based
non-Bayesian method that selects a batch of data with maximum entropy of the model predictions
H (yi | xi; θ). (4) KCenter (Sener & Savarese, 2017): A representation-based non-Bayesian method
that reformulates the coreset selection as a KCenter problem in the feature embedding space. (5)
BADGE (Batch Active Learning by Diverse Gradient Embeddings) (Ash et al., 2019): A hybrid
non-Bayesian method that samples a diverse batch of data using the k-MEANS++ seeding algorithm."
EXPERIMENT RESULTS,0.40236686390532544,Under review as a conference paper at ICLR 2022
EXPERIMENT RESULTS,0.40532544378698226,"(6) Bayesian Coreset (Pinsler et al., 2019): A Bayesian batch active learning approach based on the
Bayesian Coreset problem (Huggins et al., 2016; Campbell & Broderick, 2019)."
EXPERIMENT RESULTS,0.40828402366863903,"SABAL for Bayesian Active Learning
We ﬁrst implement our experiments on Bayesian neural
networks and perform Bayesian active learning on Fashion MNIST (LeCun et al., 1998), CIFAR-
10 (Krizhevsky et al., 2009), and CIFAR-100 (Krizhevsky et al., 2009). For fair comparison, we keep
the same experiment settings of Pinsler et al. (2019), using a Bayesian neural network consisting of
a ResNet-18 (He et al., 2016) feature extractor. The posterior inference is obtained by variational
inference (Wainwright & Jordan, 2008; Blundell et al., 2015) at the last layer, and model predictive
posteriors p(˜yj | xj, Dl) are estimated using 100 samples. Equation 13 is used to solve the ﬁnite-
dimensional optimization problem, because sampling from the posterior distribution in a Bayesian
neural network will be efﬁcient by leveraging the local reparameterization trick (Kingma et al.,
2015). Besides Random, we mainly focus on comparing with state-of-the-art approaches speciﬁcally
designed for Bayesian active learning: BALD and Bayesian Coreset."
EXPERIMENT RESULTS,0.41124260355029585,"We then evaluate SABAL’s performace. On Fashion MNIST dataset, we use 100 samples for random
projections, 1000 seed data, and query 1000 samples for 9 iterations. On CIFAR-10 and CIFAR-100,
two more complicated datasets, we use 2000 samples for random projections, 3000 (10000 for CIFAR-
100) seed data, and query 5000 samples for 4 iterations. Because Bayesian Coreset usually ﬁnds a
much smaller batch than requested, for a fair comparison, we let Bayesian Coreset acquire more data
than the batch size, and stop the acquisition as long as it has selected a full batch of data. It can be
seen in Table 1 and Figure 1 that both SABAL-IHT and SABAL-Greedy show some advantages on
Fashion MNIST dataset. On CIFAR-10 and CIFAR-100, we ﬁnd SABAL-Greedy performs better
than SABAL-IHT while outperforming other baselines, including the Bayesian Coreset, one of the
current state-of-the-art approach in the literature under Bayesian settings."
EXPERIMENT RESULTS,0.41420118343195267,"Table 1: AUC Score (± std.) for different AL methods on Bayesian active learning. AUC measures
the overall performance improvement across number of queries. Results show the proposed SABAL-*
matches or outperforms all baselines."
EXPERIMENT RESULTS,0.4171597633136095,"Dataset
Bayesian Coreset
SABAL-Greedy
SABAL-IHT
BALD
Random"
EXPERIMENT RESULTS,0.42011834319526625,"Fashion MNIST
89.53 ± 0.25
89.83 ± 0.26
89.97 ± 0.23
89.72 ± 0.23
88.39 ± 0.32
CIFAR10
77.73 ± 0.70
78.28 ± 0.53
77.55 ± 0.83
77.61 ± 0.59
76.36 ± 0.59
CIFAR100
42.40 ± 0.29
42.53 ± 0.34
42.30 ± 0.29
41.99 ± 0.60
42.10 ± 0.24"
EXPERIMENT RESULTS,0.4230769230769231,"Figure 1: Active learning results on Bayesian models. Solid lines and shaded areas represent means
and standard deviations of test accuracy over different seeds. Our method especially SABAL-Greedy
outperforms most baselines and the SOTA method Bayesian Coreset."
EXPERIMENT RESULTS,0.4260355029585799,"SABAL for General Active Learning
We then implement our experiments on general convolu-
tional neural networks, including LeNet-5 (LeCun et al., 2015) and VGG-16 (Simonyan & Zisserman,
2014) architectures without any Bayesian layers, using MNIST (LeCun et al., 1998), SVHN (Netzer
et al., 2011), and CIFAR-10 (Krizhevsky et al., 2009) datasets. We utilize calibrated prediction
of current model with temperature scaling (Guo et al., 2017) to approximate the label distribution
p(˜yj | xj, Dl). Because the gradient of the last layer represents the full-model gradient (Ash et al.,
2019), we can easily solve the optimization problem with gradient embedding as equation 14. We
compare with popular non-Bayesian baselines: Random, Entropy, KCenter, and BADGE."
EXPERIMENT RESULTS,0.4289940828402367,"To evaluate the performance of SABAL, on MNIST dataset with LeNet-5 model, we use 40 seed
data, and query 40 samples for 15 iterations. On SVHN and CIFAR-10 with VGG-16 model, which
contains more complicated real-world color images, we use 1000 (3000 for CIFAR-10) seed data,"
EXPERIMENT RESULTS,0.4319526627218935,Under review as a conference paper at ICLR 2022
EXPERIMENT RESULTS,0.4349112426035503,"and query 1000 (3000 for CIFAR-10) samples for 5 iterations. The results are shown in Table 2 and
Figure 2. In general, SABAL-Greedy also performs better than SABAL-IHT and outperform most
baselines, while achieving comparable performance to the strong baseline BADGE, the current SOTA
non-Bayesian method in literature, but SABAL requires much less acquisition time than BADGE
especially on large models. In addition, most methods perform similarly on CIFAR-10, and we
conjecture that for CIFAR-10 each sample is informative enough and thus random selection can
achieve good enough performance."
EXPERIMENT RESULTS,0.4378698224852071,"Table 2: AUC Score (± std.) for different AL methods on general active learning. AUC measures the
overall performance improvement across number of queries. Results show the proposed SABAL-*
matches or outperforms all baselines."
EXPERIMENT RESULTS,0.4408284023668639,"Dataset
BADGE
SABAL-Greedy
SABAL-IHT
KCenter
Entropy
Random"
EXPERIMENT RESULTS,0.4437869822485207,"MNIST
91.24 ± 0.48
90.89 ± 0.38
91.07 ± 0.45
89.57 ± 1.02
90.68 ± 0.81
86.48 ± 1.11
SVHN
86.92 ± 0.71
87.23 ± 0.47
86.84 ± 0.57
87.04 ± 0.80
86.28 ± 1.05
85.52 ± 0.51
CIFAR10
68.20 ± 0.56
68.01 ± 0.66
67.80 ± 0.69
67.98 ± 0.63
67.94 ± 0.64
67.05 ± 0.59"
EXPERIMENT RESULTS,0.4467455621301775,"Figure 2: Active learning results on general (non-Bayesian) models. Solid lines and shaded areas
represent means and standard deviations of test accuracy over different seeds. Our method performs
comparable or better than baselines and the SOTA method BADGE."
EXPERIMENT RESULTS,0.44970414201183434,"Run Time Comparison
Our experiments show that SABAL can achieve comparable performance
of SOTA methods while requiring much less acquisition time. We compare the empirical results of
runtime complexity of SABAL with other baselines in non-Bayesian active learning experiment as an
example. Here, we consider the acquisition time of the ﬁrst query, where the unlabeled data pool has
the largest size compared with later queries. Large models and datasets (SVHN and CIFAR-10 on
VGG-16) are used to better illustrate the runtime complexity. Results are shown in Table 3. It can be
seen that SABAL requires much less runtime than BADGE especially when queried batch is large,
and even less than KCenter in most cases."
EXPERIMENT RESULTS,0.4526627218934911,"Table 3: ﬁrst query’s acquisition time (± std.) of different AL methods on two large datasets.
SABAL-* shows big runtime advantage."
EXPERIMENT RESULTS,0.4556213017751479,"Dataset
Method
Time (unit:s)
Dataset
Method
Time (unit:s)"
EXPERIMENT RESULTS,0.45857988165680474,"SVHN
BADGE
732.18 ± 26.29
CIFAR10
BADGE
1207.19 ± 121.09
SABAL-Greedy
201.65 ± 3.54
SABAL-Greedy
333.67 ± 3.53
SABAL-IHT
211.04 ± 10.65
SABAL-IHT
174.28 ± 3.27
KCenter
309.99 ± 0.81
KCenter
258.29 ± 1.75
Entropy
16.46 ± 0.29
Entropy
11.76 ± 0.03
Random
0.81 ± 0.02
Random
1.42 ± 0.02"
CONCLUSION,0.46153846153846156,"7
CONCLUSION"
CONCLUSION,0.46449704142011833,"We introduce the SABAL as a novel framework that formulates batch active learning as a sparse
approximation problem. It balances representation and uncertainty in a principled way, and has the
ﬂexibility to adapt to both Bayesian and non-Bayesian models. We realize the SABAL framework as
a ﬁnite-dimensional optimization problem, efﬁciently solvable by the proposed greedy or proximal
IHT algorithms. Numerical experiments demonstrate the strong performance of SABAL, comparable
to the state-of-the-art with lower time complexity. For the future works, although the hyperparameter
α offers a controllable trade-off between the variance and bias, it is still not well-understood how to
strike the best balance. An in-depth theoretical analysis of the SABAL optimizations, as well as other
instantiations of our general framework, would also have the potential to inspire discoveries of even
better batch active learning algorithms."
CONCLUSION,0.46745562130177515,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.47041420118343197,"8
ETHICS STATEMENT"
ETHICS STATEMENT,0.47337278106508873,"This work is proposing an active learning approach for more efﬁcient data acquisition and model
training, we do not expect any obvious ethical issue from this work."
REPRODUCIBILITY STATEMENT,0.47633136094674555,"9
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.47928994082840237,"We have included the code and instructions to reproduce our work in the supplementary material. We
also provide the experiment settings, training details and hyperparameters in the Appendix."
REFERENCES,0.4822485207100592,REFERENCES
REFERENCES,0.48520710059171596,"Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):
4311–4322, 2006."
REFERENCES,0.4881656804733728,"Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671,
2019."
REFERENCES,0.4911242603550296,"William H Beluch, Tim Genewein, Andreas N¨urnberger, and Jan M K¨ohler. The power of ensembles
for active learning in image classiﬁcation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 9368–9377, 2018."
REFERENCES,0.4940828402366864,"Erdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh. Batch active learning using determi-
nantal point processes. arXiv preprint arXiv:1906.07975, 2019."
REFERENCES,0.4970414201183432,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613–1622. PMLR, 2015."
REFERENCES,0.5,"Trevor Campbell and Tamara Broderick. Automated scalable bayesian inference via hilbert coresets.
The Journal of Machine Learning Research, 20(1):551–588, 2019."
REFERENCES,0.5029585798816568,"Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular
optimization. In International Conference on Machine Learning, pp. 160–168. PMLR, 2013."
REFERENCES,0.5059171597633136,"Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767–1781,
2011."
REFERENCES,0.5088757396449705,"David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289–1306,
2006."
REFERENCES,0.5118343195266272,"Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin
based approach. arXiv preprint arXiv:1802.09841, 2018."
REFERENCES,0.514792899408284,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Insights and applications.
In Deep Learning Workshop, ICML, volume 1, pp. 2, 2015."
REFERENCES,0.5177514792899408,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059.
PMLR, 2016."
REFERENCES,0.5207100591715976,"Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
arXiv preprint arXiv:1703.02910, 2017."
REFERENCES,0.5236686390532544,"Yonatan Geifman and Ran El-Yaniv.
Deep active learning over the long tail.
arXiv preprint
arXiv:1711.00941, 2017."
REFERENCES,0.5266272189349113,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.5295857988165681,"Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends® in
Machine Learning, 7(2-3):131–309, 2014."
REFERENCES,0.5325443786982249,Under review as a conference paper at ICLR 2022
REFERENCES,0.5355029585798816,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.5384615384615384,"Patrick Hemmer, Niklas K¨uhl, and Jakob Sch¨offer. Deal: Deep evidential active learning for image
classiﬁcation. arXiv preprint arXiv:2007.11344, 2020."
REFERENCES,0.5414201183431953,"Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Batch mode active learning and its
application to medical image classiﬁcation. In Proceedings of the 23rd international conference
on Machine learning, pp. 417–424, 2006."
REFERENCES,0.5443786982248521,"Neil Houlsby, Ferenc Husz´ar, Zoubin Ghahramani, and M´at´e Lengyel. Bayesian active learning for
classiﬁcation and preference learning. arXiv preprint arXiv:1112.5745, 2011."
REFERENCES,0.5473372781065089,"Jonathan H Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian logistic
regression. arXiv preprint arXiv:1605.06423, 2016."
REFERENCES,0.5502958579881657,"Angelos Katharopoulos and Franc¸ois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. In International conference on machine learning, pp. 2525–2534. PMLR,
2018."
REFERENCES,0.5532544378698225,"Rajiv Khanna and Anastasios Kyrillidis. Iht dies hard: Provable accelerated iterative hard thresholding.
In International Conference on Artiﬁcial Intelligence and Statistics, pp. 188–198. PMLR, 2018."
REFERENCES,0.5562130177514792,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.5591715976331361,"Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
zation trick. arXiv preprint arXiv:1506.02557, 2015."
REFERENCES,0.5621301775147929,"Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efﬁcient and diverse batch
acquisition for deep bayesian active learning. In Advances in Neural Information Processing
Systems, pp. 7026–7037, 2019."
REFERENCES,0.5650887573964497,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."
REFERENCES,0.5680473372781065,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.5710059171597633,"Yann LeCun et al. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet,
20(5):14, 2015."
REFERENCES,0.5739644970414202,"Xin Li and Yuhong Guo. Adaptive active learning for image classiﬁcation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 859–866, 2013."
REFERENCES,0.5769230769230769,"George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for
maximizing submodular set functions—i. Mathematical programming, 14(1):265–294, 1978."
REFERENCES,0.5798816568047337,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011."
REFERENCES,0.5828402366863905,"Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jos´e Miguel Hern´andez-Lobato. Bayesian batch
active learning as sparse subset approximation. In Advances in Neural Information Processing
Systems, pp. 6359–6370, 2019."
REFERENCES,0.5857988165680473,"Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017."
REFERENCES,0.5887573964497042,"Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009."
REFERENCES,0.591715976331361,"Maohao Shen, Jacky Y. Zhang, Leihao Chen, Weiman Yan, Neel Jani, Brad Sutton, and Oluwasanmi
Koyejo. Labeling cost sensitive batch active learning for brain tumor segmentation. In 2021
IEEE 18th International Symposium on Biomedical Imaging (ISBI), pp. 1269–1273, 2021. doi:
10.1109/ISBI48211.2021.9434098."
REFERENCES,0.5946745562130178,Under review as a conference paper at ICLR 2022
REFERENCES,0.5976331360946746,"Changjian Shui, Fan Zhou, Christian Gagn´e, and Boyu Wang. Deep active learning: Uniﬁed and
principled method for query and training. In International Conference on Artiﬁcial Intelligence
and Statistics, pp. 1308–1318. PMLR, 2020."
REFERENCES,0.6005917159763313,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.6035502958579881,"Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5972–5981, 2019."
REFERENCES,0.606508875739645,"Simon Tong and Daphne Koller. Support vector machine active learning with applications to text
classiﬁcation. Journal of machine learning research, 2(Nov):45–66, 2001."
REFERENCES,0.6094674556213018,"Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and
variational inference. Now Publishers Inc, 2008."
REFERENCES,0.6124260355029586,"Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International
joint conference on neural networks (IJCNN), pp. 112–119. IEEE, 2014."
REFERENCES,0.6153846153846154,"Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-effective active learning for
deep image classiﬁcation. IEEE Transactions on Circuits and Systems for Video Technology, 27
(12):2591–2600, 2016."
REFERENCES,0.6183431952662722,"Yazhou Yang and Marco Loog. Single shot active learning using pseudo annotators. Pattern
Recognition, 89:22–31, 2019."
REFERENCES,0.621301775147929,"Jacky Y. Zhang, Rajiv Khanna, Anastasios Kyrillidis, and Oluwasanmi Koyejo. Bayesian coresets:
Revisiting the nonconvex optimization perspective. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 2782–2790. PMLR, 2021."
REFERENCES,0.6242603550295858,Under review as a conference paper at ICLR 2022
REFERENCES,0.6272189349112426,"SABAL: Sparse Approximation-based
Batch Active Learning"
REFERENCES,0.6301775147928994,Appendix
REFERENCES,0.6331360946745562,"A
THE OVERALL PROCEDURE"
REFERENCES,0.636094674556213,"Algorithm 3: SABAL: Sparse Approximation-based Batch Active Learning
Input: Intial parameters θ, initial unlabeled pool Du, initial labeled pool Dl = ∅, initial number
of samples b0, query batch size b, number of iterations T."
REFERENCES,0.6390532544378699,"1 Query a random batch S0 of b0 data from Dl, update Du ←Du\S0 and Dl ←Dl ∪S0."
REFERENCES,0.6420118343195266,2 Train the model using S0.
REFERENCES,0.6449704142011834,"3 for t = 1, 2, . . . , T do"
REFERENCES,0.6479289940828402,"4
For each data xj ∈Du, estimate its label distribution ˜yj ∼P(xj)."
REFERENCES,0.650887573964497,"5
Compute vector gj by sampling or gradient embedding using 16."
REFERENCES,0.6538461538461539,"6
Compute v, Φ, and σj for each xj ∈Du and form equation 17."
REFERENCES,0.6568047337278107,"7
Find sparse weight w s.t. ∥w∥0 = b as speciﬁed in section 4."
REFERENCES,0.6597633136094675,"8
Select a batch of data St = {xj ∈Du | wj > 0} and query their labels."
REFERENCES,0.6627218934911243,"9
Update Du ←Du\St and Dl ←Dl ∪St."
REFERENCES,0.665680473372781,"10
Reinitialize and retrain the model using updated Dl, update model parameters θ"
END,0.6686390532544378,"11 end
Return: Final model parameters θ."
END,0.6715976331360947,"B
MORE RELATED WORK"
END,0.6745562130177515,"Active learning has been widely studied by the machine learning community. As most classic
approaches have already been discussed in a detail in Settles (2009); Dasgupta (2011); Hanneke et al.
(2014), we will brieﬂy review some recent works in deep active learning."
END,0.6775147928994083,"Existing query strategies can mainly be categorized as uncertainty-based and representation-based.
Uncertainty-based approaches look for data samples the model is mostly uncertain about. Meanwhile,
under the Bayesian setting, several recent works leverage the Bayesian neural network to well measure
the model uncertainty. Gal & Ghahramani (2015; 2016) proves the Monte-Carlo dropout (MC
Dropout) as an approximation of performing Bayesian inference, and enables efﬁcient uncertainty
estimations in neural networks. Gal et al. (2017) utilizes MC Dropout for approximating posterior
distributions and adapts Houlsby et al. (2011) as their uncertainty based acquisition function, and
similarly, Kirsch et al. (2019) proposes a batch-mode approach based on Gal et al. (2017) and shows
some improvements through a more accurate measurement of mutual information between the data
batch and model parameters. While MC Dropout becomes prevalent for uncertainty estimation,
Beluch et al. (2018) shows ensemble-based methods lead to better performance because of more
calibrated uncertainty estimation, and another recent work Hemmer et al. (2020) also proposes a new
uncertainty estimation method by replacing the softmax output of a neural network with the parameter
of Dirichlet density. Other non-Bayesian approaches sometimes combine uncertainty estimation with
other metrics: Li & Guo (2013) combines an information density measure to maximize the mutual
information between selected samples and remaining unlabeled samples under the Gaussian Process
setting. Wang et al. (2016) selects data based on several classic uncertainty metrics and incorporate a
cost-efﬁcient strategy by pseudo labeling the conﬁdent samples."
END,0.6804733727810651,"Representation-based approaches attempt to query diverse data samples that could best represent
the overall unlabeled dataset. A recent work proposed by Sener & Savarese (2017) deﬁnes the
active learning as a core-set selection problem. They derive an upper bound for the core-set loss
and construct representative batches by solving a k-Center problem in the feature space. In Geifman
& El-Yaniv (2017), the authors also explore the deep active learning with core-sets, but build the
core-sets in the farthest-ﬁrst compression scheme."
END,0.6834319526627219,Under review as a conference paper at ICLR 2022
END,0.6863905325443787,"C
PROOFS"
END,0.6893491124260355,"Proposition C.1. q : L(Θ, R) →R+ deﬁned in (5) is a shift-invariant seminorm satisfying the
following properties:"
END,0.6923076923076923,"1. q(L1 + L2) ≤q(L1) + q(L2) for ∀L1, L2 ∈L(Θ, R);
(triangle inequality)"
END,0.6952662721893491,"2. q(cL) = |c|q(L) for ∀L ∈L(Θ, R), ∀c ∈R;
(absolute homogeneity)"
END,0.6982248520710059,"3. q(L + c) = q(L) for ∀L ∈L(Θ, R), ∀c ∈R;
(shift-invariance)"
END,0.7011834319526628,4. q(L) = 0 if and only if L maps every θ ∈Θ to a constant.
END,0.7041420118343196,"In other words, q deﬁnes a norm in the space of shift-equivalence classes of loss functions."
END,0.7071005917159763,Proof. Recall that
END,0.7100591715976331,"q(L) := inf
c∈R ∥L + c∥†,
∀L ∈L(Θ, R)."
END,0.7130177514792899,We prove the four properties respectively in the following.
END,0.7159763313609467,"1. The triangle inequality is inherited from the sub-additivity of the norm ∥· ∥†. For ∀L ∈
L(Θ, R), we have"
END,0.7189349112426036,"q(L1 + L2) = inf
c∈R ∥L1 + L2 + c∥† =
inf
c1,c2∈R ∥L1 + L2 + c1 + c2∥†"
END,0.7218934911242604,"≤
inf
c1,c2∈R ∥L1 + c1∥† + ∥L2 + c2∥†"
END,0.7248520710059172,"= (inf
c∈R ∥L1 + c∥†) + (inf
c∈R ∥L2 + c∥†)"
END,0.727810650887574,= q(L1) + q(L2).
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7307692307692307,"2. The absolute homogeneity is also inherited from the absolute homogeneity of the norm
∥· ∥†. The case for c = 0 is obvious, and for c ̸= 0 we have"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7337278106508875,"q(cL) = |c|q(L) = inf
c1∈R ∥cL + c1∥† = inf
c1∈R |c| · ∥L + c1/c∥†"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7366863905325444,"= inf
c2∈R |c| · ∥L + c2∥† = |c|q(L)."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7396449704142012,"3. By the deﬁnition of q(·), we have the shift-invariance of q(·)."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.742603550295858,"4. The “if” part can be proved by deﬁnition, i.e., q(c) = infc1∈R ∥c1 + c∥† = ∥0∥† = 0."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7455621301775148,"For the “only if” part, we need to be more rigorous by deﬁning fc to be the function that
maps Θ to c ∈R. We further deﬁne F := {fc | c ∈R} ⊂L(Θ, R), and we can see
(F, ∥· ∥†) is a one-dimensional normed space. Letting L ∈L(Θ, R) and q(L) = 0, we have"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7485207100591716,"inf
fcϵ∈F ∥L + fc∥† = 0."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7514792899408284,"Therefore, for ∀ϵ > 0, ∃cϵ ∈R such that"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7544378698224852,"∥L + fcϵ∥† ≤ϵ
=⇒∥fcϵ∥† = ∥L + fcϵ −L∥† ≤ϵ + ∥L∥†."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.757396449704142,"That being said, for 0 < ϵ < 1, we have ∥fcϵ∥† ≤1 + ∥L∥†. Denote B = {fc ∈F |
∥fc∥† ≤1 + ∥L∥†}, and we can see B is a closed ball in F. As F is one-dimensional, by
Riesz’s lemma we have B compact."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7603550295857988,"As limϵ→0 ∥L + fcϵ∥† = 0, i.e., fcϵ →L, by the compactness of B we have L ∈B.
Therefore, L is also a constant function. Note that this conclusion does not require L(Θ, R)
to be complete."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7633136094674556,Under review as a conference paper at ICLR 2022
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7662721893491125,"Proposition C.2 (Proposition 3.1 Restated). As w ∈Rnu
+ and ∥w∥0 = b, by replacing the P by the
improved estimation distribution Pw (equation 9) into (i) in equation 7, we have"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7692307692307693,"EPw[q(˜L −EPw[˜L])] + EPw[q(˜Lw −EPw[˜Lw])] ≤
X"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.772189349112426,"xj∈Du
1(wj = 0) · σj,"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7751479289940828,"where σj :=
1
nu EP(xj)[q(ℓ(xj, ˜yj; ·) −EP(xj)[ℓ(xj, ˜yj; ·)])] is the individual variance, and 1(·)
is the indicator function."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7781065088757396,Proof. Recall that
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7810650887573964,˜Lw(θ) := 1 b X
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7840236686390533,"xj∈Du
wjℓ(xj, ˜yj; θ),
˜L(θ) := 1 nu X"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7869822485207101,"xj∈Du
ℓ(xj, ˜yj; θ),"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7899408284023669,˜yj ∼Pw(xj) :=
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7928994082840237,"(
P(xj)
if wj = 0
δy⋆
j
if wj > 0 ,
w ∈Rnu
+ ,"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7958579881656804,"where δy⋆
j denotes the distribution that ˜yj can only be y⋆
j . Therefore, by the deﬁnition of Pw, we
have"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.7988165680473372,"EPw(xj)

q
 
ℓ(xj, ˜yj; ·) −EPw(xj)[ℓ(xj, ˜yj; ·)]

= 0,
if wj > 0."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.8017751479289941,Plugging the above deﬁnitions into EPw[q(˜Lw −EPw[˜Lw])] we have
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.8047337278106509,EPw[q(˜Lw −EPw[˜Lw])] = EPw  q  1 nu X
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.8076923076923077,"xj∈Du
wj
 
ℓ(xj, ˜yj; ·) −EPw(xj)[ℓ(xj, ˜yj; ·)]

    = EPw  q  1 nu X"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.8106508875739645,"xj∈Du
1(wj > 0)wj
 
ℓ(xj, ˜yj; ·) −EPw(xj)[ℓ(xj, ˜yj; ·)]

   = 0. (18)"
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.8136094674556213,"Therefore, we only need to care about the EPw[q(˜L −EPw[˜L])]."
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.8165680473372781,EPw[q(˜L −EPw[˜L])] = EPw  q  1 nu X
THE ABSOLUTE HOMOGENEITY IS ALSO INHERITED FROM THE ABSOLUTE HOMOGENEITY OF THE NORM,0.8195266272189349,"xj∈Du
ℓ(xj, ˜yj; ·) −EPw(xj)[ℓ(xj, ˜yj; ·)]     ≤
X xj∈Du"
NU,0.8224852071005917,"1
nu
EPw(xj)

q
 
ℓ(xj, ˜yj; ·) −EPw(xj)[ℓ(xj, ˜yj; ·)]
 =
X"
NU,0.8254437869822485,"xj∈Du
1(wj = 0) 1"
NU,0.8284023668639053,"nu
EP

q
 
ℓ(xj, ˜yj; ·) −EP(xj)[ℓ(xj, ˜yj; ·)]
 =
X"
NU,0.8313609467455622,"xj∈Du
1(wj = 0) · σj,
(19)"
NU,0.834319526627219,"where the inequality is by the triangle inequality and the absolute homogeneity of q(·) (Proposi-
tion C.1). Combining equation 18 and equation 19, we have the proposition proved."
NU,0.8372781065088757,"D
OMITTED ALGORITHMS"
NU,0.8402366863905325,"In this section we present the two sub-procedures, i.e., line search and de-bias, shared by two main
optimization algorithms (Algorithm 1&2), as well as how the optimization (line 6) in Algorithm 2 is
solved optimally."
NU,0.8431952662721893,"The line search sub-procedure (Algorithm 4) optimally solve the problem of arg minµ∈R f1(w−µu),
i.e., given a direction u what is the best step size to move the w along u. The de-bias sub-procedure
(Algorithm 5) adjusts a sparse w in its own sparse support for a better solution."
NU,0.8461538461538461,Under review as a conference paper at ICLR 2022
NU,0.849112426035503,"Algorithm 4: LineSearch(u, w)
Input: direction u; starting point w.
Output: step size µ."
NU,0.8520710059171598,"1 µ ←⟨Φw−v,Φu⟩+β⟨w−1,u⟩"
NU,0.8550295857988166,"∥Φu∥2
2+β∥u∥2
2
(optimal µ)
Return: µ"
NU,0.8579881656804734,"Algorithm 5: De-bias(w)
Input: starting point w.
Output: improved w."
NU,0.8609467455621301,"1 u ←[∇f1(w)]supp(w)
(in-support grad.)"
NU,0.863905325443787,"2 µ ←LineSearch(u, w)"
NU,0.8668639053254438,"3 w ←w −µu
(in-support adjustment)
Return: w"
NU,0.8698224852071006,Recall the inner optimization (line 6) of Algorithm 2 is
NU,0.8727810650887574,"w ←
arg min
w∈Rnu
+ ,∥w∥0≤b"
NU,0.8757396449704142,"1
2∥w −s∥2
2 + f2(w)."
NU,0.878698224852071,Noting that 1
NU,0.8816568047337278,"2∥w −s∥2
2 +f2(w) = P"
NU,0.8846153846153846,j∈[nu]( 1
NU,0.8875739644970414,"2(wj −sj)2 −ασ2
j ), this step can be done optimally by
simply picking the top-b elements, as shown in the following. Given a b-sparse support set S ⊂[nu],
we can see that"
NU,0.8905325443786982,"min
w∈Rnu
+ ,supp(w)⊆S
P"
NU,0.893491124260355,j∈[nu]( 1
NU,0.8964497041420119,"2(wj −sj)2 −ασ2
j ) = P"
NU,0.8994082840236687,j∈S( 1
NU,0.9023668639053254,"2[−sj]2
+ −ασ2
j )."
NU,0.9053254437869822,"Therefore, line 6 in Algorithm 2 can be done by: (1) ﬁnd the b smallest ( 1"
NU,0.908284023668639,"2[−sj]2
+ −ασ2
j ), denoting
the resulting b-sparse index set as S⋆; (2) let w ←[[s]S⋆]+."
NU,0.9112426035502958,"E
MORE EXPERIMENT RESULTS"
NU,0.9142011834319527,"E.1
ABLATION STUDY: TRADE-OFF OF UNCERTAINTY AND REPRESENTATION"
NU,0.9171597633136095,"Figure 3: Ablation Study Re-
sults on Bayesian models."
NU,0.9201183431952663,"We perform an ablation study to understand better the trade-off
between the variance and the bias terms in our ﬁnal formula-
tion equation 15. To remove the bias term, we query the data with
top variances. To remove the variance term, we query the data
by only minimizing the approximation bias, i.e., setting α = 0,
under both IHT and Greedy optimizations respectively. We take
two datasets MNIST and CIFAR-10 in the Bayesian experiment as
examples. Results in Figure 3 demonstrate the necessity of taking
both uncertainty and representation into consideration during the
data acquisitions for ideal performance, while for some datasets
like CIFAR-10, the variance contributes much more signiﬁcantly."
NU,0.9230769230769231,"F
IMPLEMENTATION DETAILS"
NU,0.9260355029585798,"All experiments are written in PyTorch 1.8.1. All hyper-parameters
are chosen to ensure models achieve good and stable performance
on each dataset, and they are kept identical for all active learning
approaches."
NU,0.9289940828402367,"F.1
BAYESIAN ACTIVE LEARNING EXPERIMENT"
NU,0.9319526627218935,"Model Architecture
We use the exact same model as (Pinsler
et al., 2019), it is a Bayesian neural network consisting of a ResNet-18 (He et al., 2016) feature
extractor followed by a fully connected layer with a ReLU activation, and a ﬁnal layer allows sampling
by local reparametrization (Kingma et al., 2015) with a softmax activation."
NU,0.9349112426035503,"Optimization and Hyperparameter Selection
Due to larger models and more complicated classi-
ﬁcation tasks, e.g., CIFAR-100, data augmentation(including random cropping and random horizontal
ﬂipping) and learning rate scheduler are used in this experiment to achieve good model performance.
The model is optimized with the Adam (Kingma & Ba, 2014) optimizer using default exponential
decay rates (0.9, 0.999) for the moment estimates. Table 4 shows the hyper-parameters in experiment"
NU,0.9378698224852071,Under review as a conference paper at ICLR 2022
NU,0.9408284023668639,"on Bayesian batch active learning, where bs denotes the batch size in dataloader during the model
training, lr denotes the learning rate, and wd denotes the weight decay. The hyper-parameters are
chosen through grid search."
NU,0.9437869822485208,Table 4: Hyperparameters used in Bayesian active learning experiment
NU,0.9467455621301775,"Dataset
Method
Epoch
bs
α
β
lr
wd"
NU,0.9497041420118343,"Fashion MNIST
SABAL-IHT
200
256
1
10−3
0.001
5 × 10−4"
NU,0.9526627218934911,"Fashion MNIST
SABAL-Greedy
200
256
2
0.5
0.001
5 × 10−4"
NU,0.9556213017751479,"CIFAR-10
SABAL-IHT
200
256
1
10−6
0.001
5 × 10−4"
NU,0.9585798816568047,"CIFAR-10
SABAL-Greedy
200
256
2
1
0.001
5 × 10−4"
NU,0.9615384615384616,"CIFAR-100
SABAL-IHT
200
256
1
10−6
0.001
5 × 10−4"
NU,0.9644970414201184,"CIFAR-100
SABAL-Greedy
200
256
1
0.5
0.001
5 × 10−4"
NU,0.9674556213017751,"F.2
GENERAL ACTIVE LEARNING EXPERIMENT"
NU,0.9704142011834319,"Model Architecture
On MNIST dataset, we use LeNet-5 model (LeCun et al., 2015). On SVHN
and CIFAR10 datasets, we use VGG-16 model (Simonyan & Zisserman, 2014)."
NU,0.9733727810650887,"Optimization and Hyperparameter Selection
All models are trained using the cross entropy loss
with SGD optimizer, and no data augmentation or learning rate scheduler is used. Tabel 5 shows the
hyper-parameters in experiment on general batch active learning, where bs denotes the batch size in
dataloader during the model training, lr denotes the learning rate, m denotes the momentum, and wd
denotes the weight decay. The hyper-parameters are chosen through grid search."
NU,0.9763313609467456,Table 5: Hyperparameters used in general active learning experiment
NU,0.9792899408284024,"Dataset
Method
Epoch
bs
α
β
lr
wd"
NU,0.9822485207100592,"MNIST
SABAL-IHT
150
32
10−8
10−4
0.01
0.9
5 × 10−4"
NU,0.985207100591716,"MNIST
SABAL-Greedy
150
32
10−8
10−1
0.01
0.9
5 × 10−4"
NU,0.9881656804733728,"SVHN
SABAL-IHT
150
128
10−8
10−4
0.01
0.9
5 × 10−4"
NU,0.9911242603550295,"SVHN
SABAL-Greedy
150
128
10−1
10−1
0.01
0.9
5 × 10−4"
NU,0.9940828402366864,"CIFAR-10
SABAL-IHT
100
128
10−8
10−6
0.001
0.9
5 × 10−4"
NU,0.9970414201183432,"CIFAR-10
SABAL-Greedy
100
128
10−2
10−1
0.001
0.9
5 × 10−4"
