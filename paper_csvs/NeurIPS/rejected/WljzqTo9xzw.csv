Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00199203187250996,"The computation of Wasserstein gradient direction is essential for posterior sam-
1"
ABSTRACT,0.00398406374501992,"pling problems and scientiﬁc computing. The approximation of the Wasserstein
2"
ABSTRACT,0.00597609561752988,"gradient with ﬁnite samples requires solving a variational problem. We study the
3"
ABSTRACT,0.00796812749003984,"variational problem in the family of two-layer networks with squared-ReLU activa-
4"
ABSTRACT,0.0099601593625498,"tions, towards which we derive a semi-deﬁnite programming (SDP) relaxation. This
5"
ABSTRACT,0.01195219123505976,"SDP can be viewed as an approximation of the Wasserstein gradient in a broader
6"
ABSTRACT,0.013944223107569721,"function family including two-layer networks. By solving the convex SDP, we ob-
7"
ABSTRACT,0.01593625498007968,"tain the optimal approximation of the Wasserstein gradient direction in this class of
8"
ABSTRACT,0.017928286852589643,"functions. Numerical experiments including PDE-constrained Bayesian inference
9"
ABSTRACT,0.0199203187250996,"and parameter estimation in COVID-19 modeling demonstrate the effectiveness of
10"
ABSTRACT,0.021912350597609563,"the proposed method.
11"
INTRODUCTION,0.02390438247011952,"1
Introduction
12"
INTRODUCTION,0.025896414342629483,"Bayesian inference plays an essential role in learning model parameters from the observational
13"
INTRODUCTION,0.027888446215139442,"data with applications in inverse problems, scientiﬁc computing, information science, and machine
14"
INTRODUCTION,0.029880478087649404,"learning (Stuart, 2010). The central problem in Bayesian inference is to draw samples from a posterior
15"
INTRODUCTION,0.03187250996015936,"distribution, which characterizes the parameter distribution given data and a prior distribution.
16"
INTRODUCTION,0.03386454183266932,"The Wasserstein gradient ﬂow (Otto, 2001; Ambrosio et al., 2005; Junge et al., 2017) has shown to be
17"
INTRODUCTION,0.035856573705179286,"effective in drawing samples from a posterior distribution, which attracts increasing attention in recent
18"
INTRODUCTION,0.037848605577689244,"years. For instance, the Wasserstein gradient ﬂow of Kullback-Leibler (KL) divergence connects to
19"
INTRODUCTION,0.0398406374501992,"the overdampled Langevin dynamics. The time-discretization of the overdamped Langevin dynamics
20"
INTRODUCTION,0.04183266932270916,"renders the classical Langevin Monte Carlo Markov Chain (MCMC) algorithm. In this sense, the
21"
INTRODUCTION,0.043824701195219126,"computation of Wasserstein gradient ﬂow yields a different viewpoint for sampling algorithms. In
22"
INTRODUCTION,0.045816733067729085,"particular, the Wasserstein gradient direction also provides a deterministic update of the particle
23"
INTRODUCTION,0.04780876494023904,"system (Carrillo et al., 2021b). Based on the approximation or generalization of the Wasserstein
24"
INTRODUCTION,0.049800796812749,"gradient direction, many efﬁcient sampling algorithms have been developed, including Wasserstein
25"
INTRODUCTION,0.05179282868525897,"gradient descent (WGD) with kernel density estimation (KDE) (Liu et al., 2019), Stein variational
26"
INTRODUCTION,0.053784860557768925,"gradient descent (SVGD) (Liu & Wang, 2016), and neural variational gradient descent (di Langosco
27"
INTRODUCTION,0.055776892430278883,"et al., 2021), etc.
28"
INTRODUCTION,0.05776892430278884,"Meanwhile, neural networks exhibit tremendous optimization and generalization performance in
29"
INTRODUCTION,0.05976095617529881,"learning complicated functions from data. They also have wide applications in Bayesian inverse
30"
INTRODUCTION,0.061752988047808766,"problems (Rezende & Mohamed, 2015; Onken et al., 2020; Kruse et al., 2019; Lan et al., 2021).
31"
INTRODUCTION,0.06374501992031872,"According to the universal approximation theorem of neural networks (Hornik et al., 1989; Lu et al.,
32"
INTRODUCTION,0.06573705179282868,"2017), any arbitrarily complicated functions can be learned by a two-layer neural network with
33"
INTRODUCTION,0.06772908366533864,"non-linear activations and a sufﬁcient number of neurons. Functions represented by neural networks
34"
INTRODUCTION,0.0697211155378486,"naturally provide an approximation towards the Wasserstein gradient direction.
35"
INTRODUCTION,0.07171314741035857,"However, due to the nonlinear and nonconvex structure of neural networks, optimization algorithms
36"
INTRODUCTION,0.07370517928286853,"including stochastic gradient descent may not ﬁnd the global optima of the training problem. Recently,
37"
INTRODUCTION,0.07569721115537849,"based on a line of works (Pilanci & Ergen, 2020; Sahiner et al., 2020; Bartan & Pilanci, 2021), the
38"
INTRODUCTION,0.07768924302788845,"regularized training problem of two-layer neural networks with ReLU/polynomial activation can
39"
INTRODUCTION,0.0796812749003984,"be formulated as a convex program. The optimal solution of the convex program renders a global
40"
INTRODUCTION,0.08167330677290836,"optimum of the nonconvex training problem.
41"
INTRODUCTION,0.08366533864541832,"In this paper, we study a variational problem, whose optimal solution corresponds to the Wasserstein
42"
INTRODUCTION,0.08565737051792828,"gradient direction. Focusing on the family of two-layer neural networks with squared ReLU activation,
43"
INTRODUCTION,0.08764940239043825,"we formulate the regularized variational problem in terms of samples. Directly training the neural
44"
INTRODUCTION,0.08964143426294821,"network to minimize the loss may get the neural network stuck at local minima or saddle points and
45"
INTRODUCTION,0.09163346613545817,"it often leads to biased sample distribution from the posterior. Instead, we analyze the convex dual
46"
INTRODUCTION,0.09362549800796813,"problem of the training problem and study its semi-deﬁnite program (SDP) relaxation by analyzing
47"
INTRODUCTION,0.09561752988047809,"the geometry of dual constraints. The resulting SDP is practically solvable and it can be efﬁciently
48"
INTRODUCTION,0.09760956175298804,"optimized by convex optimization solvers such as CVXPY (Diamond & Boyd, 2016). We then derive
49"
INTRODUCTION,0.099601593625498,"the corresponding relaxed bidual problem (dual of the relaxed dual problem). Thus, the optimal
50"
INTRODUCTION,0.10159362549800798,"solution to the dual problem yields an optimal approximation of the Wasserstein gradient direction
51"
INTRODUCTION,0.10358565737051793,"in a broader function family. We also present a practical implementation and analyze the choice of
52"
INTRODUCTION,0.10557768924302789,"the regularization parameter. Numerical results including PDE-constrained inference problems and
53"
INTRODUCTION,0.10756972111553785,"Covid-19 parameter estimation problems illustrate the effectiveness of our proposed method.
54"
RELATED WORKS,0.10956175298804781,"1.1
Related works
55"
RELATED WORKS,0.11155378486055777,"The time and spatial discretizations of Wasserstein gradient ﬂows are extensively studied in literature
56"
RELATED WORKS,0.11354581673306773,"(Jordan et al., 1998; Junge et al., 2017; Carrillo et al., 2021a,b; Bonet et al., 2021; Liutkus et al., 2019;
57"
RELATED WORKS,0.11553784860557768,"Frogner & Poggio, 2020). Recently, neural networks have been applied in solving or approximating
58"
RELATED WORKS,0.11752988047808766,"Wasserstein gradient ﬂows (Mokrov et al., 2021; Lin et al., 2021b,a; Alvarez-Melis et al., 2021;
59"
RELATED WORKS,0.11952191235059761,"Bunne et al., 2021; Hwang et al., 2021; Fan et al., 2021). For sampling algorithms, di Langosco
60"
RELATED WORKS,0.12151394422310757,"et al. (2021) learns the transportation function by solving an unregularized variational problem in the
61"
RELATED WORKS,0.12350597609561753,"family of vector-output deep neural networks. Compared to these studies, we focus on a convex SDP
62"
RELATED WORKS,0.1254980079681275,"relaxation of the varitional problem induced by the Wasserstein gradient direction. Meanwhile, Feng
63"
RELATED WORKS,0.12749003984063745,"et al. (2021) form the Wasserstein gradient direction as the mininimizer the Bregman score and they
64"
RELATED WORKS,0.1294820717131474,"apply deep neural networks to solve the induced variational problem.
65"
BACKGROUND,0.13147410358565736,"2
Background
66"
BACKGROUND,0.13346613545816732,"In this section, we brieﬂy review the Wasserstein gradient descent and present its variational for-
67"
BACKGROUND,0.13545816733067728,"mulation. In particular, we focus on the Wasserstein gradient descent direction of KL divergence
68"
BACKGROUND,0.13745019920318724,"functional. Later on, we design a neural network convex optimization problems to approximate the
69"
BACKGROUND,0.1394422310756972,"Wasserstein gradient in samples.
70"
WASSERSTEIN GRADIENT DESCENT,0.14143426294820718,"2.1
Wasserstein gradient descent
71"
WASSERSTEIN GRADIENT DESCENT,0.14342629482071714,"Consider an optimization problem in the probability space:
72"
WASSERSTEIN GRADIENT DESCENT,0.1454183266932271,"inf
ρ∈P DKL(ρ∥π) =
Z
ρ(x)(log ρ(x) −log π(x))dx,
(1)"
WASSERSTEIN GRADIENT DESCENT,0.14741035856573706,"Here the integral is taken over Rd and the objective functional DKL(ρ∥π) is the KL divergence from
73"
WASSERSTEIN GRADIENT DESCENT,0.14940239043824702,"ρ to π. The variable is the density function ρ in the space P = {ρ ∈C∞(Rd)|
R
ρdx = 1, ρ > 0}.
74"
WASSERSTEIN GRADIENT DESCENT,0.15139442231075698,"The function π ∈C∞(Rd) is a known probability density function of the posterior distribution. By
75"
WASSERSTEIN GRADIENT DESCENT,0.15338645418326693,"solving the optimization problem (1), we can generate samples from the posterior distribution.
76"
WASSERSTEIN GRADIENT DESCENT,0.1553784860557769,"A known fact (Villani, 2003, Chapter 8.3.1) is that the Wasserstein gradient descent ﬂow for the
77"
WASSERSTEIN GRADIENT DESCENT,0.15737051792828685,"optimization problem (1) satisﬁes
78"
WASSERSTEIN GRADIENT DESCENT,0.1593625498007968,"∂tρt =∇·

ρt∇δ"
WASSERSTEIN GRADIENT DESCENT,0.16135458167330677,"δρt
DKL(ρt∥π)
"
WASSERSTEIN GRADIENT DESCENT,0.16334661354581673,"=∇· (ρt(∇log ρt −∇log π))
=∆ρt −∇· (ρt∇log π),"
WASSERSTEIN GRADIENT DESCENT,0.16533864541832669,"where ρt(x) = ρ(x, t) and
δ
δρt is the L2 ﬁrst variation operator w.r.t. ρt. In the above third equality,
79"
WASSERSTEIN GRADIENT DESCENT,0.16733067729083664,"a fact ρt∇log ρt = ∇ρt is used. Here ∇· F denotes the divergence of a vector valued function
80"
WASSERSTEIN GRADIENT DESCENT,0.1693227091633466,"F : Rd →Rd and ∆is the Laplace operator. This equation is also known as the gradient drift
81"
WASSERSTEIN GRADIENT DESCENT,0.17131474103585656,"Fokker-Planck equation. It corresponds to the following updates in terms of samples:
82"
WASSERSTEIN GRADIENT DESCENT,0.17330677290836655,"dxt = −(∇log ρt(xt) −∇log π(xt))dt,
(2)"
WASSERSTEIN GRADIENT DESCENT,0.1752988047808765,"where xt follows the distribution of ρt. Clearly, when ρt = π, the above dynamics reach the
83"
WASSERSTEIN GRADIENT DESCENT,0.17729083665338646,"equilibrium, which implies that the samples xt are generated by the posterior distribution.
84"
WASSERSTEIN GRADIENT DESCENT,0.17928286852589642,"To solve the Wasserstein gradient ﬂow (2), we consider a forward Eulerian discretization in time.
85"
WASSERSTEIN GRADIENT DESCENT,0.18127490039840638,"In the l-th iteration, suppose that {xn
l } are samples drawn from ρl. The update rule of Wasserstein
86"
WASSERSTEIN GRADIENT DESCENT,0.18326693227091634,"gradient descent (WGD) on the particle system {xn
l } follows
87"
WASSERSTEIN GRADIENT DESCENT,0.1852589641434263,"xn
l+1 = xn
l −αl∇Φl(xn
l ),
(3)"
WASSERSTEIN GRADIENT DESCENT,0.18725099601593626,"where Φl : Rd →R is a function which approximates log ρl −log π and αl > 0 is the step size.
88"
VARIATIONAL FORMULATION OF WGD,0.1892430278884462,"2.2
Variational formulation of WGD
89"
VARIATIONAL FORMULATION OF WGD,0.19123505976095617,"Given the particles {xn}N
n=1, we design the following variational problem to choose a suitable
90"
VARIATIONAL FORMULATION OF WGD,0.19322709163346613,"function Φ approximating the function log ρ −log π. Consider
91"
VARIATIONAL FORMULATION OF WGD,0.1952191235059761,"inf
Φ∈C1(Rd)
1
2"
VARIATIONAL FORMULATION OF WGD,0.19721115537848605,"Z
∥∇Φ(x −(∇log ρ(x) −∇log π(x))∥2
2ρ(x)dx.
(4)"
VARIATIONAL FORMULATION OF WGD,0.199203187250996,"The objective functional evaluates the least-square discrepancy between ∇log ρ −∇log π and ∇Φ
92"
VARIATIONAL FORMULATION OF WGD,0.20119521912350596,"weighted by the density ρ. The optimal solution follows Φ = log ρ −log π, up to a constant shift. Let
93"
VARIATIONAL FORMULATION OF WGD,0.20318725099601595,"H ⊆C1(Rd) be a ﬁnite dimensional function space. The following proposition gives a formulation
94"
VARIATIONAL FORMULATION OF WGD,0.2051792828685259,"of (4) in H.
95"
VARIATIONAL FORMULATION OF WGD,0.20717131474103587,"Proposition 1 Let H ⊆C1(Rd) be a function space. The variational problem (4) in the domain H
96"
VARIATIONAL FORMULATION OF WGD,0.20916334661354583,"is equivalent to
97"
VARIATIONAL FORMULATION OF WGD,0.21115537848605578,"inf
Φ∈H
1
2"
VARIATIONAL FORMULATION OF WGD,0.21314741035856574,"Z
∥∇Φ(x)∥2
2ρdx +
Z
∆Φ(x)ρ(x)dx +
Z
⟨∇log π(x), ∇Φ(x)⟩ρ(x)dx.
(5)"
VARIATIONAL FORMULATION OF WGD,0.2151394422310757,"Remark 1 A similar variational problem has been studied in (di Langosco et al., 2021). If we replace
98"
VARIATIONAL FORMULATION OF WGD,0.21713147410358566,"∇Φ for Φ ∈H by a vector ﬁeld Ψ in certain function family, then, the quantity in (5) is the negative
99"
VARIATIONAL FORMULATION OF WGD,0.21912350597609562,"regularized Stein discrepancy deﬁned in (di Langosco et al., 2021) between ρ and π based on Ψ. This
100"
VARIATIONAL FORMULATION OF WGD,0.22111553784860558,"problem is also similar to the varitional problem for the score matching estimator in (Hyvärinen &
101"
VARIATIONAL FORMULATION OF WGD,0.22310756972111553,"Dayan, 2005) by parameterizing Φ in a given probabilistic model. In comparison, our method can be
102"
VARIATIONAL FORMULATION OF WGD,0.2250996015936255,"viewed as a special case of score matching by using a two-layer neural network model.
103"
VARIATIONAL FORMULATION OF WGD,0.22709163346613545,"Therefore, by replacing the density ρ by ﬁnite samples {xn}N
n=1 ∼ρ, the problem (5) in terms of
104"
VARIATIONAL FORMULATION OF WGD,0.2290836653386454,"ﬁnite samples forms
105"
VARIATIONAL FORMULATION OF WGD,0.23107569721115537,"inf
Φ∈H
1
N N
X n=1 1"
VARIATIONAL FORMULATION OF WGD,0.23306772908366533,"2∥∇Φ(xn)∥2
2 + ∆Φ(xn)

+ 1 N N
X"
VARIATIONAL FORMULATION OF WGD,0.2350597609561753,"n=1
⟨∇log π(xn), ∇Φ(xn)⟩.
(6)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.23705179282868527,"3
Optimal neural network approximation of Wasserstein gradient
106"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.23904382470119523,"In this section, we focus on functional space H of functions represented by two-layer neural networks.
107"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2410358565737052,"We derive the primal and dual problem of the regularized Wasserstein variational problems. By
108"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.24302788844621515,"analyzing the dual constraints, a convex SDP relaxation of the dual problem is obtained. We also
109"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2450199203187251,"present a practical implementation estimation of ∇log ρ −∇log π and discuss the choice of the
110"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.24701195219123506,"regularization parameter.
111"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.24900398406374502,"Let ψ be an activation function. Consider the case where H is a class of two-layer neural network
112"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.250996015936255,"with the activation function ψ(x):
113"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.25298804780876494,"H =

Φθ ∈C1(Rd)|Φθ(x) = αT ψ(W T x)
	
,
(7)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2549800796812749,"where θ = (W, α) is the parameter in the neural network with W ∈Rd×m and α ∈Rm.
114"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.25697211155378485,"Remark 2 We can extend this model to handle the bias term by add an entry of 1 in x1, . . . , xn.
115"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2589641434262948,"For two-layer neural networks, we can compute the gradient and Laplacian of Φ ∈H as follows:
116"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.26095617529880477,"∇Φθ(x) = m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.26294820717131473,"i=1
αiwiψ′(wT
i x) = W(ψ′(W T x) ◦α),
(8) 117"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2649402390438247,"∆Φθ(x) = m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.26693227091633465,"i=1
αi∥wi∥2
2ψ′′(wT
i x).
(9)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2689243027888446,"Here ◦represents the element-wise multiplication. By adding a regularization term to the variational
118"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.27091633466135456,"problem (6), we obtain
119"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2729083665338645,"min
θ
1
2N N
X n=1  m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2749003984063745,"i=1
αiwiψ′(wT
i xn)  2 2
+ 1 N N
X n=1 * m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.27689243027888444,"i=1
αiwiψ′(wT
i xn), ∇log π(xn) + + 1 N N
X n=1 m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2788844621513944,"i=1
αi∥wi∥2
2ψ′′(wT
i xn) + β"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.28087649402390436,"2 R(θ), (10)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.28286852589641437,"where β > 0 is the regularization parameter. We focus on the squared ReLU activation ψ(z) =
120"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2848605577689243,"(z)2
+ = (max{z, 0})2. Note that a non-vanishing second derivative is required for the Laplacian term
121"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2868525896414343,"in (9), which makes the ReLU activation inadequate. For this activation function, we consider the
122"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.28884462151394424,"regularization function R(θ) = Pm
i=1(∥wi∥3
2 + |αi|3).
123"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2908366533864542,"Remark 3 We note that ∇Φθ(x) and ∆Φθ(x) are all piece-wise degree-3 polynomials of the
124"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.29282868525896416,"parameters θ. Hence, we consider a speciﬁc cubic regularization term above, analogous to (Bartan &
125"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2948207171314741,"Pilanci, 2021). By choosing this regularization term, we can derive a simpliﬁed convex dual problem.
126"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.2968127490039841,"By rescaling the ﬁrst and second-layer parameters, the regularized variational problem (10) can be
127"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.29880478087649404,"formulated as follows.
128"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.300796812749004,"Proposition 2 (Primal problem) The regularized variational problem (10) is equivalent to
129"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.30278884462151395,"min
W,α
1
2 N
X n=1  m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3047808764940239,"i=1
αiwiψ′(wT
i xn)  2 + N
X n=1 m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.30677290836653387,"i=1
αi∥wi∥2
2ψ′′(wT
i xn) + N
X n=1 * m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.30876494023904383,"i=1
αiwiψ′(wT
i xn), ∇log π(xn) +"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3107569721115538,"+ ˜β∥α∥1,"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.31274900398406374,"s.t. ∥wi∥2 ≤1, i ∈[m], (11)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3147410358565737,"where ˜β = 3 · 2−5/3Nβ.
130"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.31673306772908366,"For simplicity, we write Y =  "
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3187250996015936,"∇log π(x1)T
...
∇log π(xN)T "
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3207171314741036,"∈RN×d. We introduce the slack variable zn =
131"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.32270916334661354,"Pm
i=1 αiwiψ′(xT
nwi) for n ∈[N] and denote Z = [z1
. . .
zN]T ∈RN×d. Then, we can simplify
132"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3247011952191235,"the problem (11) to
133"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.32669322709163345,"min
W,α,Z
1
2∥Z∥2
F + N
X n=1 m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3286852589641434,"i=1
αi∥wi∥2
2ψ′′(wT
i xn) + tr(Y T Z) + ˜β∥α∥1,"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.33067729083665337,"s.t. zn = m
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.33266932270916333,"i=1
αiwiψ′(xT
nwi), n ∈[N], ∥wi∥2 ≤1, i ∈[m]. (12)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3346613545816733,"Based on the above reformulation, we can derive the dual problem of (12) as follows.
134"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.33665338645418325,"Proposition 3 (Dual problem) The dual problem of the regularized variational problem (12) is
135"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3386454183266932,"max
Λ∈RN×d −1"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.34063745019920316,"2∥Λ + Y ∥2
F , s.t.
max
w:∥w∥2≤1  N
X"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3426294820717131,"n=1
∥w∥2
2ψ′′(xT
nw) −λT
nwψ′(xT
nw)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.34462151394422313,"≤˜β,
(13)"
OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT,0.3466135458167331,"which provides a lower-bound on (12).
136"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.34860557768924305,"3.1
Analysis of dual constraints and the relaxed dual problem
137"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.350597609561753,"Now, we analyze the constraint"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.35258964143426297,"max
w:∥w∥2≤1  N
X"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.3545816733067729,"n=1
∥w∥2
2ψ′′(wT xn) −λT
nwψ′(xT
nw) ≤˜β"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.3565737051792829,"in the dual problem. We note that this constraint is closely related to the regularization parameter,
138"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.35856573705179284,"which we will discuss later. For simplicity, we take ψ′′(0) = 0 as the subgradient of ψ′(z) at z = 0,
139"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.3605577689243028,"i.e., taking the left derivative of ψ′(z) at z = 0. Let X = [x1, . . . , xN]T ∈RN×d. Denote the set of
140"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.36254980079681276,"all possible hyper-plane arrangements corresponding to the rows of X as
141"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.3645418326693227,"S = {D = diag(I(Xw ≥0))|w ∈Rd, w ̸= 0}.
(14)"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.3665338645418327,"Here I(s) = 1 if the statement s is correct and I(s) = 0 otherwise. Let p = |S| be the cardinality
142"
ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM,0.36852589641434264,"of S, and write S = {D1, . . . , Dp}. According to (Cover, 1965), we have the upper bound p ≤
143"
R,0.3705179282868526,"2r

e(N−1)"
R,0.37250996015936255,"r
r
, where r = rank(X).
144"
R,0.3745019920318725,"Based on the analysis of the dual constraints, we can derive a convex SDP as a relaxed dual problem.
145"
R,0.37649402390438247,"It gives a lower bound for the optimal value of the dual problem (13).
146"
R,0.3784860557768924,"Proposition 4 (Relaxed Dual problem) Consider the following SDP:
147"
R,0.3804780876494024,max −1
R,0.38247011952191234,"2∥Λ + Y ∥2
F ,"
R,0.3844621513944223,"s.t. ˜Aj(Λ) + ˜Bj + N
X"
R,0.38645418326693226,"n=0
r(j,−)
n
H(j)
n
+ ˜βed+1eT
d+1 ⪰0,"
R,0.3884462151394422,"−˜Aj(Λ) −˜Bj + N
X"
R,0.3904382470119522,"n=0
r(j,+)
n
H(j)
n
+ ˜βed+1eT
d+1 ⪰0,"
R,0.39243027888446214,"r(j,−) ≥0, r(j,+) ≥0, j ∈[p]. (15)"
R,0.3944223107569721,"The variables are Λ ∈RN×d and r(j,−), r(j,+) ∈Rn+1 for j ∈[p]. For j ∈[p], we denote
148"
R,0.39641434262948205,"Aj(Λ) = −ΛT DjX −XT DjΛ, Bj = 2 tr(Dj)Id, ˜Aj(Λ) =

Aj(Λ)
0
0
0"
R,0.398406374501992,"
, ˜Bj =

Bj
0
0
0"
R,0.40039840637450197,"
,
149"
R,0.40239043824701193,"H(j)
0
=

Id
0
0
−1"
R,0.4043824701195219,"
and H(j)
n
=

0
(1 −2(Dj)nn)xn
(1 −2(Dj)nn)xT
n
0"
R,0.4063745019920319,"
, n ∈[N] The vector
150"
R,0.40836653386454186,"ed+1 ∈Rd+1 satisﬁes that (ed+1)i = 0 for i ∈[d] and (ed+1)d+1 = 1.
151"
R,0.4103585657370518,"The optimal value of (15) gives a lower bound on the dual problem (13), and hence on the primal
152"
R,0.4123505976095618,"problem (12).
153"
R,0.41434262948207173,"In the following proposition, we derive the relaxed bi-dual problem. It can be viewed as a convex
154"
R,0.4163346613545817,"relaxation of the primal problem (12).
155"
R,0.41832669322709165,"Proposition 5 (Relaxed bi-dual problem) The dual of the relaxed dual problem (15) is as follows
156 min 1"
R,0.4203187250996016,"2∥Z + Y ∥2
F −1"
R,0.42231075697211157,"2∥Y ∥2
F + p
X"
R,0.4243027888446215,"j=1
tr( ˜Bj(S(j,+) −S(j,−))) + ˜β p
X"
R,0.4262948207171315,"j=1
tr

(S(j,+) + S(j,−))ed+1eT
d+1

,"
R,0.42828685258964144,"s.t. Z = p
X"
R,0.4302788844621514,"j=1
˜A∗
j(S(j,−) −S(j,+)), tr(S(j,−)H(j)
n ) ≤0, tr(S(j,+)H(j)
n ) ≤0, n = 0, . . . , N, j ∈[p], (16)"
R,0.43227091633466136,"in variables Z ∈RN×d, S(j,+), S(j,−) ∈Sd+1
+
for j ∈[p]. Here A∗
j is the adjoint operator of the
157"
R,0.4342629482071713,"linear operator Aj.
158"
R,0.4362549800796813,"As (15) is a convex problem and the Slater’s condition is satisﬁed, the optimal values of (15) and
159"
R,0.43824701195219123,"(16) are same. We can show that any feasible solutions of the primal problem (11) can be mapped to
160"
R,0.4402390438247012,"feasible solutions of (16).
161"
R,0.44223107569721115,"Theorem 1 Suppose that (Z, W, α) is feasible to the primal problem (12). Then, there exist matrices
162"
R,0.4442231075697211,"{S(j,+), S(j,−)}p
j=1 constructed from (W, α) such that (Z, {S(j,+), S(j,−)}p
j=1) is feasible to the
163"
R,0.44621513944223107,"relaxed bi-dual problem (16). Moreover, the objective value of the relaxed bi-dual problem (16) at
164"
R,0.448207171314741,"(Z, {S(j,+), S(j,−)}p
j=1) is the same as objective value of the primal problem (12) at (Z, W, α).
165"
R,0.450199203187251,"Let J(Z, {S(j,+), S(j,−)}p
j=1) denote the objective value of the relaxed bi-dual problem (16) at
166"
R,0.45219123505976094,"a feasible solution (Z, {S(j,+), S(j,−)}p
j=1). Let (Z∗, W ∗, α∗) denote a globally optimal solu-
167"
R,0.4541832669322709,"tion of the primal problem (12). By Theorem 1, there exist matrices {S(j,+), S(j,−)}p
j=1 such
168"
R,0.45617529880478086,"that (Z∗, {S(j,+), S(j,−)}p
j=1) is a feasible solution of the relaxed bi-dual problem (16) and
169"
R,0.4581673306772908,"J(Z∗, {S(j,+), S(j,−)}p
j=1) is the same as the objective value of (12) at its global minimum
170"
R,0.4601593625498008,"(Z∗, W ∗, α∗). On the other hand, let ( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
j=1) denote an optimal solution of the
171"
R,0.46215139442231074,"relaxed bi-dual problem (16). From the optimality of ( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
j=1), we have
172"
R,0.4641434262948207,"J( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
j=1) ≤J(Z∗, {S(j,+), S(j,−)}p
j=1).
(17)"
R,0.46613545816733065,"Note that at (Z∗, W ∗, α∗) we obtain the optimal approximation of ∇log ρ −∇log π at x1, . . . , xN
173"
R,0.4681274900398406,"in the family of two-layer squared-ReLU networks (7). Smaller or equal objective value of the relaxed
174"
R,0.4701195219123506,"bi-dual problem (16) can be achieved at ( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
j=1) than at (Z∗, {S(j,+), S(j,−)}p
j=1).
175"
R,0.4721115537848606,"Therefore, we can view ˜Z∗gives an optimal approximation of ∇log ρ −∇log π evaluated on
176"
R,0.47410358565737054,"x1, . . . , xN in a broader function family including the two-layer squared ReLU neural networks.
177"
R,0.4760956175298805,"From the derivation of the relaxed bi-dual problem, we have the relation ˜Z∗= −Λ∗−Y , where
178"
R,0.47808764940239046,"(Λ∗, {r(j,+), r(j,−)) is optimal to the relaxed dual problem (15) and ( ˜Z∗, { ˜S(j,+), ˜S(j,−)}p
j=1) is
179"
R,0.4800796812749004,"optimal to the relaxed bi-dual problem (16). Therefore, by solving Λ∗from the relaxed dual problem
180"
R,0.4820717131474104,"(15), we can use −Λ∗−Y as the approximation of ∇log ρ −∇log π evaluated on x1, . . . , xN.
181"
R,0.48406374501992033,"Remark 4 We note that solving the proposed convex optimization problem 15 renders the approxi-
182"
R,0.4860557768924303,"mation of the Wasserstein gradient direction. Compared to the two-layer ReLU networks, it induces a
183"
R,0.48804780876494025,"broader class of functions represented by {S(j,+), S(j,−)}p
j=1. This contains more variables than the
184"
R,0.4900398406374502,"neural network function.
185"
PRACTICAL IMPLEMENTATION,0.49203187250996017,"3.2
Practical implementation
186"
PRACTICAL IMPLEMENTATION,0.4940239043824701,"Although the number p of all possible hyper-plane arrangements is upper bounded by 2r((N−1)e/r)r
187"
PRACTICAL IMPLEMENTATION,0.4960159362549801,"with r = rank(X), it is computationally costly to enumerate all possible p matrices D1, . . . , Dp to
188"
PRACTICAL IMPLEMENTATION,0.49800796812749004,"represent the constraints in the relaxed dual problem (4). In practice, we ﬁrst randomly sample M
189"
PRACTICAL IMPLEMENTATION,0.5,"i.i.d. random vectors u1, . . . , uM ∼N(0, Id) and generate a subset ˆS of S as follows:
190"
PRACTICAL IMPLEMENTATION,0.50199203187251,"ˆS = {diag(I(Xuj ≥0)|j ∈[M]}.
(18)
Then, we optimize the randomly sub-sampled version of the relaxed dual problem based on the subset
191"
PRACTICAL IMPLEMENTATION,0.5039840637450199,"ˆS and obtain the solution Λ. We then use −Λ −Y as the direction to update the particle system X.
192"
PRACTICAL IMPLEMENTATION,0.5059760956175299,"If the regularization parameter is too large, then we will have −Λ −Y = 0, which makes the particle
193"
PRACTICAL IMPLEMENTATION,0.5079681274900398,"system unchanged. Therefore, to ensure that ˜β is not too large, we decay ˜β by a factor γ1 ∈(0, 1).
194"
PRACTICAL IMPLEMENTATION,0.5099601593625498,"This also appears in (Ergen et al., 2021). On the other hand, if ˜β is too small resulting the relaxed dual
195"
PRACTICAL IMPLEMENTATION,0.5119521912350598,"problem (4) infeasible, we increase ˜β by multiplying γ−1
2 , where γ2 ∈(0, 1). Detailed explanation
196"
PRACTICAL IMPLEMENTATION,0.5139442231075697,"of the adjustment of the regularization parameter can be found in Appendix C. The overall algorithm
197"
PRACTICAL IMPLEMENTATION,0.5159362549800797,"is summarized in Algorithm 1.
198"
PRACTICAL IMPLEMENTATION,0.5179282868525896,"We note that the randomly subsampled version of the relaxed dual problem (15) involves 2N ˆp
inequality constraints and 2ˆp linear matrix inequality constraints with size (d + 1) × (d + 1).
Applying the standard interior point method (Boyd et al., 2004) leads to the computational time up to
O((max{N, d2}ˆp)6)."
PRACTICAL IMPLEMENTATION,0.5199203187250996,Algorithm 1 Convex neural Wasserstein descent
PRACTICAL IMPLEMENTATION,0.5219123505976095,"Require: initial positions {xn
0}N
n=1, step size αl, initial regularization parameter ˜β0, γ1, γ2 ∈(0, 1)."
PRACTICAL IMPLEMENTATION,0.5239043824701195,"1: while not converge do
2:
Form Xl and Yl based on {xn
l }N
n=1 and {∇log π(xn
l )}N
n=1.
3:
Solve Λl from the relaxed dual problem (15) with ˜β = ˜βl.
4:
if the relaxed dual problem with ˜β = ˜βl is infeasible then
5:
Set Xl+1 = Xl for n ∈[N] and set ˜βl+1 = γ−1
2
˜βl.
6:
else
7:
Update Xl+1 = Xl + αl(Λl + Yl) for n ∈[N] and set ˜βl+1 = γ1 ˜βl.
8:
end if
9: end while"
PRACTICAL IMPLEMENTATION,0.5258964143426295,"For high-dimensional problems, i.e., d is large, the computational cost of solving (15) can be large.
199"
PRACTICAL IMPLEMENTATION,0.5278884462151394,"In this case, we apply the dimension-reduction techniques (Zahm et al., 2018; Chen & Ghattas, 2020;
200"
PRACTICAL IMPLEMENTATION,0.5298804780876494,"Wang et al., 2021) to reduce the parameter dimension d to a data-informed intrinsic dimension ˆd,
201"
PRACTICAL IMPLEMENTATION,0.5318725099601593,"which is often very low, i.e., ˆd ≪d.
202"
NUMERICAL EXPERIMENTS,0.5338645418326693,"4
Numerical experiments
203"
NUMERICAL EXPERIMENTS,0.5358565737051793,"In this section, we present numerical results to compare WGD approximated by neural networks
204"
NUMERICAL EXPERIMENTS,0.5378486055776892,"(WGD-NN) and WGD approximated using convex optimization formulation of neural networks
205"
NUMERICAL EXPERIMENTS,0.5398406374501992,"(WGD-cvxNN). The performance of the two methods is assessed by the sample goodness-of-ﬁt
206"
NUMERICAL EXPERIMENTS,0.5418326693227091,"of the posterior. For WGD-NN, in each iteration, it updates the particle system using (3) with a
207"
NUMERICAL EXPERIMENTS,0.5438247011952191,"function Φ represented by a two-layer squared ReLU neural network. The parameters of the neural
208"
NUMERICAL EXPERIMENTS,0.545816733067729,"network is obtained by directly solving the nonconvex optimization problem (10). We note that
209"
NUMERICAL EXPERIMENTS,0.547808764940239,"it takes longer time by WGD-cvxNN (compared to WGD-NN) to solve the convex optimization
210"
NUMERICAL EXPERIMENTS,0.549800796812749,"problem. However, this optimization time is often dominated by the time in likelihood evaluation if
211"
NUMERICAL EXPERIMENTS,0.5517928286852589,"the model is expensive to solve. Moreover, the induced SDPs have speciﬁc structures of many similar
212"
NUMERICAL EXPERIMENTS,0.5537848605577689,"constraints, whose solution can be accelerated by designing a specialized convex optimization solver.
213"
NUMERICAL EXPERIMENTS,0.5557768924302788,"This is left for future work.
214"
A TOY EXAMPLE,0.5577689243027888,"4.1
A toy example
215"
A TOY EXAMPLE,0.5597609561752988,"We test the performance of WGD on a bimodal 2-dimensional double-banana posterior distribution
216"
A TOY EXAMPLE,0.5617529880478087,"introduced in (Detommaso et al., 2018). We ﬁrst generate 300 posterior samples by a Stein variational
217"
A TOY EXAMPLE,0.5637450199203188,"Newton (SVN) method (Detommaso et al., 2018) as the reference, as shown in Figure 1. We evaluate
218"
A TOY EXAMPLE,0.5657370517928287,"the performance of WGD-NN and WGD-cvxNN by calculating the maximum mean discrepancy
219"
A TOY EXAMPLE,0.5677290836653387,"(MMD) between their samples in each iteration and the reference samples. In the comparison, we
220"
A TOY EXAMPLE,0.5697211155378487,"use N = 50 samples and run for 100 iterations with step sizes αl = 10−3. For WGD-cvxNN, we
221"
A TOY EXAMPLE,0.5717131474103586,"set β = 1, γ1 = 0.95 and γ2 = 0.9510. For WGD-NN, we use m = 200 neurons and optimize the
222"
A TOY EXAMPLE,0.5737051792828686,"regularized training problem (10) using all samples with the Adam optimizer (Kingma & Ba, 2014)
223"
A TOY EXAMPLE,0.5756972111553785,"with learning rate 10−3 for 200 sub-iterations. We also set the regularization parameter β = 1 and
224"
A TOY EXAMPLE,0.5776892430278885,"decrease it by a factor of 0.95 in each iteration. We ﬁnd that this setup of parameters is more suitable.
225"
A TOY EXAMPLE,0.5796812749003984,"The posterior density and the sample distributions by WGD-cvxNN and WGD-NN at the ﬁnal
226"
A TOY EXAMPLE,0.5816733067729084,"step of 100 iterations are shown in Figure 1. It can be observed that WGD-cvxNN provides more
227"
A TOY EXAMPLE,0.5836653386454184,"representative samples than WGD-NN for the posterior density.
228"
A TOY EXAMPLE,0.5856573705179283,"Figure 1: Posterior density and sample distributions by WGD-cvxNN and WGD-NN at the ﬁnal step
of 100 iterations, compared to the reference SVN samples (right)."
A TOY EXAMPLE,0.5876494023904383,"In Figure 2, we plot the MMD of the samples by WGD-cvxNN and WGD-NN compared to the
229"
A TOY EXAMPLE,0.5896414342629482,"reference SVN samples at each iteration. We observe that the samples by WGD-cvxNN achieves
230"
A TOY EXAMPLE,0.5916334661354582,"much smaller MMD than those of WGD-NN compared to the reference SVN samples, which is
231"
A TOY EXAMPLE,0.5936254980079682,"consistent with the results shown in Figure 1. For WGD-cvxNN, it takes 572s in total, while for WGD-
232"
A TOY EXAMPLE,0.5956175298804781,"NN, it takes 16s in total. WGD-cvxNN takes much longer time than WGD-NN as WGD-cvxNN
233"
A TOY EXAMPLE,0.5976095617529881,"aims to solve for the global minimum of the relaxed convex dual problem.
234"
A TOY EXAMPLE,0.599601593625498,Figure 2: MMD of WGD-cvxNN and WGD-NN samples compared to the reference SVN samples.
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.601593625498008,"4.2
PDE-constrained nonlinear Bayesian inference
235"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.603585657370518,"In this experiment, we consider a nonlinear Bayesian inference problem constrained by the following
236"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6055776892430279,"partial differential equation (PDE) (Chen & Ghattas, 2020) with application to subsurface (Darcy)
237"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6075697211155379,"ﬂow in a physical domain D = (0, 1)2,
238"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6095617529880478,"v + ex∇u = 0
in D,
∇· v = h
in D,
(19)"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6115537848605578,"where u is pressure, v is velocity, h is force, ex is a random (permeability) ﬁeld equipped with
239"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6135458167330677,"a Gaussian prior x ∼N(x0, C) with covariance operator C = (−δ∆+ γI)−α where we set
240"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6155378486055777,"δ = 0.1, γ = 1, α = 2 and x0 = 0. This problem is widely used in many areas, for instance,
241"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6175298804780877,"estimating permeability in groundwater ﬂow, thermal conductivity in material science or electrical
242"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6195219123505976,"impedance in medical imaging, We impose Dirichlet boundary conditions u = 1 on the top boundary
243"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6215139442231076,"and u = 0 on the bottom boundary, and homogeneous Neumann boundary conditions on the left
244"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6235059760956175,"and right boundaries for u. We use a ﬁnite element method with piecewise linear elements for the
245"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6254980079681275,"discretization of the problem, resulting in 81 dimensions for the discrete parameter. The data is
246"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6274900398406374,"generated as pointwise observation of the pressure ﬁeld at 49 points equidistantly distributed in
247"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6294820717131474,"(0, 1)2, corrupted with additive 5% Gaussian noise. We use a DILI-MCMC algorithm Cui et al.
248"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6314741035856574,"(2016) with 10000 effective samples to compute the sample mean and sample variance, which are
249"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6334661354581673,"used as the reference values to assess the goodness of the samples by pWGD-cvxNN and pWGD-NN.
250"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6354581673306773,"We run pWGD-cvxNN and pWGD-NN with 64 samples for ten trials with step size αl = 10−3,
251"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6374501992031872,"where we set β = 10, γ1 = 0.95, and γ2 = 0.9510 for both methods. The RMSE of the sample
252"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6394422310756972,"mean and sample variance are shown in Figure 3 for the two methods at each of the iterations. We
253"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6414342629482072,"can observe that pWGD-cvxNN achieves smaller errors for both the sample mean and the sample
254"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6434262948207171,"variance compared to pWGD-NN at each iteration. Moreover, pWGD-cvxNN provides much smaller
255"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6454183266932271,"variation of the sample mean and sample variance for the ten trials compared to pWGD-NN.
256"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.647410358565737,"0
20
40
60
80
# iterations 1.2 1.0 0.8 0.6 0.4 0.2 0.0"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.649402390438247,Log10(RMSE of mean)
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.651394422310757,"pWGD-NN
pWGD-cvxNN"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6533864541832669,"0
20
40
60
80
# iterations 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6553784860557769,Log10(RMSE of variance)
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6573705179282868,"pWGD-NN
pWGD-cvxNN"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6593625498007968,"Figure 3: Ten trials and the RMSE of the sample mean (top) and sample variance (bottom) by
pWGD-NN and pWGD-cvxNN at different iterations. Nonlinear inference problem."
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6613545816733067,"4.3
Bayesian inference for COVID-19
257"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6633466135458167,"In this experiment, we use Bayesian inference to learn the dynamics of the transmission and severity
258"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6653386454183267,"of COVID-19 from the recorded data for New York state, as studied in Chen & Ghattas (2020).
259"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6673306772908366,"We use the model, parameter, and data as in Chen & Ghattas (2020). More speciﬁcally, we use a
260"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6693227091633466,"compartmental model for the modeling of the transmission and outcome of COVID-19. We take the
261"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6713147410358565,"number of hospitalized cases as the observation data to infer a social distancing parameter, a time-
262"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6733067729083665,"dependent stochastic process that is equipped with a Tanh–Gaussian prior to model the transmission
263"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6752988047808764,"reduction effect of social distancing, which becomes 96 dimensions after discretization.
264"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6772908366533864,"We run a projected Stein variational gradient descent (pSVGD) method Chen & Ghattas (2020) as
265"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6792828685258964,"the reference, and run pWGD-cvxNN and pWGD-NN using 64 samples for 100 iterations with step
266"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6812749003984063,"size αl = 10−3, where we set β = 10, γ1 = 0.95, and γ2 = 0.9510 for both methods as in the last
267"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6832669322709163,"example. From Figure 4 we can observe that pWGD-cvxNN produces more consistent results with
268"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6852589641434262,"pSVGD than pWGD-NN for both the sample mean and 90% credible interval, both in the inference
269"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6872509960159362,"of the social distancing parameter and in the prediction of the hospitalized cases.
270"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6892430278884463,"Mar
Apr
May
Jun 0.0 0.2 0.4 0.6 0.8 1.0"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6912350597609562,NN vs pSVGD social distancing
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6932270916334662,"mean_NN
mean_pSVGD"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6952191235059761,"Mar
Apr
May
Jun
cvxNN vs pSVGD social distancing 0.0 0.2 0.4 0.6 0.8"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6972111553784861,"1.0
mean_cvxNN
mean_pSVGD"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.6992031872509961,"Mar
Apr
May
Jun 0 5000 10000 15000 20000"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.701195219123506,NN vs pSVGD # hospitalized
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.703187250996016,"mean_NN
mean_pSVGD"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.7051792828685259,"Mar
Apr
May
Jun
cvxNN vs pSVGD # hospitalized 0 5000 10000 15000 20000"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.7071713147410359,"mean_cvxNN
mean_pSVGD"
PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE,0.7091633466135459,"Figure 4: Comparison of pWGD-cvxNN and pWGD-NN to the reference by pSVGD for Bayesian
inference of the social distancing parameter (left) from the data of the hospitalized cases (right) with
sample mean and 90% credible interval."
CONCLUSION,0.7111553784860558,"5
Conclusion
271"
CONCLUSION,0.7131474103585658,"In the context of variational Wasserstein gradient descent methods for Bayesian inference, we consider
272"
CONCLUSION,0.7151394422310757,"the approximation of the Wasserstein gradient direction by the gradient of functions in the family of
273"
CONCLUSION,0.7171314741035857,"two-layer neural networks. We propose a convex SDP relaxation of the dual of the variational primal
274"
CONCLUSION,0.7191235059760956,"problem, which can be solved efﬁciently using convex optimization methods instead of directly
275"
CONCLUSION,0.7211155378486056,"training the neural network as a nonconvex optimization problem. In particular, we established that
276"
CONCLUSION,0.7231075697211156,"the gradient obtained by the new formulation and convex optimization is at least as good as the
277"
CONCLUSION,0.7250996015936255,"optimal approximation of the Wasserstein gradient direction by functions in the family of two-layer
278"
CONCLUSION,0.7270916334661355,"neural networks, which is demonstrated by various numerical experiments. In future works, we
279"
CONCLUSION,0.7290836653386454,"expect to extend our convex neural network approximations to generalized Wasserstein ﬂows.
280"
CONCLUSION,0.7310756972111554,"Checklist
281"
CONCLUSION,0.7330677290836654,"The checklist follows the references. Please read the checklist guidelines carefully for information on
282"
CONCLUSION,0.7350597609561753,"how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or
283"
CONCLUSION,0.7370517928286853,"[N/A] . You are strongly encouraged to include a justiﬁcation to your answer, either by referencing
284"
CONCLUSION,0.7390438247011952,"the appropriate section of your paper or providing a brief inline description. For example:
285"
CONCLUSION,0.7410358565737052,"• Did you include the license to the code and datasets? [Yes] See Section ??.
286"
CONCLUSION,0.7430278884462151,"• Did you include the license to the code and datasets? [No] The code and the data are
287"
CONCLUSION,0.7450199203187251,"proprietary.
288"
CONCLUSION,0.7470119521912351,"• Did you include the license to the code and datasets? [N/A]
289"
CONCLUSION,0.749003984063745,"Please do not modify the questions and only use the provided macros for your answers. Note that the
290"
CONCLUSION,0.750996015936255,"Checklist section does not count towards the page limit. In your paper, please delete this instructions
291"
CONCLUSION,0.7529880478087649,"block and only keep the Checklist section heading above along with the questions/answers below.
292"
CONCLUSION,0.7549800796812749,"1. For all authors...
293"
CONCLUSION,0.7569721115537849,"(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
294"
CONCLUSION,0.7589641434262948,"contributions and scope? [Yes]
295"
CONCLUSION,0.7609561752988048,"(b) Did you describe the limitations of your work? [Yes]
296"
CONCLUSION,0.7629482071713147,"(c) Did you discuss any potential negative societal impacts of your work? [N/A]
297"
CONCLUSION,0.7649402390438247,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
298"
CONCLUSION,0.7669322709163346,"them? [Yes]
299"
CONCLUSION,0.7689243027888446,"2. If you are including theoretical results...
300"
CONCLUSION,0.7709163346613546,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
301"
CONCLUSION,0.7729083665338645,"(b) Did you include complete proofs of all theoretical results? [Yes]
302"
CONCLUSION,0.7749003984063745,"3. If you ran experiments...
303"
CONCLUSION,0.7768924302788844,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
304"
CONCLUSION,0.7788844621513944,"mental results (either in the supplemental material or as a URL)? [Yes]
305"
CONCLUSION,0.7808764940239044,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
306"
CONCLUSION,0.7828685258964143,"were chosen)? [Yes]
307"
CONCLUSION,0.7848605577689243,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
308"
CONCLUSION,0.7868525896414342,"ments multiple times)? [No]
309"
CONCLUSION,0.7888446215139442,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
310"
CONCLUSION,0.7908366533864541,"of GPUs, internal cluster, or cloud provider)? [No]
311"
CONCLUSION,0.7928286852589641,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
312"
CONCLUSION,0.7948207171314741,"(a) If your work uses existing assets, did you cite the creators? [N/A]
313"
CONCLUSION,0.796812749003984,"(b) Did you mention the license of the assets? [N/A]
314"
CONCLUSION,0.798804780876494,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
315 316"
CONCLUSION,0.8007968127490039,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
317"
CONCLUSION,0.8027888446215139,"using/curating? [N/A]
318"
CONCLUSION,0.8047808764940239,"(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
319"
CONCLUSION,0.8067729083665338,"information or offensive content? [N/A]
320"
CONCLUSION,0.8087649402390438,"5. If you used crowdsourcing or conducted research with human subjects...
321"
CONCLUSION,0.8107569721115537,"(a) Did you include the full text of instructions given to participants and screenshots, if
322"
CONCLUSION,0.8127490039840638,"applicable? [N/A]
323"
CONCLUSION,0.8147410358565738,"(b) Did you describe any potential participant risks, with links to Institutional Review
324"
CONCLUSION,0.8167330677290837,"Board (IRB) approvals, if applicable? [N/A]
325"
CONCLUSION,0.8187250996015937,"(c) Did you include the estimated hourly wage paid to participants and the total amount
326"
CONCLUSION,0.8207171314741036,"spent on participant compensation? [N/A]
327"
REFERENCES,0.8227091633466136,"References
328"
REFERENCES,0.8247011952191236,"Alvarez-Melis, D., Schiff, Y., and Mroueh, Y. Optimizing functionals on the space of probabilities
329"
REFERENCES,0.8266932270916335,"with input convex neural networks. arXiv preprint arXiv:2106.00774, 2021.
330"
REFERENCES,0.8286852589641435,"Ambrosio, L., Gigli, N., and Savaré, G. Gradient ﬂows: in metric spaces and in the space of
331"
REFERENCES,0.8306772908366534,"probability measures. Springer Science & Business Media, 2005.
332"
REFERENCES,0.8326693227091634,"Bartan, B. and Pilanci, M. Neural spectrahedra and semideﬁnite lifts: Global convex optimization of
333"
REFERENCES,0.8346613545816733,"polynomial activation neural networks in fully polynomial-time. arXiv preprint arXiv:2101.02429,
334"
REFERENCES,0.8366533864541833,"2021.
335"
REFERENCES,0.8386454183266933,"Bonet, C., Courty, N., Septier, F., and Drumetz, L. Sliced-wasserstein gradient ﬂows. arXiv preprint
336"
REFERENCES,0.8406374501992032,"arXiv:2110.10972, 2021.
337"
REFERENCES,0.8426294820717132,"Boyd, S., Boyd, S. P., and Vandenberghe, L. Convex optimization. Cambridge university press, 2004.
338"
REFERENCES,0.8446215139442231,"Bunne, C., Meng-Papaxanthos, L., Krause, A., and Cuturi, M. Jkonet: Proximal optimal transport
339"
REFERENCES,0.8466135458167331,"modeling of population dynamics. arXiv preprint arXiv:2106.06345, 2021.
340"
REFERENCES,0.848605577689243,"Carrillo, J. A., Craig, K., Wang, L., and Wei, C. Primal dual methods for wasserstein gradient ﬂows.
341"
REFERENCES,0.850597609561753,"Foundations of Computational Mathematics, pp. 1–55, 2021a.
342"
REFERENCES,0.852589641434263,"Carrillo, J. A., Matthes, D., and Wolfram, M.-T. Lagrangian schemes for wasserstein gradient ﬂows.
343"
REFERENCES,0.8545816733067729,"Handbook of Numerical Analysis, 22:271–311, 2021b.
344"
REFERENCES,0.8565737051792829,"Chen, P. and Ghattas, O. Projected stein variational gradient descent. Advances in Neural Information
345"
REFERENCES,0.8585657370517928,"Processing Systems, 33:1947–1958, 2020.
346"
REFERENCES,0.8605577689243028,"Cover, T. M. Geometrical and statistical properties of systems of linear inequalities with applications
347"
REFERENCES,0.8625498007968128,"in pattern recognition. IEEE transactions on electronic computers, (3):326–334, 1965.
348"
REFERENCES,0.8645418326693227,"Cui, T., Law, K. J., and Marzouk, Y. M. Dimension-independent likelihood-informed mcmc. Journal
349"
REFERENCES,0.8665338645418327,"of Computational Physics, 304:109–137, 2016.
350"
REFERENCES,0.8685258964143426,"Detommaso, G., Cui, T., Spantini, A., Marzouk, Y., and Scheichl, R. A stein variational newton
351"
REFERENCES,0.8705179282868526,"method. arXiv preprint arXiv:1806.03085, 2018.
352"
REFERENCES,0.8725099601593626,"di Langosco, L. L., Fortuin, V., and Strathmann, H. Neural variational gradient descent. arXiv
353"
REFERENCES,0.8745019920318725,"preprint arXiv:2107.10731, 2021.
354"
REFERENCES,0.8764940239043825,"Diamond, S. and Boyd, S. CVXPY: A Python-embedded modeling language for convex optimization.
355"
REFERENCES,0.8784860557768924,"Journal of Machine Learning Research, 17(83):1–5, 2016.
356"
REFERENCES,0.8804780876494024,"Ergen, T., Sahiner, A., Ozturkler, B., Pauly, J., Mardani, M., and Pilanci, M. Demystifying batch
357"
REFERENCES,0.8824701195219123,"normalization in relu networks: Equivalent convex optimization models and implicit regularization.
358"
REFERENCES,0.8844621513944223,"arXiv preprint arXiv:2103.01499, 2021.
359"
REFERENCES,0.8864541832669323,"Fan, J., Taghvaei, A., and Chen, Y.
Variational wasserstein gradient ﬂow.
arXiv preprint
360"
REFERENCES,0.8884462151394422,"arXiv:2112.02424, 2021.
361"
REFERENCES,0.8904382470119522,"Feng, X., Gao, Y., Huang, J., Jiao, Y., and Liu, X. Relative entropy gradient sampler for unnormalized
362"
REFERENCES,0.8924302788844621,"distributions. arXiv preprint arXiv:2110.02787, 2021.
363"
REFERENCES,0.8944223107569721,"Frogner, C. and Poggio, T. Approximate inference with wasserstein gradient ﬂows. In International
364"
REFERENCES,0.896414342629482,"Conference on Artiﬁcial Intelligence and Statistics, pp. 2581–2590. PMLR, 2020.
365"
REFERENCES,0.898406374501992,"Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal approxi-
366"
REFERENCES,0.900398406374502,"mators. Neural networks, 2(5):359–366, 1989.
367"
REFERENCES,0.9023904382470119,"Hwang, H. J., Kim, C., Park, M. S., and Son, H. The deep minimizing movement scheme. arXiv
368"
REFERENCES,0.9043824701195219,"preprint arXiv:2109.14851, 2021.
369"
REFERENCES,0.9063745019920318,"Hyvärinen, A. and Dayan, P. Estimation of non-normalized statistical models by score matching.
370"
REFERENCES,0.9083665338645418,"Journal of Machine Learning Research, 6(4), 2005.
371"
REFERENCES,0.9103585657370518,"Jeyakumar, V. and Li, G. Trust-region problems with linear inequality constraints: exact sdp relaxation,
372"
REFERENCES,0.9123505976095617,"global optimality and robust optimization. Mathematical Programming, 147(1):171–206, 2014.
373"
REFERENCES,0.9143426294820717,"Jordan, R., Kinderlehrer, D., and Otto, F. The variational formulation of the fokker–planck equation.
374"
REFERENCES,0.9163346613545816,"SIAM journal on mathematical analysis, 29(1):1–17, 1998.
375"
REFERENCES,0.9183266932270916,"Junge, O., Matthes, D., and Osberger, H. A fully discrete variational scheme for solving nonlinear
376"
REFERENCES,0.9203187250996016,"fokker–planck equations in multiple space dimensions. SIAM Journal on Numerical Analysis, 55
377"
REFERENCES,0.9223107569721115,"(1):419–443, 2017.
378"
REFERENCES,0.9243027888446215,"Kingma, D. P. and Ba, J.
Adam: A method for stochastic optimization.
arXiv preprint
379"
REFERENCES,0.9262948207171314,"arXiv:1412.6980, 2014.
380"
REFERENCES,0.9282868525896414,"Kruse, J., Detommaso, G., Scheichl, R., and Köthe, U. Hint: Hierarchical invertible neural transport
381"
REFERENCES,0.9302788844621513,"for density estimation and bayesian inference. arXiv preprint arXiv:1905.10687, 2019.
382"
REFERENCES,0.9322709163346613,"Lan, S., Li, S., and Shahbaba, B. Scaling up bayesian uncertainty quantiﬁcation for inverse problems
383"
REFERENCES,0.9342629482071713,"using deep neural networks. arXiv preprint arXiv:2101.03906, 2021.
384"
REFERENCES,0.9362549800796812,"Lin, A. T., Fung, S. W., Li, W., Nurbekyan, L., and Osher, S. J. Alternating the population and
385"
REFERENCES,0.9382470119521913,"control neural networks to solve high-dimensional stochastic mean-ﬁeld games. Proceedings of
386"
REFERENCES,0.9402390438247012,"the National Academy of Sciences, 118(31), 2021a.
387"
REFERENCES,0.9422310756972112,"Lin, A. T., Li, W., Osher, S., and Montúfar, G. Wasserstein proximal of gans. arXiv preprint
388"
REFERENCES,0.9442231075697212,"arXiv:2102.06862, 2021b.
389"
REFERENCES,0.9462151394422311,"Liu, C., Zhuo, J., Cheng, P., Zhang, R., and Zhu, J. Understanding and accelerating particle-based
390"
REFERENCES,0.9482071713147411,"variational inference. In International Conference on Machine Learning, pp. 4082–4092. PMLR,
391"
REFERENCES,0.950199203187251,"2019.
392"
REFERENCES,0.952191235059761,"Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference
393"
REFERENCES,0.954183266932271,"algorithm. In Advances in neural information processing systems, pp. 2378–2386, 2016.
394"
REFERENCES,0.9561752988047809,"Liutkus, A., Simsekli, U., Majewski, S., Durmus, A., and Stöter, F.-R. Sliced-wasserstein ﬂows: Non-
395"
REFERENCES,0.9581673306772909,"parametric generative modeling via optimal transport and diffusions. In International Conference
396"
REFERENCES,0.9601593625498008,"on Machine Learning, pp. 4104–4113. PMLR, 2019.
397"
REFERENCES,0.9621513944223108,"Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expressive power of neural networks: A view from
398"
REFERENCES,0.9641434262948207,"the width. In Proceedings of the 31st International Conference on Neural Information Processing
399"
REFERENCES,0.9661354581673307,"Systems, pp. 6232–6240, 2017.
400"
REFERENCES,0.9681274900398407,"Mokrov, P., Korotin, A., Li, L., Genevay, A., Solomon, J., and Burnaev, E. Large-scale wasserstein
401"
REFERENCES,0.9701195219123506,"gradient ﬂows. arXiv preprint arXiv:2106.00736, 2021.
402"
REFERENCES,0.9721115537848606,"Onken, D., Fung, S. W., Li, X., and Ruthotto, L. Ot-ﬂow: Fast and accurate continuous normalizing
403"
REFERENCES,0.9741035856573705,"ﬂows via optimal transport. arXiv preprint arXiv:2006.00104, 2020.
404"
REFERENCES,0.9760956175298805,"Otto, F. The geometry of dissipative evolution equations: the porous medium equation. Communica-
405"
REFERENCES,0.9780876494023905,"tions in Partial Differential Equations, 26(1-2):101–174, 2001.
406"
REFERENCES,0.9800796812749004,"Pilanci, M. and Ergen, T. Neural networks are convex regularizers: Exact polynomial-time convex
407"
REFERENCES,0.9820717131474104,"optimization formulations for two-layer networks. In International Conference on Machine
408"
REFERENCES,0.9840637450199203,"Learning, pp. 7695–7705. PMLR, 2020.
409"
REFERENCES,0.9860557768924303,"Rezende, D. and Mohamed, S. Variational inference with normalizing ﬂows. In International
410"
REFERENCES,0.9880478087649402,"conference on machine learning, pp. 1530–1538. PMLR, 2015.
411"
REFERENCES,0.9900398406374502,"Sahiner, A., Ergen, T., Pauly, J., and Pilanci, M. Vector-output relu neural network problems are
412"
REFERENCES,0.9920318725099602,"copositive programs: Convex analysis of two layer networks and polynomial-time algorithms.
413"
REFERENCES,0.9940239043824701,"arXiv preprint arXiv:2012.13329, 2020.
414"
REFERENCES,0.9960159362549801,"Stuart, A. M. Inverse problems: a Bayesian perspective. Acta numerica, 19:451–559, 2010.
415"
REFERENCES,0.99800796812749,"Villani, C. Topics in optimal transportation. American Mathematical Soc., 2003.
416"
