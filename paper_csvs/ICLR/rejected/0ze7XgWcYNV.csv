Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0045045045045045045,"Reliable AI agents should be mindful of the limits of their knowledge and con-
sult humans when sensing that they do not have sufﬁcient knowledge to make
sound decisions. We formulate a hierarchical reinforcement learning framework
for learning to decide when to request additional information from humans and
what type of information would be helpful to request. Our framework extends
partially-observed Markov decision processes (POMDPs) by allowing an agent
to interact with an assistant to leverage their knowledge in accomplishing tasks.
Results on a simulated human-assisted navigation problem demonstrate the ef-
fectiveness of our framework: aided with an interaction policy learned by our
method, a navigation policy achieves up to a 7× improvement in task success rate
compared to performing tasks only by itself. We ﬁnd that the ability to request
subgoals enables the agent to generalize effectively to tasks in unseen environ-
ments. We analyze beneﬁts and challenges of learning with a hierarchical policy
structure and suggest directions for future work."
INTRODUCTION,0.009009009009009009,"1
INTRODUCTION"
INTRODUCTION,0.013513513513513514,"Human-agent communication at deployment time has been under-explored in machine learning,
where the traditional focus has been on building agents that can accomplish tasks on their own
(full autonomy). Nevertheless, enabling an agent to exchange information with humans during its
operation can potentially enhance its helpfulness and trustworthiness. The ability to request and
interpret human advice would help the agent accomplish tasks beyond its built-in knowledge, while
the ability to accurately convey when and why it is about to fail would make the agent safer to use."
INTRODUCTION,0.018018018018018018,"In his classical work, Grice (1975) outlines the desired characteristics of cooperative communica-
tion, commonly known as the Gricean maxims of cooperation. Among these characteristics are
informativeness (the maxim of quantity) and faithfulness (the maxim of quality). Human-agent
communication in current work has fallen short in these two aspects. Traditional frameworks like
imitation learning and reinforcement learning employ limited communication protocols where
the agent and the human can only exchange simple intentions (requesting low-level actions or
rewards Torrey & Taylor (2013); Knox & Stone (2009)). More powerful frameworks like (Nguyen
& Daum´e III, 2019; Nguyen et al., 2019; Kim et al., 2019) allow the agent to process high-level
instructions from humans, but the agent still only requests generic help. Recent work in natural
language processing endows the agent with the ability to generate rich natural language utterances
(Camburu et al., 2018; Rao & Daum´e III, 2018; De Vries et al., 2017; Das et al., 2017), but the
communication is not faithful in the sense that the agent only mirrors human-generated utterances
without grounding its communication in self-perception of its (in)capabilities and (un)certainties.
Essentially it learns to convey what a human may be concerned about, not what it is concerned about."
INTRODUCTION,0.02252252252252252,"This paper presents a hierarchical reinforcement learning framework named HARI (Human-Assisted
Reinforced Interaction), which supports richer and more faithful human-agent communication. Our
framework allows the agent to learn to convey intrinstic needs for speciﬁc information and to incor-
porate diverse types of information from humans to make better decisions. Speciﬁcally, the agent
in HARI is equipped with three information-seeking intentions: in every step, it can choose to re-
quest more information about (i) its current state, (ii) the goal state, or (iii) a subgoal state which,
if reached, helps it make progress on the current task. Upon receiving a request, the human can
transfer new information to the agent by giving new descriptions of the requested state. These de-"
INTRODUCTION,0.02702702702702703,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03153153153153153,"C
Environment"
INTRODUCTION,0.036036036036036036,Living room
INTRODUCTION,0.04054054054054054,Kitchen
INTRODUCTION,0.04504504504504504,I’m ﬁnding a mug
INTRODUCTION,0.04954954954954955,"Hallway
B A D E"
INTRODUCTION,0.05405405405405406,"GOAL
(tell me about the"
INTRODUCTION,0.05855855855855856,goal location)
INTRODUCTION,0.06306306306306306,"The mug is in 
the kitchen, next"
INTRODUCTION,0.06756756756756757,to a sink.
INTRODUCTION,0.07207207207207207,"CUR
(tell me about my"
INTRODUCTION,0.07657657657657657,current location)
INTRODUCTION,0.08108108108108109,"You are in the 
living room, next"
INTRODUCTION,0.08558558558558559,to a couch.
INTRODUCTION,0.09009009009009009,"SUB
(break the current task 
into subtasks and give"
INTRODUCTION,0.0945945945945946,me the ﬁrst one)
INTRODUCTION,0.0990990990990991,Find the statue in
INTRODUCTION,0.1036036036036036,the hallway.
INTRODUCTION,0.10810810810810811,"DONE [subtask) 
(stop doing subtask,"
INTRODUCTION,0.11261261261261261,resume main task)
INTRODUCTION,0.11711711711711711,DONE [main task)
INTRODUCTION,0.12162162162162163,"(terminate) A B C D
E"
INTRODUCTION,0.12612612612612611,subtask stack
INTRODUCTION,0.13063063063063063,subtask stack
INTRODUCTION,0.13513513513513514,"Figure 1: An illustration of the HARI framework in a human-assisted navigation task. An agent
can only observe part of an environment and is asked to ﬁnd a mug in the kitchen. An assistant
communicates with the agent and can provide it with information about the environment and the
task. Initially (A) it may request more information about the goal, but may not know enough about
where it currently is. For example, at location B, due to limited perception, it does not recognize that
it is in a living room and stands next to a couch. It can obtain such information from the assistant.
If the current task becomes too difﬁcult (like at location C), the agent can require the assistant to
provide a simpler subtask which, if accomplished, helps it make progress on the main task. The
agent maintains a stack of tasks and always executes the task at the top. When the agent receives a
(sub)task, it pushes the (sub)task to the top of the stack. When it wants to stop executing a (sub)task,
it pops the (sub)task from the stack (e.g., at location D). At location E, the agent empties the stack
and terminates its execution."
INTRODUCTION,0.13963963963963963,"scriptions will be incorporated as new inputs to the agent’s decision-making policy. The human thus
can transfer any form of information that can be interpreted by the policy (e.g., asking the agent to
execute skills that it has learned, giving side information that connects the agent to a situation it is
more familiar with). Because the agent’s policy can implement a variety of model architectures and
learning algorithms, our framework opens up many possibilities for human-agent communication."
INTRODUCTION,0.14414414414414414,"To enable faithful communication, we teach the agent to understand its intrinsic needs by interacting
with the human and the environment (rather than imitating human behaviors). By requesting differ-
ent types of information and observing how much each type of information enhances its decisions,
the agent gradually learns to determine which information is most useful to obtain in a given situa-
tion. With this capability, at deployment time, it can choose when and what information to ask from
the human to improve its task performance. To demonstrate the effectiveness of HARI, we simulate
a human-assisted navigation problem where an agent has access to only sparse information about its
current state and the goal, and can request additional information about these states. On tasks that
take place in previously unseen environments, the ability to ask for help improves the agent’s suc-
cess rate by 7× higher compared to performing tasks only by itself. This human-assisted agent even
outperforms an agent that always has access to dense information in unseen environments, thanks
to the ability to request subgoals. We show that performance of the agent can be further improved
by recursively asking for subgoals of subgoals. We discuss limitations of the policy’s model and
feature representation, which suggest room for future improvements."
INTRODUCTION,0.14864864864864866,"2
MOTIVATION: LIMITATIONS OF THE STANDARD POMDP FRAMEWORK"
INTRODUCTION,0.15315315315315314,"We consider an environment deﬁned by a partially-observed Markov decision process (POMDP)
E = (S, A, T, c, D, ρ) with state space S, action space A, state-transition function T : S × A →
∆(S), cost function c : S × A →R, description space D, and description function ρ : S →∆(D).1
Here, ∆(Y) denotes the set of all probability distributions over a set Y. We refer to this environment
as the operation environment because it is where the agent operates to accomplish tasks."
INTRODUCTION,0.15765765765765766,"Each task in the environment is deﬁned as a tuple (s1, g1, dg
1) where s1 is the start state, g1 is the
goal state, and dg
1 is a limited description of g1. Initially, a task (s1, g1, dg
1) is sampled from a task"
INTRODUCTION,0.16216216216216217,"1We use the term “description” in lieu of “observation” in the POMDP formulation to emphasize two prop-
erties of the information the agent has access to for making decisions: (i) the information can be in various
modalities and (ii) the information can be obtained via not only perception, but also communication."
INTRODUCTION,0.16666666666666666,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.17117117117117117,"distribution T. An agent starts in s1 and is only given the goal description dg
1. It has to reach the goal
state g1 within H time steps. Let gt and dg
t be the goal state and goal description being executed at
time step t, respectively. In a standard POMDP, gt = g1 and dg
t = dg
1 for 1 ≤t ≤H. But later, we
will enable the agent to set new goals via communication with humans."
INTRODUCTION,0.17567567567567569,"At any time step t, the agent does not know its true state st but only receives a description ds
t ∼ρ(st)
of the state. Generally, the description can include any information coming from any knowledge
source (e.g., an RGB image and/or a verbal description describing the current view). Given ds
t
and dg
t , the agent then makes a decision at ∈A, transitions to the next state st+1 ∼T(st, at),
and receives a cost ct ≜c(st, at). A special action adone ∈A is taken when the agent decides
to terminate its execution. The goal of the agent is to reach g1 with minimum cumulative cost
C(τ) = PH
t=1 ct, where τ = (s1, ds
1, a1, s2, ds
2, a2, . . . , sH, ds
H) is an execution of the task."
INTRODUCTION,0.18018018018018017,"As the agent does not have access to its true state, it can only make decisions based on the (ob-
servable) partial execution τ1:t = (ds
1, a1, . . . , ds
t). Kaelbling et al. (1998) introduce the notion of a
belief state b ∈∆(S), which sufﬁciently summarizes a partial execution as a distribution over the
state space S. In practice, when S is continuous or high-dimensional, representing and updating
a full belief state (whose dimension is |S|) is intractable. We follow Hausknecht & Stone (2015),
using recurrent neural networks to learn compact representation of partial executions. We denote by
bs
t a representation of the partial execution τ1:t and by B the set of all possible representations."
INTRODUCTION,0.18468468468468469,"The agent maintains an operation policy ˆπ : B × D →∆(A) that maps a belief state bs and a goal
description dg to a distribution over A. The learning objective for solving a standard POMDP is to
estimate an operation policy that minimizes the expected cumulative cost of performing tasks:"
INTRODUCTION,0.1891891891891892,"min
π E(s1,g1,dg
1)∼T,τ∼Pπ(·|s1,dg
1) [C(τ)]
(1)"
INTRODUCTION,0.19369369369369369,"where Pπ(· | s1, dg
1) is the distribution over executions generated by a policy π given start state
s1 and goal description dg
1. In a standard POMDP, an agent performs tasks by executing its own
operation policy without asking for any external assistance. Moreover, the description function ρ and
the goal description dg
1 are assumed to be ﬁxed during a task execution. As seen from Equation 1,
given a ﬁxed environment and task distribution, the expected performance of the agent is solely
determined by the operation policy ˆπ. Thus, the standard POMDP framework does not provide any
mechanism for improving the agent’s performance other than enhancing the operation policy."
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.1981981981981982,"3
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS"
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.20270270270270271,"We introduce an assistant into the operation environment, who can provide information about the
environment’s states. We assume the agent possesses a pre-learned operation policy ˆπ. This pol-
icy serves as the common ground between the agent and the assistant, which is a prerequisite for
communication between them to occur. For example, this policy represents a set of basic tasks that
agent has mastered and the assistant can ask the agent to perform. In general, the more knowledge
encoded in this policy, the more effectively the agent can communicate with and leverage help from
the assistant. Our goal is to learn an interaction policy ψθ (parametrized by θ) that controls how
the agent communicates with the assistant to gather additional information. The operation policy
ˆπ will be invoked by the interaction policy if the latter decides that the agent does not need new
information and wants to take an operating action."
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.2072072072072072,"The assistant aids the agent by giving new (current or goal) state descriptions, connecting the agent
to situations on which it can make better decisions. Consider an object-ﬁnding navigation problem,
where a robot has been trained to reliably navigate to the kitchen from the living room of a house.
Suppose the robot is then asked to “ﬁnd a mug”, an object that it has never heard of. The assistant
can help the robot accomplish this task by giving a more informative goal description “ﬁnd a mug
in the kitchen”, relating the current task to the kitchen-ﬁnding task that the robot has been familiar
with. The robot may also have problems with localization: it knows how to get the kitchen from the
living room but it may not realize that it is currently the living room. In this case, giving a current-
state description that speciﬁes this information provides the robot with a useful hint on what actions
to take next."
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.21171171171171171,"Our framework allows the assistant to convey any form of information that the agent can incorpo-
rate into its input. As discussed in §2, the notion of “state description” in our framework is general,"
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.21621621621621623,Under review as a conference paper at ICLR 2022
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.22072072072072071,"capturing various types of information, including but not limited to visual perception and verbal de-
scription. Communication between the agent and the assistant can be ﬂexibly enriched by designing
the agent’s operation policy to be able to consume the forms of information of interest (e.g., a policy
that takes natural language as input)."
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.22522522522522523,"Communication with the Assistant.
The assistant is present all the time and knows the agent’s
current state st and the goal state gt. It is speciﬁed by two functions: a description function ρA :
S × D →∆(D) and a subgoal function ωA : S × S →∆(S). ρA(d′ | s, d) speciﬁes the probability
of giving d′ as the new description of state s given a current description d. ωA(g′ | s, g) indicates
the probability of proposing g′ as a subgoal given a current state s and a goal state g."
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.22972972972972974,"At time step t, the assistant accepts three types of request from the agent:"
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.23423423423423423,"(a) CUR: requests a new description of st and receives ds
t+1 ∼ρA (· | st, ds
t);
(b) GOAL: requests a new description of gt and receives dg
t+1 ∼ρA (· | gt, dg
t );
(c) SUB: requests a description of a subgoal gt+1 and receives dg
t+1 ∼ρA (· | gt+1, ∅) where
gt+1 ∼ωA (· | st, gt) and ∅is an empty description."
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.23873873873873874,"Interaction Policy.
The action space of the interaction policy ψθ consists of ﬁve actions:
{CUR, GOAL, SUB, DO, DONE}. The ﬁrst three actions correspond to making the three types of
request that the assistants accepts. The remaining two actions are used to traverse in the operation
environment:"
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.24324324324324326,"(d) DO: executes the action ado
t
≜arg maxa∈A ˆπ (a | bs
t, dg
t ). The agent transitions to a new
operation state st+1 ∼T(st, ado
t );
(e) DONE: determines that the current goal gt has been reached.2 If gt is a main goal (gt = g1),
the task episode ends. If gt is a subgoal (gt ̸= g1), the agent may choose a new goal to
follow. Our problem formulation leaves it open on what goal should be selected next."
LEVERAGING HUMAN KNOWLEDGE TO BETTER ACCOMPLISH TASKS,0.24774774774774774,"By selecting among these actions, the interaction policy essentially decides when to ask the assistant
for additional information, and what types of information to ask for. Our formulation does not
specify the input space of the interaction policy, as this space depends on how the agent implements
its goal memory (i.e. how it stores and retrieves the subgoals). In the next section, we introduce an
instantiation where the agent uses a stack data structure to manage (sub)goals."
HIERARCHICAL REINFORCEMENT LEARNING FRAMEWORK,0.25225225225225223,"4
HIERARCHICAL REINFORCEMENT LEARNING FRAMEWORK"
HIERARCHICAL REINFORCEMENT LEARNING FRAMEWORK,0.25675675675675674,"In this section, we describe the HARI framework. We ﬁrst formulate the POMDP environment that
the interaction policy acts in, referred to as the interaction environment (§4.1). Our construction
employs a goal stack to manage multiple levels of (sub)goals (§4.2). A goal stack stores all the tasks
the agent has been assigned but has not yet decided to terminate (by choosing the DONE action).
It is updated in every step depending on the taken action. We design a cost function (§4.4) that
speciﬁes a trade-off between the cost of taking actions (acting cost) and the cost of not completing a
task (task error)."
INTERACTION ENVIRONMENT,0.26126126126126126,"4.1
INTERACTION ENVIRONMENT"
INTERACTION ENVIRONMENT,0.26576576576576577,"Given an operation environment E = (S, A, T, c, D, ρ), the interaction environment constructed on
top of E is a POMDP ¯E = ( ¯S, ¯
A, ¯T , ¯c, ¯D, ¯ρ) with:"
INTERACTION ENVIRONMENT,0.2702702702702703,"• State space ¯S = S × D × GL where GL is the set of all goal stacks containing at most L
elements (L is a hyperparameter). Each state ¯s = (s, ds, G) ∈¯S is a tuple of an operation
state s ∈S, its description ds ∈D, and a goal stack G ∈GL. Each element in the goal
stack G is a tuple (g, dg) of a goal state g ∈S and its description dg ∈D;
• Action space ¯
A = {CUR, GOAL, SUB, DO, DONE};
• State-transition function ¯T = Ts · TG where Ts : S × D × ¯
A →∆(S × D) and TG :
GL × ¯
A →∆(GL);
• Cost function ¯c : (S×GL)× ¯
A →R (deﬁned in §4.4 to trade off acting cost and task error);"
INTERACTION ENVIRONMENT,0.2747747747747748,2Note that the agent may falsely decide that a goal has been reached.
INTERACTION ENVIRONMENT,0.27927927927927926,Under review as a conference paper at ICLR 2022
INTERACTION ENVIRONMENT,0.28378378378378377,"• Description space ¯D = D × Gd
L where Gd
L is the set of all goal-description stacks of size L.
At any time, the agent cannot access the environment’s goal stack G, which contains true
goal states. Instead, it can only observe the descriptions in G. We call this partial stack a
goal-description stack, denoted by Gd;
• Description function ¯ρ : ¯S →¯D, where ¯ρ(¯s) = ¯ρ(s, ds, G) = (ds, Gd). Unlike in the
standard POMDP formulation, this description function is deterministic."
INTERACTION ENVIRONMENT,0.2882882882882883,"A belief state ¯bt summarizes a partial execution (¯s1, ¯a1, · · · , ¯st). We formally deﬁne the interaction
policy as ψθ : ¯B →∆(A), where ¯B is the set of all interaction belief states."
GOAL STACK,0.2927927927927928,"4.2
GOAL STACK"
GOAL STACK,0.2972972972972973,"A goal stack is an ordered set of tasks that the agent has not declared completion (by calling DONE).
The initial stack G1 = {(g1, dg
1)} contains the main goal g1, and its description dg
1. Let Gt be the
goal stack at time step t. The agent executes the goal gt at the top of this stack. Only the GOAL,
SUB, DONE actions alter the stack. The GOAL action replaces the top goal description with dg
t+1,
the new description given by the assistant. The SUB action pushes a new subtask (gt+1, dg
t+1) to the
stack. The DONE action pops the top (sub)task from the stack."
GOAL STACK,0.30180180180180183,Gt.update(a) =
GOAL STACK,0.3063063063063063,"


 

"
GOAL STACK,0.3108108108108108,"Gt.pop().push(gt, dg
t+1)
if a = GOAL,
Gt.push(gt+1, dg
t+1)
if a = SUB,
Gt.pop()
if a = DONE,
Gt
otherwise (2)"
GOAL STACK,0.3153153153153153,"The SUB action is not available to the agent when the current stack contains L elements, guarantee-
ing that goal stack always has at most L elements. The goal-stack transition function TG is deﬁned
as TG(Gt+1 | Gt, ¯at) = 1{Gt+1 = Gt.update(¯at)} where 1{.} is an indicator function."
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.31981981981981983,"4.3
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION"
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.32432432432432434,"To complete the deﬁnition of the state-transition function ¯T, we deﬁne the transition function Ts.
This function is factored into two terms by the chain rule:"
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.32882882882882886,"Ts(st+1, ds
t+1 | st, ds
t, ¯at) = P(st+1 | st, ¯at) · P(ds
t+1 | st+1, ds
t, ¯at)
(3)"
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.3333333333333333,Only taking the DO action may change the current operation state
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.33783783783783783,"P(st+1 | st, ¯at) =
T
 
st+1 | st, ado
t

if ¯at = DO,
1{st+1 = st}
otherwise
(4)"
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.34234234234234234,"The description ds
t may vary when the agent moves to a new operation state (by taking the DO
action) or requests a new description of st (by taking the CUR action)"
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.34684684684684686,"P(ds
t+1 | ds
t, st+1, ¯at) = 
 "
TRANSITION OF THE CURRENT OPERATION STATE AND ITS DESCRIPTION,0.35135135135135137,"ρ(ds
t+1 | st+1)
if ¯at = DO,
ρA(ds
t+1 | st+1, ds
t)
if ¯at = CUR,
1{ds
t+1 = ds
t}
otherwise
(5)"
COST FUNCTION,0.35585585585585583,"4.4
COST FUNCTION"
COST FUNCTION,0.36036036036036034,"The interaction policy needs to balance between two types of cost: the cost of taking actions (acting
cost) and the cost of not completing a task (task error). The acting cost also subsumes the cost of
communicating with the assistant because, in reality, such interactions consume time, human effort,
and possibly trust. Assuming that the assistant is helpful, acting cost and task error usually conﬂict
with each other; for example, the agent may lower its task error if it is willing to suffer a larger
acting cost by increasing the number of requests to the assistant."
COST FUNCTION,0.36486486486486486,"We employ a simpliﬁed model where all types of cost are non-negative real numbers of the same
unit. Making a request of type a is assigned a constant cost γa. The cost of taking the DO action
is c(st, ado
t ), the cost of executing the ado
t action in the operation environment. Calling DONE to
terminate execution of the main goal g1 incurs a task error c(st, adone). We exclude the task errors"
COST FUNCTION,0.36936936936936937,Under review as a conference paper at ICLR 2022
COST FUNCTION,0.3738738738738739,"of executing subgoals because the interaction policy is only evaluated on reaching the main goal.
The magnitudes of the costs naturally specify a trade-off between acting cost and task error. For
example, setting the task errors much larger than the other costs indicates that completing tasks is
prioritized over taking few actions."
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.3783783783783784,"5
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION"
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.38288288288288286,"Problem.
We apply HARI to modeling a human-assisted navigation (HAN) problem. In HAN, a
human requests an agent to ﬁnd an object in an indoor environment. Each task request asks the
agent to go to a room of type r and ﬁnd an object of type o (e.g., ﬁnd a mug in a kitchen). The
agent is equipped with a camera and shares its camera view with the human. We assume that the
human is sufﬁciently familiar with the environment that they can recognize the agent’s location by
looking at its current view. Before issuing a task request, the human imagines a goal location (but
do not reveal it to the agent). We are primarily interested in evaluating success in goal-ﬁnding, i.e.
whether the agent can arrive at the human’s intended goal location. Even though there could be
multiple locations that match a request, the agent only succeeds if it arrives exactly at the chosen
goal location. We also determine success in request-fulﬁlling, where the agent successfully fulﬁlls a
request if it navigates to any node that is within two meters of an object that matches the request."
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.38738738738738737,"While an agent is performing a task, it may request the human to provide additional information via
telecommunication (e.g., a chat app). Speciﬁcally, it can ask for a description of its current location
(CUR), the goal location (GOAL), or a subgoal location that is on the path from its current location
to the goal location (SUB). Detail about how the subgoals are determined is in the Appendix."
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.3918918918918919,"Operation Environment.
We construct the operation environments using the environment graphs
provided by the Matterport3D simulator (Anderson et al., 2018). Each environment graph is gener-
ated from a 3D model of a house where each node is a location in the house and each edge connects
two nearby unobstructed locations. Each operation state s corresponds to a node in the graph. At
any time, the agent’s operation action space A consists of traversing to any of the nodes that are
adjacent to its current node."
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.3963963963963964,"We employ a discrete bag-of-features representation for state descriptions.3 A bag of features rep-
resents the information that the agent extracts from the raw input that the agent perceives (e.g., an
image, a language sentence). Working with this intermediate input allows us to easily vary the type
and amount of information given to the agent. Speciﬁcally, we simulate two settings of descriptions:
dense and sparse. At evaluation time, the agent perceives sparse descriptions and request the assis-
tant for dense descriptions. A dense description of a current location contains the room name at the
location, and the features of M objects restricted to be within δ meters of the location. The features
of each object consists of (i) its name, (ii) horizontal and vertical angles (relative to the current view-
point), and (iii) distance (in meters) from the object to the current location. A dense description of a
goal follows the same representation scheme. In the sparse setting, the current-location description
does not include the room name. Moreover, we remove the features of objects that are not in the top
100 most frequent objects, emulating an imperfect object detector module. The sparse goal descrip-
tion (the task request) has only features of the target object and the room name where the object is
located at. Especially, if a subgoal location is adjacent or coincides with the agent’s current location,
instead of describing room and object features, the human speciﬁes the ground-truth action to go to
the subgoal (an action is speciﬁed by its horizontal and vertical angles, and travel distance).4"
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.4009009009009009,"Experimental Procedure.
We conduct our experiments in three phases. In the pre-training phase,
we learn an operation policy ˆπ with dense descriptions of the current location and the goal. In the
training phase, the agent perceives a sparse description of its current location and is given a sparse
initial goal description. We use advantage actor-critic (Mnih et al., 2016) to learn an interaction pol-
icy ψθ that controls how the agent communicates with the human and navigates in an environment."
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.40540540540540543,"3While our representation of state descriptions simpliﬁes the object/room detection problem for the agent,
it does not necessarily make the navigation problem easier than with image input, as images may contain
information that is not captured by our representation (e.g., object shapes and colors, visualization of paths).
4Here, we emulate a practical scenario where if a destination is visible in the current view, to save effort, a
human would concisely tell an agent what to do rather than giving a verbose description of the destination."
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.4099099099099099,Under review as a conference paper at ICLR 2022
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.4144144144144144,"Table 1: Main results on test sets. For success rate, we report both goal-ﬁnding (normal font) and
request-fulﬁlling results (smaller grey font in parentheses). We also report the average number of
different types of actions taken by the agent (across all task types)."
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.4189189189189189,"Success Rate % ↑
Avg. number of actions ↓
Unseen
Unseen
Unseen
Agent
Start
Object
Environment
CUR GOAL SUB
DO"
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.42342342342342343,"No assistant and interaction policy ψθ
(ds: current-state description, dg: goal description)"
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.42792792792792794,"Sparse ds and dg
43.4 (50.4)
16.4 (23.2)
3.0
(6.8)
-
-
-
13.1
Sparse ds, dense dg
67.2 (68.4)
56.6 (58.2)
9.7 (12.3)
-
-
-
12.6
Dense ds, sparse dg
77.9 (86.0)
30.6 (40.3)
4.1
(7.5)
-
-
-
12.0
Dense ds and dg
97.8 (98.1)
81.7 (83.3)
9.4 (11.9)
-
-
-
11.0"
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.43243243243243246,"With assistant and interaction policy ψθ
Rule-based ψθ (baseline)
78.8 (78.8)
68.5 (68.5)
12.7 (12.7)
2.0
1.0
1.7
11.3
RL-learned ψθ (ours)
85.8 (86.8)
78.2 (79.6)
19.8 (22.5)
2.1
1.0
1.7
11.1
+ Perfect nav. on sub-goals (skyline)
94.3 (95.8)
95.1 (96.1)
92.6 (94.3)
0.0
0.0
6.3
7.3"
LEARNING WHEN AND WHAT TO ASK IN HUMAN-ASSISTED NAVIGATION,0.4369369369369369,"The human always returns dense descriptions. The interaction policy is trained in environments
that are previously seen as well as unseen during pre-training. Finally, in the evaluation phase, the
interaction policy is tested on three conditions: seen environment and target object type but starting
from a new room (UNSEENSTR), seen environment but new target object type (UNSEENOBJ), and
new environment (UNSEENENV). The pre-trained operation policy ˆπ is ﬁxed during the training
and evaluation phases. We create 82,104 examples for pre-training, 65,133 for training, and approx-
imately 2,000 for each validation or test set. Details about the training procedure and the dataset are
included in the Appendix."
RESULTS AND ANALYSES,0.44144144144144143,"6
RESULTS AND ANALYSES"
RESULTS AND ANALYSES,0.44594594594594594,"Settings.
In our main experiments, we set: the cost of taking a CUR, GOAL, SUB, or DO action
to be 0.01 (we will consider other settings subsequently), the cost of calling DONE to terminate
the main goal (i.e. task error) equal the (unweighted) length of the shortest-path from the agent’s
location to the goal, and the goal stack’s size (L) to be 2. We compare our RL-learned interaction
policy with a rule-based baseline that ﬁrst takes the GOAL action and then randomly selects actions.
In each episode, we enforce that the rule-based policy can take at most ⌊Xa⌋+ y actions of type a,
where y ∼Bernoulli(Xa −⌊Xa⌋) and Xa is a constant. We tune each Xa on the validation sets so
that the rule-based policy has the same average count of each action as the RL-learned policy. To
prevent early termination, we enforce that the rule-based policy cannot take more DONE actions than
SUB actions unless when its SUB action’s budget is exhausted. We also construct a skyline where
the interaction policy is also learned by RL but with an operation policy that executes subgoals
perfectly. As discussed in §5, we are primarily interested in goal-ﬁnding success rate and will refer
to this metric brieﬂy as success rate."
RESULTS AND ANALYSES,0.45045045045045046,"Main Results (Table 1).
To inspect the potential beneﬁts of asking for additional information,
we compute how much the operation policy ˆπ improves when it is supplied with dense information
about the current and/or goal states. As seen, success rate of the operation policy is lifted dramat-
ically when both the current-state and goal descriptions are dense (∼2× increase on UNSEENSTR,
∼5× on UNSEENOBJ, and ∼3× on UNSEENENV). We ﬁnd that dense information about the cur-
rent state is more helpful on UNSEENSTR, while dense information about the goal is more valuable
on UNSEENOBJ and UNSEENENV. This is reasonable because on UNSEENSTR, the agent has been
trained to ﬁnd similar goals. In contrast, the initial goal descriptions in UNSEENOBJ and UNSEE-
NENV are completely new to the agent, thus gathering more information about them is necessary."
RESULTS AND ANALYSES,0.45495495495495497,"Aided by our RL-learned interaction policy, the agent observes a substantial ∼2× increase in suc-
cess rate on UNSEENSTR, ∼5× on UNSEENOBJ, and ∼7× on UNSEENENV, compared to when
performing tasks using only the operation policy. In unseen environments, with its capability of
requesting subgoals, the agent impressively doubles the success rate of the operation policy that has
access to dense descriptions. The RL-learned interaction policy is signiﬁcantly more effective than"
RESULTS AND ANALYSES,0.4594594594594595,Under review as a conference paper at ICLR 2022 0 5 10
RESULTS AND ANALYSES,0.46396396396396394,"CUR
DO
GOAL
SUB
Action type"
RESULTS AND ANALYSES,0.46846846846846846,Average actions
RESULTS AND ANALYSES,0.47297297297297297,"unseen_str
unseen_obj
unseen_env"
RESULTS AND ANALYSES,0.4774774774774775,"(a) How frequently does the in-
teraction policy execute differ-
ent actions, on average across
different types of tasks?
Sub-
goals are requested much more
in unseen environments. G G G G G
0.0 0.5 1.0 1.5 2.0"
RESULTS AND ANALYSES,0.481981981981982,"0.2
0.4
0.6
0.8
1.0
Fraction of episode"
RESULTS AND ANALYSES,0.4864864864864865,Average actions
RESULTS AND ANALYSES,0.49099099099099097,"G
CUR
GOAL
SUB"
RESULTS AND ANALYSES,0.4954954954954955,"(b) Over the course of a trajec-
tory, how does the frequency of
different types of actions change
in unseen environments?
Sub-
goals are requested in the mid-
dle, goal information at the be-
ginning."
RESULTS AND ANALYSES,0.5,"Figure 2: Analyzing the behavior of the RL-
learned interaction policy (on validation en-
vironments). G"
RESULTS AND ANALYSES,0.5045045045045045,"G
G G
G G
G
G G 0 25 50 75"
RESULTS AND ANALYSES,0.509009009009009,"−3
−2
−1"
RESULTS AND ANALYSES,0.5135135135135135,log10(cost)
RESULTS AND ANALYSES,0.5180180180180181,Success rate (%)
RESULTS AND ANALYSES,0.5225225225225225,"G
unseen_str
unseen_obj
unseen_env"
RESULTS AND ANALYSES,0.527027027027027,"(a) Effect of cost on success
rate. G"
RESULTS AND ANALYSES,0.5315315315315315,"G
G G
G"
RESULTS AND ANALYSES,0.536036036036036,"G
G
G
G 0 3 6 9"
RESULTS AND ANALYSES,0.5405405405405406,"−3
−2
−1"
RESULTS AND ANALYSES,0.545045045045045,log10(cost)
RESULTS AND ANALYSES,0.5495495495495496,Average actions
RESULTS AND ANALYSES,0.5540540540540541,"G
CUR
GOAL
SUB
DO"
RESULTS AND ANALYSES,0.5585585585585585,"(b) Effect of cost on action
counts."
RESULTS AND ANALYSES,0.5630630630630631,"Figure 3: Analyzing the effect of simultane-
ously varying the cost of the CUR, GOAL,
SUB, DO actions (on validation environ-
ments), thus trading off success rate versus
number of actions taken."
RESULTS AND ANALYSES,0.5675675675675675,"the rule-based baseline (+7.1% on UNSEENENV). Compared to a policy that calls GOAL at the
beginning and calls CUR at every step (which is equivalent to the dense-ds-and-dg baseline), our
policy achieves higher success rate on UNSEENENV while making two times fewer requests. This
is due to the ability to request subgoals."
RESULTS AND ANALYSES,0.5720720720720721,"On UNSEENSTR and UNSEENOBJ, the RL-learned policy has not closed the gap with the operation
policy that performs tasks with dense descriptions. Our investigation ﬁnds that limited information
often causes the policy to not request information about the current state (i.e. taking CUR) and
terminate prematurely or go to a wrong place. Encoding uncertainty in the current-state description
(e.g., Finkel et al. (2006); Nguyen & O’Connor (2015)) is a plausible future direction for tackling
this issue. Finally, results obtained by replacing the learned operation policy with one that behaves
optimally on subgoals shows that further improving performance of the operation policy on short-
distance goals would effectively enhance the agent’s performance on long-distance goals."
RESULTS AND ANALYSES,0.5765765765765766,"Behavior of the RL-Learned Interaction Policy.
Figure 2a characterizes behaviors of the RL-
learned interaction policy in three evaluation conditions. We expect that tasks in UNSEENSTR are
the easiest and those in UNSEENENV are the hardest. As the difﬁculty of the evaluation condition
increases, the interaction policy issues more CUR, SUB, and DO actions. The average number of
GOAL actions does not vary, showing that the interaction policy has correctly learned that making
more than one goal-clarifying request is unnecessary. Figure 2b illustrates the distribution of each
action along the length of an episode in the validation UNSEENENV dataset. The GOAL action,
if taken, is always taken only once and immediately in the ﬁrst step. The number of CUR actions
gradually decreases over time. The agent makes most SUB requests in the middle of an episode,
after its has attempted but failed to accomplish the main goals. We observe similar patterns on the
other two validation sets."
RESULTS AND ANALYSES,0.581081081081081,"Effects of Varying Action Cost.
As mentioned, we assign the same cost to each CUR, GOAL,
SUB, or DO action. Figure 3a demonstrates the effects of changing this cost on the success rate of
the agent. Setting the cost equal to 0.5 makes it too costly to take any action, inducing a policy that
always calls DONE in the ﬁrst step and thus fails on all tasks. Overall, the success rate of the agent
rises as we reduce the action cost. The increase in success rate is most visible in UNSEENENV and
least visible in UNSEENSTR. Figure 3b provides more insights. As the action cost decreases, we
observe a growth in the number of SUB and DO actions taken by the interaction policy. Meanwhile,
the numbers of CUR and GOAL actions are mostly static. Since requesting subgoals is more helpful
in unseen environments than in seen environments, the increase in the number of SUB actions leads
the more visible boost in success rate on UNSEENENV tasks."
RESULTS AND ANALYSES,0.5855855855855856,"Performing Tasks with Deeper Goal Stacks.
In Table 2, we test the functionality of our frame-
work with a stack size 3, allowing the agent to request subgoals of subgoals. As expected, success
rate on UNSEENENV is boosted signiﬁcantly (+11.9% compared to using a stack of size 2). Success
rate on UNSEENOBJ is largely unchanged; we ﬁnd that the agent makes more SUB requests (aver-
agely 4.5 requests per episode compared to 1.0 request made when the stack size is 2), but doing"
RESULTS AND ANALYSES,0.5900900900900901,Under review as a conference paper at ICLR 2022
RESULTS AND ANALYSES,0.5945945945945946,"Table 2: Success rates and numbers of actions taken with different stack sizes (on validation). Larger
stack sizes signiﬁcantly aid success rates in unseen environments, but not in seen environments."
RESULTS AND ANALYSES,0.5990990990990991,"Goal-ﬁnding success rate (%) ↑
Average number of actions ↓
Unseen
Unseen
Unseen
Stack size
Start
Object
Environment
CUR
GOAL
SUB
DO"
RESULTS AND ANALYSES,0.6036036036036037,"1 (no subgoals)
92.2
78.4
12.5
5.1
1.9
0.0
10.7
2
86.9
77.6
21.6
2.1
1.0
1.7
11.2
3
83.2
78.6
33.5
1.3
1.0
5.0
8.2"
RESULTS AND ANALYSES,0.6081081081081081,"so does not further enhance performance. The agent makes less CUR requests, possibly in order to
offset the cost of making more SUB requests. Due to this behavior, success rate on UNSEENSTR
declines with larger stack sizes, as information about the current state is more valuable for these
tasks than subgoals. These results show that the critic model overestimates the V values in states
where SUB actions are taken, leading to the agent learning to request subgoals more than needed."
RELATED WORK AND CONCLUSION,0.6126126126126126,"7
RELATED WORK AND CONCLUSION"
RELATED WORK AND CONCLUSION,0.6171171171171171,"Transfer Learning in Reinforcement Learning.
Various frameworks have been proposed to
model knowledge transfer from a more capable agent to a novice one (Da Silva & Costa, 2019).
Torrey & Taylor (2013) introduce the action-advising framework where a learner strategically re-
quests reference actions from a teacher. Da Silva et al. (2020) investigate uncertainty-based strate-
gies for deciding when to request in this framework. In an agent-to-agent setting, (Da Silva et al.,
2017; Zimmer et al., 2014; Omidshaﬁei et al., 2019) focus on learning a teaching policy in addition
to an advice-requesting policy. An important assumption in these papers is that the teacher must
share a common action space with the learner. More recent frameworks (Kim et al., 2019; Nguyen
et al., 2019; Nguyen & Daum´e III, 2019) relax this assumption by allowing the teacher to specify
high-level subgoals instead of low-level actions. HARI can be viewed as a strict extension of these
frameworks. It allows the human to specify not only subgoals, but also any additional information
about the current state and the goal that agent can interpret. Moreover, HARI equips the agent with
multiple communication intentions and teaches it to select the most useful intention to convey in a
given situation. Another line of work employs standard RL communication protocol, where the hu-
man can only transfer knowledge through numerical scores or categorical feedback (Knox & Stone,
2009; Judah et al., 2010; Peng et al., 2016; Grifﬁth et al., 2013). Maclin & Shavlik (1996) propose
a framework where the human advises the agent using a domain-speciﬁc language, specifying rules
that can be incorporated into the agent’s model. In contrast, HARI operates with a black-box agent
model. Sumers et al. (2020) extract features from various types of language feedback to construct a
reward function for reinforcement learning. We instead focus on deployment-time communication
and directly incorporate the human feedback as input to agent’s operation policy."
RELATED WORK AND CONCLUSION,0.6216216216216216,"Task-Oriented Dialog and Generating Natural Language Questions.
HARI models a task-
oriented dialog problem. Many variants of this problem requires the agent to compose speciﬁc
questions (De Vries et al., 2017; Das et al., 2017; Thomason et al., 2020). The dominant approach
in these problems is to mimic pre-collected human utterances. As discussed previously, naively mir-
roring human external behavior cannot enable agents to understand the limits of their knowledge.
We teach the agent to understand its intrinsic needs through interaction with the human and the en-
vironment rather than through imitation of human behaviors. Another related line of work concerns
generating natural language explanations of model decisions (Camburu et al., 2018; Hendricks et al.,
2016; Rajani et al., 2019)."
RELATED WORK AND CONCLUSION,0.6261261261261262,"In summary, this paper presents a general POMDP framework for modeling human-agent commu-
nication. While we demonstrate this framework on a simpliﬁed navigation problem, our framework
can theoretically capture richer types of human-agent communication. Hence, an important empir-
ical question is how well our formulation generalizes to richer environments with more complex
interactions and state spaces (Shridhar et al., 2020). Enhancing the sample efﬁciency of the learning
policy by exploiting the hierarchical policy structure is an exciting future direction. Furthermore,
techniques for generating faithful explanations (Kumar & Talukdar, 2020; Madsen et al., 2021) can
be applied to enhance the speciﬁcity of the generated questions."
RELATED WORK AND CONCLUSION,0.6306306306306306,Under review as a conference paper at ICLR 2022
REFERENCES,0.6351351351351351,REFERENCES
REFERENCES,0.6396396396396397,"Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S¨underhauf, Ian Reid,
Stephen Gould, and Anton Van Den Hengel.
Vision-and-language navigation: Interpreting
visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 3674–3683, 2018."
REFERENCES,0.6441441441441441,"Oana-Maria Camburu, Tim Rockt¨aschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural
language inference with natural language explanations. In Proceedings of Advances in Neural
Information Processing Systems, 2018."
REFERENCES,0.6486486486486487,"Felipe Leno Da Silva and Anna Helena Reali Costa. A survey on transfer learning for multiagent
reinforcement learning systems. Journal of Artiﬁcial Intelligence Research, 64:645–703, 2019."
REFERENCES,0.6531531531531531,"Felipe Leno Da Silva, Ruben Glatt, and Anna Helena Reali Costa. Simultaneously learning and
advising in multiagent reinforcement learning. In Proceedings of the 16th conference on au-
tonomous agents and multiagent systems, pp. 1100–1108, 2017."
REFERENCES,0.6576576576576577,"Felipe Leno Da Silva, Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. Uncertainty-
aware action advising for deep reinforcement learning agents. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, volume 34, pp. 5792–5799, 2020."
REFERENCES,0.6621621621621622,"Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos´e MF Moura, Devi
Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 326–335, 2017."
REFERENCES,0.6666666666666666,"Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron
Courville. Guesswhat?! visual object discovery through multi-modal dialogue. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5503–5512, 2017."
REFERENCES,0.6711711711711712,"Jenny Rose Finkel, Christopher D Manning, and Andrew Y Ng. Solving the problem of cascading
errors: Approximate bayesian inference for linguistic annotation pipelines. In Proceedings of the
2006 Conference on Empirical Methods in Natural Language Processing, pp. 618–626, 2006."
REFERENCES,0.6756756756756757,"Herbert P Grice. Logic and conversation. In Speech acts, pp. 41–58. Brill, 1975."
REFERENCES,0.6801801801801802,"Shane Grifﬁth, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz.
Policy shaping: Integrating human feedback with reinforcement learning. Georgia Institute of
Technology, 2013."
REFERENCES,0.6846846846846847,"Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In
2015 aaai fall symposium series, 2015."
REFERENCES,0.6891891891891891,"Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor
Darrell. Generating visual explanations. In European conference on computer vision, pp. 3–19.
Springer, 2016."
REFERENCES,0.6936936936936937,"Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. A recurrent
vision-and-language bert for navigation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2020."
REFERENCES,0.6981981981981982,"Kshitij Judah, Saikat Roy, Alan Fern, and Thomas Dietterich. Reinforcement learning via practice
and critique advice. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 24,
2010."
REFERENCES,0.7027027027027027,"Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra.
Planning and acting in
partially observable stochastic domains. Artiﬁcial intelligence, 101(1-2):99–134, 1998."
REFERENCES,0.7072072072072072,"Dong-Ki Kim, Miao Liu, Shayegan Omidshaﬁei, Sebastian Lopez-Cot, Matthew Riemer, Golnaz
Habibi, Gerald Tesauro, Sami Mourad, Murray Campbell, and Jonathan P How. Learning hierar-
chical teaching policies for cooperative agents. arXiv preprint arXiv:1903.03216, 2019."
REFERENCES,0.7117117117117117,"W Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The tamer
framework. In Proceedings of the ﬁfth international conference on Knowledge capture, pp. 9–16,
2009."
REFERENCES,0.7162162162162162,Under review as a conference paper at ICLR 2022
REFERENCES,0.7207207207207207,"Sawan Kumar and Partha Talukdar. NILE : Natural language inference with faithful natural language
explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, pp. 8730–8742, Online, July 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.771. URL https://aclanthology.org/2020.acl-main.771."
REFERENCES,0.7252252252252253,"Richard Maclin and Jude W Shavlik.
Creating advice-taking reinforcement learners.
Machine
Learning, 22(1):251–281, 1996."
REFERENCES,0.7297297297297297,"Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. Evaluating the faithfulness
of importance measures in nlp by recursively masking allegedly important tokens and retraining.
arXiv preprint arXiv:2110.08412, 2021."
REFERENCES,0.7342342342342343,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016."
REFERENCES,0.7387387387387387,"Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Icml, volume 99, pp. 278–287, 1999."
REFERENCES,0.7432432432432432,"Khanh Nguyen and Hal Daum´e III. Help, anna! visual navigation with natural multimodal assistance
via retrospective curiosity-encouraging imitation learning. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP), November 2019. URL https:
//arxiv.org/abs/1909.01871."
REFERENCES,0.7477477477477478,"Khanh Nguyen and Brendan O’Connor. Posterior calibration and exploratory analysis for natural
language processing models. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 1587–1598, Lisbon, Portugal, September 2015. Association
for Computational Linguistics. doi: 10.18653/v1/D15-1182. URL https://www.aclweb.org/
anthology/D15-1182."
REFERENCES,0.7522522522522522,"Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. Vision-based navigation with
language-based assistance via imitation learning with indirect intervention. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), June 2019. URL https://arxiv.
org/abs/1812.04155."
REFERENCES,0.7567567567567568,"Shayegan Omidshaﬁei, Dong-Ki Kim, Miao Liu, Gerald Tesauro, Matthew Riemer, Christopher
Amato, Murray Campbell, and Jonathan P How. Learning to teach in cooperative multiagent
reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 33, pp. 6128–6136, 2019."
REFERENCES,0.7612612612612613,"Bei Peng, James MacGlashan, Robert Loftin, Michael L Littman, David L Roberts, and Matthew E
Taylor. A need for speed: Adapting agent action speed to improve task learning from non-expert
humans. In Proceedings of the International Joint Conference on Autonomous Agents and Multi-
agent Systems, 2016."
REFERENCES,0.7657657657657657,"Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!
leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019."
REFERENCES,0.7702702702702703,"Sudha Rao and Hal Daum´e III. Learning to ask good questions: Ranking clariﬁcation questions us-
ing neural expected value of perfect information. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 2737–2746, Melbourne,
Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1255.
URL https://aclanthology.org/P18-1255."
REFERENCES,0.7747747747747747,"St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artiﬁcial intelligence and statistics, pp. 627–635. JMLR Workshop and Conference
Proceedings, 2011."
REFERENCES,0.7792792792792793,"Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,
Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions
for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 10740–10749, 2020."
REFERENCES,0.7837837837837838,Under review as a conference paper at ICLR 2022
REFERENCES,0.7882882882882883,"Theodore R Sumers, Mark K Ho, Robert D Hawkins, Karthik Narasimhan, and Thomas L Grifﬁths.
Learning rewards from linguistic feedback. arXiv preprint arXiv:2009.14715, 2020."
REFERENCES,0.7927927927927928,"Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navi-
gation. In Conference on Robot Learning, pp. 394–406. PMLR, 2020."
REFERENCES,0.7972972972972973,"Lisa Torrey and Matthew Taylor. Teaching on a budget: Agents advising agents in reinforcement
learning. In Proceedings of the 2013 international conference on Autonomous agents and multi-
agent systems, pp. 1053–1060, 2013."
REFERENCES,0.8018018018018018,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.8063063063063063,"Matthieu Zimmer, Paolo Viappiani, and Paul Weng. Teacher-student framework: a reinforcement
learning approach. In AAMAS Workshop Autonomous Robots and Multirobot Systems, 2014."
REFERENCES,0.8108108108108109,Under review as a conference paper at ICLR 2022
REFERENCES,0.8153153153153153,"A
TRAINING PROCEDURE"
REFERENCES,0.8198198198198198,"Cost function.
The cost function in our framework is given as follows"
REFERENCES,0.8243243243243243,"¯c(st, Gt, ¯at) ="
REFERENCES,0.8288288288288288,"


 

"
REFERENCES,0.8333333333333334,"c(st, ado
t )
if ¯at = DO
γ¯at
if ¯at ∈{CUR, GOAL, SUB},
c(st, adone)
if ¯at = DONE, |Gt| = 1
0
if ¯at = DONE, |Gt| > 1, (6)"
REFERENCES,0.8378378378378378,"Subgoals.
Let pt be the shortest path from the agent’s current state st to the current goal gt, and
pt,i be the i-th node on the path (0 ≤i < |pt|). The subgoal location is chosen as pt,k where
k = min(⌊|p|/2⌋, lmax), where lmax is a pre-deﬁned constant."
REFERENCES,0.8423423423423423,"Training Algorithms.
We pre-train the operation policy ˆπ with DAgger (Ross et al., 2011), mini-
mizing the cross entropy between its action distribution with that of a shortest-path oracle (which is
a one-hot distribution with all probability concentrated on the optimal action)."
REFERENCES,0.8468468468468469,"We use advantage actor-critic (Mnih et al., 2016) to train the interaction policy ψθ. This method
simultaneously estimates an actor policy ψθ : ¯B →∆( ¯
A) and a critic function Vφ : ¯B →R. Given
an execution ¯τ = (¯s1, ¯a1, ¯c1 · · · , ¯sH), the gradients with respect to the actor and critic are"
REFERENCES,0.8513513513513513,"∇θLactor = H
X t=1"
REFERENCES,0.8558558558558559," 
Vφ(¯bv
t ) −Ct

∇θ log ψθ(¯at | ¯ba
t )
(7)"
REFERENCES,0.8603603603603603,"∇φLcritic = H
X t=1"
REFERENCES,0.8648648648648649," 
Vφ(¯bv
t ) −Ct

∇φVφ(¯bv
t )
(8)"
REFERENCES,0.8693693693693694,"where Ct = PH
j=t cj, ¯ba
t is a belief state that summarizes the partial execution ¯τ1:t for the actor, and
¯bv
t is a belief state for the critic."
REFERENCES,0.8738738738738738,"Cost function.
The cost function introduced in §4.4 is not effective for learning the interaction
policy because the task error is given only at the end of an episode. We extend the reward-shaping
method proposed by Ng et al. (1999) to goal-conditioned policies, augmenting the original cost
function with a shaping function Φ(s, g) with s, g ∈S. We set Φ(s, g) to be the (unweighted)
shortest-path distance from s to g. The cost received by the agent at time step t is ˜ct ≜¯ct +
Φ(st+1, gt+1) −Φ(st, gt). We assume that the agent transitions to a special terminal state sterm ∈S
and remains there after it terminates execution of the main goal. We set Φ(sterm, None) = 0, where
gt = None signals that the episode has ended. Hence, the cumulative cost of an execution under the
new cost function is
H
X"
REFERENCES,0.8783783783783784,"t=1
˜ct = H
X"
REFERENCES,0.8828828828828829,"t=1
¯ct + Φ(st+1, gt+1) −Φ(st, gt) = H
X"
REFERENCES,0.8873873873873874,"t=1
¯ct −Φ(s1, g1)
(9)"
REFERENCES,0.8918918918918919,"Since Φ(s1, g1) does not depend on the action taken in s1, minimizing the new cumulative cost does
not change the optimal policy for the task (s1, g1)."
REFERENCES,0.8963963963963963,"Model Architecture.
We adapt the V&L BERT architecture (Hong et al., 2020) for modeling the
operation policy ˆπ. Our model has two components: an encoder and a decoder; both are imple-
mented as Transformer models (Vaswani et al., 2017). The encoder takes as input a description
ds
t or dg
t and generates a sequence of hidden vectors. In every step, the decoder takes as input the
previous hidden vector bs
t−1, the sequence of vectors representing ds
t, and the sequence of vectors
representing dg
t . It then performs self-attention on these vectors to compute the current hidden vector
bs
t and a probability distribution over navigation actions pt."
REFERENCES,0.9009009009009009,"The interaction policy ψθ (the actor) is an LSTM-based recurrent neural network. The input of
this model is the operation policy’s model outputs, bs
t and pt, and the embedding of the previously
taken action ¯at−1. The critic model also has a similar architecture but outputs a real number (the
V value) rather than an action distribution. When training the interaction policy, we always ﬁx
the parameters of the operation policy. We ﬁnd it necessary to pre-train the critic before training it
jointly with the actor."
REFERENCES,0.9054054054054054,Under review as a conference paper at ICLR 2022
REFERENCES,0.9099099099099099,Table 3: Dataset statistics.
REFERENCES,0.9144144144144144,"Split
Number of examples"
REFERENCES,0.918918918918919,"Pre-training
82,104
Pre-training validation
3,000
Training
65,133
Validation UNSEENSTR
1,901
Validation UNSEENOBJ
1,912
Validation UNSEENENV
1,967
Test UNSEENSTR
1,653
Test UNSEENOBJ
1,913
Test UNSEENENV
1,777"
REFERENCES,0.9234234234234234,"Representation of State Descriptions.
The representation of each object, room, or action is com-
puted as follows. Let f name, f horz, f vert, f dist, and f type are the features of an object f, consisting
of its name, horizontal angle, vertical angle, distance, and type (a type is either Object, Room, or
Action; in this case, the type is Object). For simplicity, we discretize real-valued features, result-
ing in 12 horizontal angles (corresponding to π/6 · k, 0 ≤k < 12), 3 vertical angles (corresponding
to π/6 · k, −1 ≤k ≤1), and 5 distance values (we round down a real-valued distance to the nearest
integer). We then lookup the embedding of each feature from an embedding table and sum all the
embeddings into a single vector that represents the corresponding object. For a room, f horz, f vert
f dist are zeroes. For an action, f name is either ActionStop for the stop action adone or ActionGo
otherwise."
REFERENCES,0.9279279279279279,"During pre-training, we randomly drop features in ds
t and dg
t so that the operation policy is familiar
with making decisions under sparse information. Concretely, we refer to all features of an object,
room or action as a feature set. For ds
t, let M be the number objects in a description. We uniformly
randomly keep m feature sets among the M +1 feature sets of ds
t (the plus one is the room’s feature
set), where m ∼Uniform(min(5, M + 1), M + 1)."
REFERENCES,0.9324324324324325,"For ds
t, we have two cases. If g1 is not adjacent or equals to s1, we uniformly randomly alternate
between giving a dense and a sparse description. In this case, the sparse description contains the
features of the target object and the goal room’s name. Otherwise, with a probability of 1⁄3, we give
either (a) a dense description (b) a (sparse) description that contains the target object’s features and
the goal room’s name, or (c) a (sparse) description that describes the next ground-truth action."
REFERENCES,0.9369369369369369,"We pre-train the operation policy on various path lengths (ranging from 1 to 10 graph nodes) so that
it learns to accomplish both long-distance main goals and short-distance subgoals."
REFERENCES,0.9414414414414415,"Data.
Table 3 summarizes the data splits. From a total of 72 environments provided by the Matter-
port3D dataset, we use 36 environments for pre-training, 18 as unseen environments for training, 7
for validation UNSEENENV, and 11 for test UNSEENENV. We use a vocabulary of size 1738, which
includes object and room names, and special tokens representing the distance and direction values.
The length of a navigation path ranges from 5 to 10 graph nodes."
REFERENCES,0.9459459459459459,"Hyperparameters.
See Table 4."
REFERENCES,0.9504504504504504,Under review as a conference paper at ICLR 2022
REFERENCES,0.954954954954955,Table 4: Hyperparameters.
REFERENCES,0.9594594594594594,"Hyperparameter Name
Value"
REFERENCES,0.963963963963964,"Environment
Max. subgoal distance (lmax)
3 nodes
Max. stack size (L)
2
Max. object distance for ds
t
5 meters
Max. object distance for dg
t
3 meters
Max. number of objects (Mmax)
20
Cost of taking each CUR, GOAL, SUB, DO action
0.01"
REFERENCES,0.9684684684684685,"Operation policy ˆπ
Hidden size
256
Number of hidden layers
2
Attention dropout probability
0.1
Hidden dropout probability
0.1
Number of attention heads
8
Optimizer
Adam
Learning rate
10−4"
REFERENCES,0.972972972972973,"Batch size
32
Number of training iterations
105"
REFERENCES,0.9774774774774775,"Max. number of time steps (H)
15"
REFERENCES,0.9819819819819819,"Interaction policy ψθ
Hidden size
512
Number of hidden layers
1
Entropy regularization weight
0.001
Optimizer
Adam
Learning rate
10−5"
REFERENCES,0.9864864864864865,"Batch size
32
Number of critic pre-training iterations
5 × 103"
REFERENCES,0.990990990990991,"Number of training iterations
5 × 104"
REFERENCES,0.9954954954954955,"Max. number of time steps (H)
30
Max. number of time steps for executing a subgoal
3× shortest distance to the subgoal"
