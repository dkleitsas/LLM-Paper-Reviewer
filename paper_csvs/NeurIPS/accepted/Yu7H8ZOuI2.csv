Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010070493454179255,"The problem of finding suitable point embedding or geometric configurations given
only Euclidean distance information of point pairs arises both as a core task and as
a sub-problem in a variety of machine learning applications. In this paper, we aim
to solve this problem given a minimal number of distance samples. To this end, we
leverage continuous and non-convex rank minimization formulations of the problem
and establish a local convergence guarantee for a variant of iteratively reweighted
least squares (IRLS), which applies if a minimal random set of observed distances
is provided. As a technical tool, we establish a restricted isometry property (RIP)
restricted to a tangent space of the manifold of symmetric rank-r matrices given
random Euclidean distance measurements, which might be of independent interest
for the analysis of other non-convex approaches. Furthermore, we assess data
efficiency, scalability and generalizability of different reconstruction algorithms
through numerical experiments with simulated data as well as real-world data,
demonstrating the proposed algorithm’s ability to identify the underlying geometry
from fewer distance samples compared to the state-of-the-art.
The Matlab code can be found at github_EDG-IRLS"
INTRODUCTION,0.002014098690835851,"1
Introduction"
INTRODUCTION,0.0030211480362537764,"Euclidean Distance Geometry (EDG) problems have applications spanning diverse domains, from
comprehending protein structures through molecular conformations [MMLL19, MMWL22], pre-
diction of molecular conformations in computational chemistry [DPRV15, SWBB97] and aiding
dimensional reduction in machine learning [TSL00] to facilitating localization in sensor networks
[FQX19], coupled with its role in solving partial differential equations on manifolds [LL17]. This
emphasizes its broad impact in computational sciences. [LLMM14] proposes embedding entities
based on Distance Geometry Problems (DGP), where object positions are determined based on a
subset of pairwise distances or inner products which significantly reduces computational complexity
compared to traditional word embedding methods.
Problem 1. Mathematically, consider a collection of n points pi in an r-dimensional Euclidean space
with coordinates P = [p1, p2, ..., pn] ∈Rr×n whose pairwise squared Euclidean distances are given
by d2
ij = ||pi −pj||2 for each 1 ≤i ̸= j ≤n where || · || denotes the Euclidean norm. Given only
partial information of the {dij} such that only a subset of cardinality m < n(n −1)/2 is known, the
goal is to reconstruct the geometry of the points, that is to recover the point coordinates P."
INTRODUCTION,0.004028197381671702,"If all the distances between the points are provided, the problem is known as multidimensional scaling
[LT24, GCKG23], and closed solution formula exists. However, this is not the case for the incomplete
setup which is the focus of this work, and in which only partial information of the pairwise distances
is available."
INTRODUCTION,0.005035246727089627,"For instance, AlphaFold [SEJ+20, JEP+21] has shown effectiveness in predicting the three-
dimensional structure of protein given as input its amino acid sequence. AlphaFold2, which uses an
attention mechanism-based transformer architecture [VSP+17], is trained on known sequences and
structures from the Protein Data Bank [HMBFGG+00] and determines the distances between the Cα
atoms of all residue pairs in a protein. Subsequently, as a subproblem, this distance information is
used to predict the protein’s structure by identifying the foldings within the protein molecule. In the
context of this subproblem, where the input is the predicted pairwise distances, a linear mapping
can be used to predict geometric coordinates. However, many of the predicted distances might not
be accurate, which is why low confidence regions of the resulting structures, as indicated by the
predicted local-distance difference test (pLDDT) [JEP+21], could be masked, and a new structure
could subsequently be re-computed based on the distance entries of the medium-to-high confidence
regions using high-accuracy solution algorithms for Problem 1."
INTRODUCTION,0.006042296072507553,"In the context of the Problem 1, we have access to a subset of entries of this D = [d2
ij] matrix, defined
by the index set Ω⊂{1, . . . , n}2. Now, we formulate the Gram matrix X0 = P⊤P, residing in the
set of real-valued symmetric matrices Sn. This matrix has a lower rank r compared to the symmetric
distance matrix D ∈Sn = {X ∈Rn×n : X = X⊤}, which has a rank of r + 2 [DPRV15]."
INTRODUCTION,0.007049345417925478,"To make the solution translation invariant, the centroid of the points must be the origin, i.e.,
Pn
ℓ=1 pℓ= 0. To ensure this, the Gram matrix should satisfy also satisfy X0 · 1 = 0, where
1 ∈Rn is the vector of all ones. Based on these observations, we can frame Problem 1 as an instance
of the computationally challenging NP-hard rank minimization [Rec11] problem defined by"
INTRODUCTION,0.008056394763343404,"min
X∈Sn,X·1=0,X⪰0 rank(X)
subject to Xi,i + Xj,j −2Xi,j = Di,j
for all (i, j) ∈Ω,
(1)"
INTRODUCTION,0.00906344410876133,"incorporating also the positive semi-definiteness constraint X ⪰0, which comes from the observation
that X0 = P⊤P is also PSD."
INTRODUCTION,0.010070493454179255,"From an optimization perspective, this rank-minimization problem is highly complex due to its non-
convexity and non-smoothness. Being an NP-hard problem [TL18, Hen95, MW97], it is extremely
difficult to solve directly. Consequently, a significant amount of existing research [Rec11, DR16,
CR09, CW18, Gro11, TL18] has focused on minimizing the convex envelope of the rank function,
known as the nuclear norm, instead. However, it was observed that for related low-rank matrix
completion problem, such convex relaxations are not as data-efficient as other formulations [ALMT14,
TW13], while posing also computational limitations [CC14]."
INTRODUCTION,0.011077542799597181,"In this paper, we study the question of, given incomplete distance information Problem 1, how
many samples are necessary to ensure accurate recovery? [TL18] established convergence with
O(νrn log2 n) uniformly random samples for solving (1) using a nuclear norm surrogate. While
there are numerous results for non-convex methods in other low-rank recovery problems [KV21,
CLC19, Van13], there are currently only few non-convex algorithm specifically designed for EDG
problems available [TL18, SLCT23, ZCZ22, NKKS19, LS24]. To the best of our knowledge, only
[NKKS19, LS24] provide convergence guarantees in the non-convex setting. Unlike generic low-
rank matrix recovery problems [RFP10], EDG problems share the complexities of low-rank matrix
completion problems, such as the absence of a direct uniform null space property [RXH11] or a
restricted isometry property [RFP10]. Additionally, the underlying measurement basis in EDG
problems is not orthonormal, making it impossible to directly use the analysis of standard matrix
completion to provide guarantees for techniques using the Gram matrix-based low-rank modelling
(1) of Problem 1."
OUR CONTRIBUTION,0.012084592145015106,"1.1
Our Contribution"
OUR CONTRIBUTION,0.013091641490433032,"In this paper, we propose and analyze an algorithm for the EDG problem based on the iteratively
reweighted least squares framework (MatrixIRLS, see Section 3), for which we show that m =
Ω(νrn log(n)) (where ν is the coherence factor) randomly sampled distances are sufficient to
guarantee local convergence to the ground truth, X0 with a quadratic rate as shown in Theorem 4.3,
from which the geometry of the points P is trivially recovered. The sample complexity assumption"
OUR CONTRIBUTION,0.014098690835850957,"of Theorem 4.3 matches the lower bound of low-rank matrix completion problems as established
in [CT10]. In Section 4, we construct a dual basis (Lemma 4.4) that spans Sn which enables us to
show the restricted isometry property (RIP) restricted to a tangent space of the manifold of symmetric
rank-r in Theorem 4.5 for our approach, which can be of independent interest for the analysis of
other nonconvex algorithms."
OUR CONTRIBUTION,0.015105740181268883,"While the convergence statement of Theorem 4.3 only applies in a local neighborhood of a ground
truth, the indicated data-efficiency of the proposed method is numerically validated through different
experiments in Section 5 on synthetic and real data in comparison to the state-of-the-art methods.
Furthermore, we demonstrate that MatrixIRLS method is robust to ill-conditioned data, further
highlighting its flexibility. In the Supplementary material, we discuss the limitations in Appendix B.
Additionally, we provide proofs related to the the theoretical results of Section 4 in Appendices B
and C. We further discuss the numerical considerations of our experiments in Appendix E. We discuss
about the computational complexity of the algorithm in detail in Appendix F."
RELATED WORK,0.016112789526686808,"2
Related Work"
RELATED WORK,0.017119838872104734,"Early research on EDG problems focused on establishing its mathematical properties like defining
the conditions under which a distance matrix can be represented in Euclidean space[Gow85, YH38,
YH38, Alf05].A comprehensive overview of EDG applications, including molecular conformations,
wireless sensor networks, robotics, and manifold learning, is available in [Lib20]. Motivated by
the molecular conformation problem, [BJ95] relates this Euclidean distance matrix completion
to a graph realization problem by showing that such matrices can be completed if the graph is
chordal. Other than graph theoretic approaches, as a technical tool, various optimization strategies
[Hen95, MW97, Tro00] have been deployed to solve EDG problems. [AKW99] proposes a primal-
dual interior point algorithm that solves an equivalent semi-definite programming problem. However,
none these works provide theoretical reconstruction guarantees in the incomplete setup of Problem 1."
RELATED WORK,0.01812688821752266,"While providing an accurate modelling of Problem 1, the rank minimization formulation (1) poses
challenges due to its non-convex and non-smooth nature. There is a mature existing literature
[DR16, CR09, CW18, Gro11] around replacing the rank function, rank(X), with the sums of
its singular values σi(X) (also known as the nuclear norm). Building on the rank minimization
formulation (1) of the EDG problem, [TL18] minimizes the convex nuclear norm surrogate of the
inferred Gram matrix. They propose a dual basis approach that enables a theoretical guarantee for
this type of nuclear norm minimization formulation of the EDG problem."
RELATED WORK,0.019133937562940583,"From a practical point of view, it is well-known that the convex approach is computationally intensive,
as the arithmetic complexity is cubic in n, the dimension of X0 [CC14]. It also tends to demand more
data samples than non-convex alternatives, making it less efficient in terms of data [ALMT14, TW13]."
RELATED WORK,0.02014098690835851,"To mitigate these issues, recent studies have shifted focus to non-convex methods such as matrix
factorization [SL16, MWCC20, ZCZ22]. These methods optimize a function involving a data-fit
objective regularized by squared Frobenius norms of the factor matrices, which are computationally
more feasible and data-efficient. Few “non-convex” algorithms are based on matrix factorization like
the work in [BM03]."
RELATED WORK,0.021148036253776436,"Based on a similar formulation, ScaledSGD [ZCZ22] is a preconditioned stochastic gradient descent
method aimed at robustness for ill-conditioned problems. Additionally, some of the most effective
techniques for low-rank matrix completion involve minimizing smooth objectives on the Riemannian
manifold of fixed-rank matrices, providing scalability and the potential to reconstruct the matrix with
fewer samples, although they lack strong performance guarantees [Van13, WCCL20, BA15, BNZ21].
ReiEDG [SLCT23] is a Riemannian-based gradient descent strategy utilizing the sampling operator
on Ω, which is non-convex approach to solving the EDG problem. However, it does not provide
convergence guarantees. To the best of our knowledge, [NKKS19, LS24] are the non-convex
approaches for solving the EDG problem in the Gram-matrix-based low-rank modeling (1) of
Problem 1. The work in [NKKS19] proposes an algorithm based on Riemannian optimization over a
manifold and provides convergence guarantees. These guarantees are derived from extended Wolfe
conditions. However, it is not explicitly detailed how these convergence guarantees depend on
problem parameters such as the sampling model and the number of samples (see Remark III.8 in
[NKKS19]). Similarly, the study in [LS24] employs a Riemannian framework and provides local
convergence guarantees under Bernoulli sampling. Nonetheless, [LS24] does not clarify whether"
RELATED WORK,0.022155085599194362,"the proposed algorithm achieves local linear convergence. In contrast to these studies, the algorithm
proposed in this paper achieves local quadratic convergence under uniform sampling of the distances."
RELATED WORK,0.023162134944612285,"To handle the non-smoothness and non-convexity of rank minimization problems of the type (1),
iteratively reweighted least squares (IRLS) algorithms take a different route than methods based on
[BM03] or Riemannian methods by minimizing a sequence of quadratic majorizing functions of
smoothed rank surrogates [DDFG10, FRW11, MF10, KS18, KV21]. IRLS algorithms have been
studied extensively over the years, as indicated by [Law61, GR97, Bur12]. In the context of low-
rank matrix completion, IRLS algorithms are known to be among the most data-efficient methods
available, while being amenable to a rigorous convergence analysis [FRW11, MF10, KS18, KV21].
Most recently, [KV21] provided an improvement on previous instantiations of the IRLS framework
[FRW11, MF10, KS18] for low-rank optimization problems by providing an improved reweighting
strategy, for which the authors show a local convergence guarantee that is applicable for low-rank
matrix completion, given random entrywise samples of minimal sample complexity. The algorithm
we propose in Section 3 is similar to [KV21, Algorithm 1] and Theorem 4.3 follows partially the
proof strategy of a related result in [KV21]. However, the setup of Problem 1 does not allow a direct
adaptation of both the implementation and analysis of [KV21] due to the non-orthogonality of the
measurement basis."
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.02416918429003021,"3
MatrixIRLS for Euclidean Distance Geometry"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.025176233635448138,"In this section, we provide a detailed outline and description of the iteratively reweighted least squares
method in the context of the EDG reconstruction problem. To this end, we define preliminaries for
stating the algorithm. MatrixIRLS, defined in Algorithm 1 below, can be interpreted as a hybrid of a
smoothing method [Che12] and a Majorization-Minimization algorithm [SBP17]. In particular, the
proposed algorithm minimizes smoothed log-det objectives defined as Fϵ(X) := Pn
i=1 fϵ(σi(X))"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.026183282980866064,fϵ(σ) =
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.027190332326283987,"(
log |σ|,
if σ ≥ϵ,"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.028197381671701913,log(ϵ) + 1
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.02920443101711984,"2

σ2"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.030211480362537766,"ϵ2 −1

,
if σ < ϵ,
(2)"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.03121852970795569,which is a continuously differentiable function with ϵ−2-Lipschitz gradient [KV21].
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.032225579053373615,"We can decompose X by X = U dg(γσ)U⊤, where γ ∈{+1, −1}, It is clear that the optimization
landscape of Fϵ crucially depends on the smoothing parameter ϵ. Instead of minimizing Fϵ directly,
our method minimizes, for k ∈N, ϵk > 0 and X(k) a quadratic model that is related to the second-
order Taylor expansion of the function fϵ at the current iterate and its information is encoded in a
weight operator [KV21] defined below in Definition 3.1."
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.03323262839879154,"Definition 3.1 ([KV21]). X ∈Sn be a matrix with singular value decomposition X(k) =
U dg(γσ)U⊤, where γ ∈{+1, −1} i.e., U ∈Sn are orthonormal matrices. We define the weight
operator core matrix HX,ε ∈Sn of X for smoothing parameter ε > 0 such that"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.03423967774420947,"(HX,ε)ij :=

max(σi, ϵ) max(σj, ϵ)
−1
for each i, j ∈{1, . . . , n} and the weight operator WX,ε :
Sn →Sn, which maps any Z ∈Sn to"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.035246727089627394,"WX,ε(Z) := U

HX,ε ◦(U⊤ZU)

U⊤,
(3)"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.03625377643504532,where ◦denotes the entrywise ore Hadamard product of two matrices.
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.03726082578046324,"Our method is designed to provide iterates that satisfy the constraints of the formulation (1) at each
iteration. They can be encoded using the following definition."
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.038267875125881166,"Definition 3.2. Given n ∈N, let I = {α = (i, j) | 1 ≤i < j ≤n} be the index set of upper
triangular indices."
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.03927492447129909,"We define the operator basis {wα}α∈I∪{(i,i):i∈{1,...,n}} where"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.04028197381671702,"wα =
eie⊤
i + eje⊤
j −eie⊤
j −eje⊤
i ,
if α = (i, j) ∈I,
1
2(ei1⊤+ 1e⊤
i ),
if α = (i, i) for some i ∈{1, . . . , n},
(4)"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.041289023162134945,where ei ∈Rn is the i-th standard basis vector.
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.04229607250755287,"Given a set (or multiset) of indices Ω= {α1, . . . , αm} ⊂I of cardinality m = |Ω|, we define the
measurement operator A = AΩ: Sn →Rm+n which maps X ∈Sn to A(X) whose ℓ-th coordinate
is defined as A(X)ℓ= ⟨wαℓ, X⟩for ℓ≤m and as A(X)ℓ= ⟨w(ℓ−m,ℓ−m), X⟩for ℓ> m."
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.0433031218529708,"The basis {wα}α of Definition 3.2 can be considered as an extended definition of the primal basis used
in [TL18, LT24], but additionally is able to encode the constraint X · 1 = 0 which guarantees that the
Gram matrix corresponds to points whose centroid is located at the origin. Accordingly, we can define
the constraint set corresponding to Gram matrices of points that are centered and simultaneously
satisfy the pairwise distance constraints of Problem 1 as {X ∈Sn : A(X) = [DΩ; 0]}. The
algorithm below minimizes the quadratic, majorizing model of the objective Fϵk(·) given k, while
satisfying the measurement operator based on the sampled distances. Equivalently, we can define the
main computational step of the method as"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.044310171198388724,"X(k+1) =
arg min
s.t. A(X)=[DΩ;0]
⟨X, Wk(X)⟩,
(5)"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.045317220543806644,"where Wk : Sn 7→Sn is defined as Wk := WX(k),ϵk with the definition of the weight operator
Definition 3.1 above. With this preparation, we provide an outline of MatrixIRLS in Algorithm 1
below. Equation (7) provides suitable update rule for the smoothing parameter sequence (ϵk)k∈N that
enables the computation of only r = O(er) singular triplets of each algorithmic iterate [KV21]."
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.04632426988922457,Algorithm 1 MatrixIRLS for Euclidean Distance Geometry Problems
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.0473313192346425,"Input: Index pairs Ω⊂1, distances DΩ= (dij)(i,j)∈Ω, rank estimate er. Output: X(k) after
suitable stopping condition.
Initialize k = 0, ϵ0 = ∞and W0 = Id.
for k = 1, 2, . . . , do"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.04833836858006042,Solve weighted least squares: Solve (5) by
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.04934541792547835,"X(k) = W −1
k−1A∗ 
AW −1
k−1A∗−1 y
(6)"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.050352467270896276,Update smoothing: Compute er + 1-th singular value of X(k) to update
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.0513595166163142,"ϵk = min

ϵk−1, σer+1(X(k))

.
(7)"
MATRIXIRLS FOR EUCLIDEAN DISTANCE GEOMETRY,0.05236656596173213,"Update weight operator: For rk := |{i ∈[n] : σi(X(k)) > ϵk}|, compute the first rk singular
values σ(k)
i
:= σi(X(k)) and matrices U(k) ∈Rn×rk with leading rk left singular vectors of
X(k) to update Wk = WX(k),ϵk defined in Definition 3.1.
end for"
COMPUTATIONAL CONSIDERATIONS,0.05337361530715005,"3.1
Computational Considerations"
COMPUTATIONAL CONSIDERATIONS,0.054380664652567974,"The computational complexity of this above algorithm can be computed from the steps (6) and (7). In
our numerical implementation, we largely follow the tangent space formulation of the weighted least
squares step (6), cf. [KV21, Section 3], which involves the solution of an order O(nrk) = O(nr)
linear system if er = r is chosen as the ground truth rank. An additional difficulty we overcame in the
provided reference implementation arises from the fact that AA∗is not the identity. The per-iteration
time complexity of our method is dominated by O((mr + r2n)N0
inner), where N0
inner is the number
of inner iteration bound of the iterative linear system solver. Detailed FLOPs calculation is shown in
Appendix F."
THEORETICAL ANALYSIS,0.0553877139979859,"4
Theoretical Analysis"
THEORETICAL ANALYSIS,0.05639476334340383,"In this section, we discuss about the local convergence of MatrixIRLS in Section 4.1 and establish
RIP restricted to the tangent space of the manifold of symmetric rank-r matrices at the ground truth
Gram matrix X0 in Section 4.2."
THEORETICAL ANALYSIS,0.05740181268882175,"4.1
Local Convergence Analysis of Algorithm 1"
THEORETICAL ANALYSIS,0.05840886203423968,"It is well-known in the literature on low-rank matrix completion that for an entrywise measurement
basis, recovery from generic measurements is more difficult if most information of the low-rank"
THEORETICAL ANALYSIS,0.059415911379657606,"matrix is concentrated in few entries, and this observation is typically captured by the notion of
incoherence [CR09, Rec11, Che15]. For the purpose of the EDG problem of interest, we use the
following coherence notion, which has appeared in similar form in [TL18].
Definition 4.1 (Coherence for Gram matrices in the EDG problem, [TL18]). Let X ∈Sn be of
rank r. Let T = TX = {ZX + XZ⊤: Z ∈Rn×n} be the tangent space onto the rank-r manifold
Mr = {Z ∈Sn : rank(Z) = r} at X. We say that X has coherence ν with respect to the basis
{wα}α∈I of the subspace {X ∈Sn : X1 = 0} if"
THEORETICAL ANALYSIS,0.06042296072507553,"max
α∈I X"
THEORETICAL ANALYSIS,0.06143001007049345,"β∈I
⟨PT wα, wβ⟩2 ≤2ν r"
THEORETICAL ANALYSIS,0.06243705941591138,"n
and
max
α∈I X"
THEORETICAL ANALYSIS,0.0634441087613293,"β∈I
⟨PT vα, wβ⟩2 ≤4ν r n,"
THEORETICAL ANALYSIS,0.06445115810674723,"where PT : Sn →Sn denotes the projection operator onto T and {v}α∈I is a dual basis of {wα}α∈I,
which means that ⟨wα, vβ⟩= δα,β for each α, β ∈I ( δα,β = 1 for α = β and equal to 0 otherwise).
Remark 4.2. In [TL18, Definition 1], the coherence constant ν was required to satisfy a third condition
(see [TL18, (Ineq. 14)]). However, this condition is not needed for our proofs, which is why we can
use the weaker definition of Definition 4.1. Similar improvements for the standard basis incoherence
notion were achieved in [Che15]. In [TL18, Lemma 21], it was shown that up constants, the definition
above is equivalent to a coherence condition with respect to the standard basis [Rec11, Che15]."
THEORETICAL ANALYSIS,0.06545820745216516,"Following the conventional sampling approach in the existing literature [Rec11, Che15, CWB08], the
index set is Ω= (iℓ, jℓ)m
ℓ=1 ⊂I contains m samples drawn uniformly at random without replacement.
Theorem 4.3 (Local convergence of MatrixIRLS for EDG with Quadratic Rate). Let X0 ∈Sn
be a matrix of rank r that is ν-incoherent, and let A : Sn →Rm+n be the measurement operator
corresponding to an index set Ω⊂I of size m = |Ω| that is drawn uniformly without replacement.
There exist constants C∗, eC and C such that the following holds. (a) If the sample complexity fulfills
m ≥Cνrn log n, and if (b) the output matrix X(k) ∈Sn of the k-th iteration of MatrixIRLS for
EDG with inputs y = A(X0) and er = r updates the smoothing parameter in (7) such that ϵk ="
THEORETICAL ANALYSIS,0.06646525679758308,"σr+1(X(k)) and fulfills ∥X(k) −X0∥S∞≲
m
3
2"
THEORETICAL ANALYSIS,0.06747230614300101,"C∗κL2(log n)
3
2 √n−rσr(X0) where κ = σ1(X0)/σr(X0)"
THEORETICAL ANALYSIS,0.06847935548841894,"is the condition number of X0, then the local convergence rate is quadratic in the sense that
∥X(k+1)−X0∥S∞≤min(µ∥X(k)−X0∥2
S∞, ∥X(k)−X0∥S∞) with µ =

m
e
C2L log n"
THEORETICAL ANALYSIS,0.06948640483383686,"
1
4(1+6κ)σr(X0)"
THEORETICAL ANALYSIS,0.07049345417925479,"and furthermore X(k+ℓ)
ℓ→∞
−−−→X0 with high probability."
THEORETICAL ANALYSIS,0.07150050352467271,"(The values of the constants C∗= 105, eC = 21
√"
THEORETICAL ANALYSIS,0.07250755287009064,"5,C = 4900 are explicitly derived in the Supple-
mentary material.)"
THEORETICAL ANALYSIS,0.07351460221550855,"In other words, Theorem 4.3 indicates Algorithm 1 converges to the ground truth with high probability
with a sample complexity of Ω(νrn log n), if initialized close to the ground truth Gram matrix. We
refer to Appendix D for its proof."
THEORETICAL ANALYSIS,0.07452165156092648,"Our theorem’s sample complexity requirement aligns with the lower bound for generic low-rank
matrix completion problems, as established in [CT10]. We note that this theorem only provides a
local convergence guarantee for Algorithm 1. This is in line with the strongest known results for
IRLS algorithms optimizing non-convex objectives [For10, KV21, PKV22]. We provide numerical
evidence in Section 5 that the minimal sample complexity assumption (a) of Theorem 4.3 indeed
captures the generic reconstruction ability of the method.To the best of our knowledge, Theorem 4.3
represents the first convergence guarantee for any algorithm for Problem 1 that applies at the optimal
order Ω(νrn log n) of provided pairwise distances, and furthermore, the first theoretical guarantee
for any non-convex optimization framework for Problem 1."
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.0755287009063444,"4.2
Dual Basis Construction and Local Restricted Isometry Property on Tangent Spaces"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.07653575025176233,"While a convergence result similar to Theorem 4.3 had been previously obtained for an IRLS
algorithm for low-rank matrix completion [KV21], an adaptation of the proof of [KV21] to the EDG
setting is not possible due to non-orthogonality of the basis {wα}α of Definition 3.2."
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.07754279959718026,"In order to prove Theorem 4.3, we establish a restricted isometry property of a suitably defined
sampling operator (see (9)) with respect to the tangent space of the manifold of symmetric rank-r
matrices at the ground truth Gram matrix X0 = P⊤P. To formulate this sampling operator and the
respective RIP condition, we construct a dual basis to the measurement basis {wα}α of Definition 3.2."
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.07854984894259819,"Lemma 4.4 (Dual Basis Construction). Let n ∈N, I = {(i, j) | 1 ≤i < j ≤n} be the index set
and {wα}α∈I∪ID be the primal basis of Definition 3.2 with ID = {(i, i) : i ∈{1, . . . , n}}. If"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.07955689828801611,"vα =
−1"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08056394763343404,"2(aia⊤
j + aja⊤
i ),
if α = (i, j) ∈I,
eie⊤
i −aia⊤
i ,
if α = (i, i) ∈ID,
(8)"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08157099697885196,where ai = ei−1
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08257804632426989,"n1 for i ∈{1, . . . , n}, then {vα}α∈I∪ID is a dual basis with respect to {wα}α∈I∪ID,
i.e., {vα}α and {wα}α are bi-orthogonal."
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08358509566968782,"Lemma 4.4 extends the dual basis construction of [TL18, LT24], in which the duality of {vα}α∈I
with respect to {wα}α∈I was shown. The proof of Lemma 4.4 is detailed in Appendix B.1."
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08459214501510574,"Unlike the basis pair of [TL18, LT24], our bases span the entire space of symmetric matrices Sn (see
Appendix B.2), which enables us to show the following restricted isometry property.
Theorem 4.5 (Restricted Isometry Property for Sampling Operator QΩ). Let L = n(n −1)/2,
0 < ϵ ≤1"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08559919436052367,"2, and Ω⊂I be a multiset of size m sampled independently with replacement. Define the
sampling operator QΩ: Sn →Sn such that"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.0866062437059416,QΩ(X) := L m X
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08761329305135952,"α∈Ω∪(i,i)n
i=1
⟨X, wα⟩vα
(9)"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08862034239677745,"where wα and vα as in (4) and (8), respectively. Let X0 ∈Sn be a ν-incoherent matrix whose
tangent space onto the manifold Mr of symmetric rank-r matrices is denoted as T0 = TX0. Let
PT0 : Sn →Sn be the projection operator associated to T0. Then
PT0 Q*
ΩPT0 −PT0

S∞≤ϵ
holds with probability at least 1 −2"
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.08962739174219536,n provided that
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.09063444108761329,m ≥(49/ϵ2)νnr log n.
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.09164149043303121,"The dual basis as discussed in Section 3 along with the extension for the diagonal entries together
spans the space of n × n symmetric matrices. This construction has been crucial in proving the RIP
restricted to the tangent space TX of rank constrained smooth manifold Mr. This construction could
also be valuable for analyzing other non-convex algorithms. A detailed proof of Theorem 4.5 is
provided in Appendix C This proof is achieved by using concentration inequalities like the Matrix
Bernstein inequality theorem C.1 in multiple lemmas stated and proved in detail in the supplemental
material."
DUAL BASIS CONSTRUCTION AND LOCAL RESTRICTED ISOMETRY PROPERTY ON TANGENT SPACES,0.09264853977844914,"Since the existing literature for nonconvex approaches for solving Problem 1 lacks this property, this
approach of establishing RIP by restricting it to the tangent space can be useful for the analysis of other
nonconvex methods. The RIP condition, originally introduced in the context of compressed sensing
[Can08], is a fundamental assumption in the literature on low-rank matrix recovery ([ZL12, Che15,
KV21, CZ13]). A tangent-space restricted RIP has been has been useful for analyzing the convergence
and performance properties of other non-convex methods [GLM16, LZ23, LHLZ20, ZLTW18] in a
non-EDG setting."
NUMERICAL EXPERIMENTS,0.09365558912386707,"5
Numerical Experiments"
NUMERICAL EXPERIMENTS,0.094662638469285,"We evaluate the performance of MatrixIRLS, Algorithm 1, for instances of the EDG reconstruction
Problem 1 in terms of data efficiency across multiple datasets in comparison to other state-of-the-art
methods in the literature. We compare the performance of MatrixIRLS with three other algorithms:
(a) ALM, an augmented Lagrangian method that minimizes the non-convex formulation defined by
minP∈Rr×n Tr(P⊤P) subject to RΩ(P⊤P) = RΩ(X0),"
NUMERICAL EXPERIMENTS,0.09566968781470292,"with RΩbeing equal to the sampling operator QΩrestricted to Ω, which is based on a Burer-Monteiro
factorization of the Gram matrix, and which has been studied in the numerical experiments of [TL18],
(b) ScaledSGD [ZCZ22] which is a preconditioned stochastic gradient descent method designed to be
robust with respect to ill-conditioned problems, (c) RieEDG [SLCT23] which is a Riemannian-based
gradient descent approach based on the sampling operator (9) restricted to Ω. The choice of these
algorithms is based on their robustness to noise as claimed in the respective papers."
SYNTHETIC DATA,0.09667673716012085,"5.1
Synthetic Data"
SYNTHETIC DATA,0.09768378650553877,"We first consider a synthetic data, where we select n = 500 points P0 = [p1,
. . . , pn] ∈Rr×n"
SYNTHETIC DATA,0.0986908358509567,"from a standard Gaussian distribution at random such that (pi)j
i.i.d.
∼
N(0, 1) for all i, j, which"
SYNTHETIC DATA,0.09969788519637462,"defines the ground truth Gram matrix X0 = P0⊤P0. We are provided with m = |Ω| Euclidean
distances, where the point index pair set Ω⊂{(i, j) ∈[n] × [n], i < j} is sampled uniformly at
random. This is parametrized by the oversampling factor ρ =
m
nr−r(r−1)/2 where the denominator is
the degrees of freedom (discussed in Appendix E.4). To understand the efficiency of the algorithm
over a range of ranks r and across different oversampling factors ρ, we conduct phase transition
experiments for all the above mentioned algorithms. We define a successful recovery as the case
of the relative Procrustes distance dProcrustes(Prec, P0) (discussed in Appendix E.3) between the
recovered matrix Prec and ground truth coordinate matrix P0 does not exceed a tolerance threshold
tolrec = 10−3. We chose the Procrustes distance at it is a shape preserving distance that accounts
also for differences in scaling and alignment between the reconstructed geometries. We observe the
performance of the algorithms as shown in the Figure 1 for ranks between 2 to 5 and a oversampling
factor ranging from 1 to 4 over 24 instances."
SYNTHETIC DATA,0.10070493454179255,"In Figure 1, each entry on the figure represents the probability of success of an algorithm for the
given rank ground truth rank r and oversampling factor ρ over 24 instances.1 In terms of the recovery
of the ground truth, we notice a comparable performance for the MatrixIRLS and ALM. However,
the other two algorithms RieEDG and ScaledSGD, for the given success tolerance, the recovery of
a ground truth is only possible if more samplies corresponding to a larger oversampling factor are
provided, for any of the ranks r considered. This emphasizes that MatrixIRLS is able to achieve
state-of-the-art data efficiency."
SYNTHETIC DATA,0.10171198388721048,"1
2
3
4 2 3 4 5"
SYNTHETIC DATA,0.1027190332326284,Oversampling factor ρ
SYNTHETIC DATA,0.10372608257804633,Rank r
SYNTHETIC DATA,0.10473313192346426,(a) MatrixIRLS
SYNTHETIC DATA,0.10574018126888217,"1
2
3
4 2 3 4 5"
SYNTHETIC DATA,0.1067472306143001,Oversampling factor ρ
SYNTHETIC DATA,0.10775427995971802,Rank r
SYNTHETIC DATA,0.10876132930513595,(b) ALM
SYNTHETIC DATA,0.10976837865055387,"1
2
3
4 2 3 4 5"
SYNTHETIC DATA,0.1107754279959718,Oversampling factor ρ
SYNTHETIC DATA,0.11178247734138973,Rank r
SYNTHETIC DATA,0.11278952668680765,(c) RieEDG
SYNTHETIC DATA,0.11379657603222558,"1
2
3
4 2 3 4 5"
SYNTHETIC DATA,0.1148036253776435,Oversampling factor ρ
SYNTHETIC DATA,0.11581067472306143,Rank r
SYNTHETIC DATA,0.11681772406847936,"0
0.2
0.4
0.6
0.8
1"
SYNTHETIC DATA,0.11782477341389729,(d) ScaledSGD
SYNTHETIC DATA,0.11883182275931521,"Figure 1: Success probabilities for recovery for different algorithms, given Gaussian ground truths X0 of
different ranks, computed across 24 instances."
SYNTHETIC DATA,0.11983887210473314,"In order to understand the scalability of the proposed algorithm, we have provided the time of
completion of MatrixIRLS in Table 1. The first two rows shows that for fixed oversampling factorρ =
3 , we are able to recover the points in the magnitude of 104, is just 13.7 minutes. So to further test
the scalability, we also looked at the time to completion when less number of samples are provided (
ρ = 2.5). In that setup n = 10000 takes 57.5 minutes to recover with high precision."
ILL-CONDITIONED DATA,0.12084592145015106,"5.2
Ill-conditioned Data
We now assess the performance of the different EDG methods on ill-conditioned data. Ill-conditioning
in the point matrix in particular arises in situations where, for example, r-dimensional points are
approximately following an r −1-dimensional geometry, a situation which often arises when the
available distance information if affected by outliers [ZCZ22]."
ILL-CONDITIONED DATA,0.12185297079556898,"Similar to the setup above, we construct Gram matrices X0 ∈Sn of rank r = 5 with condition
number κ = σ1(X0)/σr(X0) = 105 corresponding to n = 400 random data points P0 ∈Rr×n"
ILL-CONDITIONED DATA,0.1228600201409869,"generated with random orthogonal singular vectors and singular values σi(X0) interpolated between
κ and 1 with decay of order O(i−2). Figure 2 shows the success probability of different algorithms
for ill-conditioned ground truths."
ILL-CONDITIONED DATA,0.12386706948640483,"To have a deeper insight into the algorithm’s performance, Figure 3 shows the per-iterate relative
reconstructed error of the four algorithms on this ill-conditioned data at an oversampling factor
of ρ = 2 for a representative problem instance. This reconstructed error refers to the Procrustes
distance between the ground truth P0 and the recovered Pk at each iteration k. We observe that
for ill-conditioned data, MatrixIRLS is the only method that can recover the ground truth up to a"
ILL-CONDITIONED DATA,0.12487411883182276,"1For example, for a rank r and oversampling factor ρ, if out of the 24 instances, in 12 such cases, the
Procrustes distance based error is less than the threshold, then the probability of success is 1 2."
ILL-CONDITIONED DATA,0.12588116817724068,"reasonable precision, achieving a relative error of ≈10−8 after around 35 iterations (top of Figure 3),
whereas the other methods do not achieve errors below 10−3 even after 100000 iterations. While
one iteration of MatrixIRLS typically has a longer runtime than an iteration of any of the other
algorithms due to its second-order nature, we also provide a visualization of the observed runtimes
of the methods in the bottom of Figure 3. It can be seen that MatrixIRLS is able reconstruct the
geometry of the challenging problem in around 200 seconds up to a high precision. Additionally, we
run a study on the algorithms’ behavior across different oversampling factors between ρ = 1 and
ρ = 4 for 24 random problem instances, visualized in Figure 4. The box plots indicate visualize
median relative Procrustes error together with the relevant 25% and 75% quantiles of the observed
error distribution. We observe that MatrixIRLS has consistent convergence for ill-conditioned data
even for lower oversampling rates as long as ρ ≥1.5."
ILL-CONDITIONED DATA,0.1268882175226586,"(a) MatrixIRLS
(b) ALM
(c) ScaledSGD
(d) RieEDG"
ILL-CONDITIONED DATA,0.12789526686807653,"Figure 2: Empirical success probabilities for recovery for different algorithms, given ill-conditioned ground
truths X0 of different ranks, computed across 8 instances."
ILL-CONDITIONED DATA,0.12890231621349446,"100
101
102
103
104
105
10−8 10−4 100"
ILL-CONDITIONED DATA,0.1299093655589124,Iterations
ILL-CONDITIONED DATA,0.1309164149043303,Relative Error
ILL-CONDITIONED DATA,0.13192346424974824,"ScaledSGD
RieEDG
ALM
MatrixIRLS"
ILL-CONDITIONED DATA,0.13293051359516617,"10−2
10−1
100
101
102
103
10−8 10−4 100"
ILL-CONDITIONED DATA,0.1339375629405841,Runtime in seconds
ILL-CONDITIONED DATA,0.13494461228600202,Relative Error
ILL-CONDITIONED DATA,0.13595166163141995,"ScaledSGD
RieEDG
ALM
MatrixIRLS"
ILL-CONDITIONED DATA,0.13695871097683787,"Figure 3: Top: Relative error plot. Bottom: Runtime
across iterates for one instance."
ILL-CONDITIONED DATA,0.1379657603222558,"1
2
3
4
10−8 10−4 100"
ILL-CONDITIONED DATA,0.13897280966767372,Oversampling
ILL-CONDITIONED DATA,0.13997985901309165,Relative Procrustes Error
ILL-CONDITIONED DATA,0.14098690835850958,"ScaledSGD
RieEDG
ALM
MatrixIRLS"
ILL-CONDITIONED DATA,0.1419939577039275,"Figure 4: Performance of EDG completion algorithms
over 24 instances on ill-conditioned data."
REAL DATA,0.14300100704934543,"5.3
Real Data"
REAL DATA,0.14400805639476336,"To assess the performance of Algorithm 1 in realistic setups, we consider the task of molecular
conformation, i.e., we aim to reconstruct the 3D structure of a molecule from partial information
about the distances of its atoms. For our experiments, we determine the structures of a protein
molecule (1BPM) from the Protein Data Bank [HMBFGG+00], which is collected using X-ray
diffraction experiments or nuclear magnetic resonance (NMR) spectroscopy."
REAL DATA,0.14501510574018128,"The goal of this experiment is to reconstruct the point matrix P0 from |Ω| distance samples as
defined in Problem 1. For the 1BPM protein data the rank of the input matrix is 3. We conduct
the experiments corresponding to oversampling factors ρ between 1 and 4. Like the previous setup,
here successful recovery refers to the case where the relative Procrustes distance, that is the metric
dProcrustes(Prec, P0) between the recovered matrix Prec and ground truth coordinate matrix P0 does
not exceed a tolerance threshold tolrec across 24 independent realizations. The error analysis for this
experiment is shown in Figures 7a and 7b in Appendix E."
REAL DATA,0.14602215508559918,"It is evident that MatrixIRLS and ALM have comparable results in recovering the geometry of the
data given lesser samples, where as algorithms like ScaledSGD or RieEDG are unable to reconstruct
geometries even when the oversampling rate is as high as ρ = 4. In the fig. 5, we see a convergence"
REAL DATA,0.1470292044310171,"with high precision for MatrixIRLS from ρ ∼2.5 for Protein which means that it recovered the
ground truth with around 0.5% samples."
REAL DATA,0.14803625377643503,"Figure 5: Reconstruction of Protein 1BPM molecule
by MatrixIRLS with 0.5% samples"
REAL DATA,0.14904330312185296,"Figure 6: Reconstruction of Protein 1BPM molecule
by MatrixIRLS with 0.6% samples"
REAL DATA,0.1500503524672709,"In the Figure 6 the reconstructed protein in blue is aligned with the ground truth structure of the
protein in pink. We can see that when the oversampling is 3.5, which is when there is 0.6% samples
available, the reconstruction exactly matches with the ground truth."
REAL DATA,0.1510574018126888,"In order to understand the runtime of the different algorithms Table 2, reports the reconstruction times
for each of the four algorithms applied to the 1BPM Protein data (with datapoints n = 3672) in a
low-data regime with oversampling factor of ρ = 3. . It can be seen that MatrixIRLS is with 7.08
minutes around 3 times faster than ALM, which needed 23.04 minutes until convergence. While
these times certainly depend on the precise choice of stopping criteria for each algorithm (we used
the ones indicated in Appendix E ), this shows that the proposed method is competitive in terms of
clock time."
REAL DATA,0.15206445115810674,"Datapoints n
Relative Error
Time (mins)"
REAL DATA,0.15307150050352467,"500
1.04 × 10−7
0.03
1000
1.19 × 10−7
0.07
3000
1.37 × 10−12
0.8
5000
8.56 × 10−13
5.6
7000
2.26 × 10−12
8.5
10000
4.06 × 10−11
13.7"
REAL DATA,0.1540785498489426,"Table 1: Execution time of MatrixIRLS vs
problem size n for Gaussian data with over-
sampling factor ρ = 3"
REAL DATA,0.15508559919436052,"Algorithm
Relative
Time until
Error
Convergence (mins)"
REAL DATA,0.15609264853977844,"ALM
5.6 × 10−6
23.4
RieEDG
5.6 × 10−2
116.6
ScaledSGD
2.4 × 10−2
20.1
MatrixIRLS
2.7 × 10−12
7.08"
REAL DATA,0.15709969788519637,"Table 2: Runtime comparison for different ge-
ometry reconstruction algorithms from partially
known pairwise distances for 1BPM Protein data
(n = 3672, r = 3), oversampling factor ρ = 3"
REAL DATA,0.1581067472306143,"For the implementation of the other algorithms, we use the authors’ code for the respective approaches
(discussed in Appendix E). We include another set of experiments on US cities data [UU20], in the
Appendix E."
CONCLUSION,0.15911379657603222,"6
Conclusion"
CONCLUSION,0.16012084592145015,"In this paper, we address the challenge of reconstructing suitable geometric configurations using
minimal Euclidean distance samples. By leveraging continuous and non-convex rank minimization
formulations, we develop a variant of the iteratively reweighted least squares (IRLS) algorithm and
establish a local convergence guarantee under the condition of a minimal random set of observed
distances. Our contribution also includes the proof of a restricted isometry property (RIP) restricted
to the tangent space of the manifold of symmetric rank-r matrices, a result which might be of
independent interest for the analysis of other non-convex methods. As future work further analysis
on the global convergence can be established. Through numerical validation we conclude that the
algorithm is able to achieve state-of-the-art data efficiency."
CONCLUSION,0.16112789526686808,Acknowledgement
CONCLUSION,0.162134944612286,"Abiy Tasissa acknowledges partial support from the National Science Foundation through grant
DMS-2208392."
REFERENCES,0.16314199395770393,References
REFERENCES,0.16414904330312186,"[ADSVF23] A. Andreella, R. De Santis, A. Vesely, and L. Finos. Procrustes-based distances
for exploring between-matrices similarity. Statistical Methods & Applications,
32(3):867–882, 2023."
REFERENCES,0.16515609264853978,"[AKW99] A. Y. Alfakih, A. K. Khandani, and H. Wolkowicz. Solving Euclidean Distance
Matrix Completion Problems Via Semidefinite Programming. Computational
Optimization and Applications, 12:13–30, 1999."
REFERENCES,0.1661631419939577,"[Alf05] A. Y. Alfakih. On the uniqueness of Euclidean distance matrix completions: the
case of points in general position. Linear Algebra and its Applications, 397:265–
277, 2005."
REFERENCES,0.16717019133937563,"[ALMT14] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: Phase
transitions in convex programs with random data. Information and Inference: A
Journal of the IMA, 3(3):224–294, 2014."
REFERENCES,0.16817724068479356,"[BA15] N. Boumal and P.-A. Absil. Low-rank matrix completion via preconditioned
optimization on the Grassmann manifold. Linear Algebra and its Applications,
15(475):200–239, 2015."
REFERENCES,0.1691842900302115,"[BJ95] M. Bakonyi and C. R. Johnson. The Euclidian Distance Matrix Completion Problem.
SIAM J. Matrix Anal. Appl., 16:646–654, 1995."
REFERENCES,0.1701913393756294,"[BM03] S. Burer and R. D. Monteiro. A nonlinear programming algorithm for solving
semidefinite programs via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003."
REFERENCES,0.17119838872104734,"[BNZ21] J. Bauch, B. Nadler, and P. Zilber. Rank 2r iterative least squares: efficient recovery
of ill-conditioned low rank matrices from few entries. SIAM J. Math. Data Sci.,
3(1):439–465, 2021."
REFERENCES,0.17220543806646527,"[Bou20] N. Boumal. An introduction to optimization on smooth manifolds. Available on-
line at http: // sma. epfl. ch/ ~nboumal/ book/ IntroOptimManifolds_
Boumal_ 2020. pdf , November, 2020."
REFERENCES,0.1732124874118832,"[Bur12] C. S. Burrus. Iterative reweighted least squares. OpenStax CNX. Available online:
http://cnx. org/contents/92b90377-2b34-49e4-b26f-7fe572db78a1, 12, 2012."
REFERENCES,0.17421953675730112,"[Can08] E. J. Candès. The restricted isometry property and its implications for compressed
sensing. Comptes Rendus Mathematique, 346(9):589–592, 2008."
REFERENCES,0.17522658610271905,"[CC14] Y. Chen and Y. Chi. Robust Spectral Compressed Sensing via Structured Matrix
Completion. IEEE Trans. Inf. Theory, 60(10):6576–6601, 2014."
REFERENCES,0.17623363544813697,"[Che12] X. Chen. Smoothing methods for nonsmooth, nonconvex minimization. Math.
Program., 134(1):71–99, 2012."
REFERENCES,0.1772406847935549,"[Che15] Y. Chen. Incoherence-Optimal Matrix Completion. IEEE Trans. Inf. Theory,
61(5):2909–2923, 2015."
REFERENCES,0.1782477341389728,"[CLC19] Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex Optimization Meets Low-Rank Matrix
Factorization: An Overview. IEEE Trans. Signal Process., 67(20):5239–5269,
2019."
REFERENCES,0.17925478348439072,"[CR09] E. J. Candès and B. Recht. Exact matrix completion via convex optimization.
Found. Comput. Math., 9(6):717–772, 2009."
REFERENCES,0.18026183282980865,"[CT10] E. J. Candès and T. Tao. The Power of Convex Relaxation: Near-Optimal Matrix
Completion. IEEE Trans. Inf. Theory, 56(5):2053–2080, 2010."
REFERENCES,0.18126888217522658,"[CW18] J.-F. Cai and K. Wei. Exploiting the Structure Effectively and Efficiently in Low-
Rank Matrix Recovery. Processing, Analyzing and Learning of Images, Shapes,
and Forms, 19:21 pp., 2018."
REFERENCES,0.1822759315206445,"[CWB08] E. Candès, M. B. Wakin, and S. Boyd. Enhancing Sparsity by Reweighted ℓ1-
Minimization. The Journal of Fourier Analysis and Applications, 14:877–905,
2008."
REFERENCES,0.18328298086606243,"[CZ13] T. T. Cai and A. Zhang. Sharp RIP bound for sparse signal and low-rank matrix
recovery. Applied and Computational Harmonic Analysis, 35(1):74–93, 2013."
REFERENCES,0.18429003021148035,"[DDFG10] I. Daubechies, R. DeVore, M. Fornasier, and C. Güntürk. Iteratively Reweighted
Least Squares Minimization for Sparse Recovery. Commun. Pure Appl. Math.,
63:1–38, 2010."
REFERENCES,0.18529707955689828,"[DPRV15] I. Dokmanic, R. Parhizkar, J. Ranieri, and M. Vetterli. Euclidean Distance Matrices:
Essential theory, algorithms, and applications. IEEE Signal Processing Magazine,
32(6):12–30, 2015."
REFERENCES,0.1863041289023162,"[DR16] M. A. Davenport and J. Romberg. An Overview of Low-Rank Matrix Recovery
From Incomplete Observations. IEEE J. Sel. Topics Signal Process., 10:608–622,
2016."
REFERENCES,0.18731117824773413,"[For10] M. Fornasier. Numerical Methods for Sparse Recovery. In M. Fornasier, editor,
Theoretical Foundations and Numerical Methods for Sparse Recovery, volume 9
of Radon Series on Computational and Applied Mathematics, pages 93–200. De
Gruyter, Berlin, 2010."
REFERENCES,0.18831822759315206,"[FQX19] J. Fliege, H.-D. Qi, and N. Xiu. Euclidean distance matrix optimization for sensor
network localization. In Cooperative Localization and Navigation, pages 99–126.
CRC Press, 2019."
REFERENCES,0.18932527693857,"[FRW11] M. Fornasier, H. Rauhut, and R. Ward. Low-rank Matrix Recovery via Iteratively
Reweighted Least Squares Minimization. SIAM J. Optim., 21(4):1614–1640, 2011."
REFERENCES,0.1903323262839879,"[GCKG23] B. Ghojogh, M. Crowley, F. Karray, and A. Ghodsi. Multidimensional Scaling,
Sammon Mapping, and Isomap, pages 185–205. Springer International Publishing,
Cham, 2023."
REFERENCES,0.19133937562940584,"[GLM16] R. Ge, J. D. Lee, and T. Ma. Matrix Completion has No Spurious Local Minimum.
In Advances in Neural Information Processing Systems (NIPS), pages 2973–2981,
2016."
REFERENCES,0.19234642497482377,"[Gow85] J. C. Gower. Properties of Euclidean and non-Euclidean distance matrices. Linear
Algebra and its Applications, 67:81–97, 1985."
REFERENCES,0.1933534743202417,"[GR97] I. F. Gorodnitsky and B. D. Rao. Sparse signal reconstruction from limited data
using FOCUSS: A re-weighted minimum norm algorithm. IEEE Trans. Signal
Process., pages 600–616, 1997."
REFERENCES,0.19436052366565962,"[Gro11] D. Gross. Recovering Low-Rank Matrices From Few Coefficients in Any Basis.
IEEE Trans. Inf. Theory, 57(3):1548–1566, 2011."
REFERENCES,0.19536757301107754,"[Hen95] B. Hendrickson. The Molecule Problem: Exploiting Structure in Global Optimiza-
tion. SIAM Journal on Optimization, 5(4):835–857, 1995."
REFERENCES,0.19637462235649547,"[HMBFGG+00] J. W. Helen M. Berman, Z. Feng, T. N. B. Gary Gilliland, I. N. S. Helge Weissig,
and P. E. Bourne. The Protein Data Bank. Nucleic Acids Research, Volume 28,
Issue 1, 1 January 2000, Pages 235–242, 2000."
REFERENCES,0.1973816717019134,"[HS52] M. R. Hestenes and E. Stiefel. Methods of Conjugate Gradients for Solving Linear
Systems. Journal of research of the National Bureau of Standards, 49(1), 1952."
REFERENCES,0.19838872104733132,"[JEP+21] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tun-
yasuvunakool, R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein
structure prediction with AlphaFold. Nature, 596(7873):583–589, 2021."
REFERENCES,0.19939577039274925,"[KS18] C. Kümmerle and J. Sigl. Harmonic Mean Iteratively Reweighted Least Squares
for Low-Rank Matrix Recovery. J. Mach. Learn. Res., 19(47):1–49, 2018."
REFERENCES,0.20040281973816718,"[KV21] C. Kümmerle and C. M. Verdun. A scalable second order method for ill-conditioned
matrix completion from few samples. In International Conference on Machine
Learning, pages 5872–5883, 2021."
REFERENCES,0.2014098690835851,"[Law61] C. Lawson. Contributions to the Theory of Linear Least Maximum Approximation.
Ph. D. Thesis, Univ. of Calif., Los Angeles, 1961."
REFERENCES,0.20241691842900303,"[LHLZ20] Y. Luo, W. Huang, X. Li, and A. R. Zhang. Recursive Importance Sketching for
Rank Constrained Least Squares: Algorithms and High-order Convergence. arXiv
preprint arXiv:2011.08360, 2020."
REFERENCES,0.20342396777442096,"[Lib20] L. Liberti. Distance geometry and data science. Top, 28(2):271–339, 2020."
REFERENCES,0.20443101711983888,"[LL17] R. Lai and J. Li. Solving Partial Differential Equations on Manifolds From Incom-
plete Inter-Point Distance, 2017."
REFERENCES,0.2054380664652568,"[LLMM14] L. Liberti, C. Lavor, N. Maculan, and A. Mucherino. Euclidean Distance Geometry
and Applications. SIAM Review, 56(1):3–69, 2014."
REFERENCES,0.20644511581067473,"[LS24] Y. Li and X. Sun. Sensor Network Localization via Riemannian Conjugate Gradient
and Rank Reduction. IEEE Transactions on Signal Processing, 2024."
REFERENCES,0.20745216515609266,"[LT24] S. Lichtenberg and A. Tasissa. A dual basis approach to multidimensional scaling.
Linear Algebra and its Applications, 682:86–95, 2024."
REFERENCES,0.2084592145015106,"[LZ23] Y. Luo and A. R. Zhang. Low-rank Tensor Estimation via Riemannian Gauss-
Newton: Statistical Optimality and Second-Order Convergence. The Journal of
Machine Learning Research, 24(1):18274–18321, 2023."
REFERENCES,0.2094662638469285,"[Meu06] G. Meurant. The Lanczos and Conjugate Gradient Algorithms: From Theory to
Finite Precision Computations. Society for Industrial and Applied Mathematics„
2006."
REFERENCES,0.2104733131923464,"[MF10] K. Mohan and M. Fazel. Reweighted nuclear norm minimization with application
to system identification. In Proceedings of the American Control Conference, pages
2953–2959. IEEE, 2010."
REFERENCES,0.21148036253776434,"[Mir60] L. Mirsky. Symmetric Gauge Functions And Unitarily Invariant Norms. The
Quarterly Journal of Mathematics, 11(1):50–59, 1960."
REFERENCES,0.21248741188318226,"[MM15] C. Musco and C. Musco. Randomized Block Krylov Methods for Stronger and
Faster Approximate Singular Value Decomposition. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 1396–1404, 2015."
REFERENCES,0.2134944612286002,"[MMLL19] T. Malliavin, A. Mucherino, C. Lavor, and L. Liberti. Systematic Exploration of
Protein Conformational Space Using a Distance Geometry Approach. Journal of
Chemical Information and Modeling, 59, 08 2019."
REFERENCES,0.21450151057401812,"[MMWL22] M. Masters, A. H. Mahmoud, Y. Wei, and M. A. Lill. Deep Learning Model for
Flexible and Efficient Protein-Ligand Docking. In ICLR2022 Machine Learning
for Drug Discovery, 2022."
REFERENCES,0.21550855991943604,"[MW97] J. J. Moré and Z. Wu. Global Continuation for Distance Geometry Problems. SIAM
Journal on Optimization, 7(3):814–836, 1997."
REFERENCES,0.21651560926485397,"[MWCC20] C. Ma, K. Wang, Y. Chi, and Y. Chen. Implicit Regularization in Nonconvex
Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval,
Matrix Completion, and Blind Deconvolution. Foundations of Computational
Mathematics, 20:451–632, 2020."
REFERENCES,0.2175226586102719,"[NKKS19] L. T. Nguyen, J. Kim, S. Kim, and B. Shim. Localization of IoT networks via low-
rank matrix completion. IEEE Transactions on Communications, 67(8):5833–5847,
2019."
REFERENCES,0.21852970795568982,"[PKV22] L. Peng, C. Kümmerle, and R. Vidal. Global Linear and Local Superlinear Con-
vergence of IRLS for Non-Smooth Robust Regression. In Advances in Neural
Information Processing Systems (NeurIPS), volume 35, pages 28972–28987, 2022."
REFERENCES,0.21953675730110775,"[Rec11] B. Recht. A Simpler Approach to Matrix Completion. J. Mach. Learn. Res.,
12:3413–3430, 2011."
REFERENCES,0.22054380664652568,"[RFP10] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed Minimum-Rank Solutions of
Linear Matrix Equations via Nuclear Norm Minimization. SIAM Rev., 52(3):471–
501, 2010."
REFERENCES,0.2215508559919436,"[RXH11] B. Recht, W. Xu, and B. Hassibi. Null space conditions and thresholds for rank
minimization. Mathematical programming, 127:175–202, 2011."
REFERENCES,0.22255790533736153,"[SBP17] Y. Sun, P. Babu, and D. P. Palomar. Majorization-Minimization Algorithms in
Signal Processing, Communications, and Machine Learning. IEEE Trans. Signal
Process., 65(3):794–816, 2017."
REFERENCES,0.22356495468277945,"[SEJ+20] A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin,
A. Žídek, A. W. Nelson, A. Bridgland, et al. Improved protein structure prediction
us3ing potentials from deep learning. Nature, 577(7792):706–710, 2020."
REFERENCES,0.22457200402819738,"[SL16] R. Sun and Z.-Q. Luo. Guaranteed Matrix Completion via Non-Convex Factoriza-
tion. IEEE Transactions on Information Theory, 62(11):6535–6579, November
2016."
REFERENCES,0.2255790533736153,"[SLCT23] C. M. Smith, S. P. Lichtenberg, H. Cai, and A. Tasissa. Riemannian Optimization for
Euclidean Distance Geometry. In OPT 2023: Optimization for Machine Learning,
2023."
REFERENCES,0.22658610271903323,"[Ste06] M. Stewart. Perturbation of the SVD in the presence of small singular values.
Linear Algebra Appl., 419(1):53–77, 2006."
REFERENCES,0.22759315206445116,"[SWBB97] D. C. Spellmeyer, A. K. Wong, M. J. Bower, and J. M. Blaney. Conformational
analysis using distance geometry methods. Journal of Molecular Graphics and
Modelling, 15(1):18–36, 1997."
REFERENCES,0.22860020140986909,"[TL18] A. Tasissa and R. Lai. Exact reconstruction of euclidean distance geometry problem
using low-rank matrix completion. IEEE Transactions on Information Theory,
65(5):3124–3144, 2018."
REFERENCES,0.229607250755287,"[Tro00] M. W. Trosset. Distance matrix completion by numerical optimization. Computa-
tional Optimization and Applications, 17:11–22, 2000."
REFERENCES,0.23061430010070494,"[Tro11] J. A. Tropp. User-Friendly Tail Bounds for Sums of Random Matrices. Foundations
of Computational Mathematics, 12(4):389–434, August 2011."
REFERENCES,0.23162134944612287,"[TSL00] J. B. Tenenbaum, V. d. Silva, and J. C. Langford. A global geometric framework
for nonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000."
REFERENCES,0.2326283987915408,"[TW13] J. Tanner and K. Wei. Normalized Iterative Hard Thresholding for Matrix Comple-
tion. SIAM J. Sci. Comput., 35(5):S104–S125, 2013."
REFERENCES,0.23363544813695872,"[UU20] U.S. Geological Survey and U.S. Census Bureau. US Cities Dataset. Available at
https://simplemaps.com/data/us-cities, 2020."
REFERENCES,0.23464249748237664,"[Van13] B. Vandereycken. Low-Rank Matrix Completion by Riemannian Optimization.
SIAM J. Optim., 23(2), 2013."
REFERENCES,0.23564954682779457,"[VSP+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
and I. Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017."
REFERENCES,0.2366565961732125,"[WCCL20] K. Wei, J.-F. Cai, T. F. Chan, and S. Leung. Guarantees of Riemannian optimization
for low rank matrix completion. Inverse Probl. Imaging, 14(2):233–265, 2020."
REFERENCES,0.23766364551863042,"[Woo50] M. A. Woodbury. Inverting modified matrices. Memorandum report, 42(106):336,
1950."
REFERENCES,0.23867069486404835,"[YH38] G. M. Young and A. S. Householder. Discussion of a set of points in terms of their
mutual distances. Psychometrika, 3:19–22, 1938."
REFERENCES,0.23967774420946628,"[ZCZ22] J. Zhang, H.-M. Chiu, and R. Y. Zhang.
Accelerating SGD for Highly Ill-
Conditioned Huge-Scale Online Matrix Completion. Advances in Neural Informa-
tion Processing Systems, 35:37549–37562, 2022."
REFERENCES,0.2406847935548842,"[ZL12] Y.-B. Zhao and D. Li. Reweighted ℓ1-minimization for sparse solutions to underde-
termined linear systems. SIAM J. Optim., 22(3):1065–1088, 2012."
REFERENCES,0.24169184290030213,"[ZLTW18] Z. Zhu, Q. Li, G. Tang, and M. B. Wakin. Global Optimality in Low-Rank Matrix
Optimization. IEEE Trans. Signal Process., 66(13):3614–3628, 2018."
REFERENCES,0.24269889224572003,Supplementary Material
REFERENCES,0.24370594159113795,"A
Limitations"
REFERENCES,0.24471299093655588,"For our method, the limited convergence radius leads to the convergence guarantee locally. Addi-
tionally, the runtime per iteration is larger compared to other non-convex methods [ZCZ22, TL18,
SLCT23] but our method achieves convergence in less than a tenth of the number of iterations of the
other methods. For some datasets like the Protein (1BPM) [HMBFGG+00], ALM has a superior
performance for oversampling factor (ρ) between 1 and 1.5."
REFERENCES,0.2457200402819738,"B
Dual Basis Construction"
REFERENCES,0.24672708962739173,"In this section, we provide the proof the bi-orthogonality of the basis defined in Lemma 4.4 with
respect to the measurement basis Definition 3.2, and furthermore establish properties of the basis pair
in Appendix B.2 that will be useful later on."
REFERENCES,0.24773413897280966,"B.1
Proof of Lemma 4.4"
REFERENCES,0.24874118831822759,"Proof of Lemma 4.4. For simplicity of the proof, we denote the two cases of Definition 3.2, separately.
This helps us to divide the proof into multiple subcases."
REFERENCES,0.2497482376636455,"With L =
n(n−1)"
REFERENCES,0.25075528700906347,"2
, let V =
v(1,2),
. . . ,
v(n−1,n)

∈Rn2×L be a matrix whose columns"
REFERENCES,0.25176233635448136,"enumerate the vectorized dual basis elements vα where α = (i, j) ∈I, and let VE ∈Rn2×n"
REFERENCES,0.2527693856998993,"consist of vectorized dual basis vectors v(i,i) with i ∈{1, . . . , n}. Similarly, W ∈Rn2×L and"
REFERENCES,0.2537764350453172,"WE ∈Rn2×n are defined using vectorized basis elements {wα}α∈I∪ID. As establishing duality
between the two basis sets is equivalent to bi-orthogonality of the basis elements, it remains to show
that the extended basis matrices ˜V = [V|VE] ∈Rn2×L+n and ˜
W = [W|WE] ∈Rn2×L+n satisfy
the relationship"
REFERENCES,0.25478348439073517,"˜
W⊤˜V =

W⊤V
W⊤VE
W⊤
EV
W⊤
EVE"
REFERENCES,0.25579053373615307,"
= Id,"
REFERENCES,0.256797583081571,where Id is the identity matrix.
REFERENCES,0.2578046324269889,"In coordinate-wise notation, this means that for α = (i, j) and β = (k, l), with 1 ≤i ≤j ≤n and
1 ≤k ≤l ≤n, we must show that ⟨˜wα, ˜vβ⟩= δα,β where ˜wα and ˜vβ are columns of ˜
W and ˜V
respectively."
REFERENCES,0.2588116817724068,"From Claim 3.1 of [LT24], it is clear that V is the dual of W. We require to show the biorthogonality
for the extended parts."
REFERENCES,0.2598187311178248,"First we show that, for α of the form (i, i),"
REFERENCES,0.2608257804632427,"⟨wα, vα⟩= ⟨wi,i, vi,i⟩ = 1"
REFERENCES,0.2618328298086606,"2⟨ei · 1⊤+ 1 · e⊤
i , eie⊤
i −aia⊤
i ⟩ = 1"
REFERENCES,0.2628398791540785,"2(⟨ei · 1⊤, eie⊤
i ⟩+ ⟨1 · e⊤
i , eie⊤
i ⟩−⟨ei · 1⊤, aia⊤
i ⟩−⟨1 · e⊤
i , aia⊤
i ⟩ (10)"
REFERENCES,0.2638469284994965,"Let us decompose the summand ⟨ei · 1⊤, aia⊤
i ⟩in detail, so that we can derive the remaining
calculations in a similar manner,"
REFERENCES,0.2648539778449144,"⟨ei · 1⊤, aia⊤
i ⟩= ⟨ei · 1⊤, eie⊤
i ⟩−⟨ei · 1⊤, 1"
REFERENCES,0.26586102719033233,"n1e⊤
i ⟩−⟨ei · 1⊤, 1"
REFERENCES,0.26686807653575023,"nei1⊤⟩+ ⟨ei · 1⊤, 1"
REFERENCES,0.2678751258811682,n2 1 · 1⊤⟩
REFERENCES,0.2688821752265861,= 1 −1
REFERENCES,0.26988922457200404,n −1 + 1
REFERENCES,0.27089627391742194,"n
= 0."
REFERENCES,0.2719033232628399,"Putting this value back in (10), we can complete the rest of the calculation in a similar manner and
finally arrive at the following,
⟨wα, vα⟩= ⟨wi,i, vi,i⟩ = 1"
REFERENCES,0.2729103726082578,"2(⟨ei · 1⊤, eie⊤
i ⟩+ ⟨1 · e⊤
i , eie⊤
i ⟩−⟨ei · 1⊤, aia⊤
i ⟩+ ⟨1 · e⊤
i , aia⊤
i ⟩ = 1"
REFERENCES,0.27391742195367574,2(1 + 1 −0 −0) = 1.
REFERENCES,0.27492447129909364,"Now, let us look at the case where α = (i, i), and β = (j, j) and i ̸= j. Using a similar calculation
we can show that,
⟨wα, vβ⟩= ⟨wi,i, vj,j⟩ = 1"
REFERENCES,0.2759315206445116,"2⟨ei · 1⊤−1 · e⊤
i , eje⊤
j −aja⊤
j ⟩ = 1"
REFERENCES,0.2769385699899295,"2(⟨ei · 1⊤, eje⊤
j ⟩−⟨1 · e⊤
i , eje⊤
j ⟩−⟨ei · 1⊤, aja⊤
j ⟩+ ⟨1 · e⊤
i , aja⊤
j ⟩ = 1"
REFERENCES,0.27794561933534745,2(0 −0 −0 + 0) = 0.
REFERENCES,0.27895266868076535,"Also, between the wα and vβ where α = (i, j) with 1 ≤i < j ≤n and β = (k, k) with 1 ≤k ≤n,
we can show a similar relation by
⟨wα, vβ⟩= ⟨wi,j, vk,k⟩ = 1"
REFERENCES,0.2799597180261833,"2⟨ei,i + ej,j −ei,j −ej,i, eke⊤
k −ake⊤
k ⟩ = 1 2"
REFERENCES,0.2809667673716012,"
δi,k + δj,k −0 −0 −(δi,k −1"
REFERENCES,0.28197381671701915,"n)(δi,k −1"
REFERENCES,0.28298086606243705,"n) −(δj,k −1"
REFERENCES,0.283987915407855,"n)(δj,k −1 n)
 + 1 2"
REFERENCES,0.2849949647532729,"
(δi,k −1"
REFERENCES,0.28600201409869086,"n)(δj,k −1 n)
 = 1 2"
REFERENCES,0.28700906344410876,"
δi,k + δj,k −δi,kδi,k + 1"
REFERENCES,0.2880161127895267,"nδi,k + 1"
REFERENCES,0.2890231621349446,"nδj,k −1"
REFERENCES,0.29003021148036257,"n2 −δj,kδj,k + 1"
REFERENCES,0.29103726082578046,"nδj,k + 1 nδj,k  + 1 2 
−1"
REFERENCES,0.29204431017119836,"n2 + 2δi,kδj,k −2 1"
REFERENCES,0.2930513595166163,"nδj,k −2δi,k + 2 n2  = 1"
REFERENCES,0.2940584088620342,"2(2δi,kδj,k)"
REFERENCES,0.29506545820745217,"= δi,j
= 0."
REFERENCES,0.29607250755287007,"Finally, we need to show for wα with α = (i, i) for 1 ≤i ≤n and vβ where β = (k, l) with
1 ≤k < l ≤n that the respective dot product is 0. This is verified by the calculation"
REFERENCES,0.297079556898288,"⟨wα, vβ⟩= −1"
REFERENCES,0.2980866062437059,"4⟨ei · 1⊤+ 1 · e⊤
i , eke⊤
l + ele⊤
k ⟩ = −1"
REFERENCES,0.2990936555891239,"4(⟨ei · 1⊤, eke⊤
l ⟩+ ⟨ei · 1⊤, ele⊤
k ⟩+ ⟨1 · e⊤
i , eke⊤
l ⟩+ ⟨1 · e⊤
i , ele⊤
k ⟩) = −1"
REFERENCES,0.3001007049345418,4 (δikδil + δikδil + δikδil + δikδil) = −1
REFERENCES,0.3011077542799597,4 (δikδil + δikδil + δikδil + δikδil) = −1
REFERENCES,0.3021148036253776,4 (4δikδil)
REFERENCES,0.3031218529707956,"= −δikδil
= −δkl
= 0"
REFERENCES,0.3041289023162135,"Hence combining the above parts we have proved that ˜V is the dual of ˜
W."
REFERENCES,0.30513595166163143,"B.2
Properties of Dual Basis"
REFERENCES,0.30614300100704933,"Lemma B.1.
˜
W ˜V⊤maps n × n symmetric matrices to itself."
REFERENCES,0.3071500503524673,"Proof. Let x be a vectorized representation of a symmetric matrix X. Then, using the dual basis
expansion, x can be represented as: x = ˜
W ˜VT x. We now apply the the operator ˜
W ˜VT to x."
REFERENCES,0.3081570996978852,"( ˜
W ˜VT ) ˜
W ˜VT x = ˜
W ˜VTx."
REFERENCES,0.30916414904330314,"where the last equality follows from the fact that ˜V is dual to ˜
W."
REFERENCES,0.31017119838872104,"Lemma B.2. For orthonormal basis {wα} and {vα} as defined in Definition 3.2 , the spectral norm
of ˜
W ˜V⊤is 1, that is
 ˜
W ˜V⊤
S∞= 1.
(11)"
REFERENCES,0.311178247734139,"Proof. By definition, ∥˜
W ˜VT ∥S∞= max
∥x∥2=1 ∥˜
W ˜VT x∥2. Note that"
REFERENCES,0.3121852970795569,"∥˜
W ˜VT x∥2
2 = xT ˜V( ˜
WT ˜
W) ˜VT x."
REFERENCES,0.31319234642497484,"By construction of the dual basis, ( ˜
WT ˜
W) = ( ˜VT ˜V)−1. Therefore, ˜V( ˜
WT ˜
W) ˜VT is an orthog-
onal projection operator onto the column space of ˜V. If x is a vectorized representation of any
symmetric matrix, using Lemma B.1 the projection operator is an identity operator. Hence, its
operator norm is 1."
REFERENCES,0.31419939577039274,"C
Sampling Operator Bounds and Proof of Tangent Space Restricted
Isometry Properties"
REFERENCES,0.3152064451158107,"In this section, we first state the matrix Bernstein [Tro11] concentration inequality for rectangular
matrices, which is repeatedly used in the proofs further in the section. We then provide a proof for the
bound of our sampling operator QΩand then we conclude this section by proving the RIP restricted
to the tangent space of manifold of symmetric rank-r matrices at the ground truth X0."
REFERENCES,0.3162134944612286,"Theorem C.1 (Matrix Bernstein for rectangular matrices [Tro11, Theorem 1.6]). Consider the finite
sequence {Zk} of independent, random matrices with dimension d1 × d2. Assume that each random
matrix satisfies
E[Zk] = 0 and ∥Zk∥≤R almost surely,"
REFERENCES,0.31722054380664655,Define
REFERENCES,0.31822759315206445,σ2 := max
REFERENCES,0.3192346424974824,"(
X"
REFERENCES,0.3202416918429003,"k
E[ZkZ∗
k]
,

X"
REFERENCES,0.32124874118831825,"k
E[Z∗
kZk] )"
REFERENCES,0.32225579053373615,"Then, for all t ≥0, P"
REFERENCES,0.32326283987915405,"(
X"
REFERENCES,0.324269889224572,"k
Zk]
 ≥t )"
REFERENCES,0.3252769385699899,"≤(d1 + d2) · exp

−t2/2
σ2 + Rt/3"
REFERENCES,0.32628398791540786,"
(12)"
REFERENCES,0.32729103726082576,"Lemma C.2 (Spectral Norm Bound for Sampling Operator QΩ). Let n be the dimension of the input
matrix and let Ω= (iℓ, jℓ)m
ℓ=1 ⊂I be a multiset of double indices fulfilling m < L = n(n −1)/2
that are sampled independently with replacement. Consequently, we have that with probability of at
least 1 −
2
n2 , the sampling operator QΩ: Sn →Sn of (9)"
REFERENCES,0.3282980866062437,fulfills
REFERENCES,0.3293051359516616,∥QΩ∥S∞≤20L r log n
REFERENCES,0.33031218529707956,"m
+ 1."
REFERENCES,0.33131923464249746,"Proof. We define,"
REFERENCES,0.3323262839879154,"RΩ
′ = VSΩW⊤ =
X"
REFERENCES,0.3333333333333333,"α∈Ω
vαw⊤
α
(13)"
REFERENCES,0.33434038267875127,"So from (13), RΩ= L"
REFERENCES,0.33534743202416917,"m RΩ
′"
REFERENCES,0.3363544813695871,"QΩ= RΩ+VEW⊤
E"
REFERENCES,0.337361530715005,"QΩ
′ = RΩ
′ + L"
REFERENCES,0.338368580060423,"mVEW⊤
E"
REFERENCES,0.3393756294058409,"In order to provide a bound for QΩ, we first evaluate a bound for RΩ. We would use concentration
inequality of Theorem C.1, for computing the bound of QΩ
Let us define"
REFERENCES,0.3403826787512588,zk = L
REFERENCES,0.3413897280966767,"mvαkw⊤
αk −1"
REFERENCES,0.3423967774420947,"mVW⊤
(14)"
REFERENCES,0.3434038267875126,"We compute the expectation of zk by,"
REFERENCES,0.34441087613293053,E[zk] = E[ L
REFERENCES,0.34541792547834843,"mvαkw⊤
αk −1 mVW⊤] = 1 L L
X k=1"
REFERENCES,0.3464249748237664,"L
mvαkw⊤
αk −1 mVW⊤ = 1"
REFERENCES,0.3474320241691843,mVW⊤−1 mVW⊤ = 0 (15)
REFERENCES,0.34843907351460224,Now let us compute the bound for zk
REFERENCES,0.34944612286002014,"||zk||S∞=

L
mvαkw⊤
αk −1"
REFERENCES,0.3504531722054381,"mVW⊤

S∞"
REFERENCES,0.351460221550856,"≤

L
mvαkw⊤
αk"
REFERENCES,0.35246727089627394,"S∞
+ 1 m"
REFERENCES,0.35347432024169184,"VW⊤

S∞"
REFERENCES,0.3544813695871098,"≤

L
mvαkw⊤
αk"
REFERENCES,0.3554884189325277,"S∞
+ 1 m (16)"
REFERENCES,0.3564954682779456,"We use triangle inequality in the first inequality and since the spectral norm of VW⊤is 1, we arrive
at the second inequality.
Now, we can further bound the spectral norm of vαkw⊤
αk by,"
REFERENCES,0.35750251762336355,"||vαkw⊤
αk||S∞= max ⟨V, vαkw⊤
αkW⟩
||V||2||W||2"
REFERENCES,0.35850956696878145,"≤||vαk||2
2
||vαk||2"
REFERENCES,0.3595166163141994,"||wαk||2
2
||wαk||2
≤1 · 4 = 4 (17)"
REFERENCES,0.3605236656596173,"So from (16), we get"
REFERENCES,0.36153071500503525,||zk||S∞≤4L + 1
REFERENCES,0.36253776435045315,"m
(18)"
REFERENCES,0.3635448136958711,"We take the product of zk and z∗
k as,"
REFERENCES,0.364551863041289,"z∗
kzk = ( L"
REFERENCES,0.36555891238670696,"mvαkw⊤
αk −1"
REFERENCES,0.36656596173212486,mVW⊤)⊤( L
REFERENCES,0.3675730110775428,"mvαkw⊤
αk −1 mVW⊤) = ( L"
REFERENCES,0.3685800604229607,"mwαkv⊤
αk −1"
REFERENCES,0.36958710976837866,mWV⊤)( L
REFERENCES,0.37059415911379656,"mvαkw⊤
αk −1 mVW⊤) = L2"
REFERENCES,0.3716012084592145,"m2 (wαkv⊤
αkvαkw⊤
αk) −L"
REFERENCES,0.3726082578046324,"m2 (wαkv⊤
αkVW⊤) −L"
REFERENCES,0.37361530715005037,"m2 (WV⊤vαkw⊤
αk) + 1"
REFERENCES,0.37462235649546827,m2 (WV⊤VW⊤)
REFERENCES,0.3756294058408862,Now taking expectation on both side of the above relation we get;
REFERENCES,0.3766364551863041,"||E(z∗
kzk)||S∞=
E
 L2"
REFERENCES,0.3776435045317221,"m2 (wαkv⊤
αkvαkw⊤
αk)

+ E
 L"
REFERENCES,0.37865055387714,"m2 (wαkv⊤
αkVW⊤)
"
REFERENCES,0.3796576032225579,"+ E
 L"
REFERENCES,0.3806646525679758,"m2 (WV⊤vαkw⊤
αk)

+ E
 1"
REFERENCES,0.3816717019133938,"m2 (WV⊤VW⊤)
 
S∞"
REFERENCES,0.3826787512588117,"≤
E
 L2"
REFERENCES,0.38368580060422963,"m2 (wαkv⊤
αkvαkw⊤
αk)
 
S∞
+
E
 L"
REFERENCES,0.38469284994964753,"m2 (wαkv⊤
αkVW⊤)
 
S∞"
REFERENCES,0.3856998992950655,"+
E
 L"
REFERENCES,0.3867069486404834,"m2 (WV⊤vαkw⊤
αk)
 
S∞
+
E
 1"
REFERENCES,0.3877139979859013,"m2 (WV⊤VW⊤)
 
S∞"
REFERENCES,0.38872104733131924,"=

L2 m2 ""
1
L L
X"
REFERENCES,0.38972809667673713,"k=1
(wαkv⊤
αkvαkw⊤
αk)"
REFERENCES,0.3907351460221551,"# 
S∞
+

1
L "" L
X k=1"
REFERENCES,0.391742195367573,"L
m2 (wαkv⊤
αkVW⊤)"
REFERENCES,0.39274924471299094,"# 
S∞"
REFERENCES,0.39375629405840884,"+

1
L  L"
REFERENCES,0.3947633434038268,"m2 (wαkv⊤
αkVW⊤)
 
S∞
+ 1 m2 ≤L2"
REFERENCES,0.3957703927492447,"m2 max ||wαkv⊤
αkvαkw⊤
αk||S∞+

L
m2 WV⊤VW⊤

S∞"
REFERENCES,0.39677744209466265,"+

L
m2 WV⊤VW⊤

S∞
+ 1 m2"
REFERENCES,0.39778449144008055,≤16L2 + 2L + 1
REFERENCES,0.3987915407854985,"m2
(19)"
REFERENCES,0.3997985901309164,"Since expectation is a linear operator we can preserve the equality in the first equality, then we use
triangle inequality in the first inequality. We further use that the spectral norm of WV⊤is 1 and
arrive at the second inequality. Further using (17) we bound the expression in the last inequality.
As per Theorem C.1 we now compute the product of zk and z∗
k"
REFERENCES,0.40080563947633435,"zkz∗
k = ( L"
REFERENCES,0.40181268882175225,"mvαkw⊤
αk −1"
REFERENCES,0.4028197381671702,mVW⊤)( L
REFERENCES,0.4038267875125881,"mvαkw⊤
αk −1"
REFERENCES,0.40483383685800606,mVW⊤)⊤ = ( L
REFERENCES,0.40584088620342396,"mvαkw⊤
αk −1"
REFERENCES,0.4068479355488419,mVW⊤)( L
REFERENCES,0.4078549848942598,"mwαkv⊤
αk −1 mWV⊤) = L2"
REFERENCES,0.40886203423967776,"m2 (vαkw⊤
αkwαkv⊤
αk) −L"
REFERENCES,0.40986908358509566,"m2 (vαkw⊤
αkWV⊤) −L"
REFERENCES,0.4108761329305136,"m2 (VW⊤wαkv⊤
αk) + 1"
REFERENCES,0.4118831822759315,m2 (VW⊤WV⊤)
REFERENCES,0.41289023162134947,Now taking expectation on both side of the above relation we get;
REFERENCES,0.41389728096676737,"|| E(zkz∗
k)||S∞=

L2"
REFERENCES,0.4149043303121853,"m2 [ 1 L L
X"
REFERENCES,0.4159113796576032,"k=1
(vαkw⊤
αkwαkv⊤
αk)]

S∞"
REFERENCES,0.4169184290030212,"+

1
L L
X k=1"
REFERENCES,0.4179254783484391,"L
m2 (vαkw⊤
αkWV⊤)]

S∞"
REFERENCES,0.418932527693857,"+

1
L
L
m2 (VW⊤wαkv⊤
αk)]

S∞
+ 1 m2 ≤L2"
REFERENCES,0.4199395770392749,"m2 max
vαkw⊤
αkwαkv⊤
αk"
REFERENCES,0.4209466263846928,"S∞
+

L
m2 VW⊤WV⊤

S∞"
REFERENCES,0.4219536757301108,"+

L
m2 VW⊤WV⊤

S∞
+ 1 m2"
REFERENCES,0.4229607250755287,≤16L2 + 2L + 1 m2 (20)
REFERENCES,0.42396777442094663,"So, now by Matrix Bernstein Inequality as stated Theorem C.1,"
REFERENCES,0.42497482376636453,"E(zk) = 0
The value of the bound of zk is denoted by R in Theorem C.1. So R in this case is"
REFERENCES,0.4259818731117825,||zk||S∞≤R = 4L + 1
REFERENCES,0.4269889224572004,"m
Further σ is defined by max[P"
REFERENCES,0.42799597180261834,"k E(zkz∗
k), P"
REFERENCES,0.42900302114803623,"k E(z∗
kzk)] as per Theorem C.1. We have,"
REFERENCES,0.4300100704934542,σ2 = m · 16L2 + 2L + 1
REFERENCES,0.4310171198388721,"m2
= 16L2 + 2L + 1"
REFERENCES,0.43202416918429004,"m
So as per Theorem C.1,"
REFERENCES,0.43303121852970794,"∀t > 0, P(|| L
X"
REFERENCES,0.4340382678751259,"k=1
zk||S∞≥t) ≤2n exp( −t2"
REFERENCES,0.4350453172205438,"2
σ2 + Rt"
REFERENCES,0.43605236656596175,"3
)
(21)"
REFERENCES,0.43705941591137965,"Recalling the definition of zk from (14) || L
X"
REFERENCES,0.4380664652567976,"k=1
zk||S∞≤t =⇒|| L"
REFERENCES,0.4390735146022155,"m QΩ
′ −VW⊤||s∞≤t"
REFERENCES,0.44008056394763345,"Therefore,"
REFERENCES,0.44108761329305135,|| QΩ||s∞= || L
REFERENCES,0.4420946626384693,"m QΩ
′ ||s∞ ≤|| L"
REFERENCES,0.4431017119838872,"m QΩ
′ −VW⊤||s∞+ ||VW⊤||s∞"
REFERENCES,0.44410876132930516,≤t + 1 (22)
REFERENCES,0.44511581067472306,with a probability 1 −2n exp( −t2
REFERENCES,0.446122860020141,"2
σ2+ Rt"
REFERENCES,0.4471299093655589,3 ). We have arrived at the above inequality from the definition of
REFERENCES,0.44813695871097686,"QΩand that the spectral norm of the extended basis of the dual pair, VEW⊤
E is 1.
Since L = n(n−1)"
REFERENCES,0.44914400805639476,"2
so, L2 ≤n2L"
REFERENCES,0.4501510574018127,"2 . We can further simplify the denominator by the following,

σ2 + Rt 3"
REFERENCES,0.4511581067472306,"
= 16L2 + 2L + 1"
REFERENCES,0.4521651560926485,"m
+ 4L + 1"
"M
T",0.45317220543806647,"3m
t ≤16L2"
"M
T",0.45417925478348437,"m
+ 2L m + L"
"M
T",0.4551863041289023,m + 2Lt m ≤L
"M
T",0.4561933534743202,m(16L + 3 + 2t) (23)
"M
T",0.45720040281973817,We arrive at the first inequality by using 1 m ≤L
"M
T",0.45820745216515607,m and 4L+1
M,0.459214501510574,"3m
≤6L"
M,0.4602215508559919,3m
M,0.4612286002014099,"So using this relation, let us assume that t = 20L
q log n"
M,0.4622356495468278,"m .
Further let us simply Equation (23), by using this value of t. Since by Theorem 4.3, m ≥Kn log n,
where K = Cνr, so log n"
M,0.46324269889224573,"m
≤
1
Kn, hence we can say simplify Equation (23), further by the following,"
M,0.46424974823766363,"
σ2 + Rt 3 
≤L"
M,0.4652567975830816,m(16L + 3 + 2t) ≤L m 
M,0.4662638469284995,16L + 3 + 2 · 20L r log n m ! ≤L m 
M,0.46727089627391744,16L + 3 + 2 · 20L r
KN,0.46827794561933533,"1
Kn ! ≤L"
KN,0.4692849949647533,m (60L) (24)
KN,0.4702920443101712,We arrive at the third inequality bu using the expression for L and using logn
KN,0.47129909365558914,"m
≤
1
Kn.
Then, we can deduce the following from Equation (24), exp −t2"
KN,0.47230614300100704,"2
σ2 + Rt 3 ! = exp"
KN,0.473313192346425,−202L2 log n
M,0.4743202416918429,"2m
60L2 m !"
M,0.47532729103726085,"≤exp

−400"
LOG N,0.47633434038267874,"120 log n
"
LOG N,0.4773413897280967,= exp(3.33 log( 1 n))
LOG N,0.4783484390735146,≤exp(log( 1 n3 )) = 1 n3
LOG N,0.47935548841893255,"So the probability in (21) becomes 2n ·
1
n3 If we use (22), then we can conclude that that the spectral"
LOG N,0.48036253776435045,"norm of QΩis bounded by

20L
q log n"
LOG N,0.4813695871097684,"m
+ 1

. Hence, we can say that the spectral norm of QΩis"
LOG N,0.4823766364551863,"bounded by

20L
q log n"
LOG N,0.48338368580060426,"m
+ 1

with a probability of 1 −
2
n2 ."
LOG N,0.48439073514602216,"Now we prove the restricted isometry property of the sampling operator QΩwith respect to the
tangent space of the rank-r matrix manifold at the ground truth, as stated in Theorem 4.5."
LOG N,0.48539778449144005,Proof of Theorem 4.5. Define zk = L
LOG N,0.486404833836858,"mPT0wαkv⊤
αkPT0 −1"
LOG N,0.4874118831822759,"mPT0WV⊤PT0.
Then,"
LOG N,0.48841893252769386,"E(zk) = E
 L"
LOG N,0.48942598187311176,"mPT0wαkv⊤
αkPT0 −1"
LOG N,0.4904330312185297,"mPT0WV⊤PT0  = 1 L L
X ℓ=1  L"
LOG N,0.4914400805639476,"mPT0wℓv⊤
ℓPT0 −1"
LOG N,0.49244712990936557,mPT0WV⊤PT0  = 1
LOG N,0.49345417925478346,mPT0WV⊤PT0 −1
LOG N,0.4944612286002014,mPT0WV⊤PT0 = 0 (25)
LOG N,0.4954682779456193,Now we calculate the spectral norm of zk
LOG N,0.49647532729103727,"||zk||S∞=

L
mPT0wαkv⊤
αkPT0 −1"
LOG N,0.49748237663645517,mPT0WV⊤PT0 S∞
LOG N,0.4984894259818731,"≤

L
mPT0wαkv⊤
αkPT0"
LOG N,0.499496475327291,"S∞
+ 1 m"
LOG N,0.500503524672709,"PT0WV⊤PT0

S∞ ≤L"
LOG N,0.5015105740181269,"m||PT0wαk||F ||PT0vαk||F + 1 m ≤2L m r 2νr n r 2νr n
+ 1 m ≤4νrL"
LOG N,0.5025176233635448,mn + 1 m (26)
LOG N,0.5035246727089627,The third inequality follows from the coherence bounds as in Definition 4.1 .
LOG N,0.5045317220543807,"z∗
kzk = ( L"
LOG N,0.5055387713997986,"mPT0wαkv⊤
αkPT0 −1"
LOG N,0.5065458207452165,mPT0WV⊤PT0)⊤( L
LOG N,0.5075528700906344,"mPT0wαkv⊤
αkPT0 −1"
LOG N,0.5085599194360524,mPT0WV⊤PT0)
LOG N,0.5095669687814703,= ( L2
LOG N,0.5105740181268882,"m2 PT0vαkw⊤
αkPT0wαkv⊤
αkPT0) −( L"
LOG N,0.5115810674723061,"m2 PT0vαkw⊤
αkPT0PT0WV⊤PT0) −( L"
LOG N,0.5125881168177241,"m2 PT0VW⊤PT0PT0wαkv⊤
αkPT0) + 1"
LOG N,0.513595166163142,m2 PT0VW⊤PT0PT0WV⊤PT0
LOG N,0.5146022155085599,"We use that P2
T0 = PT0 in the second equality to further calculate the expectation of the above
equality,"
LOG N,0.5156092648539778,"E(z∗
kzk) = E( L2"
LOG N,0.5166163141993958,"m2 PT0vαkw⊤
αkPT0wαkv⊤
αkPT0) −E(( L"
LOG N,0.5176233635448136,"m2 PT0vαkw⊤
αkPT0PT0WV⊤PT0)"
LOG N,0.5186304128902316,−E(( L
LOG N,0.5196374622356495,"m2 PT0VW⊤PT0PT0wαkv⊤
αkPT0)) + E( 1"
LOG N,0.5206445115810675,m2 PT0VW⊤PT0PT0WV⊤PT0)
LOG N,0.5216515609264853,= E( L2
LOG N,0.5226586102719033,"m2 PT0vαkw⊤
αkPT0wαkv⊤
αkPT0) −1"
LOG N,0.5236656596173213,m2 PT0VW⊤PT0WV⊤PT0 −1
LOG N,0.5246727089627392,m2 PT0VW⊤PT0PT0WV⊤PT0 + 1
LOG N,0.525679758308157,m2 PT0VW⊤PT0PT0WV⊤PT0
LOG N,0.526686807653575,= E( L2
LOG N,0.527693856998993,"m2 PT0vαkw⊤
αkPT0wαkv⊤
αkPT0) −1"
LOG N,0.5287009063444109,m2 PT0VW⊤PT0PT0WV⊤PT0
LOG N,0.5297079556898288,"since ˜
W ˜V
⊤= Id, we can adjust summand at the seocond equality.
Defining the random operator ezk := L"
LOG N,0.5307150050352467,"mPT0wαkv⊤
αkPT0, we see that zk from above satisfies zk =
ezk −1"
LOG N,0.5317220543806647,mPT0WV⊤PT0 and therefore
LOG N,0.5327291037260826,E[ezk] = 1
LOG N,0.5337361530715005,mPT0WV⊤PT0
LOG N,0.5347432024169184,"With this definition, and for the fact that positive semi-definite matrices satisfies"
LOG N,0.5357502517623364,"∥A −B∥S∞≤max(∥A∥S∞, ∥B∥S∞), we can bound the spectral norm of the expectation in the
following way"
LOG N,0.5367573011077543,"∥E(z∗
kzk)∥S∞=
E
ez∗
kezk

−1"
LOG N,0.5377643504531722,m2 PT0WV⊤PT0PT0VW⊤PT0 S∞
LOG N,0.5387713997985901,"≤max
 E
ez∗
kezk

S∞, 1 m2"
LOG N,0.5397784491440081,"
(27)"
LOG N,0.540785498489426,"For any M ∈Rn×n, it follows from the definition of f
zk"
LOG N,0.5417925478348439,ezk(M) = L
LOG N,0.5427995971802618,"m⟨vαk, PT0(M)⟩PT0(wαk) = L"
LOG N,0.5438066465256798,"m⟨PT0(vαk), M⟩PT0(wαk),"
LOG N,0.5448136958710977,"ez∗
k(M) = L"
LOG N,0.5458207452165156,"m⟨wαk, PT0(M)⟩PT0(vαk) = L"
LOG N,0.5468277945619335,"m⟨PT0(wαk), M⟩PT0(vαk)"
LOG N,0.5478348439073515,and thus
LOG N,0.5488418932527693,"ez∗
kezk(M) = L"
LOG N,0.5498489425981873,"m⟨PT0(wαk),ezk(M)⟩PT0(vαk) = L2"
LOG N,0.5508559919436052,"m2 ⟨PT0(wαk), ⟨PT0(vαk), M⟩PT0(wαk)⟩PT0(vαk) = L2"
LOG N,0.5518630412890232,"m2 ∥PT0(wαk)∥2
F ⟨PT0(vαk), M⟩PT0(vαk)"
LOG N,0.552870090634441,"Now using the incoherence condition such as Definition 4.1 and [TL18, Lemma 22]. we can argue
that"
LOG N,0.553877139979859,"E
ez∗
kezk

S∞≤L2"
LOG N,0.5548841893252769,"m2
L
max
ℓ=1 ∥PT0(wℓ)∥2
F
E

PT0vαkv⊤
αkPT0

S∞ = L2"
LOG N,0.5558912386706949,"m2
L
max
ℓ=1 ∥PT0(wℓ)∥2
F"
L,0.5568982880161127,"1
L L
X"
L,0.5579053373615307,"ℓ=1
PT0vℓvℓ
⊤PT0 S∞ ≤L2"
L,0.5589123867069486,"m2
L
max
ℓ=1 ∥PT0(wℓ)∥2
F
L
max
ℓ=1 ∥PT0(vαk)∥2
s∞ ≤L2"
L,0.5599194360523666,m2 (2ν r n) 1
L,0.5609264853977844,L(8ν r n) = L
L,0.5619335347432024,m2 16ν2r2 n2 .
L,0.5629405840886204,Now we get from (27)
L,0.5639476334340383,"∥E(z∗
kzk)∥S∞≤max
 L"
L,0.5649546827794562,m2 (16ν2r2
L,0.5659617321248741,"n2 ), 1 m2"
L,0.5669687814702921,"
(28)"
L,0.56797583081571,"So by Triangle Inequality, m
X"
L,0.5689828801611279,"k=1
E(z∗
kzk) S∞
≤ m
X"
L,0.5699899295065458,"k=1
∥E(z∗
kzk)∥S∞≤m · L m2"
L,0.5709969788519638,"
16ν2r2 n2 
= L m"
L,0.5720040281973817,"
16ν2r2 n2"
L,0.5730110775427996,"
(29)"
L,0.5740181268882175,"Now we need to compute similar bounds for E[zkz∗
k]."
L,0.5750251762336355,"∥E[ezkez∗
k]∥s∞=
 E
 L2"
L,0.5760322255790534,"m2 PT0wαkv⊤
αkPT0vαkw⊤
αkPT0

s∞"
L,0.5770392749244713,"=

L2 m2 L
X ℓ=1"
L,0.5780463242698892,"1
LPT0wℓv⊤
ℓPT0vℓw⊤
ℓPT0 s∞ ≤L"
L,0.5790533736153072,"m max
ℓ
∥PT0wℓv⊤
ℓPT0vℓw⊤
ℓPT0∥s∞ ≤L"
L,0.5800604229607251,"m max
ℓ
∥PT0wαk∥2
s∞max
ℓ
∥PT0vαk∥2
s∞ ≤L"
L,0.581067472306143,m2 (2ν r
L,0.5820745216515609,n)(8ν r n) = L
L,0.5830815709969789,m2 16ν2r2 n2 (30)
L,0.5840886203423967,"So similar to (29) we apply Triangle Inequality on (30) and get, m
X"
L,0.5850956696878147,"k=1
E(zkz∗
k) S∞
≤ m
X"
L,0.5861027190332326,"k=1
∥E(zkz∗
k)∥S∞≤m · L m2"
L,0.5871097683786506,"
16ν2r2 n2 
= L m"
L,0.5881168177240684,"
16ν2r2 n2"
L,0.5891238670694864,"
(31)"
L,0.5901309164149043,"Now, we can approximate this bound in the following way,
Since L = n(n−1)"
L,0.5911379657603223,"2
, so L ≤n2"
THIS BOUND CAN BE WRITTEN AS,0.5921450151057401,"2 this bound can be written as m
X"
THIS BOUND CAN BE WRITTEN AS,0.5931520644511581,"k=1
E(zkz∗
k) S∞
≤L m"
THIS BOUND CAN BE WRITTEN AS,0.594159113796576,"
16ν2r2 n2"
THIS BOUND CAN BE WRITTEN AS,0.595166163141994,"
≤8ν2r2"
THIS BOUND CAN BE WRITTEN AS,0.5961732124874118,"m
(32)"
THIS BOUND CAN BE WRITTEN AS,0.5971802618328298,"Comparing the equations (31) and (32), we can argue that the bound on (31) is larger than that of (32)
as L"
THIS BOUND CAN BE WRITTEN AS,0.5981873111782477,"2 is larger than νr. So m
X"
THIS BOUND CAN BE WRITTEN AS,0.5991943605236657,"k=1
E(zkz∗
k)"
THIS BOUND CAN BE WRITTEN AS,0.6002014098690835,"S∞
≤8ν2r2"
THIS BOUND CAN BE WRITTEN AS,0.6012084592145015,"m
≤4νrL"
THIS BOUND CAN BE WRITTEN AS,0.6022155085599195,"m
(33)"
THIS BOUND CAN BE WRITTEN AS,0.6032225579053374,"So, now by Matrix Bernstein Inequality as stated Theorem C.1,"
THIS BOUND CAN BE WRITTEN AS,0.6042296072507553,"E(zk) = 0
The value of the bound of zk is denoted by R in Theorem C.1. So R in this case is"
THIS BOUND CAN BE WRITTEN AS,0.6052366565961732,||zk||S∞≤R = 4νrL
THIS BOUND CAN BE WRITTEN AS,0.6062437059415912,mn + 1
THIS BOUND CAN BE WRITTEN AS,0.6072507552870091,"m
Further σ is defined by max E(zkz∗
k), E(z∗
kzk)] as per Theorem C.1. We have,"
THIS BOUND CAN BE WRITTEN AS,0.608257804632427,"σ2 = max
 m
X"
THIS BOUND CAN BE WRITTEN AS,0.6092648539778449,"k=1
E(zkz∗
k) S∞
,  m
X"
THIS BOUND CAN BE WRITTEN AS,0.6102719033232629,"k=1
E(z∗
kzk) S∞ "
THIS BOUND CAN BE WRITTEN AS,0.6112789526686808,σ2 = L m 4νr n 
THIS BOUND CAN BE WRITTEN AS,0.6122860020140987,"So as per Theorem C.1,"
THIS BOUND CAN BE WRITTEN AS,0.6132930513595166,"∀t > 0, P(|| L
X"
THIS BOUND CAN BE WRITTEN AS,0.6143001007049346,"k=1
zk||S∞≥t) ≤2n exp( −t2"
THIS BOUND CAN BE WRITTEN AS,0.6153071500503524,"2
σ2 + Rt"
THIS BOUND CAN BE WRITTEN AS,0.6163141993957704,"3
)
(34)"
THIS BOUND CAN BE WRITTEN AS,0.6173212487411883,"Now we would calculate the bound for the above probability, In this scenario, since L = n(n−1)"
THIS BOUND CAN BE WRITTEN AS,0.6183282980866063,"2
, we
can bound (σ2 + Rϵ"
THIS BOUND CAN BE WRITTEN AS,0.6193353474320241,"3 ) by the following inequality in the second,"
THIS BOUND CAN BE WRITTEN AS,0.6203423967774421,(σ2 + Rϵ
THIS BOUND CAN BE WRITTEN AS,0.62134944612286,3 ) = L m 4νr n
THIS BOUND CAN BE WRITTEN AS,0.622356495468278,"
+
4νrL"
THIS BOUND CAN BE WRITTEN AS,0.6233635448136958,mn + 1 m  ϵ 3
THIS BOUND CAN BE WRITTEN AS,0.6243705941591138,"=
4νrL nm"
THIS BOUND CAN BE WRITTEN AS,0.6253776435045317,"
1 + ϵ 3"
THIS BOUND CAN BE WRITTEN AS,0.6263846928499497,"
+
ϵ
3m"
THIS BOUND CAN BE WRITTEN AS,0.6273917421953675,"≤
8νrL nm"
THIS BOUND CAN BE WRITTEN AS,0.6283987915407855,"
+
ϵ
3m"
THIS BOUND CAN BE WRITTEN AS,0.6294058408862034,"≤
9νrL nm "
THIS BOUND CAN BE WRITTEN AS,0.6304128902316214,We use ϵ
THIS BOUND CAN BE WRITTEN AS,0.6314199395770392,3 is less than 1 in the first inequality and the the second summand is less than νrL
THIS BOUND CAN BE WRITTEN AS,0.6324269889224572,"nm in the
second inequality.
Here further we used that ϵ ≤1"
THIS BOUND CAN BE WRITTEN AS,0.6334340382678751,"2 and ν ≥1, r ≥1 and bound the probability as follows,"
N EXP,0.6344410876132931,"2n exp

−ϵ2"
N EXP,0.6354481369587109,"2
σ2 + Rϵ 3"
N EXP,0.6364551863041289,"
≤2n exp
 −ϵ2"
N EXP,0.6374622356495468,"2
9νrL nm "
N EXP,0.6384692849949648,"= 2n exp
−ϵ2nm 18νrL "
N EXP,0.6394763343403826,"≤2n exp
−49νrn2 log n 18νrL "
N EXP,0.6404833836858006,"≤2n exp
−98 log n 18 "
N EXP,0.6414904330312186,"So, given that m ≥49νnr"
N EXP,0.6424974823766365,"ϵ2
log n holds true, we can arrive at the first inequality above, further, since
L = n(n−1)"
N EXP,0.6435045317220544,"2
so we can say, L ≤n2/2 hence we get the second inequality. We can bound this further
by,"
N EXP,0.6445115810674723,"2n exp

−ϵ2"
N EXP,0.6455186304128903,"2
σ2 + Rϵ 3"
N EXP,0.6465256797583081,"
≤2n exp
−98 log n 18 "
N EXP,0.6475327291037261,"≤2n exp

−4 log n
"
N EXP,0.648539778449144,"≤2n exp

log
  1"
N EXP,0.649546827794562,"n
2
 = 2 n3"
N EXP,0.6505538771399798,"Hence we can conclude from here that || Pm
k=1 zk||S∞≤ϵ) holds with a probability of 1 −
2
n3 .
Now we can derive the bound
PT0 Q*
ΩPT0 −PT0

S∞≤ϵ in the statement of Theorem 4.5 by the
following argument, m
X"
N EXP,0.6515609264853978,"k=1
zk s∞
=  L
X"
N EXP,0.6525679758308157,"ℓ=1
( L"
N EXP,0.6535750251762337,"mPT0wαℓv⊤
αℓPT0 −1"
N EXP,0.6545820745216515,mPT0WV⊤PT0) s∞
N EXP,0.6555891238670695,"=
PT0 R*
ΩPT0 −PT0WV⊤PT0

S∞"
N EXP,0.6565961732124874,"=
PT0 R*
ΩPT0 + PT0wEv⊤
EPT0 −(PT0WV⊤PT0 + PT0wEv⊤
EPT0)

S∞"
N EXP,0.6576032225579054,"=
PT0 Q*
ΩPT0 −PT0

S∞≤ϵ"
N EXP,0.6586102719033232,"We get the get implication by adjusting the equation with addition and subtraction of PT0wEv⊤
EPT0
because PT0WV⊤PT0 + PT0wEv⊤
EPT0 = PT0 ˜
W ˜V⊤PT0 and ˜
W ˜V⊤PT0 = PT0
So, we have proved the assertion of Theorem 4.5 is true with a probability of 1 −
2
n3 ."
N EXP,0.6596173212487412,"Following Proposition 3.1 in [RXH11], if we change the sampling model to sampling without
replacement we should have the same bound with same failure probability."
N EXP,0.6606243705941591,"D
Proof of Local Quadratic Convergence"
N EXP,0.6616314199395771,"In this section we provide the proof of the local convergence theorem as stated in Theorem 4.3.
Lemma D.1. Let 0 < ϵ ≤1"
N EXP,0.6626384692849949,"2, let X0 ∈Sn be a ν-incoherent matrix. Let PT0 : Sn →Sn be the
projection operator associated to T0. Then assume that the following three conditions hold:"
N EXP,0.6636455186304129,"(a) For QΩ: Sn →Sn be defined as in (13) from m independent uniformly sampled locations, we
have :"
N EXP,0.6646525679758308,"∥QΩ∥S∞≤

20L r log n"
N EXP,0.6656596173212488,"m
+ 1

."
N EXP,0.6666666666666666,"(b) The tangent space T0 = TX0 onto the rank-r manifold T0 = TX0Mr fulfills :
PT0 Q*
ΩPT0 −PT0

S∞
≤ε
(35)"
N EXP,0.6676737160120846,"(c) The spectral norm distance between X and X0 fulfills:
X −X0
S∞≤
ϵ

20L
q log n"
N EXP,0.6686807653575025,"m
+ 1
σr(X0)
(36)"
N EXP,0.6696878147029205,"Then the tangent space T = TX onto the rank-r manifold at X fulfills:
PT Q*
ΩPT −PT

S∞≤4ε
(37)"
N EXP,0.6706948640483383,"Lemma D.1 is a literal adaptation of [KV21, Lemma B.3] to the operator Q*
Ω, which is why we omit
its proof. Similarly, we can use Lemmas B.8 and B.9 of [KV21] in the same way for our proofs.
Lemma D.2. Let X0 ∈Sn be a matrix of rank r that is ν-incoherent, and let Ω= (iℓ, jℓ)m
ℓ=1
be a random index set of cardinality |Ω| = m that is sampled uniformly without replacement, or,
alternatively, sampled independently with replacement. There exists constants C, eC, C1 such that if"
N EXP,0.6717019133937563,"m ≥Cνrn log n
(38)"
N EXP,0.6727089627391742,"then, with probability at least 1 −
2
n2 , the following holds: For each matrix X(k) ∈Sn fulfilling"
N EXP,0.6737160120845922,∥X(k) −X0∥S∞≤C1
N EXP,0.67472306143001,"√m
L√log nσr(X0),
(39)"
N EXP,0.675730110775428,"it follows that the projection PTk : Sn →Sn onto the tangent space Tk := TTr(X(k))Mr satisfies
PTk Q*
ΩPTk −PTk

S∞≤2 5,"
N EXP,0.676737160120846,"and furthermore,"
N EXP,0.6777442094662638,∥η∥F ≤eCL r log n
N EXP,0.6787512588116817,"m ∥PT ⊥
k (η)∥F ."
N EXP,0.6797583081570997,for each matrix η ∈ker QΩin the null space of the operator QΩ: Sn →Sn.
N EXP,0.6807653575025177,"Proof of Lemma D.2. Assume that there are m locations Ω= (iℓ, jℓ)m
ℓ=1 in n × n sampled inde-
pendently uniformly with replacement, where m fulfills (38) with C := 49/ε2 and ε = 0.1. By
Lemma C.2, it follows that the corresponding operator QΩfrom Lemma C.2 fulfills"
N EXP,0.6817724068479355,"|| QΩ∥S∞≤

20L r log n"
N EXP,0.6827794561933535,"m
+ 1

.
(40)"
N EXP,0.6837865055387714,"on an event called eΩ, which occurs with a probability of at least 1 −2"
N EXP,0.6847935548841894,"n, and by Theorem 4.5, the
tangent space T0 = TX0Mr corresponding to the µ0-incoherent rank-r matrix X0 fulfills
PT0Q*
ΩPT0 −PT0

S∞≤ε"
N EXP,0.6858006042296072,"on an event called eΩ,T0, which occurs with a probability of at least 1 −n−2. Let ˜ϵ =
1
10. If
X(k) ∈Rn×n is such that ∥X(k) −X0∥S∞≤eξσr(X0) with"
N EXP,0.6868076535750252,"eξ =
˜ϵ

20L
q log n"
N EXP,0.6878147029204431,"m
+ 1
 ≤
1

20 · 10L
q log n"
N EXP,0.6888217522658611,"m

(41)"
N EXP,0.6898288016112789,"it follows by Lemma D.1 that on the event EΩ∩EΩ,T0, the tangent space Tk := X(k) onto the rank-r
manifold at X(k) fulfills
PTkQ*
ΩPTk −PTk

S∞≤4˜ϵ = 2"
N EXP,0.6908358509566969,"5.
(42)"
N EXP,0.6918429003021148,"Next, we claim that on the event EΩ∩EΩ,T0,"
N EXP,0.6928499496475328,∥η∥F ≤eCL r log n
N EXP,0.6938569989929506,"m ∥PT ⊥
k (η)∥F .
(43)"
N EXP,0.6948640483383686,for any for each matrix η ∈ker QΩin the null space of the operator QΩ: Sn →Sn.
N EXP,0.6958710976837865,Let η ∈ker QΩ. Then
N EXP,0.6968781470292045,"∥PTk(η)∥2
F = ⟨PTk(η), PTk(η)⟩"
N EXP,0.6978851963746223,"=
D
PTk(η), PTk Q*
ΩPTk(η)
E
+
D
PTk(η), PTk(η) −PTk Q*
ΩPTk(η)
E"
N EXP,0.6988922457200403,"≤
D
PTk(η), PTk Q*
ΩPTk(η)
E
+ ∥PTk(η)∥F
PTk −PTk Q*
ΩPTk

S∞∥PTk(η)∥F"
N EXP,0.6998992950654582,"≤
D
PTk(η), PTk Q*
ΩPTk(η)
E
+ 4ϵ ∥PTk(η)∥2
F"
N EXP,0.7009063444108762,"Using (42) in the last inequality, implies that"
N EXP,0.701913393756294,"∥PTk(η)∥2
F ≤
1
1 −5ϵ⟨PTk(η), PTk Q*
ΩPTk(η)⟩"
N EXP,0.702920443101712,"≤
1
1 −5ϵ⟨PTk(η), PTk Q*
ΩPTk(η)⟩"
N EXP,0.7039274924471299,"≤
1
1 −4ϵ⟨PTk(η)2, Q*
ΩPTk(η)⟩"
N EXP,0.7049345417925479,"≤
1
1 −4ϵ⟨QΩPTk(η), PTk(η)2)⟩"
N EXP,0.7059415911379657,"≤
1
1 −4ϵ⟨QΩPTk(η), PTkPTk(η)⟩"
N EXP,0.7069486404833837,"≤
1
1 −4ϵ⟨PTk QΩPTk(η), PTk(η)⟩"
N EXP,0.7079556898288016,"≤
1
1 −4ϵ⟨PTk(η), PTk QΩPTk(η)⟩"
N EXP,0.7089627391742196,"≤
1
1 −4ϵ∥PTk(η)∥F ∥PTk∥S∞∥QΩPTk(η)∥F (44)"
N EXP,0.7099697885196374,"Dividing by ∥PTk(η)∥F on both sides we get,"
N EXP,0.7109768378650554,"∥PTk(η)∥F ≤
1
1 −4ϵ∥PTk∥S∞∥QΩPTk(η)∥F"
N EXP,0.7119838872104733,≤2∥QΩPTk(η)∥F
N EXP,0.7129909365558912,"Since spectral norm of PTk. is bounded by 1. Furthermore, we used that ϵ ≤
1
10 in the last inequality."
N EXP,0.7139979859013091,"Since η ∈ker QΩ, it holds that"
N EXP,0.7150050352467271,"0 = ∥QΩ(η)∥F =
QΩ

PTk(η) + PT ⊥
k (η)

F ≥∥QΩPTk(η)∥F −∥QΩPT ⊥
k (η)∥F"
N EXP,0.716012084592145,so that
N EXP,0.7170191339375629,"∥QΩPTk(η)∥F ≤∥QΩPT ⊥
k (η)∥F ≤

20L r log n"
N EXP,0.7180261832829808,"m
+ 1

∥PT ⊥
k (η)∥F ,"
N EXP,0.7190332326283988,"where we used (40) in the last inequality. Inserting this above, we obtain"
N EXP,0.7200402819738168,"∥η∥2
F = ∥PTk(η)∥2
F + ∥PT ⊥
k (η)∥2
F ≤ "
N EXP,0.7210473313192346,"4

20L r log n"
N EXP,0.7220543806646526,"m
+ 1
2
+ 1 !"
N EXP,0.7230614300100705,"∥PT ⊥
k (η)∥2
F"
N EXP,0.7240684793554885,"≤5

21L r log n m"
N EXP,0.7250755287009063,"2
∥PT ⊥
k (η)∥2
F"
N EXP,0.7260825780463243,Since L
N EXP,0.7270896273917422,m log n > 1 So we get
N EXP,0.7280966767371602,∥η∥F ≤eCL r log n
N EXP,0.729103726082578,"m ∥PT ⊥
k (η)∥F .
(45)"
N EXP,0.730110775427996,"for the constant eC defined by,
eC :=
√"
N EXP,0.7311178247734139,5 · 21
N EXP,0.7321248741188319,"Moreover, we observe that for C1 :=
1
20 e
C where C1 is the constant of (39), it holds that"
N EXP,0.7331319234642497,"eξ ≤
1

10 · 20L log n"
N EXP,0.7341389728096677,"m
 = C1
√m
L√log n"
N EXP,0.7351460221550856,"implying that the two statements of Lemma D.2 are satisfied on the event EΩ∩EΩ,T0 if (39) holds.
By the above mentioned probability bounds and a union bound, EΩ∩EΩ,T0 occurs with a probability
of at least 1 −2n−2, finishing the proof for the sampling with replacement model. By the argument
of Proposition 3 of [Rec11], the result extends to the model of sampling locations drawn uniformly
at random without replacement, with the same probability bound. This concludes the proof of
Lemma D.2."
N EXP,0.7361530715005036,The following lemma will also play a role in the proof of Theorem 4.3.
N EXP,0.7371601208459214,"Lemma D.3. Let C, eC, C1 be the constants of Lemma D.2 and µ0 be the incoherence factor of a
rank-r matrix X0. If
m ≥Cνrn log n
and if η(k) = X(k) −X0 fulfills
∥η(k)∥S∞≤ξσr(X0),
with"
N EXP,0.7381671701913394,"ξ := min
 C1
√m
L√log n,
10C1
√m
8L√rκ√log n "
N EXP,0.7391742195367573,"then, on the event of Lemma D.2, it holds that"
N EXP,0.7401812688821753,"∥η(k)∥S∞≤2 eCL r log n m (
√"
N EXP,0.7411883182275931,"n −r)σr+1(X(k))
(46)"
N EXP,0.7421953675730111,"Proof. First, we compute that"
N EXP,0.743202416918429,"∥PT ⊥
k (η(k))∥F ≤∥PT ⊥
k (X(k))∥F + ∥PT ⊥
k (X0)∥F ≤"
N EXP,0.7442094662638469,"v
u
u
t d
X"
N EXP,0.7452165156092648,"i=r+1
σ2
i (X(k)) +
U(k)
⊥U(k)∗
⊥
X0V(k)
⊥V(k)∗
⊥

F ≤
√"
N EXP,0.7462235649546828,"n −rσr+1(X(k)) + ∥U(k)∗
⊥
U0∥S∞∥Σ0∥F ∥V∗
0V(k)
⊥∥S∞ ≤
√"
N EXP,0.7472306143001007,"n −rσr+1(X(k)) +
2∥η(k)∥2
S∞
(1 −ζ)2σ2r(X0)
√rσ1(X0) =
√"
N EXP,0.7482376636455186,"n −rσr+1(X(k)) +
2∥η(k)∥2
S∞
(1 −ζ)2σr(X0)
√rκ,"
N EXP,0.7492447129909365,"where 0 < ζ < 1 such that ∥X(k) −X0∥S∞≤ζσr(X0), using Lemma D.4 twice in the fourth
inequality and ∥AB∥F ≤∥A∥S∞∥B∥F all matrices A and B, referring to the notations of lemma
B.8 in [KV21](see below) for U0, Σ0, V0, U(k)
⊥and V(k)
⊥."
N EXP,0.7502517623363545,"Using Lemma D.2 for η(k) = X(k) −X0, we obtain on the event on which the statement of
Lemma D.2 holds that"
N EXP,0.7512588116817724,∥η(k)∥S∞≤∥η(k)∥F ≤eCL r log n
N EXP,0.7522658610271903,"m ∥PT ⊥
k (η(k))∥F ≤eCL r log n m √"
N EXP,0.7532729103726082,"n −rσr+1(X(k)) + 8√rκ∥η(k)∥2
S∞
σr(X0) ! ≤eCL r log n m √"
N EXP,0.7542799597180262,"n −rσr+1(X(k)) + 8 · 10C1
√m√rκ
8L√log n√rκ ∥η(k)∥S∞ "
N EXP,0.7552870090634441,"since C1 =
1
20 e
C , after rearranging we get,"
N EXP,0.756294058408862,"
1 −1 2"
N EXP,0.75730110775428,"
∥η(k)∥S∞≤eCL r log n m (
√"
N EXP,0.7583081570996979,n −r)σr+1(X(k))
N EXP,0.7593152064451159,which implies the statement of this lemma.
N EXP,0.7603222557905337,"Lemma D.4 (Wedin’s bound [Ste06]). Let X and bX be two matrices of the same size and their
singular value decompositions"
N EXP,0.7613293051359517,"X = (U
U⊥)

Σ
0
0
Σ⊥"
N EXP,0.7623363544813696," 
V∗
V∗
⊥"
N EXP,0.7633434038267876,"
and
bX =
  bU
bU⊥
 bΣ
0
0
bΣ⊥"
N EXP,0.7643504531722054,"  bV∗
bV∗
⊥ 
,"
N EXP,0.7653575025176234,"where the submatrices have the sizes of corresponding dimensions. Suppose that δ, α satisfying
0 < δ ≤α are such that α ≤σmin(Σ) and σmax(bΣ⊥) < α −δ. Then"
N EXP,0.7663645518630413,"∥bU∗
⊥U∥S∞≤
√"
N EXP,0.7673716012084593,2∥X −bX∥S∞
N EXP,0.7683786505538771,"δ
and ∥bV∗
⊥V∥S∞≤
√"
N EXP,0.7693856998992951,2∥X −bX∥S∞
N EXP,0.770392749244713,"δ
.
(47)"
N EXP,0.771399798590131,"Now using the above lemma let us conclude the proof for the theorem,"
N EXP,0.7724068479355488,"Proof of Theorem 4.3. Let k = k0 and X(k) be the k-th iterate of MatrixIRLS fro EDG with
the parameters stated in Theorem 4.3. Under the sampling model of Theorem 4.3, if the number
of samples m fulfills m ≥Cνrn log n, where C is the constant of Lemma D.2, we know from
Lemma D.2"
N EXP,0.7734138972809668,if furthermore η(k) := X(k) −X0 fulfills
N EXP,0.7744209466263847,"∥η(k)∥S∞≤ξσr(X0)
(48) with"
N EXP,0.7754279959718026,"ξ ≤C1
√m
L√log n,
(49)"
N EXP,0.7764350453172205,"holds with a probability of at least 1 −2n−2. Then, by lemma B.9 of [KV21],"
N EXP,0.7774420946626385,∥X(k+1) −X0∥S∞≤
N EXP,0.7784491440080564,eC2L log n m !
N EXP,0.7794561933534743,"ϵ2
k∥W (k)(X0)∥S1.
(50)"
N EXP,0.7804632426988922,"We denote the event that this is fulfilled by E. Furthermore, on this event, if ξ ≤1/2 in (48) and
denoting the condition number by κ = σ1(X0)/σr(X0), it follows from ,lemma B.8 of [KV21], that"
N EXP,0.7814702920443102,∥X(k+1) −X0∥S∞≤
N EXP,0.7824773413897281,eC2L log n m !
N EXP,0.783484390735146,"4σr(X0)−1 
ϵ2
k + 4ϵk∥η(k)∥S∞κ + 2∥η(k)∥2
S∞κ
"
N EXP,0.7844914400805639,"Furthermore, if X(k)
r
∈Rn×n denotes the best rank-r approximation of X(k) in any unitarily invariant
norm, we estimate that"
N EXP,0.7854984894259819,"ϵk ≤σr+1(X(k)) = ∥X(k) −X(k)
r ∥S∞≤∥X(k) −X0∥S∞= ∥η(k)∥S∞,"
N EXP,0.7865055387713998,"Inserting these two bounds into (50), we obtain"
N EXP,0.7875125881168177,∥η(k+1)∥S∞= ∥X(k+1) −X0∥S∞=
N EXP,0.7885196374622356,eC2L log n m !
N EXP,0.7895266868076536,"4σr(X0)−1 (1 + 6κ) ∥η(k)∥2
S∞."
N EXP,0.7905337361530715,"Finally, if, additionally, (48) is satisfied for"
N EXP,0.7915407854984894,"ξ ≤

m
eC2L log n"
N EXP,0.7925478348439073,"
1
4(1 + 6κ),
(51)"
N EXP,0.7935548841893253,"we conclude that
∥η(k+1)∥S∞< ∥η(k)∥S∞"
N EXP,0.7945619335347432,"and also, we observe a quadratic decay in the spectral error such that"
N EXP,0.7955689828801611,"∥η(k+1)∥S∞≤µ∥η(k)∥2
S∞"
N EXP,0.796576032225579,"with a constant µ =

m
e
C2L log n"
N EXP,0.797583081570997,"
1
4(1+6κ)σr(X0). This shows condition b of Theorem 4.3."
N EXP,0.798590130916415,"To show the remaining statement, we can use Lemma D.3 to show that if X(k) is close enough to X0,
we can ensure that the (r +1)-st singular value σr+1(X(k)) of the current iterate is strictly decreasing.
More precisely, assume now the stricter assumption of"
N EXP,0.7995971802618328,"∥η(k)∥S∞≤
√m√r"
N EXP,0.8006042296072508,"12 eCL(log n)√n −r
ξσr(X0)
(52)"
N EXP,0.8016112789526687,"In fact, if ξ fulfills (49) and (51), we can conclude that on the event E,"
N EXP,0.8026183282980867,σr+1(X(k+1)) ≤∥η(k+1)∥S∞≤∥η(k+1)∥S∞ ≤
N EXP,0.8036253776435045,eC2L log n m !
N EXP,0.8046324269889225,4σr(X0)−1 (1 + 6κ) ∥η(k)∥S∞· ∥η(k)∥S∞ <
N EXP,0.8056394763343404,eC2L log n m !
N EXP,0.8066465256797583,"4σr(X0)−1 (1 + 6κ)
√m√r"
N EXP,0.8076535750251762,12 eCL(log n)√n −r
N EXP,0.8086606243705942,"· ξσr(X0)2 eCL r log n m (
√"
N EXP,0.8096676737160121,n −r)σr+1(X(k))
N EXP,0.81067472306143,≤σr+1(X(k))
N EXP,0.8116817724068479,"using Lemma D.3 for one factor ∥η(k)∥S∞and (52) for the other factor ∥η(k)∥S∞in the third
inequality, and (51) in the last inequality. Taking the update rule (7) for the smoothing parameter
into account, this implies that ϵk+1 = σr+1(X(k+1)), which ensures that the first statement of"
N EXP,0.8126888217522659,"Theorem 4.3 is fulfilled likewise for iteration k+1. By induction, this implies that X(k+ℓ)
ℓ→∞
−−−→X0,
which finishes the proof of Theorem 4.3."
N EXP,0.8136958710976838,"E
Numerical Considerations"
N EXP,0.8147029204431017,"E.1
Experiments on more real data"
N EXP,0.8157099697885196,"As a continuation of Section 5, we provide the error analysis for the 1BPM protein data whose
corresponding recovery visualizations are found in Figures 7a and 7b. The figure Figure 7a shows the
Procrustes distance between the recovered matrix from the samples provided and the ground-truth for
both datasets. Similar to the phase transition diagram of the Gaussian data, we observe the probability
of success observed over these 24 instances in fig. 7b."
N EXP,0.8167170191339376,"1
1.5
2
2.5
3
3.5
4
10−8 10−4 100"
N EXP,0.8177240684793555,Oversampling
N EXP,0.8187311178247734,Relative Procrustes Error
N EXP,0.8197381671701913,"ScaledSGD
RieEDG
ALM
MatrixIRLS"
N EXP,0.8207452165156093,"(a) The relative Procrustes error for all the algorithms for different
oversampling rates for1BPM protein data"
N EXP,0.8217522658610272,"1
2
3
4
0 0.2 0.4 0.6 0.8 1"
N EXP,0.8227593152064451,Oversampling
N EXP,0.823766364551863,Success Probability
N EXP,0.824773413897281,"ScaledSGD
RieEDG
ALM
MatrixIRLS"
N EXP,0.8257804632426989,"(b) The probability of success for all the algorithms for different over-
sampling rates for 1BPM protein data."
N EXP,0.8267875125881168,Figure 7
N EXP,0.8277945619335347,"Next, we evaluate the performance of MatrixIRLS (Algorithm 1), on the US cities dataset [UU20]
in comparison to the aforementioned algorithms. In this setup, we are given m = |Ω| Euclidean"
N EXP,0.8288016112789527,"distances
p"
N EXP,0.8298086606243706,"(λi −λj)2 + (ϕi −ϕj)2 between vectors containing longitude and latitude values λi, λj
and ϕi, ϕj of n = 2920 cities in the United States, whose squares can be arranged in an incomplete
distance matrix D ∈Sn that serves as an input for the reconstruction algorithms. Like the previous
setup of Section 5, the set Ω⊂I of point index pairs is sampled uniformly at random, and we
consider different choices of m parameterized by the oversampling factor ρ. Here, for the US cities
the rank of the input matrix is 2. In Figure 8a we observe a similar pattern to that of Figure 7a, for the
US cities data. The success probability Figure 8b also shows a similar pattern to that Figure 4."
N EXP,0.8308157099697885,"1
1.5
2
2.5
3
3.5
4
10−8 10−4 100"
N EXP,0.8318227593152064,Oversampling
N EXP,0.8328298086606244,Relative Procrustes Error
N EXP,0.8338368580060423,"Scaled SGD
Riemannian SGD
ALM
Matrix IRLS"
N EXP,0.8348439073514602,"(a) The relative Procrustes error for all the algorithms for different
oversampling rates for US cities data"
N EXP,0.8358509566968781,"1
2
3
4
0 0.2 0.4 0.6 0.8 1"
N EXP,0.8368580060422961,Oversampling
N EXP,0.837865055387714,Success Probability
N EXP,0.8388721047331319,"Scaled SGD
Riemannian SGD
AlM
Matrix IRLS"
N EXP,0.8398791540785498,"(b) The probability of success for all the algorithms for different over-
sampling rates for US cities"
N EXP,0.8408862034239678,Figure 8
N EXP,0.8418932527693856,We visualize the reconstruction of the US cities data as shown in Figure 9.
N EXP,0.8429003021148036,"−120
−100
−80
−60 30 40 50 60 (a)"
N EXP,0.8439073514602216,"−120
−100
−80
−60 30 40 50 60 (b)"
N EXP,0.8449144008056395,"−120
−100
−80
−60 30 40 50 60 (c)"
N EXP,0.8459214501510574,"Figure 9: Visualizations of the recovery of US cities data by MatrixIRLS for oversampling rates 1.5, 2.5, 3.5 in
9a,9b,9c respectively"
N EXP,0.8469284994964753,"In this paper, we have adapted the authors’ codes of ScaledSGD [ZCZ22], ALM [TL18] and Ma-
trixIRLS [KV21] from their respective githubs. The code of RieEDG [SLCT23][KV21], has been
obtained from the author through personal communication."
N EXP,0.8479355488418933,"E.2
Choice of parameters"
N EXP,0.8489425981873112,"• MatrixIRLS: The algorithm is configured with the following options:
The input parameter for MatrixIRLS includes the number of outer IRLS iterations N 0,
the number of inner iterations N 0
inner, the tolerance, which is the stopping criteria for the
algorithm tol0
inner. For large datasets like the UScities data and Protein data N = 400,
although the algorithm converges within 120 iterations. We run the experiments with
N 0
inner = 2000 and tolinner = 10−10. In a smaller setup of the synthetic data we perform
the experiments with n = 500 points with same parameters. although convergence of
MatrixIRLS is observed much faster. To put more emphasis on the per iteration error, we
study the experiment on per-iterate analysis with more iterations."
N EXP,0.8499496475327291,"• Augmented Lagrangian Method: For the Augmented Lagrangian method(ALM) we have
same number iterations which is parametrized as N 0
firstorder for the Barzilai-Borwein
gradient method in the code. There are 3 stopping criteria for the BB gradient method which
are parametrised by xtol,ftol and gtol on the iterate, functional value of the iterate and
the gradient of the function respectively. As mentioned in [TL18], the relative change in"
N EXP,0.850956696878147,"Energy is the stopping criterion for the algorithm and the tolerance for that measure is set at
10−10 for our experiments. Similar to the above setup, for larger dataset, more iterations are
observed so that we can achieve a fair comparison between all the algorithms.
• Scaled SGD: This method uses the learning rate and the number of iterations N 0
firstorder
as the parameters. For all the experiments the value of N 0
firstorder are kept constant across
the methods. however, based on the dimension of the input which is n, the iterations are
modified to achieve a fair comparison. The learning rate is set at 0.2, which is based on the
respective paper [ZCZ22].
• Riemmanian Gardient: This method has parameters, number of iterations which is same
N 0
firstorder, the thresholding tolerance which has same value as the tolinner."
N EXP,0.851963746223565,"E.3
Distance metric"
N EXP,0.8529707955689829,"The success of proabbility in the experiments of Section 5, is with respect to the Procrustes distance.
The Procrustes analysis uses similarity transformations like scaling, rotation and maps into a common
reference frame [ADSVF23]. If we consider our problem, this distance is considered with the ground
truth as the reference frame and is defined as
min
Q∈Rr×r,t∈Rr ∥Q · (P0 + t1⊤) −Prec∥F subject to Q⊤Q = I"
N EXP,0.8539778449144008,"E.4
Degrees of freedom"
N EXP,0.8549848942598187,"We count the degrees of freedom in the spectral decomposition of a rank-r symmetric matrix:
A = UDU⊤where U is an orthogonal matrix and D is a diagonal matrix consisting of the
eigenvalues of A on the diagonal. In U there are total r unit norm constraints and there are total
r(r−1)"
N EXP,0.8559919436052367,"2
constraints to the orthogonality of the columns vectors of U which follow from ⟨ui, uj⟩= 0
for i ̸= j and i ∈{1, 2, ..., r} and j ∈{1, 2, ..., r}. Therefore, the total number of constraints is
r(r−1)"
N EXP,0.8569989929506546,"2
+ r = r(r+1)"
N EXP,0.8580060422960725,"2
. The total degrees of freedom in D is r. Hence, the total degrees of freedom of
A is:"
N EXP,0.8590130916414904,nr + r −(r + r(r + 1)
N EXP,0.8600201409869084,"2
) = nr −r(r −1) 2
."
N EXP,0.8610271903323263,"F
Computational Complexity"
N EXP,0.8620342396777442,"For a symmetric matrix X ∈Sn, we write its eigendecomposition (with in magnitude decaying
eigenvalues) such that"
N EXP,0.8630412890231621,"X = [U
U⊥]

Λ
0
0
Λ⊥"
N EXP,0.8640483383685801," 
U⊤ U⊤
⊥"
N EXP,0.865055387713998,"
,
(53)"
N EXP,0.8660624370594159,"where U
∈
Rn×r, U⊥
∈
Rn×(n−r) are matrices with orthonormal columns and Λ
=
diag(γσ1, . . . γσr) and Λ⊥:= dg(γσr+1, . . . γσd) diagonal matrices with entries λi = σiγi where
σi is the i-th singular value of X and γi ∈{±1} a sign. Furthermore, we denote by Tr(X) the best
rank-r approximation of X, which can be written such that"
N EXP,0.8670694864048338,"Tr(X) :=
arg min
Z:rank(Z)≤r
∥Z −X∥= UΛU⊤,
(54)"
N EXP,0.8680765357502518,"where ∥· ∥can be any unitarily invariant norm, due to the Eckardt-Young-Mirsky theorem [Mir60],
using the established notation."
N EXP,0.8690835850956697,"The computational complexity of Algorithm 1 is dominated by the solution of the weighted least
squares problem (5) and the computational of spectral information used in the update of the smoothing
parameter ϵk of (7) and the weight operator update (7) (see also Definition 3.1)."
N EXP,0.8700906344410876,"For solving (5), we consider detailed in Appendix F.1, based on solving a linear system with the
dimensionality of tangent space TTrk (X(k))Mrk. The tangent space formulation can be derived from
(6) via the Sherman-Morrison-Woodbury [Woo50] formula using the weight operator structure."
N EXP,0.8710976837865055,"In order to update the smoothing parameter and the weight operator we compute the SVD of the
iterate by approximating top singular vectors and corresponding values following Randomized Block
Krylov Methods [MM15]. This computation takes O(((m + nr)r log n)."
N EXP,0.8721047331319235,"F.1
Tangent Space Implementation"
N EXP,0.8731117824773413,"Let Mr := {X ∈Sn : rank(X) = r} the manifold of symmetric rank-r matrices and let X ∈Sn
be as in (53). In this case, given r ∈N, we can write the tangent space of Mr at Tr(X) as"
N EXP,0.8741188318227593,"T := TTr(X)Mr :=

[U U⊥]

Rr×r
Rr×(n−r)"
N EXP,0.8751258811681772,"R(n−r)×r
0"
N EXP,0.8761329305135952,"
[U U⊥]∗
"
N EXP,0.877139979859013,"=

[UU⊥]

M1 M⊤
2
M2
0"
N EXP,0.878147029204431,"
[UU⊥]∗: M1 ∈Sr, M2 ∈Rr×(n−r), arbitrary
"
N EXP,0.879154078549849,"= {UΓ1U⊤+ UΓ⊤
2
 
I −UU⊤
+ (I −UU⊤)Γ2U⊤: Γ1 ∈Sr, Γ2 ∈Rn×r}"
N EXP,0.8801611278952669,"= {UΓ1U⊤+ UΓ⊤
2 + Γ2U⊤: Γ1 ∈Sr, Γ2 ∈Rn×r, U⊤Γ2 = 0}, (55)"
N EXP,0.8811681772406847,"see also [Van13], [Bou20, Chapter 7.5]."
N EXP,0.8821752265861027,"If PT : Sn →Sn is the orthogonal projection operator that projects symmetric matrices onto T as
used in Theorem 4.5, we note that
PT (X) = UU⊤XUU⊤+ UU⊤X(I −UU⊤) + (I −UU⊤)XUU⊤,
which can be decomposed such that
PT (X) = PT P ∗
T (X),"
N EXP,0.8831822759315207,"where the action of PT : Rr(n+r) →Sn can be described as
PT (γ) := UΓ1U⊤+ UΓ⊤
2 + Γ2U⊤"
N EXP,0.8841893252769386,"with Γ1 ∈Sr being the result of an (r2 × 1) to (r × r) reshaping of the first r2 coordinates of γ and
Γ2 ∈Rn×r the result of an (rn × 1) to (n × r) reshaping of the remaining coordinates of γ. Further,
we used the notation of the adjoint operator P ∗
T : Sn →Rr(n+r) of PT which maps X ∈Sn to the
vectorization γ of [Γ1, Γ2] :=

U⊤XU, (I −UU⊤)XU
	
."
N EXP,0.8851963746223565,"Due to the fact that the weight operator Wk = WX(k),ϵk of Definition 3.1 associated to the iterate
X(k) ∈Sn and the smoothing parameter ϵk > 0 is self-adjoint and invertible, we can write its inverse
as
W −1
k
= PTkD−1
k P ∗
Tk + ϵ2
k(I −PTkP ∗
Tk),
(56)"
N EXP,0.8862034239677744,"where
D−1
k
∈
Rrk(n+rk)×rk(n+rk)
is
a
diagonal
matrix
with
entries
max(σi(X(k)), ϵk) max(σj(X(k)), ϵk), where either i or j are smaller or equal than rk, see
also [KV21, Eq. (12)], and Tk = TTrk (X(k))Mrk is the tangent space at Trk(X(k)) onto the rank-rk
manifold."
N EXP,0.8872104733131924,Recall from (7) that the the defining equation for the next iterate X(k+1) given Wk is
N EXP,0.8882175226586103,"X(k+1) = W −1
k A∗ 
AW −1
k A∗−1 y.
(57)"
N EXP,0.8892245720040282,"Using (56), we see that
 
AW −1
k A∗
= A
 
PTkD−1
Sk P ∗
Tk + ϵ2
k
 
I −PTkP ∗
Tk

A∗"
N EXP,0.8902316213494461,"= A
 
PTk
 
D−1
Sk −ϵ2
kISk

P ∗
Tk

A∗+ ϵ2
kAA∗
(58)"
N EXP,0.8912386706948641,"and, using the Sherman-Morrison-Woodbury [Woo50] formula, we have that

A(W (k))−1A∗−1
= ϵ−2
k (AA∗)−1"
N EXP,0.892245720040282,"−ϵ−2
k
(AA∗)−1 APTk

ϵ2
kC−1 + P ∗
TkA∗(AA∗)−1 APTk
−1
P ∗
TkA∗(AA∗)−1"
N EXP,0.8932527693856999,"= ϵ−2
k (AA∗)−1 −ϵ−2
k
(AA∗)−1 APTkM−1P ∗
TkA∗(AA∗)−1 (59)"
N EXP,0.8942598187311178,"with linear system matrix M
:=

ϵ2
kC−1 + P ∗
TkA∗(AA∗)−1 APTk

, noting that C
:=
 
D−1
k
−ϵ2
kI

is invertible since (D−1
k )ii > ϵ2
k for all diagonal indices. The observation of (59)
can be turned an efficient implementation of the weighted least squares computing X(k+1), which is
presented in Algorithm 2. It can be shown that Algorithm 2 indeed computes X(k+1) implicitly, cf.
Lemma F.1. We omit its proof, which follows the proof of [KV21, Lemma A.1]."
N EXP,0.8952668680765358,Algorithm 2 Tangent space implementation of weighted least squares step in Algorithm 1
N EXP,0.8962739174219537,"Input: Set Ω, observation distances DΩ= (dij)(i,j)∈Ω, singular vectors U ∈Rd1×rk,"
N EXP,0.8972809667673716,"V(k) ∈Rn×rk and singular values σ(k)
1 , . . . , σ(k)
rk , smoothing parameter ϵk, projection γ(0)
k
=
P ∗
TkPTk−1(γk−1) ∈Rrk(n+rk) of solution γk−1 ∈Rrk−1(n+rk−1) of linear system (60) for
previous iteration k −1."
N EXP,0.8982880161127895,"1: Compute h0
k := P ∗
TkA∗(AA∗)−1 y −

ϵ2
k
 
D−1
k
−ϵ2
kI
−1 + P ∗
TkA∗(AA∗)−1 APTk

γ(0)
k
∈"
N EXP,0.8992950654582075,"Rrk(n+rk).
2: Solve
M∆γk = h0
k
(60)
for ∆γk ∈Sk by the conjugate gradient method [HS52, Meu06], where"
N EXP,0.9003021148036254,"M = ϵ2
k
 
D−1
k
−ϵ2
kI
−1 + P ∗
TkA∗(AA∗)−1 APTk."
N EXP,0.9013091641490433,"3: Compute γk = γ(0)
k
+ ∆γk."
N EXP,0.9023162134944612,4: Compute residual rk+1 := (AA∗)−1 (y −APTk(γk)) ∈Rm.
N EXP,0.9033232628398792,Output: rk+1 ∈Rm and γk ∈Rrk(n+rk).
N EXP,0.904330312185297,"Lemma F.1. If rk+1 ∈Rm and γk ∈Rrk(n+rk) is the output of Algorithm 2, then X(k+1) as in step
(6) of Algorithm 1 fulfills"
N EXP,0.905337361530715,X(k+1) = A∗(rk+1) + PTk(γk)
N EXP,0.9063444108761329,"= A∗(rk+1) + UΓ1U⊤+ UΓ⊤
2 + Γ2U⊤,"
N EXP,0.9073514602215509,"where Γ1 ∈Rrk×rk is the matricization of the first r2
k elements of γk and Γ2 ∈Rn×rk the matriciza-
tion of the reamining elements."
N EXP,0.9083585095669687,Algorithm 3 Implementation of U⊤A∗: Rm+n →Rr×n
N EXP,0.9093655589123867,"Input: Input vector y = [y1, y2, ....., ym+n], Ω= (iℓ, jℓ)m
ℓ=1 ⊂I be a multiset of double indices.
S1 = Pm
ℓ=1:(iℓ,j)∈Ωfor some j yℓeiℓeT
iℓ
S2 = Pm
ℓ=1:(i,jℓ)∈Ωfor some i yℓejℓeT
jℓ
S3 = Pm
ℓ=1:(iℓ,jℓ)∈ΩyℓeiℓeT
jℓ
S = S1 + S2 −S3 −ST
3 .
saved as a sparse matrix.
V1 = U⊤S
▷4rm flops.
V2 = 1"
N EXP,0.9103726082578046,"2U⊤· [ym+1, ..., ym+n]⊤· 1⊤
▷rn flops.
V3 = 1"
N EXP,0.9113796576032226,"2U⊤· 1 · [ym+1, ..., ym+n]
▷2rn flops
V = V1 + V2 + V3
Output: V
Total of rm + 3rn flops."
N EXP,0.9123867069486404,Algorithm 4 Implementation of AA∗: Rm+n →Rm+n
N EXP,0.9133937562940584,"Input: Ω= (iℓ, jℓ)m
ℓ=1 ⊂I be a multiset of double indices fulfilling m < L = n(n −1)/2 that
are sampled independently with replacement, the vector y = [y1, y2, ....., ym+n]."
N EXP,0.9144008056394763,"Load S from Algorithm 3 which computes A∗(y).
T1 = (e⊤
iℓSeiℓ)m
ℓ=1:(iℓ,j)∈Ωfor some j
T2 = (e⊤
jℓSejℓ)m
ℓ=1:(i,jℓ)∈Ωfor some i.
T3 = (e⊤
iℓSejℓ)m
ℓ=1:(iℓ,jℓ)∈Ω.
T5 = T1 + T2 −T3 −T⊤
3 .
▷4m + 2n flops
avg = 1"
N EXP,0.9154078549848943,"nΣm+n
i=m+1yi.
▷n flops
T6 = [ym+1 + avg, ....., ym+n + avg]
▷n flops
Output: [T5; T6]
Total of 4m + 4n flops"
N EXP,0.9164149043303121,"Algorithm 5 Implementation of P ∗
T A∗: Rm+n →Rr(n+r)"
N EXP,0.9174219536757301,"Input: Input vector y = [y1, y2, ....., ym+n] ∈Rm+n, index set Ω, left singular vector matrix
U ∈Rn×r."
N EXP,0.918429003021148,"A1 = V ∈Rr×n from Algorithm 3.
▷O(r(m + n)) flops
Γ1 = A1U ∈Rr×r.
▷O(r2n) flops
M = Γ1U⊤∈Rr×n.
▷O(r2n) flops
Γ2 = (A⊤
1 −M⊤)
Output: {Γ1, Γ2}."
N EXP,0.919436052366566,"Lemma F.1 shows that an iterate X(k+1) can be represented via only m + rk(n + rk) parameters.
In the remainder of this discussion, we will assume that rk = r, which is the case in most cases in
practice if the rank estimate er of Algorithm 1 is chosen as er = r."
N EXP,0.9204431017119838,"To quantify the computational cost of Algorithm 2, we assume a fixed number N0
inner of CG iterations
solving (60). When applying the system matrix M via matrix-vector multplication, we observe that its
first summand ϵ2
k
 
D−1
k
−ϵ2
kI
−1 is diagonal and thus results in O(r(n + r)) flops per CG iteration.
To quantify the matrix-vector multiplication cost of its second summand P ∗
TkA∗(AA∗)−1 APTk, we
define below algorithms that efficiently implement the application of the operators P ∗
T A∗: Rm+n →
Rr(n+r) (Algorithm 5), AA∗: Rm+n →Rm+n (Algorithm 4) and APT : Rr(n+r) →Rm+n"
N EXP,0.9214501510574018,"(Algorithm 6). As an auxiliary function, we also provide an implementation of U⊤A∗: Rm+n →
Rr×n (Algorithm 3) below."
N EXP,0.9224572004028198,"The application of (AA∗)−1 : Rm+n →Rm+n can be achieved by an inexact iterative solver
that applies Algorithm 4 a fixed number ˜N of iterations, which costs O((m + n) ˜N). Since the
time complexity of Algorithm 5 and Algorithm 6 are O(r2n + rm), we obtain one matrix vector
multiplication with M from Algorithm 2in a time complexity of O(r2n + rm + (m + n) ˜N)."
N EXP,0.9234642497482377,"Since N0
inner CG iterations are used, we obtain a total time complexity of O(N0
inner(r2n+rm+(m+
n) ˜N)) for Algorithm 2. In our experiments, we observe that a small number ˜N = 10 of iterations for
solving the system associated to (AA∗)−1 is sufficient to obtain high-accuracy solutions."
N EXP,0.9244712990936556,This breakdown of the computational costs of each algorithm is shown in Algorithm 3 to Algorithm 6.
N EXP,0.9254783484390735,Algorithm 6 Implementation of APT : Rr(n+r) →Rm+n
N EXP,0.9264853977844915,"Input: {Γ1, Γ2} , index set Ω, left singular vectors U ∈Rn×r."
N EXP,0.9274924471299094,"M = Γ1U⊤∈Rr×n.
▷O(r2n) flops from Algorithm 5
z = (Pr
k=1 U(jℓ,k)M(k,jℓ))(i,jℓ)∈Ωfor some i
▷O(mr) flops
z = z + (Pr
k=1 U(iℓ,k)M(k,iℓ))(iℓ,j)∈Ωfor some j
▷O(mr) flops
z = z −(Pr
k=1 U(jℓ,k)M(k,iℓ))(iℓ,jℓ)∈Ω
▷O(mr) flops
z = z −(Pr
k=1 U(iℓ,k)(M(k,jℓ))(iℓ,jℓ)∈Ω
▷O(mr) flops
α = (Pr
k=1 U(iℓ,k)(Γ⊤
2 )(k,iℓ))(iℓ,j)∈Ωfor some j
▷O(mr) flops
α = α + (Pr
k=1 U(jℓ,k)(Γ⊤
2 )(k,jℓ))(i,jℓ)∈Ωfor some i
▷O(mr) flops
α = α −(Pr
k=1 U(jℓ,k)(Γ⊤
2 )(k,iℓ))(iℓ,jℓ)∈Ω
▷O(mr) flops
α = α −(Pr
k=1(U)(iℓ,k)(Γ⊤
2 )k,jℓ)(iℓ,jℓ)∈Ω
▷O(mr) flops
ζ1 = z + 2α .
c1 = (Pn
i=1 U(i,k))r
k=1
▷O(nr) flops
γ1 = c1M
▷O(mr) flops
γ2 = c1Γ⊤
2
▷O(mr) flops
c2 = (Pn
i=1(Γ2)(iℓ,k))r
k=1
▷O(nr) flops
γ3 = c2U⊤
▷O(mr) flops
ζ2 = γ1 + γ2 + γ3
Output: [ζ1; ζ2]"
N EXP,0.9284994964753273,NeurIPS Paper Checklist
CLAIMS,0.9295065458207452,1. Claims
CLAIMS,0.9305135951661632,"Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Main claims of the paper are weaved in Abstract and Introduction. There is a
separate subsection in the Introduction dedicated to discuss the contributions of the paper.
Guidelines:"
CLAIMS,0.9315206445115811,"• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations"
CLAIMS,0.932527693856999,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In the Appendix B of this paper, we mention the limitation of this paper.
Guidelines:"
CLAIMS,0.9335347432024169,"• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs"
CLAIMS,0.9345417925478349,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer:[Yes]"
CLAIMS,0.9355488418932527,"Justification: Every lemma/ theorems stated in the paper (Section 4 and appendices C and D)
include the assumptions in the respective statements."
CLAIMS,0.9365558912386707,Guidelines:
CLAIMS,0.9375629405840886,"• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced."
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9385699899295066,4. Experimental Result Reproducibility
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9395770392749244,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9405840886203424,Answer: [Yes]
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9415911379657603,"Justification: The related link to the anonymized Github respository is provided in the
Appendix E"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9425981873111783,Guidelines:
EXPERIMENTAL RESULT REPRODUCIBILITY,0.9436052366565961,"• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results."
OPEN ACCESS TO DATA AND CODE,0.9446122860020141,5. Open access to data and code
OPEN ACCESS TO DATA AND CODE,0.945619335347432,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?"
OPEN ACCESS TO DATA AND CODE,0.94662638469285,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9476334340382678,Justification: All references of code and reference code are provided in the paper. [TODO]
OPEN ACCESS TO DATA AND CODE,0.9486404833836858,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9496475327291037,"• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted."
OPEN ACCESS TO DATA AND CODE,0.9506545820745217,6. Experimental Setting/Details
OPEN ACCESS TO DATA AND CODE,0.9516616314199395,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?"
OPEN ACCESS TO DATA AND CODE,0.9526686807653575,Answer: [Yes]
OPEN ACCESS TO DATA AND CODE,0.9536757301107754,"Justification: Experimental details are provided in Section 5 and discussion on the choice of
parameters is provided in the Appendix."
OPEN ACCESS TO DATA AND CODE,0.9546827794561934,Guidelines:
OPEN ACCESS TO DATA AND CODE,0.9556898288016112,"• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9566968781470292,7. Experiment Statistical Significance
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9577039274924471,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9587109768378651,Answer:[Yes]
EXPERIMENT STATISTICAL SIGNIFICANCE,0.959718026183283,"Justification: In the Section 5, the experiments are performed on multiple independant
realizations and respective boxplots of error are shown in Appendix E. All the figures
provide statistical significance of the experiments."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9607250755287009,Guidelines:
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9617321248741189,"• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper."
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9627391742195368,"• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text."
EXPERIMENTS COMPUTE RESOURCES,0.9637462235649547,8. Experiments Compute Resources
EXPERIMENTS COMPUTE RESOURCES,0.9647532729103726,"Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
EXPERIMENTS COMPUTE RESOURCES,0.9657603222557906,Answer: [Yes]
EXPERIMENTS COMPUTE RESOURCES,0.9667673716012085,"Justification: The experiements were performed on a single node of a compute cluster
equipped with dual 24-core Intel Xeon Gold 6248R CPUs, utilizing 32 parallel tasks."
EXPERIMENTS COMPUTE RESOURCES,0.9677744209466264,Guidelines:
EXPERIMENTS COMPUTE RESOURCES,0.9687814702920443,"• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper)."
CODE OF ETHICS,0.9697885196374623,9. Code Of Ethics
CODE OF ETHICS,0.9707955689828801,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?"
CODE OF ETHICS,0.9718026183282981,Answer: [Yes]
CODE OF ETHICS,0.972809667673716,Justification: [TODO]
CODE OF ETHICS,0.973816717019134,Guidelines:
CODE OF ETHICS,0.9748237663645518,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction)."
BROADER IMPACTS,0.9758308157099698,10. Broader Impacts
BROADER IMPACTS,0.9768378650553877,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?"
BROADER IMPACTS,0.9778449144008057,Answer: [Yes]
BROADER IMPACTS,0.9788519637462235,"Justification: In the introduction and in related work we highlight the impact of the work.
These provide foundational algorithmic research to reconstruction algorithms which has
applications in molecular conformation, sensor network localization. There is no potential
negative docial impact of this work."
BROADER IMPACTS,0.9798590130916415,Guidelines:
BROADER IMPACTS,0.9808660624370594,"• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML)."
SAFEGUARDS,0.9818731117824774,11. Safeguards
SAFEGUARDS,0.9828801611278952,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?"
SAFEGUARDS,0.9838872104733132,Answer: [NA]
SAFEGUARDS,0.9848942598187311,"Justification: The paper uses data that are publicly available. The data sources are cited
properly in Section 5."
SAFEGUARDS,0.9859013091641491,Guidelines:
SAFEGUARDS,0.9869083585095669,"• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort."
LICENSES FOR EXISTING ASSETS,0.9879154078549849,12. Licenses for existing assets
LICENSES FOR EXISTING ASSETS,0.9889224572004028,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?"
LICENSES FOR EXISTING ASSETS,0.9899295065458208,Answer:[Yes]
LICENSES FOR EXISTING ASSETS,0.9909365558912386,"Justification: Throughout the paper we cite the different information that we used from
the existing literature. Additionally, in the Appendix E, we provide links to the Github
repositories that we used for reference."
LICENSES FOR EXISTING ASSETS,0.9919436052366566,Guidelines:
LICENSES FOR EXISTING ASSETS,0.9929506545820745,"• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset."
LICENSES FOR EXISTING ASSETS,0.9939577039274925,"• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets"
LICENSES FOR EXISTING ASSETS,0.9949647532729103,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The link to the anonymized version of the repository containing our experi-
ments have been provided in Appendix E.
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9959718026183283,"• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects"
LICENSES FOR EXISTING ASSETS,0.9969788519637462,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: NA
Justification: The paper does not involve crowdsourcing nor research with human subjects.r
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.9979859013091642,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [TODO]
Guidelines:"
LICENSES FOR EXISTING ASSETS,0.998992950654582,"• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review."
