Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0032258064516129032,"In recent years, a proliferation of methods were developed for multi-agent rein-
forcement learning (MARL). In this paper, we focus on evaluating the robustness
of MARL agents in continuous control tasks. In particular, we propose the ﬁrst
model-based approach to perform adversarial attacks for continuous MARL. The
attack aims at adversarially perturbing the states of agent(s) to misguide them to
take damaging actions that lower team rewards. A deep neural network is trained
to represent the dynamics of the environment. We then solve an optimization prob-
lem with the learned dynamics model to yield small perturbations. In addition, we
discuss several strategies to optimally select adversary agents for the attack. Nu-
merical experiments on multi-agent Mujoco tasks verify the effectiveness of our
proposed approach."
INTRODUCTION,0.0064516129032258064,"1
INTRODUCTION"
INTRODUCTION,0.00967741935483871,"Deep neural networks are known to be vulnerable to adversarial examples, where a small and often
imperceptible adversarial perturbation can easily fool the state-of-the-art deep neural network clas-
siﬁers (Szegedy et al., 2013; Nguyen et al., 2015; Goodfellow et al., 2014; Papernot et al., 2016).
Since then, a wide variety of deep learning tasks have been shown to also be vulnerable to adversar-
ial attacks, ranging from various computer vision tasks to natural language processing tasks (Jia &
Liang, 2017; Zhang et al., 2020b; Jin et al., 2020; Alzantot et al., 2018)."
INTRODUCTION,0.012903225806451613,"Perhaps unsurprisingly, deep reinforcement learning (DRL) agents are also vulnerable to adversarial
attacks, as ﬁrst shown in Huang et al. (2017) for atari games DRL agents. In Huang et al. (2017), the
authors study the effectiveness of adversarial examples on a policy network trained on Atari games
under the situation where the attacker has access to the neural network of the target policy. In Lin
et al. (2017), the authors further investigate a strategically-time attack by attacking trained agents on
Atari games at a subset of the time step. Meanwhile, Kos & Song (2017) use the fast gradient sign
method (FGSM) to generate adversarial perturbation on the A3C algorithm (Mnih et al., 2016) and
explore training with random noise and FGSM perturbation to improve resilience against adversarial
examples. While the above research endeavors focused on actions that take discrete values, another
line of research tackles a more challenging problem on DRL with continuous action spaces (Weng
et al., 2019; Gleave et al., 2019). Speciﬁcally, Weng et al. (2019) consider a two-step algorithm
which determines adversarial perturbation to be closer to a target state using a learnt dynamics
model, and Gleave et al. (2019) propose a physically realistic thread model and demonstrates the
existence of adversarial policies in zero-sum simulated robotics games."
INTRODUCTION,0.016129032258064516,"While most of the existing DRL attack algorithms focus on the single DRL agent setting1, in this
work we propose to study the vulnerability of multi-agent DRL, which has been widely applied in
many safety-critical real-world applications including swarm robotics (Dudek et al., 1993), electric-
ity distribution, and trafﬁc control (OroojlooyJadid & Hajinezhad, 2019). In particular, we focus on
the collaborative multi-agent reinforcement learning (c-MARL) setting, where a group of agents is
trained to generate joint actions to maximize certain (team) reward function. We note that c-MARL
is a more challenging yet interesting setting than the single DRL agent setting, as the interactions
between agents commands consideration of additional layers of complications."
INTRODUCTION,0.01935483870967742,"Contribution.
Our contribution can be summarized as follows:"
INTRODUCTION,0.02258064516129032,"1We are aware of only one recent work (Lin et al., 2020) on attacking c-MARL agents with discrete action
space, rather than continuous action space considered in our work."
INTRODUCTION,0.025806451612903226,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02903225806451613,"• We propose a new adversarial attack framework on MARL with continuous action space,
where we name it cMBA (model-based attack on c-MARL). The attack comprises two
steps: learning a representation of the environment dynamics and then determining proper
observation perturbations based on the learned dynamics model."
INTRODUCTION,0.03225806451612903,"• We formulate the process of selecting an victim agent as a mixed-integer programming
problem and propose an approximate formulation that can be efﬁciently solved by a ﬁrst-
order method."
INTRODUCTION,0.035483870967741936,"• We evaluate our attack framework with two baselines and four multi-agent MuJoCo tasks
(Peng et al., 2020), with number of agents ranging from 2 to 6, to verify the effectiveness
of our approach. Our model-based attack consistently outperforms the two baselines in all
tested environments."
INTRODUCTION,0.03870967741935484,"Paper outline. Section 2 discusses related works in adversarial attacks for MARL along with the
problem settings. We describe the main attack framework cMBA in Section 3, including the training
of the dynamics model and solving the sub-optimization problem. In Section 3.3, we introduce a
mixed-integer program to ﬁnd the optimal set of victim agents and propose an approximate formu-
lation that can be efﬁciently solved using a ﬁrst-order optimization method. Section 4 presents the
evaluation of our approach on several multi-agent MuJoCo environments. Therein, we also study
the performance of different variants of our model-based attack as discussed in Section 3.3."
RELATED WORK,0.041935483870967745,"2
RELATED WORK"
RELATED WORK,0.04516129032258064,"Adversarial attacks for DRL agents.
In the c-MARL setting, although there exists a signiﬁcant
literature focusing on the adversarial training of MARL agents (Phan et al., 2020; 2021; Zhang et al.,
2020a), yet a very few of them addresses the scenario of adversarial attacks in c-MARL. Lin et al.
(2020) propose a two-step attack on a single agent in a c-MARL environment, where they extend
existing methods to generate adversarial examples guided by a learned adversarial policy. The attack
in Lin et al. (2020) is evaluated under the StarCraft Multi-Agent Challenge (SMAC) environment
(Samvelyan et al., 2019) where the action spaces are discrete. To the best of our knowledge, there
has not been work considering adversarial attacks on the c-MARL setting with continuous action
spaces. The closest approach to ours is the enchanting attack in Lin et al. (2017) and the model-
based attack in Weng et al. (2019); however, these two works focused on single DRL agent setting.
The enchanting attack in Lin et al. (2017) uses a video prediction model to predict the future state
and ﬁnds perturbation that minimizes a distance function between the predicted future state and a
target state. Similarly, Weng et al. (2019) consider two formulations for observation and action
perturbation that incorporates a learned dynamic model for future state prediction. In addition to the
model-based approach, in this paper, we consider the selection of victim agents, which is unique to
the multi-agent DRL setting."
RELATED WORK,0.04838709677419355,"Training c-MARL agents.
One simple approach to train c-MARL agents is to let each agent learn
their own policy independently as in Independent Q-Learning (IQL) (Tan, 1993; Tampuu et al.,
2017). However, this training strategy does not capture the interaction between agents. Alterna-
tively, one can follow a common paradigm called centralized training with decentralized execution
(CTDE) (Oliehoek et al., 2008; Oliehoek & Amato, 2016; Kraemer & Banerjee, 2016). Gupta et al.
(2017) extend DQN (Mnih et al., 2015), TRPO (Schulman et al., 2015), and DDPG (Lillicrap et al.,
2015) to c-MARL, where a parameter sharing neural network policy is trained in a centralized fash-
ion but still allows decentralized behavior across agents. Similar to this, COMA (Foerster et al.,
2018) is an actor-critic method that uses a centralized critic to estimate counterfactual advantage
for decentralized policies. While COMA uses a single critic for all agents, MADDPG (Lowe et al.,
2017) learns a centralized critic for each agent and evaluates on continuous control. Instead of hav-
ing fully centralized critic, DOP (Wang et al., 2020b) and FACMAC (Peng et al., 2020) apply the
value decomposition idea (Sunehag et al., 2017; Rashid et al., 2018; 2020; Son et al., 2019; Wang
et al., 2020a) into multi-agent actor-critic by having a centralized but factorized critic."
RELATED WORK,0.05161290322580645,Under review as a conference paper at ICLR 2022
RELATED WORK,0.054838709677419356,"3
CMBA: MODEL-BASED ATTACK FOR C-MARL"
PROBLEM SETTING,0.05806451612903226,"3.1
PROBLEM SETTING"
PROBLEM SETTING,0.06129032258064516,"We consider multi-agent tasks with continuous action spaces modeled as a Decentralized Partially
Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016). A Dec-POMDP
has a ﬁnite set of agents N ≡{1, · · · , n} associated with a set of states S describing global states,
a set of continuous actions Ai, and a set of individual partial observation Oi for each agent i ∈N.
Given the current observation oi
t ∈Oi, the action ai
t ∈Ai is selected by a parametrized policy
πθi : Oi →Ai. The next state agent i moves to is determined by the state transition function
Pi : S × Ai →S. After that, agent i receives a reward ri
t calculated from a reward function
Ri : S × Ai →R and observes a local observation oi
t+1 ∈Oi correlated with the global state
st+1 ∈S. In addition, Dec-POMDP is associated with an initial state distribution P0 and a discount
factor γ. In this setting, each agent tries to ﬁnd the policy that maximizes its own total expected
return Ri = PT
t=0 γtri
t where T is the sampling horizon."
PROBLEM FORMULATION,0.06451612903225806,"3.2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.06774193548387097,"Our goal is to generate adversarial perturbations imposed to the victim agents’ input (state) in order
to deteriorate the total team reward. The added noise encourages the victim agents’ observation to
be close to a desired adversarial state corresponding to low reward. To avoid sampling from the
environment, we use a pre-trained model that learns the dynamics of the environment to predict
the next state from the perturbed observation and current action, then ﬁnd the suitable noise that
minimizes the distance between the predicted next state and a predeﬁned target state. This process
can be formulated as an optimization problem as follows."
PROBLEM FORMULATION,0.07096774193548387,"Formally, we consider a multi-agent setting with |N| = n agents, each agent i ∈N observes state
si
t locally and takes action ai
t. Let st = (s1
t, · · · , sn
t ) ∈S be the global state at time step t which
is concatenated from local states si
t for each agent i = 1, · · · , n. We also denote the joint action
at = (a1
t, · · · , an
t ) concatenated from each agent’s action ai
t. Let Lt ∈N be the set of victim agents
at time step t, i.e. the set of agents that can be attacked. Let fd : S × A →S be a parametrized
function that approximates the dynamics of the environment, where A is the set of concatenated
actions, one from each Ai. We assume that a pre-trained MARL policy π(·) is given. Let starget
be the target state which can lead to poor performance to the agent. We denote ϵ as an upper bound
on budget constraint w.r.t some p-norm ∥·∥p. The state perturbation ∆s = (∆s1, · · · , ∆sn) (we
suppress the dependence on t of ∆s to avoid overloading the notation) to st is the solution to the
following problem
min
∆s=(∆s1,··· ,∆sn)
d(ˆst+1, starget)"
PROBLEM FORMULATION,0.07419354838709677,"s.t.
ˆst+1 = fd(st, at)
ai
t = π(si
t + ∆si)
∆si = 0, ∀i /∈Lt
ℓS ≤st + ∆s ≤uS
∥∆s∥p ≤ϵ (1)"
PROBLEM FORMULATION,0.07741935483870968,"where 0 is a zero vector, and the observation space is constrained within the (vectorized) intervals
between ℓS and uS."
PROBLEM FORMULATION,0.08064516129032258,"Let us ﬁrst provide some insights for the formulation (1). For each agent, using the trained policy π,
we can compute the corresponding action ai
t given its (possibly perturbed) local state si
t or si
t +∆si.
From the concatenated state-action pair (st, at), we can predict the next state ˆst+1 via the learned
dynamics model fd. Then by minimizing the distance between the predicted next state and the target
state under the budget constraint, we are forcing the agent to move closer to a damaging state in the
next transition. We also note that the last equality constraint indicates that we only allow perturbing
agents within the victim set Lt."
PROBLEM FORMULATION,0.08387096774193549,"Problem (1) can be efﬁciently solved by proximal-gradient-based methods. Firstly, by substituting
the deﬁnition of at into ˆst+1, (1) is equivalent to"
PROBLEM FORMULATION,0.08709677419354839,"min
x
d(fd(st, π(st + x)), starget)
s.t.
x ∈Cp
t
(2)"
PROBLEM FORMULATION,0.09032258064516129,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.0935483870967742,"where Cp
t := {x = (x1, · · · , xn) : ∥x∥p ≤ϵ, ℓS −st ≤x ≤uS −st and xi = 0 for i /∈Lt}. If
we choose the distance function as d(a, b) = ∥a −b∥2, we can use the projected gradient descent
(PGD) algorithm (Nesterov, 2003) to solve (2). The PGD iteration to update yk at iteration k can be
described as
yk+1 = projCp
t [yk −η∇yd(fd(st, π(st + y)), starget)]
(3)"
PROBLEM FORMULATION,0.0967741935483871,"where projCp
t (·) is the projection to the convex set Cp
t and η is the learning rate. The projection is
simple to calculate since Cp
t is the intersection of a unit ball in the p-norm and a box."
PROBLEM FORMULATION,0.1,The whole attack process can be summarized in Algorithm 1.
PROBLEM FORMULATION,0.1032258064516129,Algorithm 1 cMBA algorithm at timestep t
PROBLEM FORMULATION,0.1064516129032258,1: Initialization:
PROBLEM FORMULATION,0.10967741935483871,"Given st, starget, π, fd, Lt; initialize ∆s0; choose learning rate η > 0
2: For k := 0, · · · , K −1 do
3:
Compute at = (a1
t, · · · , an
t ) as"
PROBLEM FORMULATION,0.11290322580645161,"ai
t =
π(si
t + ∆si)
if i ∈Lt
π(si
t)
otherwise."
PROBLEM FORMULATION,0.11612903225806452,"4:
Compute ˆst+1 = fd(st, at)
5:
Update ∆s as
∆sk+1 = projCt [∆sk −η∇∆sd(ˆst+1, starget)]"
PROBLEM FORMULATION,0.11935483870967742,6: End For
PROBLEM FORMULATION,0.12258064516129032,"Learning dynamics model. The most important factor that affects the solution of (2) is the quality
of the learned dynamics model fd. If the dynamics is known, we can solve (2) exactly so that
the solution is indeed the optimal perturbation. However, it is hardly the case in practice because
we often approximate the dynamics using function approximators such as neural networks. The
parameter w for fd is the solution of the following optimization problem min
w X i∈D"
PROBLEM FORMULATION,0.12580645161290321,"fd(si
cur, w) −si
next
2
(4)"
PROBLEM FORMULATION,0.12903225806451613,"where D is a collection of state transition {(si
cur, si
next)}|D|
i=1 where snext is the actual state that
the environment transitions to after taking action at determined by a given policy. In particular, we
separately collect transitions using the pre-trained policy π and a random policy to obtain Dtrain
and Drandom. Then the dataset D is built as D = Dtrain ∪Drandom. Problem (4) is a standard
supervised learning problem and can be solved using gradient-based learning. We describe the full
process of training the dynamics model in Algorithm 2 where the GradientBasedUpdate step
is any gradient-descent-type update."
PROBLEM FORMULATION,0.13225806451612904,Algorithm 2 Training dynamics model
PROBLEM FORMULATION,0.13548387096774195,1: Initialization:
PROBLEM FORMULATION,0.13870967741935483,"Given pre-trained policy πtr and a random policy πrd; initialize w0
2: Form D := Dtrain ∪Drandom by collecting a set of transitions Dtrain and Drandom using
policy πtr and πrd, respectively.
3: For k := 0, 1, · · · do"
PROBLEM FORMULATION,0.14193548387096774,"wk+1 = GradientBasedUpdate(D, wk)"
PROBLEM FORMULATION,0.14516129032258066,4: End For
VICTIM AGENT SELECTION,0.14838709677419354,"3.3
VICTIM AGENT SELECTION"
VICTIM AGENT SELECTION,0.15161290322580645,"In this subsection, we discuss different strategies to select victim agents when performing adversarial
attacks in the MARL setting. Suppose that we would like to perform an attack on any agent but the
budget on the victim agents is na ≤n. In other words, an attacker can only choose a subset of"
VICTIM AGENT SELECTION,0.15483870967741936,Under review as a conference paper at ICLR 2022
VICTIM AGENT SELECTION,0.15806451612903225,"agents to attack during each time step, and the goal of an attacker is to choose the most vulnerable
agents so that the total reward is utmost affected. We note that this scenario is unique in the setting
of multi-agent DRL setting, as in the single DRL agent setting we can only attack the same agent all
the time. To start with, we ﬁrst formulate a mixed-integer program to perform the attack on a set of
optimally victim agents as below:
min∆s,w
d(ˆst+1, starget)
s.t.
ˆst+1 = fd(st, at)
at = π(st + ⟨∆s, w⟩)
ℓS ≤st + ∆s ≤uS
∥∆s∥p ≤ϵ
wi ∈{0, 1}
P"
VICTIM AGENT SELECTION,0.16129032258064516,i wi = na (5)
VICTIM AGENT SELECTION,0.16451612903225807,"where we introduce a new binary variable w to select the appropriate agents’ input to perturb and
na is the total number of victim agents we can attack."
VICTIM AGENT SELECTION,0.16774193548387098,"Due to the existence of the new binary variable, problem (5) is much harder to solve than before.
We instead solve a proxy of (5) as follows
min∆s,θ
d(ˆst+1, starget)
s.t.
ˆst+1 = fd(st, at)
at = π(st + ⟨∆s, W(st, θ)⟩)
ℓS ≤st + ∆s ≤uS
∥∆s∥p ≤ϵ
0 ≤W(st, θ) ≤1 (6)"
VICTIM AGENT SELECTION,0.17096774193548386,"where W(·, θ) is a function parametrized by θ that takes current state as input and outputs the
weight to distribute the noise to each agent. Suppose we represent W(·, θ) by a neural network, we
can rewrite the formulation (6) as
min∆s,θ
d(fd(st, π(st + ⟨∆s, W(st, θ)⟩)), starget)
s.t.
∆s ∈Cp
t
(7)"
VICTIM AGENT SELECTION,0.17419354838709677,"because the last constraint in (6) can be enforced by using a softmax activation in the neural network
W(·, θ). As a result, (7) can be efﬁciently solved by using PGD. We present the pseudo-code of
the attack in Algorithm 3. After K steps of PGD update, we deﬁne the index in−j as the j-th
largest value within W(st, θK) ∈Rn, i.e. we have Wi(n)(st, θK) ≥Wi(n−1)(st, θK) ≥· · · ≥
Wi(1)(st, θK). Let Ij be the index set of top-j largest outputs of the W(st, θK) network. The"
VICTIM AGENT SELECTION,0.1774193548387097,"ﬁnal perturbation returned by our victim agent selection strategy will be c
∆s = (( c
∆s)1, · · · , ( c
∆s)n)
where ( c
∆s)i = 0 if i /∈Ina and ( c
∆s)i = (∆sK)i if i ∈Ina."
VICTIM AGENT SELECTION,0.18064516129032257,Algorithm 3 cMBA with victim agent selection at timestep t
VICTIM AGENT SELECTION,0.18387096774193548,1: Initialization:
VICTIM AGENT SELECTION,0.1870967741935484,"Given st, starget, π, fd, na; initialize ∆s0; choose learning rate η > 0 λ > 0.
2: For k := 0, · · · , K −1 do
3:
Compute at = π(st + ⟨∆s, W(st, θ)⟩)
4:
Compute ˆst+1 = fd(st, at)
5:
Update ∆s: ∆sk+1 = projCp
t [∆sk −η∇∆sd(ˆst+1, starget)]
6:
Update θ: θk+1 = θk −λ∇θd(ˆst+1, starget)
7: End For
8: Compute Ina = {i(n), · · · , i(n−na)} such that"
VICTIM AGENT SELECTION,0.19032258064516128,"Wi(n)(st, θK) ≥Wi(n−1)(st, θK) ≥· · · ≥Wi(1)(st, θK)"
VICTIM AGENT SELECTION,0.1935483870967742,"9: Return c
∆s = (( c
∆s)1, · · · , ( c
∆s)n) such that"
VICTIM AGENT SELECTION,0.1967741935483871,"( c
∆s)i =
(∆sK)i
if i ∈Ina
0
otherwise"
VICTIM AGENT SELECTION,0.2,"Remark 3.1 For this attack, we assume each agent i has access to the other agent’s observation to
form the concatenated state st."
VICTIM AGENT SELECTION,0.2032258064516129,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2064516129032258,"4
EXPERIMENTS"
EXPERIMENTS,0.20967741935483872,"We perform the attack on various multi-agent MuJoCo (MA-MuJoCo) environments including
Walker2d (2x3), HalfCheetah (2x3), HalfCheetah (6x1), and Ant (4x2). The pair environment
name (config) indicates the name of MuJoCo environment along with the agent partition,
where a conﬁguration of 2x3 means there are in total 2 agents and each agent has 3 actions. Note
that we cannot directly apply methods such as the fast gradient sign method (FGSM) or JSMA (an
attack using saliency map) as in Lin et al. (2020), since those methods require a “target” action
which is not available in continuous action space. Therefore, we consider two baselines: Uniform
and Gaussian baselines where the perturbation follows either Uniform distribution U(−ϵ, ϵ) or
Normal distribution N(0, ϵ)."
EXPERIMENTS,0.2129032258064516,Variants of model-based attack. We consider the following variants of our model-based attack:
EXPERIMENTS,0.2161290322580645,"• Model-based attack on ﬁxed agents: perform Algorithm 1 using a ﬁxed set of victim agents
L."
EXPERIMENTS,0.21935483870967742,"• Best model-based attack on ﬁxed agents: among the model-based attack on ﬁxed agents,
pick the subset that achieves the best performance."
EXPERIMENTS,0.22258064516129034,"• Model-based attack on random agents: perform Algorithm 1 with Lt, which is sampled
uniformly from N such that |Lt| = na (na is the total victim agents)."
EXPERIMENTS,0.22580645161290322,"• Model-based attack with learned victim selection: perform Algorithm 3 to select vulnerable
agents and perform the attack on them."
EXPERIMENTS,0.22903225806451613,"• Greedy victim selection: perform a sweep over subsets of agents with size na. For each
subset, perform Algorithm 1 to obtain proper perturbation and retrieve the corresponding
objective value (distance between predicted state and target state). Select the subset corre-
sponding to the lowest objective value as the victim agents."
EXPERIMENTS,0.23225806451612904,"Experiment setup. We ﬁrst use MADDPG (Lowe et al., 2017) to train MARL agents for the
four MA-MuJoCo environments listed above. Using the trained agents, we collect datasets con-
taining one million transitions to train the dynamics model for each environment. The dynamics
model is a fully connected neural network with three hidden layers of 1000 neurons. We also use
a fully-connected neural network for W(st, θ) in (6) with two hidden layers of 200 neurons. We
use AdamW (Loshchilov & Hutter, 2017) as the optimizer and select the best learning rate from
{1, 5} × {10−5, 10−4, 10−3} (the best learning rate is the one achieving lowest prediction error on
a test set of 80, 000 samples). For our model-based attack, we run PGD for K = 30 steps to solve
(2) and (5). We perform each attack over 16 episodes then average the rewards. We also illustrate
the standard deviation of rewards using the shaded area in the plots."
EXPERIMENTS,0.23548387096774193,"Model-free baselines vs model-based attack. In this experiment, we run the two baseline attacks
along with our model-based attack on the four MA-MuJoCo environments, in which there is only
one victim agent (na = 1). Figure 1 illustrates the performance when we perform these attacks
on speciﬁc agents under multiple budget levels using ℓ∞-norm. For a ﬁxed agent, our model-based
attack consistently outperforms the two baselines. In particular, our model-based attack yields much
lower rewards under low budget constraints (small ϵ) compared to the two baselines."
EXPERIMENTS,0.23870967741935484,"To better visualize the performance difference, 2 illustrates the environment with and without attacks
captured at different time-steps. From Figure 2, our model-based attack is able to make the MuJoCo
agent ﬂip, which terminates the episode at the 409-th timestep. The episode length and total rewards
for each variant are: No attack(1000, 2644.60), Uniform(1000, 1758.97), Gaussian(891, 1399.51),
Ours(409, 287.06)."
EXPERIMENTS,0.24193548387096775,"We also investigate how the state values change during these attacks. Figure 3 presents different
recordings of state values under adversarial attacks compared to no attack. Consider state index 8,
which represents the horizontal velocity of the agent. For the HalfCheetah environment, as the
goal is to make the agent move forward as fast as possible, we expect the reward to be proportional
to this state value. From Figure 3, all three attacks have fairly sparse red fractions across timesteps,
which result in a much lower reward compared to the no-attack setting. Among the three attacks,
our model-based ones appear to have the most sparse red fractions leading to the lowest rewards. In
addition, the model-based attack appears to show its advantage in environments with more agents."
EXPERIMENTS,0.24516129032258063,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.24838709677419354,"0.0
0.1
0.2
0.3
0.4
0.5
Noise Level, || s|| 0 500 1000 1500 2000 2500 3000"
EXPERIMENTS,0.25161290322580643,Mean Reward
EXPERIMENTS,0.25483870967741934,Mean rewards - Walker2d (2x3)
EXPERIMENTS,0.25806451612903225,"Uniform noise - Agent 0
Uniform noise - Agent 1
Gaussian noise - Agent 1"
EXPERIMENTS,0.26129032258064516,"Gaussian noise - Agent 1
cMBA (ours) - Agent 0
cMBA (ours) - Agent 1"
EXPERIMENTS,0.2645161290322581,"0.0
0.1
0.2
0.3
0.4
0.5
Noise Level, || s|| 0 1000 2000 3000 4000 5000"
EXPERIMENTS,0.267741935483871,Mean Reward
EXPERIMENTS,0.2709677419354839,Mean rewards - HalfCheetah (2x3)
EXPERIMENTS,0.27419354838709675,"Uniform noise - Agent 0
Uniform noise - Agent 1
Gaussian noise - Agent 0"
EXPERIMENTS,0.27741935483870966,"Gaussian noise - Agent 1
cMBA (ours) - Agent 0
cMBA (ours) - Agent 1"
EXPERIMENTS,0.2806451612903226,"0.0
0.1
0.2
0.3
0.4
0.5
Noise Level, || s|| 0 500 1000 1500 2000 2500 3000"
EXPERIMENTS,0.2838709677419355,Mean Reward
EXPERIMENTS,0.2870967741935484,Mean rewards - HalfCheetah (6x1)
EXPERIMENTS,0.2903225806451613,"Uniform noise - Agent 0
Uniform noise - Agent 1
Uniform noise - Agent 2
Gaussian noise - Agent 0
Gaussian noise - Agent 1"
EXPERIMENTS,0.29354838709677417,"Gaussian noise - Agent 2
cMBA (ours) - Agent 0
cMBA (ours) - Agent 1
cMBA (ours) - Agent 2"
EXPERIMENTS,0.2967741935483871,"0.0
0.1
0.2
0.3
0.4
0.5
Noise Level, || s|| 0 500 1000 1500 2000 2500 3000"
EXPERIMENTS,0.3,Mean Reward
EXPERIMENTS,0.3032258064516129,Mean rewards - HalfCheetah (6x1)
EXPERIMENTS,0.3064516129032258,"Uniform noise - Agent 3
Uniform noise - Agent 4
Uniform noise - Agent 5
Gaussian noise - Agent 3
Gaussian noise - Agent 4"
EXPERIMENTS,0.3096774193548387,"Gaussian noise - Agent 5
cMBA (ours) - Agent 3
cMBA (ours) - Agent 4
cMBA (ours) - Agent 5"
EXPERIMENTS,0.31290322580645163,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Noise Level, || s|| 2000 1000 0 1000 2000 3000"
EXPERIMENTS,0.3161290322580645,Mean Reward
EXPERIMENTS,0.3193548387096774,Mean rewards - Ant (4x2)
EXPERIMENTS,0.3225806451612903,"Uniform noise - Agent 0
Uniform noise - Agent 1
Gaussian noise - Agent 0"
EXPERIMENTS,0.3258064516129032,"Gaussian noise - Agent 1
cMBA (ours) - Agent 0
cMBA (ours) - Agent 1"
EXPERIMENTS,0.32903225806451614,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Noise Level, || s|| 2000 1000 0 1000 2000 3000"
EXPERIMENTS,0.33225806451612905,Mean Reward
EXPERIMENTS,0.33548387096774196,Mean rewards - Ant (4x2)
EXPERIMENTS,0.3387096774193548,"Uniform noise - Agent 2
Uniform noise - Agent 3
Gaussian noise - Agent 2"
EXPERIMENTS,0.3419354838709677,"Gaussian noise - Agent 3
cMBA (ours) - Agent 2
cMBA (ours) - Agent 3"
EXPERIMENTS,0.34516129032258064,Figure 1: Model based attack vs baseline attacks on ﬁxed agents.
EXPERIMENTS,0.34838709677419355,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.35161290322580646,No attack
EXPERIMENTS,0.3548387096774194,"Step 0
Step 80
Step 160
Step 240
Step 320
Step 400
Step 409"
EXPERIMENTS,0.3580645161290323,"Uniform
Gaussian
Ours"
EXPERIMENTS,0.36129032258064514,"Figure 2: Various attacks on Agent 0 in Ant (4x2) environment with ∥∆s∥∞≤0.1. It can be seen
that the agent ﬂip at the end of episode under our model-based attack, demonstrating the effective-
ness of our algorithm."
EXPERIMENTS,0.36451612903225805,"In particular, our approach results in lower rewards under a smaller budget as seen in HalfCheetah
(6x1) and Ant (4x2) environments."
EXPERIMENTS,0.36774193548387096,"0
200
400
600
800
Timesteps 0 2 4 6 8 10 12 14 16 Index"
EXPERIMENTS,0.3709677419354839,"HalfCheetah_2x3 - No attack (Agent 0), Reward: 4536.39 20 10 0 10 20"
EXPERIMENTS,0.3741935483870968,"0
200
400
600
800
Timesteps 0 2 4 6 8 10 12 14 16 Index"
EXPERIMENTS,0.3774193548387097,"HalfCheetah_2x3 - Uniform noise, || s||
0.2 (Agent 0), Reward: 2209.57 20 10 0 10 20"
EXPERIMENTS,0.38064516129032255,"0
200
400
600
800
Timesteps 0 2 4 6 8 10 12 14 16 Index"
EXPERIMENTS,0.38387096774193546,"HalfCheetah_2x3 - Gaussian noise, || s||
0.2 (Agent 0), Reward: 1782.00 20 10 0 10 20"
EXPERIMENTS,0.3870967741935484,"0
200
400
600
800
Timesteps 0 2 4 6 8 10 12 14 16 Index"
EXPERIMENTS,0.3903225806451613,"HalfCheetah_2x3 - cMBA (ours), || s||
0.2 (Agent 0), Reward: 1112.37 20 10 0 10 20"
EXPERIMENTS,0.3935483870967742,"Figure 3: Recordings of state values in an episode under different attack on Agent 0 in HalfCheetah(
2x3) environment."
EXPERIMENTS,0.3967741935483871,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.4,"Effectiveness of learned victim selection. We further demonstrate the performance of our model-
based attack with and without learned victim selection on one victim agent. Figure 4 illustrates
the performance of four model-based variants: the best performing model-based attack on the ﬁxed
agent (Best Fixed Agent), model-based attack on the random agent (Random Agent), model-based
with learned victim selection as in Algorithm 3 (Learned victim Selection), and model-based with
greedy victim selection (Greedy). Surprisingly, even though the greedy approach employs a brute-
force strategy to select the agent with the closest distance, its performance reveals that selecting
victim agents greedily might not be a good strategy. The learned victim selection seems to be
better than the random or greedy strategy and is comparable with the random one in HalfCheetah
environment. The best-ﬁxed agent variant has good performance under low budget constraints in
HalfCheetah (6x1) and Ant (4x2) environments. We want to emphasize that although the learned
victim selection strategy in Section 3.3 is sub-optimal, it works well in these four environments."
EXPERIMENTS,0.4032258064516129,"0.0
0.1
0.2
0.3
0.4
0.5
Noise Level, || s|| 0 500 1000 1500 2000 2500 3000"
EXPERIMENTS,0.4064516129032258,Mean Reward
EXPERIMENTS,0.4096774193548387,Our cMBA Variants - Walker2d (2x3)
EXPERIMENTS,0.4129032258064516,"Best Fixed Agent
Random Agent"
EXPERIMENTS,0.4161290322580645,"Learned Victim Selection
Greedy"
EXPERIMENTS,0.41935483870967744,"0.0
0.1
0.2
0.3
0.4
0.5
Noise Level, || s|| 0 1000 2000 3000 4000 5000"
EXPERIMENTS,0.42258064516129035,Mean Reward
EXPERIMENTS,0.4258064516129032,Our cMBA Variants - HalfCheetah (2x3)
EXPERIMENTS,0.4290322580645161,"Best Fixed Agent
Random Agent"
EXPERIMENTS,0.432258064516129,"Learned Victim Selection
Greedy"
EXPERIMENTS,0.43548387096774194,"0.0
0.1
0.2
0.3
0.4
0.5
Noise Level, || s|| 0 500 1000 1500 2000 2500 3000"
EXPERIMENTS,0.43870967741935485,Mean Reward
EXPERIMENTS,0.44193548387096776,Our cMBA Variants - HalfCheetah (6x1)
EXPERIMENTS,0.44516129032258067,"Best Fixed Agent
Random Agent"
EXPERIMENTS,0.4483870967741935,"Learned Victim Selection
Greedy"
EXPERIMENTS,0.45161290322580644,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Noise Level, || s|| 2000 1000 0 1000 2000 3000"
EXPERIMENTS,0.45483870967741935,Mean Reward
EXPERIMENTS,0.45806451612903226,Our cMBA Variants - Ant (4x2)
EXPERIMENTS,0.4612903225806452,"Best Fixed Agent
Random Agent"
EXPERIMENTS,0.4645161290322581,"Learned Victim Selection
Greedy"
EXPERIMENTS,0.46774193548387094,"Figure 4: Performance of different variants of our proposed model-based attack (c-MBA) with dif-
ferent choices on victim agents."
CONCLUSIONS,0.47096774193548385,"5
CONCLUSIONS"
CONCLUSIONS,0.47419354838709676,"In this paper, we propose a new attack algorithm named cMBA for evaluating the robustness of
c-MARL environment with continuous action space. Our cMBA algorithm is the ﬁrst to consider
the c-MARL and the adversarial perturbation is computed by solving an optimization problem with
learned dynamics models. The cMBA approach outperforms the other baselines attack by a large
margin under 4 multi-agent MuJoCo environments especially ones with larger number of agents.
Unique to multi-agent setting, we also study different strategies to select victim agents. Extensive
experiment results on standard c-MARL benchmarks show that our proposed model-based attack
and victim-selection strategy can successfully degrade the performance of well-trained c-MARL
agents while outperforming other baselines by a large margin. A future direction is to consider a
timed attack strategy where the perturbation is added at certain timesteps. We can also consider
longer planning step, i.e. trying to reach a target state within the next T steps instead of 1."
CONCLUSIONS,0.4774193548387097,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.4806451612903226,ETHICS STATEMENT
ETHICS STATEMENT,0.4838709677419355,This paper does not contain ethics concerns.
REFERENCES,0.4870967741935484,REFERENCES
REFERENCES,0.49032258064516127,"Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating natural language adversarial examples. arXiv preprint arXiv:1804.07998,
2018."
REFERENCES,0.4935483870967742,"Gregory Dudek, Michael Jenkin, Evangelos Milios, and David Wilkes. A taxonomy for swarm
robots. In Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS’93), volume 1, pp. 441–447. IEEE, 1993."
REFERENCES,0.4967741935483871,"Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018."
REFERENCES,0.5,"Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adver-
sarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019."
REFERENCES,0.5032258064516129,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.5064516129032258,"Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66–83. Springer, 2017."
REFERENCES,0.5096774193548387,"Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017."
REFERENCES,0.5129032258064516,"Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328, 2017."
REFERENCES,0.5161290322580645,"Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline
for natural language attack on text classiﬁcation and entailment. In Proceedings of the AAAI
conference on artiﬁcial intelligence, volume 34, pp. 8018–8025, 2020."
REFERENCES,0.5193548387096775,"Jernej Kos and Dawn Song.
Delving into adversarial attacks on deep policies.
arXiv preprint
arXiv:1705.06452, 2017."
REFERENCES,0.5225806451612903,"Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing, 190:82–94, 2016."
REFERENCES,0.5258064516129032,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.5290322580645161,"Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot. On
the robustness of cooperative multi-agent reinforcement learning. In 2020 IEEE Security and
Privacy Workshops (SPW), pp. 62–68. IEEE, 2020."
REFERENCES,0.532258064516129,"Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tac-
tics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748,
2017."
REFERENCES,0.535483870967742,"Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017."
REFERENCES,0.5387096774193548,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017."
REFERENCES,0.5419354838709678,Under review as a conference paper at ICLR 2022
REFERENCES,0.5451612903225806,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.5483870967741935,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016."
REFERENCES,0.5516129032258065,"Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003."
REFERENCES,0.5548387096774193,"Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High conﬁ-
dence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 427–436, 2015."
REFERENCES,0.5580645161290323,"Frans A Oliehoek and Christopher Amato.
A concise introduction to decentralized POMDPs.
Springer, 2016."
REFERENCES,0.5612903225806452,"Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value func-
tions for decentralized pomdps. Journal of Artiﬁcial Intelligence Research, 32:289–353, 2008."
REFERENCES,0.5645161290322581,"Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep rein-
forcement learning. arXiv preprint arXiv:1908.03963, 2019."
REFERENCES,0.567741935483871,"Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European sympo-
sium on security and privacy (EuroS&P), pp. 372–387. IEEE, 2016."
REFERENCES,0.5709677419354838,"Bei Peng, Tabish Rashid, Christian A Schroeder de Witt, Pierre-Alexandre Kamienny, Philip HS
Torr, Wendelin B¨ohmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. arXiv preprint arXiv:2003.06709, 2020."
REFERENCES,0.5741935483870968,"Thomy Phan, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, Bernhard Kempter, Cornel Klein,
Horst Sauer, Reiner Schmid, Jan Wieghardt, Marc Zeller, et al. Learning and testing resilience
in cooperative multi-agent systems. In Proceedings of the 19th International Conference on Au-
tonomous Agents and MultiAgent Systems, pp. 1055–1063, 2020."
REFERENCES,0.5774193548387097,"Thomy Phan, Lenz Belzner, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, and Claudia Linnhoff-
Popien. Resilient multi-agent reinforcement learning with adversarial value decomposition. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 11308–11316,
2021."
REFERENCES,0.5806451612903226,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforce-
ment learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018."
REFERENCES,0.5838709677419355,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob N Fo-
erster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent rein-
forcement learning. J. Mach. Learn. Res., 21:178–1, 2020."
REFERENCES,0.5870967741935483,"Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019."
REFERENCES,0.5903225806451613,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR,
2015."
REFERENCES,0.5935483870967742,"Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 5887–5896. PMLR, 2019."
REFERENCES,0.5967741935483871,Under review as a conference paper at ICLR 2022
REFERENCES,0.6,"Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017."
REFERENCES,0.603225806451613,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013."
REFERENCES,0.6064516129032258,"Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PloS one, 12(4):e0172395, 2017."
REFERENCES,0.6096774193548387,"Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pp. 330–337, 1993."
REFERENCES,0.6129032258064516,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.
IEEE, 2012."
REFERENCES,0.6161290322580645,"Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020a."
REFERENCES,0.6193548387096774,"Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-policy multi-
agent decomposed policy gradients. arXiv preprint arXiv:2007.12322, 2020b."
REFERENCES,0.6225806451612903,"Tsui-Wei Weng, Krishnamurthy Dj Dvijotham, Jonathan Uesato, Kai Xiao, Sven Gowal, Robert
Stanforth, and Pushmeet Kohli. Toward evaluating robustness of deep reinforcement learning
with continuous control. In International Conference on Learning Representations, 2019."
REFERENCES,0.6258064516129033,"Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust multi-
agent reinforcement learning with model uncertainty. In NeurIPS, 2020a."
REFERENCES,0.6290322580645161,"Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li.
Adversarial attacks on
deep-learning models in natural language processing: A survey. ACM Transactions on Intelligent
Systems and Technology (TIST), 11(3):1–41, 2020b."
REFERENCES,0.632258064516129,Under review as a conference paper at ICLR 2022
REFERENCES,0.635483870967742,"A
MODE DETAILS ON EXPERIMENT SETUP IN SECTION 4"
REFERENCES,0.6387096774193548,"Specifying target state for each environment. To perform our model based attack, we need to
specify a target state that potentially worsens the total reward. In multi-agent MuJoCo environments,
each agent has access to its own observation of the agent consisting the position-related and velocity-
related information. The position-related information includes part of x, y, z coordinates and the
quarternion that represents the orientation of the agent. The velocity-related information contains
global linear velocities and angular velocities for each joint in a MuJoCo agent. We refer the reader
to Todorov et al. (2012) for more information about each MuJoCo environment. Now we describe
the design of this target state for each environment as follows:"
REFERENCES,0.6419354838709678,"• Walker (2x3) environment: Since the episode ends whenever the agent falls, i.e. the z
coordinate falls below certain threshold. In this environment, the target state has a value of
0 for the index that corresponds to the z coordinate of the MuJoCo agent (index 0)."
REFERENCES,0.6451612903225806,"• HalfCheetah (2x3) and HalfCheetah (6x1) environments: As the goal is to make agent
moves as fast as possible, we set the index corresponding to the linear velocities to 0 (index
8)."
REFERENCES,0.6483870967741936,"• Ant (4x2) environment: As the agent can move freely in a 2D-plan, we set the index
corresponding to the x, y linear velocities to 0 (indices 13 and 14)."
REFERENCES,0.6516129032258065,"B
ADDITIONAL EXPERIMENTS"
REFERENCES,0.6548387096774193,"In this section, we present experimental results in addition to ones presented in Section 4."
REFERENCES,0.6580645161290323,"Figure 5 illustrates the environment with and without attacks captured at different timesteps. From
Figure 5, our model based attack is able to make the MuJoCo agent fall down which terminates
the episode at the 65-th timestep. The episode length and total rewards for each variant are: No
attack(478, 1736.44), Uniform(382, 1037.73), Gaussian(90, 32.24), Ours(63, −34.26). Figure 6
illustrates how the state values change during an episode. These state values correspond to the agent
shown in Figure 5. If we look at how the noise values change as in Figure 7, the noise generated by
our approach appears to maximize the permissible noise budget."
REFERENCES,0.6612903225806451,"In addition to the ℓ∞-norm budget constraint, we also evaluate adversarial attacks using the ℓ1-
norm constraint. Note that using ℓ1-norm for budget constraint is more challenging as the attack
needs to distribute the noise across all states while in the ℓ∞-norm the computation of perturba-
tion for individual state is independent. Figure 8 illustrates an episode of a Walker agent with and
without attack. Our cMBA approach is able to make the agent fall at the 255-th timestep with the
episode length and rewards for each setting as: No attack(661, 2513.57), Uniform(403, 1144.77),
Gaussian(455, 1637.16), Ours(253, 759.85).
From the noise values in Figure 7, our cMBA
method appears to put noise on a few selective states at each timestep, similar to the Gausian noise
setting."
REFERENCES,0.6645161290322581,"Additional experiments using the approach in Lin et al. (2020) for continuous action spaces:
In this experiment, we follow the approach in Lin et al. (2020) in the Ant (4x2) environment where
we train an adversarial policy for one agent trying to minimize the total team reward while the
remaining agents use the trained MARL policy. The adversarial policy is trained for 1 million
timesteps. The results using this approach compared with our approach and the two baselines are
presented in Figure 11."
REFERENCES,0.667741935483871,"From Figure 11, our cMBA approach outperforms Lin et al. (2020)’s approach as it is able to achieve
lower team rewards when attacking the same agent. We also note that Lin et al. (2020)’s approach
is better than the two baselines using Uniform and Gaussian noise."
REFERENCES,0.6709677419354839,"Effect of using ∥·∥1 budget constraint:
In this experiment, we replace the ∥·∥∞by the ∥·∥1 for
budget constraint. Using ∥·∥1 constraint will produce a signiﬁcant different adversarial noise pattern"
REFERENCES,0.6741935483870968,Under review as a conference paper at ICLR 2022
REFERENCES,0.6774193548387096,No attack
REFERENCES,0.6806451612903226,"Step 0
Step 15
Step 30
Step 45
Step 60
Step 65"
REFERENCES,0.6838709677419355,"Uniform
Gaussian
Ours"
REFERENCES,0.6870967741935484,Figure 5: Various attacks on Agent 0 in Walker (2x3) environment with ∥∆s∥∞≤0.2.
REFERENCES,0.6903225806451613,"0
10
20
30
40
50
60
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.6935483870967742,"Walker_2x3 - No attack (Agent 0), Reward: 1736.44 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.6967741935483871,"0
10
20
30
40
50
60
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7,"Walker_2x3 - Uniform noise, || s||
0.2 (Agent 0), Reward: 1037.73 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.7032258064516129,"0
10
20
30
40
50
60
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7064516129032258,"Walker_2x3 - Gaussian noise, || s||
0.2 (Agent 0), Reward: 32.24 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.7096774193548387,"0
10
20
30
40
50
60
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7129032258064516,"Walker_2x3 - cMBA (ours), || s||
0.2 (Agent 0), Reward: -34.26 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0"
REFERENCES,0.7161290322580646,"Figure 6: Record of state values in an episode under different attack on Agent 0 in Walker (2x3)
environment with ∥∆s∥∞≤0.2."
REFERENCES,0.7193548387096774,Under review as a conference paper at ICLR 2022
REFERENCES,0.7225806451612903,"0
10
20
30
40
50
60
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7258064516129032,"Walker_2x3 - Uniform noise, || s||
0.2 (Agent 0), Reward: 1037.73 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7290322580645161,"0
10
20
30
40
50
60
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7322580645161291,"Walker_2x3 - Gaussian noise, || s||
0.2 (Agent 0), Reward: 32.24 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7354838709677419,"0
10
20
30
40
50
60
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7387096774193549,"Walker_2x3 - cMBA (ours), || s||
0.2 (Agent 0), Reward: -34.26 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20"
REFERENCES,0.7419354838709677,"Figure 7: Record of noise values in an episode under different attack on Agent 0 in Walker (2x3)
environment with with ∥∆s∥∞≤0.2."
REFERENCES,0.7451612903225806,No attack
REFERENCES,0.7483870967741936,"Step 0
Step 50
Step 100
Step 150
Step 200
Step 250
Step 255"
REFERENCES,0.7516129032258064,"Uniform
Gaussian
Ours"
REFERENCES,0.7548387096774194,Figure 8: Various attacks on Agent 0 in Walker (2x3) environment with ∥∆s∥1 ≤0.5.
REFERENCES,0.7580645161290323,Under review as a conference paper at ICLR 2022
REFERENCES,0.7612903225806451,"0
50
100
150
200
250
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7645161290322581,"Walker_2x3 - No attack (Agent 0), Reward: 2513.57 10 5 0 5 10"
REFERENCES,0.7677419354838709,"0
50
100
150
200
250
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7709677419354839,"Walker_2x3 - Uniform noise, || s||1
0.5 (Agent 0), Reward: 1144.77 10 5 0 5 10"
REFERENCES,0.7741935483870968,"0
50
100
150
200
250
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7774193548387097,"Walker_2x3 - Gaussian noise, || s||1
0.5 (Agent 0), Reward: 1637.16 10 5 0 5 10"
REFERENCES,0.7806451612903226,"0
50
100
150
200
250
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7838709677419354,"Walker_2x3 - cMBA (ours), || s||1
0.5 (Agent 0), Reward: 759.85 10 5 0 5 10"
REFERENCES,0.7870967741935484,"Figure 9: Record of state values in an episode under different attack on Agent 0 in Walker (2x3)
environment with with ∥∆s∥1 ≤0.5."
REFERENCES,0.7903225806451613,Under review as a conference paper at ICLR 2022
REFERENCES,0.7935483870967742,"0
50
100
150
200
250
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.7967741935483871,"Walker_2x3 - Uniform noise, || s||1
0.5 (Agent 0), Reward: 1144.77 0.4 0.2 0.0 0.2 0.4"
REFERENCES,0.8,"0
50
100
150
200
250
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.8032258064516129,"Walker_2x3 - Gaussian noise, || s||1
0.5 (Agent 0), Reward: 1637.16 0.4 0.2 0.0 0.2 0.4"
REFERENCES,0.8064516129032258,"0
50
100
150
200
250
Timesteps 0 2 4 6 8 10 12 14 16 Index"
REFERENCES,0.8096774193548387,"Walker_2x3 - cMBA (ours), || s||1
0.5 (Agent 0), Reward: 759.85 0.4 0.2 0.0 0.2 0.4"
REFERENCES,0.8129032258064516,"Figure 10: Record of noise values in an episode under different attack on Agent 0 in Walker (2x3)
environment with with ∥∆s∥1 ≤0.5."
REFERENCES,0.8161290322580645,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Noise Level, || s|| 2000 1000 0 1000 2000 3000"
REFERENCES,0.8193548387096774,Mean Reward
REFERENCES,0.8225806451612904,Mean rewards - Ant (4x2)
REFERENCES,0.8258064516129032,"Uniform noise - Agent 0
Uniform noise - Agent 1
Gaussian noise - Agent 0
Gaussian noise - Agent 1"
REFERENCES,0.8290322580645161,"Lin et al (2020) - Agent 0
Lin et al (2020) - Agent 1
cMBA (ours) - Agent 0
cMBA (ours) - Agent 1"
REFERENCES,0.832258064516129,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Noise Level, || s|| 2000 1000 0 1000 2000 3000"
REFERENCES,0.8354838709677419,Mean Reward
REFERENCES,0.8387096774193549,Mean rewards - Ant (4x2)
REFERENCES,0.8419354838709677,"Uniform noise - Agent 2
Uniform noise - Agent 3
Gaussian noise - Agent 2
Gaussian noise - Agent 3"
REFERENCES,0.8451612903225807,"Lin et al (2020) - Agent 2
Lin et al (2020) - Agent 3
cMBA (ours) - Agent 2
cMBA (ours) - Agent 3"
REFERENCES,0.8483870967741935,Figure 11: Adversarial attacks using ∥·∥∞budget constraint in Ant (4x2) environment.
REFERENCES,0.8516129032258064,Under review as a conference paper at ICLR 2022
REFERENCES,0.8548387096774194,"compared to ∥·∥∞as producing adversarial noise when using ∥·∥∞is independent for each state
while it is not the case for ∥·∥1. We run different attacks on two environments, HalfCheetah (6x1)
and Ant (4x2), and the results are presented in Figure 12 and Figure 13. From these ﬁgures, cMBA
outperforms other baselines. In particular, it is able to achieve much lower rewards under smaller
budget constraint which shows the advantage of our approach."
REFERENCES,0.8580645161290322,"0
2
4
6
8
10
Noise Level, || s||1 0 500 1000 1500 2000 2500 3000"
REFERENCES,0.8612903225806452,Mean Reward
REFERENCES,0.864516129032258,Mean rewards - HalfCheetah (6x1)
REFERENCES,0.867741935483871,"Uniform noise - Agent 0
Uniform noise - Agent 1
Uniform noise - Agent 2
Gaussian noise - Agent 0
Gaussian noise - Agent 1"
REFERENCES,0.8709677419354839,"Gaussian noise - Agent 2
cMBA (ours) - Agent 0
cMBA (ours) - Agent 1
cMBA (ours) - Agent 2"
REFERENCES,0.8741935483870967,"0
2
4
6
8
10
Noise Level, || s||1 0 500 1000 1500 2000 2500 3000"
REFERENCES,0.8774193548387097,Mean Reward
REFERENCES,0.8806451612903226,Mean rewards - HalfCheetah (6x1)
REFERENCES,0.8838709677419355,"Uniform noise - Agent 3
Uniform noise - Agent 4
Uniform noise - Agent 5
Gaussian noise - Agent 3
Gaussian noise - Agent 4"
REFERENCES,0.8870967741935484,"Gaussian noise - Agent 5
cMBA (ours) - Agent 3
cMBA (ours) - Agent 4
cMBA (ours) - Agent 5"
REFERENCES,0.8903225806451613,Figure 12: Adversarial attacks using ∥·∥1 budget constraint in HalfCheetah (6x1) environment.
REFERENCES,0.8935483870967742,"0
2
4
6
8
10
Noise Level, || s||1 1000 500 0 500 1000 1500 2000 2500 3000"
REFERENCES,0.896774193548387,Mean Reward
REFERENCES,0.9,Mean rewards - Ant (4x2)
REFERENCES,0.9032258064516129,"Uniform noise - Agent 0
Uniform noise - Agent 1
Gaussian noise - Agent 0"
REFERENCES,0.9064516129032258,"Gaussian noise - Agent 1
cMBA (ours) - Agent 0
cMBA (ours) - Agent 1"
REFERENCES,0.9096774193548387,"0
2
4
6
8
10
Noise Level, || s||1 1000 500 0 500 1000 1500 2000 2500 3000"
REFERENCES,0.9129032258064517,Mean Reward
REFERENCES,0.9161290322580645,Mean rewards - Ant (4x2)
REFERENCES,0.9193548387096774,"Uniform noise - Agent 2
Uniform noise - Agent 3
Gaussian noise - Agent 2"
REFERENCES,0.9225806451612903,"Gaussian noise - Agent 3
cMBA (ours) - Agent 2
cMBA (ours) - Agent 3"
REFERENCES,0.9258064516129032,Figure 13: Adversarial attacks using ∥·∥1 budget constraint in Ant (4x2) environment.
REFERENCES,0.9290322580645162,"Adversarial attacks using dynamics model with various accuracy:
In this test, we compare the
performance of our attack when using trained dynamics model with different mean-squared error
(MSE). We use the Ant (4x2) environment and consider the following 3 dynamics models:"
REFERENCES,0.932258064516129,"• The ﬁrst model is trained using 1 million samples for 100 epochs and select the model with
the best performance. We denote this model ”1M - Best epoch”.
• The second model is trained using 1 million samples for only 1 epoch. We denote this
model ”1M - 1st epoch”.
• We further reduce the number of samples used for training the dynamic model to only
200,000 and train for only 1 epoch. We denote this model ”200K - 1st epoch”."
REFERENCES,0.9354838709677419,Under review as a conference paper at ICLR 2022
REFERENCES,0.9387096774193548,"These models are evaluated on a predeﬁned test set consisting of 100,000 samples. The test MSE of
these models are 0.33, 0.69, 0.79, respectively, with the initial test MSE of 1.241."
REFERENCES,0.9419354838709677,"0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
Noise Level, || s|| 1000 500 0 500 1000 1500 2000 2500 3000"
REFERENCES,0.9451612903225807,Mean Reward
REFERENCES,0.9483870967741935,Effect of dynamics model accuracy - Ant (4x2)
M - BEST EPOCH,0.9516129032258065,"1M - Best epoch
1M - 1st epoch
200K - 1st epoch"
M - BEST EPOCH,0.9548387096774194,"0
2
4
6
8
10
Noise Level, || s||1 1000 500 0 500 1000 1500 2000 2500 3000"
M - BEST EPOCH,0.9580645161290322,Mean Reward
M - BEST EPOCH,0.9612903225806452,Effect of dynamics model accuracy - Ant (4x2)
M - BEST EPOCH,0.964516129032258,"1M - Best epoch
1M - 1st epoch
200K - 1st epoch"
M - BEST EPOCH,0.967741935483871,Figure 14: Adversarial attacks using 3 dynamics models in Ant (4x2) environment.
M - BEST EPOCH,0.9709677419354839,"Figure 14 depicts the attacks using these three models on the same agent in Ant (4x2) environment
using ∥·∥∞budget constraint. Interestingly, the dynamics model trained with only 200,000 sam-
ples for 1 epoch can achieve comparable performance with the other two models using much more
samples."
M - BEST EPOCH,0.9741935483870968,"Comparison between attacking all agents vs attacking 1 agent:
In this experiment, we compare
performance of different approaches when attacking one victim agent or simply perturbing all agents
under the same budget constraint. We compare different approaches in the Ant (4x2) environment.
In particular, we use the two baselines and our cMBA approach to attack each agent out of 4 agent,
denoted as (agent i) for i = 0, 1, 2, 3. We also use these approach in which we simultaneously
perturb the inputs to all agents, denoted as (4 agents). In addition, we also illustrate the performance
of our victim selection scheme when we only want to attack either 1 or two agents at the same
time. We report the mean and standard deviation of rewards using these approach with several
values of budget level ε in Table 1 where we use ∥· ∥∞for the budget constraint. From Table 1,
the performance is getting better as the attacker is stronger, i.e. more agents are attacked at the
same time. This makes sense as for ∥· ∥∞, the adversarial noise for each agent can be computed
independently."
M - BEST EPOCH,0.9774193548387097,"Now, we consider the ∥· ∥1 budget constraint. This is a more interesting setting as the total noise
budget is ﬁxed the adversarial noise of one agent will affect one from the other. The results are
shown using several values of budget level ε in Table 2. We can observe that in this case attacking
more agents is not always effective."
M - BEST EPOCH,0.9806451612903225,Under review as a conference paper at ICLR 2022
M - BEST EPOCH,0.9838709677419355,"Table 1: Comparison between adversarial attacks on a single agent and all agents in Ant (4x2)
environment under ∥· ∥∞≤ε budget constraint."
M - BEST EPOCH,0.9870967741935484,"Rewards: Mean (standard deviation)
Methods
ε = 0.025
ε = 0.05
ε = 0.075
ε = 0.1"
M - BEST EPOCH,0.9903225806451613,"Uniform noise (agent 0)
2332 (594)
2296 (88)
1773 (941)
1542 (554)
Uniform noise (agent 1)
2233 922)
2033 (565)
1579 (856)
1293 (385)
Uniform noise (agent 2)
2028 (695)
1713 (649)
988 (650)
322 (652)
Uniform noise (agent 3)
2579 (66)
2484 (73)
2159 (623)
2208 (279)
Uniform noise (4 agents)
2089 (450)
1093 (753)
299 (781)
62 (921)
Gaussian noise (agent 0)
2450 (134)
2030 (405)
1660 (663)
1067 (536)
Gaussian noise (agent 1)
2204 (709)
1992 (337)
1447 (406)
1141 (381)
Gaussian noise (agent 2)
2256 (297)
1313 (977)
287 (923)
154 (570)
Gaussian noise (agent 3)
2550 (83)
2331 (567)
1927 (1250)
2115 (312)
Gaussian noise (4 agents)
2022 (303)
761 (1083)
86 (688)
-133 (809)
cMBA (agent 0)
923 (340)
244 (737)
516 (737)
-8 (306)
cMBA (agent 1)
1417 (48)
553 (269)
108 (303)
-60 (309)
cMBA (agent 2)
227 (955)
-652 (1097)
-241 (660)
-631 (911)
cMBA (agent 3)
2024 (87)
861 (1298)
567 (1012)
-172 (725)
cMBA (4 agents)
1116 (379)
314 (734)
72 (407)
-239 (281)
Leaned Victim Selection (1 agent)
-165 (805)
624 (103)
256 (201)
-2 (123)
Learned Victim Selection (2 agent)
405 (308)
-176 (853)
-588 (1026)
-325 (730)"
M - BEST EPOCH,0.9935483870967742,"Table 2: Comparison between adversarial attacks on a single agent and all agents in Ant (4x2)
environment under ∥· ∥1 ≤ε budget constraint."
M - BEST EPOCH,0.9967741935483871,"Rewards: Mean (standard deviation)
Methods
ϵ = 1.5
ϵ = 2.0
ϵ = 2.5
ϵ = 3.0
Uniform noise (agent 0)
2189 (151)
2093 (148)
1615 (524)
1421 (780)
Uniform noise (agent 1)
2039 (139)
1715 (282)
1387 (586)
1243 (303)
Uniform noise (agent 2)
1542 (826)
1073 (755)
581 (920)
436 (317)
Uniform noise (agent 3)
2468 (75)
2332 (282)
2306 (131)
2136 (319)
Uniform noise (4 agents)
2224 (353)
2037 (412)
1734 (541)
1473 (558)
Gaussian noise (agent 0)
1752 (877)
1676 (650)
1431 (774)
1493 (554)
Gaussian noise (agent 1)
1775 (344)
1442 (619)
1257 (487)
1036 (554)
Gaussian noise (agent 2)
1326 (766)
1049 (666)
776 (763)
516. (842)
Gaussian noise (agent 3)
2103 (559)
2319 (104)
2086 (480)
1896 (603)
Gaussian noise (4 agents)
1614 (776)
1553 (793)
1544 (568)
1470 (476)
cMBA (agent 0)
2068 (145)
1699 (169)
1432 (136)
1282 (136)
cMBA (agent 1)
2005 (355)
1682 (346)
1505 (222)
1189 (585)
cMBA (agent 2)
1554 (781)
1080 (792)
433 (1202)
513 (987)
cMBA (agent 3)
2302 (80)
2032 (528)
2099 (92)
1604 (1069)
cMBA (4 agents)
2212 (225)
1909 (332)
1671 (212)
1504 (201)
Leaned Victim Selection (1 agent)
706 (1118)
355 (635)
-326 (931)
-139 (608)
Learned Victim Selection (2 agent)
1274 (779)
720 (939)
152 (787)
156 (638)"
