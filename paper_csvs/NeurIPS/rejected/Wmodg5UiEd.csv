Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0008305647840531562,"Learning from human feedback plays an important role in aligning generative
1"
ABSTRACT,0.0016611295681063123,"models, such as large language models (LLM). However, the effectiveness of
2"
ABSTRACT,0.0024916943521594683,"this approach can be influenced by adversaries, who may intentionally provide
3"
ABSTRACT,0.0033222591362126247,"misleading preferences to manipulate the output in an undesirable or harmful
4"
ABSTRACT,0.004152823920265781,"direction. To tackle this challenge, we study a specific model within this problem
5"
ABSTRACT,0.0049833887043189366,"domain–contextual dueling bandits with adversarial feedback, where the true
6"
ABSTRACT,0.005813953488372093,"preference label can be flipped by an adversary. We propose an algorithm namely
7"
ABSTRACT,0.006644518272425249,"robust contextual dueling bandits (RCDB), which is based on uncertainty-weighted
8"
ABSTRACT,0.007475083056478406,"maximum likelihood estimation. Our algorithm achieves an eO(d
√"
ABSTRACT,0.008305647840531562,"T + dC) regret
9"
ABSTRACT,0.009136212624584718,"bound, where T is the number of rounds, d is the dimension of the context, and
10"
ABSTRACT,0.009966777408637873,"0 ≤C ≤T is the total number of adversarial feedback. We also prove a lower
11"
ABSTRACT,0.01079734219269103,"bound to show that our regret bound is nearly optimal, both in scenarios with and
12"
ABSTRACT,0.011627906976744186,"without (C = 0) adversarial feedback. Additionally, we conduct experiments to
13"
ABSTRACT,0.012458471760797342,"evaluate our proposed algorithm against various types of adversarial feedback.
14"
ABSTRACT,0.013289036544850499,"Experimental results demonstrate its superiority over the state-of-the-art dueling
15"
ABSTRACT,0.014119601328903655,"bandit algorithms in the presence of adversarial feedback.
16"
INTRODUCTION,0.014950166112956811,"1
Introduction
17"
INTRODUCTION,0.015780730897009966,"Acquiring an appropriate reward proves challenging in numerous real-world applications, often
18"
INTRODUCTION,0.016611295681063124,"necessitating intricate instrumentation (Zhu et al., 2020) and time-consuming calibration (Yu et al.,
19"
INTRODUCTION,0.01744186046511628,"2020) to achieve satisfactory levels of sample efficiency. For instance, in training large language
20"
INTRODUCTION,0.018272425249169437,"models (LLM) using reinforcement learning from human feedback (RLHF), the diverse values and
21"
INTRODUCTION,0.01910299003322259,"perspectives of humans can lead to uncalibrated and noisy rewards (Ouyang et al., 2022). In contrast,
22"
INTRODUCTION,0.019933554817275746,"preference-based data, which involves comparing or ranking various actions, is a more straightforward
23"
INTRODUCTION,0.020764119601328904,"method for capturing human judgments and decisions. In this context, the dueling bandit model
24"
INTRODUCTION,0.02159468438538206,"(Yue et al., 2012) provides a problem framework that focuses on optimal decision-making through
25"
INTRODUCTION,0.022425249169435217,"pairwise comparisons, rather than relying on the absolute reward for each action.
26"
INTRODUCTION,0.023255813953488372,"However, human feedback may not always be reliable. In real-world applications, human feedback
27"
INTRODUCTION,0.02408637873754153,"is particularly vulnerable to manipulation through preference label flip. Adversarial feedback can
28"
INTRODUCTION,0.024916943521594685,"significantly increase the risk of misleading a large language model (LLM) into erroneously prioritiz-
29"
INTRODUCTION,0.02574750830564784,"ing harmful content, under the false belief that it reflects human preference. Despite the significant
30"
INTRODUCTION,0.026578073089700997,"influence of adversarial feedback, there is limited existing research on the impact of adversarial
31"
INTRODUCTION,0.027408637873754152,"feedback specifically within the context of dueling bandits. A notable exception is Agarwal et al.
32"
INTRODUCTION,0.02823920265780731,"(2021), which studies dueling bandits when an adversary can flip some of the preference labels
33"
INTRODUCTION,0.029069767441860465,"received by the learner. They proposed an algorithm that is agnostic to the amount of adversarial
34"
INTRODUCTION,0.029900332225913623,"feedback introduced by the adversary. However, their setting has the following two limitations.
35"
INTRODUCTION,0.030730897009966777,"First, their study was confined to a finite-armed setting, which renders their results less applicable
36"
INTRODUCTION,0.03156146179401993,"to modern applications such as RLHF. Second, their adversarial feedback is defined on the whole
37"
INTRODUCTION,0.03239202657807309,"comparison matrix. In each round, the adversary observes the outcomes of all pairwise comparisons
38"
INTRODUCTION,0.03322259136212625,"and then decides to corrupt some of the pairs before the agent selects the actions. This assumption
39"
INTRODUCTION,0.0340531561461794,"does not align well with the real-world scenario, where the adversary often flips the preference label
40"
INTRODUCTION,0.03488372093023256,"based on the information of the selected actions.
41"
INTRODUCTION,0.03571428571428571,"In this paper, to address the above challenge, we aim to develop contextual dueling bandit algorithms
42"
INTRODUCTION,0.036544850498338874,"that are robust to adversarial feedback. This enables us to effectively tackle problems involving
43"
INTRODUCTION,0.03737541528239203,"a large number of actions while also taking advantage of contextual information. We specifically
44"
INTRODUCTION,0.03820598006644518,"consider a scenario where the adversary knows the selected action pair and the true preference of
45"
INTRODUCTION,0.03903654485049834,"their comparison. In this setting, the adversary’s only decision is whether to flip the preference label
46"
INTRODUCTION,0.03986710963455149,"or not. We highlight our contributions as follows:
47"
INTRODUCTION,0.040697674418604654,"• We propose a new algorithm called robust contextual dueling bandits (RCDB), which integrates
48"
INTRODUCTION,0.04152823920265781,"uncertainty-dependent weights into the Maximum Likelihood Estimator (MLE). Intuitively, our
49"
INTRODUCTION,0.04235880398671096,"choice of weight is designed to induce a higher degree of skepticism about potentially “untrust-
50"
INTRODUCTION,0.04318936877076412,"worthy” feedback. The agent is encouraged to focus more on feedback that is more likely to be
51"
INTRODUCTION,0.04401993355481727,"genuine, effectively diminishing the impact of any adversarial feedback.
52"
INTRODUCTION,0.044850498338870434,"• We analyze the regret of our algorithm under at most C number of adversarial feedback. Our result
53"
INTRODUCTION,0.04568106312292359,"consists of two terms: a C-independent term eO(d
√"
INTRODUCTION,0.046511627906976744,"T), which matches the lower bound established
54"
INTRODUCTION,0.0473421926910299,"in Bengs et al. (2022) for uncorrupted linear contextual dueling bandits, and a C-dependent term
55"
INTRODUCTION,0.04817275747508306,"eO(dC). Furthermore, we establish a lower bound for dueling bandits with adversarial feedback,
56"
INTRODUCTION,0.049003322259136214,"demonstrating the optimality of our adversarial term. Consequently, our algorithm for dueling
57"
INTRODUCTION,0.04983388704318937,"bandits attains the optimal regret in both scenarios, with and without adversarial feedback.
58"
INTRODUCTION,0.050664451827242524,"• We conduct extensive experiments to validate the effectiveness of our algorithm RCDB. To compre-
59"
INTRODUCTION,0.05149501661129568,"hensively assess RCDB’s robustness against adversarial feedback, we evaluate its performance under
60"
INTRODUCTION,0.05232558139534884,"various types of adversarial feedback and compare the results with state-of-the-art dueling bandit
61"
INTRODUCTION,0.053156146179401995,"algorithms. Experimental results demonstrate the superiority of our algorithm in the presence of
62"
INTRODUCTION,0.05398671096345515,"adversarial feedback, which corroborate our theoretical analysis.
63"
INTRODUCTION,0.054817275747508304,Table 1: Comparison of algorithms for robust bandits and dueling bandits.
INTRODUCTION,0.05564784053156146,"Model
Algorithm
Setting
Regret"
INTRODUCTION,0.05647840531561462,Multi-layer Active Arm Elimination Race
INTRODUCTION,0.057308970099667775,"(Lykouris et al., 2018)
K-armed Bandits
eO
 
K1.5C
√ T
"
INTRODUCTION,0.05813953488372093,"BARBAR
(Gupta et al., 2019)
K-armed Bandits
eO
 √"
INTRODUCTION,0.058970099667774084,"KT + KC
"
INTRODUCTION,0.059800664451827246,"SBE
(Li et al., 2019)
Linear Bandits
eO
 
d2C/∆+ d5/∆2
Bandits"
INTRODUCTION,0.0606312292358804,Robust Phased Elimination
INTRODUCTION,0.061461794019933555,"(Bogunovic et al., 2021)
Linear Bandits
eO
 √"
INTRODUCTION,0.06229235880398671,dT + d1.5C + C2
INTRODUCTION,0.06312292358803986,Robust weighted OFUL
INTRODUCTION,0.06395348837209303,"(Zhao et al., 2021)
Linear Contextual Bandits
eO
 
dC
√ T
"
INTRODUCTION,0.06478405315614617,"CW-OFUL
(He et al., 2022)
Linear Contextual Bandits
eO
 
d
√"
INTRODUCTION,0.06561461794019934,"T + dC
"
INTRODUCTION,0.0664451827242525,"WIWR
(Agarwal et al., 2021)
K-armed Dueling Bandits
eO
 
K2C/∆min + P"
INTRODUCTION,0.06727574750830564,"i̸=i∗K2/∆2
i
"
INTRODUCTION,0.0681063122923588,"Versatile-DB
Dueling Bandits
(Saha and Gaillard, 2022)
K-armed Dueling Bandits
eO
 
C + P"
INTRODUCTION,0.06893687707641195,"i̸=i∗1/∆i +
√ K
"
INTRODUCTION,0.06976744186046512,"RCDB
(Our work)
Contextual Dueling Bandits
eO
 
d
√"
INTRODUCTION,0.07059800664451828,"T + dC
"
INTRODUCTION,0.07142857142857142,"Notation. In this paper, we use plain letters such as x to denote scalars, lowercase bold letters such
64"
INTRODUCTION,0.07225913621262459,"as x to denote vectors and uppercase bold letters such as X to denote matrices. For a vector x, ∥x∥2
65"
INTRODUCTION,0.07308970099667775,"denotes its ℓ2-norm. The weighted ℓ2-norm associated with a positive-definite matrix A is defined
66"
INTRODUCTION,0.0739202657807309,"as ∥x∥A =
√"
INTRODUCTION,0.07475083056478406,"x⊤Ax. For two symmetric matrices A and B, we use A ⪰B to denote A −B is
67"
INTRODUCTION,0.0755813953488372,"positive semidefinite. We use 1 to denote the indicator function and 0 to denote the zero vector. For
68"
INTRODUCTION,0.07641196013289037,"two actions a, b, we use a ≻b to denote a is more preferable to b. For a postive integer N, we use
69"
INTRODUCTION,0.07724252491694353,"[N] to denote {1, 2, . . . , N}. We use standard asymptotic notations including O(·), Ω(·), Θ(·), and
70"
INTRODUCTION,0.07807308970099668,"eO(·), eΩ(·), eΘ(·) will hide logarithmic factors.
71"
RELATED WORK,0.07890365448504984,"2
Related Work
72"
RELATED WORK,0.07973421926910298,"Bandits with Adversarial Reward. The multi-armed bandit problem, involving an agent making
73"
RELATED WORK,0.08056478405315615,"sequential decisions among multiple arms, has been studied with both stochastic rewards (Lai
74"
RELATED WORK,0.08139534883720931,"et al., 1985; Lai, 1987; Auer, 2002; Auer et al., 2002a; Kalyanakrishnan et al., 2012; Lattimore and
75"
RELATED WORK,0.08222591362126246,"Szepesvári, 2020; Agrawal and Goyal, 2012), and adversarial rewards (Auer et al., 2002b; Bubeck
76"
RELATED WORK,0.08305647840531562,"et al., 2012). Moreover, a line of works focuses on designing algorithms that can achieve near-optimal
77"
RELATED WORK,0.08388704318936877,"regret bounds for both stochastic bandits and adversarial bandits simultaneously (Bubeck and Slivkins,
78"
RELATED WORK,0.08471760797342193,"2012; Seldin and Slivkins, 2014; Auer and Chiang, 2016; Seldin and Lugosi, 2017; Zimmert and
79"
RELATED WORK,0.08554817275747509,"Seldin, 2019; Lee et al., 2021), which is known as “the best of both worlds” guarantee. Distinct from
80"
RELATED WORK,0.08637873754152824,"fully stochastic and fully adversarial models, Lykouris et al. (2018) studied a setting, where only a
81"
RELATED WORK,0.0872093023255814,"portion of the rewards is subject to corruption. They proposed an algorithm with a regret dependent
82"
RELATED WORK,0.08803986710963455,"on the corruption level C, defined as the cumulative sum of the corruption magnitudes in each round.
83"
RELATED WORK,0.0888704318936877,"Their result is C times worse than the regret without corruption. Gupta et al. (2019) improved the
84"
RELATED WORK,0.08970099667774087,"result by providing a regret guarantee comprising two terms, a corruption-independent term that
85"
RELATED WORK,0.09053156146179402,"matches the regret lower bound without corruption, and a corruption-dependent term that is linear in
86"
RELATED WORK,0.09136212624584718,"C. In addition, Gupta et al. (2019) proved a lower bound demonstrating the optimality of the linear
87"
RELATED WORK,0.09219269102990033,"dependency on C.
88"
RELATED WORK,0.09302325581395349,"Contextual Bandits with Corruption. Li et al. (2019) studied stochastic linear bandits with
89"
RELATED WORK,0.09385382059800665,"corruption and presented an instance-dependent regret bound linearly dependent on the corruption
90"
RELATED WORK,0.0946843853820598,"level C. Bogunovic et al. (2021) studied the same problem and proposed an algorithm with near-
91"
RELATED WORK,0.09551495016611296,"optimal regret in the non-corrupted case. Lee et al. (2021) studied this problem in a different setting,
92"
RELATED WORK,0.09634551495016612,"where the adversarial corruptions are generated through the inner product of a corrupted vector
93"
RELATED WORK,0.09717607973421927,"and the context vector. For linear contextual bandits, Bogunovic et al. (2021) proved that under an
94"
RELATED WORK,0.09800664451827243,"additional context diversity assumption, the regret of a simple greedy algorithm is nearly optimal
95"
RELATED WORK,0.09883720930232558,"with an additive corruption term. Zhao et al. (2021) and Ding et al. (2022) extended the OFUL
96"
RELATED WORK,0.09966777408637874,"algorithm (Abbasi-Yadkori et al., 2011) and proved a regret with a corruption term polynomially
97"
RELATED WORK,0.1004983388704319,"dependent on the total number of rounds T. He et al. (2022) proposed an algorithm for known
98"
RELATED WORK,0.10132890365448505,"corruption level C to remove the polynomial dependency on T in the corruption term, which only
99"
RELATED WORK,0.10215946843853821,"has a linear dependency on C. They also proved a lower bound showing the optimality of linear
100"
RELATED WORK,0.10299003322259136,"dependency on C for linear contextual bandits with a known corruption level. Additionally, He et al.
101"
RELATED WORK,0.10382059800664452,"(2022) extended the proposed algorithm to an unknown corruption level and provided a near-optimal
102"
RELATED WORK,0.10465116279069768,"performance guarantee that matches the lower bound. For more extensions, Kuroki et al. (2023)
103"
RELATED WORK,0.10548172757475083,"studied best-of-both-worlds algorithms for linear contextual bandits. Ye et al. (2023) proposed a
104"
RELATED WORK,0.10631229235880399,"corruption robust algorithm for nonlinear contextual bandits.
105"
RELATED WORK,0.10714285714285714,"Dueling Bandits and Logistic Bandits. The dueling bandit model was first proposed in Yue
106"
RELATED WORK,0.1079734219269103,"et al. (2012). Compared with bandits, the agent will select two arms and receive the preference
107"
RELATED WORK,0.10880398671096346,"feedback between the two arms from the environment. For general preference, there may not exist
108"
RELATED WORK,0.10963455149501661,"the “best” arm that always wins in the pairwise comparison. Therefore, various alternative winners
109"
RELATED WORK,0.11046511627906977,"are considered, including Condorcet winner (Zoghi et al., 2014; Komiyama et al., 2015), Copeland
110"
RELATED WORK,0.11129568106312292,"winner (Zoghi et al., 2015; Wu and Liu, 2016; Komiyama et al., 2016), Borda winner (Jamieson et al.,
111"
RELATED WORK,0.11212624584717608,"2015; Falahatgar et al., 2017; Heckel et al., 2018; Saha et al., 2021; Wu et al., 2023) and von Neumann
112"
RELATED WORK,0.11295681063122924,"winner (Ramamohan et al., 2016; Dudík et al., 2015; Balsubramani et al., 2016), along with their
113"
RELATED WORK,0.11378737541528239,"corresponding performance metrics. To handle potentially large action space or context information,
114"
RELATED WORK,0.11461794019933555,"Saha (2021) studied a structured contextual dueling bandit setting. In this setting, each arm possesses
115"
RELATED WORK,0.11544850498338871,"an unknown intrinsic reward. The comparison is determined based on a logistic function of the relative
116"
RELATED WORK,0.11627906976744186,"rewards. In a similar setting, Bengs et al. (2022) studied contextual linear stochastic transitivity
117"
RELATED WORK,0.11710963455149502,"model with contextualized utilities. Di et al. (2023) proposed a layered algorithm with variance
118"
RELATED WORK,0.11794019933554817,"aware regret bound. Another line of works does not make the reward assumption. Instead, they
119"
RELATED WORK,0.11877076411960133,"assume the preference feedback can be represented by a function class. Saha and Krishnamurthy
120"
RELATED WORK,0.11960132890365449,"(2022) designed an algorithm that achieves the optimal regret for K-armed contextual dueling bandit
121"
RELATED WORK,0.12043189368770764,"problem. Sekhari et al. (2023) studied contextual dueling bandits in a more general setting and
122"
RELATED WORK,0.1212624584717608,"proposed an algorithm the provides guarantees for both regret and the number of queries. Another
123"
RELATED WORK,0.12209302325581395,"related area of research is the logistic bandits, where the agent selects one arm in each round and
124"
RELATED WORK,0.12292358803986711,"receives a Bernoulli reward. Faury et al. (2020) studied the dependency with respect to the degree
125"
RELATED WORK,0.12375415282392027,"of non-linearity of the logistic function κ. They proposed an algorithm with no dependency in κ.
126"
RELATED WORK,0.12458471760797342,"Abeille et al. (2021) further improved the dependency on κ and proved a problem dependent lower
127"
RELATED WORK,0.12541528239202657,"bound. Faury et al. (2022) proposed a computationally efficient algorithm with regret performance
128"
RELATED WORK,0.12624584717607973,"still matching the lower-bound proved in Abeille et al. (2021).
129"
RELATED WORK,0.1270764119601329,"Dueling Bandits with Adversarial Feedback. A line of work has focused on dueling bandits with
130"
RELATED WORK,0.12790697674418605,"adversarial feedback or corruption. Gajane et al. (2015) studied a fully adversarial utility-based
131"
RELATED WORK,0.1287375415282392,"version of dueling bandits, which was proposed in Ailon et al. (2014). Saha et al. (2021) considered
132"
RELATED WORK,0.12956810631229235,"the Borda regret for adversarial dueling bandits without the assumption of utility. In a setting
133"
RELATED WORK,0.1303986710963455,"parallel to that in Lykouris et al. (2018); Gupta et al. (2019), Agarwal et al. (2021) studied K-armed
134"
RELATED WORK,0.13122923588039867,"dueling bandits in a scenario where an adversary has the capability to corrupt part of the feedback
135"
RELATED WORK,0.13205980066445183,"received by the learner. They designed an algorithm whose regret comprises two terms: one that
136"
RELATED WORK,0.132890365448505,"is optimal in uncorrupted scenarios, and another that is linearly dependent on the total times of
137"
RELATED WORK,0.13372093023255813,"adversarial feedback C. Later on, Saha and Gaillard (2022) achieved “best-of-both world” result for
138"
RELATED WORK,0.1345514950166113,"noncontextual dueling bandits and improved the adversarial term of Agarwal et al. (2021) in the same
139"
RELATED WORK,0.13538205980066445,"setting. For contextual dueling bandits, Wu et al. (2023) proposed an EXP3-type algorithm for the
140"
RELATED WORK,0.1362126245847176,"adversarial linear setting using Borda regret. For a comparison of the most related works for robust
141"
RELATED WORK,0.13704318936877077,"bandits and dueling bandits, please refer to Table 1. In this paper, we study the influence of adversarial
142"
RELATED WORK,0.1378737541528239,"feedback within contextual dueling bandits, particularly in a setting where only a minority of the
143"
RELATED WORK,0.13870431893687707,"feedback is adversarial. Compared to previous studies, most studies have focused on the multi-armed
144"
RELATED WORK,0.13953488372093023,"dueling bandit framework without integrating context information. The notable exception is Wu et al.
145"
RELATED WORK,0.1403654485049834,"(2023); however, this study does not provide guarantees regarding the dependency on the number of
146"
RELATED WORK,0.14119601328903655,"adversarial feedback instances.
147"
PRELIMINARIES,0.1420265780730897,"3
Preliminaries
148"
PRELIMINARIES,0.14285714285714285,"In this work, we study linear contextual dueling bandits with adversarial feedback. In each round
149"
PRELIMINARIES,0.143687707641196,"t ∈[T], the agent observes the context information xt from a context set X and the corresponding
150"
PRELIMINARIES,0.14451827242524917,"action set A. Utilizing this context information, the agent selects two actions, at and bt. Subsequently,
151"
PRELIMINARIES,0.14534883720930233,"the environment will generate a binary feedback (i.e., preference label) lt = 1(at ≻bt) ∈{0, 1}
152"
PRELIMINARIES,0.1461794019933555,"indicating the preferable action. We assume the existence of a reward function r∗(x, a) dependent on
153"
PRELIMINARIES,0.14700996677740863,"the context information x and action a, and a monotonically increasing link function σ satisfying
154"
PRELIMINARIES,0.1478405315614618,"σ(x) + σ(−x) = 1. The preference probability will be determined by the link function and the
155"
PRELIMINARIES,0.14867109634551495,"difference between the rewards of the selected arms, i.e.,
156"
PRELIMINARIES,0.14950166112956811,"P(a ≻b|x) = σ
 
r∗(x, a) −r∗(x, b)

.
(3.1)"
PRELIMINARIES,0.15033222591362128,"We assume that the reward function is linear with respect to some known feature map ϕ(x, a). To be
157"
PRELIMINARIES,0.1511627906976744,"more specific, we make the following assumption:
158"
PRELIMINARIES,0.15199335548172757,"Assumption 3.1. Let ϕ : X × A →Rd be a known feature map, with ∥ϕ(x, a)∥2 ≤1 for any
159"
PRELIMINARIES,0.15282392026578073,"(x, a) ∈X × A. We define the reward function rθ parameterized by θ ∈Rd, with rθ(x, a) =
160"
PRELIMINARIES,0.1536544850498339,"⟨θ, ϕ(x, a)⟩. Moreover, there exists θ∗satisfying rθ∗= r∗, with ∥θ∗∥2 ≤B.
161"
PRELIMINARIES,0.15448504983388706,"Similar assumptions have been made in the literature of dueling bandits (Saha, 2021; Bengs et al.,
162"
PRELIMINARIES,0.1553156146179402,"2022; Xiong et al., 2023). We also make an assumption on the derivative of the link function, which
163"
PRELIMINARIES,0.15614617940199335,"is common in the study of generalized linear models for bandits (Filippi et al., 2010).
164"
PRELIMINARIES,0.1569767441860465,"Assumption 3.2. The link function σ is differentiable. Furthermore, its first-order derivative satisfies:
165"
PRELIMINARIES,0.15780730897009967,"˙σ(·) ≥κ
for some constant κ > 0.
166"
PRELIMINARIES,0.15863787375415284,"In our setting, however, the agent does not directly observe the true binary feedback. Instead, an
167"
PRELIMINARIES,0.15946843853820597,"adversary will see both the choice of the agent and the true feedback. Based on the information, the
168"
PRELIMINARIES,0.16029900332225913,"adversary can decide whether to corrupt the binary feedback or not.1 We represent the adversary’s
169"
PRELIMINARIES,0.1611295681063123,"decision in round t by an adversarial indicator ct, which takes values from the set {0, 1}. If the
170"
PRELIMINARIES,0.16196013289036545,"adversary chooses not to corrupt the result, we have ct = 0. Otherwise, we have ct = 1, which means
171"
PRELIMINARIES,0.16279069767441862,"adversarial feedback in this round. As a result, the agent will observe a flipped preference label, i.e.,
172"
PRELIMINARIES,0.16362126245847175,"the observation ot = 1 −lt. We define C as the total level of adversarial feedback, i.e.,
173"
PRELIMINARIES,0.1644518272425249,"PT
t=1 ct ≤C.
Remark 3.3. Adversarial corruption has been firstly studied in bandits (Lykouris et al., 2018), where
174"
PRELIMINARIES,0.16528239202657807,"in each round t, the agent selects an action at and the environment generates a numerical reward
175"
PRELIMINARIES,0.16611295681063123,"rt(at). The adversary observes the reward and returns a corrupted reward ¯rt. The corruption level
176"
PRELIMINARIES,0.1669435215946844,"C is defined by PT
t=1 |rt(at) −¯rt| ≤C. Compared with the continuous perturbation of rewards
177"
PRELIMINARIES,0.16777408637873753,"in bandits, the adversary’s label flipping attack method in our model is quite different. The cost
178"
PRELIMINARIES,0.1686046511627907,"of obtaining adversarial feedback is uniformly 1, unlike in bandits where the cost depends on the
179"
PRELIMINARIES,0.16943521594684385,"intensity of the perturbation. Additionally, adversarial feedback in our setting involves comparing two
180"
PRELIMINARIES,0.17026578073089702,"arms, whereas in bandits it pertains to the reward of a single arm. The only previous work that studied
181"
PRELIMINARIES,0.17109634551495018,"label-flipping is (Agarwal et al., 2021), where the adversary cannot observe the action selected by the
182"
PRELIMINARIES,0.1719269102990033,"agent. In contrast, our setting focuses on scenarios where this information is available to adversaries,
183"
PRELIMINARIES,0.17275747508305647,"which is common in many real-life applications.
184"
PRELIMINARIES,0.17358803986710963,"As the context is changing, the optimal action is different in each round, denoted by a∗
t =
185"
PRELIMINARIES,0.1744186046511628,"argmaxa∈A r∗(xt, a). The goal of our algorithm is to minimize the cumulative gap between the
186"
PRELIMINARIES,0.17524916943521596,"rewards of both selected actions and the optimal action
187"
PRELIMINARIES,0.1760797342192691,"Regret(T) = PT
t=12r∗(xt, a∗
t ) −r∗(xt, at) −r∗(xt, bt).
(3.2)"
PRELIMINARIES,0.17691029900332225,"1Such adversary is referred to as strong adversary (He et al., 2022), compared with the weak adversary who
cannot obtain the information before the decision."
PRELIMINARIES,0.1777408637873754,"This regret definition is the same as that in Saha (2021) and the average regret defined in Bengs et al.
188"
PRELIMINARIES,0.17857142857142858,"(2022). It is typically stronger than weak regret defined in Bengs et al. (2022), which only considers
189"
PRELIMINARIES,0.17940199335548174,"the reward gap of the better action.
190"
ALGORITHM,0.18023255813953487,"4
Algorithm
191"
ALGORITHM,0.18106312292358803,"In this section, we present our new algorithm RCDB, designed for learning contextual linear dueling
192"
ALGORITHM,0.1818936877076412,"bandits. The main algorithm is illustrated in Algorithm 1. At a high level, we incorporate uncertainty-
193"
ALGORITHM,0.18272425249169436,"dependent weighting into the Maximum Likelihood Estimator (MLE) to counter adversarial feedback.
194"
ALGORITHM,0.18355481727574752,"Specifically, in each round t ∈[T], we construct the estimator of parameter θ by solving the following
195"
ALGORITHM,0.18438538205980065,"equation:
196"
ALGORITHM,0.1852159468438538,"λκθ + Pt−1
i=1wi
 
σ(ϕ⊤
i θ) −oi

ϕi = 0,
(4.1)"
ALGORITHM,0.18604651162790697,"where we denote ϕi = ϕ(xi, ai) −ϕ(xi, bi) for simplicity, wi is the uncertainty weight we are going
197"
ALGORITHM,0.18687707641196014,"to choose. To obtain an intuitive understanding of our weight, we consider any action-observation
198"
ALGORITHM,0.1877076411960133,"sequence (x1, a1, b1, o1, x2, a2, b2, o2, . . . , xt, at, bt, ot) up to round t. For simplicity, we denote
199"
ALGORITHM,0.18853820598006646,"Ft = σ(x1, a1, b1, o1, x2, a2, b2, o2, . . . , xt, at, bt) as the filtration. Suppose the estimated parameter
200"
ALGORITHM,0.1893687707641196,"θt is the solution to the unweighted version equation of (4.1), i.e.,
201"
ALGORITHM,0.19019933554817275,"λκθt + Pt
i=1
 
σ(ϕ⊤
i θt) −oi

ϕi = 0.
(4.2)"
ALGORITHM,0.19102990033222592,"When we receive ϕt = ϕ(xt, at) −ϕ(xt, bt), the probability of receiving lt = 1 can be estimated
202"
ALGORITHM,0.19186046511627908,"by σ(ϕ⊤
t θt). We consider the conditional variance of the estimated probability σ(ϕ⊤
t θt) in round t,
203"
ALGORITHM,0.19269102990033224,"i.e.,Var

σ(ϕ⊤
t θt)|Ft

, involving a posterior estimate of the prediction’s variance. First, we have
204"
ALGORITHM,0.19352159468438537,"E

σ(ϕ⊤
t θt)|Ft

≈E

σ(ϕ⊤
t θ∗) + σ′(ϕ⊤
t θ∗)ϕ⊤
t (θt −θ∗)|Ft
"
ALGORITHM,0.19435215946843853,"= E

σ(ϕ⊤
t θ∗) −σ′(ϕ⊤
t θ∗)ϕ⊤
t θ∗
|
{z
}
Ft−measurable"
ALGORITHM,0.1951827242524917,"|Ft

+ E

σ′(ϕ⊤
t θ∗)ϕ⊤
t θt|Ft

."
ALGORITHM,0.19601328903654486,"Moreover, using the Taylor’s expansion to (4.2), we have
205"
ALGORITHM,0.19684385382059802,"0 = λκθt + Pt
i=1
 
σ(ϕ⊤
i θt) −oi

ϕi"
ALGORITHM,0.19767441860465115,"≈

λκI + Pt
i=1σ′(ϕ⊤
i θ∗)ϕiϕ⊤
i

θt + Pt
i=1
 
σ(ϕ⊤
i θ∗) −oi

ϕi −Pt
i=1 σ′(ϕ⊤
i θ∗)ϕiϕ⊤
i θ∗."
ALGORITHM,0.19850498338870431,"Let Λt = λκI + Pt
i=1 σ′(ϕ⊤
i θ∗)ϕiϕ⊤
i , we have
206"
ALGORITHM,0.19933554817275748,"θt ≈Λ−1
t
hPt
i=1σ′(ϕ⊤
i θ∗)ϕiϕ⊤
i θ∗−Pt
i=1
 
σ(ϕ⊤
i θ∗) −oi

ϕi
i"
ALGORITHM,0.20016611295681064,"= Λ−1
t
hPt
i=1σ′(ϕ⊤
i θ∗)ϕiϕ⊤
i θ∗−Pt−1
i=1
 
σ(ϕ⊤
i θ∗) −oi

ϕi −σ(ϕ⊤
t θ∗)
i"
ALGORITHM,0.2009966777408638,"|
{z
}
Ft−measurable"
ALGORITHM,0.20182724252491693,"+otΛ−1
t ϕt"
ALGORITHM,0.2026578073089701,"Therefore, the variance of the estimated preference probability can be approximated by
207"
ALGORITHM,0.20348837209302326,"Var

σ(ϕ⊤
t θt)|Ft

= E
 
σ(ϕ⊤
t θt) −E

σ(ϕ⊤
t θt)|Ft
2|Ft
"
ALGORITHM,0.20431893687707642,"≈E
h
E

otσ′(ϕ⊤
t θ∗)ϕ⊤
t Λ−1
t ϕt|Ft
2Ft
i"
ALGORITHM,0.20514950166112958,"≤E

ot[σ′(ϕ⊤
t θ∗)]2∥ϕt∥2
Λ−1
t |Ft

≤[σ′(ϕ⊤
t θ∗)]2∥ϕt∥2
Λ−1
t ,"
ALGORITHM,0.2059800664451827,"where the first inequality holds due to the Jensen’s inequality and o2
t = ot, and the last inequality
208"
ALGORITHM,0.20681063122923588,"holds due to E[ot|Ft] ≤1. Using σ′(ϕ⊤
t θ∗) ≤1, ϕ⊤
t θ∗≤1, Λt ≥κΣt+1 ≥κΣt, we can see that
209"
ALGORITHM,0.20764119601328904,"Var

σ(ϕ⊤
t θt)|Ft

≤κ−1∥ϕt∥2
Σ−1
t . Since higher variance leads to larger uncertainty, which harms
210"
ALGORITHM,0.2084717607973422,"the credibility of the data, it is natural to assign a smaller weight to the data with high uncertainty.
211"
ALGORITHM,0.20930232558139536,"Thus, we choose the weight to cancel out the uncertainty as follows
212"
ALGORITHM,0.2101328903654485,"wi = min{1, α/∥ϕi∥Σ−1
i },
(4.3)"
ALGORITHM,0.21096345514950166,"where α/∥ϕi∥Σ−1
i
normalizes the variance of the estimated probability. To prevent excessively
213"
ALGORITHM,0.21179401993355482,"large weights, we apply truncation to this value. A similar weight has been used in He et al. (2022)
214"
ALGORITHM,0.21262458471760798,"for linear contextual bandits under corruption. Different from their setting where the weight is an
215"
ALGORITHM,0.21345514950166114,"estimate of the variance of the linear model, our weight is an estimate of a generalized linear model.
216"
ALGORITHM,0.21428571428571427,"Furthermore, by selecting a proper threshold parameter, e.g., α =
√"
ALGORITHM,0.21511627906976744,"d/C, the weighted MLE shares
217"
ALGORITHM,0.2159468438538206,"the same confidence radius with that of the no-adversary scenario.
218"
ALGORITHM,0.21677740863787376,"After constructing the estimator θt from the weighted MLE, the sum of the estimated reward for
219"
ALGORITHM,0.21760797342192692,"each duel (a, b) can be calculated as
 
ϕ(xt, a) + ϕ(xt, b)
⊤θt. To encourage the exploration of duel
220"
ALGORITHM,0.21843853820598005,"(a, b) with high uncertainty during the learning process, we introduce an exploration bonus with the
221"
ALGORITHM,0.21926910299003322,"following β
ϕ(xt, a) −ϕ(xt, b)

Σ−1
t , which follows a similar spirit to the bonus term in the context
222"
ALGORITHM,0.22009966777408638,"of linear bandit problems (Abbasi-Yadkori et al., 2011). However, the reward term and the bonus term
223"
ALGORITHM,0.22093023255813954,"exhibit different combinations of the feature maps ϕ(xt, a) and ϕ(xt, b), which is the key difference
224"
ALGORITHM,0.2217607973421927,"between bandits and dueling bandits. The selection of action pairs (a, b) is subsequently determined
225"
ALGORITHM,0.22259136212624583,"by maximizing the estimated reward with the exploration bonus term, i.e.,
226"
ALGORITHM,0.223421926910299," 
ϕ(xt, a) + ϕ(xt, b)
⊤θt + β
ϕ(xt, a) −ϕ(xt, b)

Σ−1
t ."
ALGORITHM,0.22425249169435216,"More discussion about the selection rule was discussed in Appendix A of Di et al. (2023).
227"
ALGORITHM,0.22508305647840532,Algorithm 1 Robust Contextual Dueling Bandit (RCDB)
ALGORITHM,0.22591362126245848,"1: Require: α > 0, Regularization parameter λ, confidence radius β.
2: for t = 1, . . . , T do"
ALGORITHM,0.22674418604651161,"3:
Compute Σt = λI + Pt−1
i=1wi
 
ϕ(xi, ai) −ϕ(xi, bi)
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤.
4:
Calculate the MLE θt by solving the following equation:"
ALGORITHM,0.22757475083056478,"λκθ + Pt−1
i=1wi
h
σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ

−oi
i 
ϕ(xi, ai) −ϕ(xi, bi)

= 0.
(4.4)"
ALGORITHM,0.22840531561461794,"5:
Observe the context vector xt."
ALGORITHM,0.2292358803986711,"6:
Choose at, bt = argmaxa,b
n 
ϕ(xt, a) + ϕ(xt, b)
⊤θt + β
ϕ(xt, a) −ϕ(xt, b)

Σ−1
t o
."
ALGORITHM,0.23006644518272426,"7:
The adversary sees the feedback lt = 1(at ≻bt) and decides the indicator ct. Observe ot = lt
when ct = 0, otherwise observe ot = 1 −lt.
8:
Set weight wt as (4.3).
9: end for"
MAIN RESULTS,0.23089700996677742,"5
Main Results
228"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23172757475083056,"5.1
Known Number of Adversarial Feedback
229"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23255813953488372,"At the center of our algorithm design is the uncertainty-weighted MLE. When faced with adversarial
230"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23338870431893688,"feedback, the estimation error of the weighted MLE θt can be characterized by the following lemma.
231"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23421926910299004,"Lemma 5.1. If we set β =
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2350498338870432,"λB +
 
αC +
p"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23588039867109634,"d log((1 + 2T/λ)/δ)

/κ, then with probability at
232"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2367109634551495,"least 1 −δ, for any t ∈[T], we have
233
θt −θ∗
Σt ≤β."
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23754152823920266,"Remark 5.2. If we set α = (
√ d +
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23837209302325582,"λB)/C, then the bonus radius β has no direct dependency on
234"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.23920265780730898,"the number of adversarial feedback C. This observation plays a key role in proving the adversarial
235"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24003322259136212,"term in the regret without polynomial dependence on the total number of rounds T.
236"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24086378737541528,"With Lemma 5.1, we can present the following regret guarantee of our algorithm RCDB in the dueling
237"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24169435215946844,"bandit framework.
238"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2425249169435216,"Theorem 5.3. Under Assumption 3.1 and 3.2, let 0 < δ < 1, the total number of adversarial feedback
239"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24335548172757476,"be C. If we set the bonus radius to be
240 β =
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2441860465116279,"λB +
 
αC +
p"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24501661129568106,"d log((1 + 2T/λ)/δ)

/κ,
then with probability at least 1 −δ, the regret in the first t rounds can be upper bounded by
241"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24584717607973422,"Regret(T) ≤4
 √"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24667774086378738,"λB + αC/κ
p"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24750830564784054,dT log(1 + 2T/λ)
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24833887043189368,"+ 4d
 √"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.24916943521594684,"T/κ +
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.25,"λB/α + 4C/κ

log
 
(1 + 2T/λ)/δ
"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.25083056478405313,"+ 4d1.5
q"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2516611295681063,"log3  
(1 + 2T/λ)/δ

/(ακ)."
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.25249169435215946,"Moreover, if we set α = (
√ d +
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.25332225913621265,"λB)/C, λ = 1/B2, the regret upper bound can be simplified to
242"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2541528239202658,"Regret(T) = eO
 
d
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2549833887043189,"T/κ + dC/κ

."
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2558139534883721,"Remark 5.4. Our regret bound consists of two terms. The first one is a C-independent term eO(d
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.25664451827242524,"T),
243"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2574750830564784,"which matches the lower bound eΩ(d
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.25830564784053156,"T) proved in Bengs et al. (2022). This indicates that our result
244"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2591362126245847,"is optimal in scenarios without adversarial feedback (C = 0). Additionally, our result includes an
245"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2599667774086379,"additive term that is linearly dependent on the number of adversarial feedback C. When C = O(
√"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.260797342192691,"T),
246"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2616279069767442,"the order of regret will be the same as the stochastic setting. It indicates the robustness of our algorithm
247"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.26245847176079734,"to adversarial feedback. Additionally, the following theorem we present establishes a lower bound
248"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2632890365448505,"for this adversarial term, indicating that our dependency on the number of adversarial feedback C
249"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.26411960132890366,"and the context dimension d is also optimal.
250"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2649501661129568,"Theorem 5.5. For any dimension d, there exists an instance of dueling bandits with |A| = d, such
251"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.26578073089701,"that any algorithm with the knowledge of the number of adversarial feedback C must incur Ω(dC)
252"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2666112956810631,"regret with probability at least 1/2.
253"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.26744186046511625,"Remark 5.6. The proof of Theorem 5.5 follows Bogunovic et al. (2021). In the constructed instances,
254"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.26827242524916944,"only one action has reward 1, while others have 0. Compared with linear bandits, where the feedback
255"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2691029900332226,"is an exact reward, dueling bandits deal with the comparison between a pair of actions. A critical
256"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.26993355481727577,"observation from our preference model, as formulated in (3.1), is that two actions with identical
257"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2707641196013289,"rewards result in a pair that is challenging to differentiate. The lower bound can be proved by
258"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.27159468438538203,"corrupting every comparison into a random guess until the total times of adversarial feedback have
259"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2724252491694352,"been used up. For detailed proof, please refer to Section B.2. Our proved lower bound Ω(dC) shows
260"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.27325581395348836,"that our result is nearly optimal because of the linear dependency on C, d and only logarithmic
261"
KNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.27408637873754155,"dependency on the total number of rounds T.
262"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2749169435215947,"5.2
Unknown Number of Adversarial Feedback
263"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2757475083056478,"In our previous analysis, the selection of parameters depends on having prior knowledge of the total
264"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.276578073089701,"number of adversarial feedback C. In this subsection, we extend our previous result to address
265"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.27740863787375414,"the challenge posed by an unknown number of adversarial feedback C. Our approach to tackle
266"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2782392026578073,"this uncertainty follows He et al. (2022), we introduce an adversarial tolerance threshold ¯C for the
267"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.27906976744186046,"adversary count. This threshold can be regarded as an optimistic estimator of the actual number of
268"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2799003322259136,"adversarial feedback C. Under this situation, the subsequent theorem provides an upper bound for
269"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2807308970099668,"regret of Algorithm 1 in the case of an unknown number of adversarial feedback C.
270"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2815614617940199,"Theorem 5.7. Under Assumptions 3.1 and 3.2, if we set the the confidence radius as
271 β =
√"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2823920265780731,"λB +

α ¯C +
q"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.28322259136212624,"d log
 
(1 + 2T/λ)/δ

/κ,"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2840531561461794,"with the pre-defined adversarial tolerance threshold ¯C and α = (
√ d+
√"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.28488372093023256,"λB)/ ¯C, then with probability
272"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2857142857142857,"at least 1 −δ, the regret of Algorithm 1 can be upper bounded as following:
273"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2865448504983389,"• If the actual number of adversarial feedback C is smaller than the adversarial tolerance threshold
274"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.287375415282392,"¯C, then we have
275"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.28820598006644516,"Regret(T) = eO
 
d
√"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.28903654485049834,"T/κ + d ¯C/κ

."
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2898671096345515,"• If the actual number of adversarial feedback C is larger than the adversarial tolerance threshold ¯C,
276"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.29069767441860467,"then we have Regret(T) = O(T).
277"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2915282392026578,"Remark 5.8. The COBE framework (Wei et al., 2022) converts any algorithm with the known
278"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.292358803986711,"adversarial level to an algorithm in the unknown case. However, such a framework only works for
279"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2931893687707641,"weak adversaries and does not work in our strong adversary setting. In fact, He et al. (2022) proved
280"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.29401993355481726,"that any algorithm cannot simultaneously achieve near-optimal regret when uncorrupted and maintain
281"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.29485049833887045,"sublinear regret with corruption level C = Ω(
√"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2956810631229236,"T). Therefore, there exists a trade-off between robust
282"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.29651162790697677,"adversarial defense and near-optimal algorithmic performance. Our algorithm achieves the same
283"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.2973421926910299,"nearly optimal eO(d
√"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.29817275747508304,"T) regret as the no-adversary case even when C = Θ(
√"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.29900332225913623,"T), which indicates
284"
UNKNOWN NUMBER OF ADVERSARIAL FEEDBACK,0.29983388704318936,"that our results are optimal in the presence of an unknown number of adversarial feedback.
285"
EXPERIMENTS,0.30066445182724255,"6
Experiments
286"
EXPERIMENT SETUP,0.3014950166112957,"6.1
Experiment Setup
287"
EXPERIMENT SETUP,0.3023255813953488,"Preference Model.
We study the effect of adversarial feedback with the preference model deter-
288"
EXPERIMENT SETUP,0.303156146179402,"mined by (3.1), where σ(x) = 1/(1 + e−x). We randomly generate the underlying parameter in
289"
EXPERIMENT SETUP,0.30398671096345514,"[−0.5, 0.5]d and normalize it to be a vector with ∥θ∗∥2 = 2. Then, we set it to be the underlying
290"
EXPERIMENT SETUP,0.30481727574750833,"parameter and construct the reward utilized in the preference model as r∗(x, a) = ⟨θ∗, ϕ(x, a)⟩.
291"
EXPERIMENT SETUP,0.30564784053156147,"We set the action set A =

−1/
√"
EXPERIMENT SETUP,0.3064784053156146,"d, 1/
√"
EXPERIMENT SETUP,0.3073089700996678,"d
	d. For simplicity, we assume ϕ(x, a) = a. In our
292"
EXPERIMENT SETUP,0.3081395348837209,"experiment, we set the dimension d = 5, with the size of action set |A| = 2d = 32.
293"
EXPERIMENT SETUP,0.3089700996677741,"Adversarial Attack Methods.
We study the performance of our algorithm using different adversar-
294"
EXPERIMENT SETUP,0.30980066445182725,"ial attack methods. We categorize the first two methods as “weak” primarily because the adversary in
295"
EXPERIMENT SETUP,0.3106312292358804,"these scenarios does not utilize information about the agent’s actions. In contrast, we classify the
296"
EXPERIMENT SETUP,0.31146179401993357,"latter two methods as “strong” attacks. In these cases, the adversary leverages a broader scope of
297"
EXPERIMENT SETUP,0.3122923588039867,"information, including knowledge of the actions selected by the agent and the true preference model.
298"
EXPERIMENT SETUP,0.3131229235880399,"This enables it to devise more targeted adversarial methods.
299"
EXPERIMENT SETUP,0.313953488372093,"• “Greedy Attack"": The adversary will flip the preference label for the first C rounds. After that, it
300"
EXPERIMENT SETUP,0.31478405315614616,"will not corrupt the result anymore.
301"
EXPERIMENT SETUP,0.31561461794019935,"• “Random Attack"": In each round, the adversary will flip the preference label with the probability of
302"
EXPERIMENT SETUP,0.3164451827242525,"0 < p < 1, until the times of adversarial feedback reach C.
303"
EXPERIMENT SETUP,0.31727574750830567,"• “Adversarial Attack"": The adversary can have access to the true preference model. It will only flip
304"
EXPERIMENT SETUP,0.3181063122923588,"the preference label when it aligns with the preference model, i.e., the probability for the preference
305"
EXPERIMENT SETUP,0.31893687707641194,"model to make that decision is larger than 0.5, until the times of adversarial feedback reach C.
306"
EXPERIMENT SETUP,0.31976744186046513,"• “Misleading Attack"": The adversary selects a suboptimal action. It will make sure this arm is
307"
EXPERIMENT SETUP,0.32059800664451826,"always the winner in the comparison until the times of adversarial feedback reach C. In this way, it
308"
EXPERIMENT SETUP,0.32142857142857145,"will mislead the agent to believe this action is the optimal one.
309"
EXPERIMENT SETUP,0.3222591362126246,"Experiment Setup.
For each experiment instance, we simulate the interaction with the environment
310"
EXPERIMENT SETUP,0.3230897009966777,"for T = 2000 rounds. In each round, the feedback for the action pair selected by the algorithm is
311"
EXPERIMENT SETUP,0.3239202657807309,"generated according to the defined preference model. Subsequently, the adversary observes both the
312"
EXPERIMENT SETUP,0.32475083056478404,"selected actions and their corresponding feedback and then engages in one of the previously described
313"
EXPERIMENT SETUP,0.32558139534883723,adversarial attack methods. We report the regret defined in (3.2) averaged across 10 random runs.
EXPERIMENT SETUP,0.32641196013289037,"0
250
500
750
1000
1250
1500
1750
2000
Epoch 0 250 500 750 1000 1250 1500 1750"
EXPERIMENT SETUP,0.3272425249169435,Regret
EXPERIMENT SETUP,0.3280730897009967,Greedy Attack
EXPERIMENT SETUP,0.3289036544850498,"MaxInp
CoLSTIM
MaxPairUCB
RCDB"
EXPERIMENT SETUP,0.329734219269103,(a) Greedy Attack
EXPERIMENT SETUP,0.33056478405315615,"0
250
500
750
1000
1250
1500
1750
2000
Epoch 0 250 500 750 1000 1250 1500 1750"
EXPERIMENT SETUP,0.3313953488372093,Regret
EXPERIMENT SETUP,0.33222591362126247,Random Attack
EXPERIMENT SETUP,0.3330564784053156,"MaxInp
CoLSTIM
MaxPairUCB
RCDB"
EXPERIMENT SETUP,0.3338870431893688,(b) Random Attack
EXPERIMENT SETUP,0.3347176079734219,"0
250
500
750
1000
1250
1500
1750
2000
Epoch 0 1000 2000 3000 4000 5000"
EXPERIMENT SETUP,0.33554817275747506,Regret
EXPERIMENT SETUP,0.33637873754152825,Adversarial Attack
EXPERIMENT SETUP,0.3372093023255814,"MaxInp
CoLSTIM
MaxPairUCB
RCDB"
EXPERIMENT SETUP,0.3380398671096346,(c) Adversarial Attack
EXPERIMENT SETUP,0.3388704318936877,"0
250
500
750
1000
1250
1500
1750
2000
Epoch 0 2000 4000 6000 8000"
EXPERIMENT SETUP,0.33970099667774084,Regret
EXPERIMENT SETUP,0.34053156146179403,Misleading Attack
EXPERIMENT SETUP,0.34136212624584716,"MaxInp
CoLSTIM
MaxPairUCB
RCDB"
EXPERIMENT SETUP,0.34219269102990035,"(d) Misleading Attack
Figure 1: Comparison of RCDB (Our Algorithm 1), MaxInp (Saha, 2021), CoLSTIM (Bengs et al.,
2022) and MaxPairUCB (Di et al., 2023). We report the cumulative regret with various adversarial
attack methods (Greedy, Random, Adversarial, Misleading). For the baselines, the parameters are
carefully tuned to achieve better results with different attack methods. The total number of adversarial
feedback is C = ⌈
√ T⌉. 314"
PERFORMANCE COMPARISON,0.3430232558139535,"6.2
Performance Comparison
315"
PERFORMANCE COMPARISON,0.3438538205980066,"We first introduce the algorithms studied in this section.
316"
PERFORMANCE COMPARISON,0.3446843853820598,"• MaxInP: Maximum Informative Pair by Saha (2021). It involves maintaining a standard MLE.
317"
PERFORMANCE COMPARISON,0.34551495016611294,"With the estimated model, it then identifies a set of promising arms possible to beat the rest. The
318"
PERFORMANCE COMPARISON,0.34634551495016613,"selection of arm pairs is then strategically designed to maximize the uncertainty in the difference
319"
PERFORMANCE COMPARISON,0.34717607973421927,"between the two arms within this promising set, referred to as “maximum informative”.
320"
PERFORMANCE COMPARISON,0.3480066445182724,"• CoLSTIM: The method by Bengs et al. (2022). It involves maintaining a standard MLE for the
321"
PERFORMANCE COMPARISON,0.3488372093023256,"estimated model. Based on this model, the first arm is selected as the one with the highest estimated
322"
PERFORMANCE COMPARISON,0.3496677740863787,"reward, implying it is the most likely to prevail over competitors. The second arm is selected to be
323"
PERFORMANCE COMPARISON,0.3504983388704319,"the first arm’s toughest competitor, with an added uncertainty bonus.
324"
PERFORMANCE COMPARISON,0.35132890365448505,"• MaxPairUCB: This algorithm was proposed in Di et al. (2023). It uses the regularized MLE to
325"
PERFORMANCE COMPARISON,0.3521594684385382,"estimate the parameter θ∗. Then it selects the actions based on a symmetric action selection rule,
326"
PERFORMANCE COMPARISON,0.35299003322259137,"i.e. the actions with the largest estimated reward plus some uncertainty bonus.
327"
PERFORMANCE COMPARISON,0.3538205980066445,"• RCDB: Algorithm 1 proposed in this paper. The key difference from the other algorithms is the
328"
PERFORMANCE COMPARISON,0.3546511627906977,"use of uncertainty weight in the calculation of MLE (4.4). The we use the same symmetric action
329"
PERFORMANCE COMPARISON,0.3554817275747508,"selection rule as MaxPairUCB. Our experiment results show that the uncertainty weight is critical
330"
PERFORMANCE COMPARISON,0.35631229235880396,"in the face of adversarial feedback.
331"
PERFORMANCE COMPARISON,0.35714285714285715,"Our results are demonstrated in Figure 1. In Figure 1(a) and Figure 1(b), we observe scenarios where
332"
PERFORMANCE COMPARISON,0.3579734219269103,"the adversary is “weak” due to the lack of access to information regarding the selected actions and the
333"
PERFORMANCE COMPARISON,0.3588039867109635,"underlying preference model. Notably, in these situations, our algorithm RCDB outperforms all other
334"
PERFORMANCE COMPARISON,0.3596345514950166,"baseline algorithms, demonstrating its robustness. Among the other algorithms, CoLSTIM performs
335"
PERFORMANCE COMPARISON,0.36046511627906974,"as the strongest competitor.
336"
PERFORMANCE COMPARISON,0.36129568106312293,"In Figure 1(c), the adversary employs a ’stronger’ adversarial method. Due to the inherent randomness
337"
PERFORMANCE COMPARISON,0.36212624584717606,"of the model, some labels may naturally be ’incorrect’. An adversary with knowledge of the selected
338"
PERFORMANCE COMPARISON,0.36295681063122925,"actions and the preference model can strategically neglect these naturally incorrect labels and
339"
PERFORMANCE COMPARISON,0.3637873754152824,"selectively flip the others. This method proves catastrophic for algorithms to learn the true model,
340"
PERFORMANCE COMPARISON,0.3646179401993355,"as it results in the agent encountering only incorrect preference labels at the beginning. Our results
341"
PERFORMANCE COMPARISON,0.3654485049833887,"indicate that this leads to significantly higher regret. However, it’s noteworthy that our algorithm
342"
PERFORMANCE COMPARISON,0.36627906976744184,"RCDB demonstrates considerable robustness.
343"
PERFORMANCE COMPARISON,0.36710963455149503,"In Figure 1(d), the adversary employs a strategy aimed at misleading algorithms into believing a
344"
PERFORMANCE COMPARISON,0.36794019933554817,"suboptimal action is the best choice. The algorithm CoLSTIM appears to be the most susceptible to
345"
PERFORMANCE COMPARISON,0.3687707641196013,"being cheated by this method. Despite the deployment of ’strong’ adversarial methods, as shown
346"
PERFORMANCE COMPARISON,0.3696013289036545,"in both Figure 1(c) and Figure 1(d), our algorithm, RCDB, consistently demonstrates exceptional
347"
PERFORMANCE COMPARISON,0.3704318936877076,"robustness against these attacks. A significant advantage of RCDB lies in that our parameter is selected
348"
PERFORMANCE COMPARISON,0.3712624584717608,"solely based on the number of adversarial feedback C, irrespective of the nature of the adversarial
349"
PERFORMANCE COMPARISON,0.37209302325581395,"methods employed. This contrasts with other algorithms where parameter tuning must be specifically
350"
PERFORMANCE COMPARISON,0.3729235880398671,adapted for each distinct adversarial method.
PERFORMANCE COMPARISON,0.37375415282392027,"25
50
75
100
125
150
175
200
Adversarial Feedback 500 1000 1500 2000 2500 3000 3500"
PERFORMANCE COMPARISON,0.3745847176079734,Regret
PERFORMANCE COMPARISON,0.3754152823920266,Cumulative Regret versus Adversarial Feedback
PERFORMANCE COMPARISON,0.37624584717607973,"MaxInp
CoLSTIM
MaxPairUCB
RCDB"
PERFORMANCE COMPARISON,0.3770764119601329,"Figure 2: The relationship between cumulative regret and the number of adversarial feedback C. For
this specific experiment, we employ the ""greedy attack"" method to generate the adversarial feedback.
C is selected from the set [20, 40, 60, 80, 100, 120, 140, 160, 180, 200] (10 adversarial levels). 351"
ROBUSTNESS TO DIFFERENT NUMBERS OF ADVERSARIAL FEEDBACK,0.37790697674418605,"6.3
Robustness to Different Numbers of Adversarial Feedback
352"
ROBUSTNESS TO DIFFERENT NUMBERS OF ADVERSARIAL FEEDBACK,0.3787375415282392,"In this section, we test the performance of algorithms with increasing times of adversarial feedback.
353"
ROBUSTNESS TO DIFFERENT NUMBERS OF ADVERSARIAL FEEDBACK,0.3795681063122924,"Our results show a linear dependency on the number of adversarial feedback C, which is consistent
354"
ROBUSTNESS TO DIFFERENT NUMBERS OF ADVERSARIAL FEEDBACK,0.3803986710963455,"with the theoretical results we have proved in Theorem 5.3 and 5.5. In comparison to other algorithms,
355"
ROBUSTNESS TO DIFFERENT NUMBERS OF ADVERSARIAL FEEDBACK,0.3812292358803987,"RCDB demonstrates superior robustness against adversarial feedback, as evidenced by its notably
356"
ROBUSTNESS TO DIFFERENT NUMBERS OF ADVERSARIAL FEEDBACK,0.38205980066445183,"smaller regret.
357"
CONCLUSION,0.38289036544850497,"7
Conclusion
358"
CONCLUSION,0.38372093023255816,"In this paper, we focus on the contextual dueling bandit problem from adversarial feedback. We
359"
CONCLUSION,0.3845514950166113,"introduce a novel algorithm, RCDB, which utilizes an uncertainty-weighted Maximum Likelihood
360"
CONCLUSION,0.3853820598006645,"Estimator (MLE) approach. This algorithm not only achieves optimal theoretical results in scenarios
361"
CONCLUSION,0.3862126245847176,"with and without adversarial feedback but also demonstrates superior performance with synthetic
362"
CONCLUSION,0.38704318936877075,"data. For future direction, we aim to extend our uncertainty-weighted method to encompass more
363"
CONCLUSION,0.38787375415282394,"general settings involving preference-based data. A particularly promising future direction of our
364"
CONCLUSION,0.38870431893687707,"research lies in addressing adversarial feedback within the process of aligning large language models
365"
CONCLUSION,0.38953488372093026,"using Reinforcement Learning from Human Feedback (RLHF).
366"
CONCLUSION,0.3903654485049834,"Limitations. We assume that the reward is linear with respect to some known feature maps. Although
367"
CONCLUSION,0.3911960132890365,"this setting is common in the literature, we observe that some recent works on dueling bandits can
368"
CONCLUSION,0.3920265780730897,"deal with nonlinear rewards (Li et al., 2024). Therefore, it’s possible to extend our results to a more
369"
CONCLUSION,0.39285714285714285,"general setting. Another assumption concerns the lower bound of the derivative of the link function.
370"
CONCLUSION,0.39368770764119604,"Notably, in the logistic bandit model, which shares similarities with our setting through Bernoulli
371"
CONCLUSION,0.3945182724252492,"variables, some work (Abeille et al., 2021; Faury et al., 2022) can improve the dependency of κ from
372"
CONCLUSION,0.3953488372093023,"1/κ to √κ. A similar improvement might be achieved in our setting as well.
373"
REFERENCES,0.3961794019933555,"References
374"
REFERENCES,0.39700996677740863,"ABBASI-YADKORI, Y., PÁL, D. and SZEPESVÁRI, C. (2011). Improved algorithms for linear
375"
REFERENCES,0.3978405315614618,"stochastic bandits. In Advances in Neural Information Processing Systems.
376"
REFERENCES,0.39867109634551495,"ABEILLE, M., FAURY, L. and CALAUZÈNES, C. (2021). Instance-wise minimax-optimal algorithms
377"
REFERENCES,0.3995016611295681,"for logistic bandits. In International Conference on Artificial Intelligence and Statistics. PMLR.
378"
REFERENCES,0.4003322259136213,"AGARWAL, A., AGARWAL, S. and PATIL, P. (2021). Stochastic dueling bandits with adversarial
379"
REFERENCES,0.4011627906976744,"corruption. In Algorithmic Learning Theory. PMLR.
380"
REFERENCES,0.4019933554817276,"AGRAWAL, S. and GOYAL, N. (2012). Analysis of thompson sampling for the multi-armed bandit
381"
REFERENCES,0.40282392026578073,"problem. In Conference on learning theory. JMLR Workshop and Conference Proceedings.
382"
REFERENCES,0.40365448504983387,"AILON, N., KARNIN, Z. and JOACHIMS, T. (2014). Reducing dueling bandits to cardinal bandits.
383"
REFERENCES,0.40448504983388706,"In International Conference on Machine Learning. PMLR.
384"
REFERENCES,0.4053156146179402,"AUER, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. Journal of
385"
REFERENCES,0.4061461794019934,"Machine Learning Research 3 397–422.
386"
REFERENCES,0.4069767441860465,"AUER, P., CESA-BIANCHI, N. and FISCHER, P. (2002a). Finite-time analysis of the multiarmed
387"
REFERENCES,0.40780730897009965,"bandit problem. Machine Learning 47 235–256.
388"
REFERENCES,0.40863787375415284,"AUER, P., CESA-BIANCHI, N., FREUND, Y. and SCHAPIRE, R. E. (2002b). The nonstochastic
389"
REFERENCES,0.40946843853820597,"multiarmed bandit problem. SIAM journal on computing 32 48–77.
390"
REFERENCES,0.41029900332225916,"AUER, P. and CHIANG, C.-K. (2016). An algorithm with nearly optimal pseudo-regret for both
391"
REFERENCES,0.4111295681063123,"stochastic and adversarial bandits. In Conference on Learning Theory. PMLR.
392"
REFERENCES,0.4119601328903654,"BALSUBRAMANI, A., KARNIN, Z., SCHAPIRE, R. E. and ZOGHI, M. (2016). Instance-dependent
393"
REFERENCES,0.4127906976744186,"regret bounds for dueling bandits. In Conference on Learning Theory. PMLR.
394"
REFERENCES,0.41362126245847175,"BENGS, V., SAHA, A. and HÜLLERMEIER, E. (2022). Stochastic contextual dueling bandits under
395"
REFERENCES,0.41445182724252494,"linear stochastic transitivity models. In International Conference on Machine Learning. PMLR.
396"
REFERENCES,0.4152823920265781,"BOGUNOVIC, I., LOSALKA, A., KRAUSE, A. and SCARLETT, J. (2021). Stochastic linear bandits
397"
REFERENCES,0.4161129568106312,"robust to adversarial attacks. In International Conference on Artificial Intelligence and Statistics.
398"
REFERENCES,0.4169435215946844,"PMLR.
399"
REFERENCES,0.41777408637873753,"BUBECK, S., CESA-BIANCHI, N. ET AL. (2012). Regret analysis of stochastic and nonstochastic
400"
REFERENCES,0.4186046511627907,"multi-armed bandit problems. Foundations and Trends® in Machine Learning 5 1–122.
401"
REFERENCES,0.41943521594684385,"BUBECK, S. and SLIVKINS, A. (2012). The best of both worlds: Stochastic and adversarial bandits.
402"
REFERENCES,0.420265780730897,"In Conference on Learning Theory. JMLR Workshop and Conference Proceedings.
403"
REFERENCES,0.4210963455149502,"CESA-BIANCHI, N. and LUGOSI, G. (2006). Prediction, learning, and games. Cambridge university
404"
REFERENCES,0.4219269102990033,"press.
405"
REFERENCES,0.4227574750830565,"DI, Q., JIN, T., WU, Y., ZHAO, H., FARNOUD, F. and GU, Q. (2023). Variance-aware regret bounds
406"
REFERENCES,0.42358803986710963,"for stochastic contextual dueling bandits. arXiv preprint arXiv:2310.00968 .
407"
REFERENCES,0.42441860465116277,"DING, Q., HSIEH, C.-J. and SHARPNACK, J. (2022). Robust stochastic linear contextual bandits
408"
REFERENCES,0.42524916943521596,"under adversarial attacks. In International Conference on Artificial Intelligence and Statistics.
409"
REFERENCES,0.4260797342192691,"PMLR.
410"
REFERENCES,0.4269102990033223,"DUDÍK, M., HOFMANN, K., SCHAPIRE, R. E., SLIVKINS, A. and ZOGHI, M. (2015). Contextual
411"
REFERENCES,0.4277408637873754,"dueling bandits. In Conference on Learning Theory. PMLR.
412"
REFERENCES,0.42857142857142855,"FALAHATGAR, M., HAO, Y., ORLITSKY, A., PICHAPATI, V. and RAVINDRAKUMAR, V. (2017).
413"
REFERENCES,0.42940199335548174,"Maxing and ranking with few assumptions. Advances in Neural Information Processing Systems
414"
REFERENCES,0.43023255813953487,"30.
415"
REFERENCES,0.43106312292358806,"FAURY, L., ABEILLE, M., CALAUZÈNES, C. and FERCOQ, O. (2020). Improved optimistic
416"
REFERENCES,0.4318936877076412,"algorithms for logistic bandits. In International Conference on Machine Learning. PMLR.
417"
REFERENCES,0.43272425249169433,"FAURY, L., ABEILLE, M., JUN, K.-S. and CALAUZÈNES, C. (2022). Jointly efficient and optimal
418"
REFERENCES,0.4335548172757475,"algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics.
419"
REFERENCES,0.43438538205980065,"PMLR.
420"
REFERENCES,0.43521594684385384,"FILIPPI, S., CAPPE, O., GARIVIER, A. and SZEPESVÁRI, C. (2010). Parametric bandits: The
421"
REFERENCES,0.436046511627907,"generalized linear case. Advances in Neural Information Processing Systems 23.
422"
REFERENCES,0.4368770764119601,"GAJANE, P., URVOY, T. and CLÉROT, F. (2015). A relative exponential weighing algorithm for
423"
REFERENCES,0.4377076411960133,"adversarial utility-based dueling bandits. In International Conference on Machine Learning.
424"
REFERENCES,0.43853820598006643,"PMLR.
425"
REFERENCES,0.4393687707641196,"GUPTA, A., KOREN, T. and TALWAR, K. (2019). Better algorithms for stochastic bandits with
426"
REFERENCES,0.44019933554817275,"adversarial corruptions. In Conference on Learning Theory. PMLR.
427"
REFERENCES,0.4410299003322259,"HE, J., ZHOU, D., ZHANG, T. and GU, Q. (2022). Nearly optimal algorithms for linear contextual
428"
REFERENCES,0.4418604651162791,"bandits with adversarial corruptions. Advances in Neural Information Processing Systems 35
429"
REFERENCES,0.4426910299003322,"34614–34625.
430"
REFERENCES,0.4435215946843854,"HECKEL, R., SIMCHOWITZ, M., RAMCHANDRAN, K. and WAINWRIGHT, M. (2018). Approximate
431"
REFERENCES,0.44435215946843853,"ranking from pairwise comparisons. In International Conference on Artificial Intelligence and
432"
REFERENCES,0.44518272425249167,"Statistics. PMLR.
433"
REFERENCES,0.44601328903654486,"JAMIESON, K., KATARIYA, S., DESHPANDE, A. and NOWAK, R. (2015). Sparse dueling bandits.
434"
REFERENCES,0.446843853820598,"In Artificial Intelligence and Statistics. PMLR.
435"
REFERENCES,0.4476744186046512,"KALYANAKRISHNAN, S., TEWARI, A., AUER, P. and STONE, P. (2012). Pac subset selection in
436"
REFERENCES,0.4485049833887043,"stochastic multi-armed bandits. In ICML, vol. 12.
437"
REFERENCES,0.44933554817275745,"KOMIYAMA, J., HONDA, J., KASHIMA, H. and NAKAGAWA, H. (2015). Regret lower bound and
438"
REFERENCES,0.45016611295681064,"optimal algorithm in dueling bandit problem. In Conference on learning theory. PMLR.
439"
REFERENCES,0.45099667774086377,"KOMIYAMA, J., HONDA, J. and NAKAGAWA, H. (2016). Copeland dueling bandit problem:
440"
REFERENCES,0.45182724252491696,"Regret lower bound, optimal algorithm, and computationally efficient algorithm. In International
441"
REFERENCES,0.4526578073089701,"Conference on Machine Learning. PMLR.
442"
REFERENCES,0.45348837209302323,"KUROKI, Y., RUMI, A., TSUCHIYA, T., VITALE, F. and CESA-BIANCHI, N. (2023). Best-of-both-
443"
REFERENCES,0.4543189368770764,"worlds algorithms for linear contextual bandits. arXiv preprint arXiv:2312.15433 .
444"
REFERENCES,0.45514950166112955,"LAI, T. L. (1987). Adaptive treatment allocation and the multi-armed bandit problem. The annals of
445"
REFERENCES,0.45598006644518274,"statistics 1091–1114.
446"
REFERENCES,0.4568106312292359,"LAI, T. L., ROBBINS, H. ET AL. (1985). Asymptotically efficient adaptive allocation rules. Advances
447"
REFERENCES,0.457641196013289,"in applied mathematics 6 4–22.
448"
REFERENCES,0.4584717607973422,"LATTIMORE, T. and SZEPESVÁRI, C. (2020). Bandit Algorithms. Cambridge University Press.
449"
REFERENCES,0.45930232558139533,"LEE, C.-W., LUO, H., WEI, C.-Y., ZHANG, M. and ZHANG, X. (2021). Achieving near instance-
450"
REFERENCES,0.4601328903654485,"optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In
451"
REFERENCES,0.46096345514950166,"International Conference on Machine Learning. PMLR.
452"
REFERENCES,0.46179401993355484,"LI, L., LU, Y. and ZHOU, D. (2017). Provably optimal algorithms for generalized linear contextual
453"
REFERENCES,0.462624584717608,"bandits. In International Conference on Machine Learning. PMLR.
454"
REFERENCES,0.4634551495016611,"LI, X., ZHAO, H. and GU, Q. (2024). Feel-good thompson sampling for contextual dueling bandits.
455"
REFERENCES,0.4642857142857143,"arXiv preprint arXiv:2404.06013 .
456"
REFERENCES,0.46511627906976744,"LI, Y., LOU, E. Y. and SHAN, L. (2019). Stochastic linear optimization with adversarial corruption.
457"
REFERENCES,0.4659468438538206,"arXiv preprint arXiv:1909.02109 .
458"
REFERENCES,0.46677740863787376,"LYKOURIS, T., MIRROKNI, V. and PAES LEME, R. (2018). Stochastic bandits robust to adversarial
459"
REFERENCES,0.4676079734219269,"corruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing.
460"
REFERENCES,0.4684385382059801,"OUYANG, L., WU, J., JIANG, X., ALMEIDA, D., WAINWRIGHT, C., MISHKIN, P., ZHANG,
461"
REFERENCES,0.4692691029900332,"C., AGARWAL, S., SLAMA, K., RAY, A. ET AL. (2022). Training language models to follow
462"
REFERENCES,0.4700996677740864,"instructions with human feedback. Advances in Neural Information Processing Systems 35 27730–
463"
REFERENCES,0.47093023255813954,"27744.
464"
REFERENCES,0.4717607973421927,"RAMAMOHAN, S. Y., RAJKUMAR, A. and AGARWAL, S. (2016). Dueling bandits: Beyond
465"
REFERENCES,0.47259136212624586,"condorcet winners to general tournament solutions. Advances in Neural Information Processing
466"
REFERENCES,0.473421926910299,"Systems 29.
467"
REFERENCES,0.4742524916943522,"SAHA, A. (2021). Optimal algorithms for stochastic contextual preference bandits. Advances in
468"
REFERENCES,0.4750830564784053,"Neural Information Processing Systems 34 30050–30062.
469"
REFERENCES,0.47591362126245845,"SAHA, A. and GAILLARD, P. (2022). Versatile dueling bandits: Best-of-both world analyses for
470"
REFERENCES,0.47674418604651164,"learning from relative preferences. In International Conference on Machine Learning. PMLR.
471"
REFERENCES,0.4775747508305648,"SAHA, A., KOREN, T. and MANSOUR, Y. (2021). Adversarial dueling bandits. In International
472"
REFERENCES,0.47840531561461797,"Conference on Machine Learning. PMLR.
473"
REFERENCES,0.4792358803986711,"SAHA, A. and KRISHNAMURTHY, A. (2022). Efficient and optimal algorithms for contextual dueling
474"
REFERENCES,0.48006644518272423,"bandits under realizability. In International Conference on Algorithmic Learning Theory. PMLR.
475"
REFERENCES,0.4808970099667774,"SEKHARI, A., SRIDHARAN, K., SUN, W. and WU, R. (2023). Contextual bandits and imitation
476"
REFERENCES,0.48172757475083056,"learning via preference-based active queries. arXiv preprint arXiv:2307.12926 .
477"
REFERENCES,0.48255813953488375,"SELDIN, Y. and LUGOSI, G. (2017). An improved parametrization and analysis of the exp3++
478"
REFERENCES,0.4833887043189369,"algorithm for stochastic and adversarial bandits. In Conference on Learning Theory. PMLR.
479"
REFERENCES,0.48421926910299,"SELDIN, Y. and SLIVKINS, A. (2014). One practical algorithm for both stochastic and adversarial
480"
REFERENCES,0.4850498338870432,"bandits. In International Conference on Machine Learning. PMLR.
481"
REFERENCES,0.48588039867109634,"WEI, C.-Y., DANN, C. and ZIMMERT, J. (2022). A model selection approach for corruption robust
482"
REFERENCES,0.4867109634551495,"reinforcement learning. In International Conference on Algorithmic Learning Theory. PMLR.
483"
REFERENCES,0.48754152823920266,"WU, H. and LIU, X. (2016). Double thompson sampling for dueling bandits. Advances in neural
484"
REFERENCES,0.4883720930232558,"information processing systems 29.
485"
REFERENCES,0.489202657807309,"WU, Y., JIN, T., LOU, H., FARNOUD, F. and GU, Q. (2023). Borda regret minimization for
486"
REFERENCES,0.4900332225913621,"generalized linear dueling bandits. arXiv preprint arXiv:2303.08816 .
487"
REFERENCES,0.4908637873754153,"XIONG, W., DONG, H., YE, C., ZHONG, H., JIANG, N. and ZHANG, T. (2023). Gibbs sam-
488"
REFERENCES,0.49169435215946844,"pling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint
489"
REFERENCES,0.4925249169435216,"arXiv:2312.11456 .
490"
REFERENCES,0.49335548172757476,"YE, C., XIONG, W., GU, Q. and ZHANG, T. (2023). Corruption-robust algorithms with uncertainty
491"
REFERENCES,0.4941860465116279,"weighting for nonlinear contextual bandits and markov decision processes. In International
492"
REFERENCES,0.4950166112956811,"Conference on Machine Learning. PMLR.
493"
REFERENCES,0.4958471760797342,"YU, T., QUILLEN, D., HE, Z., JULIAN, R., HAUSMAN, K., FINN, C. and LEVINE, S. (2020).
494"
REFERENCES,0.49667774086378735,"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In
495"
REFERENCES,0.49750830564784054,"Conference on robot learning. PMLR.
496"
REFERENCES,0.4983388704318937,"YUE, Y., BRODER, J., KLEINBERG, R. and JOACHIMS, T. (2012). The k-armed dueling bandits
497"
REFERENCES,0.49916943521594687,"problem. Journal of Computer and System Sciences 78 1538–1556.
498"
REFERENCES,0.5,"ZHAO, H., ZHOU, D. and GU, Q. (2021). Linear contextual bandits with adversarial corruptions.
499"
REFERENCES,0.5008305647840532,"arXiv preprint arXiv:2110.12615 .
500"
REFERENCES,0.5016611295681063,"ZHU, H., YU, J., GUPTA, A., SHAH, D., HARTIKAINEN, K., SINGH, A., KUMAR, V. and
501"
REFERENCES,0.5024916943521595,"LEVINE, S. (2020). The ingredients of real-world robotic reinforcement learning. arXiv preprint
502"
REFERENCES,0.5033222591362126,"arXiv:2004.12570 .
503"
REFERENCES,0.5041528239202658,"ZIMMERT, J. and SELDIN, Y. (2019). An optimal algorithm for stochastic and adversarial bandits.
504"
REFERENCES,0.5049833887043189,"In The 22nd International Conference on Artificial Intelligence and Statistics. PMLR.
505"
REFERENCES,0.5058139534883721,"ZOGHI, M., KARNIN, Z. S., WHITESON, S. and DE RIJKE, M. (2015). Copeland dueling bandits.
506"
REFERENCES,0.5066445182724253,"Advances in neural information processing systems 28.
507"
REFERENCES,0.5074750830564784,"ZOGHI, M., WHITESON, S., MUNOS, R. and RIJKE, M. (2014). Relative upper confidence bound
508"
REFERENCES,0.5083056478405316,"for the k-armed dueling bandit problem. In International conference on machine learning. PMLR.
509"
REFERENCES,0.5091362126245847,"Broader Impact
510"
REFERENCES,0.5099667774086378,"This paper studies contextual dueling bandits with adversarial feedback. Our primary objective is
511"
REFERENCES,0.510797342192691,"to propel advancements in bandit theory by introducing a more robust algorithm backed by solid
512"
REFERENCES,0.5116279069767442,"theoretical guarantees. The uncertainty-weighted approach we have developed for dueling bandits
513"
REFERENCES,0.5124584717607974,"holds significant potential to address the issue of adversarial feedback in preference-based data, which
514"
REFERENCES,0.5132890365448505,"could be instrumental in enhancing the robustness of generative models against adversarial attacks,
515"
REFERENCES,0.5141196013289037,"thereby contributing positively to the societal impact and reliability of machine learning applications.
516"
REFERENCES,0.5149501661129569,"A
Roadmap of the Proof
517"
REFERENCES,0.5157807308970099,"A.1
Uncertainty-weighted MLE with Adversarial Feedback
518"
REFERENCES,0.5166112956810631,"In this section, we offer an overview of the proof for Lemma 5.1. The general proof idea for
519"
REFERENCES,0.5174418604651163,"the uncertainty-weighted MLE with adversarial feedback lies in decomposing the estimation error
520"
REFERENCES,0.5182724252491694,"into three terms, a stochastic error term, an adversarial term, and an additional regularization term.
521"
REFERENCES,0.5191029900332226,"Following the analysis of standard (weighted) MLE (Li et al., 2017), we introduce an auxiliary
522"
REFERENCES,0.5199335548172758,"function:
523"
REFERENCES,0.520764119601329,"Gt(θ) = λκθ + t−1
X"
REFERENCES,0.521594684385382,"i=1
wi
h
σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ
"
REFERENCES,0.5224252491694352,"−σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ∗i 
ϕ(xi, ai) −ϕ(xi, bi)

."
REFERENCES,0.5232558139534884,"It satisfies two conditions: First, for the true parameter value θ∗, Gt(θ∗) has a simple expression, i.e.,
524"
REFERENCES,0.5240863787375415,Gt(θ∗) = λκθ∗.
REFERENCES,0.5249169435215947,"Second, according to (4.4), we can get the value of function Gt at the MLE θt,
525"
REFERENCES,0.5257475083056479,"Gt(θt) = t−1
X"
REFERENCES,0.526578073089701,"i=1
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)

,
(A.1)"
REFERENCES,0.5274086378737541,"where γi = oi −σ
 
(ϕ(xi, ai) −ϕ(xi, bi))⊤θ∗
. To connect the desired estimation error with the
526"
REFERENCES,0.5282392026578073,"function Gt, we use the mean value theorem. This leads to an upper bound of the estimation error:
527"
REFERENCES,0.5290697674418605,∥θt −θ∗∥Σt ≤1 κ
REFERENCES,0.5299003322259136,"Gt(θt) −Gt(θ∗)

Σ−1
t ≤1"
REFERENCES,0.5307308970099668,"κλ∥θ∗∥Σ−1
t
|
{z
}
Regularization term + 1 κ"
REFERENCES,0.53156146179402,"Gt(θt)

Σ−1
t
|
{z
}
I1 ."
REFERENCES,0.532392026578073,"For term I1, we can decompose the summation in (A.1) based on the adversarial feedback ct, i.e.,
528"
REFERENCES,0.5332225913621262,"Gt(θt) =
X"
REFERENCES,0.5340531561461794,"i<t:ci=0
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)

+
X"
REFERENCES,0.5348837209302325,"i<t:ci=1
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)
"
REFERENCES,0.5357142857142857,"|
{z
}
I2 ,"
REFERENCES,0.5365448504983389,"where I2 can be further decomposed as
529"
REFERENCES,0.5373754152823921,"I2 =
X"
REFERENCES,0.5382059800664452,"i<t:ci=1
wiϵi
 
ϕ(xi, ai) −ϕ(xi, bi)

+
X"
REFERENCES,0.5390365448504983,"i<t:ci=1
wi(γi −ϵi)
 
ϕ(xi, ai) −ϕ(xi, bi)

."
REFERENCES,0.5398671096345515,"where ϵi = li −σ
 
(ϕ(xi, ai) −ϕ(xi, bi))⊤θ∗
. With our notation of adversarial feedback, when
530"
REFERENCES,0.5406976744186046,"ci = 0, we have γi = ϵi. Therefore, we have |γi −ϵi| ≤1 and
531 I1 ≤1 κ  t−1
X"
REFERENCES,0.5415282392026578,"i=1
wiϵi
 
ϕ(xi, ai) −ϕ(xi, bi)

Σ−1
t
|
{z
}
Stochastic term + 1 κ X"
REFERENCES,0.542358803986711,"i<t:ci=1
wi
 
ϕ(xi, ai) −ϕ(xi, bi)

Σ−1
t
|
{z
}
Adversarial term ."
REFERENCES,0.5431893687707641,"The stochastic term can be upper bounded with the concentration inequality (Lemma D.2). Addition-
532"
REFERENCES,0.5440199335548173,"ally, by employing our specifically chosen weight, as (4.3), we can control the adversarial term, with
533"
REFERENCES,0.5448504983388704,"wi∥ϕ(xi, ai) −ϕ(xi, bi)∥Σ−1
t
≤α. Therefore, the adversarial term can be bounded by αC/κ.
534"
REFERENCES,0.5456810631229236,"A.2
Regret Upper Bound
535"
REFERENCES,0.5465116279069767,"With a similar discussion of the symmetric arm selection rule to Di et al. (2023), the regret defined in
536"
REFERENCES,0.5473421926910299,"(3.2) can be bounded by
537"
REFERENCES,0.5481727574750831,"Regret(T) ≤ T
X"
REFERENCES,0.5490033222591362,"t=1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o
."
REFERENCES,0.5498338870431894,"Note that in our selection of weight wt, it has two possible values. We decompose the summation
538"
REFERENCES,0.5506644518272426,"based on the two cases separately. We have
539"
REFERENCES,0.5514950166112956,"Regret(T) ≤
X"
REFERENCES,0.5523255813953488,"wt=1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o"
REFERENCES,0.553156146179402,"|
{z
}
J1 +
X"
REFERENCES,0.5539867109634552,"wt<1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o"
REFERENCES,0.5548172757475083,"|
{z
}
J2 ."
REFERENCES,0.5556478405315615,"We consider J1, J2 separately. For the term J1, we define Λt = λI + P"
REFERENCES,0.5564784053156147,"i≤t−1,wi=1
 
ϕ(xi, ai) −
540"
REFERENCES,0.5573089700996677,"ϕ(xi, bi)
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤. Then, we have Σt ⪰Λt, and therefore
541"
REFERENCES,0.5581395348837209,"∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t
≤∥ϕ(xt, at) −ϕ(xt, bt)∥Λ−1
t ."
REFERENCES,0.5589700996677741,"Using Lemma D.3 with xt = ϕ(xt, at) −ϕ(xt, bt), we have
542"
REFERENCES,0.5598006644518272,"J1 ≤4β
p"
REFERENCES,0.5606312292358804,"dT log(1 + 2T/λ).
(A.2)"
REFERENCES,0.5614617940199336,"For term J2, we note that wt < 1 implies that wt = α/∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t . Therefore, we
543"
REFERENCES,0.5622923588039868,"have
544 J2 ≤ T
X t=1 4β"
REFERENCES,0.5631229235880398,"α min
n
1, ∥√wt(ϕ(xt, at) −ϕ(xt, bt))∥2
Σ−1
t o
."
REFERENCES,0.563953488372093,"Using Lemma D.3 with x′
t = √wt(ϕ(xt, at) −ϕ(xt, bt)), we have
545"
REFERENCES,0.5647840531561462,J2 ≤4dβ log(1 + 2T/λ)
REFERENCES,0.5656146179401993,"α
.
(A.3)"
REFERENCES,0.5664451827242525,"We conclude the proof of regret by combining (A.2) and (A.3).
546"
REFERENCES,0.5672757475083057,"B
Proof of Theorems in Section 5
547"
REFERENCES,0.5681063122923588,"B.1
Proof of Theorem 5.3
548"
REFERENCES,0.5689368770764119,"In this subsection, we provide the proof of Theorem 5.3. We condition on the high-probability event
549"
REFERENCES,0.5697674418604651,"in Lemma 5.1
550"
REFERENCES,0.5705980066445183,"E =
nθt −θ∗
Σt ≤β, ∀t ∈[T]
o
."
REFERENCES,0.5714285714285714,"Let rt = 2r∗(xt, a∗
t )−r∗(xt, at)−r∗(xt, bt) be the regret incurred in round t. The following lemma
551"
REFERENCES,0.5722591362126246,"provides the upper bound of rt.
552"
REFERENCES,0.5730897009966778,"Lemma B.1. Let 0 < δ < 1. If we set β =
√"
REFERENCES,0.5739202657807309,"λB +
 
αC +
p"
REFERENCES,0.574750830564784,"d log((1 + 2T/λ)/δ)

/κ, on event E,
553"
REFERENCES,0.5755813953488372,"the regret of Algorithm 1 incurred in round t can be upper bounded by
554"
REFERENCES,0.5764119601328903,"rt ≤min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o
."
REFERENCES,0.5772425249169435,"Moreover, the regret can be upper bounded by
555"
REFERENCES,0.5780730897009967,"Regret(T) ≤ T
X"
REFERENCES,0.5789036544850499,"t=1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o
."
REFERENCES,0.579734219269103,"With Lemma B.1, we can provide the proof of Theorem 5.3.
556"
REFERENCES,0.5805647840531561,"Proof of Theorem 5.3. Using Lemma B.1, the total regret can be upper bounded by
557"
REFERENCES,0.5813953488372093,"Regret(T) ≤ T
X"
REFERENCES,0.5822259136212624,"t=1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o
."
REFERENCES,0.5830564784053156,"Our weight wt has two possible values. We decompose the summation based on the two cases
558"
REFERENCES,0.5838870431893688,"separately. We have
559"
REFERENCES,0.584717607973422,"Regret(T) ≤
X"
REFERENCES,0.5855481727574751,"wt=1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o"
REFERENCES,0.5863787375415282,"|
{z
}
J1 +
X"
REFERENCES,0.5872093023255814,"wt<1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o"
REFERENCES,0.5880398671096345,"|
{z
}
J2 ."
REFERENCES,0.5888704318936877,"For the term J1, we consider a partial summation in rounds when wt = 1. Let Λt = λI +
560
P"
REFERENCES,0.5897009966777409,"i≤k−1,wi=1
 
ϕ(xi, ai) −ϕ(xi, bi)
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤. Then we have
561"
REFERENCES,0.590531561461794,"J1 ≤4β
X"
REFERENCES,0.5913621262458472,"t:wt=1
min
n
1, ∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o ≤4β
X"
REFERENCES,0.5921926910299004,"t:wt=1
min
n
1, ∥ϕ(xt, at) −ϕ(xt, bt)∥Λ−1
t o ≤4β
s T
X"
REFERENCES,0.5930232558139535,"t:wt=1
min

1, ∥ϕ(xt, at) −ϕ(xt, bt)∥2
Λ−1
t ≤4β
p"
REFERENCES,0.5938538205980066,"dT log(1 + 2T/λ),
(B.1)
where the second inequality holds due to Σt ⪰Λt. The third inequality holds due to the Cauchy-
562"
REFERENCES,0.5946843853820598,"Schwartz inequality, The last inequality holds due to Lemma D.3.
563"
REFERENCES,0.595514950166113,"For the term J2, the weight in this summation satisfies wt < 1, and therefore wt = α/∥ϕ(xt, at) −
564"
REFERENCES,0.5963455149501661,"ϕ(xt, bt)∥Σ−1
t . Then we have
565"
REFERENCES,0.5971760797342193,"J2 =
X"
REFERENCES,0.5980066445182725,"wt<1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t wt∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t /α
o ≤ T
X"
REFERENCES,0.5988372093023255,"t=1
min
n
4, 2β/α∥√wt(ϕ(xt, at) −ϕ(xt, bt))∥2
Σ−1
t o ≤ T
X t=1 4β"
REFERENCES,0.5996677740863787,"α min
n
1, ∥√wt(ϕ(xt, at) −ϕ(xt, bt))∥2
Σ−1
t o"
REFERENCES,0.6004983388704319,≤4dβ log(1 + 2T/λ)
REFERENCES,0.6013289036544851,"α
,
(B.2)"
REFERENCES,0.6021594684385382,"where the first equality holds due to the choice of wt. The first inequality holds because each term in
566"
REFERENCES,0.6029900332225914,"the summation is positive. The last inequality holds due to Lemma D.3. Combining (B.1) and (B.2),
567"
REFERENCES,0.6038205980066446,"we complete the proof of Theorem 5.3.
568"
REFERENCES,0.6046511627906976,"B.2
Proof of Theorem 5.5
569"
REFERENCES,0.6054817275747508,"Proof of Theorem 5.5. Our proof adapts the argument in Bogunovic et al. (2021) to dueling bandits.
570"
REFERENCES,0.606312292358804,"For any dimension d, we construct d instances, each with θi = ei, where ei is the i-th standard basis
571"
REFERENCES,0.6071428571428571,"vector. We set the action set A = {ei}d
i=1. Therefore, in the i-th instance, the reward for the i-th
572"
REFERENCES,0.6079734219269103,"action will be 1. For the other actions, it will be 0. Therefore, the i-th action will be more preferable
573"
REFERENCES,0.6088039867109635,"to any other action. While for other pairs, the feedback is simply a random guess.
574"
REFERENCES,0.6096345514950167,"Consider an adversary that knows the exact instance. When the comparison involves the i-th action,
575"
REFERENCES,0.6104651162790697,"it will corrupt the feedback with a random guess. Otherwise, it will not corrupt. In the i-th instance,
576"
REFERENCES,0.6112956810631229,"the adversary stops the adversarial attack only after C times of comparison involving the i-th action.
577"
REFERENCES,0.6121262458471761,"However, after Cd/4 rounds, at least d/2 actions have not been compared for C times. For the
578"
REFERENCES,0.6129568106312292,"instances corresponding to these actions, the agent learns no information and suffers from Ω(dC)
579"
REFERENCES,0.6137873754152824,"regret. This completes the proof of Theorem 5.5.
580"
REFERENCES,0.6146179401993356,"B.3
Proof of Theorem 5.7
581"
REFERENCES,0.6154485049833887,"Proof of Theorem 5.7. Here, based on the relationship between C and the threshold ¯C, we discuss
582"
REFERENCES,0.6162790697674418,"two distinct cases separately.
583"
REFERENCES,0.617109634551495,"• In the scenario where ¯C < C, Algorithm 1 can ensures a trivial regret bound, with the guarantee
584"
REFERENCES,0.6179401993355482,"that Regret(T) ≤2T.
585"
REFERENCES,0.6187707641196013,"• In the scenario where C ≤¯C, we know that ¯C is remains a valid upper bound on the number of
586"
REFERENCES,0.6196013289036545,"adversarial feedback. Under this situation, Algorithm 1 operates successfully with ¯C adversarial
587"
REFERENCES,0.6204318936877077,"feedback. Therefore, according to Theorem 5.3, the regret is upper bounded by
588"
REFERENCES,0.6212624584717608,"Regret(T) ≤eO(d
√"
REFERENCES,0.622093023255814,T + d ¯C). 589
REFERENCES,0.6229235880398671,"C
Proof of Lemmas 5.1 and B.1
590"
REFERENCES,0.6237541528239202,"C.1
Proof of Lemma 5.1
591"
REFERENCES,0.6245847176079734,"Proof of Lemma 5.1. Using a similar reasoning in Li et al. (2017), we define some auxiliary quantities
592"
REFERENCES,0.6254152823920266,"Gt(θ) = λκθ + t−1
X"
REFERENCES,0.6262458471760798,"i=1
wi
h
σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ
"
REFERENCES,0.6270764119601329,"−σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ∗i 
ϕ(xi, ai) −ϕ(xi, bi)

,"
REFERENCES,0.627906976744186,"ϵt = lt −σ
 
ϕ(xt, at) −ϕ(xt, bt)
⊤θ∗
,"
REFERENCES,0.6287375415282392,"γt = ot −σ
 
ϕ(xt, at) −ϕ(xt, bt)
⊤θ∗
, Zt = t−1
X"
REFERENCES,0.6295681063122923,"i=1
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)

."
REFERENCES,0.6303986710963455,"In Algorithm 1, θt is chosen to be the solution to the following equation,
593"
REFERENCES,0.6312292358803987,"λκθt + t−1
X"
REFERENCES,0.6320598006644518,"i=1
wi
h
σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θt

−oi
i 
ϕ(xi, ai) −ϕ(xi, bi)

= 0.
(C.1)"
REFERENCES,0.632890365448505,"Then we have
594"
REFERENCES,0.6337209302325582,"Gt(θt) = λκθt + t−1
X"
REFERENCES,0.6345514950166113,"i=1
wi
h
σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θt
"
REFERENCES,0.6353820598006644,"−σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ∗i 
ϕ(xi, ai) −ϕ(xi, bi)
 = t−1
X"
REFERENCES,0.6362126245847176,"i=1
wi
h
oi −σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ∗i 
ϕ(xi, ai) −ϕ(xi, bi)
 = Zt."
REFERENCES,0.6370431893687708,"The analysis in Li et al. (2017); Di et al. (2023) shows that this equation has a unique solution, with
595"
REFERENCES,0.6378737541528239,"θt = G−1
t (Zt). Using the mean value theorem, for any θ1, θ2 ∈Rd, there exists m ∈[0, 1] and
596"
REFERENCES,0.6387043189368771,"¯θ = mθ1 + (1 −m)θ2, such that the following equation holds,
597"
REFERENCES,0.6395348837209303,"Gt(θ1) −Gt(θ2) = λκ(θ1 −θ2) + t−1
X"
REFERENCES,0.6403654485049833,"i=1
wi
h
σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ1
"
REFERENCES,0.6411960132890365,"−σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤θ2
i 
ϕ(xi, ai) −ϕ(xi, bi)
"
REFERENCES,0.6420265780730897,"=
h
λκI + t−1
X"
REFERENCES,0.6428571428571429,"i=1
wi ˙σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤¯θ
"
REFERENCES,0.643687707641196," 
ϕ(xi, ai) −ϕ(xi, bi)
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤i
(θ1 −θ2)."
REFERENCES,0.6445182724252492,"We define F(¯θ) as
598"
REFERENCES,0.6453488372093024,"F(¯θ) = λκI + t−1
X"
REFERENCES,0.6461794019933554,"i=1
wi ˙σ
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤¯θ
 
ϕ(xi, ai) −ϕ(xi, bi)
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤i
."
REFERENCES,0.6470099667774086,"Moreover, we can see that Gt(θ∗)
=
λκθ∗.
Recall Σt
=
λI + Pt−1
i=1 wi
 
ϕ(xi, ai) −
599"
REFERENCES,0.6478405315614618,"ϕ(xi, bi)
 
ϕ(xi, ai) −ϕ(xi, bi)
⊤. We have
600"
REFERENCES,0.6486710963455149,"Gt(θt) −Gt(θ∗)
2
Σ−1
t
= (θt −θ∗)⊤F(¯θ)Σ−1
t F(¯θ)(θt −θ∗)"
REFERENCES,0.6495016611295681,≥κ2(θt −θ∗)⊤Σt(θt −θ∗)
REFERENCES,0.6503322259136213,"= κ2∥θt −θ∗∥2
Σt,"
REFERENCES,0.6511627906976745,"where the first inequality holds due to ˙µ(·) ≥κ > 0 and F(¯θ) ⪰κΣt. Then we have the following
601"
REFERENCES,0.6519933554817275,"estimate of the estimation error:
602"
REFERENCES,0.6528239202657807,∥θt −θ∗∥Σt ≤1 κ
REFERENCES,0.6536544850498339,"Gt(θt) −Gt(θ∗)

Σ−1
t"
REFERENCES,0.654485049833887,"≤λ∥θ∗∥Σ−1
t
+ 1"
REFERENCES,0.6553156146179402,"κ∥Zt∥Σ−1
t ≤
√"
REFERENCES,0.6561461794019934,λ∥θ∗∥2 + 1
REFERENCES,0.6569767441860465,"κ∥Zt∥Σ−1
t ,"
REFERENCES,0.6578073089700996,"where the second inequality holds due to the triangle inequality and Gt(θ∗) = λκθ∗. The last
603"
REFERENCES,0.6586378737541528,"inequality holds due to Σt ⪰λI. Finally, we need to bound the ∥Zt∥Σ−1
t
term. To study the impact
604"
REFERENCES,0.659468438538206,"of adversarial feedback, we decompose the summation in (A.1) based on the adversarial feedback ct,
605"
REFERENCES,0.6602990033222591,"i.e.,
606"
REFERENCES,0.6611295681063123,"Zt =
X"
REFERENCES,0.6619601328903655,"i<t:ci=0
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)

+
X"
REFERENCES,0.6627906976744186,"i<t:ci=1
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)

,"
REFERENCES,0.6636212624584718,"When ci = 1, i.e. with adversarial feedback, |γi −ϵi| = 1. On the contrary, when ci = 0, γi = ϵi.
607"
REFERENCES,0.6644518272425249,"Therefore,
608 X"
REFERENCES,0.665282392026578,"i<t:ci=0
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)

=
X"
REFERENCES,0.6661129568106312,"i<t:ci=0
wiϵi
 
ϕ(xi, ai) −ϕ(xi, bi)

, X"
REFERENCES,0.6669435215946844,"i<t:ci=1
wiγi
 
ϕ(xi, ai) −ϕ(xi, bi)

=
X"
REFERENCES,0.6677740863787376,"i<t:ci=1
wiϵi
 
ϕ(xi, ai) −ϕ(xi, bi)
 +
X"
REFERENCES,0.6686046511627907,"i<t:ci=1
wi
 
γi −ϵi)(ϕ(xi, ai) −ϕ(xi, bi)

."
REFERENCES,0.6694352159468439,"Summing up the two equalties, we have
609 Zt = t−1
X"
REFERENCES,0.670265780730897,"i=1
wiϵi
 
ϕ(xi, ai) −ϕ(xi, bi)

+
X"
REFERENCES,0.6710963455149501,"i<t:ci=1
wi(γi −ϵi)
 
ϕ(xi, ai) −ϕ(xi, bi)

."
REFERENCES,0.6719269102990033,"Therefore,
610"
REFERENCES,0.6727574750830565,"∥Zt∥Σ−1
t
≤ t−1
X"
REFERENCES,0.6735880398671097,"i=1
wiϵi
 
ϕ(xi, ai) −ϕ(xi, bi)

Σ−1
t
|
{z
}
I1"
REFERENCES,0.6744186046511628,"+

X"
REFERENCES,0.675249169435216,"i<t:ci=1
wi
 
ϕ(xi, ai) −ϕ(xi, bi)

Σ−1
t
|
{z
}
I2 ."
REFERENCES,0.6760797342192691,"For the term I1, with probability at least 1 −δ, for all t ∈[T], it can be bounded by
611 I1 ≤ r"
LOG,0.6769102990033222,"2 log
det(Σt)1/2 det(Σ0)−1/2 δ 
,"
LOG,0.6777408637873754,"due to Lemma D.2. Using wi ≤1, we have √wi∥ϕ(xi, ai) −ϕ(xi, bi)∥2 ≤2. Moreover, we have
612"
LOG,0.6785714285714286,"det(Σt) ≤
Tr(Σt) d d"
LOG,0.6794019933554817,"=
dλ + Pt−1
i=1 wi∥(ϕ(xi, ai) −ϕ(xi, bi))∥2
2
d d"
LOG,0.6802325581395349,"≤
dλ + 2T d d
,"
LOG,0.6810631229235881,"where the first inequality holds because for every matrix A ∈Rd×d, det A ≤(Tr(A)/d)d. The
613"
LOG,0.6818936877076412,"second inequality holds due to √wi∥ϕ(xi, ai) −ϕ(xi, bi)∥2 ≤2. Easy to see that det(Σ0) = λd.
614"
LOG,0.6827242524916943,"The term I1 can be bounded by
615"
LOG,0.6835548172757475,"I1 ≤
p"
LOG,0.6843853820598007,"d log((1 + 2T/λ)/δ).
(C.2)
For I2, with our choice of the weight wi, we have
616"
LOG,0.6852159468438538,"I2 ≤
X"
LOG,0.686046511627907,"i<t:ci=1
wi
(ϕ(xi, ai) −ϕ(xi, bi))

Σ−1
t ≤
X"
LOG,0.6868770764119602,"i<t:ci=1
wi
(ϕ(xi, ai) −ϕ(xi, bi))

Σ−1
i ≤
X"
LOG,0.6877076411960132,"i<t:ci=1
α"
LOG,0.6885382059800664,"≤αC,
(C.3)
where the second inequality holds due to Σt ⪰Σi. The third inequality holds due to wi ≤
617"
LOG,0.6893687707641196,"α/∥(ϕ(xi, ai) −ϕ(xi, bi))

Σ−1
i . The last inequality holds due to the definition of C. Combining
618"
LOG,0.6901993355481728,"(C.2) and (C.3), we complete the proof of Lemma 5.1.
619"
LOG,0.6910299003322259,"C.2
Proof of Lemma B.1
620"
LOG,0.6918604651162791,"Proof of Lemma B.1. Let the regret incurred in the t-th round by rt = 2r∗(xt, a∗
t ) −r∗(xt, at) −
621"
LOG,0.6926910299003323,"r∗(xt, bt). It can be decomposed as
622"
LOG,0.6935215946843853,"rt = 2r∗(xt, a∗
t ) −r∗(xt, at) −r∗(xt, bt)
= ⟨ϕ(xt, a∗
t ) −ϕ(xt, at), θ∗⟩+ ⟨ϕ(xt, a∗
t ) −ϕ(xt, bt), θ∗⟩
= ⟨ϕ(xt, a∗
t ) −ϕ(xt, at), θ∗−θt⟩+ ⟨ϕ(xt, a∗
t ) −ϕ(xt, bt), θ∗−θt⟩
+ ⟨2ϕ(xt, a∗
t ) −ϕ(xt, at) −ϕ(xt, bt), θt⟩
≤∥ϕ(xt, a∗
t ) −ϕ(xt, at)∥Σ−1
t ∥θ∗−θt∥Σt + ∥ϕ(xt, a∗
t ) −ϕ(xt, bt)∥Σ−1
t ∥θ∗−θt∥Σt
+ ⟨2ϕ(xt, a∗
t ) −ϕ(xt, at) −ϕ(xt, bt), θt⟩
≤β∥ϕ(xt, a∗
t ) −ϕ(xt, at)∥Σ−1
t
+ β∥ϕ(xt, a∗
t ) −ϕ(xt, bt)∥Σ−1
t
+ ⟨2ϕ(xt, a∗
t ) −ϕ(xt, at) −ϕ(xt, bt), θt⟩,
where the first inequality holds due to the Cauchy-Schwarz inequality. The second inequality holds
623"
LOG,0.6943521594684385,"due to the high probability confidence event E. Using our action selection rule, we have
624"
LOG,0.6951827242524917,"⟨ϕ(xt, a∗
t ) −ϕ(xt, at), θt⟩+ β∥ϕ(xt, a∗
t ) −ϕ(xt, at)∥Σ−1
t
≤⟨ϕ(xt, bt) −ϕ(xt, at), θt⟩+ β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t
⟨ϕ(xt, a∗
t ) −ϕ(xt, bt), θt⟩+ β∥ϕ(xt, a∗
t ) −ϕ(xt, bt)∥Σ−1
t
≤⟨ϕ(xt, at) −ϕ(xt, bt), θt⟩+ β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t ."
LOG,0.6960132890365448,"Adding the above two inequalities, we have
625"
LOG,0.696843853820598,"β∥ϕ(xt, a∗
t ) −ϕ(xt, at)∥Σ−1
t
+ β∥ϕ(xt, a∗
t ) −ϕ(xt, bt)∥Σ−1
t
≤⟨ϕ(xt, at) + ϕ(xt, bt) −2ϕ(xt, a∗
t ), θt⟩+ 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t ."
LOG,0.6976744186046512,"Therefore, we prove that the regret in round t can be upper bounded by
626"
LOG,0.6985049833887044,"rt ≤2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t ."
LOG,0.6993355481727574,"With a simple observation, we have rt ≤4. Therefore, the total regret can be upper bounded by
627"
LOG,0.7001661129568106,"Regret(T) ≤ T
X"
LOG,0.7009966777408638,"t=1
min
n
4, 2β∥ϕ(xt, at) −ϕ(xt, bt)∥Σ−1
t o
. 628"
LOG,0.7018272425249169,"D
Auxiliary Lemmas
629"
LOG,0.7026578073089701,"Lemma D.1 (Azuma–Hoeffding inequality, Cesa-Bianchi and Lugosi 2006). Let {ηk}K
k=1 be a
630"
LOG,0.7034883720930233,"martingale difference sequence with respect to a filtration {Ft} satisfying |ηt| ≤R for some constant
631"
LOG,0.7043189368770764,"R, ηt is Ft+1-measurable, E[ηt|Ft] = 0. Then for any 0 < δ < 1, with probability at least 1 −δ, we
632"
LOG,0.7051495016611296,"have
633 T
X"
LOG,0.7059800664451827,"t=1
ηt ≤R
p"
LOG,0.7068106312292359,2T log 1/δ.
LOG,0.707641196013289,"Lemma D.2 (Lemma 9 Abbasi-Yadkori et al. 2011). Let {ϵt}T
t=1 be a real-valued stochastic process
634"
LOG,0.7084717607973422,"with corresponding filtration {Ft}T
t=0 such that ϵt is Ft-measurable and ϵt is conditionally R-sub-
635"
LOG,0.7093023255813954,"Gaussian, i.e.
636"
LOG,0.7101328903654485,"∀λ ∈R, E[eλϵt|Ft−1] ≤exp
λ2R2 2 
."
LOG,0.7109634551495017,"Let {xt}T
t=1 be an Rd-valued stochastic process where xt is Ft−1-measurable and for any t ∈[T],
637"
LOG,0.7117940199335548,"we further define Σt = λI + Pt
i=1 xix⊤
i . Then with probability at least 1 −δ, for all t ∈[T], we
638"
LOG,0.7126245847176079,"have
639  T
X"
LOG,0.7134551495016611,"i=1
xiηi  2"
LOG,0.7142857142857143,"Σ−1
t
≤2R2 log
det(Σt)1/2 det(Σ0)−1/2 δ 
."
LOG,0.7151162790697675,"Lemma D.3 (Lemma 11, Abbasi-Yadkori et al. 2011). For any λ > 0 and sequence {xt}T
t=1 ⊆Rd
640"
LOG,0.7159468438538206,"for t ∈[T], define Zt = λI + Pt−1
i=1 xix⊤
i . Then, provided that ∥xt∥2 ≤L holds for all t ∈[T], we
641"
LOG,0.7167774086378738,"have
642 T
X"
LOG,0.717607973421927,"t=1
min
n
1, ∥xt∥2
Z−1
t"
LOG,0.71843853820598,"o
≤2d log(1 + TL2/(dλ))."
LOG,0.7192691029900332,"NeurIPS Paper Checklist
643"
CLAIMS,0.7200996677740864,"1. Claims
644"
CLAIMS,0.7209302325581395,"Question: Do the main claims made in the abstract and introduction accurately reflect the
645"
CLAIMS,0.7217607973421927,"paper’s contributions and scope?
646"
CLAIMS,0.7225913621262459,"Answer: [Yes]
647"
CLAIMS,0.723421926910299,"Justification: The primary contribution of this paper is addressing the challenge of adversarial
648"
CLAIMS,0.7242524916943521,"feedback within the dueling bandit model, where feedback is represented as a binary
649"
CLAIMS,0.7250830564784053,"preference label. Our research introduces a new perspective to machine learning. Unlike
650"
CLAIMS,0.7259136212624585,"previous works on corruption-robust bandits, where corruption in each round affects the
651"
CLAIMS,0.7267441860465116,"single-arm exploration and exploitation process. Flipping the preference label potentially
652"
CLAIMS,0.7275747508305648,"impacts the expected reward of both actions chosen in a duel. This interaction can further
653"
CLAIMS,0.728405315614618,"affect subsequent decisions involving only one of these arms. Compared with previous
654"
CLAIMS,0.729235880398671,"adversarial dueling bandit work, we study the most direct label-flipping attack, which is
655"
CLAIMS,0.7300664451827242,"aligned with many real-life preference-based learning scenarios. Our uncertainty-weighted
656"
CLAIMS,0.7308970099667774,"maximum likelihood estimation method helps to solve this novel problem, in scenarios with
657"
CLAIMS,0.7317275747508306,"known and unknown adversarial feedback. All the scope has been discussed clearly in our
658"
ABSTRACT,0.7325581395348837,"abstract and introduction.
659"
ABSTRACT,0.7333887043189369,"Guidelines:
660"
ABSTRACT,0.7342192691029901,"• The answer NA means that the abstract and introduction do not include the claims
661"
ABSTRACT,0.7350498338870431,"made in the paper.
662"
ABSTRACT,0.7358803986710963,"• The abstract and/or introduction should clearly state the claims made, including the
663"
ABSTRACT,0.7367109634551495,"contributions made in the paper and important assumptions and limitations. A No or
664"
ABSTRACT,0.7375415282392026,"NA answer to this question will not be perceived well by the reviewers.
665"
ABSTRACT,0.7383720930232558,"• The claims made should match theoretical and experimental results, and reflect how
666"
ABSTRACT,0.739202657807309,"much the results can be expected to generalize to other settings.
667"
ABSTRACT,0.7400332225913622,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
668"
ABSTRACT,0.7408637873754153,"are not attained by the paper.
669"
LIMITATIONS,0.7416943521594684,"2. Limitations
670"
LIMITATIONS,0.7425249169435216,"Question: Does the paper discuss the limitations of the work performed by the authors?
671"
LIMITATIONS,0.7433554817275747,"Answer: [Yes]
672"
LIMITATIONS,0.7441860465116279,"Justification: We have added a Limitations setting in our main paper. We assume that
673"
LIMITATIONS,0.7450166112956811,"the reward is linear with respect to some known feature maps. Although this setting is
674"
LIMITATIONS,0.7458471760797342,"common in the literature, we observe that some recent works on dueling bandits can deal
675"
LIMITATIONS,0.7466777408637874,"with nonlinear rewards (Li et al., 2024). Therefore, it’s possible to extend our results to a
676"
LIMITATIONS,0.7475083056478405,"more general setting. Another assumption concerns the lower bound of the derivative of
677"
LIMITATIONS,0.7483388704318937,"the link function. Notably, in the logistic bandit model, which shares similarities with our
678"
LIMITATIONS,0.7491694352159468,"setting through Bernoulli variables, some work (Abeille et al., 2021; Faury et al., 2022) can
679"
LIMITATIONS,0.75,"improve the dependency of κ from 1/κ to √κ. A similar improvement might be achieved in
680"
LIMITATIONS,0.7508305647840532,"our setting as well.
681"
LIMITATIONS,0.7516611295681063,"Guidelines:
682"
LIMITATIONS,0.7524916943521595,"• The answer NA means that the paper has no limitation while the answer No means that
683"
LIMITATIONS,0.7533222591362126,"the paper has limitations, but those are not discussed in the paper.
684"
LIMITATIONS,0.7541528239202658,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
685"
LIMITATIONS,0.7549833887043189,"• The paper should point out any strong assumptions and how robust the results are to
686"
LIMITATIONS,0.7558139534883721,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
687"
LIMITATIONS,0.7566445182724253,"model well-specification, asymptotic approximations only holding locally). The authors
688"
LIMITATIONS,0.7574750830564784,"should reflect on how these assumptions might be violated in practice and what the
689"
LIMITATIONS,0.7583056478405316,"implications would be.
690"
LIMITATIONS,0.7591362126245847,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
691"
LIMITATIONS,0.7599667774086378,"only tested on a few datasets or with a few runs. In general, empirical results often
692"
LIMITATIONS,0.760797342192691,"depend on implicit assumptions, which should be articulated.
693"
LIMITATIONS,0.7616279069767442,"• The authors should reflect on the factors that influence the performance of the approach.
694"
LIMITATIONS,0.7624584717607974,"For example, a facial recognition algorithm may perform poorly when image resolution
695"
LIMITATIONS,0.7632890365448505,"is low or images are taken in low lighting. Or a speech-to-text system might not be
696"
LIMITATIONS,0.7641196013289037,"used reliably to provide closed captions for online lectures because it fails to handle
697"
LIMITATIONS,0.7649501661129569,"technical jargon.
698"
LIMITATIONS,0.7657807308970099,"• The authors should discuss the computational efficiency of the proposed algorithms
699"
LIMITATIONS,0.7666112956810631,"and how they scale with dataset size.
700"
LIMITATIONS,0.7674418604651163,"• If applicable, the authors should discuss possible limitations of their approach to
701"
LIMITATIONS,0.7682724252491694,"address problems of privacy and fairness.
702"
LIMITATIONS,0.7691029900332226,"• While the authors might fear that complete honesty about limitations might be used by
703"
LIMITATIONS,0.7699335548172758,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
704"
LIMITATIONS,0.770764119601329,"limitations that aren’t acknowledged in the paper. The authors should use their best
705"
LIMITATIONS,0.771594684385382,"judgment and recognize that individual actions in favor of transparency play an impor-
706"
LIMITATIONS,0.7724252491694352,"tant role in developing norms that preserve the integrity of the community. Reviewers
707"
LIMITATIONS,0.7732558139534884,"will be specifically instructed to not penalize honesty concerning limitations.
708"
THEORY ASSUMPTIONS AND PROOFS,0.7740863787375415,"3. Theory Assumptions and Proofs
709"
THEORY ASSUMPTIONS AND PROOFS,0.7749169435215947,"Question: For each theoretical result, does the paper provide the full set of assumptions and
710"
THEORY ASSUMPTIONS AND PROOFS,0.7757475083056479,"a complete (and correct) proof?
711"
THEORY ASSUMPTIONS AND PROOFS,0.776578073089701,"Answer: [Yes]
712"
THEORY ASSUMPTIONS AND PROOFS,0.7774086378737541,"Justification: We have clearly stated and proved all the lemmas and theorems used in our
713"
THEORY ASSUMPTIONS AND PROOFS,0.7782392026578073,"theoretical results. To help readers understand the proof without checking all the details, we
714"
THEORY ASSUMPTIONS AND PROOFS,0.7790697674418605,"provide a roadmap of our proof in Appendix A. We also write explanation and clarification
715"
THEORY ASSUMPTIONS AND PROOFS,0.7799003322259136,"for every formula in our paper.
716"
THEORY ASSUMPTIONS AND PROOFS,0.7807308970099668,"Guidelines:
717"
THEORY ASSUMPTIONS AND PROOFS,0.78156146179402,"• The answer NA means that the paper does not include theoretical results.
718"
THEORY ASSUMPTIONS AND PROOFS,0.782392026578073,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
719"
THEORY ASSUMPTIONS AND PROOFS,0.7832225913621262,"referenced.
720"
THEORY ASSUMPTIONS AND PROOFS,0.7840531561461794,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
721"
THEORY ASSUMPTIONS AND PROOFS,0.7848837209302325,"• The proofs can either appear in the main paper or the supplemental material, but if
722"
THEORY ASSUMPTIONS AND PROOFS,0.7857142857142857,"they appear in the supplemental material, the authors are encouraged to provide a short
723"
THEORY ASSUMPTIONS AND PROOFS,0.7865448504983389,"proof sketch to provide intuition.
724"
THEORY ASSUMPTIONS AND PROOFS,0.7873754152823921,"• Inversely, any informal proof provided in the core of the paper should be complemented
725"
THEORY ASSUMPTIONS AND PROOFS,0.7882059800664452,"by formal proofs provided in appendix or supplemental material.
726"
THEORY ASSUMPTIONS AND PROOFS,0.7890365448504983,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
727"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7898671096345515,"4. Experimental Result Reproducibility
728"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7906976744186046,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
729"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7915282392026578,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
730"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.792358803986711,"of the paper (regardless of whether the code and data are provided or not)?
731"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7931893687707641,"Answer: [Yes]
732"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7940199335548173,"Justification: Our paper is mainly theoretical but we also do numerical experiments to justify
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7948504983388704,"the correctness of our results. We provide all the information to reproduce our results in
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7956810631229236,"Section 6.
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7965116279069767,"Guidelines:
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7973421926910299,"• The answer NA means that the paper does not include experiments.
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7981727574750831,"• If the paper includes experiments, a No answer to this question will not be perceived
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7990033222591362,"well by the reviewers: Making the paper reproducible is important, regardless of
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7998338870431894,"whether the code and data are provided or not.
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8006644518272426,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8014950166112956,"to make their results reproducible or verifiable.
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8023255813953488,"• Depending on the contribution, reproducibility can be accomplished in various ways.
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.803156146179402,"For example, if the contribution is a novel architecture, describing the architecture fully
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8039867109634552,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8048172757475083,"be necessary to either make it possible for others to replicate the model with the same
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8056478405315615,"dataset, or provide access to the model. In general. releasing code and data is often
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8064784053156147,"one good way to accomplish this, but reproducibility can also be provided via detailed
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8073089700996677,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8081395348837209,"of a large language model), releasing of a model checkpoint, or other means that are
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8089700996677741,"appropriate to the research performed.
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8098006644518272,"• While NeurIPS does not require releasing code, the conference does require all submis-
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8106312292358804,"sions to provide some reasonable avenue for reproducibility, which may depend on the
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8114617940199336,"nature of the contribution. For example
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8122923588039868,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8131229235880398,"to reproduce that algorithm.
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.813953488372093,"(b) If the contribution is primarily a new model architecture, the paper should describe
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8147840531561462,"the architecture clearly and fully.
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8156146179401993,"(c) If the contribution is a new model (e.g., a large language model), then there should
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8164451827242525,"either be a way to access this model for reproducing the results or a way to reproduce
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8172757475083057,"the model (e.g., with an open-source dataset or instructions for how to construct
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8181063122923588,"the dataset).
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8189368770764119,"(d) We recognize that reproducibility may be tricky in some cases, in which case
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8197674418604651,"authors are welcome to describe the particular way they provide for reproducibility.
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8205980066445183,"In the case of closed-source models, it may be that access to the model is limited in
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8214285714285714,"some way (e.g., to registered users), but it should be possible for other researchers
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8222591362126246,"to have some path to reproducing or verifying the results.
767"
OPEN ACCESS TO DATA AND CODE,0.8230897009966778,"5. Open access to data and code
768"
OPEN ACCESS TO DATA AND CODE,0.8239202657807309,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
769"
OPEN ACCESS TO DATA AND CODE,0.824750830564784,"tions to faithfully reproduce the main experimental results, as described in supplemental
770"
OPEN ACCESS TO DATA AND CODE,0.8255813953488372,"material?
771"
OPEN ACCESS TO DATA AND CODE,0.8264119601328903,"Answer: [No]
772"
OPEN ACCESS TO DATA AND CODE,0.8272425249169435,"Justification: Our experiments involve synthetic data generated from a generalized linear
773"
OPEN ACCESS TO DATA AND CODE,0.8280730897009967,"model, which is quite simple and easy to reproduce. That’s why we do not provide access
774"
OPEN ACCESS TO DATA AND CODE,0.8289036544850499,"to our data and code. All the information required to reproduce the results is provided in
775"
OPEN ACCESS TO DATA AND CODE,0.829734219269103,"Section 6. Guidelines:
776"
OPEN ACCESS TO DATA AND CODE,0.8305647840531561,"• The answer NA means that paper does not include experiments requiring code.
777"
OPEN ACCESS TO DATA AND CODE,0.8313953488372093,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
778"
OPEN ACCESS TO DATA AND CODE,0.8322259136212624,"public/guides/CodeSubmissionPolicy) for more details.
779"
OPEN ACCESS TO DATA AND CODE,0.8330564784053156,"• While we encourage the release of code and data, we understand that this might not be
780"
OPEN ACCESS TO DATA AND CODE,0.8338870431893688,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
781"
OPEN ACCESS TO DATA AND CODE,0.834717607973422,"including code, unless this is central to the contribution (e.g., for a new open-source
782"
OPEN ACCESS TO DATA AND CODE,0.8355481727574751,"benchmark).
783"
OPEN ACCESS TO DATA AND CODE,0.8363787375415282,"• The instructions should contain the exact command and environment needed to run to
784"
OPEN ACCESS TO DATA AND CODE,0.8372093023255814,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
785"
OPEN ACCESS TO DATA AND CODE,0.8380398671096345,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
786"
OPEN ACCESS TO DATA AND CODE,0.8388704318936877,"• The authors should provide instructions on data access and preparation, including how
787"
OPEN ACCESS TO DATA AND CODE,0.8397009966777409,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
788"
OPEN ACCESS TO DATA AND CODE,0.840531561461794,"• The authors should provide scripts to reproduce all experimental results for the new
789"
OPEN ACCESS TO DATA AND CODE,0.8413621262458472,"proposed method and baselines. If only a subset of experiments are reproducible, they
790"
OPEN ACCESS TO DATA AND CODE,0.8421926910299004,"should state which ones are omitted from the script and why.
791"
OPEN ACCESS TO DATA AND CODE,0.8430232558139535,"• At submission time, to preserve anonymity, the authors should release anonymized
792"
OPEN ACCESS TO DATA AND CODE,0.8438538205980066,"versions (if applicable).
793"
OPEN ACCESS TO DATA AND CODE,0.8446843853820598,"• Providing as much information as possible in supplemental material (appended to the
794"
OPEN ACCESS TO DATA AND CODE,0.845514950166113,"paper) is recommended, but including URLs to data and code is permitted.
795"
OPEN ACCESS TO DATA AND CODE,0.8463455149501661,"6. Experimental Setting/Details
796"
OPEN ACCESS TO DATA AND CODE,0.8471760797342193,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
797"
OPEN ACCESS TO DATA AND CODE,0.8480066445182725,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
798"
OPEN ACCESS TO DATA AND CODE,0.8488372093023255,"results?
799"
OPEN ACCESS TO DATA AND CODE,0.8496677740863787,"Answer: [Yes]
800"
OPEN ACCESS TO DATA AND CODE,0.8504983388704319,"Justification:
801"
OPEN ACCESS TO DATA AND CODE,0.8513289036544851,"Guidelines:
802"
OPEN ACCESS TO DATA AND CODE,0.8521594684385382,"• The answer NA means that the paper does not include experiments.
803"
OPEN ACCESS TO DATA AND CODE,0.8529900332225914,"• The experimental setting should be presented in the core of the paper to a level of detail
804"
OPEN ACCESS TO DATA AND CODE,0.8538205980066446,"that is necessary to appreciate the results and make sense of them.
805"
OPEN ACCESS TO DATA AND CODE,0.8546511627906976,"• The full details can be provided either with the code, in appendix, or as supplemental
806"
OPEN ACCESS TO DATA AND CODE,0.8554817275747508,"material.
807"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.856312292358804,"7. Experiment Statistical Significance
808"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8571428571428571,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
809"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8579734219269103,"information about the statistical significance of the experiments?
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8588039867109635,"Answer: [No]
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8596345514950167,"Justification:
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8604651162790697,"Guidelines:
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8612956810631229,"• The answer NA means that the paper does not include experiments.
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8621262458471761,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8629568106312292,"dence intervals, or statistical significance tests, at least for the experiments that support
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8637873754152824,"the main claims of the paper.
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8646179401993356,"• The factors of variability that the error bars are capturing should be clearly stated (for
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8654485049833887,"example, train/test split, initialization, random drawing of some parameter, or overall
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8662790697674418,"run with given experimental conditions).
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.867109634551495,"• The method for calculating the error bars should be explained (closed form formula,
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8679401993355482,"call to a library function, bootstrap, etc.)
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8687707641196013,"• The assumptions made should be given (e.g., Normally distributed errors).
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8696013289036545,"• It should be clear whether the error bar is the standard deviation or the standard error
824"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8704318936877077,"of the mean.
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8712624584717608,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.872093023255814,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8729235880398671,"of Normality of errors is not verified.
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8737541528239202,"• For asymmetric distributions, the authors should be careful not to show in tables or
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8745847176079734,"figures symmetric error bars that would yield results that are out of range (e.g. negative
830"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8754152823920266,"error rates).
831"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8762458471760798,"• If error bars are reported in tables or plots, The authors should explain in the text how
832"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8770764119601329,"they were calculated and reference the corresponding figures or tables in the text.
833"
EXPERIMENTS COMPUTE RESOURCES,0.877906976744186,"8. Experiments Compute Resources
834"
EXPERIMENTS COMPUTE RESOURCES,0.8787375415282392,"Question: For each experiment, does the paper provide sufficient information on the com-
835"
EXPERIMENTS COMPUTE RESOURCES,0.8795681063122923,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
836"
EXPERIMENTS COMPUTE RESOURCES,0.8803986710963455,"the experiments?
837"
EXPERIMENTS COMPUTE RESOURCES,0.8812292358803987,"Answer: [Yes]
838"
EXPERIMENTS COMPUTE RESOURCES,0.8820598006644518,"Justification: We only have synthetic experiments and it can be reproduced on CPUs.
839"
EXPERIMENTS COMPUTE RESOURCES,0.882890365448505,"Guidelines:
840"
EXPERIMENTS COMPUTE RESOURCES,0.8837209302325582,"• The answer NA means that the paper does not include experiments.
841"
EXPERIMENTS COMPUTE RESOURCES,0.8845514950166113,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
842"
EXPERIMENTS COMPUTE RESOURCES,0.8853820598006644,"or cloud provider, including relevant memory and storage.
843"
EXPERIMENTS COMPUTE RESOURCES,0.8862126245847176,"• The paper should provide the amount of compute required for each of the individual
844"
EXPERIMENTS COMPUTE RESOURCES,0.8870431893687708,"experimental runs as well as estimate the total compute.
845"
EXPERIMENTS COMPUTE RESOURCES,0.8878737541528239,"• The paper should disclose whether the full research project required more compute
846"
EXPERIMENTS COMPUTE RESOURCES,0.8887043189368771,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
847"
EXPERIMENTS COMPUTE RESOURCES,0.8895348837209303,"didn’t make it into the paper).
848"
CODE OF ETHICS,0.8903654485049833,"9. Code Of Ethics
849"
CODE OF ETHICS,0.8911960132890365,"Question: Does the research conducted in the paper conform, in every respect, with the
850"
CODE OF ETHICS,0.8920265780730897,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
851"
CODE OF ETHICS,0.8928571428571429,"Answer: [Yes]
852"
CODE OF ETHICS,0.893687707641196,"Justification:
853"
CODE OF ETHICS,0.8945182724252492,"Guidelines:
854"
CODE OF ETHICS,0.8953488372093024,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
855"
CODE OF ETHICS,0.8961794019933554,"• If the authors answer No, they should explain the special circumstances that require a
856"
CODE OF ETHICS,0.8970099667774086,"deviation from the Code of Ethics.
857"
CODE OF ETHICS,0.8978405315614618,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
858"
CODE OF ETHICS,0.8986710963455149,"eration due to laws or regulations in their jurisdiction).
859"
BROADER IMPACTS,0.8995016611295681,"10. Broader Impacts
860"
BROADER IMPACTS,0.9003322259136213,"Question: Does the paper discuss both potential positive societal impacts and negative
861"
BROADER IMPACTS,0.9011627906976745,"societal impacts of the work performed?
862"
BROADER IMPACTS,0.9019933554817275,"Answer: [Yes]
863"
BROADER IMPACTS,0.9028239202657807,"Justification: This paper studies contextual dueling bandits with adversarial feedback. Our
864"
BROADER IMPACTS,0.9036544850498339,"primary objective is to propel advancements in bandit theory by introducing a more robust
865"
BROADER IMPACTS,0.904485049833887,"algorithm backed by solid theoretical guarantees. The uncertainty-weighted approach
866"
BROADER IMPACTS,0.9053156146179402,"we have developed for dueling bandits holds significant potential to address the issue of
867"
BROADER IMPACTS,0.9061461794019934,"adversarial feedback in preference-based data, which could be instrumental in enhancing the
868"
BROADER IMPACTS,0.9069767441860465,"robustness of generative models against adversarial attacks, thereby contributing positively
869"
BROADER IMPACTS,0.9078073089700996,"to the societal impact and reliability of machine learning applications.
870"
BROADER IMPACTS,0.9086378737541528,"Guidelines:
871"
BROADER IMPACTS,0.909468438538206,"• The answer NA means that there is no societal impact of the work performed.
872"
BROADER IMPACTS,0.9102990033222591,"• If the authors answer NA or No, they should explain why their work has no societal
873"
BROADER IMPACTS,0.9111295681063123,"impact or why the paper does not address societal impact.
874"
BROADER IMPACTS,0.9119601328903655,"• Examples of negative societal impacts include potential malicious or unintended uses
875"
BROADER IMPACTS,0.9127906976744186,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
876"
BROADER IMPACTS,0.9136212624584718,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
877"
BROADER IMPACTS,0.9144518272425249,"groups), privacy considerations, and security considerations.
878"
BROADER IMPACTS,0.915282392026578,"• The conference expects that many papers will be foundational research and not tied
879"
BROADER IMPACTS,0.9161129568106312,"to particular applications, let alone deployments. However, if there is a direct path to
880"
BROADER IMPACTS,0.9169435215946844,"any negative applications, the authors should point it out. For example, it is legitimate
881"
BROADER IMPACTS,0.9177740863787376,"to point out that an improvement in the quality of generative models could be used to
882"
BROADER IMPACTS,0.9186046511627907,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
883"
BROADER IMPACTS,0.9194352159468439,"that a generic algorithm for optimizing neural networks could enable people to train
884"
BROADER IMPACTS,0.920265780730897,"models that generate Deepfakes faster.
885"
BROADER IMPACTS,0.9210963455149501,"• The authors should consider possible harms that could arise when the technology is
886"
BROADER IMPACTS,0.9219269102990033,"being used as intended and functioning correctly, harms that could arise when the
887"
BROADER IMPACTS,0.9227574750830565,"technology is being used as intended but gives incorrect results, and harms following
888"
BROADER IMPACTS,0.9235880398671097,"from (intentional or unintentional) misuse of the technology.
889"
BROADER IMPACTS,0.9244186046511628,"• If there are negative societal impacts, the authors could also discuss possible mitigation
890"
BROADER IMPACTS,0.925249169435216,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
891"
BROADER IMPACTS,0.9260797342192691,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
892"
BROADER IMPACTS,0.9269102990033222,"feedback over time, improving the efficiency and accessibility of ML).
893"
SAFEGUARDS,0.9277408637873754,"11. Safeguards
894"
SAFEGUARDS,0.9285714285714286,"Question: Does the paper describe safeguards that have been put in place for responsible
895"
SAFEGUARDS,0.9294019933554817,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
896"
SAFEGUARDS,0.9302325581395349,"image generators, or scraped datasets)?
897"
SAFEGUARDS,0.9310631229235881,"Answer: [NA]
898"
SAFEGUARDS,0.9318936877076412,"Justification:
899"
SAFEGUARDS,0.9327242524916943,"Guidelines:
900"
SAFEGUARDS,0.9335548172757475,"• The answer NA means that the paper poses no such risks.
901"
SAFEGUARDS,0.9343853820598007,"• Released models that have a high risk for misuse or dual-use should be released with
902"
SAFEGUARDS,0.9352159468438538,"necessary safeguards to allow for controlled use of the model, for example by requiring
903"
SAFEGUARDS,0.936046511627907,"that users adhere to usage guidelines or restrictions to access the model or implementing
904"
SAFEGUARDS,0.9368770764119602,"safety filters.
905"
SAFEGUARDS,0.9377076411960132,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
906"
SAFEGUARDS,0.9385382059800664,"should describe how they avoided releasing unsafe images.
907"
SAFEGUARDS,0.9393687707641196,"• We recognize that providing effective safeguards is challenging, and many papers do
908"
SAFEGUARDS,0.9401993355481728,"not require this, but we encourage authors to take this into account and make a best
909"
SAFEGUARDS,0.9410299003322259,"faith effort.
910"
LICENSES FOR EXISTING ASSETS,0.9418604651162791,"12. Licenses for existing assets
911"
LICENSES FOR EXISTING ASSETS,0.9426910299003323,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
912"
LICENSES FOR EXISTING ASSETS,0.9435215946843853,"the paper, properly credited and are the license and terms of use explicitly mentioned and
913"
LICENSES FOR EXISTING ASSETS,0.9443521594684385,"properly respected?
914"
LICENSES FOR EXISTING ASSETS,0.9451827242524917,"Answer: [NA]
915"
LICENSES FOR EXISTING ASSETS,0.9460132890365448,"Justification:
916"
LICENSES FOR EXISTING ASSETS,0.946843853820598,"Guidelines:
917"
LICENSES FOR EXISTING ASSETS,0.9476744186046512,"• The answer NA means that the paper does not use existing assets.
918"
LICENSES FOR EXISTING ASSETS,0.9485049833887044,"• The authors should cite the original paper that produced the code package or dataset.
919"
LICENSES FOR EXISTING ASSETS,0.9493355481727574,"• The authors should state which version of the asset is used and, if possible, include a
920"
LICENSES FOR EXISTING ASSETS,0.9501661129568106,"URL.
921"
LICENSES FOR EXISTING ASSETS,0.9509966777408638,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
922"
LICENSES FOR EXISTING ASSETS,0.9518272425249169,"• For scraped data from a particular source (e.g., website), the copyright and terms of
923"
LICENSES FOR EXISTING ASSETS,0.9526578073089701,"service of that source should be provided.
924"
LICENSES FOR EXISTING ASSETS,0.9534883720930233,"• If assets are released, the license, copyright information, and terms of use in the
925"
LICENSES FOR EXISTING ASSETS,0.9543189368770764,"package should be provided. For popular datasets, paperswithcode.com/datasets
926"
LICENSES FOR EXISTING ASSETS,0.9551495016611296,"has curated licenses for some datasets. Their licensing guide can help determine the
927"
LICENSES FOR EXISTING ASSETS,0.9559800664451827,"license of a dataset.
928"
LICENSES FOR EXISTING ASSETS,0.9568106312292359,"• For existing datasets that are re-packaged, both the original license and the license of
929"
LICENSES FOR EXISTING ASSETS,0.957641196013289,"the derived asset (if it has changed) should be provided.
930"
LICENSES FOR EXISTING ASSETS,0.9584717607973422,"• If this information is not available online, the authors are encouraged to reach out to
931"
LICENSES FOR EXISTING ASSETS,0.9593023255813954,"the asset’s creators.
932"
NEW ASSETS,0.9601328903654485,"13. New Assets
933"
NEW ASSETS,0.9609634551495017,"Question: Are new assets introduced in the paper well documented and is the documentation
934"
NEW ASSETS,0.9617940199335548,"provided alongside the assets?
935"
NEW ASSETS,0.9626245847176079,"Answer: [NA]
936"
NEW ASSETS,0.9634551495016611,"Justification:
937"
NEW ASSETS,0.9642857142857143,"Guidelines:
938"
NEW ASSETS,0.9651162790697675,"• The answer NA means that the paper does not release new assets.
939"
NEW ASSETS,0.9659468438538206,"• Researchers should communicate the details of the dataset/code/model as part of their
940"
NEW ASSETS,0.9667774086378738,"submissions via structured templates. This includes details about training, license,
941"
NEW ASSETS,0.967607973421927,"limitations, etc.
942"
NEW ASSETS,0.96843853820598,"• The paper should discuss whether and how consent was obtained from people whose
943"
NEW ASSETS,0.9692691029900332,"asset is used.
944"
NEW ASSETS,0.9700996677740864,"• At submission time, remember to anonymize your assets (if applicable). You can either
945"
NEW ASSETS,0.9709302325581395,"create an anonymized URL or include an anonymized zip file.
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9717607973421927,"14. Crowdsourcing and Research with Human Subjects
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9725913621262459,"Question: For crowdsourcing experiments and research with human subjects, does the paper
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.973421926910299,"include the full text of instructions given to participants and screenshots, if applicable, as
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9742524916943521,"well as details about compensation (if any)?
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9750830564784053,"Answer: [NA]
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9759136212624585,"Justification:
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767441860465116,"Guidelines:
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775747508305648,"• The answer NA means that the paper does not involve crowdsourcing nor research with
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.978405315614618,"human subjects.
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.979235880398671,"• Including this information in the supplemental material is fine, but if the main contribu-
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800664451827242,"tion of the paper involves human subjects, then as much detail as possible should be
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9808970099667774,"included in the main paper.
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817275747508306,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825581395348837,"or other labor should be paid at least the minimum wage in the country of the data
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9833887043189369,"collector.
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9842192691029901,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9850498338870431,"Subjects
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858803986710963,"Question: Does the paper describe potential risks incurred by study participants, whether
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9867109634551495,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
965"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9875415282392026,"approvals (or an equivalent approval/review based on the requirements of your country or
966"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883720930232558,"institution) were obtained?
967"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.989202657807309,"Answer: [NA]
968"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9900332225913622,"Justification:
969"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908637873754153,"Guidelines:
970"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916943521594684,"• The answer NA means that the paper does not involve crowdsourcing nor research with
971"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9925249169435216,"human subjects.
972"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9933554817275747,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
973"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941860465116279,"may be required for any human subjects research. If you obtained IRB approval, you
974"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9950166112956811,"should clearly state this in the paper.
975"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9958471760797342,"• We recognize that the procedures for this may vary significantly between institutions
976"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9966777408637874,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
977"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9975083056478405,"guidelines for their institution.
978"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9983388704318937,"• For initial submissions, do not include any information that would break anonymity (if
979"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9991694352159468,"applicable), such as the institution conducting the review.
980"
