Section,Section Appearance Order,Paragraph
ROBERT BOSCH CENTRE FOR DATA SCIENCE AND ARTIFICIAL INTELLIGENCE,0.0,"1 Robert Bosch Centre for Data Science and Artificial Intelligence
2 Department of Computer Science and Engineering
Indian Institute of Technology Madras, Chennai, India
sekarnet@gmail.com, ravi@cse.iitm.ac.in"
ABSTRACT,0.002145922746781116,ABSTRACT
ABSTRACT,0.004291845493562232,"We study a contextual bandit setting where the learning agent has the ability to
perform interventions on targeted subsets of the population, apart from possess-
ing qualitative causal side-information. This novel formalism captures intricacies
in real-world scenarios such as software product experimentation where targeted
experiments can be conducted. However, this fundamentally changes the set of
options that the agent has, compared to standard contextual bandit settings, ne-
cessitating new techniques. This is also the first work that integrates causal side-
information in a contextual bandit setting, where the agent aims to learn a policy
that maps contexts to arms (as opposed to just identifying one best arm). We pro-
pose a new algorithm, which we show empirically performs better than baselines
on experiments that use purely synthetic data and on real world-inspired experi-
ments. We also prove a bound on regret that theoretically guards performance."
INTRODUCTION,0.006437768240343348,"1
INTRODUCTION"
INTRODUCTION,0.008583690987124463,"Contextual bandits have been used as natural frameworks to model interactive decision making sce-
narios such as recommendation systems (Liu et al., 2018), marketing campaign allocation (Sawant
et al., 2018) and more (Bouneffouf & Rish, 2019). In this framework, the learning agent repeatedly
interacts with an environment with the aim of learning a near-optimal decision policy that maps a
context space to a set of actions (also referred to as arms or interventions1). In the standard stochas-
tic variant, in each round of interaction, the agent observes a context from the environment and,
once the agent chooses an action, the environment returns a sampled reward that is a function of the
current context and chosen action (Lattimore & Szepesv´ari, 2020). The agent’s objective is to mini-
mize some meaningful measure of closeness of the policy to optimality (see Lattimore & Szepesv´ari
(2020) for some standard definitions)."
INTRODUCTION,0.01072961373390558,"One of the key issues in the wide application of contextual bandits (and reinforcement learning, in
general (Dulac-Arnold et al., 2021)) is their need for a large number of samples that are costly to
actualize in practice. For example, each arm might correspond to performing a product experiment
on users or to conducting a specific medical intervention. However, we show that there are nuances
in real-world situations which – though not fully captured by the standard formulations of contextual
bandits – if modeled and leveraged, allow us to build methods that can improve the rate at which
good policies can be identified."
INTRODUCTION,0.012875536480686695,"A motivating example Consider a sales-assistance software agent that is learning to suggest a
campaign that is optimal for a given sales lead (defined by a set of features). With software products,
there is often an opportunity to conduct experiments with variants of the software on defined subsets
of the population of users (each specified by a set of characteristics, or context variables) to learn
about resulting metrics; see Google (2021) for an example. We can think of these experiments as
constituting a training phase. More specifically, the agent, for example, can conduct a campaign"
INTRODUCTION,0.015021459227467811,"1We use the term intervention here because actions or arms in a (contextual) bandit setting can be interpreted
as Pearl do() interventions on a causal model (Zhang & Bareinboim, 2017; Lattimore et al., 2016). See Pearl
(2009; 2019) for more discussion on the do() operation."
INTRODUCTION,0.017167381974248927,Published as a conference paper at ICLR 2022
INTRODUCTION,0.019313304721030045,"(i.e., the intervention) on a specific subset of users (for example, the context value os=iOS could
define a subset) and observe the outcome. Thus, instead of necessarily performing an intervention
on a randomly sampled user coming from the natural population distribution, the agent can instead
choose to target the intervention on a randomly selected user with specific characteristics (as defined
by an assignment of context variable values). This type of interventions, which we call targeted
interventions, fundamentally changes the set of options that the agent has in every training round."
INTRODUCTION,0.02145922746781116,"In addition, the training phase often happens in a more lenient environment where the agent might
have access to auxiliary context variables (such as IT-spend) that are unavailable in the evaluation
phase. Further, there is also causal side-information sometimes available. For instance, we might
know that emailsubject causes openemail, and not the other way around. Encoding this
qualitative side-information as a causal graph can help the agent make use of this structure. Latti-
more et al. (2016) and Yabe et al. (2018) demonstrate this in the best-arm identification case. After
training, there is an evaluation phase (e.g., when the agent is deployed), where the agent observes
the main context variables and decides an action; its regret performance is measured at this point."
INTRODUCTION,0.023605150214592276,"Our framework Our framework captures the above intricacies (which are not restricted to software
product experimentation). The formal treatment is in Section 2; we present an overview here. The
agent has T rounds of interaction that act as the training phase, followed by a (T + 1)’th round on
which it is evaluated. In each of the T training rounds, it has the choice to perform either a targeted
intervention or a standard interaction. Further, in addition to the main context variables, the agent
can observe a (possibly empty) set of additional auxiliary context variables during training; these
auxiliary context variables are not observable during evaluation. All variables are made available
to the agent at the end of every training round (this is similar to Lattimore et al. (2016)). The
agent also has access to a causal graph that models the qualitative causal relationships between the
variables; there are very few assumptions made on this graph (discussed in Section 2.1). The causal
graph allows a factorized representation of the joint distribution of the variables, and as a result,
enables information leakage – i.e., updating beliefs about several interventions after every single
intervention. Also, importantly, we allow context variables to be categorical, and therefore the usual
assumptions that enable generalization of the learned policy across contexts, such as linearity (e.g.,
in Dimakopoulou et al. (2019)), become invalid."
INTRODUCTION,0.02575107296137339,"The agent’s problem is one of sample allocation – how to allocate samples across the T training
rounds so as to learn a policy that minimizes regret in the (T + 1)’th round that represents the
evaluation phase. In the evaluation phase, the agent observes the main context variables from the
environment, against which it chooses an action and receives a reward – much like in a standard
contextual bandit setting. Since targeted interventions are restricted to the training phase, it intro-
duces a difference between the training and evaluation phases. The agent’s challenge, therefore, is
to learn policies that minimize regret in the evaluation phase using samples it collects in the training
phase (from a combination of standard interactions and targeted interventions). The algorithm we
propose utilizes a novel entropy-like measure called Unc that guides this sample allocation in a way
that also exploits the information leakage."
CONTRIBUTIONS,0.027896995708154508,"1.1
CONTRIBUTIONS"
CONTRIBUTIONS,0.030042918454935622,"In this paper, we formalize this modified setting, which we call “causal contextual bandits with tar-
geted interventions”, provide a novel algorithm and show both theoretical and experimental results.
Specifically, our contributions are:"
CONTRIBUTIONS,0.032188841201716736,"• We formalize the more nuanced contextual bandit setting described above (Sec. 2). This
is the first work that we know of that formulates a contextual bandit setting with causal
side-information. This is also the first paper we are aware of that introduces targeted inter-
ventions in a contextual bandit setting."
CONTRIBUTIONS,0.034334763948497854,• We propose a new algorithm based on minimizing a novel entropy-like measure (Sec. 3.1)
CONTRIBUTIONS,0.03648068669527897,"• We prove a bound on its regret, providing a theoretical guard on its performance (Sec. 3.2)"
CONTRIBUTIONS,0.03862660944206009,"• We show results of experiments that use purely synthetic data (Sec. 4.1). The results
demonstrate that our algorithm performs better than baselines."
CONTRIBUTIONS,0.0407725321888412,"• We also run experiments that are inspired by proprietary data from Freshworks Inc. (Sec.
4.2), providing evidence of our algorithm’s performance in more realistic scenarios as well."
CONTRIBUTIONS,0.04291845493562232,Published as a conference paper at ICLR 2022
CONTRIBUTIONS,0.045064377682403435,"Our motivation comes from real world settings where experiments are costly; therefore, we are
interested in empirical behavior when the training budget T is relatively small."
RELATED WORK,0.04721030042918455,"1.2
RELATED WORK"
RELATED WORK,0.04935622317596566,"Causal bandits have been studied in literature recently (Lattimore et al., 2016; Sen et al., 2017;
Yabe et al., 2018; Lu et al., 2020) and they leverage causal side-information to transfer knowledge
across interventions. They, however, have been studied only in a best arm identification setting,
with each arm modeled as an intervention on a set of variables; the objective is to learn a single
best arm. Our setting differs significantly since the agent attempts to learn a policy (by which we
mean a mapping from contexts to arms) instead of a single best arm. Therefore, while the agent can
perform targeted interventions that specify both the context and action, it is still attempting to learn
an optimal context-action mapping."
RELATED WORK,0.05150214592274678,"The contextual bandit literature is well-studied (see Zhou (2016) and Lattimore & Szepesv´ari (2020)
for surveys), and has taken various approaches to enable knowledge transfer across contexts. For
example, Dimakopoulou et al. (2019) assume expected reward is a linear function of context, while
Agarwal et al. (2014) make assumptions about the existence of a certain oracle. However, there has
not been work that has looked at contextual bandits which utilize causal side-information as a way
to transfer knowledge across contexts, or considered targeted interventions in the space of options
for the agent. Our method utilizes the information leakage that the causal graph provides to not just
learn about other interventions after every intervention, but also to be smarter about the choice of
targeted interventions to conduct."
RELATED WORK,0.0536480686695279,"Another related area is active learning (Settles, 2009). In active learning, a supervised learning agent
gets the option to query an oracle actively to obtain the true labels for certain data points. However,
it is in the supervised learning setting where the agent receives true label feedback, whereas in our
setting the agent only receives bandit feedback (that is, only for the action that was taken). Never-
theless, our work can be thought of as infusing some elements of the active learning problem into the
contextual bandits setting by providing the agent the ability to perform targeted interventions. There
has been some work investigating the intersection of causality and bandits with the aim of transfer
learning. Zhang & Bareinboim (2017) study transfer of knowledge from offline data in the presence
of unobserved confounders, in a non-contextual setting. As can be seen, this differs significantly
from our setting."
FORMALISM,0.055793991416309016,"2
FORMALISM"
FORMALISM,0.05793991416309013,"We assume that the underlying environment is modeled as a causal model M, which is defined
by a directed acyclic graph G over all variables – i.e., the variable to be intervened (X), the reward
variable (Y ), and the set of context variables (C) – and a joint probability distribution P that factorizes
over G (Pearl, 2009; Koller & Friedman, 2009). C is partitioned into the set of main context variables
(Ctar) and the set of auxiliary context variables (Cother). G is sometimes called the causal graph
or causal diagram of M. Each variable takes on a finite, known set of values; note that this is quite
generic, and allows for categorical variables. An intervention do(X = x) on M involves removing
all arrows from the parents of X into X, and setting X = x (Pearl, 2009). The agent has access only
to G and not to M; therefore, the agent is not given any knowledge a priori about the underlying
conditional probability distributions (CPDs) of the variables."
FORMALISM,0.060085836909871244,"We specify a targeted intervention X = x conditioned on context Ctar = ctar succinctly by
(x, ctar). While standard interventions (Pearl, 2009) result in a distribution of the form P(. | do(x)),
targeted interventions result in P(. | do(x), Ctar = ctar). Table 1 summarizes the key notation used
in this paper."
FORMALISM,0.06223175965665236,"The agent is allowed T training rounds of interaction with the environment, at the end of which it
aims to have learned a policy ˆϕ : val(Ctar) →val(X). Specifically, in each of the T training rounds,
the agent can choose to either"
FORMALISM,0.06437768240343347,"• (Standard interaction) Observe context ctar ∼P(Ctar), choose intervention x, and observe
(y, cother) ∼P(Y, Cother | do(x), ctar), (or)"
FORMALISM,0.06652360515021459,Published as a conference paper at ICLR 2022
FORMALISM,0.06866952789699571,Table 1: Summary of key notation
FORMALISM,0.07081545064377683,Symbol/Notation Meaning
FORMALISM,0.07296137339055794,"X
variable that will be intervened upon
Y
reward variable
Ctar, Cother
set of main context variables and set of auxiliary context variables, respec-
tively; so the set of all context variables is C = Ctar ∪Cother = {..., Ci, ...}.
Capital letters
a random variable; e.g., C1 or X
Small letters
a random variable’s value; e.g., c1 or x
Small bold font
a vector of random variable values; e.g., ctar denotes a specific choice of
values taken by variables in Ctar
(x, ctar)
specifies targeted intervention X = x on subset defined by Ctar = ctar
ˆP, ˆE
estimate of distribution P and expectation E based on current beliefs
θV |paV
beliefs (vector of Dirichlet parameters) about parameters of P(V |paV );
θ(v)
V |paV is the entry corresponding to V = v.
val(V ), val(V)
set of values taken by the variable V , and set of variables V, respectively.
NV , NV
= |val(V )|, |val(V)|
paV
value of variables in PAV , the parents of V . For Y , we let PAY denote its
parents excluding X to keep proof easier to read.
T
number of training rounds
α
fraction of rounds in Phase 1 of training
a⟨B⟩
if a is an assignment of values to A, then a⟨B⟩is assignment of those values
to respective variables B ⊆A.
Set operations
for readability, we use them on vectors as well, where there is no ambiguity."
FORMALISM,0.07510729613733906,"• (Targeted intervention) Choose intervention x and a target subset given by context values
Ctar = ctar, and observe (y, cother) ∼P(Y, Cother | do(x), ctar)"
FORMALISM,0.07725321888412018,"where, when there is no ambiguity, we use P interchangeably to mean either the joint distribution
or a marginal distribution. Note that in both modes of interaction above, the intervention is only on
X; they differ, however, in whether the agent observes the context values from the environment or
whether it chooses the context values on which to condition the intervention."
FORMALISM,0.07939914163090128,"After training, the agent is evaluated in the (T + 1)’th round. Here, the agent is presented a query
context ctar ∼P(Ctar) to which it responds with an action x = ˆϕ(ctar) using the learned policy,
and receives a reward y ∼P(Y | do(x), ctar) from the environment."
FORMALISM,0.0815450643776824,"The objective of the agent is to learn a policy ˆϕ that minimizes simple-regret, which is defined as:"
FORMALISM,0.08369098712446352,"Regret ≜
X"
FORMALISM,0.08583690987124463,"ctar
[µ∗
ctar −ˆµctar] · P(ctar) =
X"
FORMALISM,0.08798283261802575,"ctar
Regret(ctar) · P(ctar)"
FORMALISM,0.09012875536480687,"where ϕ∗is an optimal policy, µ∗
ctar ≜E[Y |do(ϕ∗(ctar)), ctar] and ˆµctar ≜E[Y |do(ˆϕ(ctar)), ctar]."
ASSUMPTIONS,0.09227467811158799,"2.1
ASSUMPTIONS"
ASSUMPTIONS,0.0944206008583691,"We are interested in DAGs G where there is an edge X →Y , and no other directed path between
X and Y ; that is, X directly affects Y ; there is no context variable caused by X. This models the
natural situation we encounter in settings such as a recommendation system where click-through
rate (Y ) of a user is directly affected by the user’s features (the context) and the recommendation
(X); the recommendation does not cause any contexts, so doesn’t have a path to Y through them.
We assume that there are no unobserved confounders (UCs); but purely interactive bandit settings,
such as ours, are robust to direct UCs between X and Y since actions can be interpreted as sampling
from the P(.|do(x)) distribution (Bareinboim et al., 2015; Guo et al., 2020). We make an additional
assumption to simplify the factorization in Section 3.2: (A1) {C confounds C′ ∈Ctar and Y } =⇒
C ∈Ctar. A sufficient condition for (A1) to be true is if Ctar is ancestral (i.e., Ctar contains all its
ancestors). This assumption does not preclude UCs (for example, the graph used in the synthetic
experiments in Section 4.1 could have a UC between X and Y ), and can be relaxed in the future."
ASSUMPTIONS,0.09656652360515021,Published as a conference paper at ICLR 2022
SOLUTION APPROACH,0.09871244635193133,"3
SOLUTION APPROACH"
SOLUTION APPROACH,0.10085836909871244,"3.1
ALGORITHM (UNC CCB)"
SOLUTION APPROACH,0.10300429184549356,Algorithm 1a: Training phase of Unc CCB
SOLUTION APPROACH,0.10515021459227468,"Data: Graph G; fraction α of Phase 1 rounds.
Initialization: For all V ∈C ∪{Y }, set θV |paV = (1, ..., 1), for all values of paV ."
SOLUTION APPROACH,0.1072961373390558,1 (Phase 1) for t = 1 . . . ⌈αT⌉do
SOLUTION APPROACH,0.10944206008583691,"2
Observe context ctar ∼P(Ctar)"
SOLUTION APPROACH,0.11158798283261803,"3
Choose x ∈val(X) uniformly at random"
SOLUTION APPROACH,0.11373390557939914,"4
Observe (y, cother) ∼P(Y, Cother | do(x), ctar)"
FOR V IN C DO,0.11587982832618025,"5
for V in C do"
FOR V IN C DO,0.11802575107296137,"6
updateBeliefs(V, c⟨PAV ⟩, c⟨V ⟩) //denote ctar ∪cother by c"
END,0.12017167381974249,"7
end"
END,0.1223175965665236,"8
updateBeliefs(Y, (c⟨PAY ⟩, x), c⟨Y ⟩)"
END,0.12446351931330472,9 end
END,0.12660944206008584,"10 (Phase 2) for t = ⌈αT⌉+ 1, . . . , T do"
END,0.12875536480686695,"11
Choose x, ctar as:"
END,0.13090128755364808,"arg min
x∈val(X), ctar∈val(Ctar) 
  X"
END,0.13304721030042918,"x′∈val(X), ctar′∈val(Ctar)
Unc
 
E[Y |do(x′), ctar′]
 x, ctar

 "
END,0.1351931330472103,"12
Do targeted intervention (x, ctar) and observe (y, cother) ∼P(Y, Cother|do(x), ctar)"
FOR V IN COTHER DO,0.13733905579399142,"13
for V in Cother do"
FOR V IN COTHER DO,0.13948497854077252,"14
updateBeliefs(V, c⟨PAV ⟩, c⟨V ⟩)"
END,0.14163090128755365,"15
end"
END,0.14377682403433475,"16
updateBeliefs(Y, (x, c⟨PAY ⟩), c⟨Y ⟩)"
END,0.1459227467811159,17 end
END,0.148068669527897,"Result: Final set of beliefs for all V, paV :

..., θV |paV , ..."
END,0.15021459227467812,"18 Procedure updateBeliefs(V, paV , v)"
END,0.15236051502145923,"19
θ(v)
V |paV ←θ(v)
V |paV + 1"
END,0.15450643776824036,Algorithm 1b: Evaluation phase of Unc CCB
END,0.15665236051502146,"Data: Graph G, learned beliefs

..., θV |paV , ...
	
, user context to be decided ctar"
END,0.15879828326180256,"1 for every V, paV do"
END,0.1609442060085837,"2
for v ∈val(V ) do"
END,0.1630901287553648,"3
Set ˆP(V = v|paV ) =
θ(v)
V |paV
P"
END,0.16523605150214593,"v′ θ(v′)
V |paV
4
end"
END,0.16738197424892703,5 end
END,0.16952789699570817,6 for x ∈val(X) do
END,0.17167381974248927,"7
Compute ˆψ(x, ctar) ≜ˆE[Y |do(x), ctar] using ˆP in Equation (2)"
END,0.17381974248927037,8 end
END,0.1759656652360515,"Result: Return ˆϕ(ctar) ≜arg maxx ˆψ(x, ctar)"
END,0.1781115879828326,"We call our proposed algorithm Unc CCB. The training phase of Unc CCB (given as Algorithm
1a) consists of two phases. In each round of Phase 1 (a fraction α of the rounds), it observes a
context from the environment, then uniformly at random chooses an action, and finally observes the
remaining variables. The observed values are used to perform standard Bayesian updates over the
beliefs about parameters of all relevant CPDs. Since we do not assume any a priori beliefs for the
agent, this uniform exploration helps the agent enter Phase 2 with reasonable starting beliefs."
END,0.18025751072961374,Published as a conference paper at ICLR 2022
END,0.18240343347639484,"In Phase 2 (the remaining (1 −α) fraction of the rounds) of training, the agent tries to allocate tar-
geted interventions optimally. To this end, it needs to trade-off between exploring new contexts and
gaining more knowledge about already-explored contexts, while taking into account the information
leakage resulting from the shared pathways in the causal graph (or equivalently, shared CPDs in the
factorized representation of P; Appendix E discusses a related subcase). The novel, entropy-like,
Unc measure (discussed below) helps guide the agent in this decision in every round. Algorithm 1b
specifies the evaluation phase of Unc CCB, where the agent is evaluated on simple regret."
END,0.18454935622317598,"Intuition behind the Unc measure As mentioned before, performing a targeted intervention given
by (x, ctar) would allow the agent to update its beliefs about effects of other targeted interventions
(x′, ctar′). Intuitively, we would want the algorithm to allocate more samples to those targeted inter-
ventions that are most informative about the most “valuable” (in terms of reward Y ) targeted inter-
ventions. Unc captures this intuition by providing a measure of the effect on the agent’s knowledge
of E[Y |do(x′), ctar′] when a targeted intervention (x, ctar) is performed. The agent, then aggregates
this over all possible (x′, ctar′), providing a measure of overall resulting knowledge from targeted
intervention (x, ctar); this helps guide its sample allocation (see Line 11 of Algorithm 1a)."
END,0.18669527896995708,"Definition of the Unc measure We model the conditional distribution P(V |paV ) for any variable
V as a categorical distribution whose parameters are sampled from a Dirichlet distribution. That
is, P(V |paV ) = Cat(V ; b1, ..., br), where (b1, ..., br) ∼Dirc(θV |paV ). Here θV |paV is a vector of
length, say, r. Let denote θV |paV [i] denote the i’th entry of θV |paV . We define an object called Ent
that captures a measure of our knowledge of the CPD:"
END,0.1888412017167382,"Ent(P(V |paV )) ≜−
X i"
END,0.19098712446351931,"""
θV |paV [i]
P
j θV |paV [j] ln"
END,0.19313304721030042,"θV |paV [i]
P
j θV |paV [j] !#"
END,0.19527896995708155,"Let the parents of V be PAV = (U1, ..., Up); suppose they take a particular set of values paV =
(u1, ..., up). We define that a CPD P(V |paV ) is unaffected by targeted intervention (x, ctar) if there
exists i ∈{1, ..., p} such that either (1) Ui is X and ui ̸= x, or (2) Ui ∈Ctar and ctar⟨Ui⟩̸= ui.
In other words, the CPD P(V |paV ) is unaffected by (x, ctar) if doing this targeted intervention and
observing all variables (x, ctar, cother) does not enable us to update beliefs about P(V |paV ) using
knowledge of G. Now, define"
END,0.19742489270386265,"Ent(P(V |paV )|x, ctar) ≜
Ent(P(V |paV )), if P(V |paV ) is unaffected by (x, ctar)
Entnew(P(V |paV )), otherwise"
END,0.19957081545064378,"where Entnew is computed by averaging the resulting Ent values over the possible belief up-
dates the agent might make after performing (x, ctar). That is, Entnew(P(V |paV )) = 1 r
P"
END,0.2017167381974249,"i Ent(
Cat(b′
1, ..., b′
r)) where (b′
1, ..., b′
r) ∼Dirc
 
..., θV |paV [i −1], θV |paV [i] + 1, θV |paV [i + 1], ...

."
END,0.20386266094420602,"Letting c′ denote ctar′ ∪cother′, we can now define Unc which captures our knowledge of
E[Y |do(x′), ctar′] if we perform some other targeted intervention given by (x, ctar):"
END,0.20600858369098712,"Unc
 
E[Y |do(x′), ctar′]
x, ctar
≜
X"
END,0.20815450643776823,"cother′∈val(Cother)  
X"
END,0.21030042918454936,"V ∈Cother
Ent(P(V |c′⟨PAV ⟩)|x, ctar)+"
END,0.21244635193133046,"Ent(P(Y |x′, c′⟨PAY ⟩)|x, ctar)

· ˆP(c′) · ˆE[Y |c′, do(x′)]"
REGRET BOUND,0.2145922746781116,"3.2
REGRET BOUND"
REGRET BOUND,0.2167381974248927,"Theorem 3.1. For any 0 < δ < 1, with probability ≥1 −δ,"
REGRET BOUND,0.21888412017167383,"Regret ≤3EpaY ,ctar  "
REGRET BOUND,0.22103004291845493,"v
u
u
t"
REGRET BOUND,0.22317596566523606,"""
2
αT
NX P(paY , ctar) −ϵT
X,P AY #"
REGRET BOUND,0.22532188841201717,"ln
2NX(NC + |C|) δ   + 3
X"
REGRET BOUND,0.22746781115879827,"C∈Cother
EpaC,ctar  "
REGRET BOUND,0.2296137339055794,"v
u
u
t"
REGRET BOUND,0.2317596566523605,"""
2
αTP(paC, ctar) −ϵT
P AC #"
REGRET BOUND,0.23390557939914164,"ln
2(NC + |C|) δ  
(1)"
REGRET BOUND,0.23605150214592274,Published as a conference paper at ICLR 2022 where
REGRET BOUND,0.23819742489270387,"ϵT
P AC = sαT 2"
REGRET BOUND,0.24034334763948498,"
ln
NP AC(NC + |C|) δ"
REGRET BOUND,0.24248927038626608,"
, ϵT
X,P AY = sαT 2"
REGRET BOUND,0.2446351931330472,"
ln
NXNP AY (NC + |C|) δ "
REGRET BOUND,0.24678111587982832,"Proof. Due to space constraints, we only provide a proof sketch here. The full proof is provided in
Appendix A as part of the Supplementary Text. For readability of the proof, we assume that Y and
all variables in C are binary, whereas X can take on any finite set of values. Under Assumption (A1)
described in Section 2.1, we can factorize as follows:"
REGRET BOUND,0.24892703862660945,"E[Y |do(x), ctar] =
X"
REGRET BOUND,0.2510729613733906,cother∈val(Cother)
REGRET BOUND,0.2532188841201717,"
P(Y = 1|x, c⟨PAY ⟩)
Y"
REGRET BOUND,0.2553648068669528,"c∈cother
P(C = c|c⟨PAC⟩)

(2)"
REGRET BOUND,0.2575107296137339,"where c denotes ctar∪cother. The proof first bounds errors of estimates of ˆP(Y = 1|x, c⟨PAY ⟩) and
ˆP(C = c|c⟨PAC⟩) with high probability using union bounds, concentration bounds, and the fact
that the number of rounds in which the agent observes any set of context values is lower bounded by
the number of times it sees it in Phase 1. We also make use of the fact that all variables are observed
after every round. We then aggregate these bounds of individual CPD estimates to bound the overall
estimate of ˆE[Y |do(x), ctar], and then finally bound regret by utilizing the fact that the algorithm
chooses arms based on the estimated ˆE."
REGRET BOUND,0.259656652360515,"Discussion Theorem 3.1 bounds the regret (with high probability). Note that T →∞
=⇒
Regret →0, which shows that the regret bound has desirable limiting behavior.2 Further, regret is
inversely related to
√"
REGRET BOUND,0.26180257510729615,"T, similar to other simple-regret algorithms such as in Lattimore et al. (2016).
However, other terms that are a function of the space of possible interventions (NCNX) are different
since we are in a contextual bandit setting (whereas Lattimore et al. (2016) is non-contextual); more
specifically, regret is related proportionally to
p"
REGRET BOUND,0.26394849785407726,"NXNC ln (NXNC); in fact, our regret bound is
strictly tighter than this. We prove the bound primarily to provide a theoretical guard on regret; we
do not claim improved regret bounds since there is no comparable simple-regret contextual bandit
regret bound. We show improved performance over baselines empirically in Section 4."
EXPERIMENTAL EVALUATION,0.26609442060085836,"4
EXPERIMENTAL EVALUATION"
EXPERIMENTAL EVALUATION,0.26824034334763946,"Since there is no setting studied previously that directly maps to our setting, we adapt existing
multi-armed bandit algorithms to make use of causal side-information and targeted interventions in
a variety of ways to form a set of baselines against which we compare our algorithm’s performance."
EXPERIMENTAL EVALUATION,0.2703862660944206,"The baseline algorithms are the following. Std TS and Std UniExp follow the standard bandit
interaction protocol during training – observe context, choose action, receive reward. They treat
each ctar as a separate problem and learn an optimal arm for each; they differ in that Std TS uses
Thompson Sampling (Agrawal & Goyal, 2012) for each ctar problem, whereas Std UniExp does
uniform exploration over val(X). The other set of algorithms are TargInt TS, TargInt UniExp
and TargInt TS UniExp. These treat the space val(X) × val(Ctar) as the set of arms for targeted
interventions. TargInt TS performs Thompson Sampling over this space, and TargInt UniExp per-
forms uniform exploration over this space. TargInt TS UniExp is a variation of TargInt TS that,
with some probability,3 chooses a random targeted intervention; this infuses a controlled degree
of uniform exploration into TargInt TS. All five algorithms update CPD beliefs after each round.
No baseline is strictly stronger than another; their relative performance can vary based on the spe-
cific setting. We also compare against a non-causal version of algorithm Std TS, which we call
NonCausal Std TS, to provide one non-causal baseline (i.e., which does not utilize the causal
side-information)."
EXPERIMENTAL EVALUATION,0.27253218884120173,"The evaluation phase for each baseline algorithm is analogous to Algorithm 1b – given a ctar, the
action with the highest expected reward, based on beliefs at the end of the training phase, is chosen."
EXPERIMENTAL EVALUATION,0.27467811158798283,"2There is a technical requirement to keep the denominator in Eq (1) greater than 0; however, it is easy to see
that this only requires that T > constant, so there is no impact on asymptotic behavior.
3We choose this probability to be 0.5 for the plots. There is more discussion on this in Appendix F."
EXPERIMENTAL EVALUATION,0.27682403433476394,Published as a conference paper at ICLR 2022
EXPERIMENTAL EVALUATION,0.27896995708154504,"Figure 1: Mean regrets for Experiment 1 (Sec-
tion 4.1). Regret is normalized to [0, 1] and T
as fractions of NXNCtar."
EXPERIMENTAL EVALUATION,0.2811158798283262,"Figure 2: Mean regrets for Experiment 2 (Sec-
tion 4.1)."
PURELY SYNTHETIC DATA,0.2832618025751073,"4.1
PURELY SYNTHETIC DATA"
PURELY SYNTHETIC DATA,0.2854077253218884,"We consider a causal model M whose causal graph G consists of the following edges: C1 →
C0, C0 →X, C0 →Y, X →Y . We assume that Ctar = {C1} and that Cother = {C0}. We
choose the fraction of Phase 1 rounds α = 0.5 as we empirically found that to be a good choice.
More details and code are in the Supplementary Material. Additional experiments, analyzing the
reason for why our algorithm performs better than baselines, are in Appendix B."
PURELY SYNTHETIC DATA,0.2875536480686695,"Experiment 1 Given the wide range of possible settings arising from different parameterizations
of M, the first experiment seeks to answer the question: “what is the expected performance of our
algorithm over a set of representative settings?”. Each setting can be interpreted naturally when,
for example, we think of Ctar as the set of user features that a recommendation agent observes
post-deployment. The settings are chosen to capture the intuition that, typically, the agent sees high-
value contexts (i.e., contexts for which, if the agent learns the optimal action, can fetch high rewards)
relatively less frequently, say, 20% of the time, but there can be variation in the number of different
Ctar values over which that probability mass is spread and in how “risky” they are (for example,
very low rewards for non-optimal actions). In each run, the agent is presented with a randomly
selected setting from this representative set. Results are averaged over 300 independent runs; error
bars display ±1.96 standard errors. For details on specific parameterizations, refer Appendix D."
PURELY SYNTHETIC DATA,0.28969957081545067,"Figure 1 provides the results of this experiment. The axes are normalized since the representative
set of settings could have different ranges for T and for regret. We see that our algorithm performs
better than all baselines, especially for lower values of T. This, in fact, is a recurring theme – our
algorithm’s performance improvement is more pronounced in low T-budget situations, which, as
we stated in Section 1.1, is what we are interested in. Further, Appendix C contains plots for two
individual settings from the set, one where Std TS and Std UniExp perform significantly better
than TargInt TS, TargInt UniExp and TargInt TS UniExp, and another where it is the converse;
in contrast, our algorithm performs close to the best baseline in both. The intuition behind this is
that the Unc measure incentivizes exploring new context values (due to its entropy-like nature) while
weighting more the contexts that are likely to yield more rewards (based on our current estimates).
Also, as expected, NonCausal Std TS performs the worst as it ignores the causal graph."
PURELY SYNTHETIC DATA,0.2918454935622318,"Experiment 2 To ensure that the results are not biased due to our choice of the representative set, the
second experiment asks “what happens to performance when we directly randomize the parameters
of the CPDs in each run, subject to realistic constraints?”. Specifically, in each run, we (1) randomly
pick an i ∈{1, ..., ⌊NC1/2⌋}, (2) distribute 20% of the probability mass randomly over C1 values
{1, ..., i}, (3) distribute the remaining 80% of the mass over C1 values {i+1, ..., NC1}. The c1 values
in the 20% bucket are higher-value, while those in the 80% bucket are lower-value. Intuitively, this
captures the commonly observed 80-20 pattern (for example, 20% of the users contribute to around
80% of the revenue), but randomizes the other aspects; this gives an estimate of how the algorithms
would perform on expectation. The results are averaged over 100 independent runs; error bars
display ±1.96 standard errors. Figure 2 shows that our algorithm performs better than all baselines"
PURELY SYNTHETIC DATA,0.2939914163090129,Published as a conference paper at ICLR 2022
PURELY SYNTHETIC DATA,0.296137339055794,"in this experiment, with a more pronounced improvement for the smaller values of T. For instance,
when T = 24, our algorithm’s mean regret is around 35% lower than that of the next best baseline."
CRM SALES DATA-INSPIRED EXPERIMENT,0.2982832618025751,"4.2
CRM SALES DATA-INSPIRED EXPERIMENT"
CRM SALES DATA-INSPIRED EXPERIMENT,0.30042918454935624,"Experiment 3 This experiment seeks to answer the question: “how does the algorithm perform
on realistic scenarios?”. We setup the experiment inspired by a real world scenario. Consider a
bandit agent that could assist salespeople by learning to decide how many outgoing calls to make
in an ongoing deal, given just the type of deal (new business, etc.) and size of customer (small,
medium, etc.), so as to maximize a reward metric (which we call the ‘expected deal value’). Deal
type affects the reward via the source of the lead (for example, chat). The trained agent would be
deployed internally or to the company’s clients, where it would generally not have access to lead
source. The causal graph relating these variables is given in Figure 3a, which was obtained using
domain knowledge. ‘Deal type’ and ‘Customer size’ are Ctar, ‘Lead source’ is Cother, ‘# outgoing
calls’ is X, and ‘Exp. deal value’ is Y . The parameters of each CPD corresponding to this causal
graph was calibrated using proprietary sales data from Freshworks Inc., after some adjustments
using domain knowledge (for example, where there were coverage gaps). Note that the algorithms
use only the CPDs and not this raw data; in any case, we make the dataset available in anonymized
form at https://doi.org/10.5281/zenodo.5540348 under the ‘CC BY-NC 4.0’ license."
CRM SALES DATA-INSPIRED EXPERIMENT,0.30257510729613735,"Further, we also do a warm-start for all algorithms to account for the fact that, in practice, there is
often some starting beliefs about the CPDs from past data, domain knowledge, etc., which can be
encoded into the agent’s prior beliefs.4 Specifically, we ran pure random exploration for 15 rounds
at the beginning of each algorithm and updated all CPD beliefs; this simulates a warm-start. Due to
this, we used α = 0 for our algorithm. Results are averaged over 50 independent runs; error bars
display ±1.96 standard errors. Figure 3 shows that our algorithm performs better than all baselines
in this realistic setting as well. The Supplementary Material provides the code."
CRM SALES DATA-INSPIRED EXPERIMENT,0.30472103004291845,"(a) Causal graph.
(b) Mean regrets."
CRM SALES DATA-INSPIRED EXPERIMENT,0.30686695278969955,Figure 3: CRM sales data-inspired experiments (Section 4.2)
CONCLUSION AND FUTURE DIRECTIONS,0.3090128755364807,"5
CONCLUSION AND FUTURE DIRECTIONS"
CONCLUSION AND FUTURE DIRECTIONS,0.3111587982832618,"This work presented a contextual bandits formulation that captures real-world nuances such as the
ability to conduct targeted interventions and the presence of causal side-information, along with
a novel algorithm that exploits this to achieve improved sample efficiency. In addition to synthetic
experiments, we also performed real world-inspired experiments set up using actual CRM sales data."
CONCLUSION AND FUTURE DIRECTIONS,0.3133047210300429,"A useful direction of future investigation is the development of algorithms when G is unknown or
partially known. Another important direction is the development of methods robust to distributional
shifts. Distributional shifts have been studied for prediction tasks (Magliacane et al., 2018; Gong
et al., 2016) and for causal effects (Bareinboim & Pearl, 2014; Correa & Bareinboim, 2020); it is
an interesting question to study this in our setting, when G remains the same between training and
evaluation but M changes."
CONCLUSION AND FUTURE DIRECTIONS,0.315450643776824,4Other works such as Dimakopoulou et al. (2019) and Liu et al. (2018) have used warm starting as well.
CONCLUSION AND FUTURE DIRECTIONS,0.31759656652360513,Published as a conference paper at ICLR 2022
CONCLUSION AND FUTURE DIRECTIONS,0.3197424892703863,ACKNOWLEDGEMENTS
CONCLUSION AND FUTURE DIRECTIONS,0.3218884120171674,"This work was partly funded by Freshworks Inc. through a research grant to Balaraman Ravindran
and Chandrasekar Subramanian."
REPRODUCIBILITY STATEMENT,0.3240343347639485,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3261802575107296,"The full proof of Theorem 3.1 is provided in an appendix at the end of this PDF file (after the ref-
erences) as part of the Supplementary Text. The source code of all the experiments in the main
paper, along with a README on how to run them, is provided as part of the Supplementary Ma-
terials in a file named Supplemental Code Paper1203.zip. Further, even though the al-
gorithms use only the CPDs and not the raw data, the dataset used to calibrate the real world-
inspired experiments (see Section 4.2 of main paper) is made available in anonymized form at
https://doi.org/10.5281/zenodo.5540348 under the ‘CC BY-NC 4.0’ license. The
other experiments were synthetic and did not involve the use of any dataset."
ETHICS STATEMENT,0.3283261802575107,ETHICS STATEMENT
ETHICS STATEMENT,0.33047210300429186,"This work is a foundational work that provides an improved mathematical framework and algorithm
for contextual bandits. Some experiments were set up utilizing data from Freshworks Inc.; the data
is released in anonymized form with the consent of the company (link to dataset given in the main
paper). To the best of our knowledge, we do not believe there were any ethical issues associated with
the development of this work. Further, given the nature of the work as foundational and introducing
a new algorithm (and not specific to an application), we do not foresee any specific potential negative
ethical issues created by this work. However, we do point out that researchers utilizing this method to
their specific applications should adhere to ethical standards of their own (e.g., by avoiding targeting
interventions on subpopulations based on racial attributes)."
ETHICS STATEMENT,0.33261802575107297,Published as a conference paper at ICLR 2022
REFERENCES,0.33476394849785407,REFERENCES
REFERENCES,0.3369098712446352,"Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the Monster: A Fast and Simple Algorithm for Contextual Bandits. In Proceedings of the 31st
International Conference on Machine Learning, volume 32 of PMLR, pp. 1638–1646, 2014."
REFERENCES,0.33905579399141633,"Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit prob-
lem. In Proceedings of the 25th Annual Conference on Learning Theory, volume 23 of PMLR,
pp. 39.1–39.26, 2012."
REFERENCES,0.34120171673819744,"Elias Bareinboim and Judea Pearl. Transportability from multiple environments with limited exper-
iments: Completeness results. In Proceedings of the 27th International Conference on Neural
Information Processing Systems, volume 1, pp. 280–288, 2014."
REFERENCES,0.34334763948497854,"Elias Bareinboim, Andrew Forney, and Judea Pearl. Bandits with unobserved confounders: A causal
approach. In Proceedings of the 28th International Conference on Neural Information Processing
Systems, volume 1, pp. 1342–1350, 2015."
REFERENCES,0.34549356223175964,"Djallel Bouneffouf and Irina Rish. A survey on practical applications of multi-armed and contextual
bandits. arXiv:1904.10040, 2019."
REFERENCES,0.34763948497854075,"Juan D. Correa and Elias Bareinboim. A Calculus for Stochastic Interventions: Causal Effect Iden-
tification and Surrogate Experiments. In Proceedings of the AAAI Conference on Artificial Intel-
ligence, volume 34, 2020."
REFERENCES,0.3497854077253219,"Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, and Guido Imbens. Balanced Linear Con-
textual Bandits. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp.
3445–3453, 2019."
REFERENCES,0.351931330472103,"Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,
and Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and
analysis. Machine Learning, Apr 2021."
REFERENCES,0.3540772532188841,"Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Sch¨olkopf. Domain Adaptation with Conditional Transferable Components. In Proceedings of
the 33rd International Conference on Machine Learning, volume 48, pp. 2839–2848, 2016."
REFERENCES,0.3562231759656652,"Google.
Targeting overview.
https://support.google.com/optimize/answer/
6283420, 2021. Accessed: 2021-10-02."
REFERENCES,0.3583690987124464,"Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn, and Huan Liu. A survey of learning
causality with data: Problems and methods. ACM Comput. Surv., 53(4), July 2020."
REFERENCES,0.3605150214592275,"Daphne Koller and Nir Friedman. Probabilistic Graphical Models: principles and techniques. MIT
Press, 2009."
REFERENCES,0.3626609442060086,"Finnian Lattimore, Tor Lattimore, and Mark D. Reid. Causal bandits: Learning good interventions
via causal inference. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, volume 29, pp. 1189–1197, 2016."
REFERENCES,0.3648068669527897,"Tor Lattimore and Csaba Szepesv´ari. Bandit Algorithms. Cambridge University Press, 2020."
REFERENCES,0.3669527896995708,"Bo Liu, Ying Wei, Yu Zhang, Zhixian Yan, and Qiang Yang. Transferable contextual bandit for
cross-domain recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, pp. 3619–3626, 2018."
REFERENCES,0.36909871244635195,"Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, and William Yan. Regret analysis of bandit prob-
lems with causal background knowledge. In Proceedings of the 36th Conference on Uncertainty
in Artificial Intelligence (UAI), volume 124 of PMLR, pp. 141–150, 2020."
REFERENCES,0.37124463519313305,"Sara Magliacane, Thijs Van Ommen, Tom Claassen, Stephan Bongers, Joris M. Mooij, and Philip
Versteeg. Domain adaptation by using causal inference to predict invariant conditional distribu-
tions. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 10869–10879, 2018."
REFERENCES,0.37339055793991416,Published as a conference paper at ICLR 2022
REFERENCES,0.37553648068669526,"Judea Pearl. Causality. Cambridge University Press, 2nd edition, 2009."
REFERENCES,0.3776824034334764,"Judea Pearl. On the Interpretation of do(x). Journal of Causal Inference, 7(1), 2019."
REFERENCES,0.3798283261802575,"Neela Sawant, Chitti Babu Namballa, Narayanan Sadagopan, and Houssam Nassif.
Contextual
multi-armed bandits for causal marketing. arXiv:1810.01859, 2018."
REFERENCES,0.38197424892703863,"Rajat Sen, Karthikeyan Shanmugam, Alexandres G. Dimakis, and Sanjay Shakkottai. Identifying
best interventions through online importance sampling. In Proceedings of the 34th International
Conference on Machine Learning, volume 70 of PMLR, pp. 3057–3066, 2017."
REFERENCES,0.38412017167381973,"Burr Settles.
Active learning literature survey.
Technical Report 1648, University of Wiscon-
sin–Madison, 2009."
REFERENCES,0.38626609442060084,"Akihiro Yabe, Daisuke Hatano, Hanna Sumita, Shinji Ito, Naonori Kakimura, Takuro Fukunaga, and
Ken-ichi Kawarabayashi. Causal Bandits with Propagating Inference. In Proceedings of the 35th
International Conference on Machine Learning, volume 80 of PMLR, pp. 5512–5520, 2018."
REFERENCES,0.388412017167382,"Junzhe Zhang and Elias Bareinboim. Transfer learning in multi-armed bandits: A causal approach.
In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pp.
1340–1346, 2017."
REFERENCES,0.3905579399141631,"Li Zhou. A survey on contextual multi-armed bandits. arXiv:1508.03326, 2016."
REFERENCES,0.3927038626609442,Published as a conference paper at ICLR 2022
REFERENCES,0.3948497854077253,"A
APPENDIX (SUPPLEMENTARY TEXT): PROOF OF THEOREM 3.1"
REFERENCES,0.3969957081545064,"This is the full version of the proof sketch presented in Section 3.2 of the paper. For readability of
the proof, we assume that Y and all variables in C are binary, whereas X can have any finite set
of values. In Sections A.1 through A.5, we derive the expression for bounding Regret(ctar), which
is the simple regret given Ctar = ctar. Then, in Section A.6, we derive the final expression for
bounding overall Regret."
REFERENCES,0.39914163090128757,"In addition to the notation in Table 1 of the main paper, we use a few other notations to keep the
proof more readable:"
REFERENCES,0.4012875536480687,"• c denotes ctar ∪cother.
• a⟨B⟩is shortened to b when it is unambiguous what a is; for example, c⟨PAV ⟩is shortened
to paV when it is obvious that c is the current set of values taken by all the context variables.
• We write P(V = v|paV ) interchangeably with P(v|paV ) since we use small letters (e.g.,
v) to denote the value taken by the random variable denoted by the respective capital letter
(e.g., V )."
REFERENCES,0.4034334763948498,"Now, the factorization under Assumption (A1) can be written in this simpler notation as:"
REFERENCES,0.4055793991416309,"E[Y |do(x), ctar] =
X"
REFERENCES,0.40772532188841204,"cother∈val(Cother)
P(Y = 1|x, paY )
Y"
REFERENCES,0.40987124463519314,"c∈cother
P(c|paC)"
REFERENCES,0.41201716738197425,"Recollect that we use set operations on vectors as well, wherever there is no ambiguity."
REFERENCES,0.41416309012875535,"A.1
BOUNDING THE ERROR OF P(Y = 1|x, paY ) ESTIMATES"
REFERENCES,0.41630901287553645,"Let paY denote the value taken by PAY (i.e., Y ’s parents), excluding x. Say, for a (x, paY ),"
REFERENCES,0.4184549356223176,"P
h
|ˆP(Y = 1|x, paY ) −P(Y = 1|x, paY )| ≥ϵx,paY
i
≤δx,paY"
REFERENCES,0.4206008583690987,"Using the union bound, we can rewrite it as"
REFERENCES,0.4227467811158798,"P
h
∀x, |ˆP(Y = 1|X, paY ) −P(Y = 1|X, paY )| ≥ϵX,paY
i
≤δX,paY"
REFERENCES,0.4248927038626609,"In other words, with probability ≥1 −δX,paY , the following event EX,paY is true:"
REFERENCES,0.4270386266094421,"|∀x, ˆP(Y = 1|X, paY ) −P(Y = 1|X, paY )| ≤ϵX,paY
We derive expressions for ϵX,paY and δX,paY in Lemma A.2."
REFERENCES,0.4291845493562232,"A.2
BOUNDING THE ERROR OF P(C = c|PAC) ESTIMATES"
REFERENCES,0.4313304721030043,"Now, say, for a C ∈C (taking value C = c), and for given paC,"
REFERENCES,0.4334763948497854,"P
h
|ˆP(c|paC) −P(c|paC)| ≥ϵc|paC
i
≤δc|paC"
REFERENCES,0.4356223175965665,"Note that ˆP(0|paC) + ˆP(1|paC) = 1 and P(0|paC) + P(1|paC) = 1. Now, say the actual errors in
estimates are given by ˜ϵc. That is, ˜ϵc ≜ˆP(c|paC) −P(c|paC). Therefore,"
REFERENCES,0.43776824034334766,ˆP(0|paC) + ˆP(1|paC) = 1 ⇐⇒P(0|paC) + ˜ϵ0|paC + P(1|paC) + ˜ϵ1|paC = 1
REFERENCES,0.43991416309012876,⇐⇒1 + ˜ϵ0|paC + ˜ϵ1|paC = 1
REFERENCES,0.44206008583690987,⇐⇒˜ϵ0|paC + ˜ϵ1|paC = 0
REFERENCES,0.44420600858369097,"Now, this means that
|˜ϵ0|paC| ≤a ⇐⇒|˜ϵ1|paC| ≤a"
REFERENCES,0.44635193133047213,"Therefore, the event |˜ϵ0|paC| ≤a is the same event as the event |˜ϵ1|paC| ≤a. Therefore, given a
paC, the following is true:"
REFERENCES,0.44849785407725323,"P

∀c, |˜ϵc|paC| ≤ϵC|paC

≥1 −δC|paC"
REFERENCES,0.45064377682403434,Published as a conference paper at ICLR 2022
REFERENCES,0.45278969957081544,which is the same as
REFERENCES,0.45493562231759654,"P
h
∀c, |ˆP(c|paC) −P(c|paC)| ≤ϵC|paC
i
≥1 −δC|paC"
REFERENCES,0.4570815450643777,where δC|paC is a function of ϵC|paC. Note that we now have δC|paC instead of δc|paC.
REFERENCES,0.4592274678111588,"In other words, given a paC, with probability ≥1 −δC|paC, the following event EC|paC is true:"
REFERENCES,0.4613733905579399,"∀c, |ˆP(C = c|paC) −P(C = c|paC)| ≤ϵC|paC
We derive expressions for δC|paC and ϵC|paC in Lemma A.1."
REFERENCES,0.463519313304721,"A.3
PUTTING THEM TOGETHER FOR THE OVERALL BOUND FOR REGRET(ctar)"
REFERENCES,0.4656652360515021,"Using the union bound, for any ctar, with probability ≥1 −P
paY δX,paY −P C∈C
P"
REFERENCES,0.4678111587982833,"paC δC|paC,
all of EX,paY and EC|paC are true simultaneously ∀paY and ∀paC ∀C ∈(C \ PAY ). Let’s call this
event E. In other words, under event E, all estimates are bounded."
REFERENCES,0.4699570815450644,"Let aalg = ˆϕ(ctar) denote the action chosen by the algorithm in the evaluation phase when presented
with ctar; and let a∗denote an optimal action for ctar. Therefore, given Ctar = ctar, under event E,"
REFERENCES,0.4721030042918455,"E[Y |do(aalg), ctar] =
X"
REFERENCES,0.4742489270386266,"cother
P(Y = 1|aalg, paY )
Y"
REFERENCES,0.47639484978540775,"c∈cother
P(c|paC)"
REFERENCES,0.47854077253218885,"(Factorization due to G as earlier) =
X"
REFERENCES,0.48068669527896996,"cother
P(Y = 1|aalg, paY )
Y"
REFERENCES,0.48283261802575106,"c∈cother
{ˆP(c|paC) −˜ϵC|paC}"
REFERENCES,0.48497854077253216,"(Since ˜ϵC|paC are actual errors.) ≥
X"
REFERENCES,0.4871244635193133,"cother
{ˆP(Y = 1|aalg, paY ) −ϵX,paY }
Y"
REFERENCES,0.4892703862660944,"c∈cother
{ˆP(c|paC) −˜ϵC|paC}"
REFERENCES,0.49141630901287553,(Due to event E)
REFERENCES,0.49356223175965663,"≥ˆP(Y = 1|do(aalg), ctar) −
X"
REFERENCES,0.4957081545064378,C∈Cother∪Y X
REFERENCES,0.4978540772532189,"cother
P(cother|ctar)ϵC|paC"
REFERENCES,0.5,"x=aalg
(Algebra discussed separately in Section A.3.1)"
REFERENCES,0.5021459227467812,"= ˆE[Y |do(aalg), ctar] −
X"
REFERENCES,0.5042918454935622,C∈Cother∪Y X
REFERENCES,0.5064377682403434,"paC
P(paC|ctar)ϵC|paC"
REFERENCES,0.5085836909871244,"x=aalg
(Since Y is binary)"
REFERENCES,0.5107296137339056,"≥ˆE[Y |do(a∗), ctar] −
X"
REFERENCES,0.5128755364806867,C∈Cother∪Y X
REFERENCES,0.5150214592274678,"paC
P(paC|ctar)ϵC|paC"
REFERENCES,0.5171673819742489,"x=aalg
(Due to proposed algorithm)"
REFERENCES,0.51931330472103,"Continuing and applying similar steps, we get"
REFERENCES,0.5214592274678111,"E[Y |do(aalg), ctar] ≥E[Y |do(a∗), ctar] −2
X"
REFERENCES,0.5236051502145923,"paY
P(paY |ctar)ϵX,paY −3
X"
REFERENCES,0.5257510729613734,C∈Cother X
REFERENCES,0.5278969957081545,"paC
P(paC|ctar)ϵC|paC"
REFERENCES,0.5300429184549357,"Defining
ϵ′
X ≜
X"
REFERENCES,0.5321888412017167,"paY
P(paY |ctar)ϵX,paY"
REFERENCES,0.5343347639484979,"and
ϵ′
C ≜
X"
REFERENCES,0.5364806866952789,"paC
P(paC|ctar)ϵC|paC"
REFERENCES,0.5386266094420601,Published as a conference paper at ICLR 2022
REFERENCES,0.5407725321888412,"we get that,"
REFERENCES,0.5429184549356223,"E[Y |do(aalg), ctar] ≥E[Y |do(a∗), ctar] −2ϵ′
X −3
X"
REFERENCES,0.5450643776824035,"C∈Cother
ϵ′
C"
REFERENCES,0.5472103004291845,"Therefore, under event E (that is, with probability ≥1 −P"
REFERENCES,0.5493562231759657,"paY δX,paY −P C∈C
P"
REFERENCES,0.5515021459227468,"paC δC|paC), for
any given ctar,"
REFERENCES,0.5536480686695279,"Regret(ctar) = E[Y |do(a∗), ctar] −E[Y |do(aalg), ctar] ≤2ϵ′
X + 3
X"
REFERENCES,0.555793991416309,"C∈Cother
ϵ′
C
(3)"
REFERENCES,0.5579399141630901,"We derive expressions for each of these terms in Sections A.4 and A.5, and then arrive at the final
bound in Section A.6."
REFERENCES,0.5600858369098712,"A.3.1
ALGEBRA FOR INTERMEDIATE EXPRESSION"
REFERENCES,0.5622317596566524,"We wish to show that
X"
REFERENCES,0.5643776824034334,"cother
{ˆP(Y = 1|x, paY ) −ϵX,paY }
Y"
REFERENCES,0.5665236051502146,"c∈cother
{ˆP(c|paC) −˜ϵC|paC}"
REFERENCES,0.5686695278969958,"≥ˆP(Y = 1|do(x), ctar) −
X"
REFERENCES,0.5708154506437768,C∈Cother∪Y X
REFERENCES,0.572961373390558,"cother
P(cother|ctar)ϵC|paC"
REFERENCES,0.575107296137339,Let us denote the expression on the left hand side by (A).
REFERENCES,0.5772532188841202,"Let ci ∈cother chosen in a reverse topological order; this ensures that a child is chosen before any
of its parents. Let Ci be the corresponding variable in Cother. Then we can write"
REFERENCES,0.5793991416309013,"(A) =
X"
REFERENCES,0.5815450643776824,"cother\ci ""X ci"
REFERENCES,0.5836909871244635,"h
{ˆP(Y = 1|x, paY ) −ϵX,paY }{ˆP(ci|paCi) −˜ϵci|paCi}
i Y"
REFERENCES,0.5858369098712446,"c′∈cother\ci
{ˆP(c′|paC′) −˜ϵc′|paC′}   =
X"
REFERENCES,0.5879828326180258,"cother\ci ""X ci"
REFERENCES,0.5901287553648069,"h
ˆP(Y = 1|x, paY )ˆP(ci|paCi) −˜ϵci|paCi ˆP(Y = 1|x, paY )"
REFERENCES,0.592274678111588,"−ϵX,paY ˆP(ci|paCi) + ϵX,paY ˜ϵci|paCi ii
Y"
REFERENCES,0.5944206008583691,"c′∈cother\ci
{ˆP(c′|paC′) −˜ϵc′|paC′} ≥
X"
REFERENCES,0.5965665236051502,cother\ci
REFERENCES,0.5987124463519313,"h
ˆQCi[Y = 1|x, paCi] −ϵCi|paCi −ˆϵX,paCi i
Y"
REFERENCES,0.6008583690987125,"c′∈cother\ci
{ˆP(c′|paC′) −˜ϵc′|paC′} where"
REFERENCES,0.6030042918454935,"ˆQD[Y = 1|x, c] ≜
X"
REFERENCES,0.6051502145922747,"d
ˆP(Y = 1|x, paY )
Y"
REFERENCES,0.6072961373390557,"d∈d
{ˆP(d|paD, c)}"
REFERENCES,0.6094420600858369,"and
ˆϵX,paCi ≜
X"
REFERENCES,0.6115879828326181,"ci
ϵX,paY P(ci|paCi)"
REFERENCES,0.6137339055793991,"These make use of the fact that ˜ϵci|paCi ≤ϵCi|paCi, ˜ϵ0|paCi = −˜ϵ1|paCi and that ˆP ≤1. We can keep
reapplying the above steps in reverse topological order till we exhaust all ci, and we’ll get"
REFERENCES,0.6158798283261803,"(A) ≥ˆQCother[Y = 1|x, ctar] −
X"
REFERENCES,0.6180257510729614,C∈Cother∪Y X
REFERENCES,0.6201716738197425,"cother
P(cother|ctar)ϵC|paC"
REFERENCES,0.6223175965665236,"Noting that ˆQCother[Y = 1|x, ctar] = ˆP(Y = 1|do(x), ctar) gives us the desired inequality."
REFERENCES,0.6244635193133047,Published as a conference paper at ICLR 2022
REFERENCES,0.6266094420600858,"A.4
EXPRESSIONS FOR δC|paC AND ϵC|paC"
REFERENCES,0.628755364806867,Lemma A.1. P 
REFERENCES,0.630901287553648,"∀c, |ˆP(c|paC) −P(c|paC)| ≤"
REFERENCES,0.6330472103004292,"v
u
u
t"
REFERENCES,0.6351931330472103,"""
2
T ′P(paC, ctar) −ϵT
P AC #"
REFERENCES,0.6373390557939914,"ln

2
δC|paC "
REFERENCES,0.6394849785407726,≥1 −δC|paC
REFERENCES,0.6416309012875536,"Proof. Let TpaC be the set of time indices during training when PAC = paC was chosen/seen by
the algorithm, and let TpaC = |TpaC|. Now, for a C that is observed and not chosen5, note that
our estimate of ˆP(C = 1|paC) is computed as (θ(1)
C|paC + 1)/(TpaC + 2). We approximate6 this as"
REFERENCES,0.6437768240343348,"θ(1)
C|paC/TpaC. Using Hoeffding’s inequality, for any c,"
REFERENCES,0.6459227467811158,"P
h
|ˆP(C = c|paC) −P(C = c|paC)| ≥ϵC|paC
i
≤2 exp

−TpaC"
REFERENCES,0.648068669527897,"2
(ϵC|paC)2
"
REFERENCES,0.6502145922746781,"But since already observed, the two events (corresponding to c = 0 and c = 1) are the same, we
have"
REFERENCES,0.6523605150214592,"P
h
∀c, |ˆP(c|paC) −P(c|paC)| ≤ϵC|paC
i
≥1 −2 exp

−TpaC"
REFERENCES,0.6545064377682404,"2
(ϵC|paC)2
"
REFERENCES,0.6566523605150214,"Therefore, we let"
REFERENCES,0.6587982832618026,"δC|paC ≜2 exp

−TpaC"
REFERENCES,0.6609442060085837,"2
(ϵC|paC)2
"
REFERENCES,0.6630901287553648,"Now, let T ′ ≜αT be the number of rounds of Phase 1. Let ˜TC|paC be the number of times C
was updated in Phase 1 as a results of encountering PAC = paC. Note that the mean of ˜TC|paC is
T ′P(ctar)P(paC|ctar) = T ′P(paC, ctar). By the Hoeffding’s inequality,"
REFERENCES,0.6652360515021459,"P
h
˜TC|paC ≥T ′P(paC, ctar) −ϵT
paC"
REFERENCES,0.6673819742489271,"i
≥1 −exp "
REFERENCES,0.6695278969957081,"−
2(ϵT
paC)2 T ′ !"
REFERENCES,0.6716738197424893,"By the union bound, we have (let’s call the event as ET
C|P AC),"
REFERENCES,0.6738197424892703,"P
h
∀paC, ˜TC|paC ≥T ′P(paC, ctar) −ϵT
P AC
i
≥1 −NP AC exp "
REFERENCES,0.6759656652360515,"−2(ϵT
P AC)2 T ′ !"
REFERENCES,0.6781115879828327,where NP AC = Q
REFERENCES,0.6802575107296137,C∈P AC NC.
REFERENCES,0.6824034334763949,"Letting δT
C|P AC ≜NP AC exp

−
2(ϵT
P AC )2 T ′"
REFERENCES,0.6845493562231759,"
, we have"
REFERENCES,0.6866952789699571,"ϵT
C|P AC ="
REFERENCES,0.6888412017167382,"v
u
u
t T ′ 2 
ln"
REFERENCES,0.6909871244635193,"NP AC
δT
C|P AC !"
REFERENCES,0.6931330472103004,"Note that TC|paC ≥˜TC|paC. Therefore, given that ET
C|P AC is true, we have"
REFERENCES,0.6952789699570815,"P
h
∀c, |ˆP(c|paC) −P(c|paC)| ≤ϵC|paC
i
≥1 −2 exp

−TC|paC"
REFERENCES,0.6974248927038627,"2
(ϵC|paC)2
"
REFERENCES,0.6995708154506438,≥1 −2 exp 
REFERENCES,0.7017167381974249,"−
T ′P(paC, ctar) −ϵT
C|P AC
2
(ϵC|paC)2
!"
REFERENCES,0.703862660944206,"5This means C ∈C during Phase 1 of the algorithm, or C ∈Cother in Phase 2.
6It’s easy to see that this does not change the asymptotic behavior of the bound."
REFERENCES,0.7060085836909872,Published as a conference paper at ICLR 2022
REFERENCES,0.7081545064377682,Defining δC|paC as
REFERENCES,0.7103004291845494,δC|paC ≜2 exp 
REFERENCES,0.7124463519313304,"−
T ′P(paC, ctar) −ϵT
C|P AC
2
(ϵC|paC)2
!"
REFERENCES,0.7145922746781116,gives us
REFERENCES,0.7167381974248928,ϵC|paC =
REFERENCES,0.7188841201716738,"v
u
u
t"
REFERENCES,0.721030042918455,"""
2
T ′P(paC, ctar) −ϵT
C|P AC #"
REFERENCES,0.723175965665236,"ln

2
δC|paC 
(4)"
REFERENCES,0.7253218884120172,"A.5
EXPRESSIONS FOR δX,paY , ϵX,paY , AND ϵ′
x"
REFERENCES,0.7274678111587983,Lemma A.2.
REFERENCES,0.7296137339055794,"P
h
∀x, |ˆP(Y = 1|x, paY ) −P(Y = 1|x, paY )| ≤
v
u
u
t"
REFERENCES,0.7317596566523605,"""
2
T ′
NX P(paY , ctar) −ϵT
X,P AY #"
REFERENCES,0.7339055793991416,"ln
 2NX"
REFERENCES,0.7360515021459227,"δX,paY "
REFERENCES,0.7381974248927039,"≥1 −δX,paY"
REFERENCES,0.740343347639485,"Proof. As before, let Tx,paY be the set of time indices during training when (X, PAY ) = (x, paY )
was chosen/seen by the algorithm, and let Tx,paY = |Tx,paY |. As before, note that our estimate
of ˆP(Y = 1|x, paY ) is computed as (θ(1)
Y |x,paY + 1)/(Tx,paY + 2), which we can approximate as"
REFERENCES,0.7424892703862661,"θ(1)
Y |x,paY /Tx,paY . Now, by Hoeffding’s inequality"
REFERENCES,0.7446351931330472,"P
h
|ˆP(Y = 1|x, paY ) −P(Y = 1|x, paY )| ≥ϵx,paY
i
≤2 exp

−Tx,paY"
REFERENCES,0.7467811158798283,"2
ϵ2
x,paY "
REFERENCES,0.7489270386266095,"During Phase 1, let ˜Tx,paY be the random variable denoting the number of times that (X, PAY ) =
(x, paY ) was seen/chosen. Note that, due to the nature of Phase 1 (i.e., each x is chosen with
probability
1
NX ), the mean of ˜Tx,paY is T ′P(paY , ctar) 1"
REFERENCES,0.7510729613733905,"NX . Therefore, using the union bound and
the Hoeffding’s inequality,"
REFERENCES,0.7532188841201717,"P

∀(x, paY ), ˜Tx,paY ≥T ′P(paY , ctar) 1"
REFERENCES,0.7553648068669528,"NX
−ϵT
X,P AY"
REFERENCES,0.7575107296137339,"
≥1 −NXNP AY exp

−2"
REFERENCES,0.759656652360515,"T ′
 
ϵT
X,P AY
2"
REFERENCES,0.7618025751072961,"Since ∀(x, paY ), Tx,paY ≥˜Tx,paY , we have (let’s call this event ET
X,P AY ),"
REFERENCES,0.7639484978540773,"P

∀(x, paY ), Tx,paY ≥T ′P(paY , ctar) 1"
REFERENCES,0.7660944206008584,"NX
−ϵT
X,P AY"
REFERENCES,0.7682403433476395,"
≥1−NXNP AY exp

−2"
REFERENCES,0.7703862660944206,"T ′
 
ϵT
X,P AY
2"
REFERENCES,0.7725321888412017,"Letting δT
X,P AY ≜NXNP AY exp

−2"
REFERENCES,0.7746781115879828,"T ′
 
ϵT
X,P AY
2
, we have"
REFERENCES,0.776824034334764,"ϵT
X,P AY ="
REFERENCES,0.778969957081545,"v
u
u
t T ′ 2 
ln"
REFERENCES,0.7811158798283262,NXNP AY
REFERENCES,0.7832618025751072,"δT
X,P AY !"
REFERENCES,0.7854077253218884,"Now, we can get that, for a given paY , ∀x, (let’s call it event EX,paY ),"
REFERENCES,0.7875536480686696,"P
h
∀x, |ˆP(Y = 1|x, paY ) −P(Y = 1|x, paY )| ≤ϵX,paY
i"
REFERENCES,0.7896995708154506,"≥1 −
X"
REFERENCES,0.7918454935622318,"x
2 exp

−Tx,paY"
REFERENCES,0.7939914163090128,"2
(ϵX,paY )2
"
REFERENCES,0.796137339055794,≥1 −2NX exp  −
REFERENCES,0.7982832618025751,"T ′
NX P(paY , ctar) −ϵT
X,P AY
2
(ϵX,paY )2
!"
REFERENCES,0.8004291845493562,Published as a conference paper at ICLR 2022
REFERENCES,0.8025751072961373,Setting
REFERENCES,0.8047210300429185,"δX,paY = 2NX exp  −"
REFERENCES,0.8068669527896996,"T ′
NX P(paY , ctar) −ϵT
X,P AY
2
(ϵX,paY )2
!"
REFERENCES,0.8090128755364807,"we have,"
REFERENCES,0.8111587982832618,"ϵX,paY ="
REFERENCES,0.8133047210300429,"v
u
u
t"
REFERENCES,0.8154506437768241,"""
2
T ′
NX P(paY , ctar) −ϵT
X,P AY #"
REFERENCES,0.8175965665236051,"ln
 2NX"
REFERENCES,0.8197424892703863,"δX,paY 
(5)"
REFERENCES,0.8218884120171673,"A.6
DERIVING THE FINAL REGRET BOUND"
REFERENCES,0.8240343347639485,"The probability that events E, ET
P AC(∀C ∈C) and ET
X,P AY are all simultaneously true is"
REFERENCES,0.8261802575107297,"≥1 −
X"
REFERENCES,0.8283261802575107,"paY
δX,paY −δT
X,P AY −
X C∈C  X"
REFERENCES,0.8304721030042919,"paC
δC|paC + δT
P AC  "
REFERENCES,0.8326180257510729,"Substituting Equations (4) and (5) back into Equation (3), we get that, with the above probability,
for a given ctar,"
REFERENCES,0.8347639484978541,Regret(ctar) ≤3  X
REFERENCES,0.8369098712446352,"paY
P(paY |ctar)"
REFERENCES,0.8390557939914163,"v
u
u
t"
REFERENCES,0.8412017167381974,"""
2
T ′
NX P(paY , ctar) −ϵT
X,P AY #"
REFERENCES,0.8433476394849786,"ln
 2NX"
REFERENCES,0.8454935622317596,"δX,paY  +
X"
REFERENCES,0.8476394849785408,C∈Cother X
REFERENCES,0.8497854077253219,"paC
P(paC|ctar)"
REFERENCES,0.851931330472103,"v
u
u
t"
REFERENCES,0.8540772532188842,"""
2
T ′P(paC, ctar) −ϵT
P AC #"
REFERENCES,0.8562231759656652,"ln

2
δC|paC   where"
REFERENCES,0.8583690987124464,"ϵT
C|P AC ="
REFERENCES,0.8605150214592274,"v
u
u
t T ′ 2 
ln"
REFERENCES,0.8626609442060086,"NP AC
δT
C|P AC ! and"
REFERENCES,0.8648068669527897,"ϵT
X,P AY ="
REFERENCES,0.8669527896995708,"v
u
u
t T ′ 2 
ln"
REFERENCES,0.869098712446352,NXNP AY
REFERENCES,0.871244635193133,"δT
X,P AY !"
REFERENCES,0.8733905579399142,"We set ˜δ = δX,paY = δT
X,P AY = δC|paC = δT
P AC, ∀C, paY , paC. Thus, for a given ctar, with
probability ≥1 −NP AY ˜δ −˜δ −NC\P AY ˜δ −|C \ PAY |˜δ,"
REFERENCES,0.8755364806866953,Regret(ctar) ≤3  X
REFERENCES,0.8776824034334764,"paY
P(paY |ctar)"
REFERENCES,0.8798283261802575,"v
u
u
t"
REFERENCES,0.8819742489270386,"""
2
T ′
NX P(paY , ctar) −ϵT
X,P AY #"
REFERENCES,0.8841201716738197,"ln
2NX ˜δ  +
X"
REFERENCES,0.8862660944206009,C∈Cother X
REFERENCES,0.8884120171673819,"paC
P(paC|ctar)"
REFERENCES,0.8905579399141631,"v
u
u
t"
REFERENCES,0.8927038626609443,"""
2
T ′P(paC, ctar) −ϵT
C|P AC # ln
2 ˜δ   where"
REFERENCES,0.8948497854077253,"ϵT
C|P AC = sT ′ 2"
REFERENCES,0.8969957081545065,"
ln
NP AC ˜δ  and"
REFERENCES,0.8991416309012875,"ϵT
X,P AY = sT ′ 2"
REFERENCES,0.9012875536480687,"
ln
NXNP AY ˜δ "
REFERENCES,0.9034334763948498,Published as a conference paper at ICLR 2022
REFERENCES,0.9055793991416309,"Now, noting that NP AY ˜δ + ˜δ + NC\P AY ˜δ + |C \ PAY |˜δ ≤(NC + |C|)˜δ, we set δ = (NC + |C|)˜δ.
So we get that with probability ≥1 −δ,"
REFERENCES,0.907725321888412,Regret(ctar) ≤3  X
REFERENCES,0.9098712446351931,"paY
P(paY |ctar)"
REFERENCES,0.9120171673819742,"v
u
u
t"
REFERENCES,0.9141630901287554,"""
2
αT
NX P(paY , ctar) −ϵT
X,P AY #"
REFERENCES,0.9163090128755365,"ln
2NX(NC + |C|) δ  +
X"
REFERENCES,0.9184549356223176,C∈Cother X
REFERENCES,0.9206008583690987,"paC
P(paC|ctar)"
REFERENCES,0.9227467811158798,"v
u
u
t"
REFERENCES,0.924892703862661,"""
2
αTP(paC, ctar) −ϵT
P AC #"
REFERENCES,0.927038626609442,"ln
2(NC + |C|) δ  
(6) where"
REFERENCES,0.9291845493562232,"ϵT
P AC = sαT 2"
REFERENCES,0.9313304721030042,"
ln
NP AC(NC + |C|) δ  and"
REFERENCES,0.9334763948497854,"ϵT
X,P AY = sαT 2"
REFERENCES,0.9356223175965666,"
ln
NXNP AY (NC + |C|) δ "
REFERENCES,0.9377682403433476,"Substituting Equation (6) into the definition of Regret in Section 2 of the main paper, we get the
expression in Theorem 3.1."
REFERENCES,0.9399141630901288,"B
APPENDIX (SUPPLEMENTARY TEXT): ADDITIONAL EXPERIMENT –
UNDERSTANDING THE REASON FOR BETTER PERFORMANCE"
REFERENCES,0.9420600858369099,"(a) Std UniExp
(b) TargInt UniExp"
REFERENCES,0.944206008583691,(c) Our proposed algorithm
REFERENCES,0.9463519313304721,"Figure 4: Frequency of choosing or encountering each value of Ctar. Highlighted in red color are
the ‘high-value’ contexts (i.e., contexts for which learning the right actions provides higher expected
rewards)."
REFERENCES,0.9484978540772532,"In this analysis, we try to “understand why our algorithm exhibits better performance than baselines.”
We had pointed out intuitively that the Unc measure trades off between exploring new contexts and"
REFERENCES,0.9506437768240343,Published as a conference paper at ICLR 2022
REFERENCES,0.9527896995708155,"learning more about explored contexts based on (a) current knowledge of the various context-action
pairs, and (b) current estimates of the ‘value’ (i.e., rewards obtainable) of context-action pairs. This
section attempts to zoom into this behavior."
REFERENCES,0.9549356223175965,"Specifically, we consider one of the settings (setting #1) from the representative set used in Ex-
periment 1; it has NCtar = 10, all equally likely, but Ctar ∈{0, 1} are more valuable than
others (i.e., learning the right actions for these contexts can give higher rewards); see code
(config setting1.py) for more details. We zoom into the case when T = 15. We do 500
independent runs, count the number of times every possible Ctar value is encountered or cho-
sen, and plot the frequencies. We compare our algorithm’s behavior with two representative base-
lines – Algorithm Std UniExp (which does standard contextual bandit interactions) and Algorithm
TargInt UniExp (which does targeted interventions); the effect is similar for the other two baselines
as well. As seen in Figure 4, our algorithm chooses the higher value contexts (Ctar ∈{0, 1}) with
relatively higher frequency than the baselines, while still ensuring good exploration of other contexts
– in line with the intuition discussed in the main paper."
REFERENCES,0.9570815450643777,"C
APPENDIX (SUPPLEMENTARY TEXT): MORE DETAILS ABOUT
EXPERIMENT 1"
REFERENCES,0.9592274678111588,"In the discussion about Experiment 1 in the main paper, it was mentioned that Appendix C con-
tains plots for two individual settings from the representative set – one where Algorithms Std TS
and Std UniExp perform significantly better than Algorithms TargInt TS, TargInt UniExp and
TargInt TS UniExp, and another where it is the converse – while our algorithm performs close to
the best baseline in both. Figure 5 presents those plots. Each result is based on 50 runs."
REFERENCES,0.9613733905579399,"D
APPENDIX (SUPPLEMENTARY TEXT): REGARDING PARAMETERIZATIONS
USED IN THE EXPERIMENTS"
REFERENCES,0.9635193133047211,"For the specific parameterizations of all settings used in all the experiments, refer to the README
file in Supplemental Code Paper1203.zip as part of the Supplemental Material."
REFERENCES,0.9656652360515021,"E
APPENDIX (SUPPLEMENTARY TEXT): MORE DISCUSSION ON THE CASE
WHERE Cother = ∅"
REFERENCES,0.9678111587982833,"If Cother = ∅(that is, if it is empty), it might appear as if there would not be any information
leakage. However, this is not true, and information leakage can still exist. For instance, consider the
same graph as in used in Section 4.1. But now suppose that Ctar = {C1, C0}, whereas Cother =
∅. Consider two targeted interventions, (x, c0, c1) and (x, c0, c′
1), where c1 ̸= c′
1. Note that the
distribution of Y after these two targeted interventions would, respectively, be"
REFERENCES,0.9699570815450643,"P(Y |do(x), (c0, c1)) = P(Y |x, c0)"
REFERENCES,0.9721030042918455,"P(Y |do(x), (c0, c′
1)) = P(Y |x, c0)"
REFERENCES,0.9742489270386266,"The common term P(Y |x, c0) enables information leakage between these two targeted interventions.
To see this, note that if the agent conducts targeted intervention (x, c0, c1), then it is able to update
its beliefs about P(Y |x, c0) which improves its estimates of (x, c0, c′
1) as well due to the shared
term. So Cother = ∅does not preclude information leakage."
REFERENCES,0.9763948497854077,"Instead, suppose that we are considering the interventions (x, c0, c1) and (x, c′
0, c1), where c0 ̸= c′
0.
In this case, information leakage gains will not be possible. However, our algorithm would still
be able to achieve better performance compared to baselines by balancing between exploring new
(x, ctar) pairs and learning more about already explored “valuable” (x, ctar) pairs. To see this, note
that in the definition of Unc in Section 3.1 the “information” component is weighted by the “value”
component (the last two terms, namely, ˆP(c′) · ˆE[Y |c′, do(x′)])."
REFERENCES,0.9785407725321889,Published as a conference paper at ICLR 2022
REFERENCES,0.98068669527897,"(a) Algorithms TargInt TS, TargInt UniExp and TargInt TS UniExp perform bet-
ter than other baselines."
REFERENCES,0.9828326180257511,"(b) Algorithms Std TS, Std UniExp perform better than other baselines."
REFERENCES,0.9849785407725322,"Figure 5: Different baselines outperform each other in two different settings; however, our algorithm
performs close to the best baseline in both."
REFERENCES,0.9871244635193133,"F
APPENDIX (SUPPLEMENTARY TEXT): REGARDING THE CHOICE OF THE
PROBABILITY OF UNIFORM EXPLORATION IN TARGINT TS UNIEXP"
REFERENCES,0.9892703862660944,"Thompson Sampling aims to minimize cumulative regret in a best-arm identification setting
(Agrawal & Goyal, 2012). For cumulative regret minimization, it performs better than simple ex-
ploration algorithms such as ϵ-greedy, by controlling the exploration based on the agent’s current
beliefs. However, as discussed in the main part of the paper, in our setting, the agent is faced with
the task of learning a policy, while balancing exploration of new contexts and learning more about
already explored contexts. Thus, apart from having standard Thompson Sampling as a baseline
(TargInt TS), we also create a variation of this (called TargInt TS UniExp) where, in each round,
the algorithm, with some probability λ, chooses a targeted intervention uniformly at random (from
the space val(X) × val(Ctar)) for exploration; with probability 1 −λ, it chooses the targeted inter-
vention given by Thompson Sampling. This probability λ allows us to directly control the degree of
uniform exploration."
REFERENCES,0.9914163090128756,"In Experiment 1 (Section 4.1), we chose the probability of uniform exploration λ = 0.5 for
TargInt TS UniExp. This was to act as a midpoint between the TargInt TS algorithm (attempting
to optimize cumulative regret) and TargInt UniExp (which does uniform exploration). We noted"
REFERENCES,0.9935622317596566,Published as a conference paper at ICLR 2022
REFERENCES,0.9957081545064378,"that our algorithm continues to perform better than all baselines. Figure 6 shows that this continues
to be true even when λ in TargInt TS UniExp is not as large as 0.5, by setting λ = 0.2."
REFERENCES,0.9978540772532188,"Figure 6: Experiment 1 results when probability of uniform exploration in TargInt TS UniExp was
chosen to be 0.2 (instead of 0.5). Our algorithm continues to perform better than all baselines."
