Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0031847133757961785,"Multi-label classification is a challenging structured prediction task in which a set
of output class labels are predicted for each input. Real-world datasets often have
taxonomic relationships between labels which can be explicit, implicit, or partially
observed. Most existing multi-label classification methods either ignore the label
taxonomy or require the complete specification of the taxonomy at training and
inference time to enforce coherence in their predictions. In this work we intro-
duce the multi-label box model (MBM), a multi-label classification method that
combines the encoding power of neural networks with the inductive bias of proba-
bilistic box embeddings (Vilnis et al., 2018), which can be understood as trainable
Venn-diagrams based on hyper-rectangles. By representing labels as boxes, MBM
is able to capture taxonomic relations among labels without them being provided
explicitly. Furthermore, since MBM learns the label-label relationships from data
and represents them as calibrated conditional probabilities, it provides a high de-
gree of interpretability. This interpretability also facilitates the injection of partial
information about label-label relationships into model training, to further improve
its consistency. We provide theoretical grounding for our method and show ex-
perimentally the model’s ability to learn the true latent taxonomic structure from
data. Through extensive empirical evaluations on twelve multi-label classification
datasets, we show that MBM can significantly improve taxonomic consistency
while maintaining the state-of-the-art predictive performance.1"
INTRODUCTION,0.006369426751592357,"1
INTRODUCTION"
INTRODUCTION,0.009554140127388535,"Multi-label classification is a machine learning task in which an input is associated with multiple
categories. Many real-world multi-label classification datasets in modalities such as text categoriza-
tion (Lewis et al., 2004), image classification (Lin et al., 2015; Krishna et al., 2016), entity typing
(Murty et al., 2018; Onoe et al., 2021), functional genomics (Barutcuoglu et al., 2006; Clare, 2003),
and so on, have a rich inter-dependent label structure that can be expressed using a taxonomy graph
or a hierarchy. To be useful in practice, a model should produce predictions that are coherent with
respect to the label taxonomy. For example, if a book is classified as drama, it should also be clas-
sified as fiction according to the label taxonomy of book genres in the left-hand side of Figure 1.
More formally, we are given a label taxonomy in the form of a directed acyclic graph G = (L, T ),
where (a, b) ∈T if and only if a is a parent of b in the taxonomy. A model will assign scores sℓ(x)
for each label ℓ∈L. We say the scores are coherent with respect to the taxonomy if, for all edges
(a, b) ∈T , sa(x) ≥sb(x), and the model is consistent if this is the case for all inputs x. In the
case of the book genre classification example, this implies that sfiction must be greater than or equal
to sdrama, regardless of the input."
INTRODUCTION,0.012738853503184714,"The problem of producing coherent predictions for multi-label classification has garnered a lot of
attention in the machine learning literature (Wehrmann et al., 2018a; Giunchiglia & Lukasiewicz,
2020; Murty et al., 2018; Cerri et al., 2014). Most methods proposed to improve the coherence in
predictions require the complete label taxonomy at inference time, and occasionally at training time"
INTRODUCTION,0.01592356687898089,"1The code and implementation details are available at https://github.com/iesl/box-mlc-
iclr-2022"
INTRODUCTION,0.01910828025477707,Published as a conference paper at ICLR 2022
INTRODUCTION,0.022292993630573247,"Figure 1: Left: A label taxonomy represented as a DAG. Center: A 2-dimensional box embedding
of this taxonomy. Right: Scores assigned by the box embedding model to each label for two inputs."
INTRODUCTION,0.025477707006369428,"as well, making these models hard to scale to large label spaces (Giunchiglia & Lukasiewicz, 2020;
Wehrmann et al., 2018b). This brings forth a question: Can we utilize representation learning to
model the label-label relationships implicitly in the embedding space?"
INTRODUCTION,0.028662420382165606,"Vilnis et al. (2018) introduced probabilistic box embeddings, which represent concepts as high di-
mensional hyper-rectangles, and demonstrated that they can embed DAGs efficiently using explicit
information about the edges. Box embeddings represent edges by box-box containment as shown
in Figure 1b. Representing the input and output labels in the same geometric space of boxes allows
the multi-label taxonomy to be learned without an explicit taxonomic training signal. Moreover,
there exists a large space of possible configurations that represent the same taxonomy, and if the
label embeddings in the model gets close to any such configuration, then the model will always pro-
duce classifications that are coherent w.r.t the taxonomy, regardless of the input. We show through
empirical evidence this is the case, and provide a formal proof for latter."
INTRODUCTION,0.03184713375796178,"In this work, we propose the multi-label box model (MBM) that utilizes the geometry and proba-
bilistic semantics of box embeddings to model label-label interactions in multi-label classification.
MBM represents labels as boxes using free parameters and uses a deep neural network to embed
the inputs in the same space. We perform coherence analysis of the model using two measures and
show that MBM not only achieves state-of-the-art predictive performance but it also significantly
improves the coherence of predicted scores w.r.t latent label taxonomy. Our analysis further shows
that it is possible to retrieve the latent label-label relationships solely by analysing the learnt label
representations inside the MBM, endowing the model with high degree of interpretability. Finally,
we also present a way to utilize the interpretability of MBM to inject partial information about
label-label relationships into the model thereby improving the coherence even further."
RELATED WORK,0.03503184713375796,"2
RELATED WORK"
RELATED WORK,0.03821656050955414,"Multi-label classification tasks that exhibit strong label space structure in the form of explicit label
taxonomy are termed hierarchical multi-label classification (HMLC) in machine learning literature.
Most approaches for such tasks make use of the complete hierarchy at training time. These ap-
proaches can be categorized into two buckets (Silla & Freitas, 2010): (1) Local approaches that
focus on local information for each label or clusters of labels in the hierarchy and classify them in-
dependently (Cerri et al., 2014; Huang et al., 2019), and (2) Global approaches that treat the problem
as a structured classification task and take global interactions into account (Belanger & McCallum,
2016). In the most general setting, however, both local and global interactions between labels exist.
The recent advances in deep learning (Wehrmann et al., 2018a) propose a specialized neural network
architecture called Hierarchical Multi-Label Classification Network (HMCN-R and HMCN-F) that
takes into account both local and global interactions by creating an ensemble of classifiers that can
be trained using end-to-end gradient based training. However, HMCN does not try to enforce coher-
ence strongly, focusing solely on predictive performance. In order to improve prediction coherence,
recent works employ special loss functions on top of a neural network classifier to enforce coherence
w.r.t the label taxonomy (Murty et al., 2018; Giunchiglia & Lukasiewicz, 2020). While effective,
these approaches still use the label taxonomy explicitly, making them difficult to scale to very large
label spaces."
RELATED WORK,0.041401273885350316,Published as a conference paper at ICLR 2022
RELATED WORK,0.044585987261146494,"Recent advances in representation learning provide various methods to embed large graphs and tax-
onomies parsimoniously in non-euclidean spaces. The most prominent of these embedding methods
include hyperbolic embeddings (Nickel & Kiela, 2017; Ganea et al., 2018a;b) and box embedding
(Vilnis et al., 2018; Dasgupta et al., 2021a). The use of representations other than Euclidean vec-
tors for improving the coherence of multi-label classification has been limited to specific domains
like text (Chatterjee et al., 2021) or specific tasks like entity typing (Onoe et al., 2021). Moreover,
while both hyperbolic and box embedding can model hierarchical relationships, it has been shown
that the box embedding can also model more general graphs like DAGs much more efficiently than
hyperbolic embeddings (Patel et al., 2020; Boratko et al., 2021; Dasgupta et al., 2021b). Hence, we
propose a model that uses box embedding to capture general label-label relationships without the
explicit use of label taxonomy to improve the coherence of model predictions."
OVERVIEW OF BOX EMBEDDINGS,0.04777070063694268,"3
OVERVIEW OF BOX EMBEDDINGS"
OVERVIEW OF BOX EMBEDDINGS,0.050955414012738856,"Notations:
In the problem of multi-label classification, we are given a set of labels L where
L = |L|, and an instance can be labeled with an element s ∈{0, 1}L, where projection to the
ith coordinate πi(s) = 1 means that the ith label is true. We call the set of all such labelings S, and
the associated probability space (S, P(S), PS). We use I to denote the set of all finite closed inter-
vals [µ−, µ+] in Ω⊂R plus the empty set, i.e. I := {[µ−, µ+] ⊂Ω| µ+ ≥µ−} ∪∅. We denote the
smallest σ-algebra containing I as σ(I) and, given a valid finite measure ν, we consider the measure
space (Ω, σ(I), ν). As a high dimensional generalization, Id will denote a d-dimensional Cartesian
product of I.
Definition 1 (Box Embedding (Vilnis et al., 2018)). Let B : Id →S be a measurable function such
that B−1 ◦π−1
i
(1) = Qd
i [µ−
i , µ+
i ] ∈Id. A box embedding is defined as the function Box : L →Id"
OVERVIEW OF BOX EMBEDDINGS,0.054140127388535034,"which maps a label ℓ∈L to B−1 ◦π−1
ℓ({1}) ∈Id."
OVERVIEW OF BOX EMBEDDINGS,0.05732484076433121,"The definition of box embeddings induces a push-forward measure Q on S such that for any R ⊆S,
Q(S) = ν ◦B−1(R). The complete joint probability distribution over the labels can be modeled
using Q as defined above; however, computing B−1(R) requires the use of inclusion-exclusion
principle and hence is intractable for a general R."
OVERVIEW OF BOX EMBEDDINGS,0.06050955414012739,"In order to avoid local identifiability issues in training, Dasgupta et al. (2020) interpret µ−
i (resp.
µ+
i ) as the location parameters of random variables M −
i (resp. M +
i ) that are distributed according
to GumbelMax (resp. GumbelMin) distributions, leading to a meta-probabilistic generalization of
box embedding which they call Gumbel Box Process. Since GumbelMax (resp. GumbelMin) is
a max (resp. min) stable distribution, it enables the computation of the location parameters of the
intersection box as given in the following definition."
OVERVIEW OF BOX EMBEDDINGS,0.06369426751592357,"Definition 2 (Intersection Box (Dasgupta et al., 2020)). Let A = Qd
i=1[a−
i , a+
i ] and B =
Qd
i=1[b−
i , b+
i ] be two gumbel boxes expressed using their location parameters, then the location
parameters of the intersection of these two gumbel boxes are given as"
OVERVIEW OF BOX EMBEDDINGS,0.06687898089171974,"A˜∩B = d
Y i=1"
OVERVIEW OF BOX EMBEDDINGS,0.07006369426751592,"
β lse
a−
i
β , b−
i
β"
OVERVIEW OF BOX EMBEDDINGS,0.0732484076433121,"
,
−β lse

−a+
i
β , −b+
i
β"
OVERVIEW OF BOX EMBEDDINGS,0.07643312101910828,"
,
(1)"
OVERVIEW OF BOX EMBEDDINGS,0.07961783439490445,"where lse(x, y) = log(exp(x) + exp(y))."
OVERVIEW OF BOX EMBEDDINGS,0.08280254777070063,"The expected volume of Gumbel boxes involves the Bessel Function of the Second Kind, however,
as shown in Dasgupta et al. (2020), this integral can be reasonably approximated using softplus
function leading to the following definition for approximate bessel volume.
Definition 3 (Approximate Bessel Volume (Dasgupta et al., 2020)). For a gumbel box B =
Qd
i=1[b−
i , b+
i ] we define the approximate Bessel volume λ : Id →R+ as"
OVERVIEW OF BOX EMBEDDINGS,0.08598726114649681,"λ(B) := d
Y"
OVERVIEW OF BOX EMBEDDINGS,0.08917197452229299,"i=1
β log

1 + exp
b+
i −b−
i
β
−2γ

."
OVERVIEW OF BOX EMBEDDINGS,0.09235668789808917,"In the next section, we formally demonstrate the suitability of box embeddings for capturing taxo-
nomic label relationships, and for that we first state a couple of useful facts regarding the Gumbel
intersection and Bessel approximate volume."
OVERVIEW OF BOX EMBEDDINGS,0.09554140127388536,Published as a conference paper at ICLR 2022
OVERVIEW OF BOX EMBEDDINGS,0.09872611464968153,"Proposition 1. Approximate bessel volume is monotonic with respect to set containment. That is
for two Gumbel boxes A, B,
a−
i ≥b−
i
and
a+
i ≤b+
i ,
∀i ∈{1, . . . , d} ⇐⇒λ(A) ≤λ(B).
(2)"
OVERVIEW OF BOX EMBEDDINGS,0.10191082802547771,Proof. Follows from the monotonicity of log(1 + exp(.)).
OVERVIEW OF BOX EMBEDDINGS,0.10509554140127389,"Proposition 2. For any two Gumbel boxes A, B, λ(A˜∩B) ≤λ(B)."
OVERVIEW OF BOX EMBEDDINGS,0.10828025477707007,"Proof. The fact that max(x, y) ≤lse(x, y), and the statement of proposition 1 together imply the
desired result."
OVERVIEW OF BOX EMBEDDINGS,0.11146496815286625,"Since λ is neither normalized nor additive, it cannot be used as a probability measure on (Ω, σ(Id)).
However, we can use proposition 1 and 2 to define a conditional probability model as follows."
OVERVIEW OF BOX EMBEDDINGS,0.11464968152866242,"Corollary 1. For two gumbel boxes A, B, let PBox(A | B) = λ(A˜∩B)"
OVERVIEW OF BOX EMBEDDINGS,0.1178343949044586,"λ(B) , then"
OVERVIEW OF BOX EMBEDDINGS,0.12101910828025478,"(i) For any two gumbel boxes A, B, we have 0 ≤PBox(A | B) ≤1."
OVERVIEW OF BOX EMBEDDINGS,0.12420382165605096,"(ii) PBox(A | C) ≤PBox(B | C) for any three gumbel boxes A, B, C, with a−
i ≥b−
i , a+
i ≤b+
i ."
MULTI-LABEL BOX MODEL,0.12738853503184713,"4
MULTI-LABEL BOX MODEL"
MULTI-LABEL BOX MODEL,0.1305732484076433,"In order to perform the task of multi-label classification we need to model the conditional probabil-
ities P(Y |X) where Y ∈S and X is the input. Using definition 1, we define label box embeddings
Boxψ : L →Id as"
MULTI-LABEL BOX MODEL,0.1337579617834395,"Boxψ(ℓi) := d
Y"
MULTI-LABEL BOX MODEL,0.13694267515923567,"j=1
[ψ−
i,j , ψ−
i,j + log(1 + exp ψ+
i,j)],"
MULTI-LABEL BOX MODEL,0.14012738853503184,"where ψ−, ψ+ ∈RL×d are trainable parameters. The input instance X is encoded as a fixed-width
element of Id using a parametric instance box embedding Boxθ = Id ◦Fθ :
X →Id, where
Fθ : X →Rd is a neural network with parameters θ and Id : Rd →Id defined as"
MULTI-LABEL BOX MODEL,0.14331210191082802,"Id(x) := d
Y"
MULTI-LABEL BOX MODEL,0.1464968152866242,"i=1
[xi −δ, xi + δ],"
MULTI-LABEL BOX MODEL,0.14968152866242038,"where δ = 10−5. The conditional probability for Y ∈S given input X is computed using condi-
tional probability under the Gumbel box model as"
MULTI-LABEL BOX MODEL,0.15286624203821655,"PMBM(Y |X; ψ, θ) = L
Y"
MULTI-LABEL BOX MODEL,0.15605095541401273,"i=1
PMBM(Yi|X, ψ, θ) := L
Y"
MULTI-LABEL BOX MODEL,0.1592356687898089,"i=1
PBox(B−1 ◦π−1
i
({Y }) | Boxθ(X))"
MULTI-LABEL BOX MODEL,0.1624203821656051,"Using the definition of PBox as stated through corollary 1, we get the following expression for the
conditional probability of Yi under the model, where the intersection ˜∩is the Gumbel Intersection
and measure λ is Approximate Bessel Volume."
MULTI-LABEL BOX MODEL,0.16560509554140126,"PMBM(Yi = 1|X; ψ, θ) = λ (Boxψ(ℓi) ˜∩Boxθ(X))"
MULTI-LABEL BOX MODEL,0.16878980891719744,λ(Boxθ(X))
MODELING LABEL-SPACE INTERACTIONS,0.17197452229299362,"4.1
MODELING LABEL-SPACE INTERACTIONS"
MODELING LABEL-SPACE INTERACTIONS,0.1751592356687898,"In Section 1, we alluded to the fact that the inductive bias of the MBM allows it to efficiently model
partially specified first-order label interactions. Now we make this remark more concrete. If the
partial specification of label interaction is defined using a taxonomy that can be represented as a
directed acyclic graph (DAG), the following proposition shows that MBM has a strong inductive
bias towards maintaining coherence in its scores.
Proposition 3. Let G = (L, T ) denote a DAG defined over the labels where L is the set of all labels
and T = {(ℓi, ℓj) | ℓi, ℓj ∈L, PD(yi = 1 | yj = 1) = 1} is the set of edges. Then there exists some
ψ such that P(ψ,θ)(yi = 1 | x) ≥P(ψ,θ)(yj = 1 | x), for all x, θ."
MODELING LABEL-SPACE INTERACTIONS,0.17834394904458598,"Proof. For all (ℓi, ℓj) ∈T , let ψ be such that Boxψ(ℓj) ⊆Boxψ(ℓi). Note that such ψ exists
since for each i ∈{1, . . . , L}, Boxψ(ℓi) is defined using only ψi. It follows from corollary 1 that
PBox(Boxψ(ℓi) | Boxθ(X)) ≥PBox(Boxψ(ℓj) | Boxθ(X)) for any X, θ."
MODELING LABEL-SPACE INTERACTIONS,0.18152866242038215,Published as a conference paper at ICLR 2022
LEARNING,0.18471337579617833,"4.2
LEARNING"
LEARNING,0.18789808917197454,"The entire MBM is specified using parameters (ψ, θ) where ψ ∈R2d×L are the label embed-
ding parameters and θ are the parameters of the instance encoder neural network Fθ. Given data
D = {(x(1), y(1)), . . . , (x(N), y(N))}, the model parameters are learnt by minimizing negative log-
likelihood loss"
LEARNING,0.1910828025477707,"Lnll(ψ, θ; D) = − D
X i=1 L
X"
LEARNING,0.1942675159235669,"j=1
log P(y(i)
j
| x(i); ψ, θ),
(3)"
LEARNING,0.19745222929936307,"using the ADAM optimizer (Kingma & Ba, 2017). In order to empirically verify the intuition behind
proposition 3, we also propose the use of label interaction loss"
LEARNING,0.20063694267515925,"LG(ψ) = −
X"
LEARNING,0.20382165605095542,"(ℓi,ℓj)∈T
s(ℓi, ℓj) +
X"
LEARNING,0.2070063694267516,"(ℓi,ℓj)̸∈T
s(ℓi, ℓj)
(4)"
LEARNING,0.21019108280254778,"that utilizes the geometry of box embeddings to inject partial information about label interactions
specified using a label taxonomy G = (L, T ). When label interaction loss is applied (MBM-T), the
total loss is LT = Lnll + νLG, where ν is a hyperparameter. For the Box model, label interaction
score for a pair of labels is defined as"
LEARNING,0.21337579617834396,"sMBM(ℓi, ℓj) := log PBox(Boxψ(ℓi) | Boxψ(ℓj)).
(5)"
BASELINES,0.21656050955414013,"5
BASELINES"
BASELINES,0.2197452229299363,"Our choice of baselines reflects the focus of this work, i.e., introducing coherence in prediction us-
ing suitable representation spaces. To this end, our baselines consist of a high-performing neural
network that only uses Euclidean vector representations (MVM), and another that uses hyperbolic
representations (MHM). In order to test the importance of the probabilistic semantics used to formu-
late MBM, we also include as baseline, a non-probabilistic box model as defined in Abboud et al.
(2020). The base input encoder architecture Fθ in all these baselines is same as the one used in
MBM."
BASELINES,0.2229299363057325,"Multi-label Vector Model (MVM) An input encoder neural network Fθ : X →Rd is used to
encode the inputs and a label embedding matrix ψ is used to represent the labels. The conditional
probability of labels given the input and label interaction scores are given as"
BASELINES,0.22611464968152867,"PMVM(yl = 1 | x; ψ, θ) := σ(Fθ(X)T Mθ ψl),
and sMVM(li, lj) := σ(ψT
liMθ ψlj),"
BASELINES,0.22929936305732485,"respectively, where σ is the logistic sigmoid function, and Gθ ∈Rd×d is a matrix of trainable
parameters. The parameters (θ, ψ) are learnt through LT (Eq. 3,4). Note that, when label interaction
is not being explicitly modeled i.e., ν = 0, M is fixed as the identity matrix and MVM reduces to a
special case of multi-layer perceptron."
BASELINES,0.23248407643312102,"Multi-label Hyperbolic Model (MHM) As discussed in Chatterjee et al. (2021), the isometry be-
tween the Lorentz model and Poincare disk model for hyperbolic geometry can be used produce the
retraction formula used to project d-dimensional euclidean vectors to d-dimensional Poincar´e ball
Bd =

x ∈Rd | ∥x∥< 1
	
.
Here the projection operator Π : Rd →Bd and the distance
d : Bd × Bd →R+ in the hyperbolic space are given as:"
BASELINES,0.2356687898089172,"Π(x) :=
x 1 +
p"
BASELINES,0.23885350318471338,"1 + ∥x∥2
2
,
d(u, v) := arcosh

1 + 2
∥u −v∥
(1 −∥u∥2
2)(1 −∥v∥2
2) 
."
BASELINES,0.24203821656050956,"Specifically, the input is first encoded using Fθ and then projected into Bd. The unnormalized score
for each label is computed as the negative of the distance between the hyperbolic projections of
encoded input and label representation. Since the hyperbolic distance function consists of arcosh,
the negative distance is interpreted as log-probability score. Hence, the conditional log-probability
of the labels and the label interaction score is given, respectively, by the following expressions,
which are then used to learn the parameters (ψ, θ) in the same way as MVM and MBM."
BASELINES,0.24522292993630573,"log PMHM(yl = 1 | x) = −d(Π(Fθ(x)), Π(ψl)) and sMHM(li, lj) := −d(Π(ψli), Π(ψlj)),"
BASELINES,0.2484076433121019,Published as a conference paper at ICLR 2022
BASELINES,0.2515923566878981,"Non-probabilistic box model (BoxE): In order to test the importance of the probabilistic semantics
used to formulate the MBM, we also include as baseline, a box model that does not use probabilistic
semantics. For this, we replace the PMBM with the non-probabilistic score defined in Abboud et al.
(2020) and replace the log-likelihood loss (Lnll) with a margin based loss Lmargin. 1"
EVALUATION AND RESULTS,0.25477707006369427,"6
EVALUATION AND RESULTS"
EVALUATION AND RESULTS,0.25796178343949044,"We evaluate the performance of MBM using 12 real-world multi-label classification datasets (Dim-
itrovski et al., 2011; Clare, 2003) spanning across three domains: text (Enron), images (Imclef07a,
Imclef07d, Diatoms), and functional genomics (Expr, Cellcycle, Derisi, Spo) . These datasets are
the ideal test bed as they provide explicit label taxonomies with different connectivity: trees, forests,
and more general DAGs. Moreover, all the labels of all training and test instances respect the la-
bel taxonomy. The datasets include both categorical and continuous input features. We convert the
categorical features into one-hot feature vectors and standardize all continuous features.
The in-
put encoder Fθ uses a common architecture for all models consisting of an MLP with a maximum
of 3 layers. We perform a grid search over number of MLP layers, activation function, hidden di-
mensions, dropout, learning rate and use the best parameters for each model.2 The Mean Average
Precision (MAP), that is the mean over the average precision values across instances in the test set,
is used to evaluate the predictive performance of the models. Table 1 reports average metric values
across 10 runs with different random seeds (the standard error intervals are small and are omitted for
the sake of readability).
As seen in Table 1, the predictive performance (MAP) of MBM is better
than all other embedding based methods on at least 11 out of 12 datasets. To check the statistical
significance of our results, following Demˇsar (2006), we first perform the Friedman test and obtain a
p-value of 2 × 10−9. The critical diagram of the post-hoc Nemenyi test (Figure 2a) performed after
the Friedman test allows us to conclude that difference in predictive performance between MBM
and all other embedding based models is statistically significant. Furthermore, from these results,
one can conclude with confidence that MBM with its probabilistic formulation preforms signifi-
cantly better than BoxE, which is the non-probabilistic variant of box embeddings. In the following
section, we discuss another important aspect of performance, that is, the coherence of predictions."
COHERENCE ANALYSIS,0.2611464968152866,"6.1
COHERENCE ANALYSIS"
COHERENCE ANALYSIS,0.2643312101910828,"MAP is a good metric for predictive performance, however it does not take into account the in-
consistencies in the predicted scores w.r.t. the label taxonomy. For instance, recalling the earlier
example in Figure 1, a consistent model would always assign higher score to fiction when compared
to drama, since a book classified as drama should also be classified as fiction. Since MAP is inca-
pable of capturing such coherence conditions, we perform further analysis to check the coherence
of the predicted scores. This is done by considering two opposite perspectives towards coherence
evaluation. The first one is a punitive approach, where we count the number of instances that are
given inconsistent scores w.r.t the taxonomy. In the second one, a more constructive approach, we
try to check the amenability of the model, by applying post-hoc corrections to the predicted scores to
enforce coherence. The results of these two approaches are presented by using Constraint Violation
(CV) and Mean Average Precision post Coherence correction (CMAP), which are described below.
Note that we do not recommend the use of either CV or CMAP as the sole metrics to evaluate the
model, we present these as measures to analyse the coherence of the model predictions."
COHERENCE ANALYSIS,0.267515923566879,"Constraint violation is a punitive measure that quantifies the extent to which the label scores gen-
erated by the model violate the partial ordering of the latent label taxonomy regardless of true labels
for the instances. Hence, lower value of CV implies higher taxonomic coherence in the predictions."
COHERENCE ANALYSIS,0.27070063694267515,"CV(s) =
1
|D||T | |D|
X k=1 X"
COHERENCE ANALYSIS,0.27388535031847133,"(li,lj)∈T
1

s(k)
i
−s(k)
j
< 0

.
(6)"
COHERENCE ANALYSIS,0.2770700636942675,"Mean Average Precision post Coherence correction (CMAP): Given a complete or partial tax-
onomy G = (L, T ) for the labels, coherence can be imposed post-hoc by applying a modification
function δ : RL →RL to the label-scores produced by the model such that δ(si) −δ(sj) < 0 for"
COHERENCE ANALYSIS,0.2802547770700637,"2Please see the Reproducibility Statement after Section 8, for further details on the implementation of the
baselines and the experiments."
COHERENCE ANALYSIS,0.28343949044585987,Published as a conference paper at ICLR 2022
COHERENCE ANALYSIS,0.28662420382165604,"Table 1: Performance comparison of MBM models with the baselines for the 12 multi-label classi-
fication datasets. The left section compares the models that do not require explicit taxonomy, i.e.,
BoxE, MVM, MHM and MBM. In the left section, the models with the best performance w.r.t. MAP
are highlighted. The right section shows the performance when we include taxonomy information
in training through LG (MVM-T, MHM-T and MBM-T), where the highlighted cells indicate an
improvement in performance (MAP and CV) w.r.t. the respective non-T model. All metrics reported
are averaged across five runs with different seeds."
COHERENCE ANALYSIS,0.2898089171974522,"Dataset
Metric
BoxE
MVM
MHM
MBM
MVM-T
MHM-T
MBM-T
C-HMCNN"
COHERENCE ANALYSIS,0.2929936305732484,"MAP↑
37.30
38.37
31.91
38.45
37.94
31.90
38.42
38.41
CMAP↑
37.92
37.66
32.05
38.72
37.41
32.02
38.67
38.41
ExprFUN
CV↓
4.79
1.99
1.94
2.55
1.97
1.92
1.87
0"
COHERENCE ANALYSIS,0.2961783439490446,"MAP↑
31.96
31.68
28.76
34.20
31.61
28.74
34.61
34.35
CMAP↑
32.70
31.34
28.92
34.39
31.33
28.89
34.78
34.35
CellcycleFUN
CV↓
4.02
3.42
1.78
1.77
3.45
1.78
1.35
0"
COHERENCE ANALYSIS,0.29936305732484075,"MAP↑
26.66
23.70
26.87
28.69
24.16
24.40
28.71
28.19
CMAP↑
26.96
24.13
26.98
28.86
24.35
24.52
28.88
28.19
DerisiFUN
CV↓
2.27
4.79
0.95
1.67
4.01
0.85
1.43
0"
COHERENCE ANALYSIS,0.30254777070063693,"MAP↑
27.97
25.18
26.58
30.10
24.21
26.57
29.62
29.18
CMAP↑
28.38
25.38
26.79
30.27
24.55
26.79
29.78
29.18
SpoFUN
CV↓
2.75
4.23
1.68
1.75
4.73
1.69
1.53
0"
COHERENCE ANALYSIS,0.3057324840764331,"MAP↑
46.75
44.92
40.53
48.45
44.97
40.52
48.45
48.61
CMAP↑
47.28
41.78
40.71
48.56
41.84
40.70
48.56
48.61
ExprGO
CV↓
5.74
7.05
5.12
2.46
7.05
5.19
1.91
0"
COHERENCE ANALYSIS,0.3089171974522293,"MAP↑
43.08
44.25
39.77
44.28
44.19
39.74
44.93
45.61
CMAP↑
43.79
41.09
39.90
44.23
41.02
39.76
45.01
45.61
CellcycleGO
CV↓
5.06
3.07
2.35
2.84
3.03
2.49
2.16
0"
COHERENCE ANALYSIS,0.31210191082802546,"MAP↑
40.44
41.22
40.16
42.03
41.13
40.10
42.02
42.24
CMAP↑
40.73
38.21
40.28
42.14
38.21
40.20
42.12
42.24
DerisiGO
CV↓
3.16
3.43
1.98
2.37
3.46
2.02
1.13
0"
COHERENCE ANALYSIS,0.31528662420382164,"MAP↑
40.88
42.19
39.81
42.22
42.20
39.70
41.74
42.77
CMAP↑
41.27
38.96
39.89
42.31
39.04
39.77
41.54
42.77
SpoGO
CV↓
3.89
2.81
1.93
2.68
2.77
1.90
1.80
0"
COHERENCE ANALYSIS,0.3184713375796178,"MAP↑
80.44
73.68
75.95
79.95
73.68
75.62
80.06
80.04
CMAP↑
80.46
66.87
76
79.94
66.87
75.68
80.05
80.04
Enron
CV↓
0.20
2.53
0.29
0.04
2.53
0.36
0.03
0"
COHERENCE ANALYSIS,0.321656050955414,"MAP↑
43.71
73.06
56.97
79.14
72.65
56.86
79.14
76.23
CMAP↑
45.16
72.44
56.14
79.30
72.18
56.07
79.23
76.23
Diatoms
CV↓
6.39
18.97
5.59
3.46
19.20
5.55
0.34
0"
COHERENCE ANALYSIS,0.3248407643312102,"MAP↑
83.71
77.14
65.29
91.45
78.22
65.30
69.26
90.26
CMAP↑
84.73
76.56
66.01
91.73
77.46
66.01
69.48
90.26
Imclef07a
CV↓
12.73
23.02
4.75
5.65
22.86
4.75
2.40
0"
COHERENCE ANALYSIS,0.32802547770700635,"MAP↑
87.95
88.49
75.72
89.49
88.59
75.69
89.56
89.22
CMAP↑
88.93
86.89
76.98
89.99
86.87
76.95
90.07
89.22
Imclef07d
CV↓
11.93
10.72
7.52
7.16
11.02
7.56
5.66
0"
COHERENCE ANALYSIS,0.33121019108280253,"MAP
4.92
5.29
6.42
2.08
5.38
7.33
2.42
2.17
CMAP
4.08
6.08
6.17
1.96
6.25
6.92
2.29
2.25
Avg. Rank ↓
CV
6.67
7
3.83
4.33
7
4
2.17
1"
COHERENCE ANALYSIS,0.3343949044585987,"all (li, lj) ∈T . This can be achieved by modifying the score si for each label li to be either the
maximum (δM
G ) of the scores of any of its descendants or the minimum of the scores of its ancestors
(δm
G ) in the taxonomy G. Concretely, given the scores s ∈RL produced by a model, for an input,
the two modification functions are given as"
COHERENCE ANALYSIS,0.3375796178343949,"δm
G (s)i =
min
lj∈AncG(li)∪{li} sj,
δM
G (s)i =
max
lj∈DesG(li)∪{li} sj,"
COHERENCE ANALYSIS,0.34076433121019106,"where AncG(l) and DesG(l) is the set of ancestors and descendants, respectively, of l in the graph
G. In practice, if one is given a partial label taxonomy, one would select whichever post-hoc modifi-"
COHERENCE ANALYSIS,0.34394904458598724,Published as a conference paper at ICLR 2022
COHERENCE ANALYSIS,0.3471337579617834,"cation performs better, and we refer to the maximum MAP obtained after applying one of the mod-
ification functions δm
G , δM
G to the scores as CMAP.3 Intuitively, a CMAP value close to MAP value
implies that the model is more perceptive of the latent hierarchy in the label space.
As seen from
the critical diagram of the Nemenyi test (Figure 2b), when coherence is considered along with the
predictive performance (CMAP), MBM outperforms MVM and MHM (with statistical significance)
indicating that MBM is much more perceptive of the latent taxonomy compared to other embedding
based models (MVM and MHM). In conclusion, while on one extreme there is MVM, which ex-
hibits reasonable good predictive performance but fails to maintain coherence w.r.t the taxonomy, on
the other extreme we have MHM that exhibits lowest constraint violations but gives inadequate pre-
dictive performance. MBM, however, demonstrates good characteristics on both fronts–predictive
performance as well as coherence."
COHERENCE ANALYSIS,0.3503184713375796,"Comparison with the state-of-the-art: C-HMCNN (Giunchiglia & Lukasiewicz, 2020), is the cur-
rent state-of-the-art method for hierarchical MLC, and includes the modification function δM
G (called
MCM in their work) in their model directly, applying it to scores produced by a multi-layer percep-
tron at training as well as inference time.4 At training time, this modification is accompanied by a
clever modification of BCE loss to form a novel MCLoss which works well with the modification
function during training. There are two key differences between C-HMCNN and MBM that one
needs to keep in mind when comparing their performance. First, C-HMCNN does not attempt to
use the label representations themselves to increase coherence—an idea central to MBM and this
work. Second, unlike MBM, C-HMCNN requires the complete label taxonomy at training and infer-
ence time to enforce the coherence. Owing to the use of complete label taxonomy at inference time,
C-HMCNN always has 0 constraint violations. However, as seen from the Table 1 and the Figure
2, MBM, which does not require the label taxonomy at all, performs comparably to C-HMCNN in
terms of MAP (p-value of 0.677 in two sided Wilcoxon test between MBM and C-HMCNN shows
that there is no statistical difference between their performances)."
COHERENCE ANALYSIS,0.3535031847133758,"1
2
3
4
5"
COHERENCE ANALYSIS,0.35668789808917195,"MBM
C-HMCNN BoxE"
COHERENCE ANALYSIS,0.35987261146496813,"MVM
MHM CD"
COHERENCE ANALYSIS,0.3630573248407643,Based on MAP results across datasets
COHERENCE ANALYSIS,0.3662420382165605,"1
2
3
4
5"
COHERENCE ANALYSIS,0.36942675159235666,"MBM
C-HMCNN BoxE"
COHERENCE ANALYSIS,0.37261146496815284,"MVM
MHM CD"
COHERENCE ANALYSIS,0.37579617834394907,Based on CMAP results across datasets
COHERENCE ANALYSIS,0.37898089171974525,"1
2
3
4
5"
COHERENCE ANALYSIS,0.3821656050955414,C-HMCNN
COHERENCE ANALYSIS,0.3853503184713376,"MHM
MBM"
COHERENCE ANALYSIS,0.3885350318471338,"BoxE
MVM CD"
COHERENCE ANALYSIS,0.39171974522292996,Based on CV results across datasets
COHERENCE ANALYSIS,0.39490445859872614,Figure 2: Critical diagrams of the post-hoc Nemenyi test across all 12 datasets.
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.3980891719745223,"7
ANALYSIS OF LEARNED LABEL EMBEDDINGS"
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.4012738853503185,"In this section, we analyze the geometry of the learned label embeddings, finding that the simple
geometry of box embeddings endows the MBM model with high degree of interpretability. In order
to verify that label box embeddings are producing consistent scores by using inclusion in the box
space, we inject into the model the taxonomy information through the additional loss term (Eq.
4). As seen from the right section in Table 1, injecting explicit taxonomic information into the
label embeddings further reduces the constrain violation for MBM on all twelve datasets. However,
the same does not aid the MVM model and even drops its predictive performance significantly.
This underscores the importance of embedding geometry for inducing taxonomic coherence, and
validates our intuition about the arrangement of label embeddings in MBM."
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.40445859872611467,"To determine the extent to which the label embeddings capture the latent label taxonomy with-
out it being explicitly provided, we perform ancestor-descendant classification solely using the
learned label embeddings. Each pair of labels (li, lj) get a score β(i, j) that is determined using
their corresponding label embeddings ψi, ψj. Since MVM and MBM have different geometri-
cal interpretations, we use different scoring for each. Specifically, βMVM(i, j) = ψi · ψj/∥ψj∥,
βMBM(i, j) = sMBM(li, lj). These scores are then compared to true ancestor-descendant relations
in the taxonomy to obtain respective ROC curves as shown in Figure 3. As seen in Table 3, MBM
captures the true label taxonomy the best (AUC ≥0.87) for all datasets."
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.40764331210191085,"3We find that δm
G produces lesser change in the scores predicted by all the models (MBM, MVM and MHM),
and hence is used as the modification function for this analysis (see Appendix E for further discussion on this)
4Comparision with another competitive baseline HMCN is provided in Appendix D."
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.410828025477707,Published as a conference paper at ICLR 2022
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.4140127388535032,"0.0
0.2
0.4
0.6
0.8
1.0
FPR 0.0 0.2 0.4 0.6 0.8 1.0 TPR"
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.4171974522292994,"Random 0.5
BoxE
MVM
MHM
MBM"
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.42038216560509556,"Dataset
BoxE
MVM
MHM
MBM"
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.42356687898089174,"ExprFUN
0.59
0.61
0.40
0.89
CellcycleFUN
0.71
0.59
0.40
0.93
DerisiFUN
0.64
0.61
0.41
0.87
SpoFUN
0.65
0.61
0.39
0.87
Enron
0.77
0.66
0.49
0.92
Diatoms
0.67
0.68
0.44
0.96
Imclef07a
0.58
0.63
0.45
0.87"
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.4267515923566879,"Figure 3: The figure on the left shows the envelope of the ROC curves for the ancestor-descendant
relationship classification in the label space for different embedding based models across the datasets
and the table on the right is the area under the ROC curves. As seen, MBM implicitly captures the
label taxonomy well compared to all other models."
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.4299363057324841,"Table 2: Spearman rank correlation between the number of
descendants in the label taxonomy with each of the follow-
ing: embedding magnitude for MVM, negative embedding
magnitude for MHM and box embedding volume for MBM."
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.43312101910828027,"Model
Expr
FUN
Cellcycle
FUN
Derisi
FUN
Spo
FUN
Enron
Diatoms
Imclef07a"
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.43630573248407645,"MHM
-0.37
0.38
0.40
0.38
0.19
0.31
0.32
MVM
-0.06
-0.11
-0.01
0.06
-0.11
0.04
-0.02
MBM
0.47
0.49
0.50
0.48
0.47
0.23
0.43"
ANALYSIS OF LEARNED LABEL EMBEDDINGS,0.4394904458598726,"Furthermore,
for
hyperberbolic
space, it is suggested that the magni-
tude of embeddings relate to the level
of generality in taxonomy (Nickel
& Kiela, 2017).
We show that
the
same
observation
holds
for
box embeddings, with the vector
embedding magnitude replaced by
box embedding volume.
To see
this, we compute the Spearman rank
correlation between the number of
descendants of a node in the true taxonomy and the embedding magnitude, negative embedding
magnitude, and embedding volume for MVM, MHM, and MBM, respectively. The correlation
values reported in Table 2 confirm our intuition regarding box embeddings stated above.5 It is
interesting to note that as shown by the correlation between the vector magnitude and level of the
label node as well as low constraint violations, the MHM model does capture the depth of the label
nodes in the taxonomy. However, as shown by the ROC curves in Figure 3, unlike MBM, it fails to
capture the exact connectivity of the nodes at different depths in the taxonomy."
CONCLUSION,0.4426751592356688,"8
CONCLUSION"
CONCLUSION,0.445859872611465,"In this work, we demonstrate that box embeddings with its probabilistic formulation can effectively
capture taxonomic relations present between labels in multi-label classification without requiring
explicit access to the taxonomy. The proposed model achieves a fine balance between predictive
performance and coherence. Furthermore, we find that the taxonomic relationships between labels
can be easily injected via extra supervision during training, increasing the coherence of the predic-
tions further. Since the model has the same computational complexity as a simple neural network
model, and unlike C-HMCNN it does not require the label taxonomy, the proposed model can be
scaled to work with extremely large label spaces. Moreover, due to the flexibility of using any neural
network encoder this model can be easily extended to different input modalities, such as raw text,
images, etc. We wish to pursue these two directions in our future work."
REPRODUCIBILITY STATEMENT,0.44904458598726116,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.45222929936305734,"In this section, we provide pointers to information necessary to reproduce the results mentioned
in this paper. The description of the datasets with various statistics, links to download them, and
instructions to pre-process them are provided in Appendix B. The BoxE baseline is further described"
REPRODUCIBILITY STATEMENT,0.4554140127388535,"5Refer to Appendix H for scatter plots of box embedding volume vs. the number of children in the label
taxonomy."
REPRODUCIBILITY STATEMENT,0.4585987261146497,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.46178343949044587,"with expressions for distance and the margin based loss function in Appendix A.1. The hyper-
parameter settings for each model-dataset combination (all 96 settings) used to produce the reported
results are provided in 3. We also include a discussion on computational complexity of the models
in Appendix A.2. Finally, the code for all the models (including the baselines) used in this paper is
available at https://github.com/iesl/box-mlc-iclr-2022."
REPRODUCIBILITY STATEMENT,0.46496815286624205,ACKNOWLEDGEMENTS
REPRODUCIBILITY STATEMENT,0.4681528662420382,"The authors would like to thank the members of the Information and Extraction Synthesis Labora-
tory (IESL) at UMass Amherst for helpful discussions. This work was partially supported by IBM
Research AI through the AI Horizons Network and the Chan Zuckerberg Initiative under the project
Scientific Knowledge Base Construction. Additional support was provided by the National Science
Foundation (NSF) under Grant Numbers IIS-1763618, IIS-1922090, and IIS-1955567, the Defense
Advanced Research Projects Agency (DARPA) via Contract No. FA8750-17-C-0106 under Sub-
award No. 89341790 from the University of Southern California, and the Office of Naval Research
(ONR) via Contract No. N660011924032 under Subaward No. 123875727 from the University
of Southern California. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or implied, of IBM, CZI, NSF, DARPA, ONR,
or the U.S. Government."
REFERENCES,0.4713375796178344,REFERENCES
REFERENCES,0.4745222929936306,"Ralph Abboud, Ismail Ceylan, Thomas Lukasiewicz, and Tommaso Salvatori. BoxE: A box em-
bedding model for knowledge base completion. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in neural information processing systems, volume 33, pp.
9649–9661. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf."
REFERENCES,0.47770700636942676,"Zafer Barutcuoglu, Robert E. Schapire, and Olga G. Troyanskaya. Hierarchical multi-label pre-
diction of gene function.
Bioinformatics, 22(7):830–836, 01 2006.
ISSN 1367-4803.
doi:
10.1093/bioinformatics/btk048. URL https://doi.org/10.1093/bioinformatics/
btk048."
REFERENCES,0.48089171974522293,"David Belanger and Andrew McCallum. Structured prediction energy networks. In Maria Flo-
rina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference
on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 983–992,
New York, New York, USA, 20–22 Jun 2016. PMLR. URL http://proceedings.mlr.
press/v48/belanger16.html."
REFERENCES,0.4840764331210191,"Lukas Biewald.
Experiment tracking with weights and biases, 2020.
URL https://www.
wandb.com/. Software available from wandb.com."
REFERENCES,0.4872611464968153,"Michael Boratko, Dongxu Zhang, Nicholas Monath, Luke Vilnis, Kenneth L. Clarkson, and An-
drew McCallum. Capacity and bias of learned geometric embeddings for directed graphs. In
A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural
Information Processing Systems, 2021.
URL https://openreview.net/forum?id=
0IqTX6FcZWv."
REFERENCES,0.49044585987261147,"Ricardo Cerri, Rodrigo C. Barros, and Andr ˜A© C.P.L.F. de Carvalho.
Hierarchical multi-label
classification using local neural networks. Journal of Computer and System Sciences, 80(1):39
– 56, 2014. ISSN 0022-0000. doi: https://doi.org/10.1016/j.jcss.2013.03.007. URL http:
//www.sciencedirect.com/science/article/pii/S0022000013000718."
REFERENCES,0.49363057324840764,"Soumya Chatterjee, Ayush Maheshwari, Ganesh Ramakrishnan, and Saketha Nath Jagaralpudi. Joint
learning of hyperbolic label embeddings for hierarchical multi-label classification. In Proceedings
of the 16th Conference of the European Chapter of the Association for Computational Linguistics:
Main Volume, pp. 2829–2841, Online, April 2021. Association for Computational Linguistics.
URL https://www.aclweb.org/anthology/2021.eacl-main.247."
REFERENCES,0.4968152866242038,Published as a conference paper at ICLR 2022
REFERENCES,0.5,"Tejas Chheda, Purujit Goyal, Trang Tran, Dhruvesh Patel, Michael Boratko, Shib Sankar Dasgupta,
and Andrew McCallum. Box embeddings: An open-source library for representation learning
using geometric structures. In Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 203–211, Online and Punta Cana,
Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/
v1/2021.emnlp-demo.24. URL https://aclanthology.org/2021.emnlp-demo.24."
REFERENCES,0.5031847133757962,"Amanda Clare. Machine learning and data mining for yeast functional genomics. PhD thesis,
Citeseer, 2003."
REFERENCES,0.5063694267515924,"Shib Sankar Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Lorraine Li, and An-
drew McCallum. Improving local identifiability in probabilistic box embeddings. In Advances in
Neural Information Processing Systems, 2020. URL https://proceedings.neurips.
cc/paper/2020/file/01c9d2c5b3ff5cbba349ec39a570b5e3-Paper.pdf."
REFERENCES,0.5095541401273885,"Shib Sankar Dasgupta, Michael Boratko, Shriya Atmakuri, Xiang Lorraine Li, Dhruvesh Patel, and
Andrew McCallum. Word2box: Learning word representation using box embeddings, 2021a."
REFERENCES,0.5127388535031847,"Shib Sankar Dasgupta, Xiang Lorraine Li, Michael Boratko, Dongxu Zhang, and Andrew Mc-
Callum. Box-to-box transformations for modeling joint hierarchies. In Proceedings of the 6th
Workshop on Representation Learning for NLP (RepL4NLP-2021), pp. 277–288, Online, August
2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.repl4nlp-1.28. URL
https://aclanthology.org/2021.repl4nlp-1.28."
REFERENCES,0.5159235668789809,"Janez Demˇsar. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine
Learning Research, 7:1–30, 2006."
REFERENCES,0.5191082802547771,"Ivica Dimitrovski, Dragi Kocev, Suzana Loskovska, and Saˇso Dˇzeroski. Hierarchical annotation of
medical images. Pattern Recognition, 44(10-11):2436–2449, 2011."
REFERENCES,0.5222929936305732,"Octavian-Eugen Ganea, Gary B´ecigneul, and Thomas Hofmann.
Hyperbolic Entailment Cones
for Learning Hierarchical Embeddings.
Technical report, 2018a.
URL https://en.
wikipedia.org/wiki/."
REFERENCES,0.5254777070063694,"Octavian-Eugen Ganea, Gary B´ecigneul, and Thomas Hofmann.
Hyperbolic Neural Networks.
2018b. doi: arXiv:1805.09112v2. URL http://arxiv.org/abs/1805.09112."
REFERENCES,0.5286624203821656,"Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer. Allennlp: A deep semantic natural language
processing platform. 2017."
REFERENCES,0.5318471337579618,"Eleonora Giunchiglia and Thomas Lukasiewicz. Coherent Hierarchical Multi-Label Classification
Networks. 2020. URL http://arxiv.org/abs/2010.10151."
REFERENCES,0.535031847133758,"Wei Huang, Enhong Chen, Qi Liu, Yuying Chen, Zai Huang, Yang Liu, Zhou Zhao, Dan Zhang, and
Shijin Wang. Hierarchical multi-label text classification: An attention-based recurrent network
approach. In International Conference on Information and Knowledge Management, Proceed-
ings, pp. 1051–1060. ACM, 2019. ISBN 9781450369763. doi: 10.1145/3357384.3357885. URL
https://doi.org/10.1145/3357384.3357885."
REFERENCES,0.5382165605095541,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
REFERENCES,0.5414012738853503,"Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li.
Visual genome: Connecting language and vision using crowdsourced dense image annotations,
2016."
REFERENCES,0.5445859872611465,"David D Lewis, Yiming Yang, Tony Russell-Rose, and Fan Li. Rcv1: A new benchmark collection
for text categorization research. Journal of machine learning research, 5(Apr):361–397, 2004."
REFERENCES,0.5477707006369427,"Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro
Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll´ar. Microsoft coco: Common objects
in context, 2015."
REFERENCES,0.5509554140127388,Published as a conference paper at ICLR 2022
REFERENCES,0.554140127388535,"Shikhar Murty, Patrick Verga, Luke Vilnis, Irena Radovanovic, and Andrew McCallum. Hierarchical
losses and new resources for fine-grained entity typing and linking. In ACL 2018 - 56th Annual
Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long
Papers), volume 1, pp. 97–109, 2018. ISBN 9781948087322. doi: 10.18653/v1/p18-1010. URL
https://www.aclweb.org/anthology/P18-1010."
REFERENCES,0.5573248407643312,"Maximilian Nickel and Douwe Kiela.
Poincar\’e Embeddings for Learning Hierarchical Rep-
resentations.
2017.
ISSN 19457871.
doi: 10.1109/ICME.2013.6607554.
URL http:
//arxiv.org/abs/1705.08039."
REFERENCES,0.5605095541401274,"Yasumasa Onoe, Michael Boratko, and Greg Durrett. Modeling fine-grained entity types with box
embeddings, 2021."
REFERENCES,0.5636942675159236,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-
an-imperative-style-high-performance-deep-learning-library.pdf."
REFERENCES,0.5668789808917197,"Dhruvesh Patel, Shib Sankar Dasgupta, Michael Boratko, Xiang Li, Luke Vilnis, and Andrew Mc-
Callum. Representing joint hierarchies with box embeddings. In Automated Knowledge Base
Construction, 2020. URL https://openreview.net/forum?id=J246NSqR_l."
REFERENCES,0.5700636942675159,"C. N. Silla and A. Freitas. A survey of hierarchical classification across different application do-
mains. Data Mining and Knowledge Discovery, 22:31–72, 2010."
REFERENCES,0.5732484076433121,"Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. Probabilistic Embedding of Knowl-
edge Graphs with Box Lattice Measures. 2018. URL https://people.cs.umass.edu/
˜luke/box-lattices.pdfhttp://arxiv.org/abs/1805.06627."
REFERENCES,0.5764331210191083,"Jonatas Wehrmann, Ricardo Cerri, and Rodrigo Barros. Hierarchical multi-label classification net-
works. In International Conference on Machine Learning, pp. 5075–5084. PMLR, 2018a."
REFERENCES,0.5796178343949044,"Jˆonatas Wehrmann, Ricardo Cerri, and Rodrigo C Barros. Hierarchical Multi-Label Classification
Networks. Technical report, 2018b."
REFERENCES,0.5828025477707006,Published as a conference paper at ICLR 2022
REFERENCES,0.5859872611464968,"A
IMPLEMENTATION DETAILS"
REFERENCES,0.589171974522293,"In this section, we describe the implementation details: training procedure and hyper-parameter
search."
REFERENCES,0.5923566878980892,"Frameworks used: We implement all the models described in this work using PyTorch (Paszke
et al., 2019). We also make use of NLP specific abstractions over PyTorch provided by AllenNLP
(Gardner et al., 2017). We also make use of the abstractions provided in the Box Embedding library
Chheda et al. (2021) in our implementation of MBM. For the extensive hyper-parameter search, we
use the academic version of Weights & Biases library (Biewald, 2020)."
REFERENCES,0.5955414012738853,"Data pre-processing: The datasets were pre-processed in the exact same manner as in Giunchiglia
& Lukasiewicz (2020), i.e., the categorical features were converted to one-hot feature vectors and
the continuous input features were standardized. Also, following the Giunchiglia & Lukasiewicz
(2020); Wehrmann et al. (2018a), the labels corresponding to the root nodes were removed from
training and evaluation. Further details of the original datasets is provided in B."
REFERENCES,0.5987261146496815,"Training: We used ADAM (Kingma & Ba, 2017) with a batch size of 4 to learn the model param-
eters for all the models. Since the naive implementation of the label interaction loss described in
Eq. 4 is too expensive to compute at each mini-batch step, we approximate it by randomly sampling
(without replacement), at each mini-batch step, a subset of edges ˜T ∼T . The size of the sampled
set is a hyper-parameter (last column in Table 3)."
REFERENCES,0.6019108280254777,"Hyper-parameter search: The following hyper-parameters were searched using the performance
on the validation set: optimizer learning rate, hidden dimensions of the MLP, number of hidden
layers (maximum of 3), activation functions, dropout probabilities, weight decay and sampling per-
centage for the labels for computing the label interaction loss(LG) for the MHM-T, MBM-T and
MVM-T models. After an initial investigation, we identified that the softplus activation performed
the best for all models and hence, the activation was fixed to softplus for the final grid search. Also
note that in order to be comparable to the previous work that uses these datasets, we had to divide our
experiments into two kinds of runs—the search runs and the final runs. First, in the search runs, we
use a validation set to obtain the best hyper-parameters for each model-dataset combination. After
identifying the best set of hyper-parameters the training and validation sets were combined. We call
the models trained on this combined dataset as the final runs. Since one cannot use early stopping
without a validation set, the number of epochs to train the models in the final runs was also obtained
as a hyper-parameter and was set to the best epoch for the corresponding search run. All the results
are reported using the metrics obtained in the final runs (10 runs with different random seeds for
each dataset-model pair with best hyper-parameters obtained from the corresponding search runs).
Table 3 summarizes the search ranges used and Table 4 presents the final hyper-parameters obtained."
REFERENCES,0.6050955414012739,"Table 3: Summary of the hyper-parameter search ranges for each dataset and model. The best hyper-
parameters for each model and dataset combination were picked using grid search using MAP on
the validation set. Note that ν and label sample percent are only applicable to the MBM-T, MVM-T
and MHM-T models."
REFERENCES,0.60828025477707,"Datasets
batch size
lr
hidden dim
layers
activation
dropout
LG weight (ν)
label sample percent"
REFERENCES,0.6114649681528662,"all
4
1e-4, 5e-4, 1e-3, 1e-2
250, 500, 1000, 1750
1,2,3
sigmoid, relu, softplus
0.0, 0.1, 0.3, 0.5
1e-3, 1e-6, 1e-7, 1e-9
10,20,30"
REFERENCES,0.6146496815286624,"Table 4: Best hyper-parameters obtained through grid search for each dataset-model combination.
It should be noted that the activation function chosen for all model configurations was softplus."
REFERENCES,0.6178343949044586,"Dataset
Model
learning rate
hidden dim
layers
dropout
% label sample
ν"
REFERENCES,0.6210191082802548,"CellcycleFUN
BoxE
0.001
500
3
0.3
-
-
CellcycleFUN
C-HMCNN
0.001
500
3
0.3
100
-
CellcycleFUN
MBM
0.0001
1750
3
0
-
-
CellcycleFUN
MBM-T
0.0001
500
3
0
30
1e-07
CellcycleFUN
MHM
0.0001
1000
3
0
-
-
CellcycleFUN
MHM-T
0.0001
1000
3
0
30
1e-07
CellcycleFUN
MVM
0.001
1750
2
0
-
-
CellcycleFUN
MVM-T
0.001
1750
2
0
30
1e-07"
REFERENCES,0.6242038216560509,Published as a conference paper at ICLR 2022
REFERENCES,0.6273885350318471,Table 4 continued from previous page
REFERENCES,0.6305732484076433,"Dataset
Model
learning rate
hidden dim
layers
dropout
% label sample
ν"
REFERENCES,0.6337579617834395,"CellcycleGO
BoxE
0.001
500
3
0.3
-
-
CellcycleGO
C-HMCNN
0.001
500
2
0.5
100
-
CellcycleGO
MBM
0.0001
1750
3
0
-
-
CellcycleGO
MBM-T
0.0001
1750
3
0
10
1e-07
CellcycleGO
MHM
0.001
1750
3
0.3
-
-
CellcycleGO
MHM-T
0.001
1750
3
0.3
30
1e-07
CellcycleGO
MVM
0.001
500
2
0.5
-
-
CellcycleGO
MVM-T
0.001
500
2
0.5
30
1e-07"
REFERENCES,0.6369426751592356,"DerisiFUN
BoxE
0.001
500
3
0.3
-
-
DerisiFUN
C-HMCNN
0.0001
1000
3
0.3
100
-
DerisiFUN
MBM
0.0001
500
3
0
-
-
DerisiFUN
MBM-T
0.0001
500
3
0
30
1e-07
DerisiFUN
MHM
0.001
500
3
0.1
-
-
DerisiFUN
MHM-T
0.001
500
3
0.1
30
1e-07
DerisiFUN
MVM
0.001
1750
3
0
-
-
DerisiFUN
MVM-T
0.001
1750
3
0
30
1e-07"
REFERENCES,0.6401273885350318,"DerisiGO
BoxE
0.001
500
3
0.3
-
-
DerisiGO
C-HMCNN
0.001
1000
3
0.3
100
-
DerisiGO
MBM
0.0001
500
3
0
-
-
DerisiGO
MBM-T
0.0001
500
3
0
20
1e-07
DerisiGO
MHM
0.001
500
3
0
-
-
DerisiGO
MHM-T
0.001
500
3
0
30
1e-07
DerisiGO
MVM
0.001
500
2
0.3
-
-
DerisiGO
MVM-T
0.001
500
2
0.3
30
1e-07"
REFERENCES,0.643312101910828,"Diatoms
BoxE
0.001
1750
3
0.3
-
-
Diatoms
C-HMCNN
0.0001
500
3
0.1
100
-
Diatoms
MBM
0.0001
1750
3
0
-
-
Diatoms
MBM-T
0.0001
1750
3
0
30
1e-07
Diatoms
MHM
0.0001
1750
3
0
-
-
Diatoms
MHM-T
0.0001
1750
3
0
30
1e-07
Diatoms
MVM
0.0001
1000
3
0
-
-
Diatoms
MVM-T
0.0001
1000
3
0
30
1e-07"
REFERENCES,0.6464968152866242,"Enron
BoxE
0.001
500
2
0.3
-
-
Enron
C-HMCNN
0.0001
500
2
0.3
100
-
Enron
MBM
0.0001
1750
2
0
-
-
Enron
MBM-T
0.0001
1750
2
0
30
1e-07
Enron
MHM
0.0001
1750
2
0.1
-
-
Enron
MHM-T
0.0001
1750
2
0.1
30
1e-07
Enron
MVM
0.0005
500
3
0
-
-
Enron
MVM-T
0.0005
500
3
0
30
1e-07"
REFERENCES,0.6496815286624203,"ExprFUN
BoxE
0.001
500
3
0.3
-
-
ExprFUN
C-HMCNN
0.0001
1000
3
0.3
100
-
ExprFUN
MBM
0.0005
1000
3
0
-
-
ExprFUN
MBM-T
0.0005
1000
3
0
30
1e-07
ExprFUN
MHM
0.0001
1750
3
0
-
-
ExprFUN
MHM-T
0.0001
1750
3
0
30
1e-07
ExprFUN
MVM
0.0005
500
3
0.5
-
-
ExprFUN
MVM-T
0.0005
500
3
0.5
30
1e-07"
REFERENCES,0.6528662420382165,"ExprGO
BoxE
0.0001
1750
3
0.3
-
-
ExprGO
C-HMCNN
0.001
500
2
0.5
100
-
ExprGO
MBM
0.0001
1000
2
0
-
-
ExprGO
MBM-T
0.0001
1000
2
0
5
1e-07
ExprGO
MHM
0.0001
1750
3
0
-
-
ExprGO
MHM-T
0.0001
1750
3
0
30
1e-07
ExprGO
MVM
0.0001
1750
3
0.1
-
-
ExprGO
MVM-T
0.0001
1750
3
0.1
30
1e-07"
REFERENCES,0.6560509554140127,"Imclef07a
BoxE
0.001
1000
3
0.1
-
-
imclef07a
C-HMCNN
0.001
1000
2
0
100
-"
REFERENCES,0.6592356687898089,Published as a conference paper at ICLR 2022
REFERENCES,0.6624203821656051,Table 4 continued from previous page
REFERENCES,0.6656050955414012,"Dataset
Model
learning rate
hidden dim
layers
dropout
% label sample
ν"
REFERENCES,0.6687898089171974,"Imclef07a
MBM
0.0001
1750
3
0
-
-
Imclef07a
MBM-T
0.0005
1750
3
0.25
30
1e-07
Imclef07a
MHM
0.001
1000
2
0
-
-
Imclef07a
MHM-T
0.001
1000
2
0
30
1e-07
Imclef07a
MVM
0.001
1000
3
0
-
-
Imclef07a
MVM-T
0.001
1000
3
0
30
1e-07"
REFERENCES,0.6719745222929936,"Imclef07d
BoxE
0.001
1750
2
0.3
-
-
Imclef07d
C-HMCNN
0.001
500
2
0.1
100
-
Imclef07d
MBM
0.0005
1750
3
0
-
-
Imclef07d
MBM-T
0.0005
1750
3
0
30
1e-09
Imclef07d
MHM
0.001
1750
2
0
-
-
Imclef07d
MHM-T
0.001
1750
2
0
30
1e-07
Imclef07d
MVM
0.001
1000
2
0
-
-
Imclef07d
MVM-T
0.001
1000
2
0
30
1e-07"
REFERENCES,0.6751592356687898,"SpoFUN
BoxE
0.001
500
3
0.3
-
-
SpoFUN
C-HMCNN
0.001
500
3
0.5
100
-
SpoFUN
MBM
0.0001
1000
3
0
-
-
SpoFUN
MBM-T
0.0005
500
3
0
30
1e-09
SpoFUN
MHM
0.0001
1000
3
0
-
-
SpoFUN
MHM-T
0.0001
1000
3
0
30
1e-07
SpoFUN
MVM
0.001
1750
3
0
-
-
SpoFUN
MVM-T
0.001
1750
3
0
30
1e-07"
REFERENCES,0.678343949044586,"SpoGO
BoxE
0.001
500
3
0.3
-
-
SpoGO
C-HMCNN
0.001
1000
3
0.3
100
-
SpoGO
MBM
0.0001
1000
3
0
-
-
SpoGO
MBM-T
0.0001
1000
3
0
10
1e-07
SpoGO
MHM
0.001
500
3
0
-
-
SpoGO
MHM-T
0.001
500
3
0
30
1e-07
SpoGO
MVM
0.001
500
2
0.5
-
-
SpoGO
MVM-T
0.001
500
2
0.5
30
1e-07"
REFERENCES,0.6815286624203821,"A.1
FURTHER DETAILS FOR THE BASELINES"
REFERENCES,0.6847133757961783,"BoxE: Let p ∈Rd be a point and z, Z ∈Rd with zi < Zi be the lower left and upper right
coordinates of a non-probabilistic label box. Then a non-probabilistic compatibility score between
the point and the box can be defined as shown in Abboud et al. (2020). Concretely, Abboud et al.
(2020) define per-dimension distance as"
REFERENCES,0.6878980891719745,"dist(pi, zi, Zi) :="
REFERENCES,0.6910828025477707,( |pi−ci|
REFERENCES,0.6942675159235668,"wi+1 ,
if zi ≤pi ≤Zi
|pi−ci|"
REFERENCES,0.697452229299363,"wi+1 −κ,
otherwise,
(7)"
REFERENCES,0.7006369426751592,"where ci = (Zi −zi)/2, wi = Zi −zi are the center and width of the projection of the box
(i.e., an interval) in dimension i, and κ = 0.5 ∗wi ∗(wi + 1 −
1
wi+1) is the width dependent
factor. The score function that measures the compatibility of the point p and box (z, Z) is defined
as sBoxE(p, z, Z) = −∥v∥2, where vi = dist(pi, zi, Zi). Given the trainable parameters ψ−, ψ+ ∈
RL×d for representing the labels, the lower left and the upper right coordinates of a non-probabilistic
box for label li are taken to be zi = ψ−
i,∗and Zi = ψ−
i,∗+ log(1 + exp ψ+
i,∗), respectively. The input
x is encoded as a point p ∈Rd using a multi-layer perceptron Fθ, just like MVM. However, since
the score is negative distance, the parameters are learnt using margin based loss given as:"
REFERENCES,0.7038216560509554,"Lmargin(ψ, θ; D) = D
X i=1 X"
REFERENCES,0.7070063694267515,"p∈Pi,n∈Ni"
REFERENCES,0.7101910828025477,"max (0, 1 −(sBoxE(Fθ(x), zp, Zp) −sBoxE(Fθ(x), zn, Zn))"
REFERENCES,0.7133757961783439,"|Pi| + |Ni|
,"
REFERENCES,0.7165605095541401,"where Pi and Ni are the positive and negative labels for i-th data instance, respectively."
REFERENCES,0.7197452229299363,Published as a conference paper at ICLR 2022
REFERENCES,0.7229299363057324,"A.2
COMPUTATIONAL COMPLEXITY"
REFERENCES,0.7261146496815286,"The computational complexity of the MBM can be divided into two parts–the computation of the
instance box Boxθ(X), and the computation of probability score P(Y | X) given the encoding
of the instance and label box parameters. Computing Boxθ(X) amounts to splitting the output of a
feed-forward network ϕ(x) into min and max parameters ϕ(x)−, ϕ(x)+, and thus has the same com-
plexity as that of the MVM. Assuming that the operations P, Q, log and exp have unit complexity,
computing P(Y | X) also has equivalent complexity. The computation of conditional probability
for a single label yk using the MVM model involves computing σ(ϕ(x) · yk), an O(dL) operation.
Now, with same assumptions for the MBM, computing λ(Boxθ(x) ∩Boxψ(yk)) involves d invo-
cations of log(exp(·) + exp(·)) followed by d subtractions and d invocations of log(1 + exp(.)),
resulting in O(d) operations. The computation of λ(Boxθ(x)) omits the intersection calculation, but
otherwise is the same, and thus is also O(d). Hence, calculating λ(Box(x)∩Box(yk))"
REFERENCES,0.7292993630573248,"λ(Box(x))
is O(d), resulting
in overall complexity of O(dL), which is equal to that of MVM. We note that the complexity of our
model is not dependent on the depth of the hierarchy, as in C-HMCNN, and thus can reasonably
scale to arbitrarily deep hierarchies as might be present in extreme multi-label classification. These
theoretical statements are supported by Table 5, which provides average epoch duration in seconds
for MVM, MBM, MBM-T, and C-HMCNN with the same hidden size. Hence, we can conclude that
the computational complexity of MVM ≈MBM < MBM-T."
REFERENCES,0.732484076433121,"Table 5: Average epoch duration in seconds for MVM, MBM, MBM-T and CHMCNN with the
same hidden size"
REFERENCES,0.7356687898089171,"Model
ExprFUN
CellcycleFUN
SpoFUN
DerisiFUN"
REFERENCES,0.7388535031847133,"MVM
17.2
15.4
15.8
15.5
MBM
18.4
16.9
16.3
15.2
MBM-T
20.1
26.7
24.1
27.1
C-HMCNN
17.8
17.0
15.9
15.4
BoxE
18.9
16.6
15.9
16.1"
REFERENCES,0.7420382165605095,"Computational resources used: For datasets with number of labels less than 500, i.e., the 4 FUN-
CAT datasets, Imclef07a, Imclef07d, Diatoms and Enron, all the models were trained on TitanX
GPU (memory=12GB). For the 4 GO datasets that have number of labels greater than 4000, all the
models are trained on M40 GPU (memory=24GB)."
REFERENCES,0.7452229299363057,"A.3
CODE"
REFERENCES,0.7484076433121019,"Executable python code with detailed instructions to reproduce the results reported in 1 is provided
using at https://github.com/iesl/box-mlc-iclr-2022. We also include instructions
for obtaining the pre-processed datasets, training a new model from scratch (MBM or any baseline),
evaluating a pre-trained model on test set, directly downloading the pre-trained models for datasets."
REFERENCES,0.7515923566878981,"B
DATASETS"
REFERENCES,0.7547770700636943,"The 12 datasets used in this work wary greatly in terms of domain, number of labels, number of
instances, and the connectivity of label taxonomy. The characteristics of each dataset w.r.t these
properties is summarized in 6. These datasets do not require a licence and are available for public
usage. The links to the sources for all the datasets are provided in Table 7 below. The continuous
features are standardized by removing mean and scaling to unit variance, and the categorical features
are encoded as one-hot vectors."
REFERENCES,0.7579617834394905,Published as a conference paper at ICLR 2022
REFERENCES,0.7611464968152867,"Table 6: Summary of the datasets used in experiments. The feature based multi-label datasets span
across 3 domains: functional genomics, image and text."
REFERENCES,0.7643312101910829,"Dataset
Domain
Input/Feature
Type
Label
Taxonomy
#Labels
#Instances
Train
Val
Test"
REFERENCES,0.767515923566879,"Expr FUN
Genomics
Continuous
Forest
500
1636
849
1288
Cellcycle FUN
Genomics
Continuous
Forest
500
1628
848
1281
Derisi FUN
Genomics
Continuous
Forest
500
1608
842
1275
Spo FUN
Genomics
Continuous
Forest
500
1600
837
1266
Expr GO
Genomics
Continuous
DAG
4132
1636
849
1288
Cellcycle GO
Genomics
Continuous
DAG
4126
1625
848
1278
Derisi GO
Genomics
Continuous
DAG
4120
1605
842
1272
Spo GO
Genomics
Continuous
DAG
4120
1597
837
1263
Diatoms
Image
Continuous
Tree
399
1500
565
1054
Imclef07a
Image
Continuous
Tree
97
7000
3000
1006
Imclef07d
Image
Continuous
Tree
47
7000
3000
1006
Enron
Text
Binary
Tree
57
650
338
600"
REFERENCES,0.7707006369426752,Table 7: The table provides the links to download the data from original source.
REFERENCES,0.7738853503184714,"Dataset(s)
Download Links"
REFERENCES,0.7770700636942676,"Imclef07a, Imclef07d, Enron, Diatoms
http://kt.ijs.si/DragiKocev/PhD/resources/doku.php?id=hmc_classification
Expr, Spo, Derisi, Cellcycle (FUN/GO)
https://dtai.cs.kuleuven.be/clus/hmcdatasets/"
REFERENCES,0.7802547770700637,"C
SIGNIFICANCE TESTING"
REFERENCES,0.7834394904458599,"In section 6, we provide the results for Friedman test followed by post-hoc Nemenyi test, and also
the result of pairwise Wilcoxon test where the former are not sufficient, i.e. for the case of MBM vs
C-HMCNN w.r.t MAP. For the sake of completeness, table 8 provides results of pairwise Wilcoxon
test comparing all models with MBM and MBM-T w.r.t all three metrics."
REFERENCES,0.7866242038216561,"Table 8: The table presents the results of the Wilcoxon signed-rank test. Each cell shows the p-
value for the null hypothesis that two related paired samples, here MBM/MBM-T and the other
model (column), come from the same distribution, with the alternative hypothesis that MBM/MBM-
T models are better in performance compared to the other model."
REFERENCES,0.7898089171974523,"Metric
BoxE
MVM
MHM
MBM
MVM-T
MHM-T
MBM-T
C-HMCNN"
REFERENCES,0.7929936305732485,"MAP
MBM
0.0005
0.0002
0.0002
-
0.0002
0.0002
0.4392
0.3386
MBM-T
0.0171
0.0261
0.0002
0.5608
0.0212
0.0002
-
0.4849"
REFERENCES,0.7961783439490446,"CMAP
MBM
0.0007
0.0002
0.0002
-
0.0002
0.0002
0.3611
0.1506
MBM-T
0.0212
0.0134
0.0002
0.6389
0.0134
0.0002
-
0.3667"
REFERENCES,0.7993630573248408,"CV
MBM
0.0002
0.0012
0.7407
-
0.0012
0.6890
1.0
1.0
MBM-T
0.0002
0.0002
0.0046
0.0002
0.0002
0.0046
-
1.0"
REFERENCES,0.802547770700637,"D
COMPARISON TO C-HMCNN AND HMCN IN TERMS OF AU(PRC)"
REFERENCES,0.8057324840764332,"Due to our focus on coherence and predictive performance at the same time, we use mean average
precision as the metric to evaluate predictive performance. One could, however, also use AU(PRC)
as the metric for predictive performance. For the sake of completeness, in Table 9, we present
the AU(PRC) values for the final models obtained in our experiments, and compare it with those
provided in Giunchiglia & Lukasiewicz (2020) and Wehrmann et al. (2018a). We believe that the
difference between the AU(PRC) reported for C-HMCNN in Giunchiglia & Lukasiewicz (2020) and
our work is due to the difference in the hyper-parameter search strategy. Specifically, we use MAP as
the reference metric to identify the best hyper-parameters while Giunchiglia & Lukasiewicz (2020)
uses AU(PRC) itself."
REFERENCES,0.8089171974522293,Published as a conference paper at ICLR 2022
REFERENCES,0.8121019108280255,"Table 9: The table presents the area under the precision-recall curve (AU(PRC)) for the models
presented in this paper and HMCN-F (Wehrmann et al., 2018a). Here, the columns with * are taken
from their respective papers, and C-HMCNN is our implementation of the corresponding model for
which the best hyper-parameters are also obtained, like other models in our implementation, using
MAP on validation set."
REFERENCES,0.8152866242038217,"Dataset
C-HMCNN*
HCMN-F*
C-HMCNN
BoxE
MVM
MHM
MBM"
REFERENCES,0.8184713375796179,"ExprFUN
30.2
30.1
28.17
16.23
28.5
19.6
25.62
CellcycleFUN
25.5
25.2
24.46
12.25
21.19
18.19
21.57
DerisiFUN
19.5
19.3
17.61
10.19
14.45
17.42
18.74
SpoFUN
21.5
21.1
20.72
13.69
16.19
17.16
19.57"
REFERENCES,0.821656050955414,"ExprGO
44.7
45.2
41.92
21.74
40.5
28.91
40.89
CellcycleGO
41.3
40
40.25
25.86
39.39
33.77
38.19
DerisiGO
37
36.9
36.35
16.22
34.63
34.59
36.42
SpoGO
38.2
37.6
37.09
23.4
36.54
34.37
36.38"
REFERENCES,0.8248407643312102,"Enron
75.6
72.4
75.18
72.67
69
24.76
74.53
Diatoms
75.8
53
70.39
1.86
68.3
38.29
78.21
Imclef07a
95.6
95
92.81
5.43
80.57
58.06
94.61
Imclef07d
92.7
92
89.29
7.96
89.73
64.33
90.48"
REFERENCES,0.8280254777070064,"E
MAP AND CMAP"
REFERENCES,0.8312101910828026,"In MCL problems, where predictions consistent with a taxonomy are necessary, the model designer
might be ready to sacrifice some absolute predictive performance for increased coherence, where
the predictive performance can be measure using Mean Average Precision (MAP). First recall that
Average Precision (AP) for an instance is the weighted mean of precisions achieved at each thresh-
old, with the increase in recall from the previous threshold used as the weight, and Mean Average
Precision (MAP) is the mean of AP across instances. Given data D, let s(k)
i
= P(yi = 1|x(k))
denote the score generated by the model for label i given the input x(k). Then MAP is computed as"
REFERENCES,0.8343949044585988,"MAP(s) =
1
|D| |D|
X"
REFERENCES,0.8375796178343949,"k=1
AP

s(k), y(k)
."
REFERENCES,0.8407643312101911,"As described in Section 6.1, given the scores s ∈RL produced by a model, for an input, the two
post-hoc modification functions that can be applied to make these score coherent are given as"
REFERENCES,0.8439490445859873,"δm
G (s)i =
min
lj∈AncG(li)∪{li} sj,
δM
G (s)i =
max
lj∈DesG(li)∪{li} sj,
(8)"
REFERENCES,0.8471337579617835,"where AncG(l) and DesG(l) is the set of ancestors and descendants, respectively, of l in the graph
G."
REFERENCES,0.8503184713375797,"One can compute MAP after applying either one of the two modification functions. If a human were
to make consistent score corrections based on the complete hierarchy, they would either pick the
score of a label to be the minimum of its ancestors or maximum of its descendants. Once picked, the
same approach has to be applied to all the labels. We argue that in terms of correlation with human
judgment, both these methods are equivalent. Hence, one can pick the modification function based
on empirical performance of the post-hoc correction algorithm. We observe that δm
G , i.e., “min of
ancestors” approach, produced higher MAP with both MVM and MBM models compared to δM
G . as
can be seen in table 10. We believe that ”min of ancestors” works better because it promotes sparsity
in predictions, which is essential for the task of MLC."
REFERENCES,0.8535031847133758,Published as a conference paper at ICLR 2022
REFERENCES,0.856687898089172,"Table 10: CMAP represents post-hoc correction of scores by taking the ”minimum of ancestors”,
CMAP’ represents post-hoc correction of scores by taking ”maximum of descendants”. As seen,
CMAP produces better results for both MBM and MVM compared to CMAP’."
REFERENCES,0.8598726114649682,"Dataset
MVM
MBM
CMAP
CMAP’
MAP
CMAP
CMAP’
MAP"
REFERENCES,0.8630573248407644,"ExprFUN
37.66
14.83
38.37
38.72
1.78
38.45
CellcycleFUN
31.34
10.62
31.68
34.39
1.79
34.2
DerisiFUN
24.13
6.28
23.7
28.86
1.79
28.69
SpoFUN
25.38
7.4
25.18
30.27
1.79
30.1
ExprGO
41.78
26.15
44.92
48.56
0.8
48.45
CellcycleGO
41.09
21.64
44.25
44.23
0.8
44.28
DerisiGO
38.21
19.68
41.22
42.14
0.8
42.03
SpoGO
38.96
19.79
42.19
42.31
0.8
42.22
Enron
66.87
55.48
73.68
79.94
9.96
79.95
Diatoms
72.44
58.36
73.06
79.3
0.49
79.14
Imclef07a
76.56
69.66
77.14
91.73
3.12
91.45
Imclef07d
86.89
81.32
88.49
89.99
6.52
89.49"
REFERENCES,0.8662420382165605,"F
PERFORMANCE ON LONG TAIL OF LABELS"
REFERENCES,0.8694267515923567,"Since MVM and MBM both are embedding based models, we expect the performance of the MBM
model to wary with the label frequency in the same manner as MVM. This hypothesis is corroborated
by the following table which shows the spearman rank correlation between the label frequency and
the mean average precision (MAP) for that label. The high positive correlation for all the models
suggests that the performance of both MVM and MBM often degrades as the frequency of the labels
drop. However, as shown by the table 11, both the models perform reasonably well for extremely low
frequency labels. Here, tail MAP is the mean average precision for labels with frequency 0.001%
or lower."
REFERENCES,0.8726114649681529,"Table 11: Comparing the performance of long tails of labels for MBM, MBM-T and MVM. Here,
correlation is the spearman rank correlation between label frequency and MAP of the label and
tail MAP is the MAP for labels with frequency 0.001% or lower."
REFERENCES,0.8757961783439491,"Dataset
MBM
MBM-T
MVM
Correlation
tail MAP
Correlation
tail MAP
Correlation
tail MAP"
REFERENCES,0.8789808917197452,"ExprFUN
0.72
48.91
0.73
48.68
0.75
49.45
CellcycleFUN
0.74
46.11
0.75
46.17
0.82
44.02
DerisiFUN
0.79
41.86
0.82
41.62
0.79
41.78
SpoFUN
0.8
42.67
0.77
42.75
0.79
42.15
Enron
0.84
79.19
0.83
74.97
0.91
67.63
Diatoms
0.29
86.3
0.29
85.46
0.28
82.2
Imclef07a
0.73
59.59
0.74
60.2
0.82
58.78"
REFERENCES,0.8821656050955414,"G
EFFECT OF EMBEDDING DIMENSION"
REFERENCES,0.8853503184713376,"To study the effect of embedding dimensions on the performance, we plot the MAP and CV vs the
hidden dimension size for a subset of the datasets. As shown in figure 4, at extremely low dimen-
sions, the MBM model consistently outperforms the the most competitive embedding based model
MVM, demonstrating the usefulness of the favorable inductive bias of the box space in inducing
parsimonious representations."
REFERENCES,0.8885350318471338,Published as a conference paper at ICLR 2022
REFERENCES,0.89171974522293,"0
200
400
600
800
1000
Embedding dimension 38 40 42 44 46 48 MAP"
REFERENCES,0.8949044585987261,Performance variation w.r.t embedding dimension
REFERENCES,0.8980891719745223,(a) MAP
REFERENCES,0.9012738853503185,"0
200
400
600
800
1000
Embedding dimension 1.5 2.0 2.5 3.0 3.5 CV"
REFERENCES,0.9044585987261147,Performance variation w.r.t embedding dimension
REFERENCES,0.9076433121019108,(b) CV
REFERENCES,0.910828025477707,"Figure 4: Above figure shows the variation of performance w.r.t the embedding dimensions for
model MBM and MVM on FUNCAT datasets. Note that the training and evaluation performed for
this analysis includes the root node."
REFERENCES,0.9140127388535032,Published as a conference paper at ICLR 2022
REFERENCES,0.9171974522292994,"H
EFFECT OF HIERARCHY ON THE MARGINAL SCORE"
REFERENCES,0.9203821656050956,"0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
2
10"
REFERENCES,0.9235668789808917,number of children 405 410 415 420 425 430 435 440
REFERENCES,0.9267515923566879,marginal score
REFERENCES,0.9299363057324841,(a) Expr FUN
REFERENCES,0.9331210191082803,"0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
2
10"
REFERENCES,0.9363057324840764,number of children 125 150 175 200 225 250 275 300
REFERENCES,0.9394904458598726,marginal score
REFERENCES,0.9426751592356688,(b) Cellcycle FUN
REFERENCES,0.945859872611465,"0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
2
10"
REFERENCES,0.9490445859872612,number of children 114 116 118 120 122 124 126
REFERENCES,0.9522292993630573,marginal score
REFERENCES,0.9554140127388535,(c) Derisi FUN
REFERENCES,0.9585987261146497,"0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
2
10"
REFERENCES,0.9617834394904459,number of children 232.5 235.0 237.5 240.0 242.5 245.0 247.5
REFERENCES,0.964968152866242,marginal score
REFERENCES,0.9681528662420382,(d) Spo FUN
REFERENCES,0.9713375796178344,"0
2
1
2
2
2
3
2
4
2
5
2
6
2
7"
REFERENCES,0.9745222929936306,number of children 347.5 350.0 352.5 355.0 357.5 360.0 362.5 365.0 367.5
REFERENCES,0.9777070063694268,marginal score
REFERENCES,0.9808917197452229,(e) Enron
REFERENCES,0.9840764331210191,"0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
2
10"
REFERENCES,0.9872611464968153,number of children 405 410 415 420 425 430 435 440
REFERENCES,0.9904458598726115,marginal score
REFERENCES,0.9936305732484076,(f) Diatoms
REFERENCES,0.9968152866242038,"Figure 5: As discussed in Section 7, the scatter plots further show that the magnitude of box em-
beddings(box volume) relate to the level of generality in taxonomy.In the figure, the marginal score
refers to box volume of a label and the number of children in the label space taxonomy refers to the
level of generality.Note that the training and evaluation performed for this analysis includes the root
node."
