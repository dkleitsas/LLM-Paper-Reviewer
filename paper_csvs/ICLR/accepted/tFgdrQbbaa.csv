Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001282051282051282,"Sequential training from task to task is becoming one of the major objects in deep
learning applications such as continual learning and transfer learning. Nevertheless,
it remains unclear under what conditions the trained model‚Äôs performance improves
or deteriorates. To deepen our understanding of sequential training, this study
provides a theoretical analysis of generalization performance in a solvable case of
continual learning. We consider neural networks in the neural tangent kernel (NTK)
regime that continually learn target functions from task to task, and investigate
the generalization by using an established statistical mechanical analysis of kernel
ridge-less regression. We Ô¨Årst show characteristic transitions from positive to
negative transfer. More similar targets above a speciÔ¨Åc critical value can achieve
positive knowledge transfer for the subsequent task while catastrophic forgetting
occurs even with very similar targets. Next, we investigate a variant of continual
learning which supposes the same target function in multiple tasks. Even for the
same target, the trained model shows some transfer and forgetting depending on the
sample size of each task. We can guarantee that the generalization error monotoni-
cally decreases from task to task for equal sample sizes while unbalanced sample
sizes deteriorate the generalization. We respectively refer to these improvement and
deterioration as self-knowledge transfer and forgetting, and empirically conÔ¨Årm
them in realistic training of deep neural networks as well."
INTRODUCTION,0.002564102564102564,"1
INTRODUCTION"
INTRODUCTION,0.0038461538461538464,"As deep learning develops for a single task, it enables us to work on more complicated learning frame-
works where the model is sequentially trained on multiple tasks, e.g., transfer learning, curriculum
learning, and continual learning. Continual learning deals with the situation in which the learning
machine cannot access previous data due to memory constraints or privacy reasons. It has attracted
much attention due to the demand on applications, and fundamental understanding and algorithms
are being explored (Hadsell et al., 2020). One well-known phenomenon is catastrophic forgetting;
when a network is trained between different tasks, naive training cannot maintain performance on the
previous task (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017)."
INTRODUCTION,0.005128205128205128,"It remains unclear in most cases under what conditions a trained model‚Äôs performance improves
or deteriorates in sequential training. Understanding its generalization performance is still limited
(Pentina & Lampert, 2014; Bennani et al., 2020; Lee et al., 2021). For single-task training, however,
many empirical and theoretical studies have succeeded in characterizing generalization performance
in over-parameterized neural networks and given quantitative evaluation on sample-size dependencies,
e.g., double descent (Nakkiran et al., 2020). For further development, it will be helpful to extend the
analyses on single-task training to sequential training on multiple tasks and give theoretical backing."
INTRODUCTION,0.00641025641025641,"To deepen our understanding of sequential training, this study shows a theoretical analysis of its
generalization error in the neural tangent kernel (NTK) regime. In more details, we consider the
NTK formulation of continual learning proposed by Bennani et al. (2020); Doan et al. (2021). By
extending a statistical mechanical analysis of kernel ridge-less regression, we investigate learning
curves, i.e., the dependence of generalization error on sample size or number of tasks. The analysis"
INTRODUCTION,0.007692307692307693,Published as a conference paper at ICLR 2022
INTRODUCTION,0.008974358974358974,"focuses on the continual learning with explicit task boundaries. The model learns data generated
by similar teacher functions, which we call target functions, from one task to another. All input
samples are generated from the same distribution in an i.i.d. manner, and each task has output samples
(labels) generated by its own target function. Our main contributions are summarized as follows.
First, we revealed characteristic transitions from negative to positive transfer depending on the target
similarity. More similar targets above a speciÔ¨Åc critical value can achieve positive knowledge transfer
(i.e., better prediction on the subsequent task than training without the Ô¨Årst task). Compared to
this, backward transfer (i.e., prediction on the previous task) has a large critical value, and subtle
dissimilarity between targets causes negative transfer. The error can rapidly increase, which clariÔ¨Åes
that catastrophic forgetting is literally catastrophic (Section 4.1)."
INTRODUCTION,0.010256410256410256,"Second, we considered a variant of continual learning, that is, learning of the same target function
in multiple tasks. Even for the same target function, the trained model‚Äôs performance improves
or deteriorates in a non-monotonic way. This depends on the sample size of each task; for equal
sample sizes, we can guarantee that the generalization error monotonically decreases from task to task
(Section 4.2 for two tasks & Section 5 for more tasks). Unbalanced sample sizes, however, deteriorates
generalization (Section 4.3). We refer to these improvement and deterioration of generalization as
self-knowledge transfer and forgetting, respectively. Finally, we empirically conÔ¨Årmed that self-
knowledge transfer and forgetting actually appear in the realistic training of multi-layer perceptron
(MLP) and ResNet-18 (Section 5.1). Thus, this study sheds light on fundamental understanding and
the universal behavior of sequential training in over-parameterized learning machines."
RELATED WORK,0.011538461538461539,"2
RELATED WORK"
RELATED WORK,0.01282051282051282,"Method of analysis. As an analysis tool, we use the replica method originally developed for statistical
mechanics. Statistical mechanical analysis enables us typical-case evaluation, that is, the average
evaluation over data samples or parameter conÔ¨Ågurations. It sometimes provides us with novel insight
into what the worst-case evaluation has not captured (Abbaras et al., 2020; Spigler et al., 2020).
The replica method for kernel methods has been developed in Dietrich et al. (1999), and recently
in Bordelon et al. (2020); Canatar et al. (2021). These recent works showed excellent agreement
between theory and experiments on NTK regression, which enables us to quantitatively understand
sample-size dependencies including implicit spectral bias, double descent, and multiple descent."
RELATED WORK,0.014102564102564103,"Continual learning. Continual learning dynamics in the NTK regime was Ô¨Årst formulated by Bennani
et al. (2020); Doan et al. (2021), though the evaluation of generalization remains unclear. They derived
an upper bound of generalization via the Rademacher complexity, but it includes naive summation
over single tasks and seems conservative. In contrast, our typical-case evaluation enables us to newly
Ô¨Ånd such rich behaviors as negative/positive transfer and self-knowledge transfer/forgetting. The
continual learning in the NTK regime belongs to so-called single-head setting (Farquhar & Gal, 2018),
and it allows the model to revisit the previous classes (target functions) in subsequent tasks. This is
complementary to earlier studies on incremental learning of new classes and its catastrophic forgetting
(Ramasesh et al., 2020; Lee et al., 2021), where each task includes different classes and does not allow
the revisit. Note that the basic concept of continual learning is not limited to incremental learning
but allows the revisit (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017). Under the limited data
acquisition or resources of memory, we often need to train the same model with the same targets (but
different samples) from task to task. Therefore, the setting allowing the revisit seems reasonable."
PRELIMINARIES,0.015384615384615385,"3
PRELIMINARIES"
NEURAL TANGENT KERNEL REGIME,0.016666666666666666,"3.1
NEURAL TANGENT KERNEL REGIME"
NEURAL TANGENT KERNEL REGIME,0.017948717948717947,"We summarize conventional settings of the NTK regime (Jacot et al., 2018; Lee et al., 2019). Let us
consider a fully connected neural network f = uL given by"
NEURAL TANGENT KERNEL REGIME,0.019230769230769232,"ul = œÉwWlhl‚àí1/
p"
NEURAL TANGENT KERNEL REGIME,0.020512820512820513,"Ml‚àí1 + œÉbbl, hl = œÜ(ul)
(l = 1, ..., L),
(1)"
NEURAL TANGENT KERNEL REGIME,0.021794871794871794,"where we deÔ¨Åne weight matrices Wl ‚ààRMl√óMl‚àí1, bias terms bl ‚ààRMl, and their variances œÉ2
w
and œÉ2
b. We set random Gaussian initialization Wl,ij, bl,i ‚àºN(0, 1) and focus on the mean squared
error loss: L(Œ∏) = PN
¬µ=1 ‚à•y¬µ ‚àíf(x¬µ)‚à•2, where the training samples {x¬µ, y¬µ}N
¬µ=1 are composed of"
NEURAL TANGENT KERNEL REGIME,0.023076923076923078,Published as a conference paper at ICLR 2022
NEURAL TANGENT KERNEL REGIME,0.02435897435897436,"inputs x¬µ ‚ààRD normalized by ‚à•x¬µ‚à•2 = 1 and labels y¬µ ‚ààRC. The set of all parameters is denoted
as Œ∏, and the number of training samples is N. Assume the inÔ¨Ånite-width limit for hidden layers
(Ml ‚Üí‚àû), Ô¨Ånite sample size and depth. The gradient descent dynamics with a certain learning rate
then converges to a global minimum sufÔ¨Åciently close to the random initialization. This is known as
the NTK regime, and the trained model is explicitly obtained as"
NEURAL TANGENT KERNEL REGIME,0.02564102564102564,"f (c)(x‚Ä≤) = f (c)
0 (x‚Ä≤) + Œò(x‚Ä≤, X)Œò(X)‚àí1(y(c) ‚àíf (c)
0 (X))
(c = 1, ..., C).
(2)
We denote the NTK matrix as Œò, arbitrary input samples (including test samples) as x‚Ä≤, and the set
of training samples as X. The indices of f mean 0 for the model at initialization and c for the head
of the network. Entries of NTK Œò(x‚Ä≤, x) are deÔ¨Åned by ‚àáŒ∏f0(x‚Ä≤)‚àáŒ∏f0(x)‚ä§. We write Œò(X) as an
abbreviation for Œò(X, X). The trained network is equivalent to a linearized model around random
initialization (Lee et al., 2019), that is, f (c) = f (c)
0
+ ‚àáŒ∏f (c)
0 ‚àÜŒ∏ with"
NEURAL TANGENT KERNEL REGIME,0.026923076923076925,"‚àÜŒ∏ = Œ∏ ‚àíŒ∏0 = C
X"
NEURAL TANGENT KERNEL REGIME,0.028205128205128206,"c=1
‚àáŒ∏f (c)
0 (X)‚ä§Œò(X)‚àí1(y(c) ‚àíf (c)
0 (X)).
(3)"
NEURAL TANGENT KERNEL REGIME,0.029487179487179487,"While over-parameterized models have many global minima, the NTK dynamics implicitly select
the above Œ∏, which corresponds to the L2 min-norm solution. Usually, we ignore f0 by taking the
average over random initialization. The trained model (2) is then equivalent to the kernel ridge-less
regression (KRR)."
NEURAL TANGENT KERNEL REGIME,0.03076923076923077,"NTK regime also holds in various architectures including ResNets and CNNs (Yang & Littwin,
2021), and the difference only appears in the NTK matrix. Although we focus on the fully connected
network in synthetic experiments, the following NTK formulation of sequential training and our
theoretical results hold in any architecture under the NTK regime."
NEURAL TANGENT KERNEL REGIME,0.03205128205128205,"NTK formulation of Continual learning. We denote the set of training samples in the n-th task as
(Xn, yn) (n = 1, 2, ..., K), a model trained in a sequential manner from task 1 to task n as fn and its
parameters as Œ∏n. That is, we train the network initialized at Œ∏n‚àí1 for the n-th task and obtain fn.
Assume that the number of tasks K is Ô¨Ånite. The trained model within the NTK regime is then given
as follows (Bennani et al., 2020; Doan et al., 2021):
fn(x‚Ä≤) = fn‚àí1(x‚Ä≤) + Œò(x‚Ä≤, Xn)Œò(Xn)‚àí1(yn ‚àífn‚àí1(Xn)),
(4)"
NEURAL TANGENT KERNEL REGIME,0.03333333333333333,"Œ∏n ‚àíŒ∏n‚àí1 = ‚àáŒ∏f0(Xn)‚ä§Œò(Xn)‚àí1(yn ‚àífn‚àí1(Xn)).
(5)
We omit the index c because each head is updated independently. The model fn completely Ô¨Åts
the n-th task, i.e., yn = fn(Xn). The main purpose of this study is to analyze the generalization
performance of the sequentially trained model (4). At each task, the model has an inductive bias of
KRR in the function space and L2 min-norm solution in the parameter space. The next task uses
this inductive bias as the initialization of training. The problem is whether this inductive bias helps
improve the generalization in the subsequent tasks."
NEURAL TANGENT KERNEL REGIME,0.03461538461538462,"Remark (independence between different heads).
For a more accurate understanding of the
continual learning in the NTK regime, it may be helpful to remark on the heads‚Äô independence, which
previous studies did not explicitly mention. As in Eq. (2), all heads share the same NTK, and f (c)"
NEURAL TANGENT KERNEL REGIME,0.035897435897435895,"depends only on the label of its class y(c). While the parameter update (3) includes information of all
classes, the c-th head can access only the information of the c-th class1. For example, suppose that
the n-th task includes all classes except 1, i.e., {2, ..., C}. Then, the model update on the the class 1
at the n-th task, i.e., f 1
n ‚àíf 1
n‚àí1, becomes"
NEURAL TANGENT KERNEL REGIME,0.03717948717948718,"‚àáŒ∏f (1)
0 (x‚Ä≤)(Œ∏n ‚àíŒ∏n‚àí1) = ‚àáŒ∏f (1)
0 (x‚Ä≤)
X"
NEURAL TANGENT KERNEL REGIME,0.038461538461538464,"c=2
‚àáŒ∏f (c)
0 (Xn)‚ä§Œò(Xn)‚àí1(y(c)
n ‚àíf (c)
n‚àí1) = 0"
NEURAL TANGENT KERNEL REGIME,0.03974358974358974,"because ‚àáŒ∏f (c)
0 ‚àáŒ∏f (c‚Ä≤)‚ä§
0
= 0 (c Ã∏= c‚Ä≤) in the inÔ¨Ånite-width limit (Jacot et al., 2018; Yang, 2019).
Thus, we can deal with each head independently and analyze the generalization by setting C = 1
without loss of generality. This indicates that in the NTK regime, interaction between different
heads do not cause knowledge transfer and forgetting. One may wonder if there are any non-trivial
knowledge transfer and forgetting in such a regime. Contrary to such intuition, we reveal that when
the subsequent task revisits previous classes (targets), the generalization shows interesting increase
and decrease."
NEURAL TANGENT KERNEL REGIME,0.041025641025641026,"1We use the term ‚Äúclass‚Äù, although the regression problem is assumed in NTK theory. Usually, NTK studies
solve the classiÔ¨Åcation problem by regression with a target y(c) = {0, 1}."
NEURAL TANGENT KERNEL REGIME,0.04230769230769231,Published as a conference paper at ICLR 2022
LEARNING CURVE ON SINGLE-TASK TRAINING,0.04358974358974359,"3.2
LEARNING CURVE ON SINGLE-TASK TRAINING"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.04487179487179487,"To evaluate the generalization performance of (4), we extend the following theory to our sequential
training. Bordelon et al. (2020) obtained an analytical expression of the generalization for NTK
regression on a single task (2) as follows. Assume that training samples are generated in an i.i.d.
manner ( x¬µ ‚àºp(x)) and that labels are generated from a square integrable target function ¬Øf:"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.046153846153846156,"¬Øf(x) = ‚àû
X"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.047435897435897434,"i=0
¬Øwiœài(x),
y¬µ = ¬Øf(x¬µ) + Œµ¬µ
(¬µ = 1, ..., N),
(6)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.04871794871794872,"where ¬Øwi are constant coefÔ¨Åcients and Œµ represents Gaussian noise with ‚ü®Œµ¬µŒµŒΩ‚ü©= Œ¥¬µŒΩœÉ2. We deÔ¨Åne
œài(x) := ‚àöŒ∑iœÜi(x) with basis functions œÜi(x) given by Mercer‚Äôs decomposition:
Z
dx‚Ä≤p (x‚Ä≤) Œò (x, x‚Ä≤) œÜi (x‚Ä≤) = Œ∑iœÜi(x)
(i = 0, 1, . . . , ‚àû).
(7)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05,"Here, Œ∑i denotes NTK‚Äôs eigenvalue and we assume the Ô¨Ånite trance of NTK P"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05128205128205128,"i Œ∑i < ‚àû. We
set Œ∑0 = 0 in the main text to avoid complicated notations.
We can numerically compute
eigenvalues by using the Gauss-Gegenbauer quadrature. Generalization error is expressed by
E1 :=
DR
dxp(x)
  ¬Øf(x) ‚àíf ‚àó(x)
2E"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.052564102564102565,"D where f ‚àóis a trained model and ‚ü®¬∑ ¬∑ ¬∑ ‚ü©D is the average
over training samples. Bordelon et al. (2020) derived a typical-case evaluation of the generalization
error by using the replica method: for a sufÔ¨Åciently large N, we have asymptotically"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05384615384615385,"E1 =
1
1 ‚àíŒ≥ ‚àû
X"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05512820512820513,"i=0
Œ∑i ¬Øw2
i"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05641025641025641,"
Œ∫
Œ∫ + NŒ∑i"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.057692307692307696,"2
+
Œ≥
1 ‚àíŒ≥ œÉ2.
(8)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.05897435897435897,"Although the replica method takes a large sample size N, Bordelon et al. (2020); Canatar et al. (2021)
reported that the analytical expression (8) coincides well with empirical results even for small N.
The constants Œ∫ and Œ≥ are deÔ¨Åned as follows and characterize the increase and decrease of E1: 1 = ‚àû
X i=0"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.06025641025641026,"Œ∑i
Œ∫ + NŒ∑i
, Œ≥ = ‚àû
X i=0"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.06153846153846154,"NŒ∑2
i
(Œ∫ + NŒ∑i)2 .
(9)"
LEARNING CURVE ON SINGLE-TASK TRAINING,0.06282051282051282,"The Œ∫ is a positive solution of the Ô¨Årst equation and obtained by numerical computation. The Œ≥
satisÔ¨Åes 0 < Œ≥ < 1 by deÔ¨Ånition. For N = Œ±Dl (Œ± > 0, l ‚ààN, D ‚â´1), we can analytically solve it
and obtain more detailed evaluation. For example, a positive Œ∫ decreases to zero as the sample size
increases and E1 also decreases for œÉ2 = 0. For œÉ2 > 0, the generalization error shows multiple
descent depending on the decay of eigenvalue spectra. Since multiple descent is not a main topic of
this paper, we brieÔ¨Çy summarize it in Section A.5 of the Supplementary Materials."
LEARNING CURVES BETWEEN TWO TASKS,0.0641025641025641,"4
LEARNING CURVES BETWEEN TWO TASKS"
LEARNING CURVES BETWEEN TWO TASKS,0.06538461538461539,"In this section, we analyze the NTK formulation of continual learning (4) between two tasks (K = 2).
One can also regard this setting as transfer learning. We sequentially train the model from task A to
task B, and each one has a target function deÔ¨Åned by"
LEARNING CURVES BETWEEN TWO TASKS,0.06666666666666667,"¬ØfA(x) =
X"
LEARNING CURVES BETWEEN TWO TASKS,0.06794871794871794,"i
¬ØwA,iœài(x), ¬ØfB(x) =
X"
LEARNING CURVES BETWEEN TWO TASKS,0.06923076923076923,"i
¬ØwB,iœài(x), [ ¬ØwA,i, ¬ØwB,i] ‚àºN(0, Œ∑i"
LEARNING CURVES BETWEEN TWO TASKS,0.07051282051282051,"
1
œÅ
œÅ
1"
LEARNING CURVES BETWEEN TWO TASKS,0.07179487179487179,"
).
(10)"
LEARNING CURVES BETWEEN TWO TASKS,0.07307692307692308,"The target functions are dependent on each other and belong to the reproducing kernel Hilbert
space (RKHS). By denoting the RKHS by H, one can interpret the target similarity œÅ as the inner
product ‚ü®¬ØfA, ¬ØfB‚ü©H/(‚à•¬ØfA‚à•H‚à•¬ØfB‚à•H) = œÅ. These targets have dual representation such as ¬Øf(x) =
P
i Œ±iŒò(x‚Ä≤
i, x) with i.i.d. Gaussian variables Œ±i (Bordelon et al., 2020), as summarized in Section
E.1. We generate training samples by y¬µ
A = ¬ØfA(x¬µ
A) + Œµ¬µ
A„ÄÄ(¬µ = 1, ..., NA) and y¬µ
B = ¬ØfB(x¬µ
B) +
Œµ¬µ
B„ÄÄ(¬µ = 1, ..., NB), although we focus on the noise-less case (œÉ = 0) in this section. Input samples
x¬µ
A and x¬µ
B are i.i.d. and generated by the same distribution p(x). We can measure the generalization
error in two ways: generalization error on subsequent task B and that on previous task A:"
LEARNING CURVES BETWEEN TWO TASKS,0.07435897435897436,"EA‚ÜíB(œÅ) =
Z
dxp(x)( ¬ØfB(x) ‚àífA‚ÜíB(x))2

,
(11)"
LEARNING CURVES BETWEEN TWO TASKS,0.07564102564102564,"Eback
A‚ÜíB(œÅ) =
Z
dxp(x)( ¬ØfA(x) ‚àífA‚ÜíB(x))2

,
(12)"
LEARNING CURVES BETWEEN TWO TASKS,0.07692307692307693,Published as a conference paper at ICLR 2022
LEARNING CURVES BETWEEN TWO TASKS,0.0782051282051282,"where we write f2 as fA‚ÜíB to emphasize the sequential training from A to B. The notation Eback
A‚ÜíB(œÅ)
is referred to as backward transfer (Lopez-Paz & Ranzato, 2017). Large negative backward transfer
is known as catastrophic forgetting. We take the average ‚ü®¬∑ ¬∑ ¬∑ ‚ü©over training samples of two tasks
{DA, DB}, and target coefÔ¨Åcients ¬Øw. In fact, we can set ¬Øw as constants, and it is unnecessary to take
the average. To avoid complicated notation, we take the average in the main text. We then obtain the
following result.
Theorem 1. Using the replica method under sufÔ¨Åciently large NA and NB, for œÉ = 0, we have"
LEARNING CURVES BETWEEN TWO TASKS,0.07948717948717948,"EA‚ÜíB(œÅ) =
X i """
LEARNING CURVES BETWEEN TWO TASKS,0.08076923076923077,"2(1 ‚àíœÅ) (1 ‚àíqA,i) +
q2
A,i
1 ‚àíŒ≥A #"
LEARNING CURVES BETWEEN TWO TASKS,0.08205128205128205,"EB,i,
(13)"
LEARNING CURVES BETWEEN TWO TASKS,0.08333333333333333,"Eback
A‚ÜíB(œÅ) =
X i """
LEARNING CURVES BETWEEN TWO TASKS,0.08461538461538462,"2(1 ‚àíœÅ)(1 + FŒ≥B(qA,i, qB,i))Œ∑2
i +
q2
A,i
1 ‚àíŒ≥A
EB,i #"
LEARNING CURVES BETWEEN TWO TASKS,0.0858974358974359,",
(14)"
LEARNING CURVES BETWEEN TWO TASKS,0.08717948717948718,"where we deÔ¨Åne qA,i = Œ∫A/(Œ∫A + NAŒ∑i), qB,i = Œ∫B/(Œ∫B + NBŒ∑i), EB,i = q2
B,iŒ∑2
i /(1 ‚àíŒ≥B) and
FŒ≥B(a, b) = b(a ‚àí2) + b2(1 ‚àía)/(1 ‚àíŒ≥B)."
LEARNING CURVES BETWEEN TWO TASKS,0.08846153846153847,"The constants Œ∫A and Œ≥A (Œ∫B and Œ≥B, respectively) are given by setting N = NA(NB) in (9). The
detailed derivation is given in Section A. Technically speaking, we use the following lemma:
Lemma 2. Denote the trained model (2) on single task A as fA = P
i w‚àó
A,iœài, and deÔ¨Åne the
following cost function: E =

P"
LEARNING CURVES BETWEEN TWO TASKS,0.08974358974358974,"i œÜi(w‚àó
A,i ‚àíui)2"
LEARNING CURVES BETWEEN TWO TASKS,0.09102564102564102,"DA for arbitrary constants œÜi and ui. Using the
replica method under a sufÔ¨Åciently large NA, we have E =
X i"
LEARNING CURVES BETWEEN TWO TASKS,0.09230769230769231,"
( ¬ØwA,i ‚àíui)2 ‚àí2 ¬ØwA,i( ¬ØwA,i ‚àíui)qA,i +

¬Øw2
A,i + Œ∑iNA"
LEARNING CURVES BETWEEN TWO TASKS,0.09358974358974359,"Œ∫2
A
(EA + œÉ2)

q2
A,i 
œÜi,"
LEARNING CURVES BETWEEN TWO TASKS,0.09487179487179487,where EA denotes generalization error E1 on single task A.
LEARNING CURVES BETWEEN TWO TASKS,0.09615384615384616,"For example, we can see that EA is a special case of E with œÜi = Œ∑i and u = ¬ØwA, and that EA‚ÜíB(œÅ)
is reduced to œÜi = q2
B,iŒ∑2
i /(1 ‚àíŒ≥B) and u = ¬ØwB after certain calculation."
LEARNING CURVES BETWEEN TWO TASKS,0.09743589743589744,"Spectral Bias. The generalization errors obtained in Theorem 1 are given by the summation over
spectral modes like the single-task case. The EB,i corresponds to the i-th mode of generalization
error (8) on single task B. The study on the single task (Bordelon et al., 2020) revealed that as the
sample size increases, the modes of large eigenvalues decreases Ô¨Årst. This is known as spectral
bias and clariÔ¨Åes the inductive bias of the NTK regression. Put the eigenvalues in descending order,
i.e., Œªi ‚â•Œªi+1. When D is sufÔ¨Åciently large and NB = Œ±Dl (Œ± > 0, l ‚ààN), the error of each
mode is asymptotically given by EB,i = 0 (i < l) and Œ∑2
i (i > l). We see that the spectral bias also
holds in EA‚ÜíB because it is a weighted sum over EB,i. In contrast, Eback
A‚ÜíB includes a constant term
2(1 ‚àíœÅ)Œ∑2
i . This constant term causes catastrophic forgetting, as we show later."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.09871794871794871,"4.1
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.1,"We now look at more detailed behaviors of generalization errors obtained in Theorem 1. We Ô¨Årst
discuss the role of the target similarity œÅ for improving generalization. Figure 1(a) shows the
comparison of the generalization between single-task training and sequential training. Solid lines
show theory, and markers show experimental results of trained neural networks in the NTK regime.
We trained the model (1) with ReLU activation, L = 3, and Ml = 4, 000 by using the gradient descent
over 50 trials. More detailed settings of our experiments are summarized in Section E. Because
we set NA = NB = 100, we have EA = EB. The point is that both EA‚ÜíB(œÅ) and Eback
A‚ÜíB(œÅ) are
lower than EA(= EB) for large œÅ. This means that the sequential training degrades generalization if
the targets are dissimilar, that is, negative transfer. In particular, Eback
A‚ÜíB(œÅ) rapidly deteriorates for
the dissimilarity of targets. Note that both EA‚ÜíB and Eback
A‚ÜíB are linear functions of œÅ. Figure 1(a)
indicates that the latter has a large slope. We can gain quantitative insight into the critical value of œÅ
for the negative transfer as follows."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10128205128205128,Knowledge transfer. The following asymptotic equation gives us the critical value for EA‚ÜíB:
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10256410256410256,"EA‚ÜíB(œÅ)/EB ‚àº2(1 ‚àíœÅ)
for NA ‚â´NB.
(15)"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10384615384615385,Published as a conference paper at ICLR 2022
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10512820512820513,"œÅ
(b)
(a)"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.1064102564102564,"Figure 1: (a) Transitions from positive to negative transfer caused by target similarity œÅ. We set
NA = NB. (b) Learning curves show negative transfer (EA‚ÜíB/EB) in a highly non-linear way
depending on unbalanced sample sizes. We changed NA and set NB = 103."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.1076923076923077,"A straightforward algebra leads to this (Section A.3). In the context of transfer learning, it is
reasonable that the target domain has a limited sample size compared to the Ô¨Årst task. For œÅ > 1/2, we
have EA‚ÜíB < EB, that is, previous task A contributes to improving the generalization performance
on subsequent task B (i.e., positive transfer). For œÅ < 1/2, however, negative transfer appears."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.10897435897435898,"The following sufÔ¨Åcient condition for the negative transfer is also noteworthy. By evaluating EA‚ÜíB >
EB, we can prove that for any NA and NB, the negative transfer always appears for
œÅ < œÅ‚àó:= ‚àöŒ≥A/(1 + ‚àöŒ≥A).
(16)
This is just a sufÔ¨Åcient condition and may be loose. For example, we asymptotically have the critical
target similarity œÅ = 1/2 > œÅ‚àófor NA ‚â´NB. Nevertheless, this sufÔ¨Åcient condition is attractive in
the sense that it clariÔ¨Åes the unavoidable negative transfer for the small target similarity."
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.11025641025641025,"Backward transfer. The Eback
A‚ÜíB(0) includes the constant term P"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.11153846153846154,"i Œ∑2
i independent of sample sizes.
Note that qA and qB decrease to zero for large sample sizes (Bordelon et al., 2020), and we have
Eback
A‚ÜíB(œÅ) ‚àºP"
TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER,0.11282051282051282,"i Œ∑2
i (1 ‚àíœÅ). In contrast, EA converges to zero for a large NA. Therefore, the
intersection between Eback
A‚ÜíB(œÅ) and EA reach œÅ = 1 as NA and NB increase. This means that when
we have a sufÔ¨Åcient number of training samples, negative backward transfer (Eback
A‚ÜíB(œÅ) > EA) occurs
even for very similar targets. Figure 1(a) conÔ¨Årms this result, and Figure 6 in Section A.6 shows more
detailed learning curves of backward transfer. Catastrophic forgetting seems literally ‚Äúcatastrophic‚Äù
in the sense that the backward transfer rapidly deteriorates by the subtle target dissimilarity."
SELF-KNOWLEDGE TRANSFER,0.1141025641025641,"4.2
SELF-KNOWLEDGE TRANSFER"
SELF-KNOWLEDGE TRANSFER,0.11538461538461539,"We have shown that target similarity is a key factor for knowledge transfer. We reveal that the sample
size is another key factor. To clarify the role of sample sizes, we focus on the same target function
(œÅ = 1) in the following analysis. We refer to the knowledge transfer in this case as self-knowledge
transfer to emphasize the network learning the same function by the same head. As is the same in
œÅ < 1, the knowledge obtained in the previous task is transferred as the network‚Äôs initialization for
the subsequent training and determines the eventual performance."
SELF-KNOWLEDGE TRANSFER,0.11666666666666667,"Positive transfer by equal sample sizes. We Ô¨Ånd that positive transfer is guaranteed under equal
sample sizes, that is, NA = NB. To characterize the advantage of the sequential training, we compare
it with a model average: (fA + fB)/2, where fA (fB) means the model obtained by a single-task
training on A (B). Note that since the model is a linear function of the parameter in the NTK regime,
this average is equivalent to that of the trained parameters: (Œ∏A + Œ∏B)/2. After straightforward
calculation in Section D, the generalization error of the model average is given by
Eave = (1 ‚àíŒ≥B/2) EB.
(17)
Sequential training and model average include information of both tasks A and B; thus, it is interesting
to compare it with EA‚ÜíB. We Ô¨Ånd
Proposition 3. For œÅ = 1 and any NA = NB,
EA‚ÜíB(1) < Eave < EA = EB.
(18)"
SELF-KNOWLEDGE TRANSFER,0.11794871794871795,Published as a conference paper at ICLR 2022
SELF-KNOWLEDGE TRANSFER,0.11923076923076924,"The derivation is given in Section D. This proposition clariÔ¨Åes the superiority of sequential training
over single-task training and even the average. The Ô¨Årst task contributes to the improvement on the
second task; thus, we have positive transfer."
SELF-KNOWLEDGE TRANSFER,0.12051282051282051,"Negative transfer by unbalanced sample sizes. While equal sample size leads to positive transfer,
the following unbalanced sample sizes cause the negative transfer of self-knowledge:
EA‚ÜíB(1)/EB ‚àº1/(1 ‚àíŒ≥A)
for NB ‚â´NA.
(19)
The derivation is based on Jensen‚Äôs inequality (Section A.3). While EA‚ÜíB and EB asymptotically
decrease to zero for the large NB, their ratio remains Ô¨Ånite. Because 0 < Œ≥A < 1, EA‚ÜíB(1) > EB.
It indicates that the small sample size of task A leads to a bad initialization of subsequent training
and makes the training on task B hard to Ô¨Ånd a better solution."
SELF-KNOWLEDGE TRANSFER,0.12179487179487179,"Figure 1(b) summarizes the learning curves which depend on sample sizes in a highly non-linear
way. Solid lines show theory, and markers show the results of NTK regression over 100 trials. The
Ô¨Ågure shows excellent agreement between theory and experiments. Although we have complicated
transitions from positive to negative transfer, our theoretical analyses capture the basic characteristics
of the learning curves. For self-knowledge transfer (œÅ = 1), we can achieve positive transfer at
NA/NB = 1, as shown in Proposition 3, and for large NA/NB, as shown in (15). In contrast,
negative transfer appears for small NA/NB, as shown in (19). For œÅ < 1, large NA/NB produces
positive transfer for œÅ < 1/2, as shown in (15)."
SELF-KNOWLEDGE TRANSFER,0.12307692307692308,"If we set a relatively large œÉ > 0, the learning curve may become much more complicated due
to multiple descent. Figure 5 in Section A.5 conÔ¨Årms the case in which multiple descent appears
in EA‚ÜíB. The curve shape is generally characterized by the interaction among target similarity,
self-knowledge transfer depending on sample size, and multiple descent caused by the noise."
SELF-KNOWLEDGE FORGETTING,0.12435897435897436,"4.3
SELF-KNOWLEDGE FORGETTING"
SELF-KNOWLEDGE FORGETTING,0.12564102564102564,"Figure 2:
Self-knowledge forget-
ting: unbalanced sample sizes de-
grade generalization."
SELF-KNOWLEDGE FORGETTING,0.12692307692307692,"We have shown in Section 4.1 that backward transfer is likely
to cause catastrophic forgetting for œÅ < 1. We show that even
for œÅ = 1, sequential training causes forgetting. That is, the
training on task B degrades generalization even though both
tasks A and B learn the same target."
SELF-KNOWLEDGE FORGETTING,0.1282051282051282,"We have Eback
A‚ÜíB(1) = EA‚ÜíB(1) by deÔ¨Ånition, and Proposi-
tion 3 tells us that EA‚ÜíB < EA. Therefore, no forgetting
appears for equal sample sizes. In contrast, we have
EA‚ÜíB(1)/EA ‚àº1/(1 ‚àíŒ≥B)
for NA ‚â´NB.
(20)
One can obtain this asymptotic equation in the same manner
as (19) since EA‚ÜíB(1) is a symmetric function in terms of
indices A and B. Combining (20) with (15), we have EA <
EA‚ÜíB(1) ‚â™EB. Sequential training is better than using
only task B, but training only on the Ô¨Årst task is the best. One
can say that the model forgets the target despite learning the
same one. We call this self-knowledge forgetting. In the
context of continual learning, many studies have investigated the catastrophic forgetting caused
by different heads (i.e., in the situation of incremental learning). Our results suggest that even the
sequential training on the same task and the same head shows such forgetting. Intuitively speaking,
the self-knowledge forgetting is caused by the limited sample size of task B. Note that we have
EA‚ÜíB(1) = P"
SELF-KNOWLEDGE FORGETTING,0.1294871794871795,"i q2
A,iEB,i/(1 ‚àíŒ≥A). The generalization error of single-task training on task B
(EB,i) takes a large value for a small NB and this causes the deterioration of EA‚ÜíB as well. Figure
2 conÔ¨Årms the self-knowledge forgetting in NTK regression. We set NA = NB as the red line and
NB = 100 as the yellow line. The dashed line shows the point where forgetting appears."
LEARNING CURVES OF MANY TASKS,0.13076923076923078,"5
LEARNING CURVES OF MANY TASKS"
LEARNING CURVES OF MANY TASKS,0.13205128205128205,"We can generalize the sequential training between two tasks to that of more tasks. For simplicity, let us
focus on the self-knowledge transfer (œÅ = 1) and equal sample sizes. Applying Lemma 2 recursively
from task to task, we can evaluate generalization deÔ¨Åned by En =

R
dxp(x)( ¬Øf(x) ‚àífn(x))2 D."
LEARNING CURVES OF MANY TASKS,0.13333333333333333,Published as a conference paper at ICLR 2022
LEARNING CURVES OF MANY TASKS,0.1346153846153846,"Theorem 4. Assume that (i) (Xn, yn) (n = 1, ..., K) are given by the same distribution Xn ‚àºP(X)
and target yn = P"
LEARNING CURVES OF MANY TASKS,0.1358974358974359,"i ¬Øwiœà(Xn) + Œµn. (ii) sample sizes are equal: Nn = N. For n = 1, ..., K,"
LEARNING CURVES OF MANY TASKS,0.1371794871794872,"En+1 =
1
(1 ‚àíŒ≥)2 Àúq‚ä§Qn‚àí1Àúq + Rn+1œÉ2,
Q := diag
 
q2
+
N
(1 ‚àíŒ≥)Œ∫2 ÀúqÀúq‚ä§,
(21)"
LEARNING CURVES OF MANY TASKS,0.13846153846153847,"where qi = Œ∫/(Œ∫ + Œ∑iN), Àúqi = Œ∑iq2
i and diag(q2) denotes a diagonal matrix whose entries are q2
i .
The noise term Rn is a positive constant. In the noise-less case (œÉ = 0), the learning curve shows
monotonic decrease: En+1 ‚â§En. If all eigenvalues are positive, we have
En+1 < En
(n = 1, 2, ...).
(22)"
LEARNING CURVES OF MANY TASKS,0.13974358974358975,"See Section B for details of derivation. The learning curve (i.e., generalization error to the number
of tasks) monotonically decreases for the noise-less case. the monotonic decrease comes from
Œªmax(Q) < 1. This result means that the self-knowledge is transferred and accumulated from task to
task and contributes in improving generalization. It also ensures that no self-knowledge forgetting
appears. We can also show that Rn converges to a positive constant term for a large n and the
contribution of noise remains as a constant."
LEARNING CURVES OF MANY TASKS,0.14102564102564102,"KRR-like expression. The main purpose of this work was to address the generalization of the
continually trained model fn. As a side remark, we show another expression of fn:"
LEARNING CURVES OF MANY TASKS,0.1423076923076923,"[Œò (x‚Ä≤, X1) ¬∑ ¬∑ ¬∑ Œò (x‚Ä≤, Xn)] Ô£Æ Ô£ØÔ£ØÔ£ØÔ£∞"
LEARNING CURVES OF MANY TASKS,0.14358974358974358,"Œò (X1)
O
¬∑ ¬∑ ¬∑
O"
LEARNING CURVES OF MANY TASKS,0.14487179487179488,"Œò (X2, X1)
Œò (X2)
...
...
...
O
Œò (Xn, X1)
¬∑ ¬∑ ¬∑
Œò (Xn) Ô£π Ô£∫Ô£∫Ô£∫Ô£ª ‚àí1 Ô£Æ Ô£ØÔ£∞"
LEARNING CURVES OF MANY TASKS,0.14615384615384616,"y1
...
yn Ô£π"
LEARNING CURVES OF MANY TASKS,0.14743589743589744,"Ô£∫Ô£ª.
(23)"
LEARNING CURVES OF MANY TASKS,0.14871794871794872,"This easily comes from comparing the update (4) with a formula for the inversion of triangular block
matrices (Section C). One can see this expression as an approximation of KRR, where the upper
triangular block of NTK is set to zero. Usual modiÔ¨Åcation of KRR is diagonal, e.g., L2 regularization
and block approximation, and it seems that the generalization error of this type of model has never
been explored. Our result revealed that this model provides non-trivial sample size dependencies
such as self-knowledge transfer and forgetting."
EXPERIMENTS,0.15,"5.1
EXPERIMENTS"
EXPERIMENTS,0.15128205128205127,"Although Theorem 4 guarantees the monotonic learning curve for equal sizes, unbalanced sample
sizes should cause a non-monotonic learning curve. We empirically conÔ¨Årmed this below."
EXPERIMENTS,0.15256410256410258,"Synthetic data.
First, we empirically conÔ¨Årm En on synthetic data (10). Figure 3(a1) conÔ¨Årms
the claim of Theorem 4 that the generalization error monotonically decreases for œÉ = 0 as the
task number increases. Dashed lines are theoretical values calculated using the theorem, and points
with error bars were numerically obtained by fn (4) over 100 trials. For œÉ > 0, the decrease was
suppressed. We set œÉ2 = {0, 10‚àí5, 10‚àí4, 10‚àí3} and Ni = 100. Figure 3(a2) shows self-knowledge
forgetting. When the Ô¨Årst task has a large sample size, the generalization error by the second task can
increase for small subsequent sample sizes Ni. For smaller Ni, there was a tendency for the error to
keep increasing and taking higher errors than that of the Ô¨Årst task during several tasks. In practice,
one may face a situation where the model is initialized by the Ô¨Årst task training on many samples and
then trained in a continual learning manner under a memory constraint. The Ô¨Ågure suggests that if
the number of subsequent tasks is limited, we need only the training on the Ô¨Årst task. If we have a
sufÔ¨Åciently large number of tasks, generalization eventually improves."
EXPERIMENTS,0.15384615384615385,"MLP on MNIST / ResNet-18 on CIFAR-10. We mainly focus on the theoretical analysis in the
NTK regime, but it will be interesting to investigate whether our results also hold in more practical
settings of deep learning. We trained MLP (fully-connected neural networks with 4 hidden layers)
and ResNet-18 with stochastic gradient descent (SGD) and cross-entropy loss. We set the number of
epochs sufÔ¨Åcient for the training error to converge to zero for each task. We conÔ¨Årmed that they show
qualitatively similar results as in the NTK regime. We randomly divided the dataset into tasks without
overlap of training samples. Figures 3(b1,c1) show the monotonic decrease for an equal sample
size and that the noise suppressed the decrease. We set Ni = 500 and generated the noise by the
label corruption with a corruption probability {0, 0.2, ..., 0.8} (Zhang et al., 2017). The vertical axis
means the error, i.e., 1 ‚àí(Test accuracy [%])/100. Figures 3(b2,c2) show that unbalanced sample
sizes caused the non-monotonic learning curve, similar to NTK regression."
EXPERIMENTS,0.15512820512820513,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.1564102564102564,"(a1)
(b1)
(c1)"
EXPERIMENTS,0.1576923076923077,"(a2)
(b2)
(c2)"
EXPERIMENTS,0.15897435897435896,"NTK
MLP
ResNet-18"
EXPERIMENTS,0.16025641025641027,"Figure 3: (a1)-(c1) Learning curves for equal sample sizes. For noise-less case, they monotonically
decreased (orange lines). For noisy case, decrease was suppressed (grey lines; we plotted learning
curves for several œÉ2 and those with larger test errors correspond to larger œÉ2). We trained MLP
on MNIST and ResNet-18 on CIFAR-10. (a2)-(c2) Learning curves for unbalanced sample sizes
were non-monotonic (N1 = 4, 000 for NTK regression, N1 = 104 for SGD training of MLP and
ResNet-18). Numbers in the legend mean Nn
(n ‚â•2)."
DISCUSSION,0.16153846153846155,"6
DISCUSSION"
DISCUSSION,0.16282051282051282,"We provided novel quantitative insight into knowledge transfer and forgetting in the sequential
training of neural networks. Even in the NTK regime, where the model is simple and linearized,
learning curves show rich and non-monotonic behaviors depending on both target similarity and
sample size. In particular, learning on the same target shows successful self-knowledge transfer
or undesirable forgetting depending on the balance of sample sizes. These results indicate that the
performance of the sequentially trained model is more complicated than we thought, but we can still
Ô¨Ånd some universal laws behind it."
DISCUSSION,0.1641025641025641,"There are other research directions to be explored. While we focused on reporting novel phenomena
on transfer and forgetting, it is also important to develop algorithms to achieve better performance. To
mitigate catastrophic forgetting, previous studies proposed several strategies such as regularization,
parameter isolation, and experience replay (Mirzadeh et al., 2020). Evaluating such strategies with
theoretical backing would be helpful for further development of continual learning. For example,
orthogonal projection methods modify gradient directions (Doan et al., 2021), and replay methods
allow the reuse of the previous samples. We conjecture that these could be analyzed by extending our
calculations in a relatively straightforward way. It would also be interesting to investigate richer but
complicated situations required in practice, such as streaming of non-i.i.d. data and distribution shift
(Aljundi et al., 2019). The current work and other theories in continual learning or transfer learning
basically assume the same input distribution between different tasks (Lee et al., 2021; Tripuraneni
et al., 2020). Extending these to the case with an input distribution shift will be essential for some
applications including domain incremental learning. Our analysis may also be extended to topics
different from sequential training. For example, self-distillation uses trained model‚Äôs outputs for the
subsequent training and plays an interesting role of regularization (Mobahi et al., 2020)."
DISCUSSION,0.16538461538461538,"While our study provides universal results, which do not depend on speciÔ¨Åc eigenvalue spectra or
architectures, it is interesting to investigate individual cases. Studies on NTK eigenvalues have made
remarkable progress, covering shallow and deep ReLU neural networks (Geifman et al., 2020; Chen
& Xu, 2020), skip connections (Belfer et al., 2021), and CNNs (Favero et al., 2021). We expect that
our analysis and Ô¨Åndings will serve as a foundation for further understanding and development on
theory and experiments of sequential training."
DISCUSSION,0.16666666666666666,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.16794871794871793,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.16923076923076924,"The main contributions of this work are theoretical claims, and we clearly explained their assumptions
and settings in the main text. Complete proofs of the claims are given in the Supplementary Materials.
For experimental results of training deep neural networks, we used only already-known models and
algorithms implemented in PyTorch. All of the detailed settings, including learning procedures and
hyperparameters, are clearly explained in Section E."
REPRODUCIBILITY STATEMENT,0.17051282051282052,ACKNOWLEDGMENTS
REPRODUCIBILITY STATEMENT,0.1717948717948718,"This work was funded by JST ACT-X Grant Number JPMJAX190A and JSPS KAKENHI Grant
Number 19K20366."
REFERENCES,0.17307692307692307,REFERENCES
REFERENCES,0.17435897435897435,"Alia Abbaras, Benjamin Aubin, Florent Krzakala, and Lenka Zdeborov√°. Rademacher complexity
and spin glasses: A link between the replica and statistical theories of learning. In Mathematical
and ScientiÔ¨Åc Machine Learning (MSML), pp. 27‚Äì54. PMLR, 2020."
REFERENCES,0.17564102564102563,"Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In IEEE/CVF"
REFERENCES,0.17692307692307693,"Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11254‚Äì11263, 2019."
REFERENCES,0.1782051282051282,"Yuval Belfer, Amnon Geifman, Meirav Galun, and Ronen Basri. Spectral analysis of the neural
tangent kernel for deep residual networks. arXiv preprint arXiv:2104.03093, 2021."
REFERENCES,0.1794871794871795,"Mehdi Abbana Bennani, Thang Doan, and Masashi Sugiyama. Generalisation guarantees for continual
learning with orthogonal gradient descent. arXiv preprint arXiv:2006.11942, 2020."
REFERENCES,0.18076923076923077,"Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves
in kernel regression and wide neural networks. In International Conference on Machine Learning
(ICML), pp. 1024‚Äì1034. PMLR, 2020."
REFERENCES,0.18205128205128204,"Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and inÔ¨Ånitely wide neural networks. Nature
communications, 12(1):1‚Äì12, 2021."
REFERENCES,0.18333333333333332,Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS. In
REFERENCES,0.18461538461538463,"International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.1858974358974359,"Rainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support vector
networks. Physical review letters, 82(14):2975, 1999."
REFERENCES,0.18717948717948718,"Thang Doan, Mehdi Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and Pierre Alquier.
A theoretical analysis of catastrophic forgetting through the NTK overlap matrix. In International
Conference on ArtiÔ¨Åcial Intelligence and Statistics (AISTATS), pp. 1072‚Äì1080. PMLR, 2021."
REFERENCES,0.18846153846153846,Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint
REFERENCES,0.18974358974358974,"arXiv:1805.09733, 2018."
REFERENCES,0.191025641025641,"Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
sionality in convolutional teacher-student scenarios. arXiv preprint arXiv:2106.08619, 2021."
REFERENCES,0.19230769230769232,"Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the Laplace and neural tangent kernels. In Advances in neural information
processing systems (NeurIPS), pp. 1451‚Äì1461, 2020."
REFERENCES,0.1935897435897436,"Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual
learning in deep neural networks. Trends in cognitive sciences, 2020."
REFERENCES,0.19487179487179487,"Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571‚Äì8580, 2018."
REFERENCES,0.19615384615384615,Published as a conference paper at ICLR 2022
REFERENCES,0.19743589743589743,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521‚Äì3526, 2017."
REFERENCES,0.1987179487179487,"Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In
Advances in neural information processing systems (NeurIPS), pp. 8572‚Äì8583, 2019."
REFERENCES,0.2,"Sebastian Lee, Sebastian Goldt, and Andrew Saxe. Continual learning in the teacher-student setup:
Impact of task similarity. In International Conference on Machine Learning (ICML), pp. 6109‚Äì
6119. PMLR, 2021."
REFERENCES,0.2012820512820513,David Lopez-Paz and Marc‚ÄôAurelio Ranzato. Gradient episodic memory for continual learning. In
REFERENCES,0.20256410256410257,"Advances in Neural Information Processing Systems (NIPS), pp. 6467‚Äì6476, 2017."
REFERENCES,0.20384615384615384,"Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109‚Äì165.
Elsevier, 1989."
REFERENCES,0.20512820512820512,"Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh.
Linear mode connectivity in multitask and continual learning. In International Conference on
Learning Representations (ICLR), 2020."
REFERENCES,0.2064102564102564,"Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation ampliÔ¨Åes regularization in
Hilbert space. In Advances in Neural Information Processing Systems (NeurIPS), pp. 3351‚Äì3361,
2020."
REFERENCES,0.2076923076923077,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations (ICLR), 2020."
REFERENCES,0.20897435897435898,"Anastasia Pentina and Christoph Lampert.
A PAC-Bayesian bound for lifelong learning.
In
International Conference on Machine Learning (ICML), pp. 991‚Äì999. PMLR, 2014."
REFERENCES,0.21025641025641026,"Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hid-
den representations and task semantics. In International Conference on Learning Representations
(ICLR), 2020."
REFERENCES,0.21153846153846154,"Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher‚Äìstudent paradigm. Journal of Statistical Mechanics: Theory and
Experiment, 2020(12):124001, 2020."
REFERENCES,0.2128205128205128,"Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of
task diversity. In Advances in Neural Information Processing Systems (NeurIPS), pp. 7852‚Äì7862,
2020."
REFERENCES,0.2141025641025641,"Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019."
REFERENCES,0.2153846153846154,"Greg Yang and Etai Littwin. Tensor programs IIb: Architectural universality of neural tangent kernel
training dynamics. In International Conference on Machine Learning (ICML), pp. 11762‚Äì11772.
PMLR, 2021."
REFERENCES,0.21666666666666667,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations (ICLR), 2017."
REFERENCES,0.21794871794871795,Published as a conference paper at ICLR 2022
REFERENCES,0.21923076923076923,Supplementary Materials
REFERENCES,0.2205128205128205,"A
SEQUENTIAL TRAINING BETWEEN TWO TASKS"
REFERENCES,0.22179487179487178,"A.1
NOTATIONS"
REFERENCES,0.2230769230769231,The kernel ridge(-less) regression is given by
REFERENCES,0.22435897435897437,"f ‚àó= arg min
f‚ààH 1
2Œª N
X"
REFERENCES,0.22564102564102564,"¬µ=1
(f(x¬µ) ‚àíy¬µ)2 + 1"
REFERENCES,0.22692307692307692,"2‚ü®f, f‚ü©H
(S.1)"
REFERENCES,0.2282051282051282,"where N is the sample size and H is the reproducing kernel Hilbert space (RKHS) induced by the
neural tangent kernel Œò. We assume that the input samples x¬µ are generated from a probabilistic dis-
tribution p(x) and that the NTK has Ô¨Ånite trace, i.e.,
R
dxp(x)Œò(x, x) < ‚àû. Mercer‚Äôs decomposition
of Œò is expressed by
Z
dx‚Ä≤p (x‚Ä≤) Œò (x, x‚Ä≤) œÜi (x‚Ä≤) = Œ∑iœÜi(x)
(i = 0, 1, . . . , ‚àû).
(S.2)"
REFERENCES,0.22948717948717948,"In other words, we have Œò(x‚Ä≤, x) = P‚àû
i=0 Œ∑iœÜi(x‚Ä≤)œÜi(x). Note that a function belonging to RKHS
is given by"
REFERENCES,0.23076923076923078,"f(x) = ‚àû
X"
REFERENCES,0.23205128205128206,"i=0
aiœÜi(x)
(S.3)"
REFERENCES,0.23333333333333334,"with ‚à•f‚à•2
H = P‚àû
i=0 a2
i /Œ∑i < ‚àû. Then, using the orthonormal bases {œÜi}‚àû
i=0, the solution of the
regression is given by"
REFERENCES,0.23461538461538461,"f ‚àó(x) =
X"
REFERENCES,0.2358974358974359,"i
w‚àó
i œài(x), w‚àó=
 
Œ®Œ®‚ä§+ ŒªI
‚àí1 Œ®y
(S.4)"
REFERENCES,0.23717948717948717,"where œài(x) := ‚àöŒ∑iœÜi(x). Each column of Œ® is given by œÜ(x¬µ) (¬µ = 1, ..., N). We focus on the
NTK regime and take the ridge-less limit (Œª ‚Üí0)."
REFERENCES,0.23846153846153847,The sequentially trained model (4) between tasks A and B is written as
REFERENCES,0.23974358974358975,"fA‚ÜíB(x) ‚àífA(x) = Œò(x, XB)Œò(XB)‚àí1 (yB ‚àífA(XB)) ,
(S.5)"
REFERENCES,0.24102564102564103,"fA(x) = Œò(x, XA)Œò(XA)‚àí1yA.
(S.6)
The model fA is trained on single task A and represented by"
REFERENCES,0.2423076923076923,"fA(x) = w‚àó‚ä§
A œà(x)
(S.7)"
REFERENCES,0.24358974358974358,"with
w‚àó
A = lim
Œª‚Üí0 argminwA HA(wA),
(S.8)"
REFERENCES,0.24487179487179486,"HA(wA) := 1 2Œª NA
X ¬µ=1"
REFERENCES,0.24615384615384617," 
w‚ä§
Aœà(x¬µ
A) ‚àíy¬µ
A
2 + 1"
REFERENCES,0.24743589743589745,"2‚à•wA‚à•2
2.
(S.9)"
REFERENCES,0.24871794871794872,"Eq. (S.9 ) is the objective function equivalent to (S.1 ) because ‚ü®fA, fA‚ü©H = ‚à•wA‚à•2
2. Similarly, we
can represent fA‚ÜíB by the series expansion with the bases of RKHS. Note that the right-hand side of
(S.5 ) is equivalent to the kernel ridge-less regression on input samples XB and labels yB ‚àífA(B).
We have"
REFERENCES,0.25,"fA‚ÜíB(x) = (w‚àó
A + w‚àó
B)‚ä§œà(x)
(S.10)"
REFERENCES,0.2512820512820513,"with
w‚àó
B = lim
Œª‚Üí0 argminwB H(wB, w‚àó
A),
(S.11)"
REFERENCES,0.25256410256410255,"H(wB, w‚àó
A) := 1 2Œª NB
X ¬µ=1"
REFERENCES,0.25384615384615383," 
w‚ä§
Bœà(x¬µ
B) ‚àí(y¬µ
B ‚àíw‚àó‚ä§
A œà(x¬µ
B))
2 + 1"
REFERENCES,0.2551282051282051,"2‚à•wB‚à•2
2.
(S.12)"
REFERENCES,0.2564102564102564,Published as a conference paper at ICLR 2022
REFERENCES,0.25769230769230766,"A.2
PROOF OF THEOREM 1."
REFERENCES,0.258974358974359,"A.2.1
KNOWLEDGE TRANSFER EA‚ÜíB"
REFERENCES,0.2602564102564103,"First, we take the average over training samples of task B conditioned by task A, that is,"
REFERENCES,0.26153846153846155,"EA‚ÜíB|A :=
Z
dxp(x)( ¬ØfB(x) ‚àí(w‚àó
A + w‚àó
B)‚ä§œà(x))2"
REFERENCES,0.26282051282051283,"DB
(S.13)"
REFERENCES,0.2641025641025641,"=
Z
dxp(x)((w‚àó
B ‚àí( ¬ØwB ‚àíw‚àó
A))‚ä§œà(x))2"
REFERENCES,0.2653846153846154,"DB
(S.14)"
REFERENCES,0.26666666666666666,where the set of task B‚Äôs training samples is denoted by DB. We have
REFERENCES,0.26794871794871794,"EA‚ÜíB = ‚ü®EA‚ÜíB|A‚ü©DA.
(S.15)"
REFERENCES,0.2692307692307692,"Note that the objective function H(wB, w‚àó
A) is transformed to"
REFERENCES,0.2705128205128205,"H(wB, w‚àó
A) = 1 2Œª NB
X ¬µ=1"
REFERENCES,0.2717948717948718,"
(wB ‚àí( ¬ØwB ‚àíw‚àó
A))‚ä§œà(x(¬µ)
B )
2
+ 1"
REFERENCES,0.27307692307692305,"2‚à•wB‚à•2
2.
(S.16)"
REFERENCES,0.2743589743589744,"Comparing (S.14 ) and (S.16 ), one can see that the average over task B conditioned by task A is
equivalent to single-task training with the target function ( ¬ØwB ‚àíw‚àó
A)‚ä§œà(x). Therefore, by using (8),
we immediately obtain"
REFERENCES,0.27564102564102566,"EA‚ÜíB|A =
1
1 ‚àíŒ≥B X"
REFERENCES,0.27692307692307694,"i
Œ∑i( ¬ØwB,i ‚àíw‚àó
A,i)2

Œ∫B
Œ∫B + NBŒ∑i"
REFERENCES,0.2782051282051282,"2
+
Œ≥B
1 ‚àíŒ≥B
œÉ2
(S.17) =:
X"
REFERENCES,0.2794871794871795,"i
œÜi( ¬ØwB,i ‚àíw‚àó
A,i)2 +
Œ≥B
1 ‚àíŒ≥B
œÉ2.
(S.18)"
REFERENCES,0.28076923076923077,"Next, we take the average of (S.18 ) over task A. Only ¬Øw‚àó
A depends on the task A and is determined by
the single-task training (S.8 ). This corresponds to Lemma 2 with u = ¬ØwB and œÜi = Œ∑iq2
B,i/(1‚àíŒ≥B).
We obtain"
REFERENCES,0.28205128205128205,"EA‚ÜíB =
X i """
REFERENCES,0.2833333333333333,"( ¬ØwA,i ‚àí¬ØwB,i)2 ‚àí2 ¬ØwA,i( ¬ØwA,i ‚àí¬ØwB,i)qA,i + Ô£´"
REFERENCES,0.2846153846153846,"Ô£≠¬ØwA,i +
1
1 ‚àíŒ≥A Œ∑iNA Œ∫2
A X"
REFERENCES,0.2858974358974359,"j
Œ∑j ¬Øw2
A,jq2
A,j Ô£∂"
REFERENCES,0.28717948717948716,"Ô£∏q2
A,i"
REFERENCES,0.28846153846153844,"#
Œ∑iq2
B,i
1 ‚àíŒ≥B"
REFERENCES,0.28974358974358977,"+ œÉ2
 
1
1 ‚àíŒ≥A"
REFERENCES,0.29102564102564105,"1
1 ‚àíŒ≥B NA Œ∫2
A X"
REFERENCES,0.2923076923076923,"i
Œ∑2
i q2
A,iq2
B,i +
Œ≥B
1 ‚àíŒ≥B !"
REFERENCES,0.2935897435897436,".
(S.19)"
REFERENCES,0.2948717948717949,"Although this is a general result that holds for any ¬ØwA and ¬ØwB, it is a bit complicated and seems not
easy to give an intuitive explanation. Let us take the average over"
REFERENCES,0.29615384615384616,"[ ¬ØwA,i, ¬ØwB,i] ‚àºN(0, Œ∑i"
REFERENCES,0.29743589743589743,"
1
œÅ
œÅ
1"
REFERENCES,0.2987179487179487,"
).
(S.20)"
REFERENCES,0.3,The generalization error is then simpliÔ¨Åed to
REFERENCES,0.30128205128205127,"EA‚ÜíB(œÅ) = ‚ü®EA‚ÜíB‚ü©w
(S.21) =
X i """
REFERENCES,0.30256410256410254,"2(1 ‚àíœÅ) (1 ‚àíqA,i) +
q2
A,i
1 ‚àíŒ≥A # EB,i"
REFERENCES,0.3038461538461538,"+ œÉ2
 
1
1 ‚àíŒ≥A"
REFERENCES,0.30512820512820515,"1
1 ‚àíŒ≥B NA Œ∫2
A X"
REFERENCES,0.30641025641025643,"i
Œ∑2
i q2
A,iq2
B,i +
Œ≥B
1 ‚àíŒ≥B !"
REFERENCES,0.3076923076923077,",
(S.22)"
REFERENCES,0.308974358974359,"where we used ‚ü®w2
A,i‚ü©w = ‚ü®w2
B,i‚ü©w = Œ∑i and ‚ü®wA,i, wB,i‚ü©w = œÅ."
REFERENCES,0.31025641025641026,Published as a conference paper at ICLR 2022
REFERENCES,0.31153846153846154,"A.2.2
BACKWARD TRANSFER"
REFERENCES,0.3128205128205128,Backward transfer is measured by the prediction on the previous task A:
REFERENCES,0.3141025641025641,"Eback
A‚ÜíB :=
Z
dxp(x)( ¬ØfA(x) ‚àífA‚ÜíB(x))2

(S.23)"
REFERENCES,0.3153846153846154,"=
Z
dxp(x)((w‚àó
B ‚àí( ¬ØwA ‚àíw‚àó
A))‚ä§œà(x))2

.
(S.24)"
REFERENCES,0.31666666666666665,"First, we take the average over task B,"
REFERENCES,0.31794871794871793,"Eback
A‚ÜíB|A =
Z
dxp(x)((w‚àó‚àí( ¬ØwA ‚àíw‚àó
A))‚ä§œà(x))2"
REFERENCES,0.3192307692307692,"DB
.
(S.25)"
REFERENCES,0.32051282051282054,"This corresponds to Lemma 2 with the replacement œÜi ‚ÜêŒ∑i and u ‚Üê¬ØwA ‚àíw‚àó
A. Recall that the
objective function of NTK regression was given by (S.16). The target ¬ØwA in Lemma 2 is replaced as
¬ØwA ‚Üê¬ØwB ‚àíw‚àó
A. We then have"
REFERENCES,0.3217948717948718,"Eback
A‚ÜíB|A =
X i """
REFERENCES,0.3230769230769231,"( ¬ØwB,i ‚àí¬ØwA,i)2 ‚àí2( ¬ØwB,i ‚àíw‚àó
A,i)( ¬ØwB,i ‚àí¬ØwA,i)qB,i + Ô£±
Ô£≤"
REFERENCES,0.3243589743589744,"Ô£≥( ¬ØwB,i ‚àíw‚àó
A,i)2 +
1
1 ‚àíŒ≥B Œ∑iNB"
REFERENCES,0.32564102564102565,"Œ∫2
B
(
X"
REFERENCES,0.3269230769230769,"j=0
Œ∑j( ¬ØwB,j ‚àíw‚àó
A,j)2q2
B,j + œÉ2) Ô£º
Ô£Ω"
REFERENCES,0.3282051282051282,"Ô£æq2
B,i # Œ∑i"
REFERENCES,0.3294871794871795,(S.26)
REFERENCES,0.33076923076923076,"= Œ≥B
X"
REFERENCES,0.33205128205128204,"i
Œ∑i( ¬ØwB,i ‚àí¬ØwA,i)2 +
X i"
REFERENCES,0.3333333333333333,"Œ∑iq2
B,i
1 ‚àíŒ≥B"
REFERENCES,0.3346153846153846,"
wA,i ‚àí( ¬ØwB,i ‚àí1 ‚àíŒ≥B"
REFERENCES,0.33589743589743587,"qB,i
( ¬ØwB,i ‚àí¬ØwA,i))
2"
REFERENCES,0.3371794871794872,"+
Œ≥B
1 ‚àíŒ≥B
œÉ2.
(S.27)"
REFERENCES,0.3384615384615385,"Next, we take the average over DA. Since the Ô¨Årst and third terms of (S.27 ) are independent of
DA, we need to evaluate only the second term. The second term corresponds to Lemma 2 with
œÜi = Œ∑iq2
B,i/(1 ‚àíŒ≥B) and ui = ¬ØwB,i ‚àí(1 ‚àíŒ≥B)/qB,i( ¬ØwB,i ‚àí¬ØwA,i). We obtain"
REFERENCES,0.33974358974358976,"Eback
A‚ÜíB =
D
Eback
A‚ÜíB|A
E DA"
REFERENCES,0.34102564102564104,"= Œ≥B
X"
REFERENCES,0.3423076923076923,"i
Œ∑i( ¬ØwB,i ‚àí¬ØwA,i)2 +
X i """
REFERENCES,0.3435897435897436,"( ¬ØwA,i ‚àí¬ØwB,i)2(qB,i ‚àí(1 ‚àíŒ≥B))2"
REFERENCES,0.34487179487179487,"‚àí2 ¬ØwA,i( ¬ØwA,i ‚àí¬ØwB,i)(qB,i ‚àí(1 ‚àíŒ≥B))qA,iqB,i + Ô£±
Ô£≤"
REFERENCES,0.34615384615384615,"Ô£≥¬Øw2
A,i +
1
1 ‚àíŒ≥A Œ∑iNA"
REFERENCES,0.3474358974358974,"Œ∫2
A
(
X"
REFERENCES,0.3487179487179487,"j
Œ∑jw2
A,jq2
A,j + œÉ2) Ô£º
Ô£Ω"
REFERENCES,0.35,"Ô£æq2
B,iq2
B,i"
REFERENCES,0.35128205128205126,"#
Œ∑i
1 ‚àíŒ≥B"
REFERENCES,0.3525641025641026,"+ œÉ2
 
1
1 ‚àíŒ≥A"
REFERENCES,0.35384615384615387,"1
1 ‚àíŒ≥B NA Œ∫2
A X"
REFERENCES,0.35512820512820514,"i
Œ∑2
i q2
A,iq2
B,i +
Œ≥B
1 ‚àíŒ≥B !"
REFERENCES,0.3564102564102564,".
(S.28)"
REFERENCES,0.3576923076923077,Published as a conference paper at ICLR 2022
REFERENCES,0.358974358974359,"Finally, by taking the average over (S.20 ), we have"
REFERENCES,0.36025641025641025,"Eback
A‚ÜíB(œÅ) = ‚ü®Eback
A‚ÜíB‚ü©w =
X i"
REFERENCES,0.36153846153846153,"
2Œ≥BŒ∑2
i (1 ‚àíœÅ) + 2 (qB,i ‚àí(1 ‚àíŒ≥B))2
Œ∑2
i
1 ‚àíŒ≥B
(1 ‚àíœÅ)"
REFERENCES,0.3628205128205128,"‚àí2 Œ∑2
i (qB,i ‚àí1 + Œ≥B)qB,iqA,i"
REFERENCES,0.3641025641025641,"1 ‚àíŒ≥B
(1 ‚àíœÅ) +
1
1 ‚àíŒ≥A"
REFERENCES,0.36538461538461536,"1
1 ‚àíŒ≥B
Œ∑2
i q2
A,iq2
B,i "
REFERENCES,0.36666666666666664,"+ œÉ2
 
1
1 ‚àíŒ≥A"
REFERENCES,0.367948717948718,"1
1 ‚àíŒ≥B NA Œ∫2
A X"
REFERENCES,0.36923076923076925,"i
Œ∑2
i q2
A,iq2
B,i +
Œ≥B
1 ‚àíŒ≥B !"
REFERENCES,0.37051282051282053,"= 2(1 ‚àíœÅ)
X"
REFERENCES,0.3717948717948718,"i
Œ∑2
i """
REFERENCES,0.3730769230769231,"1 + qB,i(qA,i ‚àí2) +
q2
B,i(1 ‚àíqA,i) 1 ‚àíŒ≥B #"
REFERENCES,0.37435897435897436,"+
1
1 ‚àíŒ≥A"
REFERENCES,0.37564102564102564,"1
1 ‚àíŒ≥B √ó
X"
REFERENCES,0.3769230769230769,"i
Œ∑2
i q2
A,iq2
B,i + œÉ2
 
1
1 ‚àíŒ≥A"
REFERENCES,0.3782051282051282,"1
1 ‚àíŒ≥B NA Œ∫2
A X"
REFERENCES,0.37948717948717947,"i
Œ∑2
i q2
A,iq2
B,i +
Œ≥B
1 ‚àíŒ≥B !"
REFERENCES,0.38076923076923075,".
(S.29)"
REFERENCES,0.382051282051282,"A.2.3
LEMMA 2"
REFERENCES,0.38333333333333336,"Lemma 2. Suppose training on single task A, the target of which is given by ¬Øf = P"
REFERENCES,0.38461538461538464,"i ¬ØwA,iœài, and
denote the trained model (2) as f ‚àó= P"
REFERENCES,0.3858974358974359,"i w‚àó
A,iœài. DeÔ¨Åne the following cost function: E = *X"
REFERENCES,0.3871794871794872,"i
œÜi(w‚àó
A,i ‚àíui)2
+"
REFERENCES,0.38846153846153847,"DA
(S.30)"
REFERENCES,0.38974358974358975,"for arbitrary constants œÜi and ui. Using the replica method under a sufÔ¨Åciently large NA, we have E =
X i=0 """
REFERENCES,0.391025641025641,"( ¬ØwA,i ‚àíui)2 ‚àí2 ¬ØwA,i( ¬ØwA,i ‚àíui)qA,i + Ô£±
Ô£≤"
REFERENCES,0.3923076923076923,"Ô£≥¬Øw2
A,i +
1
1 ‚àíŒ≥A Œ∑iNA"
REFERENCES,0.3935897435897436,"Œ∫2
A
(
X"
REFERENCES,0.39487179487179486,"j=0
Œ∑j ¬Øw2
A,jq2
A,j + œÉ2) Ô£º
Ô£Ω"
REFERENCES,0.39615384615384613,"Ô£æq2
A,i #"
REFERENCES,0.3974358974358974,"œÜi.
(S.31)"
REFERENCES,0.39871794871794874,"Proof. The process of derivation is similar to Canatar et al. (2021), but we have additional constants
œÜi and ui. They cause several differences in the detailed form of equations. DeÔ¨Åne"
REFERENCES,0.4,"Z[J] =
Z
dwA exp(‚àíŒ≤HA(wA) + J Œ≤N"
REFERENCES,0.4012820512820513,"2 E(wA))
(S.32)"
REFERENCES,0.4025641025641026,"where E(wA) =

P"
REFERENCES,0.40384615384615385,"i œÜi(wA,i ‚àíui)2"
REFERENCES,0.40512820512820513,DA. We omit the index A in the following. We have
REFERENCES,0.4064102564102564,"E = lim
Œ≤‚Üí‚àû
2
Œ≤N
‚àÇ
‚àÇJ ‚ü®log Z[J]‚ü©D"
REFERENCES,0.4076923076923077,"J=0
.
(S.33)"
REFERENCES,0.40897435897435896,"To evaluate ‚ü®log Z‚ü©, we use the replica method (a.k.a. replica trick):"
REFERENCES,0.41025641025641024,"‚ü®log Z‚ü©D = lim
n‚Üí0
1
n (‚ü®Zn‚ü©D ‚àí1)
(S.34)"
REFERENCES,0.4115384615384615,"The point of the replica method is that we Ô¨Årst calculate Zn for n ‚ààN then take the limit of n to zero
by treating it as a real number. In addition, we calculate the average ‚ü®Zn‚ü©D under a replica symmetric
ans√§tz and a Gaussian approximation by following the calculation procedure of the previous works
(Dietrich et al., 1999; Bordelon et al., 2020; Canatar et al., 2021)."
REFERENCES,0.4128205128205128,Published as a conference paper at ICLR 2022
REFERENCES,0.41410256410256413,We have
REFERENCES,0.4153846153846154,"‚ü®Zn‚ü©D =
Z
dWn exp  ‚àíŒ≤ 2 n
X"
REFERENCES,0.4166666666666667,"a=1
‚à•wa‚à•2 + Œ≤JN 2 n
X"
REFERENCES,0.41794871794871796,"a=1
(wa ‚àíu)‚ä§Œ¶ (wa ‚àíu) !"
REFERENCES,0.41923076923076924,"‚ü®Q‚ü©NA
{x¬µ,Œµ¬µ} ,"
REFERENCES,0.4205128205128205,(S.35)
REFERENCES,0.4217948717948718,"Q := exp  ‚àíŒ≤ 2Œª n
X a=1"
REFERENCES,0.4230769230769231," 
(wa
A ‚àí¬ØwA)‚ä§œà(x¬µ) ‚àíŒµ¬µ2
!"
REFERENCES,0.42435897435897435,",
(S.36)"
REFERENCES,0.4256410256410256,"where we deÔ¨Åne dWn = Qn
a=1 dwa and Œ¶ is a diagonal matrix whose diagonal entries given by œÜi.
We take the shift of wa ‚Üíwa + ¬Øw. Then,"
REFERENCES,0.4269230769230769,"‚ü®Zn‚ü©D =
Z
dWn exp(‚àínŒ≤"
REFERENCES,0.4282051282051282,2 ‚à•¬Øw‚à•2 + nŒ≤JN
REFERENCES,0.42948717948717946,"2
( ¬Øw ‚àíu)‚ä§Œ¶( ¬Øw ‚àíu))
|
{z
}
=: ¬Ø
Z(J)"
REFERENCES,0.4307692307692308,"¬∑ exp(
X a
‚àíŒ≤"
REFERENCES,0.43205128205128207,2 ‚à•wa‚à•2 + Œ≤JN
REFERENCES,0.43333333333333335,"2
wa‚ä§Œ¶wa ‚àíŒ≤k‚ä§wa) ‚ü®Q‚ü©N
{x¬µ,Œµ¬µ} ,
(S.37)"
REFERENCES,0.4346153846153846,"Q = exp(‚àíŒ≤ 2Œª n
X"
REFERENCES,0.4358974358974359,"a=1
(wa‚ä§œà(x¬µ) ‚àíŒµ¬µ)2),
(S.38)"
REFERENCES,0.4371794871794872,"¬Øk := ¬Øw ‚àíJNŒ¶( ¬Øw ‚àíu).
(S.39)"
REFERENCES,0.43846153846153846,"First, let us calculate ‚ü®Q‚ü©{x¬µ,Œµ¬µ}. This term is exactly the same as appeared in the previous works
(Bordelon et al., 2020; Canatar et al., 2021). To describe notations, we overview their derivation.
DeÔ¨Åne qa = wa‚ä§œà (x) + Œµ, which are called order parameters. We approximate the probability
distribution of qa by a multivariate Gaussian:"
REFERENCES,0.43974358974358974,"P({qa}) =
1
p"
REFERENCES,0.441025641025641,"(2œÄ)n det(C)
exp Ô£´ Ô£≠‚àí1 2 X"
REFERENCES,0.4423076923076923,"a,b
(qa ‚àí¬µa) [C‚àí1]ab
 
qb ‚àí¬µb
Ô£∂"
REFERENCES,0.44358974358974357,"Ô£∏,
(S.40)"
REFERENCES,0.44487179487179485,"with
¬µa := ‚ü®qa‚ü©{x,Œµ} =

wa‚ä§œà(x)"
REFERENCES,0.4461538461538462,"x + ‚ü®Œµ‚ü©Œµ = ‚àöŒ∑0wa
0,
(S.41)"
REFERENCES,0.44743589743589746,"Cab :=

qaqb"
REFERENCES,0.44871794871794873,"{x,Œµ} = wa‚ä§‚ü®œà(x)œà(x)‚ä§‚ü©xwb + ‚ü®ŒµaŒµb‚ü©Œµ = wa‚ä§Œõwa + Œ£ab,
(S.42)"
REFERENCES,0.45,"where Œ£ab = œÉŒ¥ab, ‚ü®¬∑ ¬∑ ¬∑ ‚ü©x denotes the average over p(x) and Œõ is a diagonal matrix whose entries
are Œ∑i. We have ‚àöŒ∑0wa
0 in (S.41 ) since Œ∑0 corresponds to the constant shift œÜ0(x) = 1. Training
samples are i.i.d. and we omitted the index ¬µ. We then have"
REFERENCES,0.4512820512820513,"‚ü®Q‚ü©{x,Œµ} = exp  ‚àí1"
LOG DET,0.45256410256410257,"2 log det

I + Œ≤"
LOG DET,0.45384615384615384,"ŒªC

‚àíŒ≤"
LOG DET,0.4551282051282051,"2Œª¬µ‚ä§

I + Œ≤"
LOG DET,0.4564102564102564,"ŒªC
‚àí1
¬µ !"
LOG DET,0.4576923076923077,",
(S.43)"
LOG DET,0.45897435897435895,"where I denotes the identity matrix. DeÔ¨Åne conjugate variables {ÀÜ¬µa, ÀÜCab} by the following identity:"
LOG DET,0.46025641025641023,"1 = C
Z Ô£´ Ô£≠Y"
LOG DET,0.46153846153846156,"a‚â•b
d¬µadÀÜ¬µadCabd ÀÜCab Ô£∂ Ô£∏ √ó exp Ô£Æ Ô£∞‚àíN
X"
LOG DET,0.46282051282051284,"a
ÀÜ¬µa  
¬µa ‚àíwa
A,0
‚àöŒ∑0

‚àíN
X"
LOG DET,0.4641025641025641,"a‚â•b
ÀÜCab  
Cab ‚àíwa‚ä§Œõwb ‚àíŒ£ab
Ô£π"
LOG DET,0.4653846153846154,"Ô£ª,
(S.44)"
LOG DET,0.4666666666666667,where C denotes an uninteresting constant and we took the conjugate variables on imaginary axes.
LOG DET,0.46794871794871795,"Next, we perform the integral over Wn. Eq. (S.37 ) becomes
‚ü®Zn‚ü©D"
LOG DET,0.46923076923076923,"= C ¬ØZ(J)
Z
dWn
Y"
LOG DET,0.4705128205128205,"a‚â•b
d‚Ñ¶ab exp(‚àíN(
X"
LOG DET,0.4717948717948718,"a
ÀÜ¬µa¬µa +
X"
LOG DET,0.47307692307692306,"a‚â•b
ÀÜCab(Cab ‚àíŒ£ab))) exp[‚àíNG ‚àíGS]"
LOG DET,0.47435897435897434,(S.45)
LOG DET,0.4756410256410256,Published as a conference paper at ICLR 2022
LOG DET,0.47692307692307695,where d‚Ñ¶ab = d¬µadÀÜ¬µadCabd ÀÜCab and deÔ¨Åne G = 1
LOG DET,0.4782051282051282,"2 log det

I + Œ≤"
LOG DET,0.4794871794871795,"ŒªC

+ Œ≤"
LOG DET,0.4807692307692308,"2Œª¬µ‚ä§

I + Œ≤"
LOG DET,0.48205128205128206,"ŒªC
‚àí1
¬µ,
(S.46)"
LOG DET,0.48333333333333334,"exp(‚àíGS) = exp(
X a
(‚àíŒ≤"
LOG DET,0.4846153846153846,2 ‚à•wa‚à•2 + Œ≤JN
LOG DET,0.4858974358974359,"2
wa‚ä§Œ¶wa ‚àíŒ≤k‚ä§wa)) √ó exp Ô£Æ Ô£∞N
X"
LOG DET,0.48717948717948717,"a
ÀÜ¬µawa
A,0
‚àöŒ∑0 + N
X"
LOG DET,0.48846153846153845,"a‚â•b
ÀÜCabwa‚ä§Œõwb Ô£π"
LOG DET,0.4897435897435897,"Ô£ª.
(S.47)"
LOG DET,0.491025641025641,"We can represent
R
dWn exp(‚àíGS) by
Z
dWn exp(‚àíGS) =
Y i"
LOG DET,0.49230769230769234,"Z
dxi exp

‚àíŒ≤"
LOG DET,0.4935897435897436,"2 x‚ä§
i ÀÜQixi ‚àíŒ≤b‚ä§
i xi"
LOG DET,0.4948717948717949,"
(S.48)"
LOG DET,0.49615384615384617,"where xi ‚ààRn denotes a vector [w1
i , ..., wa
i , ..., wn
i ]‚ä§and i is the index of kernel‚Äôs eigenvalue mode
(i = 0, 1, ...). We deÔ¨Åned"
LOG DET,0.49743589743589745,ÀÜQi = (1 ‚àíœÜiJN)In ‚àíŒ∑iN
LOG DET,0.4987179487179487,"Œ≤ ( ÀúC + diag( ÀúC)),
(S.49)"
LOG DET,0.5,"bi = ¬Øki1n
(i ‚â•1),
(S.50)"
LOG DET,0.5012820512820513,b0 = ¬Øk01n ‚àíN‚àöŒ∑0
LOG DET,0.5025641025641026,"Œ≤
ÀÜ¬µ,
(S.51)"
LOG DET,0.5038461538461538,"where In is an n √ó n identity matrix and 1n is an n-dimensional vector whose all entries are 1. The
GS term includes œÜ and c that are speciÔ¨Åc to our study. When œÜ = Œ∑ and u = ¬Øw, it is reduced to
previous works (Bordelon et al., 2020; Canatar et al., 2021). Taking the integral over {xi}, we have
Z
dWn exp(‚àíGS) = C
Y"
LOG DET,0.5051282051282051,"i=0
exp(Œ≤"
LOG DET,0.5064102564102564,"2 b‚ä§
i ÀÜQ‚àí1
i bi)/
q"
LOG DET,0.5076923076923077,"det ÀÜQi.
(S.52)"
LOG DET,0.5089743589743589,"That is,"
LOG DET,0.5102564102564102,GS = 1 2 X
LOG DET,0.5115384615384615,"i=0
log det ÀÜQi ‚àíŒ≤ 2 X"
LOG DET,0.5128205128205128,"i=0
b‚ä§
i ÀÜQ‚àí1
i bi.
(S.53)"
LOG DET,0.514102564102564,REPLICA SYMMETRY AND SADDLE-POINT METHOD
LOG DET,0.5153846153846153,"Next, we carry out the integral (S.47 ) by the saddle-point method. Assume the replica symmetry:"
LOG DET,0.5166666666666667,"¬µ = ¬µa,
r = Caa,
c = CaÃ∏=b,
(S.54)"
LOG DET,0.517948717948718,"ÀÜ¬µ = ÀÜ¬µa,
ÀÜr = ÀÜCaa,
ÀÜc = ÀÜCaÃ∏=b.
(S.55)"
LOG DET,0.5192307692307693,The following three terms are the same as in the previous works:
LOG DET,0.5205128205128206,"det

I + Œ≤"
LOG DET,0.5217948717948718,"ŒªC

=

1 + Œ≤"
LOG DET,0.5230769230769231,"Œª (r ‚àíc)
n 
1 + n
Œ≤c
Œª + Œ≤ (r ‚àíc)"
LOG DET,0.5243589743589744,"
,
(S.56)"
LOG DET,0.5256410256410257,"
I + Œ≤"
LOG DET,0.5269230769230769,"ŒªC
‚àí1
=
1
1 + Œ≤"
LOG DET,0.5282051282051282,Œª (r ‚àíc)
LOG DET,0.5294871794871795,"
I ‚àí
Œ≤c
Œª + Œ≤ (r ‚àíc) + nŒ≤c11‚ä§

,
(S.57)"
LOG DET,0.5307692307692308,"ÀÜ¬µ‚ä§¬µ +
X"
LOG DET,0.532051282051282,"a‚â•b
ÀÜCab(Cab ‚àíŒ£ab) = n

ÀÜ¬µ¬µ + ÀÜr(r ‚àíœÉ2) ‚àí1"
LOG DET,0.5333333333333333,"2 ÀÜq(q ‚àíœÉ2)

,
(S.58)"
LOG DET,0.5346153846153846,"where 11‚ä§denotes a matrix whose all entries are 1. Regarding the leading term of order n (n ‚Üí0),"
LOG DET,0.5358974358974359,"log det

I + Œ≤"
LOG DET,0.5371794871794872,"ŒªC

= n log

1 + Œ≤"
LOG DET,0.5384615384615384,"Œª (r ‚àíc)

+ n
Œ≤C
Œª + Œ≤ (r ‚àíc),
(S.59)"
LOG DET,0.5397435897435897,"¬µ‚ä§

I + Œ≤"
LOG DET,0.541025641025641,"ŒªC
‚àí1
¬µ =
n¬µ2 1 + Œ≤"
LOG DET,0.5423076923076923,"Œª (r ‚àíc)
.
(S.60)"
LOG DET,0.5435897435897435,Published as a conference paper at ICLR 2022
LOG DET,0.5448717948717948,Furthermore. we have
LOG DET,0.5461538461538461,ÀÜQi = (1 ‚àíœÜiJN ‚àíŒ∑iN
LOG DET,0.5474358974358975,Œ≤ (2ÀÜr ‚àíÀÜc))In ‚àíŒ∑iNÀÜc
LOG DET,0.5487179487179488,"Œ≤
1n1‚ä§
n .
(S.61)"
LOG DET,0.55,"After straightforward algebra, the leading terms become"
LOG DET,0.5512820512820513,log det ÀÜQi = n ln gi ‚àínŒ∑iNÀÜc
LOG DET,0.5525641025641026,"giŒ≤ ,
(S.62)"
LOG DET,0.5538461538461539,"1‚ä§
n ÀÜQ‚àí1
i 1n = n"
LOG DET,0.5551282051282052,"gi
,
(S.63) with"
LOG DET,0.5564102564102564,gi := 1 ‚àíœÜiJN ‚àíŒ∑iN
LOG DET,0.5576923076923077,"Œ≤ (2ÀÜr ‚àíÀÜc).
(S.64)"
LOG DET,0.558974358974359,"Substituting these leading terms into (S.45 ), we obtain ‚ü®Zn‚ü©D = C
R
dŒ∏ exp(‚àínNS(Œ∏)) with"
LOG DET,0.5602564102564103,S(Œ∏) = ‚àíŒ≤J
LOG DET,0.5615384615384615,2 ( ¬Øw ‚àíu)‚ä§Œ¶( ¬Øw ‚àíu)
LOG DET,0.5628205128205128,"=

ÀÜ¬µ¬µ + ÀÜr(r ‚àíœÉ2) ‚àí1"
LOG DET,0.5641025641025641,"2ÀÜc(c ‚àíœÉ2)

+ 1 2"
LOG DET,0.5653846153846154,"
log Œ≤ (r ‚àíc) + c + ¬µ2 r ‚àíc  + 1"
N,0.5666666666666667,2N X
N,0.5679487179487179,"i=0
(ln gi ‚àí(Œ∑iNÀÜc"
N,0.5692307692307692,"Œ≤
+ Œ≤¬Øk2
i )/gi) ‚àí
1
2g0
(‚àí2¬Øk0ÀÜ¬µ‚àöŒ∑0 + NŒ∑0ÀÜ¬µ2/Œ≤),
(S.65)"
N,0.5705128205128205,"where Œ∏ denotes a set of variables {r, ÀÜr, c, ÀÜc, ¬µ, ÀÜ¬µ}. Here, we take N ‚â´1 and use the saddle-point
method. We calculate saddle-point equations ‚àÇS(Œ∏‚àó)/‚àÇŒ∏ = 0 and obtain"
N,0.5717948717948718,"ÀÜ¬µ = ‚àí
¬µ
r ‚àíc,
(S.66)"
N,0.573076923076923,ÀÜr = 1 2
N,0.5743589743589743, c + ¬µ2
N,0.5756410256410256,"(r ‚àíc)2 ‚àí
1
r ‚àíc"
N,0.5769230769230769,"
,
(S.67)"
N,0.5782051282051283,ÀÜc = c + ¬µ2
N,0.5794871794871795,"(r ‚àíc)2 ,
(S.68)"
N,0.5807692307692308,¬µ = NŒ∑0ÀÜ¬µ
N,0.5820512820512821,"g0Œ≤
‚àí
¬Øk0
g0"
N,0.5833333333333334,"‚àöŒ∑0,
(S.69) r =
X"
N,0.5846153846153846,"i=0
(Œ∑iNÀÜc"
N,0.5858974358974359,"Œ≤
+ Œ≤¬Øk2
i ) Œ∑i"
N,0.5871794871794872,"g2
i Œ≤ + Œ∑0N"
N,0.5884615384615385,"g2
0Œ≤ (‚àí2¬Øk0ÀÜ¬µ‚àöŒ∑0 + NŒ∑0ÀÜ¬µ2Œ≤) + œÉ2,
(S.70) c =
X"
N,0.5897435897435898,"i=0
(Œ∑iNÀÜc"
N,0.591025641025641,"Œ≤
+ Œ≤¬Øk2
i ) Œ∑i"
N,0.5923076923076923,"g2
i Œ≤ ‚àí1 Œ≤ X i=0"
N,0.5935897435897436,"Œ∑i
gi
+ Œ∑0N"
N,0.5948717948717949,"g2
0Œ≤ (‚àí2¬Øk0ÀÜ¬µ‚àöŒ∑0 + NŒ∑0ÀÜ¬µ2Œ≤) + œÉ2.
(S.71)"
N,0.5961538461538461,"From (S.67 ) and (S.68 ),"
N,0.5974358974358974,"r ‚àíc = ‚àí
1
2ÀÜr ‚àíÀÜc.
(S.72)"
N,0.5987179487179487,"From (S.70 ) and (S.71 ), we also have"
N,0.6,"Œ≤(r ‚àíc) =
X i=0"
N,0.6012820512820513,"Œ∑i
gi
=: Œ∫.
(S.73)"
N,0.6025641025641025,"Substituting (S.64 ) and (S.72 ) into (S.73 ), We obtain the implicit function for Œ∫: 1 =
X i=0"
N,0.6038461538461538,"Œ∑i
W(œÜi), W(œÜi) := Œ∫(1 ‚àíœÜiJN) + Œ∑iN.
(S.74)"
N,0.6051282051282051,Published as a conference paper at ICLR 2022
N,0.6064102564102564,"Using Œ∫ and W, the saddle-point equations become"
N,0.6076923076923076,ÀÜ¬µ = ‚àíŒ≤¬µ
N,0.6089743589743589,"Œ∫ ,
(S.75)"
N,0.6102564102564103,ÀÜr = 1 2
N,0.6115384615384616,c + ¬µ2
N,0.6128205128205129,"Œ∫2
Œ≤2 ‚àíŒ≤ Œ∫"
N,0.6141025641025641,"
,
(S.76)"
N,0.6153846153846154,ÀÜc = Œ≤2
N,0.6166666666666667,"Œ∫2 (c + ¬µ2),
(S.77)"
N,0.617948717948718,"¬µ = ‚àí
‚àöŒ∑0¬Øk0Œ∫
NŒ∑0 + W(œÜ0),
(S.78)"
N,0.6192307692307693,r = c + Œ∫
N,0.6205128205128205,"Œ≤ .
(S.79)"
N,0.6217948717948718,"Substituting back these quantities into (S.65 ), we obtain"
N,0.6230769230769231,S(Œ∏‚àó) = ‚àíŒ≤J
N,0.6243589743589744,2 ( ¬Øw ‚àíu)‚ä§Œ¶( ¬Øw ‚àíu) + 1
N,0.6256410256410256,2 ln Œ∫ + 1
N,0.6269230769230769,2N X
N,0.6282051282051282,"i=0
ln gi ‚àíŒ≤"
N,0.6294871794871795,2N X i=0
N,0.6307692307692307,"¬Øk2
i
gi"
N,0.632051282051282,"+
Œ≤Œ∑0Œ∫
2W(œÜ0)"
N,0.6333333333333333,"¬Øk2
0
NŒ∑0 + W(œÜ0) + Œ≤"
N,0.6346153846153846,"2Œ∫œÉ2 + const.
(S.80)"
N,0.6358974358974359,"Recall ‚ü®Zn‚ü©D ‚âàexp(‚àínNS(Œ∏‚àó)). We have
‚ü®log Z‚ü©D = ‚àíNS(Œ∏‚àó).
(S.81)"
N,0.6371794871794871,GENERALIZATION ERROR
N,0.6384615384615384,We evaluate
N,0.6397435897435897,"E = lim
Œ≤‚Üí‚àû
2
Œ≤N
‚àÇ
‚àÇJ ‚ü®log Z‚ü©D"
N,0.6410256410256411,"J=0
= ‚àílim
Œ≤‚Üí‚àû
2
Œ≤
‚àÇ
‚àÇJ S(Œ∏‚àó)

J=0
.
(S.82)"
N,0.6423076923076924,"Note that W, g, Œ∫ and ¬Øk depend on J. At the point of J = 0,
‚àÇW(œÜi)"
N,0.6435897435897436,"‚àÇJ
= ‚àíœÜiNŒ∫ + ‚àÇŒ∫"
N,0.6448717948717949,"‚àÇJ ,
(S.83) ‚àÇgi"
N,0.6461538461538462,‚àÇJ = ‚àí(œÜi + 1
N,0.6474358974358975,"Œ∫2
‚àÇŒ∫
‚àÇJ Œ∑i)N,
(S.84)"
N,0.6487179487179487,"‚àÇŒ∫
‚àÇJ = Œ∫2N 1 ‚àíŒ≥ X i=0"
N,0.65,"Œ∑iœÜi
(Œ∫ + Œ∑iN)2 ,
(S.85) ‚àÇ¬Øki"
N,0.6512820512820513,"‚àÇJ = ‚àíNœÜi( ¬Øwi ‚àíui).
(S.86)"
N,0.6525641025641026,"Substituting (S.83 -S.86 ) into (S.82 ), we obtain
E = X i=0 """
N,0.6538461538461539,"( ¬Øwi ‚àíui)2 ‚àí2 ¬Øwi( ¬Øwi ‚àíui)qi + Ô£±
Ô£≤"
N,0.6551282051282051,"Ô£≥¬Øw2
i +
1
1 ‚àíŒ≥
Œ∑iN"
N,0.6564102564102564,"Œ∫2 (
X"
N,0.6576923076923077,"j=0
Œ∑j ¬Øw2
jq2
j + œÉ2) Ô£º
Ô£Ω Ô£æq2
i #"
N,0.658974358974359,"œÜi + Œæ0,"
N,0.6602564102564102,"(S.87)
where Œæ0 is"
N,0.6615384615384615,"Œæ0 = Œ∑0 ¬Øw2
0"
N,0.6628205128205128,"""
Œ∫
NŒ∑0 + 2W0
W 2
0 (NŒ∑0 + W0)2 ‚àí
1
W0(NŒ∑0 + W0)"
N,0.6641025641025641,"  
N
1 ‚àíŒ≥ X"
N,0.6653846153846154,"i
Œ∑iq2
i œÜi !"
N,0.6666666666666666,"‚àíœÜ0NŒ∫2
NŒ∑0 + 2W0
W 2
0 (NŒ∑0 + W0)2 #"
N,0.6679487179487179,"+ 2Œ∑0
œÜ0Œ∫N ¬Øw0( ¬Øw0 ‚àíu0)"
N,0.6692307692307692,"W0(NŒ∑0 + W0)
(S.88)"
N,0.6705128205128205,"with W0 = Œ∫ + NŒ∑0. When Œ∑0 = 0, we have Œæ0 = 0. Note that the additional term Œæ0 is not speciÔ¨Åc
to our case but also appeared in the previous work (Canatar et al., 2021). We have Œæ0 = O(1/Œ∑0) for
a large Œ∑0 and Œæ0 is often negligibly small."
N,0.6717948717948717,Published as a conference paper at ICLR 2022
N,0.6730769230769231,"A.3
EQUATIONS FOR UNDERSTANDING NEGATIVE TRANSFER"
N,0.6743589743589744,"A.3.1
DERIVATION OF EQUATION 15"
N,0.6756410256410257,"First, let us evaluate the term including qA,i = Œ∫A/(Œ∫A + NAŒ∑i). Note that Œ∫A is a monotonically
decreasing function of NA because"
N,0.676923076923077,"‚àÇŒ∫A
‚àÇNA
= ‚àí X i"
N,0.6782051282051282,"Œ∑i
(Œ∫A + NAŒ∑i)2 !‚àí1 X i"
N,0.6794871794871795,"Œ∑2
i
(Œ∫A + NAŒ∑i)2 < 0,
(S.89)"
N,0.6807692307692308,"which comes from the implicit function theorem. Let us write Œ∫A at a Ô¨Ånite NA = c as Œ∫A(c). We
then have"
N,0.6820512820512821,"qA,i ‚â§Œ∫A(c)"
N,0.6833333333333333,"NAŒ∑i
,
(S.90)"
N,0.6846153846153846,"for NA > c. Next, we evaluate"
N,0.6858974358974359,"EA‚ÜíB(œÅ) = 2(1 ‚àíœÅ)EB ‚àí2(1 ‚àíœÅ)
X"
N,0.6871794871794872,"i
qA,iEB,i +
X i"
N,0.6884615384615385,"q2
A,i
1 ‚àíŒ≥A
EB,i.
(S.91)"
N,0.6897435897435897,"The second term is negligibly small for a sufÔ¨Åciently large NA > c because
X"
N,0.691025641025641,"i
qA,iEB,i ‚â§Œ∫A(c) NA X i"
N,0.6923076923076923,"Œ∑iq2
B,i
1 ‚àíŒ≥B
= Œ∫A(c)"
N,0.6935897435897436,"NA
Œ∫B.
(S.92)"
N,0.6948717948717948,Note that we have 1 ‚àíŒ≥ = Œ∫ P
N,0.6961538461538461,"i=0 Œ∑i/(Œ∫ + Œ∑iN)2. The third term is
X i"
N,0.6974358974358974,"q2
A,i
1 ‚àíŒ≥A
EB,i ‚â§
EA
1 ‚àíŒ≥B
=
1
1 ‚àíŒ≥B"
N,0.6987179487179487,"Œ≥A
1 ‚àíŒ≥A"
N,0.7,"Œ∫2
A
NA
,
(S.93)"
N,0.7012820512820512,"where we used qB,i ‚â§1. It decreases to zero as NA increases. Thus, we have EA‚ÜíB(œÅ) ‚àº
2(1 ‚àíœÅ)EB."
N,0.7025641025641025,"A.3.2
DERIVATION OF EQUATION 19"
N,0.7038461538461539,EA‚ÜíB(1)
N,0.7051282051282052,"EB
=
1
1 ‚àíŒ≥A P"
N,0.7064102564102565,"i q2
A,iq2
B,iŒ∑2
i
P"
N,0.7076923076923077,"i q2
B,iŒ∑2
i
(S.94)"
N,0.708974358974359,"‚â•
1
1 ‚àíŒ≥A"
N,0.7102564102564103,"Œ∫A
Œ∫A + P"
N,0.7115384615384616,"i q2
B,iŒ∑3
i NA !2"
N,0.7128205128205128,(S.95)
N,0.7141025641025641,"where the second line comes from Jensen‚Äôs inequality. Using (S.90 ), we have
X"
N,0.7153846153846154,"i
q2
B,iŒ∑3
i ‚â§Œ∫B(c)2 N 2
B X"
N,0.7166666666666667,"i
Œ∑i,
(S.96)"
N,0.717948717948718,"for NB ‚â•c. Since we assumed the Ô¨Ånite trance of NTK (i.e., P"
N,0.7192307692307692,"i Œ∑i < ‚àû), the left-hand side of
(S.96 ) is Ô¨Ånite and converges to zero for a sufÔ¨Åciently large NB. Therefore, we have
EA‚ÜíB(1)"
N,0.7205128205128205,"EB
‚â≥
1
1 ‚àíŒ≥A
.
(S.97)"
N,0.7217948717948718,"In contrast, EA‚ÜíB/EB ‚â§1/(1 ‚àíŒ≥A) since qA ‚â§1. Thus, the ratio is sandwiched by 1/(1 ‚àíŒ≥A)."
N,0.7230769230769231,"A.3.3
SUFFICIENT CONDITION FOR THE NEGATIVE TRANSFER"
N,0.7243589743589743,"Let us denote the i-th mode of EA‚ÜíB as EA‚ÜíB,i and that of EB as EB,i. From EA‚ÜíB,i > EB,i,
we have"
N,0.7256410256410256,"f(qi) := 2(1 ‚àíœÅ)(1 ‚àíqi) +
q2
i
1 ‚àíŒ≥ ‚àí1 > 0,
(S.98)"
N,0.7269230769230769,"where f(q) is a quadratic function and takes the minimum at q‚àó= (1 ‚àíœÅ)(1 ‚àíŒ≥). The condition
f(q) > 0 holds if and only if"
N,0.7282051282051282,"f(q‚àó) = ‚àí(1 ‚àíŒ≥)œÅ2 ‚àí2Œ≥œÅ + Œ≥ > 0.
(S.99)
Solving this, we Ô¨Ånd œÅ < ‚àöŒ≥/(1 + ‚àöŒ≥)."
N,0.7294871794871794,Published as a conference paper at ICLR 2022
N,0.7307692307692307,"A.4
PROOF OF PROPOSITION 3."
N,0.732051282051282,"For NA = NB, we have qA,i = qB,i = qi. Then,"
N,0.7333333333333333,"EA‚ÜíB(1) =
1
(1 ‚àíŒ≥)2
X"
N,0.7346153846153847,"i
q4
i Œ∑2
i
(S.100)"
N,0.735897435897436,and the generalization error of single-task training is given by
N,0.7371794871794872,"E =
1
1 ‚àíŒ≥ X"
N,0.7384615384615385,"i
q2
i Œ∑2
i .
(S.101)"
N,0.7397435897435898,"Model average is obtained in Section D and we have Eave = (1 ‚àíŒ≥/2)E < E because 0 < Œ≥ < 1.
Thus, we only need to evaluate the relation between Eave and EA‚ÜíB:"
N,0.7410256410256411,Eave ‚àíEA‚ÜíB = (1 ‚àíŒ≥
N,0.7423076923076923,"2 )
1
1 ‚àíŒ≥ X"
N,0.7435897435897436,"i
q2
i Œ∑2
i ‚àí
1
(1 ‚àíŒ≥)2
X"
N,0.7448717948717949,"i
Œ∑2
i q4
i
(S.102)"
N,0.7461538461538462,= (1 ‚àíŒ≥
N,0.7474358974358974,"2 )
1
1 ‚àíŒ≥ Œ∫2 X"
N,0.7487179487179487,"i
a2
i ‚àí
Œ∫2"
N,0.75,"(1 ‚àíŒ≥)2
X"
N,0.7512820512820513,"i
a2
i q2
i
(S.103) =
Œ∫2"
N,0.7525641025641026,(1 ‚àíŒ≥)2N (X
N,0.7538461538461538,"i
N 2a3
i (2 ‚àíNai) ‚àí1"
N,0.7551282051282051,2Œ≥2(3 ‚àíŒ≥) )
N,0.7564102564102564,"|
{z
}
=:F"
N,0.7576923076923077,(S.104)
N,0.7589743589743589,"where we deÔ¨Åned
ai :=
Œ∑i
Œ∫ + Œ∑iN ,
(S.105)"
N,0.7602564102564102,and used qi = 1‚àíaiN and Œ≥ = N P
N,0.7615384615384615,"i a2
i . We can provide a lower bound of F by using the following
cubic function:
G(ai) := N 2a2
i (2 ‚àíNai).
We have 0 ‚â§ai ‚â§1/N by deÔ¨Ånition. Let us consider a lower bound of G by using its tangent line at
ai = t/N (0 ‚â§t ‚â§1), that is, Nt(4 ‚àí3t)ai ‚àí2t2(1 ‚àít). DeÔ¨Åne"
N,0.7628205128205128,"H(ai) := G(ai) ‚àí(Nt(4 ‚àí3t)ai ‚àí2t2(1 ‚àít)).
We have H(0) = 2t2(1 ‚àít) ‚â•0. Since H(1/N) = ‚àí2(t ‚àí1)2(t ‚àí1/2), we need t ‚â§1/2 to
guarantee H(ai) ‚â•0 for all 0 ‚â§ai ‚â§1/N. Here, note that H is a cubic function of ai and has two
Ô¨Åxed points ai = t/N and (4 ‚àí3t)/(3N). We can see that for t ‚â§1/2, H has the local minimum at
ai = t/N and"
N,0.764102564102564,"H(t/N) = t2(2 ‚àít) ‚àí(t2(4 ‚àí3t) ‚àí2t2(1 ‚àít))
= 0.
(S.106)
Therefore, we have H(ai) ‚â•0 for all 0 ‚â§ai ‚â§1/N when the Ô¨Åxed constant t satisÔ¨Åes t ‚â§1/2.
Thus, we have the lower bound of G:
G(ai) ‚â•Nt(4 ‚àí3t)ai ‚àí2t2(1 ‚àít).
(S.107)
Using this lower bound, we have F =
X"
N,0.7653846153846153,"i
aiG(ai) ‚àí1"
N,0.7666666666666667,2Œ≥2(3 ‚àíŒ≥)
N,0.767948717948718,‚â•Œ≥t(4 ‚àí3t) ‚àí2t2(1 ‚àít) ‚àí1
N,0.7692307692307693,"2Œ≥2(3 ‚àíŒ≥),
(S.108)"
N,0.7705128205128206,where we used Œ≥ = N P
N,0.7717948717948718,"i a2
i and P"
N,0.7730769230769231,"i ai = 1. By setting t = Œ≥/2, we obtain F ‚â•Œ≥2"
N,0.7743589743589744,2 (4 ‚àí3Œ≥/2) ‚àí2Œ≥2
N,0.7756410256410257,4 (1 ‚àíŒ≥/2) ‚àí1
N,0.7769230769230769,2Œ≥2(3 ‚àíŒ≥)
N,0.7782051282051282,"= 0.
(S.109)
Therefore, we have Eave > EA‚ÜíB."
N,0.7794871794871795,"A.5
MULTIPLE DESCENT"
N,0.7807692307692308,Published as a conference paper at ICLR 2022
N,0.782051282051282,"Figure 5: Theoretical values of EA‚ÜíB(1). (Left) noise-less case (œÉ2 = 0), (Right) noisy case
(œÉ2 = 10‚àí5). We set D = 100 and other settings are the same as in Figure 1. In noise-less case,
generalization error monotonically decrease as NA or NB increases. In contrast, when noise is added,
it shows multiple descents."
N,0.7833333333333333,"Figure 4: Typical behaviors of Œ∫
and Œ≥."
N,0.7846153846153846,"We overview the multiple descent of E1 investigated in Canatar
et al. (2021). It appears for œÉ > 0. Let us set NB = Œ±Dl
(Œ± > 0, l ‚ààN, D ‚â´1), which is called the l-th learning
stage. We have EB,i = 0 (i < l), EB,l(Œ±) and Œ∑2
i (i > l).
EB,l(Œ±) is a function of Œ±, and becomes a one-peak curve
depending on the noise and the decay of kernel‚Äôs eigenvalue
spectrum. Because each learning stage has a one-peak curve,
the whole learning curve shows multiple descents as the sam-
ple size increases. Roughly speaking, the appearance of the
multiple descent is controlled by Œ≥. The Œ≥ is determined by the
kernel‚Äôs spectrum and sample size N. For example, previous
studies showed that if we assume the l-th learning stage, Œ≥
is a non-monotonic function of Œ±, the maximum of which is
Œ≥ = 1/(
p¬ØŒªl +
p¬ØŒªl + 1)2 for a constant ¬ØŒªl = P
k>l ¬ØŒ∑k/¬ØŒ∑l,
where ¬ØŒ∑ is a normalized eigenvalue of the kernel (Canatar et al., 2021). This tells us that Œ≥ repeats
the increase and decrease depending on the increase of the learning stage as is shown in Figure 4.
Roughly speaking, this non-monotonic change of Œ≥ leads to the multiple descent."
N,0.7858974358974359,"In sequential training, EB,l(Œ±) can similarly cause multiple descent as in single tasks because EA‚ÜíB
is a weighted summation over EB,i. Figure 5 is an example in which noise causes multiple decreases
and increases depending on the sample sizes. We set œÅ = 1 and obtained the theoretical values by
(S.22 ). While EA‚ÜíB(1) is a symmetric function of indices A and B for œÉ = 0, it is not for œÉ > 0.
Depending on NA and NB, the learning ‚Äúsurface‚Äù becomes much more complicated than the learning
curve of a single task."
N,0.7871794871794872,"A.6
ADDITIONAL EXPERIMENT ON BACKWARD TRANSFER"
N,0.7884615384615384,"Figure 6 shows the learning curves of backward transfer. Solid lines show theory, and markers show
the mean and interquartile range of NTK regression over 100 trials. The Ô¨Ågure shows excellent
agreement between theory and experiments. As is shown in Section 4.1, subtle target dissimilarity
(œÅ < 1) causes negative backward transfer (i.e., catastrophic forgetting). For a large NA, Eback
A‚ÜíB(œÅ <
1) approaches a non-zero constant while EA approaches zero. Thus, we have Eback
A‚ÜíB > EA even for
the target similarity close to 1. When sample sizes are unbalanced (NA ‚â´NB), the learning curve
shows negative transfer even for œÅ = 1. This is the self-knowledge forgetting revealed in (20). The
ratio Eback
A‚ÜíB(1)/EA takes a constant larger than 1, that is, 1/(1 ‚àíŒ≥B)."
N,0.7897435897435897,Published as a conference paper at ICLR 2022 œÅ
N,0.791025641025641,"Figure 6: Negative backward transfer easily appears depending on target similarity and sample sizes.
We changed NA and set NB = 102. We set D = 20 and other experimental details are the same as
in Figure 1(b)."
N,0.7923076923076923,"B
PROOF OF THEOREM 4."
N,0.7935897435897435,"B.1
LEARNING CURVE OF MANY TASKS"
N,0.7948717948717948,Eq. (4) is written as
N,0.7961538461538461,"fn(x) = n
X"
N,0.7974358974358975,"k=1
Œò(x, Xk)Œò(Xk)‚àí1(yk ‚àífk‚àí1(Xk)).
(S.110)"
N,0.7987179487179488,"Each term of this summation is equivalent to the kernel ridge-less regression on input samples Xk
and labels yk ‚àífk‚àí1(Xk). Therefore, we can represent fn by"
N,0.8,"fn(x) = n
X"
N,0.8012820512820513,"k=1
w‚àó‚ä§
k œà(x)
(S.111)"
N,0.8025641025641026,"with the minimization of the objective function H:
w‚àó
n = lim
Œª‚Üí0 argminwn H(wn, w‚àó
1:(n‚àí1)),
(S.112)"
N,0.8038461538461539,"H(wn, w‚àó
1:(n‚àí1)) = 1 2Œª Nn
X ¬µ=1 "
N,0.8051282051282052,"w‚ä§
n œà(x¬µ) ‚àí(y¬µ
n ‚àí n‚àí1
X"
N,0.8064102564102564,"k=1
w‚àó‚ä§
k œà(x¬µ)) !2 + 1"
N,0.8076923076923077,"2‚à•wn‚à•2
2
(S.113) = 1 2Œª Nn
X ¬µ=1 "
N,0.808974358974359,"(wn ‚àí( ¬Øw ‚àí n‚àí1
X"
N,0.8102564102564103,"k=1
w‚àó
k))‚ä§œà(x¬µ) ‚àíŒµ¬µ
n !2 + 1"
N,0.8115384615384615,"2‚à•wn‚à•2
2.
(S.114)"
N,0.8128205128205128,The generalization error is
N,0.8141025641025641,"En+1 :=
Z
dxp(x)( ¬Øf(x) ‚àífn+1(x))2"
N,0.8153846153846154,"D
(S.115) ="
N,0.8166666666666667,"*Z
dxp(x)((w‚àó
n+1 ‚àí( ¬Øw ‚àí n
X"
N,0.8179487179487179,"k=1
w‚àó
k))‚ä§œà(x))2
+"
N,0.8192307692307692,"D
(S.116)"
N,0.8205128205128205,"where D = {D1, ..., Dn}."
N,0.8217948717948718,"Since the data samples are independent of each other among different tasks, we can apply Lemma 2
sequentially from wn+1 to w1. First, we take the average over the (n + 1)-th task, that is,"
N,0.823076923076923,En+1|1:n =
N,0.8243589743589743,"*Z
dxp(x)((w‚àó
n+1 ‚àí( ¬Øw ‚àí n
X"
N,0.8256410256410256,"k=1
w‚àó
k))‚ä§œà(x))2
+"
N,0.8269230769230769,"Dn+1
(S.117)"
N,0.8282051282051283,Published as a conference paper at ICLR 2022
N,0.8294871794871795,"This corresponds to Lemma 2 with œÜ = Œ∑ and u = ¬ØwA = ¬Øw ‚àíPn
k=1 w‚àó
k. We have"
N,0.8307692307692308,"En+1|1:n =
X"
N,0.8320512820512821,"i=1
œÜ1,i( ¬Øwi ‚àí n
X"
N,0.8333333333333334,"k=1
w‚àó
k,i)2 + R1œÉ2
(S.118) with"
N,0.8346153846153846,"œÜ1,i := Œ∑iq2
n+1,i
1 ‚àíŒ≥n+1
, R1 :=
Œ≥n+1
1 ‚àíŒ≥n+1
.
(S.119)"
N,0.8358974358974359,"Here, Œ∫n, Œ≥n and qn,k are determined by Nn. Next, we take the average over the n-th task:"
N,0.8371794871794872,"En+1|1:(n‚àí1) =

En+1|1:n"
N,0.8384615384615385,"Dn,Œµn
(S.120) = *X"
N,0.8397435897435898,"i
œÜ1,i(w‚àó
n,i ‚àí( ¬Øwi ‚àí n‚àí1
X"
N,0.841025641025641,"k=1
w‚àó
k,i))2
+"
N,0.8423076923076923,"Dn
+ R1œÉ2
(S.121)"
N,0.8435897435897436,"This corresponds to Lemma 2 with œÜ = œÜ1 and u = ¬ØwA = ¬Øw ‚àíPn‚àí1
k=1 w‚àó
k. We obtain"
N,0.8448717948717949,"En+1|1:(n‚àí1) =
X"
N,0.8461538461538461,"i=1
œÜ2,i( ¬Øw ‚àí n‚àí1
X"
N,0.8474358974358974,"k=1
w‚àó
k)2 + R2œÉ2
(S.122) with"
N,0.8487179487179487,"œÜ2,i = œÜ1,iq2
n,i +
Nn
Œ∫2n(1 ‚àíŒ≥n)Œ∑iq2
n,i
X"
N,0.85,"j
Œ∑jœÜ1,jq2
n,j,
(S.123)"
N,0.8512820512820513,"R2 = R1 +
Nn
Œ∫2n(1 ‚àíŒ≥n) X"
N,0.8525641025641025,"j
Œ∑jœÜ1,jq2
n,j.
(S.124)"
N,0.8538461538461538,"Similarly, we can take the averages from the (n ‚àí1)-th task to the Ô¨Årst task, and obtain"
N,0.8551282051282051,"En+1 =
X"
N,0.8564102564102564,"i=1
œÜn+1,i ¬Øw2
i + Rn+1œÉ2,
(S.125)"
N,0.8576923076923076,"œÜm+1,i = œÜm,iq2
n‚àím+1,i +
Nn‚àím+1
Œ∫2
n‚àím+1(1 ‚àíŒ≥n‚àím+1)Œ∑iq2
n‚àím+1,i Ô£´ Ô£≠X"
N,0.8589743589743589,"j
Œ∑jœÜm,jq2
n‚àím+1,j Ô£∂ Ô£∏,"
N,0.8602564102564103,"(S.126)
for m = 1, ..., n, and"
N,0.8615384615384616,"Rn+1 = n
X m=1"
N,0.8628205128205129,"Nn‚àím+1
Œ∫2
n‚àím+1(1 ‚àíŒ≥n‚àím+1) X"
N,0.8641025641025641,"j
Œ∑jœÜm,jq2
n‚àím+1,j.
(S.127)"
N,0.8653846153846154,"They are general results for any Nn. By setting Nn = N for all n, we can obtain a simpler formulation.
In a vector representation, œÜn+1 is explicitly written as"
N,0.8666666666666667,"œÜn+1 =
1
1 ‚àíŒ≥"
N,0.867948717948718,"
diag(q2) +
N
(1 ‚àíŒ≥)Œ∫2 ÀúqÀúq‚ä§
n
Àúq.
(S.128)"
N,0.8692307692307693,"Finally, by taking the average over ¬Øwi ‚àºN(0, Œ∑i) which is the one-dimensional case of (S.20 ), we
have"
N,0.8705128205128205,"En+1 =
1
(1 ‚àíŒ≥)2 Àúq‚ä§

diag(q2) +
N
(1 ‚àíŒ≥)Œ∫2 ÀúqÀúq‚ä§
n‚àí1
Àúq + Rn+1œÉ2
(S.129) with"
N,0.8717948717948718,"Rn+1 =
N
Œ∫2(1 ‚àíŒ≥)2 n
X"
N,0.8730769230769231,"m=1
Àúq‚ä§

diag(q2) +
N
(1 ‚àíŒ≥)Œ∫2 ÀúqÀúq‚ä§
m‚àí1
Àúq +
Œ≥
1 ‚àíŒ≥ .
(S.130)"
N,0.8743589743589744,Published as a conference paper at ICLR 2022
N,0.8756410256410256,"B.2
MONOTONIC DECREASE OF En"
N,0.8769230769230769,"For œÉ2 = 0, we have"
N,0.8782051282051282,"En+1 =
1
(1 ‚àíŒ≥)2 Àúq‚ä§Qn‚àí1Àúq.
(S.131)"
N,0.8794871794871795,"Note that Q is a positive semi-deÔ¨Ånite matrix. If the maximum eigenvalue is no greater than 1, En
is monotonically non-increasing with n. We denote the inÔ¨Ånite norm as ‚à•A‚à•‚àû= maxi
P"
N,0.8807692307692307,"j |Aij|.
Because the inÔ¨Ånite norm bounds the eigenvalues, we have"
N,0.882051282051282,"Œª1(Q) ‚â§max
i X"
N,0.8833333333333333,"j
|Qij|
(S.132)"
N,0.8846153846153846,"= max
i"
N,0.8858974358974359,"
1 + N Œ∫ Œ∑i"
N,0.8871794871794871," 
Œ∫
Œ∫ + Œ∑iN 2"
N,0.8884615384615384,"|
{z
}
:=G(Œ∑i)"
N,0.8897435897435897,",
(S.133)"
N,0.8910256410256411,"where G(Œ∑) is monotonically decreasing for Œ∑ ‚â•0 and G(0) = 1. Therefore, the largest eigenvalue
Œª1(Q) is upper-bounded by 1, and En becomes monotonically decreasing. In particular. if Œ∑i > 0
for all i, we have Œª1(Q) < 1 and obtain En+1 < En."
N,0.8923076923076924,"For Œª1(Q) < 1, it is also easy to check that the series (S.130 ) in Rn+1 converges to a constant for
large n."
N,0.8935897435897436,"C
DERIVATION OF KRR-LIKE EXPRESSION"
N,0.8948717948717949,"KRR-like expression comes from the inverse formula of a block triangular matrix. Let us write (23)
as"
N,0.8961538461538462,"fn(x‚Ä≤) = [Œò(x‚Ä≤, 1 : (n ‚àí1)) Œò(x‚Ä≤, n)]Z‚àí1
n"
N,0.8974358974358975,"
y1:(n‚àí1)
yn"
N,0.8987179487179487,"
,
(S.134)"
N,0.9,where Zn is a block triangular matrix recursively deÔ¨Åned by
N,0.9012820512820513,"Zn =

Zn‚àí1
O
Œò(n, 1 : (n ‚àí1))
Œò(n)"
N,0.9025641025641026,"
.
(S.135)"
N,0.9038461538461539,"Then, (S.134 ) is transformed to"
N,0.9051282051282051,"[Œò(x‚Ä≤, 1 : (n ‚àí1)) Œò(x‚Ä≤, n)]

Z‚àí1
n‚àí1
O
‚àíŒò(n)‚àí1Œò(n, 1 : (n ‚àí1))Z‚àí1
n‚àí1
Œò(n)‚àí1"
N,0.9064102564102564," 
y1:(n‚àí1)
yn"
N,0.9076923076923077,"
(S.136)"
N,0.908974358974359,"= Œò(x‚Ä≤, 1 : (n ‚àí1))Z‚àí1
n‚àí1y1:(n‚àí1) + Œò(x‚Ä≤, n)Œò(n)‚àí1(yn ‚àíŒò(n, 1 : (n ‚àí1))Z‚àí1
n‚àí1y1:(n‚àí1))
(S.137)"
N,0.9102564102564102,"= fn‚àí1(x‚Ä≤) + Œò(x‚Ä≤, n)Œò(n)‚àí1(yn ‚àífn‚àí1(Xn)).
(S.138)"
N,0.9115384615384615,"Thus, one can see that the KRR-like expression is equivalent to our continually trained model."
N,0.9128205128205128,"D
DERIVATION OF MODEL AVERAGE"
N,0.9141025641025641,"For clarity, we set œÉ = 0, NA = NB, and œÅ = 1 (that is, ¬ØwA = ¬ØwB). Generalization error of a model
average is expressed by"
N,0.9153846153846154,Eave =
N,0.9166666666666666,"*Z
dxp(x)

¬ØfB(x) ‚àífA(x) + fB(x) 2 2+"
N,0.9179487179487179,"D
(S.139) ="
N,0.9192307692307692,"*Z
dxp(x)"
N,0.9205128205128205,"
¬ØwB ‚àíwA + wB 2"
N,0.9217948717948717,"‚ä§
œà(x) !2+"
N,0.9230769230769231,"D
(S.140) ="
N,0.9243589743589744,"*
¬ØwB ‚àíwA + wB 2"
N,0.9256410256410257,"‚ä§
Œõ

¬ØwB ‚àíwA + wB 2 +"
N,0.926923076923077,"D
(S.141)"
N,0.9282051282051282,Published as a conference paper at ICLR 2022
N,0.9294871794871795,"We can evaluate Eave in a similar way to EA‚ÜíB. First, take the average over DB. We deÔ¨Åne"
N,0.9307692307692308,Eave|A = 1
N,0.9320512820512821,"4

((wB ‚àí(2 ¬ØwB ‚àíwA))‚ä§Œõ(wB ‚àí(2 ¬ØwB ‚àíwA))"
N,0.9333333333333333,"DB .
(S.142)"
N,0.9346153846153846,"This average corresponds to Lemma 2 with replacement from the target task A to B, œÜ ‚ÜêŒ∑ and
u ‚Üê2 ¬ØwB ‚àíwA. Then, we have"
N,0.9358974358974359,Eave|A = 1 4 X i=0
N,0.9371794871794872,"
(wA,i ‚àí¬ØwB,i)2 ‚àí2 ¬ØwB,i(wA,i ‚àí¬ØwB,i)qB,i +

¬Øw2
B,i + Œ∑iNB"
N,0.9384615384615385,"Œ∫2
B
EB"
N,0.9397435897435897,"
q2
B,i 
Œ∑i"
N,0.941025641025641,(S.143) = 1 4 X
N,0.9423076923076923,"i=0
(wA,i ‚àí(1 + qB,i) ¬ØwB,i)2Œ∑i + Œ≥B"
N,0.9435897435897436,"4 EB.
(S.144)"
N,0.9448717948717948,"Next, we take the average over DA. The Ô¨Årst term of (S.144 ) corresponds to Lemma 2 with
replacement œÜ ‚ÜêŒ∑ and u ‚Üê(1 + qB,i) ¬ØwB. Then, we get"
N,0.9461538461538461,"Eave = ‚ü®Eave|A‚ü©DA
(S.145) = 1 4 X"
N,0.9474358974358974,"i=0
((1 ‚àíqA,i) ¬ØwA,i ‚àí(1 + qB,i) ¬ØwB,i)2Œ∑i + Œ≥A"
N,0.9487179487179487,4 EA + Œ≥B
N,0.95,"4 EB.
(S.146)"
N,0.9512820512820512,"For NA = NB, we have qA = qB, Œ≥A = Œ≥B, and EA = EB. In addition, since we focused on
¬ØwA = ¬ØwB, we have"
N,0.9525641025641025,"Eave =
X"
N,0.9538461538461539,"i=0
Œ∑i ¬Øw2
B,iq2
B,i + Œ≥B"
EB,0.9551282051282052,"2 EB
(S.147)"
EB,0.9564102564102565,"=

1 ‚àíŒ≥B 2"
EB,0.9576923076923077,"
EB
(S.148)"
EB,0.958974358974359,"E
EXPERIMENTAL SETTINGS"
EB,0.9602564102564103,"E.1
DUAL REPRESENTATION"
EB,0.9615384615384616,"In dual representation, the targets (10) are given by"
EB,0.9628205128205128,"[Œ±A,i, Œ±B,i] ‚àºN(0,

1
œÅ
œÅ
1"
EB,0.9641025641025641,"
/P ‚Ä≤),
(S.149)"
EB,0.9653846153846154,"¬ØfA(x) = P ‚Ä≤
X"
EB,0.9666666666666667,"j
Œ±A,jŒò(x‚Ä≤
j, x), ¬ØfB(x) = P ‚Ä≤
X"
EB,0.967948717948718,"j
Œ±B,jŒò(x‚Ä≤
j, x)
(S.150)"
EB,0.9692307692307692,"for a sufÔ¨Åciently large P ‚Ä≤. We sampled x‚Ä≤ from the same input distribution p(x) and set P ‚Ä≤ = 104
in all experiments. It is easy to check that targets (10) and (S.150 ) are asymptotically equiv-
alent for a large P ‚Ä≤.
Because Œò(x‚Ä≤, x) = P"
EB,0.9705128205128205,"i œài(x‚Ä≤)œài(x), we have PP ‚Ä≤"
EB,0.9717948717948718,"j Œ±A,jŒò(x‚Ä≤
j, x) =
P"
EB,0.9730769230769231,"i
PP ‚Ä≤"
EB,0.9743589743589743,"j (Œ±A,jœài(x‚Ä≤
j))œài(x) ‚àºP"
EB,0.9756410256410256,"i ¬Øwiœài(x), where ¬Øwi is sampled from N(0, Œ∑i)."
EB,0.9769230769230769,"E.2
SUMMARIES ON EXPERIMENTAL SETUP"
EB,0.9782051282051282,"Figure 1(a). We trained the deep neural network (1) with ReLU, L = 3, and Ml = 4, 000 by the
gradient descent. We set a learning rate 0.5 and trained the network during 10,000 steps over 50 trials
with different random seeds. The target is given by (S.150 ) with C = 1, D = 10, and Gaussian input
samples xi ‚àºN(0, 1). We initialized the network with (œÉ2
w, œÉ2
b) = (2, 0), set NA = NB = 100,
and calculated the generalization error over 4, 000 samples. Each marker shows the mean and
interquartile range over the trials. To calculate the theoretical curves, we need eigenvalues Œ∑i of the
NTK. We numerically compute them by the Gauss-Gegenbauer quadrature following the instruction
of Bordelon et al. (2020). Its implementation is also given by Canatar et al. (2021). Since the input"
EB,0.9794871794871794,Published as a conference paper at ICLR 2022
EB,0.9807692307692307,"samples are deÔ¨Åned on a hyper-sphere Sd‚àí1 and the NTK is a dot product kernel (i.e., Œò(x‚Ä≤, x) can
be represented by Œò(x‚Ä≤‚ä§x)), the NTK can be decomposed into Gegenbauer polynomials:"
EB,0.982051282051282,"Œò(z) = ‚àû
X"
EB,0.9833333333333333,"i=0
Œ∑iN(d, i)Qi(z),
(S.151)"
EB,0.9846153846153847,"where {Qi} are the Gegenbauer polynomials and N(d, i) are constants depending d and i. Since the
Gegenbauer polynomials are orthogonal polynomials, we have"
EB,0.985897435897436,"Œ∑i = c
Z 1"
EB,0.9871794871794872,"‚àí1
Œò(z)Qi(z)dœÑ(z),
(S.152)"
EB,0.9884615384615385,"for a certain constant c and dœÑ(z) = (1 ‚àíz2)(d‚àí3)/2dz. The Gauss-Gegenbaur quadrature approxi-
mates this integral by Œ∑i ‚âàc r
X"
EB,0.9897435897435898,"j=1
wjŒò(zj)Qi(zj),
(S.153)"
EB,0.9910256410256411,"where wj are constant weights and zj are the r roots of Qr(z). The deÔ¨Ånitions of constants (N(d, i),
c and wj) are given in Section 8 of Bordelon et al. (2020) and we set r = 1000 as is the same in this
previous work. We computed Œ∑i (i = 1, .., 1000) and substituted them to the analytical expressions."
EB,0.9923076923076923,"Figure 1(b). This Ô¨Ågure shows the results of NTK regression; (2) for the single task and (4) for
sequential training. We set NB = 4, 000, D = 50 and other settings were the same as in Figure 1(a)."
EB,0.9935897435897436,"Figure 2. This Ô¨Ågure shows the results of NTK regression; we set D = 20, and other settings are the
same as Figure 1(a)."
EB,0.9948717948717949,"Figure 3. (NTK regression) This Ô¨Ågure shows the results of NTK regression; We set D = 10 and
show the means and error bars over 100 trials. Other settings were the same as in Figure 1(a)."
EB,0.9961538461538462,"(MLP) We trained the deep neural network (1) with ReLU, L = 5, and Ml = 512 by SGD with a
learning rate 0.001, momentum 0.9, and mini-batch size 32 over 10 trials with Pytorch seeds 1-10. We
set the number of epochs to 150 and scaled the learning rate √ó1/10 at the 100-th epoch, conÔ¨Årming
that the training accuracy reached 1 for each task."
EB,0.9974358974358974,"(ResNet-18) We trained ResNet-18 by SGD with a learning rate 0.01, momentum 0.9, and mini-batch
size 128 over 10 trials with Pytorch seeds 1-10. We set the number of epochs to 150 and scaled the
learning rate √ó1/10 at the 100-th epoch, conÔ¨Årming that the training accuracy reached 1 for each
task."
EB,0.9987179487179487,"Note that the purpose of these experiments was not to achieve performance comparable to state-of-
the-art but to conÔ¨Årm self-knowledge transfer and forgetting. Therefore, we did not apply any data
augmentation or regularization method such as weight decay and dropout."
