Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,"ABSTRACT
In this paper, we develop BitRand, a bit-aware randomized response algorithm, to
preserve local differential privacy (LDP) in federated learning (FL). We encode
embedded features extracted from clients’ local data into binary encoding bits, in
which different bits have different impacts on the embedded features. Based upon
that, we randomize all the bits to preserve LDP with three key advantages: (1)
Bit-aware: Bits with a more substantial inﬂuence on the model utility have smaller
randomization probabilities, and vice-versa, under the same privacy protection;
(2) Dimension-elastic: Increasing the dimensions of embedded features, gradients,
model outcomes, and training rounds marginally affect the randomization proba-
bilities of binary encoding bits under the same privacy protection; and (3) LDP
protection is achieved for both embedded features and labels with tight privacy
loss and expected error bounds ensuring high model utility. Extensive theoretical
and experimental results show that our BitRand signiﬁcantly outperforms various
baseline approaches in text and image classiﬁcation."
INTRODUCTION,0.0014184397163120568,"1
INTRODUCTION
Recent data privacy and security regulations (GDPR, 2018; Regulation, 2018; Cybersecurity Law,
2016) pose major challenges in collecting and using personally sensitive data in different places for
machine learning (ML) applications. Federated Learning (FL) (McMahan et al., 2017; Kairouz et al.,
2019) is a promising way to address these challenges, enabling clients to jointly train ML models by
sharing and aggregating gradients computed from clients’ local data through a coordinating server
for model updates. However, recent attacks (Zhu et al., 2019; Y. et al., 2021; Zhao et al., 2020a) have
shown that clients’ training samples, each of which includes an input x and a ground-truth label yx,
can be extracted from the shared gradients. These attacks underscore the implicit privacy risk in FL."
INTRODUCTION,0.0028368794326241137,"Our main goal is to provide a strong guarantee that the shared gradients protect the privacy of clients’
local data without undue sacriﬁce in model utility. Local differential privacy (LDP) has emerged as a
crucial component in various FL applications (Yang et al., 2019; Kairouz et al., 2019). To achieve
our goal, we focus on preserving LDP in cross-device FL, i.e., in which clients jointly train an FL
model (Kairouz et al., 2019)."
INTRODUCTION,0.00425531914893617,"In cross-device FL, existing LDP-preserving approaches can be categorized into three lines: Clients
(1) add noise into local gradients derived from their local training samples, e.g., using Gaussian
mechanism (Abadi et al., 2016), to protect membership information at the training sample level with
DP guarantees (Zheng et al., 2021; Dong et al., 2019; Malekzadeh et al., 2021; Geyer et al., 2017;
Huang et al., 2020), (2) add noise to local gradients using Randomized Response (RR) mechanisms
to protect the values of the local gradients with LDP guarantees (Sun et al., 2021; Liu et al., 2020;
Zhao et al., 2020b; Wang et al., 2019a), and (3) add noise into each training sample, e.g., embedded
features and labels, using RR mechanisms to protect the value of each training sample with LDP
guarantees (Arachchige et al., 2019; Lyu et al., 2020a), then the clients use LDP-preserved training
samples to derive local gradients. For all three approaches, in each training round, clients send DP or
LDP-preserved local gradients to the coordinating server for model updates, which will be sent back
to the clients for the next training round."
INTRODUCTION,0.005673758865248227,"In this paper, we focus on protecting clients’ training data at the value level with LDP guarantees.
Existing RR mechanisms to preserve LDP in FL suffer from the curse of privacy composition, in
which excessive privacy budgets are consumed proportionally to the large dimensions of input or
embedded features (Arachchige et al., 2019), gradients (Zhao et al., 2020b; Wang et al., 2019a), and
training rounds (Zhao et al., 2020b; Wang et al., 2019a), causing loose privacy protection or inferior
model accuracy (Wagh et al., 2021)."
INTRODUCTION,0.0070921985815602835,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.00851063829787234,"Addressing the curse of privacy composition is non-trivial. Existing approaches, such as anonymizers
(assumed to be trusted), i.e., shufﬂer (Erlingsson et al., 2019; Sun et al., 2021; L. et al., 2020; Wang
et al., 2019b; Cheu et al., 2019; Balle et al., 2019) or anonymity approaches (e.g., faking source IP,
VPN, Proxy, etc. (Sun et al., 2021; Cormode et al., 2018)), and dimension reduction (Liu et al., 2020;
Zhao et al., 2020b; Shin et al., 2018; Xu et al., 2019), mitigate the problem but also have limitations.
In the real world, it is possible that the anonymizers can either be compromised or collude with the
coordinating server to extract sensitive information from observing LDP-preserved local gradients
(Erlingsson et al., 2019). Meanwhile, applying RR mechanisms on reduced sets of embedded features
or gradients using dimension reduction techniques can work well with lightweight models, such as
logistic regression and SVM (Liu et al., 2020; Zhao et al., 2020b; Wang et al., 2019a). However, it
is challenging for these techniques to achieve good model utility under tight LDP guarantees given
complex models and tasks, such as DNNs, since the dimensions of reduced embedded features,
gradients, and training rounds still need to be sufﬁciently large (Zhao et al., 2020b; Liu et al., 2020)."
INTRODUCTION,0.009929078014184398,"Hence, the curse of privacy composition in preserving LDP by applying RR mechanisms in FL
remains a largely open problem. Orthogonal to this, preserving LDP to protect ground-truth labels
yx in FL has not been well-studied. Two known approaches for centralized training are 1) injecting
Laplace noise into the labels (Phan et al., 2020) and 2) applying RR mechanisms on the labels to
achieve DP at the label level (Ghazi et al., 2021). However, centralized training in Ghazi et al. (2021)
has not been designed for FL with LDP guarantees since they require centralized and trusted databases.
The model utility in Phan et al. (2020) is notably affected by the number of model outcomes."
INTRODUCTION,0.011347517730496455,"Key Contributions. To mitigate the curse of privacy composition and optimize the trade-off between
privacy and model utility, our paper is structured around the following contributions:
1) We propose BitRand, which is a combination of a novel bit-aware f-RR mechanism and label-RR
mechanism, to preserve LDP at both levels of embedded features and labels in FL. In f-RR, we encode
embedded features (extracted from x) into a binary-bit string, which will be adaptively randomized
such that bits with a more substantial impact on model utility will have smaller randomization
probabilities and vice-versa under the same privacy budget. To preserve LDP on yx, we develop a
generalized randomization, in which the probability of randomizing label yx from one to another
class is a function of the number of model outcomes C. By doing that, we can optimize the trade-off
between model utility and privacy loss with signiﬁcantly tighter expected error bounds.
2) By incorporating sensitivities of binary encoding bits into a generalized privacy loss bound, we
show that increasing the dimensions of embedded features r, encoding bits l, and model outcomes C
marginally affect the randomization probabilities in BitRand under the same privacy budget. This
dimension-elastic property is crucial to evade the curse of privacy composition by retaining a high
value of data transmitted correctly through our randomization given large dimensions of r, l, and C.
3) These bit-aware and dimension-elastic properties allow us to work with complex models and
tasks with formal LDP guarantees for training samples (x, yx) while retaining high model utility.
Extensive theoretical analysis and experimental results conducted on fundamental FL tasks, i.e.,
text and image classiﬁcation, using benchmark datasets and our collected Security and Exchange
Commission ﬁnancial contract dataset show that our BitRand signiﬁcantly outperforms a variety of
baseline approaches in terms of model utility under the same privacy budget."
BACKGROUND,0.01276595744680851,"2
BACKGROUND
LDP-preserving mechanisms (Erlingsson et al., 2014; Duchi et al., 2018; Wang et al., 2017; Acharya
et al., 2019; Bassily & Smith, 2015) generally build on the ideas of randomized response (Warner,
1965), which was initially introduced to allow survey respondents to provide their correct inputs
while maintaining their conﬁdentiality. ϵ-LDP is presented as follows:
Deﬁnition 1. ϵ-LDP. A randomized algorithm M fulﬁlls ϵ-LDP, if for any two inputs x and x′, and
for all possible outputs O ∈Range(M), we have: Pr[M(x) = O] ≤eϵPr[M(x′) = O], where ϵ
is a privacy budget and Range(M) denotes every possible output of the algorithm M.
The privacy budget ϵ controls the amount by which the distributions induced by inputs x and x′
may differ. A smaller ϵ enforces a stronger privacy guarantee. We revisit RR mechanisms for
LDP preservation in Appendix A. Our approach is a binary encoding-based approach, similar to
(Arachchige et al., 2019; Lyu et al., 2020a), since it has the potential to overcome the curse of privacy
composition. In binary encoding, x is converted into an l-bit vector v consisting of 1 sign bit, m bits
for the integer part, and l−m−1 bits for the fraction part (Figure 1), as follows:"
BACKGROUND,0.014184397163120567,"∀i ∈[0, l −1] : vi = ⌊2i−m|x|⌋
mod 2
(1)"
BACKGROUND,0.015602836879432624,Under review as a conference paper at ICLR 2022
BACKGROUND,0.01702127659574468,Figure 1: Binary encoding.
BACKGROUND,0.018439716312056736,"Each bit in v is randomized by applying a RR mechanism,
e.g., (Erlingsson et al., 2014; Bassily & Smith, 2015; Wang
et al., 2017), to generate a perturbed l-bit vector v′, which
preserves LDP. However, in our theoretical reassessment
(Appendices I and J), directly applying RR mechanisms
on binary encoded vectors as in existing mechanisms, i.e.,
LATENT (Arachchige et al., 2019) and OME (Lyu et al., 2020a), consumes huge privacy budgets
since each binary encoding bit cannot be treated as a bit in a hash. Each binary encoding bit i has a
different sensitivity, i.e., ∆i = 2m−i for the integer and fraction parts or ∆i = 2m+1 for the sign bit
(Lemma 1), compared with a bit Bi in a hash, i.e., ∀Bi : ∆Bi = 1. Our mechanism does not suffer
from this problem, thanks to our bit-aware randomization in binary encoding (Theorem 2)."
BITRAND ALGORITHM,0.019858156028368795,"3
BITRAND ALGORITHM"
BITRAND ALGORITHM,0.02127659574468085,Figure 2: Federated Learning with BitRand Algorithm.
BITRAND ALGORITHM,0.02269503546099291,"Let us now present our FL setting, threat
model, and BitRand algorithm (Figure 2 and
Alg. 1, Appendix B), and privacy guaran-
tees. Then, we will show that our algorithm
is dimension-elastic and the ability to opti-
mize the randomization probabilities with expected error bounds in our theoretical analysis."
BITRAND ALGORITHM,0.024113475177304965,"Federated Learning Given N clients, each client u ∈[1, N] has a set of training samples Du =
{(x, yx)}nu where x ∈Rd is the input features, and its associated ground-truth label yx ∈ZC is
one-hot encoded with C categorical model outcomes {yx,1, . . . , yx,C}, and nu is the number of
training samples. In a pre-processing step, each client u extracts r numerical embedded features from
x, denoted ex, by using a pre-trained model f pre. In practice, f pre could be trained on large-scale
and publicly available datasets without introducing any extra privacy cost to the clients’ local data
(He et al., 2016; Devlin et al., 2018). N clients jointly train the model f θ : Rr →RC by minimizing
a loss function L
 
f θ(ex), yx

on their local training samples in Du that penalizes mismatching
between the prediction f θ(ex) and the ground-truth label yx, given the model’s parameters θ. In each
training round t, each client u receives the most updated model parameters θt from the coordinating
server and then computes local gradients ▽u
θt = P"
BITRAND ALGORITHM,0.02553191489361702,"(vx,yx)∈Du ▽θtL(f(ex), yx)/nu, which are sent
to the coordinating server for aggregating and model updating: θt+1 = θt −ηt
P"
BITRAND ALGORITHM,0.02695035460992908,"u∈[1,N] ▽u
θt/N."
BITRAND ALGORITHM,0.028368794326241134,"Threat Model. The coordinating server strictly follows the training procedure but curious about
the training data Du. This is a practical threat model in the real world since service providers
always aim at providing the best services to the clients (Haeberlen et al., 2011; Truex et al., 2019;
Lyu et al., 2020b). Given the observed gradients ▽u
θt, the coordinating server can extract the
clients’ data {ex, yx}nu by using recently developed attacks (Carlini et al., 2020; Fredrikson et al.,
2015). In a defense-free environment, {ex, yx}nu can be used to infer the sensitive training data
Du = {(x, yx)}nu using the pre-trained model f pre, since f pre(x) = ex (Song & Raghunathan,
2020). This poses a severe privacy risk to the sensitive data Du."
BITRAND ALGORITHM,0.029787234042553193,"BitRand Algorithm. To protect the sensitive training data Du against the threat model, in our
algorithm, we preserve LDP on both embedded features ex and labels yx."
BITRAND ALGORITHM,0.031205673758865248,"Each of the r embedded features in ex is encoded into l binary bits following Eq. 1. Binary encoded
features are concatenated together into a vector vx consisting of rl binary bits to represent the
embedded features ex (Alg. 1, line 16). Each bit i ∈[0, rl −1] in vx is randomized by our f-RR
mechanism (Alg. 1, line 17) with a bit-aware term i%l"
BITRAND ALGORITHM,0.032624113475177303,"l
optimizing randomization probabilities:"
BITRAND ALGORITHM,0.03404255319148936,"(f-RR mechanism) ∀i ∈[0, rl −1] : P(v′
x(i) = 1) ="
BITRAND ALGORITHM,0.03546099290780142,"



"
BITRAND ALGORITHM,0.03687943262411347,"


"
BITRAND ALGORITHM,0.03829787234042553,"pX =
1
1 + α exp( i%l"
BITRAND ALGORITHM,0.03971631205673759,"l ϵX)
, if vx(i) = 1"
BITRAND ALGORITHM,0.04113475177304964,"qX =
α exp( i%l l ϵX)"
BITRAND ALGORITHM,0.0425531914893617,1 + α exp( i%l
BITRAND ALGORITHM,0.04397163120567376,"l ϵX)
, if vx(i) = 0
(2)"
BITRAND ALGORITHM,0.04539007092198582,"where vx(i) ∈{0, 1} is the value of vx at the bit i, v′
x is the perturbed vector created by randomizing
all the bits in vx, ϵX is a privacy budget, and α is a parameter bounded in Theorem 2. From Eq. 2,
we also have that P(v′
x(i) = 0) = 1 −pX if vx(i) = 1, and P(v′
x(i) = 0) = 1 −qX if vx(i) = 0.
We use the bit-aware term i%l"
BITRAND ALGORITHM,0.04680851063829787,"l
to indicate the location of bit i, which is associated with the sensitivity
of the bit at that location, in its l-bit binary encoded vector among rl concatenated binary bits."
BITRAND ALGORITHM,0.04822695035460993,Under review as a conference paper at ICLR 2022
BITRAND ALGORITHM,0.04964539007092199,"One of the key differences between our mechanism and existing works (Sun et al., 2021; Zhao
et al., 2020b; Wang et al., 2019a; Liu et al., 2020; Arachchige et al., 2019; Lyu et al., 2020a) is the
bit-aware randomization probabilities. By introducing the bit-aware term i%l"
BITRAND ALGORITHM,0.05106382978723404,"l , we are able to: 1)
Derive signiﬁcantly tighter privacy loss and expected error bounds compared with existing approaches
(Sections 4 and 5); and 2) Adaptively control the randomization probabilities across bits, such that
bits with a stronger inﬂuence on the model utility, e.g., sign bits and integer bits, have smaller
randomization probabilities qX, and vice-versa (Section 5). These advantages are crucial to evade
the curse of privacy composition enabling us to work with complex tasks and models (Section 6)."
BITRAND ALGORITHM,0.0524822695035461,"In addition, inspired by the LabelDP (Ghazi et al., 2021), we randomize yx using the following
label-RR mechanism (Alg. 1, line 18):"
BITRAND ALGORITHM,0.05390070921985816,"(label-RR mechanism) P(y′
x = ¯yx) ="
BITRAND ALGORITHM,0.05531914893617021,"


 

"
BITRAND ALGORITHM,0.05673758865248227,"pY =
exp(β)
1 + exp(β),
if ¯yx = yx"
BITRAND ALGORITHM,0.05815602836879433,"qY =
1
(1 + exp(β))(C −1), if ¯yx ̸= yx, ¯yx ∈ZC
(3)"
BITRAND ALGORITHM,0.059574468085106386,"where ¯yx is one-hot encoded, and β is a parameter bounded in Theorem 3 under a privacy budget ϵY ."
BITRAND ALGORITHM,0.06099290780141844,"Randomizing the label yx provides a complete LDP protection to each local training sample (ex, yx).
All the perturbed training samples (e′
x, y′
x) are included in a local dataset D′
u, which will be used to
train the model f θ : Rr →RC, i.e., e′
x = E(v′
x) where E(·) is a decoding function, (Alg. 1, lines
20-22). The training will never access the original data Du. All other operations remain the same
with our aforementioned FL setting."
PRIVACY GUARANTEES OF BITRAND,0.062411347517730496,"4
PRIVACY GUARANTEES OF BITRAND
In this section, we focus on bounding formal privacy loss of BitRand for input and label protection.
To achieve our goal, we need to bound α and β in Eqs. 2 and 3 such that our algorithm preserves LDP
for each training sample (x, yx) ∈Du given (v′
x, y′
x) ∈D′
u. Note that the decoding e′
x = E(v′
x) does
not incur any extra privacy risk following the post-processing property (Dwork & Roth, 2014). Given
vx and evx can be different at any bit, for the LDP condition to hold, the ratio of two probabilities
P (f-RR(vx)=vz)
P (f-RR(evx)=vz) needs to be bounded by exp(ϵX) with vz ∈Range(f-RR)."
PRIVACY GUARANTEES OF BITRAND,0.06382978723404255,"Given a feature a, i.e., one of the r features, let us ﬁrst consider a bit i in the encoding vector va.
Intuitively, when we apply our f-RR only on the bit i in va (i.e., all the other bits remain the same),
denoted as f-RR(va, i), the l1-sensitivity of the bit i is 1. This is because, given two neighboring
vectors va and va|i that differs only at the bit i, we have that ∀i ∈va : arg maxva∥f-RR(va, i) −
f-RR(va|i, i)∥1 = 1. However, the coordinating server can infer the decoded features E(f-RR(va))
instead of just the intermediate result f-RR(va). Thus, we need to quantify the l1-sensitivity of
a single encoding bit i ∈va to determine just how accurately we can return the decoded features
E(f-RR(va)) through our randomization f-RR applied on va. Following (Dwork & Roth, 2014), the
sensitivity ∆i of a bit i can be quatiﬁed as follows:
Deﬁnition 2. Bit l1-sensitivity. Given two neighboring vectors va and va|i that differs only at a bit
i, the sensitivity ∆i captures the magnitude by which the bit i can change the decoding function
E(f-RR(·)) in the worst case, as follows:"
PRIVACY GUARANTEES OF BITRAND,0.06524822695035461,"∀i ∈[0, rl −1] : ∆i = max
va ∥E
 
f-RR(va)

−E
 
f-RR(va|i)

∥1
(4)"
PRIVACY GUARANTEES OF BITRAND,0.06666666666666667,"Based on Eq. 4, sensitivities ∆i of all the bits in vx are bounded in the following lemma.
Lemma 1. The l1-sensitivity of a single binary encoding bit is bounded as follows:"
PRIVACY GUARANTEES OF BITRAND,0.06808510638297872,"∀i ∈[0, rl −1] : ∆i ="
PRIVACY GUARANTEES OF BITRAND,0.06950354609929078,"(
2m+1,
if i is a sign bit"
PRIVACY GUARANTEES OF BITRAND,0.07092198581560284,"2m−i%l,
if i is an integer/fraction bit
(5)"
PRIVACY GUARANTEES OF BITRAND,0.07234042553191489,"All the proofs are in Appendix. Unlike existing RR mechanisms, we incorporate the l1-sensitivity
∆i into the privacy loss bound of f-RR by ensuring that the privacy loss in randomizing a bit i is
bounded by the privacy loss in the embedded feature space. By doing so, we can derive tighter privacy
loss bounds as discussed next."
PRIVACY GUARANTEES OF BITRAND,0.07375886524822695,"Given ∆i, there always exists a Laplace noise injected into the feature a, i.e., M(va, i) = E(va) +
Lap(∆i/ϵi), to achieve ϵi-LDP in the embedded feature space (Dwork & Roth, 2014). In other
words,
P (M(va,i)=z)
P (M(va|i,i)=z) ≤exp(
ϵi|E(va)−E(va|i)|"
PRIVACY GUARANTEES OF BITRAND,0.075177304964539,"∆i
) and z ∈Range(M). To ensure that the privacy loss"
PRIVACY GUARANTEES OF BITRAND,0.07659574468085106,Under review as a conference paper at ICLR 2022
PRIVACY GUARANTEES OF BITRAND,0.07801418439716312,"in randomizing the bit i is bounded by the privacy loss in the embedded feature space is to ﬁnd α in Eq.
2 such that:
P (f-RR(va,i)=vz)
P (f-RR(va|i,i)=vz) ≤exp(
ϵi|E(va)−E(va|i)|"
PRIVACY GUARANTEES OF BITRAND,0.07943262411347518,"∆i
). However, randomizing a binary encoding
bit i given va and va|i results in a smaller l1-distance |E(f-RR(va, i)) −E(f-RR(va|i, i))| compared
with |E(va) −E(va|i)|; since |E(f-RR(va, i)) −E(f-RR(va|i, i))| ≤|E(va) −E(va|i)|. Thus, we
can derive a tighter bound by replacing |E(va) −E(va|i)| with |E(f-RR(va, i)) −E(f-RR(va|i, i))|.
Given va(i) is the bit i in va, we have"
PRIVACY GUARANTEES OF BITRAND,0.08085106382978724,"P(f-RR(va, i) = vz)
P(f-RR(va|i, i) = vz) = P(f-RR(va(i)) = vz(i))"
PRIVACY GUARANTEES OF BITRAND,0.08226950354609928,"P(f-RR(va|i(i)) = vz(i)) ×
Y"
PRIVACY GUARANTEES OF BITRAND,0.08368794326241134,"j̸=i,j∈[0,l−1]"
PRIVACY GUARANTEES OF BITRAND,0.0851063829787234,"P(va(j) = vz(j))
P(va|i(j) = vz(j))"
PRIVACY GUARANTEES OF BITRAND,0.08652482269503546,= P(f-RR(va(i)) = vz(i))
PRIVACY GUARANTEES OF BITRAND,0.08794326241134752,"P(f-RR(va|i(i)) = vz(i)) ≤exp(ϵi|E(f-RR(va, i)) −E(f-RR(va|i, i))|"
PRIVACY GUARANTEES OF BITRAND,0.08936170212765958,"∆i
)
(6)"
PRIVACY GUARANTEES OF BITRAND,0.09078014184397164,"However, ﬁnding a closed-form solution of α for the tight privacy bound in Eq. 6 is non-trivial, since
ϵi is intractable. To address this, we consider two cases: (1)
P (f-RR(va(i))=vz(i))
P (f-RR(va|i(i))=vz(i)) ≥1 below, and (2)"
PRIVACY GUARANTEES OF BITRAND,0.09219858156028368,"0 <
P (f-RR(va(i))=vz(i))
P (f-RR(va|i(i))=vz(i)) < 1 in Appendix D. Since ∆i captures the magnitude by which the bit i"
PRIVACY GUARANTEES OF BITRAND,0.09361702127659574,"can change the decoding function E(·) in the worst case, we have
∆i
|E(f-RR(va,i))−E(f-RR(va|i,i))| ≥1.
As a result, we have"
PRIVACY GUARANTEES OF BITRAND,0.0950354609929078,"P(f-RR(va(i)) = vz(i))
P(f-RR(va|i(i)) = vz(i)) ≤
 P(f-RR(va(i)) = vz(i))"
PRIVACY GUARANTEES OF BITRAND,0.09645390070921986,P(f-RR(va|i(i)) = vz(i))
PRIVACY GUARANTEES OF BITRAND,0.09787234042553192,"
∆i
|E(f-RR(va,i))−E(f-RR(va|i,i))| ≤exp(ϵi)
(7)"
PRIVACY GUARANTEES OF BITRAND,0.09929078014184398,"Eq. 7 enables us to quantify a generalized privacy loss bound of a RR mechanism, in which different
bits have different sensitivities by randomizing all the bits in vx independently in Theorem 1.
Theorem 1. Generalized privacy loss bound. The privacy loss for randomizing a binary encoding
vector vx is bounded as follows:"
PRIVACY GUARANTEES OF BITRAND,0.10070921985815603,"P(f-RR(vx) = vz)
P(f-RR(evx) = vz) ≤"
PRIVACY GUARANTEES OF BITRAND,0.10212765957446808,"rl−1
Y i=0"
PRIVACY GUARANTEES OF BITRAND,0.10354609929078014, P(f-RR(vx(i)) = vz(i))
PRIVACY GUARANTEES OF BITRAND,0.1049645390070922,P(f-RR(vx|i(i)) = vz(i))
PRIVACY GUARANTEES OF BITRAND,0.10638297872340426,"
∆i
|E(f-RR(vx,i))−E(f-RR(vx|i,i))| ≤exp("
PRIVACY GUARANTEES OF BITRAND,0.10780141843971631,"rl−1
X"
PRIVACY GUARANTEES OF BITRAND,0.10921985815602837,"i=0
ϵi)
(8)"
PRIVACY GUARANTEES OF BITRAND,0.11063829787234042,"Now, we enforce the condition Prl−1
i=0 ϵi = ϵX to ensure that the total privacy budget will be bounded
by ϵX. Based upon that, we derive a closed-form solution showing that there always exists an upper
bound of α so that v′
x preserves ϵX-LDP given vx in Theorem 2 (Alg. 1, line 17)."
PRIVACY GUARANTEES OF BITRAND,0.11205673758865248,"Theorem 2. ∀α : 0 < α ≤
q"
PRIVACY GUARANTEES OF BITRAND,0.11347517730496454,"ϵX+rl
2r Pl−1
i=0 exp(2 ϵX"
PRIVACY GUARANTEES OF BITRAND,0.1148936170212766,"l i%l), the f-RR mechanism satisﬁes ϵX-LDP:"
PRIVACY GUARANTEES OF BITRAND,0.11631205673758865,"P (f-RR(vx)=z)
P (f-RR(evx)=z) < ϵX, where vx and evx can be different at any bit, and z ∈Range(f-RR)."
PRIVACY GUARANTEES OF BITRAND,0.11773049645390071,"Regarding to the ground-truth label yx, given a privacy budget ϵY , we show that there always exists
an upper bound of β so that y′
x preserves ϵY -LDP given yx.
Theorem 3. ∀β : β ≤ϵY −ln(C −1), the label-RR mechanism (Alg. 1, line 18) satisﬁes ϵY -LDP:
P (label-RR(yx)=z|yx)
P (label-RR(eyx)=z|eyx) ≤exp(ϵY ), given any distinct labels yx and eyx, and z ∈Range(label-RR)."
PRIVACY GUARANTEES OF BITRAND,0.11914893617021277,"From Theorems 2 and 3, our BitRand preserves ϵX-LDP for the vector vx and ϵY -LDP for label
yx. The gradients ▽u
θt preserve (ϵX, ϵY )-LDP in any training rounds t for any training samples
(x, yx) ∈Du and for all clients u ∈[1, N]; since, ∀u, t: ▽u
θt are computed from the randomized
training samples {(v′
x, y′
x)}nu without accessing any further information from {(x, yx)}nu."
PRIVACY AND UTILITY TRADE-OFF,0.12056737588652482,"5
PRIVACY AND UTILITY TRADE-OFF"
PRIVACY AND UTILITY TRADE-OFF,0.12198581560283688,"Figure 3: Expected error bound comparison
for an embedded feature a with r = 1, 000,
l = 10, and m = 5."
PRIVACY AND UTILITY TRADE-OFF,0.12340425531914893,"As shown in Theorem 1, we can derive a tighter pri-
vacy loss bound. Therefore, we focus on understand-
ing how BitRand can address the privacy-utility trade-
off in comparison with existing approaches by theo-
retically studying: (1) the utility of f-RR regarding
the expected error bound (Theorem 4), and (2) the
trade-off between privacy budget and randomization
probabilities. All statistical tests are 2-tail t-tests."
PRIVACY AND UTILITY TRADE-OFF,0.12482269503546099,"Expected Error Bounds. We analyze the data utility
through expected error, denoted as ξa, measuring the"
PRIVACY AND UTILITY TRADE-OFF,0.12624113475177304,Under review as a conference paper at ICLR 2022
PRIVACY AND UTILITY TRADE-OFF,0.1276595744680851,"expected change of each embedded feature a after applying f-RR: ξa = E|E(f-RR(va)) −E(va)|.
The smaller expected error is, the better data utility the randomization mechanism achieves. The
expected error ξa is bounded by P"
PRIVACY AND UTILITY TRADE-OFF,0.12907801418439716,"i∈[0,l−1] qXi × ∆i."
PRIVACY AND UTILITY TRADE-OFF,0.13049645390070921,"Theorem 4. The f-RR expected error bound is quantiﬁed by ξa = E|E(f-RR(va)) −E(va)| =
P"
PRIVACY AND UTILITY TRADE-OFF,0.13191489361702127,"i∈[0,l−1] qXi × ∆i."
PRIVACY AND UTILITY TRADE-OFF,0.13333333333333333,"Theorem 4 can be directly applied to quantify the expected error bounds of f-RR without the bit-
aware term i%l/l, corrected LATENT (Appendix I), and corrected OME (Appendix J). Regarding
existing mechanisms applied on the embedded feature a, including Duchi mechanism (DM) (Duchi
et al., 2013), Piecewise mechanism (PM) (Wang et al., 2019a), Hybrid mechanism (HM) (Wang
et al., 2019a), Suboptimal mechanism (PM-SUB) (Zhao et al., 2020b), Gaussian and Laplace (Dwork
& Roth, 2014), we derive a general form of expected error bounds ξa for these mechanisms as:
ξa = E|M(a) −a| ≊1/r P"
PRIVACY AND UTILITY TRADE-OFF,0.1347517730496454,"a∈[1,r] |M(a) −a| where M is an ϵX-LDP preserving mechanism,
since limr→∞E|M(a) −a| = 1/r P"
PRIVACY AND UTILITY TRADE-OFF,0.13617021276595745,"a∈[1,r] |M(a) −a|."
PRIVACY AND UTILITY TRADE-OFF,0.1375886524822695,"Figure 3 illustrates the expected error bound of each algorithm as a function of ϵX. It is obvious that
f-RR has signiﬁcantly tighter expected error bounds compared with baseline approaches under a
wide range of privacy budgets given reasonable large numbers of embedded features and encoding
bits (p = 0.02); thus indicating that our mechanism achieves better data utility compared with the
baselines. The improvement of our f-RR over existing mechanisms is larger when ϵX increases."
PRIVACY AND UTILITY TRADE-OFF,0.13900709219858157,"To profoundly study the data utility, we take a depth look into bit-level analysis by relaxing Theorem
4 into (1) an expected error bound ξi = qXi × ∆i for each bit i ∈[0, l −1], and (2) an average top-k
expected error bound ξtop−k = 1/k Pk
i=0 ξi, ∀k ∈[0, l −1]. Figure 11 (Appendix K) show that, at
the bit-level, f-RR achieves smaller values of ξi for most important bits, especially the sign bit and
integer bits, and comparable ξi for least important bits, i.e., fraction bits. The gap between f-RR
and the baselines are signiﬁcantly larger given the ξtop−k. We obtain smaller values of ξtop−k for all
k ∈[0, l −1], under a tight privacy budget ϵX = 0.1. Importantly, when increasing privacy budget
ϵX (Figures 11b,c), the gap between f-RR and the baselines is larger. In fact, the expected error
bounds in f-RR is reduced while the expected error bounds in the baselines are remained the same."
PRIVACY AND UTILITY TRADE-OFF,0.14042553191489363,"Privacy budget and randomization probabilities trade-off. Compared with existing RR mech-
anisms (Lyu et al., 2020a; Arachchige et al., 2019; Sun et al., 2021; Zhao et al., 2020b; Duchi &
Rogers, 2019; Wang et al., 2019a; Liu et al., 2020), in our algorithm, bits with a stronger inﬂuence on
the model utility, e.g., sign bits and integer bits, have smaller randomization probabilities qX, and
vice-versa. This is because we consider the inﬂuence of each bit i through the term i%l"
PRIVACY AND UTILITY TRADE-OFF,0.14184397163120568,"l
in modeling
the randomization probabilities qX (and pX). This unique property of our algorithm enables us to
better optimize the trade-off between privacy loss and model utility."
PRIVACY AND UTILITY TRADE-OFF,0.14326241134751774,"Figure 4: Randomization proba-
bility qX and qtop-k, given l = 10,
r = 1, 000, and ϵX = 1."
PRIVACY AND UTILITY TRADE-OFF,0.14468085106382977,"To demonstrate this, we examine the behavior of qX across bi-
nary encoding bits under a wide range of ϵX ∈{0.1, 1, 2}, given
reasonable values of r and l, i.e., r = 1, 000 and l = 10 (Fig-
ures 4 and 12, Appendix K). Our mechanism achieves a smaller
randomization probability qX than the corrected LATENT in all
cases (p = 3.7e −9), especially more signiﬁcant bits, such as
sign bits and integer bits. The gap is more prominent when ϵX
increases. This observation is less obvious when comparing our
mechanism with the corrected OME, given an uneven randomiza-
tion probability qX across bits in the corrected OME. To better
show the comparison, we draw an average top-k measure curve:
∀k ∈[0, l −1] : qtop-k = 1/k Pk
i=0 qi, where qi is the bit i’s qX,
to evaluate the average qX across bits. The smaller qtop-k is, the better the randomization probability
qX is. Given a tight privacy budget ϵX = 0.1, our mechanism and the corrected OME have a similar
qtop-k. However, when ϵX increases, i.e., ϵX ∈{1, 2}, our mechanism achieves signiﬁcantly smaller
values of qtop-k than the corrected OME (p = 5.1e −3)."
DIMENSION-ELASTIC ANALYSIS,0.14609929078014183,"6
DIMENSION-ELASTIC ANALYSIS
In existing RR mechanisms, the curse of privacy composition is rooted in the privacy composition
across bits l and features r (Duchi et al., 2013; Lyu et al., 2020a; Arachchige et al., 2019), gradients
▽u
θt (Zhao et al., 2020b; Wang et al., 2019a), and training rounds T (Zhao et al., 2020b; Wang et al.,"
DIMENSION-ELASTIC ANALYSIS,0.1475177304964539,Under review as a conference paper at ICLR 2022
DIMENSION-ELASTIC ANALYSIS,0.14893617021276595,"2019a). Thus, increasing the dimensions of l, r, ▽u
θt, and T either signiﬁcantly increases the privacy
budget (i.e., resulting in loose privacy protection) (Wang et al., 2019a; 2017) or notably affects the
randomization probabilities (i.e., the value transmitted correctly through the randomization becomes
smaller resulting in poor utility (Wang et al., 2016a)) (Lyu et al., 2020a; Arachchige et al., 2019)."
DIMENSION-ELASTIC ANALYSIS,0.150354609929078,"In Theorem 1, the privacy composition across bits and features is unavoidable. However, the values of
embedded features transmitted correctly through our randomization mechanism is minimally affected
when increasing the dimensions of r, l, ▽u
θt, T, and C under the same ϵX and ϵY . That enable us to
evade the curse of privacy composition when working with complex models and tasks under rigorous
LDP protection. To shed light into this property, we conduct a theoretical analysis to examine: (1)
How the dimensions of r, l, gradients ▽u
θt, and training rounds T impact the privacy budget ϵX and
the randomization probabilities qX and pX (= 1 −qX); and (2) How the number of model outcomes
C impacts the privacy budget ϵY and the randomization probabilities qY and pY . We select the upper
bounds of α =
q"
DIMENSION-ELASTIC ANALYSIS,0.15177304964539007,"(ϵX + rl)/(2r Pl−1
i=0 exp(2ϵX i"
DIMENSION-ELASTIC ANALYSIS,0.15319148936170213,"l )) and β = ϵY −ln(C −1) (Theorems 2, 3) in our analysis."
DIMENSION-ELASTIC ANALYSIS,0.15460992907801419,"Figure 5: Randomization proba-
bility qX as a function of r with
ﬁxed l = 10 and ϵX = 1."
DIMENSION-ELASTIC ANALYSIS,0.15602836879432624,"Dimensions of gradients ▽u
θt and training rounds T. In Bi-
tRand, the clients only use the perturbed samples (v′
x, y′
x) to train
their local models without accessing any further information from
(x, yx). Thus, the privacy budgets ϵX and ϵY are independent of
the dimension of gradients ▽u
θt and the training rounds T, i.e.,
following the post-processing property in DP (Dwork & Roth,
2014). Also, the size of ▽u
θt and T do not affect the random-
ization probabilities, since ▽u
θt and T are not used to model qX,
pX, qY , and pY as in Eqs. 2 and 3."
DIMENSION-ELASTIC ANALYSIS,0.1574468085106383,"Dimensions of embedded features r and encoding bits l.
Varying the dimensions of r and l does not affect the privacy
budget ϵX ∈R+, since there always exists an α for Theorem 2
to hold. However, it is necessary to understand the inﬂuence of varying r and l on the privacy-utility
trade-off. We theoretically analyze the impacts of r and l on the randomization probabilities qX and
pX, given ﬁxed values of ϵX. A model is expected to achieve higher model utility given smaller
values of qX (higher values of pX) under the same privacy budget. We examine qX and pX in the
following experiments: (i) Fixing l, then varying r; and (ii) Fixing r, then varying l."
DIMENSION-ELASTIC ANALYSIS,0.15886524822695036,"Figure 6: Randomization proba-
bility qX as a function of l with
ﬁxed r = 1, 000 and ϵX = 1."
DIMENSION-ELASTIC ANALYSIS,0.16028368794326242,"Figures 5 and 13 (Appendix K) illustrate qX as a function of r,
under l ∈{5, 20, 100, 1, 000} and ϵX ∈{0.1, 1, 2}. Varying r
does not affect the randomization probability qX (and pX) for
all the bits in vx. To explain this, we take a deeper look into the
α’s bound, which is the only factor affecting qX and pX. Given
ﬁxed ϵX and l, Pl−1
i=0 exp(2ϵX i"
DIMENSION-ELASTIC ANALYSIS,0.16170212765957448,"l) is a constant, denoted C. So,
we have α =
p"
DIMENSION-ELASTIC ANALYSIS,0.16312056737588654,"(ϵX + rl)/(2rC) =
p ( ϵX"
DIMENSION-ELASTIC ANALYSIS,0.16453900709219857,"r + l)/(2C) ≊
p"
DIMENSION-ELASTIC ANALYSIS,0.16595744680851063,"l/(2C),
since ϵX"
DIMENSION-ELASTIC ANALYSIS,0.1673758865248227,"r ≊0.0 in practice. Thus, with ﬁxed ϵX and l, qX and
pX are r-elastic, given α approximately is a constant
p"
DIMENSION-ELASTIC ANALYSIS,0.16879432624113475,l/(2C).
DIMENSION-ELASTIC ANALYSIS,0.1702127659574468,"Now, we ﬁx r = 1, 000, which is a decent number of embedded
features, and show qX as a function of l under a wide range
of r ∈[10, 10, 000] and ϵX ∈{0.1, 1, 2} (Figures 6 and 14,
Appendix K). Given a tight privacy budget ϵX = 0.1, varying l does not affect the randomization
probability qX, i.e., m = ⌊l/2⌋in our analysis covering most values of embedded features in practice.
However, with higher values of ϵX ∈{1.0, 2.0}, using more encoding bits l lowers qX for most
important bits, i.e., sign and integer bits; while increasing qX for least important bits, i.e., fraction
bits. When l is large enough (l ≥20), the randomization probability qX is l-elastic since the impact
of l becomes marginal to all the bits. This is also true for pX."
DIMENSION-ELASTIC ANALYSIS,0.17163120567375886,"Number of model outcomes C. Similar to ϵX, from Eq. 3 and Theorem 3, the privacy budget
ϵY is not directly affected by the number of model outcomes C, since ∀C, ϵX: ∃β for Theorem
3 to hold. However, C may impact the trade-off between privacy and model utility by affecting
the randomization probabilities qY and pY . Figure 7 shows that, when C is sufﬁciently large, i.e.,
C ≥100, the randomization probability qY is C-elastic since the impact of C on qY is marginal
given a wide range of the privacy budget ϵY . This is also true for pY . When ϵY increases, the
randomization probability qY becomes smaller (pY becomes larger). This is a reasonable observation."
DIMENSION-ELASTIC ANALYSIS,0.17304964539007092,Under review as a conference paper at ICLR 2022
DIMENSION-ELASTIC ANALYSIS,0.17446808510638298,"Thanks to the bit-aware and dimension-elastic properties, our BitRand can achieve high data utility,
especially under tight privacy budgets and expected error bounds."
EXPERIMENTAL RESULTS,0.17588652482269504,"7
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.1773049645390071,"Figure 7: Randomization proba-
bility qY with varying ϵY and C."
EXPERIMENTAL RESULTS,0.17872340425531916,"We have conducted an extensive experiment on benchmark
datasets under two fundamental FL tasks, text and image classi-
ﬁcation, to shed light on understanding 1) the interplay among
privacy budget and model utility in BitRand, 2) the effectiveness
of the dimension-elastic and bit-aware properties on model utility,
and 3) different settings of applying RR to preserve LDP."
EXPERIMENTAL RESULTS,0.18014184397163122,"Baseline Approaches. We consider a variety of LDP-preserving
mechanisms as baseline approaches: (1) Binary encoding ap-
proaches, including the corrected LATENT (Arachchige et al., 2019) and the corrected OME (Lyu
et al., 2020a) (Appendices I and J); (2) LDP-FL (Sun et al., 2021); (3) Duchi mechanism (DM)
(Duchi et al., 2013); (4) Piecewise mechanism (PM) (Wang et al., 2019a); (5) Hybrid mechanism
(HM) (Wang et al., 2019a); (6) Three-Outputs mechanism (Zhao et al., 2020b); (7) Suboptimal
mechanism (PM-SUB) (Zhao et al., 2020b); and (8) Label-Laplace (Phan et al., 2020). Each base-
line approach is applied to randomize (when applicable): (i) Embedded features ex; (ii) Gradients
▽u
θt; and (iii) Gradients ▽u
θt with a recent anonymizer (Sun et al., 2021) to reduce the privacy budget
consumption. In addition, Label-Laplace is used as a baseline to protect the ground-truth labels
yx. Note that, in our experiment, we use the upper bound of β resulting in the same randomizing
probabilities as in LabelDP. Therefore, we do not include LabelDP in comparison. More details
about the difference of label-RR and LabelDP are in Appendix G. These settings are widely accepted
to preserve LDP in FL; thus, offering a comprehensive view of preserving LDP in FL. Note that
LATENT, OME, and BitRand can only be applied on embedded features ex; while LDP-FL can only
be applied on gradients ▽u
θt with and without the anonymizer (Sun et al., 2021). We include the
Noiseless FL model trained on the original data Du to show upper-bounds and a Random guess
model to understanding model utility better."
EXPERIMENTAL RESULTS,0.18156028368794327,"Datasets, Metrics, and Model Conﬁguration. The complete details of the datasets, metrics, and
model conﬁguration are in Appendix L. We carried out our experiments on two textual datasets and
two image datasets, including the AG dataset (Gulli et al., 2012), our collected Security and Exchange
Commission (SEC) ﬁnancial contract dataset, the large-scale celebFaces attributes (CelebA) dataset
(Liu et al., 2015), and the Federated Extended MNIST (FEMNIST) dataset (Caldas et al., 2018). We
use the test accuracy and the test area under the curve (AUC) as evaluation metrics. Models with
higher values of test accuracy and AUC are better. We use the BERT-Base (Uncased) pre-trained
model (ber; Devlin et al., 2018) to extract embedded features in the AG and SEC datasets. In the
CelebA and FEMNIST datasets, we use the ResNet-18 pre-trained model (img; He et al., 2016). For
text and image classiﬁcation tasks, we use two fully connected layers on top of embedded features,
each of which consists of 1, 500 hidden neurons and uses a ReLU activation function."
EXPERIMENTAL RESULTS,0.1829787234042553,"Evaluation Results. Comprehensive results show that BitRand offers stronger privacy protection
with higher model utility, compared with all baseline approaches, as discussed next."
EXPERIMENTAL RESULTS,0.18439716312056736,"LDP-preserving approaches applied on the embedded features ex. Baseline approaches do not
work well when they are applied on embedded features ex (Figures 15 and 19, Table 3, Appendix L).
In SEC, AG, and FEMNIST datasets, BitRand achieves the highest model utility compared with the
best baseline approach, which is the corrected OME, under a tight privacy budget ϵX = 1. In terms
of accuracy and AUC values, BitRand (ϵY = ∞) achieves an improvement of 46.03% and 38.51% in
the AG dataset (p = 2.7e−22), 13.69% and 13.79% in the SEC dataset (p = 4.1e−12), and 21.62%
and 13.42% in the FEMNIST dataset (p = 5.6e −11) respectively. In the CelebA dataset (Table
3, Appendix L), BitRand outperforms the best baseline approach, i.e., PM-SUB, with an average
improvement of 1.66% across all 40 attributes in terms of AUC measure (p = 1.2e −2). Since the
CelebA dataset is highly imbalanced, we use the AUC measure instead of the model accuracy. The
gaps between BitRand and the baseline approaches are signiﬁcantly wider when the privacy budget
ϵX is larger. In addition to ϵX = 1, with a tight privacy budget for the class labels ϵY ∈{1, 2.5},
BitRand still outperforms baseline approaches in most of cases, offering stronger privacy protection
with better model utility, i.e., LDP at both embedded feature and label levels instead of only LDP on
the embedded features as in baseline approaches."
EXPERIMENTAL RESULTS,0.18581560283687942,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.18723404255319148,"Figure 8: AUC values of each algorithm applied on the gradients ▽u
θt with the anonymizer."
EXPERIMENTAL RESULTS,0.18865248226950354,"The key reason is that, in the baseline approaches, the model utility is signiﬁcantly affected by the
size of the embedded features. Thanks to the dimension-elastic and bit-aware properties, BitRand can
achieve high model accuracy and AUC values, especially under tight privacy budgets. In addition,
BitRand achieves the highest improvement in the AG dataset, since it is a balanced dataset compared
with the highly imbalanced CelebA dataset, in which BitRand achieves the least improvement.
Addressing imbalanced data in FL under DP (Huang et al., 2020) is out-of-scope of our study."
EXPERIMENTAL RESULTS,0.1900709219858156,"LDP-preserving approaches applied on on gradients ▽u
θt without and with the anonymizer
(Sun et al., 2021). We observe the same phenomenon when baseline approaches are applied on
gradients ▽u
θt without and with the anonymizer (Sun et al., 2021), even though the gaps between
BitRand and baseline approaches are (marginally) smaller (Figures 8, 16-17, 20, and Tables 4, 5,
Appendix L). Without using the anonymizer in the baseline approaches, in terms of accuracy and
AUC, compared with the best baseline approach PM-SUB, BitRand (ϵX = 1, ϵY = 1) achieves an
improvement of 44.95% and 37.52% in the AG dataset (p = 3.9e −20), 12.82% and 12.92% in the
SEC dataset (p = 1.3e −11), 24.17% and 13.40% in the FEMNIST dataset (p = 4.1e −11). When
the anonymizer is applied in the baseline approaches, in terms of accuracy and AUC, compared with
the best baseline approache HM, our BitRand achieves an improvement of 39.38% and 32.75% in the
AG dataset (p = 1.2e −16), 13.59% and 13.69% in the SEC dataset (p = 2.1e −10), and 23.12%
and 37.02% in the FEMNIST dataset (p = 2.8e −11). Regarding the CelebA dataset, BitRand
outperforms the best baseline approaches, which are Three-outputs and HM, with improvements of
1.28% and 0.3%, in the cases of with and without the anonymizer, across all 40 attributes in terms of
AUC (p = 3.1e−2). The model utility in baselines is affected by the size of gradients and the training
rounds (when the anonymizer is not applied) and their ﬁnite numbers of randomization outputs of the
gradients (Zhao et al., 2020b). Thanks to the bit-aware and dimension-agnostic properties, BitRand
can achieve high model utility under rigorous LDP protection."
EXPERIMENTAL RESULTS,0.19148936170212766,"LDP-preserving labels. Figures 18, 21, and Table 3 (Appendix L) present the model utility of
BitRand as a function of the privacy budgets ϵX and ϵY , in which label-RR is replaced by the
Label-Laplace, denoted as f-RR & Label-Laplace. label-RR outperforms the Label-Laplace in all
values of ϵX, ϵY , and datasets. Under rigorous LDP protection ϵY = 1 given ϵX ∈[1, 10], there is
an average improvement of 9.65% accuracy and 7.92% AUC in the AG dataset (7.3e −6), 7.84%
accuracy and 7.35% AUC in the SEC dataset (1.2e −5), and 9.91% accuracy and 10.55% AUC in
the FEMNIST dataset (p = 9.8e −5), and 2.04% AUC in the CelebA dataset (p = 1.6e −2). The
gaps are delicately smaller in the AG and SEC datasets, and substantially larger in the FEMNIST and
CelebA datasets when ϵY is increased. Given ϵY = 2.5, there is an average improvement of 1.71%
accuracy and 1.37% AUC in the AG dataset (8.5e −2), 4.65% accuracy and 4.73% AUC in the SEC
dataset (6.9e −3), and 14.43% accuracy and 12.02% AUC in the FEMNIST dataset (p = 1.9e −4),
and 4.24% AUC in the CelebA dataset (p = 2.3e −2). The reason is that Label-Laplace injects
Laplace noise across C classes in a label yx causing more noisy model outcomes compared with
label-RR, in which only one of the C model outcomes is selected as the result of the randomization."
CONCLUSION,0.19290780141843972,"8
CONCLUSION
In this paper, we introduced a bit-aware algorithm, called BitRand, providing rigorous LDP protection
to both embedded features and labels in FL via binary encoding. In BitRand, the trade-off between
the privacy budget consumption and randomization probabilities is dimension-elastic to the numbers
of embedded features, encoding bits, gradients, model outcomes, and training rounds, enabling us
to work with complex models and FL tasks. We further optimize the randomization probabilities
by having smaller randomization probabilities assigned to more critical bits and vice-versa under
the same privacy budgets. Theoretical analysis and extensive experiments showed that our BitRand
outperforms baseline approaches in text and image classiﬁcation."
CONCLUSION,0.19432624113475178,Under review as a conference paper at ICLR 2022
REFERENCES,0.19574468085106383,REFERENCES
REFERENCES,0.1971631205673759,Cometomyhead academic news search engine. http://newsengine.di.unipi.it.
REFERENCES,0.19858156028368795,"Google ai. pre-trained bert model. https://bert-as-service.readthedocs.io/en/
latest/section/get-start.html#installation."
REFERENCES,0.2,Pre-trained resnet-18 model. https://github.com/christiansafka/img2vec.
REFERENCES,0.20141843971631207,Induction proofs. https://www.purplemath.com/modules/inductn3.htm.
REFERENCES,0.2028368794326241,"M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep
learning with differential privacy. In ACM SIGSAC Conference on Computer and Communications
Security, pp. 308–318, 2016."
REFERENCES,0.20425531914893616,"J. Acharya, Z. Sun, and H. Zhang. Hadamard response: Estimating distributions privately, efﬁciently,
and with little communication. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics, pp. 1120–1129, 2019."
REFERENCES,0.20567375886524822,"M. Alvim, K. Chatzikokolakis, C. Palamidessi, and A. Pazii. Local differential privacy on metric
spaces: optimizing the trade-off with utility. In 2018 IEEE 31st Computer Security Foundations
Symposium, pp. 262–267, 2018."
REFERENCES,0.20709219858156028,"P. C. M. Arachchige, P. Bertok, I. Khalil, D. Liu, S. Camtepe, and M. Atiquzzaman. Local differential
privacy for deep learning. IEEE Internet of Things Journal, 7(7):5827–5842, 2019."
REFERENCES,0.20851063829787234,"B. Balle, J. Bell, A. Gasc´on, and K. Nissim. The privacy blanket of the shufﬂe model. In Annual
International Cryptology Conference, pp. 638–667, 2019."
REFERENCES,0.2099290780141844,"R. Bassily and A. Smith. Local, private, efﬁcient protocols for succinct histograms. In ACM
Symposium on Theory of Computing, pp. 127–135, 2015."
REFERENCES,0.21134751773049645,"R. Bassily, K. Nissim, U. Stemmer, and A. Thakurta. Practical locally private heavy hitters. arXiv
preprint arXiv:1707.04982, 2017."
REFERENCES,0.2127659574468085,"Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protec-
tion against reconstruction and its applications in private federated learning. arXiv preprint
arXiv:1812.00984, 2018."
REFERENCES,0.21418439716312057,"Robert Istvan Busa-Fekete, Umar Syed, Sergei Vassilvitskii, et al. On the pitfalls of label differential
privacy. In NeurIPS 2021 Workshop LatinX in AI, 2021."
REFERENCES,0.21560283687943263,"S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Koneˇcn`y, H. B. McMahan, V. Smith, and A. Talwalkar.
Leaf: A benchmark for federated settings. arXiv preprint:1812.01097, 2018."
REFERENCES,0.2170212765957447,"N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown,
D. Song, U. Erlingsson, et al. Extracting training data from large language models. arXiv preprint
arXiv:2012.07805, 2020."
REFERENCES,0.21843971631205675,"A. Cheu, A. Smith, J. Ullman, D. Zeber, and M. Zhilyaev. Distributed differential privacy via
shufﬂing. In EUROCRYPT, pp. 375–403, 2019."
REFERENCES,0.2198581560283688,"G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. EMNIST: Extending MNIST to handwritten
letters. In International Joint Conference on Neural Networks, pp. 2921–2926, 2017."
REFERENCES,0.22127659574468084,"G. Cormode, T. Kulkarni, and D. Srivastava. Marginal release under local differential privacy. In
ACM SIGMOD, pp. 131–146, 2018."
REFERENCES,0.2226950354609929,"J. Devlin, M. W. Chang, K. Lee, and K. Toutanova.
Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.22411347517730495,"J. Dong, A. Roth, and W. J. Su. Gaussian differential privacy. arXiv preprint arXiv:1905.02383,
2019."
REFERENCES,0.225531914893617,"J. Duchi and R.n Rogers. Lower bounds for locally private estimation via communication complexity.
In COLT, pp. 1161–1191, 2019."
REFERENCES,0.22695035460992907,Under review as a conference paper at ICLR 2022
REFERENCES,0.22836879432624113,"J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and statistical minimax rates. In IEEE
Protocols for secure computations, pp. 429–438, 2013."
REFERENCES,0.2297872340425532,"J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Minimax optimal procedures for locally private
estimation. Journal of the American Statistical Association, 113(521):182–201, 2018."
REFERENCES,0.23120567375886525,"C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Found. Trends Theor.
Comput. Sci., 9(3&#8211;4):211–407, 2014. ISSN 1551-305X."
REFERENCES,0.2326241134751773,"U. Erlingsson, V. Pihur, and A. Korolova. Rappor: Randomized aggregatable privacy-preserving
ordinal response. In Proceedings of the 2014 ACM SIGSAC CCS, pp. 1054–1067, 2014."
REFERENCES,0.23404255319148937,"´U. Erlingsson, V. Feldman, I. Mironov, A. Raghunathan, K. Talwar, and A. Thakurta. Ampliﬁcation
by shufﬂing: From local to central differential privacy via anonymity. In ACM-SIAM SODA, pp.
2468–2479, 2019."
REFERENCES,0.23546099290780143,"G. Fanti, V. Pihur, and ´U. Erlingsson. Building a rappor with the unknown: Privacy-preserving
learning of associations and data dictionaries. arXiv preprint arXiv:1503.01214, 2015."
REFERENCES,0.23687943262411348,"M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit conﬁdence information
and basic countermeasures. In ACM SIGSAC CCS, pp. 1322–1333, 2015."
REFERENCES,0.23829787234042554,"GDPR. The european data protection regulation. https://gdpr-info.eu/, 2018."
REFERENCES,0.2397163120567376,"R. C. Geyer, T. Klein, and M. Nabi. Differentially private federated learning: A client level perspective.
arXiv preprint arXiv:1712.07557, 2017."
REFERENCES,0.24113475177304963,"B. Ghazi, N. Golowich, R. Kumar, P. Manurangsi, and C. Zhang. On deep learning with label
differential privacy. arXiv preprint arXiv:2102.06062, 2021."
REFERENCES,0.2425531914893617,"M. E. Gursoy, A. Tamersoy, S. Truex, W. Wei, and L. Liu. Secure and utility-aware data collection with
condensed local differential privacy. IEEE Transactions on Dependable and Secure Computing,
2019."
REFERENCES,0.24397163120567375,"A. Haeberlen, B. C. Pierce, and A. Narayan. Differential privacy under ﬁre. In USENIX Security
Symposium, volume 33, 2011."
REFERENCES,0.2453900709219858,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016."
REFERENCES,0.24680851063829787,"X. Huang, Y. Ding, Z. L. Jiang, S. Qi, X. Wang, and Q. Liao. DP-FL: a novel differentially private
federated learning framework for the unbalanced data. World Wide Web, 23(4):2529–2545, 2020."
REFERENCES,0.24822695035460993,"P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. arXiv
preprint arXiv:1912.04977, 2019."
REFERENCES,0.24964539007092199,"S. Kim, H. Shin, C. Baek, S. Kim, and J. Shin. Learning new words from keystroke data with local
differential privacy. IEEE TKDE, pp. 479–491, 2018."
REFERENCES,0.251063829787234,"Ruixuan L., Yang C., Hong C., Ruoyang G., and Masatoshi Y. FLAME: differentially private
federated learning in the shufﬂe model. CoRR, abs/2009.08063, 2020."
REFERENCES,0.2524822695035461,"R. Liu, Y. Cao, M. Yoshikawa, and H. Chen. Fedsel: Federated sgd under local differential privacy
with top-k dimension selection. In International Conference on Database Systems for Advanced
Applications, pp. 485–501, 2020."
REFERENCES,0.25390070921985813,"Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In ICCV, December
2015."
REFERENCES,0.2553191489361702,"L. Lyu, Y. Li, X. He, and T. Xiao. Towards differentially private text representations. In Proceedings
of the 43rd International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 1813–1816, 2020a."
REFERENCES,0.25673758865248225,Under review as a conference paper at ICLR 2022
REFERENCES,0.2581560283687943,"L. Lyu, H. Yu, and Q. Yang. Threats to federated learning: A survey. arXiv preprint arXiv:2003.02133,
2020b."
REFERENCES,0.25957446808510637,"M. Malekzadeh, B. Hasircioglu, N. Mital, K. Katarya, M. E. Ozfatura, and D. G¨und¨uz. Dopamine:
Differentially private federated learning on medical data. arXiv preprint arXiv:2101.11693, 2021."
REFERENCES,0.26099290780141843,"B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient
learning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics, pp.
1273–1282, 2017."
REFERENCES,0.2624113475177305,"Ilya Mironov. On signiﬁcance of the least signiﬁcant bits for differential privacy. In Proceedings of
the 2012 ACM conference on Computer and communications security, pp. 650–661, 2012."
REFERENCES,0.26382978723404255,"N. H. Phan, M. T. Thai, H. Hu, R. Jin, T. Sun, and D. Dou. Scalable differential privacy with certiﬁed
robustness in adversarial learning. In ICML, pp. 7683–7694, 2020."
REFERENCES,0.2652482269503546,"Protection Regulation. General data protection regulation. Intouch, 2018."
REFERENCES,0.26666666666666666,"X. Ren, C. M. Yu, W. Yu, S. Yang, X. Yang, J. A. McCann, and S. Y. Philip. Lopub: High-dimensional
crowdsourced data publication with local differential privacy. IEEE Transactions on Information
Forensics and Security, 13(9):2151–2166, 2018."
REFERENCES,0.2680851063829787,"H. Shin, S. Kim, J. Shin, and X. Xiao. Privacy enhanced matrix factorization for recommendation
with local differential privacy. IEEE Transactions on Knowledge and Data Engineering, 30(9):
1770–1782, 2018."
REFERENCES,0.2695035460992908,"C. Song and A. Raghunathan. Information leakage in embedding models. In ACM SIGSAC CCS, pp.
377–390, 2020."
REFERENCES,0.27092198581560284,"L. Sun, J. Qian, and X. Chen. LDP-FL: Practical private aggregation in federated learning with local
differential privacy. IJCAI, 2021."
REFERENCES,0.2723404255319149,"Cybersecurity Law.
Cybersecurity law of the people’s republic of china.
https:
//en.wikipedia.org/wiki/Cybersecurity_Law_of_the_People%27s_
Republic_of_China, 2016."
REFERENCES,0.27375886524822696,"Gulli et al. Ag’s corpus of news articles. http://groups.di.unipi.it/˜gulli/AG_
corpus_of_news_articles.html, 2012."
REFERENCES,0.275177304964539,"S. Truex, N. Baracaldo, A. Anwar, T. Steinke, H. Ludwig, R. Zhang, and Y. Zhou. A hybrid approach
to privacy-preserving federated learning. In Proceedings of the 12th ACM Workshop on Artiﬁcial
Intelligence and Security, pp. 1–11, 2019."
REFERENCES,0.2765957446808511,"S. Wagh, X. He, A. Machanavajjhala, and P. Mittal. Dp-cryptography: marrying differential privacy
and cryptography in emerging applications. Communications of the ACM, 64(2):84–93, 2021."
REFERENCES,0.27801418439716313,"D. Wang and J. Xu. On sparse linear regression in the local differential privacy model. In ICML, pp.
6628–6637, 2019."
REFERENCES,0.2794326241134752,"N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, and G. Yu. Collecting and analyzing
multidimensional data with local differential privacy. In IEEE ICDE, pp. 638–649, 2019a."
REFERENCES,0.28085106382978725,"S. Wang, L. Huang, P. Wang, H. Deng, H. Xu, and W. Yang. Private weighted histogram aggregation
in crowdsourcing. In International Conference on Wireless Algorithms, Systems, and Applications,
pp. 250–261, 2016a."
REFERENCES,0.2822695035460993,"S. Wang, L. Huang, P. Wang, Y. Nie, H. Xu, W. Yang, X. Li, and C. Qiao. Mutual information
optimally local private discrete distribution estimation. arXiv preprint:1607.08025, 2016b."
REFERENCES,0.28368794326241137,"T. Wang, J. Blocki, N. Li, and S. Jha. Locally differentially private protocols for frequency estimation.
In 26th USENIX Security Symposium, pp. 729–745, 2017."
REFERENCES,0.2851063829787234,"T. Wang, B. Ding, M. Xu, Z. Huang, C. Hong, J. Zhou, N. Li, and S. Jha. Improving utility and
security of the shufﬂer-based differential privacy. arXiv preprint arXiv:1908.11515, 2019b."
REFERENCES,0.2865248226950355,Under review as a conference paper at ICLR 2022
REFERENCES,0.28794326241134754,"T. Wang, B. Ding, M. Xu, et al. Murs: practical and robust privacy ampliﬁcation with multi-party
differential privacy. In ACSAC, 2019c."
REFERENCES,0.28936170212765955,"S. L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal
of the American Statistical Association, 60(309):63–69, 1965."
REFERENCES,0.2907801418439716,"X. Xiong, S. Liu, D. Li, J. Wang, and X. Niu. Locally differentially private continuous location
sharing with randomized response. International Journal of Distributed Sensor Networks, 15(8):
1550147719870379, 2019."
REFERENCES,0.29219858156028367,"C. Xu, J. Ren, L. She, Y. Zhang, Z. Qin, and K. Ren. Edgesanitizer: Locally differentially private deep
inference at the edge for mobile data analytics. IEEE Internet of Things Journal, 6(3):5140–5151,
2019."
REFERENCES,0.2936170212765957,"Hongxu Y., Arun M., Arash V., Jose M. A., Jan K., and Pavlo M. See through gradients: Image batch
recovery via gradinversion, 2021."
REFERENCES,0.2950354609929078,"Q. Yang, Y. Liu, T. Chen, and Y. Tong. Federated machine learning: Concept and applications. ACM
Transactions on Intelligent Systems and Technology, 10(2):1–19, 2019."
REFERENCES,0.29645390070921984,"B. Zhao, K. R. Mopuri, and H. Bilen. idlg: Improved deep leakage from gradients. arXiv preprint
arXiv:2001.02610, 2020a."
REFERENCES,0.2978723404255319,"X. Zhao, Y. Li, Y. Yuan, X. Bi, and G. Wang. Ldpart: effective location-record data publication via
local differential privacy. IEEE Access, 2019."
REFERENCES,0.29929078014184396,"Y. Zhao, J. Zhao, M. Yang, T. Wang, N. Wang, L. Lyu, D. Niyato, and K. Y. Lam. Local differential
privacy based federated learning for internet of things. IEEE Internet of Things Journal, 2020b."
REFERENCES,0.300709219858156,"K. Zheng, W. Mou, and L. Wang. Collect at once, use effectively: Making non-interactive locally
private learning possible. In ICML, pp. 4130–4139, 2017."
REFERENCES,0.3021276595744681,"Q. Zheng, S. Chen, Q. Long, and W. Su. Federated f-differential privacy. In International Conference
on Artiﬁcial Intelligence and Statistics, volume 130, pp. 2251–2259, 2021."
REFERENCES,0.30354609929078014,"L. Zhu, Z. Liu, and S. Han. Deep leakage from gradients. In NeurIPS, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
60a6c4002cc7b29142def8871531281a-Paper.pdf."
REFERENCES,0.3049645390070922,Under review as a conference paper at ICLR 2022
REFERENCES,0.30638297872340425,APPENDIX
REFERENCES,0.3078014184397163,"A
REVISITING RANDOMIZED RESPONSE MECHANISMS FOR LDP"
REFERENCES,0.30921985815602837,"To preserve LDP given the client’s input x, we can apply existing RR mechanisms (Wang et al.,
2016b; Fanti et al., 2015; Bassily et al., 2017; Kim et al., 2018; Ren et al., 2018; Zheng et al., 2017;
Wang & Xu, 2019; Zhao et al., 2019; Gursoy et al., 2019; Alvim et al., 2018; Xiong et al., 2019),
such as unary encoding-based approaches (Wang et al., 2017; Erlingsson et al., 2014), hash-based
approaches (Wang et al., 2017; Bassily & Smith, 2015; Acharya et al., 2019; Wang et al., 2019c),
binary encoding-based approaches (Arachchige et al., 2019; Lyu et al., 2020a), etc. For instance,
hash-based approaches such as those of Google RAPPOR (Erlingsson et al., 2014) and OLH (Wang
et al., 2017) hash the client’s input x onto a bloom ﬁlter B of size k using h hash functions. Then, for
each client’s input x and a bit i ∈B, RAPPOR creates a perturbed binary value B′
i from Bi with the
following randomization probability:"
REFERENCES,0.31063829787234043,"B′
i = 
 "
REFERENCES,0.3120567375886525,"1,
with probability p/2
0,
with probability p/2
Bi,
with probability 1 −p
(9)"
REFERENCES,0.31347517730496455,"where p is a hyper-parameter. This B′ is reused as the basis for all future analysis, learning, and
reports on this distinct input x. This approach achieves ϵX-LDP, where ϵX = 2h ln((1 −p 2)/ p"
REFERENCES,0.3148936170212766,"2),
given that the sensitivity of every bit Bi is ∆Bi = 1 (Erlingsson et al., 2014)."
REFERENCES,0.31631205673758866,"To deal with numerical inputs, e.g., embedded features, generalized RR mechanisms such as Duchi
(Duchi & Rogers, 2019; Bhowmick et al., 2018), Piece-wise (Wang et al., 2019a), Hybrid (Wang
et al., 2019a), Three-outputs (Zhao et al., 2020b), Suboptimal (Zhao et al., 2020b), LDP-FL (Sun
et al., 2021), LATENT (Arachchige et al., 2019), and OME (Lyu et al., 2020a) can be applied."
REFERENCES,0.3177304964539007,"Asymmetric version of RAPPOR (e.g., (Wang et al., 2017)) designs different randomization prob-
abilities for different inputs. The technique is well-applied in the context of frequency estimation
and successfully reduce the communication cost from O(d) to O(log n) (d is data dimension and n
is the number of samples). However, simply applying the mechanism (Wang et al., 2017) does not
optimize the model utility and the privacy-utility trade-off when working with machine learning or
deep learning models."
REFERENCES,0.3191489361702128,"Another line of work in LDP is Mironov (2012), which addresses the ﬂoating-point arithmetic in
implementation of DP applications. The inconsistency between mathematical abstraction of Laplace
mechanism with sampling “uniform” ﬂoating-point numbers can be exploited to carry out privacy
attacks. Floating-point arithmetic is a leaky abstraction, which is ubiquitous in computer systems
and is difﬁcult to argue about formally and hard to get right in applications, including all the RR
mechanisms."
REFERENCES,0.32056737588652484,"However, different from the asymmetric version of RAPPOR and the ﬂoating-point arithmetic, our
proposed f-RR mechanism focuses on mitigating the privacy-utility trade-off. To achieve that,
besides the asymmetric nature of the randomization probabilities, our designed f-RR consists of
two key components: 1) The bit-aware term i%l/l, which indicates the location of the bit i in each
embedded feature associated with the sensitivity of the bit at that location; and 2) The adjustable
but bounded α, which takes into account the correlation between privacy loss and the sensitivity of
embedded features to mitigate the privacy-utility trade-off and the curse of privacy composition."
REFERENCES,0.3219858156028369,"The bit-aware property refers to the bits with a more substantial inﬂuence on the model utility
have smaller randomization probabilities, and vice-versa, under the same privacy protection. By
incorporating sensitivities of binary encoding bits into a generalized privacy loss bound, we show
that increasing the dimensions of embedded features r, encoding bits l, and model outcomes C
marginally affect the randomization probabilities in BitRand under the same privacy budget. This
dimension-elastic property is crucial to mitigate the curse of privacy composition by retaining a high
value of data transmitted correctly through our randomization given large dimensions of r, l, and C."
REFERENCES,0.32340425531914896,"Besides the f-RR for protecting the data, we also include the label-RR for protecting the label in our
proposed BitRand mechanism, that provides a complete protection for every data sample."
REFERENCES,0.324822695035461,Under review as a conference paper at ICLR 2022
REFERENCES,0.3262411347517731,"B
BITRAND ALGORITHM PSEUDO-CODE"
REFERENCES,0.3276595744680851,"1: Input: Privacy budget ϵX and ϵY , number of
training iterations T, learning rate ηt, binary
encoding parameters (l and m)
2: At server side:
3: Initialize model θ0"
REFERENCES,0.32907801418439714,"4: Send the pre-trained model f pre to clients
5: for t ∈T do
6:
Distribute model parameter θt to each client
7:
for each client u do
8:
▽u
θt ←Client-Update(θt)
9:
end for
10:
θt+1 = θt −ηt
P"
REFERENCES,0.3304964539007092,"u∈[1,N] ▽u
θt/N"
REFERENCES,0.33191489361702126,"11: end for
12: Output: (ϵX, ϵY )-LDP θ"
REFERENCES,0.3333333333333333,"13: At client side u ∈[1, N]:
14: for each data sample (x, yx) ∈Du do
15:
Extracting embedded features: ex ←f pre(x)
16:
vx ←BinaryEncoding(ex) # using Eq. 1
17:
Randomizing vx: v′
x ←f-RR(vx) with α =
r"
REFERENCES,0.3347517730496454,"ϵX+rl
2r Pl−1
i=0 exp(2ϵX i"
REFERENCES,0.33617021276595743,l ) # using Eq. 2
REFERENCES,0.3375886524822695,"18:
Randomizing the label yx: y′
x ←label-RR(yx)
with β = ϵY −ln(C −1) # using Eq. 3
19: end for
20: Client-Update(θt):
21:
▽u
θt = P"
REFERENCES,0.33900709219858155,"(v′x,y′x)∈D′u ▽θtL(f(e′
x), y′
x)/nu"
REFERENCES,0.3404255319148936,"22:
return ▽u
θt"
REFERENCES,0.34184397163120567,Algorithm 1: BitRand Algorithm in Federated Learning
REFERENCES,0.3432624113475177,"C
PROOF OF LEMMA 1"
REFERENCES,0.3446808510638298,"Proof. Without loss of generality, let us study the l1-sensitivity of binary encoding bits of a feature’s
value a in ex. It is obvious that f-RR(va, i) and f-RR(va|i, i) differs at the bit i in the worst case.
The decoded feature of f-RR(va, i) is a′ = E(f-RR(va, i))."
REFERENCES,0.34609929078014184,"Let us denote b0b1 . . . bl−1 as the binary representation f-RR(va, i) of a′, where b0 is the value of
a sign bit (i.e., b0 = 1 if a′ >= 0 and b0 = 0 if a′ < 0), and {bi}l−1
i=1 is the value of integer bits
and fraction bits. We have a decoding function: E(f-RR(va, i)) = (2b0 −1) Pl−1
i=1 bi × 2m−i. This
decoding function is also applicable to E(f-RR(va|i, i)). Let us denote {b′
i}l−1
i=0 as the value of the
bit i in f-RR(va|i, i). Following Eq. 4, the l1-sensitivity of the bit bi is computed as follows:"
REFERENCES,0.3475177304964539,"• If i is an integer or a fraction bit (i
∈
[1, l −1]), we have:
∆i
=
maxva
∥"
REFERENCES,0.34893617021276596,"E(f-RR(va, i))−E
 
f-RR(va|i, i)

∥1= maxva ∥(2b0−1)
 Pl−1
j=1,j̸=i bj2m−j+bi2m−i
−(2b0−"
REFERENCES,0.350354609929078,"1)
 Pl−1
j=1,j̸=i bj2m−j + b′
i2m−i
∥1 = maxva ∥(2b0 −1)(bi −b′
i)2m−i∥1 ≤maxva

|(2b0 −"
REFERENCES,0.3517730496453901,"1)|∥bi −b′
i∥12m−i
. The ∆i is maximized when bi and b′
i are different, and |2b0 −1| = 1."
REFERENCES,0.35319148936170214,"Therefore, ∆i = 2m−i."
REFERENCES,0.3546099290780142,"• If i is the sign bit (i
=
0),
we have:
∆i
=
maxva
∥
E(f-RR(va, 0)) −
E
 
f-RR(va|i, 0)

∥1=
maxva ∥(2b0 −1) Pl−1
i=1 bi2m−i −(2b′
0 −1) Pl−1
i=1 bi2m−i∥1
="
REFERENCES,0.35602836879432626,"maxva ∥(2b0 −2b′
0) Pl−1
i=1 bi2m−i∥1 ≤maxva

|2b0 −2b′
0|∥Pl−1
i=1 bi2m−i∥1

. The ∆i is maxi-"
REFERENCES,0.3574468085106383,"mized when b0 and b′
0 are different and all {bi}l−1
i=1 = 1. Then, ∆i ≤maxva|0 2 Pl−1
i=1 2m−i. Since
21 + . . . + 2l−2 = 2l−1 −2 (mat), we have: 2Pl−1
i=1 2m−i = 2m+1−(l−1)(2l−1 −1) < 2m+1. As
a result, ∆i = 2m+1."
REFERENCES,0.3588652482269504,"From the aforementioned ∆i of a feature’s value in ex, it is easily expanded to the entire ex. Since ex
is the concatenation of all l bits of r values in ex, all bits with the same value of i%l have the same
l1-sensitivity across rl bits. Therefore, the ∆i of the sign bit and the integer/fraction bits are 2m+1"
REFERENCES,0.36028368794326243,"and 2m−i%l, respectively. Consequently, Lemma 1 hold."
REFERENCES,0.3617021276595745,"D
PROOF OF THEOREM 1"
REFERENCES,0.36312056737588655,"Let us consider a bit i belonging to a feature a, i.e., one of the r features. We denote f-RR(va, i)
as vector va with only the bit i randomized by our f-RR, i.e., all the bits different from i are kept
the same in va; that is, we only protect the bit i when using the notation f-RR(va, i). Given the
l1-sensitivity ∆i, there always exists a Laplace noise injected into the embedded feature a, i.e.,"
REFERENCES,0.3645390070921986,Under review as a conference paper at ICLR 2022
REFERENCES,0.3659574468085106,"M(va, i) = E(va) + Lap(∆i/ϵi), to achieve ϵi-LDP in the embedded feature space (Dwork & Roth,
2014). In other words,
P (M(va,i)=z)
P (M(va|i,i)=z) ≤exp(
ϵi|E(va)−E(va|i)|"
REFERENCES,0.36737588652482267,"∆i
) where va|i is the vector that differs
from va only at the bit i and z ∈Range(M)."
REFERENCES,0.36879432624113473,"To ensure that the privacy loss in randomizing the bit i, we need to bound α in Eq. 2 such that
P (f-RR(va,i)=vz)
P (f-RR(va|i,i)=vz) ≤exp(
ϵi|E(va)−E(va|i)|"
REFERENCES,0.3702127659574468,"∆i
). However, randomizing a binary encoding bit i given
va and va|i results in a smaller l1-distance |E(f-RR(va, i)) −E(f-RR(va|i, i))| compared with
|E(va) −E(va|i)|; since |E(f-RR(va, i)) −E(f-RR(va|i, i))| ≤|E(va) −E(va|i)|. Thus, we can
derive a tighter bound by replacing |E(va) −E(va|i)| with |E(f-RR(va, i)) −E(f-RR(va|i, i))|.
Given va(i) is the bit i in va, we have"
REFERENCES,0.37163120567375885,"P(f-RR(va, i) = vz)
P(f-RR(va|i, i) = vz) = P(f-RR(va(i)) = vz(i))"
REFERENCES,0.3730496453900709,"P(f-RR(va|i(i)) = vz(i)) ×
Y"
REFERENCES,0.37446808510638296,"j̸=i,j∈[0,l−1]"
REFERENCES,0.375886524822695,"P(va(j) = vz(j))
P(va|i(j) = vz(j))"
REFERENCES,0.3773049645390071,= P(f-RR(va(i)) = vz(i))
REFERENCES,0.37872340425531914,"P(f-RR(va|i(i)) = vz(i)) ≤exp(ϵi|E(f-RR(va, i)) −E(f-RR(va|i, i))|"
REFERENCES,0.3801418439716312,"∆i
)
(10)"
REFERENCES,0.38156028368794326,"However, ﬁnding a closed-form solution of α for the tight privacy bound in Eq. 10 is non-trivial,
since ϵi is intractable. To address this problem, we consider two cases: (1)
P (f-RR(va(i))=vz(i))
P (f-RR(va|i(i))=vz(i)) ≥1"
REFERENCES,0.3829787234042553,"and (2) 0 <
P (f-RR(va(i))=vz(i))
P (f-RR(va|i(i))=vz(i)) < 1. Also, since ∆i captures the magnitude by which a bit i can"
REFERENCES,0.3843971631205674,"change the decoding function E() in the worst case, we have
∆i
|E(f-RR(va,i))−E(f-RR(va|i,i))| ≥1."
REFERENCES,0.38581560283687943,"In the ﬁrst case, the privacy loss for a bit i can be bounded as follows:"
REFERENCES,0.3872340425531915,"P(f-RR(va(i)) = vz(i))
P(f-RR(va|i(i)) = vz(i)) ≤
 P(f-RR(va(i)) = vz(i))"
REFERENCES,0.38865248226950355,P(f-RR(va|i(i)) = vz(i))
REFERENCES,0.3900709219858156,"
∆i
|E(f-RR(va,i))−E(f-RR(va|i,i))| ≤exp(ϵi)
(11)"
REFERENCES,0.39148936170212767,"Since the RR mechanism is independently applied on each bit i in va and on every va of the r features,
Eq. 11 enables us to quantify a generalized privacy loss bound of a RR mechanism, in which different
bits have different sensitivities to the randomized outcome as follows."
REFERENCES,0.39290780141843973,"P(f-RR(vx) = vz)
P(f-RR(evx) = vz) ="
REFERENCES,0.3943262411347518,"rl−1
Y i=0"
REFERENCES,0.39574468085106385,"P(f-RR(vx(i)) = vz(i))
P(f-RR(vx|i(i)) = vz(i)) ≤"
REFERENCES,0.3971631205673759,"rl−1
Y i=0"
REFERENCES,0.39858156028368796, P(f-RR(vx(i)) = vz(i))
REFERENCES,0.4,P(f-RR(vx|i(i)) = vz(i))
REFERENCES,0.4014184397163121,"
∆i
|E(f-RR(vx,i))−E(f-RR(vx|i,i))| ≤"
REFERENCES,0.40283687943262414,"rl−1
Y"
REFERENCES,0.40425531914893614,"i=0
exp(ϵi) = exp("
REFERENCES,0.4056737588652482,"rl−1
X"
REFERENCES,0.40709219858156026,"i=0
ϵi) (12)"
REFERENCES,0.4085106382978723,"In the second case, the privacy loss for a bit i can be bounded as follows:"
REFERENCES,0.4099290780141844,"P(f-RR(va(i)) = vz(i))
P(f-RR(va|i(i)) = vz(i)) ≤P(f-RR(va|i(i)) = vz(i))"
REFERENCES,0.41134751773049644,P(f-RR(va(i)) = vz(i))
REFERENCES,0.4127659574468085,"≤
P(f-RR(va|i(i)) = vz(i))"
REFERENCES,0.41418439716312055,P(f-RR(va(i)) = vz(i))
REFERENCES,0.4156028368794326,"
∆i
|E(f-RR(va,i))−E(f-RR(va|i,i))| ≤exp(ϵi) (13)"
REFERENCES,0.41702127659574467,"Similarly, we obtain the same result with Eq. 12 in the second case:"
REFERENCES,0.41843971631205673,"P(f-RR(vx) = vz)
P(f-RR(evx) = vz) ="
REFERENCES,0.4198581560283688,"rl−1
Y i=0"
REFERENCES,0.42127659574468085,"P(f-RR(vx(i)) = vz(i))
P(f-RR(vx|i(i)) = vz(i)) ≤"
REFERENCES,0.4226950354609929,"rl−1
Y i=0"
REFERENCES,0.42411347517730497,P(f-RR(vx|i(i)) = vz(i))
REFERENCES,0.425531914893617,P(f-RR(vx(i)) = vz(i)) ≤
REFERENCES,0.4269503546099291,"rl−1
Y i=0"
REFERENCES,0.42836879432624114,P(f-RR(vx|i(i)) = vz(i))
REFERENCES,0.4297872340425532,P(f-RR(vx(i)) = vz(i))
REFERENCES,0.43120567375886526,"
∆i
|E(f-RR(vx,i))−E(f-RR(vx|i,i))| ≤"
REFERENCES,0.4326241134751773,"rl−1
Y"
REFERENCES,0.4340425531914894,"i=0
exp(ϵi) = exp("
REFERENCES,0.43546099290780144,"rl−1
X"
REFERENCES,0.4368794326241135,"i=0
ϵi) (14)"
REFERENCES,0.43829787234042555,"Consequently, Theorem 1 holds."
REFERENCES,0.4397163120567376,Under review as a conference paper at ICLR 2022
REFERENCES,0.44113475177304967,"E
PROOF OF THEOREM 2"
REFERENCES,0.4425531914893617,"To ensure that the privacy loss in randomizing the bit i is bounded by the privacy loss in the em-
bedded space and to take into account different sensitivities of different bits (Theorem 1), we need
to ﬁnd α in Eq. 2 to solve Eq. 12. However, solving Eq. 12 is not straightforward since the
privacy budget ϵi and the actual l1-distance |E(f-RR(vx, i)) −E(f-RR(vx|i, i))| are intractable.
To address this problem, from Eq. 12, we ﬁrst consider the bit i in all the r embedded fea-
tures, as follows: Q"
REFERENCES,0.44397163120567373,"a∈[1,r]
P (f-RR(va(i))=vz(i))
P (f-RR(va|i(i))=vz(i)) ≤exp(P"
REFERENCES,0.4453900709219858,"a∈[1,r]
ϵi|E(f-RR(va,i))−E(f-RR(va|i,i))|"
REFERENCES,0.44680851063829785,"∆i
) =
exp( ϵi ∆i
P"
REFERENCES,0.4482269503546099,"a∈[1,r] |E(f-RR(va, i)) −E(f-RR(va|i, i))|).
Note that all the bits i, e.g., sign
bits, in all the features a
∈
[1, r] consume the same privacy budget ϵi with the same
sensitivity ∆i.
The term P"
REFERENCES,0.44964539007092197,"a∈[1,r] |E(f-RR(va, i)) −E(f-RR(va|i, i))| can be unbiasedly
replaced with r × E|E(f-RR(va, i)) −E(f-RR(va|i, i))| where E is the expectation of
|E(f-RR(va, i)) −E(f-RR(va|i, i))|, since limr→∞E|E(f-RR(va, i)) −E(f-RR(va|i, i))|
=
P"
REFERENCES,0.451063829787234,"a∈[1,r] |E(f-RR(va,i))−E(f-RR(va|i,i))|"
REFERENCES,0.4524822695035461,"r
. Hence, we have that Y"
REFERENCES,0.45390070921985815,"a∈[1,r]"
REFERENCES,0.4553191489361702,"P(f-RR(va(i)) = vz(i))
P(f-RR(va|i(i)) = vz(i)) ≤exp(rϵi × E|E(f-RR(va, i)) −E(f-RR(va|i, i))| ∆i
) ⇔
Y"
REFERENCES,0.45673758865248226,"a∈[1,r]"
REFERENCES,0.4581560283687943,  P(f-RR(va(i)) = vz(i))
REFERENCES,0.4595744680851064,"P(f-RR(va|i(i)) = vz(i))

∆i
E|E(f-RR(va,i))−E(f-RR(va|i,i))| ≤exp(rϵi)
(15)"
REFERENCES,0.46099290780141844,"Note that, in our work, we consider the worst case is the case that all the bits of two neighboring
vectors can be different. The expectation E|E(f-RR(va, i)) −E(f-RR(va|i, i))| in Eq. 15 is used to
quantify the difference of every two extreme vectors at bit i. Then for the whole vector, it is the sum
over the expectation E(·) of all bits i, as follows: P"
REFERENCES,0.4624113475177305,"a∈[1,r] |E(f-RR(va, i)) −E(f-RR(va|i, i))| =
r × E|E(f-RR(va, i)) −E(f-RR(va|i, i))|. Therefore, the expectation in Eq. 15 does not imply the
average-case scenario."
REFERENCES,0.46382978723404256,"The sum over the expectation E(·) of all bits i, i.e., P"
REFERENCES,0.4652482269503546,"a∈[1,r]
P"
REFERENCES,0.4666666666666667,"i∈[0,l−1] |E(f-RR(va, i)) −
E(f-RR(va|i, i))| = r × P"
REFERENCES,0.46808510638297873,"i∈[0,l−1] E|E(f-RR(va, i)) −E(f-RR(va|i, i))|, is used to bound the
privacy loss as follows:"
REFERENCES,0.4695035460992908,"P(f-RR(vx) = vz)
P(f-RR(evx) = vz) ≤
Y"
REFERENCES,0.47092198581560285,"a∈[1,r] Y"
REFERENCES,0.4723404255319149,"i∈[0,l−1]"
REFERENCES,0.47375886524822697,  P(f-RR(va(i)) = vz(i))
REFERENCES,0.475177304964539,"P(f-RR(va|i(i)) = vz(i))

∆i
E|E(f-RR(va,i))−E(f-RR(va|i,i))|"
REFERENCES,0.4765957446808511,"≤exp(
X"
REFERENCES,0.47801418439716314,"a∈[1,r] X"
REFERENCES,0.4794326241134752,"i∈[0,l−1]"
REFERENCES,0.4808510638297872,"ϵi|E(f-RR(va, i)) −E(f-RR(va|i, i))| ∆i
)"
REFERENCES,0.48226950354609927,"= exp(
X"
REFERENCES,0.4836879432624113,"i∈[0,l−1]"
REFERENCES,0.4851063829787234,"ϵi
 
r × E|E(f-RR(va, i)) −E(f-RR(va|i, i))|
 ∆i
)"
REFERENCES,0.48652482269503544,"≤exp(r
X"
REFERENCES,0.4879432624113475,"i∈[0,l−1]
ϵi) ⇔"
REFERENCES,0.48936170212765956,"rl−1
Y i=0"
REFERENCES,0.4907801418439716,  P(f-RR(vx(i)) = vz(i))
REFERENCES,0.4921985815602837,"P(f-RR(vx|i(i)) = vz(i))

∆i
E|E(f-RR(vx,i))−E(f-RR(vx|i,i))| ≤exp("
REFERENCES,0.49361702127659574,"rl−1
X"
REFERENCES,0.4950354609929078,"i=0
ϵi)
(16)"
REFERENCES,0.49645390070921985,"Now, we need to bound the generalized privacy loss by discovering closed-form solutions of α
given the privacy budgets ϵX (Theorem 2). In other words, we need to solve Eq. 16 for discov-
ering the closed-form solution of α. To solve it, ﬁrst we need to calculate E|E(f-RR(vx, i)) −
E(f-RR(vx|i, i))|. To be more precise, let us denote pXi and qXi as pX and qX in Eq. 2 for a
particular bit i, respectively. Given the worse case of vx and vx|i, there are four possible cases of
|E(f-RR(vx, i)) −E(f-RR(vx|i, i))|:"
REFERENCES,0.4978723404255319,"• If f-RR(vx(i)) = 1 and f-RR(vx|i(i)) = 1, then |E(f-RR(vx, i)) −E(f-RR(vx|i, i))| = 0."
REFERENCES,0.49929078014184397,Under review as a conference paper at ICLR 2022
REFERENCES,0.500709219858156,"• If f-RR(vx(i)) = 0 and f-RR(vx|i(i)) = 0, then |E(f-RR(vx, i)) −E(f-RR(vx|i, i))| = 0."
REFERENCES,0.502127659574468,"• If f-RR(vx(i)) = 1 and f-RR(vx|i(i)) = 0, then |E(f-RR(vx, i)) −E(f-RR(vx|i, i))| = ∆i.
This happens with the probability P(f-RR(vx(i)) = 1, f-RR(vx|i(i)) = 0). To compute this
probability, we use marginal probability and Bayes’ theorem, as follows:"
REFERENCES,0.5035460992907801,"P(f-RR(vx(i)) = 1, f-RR(vx|i(i)) = 0)"
REFERENCES,0.5049645390070922,"= P

f-RR(vx(i)) = 1, f-RR(vx|i(i)) = 0, vx(i) = 1, vx|i(i) = 0
"
REFERENCES,0.5063829787234042,"+ P

f-RR(vx(i)) = 1, f-RR(vx|i(i)) = 0, vx(i) = 0, vx|i(i) = 1
"
REFERENCES,0.5078014184397163,"= P

f-RR(vx(i)) = 1|f-RR(vx|i(i)) = 0, vx(i) = 1, vx|i(i) = 0
"
REFERENCES,0.5092198581560283,"× P(f-RR(vx|i(i)) = 0|vx(i) = 1, vx|i(i) = 0)"
REFERENCES,0.5106382978723404,× P(vx(i) = 1|vx|i(i) = 0) × P(vx|i(i) = 0)
REFERENCES,0.5120567375886524,"+ P

f-RR(vx(i)) = 1|f-RR(vx|i(i)) = 0, vx(i) = 0, vx|i(i) = 1
"
REFERENCES,0.5134751773049645,"× P(f-RR(vx|i(i)) = 0|vx(i) = 0, vx|i(i) = 1)"
REFERENCES,0.5148936170212766,× P(vx(i) = 0|vx|i(i) = 1) × P(vx|i(i) = 1)
REFERENCES,0.5163120567375886,"= P

f-RR(vx(i)) = 1|vx(i) = 1
"
REFERENCES,0.5177304964539007,× P(f-RR(vx|i(i)) = 0|vx|i(i) = 0) × P(vx|i(i) = 0)
REFERENCES,0.5191489361702127,"+ P

f-RR(vx(i)) = 1|vx(i) = 0
"
REFERENCES,0.5205673758865248,× P(f-RR(vx|i(i)) = 0|vx|i(i) = 1) × P(vx|i(i) = 1)
REFERENCES,0.5219858156028369,"= p2
XiP(vx|i(i) = 0) + q2
XiP(vx|i(i) = 1)
(17)"
REFERENCES,0.5234042553191489,"• If f-RR(vx(i)) = 0 and f-RR(vx|i(i)) = 1, then |E(f-RR(vx(i))) −E(f-RR(vx|i(i)))| = ∆i.
This happens with the probability P(f-RR(vx(i)) = 0, f-RR(vx|i(i)) = 1). To compute this
probability, we use marginal probability and Bayes’ theorem, as follows:"
REFERENCES,0.524822695035461,"P(f-RR(vx(i)) = 0, f-RR(vx|i(i)) = 1)"
REFERENCES,0.526241134751773,"= P

f-RR(vx(i)) = 0, f-RR(vx|i(i)) = 1, vx(i) = 1, vx|i(i) = 0
"
REFERENCES,0.5276595744680851,"+ P

f-RR(vx(i)) = 0, f-RR(vx|i(i)) = 1, vx(i) = 0, vx|i(i) = 1
"
REFERENCES,0.5290780141843971,"= q2
XiP(vx|i(i) = 0) + p2
XiP(vx|i(i) = 1)
(18)"
REFERENCES,0.5304964539007092,"Consequently, the expectation E|E(f-RR(vx, i)) −E(f-RR(vx|i, i))| is computed as follows:"
REFERENCES,0.5319148936170213,"E|E(f-RR(vx, i)) −E(f-RR(vx|i, i))|"
REFERENCES,0.5333333333333333,"=

p2
XiP(vx|i(i) = 0) + q2
XiP(vx|i(i) = 1)

∆i"
REFERENCES,0.5347517730496454,"+

q2
XiP(vx|i(i) = 0) + p2
XiP(vx|i(i) = 1)

∆i"
REFERENCES,0.5361702127659574,"= (p2
Xi + q2
Xi)∆i
(19)"
REFERENCES,0.5375886524822695,"From Eqs. 12, 16, and 19, we have that"
REFERENCES,0.5390070921985816,Under review as a conference paper at ICLR 2022
REFERENCES,0.5404255319148936,"P(f-RR(vx) = vz)
P(f-RR(evx) = vz) ≤"
REFERENCES,0.5418439716312057,"rl−1
Y i=0"
REFERENCES,0.5432624113475177, P(f-RR(vx(i)) = vz(i))
REFERENCES,0.5446808510638298,P(f-RR(vx|i(i)) = vz(i))
REFERENCES,0.5460992907801419,"
∆i
E|E(f-RR(vx,i))−E(f-RR(vx|i,i))| ="
REFERENCES,0.5475177304964539,"rl−1
Y i=0"
REFERENCES,0.548936170212766,P(f-RR(vx(i)) = 0|vx(i) = 1)P(f-RR(vx(i)) = 1|vx(i) = 0)
REFERENCES,0.550354609929078,P(f-RR(vx(i)) = 1|vx(i) = 1)P(f-RR(vx(i)) = 0|vx(i) = 0)
REFERENCES,0.5517730496453901,"
1
p2
Xi+q2
Xi = l−1
Y i=0"
REFERENCES,0.5531914893617021,"
α2 exp(2ϵX
i
l )

r
p2
Xi+(1−pXi)2
(20)"
REFERENCES,0.5546099290780142,Taking the natural logarithm of two sides of Eq. 20:
REFERENCES,0.5560283687943263,ln P(f-RR(vx) = vz)
REFERENCES,0.5574468085106383,"P(f-RR(evx) = vz) ≤ l−1
X"
REFERENCES,0.5588652482269504,"i=0
ln

α2 exp(2ϵX
i
l )

r
p2
Xi+(1−pXi)2 = l−1
X i=0"
REFERENCES,0.5602836879432624,"
r
p2
Xi + (1 −pXi)2 ln
 
α2 exp(2ϵX
i
l )

(21)"
REFERENCES,0.5617021276595745,Let us bound the summation in Eq. 21 using the following inequality:
REFERENCES,0.5631205673758866,"ln(a) ≤a −1 for a > 0
(22)"
REFERENCES,0.5645390070921986,"The proof of Eq. 22 is as follows. Let a > 0, we deﬁne h(a) = ln(a) −a + 1. We have:
h′(a) = 1"
REFERENCES,0.5659574468085107,"a −1 = 0 ⇔a = 1, and since h′′(a) = −1"
REFERENCES,0.5673758865248227,"a2 < 0, ∀a > 0, we get the maximal point
at a = 1. We also have: lima→0+ h(a) = −∞= lima→∞h(a). Therefore, a = 1 is the global
maximal point and than ∀a > 0, h(a) ≤h(a = 1) = 0, so ln(a) −a + 1 ≤0. Therefore, Eq. 22
does hold."
REFERENCES,0.5687943262411348,"Note that, to simultaneously satisfy the randomization probabilities pX =
1
1+α exp( i%l"
REFERENCES,0.5702127659574469,l ϵX) ≥0
REFERENCES,0.5716312056737589,"and qX =
α exp( i%l"
REFERENCES,0.573049645390071,"l ϵX)
1+α exp( i%l"
REFERENCES,0.574468085106383,"l ϵX) ≥0 in Eq. 2, we need to have: (i) 1 + α exp( i%l"
REFERENCES,0.5758865248226951,l ϵX) ≥0 and (ii)
REFERENCES,0.577304964539007,α exp( i%l
REFERENCES,0.5787234042553191,l ϵX) ≥0. Since exp( i%l
REFERENCES,0.5801418439716312,"l ϵX) ≥0 is always true, from (i), α ≥−exp(−i%l"
REFERENCES,0.5815602836879432,"l ϵX), and from
(ii), α ≥0. Therefore, α ≥0 is necessary to satisfy the condition pX ≥0 and qX ≥0. To apply
Eq. 22 into Eq. 21, we need to have a = α2 exp(2ϵX i"
REFERENCES,0.5829787234042553,"l) > 0 ⇒α ̸= 0. As a result, we have that"
REFERENCES,0.5843971631205673,"α > 0
(23)"
REFERENCES,0.5858156028368794,Applying Eq. 22 into Eq. 21 where a = α2 exp(2ϵX i l):
REFERENCES,0.5872340425531914,ln P(f-RR(vx) = vz)
REFERENCES,0.5886524822695035,"P(f-RR(evx) = vz) ≤ l−1
X i=0"
REFERENCES,0.5900709219858156,"
r
p2
Xi + (1 −pXi)2 ln
 
α2 exp(2ϵX
i
l )
 ≤ l−1
X i=0"
REFERENCES,0.5914893617021276,rα2 exp(2ϵX i
REFERENCES,0.5929078014184397,"l)
p2
Xi + (1 −pXi)2 − l−1
X i=0"
REFERENCES,0.5943262411347517,"r
p2
Xi + (1 −pXi)2
(24)"
REFERENCES,0.5957446808510638,"To bound the logarithm in Eq. 24, we use: p2
Xi + q2
Xi = p2
Xi + (1 −pXi)2 =

1
1+α exp( i%l l
ϵX) 2
+"
REFERENCES,0.5971631205673759,"
α exp( i%l l
ϵX)"
REFERENCES,0.5985815602836879,"1+α exp( i%l l
ϵX)"
REFERENCES,0.6,"2
=
1+
"
REFERENCES,0.601418439716312,α exp( i%l
REFERENCES,0.6028368794326241,"l
ϵX)
2 "
REFERENCES,0.6042553191489362,1+α exp( i%l
REFERENCES,0.6056737588652482,"l
ϵX)
2 ≤1, and p2
Xi + (1 −pXi)2 ≥
(pXi+1−pXi)2"
REFERENCES,0.6070921985815603,"2
=
1
2 (In fact,"
REFERENCES,0.6085106382978723,Under review as a conference paper at ICLR 2022
REFERENCES,0.6099290780141844,"∀a, b : a2 + b2 ≥(a+b)2"
REFERENCES,0.6113475177304964,"2
⇔(a −b)2 ≥0, which is true). Note that, from Eq. 19, we have that
∆i
E|E(f-RR(va,i))−E(f-RR(va|i,i))| =
1
p2
Xi+q2
Xi ≥1."
REFERENCES,0.6127659574468085,"Applying these inequalities in Eq. 24, we obtain:"
REFERENCES,0.6141843971631206,ln P(f-RR(vx) = vz)
REFERENCES,0.6156028368794326,"P(f-RR(evx) = vz) ≤ l−1
X i=0"
REFERENCES,0.6170212765957447,rα2 exp(2ϵX i
REFERENCES,0.6184397163120567,"l)
p2
Xi + (1 −pXi)2 − l−1
X i=0"
REFERENCES,0.6198581560283688,"r
p2
Xi + (1 −pXi)2 < l−1
X"
REFERENCES,0.6212765957446809,"i=0
2rα2 exp(2ϵX
i
l ) − l−1
X"
REFERENCES,0.6226950354609929,"i=0
r = l−1
X"
REFERENCES,0.624113475177305,"i=0
2rα2 exp(2ϵX
i
l ) −rl ≤ϵX
(25)"
REFERENCES,0.625531914893617,"By solving Eq. 25, we have that"
REFERENCES,0.6269503546099291,"α2 ≤
ϵX + rl"
REFERENCES,0.6283687943262412,"2r Pl−1
i=0 exp(2ϵX i"
REFERENCES,0.6297872340425532,"l)
⇔|α| ≤ s"
REFERENCES,0.6312056737588653,ϵX + rl
REFERENCES,0.6326241134751773,"2r Pl−1
i=0 exp(2ϵX i"
REFERENCES,0.6340425531914894,"l)
(26)"
REFERENCES,0.6354609929078014,"Therefore, from Eqs. 23 and 26, we have that ∀α : 0 < α ≤
q"
REFERENCES,0.6368794326241135,"ϵX+rl
2r Pl−1
i=0 exp(2ϵX i"
REFERENCES,0.6382978723404256,"l ), the f-RR"
REFERENCES,0.6397163120567376,"mechanism satisﬁes ϵX-LDP. Consequently, Theorem 2 holds."
REFERENCES,0.6411347517730497,"F
PROOF OF THEOREM 3"
REFERENCES,0.6425531914893617,"Proof. We have:
P (label-RR(yx)=z|yx)
P (label-RR(eyx)=z|eyx) ≤
max P (label-RR(yx)=z|yx)"
REFERENCES,0.6439716312056738,min P (label-RR(eyx)=z|eyx) =
REFERENCES,0.6453900709219859,"exp(β)
1+exp(β)"
REFERENCES,0.6468085106382979,"1
(1+exp(β))(C−1) = exp(β +"
REFERENCES,0.64822695035461,"ln(C −1)) ≤exp(ϵY ) ⇔β ≤ϵY −ln(C −1). Consequently, Theorem 3 does hold."
REFERENCES,0.649645390070922,"G
label-RR AND LABELDP COMPARISON"
REFERENCES,0.6510638297872341,"Although our label-RR is inspired by the randomizing probability (Eq. 1, (Ghazi et al., 2021)) in the
LabelDP showcased by (Ghazi et al., 2021), there are two major differences between our label-RR
and LabelDP discussed next."
REFERENCES,0.6524822695035462,"(1) We combine f-RR and label-RR to completely protect a data sample. As pointed out in (Busa-
Fekete et al., 2021), only protecting the label as in the LabelDP offers a weaker privacy protection
than it appears, as the features are sufﬁciently predictive of the label, obscuring the label is not
enough, as a classiﬁer can still be trained on such noisy data; hence, a user experiences privacy loss
due to both the public release of the features and the private release of the label."
REFERENCES,0.6539007092198581,"(2) The RRWithPrior algorithm that is used to guarantee LabelDP requiring publicly available priors
and the multi-stage training (LP-MST) illustrating RRWithPrior algorithm cannot be straightforwardly
applied to federated learning. In LP-MST algorithm, the dataset is partitioned into subsets, then based
on the prior probability to randomize the label and add the data with that randomized label to the
training data. These steps are presently applied on centralized training and it has not been show how
to be effectively applied in federated learning."
REFERENCES,0.6553191489361702,"Using the upper bound of β (Theorem 3) results in the same randomizing probabilities between label-
RR and LabelDP. For instance, in Eq. 3, pY =
exp(β)
1+exp(β) =
exp(ϵY −ln(C−1))
1+exp(ϵY −ln(C−1)) =
exp(ϵY )
C−1+exp(ϵY ) and
qY =
1
(1+exp(β))(C−1) =
1
C−1+exp(ϵY ), which is equivalent to Eq. 1 (Ghazi et al., 2021). Therefore,
we did not include LabelDP (Ghazi et al., 2021) in comparison."
REFERENCES,0.6567375886524822,"H
PROOF OF THEOREM 4"
REFERENCES,0.6581560283687943,"Proof. We have ξa
= E|E(f-RR(va)) −E(va)| = P"
REFERENCES,0.6595744680851063,"i∈[0,l−1](pXi × 0 + qXi × ∆i) =
P"
REFERENCES,0.6609929078014184,"i∈[0,l−1] qXi × ∆i. Therefore, Theorem 4 hold."
REFERENCES,0.6624113475177305,Under review as a conference paper at ICLR 2022
REFERENCES,0.6638297872340425,"I
CORRECTED PRIVACY BUDGET BOUNDS IN LATENT (ARACHCHIGE ET AL., 2019)"
REFERENCES,0.6652482269503546,"In this section, we aim at providing corrected privacy budget bounds for LATENT (Arachchige et al.,
2019). LATENT ﬁrst encodes embedded features ex into an rl-bit binary vector vx. Then, each bit
i ∈[0, rl −1] is randomized by a RR mechanism (i.e., the MOUE algorithm for high sensitivities in
Theorem 3.3 (Arachchige et al., 2019)), denoted f-LT, as follows:"
REFERENCES,0.6666666666666666,"∀i ∈[0, rl −1] : P(v′
x(i) = 1) ="
REFERENCES,0.6680851063829787,"


 

"
REFERENCES,0.6695035460992907,"pX =
1
1 + α,
if vx(i) = 1"
REFERENCES,0.6709219858156028,"qX =
1
1 + α exp( ϵX"
REFERENCES,0.6723404255319149,"rl ),
if vx(i) = 0
(27)"
REFERENCES,0.6737588652482269,"From Eq. 27, we also have that P(v′
x(i) = 0) = 1 −pX =
α
1+α if vx(i) = 1, and P(v′
x(i) = 0) ="
REFERENCES,0.675177304964539,"1 −qX =
α exp( ϵX"
REFERENCES,0.676595744680851,"rl )
1+α exp( ϵX"
REFERENCES,0.6780141843971631,rl ) if vx(i) = 0.
REFERENCES,0.6794326241134752,"Theorem 5. LATENT with the randomization probabilities as in Eq. 27 preserves ϵcorrected-LDP,
where ϵcorrected = (1+α)(1+α exp( ϵX"
REFERENCES,0.6808510638297872,"rl ))
α(1+exp( ϵX"
REFERENCES,0.6822695035460993,"rl ))
ϵX."
REFERENCES,0.6836879432624113,"Proof. Similar to the analysis in Appendix E, we obtain the following inequality:"
REFERENCES,0.6851063829787234,"P(f-LT(vx) = vz)
P(f-LT(evx) = vz) ≤"
REFERENCES,0.6865248226950355,"rl−1
Y i=0"
REFERENCES,0.6879432624113475, P(f-LT(vx(i)) = vz(i))
REFERENCES,0.6893617021276596,P(f-LT(vx|i(i)) = vz(i))
REFERENCES,0.6907801418439716,"
∆i
E|E(f-LT(vx,i))−E(f-LT(vx|i,i))| ≤exp(ϵX) (28)"
REFERENCES,0.6921985815602837,"and the expectation E|E(f-LT(vx, i)) −E(f-LT(vx|i, i))| is computed as follows:"
REFERENCES,0.6936170212765957,"E|E(f-LT(vx, i)) −E(f-LT(vx|i, i))|"
REFERENCES,0.6950354609929078,"=

P(f-LT(vx, i) = 1|vx(i) = 1)"
REFERENCES,0.6964539007092199,"× P(f-LT(vx|i, i) = 0|vx|i(i) = 0) × P(vx|i(i) = 0)"
REFERENCES,0.6978723404255319,"+ P(f-LT(vx, i) = 1|vx(i) = 0)"
REFERENCES,0.699290780141844,"× P(f-LT(vx|i, i) = 0|vx|i(i) = 1) × P(vx|i(i) = 1)

∆i"
REFERENCES,0.700709219858156,"+

P(f-LT(vx, i) = 0|vx(i) = 1)"
REFERENCES,0.7021276595744681,"× P(f-LT(vx|i, i) = 1|vx|i(i) = 0) × P(vx|i(i) = 0)"
REFERENCES,0.7035460992907802,"+ P(f-LT(vx, i) = 0|vx(i) = 0)"
REFERENCES,0.7049645390070922,"× P(f-LT(vx|i, i) = 1|vx|i(i) = 1) × P(vx|i(i) = 1)

∆i"
REFERENCES,0.7063829787234043,"=

pXi(1 −qXi)P(vx|i(i) = 0) + qXi(1 −pXi)P(vx|i(i) = 1)"
REFERENCES,0.7078014184397163,"+ (1 −pXi)qXiP(vx|i(i) = 0) + (1 −qXi)pXiP(vx|i(i) = 1)

∆i"
REFERENCES,0.7092198581560284,"=

pXi(1 −qXi) + qXi(1 −pXi)

∆i
(29)"
REFERENCES,0.7106382978723405,"Furthermore, we have:"
REFERENCES,0.7120567375886525,"pXi(1 −qXi) + qXi(1 −pXi) =
α(1 + exp( ϵX"
REFERENCES,0.7134751773049646,"rl ))
(1 + α)(1 + α exp( ϵX"
REFERENCES,0.7148936170212766,"rl ))
(30)"
REFERENCES,0.7163120567375887,Under review as a conference paper at ICLR 2022
REFERENCES,0.7177304964539007,"From Eqs. 28-30, we have that"
REFERENCES,0.7191489361702128,"P(f-LT(vx) = vz)
P(f-LT(evx) = vz) ≤"
REFERENCES,0.7205673758865249,"rl−1
Y i=0"
REFERENCES,0.7219858156028369, P(f-LT(vx(i)) = vz(i))
REFERENCES,0.723404255319149,P(f-LT(vx|i(i)) = vz(i))
REFERENCES,0.724822695035461,"
∆i
E|E(f-LT(vx,i))−E(f-LT(vx|i,i))| ="
REFERENCES,0.7262411347517731,"rl−1
Y i=0"
REFERENCES,0.7276595744680852,P(f-LT(vx(i)) = 1|vx(i) = 1)
REFERENCES,0.7290780141843972,P(f-LT(vx(i)) = 0|vx(i) = 1)
REFERENCES,0.7304964539007093,"
∆i
(pXi(1−qXi)+qXi(1−pXi))∆i ×"
REFERENCES,0.7319148936170212,"rl−1
Y i=0"
REFERENCES,0.7333333333333333,P(f-LT(vx(i)) = 0|vx(i) = 0)
REFERENCES,0.7347517730496453,P(f-LT(vx(i)) = 1|vx(i) = 0)
REFERENCES,0.7361702127659574,"
∆i
(pXi(1−qXi)+qXi(1−pXi))∆i ="
REFERENCES,0.7375886524822695,"rl−1
Y i=0"
REFERENCES,0.7390070921985815,"
exp(ϵX"
REFERENCES,0.7404255319148936,"rl )

1
pXi(1−qXi)+qXi(1−pXi)
(31)"
REFERENCES,0.7418439716312056,"Then, from Eq. 31, we have:"
REFERENCES,0.7432624113475177,"ϵcorrected = ln

Πrl−1
i=0
 
exp(ϵX"
REFERENCES,0.7446808510638298,"rl )

1
pXi(1−qXi)+qXi(1−pXi) 
= (1 + α)(1 + α exp( ϵX"
REFERENCES,0.7460992907801418,"rl ))
α(1 + exp( ϵX"
REFERENCES,0.7475177304964539,"rl ))
ϵX
(32)"
REFERENCES,0.7489361702127659,"Consequently, Theorem 5 holds."
REFERENCES,0.750354609929078,"From Theorem 5, we show the proportion ϵcorrected/ϵX as a function of r in Figure 9a and as a
function of l in Figure 9b. Following the experiment settings in LATENT (Arachchige et al., 2019),
with the commonly used α = 7, when changing r ∈{10, 100, 1, 000, 10, 000} with a ﬁxed l = 10
(Figure 9a), or when changing l ∈{5, 10, 20, 100, 1, 000} with a ﬁxed r = 1, 000 (Figure 9b) and
under a tight privacy budget ϵX = 0.1, the proportion ϵcorrected/ϵX moderately changes among
[4.57, 4.75]. In other words, the ϵcorrected is remarkably larger than ϵX, for most r and l values
in practice. Unlike LATENT, our mechanism does not suffer from this problem, i.e., in BitRand,
ϵcorrected/ϵX = 1, thanks to our bit-aware randomization probabilities for LDP in binary encoding
(Theorem 2)."
REFERENCES,0.75177304964539,"J
CORRECTED PRIVACY BUDGET BOUNDS IN OME (LYU ET AL., 2020A)"
REFERENCES,0.7531914893617021,"In this section, we aim at providing corrected privacy budget bounds for OME. OME ﬁrst encodes
embedded features ex into an rl-bit binary vector vx. Then, each bit i ∈[0, rl −1] is randomized by
the following f-OME mechanism:"
REFERENCES,0.7546099290780142,"∀i ∈[0, rl −1] : P(v′
x(i) = 1) ="
REFERENCES,0.7560283687943262,"





"
REFERENCES,0.7574468085106383,"




"
REFERENCES,0.7588652482269503,"p1X =
α
1 + α,
if i ∈2j, vx(i) = 1"
REFERENCES,0.7602836879432624,"p2X =
1
1 + α3 ,
if i ∈2j + 1, vx(i) = 1"
REFERENCES,0.7617021276595745,"qX =
1
1 + α exp( ϵX"
REFERENCES,0.7631205673758865,"rl ),
if vx(i) = 0 (33)"
REFERENCES,0.7645390070921986,"From Eq. 33, we also have that P(v′
x(i) = 0) = 1−p1X =
1
1+α if vx(i) = 1 and i ∈2j, P(v′
x(i) ="
REFERENCES,0.7659574468085106,"0) = 1 −p2X =
α3
1+α3 if vx(i) = 1 and i ∈2j + 1, and P(v′
x(i) = 0) = 1 −qX =
α exp( ϵX"
REFERENCES,0.7673758865248227,"rl )
1+α exp( ϵX"
REFERENCES,0.7687943262411348,rl ) if
REFERENCES,0.7702127659574468,"vx(i) = 0.
Theorem 6. OME with the randomization probabilities as in Eq. 33 preserves ϵcorrected-LDP, where
ϵcorrected = ( rl"
REFERENCES,0.7716312056737589,Q1 −rl
REFERENCES,0.7730496453900709,"Q2 ) ln(α) +
ϵX
2Q1 +
ϵX
2Q2 in which Q1 =
α
1+α
α exp( ϵX"
REFERENCES,0.774468085106383,"rl )
1+α exp( ϵX"
REFERENCES,0.775886524822695,"rl ) +
1
1+α exp( ϵX"
REFERENCES,0.7773049645390071,"rl )
1
1+α"
REFERENCES,0.7787234042553192,"and Q2 =
1
1+α3
α exp( ϵX"
REFERENCES,0.7801418439716312,"rl )
1+α exp( ϵX"
REFERENCES,0.7815602836879433,"rl ) +
1
1+α exp( ϵX"
REFERENCES,0.7829787234042553,"rl )
α3
1+α3 ."
REFERENCES,0.7843971631205674,"Proof. Similar to the analysis in Appendix E and Appendix I, we obtain:"
REFERENCES,0.7858156028368795,Under review as a conference paper at ICLR 2022
REFERENCES,0.7872340425531915,"(a) Fixed ϵX = 0.1, l = 10
(b) Fixed ϵX = 0.1, r = 1, 000"
REFERENCES,0.7886524822695036,"Figure 9: Impacts of r and l on ϵCorrected/ϵX in LATENT (Arachchige et al., 2019) and OME (Lyu
et al., 2020a)."
REFERENCES,0.7900709219858156,"P(f-OME(vx) = vz)
P(f-OME(evx) = vz) ≤"
REFERENCES,0.7914893617021277,"rl−1
Y i=0"
REFERENCES,0.7929078014184398, P(f-OME(vx(i)) = vz(i))
REFERENCES,0.7943262411347518,P(f-OME(vx|i(i)) = vz(i))
REFERENCES,0.7957446808510639,"
∆i
E|E(f-OME(vx,i))−E(f-OME(vx|i,i))| ≤exp(ϵX) (34)"
REFERENCES,0.7971631205673759,"and the expectation E|E(f-OME(vx, i)) −E(f-OME(vx|i, i))| is computed as follows:"
REFERENCES,0.798581560283688,"E|E(f-OME(vx, i)) −E(f-OME(vx|i, i))| ="
REFERENCES,0.8,"( 
p1Xi(1 −qXi) + qXi(1 −p1Xi)

∆i = Q1∆i, if i ∈2j
 
p2Xi(1 −qXi) + qXi(1 −p2Xi)

∆i = Q2∆i, if i ∈2j + 1
(35)"
REFERENCES,0.8014184397163121,"where Q1 = p1Xi(1 −qXi) + qXi(1 −p1Xi) =
α
1+α
α exp( ϵX"
REFERENCES,0.8028368794326242,"rl )
1+α exp( ϵX"
REFERENCES,0.8042553191489362,"rl ) +
1
1+α exp( ϵX"
REFERENCES,0.8056737588652483,"rl )
1
1+α,"
REFERENCES,0.8070921985815603,"and Q2 = p2Xi(1 −qXi) + qXi(1 −p2Xi) =
1
1+α3
α exp( ϵX"
REFERENCES,0.8085106382978723,"rl )
1+α exp( ϵX"
REFERENCES,0.8099290780141843,"rl ) +
1
1+α exp( ϵX"
REFERENCES,0.8113475177304964,"rl )
α3
1+α3 ."
REFERENCES,0.8127659574468085,"From Eqs. 34 and 35, we have:"
REFERENCES,0.8141843971631205,"P(f-OME(vx) = vz)
P(f-OME(evx) = vz) ≤"
REFERENCES,0.8156028368794326,"rl−1
Y i=0"
REFERENCES,0.8170212765957446, P(f-OME(vx(i)) = vz(i))
REFERENCES,0.8184397163120567,P(f-OME(vx|i(i)) = vz(i))
REFERENCES,0.8198581560283688,"
∆i
E|E(f-OME(vx,i))−E(f-OME(vx|i,i))| =
Y i∈2j"
REFERENCES,0.8212765957446808,P(f-OME(vx(i)) = 1|vx(i) = 1)P(f-OME(vx(i)) = 0|vx(i) = 0)
REFERENCES,0.8226950354609929,P(f-OME(vx(i)) = 1|vx(i) = 0)P(f-OME(vx(i)) = 0|vx(i) = 1)
REFERENCES,0.8241134751773049,"
∆i
Q1∆i ×
Y"
REFERENCES,0.825531914893617,i∈2j+1
REFERENCES,0.826950354609929,P(f-OME(vx(i)) = 1|vx(i) = 1)P(f-OME(vx(i)) = 0|vx(i) = 0)
REFERENCES,0.8283687943262411,P(f-OME(vx(i)) = 1|vx(i) = 0)P(f-OME(vx(i)) = 0|vx(i) = 1)
REFERENCES,0.8297872340425532,"
∆i
Q2∆i = α"
REFERENCES,0.8312056737588652,"rl
Q1 −rl"
REFERENCES,0.8326241134751773,Q2 exp( ϵX
REFERENCES,0.8340425531914893,"2Q1
+ ϵX"
REFERENCES,0.8354609929078014,"2Q2
)
(36)"
REFERENCES,0.8368794326241135,"Then, from Eq. 36, we have:"
REFERENCES,0.8382978723404255,"ϵcorrected = ln

α"
REFERENCES,0.8397163120567376,"rl
Q1 −rl"
REFERENCES,0.8411347517730496,Q2 exp( ϵX
REFERENCES,0.8425531914893617,"2Q1
+ ϵX"
REFERENCES,0.8439716312056738,"2Q2
)

= ( rl"
REFERENCES,0.8453900709219858,"Q1
−rl"
REFERENCES,0.8468085106382979,"Q2
) ln(α) + ϵX"
REFERENCES,0.8482269503546099,"2Q1
+ ϵX"
REFERENCES,0.849645390070922,"2Q2
(37)"
REFERENCES,0.851063829787234,"Consequently, Theorem 6 does hold."
REFERENCES,0.8524822695035461,Under review as a conference paper at ICLR 2022
REFERENCES,0.8539007092198582,"From Theorem 6, we show the proportion ϵcorrected/ϵX as a function of r in Figure 9a and as a
function of l in Figure 9b. Following the experiment settings in OME (Lyu et al., 2020a), with
the commonly used α = 100, when changing r ∈{10, 100, 1, 000, 10, 000} with a ﬁxed l = 10
(Figure 9a), or when changing l ∈{5, 10, 20, 100, 1, 000} with a ﬁxed r = 1, 000 (Figure 9b) and
under a tight privacy budget ϵX = 0.1, the proportion ϵcorrected/ϵX signiﬁcantly changes among
[4.6e + 6, 4.6e + 9]. In other words, the ϵcorrected is extremely larger than ϵX, for most r and l
values in practice. Since α = 100 causes the extreme privacy exaggeration, in our experiment, to
compare with OME, we use α = 1. This value is used in OME (Lyu et al., 2020a) and generates
ϵcorrected/ϵX ≈2, which offers a reasonable range to apply OME in practice. Unlike OME, our
mechanism does not suffer from this problem, i.e., in BitRand, ϵcorrected/ϵX = 1, thanks to our
bit-aware randomization probabilities for LDP in binary encoding (Theorem 2)."
REFERENCES,0.8553191489361702,"K
SUPPLEMENTARY THEORETICAL RESULTS"
REFERENCES,0.8567375886524823,"Figure 10: RMSE error comparison as a func-
tion of ϵX."
REFERENCES,0.8581560283687943,"Setting for Gaussian and Laplace mechanisms.
The Gaussian and Laplace mechanisms naturally ap-
ply an addition operation, which add noise into the
data or embedded features. Therefore, in our anal-
ysis of expected error bound comparison for an em-
bedded feature (Figure 3), we add noise into the em-
bedded feature following the Gaussian and Laplace
mechanisms. The sensitivity captures the magnitude
by which an embedded feature can change in the
worst case. In our experiment and analysis, we use l = 10 bits in which 1 sign bit, 5 bits for the
integer, and 4 bits for the fraction part. Therefore, the maximum the embedded feature can be change,
i.e., the sensitivity, is 2 P4
i=−4 2i. Note that, we multiply P4
i=−4 2i by 2 since when we ﬂip the sign
bit, it signiﬁcantly changes the value of the embedded feature from −a to a in which a = P4
i=−4 2i."
REFERENCES,0.8595744680851064,"RMSE error comparison in mean estimation. To investigate how our proposed approach f-RR
works with statistical query, we study our f-RR and other baselines with a mean estimation. We
created a synthetic data that consists of N = 1, 000 data samples {xi}N
i=1, each of them has d = 768
dimensions. The mean estimation is calculated over each dimension as fj(D) =
1
N
PN
i=1 xij for
j ∈[1, d]. Root mean square error (RMSE) is used to evaluate the error between the original vector
and the randomized/estimated vector. The binary-encoding-based approaches (i.e., f-RR, corrected
LATENT, and corrected OME) achieve a signiﬁcantly small error compared with others. As can
be seen in Figure 10, f-RR obtains the smallest error, which further shows the effectiveness of our
proposed mechanism."
REFERENCES,0.8609929078014185,Figure 11: Expected error bound as a function of ϵX with ﬁxed r and l.
REFERENCES,0.8624113475177305,"(a) ϵX = 0.1
(b) ϵX = 1.0
(c) ϵX = 2.0"
REFERENCES,0.8638297872340426,"Figure 12: Randomization probability qX and qtop-k, given l = 10 and r = 1, 000."
REFERENCES,0.8652482269503546,Under review as a conference paper at ICLR 2022
REFERENCES,0.8666666666666667,"(a) l = 5 and ϵX = 0.1
(b) l = 5 and ϵX = 1.0
(c) l = 5 and ϵX = 2.0"
REFERENCES,0.8680851063829788,"(d) l = 20 and ϵX = 0.1
(e) l = 20 and ϵX = 1.0
(f) l = 20 and ϵX = 2.0"
REFERENCES,0.8695035460992908,"(g) l = 100 and ϵX = 0.1
(h) l = 100 and ϵX = 1.0
(i) l = 100 and ϵX = 2.0"
REFERENCES,0.8709219858156029,"(j) l = 1, 000 and ϵX = 0.1
(k) l = 1, 000 and ϵX = 1.0
(l) l = 1, 000 and ϵX = 2.0"
REFERENCES,0.8723404255319149,Figure 13: Randomization probability q (p = 1 −q) as a function of r with ﬁxed l and ϵ.
REFERENCES,0.873758865248227,Under review as a conference paper at ICLR 2022
REFERENCES,0.875177304964539,"(a) r = 10 and ϵX = 0.1
(b) r = 10 and ϵX = 1.0
(c) r = 10 and ϵX = 2.0"
REFERENCES,0.8765957446808511,"(d) r = 100 and ϵX = 0.1
(e) r = 100 and ϵX = 1.0
(f) r = 100 and ϵX = 2.0"
REFERENCES,0.8780141843971632,"(g) r = 1, 000 and ϵX = 0.1
(h) r = 1, 000 and ϵX = 1.0
(i) r = 1, 000 and ϵX = 2.0"
REFERENCES,0.8794326241134752,"(j) r = 10, 000 and ϵX = 0.1
(k) r = 10, 000 and ϵX = 1.0
(l) r = 10, 000 and ϵX = 2.0"
REFERENCES,0.8808510638297873,Figure 14: Randomization probability qX (pX = 1 −qX) as a function of l with ﬁxed r and ϵ.
REFERENCES,0.8822695035460993,Under review as a conference paper at ICLR 2022
REFERENCES,0.8836879432624114,"L
SUPPLEMENTARY EXPERIMENTAL RESULTS"
REFERENCES,0.8851063829787233,"Datasets and Data Processing. We carried out our experiments on two textual datasets and two
image datasets, including the AG dataset (Gulli et al., 2012), our collected Security and Exchange
Commission (SEC) ﬁnancial contract dataset, the large-scale celebFaces attributes (CelebA) dataset
(Liu et al., 2015), and the Federated Extended MNIST (FEMNIST) dataset (Caldas et al., 2018). The
AG dataset is a collection of news articles gathered from more than 2, 000 news sources by (Com).
It is categorized into four classes: world, sport, business, and science/technology classes. Our SEC
dataset consists of over 1, 000 contract clauses collected from contracts submitted in SEC ﬁlings1.
The CelebA dataset consists of more than 200, 000 celebrity images, each with 40 attributes, e.g.,
attractive face, big lips, big noses, black hair, etc., which are used as binary classes. The FEMNIST
dataset is built by partitioning the images in Extended MNIST (Cohen et al., 2017) based on the
writer of the handwritten digits and characters. For data preprocessing, we changed all words in the
AG and SEC datasets to lower-case and removed punctuation marks. The breakdown of the datasets
is in Table 1."
REFERENCES,0.8865248226950354,Table 1: Dataset breakdown.
REFERENCES,0.8879432624113475,"Dataset
Train
Test
Samples/client
(Average)
# classes
# samples
# samples
AG
120, 000
7, 600
43
4
SEC
1, 021
134
3
2
CelebA
155, 529
19, 962
20
40 (binary)
FEMNIST
734, 033
83, 818
227
62"
REFERENCES,0.8893617021276595,"Model Conﬁguration. We use the test accuracy and the test area under the curve (AUC) as evaluation
metrics. Models with higher values of test accuracy and AUC are better. We use the BERT-Base
(Uncased) pre-trained model (ber; Devlin et al., 2018) to extract embedded features in the AG and
SEC datasets. In the CelebA and FEMNIST datasets, we use the ResNet-18 pre-trained model (img;
He et al., 2016). Dimension of the extracted embedded features in the AG and SEC datasets is
r = 768, and in the CelebA and FEMNIST datasets is r = 512. For text and image classiﬁcation
tasks, we use two fully connected layers on top of embedded features, each of which consists of
1, 500 hidden neurons and uses a ReLU activation function. The output dimension is corresponding
to the number of classes, i.e., 4, 2, 40, and 62 in the AG, SEC, CelebA, and FEMNIST datasets. SGD
optimizer with the learning rate is 0.01 in the AG and SEC datasets, 0.1 in the FEMNIST and CelebA
datasets."
REFERENCES,0.8907801418439716,"Experimental setting for anonymization (Sun et al., 2021)."
REFERENCES,0.8921985815602836,"In LDP-FL (Sun et al., 2021), they design a LDP mechanism to perturb the weights at the local
client, then each local client applies a split and shufﬂe mechanism on the weights of local model
and sends each weight through an anonymous mechanism to the cloud. The purpose of the shufﬂing
mechanism is to break the linkage among the model weight updates from the same clients and to mix
them among updates from other clients, making it harder for the cloud to combine more than one
piece of updates to infer more information about any client. Therefore, the key idea of the shufﬂe
mechanism in LDP-FL is to mitigate the privacy degradation by high data dimension and many
training/query iterations. In other words, the client anonymity is preserved, and the privacy budget
will not accumulate."
REFERENCES,0.8936170212765957,"When comparing with LDP-FL, we maintain their mechanism’ spirits of no privacy accumulation.
In the submission, we consider there is no privacy accumulation over the training iterations. It
is equivalent to the shufﬂing step that breaks the linkage among the model weight updates with
associated clients. We also used the same randomized response mechanism in the paper, which is
Eq. 2 (Sun et al., 2021), to perturb the weight."
REFERENCES,0.8950354609929078,"In the revision, we added an experiment that do not consider the privacy accumulation over data
dimension and training/query iterations. This completely follows the gist of LDP-FL. In addition,"
REFERENCES,0.8964539007092198,1https://www.sec.gov/edgar.shtml
REFERENCES,0.8978723404255319,Under review as a conference paper at ICLR 2022
REFERENCES,0.8992907801418439,"the weights we used for LDP-FL. without actual shufﬂing or splitting can be considered a lossless
process, therefore the results we reported here can be counted as an upper-bound result for LDP-FL."
REFERENCES,0.900709219858156,"As shown in Table 2, we obtained the slightly higher accuracy of LDP-FL compared with f-RR
on the AG dataset. The key component of LDP-FL that helps to reduce the privacy accumulation
issue is the shufﬂing mechanism. However, as pointed out in (Erlingsson et al., 2019), in the real
world, it is possible that the anonymizers (i.e., shufﬂer) can either be compromised or collude with
the coordinating server to extract sensitive information. Even though there is a marginally lower
trade-off between privacy loss and model utility compared with LDP-FL, the advantage of f-RR is
that it perturbs the data only once, then used the perturbed data for training process without facing an
extra privacy risk potentially caused by the compromised or colluded anonymizer."
REFERENCES,0.902127659574468,Table 2: Results of LDP-FL without privacy accumulation.
REFERENCES,0.9035460992907801,"ϵX
1
2
3
4
5
6
7
8
9
10
LDP-FL
78.73
82.21
83.25
84.19
84.36
84.22
84.64
84.43
84.31
84.72
f-RR
72.46
79.72
81.42
79.35
81.11
79.84
79.50
80.05
81.20
82.72
Noiseless model
87.59"
REFERENCES,0.9049645390070922,"Figure 15: Accuracy of LDP algorithms applied on the embedded features ex in the AG, SEC, and
FEMNIST datasets."
REFERENCES,0.9063829787234042,"Figure 16: Accuracy of LDP algorithms applied on the gradients ▽u
θt in the AG, SEC, and FEMNIST
datasets."
REFERENCES,0.9078014184397163,"Figure 17: Accuracy of LDP algorithms applied on the gradients ▽u
θt with the anonymizer (Sun et al.,
2021)."
REFERENCES,0.9092198581560283,Under review as a conference paper at ICLR 2022
REFERENCES,0.9106382978723404,"Figure 18: Accuracy of each mechanism applied on labels in the AG, SEC, and FEMNIST datasets."
REFERENCES,0.9120567375886525,"Figure 19: AUC values of LDP algorithms applied on the embedded features ex in the AG, SEC, and
FEMNIST dataset."
REFERENCES,0.9134751773049645,"Figure 20: AUC values of LDP algorithms applied on the gradients ▽u
θt in the AG, SEC, and
FEMNIST datasets."
REFERENCES,0.9148936170212766,"Figure 21: AUC values of each mechanism applied on labels in the AG, SEC, and FEMNIST datasets."
REFERENCES,0.9163120567375886,Under review as a conference paper at ICLR 2022
REFERENCES,0.9177304964539007,"Table 3: AUC values of each algorithm applied on ex in the CelebA dataset. Average is the average
of all 40 attributes."
REFERENCES,0.9191489361702128,"Algorithm [▽u
θt
with Anon]"
REFERENCES,0.9205673758865248,"Attribute
Attractive
Heavy
Makeup"
REFERENCES,0.9219858156028369,"High
Cheekbones
Male"
REFERENCES,0.9234042553191489,"Mouth
Slightly
Open"
REFERENCES,0.924822695035461,"Smiling
Lipstick
Average"
REFERENCES,0.926241134751773,"Noiseless
ϵX = ∞
78.05
85.47
76.53
92.77
72.91
79.15
88.85
68.09 DM"
REFERENCES,0.9276595744680851,"ϵX = 1
51.96
50.00
50.9
50.12
51.54
51.77
50.63
50.17
ϵX = 5
52.48
50.00
51.04
50.14
51.64
52.08
50.94
50.21
ϵX = 10
52.08
50.00
50.87
50.20
51.91
52.03
50.40
50.19 PM"
REFERENCES,0.9290780141843972,"ϵX = 1
51.83
50.00
51.13
50.09
52.22
51.95
50.58
50.20
ϵX = 5
51.67
50.00
50.93
50.39
52.31
51.63
50.59
50.19
ϵX = 10
52.62
50.00
50.98
50.16
52.21
52.02
50.49
50.21 HM"
REFERENCES,0.9304964539007092,"ϵX = 1
52.55
50.00
50.56
50.13
51.90
51.53
50.68
50.18
ϵX = 5
52.07
50.00
50.72
50.16
51.65
51.99
50.61
50.18
ϵX = 10
51.86
50.00
50.56
50.09
52.26
52.19
50.63
50.19"
REFERENCES,0.9319148936170213,"Three
outputs"
REFERENCES,0.9333333333333333,"ϵX = 1
52.57
50.00
50.59
50.19
51.80
51.83
50.62
50.19
ϵX = 5
52.07
50.00
50.75
50.12
52.14
51.66
50.60
50.18
ϵX = 10
51.94
50.00
50.54
50.19
51.44
52.30
50.38
50.17"
REFERENCES,0.9347517730496454,PM-SUB
REFERENCES,0.9361702127659575,"ϵX = 1
52.44
50.01
51.10
50.13
52.02
51.66
50.74
50.20
ϵX = 5
52.49
50.00
50.87
50.15
52.18
51.64
50.51
50.20
ϵX = 10
51.94
50.00
50.30
50.19
52.02
52.28
50.85
50.19"
REFERENCES,0.9375886524822695,"f-RR
and
Label
-Laplace"
REFERENCES,0.9390070921985816,"ϵX = 1, ϵY = 1
50.00
50.81
50.00
50.03
50.00
50.00
50.00
50.12
ϵX = 1, ϵY = 2.5
50.00
50.00
50.00
51.39
50.00
50.00
50.00
50.20
ϵX = 1, ϵY = 5
51.92
50.06
50.31
51.41
50.00
50.01
50.00
50.22
ϵX = 5, ϵY = 1
50.64
50.00
50.07
50.01
50.00
50.39
50.00
50.21
ϵX = 5, ϵY = 2.5
50.00
50.23
50.00
49.99
50.00
50.00
50.00
50.10
ϵX = 5, ϵY = 5
51.87
52.40
50.00
50.00
50.00
50.00
50.00
50.21
ϵX = 10, ϵY = 1
51.70
50.00
50.00
50.00
50.00
51.03
50.00
50.12
ϵX = 10, ϵY = 2.5
50.01
50.12
50.00
50.00
50.00
50.05
51.12
50.16
ϵX = 10, ϵY = 5
50.00
50.00
50.00
50.13
50.01
50.00
50.00
50.21"
REFERENCES,0.9404255319148936,BitRand
REFERENCES,0.9418439716312057,"ϵX = 1, ϵY = ∞
59.8
60.65
56.7
64.57
54.56
56.44
64.93
51.86
ϵX = 1, ϵY = 1
54.52
55.03
53.37
56.99
52.63
53.34
57.43
50.91
ϵX = 1, ϵY = 2.5
59.08
58.20
55.36
62.02
54.29
55.86
61.91
51.52
ϵX = 1, ϵY = 5
59.98
60.15
56.28
63.67
54.14
56.91
63.98
51.77
ϵX = 5, ϵY = ∞
67.15
71.81
60.65
77.91
58.16
61.82
76.15
54.99
ϵX = 5, ϵY = 1
58.04
59.87
54.24
62.99
54.05
55.67
62.35
51.82
ϵX = 5, ϵY = 2.5
64.61
67.70
58.78
73.56
56.2
60.04
72.09
53.50
ϵX = 5, ϵY = 5
67.51
71.41
60.94
77.96
57.82
61.57
76.11
54.82
ϵX = 10, ϵY = ∞
75.09
81.96
70.14
90.22
65.26
71.76
86.68
63.31
ϵX = 10, ϵY = 1
61.87
64.88
59.04
68.82
56.8
60.33
67.20
53.84
ϵX = 10, ϵY = 2.5
71.35
76.69
66.88
84.48
62.74
68.33
81.52
58.16
ϵX = 10, ϵY = 5
74.45
82.28
69.23
89.88
64.81
71.47
86.38
62.25"
REFERENCES,0.9432624113475178,Under review as a conference paper at ICLR 2022
REFERENCES,0.9446808510638298,"Table 4: AUC values of each algorithm applied on the gradients ▽u
θt with the anonymizer (Sun et al.,
2021) in the CelebA dataset."
REFERENCES,0.9460992907801419,"Algorithm [▽u
θt
with Anon]"
REFERENCES,0.9475177304964539,"Attribute
Attractive
Heavy
Makeup"
REFERENCES,0.948936170212766,"High
Cheekbones
Male"
REFERENCES,0.950354609929078,"Mouth
Slightly
Open"
REFERENCES,0.9517730496453901,"Smiling
Lipstick
Average"
REFERENCES,0.9531914893617022,"Noiseless
ϵX = ∞
78.05
85.47
76.53
92.77
72.91
79.15
88.85
68.09 DM"
REFERENCES,0.9546099290780142,"ϵX = 1
43.44
55.86
49.63
65.42
48.86
43.33
60.50
49.85
ϵX = 5
46.94
44.25
49.00
49.68
49.97
51.73
59.24
51.00
ϵX = 10
38.60
40.81
51.87
64.81
49.55
49.05
48.58
50.25 PM"
REFERENCES,0.9560283687943263,"ϵX = 1
47.48
62.64
53.94
49.60
50.88
49.07
52.04
50.81
ϵX = 5
53.65
67.99
47.72
44.75
50.13
52.34
43.20
49.72
ϵX = 10
56.85
45.04
52.46
48.42
50.15
49.16
49.81
50.30 HM"
REFERENCES,0.9574468085106383,"ϵX = 1
49.61
63.99
52.06
47.27
51.90
48.43
38.25
50.05
ϵX = 5
53.39
42.84
47.26
59.54
53.23
55.21
67.67
51.54
ϵX = 10
44.02
68.26
51.07
54.43
50.17
49.91
44.04
50.42"
REFERENCES,0.9588652482269504,"Three
outputs"
REFERENCES,0.9602836879432625,"ϵX = 1
54.71
43.89
55.70
55.37
48.46
54.93
43.11
49.99
ϵX = 5
35.45
65.80
50.13
42.53
50.21
45.62
50.36
51.54
ϵX = 10
48.53
46.76
50.03
51.96
53.64
49.57
31.48
49.21"
REFERENCES,0.9617021276595744,PM-SUB
REFERENCES,0.9631205673758865,"ϵX = 1
41.65
51.61
50.05
63.01
50.35
49.02
51.05
50.29
ϵX = 5
47.62
43.88
50.86
54.82
51.76
53.90
46.81
51.79
ϵX = 10
40.82
74.82
49.86
55.00
54.19
52.39
45.04
51.34"
REFERENCES,0.9645390070921985,LDP-FL
REFERENCES,0.9659574468085106,"ϵX = 1
41.51
49.43
50.24
33.03
49.01
49.40
53.76
48.71
ϵX = 5
50.35
53.27
51.99
56.76
49.71
50.02
56.31
51.74
ϵX = 10
46.71
46.90
50.11
52.04
47.74
48.50
49.42
50.28"
REFERENCES,0.9673758865248226,"f-RR
and
Label
-Laplace"
REFERENCES,0.9687943262411347,"ϵX = 1, ϵY = 1
50.00
50.81
50.00
50.03
50.00
50.00
50.00
50.12
ϵX = 1, ϵY = 2.5
50.00
50.00
50.00
51.39
50.00
50.00
50.00
50.20
ϵX = 1, ϵY = 5
51.92
50.06
50.31
51.41
50.00
50.01
50.00
50.22
ϵX = 5, ϵY = 1
50.64
50.00
50.07
50.01
50.00
50.39
50.00
50.21
ϵX = 5, ϵY = 2.5
50.00
50.23
50.00
49.99
50.00
50.00
50.00
50.10
ϵX = 5, ϵY = 5
51.87
52.40
50.00
50.00
50.00
50.00
50.00
50.21
ϵX = 10, ϵY = 1
51.70
50.00
50.00
50.00
50.00
51.03
50.00
50.12
ϵX = 10, ϵY = 2.5
50.01
50.12
50.00
50.00
50.00
50.05
51.12
50.16
ϵX = 10, ϵY = 5
50.00
50.00
50.00
50.13
50.01
50.00
50.00
50.21"
REFERENCES,0.9702127659574468,BitRand
REFERENCES,0.9716312056737588,"ϵX = 1, ϵY = ∞
59.8
60.65
56.7
64.57
54.56
56.44
64.93
51.86
ϵX = 1, ϵY = 1
54.52
55.03
53.37
56.99
52.63
53.34
57.43
50.91
ϵX = 1, ϵY = 2.5
59.08
58.20
55.36
62.02
54.29
55.86
61.91
51.52
ϵX = 1, ϵY = 5
59.98
60.15
56.28
63.67
54.14
56.91
63.98
51.77
ϵX = 5, ϵY = ∞
67.15
71.81
60.65
77.91
58.16
61.82
76.15
54.99
ϵX = 5, ϵY = 1
58.04
59.87
54.24
62.99
54.05
55.67
62.35
51.82
ϵX = 5, ϵY = 2.5
64.61
67.70
58.78
73.56
56.2
60.04
72.09
53.50
ϵX = 5, ϵY = 5
67.51
71.41
60.94
77.96
57.82
61.57
76.11
54.82
ϵX = 10, ϵY = ∞
75.09
81.96
70.14
90.22
65.26
71.76
86.68
63.31
ϵX = 10, ϵY = 1
61.87
64.88
59.04
68.82
56.8
60.33
67.20
53.84
ϵX = 10, ϵY = 2.5
71.35
76.69
66.88
84.48
62.74
68.33
81.52
58.16
ϵX = 10, ϵY = 5
74.45
82.28
69.23
89.88
64.81
71.47
86.38
62.25"
REFERENCES,0.9730496453900709,Under review as a conference paper at ICLR 2022
REFERENCES,0.9744680851063829,"Table 5: AUC values of each algorithm applied on ▽u
θt in the CelebA dataset. Average is the average
of all 40 attributes."
REFERENCES,0.975886524822695,"Algorithm [▽u
θt
with Anon]"
REFERENCES,0.9773049645390071,"Attribute
Attractive
Heavy
Makeup"
REFERENCES,0.9787234042553191,"High
Cheekbones
Male"
REFERENCES,0.9801418439716312,"Mouth
Slightly
Open"
REFERENCES,0.9815602836879432,"Smiling
Lipstick
Average"
REFERENCES,0.9829787234042553,"Noiseless
ϵX = ∞
78.05
85.47
76.53
92.77
72.91
79.15
88.85
68.09 DM"
REFERENCES,0.9843971631205674,"ϵX = 1
57.96
29.57
46.45
61.43
50.58
50.9
47.61
47.96
ϵX = 5
59.85
39.72
51.01
53.36
51.15
49.97
32.64
48.62
ϵX = 10
56.33
44.01
50.05
55.19
49.8
52.05
30.79
50.00 PM"
REFERENCES,0.9858156028368794,"ϵX = 1
51.87
39.38
49.3
73.24
49.41
48.9
52.99
49.66
ϵX = 5
48.42
40.86
46.84
72.35
50.55
49.52
47.68
50.66
ϵX = 10
49.02
36.59
49.03
59.12
50.39
50.46
53.36
49.22 HM"
REFERENCES,0.9872340425531915,"ϵX = 1
46.31
33.84
52.46
42.45
48.33
50.98
48.23
50.41
ϵX = 5
52.39
34.32
52.49
29.79
50.02
48.04
49.90
49.37
ϵX = 10
53.3
49.65
52.23
35.33
50.82
47.69
47.19
50.33"
REFERENCES,0.9886524822695035,"Three
outputs"
REFERENCES,0.9900709219858156,"ϵX = 1
45.91
63.71
50.16
61.60
51.35
45.39
61.21
50.58
ϵX = 5
35.89
55.43
48.95
55.22
47.60
46.30
58.41
50.39
ϵX = 10
35.03
60.51
49.85
60.69
47.70
43.97
54.05
49.88"
REFERENCES,0.9914893617021276,PM-SUB
REFERENCES,0.9929078014184397,"ϵX = 1
61.56
63.02
46.78
38.77
52.23
50.38
48.60
48.90
ϵX = 5
58.59
57.14
48.93
31.43
50.54
53.93
51.75
47.93
ϵX = 10
65.09
55.82
47.10
33.13
49.70
51.11
59.87
49.61"
REFERENCES,0.9943262411347518,"f-RR
and
Label
-Laplace"
REFERENCES,0.9957446808510638,"ϵX = 1, ϵY = 1
50.00
50.81
50.00
50.03
50.00
50.00
50.00
50.12
ϵX = 1, ϵY = 2.5
50.00
50.00
50.00
51.39
50.00
50.00
50.00
50.20
ϵX = 1, ϵY = 5
51.92
50.06
50.31
51.41
50.00
50.01
50.00
50.22
ϵX = 5, ϵY = 1
50.64
50.00
50.07
50.01
50.00
50.39
50.00
50.21
ϵX = 5, ϵY = 2.5
50.00
50.23
50.00
49.99
50.00
50.00
50.00
50.10
ϵX = 5, ϵY = 5
51.87
52.40
50.00
50.00
50.00
50.00
50.00
50.21
ϵX = 10, ϵY = 1
51.70
50.00
50.00
50.00
50.00
51.03
50.00
50.12
ϵX = 10, ϵY = 2.5
50.01
50.12
50.00
50.00
50.00
50.05
51.12
50.16
ϵX = 10, ϵY = 5
50.00
50.00
50.00
50.13
50.01
50.00
50.00
50.21"
REFERENCES,0.9971631205673759,BitRand
REFERENCES,0.9985815602836879,"ϵX = 1, ϵY = ∞
59.8
60.65
56.7
64.57
54.56
56.44
64.93
51.86
ϵX = 1, ϵY = 1
54.52
55.03
53.37
56.99
52.63
53.34
57.43
50.91
ϵX = 1, ϵY = 2.5
59.08
58.20
55.36
62.02
54.29
55.86
61.91
51.52
ϵX = 1, ϵY = 5
59.98
60.15
56.28
63.67
54.14
56.91
63.98
51.77
ϵX = 5, ϵY = ∞
67.15
71.81
60.65
77.91
58.16
61.82
76.15
54.99
ϵX = 5, ϵY = 1
58.04
59.87
54.24
62.99
54.05
55.67
62.35
51.82
ϵX = 5, ϵY = 2.5
64.61
67.70
58.78
73.56
56.2
60.04
72.09
53.50
ϵX = 5, ϵY = 5
67.51
71.41
60.94
77.96
57.82
61.57
76.11
54.82
ϵX = 10, ϵY = ∞
75.09
81.96
70.14
90.22
65.26
71.76
86.68
63.31
ϵX = 10, ϵY = 1
61.87
64.88
59.04
68.82
56.8
60.33
67.20
53.84
ϵX = 10, ϵY = 2.5
71.35
76.69
66.88
84.48
62.74
68.33
81.52
58.16
ϵX = 10, ϵY = 5
74.45
82.28
69.23
89.88
64.81
71.47
86.38
62.25"
