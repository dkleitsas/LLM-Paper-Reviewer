Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001976284584980237,"Modeling dynamical systems combining prior physical knowledge and machine
learning (ML) is promising in scientiﬁc problems when the underlying processes
are not fully understood, e.g. when the dynamics is partially known. A com-
mon practice to identify the respective parameters of the physical and ML compo-
nents is to formulate the problem as supervised learning on observed trajectories.
However, this formulation leads to an inﬁnite number of possible decompositions.
To solve this ill-posedness, we reformulate the learning problem by introducing
an upper bound on the prediction error of a physical-statistical model. This al-
lows us to control the contribution of both the physical and statistical compo-
nents to the overall prediction. This framework generalizes several existing hybrid
schemes proposed in the literature. We provide theoretical guarantees on the well-
posedness of our formulation along with a proof of convergence in a simple afﬁne
setting. For more complex dynamics, we validate our framework experimentally."
INTRODUCTION,0.003952569169960474,"1
INTRODUCTION"
INTRODUCTION,0.005928853754940711,"Dynamical systems prediction and identiﬁcation ﬁnd crucial applications ranging from medicine
and the study of tumors (Hanahan & Weinberg, 2011; Lu & Fei, 2014) to oceanic and climate fore-
casting (Oreskes et al., 1994; Caers, 2011). The modeling of such systems traditionally rely on
ordinary or partial differential equations (ODE/PDE) (Madec, 2008; Marti et al., 2010), and their
resolution via numerical solvers and data assimilation (Ghil & Malanotte-Rizzoli, 1991). In real
world applications, two main pitfalls occur: ﬁrst the dynamics may only be partially known and thus
do not fully represent the studied phenomena (Rio & Santoleri, 2018); second, the system state may
only be partially observed as in ocean models (Gaultier et al., 2013). Machine learning (ML) has be-
come a complementary approach to traditional physics based models (denoted MB for model based)
(Reichstein et al., 2019; Dueben & Bauer, 2018). Both offer advantages: whereas MB approaches
generalize and extrapolate better, ML high expressivity approaches beneﬁt from the ongoing growth
of available data such as satellite observations, with reduced costs compared to data assimilation.
In that perspective, recent lines of work tackle the learning of hybrid models relying on prior physi-
cal knowledge and machine learning (Yin et al., 2021; Mehta et al., 2020). Efﬁciently learning such
decompositions actually means solving two different tasks: system identiﬁcation, i.e. estimating the
parameters of the physical model, and prediction, i.e. recovering the trajectories associated to the
dynamics. Both are essential for hybrid MB/ML models of dynamical systems. Whereas predic-
tion aims at robust extrapolation, identiﬁcation accounts for physical interpretability of the MB/ML
model. While solving both problems using model-based formulation admits well-known numeri-
cal solutions, for example using the adjoint method (Le Dimet & Talagrand, 1986; Courtier et al.,
1994), the combination of physical models and deep learning is still an open area of research. In this
context, ML applications mainly focus on the prediction task, at the expense of the system identiﬁca-
tion: Willard et al. (2020) underlines the lack of generalizability of black-box ML models and their"
INTRODUCTION,0.007905138339920948,∗Equal contribution
INTRODUCTION,0.009881422924901186,Published as a conference paper at ICLR 2022
INTRODUCTION,0.011857707509881422,"inability to produce physically sound results. Indeed, Ayed et al. (2020) show that without any prior
knowledge, the recovered estimates of a dynamical system states are not physically plausible despite
accurate predictions. Moreover, as noted by Yin et al. (2021), learning a linear MB/ML decompo-
sition with the sole supervision on the system trajectories is ill-posed and admits an inﬁnite number
of decompositions. Such observations highlight the need to incorporate physically motivated con-
straints in the learning of hybrid models, e.g. through regularization penalties, and several works
propose additional constraints to guide the model towards physical solutions (Jia et al., 2019; Yin
et al., 2021; Linial et al., 2021). Finally, to complete prior dynamical knowledge with a data-driven
component and ensure interpretability of the decomposition, we work out a principled framework
that generalizes previous attempts in the regularization of hybrid models. Our contributions are :"
INTRODUCTION,0.01383399209486166,"• In section 3.1, we introduce a novel way to recover well-posedness and interpretability in
the learning of hybrid MB/ML models via the control of an upper bound. We extend our
framework to incorporate auxiliary data when available to handle complex real-world data.
• In section 3.2, we propose a novel alternate-optimization algorithm to learn hybrid models.
• In section 3.3, we provide an analysis of the convergence of the proposed algorithm on a
simpliﬁed case and experimentally evidence the soundness of our approach on more com-
plex settings of increasing difﬁculty including challenging real world problems (section 4)."
BACKGROUND AND PROBLEM SETUP,0.015810276679841896,"2
BACKGROUND AND PROBLEM SETUP"
BACKGROUND AND PROBLEM SETUP,0.017786561264822136,"We consider a dynamical system with state at time t denoted Zt = Z(t). Zt might be fully or
only partially observed: we write Zt = (Xt, Yt), where Xt is the observed component and Yt the
unobserved one. The evolution of Z is governed by a differential equation with dynamics : dZt"
BACKGROUND AND PROBLEM SETUP,0.019762845849802372,dt = d dt
BACKGROUND AND PROBLEM SETUP,0.021739130434782608,"
Xt
Yt"
BACKGROUND AND PROBLEM SETUP,0.023715415019762844,"
=

fX(Zt)
fY (Zt) 
(1)"
BACKGROUND AND PROBLEM SETUP,0.025691699604743084,"The objective is to predict trajectories of X, i.e. to model the evolution of the observable part fol-
lowing dXt"
BACKGROUND AND PROBLEM SETUP,0.02766798418972332,"dt = fX(Zt). For simplicity, we omit the index X in fX and write f(.)
∆= fX(.)."
BACKGROUND AND PROBLEM SETUP,0.029644268774703556,"Dynamical Hypothesis
We assume partial knowledge of the dynamics of the observed Xt: dXt"
BACKGROUND AND PROBLEM SETUP,0.03162055335968379,"dt
= f(Zt) = fk(Zt) + fu(Zt)
(2)"
BACKGROUND AND PROBLEM SETUP,0.03359683794466403,"where fk ∈Hk is a known operator with unknown parameters θ∗, and fu ∈Hu is the unknown
residual dynamics. Hk and Hu denote function spaces, see discussion in appendix B."
BACKGROUND AND PROBLEM SETUP,0.03557312252964427,"Learning Problem
Our objective is to approximate f with a function h learned from the observed
data. According to eq. (2), we assume h = hk + hu. hk ∈Hk, i.e. belongs to the same hy-
pothesis space as fk: it has the same parametric form. Its parameters are denoted θk. Note that
hk(., θ∗) = fk. hu ∈Hu is represented by a free form functional with parameters θu, e.g. a neural
network. The learning problem is to estimate from data the parameters of hk so that they match
the true physical ones and hu to approximate at best the unknown dynamics f. In this regard, an
intuitive training objective is to minimize a distance d between h = hk + hu and f:"
BACKGROUND AND PROBLEM SETUP,0.037549407114624504,"d(h, f) = EZ∼pZ∥h(Z) −f(Z)∥2,
(3)"
BACKGROUND AND PROBLEM SETUP,0.039525691699604744,"where pZ is the distribution of the state Z that accounts for varying initial states. Each Z deﬁnes
a training sample. Minimizing eq. (3) with h = hk + hu enables to predict accurate trajectories
but may have an inﬁnite number of solutions and hu may bypass the physical hypothesis hk. Thus,
interpretability is not guaranteed. We now develop our proposition to overcome this ill-posedness."
METHOD,0.041501976284584984,"3
METHOD"
METHOD,0.043478260869565216,"In hybrid modeling, two criteria are essentials: 1. identiﬁability, i.e. the estimated parameters of hk
should correspond to the true physical ones; 2. prediction power, i.e. the statistical component hu
should complete hk so that h = hk + hu performs accurate prediction over the system states. To"
METHOD,0.045454545454545456,Published as a conference paper at ICLR 2022
METHOD,0.04743083003952569,"control the contribution of each term hk and hu, we work upper bounds out of eq. (3) (section 3.1).
We then propose to minimize d(h, f) while constraining the upper bounds, which provide us with
a well-posed learning framework (section 3.2). Besides, we show that several previous works that
introduced constrained optimization to solve related problems are speciﬁc cases of our formulation
(Yin et al., 2021; Jia et al., 2019; Linial et al., 2021). Finally, we introduce an alternate optimization
algorithm which convergence is shown in section 3.3 for a linear approximation of f."
STRUCTURAL CONSTRAINTS FOR DYNAMICAL SYSTEMS,0.04940711462450593,"3.1
STRUCTURAL CONSTRAINTS FOR DYNAMICAL SYSTEMS"
STRUCTURAL CONSTRAINTS FOR DYNAMICAL SYSTEMS,0.05138339920948617,"To ensure identiﬁability, we derive regularizations on hk and hu ﬂowing from the control of an upper
bound of d(h, f). In particular, to minimize d(hk, fk) would enable us to accurately interpret hk as
the true fk, and hu as the residual dynamics fu. However, since we do not access the parameters of
fk, computing d(hk, fk) is not tractable. We then consider two possible situations. In the ﬁrst one,
the only available information on the physical system is the parametric form of fk (or equivalently
of hk), training thus only relies on observed trajectories (section 3.1.1). In the second one, we
consider available auxiliary information about fk that will be used to minimize the distance between
hk and fk (section 3.1.2). While the ﬁrst setting is the more general, the physical prior it relies on is
often insufﬁcient to effectively handle real world situations. The second setting makes use of more
informative priors and better corresponds to real cases as shown in the experiments (section 4.2)."
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.0533596837944664,"3.1.1
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS"
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.05533596837944664,"We propose a general approach to constrain the learning of hybrid models when one solely access
the functional form of hk. In this case, to make hk accountable in our observed phenomena, a
solution is to minimize d(hk, f). Following the triangle inequality we link up both errors d(h, f)
and d(hk, f) (computations available in appendix C.1):"
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.05731225296442688,"d(h, f) ≤d(h, hk) + d(hk, f) = d(hu, 0) + d(hk, f)
(4)"
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.05928853754940711,"We want the physical-statistical model h = hk + hu to provide high quality forecasts. Minimizing
the sole upper bound does not ensure such aim, as hu is only penalized through d(hu, 0) and is
not optimized to contribute to predictions. We thus propose to minimize d(h, f) while controlling
both d(hu, 0) and d(hk, f). Such a control of the upper bound of eq. (4) amounts to balancing the
contribution of the ML and the MB components. This will be formally introduced in section 3.2."
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.06126482213438735,"Link to the Literature
The least action principle on the ML component i.e. constraining d(hu, 0)
is invoked for a geometric argument in (Yin et al., 2021), and appears as a co-product of the intro-
duction of d(hk, f) in eq. (4). Optimizing d(hk, f) to match the physical model with observations
is investigated in (Forssell & Lindskog, 1997)."
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.06324110671936758,"The general approach of eq. (4) allows us to perform prediction (via h) and system identiﬁcation
(via hk) on simple problems (see section 4.1). The learning of real-world complex dynamics, via
data-driven hybrid models, often fails at yielding a physically sound estimation, as illustrated in
section 4.2. This suggests that learning complex dynamics requires additional information. In many
real-world cases, auxiliary information is available in the form of measurements providing comple-
mentary information on fk. Indeed, a common issue in physics is to infer an unobserved variable of
interest (in our case fk parameters θ⋆) from indirect or noisy measurements that we refer to as proxy
data. For instance, one can access a physical quantity but only at a coarse resolution, as in (Um et al.,
2020; Belbute-Peres et al., 2020) and in the real world example detailed in section 4.2. We show in
the next subsection how to incorporate such an information in order to approximate d(hk, fk)."
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.06521739130434782,"3.1.2
MATCHING THE PHYSICAL HYPOTHESES: INTRODUCING AUXILIARY DATA"
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.06719367588932806,"We here assume one accesses a proxy of fk, denoted f pr
k
∈Hk. Our goal is to adapt our framework
to incorporate such auxiliary information, bringing the regularization induced by f pr
k
within the
scope of the control of an upper bound. This enables us to extend our proposition towards the
solving of real world physical problems, still largely unexplored by the ML community. We have:"
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.0691699604743083,"d(h, f) ≤d(h, hk) + d(hk, f pr
k ) + Γ = d(hu, 0) + d(hk, f pr
k ) + Γ
(5)"
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.07114624505928854,"where Γ is a constant of the problem that cannot be optimized (see appendix C.2). In that context,
we can beneﬁt from auxiliary information providing us with coarse estimates of θ⋆, denoted θpr,"
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.07312252964426877,Published as a conference paper at ICLR 2022
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.07509881422924901,"such that f pr
k
= hk(., θpr) ≈fk. To use the available θpr to guide our estimation towards the
true parameters θ⋆of fk, a simple solution is to directly enforce the minimization of d(hk, f pr
k )
in the parameter space by minimizing ∥θk −θpr∥2, where θk are the parameters of hk. Indeed,
because fk and f pr
k
have identical parametric forms (as both belong to the same functional space
Hk), minimizing ∥θk −θpr∥2 will bring hk closer to f pr
k
and thus to fk. As above, we propose to
minimize d(h, f) while controlling both d(hu, 0) and d(hk, f pr
k ), as described in section 3.2."
CONTROLLING THE ML COMPONENT AND THE MB HYPOTHESIS,0.07707509881422925,"Link to the Literature
In (Linial et al., 2021) f pr
k stands for true observations used to constrain a
learned latent space, minimizing d(hk, f pr
k ). Jia et al. (2019) uses synthetic data as f pr
k to pre-train
their model which amounts to the control an upper bound, see appendix C.3. Finally, this setting
ﬁnds an extension, when the model f pr
k is a learned model, for example trained using eq. (4), leading
to a self-supervision approach described in appendix C.4."
LEARNING ALGORITHM AND OPTIMIZATION PROBLEM,0.07905138339920949,"3.2
LEARNING ALGORITHM AND OPTIMIZATION PROBLEM"
LEARNING ALGORITHM AND OPTIMIZATION PROBLEM,0.08102766798418973,"From the upper bounds, we ﬁrst recover the well-posedness of the optimization and derive a theo-
retical learning scheme (section 3.2.1). We then discuss its practical implementation (section 3.2.2)."
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.08300395256916997,"3.2.1
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.08498023715415019,"Recovering Well-Posedness
We reformulate the ill-posed learning of minhk,hu∈Hk×Hu d(h, f),
by instead optimizing d(h, f) while constraining the upper bounds. Let us deﬁne Sk and Su as"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.08695652173913043,"Sk = { hk ∈Hk | ℓ(hk) ≤µk }
Su = { hu ∈Hu | d(hu, 0) ≤µu }
(6)"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.08893280632411067,"where µk, µu are two positive scalars and ℓ(hk) = d(hk, f) in the case of section 3.1.1 and
ℓ(hk) = d(hk, f pr
k ) in the case of section 3.1.2. Our proposition then amounts to optimizing d(h, f)
over the Minkowski-sum Sk + Su = { h = hk + hu | hk ∈Sk, hu ∈Su } :"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.09090909090909091,"min
h∈Sk+Su d(h, f),
(7)"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.09288537549407115,"This constrained optimization setting enables us to recover the well-posedness of the optimization
problem under the relative compactness of the family of function Hk (proof in appendix D.3)."
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.09486166007905138,"Proposition 1 (Well-posedness). Under the relative compactness of Sk, eq. (7) ﬁnds a solution h
that writes as h = hk + hu ∈Sk + Su. Moreover, this solution is unique."
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.09683794466403162,"Alternate Optimization Algorithm
As the terms in both upper bounds of eqs. (4) and (5) specif-
ically address either hk or hu, we isolate losses relative to hk and hu and alternate projections of
hk on Sk and hu on Su, as described in Algorithm 1. Said otherwise, we learn h by alternately
optimizing hk (hu being ﬁxed) and hu (hk being ﬁxed). In practice, we rely on a dual formulation
(see section 3.2.2 and the SGD version of Algorithm 1 in Appendix F)."
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.09881422924901186,"Algorithm 1 Alternate estimation: General Setting
Result: Converged hk and hu
Set h0
u = 0, h0
k = minhk∈Hk d(hk, f), tol ∈R+
while d(h, f) > tol do"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.1007905138339921,"hn+1
k
= arg min
hk∈Sk
d(hk + hn
u, f);
hn+1
u
= arg min
hu∈Su
d(hn+1
k
+ hu, f)
(8)"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.10276679841897234,"n ←n + 1
end"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.10474308300395258,"The convergence of the alternate projections is well studied for the intersection of convex sets or
smooth manifolds (von Neumann, 1950; Lewis & Malick, 2008) and has been extended in our
setting of Minkowski-sum of convex sets (Lange et al., 2019). Because d as deﬁned in eq. (3) is
convex, Su and Sk are convex sets as soon as Hk and Hu are convex (Appendix A). Thus, if d(., f)
is strongly convex, eq. (8) ﬁnds one and only one solution (Boyd et al., 2004). However, neither the
convexity of Hu nor of Hk is practically ensured. Nonetheless, we recover the well-posedness of
eq. (7) and show the convergence of Algorithm 1 in the simpliﬁed case where h is an afﬁne function"
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.1067193675889328,Published as a conference paper at ICLR 2022
WELL-POSEDNESS AND ALTERNATE OPTIMIZATION ALGORITHM,0.10869565217391304,"of Xt (see section 3.3). For complex PDE where convexity may not hold, we validate our approach
experimentally and we evidence in section 4 that this formulation enables us to recover both an
interpretable decomposition h = hk + hu and improved prediction and identiﬁcation performances."
PRACTICAL OPTIMIZATION,0.11067193675889328,"3.2.2
PRACTICAL OPTIMIZATION"
PRACTICAL OPTIMIZATION,0.11264822134387352,"Equation (6) involves the choice of µk and µu. In practice, we implement the projection algorithm
by descending gradients on the parameters of hk and hu, with respect to the following losses:
Lk(hk) = λhd(h, f) + λhkℓ(hk)
Lu(hu) = λhd(h, f) + λhud(hu, 0)
(9)
where λh, λhk, λhu are positive real values, dynamically increased/decreased during training. In-
deed, d(hu, 0) can be interpreted as a stability loss, preventing the neural networks to trump the
physical component. On the other hand, d(hk, f) can be interpreted has an initialization loss yield
a ﬁrst estimate of θk explaining the dynamics."
PRACTICAL OPTIMIZATION,0.11462450592885376,"Yet, f being unknown: d(h, f) is not tractable. To estimate d(h, f), we rely on the trajectories
associated to the dynamics. We minimize the distance between the ODE ﬂows φh and φf deﬁned
by h and f, dφ(φh, φf), over all initial conditions X0:"
PRACTICAL OPTIMIZATION,0.116600790513834,"dφ(φh, φf) = EX0 Z t"
PRACTICAL OPTIMIZATION,0.11857707509881422,"t0
∥φh(τ, X0) −φf(τ, X0)∥2dτ
(10)"
PRACTICAL OPTIMIZATION,0.12055335968379446,"We have: dφ(φh, φf) = 0 ⇔d(h, f) = 0. Deﬁnitions of ﬂows for ODE and in depth considera-
tion on these distances are available in appendix A. The gradients of dφ(φh, φf) with respect to the
parameters of hk or hu can be either estimated analytically using the adjoint method (Chen et al.,
2018) or using explicit solvers, e.g. Rk45, and computing the gradients thanks to the backpropaga-
tion, see (Onken & Ruthotto, 2020). To compute eq. (10), we rely on a temporal sampling of X:
our datasets are composed of n sequences of observations of length N, Xi = (Xi
t0, . . . , Xi
t0+N∆t),
where each sequence Xi follows eq. (2) and corresponds to one initial condition Xi
t0. We then sam-
ple the space of initial conditions Xi
t0 to compute a Monte-Carlo approximation to dφ(φh, φf). Let
ODESolve be the function integrating any arbitrary initial state xt0 up to time t with dynamics h,
so that xt = ODESolve(xt0, h, t). The estimate of dφ(φh, φf) then writes as:"
PRACTICAL OPTIMIZATION,0.1225296442687747,"dφ(φh, φf) ≈1 n n
X i=1 N
X j=1"
PRACTICAL OPTIMIZATION,0.12450592885375494,"ODEsolve(Xi
t0, h, tj) −Xi
tj

2"
PRACTICAL OPTIMIZATION,0.12648221343873517,Note that the way to compute ODEsolve differs across the experiments (see section 4).
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.12845849802371542,"3.3
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.13043478260869565,"We investigate the validity of our proposition when approximating an unknown derivative with an
afﬁne function (interpretable ﬁrst guess approximators). We here consider hk as a linear function.
We do not assume any information on f, thus relieving this section from the need of an accurate
prior knowledge fk. In this context, we show the convergence of the learning scheme introduced
in Algorithm 1 with ℓ= d(hk, f), demonstrating the validity of our framework in this simpliﬁed
setting. For more complex cases, for which theoretical analysis cannot be conducted, our framework
is validated experimentally in section 4. All proofs of this section are conducted using the distance
dφ. Let Xs be the unique solution to the initial value problem:
dXt"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.1324110671936759,"dt
= f(Xt)
with
Xt=0 = X0
(11)"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.13438735177865613,"With hk(X) = AX and hu(X) = DA, the afﬁne approximation of f writes as:
dXt"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.13636363636363635,"dt
= AXt + DA
with
Xt=0 = X0
(12)"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.1383399209486166,"where A ∈Mp,p(R), DA ∈Rp. We write XD the solution to eq. (12) and XA the solution to
eq. (12) when DA = 0. The alternate projection algorithm with the distance dφ writes as:"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.14031620553359683,"ˆA = arg min
A Z t t0"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.1422924901185771,"Xs(τ) −XD(τ)

2 dτ + λA Z t t0"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.1442687747035573,"Xs(τ) −XA(τ)

2 dτ
(13)"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.14624505928853754,"ˆDA = arg min
DA Z t t0"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.1482213438735178,"Xs(τ) −XD(τ)

2 dτ + λD∥DA∥2
(14)"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.15019762845849802,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.15217391304347827,"where λD, λA > 0. As the optimization of eq. (13) is not convex on A, the solution existence and
uniqueness is not ensured. The well-posedness w.r.t A can be recovered by instead considering a
simple discretization scheme, e.g. Xt+1 ≈(AXt + DA)∆t + Xt and solving the associated least
square regression, which well-posedness is guaranteed, see details in appendix D.2. Such strategy is
common practice in system identiﬁcation. Theoretical considerations on existence and uniqueness
of solutions to eqs. (13) and (14) are hard to retrieve. If A is an invertible matrix:"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.1541501976284585,"Proposition 2 (Existence and Uniqueness). If ˆA is invertible, There exists a unique DA, hence a
unique XD, solving eq. (14). (proof in appendix D.4)"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.15612648221343872,"Finally, formulating Algorithm 1 as a least square problem in an afﬁne setting (see appendix D.5),
we prove the convergence of the alternate projection algorithm (appendix D.6) :"
THEORETICAL ANALYSIS FOR A LINEAR APPROXIMATION,0.15810276679841898,"Proposition 3. For λD and λA sufﬁciently high, the algorithm that alternates between the estimation
of A and the estimation of DA following eqs. (13) and (14) converges."
EXPERIMENTS,0.1600790513833992,"4
EXPERIMENTS"
EXPERIMENTS,0.16205533596837945,"We validate Algorithm 1 on datasets of increasing difﬁculty (see appendix E), where the system state
is either fully or partially observed (resp. section 4.1 and section 4.2). We no longer rely on an afﬁne
prior and explicit hk and hu for each dataset. Performances are evaluated via standard metrics: MSE
(lower is better) and relative Mean Absolute Error (rMAE, lower is better). We assess the relevance
of our proposition based on eqs. (4) and (5), against NeuralODE (Chen et al., 2018), Aphynity (Yin
et al., 2021) and ablation studies. We denote Ours eq. (4) (resp. Ours eq. (5)) the results when
ℓ= d(hk, f) i.e eq. (4), (resp. ℓ= d(hk, f pr
k ) i.e. eq. (5)) When d(hk, f) (resp. d(hu, 0)) is not
considered in the optimization, we refer to the results as d(h, f)+d(hu, 0) (resp. d(h, f)+d(hk, f)).
When h is trained by only minimizing the discrepancy between actual and predicted trajectories the
results are denoted «Only d(h, f)». We report between brackets the standard deviation of the metrics
over 5 runs and refer to Appendices F and G for training information and additional results."
FULLY OBSERVABLE DYNAMICS,0.16403162055335968,"4.1
FULLY OBSERVABLE DYNAMICS"
FULLY OBSERVABLE DYNAMICS,0.16600790513833993,"To illustrate the learning scheme induced by eq. (4), we focus on fully observed low dimensional dy-
namics: a simple example emerging from Newtonian mechanics and a population dynamics model."
FULLY OBSERVABLE DYNAMICS,0.16798418972332016,"Damped Pendulum (DPL)
Now a standard benchmark for hybrid models, we consider the motion
of a pendulum of length L damped due to viscous friction (Greydanus et al., 2019; Yin et al., 2021).
Newtonian mechanics provide an ODE describing the evolution of the angle x of the pendulum:"
FULLY OBSERVABLE DYNAMICS,0.16996047430830039,"¨x −g/L sin(x) + k ˙x = 0
(15)"
FULLY OBSERVABLE DYNAMICS,0.17193675889328064,"We suppose access to observations of the system state Z = (x, ˙x). We consider as physical motion
hypothesis hk(x, θk) = θk sin(x). The true pulsation θ∗= g/L of the pendulum has to be estimated
with θk. The viscous friction term k ˙x remains to be estimated by hu."
FULLY OBSERVABLE DYNAMICS,0.17391304347826086,"Population Dynamics (LV)
Lotka-Volterra ODE system models a prey/predator population dy-
namics describing the growth of the preys (x) without predators (y), and the extinction of predators
without preys, the non linear terms expressing the encounters between both species:"
FULLY OBSERVABLE DYNAMICS,0.17588932806324112,"˙x = αx −βxy,
and
˙y = −γy + δxy
(16)"
FULLY OBSERVABLE DYNAMICS,0.17786561264822134,"We observe the system state Z = (x, y) and set as prior knowledge: hk(x, y) = (θ1
kx, −θ2
ky).
θ⋆= (α, γ) has to be estimated by θk = (θ1
k, θ2
k). hu accounts for the non linear terms (βxy, δxy)."
FULLY OBSERVABLE DYNAMICS,0.17984189723320157,"Experimental Setting
For both DPL and LV experiments, we consider the following setting: we
sample the space of initial conditions building 100/50/50 trajectories for the train, validation and test
sets. The sequences share the same parameters; respectively ( g"
FULLY OBSERVABLE DYNAMICS,0.18181818181818182,"L, k), for DPL, and (α, β, γ, δ) for
LV. The parameter θk is set to a neuron (of dimension 1 in the pendulum and 2 for LV) and hu is a
2-layer MLP. Further experimental details are available in appendices E.1, E.2 and F."
FULLY OBSERVABLE DYNAMICS,0.18379446640316205,Published as a conference paper at ICLR 2022
FULLY OBSERVABLE DYNAMICS,0.1857707509881423,"Table 1: Experimental Results for PDL and LV data. The presented metric for parameter evaluation is the
rMAE reported in %. Pred. columns report the prediction log MSE on trajectories on test set."
FULLY OBSERVABLE DYNAMICS,0.18774703557312253,"Model
PDL
LV
rMAE(θk, θ⋆)
Pred. logMSE
rMAE(θk, θ⋆)
Pred. logMSE"
FULLY OBSERVABLE DYNAMICS,0.18972332015810275,"Ours eq. (4)
1.56 (0.009)
-13.7 (0.84)
7.80 (0.011)
-9.28 (0.75)
Only d(h, f)
9.35 (0.04)
-13.3 (0.65)
24.5 (0.017)
-9.21 (0.91)
d(h, f) + d(hk, f)
1.82 (0.01)
-13.4 (0.56)
7.91 (0.02)
-9.01 (0.99)
d(h, f) + d(hu, 0)
11.1 (0.03)
-12.9 (0.29)
9.80 (0.098)
-9.45 (0.55)
Aphynity
6.15 (0.009)
-12.2 (0.13)
21.1 (0.016)
-9.89 (0.53)
NeuralODE
–
-10.1 (0.32)
–
-9.11 (1.1)"
FULLY OBSERVABLE DYNAMICS,0.191699604743083,"0
5
10
15
20
25
30"
FULLY OBSERVABLE DYNAMICS,0.19367588932806323,gradient iterations (×200) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
FULLY OBSERVABLE DYNAMICS,0.1956521739130435,"||A
A||2
2"
FULLY OBSERVABLE DYNAMICS,0.1976284584980237,"||D
D||2
2"
FULLY OBSERVABLE DYNAMICS,0.19960474308300397,"Figure 1: Afﬁne Case : Evolution of the MSE
between estimated dynamics ( ˆA, ˆD) and the
true one (A, D) with the number of gradients
steps for linearized DPL."
FULLY OBSERVABLE DYNAMICS,0.2015810276679842,"Identiﬁcation
and
Prediction
Results
Table
1
shows that despite accurate trajectory forecasting, the
unconstrained setting «Only d(h, f)» fails at estimating
the models parameters, showing the need for regular-
ization for identiﬁcation. Constraining the norm of the
ML component can be insufﬁcient: for LV data, both
Aphynity and d(h, f) + d(hu, 0) do not accurately es-
timate the model parameters. However, the control of
d(hk, f), following eq. (4), signiﬁcantly improves the
parameter identiﬁcation for both datasets. Indeed, in
the PDL case, hk and f are (pseudo)-periodic of the
same period, hence the gain in the performances. Fi-
nally, our proposition based on eq. (4) is able to identify
the parameters of DPL and LV equation with a preci-
sion of respectively 1.56% and 7.8% beating all consid-
ered baselines. Regarding prediction performances, in under-constrained settings ( «Only d(h, f)»
in Table 1), hu learns to corrects the inaccurate hk. Table 1 and ﬁgs. 4 and 5 (appendix G.1) show
that our proposition provides more consistent prediction performances. These experiments conﬁrm
that the constraints on hk and hu arising from the control of the upper bound of eq. (4) increase
interpretability and maintain prediction performances."
FULLY OBSERVABLE DYNAMICS,0.20355731225296442,"Throwback to the Afﬁne Case
We verify the convergence proved in section 3.3 using the damped
pendulum (eq. (15)) linearized in the small oscillations regime (see appendix E.1). Making an afﬁne
hypothesis following eq. (12), we apply our alternate projection algorithm and optimize A and DA
alternately using SGD. Figure 1 shows that we are able to accurately estimate A and D using our
proposition, recovering both the oscillation pulsation and the damping coefﬁcient."
HIGH DIMENSIONAL DYNAMICS,0.20553359683794467,"4.2
HIGH DIMENSIONAL DYNAMICS"
HIGH DIMENSIONAL DYNAMICS,0.2075098814229249,"We now address the learning of transport equations, describing a wide range of physical phenomena
such as chemical concentration, ﬂuid dynamics or material properties. We evaluate the learning
setting induced by eq. (4) and (5) on two physical datasets depicting the evolution of the temperature
T advected by a time-dependent velocity ﬁeld U and subject to forcing S, following: ∂T"
HIGH DIMENSIONAL DYNAMICS,0.20948616600790515,"∂t + ∇.(TU) = S(U)
(17)"
HIGH DIMENSIONAL DYNAMICS,0.21146245059288538,"The system state Z = (T, U, S) is partially observed, we only access T. Every quantities, observed
or to estimate, are regularly sampled on a spatiotemporal grid: at each timestep t, the time varying
velocity ﬁeld Ut writes as Ut = (ut, vt) and ut, vt, Tt and the forcing term St are all of size 64×64."
HIGH DIMENSIONAL DYNAMICS,0.2134387351778656,"Experimental Setting
We consider as physical prior the advection i.e hk(T, θk) = −∇.(Tθk).
Thus, θk is time-dependent, as we learn it to approximate θ⋆= U. We identify the velocity ﬁeld θk
from observations of T, learning a mapping between T and U parameterized by a neural network
Gψ, so that θk = Gψ(Tt−l, ..., Tt) ≈Ut, which is common practice in oceanography (Béréziat &
Herlin, 2015). Gψ is optimized following eq. (9). S remains to be learned by hu. hk implements"
HIGH DIMENSIONAL DYNAMICS,0.21541501976284586,Published as a conference paper at ICLR 2022
HIGH DIMENSIONAL DYNAMICS,0.21739130434782608,"Table 2: Results for Adv+S and Natl data. We report the MSE (× 100) on the predicted observations T, the
velocity ﬁelds U and the source term S over 6 time steps on test set."
HIGH DIMENSIONAL DYNAMICS,0.21936758893280633,"Models
Adv+S
Natl"
HIGH DIMENSIONAL DYNAMICS,0.22134387351778656,"T
U
S
T
U
S"
HIGH DIMENSIONAL DYNAMICS,0.22332015810276679,"Ours eq. (4)
0.74 (0.05)
1.99 (0.13)
0.17 (0.01)
8.27 (0.06)
11.72 (0.07)
6.01 (0.08)
Ours eq. (5)
–
–
–
6.86 (0.12)
6.81 (0.07)
4.35 (0.11)
Aphynity
0.85 (0.35)
3.07 (0.74)
0.18 (0.05)
8.18 (0.16)
11.75 (0.49)
6.02 (0.02)
NeuralODE
1.35 (0.02)
–
–
8.83 (0.98)
–
–"
HIGH DIMENSIONAL DYNAMICS,0.22529644268774704,"a differentiable semi-Lagrangian scheme (Jaderberg et al., 2015) (see appendix E.3) and hu is a
ResNet. Gψ is a UNet. Training details and a schema of our model are to be found in appendix F."
HIGH DIMENSIONAL DYNAMICS,0.22727272727272727,"Figure 2: Best viewed in color. Estimations of S, T
and U = (u, v) on Adv+S. Prediction ranges from
1 to 20 half-days."
HIGH DIMENSIONAL DYNAMICS,0.22924901185770752,"Synthetic Advection and Source (Adv+S)
To
test the applicability of the learning setting in-
duced by eq. (4) on partially observed settings,
we ﬁrst study a synthetic setting (denoted Adv+S)
of eq. (17) by generating velocity ﬁelds U, simu-
lated following (Boffetta et al., 2001) and adding
a source term S inspired by (Frankignoul, 1985).
The simulation details are given in appendix E.3."
HIGH DIMENSIONAL DYNAMICS,0.23122529644268774,"Real Ocean Dynamics (Natl)
We consider a
dataset emulating real world observations of the
North ATLantic ocean (denoted Natl) (Ajayi et al.,
2019).
Modeling the evolution of T in Natl is
challenging as its dynamics is chaotic and highly
non-linear. This simulation is representative of the
complexity encountered in real world data. The
principled approach of eq. (4) is insufﬁcient here
and one must resort to additional physical informa-
tion. We illustrate section 4.2 and make use of aux-
iliary data: satellite observations provide a coarse
estimate of surface velocity ﬁelds (appendix E).
The goal is to reﬁne the approximated velocity ﬁelds to ﬁt the ocean dynamics. We proceed as
described in eq. (5) and enforce d(hk, f pr
k ) supervising Gψ with the proxy data (appendix E.3)."
HIGH DIMENSIONAL DYNAMICS,0.233201581027668,"Identiﬁcation and Prediction Results
Table 2 indicates that for Adv+S dataset, we estimate ac-
curately the unobserved velocity ﬁelds. Qualitatively, Figure 2 shows that controlling our proposed
upper bound eq. (4) facilitates the recovery of truthful velocity ﬁelds U along with an accurate pre-
diction of T. For the highly complex Natl, Table 2 shows that the introduction of auxiliary data
following the formulation in eq. (5) signiﬁcantly helps identiﬁcation, as the dynamics is too com-
plex to be able to recover physically interpretable velocity ﬁelds using the bound of eq. (4).
Regarding prediction performances on the Adv+S data, Table 2 shows that thanks to our truthful
estimates of U, our model provides more precise prediction than NODE and Aphynity. For real
world data, thanks to the proxy data our model recovers better velocity ﬁelds terms while providing
a better estimate for T. Besides, adding prior knowledge in the prediction systems improves pre-
diction performances: appendix G shows that NODE minimizes d(h, f) by predicting average and
blurred frames. This shows the need for regularization when learning on structured physical data."
HIGH DIMENSIONAL DYNAMICS,0.23517786561264822,"Ablation Study
We present in Table 3 an ablation study on the Adv+S dataset evidencing the
inﬂuence of our learning choices on the resolution of both identiﬁcation and prediction tasks (see
appendix G for detailed results). “Joint” rows of Table 3 indicate that the learning of hu and hk is
done simultaneously. Table 3 shows that the sole optimization of d(h, f) fails at estimating phys-
ically sounded U. This evidences the ill-posedness in such unconstrained optimization. Table 3
indicates that all introduced regularizations improve the recovery of U w.r.t. the «Only d(h, f)»"
HIGH DIMENSIONAL DYNAMICS,0.23715415019762845,Published as a conference paper at ICLR 2022
HIGH DIMENSIONAL DYNAMICS,0.2391304347826087,"baseline, while adding d(hu, 0) signiﬁcantly improves both prediction performances and velocity
ﬁelds estimation. We highlight that the alternate optimization performs better compared to optimiz-
ing jointly all parameters of hk and hu. Notably, our proposition to optimize hk and hu alternately
beats all baselines on both T prediction and U identiﬁcation (Table 3, Joint rows). Finally, jointly
trained models fail at estimating U in Table 3, forcing hu to capture the whole dynamics."
HIGH DIMENSIONAL DYNAMICS,0.24110671936758893,"Table 3: Ablation Study on Adv+S. We report the MSE (× 100) on the predicted observations T, the velocity
ﬁelds U and the source term S over 6 time steps. “Joint” rows refer to the simultaneous optim. of hk and hu."
HIGH DIMENSIONAL DYNAMICS,0.24308300395256918,"Training
Models
T
U
S"
HIGH DIMENSIONAL DYNAMICS,0.2450592885375494,"Ours (U known)
0.52
n/a
0.19"
HIGH DIMENSIONAL DYNAMICS,0.24703557312252963,Alternate
HIGH DIMENSIONAL DYNAMICS,0.2490118577075099,"Ours eq. (4)
0.74 (0.05)
1.99 (0.13)
0.17 (0.01)
Only d(h, f)
1.02 (0.16)
4.08 (0.23)
0.19 (0.06)
d(h, f) + d(hk, f)
1.02 (0.09)
3.66 (0.15)
0.19 (0.03)
d(h, f) + d(hu, 0)
0.77 (0.06)
2.38 (0.17)
0.19 (0.01)"
HIGH DIMENSIONAL DYNAMICS,0.2509881422924901,"Joint
Ours eq. (4)
1.44 (0.08)
3.30 (0.18)
0.30 (0.03)
Only d(h, f)
1.38 (0.19)
6.96 (0.21)
0.39 (0.08)"
RELATED WORK,0.25296442687747034,"5
RELATED WORK"
RELATED WORK,0.2549407114624506,"Grey-box or hybrid modeling, combining ODE/PDE and data based models, has received an increas-
ing focus in the machine learning community (Rico-Martinez et al., 1994; Thompson & Kramer,
1994; Raissi et al., 2020b). Hybrid approaches allow for alleviated computational costs for ﬂuid
simulation (Tompson et al., 2017; De Avila Belbute-Peres et al., 2020; Wandel et al., 2021), and
show better prediction performances through data speciﬁc constraints that preserve physics (Raissi
et al., 2020a; Jia et al., 2019). They offer increased interpretability via constraints on convolutional
ﬁlters (Long et al., 2018; 2019) or on learned residual (Geneva & Zabaras, 2020). Physical knowl-
edge, introduced through ODE/PDE regularization (Psichogios & Ungar, 1992; Bongard & Lipson,
2007; de Bézenac et al., 2018) or Hamiltonian priors (Greydanus et al., 2019; Lee et al., 2021),
increases generalization power w.r.t pure ML approaches. Closer to our work, (Mehta et al., 2020;
San & Maulik, 2018; Young et al., 2017; Saha et al., 2020) study the learning of a physical model
augmented with a statistical component. Yin et al. (2021) tackle the same task, ensuring the unique-
ness in the decomposition by constraining the norm of the ML component. We generalize latter
approaches and address the well-posedness in the learning of hybrid ML/MB models through ad-
ditional regularization on the estimated parameters of the physical part. Indeed, to describe natural
phenomena relying on hybrid MB/ML models, one major task lies in the estimation of the MB part
parameters. This can be done using neural networks (Raissi et al., 2019; Mehta et al., 2020). How-
ever, identiﬁcation tasks being intrinsically ill-posed (Sabatier, 2000), imposing prior knowledge or
regularization is necessary to ensure sound estimations (Stewart & Ermon, 2017). Yet, using only
prediction as supervision, the recovered parameters are not physically interpretable (de Bézenac
et al., 2018; Ayed et al., 2020). To ensure uniqueness of the estimation solution, Ardizzone et al.
(2018) use invertible neural networks. Linial et al. (2021); Tait & Damoulas (2020); Saemundsson
et al. (2020) combine variational encoding (Kingma & Welling, 2013) and a PDE model, sampling
the space of initial conditions and parameters to solve both identiﬁcation and prediction. However,
such methods only deal with low-dimensional dynamics. Besides low dimensional systems, we also
show the soundness of our approach on complex high dimensional and partially observed dynamics."
DISCUSSION,0.25691699604743085,"6
DISCUSSION"
DISCUSSION,0.25889328063241107,"We propose in this work an algorithm to learn hybrid MB/ML models. For interpretability purposes,
we impose constraints ﬂowing from an upper bound of the prediction error and derive a learning
algorithm in a general setting. We prove its well posedness and its convergence in a linear ap-
proximation setting. Empirically, we evidence the soundness of our approach thanks to ablation
studies and comparison with recent baselines on several low and high dimensional datasets. This
work can see several extensions: considering non uniform 2 or 3-D grid for climate models, further
considerations on the investigated upper bounds, or different decomposition hypothesis."
DISCUSSION,0.2608695652173913,Published as a conference paper at ICLR 2022
DISCUSSION,0.2628458498023715,ACKNOWLEDGEMENTS
DISCUSSION,0.2648221343873518,"We would like to thank all members of the MLIA team from the ISIR laboratory of Sorbonne Uni-
versité for helpful discussions and comments. We acknowledge ﬁnancial support from ANR AI
Chairs program via the DL4CLIM ANR-19-CHIA- 0018-01 project, the LOCUST ANR project
(ANR-15-CE23-0027) and the European Union’s Horizon 2020 research and innovation program
under grant agreement 825619 (AI4EU). The Natl60 data were provided by MEOM research team,
from the IGE laboratory from the Université Grenoble Alpes."
REFERENCES,0.26679841897233203,REFERENCES
REFERENCES,0.26877470355731226,"Adekunle Ajayi, Julien Le Sommer, Eric Chassignet, Jean-Marc Molines, Xiaobiao Xu, Aurelie
Albert, and Emmanuel Cosme. Spatial and temporal variability of north atlantic eddy ﬁeld at
scale less than 100km. Earth and Space Science Open Archive, pp. 28, 2019. doi: 10.1002/
essoar.10501076.1."
REFERENCES,0.2707509881422925,"Lynton Ardizzone, Jakob Kruse, Carsten Rother, and Ullrich Köthe. Analyzing inverse problems
with invertible neural networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.2727272727272727,"Ibrahim Ayed, Emmanuel de Bézenac, Arthur Pajot, and Patrick Gallinari. Learning the spatio-
temporal dynamics of physical processes from partial observations. In ICASSP 2020 - 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3232–3236,
2020."
REFERENCES,0.274703557312253,"Filipe de Avila Belbute-Peres, Thomas Economon, and Zico Kolter. Combining differentiable pde
solvers and graph neural networks for ﬂuid ﬂow prediction. In International Conference on Ma-
chine Learning, pp. 2402–2411. PMLR, 2020."
REFERENCES,0.2766798418972332,"Dominique Béréziat and Isabelle Herlin. Coupling dynamic equations and satellite images for mod-
elling ocean surface circulation. Communications in Computer and Information Science, 550:
191–205, 2015. doi: 10.1007/978-3-319-25117-2\_12. URL https://hal.inria.fr/hal-01245369."
REFERENCES,0.27865612648221344,"Guido Boffetta, G Lacorata, G Redaelli, and A Vulpiani. Detecting barriers to transport: a review of
different techniques. Physica D: Nonlinear Phenomena, 159(1-2):58–70, 2001."
REFERENCES,0.28063241106719367,"Josh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems.
Proceedings of the National Academy of Sciences, 104(24):9943–9948, 2007."
REFERENCES,0.2826086956521739,"Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004."
REFERENCES,0.2845849802371542,"Jef Caers. Modeling uncertainty in the earth sciences. John Wiley & Sons, 2011."
REFERENCES,0.2865612648221344,"Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary dif-
ferential equations. In Samy Bengio, Hanna Wallach, Hugo Larochelle, Kristen Grauman, Nicolò
Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
31, pp. 6571–6583. Curran Associates, Inc., 2018."
REFERENCES,0.2885375494071146,"PHILIPPE Courtier, J-N Thépaut, and Anthony Hollingsworth. A strategy for operational imple-
mentation of 4d-var, using an incremental approach. Quarterly Journal of the Royal Meteorolog-
ical Society, 120(519):1367–1387, 1994."
REFERENCES,0.29051383399209485,"Filipe De Avila Belbute-Peres, Thomas Economon, and Zico Kolter. Combining differentiable PDE
solvers and graph neural networks for ﬂuid ﬂow prediction. In Hal Daumé III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 2402–2411. PMLR, 13–18 Jul 2020."
REFERENCES,0.2924901185770751,"Emmanuel de Bézenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes:
Incorporating prior scientiﬁc knowledge. In International Conference on Learning Representa-
tions, 2018."
REFERENCES,0.29446640316205536,Published as a conference paper at ICLR 2022
REFERENCES,0.2964426877470356,"Michail Diamantakis. The semi-lagrangian technique in atmospheric modelling: current status and
future challenges. In Seminar on Recent Developments in Numerical Methods for Atmosphere and
Ocean Modelling, 2-5 September 2013, pp. 183–200, Shinﬁeld Park, Reading, 2014. ECMWF,
ECMWF."
REFERENCES,0.2984189723320158,"Jérôme Droniou.
Intégration et Espaces de Sobolev à Valeurs Vectorielles.
working paper or
preprint, April 2001. URL https://hal.archives-ouvertes.fr/hal-01382368."
REFERENCES,0.30039525691699603,"Peter D Dueben and Peter Bauer. Challenges and design choices for global weather and climate
models based on machine learning. Geoscientiﬁc Model Development, 11(10):3999–4009, 2018."
REFERENCES,0.30237154150197626,"U. Forssell and P. Lindskog.
Combining semi-physical and neural network modeling: An ex-
ample oﬁts usefulness. IFAC Proceedings Volumes, 30(11):767–770, 1997. ISSN 1474-6670.
doi: https://doi.org/10.1016/S1474-6670(17)42938-7. IFAC Symposium on System Identiﬁca-
tion (SYSID’97), Kitakyushu, Fukuoka, Japan, 8-11 July 1997."
REFERENCES,0.30434782608695654,"Claude Frankignoul. Sea surface temperature anomalies, planetary waves, and air-sea feedback in
the middle latitudes. Reviews of geophysics, 23(4):357–390, 1985."
REFERENCES,0.30632411067193677,"Lucile Gaultier, Jacques Verron, Jean-Michel Brankart, Olivier Titaud, and Pierre Brasseur. On
the inversion of submesoscale tracer ﬁelds to estimate the surface ocean circulation. Journal of
Marine Systems, 126:33–42, 2013."
REFERENCES,0.308300395256917,"Nicholas Geneva and Nicholas Zabaras.
Modeling the dynamics of pde systems with physics-
constrained deep auto-regressive networks. Journal of Computational Physics, 403:109056, 2020.
ISSN 0021-9991. doi: https://doi.org/10.1016/j.jcp.2019.109056."
REFERENCES,0.3102766798418972,"Michael Ghil and Paola Malanotte-Rizzoli. Data assimilation in meteorology and oceanography.
Advances in geophysics, 33:141–266, 1991."
REFERENCES,0.31225296442687744,"Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Hanna
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché Buc, Emily Fox, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 15379–15389. Curran
Associates, Inc., 2019."
REFERENCES,0.3142292490118577,"Douglas Hanahan and Robert A Weinberg. Hallmarks of cancer: the next generation. cell, 144(5):
646–674, 2011."
REFERENCES,0.31620553359683795,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. CVPR, 2017."
REFERENCES,0.3181818181818182,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. Spatial transformer
networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in
Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https:
//proceedings.neurips.cc/paper/2015/ﬁle/33ceb07bf4eeb3da587e268d663aba1a-Paper.pdf."
REFERENCES,0.3201581027667984,"Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan Read, Jacob Zwart, Michael S Steinbach, and
Vipin Kumar. Physics guided rnns for modeling dynamical systems: A case study in simulating
lake temperature proﬁles. In SIAM International Conference on Data Mining, SDM 2019, SIAM
International Conference on Data Mining, SDM 2019, pp. 558–566. Society for Industrial and
Applied Mathematics Publications, 2019. doi: 10.1137/1.9781611975673.63."
REFERENCES,0.3221343873517787,"Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013."
REFERENCES,0.3241106719367589,"Kenneth Lange, Joong-Ho Won, and Jason Xu. Projection onto Minkowski sums with application
to constrained learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 3642–3651. PMLR, 09–15 Jun 2019."
REFERENCES,0.32608695652173914,"François-Xavier Le Dimet and Olivier Talagrand. Variational algorithms for analysis and assimi-
lation of meteorological observations: theoretical aspects. Tellus A: Dynamic Meteorology and
Oceanography, 38(2):97–110, 1986."
REFERENCES,0.32806324110671936,Published as a conference paper at ICLR 2022
REFERENCES,0.3300395256916996,"Seungjun Lee, Haesang Yang, and Woojae Seong. Identifying physical law of hamiltonian systems
via meta-learning. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=45NZvF1UHam."
REFERENCES,0.33201581027667987,"Adrian S. Lewis and Jérôme Malick. Alternating projections on manifolds. Mathematics of Opera-
tions Research, 33(1):216–234, 2008. doi: 10.1287/moor.1070.0291."
REFERENCES,0.3339920948616601,"Ori Linial, Neta Ravid, Danny Eytan, and Uri Shalit. Generative ode modeling with known un-
knowns. In Proceedings of the Conference on Health, Inference, and Learning, pp. 79–94, 2021."
REFERENCES,0.3359683794466403,"Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. PDE-Net: Learning PDEs from data.
In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3208–3216,
Stockholmsmässan, Stockholm Sweden, July 2018. PMLR."
REFERENCES,0.33794466403162055,"Zichao Long, Yiping Lu, and Bi Dong. PDE-Net 2.0: Learning PDEs from data with a numeric-
symbolic hybrid deep network. Journal of Computational Physics, 399:108925, 2019."
REFERENCES,0.33992094861660077,"Guolan Lu and Baowei Fei. Medical hyperspectral imaging: a review. Journal of biomedical optics,
19(1):010901, 2014."
REFERENCES,0.34189723320158105,"Gurvan Madec. NEMO ocean engine. Note du Pôle de modélisation, Institut Pierre-Simon Laplace
(IPSL), France, No 27, 2008."
REFERENCES,0.3438735177865613,"Olivier Marti, Pascale Braconnot, J-L Dufresne, Jacques Bellier, Rachid Benshila, Sandrine Bony,
Patrick Brockmann, Patricia Cadule, Arnaud Caubel, Francis Codron, et al. Key features of the
ipsl ocean atmosphere model and its sensitivity to atmospheric resolution. Climate Dynamics, 34
(1):1–26, 2010."
REFERENCES,0.3458498023715415,"Viraj Mehta, Ian Char, Willie Neiswanger, Youngseog Chung, Andrew Oakleigh Nelson, Mark D
Boyer, Egemen Kolemen, and Jeff Schneider. Neural dynamical systems: Balancing structure and
ﬂexibility in physical prediction. arXiv preprint arXiv:2006.12682, 2020."
REFERENCES,0.34782608695652173,"Derek Onken and Lars Ruthotto. Discretize-optimize vs. optimize-discretize for time-series regres-
sion and continuous normalizing ﬂows. arXiv preprint arXiv:2005.13420, 2020."
REFERENCES,0.34980237154150196,"Naomi Oreskes, Kristin Shrader-Frechette, and Kenneth Belitz. Veriﬁcation, validation, and conﬁr-
mation of numerical models in the earth sciences. Science, 263(5147):641–646, 1994."
REFERENCES,0.35177865612648224,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. In Hanna Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché
Buc, Emily Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 8026–8037. Curran Associates, Inc., 2019."
REFERENCES,0.35375494071146246,"Dimitris C. Psichogios and Lyle H. Ungar. A hybrid neural network-ﬁrst principles approach to
process modeling. AIChE Journal, 38(10):1499–1511, 1992. doi: https://doi.org/10.1002/aic.
690381003. URL https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690381003."
REFERENCES,0.3557312252964427,"Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686–707, 2019."
REFERENCES,0.3577075098814229,"Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden ﬂuid mechanics: Learning
velocity and pressure ﬁelds from ﬂow visualizations. Science, 367(6481):1026–1030, 2020a.
ISSN 0036-8075. doi: 10.1126/science.aaw4741."
REFERENCES,0.35968379446640314,"Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden ﬂuid mechanics: Learning
velocity and pressure ﬁelds from ﬂow visualizations. Science, 367(6481):1026–1030, 2020b."
REFERENCES,0.3616600790513834,Published as a conference paper at ICLR 2022
REFERENCES,0.36363636363636365,"Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens, Martin Jung, Joachim Denzler, Nuno Car-
valhais, et al. Deep learning and process understanding for data-driven earth system science.
Nature, 566(7743):195–204, 2019."
REFERENCES,0.36561264822134387,"R Rico-Martinez, JS Anderson, and IG Kevrekidis. Continuous-time nonlinear signal processing: a
neural network based approach for gray box identiﬁcation. In Proceedings of IEEE Workshop on
Neural Networks for Signal Processing, pp. 596–605. IEEE, 1994."
REFERENCES,0.3675889328063241,"M-H Rio and R Santoleri. Improved global surface currents from the merging of altimetry and sea
surface temperature data. Remote sensing of Environment, 216:770–785, 2018."
REFERENCES,0.3695652173913043,"Pierre C Sabatier. Past and future of inverse problems. Journal of Mathematical Physics, 41(6):
4082–4124, 2000."
REFERENCES,0.3715415019762846,"Steindor Saemundsson, Alexander Terenin, Katja Hofmann, and Marc Deisenroth. Variational inte-
grator networks for physically structured embeddings. In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 3078–3087. PMLR, 2020."
REFERENCES,0.37351778656126483,"Priyabrata Saha, Saurabh Dash, and Saibal Mukhopadhyay.
Phicnet:
Physics-incorporated
convolutional recurrent neural networks for modeling dynamical systems.
arXiv preprint
arXiv:2004.06243, 2020."
REFERENCES,0.37549407114624506,"Omer San and Romit Maulik. Machine learning closures for model order reduction of thermal ﬂuids.
Applied Mathematical Modelling, 60, 04 2018. doi: 10.1016/j.apm.2018.03.037."
REFERENCES,0.3774703557312253,"Russell Stewart and Stefano Ermon. Label-free supervision of neural networks with physics and
domain knowledge. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31,
2017."
REFERENCES,0.3794466403162055,"Daniel J Tait and Theodoros Damoulas. Variational autoencoding of pde inverse problems. arXiv
preprint arXiv:2006.15641, 2020."
REFERENCES,0.3814229249011858,"Michael L Thompson and Mark A Kramer. Modeling chemical processes using prior knowledge
and neural networks. AIChE Journal, 40(8):1328–1340, 1994."
REFERENCES,0.383399209486166,"Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. Accelerating Eulerian
ﬂuid simulation with convolutional networks. In Doina Precup and Yee Whye Teh (eds.), Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 3424–3433. PMLR, 06–11 Aug 2017."
REFERENCES,0.38537549407114624,"Kiwon Um, Robert Brand, Fei Yun, Philipp Holl, and Nils Thuerey. Solver-in-the-loop: Learning
from differentiable physics to interact with iterative pde-solvers. In Neural Information Process-
ing Systems (NeurIPS), volume 33, pp. 6111–6122, 2020."
REFERENCES,0.38735177865612647,"John von Neumann.
Functional Operators (AM-22), Volume 2: The Geometry of Orthogonal
Spaces. Princeton University Press, 1950. ISBN 978-1-4008-8189-5. doi: https://doi.org/10.
1515/9781400881895."
REFERENCES,0.3893280632411067,"Nils Wandel, Michael Weinmann, and Reinhard Klein. Learning incompressible ﬂuid dynamics
from scratch - towards fast, differentiable ﬂuid models that generalize. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=KUDUoRsEphu."
REFERENCES,0.391304347826087,"Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar.
Integrating
physics-based modeling with machine learning: A survey. arXiv preprint arXiv:2003.04919,
2020."
REFERENCES,0.3932806324110672,"Yuan Yin, Vincent Le Guen, Jérémie Dona, Emmanuel de Bezenac, Ibrahim Ayed, Nicolas Thome,
and Patrick Gallinari. Augmenting Physical Models with Deep Networks for Complex Dynamics
Forecasting. In ICLR, 2021."
REFERENCES,0.3952569169960474,"Chih-Chieh Young, Wen-Cheng Liu, and Ming-Chang Wu. A physically based and machine learning
hybrid approach for accurate rainfall-runoff modeling during extreme typhoon events. Applied
Soft Computing, 53:205–216, 2017."
REFERENCES,0.39723320158102765,Published as a conference paper at ICLR 2022
REFERENCES,0.39920948616600793,"A
DISTANCE"
REFERENCES,0.40118577075098816,"A.1
DISTANCE BETWEEN DYNAMICS"
REFERENCES,0.4031620553359684,"We here give the deﬁnition of the distance d. Let u and v be two functions of L2(Rp, Rp). We
consider the distance:
d(u, v) = EX∼pX∥u(X) −v(X)∥2
(18)"
REFERENCES,0.4051383399209486,"Naturally, eq. (18) veriﬁes the triangle inequality, the symmetry and the positiveness. Moreover, in
this case, for all functions f, d(., f) is convex. Indeed, for u, v two functions, and λ ∈[0, 1]:"
REFERENCES,0.40711462450592883,"d(λu + (1 −λ)v, f) = EX∼pX∥λu(X) + (1 −λ)v(X) −f(X)∥2
= EX∼pX∥λu(X) −λf(X) −(1 −λ)f(X) + (1 −λ)v(X)∥2
≤λEX∼pX∥u(X) −f(X)∥2 + (1 −λ)EX∼pX∥v(X) −f(X)∥2"
REFERENCES,0.4090909090909091,"Hence the convexity of d(., f). This consideration sufﬁces to ensure the convexity of Sk and Su
deﬁned in section 3."
REFERENCES,0.41106719367588934,"A.2
DISTANCE BETWEEN FLOWS"
REFERENCES,0.41304347826086957,"Consider the ODE with X(t), X0 ∈Rp: dX(t)"
REFERENCES,0.4150197628458498,"dt
= f(X(t)),
X(t = 0) = X0
(19)"
REFERENCES,0.41699604743083,"Equation (19) admits a unique solution as soon as f is Lipschitz. We note X⋆this solution. Then,
we can deﬁned the ﬂow φf of such ODE as :"
REFERENCES,0.4189723320158103,"[0,T]×Rp →Rp"
REFERENCES,0.4209486166007905,"t,
X0 →φf(t, X0) = X⋆(t)
(20)"
REFERENCES,0.42292490118577075,"With the deﬁnition of eq. (20), we can deﬁne the distance between two ﬂows of ODE as:"
REFERENCES,0.424901185770751,"dφ(φu, φf) = EX0∼pX0 Z τ"
REFERENCES,0.4268774703557312,"t0
∥φu(t, X0) −φf(t, X0)∥dt
(21)"
REFERENCES,0.4288537549407115,"dφ is positive and symmetric. Let φu, φv be two ﬂows, we have the triangle inequality:"
REFERENCES,0.4308300395256917,"dφ(φu, φf) = EX0∼pX0 Z τ"
REFERENCES,0.43280632411067194,"t0
∥φu(t, X0) −φf(t, X0)∥dt"
REFERENCES,0.43478260869565216,= EX0∼pX0 Z τ
REFERENCES,0.4367588932806324,"t0
∥φu(t, X0) −φv(t, X0) + φv(t, X0) + φf(t, X0)∥dt"
REFERENCES,0.43873517786561267,≤EX0∼pX0 Z τ
REFERENCES,0.4407114624505929,"t0
∥φu(t, X0) −φv(t, X0)∥+ ∥φv(t, X0) + φf(t, X0)∥dt"
REFERENCES,0.4426877470355731,"≤dφ(φv, φv) + dφ(φv, φf)"
REFERENCES,0.44466403162055335,"Let φf be ﬁxed, we also have the convexity of dφ(., φf) with respect to the ﬁrst argument. Indeed
for λ ∈[0, 1]:"
REFERENCES,0.44664031620553357,"dφ(λφu + (1 −λ)φv, f) = EX0∼pX0 Z τ"
REFERENCES,0.44861660079051385,"t0
∥λφu(t, X0) + (1 −λ)φv −φf(t, X0)∥dt"
REFERENCES,0.4505928853754941,= EX0∼pX0 Z τ
REFERENCES,0.4525691699604743,"t0
∥λφu(t, X0) + (1 −λ)φv −λφf(t, X0) −(1 −λ)φf(t, X0)∥dt"
REFERENCES,0.45454545454545453,"≤λ dφ(φu, φv) + (1 −λ)dφ(φv, φf)"
REFERENCES,0.45652173913043476,"However, in this case the convexity is not ensured with respect to u and vThis is the reason why for
theoretical investigations, we consider the distance d instead of dφ."
REFERENCES,0.45849802371541504,"Nonetheless, dφ(φu, φf) = 0 =⇒φu = φf =⇒u = f."
REFERENCES,0.46047430830039526,Published as a conference paper at ICLR 2022
REFERENCES,0.4624505928853755,"B
REMARK ON ADDITIVE DECOMPOSITION"
REFERENCES,0.4644268774703557,"First, note that in the case of a metric space the decomposition as deﬁned in eq. (2) always exists."
REFERENCES,0.466403162055336,We now detail an intuition for the well-posedness of such decomposition.
REFERENCES,0.4683794466403162,"Let Hk be a closed convex subset of functions of an Hilbert space (E, <, >), and f the function
we want to approximate with partial knowledge (represented by the space of hypothesis Hk). Then,
thanks to Hilbert projection lemma, we have the uniqueness of the minimizer of ming∈Hk ∥f −g∥,
i.e it exists one unique hk ∈Hk such that: ∀g ∈Hk, ∥f −hk∥≤∥f −g∥."
REFERENCES,0.47035573122529645,"Finally, the additive decomposition hypothesis presents a remarkable advantage in the case of a
vector space. Indeed, if Hk is a (closed) vector space, let H⊥
k be its supplementary in E, then we
have the uniqueness in the decomposition: f = fHk + fH⊥
k , where fH⊥
k ∈H⊥
k and fHk ∈Hk."
REFERENCES,0.4723320158102767,"The existence and uniqueness ﬂowing directly from the additive decomposition hypothesis, this can
explain why such assumption is common when bridging ML and MB hypothesis."
REFERENCES,0.4743083003952569,"C
UPPER BOUNDS"
REFERENCES,0.4762845849802372,"C.1
DERIVATION OF EQUATION (4)"
REFERENCES,0.4782608695652174,The ﬁrst upper bound is a simple use of the triangle inequality:
REFERENCES,0.48023715415019763,"d(h, f) = d(h, f) + d(hk, f) −d(hk, f)
≤d(hk, f) + |d(h, f) −d(hk, f)|
≤d(hk, f) + d(h, hk)"
REFERENCES,0.48221343873517786,"C.2
DERIVATION OF EQUATION (5)"
REFERENCES,0.4841897233201581,"To derive the second upper bound, we assume that f pr
k comes from an overall dynamics f pr obeying
the additive decomposition hypothesis of eq. (2) so that f pr and f pr
k veriﬁes: f pr = f pr
k +f pr
u . First,
with computations similar to eq. (4), we have:"
REFERENCES,0.48616600790513836,"d(h, f) ≤d(h, f pr) + d(f pr, f)
(22) Then:"
REFERENCES,0.4881422924901186,"d(h, f pr) = d(h, f pr) + d(hk, f pr
k ) −d(hk, f pr
k )"
REFERENCES,0.4901185770750988,"≤d(hk, f pr
k ) + |d(h, f pr) −d(hk, f pr
k )|"
REFERENCES,0.49209486166007904,"≤d(hk, f pr
k ) + |d(h, f pr) −d(h, f pr
k ) −d(h, f pr
k ) + d(hk, f pr
k )|"
REFERENCES,0.49407114624505927,"≤d(hk, f pr
k ) + |d(h, f pr) −d(h, f pr
k )| + |d(hk, f pr
k ) −d(h, hk)|"
REFERENCES,0.49604743083003955,"≤d(hk, f pr
k ) + d(f pr, f pr
k ) + d(h, hk)
(23)"
REFERENCES,0.4980237154150198,"Combining Equations (22) and (23), we retrieve eq. (5):"
REFERENCES,0.5,"d(h, f) ≤d(hk, f pr
k ) + d(h, hk) + d(f pr, f pr
k ) + d(f pr, f)
(24)"
REFERENCES,0.5019762845849802,"and we have: Γ = d(f pr, f pr
k ) + d(f pr, f). Γ is a constant of the problem that cannot be optimized."
REFERENCES,0.5039525691699605,"C.3
UPPER-BOUND USING AUXILIARY DYNAMICS f pr"
REFERENCES,0.5059288537549407,"Let f pr be the dynamics of model data, we can link up the error made by h on true data (following
dynamics f) and the error made by h on model data (with dynamics f pr) via:"
REFERENCES,0.5079051383399209,"d(h, f) ≤d(h, f pr) + d(f pr, f)
(25)"
REFERENCES,0.5098814229249012,"Thus a pre-training on auxiliary data of dynamics f pr amounts to control the term d(h, f pr) in the
upper-bound of eq. (25)."
REFERENCES,0.5118577075098815,Published as a conference paper at ICLR 2022
REFERENCES,0.5138339920948617,"C.4
SELF-SUPERVISION"
REFERENCES,0.5158102766798419,"Let h = hk + hu be the function to learn and Gψ the recognition network providing an estimate ˆθi
k
of the parameters from an initial sequence (Xi
t0, . . . , Xi
t0+k∆t). This learning setting corresponds to
how velocity ﬁelds are learned from consecutive measurements of the tracer ﬁelds T in section 4.2."
REFERENCES,0.5177865612648221,"To compute d(hk, f pr
k ) in the case where f pr = h⋆, where h⋆= h⋆
k +h⋆
u is a learned model, we rely
on the computed θk associated to h⋆
k (thanks for example to the algorithm of section 3.2 associated
to eq. (4)) to generate a synthetic dataset with achievable supervision in the space of the parameters
θk."
REFERENCES,0.5197628458498024,"From a real initial sequence (Xi
t0, . . . , Xi
t0+k∆t), we can estimate the unknown parame-
ter θi
k associated to sequence i with the recognition network G⋆
ψ learned with h⋆, i.e
θi
k = G⋆
ψ
 
Xi
t0, . . . , Xi
t0+k∆t

. Then, integrating from the initial condition Xi
t0, we generate a tra-
jectory of known parameters θi
k with dynamics h⋆denoted by: ˜Xi =
  ˜Xi
t0, . . . , ˜Xi
tn

. Sampling
the space of initial conditions, we obtain a synthetic dataset:
 
( ˜X1, θ1
k), . . . , ( ˜Xm, θm
k )

enabling us
to perform self-supervision for Gψ. Let ˆθi
k be the parameters estimated by Gψ from the simulated
  ˜Xi
t, . . . , ˜Xi
t+k∆t

, we make the following approximation:"
REFERENCES,0.5217391304347826,"d(hk, fk) ≈1 m m
X i=1"
REFERENCES,0.5237154150197628,"ˆθi
k −θi
k

2
(26)"
REFERENCES,0.525691699604743,"D
PROOFS"
REFERENCES,0.5276679841897233,"D.1
NOTE ON THE CONVEXITY OF Sk AND Su"
REFERENCES,0.5296442687747036,Convexity of Sk
REFERENCES,0.5316205533596838,"Proof. Let u, v ∈Sk:
d(tu + (1 −t)v, f) = ∥tu + (1 −t)v −f∥= ∥tu −tf + (1 −t)v −(1 −t)f∥
≤tµ1 + (1 −t)µ1 = µ1
Hence the convexity of Sk."
REFERENCES,0.5335968379446641,Convexity of Su
REFERENCES,0.5355731225296443,"Proof. Let t ∈[0, 1] and u, v ∈Su.
d(hk, hk + tu + (1 −t)v) = d(0, tu + (1 −t)v)
≤td(u, 0) + (1 −t)d(v, 0)
≤µ2
Hence the convexity of Su."
REFERENCES,0.5375494071146245,"D.2
ODE IDENTIFICATION"
REFERENCES,0.5395256916996047,"Consider the following set: SA = {X(t) ∈C1([0, T], Rp) such that: ∃A ∈Mp,p(R), X′ = AX},
where T > 0."
REFERENCES,0.541501976284585,"SA is not a convex set. Consider u and v in SA, and consider Au and Av so that u′(t) = Auu(t)
and v′(t) = Avv(t). For λ ∈]0, 1[: we have:
[λu + (1 −λ)v]′ = λu′ + (1 −λ)v′"
REFERENCES,0.5434782608695652,= λAuu + (1 −λ)Avv
REFERENCES,0.5454545454545454,"In general the last term is not equal to Aλu+(1−λ)v(λu + (1 −λ)v), for some matrix Aλu+(1−λ)v.
Thus SA is not a convex set. However, discretizing the trajectories and employing a simple integra-
tion scheme leads to considering the following cost function:"
REFERENCES,0.5474308300395256,"L(A) =
X"
REFERENCES,0.549407114624506,"t
∥
 
Xs(t + 1) −(A∆t + Id)XA(t)

∥2
2,
(27)"
REFERENCES,0.5513833992094862,Published as a conference paper at ICLR 2022
REFERENCES,0.5533596837944664,"As a least square regression problem, L(A) is convex with respect to A. A least square regres-
sion setting can also be recovered using more complex integration schemes, or several time steps
integration."
REFERENCES,0.5553359683794467,"D.3
PROOF FOR WELL-POSEDNESS OF EQUATION (7)"
REFERENCES,0.5573122529644269,"We set ourselves in the Hilbert space of squared integrable functions with the canonical scalar prod-
uct
 
L2(Rp, Rp), <, >

. For further consideration on such functional space we refer to (Droniou,
2001)."
REFERENCES,0.5592885375494071,We assume that Hk hence Sk is convex and a relatively compact family of functions.
REFERENCES,0.5612648221343873,"Convexity of Sk + Su
Let S = Sk + Su = {f |∃fk ∈Sk, fu ∈Su, f = fk + fu}."
REFERENCES,0.5632411067193676,"Let f, g ∈S and λ ∈]0, 1[:"
REFERENCES,0.5652173913043478,λf + (1 −λ)g = λfk + (1 −λ)gk + λfu + (1 −λ)gu ∈Sk + Su
REFERENCES,0.567193675889328,Hence the convexity of S.
REFERENCES,0.5691699604743083,"CLOSENESS OF Su
We show that Su is a closed set.
Indeed, Su = g−1([0, µu]), where
g(u) = ∥u∥, Because g is 1-Lipschitz (using the triangle inequality), g is continuous. Therefore
Su is closed set as the inverse image of a closed set by continuous function."
REFERENCES,0.5711462450592886,"Sequential Limit
We now show that S is a closed set thanks to the sequential characterisation: let
f n a converging sequence of elements of S and denote f its limit. We prove that f n converges in S."
REFERENCES,0.5731225296442688,"Because ∀n, fn ∈S, we have: f n = f n
k + f n
u , where f n
u ∈Su and f n
k ∈Sk."
REFERENCES,0.575098814229249,"Thanks to the relative compactness of Sk, we can extract a converging sub-sequence, of indexes nj,
from f n
k so that f nj
k
→fk ∈Sk."
REFERENCES,0.5770750988142292,"Because f n →f, the sub-sequence f nj converges: f nj →f."
REFERENCES,0.5790513833992095,"By deﬁnition, f nj
u
is a sequence of Su and we also have that: f nj
u
= f nj −f nj
k . Because the
right member of the equation converges (as a sum of converging functions), the left member of the
equation converges i.e. f nj
u
converges."
REFERENCES,0.5810276679841897,"Since Su is a closed set f nj
u
converges in Su.
We write fu its limit.
Therefore, f nj
u
=
f nj −f nj
k
→f −fk = fu ∈Su. Hence, f = fu + fk with fu ∈Su and fk ∈Sk."
REFERENCES,0.5830039525691699,Therefore S is a closed set.
REFERENCES,0.5849802371541502,"Finally, we can apply Hilbert projection lemma on the closed convex set S and retrieve the unique-
ness of the minimizer of eq. (7)."
REFERENCES,0.5869565217391305,"Remark
The relative compactness of a family of functions is a common assumption in functional
analysis. For example, in the study of differential equation Cauchy-Peano theorem provides the
existence to the solution of an ODE under the assumption of relative compactness."
REFERENCES,0.5889328063241107,"Also, Ascoli theorem provides the relative compactness of a family of function F under
the hypothesis of the equi-continuity of F and the relative compactness of the image space
A(x) = {f(x)|f ∈F}."
REFERENCES,0.5909090909090909,"D.4
PROOF OF PROPOSITION 2"
REFERENCES,0.5928853754940712,"We now set ourselves in the Hilbert space
 
L2([0, T], Rp), <, >

of squared integrable functions,
where <, > is the canonical scalar product of L2([0, T], Rp)."
REFERENCES,0.5948616600790514,"Proof. Let A be a given invertible matrix.
We consider the following space SD = {X ∈
C1([0, T], Rp) such that: ∃D ∈Rp, X′ = AX + D and X(t = 0) = X0}, where T > 0. We
show that SD is a closed convex set."
REFERENCES,0.5968379446640316,Published as a conference paper at ICLR 2022
REFERENCES,0.5988142292490118,"Convexity
Indeed, let λ ∈]0, 1[ and u, v ∈SD. λu + (1 −λ)v is differentiable and:"
REFERENCES,0.6007905138339921,"[λu + (1 −λ)v)]′ = λu′ + (1 −λ)v′ = A(λu + (1 −λ)v) + D,"
REFERENCES,0.6027667984189723,Where D = λDu + (1 −λ)Dv. Hence λu + (1 −λ)v ∈SD.
REFERENCES,0.6047430830039525,"Closeness via Afﬁne-Space
To prove the closeness of SD, we prove that it is an afﬁne space of
ﬁnite dimension."
REFERENCES,0.6067193675889329,Let g the application that to any vector D ∈Rd associate the solution XD.
REFERENCES,0.6086956521739131,"Let D0 ∈RD, we show that gD0 : D →g(D0 + D) −g(D0) is a linear application."
REFERENCES,0.6106719367588933,"Naturally, for gD0(0Rp) = 0L2. Then for D ̸= 0Rp we have:"
REFERENCES,0.6126482213438735,"gD0(D) = eAt(X0 + A−1(D0 + D)) −A−1(D0 + D) −eAt(X0 + A−1(D0) + A−1D0
= eAtA−1D"
REFERENCES,0.6146245059288538,Therefore gD0 is a linear function and g is an afﬁne function.
REFERENCES,0.616600790513834,"Moreover, g is an injection. Indeed, if two functions are equals, then they have at most one inverse
image by g thanks to Cauchy-Lipschitz theorem. Therefore it deﬁnes a bijection of Rd in g(Rd).
Since, SD = g(Rd), SD is an afﬁne space of dimension p and g is continuous in particular for the
canonical norm induced on L2([0, T], Rp). Therefore SD is an afﬁne space of ﬁnite dimension and
is a closed set."
REFERENCES,0.6185770750988142,"Finding a Unique Minimizer
We conclude by applying Hilbert projection lemma: our problem
of minimizing
R T
0
Xs(τ) −XD(τ)
, amounts to an orthogonal projection problem. Because SD
is a closed convex set, we have existence and uniqueness of such projection. Therefore, it exists a
unique function XD ∈SD and a unique vector D minimizing its distance to the function Xs."
REFERENCES,0.6205533596837944,"D.5
ALGORITHM IN LINEAR SETTING"
REFERENCES,0.6225296442687747,"We detail in Algorithm 2 the alternate projection algorithm in a linear setting.
We denote
Y = (Xi
t0+∆t, Xi
t0+n∆t) and X = (Xi
t0, Xi
t0+(n−1).∆t). For readability purposes we set ∆t = 1."
REFERENCES,0.6245059288537549,"Algorithm 2 Alternate estimation: Linear Setting
Result: A ∈Mp,p(R), D ∈Rp"
REFERENCES,0.6264822134387352,"k = 0, D0 = 0, A−1
0
= 0 A0
0 = minA∥Y −XA∥
while ∥Dk −Dk−1∥> ϵ and ∥Ak −Ak−1∥> ϵ do"
REFERENCES,0.6284584980237155,"Dk+1 = minD ∥Y −XAk −D∥2
2 + λ∥D∥2
2
Ak+1 = minA ∥Y −XA −Dk+1∥2
2 + γ∥Y −XA∥2
2
k ←k + 1
end"
REFERENCES,0.6304347826086957,"D.6
PROOF TO PROPOSITION 3"
REFERENCES,0.6324110671936759,"Naturally, one could estimate jointly D and A using least square regression. However, the idea is to
verify the convergence of such alternate algorithm in a simple case. We conduct the proof for the
ﬁrst dimension of Y to lighten notations, meaning that we are regressing the ﬁrst dimension of Y
against the X."
REFERENCES,0.6343873517786561,A similar reasoning for the other dimension completes the proof.
REFERENCES,0.6363636363636364,Proof. We ﬁrst give the analytical solution for D. Let An be ﬁxed.
REFERENCES,0.6383399209486166,"Estimation of D
Consider:"
REFERENCES,0.6403162055335968,"LD = ∥Y −XAn −D∥2
2 + λ∥D∥2
2
(28)"
REFERENCES,0.642292490118577,"where D = (d, . . . , d) ∈RQ. For Q samples, we ﬁnd d so that ∂L"
REFERENCES,0.6442687747035574,∂d = 0:
REFERENCES,0.6462450592885376,Published as a conference paper at ICLR 2022 ∂L
REFERENCES,0.6482213438735178,"∂d = 0 ⇔−2 ∗ Q
X"
REFERENCES,0.650197628458498,"i=1
(yi −XiAn −d) + 2λd = 0"
REFERENCES,0.6521739130434783,"⇔Qd + λd = Q
X"
REFERENCES,0.6541501976284585,"i=1
(yi −XiAn)"
REFERENCES,0.6561264822134387,"⇔d(Q + λ) = Q
X"
REFERENCES,0.658102766798419,"i=1
(yi −XiAn)"
REFERENCES,0.6600790513833992,⇔d = Y −XA
REFERENCES,0.6620553359683794,1 + λ/Q
REFERENCES,0.6640316205533597,where Y −XA = 1
REFERENCES,0.66600790513834,"Q
PQ
i=1(yi −XiAn)."
REFERENCES,0.6679841897233202,"Estimation of A
Let D be ﬁxed and consider:"
REFERENCES,0.6699604743083004,"LA = ∥Y −XA −D∥2
2 + γ∥Y −XA∥2
2
(29)"
REFERENCES,0.6719367588932806,"Similarly, we aim to cancel the ﬁrst derivative of LA with respect to all parameters of A =
(a1, .., ap): ∂LA"
REFERENCES,0.6739130434782609,"∂aj
= 0 ⇔−2 ∗ Q
X"
REFERENCES,0.6758893280632411,"i=1
xi,j(yi −a0xi,0 + · · · + apxi,p −d) −2γ ∗ Q
X"
REFERENCES,0.6778656126482213,"i=1
xi,j(yi −a0xi,0 + · · · + apxi,p) = 0"
REFERENCES,0.6798418972332015,⇔−2Xt(Y −XA −D) −2γXt(Y −XA) = 0
REFERENCES,0.6818181818181818,⇔(1 + γ)XtXA −Xt(Y −D) −γXtY = 0
REFERENCES,0.6837944664031621,"⇔(1 + γ)XtXA = Xt 
γY + (Y −D)
"
REFERENCES,0.6857707509881423,⇔A = B−1Xt
REFERENCES,0.6877470355731226,"1 + γ
 
(1 + γ)Y −D

(30)"
REFERENCES,0.6897233201581028,"where B = XtX. Equation (30) indicates that as soon a D converges, An converges. Thus, we now
prove the convergence of (Dn). Then, for n > 1 consider:"
REFERENCES,0.691699604743083,"Dn+1 −Dn =
1
1 + λ/Q"
REFERENCES,0.6936758893280632,Y −XAn −Y −XAn−1
REFERENCES,0.6956521739130435,"=
1
1 + λ/Q"
REFERENCES,0.6976284584980237,X(An −An−1)
REFERENCES,0.6996047430830039,"=
1
(1 + λ/Q)(1 + γ)"
REFERENCES,0.7015810276679841,"XB−1Xt 
[(1 + γ)Y −Dn] −[(1 + γ)Y −Dn−1
]"
REFERENCES,0.7035573122529645,"=
1
(1 + λ/Q)(1 + γ)"
REFERENCES,0.7055335968379447,XB−1Xt[Dn−1 −Dn]
REFERENCES,0.7075098814229249,"≤
K
(1 + λ/Q)(1 + γ)"
REFERENCES,0.7094861660079052,Dn−1 −Dn
REFERENCES,0.7114624505928854,where K = ∥XB−1Xt∥.
REFERENCES,0.7134387351778656,"Therefore, for λ, γ, sufﬁciently large,
K
(1+λ/Q)(1+γ) < 1. ∥Dn −Dn−1∥converges as a positive
decreasing sequence. Finally, the sequence of (Dn) converge and so the sequence of (An)."
REFERENCES,0.7154150197628458,"In conclusion, the proposed algorithm converges."
REFERENCES,0.717391304347826,Published as a conference paper at ICLR 2022
REFERENCES,0.7193675889328063,"E
DATASETS"
REFERENCES,0.7213438735177866,"In this section, we provide exhaustive simulation details for the damped pendulum, Lotka-Volterra,
and both geophysical datasets."
REFERENCES,0.7233201581027668,"E.1
DAMPED-PENDULUM"
REFERENCES,0.7252964426877471,"For the damped pendulum data, eq. (15) is integrated with ∆t = 0.2s using a Runge-Kutta 4-
5 scheme from t = 0 up to t = 10s. Both the pulsation ω0 and the damping coefﬁcient k are
ﬁxed across the dataset. We generate 100/50/50 sequences respectively for train, validation and test
sampling over the initial conditions so that (θ, ˙θ) ∼U
 
[−π/2, π/2] × [−0.1, 0.1]

."
REFERENCES,0.7272727272727273,"Small Oscillations
To linearize the pendulum, we consider the small oscillations regime and take
the initial conditions so that : (θ, ˙θ) ∼U
 
[−π/6, π/6] × [−0.1, 0.1]

. In that case eq. (15) writes
as:
d
dt"
REFERENCES,0.7292490118577075," ˙θ
θ"
REFERENCES,0.7312252964426877,"
=

−λ
g
L
1
0"
REFERENCES,0.733201581027668,"  ˙θ
θ"
REFERENCES,0.7351778656126482,"
(31)"
REFERENCES,0.7371541501976284,"and following notations of section 3.3, we have: DA = 0 and A =

−λ
g
L
1
0 "
REFERENCES,0.7391304347826086,"E.2
LOTKA-VOLTERRA"
REFERENCES,0.741106719367589,"For Lotka-Volterra data, eq. (16) is integrated with ∆t = 0.05 using a Runge-Kutta 4-5 scheme
from t = 0 up to t = 20. All parameters α, β, γ, δ are set to 1 across the dataset. We generate
100/50/50 sequences respectively for train, validation and test sampling over the initial prey and
predators populations so that (x, y) ∼U
 
[0, 2]2
."
REFERENCES,0.7430830039525692,"Practical Issues and Adaptation
Assuming that α and γ have positive values makes the following
problem arises: the trajectories deﬁned by hk for the prey are unbounded, whereas the trajectories
deﬁned by eq. (16) are. Minimizing d(hk, f) over long term horizon will lead in an underestimation
of α to match the bounded behaviour of true data. Therefore, we enforce d(hk, f) on the prey
component as soon as the number of predator is small. In practice, we set this threshold to 0.15."
REFERENCES,0.7450592885375494,"E.3
GEOPHYSICAL DATASETS"
REFERENCES,0.7470355731225297,"We present in this section introductory tools for the understanding of the ﬂuid dynamics data pre-
sented in section 4.2. We ﬁrst introduce the physical modeling of ocean dynamics. Then, we outline
the Adv+S dataset simulation which draws from ocean modeling. Finally, we introduce the Natl
dataset and the proxy data used in the experiments."
REFERENCES,0.7490118577075099,"Introduction To Ocean Modeling
The increase in ocean observations thanks to satellites and
ﬂoats enabled a great development in Earth modeling over the last decades. The ocean circula-
tion, that is the current velocity ﬁelds dynamics, are now realistically modeled in tri-dimensional
structured models such as NEMO (Madec, 2008)."
REFERENCES,0.7509881422924901,"Such models rely on in-depth physical knowledge of the studied system and its representation
through partial differential equations. Integrated over depth, the equations associated to the transport
of the Sea Surface Temperature T by a time-varying horizontal velocity ﬁeld U can be written as: ∂T"
REFERENCES,0.7529644268774703,"∂t = −∇.(TU) + DT + F T
(32) ∂U"
REFERENCES,0.7549407114624506,"∂t = −(U.∇)U + f ∧U −g′∇h + DU + F U
(33)"
REFERENCES,0.7569169960474308,"where f is the Coriolis parameter, h the depth of the surface layer obtained from sea surface height
(SSH) observations, g′ the reduced gravity which takes the stratiﬁcation in density of the ocean into
account such that g′ ≈g.10−3. In a two-dimensional setting, ∇(TU) refers to the advection of a"
REFERENCES,0.758893280632411,Published as a conference paper at ICLR 2022
REFERENCES,0.7608695652173914,"scalar quantity T by a velocity ﬁeld U = (u, v) and writes as : ∇(TU) = ∂T"
REFERENCES,0.7628458498023716,∂x u + ∂T
REFERENCES,0.7648221343873518,"∂y v. The mixing
terms, referred to as DT/U and the forcings F T/U, are not known."
REFERENCES,0.766798418972332,"In the context of the presented work, the physical state Zt = (Tt, Ut), fX and fY from eq. (1) can
be interpreted as follows: fX represents the dynamics of the observed T, i.e. fX(T) = −∇.(TU)+
DT + F T in eq. (32). fY represents the dynamics of U in eq. (33), i.e. fY (U, h) = −(U.∇)U +
f ∧U −g′∇h + DU + F U."
REFERENCES,0.7687747035573123,"Whereas T is observed by satellites, U is not known. However, the Sea Surface Height (SSH) could
be used to compute coarse estimates of U. Indeed, under hypothesis such as stationarity ( ∂U"
REFERENCES,0.7707509881422925,"∂t = 0),
incompressibility ((U.∇)U = 0)), forcings can be omited. In this case, eq. (33) can be rewritten
into
f ∧U = −g′∇h
(34)
When projected onto x and y axis, eq. (34) becomes"
REFERENCES,0.7727272727272727,−fv = −g′ ∂h
REFERENCES,0.7747035573122529,"∂x,
fu = −g′ ∂h"
REFERENCES,0.7766798418972332,"∂y ,
(35)"
REFERENCES,0.7786561264822134,"Note that eq. (34) and eq. (35) do not hold at ﬁne scales as the stationarity and incompressibility
assumptions only hold at large scale. In this case, the SSH h can be regarded as a stream function."
REFERENCES,0.7806324110671937,"Both datasets considered in the paper follow the same equations approximating the tracer equation
(eq. (17)) inspired by eq. (32):
∂T"
REFERENCES,0.782608695652174,"∂t = −∇.(TU) + S
(36)"
REFERENCES,0.7845849802371542,"We study the equations 32 and 33 in an incremental approach. In the following parts, we describe
how T, U and S are computed in both datasets Adv+S and Natl."
REFERENCES,0.7865612648221344,"E.3.1
ADV+S"
REFERENCES,0.7885375494071146,"We ﬁrst investigate a dataset generated following simplifying assumptions (Adv+S). We don’t rely
on true U and S, we instead build them so that they correspond to our hypothesis."
REFERENCES,0.7905138339920948,"Building a Velocity Field U
Under stationarity and incompressibility hypothesis, U can be ap-
proximated from a stream function H. Note that, in this dataset, H is not equal to the SSH h, it is
simulated following (Boffetta et al., 2001):"
REFERENCES,0.7924901185770751,"H(x, y, t) = −tanh[
y −B(t) × cos kx
p"
REFERENCES,0.7944664031620553,"1 + k2B(t)2 × sin2kx
] + cy,
(37)"
REFERENCES,0.7964426877470355,"As introduced precedently (see eq. (34)), eq. (33) can be simpliﬁed and we compute U = (u, v) so
that it follows:
u = −∂H"
REFERENCES,0.7984189723320159,"∂y ,
v = ∂H"
REFERENCES,0.8003952569169961,"∂x
(38)"
REFERENCES,0.8023715415019763,"Note that B varies periodically with time according to B = B0 + ϵ cos(ωt + φ). We compute 10
different velocity ﬁelds sampling random parameters B0, k, c, ω, ϵ, φ."
REFERENCES,0.8043478260869565,"Building a Source Term S
In eq. (32), the diffusion term DT is omitted. We generate the source
term S so that it represents the forcing term F T in eq. (36). To illustrate heat exchanges, we draw
from Frankignoul (1985). This source term is a non linear transformation of U = (u, v) multiplied
by the difference between the ocean temperature and a reference temperature:"
REFERENCES,0.8063241106719368,"S(U, T) = we × (T −Te)
where
we =

0
if ∂H"
REFERENCES,0.808300395256917,"∂t < 10−4
1
otherwise.
where Te is the sequence mean image (computed without source)."
REFERENCES,0.8102766798418972,"Dataset Generation
Using computed U and S, we integrate eq. (36) with ∆t = 8640s over
30 days, using a Semi-Lagrangian scheme (see explanations below). We generate 800/100/200
sequences respectively for train, validation and test sampling over the initial conditions, which are
images of size 64 × 64 sampled from Natl dataset. Finally, for integration, we impose East-West
periodic conditions, implying that what comes out the left part re-enters at the right, and reciprocally.
We also impose velocity to be null on both top and bottom parts of the image."
REFERENCES,0.8122529644268774,Published as a conference paper at ICLR 2022
REFERENCES,0.8142292490118577,"Semi-Lagrangian Integration
Unlike Eulerian scheme, relying on time discretization of the
derivative, the semi Lagrangian scheme relies on the constancy of the solution of a PDE along a
characteristic curve. Consider a solution to the advection equation, i.e. eq. (36) with S = 0. The
method of characteristics consists in exhibiting curves (x(s), t(s)) along which the derivative of the
solution T is simple, i.e ∂T"
REFERENCES,0.8162055335968379,"∂s (x(s), t(s)) = 0. For a 1D constant advection scheme, computations
lead to:"
REFERENCES,0.8181818181818182,"dt
ds = 1 =⇒s = t dx"
REFERENCES,0.8201581027667985,ds = U =⇒x = x0 + Ut
REFERENCES,0.8221343873517787,"giving therefore, T(x, t) = T0(x −Ut), linking the value of the solution at all time to its initial
condition. Therefore from a single observation at t0, it sufﬁces to estimate the original departure
points x0 −Ut to infer the prediction at t."
REFERENCES,0.8241106719367589,"However, when U is not constant in time, the method remains doable, not along characteristic lines
deﬁned by : (x0 + Ut), but along characteristic curves which are given by:"
REFERENCES,0.8260869565217391,"dt
ds = 1 =⇒s = t dx"
REFERENCES,0.8280632411067194,"ds = U(x, t)
(39)"
REFERENCES,0.8300395256916996,"A great deal in the semi-Lagrangian literature involves solving correctly eq. (39). We use the con-
ventional mid-point integration rule and the semi-Lagrangian is implemented using Pytorch function
gridsample, following in (Jaderberg et al., 2015). Further developments can be found for exam-
ple in (Diamantakis, 2014)."
REFERENCES,0.8320158102766798,"E.3.2
NATL"
REFERENCES,0.83399209486166,"This second dataset depicts the actual ocean circulation, i.e. we consider both eq. (32) and eq. (33).
In this case, no assumptions are made on U and S represents both diffusion terms DT and forcing
terms F T . We access daily data over a year of ocean surface temperature of the North Atlantic
observations model resulting from (Ajayi et al., 2019) 1. The dataset covers a 2300km × 2560km
zone at 1.5km resolution, in the North Atlantic Ocean."
REFERENCES,0.8359683794466403,"In this real-life dataset, sea surface height (SSH) partial derivative with respect to x and y serves as
proxies to the (unobserved) velocity ﬁelds U. Indeed, recall that simplifying hypotheses led us to
eq. (35)."
REFERENCES,0.8379446640316206,"We divide the Natl zone into 270 patches of size 64 × 64. For each region, we extract sea surface
temperatures, velocity ﬁelds, source terms and height variables. We sample 200/20/50 sequences of
1 year, for respectively train, validation and test. In this case, ∆t = 86400s (1 day)."
REFERENCES,0.8399209486166008,"F
TRAINING DETAILS"
REFERENCES,0.841897233201581,"All experiments were conducted on NVIDIA TITAN X GPU using Pytorch (Paszke et al., 2019)."
REFERENCES,0.8438735177865613,"Hyper-Parameters Interpretation
From eq. (4), two independent terms appear justifying an al-
ternate projections approach."
REFERENCES,0.8458498023715415,"First, we highlight that strictly minimizing d(hk, f) biases our estimation of hk. However, it may
yield a good estimation of hk provided that fk contributes signiﬁcantly to the prediction of f. Hence,
we interpret this loss as an initialization loss. Thus, in most applications, we progressively decrease
its magnitude along training as detailed in appendices F.1 to F.3."
REFERENCES,0.8478260869565217,"On the other hand, d(hu, 0) aims at constraining the free form function hu to make its action as
small as possible. We interpret this loss as a stability penalty."
REFERENCES,0.849802371541502,1Details available at : https://meom-group.github.io/swot-natl60/access-data.html
REFERENCES,0.8517786561264822,Published as a conference paper at ICLR 2022
REFERENCES,0.8537549407114624,"Finally, aiming to recover exact trajectories of observations, we proceed as suggested in (Yin et al.,
2021) progressively increasing the hyper-parameters associated to d(h, f)."
REFERENCES,0.8557312252964426,The practical implementation is summarized in the following algorithm:
REFERENCES,0.857707509881423,"Algorithm 3 Alternate estimation: Practical Setting
Initialization: θ0
u = 0, θ0
k = minhk∈Hk d(hk, f), λh, λhk, λhu
for epoch = 1 : Nepochs do"
REFERENCES,0.8596837944664032,for batch = 1 : Bk do
REFERENCES,0.8616600790513834,"θn+1
k
= θn
k −τ1∇θk[λhd(h, f) + λhkℓ(hk)]"
REFERENCES,0.8636363636363636,"end
for batch = Bk : Bu do"
REFERENCES,0.8656126482213439,"θn+1
u
= θn
u −τ1∇θu[λhd(h, f) + λhud(hu, 0)]"
REFERENCES,0.8675889328063241,"end
λh = τ2λh; λhk =
1
τ2 λhk; λhu =
1
τ2 λhu
end"
REFERENCES,0.8695652173913043,"F.1
DAMPED PENDULUM"
REFERENCES,0.8715415019762845,"Architecture Details
The physical parameters to be learned is a scalar of dimension 1, and hu is
a 1-hidden layer MLP with 200-hidden neurons with leaky-relu activation."
REFERENCES,0.8735177865612648,"Optimization
For this dataset we use RMSProp optimizer with learning rate 0.0004 for 100
epochs with batch size 128. We supervise the trajectories up to t = ∆t × 50, i.e we enforce dφ
over (t0 + ∆t, .., t0 + 50∆t). Overall the number of optimization subsequences for training is
17000. We alternate projection on Sk and Su by descending the gradient 10-batches on hk then
10-batches on hu."
REFERENCES,0.8754940711462451,"Hyperparameters
We initialize λhk = 0.1 and decrease it geometrically down to λhk = 0.001.
We initialize λh = 0.1 and increase it geometrically up to λh = 100. λhu is ﬁxed through training
at 0.1."
REFERENCES,0.8774703557312253,"The hyper-parameters were chosen by randomly exploring the hyper-parameters space by sampling
them so that λ ∼U(1, 0.1, . . . , 10−4). We select the ones with the lowest prediction errors, i.e with
lowest dφ(h, f)."
REFERENCES,0.8794466403162056,"For the ablation study of Table 1, we set to 0 the hyper-parameters associated to the non-considered
loss."
REFERENCES,0.8814229249011858,The training time for this dataset is 1 hour.
REFERENCES,0.883399209486166,"F.2
LOTKA-VOLTERRA"
REFERENCES,0.8853754940711462,"Architecture Details
The physical parameters to be learned is a vector of dimension 2 accounting
for (α, β) in eq. (16), and hu is a 1-hidden layer MLP with 200-hidden neurons with leaky-relu
activation."
REFERENCES,0.8873517786561265,"Optimization
We use Adam optimizer with learning rate 0.0005 for 200 epochs with batch size
128. Overall the number of sequences for training is 15000. We supervise the trajectories up to
t = ∆t × 25, i.e we enforce dφ over (t0 + ∆t, .., t0 + 25∆t). We alternate projection on Sk and Su
by descending the gradient 10-batches on hk then 10-batches on hu."
REFERENCES,0.8893280632411067,"Hyperparameters
We initialize λhk = 0.1 and decrease it geometrically down to λhk = 0.001.
We initialize λh = 0.001 and increase it geometrically up to λh = 1. λhu is ﬁxed through training
at 0.001."
REFERENCES,0.8913043478260869,Published as a conference paper at ICLR 2022
REFERENCES,0.8932806324110671,"The hyper-parameters were chosen by randomly exploring the hyper-parameters space by sampling
them so that λ ∼U(1, 0.1, . . . , 10−4). We select the ones with the lowest prediction errors (i.e
lowest d(h, f))."
REFERENCES,0.8952569169960475,"For the ablation study of Table 1, we set to 0 the hyper-parameters associated to the non-considered
loss."
REFERENCES,0.8972332015810277,The training time for this dataset is 2 hours.
REFERENCES,0.8992094861660079,"F.3
ADV+S"
REFERENCES,0.9011857707509882,"Architectures Details
The physical parameters to be estimated are the velocity ﬁelds U, of dimen-
sion (2, 64, 64). As U varies over time, we follow data assimilation principles to map a sequence
of 4 consecutive measurements of the tracer ﬁeld T to the associated velocity ﬁeld (Gaultier et al.,
2013). To do so, we parameterize a recognition network Gψ by U-net with at most 512 latent chan-
nels also following the implementation of (Isola et al., 2017), taking as input a sequence of 4 time
steps of T: (Tt0, .., Tt0+3∆t). The residual dynamics hu is learned by a convolutional ResNet, with
1 residual block taking as entry the same sequence of T. We implement hk via a semi-lagrangian
scheme, taking as input Tt and the estimated Ut to predict Tt+1."
REFERENCES,0.9031620553359684,"Optimization
We use Adam optimizer with learning rate 0.0001 for 30 epochs with batch size 32.
We supervise the trajectories up to t = ∆t × 6, i.e we enforce dφ on (Tt0+∆t, ..., Tt0+6∆t). Overall
the number of sequences for training is 36800. We alternate projection on Sk and Su by descending
the gradient 4-batches on hk then 6-batches on hu."
REFERENCES,0.9051383399209486,"Figure 3:
Best viewed in color. Schematic view of our model in the context of section 5.2, applied on the
Adv+S dataset."
REFERENCES,0.9071146245059288,"Hyperparameters, setting of eq. (4)
We initialize λhk = 0.1 and decrease it geometrically down
to λhk = 0.00001. We initialize λh = 0.01 and increase it geometrically every epoch up to λh,f =
1000. λhu is ﬁxed through training at 0.1. We select the hyperparameters with the lowest prediction
errors (i.e lowest d(h, f)). For the ablation study of Table 1, we set to 0 the hyper-parameters
associated to the non-considered loss."
REFERENCES,0.9090909090909091,The training time for this dataset is 8 hours.
REFERENCES,0.9110671936758893,"F.4
NATL"
REFERENCES,0.9130434782608695,"Architecture Details
The architectures in this setting are identical to the ones described in ap-
pendix F.3."
REFERENCES,0.9150197628458498,"Optimization
We use Adam optimizer with learning rate 0.00001 for 50 epochs with batch size
32. Overall the number of sequences for training is 67000. We enforce dφ over 6 time-steps, i.e we
supervise the predictions on timesteps: (t0 + ∆t, .., t0 + 6∆t). We use dropout in both Gψ and hu."
REFERENCES,0.9169960474308301,Published as a conference paper at ICLR 2022
REFERENCES,0.9189723320158103,"Hyperparameters, setting of eq. (4)
For this setting, λh geometrically increases from 0.01 up to
100. We initialize λhk = 0.1 and decrease it geometrically down to λhk = 0.00001. λhu is ﬁxed
through training at 0.1. We alternate projection on Sk and Su by descending the gradient 10-batches
on both hk and hu."
REFERENCES,0.9209486166007905,"The selected model is the one with lowest prediction errors on validation set (i.e lowest d(h, f)),
sampling uniformly the hyperparameters: λ ∼U(1, 0.1, . . . , 10−4)."
REFERENCES,0.9229249011857708,"Hyperparameters, setting of eq. (5)
Because the dynamics of Natl is highly non linear and
chaotic, we follow (Jia et al., 2019) and ﬁrst warm-up the parameters recognition network Gψ on
the velocity ﬁelds proxies for 10 epochs. For this setting, λh geometrically increase from 0.01 up to
1. λhk is set equal to λh. λhu is ﬁxed through training at 0.01."
REFERENCES,0.924901185770751,"After warm-up, we alternate projection on Sk and Su by descending the gradient 100-batches on
hk and 300 on hu. In this setting of eq. (5), the selected model is the one with lowest d(h, f) +
d(hk, f pr
k ) error, sampling uniformly the hyperparameters: λ ∼U(1, 0.1, . . . , 10−4)."
REFERENCES,0.9268774703557312,The training time for this dataset is 12 hours.
REFERENCES,0.9288537549407114,"Baselines
We train NODE (Chen et al., 2018) and Aphynity (Yin et al., 2021) on both the Adv+S
and Natl dataset. For the training of Aphinity, we set the learning rate at 0.0001 and train on 30
epochs. We initialize λh = 0.01 and increase it geometrically every epoch up to λh = 100. λhu
is ﬁxed through training at 0.1. For the training of NODE, we set the learning rate at 0.00004 and
train on 50 epochs. To perform prediction, we ﬁrst encode the 4-consecutive measurements of T (as
a 3 × 64 × 64 state) then learn to integrate this state in time thanks to a network h. h is a 3-layer
convolutional networks, with 64 hidden channels. It is integrated using RK4 scheme available from
https://github.com/rtqichen/torchdiffeq."
REFERENCES,0.9308300395256917,"G
ADDITIONAL RESULTS AND SAMPLES"
REFERENCES,0.932806324110672,"G.1
RESULTS FOR PENDULUM AND LOTKA-VOLTERRA DATASETS"
REFERENCES,0.9347826086956522,"We provide respectively in ﬁgs. 4 and 5 phase diagrams for the damped pendulum and Lotka-Volterra
experiments. Both graphs in the phase space indicate that the trajectories and their nature are well
handled by the learned decomposition, providing a periodic phase space for Lotka-Volterra (ﬁg. 5),
and a converging spiral for the damped pendulum (ﬁg. 4)."
REFERENCES,0.9367588932806324,"G.2
RESULTS FOR ADV+S AND NATL"
REFERENCES,0.9387351778656127,"In this section, we provide additionial results on both Adv+S and Natl datasets. A thorough abla-
tion study (table 4) gives results with constant hyperparameters λh and λhk (row Vanilla Optim),
which validates our hyper-parameters interpretation. Indeed, the results are better when respectively
increasing and decreasing λh and λhk. Besides, the row Ours eq. (5) refers to a training performed
as introduced in appendix C.4 with f pr = h⋆trained on eq. (4). Figure 7 shows predictions up to 4
days on the Adv+S data. Finally, ﬁgs. 9 and 11 provide results on Natl dataset associated to training
relying on both eq. (4) and eq. (5) and with NODE (Chen et al., 2018)."
REFERENCES,0.9407114624505929,Published as a conference paper at ICLR 2022
REFERENCES,0.9426877470355731,"0.6
0.4
0.2
0.0
0.2 0.03 0.02 0.01 0.00 0.01 0.02 0.03 0.04 0.05"
REFERENCES,0.9446640316205533,"true
prediction"
REFERENCES,0.9466403162055336,"Figure 4: Damped Pendulum Phase Diagram. The true phase diagram (blue) and learned (orange dashed) are
close, indicating consistency in the prediction"
REFERENCES,0.9486166007905138,"0.5
1.0
1.5
2.0
2.5"
REFERENCES,0.950592885375494,x (prey pop) 0.5 1.0 1.5 2.0 2.5
REFERENCES,0.9525691699604744,y (pred pop)
REFERENCES,0.9545454545454546,"true
prediction"
REFERENCES,0.9565217391304348,"Figure 5: Lotka-Volterra Phase Diagram. The true phase diagram (blue) and learned (orange dashed) are close,
indicating consistency in the prediction"
REFERENCES,0.958498023715415,Published as a conference paper at ICLR 2022
REFERENCES,0.9604743083003953,"Table 4: Ablation Study on Adv+S. We report the MSE (× 100) on the predicted observations T, the estimated
velocity ﬁelds U and the residual source term S over 6 and 20 time steps from an initial datum t0. Unlike
alternate training, i.e. Algorithm 1, “Joint” rows refer to the simultaneous optimization of hk and hu."
REFERENCES,0.9624505928853755,"Training
Models
t0 + 6
t0 + 20"
REFERENCES,0.9644268774703557,"T
U
S
T
U
S"
REFERENCES,0.9664031620553359,"Ours (U known)
0.52
n/a
0.19
2.0
n/a
0.32"
REFERENCES,0.9683794466403162,Alternate
REFERENCES,0.9703557312252964,"Ours eq. (4)
0.74
1.99
0.17
8.49
2.26
0.31
only d(h, f)
1.02
4.08
0.19
10.59
4.19
0.32
d(h, f) + d(hk, f)
1.02
3.66
0.19
11.42
3.84
0.34
d(h, f) + d(h, hk)
0.77
2.38
0.19
9.5
2.45
0.34
Ours eq. (5)
0.75
2.77
0.17
8.36
2.84
0.29"
REFERENCES,0.9723320158102767,"Vanilla optim.
1.51
3.77
0.3
13.33
4.1
5.15"
REFERENCES,0.974308300395257,"Joint
Ours eq. (4)
1.44
3.3
0.3
12.82
3.5
0.5
only d(h, f)
1.38
6.96
0.39
11.9
7.09
0.54"
REFERENCES,0.9762845849802372,"Figure 6:
Best viewed in color. Estimations, targets and differences between estimations and targets on T,
U = (u, v) and S for Adv+S. Each column refers to a time step, ranging from 1 to 6 half-days. On the left,
true and estimated U = (u, v) over 6 time steps, and differences between targets and estimations. On the right,
prediction of T and S over 6 time steps, and differences between targets and estimations."
REFERENCES,0.9782608695652174,Published as a conference paper at ICLR 2022
REFERENCES,0.9802371541501976,"Figure 7: Best viewed in color. Estimations and targets on T, U = (u, v) and S for Adv+S. Each column
refers to a time step, ranging from 1 to 8 half-days. On the left, sequence of T inputs (4 time steps). In the
middle, prediction of T, U = (u, v) and S over 8 time steps. On the right, true T, U and S over 8 time steps."
REFERENCES,0.9822134387351779,"Figure 8:
Best viewed in color. Estimations, targets and differences between estimations and targets on T,
U = (u, v) and S for Adv+S. Each column refers to a time step, ranging from 1 to 5 half-days. On the left,
true T, U and S over 5 time steps.. In the middle, prediction of T, U = (u, v) and S over 8 time steps. On the
right, differences between targets and estimations."
REFERENCES,0.9841897233201581,"Figure 9: Best viewed in color. Sequence of estimations on U = (u, v) for the Natl data. The second and third
row respectively refer to training according to eq. (4) and eq. (5). The loss term d(hk, f pr
k ) in eq. (5) enables
our model to learn more accurate velocity ﬁelds than when only trained following eq. (4)."
REFERENCES,0.9861660079051383,Published as a conference paper at ICLR 2022
REFERENCES,0.9881422924901185,"t+1
t+3
t+2
t+4
t+5
t+6"
REFERENCES,0.9901185770750988,True SST
REFERENCES,0.9920948616600791,Ours eq.5
REFERENCES,0.9940711462450593,Neural-ODE
REFERENCES,0.9960474308300395,"Figure 10: Best viewed in color. Sequence of prediction on T for the Natl data. Contrary to our model (row
eq. (5)), NODE (row Neural-ODE) struggles to predict any motion in T."
REFERENCES,0.9980237154150198,"Figure 11: Best viewed in color. Sequence of prediction on T, u, v, S for the Natl data across 3 days trained
using proxy data according to eq. (5)"
