Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0045045045045045045,"While instance-level explanation of GNN is a well-studied problem with plenty of
approaches being developed, providing a global explanation for the behaviour of a
GNN is much less explored, despite its potential in interpretability and debugging.
Existing solutions either simply list local explanations for a given class, or generate
a synthetic prototypical graph with maximal score for a given class, completely
missing any combinatorial aspect that the GNN could have learned. In this work
we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global
Explainer capable of generating explanations as arbitrary Boolean combinations
of learned graphical concepts. GLGExplainer is a fully differentiable architecture
that takes local explanations as inputs and combines them into a logic formula
over graphical concepts, represented as clusters of local explanations. Contrary
to existing solutions, GLGExplainer manages to provide accurate and human-
interpretable global explanations in both synthetic and real world datasets."
INTRODUCTION,0.009009009009009009,"1
Introduction"
INTRODUCTION,0.013513513513513514,"Graph Neural Networks (GNNs) have become increasingly popular for predictive tasks on graph
structured data. However, as many other deep learning models, their inner working remains a black
box. The ability to understand the reason for a certain prediction represents a critical requirement for
any decision-critical application, thus representing a big issue for the transition of such algorithms
from benchmarks to real-world critical applications."
INTRODUCTION,0.018018018018018018,"Over the last years, many works proposed Local Explainers [1–9] to explain the decision process
of a GNN in terms of factual explanations often represented as subgraphs for each sample in the
dataset. Overall, they shed light over why the network predicted a certain value for a specific input
sample. However, they still lack a global understanding of the model. Global Explainers, on the other
hand, are aimed at capturing the behaviour of the model as a whole, abstracting individual noisy local
explanations in favor of a single robust overview of the model. However, despite their potential in
interpretability and debugging little has been done in this direction [10]. GLocalX [11] is a general
solution to produce global explanations of black-box models by hierarchically aggregating local
explanations into global rules via an heuristic-based iterative procedure. This solution is however not
readily applicable to GNNs as it requires local explanations to be expressed as logical rules. Yuan et
al. [10] proposed to frame the Global Explanation problem for GNN as a form of input optimization,"
INTRODUCTION,0.02252252252252252,Global Explainability of GNNs via Logic Combination of Learned Concepts
INTRODUCTION,0.02702702702702703,"Figure 1: Illustration of the proposed method for a task of binary classification. Each step is described
in detail in Section 2 ."
INTRODUCTION,0.03153153153153153,"similarly as done for some vision models [12], using policy gradient to generate synthetic prototypical
graphs for each class. The approach requires prior domain knowledge, which is not always available,
to drive the generation of valid prototypes. Additionally, it cannot identify any compositionality in
the returned explanation, and has no principled way to generate alternative explanations for a given
class."
INTRODUCTION,0.036036036036036036,"Concept-based Explainability [13–15] is a parallel line of research where explanations are constructed
using “concepts” i.e., intermediate, high-level and semantically meaningful units of information com-
monly used by humans to explain their decisions. Concept Bottleneck Models [16] and Prototypical
Part networks [17] are two popular architectures that leverage concept learning to learn explainable-
by-design neural networks. Both approaches have been recently adapted to GNNs [18, 19]. However,
these solutions are not conceived for explaining already learned GNNs."
INTRODUCTION,0.04054054054054054,"Our contribution consists in the first Global Explainer for GNNs which i) provides a Global
Explanation in terms of logic formulas, extracted by combining in a fully differentiable manner
graphical concepts derived from local explanations; ii) is faithful to the data domain, i.e., the logic
formulas, being derived from local explanations, are intrinsically part of the input domain without
requiring any prior knowledge. We validated our approach on both synthetic and real-world datasets,
showing that our method is able to accurately summarize the behaviour of the model to explain, while
providing explanations in terms of concise logic formulas."
IMPLEMENTATION/METHODS,0.04504504504504504,"2
Proposed Method"
IMPLEMENTATION/METHODS,0.04954954954954955,"Our proposed Global Explainer, named GLGExplainer (Global Logic-based GNN Explainer), is
summarized in Figure 1. In the following we will describe each step in greater detail."
IMPLEMENTATION/METHODS,0.05405405405405406,"Local Explanations Extraction:
The first step of our pipeline consists in extracting local explana-
tions. Let LEXP(f, G) = ˆG be the weighted graph obtained by applying the local explainer LEXP to
generate a local explanation for the prediction of the GNN f over the input graph G. In principle,
every Local Explainer whose output can be mapped to a subgraph of the input sample is compatible
with our pipeline [1–6]. Nonetheless, in this work, we relied on PGExplainer [2] since it allows
the extraction of arbitrary disconnected motifs as explanations and it gave excellent results in our
experiments. By binarizing the output of the local explainer ˆG with threshold θ ∈R we achieve a
set of connected components ¯Gi such that S"
IMPLEMENTATION/METHODS,0.05855855855855856,"i ¯Gi ⊆ˆG. For convenience, we will henceforth refer to
each of these ¯Gi as local explanation. Given that we want to emulate the behaviour of f on correctly
predicted samples, we will discard every input graph G belonging to wrongly predicted samples."
IMPLEMENTATION/METHODS,0.06306306306306306,Global Explainability of GNNs via Logic Combination of Learned Concepts
IMPLEMENTATION/METHODS,0.06756756756756757,"Table 1: Mean and standard deviation for Fidelity, Formula Accuracy and Concept Purity computed
on the Test set over 5 runs with different random seeds. Since Concept Purity is computed for every
cluster independently, here we report mean and standard deviation for the best run only."
IMPLEMENTATION/METHODS,0.07207207207207207,"Dataset
Fidelity
Formula Accuracy
Concept Purity"
IMPLEMENTATION/METHODS,0.07657657657657657,"BAMultiShapes
0.99 ±0.00
0.99 ±0.00
0.85 ±0.22
Mutagenicity
0.85 ±0.01
0.85 ±0.01
0.99 ±0.01"
IMPLEMENTATION/METHODS,0.08108108108108109,"The result of this extraction thus consists in a list D of local explanations. More details about the
binarization are available in the Appendix."
IMPLEMENTATION/METHODS,0.08558558558558559,"Embedding Local Explanations:
The following step consists in learning an embedding for
each local explanation that allows to cluster together functionally similar local explanations. This
can be achieved with a standard GNN h which maps any graph ¯G into a fixed-sized embedding
h( ¯G) ∈Rd. Since each local explanation ¯G is a subgraph of an input graph G, in our experiments
we used the original node features of the dataset. The outcome of this aggregation consists in a set
E = {h( ¯G), ∀¯G ∈D} of graph embeddings."
IMPLEMENTATION/METHODS,0.09009009009009009,"Concept Projection:
Inspired by previous works on prototype learning [20, 21], we project each
graph embedding e ∈E into a set P of m ∈N prototypes {pi ∈Rd|i = 1, . . . , m} via a
distance function d(pi, e) = softmax

log( ∥e−p1∥2+1"
IMPLEMENTATION/METHODS,0.0945945945945946,"∥e−p1∥2+ϵ ), . . . , log( ∥e−pm∥2+1"
IMPLEMENTATION/METHODS,0.0990990990990991,"∥e−pm∥2+ϵ )
"
IMPLEMENTATION/METHODS,0.1036036036036036,"i. Prototypes are
initialized randomly from a uniform distribution and are learned along with the other parameters
of the architecture. As training progresses, the prototypes will align as prototypical representations
of every cluster of local explanations, which will represent the final groups of graphical concepts.
The output of this projection is thus a set V = {ve, ∀e ∈E} where ve = [d(p1, e), .., d(pm, e)] is a
vector containing the normalized probabilities of local explanation i belonging to the m concepts,
and will be henceforth referred to as concept vector."
IMPLEMENTATION/METHODS,0.10810810810810811,"Formulas Learning:
The final step consists of an E-LEN, i.e., a Logic Explainable Network [22]
implemented with an Entropy Layer as first layer [23]. An E-LEN learns to map a concept activation
vector to a class while encouraging a sparse use of concepts that allows to reliably extract Boolean
formulas emulating the network behaviour. We train an E-LEN to emulate the behaviour of the
GNN f feeding it with the graphical concepts extracted from the local explanations. Given a set of
local explanations ¯Ga . . . ¯Gni for an input graph Gi and a corresponding set of the concept vectors
va . . . vni, we aggregate the concept vectors via a pooling operator and feed the resulting aggregated
concept vector to the E-LEN, providing f(Gi) as supervision. In our experiments we used a max-
pooling operator. Thus, the Entropy Layer learns a mapping from the pooled concept vector to (i)
the embeddings z (as any linear layer) which will be used by the successive MLP for matching
the predictions of f. (ii) a truth table T explaining how the network leveraged concepts to make
predictions for the target class. Since the input pooled concept vector will constitute the premise
in the truth table T, a desirable property to improve human readability is discreteness, which we
achieved using the Straight-Through (ST) trick used for discrete Gumbel-Softmax Estimator [24]. In
practice, we compute the forward pass discretizing each vi via argmax, then, in the backward pass to
favor the flow of informative gradient we use its continuous version."
IMPLEMENTATION/METHODS,0.11261261261261261,"Supervision Losses:
Our proposed GLGExplainer is trained end-to-end with the following loss:
L = Lsurr + λ1LR1 + λ2LR2, where Lsurr corresponds to a Focal BCELoss [25] between the
prediction of our E-LEN and the predictions to explain, while LR1 and LR2 are respectively aimed to
push every prototype to be close to at least one local explanation and to push each local explanation
to be close to at least one prototype [20]. The losses are defined as follows:"
IMPLEMENTATION/METHODS,0.11711711711711711,"Lsurr = −y(1 −p)γ log p −(1 −y)pγ log(1 −p)
(1)"
IMPLEMENTATION/METHODS,0.12162162162162163,"LR1 = 1 m m
X"
IMPLEMENTATION/METHODS,0.12612612612612611,"j=1
min
¯G∈D ∥pj −h( ¯G)∥2
(2)"
IMPLEMENTATION/METHODS,0.13063063063063063,Global Explainability of GNNs via Logic Combination of Learned Concepts
IMPLEMENTATION/METHODS,0.13513513513513514,"Figure 2: Global explanations of GLGExplainer (ours) and XGNN. The class probability predicted
by XGNN for the generated explanations is around 1 for every explanation, except for Class 0 of
BAMultiShapes where it was not able to generate a graph with confidence ≥0.5."
IMPLEMENTATION/METHODS,0.13963963963963963,"LR2 =
1
|D| X"
IMPLEMENTATION/METHODS,0.14414414414414414,"¯G∈D
min
j∈[1,m] ∥pj −h( ¯G)∥2
(3)"
IMPLEMENTATION/METHODS,0.14864864864864866,"where p and γ represent respectively the probability for positive class prediction and the focusing
parameter which controls how much to penalize hard examples."
RESULTS/EXPERIMENTS,0.15315315315315314,"3
Experiments"
RESULTS/EXPERIMENTS,0.15765765765765766,"We tested our proposed approach on two datasets, namely:"
RESULTS/EXPERIMENTS,0.16216216216216217,"BAMultiShapes: BAMultiShapes is a newly introduced extension of some popular synthetic bench-
marks [1] aimed to assess the ability of a Global Explainer to deal with logical combinations of
concepts. In particular, we created a dataset composed of Barabási-Albert (BA) graphs with attached
in random positions the following network motifs: house, grid, wheel. Class 0 contains plain BA
graphs and BA graphs enriched with a house, a grid, a wheel, or the three motifs together. Class 1
contains BA graphs enriched with a house and a grid, a house and a wheel or a wheel and a grid."
RESULTS/EXPERIMENTS,0.16666666666666666,"Mutagenicity: The Mutagenicity dataset is a collection of molecule graphs where each graph is
labelled as either having a mutagenic effect or not. Based on [26], the mutagenicity of a molecule is
correlated with the presence of electron-attracting elements conjugated with nitro groups (e.g. NO2)."
RESULTS/EXPERIMENTS,0.17117117117117117,"For Mutagenicity we replicated the model accuracy and the local explanations presented in [2], while
for BAMultiShapes we trained until convergence a 3-layers GCN. Details about the implementation
and the pre-processing of local explanations, along with model accuracies, are in the Appendix."
RESULTS/EXPERIMENTS,0.17567567567567569,"In order to show the robustness of our proposed methodology, we have evaluated GLGExplainer on
a number of metrics, namely: FIDELITY, FORMULA ACCURACY, and CONCEPT PURITY. A
detailed description of those metrics can be found in the Appendix. Table 1 reports the results in terms
of the three metrics, showing how GLGExplainer manages to provide reliable explanations under all
these perspectives. Note that XGNN [10], the only available competitor for global explanations of
GNN, cannot be evaluated according to these metrics. Figure 2 presents the final global explanations
where we substituted each literal with its corresponding prototypical graphical concept, and report
the explanations generated by XGNN for comparison. It’s easy to see that GLGExplainer produces
highly interpretable explanations that match the ground-truth formula (for BAMultiShapes) and
existing knowledge (for Mutagenesis) with remarkable accuracy. It is worth mentioning that the
global explanations for Class 0 of BAMultiShapes do not comprise the case with all three motifs
together. We observed that the reason resides in the GNN to explain failing at classifying every
sample with such structure. So, GLGExplainer is effectively explaining the GNN f and not simply"
RESULTS/EXPERIMENTS,0.18018018018018017,Global Explainability of GNNs via Logic Combination of Learned Concepts
RESULTS/EXPERIMENTS,0.18468468468468469,"the dataset structure. Conversely, XGNN fails to generate interpretable explanations in most cases.
Details about concepts compositions and formula extraction are available in the Appendix."
CONCLUSION/DISCUSSION,0.1891891891891892,"4
Discussion & Conclusions"
CONCLUSION/DISCUSSION,0.19369369369369369,"Given the results presented in the section above, it is worth noting that concept clusters emerge solely
based on the supervision defined in Section 2, while no specific supervision was added to cluster local
explanations based on their similarity. Further details about the clusters’ composition are available in
the Appendix. Overall, the results confirm the ability of GLGExplainer in providing logic formulas,
expressed over learned graphical concepts, which are accurately summarizing the global behaviour of
the model, whereas the existing XGNN fails at providing concise and faithful explanations."
CONCLUSION/DISCUSSION,0.1981981981981982,Acknowledgements
CONCLUSION/DISCUSSION,0.20270270270270271,"This research was partially supported by TAILOR, a project funded by EU Horizon 2020 research
and innovation programme under GA No 952215"
REFERENCES,0.2072072072072072,References
REFERENCES,0.21171171171171171,"[1] Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks, 2019. URL https://arxiv.org/abs/
1903.03894. 1, 2, 4"
REFERENCES,0.21621621621621623,"[2] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang
Zhang. Parameterized explainer for graph neural network, 2020. URL https://arxiv.org/
abs/2011.04573. 2, 4, 7"
REFERENCES,0.22072072072072071,"[3] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural
networks via subgraph explorations, 2021. URL https://arxiv.org/abs/2102.05152."
REFERENCES,0.22522522522522523,"[4] Minh N. Vu and My T. Thai. Pgm-explainer: Probabilistic graphical model explanations for
graph neural networks, 2020. URL https://arxiv.org/abs/2010.05788."
REFERENCES,0.22972972972972974,"[5] Caihua Shan, Yifei Shen, Yao Zhang, Xiang Li, and Dongsheng Li. Reinforcement learning
enhanced explainer for graph neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.
Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,
volume 34, pages 22523–22533. Curran Associates, Inc., 2021. URL https://proceedings.
neurips.cc/paper/2021/file/be26abe76fb5c8a4921cf9d3e865b454-Paper.pdf."
REFERENCES,0.23423423423423423,"[6] Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoff-
mann. Explainability methods for graph convolutional neural networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10772–10781, 2019.
2"
REFERENCES,0.23873873873873874,"[7] Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local
interpretable model explanations for graph neural networks. IEEE Transactions on Knowledge
and Data Engineering, 2022."
REFERENCES,0.24324324324324326,"[8] Wanyu Lin, Hao Lan, and Baochun Li. Generative causal explanations for graph neural networks.
In International Conference on Machine Learning, pages 6666–6679. PMLR, 2021."
REFERENCES,0.24774774774774774,"[9] Wanyu Lin, Hao Lan, Hao Wang, and Baochun Li. Orphicx: A causality-inspired latent variable
model for interpreting graph neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 13729–13738, 2022. 1"
REFERENCES,0.25225225225225223,"[10] Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. XGNN: Towards model-level explanations
of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery &amp Data Mining. ACM, aug 2020. doi: 10.1145/3394486.3403085.
URL https://doi.org/10.1145%2F3394486.3403085. 1, 4"
REFERENCES,0.25675675675675674,"[11] Mattia Setzu, Riccardo Guidotti, Anna Monreale, Franco Turini, Dino Pedreschi, and Fosca
Giannotti. GLocalX - from local to global explanations of black box AI models. Artificial
Intelligence, 294:103457, may 2021. doi: 10.1016/j.artint.2021.103457. URL https://doi.
org/10.1016%2Fj.artint.2021.103457. 1"
REFERENCES,0.26126126126126126,Global Explainability of GNNs via Logic Combination of Learned Concepts
REFERENCES,0.26576576576576577,"[12] Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R. Lyu, and Yu-Wing
Tai. Towards global explanations of convolutional neural networks with concept attribution.
In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
8649–8658, 2020. doi: 10.1109/CVPR42600.2020.00868. 2
[13] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James Wexler, Fernanda B. Viégas,
and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with con-
cept activation vectors (TCAV). In International Conference on Machine Learning (ICML),
volume 80 of Proceedings of Machine Learning Research, pages 2673–2682. PMLR, 2018. 2
[14] Amirata Ghorbani, James Wexler, James Y. Zou, and Been Kim. Towards automatic concept-
based explanations. In Neural Information Processing Systems (NeurIPS), pages 9273–9282,
2019.
[15] Chih-Kuan Yeh, Been Kim, Sercan Ömer Arik, Chun-Liang Li, Tomas Pfister, and Pradeep
Ravikumar. On completeness-aware concept-based explanations in deep neural networks. In
Neural Information Processing Systems (NeurIPS), 2020. 2
[16] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim,
and Percy Liang. Concept bottleneck models. In International Conference on Machine Learning
(ICML), volume 119 of Proceedings of Machine Learning Research, pages 5338–5348. PMLR,
2020. 2
[17] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su.
This looks like that: Deep learning for interpretable image recognition. Advances in Neural
Information Processing Systems, 32:8930–8941, 2019. 2
[18] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. Protgnn: Towards
self-explaining graph neural networks. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 36, pages 9127–9135, 2022. 2
[19] Dobrik Georgiev, Pietro Barbiero, Dmitry Kazhdan, Petar Veliˇckovi´c, and Pietro Liò. Algorith-
mic concept-based explainable reasoning. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 36, pages 6685–6693, 2022. 2
[20] Oscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning
through prototypes: A neural network that explains its predictions, 2017. URL https://
arxiv.org/abs/1710.04806. 3
[21] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This
looks like that: Deep learning for interpretable image recognition. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf. 3
[22] Gabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Marco Gori, Pietro Lió, Marco
Maggini, and Stefano Melacci. Logic explained networks. arXiv preprint arXiv:2108.05149,
2021. 3
[23] Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Pietro Lió, Marco Gori, and Stefano
Melacci. Entropy-based logic explanations of neural networks, 2021. URL https://arxiv.
org/abs/2106.06804. 3, 14
[24] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax,
2016. URL https://arxiv.org/abs/1611.01144. 3
[25] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense
object detection, 2017. URL https://arxiv.org/abs/1708.02002. 3
[26] Asim Kumar Debnath, Rosa L. Lopez de Compadre, Gargi Debnath, Alan J. Shusterman, and
Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic
nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal
of Medicinal Chemistry, 34(2):786–797, 1991. doi: 10.1021/jm00106a046. URL https:
//doi.org/10.1021/jm00106a046. 4
[27] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907, 2016. 7, 14
[28] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018. 7, 14"
REFERENCES,0.2702702702702703,Global Explainability of GNNs via Logic Combination of Learned Concepts
REFERENCES,0.2747747747747748,"[29] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems, 30, 2017. 14
[30] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. stat, 1050:20, 2017. 14"
APPENDIX,0.27927927927927926,"A
Appendix"
APPENDIX,0.28378378378378377,"A.1
Training the GNN f"
APPENDIX,0.2882882882882883,"For both BAMultiShapes and Mutagenicity we relied on the codebase provided by [2] for training
the GNN f to explain and to train the Local Explainer. For BAMultiShapes we trained a 3-layers
GCN [27] (20-20-20 hidden units) with mean graph pooling for the final prediction, whereas for
Mutagenicty we reproduced the results of [2]. A summary of model’s performance is available in
Table 2. Despite the high accuracy over BAMultiShapes, after a closer look we observed that the
network did not actually learn the All concept, i.e., the three motifs together. Such detailed view is
available in Table 3. This explains why the global explanations in Figure 2 Class 0 do not present
such concept."
APPENDIX,0.2927927927927928,"Table 2: GNN accuracies for BAMultiShapes and Mutagenicity. The results for Mutagenicity are in
line with the one reported in [2]."
APPENDIX,0.2972972972972973,"Split
BAMultiShapes
Mutagenicity"
APPENDIX,0.30180180180180183,"Train
0.94
0.87
Val
0.94
0.86
Test
0.99
0.86"
APPENDIX,0.3063063063063063,"Table 3: Accuracy of the model on the train set of BAMultiShapes with respect to every combination
of motifs to be added to the Barabási-Albert base graph. H, G, W stand respectively for House, Grid,
and Wheel."
APPENDIX,0.3108108108108108,"Class 0
Class 1"
APPENDIX,0.3153153153153153,"Motifs
∅
H
G
W
All
H + G
H + W
G + W
Accuracy (%)
1.0
1.0
0.85
1.0
0.0
1.0
0.98
1.0"
APPENDIX,0.31981981981981983,"A.2
Local Explanations Processing"
APPENDIX,0.32432432432432434,"As detailed in [2], the output of PGExplainer consists in a weighted edge mask wij ∈V × V where
each wij is the likelihood of the edge being an important edge. For Mutagenicity, we sticked to the
original implementation which was correctly able to reproduce the results presented in the paper [2].
The only difference resides in the procedure for cutting the explanation, which is needed to remove
from the final local explanation the edges which were assigned low scores. The authors in [2] limited
their analysis to graphs that were containing the ground truth motifs, and proposed to just keep the
top-k edges. We, instead, selected the numeric threshold θ ∈R which maximises the F1 score of the
explainer over all graphs. Afterwards, such threshold will be used to cut out the irrelevant edges, by
applying the indicator function 1wij≥θ to the edge mask. The resulting edge mask is thus the binary
adjacency matrix of the final explanation. For BAMultiShapes, however, we adopted a dynamic
algorithm to select θ that does not require any prior knowledge about the ground truth motifs. This
algorithm resembles the elbow-method, i.e., for each local explanation choose as θ the first value that
is different enough from the previous ordered values. Figure 3 shows some examples for each dataset
along with their local explanations in bold."
APPENDIX,0.32882882882882886,"A.3
The GLGExplainer"
APPENDIX,0.3333333333333333,"The reference implementation of our Local Explanation Embedder h is constituted by a 2-layers
GIN [28] network with 20 hidden units, followed by a non-linear combination of max, mean, and"
APPENDIX,0.33783783783783783,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.34234234234234234,Class 0
APPENDIX,0.34684684684684686,"Class 1 C C C C C C O O O H H H H H H N N C C
C C C C
C C
C C O H H H H H
H H H H H
H H H H H N C C C C C C C C C C C C
C C C C C O O O O H H H H H H H
H H H H H H H C C C
C C C C C O O O O H H
H H H H H H
N N"
APPENDIX,0.35135135135135137,"Class 0 C C C C C C C C C C O O H H H H H H H H H N S C C C C C
C C C C C C
C C C O O O O O O H H H H H H H N N N C C C C
C C C C
C C O O O O H H H H
H H N N C C C C C C C C C C H H H H H H H H H
H H H H H C C C C C C C C H H H H H H H H Br Br C C C C C C H H H H H
H H H H H H H H H H H
H H H H H H H
H N N N N N N N N N P P P C C C C C C C C H H H H H H H H H H"
APPENDIX,0.35585585585585583,"Class 1 C C C C C C C C O O Cl
Cl Cl H H H H H H H C C C C C
C C C C C H
H H H H H H H H
H H H H H C
C C C C C C C C C C C C C C C C C C C C C C C C
C C C
C C C C C O O O O
O O H H
H
H H H H H H HH H
H H H H H H H
H H
H HH H H H H H
H H H H
H H H H
H H
H H
H H H H H H
H
H H N N N
N S"
APPENDIX,0.36036036036036034,"Figure 3: Random examples of input graphs along with their explanations in bold as extracted by
PGExplainer, for respectively BAMultiShapes and Mutagenicity."
APPENDIX,0.36486486486486486,"sum graph pooling. We chose a number m of 6 and 2 prototypes for, respectively, BAMultiShapes
and Mutagenicity, keeping the dimensionality d to 10. We trained using ADAM optimizer with
early stopping and with a learning rate for h and the prototypes P of 1e−3, while for the E-LEN of
5e−4. The batch size is set to 128, while the auxiliary loss coefficients λ1 and λ2 are chosen via
cross-validation and set respectively to 0.09 and 0.00099, while the focusing parameter γ is kept fixed
at 2. The E-LEN is constituted by the input Entropy Layer (Entr.Layer : Rm →R10), a hidden
layer (HiddenLayer : R10 →R5), and the output layer with LeakyReLU activation function."
APPENDIX,0.36936936936936937,"In the rest of this section we provide an ablation study to demonstrate the effectiveness of the Focal
loss, the Discretization trick, and the impact of the number of prototypes in use."
APPENDIX,0.3738738738738739,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.3783783783783784,"Focal loss: Figure 4 presents a comparison of the learning curve for BAMultiShapes showing that
using Focal loss with a focusing parameter of 2 helps to achieve a faster convergence while not being
detrimental for the overall performances."
APPENDIX,0.38288288288288286,"0
200
400
600
800
epochs 0.0 0.2 0.4 0.6 0.8 1.0"
APPENDIX,0.38738738738738737,fidelity (%)
APPENDIX,0.3918918918918919,Fidelity
APPENDIX,0.3963963963963964,"W Focal loss
W/o Focal loss"
APPENDIX,0.4009009009009009,"0
200
400
600
800
epochs 0.0 0.2 0.4 0.6 0.8 1.0"
APPENDIX,0.40540540540540543,accuracy (%)
APPENDIX,0.4099099099099099,Formula Accuracy
APPENDIX,0.4144144144144144,"W Focal loss
W/o Focal loss"
APPENDIX,0.4189189189189189,"0
200
400
600
800
epochs 0.4 0.6 0.8 1.0"
APPENDIX,0.42342342342342343,purity (%)
APPENDIX,0.42792792792792794,Concept Purity
APPENDIX,0.43243243243243246,W Focal loss
APPENDIX,0.4369369369369369,W/o Focal loss
APPENDIX,0.44144144144144143,"Figure 4: Learning curves for BAMultiShapes with and without Focal loss. Results show that the
Focal loss, with a focusing parameter set to 2, helps to achieve faster convergence while not reducing
the final performances."
APPENDIX,0.44594594594594594,"Number of prototypes: An effective approach to select an appropriate value m for the number of
prototypes in use is via cross-validation, and by selecting the smallest m which achieves a competitive
fidelity. In Figure 5 we show how different values of m impact the Fidelity and the Formula Accuracy."
APPENDIX,0.45045045045045046,"Discretization trick: The Discretization trick was introduced in Section 2 to enforce a discrete
prototype assignment, something essential for an unambiguous definition of the concepts on which
the formulas are based on. In Figure 6 we show for BAMultiShapes that this trick is also effective in
improving the overall performance of GLGExplainer, since it forces the hidden layers of the E-LEN
to just exploit the information relative to the closest prototype, while not relying on other positional
information. Thus, the E-LEN’s predictions are much more aligned with the discrete formulas being
extracted. In the Figure we further compare against a plain model without Discretization and against
the addition to the overall loss of an entropy loss over the concept vector (Concept Entropy loss)
with different scaling parameters λ3 ∈{0.01, 0.1}. This Concept Entropy loss (CE loss) pushes the
pre-pooling concept vector to have low entropy, thus effectively pushing every local explanation to
be assigned with confidence to just one prototype."
APPENDIX,0.45495495495495497,"A.4
Cluster Composition & Formulas Renaming"
APPENDIX,0.4594594594594595,"To effectively explore the content of each local explanations cluster, we plot in Figure 7 some random
elements for each dataset. In most cases, the clusters contain atomic motifs (House, Grid, NO2,
etc..) while in others the embedder h clustered together heterogeneous motifs. This is particularly
evident for the cluster relative to the prototype p3 of BAMultiShapes in which every local explanation
comprising two atomic motifs are aggregated. The reason for this behaviour is that we are aggregating
local explanation solely based on the ability of the E-LEN to emulate the predictions of f. Thus,
since the simultaneous presence of two motifs appears only in Class 1, one single cluster aggregating
all these mixed local explanations is enough for maximizing the performances. This is also the reason"
APPENDIX,0.46396396396396394,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.46846846846846846,"0
200
400
600
800
1000
epochs 0.0 0.2 0.4 0.6 0.8 1.0"
APPENDIX,0.47297297297297297,fidelity (%)
APPENDIX,0.4774774774774775,Fidelity
APPENDIX,0.481981981981982,"2 prots
4 prots
6 prots
8 prots"
APPENDIX,0.4864864864864865,"0
200
400
600
800
1000
epochs 0.0 0.2 0.4 0.6 0.8 1.0"
APPENDIX,0.49099099099099097,accuracy (%)
APPENDIX,0.4954954954954955,Formula Accuracy
APPENDIX,0.5,"2 prots
4 prots
6 prots
8 prots"
APPENDIX,0.5045045045045045,"0
100
200
300
400
500
600
epochs 0.0 0.2 0.4 0.6 0.8"
APPENDIX,0.509009009009009,fidelity (%)
APPENDIX,0.5135135135135135,Fidelity
APPENDIX,0.5180180180180181,"2 prots
4 prots
6 prots
8 prots"
APPENDIX,0.5225225225225225,"0
100
200
300
400
500
600
epochs 0.0 0.2 0.4 0.6 0.8"
APPENDIX,0.527027027027027,accuracy (%)
APPENDIX,0.5315315315315315,Formula Accuracy
APPENDIX,0.536036036036036,"2 prots
4 prots
6 prots
8 prots"
APPENDIX,0.5405405405405406,"Figure 5: Ablation study on the number of prototypes to use. The first row is referred to BAMulti-
Shapes, while the second to Mutagenicity."
APPENDIX,0.545045045045045,"Table 4: Raw formulas as extracted by the Entropy Layer. Each formula was rewritten following the
Closed-World Assumption for convenience."
APPENDIX,0.5495495495495496,"Dataset
Raw Formulas"
APPENDIX,0.5540540540540541,"BAMultiShapes
Class0 ⇐⇒P0 ∨P5 ∨P1 ∨P4 ∨P2 ∨(P4 ∧P2)
Class1 ⇐⇒P3 ∨(P5 ∧P2) ∨(P5 ∧P1) ∨(P2 ∧P1)"
APPENDIX,0.5585585585585585,"Mutagenicity
Class0 ⇐⇒P1 ∨(P0 ∧P1)
Class1 ⇐⇒P0"
APPENDIX,0.5630630630630631,"for the high variability in Concept Purity reported in Table 1, since it is computed considering the
Purity in terms of labelled atomic motifs. For completeness, we additionally report in Figure 8 a 2D
PCA-reduced view of the embedding space, annotated with the prototypes position."
APPENDIX,0.5675675675675675,"Given that the default implementation of the Entropy layer returns formulas expressed in terms of the
single concepts in input, Figure 7 is also useful to rename each literal into its corresponding graphical
concept. Table 4 shows an example of such raw formulas, while Figure 2 presents the final formulas
after replacing each raw name with the corresponding graphical concept."
APPENDIX,0.5720720720720721,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.5765765765765766,"0
200
400
600
800
epochs 0.0 0.2 0.4 0.6 0.8 1.0"
APPENDIX,0.581081081081081,fidelity (%)
APPENDIX,0.5855855855855856,Fidelity
APPENDIX,0.5900900900900901,"W Discretization
W/o Discretization
W CE loss 0.01
W CE loss 0.1"
APPENDIX,0.5945945945945946,"0
200
400
600
800
epochs 0.0 0.2 0.4 0.6 0.8 1.0"
APPENDIX,0.5990990990990991,accuracy (%)
APPENDIX,0.6036036036036037,Formula Accuracy
APPENDIX,0.6081081081081081,"W Discretization
W/o Discretization
W CE loss 0.01
W CE loss 0.1"
APPENDIX,0.6126126126126126,"0
200
400
600
800
epochs 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75"
APPENDIX,0.6171171171171171,entropy
APPENDIX,0.6216216216216216,Concept Vector Entropy
APPENDIX,0.6261261261261262,"W Discretization
W/o Discretization
W CE loss 0.01
W CE loss 0.1"
APPENDIX,0.6306306306306306,"Figure 6: Ablation study for the impact of the Discretization trick, discussed in Section 2. We
compared the performances with and without it, and against the addition of a Concept Entropy loss
(CE loss) with different scaling parameters λ3."
APPENDIX,0.6351351351351351,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.6396396396396397,"P0
 Others C O
O O S C O O N S
C
C C O O O S C O O N S"
APPENDIX,0.6441441441441441,"P1
 NO2 C O O N O O N O O N O O N O O N"
APPENDIX,0.6486486486486487,Figure 7: Random representative elements for each prototype in BAMultiShapes and Mutagenicity.
APPENDIX,0.6531531531531531,"1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
0.3 0.2 0.1 0.0 0.1 0.2 0.3 p0
p1 p2 p3 p4 p5"
APPENDIX,0.6576576576576577,local explanations embeddings
APPENDIX,0.6621621621621622,"house
grid
wheel
ba
house+grid
house+wheel
wheel+grid"
APPENDIX,0.6666666666666666,"1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
0.3 0.2 0.1 0.0 0.1 0.2 0.3"
APPENDIX,0.6711711711711712,prototype assignments
APPENDIX,0.6756756756756757,"p0
p1
p2
p3
p4
p5"
APPENDIX,0.6801801801801802,principal comp. 1
APPENDIX,0.6846846846846847,principal comp. 2
APPENDIX,0.6891891891891891,"0.4
0.2
0.0
0.2
0.4
0.6 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 p0 p1"
APPENDIX,0.6936936936936937,local explanations embeddings
APPENDIX,0.6981981981981982,"NO2
Others"
APPENDIX,0.7027027027027027,"0.4
0.2
0.0
0.2
0.4
0.6 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8"
APPENDIX,0.7072072072072072,"prototype assignments p0
p1"
APPENDIX,0.7117117117117117,principal comp. 1
APPENDIX,0.7162162162162162,principal comp. 2
APPENDIX,0.7207207207207207,"Figure 8: 2D view of the embedding space annotated with prototypes positions. The first line is
referred to BAMultiShapes, while the second to Mutagenicity."
APPENDIX,0.7252252252252253,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.7297297297297297,"A.5
Benefits and Limitations"
APPENDIX,0.7342342342342343,"As previously discussed, the proposed GLGExplainer is inherently faithful to the data domain since
it processes local explanations provided by a Local Explainer. However, the quality of those local
explanations, in terms of representativeness and discriminability with respect to the task-specific class,
has a direct effect on the Fidelity. If the generated concept vector does not exhibit any class-specific
pattern, then the E-LEN will not be able to emulate the predictions of the model to explain. Despite
being a potential limitation of GLGExplainer, this can actually open to the possibility of using the
Formula Accuracy as a proxy of local explanations quality, which is notoriously difficult to assess.
We leave this investigation to future work. Despite tailoring our discussion on graph classification,
our approach can be readily extended to any kind of classification task on graphs, provided that a
suitable Local Explainer is available."
APPENDIX,0.7387387387387387,"A.6
Evaluation Metrics"
APPENDIX,0.7432432432432432,Here we will describe in mode detail the metrics briefly introduced in Section 3:
APPENDIX,0.7477477477477478,"• Fidelity measures the accuracy between the prediction of the E-LEN and the one of the GNN
to explain. It is computed as the accuracy between the class predictions of the E-LEN and the
GNN f.
• Formula Accuracy represents how well the learned formulas can correctly predict the class
labels. To compute this metric, we treat the final formulas as a classifier that given an input
concept vector predicts the class corresponding to the clause evaluated to true. In the cases in
which either no clause or more clauses of different classes are evaluated to be true, the sample is
always considered as wrongly predicted.
• Concept Purity is computed for every cluster independently and measures how good the embed-
ding is at clustering the local explanations. Specifically, it requires each local explanation to be
annotated with a label, which in our cases corresponds to the typology of the motif represented
by the local explanation. Then, the computation of the metric can be summarized by:"
APPENDIX,0.7522522522522522,ConceptPurity(Ci) = count_most_frequent_label(Ci)
APPENDIX,0.7567567567567568,"|Ci|
(4)"
APPENDIX,0.7612612612612613,"where Ci corresponds to the cluster having pi as prototype (i.e., the cluster containing every local
explanation associated to prototype pi by the distance function d(., .) described in Section 2).
count_most_frequent_label(Ci) instead returns the number of local explanations annotated
with the most present label in cluster Ci. The Concept Purity results reported in Table 1 are
computed by taking the mean and the standard deviation across the m clusters."
APPENDIX,0.7657657657657657,"A.7
Additional Experiments"
APPENDIX,0.7702702702702703,"In this section, we report further experimental results in addition to those presented in Section 3."
APPENDIX,0.7747747747747747,"A.7.1
Multi-Class Dataset"
APPENDIX,0.7792792792792793,"To challenge GLGExplainer also in a multi-class setting, we extended the previously introduced
BAMultiShapes dataset with an additional class, constituted by the usual random BA base graph
with attached house-like motifs and a cycle of length 6. We will henceforth refer to this dataset as
BAMultiShapesMC. As for BAMultiShapes, node features are represented by a fixed vector with
values 0.1. Similarly as done for BAMultiShapes, we trained an extension of the 3-layers GCN
used before, using both a max and mean aggregator for graph pooling. The resulting accuracies are
0.95% and 0.98% for respectively the train and test set. We kept the same setting for extracting and
processing local explanations, with the only difference that we used 128 hidden units for PGExplainer,
instead of the original 64, to favour the extraction of good local explanations which otherwise were
of very low quality. For GLGExplainer, we kept almost all of the hyper-parameters presented in
Appendix A.3. We found, however, to be very beneficial for the final embedding to use m = 8. Note
that this is a reasonable modification, given that we added completely new motifs and the overall
dataset composition is more complex."
APPENDIX,0.7837837837837838,"In Table 5 we report the results over 5 different random seeds, while in Figure 10 we provide an
illustration of the PCA 2D-reduced embedding for the best run along with the relative formulas"
APPENDIX,0.7882882882882883,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.7927927927927928,"Table 5: Mean and standard deviation for Fidelity, Formula Accuracy and Concept Purity computed
on the Test set over 5 runs with different random seeds. Since the Concept Purity is computed for
every cluster independently, here we report mean and standard deviation for the best run only."
APPENDIX,0.7972972972972973,"Dataset
Fidelity
Formula Accuracy
Concept Purity"
APPENDIX,0.8018018018018018,"BAMultiShapesMC
0.97 ±0.02
0.97 ±0.01
0.82 ±0.23"
APPENDIX,0.8063063063063063,Table 6: Raw formulas as extracted by the Entropy Layer along with their test Fidelity.
APPENDIX,0.8108108108108109,"Dataset
Raw Formulas
Fidelity"
APPENDIX,0.8153153153153153,BAMultiShapesMC
APPENDIX,0.8198198198198198,"Class0 ⇐⇒
P0 ∨P2 ∨P3 ∨P5 ∨P7"
APPENDIX,0.8243243243243243,"0.98
Class1 ⇐⇒"
APPENDIX,0.8288288288288288,"P1 ∨P6 ∨(P0 ∧P2) ∨(P0 ∧P5) ∨
(P0 ∧P7) ∨(P5 ∧P7) ∨(P6 ∧P7) ∨
(P5 ∧P6) ∨(P0 ∧P4) ∨(P0 ∧P6) ∨
(P2 ∧P6) ∨(P2 ∧P7) ∨(P3 ∧P5 ∧P6)"
APPENDIX,0.8333333333333334,"Class2 ⇐⇒
P4 ∨(P3 ∧P5) ∨(P2 ∧P3) ∨
(P3 ∧P4) ∨(P2 ∧P3 ∧P5)"
APPENDIX,0.8378378378378378,"in Table 6. As it is possible to inspect from the output of the E-LEN, the resulting extracted raw
formulas are still well representing the underlying ground truth modelling correctly the presence
of the new class, despite containing some additional noise. For example, given the overlapping
between some prototype assignments (like the cluster of P5 that, even if representing the vast majority
of BA base graphs, it contains some spurious houses, or similarly for P3 that despite containing
every local explanation representing a circle, contains a few BA graphs) the E-LEN learned some
spurious clauses which are not correctly modelling the underlying ground truth. Those cases represent
however the real minority of cases, being the overall formulas and Fidelity well aligned with the
results obtained for BAMultiShapes, where the quality of local explanations allowed more distinctive
clusters. Note indeed that as described in [23] and implemented in the official codebase1, it is possible
to rank the clauses in the truth table T created by the E-LEN by their support (for how many samples
they hold), allowing to select only the top-ranked clauses either by evaluating on a validation set,
or by specifying a minimum support. Nonetheless, we did not apply such filter in order to stick to
the experimental setting previously defined. In the same vein, despite the possibility of arbitrarily
augmenting the local explanations’ node features with any hand-crafted feature in order to better
separate the clusters, we kept only the original datasets’ node features."
APPENDIX,0.8423423423423423,"As mentioned in Appendix A.5, GLGExplainer can be used to get insights into the Local Explainer
in use. To this end, it is possible to understand the reason behind, for example, the clause P1 for
Class1 in Table 6 by the fact that each of the ∼20 samples in this cluster corresponds to the external
border of the grid-like motif, which are extracted for the vast majority in Class1. This means that, in
this specific case, PGExplainer had a bias in extracting solely the border of the grid motif in Class1,
ignoring the other motif present in the same sample (recall that every sample of Class1 has two
motifs). An example of such motif along with the overall sample-wise local explanations are reported
in Figure 9."
APPENDIX,0.8468468468468469,"A.7.2
Impact of the GNN Architecture on the Embedder h"
APPENDIX,0.8513513513513513,"In our previous experiments we implemented the Local Explanations Embedder h as a 2-layers GIN
network. However, any compatible GNN architecture can be used instead. In Figure 11 we present
an ablation study testing different GNN architectures, and comparing their respective performances
in terms of Fidelity and Concept Purity. For every architecture under analysis we kept the number
of layers equal to 2. Specifically, we tested GCN [27], GIN [28], SAGE [29], and GAT [30]. For
BAMultiShapes, in which are present different and heterogeneous motifs, architectures with a sum
local aggregator like SAGE and GIN seem to be preferable over GCN, where presumably the node-
degree normalization in its propagation rule limits the richness of the learned representation. Given
the absence of informative node features, GAT is not able to properly learn a useful embedding. For"
APPENDIX,0.8558558558558559,1https://pypi.org/project/torch-explain/
APPENDIX,0.8603603603603603,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.8648648648648649,Partial Grid Motifs
APPENDIX,0.8693693693693694,"Graph id: 889
Graph id: 936
Graph id: 960"
APPENDIX,0.8738738738738738,"Figure 9: Illustration of the grid’s external border only local explanation extracted for some samples
of Class1, along with three examples of instance-level local explanations with the selected explanation
in bold. The expected local explanation is comprised by both the entire grid and the house."
APPENDIX,0.8783783783783784,"Mutagenicity, instead, given the presence of informative node features, and given the absence of rich
topological motifs, GAT and GCN perform comparably to GIN and SAGE."
APPENDIX,0.8828828828828829,Global Explainability of GNNs via Logic Combination of Learned Concepts
APPENDIX,0.8873873873873874,"0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25
0.4 0.2 0.0 0.2 0.4 0.6 p0 p1"
APPENDIX,0.8918918918918919,"p2
p3
p4 p5 p6 p7"
APPENDIX,0.8963963963963963,local explanations embeddings
APPENDIX,0.9009009009009009,"house
grid
wheel
circle
ba
house+grid
house+wheel
wheel+grid
house+circle
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.25
0.4 0.2 0.0 0.2 0.4"
APPENDIX,0.9054054054054054,"0.6
prototype assignments"
APPENDIX,0.9099099099099099,"p0
p1
p2
p3
p4
p5
p6
p7"
APPENDIX,0.9144144144144144,principal comp. 1
APPENDIX,0.918918918918919,"Figure 10: 2D view of the embedding space annotated with prototypes positions for the train split of
BAMultiShapesMC."
APPENDIX,0.9234234234234234,"0
200
400
600
800
1000
1200
epochs 0.0 0.2 0.4 0.6 0.8 1.0"
APPENDIX,0.9279279279279279,fidelity (%)
APPENDIX,0.9324324324324325,Fidelity
APPENDIX,0.9369369369369369,"GIN
GAT
SAGE
GCN"
APPENDIX,0.9414414414414415,"0
200
400
600
800
1000
1200
epochs 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90"
APPENDIX,0.9459459459459459,concept purity (%)
APPENDIX,0.9504504504504504,Concept Purity
APPENDIX,0.954954954954955,"GIN
GAT
SAGE
GCN"
APPENDIX,0.9594594594594594,"0
50
100
150
200
250
300
350
epochs 0.0 0.2 0.4 0.6 0.8"
APPENDIX,0.963963963963964,fidelity (%)
APPENDIX,0.9684684684684685,Fidelity
APPENDIX,0.972972972972973,"GIN
GAT
SAGE
GCN"
APPENDIX,0.9774774774774775,"0
50
100
150
200
250
300
350
epochs 0.75 0.80 0.85 0.90 0.95"
APPENDIX,0.9819819819819819,concept purity (%)
APPENDIX,0.9864864864864865,Concept Purity
APPENDIX,0.990990990990991,"GIN
GAT
SAGE
GCN"
APPENDIX,0.9954954954954955,"Figure 11: Ablation study on the architecture in use for the Local Explanations Embedder h. The
first line is referred to BAMultiShapes, while the last to Mutagenicity"
