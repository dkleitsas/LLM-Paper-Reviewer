Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0034965034965034965,"The lack of relevant physical constraints in data-driven models of physical systems,
such as neural network parameterized partial differential equations (PDEs), might
lead to unrealistic modeling outcomes. A majority of approaches to solving this
problem are based on forcing a model to satisfy a set of equations representing
physical constraints. Currently available approaches can enforce a very limited
set of constraints and are applicable only to uniform spatial grids. We propose a
method for enforcing general pointwise, differential and integral constraints on
unstructured spatial grids. Our method is based on representing a model’s output in
terms of a function approximation and enforcing constraints on that approximation.
We demonstrate wide applicability and strong performance of our approach in
data-driven learning of dynamical PDE systems and distributions of physical ﬁelds."
INTRODUCTION,0.006993006993006993,"1
INTRODUCTION"
INTRODUCTION,0.01048951048951049,"Multiple works have shown the capability of neural networks to solve complex physical problems and
learn the behavior of physical systems from data. Examples include learning and solving ordinary
differential equations (ODEs) [6], partial differential equations (PDEs) [28; 20] and rigid body
dynamics [31; 5]. Purely data-driven models are typically not forced to satisfy physical constraints of
the system that generated the data. This might lead to unrealistic predictions that violate some known
properties of the underlying physical system."
INTRODUCTION,0.013986013986013986,"Incorporation of relevant constraints allows to make a better use of the available data and makes
predictions more physically plausible. The ﬁeld dealing with physics-constrained learning is diverse
and offers many approaches to adding constraints to models. We refer the reader to many reviews for
details [30; 3; 36; 19]. The approach we consider in this work is based on forcing a model to satisfy
algebraic constraints represented by a set of equalities and inequalities. This is the most commonly
used approach which allows to represent a wide range of constraints and has been shown to work
well in many cases [18; 17; 25]. However, while many constraints can be represented algebraically, it
is not always clear how to evaluate and enforce them."
INTRODUCTION,0.017482517482517484,"Currently available approaches to enforcing algebraic constraints are limited to uniform grids and
have a very narrow range of constraints they can enforce (e.g. only pointwise, or speciﬁc differential
constraints), see Section 5 for details of related work. Such approaches can be readily applied
to models based on convolutional neural networks (CNNs) but cannot be extended to recently
developed models based on graph neural networks (GNNs) [33; 27; 15] and other models working on
unstructured grids."
INTRODUCTION,0.02097902097902098,"We propose a much more general method which allows to enforce pointwise, differential and integral
constraints on unstructured spatial grids and demonstrate its applicability in learning of PDE-driven
dynamical systems and distributions of physical ﬁelds. The method is based on using a models’s
output at the nodes of a grid to construct an interpolant and applying constraints directly to that
interpolant (Section 3)."
INTRODUCTION,0.024475524475524476,Code and data will be made publicly available.
INTRODUCTION,0.027972027972027972,Under review as a conference paper at ICLR 2022
BACKGROUND,0.03146853146853147,"2
BACKGROUND"
BACKGROUND,0.03496503496503497,"PDE-driven dynamical systems.
Many physical systems can be described in terms of PDEs. Such
systems are deﬁned on a bounded domain on which they evolve over time. We consider continuous
dynamical systems with state u(t, x) ∈Rp that evolves over time t ∈R≥0 and spatial locations
x ∈Ω⊂RD. For physical systems, D is typically limited to {1, 2, 3} although our method will
work with any value of D. We assume the system is governed by an unknown PDE"
BACKGROUND,0.038461538461538464,"∂u(t, x)"
BACKGROUND,0.04195804195804196,"∂t
= F(x, u(t, x), ∇xu(t, x), ∇2
xu(t, x), ...)
(1)"
BACKGROUND,0.045454545454545456,"which describes the temporal evolution of the system in terms of the locations x, state u and its ﬁrst
and higher-order partial derivatives w.r.t. x. The goal of a data-driven PDE model is to learn the
dynamics F from data."
BACKGROUND,0.04895104895104895,"Data for learning F is collected by measuring the state of the system at observation locations
(x1, . . . , xN) over increasing time points (t0, . . . , tM). This results in a dataset (y(t0), . . . , y(tM)),
where y(ti) = (u(ti, x1), . . . , u(ti, xN)) is a collection of observations. The dataset is used to train
the model to predict (y(t1), . . . , y(tM)) starting from the initial state y(t0). Training is typically
done by minimizing an average loss between the model’s predictions u(t) and the data y(t)."
BACKGROUND,0.05244755244755245,"PDE models differ in restrictions they impose on time points (temporal grid) and observation
locations (spatial grid). Some models require both grids to be uniform [23], other models relax
these requirements and allow arbitrary spatial [27] and spatio-temporal grids [15]. We build our
algebraic constraints method using the model from [15] as the most general one. The model is based
on application of the method of lines [32] to Equation 1 which results into a system of ODEs"
BACKGROUND,0.055944055944055944,"˙u(t) :=  
"
BACKGROUND,0.05944055944055944,"du(t,x1)"
BACKGROUND,0.06293706293706294,"dt...
du(t,xN) dt  
≈  
"
BACKGROUND,0.06643356643356643,"Fθ(x1, xN(1), u1, uN(1))
...
Fθ(xN, xN(N), uN, uN(N)) "
BACKGROUND,0.06993006993006994,"

(2)"
BACKGROUND,0.07342657342657342,"which approximates the solution of Equation 1 at the observation locations xi using their neighboring
points N(i), where xN(i) and uN(i) are the neighbors’ positions and states respectively, and ui is
u(t, xi). The approximate solution converges to the true solution as N increases. The true dynamics
F is approximated by a parametric model Fθ whose parameters θ are learned by minimizing the
difference between the model’s predictions"
BACKGROUND,0.07692307692307693,"u(t) = u(0) +
Z t"
BACKGROUND,0.08041958041958042,"0
˙u(τ)dτ
(3)"
BACKGROUND,0.08391608391608392,"and the data y(t). The integral in Equation 3 is solved using a numerical ODE solver. In [15], the
function Fθ was represented by a graph neural network (GNN) which takes states and locations at an
observation point i and its neighboring points N(i). The observation points are connected into a grid
using Delaunay triangulation which allows to naturally deﬁne N(i) as a set of points connected to the
point i. However, Fθ can be represented by other models and a different neighbor selection criterion
can be used. The model parameters θ are learned by minimizing the MSE between y(t) and u(t)"
BACKGROUND,0.08741258741258741,"Ldata = 1 M M
X"
BACKGROUND,0.09090909090909091,"i=1
∥u(ti) −y(ti)∥2
2.
(4)"
BACKGROUND,0.0944055944055944,The gradient of Ldata w.r.t. θ is evaluated using the adjoint method as shown in [7].
BACKGROUND,0.0979020979020979,"Generative Adversarial Networks
One of the tasks that we consider is learning distributions of
physical ﬁelds. For that purpose we utilize generative adversarial networks (GANs). A GAN is a
generative model consisting of a generator and a discriminator [12]. The generator, G, learns to
transform a random variable Z ∼pZ over a latent space Z to the data space Y in such a way that the
discriminator, D, cannot tell the difference between samples generated by G and samples from the
data distribution pdata. Both, G and D are learned by solving the following minimax problem"
BACKGROUND,0.10139860139860139,"min
G max
D V (G, D) = EY ∼pdata [log D(Y )] + EZ∼pZ [log (1 −D(G(Z)))] .
(5)"
BACKGROUND,0.1048951048951049,"Solution of this problem exists and is unique with the optimal generator perfectly mimicking the data
distribution [12]."
BACKGROUND,0.10839160839160839,Under review as a conference paper at ICLR 2022
METHODS,0.11188811188811189,"3
METHODS"
METHODS,0.11538461538461539,"In this section we presents an approach to evaluating pointwise, differential and integral constraints
on unstructured grids. Then, we demonstrate how this approach can be used to enforce arbitrary soft
and linear hard constraints."
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.11888111888111888,"3.1
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.12237762237762238,"We assume the data y(t) is available at observation points (x1, . . . , xN) and time points (t1, . . . , tM)
and that a model makes predictions u(t) at these points. We assume the predictions to be evaluations
of an unknown underlying function. Since the underlying function is unknown, we cannot impose
constraints on it directly. Instead, we approximate it by an interpolant uf(t, x) and impose constraints
on uf(t, x) (Figure 1). The approximation is constructed from u(t) by placing a basis function at
each xi and representing uf(t, x) as"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.1258741258741259,"uf(t, x) = N
X"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.12937062937062938,"j=1
αj(t)φj(x),
(6)"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.13286713286713286,"Figure 1: Example of approx-
imating an unknown under-
lying function (green) by an
interpolant (red) constructed
from observations u1, . . . , u4."
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.13636363636363635,"where φj is a scalar basis function at xj and αj ∈Rp. The coefﬁ-
cients αj(t) are obtained from u(t) (see Section 3.4)."
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.13986013986013987,"Next, we show how to evaluate constraints on uf(t, x) using basic
building blocks. To avoid cluttered notation, we consider equality
constraints and assume u(t, x), x ∈R. Generalization to inequality
constraints, vector ﬁelds and higher spatial dimensions is straight-
forward."
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.14335664335664336,"Pointwise constraints.
Consider points z = (z1, . . . , zK) in Ωon
which a pointwise constraint h(uf(t, zi)) = 0 should be evaluated.
Assume the function h : R →R is representable in terms of a ﬁnite
number of functions γm(uf(t, zi)) : R →R indexed by m. For
example, should the constraint be h(uf) = 3uf + u2
f = 0, then we
would deﬁne γ1(uf) = uf, γ2(uf) = u2
f and h(uf) = 3 · γ1(uf) + γ2(uf) = 0. Then, h can be
evaluated by evaluating each γm as"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.14685314685314685,"γm(uf(t, zi)) = γm  
N
X"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.15034965034965034,"j=1
αj(t)φj(zi) "
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.15384615384615385,"= γm (Φi,·α(t)) ,
(7)"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.15734265734265734,"where α(t) = (α1(t), . . . , αN(t))T , Φ is K-by-N matrix with elements Φi,j = φj(zi), and Φi,· is
the i’th row of Φ."
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.16083916083916083,"Differential constraints.
Consider the same setup as before but now h(uf(t, zi)) = 0 consists of
differential operators and is representable in terms of a ﬁnite number of functions ∂qγm(uf (t,zi))"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.16433566433566432,"∂zq
i
:
R →R indexed by m, where the derivative order q could be different for each m. For example, should"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.16783216783216784,"the constraint be h(uf) = 3uf + uf ·
∂u2
f
∂x = 0, then we would deﬁne γ1(uf) = uf, γ2(uf) = u2
f
and h(uf) = 3 · γ1(uf) + γ1(uf) · ∂γ2(uf )"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.17132867132867133,"∂z
= 0. Then, h can be evaluated by evaluating each
∂qγm(uf (t,zi))"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.17482517482517482,"∂zq
i
using the generalization of the chain rule (Appendix A) which contains only two types"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.17832167832167833,of terms. The ﬁrst type of terms dγm
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.18181818181818182,"duf , . . . , dqγm"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.1853146853146853,"duq
f can be evaluated using automatic differentiation"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.1888111888111888,while the second type of terms ∂uf
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.19230769230769232,"∂zi , . . . , ∂quf"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.1958041958041958,"∂zq
i can be evaluated as ∂quf"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.1993006993006993,"∂zq
i
= N
X"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.20279720279720279,"j=1
αj(t)∂qφj(zi)"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.2062937062937063,"∂zq
i
= Φ(q)
i,· α(t),
(8)"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.2097902097902098,"where Φ(q)
i,j = ∂qφj(zi)"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.21328671328671328,"∂zq
i
. Mixed partial derivatives can be handled in a similar way (Appendix A)."
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.21678321678321677,Under review as a conference paper at ICLR 2022
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.2202797202797203,"Integral
constraints.
Consider
the
same
setup
as
before
but
with
h(uf(t, x))
=
R"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.22377622377622378,"Ωτ(uf(t, x))dx = 0, where the function τ : R →R is representable in terms of functions
γm(uf(t, zi)) : R →R similarly to the pointwise constraints. Then,
R"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.22727272727272727,"Ωτ(uf(t, x))dx can be evalu-
ated using a numerical integration technique, e.g. midpoint rule, Gaussian quadrature or Monte-Carlo
integration, as
Z"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.23076923076923078,"Ω
τ(uf(t, x))dx ≈ K
X"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.23426573426573427,"i=1
τ(uf(t, zi))µi,
(9)"
EVALUATING CONSTRAINTS ON UNSTRUCTURED GRIDS,0.23776223776223776,"where K is the number of integration points, µi are integration coefﬁcients which depend on the grid
and integration method, and τ(uf(t, zi)) is evaluated as in Equation 7."
SOFT CONSTRAINTS,0.24125874125874125,"3.2
SOFT CONSTRAINTS
Soft constraints are implemented by minimizing the following loss Ldata + λr(h(uf)), where λ ∈R
and Ldata is deﬁned as in Equation 4. We set r(h(uf)) =
1
KM
PK
i=1
PM
j=1 h(uf(tj, zi))2 for point-"
SOFT CONSTRAINTS,0.24475524475524477,"wise and differential constraints and r(h(uf)) =
1
M
PM
j=1 h(uf(tj, x))2 for integral constraints."
HARD CONSTRAINTS,0.24825174825174826,"3.3
HARD CONSTRAINTS
Our method allows to implement hard constraints by projecting the interpolant uf(t, x) to a subset of
functions which satisfy the required constraints. Namely, if uf(t, x) does not satisfy constraints g
and h, it is projected to a subset of functions which satisfy the constraint by solving the following
optimization problem
min
ˆuf ∈Vφ
∥uf −ˆuf∥2
L2"
HARD CONSTRAINTS,0.2517482517482518,"s.t.
h(ˆuf) = 0,
g(ˆuf) ≤0, (10)"
HARD CONSTRAINTS,0.25524475524475526,"where the projection is denoted by ˆuf(t, x) and Vφ is spanned by the basis functions."
HARD CONSTRAINTS,0.25874125874125875,"Using the basis representation uf(t, x) = PN
i=1 αi(t)φi(x) and ˆuf(t, x) = PN
i=1 βi(t)φi(x) we
can rewrite the optimization problem (10) as"
HARD CONSTRAINTS,0.26223776223776224,"min
β(t)∈RN
(α(t) −β(t))T ˆΦ(α(t) −β(t))"
HARD CONSTRAINTS,0.26573426573426573,"s.t.
h(ˆuf) = 0,
g(ˆuf) ≤0, (11)"
HARD CONSTRAINTS,0.2692307692307692,"where β(t) = (β1(t), . . . , βN(t))T and ˆΦi,j =
R"
HARD CONSTRAINTS,0.2727272727272727,Ωφi(x)φj(x)dx.
HARD CONSTRAINTS,0.2762237762237762,"To train the model end-to-end, the problem (11) should be differentiable. Agrawal et. al. [1] proposed
differentiable convex optimization which could be used in this case if the problem (11) could be
expressed in a DPP-compliant way (see [1]). To do that, we restrict ourselves to constraints that can
be expressed as an equality or inequality between Aβ(t) and b, where A is a constant matrix and
b is a constant vector. This formulation admits pointwise, differential and integral constraints on
untransformed uf. The objective function is convex since its Hessian is positive-semideﬁnite i.e. for
any v ∈RN"
HARD CONSTRAINTS,0.27972027972027974,"vT ˆΦv = N
X"
HARD CONSTRAINTS,0.28321678321678323,"i,j=1
vivj ˆΦi,j = N
X"
HARD CONSTRAINTS,0.2867132867132867,"i,j=1
⟨viφi, vjφj⟩L2 = ⟨ N
X"
HARD CONSTRAINTS,0.2902097902097902,"i=1
viφi, N
X"
HARD CONSTRAINTS,0.2937062937062937,"j=1
vjφj⟩L2 ≥0.
(12)"
HARD CONSTRAINTS,0.2972027972027972,"This allows to solve the problem (11) and differentiate its solution β∗(t) w.r.t. α(t). The model
parameters are found by minimizing the following loss function Ldata +λLproj, where λ ∈R and Ldata
is deﬁned as in Equation 4 but with u(ti) replaced by ˆu(ti) = (ˆuf(ti, x1), . . . , ˆuf(ti, xN)). We
set Lproj =
1
NM
PN
i=1
PM
j=1 ∥uf(tj, xi) −ˆuf(tj, xi)∥2
2. The second term makes the optimization
procedure prefer models that predict uf close to the feasible set of the problem (11)."
HARD CONSTRAINTS,0.3006993006993007,"We note that the proposed approach is currently limited to small-scale problems due to existing
computational bottlenecks in the implementation of differentiable convex optimization [1]."
HARD CONSTRAINTS,0.3041958041958042,Under review as a conference paper at ICLR 2022
BASIS FUNCTIONS,0.3076923076923077,"3.4
BASIS FUNCTIONS"
BASIS FUNCTIONS,0.3111888111888112,"Figure 2: 1D and 2D piecewise
linear basis functions (colored)
and function built from them
(grey). Black dots represent ob-
servation points."
BASIS FUNCTIONS,0.3146853146853147,"Selecting appropriate basis is crucial for efﬁciency and applica-
bility of the proposed method. Ideally, the basis should allow
efﬁcient construction of uf(t, x) from u(t), contain no tunable
parameters, and lead to sparse matrices Φ, Φ(q) and ˆΦ. We con-
sider bases from two families: Lagrange basis functions and radial
basis functions (RBFs)."
BASIS FUNCTIONS,0.3181818181818182,"Lagrange basis functions do not have tunable parameters and have
compact support which leads to sparse Φ, Φ(q) and ˆΦ. For the
piecewise linear basis the interpolant uf(t, x) can be constructed
directly from the predictions by setting α(t) = u(t). However,
constructing uf(t, x) for a higher order basis, e.g. piecewise
quadratic, requires the model to make predictions not only at the observation points, but also at some
extra points where the data is not available. In Section 4 we demonstrate one approach to solving this
problem. After extending the state u(t) by predictions at the extra nodes, the coefﬁcients α(t) can be
evaluated similarly to the piecewise linear basis. In this work we use piecewise linear (PWL) and
piecewise quadratic (PWQ) bases. Examples of PWL basis functions are shown in Figure 2."
BASIS FUNCTIONS,0.32167832167832167,"Radial basis functions have a wider range of properties. Some RBFs have tunable parameters, some
don’t. The matrices Φ, Φ(q) and ˆΦ evaluated with RBFs are typically dense, but RBFs with compact
support exist (e.g. bump function). The interpolant uf(t, x) can be constructed by evaluating
α(t) = K−1u(t), where K−1 is the inverse of the interpolation matrix of the given RBF and
Kij = φ(∥xi −xj∥), where φ is an RBF and xi, xj are observation locations. In this work we use
the cubic RBF basis i.e. φ(r) = r3."
BASIS FUNCTIONS,0.32517482517482516,We use PyTorch [26] to handle sparse matrices and to evaluate K−1u(t) in a differentiable way.
EXPERIMENTS,0.32867132867132864,"4
EXPERIMENTS"
EXPERIMENTS,0.3321678321678322,"In the following experiments we use the relative error between the data y(t) and model predictions
u(t) deﬁned as ∥y(t)−u(t)∥2"
EXPERIMENTS,0.3356643356643357,"∥y(t)∥2
and consider only soft constraints. We present an experiment with hard
constraints implemented as shown in Section 3.3 in Appendix D. Data generation is described in
Appendix B. Training, testing and modeling details are in Appendix C. All experiments were run on
a single NVIDIA Quadro P5000 GPU. All errors bars represent one standard deviation of the results
over ﬁve random seeds."
"REPLACING EXISTING METHODS
IN THIS EXPERIMENT WE TAKE EXISTING MODELS WHICH INCORPORATE PHYSICS-BASED CONSTRAINTS IN TRAINING",0.33916083916083917,"4.1
REPLACING EXISTING METHODS
In this experiment we take existing models which incorporate physics-based constraints in training
and replace their constraint enforcing approaches with ours. We consider two works. First, [37] which
trains a GAN to produce divergence-free vector ﬁelds using zero-divergence constraint. Second, [10]
which predicts warping ﬁelds driving the evolution of sea surface temperature by observing snapshots
of the temperature over time while enforcing gradient and divergence constraints on the warping
ﬁelds (see Appendix C for more details). Both models work on uniform grids which allows them to
evaluate constraints using ﬁnite differences. For comparison, we replace ﬁnite differences with our
method and observe how it changes the models’ performance. In both cases we use the PWL basis."
"REPLACING EXISTING METHODS
IN THIS EXPERIMENT WE TAKE EXISTING MODELS WHICH INCORPORATE PHYSICS-BASED CONSTRAINTS IN TRAINING",0.34265734265734266,"For [37] we track the mean divergence and discriminator loss. Results of the original approach are as
follows: mean divergence 0.079 and discriminator loss 0.091. With our method the mean divergence
was 0.014 and the discriminator loss was 0.088. Both approaches results in similar discriminator
losses but our approach produces a smaller mean divergence (smaller is better). Our method increased
the runtime per epoch by 6%."
"REPLACING EXISTING METHODS
IN THIS EXPERIMENT WE TAKE EXISTING MODELS WHICH INCORPORATE PHYSICS-BASED CONSTRAINTS IN TRAINING",0.34615384615384615,"For [10] we track the total, divergence and smoothness losses which, with the original approach, were
0.139, 8.4 · 10−5 and 1.51 · 10−4, respectively. With our approach the losses were 0.139, 8.3 · 10−5
and 1.51 · 10−4, respectively. Both methods produce very similar results. Our method increased the
runtime per epoch by 30%."
"REPLACING EXISTING METHODS
IN THIS EXPERIMENT WE TAKE EXISTING MODELS WHICH INCORPORATE PHYSICS-BASED CONSTRAINTS IN TRAINING",0.34965034965034963,"Overall, replacing existing constraint enforcing approaches by ours on data from uniform grids
resulted in comparable model performance, except for runtime which was slightly increased."
"REPLACING EXISTING METHODS
IN THIS EXPERIMENT WE TAKE EXISTING MODELS WHICH INCORPORATE PHYSICS-BASED CONSTRAINTS IN TRAINING",0.3531468531468531,Under review as a conference paper at ICLR 2022
"REPLACING EXISTING METHODS
IN THIS EXPERIMENT WE TAKE EXISTING MODELS WHICH INCORPORATE PHYSICS-BASED CONSTRAINTS IN TRAINING",0.35664335664335667,"Figure 4: Effects of amount of data and grid sizes on relative errors and constraint violations for the
Cahn-Hilliard equation. All results are for the test set. Constraint violations are evaluated as the mean
absolute violation of the constraint, |
R"
"REPLACING EXISTING METHODS
IN THIS EXPERIMENT WE TAKE EXISTING MODELS WHICH INCORPORATE PHYSICS-BASED CONSTRAINTS IN TRAINING",0.36013986013986016,"Ωuf(t, x)dx −C| over all simulations and time points. In
most simulations C ≈0.5."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.36363636363636365,"4.2
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT
We start with the 1D Cahn-Hilliard equation ∂u"
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.36713286713286714,"∂t = 2∇2(u(1 −u)2 −u2(1 −u) −ϵ2∇2u)
(13)"
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3706293706293706,"which is known to conserve the state u i.e. h(u) =
R"
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3741258741258741,"Ωu(t, x)dx −C = 0 at all time points, where
C =
R"
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3776223776223776,"Ωu(0, x)dx is a constant. This is an example of a conservation law which are abundant in
nature and are important class of constraints that data-driven models of physical systems should
satisfy. Conservation laws can be expressed in differential and integral forms and this experiment
demonstrates how the integral form can be enforced. The constraint is evaluated using the midpoint
rule as shown in the previous section with a single γ1 being the identity function. We use PWL, PWQ
and cubic RBF bases and compare the results to an unconstrained model."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3811188811188811,"Figure 3: 1D spatial grid for
the Cahn-Hilliard equation."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.38461538461538464,"For training we use 30, 60 and 120 simulations while the test set
consist of 60 simulations. Simulations in the training/test data last
for 0.0015/0.0030 seconds and contain 50/100 uniformly spaced
time points. The full spatial grid consists of 101 uniformly spaced
nodes. We randomly sample 50%, 75% and 100% of the nodes and
train/test on the resulting (irregular) spatial grid. Training and testing
is done with identical spatial grids. An example of a spatial grid
with 50% of nodes is shown in Figure 3. We evaluate the constraint on a uniform grid with 200 nodes
placed on top of the original grid."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3881118881118881,"To learn the dynamics of the system we use the model from [15] (Section 2). We found that using
a GNN produced poor results. For that reason we represented the function Fθ with a multiplayer
perceptron (MLP) which updates the state of each node based on the states of all other nodes in the
grid (results for a GNN are in Appendix E). The MLP contains two hidden layers with Leaky ReLU
nonlinearities. The number of hidden neurons was set to the number of nodes in the grid."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3916083916083916,"The coefﬁcients α(t) for the PWL and cubic bases can be evaluated directly from the model
predictions at the grid nodes. But the PWQ basis requires extra predictions to be available between
the nodes. This is problematic since there is no data at these points to guide the model’s predictions.
To solve this problem we introduce a small MLP which is applied to consecutive pairs of nodes. The
MLP takes the states at both nodes and the distance between them as the input and estimates the state
at the midpoint between the two nodes. The MLP is trained jointly with the main model and uses
only the constraint-related loss term during training."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3951048951048951,"For testing, we construct the interpolant uf(t, x) using the thin plate spline basis (φ(r) = r2 log r)
and evaluate the constraint on that interpolant. This allows to make a fair comparison between the
unconstrained model and different bases and avoid biasing or oveﬁtting to bases used for training."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.3986013986013986,Under review as a conference paper at ICLR 2022
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.4020979020979021,"Figure 4 shows results of the experiment. We observe that changing the node fraction does not
signiﬁcantly affect the relative errors but has noticeable effect on constraint violations, especially for
the unconstrained model. Constrained models tend to show similar or better performance than the
unconstrained model. Among all bases, the cubic basis consistently results in lower relative errors
and constraint violations. However, the simpler PWL basis often performs on par with the cubic
basis, especially on denser spatial grids. We also observe that coarsening of the grid increases the
constraint violation gap between constrained and unconstrained models and that this gap seems to
not close as we increase the amount of training data."
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT,0.40559440559440557,"The PWQ basis performs rather poorly on ﬁne grids which is likely due to a suboptimal approach to
evaluating the state at the extra nodes. A better approach could consider not only pairs of points but
also larger neighborhoods. Nonetheless, the PWQ basis achieves good performance on coarse grids
which shows that piecewise bases of higher order could potentially be used to enforce constraints.
This will allow to scale to grids with a large number of nodes due to sparsity of the constraint matrices
and efﬁcient evaluation of α."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.4090909090909091,"4.3
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT"
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.4125874125874126,"Figure 5: Grid for
the heat equation."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.4160839160839161,We impose constraints on a 2D system governed by the heat equation ∂u
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.4195804195804196,"∂t =
∇2u for which the generated initial conditions (ICs) are monotone in one
direction. Since the ICs are monotone, the state u remains monotone at all
time points as well. We enforce the monotonicity constraint as ∂u"
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.4230769230769231,"∂x ≥0. The
constraint is evaluated as shown in the previous section with γ1 being the
identity function."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.42657342657342656,"For training we use 15, 30 and 90 simulations while the test set consist of
120 simulations. Simulations in the training/test data last for 0.1/0.2 seconds
and contain 21/41 uniformly spaced time points. The full spatial grid consists
of 1087 nodes. We randomly sample 33%, 66% and 100% of the nodes
and train/test on the resulting (irregular) spatial grid. Training and testing
is done with identical spatial grids. Spatial grid with 100% of nodes is shown in Figure 5. The
constraint is evaluated at the nodes of a uniform 51 × 51 grid placed on top of the original grid."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.43006993006993005,"Figure 6: Comparison of data with predic-
tions of unconstrained and constrained mod-
els trained on 30 simulations, full spatial
grid and using PWL basis for the constrained
model. The predictions are for a test case."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.43356643356643354,"To learn the dynamics of the system we use the model
from [15] directly with the messaging and aggrega-
tion networks being MLPs with a single hidden layer
consisting of 60 neurons with Tanh nonlinearities and
the input/output sizes of 4/40 and 41/1 respectively."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.4370629370629371,"During testing, we use predictions of the models to
construct an interpolant uf(t, x) using the thin plate
spline basis and evaluate the constraint on that in-
terpolant. This allows to make a fair comparison
between the unconstrained model and different bases."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.4405594405594406,"Figure 7 shows results of the experiment. We observe
that changing the node fraction equally increases rel-
ative errors of all models and has noticeable effect on
constraint violations, especially for the unconstrained
model. Constrained models tend to show slightly
higher or comparable relative errors but noticeably
lower constraint violations than the unconstrained
model. The cubic and PWL bases perform equally
well in this case. Similarly to the experiment in the
previous section, we observe that coarsening of the
grid introduces a larger constraint violation gap between constrained and unconstrained models and
that this gap seems to not close as we increase the amount of training data."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.44405594405594406,"Figure 6 shows qualitative difference between predictions of constrained and unconstrained models.
It can be noted that predictions from the constrained model have noticeably smoother contours thus
making the ﬁeld more monotonous in the horizontal direction."
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.44755244755244755,Under review as a conference paper at ICLR 2022
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.45104895104895104,"Figure 7: Effects of amount of data and grid sizes on relative errors and constraint violations for the
heat equation. Results are for the test set. Constraint violations are evaluated as the mean absolute
violation of the constraint ∂uf"
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT,0.45454545454545453,∂x ≥0 over all simulations and time points.
LEARNING DISTRIBUTIONS OF PHYSICAL FIELDS,0.458041958041958,"4.4
LEARNING DISTRIBUTIONS OF PHYSICAL FIELDS"
LEARNING DISTRIBUTIONS OF PHYSICAL FIELDS,0.46153846153846156,"Figure 8: First row: LD, LC and histograms of
divergences of samples from data, constrained and
unconstrained WGANs. Second row: LD, LC
and histograms of Laplacians of samples from
data, constrained and unconstrained WGANs. Con-
straints are evaluated at cell centroids."
LEARNING DISTRIBUTIONS OF PHYSICAL FIELDS,0.46503496503496505,"We demonstrate the effect of adding constraints
to a GAN when learning distributions of physi-
cal ﬁelds on unstructured grids. We use Wasser-
stein GAN (WGAN) [2] as a more stable variant
of a GAN. We use MLPs as a generator and
discriminator. Unconstrained and constrained
models are trained for 1.2M iterations. Con-
straints are enabled only after 600k iterations.
Constrained models are trained similarly to the
unconstrained ones but with a modiﬁed genera-
tor loss deﬁned as LG + λ ln (1 + LC), where
LG is the standard generator loss and LC is the
constraint-based loss. We deﬁne LC as the mean
value of h(u)2, where h is a constraint evaluated
at the centroid of each cell in the grid."
ZERO-DIVERGENCE FIELDS,0.46853146853146854,"4.4.1
ZERO-DIVERGENCE FIELDS
Divergence-free vector ﬁelds are often encoun-
tered in solutions of ﬂuid dynamics problems.
The divergence-free constraint on a vector ﬁeld
u(x, y) = (u1(x, y), u2(x, y))T is deﬁned as
h(u) = ∂u1"
ZERO-DIVERGENCE FIELDS,0.47202797202797203,∂x + ∂u2
ZERO-DIVERGENCE FIELDS,0.4755244755244755,"∂y = 0. The constraint is en-
forced using the PWL basis. We generated a
dataset with 10k divergence-free ﬁelds on an
unstructured grid with 1050 nodes (Figure 14)
and used a WGAN to learn a distribution over such ﬁelds. Note that the generated ﬁelds are not
entirely divergence-free but have small residual divergence due to discretization errors."
ZERO-DIVERGENCE FIELDS,0.479020979020979,"Figure 9a shows that there is a clear difference in the quality of the samples generated by the
unconstrained and constrained models. Samples from the constrained model are smoother and
more similar to the data. Quantitative comparison of the samples presented in Figure 8 shows that
the constrained model generates ﬁelds that have much lower constraint violation and divergence
distribution very similar to that of the data."
ZERO-LAPLACIAN FIELDS,0.4825174825174825,"4.4.2
ZERO-LAPLACIAN FIELDS
Fields with zero Laplacian represent solutions to some PDEs, for example the steady-state heat
equation. The zero-Laplacian constraint on a scalar ﬁeld u(x, y) is deﬁned as h(u) = ∂2u"
ZERO-LAPLACIAN FIELDS,0.486013986013986,∂x2 + ∂2u
ZERO-LAPLACIAN FIELDS,0.48951048951048953,"∂y2 = 0.
The constraint is enforced using the cubic basis as the PWL basis has zero second derivatives"
ZERO-LAPLACIAN FIELDS,0.493006993006993,Under review as a conference paper at ICLR 2022
ZERO-LAPLACIAN FIELDS,0.4965034965034965,"(a)
(b)"
ZERO-LAPLACIAN FIELDS,0.5,"Figure 9: Magnitudes of random samples from the dataset, unconstrained and constrained WGANs.
a) zero-divergence ﬁelds, b) zero-Laplacian ﬁelds."
ZERO-LAPLACIAN FIELDS,0.5034965034965035,"everywhere. We generated a dataset with 10k Laplacian-free ﬁelds on an unstructured grid with 1050
nodes (Figure 14) and used a WGAN to learn a distribution over such ﬁelds. Note that the generated
ﬁelds are not entirely Laplacian-free due to discretization errors."
ZERO-LAPLACIAN FIELDS,0.506993006993007,"Results of the experiment are shown in Figures 9b and 8. Similarly to the divergence-free case,
visual quality of the ﬁelds generated by the constrained model is signiﬁcantly better than for the
unconstrained model. Quantitative comparison of the samples presented in Figure 8 shows that
the constrained model generates ﬁelds that have much lower constraint violation and Laplacian
distribution very similar to that of the data."
RELATED WORK,0.5104895104895105,"5
RELATED WORK"
RELATED WORK,0.513986013986014,"Soft constraints.
Soft constraints are widely used due to being relatively easy to implement.
Examples include lake temperature prediction [18; 16], trafﬁc simulation [22], ﬂuid and climate
modeling [11; 10; 4], where constraints are evaluated pointwise or using ﬁnite differences."
RELATED WORK,0.5174825174825175,"Hard constraints.
Approaches to implementing hard constraints are diverse and can be categorized
as processing the output of an unconstrained model [4; 17; 24; 34] and designing a model that
produces feasible predictions by default [23; 25; 14; 9; 13; 8; 38]."
RELATED WORK,0.5209790209790209,"Constrained PDE models
Current approaches to enforcing soft [10; 11] and hard [21; 25; 17]
constraints are limited to speciﬁc types of constraints and spatial grids. For example, [25; 17]
implement only hard differential constraints and both are limited to uniform grids. Uniform grids
allow to evaluate constraints efﬁciently e.g. using ﬁnite differences [10; 21; 25] or fast Fourier
transform [17] but assuming that the data lies on a uniform grid might be limiting."
RELATED WORK,0.5244755244755245,"Constrained GANs
Works such as [37; 17] showed how physics-based constraints beneﬁt training
and quality of the generated samples but are also limited to uniform grids."
CONCLUSION,0.527972027972028,"6
CONCLUSION"
CONCLUSION,0.5314685314685315,"We presented a general approach to enforcing algebraic constraints on unstructured grids and showed
how it can be used to enforce soft and hard constraints. We demonstrated applicability of the approach
to learning of PDE-driven dynamical systems and distributions of physical ﬁelds. We considered two
families of basis functions for constructing the interpolant and showed how Lagrange basis functions
of order higher than one can be used. Our method allows to drop the unrealistic assumption about
uniformity of spatial grids and shows promising results on various tasks."
CONCLUSION,0.534965034965035,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5384615384615384,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.541958041958042,"All details required to reproduce the experiments are provided in Section 4 and Appendices. Code
and data used to run the experiments will be made publicly available after the review process."
REFERENCES,0.5454545454545454,REFERENCES
REFERENCES,0.548951048951049,"[1] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter. Differentiable convex
optimization layers. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.5524475524475524,"[2] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 214–223. PMLR, 06–11 Aug 2017. URL http://proceedings.mlr.press/v70/
arjovsky17a.html."
REFERENCES,0.5559440559440559,"[3] Nathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm,
Manish Parashar, Abani Patra, James Sethian, Stefan Wild, Karen Willcox, and Steven Lee.
Workshop report on basic research needs for scientiﬁc machine learning: Core technologies
for artiﬁcial intelligence. 2 2019. doi: 10.2172/1478744. URL https://www.osti.gov/
biblio/1478744."
REFERENCES,0.5594405594405595,"[4] Tom Beucler, Michael Pritchard, Stephan Rasp, Jordan Ott, Pierre Baldi, and Pierre Gentine.
Enforcing analytic constraints in neural networks emulating physical systems. Physical Review
Letters, 126(9), Mar 2021. ISSN 1079-7114. doi: 10.1103/physrevlett.126.098302. URL
http://dx.doi.org/10.1103/PhysRevLett.126.098302."
REFERENCES,0.5629370629370629,"[5] Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B Tenenbaum. A compositional
object-based approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016."
REFERENCES,0.5664335664335665,"[6] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/
file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf."
REFERENCES,0.5699300699300699,"[7] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/
file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf."
REFERENCES,0.5734265734265734,"[8] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley
Ho. Lagrangian neural networks, 2020."
REFERENCES,0.5769230769230769,"[9] Arka Daw, R. Quinn Thomas, Cayelan C. Carey, Jordan S. Read, Alison P. Appling, and Anuj
Karpatne. Physics-guided architecture (pga) of neural networks for quantifying uncertainty in
lake temperature modeling.
Proceedings of the 2020 SIAM International Conference on
Data Mining, pp. 532–540, Jan 2020.
doi: 10.1137/1.9781611976236.60.
URL http:
//dx.doi.org/10.1137/1.9781611976236.60."
REFERENCES,0.5804195804195804,"[10] Emmanuel de Bézenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical pro-
cesses: incorporating prior scientiﬁc knowledge. Journal of Statistical Mechanics: Theory and
Experiment, 2019(12):124009, Dec 2019. ISSN 1742-5468. doi: 10.1088/1742-5468/ab3195.
URL http://dx.doi.org/10.1088/1742-5468/ab3195."
REFERENCES,0.583916083916084,"[11] N. Benjamin Erichson, Michael Muehlebach, and Michael W. Mahoney. Physics-informed
autoencoders for lyapunov-stable ﬂuid ﬂow prediction, 2019."
REFERENCES,0.5874125874125874,"[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-
jil Ozair, Aaron Courville, and Yoshua Bengio.
Generative adversarial nets.
In
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, volume 27. Curran Associates,"
REFERENCES,0.5909090909090909,Under review as a conference paper at ICLR 2022
REFERENCES,0.5944055944055944,"Inc.,
2014.
URL https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf."
REFERENCES,0.5979020979020979,"[13] Sam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks, 2019."
REFERENCES,0.6013986013986014,"[14] Johannes Hendriks, Carl Jidling, Adrian Wills, and Thomas Schön. Linearly constrained neural
networks, 2020."
REFERENCES,0.6048951048951049,"[15] Valerii Iakovlev, Markus Heinonen, and Harri Lähdesmäki. Learning continuous-time pdes
from sparse data with graph neural networks, 2020."
REFERENCES,0.6083916083916084,"[16] Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan Read, Jacob Zwart, Michael Steinbach,
and Vipin Kumar. Physics guided rnns for modeling dynamical systems: A case study in
simulating lake temperature proﬁles. Proceedings of the 2019 SIAM International Conference
on Data Mining, pp. 558–566, May 2019. doi: 10.1137/1.9781611975673.63. URL http:
//dx.doi.org/10.1137/1.9781611975673.63."
REFERENCES,0.6118881118881119,"[17] Chiyu ”Max” Jiang, Karthik Kashinath, Prabhat, and Philip Marcus. Enforcing physical
constraints in {cnn}s through differentiable {pde} layer. In ICLR 2020 Workshop on Integration
of Deep Neural Models and Differential Equations, 2020. URL https://openreview.
net/forum?id=q2noHUqMkK."
REFERENCES,0.6153846153846154,"[18] Anuj Karpatne, William Watkins, Jordan Read, and Vipin Kumar. Physics-guided neural
networks (pgnn): An application in lake temperature modeling, 2017."
REFERENCES,0.6188811188811189,"[19] K. Kashinath, M. Mustafa, A. Albert, J-L. Wu, C. Jiang, S. Esmaeilzadeh, K. Azizzade-
nesheli, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli, D. Chirila, R. Yu, R. Walters,
B. White, H. Xiao, H. A. Tchelepi, P. Marcus, A. Anandkumar, P. Hassanzadeh, and null
Prabhat. Physics-informed machine learning: case studies for weather and climate mod-
elling. Philosophical Transactions of the Royal Society A: Mathematical, Physical and En-
gineering Sciences, 379(2194):20200093, 2021. doi: 10.1098/rsta.2020.0093. URL https:
//royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0093."
REFERENCES,0.6223776223776224,"[20] Y. Khoo, J. Lu, and Lexing Ying. Solving pde problems with uncertainty using neural-networks.
2018."
REFERENCES,0.6258741258741258,"[21] Byungsoo Kim, Vinicius C. Azevedo, Nils Thuerey, Theodore Kim, Markus Gross, and Barbara
Solenthaler. Deep ﬂuids: A generative network for parameterized ﬂuid simulations. Computer
Graphics Forum, 38(2):59–70, May 2019. ISSN 1467-8659. doi: 10.1111/cgf.13619. URL
http://dx.doi.org/10.1111/cgf.13619."
REFERENCES,0.6293706293706294,"[22] Jiachen Li, Hengbo Ma, and Masayoshi Tomizuka. Conditional generative neural system for
probabilistic trajectory prediction, 2019."
REFERENCES,0.6328671328671329,"[23] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data,
2017."
REFERENCES,0.6363636363636364,"[24] Gaurav Manek and J. Zico Kolter. Learning stable deep dynamics models, 2020."
REFERENCES,0.6398601398601399,"[25] Arvind T. Mohan, Nicholas Lubbers, Daniel Livescu, and Michael Chertkov. Embedding hard
physical constraints in neural network coarse-graining of 3d turbulence, 2020."
REFERENCES,0.6433566433566433,"[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf."
REFERENCES,0.6468531468531469,"[27] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning
mesh-based simulation with graph networks, 2020."
REFERENCES,0.6503496503496503,Under review as a conference paper at ICLR 2022
REFERENCES,0.6538461538461539,"[28] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686–707, 2019."
REFERENCES,0.6573426573426573,"[29] Martin Riedmiller and Heinrich Braun. Rprop - a fast adaptive learning algorithm. Technical
report, Proc. of ISCIS VII), Universitat, 1992."
REFERENCES,0.6608391608391608,"[30] L. V. Rueden, Sebastian Mayer, Katharina Beckh, B. Georgiev, Sven Giesselbach, R. Heese,
Birgit Kirsch, Julius Pfrommer, Annika Pick, R. Ramamurthy, Michal Walczak, J. Garcke,
C. Bauckhage, and Jannis Schuecker. Informed machine learning – a taxonomy and survey of
integrating knowledge into learning systems. arXiv: Machine Learning, 2019."
REFERENCES,0.6643356643356644,"[31] Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Ried-
miller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for
inference and control. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 4470–4479. PMLR, 10–15 Jul 2018."
REFERENCES,0.6678321678321678,"[32] W.E. Schiesser. The Numerical Method of Lines: Integration of Partial Differential Equations.
Elsevier Science, 2012. ISBN 9780128015513. URL https://books.google.fi/
books?id=2YDNCgAAQBAJ."
REFERENCES,0.6713286713286714,"[33] Sungyong Seo and Yan Liu. Differentiable physics-informed graph networks, 2019."
REFERENCES,0.6748251748251748,"[34] Naoya Takeishi and Yoshinobu Kawahara. Learning dynamics models with stable invariant sets,
2020."
REFERENCES,0.6783216783216783,"[35] T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average
of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012."
REFERENCES,0.6818181818181818,"[36] Jared Willard, Xiaowei Jia, Shaoming Xu, M. Steinbach, and V. Kumar. Integrating physics-
based modeling with machine learning: A survey. ArXiv, abs/2003.04919, 2020."
REFERENCES,0.6853146853146853,"[37] Zeng Yang, Jin-Long Wu, and Heng Xiao. Enforcing deterministic constraints on generative
adversarial networks for emulating physical systems, 2019."
REFERENCES,0.6888111888111889,"[38] Haijun Yu, Xinyuan Tian, Weinan E, and Qianxiao Li. Onsagernet: Learning stable and
interpretable dynamics using a generalized onsager principle, 2020."
REFERENCES,0.6923076923076923,Under review as a conference paper at ICLR 2022
REFERENCES,0.6958041958041958,"A
GENERALIZED CHAIN RULE AND HANDLING MIXED PARTIAL DERIVATIVES."
REFERENCES,0.6993006993006993,"Let y = g(x1, . . . , xn) with all arguments being either identical, distinct or grouped. Then, partial
derivatives of f(y) can be evaluated using the Faà di Bruno’s formula"
REFERENCES,0.7027972027972028,"∂nf(y)
∂x1 · · · ∂xn
=
X"
REFERENCES,0.7062937062937062,"π∈Π
f (|π|)(y)
Y B∈π"
REFERENCES,0.7097902097902098,"∂|B|y
Q"
REFERENCES,0.7132867132867133,"j∈B ∂xj
,"
REFERENCES,0.7167832167832168,"where Π is the set of all partitions of the set {1, . . . , n}, B runs through elements of the partition π,
f (m) denotes m’th derivative, and | · | is cardinality."
REFERENCES,0.7202797202797203,"The formula consists of two terms: f (|π|)(y), which can be evaluated using automatic differentiation,
and
∂|B|y
Q"
REFERENCES,0.7237762237762237,"j∈B ∂xj , which can be evaluated as shown in Equation 8."
REFERENCES,0.7272727272727273,"In case that all x1, . . . , xn are identical, the mixed derivative
∂nf(y)
∂x1···∂xn reduces to ∂nf(y)"
REFERENCES,0.7307692307692307,"∂xn
1
."
REFERENCES,0.7342657342657343,"B
DATA GENERATION"
REFERENCES,0.7377622377622378,"In all cases we run simulation on a ﬁne grid and then interpolate the results to a coarser grid
represented as the ""full grid"" in the experiments."
REFERENCES,0.7412587412587412,"B.1
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT
Training and testing data was obtained by solving ∂u"
REFERENCES,0.7447552447552448,"∂t = 2∇2(u(1 −u)2 −u2(1 −u) −ϵ2∇2u)
(14)"
REFERENCES,0.7482517482517482,"on a unit interval with periodic boundary conditions and ϵ = 0.04. The domain was represented by
a uniform grid with 100 nodes and the time step was set to 1.0e-6 sec. The initial conditions u0(x)
were generated as follows"
REFERENCES,0.7517482517482518,˜u0(x) =
X,0.7552447552447552,"10
X"
X,0.7587412587412588,"i=1
(λi cos ((x −s)2π) + γi sin ((x −s)2π)) + λ0"
X,0.7622377622377622,"2 ,
(15)"
X,0.7657342657342657,"u0(x) =
˜u0(x) −min ˜u0(x)
max ˜u0(x) −min ˜u0(x),
(16)"
X,0.7692307692307693,"where λi, γi ∼Unif(−1, 1) and s ∼Unif(0, 1)."
X,0.7727272727272727,Examples of the simulations are shown in Figure 10.
X,0.7762237762237763,"B.2
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT
Training and testing data was obtained by solving ∂u"
X,0.7797202797202797,"∂t = D∇2u
(17)"
X,0.7832167832167832,"on a unit square with zero Neumann boundary conditions and D = 0.2. The domain was represented
by an unstructured grid with 2971 nodes and the time step was set to 0.001 sec. The initial conditions
u0(x) were generated as"
X,0.7867132867132867,f(x) =
X,0.7902097902097902,"6
X"
X,0.7937062937062938,"i=0
ωixi,
(18)"
X,0.7972027972027972,g(y) = 1 2
X,0.8006993006993007,"3
X"
X,0.8041958041958042,"i=1
(λi cos ((x + s)2π) + γi sin ((x + s)2π)) + λ0"
X,0.8076923076923077,"2 ,
(19)"
X,0.8111888111888111,"˜u0(x, y) = f(x) + g(y),
(20)"
X,0.8146853146853147,"u0(x, y) =
˜u0(x, y) −min ˜u0(x, y)
max ˜u0(x, y) −min ˜u0(x, y),
(21)"
X,0.8181818181818182,"where ωi ∼Unif(0.1, 1.1), λi, γi, s ∼Unif(−1, 1)."
X,0.8216783216783217,Examples of the simulations are shown in Figure 11.
X,0.8251748251748252,Under review as a conference paper at ICLR 2022
X,0.8286713286713286,Figure 10: Examples of simulations for the Cahn-Hilliard equation on a unit interval.
X,0.8321678321678322,"B.3
GAN WITH A DIVERGENCE CONSTRAINT
The data was generated by sampling random velocity ﬁelds and then projecting them to the space
of divergence-free ﬁelds. The procedure was as follows. First, a random velocity ﬁeld u0(x, y) was
generated on a unit square by generating each component i as"
X,0.8356643356643356,"˜u0i(x, y) = N
X"
X,0.8391608391608392,"k,l=−N
λkl cos (kx + ly) + γkl sin (kx + ly),
(22)"
X,0.8426573426573427,"u0i(x, y) = 6 ×

˜u0i(x, y) −min ˜u0i(x, y)
max ˜u0i(x, y) −min ˜u0i(x, y) −0.5

,
(23)"
X,0.8461538461538461,"where N = 10 and λkl, γkl ∼N(0, 1). Then, the divergence-free component of u0(x, y), denoted
by u∗
0(x, y), was extracted by using the projection method by solving ∇· u0 = ∇2φ for φ and then
evaluating u∗
0(x, y) = u0(x, y) −∇φ. Finally, the data was scaled to [−1, 1]."
X,0.8496503496503497,"B.4
GAN WITH A LAPLACIAN CONSTRAINT
The data was generated by solving
∇2u = 0
(24)
on a unit square with Dirichlet boundary conditions. The domain was represented by an unstructured
grid with 2971 nodes. The boundary conditions were generated by generating random functions u0(x)
and using their boundary values as the boundary conditions. The functions u0(x) were generated as"
X,0.8531468531468531,"u0(x, y) = N
X"
X,0.8566433566433567,"k,l=−N
λkl cos (kx + ly) + γkl sin (kx + ly)
(25)"
X,0.8601398601398601,"where N = 5 and λkl, γkl ∼N(0, 1). The data was then scaled to [0, 1]."
X,0.8636363636363636,Under review as a conference paper at ICLR 2022
X,0.8671328671328671,Figure 11: Examples of simulations for the heat equation on a unit square.
X,0.8706293706293706,"C
MODELS, TRAINING AND TESTING"
X,0.8741258741258742,"C.1
REPLACING EXISTING METHODS"
X,0.8776223776223776,"For our comparisons we considered experiments from two works. Next, we provide some details
about these experiments."
X,0.8811188811188811,"The ﬁrst experiment was taken from [37] Section 3.2. The experiment shows how soft physics-based
constraints affect predictions of a GAN learning a distribution of divergence-free ﬁelds. The data
is generated on a uniform grid which allows to evaluate divergence using ﬁnite differences. The
constraint is enforced through an extra loss term which penalizes violation of the constraint. The
performance metric used is the Frobenius norm of the divergence averaged over all ﬁelds in a batch.
For training we used code provided by the authors with original parameters. We replaced ﬁnite
differences in the constrained evaluation function with our method."
X,0.8846153846153846,"The second experiment was taken from [10]. This work deals with the task of predicting sea surface
temperatures at future times given snapshots of the temperature over current and previous times. The
model proposed by the authors accomplishes this tasks by taking a sequence of surface temperatures
at times ti−k, . . . , ti and predicting the underlying motion ﬁeld which is then used to predict the
temperature at time ti+1. Insights about physical properties of the motion ﬁeld were used to constrain
the model’s predictions. Constraints are imposed on divergence, magnitude and gradients of the
motion ﬁeld. The data is generated on a uniform grid which allows to evaluate the constraints using
ﬁnite differences. The constraints are enforced through extra loss terms which penalize violation of
the constraints. Performance metrics that were used are MSE between the data and model predictions,
smoothness loss and divergence loss. For training we used code provided by the authors with original
parameters. We replaced ﬁnite differences in the constrained evaluation function with our method."
X,0.8881118881118881,Under review as a conference paper at ICLR 2022
X,0.8916083916083916,"C.2
CAHN-HILLIARD EQUATION WITH AN INTEGRAL CONSTRAINT"
X,0.8951048951048951,"In all experiments with the Cahn-Hilliard equation we represent the dynamics function Fθ by an MLP
with 2 hidden layers and LeakyReLU nonlinearities (negative slope 0.2). The number of neurons in
each layer was set to the number of nodes in the spatial grid on which the model was trained. The
predictions u(t) were obtained by simulating the system forward in time using adaptive Heun solver
from torchdiffeq package [6] with rtol and atol set to 1.0e-5 and 1.0e-5 respectively. All models were
trained for 1500 epochs using Rprop optimizer [29] with learning rate set to 1.0 · 10−6 and batch size
set to the number of simulations in the training set. Mean squared error was used as the loss function.
Spatial and temporal grids in the testing data were the same as in the training data. We set λ = 2."
X,0.8986013986013986,"C.3
HEAT EQUATION WITH A MONOTONICITY CONSTRAINT"
X,0.9020979020979021,"In all experiments with the heat equation we represent the dynamics function Fθ by a GNN with the
messaging and aggregation networks being MLPs with a single hidden layer consisting of 60 neurons
with Tanh nonlinearities and the input/output sizes of 4/40 and 41/1 respectively. The predictions u(t)
were obtained by simulating the system forward in time using adaptive Heun solver from torchdiffeq
package [6] with rtol and atol set to 1.0e-5 and 1.0e-5 respectively. All models were trained for
750 epochs using Rprop optimizer [29] with learning rate set to 1.0 · 10−6 and batch size set to the
number of simulations in the training set. Mean squared error was used as the loss function. Spatial
and temporal grids in the testing data were the same as in the training data. We set λ = 0.1."
X,0.9055944055944056,"C.4
LEARNING DISTRIBUTIONS OF PHYSICAL FIELDS"
X,0.9090909090909091,"In both cases we used identical architectures and training process for the constrained and uncon-
strained models. Both models were trained for 1.2M iterations using the same random seed. Con-
straints in the constrained model were enabled only after 600k iterations. The base distribution was
set to a 128-dimensional isotropic standard normal. Models were trained using RMSProp optimizer
[35] with batch size and learning rate set to 64 and 0.00001 respectively. The discriminator’s weights
were clipped to [−0.01, 0.01]."
X,0.9125874125874126,"C.4.1
ZERO-DIVERGENCE FIELDS
We used MLPs as a discriminator and generator. The discriminator consisted of 3 hidden layers
of sizes 1024-512-256 with LeakyReLU nonlinearities (negative slope 0.2) and input/output size
of 2010 and 1 respectively. The generator consisted of 3 hidden layers of sizes 256-512-1024 with
LeakyReLU nonlinearities (negative slope 0.2), input/output size of 128 and 2010 respectively, and a
ﬁnal hyperbolic tangent nonlinearity applied to the output. We set λ = 0.2."
X,0.916083916083916,"C.4.2
ZERO-LAPLACIAN FIELDS
We used MLPs as a discriminator and generator. The discriminator consisted of 3 hidden layers
of sizes 1024-512-256 with LeakyReLU nonlinearities (negative slope 0.2) and input/output size
of 1086 and 1 respectively. The generator consisted of 3 hidden layers of sizes 256-512-1024 with
LeakyReLU nonlinearities (negative slope 0.2), input/output size of 128 and 1086 respectively, and
sigmoid function applied to the output. We set λ = 0.0075."
X,0.9195804195804196,"D
CAHN-HILLIARD EQUATION WITH HARD INTEGRAL CONSTRAINTS"
X,0.9230769230769231,"Here we demonstrate how the approach to enforcing hard constraints described in Section 3.3 can be
used to enforce integral constraints on a nonuniform grid."
X,0.9265734265734266,"We use the same setup as in Section 4.2 with 30 training simulations and 50% of nodes in the grid.
We compare three models: unconstrained model, model with soft constraint and model with hard
constraint. We use the PWL basis during training and testing."
X,0.9300699300699301,"Table 1 shows that relative errors of all three models are practically similar but, as Figure 12
demonstrates, constraint violations differ signiﬁcantly. We see that constraint violations of the model
with hard constraint are zero at all time points as expected."
X,0.9335664335664335,"Being able to produce predictions that satisfy some constraints exactly might be very useful for
some applications, however, as we mention in Section 3.3, currently this approach to enforcing hard
constraints is limited to systems with a relatively small number of nodes and is signiﬁcantly slower
than models with soft constraints. We report training times in Table 1."
X,0.9370629370629371,Under review as a conference paper at ICLR 2022
X,0.9405594405594405,Table 1: Test relative errors and training times for the Cahn-Hilliard equation.
X,0.9440559440559441,"Model
Test rel. err
Training time"
X,0.9475524475524476,"Unc.
0.041 ± 0.002
15 min.
Soft-con.
0.044 ± 0.002
17 min.
Hard-con.
0.043 ± 0.002
207 min."
X,0.951048951048951,"Figure 12: Constraint violation plots for the Cahn-Hilliard equation. Each panel shows the mean
absolute violation of the constraint, |
R"
X,0.9545454545454546,"Ωuf(t, x)dx −C|, over all train/test simulations for each time
point."
X,0.958041958041958,"E
LEARNING CAHN-HILLIARD EQUATION WITH GNNS"
X,0.9615384615384616,"We use the same setup as in Section 4.2 with 75% of nodes in the grid but trained the models for 1500
epochs. Instead of an MLP we use a GNN with messaging and aggregation networks being MLPs
with two hidden layers of size 64 and LeakyReLU nonlinearities (negative slope 0.2). For each node,
the GNN evaluates the output as dui"
X,0.965034965034965,dt = γ 
X,0.9685314685314685,"
1
|N(i)| N
X"
X,0.972027972027972,"j∈N(i)
φ(uproj
i
, uproj
j
, xproj
ij ), uproj
i "
X,0.9755244755244755,",
(26)"
X,0.9790209790209791,"where uproj
i
and uproj
j
are linear projections of the state at nodes i and j and xproj
ij
is a linear projection
of the pair consisting of the distance between nodes i and j and a unit vector pointing from node i to
node j. All projections have dimension 16."
X,0.9825174825174825,"We compare constrained and unconstrained models. We use the PWL, PWQ and cubic RBF bases.
Results of the experiment are shown in Figure 13. The ﬁgure shows that relative errors and constraint
violations of all models are signiﬁcantly higher than for MLP-based models."
X,0.986013986013986,Under review as a conference paper at ICLR 2022
X,0.9895104895104895,"Figure 13: Relative errors and constraint violations for a GNN trained on the Cahn-Hilliard equation.
All resuts are for the test set."
X,0.993006993006993,"F
EXTRA FIGURES"
X,0.9965034965034965,Figure 14: Grid used for training GANs.
