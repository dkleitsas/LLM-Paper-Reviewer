Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003937007874015748,"We consider a remote Contextual Multi-Armed Bandit (CMAB) problem, in
which the decision-maker observes the context and the reward, but must communi-
cate the actions to be taken by the agents over a rate-limited communication chan-
nel. This can model, for example, a personalized ad placement application, where
the content owner observes the individual visitors to its website, and hence has the
context information, but must convey the ads that must be shown to each visitor
to a separate entity that manages the marketing content. In this Rate-Constrained
CMAB (RC-CMAB) problem, the constraint on the communication rate between
the decision-maker and the agents imposes a trade-off between the number of bits
sent per agent and the acquired average reward. We are particularly interested in
the scenario in which the number of agents and the number of possible actions are
large, while the communication budget is limited. Consequently, it can be consid-
ered as a policy compression problem, where the distortion metric is induced by
the learning objectives. We ﬁrst consider the fundamental information theoretic
limits of this problem by letting the number of agents go to inﬁnity, and study
the regret that can be achieved. In particular, we identify two distinct rate regions
resulting in linear and sub-linear regret behaviour, respectively. Then, we propose
a practical coding scheme, and provide numerical results for the achieved regret."
INTRODUCTION,0.007874015748031496,"1
INTRODUCTION"
INTRODUCTION,0.011811023622047244,"In the last few years, synergies between Machine Learning (ML) and communication networks have
attracted a lot of interest in the research community, thanks to the fruitful interplay of the two ﬁelds
in emerging applications from Internet of things (IoT) to autonomous vehicles and other edge ser-
vices. In most of these applications, both the generated data and the processing power is distributed
across a network of physically distant devices, thus a reliable communication infrastructure is piv-
otal to run ML algorithms that can leverage the collected distributed knowledge (Park et al., 2019).
To this end, a lot of recent works have tried to redesign networks and to efﬁciently represent infor-
mation to support distributed ML applications, where the activities of data collection, processing,
learning and inference are performed in different geographical locations, and should consider limited
communication, memory, or processing resources, as well as addressing privacy issues."
INTRODUCTION,0.015748031496062992,"In contrast to the insatiable growth in our desire to gather more data and intelligence, available com-
munication resources (bandwidth and power, in particular) are highly limited, and must be shared
among many different devices and applications. This requires the design of highly communication-
efﬁcient distributed learning algorithms , particularly for edge applications. Information theory, in
particular the rate-distortion theory, have laid the fundamental limits of efﬁcient data compression
with the aim to reconstructing the source signal with the highest ﬁdelity (Cover & Thomas, 2006b).
However, in the aforementioned applications, the goal is often not to reconstruct the source signal,
but to make some inference based on that. This requires task-oriented compression, ﬁltering out the
unnecessary information for the target application, and thus decreasing the number of bits that have
to be transmitted over the communication networks. This approach should target the questions of
what is the most useful information that has to be sent, and how to represent it, in order to meet the
application requirements consuming the minimum amount of network resources."
INTRODUCTION,0.01968503937007874,"Our goal in this paper is to investigate a theoretically grounded method to efﬁciently transmit data in
a Contextual Multi-Armed Bandit (CMAB) problem, in which the context information is available to"
INTRODUCTION,0.023622047244094488,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.027559055118110236,"a decision-maker, whereas the actions can be taken by a remote entity, called controller, controlling a
multitude of agents. We assume that a limited communication link is available between the decision-
maker and the controller to communicate at each round the intended actions. The controller must
decide on the actions to take based on the message received over the channel, while the decision-
maker observes the rewards at each round, and updates its policy accordingly."
INTRODUCTION,0.031496062992125984,"This scenario can model, for example, a personalized ad placement application, where the content
owner observes the individual visitors to its website; and hence, has the context information, but
must convey the ads that must be shown to each visitor to a separate entity that manages the mar-
keting content. This will require communicating hundreds or thousands of adds to be placed at
each round, from among a large set of possible adds, within the communication resource and de-
lay constraints of the underlying communication channel, which is quantiﬁed as the number of bits
available per agent. This problem may arise in other similar applications of CMABs with commu-
nication constraints between the decision-maker and the controller (Bouneffouf & Rish, 2019)."
RELATED WORK,0.03543307086614173,"1.1
RELATED WORK"
RELATED WORK,0.03937007874015748,"Given the amount of data that is generated by machines, sensors and mobile devices, the design of
distributed learning algorithms is a hot topic in the ML literature. These algorithms often impose
communication constraints among agents, requiring the design of methods that would allow efﬁcient
representation of messages to be exchanged. While rate-distortion theory deals with efﬁcient lossy
transmission of signals (Cover & Thomas, 2006b), in ML applications, we typically do not need to
reconstruct the underlying signal, but make some inference based on that. These applications can be
modeled through distributed hypothesis testing (Berger, Sep. 1979; Ahlswede & Csisz´ar, 1986) and
estimation (Zhang et al., 2013; Xu & Raginsky, 2017) problems under rate constraints."
RELATED WORK,0.04330708661417323,"In parallel to the theoretical rate-distortion analysis, signiﬁcant research efforts have been invested
in the design of practical data compression algorithms, focusing on speciﬁc information sources,
such as JPEG and BPG for image compression, or MPEG and H.264 for video compression. While
adapting these tools to speciﬁc inference tasks is difﬁcult, recently deep learning techniques have
been employed to learn task-speciﬁc compression algorithms (Torfason et al., 2018; Jankowski et al.,
2021), which achieve signiﬁcant efﬁciency by bypassing image reconstruction."
RELATED WORK,0.047244094488188976,"While the above mainly focus on the inference task through supervised learning, here we con-
sider the CMAB problem. There is a growing literature on multi-agent Reinforcement Learning
(RL) problems with communication links (Foerster et al., 2016; Sukhbaatar et al., 2016; Havrylov
& Titov, 2017; Lazaridou et al., 2017). These papers consider a multi-agent partially observable
Markov decision process (POMDP), where the agents collaborate to resolve a speciﬁc task. In ad-
dition to the usual reward signals, agents can also beneﬁt from the available communication links
to better cooperate and coordinate their actions. It is shown that communication can help overcome
the inherent non-stationarity of the multi-agent environment. Our problem can be considered as
a special case of this general RL formulation, where the state at each time is independent of the
past states and actions. Moreover, we focus on a particular setting in which the communication is
one-way, from the decision-maker that observes the state and the reward, towards the controller that
takes the actions. This formulation is different from the existing results in the literature involving
multi-agent Multi-Armed Bandit (MAB). In Agarwal et al. (2021), each agent can pull an arm and
communicate with others. They do not consider the contextual case, and focus on a particular com-
munication scheme, where each agent shares the index of the best arm according to its experience.
Another related formulation is proposed in Hanna et al. (2021), where a pool of agents collaborate
to solve a common MAB problem with a rate-constrained communication channel from the agents
to the server. In this case, agents observe their rewards and upload them to the server, which in turn
updates the policy used to instruct them. In Park & Faradonbeh (2021), the authors consider a par-
tially observable CMAB scenario, where the agent has only partial information about the context.
However, this paper does not consider any communication constraint, and the partial/ noisy view
of the context is generated by nature. Differently from the existing literature, our goal here is to
identify the fundamental information theoretic limits of learning with communication constraints in
this particular scenario."
RELATED WORK,0.051181102362204724,Under review as a conference paper at ICLR 2022
PROBLEM FORMULATION,0.05511811023622047,"2
PROBLEM FORMULATION"
PROBLEM FORMULATION,0.05905511811023622,"2.1
CONTEXTUAL MULTI-ARMED BANDIT (CMAB) PROBLEM"
PROBLEM FORMULATION,0.06299212598425197,"We consider N agents, which experience independent realizations of the same CMAB problem. The
CMAB is a sequential decision game in which the environment imposes a probability distribution
PS over a set of contexts, or states, S, which is ﬁnite in our case. The game proceeds in rounds, and
at each round t = 1, . . . , T, a realization of the state st,i ∈S is sampled from the distribution PS for
each agent i ∈{1, . . . , N}. At each time step t, and for each agent i, states are sampled iid according
to PS . In the usual CMAB setting, the decision-maker would observe the states {st,i}N
i=1 of the
agents, and choose an action (or arm) at,i ∈{1, . . . , K} = A, for each agent, where K is the total
number of available actions, with probability πt,i(at,i|st,i) given by a (possibly) stochastic policy
πt : S →∆K. Here ∆K is the K-dimensional simplex, containing all possible distributions over the
set of actions. Once the actions have been taken by all the agents, at each round t the environment
returns rewards for all the agents following independent realizations of the same reward process,
rt,i = r(st,i, at,i) ∼PR(r|st,i, at,i),
∀i ∈{1, . . . , N}, which depends on the state and the action
of the corresponding agent. Based on the history of returns up to round t −1, the decision-maker
can optimize its policy to maximize the sum of the rewards G = PT
t=1
PN
i=1 rt,i."
RATE-CONSTRAINED CMAB,0.06692913385826772,"2.2
RATE-CONSTRAINED CMAB"
RATE-CONSTRAINED CMAB,0.07086614173228346,"In our modiﬁed version, the process of observing the system states is spatially separated from
the process of taking the actions.
The environment states, {st,i}N
i=1, are observed by a cen-
tral entity, called the decision-maker, that has to communicate to the controller over a rate-
constrained communication channel, at each round t, the information about the actions {at,i}N
i=1
the agents should take.
The decision-maker can exploit the knowledge accumulated from the
states and rewards observed by all the agents up to round t −1, denoted by H(t −1) =
n
({s1,i}N
i=1 , {a1,i}N
i=1 , {r1,i}N
i=1), . . . , ({st−1,i}N
i=1 , {at−1,i}N
i=1 , {rt−1,i}N
i=1),
o
∈H(t−1), to
optimize the policy to be used at round t. Consequently, the problem is to communicate the ac-
tion distribution, i.e., the policy πt(a|st), which depends on the speciﬁc state realizations observed
in round t, to the controller within the available communication resources while inciting the minimal
impact on the performance of the learning algorithm."
RATE-CONSTRAINED CMAB,0.07480314960629922,"Speciﬁcally, the decision-maker employs function f (N)
t
: H(t−1) ×SN →{1, 2, . . . , B} to map the
history up to time t and the states of the agents at time t to a message index to be transmitted over
the channel. The controller, on the other hand, employs a function g(N)
t
: {1, 2, . . . , B} →AN to
map the received message to a set of actions for the agents. In general, both functions f (N)
t
and g(N)
t
can be stochastic. Average per period regret achieved by sequences
n
f (N)
t
, g(N)
t
oT"
RATE-CONSTRAINED CMAB,0.07874015748031496,t=1 is given by
RATE-CONSTRAINED CMAB,0.08267716535433071,"ρ(N)(T) = 1 T E "" T
X t=1 N
X"
RATE-CONSTRAINED CMAB,0.08661417322834646,"i=1
r(st,i, a∗(st,i) −r(st,i, at) # ,
(1)"
RATE-CONSTRAINED CMAB,0.09055118110236221,"where at,i
=
gt,i(m(t)) is the action taken by agent i based on message m(t)
="
RATE-CONSTRAINED CMAB,0.09448818897637795,"f (N)
t

H(t −1), {st,i}N
i=1

transmitted in round t, a∗(st,i) is the action with maximum mean re-
ward in state st,i, i.e., the optimal action, and the expectation is taken with respect to the state distri-
bution PS and reward distribution PR. We say that, for a given time period T and N agents, an av-"
RATE-CONSTRAINED CMAB,0.0984251968503937,"erage regret- communication rate pair (ρ, R) is achievable if there exist functions
n
f (N)
t
, g(N)
t
oT"
RATE-CONSTRAINED CMAB,0.10236220472440945,"t=1
as deﬁned above with rate 1"
RATE-CONSTRAINED CMAB,0.1062992125984252,N log2 B ≤R and regret ρ(N)(T) ≤ρ.
RATE-CONSTRAINED CMAB,0.11023622047244094,"If a sufﬁciently large rate is available for communication, i.e., R ≥log K, then the intended action
for each agent can be reliably conveyed to the controller. Otherwise, to achieve the learning goal
while satisfying the rate constraint, the decision-maker must apply a lossy compression scheme,
such that the action distribution adopted by the pool of agents resembles the intended policy as
much as possible."
RATE-CONSTRAINED CMAB,0.1141732283464567,Under review as a conference paper at ICLR 2022
RATE-CONSTRAINED CMAB,0.11811023622047244,Figure 1: The problem of communicating policies.
SOLUTION,0.1220472440944882,"3
SOLUTION"
SOLUTION,0.12598425196850394,"In this section, we present the proposed solution to the problem. First of all, we brieﬂy discuss the
algorithm adopted by the decision-maker to solve the CMAB. Then, the communication scheme for
the RC-CMAB with a particular distortion function is provided, which is inherited by the learning
task. We then identify two distinct rate regions resulting in linear and sub-linear regret, and propose
a more practical coding scheme."
THOMPSON SAMPLING,0.12992125984251968,"3.1
THOMPSON SAMPLING"
THOMPSON SAMPLING,0.13385826771653545,"In the proposed solution to the CMAB problem, the decision-maker adopts Thompson Sampling
(TS) Thompson (1935). In particular, it makes use of one TS instance for each state s ∈S. Con-
sequently, the decision-maker maintains an estimate of the distribution ps,a
t (µ) of the mean reward
µs,a ∈D ⊆R of action a in state s at time t. To take the decision in state st, the decision-
maker would sample µst,a
t
∼pst,a
t
, ∀a ∈A, and takes the action a∗= arg maxa∈A{µst,a
t
}. This
procedure is repeated for each agent i ∈{1, . . . , N}. After receiving the rewards {rt,i}N
i=1, the
decision-maker can update its belief on µs,a, i.e., the probabilities ps,a
t (µ), in order to minimize
the regret. We notice that this strategy induces a probability distribution πt(a|s) over the actions
that is πt(a|s) =
R"
THOMPSON SAMPLING,0.1377952755905512,"D ps,a
t (µ) QK
j=1,j̸=a P s,j
t
(µ)dµ, where P s,j
t
(µ) is the Cumulative Distribution
Function (CDF) of µj, where the random variables µs,a are considered independently distributed.
However, the constraint on the rate imposed by the RC-CMAB formulation makes it infeasible for
the decision-maker to sample, through the pool of agents, the actions directly from the true distribu-
tion πt(a|s). The agents have to use a proxy Qt(a|s), which is the one obtained from the message
received. This problem is similar to approximate TS, where a proxy distribution is used to sample
the actions, or the reward means, given that the true distribution is too complex to sample from. In
that case, the bottleneck is given by the complexity of the mean reward distributions, whereas in this
work, it is imposed by the limited-rate communication channel between the decision-maker and the
controller."
OPTIMAL SOLUTION FOR THE RC-CMAB,0.14173228346456693,"3.2
OPTIMAL SOLUTION FOR THE RC-CMAB"
OPTIMAL SOLUTION FOR THE RC-CMAB,0.14566929133858267,"We model the environment as a Discrete Memoryless Source (DMS), that generates states from a
ﬁnite alphabet S with probability PS, emitting sequences of N symbols sN = (s1, . . . , sN), one per
agent. We then denote with ˆQsN (m) the empirical probability of state m in sN. We also consider
the sequence of actions aN, and denote with ˆQzN (m, j) the empirical joint probability of the pair
(m, j) in zN = ((s1, a1), . . . , (sN, aN)). The whole picture can be seen in Fig. 1. In the ﬁgure
above, the actions taken by the agents are denoted by ˆa to indicate that they can differ from a.
However, we are interested in the probability distributions generating the sequences, thus we will
denote with A the random variables indicating both the actions at the controller and decision-maker
side. We assume that the distribution PS is known (or accurately estimated)."
OPTIMAL SOLUTION FOR THE RC-CMAB,0.14960629921259844,"The decision-maker can observe the realization sN of the contexts, and its task is to transmit an
index u ∈{1, . . . , B}, and the agents can generate from u a sequence aN, such that ˆQsNaN is close
to PSA(sa) = PS(s)π(a|s), where closeness depends on a distortion measure E[d( ˆQSNAN , PSA)],
which in general is not an average of a per-letter distortion measure. The problem is a compression
task in which the server has complete (or partial) knowledge of the states sN, and wants to transmit
a conditional probability distribution πA|S to the agents, consuming the minimum amount of bits,
in such a way that the empirical distributions ˆQsNaN given by the sequence induced by the agents"
OPTIMAL SOLUTION FOR THE RC-CMAB,0.15354330708661418,Under review as a conference paper at ICLR 2022
OPTIMAL SOLUTION FOR THE RC-CMAB,0.15748031496062992,"is close to the joint distribution PSA induced by the policy. For a distortion function d(QSA, PSA)
that is 1) nonnegative, 2) upper bounded by a constant Dmax, 3) continuous in QSA, and 4) convex
in QSA, in Kramer & Savari (2007), the authors provide the rate-distortion function R(D), i.e., the
minimum rate R = log2 B"
OPTIMAL SOLUTION FOR THE RC-CMAB,0.16141732283464566,"N
bits per symbol such that E[d( ˆQSNAN , PSA)] ≤D, in the limit when N
is arbitrarily large. The solution is given by"
OPTIMAL SOLUTION FOR THE RC-CMAB,0.16535433070866143,"R(D) =
min
QA|S:d(QSA,PSA)≤D I(S; A),
(2)"
OPTIMAL SOLUTION FOR THE RC-CMAB,0.16929133858267717,"where QSA = PSQA|S is the joint probability induced by the environment distribution PS and
policy QA|S, which depends on the information sent by the decision-maker. As we can see, in the
asymptotic limit of N agents, the problem admits a single-letter solution, which also serves as a
lower bound on the ﬁnite agent scenario. The studied RC-CMAB model described in Sec. 2.2 ﬁts
into this framework, and in the following section we identify an appropriate distortion function and
provide its characterization."
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.1732283464566929,"3.3
THE KL-DIVERGENCE AS DISTORTION FUNCTION"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.17716535433070865,"When applying TS to the RC-CMAB problem with limited communication rate, the decision-maker
may not be able to induce the controller to take samples from the true policy πs,a
t
. In Phan et al.
(2019), the authors provide some theoretical guidelines to construct approximate sampling policies
to make the posteriors, i.e., ps,a
t , concentrate achieving sub-linear regret. In particular, they studied
the case in which the sampling distribution Q differs from the target posterior π, using Dα(π, Q) as
distortion measure, which denotes the α-divergence between the two, and is deﬁned as"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.18110236220472442,"Dα(π, Q) = 1 −
R
π(x)αQ(x)1−αdx"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.18503937007874016,"α(1 −α
.
(3)"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.1889763779527559,"In Phan et al. (2019), it is shown in Theorem 1 that, for α > 0, the condition Dα(π, Q) < δ with
δ > 0, cannot guarantee that the posterior π converges in sub-linear time, even for very small values
of δ. On the contrary, for α ≤0, the authors provide a scheme that can guarantee sub-linear regret.
In particular, they suggest to introduce an exploration term ρt ∈o(1) such that P∞
t=1 ρt = ∞, and
the actions are sampled from Qt with probability 1−ρt, and uniformly at random with probability ρt,
while Dα(πt, Qt) < δ for all t. In this case, it is possible to obtain a sub-linear regret. Consequently,
in order to solve the proposed RC-CMAB problem, we decide to investigate this strategy with α = 0,
which leads to the reverse KL-divergence from Q to π, or equivalently, the KL-divergence from π
to Q, deﬁned as DKL(Q, π) = P"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.19291338582677164,x∈X Q(x) log Q(x)
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.1968503937007874,"π(x) , when x takes values in the discrete set X.
Consequently, to ﬁnd the optimal constrained policy Qt(a|s), we need to ﬁnd a solution to Eq. (2),
which is a rate-distortion optimization problem, when the distortion function is given by the reverse
KL-divergence."
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.20078740157480315,"We can rewrite the optimization objective of Eq. (2) as a double minimization problem (Sec. 10.8,
(Cover & Thomas, 2006a))"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2047244094488189,"R(D) = min
˜
Q(a)
min
QA|S:d(QSA,PSA)≤D X"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.20866141732283464,"s,a
P(s)Q(a|s) log2
Q(a|s)"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2125984251968504,"˜Q(a)
.
(4)"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.21653543307086615,"Following (Lemma 10.8.1, (Cover & Thomas, 2006a)), the marginal Q∗(y) = P"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2204724409448819,"x P(x)Q(y|x) has
the property
Q∗(y) = arg min
˜
Q(y)
DKL(P(x)Q(y|x)||P(x) ˜Q(y)),
(5)"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.22440944881889763,"that is, it minimizes the KL-divergence between the joint and the product P(x)Q(y),
∀Q ∈∆K.
This means that ˜Q(a) obtained by solving Eq. (4) is indeed the marginal over the actions induced by
Q(a|s). Exploiting this formulation, it is possible to apply the iterative Blahut-Arimoto algorithm
to solve the problem and ﬁnd the solution. In particular, given that the two sets ∆K and ∆S are
made of probability distributions, and the target measure is the KL-divergence, the algorithm does
converge to the minimum (Csisz´ar & Tusn´ady, 1984). The process is initialized by setting a random
˜Q0(a), which is used as a ﬁxed point to compute"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2283464566929134,"Q∗
1(a|s) = argminQA|S:d(QSA,PSA)≤D
X"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.23228346456692914,"s
P(s)
X"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.23622047244094488,"a
Q(a|s) log2
Q(a|s)"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.24015748031496062,"˜Q0(a)
.
(6)"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2440944881889764,Under review as a conference paper at ICLR 2022
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.24803149606299213,"From Q∗
1(a|s), we compute the optimal Q∗
1(a) by solving Eq. (5), which is simply the marginal
Q∗
1(a) = P"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.25196850393700787,"s P(s)Q∗
1(a|s). The process is iterated until convergence."
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2559055118110236,"We now solve the inner minimization problem, i.e., Eq. (6) with ﬁxed ˜Q(a). As we can see, the
constraint on the distortion tries to keep QA|S close to πA|S, which is the target distribution. By
minimizing the mutual information, we want QA|S as close to QA as possible, which is the marginal
known by the agent (it is sent once at the beginning of the round and does not depend on the state
realizations), in order to reduce the number of necessary bits to be transmitted. The solution to
Eq. (2) is a conditional distribution QA|S that is a combination of the target policy conditioned on
S, and the marginal distribution."
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.25984251968503935,"We now discuss some simple corner cases. If the maximum acceptable distortion is 0, we need
QA|S = πA|S, thus the rate is equal to EPS

DKL
 
πA|S||πA

. If S and A are independent, i.e., the
policy does not depend on the state, we have πA|S = πA, thus the decision-maker does not need to
send any bits to the agents, which can just sample A ∼πA. Moreover, if we can ﬁnd a distribution
QA such that EPS

DKL
 
QA||πA|S

≤D, then the best strategy is not to convey any message
to the controller, as the constraint is already satisﬁed with rate R(D) = 0. The problem is to ﬁnd,
among all conditional distributions Q(a|s) that satisfy the constraint, the one with the minimum
divergence from Q(a), in order to minimize the rate log2 B"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2637795275590551,"N
, i.e., the number of bits to be consumed
per agent, for an arbitrarily large N, in the general case. We solve the problem using the Lagrangian
multipliers, and obtain the shape of the optimal distribution given by"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2677165354330709,"Qγ∗(a|s) =
˜Q(a)γ∗P(a|s)1−γ∗
P"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.27165354330708663,"a′∈A ˜Q(a′)γ∗P(a′|s)1−γ∗,
∀s ∈S, a ∈A,
(7)"
THE KL-DIVERGENCE AS DISTORTION FUNCTION,0.2755905511811024,where γ∗is s.t. EPS [DKL(Qγ∗||P)] = D. The derivation is provided in Appendix A.1.
ASYMPTOTIC REGRET,0.2795275590551181,"3.4
ASYMPTOTIC REGRET"
ASYMPTOTIC REGRET,0.28346456692913385,"To prove the results on the achievable regret, we need additional assumptions, which are contained
in the Assumption 1 in Russo (2016), which states that rewards have to be distributed following
canonical exponential families, and the priors used by TS over the average rewards have to be uni-
formly bounded, i.e., bounded away from zero ∀(s, a). The proofs of both Lemmas are reported in
the Appendix D."
ASYMPTOTIC REGRET,0.2874015748031496,"In the following, we provide the minimum rate needed to achieve sub-linear regret in all states s ∈S.
We now deﬁne H(A∗) as the marginal entropy of the optimal arm, computed based on the marginal
π∗(a) = P"
ASYMPTOTIC REGRET,0.29133858267716534,"s PS(s)π∗(a|s), and deﬁned as H(A∗) = P"
ASYMPTOTIC REGRET,0.2952755905511811,"a π∗(a) log
1
π∗(a), and we prove that it is
the minimum rate required to achieve sub-linear regret."
ASYMPTOTIC REGRET,0.2992125984251969,"Lemma 3.1. If R < H(A∗), then it is not possible to convey a policy Qt(s, a) that achieves sub-
linear regret in all states s ∈S."
ASYMPTOTIC REGRET,0.3031496062992126,The following Lemma provides the achievibility part.
ASYMPTOTIC REGRET,0.30708661417322836,"Lemma 3.2. If R > H(A∗), then achieving sub-linear regret is possible in all states s ∈S."
ASYMPTOTIC REGRET,0.3110236220472441,"The consequence of this second Lemma is that, even if the exact TS policy πt cannot be transmitted
∀t, as long as sufﬁcient rate is available, i.e., R > H(A∗), it is still possible to achieve sub-linear
regret. Following the notation introduced in Sec. 2.2, this implies that, as T →∞, and for all
sub-linear regrets ρ, the regret-communication pair (ρ, R) is achievable as long as R > H(A∗).
Moreover, we argue that the policy construction found in Sec. 3.3 can achieve better empirical
performance w.r.t. the scheme used to prove Lemma 3.2."
PRACTICAL CODING SCHEME,0.31496062992125984,"3.5
PRACTICAL CODING SCHEME"
PRACTICAL CODING SCHEME,0.3188976377952756,"The above analysis allows us to characterize an information theoretical bound on the optimal per-
formance, but does not provide a constructive communication scheme. To ﬁnd a practical coding
scheme, we propose a solution that is based on state reduction and computes a compact state repre-
sentation. In essence, the decision-maker constructs a message containing the new state representa-
tions ˆs(s) ∈ˆS of s, one for each agent, and send it over the channel. Once the agents have received"
PRACTICAL CODING SCHEME,0.3228346456692913,Under review as a conference paper at ICLR 2022
PRACTICAL CODING SCHEME,0.32677165354330706,"the message, they can sample the actions according to a common policy Qˆs(a|ˆs), which is deﬁned
on the compressed state space ˆS. If the rate constraint imposes B bits per agent, it means that it is
possible to transmit at most 2B different states to each agent. The idea is to group the states into
2B = M clusters ˆs1, . . . , ˆsM, minimizing P"
PRACTICAL CODING SCHEME,0.33070866141732286,"s P(s)DKL(Qˆs(a|ˆs)||π(a|s)), where Qˆs(a|ˆs) is the
policy deﬁned on the state ˆs(s) ∈ˆS."
PRACTICAL CODING SCHEME,0.3346456692913386,"To ﬁnd the clusters and relative policies we employ the well-known Lloyd algorithm, which is an
iterative process to group states into 2B clusters. First of all, knowing the policy π, the decision-
maker maps each state si to a K-dimensional point αi = π(·|si) ∈∆K, ﬁnding |S| = L different
points α1, . . . , αL. Then, it generates 2B = M random points µ1, . . . , µM ∈∆K as initial
centroids, i.e., representative policies, and iterates over the following two steps:"
PRACTICAL CODING SCHEME,0.33858267716535434,"1. Assign to each point αi the class j∗∈{1, . . . , M} such that j∗= arg minj DKL(µj||αi),
i.e., minimizing the divergence between the representative µj∗and the original policy,
which is the point αi. For each cluster j, we now deﬁne with Sj the set containing the
states associated to the policies in the cluster.
2. Update µ1, . . . , µM such that µj = arg minµ∈∆K
P"
PRACTICAL CODING SCHEME,0.3425196850393701,"s∈Sj P(s)DKL(µ||π(·|s)), which
is still a convex optimization problem, and can be solved again applying the Lagrangian
multipliers. The solution is µj = Q"
PRACTICAL CODING SCHEME,0.3464566929133858,s∈Sj π(·|s)
PRACTICAL CODING SCHEME,0.35039370078740156,"P (s)
A(Sj)"
PRACTICAL CODING SCHEME,0.3543307086614173,"Z
,
(8)"
PRACTICAL CODING SCHEME,0.35826771653543305,"where the product has to be considered element-wise, A (Sj) is the sum of the probabilities
of states in Sj, i.e., A (Sj) = P"
PRACTICAL CODING SCHEME,0.36220472440944884,"s∈Sj P(s), and Z is the normalizing factor. After com-
puting the new centroids, we go back to step (1). The derivation of Eq. (8) is provided in
Appendix A.2."
PRACTICAL CODING SCHEME,0.3661417322834646,"The process continues until the new solution does not decrease the KL-divergence between the
clustered and the target policy DKL(Qˆs(ˆs, a)||P(s, a)) = PM
j=1
P"
PRACTICAL CODING SCHEME,0.3700787401574803,s∈Sj P(s)DKL(µj||π(·|s)).
PRACTICAL CODING SCHEME,0.37401574803149606,"Observation Note that the controller is assumed to know the 2NR policies from which it samples
the actions of the agents. This can be transmitted at the beginning of each round. In this case, the
scheme is efﬁcient as long as N log2 K >> BLλ log2 K, where λ is the number of bits used to
represent the values of the Probability Mass Function (PMF) Qˆs(·|ˆs). For this reason, we provide a
scheme where the new policy is updated not at every transmission, but just when the new target π has
changed considerably. In particular, if we denote with πcls the policy deﬁned over the compressed
state representation, with πlast the last policy used to compute πcls, and with π the updated target
policy, we compute and transmit πcls every time DKL(πlast||π) > β."
NUMERICAL RESULTS,0.3779527559055118,"4
NUMERICAL RESULTS"
NUMERICAL RESULTS,0.38188976377952755,"In this section, we provide numerical results in support of our theoretical analysis. Additional ex-
periments and details can be found in Appendix C"
RATE-DISTORTION FUNCTION,0.3858267716535433,"4.1
RATE-DISTORTION FUNCTION"
RATE-DISTORTION FUNCTION,0.38976377952755903,"In this ﬁrst experiment, we analyze the rate-distortion function, and the related optimal Q(a|s), when
the distortion is given by the KL-divergence in three different problems, when the numbers of states,
L, and of actions, K, are both equal to 16. The target policies π(a|s) have a one-to-one relation with
the states, and are generated such that π(a = i|s = j) = 0.99 if j = i, and uniformly distributed
otherwise. We refer to this as the Deterministic case. In the second experiment, the target policies are
generated in a similar way, but the other actions’ probabilities take random values in (0, 0.05], and
then normalized. This, we call the Random Deterministic case. In the third experiment, the states
are grouped in 4 blocks, such that π(a = i|s = j) = 0.99 if ⌊j/4⌋= i, and uniformly distributed
otherwise. This case is denoted as the Block one. In all three experiments, the distribution PS
is uniform over the state space S. The rate-distortion curves are reported in Fig. 2a. As we can
see, in the Deterministic case, the point with zero distortion has R(0) ∼H(PS), where H(PS) is"
RATE-DISTORTION FUNCTION,0.3937007874015748,Under review as a conference paper at ICLR 2022 (a) (b) (c)
RATE-DISTORTION FUNCTION,0.39763779527559057,"Figure 2: Rate-Distortion function for the 3 different experiments (a). π(a|s = 8) and Q(a|s = 8)
in the third experiment, when the rate R is equal to 1 (b). π(a|s = 8) and Q(a|s = 8) in the second
experiment, when the rate R is equal to 1."
RATE-DISTORTION FUNCTION,0.4015748031496063,"the Shannon entropy of PS, as expected. Indeed, in this case, the action distributions are strongly
correlated with the state, thus we need an accurate knowledge of it. In the second experiment, the
starting point is similar, but the curve decreases more rapidly, caused by the random values of the
other actions’ probabilities. In the third experiment, the zero distortion point is achieved at R(0) ∼2
bits, since, similarly to the Deterministic experiment, action probabilities are correlated with the state
realization, but are grouped into four different cases. Again, given the uniform distribution over the
state, R(0) ∼H(Unif[4]), where Unif[4] is the uniform distribution over a set of 4 elements. In
Fig. 2c, the approximate distributions for state s = 8 are reported for the Deterministic and Random
Deterministic cases, when R = 1."
CONTEXTUAL MULTI-ARMED BANDIT,0.40551181102362205,"4.2
CONTEXTUAL MULTI-ARMED BANDIT"
CONTEXTUAL MULTI-ARMED BANDIT,0.4094488188976378,"We now analyze the RC-CMAB problem presented in Sec. 2, and apply the clustered policy schemes
to solve it. In particular, we compare the performance of the Perfect agent, which applies TS with-
out any rate constraint, thus admits samples from the true posterior π, with the performance of
the Comm, Cluster, and Marginal agents. The Comm agent uses the optimal scheme provided
in Sec. 3.2, the Cluster agent implements the practical coding scheme provided in Sec. 3.5, with
B = ⌈R⌉bits per agent, where R is the rate adopted by the Comm agent; the Marginal agent adopts
the marginal over the states, computed from the target policy π and the environment distribution
PS, and serves as a lower bound on the performance. State distribution PS is uniform over L = 16
states, and there are K = 16 actions, N = 100 agents, and the total number of rounds is T = 200.
The threshold for changing the clustered policy is set to β = 0.2, and the communication rate con-
straint to R = 2.5. In this experiment, for each state si ∈S, the best reward is given by the arm
aj such that i = j, with i, j ∈{0, . . . , 15}. In particular, the reward behind arm i when in state
j is a Bernoulli random variable with parameter µj = 0.8 if i = j, whereas µj ∼Unif[0,0.75] if
i ̸= j. In this case, the best action response is strongly correlated with the state realization, thus
a sufﬁciently high rate is required to sample from the target policy π. As it can be observed from
Fig. 3(a), the theoretical rate to transmit the target policy is related to the amount of information the
agent is learning from the environment, and it is computed using Eq. (2). Indeed, during the ﬁrst
∼40 iterations, the rate is below R = 2.5. As the learning process continues, the required rate for
reliable transmission increases, as the mutual information between S and A increases. We highlight
that, in order to fairly represent the 16 different actions, one would need 4 bits. Indeed, another way
of looking at Eq. (2) is through bottleneck principle (Igl et al., 2019), which is used to constrain the
mutual information between the states and the actions, and to encourage exploration, that can be
exploited to reduce the required rate when training multi-agent systems."
CONTEXTUAL MULTI-ARMED BANDIT,0.41338582677165353,"In Fig 3 (b), the KL-divergence between the last policy used to compute the compressed state rep-
resentation, i.e., πlast in Sec. 3.5, and the target known by the decision-maker is reported. The
trend shows that, at the beginning of the learning process, it oscillates rapidly, as the target policy is
changing signiﬁcantly between two iterations. In gray, the threshold β = 2 is highlighted, indicating
when a new cluster policy has to be sent. In this experiment, the decision-maker had to send a new
policy, on average, once every ∼7 rounds. Fig. 4 (a) reports the regret at state s = 10 (the other"
CONTEXTUAL MULTI-ARMED BANDIT,0.41732283464566927,Under review as a conference paper at ICLR 2022
CONTEXTUAL MULTI-ARMED BANDIT,0.421259842519685,"plots can be seen in Appendix C.2), for the four different agents, as deﬁned in Eq. (1). We can see
that both the Cluster and the Comm agents can achieve sub-linear regret in this particular state. In
Fig. 4 (b), the average rewards obtained by the agents are reported, for the different states separately.
We can see that the developed scheme is still able to achieve good performance."
CONTEXTUAL MULTI-ARMED BANDIT,0.4251968503937008,"(a)
(b)"
CONTEXTUAL MULTI-ARMED BANDIT,0.42913385826771655,"Figure 3: Asymptotic rates to convey the Perfect and Comm policies, and the bits used by the Cluster
agent, averaged over 5 runs (a). DKL(πlast||π), averaged over 5 runs (b)."
CONTEXTUAL MULTI-ARMED BANDIT,0.4330708661417323,"(a)
(b)"
CONTEXTUAL MULTI-ARMED BANDIT,0.43700787401574803,"Figure 4: Cumulative regret for different agents, together with the performance of the Marginal
agent, evaluated for the state s = 10 (a). Average reward per state (b)."
CONCLUSION,0.4409448818897638,"5
CONCLUSION"
CONCLUSION,0.4448818897637795,"We have studied the RC-CMAB problem, in which an intelligent entity, i.e., the decision-maker,
observes the contexts of N parallel CMAB processes, and has to decide on the actions depending
on the current contexts and the past actions and rewards. However, the actions are implemented
by a controller that is connected to the decision-maker through a rate-constrained communication
link. First, we cast the problem into the proper information-theoretic framework, formulating it
as a policy compression problem, and provided the optimal compression scheme in the limit of an
inﬁnite number of agents, when the adopted distortion measure is the KL-divergence between the
compressed policy adopted by the controller and the policy of the decision-maker. We then char-
acterize the minimum needed rate to obtain sub-linear regret, and prove that any rates above this
threshold can achieve it. In the end, we designed a practical coding scheme to transmit the actions
for a ﬁnite N, which relies on a compressed state representation Finally, we evaluated the perfor-
mances of the policy obtained through the asymptotic information theoretic formulation, and the one
obtained through the clustering scheme, and observed a close gap between the two. We numerically
showed the relation between the asymptotic rate bound and the learning phase of agents, showing
how it is possible to save communication resources when training a multi-agent system like the one
considered. We believe that this work can serve as a ﬁrst step towards understanding the fundamen-
tal performance limits of multi-agent decision-making problems under communication constraints,
and highlights the intimate relation between the communication scheme and the learning process.
Ongoing work include deriving an information theoretic converse result on the regret performance,
and generalizing the framework to reinforcement learning problems."
CONCLUSION,0.44881889763779526,Under review as a conference paper at ICLR 2022
REFERENCES,0.452755905511811,REFERENCES
REFERENCES,0.4566929133858268,"Mridul Agarwal, Vaneet Aggarwal, and Kamyar Azizzadenesheli. Multi-agent multi-armed bandits
with limited communication. In arXiv:2102.08462 [cs], 2021."
REFERENCES,0.46062992125984253,"Rudolf Ahlswede and Imre Csisz´ar. Hypothesis testing with communication constraints. 32(4):
533–542, Jul. 1986."
REFERENCES,0.4645669291338583,"Toby Berger. Decentralized estimation and decision theory. In IEEE 7th. Spring Workshop on Inf.
Theory, Mt. Kisco, NY, Sep. 1979."
REFERENCES,0.468503937007874,"Djallel Bouneffouf and Irina Rish. A survey on practical applications of multi-armed and contextual
bandits. arXiv cs.LG:1904.10040, 2019."
REFERENCES,0.47244094488188976,"Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, USA, 2006a. ISBN 0471241954."
REFERENCES,0.4763779527559055,"Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, USA, 2006b."
REFERENCES,0.48031496062992124,"Imre Csisz´ar and Gabor Tusn´ady. Information Geometry and Alternating Minimization Procedures.
Statistics and Decisions, Supplement Issue, pp. 205–237, 1984."
REFERENCES,0.484251968503937,"Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to Com-
municate with Deep Multi-Agent Reinforcement Learning. arXiv:1605.06676 [cs], May 2016.
arXiv: 1605.06676."
REFERENCES,0.4881889763779528,"Osama A. Hanna, Lin F. Yang, and Christina Fragouli. Solving multi-arm bandit using a few bits of
communication. In 38 th International Conference on Machine Learning, 2021."
REFERENCES,0.4921259842519685,"Serhii Havrylov and Ivan Titov. Emergence of language with multi-agent games: Learning to com-
municate with sequences of symbols. pp. 11, 2017."
REFERENCES,0.49606299212598426,"Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin,
and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and
information bottleneck. In Advances in Neural Information Processing Systems, volume 32, pp.
13956–13968, 2019."
REFERENCES,0.5,"Mikolaj Jankowski, Deniz G¨und¨uz, and Krystian Mikolajczyk. Wireless image retrieval at the edge.
IEEE Journal on Selected Areas in Communications, 39(1):89–100, 2021. doi: 10.1109/JSAC.
2020.3036955."
REFERENCES,0.5039370078740157,"Cem Kalkanli and Ayfer Ozgur.
Asymptotic convergence of Thompson sampling.
In
arXiv:2011.03917v1, 2020."
REFERENCES,0.5078740157480315,"Gerhard Kramer and Serap A. Savari. Communicating probability distributions. IEEE Transactions
on Information Theory, 53(2):518–525, 2007. doi: 10.1109/TIT.2006.889015."
REFERENCES,0.5118110236220472,"Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the
emergence of (natural) language. arXiv:1612.07182 [cs], March 2017. arXiv: 1612.07182."
REFERENCES,0.515748031496063,"Hongju Park and Mohamad Kazem Shirani Faradonbeh. Analysis of Thompson sampling for par-
tially observable contextual multi-armed bandits, 2021."
REFERENCES,0.5196850393700787,"Jihong Park, Sumudu Samarakoon, Mehdi Bennis, and M´erouane Debbah. Wireless network intel-
ligence at the edge. Proceedings of the IEEE, 107(11):2204–2239, 2019."
REFERENCES,0.5236220472440944,"My Phan, Yasin Abbasi Yadkori, and Justin Domke. Thompson sampling and approximate infer-
ence. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.5275590551181102,"Daniel Russo. Simple bayesian algorithms for best arm identiﬁcation. In Vitaly Feldman, Alexander
Rakhlin, and Ohad Shamir (eds.), 29th Annual Conference on Learning Theory, volume 49 of
Proceedings of Machine Learning Research, pp. 1417–1418, Columbia University, New York,
New York, USA, 23–26 Jun 2016. PMLR. URL https://proceedings.mlr.press/
v49/russo16.html."
REFERENCES,0.531496062992126,Under review as a conference paper at ICLR 2022
REFERENCES,0.5354330708661418,"Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. In Proc. of 30th Int’l Conf. on Neural Information Proc. Systems, NIPS’16, pp.
2252–2260, Red Hook, NY, December 2016."
REFERENCES,0.5393700787401575,"William R. Thompson. On the theory of apportionment. American Journal of Mathematics, 57(2):
450–456, 1935."
REFERENCES,0.5433070866141733,"R´obert Torfason, Fabian Mentzer, Eir´ıkur ´Ag´ustsson, Michael Tschannen, Radu Timofte, and
Luc Van Gool.
Towards image understanding from deep compression without decoding.
In
International Conference on Learning Representations, 2018."
REFERENCES,0.547244094488189,"Aolin Xu and Maxim Raginsky. Information-theoretic lower bounds on Bayes risk in decentralized
estimation. IEEE Transactions on Information Theory, 63(3):1580–1600, 2017. doi: 10.1109/
TIT.2016.2646342."
REFERENCES,0.5511811023622047,"Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic
lower bounds for distributed statistical estimation with communication constraints. In Advances
in Neural Information Processing Systems, volume 26, 2013."
REFERENCES,0.5551181102362205,APPENDIX
REFERENCES,0.5590551181102362,"A
POLICIES DERIVATION"
REFERENCES,0.562992125984252,"A.1
OPTIMAL POLICY"
REFERENCES,0.5669291338582677,"To solve this problem, we solve the related Lagrangian"
REFERENCES,0.5708661417322834,"L(Q(a|s), λ, µ) =
X"
REFERENCES,0.5748031496062992,"s
P(s)
X"
REFERENCES,0.5787401574803149,"a
Q(a|s) log Q(a|s)"
REFERENCES,0.5826771653543307,"˜Qa
+ λ X"
REFERENCES,0.5866141732283464,"s
P(s)
X"
REFERENCES,0.5905511811023622,"a
Q(a|s) log Q(a|s)"
REFERENCES,0.594488188976378,P(a|s) −D ! + + µ X
REFERENCES,0.5984251968503937,"s
P(s)
X"
REFERENCES,0.6023622047244095,"a
Q(a|s) −1 !"
REFERENCES,0.6062992125984252,"where the Lagrangian multiplier λ has to be optimized to meet the constraints on the divergence,
whereas µ ensures that the solution is a probability distributions, i.e., the elements sum up to one.
The positivity constraints on the terms are already satisﬁed by the fact that the solution has an
exponential shape. We ﬁrst take the derivative of the Lagrangian w.r.t. to the terms Q(a|s) and set it
to zero
∂L(Q(a|s), λ, µ)"
REFERENCES,0.610236220472441,"∂Q(a|s)
= P(s) log Q(a|s)"
REFERENCES,0.6141732283464567,"˜Q(a)
+ P(s) −
X"
REFERENCES,0.6181102362204725,"s′
Ps′Q(a|s′) P(s)"
REFERENCES,0.6220472440944882,"˜Q(a)
+"
REFERENCES,0.6259842519685039,"+ λP(s)

log Q(a|s)"
REFERENCES,0.6299212598425197,"P(a|s) + 1

+ µP(s) = 0"
REFERENCES,0.6338582677165354,ﬁnding
REFERENCES,0.6377952755905512,"log
Q(a|s)1+λ"
REFERENCES,0.6417322834645669,˜Q(a)P(a|s)λ = −(λ + µ)
REFERENCES,0.6456692913385826,Q(a|s)1+λ = e−(µ+λ) ˜Q(a)P(a|s)λ
REFERENCES,0.6496062992125984,Q(a|s) = e
REFERENCES,0.6535433070866141,−(µ+λ)
REFERENCES,0.65748031496063,"1+λ
˜Q(a)
1
1+λ P(a|s)
λ
1+λ"
REFERENCES,0.6614173228346457,"and now we rewrite with γ =
1
1+λ, γ ∈[0, 1], and µ such that e"
REFERENCES,0.6653543307086615,−(µ+λ)
REFERENCES,0.6692913385826772,"1+λ
is the normalizing factor.
The solution has thus the shape"
REFERENCES,0.6732283464566929,"Qγ(a|s) =
˜Q(a)γP(a|s)1−γ
P"
REFERENCES,0.6771653543307087,"a′∈A ˜Q(a′)γP(a′|s)1−γ ,
∀s ∈S, a ∈A.
(9)"
REFERENCES,0.6811023622047244,"By the convexity of KL-Divergence and its triangular inequality, we know the solution lies on the
boundary of the constraints, i.e., when EPS [DKL(Qγ∗||P)] = D. Consequently, if we can numeri-
cally ﬁnd γ∗s.t. EPS [DKL(Qγ∗||P)] = D, we solve Eq. (6)."
REFERENCES,0.6850393700787402,Under review as a conference paper at ICLR 2022
REFERENCES,0.6889763779527559,"A.2
UPDATE CENTROIDS"
REFERENCES,0.6929133858267716,"Again, we compute the optimal centroids by solving the Lagrangian"
REFERENCES,0.6968503937007874,"L(µj
a, λ) =
X"
REFERENCES,0.7007874015748031,"s∈Sj
P(s)
X"
REFERENCES,0.7047244094488189,"a∈A
µj
a log
µj
a
π(a|s) + λ X"
REFERENCES,0.7086614173228346,"a∈A
µj
a −1 !"
REFERENCES,0.7125984251968503,taking its derivative and solving the equality
REFERENCES,0.7165354330708661,"∂L(µj
a, λ)"
REFERENCES,0.7204724409448819,"∂µj
a
=
X"
REFERENCES,0.7244094488188977,"s∈Sj
P(s)

log
µj
a
π(a|s) + 1

+ λ = 0"
REFERENCES,0.7283464566929134,ﬁnding
REFERENCES,0.7322834645669292,"log µj
aA (Sj) =
X"
REFERENCES,0.7362204724409449,"s∈Sj
P(s) log π(a|s) + A (Sj) + λ"
REFERENCES,0.7401574803149606,"log µj
a =
X s∈Sj"
REFERENCES,0.7440944881889764,"P(s)
A (Sj) log π(a|s) + 1 +
λ
A (Sj) µj = Q"
REFERENCES,0.7480314960629921,s∈Sj π(·|s)
REFERENCES,0.7519685039370079,"P (s)
A(Sj) Z
,"
REFERENCES,0.7559055118110236,"where Z is the normalizing factor, obtaining the shape expressed in Eq. (8)."
REFERENCES,0.7598425196850394,Under review as a conference paper at ICLR 2022
REFERENCES,0.7637795275590551,"B
PRESENTED ALGORITHMS"
REFERENCES,0.7677165354330708,"In this appendix, the algorithms for the theoretically-optimal and cluster policies are described."
REFERENCES,0.7716535433070866,"B.1
THEORETICAL OPTIMAL POLICY"
REFERENCES,0.7755905511811023,"This is the pseudocode of the algorithm at the decision-maker side, when adopting the information-
theoretic inspired compressed policy. The only task for the controller is to decode the received
message, and to communicate the actions to the agents. We deﬁne with [A] the set {1, . . . , A}, and
with UnifB the uniform probability over the set B."
REFERENCES,0.7795275590551181,"Algorithm 1 Optimal Policy
Input rate R, n° agents N, n° actions K and state distribution PS
Initialize Thompson Sampling policy π, and compressed policy Q(·|s) to Unif[K]
foreach t ∈1, . . . , T do"
REFERENCES,0.7834645669291339,"Observe sN
t = (st,1, . . . , st,N)
Sample aN
t = (at,1, . . . , at,N) from Q(·|s)
Code aN
t into ut = ft(aN
t ) and transmit to the controller
Observe rN
t = (rt,1, . . . , rt,N)
Update action posteriors π(·|s) using the Thompson Sampling Algorithm
Update Q(a|s) using the Blahut-Arimoto Algorithm (Iterate over Eq. 5 and Eq. 6)"
REFERENCES,0.7874015748031497,"B.2
CLUSTER POLICY"
REFERENCES,0.7913385826771654,"This is the pseudocode of the algorithm at the decision-maker side, when adopting the cluster policy.
The task for the controller is to sample, at time t and for each agent i, the action at,i according to
the last received centroid µj(st,i), whose components convey the action probabilities."
REFERENCES,0.7952755905511811,"Algorithm 2 Cluster Policy
Input per-agent bit B, n° agents N, n° actions K and state distribution PS, threshold β
Initialize Thompson Sampling policy π
Compute the centroids µj for j = 1, . . . , 2B using π with Lloyd algorithm and the rule in Eq. 8
Transmit the centroids µj to all agents.
πlast = π
foreach t ∈1, . . . , T do"
REFERENCES,0.7992125984251969,"Observe sN
t = (st,1, . . . , st,N)
if DKL
 
πlast||π

> β then
Update the centroids µj using π with Lloyd algorithm and the rule in Eq. 8
Transmit the centroids µj to all agents.
∀st,i compute the index j(st,i) indicating its belonging cluster
Transmit the vector jN
t = (j(st,1), . . . , j(st,N))
Observe rN
t = (rt,1, . . . , rt,N)
Update action posteriors π(·|s) using the Thompson Sampling Algorithm"
REFERENCES,0.8031496062992126,Under review as a conference paper at ICLR 2022
REFERENCES,0.8070866141732284,"C
EXPERIMENTS"
REFERENCES,0.8110236220472441,"In this section, additional experiments are provided to clarify the proposed scheme, and quantify the
trade-offs between rate and regret."
REFERENCES,0.8149606299212598,"C.1
STATE REPRESENTATION"
REFERENCES,0.8188976377952756,"In Fig. 5 we report the policy clustering results when applying the coding scheme explained in
Sec. 3.5, when R = 2 bits can be used to represent 20 randomly generated policies. In the ﬁgure,
it is possible to see the 4 policies representative of the 4 compressed state representations. As we
can see, the policy centroids found by the coding scheme try to fairly resemble the shapes of all the
policies within the same cluster."
REFERENCES,0.8228346456692913,Figure 5: Clusters and their representatives with L = 20 and b = 2.
REFERENCES,0.8267716535433071,"C.2
DETERMINISTIC"
REFERENCES,0.8307086614173228,In this section we reported some more details on the RC-CMAB experiments presented in Sec. 4.2.
REFERENCES,0.8346456692913385,"(a)
(b) (c)"
REFERENCES,0.8385826771653543,"Figure 6: Best action probability for a given state, for the posterior π(a∗|s) (a) and compressed
policy Q(a∗|s) (b) for the different agents. KL-divergence between the agents’ action posteriors,
and the target one (c)."
REFERENCES,0.84251968503937,Under review as a conference paper at ICLR 2022
REFERENCES,0.8464566929133859,Under review as a conference paper at ICLR 2022
REFERENCES,0.8503937007874016,"Figure 8: Cumulative average regret ± one std for the different agents, from state s = 0 to state
s = 15"
REFERENCES,0.8543307086614174,Under review as a conference paper at ICLR 2022
REFERENCES,0.8582677165354331,"C.3
8-BLOCK EXPERIMENT"
REFERENCES,0.8622047244094488,"In this second RC-CMAB experiment, the setting is similar to the one presented above, but the
best action response is not a one-to-one mapping with the state. Again, a ∈{0, . . . , 15} and s ∈
{0, . . . , 15}, but the Bernoulli parameter µa(s) for action a in state s is 0.8 if ⌊s"
REFERENCES,0.8661417322834646,"2⌋= a, and sampled
uniformly in (0, 0.75] otherwise. Thus, the best action responses are grouped into 8 different classes,
depended on the state realization. In this case, the rate is limited to R = 2."
REFERENCES,0.8700787401574803,"(a)
(b)"
REFERENCES,0.8740157480314961,"Figure 9: Block experiment : asymptotic rates to convey the Perfect and Comm policies, and the bits
used by the Cluster agent, averaged over 5 runs (a). Average reward achieved in each state by the
pool of agents (b)"
REFERENCES,0.8779527559055118,"(a)
(b) (c)"
REFERENCES,0.8818897637795275,"Figure 10: Block experiment : Best action probability for a given state, for the posterior π(a∗|s) (a)
and compressed policy Q(a∗|s) (b) for the different agents. KL-divergence between the agents’
action posteriors, and the target one (c)."
REFERENCES,0.8858267716535433,Under review as a conference paper at ICLR 2022
REFERENCES,0.889763779527559,Under review as a conference paper at ICLR 2022
REFERENCES,0.8937007874015748,"Figure 12: Block experiment : cumulative average regret ± one std for the different agents, from
state s = 0 to state s = 15"
REFERENCES,0.8976377952755905,Under review as a conference paper at ICLR 2022
REFERENCES,0.9015748031496063,"C.4
REGRET VS NUMBER OF CLASSES"
REFERENCES,0.905511811023622,"In this last experiment, the environment is the same as the one described in Sec. 4.2, a part form
the number of agents, that here is N = 50. The task is to quantify the effect of the number of
per-agent bits B in the cluster policy on the achievable regret, which varies from 1 to 4. As we
can see from the plots below, when B is equal to 1 or 2, which in turn means that the number of
clusters is, respectively, 2 and 4, the regret is not sub-linear in several states. However, the policy
with just 2 bits can achieve performance comparable with the 3 and 4 bits policies in some states,
e.g., state 4 and 6. This is due to the fact that even in those cases with not enough bits, the posteriors
still converges to good solutions, thus if a cluster contains, for example, one state, it will convey the
best policy, achieving sub-linear regret. On the contrary, when a cluster contains more states, the
representative policy can fail to achieve optimal performance, if those are substantially different."
REFERENCES,0.9094488188976378,Under review as a conference paper at ICLR 2022
REFERENCES,0.9133858267716536,"Figure 14: Deterministic experiment : cumulative average regret ± one std for cluster policy, when
B = {1, 2, 3, 4}, from state s = 0 to state s = 15
."
REFERENCES,0.9173228346456693,Under review as a conference paper at ICLR 2022
REFERENCES,0.9212598425196851,"D
REGRET BOUND"
REFERENCES,0.9251968503937008,"To prove our statements, we deﬁne with Hq(X) and Iq(X; Y ) the marginal entropy and mutual
information w.r.t. to the joint probability q, i.e., H(A∗) = Hπ∗(A). We start by proving Lemma D.1.
Lemma D.1. If the exact Thompson Sampling policies πt(a|s) achieve sub-linear Bayesian regret
for all state s ∈S, then limt→∞Iπt(S; A) = limt→∞Hπt(A) = H(A∗)."
REFERENCES,0.9291338582677166,"Proof. First of all, we write Iπt(S; A) = Hπt(A)−Hπt(A|S). Following Theorem 2 from Kalkanli
& Ozgur (2020), if πt(a|s) achieves sub-linear regret ∀s ∈S, then ∀s ∈S"
REFERENCES,0.9330708661417323,"lim
t→∞πt(a = a∗|s) = 1
when a∗is the optimal arm"
REFERENCES,0.937007874015748,"lim
t→∞πt(a = a′|s) = 0
when a′ ̸= a∗."
REFERENCES,0.9409448818897638,"Consequently, we have that in the limit πt(a|s) is a deterministic function, thus"
REFERENCES,0.9448818897637795,"lim
t→∞Hπt(A∗|S) = 0,"
REFERENCES,0.9488188976377953,which concludes our proof.
REFERENCES,0.952755905511811,"Then, we prove Lemma 3.1, which is repeated below."
REFERENCES,0.9566929133858267,"Lemma 3.1 If R < H(A∗), then it is not possible to convey a policy Qt(s, a) that achieves sub-
linear regret in all state s ∈S."
REFERENCES,0.9606299212598425,"Proof. Following Lemma 10 in Phan et al. (2019), if limt→∞πt(a∗|s) = 1 and DKL (Qt||πt) < ϵ,
∀t, for some ϵ > 0, then limt→∞Qt(a∗|s) = 1, inducing limt→∞DKL (Qt||πt) = 0."
REFERENCES,0.9645669291338582,"Now, for any policy Qt, if the minimal rate to convey it, ˆRt = IQt(S; A), is below the threshold of
the optimal policy, i.e., ˆRt < H(A∗), by Eq. (2), we have"
REFERENCES,0.968503937007874,"DKL (Qt||π∗) > 0,
(10)"
REFERENCES,0.9724409448818898,"which holds also in the limit. Otherwise, we would have a policy Qt s.t. DKL (Qt||π∗) = 0, which
could be conveyed at a rate strictly below that characterized by Eq. (2), which would contradict with
the deﬁnition of the rate-distortion function."
REFERENCES,0.9763779527559056,"Consequently, by Lemma 10 in Phan et al. (2019) and Eq. (10), there cannot be any ϵ > 0 s.t.
limt→∞DKL (Qt||πt) < ϵ; and hence, limt→∞DKL (Qt||πt) = ∞."
REFERENCES,0.9803149606299213,"However, if limt→∞DKL (Qt||πt) = ∞, ∃(s, a) ∈S × A s.t. limt→∞Qt(a|s) = c > 0, when
a ̸= a∗. This implies that, at step t, Qt(s, a) plays a sub-optimal arm in state s with constant
probability, and so it can not achieve sub-linear regret in all states."
REFERENCES,0.984251968503937,"Finally, we provide a proof for Lemma 3.2, which we repeat below."
REFERENCES,0.9881889763779528,"Lemma 3.2 If R > H(A∗), then achieving sub-linear regret is possible in all states s ∈S."
REFERENCES,0.9921259842519685,"Proof. We deﬁne by Rπt the rate needed to convey the TS policy perfectly to the controller at time
t, and let δ > 0 s.t. R = H(A∗) + δ, where R is the available communication rate. We now provide
a scheme that guarantees sub-linear regret."
REFERENCES,0.9960629921259843,"First, generate ρt parameters as in Sec. 3.3. As long as Rπt > R, with probability ρt play at
uniformly at random, and with probability 1−ρt, play according to a policy Qt(a|s), which satisﬁes
the rate-distortion constraint I(A; S) < R under Qt(a|s), which can be transmitted to the controller,
and will have a bounded reverse KL divergence from the TS policy πt at time t. Following Lemma 14
in Russo (2016), in this way enough exploration is guaranteed for the TS policy to concentrate, and
there exists a ﬁnite t0 s.t. ∀t > t0, Rπt < H(A∗) + δ. This means that, for the ﬁrst t0 rounds, both
the target and the approximating policies are playing sub-optimal arms with non-zero probabilities.
However, their average gap is within a constant, given the average rewards are bounded within [0, 1].
Then, ∀t > t0 it is possible to play the exact TS policy, leading to the optimal policy for all future
steps, and hence, a sub-linear regret."
