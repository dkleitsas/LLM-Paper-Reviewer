Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0064516129032258064,"Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves
retrieving nodes from a knowledge graph (KG) to answer natural language ques-
tions. Recent GNN-based approaches formulate this task as a KG path searching
problem, where messages are sequentially propagated from the seed node towards
the answer nodes. However, these messages are past-oriented, and they do not
consider the full KG context. To make matters worse, KG nodes often represent
proper noun entities and are sometimes encrypted, being uninformative in selecting
between paths. To address these problems, we propose Neural Tree Search (Nu-
Trea), a tree search-based GNN model that incorporates the broader KG context.
Our model adopts a message-passing scheme that probes the unreached subtree
regions to boost the past-oriented embeddings. In addition, we introduce the Rela-
tion Frequency‚ÄìInverse Entity Frequency (RF-IEF) node embedding that considers
the global KG context to better characterize ambiguous KG nodes. The general
effectiveness of our approach is demonstrated through experiments on three major
multi-hop KGQA benchmark datasets, and our extensive analyses further validate
its expressiveness and robustness. Overall, NuTrea provides a powerful means
to query the KG with complex natural language questions. Code is available at
https://github.com/mlvlab/NuTrea."
INTRODUCTION,0.012903225806451613,"1
Introduction"
INTRODUCTION,0.01935483870967742,"The knowledge graph (KG) is a multi-relational data structure that defines entities in terms of their
relationships. Given its enormous size and complexity, it has long been a challenge to properly
query the KG via human languages [1‚Äì6]. A corresponding machine learning task is knowledge
graph question answering (KGQA), which entails complex reasoning on the KG to retrieve the nodes
that correctly answers the given natural language question. To resolve the task, several approaches
focused on parsing the natural language to a KG-executable form [7‚Äì10], whereas others tried to
process the KG so that answer nodes can be ranked and retrieved [11‚Äì14, 2]. Building on these
works, there has been a recent stream of research focusing on answering more complex questions
with intricate constraints, which demand multi-hop reasoning on the KG."
INTRODUCTION,0.025806451612903226,"‚àóWork done at Korea University.
‚Ä†Corresponding author."
INTRODUCTION,0.03225806451612903,"Answering complex questions on the KG requires processing both the KG nodes (entities) and
edges (relations). Recent studies have addressed multi-hop KGQA by aligning the question text
with KG edges (relations), to identify the correct path from seed nodes (i.e., nodes that represent
the question subjects) towards answer nodes. Many of these methods, however, gradually expand
the search area outwards via message passing, whose trailing path information is aggregated onto
the nodes, resulting in node embeddings that are past-oriented. Also, as many complex multi-hop
KGQA questions require selecting nodes that satisfy specific conditions, subgraph-level (subtree-
level) comparisons are necessary in distinguishing the correct path to the answer node. Furthermore,
KG node entities often consist of uninformative proper nouns, and sometimes, for privacy concerns,
they may be encrypted [15]. To address these issues, we propose Neural Tree Search (NuTrea), a
graph neural network (GNN) model that adopts a tree search scheme to consider the broader KG
contexts in searching for the path towards the answer nodes."
INTRODUCTION,0.03870967741935484,"NuTrea leverages expressive message passing layers that propagate subtree-level messages to ex-
plicitly consider the complex question constraints in identifying the answer node. Each message
passing layer consists of three steps, Expansion ‚ÜíBackup ‚ÜíNode Ranking, whose Backup step
probes the unreached subtree regions to boost the past-oriented embeddings with future information.
Moreover, we introduce the Relation Frequency‚ÄìInverse Entity Frequency (RF-IEF) node embedding,
which takes advantage of the global KG statistics to better characterize the KG node entities. Overall,
NuTrea provides a novel approach in addressing the challenges of querying the KG, by allowing
it to have a broader view of the KG context in aligning it with human language questions. The
general effectiveness of NuTrea is evaluated on three major multi-hop KGQA benchmark datasets:
WebQuestionsSP [16], ComplexWebQuestions [17], and MetaQA [18]."
INTRODUCTION,0.04516129032258064,"Then, our contributions are threefold:"
INTRODUCTION,0.05161290322580645,"‚Ä¢ We propose Neural Tree Search (NuTrea), an effective GNN model for multi-hop KGQA, which
adopts a tree search scheme with an expressive message passing algorithm that refers to the
future-oriented subtree contexts in searching paths towards the answer nodes.
‚Ä¢ We introduce Relation Frequency-Inverse Entity Frequency (RF-IEF), a simple node embedding
technique that effectively characterizes uninformative nodes using the global KG context.
‚Ä¢ We achieve the state-of-the-art on multi-hop KGQA datasets, WebQuestionsSP and ComplexWe-
bQuestions, among weakly supervised models that do not use ground-truth logical queries."
RELATED WORKS,0.05806451612903226,"2
Related Works"
RELATED WORKS,0.06451612903225806,"A knowledge graph (KG) is a type of a heterogeneous graph [19, 20] G = (V, E), whose edges in E
are each assigned to a relation type by the mapping function ùëì: E ‚ÜíR. The KGs contain structured
information from commonsense knowledge [15, 21] to domain-specific knowledge [22], and the
Knowledge Graph Question Answering (KGQA) task aims to answer natural language questions
grounding on these KGs by selecting the set of answer nodes. Recent methods challenge on more
complex questions that require multi-hop graph traversals to arrive at the correct answer node. Thus,
the task is aliased as multi-hop KGQA or complex KGQA, which is generally discussed in terms of
two mainstream approaches [23]: Semantic Parsing and Information Retrieval.
Semantic Parsing.
The main idea of semantic parsing-based methods is to first parse natural
language questions into a logical query. The logical query is then grounded on the given KG in an
executable form. For example, [8] applies the semantic query graph generated by natural language
questions. In replace of hand-crafted query templates, [7] introduced a framework that automatically
learns the templates from the question-answer pairs. Also, [24] proposed a novel graph generation
method for query structure prediction. Other methods take a case-based reasoning (CBR) approach,
where previously seen questions are referenced to answer a complex question. Approaches like
[9, 25, 26] use case-based reasoning by referring to similar questions or KG structures. Recently,
[10] proposed a framework that jointly infers logical forms and direct answers to reduce semantic
errors in the logical query. A vast majority of methods that take the semantic parsing approach utilize
the ground-truth logical forms or query executions during training. Thus, these supervised methods
are generally susceptible to incomplete KG settings, but have high explainability.
Information Retrieval.
Information Retrieval-based methods focus on processing the KG to
retrieve the answer nodes. The answer nodes are selected by ranking the subgraph nodes conditioned
on the given natural language question. One of the former works [27] proposes an enhanced Key-Value"
RELATED WORKS,0.07096774193548387,"Memory neural network to answer more complex natural language questions. [28] and [29] extract
answers from question-specific subgraphs generated with text corpora. To deal with incompleteness
and sparsity of KG, [11] presents a KG embedding method to answer questions. Also, [12] handles
this problem by executing query in the latent embedding space. In an effort to improve explainability
of the information retrieval approach, [13] infers an adjacency matrix by learning the activation
probability of each relation type. By adapting the teacher-student framework with the Neural State
Machine (NSM), [14] made learning more stable and efficient. Furthermore, [30] utilized multi-task
learning that jointly trains on the KG completion task and multi-hop KGQA, while [2] provides a
novel link prediction framework. Recently, there has been a trend of borrowing the concept of logical
queries from Semantic Parsing approaches, attempting to learn the logical instructions that guide the
path search on the KG [2, 1]. Our NuTrea also builds on these approaches."
METHOD,0.07741935483870968,"3
Method
In recent studies, models that sequentially process the knowledge graph (KG) from the seed nodes
have shown promising results on the KGQA task. Building upon this approach, we propose Neural
Tree Search (NuTrea). NuTrea adopts a novel message passing scheme that propagates the broader
subtree-level information between adjacent nodes, which provides more context in selecting nodes
that satisfy the complex question constraints. Additionally, we introduce a node embedding technique
called Relation Frequency‚ÄìInverse Entity Frequency (RF-IEF), which considers the global KG
information when initializing node features. These methods allow for a richer representation of each
node by leveraging the broader KG context in answering complex questions on the KG."
PROBLEM DEFINITION,0.08387096774193549,"3.1
Problem Definition"
PROBLEM DEFINITION,0.09032258064516129,"Here, we first define the problem settings for Neural Tree Search (NuTrea). The multi-hop KGQA
task is primarily a natural language processing problem that receives a human language question ùë•ùëûas
input, and requires retrieving the set of nodes from G = (V, E) that answer the question. Following
the standard protocol in KGQA [11], the subject entities in ùë•ùëûare given and assumed always to
be mapped to a node in V via entity-linking algorithms [31]. These nodes are called seed nodes,
denoted ùë£ùë†‚ààVùë†, which are used to extract a subgraph Gùëû= (Vùëû, Eùëû) from G so that Vùëûis likely to
contain answer nodes ùë£ùëé‚ààVùëé. Then, the task reduces to a binary node classification problem of
whether each node ùë£‚ààVùëûsatisfies ùë£‚ààVùëé."
PROBLEM DEFINITION,0.0967741935483871,"Following prior works [14, 1], we first compute different question representation vectors with a
language model based module, called Instruction Generators (IG). We have two IG modules, each for
the Expansion (section 3.2.1) and Backup (section 3.2.2) step, to compute"
PROBLEM DEFINITION,0.1032258064516129,"{q(ùëñ)
exp}ùëÅ
ùëñ=1 = IGexp(ùë•ùëû),
{q( ùëó)
bak}ùëÄ
ùëó=1 = IGbak(ùë•ùëû),
(1)"
PROBLEM DEFINITION,0.10967741935483871,"where we name q(ùëñ)
exp, q( ùëó)
bak ‚ààRùê∑as the expansion instruction and backup instruction, respectively.
Detailed description on the IG module is in the supplement. Then, the learnable edge (relation) type
embeddings ùëπ‚ààR|R|√óùê∑are randomly initialized or computed with a pretrained language model.
On the other hand, node embeddings ùëØ‚ààR|Vùëû|√óùê∑of Vùëûare initialized using the edges in Eùëûand
their relation types by function H as ùëØ= H (Eùëû, ùëπ) (e.g., H ‚â°arithmetic mean of incident edge
relations). This is because the node entities are often uninformative proper nouns or encrypted codes.
Then, our NuTrea model F function is defined as
ÀÜy = F ({q(ùëñ)
exp}ùëÅ
ùëñ=1, {q( ùëó)
bak}ùëÄ
ùëó=1, Vùë†; Gùëû, ùëØ, ùëπ),
(2)"
PROBLEM DEFINITION,0.11612903225806452,"where ÀÜy ‚ààR|Vùëû| is the predicted node score vector normalized across nodes in Vùëû, whose ground-
truth labels are y = [1(ùë£‚ààVùëé)]ùë£‚ààVùëû‚ààB|Vùëû|. Then, the model is optimized with the KL divergence
loss between ÀÜy and y. In this work, we claim our contributions in F (section 3.2) and H (section 3.3)."
PROBLEM DEFINITION,0.12258064516129032,"3.2
Neural Tree Search (F )"
PROBLEM DEFINITION,0.12903225806451613,"Our Neural Tree Search (NuTrea) model consists of multiple layers that each performs message
passing in three consecutive steps: (1) Expansion, (2) Backup, and (3) Node ranking. The Expansion
step propagates information outwards to expand the search tree, which is followed by the Backup
step, where the depth-ùêæsubtree content is aggregated to each of their root nodes, to enhance the
past-oriented messages from the Expansion step. Then, the nodes are scored based on how likely
they answer the given question."
PROBLEM DEFINITION,0.13548387096774195,"Figure 1: Neural Tree Search. Given a natural language question, a corresponding KG subgraph Gùëûis extracted,
and expansion instructions qexp and backup instructions qbak are computed with Instruction Generators (IG).
The node embeddings of Gùëûare first defined by our RF-IEF (section 3.3), which characterizes nodes based on
the global KG information by suppressing prevalent relation types that hold less meaning. Then, in each NuTrea
layer (section 3.2), messages are propagated outwards from the seed node with respect to qexp (Expansion),
which contain future-oriented subtree information conditioned by qbak (Backup). Then, the nodes are scored
by adding and normalizing the logits (Node Ranking), which are utilized in the subsequent layer. The figure
describes the first layer of NuTrea whose Backup subtree depth ùêæis 2, and ùúôis the softmax function. Overall,
NuTrea considers the broader KG contexts in distinguishing the correct paths, contrary to previous methods that
do not incorporate the Backup step."
EXPANSION,0.14193548387096774,"3.2.1
Expansion"
EXPANSION,0.14838709677419354,"Starting from the seed node ùë£ùë†‚ààVùë†, a NuTrea layer first expands the search tree by sequentially
propagating messages outwards to the adjacent nodes. The propagated messages f (ùëñ)
ùë¢ùë£are conditioned
on the expansion instructions {q(ùëñ)
exp}ùëÅ
ùëñ=1, which are computed as"
EXPANSION,0.15483870967741936,"f (ùëñ)
ùë¢ùë£= ReLU(ùëæfrùë¢ùë£‚äôq(ùëñ)
exp),
(3)"
EXPANSION,0.16129032258064516,"where ùëñ‚àà[1, ùëÅ], ùëæf ‚ààRùê∑√óùê∑is a learnable linear projection, rùë¢ùë£= row(ùëπ) ‚ààRùê∑is the relation
type embedding of edge ùë¢‚Üíùë£, and ‚äôis an element-wise product operator. Optionally, we use the
relative position embedding eùë¢ùë£‚ààRùê∑as"
EXPANSION,0.16774193548387098,"f (ùëñ)
ùë¢ùë£= ReLU((ùëæfrùë¢ùë£+ eùë¢ùë£) ‚äôq(ùëñ)
exp),
(4)"
EXPANSION,0.17419354838709677,"where eùë¢ùë£is defined for each relation type. Then, for an edge ùë¢‚Üíùë£, ùëÅtypes of messages are
propagated, which are element-wise products between the edge relation and the ùëÅdifferent question
representation vectors. This operation highlights the edges that are relevant to the given question.
Then, the messages are aggregated to a node ùë£via an MLP aggregator, computed as"
EXPANSION,0.18064516129032257,"Àúfùë£=
ùëÅ
ùëñ=1
‚àëÔ∏Å"
EXPANSION,0.1870967741935484,"ùë¢‚ààN(ùë£)
ùë†ùë¢f (ùëñ)
ùë¢ùë£"
EXPANSION,0.1935483870967742,"fùë£= MLP(hùë£‚à•Àúfùë£),
(5)"
EXPANSION,0.2,"where hùë£= row(ùëØ) ‚ààRùê∑is the node embedding and ùë†ùë¢‚ààR is the score value of a head node
ùë¢. In the first layer, the seed nodes are the only head nodes, whose scores are initially set to 1. In
subsequent layers, we use the updated node scores as ùë†ùë¢, whose computation will be introduced
shortly in section 3.2.3. Notably, the nodes with score ùë†ùë¢= 0, which are typically nodes that are yet
to be reached, do not pass any message to their neighbors."
BACKUP,0.2064516129032258,"3.2.2
Backup"
BACKUP,0.2129032258064516,"After the Expansion step grows the search tree, the leaf nodes of the tree naturally contain the trailing
path information from the seed nodes, which is past-oriented. To provide future context, we employ"
BACKUP,0.21935483870967742,"the Backup step to aggregate contextual information from subtrees rooted at the nodes reached by
previous NuTrea layers. We denote a subtree of depth ùêærooted at node ùë£as T ùêæ
ùë£
= (Vùë£, Eùë£) ‚äÇ
Gùëû= (Vùëû, Eùëû). Here, Vùë£= {ùë¢| SP(ùë¢, ùë£) ‚â§ùêæand ùë¢‚ààVùëû} and Eùë£= {(ùë¢1, ùë¢2) | (ùë¢1, ùë¢2) ‚àà
E and ùë¢1, ùë¢2 ‚ààVùë£}, where SP(ùë¢, ùë£) is a function that returns the length of the shortest path between
nodes ùë¢and ùë£. For the Backup step, we consider only the edge set Eùë£of T ùêæ
ùë£. The reason behind this
is that the edges (relation types) better represent the question context in guiding the search on the KG.
Also, using both the node and edge sets may introduce computational redundancy, as the initial node
features originate from the edge embeddings [14, 1] (See section 3.1)."
BACKUP,0.22580645161290322,"To pool the constraint information from Eùë£, we apply max-pooling conditioned on the question
content. Specifically, we take a similar measure as the Expansion step by computing ùëÄtypes of
messages conditioned on the backup instructions {q( ùëó)
bak}ùëÄ
ùëó=1."
BACKUP,0.23225806451612904,"c( ùëó)
ùë¢1ùë¢2 = ReLU(ùëæcrùë¢1ùë¢2 ‚äôq( ùëó)
bak),
(6)"
BACKUP,0.23870967741935484,"where ùëó‚àà[1, ùëÄ] and ùëæc ‚ààRùê∑√óùê∑. Then, we max-pool the messages as"
BACKUP,0.24516129032258063,"c( ùëó)
ùë£
= MAX-POOL({c( ùëó)
ùë¢1ùë¢2 | (ùë¢1, ùë¢2) ‚ààEùë£}),
(7)"
BACKUP,0.25161290322580643,"which represents the extent to which the local subtree context of ùë£is relevant to the conditions and
constraints in the question. Next, the information is aggregated with an MLP layer, and the node
embedding hùë£is updated as"
BACKUP,0.25806451612903225,"cùë£=
ùëÄ
ùëó=1 c( ùëó)
ùë£"
BACKUP,0.2645161290322581,"hùë£:= MLP(fùë£‚à•cùë£),
(8)"
BACKUP,0.2709677419354839,"where fùë£refers to the propagated embeddings originating from Eq. (5). This serves as a correction of
the original past-oriented message with respect to question constraints, providing the next NuTrea
layer with rich local context."
NODE RANKING,0.27741935483870966,"3.2.3
Node Ranking"
NODE RANKING,0.2838709677419355,"Finally, each node is scored and ranked based on the embeddings before and after the Backup
step. For node ùë£, fùë£from Eq. (5) and hùë£from Eq. (8) are projected to the expansion-score ùë†(ùëí)
ùë£
and
backup-score ùë†(ùëè)
ùë£, respectively, as"
NODE RANKING,0.2903225806451613,"ùë†(ùëí)
ùë£
= fùë£¬∑ ùëäùëí
ùë†(ùëè)
ùë£
= hùë£¬∑ ùëäùëè,
(9)"
NODE RANKING,0.2967741935483871,"where ùëäùëí, ùëäùëè‚ààRùê∑. The final node score is retrieved by adding the two scores. We use a context
coefficient ùúÜto control the effect of the Backup step, and apply softmax to normalize the scores as"
NODE RANKING,0.3032258064516129,"ùë†ùë£= Softmax([ùë†(ùëí)
ùë£
+ ùúÜ¬∑ ùë†(ùëè)
ùë£])ùë£‚ààVùëû,
(10)"
NODE RANKING,0.3096774193548387,"which is passed on to the next layer to be used for Eq. (5). Figure 1 provides a holistic view of our
method, and pseudocode is in the supplement."
NODE RANKING,0.3161290322580645,"Overall, the message passing scheme of NuTrea resembles the algorithm of Monte Carlo Tree
Search (MCTS): Selection ‚ÜíExpansion ‚ÜíSimulation ‚ÜíBackup. The difference is that our method
replaces the node ‚ÄòSelection‚Äô and ‚ÄòSimulation‚Äô steps with a soft GNN-based approach that rolls out
subtrees and updates the nodes at once, rather than applying Monte Carlo sampling methods."
NODE RANKING,0.3225806451612903,"3.3
RF-IEF Node Embedding (H)"
NODE RANKING,0.32903225806451614,"Another notable challenge with KGs is embedding nodes. Many KG entities (nodes) are proper nouns
that are not informative, and several KGQA datasets [16, 17] consist of encrypted entity names. Thus,
given no proper node features, the burden is on the model layers to learn meaningful node embeddings.
To alleviate this, we propose a novel node embedding method, Relation Frequency-Inverse Entity
Frequency (RF-IEF), which grounds on the local and global topological information of the KG nodes."
NODE RANKING,0.33548387096774196,"Term frequency-inverse document frequency (TF-IDF) is one effective feature that characterizes a
textual document [32‚Äì34]. The bag-of-words model computes the frequency of terms in a document
and penalizes frequent but noninformative words, e.g., ‚Äòa‚Äô, ‚Äòthe‚Äô, ‚Äòis‚Äô, ‚Äòare‚Äô, by the frequency of the"
NODE RANKING,0.3419354838709677,"Figure 2: Expressiveness of NuTrea layers. In the input KG subgraph (left-most figure), let node 1 and 2
be the seed and answer node, respectively, where information ùëëis critical in choosing node 2 as the answer.
The shaded nodes indicate the regions that are sequentially reached by consecutive GNN layers. Compared to
(a) previous methods [14, 1] that require 3 message passing steps for node 2 to access ùëë, our (b) NuTrea layer
requires only 1 step for node 2 to acquire information ùëëto determine it as the answer."
NODE RANKING,0.34838709677419355,"term across documents. Motivated by the idea, we represent a node on a KG as a bag of relations. An
entity node is characterized by the frequencies of rare (or informative) relations. Similar to TF-IDF,
we define two functions: Relation Frequency (RF) and Inverse Entity Frequency (IEF). The RF
function is defined for node ùë£‚ààVùëûand relation type ùëü‚ààR as"
NODE RANKING,0.3548387096774194,"RF(ùë£, ùëü) =
‚àëÔ∏Å"
NODE RANKING,0.36129032258064514,"ùëí‚ààI(ùë£)
1{ ùëì(ùëí) = ùëü},
(11)"
NODE RANKING,0.36774193548387096,"where I(ùë£) is the set of incident edges of node ùë£, 1 is an indicator function, and ùëì: E ‚ÜíR is a
function that retrieves the relation type of an edge. Then, the output of the RF function is a matrix
ùëÖùêπ‚ààR|Vùëû|√ó|R| that counts the occurrence of each relation type incident to each node in the KG
subgraph. We used raw counts for relation frequency (RF) to reflect the local degree information of a
node. On the other hand, the IEF function is defined as"
NODE RANKING,0.3741935483870968,"IEF(ùëü) = log
|Vùëû|
1 + EF(ùëü) ,
(12)"
NODE RANKING,0.38064516129032255,"where
EF(ùëü) =
‚àëÔ∏Å"
NODE RANKING,0.3870967741935484,"ùë£‚ààVùëû
1{‚àÉùëí‚ààI(ùë£) s.t. ùëì(ùëí) = ùëü}.
(13)"
NODE RANKING,0.3935483870967742,"EF counts the global frequency of nodes across KG subgraphs that have relation ùëüwithin its incident
edge set. With ùêºùê∏ùêπ‚ààR|R|, the RF-IEF matrix ùë≠‚ààR|Vùëû|√ó|R| is computed as"
NODE RANKING,0.4,"ùë≠= ùëÖùêπdiag(ùêºùê∏ùêπ),
(14)"
NODE RANKING,0.4064516129032258,where diag(ùêºùê∏ùêπ) ‚ààR|Vùëû|√ó|R| denotes a diagonal matrix constructed with the elements of ùêºùê∏ùêπ.
NODE RANKING,0.4129032258064516,"The RF-IEF matrix ùë≠captures both the local and global KG structure and it can be further enhanced
with the rich semantic information on the edges of KGs. Unlike entity nodes with uninformative
text, e.g., proper nouns and encrypted entity names, edges generally are accompanied by linguistic
descriptions (relation types). Hence, the relations are commonly embedded by a pre-trained language
model in KGQA. Combining with the relation embeddings ùëπ‚ààR|R|√óùê∑, our final RF-IEF node
embeddings ùëØis computed as
ùëØ= ùë≠ùëπùëæ‚Ñé,
(15)
where ùëØ‚ààR|Vùëû|√óùê∑, and ùëæ‚Ñé‚ààRùê∑√óùê∑. The RF-IEF node embeddings can be viewed as the
aggregated semantics of relations, which are represented by a language model, based on graph
topology as well as the informativeness (or rareness) of relations. A row in ùëØis a node embedding
vector hùë£, which is used in Eq. (5) at the first NuTrea layer."
DISCUSSION,0.41935483870967744,"3.4
Discussion"
DISCUSSION,0.4258064516129032,"Expressiveness of the NuTrea layer.
While many previous multi-hop KGQA models simultane-
ously update all node embeddings, recent works [14, 2, 1] have shown the superiority of approaches
that search paths on the KG. These methods gradually expand the searching area by sequentially"
DISCUSSION,0.432258064516129,"updating nodes closer to the seed node towards the answer nodes. Our model builds on the latter
sequential search scheme as well, enhancing expressiveness with our proposed NuTrea layers."
DISCUSSION,0.43870967741935485,"With a simple toy example in Figure 2, we compare the message flow of previous sequential search
models and our NuTrea. The example demonstrates that our NuTrea layer (b) can probe subtrees to
quickly gather fringe node (i.e., node 4 or 5) information without exhaustively visiting them. This is
accomplished by our Backup step, which boosts the original past-oriented node embeddings with
future-oriented subtree information."
EXPERIMENTS,0.44516129032258067,"4
Experiments"
EXPERIMENTAL SETTINGS,0.45161290322580644,"4.1
Experimental Settings
Datasets.
We experiment on three large-scale multi-hop KGQA datasets: MetaQA [35], WebQues-
tionsSP (WQP) [16] and ComplexWebQuestions (CWQ) [17]. Meta-QA consists of three different
splits, 1-hop, 2-hop, and 3-hop, each indicating the number of hops required to reach the answer node
from the seed node. The questions are relatively easy with less constraints. WQP and CWQ, on the
other hand, contain more complex questions with diverse constraints. WQP is relatively easier, as
CWQ is derived from WQP by extending its questions with additional constraints. MetaQA is answer-
able with the WikiMovies knowledge base [27], while WQP and CWQ require the Freebase KG [15]
to answer questions. Further dataset information and statistics are provided in the supplement."
EXPERIMENTAL SETTINGS,0.45806451612903226,"Baselines.
We mainly compare with previous multi-hop KGQA methods that take the Information
Retrieval approach (section 2). These models, unlike Semantic Parsing approaches, do not access the
ground truth logical queries and focus on processing the KG subgraph to rank the nodes to identify
answer nodes. To introduce the three most recent baseline models: (1) SQALER [3] proposes a
scalable KGQA method whose complexity scales linearly with the number of relation types, rather
than nodes or edges. (2) TERP [2] introduces the rotate-and-scale entity link prediction framework to
integrate textual and KG structural information. (3) ReaRev [1] adaptively selects the next reasoning
step with a variant of breadth-first search (BFS). Other baselines are introduced in the supplement."
EXPERIMENTAL SETTINGS,0.4645161290322581,"Implementation Details.
For WQP, 2 NuTrea layers with subtree depth ùêæ= 1 is used, while CWQ
with more complex questions uses 3 layers with depth ùêæ= 2. In the case of RF-IEF node embedding,
we pre-compute the Entity Frequency (EF) values in Eq. (13) for subgraphs in the training set before
training. We use the same EF values throughout training, validation, and testing. This stabilizes
computation by mitigating the large variance induced by relatively small batch sizes. For MetaQA,
the number of NuTrea layers are selected from {2, 3}, and ùêæfor ego-graph pooling from {1,2}. See
the supplement for further hyperparameter settings and details."
MAIN EXPERIMENTS,0.47096774193548385,"4.2
Main Experiments"
MAIN EXPERIMENTS,0.4774193548387097,"Here, we present the experimental results of NuTrea. Following the common evaluation practice of
previous works, we test the model that achieved the best performance on the validation set. In the
WQP dataset experiments in Table 1, we achieved the best performance of 77.4 H@1 among strong
KGQA baselines that take an information retrieval approach, as discussed in Section 2. Compared to
the previous best, this is a large improvement of 0.6 points. In terms of the F1 score, which evaluates
the answer set prediction, our method achieved a score of 72.7, exceeding the previously recorded
value by a large margin of 1.8 points. In addition, we also improved the previous state-of-the-art
performance on the CWQ dataset by achieving an 53.6 H@1, which is an improvement of 0.7 points."
MAIN EXPERIMENTS,0.4838709677419355,"We also experimented NuTrea on MetaQA to see if it performs reasonably well with easy questions
as well. On three data splits, NuTrea achieved comparable performance with previous state-of-the-art
methods for simple question answering. Evaluating with the average H@1 score of the three splits,
NuTrea performs second best among all baseline models."
INCOMPLETE KG EXPERIMENTS,0.49032258064516127,"4.3
Incomplete KG Experiments"
INCOMPLETE KG EXPERIMENTS,0.4967741935483871,"The KG is often human-made and the contents are prone to being incomplete. Hence, it is a norm to
test a model‚Äôs robustness on incomplete KG settings where a certain portion of KG triplets, i.e., a
tuple of ‚ü®head, relation, tail‚ü©, are dropped. This experiment evaluates the robustness of our
model to missing relations in a KG. We follow the experiment settings in [1], and use the identical
incomplete KG dataset which consists of WQP samples with [50%, 30%, 10%] of the original KG
triplets remaining. In Table 2, NuTrea performs the best in most cases, among the GNN models"
INCOMPLETE KG EXPERIMENTS,0.5032258064516129,"Table 1: Results on multi-hop KGQA datasets. The Hit@1 and F1 scores are reported. The baselines are
taken from the original papers. The best performances are in bold, and the second best are underlined."
INCOMPLETE KG EXPERIMENTS,0.5096774193548387,"Models
WQP
CWQ
MetaQA
H@1
F1
H@1
F1
1-hop
2-hop
3-hop
Avg. H@1"
INCOMPLETE KG EXPERIMENTS,0.5161290322580645,"KV-Mem [27]
46.7
38.6
21.1
-
95.8
25.1
10.1
43.7
GraftNet [28]
66.7
62.4
32.8
-
-
-
-
96.8
PullNet [29]
68.1
-
45.9
-
97.0
99.9
91.4
96.1
EmbedKGQA [11]
66.6
-
-
-
97.5
98.8
94.8
97.0
ReifiedKB [36]
52.7
-
-
-
96.2
81.1
72.3
83.2
EMQL [37]
75.5
-
-
-
97.2
98.6
99.1
98.3
TransferNet [13]
71.4
48.6
48.6
-
97.5
100.0
100.0
99.2
NSM(+p) [14]
73.9
66.2
48.3
44.0
97.3
99.9
98.9
98.7
NSM(+h) [14]
74.3
67.4
48.8
44.0
97.2
99.9
98.9
98.6
Rigel [38]
73.3
-
48.7
-
-
-
-
-
SQALER+GNN [3]
76.1
-
-
-
-
99.9
99.9
-
TERP [2]
76.8
-
49.2
-
97.5
99.4
98.9
98.6
ReaRev [1]
76.4
70.9
52.9
-
-
-
-
-"
INCOMPLETE KG EXPERIMENTS,0.5225806451612903,"NuTrea (Ours)
77.4
72.7
53.6
49.5
97.4
100.0
98.9
98.8"
INCOMPLETE KG EXPERIMENTS,0.5290322580645161,"Table 2: Incomplete KG experiments. NuTrea also
performs well in incomplete KG settings. The baseline
figures were taken from [1]."
INCOMPLETE KG EXPERIMENTS,0.535483870967742,"Portion of KG triplets (%)
50%
30%
10%
H@1 F1 H@1 F1 H@1 F1"
INCOMPLETE KG EXPERIMENTS,0.5419354838709678,"Graftnet [28]
47.7 34.3 34.9 20.4 15.5 6.5
SGReader [39]
49.2 33.5 35.9 20.2 17.1 7.0
HGCN [40]
49.3 34.3 35.2 21.0 18.3 7.9
ReaRev [1]
53.4 39.9 37.9 23.6 19.4 8.6"
INCOMPLETE KG EXPERIMENTS,0.5483870967741935,"NuTrea (Ours)
53.7 40.1 38.3 24.1 18.9 8.7"
INCOMPLETE KG EXPERIMENTS,0.5548387096774193,"Table 3: Component ablation experiments. The
ablation experiments were done on the WQP dataset.
Two main contributions are studied."
INCOMPLETE KG EXPERIMENTS,0.5612903225806452,"RF-IEF
Node Emb.
Backup
step
WQP H@1
WQP F1"
INCOMPLETE KG EXPERIMENTS,0.567741935483871,"‚úì
‚úì
77.4 (‚Äì0.0)
72.7 (‚Äì0.0)
‚úì
74.8 (‚Äì2.6)
70.4 (‚Äì2.3)
‚úì
76.8 (‚Äì0.6)
71.5 (‚Äì1.2)
73.4 (‚Äì4.0)
70.9 (‚Äì1.8)"
INCOMPLETE KG EXPERIMENTS,0.5741935483870968,"designed to handle incomplete KGs. We believe that our model adaptively learns multiple alternative
reasoning processes and can plan for future moves beforehand via our Backup step, so that it provides
robust performance with noisy KGs."
ANALYSIS,0.5806451612903226,"5
Analysis"
ANALYSIS,0.5870967741935483,"In this section, we provide comprehensive analyses on the contributions of NuTrea to ensure its
effectiveness in KGQA. We try to answer the following research questions: Q1. How does each
component contribute to the performance of NuTrea? Q2. What is the advantage of NuTrea‚Äôs tree
search algorithm over previous methods? Q3. What are the effects of the RF-IEF node embeddings?"
ABLATION STUDY,0.5935483870967742,"5.1
Ablation Study"
ABLATION STUDY,0.6,"Here, we evaluate the effectiveness of each component in NuTrea to answer Q1. An ablation study is
performed on our two major contributions: the Backup step in our NuTrea layers and the RF-IEF
node embeddings. By removing the RF-IEF node embeddings, we instead apply the common node
initialization method used in [14, 1], which simply averages the relation embeddings incident to each
node. Another option is to use zero embeddings, but we found it worse than the simple averaging
method. For NuTrea layer ablation, we remove the Backup step which plays a key role in aggregating
the future-oriented subtree information onto the KG nodes. Then, only the expansion-score (ùë†(ùëí)
ùë£)
is computed, and the backup-score (ùë†(ùëè)
ùë£) is always 0. Also, the embedding fùë£(Eq. (5)), is directly
output from the NuTrea layer and no further updates are made via the Backup step."
ABLATION STUDY,0.6064516129032258,"In Table 3, we can see that the largest performance drop is observed when the Backup step is removed.
Without it, the model has limited access to the broader context of the KG and cannot reflect the
complex question constraints in node searching. Further discussion on this property of NuTrea‚Äôs
message passing is provided in the next Section 5.2. Also, we observed a non-trivial 0.6 point drop"
ABLATION STUDY,0.6129032258064516,"Figure 3: Qualitative examples (without
Backup ‚áíwith Backup) (section 5.2.2).
Figure 4: Weights computed by RF-IEF (section 5.3)."
ABLATION STUDY,0.6193548387096774,"in H@1 by removing the RF-IEF initialization method. By ablating both components, there was a
significant degradation of 4.0 H@1 points."
ADVANTAGES OF NUTREA,0.6258064516129033,"5.2
Advantages of NuTrea"
ADVANTAGES OF NUTREA,0.632258064516129,"To answer Q2, we highlight the key advantages of NuTrea over recent approaches. We analyze the
efficiency of NuTrea, and provide qualitative results."
EFFICIENCY OF NUTREA,0.6387096774193548,"5.2.1
Efficiency of NuTrea"
EFFICIENCY OF NUTREA,0.6451612903225806,Table 4: Latency of ReaRev and NuTrea.
EFFICIENCY OF NUTREA,0.6516129032258065,"Models
NuTrea (Ours)
ReaRev"
EFFICIENCY OF NUTREA,0.6580645161290323,"Training Latency (per epoch)
100.2 s
78.0 s
Inference Latency (per sample)
67.7 ms
51.3 ms
Training GPU Hours
2.9 H
4.3 H"
EFFICIENCY OF NUTREA,0.6645161290322581,"In addition, to further verify the utility of
our model, we provide analyses on the
latency of NuTrea with WQP in Table 4.
Compared to the most recent ReaRev [1]
model, training and inference latency per
epoch/sample is slightly bigger, due to
our additional Backup module. However,
thanks to our expressive message passing scheme, NuTrea converges way faster, allowing the training
GPU hours to be reduced from 4.3 hours to 2.9 hours."
EFFICIENCY OF NUTREA,0.6709677419354839,Figure 5: Effect of number of layers.
EFFICIENCY OF NUTREA,0.6774193548387096,"To provide more insight in terms of number
of NuTrea layers, we also reveal its effect on
model performance, in Figure 5. The figure re-
ports the F1 scores for both with and without
the Backup module, evaluated on the WebQues-
tionsSP dataset. ‚ÄúNuTrea without Backup"" has
an equivalent model configuration used in the
Backup ablation experiment of Table 3. Overall,
the performance of ‚ÄúNuTrea without Backup""
does generally improve with additional layers,
but ‚ÄúNuTrea with Backup"" reached the highest
score of 72.7 with only 2 NuTrea layers. This
is enabled by our Backup module, which allevi-
ates the burden of exhaustively searching deeper
nodes. This is computationally more efficient than stacking multiple layers to achieve higher perfor-
mance. Specifically, by comparing the 2-layer ‚ÄúNuTrea with Backup"" and 5-layer ‚ÄúNuTrea without
Backup"", the former required an average of 73.8 ms of inference time per question, whereas the
latter required 108.6 ms. With only 68% of compute, our NuTrea achieved comparable performance
with the deeper ‚ÄúNuTrea without Backup"". Note, these latency values were evaluated on a different
environment from values reported in Table 4."
QUALITATIVE RESULTS,0.6838709677419355,"5.2.2
Qualitative Results
In Figure 3, we demonstrate several qualitative examples from our error analysis. In each question,
the blue (node) entity is the correct answer choice, while the red is the wrong choice made by the"
QUALITATIVE RESULTS,0.6903225806451613,"model without the Backup step. The values in parentheses demonstrate the difference in node scores
between models without and with Backup. Without it, the sequential search model cannot refer to
local contexts of a node and frequently predicts an extremely low score for the correct answer node
(e.g., 0.0062 for ‚ÄúQueen Elizabeth the Queen Mother"" in the first question of Figure 3). Such a
problem is mitigated by our NuTrea‚Äôs Backup step, which tends to boost scores of correct answers and
tone down wrong choices that were wrongly assigned with a high score. More qualitative examples
are provided in the supplement, along with an analysis on the different importances of the Backup
step in different datasets, by controlling the context coefficient ùúÜ."
EFFECT OF RF-IEF,0.6967741935483871,"5.3
Effect of RF-IEF"
EFFECT OF RF-IEF,0.7032258064516129,"The RF-IEF node embedding is a simple method inspired by an effective text representation technique
in natural language processing. Here, we disclose the specific effect of RF-IEF on the relation
embedding aggregation weights, thereby answering Q3. In Figure 4 (left), the log-scaled ùê∏ùêπ(Eq. 13)
value of each relation type is sorted. The globally most frequent relation types, including ‚Äúself_loop‚Äùs,
are too general to provide much context in characterizing a KG node. Our RF-IEF suppresses such
uninformative relation types for node embedding initialization, resulting in a weight distribution like
Figure 4 (right). The pie charts display two examples on the difference between aggregation weights
for a node entity before and after RF-IEF is applied. To illustrate, the weight after RF-IEF corresponds
to a row of ùë≠in Eq. (14), while the weight before RF-IEF would be uniform across incident edges.
As ‚ÄúAlexander Bustamante‚Äù is a politician, the relation types ‚Äúorgainizations founded‚Äù and ‚Äúfounders‚Äù
become more lucid via RF-IEF, while relations like ‚Äúevents_competed_in‚Äù and ‚Äúcompetitors‚Äù are
emphasized for an athlete like ‚ÄúKemar Bailey-Cole‚Äù. Likewise, RF-IEF tends to scale up characteristic
relation types in initializing the node features, thereby enhancing differentiability between entities. To
further demonstrate RF-IEF‚Äôs general applicability, we also provide a plug-in experiment on another
baseline model in the supplement."
CONCLUSION,0.7096774193548387,"6
Conclusion"
CONCLUSION,0.7161290322580646,"Neural Tree Search (NuTrea) is an effective GNN model for multi-hop KGQA, which aims to
better capture the complex question constraints by referring to the broader KG context. The high
expressiveness of NuTrea is attained via our message passing scheme that resembles the MCTS
algorithm, which leverages the future-oriented subtree information conditioning on the question
constraints. Moreover, we introduce the RF-IEF node embedding technique to also consider the
global KG context. Combining these methods, our NuTrea achieves the state-of-the-art in two major
multi-hop KGQA benchmarks, WebQuestionsSP and ComplexWebQuestions. Further analyses on
KG incompleteness and the qualitative results support the effectiveness of NuTrea. Overall, NuTrea
reveals the importance of considering the broader KG context in harnessing the knowledge graph via
human languages."
CONCLUSION,0.7225806451612903,Acknowledgments and Disclosure of Funding
CONCLUSION,0.7290322580645161,"This work was partly supported by ICT Creative Consilience program (IITP-2023-2020-0-01819)
supervised by the IITP, the National Research Foundation of Korea (NRF) grant funded by the Korea
government (MSIT) (NRF-2023R1A2C2005373), and KakaoBrain corporation."
REFERENCES,0.7354838709677419,References
REFERENCES,0.7419354838709677,"[1] Costas Mavromatis and George Karypis.
Rearev: Adaptive reasoning for question answering over
knowledge graphs. In EMNLP-Findings, 2022."
REFERENCES,0.7483870967741936,"[2] Zile Qiao, Wei Ye, Tong Zhang, Tong Mo, Weiping Li, and Shikun Zhang. Exploiting hybrid semantics of
relation paths for multi-hop question answering over knowledge graphs. In COLING, 2022."
REFERENCES,0.7548387096774194,"[3] Mattia Atzeni, Jasmina Bogojeska, and Andreas Loukas. Sqaler: Scaling question answering by decoupling
multi-hop and logical reasoning. NeurIPS, 2021."
REFERENCES,0.7612903225806451,"[4] Jinyoung Park, Hyeong Kyu Choi, Juyeon Ko, Hyeonjin Park, Ji-Hoon Kim, Jisu Jeong, Kyungmin Kim,
and Hyunwoo Kim. Relation-aware language-graph transformer for question answering. In AAAI, 2023."
REFERENCES,0.7677419354838709,"[5] Kuan Wang, Yuyu Zhang, Diyi Yang, Le Song, and Tao Qin. Gnn is a counter? revisiting gnn for question
answering. In ICLR, 2021."
REFERENCES,0.7741935483870968,"[6] X Zhang, A Bosselut, M Yasunaga, H Ren, P Liang, C Manning, and J Leskovec. Greaselm: Graph
reasoning enhanced language models for question answering. In ICLR, 2022."
REFERENCES,0.7806451612903226,"[7] Abdalghani Abujabal, Mohamed Yahya, Mirek Riedewald, and Gerhard Weikum. Automated template
generation for question answering over knowledge graphs. In WWW, 2017."
REFERENCES,0.7870967741935484,"[8] Sen Hu, Lei Zou, and Xinbo Zhang. A state-transition framework to answer complex questions over
knowledge base. In EMNLP, 2018."
REFERENCES,0.7935483870967742,"[9] Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros
Polymenakos, and Andrew McCallum. Case-based reasoning for natural language queries over knowledge
bases. In EMNLP, 2021."
REFERENCES,0.8,"[10] Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Yiqun Hu, William
Wang, Zhiguo Wang, and Bing Xiang. Decaf: Joint decoding of answers and logical forms for question
answering over knowledge bases. arXiv preprint arXiv:2210.00063, 2022."
REFERENCES,0.8064516129032258,"[11] Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. Improving multi-hop question answering over
knowledge graphs using knowledge base embeddings. In ACL, 2020."
REFERENCES,0.8129032258064516,"[12] Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure
Leskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop question answering
on knowledge graphs. In ICML, 2021."
REFERENCES,0.8193548387096774,"[13] Jiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, and Hanwang Zhang. Transfernet: An effective and transparent
framework for multi-hop question answering over relation graph. In EMNLP, 2021."
REFERENCES,0.8258064516129032,"[14] Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. Improving multi-hop knowledge
base question answering by learning intermediate supervision signals. In WSDM, 2021."
REFERENCES,0.832258064516129,"[15] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively
created graph database for structuring human knowledge. In ACM SIGMOD, 2008."
REFERENCES,0.8387096774193549,"[16] Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. The value of
semantic parse labeling for knowledge base question answering. In ACL, 2016."
REFERENCES,0.8451612903225807,"[17] Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions. In
NAACL, 2018."
REFERENCES,0.8516129032258064,"[18] Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational reasoning for
question answering with knowledge graph. In AAAI, 2018."
REFERENCES,0.8580645161290322,"[19] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V Chawla. Heterogeneous
graph neural network. In ACM SIGKDD, 2019."
REFERENCES,0.864516129032258,"[20] Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye, and S Yu Philip. A survey on heterogeneous
graph embedding: methods, techniques, applications and sources. IEEE Transactions on Big Data, 2022."
REFERENCES,0.8709677419354839,"[21] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general
knowledge. In AAAI, 2017."
REFERENCES,0.8774193548387097,"[22] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease
does this patient have? a large-scale open domain question answering dataset from medical exams. Applied
Sciences, 11(14):6421, 2021."
REFERENCES,0.8838709677419355,"[23] Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. A survey on complex
knowledge base question answering: Methods, challenges and solutions. In IJCAI, 2021."
REFERENCES,0.8903225806451613,"[24] Yongrui Chen, Huiying Li, Yuncheng Hua, and Guilin Qi. Formal query building with query structure
prediction for complex question answering over knowledge base. In IJCAI, 2020."
REFERENCES,0.896774193548387,"[25] Dung Thai, Srinivas Ravishankar, Ibrahim Abdelaziz, Mudit Chaudhary, Nandana Mihindukulasooriya,
Tahira Naseem, Rajarshi Das, Pavan Kapanipathi, Achille Fokoue, and Andrew McCallum. Cbr-ikb: A
case-based reasoning approach for question answering over incomplete knowledge bases. arXiv preprint
arXiv:2204.08554, 2022."
REFERENCES,0.9032258064516129,"[26] Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Manzil Zaheer, Hannaneh Hajishirzi, Robin Jia,
and Andrew McCallum. Knowledge base question answering by case-based reasoning over subgraphs. In
ICML. PMLR, 2022."
REFERENCES,0.9096774193548387,"[27] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston.
Key-value memory networks for directly reading documents. In EMNLP, 2016."
REFERENCES,0.9161290322580645,"[28] Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William
Cohen. Open domain question answering using early fusion of knowledge bases and text. In EMNLP,
2018."
REFERENCES,0.9225806451612903,"[29] Haitian Sun, Tania Bedrax-Weiss, and William Cohen. Pullnet: Open domain question answering with
iterative retrieval on knowledge bases and text. In EMNLP-IJCNLP, 2019."
REFERENCES,0.9290322580645162,"[30] Lihui Liu, Boxin Du, Jiejun Xu, Yinglong Xia, and Hanghang Tong. Joint knowledge graph completion
and question answering. In ACM SIGKDD, 2022."
REFERENCES,0.9354838709677419,"[31] Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. Semantic parsing via staged query
graph generation: Question answering with knowledge base. In Proceedings of the Joint Conference of
the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language
Processing of the AFNLP, 2015."
REFERENCES,0.9419354838709677,"[32] Hans Peter Luhn. The automatic creation of literature abstracts. IBM Journal of research and development,
2(2):159‚Äì165, 1958."
REFERENCES,0.9483870967741935,"[33] Stephen E Robertson and K Sparck Jones. Relevance weighting of search terms. Journal of the American
Society for Information science, 27(3):129‚Äì146, 1976."
REFERENCES,0.9548387096774194,"[34] Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information
processing & management, 24(5):513‚Äì523, 1988."
REFERENCES,0.9612903225806452,"[35] Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational reasoning for
question answering with knowledge graph. In AAAI, 2018."
REFERENCES,0.967741935483871,"[36] William W Cohen, Haitian Sun, R Alex Hofer, and Matthew Siegler. Scalable neural methods for reasoning
with a symbolic knowledge base. In ICLR, 2020."
REFERENCES,0.9741935483870968,"[37] Haitian Sun, Andrew Arnold, Tania Bedrax Weiss, Fernando Pereira, and William W Cohen. Faithful
embeddings for knowledge base queries. NeurIPS, 2020."
REFERENCES,0.9806451612903225,"[38] Priyanka Sen, Armin Oliya, and Amir Saffari. Expanding end-to-end question answering on differentiable
knowledge graphs with intersection. In EMNLP, 2021."
REFERENCES,0.9870967741935484,"[39] Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. Improving question
answering over incomplete kbs with knowledge-aware reader. ACL, 2019."
REFERENCES,0.9935483870967742,"[40] Jiale Han, Bo Cheng, and Xu Wang. Open domain question answering based on text enhanced knowledge
graph with hyperedge infusion. In EMNLP-Findings, 2020."
