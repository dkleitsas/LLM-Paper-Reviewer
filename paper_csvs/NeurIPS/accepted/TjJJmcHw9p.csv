Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001876172607879925,"Network clustering tackles the problem of identifying sets of nodes (communities)
that have similar connection patterns. However, in many scenarios, nodes also
have attributes that are correlated with the clustering structure. Thus, network
information (edges) and node information (attributes) can be jointly leveraged
to design high-performance clustering algorithms. Under a general model for
the network and node attributes, this work establishes an information-theoretic
criterion for the exact recovery of community labels and characterizes a phase
transition determined by the Chernoff-Hellinger divergence of the model. The
criterion shows how network and attribute information can be exchanged in order to
have exact recovery (e.g., more reliable network information requires less reliable
attribute information). This work also presents an iterative clustering algorithm that
maximizes the joint likelihood, assuming that the probability distribution of network
interactions and node attributes belong to exponential families. This covers a broad
range of possible interactions (e.g., edges with weights) and attributes (e.g., non-
Gaussian models), as well as sparse networks, while also exploring the connection
between exponential families and Bregman divergences. Extensive numerical
experiments using synthetic data indicate that the proposed algorithm outperforms
classic algorithms that leverage only network or only attribute information as well
as state-of-the-art algorithms that also leverage both sources of information. The
contributions of this work provide insights into the fundamental limits and practical
techniques for inferring community labels on node-attributed networks."
INTRODUCTION,0.00375234521575985,"1
Introduction"
INTRODUCTION,0.005628517823639775,"Community detection or network clustering‚Äìthe task of identifying sets of similar nodes in a network‚Äì
is a fundamental problem in network analysis [4, 1, 18], with applications in diverse fields such
as digital humanities, data science and biology. In the classic formulation, a set of communities
must be determined from the connection patterns among the nodes of a single network. A simple
random graph model with community structure, the Stochastic Block Model (SBM), has been the
canonical model to characterise theoretical limitations and evaluate different community detection
algorithms [1]."
INTRODUCTION,0.0075046904315197,"However, nodes of many real-world networks have attributes or features that can reveal their identity
as an individual or within a group. For example, the age, gender and ethnicity of individuals in a
social network [27], the title, keywords and co-authors of papers in a citation network [32], or the
longitude and latitude of meteorological stations in weather forecast networks [8]. In some scenarios,
such attributes can be leveraged alone to identify node communities (clusters) without even using the
network."
INTRODUCTION,0.009380863039399626,"Thus, a modern formulation for community detection must consider network information (edges) and
node information (attributes). Indeed, recent works have designed community detection algorithms
that can effectively leverage both sources of information to improve performance. Various methods
have been proposed for clustering node-attributed networks, including modularity optimisation [14],
belief propagation [15], expected-maximisation algorithms [24, 34], information flow compres-
sion [33], semidefinite programming [37], spectral algorithms [2, 7, 25] and iterative likelihood based
methods [8]."
INTRODUCTION,0.01125703564727955,"A fundamental problem in this new formulation is fusing both sources of information: how important
is network information in comparison to node information given a problem instance? Intuitively,
this depends on the noise associated with network edges and node attributes. For example, if edges
are reliable then the clustering algorithm should prioritize them when determining the communities.
However, most prior approaches adopt some form of heuristic when merging the two sources of
information [14, 16, 37]. A rigorous approach to this problem requires a mathematical model, and
one has been recently proposed."
INTRODUCTION,0.013133208255159476,"The Contextual Stochastic Block Model (CSBM) is a generalization of the SBM where each node has
a random attribute that depends on its community label. While the model formulation is general (in
terms of distribution for edges and attributes), CSBM has only been rigorously studied in the restrictive
setting where the pairwise interactions are binary (edges are present or not) and the node attributes are
Gaussian [2, 8, 15]. In this scenario, the phase transitions for exactly recovering the community labels
and for detecting them better than a random guess has been established. Moreover, the comparison
with the respective phase transitions in SBM [1] and in Gaussian mixture model [11, 26] (with
no network) highlight the value of jointly leveraging network and node information in recovering
community labels."
INTRODUCTION,0.0150093808630394,"However, real networks often depart from binary edges and Gaussian attributes. Indeed, in many
scenarios network edges have weights that reveal information about that interaction and nodes have
discrete or non-Gaussian attributes. This work tackles this scenario by considering a CSBM where
edges have weights and nodes have attributes that follow some arbitrary distributions. Under this
general model, this work is the first to characterise the phase transition for the exact recovery of
community labels. In particular, the Chernoff-Hellinger divergence, initially defined just for binary
networks [3], is extended to this more general model. This divergence effectively captures the
difficulty of distinguishing different communities and thus plays a crucial role in determining the
limits of exact recovery. The analysis reveals an additional term in the divergence that quantifies the
information provided by the attributes of the nodes. Moreover, it quantifies the trade-off between
network and node information in meeting the threshold for exact recovery."
INTRODUCTION,0.016885553470919325,"The CSBM generates weighted networks that are complete (all possible edges are present) when
edge weights follow a continuous distribution. However, most weighted real networks are sparse.
To model sparse weighted networks and to provide a practical community detection algorithm,
we consider a CSBM whose weights belong to zero-inflated distributions. More precisely, we
suppose that conditioned on observing an edge, the distribution of the weight of this edge belongs
to an exponential family. Similarly, the node attribute distributions are also assumed to belong
to an exponential family. Working with exponential families is motivated by two factors. Firstly,
exponential families encompass a broad range of parametric distributions, including the commonly
used Bernoulli, Poisson, Gaussian, or Gamma distributions. Secondly, there exists an intricate
connection between exponential families of distributions and Bregman divergences, which has proven
to be a powerful tool for developing algorithms across a variety of problems such as clustering,
classification, and dimensionality reduction [6, 13]."
INTRODUCTION,0.01876172607879925,"This connection between Bregman divergences and exponential families has been previously explored
in the context of clustering dense networks (all possible edges are present) [23]. In contrast, this work
proposes an iterative algorithm that maximizes the log-likelihood of the model, for both dense and
sparse networks. This is a key difference with many previous works which either study only dense"
INTRODUCTION,0.020637898686679174,"weighted networks [9, 24] or binary networks with Gaussian attributes [2, 15, 34]. Simulations on
synthetic networks demonstrate that our algorithm outperforms state-of-the-art approaches in various
settings, providing practical techniques for achieving accurate clustering results."
INTRODUCTION,0.0225140712945591,"The article is structured as follows. The relevant related work is discussed in Section 2. Section 3
introduces the model under consideration along with the main theoretical contributions on exact
recovery. Section 4 focuses on sparse networks with edge weights and node attributes drawn from
exponential families and introduces an iterative algorithm for clustering such networks. Numerical
results and comparisons to prior works are presented in Section 5, and Section 6 concludes the paper."
INTRODUCTION,0.024390243902439025,"Notations
Let Ber(p) denote a Bernoulli random variable (r.v.) with parameter p, Nor(m, œÉ) a
Gaussian r.v. with mean m and standard deviation œÉ, and Exp(Œª) an exponential r.v. with mean Œª‚àí1.
The notation [K] refers to the set {1, ¬∑ ¬∑ ¬∑ , K}, while Ai¬∑ stands for the i-th row of matrix A."
RELATED WORK,0.02626641651031895,"2
Related work"
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.028142589118198873,"2.1
Exact recovery in SBM with edge weights and node attributes"
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.0300187617260788,"Community detection in classic SBM (binary edges) is a well-understood problem with strong
theoretical results concerning exact recovery and efficient algorithms with guaranteed accuracy
[1, 39]. However, extending the classic SBM to weighted networks (non-binary edges) with arbitrary
distributions is an ongoing research area. Most existing work in this scenario has been restricted to
the homogeneous model1, where edge weights within and across communities are determined by
two respective distributions. Moreover, existing works often restrict to categorical or real-valued
weights [22, 36], or to multiplex networks (multiple edge types) with independent and identically
distributed layers [29]. However, a recent work has provided a strong theoretical foundation the
homogeneous model with arbitrary distributions [5], highlighting the role of the R√©nyi divergence as
the key information-theoretic quantity for the homogeneous model."
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.03189493433395872,"In non-homogeneous models, a more complex divergence called the Chernoff-Hellinger divergence
is the appropriate information-theoretic quantity for exact community recovery [3]. However, the
expression of the Chernoff-Hellinger divergence as originally defined in [3] for binary networks
does not have an intuitive interpretation, and its extension to non-binary (weighted) networks is
challenging. For example, the exact recovery threshold for non-homogeneous SBM whose edges are
categorical random variables has been established [38], but this threshold is expressed as a condition
involving the minimization of a mixture of Kullback-Leibler divergences over the space of probability
distributions. Although the relationship between Kullback-Leibler and Chernoff divergences are
known (see for example [35, Theorem 30-32]), the specific technical lemma required to link them to
the Chernoff-Hellinger divergence is not straightforward (see [38, Claim 4])."
EXACT RECOVERY IN SBM WITH EDGE WEIGHTS AND NODE ATTRIBUTES,0.03377110694183865,"Another generalization of the SBM allows for nodes to have attributes that provide information
about their community, such as the Contextual SBM (CSBM) [15]. The CSBM has only been
rigorously studied in the setting where edges are binary and node attributes follow a Gaussian
distribution. In this scenario, the phase transition for exact recovery for the community labels has
been established [2, 8, 15]. A natural generalization is to investigate the model where network edges
have weights and nodes have attributes that follow arbitrary distributions. Indeed, this is one of the
main contributions of this work: Expression (3.4) gives a straightforward yet crucial formula for the
phase transition for exact recovery, also providing a natural interpretation for the influence of both
the network and node attributes. Moreover, Expression (3.4) also applies when no node attribute is
available, thus providing the exact recovery threshold for a non-homogeneous model and arbitrary
edge weight distribution, a significant advancement in the state of the art."
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.03564727954971857,"2.2
Algorithms for clustering weighted networks with node attributes"
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.0375234521575985,"Algorithms leveraging different approaches have been proposed to tackle community detection in
networks with edge weights and node attributes. A common principled approach is to determine the
community assignment that maximizes the likelihood function of a model for the data. However,
optimizing the likelihood function is computationally intractable even for binary networks. Thus,"
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.039399624765478425,1Also known as the planted partition model.
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.04127579737335835,"approximation schemes such as variational inference and pseudo-likelihood methods are often
adopted. For instance, [24] introduced a variational-EM algorithm for clustering non-homogeneous
weighted SBM with arbitrary distributions. Another approach for clustering node-attributed SBM
whose edge weights and attribute distributions belong to exponential families is [23]. These two
approaches assume that the network is dense (all edges are present and have non-zero edge weight).
However, most real networks are very sparse (most node pairs do not have an edge) and this work
focuses on this scenario. Another very recent work tackling sparse networks is the IR_sLs algorithm
from [8], although its theoretical guarantees assume binary networks with Gaussian attributes."
ALGORITHMS FOR CLUSTERING WEIGHTED NETWORKS WITH NODE ATTRIBUTES,0.043151969981238276,"The iterative clustering algorithm presented in this work maximizes the pseudo-likelihood likelihood
by assuming that the probability distribution of network edges and node attributes belong to exponen-
tial families. This yields a direct connection with Bregman divergences and establishes an elegant
expression for the likelihood function. This connection has also been leveraged in [23], however,
their model is restricted to dense weighted networks (all edges are present). This work (more specifi-
cally, Lemma 2), demonstrates that this connection can also be applied to sparse weighted networks
(using zero-inflated distributions to model the weights). This extension enhances the applicability
of pseudo-likelihood algorithms using Bregman divergence to a broader class of scenarios, namely
weighted sparse networks with node attributes."
MODEL AND EXACT RECOVERY IN NODE-ATTRIBUTED SBM,0.0450281425891182,"3
Model and exact recovery in node-attributed SBM"
MODEL DEFINITION,0.04690431519699812,"3.1
Model definition"
MODEL DEFINITION,0.04878048780487805,"Consider a population of n objects, called nodes, partitioned into K ‚â•2 disjoint sets, called blocks
or communities. A node-labelling vector z = (z1, ¬∑ ¬∑ ¬∑ , zn) ‚àà[K]n represents this partitioning so
that zi indicates the block of node i. The labels (blocks) of nodes are random variables assumed to
be independent and identically distributed such that P(zi = k) = œÄk for some vector œÄ ‚àà(0, 1)K
verifying that P"
MODEL DEFINITION,0.05065666041275797,"k œÄk = 1. The nodes interact in unordered pairs giving rise to undirected edges,
and X is the measurable space of all possible pairwise interactions. Additionally, each node has an
attribute that is an element of a measurable space Y. Let X ‚ààX N√óN denote the symmetric matrix
such that Xij represents the interaction between node pair (ij), and by Y = (Y1, ¬∑ ¬∑ ¬∑ , Yn) ‚ààYn the
node attribute vector."
MODEL DEFINITION,0.0525328330206379,"Assume that interactions and attributes are independent conditionally on the community labels of the
nodes. Let fk‚Ñì(x) denote the probability that two nodes in blocks k and ‚Ñìhave an interaction x ‚ààX,
and hk(y) denote the probability that a node in block k ‚àà[K] has an attribute y ‚ààY. Thus,"
MODEL DEFINITION,0.054409005628517824,"P (X, Y | z) =
Y"
MODEL DEFINITION,0.05628517823639775,"1‚â§i<j‚â§n
fzizj(Xij) n
Y"
MODEL DEFINITION,0.058161350844277676,"i=1
hzi(Yi).
(3.1)"
MODEL DEFINITION,0.0600375234521576,"In the following, the interaction spaces X, Y might depend on n, as well as the respective interaction
probabilities f, h. The number of nodes n will increase to infinity while K and œÄ are constant. For
an estimator ÀÜz ‚àà[K]n of z, we define the classification error as
loss(z, ÀÜz) = min
œÑ‚ààSK Ham(z, œÑ ‚ó¶ÀÜz),"
MODEL DEFINITION,0.06191369606003752,"where SK is the set of permutations of [K] and Ham(¬∑, ¬∑) is the hamming distance between two
vectors. An estimator ÀÜz = ÀÜz(X, Y ) achieves exact recovery if P (loss(z, ÀÜz) ‚â•1) = o(1)."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.06378986866791744,"3.2
Exact recovery threshold in node-attributed SBM"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.06566604127579738,"The difficulty of classifying empirical data in one of K possible classes is traditionally measured by
the Chernoff information [12]. More precisely, in the context of network clustering, let CH(a, b) =
CH(a, b, œÄ, f, h) denote the hardness of distinguishing nodes that belong to block a from block b.
This quantity is defined by
CH(a, b) =
sup
t‚àà(0,1)
CHt(a, b),
(3.2) where"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0675422138836773,"CHt(a, b) = (1 ‚àít) "" K
X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.06941838649155722,"c=1
œÄc Dt (fbc‚à•fac) + 1"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07129455909943715,n Dt (hb‚à•ha) # (3.3)
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07317073170731707,"is the Chernoff coefficient of order t across blocks a and b, and Dt(f‚à•g) =
1
t‚àí1 log
R
f t(x)g1‚àít(x)dx
is the R√©nyi divergence of order t between two probability densities f, g [35]. The key quantity
assessing the possibility or impossibility of exact recovery in SBM is then the minimal Chernoff
information across all pairs of clusters. We denote it by I = I(œÄ, f, h), and it is defined by
I =
min
a,b‚àà[K]
aÃ∏=b
CH(a, b).
(3.4)"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.075046904315197,"The following Theorem provides the information-theoretic threshold for exact recovery in node-
attributed SBM.
Theorem 1. Consider model (3.1) with œÄa > 0 for all a ‚àà[K]. Denote by a‚àó, b‚àóthe two hardest
blocks to estimate, that is CH(a‚àó, b‚àó) = I. Suppose that t ‚àà(0, 1) 7‚Üílim
n‚Üí‚àû
n
log nCHt(a‚àó, b‚àó) exists
and is strictly concave. Then the following holds:"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07692307692307693,"(i) exact recovery is information-theoretically impossible if lim
n‚Üí‚àû
n
log nI < 1;"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.07879924953095685,"(ii) exact recovery is information-theoretically possible if lim
n‚Üí‚àû
n
log nI > 1."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08067542213883677,"The proof for Theorem 1 is provided in the supplemental material. The main ingredient of the
proof is the asymptotic study of log-likelihood ratios. More precisely, let L(z) = P(X, Y | z). The
application of Chernoff bounds provides an upper bound on ‚àílog P (log L(z) ‚â•log L(z‚Ä≤)), where z
denotes the correct block structure and z‚Ä≤ ‚àà[K]n is another node-labelling vector (see Lemma 1
in the supplement). We can use this upper bond to prove that the maximum likelihood estimator
(MLE) achieves exact recovery if I > n‚àí1 log n. Reciprocally, if I < n‚àí1 log n, we show that whp
there exist some ""bad"" nodes i for which zi Ã∏= arg maxa‚àà[k] P (X, Y | z‚àíi, zi = a). In other words,
even in a setting where an oracle would reveal z‚àíi (i.e., the correct block assignment of all nodes
except node i), the MLE would fail at recovering zi. Establishing this fact requires to lower bound
‚àílog P (log L(z) ‚â•log L(z‚Ä≤)) where z‚Ä≤ ‚àà[K]n is such that Ham(z, z‚Ä≤) = 1 (i.e., z‚Ä≤ correctly labels
all nodes except one). This lower bound is obtained using large deviation results for general random
variables [10, 30]. To apply these results, the strict concavity of the limit n(log n)‚àí1I is needed. In
most practical settings, this assumption is verified, except in some edge cases (see Examples 1 and 2).
Let us now provide some examples of applications of Theorem 1.
Example 1 (Binary SBM with no attributes). Suppose that fab ‚àºBer(Œ±abn‚àí1 log n) where Œ±ab are
constants. A Taylor-expansion of the R√©nyi divergence between Bernoulli distributions leads to"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0825515947467167,I = (1 + o(1))log n
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08442776735459662,"n
min
aÃ∏=b sup
t‚àà(0,1) X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08630393996247655,"c
œÄc

tŒ±bc + (1 ‚àít)Œ±ac ‚àíŒ±t
bcŒ±1‚àít
ac

! ,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.08818011257035648,"which indeed coincides with the expression of the Chernoff-Hellinger divergence defined in [3]. We
also note that the limit n(log n)‚àí1I is strictly concave as long as the Œ±ab are not all equals2.
Example 2 (Binary SBM with Gaussian attributes). Suppose that fab ‚àºBer(Œ±abn‚àí1 log n) and
ha ‚àºNor(¬µa log n, œÉ2Id), where Œ±ab and ¬µa are independent of n. Then,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0900562851782364,I = (1 + o(1))log n
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09193245778611632,"n
min
aÃ∏=b sup
t‚àà(0,1) X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09380863039399624,"c
œÄc

tŒ±bc + (1 ‚àít)Œ±ac ‚àíŒ±t
bcŒ±1‚àít
ac

+ t‚à•¬µb ‚àí¬µa‚à•2
2
2œÉ2 ! ."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09568480300187618,"In particular, the technical conditions of Theorem 1 are verified if we rule out the uninformative case
where all the Œ±ab‚Äôs and the ¬µa‚Äôs are equal to each other. Thus, exact recovery is possible if"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.0975609756097561,"min
aÃ∏=b sup
t‚àà(0,1) X"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.09943714821763602,"c
œÄc

tŒ±bc + (1 ‚àít)Œ±ac ‚àíŒ±t
bcŒ±1‚àít
ac

+ t‚à•¬µb ‚àí¬µa‚à•2
2
2œÉ2 ! > 1."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10131332082551595,"Further assuming that Œ±ab = Œ±1(a = b) + Œ≤1(a Ã∏= b) (homogeneous interactions) and œÄ =
  1"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10318949343339587,"K , ¬∑ ¬∑ ¬∑ , 1"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1050656660412758,"K

(uniform block probabilities), the expression of I simplifies to"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10694183864915573,I = (1 + o(1))log n n
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.10881801125703565,""" ‚àöŒ± ‚àí‚àöŒ≤
2"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11069418386491557,"K
+ ‚àÜ2 8œÉ2 # ,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1125703564727955,"2Indeed, since the matrix Œ± = (Œ±ab) is symmetric, this implies that there exists a Ã∏= b such that ‚àÉc‚àó‚àà
[k]: Œ±ac‚àóÃ∏= Œ±bc‚àó. The function f(t) = P"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11444652908067542,"c œÄc

tŒ±bc + (1 ‚àít)Œ±ac ‚àíŒ±t
bcŒ±1‚àít
ac
 is continuous and strictly
concave, hence supt‚àà(0,1) f(t) is strictly concave."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11632270168855535,"where ‚àÜ= min
aÃ∏=b ‚à•¬µa ‚àí¬µb‚à•2. This last scenario recovers the recently established threshold for exact"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.11819887429643527,"recovery in the Contextual SBM [8].
Example 3 (Semi-supervised clustering in SBM). Consider binary interactions given by"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1200750469043152,"fab ‚àº
Ber(Œ±n‚àí1 log n)
if a = b,
Ber(Œ≤n‚àí1 log n)
otherwise,"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.12195121951219512,"where Œ±, Œ≤ are independent of n. Consider a semi-supervised model in which the vector of attributes
Y is a noisy oracle of the true community labels z. More precisely, for a node i such that zi = k, let"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.12382739212007504,"hk(y) = P(Yi = y) = Ô£±
Ô£≤ Ô£≥"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.12570356472795496,"1 ‚àíŒ∑
if y = 0,
Œ∑1
if y = k,
Œ∑0
K‚àí1
if y ‚àà[K]\{k},"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1275797373358349,"with Œ∑0 + Œ∑1 = Œ∑. A bit of algebra shows that exact recovery is possible if
‚àöŒ± ‚àí
p"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1294559099437148,"Œ≤
2
‚àí
2
log n log

1 ‚àíŒ∑ + 2(K ‚àí1)‚àí1/2‚àöŒ∑0Œ∑1

> K."
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.13133208255159476,"When Œ∑0 = 0 (perfect oracle), the condition simplifies to (‚àöa ‚àí
‚àö"
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.13320825515947468,b)2 ‚àí2 log(1‚àíŒ∑)
EXACT RECOVERY THRESHOLD IN NODE-ATTRIBUTED SBM,0.1350844277673546,"log n
> K. Note that
the oracle term is non-negligible only if ‚àílog (1 ‚àíŒ∑) ‚â≥log n, as previously established [31]. This
last condition is very strong since it implies Œ∑ ‚â≥1 ‚àí1/n, and hence the oracle must provide the
correct label for almost all nodes."
BREGMAN HARD CLUSTERING OF SPARSE WEIGHTED NODE-ATTRIBUTED NETWORKS,0.13696060037523453,"4
Bregman hard clustering of sparse weighted node-attributed networks"
BREGMAN HARD CLUSTERING OF SPARSE WEIGHTED NODE-ATTRIBUTED NETWORKS,0.13883677298311445,"In this section, we will propose an algorithm for clustering sparse, weighted networks with node
attributes. When present, the weights are sampled from an exponential family, and the node attributes
also belong to an exponential family. In Section 4.1, we provide some reminder of exponential families.
We derive the likelihood of the model in Section 4.2, and present the algorithm in Section 4.3."
EXPONENTIAL FAMILY,0.14071294559099437,"4.1
Exponential family"
EXPONENTIAL FAMILY,0.1425891181988743,"An exponential family Eœà is a parametric class of probability distributions whose densities can
be canonically written as pŒ∏,œà(x) = e<Œ∏,x>‚àíœà(Œ∏), where the density is taken with respect to an
appropriate measure, Œ∏ ‚ààŒò is a function of the parameters of the distribution that must belong to an
open convex space Œò, and œà is a convex function."
EXPONENTIAL FAMILY,0.14446529080675422,"We consider the model defined in (3.1), such that fab are zero-inflated distributions and are given by"
EXPONENTIAL FAMILY,0.14634146341463414,"fab(x) = (1 ‚àípab)Œ¥0(x) + pab Àúfab(x),
(4.1)"
EXPONENTIAL FAMILY,0.14821763602251406,"where pab ‚àà[0, 1] is the interaction probability between blocks a and b, Œ¥0(x) is the Dirac delta at
zero, and Àúfab is a probability density with no mass at zero. Note that this model can represent sparse
weighted networks, as edges between nodes in blocks a and b are absent with probability 1 ‚àípab."
EXPONENTIAL FAMILY,0.150093808630394,"Finally, suppose that the distributions { Àúfab} and {ha} belong to exponential families. More precisely,"
EXPONENTIAL FAMILY,0.15196998123827393,"Àúfab(x) = e<Œ∏ab,x>‚àíœà(Œ∏ab)
and
ha(y) = e<Œ∑a,y>‚àíœï(Œ∑a),
(4.2)"
EXPONENTIAL FAMILY,0.15384615384615385,"for some parameters Œ∏ab, Œ∑a and functions œà, œï. The following lemma provides the expression of the
Chernoff divergence of this model.
Lemma 1. Let fab and ha be defined as in (4.1)-(4.2). Suppose that pab = Œ±abŒ¥ where Œ±ab is
constant and Œ¥ ‚â™1. We have"
EXPONENTIAL FAMILY,0.15572232645403378,"I = (1 + o(1)) min
aÃ∏=b sup
t‚àà(0,1) Ô£±
Ô£≤ Ô£≥ X"
EXPONENTIAL FAMILY,0.1575984990619137,"c‚àà[K]
œÄc
h
tpac + (1 ‚àít)pbc ‚àípt
acp1‚àít
bc
e‚àíJœà(Œ∏ac‚à•Œ∏bc)i
+ Jœï(Œ∑a‚à•Œ∑b) Ô£º
Ô£Ω Ô£æ,"
EXPONENTIAL FAMILY,0.15947467166979362,where Jœà(Œ∏ab‚à•Œ∏bc) = tœà(Œ∏ac) + (1 ‚àít)œà(Œ∏bc) ‚àíœà(tŒ∏ac + (1 ‚àít)Œ∏bc).
EXPONENTIAL FAMILY,0.16135084427767354,"In combination with Theorem 1, Lemma 1 provides the expression for the exact recovery threshold
for node-attributed networks with interaction and attribute distributions given by (4.1)-(4.2)."
LOG-LIKELIHOOD,0.16322701688555347,"4.2
Log-likelihood"
LOG-LIKELIHOOD,0.1651031894934334,"Given a convex function œà, the Bregman divergence dœà : Rm √ó Rm ‚ÜíR+ is defined by"
LOG-LIKELIHOOD,0.1669793621013133,"dœà(x, y) = œà(x) ‚àíœà(y)‚àí< x ‚àíy, ‚àáœà(y) > ."
LOG-LIKELIHOOD,0.16885553470919323,"The log-likelihood of the density pœà,Œ∏ of an exponential family distribution is linked to the Bregman
divergence by the following relationship (see for example [6, Equation 13])"
LOG-LIKELIHOOD,0.17073170731707318,"log pœà,Œ∏(x) = ‚àídœà‚àó(x, ¬µ) + œà‚àó(x),
(4.3)"
LOG-LIKELIHOOD,0.1726078799249531,"where ¬µ = Epœà,Œ∏(X) is the mean of the distribution, and œà‚àódenotes the Legendre transform of œà,
defined by œà‚àó(t) = supŒ∏{< Œ∏, t > ‚àíœà(Œ∏)}. The following Lemma provides an expression for the
log-likelihood of fab when fab is a distribution belonging to a zero-inflated exponential family."
LOG-LIKELIHOOD,0.17448405253283303,"Lemma 2. Let fab be a probability density as defined in (4.1)-(4.2). For x, y ‚àà(0, 1), let dKL(x, y)
be the Kullback-Leibler divergences between Ber(x) and Ber(y), and let H(x) = x log x + (1 ‚àí
x) log(1 ‚àíx), with the usual convention 0 log 0 = 1. Then,"
LOG-LIKELIHOOD,0.17636022514071295,"‚àílog fab(x) = dKL (x‚à•pab) + sdœà‚àó(x, ¬µab) ‚àísœà‚àó(x) ‚àíH(s),"
LOG-LIKELIHOOD,0.17823639774859287,where s = 1(x Ã∏= 0).
LOG-LIKELIHOOD,0.1801125703564728,"Proof. To express log fab(x), we first note that"
LOG-LIKELIHOOD,0.18198874296435272,"log fab(x) = (1 ‚àís) log(1 ‚àípab) + s log pab + s log( Àúfab(x)),"
LOG-LIKELIHOOD,0.18386491557223264,"where c = 1(x Ã∏= 0). The result follows by adding and subtracting H(b) in the previous expression
and expressing log Àúfab(x) with a Bregman divergence as in (4.3)."
LOG-LIKELIHOOD,0.18574108818011256,"Suppose that X, Y follow the model (3.1) with probability distributions given by (4.1)-(4.2). Let A
be a binary matrix such that Aij = 1(Xij Ã∏= 0). We have"
LOG-LIKELIHOOD,0.18761726078799248,"‚àílog P(X, Y | z) =
X i Ô£±
Ô£≤ Ô£≥
1
2 X jÃ∏=i"
LOG-LIKELIHOOD,0.1894934333958724,"
dKL(Aij, pzizj) + Aijdœà‚àó 
Xij, ¬µzizj

+ dœï‚àó(Yi, ŒΩzi) Ô£º
Ô£Ω Ô£æ+ c,"
LOG-LIKELIHOOD,0.19136960600375236,"where the additional term c is a function of X, Y but does not depend on z. Denoting Z ‚àà{0, 1}n√óK"
LOG-LIKELIHOOD,0.19324577861163228,"the one-hot membership matrix such that Zik = 1(zi = k), observe that pzizj =
 
ZpZT "
LOG-LIKELIHOOD,0.1951219512195122,ij where p
LOG-LIKELIHOOD,0.19699812382739212,"is a symmetric matrix with the interaction probabilities between different blocks, ¬µzizj =
 
Z¬µZT "
LOG-LIKELIHOOD,0.19887429643527205,"ij
where ¬µ is a symmetric matrix with the expected value of the interaction between different blocks
(edge weights), and ŒΩzi =
 
ZT ŒΩ
"
LOG-LIKELIHOOD,0.20075046904315197,"i where ŒΩ is a vector with the expected value of the attribute for
different blocks. Thus, up to some additional constants, the negative log-likelihood ‚àílog P(X, Y | Z)
is equal to X i 1"
DKL,0.2026266416510319,"2 dKL
 
Ai¬∑,
 
ZpZT "
DKL,0.2045028142589118,"i¬∑

+ 1"
DKL,0.20637898686679174,"2d‚Ä≤
œà‚àó
 
Xi¬∑,
 
Z¬µZT "
DKL,0.20825515947467166,"i¬∑

+ dœï‚àó 
Yi,
 
ZT ŒΩ
"
DKL,0.2101313320825516,"i

+ c,
(4.4)"
DKL,0.21200750469043153,"where d‚Ä≤
œà‚àó(B, C) = Pn
j=1 1(Bj Ã∏= 0)dœà‚àó(Bj, Cj) for two vectors B, C ‚ààRn."
CLUSTERING BY ITERATIVE LIKELIHOOD MAXIMISATION,0.21388367729831145,"4.3
Clustering by iterative likelihood maximisation"
CLUSTERING BY ITERATIVE LIKELIHOOD MAXIMISATION,0.21575984990619138,"Following the log-likelihood expression derived in (4.4), we propose an iterative clustering algorithm
that places each node in the block maximising P (X, Y | z‚àíi, zi = a) for 1 ‚â§a ‚â§K, the likelihood
that node i is in community a given the community labels of the other nodes, z‚àíi. Let Z(ia) denote
the membership matrix obtained from Z by placing node i in block a, and let Lia(Z(ia)) denote the
contribution of node i to the negative log-likelihood when node i is placed in block a. Equation (4.4)
shows that"
CLUSTERING BY ITERATIVE LIKELIHOOD MAXIMISATION,0.2176360225140713,Lia(Z) = 1
DKL,0.21951219512195122,"2 dKL
 
Ai¬∑,
 
ZpZT "
DKL,0.22138836772983114,"i¬∑

+ 1"
DKL,0.22326454033771106,"2d‚Ä≤
œà‚àó
 
Xi¬∑,
 
Z¬µZT "
DKL,0.225140712945591,"i¬∑

+ dœï‚àó 
Yi,
 
ZT ŒΩ
"
DKL,0.2270168855534709,"i

,
(4.5)"
DKL,0.22889305816135083,"where the p, ¬µ and ŒΩ in the equation above must be estimated from X, Y , and the community
membership matrix Z. Let ÀÜp = ÀÜp(A, Z), ÀÜ¬µ = ÀÜ¬µ(X, Z), and ÀÜŒΩ = ÀÜŒΩ(Y, Z) denote the estimators for p,
¬µ and ŒΩ, respectively. Their values can be computed as follows:"
DKL,0.23076923076923078,"ÀÜp(A, Z) =
 
ZT Z
‚àí1 ZT AZ
 
ZT Z
‚àí1 ,"
DKL,0.2326454033771107,"ÀÜ¬µ(X, Z) =
 
ZT AZ
‚àí1 ZT XZ,"
DKL,0.23452157598499063,"ÀÜŒΩ(Y, Z) =
 
ZT Z
‚àí1 ZT Y. (4.6)"
DKL,0.23639774859287055,"Note that the matrix inverse
 
ZT Z
‚àí1 can be easily computed since ZT Z is a K-by-K diagonal
matrix. This approach is described in Algorithm 1."
DKL,0.23827392120075047,"Algorithm 1: Bregman hard clustering for node-attributed SBM.
Input: Interactions X ‚ààX n√ón, attributes Y ‚ààYn, convex functions œà‚àó, œï‚àó, clustering Z0
1 Let Z = Z0
2 repeat"
DKL,0.2401500938086304,"3
Compute ÀÜp, ÀÜ¬µ, ÀÜŒΩ according to (4.6)"
DKL,0.24202626641651032,"4
Let Znew = 0n√óK
5
for i = 1, . . . , n do"
DKL,0.24390243902439024,"6
Let Z(ia) be the membership matrix obtained from Z by placing node i in community a"
DKL,0.24577861163227016,"7
Find k‚àó= arg max
a‚àà[K]
Lia
 
Z(ia)
, where Lia
 
Z(ia)
is defined in (4.5);"
"LET ZNEW
IK",0.24765478424015008,"8
Let Znew
ik
= 1(k = k‚àó) for all k = 1, . . . , K"
"LET ZNEW
IK",0.24953095684803,"9
Let Z = Znew"
"LET ZNEW
IK",0.25140712945590993,10 until convergence;
"LET ZNEW
IK",0.25328330206378985,Return: Node-membership matrix Z
"LET ZNEW
IK",0.2551594746716698,"A fundamental aspect of many likelihood maximization iterative algorithms such as Algorithm 1
is the initial membership assignment, Z0. This initial assignment often has a profound influence
on the final membership assignment, and thus, it is important to have an adequate initialization. In
the numerical section, we proceed as follows. We construct the matrix W ‚ààRn√ó2K such that the
first K columns of W are the first K eigenvectors of the graph normalised Laplacian, while the
last K columns of W are the first K eigenvectors of the Gram matrix Y Y T ."
NUMERICAL EXPERIMENTS,0.2570356472795497,"5
Numerical experiments"
NUMERICAL EXPERIMENTS,0.2589118198874296,"5.1
Performance of Algorithm 1"
NUMERICAL EXPERIMENTS,0.2607879924953096,"We first compare in Figure 1 the performance of Algorithm 1 in terms of exact recovery (fraction of
times the algorithm correctly recovers the community of all nodes) with the theoretical threshold for
exact recovery proved in the paper (red curve in the plots) in two settings: Figure 1a shows binary
weight with Gaussian attributes, and Figure 1b shows zero-inflated Gaussian weights with Gaussian
attributes. A solid black (resp., white) square means that over 50 trials, the algorithms failed 50 times
(resp., succeeded 50 times) at exactly recovering the block structure."
COMPARISON WITH OTHER ALGORITHMS,0.2626641651031895,"5.2
Comparison with other algorithms"
COMPARISON WITH OTHER ALGORITHMS,0.26454033771106944,"In this section, we compare Algorithm 1 with other algorithms presented in the literature. We used
the Adjusted Rand Index (ARI) [21] between the predicted clusters and the ground truth ones to
evaluate the performance of each algorithm."
COMPARISON WITH OTHER ALGORITHMS,0.26641651031894936,"In Figure 2, we compare Algorithm 1 with the variational-EM algorithm of [24] and the algorithm
of [23] (which is also based on Bregman divergences, but tailored for dense networks). Because both
of these algorithms are designed for dense networks, we observe that Algorithm 1 has overall better
performance on sparse networks."
COMPARISON WITH OTHER ALGORITHMS,0.2682926829268293,"We also compare Algorithm 1 with the IR_sLs algorithm from [8]. This is one of the most recent
algorithms for node-attributed SBM and it comes with theoretical guarantees (for binary networks"
COMPARISON WITH OTHER ALGORITHMS,0.2701688555347092,"1
2
3
4
5
6
alpha 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 r 0.0 0.2 0.4 0.6 0.8 1.0"
COMPARISON WITH OTHER ALGORITHMS,0.27204502814258913,(a) Binary weights with Gaussian attributes
COMPARISON WITH OTHER ALGORITHMS,0.27392120075046905,0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 mu 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 r 0.0 0.2 0.4 0.6 0.8 1.0
COMPARISON WITH OTHER ALGORITHMS,0.275797373358349,"(b) zero-inflated Gaussian weights with Gaussian
attributes."
COMPARISON WITH OTHER ALGORITHMS,0.2776735459662289,"Figure 1: Phase transition of exact recovery. Each pixel represents the empirical probability that
Algorithm 1 succeeds at exactly recovering the clusters (over 50 runs), and the red curve shows the
theoretical threshold.
(a) n = 500, K = 2, fin = Ber(Œ±n‚àí1 log n), fout = Ber(n‚àí1 log n). The attributes are 2d-
spherical Gaussian with radius (¬±r‚àölog n, 0) and identity covariance matrix.
(b) n = 600, K = 3, fin = (1 ‚àíœÅ)Œ¥0 + œÅ Nor(¬µ, 1), fout = (1 ‚àíœÅ)Œ¥0 + œÅ Nor(0, 1) with
œÅ = 5n‚àí1 log n. The attributes are 2d-spherical Gaussian whose means are the vertices of a regular
polygon on the circle of radius r‚àölog n."
COMPARISON WITH OTHER ALGORITHMS,0.2795497185741088,"0.03
0.05
0.07
0.09
0.11
p 0.00 0.25 0.50 0.75 1.00 ARI"
COMPARISON WITH OTHER ALGORITHMS,0.28142589118198874,"Algorithm 1
bregman [23]
vem [24]
initialisation"
COMPARISON WITH OTHER ALGORITHMS,0.28330206378986866,"1
3
5
7
9
11
muin 0.00 0.25 0.50 0.75 1.00 ARI"
COMPARISON WITH OTHER ALGORITHMS,0.2851782363977486,"Algorithm 1
bregman [23]
vem [24]
initialisation"
COMPARISON WITH OTHER ALGORITHMS,0.2870544090056285,"Figure 2: Comparison of Algorithm 1 with algorithms of [23] and [24]. Error bars show the standard
deviations over 25 realisations. Attributes are 2d-spherical Gaussian attributes with radius (¬±1, 0).
(a) n = 100, K = 2, fin = (1 ‚àípin)Œ¥0 + pin Poi(5), fout = (1 ‚àí0.03)Œ¥0 + 0.03 Poi(1).
(b) n = 100, K = 2, fin = (1 ‚àí0.07)Œ¥0 + 0.07 Poi(¬µin), fout = (1 ‚àí0.04)Œ¥0 + 0.04 Poi(1)."
COMPARISON WITH OTHER ALGORITHMS,0.28893058161350843,"with Gaussian attributes). We also compare with the EM algorithm of [34], attSBM, which is
designed for binary networks with Gaussian attributes. Finally, we compare with baseline methods
for clustering using the network or the attributes alone. EM-GMM refers to fitting a Gaussian Mixture
Model via EM on attribute data Y , and sc refers to spectral clustering on network data X."
COMPARISON WITH OTHER ALGORITHMS,0.29080675422138835,"Figure 3 shows the results for binary networks with Gaussian attributes. Algorithm 1 successfully
learns from both the signal coming from the network and the attributes, even in scenarios where
one of them is non-informative. Moreover, Algorithm 1 has better performance than the two other
node-attributed clustering algorithms, and those algorithms also show a large variance3. We also note
that IR_sLs and attSBM are both tailor-made for binary edges and Gaussian attributes. Even in such
a setting, Algorithm 1 outperforms these two algorithms. We show in the supplement material that
when the network is weighted and the attributes non-Gaussian, IR_sLs and attSBM perform poorly."
EVALUATION USING REAL DATASETS,0.2926829268292683,"5.3
Evaluation using real datasets"
EVALUATION USING REAL DATASETS,0.2945590994371482,"The following three benchmark datasets were used to evaluate and compare the proposed algorithm:
CiteSeer (n = 3279, m = 9104, K = 6, d = 3703), Cora (n = 2708, m = 10556, K = 7,"
EVALUATION USING REAL DATASETS,0.2964352720450281,"3As the large variance makes the figures less readable, we provide all results in the supplemental material."
EVALUATION USING REAL DATASETS,0.29831144465290804,"(a) Varying edge distribution.
(b) Varying attribute distributions."
EVALUATION USING REAL DATASETS,0.300187617260788,"Figure 3: Performance on binary networks with Gaussian attributes. We take n = 600, K = 2,
fin = Ber(an‚àí1 log n), fout = Ber(5n‚àí1 log n), and 2-dimensional Gaussian attributes with unit
variance and mean (¬±r, 0). Results are averaged over 60 runs."
EVALUATION USING REAL DATASETS,0.30206378986866794,"d = 1433), and Cornell (n = 183, m = 298, K = 5, d = 1703) (all available in Pytorch Geometric).
For each network, the original node attribute vector was reduced to have dimension d = 10 by
selecting the 10 best features according to the chi-square test. Algorithm 1 assumed a multivariate
Gaussian distribution with d = 10 for node attributes and Bernoulli edges (these networks have no
edge weights). The initialization for Algorithm 1 and attSBM used spectral clustering of both the
node similarity matrix (using node attributes) and network edges."
EVALUATION USING REAL DATASETS,0.30393996247654786,"Dataset
CiteSeer
Cora
Cornell"
EVALUATION USING REAL DATASETS,0.3058161350844278,"Algorithm 1
0.20
0.12
0.49
attSBM
0.17
0.09
0.46
EM-GMM
0.13
0.06
0.37
sc
0.00
0.00
0.02
Table 1: Average ARI results (over independent runs) for the three benchmark datasets."
EVALUATION USING REAL DATASETS,0.3076923076923077,"Table 1 shows that Algorithm 1 outperformed the other three algorithms. Spectral clustering (sc) has
near zero performance, indicating that the network structure in these data sets has little information
concerning the clusters of the nodes. Moreover, both Algorithm 1 and attSBM (that leverage network
and node attributes) outperform EM-GMM that use only node attributes. These preliminary results
indicate that Algorithm 1 is promising even in real data sets with little pre-processing."
CONCLUSION,0.30956848030018763,"6
Conclusion"
CONCLUSION,0.31144465290806755,"This work made the following contributions to community detection in node-attributed networks: i)
extended the known thresholds for exact recovery in binary SBM to non-binary (weighted) networks
with node attributes, providing a clean expression for a new information-theoretic quantity, known
in the binary setting as the Chernoff-Hellinger divergence; ii) proposed an iterative algorithm based
on the likelihood function that can infer community memberships from a problem instance. The
algorithm leverages the framework of Bregman divergences and is simple and computationally
efficient. Numerical experiments indicate the superiority of this algorithm when compared to recent
state-of-the-art approaches."
CONCLUSION,0.3133208255159475,Acknowledgements
CONCLUSION,0.3151969981238274,The first author would like to thank Lasse Leskel√§ for helpful discussions and comments.
CONCLUSION,0.3170731707317073,"This work has been partially funded by the Brazilian-Swiss Joint Research Program (grant
IZBRZ2_186313), the Brazilian National Council for Scientific and Technological Development
(CNPq), and the Carlos Chagas Filho Research Foundation of the State of Rio de Janeiro (FAPERJ)."
REFERENCES,0.31894934333958724,References
REFERENCES,0.32082551594746717,"[1] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of
Machine Learning Research, 18(1):6446‚Äì6531, 2017."
REFERENCES,0.3227016885553471,"[2] Emmanuel Abbe, Jianqing Fan, and Kaizheng Wang. An ‚Ñìp theory of pca and spectral clustering. The
Annals of Statistics, 50(4):2359‚Äì2385, 2022."
REFERENCES,0.324577861163227,"[3] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models: Fundamental
limits and efficient algorithms for recovery. In 2015 IEEE 56th Annual Symposium on Foundations of
Computer Science, pages 670‚Äì688, Los Alamitos, CA, USA, 2015. IEEE Computer Society."
REFERENCES,0.32645403377110693,"[4] Konstantin Avrachenkov and Maximilien Dreveton. Statistical Analysis of Networks. Now Publishers,
2022."
REFERENCES,0.32833020637898686,"[5] Konstantin Avrachenkov, Maximilien Dreveton, and Lasse Leskel√§. Community recovery in non-binary
and temporal stochastic block models. arXiv preprint arXiv:2008.04790, 2022."
REFERENCES,0.3302063789868668,"[6] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering with
Bregman divergences. Journal of machine learning research, 6(10), 2005."
REFERENCES,0.3320825515947467,"[7] Norbert Binkiewicz, Joshua T Vogelstein, and Karl Rohe. Covariate-assisted spectral clustering. Biometrika,
104(2):361‚Äì377, 2017."
REFERENCES,0.3339587242026266,"[8] Guillaume Braun, Hemant Tyagi, and Christophe Biernacki. An iterative clustering algorithm for the
contextual stochastic block model with optimality guarantees. In International Conference on Machine
Learning, pages 2257‚Äì2291. PMLR, 2022."
REFERENCES,0.33583489681050654,"[9] Andressa Cerqueira and Elizaveta Levina. A pseudo-likelihood approach to community detection in
weighted networks. arXiv preprint arXiv:2303.05909, 2023."
REFERENCES,0.33771106941838647,"[10] Narasinga Rao Chaganty and Jayaram Sethuraman. Strong large deviation and local limit theorems. The
Annals of Probability, pages 1671‚Äì1690, 1993."
REFERENCES,0.3395872420262664,"[11] Xiaohui Chen and Yun Yang. Cutoff for exact recovery of Gaussian mixture models. IEEE Transactions
on Information Theory, 67(6):4223‚Äì4238, 2021."
REFERENCES,0.34146341463414637,"[12] Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pages 493‚Äì507, 1952."
REFERENCES,0.3433395872420263,"[13] Michael Collins, Sanjoy Dasgupta, and Robert E Schapire. A generalization of principal components
analysis to the exponential family. Advances in neural information processing systems, 14, 2001."
REFERENCES,0.3452157598499062,"[14] David Combe, Christine Largeron, Mathias G√©ry, and ElÀùod Egyed-Zsigmond. I-Louvain: An attributed
graph clustering method. In Advances in Intelligent Data Analysis XIV: 14th International Symposium,
IDA 2015, Saint Etienne. France, October 22-24, 2015. Proceedings 14, pages 181‚Äì192. Springer, 2015."
REFERENCES,0.34709193245778613,"[15] Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Contextual stochastic block
models. Advances in Neural Information Processing Systems, 31, 2018."
REFERENCES,0.34896810506566606,"[16] Issam Falih, Nistor Grozavu, Rushed Kanawati, and Youn√®s Bennani. Community detection in attributed
network. In Companion proceedings of the the web conference 2018, pages 1299‚Äì1306, 2018."
REFERENCES,0.350844277673546,"[17] Cees M Fortuin, Pieter W Kasteleyn, and Jean Ginibre. Correlation inequalities on some partially ordered
sets. Communications in Mathematical Physics, 22:89‚Äì103, 1971."
REFERENCES,0.3527204502814259,"[18] Santo Fortunato and Darko Hric. Community detection in networks: A user guide. Physics reports,
659:1‚Äì44, 2016."
REFERENCES,0.3545966228893058,"[19] Geoffrey Grimmett. Percolation. Springer, 1999."
REFERENCES,0.35647279549718575,"[20] Theodore E. Harris. A lower bound for the critical probability in a certain percolation process. In
Mathematical Proceedings of the Cambridge Philosophical Society, volume 56, pages 13‚Äì20. Cambridge
University Press, 1960."
REFERENCES,0.35834896810506567,"[21] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2:193‚Äì218, 1985."
REFERENCES,0.3602251407129456,"[22] Varun Jog and Po-Ling Loh. Recovering communities in weighted stochastic block models. In Proceedings
of the 53rd Annual Allerton Conference on Communication, Control, and Computing, Los Alamitos, CA,
USA, October 2015. IEEE Computer Society."
REFERENCES,0.3621013133208255,"[23] Bo Long, Zhongfei Mark Zhang, and Philip S Yu. A probabilistic framework for relational clustering. In
ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 470‚Äì479,
2007."
REFERENCES,0.36397748592870544,"[24] Mahendra Mariadassou, St√©phane Robin, and Corinne Vacher. Uncovering latent structure in valued graphs:
A variational approach. The Annals of Applied Statistics, 4(2):715 ‚Äì 742, 2010."
REFERENCES,0.36585365853658536,"[25] Angelo Mele, Lingxin Hao, Joshua Cape, and Carey E Priebe. Spectral inference for large stochastic
blockmodels with nodal covariates. arXiv preprint arXiv:1908.06438, 2019."
REFERENCES,0.3677298311444653,"[26] Mohamed Ndaoud. Sharp optimal recovery in the two component Gaussian mixture model. The Annals of
Statistics, 50(4):2096‚Äì2126, 2022."
REFERENCES,0.3696060037523452,"[27] Mark EJ Newman and Aaron Clauset. Structure and inference in annotated networks. Nature communica-
tions, 7(1):11863, 2016."
REFERENCES,0.3714821763602251,"[28] Frank Nielsen. Chernoff information of exponential families. arXiv preprint arXiv:1102.2684, 2011."
REFERENCES,0.37335834896810505,"[29] Subhadeep Paul and Yuguo Chen. Consistent community detection in multi-relational data through
restricted multi-layer stochastic blockmodel. Electronic Journal of Statistics, 10(2):3807 ‚Äì 3870, 2016."
REFERENCES,0.37523452157598497,"[30] Detlef Plachky and Joseph Steinebach. A theorem about probabilities of large deviations with an application
to queuing theory. Periodica Mathematica Hungarica, 6(4):343‚Äì345, 1975."
REFERENCES,0.3771106941838649,"[31] Hussein Saad and Aria Nosratinia. Community detection with side information: Exact recovery under the
stochastic block model. IEEE Journal of Selected Topics in Signal Processing, 12(5):944‚Äì958, 2018."
REFERENCES,0.3789868667917448,"[32] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93‚Äì93, 2008."
REFERENCES,0.3808630393996248,"[33] Laura M Smith, Linhong Zhu, Kristina Lerman, and Allon G Percus. Partitioning networks with node
attributes by compressing information flow. ACM Transactions on Knowledge Discovery from Data
(TKDD), 11(2):1‚Äì26, 2016."
REFERENCES,0.3827392120075047,"[34] Natalie Stanley, Thomas Bonacci, Roland Kwitt, Marc Niethammer, and Peter J Mucha. Stochastic block
models with multiple continuous attributes. Applied Network Science, 4(1):1‚Äì22, 2019."
REFERENCES,0.38461538461538464,"[35] Tim Van Erven and Peter Harremos. R√©nyi divergence and Kullback-Leibler divergence. IEEE Transactions
on Information Theory, 60(7):3797‚Äì3820, 2014."
REFERENCES,0.38649155722326456,"[36] Min Xu, Varun Jog, and Po-Ling Loh. Optimal rates for community estimation in the weighted stochastic
block model. The Annals of Statistics, 48(1):183 ‚Äì 204, 2020."
REFERENCES,0.3883677298311445,"[37] Bowei Yan and Purnamrita Sarkar. Covariate regularized community detection in sparse graphs. Journal
of the American Statistical Association, 116(534):734‚Äì745, 2021."
REFERENCES,0.3902439024390244,"[38] Se-Young Yun and Alexandre Proutiere. Optimal cluster recovery in the labeled stochastic block model.
Advances in Neural Information Processing Systems, 29, 2016."
REFERENCES,0.3921200750469043,"[39] Anderson Y. Zhang and Harrison H. Zhou. Minimax rates of community detection in stochastic block
models. The Annals of Statistics, 44(5):2252 ‚Äì 2280, 2016."
REFERENCES,0.39399624765478425,"A
Establishing the exact recovery threshold"
REFERENCES,0.39587242026266417,"The proof of Theorem 1 is structured as follows. We start by establishing some concentration
results on the block sizes in Section A.1. In Section A.2, we delve into the asymptotic analysis
of log-likelihood ratios, establishing fundamental results. Building upon these findings, we prove
the converse statement, demonstrating the impossibility of exact recovery below the threshold, in
Section A.3. Conversely, in Section A.4, we establish the positive statement, demonstrating the
possibility of exact recovery above the threshold. To complement the proof, Section A.5 presents a
set of technical combinatorial lemmas."
REFERENCES,0.3977485928705441,"Notations
In the following, we denote by z ‚àà[K]n the true block-labelling vector, and by z‚Ä≤ ‚àà
[K]n another block-labelling vector, and the conditional probabilities are denoted by Pz(¬∑) = P(¬∑ | z)."
REFERENCES,0.399624765478424,"A.1
Preliminaries on the block sizes"
REFERENCES,0.40150093808630394,"For any z‚Ä≤ ‚àà[K]n, we for all c ‚àà[K] define ÀÜœÄc(z‚Ä≤), the empirical size of block c, as"
REFERENCES,0.40337711069418386,"ÀÜœÄc(z‚Ä≤) = |{i ‚àà[n]: z‚Ä≤
i = c}|
n
."
REFERENCES,0.4052532833020638,"For any 0 < Œæ < 1, we define"
REFERENCES,0.4071294559099437,"ZŒæ = {z ‚àà[K]n : (1 ‚àíŒæ)œÄc ‚â§ÀÜœÄc(z) ‚â§(1 + Œæ)œÄc
‚àÄc ‚àà[K]} .
(A.1)"
REFERENCES,0.4090056285178236,"Recalling that the true block-labelling vector z is sampled from œÄ‚äón, we have by concentration of
multinomial distributions (see for example [5, Lemma A1])"
REFERENCES,0.41088180112570355,"P(ZŒæ) ‚â•1 ‚àí2K exp

‚àíŒæ2"
N MIN,0.41275797373358347,"3 n min
c‚àà[K] œÄc 
."
N MIN,0.4146341463414634,"In the following, we chose Œæ = log log n
‚àön
, and hence P(ZŒæ) = 1 ‚àío(1)."
N MIN,0.4165103189493433,"A.2
Asymptotic study of log-likelihood ratios"
N MIN,0.41838649155722324,"This section studies the asymptotic behaviour of Pz (L(z) ‚â•L(z‚Ä≤)), where L(z‚Ä≤) = Pz‚Ä≤(X, Y ) the
likelihood of the block-labelling vector z‚Ä≤ given the observed data X, Y ."
N MIN,0.4202626641651032,"A.2.1
Upper-bound"
N MIN,0.42213883677298314,"The following lemma provides an upper bound on Pz (L(z) ‚â•L(z‚Ä≤)).
Lemma 3. Let z, z‚Ä≤ ‚ààZ be two block labelling vectors such that Ham(z, z‚Ä≤) = m ‚â•1, and let
L(z) = P(X, Y | z) be the likelihood of labelling z. We have"
N MIN,0.42401500938086306,"Pz (L(z‚Ä≤) ‚â•L(z)) ‚â§e
‚àí(1+o(1))

1‚àí
m
nœÄmin"
N MIN,0.425891181988743,"
mnI."
N MIN,0.4277673545966229,"Moreover, let d = sup
t‚àà(0,1)
min
aÃ∏=b‚àà[K]
c‚àà[K]
(1 ‚àít) Dt(fac‚à•fbc). Then we also have"
N MIN,0.42964352720450283,Pz (L(z‚Ä≤) ‚â•L(z)) ‚â§e‚àím(n‚àím)d.
N MIN,0.43151969981238275,"Proof. Using Chernoff‚Äôs bound, we have for all t ‚â•0, Pz"
N MIN,0.4333958724202627,"
log Pz‚Ä≤(X, Y )"
N MIN,0.4352720450281426,"Pz(X, Y ) > œµ

‚â§e‚àítœµ Ez"
N MIN,0.4371482176360225,"
et log
Pz‚Ä≤ (X,Y )"
N MIN,0.43902439024390244,"Pz(X,Y )

= e‚àítœµ‚àí(1‚àít) Dt(Pz)‚à•Pz‚Ä≤),
(A.2)"
N MIN,0.44090056285178236,"where the probability measure Pz is defined on X √ó Y by (3.1) in the main text. The linearity of the
R√©nyi divergence with respect to product distributions implies"
N MIN,0.4427767354596623,Dt (Pz‚à•Pz‚Ä≤) = 1 2 X
N MIN,0.4446529080675422,"iÃ∏=j
Dt

fzizj‚à•fz‚Ä≤
iz‚Ä≤
j 
+
X"
N MIN,0.44652908067542213,"1‚â§i‚â§n
Dt(hzi‚à•hz‚Ä≤
i) = 1 2 X"
N MIN,0.44840525328330205,"1‚â§a,b,c,d‚â§K
M(ab)(cd) Dt(fac‚à•fbd) +
X"
N MIN,0.450281425891182,"1‚â§a,b‚â§K
Nab Dt(ha‚à•hb),
(A.3) where"
N MIN,0.4521575984990619,"Nab = |{i ‚àà[n]: (zi, z‚Ä≤
i) = (a, b)}| ,"
N MIN,0.4540337711069418,"M(ab)(cd) =

i Ã∏= j : (zi, z‚Ä≤
i) = (a, b) and (zj, z‚Ä≤
j) = (c, d)
	 ."
N MIN,0.45590994371482174,"Moreover, Lemma 6 ensures that"
N MIN,0.45778611632270166,"M(ab)(cd) = Nab
 
Ncd ‚àí1(ab)=(cd)

= Ncd
 
Nab ‚àí1(ab)=(cd)

."
N MIN,0.4596622889305816,"Therefore,
X"
N MIN,0.46153846153846156,"1‚â§a,b,c,d‚â§K
M(ab)(cd) Dt(fac‚à•fbd) ‚â•
X aÃ∏=b X"
N MIN,0.4634146341463415,"c
M(ab)(cc) Dt(fac‚à•fbc) +
X a X"
N MIN,0.4652908067542214,"cÃ∏=d
M(aa)(cd) Dt(fac‚à•fad) = 2
X"
N MIN,0.46716697936210133,"aÃ∏=b
Nab
X"
N MIN,0.46904315196998125,"c
Ncc Dt(fac‚à•fbc).
(A.4)"
N MIN,0.4709193245778612,"Using Ncc =
z‚àí1(c)
 ‚àíP"
N MIN,0.4727954971857411,"dÃ∏=c Ncd and that P c
P"
N MIN,0.474671669793621,"dÃ∏=c Ncd = m, we obtain that 1
2 X"
N MIN,0.47654784240150094,"1‚â§a,b,c,d‚â§K
M(ab)(cd) Dt(fac‚à•fbd) ‚â•n
X"
N MIN,0.47842401500938087,"aÃ∏=b
Nab
X"
N MIN,0.4803001876172608,"c
ÀÜœÄc(z)

1 ‚àí P"
N MIN,0.4821763602251407,"dÃ∏=c Ncd
nÀÜœÄc(z)"
N MIN,0.48405253283302063,"
Dt(fac‚à•fbc)"
N MIN,0.48592870544090055,"‚â•n

1 ‚àí
m
n minc ÀÜœÄc(z)  X"
N MIN,0.4878048780487805,"aÃ∏=b
Nab
X"
N MIN,0.4896810506566604,"c
ÀÜœÄc(z) Dt(fac‚à•fbc)."
N MIN,0.4915572232645403,"Hence,"
N MIN,0.49343339587242024,"(1 ‚àít) Dt (Pz‚Ä≤‚à•Pz) ‚â•n

1 ‚àí
m
n minc ÀÜœÄc(z)  X"
N MIN,0.49530956848030017,"aÃ∏=b
Nab(1 ‚àít) X"
N MIN,0.4971857410881801,"c
ÀÜœÄc(z) Dt(fac‚à•fbc) + 1"
N MIN,0.49906191369606,n Dt(ha‚à•hb) !
N MIN,0.50093808630394,"= n

1 ‚àí
m
n minc ÀÜœÄc(z)  X"
N MIN,0.5028142589118199,"aÃ∏=b
NabCHt(a, b, ÀÜœÄ(z))."
N MIN,0.5046904315196998,"Using the concentration of ÀÜz, we have ÀÜœÄc(z) = (1 + o(1))œÄc for all c, and hence"
N MIN,0.5065666041275797,"sup
t‚àà(0,1)
(1 ‚àít) Dt (Pz‚Ä≤‚à•Pz) ‚â•(1 + o(1))

1 ‚àí
m
nœÄmin"
N MIN,0.5084427767354597,"
mnI,"
N MIN,0.5103189493433395,where we used P
N MIN,0.5121951219512195,"aÃ∏=b Nab = m and I ‚â§CH(a, b, ÀÜœÄ(ÀÜz)). The first upper bound on Pz(L(z‚Ä≤) ‚â•L(z))
follows by taking œµ = 0 and the supremum over t ‚àà(0, 1) in (A.2)."
N MIN,0.5140712945590994,"Finally, let d = sup
t‚àà(0,1)
min
aÃ∏=b‚àà[K]
c‚àà[K]
(1 ‚àít) Dt(fac‚à•fbc). Combining (A.3) and (A.4) leads to"
N MIN,0.5159474671669794,"sup
t‚àà(0,1)
(1 ‚àít) Dt(Pz‚à•Pz‚Ä≤) ‚â•d
X"
N MIN,0.5178236397748592,"aÃ∏=b
Nab
X"
N MIN,0.5196998123827392,"c
Ncc = dm(n ‚àím),"
N MIN,0.5215759849906192,and this ends the proof.
N MIN,0.5234521575984991,"A.2.2
Lower bound"
N MIN,0.525328330206379,"We now focus on lower-bounding Pz (L(z) ‚â•L(z‚Ä≤)) when Ham(z, z‚Ä≤) = 1. In particular, the
condition Ham(z, z‚Ä≤) = 1 implies that there exists a unique node u ‚àà[n] such that zu Ã∏= z‚Ä≤
u. Let
a, b ‚àà[K] such that zu = a and z‚Ä≤
u = b. By definition of the likelihood, we have L(z) ‚àíL(z‚Ä≤) =
‚àÜub(X, Y, z) where"
N MIN,0.5272045028142589,"‚àÜub(X, Y, z) = n
X"
N MIN,0.5290806754221389,"v=1
vÃ∏=u"
N MIN,0.5309568480300187,log fbzv
N MIN,0.5328330206378987,"fazv
(Xuv) + log hb"
N MIN,0.5347091932457786,"ha
(Yu).
(A.5)"
N MIN,0.5365853658536586,"Before studying the large deviation rates of likelihood ratios such as Pz (‚àÜub > 0) and
Pz (‚àÜub > 0, ‚àÜvb > 0), we first recall some large deviation result for arbitrary sequences of random
variables [30, 10]."
N MIN,0.5384615384615384,"Proposition 1 (Strong large deviations for arbitrary sequences of random variables ‚Äì Theorem 3.3
of [10]). Let Tn be a sequence of random variables whose moment generating function œïn(t) =
E

etTn
is non-vanishing and analytic in the region ‚Ñ¶= {z ‚ààC: |z| < a} for some a > 0. Let kn
be a sequence of real numbers, and let œàn(t) = k‚àí1
n log œïn(t). Let (mn) be a bounded sequence of
real numbers such that there exists a sequence (œÑn) verifying œà‚Ä≤
n(œÑn) = mn and 0 < œÑn < Œ±0 < Œ±.
Suppose that:"
N MIN,0.5403377110694184,1. there exists b > 0 such that |œàn(z)| ‚â§b for all z ‚àà‚Ñ¶;
N MIN,0.5422138836772983,"2. there exists a > 0 such that œà‚Ä≤‚Ä≤
n(œÑn) > c;"
N MIN,0.5440900562851783,"3. there exists Œ¥0 > 0 such that supŒ¥<|t|<ŒªœÑn
 œïn(œÑn+it)"
N MIN,0.5459662288930581,"œïn(œÑn)
 = o(k‚àí1/2
n
) for any given Œ¥, Œª such
that 0 < Œ¥ < Œ¥0 < Œª. Then,"
N MIN,0.5478424015009381,"P (Tn > anmn) =
1 + o(1) œÑn
p"
N MIN,0.549718574108818,"2œÄanœà‚Ä≤‚Ä≤(œÑn)
e‚àían(mnœÑn‚àíœàn(œÑn))."
N MIN,0.551594746716698,"Proposition 1 is an extension of Cramer‚Äôs large deviation theorem for sums of iid r.v. to sequences of
arbitrary random variables (see [10, Remark 3.6]). We will apply it to the study of P (‚àÜub > 0).
Lemma 4. Let z ‚ààZŒæ (where ZŒæ is defined in (A.1)) with Œæ = o(1). Let u ‚àà[n] such that
zu = a and let ‚àÜub be given in (A.5). Suppose that there exists Œ¥n = œâ(1) such that Œ≤ : t ‚àà
(0, 1) 7‚Üí‚àílim
n‚Üí‚àûnŒ¥‚àí1
n CHt(a, b) exists and is strictly convex. Then, for any sequence œµn such that"
N MIN,0.5534709193245778,"Œ¥nœµn ‚Üíœµ ‚àà{Œ≤‚Ä≤(t), t ‚àà(0, 1)} we have"
N MIN,0.5553470919324578,"Pz (‚àÜub > Œ¥nœµn) ‚àº
1 tœµ
p"
N MIN,0.5572232645403377,"2œÄŒ¥nŒ≤‚Ä≤‚Ä≤(tœµ)
e‚àíŒ¥nŒ≤‚àó(tœµ),"
N MIN,0.5590994371482176,"where Œ≤‚àó(x) = sup
t>0
{tx ‚àíŒ≤(t)} is the Legendre transform of Œ≤, and tœµ is such that Œ≤‚Ä≤(tœµ) = œµ. In"
N MIN,0.5609756097560976,"particular 0 ‚àà{Œ≤‚Ä≤(t), t ‚àà(0, 1)}, and therefore for any sequence œµn = o(Œ¥‚àí1
n ) we have"
N MIN,0.5628517823639775,"Pz (‚àÜub > Œ¥nœµn) ‚àº
1 t0
p"
N MIN,0.5647279549718575,"2œÄŒ¥nŒ≤‚Ä≤‚Ä≤(t0)
e‚àínCH(a,b)."
N MIN,0.5666041275797373,"Proof of Lemma 4. We will apply Proposition 1 with ‚Ñ¶= {z ‚ààC: |z| < 1} and kn = Œ¥‚àí1
n
to obtain
the stated results."
N MIN,0.5684803001876173,"Let us first compute œïn(t) = Ez

et‚àÜub
and œàn(t) = Œ¥‚àí1
n log œïn(t) for t ‚àà(0, 1). We first notice
that for all v ‚àà[n], Ez"
N MIN,0.5703564727954972,"
et log
fbzv
fazv (Xuv)

= E"
N MIN,0.5722326454033771,""" fbzv fazv"
N MIN,0.574108818011257,"t
(Xuv) #"
N MIN,0.575984990619137,"= e‚àí(1‚àít) Dt(fbzv ‚à•fazv ),"
N MIN,0.5778611632270169,by definition of the R√©nyi divergence. Similar computations show that
N MIN,0.5797373358348968,"Ez
h
et log
hb
ha (Yu)i
= e‚àí(1‚àít) Dt(hb‚à•ha)."
N MIN,0.5816135084427767,"Therefore,"
N MIN,0.5834896810506567,"œïn(t) = et log
œÄb
œÄa ‚àínCHt(a,b,z), where"
N MIN,0.5853658536585366,"CHt(a, b, z) = (1 ‚àít) "" K
X"
N MIN,0.5872420262664165,"c=1
ÀÜœÄc Dt (fbc‚à•fac) + 1"
N MIN,0.5891181988742964,n Dt (hb‚à•ha) #
N MIN,0.5909943714821764,",
(A.6)"
N MIN,0.5928705440900562,"with ÀÜœÄc = n‚àí1 z‚àí1(k)
. We note that since z ‚ààZŒæ we have CHt(a, b, z) = (1 + O(Œæ))CHt(a, b).
Thus, œàn(t) = ‚àí(1 + o(1))nCHt(a, b). Moreover, the assumption CH(a, b) = Œò
 
n‚àí1Œ¥n

implies"
N MIN,0.5947467166979362,"that CHt(a, b) = Œò
 
n‚àí1Œ¥n

for all t ‚àà(0, 1) since the R√©nyi divergences of orders t ‚àà(0, 1) are
equivalent [35, Theorem 16]. Hence, Œ¥‚àí1
n œàn(t) = (1+o(1))Œ≤(t). Since Œ≤ is well-defined and strictly
convex on (0, 1), this ensures that Assumptions 1 and 2 of Proposition 1 are verified. Moreover, we
notice that eŒ≤(t) is the m.g.f of some r.v. X, and let œÜ(t) = EeitX be the characteristic function"
N MIN,0.5966228893058161,"of X. Then, œïn(t‚àó+it)"
N MIN,0.5984990619136961,"œïn(t‚àó)
=

œÜ(t‚àó+it)"
N MIN,0.600375234521576,"œÜ(t‚àó)
Œ¥n
. Since œÜ(t‚àó+it)"
N MIN,0.6022514071294559,"œÜ(t‚àó)
is the characteristic function of some r.v., its
module is strictly less than 1 on an interval not containing 0, and hence Assumption 3 of Proposition 1
is verified."
N MIN,0.6041275797373359,"Finally, Lemma 8 ensures that Œ≤‚Ä≤(0) ‚â§0 and Œ≤‚Ä≤(1) ‚â•0 and hence 0 ‚àà{Œ≤‚Ä≤(t), t ‚àà(0, 1)}."
N MIN,0.6060037523452158,"A.2.3
Asymptotic independence"
N MIN,0.6078799249530957,"Finally, the following lemma shows that the events ‚àÜub > 0 and ‚àÜvb > 0 are asymptotically
independent.
Lemma 5. Let z ‚ààZŒæ, u, v ‚ààz‚àí1(a), and b ‚àà[K]\{a}. Then,"
N MIN,0.6097560975609756,"Pz (‚àÜub > 0, ‚àÜvb > 0)
Pz (‚àÜub > 0) Pz (‚àÜvb > 0) = 1 + o(1)."
N MIN,0.6116322701688556,Proof. Let Œæ(Xuv) = log faa
N MIN,0.6135084427767354,fab (Xuv). We have
N MIN,0.6153846153846154,"‚àÜub = log œÄb œÄa
+ n
X"
N MIN,0.6172607879924953,"w=1
vÃ∏‚àà{u,v}"
N MIN,0.6191369606003753,log fbzw
N MIN,0.6210131332082551,"fazw
(Xuw) + log hb"
N MIN,0.6228893058161351,"ha
(Yu) + log fba"
N MIN,0.624765478424015,"faa
(Xuv),"
N MIN,0.626641651031895,"‚àÜvb = log œÄb œÄa
+ n
X"
N MIN,0.6285178236397748,"w=1
vÃ∏‚àà{u,v}"
N MIN,0.6303939962476548,log fbzw
N MIN,0.6322701688555347,"fazw
(Xvw) + log hb"
N MIN,0.6341463414634146,"ha
(Yv) + log fba"
N MIN,0.6360225140712945,"faa
(Xuv)."
N MIN,0.6378986866791745,"Therefore,"
N MIN,0.6397748592870544,"‚àÜub = Uu ‚àíŒæ(Xuv)
and
‚àÜvb = Uv ‚àíŒæ(Xuv)"
N MIN,0.6416510318949343,"where Uu and Uv are iid. Therefore,"
N MIN,0.6435272045028143,"Pz (‚àÜub > 0, ‚àÜvb > 0) ‚àíPz (‚àÜub > 0) Pz (‚àÜvb > 0) = Covz(1(‚àÜub > 0), 1(‚àÜvb > 0)."
N MIN,0.6454033771106942,"Conditioning on Uu and Uv, we observe that"
N MIN,0.6472795497185742,"Covz(1(‚àÜub > 0), 1(‚àÜvb > 0) =
Z Z
Covz (œïu(Œæ), œïu‚Ä≤(Œæ)) ¬µ(du)¬µ(du‚Ä≤),"
N MIN,0.649155722326454,"where œït(Œæ) = 1(t > Œæ) and ¬µ = Law(Œæ).
Because œïu and œïu‚Ä≤ are monotonous, the
Fortuin‚ÄìKasteleyn‚ÄìGinibre (FKG) inequality [20, 17] (see also [19, Section 2.2]) implies that
Cov (œïu(Œæ), œïu‚Ä≤(Œæ)) ‚â•0. Therefore,"
N MIN,0.651031894934334,"Pz (‚àÜub > 0, ‚àÜvb > 0)
Pz (‚àÜub > 0) Pz (‚àÜvb > 0) ‚â•1."
N MIN,0.6529080675422139,"Let us now derive an upper bound for this ratio. First, Lemma 4 with œµn = 0 and Œ¥n = log n implies"
N MIN,0.6547842401500938,"P (‚àÜub > 0) =
1 + o(1) t‚àóp"
N MIN,0.6566604127579737,"2œÄ log nŒ≤‚Ä≤‚Ä≤(t‚àó)
e‚àínCH(a,b),"
N MIN,0.6585365853658537,"and the same relation holds for P (‚àÜvb > 0). Moreover, conditionally on Xuv we have"
N MIN,0.6604127579737336,"Pz (‚àÜub > 0, ‚àÜvb > 0) =
Z"
N MIN,0.6622889305816135,"x‚ààX
(Œ≥ (x))2 faa(x)dx,"
N MIN,0.6641651031894934,"where Œ≥(x) = Pz (U > Œæ(x)). Since CHt(a, b) = o(1), then fac and fbc are mutually contiguous4
for all c (see [35, Theorem 25]). In particular, this implies that Œæ(x) < ‚àûfor all x ‚ààX such that"
N MIN,0.6660412757973734,"4Let (pn) and (qn) be two sequences of distributions. Then pn is contiguous with respect to qn if for all
sequence of event An such that qn(An) = o(1) we also have pn(An) = o(1)."
N MIN,0.6679174484052532,"faa(x) > 0. Thus, we can apply Lemma 4 with œµn = Œæ(x) and Œ¥n = log n, verifying Œ¥nœµn ‚Üí0 to
obtain"
N MIN,0.6697936210131332,"Œ≥(x) =
1 + o(1) t‚àóp"
N MIN,0.6716697936210131,"2œÄ log nŒ≤‚Ä≤‚Ä≤(t‚àó)
e‚àínCH(a,b),"
N MIN,0.6735459662288931,and this ends the proof.
N MIN,0.6754221388367729,"A.3
Impossibility of exact recovery"
N MIN,0.6772983114446529,"For a, b ‚àà[K] two block indexes, we recall the quantity CH(a, b) defined in (3.2) of the main text
denotes the hardness to distinguish a node in block a from a node in block b. We suppose that
I = min
aÃ∏=b CH(a, b) = Œò

log n"
N MIN,0.6791744840525328,"n

with lim
n‚Üí‚àû
n
log nI < 1. We prove the failure of the MAP estimator"
N MIN,0.6810506566604128,for exact recovery in three steps:
N MIN,0.6829268292682927,"(i) We start by showing that we can restrict the study to the node labelling vector z for which the
relative size of the communities are close to their expectations, i.e., |z‚àí1(a)|"
N MIN,0.6848030018761726,"n
= (1 + o(1))œÄa for
all a ‚àà[K];"
N MIN,0.6866791744840526,"(ii) Let Mab(z) be the number of nodes in block a for which changing their community label to
block b results in an increase of likelihood. We show that EMab(z) ‚â´1 when a, b are the two
hardest blocks to distinguish."
N MIN,0.6885553470919324,(iii) We show that Mab(z) > 0 almost surely using the second-moment method.
N MIN,0.6904315196998124,"(i) Conditioning on well-behaving community sizes.
Recall the definition of ZŒæ in (A.1). We
established in Section A.1 that P(ZŒæ) = 1 ‚àío(1) for Œæ = log log n
‚àön
. Hence,"
N MIN,0.6923076923076923,"P (MAP fails ) =
X"
N MIN,0.6941838649155723,"z‚ààZ
Pz
 
zMAP Ã∏= z

P(z) ‚â•
X"
N MIN,0.6960600375234521,"z‚ààZŒæ
Pz
 
zMAP Ã∏= z

P(z)"
N MIN,0.6979362101313321,"‚â•P (ZŒæ) inf
z‚ààZŒæ Pz
 
zMAP Ã∏= z
"
N MIN,0.699812382739212,"‚â•(1 ‚àío(1)) inf
z‚ààZŒæ Pz
 
zMAP Ã∏= z

."
N MIN,0.701688555347092,"The rest of the proof is devoted to show that Pz
 
zMAP Ã∏= z

= 1 ‚àío(1) for all z ‚ààZŒæ."
N MIN,0.7035647279549718,"(ii) Expected number of bad nodes.
Given a block structure z ‚ààZŒæ, an arbitrary node u ‚àà[n]
such that zu = a and a block b Ã∏= a. We define by Àúz the block structure obtained from z by swapping
the block of node u to b, i.e.,"
N MIN,0.7054409005628518,‚àÄv ‚àà[n]: Àúzv = (1 ‚àíŒ¥vu)zv + Œ¥vub.
N MIN,0.7073170731707317,"Suppose that X, Y are generating from a true block structure z, and let"
N MIN,0.7091932457786116,"‚àÜub(X, Y, z) = log P(Àúz | X, Y ) ‚àílog P(z | X, Y )
(A.7)"
N MIN,0.7110694183864915,"the change in the MAP estimation of z obtained by swapping the label of node u from its true block
a to a wrong block b. By the definition of Àúz, we have using Bayes‚Äô law"
N MIN,0.7129455909943715,"‚àÜub(X, Y, z) = log
 P (zu = b | X, Y, z‚àíu)"
N MIN,0.7148217636022514,"P (zu = a | X, Y, z‚àíu) "
N MIN,0.7166979362101313,"= log
 œÄbP (X, Y | z‚àíu, zu = b)"
N MIN,0.7185741088180112,"œÄaP (X, Y | z‚àíu, zu = a) 
."
N MIN,0.7204502814258912,"Moreover,"
N MIN,0.7223264540337712,"log P (X, Y | z‚àíu, zu = b) =
X"
N MIN,0.724202626641651,"1‚â§v<w‚â§n
log fzvzw(Xvw) + n
X"
N MIN,0.726078799249531,"v=1
log hzv(Yv) = n
X"
N MIN,0.7279549718574109,"v=1
vÃ∏=u"
N MIN,0.7298311444652908,log fbzv(Xuv) + log hb(Yu) + C
N MIN,0.7317073170731707,where C does not depend on b. Hence
N MIN,0.7335834896810507,"‚àÜub(X, Y, z) = log œÄb œÄa
+ n
X"
N MIN,0.7354596622889306,"v=1
vÃ∏=u"
N MIN,0.7373358348968105,log fbzv
N MIN,0.7392120075046904,"fazv
(Xuv) + log hb"
N MIN,0.7410881801125704,"ha
(Yu)."
N MIN,0.7429643527204502,"The number of nodes in block a for which updating the label to b would cause the change of likelihood
to be strictly positive is given by"
N MIN,0.7448405253283302,"Mab(z) =
X"
N MIN,0.7467166979362101,"u‚ààz‚àí1(a)
1 (‚àÜub(X, Y, z) > 0) ."
N MIN,0.7485928705440901,"In the following, we select a, b as the indexes of the two hardest blocks to distinguish. Hence,
I = CH(a, b, œÄ, f, h) and we have using Lemma 4 that"
N MIN,0.7504690431519699,"EzMab(z) ‚àºœÄane‚àí(1+o(1))nCH(a,b,ÀÜœÄ,f,g)."
N MIN,0.7523452157598499,"Since z ‚ààZŒæ we have that CH(a, b, ÀÜœÄ, f, h) = (1+o(1))I. Moreover, by assumption I < (1‚àíœµ) log n"
N MIN,0.7542213883677298,"n
for some œµ > 0 and for any n large enough. Hence EzMab(z) ‚â´1."
N MIN,0.7560975609756098,"(iii) Conclusion.
We will conclude that Mab(z) > 0 almost surely using the second-moment
method. Denote by Œ± = Pz(‚àÜub > 0) and Œ≤ = P (‚àÜub > 0, ‚àÜvb > 0) for two arbitrary distinct
nodes u, v ‚ààz‚àí1(a). We have EzMab =
z‚àí1(k)
 Œ± and"
N MIN,0.7579737335834896,"Varz Mab =
X"
N MIN,0.7598499061913696,"u,v‚ààz‚àí1(a)
Cov (1(‚àÜu‚Ñì> 0), 1(‚àÜv‚Ñì> 0))"
N MIN,0.7617260787992496,"=
z‚àí1(a)
  
Œ± ‚àíŒ±2
+
z‚àí1(a)
  z‚àí1(a)
 ‚àí1
  
Œ≤ ‚àíŒ±2"
N MIN,0.7636022514071295,"‚â§EzMab + (EzMab)2 Œ≤ ‚àíŒ±2 Œ±2
."
N MIN,0.7654784240150094,"Hence, the second-moment method implies that"
N MIN,0.7673545966228893,Pz(Mab = 0) ‚â§Varz Mab
N MIN,0.7692307692307693,"(EzMab)2 ‚â§
1
EzMab
+ Œ≤"
N MIN,0.7711069418386491,Œ±2 ‚àí1.
N MIN,0.7729831144465291,We end the proof using Lemma 5.
N MIN,0.774859287054409,"A.4
Possibility of exact recovery"
N MIN,0.776735459662289,"To show that the MAP estimator achieves exact recovery up to the desired threshold, we need to show
that there is no possibility of reducing the likelihood by swapping m vertices from each community.
In all the following, we recall that z denotes the true community labelling, and for any z‚Ä≤ ‚àà[K] we
denote by L(z‚Ä≤) = P(X, Y | z‚Ä≤) the likelihood of labelling z‚Ä≤. For any m ‚â•1, let us denote the set of
node-labelling at a distance m of z by"
N MIN,0.7786116322701688,"Œìm = {z‚Ä≤ ‚ààZ : loss(z, z‚Ä≤) = m} .
(A.8)"
N MIN,0.7804878048780488,The probability that there exists a labelling z‚Ä≤ ‚ààŒìm with higher likelihood than z is
N MIN,0.7823639774859287,Pm = Pz (‚àÉz‚Ä≤ ‚ààŒìm : L(z‚Ä≤) ‚â•L(z)) .
N MIN,0.7842401500938087,Let ÀÜz be the block labelling estimated by the MAP. Lemma 7 ensures that |Œìm| = 0 if m ‚â•n K‚àí1
N MIN,0.7861163227016885,"K .
Thus, we have, using union bounds,"
N MIN,0.7879924953095685,"Pz (loss (z, ÀÜz) ‚â•1) ‚â§ 2K‚àí1"
"K
X",0.7898686679174484,"2K
X"
"K
X",0.7917448405253283,"m=1
|Œìm| max
z‚Ä≤‚ààŒìm Pz (L(z‚Ä≤) ‚â•L(z)) ."
"K
X",0.7936210131332082,"Moreover, [39, Proposition 5.2] show that"
"K
X",0.7954971857410882,"|Œìm| ‚â§min
enK m"
"K
X",0.797373358348968,"m
, Kn

,"
"K
X",0.799249530956848,"while Lemma 3 provides an upper bounds on max
z‚Ä≤‚ààŒìm Pz (L(z‚Ä≤) ‚â•L(z)). Combining these upper"
"K
X",0.801125703564728,"bounds ensure that for some sequence Œ∑ = o(1), we have"
"K
X",0.8030018761726079,"Pz (loss(z, ÀÜz) ‚â•1) ‚â§"
"K
X",0.8048780487804879,n 2K‚àí1
"K
X",0.8067542213883677,"2K
X"
"K
X",0.8086303939962477,"m=1
(sm)m,
(A.9)"
"K
X",0.8105065666041276,"where sm
=
enK"
"K
X",0.8123827392120075,"m min

e
‚àí(1‚àíŒ∑)

1‚àí
m
nœÄmin"
"K
X",0.8142589118198874,"
nI; e‚àí(n‚àím)d

with d
=
sup
t‚àà(0,1)
min
aÃ∏=b‚àà[K]
c‚àà[K]
(1 ‚àí"
"K
X",0.8161350844277674,t) Dt(fac‚à•fbc).
"K
X",0.8180112570356473,Let œµ > 0 such that I > (1 + œµ) log n
"K
X",0.8198874296435272,"n . We will now show that the sum on the right-hand side of
Equation (A.9) goes to zero, by considering the cases (i) m ‚â§œÄmin"
"K
X",0.8217636022514071,2 n and (ii) œÄmin
"K
X",0.8236397748592871,2 n ‚â§m ‚â§1 2n.
"K
X",0.8255159474671669,"(i) First of all, suppose that m ‚â§œÄmin"
"K
X",0.8273921200750469,"2 n. Then,"
"K
X",0.8292682926829268,"sm ‚â§
eK
nœµ(1‚àíŒ∑)+Œ∑ e(1‚àíŒ∑)(1+œµ)
m
œÄmin
log n"
"K
X",0.8311444652908068,"n
‚àílog m"
"K
X",0.8330206378986866,"‚â§
eK
nœµ/2 e(1+œµ)
m
œÄmin
log n"
"K
X",0.8348968105065666,"n
‚àílog m."
"K
X",0.8367729831144465,Let f(m) = log m
"K
X",0.8386491557223265,"m
‚àí1+œµ"
"K
X",0.8405253283302064,"œÄmin
log n"
"K
X",0.8424015009380863,n . We will show that f(m) ‚â•log m
"K
X",0.8442776735459663,"2m for m ‚â•3. Indeed, for x ‚â•3, the
function x 7‚Üílog x"
"K
X",0.8461538461538461,"x
is decreasing. Therefore, for 3 ‚â§m ‚â§œÄmin"
N WE HAVE,0.8480300187617261,2 n we have
N WE HAVE,0.849906191369606,f(m) ‚àílog m
M,0.851782363977486,"2m
‚â•1"
LOG M,0.8536585365853658,"2
log m"
LOG M,0.8555347091932458,"m
‚àí1 + œµ œÄmin log n"
LOG M,0.8574108818011257,"n
‚â•
2
œÄmin"
LOG M,0.8592870544090057,log n ‚àílog(œÄmin/2)
LOG M,0.8611632270168855,"n
‚àí1 + œµ œÄmin log n n
> 0"
LOG M,0.8630393996247655,for n large enough. Hence sm ‚â§eKn‚àíœµ/2e‚àí1
LOG M,0.8649155722326454,2 log m and P œÄmin
N,0.8667917448405253,"2
n
m=1
sm
m = o(1)."
N,0.8686679174484052,"(ii) Next, suppose that 1"
N,0.8705440900562852,2œÄminn ‚â§m ‚â§n K‚àí1
N,0.8724202626641651,"K . Then, the assumption Dt(fac‚à•fbc) = Œò

log n n
"
N,0.874296435272045,implies that d > Œ∫ log n
N,0.8761726078799249,"n
for some positive constant Œ∫. Hence,"
N,0.8780487804878049,sm ‚â§2eK
N,0.8799249530956847,"œÄmin
e‚àíŒ∫"
N,0.8818011257035647,"K log n,"
N,0.8836772983114447,"and thus K‚àí1 K
n
P"
N,0.8855534709193246,m= œÄmin
"N
SM",0.8874296435272045,"2
n
sm
m = o(1)."
"N
SM",0.8893058161350844,"Hence, (A.9) shows that Pz (loss(z, ÀÜz) ‚â•1) = o(1), and thus the MAP estimator exactly recovers
the true community structure z."
"N
SM",0.8911819887429644,"A.5
Additional lemmas"
"N
SM",0.8930581613508443,"Lemma 6. Let z, z‚Ä≤ ‚àà[K]n and define for all a, b, c, d ‚àà[K]:"
"N
SM",0.8949343339587242,"Aab = |{i ‚àà[n]: (zi, z‚Ä≤
i) = (a, b)}| ,"
"N
SM",0.8968105065666041,"M(ab)(cd) =

1 ‚â§i < j ‚â§n: (zi, z‚Ä≤
i) = (a, b) and (zj, z‚Ä≤
j) = (c, d)
	 ."
"N
SM",0.8986866791744841,We have
"N
SM",0.900562851782364,"M(ab)(cd) = Aab
 
Acd ‚àí1(ab)=(cd)

= Acd
 
Aab ‚àí1(ab)=(cd)

."
"N
SM",0.9024390243902439,"Proof. Denote by Ca = z‚àí1(a) and C‚Ä≤
a = z‚Ä≤‚àí1(a), and let Cab = Ca ‚à©C‚Ä≤
b. Then, Aab = |Cab| and
M(ab)(cd) = | {1 ‚â§i < j ‚â§n: i ‚ààCab and j ‚ààCcd} |. This proves the lemma."
"N
SM",0.9043151969981238,"Lemma 7. For any z, z‚Ä≤ ‚àà[K]n, we have loss(z, z‚Ä≤) ‚â§n(1 ‚àí1 K )."
"N
SM",0.9061913696060038,"Proof. Without loss of generality, suppose that loss(z, z) = Ham(z, z‚Ä≤) (that is, the optimal per-
mutation in the definition of the loss is simply the identity). For any a ‚àà[K], let us denote by
Ca = z‚àí1(a), C‚Ä≤
a = z‚àí1(b). The confusion matrix of the two node labellings is the K-by-K matrix
having entries Nab = |Ca ‚à©Cb|. In particular, we have n = P"
"N
SM",0.9080675422138836,"a,b Nab and"
"N
SM",0.9099437148217636,"loss(z, z‚Ä≤) = K
X a=1 K
X"
"N
SM",0.9118198874296435,"b=1
bÃ∏=a Nab"
"N
SM",0.9136960600375235,"= n ‚àí
X"
"N
SM",0.9155722326454033,"a
Naa."
"N
SM",0.9174484052532833,"[5, Lemma B.2] show that Nab ‚â§Naa + Nbb ‚àíNba, and therefore"
"N
SM",0.9193245778611632,"loss(z, z‚Ä≤) ‚â§ K
X a=1 K
X"
"N
SM",0.9212007504690432,"b=1
bÃ∏=a"
"N
SM",0.9230769230769231,"(Naa + Nbb ‚àíNba) .
(A.10)"
"N
SM",0.924953095684803,"We notice that
X a X"
"N
SM",0.926829268292683,"bÃ∏=a
Naa = (K ‚àí1)
X"
"N
SM",0.9287054409005628,"a
Naa = (K ‚àí1)(n ‚àíloss(z, z‚Ä≤)),"
"N
SM",0.9305816135084428,"and similarly, X a X"
"N
SM",0.9324577861163227,"bÃ∏=a
Nbb =
X a X"
"N
SM",0.9343339587242027,"b
Nbb ‚àíNaa !"
"N
SM",0.9362101313320825,"= (K ‚àí1)(n ‚àíloss(z, z‚Ä≤))."
"N
SM",0.9380863039399625,"Finally, P a
P"
"N
SM",0.9399624765478424,"bÃ∏=a Nba = loss(z, z‚Ä≤). Thus, going back to (A.10) leads to"
"N
SM",0.9418386491557224,"loss(z, z‚Ä≤) ‚â§2(K ‚àí1)n ‚àí(2K ‚àí1)loss(z, z‚Ä≤),"
"N
SM",0.9437148217636022,and this ends the proof.
"N
SM",0.9455909943714822,"Lemma 8. Let f, g be two probability distributions and denote c(t) = (1 ‚àít) Dt(f‚à•g) the Chernoff
coefficient of order t between f and g. We have c‚Ä≤(0) = ‚àídKL(g‚à•f) and c‚Ä≤(1) = dKL(f‚à•g)."
"N
SM",0.9474671669793621,"Proof. From c(t) = log
R
f tg1‚àít, we notice that c‚Ä≤(t) ="
"N
SM",0.949343339587242,"R
f tg1‚àít log f"
"N
SM",0.9512195121951219,"g
R
f tg1‚àít
, and the result follows."
"N
SM",0.9530956848030019,"B
Proof of Lemma 1"
"N
SM",0.9549718574108818,"Proof of Lemma 1. Using a Taylor expansion, we have"
"N
SM",0.9568480300187617,"(1 ‚àít) Dt(fac‚à•fbc) = log

(1 ‚àípac)t(1 ‚àípbc)1‚àít + pt
acp1‚àít
bc"
"N
SM",0.9587242026266416,"Z
( Àúfab)t( Àúfbc)1‚àít
"
"N
SM",0.9606003752345216,"= tp + (1 ‚àít)q ‚àíptq1‚àít
Z
( Àúfac)t( Àúfbc)1‚àít + o(Œ¥),"
"N
SM",0.9624765478424016,"and we finish the proof using
R
( Àúfac)t( Àúfbc)1‚àít = e‚àíJœà(Œ∏ab‚à•Œ∏bc) and (1 ‚àít) Dt(ha‚à•hb) = Jœï(Œ∑a‚à•Œ∑b)
(see for example [28])."
"N
SM",0.9643527204502814,"C
Additional numerical results"
"N
SM",0.9662288930581614,"We present in Table 2 and 3 the numerical results that are drawn in Figure (4a) and (4b), respectively.
We observe that the variance of Algorithm 1 is very low, while all other algorithms (excepted
EM-GMM) exhibit a very large variance.
Table 2: Average ARI of the different algorithms over 60 trials with standard deviation in brackets
obtained for Figure (4a)."
"N
SM",0.9681050656660413,"PARAMETER a
5
7
9
11
13
15"
"N
SM",0.9699812382739212,"ALGORITHM 1
0.40 (0.15)
0.62 (0.04)
0.93 (0.02)
1 (0)
1 (0)
1 (0)
EM-GMM
0.45 (0.04)
0.46 (0.03)
0.46 (0.04)
0.46 (0.04)
0.46 (0.04)
0.45 (0.07)
SC
0 (0)
0.07 (0.05)
0.83 (0.15)
0.85 (0.33)
0.93 (0.24)
0.82 (0.37)
IR_LS
0 (0)
0.20 (0.13)
0.93 (0.14)
1 (0)
0.98 (0.12)
1 (0)
attSBM
0 (0)
0.07 (0.05)
0.82 (0.15)
0.86 (0.33)
0.42 (0.48)
0.59 (0.49)"
"N
SM",0.9718574108818011,"Table 3: Average ARI of the different algorithms over 60 trials with standard deviation in brackets
obtained for Figure (4b)."
"N
SM",0.9737335834896811,"PARAMETER r
0
1
2
3
4
5"
"N
SM",0.975609756097561,"ALGORITHM 1
0.47 (0.29)
0.82 (0.03)
0.97 (0.02)
1 (0)
1 (0)
1 (0)
EM-GMM
0 (0)
0.46 (0.04)
0.9 (0.02)
1 (0)
1 (0)
1 (0)
SC
0.36 (0.28)
0.38 (0.27)
0.35 (0.26)
0.40 (0.26)
0.37 (0.28)
0.35 (0.26)
IR_LS
0.42 (0.29)
0.65 (0.31)
0.91 (0.21)
0.97 (0.15)
0.97 (0.14)
0.98 (0.12)
attSBM
0.36 (0.28)
0.38 (0.27)
0.38 (0.30)
0.59 (0.43)
0.57 (0.46)
0.50 (0.43)"
"N
SM",0.9774859287054409,"We also provide in Figure 4 comparison of Algorithm 1 with IR_sLs and attSBM on weighted
networks with attributes exponentially distributed. We observe that contrary to Algorithm 1, IR_sLs
and attSBM do not perform well in that setting (which is expected as they are designed for binary
network and Gaussian attributes)."
"N
SM",0.9793621013133208,"(a) Varying Œª‚àí1
w , with Œªa = 1/2.
(b) Varying Œª‚àí1
a , with Œªw = 1/2."
"N
SM",0.9812382739212008,"Figure 4: Performance on weighted networks (n = 600, K = 2) with edge-weight distributions
fin = (1 ‚àíp)Œ¥0 + p Exp(Œªw), fout = (1 ‚àíp)Œ¥0 + p Exp(1) with p = 5n‚àí1 log n, and exponentially
distributed attributes h1 ‚àºExp(1) and h2 = Exp(Œªa). Results are averaged over 20 runs."
"N
SM",0.9831144465290806,"C.1
Robustness to the choice of dœà‚àóand dœï‚àó"
"N
SM",0.9849906191369606,"We show in Figure 5 that using a divergence (distribution) for edge weights (Figure 5a and node
attributes (Figure 5b different from the distribution used to generate the data does not impact the
performance of Algorithm 1. We note that a similar observation was done in previous papers using
Bregman divergence for clustering [6, 23]."
"N
SM",0.9868667917448405,"5
7
9
11
13
15
muin 0.2 0.4 0.6 0.8 1.0 ARI"
"N
SM",0.9887429643527205,"euclidean
manhattan
Itakura-Saito
poisson"
"N
SM",0.9906191369606003,(a) Various dœà‚àó. Poisson is the correct model.
"N
SM",0.9924953095684803,"3
5
7
9
11
nu1 0.0 0.2 0.4 0.6 0.8 1.0 ARI"
"N
SM",0.9943714821763602,"euclidean
manhattan
Itakura-Saito
poisson"
"N
SM",0.9962476547842402,(b) Various dœï‚àó. Poisson is the correct model.
"N
SM",0.99812382739212,"Figure 5: Performance of Algorithm 1 when dœà‚àóor dœï‚àódo not correspond to the model that generated
the data. The different curves show the Adjusted Rand Index (ARI) [21] averaged over 20 realisations
with the standard deviations as error bars.
(a) n = 400, K = 4, fin = (1 ‚àíp)Œ¥0(x) + pPoi(¬µin) and fout = (1 ‚àíq)Œ¥0(x) + qPoi(5), with
p = 0.04 and q = 0.01. Attributes are 2d-Gaussians with unit variances and mean equally spaced the
circle of radius r = 2.
(b) n = 400, K = 2, fin = (1 ‚àíp)Œ¥0(x) + pNor(2, 1) and fout = (1 ‚àíq)Œ¥0(x) + qNor(0, 1), with
p = 0.04 and q = 0.01. Attributes are Poisson with means ŒΩ1 (for nodes in cluster 1) and 3 (for
nodes in cluster 2)."
