Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033783783783783786,"Graph convolution operator of the GCN model is originally motivated from a
localized first-order approximation of spectral graph convolutions. This work
stands on a different view; establishing a mathematical connection between graph
convolution and graph-regularized PCA (GPCA). Based on this connection, the
GCN architecture, shaped by stacking graph convolution layers, shares a close rela-
tionship with stacking GPCA. We empirically demonstrate that the unsupervised
embeddings by GPCA paired with a 1- or 2-layer MLP achieves similar or even
better performance than many sophisticated baselines on semi-supervised node
classification tasks across five datasets including Open Graph Benchmark. This
suggests that the prowess of graph convolution is driven by graph based regular-
ization. In addition, we extend GPCA to the (semi-)supervised setting and show
that it is equivalent to GPCA on a graph extended with “ghost” edges between
nodes of the same label. Finally, we capitalize on the discovered relationship to
design an effective initialization strategy based on stacking GPCA, enabling GCN
to converge faster and achieve robust performance at large number of layers."
INTRODUCTION,0.006756756756756757,"1
INTRODUCTION"
INTRODUCTION,0.010135135135135136,"Graph neural networks (GNNs) are neural networks designed for the graph domain. Since the
breakthrough of GCN (Kipf & Welling, 2017), which notably improved performance on the semi-
supervised node classification problem, many GNN variants have been proposed; including GAT
(Veliˇckovi´c et al., 2018), GraphSAGE (Hamilton et al., 2017), DGI (Veliˇckovi´c et al., 2019), GIN
(Xu et al., 2019), PPNP and APPNP (Klicpera et al., 2019), to name a few."
INTRODUCTION,0.013513513513513514,"Despite the empirical successes of GNNs in both node-level and graph-level tasks, they remain not
well understood due to limited systematic and theoretical analysis of GNNs. For example, researchers
have found that GNNs, unlike their non-graph counterparts, suffer from performance degradation
with increasing depth, their expressive power decaying exponentially in number of layers (Oono &
Suzuki, 2020). Such behavior is only partially explained by the oversmoothing phenomenon (Li
et al., 2018; Zhao & Akoglu, 2020). Another surprising observation shows that a Simplified Graph
Convolution model, named SGC (Wu et al., 2019), can achieve similar performance to various more
complex GNNs on a variety of node classification tasks. Moreover, a simple baseline that does not
utilize the graph structure altogether performs similar to state-of-the-art GNNs on graph classification
tasks (Errica et al., 2020). These observations call attention to studies for a better understanding of
GNNs (NT & Maehara, 2019; Morris et al., 2019; Xu et al., 2019; Oono & Suzuki, 2020; Loukas,
2020; Srinivasan & Ribeiro, 2020). (See Sec. 2 for more on understanding GNNs.)"
INTRODUCTION,0.016891891891891893,"Toward a systematic analysis and better understanding of GNNs, we establish a connection between
the graph convolution operator of GCN (and PPNP) and Graph-regularized PCA (GPCA) (Zhang &
Zhao, 2012), and show the similarity between GCN and stacking GPCA. This connection provides
a deeper understanding of GCN’s power and limitation. Empirically, we also find that GPCA
performance matches that of many GNN baselines on benchmark semi-supervised node classification
tasks. We argue that the simple GPCA should be a strong baseline in future. What is more, the
unsupervised stacking GPCA can be viewed as “unsupervised GCN” and provides a straightforward,
yet systematic way to initialize GCN training. We summarize our contributions as follows:"
INTRODUCTION,0.02027027027027027,"• Connection between Graph Convolution and GPCA: We establish the connection between the
graph convolution operator of GCN (also PPNP) and the closed-form solution of graph-regularized
PCA (GPCA) formulation. We demonstrate that a simple graph-regularized PCA paired with 1-
or 2-layer MLP can achieve similar or even better results than state-of-the-art GNN baselines over"
INTRODUCTION,0.02364864864864865,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02702702702702703,"several benchmark datasets. We further extend GPCA to (semi-)supervised setting which can generate
embeddings using information of labels, which yields better performance on 3 out of 5 datasets. The
outstanding performance of simple GPCA supports that the prowess of GCN on node classification
task comes from graph based regularization. This motivates the study and design of other graph
regularization techniques in the future."
INTRODUCTION,0.030405405405405407,"• GPCANET: New Stacking GPCA model: Capitalizing on the connection between GPCA and
graph convolution, we design a new GNN model called GPCANET shaped by (1) stacking multiple
GPCA layers and nonlinear transformations, and (2) fine-tuning end-to-end via supervised training.
GPCANET is a generalized GCN model with adjustable hyperparameters that control the strength of
graph regularization of each layer. We show that with stronger regularization, we can train GPCANET
with fewer (1–3) layers and achieve comparable performance to much deeper GCNs."
INTRODUCTION,0.033783783783783786,"• First initialization strategy for GNNs: Capitalizing on the connection between GCN and GP-
CANET, we design a new strategy to initialize GCN training based on stacking GPCA, outperforming
the popular Xaiver initialization (Glorot & Bengio, 2010). We show that the GPCANET-initialization
is extremely effective for training deeper GCNs, that significantly improves the convergence speed,
performance, and robustness. Notably, GPCANET-initialization is general-purpose and also applies
to other GNNs. To our knowledge, it is the first initialization method specifically designed for GNNs."
INTRODUCTION,0.037162162162162164,We open-source code at http://bit.ly/GPCANet. All datasets are public-domain.
RELATED WORK,0.04054054054054054,"2
RELATED WORK"
RELATED WORK,0.04391891891891892,"Understanding GNNs. Our work concerns learning on a single graph, hence we limit discussion
of related work to node-level GNNs. GCN’s graph convolution is originally motivated from the
approximation of graph filters in graph signal processing (Kipf & Welling, 2017). NT & Maehara
(2019) show that graph convolution only performs low-pass filtering on original feature vectors, and
also state a connection between graph filtering and Laplacian regularized least squares. Motivated by
the oversmoothing phenomenon of graph convolution, Oono & Suzuki (2020) theoretically prove that
GCN can only preserve information of node degrees and connected components when the number
of layers goes to infinity, under some conditions of GCN weights. Recently several papers revisited
the connection of graph convolution to graph-regularized optimization problem (Li et al., 2019; Ma
et al., 2020; Pan et al., 2021; Zhao & Akoglu, 2020; Zhu et al., 2021), which is originally discussed
in graph signal processing (Shuman et al., 2013). More specifically, both Ma et al. (2020) and
Zhu et al. (2021) relate graph-regularization optimization to several GNNs such as GCN (Kipf &
Welling, 2017), APPNP (Klicpera et al., 2019), and GAT (Veliˇckovi´c et al., 2018). However, all
previous work study these connections while ignoring the learnable parameters, which are essential
for high-performance deep learning. Our work differs from these by establishing a stronger and closer
connection to graph-regularized PCA that also takes learnable parameters into account."
RELATED WORK,0.0472972972972973,"Graph-regularized PCA. PCA and its variants are standard linear dimensionality reduction ap-
proaches. Several work extend PCA to graph-structured data, such as Graph-Laplacian PCA (Jiang
et al., 2013) and Manifold-regularized Matrix Factorization (Zhang & Zhao, 2012). For other variants,
see Shahid et al. (2016)."
RELATED WORK,0.05067567567567568,"Stacking Models and Deep Learning. The connection between CNN and stacking PCA has been
explored in PCANet (Chan et al., 2015), which demonstrated that the (unsupervised) simple stacking
PCA works as well as supervised CNN over a large variety of vision tasks. The original PCANet is
shallow and does not have nonlinear transformations, while PCANet+ (Low et al., 2017) overcomes
these limitations and pushes the architecture much deeper. The idea of layerwise stacking for feature
extraction is not new and was empirically observed to exhibit better representation ability in terms of
classification. For a comprehensive review, we refer to Bengio et al. (2013)."
RELATED WORK,0.05405405405405406,"Initialization. Traditionally, neural networks (NNs) were initialized with random weights generated
from Gaussian distribution with zero mean and a small standard deviation (Krizhevsky et al., 2012).
As training deeper NNs became extremely difficult due to vanishing gradient and activation functions,
Glorot & Bengio (2010) provided a specific weight initialization formula, named Xavier initialization,
based on variance analysis without considering activation function. Xavier initialization is widely used
for any type of NN even today, and it is the main initialization strategy used for GNNs. Later, He et al.
(2015) adapted Xavier initialization to ReLU activation by considering a multiplier. Taking another"
RELATED WORK,0.057432432432432436,Under review as a conference paper at ICLR 2022
RELATED WORK,0.060810810810810814,"direction, Saxe et al. (2013) analyzed the dynamics of training deep NNs and proposed random
orthonormal initialization. Mishkin & Matas (2015) further improved orthonormal initialization for
batch normalization (Ioffe & Szegedy, 2015). Different from these data-independent approaches,
others (Kr¨ahenb¨uhl et al., 2016; Seuret et al., 2017; Wagner et al., 2013) have employed data-
dependent techniques, like PCA, to initialize deep NNs. Although initialization has been widely
studied for general NNs, no specific initialization has been proposed for GNNs. In this work, we
propose a data-driven initialization technique (based on GPCA), specific to GNNs for the first time."
GRAPH CONVOLUTION AND GPCA,0.06418918918918919,"3
GRAPH CONVOLUTION AND GPCA"
GRAPH CONVOLUTION,0.06756756756756757,"3.1
GRAPH CONVOLUTION"
GRAPH CONVOLUTION,0.07094594594594594,"Consider a node-attributed input graph G = (V, E, X) with |V | = n nodes and |E| = m edges,
where X ∈Rn×d denotes the node feature matrix with d features. Broadly, graph convolution
operation convolves the features (or representations) over the graph structure."
GRAPH CONVOLUTION,0.07432432432432433,"GCN. Similar to other neural networks stacked with repeated layers, GCN contains multiple graph
convolution layers each of which is followed by a nonlinear activation. Let H(l) be the l-th hidden
layer representation, then, each GCN layer performs
H(l+1) = σ( ˜AsymH(l)W (l))
(1)"
GRAPH CONVOLUTION,0.0777027027027027,where ˜Asym = ˜D−1
GRAPH CONVOLUTION,0.08108108108108109,2 (A+I) ˜D−1
GRAPH CONVOLUTION,0.08445945945945946,"2 denotes the n×n symmetrically normalized adjacency matrix with
self-loops, ˜D is the diagonal degree matrix where ˜Dii = 1 + Pn
j=1 Aij, W (l) depicts the l-th layer
parameters (to be learned), and σ is the nonlinear activation function. Formally, graph convolution is
parameterized with W and maps an input X to a new representation Z as
Z = ˜AsymXW .
(2)"
GRAPH CONVOLUTION,0.08783783783783784,"PPNP. For PPNP (Klicpera et al., 2019), the features are first transformed by an MLP before
convolving over the graph. Formally, the operation is revised as"
GRAPH CONVOLUTION,0.09121621621621621,"Z = µ

I −(1 −µ) ˜Asym
−1
MLPW (X) =

I + α˜L
−1
MLPW (X)
(3)"
GRAPH CONVOLUTION,0.0945945945945946,"where we replace µ with α = (1 −µ)/µ, ˜L := I −˜Asym denotes the normalized graph Laplacian,
and W depicts the learnable MLP parameters. As matrix inverse is expensive, an approximate version
called APPNP that employs the power method (Golub & Van Loan, 1989) is often used in practice."
GRAPH CONVOLUTION,0.09797297297297297,"3.2
GRAPH-REGULARIZED PCA (GPCA)"
GRAPH CONVOLUTION,0.10135135135135136,"As stated by Bengio et al. (2013), “Although depth is an important part of the story, many other
priors are interesting and can be conveniently captured when the problem is cast as one of learning a
representation.” GPCA is one such representation learning technique with a graph-based prior."
GRAPH CONVOLUTION,0.10472972972972973,"Standard PCA learns k-dimensional projections Z ∈Rn×k of feature matrix X ∈Rn×d, aiming to
minimize the reconstruction error
∥X −ZW T ∥2
F ,
(4)
subject to W ∈Rd×k being an orthonormal basis. GPCA extends this formalism to graph-structured
data by additionally assuming either smoothing bases (Jiang et al., 2013) or smoothing projections
(Zhang & Zhao, 2012) over the graph. In this work we consider the latter case where low-dimensional
projections are smooth over the input graph G, where ˜L = I −˜Asym denotes its normalized Laplacian
matrix. The objective formulation of GPCA is then given as
min
Z,W
∥X −ZW T ∥2
F + α Tr(ZT ˜LZ)
s.t.
W T W = I
(5)"
GRAPH CONVOLUTION,0.10810810810810811,"where α is a hyperparameter that balances reconstruction error and the variation of the projections
over the graph. Note that the first part of Eq. equation 5, along with the constraint, corresponds
to the objective of the original PCA, while the second part is a graph regularization term that aims
to “smooth” the learned representations Z over the graph structure. As such, GPCA becomes the
standard PCA when α = 0."
GRAPH CONVOLUTION,0.11148648648648649,"Similar to PCA, the problem (5) is non-convex but has a closed-form solution (Zhang & Zhao, 2012).
Surprisingly, as we show, it has a close connection with the graph convolution formulation in Eq.
equation 2. In the following, we give the GPCA solution and then detail its connection to graph
convolution."
GRAPH CONVOLUTION,0.11486486486486487,Under review as a conference paper at ICLR 2022
GRAPH CONVOLUTION,0.11824324324324324,"Theorem 3.1. GPCA with formulation shown in (5) has the optimal solution (Z∗, W ∗) following
Z∗= (I + α˜L)−1XW ∗, and
W ∗= (w1, w2, ..., wk)
(6)"
GRAPH CONVOLUTION,0.12162162162162163,"where w1, ..., wk are the eigenvectors of XT (I+α˜L)−1X corresponding to the largest k eigenvalues."
GRAPH CONVOLUTION,0.125,Proof. The proof can be found in Appendix. A.1.
CONNECTION BETWEEN GCN AND GPCA,0.12837837837837837,"3.3
CONNECTION BETWEEN GCN AND GPCA"
CONNECTION BETWEEN GCN AND GPCA,0.13175675675675674,"Let Φα := I + α˜L. The normalized Laplacian matrix ˜L has absolute eigenvalues bounded by 1, thus,
all its positive powers have bounded operator norm. When α ≤1, Φ−1
α
can be decomposed into
Taylor series as (I + α˜L)−1 = I −α˜L + . . . + (−α)t ˜Lt + . . .. The first-order truncated form (i.e.
approximation) of the series is
(I + α˜L)−1 ≈I −α˜L = (1 −α)I + α ˜Asym .
(7)
When α = 1, the first-order approximation of Z∗in Theorem 3.1 follows
Z∗≈˜AsymXW ∗.
(8)
The (approximate) solution to GPCA in Eq. equation 8 matches the form of graph convolution
operation in Eq. equation 2, with W ∗plugged in as the eigenvectors of the matrix XT Φ−1
α X. In
other words, there exists some parameter W ∗with which GCN becomes the first-order approximation
of GPCA."
CONNECTION BETWEEN GCN AND GPCA,0.13513513513513514,"To reiterate, a key contribution of this work is to show that the graph convolution operation in GCN
can be viewed as the first-order approximation of GPCA with α = 1 with a learnable W. Put
differently, the first-order approximation of (unsupervised) GPCA with α = 1 can be viewed as a
graph convolution with a fixed, data-driven W. In other words, Notably, for α < 1, Eq. equation 7
shows the connection between GPCA and graph convolution equipped with 1-step (scaled) residual
connection."
CONNECTION BETWEEN PPNP AND GPCA,0.13851351351351351,"3.4
CONNECTION BETWEEN PPNP AND GPCA"
CONNECTION BETWEEN PPNP AND GPCA,0.14189189189189189,"Replacing the MLP in Eq. equation 3 with a single linear layer without activation results in Z =

I + α˜L
−1
XW, which has exactly the same formulation as the solution Z∗in Theorem 3.1 Eq.
equation 6. The connection states that the graph convolution in PPNP can be viewed as the GPCA
solution with a learnable W. Interestingly, the empirical performance improvement of PPNP over
GCN (see Table 2 in Klicpera et al. (2019)) may be explained through these connections that they
have to GPCA; where PPNP relates to the exact solution of GPCA while GCN is related to its
(first-order) approximation."
SUPERVISED GPCA,0.14527027027027026,"3.5
SUPERVISED GPCA"
SUPERVISED GPCA,0.14864864864864866,"The standard GPCA problem in (5) is unsupervised. Motiviated from LDA (Balakrishnama &
Ganapathiraju, 1998) and PLS (Geladi & Kowalski, 1986), in this section we show how to extend it
to the supervised setting, by learning embeddings that not only (1) provide good reconstruction and
(2) vary smoothly over the graph structure, but also (3) highly correlate with the response variable(s).
For simplicity of presentation, let z ∈Rd be a 1-d embedding and Y denote the response matrix (in
the general case of multiple responses). We write the additional, i.e. (3)rd objective above, as1"
SUPERVISED GPCA,0.15202702702702703,"max
z

corr(Y, z)
T 
corr(Y, z)

var(z)
≡
max
z
zT Y Y T z
(9)"
SUPERVISED GPCA,0.1554054054054054,"The form of equation 9 (See Appendix. A.3) and the variance-maximizing term var(z) are for
mathematical convenience. Despite agnostic to labels, including var(z) is intuitive since an implicit
objective of data projection (embedding) is to ensure that inherent variation in data is captured as much
as possible. In general, we would aim to maximize the trace of ZT Y Y T Z for multi-dimensional
embeddings."
SUPERVISED GPCA,0.15878378378378377,"Interpretation. For semi-supervised node classification with c classes, let L ⊂V denote the set
of labeled nodes. For this task, Y ∈{0, 1}n×c would encode the node labels where the v-th row
of Y , denoted Yv, depicts the one-hot encoded label for each v ∈L. For u ∈V \L with unknown
labels, Yu = 0, set as the c-dimensional all-zero vector. Then, (Y Y T )ij is simply equal to 1 when"
SUPERVISED GPCA,0.16216216216216217,"1For the optimization to be well-posed, constraints on z are required, omitted for simplicity of presentation."
SUPERVISED GPCA,0.16554054054054054,Under review as a conference paper at ICLR 2022
SUPERVISED GPCA,0.16891891891891891,"nodes i and j share the same label, and otherwise 0 (either because they have different labels or
labels are unknown). This term simply enforces the representations Zi and Zj of two same-labeled
nodes to be similar. In a sense, Y Y T adds “ghost” edges between the same-label nodes, further
guiding the smoothness of their representations over this extended graph structure. We remark that
earlier work (Gallagher et al., 2008) has heuristically introduced edges between same-label nodes to
enhance a given graph for the node classification task. In this work, we have derived the theoretical
underpinning for this strategy."
SUPERVISED GPCA,0.17229729729729729,"Supervised formulation. We have shown that requiring the embeddings to correlate with the known
labels can be interpreted as additional smoothing over “ghost” edges between the same-label nodes in
the graph. As such, we extend the GPCA problem in (5) to the (semi-)supervised setting as
min
Z,W
∥X −ZW T ∥2
F + α Tr(ZT ˜LsprZ)
s.t.
W T W = I
;
(10)"
SUPERVISED GPCA,0.17567567567567569,"where ˜Lspr = I −˜Aspr ,
˜Aspr = (1 −β) ˜Asym + βD−1"
SUPERVISED GPCA,0.17905405405405406,2 (Y Y T )D−1
SUPERVISED GPCA,0.18243243243243243,"2
(11)
In Eq. equation 11, β is an additional hyperparameter for trading-off the graph-based regularization
(i.e. smoothing) due to the actual input graph edges versus the ones introduced through Y Y T between
the nodes of the same label, and D is the diagonal matrix with Dii = Pn
j=1(Y Y T )ij."
SUPERVISED GPCA,0.1858108108108108,"Theorem 3.2. Supervised GPCA, as shown in (10) has the optimal solution (Z∗, W ∗) following
Z∗= (I + α˜Lspr)−1XW ∗, and
W ∗= (w1, w2, ..., wk)
(12)"
SUPERVISED GPCA,0.1891891891891892,"where w1, . . . , wk are the top eigenvectors of the matrix XT (I + α˜Lspr)−1X, equivalently XT  
(1 +"
SUPERVISED GPCA,0.19256756756756757,"α)I −

α(1 −β) ˜Asym + αβD−1"
SUPERVISED GPCA,0.19594594594594594,2 Y Y T D−1
SUPERVISED GPCA,0.19932432432432431,"2 −1X, corresponding to the largest k eigenvalues."
SUPERVISED GPCA,0.20270270270270271,Proof. The proof is similar to that of Theorem 3.1.
APPROXIMATION AND COMPLEXITY ANALYSIS,0.20608108108108109,"3.6
APPROXIMATION AND COMPLEXITY ANALYSIS"
APPROXIMATION AND COMPLEXITY ANALYSIS,0.20945945945945946,"According to formulations in Theorems 3.1 and 3.2, obtaining Z∗∈Rn×k and W ∗∈Rd×k requires
two demanding computations (1) the inverse of Φα = (I + αL) ∈Rn×n, or in the supervised case
Φα = (I +α˜Lspr); and (2) top k eigenvectors of the matrix XT Φ−1
α X ∈Rd×d. Eigen-decomposition
takes O(d3) (Pan & Chen, 1999), which is scalable as d is usually small. Computing matrix inverse,
on the other hand, can take O(n3) and require O(n2) memory, which would be infeasible for very
large graphs."
APPROXIMATION AND COMPLEXITY ANALYSIS,0.21283783783783783,"To reduce computation and memory complexity, we instead approximately compute F := ϕ−1
α X,
which is a common term for both Z∗and W ∗. We can equivalently write"
APPROXIMATION AND COMPLEXITY ANALYSIS,0.21621621621621623,"(I + αL)F = X =⇒F + αF = αPF + X =⇒F =
α
1 + αPF +
1
1 + αX"
APPROXIMATION AND COMPLEXITY ANALYSIS,0.2195945945945946,for P = ˜Asym in the unsupervised case and P = (1−β) ˜Asym +βD−1
APPROXIMATION AND COMPLEXITY ANALYSIS,0.22297297297297297,2 (Y Y T )D−1
APPROXIMATION AND COMPLEXITY ANALYSIS,0.22635135135135134,2 when supervised.
APPROXIMATION AND COMPLEXITY ANALYSIS,0.22972972972972974,"Then, we can iteratively (with total T iterations) use the power method (Golub & Van Loan, 1989) to
compute F as"
APPROXIMATION AND COMPLEXITY ANALYSIS,0.23310810810810811,"F (t+1) ←
α
1 + αPF (t) +
1
1 + αX
(13)"
APPROXIMATION AND COMPLEXITY ANALYSIS,0.23648648648648649,"where t ∈{0, ..., T} depicts the iteration and F (0) ∈Rn×d is initialized as X (or randomly). For the
supervised case, PF (t) is computed through a series of (from right to left) matrix-matrix products.
This avoids the explicit construction of matrix Y Y T in memory. Overall, solving for F takes
O(T(m + n)d) where m is the number of edges in the graph. The supervised case has an additional
term O(Td|L|c) with c being the number of classes and |L| ≤n be the number of labeled nodes,
which can also be upper-bounded by O(T(m + n)d) when treating c as constant."
APPROXIMATION AND COMPLEXITY ANALYSIS,0.23986486486486486,"Having solved for F, we perform the matrix-matrix product Z∗= FW ∗in O(ndk) and then the
eigen-decomposition of XT F in O(d3 + nd2) = O(nd2) (for n ≥d). Assuming O(d) = O(k),
overall complexity for computing the 1-layer GPCA is given as O(Tmd + Tnd + nd2), which is
linear in the number of nodes and edges. Note that empirically we found 5 ≤T ≤10 to be sufficient."
APPROXIMATION AND COMPLEXITY ANALYSIS,0.24324324324324326,Under review as a conference paper at ICLR 2022
APPROXIMATION AND COMPLEXITY ANALYSIS,0.24662162162162163,Algorithm 1 GPCANET Forward Pass and Pre-training
APPROXIMATION AND COMPLEXITY ANALYSIS,0.25,"1: Input: graph G = (V, E, X), GPCA hyper-parameter(s) α (and β if supervised, β = 0 otherwise), #layers
L, hidden layer sizes {d1, . . . , dL}, activation function σ(·), #approximation steps T
2: Output: pre-set layer-wise parameters {W (1), . . . , W (L)}
3: Initialize H(0) := X
4: for l = 1 to L do
5:
Center H(l−1) by subtracting mean of row vectors
6:
F ←−H(l−1)"
APPROXIMATION AND COMPLEXITY ANALYSIS,0.2533783783783784,"7:
for t = 1 to T do
8:
PF ←−(1 −β) ˜AsymF + βD−1"
APPROXIMATION AND COMPLEXITY ANALYSIS,0.25675675675675674,2 (Y Y T )D−1
F,0.26013513513513514,"2 F
9:
F ←−
α
1+αPF +
1
1+αH(l−1)"
F,0.2635135135135135,"10:
end for
11:
W (l) ←−top dl eigenvectors of H(l−1)T F
12:
H(l) ←−σ(FW (l))
13: end for"
F,0.2668918918918919,"4
GPCANET: A STACKING GPCA MODEL"
GPCANET,0.2702702702702703,"4.1
GPCANET"
GPCANET,0.27364864864864863,"Thus far, we drew a connection between the geometrically motivated, manifold-based GPCA and the
graph convolution operation of deep NN based GCN. Next we leverage this connection to design a
new model called GPCANET that takes advantage of the relative strengths of each paradigm; namely,
GPCA’s ability to capture data variation and structure, and GCN’s ability to capture multiple levels of
abstraction (i.e. high-level concepts) through stacked layers and non-linearity."
GPCANET,0.27702702702702703,"In a nustshell, GPCANET is a stacking of multiple (unsupervised or supervised) GPCA layers and
nonlinear transformations, which shares the same architecture as a multi-layer GCN. It consists of
two main stages: (1) Pre-training, which initializes the layer-wise parameters through closed-form
GPCA solutions, and (2) End-to-end-training, which refines these parameters through end-to-end
gradient-based minimization of a global supervised loss criterion at the output layer."
GPCANET,0.28040540540540543,"We remark that GPCANET is not the same as GCN, as each layer uses the formulation in Thm.s
3.1 and 3.2 (with approximation shown in Sec. 3.6). In fact, when α = 1 and β = 0, GPCANET
is the GCN model initialized with GPCANET-initialization, which we discuss more in Sec. 4.2. In
other words, GPCANET is a generalized GCN model with additional hyperparameters, α and β,
controlling the strength of graph regularization based on the existing or “ghost” edges, respectively."
GPCANET,0.28378378378378377,"Forward Pass and Pre-training stage.
During pre-training, weights of the l-th layer, denoted
as W (l) ∈Rdl−1×dl, are pre-set (i.e. initialized) as the leading dl eigenvectors of the matrix
H(l−1)T Φ−1
α H(l−1),2 where H(l−1) is the representation as output by the (l −1)-th layer (with
H(0) := X), and Φα can be the unsupervised (I+αL) or the supervised (I+α˜Lspr). The pre-training
stage takes a single forward pass. Algo. 1 shows both forward pass during end-to-end-training and
the pre-training procedure, where line 11 in blue is a step used only for pre-training."
GPCANET,0.28716216216216217,"Additional treatment for ReLU: Nonlinear transformations like ReLU improves model capacity,
however at pre-training stage, it causes information loss as all negative values are truncated to 0. This
hinders the advantage of using the leading dl eigenvectors to initialize the weights so as to convey
maximum variance (i.e. information) to the next layers. To address this issue, we instead use the
leading dl/2 eigenvectors {wi}dl/2
i=1 and their negatives {−wi}dl/2
i=1 to initialize W (l). Empirically
we observe this always improves performance when using ReLU activation."
GPCANET,0.2905405405405405,"End-to-end training stage. Pre-training can be seen as an information-preserving initialization, as
compared to an uninformative random initialization, after which we refine the layer-wise parameters
via gradient-based optimization w.r.t. a supervised loss criterion at the output layer. Specifically for
semi-supervised node classification, we perform an end-to-end training w.r.t. the cross-entropy loss
on the labeled nodes. All parameters are updated jointly through backpropagation during this stage,
with forward computation shown in Algo.1 (excluding line 11)."
GPCANET,0.2939189189189189,"2If d(l) is greater than the number of eigenvectors, all eigenvectors are used, with additional vectors generated
from random projection of eigenvectors."
GPCANET,0.2972972972972973,Under review as a conference paper at ICLR 2022
GPCANET-INITIALIZATION FOR GCN,0.30067567567567566,"4.2
GPCANET-INITIALIZATION FOR GCN"
GPCANET-INITIALIZATION FOR GCN,0.30405405405405406,"When we set α = 1, β = 0, and approximate the matrix inverse (I + αL)−1 via first-order truncated
Taylor expansion as shown in Eq. equation 7 , GPCANET has the same architecture with GCN. As
such, we can use the pre-training stage of GPCANET to initialize GCN with only minor modification.
Specifically, we replace lines 6 through 10 in Algo. 1 with the following single line:
F ←−˜AsymH(l−1)
(14)
The modified initialization is for GCN and is driven by the mathematical connection between
GPCANET and GCN that we established. We expect that adapting it for other GNNs is also possible
although we do not pursue this direction here."
EXPERIMENTS,0.30743243243243246,"5
EXPERIMENTS"
EXPERIMENTS,0.3108108108108108,"In this section we design extensive experiments to answer the following questions. (Q1) How does
the simple, unsupervised and shallow GPCA compare to its multi-layer extension GPCANET, as well
as to existing GNNs? (Q2) How does our extended, semi-supervised GPCA compare to the original,
unsupervised GPCA? (Q3) Does GPCANET-initialization improve GCN accuracy and robustness?"
EXPERIMENTAL SETUP,0.3141891891891892,"5.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.31756756756756754,"Datasets. We focus on semi-supervised node classification (SSNC) and use 5 benchmark datasets:
First three, CORA, CITESEER, PUBMED (Sen et al., 2008), are relatively small (2K to 10K nodes)
but widely-used citation graphs. For these we use the data splits in Kipf & Welling (2017). The
others, ARXIV and PRODUCTS, are newest and much larger (100K to 2000K) node classification
benchmarks from Open Graph Benchmark (Hu et al., 2020), for which we use the official data splits.
Data statistics can be found in Appendix. A.4.
Baselines. We compare (unsupervised & semi-supervised) GPCA and GPCANET to state-of-the-art
(SOTA) GNNs, including GCN (Kipf & Welling, 2017), APPNP (Klicpera et al., 2019), GAT
(Veliˇckovi´c et al., 2018), and GraphSAGE (G-SAGE) (Hamilton et al., 2017).
Model configuration and training. For each dataset, we define a separate pool of values for the
hyperparameters (HPs): learning rate, weight decay, number of layers, hidden size, dropout rate, and
regularization trade-off terms α, β. For fair comparison, all models share the same HP pools during
training. See Appendix. A.5 for HP configurations and other details."
EXPERIMENTAL SETUP,0.32094594594594594,"5.2
Q1: PERFORMANCE OF (UNSUPERVISED) GPCA AND GPCANET"
EXPERIMENTAL SETUP,0.32432432432432434,"GPCA. Having proved the mathematical connection between GPCA, GCN, and PPNP, we expect
unsupervised GPCA (β = 0) to generate comparable representations. We perform GPCA with
different α ∈{1, 5, 10, 20, 50} (Appendix. Table 5) to obtain node representations and pass those to
a 1- or 2-layer MLP. We compare to GCN, APPNP, as well as other GNNs; GAT and G-SAGE."
EXPERIMENTAL SETUP,0.3277027027027027,"The performance results are given in Table 1. Due to the scale of the largest two datasets, ARXIV and
PRODUCTS, we list the reported performance at OGB-leaderboard3 (depicted by ∗) for G-SAGE on
both datasets, and that of (Cluster-)GAT on PRODUCTS."
EXPERIMENTAL SETUP,0.3310810810810811,"We find that the simple 1-layer GPCA paired with MLP performs consistently better than the multi-
layer GCN model across all 5 datasets. GPCA’s performance is also comparable to or better than
other SOTA GNNs. This is quite notable, given that GPCA is not only shallow but also unsupervised,
whereas all other baselines are trained end-to-end, and with the exception of APPNP, they exhibit
a multi-layer architecture. By carefully looking at the performance of GPCA with varying α (see
Appendix. A.6), we find that different datasets have different best selected α∗(in Table 1 top to
bottom: α∗= {50, 5, 10, 20, 20}) but in general a relatively larger α (compared to graph convolution
of GCN that is equivalent to α = 1) is preferable for all datasets. Larger α implies stronger graph-
regularization on the representations. The outstanding performance of the simple GPCA empirically
confirms that the power of GNNs on the SSNC problem is mainly driven by graph regularization."
EXPERIMENTAL SETUP,0.3344594594594595,"GPCANET.
Compared to the 1-layer GPCA, GPCANET has a deeper architecture along with
nonlinear activation function. Moreover, it employs hyperparameter α at every layer to control the
degree of graph regularization. As each graph convolution has fixed level of graph regularization,
one may hypothesize that increasing the number of layers (L) corresponds to increasing the degree"
EXPERIMENTAL SETUP,0.33783783783783783,3https://ogb.stanford.edu/docs/leader_nodeprop/
EXPERIMENTAL SETUP,0.34121621621621623,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.34459459459459457,"Table 1: Comparison btwn. unsupervised GPCA (β = 0), GPCANET, and existing (supervised)
SOTA GNNs on 5 datasets, w.r.t. mean test accuracy and standard deviation (in parentheses) over 5
different seeds. Those marked with ∗are reported values at the OGB-leaderboard3. Highest mean
performance is in bold and the second highest is underlined."
EXPERIMENTAL SETUP,0.34797297297297297,"GPCA
GPCANET
GCN
APPNP
GAT
G-SAGE"
EXPERIMENTAL SETUP,0.35135135135135137,"CORA
81.10 (0.00) 80.64 (0.33)
80.62 (0.90) 81.35 (0.18)
79.27 (0.50)
81.48 (0.83)
CITESEER
71.80 (0.75) 71.36 (0.21)
71.25 (0.05) 70.33 (0.75)
69.65 (0.59)
71.20 (0.92)
PUBMED
78.78 (0.36) 78.52 (0.17)
78.42 (0.25) 78.95 (0.36)
78.23 (0.54)
77.78 (0.29)
ARXIV
71.86 (0.18) 72.20 (0.15)
70.64 (0.17) 70.55 (0.27)
71.11 (0.11)
71.49∗(0.27)
PRODUCTS 79.23 (0.14) 80.05 (0.29)
77.90 (0.33) 77.96 (0.34) 79.23∗(0.78) 78.29∗(0.16)"
EXPERIMENTAL SETUP,0.3547297297297297,"of graph regularization. We empirically test this hypothesis using GPCANET, by varying both L (2
to 10) and α (0.1 to 10) to show their connection (hidden size is fixed as 128). The result is shown
in Figure 1. The diagonal pattern (in dark blue) empirically suggests that increasing the number of
layers has the same effect as increasing graph regularization via α."
EXPERIMENTAL SETUP,0.3581081081081081,"Figure 1: GPCANET performance
(avg. over 5 seeds) with varying
number of layers (L) and α on
CORA. Increasing L has similar
effect as increasing α. Results also
hold for the other datasets."
EXPERIMENTAL SETUP,0.3614864864864865,"The corresponding interaction between α and number of layers
suggests that we can train a GPCANET with fewer number of
layers yet achieve similar regularization by increasing α. Such
a shallow model that in fact behaves like a deep one has the
advantage of less memory requirement and faster training due
to fewer parameters."
EXPERIMENTAL SETUP,0.36486486486486486,"To this end, we train 1–3-layer GPCANET with varying α
(higher α’s for fewer layers, see Appendix. A.7), and select
the best α and number of layers using validation set. We report
test set performance in Table 1. We do not observe much im-
provement by GPCANET over other models on smaller datasets
CORA, CITESEER, PUBMED, but notable gains on the larger
ARXIV and PRODUCTS. As such, GPCANET enables shal-
low model training via tunable hyperparameter α, achieving
comparable or better performance."
EXPERIMENTAL SETUP,0.36824324324324326,"5.3
Q2:UNSUPERVISED VS. SEMI-SUPERVISED GPCA"
EXPERIMENTAL SETUP,0.3716216216216216,"The representations generated by unsupervised GPCA does not use any label information from
training data. In this work, we have extended GPCA to (semi-)supervised setting with an additional
HP, namely β ∈[0, 1] that trades-off graph regularization due to the actual input graph edges versus
the “ghost” ones added through Y Y T . Overfitting can hurt performance when β is too large or
when there is a distribution shift between the training and test sets. For ARXIV and PRODUCTS, we
empirically observe that β > 0 always degrades performance, possibly because of the distribution
difference between the training and test sets as described in OGB (Hu et al., 2020). Therefore we
only study the effect of β on CORA, CITESEER and PUBMED. The pool for β > 0 is {0.1, 0.2}."
EXPERIMENTAL SETUP,0.375,"Table 2: Comparison btwn. Supervised (S-)GPCA (β>0) and
Unsupervised (U-)GPCA (β=0), w.r.t. mean test accuracy and
standard deviation (in parentheses) over 5 different seeds. Also
shown (bottom row) is the performance by the best method in
Table 1. Highest mean performance is highlighted in bold."
EXPERIMENTAL SETUP,0.3783783783783784,"CORA
CITESEER
PUBMED"
EXPERIMENTAL SETUP,0.38175675675675674,"U-GPCA
81.10 (0.00) 71.80 (0.75) 78.78 (0.36)"
EXPERIMENTAL SETUP,0.38513513513513514,S-GPCA (ALL β>0) 81.17 (0.27) 73.20 (0.71) 79.40 (0.69)
EXPERIMENTAL SETUP,0.3885135135135135,"S-GPCA β=0.1
81.17 (0.27) 72.07 (0.37) 79.40 (0.69)
S-GPCA β=0.2
81.90 (0.00) 73.20 (0.71) 78.73 (0.59)"
EXPERIMENTAL SETUP,0.3918918918918919,"TABLE 1 BEST
81.48 (0.83) 71.80 (0.75) 78.95 (0.36)"
EXPERIMENTAL SETUP,0.3952702702702703,"Results are shown in Table 2,
where (ALL β>0) depicts the se-
lected configuration for which S-
GPCA achieves highest valida-
tion accuracy. The performance
of the best method in Table 1,
respectively of G-SAGE, (unsu-
pervised) GPCA, and APPNP,
is also shown for comparison.
Notably, supervised GPCA pro-
vides a slight gain over unsuper-
vised GPCA across all 3 datasets,
which also improves over the
competing baseline methods."
EXPERIMENTAL SETUP,0.39864864864864863,Under review as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.40202702702702703,"5.4
Q3: GPCANET-INITIALIZATION FOR GCN"
EXPERIMENTAL SETUP,0.40540540540540543,"Finally, we evaluate the effectiveness of GPCANET-initialization for GCN in terms of performance
and robustness under different model sizes, i.e. number of layers L or number of training parameters.
For comparison, Xavier initialization (Glorot & Bengio, 2010) is also used to initialize GCN."
EXPERIMENTAL SETUP,0.40878378378378377,"Table 3: Test set performance of GCN with Xaiver- versus
GPCANET-initialization, w.r.t. varying number of layers (L)
across all datasets. Each reported value is based on the best se-
lected configuration on validation data. GPCANET-init. enables
higher performance that is also stable with increasing depth."
EXPERIMENTAL SETUP,0.41216216216216217,"DATASET
L=2
L=3
L=5
L=10
L=15"
EXPERIMENTAL SETUP,0.4155405405405405,"CORA XAIVER-INIT
80.62 80.62 79.40 76.37 66.07
CORA GPCANET-INIT
81.67 79.50 80.90 79.82 78.00"
EXPERIMENTAL SETUP,0.4189189189189189,"CITESEER XAIVER-INIT
71.25 70.15 71.10 61.90 57.40
CITESEER GPCANET-INIT
71.27 69.27 70.15 68.67 67.87"
EXPERIMENTAL SETUP,0.4222972972972973,"PUBMED XAIVER-INIT
78.42 77.90 77.07 77.00 45.80
PUBMED GPCANET-INIT
78.05 77.25 78.07 77.80 78.03"
EXPERIMENTAL SETUP,0.42567567567567566,"ARXIV XAIVER-INIT
69.61 70.64 70.33 68.32 61.68
ARXIV GPCANET-INIT
69.76 70.72 70.52 69.77 66.28"
EXPERIMENTAL SETUP,0.42905405405405406,"PRODUCTS XAIVER-INIT
77.90 78.65 78.08 76.27 74.70
PRODUCTS GPCANET-INIT 78.13 78.71 78.22 77.47 75.90"
EXPERIMENTAL SETUP,0.43243243243243246,"We report the test set perfor-
mance (averaged over 5 seeds)
of the GCN model using both
initializations in Table 3. The
results show that GPCANET-
initialization tends to outper-
form the widely-used Xavier
initialization.
The improve-
ment grows with increasing num-
ber of layers, which is sig-
nificant at large depths.
No-
tably, GCN with GPCANET-
initialization exhibits stable per-
formance across all layers."
EXPERIMENTAL SETUP,0.4358108108108108,"Besides
looking
at
the
av-
erage
performance,
we
fur-
ther study whether GPCANET-
initialization improves the train-
ing robustness, by reducing performance variation across different seeds. To this end, we first choose
the best configuration for each initialization method based on validation performance, and train the
GCN model with the chosen configuration using 100 random seeds."
EXPERIMENTAL SETUP,0.4391891891891892,"69.4
69.6
69.8
70.0
Test accuracy 0 2 4 6 8"
EXPERIMENTAL SETUP,0.44256756756756754,Counts
EXPERIMENTAL SETUP,0.44594594594594594,"Arxiv, 2-Layer GCN"
EXPERIMENTAL SETUP,0.44932432432432434,"GPCANet-init
Xavier-init"
EXPERIMENTAL SETUP,0.4527027027027027,"56
58
60
62
64
66
68
Test accuracy 0 2 4 6 8 10 12"
EXPERIMENTAL SETUP,0.4560810810810811,Counts
EXPERIMENTAL SETUP,0.4594594594594595,"Arxiv, 15-Layer GCN"
EXPERIMENTAL SETUP,0.46283783783783783,"GPCANet-init
Xavier-init"
EXPERIMENTAL SETUP,0.46621621621621623,"Figure 2: Comparison between Xavier-init. and GPCANET-init.
in terms of test accuracy robustness over 100 seeds on ARXIV.
GPCANET-init. enables robust training especially at larger depth."
EXPERIMENTAL SETUP,0.46959459459459457,"In Figure 2 we present the
histogram of test set accuracy
over 100 runs with different
seeds for ARXIV.
(For re-
sults on other datasets, see Ap-
pendix. A.8.) For both 2-layer
and 15-layer GCN, GPCANET-
initialization not only outper-
forms Xavier-initialization w.r.t.
average performance, but also
in terms of robustness, achiev-
ing much lower performance
variation and few bad outliers, especially for deeper GCN. As such, it acts as a strong data-driven
prior, facilitating the training of numerous parameters across many layers by identifying a promising
region of the parameter space from which supervised fine-tuning is initiated."
CONCLUSION,0.47297297297297297,"6
CONCLUSION"
CONCLUSION,0.47635135135135137,"In this work we have (1) discovered a mathematical connection between GPCA and graph convolution
of GCN and PPNP; (2) extended GPCA to the (semi-)supervised setting; (3) proposed GPCANET,
by stacking GPCA and nonlinear activation, which is a generalized GCN model with an additional
hyperparameter to control the degree of graph regularization, and (4) introduced the GPCANET-
initialization based on the established connection. Accordingly, we designed extensive experiments
demonstrating that (i) the unsupervised shallow GPCA achieves comparable or better performance
than GCN, APPNP, as well as other modern GNNs which suggests that graph convolution’s power is
mainly driven by graph regularization; (ii) semi-supervised GPCA helps improve performance and
should be a powerful yet simple baseline in future research; (iii) GPCANET enables the training of
shallow models with competitive performance via increasing the degree of graph regularization at
each layer, with reduced memory and training time cost; and finally (iv) GPCANET-initialization
acts as a strong data-driven prior for GCN training, enabling robust performance. Our methodological
contributions ( 3) & 4) above) capitalize on the discovery of our theoretical findings ( 1) & 2) ),
shedding new light toward a better understanding and design of GNNs."
CONCLUSION,0.4797297297297297,Under review as a conference paper at ICLR 2022
REFERENCES,0.4831081081081081,REFERENCES
REFERENCES,0.4864864864864865,"Suresh Balakrishnama and Aravind Ganapathiraju. Linear discriminant analysis-a brief tutorial.
Institute for Signal and information Processing, 18(1998):1–8, 1998."
REFERENCES,0.48986486486486486,"Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, August 2013.
ISSN 0162-8828. doi: 10.1109/TPAMI.2013.50. URL http://ieeexplore.ieee.org/
document/6472238/. Zu bearbeitendes Review."
REFERENCES,0.49324324324324326,"Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma. PCANet: A simple
deep learning baseline for image classification? IEEE Transactions on Image Processing, 24(12):
5017–5032, 2015."
REFERENCES,0.4966216216216216,"Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257–266, 2019."
REFERENCES,0.5,"Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph
neural networks for graph classification. In International Conference on Learning Representations
(ICLR), 2020. URL https://openreview.net/forum?id=HygDF6NFPB."
REFERENCES,0.5033783783783784,"Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019."
REFERENCES,0.5067567567567568,"Brian Gallagher, Hanghang Tong, Tina Eliassi-Rad, and Christos Faloutsos. Using ghost edges for
classification in sparsely labeled networks. In International Conference on Knowledge Discovery &
Data Mining, pp. 256–264. ACM, 2008. URL http://dblp.uni-trier.de/db/conf/
kdd/kdd2008.html#GallagherTEF08."
REFERENCES,0.5101351351351351,"Paul Geladi and Bruce R Kowalski. Partial least-squares regression: a tutorial. Analytica chimica
acta, 185:1–17, 1986."
REFERENCES,0.5135135135135135,"Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the 13th International Conference on Artificial Intelligence and
Statistics, pp. 249–256, 2010."
REFERENCES,0.5168918918918919,"G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press, 1989."
REFERENCES,0.5202702702702703,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in neural information processing systems, pp. 1024–1034, 2017."
REFERENCES,0.5236486486486487,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
Conference on Computer Vision, pp. 1026–1034, 2015."
REFERENCES,0.527027027027027,"Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020."
REFERENCES,0.5304054054054054,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456.
PMLR, 2015."
REFERENCES,0.5337837837837838,"Bo Jiang, Chris Ding, Bio Luo, and Jin Tang. Graph-laplacian PCA: Closed-form solution and
robustness. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3492–3498, 2013."
REFERENCES,0.5371621621621622,"Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.5405405405405406,Under review as a conference paper at ICLR 2022
REFERENCES,0.543918918918919,"Johannes Klicpera, Aleksandar Bojchevski, and Stephan G¨unnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations (ICLR), 2019."
REFERENCES,0.5472972972972973,"Philipp Kr¨ahenb¨uhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations
of convolutional neural networks. In International Conference on Learning Representations (ICLR),
2016."
REFERENCES,0.5506756756756757,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems, pp. 1097–1105,
2012."
REFERENCES,0.5540540540540541,"Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018."
REFERENCES,0.5574324324324325,"Qimai Li, Xiao-Ming Wu, Han Liu, Xiaotong Zhang, and Zhichao Guan. Label efficient semi-
supervised learning via graph filtering. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9582–9591, 2019."
REFERENCES,0.5608108108108109,"Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=B1l2bp4YwS."
REFERENCES,0.5641891891891891,"Cheng-Yaw Low, Andrew Beng-Jin Teoh, and Kar-Ann Toh. Stacking PCANet+: An overly simplified
convnets baseline for face recognition. IEEE Signal Processing Letters, 24(11):1581–1585, 2017."
REFERENCES,0.5675675675675675,"Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on graph
neural networks as graph signal denoising. arXiv preprint arXiv:2010.01777, 2020."
REFERENCES,0.5709459459459459,"Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015."
REFERENCES,0.5743243243243243,"Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4602–4609, 2019."
REFERENCES,0.5777027027027027,"Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
arXiv preprint arXiv:1905.09550, 2019."
REFERENCES,0.581081081081081,"Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations (ICLR), 2020. URL
https://openreview.net/forum?id=S1ldO2EFPr."
REFERENCES,0.5844594594594594,"Victor Y Pan and Zhao Q Chen. The complexity of the matrix eigenproblem. In Proceedings of the
31th annual ACM Cymposium on Theory of Computing, pp. 507–516, 1999."
REFERENCES,0.5878378378378378,"Xuran Pan, Shiji Song, and Gao Huang. A unified framework for convolution-based graph neural
networks, 2021. URL https://openreview.net/forum?id=zUMD--Fb9Bt."
REFERENCES,0.5912162162162162,"Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013."
REFERENCES,0.5945945945945946,"Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93–93, 2008."
REFERENCES,0.597972972972973,"Mathias Seuret, Michele Alberti, Marcus Liwicki, and Rolf Ingold. Pca-initialized deep neural
networks applied to document image analysis. In 2017 14th IAPR International Conference on
Document Analysis and Recognition, volume 1, pp. 877–882. IEEE, 2017."
REFERENCES,0.6013513513513513,"Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Gilles Puy, and Pierre Vandergheynst.
Fast robust PCA on graphs. IEEE Journal of Selected Topics in Signal Processing, 10(4):740–756,
2016."
REFERENCES,0.6047297297297297,"David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.
The emerging field of signal processing on graphs: Extending high-dimensional data analysis to
networks and other irregular domains. IEEE Signal Processing magazine, 30(3):83–98, 2013."
REFERENCES,0.6081081081081081,Under review as a conference paper at ICLR 2022
REFERENCES,0.6114864864864865,"Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node
embeddings and structural graph representations. In International Conference on Learning Repre-
sentations, 2020. URL https://openreview.net/forum?id=SJxzFySKwH."
REFERENCES,0.6148648648648649,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph Attention Networks. In International Conference on Learning Representations
(ICLR), 2018."
REFERENCES,0.6182432432432432,"Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In International Conference on Learning Representations (ICLR),
2019. URL https://openreview.net/forum?id=rklz9iAcKQ."
REFERENCES,0.6216216216216216,"Raimar Wagner, Markus Thom, Roland Schweiger, G¨unther Palm, and Albrecht Rothermel. Learning
convolutional neural networks from few samples. In The 2013 International Joint Conference on
Neural Networks (IJCNN), pp. 1–7. IEEE, 2013."
REFERENCES,0.625,"Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-
plifying graph convolutional networks. In International Conference on Machine Learning, pp.
6861–6871, 2019."
REFERENCES,0.6283783783783784,"Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations (ICLR), 2019. URL https:
//openreview.net/forum?id=ryGs6iA5Km."
REFERENCES,0.6317567567567568,"Zhenyue Zhang and Keke Zhao. Low-rank matrix approximation with manifold regularization. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 35(7):1717–1729, 2012."
REFERENCES,0.6351351351351351,"Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations (ICLR), 2020. URL https://openreview.net/
forum?id=rkecl1rtwB."
REFERENCES,0.6385135135135135,"Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural
networks with an optimization framework. arXiv preprint arXiv:2101.11859, 2021."
REFERENCES,0.6418918918918919,Under review as a conference paper at ICLR 2022
REFERENCES,0.6452702702702703,"A
APPENDIX"
REFERENCES,0.6486486486486487,"A.1
PROOF OF THEOREM 3.1"
REFERENCES,0.652027027027027,Proof. We give the proof in two steps.
REFERENCES,0.6554054054054054,"Step 1: For a fixed W, Solve optimal Z∗as a function of W: When fixing W as constant, the
problem becomes quadratic and convex. There is a unique solution, given by first-order optimal
condition. Let ℓdenote the objective function as given in equation 5. Its gradient can be calculated as
∂ℓ
∂Z = 2(I + α˜L)Z −2XW .
(15)"
REFERENCES,0.6587837837837838,Setting equation 15 to 0 leads to the solution Z∗= (I + α˜L)−1XW.
REFERENCES,0.6621621621621622,"Step 2: Replace Z with Z∗, Solve optimal W ∗:
Substituting Z in objective ℓwith Z∗= (I +
α˜L)−1XW, we reduce the optimization to"
REFERENCES,0.6655405405405406,"min
W,W T W =I ∥X −(I +α˜L)−1XWW T ∥2
F + α Tr

W T XT (I +α˜L)−1 ˜L(I +α˜L)−1XW

.
(16)"
REFERENCES,0.668918918918919,"For this part only, let M = (I + α˜L)−1 to simplify notation. We can show that equation 16 is
equivalent to
min
W,W T W =I
Tr(XXT + MXWW T WW T XT M)"
REFERENCES,0.6722972972972973,"−2 Tr(MXWW T XT ) + α Tr(W T XT M ˜LMXW)
(17)"
REFERENCES,0.6756756756756757,"Using the cyclic property of (Tr)ace (and plugging (I + α˜L)−1 for M back), we can write it as (see
Supp. A.2 for detailed derivation.)
max
W,W T W =I
Tr

W T XT (I + α˜L)−1XW

.
(18)"
REFERENCES,0.6790540540540541,"Based on the spectral theorem of PSD matrices, the optimal solution W ∗of problem equation 18
is the combination of eigenvectors, associated with the largest c eigenvalues of the graph-revised
covariance matrix XT (I + α˜L)−1X."
REFERENCES,0.6824324324324325,"A.2
DERIVATION FROM EQ. EQUATION 17 TO EQ. EQUATION 18"
REFERENCES,0.6858108108108109,"For this part only, let A = (I + α˜L)−1 to simplify the notation. We can show that equation 16 is
equivalent to
min
W,W T W =I
Tr(XXT ) −2 Tr(AXWW T XT )"
REFERENCES,0.6891891891891891,+ Tr(AXWW T WW T XT A) + α Tr(W T XT A˜LAXW)
REFERENCES,0.6925675675675675,"≡
max
W,W T W =I
2 Tr(AXWW T XT ) −Tr(AXWW T XT A)"
REFERENCES,0.6959459459459459,"−α Tr(W T XT A˜LAXW)
(19)
Using the cyclic property of (Tr)ace, we can write
max
W,W T W =I
2 Tr(W T XT AXW) −Tr(W T XT AAXW)"
REFERENCES,0.6993243243243243,−α Tr(W T XT A˜LAXW)
REFERENCES,0.7027027027027027,"max
W,W T W =I
Tr

W T XT (2A −AA −A(α˜L)A)XW
"
REFERENCES,0.706081081081081,"max
W,W T W =I
Tr

W T XT  
A + {I −A(I + α˜L)}A

XW
"
REFERENCES,0.7094594594594594,"max
W,W T W =I
Tr

W T XT (I + α˜L)−1XW

(20)"
REFERENCES,0.7128378378378378,where the objective simplifies upon replacing A with (I + α˜L)−1.
REFERENCES,0.7162162162162162,Under review as a conference paper at ICLR 2022
REFERENCES,0.7195945945945946,"A.3
DERIVATION OF EQUIVALENCE IN EQ. EQUATION 9"
REFERENCES,0.722972972972973,"max
z

corr(Y, z)
T 
corr(Y, z)

var(z)"
REFERENCES,0.7263513513513513,"≡max
z
var(Y )

corr(Y, z)
T 
corr(Y, z)

var(z)
(21)"
REFERENCES,0.7297297297297297,"≡max
z

cov(Y, z)
T 
cov(Y, z)

(22)"
REFERENCES,0.7331081081081081,"where cov(Y, z) =
p"
REFERENCES,0.7364864864864865,"var(Y )corr(Y, z)
p"
REFERENCES,0.7398648648648649,var(z)
REFERENCES,0.7432432432432432,"≡max
z

Y T z
T 
Y T z

(23)"
REFERENCES,0.7466216216216216,"≡max
z
zT Y Y T z
(24)"
REFERENCES,0.75,"Note that in equation 21 we added the term var(Y ) without affecting the optimization problem as it
is with respect to z."
REFERENCES,0.7533783783783784,"A.4
DATASET STATISTICS"
REFERENCES,0.7567567567567568,Table 4: Statistics of used datasets.
REFERENCES,0.7601351351351351,"DATASET
#NODES
#EDGES #FEATURES #CLASSES
TRAIN/VAL./TEST"
REFERENCES,0.7635135135135135,"CORA
2,708
5,429
1,433
7 5.2%/18.5%/36.9%
CITESEER
3,327
4,732
3,703
6
3.6%/15%/30%
PUBMED
19,717
44,338
500
3
0.3%/2.5%/5%
ARXIV
169,343
1,166,243
128
40
54%/18%/28%
PRODUCTS 2,449,029 61,859,140
100
47
8%/2%/90%"
REFERENCES,0.7668918918918919,"Datasets used in the experiments are presented in Table 4. Cora, CiteSeer, and PubMed can be
downloaded in Pytorch Geometric Library Fey & Lenssen (2019). Arxiv and Products can be
accessed in https://ogb.stanford.edu/."
REFERENCES,0.7702702702702703,"A.5
HYPERPARAMETER CONFIGURATIONS"
REFERENCES,0.7736486486486487,"We setup hyperparameters pool for each dataset, presented in Table 5. All methods use the same pool.
The only exception is GPCA, as GPCA is just a 1-layer shallow model which can be trained with
lager learning rate; we use 0.1 learning rate for it on all datasets."
REFERENCES,0.777027027027027,"Table 5: Hyperparameters pool for each dataset, includes learning rate (LR), weight decay (WD),
number of layers (#Layers), hidden size, dropout, α, and β. For ARXIV and PRODUCTS, weight
decay is set as 0 because the dataset is large and no overfit happened. Same reason for choosing
smaller dropout rate for them."
REFERENCES,0.7804054054054054,"DATASET
LR
WD
#LAYERS
HIDDEN"
REFERENCES,0.7837837837837838,"CORA
0.001 [0.0005, 0.005, 0.05] [2, 3, 5, 10, 15] [128, 256]
CITESEER
0.001 [0.0005, 0.005, 0.05] [2, 3, 5, 10, 15] [128, 256]
PUBMED
0.001 [0.0005, 0.005, 0.05] [2, 3, 5, 10, 15] [128, 256]
ARXIV
0.005
0
[2, 3, 5, 10, 15] [128, 256]
PRODUCTS 0.001
0
[2, 3, 5, 10, 15] [128, 256]"
REFERENCES,0.7871621621621622,"DATASET
DROPOUT
α
β"
REFERENCES,0.7905405405405406,"CORA
[0, 0.5]
[1, 5, 10, 20, 50] [0, 0.1, 0.2]
CITESEER
[0, 0.5]
[1, 5, 10, 20, 50] [0, 0.1, 0.2]
PUBMED
[0, 0.5]
[1, 5, 10, 20, 50] [0, 0.1, 0.2]
ARXIV
[0, 0.2]
[1, 5, 10, 20, 50]
0
PRODUCTS
[0, 0.1]
[1, 5, 10, 20, 50]
0"
REFERENCES,0.793918918918919,"Models are trained on every configuration across HP pools and picked based on validation perfor-
mance. We use the Adam optimizer for all models. Learning rate is first manually tuned for each
dataset to achieve stable training, and the same learning rate is fixed for all models—we empirically"
REFERENCES,0.7972972972972973,Under review as a conference paper at ICLR 2022
REFERENCES,0.8006756756756757,"observed that learning rate is sensitive to datasets but insensitive to models. For GPCA and GP-
CANET, number of power iterations in Eq. equation 13 is always set to 5. All experiments use the
maximum training epoch as 1000 and repeat 5 times. Detailed configuration of HPs can be found
in Supp. A.5. We mainly use a single GTX-1080ti GPU for small datasets CORA, CITESEER, and
PUBMED. RTX-3090 GPU is used for ARXIV and PRODUCTS."
REFERENCES,0.8040540540540541,"Mini-batch training. As nodes are not independent, GNN is mostly trained in full-batch under
semi-supervised setting. We use full-batch training for all datasets except PRODUCTS, which is too
large to fit into GPU memory during training. ClusterGCN Chiang et al. (2019), a subgraph based
mini-batch training algorithm, is used to train GCN and GPCANET. For evaluation, we still use
full-batch since a single forward pass can be conducted without memory issues. Initialization is also
employed in full-batch."
REFERENCES,0.8074324324324325,"Fair evaluation. Instead of picking the hyperparameter configurations manually, reported (test)
performance is based on the best configuration selected using validation performance, where all
models leverage the same hyperparameter pools. Further, each configuration from the pool is
conducted 5 times to reduce randomness."
REFERENCES,0.8108108108108109,"A.6
GPCA WITH VARYING α"
REFERENCES,0.8141891891891891,"Table 6: Performance of unsupervised GPCA (β = 0) for varying α w.r.t. mean test accuracy and
standard deviation (in parentheses). GPCA (best α) selects α ∈{1, 5, 10, 20, 50} based on validation,
whereas GPCA with specific α uses the specified fixed α."
REFERENCES,0.8175675675675675,CORA CITESEER PUBMED ARXIV PRODUCTS
REFERENCES,0.8209459459459459,"GPCA (BEST α)
81.10
71.80
78.78
71.86
79.23
(0.00)
(0.75)
(0.36)
(0.18)
(0.14)"
REFERENCES,0.8243243243243243,"GPCA-α=1
72.57
70.90
76.92
65.47
73.65
(0.79)
(0.58)
(0.30)
(0.26)
(0.07)"
REFERENCES,0.8277027027027027,"GPCA-α=5
80.95
71.80
79.40
70.69
78.66
(0.17)
(0.75)
(0.29)
(0.11)
(0.09)"
REFERENCES,0.831081081081081,"GPCA-α=10
82.23
71.65
78.78
71.37
79.24
(0.58)
(0.53)
(0.36)
(0.09)
(0.09)"
REFERENCES,0.8344594594594594,"GPCA-α=20
82.05
72.15
78.15
71.86
79.23
(0.54)
(0.47)
(0.50)
(0.18)
(0.14)"
REFERENCES,0.8378378378378378,"GPCA-α=50
81.10
71.50
78.00
71.48
78.92
(0.00)
(0.32)
(0.19)
(0.15
(0.10)"
REFERENCES,0.8412162162162162,"A.7
CONFIGURATIONS FOR EXPERIMENTS OF 1∼3-LAYER GPCANET"
REFERENCES,0.8445945945945946,"To train a shallow GPCANET with tunable α (β=0 is used), we setup different α pool for different
number of layers, because the effect of increasing α is the same to increasing number of layers
(shown in Figure 1). We report the pool for α for each layer in Table 7. For other parameters we use
the same setting mentioned in Table 5."
REFERENCES,0.847972972972973,"Table 7: Pool of α for 1∼3-layer GPCANET, same across all datasets."
REFERENCES,0.8513513513513513,"# LAYERS
POOL OF α"
-LAYER,0.8547297297297297,"1-LAYER
[10, 20, 30]
2-LAYER
[3, 5, 10]
3-LAYER
[1, 2, 3, 5]"
-LAYER,0.8581081081081081,"A.8
GPCANET-INIT’S ROBUSTNESS FOR ADDITIONAL DATASETS"
-LAYER,0.8614864864864865,"Histogram of test set accuracy over 100 runs for GCN initialized by Xavier-initialization and
GPCANET-initialization in CORA (Figure 3), CITESEER (Figure 4), and PUBMED (Figure 5).
We have ignored PRODUCTS as it takes too long to run 100 times, but the result should be similar."
-LAYER,0.8648648648648649,Under review as a conference paper at ICLR 2022
-LAYER,0.8682432432432432,"79
80
81
82
Test accuracy 0 5 10 15 20"
-LAYER,0.8716216216216216,Counts
-LAYER,0.875,"Cora, 2-Layer GCN"
-LAYER,0.8783783783783784,"GPCANet-init
Xavier-init"
-LAYER,0.8817567567567568,"30
40
50
60
70
80
Test accuracy 0 5 10 15"
-LAYER,0.8851351351351351,Counts
-LAYER,0.8885135135135135,"Cora, 15-Layer GCN"
-LAYER,0.8918918918918919,"GPCANet-init
Xavier-init"
-LAYER,0.8952702702702703,"Figure 3: Comparison between Xavier-init and GPCANET-init in terms of test accuracy robustness
over 100 seeds on CORA."
-LAYER,0.8986486486486487,70.25 70.50 70.75 71.00 71.25 71.50 71.75 72.00
-LAYER,0.902027027027027,Test accuracy 0 5 10 15 20
-LAYER,0.9054054054054054,Counts
-LAYER,0.9087837837837838,"CiteSeer, 2-Layer GCN"
-LAYER,0.9121621621621622,"GPCANet-init
Xavier-init"
-LAYER,0.9155405405405406,"50
55
60
65
70
Test accuracy 0 2 4 6 8"
-LAYER,0.918918918918919,Counts
-LAYER,0.9222972972972973,"CiteSeer, 15-Layer GCN"
-LAYER,0.9256756756756757,"GPCANet-init
Xavier-init"
-LAYER,0.9290540540540541,"Figure 4: Comparison between Xavier-init and GPCANET-init in terms of test accuracy robustness
over 100 seeds on CITESEER."
-LAYER,0.9324324324324325,"77.5
78.0
78.5
79.0
Test accuracy 0 5 10 15"
-LAYER,0.9358108108108109,Counts
-LAYER,0.9391891891891891,"PubMed, 2-Layer GCN"
-LAYER,0.9425675675675675,"GPCANet-init
Xavier-init"
-LAYER,0.9459459459459459,"40
50
60
70
80
Test accuracy 0 20 40 60"
-LAYER,0.9493243243243243,Counts
-LAYER,0.9527027027027027,"PubMed, 15-Layer GCN"
-LAYER,0.956081081081081,"GPCANet-init
Xavier-init"
-LAYER,0.9594594594594594,"Figure 5: Comparison between Xavier-init and GPCANET-init in terms of test accuracy robustness
over 100 seeds on PUBMED."
-LAYER,0.9628378378378378,Under review as a conference paper at ICLR 2022
-LAYER,0.9662162162162162,"A.9
TRAINING CURVE COMPARISON FOR GPCANET-INIT"
-LAYER,0.9695945945945946,"Figure 6: Training curve of 10-layer GCN initialized with Xavier initialization and GPCANET-Init
on CORA."
-LAYER,0.972972972972973,"Figure 7: Training curve of 10-layer GCN initialized with Xavier initialization and GPCANET-Init
on ARXIV."
-LAYER,0.9763513513513513,"A.10
RUNTIME COMPARISON"
-LAYER,0.9797297297297297,"We have analyzed the runtime complexity of GPCA, GPCANET, and GPCANET-Init in Sec.3.6 and
show their runtime is linear in number of nodes. Table 8 presents the practical runtime comparison
among all methods, measured in seconds/epoch for all models, and total initialization seconds for
GPCANET-Init, which verified the complexity analysis. Besides, GPCA is a extremely fast method
with strong performance, and should be used as a strong baseline in future research."
-LAYER,0.9831081081081081,Table 8: Runtime comparison for different methods over all datasets.
-LAYER,0.9864864864864865,"CORA CITESEER PUBMED
ARXIV
PRODUCTS"
-LAYER,0.9898648648648649,"Num Nodes n
2,708
3,327
19,717
169,343
2,449,029
Features d
1,433
3,708
500
128
100"
-LAYER,0.9932432432432432,"GCN (seconds/epoch)
0.0025
0.0025
0.0040
0.0469
30.9544
GPCA (seconds/epoch)
0.0010
0.0010
0.0010
0.0072
0.0443
GPCANET (seconds/epoch) 0.0062
0.0101
0.0202
0.2172
31.3664"
-LAYER,0.9966216216216216,"GPCANET-Init (seconds)
0.836
1.614
0.659
0.657
2.477"
