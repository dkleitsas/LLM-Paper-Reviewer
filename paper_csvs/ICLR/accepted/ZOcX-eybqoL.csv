Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002638522427440633,"We leverage logical composition in reinforcement learning to create a framework
that enables an agent to autonomously determine whether a new task can be
immediately solved using its existing abilities, or whether a task-speciﬁc skill
should be learned. In the latter case, the proposed algorithm also enables the
agent to learn the new task faster by generating an estimate of the optimal policy.
Importantly, we provide two main theoretical results: we bound the performance
of the transferred policy on a new task, and we give bounds on the necessary and
sufﬁcient number of tasks that need to be learned throughout an agent’s lifetime to
generalise over a distribution. We verify our approach in a series of experiments,
where we perform transfer learning both after learning a set of base tasks, and
after learning an arbitrary set of tasks. We also demonstrate that, as a side effect
of our transfer learning approach, an agent can produce an interpretable Boolean
expression of its understanding of the current task. Finally, we demonstrate our
approach in the full lifelong setting where an agent receives tasks from an unknown
distribution. Starting from scratch, an agent is able to quickly generalise over the
task distribution after learning only a few tasks, which are sub-logarithmic in the
size of the task space."
INTRODUCTION,0.005277044854881266,"1
INTRODUCTION"
INTRODUCTION,0.0079155672823219,"Reinforcement learning (RL) is a framework that enables agents to learn desired behaviours by
maximising the rewards received through interaction with an environment (Sutton et al., 1998). While
RL has achieved recent success in several difﬁcult, high-dimensional domains (Mnih et al., 2015;
Levine et al., 2016; Lillicrap et al., 2016; Silver et al., 2017), these methods require millions of samples
from the environment to learn optimal behaviours. This is ultimately a fatal ﬂaw, since learning to
solve complex, real-world tasks from scratch for every task of interest is typically infeasible. Hence a
major challenge in RL is building general-purpose agents that are able to use existing knowledge to
quickly solve new tasks in the environment. The question of interest is then: after learning n tasks
sampled from some distribution, how can an agent transfer or leverage the skills learned from those n
tasks to improve its starting performance or learning speed in task n + 1?"
INTRODUCTION,0.010554089709762533,"This problem setting is formalised by lifelong RL (Thrun, 1996; Abel et al., 2018). One approach to
transfer in lifelong RL is composition (Todorov, 2009), which allows an agent to leverage its existing
skills to build complex, novel behaviours that can then be used to solve or speed up learning of a new
task (Todorov, 2009; Saxe et al., 2017; Haarnoja et al., 2018; van Niekerk et al., 2019; Hunt et al.,
2019; Peng et al., 2019). Recently, Nangue Tasse et al. (2020) proposed a framework for deﬁning a
Boolean algebra over the space of tasks and their optimal value functions. This allowed for tasks and
value functions to be composed using the union, intersection and negation operators in a principled
manner to yield optimal skills zero-shot."
INTRODUCTION,0.013192612137203167,"In this work, we propose a framework for lifelong RL that focuses not only on transfer between tasks
for faster RL, but also provides guarantees on the generalisation of an agent’s skills over an unknown
task distribution. We ﬁrst extend the logical composition framework of Nangue Tasse et al. (2020) to
discounted and stochastic tasks. We provide theoretical bounds for our approach in stochastic settings,"
INTRODUCTION,0.0158311345646438,Published as a conference paper at ICLR 2022
INTRODUCTION,0.018469656992084433,"and also compare them to previous work in the discounted setting. We then show how our framework
leverages logical composition to tackle the lifelong RL problem. The framework enables agents to
iteratively solve tasks as they are given, while at the same time constructing a library of skills that
can be composed to obtain behaviours for solving future tasks faster, or even without further learning."
INTRODUCTION,0.021108179419525065,"We empirically verify our framework in a series of experiments, where an agent is i) pretrained on
a set of base tasks provided by the Boolean algebra framework, and ii) when the pretrained tasks
are not base tasks. We show that agents here are able to achieve signiﬁcant jumpstarts on new tasks.
Finally, we demonstrate our framework in the lifelong RL setting where an agent receives tasks from
an unknown (possibly non-stationary) distribution and must determine what skills to learn and add to
its library, and how to combine its current skills to solve new tasks. Results demonstrate that this
framework enables agents to quickly learn a set of skills, resulting in a combinatorial explosion in
their abilities. Consequently, even when tasks are sampled randomly from an unknown distribution, an
agent can leverage its existing skills to solve new tasks without further learning, thereby generalising
over task distributions."
BACKGROUND,0.023746701846965697,"2
BACKGROUND"
BACKGROUND,0.026385224274406333,"We consider tasks modelled by Markov Decision Processes (MDPs). An MDP is deﬁned by the
tuple (S, A, p, r, γ), where (i) S is the state space, (ii) A is the action space, (iii) p(s′|s, a) is a
Markov transition probability, (iv) r is the real-valued reward function bounded by [rMIN, rMAX], and
(v) γ ∈[0, 1) is the discount factor. In this work, we focus on tasks where an agent is required to
reach a set of desirable goals in a goal space G ⊆S (a set of boundary states). Here, termination in
G is modelled similarly to van Niekerk et al. (2019) by augmenting the state space with a virtual
state, ω, such that p(ω|s, a) = 1 ∀(s, a) ∈(G × A) and the rewards are zero after reaching ω. We
hence consider the set of tasks M such that the tasks are in the same environment—described by a
background MDP (S, A, p, γ, r0)—and each task can be uniquely speciﬁed by a set of desirable and
undesirable goals:"
BACKGROUND,0.029023746701846966,"M(S, A, p, γ, r0) := {(S, A, p, γ, r) | ∀a ∈A, r(s, a) = r0(s, a) ∀s ∈S \ G;
r(g, a) = rg ∈{rMIN, rMAX} ∀g ∈G}
(1)"
BACKGROUND,0.0316622691292876,"The goal of the agent is to compute a Markov policy π from S to A that optimally solves a given task.
A given policy π is characterised by a value function V π(s) = Eπ [P∞
t=0 γtr(st, at)], specifying
the expected return obtained under π starting from state s. The optimal policy π∗is the policy that
obtains the greatest expected return at each state: V π∗(s) = V ∗(s) = maxπ V π(s) for all s in S. A
related quantity is the Q-value function, Qπ(s, a), which deﬁnes the expected return obtained by
executing a from s, and thereafter following π. Similarly, the optimal Q-value function is given by
Q∗(s, a) = maxπ Qπ(s, a) for all s in S and a in A."
LOGICAL COMPOSITION,0.03430079155672823,"2.1
LOGICAL COMPOSITION"
LOGICAL COMPOSITION,0.036939313984168866,"Nangue Tasse et al. (2020) recently proposed the notion of a Boolean task algebra, which allows
an agent to perform logical operations—conjunction (∧), disjunction (∨) and negation (¬)—over
the space of tasks and value functions. While they only considered deterministic shortest path
tasks (γ = 1 with deterministic dynamics), we summarise their approach here and later extend it to
discounted stochastic tasks (Section 3.1)."
LOGICAL COMPOSITION,0.0395778364116095,"To achieve zero-shot logical composition, Nangue Tasse et al. (2020) extend the standard rewards
and value functions used by an agent to deﬁne goal-oriented versions as follows:
Deﬁnition 1. The extended reward function ¯r : S × G × A →R is given by the mapping"
LOGICAL COMPOSITION,0.04221635883905013,"(s, g, a) 7→
¯rMIN
if g ̸= s and s ∈G
r(s, a)
otherwise,
(2)"
LOGICAL COMPOSITION,0.044854881266490766,"where ¯rMIN ≤min{rMIN, (rMIN −rMAX)D}, and D is the diameter of the MDP (Jaksch et al., 2010).
Deﬁnition 2. The extended Q-value function ¯Q : S × G × A →R is given by the mapping"
LOGICAL COMPOSITION,0.047493403693931395,"(s, g, a) 7→¯r(s, g, a) + γ
X"
LOGICAL COMPOSITION,0.05013192612137203,"s′∈S
p(s′|s, a) ¯V ¯π(s′, g),
(3)"
LOGICAL COMPOSITION,0.052770448548812667,Published as a conference paper at ICLR 2022
LOGICAL COMPOSITION,0.055408970976253295,"where ¯V ¯π(s, g) = E¯π [P∞
t=0 ¯r(st, g, at)]."
LOGICAL COMPOSITION,0.05804749340369393,"By penalising the agent for achieving goals different from those it wanted to reach (¯rMIN if g ̸=
s and s ∈G), the extended reward function has the effect of driving the agent to learn how
to separately achieve all desirable goals.
Importantly, the standard reward and value func-
tions can be recovered from their extended versions by simply maximising over goals.
As
such, the agent can also recover the task policy by maximising over both goals and actions:
π(s) ∈arg maxa∈A maxg∈G ¯Q(s, g, a)."
LOGICAL COMPOSITION,0.06068601583113457,"The logic operators over tasks and extended action-value functions are then deﬁned as follows:
Deﬁnition 3. Let M be a set of tasks with bounds MMIN, MMAX ∈M such that,
rMMAX(s, a) := max
M∈M rM(s, a)
rMMIN (s, a) := min
M∈M rM(s, a)"
LOGICAL COMPOSITION,0.0633245382585752,"Deﬁne the ¬, ∨, and ∧operators over M as"
LOGICAL COMPOSITION,0.06596306068601583,"¬(M) := (S, A, p, r¬M), where r¬M(s, a) := (rMMAX(s, a) + rMMIN (s, a)) −rM(s, a)"
LOGICAL COMPOSITION,0.06860158311345646,"∨(M1, M2) := (S, A, p, rM1∨M2), where rM1∨M2(s, a) := max{rM1(s, a), rM2(s, a)}"
LOGICAL COMPOSITION,0.0712401055408971,"∧(M1, M2) := (S, A, p, rM1∧M2), where rM1∧M2(s, a) := min{rM1(s, a), rM2(s, a)}"
LOGICAL COMPOSITION,0.07387862796833773,"Deﬁnition 4. Let ¯Q∗be the set of optimal extended ¯Q-value functions for tasks in M, with
bounds ¯Q∗
MIN, ¯Q∗
MAX
∈
¯Q∗which are respectively the optimal ¯Q-functions for the tasks
MMIN, MMAX ∈M. Deﬁne the ¬, ∨, and ∧operators over ¯Q∗as,"
LOGICAL COMPOSITION,0.07651715039577836,"¬( ¯Q∗)(s, g, a) :=
  ¯Q∗
MIN(s, g, a) + ¯Q∗
MAX(s, g, a)

−¯Q∗(s, g, a)"
LOGICAL COMPOSITION,0.079155672823219,"∨( ¯Q∗
1, ¯Q∗
2)(s, g, a) := max{ ¯Q∗
1(s, g, a), ¯Q∗
2(s, g, a)}"
LOGICAL COMPOSITION,0.08179419525065963,"∧( ¯Q∗
1, ¯Q∗
2)(s, g, a) := min{ ¯Q∗
1(s, g, a), ¯Q∗
2(s, g, a)}"
LOGICAL COMPOSITION,0.08443271767810026,"Using the deﬁnitions for the logical operations over M and ¯Q∗given above, Nangue Tasse et al.
(2020) construct a Boolean algebra over tasks and extended value functions. Furthermore, by
leveraging the goal-oriented deﬁnition of extended value functions, they also show that M and ¯Q∗
are homomorphic. As a result, if a task can be expressed using the Boolean algebra, the optimal value
function for the task can immediately be computed. This enables agents to solve any new task that is
given as the logical combination of learned ones."
LIFELONG TRANSFER THROUGH COMPOSITION,0.0870712401055409,"3
LIFELONG TRANSFER THROUGH COMPOSITION"
LIFELONG TRANSFER THROUGH COMPOSITION,0.08970976253298153,"In lifelong RL, an agent is presented with a series of tasks sampled from some distribution D. The
agent then needs to not only transfer knowledge learned from previous tasks to solve new but related
tasks quickly, but it also should not forget learned knowledge in the process. We formalise this
lifelong learning problem as follows:
Deﬁnition 5. Let D be an unknown, possibly non-stationary, distribution over a set of tasks
M(S, A, p, γ, r0). The lifelong learning problem consists of the repetition of the following steps for
t ∈N:"
LIFELONG TRANSFER THROUGH COMPOSITION,0.09234828496042216,"1. The agent is given a task Mt ∼D(t),"
LIFELONG TRANSFER THROUGH COMPOSITION,0.09498680738786279,"2. The agent interacts with the MDP Mt until it is ϵ-optimal in M0, ..., Mt."
LIFELONG TRANSFER THROUGH COMPOSITION,0.09762532981530343,"This formulation of lifelong RL is similar to that of Abel et al. (2018); the main difference is that we
do not assume that D is stationary, and we explicitly require an agent to retain learned skills."
LIFELONG TRANSFER THROUGH COMPOSITION,0.10026385224274406,"As discussed in the introduction, one of the main goals in this setting is that of transfer (Taylor &
Stone, 2009). We add an important question to this setting: how many tasks should an agent learn"
LIFELONG TRANSFER THROUGH COMPOSITION,0.10290237467018469,Published as a conference paper at ICLR 2022
LIFELONG TRANSFER THROUGH COMPOSITION,0.10554089709762533,"during its lifetime in order to generalise over the task distribution? In other words, how many tasks
should it learn to be able to solve any new task immediately? While most approaches focus on the
goal of transfer, the question of the number of tasks is often neglected by simply assuming the case
where the agent has already learned n tasks (Abel et al., 2018; Barreto et al., 2018). Consider, for
example, a task space with only |G| = 40 goals. Then, given the combination of all possible goals,
the size of the task space is |M| = 2|G| ≈1012. If D is a uniform distribution over |M|, then for
most transfer learning methods an agent will have to learn most of the tasks it is presented with, since
the probability of observing the same task will be approximately zero. This is clearly impractical for
a setting like RL, where learning methods often have a high sample complexity even with transfer
learning. It is also extremely memory inefﬁcient, since the learned skills of most tasks must be stored."
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.10817941952506596,"3.1
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK"
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.11081794195250659,"In this section, we show how logical composition can be leveraged to learn a subset of tasks
that is sufﬁcient to generalise over the task distribution. Since the logical composition results of
Nangue Tasse et al. (2020) were only shown for deterministic shortest path tasks (where γ = 1), we
extend the framework to discounted and stochastic tasks M (Equation 1). To achieve this, we ﬁrst
redeﬁne the extended reward function (Deﬁnition 1) to use the simpler penalty ¯rMIN = rMIN. We
also redeﬁne ¬ over ¯Q∗as follows:"
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.11345646437994723,"¬( ¯Q∗)(.) :=
 ¯Q∗
MAX(.)
if | ¯Q∗(.) −¯Q∗
MIN(.)| ≤| ¯Q∗(.) −¯Q∗
MAX(.)|
¯Q∗
MIN(.)
otherwise,
, ∀(.) ∈S × G × A."
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.11609498680738786,"The intuition behind this re-deﬁnition of the negation operator is as follows: since each goal is
either desirable or not, the optimal extended value function ¯Q∗(s, g, a) is either ¯Q∗
MAX(s, g, a)
or ¯Q∗
MIN(s, g, a). Hence, if ¯Q∗(s, g, a) is closer to ¯Q∗
MIN(s, g, a), then its negation should be
¯Q∗
MAX(s, g, a), and vice versa. For tasks in M, this is equivalent to the previous deﬁnition of ¬
for optimal ¯Q-value functions, but it will give us tight bounds when composing ϵ-optimal ¯Q-value
functions (see Theorem 1)."
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.11873350923482849,"We now show that the Boolean algebra and zero-shot composition results of Nangue Tasse et al.
(2020) also hold for tasks in M."
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.12137203166226913,"Proposition 1. Let ¯Q∗be the set of optimal ¯Q-value functions for tasks in M. Let A : M →¯Q∗
be any map from M to ¯Q∗such that A (M) = ¯Q∗
M for all M in M. Then,"
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.12401055408970976,"(i) M and ¯Q∗respectively form a Boolean task algebra (M, ∨, ∧, ¬, MMAX, MMIN) and
a Boolean extended value functions algebra ( ¯Q∗, ∨, ∧, ¬, ¯Q∗
MAX, ¯Q∗
MIN),"
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.1266490765171504,(ii) A is a homomorphism between M and ¯Q∗.
EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK,0.12928759894459102,"We can now solve any new task in M zero-shot if we are given the correct Boolean expression that
informs the agent how to compose its optimal skills. This is essential for the following results."
TRANSFER BETWEEN TASKS,0.13192612137203166,"3.2
TRANSFER BETWEEN TASKS"
TRANSFER BETWEEN TASKS,0.1345646437994723,"In this section, we leverage the logical composition results to address the following question of interest:
given an arbitrary set of learned tasks, can we transfer their skills to solve new tasks faster? As we
will show in Theorem 1, we answer this question in the afﬁrmative. To achieve this, we ﬁrst note that
each task M ∈M can be associated with a binary vector T ∈{0, 1}|G| which represents its set of
desirable goals, as illustrated by the tasks in Table 1. The approximation ˜T of this task representation
can be learned just from task rewards (rM(s, a)) by simply computing ˜T(s) = 1rM(s,a)=rMAX at
each terminal state s that the agent reaches. We can then use any generic method, such as the sum-of-
products (SOP), to determine a candidate Boolean expression (BEXP ) in terms of the learned binary
representations ˜Tn = { ˜T1, ..., ˜Tn} of a set of past tasks ˆ
M = {M1, ..., Mn} ⊆M. An estimate of
the optimal ¯Q-value function of M can then be obtained by composing the learned ¯Q-value functions
˜¯Q∗
n = { ˜¯Q∗
1, ..., ˜¯Q∗
n} according to BEXP . Theorem 1 shows the optimality of this process.1"
TRANSFER BETWEEN TASKS,0.13720316622691292,1See Appendix A for proofs of theorems and Appendix B for a brief description of the SOP method.
TRANSFER BETWEEN TASKS,0.13984168865435356,Published as a conference paper at ICLR 2022
TRANSFER BETWEEN TASKS,0.1424802110817942,"Theorem 1. Let M ∈M be a task with reward function r, binary representation T and optimal
extended action-value function ¯Q∗. Given ϵ-approximations of the binary representations ˜Tn =
{ ˜T1, ..., ˜Tn} and optimal ¯Q-functions ˜¯Q∗
n = { ˜¯Q∗
1, ..., ˜¯Q∗
n} for n tasks ˆ
M = {M1, ..., Mn} ⊆M, let"
TRANSFER BETWEEN TASKS,0.14511873350923482,"TF = BEXP ( ˜Tn) and ¯QF = BEXP ( ˜¯Q∗
n),"
TRANSFER BETWEEN TASKS,0.14775725593667546,"where BEXP is derived from
˜Tn and
˜T using a generic method F.
Deﬁne π(s)
∈
arg maxa∈A QF where QF := maxg∈G ¯QF(s, g, a). Then,"
TRANSFER BETWEEN TASKS,0.1503957783641161,"(i) ∥Q∗−Qπ∥∞≤
2
1−γ ((1T ̸=TF + 1r /∈{rg}|G|)r∆+ ϵ),"
TRANSFER BETWEEN TASKS,0.15303430079155672,"(ii) if the dynamics are deterministic,"
TRANSFER BETWEEN TASKS,0.15567282321899736,"∥Q∗−QF∥∞≤(1T ̸=TF )r∆+ ϵ,"
TRANSFER BETWEEN TASKS,0.158311345646438,"where 1 is the indicator function, rg(s, a) := ¯r(s, g, a), r∆:= rMAX −rMIN, and ∥f −h∥∞:=
maxs,g,a |f(s, g, a) −h(s, g, a)|."
TRANSFER BETWEEN TASKS,0.16094986807387862,"Theorem 1(i) states that if ¯QF is close to optimal, then acting greedily with respect to it is also close
to optimal. Interestingly, this is similar to the bound obtained by Barreto et al. (2018) (Proposition 1)
for transfer learning using generalised policy improvement (GPI), but stronger.2 This is unsurprising,
since π(s) ∈arg maxa∈A maxg∈G ¯QF(s, g, a) can be interpreted as generalised policy improvement
on the set of goal policies of the extended value function ¯QF. Importantly, if the environment is
deterministic, then we obtain a strong bound on the composed value functions (Theorem 1(ii)). This
bound shows that transfer learning using logical composition is ϵ-optimal—that is, there is no loss
in optimality—when the new task is expressible as a logical combination of past ones. With the
exponential nature of logical combination, this gives agents a strong generalisation ability over the
task space—and hence over any task distribution—as we will show in Theorem 2."
GENERALISATION OVER A TASK DISTRIBUTION,0.16358839050131926,"3.3
GENERALISATION OVER A TASK DISTRIBUTION"
GENERALISATION OVER A TASK DISTRIBUTION,0.1662269129287599,"We leverage Theorem 1 to design an algorithm that combines the SOP approach with goal-oriented
learning to achieve fast transfer in lifelong RL. Given an off-policy RL algorithm A , the agent
initializes its extended value function ˜¯Q, the task binary vector ˜T, and a goal buffer. At the beginning
of each episode, the agent computes TSOP and QSOP for ˜T using the SOP method and its library of
learned task vectors and extended Q-functions. It then acts using the behaviour policy (ϵ-greedy for
example) of A with ¯QSOP for the action-value function if TSOP = ˜T, and ¯QSOP ∨˜¯Q otherwise.3"
GENERALISATION OVER A TASK DISTRIBUTION,0.16886543535620052,"If TSOP ̸= ˜T, the agent also updates ˜¯Q for each goal in the goal buffer using A . Additionally, when
the agent reaches a terminal state s, it adds it to the goal buffer and updates ˜T(s) using the reward it
receives ( ˜T(s) = 1rM(s,a)=rMAX). Training stops when the agent has reached the desired level of
optimality (or after n episodes in practice), after which the agent adds the learned ˜T and ˜¯Q to its
library if TSOP ̸= ˜T. The full algorithm is included in Appendix B. We refer to this algorithm as
SOPGOL (Sum Of Products with Goal-Oriented Learning)."
GENERALISATION OVER A TASK DISTRIBUTION,0.17150395778364116,"When G is ﬁnite, we show in Theorem 2 that SOPGOL generalises over any unknown task distribution
after learning only a number of tasks logarithmic in the size of the task space. The lower bound is
⌈log|G|⌉, since this is the minimum number of tasks that span the task space, as can be seen in Table
1 (top) for example. The upper bound is |G| because that is the dimensionality of the task binary
representations {0, 1}|G|. Since the number of tasks is |M| = 2|G|, we have that the upper bound
|G| = log|M| is logarithmic in the size of the task space."
GENERALISATION OVER A TASK DISTRIBUTION,0.1741424802110818,"2See Section 1.4 of the appendix for a detailed discussion of this with the simpliﬁcation of the bound in
Proposition 1 (Barreto et al., 2018) to the same form as Theorem 1(i).
3Since ¯QSOP ∨˜¯Q = max{ ¯QSOP , ˜¯Q}, it is equivalent to GPI and hence is guaranteed to be equal or
more optimal than the individual value functions. Hence using ¯QSOP ∨˜¯Q in the behaviour policy gives a
straightforward way of leveraging ¯QSOP to learn ˜¯Q faster."
GENERALISATION OVER A TASK DISTRIBUTION,0.17678100263852242,Published as a conference paper at ICLR 2022
GENERALISATION OVER A TASK DISTRIBUTION,0.17941952506596306,"Theorem 2. Let D be an unknown, possibly non-stationary, distribution over a set of tasks
M(S, A, p, γ, r0) with ﬁnite G. Let A
: M →
¯Q∗be any map from M to ¯Q∗such that
A (M) = ¯Q∗
M for all M in M. Let"
GENERALISATION OVER A TASK DISTRIBUTION,0.1820580474934037,"˜Tt+1, ˜¯Q∗
t+1 = SOPGOL(A , Mt, ˜Tt, ˜¯Q∗
t ) where Mt ∼D(t) and ˜T0 = ˜¯Q∗
0 = ∅∀t ∈N."
GENERALISATION OVER A TASK DISTRIBUTION,0.18469656992084432,"Then,
⌈log |G|⌉≤lim
t→∞Nt ≤|G|
where Nt := | ˜Tt| = | ˜¯Q∗
t |."
GENERALISATION OVER A TASK DISTRIBUTION,0.18733509234828497,"Interestingly, Theorem 2 holds even in the case where a new task is expressible in terms of past tasks
(TSOP = ˜T), but we wish to solve it to a higher degree of optimality than past tasks. In this case, we
can pretend TSOP ̸= ˜T and learn a new ¯Q-function to the desired degree of optimality. We can then
add it to our library, and remove any other skill from our library (the least optimal for example)."
EXPERIMENTS,0.18997361477572558,"4
EXPERIMENTS"
EXPERIMENTS,0.19261213720316622,"Figure 1:
PICKUPOBJ domain.
The red triangle represents the
agent. Goals"
EXPERIMENTS,0.19525065963060687,"Ta
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1"
EXPERIMENTS,0.19788918205804748,"Tb
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1"
EXPERIMENTS,0.20052770448548812,"Tc
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1"
EXPERIMENTS,0.20316622691292877,"Td
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1 Goals"
EXPERIMENTS,0.20580474934036938,"T1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
T2
0
0
1
0
1
1
1
0
1
1
0
0
1
0
0
T3
1
1
0
1
0
0
0
1
0
1
1
0
0
1
1"
EXPERIMENTS,0.20844327176781002,"Table 1: Binary representation for base (top) and test (bottom)
tasks. 0 or 1 corresponds to a goal reward of rMIN or rMAX."
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.21108179419525067,"4.1
TRANSFER AFTER PRETRAINING ON A SET OF TASKS"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.21372031662269128,"We consider the PICKUPOBJ domain from the MINIGRID environment (Chevalier-Boisvert et al.,
2018), illustrated by Figure 1, where an agent must navigate in a 2D room to pick up objects of
various shapes and colours from pixel observations.4 This type of domain is prototypical in the
literature (Nangue Tasse et al., 2020; Barreto et al., 2020; van Niekerk et al., 2019; Abel et al., 2018),
because it allows for easy demonstration of transfer learning in many-goal tasks. In this domain,
there are |G| = 15 goals each corresponding to picking up objects of 3 possible types—box, ball,
key—and 5 possible colours—red, blue, green, purple, and yellow. Hence a set of ⌈log2 |G|⌉= 4
base tasks can be selected that can be used to solve all 2|G| = 32768 possible tasks under a Boolean
composition of goals. The agent receives a reward of 2 when it picks up desired objects, and −0.1
otherwise. For all of our experiments in this section, we use deep Q-learning (Mnih et al., 2015)
as the RL method for SOPGOL and as the performance baseline. We also compare SOPGOL to
SOPGOL-transfer, or to SOPGOL-continual. SOPGOL-transfer refers to when no new skill is
learned and SOPGOL-continual refers to when a new skill is always learned using the SOP Q
estimate to speed up learning, even if the new task could be solved zero shot. Since SOPGOL
determines automatically which one to use, we compare whichever one it chooses with the other one
in each of our experiments."
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.21635883905013192,"We ﬁrst demonstrate transfer learning after pretraining on a set of base tasks—a minimal set of tasks
that span the task space. This can be done if the set of goals is known upfront, by ﬁrst assigning a
Boolean label to each goal in a table and then using the rows of the table as base tasks. These are
illustrated in Table 1 (top). Having learned the ϵ-optimal extended value functions for our base tasks,"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.21899736147757257,4Further environment details are given in Appendix D.
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.22163588390501318,Published as a conference paper at ICLR 2022
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.22427440633245382,"0.0
0.5
1.0
1.5
2.0
2.5
3.0
Episodes
1e4 10 8 6 4 2 0 2"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.22691292875989447,Returns
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.22955145118733508,"SOPGOL
SOPGOL continual
DQN"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.23218997361477572,"0
1
2
3
4
5
Episodes
1e2 Goals"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.23482849604221637,(a) M1
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.23746701846965698,"0.0
0.5
1.0
1.5
2.0
2.5
Episodes
1e4 10 8 6 4 2 0 2"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.24010554089709762,Returns
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.24274406332453827,"SOPGOL
SOPGOL continual
DQN"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.24538258575197888,"0
1
2
3
4
5
Episodes
1e2 Goals"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.24802110817941952,(b) M2
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.25065963060686014,"0
1
2
3
4
5
6
7
Episodes
1e3 10 8 6 4 2 0 2"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2532981530343008,Returns
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2559366754617414,"SOPGOL
SOPGOL continual
DQN"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.25857519788918204,"0
1
2
3
4
5
Episodes
1e2 Goals"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2612137203166227,(c) M3
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2638522427440633,"Figure 2: Episodic returns (top) and learned binary representations (bottom) for test tasks M1, M2
and M3 after pretraining on the base set of tasks Ma, Mb, Mc and Md. The shaded regions on the
episodic returns indicate one standard deviation over 4 runs. The learned binary representations are
similarly averaged over 4 runs, and reported for the ﬁrst 500 episodes. The initial drop in DQN
performance is as a result of the initial exploration phase where the exploration constant decays from
0.5 to 0.05. The Boolean expressions generated by SOPGOL during training for the respective test
tasks are:
M1
=
Ma ∧Mb ∧Mc ∧Md,
M2
=
(Ma ∧¬Mb ∧¬Md) ∨(Ma ∧Mc ∧Md) ∨(¬Ma ∧Mb ∧¬Mc ∧¬Md) ∨(¬Ma ∧
¬Mb ∧¬Mc ∧Md),
M3
=
(Ma ∧Mb ∧Mc) ∨(Ma ∧¬Mb ∧¬Md) ∨(Ma ∧Mc ∧Md) ∨(¬Ma ∧Mb ∧¬Mc ∧
¬Md) ∨(¬Ma ∧¬Mb ∧¬Mc ∧Md) ∨(¬Mb ∧Mc ∧¬Md)."
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.26649076517150394,"we can now leverage logical composition for transfer learning on test tasks. We consider the three test
tasks shown in Table 1 (bottom). For each, we run SOPGOL, SOPGOL-continual, and a standard
DQN. Figure 2 illustrates the results where, as predicted by our theoretical results in Section 3.2,
SOPGOL correctly determines that the current test tasks are solvable from the logical combinations
of the learned base tasks. Its performance from the start of training is hence the best."
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2691292875989446,"Now that we have demonstrated how SOPGOL enables an agent to solve any new task in an
environment after training on base tasks, we consider the more practical case where new tasks are
not fully expressible as a Boolean expression of previously learned tasks. The agent in this case"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2717678100263852,"0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Episodes
1e4 10 8 6 4 2 0 2"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.27440633245382584,Returns
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2770448548812665,"SOPGOL
SOPGOL transfer
DQN"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2796833773087071,"0
1
2
3
4
5
Episodes
1e2 Goals"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.28232189973614774,(a) M1
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2849604221635884,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Episodes
1e4 10 8 6 4 2 0 2"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.287598944591029,Returns
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.29023746701846964,"SOPGOL
SOPGOL transfer
DQN"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2928759894459103,"0
1
2
3
4
5
Episodes
1e2 Goals"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.2955145118733509,(b) M2
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.29815303430079154,"0
1
2
3
4
5
6
Episodes
1e3 10 8 6 4 2 0 2"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.3007915567282322,Returns
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.3034300791556728,"SOPGOL
SOPGOL transfer
DQN"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.30606860158311344,"0
1
2
3
4
5
Episodes
1e2 Goals"
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.3087071240105541,(c) M3
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.3113456464379947,"Figure 3: Episodic returns (top) and learned binary representations (bottom) for test tasks M1, M2
and M3 after pretraining on the non-base set of tasks
, ,
and . The shaded regions on the episodic
returns indicate one standard deviation over 4 runs. The learned binary representations are similarly
averaged over 4 runs, and reported for the ﬁrst 500 episodes. The initial drop in DQN performance is
a result of the initial exploration phase where the exploration constant decays from 0.5 to 0.05. The
Boolean expressions generated by SOPGOL for the respective test tasks are:
f
M1
=
¬
∧¬
∧
∧¬ ,
f
M2
=
(
∧¬
∧¬ ) ∨(¬
∧
∧¬
∧) ∨(¬
∧¬
∧
∧) ∨(¬
∧¬
∧¬ ),
f
M3
=
(¬
∧¬
∧¬ ) ∨(¬
∧¬ ) ∨(¬
∧¬
∧¬ )."
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.31398416886543534,Published as a conference paper at ICLR 2022
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.316622691292876,"is pretrained on a set of tasks that do not span the task space, { ,
,
, }, corresponding to the
tasks of picking up green objects, blue objects, yellow objects, and keys. We then train the agent
with SOPGOL, SOPGOL-transfer, and a standard DQN on the same set of test tasks considered
previously (Table 1 (bottom)). The results in Figure 3 demonstrate how SOPGOL now chooses to
learn a task-speciﬁc skill after transfer, and hence outperforms SOPGOL-transfer since the test tasks
are not entirely expressible in terms of the pretrained ones. Consider Figure 3a, for example. The
test task is to pick up a yellow box, but the agent has only learned how to pick up red objects, blue
objects, yellow objects, and keys. It has not learned how to pick up boxes. However, we note from
the inferred Boolean expression (f
M1) that the agent correctly identiﬁes that the desired objects are, at
the very least, yellow. Without further improvements to this transferred policy (SOPGOL-transfer),
we can see that this approach outperforms DQN from the start. This is due to two main factors: (i) the
transferred policy navigates to objects more reliably, so takes fewer random actions; and (ii) although
the transferred policy does not have a complete understanding of which are the desirable objects, it at
least navigates to yellow objects, which are sometimes yellow boxes."
TRANSFER AFTER PRETRAINING ON A SET OF TASKS,0.31926121372031663,"Finally, since SOPGOL is able to determine that the current task is not entirely expressible in terms
of its previous tasks (by checking whether TSOP = ˜T), it is able to learn a new ¯Q-value function
that improves on the transferred policy. Additionally, its returns are strictly higher than those of
SOPGOL-transfer because SOPGOL learns the new ¯Q-value function faster by using ¯QSOP ∨˜¯Q in
the behaviour policy."
LIFELONG TRANSFER,0.32189973614775724,"4.2
LIFELONG TRANSFER"
LIFELONG TRANSFER,0.3245382585751979,"In this section, we consider the more general setting where the agent is not necessarily given pretrained
skills upfront, but is rather presented with tasks sampled from some unknown distribution. We revisit
the example given in Section 3, but now more concretely by using a stochastic Four Rooms domain
(Sutton et al., 1999), with a goal space of size |G| = 40 and a task space of size |M| = 2|G| ≈1012.
Complete environment details are given in Appendix C."
LIFELONG TRANSFER,0.32717678100263853,"We demonstrate the ability of SOPGOL to generalise over task distributions by evaluating the
approach with the following distributions: (i) Dsampled: the goals for each task are chosen uniformly
at random over G; (ii) Dbest: the ﬁrst ⌈log2 |G|⌉tasks are the base tasks, while the rest follow
Dsampled. This distribution gives the agent the minimum number of tasks to learn and store, since
the agent learns the base tasks ﬁrst before being presented with any other task. (iii) Dworst: the ﬁrst
|G| tasks are each deﬁned by a single goal that differs from the previous tasks, while the rest follow
Dsampled. This distribution forces the agent to learn and store the maximum number of tasks, since
none of the |G| tasks can be expressed as a logical combination of the others. We use Q-learning
(Watkins, 1989) as the RL method for SOPGOL, and Q-learning with maxQ initialisation as a
baseline. This has been shown by previous work (Abel et al., 2018) to be a practical method of
initialising value functions with a theoretically optimal optimism criterion that speeds-up convergence
during training. Our results (Figure 4) show that SOPGOL enables a lifelong agent to quickly
generalise over an unknown task distribution. Interestingly, both graphs show that the convergence"
LIFELONG TRANSFER,0.32981530343007914,"0
10
20
30
40
50
Tasks 0 1 2 3 4 5"
LIFELONG TRANSFER,0.3324538258575198,Policies stored ×101
LIFELONG TRANSFER,0.33509234828496043,"Dbest
Dworst
DSampled
maxQ"
LIFELONG TRANSFER,0.33773087071240104,"(a) Number of policies learned and stored after
solving n tasks."
LIFELONG TRANSFER,0.3403693931398417,"0
10
20
30
40
50
Tasks 0.0 0.5 1.0 1.5 2.0 2.5"
LIFELONG TRANSFER,0.34300791556728233,Timesteps ×104
LIFELONG TRANSFER,0.34564643799472294,"Dbest
Dworst
DSampled
maxQ"
LIFELONG TRANSFER,0.3482849604221636,"(b) Number of samples required to learn ϵ-
optimal policies for each task."
LIFELONG TRANSFER,0.35092348284960423,"Figure 4: Number of policies learned and samples required for the ﬁrst 50 tasks of an agent’s lifetime
in the Four Rooms domain. The shaded regions represent standard deviations over 25 runs."
LIFELONG TRANSFER,0.35356200527704484,Published as a conference paper at ICLR 2022
LIFELONG TRANSFER,0.3562005277044855,"speed during a randomly sampled task distribution Dsampled is very close to that of the best task
distribution Dbest. This suggests that there is room to make the bound in Theorem 2 even tighter by
making some assumptions on the task distribution—an interesting avenue for future work."
RELATED WORK,0.35883905013192613,"5
RELATED WORK"
RELATED WORK,0.36147757255936674,"There have been several approaches in recent years for tackling the problem of transfer in lifelong
RL. Most closely related is the line of work on concurrent skill composition (Todorov, 2009; Saxe
et al., 2017; Haarnoja et al., 2018; van Niekerk et al., 2019; Hunt et al., 2019). These methods
usually focus on multi-goal tasks, where they address the combinatorial amount of desirable goals by
composing learned skills to create new ones. Given a reward function that is well approximated by
a linear function, Barreto et al. (2020) propose a scheme for few-shot transfer in RL by combining
GPI and successor features (SF) (Barreto et al., 2017). In general, approaches based on GPI with SFs
(Barreto et al., 2021) are suitable for tasks deﬁned by linear preferences over features (latent goal
states). Given the set of features for an environment, Alver & Precup (2022) shows that a base set of
successor features can be learned, which is sufﬁcient to span the task space. While these approaches
also support tasks where goals are not terminal, the smallest number of successor features that must
be learned to span the task space is |G| (the upper-bound in Theorem 2). Our work is similar to these
approaches in that it can be interpreted as performing GPI with the logical composition of extended
value functions, which leads to stronger theoretical bounds than GPI with the linear composition of
successor features (see Appendix A.4). Finally, none of these works consider the lifelong RL setting
where an agent starts with no skill and receives tasks sampled from an unknown distribution (without
additional knowledge like base features or true task representations). In contrast, SOPGOL is able to
handle this setting with logarithmic bounds on the number of skills needed to generalise over the task
distribution (Theorem 2)."
RELATED WORK,0.3641160949868074,"Other approaches like options (Sutton et al., 1999) and hierarchical RL (Barto & Mahadevan, 2003)
address the lifelong RL problem via temporal compositions. These methods are usually focused on
single-goal tasks, where they address the potentially long trajectories needed to reach a desired goal
by composing sub-goal skills sequentially (Levy et al., 2017; Bagaria & Konidaris, 2019). While they
do not consider the multi-goal setting, they can be used in conjunction with concurrent composition
to learn how to achieve a combinatorial amount of desirable long horizon goals. Finally, there are
also non-compositional approaches (Finn et al., 2017; Abel et al., 2018; Singh et al., 2021), which
usually aim to learn the policy for a new task faster by initializing the networks with some pre-training
procedure. These can be used in combination with SOPGOL to learn new skills faster."
CONCLUSION,0.36675461741424803,"6
CONCLUSION"
CONCLUSION,0.36939313984168864,"In this work, we proposed an approach for efﬁcient transfer learning in RL. Our framework, SOPGOL,
leverages the Boolean algebra framework of Nangue Tasse et al. (2020) to determine which skills
should be reused in a new task. We demonstrated that, if a new task is solvable using existing skills,
an agent is able to solve it with no further learning. However, even if this is not the case, an estimate
of the optimal value function can still be obtained to speed up training. This allows agents in a lifelong
learning setting to quickly generalise over any unknown (possibly non-stationary) task distribution."
CONCLUSION,0.3720316622691293,"The main limitation of this work is that it only consider tasks with binary goal rewards—where goals
are either desirable or not. Although this covers a vast number of many-goal tasks, combining our
framework with works on weighted composition (van Niekerk et al., 2019; Barreto et al., 2020) could
enable a similar level of generalisation over tasks with arbitrary goal rewards. Another exciting
avenue for future work would be to extend our transfer learning and generalisation results to include
temporal tasks by leveraging temporal composition approaches like options. Finally, we note that
just like previous work, we rely on the existence of an off-the-shelf RL method that is able to learn
goal-reaching tasks in a given environment. Since that is traditionally very sample inefﬁcient, our
framework can be complemented with other transfer learning methods like MAXQINIT (Abel et al.,
2018) to speed up the learning of new skills (over and above the transfer learning and task space
generalisation shown here). Our approach is a step towards the goal of truly general, long-lived
agents, which are able to generalise both within tasks, as well as over the distribution of possible
tasks it may encounter."
CONCLUSION,0.37467018469656993,Published as a conference paper at ICLR 2022
CONCLUSION,0.37730870712401055,ACKNOWLEDGEMENTS
CONCLUSION,0.37994722955145116,"GNT is supported by an IBM PhD Fellowship. This research was supported, in part, by the National
Research Foundation (NRF) of South Africa under grant number 117808. The content is solely the
responsibility of the authors and does not necessarily represent the ofﬁcial views of the NRF."
CONCLUSION,0.38258575197889183,"The authors acknowledge the Centre for High Performance Computing (CHPC), South Africa, for
providing computational resources to this research project. Computations were also performed using
High Performance Computing Infrastructure provided by the Mathematical Sciences Support unit at
the University of the Witwatersrand."
REFERENCES,0.38522427440633245,REFERENCES
REFERENCES,0.38786279683377306,"D. Abel, Y. Jinnai, S. Y. Guo, G. Konidaris, and M. Littman. Policy and value transfer in lifelong
reinforcement learning. In International Conference on Machine Learning, pp. 20–29, 2018."
REFERENCES,0.39050131926121373,"S. Alver and D. Precup. Constructing a good behavior basis for transfer using generalized policy
updates. In International Conference on Learning Representations, 2022. URL https://
openreview.net/forum?id=7IWGzQ6gZ1D."
REFERENCES,0.39313984168865435,"A. Bagaria and G. Konidaris. Option discovery using deep skill chaining. In International Conference
on Learning Representations, 2019."
REFERENCES,0.39577836411609496,"A. Barreto, W. Dabney, R. Munos, J. Hunt, T. Schaul, H. van Hasselt, and D. Silver. Successor
features for transfer in reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 4055–4065, 2017."
REFERENCES,0.39841688654353563,"A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. Zidek, and
R. Munos. Transfer in deep reinforcement learning using successor features and generalised policy
improvement. In International Conference on Machine Learning, pp. 501–510. PMLR, 2018."
REFERENCES,0.40105540897097625,"A. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup. Fast reinforcement learning with generalized
policy updates. Proceedings of the National Academy of Sciences, 117(48):30079–30087, 2020."
REFERENCES,0.40369393139841686,"A. Barreto, D. Borsa, S. Hou, G. Comanici, E. Aygün, P. Hamel, D. Toyama, J. Hunt, S. Mourad,
D. Silver, et al. The option keyboard: Combining skills in reinforcement learning. arXiv preprint
arXiv:2106.13105, 2021."
REFERENCES,0.40633245382585753,"A. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete event
dynamic systems, 13(1):41–77, 2003."
REFERENCES,0.40897097625329815,"M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for openai gym.
https://github.com/maximecb/gym-minigrid, 2018."
REFERENCES,0.41160949868073876,"C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In International Conference on Machine Learning, pp. 1126–1135. PMLR, 2017."
REFERENCES,0.41424802110817943,"T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. Composable deep reinforcement
learning for robotic manipulation. In 2018 IEEE International Conference on Robotics and
Automation, pp. 6244–6251, 2018."
REFERENCES,0.41688654353562005,"J. Hunt, A. Barreto, T. Lillicrap, and N. Heess. Composing entropic policies using divergence
correction. In International Conference on Machine Learning, pp. 2911–2920, 2019."
REFERENCES,0.41952506596306066,"T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of
Machine Learning Research, 11(Apr):1563–1600, 2010."
REFERENCES,0.42216358839050133,"S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The
Journal of Machine Learning Research, 17(1):1334–1373, 2016."
REFERENCES,0.42480211081794195,"A. Levy, R. Platt, and K. Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 12,
2017."
REFERENCES,0.42744063324538256,Published as a conference paper at ICLR 2022
REFERENCES,0.43007915567282323,"T.. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. In International Conference on Learning Representations,
2016."
REFERENCES,0.43271767810026385,"V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller,
A. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature,
518(7540):529, 2015."
REFERENCES,0.43535620052770446,"G. Nangue Tasse, S. James, and B. Rosman. A Boolean task algebra for reinforcement learning.
Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.43799472295514513,"X. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine. MCP: Learning composable hierarchical
control with multiplicative compositional policies. In Advances in Neural Information Processing
Systems, pp. 3686–3697, 2019."
REFERENCES,0.44063324538258575,"A. Saxe, A. Earle, and B. Rosman. Hierarchy through composition with multitask LMDPs. Interna-
tional Conference on Machine Learning, pp. 3017–3026, 2017."
REFERENCES,0.44327176781002636,"D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):
354, 2017."
REFERENCES,0.44591029023746703,"A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, and S. Levine. Parrot: Data-driven behavioral priors
for reinforcement learning. In International Conference on Learning Representations, 2021."
REFERENCES,0.44854881266490765,"N. Subrahmanyam. Boolean vector spaces. Mathematische Zeitschrift, 83(5):422–433, 1964."
REFERENCES,0.45118733509234826,"R. Sutton, A. Barto, et al. Introduction to reinforcement learning, volume 135. MIT press Cambridge,
1998."
REFERENCES,0.45382585751978893,"R. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artiﬁcial Intelligence, 112(1-2):181–211, 1999."
REFERENCES,0.45646437994722955,"M. Taylor and P. Stone. Transfer learning for reinforcement learning domains: a survey. Journal of
Machine Learning Research, 10:1633–1685, 2009."
REFERENCES,0.45910290237467016,"S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst?
In Advances in neural
information processing systems, pp. 640–646, 1996."
REFERENCES,0.46174142480211083,"E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information Processing
Systems, pp. 1856–1864, 2009."
REFERENCES,0.46437994722955145,"B. van Niekerk, S. James, A. Earle, and B. Rosman. Composing value functions in reinforcement
learning. In International Conference on Machine Learning, pp. 6401–6409, 2019."
REFERENCES,0.46701846965699206,"C. Watkins. Learning from delayed rewards. PhD thesis, King’s College, Cambridge, 1989."
REFERENCES,0.46965699208443273,Published as a conference paper at ICLR 2022
REFERENCES,0.47229551451187335,"A
PROOFS OF THEORETICAL RESULTS"
REFERENCES,0.47493403693931396,"A.1
BOOLEAN ALGEBRA DEFINITION"
REFERENCES,0.47757255936675463,"Deﬁnition 6. A Boolean algebra is a set B equipped with the binary operators ∨(disjunction) and ∧
(conjunction), and the unary operator ¬ (negation), which satisﬁes the following Boolean algebra
axioms for a, b, c in B:"
REFERENCES,0.48021108179419525,(i) Idempotence: a ∧a = a ∨a = a.
REFERENCES,0.48284960422163586,(ii) Commutativity: a ∧b = b ∧a and a ∨b = b ∨a.
REFERENCES,0.48548812664907653,(iii) Associativity: a ∧(b ∧c) = (a ∧b) ∧c and a ∧(b ∨c) = (a ∨b) ∨c.
REFERENCES,0.48812664907651715,(iv) Absorption: a ∧(a ∨b) = a ∨(a ∧b) = a.
REFERENCES,0.49076517150395776,(v) Distributivity: a ∧(b ∨c) = (a ∧b) ∨(a ∧c) and a ∨(b ∧c) = (a ∨b) ∧(a ∨c).
REFERENCES,0.49340369393139843,"(vi) Identity: there exists 0, 1 in B such that"
REFERENCES,0.49604221635883905,"0 ∧a = 0
0 ∨a = a
1 ∧a = a
1 ∨a = 1"
REFERENCES,0.49868073878627966,"(vii) Complements: for every a in B, there exists an element a′ in B such that a ∧a′ = 0 and
a ∨a′ = 1."
REFERENCES,0.5013192612137203,"A.2
PROOFS FOR PROPOSITION 2"
REFERENCES,0.503957783641161,"Lemma 1. Let M be a set of tasks. Then (M, ∨, ∧, ¬, MMAX, MMIN) is a Boolean algebra."
REFERENCES,0.5065963060686016,"Proof. Let M1, M2 ∈M. We show that ¬, ∨, ∧satisfy the Boolean properties (i) – (vii)."
REFERENCES,0.5092348284960422,"(i)–(v): These easily follow from the fact that the min and max functions satisfy the idempotent,
commutative, associative, absorption and distributive laws."
REFERENCES,0.5118733509234829,"(vi): Let rMMAX∧M1 and rM1 be the reward functions for MMAX ∧M1 and M1 respectively.
Then for all (s, a) in S × A,"
REFERENCES,0.5145118733509235,"rMMAX∧M1(s, a) =
min{rMAX, rM1(s, a)},
if s ∈G
min{r0(s, a), r0(s, a)},
otherwise."
REFERENCES,0.5171503957783641,"=
rM1(s, a),
if s ∈G
r0(s, a),
otherwise.
(rM1(s, a) ∈{rMIN, rMAX} for s ∈G)"
REFERENCES,0.5197889182058048,"= rM1(s, a)."
REFERENCES,0.5224274406332454,"Thus MMAX ∧M1 = M1. Similarly MMAX ∨M1 = MMAX, MMIN ∧M1 = MMIN,
and MMIN ∨M1 = M1 . Hence MMIN and MMAX are the universal bounds of M."
REFERENCES,0.525065963060686,"(vii): Let rM1∧¬M1 be the reward function for M1 ∧¬M1. Then for all (s, a) in S × A,"
REFERENCES,0.5277044854881267,"rM1∧¬M1(s, a) =
min{rM1(s, a), (rMAX + rMIN) −rM1(s, a)},
if s ∈G
min{r0(s, a), (r0(s, a) + r0(s, a)) −r0(s, a)},
otherwise. = 
 "
REFERENCES,0.5303430079155673,"rMIN,
if s ∈G and rM1(s, a) = rMAX
rMAX,
if s ∈G and rM1(s, a) = rMIN
r0(s, a),
otherwise."
REFERENCES,0.5329815303430079,"= rMMIN (s, a)."
REFERENCES,0.5356200527704486,"Thus M1 ∧¬M1 = MMIN, and similarly M1 ∨¬M1 = MMAX."
REFERENCES,0.5382585751978892,Published as a conference paper at ICLR 2022
REFERENCES,0.5408970976253298,"Lemma 2. Let
¯Q∗be the set of optimal
¯Q-value functions for tasks in M.
Then
( ¯Q∗, ∨, ∧, ¬, ¯Q∗
MAX, ¯Q∗
MIN) is a Boolean Algebra."
REFERENCES,0.5435356200527705,"Proof. Let ¯Q∗
M1, ¯Q∗
M2 ∈¯Q∗be the optimal ¯Q-value functions for tasks M1, M2 ∈M with reward
functions rM1 and rM2. We show that ¬, ∨, ∧satisfy the Boolean properties (i) – (vii)."
REFERENCES,0.5461741424802111,(i)–(v): These follow directly from the properties of the min and max functions.
REFERENCES,0.5488126649076517,"(vi): For all (s, g, a) in S × G × A,"
REFERENCES,0.5514511873350924,"( ¯Q∗
MAX ∧¯Q∗
M1)(s, g, a) = min{ ¯Q∗
MAX(s, g, a), ¯Q∗
M1(s, g, a)}"
REFERENCES,0.554089709762533,"=
min{ ¯Q∗
MAX(s, g, a), ¯Q∗
MAX(s, g, a)},
if rM1(g, a′) = rMAX ∀a′ ∈A
min{ ¯Q∗
MAX(s, g, a), ¯Q∗
MIN(s, g, a)},
otherwise."
REFERENCES,0.5567282321899736,"=
 ¯Q∗
MAX(s, g, a),
if rM1(g, a) = rMAX ∀a′ ∈A
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.5593667546174143,"= ¯Q∗
M1(s, g, a)
(since rM1(g, a′) ∈{rMIN, rMAX} ∀a′ ∈A)."
REFERENCES,0.5620052770448549,"Similarly, ¯Q∗
MAX ∨¯Q∗
M1 = ¯Q∗
MAX, ¯Q∗
MIN ∧¯Q∗
M1 = ¯Q∗
MIN, and ¯Q∗
MIN ∨¯Q∗
M1 = ¯Q∗
M1."
REFERENCES,0.5646437994722955,"(vii): For all (.) in S × G × A,"
REFERENCES,0.5672823218997362,"( ¯Q∗
M1 ∧¬ ¯Q∗
M1)(.) = min{ ¯Q∗
M1(.), ¬ ¯Q∗
M1(.)}"
REFERENCES,0.5699208443271768,"=
min{ ¯Q∗
MIN(.), ¯Q∗
MAX(.)}
if | ¯Q∗(.) −¯Q∗
MIN(.)| ≤| ¯Q∗(.) −¯Q∗
MAX(.)|
min{ ¯Q∗
MAX(.), ¯Q∗
MIN(.)}
otherwise,"
REFERENCES,0.5725593667546174,"= ¯Q∗
MIN(.)."
REFERENCES,0.575197889182058,"Similarly, ¯Q∗
M1 ∨¬ ¯Q∗
M1 = ¯Q∗
MAX."
REFERENCES,0.5778364116094987,"Lemma 3. Let ¯Q∗be the set of optimal extended ¯Q-value functions for tasks in M.
Then
for all M1, M2 ∈M, we have (i) ¯Q∗
¬M1
= ¬ ¯Q∗
M1, (ii) ¯Q∗
M1∨M2
=
¯Q∗
M1 ∨¯Q∗
M2, and
(iii) ¯Q∗
M1∧M2 = ¯Q∗
M1 ∧¯Q∗
M2."
REFERENCES,0.5804749340369393,"Proof. Let M1, M2 ∈M. Then for all (s, g, a) in S × G × A, (i):"
REFERENCES,0.58311345646438,"¯Q∗
¬M1(s, g, a)"
REFERENCES,0.5857519788918206,"=
 ¯Q∗
MAX(s, g, a),
if r¬M1(g, a′) = rMAX ∀a′ ∈A
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.5883905013192612,"=
 ¯Q∗
MAX(s, g, a),
if rM1(g, a′) = rMIN ∀a′ ∈A
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.5910290237467019,"=
 ¯Q∗
MAX(s, g, a),
if ¯Q∗
M1(s, g, a) = ¯Q∗
MIN(s, g, a)
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.5936675461741425,"=
 ¯Q∗
MAX(s, g, a),
if | ¯Q∗
M1(s, g, a) −¯Q∗
MIN(s, g, a)| ≤| ¯Q∗
M1(s, g, a) −¯Q∗
MAX(s, g, a)|
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.5963060686015831,"= ¬ ¯Q∗
M1(s, g, a)."
REFERENCES,0.5989445910290238,Published as a conference paper at ICLR 2022 (ii):
REFERENCES,0.6015831134564644,"¯Q∗
M1∨M2(s, g, a) =
 ¯Q∗
MAX(s, g, a),
if rM1∨M2(g, a′) = rMAX ∀a′ ∈A
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.604221635883905,"=
 ¯Q∗
MAX(s, g, a),
if max{rM1(g, a′), rM2(g, a′)} = rMAX ∀a′ ∈A
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.6068601583113457,"=
 ¯Q∗
MAX(s, g, a),
if max{ ¯Q∗
M1(s, g, a), ¯Q∗
M2(s, g, a)} = ¯Q∗
MAX(s, g, a)
¯Q∗
MIN(s, g, a),
otherwise."
REFERENCES,0.6094986807387863,"= max{ ¯Q∗
M1(s, g, a), ¯Q∗
M2(s, g, a)}"
REFERENCES,0.6121372031662269,"= ( ¯Q∗
M1 ∨¯Q∗
M2)(s, g, a)."
REFERENCES,0.6147757255936676,(iii): Follows similarly to (ii).
REFERENCES,0.6174142480211082,"Proposition 2. Let ¯Q∗be the set of optimal ¯Q-value functions for tasks in M. Let A : M →¯Q∗
be any map from M to ¯Q∗such that A (M) = ¯Q∗
M for all M in M. Then,"
REFERENCES,0.6200527704485488,"(i) M and ¯Q∗respectively form a Boolean task algebra (M, ∨, ∧, ¬, MMAX, MMIN) and
a Boolean extended value functions algebra ( ¯Q∗, ∨, ∧, ¬, ¯Q∗
MAX, ¯Q∗
MIN),"
REFERENCES,0.6226912928759895,(ii) A is a homomorphism between M and ¯Q∗.
REFERENCES,0.6253298153034301,Proof. (i): Follows from Lemma 1 and 2.
REFERENCES,0.6279683377308707,(ii): Follows from Lemma 3.
REFERENCES,0.6306068601583114,"A.3
PROOFS FOR THEOREM 1"
REFERENCES,0.633245382585752,"Lemma 4. Let ¯Q∗be the set of optimal ¯Q-value functions for tasks in M. Denote ˜¯Q∗
M as the
ϵ-optimal ¯Q-value function for a task M ∈M such that"
REFERENCES,0.6358839050131926,"| ¯Q∗
M(s, g, a) −˜¯Q∗
M(s, g, a)| ≤ϵ for all (s, g, a) ∈S × G × A."
REFERENCES,0.6385224274406333,"Then for all M1, M2 in M and (s, g, a) in S × G × A, (i)"
REFERENCES,0.6411609498680739,"[ ¯Q∗
M1 ∨¯Q∗
M2](s, g, a) −[ ˜¯Q∗
M1 ∨˜¯Q∗
M2](s, g, a)
 ≤ϵ (ii)"
REFERENCES,0.6437994722955145,"[ ¯Q∗
M1 ∧¯Q∗
M2](s, g, a) −[ ˜¯Q∗
M1 ∧˜¯Q∗
M2](s, g, a)
 ≤ϵ (iii)"
REFERENCES,0.6464379947229552,"¬ ¯Q∗
M1(s, g, a) −¬ ˜¯Q∗
M1(s, g, a)
 ≤ϵ"
REFERENCES,0.6490765171503958,"Proof.
(i):
[ ¯Q∗
M1 ∨¯Q∗
M2](s, g, a) −[ ˜¯Q∗
M1 ∨˜¯Q∗
M2](s, g, a)"
REFERENCES,0.6517150395778364,"=

max
M∈{M1,M2}
¯Q∗
M(s, g, a) −
max
M∈{M1,M2}
˜¯Q∗
M(s, g, a)"
REFERENCES,0.6543535620052771,"≤
max
M∈{M1,M2}"
REFERENCES,0.6569920844327177,"¯Q∗
M(s, g, a) −˜¯Q∗
M(s, g, a) ≤ϵ."
REFERENCES,0.6596306068601583,Published as a conference paper at ICLR 2022
REFERENCES,0.662269129287599,"(ii):
[ ¯Q∗
M1 ∧¯Q∗
M2](s, g, a) −[ ˜¯Q∗
M1 ∧˜¯Q∗
M2](s, g, a)"
REFERENCES,0.6649076517150396,"=

min
M∈{M1,M2}
¯Q∗
M(s, g, a) −
min
M∈{M1,M2}
˜¯Q∗
M(s, g, a)"
REFERENCES,0.6675461741424802,"≤
min
M∈{M1,M2}"
REFERENCES,0.6701846965699209,"¯Q∗
M(s, g, a) −˜¯Q∗
M(s, g, a) ≤ϵ."
REFERENCES,0.6728232189973615,"(iii):
¬ ¯Q∗
M1(s, g, a) −¬ ˜¯Q∗
M1(s, g, a) ="
REFERENCES,0.6754617414248021,"(
| ¯Q∗
MAX(s, g, a) −¬ ˜¯Q∗
MIN(s, g, a)|,
if ¯Q∗
M1 = ¯Q∗
MIN(s, g, a)
| ¯Q∗
MIN(s, g, a) −¬ ˜¯Q∗
MAX(s, g, a)|,
otherwise. ="
REFERENCES,0.6781002638522428,"(
| ¯Q∗
MAX(s, g, a) −˜¯Q∗
MAX(s, g, a)|,
if ¯Q∗
M1 = ¯Q∗
MIN(s, g, a)
| ¯Q∗
MIN(s, g, a) −˜¯Q∗
MIN(s, g, a)|,
otherwise. ≤ϵ."
REFERENCES,0.6807387862796834,"Lemma 5. Let M ∈M be a task with reward function r, binary representation T and optimal
extended action-value function ¯Q∗. Given ϵ-approximations of the binary representations ˜Tn =
{ ˜T1, ..., ˜Tn} and optimal ¯Q-functions ˜¯Q∗
n = { ˜¯Q∗
1, ..., ˜¯Q∗
n} for n tasks ˆ
M = {M1, ..., Mn} ⊆M, let"
REFERENCES,0.683377308707124,"TF = BEXP ( ˜Tn) and ¯QF = BEXP ( ˜¯Q∗
n),"
REFERENCES,0.6860158311345647,"where BEXP is derived from
˜Tn and
˜T using a generic method F.
Deﬁne π(s)
∈
arg maxa∈A QF where QF := maxg∈G ¯QF(s, g, a). Then,"
REFERENCES,0.6886543535620053,"∥¯Q∗−¯QF∥∞≤(1T ̸=TF )r∆+ ϵ,"
REFERENCES,0.6912928759894459,"where 1 is the indicator function, r∆:= rMAX −rMIN, and ∥f −h∥∞:= maxs,g,a |f(s, g, a) −
h(s, g, a)|."
REFERENCES,0.6939313984168866,Proof.
REFERENCES,0.6965699208443272,"| ¯Q∗(s, g, a) −¯QF(s, g, a)| = | ¯Q∗(s, g, a) −¯Q∗
F(s, g, a) + ¯Q∗
F(s, g, a) −¯QF(s, g, a)|"
REFERENCES,0.6992084432717678,"≤| ¯Q∗(s, g, a) −¯Q∗
F(s, g, a)| + | ¯Q∗
F(s, g, a) −¯QF(s, g, a)|"
REFERENCES,0.7018469656992085,"≤| ¯Q∗(s, g, a) −¯Q∗
F(s, g, a)| + ϵ.
(Using Lemma 4)"
REFERENCES,0.7044854881266491,"If T = TF, then ¯Q∗(s, g, a) = ¯Q∗
F(s, g, a), and we are done. Let T ̸= TF. Without loss of generality,
let ¯Q∗(s, g, a) = ¯Q∗
MAX(s, g, a) and ¯Q∗
F(s, g, a) = ¯Q∗
MIN(s, g, a). Then,"
REFERENCES,0.7071240105540897,"| ¯Q∗(s, g, a) −¯Q∗
F(s, g, a)| ≤| ¯Q∗
MAX(s, g, a) −¯Q∗
MIN(s, g, a)|
≤r∆."
REFERENCES,0.7097625329815304,"Lemma 6. Let Q∗and ¯Q∗be the optimal Q-value function and optimal extended Q-value function
respectively for a deterministic task in M. Then for all (s, a) in S × A, we have"
REFERENCES,0.712401055408971,"Q∗(s, a) = max
g∈G
¯Q∗(s, g, a)."
REFERENCES,0.7150395778364116,Published as a conference paper at ICLR 2022
REFERENCES,0.7176781002638523,Proof. We ﬁrst note that
REFERENCES,0.7203166226912929,"max
g∈G ¯r(s, g, a) ="
REFERENCES,0.7229551451187335,"(
max{rMIN, r(s, a)},
if s ∈G
max
g∈G r(s, a),
otherwise. = r(s, a).
(4)"
REFERENCES,0.7255936675461742,"Now deﬁne
¯Q∗
max(s, a) := max
g∈G
¯Q∗(s, g, a)."
REFERENCES,0.7282321899736148,"Then it follows that

T ¯Q∗
max

(s, a) = r(s, a) + γ
X"
REFERENCES,0.7308707124010554,"s′∈S
p(s′|s, a) max
a′∈A
¯Q∗
max(s′, a′)"
REFERENCES,0.7335092348284961,"= r(s, a) + γ
X"
REFERENCES,0.7361477572559367,"s′∈S
p(s′|s, a) max
a′∈A"
REFERENCES,0.7387862796833773,"
max
g∈G
¯Q∗(s′, g, a′)
"
REFERENCES,0.741424802110818,"= r(s, a) + γ
X"
REFERENCES,0.7440633245382586,"s′∈S
p(s′|s, a) max
g∈G"
REFERENCES,0.7467018469656992,"
max
a′∈A
¯Q∗(s′, g, a′)
"
REFERENCES,0.7493403693931399,"= r(s, a) + max
g∈G "" γ
X"
REFERENCES,0.7519788918205804,"s′∈S
p(s′|s, a) max
a′∈A
¯Q∗(s′, g, a′) #"
REFERENCES,0.7546174142480211,(Since p is deterministic)
REFERENCES,0.7572559366754618,"= max
g∈G ¯r(s, g, a) + max
g∈G "" γ
X"
REFERENCES,0.7598944591029023,"s′∈S
p(s′|s, a) max
a′∈A
¯Q∗(s′, g, a′) #"
REFERENCES,0.762532981530343,(Using Equation 4)
REFERENCES,0.7651715039577837,"= max
g∈G """
REFERENCES,0.7678100263852242,"¯r(s, g, a) + γ
X"
REFERENCES,0.7704485488126649,"s′∈S
p(s′|s, a) max
a′∈A
¯Q∗(s′, g, a′) # ,"
REFERENCES,0.7730870712401056,"since ¯r(s, g, a) = r0(s, a) ∀s /∈G and p(s, a, ω) = 1 with ¯Q∗(ω, g, a′) = 0 ∀s ∈G."
REFERENCES,0.7757255936675461,"= max
g∈G
¯Q∗(s, g, a)"
REFERENCES,0.7783641160949868,"= ¯Q∗
max(s, a)."
REFERENCES,0.7810026385224275,"Hence ¯Q∗
max is a ﬁxed point of the Bellman optimality operator."
REFERENCES,0.783641160949868,"If s ∈G, then"
REFERENCES,0.7862796833773087,"¯Q∗
max(s, a) = max
g∈G Q∗(s, g, a) = max
g∈G ¯r(s, g, a) = r(s, a) = Q∗(s, a)."
REFERENCES,0.7889182058047494,"Since ¯Q∗
max = Q∗holds in G and ¯Q∗
max is a ﬁxed point of the Bellman operator, then ¯Q∗
max = Q∗
holds everywhere."
REFERENCES,0.7915567282321899,"Theorem 1. Let M ∈M be a task with reward function r, binary representation T and optimal
extended action-value function ¯Q∗. Given ϵ-approximations of the binary representations ˜Tn =
{ ˜T1, ..., ˜Tn} and optimal ¯Q-functions ˜¯Q∗
n = { ˜¯Q∗
1, ..., ˜¯Q∗
n} for n tasks ˆ
M = {M1, ..., Mn} ⊆M, let"
REFERENCES,0.7941952506596306,"TF = BEXP ( ˜Tn) and ¯QF = BEXP ( ˜¯Q∗
n),"
REFERENCES,0.7968337730870713,"where BEXP is derived from
˜Tn and
˜T using a generic method F.
Deﬁne π(s)
∈
arg maxa∈A QF where QF := maxg∈G ¯QF(s, g, a). Then,"
REFERENCES,0.7994722955145118,"(i) ∥Q∗−Qπ∥∞≤
2
1−γ ((1T ̸=TF + 1r /∈{rg}|G|)r∆+ ϵ),"
REFERENCES,0.8021108179419525,"(ii) if the dynamics are deterministic,"
REFERENCES,0.8047493403693932,"∥Q∗−QF∥∞≤(1T ̸=TF )r∆+ ϵ,"
REFERENCES,0.8073878627968337,Published as a conference paper at ICLR 2022
REFERENCES,0.8100263852242744,"where 1 is the indicator function, rg(s, a) := ¯r(s, g, a), r∆:= rMAX −rMIN, and ∥f −h∥∞:=
maxs,g,a |f(s, g, a) −h(s, g, a)|."
REFERENCES,0.8126649076517151,"Proof. (i): We ﬁrst note that each g in G can be thought of as deﬁning an MDP Mg :=
(S, A, p, rg, γ) with reward function rg(s, a) := ¯r(s, g, a), optimal policy π∗
g(s) = ¯π∗(s, g)
and optimal Q-value function Qπ∗
g(s, a) = ¯Q∗(s, g, a). Then this proof follows similarly to that
of Barreto et al. (2017) Theorem 2,"
REFERENCES,0.8153034300791556,"Q∗(s, a) −Qπ(s, a)"
REFERENCES,0.8179419525065963,"≤Q∗(s, a) −Qπ∗
g(s, a) +
2
1 −γ ((1T ̸=TF )r∆+ ϵ)
(Barreto et al. (2017) Theorem 1)"
REFERENCES,0.820580474934037,"≤
2
1 −γ max
s,a |r(s, a) −rg(s, a)| +
2
1 −γ ((1T ̸=TF )r∆+ ϵ)
(Barreto et al. (2017) Lemma 1)"
REFERENCES,0.8232189973614775,"≤
2
1 −γ (1r̸=rg)r∆+
2
1 −γ ((1T ̸=TF )r∆+ ϵ)"
REFERENCES,0.8258575197889182,"(Since rewards only differ in G where r(s, a), rg(s, a) ∈{rMIN, rMAX} for s ∈G)"
REFERENCES,0.8284960422163589,"≤
2
1 −γ ((1T ̸=TF + 1r̸=rg)r∆+ ϵ)."
REFERENCES,0.8311345646437994,"Hence,"
REFERENCES,0.8337730870712401,"∥Q∗−Qπ∥∞≤
2
1 −γ ((1T ̸=TF + min
g
1r̸=rg)r∆+ ϵ)"
REFERENCES,0.8364116094986808,"≤
2
1 −γ ((1T ̸=TF + 1r /∈{rg}|G|)r∆+ ϵ)"
REFERENCES,0.8390501319261213,"(Since min
g
1r̸=rg = 0 only when r ∈{rg}|G| ). (ii):"
REFERENCES,0.841688654353562,"|Q∗(s, a) −QF(s, a)| = | max
g
¯Q∗(s, g, a) −max
g
¯QF(s, g, a)|
(Lemma 6)"
REFERENCES,0.8443271767810027,"≤max
g
| ¯Q∗(s, g, a) −¯QF(s, g, a)|"
REFERENCES,0.8469656992084432,"≤(1T ̸=TF )r∆+ ϵ.
(Lemma 5)"
REFERENCES,0.8496042216358839,"A.4
COMPARING THE BOUNDS OF THEOREM 1 WITH THAT OF GPI IN BARRETO ET AL.
(2018)"
REFERENCES,0.8522427440633246,"We ﬁrst restate Proposition 1 (Barreto et al., 2018) here."
REFERENCES,0.8548812664907651,"Proposition 3 ((Barreto et al., 2018)). Let M ∈M and let Q
π∗
j
i
be the action value function of an
optimal policy of Mj ∈M when executed in Mi ∈M. Given approximations { ˜Qπ1
i , ..., ˜Qπn
i } such
that |Qπj
i
−˜Qπj
i | ≤ϵ for all s, a ∈S × A, and j ∈{1, ..., n}, let"
REFERENCES,0.8575197889182058,"π(s) ∈arg max
a
max
j
˜Qπj
i (s, a). then,"
REFERENCES,0.8601583113456465,"∥Q∗−Qπ∥∞≤
2
1 −γ (∥r −ri∥∞+ min
j
∥ri −rj∥∞+ ϵ),"
REFERENCES,0.862796833773087,"where Q∗is the optimal value function of M, Qπ is the value function of π in M, and ∥f −h∥∞:=
maxs,g,a |f(s, g, a) −h(s, g, a)|."
REFERENCES,0.8654353562005277,Published as a conference paper at ICLR 2022
REFERENCES,0.8680738786279684,We can simplify the bound in Proposition 3 as follows:
REFERENCES,0.8707124010554089,"∥Q∗−Qπ∥∞≤
2
1 −γ (∥r −ri∥∞+ min
j
∥ri −rj∥∞+ ϵ)"
REFERENCES,0.8733509234828496,"≤
2
1 −γ ((1r̸=ri)r∆+ min
j
∥ri −rj∥∞+ ϵ)"
REFERENCES,0.8759894459102903,"(Since rewards only differ in G where r(s, a), ri(s, a) ∈{rMIN, rMAX} for s ∈G)"
REFERENCES,0.8786279683377308,"≤
2
1 −γ ((1r̸=ri)r∆+ (min
j
1ri̸=rj)r∆+ ϵ)"
REFERENCES,0.8812664907651715,"≤
2
1 −γ ((1r̸=ri)r∆+ (1ri /∈{rj}n)r∆+ ϵ)"
REFERENCES,0.8839050131926122,"(Since min
j
1ri̸=rj = 0 only when ri ∈{rj}n )"
REFERENCES,0.8865435356200527,"≤
2
1 −γ ((1r̸=ri + 1ri /∈{rj}n)r∆+ ϵ)."
REFERENCES,0.8891820580474934,"where 1 is the indicator function, and r∆:= rMAX −rMIN. We can see that this bound is similar to
that of Theorem 1(i) but weaker. This because:"
REFERENCES,0.8918205804749341,"(i) The ﬁrst term of this bound (1r̸=ri) requires that reward function of the current task (r) be
identical to that of a reference task (ri). In Barreto et al. (2018), ri is taken as the best linear
approximation of r. In contrast, the ﬁrst term of Theorem 1(i) (1T ̸=TF ) only requires the
current task to be expressible as a Boolean composition of past tasks.
(ii) The second term of this bound (1ri /∈{rj}n) requires that the reference task (the best linear
approximation to the current task) is exactly one of the past tasks. In contrast, the second
term of Theorem 1(i) (1r /∈{rg}|G|) only requires the current task to have a single desirable
goal."
REFERENCES,0.8944591029023746,"This suggests that we can can think of the logical composition approach as an efﬁcient way of doing
GPI, one which leads to tight performance bounds on the transferred policy (Theorem 1(ii))."
REFERENCES,0.8970976253298153,"A.5
PROOFS FOR THEOREM 2"
REFERENCES,0.899736147757256,"Theorem 2. Let D be an unknown distribution, possibly non-stationary, over a set of tasks
M(S, A, p, γ, r0). Let A
: M →¯Q∗be any map from M to ¯Q∗such that A (M) = ¯Q∗
M
for all M in M. Let"
REFERENCES,0.9023746701846965,"˜Tt+1, ˜¯Q∗
t+1 = SOPGOL(A , Mt, ˜Tt, ˜¯Q∗
t ) where Mt ∼D(t) and ˜T0 = ˜¯Q∗
0 = ∅∀t ∈N."
REFERENCES,0.9050131926121372,"Then,
⌈log |G|⌉≤lim
t→∞Nt ≤|G|
where Nt := | ˜Tt| = | ˜¯Q∗
t |."
REFERENCES,0.9076517150395779,"Proof. Let ˜Tt be the approximate binary representation of task Mt learned by SOPGOL. We ﬁrst
note that SOPGOL returns ˜Tt ∪{ ˜Tt} only if ˜Tt is not in the span of ˜Tt. That is,"
REFERENCES,0.9102902374670184,"˜Tt+1 = ˜Tt ∪{ ˜Tt} iff ˜Tt ̸= BEXP ( ˜Tt) where BEXP = SOP( ˜Tt, ˜Tt)."
REFERENCES,0.9129287598944591,"Hence, it is sufﬁcient to show that the number, N, of linearly independent binary vectors, ˜T ∈
{0, 1}|G|, that span the Boolean vector space (Subrahmanyam, 1964), GF(2)|G|,5 is bounded by"
REFERENCES,0.9155672823218998,⌈log |G|⌉≤N ≤|G|.
REFERENCES,0.9182058047493403,"This follows from the fact that ⌈log |G|⌉is the size of a minimal set of generators for GF(2)|G| (as
can easily be seen with a Boolean table), and |G| is its dimensionality."
REFERENCES,0.920844327176781,"5GF(2) is the Galois ﬁeld with two elements, ({0, 1}, +, .), where + := XOR and . := AND."
REFERENCES,0.9234828496042217,Published as a conference paper at ICLR 2022
REFERENCES,0.9261213720316622,"B
SUM OF PRODUCTS WITH GOAL ORIENTED LEARNING"
REFERENCES,0.9287598944591029,"Algorithm 1 shows the full pseudo-code for SOPGOL. Here, SOP( ˜T , ˜T) is the classical sum
of products method in Boolean logic. Given a list of binary vectors T = [ ˜T1, ..., ˜Tn], a Boolean
expression for a new binary vector ˜T is obtained as follows:"
REFERENCES,0.9313984168865436,1. Identify all rows of ˜T with a 1.
REFERENCES,0.9340369393139841,"2. For each such row: Make a product (conjunction) of all the input variables and make the
negation of each variable with a 0 in this row."
REFERENCES,0.9366754617414248,3. Take the sum (disjunction) of all these product terms.
REFERENCES,0.9393139841688655,"The output of SOP is a function BEXP that takes in | ˜T | variables, and applies disjunctions, con-
junctions and negations to them according to the Boolean expression obtained above."
REFERENCES,0.941952506596306,"Algorithm 1: SOPGOL
Input :off-policy RL algorithm A ,
/* e.g DQN */
task MDP M,
set of ϵ-optimal task binary representations ˜T ,
set of ϵ-optimal ¯Q-value functions ˜¯Q.
Initialise ˜T : G →{0, 1}
Initialise ˜¯Q : S × G × A →R according to A
Initialise goal buffer ˜G with terminal states observed from a random policy
while ˜¯Q is not converged do
Initialise state s from M
BEXP ←SOP( ˜T , ˜T)
TSOP , ¯QSOP ←BEXP ( ˜T ), BEXP ( ˜¯Q∗)
¯Q ←¯QSOP if ˜T = TSOP else ˜¯Q ∨¯QSOP"
REFERENCES,0.9445910290237467,"g ←arg max
g′∈˜G"
REFERENCES,0.9472295514511874,"
max
a∈A
¯Q(s, g′, a)
"
REFERENCES,0.9498680738786279,"while s is not terminal do
Select action a using the behaviour policy from A : a ←¯π(s, g) /* e.g ϵ-greedy
*/
Take action a, observe reward r and next state s′ in M
if ˜T ̸= TSOP then
foreach g′ ∈˜G do
¯r ←rMIN if g′ ̸= s ∈˜G else r
Update ˜¯Q with (s, g′, a, ¯r, s′) according to A
end
if s is terminal then
˜T(s) ←1r=rMAX
˜G ←˜G ∪{s}
else
s ←s′
end
end
end
BEXP ←SOP( ˜T , ˜T)
˜T , ˜¯Q ←( ˜T , ˜¯Q) if ˜T = BEXP ( ˜T ) else ( ˜T ∪{ ˜T}, ˜¯Q ∪{ ˜¯Q})
return ˜T , ˜¯Q"
REFERENCES,0.9525065963060686,Published as a conference paper at ICLR 2022
REFERENCES,0.9551451187335093,"C
FOUR ROOMS ENVIRONMENT"
REFERENCES,0.9577836411609498,"We use the Four Rooms domain (Sutton et al., 1999), where an agent must navigate in a grid world to
particular locations. The goal locations are placed along the sides of the walls and at the centre of
rooms (Figure 5). This gives a goal space of size |G| = 40 and a task space of size |M| = 2|G| ≈1012.
The agent can move in any of the four cardinal directions at each timestep, but colliding with a wall
leaves the agent in the same location. We add a 5th action for “stay” that the agent chooses to achieve
goals. A goal location only becomes terminal if the agent chooses to stay in it. All rewards are 0 at
non-terminal states, and 1 at the desirable goals. The transition dynamics are stochastic with a slip
probability (sp = 0.1). That is, with probability 1-sp the agent moves in the direction it chooses, and
with probability sp it moves in one of the other three chosen uniformly at random."
REFERENCES,0.9604221635883905,Figure 5: 40 goals Four Rooms domain with goals in green and the agent in red.
REFERENCES,0.9630606860158312,Published as a conference paper at ICLR 2022
REFERENCES,0.9656992084432717,"D
FUNCTION APPROXIMATION EXPERIMENT DETAILS"
REFERENCES,0.9683377308707124,"D.1
ENVIRONMENT"
REFERENCES,0.9709762532981531,"The PICKUPOBJ environment is fully observable, where each state observation is a 56 ∗56 ∗3 RGB
image (Figure 1). The agent has 7 actions it can take in this environment corresponding to: 1 - rotate
left, 2 - rotate right, 3 - move one step forward if there is no wall or object in front, 4 - pickup object
if there is an object in front and no object has been picked, 5 - drop the object in front if an object has
been picked and there is no wall or object in front, 6 - open the door in front if there is a closed-door
in front, and 7 - close the door in front if there is an opened door in front."
REFERENCES,0.9736147757255936,"For each task, each episode starts with 1 desirable object and 4 other randomly chosen objects placed
randomly in the environment. The agent is also placed at a random position with a random orientation
at the start of each episode. The agent receives a reward of -0.1 at every timestep, and a reward
of 2 when it picks up a desirable object. The environment transitions to a terminal state once the
agent picks up any object and the agent observes the picked object. There are 15 types of objects
(illustrated in Table 1) resulting in 15 possible goal states. Hence, the dimension of the state space is
|S| = 56 × 56 × 3, the goal space is |G| = 15, and the action space is |A| = 7."
REFERENCES,0.9762532981530343,"D.2
NETWORK ARCHITECTURE AND HYPERPARAMETERS"
REFERENCES,0.978891820580475,"In our function approximation experiments, we represent each extended value function ˜¯Q∗with a list
of |G| DQNs, such that the value function for each goal ˜Q∗
g(s, a) := ˜¯Q∗(s, g, a) is approximated with
a separate DQN. The DQNs used have the following architecture, with the CNN part being identical
to that used by Mnih et al. (2015):"
REFERENCES,0.9815303430079155,1. Three convolutional layers:
REFERENCES,0.9841688654353562,"(a) Layer 1 has 3 input channels, 32 output channels, a kernel size of 8 and a stride of 4.
(b) Layer 2 has 32 input channels, 64 output channels, a kernel size of 4 and a stride of 2."
REFERENCES,0.9868073878627969,"(c) Layer 3 has 64 input channels, 64 output channels, a kernel size of 3 and a stride of 1."
REFERENCES,0.9894459102902374,2. Two fully-connected linear layers:
REFERENCES,0.9920844327176781,"(a) Layer 1 has input size 3136 and output size 512 and uses a ReLU activation function.
(b) Layer 2 has input size 512 and output size 7 with no activation function."
REFERENCES,0.9947229551451188,"We used the ADAM optimiser with batch size 256 and a learning rate of 10−3. We started training
after 1000 steps of random exploration and updated the target Q-network every 1000 steps. Finally,
we used ϵ-greedy exploration, annealing ϵ from 0.5 to 0.05 over 100000 timesteps."
REFERENCES,0.9973614775725593,"Finally, we used the same DQN architecture and training hyperparameters for the baseline in all
experiments."
