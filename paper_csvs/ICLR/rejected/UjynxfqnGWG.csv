Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010626992561105207,"Self-attention, an architectural motif designed to model long-range interactions
in sequential data, has driven numerous recent breakthroughs in natural language
processing and beyond. This work provides a theoretical analysis of the inductive
biases of self-attention modules, where our focus is to rigorously establish which
functions and long-range dependencies self-attention blocks prefer to represent.
Our main result shows that bounded-norm Transformer layers create sparse vari-
ables: they can represent sparse functions of the input sequence, with sample
complexity scaling only logarithmically with the context length. Furthermore, we
propose new experimental protocols to support this analysis and to guide the prac-
tice of training Transformers, built around the large body of work on provably
learning sparse Boolean functions."
INTRODUCTION,0.0021253985122210413,"1
INTRODUCTION"
INTRODUCTION,0.003188097768331562,"Self-attention mechanisms have comprised a dramatic paradigm shift in deep learning in recent
years, appearing ubiquitously in recent empirical breakthroughs in sequence modeling and unsuper-
vised representation learning. Starting with large-scale natural language processing (Vaswani et al.,
2017), self-attention has enjoyed surprising empirical successes in numerous and diverse modali-
ties of data. In many of these settings, self-attention has supplanted traditional recurrent and con-
volutional architectures, which are understood to incorporate inductive biases about temporal and
translational invariances in the data. Self-attention models discard these functional forms, in favor
of directly and globally modeling long-range interactions within the input context."
INTRODUCTION,0.004250797024442083,"The proliferation of self-attention raises countless mysteries for theorists and empiricists. One fun-
damental question concerns its statistical properties: How should we think about the inductive biases
of self-attention models? More speciÔ¨Åcally, we can ask: Which functions do self-attention blocks
prefer to represent? How many (approximately) distinct functions can they represent? To this end,
this work initiates an analysis of the statistical foundations of self-attention, as it is used in today‚Äôs
state-of-the-art models."
INTRODUCTION,0.005313496280552604,"Our main technical contribution is a classical norm-based generalization bound for a Transformer
network, which can be extended to related and future architectures via a modular abstraction of at-
tention mechanisms. In particular, the capacity (in terms of log covering number) of the function
class of bounded weight self-attention heads (and Transformers) grows only logarithmically in the
context length, which provides theoretical justiÔ¨Åcation for the empirical observation that attention
models can learn long-term dependencies without overÔ¨Åtting. Next, we show that bounded-norm
self-attention heads are capable of representing wide classes of sparse functions. This representa-
tional capacity result, combined with the generalization results, provides a partial theoretical expla-
nation for the observed sparsity bias of attention models, which we term sparse variable creation."
INTRODUCTION,0.006376195536663124,"We accompany this analysis with an experimental study of the sample complexity needed by Trans-
formers to learn sparse Boolean conjunctions, and verify the sample complexity scaling law pre-
dicted by the theory in this clean synthetic setting. We discuss how to extend and repurpose this
experimental protocol of benchmarking long-context sequence models on synthetic ‚Äúcryptographic‚Äù
tasks. We Ô¨Ånd that Transformers trained with gradient descent can learn sparse parities with noise,
which may be of independent interest, exposing the empirical study of Transformers to the rich
theory established around this problem."
INTRODUCTION,0.007438894792773645,Under review as a conference paper at ICLR 2022
RELATED WORK,0.008501594048884165,"1.1
RELATED WORK"
RELATED WORK,0.009564293304994687,"The direct precursors to modern self-attention architectures were recurrent and convolutional net-
works augmented with attention mechanisms (Bahdanau et al., 2014; Luong et al., 2015; Xu et al.,
2015). Landmark work by Vaswani et al. (2017) demonstrated signiÔ¨Åcantly improved machine trans-
lation models via self-attention only; autoregressive language models followed shortly (Liu et al.,
2018; Radford et al., 2018; 2019; Brown et al., 2020), as well as self-supervised representation
learning (Devlin et al., 2018). More recently, self-attention has demonstrated promise in computer
vision (Dosovitskiy et al., 2020), protein folding (Jumper et al., 2021), theorem proving (Polu &
Sutskever, 2020), program synthesis (Chen et al., 2021b), and reinforcement learning (Chen et al.,
2021a; Janner et al., 2021)."
RELATED WORK,0.010626992561105207,"Norm-based generalization bounds. There is a vast body of literature dedicated to establishing
statistical guarantees for neural networks, including VC-dimension and shattering bounds (dating
back to Anthony et al. (1999)). In recent years, generalization bounds have been established for
various architectures under norm bounds including (Bartlett et al., 2017; Neyshabur et al., 2015;
2017; Golowich et al., 2018; Long & Sedghi, 2019; Chen et al., 2019) using covering-based argu-
ments; Jiang et al. (2019) provide an extensive empirical study of how well these bounds predict
generalization in practice. Our work complements these bounds by establishing norm-based bounds
for attention models. Our main results rely on a novel reduction to the ‚Ñì‚àûcovering number bound
for linear function classes given by Zhang (2002)."
RELATED WORK,0.011689691817215728,"Theory for attention models. Our work complements various existing theoretical perspectives on
attention-based models. Vuckovic et al. (2020) formulate a dynamical system abstraction of at-
tention layers, arriving at similar Lipschitz constant calculations to ours (which are coarser-grained,
since they focus on contractivity and stability rather than Ô¨Ånite-sample statistical guarantees). Zhang
et al. (2019); Snell et al. (2021) study idealizations of optimization problems for self-attention
heads. Wei et al. (2021) propose a deÔ¨Ånition of statistically meaningful approximation of func-
tion classes that ties statistical learnability with expressivity, and show that Boolean circuits can
be SM-approximated by Transformers with a sample complexity bound that depends mildly on cir-
cuit depth (rather than context size), using a margin ampliÔ¨Åcation procedure.1 See Appendix D for
a broader survey of related work, including universal function approximation-style results (which
ignore statistical considerations)."
BACKGROUND AND NOTATION,0.012752391073326248,"2
BACKGROUND AND NOTATION"
BACKGROUND AND NOTATION,0.01381509032943677,"Notation. We use d to denote the embedding dimension for input tokens. T denotes the number
of tokens in an input sequence, a.k.a. the context length or context size of an attention mechanism.
And m refers to the number of samples (input sequences) in a data set. ‚à•¬∑ ‚à•2 denotes the spectral
norm for matrices, and ‚à•¬∑ ‚à•p,q denotes the (p, q) matrix norm where the p-norm is over columns
and q-norm over rows. ‚à•¬∑ ‚à•p denotes the ‚Ñìp norm for vectors. For ‚Ñì2-norm, we drop the subscript.
B is generally used to quantify bounds on norms of matrices and L for Lipschitz constants. ‚àÜn‚àí1
denotes the simplex of dimension n, that is, ‚àÜn‚àí1 := {x ‚ààRn : x ‚â•0, ‚à•x‚à•1 = 1}."
BACKGROUND AND NOTATION,0.01487778958554729,"Covering numbers and uniform generalization bounds. Our main technical contribution is a
generalization bound arising from carefully counting the number of functions representable by a
Transformer. This main complexity notion we use is covering number."
BACKGROUND AND NOTATION,0.015940488841657812,We will use the following deÔ¨Ånition of ‚àû-norm covering number adapted from Zhang (2002):
BACKGROUND AND NOTATION,0.01700318809776833,"DeÔ¨Ånition 2.1 (Covering number). For a given class of vector-valued functions F, the covering
number N‚àû(F; Œµ; {z(i)}m
i=1; ‚à•¬∑ ‚à•) is the smallest size of a collection (a cover) C ‚äÇF such that
‚àÄf ‚ààF, ‚àÉbf ‚ààC satisfying maxi ‚à•f(z(i)) ‚àíbf(z(i))‚à•‚â§Œµ. Further, deÔ¨Åne N‚àû(F, Œµ, m, ‚à•¬∑ ‚à•) =
supz(1)...z(m) N‚àû(F; Œµ; z(1), . . . , z(m), ‚à•¬∑ ‚à•)."
BACKGROUND AND NOTATION,0.018065887353878853,"If F is real-valued (instead of vector-valued), we drop the norm from the notation. Furthermore for
functions parameterized by a set of parameters Œò, we exploit the notation to replace F by Œò."
BACKGROUND AND NOTATION,0.019128586609989374,"1Quoting the discussion following Theorem 4.1 of (Wei et al., 2021): ‚ÄúThe correct norm-based Rademacher
complexity bound to use for Transformers is unclear.‚Äù"
BACKGROUND AND NOTATION,0.020191285866099893,"Under review as a conference paper at ICLR 2022 x1
xT y z"
BACKGROUND AND NOTATION,0.021253985122210415,"x2
‚ãØ
x1
xT
x2
‚ãØ"
BACKGROUND AND NOTATION,0.022316684378320937,"y1
yT
y2
‚ãØ xt yt"
BACKGROUND AND NOTATION,0.023379383634431455,"x1
x[ùô≤ùôªùöÇ]
x2
‚ãØ"
BACKGROUND AND NOTATION,0.024442082890541977,"y1
y[ùô≤ùôªùöÇ]
y2
‚ãØ xT"
BACKGROUND AND NOTATION,0.025504782146652496,"yT
linear
yscalar"
BACKGROUND AND NOTATION,0.026567481402763018,"attention head
self-attention layer
scalar self-attention output"
BACKGROUND AND NOTATION,0.02763018065887354,"Figure 1: Diagrams of attention modules described in Section 3: alignment scores (grey edges) de-
termine normalized attention weights (blue), which are used to mix the inputs x1:T . Left: Attention
with a general context. Center: Stackable self-attention layer, where x1:T is the input as well as
the context. Right: Auxiliary [CLS] token to extract a single scalar from a self-attention layer,
providing a real-valued function class for classiÔ¨Åcation or regression tasks."
BACKGROUND AND NOTATION,0.028692879914984058,"Recall from Zhang (2002) that for the class of linear functions, Flin
= {x 7‚Üíw ¬∑ x
:
w ‚ààRd, ‚à•w‚à•s
‚â§BW }, we have the covering number bound of N‚àû(F; Œµ; {x(i)}m
i=1) ‚â§
O
 
B2
XB2
W /Œµ2 ¬∑ log(BXBW m/Œµ

, where ‚à•x(i)‚à•‚â§BX for i ‚àà[m]. Importantly, note that the
covering number has a mild dependence on m, only logarithmic; this logarithmic dependence on m
will be helpful when we turn our analysis to the capacity of attention mechanisms."
BACKGROUND AND NOTATION,0.02975557917109458,"Generalization bounds. This work focuses on providing log-covering number bounds, which de-
termine the generalization error via standard Rademacher complexity and chaining arguments. The
following lemma relates these quantities; we refer the reader to Appendix A.1 for more details."
BACKGROUND AND NOTATION,0.030818278427205102,"Lemma 2.2 (Generalization bound via covering number). Let D be a distribution over X √ó
R and let ‚Ñì
:
R √ó R be a b-bounded loss function that is L-Lipschitz in its Ô¨Årst argu-
ment.
For a given function class F and f ‚ààF, let risk(f; D) := E(x,y)‚àºD[‚Ñì(f(x), y)] and
d
risk
 
f; (z(i), y(i))m
i=1

:=
1
m
Pm
i=1 ‚Ñì(f(z(i)), y(i)). Suppose F satisÔ¨Åes |f| ‚â§A for all f ‚ààF
and log N‚àû(F; Œµ; x(1), . . . , x(m)) ‚â§CF/Œµ2 for all for all x(1), . . . , x(m) ‚ààX m. Then for any
Œ¥ > 0, with probability at least 1 ‚àíŒ¥, simultaneously for all f ‚ààF and some constant c > 0,"
BACKGROUND AND NOTATION,0.031880977683315624,"risk(f; D) ‚àíd
risk

f; (x(i), y(i))m
i=1
 ‚â§4cL r CF m"
BACKGROUND AND NOTATION,0.03294367693942614,"
1 + log

A
p"
BACKGROUND AND NOTATION,0.03400637619553666,"m/CF

+ 2b r"
BACKGROUND AND NOTATION,0.03506907545164718,log(1/Œ¥)
M,0.036131774707757705,"2m
."
ABSTRACTIONS OF ATTENTION AND SELF-ATTENTION,0.03719447396386823,"3
ABSTRACTIONS OF ATTENTION AND SELF-ATTENTION"
ABSTRACTIONS OF ATTENTION AND SELF-ATTENTION,0.03825717321997875,"Attention is not straightforward to deÔ¨Åne ‚Äî unlike other architectural components such as convo-
lutions and residual connections, it is a broader model design principle, with numerous variations
possible. In this section, we present an abstraction of attention mechanisms, guided by the different
manifestations discussed in (Luong et al., 2015), with the goal of enabling a more uniÔ¨Åed and mod-
ular statistical analysis. Subsequently, we show how to represent the Transformer (the predominant
attention-based architecture) as a special case of this formulation. Our goal is not necessarily an
all encompassing formulation of attention mechanisms, but rather an abstraction that helps to guide
general design principles and theoretical analysis."
ATTENTION,0.039319872476089264,"3.1
ATTENTION"
ATTENTION,0.040382571732199786,"Intuitively, we would like to capture the notion that an output variable selects (‚Äúattends to‚Äù) a part
of the input context on which it will depend, based on a learned function of global interactions
(see Figure 1, left). To this end, we deÔ¨Åne an attention head as a function which maps a context
X of T inputs {xt ‚ààX}T
t=1 (e.g. the tokens in a sentence, pixels in an image, or intermediate
activations in a neural network) and a context z ‚ààZ to an output y ‚ààY. In this work, we will
exclusively consider X, Y, Z to be Rd; d is the embedding dimension, and we deÔ¨Åne the matrix of
inputs X = [x1x2 . . . xT ]‚ä§‚ààRT √ód. An attention head uses z to select the inputs in X to which the
output y will ‚Äúattend‚Äù, which we formalize below:"
ATTENTION,0.04144527098831031,"DeÔ¨Ånition 3.1 (Attention head). An attention head is a function f : X ‚ÜíY, speciÔ¨Åed by an ‚Äúalign-
ment score‚Äù function Score : X √ó Z ‚ÜíR parameterized by Œ∏s ‚ààŒòs, normalization function
Norm : RT ‚Üí‚àÜT ‚àí1, and position-wise transformations œÜin : X ‚ÜíV, œÜout : V ‚ÜíY parameter-"
ATTENTION,0.04250797024442083,Under review as a conference paper at ICLR 2022
ATTENTION,0.04357066950053135,"ized by Œ∏in ‚ààŒòin and Œ∏out ‚ààŒòout. The output of an attention head on input X ‚ààX T , z ‚ààZ is"
ATTENTION,0.044633368756641874,"y = œÜout T
X t=1"
ATTENTION,0.04569606801275239,"h
Norm

Score(x1, z; Œ∏s), . . . , Score(xT , z; Œ∏s)
i"
ATTENTION,0.04675876726886291,tœÜin(xt; Œ∏in); Œ∏out !
ATTENTION,0.04782146652497343,"= œÜout

œÜin(X; Œ∏in)‚ä§Norm

Score(x1, z; Œ∏s), . . . , Score(xT , z; Œ∏s)

; Œ∏out
"
ATTENTION,0.048884165781083955,where œÜin(X; Œ∏) = [œÜin(x1; Œ∏) . . . œÜin(xT ; Œ∏)]‚ä§denotes the row-wise application of œÜin.
ATTENTION,0.04994686503719448,"The above deÔ¨Ånition corresponds to leftmost diagram in Figure 1. Here, V is a vector space of input
representations ‚Äúmixed‚Äù by the normalized alignment scores; in this work, we will set V = Rk. A
function class of attention heads is induced by specifying parameter classes for {Œòs, Œòin, Œòout}."
SELF-ATTENTION AND TRANSFORMERS,0.05100956429330499,"3.2
SELF-ATTENTION AND TRANSFORMERS"
SELF-ATTENTION AND TRANSFORMERS,0.052072263549415514,"A self-attention head is a special case of an attention head, in which the context z is one of the
inputs xt themselves: interactions between elements in X are used to select the elements of X on
which f depends. In this case, we will use the term context to denote X. The focus of this work is
to analyze the inductive biases of such a construction. For example, for a self-attention head (see
Figure 1 (center)), we would have that the t-th component is:"
SELF-ATTENTION AND TRANSFORMERS,0.053134962805526036,"yt = œÜout
 
œÜin(X; Œ∏in)‚ä§Norm(Score(X, xt; Œ∏s)); Œ∏out

,"
SELF-ATTENTION AND TRANSFORMERS,0.05419766206163656,"We now deÔ¨Åne the Transformer self-attention architecture as a special case of the above. Since a
Transformer layer has shared parameters between multiple output heads, it will be convenient to
deÔ¨Åne all T outputs of this layer at once.
DeÔ¨Ånition 3.2 (Transformer self-attention layer). A Transformer attention layer is a collection of T
attention heads with outputs y1, . . . , yT , speciÔ¨Åed by the following choices of function classes (with
shared parameters between the heads) where the context for yœÑ is xœÑ.
‚Ä¢ Score(x, xœÑ; {WQ, WK}) := x‚ä§
œÑ WQW ‚ä§
Kx,
WQ, WK ‚ààRd√ók(for output yt)
‚Ä¢ œÜin(x; WV ) := W ‚ä§
V x,
WV ‚ààRd√ók"
SELF-ATTENTION AND TRANSFORMERS,0.05526036131774708,"‚Ä¢ œÜout(x; WC) := W ‚ä§
C œÉ(x),
WC ‚ààRk√ód, LœÉ-Lipschitz function œÉ : R ‚ÜíR applied position-
wise, with œÉ(0) = 0.
‚Ä¢ Norm(x) := softmax (x) =
exp(x)
1‚ä§exp(x)"
SELF-ATTENTION AND TRANSFORMERS,0.0563230605738576,"DeÔ¨Åning Y := [y1y2 . . . yT ]‚ä§‚ààRT √ód and [RowSoftmax(M)]t,: := softmax(Mt,:), we have"
SELF-ATTENTION AND TRANSFORMERS,0.057385759829968117,"Y = œÉ
 
RowSoftmax
 
XWQ(XWK)‚ä§
XWV

WC."
SELF-ATTENTION AND TRANSFORMERS,0.05844845908607864,"Functions from the above class of Transformer layers map RT √ód to itself, so that instances from
this function class can be composed. Although DeÔ¨Ånition 3.2 only contains the ‚Äúself-attention‚Äù
component, it is not merely a simpliÔ¨Åed idealization of the full Transformer architecture used in
practice. We discuss some remaining discrepancies (positional embeddings, layer normalization,
parallel heads, position-wise feedforward networks) in Section 4.3 and the appendix."
SELF-ATTENTION AND TRANSFORMERS,0.05951115834218916,"Extracting scalar outputs from a Transformer. We introduce one more construction: the canoni-
cal way to extract a scalar prediction from the Ô¨Ånal layer of a Transformer. This is the setup used by
the classiÔ¨Åcation modules in BERT (Devlin et al., 2018) and all of its derivatives. For a context of
size T, a Transformer layer with T +1 inputs is constructed, with a special input index [CLS]. The
input at this position is a vector x[CLS] ‚ààRd (which can be considered as a constant, a part of the
input, or a trainable parameter); the output is a linear function w‚ä§y[CLS], for a trainable parameter
w ‚ààRd. This deÔ¨Ånes a class of functions mapping RT √ód ‚ÜíR, parameterized by a Transformer
layer‚Äôs parameters and w, which we call the class of scalar-output Transformers."
CAPACITY MEASURES OF ATTENTION MODULES,0.06057385759829968,"4
CAPACITY MEASURES OF ATTENTION MODULES"
CAPACITY MEASURES OF ATTENTION MODULES,0.061636556854410204,"In this section, we present our main technical results, along with overviews of their proofs. Sec-
tion 4.1 bounds the capacity of a general attention head. Section 4.2 instantiates this bound for the"
CAPACITY MEASURES OF ATTENTION MODULES,0.06269925611052073,Under review as a conference paper at ICLR 2022
CAPACITY MEASURES OF ATTENTION MODULES,0.06376195536663125,"case of a single Transformer self-attention head. Section 4.3 generalizes this bound for full depth-L
Transformer networks. Our sample complexity guarantees scale only logarithmically in the context
length T, providing rigorous grounding for the intuition that the architecture‚Äôs inductive bias selects
sparse functions of the context. Lastly, in Section 4.4, we complement this capacity analysis by
exhibiting classes of functions expressible using low-norm Transformer architectures. Combining
these representation results and corresponding capacity bounds, we coin the term sparse variable
creation to refer to this inductive bias."
CAPACITY MEASURES OF ATTENTION MODULES,0.06482465462274177,"Note: Throughout this section, assume that ‚à•xt‚à•2 ‚â§BX for all t ‚àà[T]. Note that this allows
for the Frobenius norm ‚à•X‚à•F to scale with
‚àö"
CAPACITY MEASURES OF ATTENTION MODULES,0.06588735387885228,"T. The key challenge throughout our analysis is to
avoid incurring factors of norms which take a sum over the t dimension, by analyzing the attention
parameters in appropriately chosen geometries."
CAPACITY OF A GENERAL ATTENTION HEAD,0.0669500531349628,"4.1
CAPACITY OF A GENERAL ATTENTION HEAD"
CAPACITY OF A GENERAL ATTENTION HEAD,0.06801275239107332,"Recall that the attention head architecture can be represented as a function fhead : RT √ód √óRd ‚ÜíRd
parameterized by Œ∏s, Œ∏in, Œ∏out as"
CAPACITY OF A GENERAL ATTENTION HEAD,0.06907545164718384,"fhead(X, z; Œ∏s, Œ∏in, Œ∏out) = œÜout
 
œÜin(X; Œ∏in)‚ä§Norm(Score(X, z; Œ∏s)); Œ∏out

."
CAPACITY OF A GENERAL ATTENTION HEAD,0.07013815090329437,"Denote the corresponding function class by Fhead := {(X, z) 7‚Üífhead(X, z; Œ∏s, Œ∏in, Œ∏out) : Œ∏s ‚àà
Œòs, Œ∏in ‚ààŒòin, Œ∏out ‚ààŒòout}. To convert the vector-valued function class to a scalar output function
class, we deÔ¨Åne Fscalar := {(X, z) 7‚Üíw‚ä§f(X, z) : f ‚ààFhead, w ‚ààRd, ‚à•w‚à•‚â§Bw}."
CAPACITY OF A GENERAL ATTENTION HEAD,0.07120085015940489,"For simplicity, we will focus only on the attention part and assume that œÜout is a Ô¨Åxed function (no
parameters) and w is Ô¨Åxed. It is not hard to handle these even if allowed to be trainable. For the case
of Transformers, we handle this more generally (see Appendix A.6)."
CAPACITY OF A GENERAL ATTENTION HEAD,0.07226354941551541,"Assumption 4.1. We make the following assumptions:
1. œÜout is Lout-Lipschitz in the ‚Ñì2-norm, that is, ‚àÄa, b ‚ààRk, ‚à•œÜout(a) ‚àíœÜout(b)‚à•‚â§Lout‚à•a ‚àíb‚à•.
2. œÜin is Bin-bounded in ‚Ñì2-norm, that is, ‚à•œÜin(a; Œ∏in)‚à•‚â§Bin‚à•a‚à•for all a ‚ààRd and Œ∏in ‚ààŒòin.
3. Norm is continuously differentiable and its Jacobian satisÔ¨Åes ‚àÄŒ∏ ‚ààRT , ‚à•J Norm(Œ∏)‚à•1,1 ‚â§
CNorm."
CAPACITY OF A GENERAL ATTENTION HEAD,0.07332624867162593,"The Jacobian assumption might seem strong. However, softmax (the most commonly used Norm
function) satisÔ¨Åes this with Csoftmax = 2 (see Corollary A.7)."
CAPACITY OF A GENERAL ATTENTION HEAD,0.07438894792773645,"We prove the following bound on the covering number of Fhead for m samples,"
CAPACITY OF A GENERAL ATTENTION HEAD,0.07545164718384698,"Theorem 4.2 (Attention head capacity). Under Assumptions 4.1, the covering number of Fhead
satisÔ¨Åes"
CAPACITY OF A GENERAL ATTENTION HEAD,0.0765143464399575,"log N‚àû

Fhead; Œµ;
n
(X(i), z(i))
om"
CAPACITY OF A GENERAL ATTENTION HEAD,0.077577045696068,"i=1 , ‚à•¬∑ ‚à•2
"
CAPACITY OF A GENERAL ATTENTION HEAD,0.07863974495217853,"‚â§
inf
Œ±‚àà[0,1]"
CAPACITY OF A GENERAL ATTENTION HEAD,0.07970244420828905,"
log N‚àû"
CAPACITY OF A GENERAL ATTENTION HEAD,0.08076514346439957,"
FScore;
Œ±Œµ
CNormLoutBinBX
; {(x(i)
t , z(i))}i‚àà[m],t‚àà[T ] "
CAPACITY OF A GENERAL ATTENTION HEAD,0.0818278427205101,+ log N‚àû
CAPACITY OF A GENERAL ATTENTION HEAD,0.08289054197662062,"
Fin; (1 ‚àíŒ±)Œµ"
CAPACITY OF A GENERAL ATTENTION HEAD,0.08395324123273114,"Lout
; {x(i)
t }i‚àà[m],t‚àà[T ]; ‚à•¬∑ ‚à•2 
,"
CAPACITY OF A GENERAL ATTENTION HEAD,0.08501594048884166,"where FScore = {(x, z) 7‚ÜíScore(x, z; Œ∏s) : Œ∏s ‚ààŒòs}, and Fin = {x 7‚ÜíœÜin(x; Œ∏in) : Œ∏in ‚ààŒòin}."
CAPACITY OF A GENERAL ATTENTION HEAD,0.08607863974495218,"Note that the bound is in terms of the N‚àûcovering number of functions that dependent on dimen-
sions d or k and not T. The effect of T only shows up in the number of samples to cover. The
N‚àûnumber for many classes scales only logarithmically with the number of samples (for eg, linear
functions Zhang (2002)). This is exactly what allows us to get a log T dependence for Transformers."
CAPACITY OF A GENERAL ATTENTION HEAD,0.0871413390010627,"Since w is Ô¨Åxed, an Œµ-covering of Fhead directly gives us an ŒµBw-covering for Fscalar implying,"
CAPACITY OF A GENERAL ATTENTION HEAD,0.08820403825717323,"log N‚àû

Fscalar; Œµ;
n
(X(i), z(i))
om i=1"
CAPACITY OF A GENERAL ATTENTION HEAD,0.08926673751328375,"
‚â§log N‚àû

Fhead; Œµ/Bw;
n
(X(i), z(i))
om"
CAPACITY OF A GENERAL ATTENTION HEAD,0.09032943676939426,"i=1 , ‚à•¬∑ ‚à•2

."
CAPACITY OF A GENERAL ATTENTION HEAD,0.09139213602550478,"Proof overview. In order to prove the bound, we Ô¨Årst show a Lipschitzness property of ftf-head. This
property allows us to construct the cover by using the covers for FScore and Fin."
CAPACITY OF A GENERAL ATTENTION HEAD,0.0924548352816153,Under review as a conference paper at ICLR 2022
CAPACITY OF A GENERAL ATTENTION HEAD,0.09351753453772582,"Lemma 4.3 (‚Ñì‚àû-Lipschitzness of ftf-head). For any Œ∏s, bŒ∏s ‚ààŒòs, Œ∏in, bŒ∏in ‚ààŒòin; for all X ‚ààRT √ód,
such that
X‚ä§
2,‚àû‚â§BX,
fhead(X, z; Œ∏s, Œ∏in, w) ‚àífhead(X, z; bŒ∏s, bŒ∏in, w)"
CAPACITY OF A GENERAL ATTENTION HEAD,0.09458023379383634,"‚â§CNormLoutBinBX
Score(X, z; Œ∏s) ‚àíScore(X, z; bŒ∏s)

‚àû+ Lout
œÜin(X; Œ∏in) ‚àíœÜin(X; bŒ∏in)

2,‚àû."
CAPACITY OF A GENERAL ATTENTION HEAD,0.09564293304994687,"The most crucial part of this proof is to ensure that we do not get a spurious T dependence when
accounting for the attention mechanism. The key observation here is that the attention part of the
network is computed using Norm, whose Jacobian norm is bounded. This allows us to use the mean-
value theorem to move to the maximum (‚Ñì‚àû) error over T tokens instead of sum (‚Ñì1), which could
potentially incur a T factor. Furthermore, this allows us to combine all samples and tokens and
construct a ‚Ñì‚àû-cover for mT samples."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.09670563230605739,"4.2
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.09776833156216791,"Let us now look at the case of a Transformer self-attention head and instantiate the covering bound.
For ease of presentation and to focus on the self-attention part, we collapse WQW ‚ä§
K to a single
matrix (this does not change the representation), set k = d and remove the linear layer WC2. Then
the Transformer self-attention head (for any Ô¨Åxed œÑ) can be described as"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.09883103081827843,"ftf-head(X; WV , WQK) := œÉ
 
W ‚ä§
V X‚ä§softmax
 
XW ‚ä§
QKxœÑ
"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.09989373007438895,"which
is
obtained
from
the
general
formulation
by
setting
the
context
to
be
xœÑ,
Score(X, xœÑ; WQK) = XW ‚ä§
QKxœÑ, Norm = softmax and œÜout = œÉ."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.10095642933049948,"Let us deÔ¨Åne the function class of self-attention heads with bounded norm, Ftf-head := {X 7‚Üí
ftf-head(X; WV , WQK) : ‚à•W T
V ‚à•2,‚àû‚â§B‚àû
V , ‚à•WV ‚à•‚â§BV , ‚à•WQK‚à•2,‚àû‚â§B‚àû
QK}.
Since
WV , WQK have dimensions dependent on d and k, bounding their norms does not hide a T de-
pendence. As before, to convert this vector-valued function class to a scalar output function class,
we deÔ¨Åne Ftf-scalar := {X 7‚Üíw‚ä§f(X) : f ‚ààFtf-head, w ‚ààRd, ‚à•w‚à•‚â§Bw}."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.10201912858660998,We obtain the following bound on the covering number of Ftf-head as a corollary of Theorem 4.2:
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.1030818278427205,"Corollary 4.4. For any Œµ > 0 and X(1), . . . , X(m) ‚ààRT √ód such that
X(i)‚ä§
2,‚àû‚â§BX for all"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.10414452709883103,"i ‚àà[m], the covering number of Ftf-head satisÔ¨Åes"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.10520722635494155,"log N‚àû(Ftf-head; Œµ; X(1), . . . , X(m), ‚à•¬∑ ‚à•2) ‚â≤(dLœÉBX)2 ¬∑"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.10626992561105207,"
(B‚àû
V )
2
3 + (B‚àû
QKBV )
2
3
3"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.1073326248671626,"Œµ2
¬∑ log(mT)"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.10839532412327312,Here ‚â≤hides logarithmic dependencies on quantities besides m and T.
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.10945802337938364,"Our bounds have a logarithmic dependence on T, highlighting the inductive bias of the transformer
towards selecting sparse functions of the context."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.11052072263549416,"Proof overview. The above result follows from bounding the covering numbers of FQK := {z 7‚Üí
x‚ä§
œÑ WQKz : ‚à•WQK‚à•2,‚àû‚â§B‚àû
QK} and FV := {z ‚ÜíW ‚ä§
V z : ‚à•W T
V ‚à•2,‚àû‚â§B‚àû
V , ‚à•WV ‚à•‚â§BV }."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.11158342189160468,"Note that |x‚ä§
œÑ WQKx‚àíx‚ä§
œÑ WQKx| ‚â§‚à•WQKx‚àíWQKx‚à•since ‚à•xœÑ‚à•‚â§1, so the covering number of
FQK is at most the covering number of the class of functions of the form x 7‚ÜíWQKx. Therefore,
a bound on the vector-valued linear function class sufÔ¨Åces to handle both covering numbers. We
derive the following covering bound which gives the desired result."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.1126461211477152,"Lemma 4.5. Let W : {W ‚ààRd1√ód2 : ‚à•W‚à•2,‚àû‚â§B‚àû}, and consider the function class F : {x 7‚Üí
Wx : W ‚ààW}. For any Œµ > 0 and x(1), . . . , x(N) ‚ààRd1 satisfying ‚àÄi ‚àà[N],
x(i) ‚â§BX,"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.11370882040382571,"log N‚àû(F; Œµ; x(1), . . . , x(N); ‚à•¬∑ ‚à•2) ‚â≤(d2B‚àûBX)2"
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.11477151965993623,"Œµ2
¬∑ log(N)."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.11583421891604676,2See Appendix 4.3 for a general analysis.
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.11689691817215728,Under review as a conference paper at ICLR 2022
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.1179596174282678,"The proof of Lemma 4.5 actually proves a somewhat stronger bound for the function class given
by {W ‚ààRd1√ód2 : ‚à•W‚à•2,‚àû‚â§B‚àû, ‚à•W‚à•2,1 ‚â§B1}, with d2B‚àûB1 in the numerator instead of
(d2B‚àû)2, but we have kept the latter formulation for simplicity of presentation."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.11902231668437832,"Finally, we discuss how to account for some important architectural modiÔ¨Åcations."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.12008501594048884,"Positional embeddings. In practice, the permutation-invariant symmetry of a Transformer network
is broken by adding a positional embedding matrix P ‚ààRT √ód to the input X at the Ô¨Årst layer. In
practice, the embedding matrix is often Ô¨Åxed and non-trainable. Our results extend to this setting in
a straightforward way; see Appendix A.4. If these matrices are to be trained from a sufÔ¨Åciently large
class (say,
P ‚ä§
2,‚àû‚â§1), the dependence of the log-covering number on T could become linear."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.12114771519659936,"Multi-head self-attention.
In almost all applications of Transformers, multiple parallel self-
attention heads are used, and their outputs aggregated, to allow for a richer representation. Our anal-
ysis directly extends to this setting; see Appendix A.5 for details. When a single attention head is re-
placed with the sum of H parallel heads, the log-covering number scales up by a factor of poly(H)."
CAPACITY OF A TRANSFORMER SELF-ATTENTION HEAD,0.12221041445270989,"Layer normalization. State-of-the-art Transformer networks are trained with layer normalization
modules (Ba et al., 2016), which is generally understood to aid optimization. We keep a variant of
layer normalization in the covering number analysis‚Äì it proves to be useful in the analysis of full
attention blocks (see Appendix A.6), as it keeps the norm of the embedding of each token bounded.
Removing these layers would lead to a worse dependence on the spectral norm of the matrices."
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.12327311370882041,"4.3
CAPACITY OF DEEP TRANSFORMER NETWORKS"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.12433581296493093,"In this section, we will extend our results for L-layer Transformer blocks. Denote the weights of
layer i by W (i) :=
n
W (i)
Q , W (i)
K , W (i)
V , W (i)
C
o
. Further denote the set of weights up to layer i by"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.12539851222104145,"W 1:i = (W (1), . . . , W i‚àí1). Denote the input representation of layer i by g(i)
tf-block(X; W 1:i). We
inductively deÔ¨Åne g(i)
tf-block : RT √ód ‚ÜíRT √ód starting with g(1)
tf-block(X; W 1:1) = X (the input):"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.12646121147715197,"g(i+1)
tf-block
 
X; W 1:i+1
:= Œ†norm

œÉ

Œ†norm

f

g(i)
tf-block
 
X; W 1:i
; W (i)
W (i)
C
"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.1275239107332625,"with f (Z; {WQ, WK, WV , ¬∑}) := RowSoftmax

ZWQ (ZWK)‚ä§
ZWV ,"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.12858660998937302,"where Œ†norm denotes layer normalization3 applied to each row. We use a slightly modiÔ¨Åed version
of LayerNorm where instead of normalizing to norm 1, we project it to the unit ball. Let us denote
the class of depth-L transformer blocks by"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.12964930924548354,"F(L)
tf-block :=

X ‚Üíg(L+1)
tf-block(X; W 1:L+1) : ‚àÄi ‚àà[L],
W (i)
V

2 ,
W (i)
K W (i)
Q"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.13071200850159406,"‚ä§
2
,
W (i)
C

2 ‚â§C2,"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.13177470775770456,"W (i)
V

2,‚àû,
W (i)
K W (i)
Q"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.13283740701381508,"‚ä§
2,‚àû
,
W (i)
C

2,‚àû‚â§C‚àû ) ."
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.1339001062699256,"To
obtain
a
Ô¨Ånal
scalar
output,
we
use
a
linear
function
of
the
[CLS]
output,
gtf-scalar(X; W 1:L+1, w) = w‚ä§
g
 
X; W 1:L+1"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.13496280552603612,"[CLS],:. Let the scalar output function class be"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.13602550478214664,"F(L)
tf-scalar := {X ‚Üíw‚ä§f(X)[CLS] : f ‚ààF(L)
tf-block, w ‚ààRd, ‚à•w‚à•‚â§Bw}."
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.13708820403825717,"Theorem 4.6 (Theorem A.17 (SimpliÔ¨Åed)). Suppose ‚àÄi ‚àà[m],
X(i)
2,‚àû‚â§BX, then we have"
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.1381509032943677,"log N‚àû(F(L)
tf-block; Œµ; X(1), . . . , X(m)) ‚â≤(C2LœÉ)O(L) ¬∑ d2B2
XB2
wC2
‚àû
Œµ2
¬∑ log(mT)."
CAPACITY OF DEEP TRANSFORMER NETWORKS,0.1392136025504782,"Note that the dependence on T is only logarithmic even for deeper networks. The dependence on
embedding dimension and (2, ‚àû) norms of the weight matrices is quadratic. As long as the spectral
norms of the matrices are bounded by 1 and œÉ is 1-Lipschitz (which holds for sigmoids and ReLUs),
the exponential dependence on L can be avoided."
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14027630180658873,"3Layer normalization allows for the norms of the outputs of each token in each layer to remain bounded by
1. Note that the norm of the entire input can still have a dependence on T. Our results would go through with
a worse dependence on the spectral norms if we were to remove layer norm."
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14133900106269925,Under review as a conference paper at ICLR 2022
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14240170031880978,"4.4
SPARSE VARIABLE CREATION: AN INDUCTIVE BIAS FOR SELF-ATTENTION"
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.1434643995749203,"The above analysis shows that function classes bottlenecked by self-attention mechanisms are
‚Äúsmall‚Äù in terms of the context size. In this section, we answer the converse question: which func-
tions of interest can they express? To this end, we show in this section that sparse Boolean functions
are realizable by bounded-norm Transformers."
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14452709883103082,"Given a Boolean function f : {0, 1}T ‚ÜíR which only depends on s of its inputs, we represent f
using a self-attention head ftf-head composed with a feedforward network fmlp; this is the repeated
block in the standard Transformer architecture. Intuitively, ftf-head can select an s-dimensional sub-
set of inputs to ‚Äúattend to‚Äù, while fmlp memorizes an arbitrary function of these s inputs (requiring
up to ‚âà2s parameters). In the regime of s ‚â™log T (think of ftf-head ‚ó¶fmlp as implementing a single
composable Boolean gate), the corresponding statistical guarantees are meaningful. In order to de-
scribe this combination of sample-efÔ¨Åcient sparsiÔ¨Åcation of rich contexts and subsequent restricted
use of universal function approximation, we coin the term sparse variable creation."
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14558979808714134,"Setup. We consider the classes of Boolean functions f : {0, 1}T ‚ÜíR representable by bounded-
norm scalar-output Transformer heads ftf-scalar : RT √ód ‚ÜíR. To do this, we must Ô¨Årst Ô¨Åx a mapping
from {0, 1}T to RT √ód; we discuss several natural choices in Appendix B.1. The simplest of these
uses a sum of token and positional embeddings X(b)t,: := ebt + vt, for a set of approximately
orthogonal unit vectors {e0, e1, v1, . . . , vT }. After choosing a mapping X(b), the setup of the repre-
sentation problem is evident: given f(b), Ô¨Ånd Transformer weights Œ∏tf-head and feedforward network
weights Œ∏mlp such that"
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14665249734325186,"ftf+mlp(X(b); Œ∏tf-head, Œ∏mlp) := fmlp (ftf-head(X(b); Œ∏tf-head); Œ∏mlp) ‚âàf(b),
‚àÄb ‚àà{0, 1}T ."
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14771519659936239,"Main representational results. We show that Transformer blocks can represent I-sparse Boolean
functions, whose values only depend on some subset of indices I ‚äÜ[T]. We present informal
statements of these approximation results below, and present the precise statements in Appendix B.2."
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.1487778958554729,"Proposition 4.7 (Sparse variable creation via Transformers; informal). Under any of the input map-
pings X(b), we have the following guarantees:
‚Ä¢ ftf-scalar alone can approximate a particular monotone symmetric s-sparse Boolean function, with
weight norms ‚à•WQ‚à•F ‚â§O (log(Ts)) , ‚à•WK‚à•F , ‚à•WV ‚à•F , ‚à•WC‚à•F ‚â§O(s).
‚Ä¢ ftf+mlp can exactly represent symmetric s-sparse functions, with the same Transformer weight
norms as above; the feedforward network weights satisfy ‚à•W1‚à•F , ‚à•W2‚à•F , ‚à•w‚à•F ‚â§O(poly(s)).
‚Ä¢ ftf+mlp can exactly represent general s-sparse functions, with the same Transformer weight norms
as above; the feedforward network weights satisfy ‚à•W1‚à•F , ‚à•W2‚à•F , ‚à•w‚à•F ‚â§O(2s ¬∑ poly(s))."
LAYER NORMALIZATION ALLOWS FOR THE NORMS OF THE OUTPUTS OF EACH TOKEN IN EACH LAYER TO REMAIN BOUNDED BY,0.14984059511158343,"Proof ideas. Each construction uses the same idea: select WQ, WK so that the attention mixture
weights approximate the uniform distribution over the relevant positions, then use the ReLU network
to memorize all distinct values of f. Full proofs are given in Appendix B.4."
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.15090329436769395,"5
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.15196599362380447,"Our theoretical analysis has shown that Transformers can represent sparse Boolean functions, with
sample complexity scaling mildly with the context size. In this section, we present an empirical
study of whether Transformer architectures (as they are trained and used in state-of-the-art language
modeling) exhibit these scalings in practice."
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.153028692879915,"We introduce a rigorous benchmark for probing the empirical sample complexity of a Transformer:
attribute-efÔ¨Åcient learning of a planted sparse Boolean function. We choose a family of distinct
distributions {D1, . . . , DN} on {0, 1}T √ó {0, 1}, corresponding to supervised learning problems,
such that the feature distributions are identical, and the uniform mixture
1
N
PN
i=1 Di is invariant
under all permutations of the indices 1, . . . , T. We then select an i‚àó‚àà[N] uniformly at random,
train a Transformer binary classiÔ¨Åer on m samples from Di‚àó, then evaluate the generalization error
via cross-validation. Since the architecture, initialization, training algorithm, and training data are
permutation-invariant, at least ‚Ñ¶(log N) samples are required to learn this distribution: one sample
can only reveal a single bit of information about i‚àó, via its binary label. We are interested in the
empirical scaling of the sufÔ¨Åcient sample size m to solve this problem, in terms of N."
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.15409139213602552,Under review as a conference paper at ICLR 2022
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.155154091392136,"60
80
100
120
140
160
180
200
sample size 0.0 0.2 0.4 0.6 0.8 1.0"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.15621679064824653,Pr[val acc > 0.99]
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.15727948990435706,success probabilities
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.15834218916046758,"T=100
T=200
T=400
T=800"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.1594048884165781,"102
2 √ó 102
3 √ó 1024 √ó 102
6 √ó 102"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.16046758767268862,context length T 40 60 80 100 120 140 160 180 200
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.16153028692879914,critical sample size
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.16259298618490967,empirical sample complexity
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.1636556854410202,"0
100
200
300
400
500 0.0 0.2 0.4 0.6 0.8"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.1647183846971307,training log loss
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.16578108395324123,example training curves (T=400)
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.16684378320935175,"m=200
m=100 (overfit)"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.16790648246546228,"0
100
200
300
400
500
training iterations 0.7 0.8 0.9 1.0"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.1689691817215728,validation accuracy
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.17003188097768332,"Figure 2: Statistically probing a Transformer by training it on a 3-way AND of a hidden subset
of i.i.d. random bits. Left, center: Sublinear scaling of the empirical sample complexity. Right:
Example training curves in the {overÔ¨Åtting, correct} regimes: T = 400, m = {100, 200}."
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.17109458023379384,"0
100
200
300
400
500
training iterations 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.17215727948990436,log loss
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.17321997874601489,3-way parity of T=10 bits
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.1742826780021254,"0
250
500
750
1000
1250
1500
1750
2000
training iterations 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.17534537725823593,3-way parity of T=15 bits
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.17640807651434645,"0
1000
2000
3000
4000
5000
training iterations 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.17747077577045697,3-way parity of T=20 bits
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.1785334750265675,"Figure 3: A curious empirical Ô¨Ånding: Transformers can learn sparse parities. 10 loss curves (with
online batches) are given for this setup with s = 3, T = {10, 15, 20}, showing abrupt phase transi-
tions from random guessing to perfect classiÔ¨Åcation. See Appendix C.2 for details."
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.179596174282678,"Learning sparse AND gates. The simplest instantiation of this experimental framework is a hidden
s-way AND under the uniform distribution (i.e. T i.i.d. random bits). The Transformer must learn
which of the N =
 T
s

combinations of inputs determines the label, which requires m ‚â•‚Ñ¶(s log T)
samples. The theory predicts the sample complexity of learning a bounded-norm Transformer should
match this scaling in T. With hyperparameters typical of Transformer setups used for natural data,
we indeed observe that the empirical sample complexity scales sublinearly with T. Figure 2 sum-
marizes our Ô¨Åndings; details are provided in Appendix C.1."
EMPIRICAL SCALING LAWS FOR LEARNING BOOLEAN GATES,0.1806588735387885,"Learning sparse parities. We can also replace the sparse AND operation with XOR: the label is
the parity of a hidden subset of input bits. This variant emphasizes the ‚Äúcryptographic‚Äù nature of
this experimental setup, due to its known computational hardness. ‚Ñ¶(T s) statistical queries (thus,
batch gradient descent steps) are necessary (Kearns, 1998); in the presence of noise, the fastest
known algorithms for learning parities with noise require T ‚Ñ¶(s) time (Valiant, 2012). Figure 3
(with details in Appendix C.2) show that Transformer models can Ô¨Åt sparse parities. This raises an
intriguing question: if theory suggests that ‚Äúexhaustive search-like‚Äù methods are computationally
necessary for this problem, why does local search (i.e. gradient-based training) succeed? We leave
this computational (as opposed to statistical) mystery as an open direction for future work."
CONCLUSION AND FUTURE WORK,0.18172157279489903,"6
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.18278427205100956,"We have presented a theoretical analysis of the inductive biases of self-attention models, Ô¨Ånding
that they can learn sparse Boolean functions with sample complexity scaling logarithmically in the
context length. We call this phenomenon sparse variable creation. Our analysis is accompanied
by new empirical probes involving training Transformers on sparse Boolean functions, where we
corroborate this scaling of the sample complexity in practice. We believe our capacity bounds are
improvable (we have only sought to obtain an optimal dependence on T). Incorporating aspects of
computation and depth remains a perennial challenge in this line of inquiry."
CONCLUSION AND FUTURE WORK,0.18384697130712008,"Building further upon the principles of attention constitutes a vibrant frontier of empirical research
(Tolstikhin et al., 2021; Lee-Thorp et al., 2021; Jaegle et al., 2021b;a; d‚ÄôAscoli et al., 2021). We
hope that the theoretical foundations and experimental probes presented in this paper will assist
in further expanding the breadth of applications of self-attention, and developing more compute-
efÔ¨Åcient, data-efÔ¨Åcient, controllable, and reliable algorithmic interventions."
CONCLUSION AND FUTURE WORK,0.1849096705632306,Under review as a conference paper at ICLR 2022
REFERENCES,0.18597236981934112,REFERENCES
REFERENCES,0.18703506907545164,"Martin Anthony, Peter L Bartlett, and Peter L Bartlett. Neural network learning: Theoretical foun-
dations, volume 9. cambridge university press Cambridge, 1999."
REFERENCES,0.18809776833156217,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.1891604675876727,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014."
REFERENCES,0.1902231668437832,"Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017."
REFERENCES,0.19128586609989373,"Peter L. Bartlett and Shahar Mendelson.
Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research, 3:463‚Äì482, 2002. URL http:
//www.jmlr.org/papers/v3/bartlett02a.html."
REFERENCES,0.19234856535600425,"Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers
to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020a."
REFERENCES,0.19341126461211477,"Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers
and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020b."
REFERENCES,0.1944739638682253,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.19553666312433582,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. arXiv preprint arXiv:2106.01345, 2021a."
REFERENCES,0.19659936238044634,"Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Ed-
wards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021b."
REFERENCES,0.19766206163655686,"Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural
networks. arXiv preprint arXiv:1910.12947, 2019."
REFERENCES,0.19872476089266738,"Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does BERT look
at? an analysis of BERT‚Äôs attention. arXiv preprint arXiv:1906.04341, 2019."
REFERENCES,0.1997874601487779,"George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303‚Äì314, 1989."
REFERENCES,0.20085015940488843,"St¬¥ephane d‚ÄôAscoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.
Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint
arXiv:2103.10697, 2021."
REFERENCES,0.20191285866099895,"Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and ≈Åukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018."
REFERENCES,0.20297555791710944,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.20403825717321997,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020."
REFERENCES,0.2051009564293305,"Richard M Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian pro-
cesses. Journal of Functional Analysis, 1(3):290‚Äì330, 1967."
REFERENCES,0.206163655685441,"Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297‚Äì299. PMLR, 2018."
REFERENCES,0.20722635494155153,Under review as a conference paper at ICLR 2022
REFERENCES,0.20828905419766205,"Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio,
and Bernhard Sch¬®olkopf. Recurrent independent mechanisms. In International Conference on
Learning Representations, 2020."
REFERENCES,0.20935175345377258,"Anirudh Goyal, Aniket Didolkar, Nan Rosemary Ke, Charles Blundell, Philippe Beaudoin, Nicolas
Heess, Michael C Mozer, and Yoshua Bengio. Neural production systems. Advances in Neural
Information Processing Systems, 34, 2021."
REFERENCES,0.2104144527098831,"Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359‚Äì366, 1989."
REFERENCES,0.21147715196599362,"Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. InÔ¨Ånite attention: Nngp and
ntk for deep attention networks. In International Conference on Machine Learning, pp. 4376‚Äì
4386. PMLR, 2020."
REFERENCES,0.21253985122210414,"Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David
Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A
general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021a."
REFERENCES,0.21360255047821466,"Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira.
Perceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021b."
REFERENCES,0.2146652497343252,"Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence mod-
eling problem. arXiv preprint arXiv:2106.02039, 2021."
REFERENCES,0.2157279489904357,"Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to Ô¨Ånd them. arXiv preprint arXiv:1912.02178, 2019."
REFERENCES,0.21679064824654623,"William B Johnson, Joram Lindenstrauss, and Gideon Schechtman. Extensions of lipschitz maps
into banach spaces. Israel Journal of Mathematics, 54(2):129‚Äì138, 1986."
REFERENCES,0.21785334750265675,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ÀáZ¬¥ƒ±dek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583‚Äì589, 2021."
REFERENCES,0.21891604675876727,"Michael Kearns.
EfÔ¨Åcient noise-tolerant learning from statistical queries.
Journal of the ACM
(JACM), 45(6):983‚Äì1006, 1998."
REFERENCES,0.2199787460148778,"Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal, Kyle Goyette, Yoshua Bengio, and Guillaume
Lajoie. Untangling tradeoffs between recurrence and self-attention in artiÔ¨Åcial neural networks.
Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.22104144527098832,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.22210414452709884,"James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with
fourier transforms. arXiv preprint arXiv:2105.03824, 2021."
REFERENCES,0.22316684378320936,"Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of
self-attention matrices. arXiv preprint arXiv:2106.03764, 2021."
REFERENCES,0.22422954303931988,"Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer.
Generating wikipedia by summarizing long sequences.
arXiv preprint
arXiv:1801.10198, 2018."
REFERENCES,0.2252922422954304,"Philip M Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks.
arXiv preprint arXiv:1905.12600, 2019."
REFERENCES,0.2263549415515409,"Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal
computation engines. arXiv preprint arXiv:2103.05247, 2021."
REFERENCES,0.22741764080765142,"Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. arXiv preprint arXiv:1508.04025, 2015."
REFERENCES,0.22848034006376194,Under review as a conference paper at ICLR 2022
REFERENCES,0.22954303931987247,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376‚Äì1401. PMLR, 2015."
REFERENCES,0.230605738575983,"Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks.
arXiv preprint arXiv:1707.09564,
2017."
REFERENCES,0.2316684378320935,"Michael A Nielsen. Neural networks and deep learning, volume 25. Determination press San
Francisco, CA, 2015."
REFERENCES,0.23273113708820403,"Ryan O‚ÄôDonnell. Analysis of boolean functions. arXiv preprint arXiv:2105.10386, 2021."
REFERENCES,0.23379383634431455,"Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.
arXiv preprint arXiv:2009.03393, 2020."
REFERENCES,0.23485653560042508,"Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gener-
alization beyond overÔ¨Åtting on small algorithmic datasets. In ICLR MATH-AI Workshop, 2021."
REFERENCES,0.2359192348565356,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. 2018."
REFERENCES,0.23698193411264612,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.23804463336875664,"Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about
how bert works. Transactions of the Association for Computational Linguistics, 8:842‚Äì866, 2020."
REFERENCES,0.23910733262486716,"Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of
computer and system sciences, 50(1):132‚Äì150, 1995."
REFERENCES,0.24017003188097769,"Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head
attention learns. arXiv preprint arXiv:2103.07601, 2021."
REFERENCES,0.2412327311370882,"Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efÔ¨Åcient
transformers. arXiv preprint arXiv:2011.04006, 2020."
REFERENCES,0.24229543039319873,"Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. arXiv
preprint arXiv:1905.05950, 2019."
REFERENCES,0.24335812964930925,"Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021."
REFERENCES,0.24442082890541977,"Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and
juntas. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science, pp. 11‚Äì20.
IEEE, 2012."
REFERENCES,0.2454835281615303,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998‚Äì6008, 2017."
REFERENCES,0.24654622741764082,"James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of atten-
tion. arXiv preprint arXiv:2007.02876, 2020."
REFERENCES,0.24760892667375134,"Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on
approximating turing machines with transformers. arXiv preprint arXiv:2107.13163, 2021."
REFERENCES,0.24867162592986186,"Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048‚Äì2057. PMLR, 2015."
REFERENCES,0.24973432518597238,"Greg Yang.
Tensor programs ii: Neural tangent kernel for any architecture.
arXiv preprint
arXiv:2006.14548, 2020."
REFERENCES,0.2507970244420829,Under review as a conference paper at ICLR 2022
REFERENCES,0.2518597236981934,"Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions?
arXiv preprint
arXiv:1912.10077, 2019."
REFERENCES,0.25292242295430395,"Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? arXiv preprint
arXiv:1912.03194, 2019."
REFERENCES,0.25398512221041447,"Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal of
Machine Learning Research, 2(Mar):527‚Äì550, 2002."
REFERENCES,0.255047821466525,Under review as a conference paper at ICLR 2022
REFERENCES,0.2561105207226355,"A
PROOFS OF CAPACITY BOUNDS"
REFERENCES,0.25717321997874604,"In this section we present the full proofs (including the omitted proofs) of our capacity bounds. We
also cover relevant background and useful technical lemmas."
REFERENCES,0.25823591923485656,"A.1
RADEMACHER COMPLEXITY AND GENERALIZATION BOUNDS"
REFERENCES,0.2592986184909671,"Here we brieÔ¨Çy review Rademacher complexity and its relationship to covering numbers and gener-
alization bounds. We refer the reader to Bartlett & Mendelson (2002) for a more detailed exposition.
DeÔ¨Ånition A.1 (Empirical Rademacher complexity). For a given class of functions F = {f : X ‚Üí
R} and {z(i) ‚ààX}m
i=1, the empirical Rademacher complexity bR(F; z(1), . . . , z(m)) is deÔ¨Åned as"
REFERENCES,0.2603613177470776,"bR(F; z(1), . . . , z(m)) = 1 mEŒµ """
REFERENCES,0.2614240170031881,"sup
f‚ààF m
X"
REFERENCES,0.2624867162592986,"i=1
Œµif(z(i)) # ,"
REFERENCES,0.2635494155154091,where Œµ is a vector of m i.i.d. Rademacher random variables (Pr[Œµi = 1] = Pr[Œµi = ‚àí1] = 1/2).
REFERENCES,0.26461211477151964,"In order to relate the Rademacher complexity and ‚Ñì‚àû-covering numbers, we use a modiÔ¨Åed version
of Dudley‚Äôs metric entropy.
Lemma A.2 (Dudley (1967); modiÔ¨Åed). Consider a real-valued function class F such that |f| ‚â§A
for all f ‚ààF. Then"
REFERENCES,0.26567481402763016,"bR(F; z(1), . . . , z(m)) ‚â§c ¬∑ inf
Œ¥‚â•0 "
REFERENCES,0.2667375132837407,"Œ¥ +
Z A Œ¥ r"
REFERENCES,0.2678002125398512,"log N‚àû(F; Œµ; z(1), . . . , z(m)) m
dŒµ !"
REFERENCES,0.2688629117959617,for some constant c > 0.
REFERENCES,0.26992561105207225,"Proof sketch. The original statement is for 2-norm covering number, but the ‚àû-norm case reduces
to the 2-norm case because N2(¬∑) ‚â§N‚àû(¬∑). The original statement also Ô¨Åxes Œ¥ = 0 rather than
taking an inÔ¨Åmum. Also, the standard statement has the integral go from 0 to ‚àû, but these are easily
replaced with Œ¥ and A."
REFERENCES,0.27098831030818277,"For our paper, we will instantiate the above lemma for log covering numbers scaling as 1/Œµ2.
Corollary A.3 (Rademacher Complexity via covering number). Consider a real-valued function
class F such that |f| ‚â§A for all f ‚ààF. Suppose log N‚àû(F; Œµ; z(1), . . . , z(m)) ‚â§CF/Œµ2, then"
REFERENCES,0.2720510095642933,"bR(F; z(1), . . . , z(m)) ‚â§c ¬∑ r CF"
REFERENCES,0.2731137088204038,"m ¬∑

1 + log

A
p"
REFERENCES,0.27417640807651433,"m/CF
"
REFERENCES,0.27523910733262485,for some constant c > 0.
REFERENCES,0.2763018065887354,"Proof. Using Lemma A.2, we have for some constant c > 0,"
REFERENCES,0.2773645058448459,"bR(F; z(1), . . . , z(m)) ‚â§c inf
Œ¥‚â•0 "
REFERENCES,0.2784272051009564,"Œ¥ +
Z A Œ¥ r"
REFERENCES,0.27948990435706694,"log N‚àû(F; Œµ; z(1), . . . , z(m)) m
dŒµ !"
REFERENCES,0.28055260361317746,"‚â§c inf
Œ¥‚â•0 "
REFERENCES,0.281615302869288,"Œ¥ +
Z A Œ¥ r"
REFERENCES,0.2826780021253985,"CF
Œµ2m dŒµ !"
REFERENCES,0.28374070138150903,"= c inf
Œ¥‚â•0  Œ¥ + r CF m Z A Œ¥"
REFERENCES,0.28480340063761955,"1
Œµ dŒµ !"
REFERENCES,0.2858660998937301,"= c inf
Œ¥‚â•0  Œ¥ + r CF"
REFERENCES,0.2869287991498406,m log(A/Œ¥) ! = c r CF m
REFERENCES,0.2879914984059511,"
1 + log

A
p"
REFERENCES,0.28905419766206164,"m/CF

."
REFERENCES,0.29011689691817216,Under review as a conference paper at ICLR 2022
REFERENCES,0.2911795961742827,"We can now obtain a generalization guarantee from the Rademacher complexity of a function class:
Theorem A.4 (Bartlett & Mendelson (2002)). Let D be a distribution over X √ó R and let
‚Ñì: R √ó R be a b-bounded loss function that is L-Lipschitz in its Ô¨Årst argument. For a given func-
tion class F and f ‚ààF, let risk(f; D) := E(x,y)‚àºD[‚Ñì(f(x), y)] and d
risk
 
f; (z(i), y(i))m
i=1

:=
1
m
Pm
i=1 ‚Ñì(f(z(i)), y(i)). Then for any Œ¥ > 0, with probability at least 1 ‚àíŒ¥, simultaneously for all
f ‚ààF,"
REFERENCES,0.2922422954303932,"risk(f; D) ‚àíd
risk

f; (z(i), y(i))m
i=1
 ‚â§4L bR

F; z(1), . . . , z(m)
+ 2b r"
REFERENCES,0.29330499468650373,log(1/Œ¥)
M,0.29436769394261425,"2m
."
M,0.29543039319872477,"Combining the above, we get:
Lemma A.5 (Lemma 2.2 (restated)). Consider a function class F such that |f| ‚â§A for all f ‚ààF
and log N‚àû(F; Œµ; x(1), . . . , x(m)) ‚â§CF/Œµ2 for all x(1), . . . , x(m) ‚ààX m. Then for any Œ¥ > 0, with
probability at least 1 ‚àíŒ¥, simultaneously for all f ‚ààF,"
M,0.2964930924548353,"risk(f; D) ‚àíd
risk

f; (x(i), y(i))m
i=1
 ‚â§4cL r CF m"
M,0.2975557917109458,"
1 + log

A
p"
M,0.29861849096705634,"m/CF

+ 2b r"
M,0.29968119022316686,log(1/Œ¥)
M,0.3007438894792774,"2m
,"
M,0.3018065887353879,for some constant c > 0.
M,0.3028692879914984,"A.2
USEFUL LEMMAS"
M,0.30393198724760895,"Lemma A.6. Consider function f : Rd ‚Üí‚àÜd‚àí1 such that the Jacobian of the function satisÔ¨Åes
‚à•J f(Œ∏)‚à•1,1 ‚â§cf for all Œ∏ ‚ààRd, then for any vectors Œ∏1, Œ∏2 ‚ààRp,"
M,0.30499468650371947,‚à•f(Œ∏1) ‚àíf(Œ∏2)‚à•1 ‚â§cf‚à•Œ∏1 ‚àíŒ∏2‚à•‚àû.
M,0.30605738575983,"Proof. By the fundamental theorem of calculus applied to g(t) = f(tŒ∏1 + (1 ‚àít)Œ∏2), followed by
a change of variables:"
M,0.3071200850159405,"f(Œ∏1) ‚àíf(Œ∏2) =
Z 1"
M,0.30818278427205104,"0
J (tŒ∏1 + (1 ‚àít)Œ∏2) dt

(Œ∏1 ‚àíŒ∏2),"
M,0.30924548352816156,We have
M,0.310308182784272,‚à•f(Œ∏1) ‚àíf(Œ∏2)‚à•1 = Z 1
M,0.31137088204038255,"0
J (tŒ∏1 + (1 ‚àít)Œ∏2) (Œ∏1 ‚àíŒ∏2)dt

1
By Jensen‚Äôs inequality: ‚â§
Z 1"
M,0.31243358129649307,"0
‚à•J (tŒ∏1 + (1 ‚àít)Œ∏2) (Œ∏1 ‚àíŒ∏2)‚à•1 dt"
M,0.3134962805526036,"Using ‚à•Ax‚à•1 ‚â§‚à•A‚à•1,1 ‚à•x‚à•‚àû: ‚â§
Z 1"
M,0.3145589798087141,"0
‚à•J (tŒ∏1 + (1 ‚àít)Œ∏2)‚à•1,1 ‚à•Œ∏1 ‚àíŒ∏2‚à•‚àûdt"
M,0.31562167906482463,"By assumption on the Jacobian:
‚â§cf ‚à•Œ∏1 ‚àíŒ∏2‚à•‚àû."
M,0.31668437832093516,"Corollary A.7. For vectors Œ∏1, Œ∏2 ‚ààRp, ‚à•softmax(Œ∏1) ‚àísoftmax(Œ∏2)‚à•1 ‚â§2‚à•Œ∏1 ‚àíŒ∏2‚à•‚àû."
M,0.3177470775770457,"Proof. Observe that for softmax, the Jacobian satisÔ¨Åes:"
M,0.3188097768331562,J(Œ∏) = diag(softmax(Œ∏)) ‚àísoftmax(Œ∏)softmax(Œ∏)‚ä§).
M,0.3198724760892667,"We have for all Œ∏, h,"
M,0.32093517534537724,"‚à•J(Œ∏)‚à•1,1 = p
X i=1 p
X"
M,0.32199787460148777,"j=1
|softmax(Œ∏)i(1[i = j] ‚àísoftmax(Œ∏)j)|"
M,0.3230605738575983,"Under review as a conference paper at ICLR 2022 = p
X"
M,0.3241232731137088,"i=1
softmax(Œ∏)i Ô£´"
M,0.32518597236981933,"Ô£≠1 ‚àísoftmax(Œ∏)i +
X"
M,0.32624867162592985,"jÃ∏=i
softmax(Œ∏)j Ô£∂ Ô£∏ = 2 p
X"
M,0.3273113708820404,"i=1
softmax(Œ∏)i (1 ‚àísoftmax(Œ∏)i) ‚â§2."
M,0.3283740701381509,Combining the above with Lemma A.6 gives the desired result.
M,0.3294367693942614,"Lemma A.8. For Œ±i, Œ≤i ‚â•0, the solution to the following optimization"
M,0.33049946865037194,"min
x1,...,xn n
X i=1"
M,0.33156216790648246,"Œ±i
x2
i"
M,0.332624867162593,"subject to n
X"
M,0.3336875664187035,"i=1
Œ≤ixi = C is Œ≥3"
M,0.33475026567481403,C2 and is achieved at xi = C
M,0.33581296493092455,"Œ≥

Œ±i
Œ≤i"
M,0.3368756641870351,"1/3
where Œ≥ = Pn
i=1 Œ±1/3
i
Œ≤"
M,0.3379383634431456,"2
3
i ."
M,0.3390010626992561,Proof. The proof follows by a standard Lagrangian analysis.
M,0.34006376195536664,"Lemma A.9 (Contraction of Œ†norm). Let Œ†norm be the projection operator onto the unit norm ball.
For any vectors u, v, we have ‚à•Œ†norm(u) ‚àíŒ†norm(v)‚à•‚â§‚à•u ‚àív‚à•."
M,0.34112646121147716,"Proof. If u, v are both in the unit ball then this follows trivially. Let us assume that ‚à•u‚à•‚â•‚à•v‚à•and
‚à•u‚à•‚â•1 WLOG. First suppose ‚à•v‚à•‚â§1. Let B(1)
V
= Œ±u be the projection of v in the direction of u,
and let B2
V = v ‚àíB(1)
V . Then"
M,0.3421891604675877,‚à•Œ†norm(u) ‚àíŒ†norm(v)‚à•2 = ‚à•u/‚à•u‚à•‚àív‚à•2
M,0.3432518597236982,"= ‚à•u/‚à•u‚à•‚àí(Œ±u + B2
V )‚à•2"
M,0.3443145589798087,"= ‚à•(‚à•u‚à•‚àí1 ‚àíŒ±)u ‚àíB2
V ‚à•2"
M,0.34537725823591925,"= (‚à•u‚à•‚àí1 ‚àíŒ±)2‚à•u‚à•2 + ‚à•B2
V ‚à•2"
M,0.34643995749202977,"‚â§(1 ‚àíŒ±2)‚à•u‚à•2 + ‚à•B2
V ‚à•2
since ‚à•u‚à•‚àí1 < Œ± < 1"
M,0.3475026567481403,"= ‚à•u ‚àí(Œ±u + B2
V )‚à•2"
M,0.3485653560042508,= ‚à•u ‚àív‚à•2
M,0.34962805526036134,"If ‚à•v‚à•> 1, then"
M,0.35069075451647186,‚à•Œ†norm(u) ‚àíŒ†norm(v)‚à•= ‚à•Œ†norm(u/‚à•v‚à•) ‚àíŒ†norm(v/‚à•v‚à•)‚à•‚â§‚à•u/‚à•v‚à•‚àív/‚à•v‚à•‚à•< ‚à•u ‚àív‚à•.
M,0.3517534537725824,where the second-to-last inequality follows from the ‚à•v‚à•< 1 case.
M,0.3528161530286929,"Lemma A.10 (Zhang (2002) Theorem 4). Let V : {v : v ‚ààRd1, ‚à•v‚à•‚â§B1} and Flinear = {x 7‚Üí
v‚ä§x : v ‚ààV}. For any Œ¥ > 0 and x(1), . . . , x(N) satisfying ‚à•x(i)‚à•‚â§B2 ‚àÄi,"
M,0.3538788522848034,"log N‚àû(Flinear; Œµ; x(1), ¬∑ ¬∑ ¬∑ , x(N)) ‚â§36B2
1B2
2
Œµ2
log(2‚åà4B1B2/Œµ + 2‚åâN + 1)."
M,0.35494155154091395,"A.3
OMITTED PROOFS"
M,0.35600425079702447,"Proof of Lemma 4.3. Observe that,
fhead(X, z; Œ∏s, Œ∏in) ‚àífhead(X, z; bŒ∏s, bŒ∏in))"
M,0.357066950053135,"=
œÜout
 
œÜin(X; Œ∏in)‚ä§Norm(Score(X, z; Œ∏s))

‚àíœÜout

œÜin(X; bŒ∏in)‚ä§Norm(Score(X, z; bŒ∏s))
"
M,0.35812964930924546,Under review as a conference paper at ICLR 2022
M,0.359192348565356,By Lout-Lipschitzness of œÜout and bound on ‚à•w‚à•:
M,0.3602550478214665,"‚â§Lout
œÜin(X; Œ∏in)‚ä§Norm(Score(X, z; Œ∏s)) ‚àíœÜin(X; bŒ∏in)‚ä§Norm(Score(X, z; bŒ∏s))"
M,0.361317747077577,By triangle inequality:
M,0.36238044633368754,"‚â§Lout
œÜin(X; Œ∏in)‚ä§
Norm(Score(X, z; Œ∏s)) ‚àíNorm(Score(X, z; bŒ∏s))
"
M,0.36344314558979807,+ Lout
M,0.3645058448459086,"
œÜin(X; Œ∏in) ‚àíœÜin(X; bŒ∏in)
‚ä§
Norm(Score(X, z; bŒ∏s))"
M,0.3655685441020191,"Using ‚à•Pv‚à•‚â§‚à•P‚à•2,‚àû‚à•v‚à•1 and Bin-boundedness of œÜin:"
M,0.36663124335812963,"‚â§LoutBin
Norm(Score(X, z; Œ∏s)) ‚àíNorm(Score(X, z; bŒ∏s))

1"
M,0.36769394261424015,+ Lout
M,0.3687566418703507,"
œÜin(X; Œ∏in) ‚àíœÜin(X; bŒ∏in)
‚ä§
2,‚àû"
M,0.3698193411264612,"Norm(Score(X, z; bŒ∏s))

1"
M,0.3708820403825717,By Lemma A.6 and the assumption on Norm:
M,0.37194473963868224,"‚â§LoutCNorm
œÜin(X; Œ∏in)‚ä§
2,‚àû"
M,0.37300743889479276,"Score(X, z; Œ∏s) ‚àíScore(X, z; bŒ∏s)

‚àû+ Lout"
M,0.3740701381509033,"
œÜin(X; Œ∏in) ‚àíœÜin(X; bŒ∏in)
‚ä§
2,‚àû
By boundedness of œÜin and
X‚ä§
2,‚àû‚â§BX:"
M,0.3751328374070138,"‚â§LoutCNormBinBX
Score(X, z; Œ∏s) ‚àíScore(X, z; bŒ∏s)

‚àû+ Lout"
M,0.37619553666312433,"
œÜin(X; Œ∏in) ‚àíœÜin(X; bŒ∏in)
‚ä§
2,‚àû
."
M,0.37725823591923485,"Proof of Theorem 4.2. Our goal is to show that for every Œµ
>
0, collection of inputs
(X(1), z(1)), . . . , (X(m), z(m)), there is a cover Chead such that for all Œ∏s ‚ààŒòs, Œ∏in ‚ààŒòin, there"
M,0.3783209351753454,"is some (bŒ∏s, bŒ∏in) ‚ààChead such that maxi
fhead(X(i), z(i); Œ∏s, Œ∏in) ‚àífhead(X(i), z(i); bŒ∏s, bŒ∏in)
 ‚â§Œµ."
M,0.3793836344314559,"Observe that for all Œ∏s, bŒ∏s,"
M,0.3804463336875664,"max
i‚àà[m] ‚à•Score(X(i), z(i); Œ∏s)‚àíScore(X(i), z(i); bŒ∏s)‚à•‚àû=
max
i‚àà[m],t‚àà[T ]"
M,0.38150903294367694,"Score(x(i)
t , z(i); Œ∏s) ‚àíScore(x(i)
t , z(i); bŒ∏s)
 ."
M,0.38257173219978746,"Similarly, for all Œ∏in, bŒ∏in,"
M,0.383634431455898,"max
i‚àà[m]"
M,0.3846971307120085,"
œÜin(X(i); Œ∏in) ‚àíœÜin(X(i); bŒ∏in)
‚ä§
2,‚àû
=
max
i‚àà[m],t‚àà[T ]"
M,0.38575982996811903,"œÜin(x(i)
t ; Œ∏in) ‚àíœÜin(x(i)
t ; bŒ∏in)
 ."
M,0.38682252922422955,"This crucially allows us to aggregate over the i and t dimensions together.4 Therefore, we can
consider N‚àûcovers for the above to bound the overall covering number."
M,0.38788522848034007,"Let CScore be the ŒµScore-cover (‚àû) for FScore over inputs
n
(x(i)
t , z(i))
o"
M,0.3889479277364506,"i‚àà[m],t‚àà[T ] of size"
M,0.3900106269925611,"N‚àû

FScore; ŒµScore; {(x(i)
t , z(i))}i‚àà[m],t‚àà[T ]

."
M,0.39107332624867164,"Also, Let Cin be the Œµin-cover (‚àû) for Fin over inputs {x(i)
t }i‚àà[m],t‚àà[T ] of size"
M,0.39213602550478216,"N‚àû

Fin; Œµin; {x(i)
t }i‚àà[m],t‚àà[T ]; ‚à•¬∑ ‚à•2

."
M,0.3931987247608927,"We are ready to construct the cover for Fhead. Set Chead = {fhead(¬∑; bŒ∏s, bŒ∏in))i‚àà[m] : bŒ∏s ‚ààCScore, bŒ∏in ‚àà
Cin}. Then for any Œ∏s ‚ààŒòs, Œ∏in ‚ààŒòin, there exists bŒ∏s, bŒ∏in ‚ààChead, such that for all i ‚àà[m], using
Lemma 4.3:
fhead(X(i), z(i); Œ∏s, Œ∏in) ‚àífhead(X(i), z(i); bŒ∏s, bŒ∏in)
 ‚â§CNormLoutBinBXŒµScore + LoutŒµin."
M,0.3942614240170032,"4In the case of the Transformer self-attention mechanism, we will obtain ‚àû-norm covering numbers for
Score and œÜin that have only logarithmic dependence on the number of examples. Because of this aggregation
trick, the resulting covering number for the whole layer will have merely logarithmic dependence on the context
length T."
M,0.3953241232731137,Under review as a conference paper at ICLR 2022
M,0.39638682252922425,"The size of the cover we have constructed is,"
M,0.39744952178533477,log |Chead| = log |CScore| + log |Cin|
M,0.3985122210414453,"= log N‚àû

FScore; ŒµScore; {(x(i)
t , z(i))}i‚àà[m],t‚àà[T ]

+ log N‚àû

Fin; Œµin; {x(i)
t }i‚àà[m],t‚àà[T ]; ‚à•¬∑ ‚à•2
"
M,0.3995749202975558,and we are done.
M,0.40063761955366634,"Proof of Corollary 4.4. By Theorem 4.2, the covering number of Ftf-head satisÔ¨Åes"
M,0.40170031880977686,"log N‚àû

Ftf-head; Œµ;
n
(X(i), z(i))
om i=1 "
M,0.4027630180658874,"‚â§
inf
Œ±‚àà[0,1]"
M,0.4038257173219979,"
log N‚àû"
M,0.40488841657810837,"
FQK;
Œ±Œµ
2LœÉBV BX
; {(x(i)
t , z(i))}i‚àà[m],t‚àà[T ] "
M,0.4059511158342189,+ log N‚àû
M,0.4070138150903294,"
FV ; (1 ‚àíŒ±)Œµ"
M,0.40807651434643993,"LœÉ
; {x(i)
t }i‚àà[m],t‚àà[T ]; ‚à•¬∑ ‚à•2 
."
M,0.40913921360255046,where we have used the fact that for a scalar-output Transformer layer:
M,0.410201912858661,‚Ä¢ softmax satisÔ¨Åes the Jacobian assumption with Csoftmax = 2 using Corollary A.7.
M,0.4112646121147715,‚Ä¢ Lout is the Lipschitz constant of œÉ: LœÉ.
M,0.412327311370882,"‚Ä¢ Bin is a bound on the norm of W ‚ä§
V x with respect to norm of x: BV ."
M,0.41339001062699254,"By Lemma 4.5, for any ŒµQK, ŒµV > 0:"
M,0.41445270988310307,"log N‚àû

FQK; ŒµQK; {(x(i)
t , z(i))}i‚àà[m],t‚àà[T ]

‚â≤
(dBQK
2,infBX)2 log(mT) Œµ2
QK"
M,0.4155154091392136,"log N‚àû

FV ; ŒµV ; {(x(i)
t , z(i))}i‚àà[m],t‚àà[T ]; ‚à•¬∑ ‚à•2

‚â≤
(dBV
2,infBX)2 log(mT)"
M,0.4165781083953241,"Œµ2
V
since WQK, WV ‚ààRd√ód (k = d). We want to choose ŒµQK and ŒµV to minimize the sum of the
above two terms, subject to
2LœÉBV BXŒµQK + LœÉŒµV ‚â§Œµ.
By Lemma A.8, the solution to this optimization leads to an optimal bound of:"
M,0.41764080765143463,"log N‚àû(Ftf-head; Œµ; X(1), . . . , X(M)) ‚â≤(dLœÉBX)2 ¬∑"
M,0.41870350690754515,"
(B‚àû
V )
2
3 + (B‚àû
QKBV BX)
2
3
3"
M,0.4197662061636557,"Œµ2
¬∑ log(mT)."
M,0.4208289054197662,"Proof of Lemma 4.5. Let B‚àûbe an upper bound on ‚à•W‚à•2,‚àûand B1 be an upper bound on ‚à•W‚à•2,1."
M,0.4218916046758767,"The approach will be to cover each of the columns of W independently, treating each as specifying
a linear function from Rd1 ‚ÜíR."
M,0.42295430393198724,"By Lemma A.10, letting V(b) : {v : v ‚ààRd1, ‚à•v‚à•‚â§b} and Flinear(b) = {x 7‚Üív‚ä§x : v ‚ààV(b)},
for any Œ¥ > 0"
M,0.42401700318809776,"log N‚àû(Flinear(b); Œ¥; x(1), ¬∑ ¬∑ ¬∑ , x(N)) ‚â§cb2B2
X log((1 + bBX/Œ¥)N) Œ¥2
."
M,0.4250797024442083,given that ‚à•x(i)‚à•‚â§BX for all i.
M,0.4261424017003188,"In fact the cover, which we denote by bFlinear(b; Œ¥), is proper: bFlinear(b; Œ¥) = {x 7‚Üíbv‚ä§x : bv ‚ààbV }
for some Ô¨Ånite subset bV ‚äÇV(b). Let S = ("
M,0.42720510095642933,"(k1, k2, . . . , kd2) : ki ‚àà{0, 1, . . . , d2} for all i and 0 ‚â§ d2
X"
M,0.42826780021253985,"i=1
ki ‚â§d2 )"
M,0.4293304994686504,Under review as a conference paper at ICLR 2022
M,0.4303931987247609,"Given any W ‚ààW, let vi denote the ith column of W. For each i, let"
M,0.4314558979808714,"k‚àó
i =
 d2"
M,0.43251859723698194,"B‚àû
‚à•vi‚à•
"
M,0.43358129649309246,"Let SW = (k‚àó
1, . . . , k‚àó
d2). Then SW ‚ààS, and B‚àû"
M,0.434643995749203,"d2
k‚àó
i ‚â§‚à•vi‚à•‚â§B‚àû"
M,0.4357066950053135,"d2
(k‚àó
i + 1)
‚àÄi."
M,0.436769394261424,"For every tuple S = (k1, k2, . . . , kd2) ‚ààS, let"
M,0.43783209351753455,"c
WS =
n
[bv1bv2 . . . bvd2]‚ä§: bvi ‚ààbFlinear

B‚àû(ki + 1)/d2; Œµ
p"
M,0.43889479277364507,"(ki + 1)/(2d2)

for all i
o
."
M,0.4399574920297556,"Our cover will be bF : {x 7‚Üíc
Wx : c
W ‚ààS"
M,0.4410201912858661,"S‚ààS c
WS}. Note that"
M,0.44208289054197664,"| bF| ‚â§
X S‚ààS d2
Y"
M,0.44314558979808716,"i=1
N‚àû

Flinear(B‚àû(ki + 1)/d2); Œµ
p"
M,0.4442082890541977,"(ki + 1)/(2d2); x(1), ¬∑ ¬∑ ¬∑ , x(N) ‚â≤
X"
M,0.4452709883103082,"S‚ààS
exp d2
X i=1"
M,0.4463336875664187,"B2
‚àûa2(ki + 1)2 log(N)
d2
2Œµ2(ki + 1)/(2d2) ! =
X"
M,0.44739638682252925,"S‚ààS
exp"
M,0.44845908607863977,"2B2
‚àûB2
X log(N)
d2Œµ2 d2
X"
M,0.4495217853347503,"i=1
(ki + 1) !"
M,0.4505844845908608,"‚â§|S| exp
4B1B‚àûB2
X log(N)
Œµ2 "
M,0.45164718384697133,"where in the last step we used the fact that d2
X"
M,0.4527098831030818,"i=1
ki ‚â§d2 B‚àû d
X"
M,0.4537725823591923,"i=1
‚à•vi‚à•= d2B1/B‚àû"
M,0.45483528161530284,"Since |S| ‚â§
 2d2
d2

= exp(O(d2 log d2)), we obtain"
M,0.45589798087141337,"log | bF| ‚â≤d2B1B‚àûB2
X log(N)
Œµ2"
M,0.4569606801275239,"as desired. In particular, we obtain the more concise, but looser, bound from the lemma statement
by using the fact that B1 ‚â§d2B‚àû"
M,0.4580233793836344,"For a particular W = [v1 . . . vd2] ‚ààW it is guaranteed that there is a matrix c
W ‚ààc
WSW , c
W =
[bv1 . . . bvd2], such that for each i ‚àà[d2],"
M,0.45908607863974493,"|v‚ä§
i xn ‚àíbv‚ä§
i xn| ‚â§Œµ r"
M,0.46014877789585545,ki + 1
M,0.461211477151966,"2d2
‚àÄn ‚àà[N]."
M,0.4622741764080765,where ki is the ith element of SW . We then obtain the desired covering property:
M,0.463336875664187,"max
n‚àà[N]"
M,0.46439957492029754,"Wxn ‚àíc
Wxn
 = max
n‚àà[N]"
M,0.46546227417640806,"v
u
u
t d2
X"
M,0.4665249734325186,"i=1
(v‚ä§
i xn ‚àíbv‚ä§
i xn)2 ‚â§"
M,0.4675876726886291,"v
u
u
t d2
X"
M,0.46865037194473963,"i=1
Œµ2
ki + 1 2d2  = Œµ s"
M,0.46971307120085015,"d2 + Pd2
i=1 ki
2d2
‚â§Œµ"
M,0.4707757704569607,Under review as a conference paper at ICLR 2022
M,0.4718384697130712,"A.4
CAPACITY WITH POSITIONAL EMBEDDINGS"
M,0.4729011689691817,"Since the Transformer architecture is permutation invariant for all t Ã∏= œÑ, positional embeddings
(Ô¨Åxed or trainable) are typically added to the inputs to distinguish the different positions of the
tokens. These positional embeddings are matrices P ‚ààRT √ód such that P = [p1 . . . pT ]‚ä§for
pi ‚ààRd. Accounting for the positional embeddings as input, a single Transformer attention head
can be expressed as:"
M,0.47396386822529224,"ftf-pos(X, P; WV , WQK) := œÉ
 
W ‚ä§
V (X + P)‚ä§softmax
 
(X + P)W ‚ä§
QK(xœÑ + pœÑ)

."
M,0.47502656748140276,"For a Ô¨Åxed positional embedding P, let us deÔ¨Åne"
M,0.4760892667375133,"Ftf-pos(P) := {X ‚Üíftf-pos(X, P; WV , WQK) : ‚à•W ‚ä§
V ‚à•2,‚àû‚â§B‚àû
V , ‚à•WV ‚à•‚â§BV , ‚à•WQK‚à•2,‚àû‚â§B‚àû
QK}"
M,0.4771519659936238,". Position embedding just impacts the input into the covering bound argument which effects the
bound in terms of the
P ‚ä§
2,‚àûas given below,"
M,0.4782146652497343,"Lemma A.11. For all X(1), . . . , X(m) ‚ààRT √ód such that
X(i)‚ä§
2,‚àû‚â§BX for all i ‚àà[m], and"
M,0.47927736450584485,"P ‚ààRT √ód such that ‚à•P ‚ä§‚à•2,‚àû‚â§BP , the covering number of Ftf-pos(P) satisÔ¨Åes"
M,0.48034006376195537,"log N‚àû(Ftf-pos(P); Œµ; X(1), . . . , X(m), ‚à•¬∑‚à•2) ‚â≤(dLœÉ(BX+BP ))2¬∑"
M,0.4814027630180659,"
(B‚àû
V )
2
3 + (2B‚àû
QKBV (BX + BP ))
2
3
3"
M,0.4824654622741764,"Œµ2
¬∑ log(mT)."
M,0.48352816153028694,"Proof. Observe that ftf-pos(X, P; WV , WQK) = ftf-head(X + P; WV , WQK). Thus we have,"
M,0.48459086078639746,"log N‚àû

Ftf-pos(P); Œµ;
n
(X(i))
om"
M,0.485653560042508,"i=1 , ‚à•¬∑ ‚à•2

= log N‚àû

Ftf-head; Œµ;
n
X(i) + P
om"
M,0.4867162592986185,"i=1 , ‚à•¬∑ ‚à•2

."
M,0.487778958554729,"For all i ‚àà[m],
(X(i) + P)‚ä§
2,‚àû‚â§
X(i)‚ä§
2,‚àû+
P ‚ä§
2,‚àû‚â§BX + BP . Therefore, using"
M,0.48884165781083955,"Corollary 4.4, we get the desired result."
M,0.48990435706695007,"Therefore our bounds go through for Ô¨Åxed positional embeddings. If we were to train the embed-
dings, we would need a much Ô¨Åner cover on the embeddings which could incur a T dependence."
M,0.4909670563230606,"A.5
CAPACITY OF MULTIPLE PARALLEL HEADS"
M,0.4920297555791711,"In virtually all practical applications of Transformers since their inception, instead of using one set
of weights for an attention head, there are parallel attention heads, which have separate identically-
shaped parameters; their outputs are concatenated. For the purposes of this analysis, suppose we
have"
M,0.49309245483528164,ftf-heads
M,0.49415515409139216,"
X;
n
W [h]
V , W [h]
QK
oH h=1 
:= H
X"
M,0.4952178533475027,"h=1
ftf-head

X; W [h]
V , W [h]
QK

."
M,0.4962805526036132,Let us deÔ¨Åne the class of multi-head self-attention with H heads as
M,0.4973432518597237,"Ftf-heads :=
n
X 7‚Üíftf-heads"
M,0.49840595111583424,"
X;
n
W [h]
V , W [h]
QK
oH h=1 
:"
M,0.49946865037194477,"‚àÄh ‚àà[H], ‚à•W [h]
V"
M,0.5005313496280552,"‚ä§‚à•2,‚àû‚â§B‚àû
V
[h], ‚à•W [h]
V ‚à•‚â§B[h]
V , ‚à•W [h]
QK‚à•2,‚àû‚â§B‚àû
QK
[h]o
."
M,0.5015940488841658,"Lemma A.12. For all X(1), . . . , X(m) ‚ààRT √ód such that
X(i)‚ä§
2,‚àû‚â§BX for all i ‚àà[m], the"
M,0.5026567481402763,covering number of Ftf-heads satisÔ¨Åes
M,0.5037194473963869,"log N‚àû(Ftf-heads; Œµ; X(1), . . . , X(m), ‚à•¬∑‚à•2) ‚â≤(dLœÉBX)2¬∑"
M,0.5047821466524973,"PH
h=1(B‚àû
V
[h])
2
3 + (2B‚àû
QK
[h]B[h]
V )
2
3
3"
M,0.5058448459086079,"Œµ2
¬∑log(mT)."
M,0.5069075451647184,"Proof. For all h ‚àà[H], let Ch be an Œµh-covering of Ftf-head with weight bounds corresponding"
M,0.5079702444208289,to head h. Since ftf-heads
M,0.5090329436769394,"
X;
n
W [h]
V , W [h]
QK
oH h=1"
M,0.51009564293305,"
= PH
h=1 ftf-head

X; W [h]
V , W [h]
QK

, we have"
M,0.5111583421891605,Under review as a conference paper at ICLR 2022
M,0.512221041445271,"C := C1 √ó . . . √ó CH5 is an
PH
h=1 Œµh

-covering for Ftf-heads. Using Corollary 4.4 (and optimizing
for Œµh using Lemma A.8, by breaking them into individual errors for each head), we have"
M,0.5132837407013815,"log |C| = H
X"
M,0.5143464399574921,"h=1
log |Ch| ‚â§ H
X"
M,0.5154091392136025,"h=1
‚â§(dLœÉBX)2 ¬∑"
M,0.5164718384697131,"PH
h=1(B‚àû
V
[h])
2
3 + (2B‚àû
QK
[h]B[h]
V )
2
3
3"
M,0.5175345377258236,"Œµ2
¬∑ log(mT)."
M,0.5185972369819342,"To see the dependence on H, consider the setting where the weight bounds are the same for each
head (dropping the [h] subscript), then we get,"
M,0.5196599362380446,"log N‚àû(Ftf-heads; Œµ; X(1), . . . , X(m), ‚à•¬∑‚à•2) ‚â≤(dLœÉBX)2¬∑H3¬∑"
M,0.5207226354941552,"
(B‚àû
V )
2
3 + (2B‚àû
QKBV )
2
3
3"
M,0.5217853347502657,"Œµ2
¬∑log(mT)."
M,0.5228480340063762,"A.6
CAPACITY OF DEEP TRANSFORMER NETWORKS"
M,0.5239107332624867,"We will consider an L-layer transformer.
Let us denote the weights of layer i by W (i) :=
n
W (i)
Q , W (i)
K , W (i)
V , W (i)
C
o
such that
W (i)
K W (i)
Q"
M,0.5249734325185972,"‚ä§
2
‚â§B(i)
QK,
W (i)
V

2 ‚â§B(i)
V ,
W (i)
C

2 ‚â§B(i)
C"
M,0.5260361317747078,"and
W (i)
K W (i)
Q"
M,0.5270988310308182,"‚ä§
2,‚àû
‚â§B‚àû
QK
(i),
W (i)
V

2,‚àû‚â§B‚àû
V
(i) and
W (i)
C

2,‚àû‚â§B‚àû
C
(i). Let us further"
M,0.5281615302869288,"denote the set of weights up to layer i by W 1:i = (W (1), . . . , W i‚àí1). Let the input representation
of layer i be g(i)
tf-head(X; W 1:i). We inductively deÔ¨Åne g with g(1)
tf-head(X; W 1:1) = X"
M,0.5292242295430393,"g(i+1)
tf-head
 
X; W 1:i+1
= Œ†norm

œÉ

Œ†norm

f

g(i)
tf-head
 
X; W 1:i
; W (i)
W (i)
C

with"
M,0.5302869287991498,"f (Z; {WQ, WK, WV , WC}) = RowSoftmax

ZWQ (ZWK)‚ä§
ZWV ,"
M,0.5313496280552603,"where
Œ†norm
is
applied
row-wise.
Our
Ô¨Ånal
output
is
gtf-scalar(X; W 1:L+1, w)
=
w‚ä§g(L)
tf-head
 
X; W 1:L+1
[CLS] for ‚à•w‚à•‚â§Bw."
M,0.5324123273113709,"In order to construct a cover, we will Ô¨Årst bound the distance between the function g with different
weight parameters W 1:L+1 and c
W 1:L+1. This bound will depend on the closeness of the parameters
which will allow us to construct a cover of the network in an iterative fashion by constructing covers
of each layer."
M,0.5334750265674814,"A.6.1
LIPSCHITZNESS OF THE NETWORK"
M,0.5345377258235919,"To bound the Lipschitzness of the network, we will Ô¨Årst bound the distance between f with different
weights and inputs."
M,0.5356004250797024,"Lemma A.13 (Instantiation of Lemma 4.3). For any WK, c
WK, WV , c
WV , WQ, c
WQ ‚ààRd√ók, for all
Z ‚ààRT √ód such that
Z‚ä§
2,‚àû‚â§1,


f (Z; {WQ, WK, WV , ¬∑}) ‚àíf

Z; {c
WQ, c
WK, c
WV , ¬∑}
‚ä§
2,‚àû"
M,0.536663124335813,"‚â§2 ‚à•WV ‚à•2


WQW ‚ä§
K ‚àíc
WQc
W ‚ä§
K

Z‚ä§
2,‚àû+
(WV ‚àíc
WV )‚ä§Z‚ä§
2,‚àû"
M,0.5377258235919234,"Proof. Consider a Ô¨Åxed row œÑ of the output of the functions,
f (Z; {WQ, WK, WV , ¬∑}) [œÑ] ‚àíf

Z; {c
WQ, c
WK, c
WV , ¬∑}

[œÑ]"
M,0.538788522848034,"=
W ‚ä§
V Z‚ä§softmax
 
ZWKW ‚ä§
Q zœÑ

‚àíc
W ‚ä§
V Z‚ä§softmax

Zc
WKc
W ‚ä§
Q zœÑ
"
M,0.5398512221041445,"5Here, √ó denotes the Cartesian product: the functions obtained by using the every combination of parame-
ters of each individual cover."
M,0.5409139213602551,Under review as a conference paper at ICLR 2022
M,0.5419766206163655,By triangle inequality:
M,0.5430393198724761,"‚â§
W ‚ä§
V Z‚ä§
softmax
 
ZWKW ‚ä§
Q zœÑ

‚àísoftmax

Zc
WKc
W ‚ä§
Q zœÑ
"
M,0.5441020191285866,"+
(WV ‚àíc
WV )‚ä§Z‚ä§softmax

Zc
WKc
W ‚ä§
Q zœÑ
"
M,0.5451647183846972,"Using ‚à•Pv‚à•‚â§‚à•P‚à•2,‚àû‚à•v‚à•1:"
M,0.5462274176408076,"‚â§
W ‚ä§
V Z‚ä§
2,‚àû"
M,0.5472901168969182,"softmax
 
ZWKW ‚ä§
Q zœÑ

‚àísoftmax

Zc
WKc
W ‚ä§
Q zœÑ

1"
M,0.5483528161530287,"+
(WV ‚àíc
WV )‚ä§Z‚ä§
2,‚àû"
M,0.5494155154091392,"softmax

Zc
WKc
W ‚ä§
Q zœÑ

1
By Corollary A.7,
Z‚ä§
2,‚àû‚â§1, ‚à•PQ‚à•2,‚àû‚â§‚à•P‚à•2‚à•Q‚à•2,‚àû, and ‚à•P ‚ä§‚à•2 = ‚à•P‚à•2:"
M,0.5504782146652497,"‚â§2 ‚à•WV ‚à•2
ZWKW ‚ä§
Q zœÑ ‚àíZc
WKc
W ‚ä§
Q zœÑ

‚àû+
(WV ‚àíc
WV )‚ä§Z‚ä§
2,‚àû"
M,0.5515409139213603,"‚â§2 ‚à•WV ‚à•2


WQW ‚ä§
K ‚àíc
WQc
W ‚ä§
K

Z‚ä§
2,‚àû+
(WV ‚àíc
WV )‚ä§Z‚ä§
2,‚àû."
M,0.5526036131774708,"Lemma A.14. For any WK, WV , WQ ‚ààRd√ók, for all Z, bZ ‚ààRT √ód such that
Z‚ä§
2,‚àû‚â§"
M,0.5536663124335813,"1, ‚à•bZ‚ä§‚à•2,‚àû‚â§1,


f (Z; {WQ, WK, WV , ¬∑}) ‚àíf

bZ; {WQ, WK, WV , ¬∑}
‚ä§
2,‚àû"
M,0.5547290116896918,"‚â§‚à•WV ‚à•2

1 + 4
WKW ‚ä§
Q

2"
M,0.5557917109458024," (Z ‚àíbZ)‚ä§
2,‚àû."
M,0.5568544102019128,"Proof. Consider a Ô¨Åxed row œÑ of the output of the functions,
f (Z; {WQ, WK, WV , ¬∑}) [œÑ] ‚àíf

bZ; {WQ, WK, WV , ¬∑}

[œÑ]"
M,0.5579171094580234,"=
W ‚ä§
V Z‚ä§softmax
 
ZWKW ‚ä§
Q zœÑ

‚àíW ‚ä§
V bZ‚ä§softmax

bZWKW ‚ä§
Q bzœÑ
"
M,0.5589798087141339,By triangle inequality:
M,0.5600425079702445,"‚â§
W ‚ä§
V

Z ‚àíbZ
‚ä§
softmax
 
ZWKW ‚ä§
Q zœÑ
 +
W ‚ä§
V bZ‚ä§
softmax
 
ZWKW ‚ä§
Q zœÑ

‚àísoftmax

bZWKW ‚ä§
Q bzœÑ
"
M,0.5611052072263549,"Using ‚à•Pv‚à•‚â§‚à•P‚à•2,‚àû‚à•v‚à•1:"
M,0.5621679064824655,"‚â§
W ‚ä§
V

Z ‚àíbZ

2,‚àû"
M,0.563230605738576,"softmax
 
ZWKW ‚ä§
Q zœÑ

1"
M,0.5642933049946866,"+
W ‚ä§
V bZ‚ä§
2,‚àû"
M,0.565356004250797,"softmax
 
ZWKW ‚ä§
Q zœÑ

‚àísoftmax

bZWKW ‚ä§
Q bzœÑ

1"
M,0.5664187035069076,"By Corollary A.7,
 bZ‚ä§
2,‚àû‚â§1 and ‚à•PQ‚à•2,‚àû‚â§‚à•P‚à•2‚à•Q‚à•2,‚àû:"
M,0.5674814027630181,"‚â§‚à•WV ‚à•2
(Z ‚àíbZ)‚ä§
2,‚àû+ 2 ‚à•WV ‚à•2
ZWKW ‚ä§
Q zœÑ ‚àíbZWKW ‚ä§
Q bzœÑ

‚àû
By triangle inequality:"
M,0.5685441020191286,"‚â§‚à•WV ‚à•2
(Z ‚àíbZ)‚ä§
2,‚àû+ 2 ‚à•WV ‚à•2
(Z ‚àíbZ)WKW ‚ä§
Q zœÑ

‚àû+
 bZWKW ‚ä§
Q (zœÑ ‚àíbzœÑ)

‚àû "
M,0.5696068012752391,"Since
 bZ‚ä§
2,‚àû‚â§1 and ‚à•Pv‚à•‚àû‚â§‚à•P ‚ä§‚à•2,‚àû‚à•v‚à•:"
M,0.5706695005313497,"‚â§‚à•WV ‚à•2

1 + 4
WKW ‚ä§
Q

2"
M,0.5717321997874601," (Z ‚àíbZ)‚ä§
2,‚àû."
M,0.5727948990435706,"With the above lemmas, we are ready to prove the effect of change of weights on g."
M,0.5738575982996812,Under review as a conference paper at ICLR 2022
M,0.5749202975557917,"Lemma A.15. For any W i+1
1
, c
W i+1
1
satisfying the norm constraints,


g(i+1)
tf-block(X; W 1:i+1) ‚àíg(i+1)
tf-block(X; c
W 1:i+1)
‚ä§
2,‚àû"
M,0.5759829968119022,"‚â§


W (i)
C ‚àíc
W (i)
C
‚ä§
œÉ

Œ†norm

f

X; c
W 1:i
; c
W (i)‚ä§
2,‚àû"
M,0.5770456960680127,"+ LœÉB(i)
C B(i)
V

1 + 4B(i)
QK
 

g(i)
tf-block
 
X; W 1:i
‚àíg(i)
tf-block

X; c
W 1:i‚ä§
2,‚àû"
M,0.5781083953241233,"+ 2LœÉB(i)
C B(i)
V "
M,0.5791710945802337,"
W (i)
Q W (i)
K"
M,0.5802337938363443,"‚ä§‚àíc
W (i)
Q c
W (i)
K"
M,0.5812964930924548,"‚ä§
g(i)
tf-block

X; c
W 1:i‚ä§
2,‚àû"
M,0.5823591923485654,"+ LœÉB(i)
C"
M,0.5834218916046758,"(WV ‚àíc
WV )‚ä§g(i)
tf-block

X; c
W 1:i‚ä§
2,‚àû
."
M,0.5844845908607864,"Proof. Unrolling one layer, we have


g(i+1)
tf-head
 
X; W 1:i+1
‚àíg(i+1)
tf-head

X; c
W 1:i+1‚ä§
2,‚àû"
M,0.5855472901168969,"=


Œ†norm

œÉ

Œ†norm

f

g(i)
tf-head
 
X; W 1:i
; W (i)
W (i)
C
"
M,0.5866099893730075,"‚àíŒ†norm

œÉ

Œ†norm

f

g(i)
tf-head

X; c
W 1:i
; c
W (i)
c
W (i)
C
‚ä§
2,‚àû
Using Lemma A.9 for each row:"
M,0.5876726886291179,"‚â§
W (i)
C"
M,0.5887353878852285,"‚ä§œÉ

Œ†norm

f

g(i)
tf-head
 
X; W 1:i
; W (i)‚ä§
‚àíc
W (i)
C"
M,0.589798087141339,"‚ä§œÉ

Œ†norm

f

g(i)
tf-head

X; c
W 1:i
; c
W (i)
2,‚àû
By triangle inequality for each row:"
M,0.5908607863974495,"‚â§
W (i)
C"
M,0.59192348565356,"‚ä§
œÉ

Œ†norm

f

g(i)
tf-head
 
X; W 1:i
; W (i)
‚àíœÉ

Œ†norm

f

g(i)
tf-head

X; c
W 1:i
; c
W (i)‚ä§
2,‚àû
|
{z
}
(A)"
M,0.5929861849096706,"+


W (i)
C ‚àíc
W (i)
C
‚ä§
œÉ

Œ†norm

f

g(i)
tf-head

X; c
W 1:i
; c
W (i)‚ä§
2,‚àû
."
M,0.594048884165781,Let us focus on term (A).
M,0.5951115834218916,Bounding the norm per row:
M,0.5961742826780021,"(A) ‚â§
W (i)
C

2"
M,0.5972369819341127,"œÉ

Œ†norm

f

g(i)
tf-head
 
X; W 1:i
; W (i)‚ä§
‚àíœÉ

Œ†norm

f

g(i)
tf-head

X; c
W 1:i
; c
W (i)‚ä§
2,‚àû"
M,0.5982996811902231,"Since œÉ is LœÉ-Lipschitz and
W (i)
C

2 ‚â§B(i)
C , for each row:"
M,0.5993623804463337,"‚â§LœÉB(i)
C"
M,0.6004250797024442,"Œ†norm

f

g(i)
tf-head
 
X; W 1:i
; W (i)‚ä§
‚àíŒ†norm

f

g

X; c
W 1:i
; c
W (i)‚ä§
2,‚àû
Using Lemma A.9 for each row:"
M,0.6014877789585548,"‚â§LœÉB(i)
C"
M,0.6025504782146652,"f

g(i)
tf-head
 
X; W 1:i
; W (i)‚ä§
‚àíf

g(i)
tf-head

X; c
W 1:i
; c
W (i)‚ä§
2,‚àû
By triangle inequality:"
M,0.6036131774707758,"‚â§LœÉB(i)
C"
M,0.6046758767268863,"f

g(i)
tf-head
 
X; W 1:i
; W (i)‚ä§
‚àíf

g(i)
tf-head

X; c
W 1:i
; W (i)‚ä§
2,‚àû"
M,0.6057385759829969,"+ LœÉB(i)
C"
M,0.6068012752391073,"f

g(i)
tf-head

X; c
W 1:i
; W (i)‚ä§
‚àíf

g(i)
tf-head

X; c
W 1:i
; c
W (i)‚ä§
2,‚àû"
M,0.6078639744952179,Under review as a conference paper at ICLR 2022
M,0.6089266737513284,By Lemma A.13 and A.14 and norm bounds on the matrices:
M,0.6099893730074389,"‚â§LœÉB(i)
C B(i)
V

1 + 4B(i)
QK
 g(i)
tf-head
 
X; W 1:i‚ä§‚àíg

X; c
W 1:i‚ä§
2,‚àû"
M,0.6110520722635494,"+ 2LœÉB(i)
C B(i)
V "
M,0.61211477151966,"
W (i)
Q W (i)
K"
M,0.6131774707757705,"‚ä§‚àíc
W (i)
Q c
W (i)
K"
M,0.614240170031881,"‚ä§
g(i)
tf-head

X; c
W 1:i‚ä§
2,‚àû"
M,0.6153028692879915,"+ LœÉB(i)
C"
M,0.6163655685441021,"(WV ‚àíc
WV )‚ä§g(i)
tf-head

X; c
W 1:i‚ä§
2,‚àû
."
M,0.6174282678002125,Combining the above gives us the desired result.
M,0.6184909670563231,"Lastly, we take account of the last linear weight and observe that,"
M,0.6195536663124336,"Lemma A.16. For any W 1:L+1, c
W 1:L+1 and w, bw,
gtf-scalar
 
X; W 1:L+1, w

‚àígtf-scalar

X; c
W 1:L+1, bw
"
M,0.620616365568544,"‚â§‚à•w‚à•
g(L+1)
tf-block
 
X; W 1:L+1"
M,0.6216790648246546,"[CLS] ‚àíg(L+1)
tf-block

X; c
W 1:L+1 [CLS]"
M,0.6227417640807651,"+
(w ‚àíbw)‚ä§g(L+1)
tf-block

X; c
W 1:L+1 [CLS] ."
M,0.6238044633368757,"Proof. Observe that,
gtf-scalar
 
X; W 1:L+1, w

‚àígtf-scalar

X; c
W 1:L+1, bw
"
M,0.6248671625929861,"=
w‚ä§g(L+1)
tf-block
 
X; W 1:L+1"
M,0.6259298618490967,"[CLS] ‚àíbw‚ä§g(L+1)
tf-block

X; c
W 1:L+1 [CLS] "
M,0.6269925611052072,By triangle inequality:
M,0.6280552603613178,"‚â§
w‚ä§

g(L+1)
tf-block
 
X; W 1:L+1"
M,0.6291179596174282,"[CLS] ‚àíg(L+1)
tf-block

X; c
W 1:L+1 [CLS]"
M,0.6301806588735388," +
(w ‚àíbw)‚ä§g(L+1)
tf-block

X; c
W 1:L+1 [CLS] "
M,0.6312433581296493,Bounding the inner product by norms:
M,0.6323060573857598,"‚â§‚à•w‚à•
g(L+1)
tf-block
 
X; W 1:L+1"
M,0.6333687566418703,"[CLS] ‚àíg(L+1)
tf-block

X; c
W 1:L+1 [CLS]"
M,0.6344314558979809,"+
(w ‚àíbw)‚ä§g(L+1)
tf-block

X; c
W 1:L+1 [CLS] ."
M,0.6354941551540914,"A.6.2
CONSTRUCTING THE COVER"
M,0.6365568544102019,"The cover construction follows the standard recipe of composing covers per layer (as in Bartlett et al.
(2017))."
M,0.6376195536663124,"Theorem A.17. Let F(L)
tf-scalar represent the class of functions of L-layer Transformer blocks satisfy-
ing the norm bounds (speciÔ¨Åed before) followed by linear layer on the [CLS] token. Then, for all
X(i)"
M,0.638682252922423,"log N‚àû(F(L)
tf-scalar; Œµ; X(1), . . . , X(m), ‚à•¬∑ ‚à•2) ‚â≤"
M,0.6397449521785334,"log(mT) Œµ2
√ó  B"
M,0.640807651434644,"2
3w + L
X"
M,0.6418703506907545,"i=1
Œ±i"
M,0.6429330499468651,"2
3

d
2
3 B‚àû
C
(i)
2
3 + d
2
3

2LœÉB(i)
C B(i)
V B‚àû
QK
(i) 2"
M,0.6439957492029755,"3 + k
2
3

LœÉB(i)
C B‚àû
V
(i) 2 3 !3"
M,0.6450584484590861,"where Œ±i = Q
j<i LœÉB(j)
C B(j)
V (1 + 4B(j)
QK)."
M,0.6461211477151966,"Proof. Our goal is to show that for every Œµ > 0, and collection of inputs X(1), . . . , X(m), there is
a cover C of vectors in R(m) such that for all W 1:L+1 and w satisfying the norm bounds, there is
some v ‚ààC such that maxi |gtf-scalar(X(i); W 1:L+1, w) ‚àív| ‚â§Œµ."
M,0.6471838469713072,"In each layer of the transformer, W (i)
Q and W (i)
K always appear together in the form W (i)
K W (i)
Q ‚ä§."
M,0.6482465462274176,"Therefore, we will overload notation and deÔ¨Åne W (i)
QK : W (i)
K W (i)
Q"
M,0.6493092454835282,"‚ä§. Our cover C will be proper,"
M,0.6503719447396387,Under review as a conference paper at ICLR 2022
M,0.6514346439957492,"consisting of vectors of the form (gtf-scalar(X(i); c
W 1:L+1, bw))i‚àà[m]. We will build the cover itera-
tively by Ô¨Ånding Ô¨Ånite collections of matrices c
W1:i for each layer."
M,0.6524973432518597,"First observe that for any collection of Z(1), . . . , Z(m) ‚ààRT √ód1, and any W, c
W ‚ààRd1√ód2,"
M,0.6535600425079703,"max
i‚àà[m]"
M,0.6546227417640808,"W ‚ä§Z(i)‚ä§‚àíc
W ‚ä§Z(i)‚ä§
2,‚àû=
max
i‚àà[m],t‚àà[T ]"
M,0.6556854410201913,"W ‚ä§z(i)
t
‚àíc
W ‚ä§z(i)
t
 ."
M,0.6567481402763018,"This crucially allows us to aggregate over the samples and context length. In particular, we can apply
Lemma 4.5 with the input vectors (z(i)
t )i‚àà[m],t‚àà[T ]; a total of mT input vectors. SpeciÔ¨Åcally, for any"
M,0.6578108395324124,"Œµ and W(d1, d2, Œ±) := {W ‚ààRd1√ód2 | ‚à•W ‚ä§‚à•2,‚àû‚â§Œ±} with Ô¨Åxed Z(i) satisfying
Z(i)‚ä§
2,‚àû‚â§"
M,0.6588735387885228,"1, Lemma 4.5 gives us such a cover."
M,0.6599362380446334,"First let us build a cover for one Transformer layer with inputs Z(1), . . . , Z(m). We will begin with
creating an ŒµV -cover c
WV for the function class of linear transformations given by WV : {W ‚àà
Rd√ók,
W ‚ä§
2,‚àû‚â§Œ±, ‚à•W‚à•2 ‚â§s} and ŒµQK-cover c
WQK for WQK := {W ‚ààRd√ód, ‚à•W‚à•2,‚àû‚â§"
M,0.6609989373007439,"Œ≤, ‚à•W‚à•2 ‚â§r} and inputs Z(1), . . . , Z(m). For each pair of c
WV ‚ààc
WV and c
WQK ‚ààc
WQK, we
construct an ŒµC-cover c
WC(c
WV , c
WQK) for WC : {W ‚ààRk√ód, ‚à•W‚à•2,‚àû‚â§Œ≥, ‚à•W‚à•2 ‚â§c} and"
M,0.6620616365568545,"inputs
n
œÉ

Œ†norm

f

Z(i); c
WV , c
WQK
om"
M,0.6631243358129649,i=1. Our Ô¨Ånal cover is
M,0.6641870350690755,"c
W :=
n
(c
WV , c
WQK, c
WC) : c
WV ‚ààc
WV , c
WV ‚ààc
WV , c
WC ‚ààc
WC(c
WV , c
WQK)
o
."
M,0.665249734325186,"Using Lemma A.15, we can show that c
W is an Œµ-cover for g(¬∑; {WV , WQK, WC}) and inputs
Z(1), . . . , Z(m) where
Œµ = ŒµC + 2LœÉcsŒµQK + LœÉcŒµV .
Using Lemma 4.5, the size of the cover is,"
M,0.6663124335812965,"|c
W| ‚â§|c
WV ||c
WQK|
max
c
WV ‚ààc
WV
c
WQK‚ààc
WQK"
M,0.667375132837407,"c
WC(c
WV , c
WQK)"
M,0.6684378320935175,"=‚áílog |c
W| ‚â≤ kŒ±2"
M,0.6695005313496281,"Œµ2
V
+ kŒ≤2"
M,0.6705632306057385,"Œµ2
QK
+ dŒ≥2 Œµ2
C !"
M,0.6716259298618491,log(mT).
M,0.6726886291179596,"We are now ready to inductively construct a cover for the deeper network.
Suppose
we have a Œµ(i)-cover c
W1:i for g(¬∑; W 1:i) on X(1), ¬∑ ¬∑ ¬∑ , X(m).
We show how to con-
struct an Œµ(i+1)-cover for g(¬∑; W 1:i+1).
For every element c
W 1:i
‚àà
c
W1:i we construct a

Œµ(i)
C + 2LœÉB(i)
C B(i)
V Œµ(i)
QK + LœÉB(i)
C Œµ(i)
V

-cover c
Wi(c
W 1:i) for the transformer layer (as above) on"
M,0.6737513283740701,"inputs
n
g(X(j); c
W 1:i)
om"
M,0.6748140276301806,j=1. Consider the cover
M,0.6758767268862912,"c
W1:i+1 :=
n
(c
W 1:i, c
W (i)) : c
W 1:i ‚ààc
W1:i, c
W (i) ‚ààc
Wi(c
W 1:i)
o
."
M,0.6769394261424017,"By Lemma A.15, this gives,"
M,0.6780021253985122,"Œµ(i+1) = LœÉB(i)
C B(i)
V (1 + 4B(i)
QK)Œµ(i) + Œµ(i)
C + 2LœÉB(i)
C B(i)
V Œµ(i)
QK + LœÉB(i)
C Œµ(i)
V ."
M,0.6790648246546227,The size of the cover is
M,0.6801275239107333,"|c
W1:i+1| ‚â§|c
W1:i|
max
c
W 1:i‚ààc
W1:i"
M,0.6811902231668437,"c
Wi(c
W 1:i)
 ."
M,0.6822529224229543,"Inductively applying this, we get"
M,0.6833156216790648,"Œµ(L+1) = L
X i=1 Ô£´ Ô£≠Y"
M,0.6843783209351754,"j<i
LœÉB(j)
C B(j)
V (1 + 4B(j)
QK) Ô£∂"
M,0.6854410201912858,"Ô£∏

Œµ(i)
C + 2LœÉB(i)
C B(i)
V Œµ(i)
QK + LœÉB(i)
C Œµ(i)
V
"
M,0.6865037194473964,"Under review as a conference paper at ICLR 2022 = L
X"
M,0.6875664187035069,"i=1
Œ±i

Œµ(i)
C + 2LœÉB(i)
C B(i)
V Œµ(i)
QK + LœÉB(i)
C Œµ(i)
V
"
M,0.6886291179596175,where Œ±i = Q
M,0.6896918172157279,"j<i LœÉB(j)
C B(j)
V (1 + 4B(j)
QK)."
M,0.6907545164718385,The size of the cover is
M,0.691817215727949,"log

|c
W1:L+1|

‚â§ L
X i=1 Ô£´"
M,0.6928799149840595,"Ô£≠k2B‚àû
V
(i)2"
M,0.69394261424017,"Œµ(i)
V"
M,0.6950053134962806,"2
+
d2B‚àû
QK
(i)2"
M,0.696068012752391,"Œµ(i)
QK"
M,0.6971307120085016,"2
+ d2B‚àû
C
(i)2"
M,0.6981934112646121,"Œµ(i)
C 2 Ô£∂"
M,0.6992561105207227,Ô£∏log(mT).
M,0.7003188097768331,"Notice that the layer-norm maintains the norm bound on the inputs. Lastly, we need to cover the
linear layer on the [CLS] token and compose it with the cover of g1:L (as before). Using Lemma
A.10 and A.16, we can get the Ô¨Ånal Œµ-cover C with"
M,0.7013815090329437,"Œµ = Bw L
X"
M,0.7024442082890542,"i=1
Œ±i

Œµ(i)
C + 2LœÉB(i)
C B(i)
V Œµ(i)
QK + LœÉB(i)
C Œµ(i)
V

+ Œµw"
M,0.7035069075451648,and size
M,0.7045696068012752,"log |C| ‚â≤B2
w log(m) Œµ2w
+ L
X i=1 Ô£´"
M,0.7056323060573858,"Ô£≠k2B‚àû
V
(i)2"
M,0.7066950053134963,"Œµ(i)
V"
M,0.7077577045696068,"2
+
d2B‚àû
QK
(i)2"
M,0.7088204038257173,"Œµ(i)
QK"
M,0.7098831030818279,"2
+ d2B‚àû
C
(i)2"
M,0.7109458023379384,"Œµ(i)
C 2 Ô£∂"
M,0.7120085015940489,Ô£∏log(mT)
M,0.7130712008501594,"Using Lemma A.8, the size of the cover for Ô¨Åxed Œµ gives us the desired result."
M,0.71413390010627,"B
PROOFS FOR SPARSE FUNCTION REPRESENTATION"
M,0.7151965993623804,"B.1
SETUP"
M,0.7162592986184909,"Reductions from Boolean functions to Transformers.
In order to establish our function ap-
proximation results, we must Ô¨Årst deÔ¨Åne a canonical mapping between length-T Boolean strings
b ‚àà{0, 1}T and Transformer inputs X ‚ààRT √ód. The key point (which has also been considered
since the inception of the Transformer (Vaswani et al., 2017), and continues to be a crucial consider-
ation in practice (Dosovitskiy et al., 2020)) is that the network‚Äôs permutation-equivariant symmetry
needs to be broken by assigning different embeddings to different indices of b. There are several
possible natural choices here, which are all of practical interest:"
M,0.7173219978746015,"‚Ä¢ Deterministic positional embeddings. Fix positional embedding matrices P ‚ààRT √ód, E ‚àà
R{0,1}√ód, and a special direction v[CLS] ‚ààRd, such that the T + 3 vectors {Pt,:}T
t=1 ‚à™
{Ej,:}j‚àà0,1 ‚à™{v[CLS]} are an approximately orthonormal basis for Rd (see below). The
input to the Transformer is then X = Eb + P, where Eb ‚ààRT √ód such that [Eb]t,: = Ebt,:
for each t ‚àà[T]. In the ftf-scalar formulation, we choose the auxiliary input x[CLS] to
be the constant vector v[CLS]. This closely matches applications of Transformers in NLP
(Vaswani et al., 2017)."
M,0.718384697130712,"‚Ä¢ Trainable positional embeddings. Like the above, but P is a trainable parameter; we still
require approximate orthogonality of {Ej,:}j‚àà0,1 ‚à™{v[CLS]}. It is also possible to consider
the case where E and v[CLS] are trainable (matching the way token embeddings are trained
in practice). This becomes important in the regime of large vocabulary sizes that require
embeddings to capture shared information between tokens; however, this is not necessary
for our constructions, as we limit our consideration to binary tokens. This simpliÔ¨Åes our
constructions and improves statistical rates; additionally, it is a popular and well-studied
alternative (Vaswani et al., 2017; Devlin et al., 2018; Radford et al., 2018; 2019; Brown
et al., 2020)."
M,0.7194473963868225,"‚Ä¢ Bag of vectors. Fix a matrix V ‚ààRT √ód with approximately orthogonal rows (like the
deterministic P), but choose the Transformer input"
M,0.720510095642933,X := V diag(b).
M,0.7215727948990436,Under review as a conference paper at ICLR 2022
M,0.722635494155154,"This construction replaces positional embeddings with positional ‚Äúindicator vectors‚Äù which
can be swapped between any of the Transformer‚Äôs input positions. It has the advantage of
being symmetric with respect to permutation of the Transformer‚Äôs input positions: it turns
out that
ftf-scalar(V diag(b)) = ftf-scalar(V Œ†diag(b)),
for any T √óT permutation matrix Œ†. It is also the most natural construction when consider-
ing the composition of sparse Boolean functions across multiple layers: a layer can output
combinations of the basis rows vi for further function composition, like Boolean gates."
M,0.7236981934112646,"Approximately orthonormal basis.
Each of the Boolean function approximation constructions
will rely on a basis set of vectors, which will be used as positional embeddings (or the variable
indices in the bag-of-vectors construction). We will Ô¨Åx a set of approximately orthonormal vectors
{vi : ‚à•vi‚à•= 1}T ‚Ä≤
i=1 in Rd: for each i Ã∏= j, we have |v‚ä§
i vj| ‚â§‚àÜ. When ‚àÜ= 0, the maximal T ‚Ä≤"
M,0.7247608926673751,"for which such a set exists is d; for ‚àÜ‚àà(0, 1"
M,0.7258235919234857,"2), the Johnson-Lindenstrauss lemma (Johnson et al.,
1986) implies that the maximal set of is of size exp(Œò(d‚àÜ2)). For given choices of d, ‚àÜand a
maximal {v1, . . . , vT ‚Ä≤}, our construction is valid for contexts of length T ‚â§T ‚Ä≤. For the special
vectors e0, e1, v[CLS], we will assume that these are exactly orthogonal to the vi and each other, so
that the vi must be a basis in dimension d‚àí1 or d‚àí3. This is for clarity only‚Äì it reduces the number
of error terms to propagate through the analysis."
M,0.7268862911795961,"Self-attention block.
In each construction (which speciÔ¨Åes an input X ‚ààRT √ód, we will specify
the parameters WQ, WK, WV , WC, w = e1 of a scalar-output Transformer ftf-scalar, which takes an
input X ‚ààR(T +1)√ód; the auxiliary token input will be the constant vector x[CLS] := v[CLS] ‚ààRd.
The internal activation function œÉ is chosen to be the identity. Summarizing, the functional form of
ftf-scalar ‚ààFtf-scalar in these constructions is
ftf-scalar(X; WQ, WK, WV , WC, e1) = softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
XWV WCe1.
In the intermediate lemmas, it will also be useful to consider the corresponding attention head output
ftf-head(X; WQ, WK, WV , WC) = softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
XWV WC,
and its projections ftf-head ‚ó¶Œ†dproj onto the Ô¨Årst dproj coordinates."
M,0.7279489904357067,"Feedforward networks.
We establish some notation for feedforward networks. An L-layer feed-
forward network, with activation function œÉ : R ‚ÜíR and dimensions d1, . . . , dL+1, is parame-
terized by weight matrices Wi ‚ààRdi+1√ódi, and maps x ‚ààRd1 to y ‚ààRdL+1, by the iterative
equations
y‚ä§
1 := œÉ(x‚ä§W1),
y‚ä§
i+1 := œÉ(y‚ä§
i Wi),
i = 1, . . . , L ‚àí1,
fmlp(x; W1, . . . , WL)‚ä§= y‚ä§:= y‚ä§
L Wi.
When dL+1 = 1, we will use the notation w instead of WL. It will be convenient to incorporate bias
weights by introducing an extra input coordinate Wi ‚ààR(di+1+1)√ódi, and augmenting the linear
function accordingly:
y‚ä§
i Wi 7‚Üí[y‚ä§
i 1]Wi."
M,0.7290116896918172,"Self-attention composed with a feedforward network.
The full deÔ¨Ånition of the Transformer
layer composes a self-attention layer (ftf-layer : RT √ód ‚ÜíRT √ód) with a position-wise feedforward
network (fmlp : Rd ‚ÜíRd). We will use this combination of modules to establish our function
approximation results: ftf-layer acts as a sparse bottleneck, while fmlp approximates an arbitrary
function of the selected coordinates. For our single-layer constructions, it is most convenient to
establish notation for a scalar-output Transformer with a feedforward network. To this end, deÔ¨Åne
Ftf+mlp to be the function class with the same Score, Norm, œÜin functions as in Ftf-scalar (thus, the
same parameters WQ, WK, WV ), with identity activation function, but a feedforward neural network
replacing the linear œÜout and w. Concretely, with L = 3 and the ReLU activation function (¬∑)+,
Ftf+mlp contains functions of the form"
M,0.7300743889479278,"ftf+mlp(X; Œ∏) =
 
(y‚ä§W1)+W2
 + w,"
M,0.7311370882040382,"y = softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
XWV WCw,
with parameters Œ∏ := (WQ, WK, WV , WC, W1, W2, w)."
M,0.7321997874601488,Under review as a conference paper at ICLR 2022
M,0.7332624867162593,"Multiple self-attention heads.
The Ô¨Ånal component we will need for the function approximation
setup is multi-headed self-attention. We will extend the deÔ¨Ånition of the single-headed ftf-head to"
M,0.7343251859723698,ftf-heads
M,0.7353878852284803,"
X;
n
W [h]
Q , W [h]
K , W [h]
V , W [h]
C
oH h=1 
:= H
X"
M,0.7364505844845909,"h=1
ftf-head

X; W [h]
Q , W [h]
K , W [h]
V , W [h]
C

,"
M,0.7375132837407014,and substitute this deÔ¨Ånition into ftf+mlp when discussing a multi-headed construction.
M,0.7385759829968119,"Classes and properties of Boolean functions.
We will call a Boolean function f : {0, 1}T ‚ÜíY
I-sparse if it only depends on a Ô¨Åxed subset I ‚äÜ[T] of its inputs:"
M,0.7396386822529224,"bi = b‚Ä≤
i
‚àÄi ‚ààI =‚áíf(b) = f(b‚Ä≤)."
M,0.740701381509033,"Overloading notation, if I = s, we will also call f s-sparse. We will call an I-sparse Boolean
function f symmetric if its value is invariant under permutation of the indices in I:"
M,0.7417640807651434,"|{i ‚ààI : bi = 1}| = |{i ‚ààI : b‚Ä≤
i = 1}| =‚áíf(b) = f(b‚Ä≤)."
M,0.742826780021254,"Further, we will call an I-sparse real-valued symmetric Boolean function f : {0, 1}T ‚ÜíY mono-
tone if f(b) is monotonically increasing in r := |{i ‚ààI : bi = 1}|. If, for some Œ≥ > 0, it holds
that f(r + 1) ‚â•f(r) + Œ≥ for each r = 0, . . . , s ‚àí1, we call f Œ≥-strictly monotone. A vector-valued
I-sparse Boolean function f : {0, 1}T ‚ÜíRdf is Œ≥-injective if"
M,0.7438894792773645,‚à•f(b) ‚àíf(b‚Ä≤)‚à•‚àû‚â•Œ≥
M,0.7449521785334751,"for each b, b‚Ä≤ that differ at some position i ‚ààI; f is called B-bounded if ‚à•f(b)‚à•‚àû‚â§B for all
b ‚àà{0, 1}T ."
M,0.7460148777895855,"Uniform approximation.
For some Œµ ‚â•0 and a function f : {0, 1}T ‚ÜíRd, we say that bf ‚ààF
Œµ-uniformly approximates f under the mapping b 7‚ÜíX(b) if
 bf(X(b)) ‚àíf(b)

‚àû‚â§Œµ,
‚àÄb ‚àà{0, 1}T ."
M,0.7470775770456961,"B.2
RESULTS"
M,0.7481402763018066,"We give an overview of the function approximation results under each input mapping b 7‚ÜíX(b), as
a multi-part proposition:
Proposition
B.1
(Sparse
variable
creation
with
Transformers).
The
function
classes
Ftf-scalar, Ftf+mlp contain the following classes of sparse Boolean functions:"
M,0.7492029755579172,"‚Ä¢ Deterministic positional embeddings: For any I, Ftf-scalar can approximate a particular
monotone symmetric I-sparse f, with Transformer weight norm bounds from the real-
valued construction in Lemma B.2. Ftf+mlp with 1 head can exactly represent any sym-
metric s-sparse f, with the same bounds on Transformer weight norms, and feedforward
network weight norms scaling as O(poly(s)). Ftf+mlp with s heads can exactly represent
any s-sparse f, with Transformer weight norm bounds from the vector-valued construction
in Lemma B.2, and feedforward network weight norms scaling as O(poly(s) ¬∑ 2s).
‚Ä¢ Trainable positional embeddings: For any I, Ftf-scalar can approximate a particular mono-
tone symmetric I-sparse f, with positional embedding and Transformer weight norm
bounds from the real-valued construction in Lemma B.3. Ftf+mlp with 1 head can ex-
actly represent any symmetric s-sparse f, with the same bounds on P and Transformer
weight norms, and feedforward network weight norms scaling as O(poly(s)). Ftf+mlp with
s heads can exactly represent any sparse f, with P and Transformer weight norm bounds
from the vector-valued construction in Lemma B.3, and feedforward network weight norms
scaling as O(poly(s) ¬∑ 2s).
‚Ä¢ Bag of vectors: For any I, Ftf-scalar can approximate a particular monotone symmetric
I-sparse f, with Transformer weight norms from Lemma B.4. Ftf+mlp with 1 head can
represent any symmetric s-sparse f, with the same Transformer weight norm bounds, and
feedforward network weight norms scaling as O(poly(s)). Ftf+mlp with 1 head can also
exactly represent any s-sparse f, with the same bounds on Transformer weight norms, and
feedforward network weight norms scaling as O(poly(s) ¬∑ 2s)."
M,0.7502656748140276,Under review as a conference paper at ICLR 2022
M,0.7513283740701382,"The formal statements are obtained by (Œ≥/4)-uniformly approximating a Œ≥-strictly monotone or
Œ≥-injective function with self-attention alone (Lemmas B.2, B.3, B.4), then applying a robust uni-
versal function representation construction (Lemmas B.5, B.6) appropriately. They are organized as
follows:
Lemma B.2 (Deterministic P, no MLP). Suppose X(b) = P+Eb with deterministic P. Let I ‚äÜ[T]
such that |I| = s ‚â§d, k, and ‚àÜ< 1/s. Then, for all 0 < Œ≥ ‚â§1, there exists a 1-bounded, (2/s)-
strictly monotone symmetric I-sparse Boolean function gI : {0, 1}T ‚ÜíR and Transformer head
parameters such that ftf-scalar(X(b); WQ, WK, WV , WC, w = e1) (Œ≥/4)-uniformly approximates
gI. The norms satisfy"
M,0.7523910733262487,"‚à•WQ‚à•F ‚â§
log

8T Œ≥
"
M,0.7534537725823592,"1 ‚àís‚àÜ,
‚à•WK‚à•F ‚â§s,
‚à•WV ‚à•F ‚â§2,
‚à•WC‚à•F ‚â§1."
M,0.7545164718384697,"Also, there exists a 1-bounded, 2-injective I-sparse Boolean function g‚Ä≤
I : {0, 1}T ‚ÜíRs and"
M,0.7555791710945803,"s-headed Transformer parameters such that ftf-head

X(b);
n
W [h]
Q , W [h]
K , W [h]
V , W [h]
C
os h=1"
M,0.7566418703506907,"
‚ó¶Œ†s
uniformly approximates g‚Ä≤
I. The norms of each head satisfy"
M,0.7577045696068013,"W [h]
Q

F ‚â§
log

8T Œ≥
"
M,0.7587672688629118,"1 ‚àís‚àÜ,
W [h]
K

F ‚â§1,
W [h]
V

F ‚â§2,
W [h]
C

F ‚â§1."
M,0.7598299681190224,"Lemma B.3 (Trainable P, no MLP). Suppose X(b) = P + Eb with trainable P. Let I ‚äÜ[T]
such that |I| = s ‚â§d, k. Then, for any 0 < Œ≥ ‚â§1, and with the same gI as in Lemma B.2, there
exists P and Transformer head parameters such that ftf-scalar(X(b); WQ, WK, WV , WC, w = e1)
(Œ≥/4)-uniformly approximates gI. The norms satisfy"
M,0.7608926673751328,"P ‚ä§
2,1 ‚â§s,
‚à•WQ‚à•F ‚â§log
8T Œ≥"
M,0.7619553666312433,"
,
‚à•WK‚à•F ‚â§1,
‚à•WV ‚à•F ‚â§2,
‚à•WC‚à•F ‚â§1."
M,0.7630180658873539,"Also, for the same g‚Ä≤
I as in Lemma B.2, there exists P and s-headed Transformer parameters such"
M,0.7640807651434643,"that ftf-head

X(b);
n
W [h]
Q , W [h]
K , W [h]
V , W [h]
C
os h=1"
M,0.7651434643995749,"
‚ó¶Œ†s uniformly approximates g‚Ä≤
I. The norms of
each head satisfy
P ‚ä§
2,1 ‚â§s,
W [h]
Q

F ‚â§log
8T Œ≥"
M,0.7662061636556854,"
,
W [h]
K

F ‚â§1,
W [h]
V

F ‚â§2,
W [h]
C

F ‚â§1."
M,0.767268862911796,"Lemma B.4 (Bag of vectors, no MLP). Suppose X(b) = V + diag(b). Let I ‚äÜ[T] such that
|I| = s ‚â§d, k, and ‚àÜ< 1/s. Then, for all s‚àÜ< Œ≥ < 1, there exists an s-bounded, (1/s)-
strictly monotone symmetric I-sparse Boolean function gI : {0, 1}T ‚ÜíR and Transformer head
parameters such that ftf-scalar(X(b); WQ, WK, WV , WC, w = e1) (Œ≥/4)-uniformly approximates
gI. The norms satisfy"
M,0.7683315621679064,"‚à•WQ‚à•F ‚â§
log

8T s(1+‚àÜ)"
M,0.769394261424017,"Œ≥‚àís‚àÜ
"
M,0.7704569606801275,"1 ‚àís‚àÜ
,
‚à•WK‚à•F ‚â§s + 1,
‚à•WV ‚à•F ‚â§2s,
‚à•WC‚à•F ‚â§s."
M,0.7715196599362381,"Also, there exists a 1-bounded, (1/s)-injective I-sparse Boolean function g‚Ä≤
I : {0, 1}T ‚ÜíRs and
Transformer head parameters such that ftf-head(X(b); WQ, WK, WV , WC) ‚ó¶Œ†s uniformly approx-
imates g‚Ä≤
I. The norms satisfy the same bounds as above."
M,0.7725823591923485,"Lemma B.5 (Monotone to symmetric functions via MLP). Let f : {0, 1}T ‚ÜíR be any real-valued
symmetric s-sparse Boolean function with index set I. Let WQ, WK, WV , WC be the parameters of
a function"
M,0.7736450584484591,"ftf-head(X; WQ, WK, WV , WC) := softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
XWV WC,"
M,0.7747077577045696,"and let Œ†1 : Rd ‚ÜíR be the projection onto the Ô¨Årst coordinate. Suppose that under some mapping
b 7‚ÜíX(b), ftf-head ‚ó¶Œ†s (Œ≥/4)-uniformly approximates a B-bounded Œ≥-strictly monotone sym-
metric I-sparse Boolean function g : {0, 1}T ‚ÜíR, for some Œ≥. Then, there exists a function
ftf+mlp ‚ààFtf+mlp with the same weights WQ, WK, WV , WC, and 3-layer feedforward network
weights W1, W2, w, such that"
M,0.7757704569606801,"ftf+mlp(X(b)) = f(b),
‚àÄb ‚àà{0, 1}T ,"
M,0.7768331562167906,Under review as a conference paper at ICLR 2022
M,0.7778958554729012,"with dimensions (d2, d3) = (4(s + 1), 2(s + 1)) and weight norms satisfying"
M,0.7789585547290117,"‚à•W1‚à•‚àû‚â§8 max(1, B)"
M,0.7800212539851222,"Œ≥
,
‚à•W2‚à•‚àû‚â§8s"
M,0.7810839532412327,"Œ≥ ,
‚à•w‚à•‚àû‚â§
max
b‚àà{0,1}T |f(b)|."
M,0.7821466524973433,"Lemma B.6 (Injective to arbitrary functions via MLP). Let f : {0, 1}T ‚ÜíR be any real-valued
s-sparse Boolean function with index set I such that |I| = s ‚â§d. Let WQ, WK, WV , WC be the
parameters of a function"
M,0.7832093517534537,"ftf-head(X; WQ, WK, WV , WC) := softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
XWV WC,"
M,0.7842720510095643,"and let Œ†s : Rd ‚ÜíRs be the projection onto the Ô¨Årst s coordinates. Suppose that under some
mapping b 7‚ÜíX(b), ftf-head‚ó¶Œ†s (Œ≥/4)-uniformly approximates a Œ≥-injective function g : {0, 1}T ‚Üí
Rs satisfying ‚à•g(b)‚à•‚àû‚â§B. Then, there exists a function ftf+mlp ‚ààFtf+mlp with the same weights
WQ, WK, WV , WC, and 3-layer feedforward network weights W1, W2, w, such that"
M,0.7853347502656748,"ftf+mlp(X(b)) = f(b),
‚àÄb ‚àà{0, 1}T ,
with dimensions (d2, d3) = (4s2s, 2 ¬∑ 2s) and weight norms satisfying"
M,0.7863974495217854,"‚à•W1‚à•‚àû‚â§8 max(1, B)"
M,0.7874601487778958,"Œ≥
,
‚à•W2‚à•‚àû‚â§8s"
M,0.7885228480340064,"Œ≥ ,
‚à•w‚à•‚àû‚â§
max
b‚àà{0,1}T |f(b)|."
M,0.7895855472901169,"B.3
USEFUL LEMMAS"
M,0.7906482465462275,"We will use a construction which approximates a ‚Äúhard selection‚Äù of s indices using the softmax
mixture; for this, we will need to quantify the approximation error when the inputs to the softmax
function are bounded.
Lemma B.7 (Softmax truncation). Let z ‚àà(R ‚à™{‚àí‚àû})T such that zt ‚â•R for each 1 ‚â§t ‚â§s,
and zt ‚â§0 for each s + 1 ‚â§t ‚â§T. DeÔ¨Åne z‚Ä≤ ‚àà(R ‚à™{‚àí‚àû})T so that z‚Ä≤
t = zt for 1 ‚â§t ‚â§s, and
zt = ‚àí‚àûfor s + 1 ‚â§t ‚â§T. Then, letting e‚àí‚àû= 0 in the deÔ¨Ånition of softmax(¬∑), we have"
M,0.7917109458023379,‚à•softmax(z‚Ä≤) ‚àísoftmax(z)‚à•1 ‚â§2 T ‚àís
M,0.7927736450584485,"s exp(R) <
2T
exp(R)."
M,0.793836344314559,Proof. We have
M,0.7948990435706695,"‚à•softmax(z‚Ä≤) ‚àísoftmax(z)‚à•1 = s
X"
M,0.79596174282678,"t=1
exp(zt)

1
1‚ä§exp(z‚Ä≤) ‚àí
1
1‚ä§exp(z) 
+ T
X t=s+1"
M,0.7970244420828906,"exp(zt)
1‚ä§exp(z)."
M,0.798087141339001,The Ô¨Årst summation is equal to
M,0.7991498405951116,1 ‚àí1‚ä§exp(z‚Ä≤)
M,0.8002125398512221,"1‚ä§exp(z) ‚â§
T ‚àís
s exp(R),"
M,0.8012752391073327,"while the same upper bound holds for the second summation, since each term is at most
1
s exp(R)."
M,0.8023379383634431,"Our results on approximating arbitrary sparse Boolean functions will depend on a generic con-
struction for robustly approximating an arbitrary function f : Rd ‚ÜíR with a feedforward neural
network. For simplicity of presentation, we use a standard6 3-layer ReLU network construction,
which exactly represents a piecewise constant function in speciÔ¨Åed regions.
Lemma B.8 (Exact function representation with a 3-layer ReLU net). Let f : Rdf ‚ÜíR, and
let x1, . . . , xn ‚ààRdf such that ‚à•xi‚à•‚àû‚â§B for each i ‚àà[n], ‚à•xi ‚àíxj‚à•‚àû‚â•4Œ¥ for each i Ã∏=
j ‚àà[n]. Then, there is a 3-layer feedforward network with ReLU activations, with parameters
W1 ‚ààR(df +1)√ód2, W2 ‚ààR(d2+1)√ód3, w ‚ààRd37, such that
fmlp(xi + z) = f(xi)
for all i ‚àà[n] and ‚à•z‚à•‚àû‚â§Œ¥, where ReLU(x) := x+ = max(0, x) is applied entrywise, with
d2 = 4ndf,
d3 = 2n,"
M,0.8034006376195537,"‚à•W1‚à•‚àû‚â§max(1, B)"
M,0.8044633368756642,"Œ¥
,
‚à•W2‚à•‚àû‚â§df"
M,0.8055260361317748,"Œ¥ ,
‚à•w‚à•‚àû‚â§max
i‚àà[n] |f(xi)|."
M,0.8065887353878852,"6For example, this follows from the discussion in Chapter 4 of (Nielsen, 2015).
7Here, W1, W2 have bias terms; w does not."
M,0.8076514346439958,Under review as a conference paper at ICLR 2022
M,0.8087141339001063,"Proof. First, we construct a one-dimensional ‚Äúbump‚Äù function basis, and propagate the Lipschitz
constants. A threshold function with a linear ‚Äúramp‚Äù of width Œ¥ can be obtained from a linear
combination of 2 ReLU functions:"
M,0.8097768331562167,"ŒΩŒ¥(x) := (x/Œ¥ + 1)+ ‚àí(x/Œ¥)+ = Ô£±
Ô£≤ Ô£≥"
M,0.8108395324123273,"0
x ‚â§‚àíŒ¥
x/Œ¥ + 1
‚àíŒ¥ ‚â§x ‚â§0
1
x ‚â•0
."
M,0.8119022316684378,"Next, we construct the bump function"
M,0.8129649309245484,œàŒ¥(x) := ŒΩŒ¥(x) ‚àíŒΩŒ¥(2Œ¥ ‚àíx).
M,0.8140276301806588,"By this construction, we have œàŒ¥(x) = 1 for 0 ‚â§x ‚â§2Œ¥ and œàŒ¥(x) = 0 for x ‚â§‚àíŒ¥ and x ‚â•3Œ¥,
interpolating linearly on [‚àíŒ¥, 0] and [2Œ¥, 3Œ¥]. Next, deÔ¨Åne"
M,0.8150903294367694,œàŒ¥(x; x0) := œàŒ¥(x ‚àíx0 + Œ¥)
M,0.8161530286928799,"=
x ‚àíx0"
M,0.8172157279489904,"Œ¥
+ 2
"
M,0.8182784272051009,"+
‚àí
x ‚àíx0"
M,0.8193411264612115,"Œ¥
+ 1
"
M,0.820403825717322,"+
‚àí
x0 ‚àíx"
M,0.8214665249734325,"Œ¥
+ 2
"
M,0.822529224229543,"+
+
x0 ‚àíx"
M,0.8235919234856536,"Œ¥
+ 1
"
M,0.824654622741764,"+
so that œàŒ¥(x; x0) = 1 for |x ‚àíx0| ‚â§Œ¥, œàŒ¥(x; x0) = 0 for |x ‚àíx0| ‚â•2Œ¥."
M,0.8257173219978746,"We construct the Ô¨Årst layer W1 ‚ààR(d+1)√ó(4nd) using these bump functions: indexing the 4nd
dimension by (h ‚àà[4], i ‚àà[n], j ‚àà[d]), we construct"
M,0.8267800212539851,"[W1]:,(1,i,:) :=

1
Œ¥ I
‚àíxi"
M,0.8278427205100957,Œ¥ + 2 ¬∑ 1‚ä§
M,0.8289054197662061,"
,
[W1]:,(2,i,:) :=

1
Œ¥ I
‚àíxi"
M,0.8299681190223167,"Œ¥ + 1‚ä§ 
,"
M,0.8310308182784272,"[W1]:,(3,i,:) :=

‚àí1"
M,0.8320935175345378,"Œ¥ I
xi"
M,0.8331562167906482,Œ¥ + 2 ¬∑ 1‚ä§
M,0.8342189160467588,"
,
[W1]:,(4,i,:) :=

‚àí1"
M,0.8352816153028693,"Œ¥ I
xi"
M,0.8363443145589798,"Œ¥ + 1‚ä§ 
,"
M,0.8374070138150903,"so that
 
[x 1]‚ä§
[W1]j,(1,i,:) [W1]j,(2,i,:) [W1]j,(3,i,:) [W1]j,(4,i,:)
"
M,0.8384697130712009,+ [1 ‚àí1 ‚àí1 1]‚ä§= œàŒ¥(x; [xi]j).
M,0.8395324123273114,"The second layer is used to construct n activations which are indicators of whether x is in the neigh-
borhood of each xi. For each xi, we will simply average the df one-dimensional indicators for each
coordinate, and implement a threshold function ŒΩŒ¥/df (1 ‚àíx). We choose W2 ‚ààR(4ndf +1)√ó(2n),
with the 4ndf + 1 dimension indexed by (h, i, j) and an extra bias dimension ‚ä•, and the 2n dimen-
sion indexed by (h‚Ä≤ ‚àà{1, 2}, i‚Ä≤ ‚àà[n]) so that"
M,0.8405951115834219,"[W2](h,i,:),(h‚Ä≤,i‚Ä≤) := [1 ‚àí1 ‚àí1 1]h ¬∑ 1[i = i‚Ä≤] ¬∑ 1"
M,0.8416578108395324,"Œ¥ ¬∑ 1,"
M,0.842720510095643,"[W2]‚ä•,(1,i‚Ä≤) := 1 ‚àídf"
M,0.8437832093517534,"Œ¥ ,
[W2]‚ä•,(2,i‚Ä≤) := ‚àídf Œ¥ ."
M,0.844845908607864,"Finally, the third (output) layer w ‚ààR2n, with dimensions indexed by (h ‚àà{1, 2}, i ‚àà[n]),
multiplies the indicators of each xi by the desired f(xi):"
M,0.8459086078639745,"w(1,i) := f(xi),
w(2,i) := ‚àíf(xi)."
M,0.8469713071200851,"For any x0 ‚ààRdf , let BŒ¥(x0) be the set of x such that ‚à•x ‚àíx0‚à•‚àû‚â§Œ¥. By this construction, for
each x ‚ààBŒ¥(xi), we have f(x) = xi, as required."
M,0.8480340063761955,"Note that we use 3-layer ReLU networks for function approximation in order to minimize the intro-
duction of unnecessary notation. Some remarks:"
M,0.8490967056323061,"‚Ä¢ It would be routine to replace this construction with any architecture which can represent
an arbitrary function approximately (Hornik et al., 1989; Cybenko, 1989); this includes the
2-layer feedforward networks (and nonlinear activations other than the ReLU) which are
typically used by Transformers in practice.
‚Ä¢ It is possible to embed this construction in ftf+mlp with a 2-layer ReLU network, by using
(WC, W1, W2) and introducing a nonlinearity after WC, without changing the results.
‚Ä¢ When df = 1, W2 is unnecessary (one can represent f directly using the bump function
basis)."
M,0.8501594048884166,Under review as a conference paper at ICLR 2022
M,0.8512221041445271,"B.4
PROOFS"
M,0.8522848034006376,"Throughout the constructions in each case, we will refer to standard coordinate bases in several
spaces:"
M,0.8533475026567482,"‚Ä¢ E0, E1 ‚ààRd denote the embeddings of the 0, 1 tokens E0,:, E1,:."
M,0.8544102019128587,"‚Ä¢ e(k)
1 , . . . , e(k)
k
denotes the standard basis in Rk."
M,0.8554729011689692,"‚Ä¢ e(d)
i
denotes the standard basis in Rd."
M,0.8565356004250797,"‚Ä¢ e(T )
1
, . . . , e(T )
T , e(T )
[CLS] denotes the standard basis in RT +1 with the special [CLS] index."
M,0.8575982996811902,"‚Ä¢ Recall that the vi form a ‚àÜ-approximate orthonormal basis for Rd, v[CLS], e0, e1 are exactly
orthogonal to each of them as well as each other, and d is chosen such that these conditions
can be met."
M,0.8586609989373007,Let n(i) be a unique bijection between I and [s]. Let vI := P
M,0.8597236981934112,i‚ààI vi.
M,0.8607863974495218,"Approximate vector equality.
We will use u ‚âàŒµ v to denote that two vectors u, v ‚ààRdu satisfy
‚à•u ‚àív‚à•‚àû‚â§Œµ."
M,0.8618490967056323,"Proof of Lemma B.2. We construct attention heads such that the softmax mixture always selects the
indices in I."
M,0.8629117959617428,"Single head, deterministic P. We seek to approximate the 1-bounded, (2/s)-strictly monotone
function
1
s X"
M,0.8639744952178533,"i‚ààI
œái,"
M,0.8650371944739639,where œái = +1 if bi = 0 and ‚àí1 if bi = 1. Set
M,0.8660998937300743,"WQ := Rv[CLS]e(k)‚ä§
1
,
WK := vIe(k)‚ä§
1
,
WV := (E0 ‚àíE1)e(k)‚ä§
1
,
WC := e(k)
1 e(d)‚ä§
1
,"
M,0.8671625929861849,"where R will be chosen later. Then, by approximate orthogonality,"
M,0.8682252922422954,"v‚ä§
[CLS]WQW ‚ä§
KX‚ä§= v‚ä§
[CLS]WQW ‚ä§
K(P + Eb)‚ä§= v‚ä§
[CLS]WQW ‚ä§
KP ‚ä§‚âàRs‚àÜR
X"
M,0.869287991498406,"i‚ààI
e(T )‚ä§
i
."
M,0.8703506907545164,"By Lemma B.7,
softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
‚àí1 s X"
M,0.871413390010627,"i‚ààI
e(T )‚ä§
i"
M,0.8724760892667375,"1
‚â§
2T
exp(R ‚àí2Rs‚àÜ)."
M,0.873538788522848,"Finally, we have"
M,0.8746014877789585,XWV WC = EbWV WC = Ô£´ Ô£≠X
M,0.8756641870350691,"i‚àà[T ]
œáie(T )
i Ô£∂"
M,0.8767268862911796,"Ô£∏e(d)‚ä§
1
,"
M,0.8777895855472901,"so that by H¬®older‚Äôs inequality,"
M,0.8788522848034006,"ftf-head(X) ‚ó¶Œ†1 = softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
XWV WCe(d)
1
‚âà
2T
exp(R‚àí2Rs‚àÜ)
1
s X"
M,0.8799149840595112,"i‚ààI
œái."
M,0.8809776833156217,"To get (Œ≥/4)-uniform approximation, we choose"
M,0.8820403825717322,"R =
log

8T Œ≥
"
M,0.8831030818278427,1 ‚àís‚àÜ.
M,0.8841657810839533,"Multiple heads, deterministic P. For h = 1, . . . , s, and the same R as above:"
M,0.8852284803400637,"W [h]
Q := Rv[CLS]e(k)‚ä§
1
,
W [h]
K := vn‚àí1(h)e(k)‚ä§
1
,
W [h]
V
:= (E0‚àíE1)e(k)‚ä§
2
,
W [h]
C
:= e(k)
1 e(d)‚ä§
h
."
M,0.8862911795961743,Under review as a conference paper at ICLR 2022
M,0.8873538788522848,"This is the same construction as above, but each head only selects one of the coordinates in I. Thus,
by the same analysis,
ftf-head(X) ‚ó¶Œ†s ‚âà
2T
exp(R‚àí2Rs‚àÜ)
X"
M,0.8884165781083954,"i‚ààI
œáie(d)
n(i)."
M,0.8894792773645058,This function is clearly 1-bounded and 2-injective.
M,0.8905419766206164,"Proof of Lemma B.3. The constructions closely follow Lemma B.2, but are simpler."
M,0.8916046758767269,"Single head, trainable P. For each i ‚ààI, set the trainable positional embeddings to be"
M,0.8926673751328374,"Pi,: :=
v1
i ‚ààI
0
otherwise . Set"
M,0.8937300743889479,"WQ := Rv[CLS]e(k)‚ä§
1
,
WK := v1e(k)‚ä§
1
,
WV := (E0 ‚àíE1)e(k)‚ä§
1
,
WC := e(k)
1 e(d)‚ä§
1
."
M,0.8947927736450585,"Now, we have (with equality)"
M,0.895855472901169,"v‚ä§
[CLS]WQW ‚ä§
KX‚ä§= R
X"
M,0.8969181721572795,"i‚ààI
e(T )‚ä§
i
,"
M,0.89798087141339,"so that Lemma B.7 gives
softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
‚àí1 s X"
M,0.8990435706695006,"i‚ààI
e(T )‚ä§
i"
M,0.900106269925611,"1
‚â§
2T
exp(R)."
M,0.9011689691817216,"Like before, we have"
M,0.9022316684378321,"ftf-head(X) ‚ó¶Œ†1 = softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
XWV WCe(d)
1
‚âà
2T
exp(R)
1
s X"
M,0.9032943676939427,"i‚ààI
œái."
M,0.9043570669500531,"To get (Œ≥/4)-uniform approximation, we choose"
M,0.9054197662061636,"R = log
8T Œ≥ 
."
M,0.9064824654622742,"Multiple heads, trainable P. For each i ‚ààI, set the trainable positional embeddings to be"
M,0.9075451647183846,"Pi,: :="
M,0.9086078639744952,"(
e(d)
n(i)
i ‚ààI
0
otherwise ."
M,0.9096705632306057,"For h = 1, . . . , s, and the same R as above:"
M,0.9107332624867163,"W [h]
Q := Rv[CLS]e(k)‚ä§
1
,
W [h]
K := e(d)
h e(k)‚ä§
1
,
W [h]
V
:= (E0 ‚àíE1)e(k)‚ä§
1
,
W [h]
C
:= e(k)
1 e(d)‚ä§
h
."
M,0.9117959617428267,"This is the same construction as above, but each head only selects one of the coordinates in I. Thus,
by the same analysis,
ftf-head(X) ‚ó¶Œ†s ‚âà
2T
exp(R)
X"
M,0.9128586609989373,"i‚ààI
œáie(d)
n(i)."
M,0.9139213602550478,"Proof of Lemma B.4. This input mapping does not use position embeddings, and does not need
multiple heads to implement arbitrary (non-symmetric) functions. The constructed monotone and
injective functions are slightly different, but the proof strategy is very similar. The key difference is
that the softmax mixture is uniform only on the positions i ‚ààI where bi = 1."
M,0.9149840595111584,"Bag of vectors, scalar output. The function we will approximate is deÔ¨Åned as follows:"
M,0.9160467587672688,gI(r) := r ‚àís
M,0.9171094580233794,"r + 1,
where r =
X"
M,0.9181721572794899,"i‚ààI
bi,
s = |I|."
M,0.9192348565356004,Under review as a conference paper at ICLR 2022
M,0.9202975557917109,"Note that this function is (1/s)-strictly monotone, and has absolute value bounded by s. Set"
M,0.9213602550478215,"WQ := Rv[CLS]e(k)‚ä§
1
,
WK := (vI + v[CLS])e(k)‚ä§
1
,"
M,0.922422954303932,"WV :=
X"
M,0.9234856535600425,"i‚ààI
vie(k)‚ä§
n(i) ‚àív[CLS] X"
M,0.924548352816153,"i‚ààI
e(k)
n(i) !‚ä§"
M,0.9256110520722636,",
WC :=
X"
M,0.926673751328374,"i‚ààI
e(k)
n(i)e(d)‚ä§
1
,"
M,0.9277364505844846,"where R will be chosen later. Then, by approximate orthogonality,"
M,0.9287991498405951,"v‚ä§
[CLS]WQW ‚ä§
KX‚ä§‚âàRs‚àÜR "
M,0.9298618490967057,"v[CLS] +
X"
M,0.9309245483528161,"i‚ààI
bie(T )‚ä§
i ! ,"
M,0.9319872476089267,"so that by Lemma B.7,
softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
‚àí
1
r + 1 "
M,0.9330499468650372,"e(T )‚ä§
[CLS] +
X"
M,0.9341126461211477,"i‚ààI
bie(T )‚ä§
i"
M,0.9351753453772582,"!
1
‚â§
2T
exp(R ‚àí2Rs‚àÜ)."
M,0.9362380446333688,"Finally, we have"
M,0.9373007438894793,"XWV WCe(d)
1
= ‚àíse(T )
[CLS] +
X"
M,0.9383634431455898,"i‚àà[T ]
bi ¬∑ v‚ä§
i vI ¬∑ e(T )
i
‚âàs‚àÜ‚àíse(T )
[CLS] +
X"
M,0.9394261424017003,"i‚ààI
bie(T )
i
,"
M,0.9404888416578109,"so that
|ftf-head(X) ‚ó¶Œ†1 ‚àígI(r)| ‚â§
softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
‚àí
1
r + 1 "
M,0.9415515409139213,"e(T )‚ä§
[CLS] +
X"
M,0.9426142401700319,"i‚ààI
bie(T )‚ä§
i"
M,0.9436769394261424,"!
1"
M,0.944739638682253,"XWV WCe(d)
1

‚àû+ s‚àÜ
"
M,0.9458023379383634,"+
softmax
 
v‚ä§
[CLS]WQW ‚ä§
KX‚ä§
1 (s‚àÜ)"
M,0.946865037194474,"‚â§
2Ts(1 + ‚àÜ)
exp(R ‚àí2Rs‚àÜ) + s‚àÜ."
M,0.9479277364505845,"To get (Œ≥/4)-uniform approximation, we choose"
M,0.9489904357066951,"R =
log

8T s(1+‚àÜ)"
M,0.9500531349628055,"Œ≥‚àís‚àÜ
"
M,0.9511158342189161,"1 ‚àís‚àÜ
."
M,0.9521785334750266,"Bag of vectors, s-dimensional output. We use the same construction as above, except"
M,0.953241232731137,"WC :=
X"
M,0.9543039319872476,"i‚ààI
e(k)
n(i)e(d)‚ä§
n(i) ."
M,0.9553666312433581,This will allow us to approximate the function
M,0.9564293304994687,"g‚Ä≤
I(b) =
1
r + 1 X"
M,0.9574920297555791,"i‚ààI
(bi ‚àí1)e(d)
n(i),"
M,0.9585547290116897,"which is (1/s)-injective and has absolute value is bounded by 1. Then, for each i ‚ààI, we have"
M,0.9596174282678002,"XWV WCe(d)
i
= ‚àíe(T )
[CLS] + v‚ä§
i vI ¬∑ e(T )
i
‚âàs‚àÜ‚àíe(T )
[CLS] + bie(T )
i
.
Repeating the above analysis for each coordinate, we have
ftf-head(X) ‚ó¶Œ†s ‚âàŒµ g‚Ä≤
I(r),
where a slightly tighter bound"
M,0.9606801275239107,"Œµ =
2T(1 + s‚àÜ)
exp(R ‚àí2Rs‚àÜ) + s‚àÜ"
M,0.9617428267800212,"comes from the fact that
XWV WCe(d)
i

‚àûis now bounded by 1 instead of s. The previous choice"
M,0.9628055260361318,of R sufÔ¨Åces for (Œ≥/4)-uniform approximation.
M,0.9638682252922423,"Proof of Lemma B.5. This follows by instantiating Lemma B.8 with Œ¥ = Œ≥/8, df = 1, n = s + 1.
Notice that a (Œ≥/4)-uniform approximation of a Œ≥-strictly monotone function satisÔ¨Åes the conditions
needed for Lemma B.8."
M,0.9649309245483528,"Proof of Lemma B.6. This follows by instantiating Lemma B.8 with Œ¥ = Œ≥/8, df = s, n = 2s.
Notice that a (Œ≥/4)-uniform approximation of a Œ≥-injective function satisÔ¨Åes the conditions needed
for Lemma B.8."
M,0.9659936238044633,Under review as a conference paper at ICLR 2022
M,0.9670563230605739,"C
EXPERIMENT DETAILS"
M,0.9681190223166843,"C.1
EMPIRICAL SCALING LAWS FOR LEARNING SPARSE AND GATES"
M,0.9691817215727949,"In this section, we provide details for the empirical sample complexity scaling law experiments,
which are the main empirical veriÔ¨Åcation of the log T dependence of the sample complexity arising
from the analysis."
M,0.9702444208289054,"Experimental setup.
Synthetic tasks, parameterized by sample size m and context T, were gen-
erated by the protocol described in the main paper: in each trial, one of the
 T
3

subsets of indices
was selected uniformly at random, under i.i.d. random inputs X ‚àºUnif({0, 1}T ). A 1-layer
Transformer network (with the scalar output convention) was trained with Adam (Kingma & Ba,
2014) and batch gradients of the cross entropy loss for binary classiÔ¨Åcation. For each choice of
T ‚àà{100, 150, . . . , 350, 400} ‚à™{500, 600, 700, 800} and B ‚àà{50, 60, 70, . . . , 200}, 50 inde-
pendent trials were performed (re-randomizing the dataset generation, random initialization, and
dropout masks). Cross-validation was performed on a holdout sample of size 104 every 10 itera-
tions. At the end of 1000 training iterations, the trial was counted as a success if the maximum
validation accuracy throughout training was greater than 0.99. (In 100% of runs, the training loss
was driven to < 10‚àí4, with 100% training accuracy, within 1000 iterations.)"
M,0.971307120085016,"Architecture.
Like (Chen et al., 2021a), our experimental setup is based on a popular PyTorch
implementation (https://github.com/karpathy/minGPT), with some optimizations for
faster 1-layer training and inference. This implementation includes widely-used architectural details
which deviate slightly from the theoretical presentation; refer to the referenced repository for details.
All hyperparameter settings left undiscussed are taken from the defaults in this codebase."
M,0.9723698193411264,"Hyperparameters.
A Ô¨Åxed architecture was used (d = 64, 16 parallel heads), with trainable
positional embeddings initialized with Gaussian entries N(0, œÉ2), œÉ = 0.02, 3 input token embed-
dings (corresponding to 0, 1, [CLS]), and 2 output embeddings (corresponding to the possible la-
bels 0, 1). For regularization mechanisms, typical choices were used: 0.1 for {attention, embedding,
output} dropout; 10‚àí4 weight decay. The Adam optimizer was instantiated with typical parameters
Œ∑ = 10‚àí3, Œ≤1 = 0.9, Œ≤2 = 0.999."
M,0.973432518597237,"Results and discussion.
Our Ô¨Åndings are summarized by Figure 4, enlarged from the main paper.
On the left, the fraction of successful trials is plotted for T = {100, 200, 400, 800} with standard
errors derived from the normal approximation. Notice that in this ‚Äúlow-data‚Äù regime, Transformer
training is quite sensitive to the stochasticity in the experiments, including the training data sample
(which needs to be sublinear in the context size), so there is no sharp ‚Äúphase transition‚Äù. On the
right, critical sample sizes are shown, deÔ¨Åned as the smallest m for which the fraction of successes
was greater than 0.5, with standard errors derived from 50 bootstrap samples. We observe that this
architecture is able to solve this ‚Äúplanted sparse Boolean function‚Äù task with a sublinear scaling in
the sample size."
M,0.9744952178533475,"Infrastructure and computational costs.
Each training run took at most 20 minutes on an
NVIDIA RTX A6000 GPU (with most of computation time spent on cross-validation). Although
these experiments are not in the same regime as state-of-the-art settings (such as BERT pretraining),
they require optimized GPU implementations to run in a reasonable timeframe (days, as opposed to
months)."
M,0.975557917109458,"C.2
LEARNING PARITIES"
M,0.9766206163655685,"A natural question for further exploration is whether Transformers can learn other Boolean func-
tions. In particular, the statistical probe presented for the s-way AND function is equally valid for
XOR. The latter is arguably more intriguing: parities are the basis elements in the monomial (i.e.
Fourier) expansion of a Boolean function; (O‚ÄôDonnell, 2021); furthermore, there are computational
hardness barriers; see the works cited in the main paper for further context."
M,0.9776833156216791,Under review as a conference paper at ICLR 2022
M,0.9787460148777896,"60
80
100
120
140
160
180
200
sample size 0.0 0.2 0.4 0.6 0.8 1.0"
M,0.9798087141339001,Pr[val acc > 0.99]
M,0.9808714133900106,success probabilities
M,0.9819341126461212,"T=100
T=200
T=400
T=800"
M,0.9829968119022316,"102
2 √ó 102
3 √ó 1024 √ó 102
6 √ó 102"
M,0.9840595111583422,context length T 40 60 80 100 120 140 160 180 200
M,0.9851222104144527,critical sample size
M,0.9861849096705633,empirical sample complexity
M,0.9872476089266737,"Figure 4: Enlarged plots from Figure 2, the main experimental validation of our theory."
M,0.9883103081827843,"Experimental setup.
These experiments match the setting of the main AND experiments, except
for a few differences, which we enumerate here. Gradient-based training is done on streaming online
losses (so that there is no training/validation split), with batch size 2048, for 10000 iterations (since
parities take signiÔ¨Åcantly longer to Ô¨Åt)."
M,0.9893730074388948,"Discussion.
The key observation of this exploratory experiment is to point out the phenomenon
that Transformer architectures can Ô¨Åt sparse parities at all (rather than to measure any particular
trend), pointing to computational mysteries discussed in the main paper. We do not present empirical
scaling laws for this set of experiments, as the experiments with small batch sizes are higher in
variance, and we are unable to scale this phenomenon (i.e. learn any parities) at context sizes above
T = 50."
M,0.9904357066950054,"D
ADDITIONAL RELATED WORK"
M,0.9914984059511158,"In this section, we discuss some additional related work."
M,0.9925611052072264,"Attention-based architectures.
Building upon the success of modern attention-based architec-
tures, a large body of work (e.g. Goyal et al. (2020; 2021), and the works cited within) has sought
to explicitly induce model sparsity and modularity in the architecture design. Our analysis is rele-
vant to any architecture that uses a softmax (or similar) bottleneck for statistical capacity, and could
inform design principles for norm-based capacity control of these architectures."
M,0.9936238044633369,"Expressive power of Transformers.
Several works establish results on the representational power
of self-attention architectures in regimes where the statistical guarantees are necessarily weak or
vacuous (i.e. there are too many functions in the class). Dehghani et al. (2018); Yun et al. (2019);
Bhattamishra et al. (2020a;b) establish universal function approximation and Turing-completeness,
which have been known for previous architectures (Siegelmann & Sontag, 1995). Our work is a
signiÔ¨Åcantly Ô¨Åner-grained analysis, in which we establish a hierarchy of function classes (indexed
by sparsity s) representable by these architectures, with tight (in terms of T) statistical guarantees.
Hron et al. (2020); Yang (2020) analyze properties of the kernels induced by Transformers at the
inÔ¨Ånite-width limit."
M,0.9946865037194474,"Likhosherstov et al. (2021) analyze the sparsity patterns representable by a self-attention head, with
results superÔ¨Åcially similar to ours: when the embedding dimension is at least logarithmic in the
context length, all sparse matrices can be approximately realized by an attention head. However,
their analysis is not about the capacity of the function class: it quantiÔ¨Åes over the input X, and
holds the parameters (WQ, WK, . . .) to be constant (rather than vice versa). This Ô¨Ånding serves
as an interesting complement to our result: even though the attention mixture weights can take on
exponentially many sparsity patterns for distinct inputs, the generalization error scales as log(T)."
M,0.9957492029755579,Under review as a conference paper at ICLR 2022
M,0.9968119022316685,"Interpreting attention mixtures.
A line of empirical work (‚ÄúBERTology‚Äù) has made progress on
understanding and interpreting state-of-the-art Transformer language models by examining the acti-
vations of their attention mechanisms (Clark et al., 2019; Tenney et al., 2019; Rogers et al., 2020). In
some cases, these works have found instances in which Transformers seem to have learned features
that are reminiscent of (sparse) hand-crafted features used in natural language processing, without
explicit supervision. Our analysis formalizes the intuition that self-attention heads can represent
sparse interactions within the context in a statistically meaningful way."
M,0.997874601487779,"Other theoretical work on self-attention.
Kerg et al. (2020) analyze self-attention and its ben-
eÔ¨Åt for learning long-term dependencies by establishing gradient norms bounds and showing how
attention helps address the problem of gradient vanishing in recurrent networks. In contrast to our
results that analyze the statistical and representational properties of attention-based architectures,
this work focuses on the computational aspects of gradient-based methods on recurrent networks
with self-attention."
M,0.9989373007438895,"Synthetic experiments with Transformers.
Power et al. (2021) train small Transformer networks
on synthetic algebraic tasks, and discover an abrupt phase transition from overÔ¨Åtting to correct
generalization similar to ours. Tay et al. (2020) propose some synthetic tasks for benchmarking
the ability of Transformer variants to capture long-range dependences. Chen et al. (2021a) present
a synthetic demonstration of extrapolation (inferring a maximum-reward path from random walk
observations) when using Transformers for ofÔ¨Çine reinforcement learning. Lu et al. (2021) probe the
transfer learning capabilities of pretrained Transformers, and consider some simple Boolean tasks.
Our experimental protocol of learning sparse Boolean functions provides a simple and fundamental
setting for elucidating computational and statistical properties of sequence modeling architectures."
