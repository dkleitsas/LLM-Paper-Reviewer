Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0019120458891013384,"Reinforcement Learning (RL) algorithms have shown tremendous success in simu-
1"
ABSTRACT,0.0038240917782026767,"lation environments, but their application to real-world problems faces signiﬁcant
2"
ABSTRACT,0.0057361376673040155,"challenges, with safety being a major concern. In particular, enforcing state-wise
3"
ABSTRACT,0.0076481835564053535,"constraints is essential for many challenging tasks such as autonomous driving
4"
ABSTRACT,0.009560229445506692,"and robot manipulation. However, existing safe RL algorithms under the frame-
5"
ABSTRACT,0.011472275334608031,"work of Constrained Markov Decision Process (CMDP) do not consider state-wise
6"
ABSTRACT,0.01338432122370937,"constraints. To address this gap, we propose State-wise Constrained Policy Opti-
7"
ABSTRACT,0.015296367112810707,"mization (SCPO), the ﬁrst general-purpose policy search algorithm for state-wise
8"
ABSTRACT,0.017208413001912046,"constrained reinforcement learning. SCPO provides guarantees for state-wise con-
9"
ABSTRACT,0.019120458891013385,"straint satisfaction in expectation. In particular, we introduce the framework of
10"
ABSTRACT,0.021032504780114723,"Maximum Markov Decision Process, and prove that the worst-case safety violation
11"
ABSTRACT,0.022944550669216062,"is bounded under SCPO. We demonstrate the effectiveness of our approach on
12"
ABSTRACT,0.0248565965583174,"training neural network policies for extensive robot locomotion tasks, where the
13"
ABSTRACT,0.02676864244741874,"agent must satisfy a variety of state-wise safety constraints. Our results show
14"
ABSTRACT,0.028680688336520075,"that SCPO signiﬁcantly outperforms existing methods and can handle state-wise
15"
ABSTRACT,0.030592734225621414,"constraints in high-dimensional robotics tasks.
16"
INTRODUCTION,0.032504780114722756,"1
Introduction
17"
INTRODUCTION,0.03441682600382409,"Reinforcement learning (RL) has achieved remarkable progress in games and control tasks [Mnih
18"
INTRODUCTION,0.036328871892925434,"et al., 2015, Vinyals et al., 2019, Brown and Sandholm, 2018, He et al., 2022, Zhao et al., 2019].
19"
INTRODUCTION,0.03824091778202677,"However, one major barrier that limits the application of RL algorithms to real-world problems is
20"
INTRODUCTION,0.040152963671128104,"the lack of safety assurance. RL agents learn to make reward-maximizing decisions, which may
21"
INTRODUCTION,0.04206500956022945,"violate safety constraints. For example, an RL agent controlling a self-driving car may receive high
22"
INTRODUCTION,0.04397705544933078,"rewards by driving at high speeds but will be exposed to high chances of collision. Although the
23"
INTRODUCTION,0.045889101338432124,"reward signals can be designed to penalize risky behaviors, there is no guarantee for safety. In other
24"
INTRODUCTION,0.04780114722753346,"words, RL agents may sometimes prioritize maximizing the reward over ensuring safety, which can
25"
INTRODUCTION,0.0497131931166348,"lead to unsafe or even catastrophic outcomes [Gu et al., 2022].
26"
INTRODUCTION,0.05162523900573614,"Emerging in the literature, safe RL aims to provide safety guarantees during or after training. Early
27"
INTRODUCTION,0.05353728489483748,"attempts have been made under the framework of constrained Markov Decision Process, where the
28"
INTRODUCTION,0.055449330783938815,"majority of works enforce cumulative constraints or chance constraints [Ray et al., 2019, Achiam
29"
INTRODUCTION,0.05736137667304015,"et al., 2017a, Liu et al., 2021]. In real-world applications, however, many critical constraints are
30"
INTRODUCTION,0.05927342256214149,"instantaneous. For instance, collision avoidance must be enforced at all times for autonomous
31"
INTRODUCTION,0.06118546845124283,"cars [Zhao et al., 2023]. Another example is that when a robot holds a glass, the robot can only
32"
INTRODUCTION,0.06309751434034416,"release the glass when the glass is on a stable surface. The violation of those constraints will lead to
33"
INTRODUCTION,0.06500956022944551,"irreversible failures of the task. In this work, we focus on state-wise (instantaneous) constraints.
34"
INTRODUCTION,0.06692160611854685,"The State-wise Constrained Markov Decision Process (SCMDP) is a novel formulation in reinforce-
35"
INTRODUCTION,0.06883365200764818,"ment learning that requires policies to satisfy hard state-wise constraints. Unlike cumulative or
36"
INTRODUCTION,0.07074569789674952,"probabilistic constraints, state-wise constraints demand full compliance at each time step as for-
37"
INTRODUCTION,0.07265774378585087,"malized by Zhao et al. [2023]. Existing state-wise safe RL methods can be categorized based on
38"
INTRODUCTION,0.0745697896749522,"whether safety is ensured during training. There is a fundamental limitation that it is impossible to
39"
INTRODUCTION,0.07648183556405354,"guarantee hard state-wise safety during training without prior knowledge of the dynamic model. The
40"
INTRODUCTION,0.07839388145315487,"best we can achieve in a model free setting is to learn to satisfy the constraints using as few samples
41"
INTRODUCTION,0.08030592734225621,"as possible, which is the focus of this paper. We aim to provide theoretical guarantees on state-wise
42"
INTRODUCTION,0.08221797323135756,"safety violation and worst case reward degredation during training.
43"
INTRODUCTION,0.0841300191204589,"Our approach is underpinned by a key insight that constraining the maximum violation is equivalent
44"
INTRODUCTION,0.08604206500956023,"to enforcing state-wise safety. This insight leads to a novel formulation of MDP called the Maximum
45"
INTRODUCTION,0.08795411089866156,"Markov Decision Process (MMDP). With MMDP, we establish a new theoretical result that provides
46"
INTRODUCTION,0.08986615678776291,"a bound on the difference between the maximum cost of two policies for episodic tasks. This result
47"
INTRODUCTION,0.09177820267686425,"expands upon the cumulative discounted reward and cost bounds for policy search using trust regions,
48"
INTRODUCTION,0.09369024856596558,"as previously documented in literature [Achiam et al., 2017b]. We leverage this result to design a
49"
INTRODUCTION,0.09560229445506692,"policy improvement step that not only guarantees worst-case performance degradation but also ensures
50"
INTRODUCTION,0.09751434034416825,"state-wise cost constraints. Our proposed algorithm, State-wise Constrained Policy Optimization
51"
INTRODUCTION,0.0994263862332696,"(SCPO), approximates the theoretically-justiﬁed update, which achieves a state-of-the-art trade-off
52"
INTRODUCTION,0.10133843212237094,"between safety and performance. Through experiments, we demonstrate that SCPO effectively
53"
INTRODUCTION,0.10325047801147227,"trains neural network policies with thousands of parameters on high-dimensional simulated robot
54"
INTRODUCTION,0.10516252390057361,"locomotion tasks; and is able to optimize rewards while enforcing state-wise safety constraints. This
55"
INTRODUCTION,0.10707456978967496,"work represents a signiﬁcant step towards developing practical safe RL algorithms that can be applied
56"
INTRODUCTION,0.1089866156787763,"to many real-world problems.
57"
RELATED WORK,0.11089866156787763,"2
Related Work
58"
CUMULATIVE SAFETY,0.11281070745697896,"2.1
Cumulative Safety
59"
CUMULATIVE SAFETY,0.1147227533460803,"Cumulative safety requires that the expected discounted return with respect to some cost function is
60"
CUMULATIVE SAFETY,0.11663479923518165,"upper-bounded over the entire trajectory. One representative approach is constrained policy optimiza-
61"
CUMULATIVE SAFETY,0.11854684512428298,"tion (CPO) [Achiam et al., 2017a], which builds on a theoretical bound on the difference between
62"
CUMULATIVE SAFETY,0.12045889101338432,"the costs of different policies and derives a policy improvement procedure to ensure constraints
63"
CUMULATIVE SAFETY,0.12237093690248566,"satisfaction. Another approach is interior-point policy optimization (IPO) [Liu et al., 2019], which
64"
CUMULATIVE SAFETY,0.124282982791587,"augments the reward-maximizing objective with logarithmic barrier functions as penalty functions
65"
CUMULATIVE SAFETY,0.12619502868068833,"to accommodate the constraints. Other methods include Lagrangian methods [Ray et al., 2019]
66"
CUMULATIVE SAFETY,0.12810707456978968,"which use adaptive penalty coefﬁcients to enforce constraints and projection-based constrained
67"
CUMULATIVE SAFETY,0.13001912045889102,"policy optimization (PCPO) [Yang et al., 2020a] which projects trust-region policy updates onto the
68"
CUMULATIVE SAFETY,0.13193116634799235,"constraint set. Although our focus is on a different setting of constraints, existing methods are still
69"
CUMULATIVE SAFETY,0.1338432122370937,"valuable references for illustrating the advantages of our SCPO. By utilizing MMDP, SCPO breaks
70"
CUMULATIVE SAFETY,0.13575525812619502,"the conventional safety-reward trade-off, which results in stronger convergence of state-wise safety
71"
CUMULATIVE SAFETY,0.13766730401529637,"constraints and guaranteed performance degradation bounds.
72"
STATE-WISE SAFETY,0.13957934990439771,"2.2
State-wise Safety
73"
STATE-WISE SAFETY,0.14149139579349904,"Hierarchical Policy
One way to enforce state-wise safety constraints is to use hierarchical policies,
74"
STATE-WISE SAFETY,0.14340344168260039,"with an RL policy generating reward-maximizing actions, and a safety monitor modifying the actions
75"
STATE-WISE SAFETY,0.14531548757170173,"to satisfy state-wise safety constraints. Such an approach often requires a perfect safety critic to
76"
STATE-WISE SAFETY,0.14722753346080306,"function well. For example, conservative safety critics (CSC) [Bharadhwaj et al., 2020] propose
77"
STATE-WISE SAFETY,0.1491395793499044,"a safe critic QC(s, a), providing a conservative estimate of the likelihood of being unsafe given a
78"
STATE-WISE SAFETY,0.15105162523900573,"state-action pair. If the safety violation exceeds a predeﬁned threshold, a new action is re-sampled
79"
STATE-WISE SAFETY,0.15296367112810708,"from the policy until it passes the safety critic. However, this approach is time-consuming. On
80"
STATE-WISE SAFETY,0.15487571701720843,"the other hand, optimization-based methods such as gradient descent or quadratic programming
81"
STATE-WISE SAFETY,0.15678776290630975,"can be used to ﬁnd a safe action that satisﬁes the constraint while staying close to the reference
82"
STATE-WISE SAFETY,0.1586998087954111,"action. Unrolling safety layer (USL) [Zhang et al., 2022a] follows a similar hierarchical structure as
83"
STATE-WISE SAFETY,0.16061185468451242,"CSC but performs gradient descent on the reference action iteratively until the constraint is satisﬁed
84"
STATE-WISE SAFETY,0.16252390057361377,"based on learned safety critic QC(s, a). Finally, instead of using gradient descent, Lyapunov-based
85"
STATE-WISE SAFETY,0.16443594646271512,"policy gradient (LPG) [Chow et al., 2019] and SafeLayer [Dalal et al., 2018] directly solve quadratic
86"
STATE-WISE SAFETY,0.16634799235181644,"programming (QP) to project actions to the safe action set induced by the linearized versions of some
87"
STATE-WISE SAFETY,0.1682600382409178,"learned critic QC(s, a). All these approaches suffer from safety violations due to imperfect critic
88"
STATE-WISE SAFETY,0.1701720841300191,"QC(s, a), while those solving QPs further suffer from errors due to the linear approximation of the
89"
STATE-WISE SAFETY,0.17208413001912046,"critic. To avoid those issues, we propose SCPO as an end-to-end policy which does not explicitly
90"
STATE-WISE SAFETY,0.1739961759082218,"maintain a safety monitor.
91"
STATE-WISE SAFETY,0.17590822179732313,"End-to-End Policy
End-to-end policies maximize task rewards while ensuring safety at the same
92"
STATE-WISE SAFETY,0.17782026768642448,"time. Related work regarding state-wise safety after convergence has been explored recently. Some
93"
STATE-WISE SAFETY,0.17973231357552583,"approaches [Liang et al., 2018, Tessler et al., 2018] solve a primal-dual optimization problem to
94"
STATE-WISE SAFETY,0.18164435946462715,"satisfy the safety constraint in expectation. However, the associated optimization is hard in practice
95"
STATE-WISE SAFETY,0.1835564053537285,"because the optimization problem changes at every learning step. Bohez et al. [2019] approaches
96"
STATE-WISE SAFETY,0.18546845124282982,"the same setting by augmenting the reward with the sum of the constraint penalty weighted by the
97"
STATE-WISE SAFETY,0.18738049713193117,"Lagrangian multiplier. Although claimed state-wise safety performance, the aforementioned methods
98"
STATE-WISE SAFETY,0.18929254302103252,"do not provide theoretical guarantee and fail to achieve near-zero safety violation in practice. He
99"
STATE-WISE SAFETY,0.19120458891013384,"et al. [2023] proposes AutoCost to automatically ﬁnd an appropriate cost function using evolutionary
100"
STATE-WISE SAFETY,0.1931166347992352,"search over the space of cost functions as parameterized by a simple neural network. It is empirically
101"
STATE-WISE SAFETY,0.1950286806883365,"shown that the evolved cost functions achieve near-zero safety violation, however, no theoretical
102"
STATE-WISE SAFETY,0.19694072657743786,"guarantee is provided, and extensive computation is required. FAC [Ma et al., 2021] does provide
103"
STATE-WISE SAFETY,0.1988527724665392,"theoretically guaranteed state-wise safety via parameterized Lagrange functions. However, FAC
104"
STATE-WISE SAFETY,0.20076481835564053,"replies on strong assumptions and performs poorly in practice. To resolve the above issues, we
105"
STATE-WISE SAFETY,0.20267686424474188,"propose SCPO as an easy-to-implement and theoretically sound approach with no prior assumptions
106"
STATE-WISE SAFETY,0.2045889101338432,"on the underlying safety functions.
107"
PROBLEM FORMULATION,0.20650095602294455,"3
Problem Formulation
108"
PRELIMINARIES,0.2084130019120459,"3.1
Preliminaries
109"
PRELIMINARIES,0.21032504780114722,"In this paper, we are especially interested in guaranteeing safety for episodic tasks, which falls within
110"
PRELIMINARIES,0.21223709369024857,"in the scope of ﬁnite-horizon Markov Decision Process (MDP). An MDP is speciﬁed by a tuple
111"
PRELIMINARIES,0.21414913957934992,"(S, A, γ, R, P, µ), where S is the state space, and A is the control space, R : S ⇥A 7! R is the
112"
PRELIMINARIES,0.21606118546845124,"reward function, 0 γ < 1 is the discount factor, µ : S 7! R is the initial state distribution, and
113"
PRELIMINARIES,0.2179732313575526,"P : S ⇥A⇥S 7! R is the transition probability function. P(s0|s, a) is the probability of transitioning
114"
PRELIMINARIES,0.2198852772466539,"to state s0 given that the previous state was s and the agent took action a at state s. A stationary
115"
PRELIMINARIES,0.22179732313575526,"policy ⇡: S 7! P(A) is a map from states to a probability distribution over actions, with ⇡(a|s)
116"
PRELIMINARIES,0.2237093690248566,"denoting the probability of selecting action a in state s. We denote the set of all stationary policies by
117"
PRELIMINARIES,0.22562141491395793,"⇧. Subsequently, we denote ⇡✓as the policy that is parameterized by the parameter ✓.
118"
PRELIMINARIES,0.22753346080305928,"The standard goal for MDP is to learn a policy ⇡that maximizes a performance measure J0(⇡) which
119"
PRELIMINARIES,0.2294455066921606,"is computed via the discounted sum of reward:
120"
PRELIMINARIES,0.23135755258126195,"J0(⇡) = E⌧⇠⇡ "" H
X t=0"
PRELIMINARIES,0.2332695984703633,"γtR(st, at, st+1) # ,
(1)"
PRELIMINARIES,0.23518164435946462,"where H 2 N is the horizon, ⌧= [s0, a0, s1, · · · ], and ⌧⇠⇡is shorthand for that the distribution
121"
PRELIMINARIES,0.23709369024856597,"over trajectories depends on ⇡: s0 ⇠µ, at ⇠⇡(·|st), st+1 ⇠P(·|st, at).
122"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2390057361376673,"3.2
State-wise Constrained Markov Decision Process
123"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.24091778202676864,"A constrained Markov Decision Process (CMDP) is an MDP augmented with constraints that restrict
124"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.24282982791587,"the set of allowable policies. Speciﬁcally, CMDP introduces a set of cost functions, C1, C2, · · · , Cm,
125"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2447418738049713,"where Ci : S ⇥A ⇥S 7! R maps the state action transition tuple into a cost value. Analogous to (1),
126"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.24665391969407266,"we denote
127"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.248565965583174,"JCi(⇡) = E⌧⇠⇡ "" H
X t=0"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.25047801147227533,"γtCi(st, at, st+1) # (2)"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.25239005736137665,"as the cost measure for policy ⇡with respect to cost function Ci. Hence, the set of feasible stationary
128"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.25430210325047803,"policies for CMDP is then deﬁned as follows, where di 2 R:
129"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.25621414913957935,⇧C = {⇡2 ⇧
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.25812619502868067,"$$ 8i, JCi(⇡) di}.
(3)"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.26003824091778205,"In CMDP, the objective is to select a feasible stationary policy ⇡✓that maximizes the performance
130"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.26195028680688337,"measure:
131 max"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2638623326959847,"⇡
J0(⇡), s.t. ⇡2 ⇧C.
(4)"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.26577437858508607,"In this paper, we are interested in a special type of CMDP where the safety speciﬁcation is to persis-
132"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2676864244741874,"tently satisfy a hard cost constraint at every step (as opposed to cumulative costs over trajectories),
133"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2695984703632887,"which we refer to as State-wise Constrained Markov Decision Process (SCMDP). Like CMDP,
134"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.27151051625239003,"SCMDP uses the set of cost functions C1, C2, · · · , Cm to evaluate the instantaneous cost of state
135"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2734225621414914,"action transition tuples. Unlike CMDP, SCMDP requires the cost for every state action transition to
136"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.27533460803059273,"satisfy a hard constraint. Hence, the set of feasible stationary policies for SCMDP is deﬁned as
137"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.27724665391969405,¯⇧C = {⇡2 ⇧
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.27915869980879543,"$$8i, E(st,at,st+1)⇠⌧,⌧⇠⇡ ⇥"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.28107074569789675,"Ci(st, at, st+1) ⇤"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2829827915869981,"wi}
(5)"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.28489483747609945,"where wi 2 R. Then the objective for SCMDP is to ﬁnd a feasible stationary policy from ¯⇧C that
138"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.28680688336520077,"maximizes the performance measure. Formally,
139 max"
STATE-WISE CONSTRAINED MARKOV DECISION PROCESS,0.2887189292543021,"⇡
J0(⇡), s.t. ⇡2 ¯⇧C
(6)"
MAXIMUM MARKOV DECISION PROCESS,0.29063097514340347,"3.3
Maximum Markov Decision Process
140"
MAXIMUM MARKOV DECISION PROCESS,0.2925430210325048,"Note that for (6), every state-action transition pair corresponds to a constraint, which is intractable to
141"
MAXIMUM MARKOV DECISION PROCESS,0.2944550669216061,"solve using conventional reinforement learning algorithms. Our intuition is that, instead of directly
142"
MAXIMUM MARKOV DECISION PROCESS,0.29636711281070743,"constraining the cost of each possible state-action transition, we can constrain the expected maximum
143"
MAXIMUM MARKOV DECISION PROCESS,0.2982791586998088,"state-wise cost along the trajectory, which is much easier to solve. Following that intuition, we deﬁne
144"
MAXIMUM MARKOV DECISION PROCESS,0.30019120458891013,"a novel Maximum Markov-Decision Process (MMDP), which further extends CMDP via (i) a set of
145"
MAXIMUM MARKOV DECISION PROCESS,0.30210325047801145,"up-to-now maximum state-wise costs M .= [M1, M2, · · · , Mm] where Mi 2 M ⇢R, and (ii) a set
146"
MAXIMUM MARKOV DECISION PROCESS,0.30401529636711283,"of cost increment functions, D1, D2, · · · , Dm, where Di : (S, Mm) ⇥A ⇥S 7! [0, R+] maps the
147"
MAXIMUM MARKOV DECISION PROCESS,0.30592734225621415,"augmented state action transition tuple into a non-negative cost increment. We deﬁne the augmented
148"
MAXIMUM MARKOV DECISION PROCESS,0.3078393881453155,"state ˆs = (s, M) 2 (S, Mm) .= ˆS, where ˆS is the augmented state space. Formally,
149 Di '"
MAXIMUM MARKOV DECISION PROCESS,0.30975143403441685,"ˆst, at, ˆst+1 ("
MAXIMUM MARKOV DECISION PROCESS,0.31166347992351817,"= max{Ci(st, at, st+1) −Mit, 0}.
(7)"
MAXIMUM MARKOV DECISION PROCESS,0.3135755258126195,By setting Di '
MAXIMUM MARKOV DECISION PROCESS,0.3154875717017208,"ˆs0, a0, ˆs1 ("
MAXIMUM MARKOV DECISION PROCESS,0.3173996175908222,"= Ci(s0, a0, s1), we have Mit = Pt−1"
MAXIMUM MARKOV DECISION PROCESS,0.3193116634799235,k=0 Di '
MAXIMUM MARKOV DECISION PROCESS,0.32122370936902483,"ˆsk, ak, ˆsk+1 ("
MAXIMUM MARKOV DECISION PROCESS,0.3231357552581262,"for t ≥1.
150"
MAXIMUM MARKOV DECISION PROCESS,0.32504780114722753,"Hence, we deﬁne expected maximum state-wise cost (or Di-return) for ⇡:
151"
MAXIMUM MARKOV DECISION PROCESS,0.32695984703632885,"JDi(⇡) = E⌧s⇡ "" H
X t=0 Di '"
MAXIMUM MARKOV DECISION PROCESS,0.32887189292543023,"ˆst, at, ˆst+1 ( # .
(8)"
MAXIMUM MARKOV DECISION PROCESS,0.33078393881453155,"Importantly, (8) is the key component of MMDP and differs our work from existing safe RL ap-
152"
MAXIMUM MARKOV DECISION PROCESS,0.3326959847036329,"proaches that are based on CMDP cost measure (2). With (8), (6) can be rewritten as:
153 max"
MAXIMUM MARKOV DECISION PROCESS,0.33460803059273425,"⇡
J (⇡), s.t. 8i, JDi(⇡) wi,
(9)"
MAXIMUM MARKOV DECISION PROCESS,0.3365200764818356,where J (⇡) = E⌧⇠⇡ hPH
MAXIMUM MARKOV DECISION PROCESS,0.3384321223709369,"t=0 γtR(ˆst, at, ˆst+1) i"
MAXIMUM MARKOV DECISION PROCESS,0.3403441682600382,"and R(ˆs, a, ˆs0) .= R(s, a, s0). With R(⌧) being the
154"
MAXIMUM MARKOV DECISION PROCESS,0.3422562141491396,"discounted return of a trajectory, we deﬁne the on-policy value function as V ⇡(ˆs) .= E⌧⇠⇡[R(⌧)|ˆs0 =
155"
MAXIMUM MARKOV DECISION PROCESS,0.3441682600382409,"ˆs], the on-policy action-value function as Q⇡(ˆs, a) .= E⌧⇠⇡[R(⌧)|ˆs0 = ˆs, a0 = a], and the advantage
156"
MAXIMUM MARKOV DECISION PROCESS,0.34608030592734224,"function as A⇡(ˆs, a) .= Q⇡(ˆs, a) −V ⇡(ˆs). Lastly, we deﬁne on-policy value functions, action-value
157"
MAXIMUM MARKOV DECISION PROCESS,0.3479923518164436,"functions, and advantage functions for the cost increments in analogy to V ⇡, Q⇡, and A⇡, with Di
158"
MAXIMUM MARKOV DECISION PROCESS,0.34990439770554493,"replacing R, respectively. We denote those by V ⇡"
MAXIMUM MARKOV DECISION PROCESS,0.35181644359464626,"Di, Q⇡"
MAXIMUM MARKOV DECISION PROCESS,0.35372848948374763,Di and A⇡
MAXIMUM MARKOV DECISION PROCESS,0.35564053537284895,"Di.
159"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3575525812619503,"4
State-wise Constrained Policy Optimization
160"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.35946462715105165,"To solve large and continuous MDPs, policy search algorithms search for the optimal policy within a
161"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.361376673040153,"set ⇧✓⇢⇧of parametrized policies. In local policy search [Peters and Schaal, 2008], the policy is
162"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3632887189292543,"iteratively updated by maximizing J (⇡) over a local neighborhood of the most recent policy ⇡k. In
163"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3652007648183556,"local policy search for SCMDPs, policy iterates must be feasible, so optimization is over ⇧✓"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.367112810707457,"T ¯⇧C.
164"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3690248565965583,"The optimization problem is:
165"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.37093690248565964,⇡k+1 = argmax ⇡2⇧✓
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.372848948374761,"J (⇡),
(10)"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.37476099426386233,"s.t. Dist(⇡, ⇡k) δ,"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.37667304015296366,"JDi(⇡) wi, i = 1, · · · , m."
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.37858508604206503,"where Dist is some distance measure, and δ > 0 is a step size. For actual implementation, we need
166"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.38049713193116635,"to evaluate the constraints ﬁrst in order to determine the feasible set. However, it is challenging to
167"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3824091778202677,"evaluate the constraints using samples during the learning process. In this work, we propose SCPO
168"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.384321223709369,"inspired by recent trust region optimization methods Schulman et al. [2015]. SCPO approximates
169"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3862332695984704,"(10) using (i) KL divergence distance metric Dist and (ii) surrogate functions for the objective and
170"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3881453154875717,"constraints, which can be easily estimated from samples on ⇡k. Mathematically, SCPO requires
171"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.390057361376673,"the policy update at each iteration is bounded within a trust region, and updates policy via solving
172"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3919694072657744,"following optimization:
173"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3938814531548757,⇡k+1 = argmax ⇡2⇧✓
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.39579349904397704,"E
ˆs⇠d⇡k a⇠⇡"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.3977055449330784,"[A⇡k(ˆs, a)]
(11)"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.39961759082217974,"s.t. Eˆs⇠¯d⇡k [DKL(⇡k⇡k)[ˆs]] δ,"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.40152963671128106,"JDi(⇡k) +
E
ˆs⇠¯d⇡k a⇠⇡ "" A⇡k"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.40344168260038243,"Di(ˆs, a) #"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.40535372848948376,+ 2(H + 1)✏⇡ Di r
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4072657743785851,"1
2δ wi, i = 1, · · · , m."
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4091778202676864,"where DKL(⇡0k⇡)[ˆs] is KL divergence between two policy (⇡0, ⇡) at state ˆs, the set {⇡2
174"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4110898661567878,"⇧✓: Eˆs⇠¯d⇡k [DKL(⇡k⇡k)[ˆs]] δ} is called trust region, d⇡k .= (1 −γ) PH"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4130019120458891,"t=0 γtP(ˆst = ˆs|⇡k),
175"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4149139579349904,¯d⇡k .= PH
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4168260038240918,t=0 P(ˆst = ˆs|⇡k) and ✏⇡ Di
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4187380497131931,.= maxˆs|Ea⇠⇡[A⇡k
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.42065009560229444,"Di(ˆs, a)]|. We then show that SCPO guaran-
176"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4225621414913958,"tees (i) worst case maximum state-wise cost violation, and (ii) worst case performance degradation
177"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.42447418738049714,"for policy update, by establishing new bounds on the difference in returns between two stochastic
178"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.42638623326959846,"policies ⇡and ⇡0 for MMDPs.
179"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.42829827915869984,"Theoretical Guarantees for SCPO
We start with the theoretical foundation for our approach,
180"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.43021032504780116,"i.e. a new bound on the difference in state-wise maximum cost between two arbitrary policies. The
181"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4321223709369025,"following theorem connects the difference in maximum state-wise cost between two arbitrary policies
182"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4340344168260038,"to the total variation divergence between them. Here total variation divergence between discrete
183"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4359464627151052,"probability distributions p, q is deﬁned as DT V (pkq) = 1 2 P"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4378585086042065,"i |pi −qi|. This measure can be easily
184"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4397705544933078,"extended to continuous states and actions by replacing the sums with integrals. Thus, the total variation
185"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4416826003824092,"divergence between two policy (⇡0, ⇡) at state ˆs is deﬁned as: DT V (⇡0k⇡)[ˆs] = DT V (⇡0(·|ˆs)k⇡(·|ˆs)).
186"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4435946462715105,"Theorem 1 (Trust Region Update State-wise Maximum Cost Bound). For any policies ⇡0, ⇡, with
187 ✏⇡0 D"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.44550669216061184,.= maxˆs|Ea⇠⇡0[A⇡
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4474187380497132,"D(ˆs, a)]|, and deﬁne ¯d⇡= PH"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.44933078393881454,"t=0 P(ˆst = ˆs|⇡) as the non-discounted aug-
188"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.45124282982791586,"mented state distribution using ⇡, then the following bound holds:
189"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.45315487571701724,"JD(⇡0) −JD(⇡) 
E
ˆs⇠¯d⇡
a⇠⇡0 h A⇡"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.45506692160611856,"D(ˆs, a) + 2(H + 1)✏⇡0"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4569789674952199,D DT V (⇡0||⇡)[ˆs] i
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4588910133843212,".
(12)"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4608030592734226,"The proof for Theorem 1 is summarized in Appendix A. Next, we note the following relationship
190"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4627151051625239,"between the total variation divergence and the KL divergence [Boyd et al., 2003, Achiam et al., 2017a]:
191"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4646271510516252,Eˆs⇠¯d⇡[DT V (pkq)[ˆs]]  q
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4665391969407266,"1
2Eˆs⇠¯d⇡[DKL(pkq)[ˆs]]. The following bound then follows directly from
192"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4684512428298279,"Theorem 1:
193"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.47036328871892924,"JD(⇡0) JD(⇡) +
E
ˆs⇠¯d⇡
a⇠⇡0 "" A⇡"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4722753346080306,"D(ˆs, a) + 2(H + 1)✏⇡0 D r"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.47418738049713194,"1
2Eˆs⇠¯d⇡[DKL(⇡0k⇡)[ˆs]] #"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.47609942638623326,".
(13)"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4780114722753346,"By Equation (13), we have a guarantee for satisfaction of maximum state-wise constraints:
194"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.47992351816443596,"Proposition 1 (SCPO Update Constraint Satisfaction). Suppose ⇡k, ⇡k+1 are related by (11), then
195"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4818355640535373,"Di-return for ⇡k+1 satisﬁes
196"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4837476099426386,"8i, JDi(⇡k+1) wi. 197"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.48565965583174,"Proposition 1 presents the ﬁrst constraint satisfaction guarantee under MMDP. Unlike trust region
198"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4875717017208413,"methods such as CPO and TRPO, which assume a discounted sum characteristic, MMDP’s non-
199"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4894837476099426,"discounted sum characteristic invalidates these theories. As the maximum state-wise cost is calculated
200"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.491395793499044,"through a summation of non-discounted increments, analysis must be performed on a ﬁnite horizon to
201"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.4933078393881453,"upper bound the worst-case summation. In contrast, the theory behind CPO relies on inﬁnite horizon
202"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.49521988527724664,"analysis with discounted constraint assumptions, which is not applicable for MMDP settings.
203"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.497131931166348,"Next, we provide the performance guarantee of SCPO. Previous analyses of performance guarantees
204"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.49904397705544934,"have focused on inﬁnite-horizon MDP. We generalize the analysis to ﬁnite-horizon MDP, inspired
205"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.5009560229445507,"by previous work [Kakade and Langford, 2002, Schulman et al., 2015, Achiam et al., 2017a], and
206"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.502868068833652,"prove it in Appendix B. The inﬁnite-horizon case can be viewed as a special case of the ﬁnite-horizon
207"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.5047801147227533,"setting.
208"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.5066921606118547,"Proposition 2 (SCPO Update Worst Performance Degradation). Suppose ⇡k, ⇡k+1 are related by
209"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.5086042065009561,"(11), with ✏⇡k+1 .= maxˆs|Ea⇠⇡k+1[A⇡k(ˆs, a)]|, then performance return for ⇡k+1 satisﬁes
210"
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.5105162523900574,J (⇡k+1) −J (⇡k) ≥− p
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.5124282982791587,2δγ✏⇡k+1
STATE-WISE CONSTRAINED POLICY OPTIMIZATION,0.51434034416826,"1 −γ
."
PRACTICAL IMPLEMENTATION,0.5162523900573613,"5
Practical Implementation
211"
PRACTICAL IMPLEMENTATION,0.5181644359464627,"In this section, we show how to (a) implement an efﬁcient approximation to the update (11), (b)
212"
PRACTICAL IMPLEMENTATION,0.5200764818355641,"encourage learning even when (11) becomes infeasible, and (c) handle the difﬁculty of ﬁtting
213"
PRACTICAL IMPLEMENTATION,0.5219885277246654,augmented value V ⇡
PRACTICAL IMPLEMENTATION,0.5239005736137667,"Di which is unique to our novel MMDP formulation. The full SCPO pseudocode
214"
PRACTICAL IMPLEMENTATION,0.5258126195028681,"is given as algorithm 1 in appendix C.
215"
PRACTICAL IMPLEMENTATION,0.5277246653919694,"Practical implementation with sample-based estimation
We ﬁrst estimate the objective and
216"
PRACTICAL IMPLEMENTATION,0.5296367112810707,"constraints in (11) using samples. Note that we can replace the expected advantage on rewards using
217"
PRACTICAL IMPLEMENTATION,0.5315487571701721,"an importance sampling estimator with a sampling distribution ⇡k [Achiam et al., 2017a] as
218"
PRACTICAL IMPLEMENTATION,0.5334608030592735,"Eˆs⇠d⇡k , a⇠⇡[A⇡k(ˆs, a)] = Eˆs⇠d⇡k , a⇠⇡k"
PRACTICAL IMPLEMENTATION,0.5353728489483748,⇡(a|ˆs)
PRACTICAL IMPLEMENTATION,0.5372848948374761,"⇡k(a|ˆs)A⇡k(ˆs, a) 0"
PRACTICAL IMPLEMENTATION,0.5391969407265774,".
(14)"
PRACTICAL IMPLEMENTATION,0.5411089866156787,"(14) allows us to replace A⇡k with empirical estimates at each state-action pair (ˆs, a) from rollouts
219"
PRACTICAL IMPLEMENTATION,0.5430210325047801,"by the previous policy ⇡k. The empirical estimate of reward advantage is given by R(ˆs, a, ˆs0) +
220"
PRACTICAL IMPLEMENTATION,0.5449330783938815,"γV ⇡k(ˆs0) −V ⇡k(ˆs). V ⇡k(ˆs) can be computed at each augmented state by taking the discounted
221"
PRACTICAL IMPLEMENTATION,0.5468451242829828,"future return. The same can be applied to the expected advantage with respect to cost increments, with
222"
PRACTICAL IMPLEMENTATION,0.5487571701720841,"the sample estimates given by Di(ˆs, a, ˆs0) + V ⇡k"
PRACTICAL IMPLEMENTATION,0.5506692160611855,Di (ˆs0) −V ⇡k
PRACTICAL IMPLEMENTATION,0.5525812619502868,Di (ˆs). V ⇡k
PRACTICAL IMPLEMENTATION,0.5544933078393881,"Di (ˆs) is computed by taking the
223"
PRACTICAL IMPLEMENTATION,0.5564053537284895,"non-discounted future Di-return. To proceed, we convexify (11) by approximating the objective and
224"
PRACTICAL IMPLEMENTATION,0.5583173996175909,"cost constraint via ﬁrst-order expansions, and the trust region constraint via second-order expansions.
225"
PRACTICAL IMPLEMENTATION,0.5602294455066922,"Then, (11) can be efﬁciently solved using duality [Achiam et al., 2017a].
226"
PRACTICAL IMPLEMENTATION,0.5621414913957935,"Infeasible constraints
An update to ✓is computed every time (11) is solved. However, due to
227"
PRACTICAL IMPLEMENTATION,0.5640535372848948,"approximation errors, sometimes (11) can become infeasible. In that case, we follow [Achiam
228"
PRACTICAL IMPLEMENTATION,0.5659655831739961,"et al., 2017a] to propose an recovery update that only decreases the constraint value within the trust
229"
PRACTICAL IMPLEMENTATION,0.5678776290630975,"region. In addition, approximation errors can also cause the proposed policy update (either feasible
230"
PRACTICAL IMPLEMENTATION,0.5697896749521989,"or recovery) to violate the original constraints in (11). Hence, each policy update is followed by
231"
PRACTICAL IMPLEMENTATION,0.5717017208413002,"a backtracking line search to ensure constraint satisfaction. If all these fails, we relax the search
232"
PRACTICAL IMPLEMENTATION,0.5736137667304015,"condition by also accepting decreasing expected advantage with respect to the costs, when the cost
233"
PRACTICAL IMPLEMENTATION,0.5755258126195029,constraints are already violated. Denoting ci .= JDi(⇡k)+2(H +1)✏⇡ D p
PRACTICAL IMPLEMENTATION,0.5774378585086042,"δ/2−wi, the above criteria
234"
PRACTICAL IMPLEMENTATION,0.5793499043977055,"can be summarized as
235"
PRACTICAL IMPLEMENTATION,0.5812619502868069,"Eˆs⇠¯d⇡k [DKL(⇡k⇡k)[ˆs]] δ
(15)"
PRACTICAL IMPLEMENTATION,0.5831739961759083,"Eˆs⇠¯d⇡k ,a⇠⇡ ⇥ A⇡k"
PRACTICAL IMPLEMENTATION,0.5850860420650096,"Di(ˆs, a) ⇤"
PRACTICAL IMPLEMENTATION,0.5869980879541109,"−Eˆs⇠¯d⇡k ,a⇠⇡k ⇥ A⇡k"
PRACTICAL IMPLEMENTATION,0.5889101338432122,"Di(ˆs, a) ⇤"
PRACTICAL IMPLEMENTATION,0.5908221797323135,"max(−ci, 0).
(16)"
PRACTICAL IMPLEMENTATION,0.5927342256214149,"Note that the previous expected advantage Eˆs⇠¯d⇡k ,a⇠⇡k ⇥ A⇡k"
PRACTICAL IMPLEMENTATION,0.5946462715105163,"Di(ˆs, a) ⇤"
PRACTICAL IMPLEMENTATION,0.5965583173996176,"is also estimated from rollouts
236"
PRACTICAL IMPLEMENTATION,0.5984703632887189,"by ⇡k and converges to zero asymptotically, which recovers the original cost constraints in (11).
237"
PRACTICAL IMPLEMENTATION,0.6003824091778203,"Imbalanced cost value targets
A critical step in solving (11) is to ﬁt the cost increment value
238"
PRACTICAL IMPLEMENTATION,0.6022944550669216,functions V ⇡k
PRACTICAL IMPLEMENTATION,0.6042065009560229,"Di (ˆst). By deﬁnition, V ⇡k"
PRACTICAL IMPLEMENTATION,0.6061185468451242,"Di (ˆst) is equal to the maximum cost increment in any future
239"
PRACTICAL IMPLEMENTATION,0.6080305927342257,"state over the maximal state-wise cost so far. In other words, the true V ⇡k"
PRACTICAL IMPLEMENTATION,0.609942638623327,"Di will always be zero for all
240"
PRACTICAL IMPLEMENTATION,0.6118546845124283,"ˆst:H when the maximal state-wise cost has already occurred before time t. In practice, this causes
241"
PRACTICAL IMPLEMENTATION,0.6137667304015296,"the distribution of cost increment value function to be highly zero-skewed and makes the ﬁtting very
242"
PRACTICAL IMPLEMENTATION,0.615678776290631,"hard. To mitigate the problem, we sub-sample the zero-valued targets to match the population of
243"
PRACTICAL IMPLEMENTATION,0.6175908221797323,"non-zero values. We provide more analysis on this trick in Q3 in section 6.2.
244"
EXPERIMENTS,0.6195028680688337,"6
Experiments
245"
EXPERIMENTS,0.621414913957935,"(a) Ant-Hazard-8
(b) Walker-Hazard-8"
EXPERIMENTS,0.6233269598470363,"Figure 1: Comparison of results from two repre-
sentative test suites in high dimensional systems
(Ant and Walker)."
EXPERIMENTS,0.6252390057361377,"In our experiments, we aim to answer these questions:
246"
EXPERIMENTS,0.627151051625239,"Q1 How does SCPO compare with other state-of-the-
247"
EXPERIMENTS,0.6290630975143403,"art methods for safe RL?
248"
EXPERIMENTS,0.6309751434034416,"Q2 What beneﬁts are demonstrated by constraining
249"
EXPERIMENTS,0.6328871892925431,"the maximum state-wise cost?
250"
EXPERIMENTS,0.6347992351816444,"Q3 How do the sub-sampling trick of SCPO impact
251"
EXPERIMENTS,0.6367112810707457,"its performance?
252"
EXPERIMENT SETUPS,0.638623326959847,"6.1
Experiment Setups
253"
EXPERIMENT SETUPS,0.6405353728489483,"New Safety Gym
To showcase the effectiveness
254"
EXPERIMENT SETUPS,0.6424474187380497,"of our state-wise constrained policy optimization ap-
255"
EXPERIMENT SETUPS,0.6443594646271511,"proach, we enhance the widely recognized safe rein-
256"
EXPERIMENT SETUPS,0.6462715105162524,"forcement learning benchmark environment, Safety
257"
EXPERIMENT SETUPS,0.6481835564053537,"Gym Ray et al. [2019], by incorporating additional
258"
EXPERIMENT SETUPS,0.6500956022944551,"robots and constraints. Subsequently, we perform a
259"
EXPERIMENT SETUPS,0.6520076481835564,"series of experiments on this augmented environment.
260"
EXPERIMENT SETUPS,0.6539196940726577,"Our experiments are based on ﬁve different robots: (i)
261"
EXPERIMENT SETUPS,0.655831739961759,"Point: Figure 2a A point-mass robot (A ✓R2) that
262"
EXPERIMENT SETUPS,0.6577437858508605,"can move on the ground. (ii) Swimmer: Figure 2b
263"
EXPERIMENT SETUPS,0.6596558317399618,"A three-link robot (A ✓R2) that can move on the
264"
EXPERIMENT SETUPS,0.6615678776290631,"ground. (iii) Walker:
Figure 2d A bipedal robot
265"
EXPERIMENT SETUPS,0.6634799235181644,"(A ✓R10) that can move on the ground. (iv) Ant: Figure 2c A quadrupedal robot (A ✓R8) that
266"
EXPERIMENT SETUPS,0.6653919694072657,"can move on the ground. (v) Drone: Figure 2e A quadrotor robot (A ✓R4) that can move in the air.
267"
EXPERIMENT SETUPS,0.6673040152963671,"All of the experiments are based on the goal task where the robot must navigate to a goal. Additionally,
268"
EXPERIMENT SETUPS,0.6692160611854685,"since we are interested in episodic tasks (ﬁnite-horizon MDP), the environment will be reset once the
269"
EXPERIMENT SETUPS,0.6711281070745698,"goal is reached. For the robots that can move in 3D spaces (e.g, the Drone robot), we also design a
270"
EXPERIMENT SETUPS,0.6730401529636711,"new 3D goal task with a sphere goal ﬂoating in the 3D space. Three different types of constraints are
271"
EXPERIMENT SETUPS,0.6749521988527725,"considered: (i) Hazard: Dangerous areas as shown in Figure 3a. Hazards are trespassable circles on
272"
EXPERIMENT SETUPS,0.6768642447418738,"the ground. The agent is penalized for entering them. (ii) 3D Hazard: 3D Dangerous areas as shown
273"
EXPERIMENT SETUPS,0.6787762906309751,"in Figure 3b. 3D Hazards are trespassable spheres in the air. The agent is penalized for entering them.
274"
EXPERIMENT SETUPS,0.6806883365200764,(iii) Pillar: Fixed obstacles as shown in Figure 3c. The agent is penalized for hitting them.
EXPERIMENT SETUPS,0.6826003824091779,"(a) Point
(b) Swimmer
(c) Ant
(d) Walker
(e) Drone"
EXPERIMENT SETUPS,0.6845124282982792,Figure 2: Robots for benchmark problems in upgraded Safety Gym.
EXPERIMENT SETUPS,0.6864244741873805,"(a) Hazard
(b) 3D Hazard
(c) Pillar"
EXPERIMENT SETUPS,0.6883365200764818,Figure 3: Constraints for benchmark problems in upgraded Safety Gym. 275
EXPERIMENT SETUPS,0.6902485659655831,"(a) Point-Hazard-8
(b) Point-Pillar-4
(c) Swimmer-Hazard-8
(d) Drone-3DHazard-8"
EXPERIMENT SETUPS,0.6921606118546845,"Figure 4: Comparison of results from four representative test suites in low dimensional systems (Point, Swimmer,
and Drone)."
EXPERIMENT SETUPS,0.6940726577437859,"Considering different robots, constraint types, and constraint difﬁculty levels, we design 14 test suites
276"
EXPERIMENT SETUPS,0.6959847036328872,"with 5 types of robots and 9 types of constraints, which are summarized in Table 1 in Appendix. We
277"
EXPERIMENT SETUPS,0.6978967495219885,"name these test suites as {Robot}-{Constraint Type}-{Constraint Number}.
278"
EXPERIMENT SETUPS,0.6998087954110899,"Comparison Group
The methods in the comparison group include: (i) unconstrained RL algorithm
279"
EXPERIMENT SETUPS,0.7017208413001912,"TRPO [Schulman et al., 2015] (ii) end-to-end constrained safe RL algorithms CPO [Achiam et al.,
280"
EXPERIMENT SETUPS,0.7036328871892925,"2017a], TRPO-Lagrangian [Bohez et al., 2019], TRPO-FAC [Ma et al., 2021], TRPO-IPO [Liu et al.,
281"
EXPERIMENT SETUPS,0.7055449330783938,"2020], PCPO [Yang et al., 2020b], and (iii) hierarchical safe RL algorithms TRPO-SL (TRPO-Safety
282"
EXPERIMENT SETUPS,0.7074569789674953,"Layer) [Dalal et al., 2018], TRPO-USL (TRPO-Unrolling Safety Layer) [Zhang et al., 2022b]. We
283"
EXPERIMENT SETUPS,0.7093690248565966,"select TRPO as our baseline method since it is state-of-the-art and already has safety-constrained
284"
EXPERIMENT SETUPS,0.7112810707456979,"derivatives that can be tested off-the-shelf. For hierarchical safe RL algorithms, we employ a warm-up
285"
EXPERIMENT SETUPS,0.7131931166347992,"phase (1/3 of the whole epochs) which does unconstrained TRPO training, and the generated data
286"
EXPERIMENT SETUPS,0.7151051625239006,"will be used to pre-train the safety critic for future epochs. For all experiments, the policy ⇡, the value
287"
EXPERIMENT SETUPS,0.7170172084130019,"(V ⇡, V ⇡"
EXPERIMENT SETUPS,0.7189292543021033,"D) are all encoded in feedforward neural networks using two hidden layers of size (64,64)
288"
EXPERIMENT SETUPS,0.7208413001912046,"with tanh activations. More details are provided in Appendix D.
289"
EXPERIMENT SETUPS,0.722753346080306,"Evaluation Metrics
For comparison, we evaluate algorithm performance based on (i) reward
290"
EXPERIMENT SETUPS,0.7246653919694073,"performance, (ii) average episode cost and (iii) cost rate. Comparison metric details are provided
291"
EXPERIMENT SETUPS,0.7265774378585086,"in Appendix D.3. We set the limit of cost to 0 for all the safe RL algorithms since we aim to avoid
292"
EXPERIMENT SETUPS,0.7284894837476099,"any violation of the constraints. For our comparison, we implement the baseline safe RL algorithms
293"
EXPERIMENT SETUPS,0.7304015296367112,"exactly following the policy update / action correction procedure from the original papers. We
294"
EXPERIMENT SETUPS,0.7323135755258127,"emphasize that in order for the comparison to be fair, we give baseline safe RL algorithms every
295"
EXPERIMENT SETUPS,0.734225621414914,"advantage that is given to SCPO, including equivalent trust region policy updates.
296"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7361376673040153,"6.2
Evaluating SCPO and Comparison Analysis
297"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7380497131931166,"Low Dimension System
We select four representative test suites on low dimensional system
298"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.739961759082218,"(Point, Swimmer, Drone) and summarize the comparison results on Figure 4, which demonstrate
299"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7418738049713193,"that SCPO is successful at approximately enforcing zero constraints violation safety performance
300"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7437858508604207,"in all environments after the policy converges. Speciﬁcally, compared with the baseline safe RL
301"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.745697896749522,"methods, SCPO is able to achieve (i) near zero average episode cost and (ii) signiﬁcantly lower
302"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7476099426386233,"cost rate without sacriﬁcing reward performance. The baseline end-to-end safe RL methods (TRPO-
303"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7495219885277247,"Lagrangian, TRPO-FAC, TRPO-IPO, CPO, PCPO) fail to achieve the near zero cost performance
304"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.751434034416826,"even when the cost limit is set to be 0. The baseline hierarchical safe RL methods (TRPO-SL,
305"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7533460803059273,"TRPO-USL) also fail to achieve near zero cost performance even with an explicit safety layer to
306"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7552581261950286,"correct the unsafe action at every time step. End-to-end safe RL algorithms fail since all methods
307"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7571701720841301,"rely on CMDP to minimize the discounted cumulative cost while SCPO directly work with MMDP
308"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7590822179732314,"to restrict the state-wise maximum cost by Proposition 1. We also observe that TRPO-SL fails to
309"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7609942638623327,"lower the violation during training, due to the fact that the linear approximation of cost function
310"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.762906309751434,"C(ˆst, a, ˆst+1) [Dalal et al., 2018] becomes inaccurate when the dynamics are highly nonlinear like
311"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7648183556405354,"the ones we used in MuJoCo [Todorov et al., 2012]. More detailed metrics for comparison and
312"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7667304015296367,"experimental results on test suites with low dimension systems are summarized in Appendix D.3.
313"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.768642447418738,"Figure 5:
Maximum state-wise cost"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7705544933078394,"High Dimension System
To demonstrate the scalability and per-
314"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7724665391969407,"formance of SCPO in high-dimensional systems, we conducted ad-
315"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7743785850860421,"ditional tests on the Ant-Hazard-8 and Walker-Hazard-8 suites, with
316"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7762906309751434,"8-dimensional and 10-dimensional control spaces, respectively. The
317"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7782026768642447,"comparison results for high-dimensional systems are summarized in
318"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.780114722753346,"Figure 1, which show that SCPO outperforms all other baselines in
319"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7820267686424475,"enforcing zero safety violation without compromising performance
320"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7839388145315488,"in terms of return. SCPO rapidly stabilizes the cost return around
321"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7858508604206501,"zero and signiﬁcantly reduces the cost rate, while the other baselines
322"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7877629063097514,"fail to converge to a policy with near-zero cost. The comparison
323"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7896749521988528,"results of both low dimension and high dimension systems answer
324"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7915869980879541,"Q1.
325"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7934990439770554,"Maximum State-wise Cost
As pointed in Section 3.3, the under-
326"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7954110898661568,"lying magic for enabling near-zero safety violation is to restrict the maximum state-wise cost to stay
327"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7973231357552581,"around zero. To have a better understanding of this process, we visualize the evolution of maximum
328"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.7992351816443595,"state-wise cost for SCPO on the challenging high-dimensional Ant-Hazard-8 and Walker-Hazard-8
329"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8011472275334608,"test suites in Figure 5 , which answers Q2.
330"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8030592734225621,"Figure 6: SCPO sub-sampling ablation study with
Drone-3DHazard-8"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8049713193116634,"Ablation on Sub-sampling Imbalanced Cost Incre-
331"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8068833652007649,"ment Value Targets
As pointed in Section 5, ﬁt-
332"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8087954110898662,ting V ⇡k
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8107074569789675,"Di (ˆst) is a critical step towards solving SCPO,
333"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8126195028680688,"which is challenging due to zero-skewed distribution
334"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8145315487571702,"of cost increment value function. To demonstrate
335"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8164435946462715,"the necessity of sub-sampling for solving this chal-
336"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8183556405353728,"lenge, we compare the performance of SCPO with
337"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8202676864244742,"and without sub-sampling trick on the aerial robot
338"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8221797323135756,"test suite, summarized in Figure 6. It is evident that
339"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8240917782026769,"with sub-sampling, the agent achieves higher rewards
340"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8260038240917782,"and more importantly, converges to near-zero costs.
341"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8279158699808795,"That is because sub-sampling effectively balances the cost increment value targets and improves
342"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8298279158699808,the ﬁtting of V ⇡k
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8317399617590823,"Di (ˆst). We also attempted to solve the imbalance issue via over-sampling non-zero
343"
EVALUATING SCPO AND COMPARISON ANALYSIS,0.8336520076481836,"targets, but did not observe promising results. This ablation study provides insights into Q3.
344"
CONCLUSION AND FUTURE WORK,0.8355640535372849,"7
Conclusion and Future Work
345"
CONCLUSION AND FUTURE WORK,0.8374760994263862,"This paper proposed SCPO, the ﬁrst general-purpose policy search algorithm for state-wise con-
346"
CONCLUSION AND FUTURE WORK,0.8393881453154876,"strained RL. Our approach provides guarantees for state-wise constraint satisfaction at each iteration,
347"
CONCLUSION AND FUTURE WORK,0.8413001912045889,"allows training of high-dimensional neural network policies while ensuring policy behavior, and is
348"
CONCLUSION AND FUTURE WORK,0.8432122370936902,"based on a new theoretical result on Maximum Markov Decision Process. We demonstrate SCPO’s
349"
CONCLUSION AND FUTURE WORK,0.8451242829827916,"effectiveness on robot locomotion tasks, showing its signiﬁcant performance improvement compared
350"
CONCLUSION AND FUTURE WORK,0.847036328871893,"to existing methods and ability to handle state-wise constraints.
351"
CONCLUSION AND FUTURE WORK,0.8489483747609943,"Limitation and future work
One limitation of our work is that, although SCPO satisﬁes state-wise
352"
CONCLUSION AND FUTURE WORK,0.8508604206500956,"constraints, the theoretical results are valid only in expectation, meaning that constraint violations
353"
CONCLUSION AND FUTURE WORK,0.8527724665391969,"are still possible during deployment. To address that, we will study absolute state-wise constraint
354"
CONCLUSION AND FUTURE WORK,0.8546845124282982,"satisfaction, i.e. bounding the maximal possible state-wise cost, which is even stronger than the
355"
CONCLUSION AND FUTURE WORK,0.8565965583173997,"current result (satisfaction in expectation).
356"
REFERENCES,0.858508604206501,"References
357"
REFERENCES,0.8604206500956023,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
358"
REFERENCES,0.8623326959847036,"Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
359"
REFERENCES,0.864244741873805,"through deep reinforcement learning. nature, 518(7540):529–533, 2015.
360"
REFERENCES,0.8661567877629063,"Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
361"
REFERENCES,0.8680688336520076,"Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
362"
REFERENCES,0.869980879541109,"starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.
363"
REFERENCES,0.8718929254302104,"Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top
364"
REFERENCES,0.8738049713193117,"professionals. Science, 359(6374):418–424, 2018.
365"
REFERENCES,0.875717017208413,"Tairan He, Yuge Zhang, Kan Ren, Minghuan Liu, Che Wang, Weinan Zhang, Yuqing Yang, and
366"
REFERENCES,0.8776290630975143,"Dongsheng Li. Reinforcement learning with automated auxiliary loss search. arXiv preprint
367"
REFERENCES,0.8795411089866156,"arXiv:2210.06041, 2022.
368"
REFERENCES,0.8814531548757171,"Wei-Ye Zhao, Xi-Ya Guan, Yang Liu, Xiaoming Zhao, and Jian Peng. Stochastic variance reduction
369"
REFERENCES,0.8833652007648184,"for deep q-learning. arXiv preprint arXiv:1905.08152, 2019.
370"
REFERENCES,0.8852772466539197,"Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and
371"
REFERENCES,0.887189292543021,"Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. arXiv
372"
REFERENCES,0.8891013384321224,"preprint arXiv:2205.10330, 2022.
373"
REFERENCES,0.8910133843212237,"Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement
374"
REFERENCES,0.892925430210325,"learning. CoRR, abs/1910.01708, 2019.
375"
REFERENCES,0.8948374760994264,"Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
376"
REFERENCES,0.8967495219885278,"International conference on machine learning, pages 22–31. PMLR, 2017a.
377"
REFERENCES,0.8986615678776291,"Yongshuai Liu, Avishai Halev, and Xin Liu. Policy learning with constraints in model-free reinforce-
378"
REFERENCES,0.9005736137667304,"ment learning: A survey. In The 30th International Joint Conference on Artiﬁcial Intelligence
379"
REFERENCES,0.9024856596558317,"(IJCAI), 2021.
380"
REFERENCES,0.904397705544933,"Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and Changliu Liu. State-wise safe reinforcement
381"
REFERENCES,0.9063097514340345,"learning: A survey. The 32nd International Joint Conference on Artiﬁcial Intelligence (IJCAI),
382"
REFERENCES,0.9082217973231358,"2023.
383"
REFERENCES,0.9101338432122371,"Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
384"
REFERENCES,0.9120458891013384,"International Conference on Machine Learning, pages 22–31. PMLR, 2017b.
385"
REFERENCES,0.9139579349904398,"Yongshuai Liu, Jiaxin Ding, and Xin Liu. IPO: interior-point policy optimization under constraints.
386"
REFERENCES,0.9158699808795411,"CoRR, abs/1910.09615, 2019. URL http://arxiv.org/abs/1910.09615.
387"
REFERENCES,0.9177820267686424,"Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Projection-based
388"
REFERENCES,0.9196940726577438,"constrained policy optimization. CoRR, abs/2010.03152, 2020a. URL https://arxiv.org/
389"
REFERENCES,0.9216061185468452,"abs/2010.03152.
390"
REFERENCES,0.9235181644359465,"Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and
391"
REFERENCES,0.9254302103250478,"Animesh Garg. Conservative safety critics for exploration. arXiv preprint arXiv:2010.14497, 2020.
392"
REFERENCES,0.9273422562141491,"Linrui Zhang, Qin Zhang, Li Shen, Bo Yuan, Xueqian Wang, and Dacheng Tao. Evaluating model-free
393"
REFERENCES,0.9292543021032504,"reinforcement learning toward safety-critical tasks. arXiv preprint arXiv:2212.05727, 2022a.
394"
REFERENCES,0.9311663479923518,"Yinlam Chow, Oﬁr Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
395"
REFERENCES,0.9330783938814532,"Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. ICML 2019
396"
REFERENCES,0.9349904397705545,"Workshop RL4RealLife, abs/1901.10031, 2019.
397"
REFERENCES,0.9369024856596558,"Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
398"
REFERENCES,0.9388145315487572,"Tassa. Safe exploration in continuous action spaces. CoRR, abs/1801.08757, 2018.
399"
REFERENCES,0.9407265774378585,"Qingkai Liang, Fanyu Que, and Eytan Modiano. Accelerated primal-dual policy optimization for
400"
REFERENCES,0.9426386233269598,"safe reinforcement learning. arXiv preprint arXiv:1802.06480, 2018.
401"
REFERENCES,0.9445506692160612,"Chen Tessler, Daniel J Mankowitz, and Shie Mannor. arXiv preprint arXiv:1805.11074, 2018.
402"
REFERENCES,0.9464627151051626,"Steven Bohez, Abbas Abdolmaleki, Michael Neunert, Jonas Buchli, Nicolas Heess, and Raia Hadsell.
403"
REFERENCES,0.9483747609942639,"Value constrained model-free continuous control. arXiv preprint arXiv:1902.04623, 2019.
404"
REFERENCES,0.9502868068833652,"Tairan He, Weiye Zhao, and Changliu Liu. Autocost: Evolving intrinsic cost for zero-violation
405"
REFERENCES,0.9521988527724665,"reinforcement learning. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2023.
406"
REFERENCES,0.9541108986615678,"Haitong Ma, Yang Guan, Shegnbo Eben Li, Xiangteng Zhang, Sifa Zheng, and Jianyu Chen. Feasible
407"
REFERENCES,0.9560229445506692,"actor-critic: Constrained reinforcement learning for ensuring statewise safety. arXiv preprint
408"
REFERENCES,0.9579349904397706,"arXiv:2105.10682, 2021.
409"
REFERENCES,0.9598470363288719,"Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
410"
REFERENCES,0.9617590822179732,"networks, 21(4):682–697, 2008.
411"
REFERENCES,0.9636711281070746,"John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
412"
REFERENCES,0.9655831739961759,"policy optimization. In International conference on machine learning, pages 1889–1897. PMLR,
413"
REFERENCES,0.9674952198852772,"2015.
414"
REFERENCES,0.9694072657743786,"Stephen Boyd, Lin Xiao, and Almir Mutapcic. Subgradient methods. lecture notes of EE392o,
415"
REFERENCES,0.97131931166348,"Stanford University, Autumn Quarter, 2004:2004–2005, 2003.
416"
REFERENCES,0.9732313575525813,"Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
417"
REFERENCES,0.9751434034416826,"Proceedings of the Nineteenth International Conference on Machine Learning, pages 267–274,
418"
REFERENCES,0.9770554493307839,"2002.
419"
REFERENCES,0.9789674952198852,"Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point policy optimization under constraints.
420"
REFERENCES,0.9808795411089866,"In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 34, pages 4940–4947,
421"
REFERENCES,0.982791586998088,"2020.
422"
REFERENCES,0.9847036328871893,"Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based
423"
REFERENCES,0.9866156787762906,"constrained policy optimization. arXiv preprint arXiv:2010.03152, 2020b.
424"
REFERENCES,0.988527724665392,"Linrui Zhang, Qin Zhang, Li Shen, Bo Yuan, and Xueqian Wang. Saferl-kit: Evaluating efﬁcient
425"
REFERENCES,0.9904397705544933,"reinforcement learning methods for safe autonomous driving. arXiv preprint arXiv:2206.08528,
426"
REFERENCES,0.9923518164435946,"2022b.
427"
REFERENCES,0.994263862332696,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
428"
REFERENCES,0.9961759082217974,"In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033.
429"
REFERENCES,0.9980879541108987,"IEEE, 2012.
430"
