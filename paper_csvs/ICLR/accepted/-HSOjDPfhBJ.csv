Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0009174311926605505,"Emphatic temporal difference (ETD) learning (Sutton et al., 2016) is a successful
method to conduct the off-policy value function evaluation with function approxi-
mation. Although ETD has been shown to converge asymptotically to a desirable
value function, it is well-known that ETD often encounters a large variance so
that its sample complexity can increase exponentially fast with the number of it-
erations. In this work, we propose a new ETD method, called PER-ETD (i.e.,
PEriodically Restarted-ETD), which restarts and updates the follow-on trace only
for a ﬁnite period for each iteration of the evaluation parameter. Further, PER-
ETD features a design of the logarithmical increase of the restart period with the
number of iterations, which guarantees the best trade-off between the variance
and bias and keeps both vanishing sublinearly. We show that PER-ETD converges
to the same desirable ﬁxed point as ETD, but improves the exponential sample
complexity of ETD to be polynomials. Our experiments validate the superior per-
formance of PER-ETD and its advantage over ETD."
INTRODUCTION,0.001834862385321101,"1
INTRODUCTION"
INTRODUCTION,0.0027522935779816515,"As a major value function evaluation method, temporal difference (TD) learning (Sutton, 1988;
Dayan, 1992) has been widely used in various planning problems in reinforcement learning. Al-
though TD learning performs successfully in the on-policy settings, where an agent can interact with
environments under the target policy, it can perform poorly or even diverge under the off-policy set-
tings when the agent only has access to data sampled by a behavior policy (Baird, 1995; Tsitsiklis &
Van Roy, 1997; Mahmood et al., 2015). To address such an issue, the gradient temporal-difference
(GTD) (Sutton et al., 2008) and least-squares temporal difference (LSTD) (Yu, 2010) algorithms
have been proposed, which have been shown to converge in the off-policy settings. However, since
GTD and LSTD consider an objective function based on the behavior policy, which adjusts only
the distribution mismatch of the action and does not adjust the distribution mismatch of the state,
their converging points can be largely biased from the true value function due to the distribution
mismatch between the target and behavior policies, even when the express power of the function
approximation class is arbitrarily large (Kolter, 2011)."
INTRODUCTION,0.003669724770642202,"In order to provide a more accurate evaluation, Sutton et al. (2016) proposed the emphatic temporal
difference (ETD) algorithm, which introduces the follow-on trace to address the distribution mis-
match issue and thus adjusts both state and action distribution mismatch. The stability of ETD was
then shown in Sutton et al. (2016); Mahmood et al. (2015), and the asymptotic convergence guaran-
tee for ETD was established in Yu (2015), it has also achieved great success in many tasks (Ghiassian
et al., 2016; Ni, 2021). However, although ETD can address the distribution mismatch issue to yield
a more accurate evaluation, it often suffers from very large variance error due to the follow-on trace
estimation over a long or inﬁnite time horizon (Hallak et al., 2016). Consequently, the convergence
of ETD can be unstable. It can be shown that the variance of ETD can grow exponentially fast as the
number of iterations grow so that ETD requires exponentially large number of samples to converge.
Hallak et al. (2016) proposed an ETD method to keep the follow-on trace bounded but at the cost of
a possibly large bias error. This thus poses the following intriguing question:"
INTRODUCTION,0.0045871559633027525,Published as a conference paper at ICLR 2022
INTRODUCTION,0.005504587155963303,"Can we design a new ETD method, which overcomes its large variance without introducing a large
bias error, and improves its exponential sample complexity to be polynomial at the same time?"
INTRODUCTION,0.006422018348623854,"In this work, we provide an afﬁrmative answer."
MAIN CONTRIBUTIONS,0.007339449541284404,"1.1
MAIN CONTRIBUTIONS"
MAIN CONTRIBUTIONS,0.008256880733944955,"We propose a novel ETD approach, called PER-ETD (i.e., PEriodically Restarted-ETD), in which
for each update of the value function parameter we restart the follow-on trace iteration and update it
only for b times (where we call b as the period length). Such a periodic restart effectively reduces
the variance of the follow-on trace. More importantly, with the design of the period length b to
increase logarithmically with the number of iterations, PER-ETD attains the polynomial rather than
exponential sample complexity required by ETD."
MAIN CONTRIBUTIONS,0.009174311926605505,"We provide the theoretical guarantee of the sample efﬁciency of PER-ETD via the ﬁnite-time anal-
ysis. We show that PER-ETD (both PER-ETD(0) and PER-ETD(λ)) converges to the same ﬁxed
points of ETD(0) and ETD(λ), respectively, but with only polynomial sample complexity (whereas
ETD takes exponential sample complexity). Our analysis features the following key insights. (a)
The period length b plays the role of trading off between the variance (of the follow-on trace) and
bias error (with respect to the ﬁxed point of ETD), and its optimal choice of logarithmical increase
with the number of iterations achieves the best tradeoff and keeps both errors vanishing sublin-
early. (b) Our analysis captures how the mismatch between the behavior and target policies affects
the convergence rate of PER-ETD. Interestingly, the mismatch level determines a phase-transition
phenomenon of PER-ETD: as long as the mismatch is below a certain threshold, then PER-ETD
achieves the same convergence rate as the on-policy TD algorithm; and if the mismatch is above the
threshold, the converge rate of PER-ETD gradually decays as the level of mismatch increases."
MAIN CONTRIBUTIONS,0.010091743119266056,"Experimentally, we demonstrate that PER-ETD converges in the case that neither TD nor ETD
converges. Further, our experiments provide the following two interesting observations. (a) There
does exist a choice of the period length for PER-ETD, which attains the best tradeoff between the
variance and bias errors. Below such a choice, the bias error is large so that evaluation is not
accurate, and above it the variance error is large so that the convergence is unstable. (b) Under a
small period length b, it is not always the case that PER-ETD(λ) with λ = 1 attains the smallest
error with respect to the ground truth value function. The best λ depends on the geometry of the
locations of ﬁxed points of PER-ETD(λ) for 0 ≤λ ≤1, which is determined by chosen features."
RELATED WORKS,0.011009174311926606,"1.2
RELATED WORKS"
RELATED WORKS,0.011926605504587157,"TD learning and GTD: The asymptotic convergence of TD learning was established by Sutton
(1988); Jaakkola et al. (1994); Dayan & Sejnowski (1994); Tsitsiklis & Van Roy (1997), and its non-
asymptotic convergence rate was further characterized recently in Dalal et al. (2018a); Bhandari et al.
(2018); Kotsalis et al. (2020); Chen et al. (2019); Kaledin et al. (2020); Hu & Syed (2019); Srikant
& Ying (2019). The gradient temporal-difference (GTD) was proposed in Sutton et al. (2008) for
off-policy evaluation and was shown to converge asymptotically. Then, Dalal et al. (2018b); Gupta
et al. (2019); Wang et al. (2018); Xu et al. (2019); Xu & Liang (2021) provided the ﬁnite-time
analysis of GTD and its variants."
RELATED WORKS,0.012844036697247707,"Emphatic Temporal Difference (ETD) Learning: The ETD approach was originally proposed
in the seminal work Sutton et al. (2016), which introduced the follow-on trace to overcome the
distribution mismatch between the behavior and target policies. Yu (2015) provided the asymptotic
convergence guarantee for ETD. Hallak et al. (2016) showed that the variance of the follow-on trace
may be unbounded. They further proposed an ETD method with a variable decay rate to keep the
follow-on trace bounded but at the cost of a possibly large bias error. Our approach is different and
keeps both the variance and bias vanishing sublinearly with the number of iterations. Imani et al.
(2018) developed a new policy gradient theorem, where the emphatic weight is used to correct the
distribution shift. Zhang et al. (2020b) provided a new variant of ETD, where the emphatic weights
are estimated through function approximation. Van Hasselt et al. (2018); Jiang et al. (2021) studied
ETD with deep neural function class."
RELATED WORKS,0.013761467889908258,"Comparison to concurrent work: During our preparation of this paper, a concurrent work (Zhang
& Whiteson, 2021) was posted on arXiv, and proposed a truncated ETD (which we refer to as"
RELATED WORKS,0.014678899082568808,Published as a conference paper at ICLR 2022
RELATED WORKS,0.015596330275229359,"T-ETD for short here), which truncates the update of the follow-on trace to reduce the variance
of ETD. While T-ETD and our PER-ETD share a similar design idea, there are several critical
differences between our work from Zhang & Whiteson (2021). (a) Our PER-ETD features a design
of the logarithmical increase of the restart period with the number of iterations, which guarantees
the convergence to the original ﬁxed point of ETD, with both the variance and bias errors vanishing
sublinearly. However, T-ETD is guaranteed to converge only to a truncation-length-dependent ﬁxed
point, where the convergence is obtained by treating the truncation length as a constant. A careful
review of the convergence proof indicates that the variance term scales exponentially fast with the
truncation length, and hence the polynomial efﬁciency is not guaranteed as the truncation length
becomes large. (b) Our convergence rate for PER-ETD does not depend on the cardinality of the state
space and has only polynomial dependence on the mismatch parameter of the behavior and target
policies. However, the convergence rate in Zhang & Whiteson (2021) scales with the cardinality
of the state space, and increases exponentially fast with the mismatch parameter of behavior and
target policies. (c) This paper further studies PER-ETD(λ) and the impact of λ on the converge rate
and variance and bias errors, whereas Zhang & Whiteson (2021) considers further the application of
T-ETD to the control problem."
BACKGROUND AND PRELIMINARIES,0.01651376146788991,"2
BACKGROUND AND PRELIMINARIES"
MARKOV DECISION PROCESS,0.01743119266055046,"2.1
MARKOV DECISION PROCESS"
MARKOV DECISION PROCESS,0.01834862385321101,"We consider the inﬁnite-horizon Markov decision process (MDP) deﬁned by the ﬁve tuple
(S, A, r, P, γ). Here, S and A denote the state and action spaces respectively, which are both as-
sumed to be ﬁnite sets, r : S ×A →R denotes the reward function, P : S ×A →∆(S) denotes the
transition kernel, where ∆(S) denotes the probability simplex over the state space S, and γ ∈(0, 1)
is the discount factor."
MARKOV DECISION PROCESS,0.01926605504587156,"A policy π : S →∆(A) of an agent maps from the state space to the probability simplex over the
action space A, i.e., π(a|s) represents the probability of taking the action a under the state s. At
any time t, given that the system is at the state st, the agent takes an action at with the probability
π(at|st), and receives a reward r(st, at). The system then takes a transition to the next state st+1 at
time t + 1 with the probability P(st+1|st, at)."
MARKOV DECISION PROCESS,0.02018348623853211,"For a given policy π, we deﬁne the value function corresponding to an initial state s0 = s ∈S
as Vπ(s) = E [P∞
t=0 γtr(st, at)|s0 = s, π]. Then the value function over the state space can be
expressed as a vector Vπ = (Vπ(1), Vπ(2), . . . , Vπ(|S|))⊤∈R|S|. Here, Vπ is a deterministic
function of the policy π. We use capitalized characters to be consistent with the literature."
MARKOV DECISION PROCESS,0.02110091743119266,"When the state space is large, we approximate the value function Vπ via a linear function class as
Vθ(s) = φ⊤(s)θ, where φ(s) ∈Rd denotes the feature vector, and θ ∈Rd denotes the parameter
vector to be learned. We further let Φ = [φ(1), φ(2), . . . , φ(|S|)]⊤denote the feature matrix, and
then Vθ = Φθ. We assume that the feature matrix Φ has linearly independent columns and each
feature vector has bounded ℓ2-norm, i.e., ∥φ(s)∥2 ≤Bφ for all s ∈S."
MARKOV DECISION PROCESS,0.022018348623853212,"2.2
TEMPORAL DIFFERENCE (TD) LEARNING FOR ON-POLICY EVALUATION"
MARKOV DECISION PROCESS,0.022935779816513763,"In order to evaluate the value function for a given target policy π (i.e., ﬁnd the linear function
approximation parameter θ), the temporal difference (TD) learning can be employed based on a
sampling trajectory, which takes the following update rule at each time t:"
MARKOV DECISION PROCESS,0.023853211009174313,"θt+1 = θt + ηt
 
r(st, at) + γθ⊤
t φ(st+1) −θ⊤
t φ(st)

φ(st),
(1)"
MARKOV DECISION PROCESS,0.024770642201834864,"where ηt is the stepsize at time t. The main idea here is to follow the Bellman operation update to
approach its ﬁxed point, and the above sampled version update can be viewed as the so-called semi-
gradient descent update. If the trajectory is sampled by the target policy π, then the above TD algo-
rithm can be shown to converge to the ﬁxed point solution, where the convergence is guaranteed by
the negative deﬁniteness of the so-called key matrix A := limt→∞E

(γφ(st + 1) −φ(st))φ⊤(st)

."
MARKOV DECISION PROCESS,0.025688073394495414,Published as a conference paper at ICLR 2022
MARKOV DECISION PROCESS,0.026605504587155965,"2.3
EMPHATIC TD (ETD) LEARNING FOR OFF-POLICY EVALUATION"
MARKOV DECISION PROCESS,0.027522935779816515,"Consider the off-policy setting, where the goal is still to evaluate the value function for a given target
policy π, but the agent has access only to trajectories sampled under a behavior policy µ. Namely, at
each time t, the probability of taking an action at given st is µ(at|st). Let dµ denote the stationary
distribution of the Markov chain induced by the behavior policy µ, i.e., dµ satisﬁes d⊤
µ = d⊤
µ Pπ.
We assume that dµ(s) > 0 for all states. The mismatch between the target and behavior policies can
be addressed by incorporating the importance sampling factor ρ(s, a) := π(a|s)"
MARKOV DECISION PROCESS,0.028440366972477066,"µ(a|s) into eq. (1) to adjust
the TD learning update direction. However, with such modiﬁcation, the key matrix A may not be
negative deﬁnite so that the algorithm is no longer guaranteed to converge."
MARKOV DECISION PROCESS,0.029357798165137616,"In order to address this divergence issue, the emphatic temporal difference (ETD) algorithm has
been proposed by Sutton et al. (2016), which takes the following update"
MARKOV DECISION PROCESS,0.030275229357798167,"θt+1 = θt + ηtρ(st, at)Ft
 
r(st, at) + γθ⊤
t φ(st+1) −θ⊤
t φ(st)

φ(st).
(2)"
MARKOV DECISION PROCESS,0.031192660550458717,"In eq. (2), in addition to the importance sampling factor ρ, a follow-on trace coefﬁcient Ft is intro-
duced as a calibration factor, which is updated as"
MARKOV DECISION PROCESS,0.03211009174311927,"Ft = γρ(st−1, at−1)Ft−1 + 1,
(3)"
MARKOV DECISION PROCESS,0.03302752293577982,"with initialization F0 = 1. With such a follow-on trace factor, the key matrix becomes negative
deﬁnite, and ETD has been shown to converge asymptotically in Yu (2015) to the ﬁxed point"
MARKOV DECISION PROCESS,0.03394495412844037,"θ∗=
 
Φ⊤F(I −γPπ)Φ
−1 Φ⊤Frπ,
(4)"
MARKOV DECISION PROCESS,0.03486238532110092,"where F = diag(f(1), f(2), . . . , f(|S|)) and f(i) = dµ(i) limt→∞E [Ft|st = i]."
MARKOV DECISION PROCESS,0.03577981651376147,"Similarly, the ETD(λ) algorithm can be further derived, which has the following update"
MARKOV DECISION PROCESS,0.03669724770642202,"θt+1 = θt + ηtρ(st, at)
 
r(st, at) + γθ⊤
t φ(st+1) −θ⊤
t φ(st)

et,"
MARKOV DECISION PROCESS,0.03761467889908257,"where et is updated as et = γλρ(st−1, at−1)et−1+Mtφ(st) and Mt = λ+(1−λ)Ft, where M0 = 1
and e0 = φ(s0). It has been shown that with a diminishing stepsize (Yu, 2015), ETD(λ) converges
to the ﬁxed point given by θ∗
λ =
 
Φ⊤M(I −γλPπ)−1(I −γPπ)Φθ
−1 Φ⊤M(I −γλPπ)−1rπ,
where M = diag(m(1), m(2), . . . , m(|S|)) and m(i) = dµ(i) limt→∞E [Mt|st = i]."
NOTATIONS,0.03853211009174312,"2.4
NOTATIONS"
NOTATIONS,0.03944954128440367,"For the simplicity of expression, we adopt the following shorthand notations. For a ﬁxed integer b, let
sτ
t := st(b+1)+τ, aτ
t := at(b+1)+τ, ρτ
t = π(aτ
t |sτ
t )
µ(aτ
t |sτ
t ) and φτ
t = φ(sτ
t ). We also deﬁne the ﬁltration Ft ="
NOTATIONS,0.04036697247706422,"σ
 
s0, a0, s1, a1, . . . , st(b+1)+b, at(b+1)+b, st(b+1)+b+1

. Further, let rπ ∈R|S|, where rπ(s) =
P"
NOTATIONS,0.04128440366972477,"a∈A r(s, a)π(a|s). Let Pπ ∈R|S|×|S|, where Pπ(s′|s) = P"
NOTATIONS,0.04220183486238532,"a∈A π(a|s)P(s′|s, a). For a matrix
M ∈RN×N, M(s,·) denotes its s-th row and M(·,s) denotes its s-th column. We deﬁne Bφ :=
maxs ∥φ(s)∥2 as the upper bound on the feature vectors, and deﬁne ρmax := maxs,a
π(a|s)
µ(a|s) as the
maximum of the distribution mismatch over all state-action pairs."
PROPOSED PER-ETD ALGORITHMS,0.043119266055045874,"3
PROPOSED PER-ETD ALGORITHMS"
PROPOSED PER-ETD ALGORITHMS,0.044036697247706424,"Drawbacks of ETD: In the original design of ETD (Sutton et al., 2016) described in Section 2.3,
the follow-on trace coefﬁcient Ft is updated throughout the execution of the algorithm. As a result,
its variance can increase exponentially with the number of iterations, which causes the algorithm to
be unstable and diverge, as observed in Hallak et al. (2016) (also see our experiment in Section 5)."
PROPOSED PER-ETD ALGORITHMS,0.044954128440366975,"In order to overcome the divergence issue of ETD, we propose to PEriodically Restart the follow-on
trace update for ETD, which we call as the PER-ETD algorithm (see Algorithm 1). At iteration t,
PER-ETD reinitiates the follow-on trace F and update it for b iterations to obtain an estimate F b
t ,
where we call b as the period length. The emphatic update operator at t is then given by"
PROPOSED PER-ETD ALGORITHMS,0.045871559633027525,"bTt(θ) = F b
t ρb
tφb
t(φb
t −γφb+1
t
)⊤θ −F b
t ρb
tφb
trb
t,
(5)"
PROPOSED PER-ETD ALGORITHMS,0.046788990825688076,Published as a conference paper at ICLR 2022
PROPOSED PER-ETD ALGORITHMS,0.047706422018348627,Algorithm 1 PER-ETD(0)
PROPOSED PER-ETD ALGORITHMS,0.04862385321100918,"1: Input: Parameters T, b, and ηt.
2: Initialize: θ0 = 0.
3: for t = 0, 1, ..., T do
4:
F update: F τ+1
t
= γρτ
t F τ
t + 1, where τ = 0, 1, . . . , b −1 and F 0
t = 1;
5:
θ update: θt+1 = ΠΘ
 
θt + ηtF b
t ρb
t(rb
t + γθ⊤
t φb+1
t
−θ⊤
t φb
t)φb
t
"
PROPOSED PER-ETD ALGORITHMS,0.04954128440366973,6: end for
PROPOSED PER-ETD ALGORITHMS,0.05045871559633028,"and PER-ETD updates the value function parameter θt as θt+1 = ΠΘ

θt −ηt bTt(θt)

, where the
projection onto an bounded closed convex set Θ helps to stabilize the algorithm. It can be shown
that limb→∞E[bTt(θ)|Ft−1] = T (θ) where T (θ) :=
 
Φ⊤F(I −γPπ)Φ

θ −Φ⊤Frπ. The ﬁxed
point of the operator T (θ) is θ∗deﬁned in eq. (4), which is exactly the ﬁxed point of original ETD.
Deﬁnition 1 (Optimal point and ϵ-accurate convergence). We call the unique ﬁxed point θ∗of T (θ)
as the optimal point (which is the same as the ﬁxed point of ETD). The algorithm attains an ϵ-
accurate optimal point if its output θT satisﬁes ∥θT −θ∗∥2
2 ≤ϵ."
PROPOSED PER-ETD ALGORITHMS,0.05137614678899083,"The goal of PER-ETD is to ﬁnd the original optimal point θ∗of ETD, which is independent from
the period length b. Our analysis will provide a guidance to choose the period length b in order for
PER-ETD to keep both the variance and bias errors below the target ϵ-accuracy with polynomial
sample efﬁciency."
PROPOSED PER-ETD ALGORITHMS,0.05229357798165138,Algorithm 2 PER-ETD(λ)
PROPOSED PER-ETD ALGORITHMS,0.05321100917431193,"1: Input: Parameters T, b, and ηt.
2: Initialize: θ0 = 0.
3: for t = 0, 1, . . . , T do
4:
Set F 0
t = M 0
t = 1 and e0
t = φ0
t
5:
for τ = 1, . . . , b do
6:
F τ
t = ρτ−1
t
γF τ−1
t
+ 1,
M τ
t = λ + (1 −λ)F τ
t ,
eτ
t = γλρτ−1
t
eτ−1
t
+ M τ
t φτ
t
7:
end for
8:
θ update: θt+1 = ΠΘ
 
θt + ηtρb
t
 
rb
t + γθ⊤
t φb+1
t
−θ⊤
t φb
t

eb
t
"
PROPOSED PER-ETD ALGORITHMS,0.05412844036697248,9: end for
PROPOSED PER-ETD ALGORITHMS,0.05504587155963303,"We then extend PER-ETD(0) to PER-ETD(λ) (see Algorithm 2), which incorporates the eligible
trace. Speciﬁcally, at each iteration t, PER-ETD(λ) reinitiates the follow-on trace Ft and updates it
together with Mt and the eligible trace et for b iterations to obtain an estimate eb
t. Then the emphatic
update operator at t is given by"
PROPOSED PER-ETD ALGORITHMS,0.05596330275229358,"bT λ
t (θ) = ρb
teb
t
 
φb
t −γφb+1
t
⊤θ −ρb
trb
teb
t,
(6)"
PROPOSED PER-ETD ALGORITHMS,0.05688073394495413,"and the value function parameter θt is updated as θt+1 = ΠΘ

θt −ηt bT λ
t (θt)

. It can be shown that"
PROPOSED PER-ETD ALGORITHMS,0.05779816513761468,"limb→∞E
h
bT λ
t (θ)
Ft−1
i
= T λ(θ), where T λ(θ) = Φ⊤M(I−γλPπ)−1(I−γPπ)Φθ−Φ⊤M(I−"
PROPOSED PER-ETD ALGORITHMS,0.05871559633027523,"γλPπ)−1rπ, which takes a unique ﬁxed point θ∗
λ as the original ETD(λ). The optimal point and the
ϵ-accurate convergence can be deﬁned in the same fashion as in Deﬁnition 1. It has been shown in
Hallak et al. (2016) that θ∗
λ is exactly the orthogonal projection of Vπ to the function space when
λ = 1, and thus is the optimal approximation to the value function."
FINITE-TIME ANALYSIS OF PER-ETD ALGORITHMS,0.05963302752293578,"4
FINITE-TIME ANALYSIS OF PER-ETD ALGORITHMS"
TECHNICAL ASSUMPTIONS,0.060550458715596334,"4.1
TECHNICAL ASSUMPTIONS"
TECHNICAL ASSUMPTIONS,0.061467889908256884,"We take the following standard assumptions for analyzing the TD-type algorithms in the literature
(Jiang et al., 2021; Zhang & Whiteson, 2021; Yu, 2015).
Assumption 1 (Coverage of behavior policy). For all s ∈S and a ∈A, the behavior policy µ
satisﬁes µ(a|s) > 0 as long as π(a|s) > 0."
TECHNICAL ASSUMPTIONS,0.062385321100917435,Published as a conference paper at ICLR 2022
TECHNICAL ASSUMPTIONS,0.06330275229357799,Assumption 2. The Markov chain induced by the behavior policy µ is irreducible and recurrent.
TECHNICAL ASSUMPTIONS,0.06422018348623854,The following lemma on the geometric ergodicity has been established.
TECHNICAL ASSUMPTIONS,0.06513761467889909,"Lemma 1 (Geometric ergodicity).
(Levin & Peres, 2017, Thm. 4.9) Suppose Assumption 2 holds.
Then the Markov chain induced by the behavior policy µ has a unique stationary distribution dµ
over the state space S. Moreover, the Markov chain is uniformly geometric ergodic, i.e., there exist
constants CM ≥0 and 0 < χ < 1 such that for every initial state s0 ∈S, the state distribution
dµ,t(s) = P (st = s|s0) after t transitions satisﬁes ∥dµ,t −dµ∥1 ≤CMχt."
TECHNICAL ASSUMPTIONS,0.06605504587155964,"4.2
FINITE-TIME ANALYSIS OF PER-ETD(0)"
TECHNICAL ASSUMPTIONS,0.06697247706422019,"In PER-ETD(0), the update of the value function parameter is fully determined by the empirical
emphatic operator bTt(θ) deﬁned in eq. (5). Thus, we ﬁrst characterize the bias and variance errors
of bTt(θ), which serve the central role in establishing the convergence rate for PER-ETD(0)."
TECHNICAL ASSUMPTIONS,0.06788990825688074,Proposition 1 (Bias bound). Suppose Assumptions 1 and 2 hold. Then we have
TECHNICAL ASSUMPTIONS,0.06880733944954129,"E
hT (θt) −E
h
bTt(θt)
Ft−1
i
2"
TECHNICAL ASSUMPTIONS,0.06972477064220184,"i
≤Cb (Bφ∥θt −θ∗∥2 + ϵapprox) ξb,"
TECHNICAL ASSUMPTIONS,0.07064220183486239,"where ϵapprox = ∥Φθ∗−Vπ∥∞is the approximation error of the ﬁxed point, ξ = max {γ, χ} < 1,
Bφ = maxs ∥φ(s)∥2, and Cb > 0 is a constant whose exact form can be found in the proof."
TECHNICAL ASSUMPTIONS,0.07155963302752294,"Proposition 1 characterizes the conditional expectation of the bias error of the empirical emphatic
operator bTt(θ). Since ξ = max {γ, χ} < 1, such a bias error decays exponentially fast as b increases."
TECHNICAL ASSUMPTIONS,0.07247706422018349,Proposition 2 (Variance bound). Suppose Assumptions 1 and 2 hold. Then we have
TECHNICAL ASSUMPTIONS,0.07339449541284404,"E
bTt(θt)

2 2 Ft−1"
TECHNICAL ASSUMPTIONS,0.07431192660550459,"
≤σ2,
where
σ2 = 
 "
TECHNICAL ASSUMPTIONS,0.07522935779816514,"O(1),
if
γ2ρmax < 1,
O(b),
if
γ2ρmax = 1,
O
 
(γ2ρmax)b
,
if
γ2ρmax > 1,
(7)"
TECHNICAL ASSUMPTIONS,0.07614678899082569,"where O(·) is with respect to the scaling of b, and ρmax = maxs,a
π(a|s)
µ(a|s)."
TECHNICAL ASSUMPTIONS,0.07706422018348624,"Proposition 2 captures the variance bound of the empirical emphatic operator. It can be seen that if
the distribution mismatch is large (i.e., γ2ρmax > 1), the variance bound grows exponentially large
as b increases, which is consistent with the ﬁnding in Hallak et al. (2016). However, as we show
below, as long as b is controlled to grow only logarithmically with the number of iterations, such
a variance error will decay sublinearly with the number of iterations. At the same time, the bias
error can also be controlled to decay sublinearly, so that the overall convergence of PER-ETD can
be guaranteed with polynomial sample complexity efﬁciency."
TECHNICAL ASSUMPTIONS,0.0779816513761468,"Theorem 1. Suppose Assumptions 1 and 2 hold. Consider PER-ETD(0) speciﬁed in Algorithm 1.
Let the stepsize ηt = O
  1"
TECHNICAL ASSUMPTIONS,0.07889908256880734,"t

and suppose the period length b and the projection set Θ are properly
chosen (see Appendix D.3 for the precise conditions). Then the output θT of PER-ETD(0) falls into
the following two cases."
TECHNICAL ASSUMPTIONS,0.0798165137614679,"(a) If γ2ρmax ≤1, then E

∥θT −θ∗∥2
2

≤˜O
  1 T

."
TECHNICAL ASSUMPTIONS,0.08073394495412844,"(b) If γ2ρmax > 1, then E

∥θT −θ∗∥2
2

≤O
  1"
TECHNICAL ASSUMPTIONS,0.081651376146789,"T a

, where a = 1/(log1/ξ(γ2ρmax) + 1) < 1."
TECHNICAL ASSUMPTIONS,0.08256880733944955,"Thus, PER-ETD(0) attains an ϵ-accurate solution with ˜O
  1"
TECHNICAL ASSUMPTIONS,0.0834862385321101,"ϵ

samples if γ2ρmax ≤1, and with
˜O
 
1
ϵ1/a

samples if γ2ρmax > 1."
TECHNICAL ASSUMPTIONS,0.08440366972477065,"Theorem 1 captures how the convergence rate depends on the mismatch between the behavior and
target policies via the parameter ρmax (where ρmax ≥1). (a) If γ2ρmax ≤1, i.e., the mismatch is
less than a threshold, then PER-ETD(0) converges at the rate of ˜O
  1"
TECHNICAL ASSUMPTIONS,0.0853211009174312,"T

, which is the same as that of
on-policy TD learning (Bhandari et al., 2018). This result indicates that even under a mild mismatch
1 < ρmax ≤1/γ2, PER-ETD achieves the same convergence rate as on-policy TD learning. (b)
If γ2ρmax ≥1, i.e., the mismatch is above the threshold, then PER-ETD(0) converges at a slower"
TECHNICAL ASSUMPTIONS,0.08623853211009175,Published as a conference paper at ICLR 2022
TECHNICAL ASSUMPTIONS,0.0871559633027523,"rate of ˜O
  1"
TECHNICAL ASSUMPTIONS,0.08807339449541285,"T a

because a < 1. Further, as the mismatch parameter ρmax gets larger, the converge
becomes slower, because a becomes smaller."
TECHNICAL ASSUMPTIONS,0.0889908256880734,"Bias and variance tradeoff: Theorem 1 also indicates that although PER-ETD(0) updates the
follow-on trace only over a ﬁnite period length b, it still converges to the optimal ﬁxed point θ∗.
This beneﬁts from the proper choice of the period length, which achieves the best bias and variance
tradeoff as we explain as follows. The proof of Theorem 1 shows that the output θT of PER-ETD(0)
satisﬁes the following convergence rate:"
TECHNICAL ASSUMPTIONS,0.08990825688073395,"E

∥θT −θ∗∥2
2

≤O

∥θ0−θ∗∥2
2
T 2

+ O

σ2 T
"
TECHNICAL ASSUMPTIONS,0.0908256880733945,"| {z }
variance"
TECHNICAL ASSUMPTIONS,0.09174311926605505,"+ O

ξ2b"
TECHNICAL ASSUMPTIONS,0.0926605504587156,"T

+ O
 
ξb"
TECHNICAL ASSUMPTIONS,0.09357798165137615,"|
{z
}
bias .
(8)"
TECHNICAL ASSUMPTIONS,0.0944954128440367,"If γ2ρmax ≤1, then σ2 in the variance term in eq. (8) satisﬁes σ2 ≤O(b) as given in eq. (7), which"
TECHNICAL ASSUMPTIONS,0.09541284403669725,"increases at most linearly fast with b. Then we set b = O

log T
log(1/ξ)

so that both the variance and"
TECHNICAL ASSUMPTIONS,0.0963302752293578,"the bias terms in eq. (8) achieve the same order of O
  1"
TECHNICAL ASSUMPTIONS,0.09724770642201835,"T

, which dominates the overall convergence."
TECHNICAL ASSUMPTIONS,0.0981651376146789,"If γ2ρmax > 1, then σ2 in the variance term in eq. (8) satisﬁes σ2 = O
 
(γ2ρmax)b
as given in"
TECHNICAL ASSUMPTIONS,0.09908256880733946,"eq. (7), Now, we need to set b as b = O

log(T )
log(γ2ρmax)+log(1/ξ)

, where the increase with log T has
a smaller coefﬁcient than the previous case, so that both the variance and the bias terms in eq. (8)
achieve the same order of O
  1"
TECHNICAL ASSUMPTIONS,0.1,"T a

. Such a choice of b balances the exponentially increasing variance
and exponentially decaying bias to achieve the same rate."
TECHNICAL ASSUMPTIONS,0.10091743119266056,"4.3
FINITE-TIME ANALYSIS OF PER-ETD(λ)"
TECHNICAL ASSUMPTIONS,0.1018348623853211,"In PER-ETD(λ), the update of the value function parameter is determined by the empirical emphatic
operator bT λ
t (θ) deﬁned in eq. (6). Thus, we ﬁrst obtain the bias and variance errors of bT λ
t (θ), which
facilitate the analysis of the convergence rate for PER-ETD(λ)."
TECHNICAL ASSUMPTIONS,0.10275229357798166,"Proposition 3. Suppose Assumptions 1 and 2 hold. Then we have
E
h
bT λ
t (θt)
Ft−1
i
−T λ(θt)

2 ≤Cb,λ (Bφ∥θt −θ∗
λ∥2 + ϵapprox) ξb,"
TECHNICAL ASSUMPTIONS,0.10366972477064221,"where ϵapprox = ∥Φθ∗
λ −Vπ∥∞is the approximation error of the ﬁxed point, ξ = max{χ, γ} < 1,
Bφ = maxs ∥φ(s)∥2, and Cb,λ is a constant given a ﬁxed λ whose exact form can be found in proof."
TECHNICAL ASSUMPTIONS,0.10458715596330276,"The above proposition shows that the bias error of the empirical emphatic operator bT λ
t (θ) in PER-
ETD(λ) decays exponentially fast as b increases, because ξ = max {γ, χ} < 1."
TECHNICAL ASSUMPTIONS,0.10550458715596331,"Proposition 4. Suppose Assumptions 1 and 2 hold. Then we have E
h bT λ
t (θt)

2 2"
TECHNICAL ASSUMPTIONS,0.10642201834862386,"Ft−1
i
≤σ2
λ,"
TECHNICAL ASSUMPTIONS,0.10733944954128441,"where σ2
λ = O
 
ρb
max

."
TECHNICAL ASSUMPTIONS,0.10825688073394496,"Compared with Proposition 2 of PER-ETD(0), Proposition 4 indicates that ETD(λ) has a larger
variance, which always increases exponentially with b when ρmax > 1. This is due to the fact that
the eligible trace eb
t carries the historical information and is less stable than φb
t."
TECHNICAL ASSUMPTIONS,0.10917431192660551,"Theorem 2. Suppose Assumptions 1 and 2 hold. Consider PER-ETD(λ) speciﬁed in Algorithm 2.
Let the stepsize ηt = O
  1"
TECHNICAL ASSUMPTIONS,0.11009174311926606,"t

and suppose the period length b and the projection set Θ are properly
chosen (see Appendix E.3 for the precise conditions). Then the output θT of PER-ETD(λ) satis-
ﬁes E

∥θT −θ∗
λ∥2
2

≤O
 
1
T aλ

, where aλ =
1
log1/ξ(ρmax)+1. PER-ETD(λ) attains an ϵ-accurate"
TECHNICAL ASSUMPTIONS,0.11100917431192661,"solution with ˜O

1
ϵ1/aλ

samples."
TECHNICAL ASSUMPTIONS,0.11192660550458716,"Theorem 2 indicates that PER-ETD(λ) converges to the optimal ﬁxed point θ∗
λ determined by the
inﬁnite-length update of the follow-on trace. Furthermore, PER-ETD(λ) converges at the rate of
˜O
 
1
T aλ

which is slower than PER-ETD(0) (as aλ < a) due to the larger variance of PER-ETD(λ)."
TECHNICAL ASSUMPTIONS,0.11284403669724771,"Bias and variance tradeoff: We next explain how the period length b achieves the best tradeoff
between the bias and variance errors and thus yields polynomial sample efﬁciency. The proof of"
TECHNICAL ASSUMPTIONS,0.11376146788990826,Published as a conference paper at ICLR 2022
TECHNICAL ASSUMPTIONS,0.11467889908256881,Theorem 2 shows that the output θT of PER-ETD(λ) satisﬁes the following convergence rate:
TECHNICAL ASSUMPTIONS,0.11559633027522936,"E

∥θT −θ∗
λ∥2
2

≤O

∥θ0−θ∗
λ∥2
2
T 2

+ O

σ2
λ
T
"
TECHNICAL ASSUMPTIONS,0.11651376146788991,"| {z }
variance"
TECHNICAL ASSUMPTIONS,0.11743119266055047,"+ O

ξ2b"
TECHNICAL ASSUMPTIONS,0.11834862385321102,"T

+ O
 
ξb"
TECHNICAL ASSUMPTIONS,0.11926605504587157,"|
{z
}
bias .
(9)"
TECHNICAL ASSUMPTIONS,0.12018348623853212,"In eq. (9), σ2
λ in the variance term takes the form σ2
λ = O
 
ρb
max

as given in Proposition 4. We need"
TECHNICAL ASSUMPTIONS,0.12110091743119267,"to set b = O

log(T )
log(ρmax)+log(1/ξ)

so that both the variance and the bias terms in eq. (9) achieve the"
TECHNICAL ASSUMPTIONS,0.12201834862385322,"same order of O
 
1
T aλ

. Thus, such a choice of b balances the exponentially increasing variance and
exponentially decaying bias to achieve the same rate."
TECHNICAL ASSUMPTIONS,0.12293577981651377,"Impact of the eligible trace (via the parameter λ) on error bound: It has been shown that with
the aid of eligible trace, both TD and ETD achieve smaller error bounds (Sutton & Barto, 2018;
Hallak et al., 2016). However, this is not always the case for PER-ETD. Since PER-ETD applies a
ﬁnite period length b, the ﬁxed point of PER-ETD(1) is generally not the same as the projection of
the ground truth to the function approximation space. Thus, as λ changes from 0 to 1, depending
on the geometrical locations of the ﬁxed points of PER-ETD(λ) for all λ (determined by chosen
features) with respect to the ground truth projection, any value 0 ≤λ ≤1 may achieve the smallest
bias error. We illustrate this further by experiments in Section 5.2."
EXPERIMENTS,0.12385321100917432,"5
EXPERIMENTS"
EXPERIMENTS,0.12477064220183487,"5.1
PERFORMANCE OF PER-ETD(0)"
EXPERIMENTS,0.12568807339449542,"We consider the BAIRD counter-example. The details of the MDP setting and behavior and tar-
get policies could be found in Appendix A.1. We adopt a constant learning rate for both PER-
ETD(0) and PER-ETD(λ) and all experiments take an average over 20 random initialization. We
set the stepsize η = 2−9 for all algorithms for fair comparison. For PER-ETD(0), we adopt one-
dimensional features Φ1 = (0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.37)⊤. The ground truth value func-
tion Vπ = (10, 10, 10, 10, 10, 10, 10)⊤and does not lie inside the linear function class."
EXPERIMENTS,0.12660550458715597,"(a) Comparison of TD, ETD, PER-ETD(0)
(b) Tradeoff between bias and variance by b"
EXPERIMENTS,0.12752293577981652,Figure 1: Performance of PER-ETD(0) and comparison
EXPERIMENTS,0.12844036697247707,"In Figure 1(a), we compare the performance of of TD, vanilla ETD(0) and PER-ETD(0) with b =
2, 4, 8 in terms of the distance between the ground truth and the learned value functions. It can be
observed that our proposed PER-ETD(0) converges close to the ground truth at a properly chosen
period length such as b = 4 and b = 8, whereas TD diverges due to no treatment on off-policy data
historically, and ETD (0) also diverges due to the very large variance."
EXPERIMENTS,0.12935779816513762,"In Figure 1(b), we plot how the bias and the variance of PER-ETD(0) change as the period length b
changes. Clearly, small b (e.g., b = 4) yields a small variance but a large bias. Then as b increases
from 4 to 6, bias is substantially reduced. As b continues to increase from 8 to 20, there is a
signiﬁcant increase in variance. This demonstrates a clear tradeoff between the bias and variance as
we capture in our theory."
EXPERIMENTS,0.13027522935779817,"5.2
PERFORMANCE OF PER-ETD(λ)"
EXPERIMENTS,0.13119266055045872,"We next focus on PER-ETD(λ) under the same experiment setting as in Section 5.1 and study how λ
affects the performance. We conduct our experiments under three features Φ1, Φ2, and Φ3 speciﬁed"
EXPERIMENTS,0.13211009174311927,Published as a conference paper at ICLR 2022
EXPERIMENTS,0.13302752293577982,"(a) Feature Φ1
(b) Feature Φ2
(c) Feature Φ3"
EXPERIMENTS,0.13394495412844037,Figure 2: Performance of PER-ETD(λ) and dependence on features
EXPERIMENTS,0.13486238532110092,"(a) Feature Φ1
(b) Feature Φ2
(c) Feature Φ3"
EXPERIMENTS,0.13577981651376148,"Figure 3: Fixed points of PER-ETD(λ) and project of the value function. (a): θ lies in 1-dimensional
Euclidean space R1 along horizontal direction; (b), (c): θ lies in 2-dimensional Euclidean space R2."
EXPERIMENTS,0.13669724770642203,"in Appendix A.2. Figure 2 shows how the bias error with respect to the ground truth changes as
λ increases under the three chosen features. As shown in Figure 2 (a), (b), and (c), λ = 0, 1, and
some value between 0 and 1 respectively achieve the smallest error under the corresponding feature.
This is in contrast to the general understanding that λ = 1 typically achieves the smallest error.
In fact, each case can be explained by the plot in Figure 3 under the same feature. Each plot in
Figure 3 illustrates how the ﬁxed points of PER-ETD(λ) are located with respect to the ground truth
projection (as Vπ projection) for b = 4. Since the period length b is ﬁnite, the ﬁxed point of PER-
ETD(1) is not located at the same point as the ground truth projection. The geometric locations of
the ﬁxed points of PER-ETD(λ) for 0 ≤λ ≤1 are determined by chosen features. The bias error
corresponds to the distance between the ﬁxed point of PER-ETD(λ) and the Vπ projection. Then
under each feature, the value of λ that attains the smallest error with respect to the Vπ projection can
be readily seen from the plot in Figure 3. For example, under the feature Φ3, Figure 3 (c) suggests
that neither λ = 0 nor λ = 1, but some λ between 0 and 1 achieves the smallest error. This explains
the result in Figure 2 (c) that λ = 0.4 achieves the smallest error among other curves."
EXPERIMENTS,0.13761467889908258,"As a summary, our experiment suggests that the best λ, under which PER-ETD(λ) attains the small-
est error, depends on the geometry of the problem determined by chosen features. In practice, if
PER-ETD(λ) is used as a critic in policy optimization problems, λ may be tuned via the ﬁnal reward
achieved by the algorithm."
CONCLUSION,0.13853211009174313,"6
CONCLUSION"
CONCLUSION,0.13944954128440368,"In this paper, we proposed a novel PER-ETD algorithm, which uses a periodic restart technique to
control the variance of follow-on trace update. Our analysis shows that by selecting the period length
properly, both bias and variance of PER-ETD vanishes sublinearly with the number of iterations,
leading to the polynomial sample efﬁciency to the desired unique ﬁxed point of ETD, whereas
ETD requires exponential sample complexity. Our experiments veriﬁed the advantage of PER-ETD
against both TD and ETD. Moreover, our experiments of PER-ETD(λ) illustrated that under the
ﬁnite period length in practice, the best λ that achieves the smallest bias error is feature dependent.
We anticipate that PER-ETD can be applied to various off-policy optimal control algorithms such as
actor-critic algorithms and multi-agent reinforcement learning algorithms."
CONCLUSION,0.14036697247706423,Published as a conference paper at ICLR 2022
REFERENCES,0.14128440366972478,REFERENCES
REFERENCES,0.14220183486238533,"Leemon Baird.
Residual algorithms: Reinforcement learning with function approximation.
In
Machine Learning, pp. 30–37. Elsevier, 1995."
REFERENCES,0.14311926605504588,"Jalaj Bhandari, Daniel Russo, and Raghav Singal. A ﬁnite time analysis of temporal difference
learning with linear function approximation. In Proc. Annual Conference on Learning Theory
(COLT), pp. 1691–1692. PMLR, 2018."
REFERENCES,0.14403669724770643,"Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Perfor-
mance of q-learning with linear function approximation: Stability and ﬁnite-time analysis. arXiv
preprint arXiv:1905.11425, 2019."
REFERENCES,0.14495412844036698,"Gal Dalal, Bal´azs Sz¨or´enyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with
function approximation. In Proc. AAAI Conference on Artiﬁcial Intelligence (AAAI), 2018a."
REFERENCES,0.14587155963302753,"Gal Dalal, Gugan Thoppe, Bal´azs Sz¨or´enyi, and Shie Mannor.
Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning. In Proc. Annual
Conference on Learning Theory (COLT), pp. 1199–1233. PMLR, 2018b."
REFERENCES,0.14678899082568808,"Peter Dayan. The convergence of TD (λ) for general λ. Machine learning, 8(3-4):341–362, 1992."
REFERENCES,0.14770642201834863,"Peter Dayan and Terrence J Sejnowski. TD (λ) converges with probability 1. Machine Learning, 14
(3):295–301, 1994."
REFERENCES,0.14862385321100918,"Sina Ghiassian, Banafsheh Raﬁee, and Richard S Sutton. A ﬁrst empirical study of emphatic tempo-
ral difference learning. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
Continual Learning and Deep Networks workshop, 2016."
REFERENCES,0.14954128440366973,"Harsh Gupta, R Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate
selection for two time-scale reinforcement learning. Proc. Advances in Neural Information Pro-
cessing Systems (NeurIPS), 32:4704–4713, 2019."
REFERENCES,0.15045871559633028,"Assaf Hallak, Aviv Tamar, R´emi Munos, and Shie Mannor. Generalized emphatic temporal differ-
ence learning: Bias-variance analysis. In Proc. AAAI Conference on Artiﬁcial Intelligence (AAAI),
2016."
REFERENCES,0.15137614678899083,"Bin Hu and Usman Ahmed Syed. Characterizing the exact behaviors of temporal difference learn-
ing algorithms using markov jump linear system theory. Proc. Advances in Neural Information
Processing Systems (NeurIPS), 2019."
REFERENCES,0.15229357798165138,"Ehsan Imani, Eric Graves, and Martha White. An off-policy policy gradient theorem using emphatic
weightings. Proc. Advances in Neural Information Processing Systems (NeurIPS), 2018."
REFERENCES,0.15321100917431194,"Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. On the convergence of stochastic iterative
dynamic programming algorithms. Neural computation, 6(6):1185–1201, 1994."
REFERENCES,0.15412844036697249,"Ray Jiang, Shangtong Zhang, Veronica Chelu, Adam White, and Hado van Hasselt. Learning ex-
pected emphatic traces for deep RL. arXiv preprint arXiv:2107.05405, 2021."
REFERENCES,0.15504587155963304,"Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time
analysis of linear two-timescale stochastic approximation with markovian noise. In Proc. Annual
Conference on Learning Theory (COLT), pp. 2144–2203. PMLR, 2020."
REFERENCES,0.1559633027522936,"J Kolter. The ﬁxed points of off-policy TD. Proc. Advances in Neural Information Processing
Systems (NeurIPS), 24:2169–2177, 2011."
REFERENCES,0.15688073394495414,"Georgios Kotsalis, Guanghui Lan, and Tianjiao Li. Simple and optimal methods for stochastic vari-
ational inequalities, ii: Markovian noise and policy evaluation in reinforcement learning. arXiv
preprint arXiv:2011.08434, 2020."
REFERENCES,0.1577981651376147,"Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer
Nature, 2020."
REFERENCES,0.15871559633027524,Published as a conference paper at ICLR 2022
REFERENCES,0.1596330275229358,"David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathe-
matical Soc., 2017."
REFERENCES,0.16055045871559634,"A Rupam Mahmood, Huizhen Yu, Martha White, and Richard S Sutton.
Emphatic temporal-
difference learning. European Workshop on Reinforcement Learning, 2015."
REFERENCES,0.1614678899082569,"Jingjiao Ni. Toward emphatic reinforcement learning. Master’s thesis, University of Alberta, 2021."
REFERENCES,0.16238532110091744,"Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Proc. Annual Conference on Learning Theory (COLT), pp. 2803–2830. PMLR,
2019."
REFERENCES,0.163302752293578,"Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9–44, 1988."
REFERENCES,0.16422018348623854,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.1651376146788991,"Richard S Sutton, Csaba Szepesv´ari, and Hamid Reza Maei. A convergent o (n) algorithm for off-
policy temporal-difference learning with linear function approximation. Proc. Advances in Neural
Information Processing Systems (NeurIPS), 21(21):1609–1616, 2008."
REFERENCES,0.16605504587155964,"Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of
off-policy temporal-difference learning. Journal of Machine Learning Research (JMLR), 17(1):
2603–2631, 2016."
REFERENCES,0.1669724770642202,"John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674–690, 1997."
REFERENCES,0.16788990825688074,"Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Mo-
dayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018."
REFERENCES,0.1688073394495413,"Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, and Tie-Yan Liu. Finite sample analysis of the
GTD policy evaluation algorithms in markov setting. arXiv preprint arXiv:1809.08926, 2018."
REFERENCES,0.16972477064220184,"Tengyu Xu and Yingbin Liang. Sample complexity bounds for two timescale value-based reinforce-
ment learning algorithms. In International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), pp. 811–819. PMLR, 2021."
REFERENCES,0.1706422018348624,"Tengyu Xu, Shaofeng Zou, and Yingbin Liang.
Two time-scale off-policy TD learning: Non-
asymptotic analysis over markovian samples. Proc. Advances in Neural Information Processing
Systems (NeurIPS), 2019."
REFERENCES,0.17155963302752295,"Huizhen Yu. Convergence of least squares temporal difference methods under general conditions.
In Proc. International Conference on Machine Learning (ICML), pp. 1207–1214, 2010."
REFERENCES,0.1724770642201835,"Huizhen Yu. On convergence of emphatic temporal-difference learning. In Proc. Annual Conference
on Learning Theory (COLT), pp. 1724–1751. PMLR, 2015."
REFERENCES,0.17339449541284405,"Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized ofﬂine estimation of
stationary values. Proc. International Conference on Learning Representations (ICLR), 2020a."
REFERENCES,0.1743119266055046,"Shangtong Zhang and Shimon Whiteson. Truncated emphatic temporal difference methods for pre-
diction and control. arXiv preprint arXiv:2108.05338, 2021."
REFERENCES,0.17522935779816515,"Shangtong Zhang, Bo Liu, Hengshuai Yao, and Shimon Whiteson.
Provably convergent two-
timescale off-policy actor-critic with function approximation. In Proc. International Conference
on Machine Learning (ICML), pp. 11204–11213. PMLR, 2020b."
REFERENCES,0.1761467889908257,Published as a conference paper at ICLR 2022
REFERENCES,0.17706422018348625,SUPPLEMENTARY MATERIALS
REFERENCES,0.1779816513761468,"A
SPECIFICATION OF EXPERIMENTS IN SECTION 5"
REFERENCES,0.17889908256880735,"A.1
EXPERIMENT SETTINGS"
REFERENCES,0.1798165137614679,"The BAIRD counter-example is illustrated in Figure 4, which has 7 states and 2 actions. If the ﬁrst
action (illustrated as dashed lines) is taken, then the environment transitions from the current state
to states 1 to 6 following the uniform distribution and returns a reward 0; and if the second action
(illustrated as solid lines) is taken, the environment transitions from the current state to state 7 with
probability 1 and returns a reward 1. We choose the target policy as π(0|s) = 0.1 and π(1|s) = 0.9
for all states; and choose the behavior policy as µ(0|s) = 6/7 and µ(1|s) = 1/7 for all states.
Moreover, we specify the discount factor γ = 0.99."
REFERENCES,0.18073394495412845,"Figure 4: BAIRD example (Sutton & Barto, 2018)"
REFERENCES,0.181651376146789,"A.2
FEATURES FOR EXPERIMENTS IN SECTION 5.2"
REFERENCES,0.18256880733944955,"In the experiment in Section 5.2, we choose the following features:"
REFERENCES,0.1834862385321101,"Φ1 =(0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.37)⊤;"
REFERENCES,0.18440366972477065,"Φ2 =((0.3425, 0.0171)⊤, (0.1902, 0.4248)⊤, (0.1354, 0.76)⊤, (0.1357, 0.7973)⊤,"
REFERENCES,0.1853211009174312,"(0.8674, 0.8774)⊤, (0.5166, 0.9493)⊤, (0.3094, 0.8535)⊤)⊤;"
REFERENCES,0.18623853211009175,"Φ3 =((0.5162, 0.9013)⊤, (0.5128, 0.5999)⊤, (0.289, 0.4649)⊤, (0.3399, 0.5334)⊤,"
REFERENCES,0.1871559633027523,"(0.315, 0.2278)⊤, (0.667, 0.461)⊤, (0.3706, 0.1457)⊤)⊤."
REFERENCES,0.18807339449541285,"A.3
COMPUTATION OF THE FIXED POINT OF PER-ETD(λ)"
REFERENCES,0.1889908256880734,"In this section, we provide the steps to compute the ﬁxed point of PER-ETD(λ) in Figure 3. We ﬁrst
deﬁne the matrix A and c as follows"
REFERENCES,0.18990825688073396,"A := lim
t→∞E
h
At

:=
 
ρb
teb
t(φb
t −γφb+1
t
⊤i
,"
REFERENCES,0.1908256880733945,"c := lim
t→∞E

ct
 := ρb
trb
teb
t

."
REFERENCES,0.19174311926605506,"It can be shown that, the ﬁxed point of PER-ETD(λ) algorithm is θ∗= A−1c."
REFERENCES,0.1926605504587156,"We next show how to derive the formulation of the matrix A and vector c. As we will show later in
eqs. (51), (53) and (55), we have"
REFERENCES,0.19357798165137616,"A = lim
t→∞E [At|Ft−1] = ¯βb(I −γPπ)Φ,
(10)"
REFERENCES,0.1944954128440367,"c = lim
t→∞E [ct|Ft−1] = ¯βbrπ,
(11)"
REFERENCES,0.19541284403669726,where ¯βb := limt→∞E [βb] and
REFERENCES,0.1963302752293578,"βb(s) = λΦ⊤Dµ,b + (1 −λ)Φ⊤Fb + γλβb−1Pπ,
(12)"
REFERENCES,0.19724770642201836,Published as a conference paper at ICLR 2022
REFERENCES,0.1981651376146789,"where Dµ,τ = diag(dµ,τ), dµ,τ(s) = P(sτ
t = s|Ft−1), Fb = diag(fb), and fb is determined
iteratively by eq. (25) as follows"
REFERENCES,0.19908256880733946,"fb = dµ,b + γP ⊤
π fb−1,
with
f0 = dµ,0.
(13)"
REFERENCES,0.2,Taking expectation on both sides of eqs. (12) and (13) with respect to Ft−1 and letting t →∞yield
REFERENCES,0.20091743119266056,"¯fb = dµ + γP ⊤
π ¯fb−1,
with
¯f0 = dµ,
(14)
¯βb = λΦ⊤Dµ + (1 −λ)Φ⊤¯Fb + γλ¯βb−1Pπ,
with
¯β0 = Φ⊤Dµ
(15)"
REFERENCES,0.2018348623853211,"where ¯fb := limt→∞E[fb] and ¯Fb = diag( ¯fb). The explicit formulation of ¯βb can be derived by
applying eqs. (14) and (15) iteratively. We can then obtain A and c by substituting the obtained
formulation of ¯βb into eq. (10) and eq. (11), respectively."
REFERENCES,0.20275229357798166,"A.4
REPLOTTED FIGURES 1 AND 2 WITH VARIANCE BARS"
REFERENCES,0.2036697247706422,"In this subsection, we replotted Figures 1 and 2 with variance bars (rather than error bands) in
Figures 5 and 6, respectively."
REFERENCES,0.20458715596330276,"(a) Comparison of TD, ETD, PER-ETD(0)
(b) Tradeoff between bias and variance by b"
REFERENCES,0.20550458715596331,Figure 5: Performance of PER-ETD(0) and comparison
REFERENCES,0.20642201834862386,"(a) Feature Φ1
(b) Feature Φ2
(c) Feature Φ3"
REFERENCES,0.20733944954128442,Figure 6: Performance of PER-ETD(λ) and dependence on features
REFERENCES,0.20825688073394497,"B
MORE EXPERIMENTS"
REFERENCES,0.20917431192660552,"In this section, we conduct further experiments to answer the following two intriguing questions:"
REFERENCES,0.21009174311926607,"• If the distribution mismatch parameter ρmax changes, how will different approaches per-
form and compare with each other?"
REFERENCES,0.21100917431192662,"• Focusing on our algorithm PER-ETD, how do the choices of behavior policy and target
policy affect its convergence?"
REFERENCES,0.21192660550458717,Published as a conference paper at ICLR 2022
REFERENCES,0.21284403669724772,"(a) ρmax = 1.17
(b) ρmax = 5.60"
REFERENCES,0.21376146788990827,"Figure 7: Comparisons of TD, ETD, PER-ETD(0) with different target policies"
REFERENCES,0.21467889908256882,"(a) b = 4
(b) b = 6"
REFERENCES,0.21559633027522937,"Figure 8: Performance of PER-ETD(0) under different target policies (marked by their different
resulting distribution mismatch parameter ρmax). The behavior policy is kept the same."
REFERENCES,0.21651376146788992,"(a) b = 4
(b) b = 6"
REFERENCES,0.21743119266055047,"Figure 9: Performance of PER-ETD(0) under different behavior policies (marked by their different
resulting distribution mismatch parameter ρmax). The target policy is kept the same."
REFERENCES,0.21834862385321102,"We focus on the MDP environment in Appendix A.1. In our experiments, the performance of every
algorithm is averaged over 20 random initializations and the error band in our plots captures the
actual variation of the performance during these experimental runs (which can be viewed as the
variance of the algorithms)."
REFERENCES,0.21926605504587157,"In Figure 7, we consider two settings with the distribution mismatch parameter ρmax = 1.17 and
5.60, respectively, and compare the performance of three off-policy algorithms: TD, vanilla ETD
and our PER-ETD. More speciﬁcally, we choose the behavior policy as µ(1|s) = 6"
REFERENCES,0.22018348623853212,7 and µ(2|s) = 1 7
REFERENCES,0.22110091743119267,Published as a conference paper at ICLR 2022
REFERENCES,0.22201834862385322,"for all states, and choose two target polices, whose probabilities to take the second action are 0.167
and 0.8, respectively, on all states. (Then their probabilities to take the ﬁrst action are determined
automatically through π(1|s) + π(2|s) = 1). Therefore, the maximum distribution mismatch ρmax
for the these two target policies are 1.17 and 5.60, respectively. Figure 7 (a) shows that under only
slightly mismatch (i.e., ρmax = 1.17), TD suffers from a large convergence error (i.e., the error with
respect to the ground truth value function at the point of convergence) and converges slowly. Vanilla
ETD converges with the fastest rate and achieves a smaller convergence error than TD, but suffers
from a relatively large variance. Our PER-ETD achieves a better tradeoff between the convergence
rate and the variance (faster rate than TD, almost the same convergence error as ETD but with
smaller variance). Figure 7 (b) shows that under a large distribution mismatch (i.e., ρmax = 5.6),
TD does not converge, and vanilla ETD experiences a substantially large variance. However, our
PER-ETD still convergences fast as long as the period length b is chosen properly, e.g., b = 4, 6, 8.
Further note that PER-ETD has a smaller convergence error as the period length b increases, but the
variance gets larger; which are consistent with our theorem."
REFERENCES,0.22293577981651377,"In Figure 8, we focus on our PER-ETD, and study how different target policies affect the per-
formance. We choose the same behavior policy as the above experiment, i.e., µ(1|s) =
6
7 and
µ(2|s) = 1"
REFERENCES,0.22385321100917432,"7 for all states. We choose 5 target polices, whose probabilities to take the second action
are 0.167, 0.2, 0.4, 0.6, 0.8 for all states, respectively. These different target policies affect the per-
formance via their resulting distribution mismatch ρmax = 1.17, 1.40, 2.80, 4.20, 5.60, respectively.
For both b = 4 and b = 6, Figure 8 indicates that larger mismatch causes slower convergence rate,
larger convergence error and larger variance, which agrees with our theorem."
REFERENCES,0.22477064220183487,"In Figure 9, we also focus on our PER-ETD, and study how different behavior policies affect the
performance. We pick the target policy to be π(1|s) = 0.1 and π(2|s) = 0.9 for all states, and 5
different behavior policies with µ(2|s) = 0.2, 0.4, 0.6, 0.7 and 0.8 for all state, respectively. These
different behavior policies affect the performance via their different resulting distribution mismatch
ρmax = 1.12, 1.29, 1.5, 2.25, 4.5, respectively. Figure 9 clearly demonstrates that larger distribution
mismatch results in slower convergence rate, larger convergence error and larger variance, which is
in the same nature as changing the target policy shown in Figure 8 and is consistent with our theorem."
REFERENCES,0.22568807339449543,"C
SUPPORTING LEMMAS"
REFERENCES,0.22660550458715598,The following lemma is well-known. We include it for the convenience of our proof.
REFERENCES,0.22752293577981653,"Lemma 2. Consider a transition matrix P ∈RN×N, where P"
REFERENCES,0.22844036697247708,"j P(i,j) = 1 for all i and 0 ≤
P(i,j) ≤1 for all j. We have for any n ∈N, ∥P n∥∞= 1, and ∥(P ⊤)n∥1 = 1."
REFERENCES,0.22935779816513763,"Lemma 3. Consider 0 < p, q < 1, with p ̸= q. We have Pn−1
k=0 pkqn−k ≤
1
|p−q|ξn, where
ξ = max{p, q}."
REFERENCES,0.23027522935779818,"Proof of Lemma 3. If p > q, we have n−1
X"
REFERENCES,0.23119266055045873,"k=0
pkqn−k ≤ n−1
X"
REFERENCES,0.23211009174311928,"k=0
pk+1qn−1−k ≤ n−1
X"
REFERENCES,0.23302752293577983,"m=0
pn−mqm."
REFERENCES,0.23394495412844038,"Without loss of generality, we assume p < q and ξ = q. We have n−1
X"
REFERENCES,0.23486238532110093,"k=0
pkqn−k = qn · n−1
X"
REFERENCES,0.23577981651376148,"k=0
( p"
REFERENCES,0.23669724770642203,q )k = qn · 1 −(p/q)n
REFERENCES,0.23761467889908258,"1 −p/q
≤
1
q −p · qn =
1
|p −q| · ξn."
REFERENCES,0.23853211009174313,Lemma 4. Consider a matrix P ∈RN×N where P
REFERENCES,0.23944954128440368,"j P(i,j) ≤C for all i and 0 ≤P(i,j) ≤1 for all
j, and a positive vector x where x ∈RN and xi ≥0 for all i. We have"
REFERENCES,0.24036697247706423,1⊤Px ≤C1⊤x.
REFERENCES,0.24128440366972478,Published as a conference paper at ICLR 2022
REFERENCES,0.24220183486238533,Proof of Lemma 4. We have
REFERENCES,0.24311926605504589,"1⊤Px =
X"
REFERENCES,0.24403669724770644,"1≤i,j≤N
Pi,jxi =
X"
REFERENCES,0.24495412844036699,"1≤i≤N
xi  X"
REFERENCES,0.24587155963302754,"1≤j≤N
Pi,j  ≤C
X"
REFERENCES,0.2467889908256881,"1≤i≤N
xi = C1⊤x."
REFERENCES,0.24770642201834864,"Lemma 5. Consider a diagonal matrix D ∈R|S|×|S|, where D = diag(d(1), d(2), . . . , d(|S|)). a
transition matrix P ∈R|S|×|S| where P"
REFERENCES,0.2486238532110092,"j Pi,j = 1 for all i = 1, 2, . . . , |S| and Pi,j ≥0 for all j,
and an arbitrary vector x ∈R|S|. We have"
REFERENCES,0.24954128440366974,∥Φ⊤DPx∥2 ≤Bφ∥d∥1∥x∥∞.
REFERENCES,0.25045871559633026,Proof of Lemma 5. We have
REFERENCES,0.25137614678899084,"Φ⊤D = (d(1)φ(1), d(2)φ(2), . . . , d(|S|)φ(|S|)) ,"
REFERENCES,0.25229357798165136,"and
 
Φ⊤DP
"
REFERENCES,0.25321100917431194,"(·,s) =
X"
REFERENCES,0.25412844036697246,"˜s
P˜s,sd(˜s)φ(˜s),"
REFERENCES,0.25504587155963304,which implies
REFERENCES,0.25596330275229356,"Φ⊤DPx =
X"
REFERENCES,0.25688073394495414,"s
x(s) X"
REFERENCES,0.25779816513761467,"˜s
P˜s,sd(˜s)φ(˜s) ! =
X ˜s X"
REFERENCES,0.25871559633027524,"s
x(s)P˜s,s !"
REFERENCES,0.25963302752293577,d(˜s)φ(˜s).
REFERENCES,0.26055045871559634,Taking ℓ2 norm on both sides of the above equality yields
REFERENCES,0.26146788990825687,"Φ⊤DPx

2 =  X ˜s X"
REFERENCES,0.26238532110091745,"s
x(s)P˜s,s !"
REFERENCES,0.26330275229357797,"d(˜s)φ(˜s) 2 ≤
X ˜s  X"
REFERENCES,0.26422018348623855,"s
x(s)P˜s,s"
REFERENCES,0.26513761467889907,· |d(˜s)| · ∥φ(˜s)∥2
REFERENCES,0.26605504587155965,"= Bφ∥d∥1 · max
˜s  X"
REFERENCES,0.26697247706422017,"s
x(s)P˜s,s "
REFERENCES,0.26788990825688075,"≤Bφ∥d∥1∥x∥∞max
˜s  X"
REFERENCES,0.26880733944954127,"s
P˜s,s "
REFERENCES,0.26972477064220185,= Bφ∥d∥1∥x∥∞.
REFERENCES,0.2706422018348624,"Lemma 6. Consider a diagonal matrix D ∈R|S|×|S| where D = diag(d(1), d(2), . . . , d(|S|)), a
transition matrix P ∈R|S|×|S| where P"
REFERENCES,0.27155963302752295,"j Pi,j = 1 for all i = 1, 2, . . . , |S| and Pi,j ≥0 for all j, a
matrix Q ∈R|S|×|S| that satisﬁes 0 ≤Qi,j ≤CPi,j, where C > 1 is a constant, and an arbitrary
vector x ∈R|S|. We have"
REFERENCES,0.2724770642201835,"trace
 
QmP nΦΦ⊤D

≤CmB2
φ∥d∥1, and,"
REFERENCES,0.27339449541284405,"trace
 
P nQmΦΦ⊤D

≤CmB2
φ∥d∥1,"
REFERENCES,0.2743119266055046,for any m and n ∈N≥0.
REFERENCES,0.27522935779816515,"Proof of Lemma 6. For any given τ ≥m ≥0 and n ≥0,"
REFERENCES,0.2761467889908257,"(QmP nΦΦ⊤)(i,j)
 =

(Qm)(i,·), (P nΦΦ⊤)(·,j)

(i)
≤∥(Qm)(i,·)∥1∥(P nΦΦ⊤)(·,j)∥∞,
(16)"
REFERENCES,0.27706422018348625,Published as a conference paper at ICLR 2022
REFERENCES,0.2779816513761468,where (i) follows from the H¨older’s inequality.
REFERENCES,0.27889908256880735,"Furthermore, for the term of (P nΦΦ⊤)(·,j), we have,"
REFERENCES,0.2798165137614679,"∥(P nΦΦ⊤)(·,j)∥∞≤∥P n∥∞∥(ΦΦ⊤)(·,j)∥∞
(i)
= ∥(ΦΦ⊤)(·,j)∥∞,"
REFERENCES,0.28073394495412846,where (i) follows from Lemma 2.
REFERENCES,0.281651376146789,"Moreover, for the ith entry of (ΦΦ⊤)(·,j), we have
(ΦΦ⊤)(i,j)
 =
φ(i)⊤φ(j)
 ≤∥φ(i)∥2∥φ(j)∥2 ≤B2
φ"
REFERENCES,0.28256880733944956,"The above uniform bounds over all i imply that ∥(ΦΦ⊤)(·,j)∥∞≤B2
φ. Hence,"
REFERENCES,0.2834862385321101,"∥(P nΦΦ⊤)(·,j)∥∞≤B2
φ."
REFERENCES,0.28440366972477066,"Substituting the above inequality back into eq. (16), we obtain"
REFERENCES,0.2853211009174312,"(QmP nΦΦ⊤)(i,j)
 ≤B2
φ∥(Qm)(i,·)∥1"
REFERENCES,0.28623853211009176,"(i)
≤Cm∥(P τ−m)(i,·)∥1B2
φ"
REFERENCES,0.2871559633027523,"≤CmB2
φ
X"
REFERENCES,0.28807339449541286,"j
P τ−m(j|i) = CmB2
φ,
(17)"
REFERENCES,0.2889908256880734,"where (i) follows by the condition of Q, Q(i,j) ≤CP(i,j) for all i, j."
REFERENCES,0.28990825688073396,"Finally, we have"
REFERENCES,0.2908256880733945,"trace
 
QmP nΦΦ⊤D

=
X"
REFERENCES,0.29174311926605506,"i
d(i)(QmP nΦΦ⊤)(i,i) ≤
X"
REFERENCES,0.2926605504587156,"i
|d(i)|
(QmP nΦΦ⊤)(i,i)"
REFERENCES,0.29357798165137616,"≤∥d∥1 max
i
(QmP nΦΦ⊤)(i,i)

(i)
≤CmB2
φ∥d∥1,
(18)"
REFERENCES,0.2944954128440367,where (i) follows from eq. (17).
REFERENCES,0.29541284403669726,"Following steps similar to those in eqs. (16) to (18), we can obtain"
REFERENCES,0.2963302752293578,"trace
 
P nQmΦΦ⊤D

≤CmB2
φ∥d∥1."
REFERENCES,0.29724770642201837,"Lemma 7. The operators T (θ) and T λ(θ) satisfy the generalized monotone variational inequality.
There exist µ0, µλ > 0, s.t., ⟨T (θ), θ −θ∗⟩≥µ0∥θ −θ∗∥2
2 ,and

T λ(θ), θ −θ∗
≥µλ∥θ −θ∗∥2
2."
REFERENCES,0.2981651376146789,Proof of Lemma 7. We have
REFERENCES,0.29908256880733947,"⟨T (θ), θ −θ∗⟩
(i)
=

 
Φ⊤F(I −γPπ)Φ

(θ −θ∗), θ −θ∗"
REFERENCES,0.3,"= (θ −θ∗)⊤ 
Φ⊤F(I −γPπ)Φ

(θ −θ∗)"
REFERENCES,0.30091743119266057,"≥λmin
 
Φ⊤F(I −γPπ)Φ

∥θ −θ∗∥2
2,
(19)"
REFERENCES,0.3018348623853211,where (i) follows from the deﬁnition of the T and θ∗.
REFERENCES,0.30275229357798167,"Recall that F(I −γPπ) is positive deﬁnite(Sutton et al., 2016; Mahmood et al., 2015) and Φ has
linearly independent columns. For any x ∈Rd with x ̸= 0, we have Φx ̸= 0 and"
REFERENCES,0.3036697247706422,x⊤Φ⊤F(I −γPπ)Φx = (Φx)⊤F(I −γPπ)(Φx) > 0.
REFERENCES,0.30458715596330277,"The above inequality shows that Φ⊤F(I −γPπ)Φ is positive deﬁnite and thus, there exists µ0 > 0
such that µ0 = λmin
 
Φ⊤F(I −γPπ)Φ

."
REFERENCES,0.3055045871559633,"Following steps similar to those in eq. (19) and applying the positive deﬁniteness of M(I −
γλPπ)−1(I −γPπ) (Sutton et al., 2016; Mahmood et al., 2015) yield

T λ(θ), θ −θ∗
≥µλ∥θ −θ∗∥2
2."
REFERENCES,0.30642201834862387,Published as a conference paper at ICLR 2022
REFERENCES,0.3073394495412844,"Lemma 8. The operators T (θ) and T λ(θ) satisfy the Lipschitz condition. There exist L0, Lλ > 0,
such that, ∥T (θ1) −T (θ2)∥2 ≤L0∥θ1 −θ2∥2, and
T λ(θ1) −T λ(θ2)

2 ≤Lλ∥θ1 −θ2∥2."
REFERENCES,0.30825688073394497,Proof of lemma 8. We have
REFERENCES,0.3091743119266055,"∥T (θ1) −T (θ2)∥2 =
 
Φ⊤F(I −γPπ)Φ

(θ1 −θ2)

2
≤
Φ⊤F(I −γPπ)Φ

2 ∥θ1 −θ2∥2.
(20)"
REFERENCES,0.3100917431192661,"Let L0 :=
Φ⊤F(I −γPπ)Φ

2, eq. (20) completes the proof of the ﬁrst inequality in the Lemma.
Let Lλ := ∥Φ⊤M(I −γλPπ)−1(I −γPπ)Φ∥2, the steps similar to those in eq. (20) ﬁnalizes the
proof of the second inequality in the Lemma."
REFERENCES,0.3110091743119266,"Lemma 9 (Three point lemma). Suppose Θ is a closed and bounded subset of Rd, and θ∗is the
solution of the following maximization problem, maxθ∈Θ η ⟨G, θ⟩+ 1"
REFERENCES,0.3119266055045872,"2∥θ −θ0∥2
2, where G ∈Rd is
a vector. Then, we have, for any θ ∈Θ,"
REFERENCES,0.3128440366972477,"η ⟨G, θ∗−θ⟩+ 1"
REFERENCES,0.3137614678899083,"2∥θ0 −θ∗∥2
2 ≤1"
REFERENCES,0.3146788990825688,"2∥θ0 −θ∥2
2 −1"
REFERENCES,0.3155963302752294,"2∥θ∗−θ∥2
2."
REFERENCES,0.3165137614678899,Proof. The proof can be found in Lan (2020).
REFERENCES,0.3174311926605505,"D
PROOFS OF PROPOSITIONS AND THEOREM FOR PER-ETD(0)"
REFERENCES,0.318348623853211,"D.1
PROOF OF PROPOSITION 1"
REFERENCES,0.3192660550458716,"First, by the deﬁnition of bTt(θt), we have"
REFERENCES,0.3201834862385321,"E
h
bTt(θt)
Ft−1
i"
REFERENCES,0.3211009174311927,"(i)
=
X s∈S X a∈A X s′∈S"
REFERENCES,0.3220183486238532,"· P
 
sb
t = s, ab
t = a, sb+1
t
= s′Ft−1

E
h
bTt(θt)
Ft−1, sb
t = s, ab
t = a, sb+1
t
= s′i"
REFERENCES,0.3229357798165138,"(ii)
=
X s∈S X a∈A X"
REFERENCES,0.3238532110091743,"s′∈S
P
 
sb
t = s
Ft−1

µ(a|s)P(s′|s, a)"
REFERENCES,0.3247706422018349,"· E

ρb
tF b
t φ(s)[φ⊤(s)θt −r(s, a) −γφ⊤(s′)θt]
Ft−1, sb
t = s, ab
t = a, sb+1
t
= s′"
REFERENCES,0.3256880733944954,"(iii)
=
X s∈S X a∈A X"
REFERENCES,0.326605504587156,"s′∈S
P
 
sb
t = s
Ft−1

π(a|s)P(s′|s, a)φ(s)[φ⊤(s)θt −r(s, a) −γφ⊤(s′)θt]"
REFERENCES,0.3275229357798165,"· E

F b
t
Ft−1, sb
t = s
 =
X"
REFERENCES,0.3284403669724771,"s∈S
P
 
sb
t = s
Ft−1

E

F b
t
Ft−1, sb
t = s

φ(s)

φ⊤(s)θt −γ[PπΦ](s,·)θt −rπ(s)

,
(21)"
REFERENCES,0.3293577981651376,"where
(i)
follows
from
the
law
of
total
probability,
(ii)
follows
from
rewriting
P
 
sb
t = s, ab
t = a, sb+1
t
= s′Ft−1

, and (iii) follows from the facts that F b
t
only depends
on (s0, a0, s1, a1, . . . , st(b+1)+b−1, at(b+1)+b−1) and the chain is Markov."
REFERENCES,0.3302752293577982,"Recall the deﬁnition of T (θt), we have"
REFERENCES,0.3311926605504587,"T (θt) = Φ⊤F [(I −γPπ)Φθt −rπ] =
X"
REFERENCES,0.3321100917431193,"s∈S
f(s)
 
φ⊤(s)θt −γ[PπΦ](s,·)θt −rπ(s)

φ(s).
(22)"
REFERENCES,0.3330275229357798,"Equations (21) and (22) together imply the following,"
REFERENCES,0.3339449541284404,"T (θt) −E
h
bTt(θt)
Ft−1
i
=
X s∈S"
REFERENCES,0.3348623853211009," 
f(s) −P
 
sb
t = s
Ft−1

E

F b
t
Ft−1, sb
t = s
"
REFERENCES,0.3357798165137615,Published as a conference paper at ICLR 2022
REFERENCES,0.336697247706422,"·

φ⊤(s)θt −γ [PπΦ](s,·) θt −rπ(s)

φ(s)"
REFERENCES,0.3376146788990826,"(i)
=
X"
REFERENCES,0.3385321100917431,"s∈S
(f(s) −fb(s))

φ⊤(s)θt −γ [PπΦ](s,·) θt −rπ(s)

φ(s) =
X"
REFERENCES,0.3394495412844037,"s∈S
(f(s) −fb(s)) ((I −γPπ) Φθt −rπ)s φ(s),"
REFERENCES,0.3403669724770642,"where in (i) We deﬁne fb(s) := P
 
sb
t = s
Ft−1

E

F b
t
Ft−1, sb
t = s

. Taking ℓ2 norm on both
sides of the above equality yields
T (θt) −E
h
bTt(θt)
Ft−1
i
2 ≤  X"
REFERENCES,0.3412844036697248,"s∈S
(f(s) −fb(s)) ((I −γPπ) Φθt −rπ)s φ(s) 2
≤
X"
REFERENCES,0.3422018348623853,"s∈S
|f(s) −fb(s)| · |((I −γPπ) Φθt −rπ)s| · ∥φ(s)∥2"
REFERENCES,0.3431192660550459,"≤max
s∈S {∥φ(s)∥2} max
s∈S {|((I −γPπ) Φθt −rπ)s|}
X"
REFERENCES,0.3440366972477064,"s∈S
|f(s) −fb(s)|"
REFERENCES,0.344954128440367,"= Bφ∥(I −γPπ)Φθt −rπ∥∞∥f −fb∥1.
(23)"
REFERENCES,0.3458715596330275,"We next proceed to bound ∥f −fb∥1. Consider fb(s), we have"
REFERENCES,0.3467889908256881,"fb(s) = P
 
sb
t = s
Ft−1

E

F b
t
sb
t = s, Ft−1
"
REFERENCES,0.3477064220183486,"(i)
= P
 
sb
t = s
Ft−1
 X"
REFERENCES,0.3486238532110092,"˜s∈S,˜a∈A
P
 
sb−1
t
= ˜s, ab−1
t
= ˜a
sb
t = s, Ft−1
"
REFERENCES,0.3495412844036697,"· E

γρb−1
t
F b−1
t
+ 1
Ft−1, sb
t = s, sb−1
t
= ˜s, ab−1
t
= ˜a
"
REFERENCES,0.3504587155963303,"(ii)
= P
 
sb
t = s
Ft−1
 · "
REFERENCES,0.3513761467889908,"1 +
X"
REFERENCES,0.3522935779816514,"˜s∈S,˜a∈A"
REFERENCES,0.3532110091743119,"P
 
sb−1
t
= ˜s
Ft−1

µ(˜a|˜s)P(s|˜s, ˜a)
P
 
sb
t = s
Ft−1

E

γ π(˜a|˜s)"
REFERENCES,0.3541284403669725,"µ(˜a|˜s)F b−1
t"
REFERENCES,0.355045871559633,"Ft−1, sb−1
t
= ˜s
 "
REFERENCES,0.3559633027522936,"= P
 
sb
t = s
Ft−1

+ γ
X"
REFERENCES,0.3568807339449541,"˜s∈S
P(sb−1
t
= ˜s|Ft−1)Pπ(s|˜s)E

F b−1
t
Ft−1, sb−1
t
= s

,"
REFERENCES,0.3577981651376147,"where (i) follows from the law of total probability and (ii) follows from the Bayes rule and the facts
that F b−1
t
only depends on the chain elements (s0
t, a0
t, s1
t, . . . , sb−1
t
, ab−1
t
) and the Markov property."
REFERENCES,0.3587155963302752,"Deﬁne dµ,b(s) = P
 
sb
t = s
Ft−1

, the above equality can be rewritten as"
REFERENCES,0.3596330275229358,"fb(s) = dµ,b(s) + γ
X"
REFERENCES,0.3605504587155963,"˜s
Pπ(s|˜s)fb−1(˜s).
(24)"
REFERENCES,0.3614678899082569,"Since eq. (24) holds for all s ∈S, we have"
REFERENCES,0.3623853211009174,"fb = dµ,b + γP ⊤
π fb−1.
(25)"
REFERENCES,0.363302752293578,"Note that for f we have the following holds (Sutton et al., 2016; Zhang et al., 2020a)"
REFERENCES,0.3642201834862385,"f = dµ + γP ⊤
π f.
(26)"
REFERENCES,0.3651376146788991,Equations (25) and (26) imply
REFERENCES,0.3660550458715596,"f −fb = dµ −dµ,b + γP ⊤
π (f −fb−1)."
REFERENCES,0.3669724770642202,Applying the above equality recursively yields
REFERENCES,0.3678899082568807,"f −fb = b−1
X"
REFERENCES,0.3688073394495413,"τ=0
(γP ⊤
π )τ(dµ −dµ,b−τ) + γb(P ⊤
π )b(f −f0)."
REFERENCES,0.3697247706422018,Published as a conference paper at ICLR 2022
REFERENCES,0.3706422018348624,"Take ℓ1 norm on both sides of the above equality, we have"
REFERENCES,0.37155963302752293,"∥f −fb∥1 =  b−1
X"
REFERENCES,0.3724770642201835,"τ=0
γτ(P ⊤
π )τ(dµ −dµ,b−τ) + γb(P ⊤
π )b(f −f0)  ≤ b−1
X"
REFERENCES,0.37339449541284403,"τ=0
γτ (P ⊤
π )τ(dµ −dµ,b−τ)

1 + γb (P ⊤
π )b(f −f0)

1 ≤ b−1
X"
REFERENCES,0.3743119266055046,"τ=0
γτ (P ⊤
π )τ
1 ∥dµ −dµ,b−τ∥1 + γb (P ⊤
π )b ∥f −f0∥1 (i)
≤ b−1
X"
REFERENCES,0.37522935779816513,"τ=0
γτ ∥dµ −dµ,b−τ∥1 + γb ∥f −f0∥1"
REFERENCES,0.3761467889908257,"(ii)
≤ b−1
X"
REFERENCES,0.37706422018348623,"τ=0
CMγτχb−τ + γb∥f −f0∥1"
REFERENCES,0.3779816513761468,"(ii)
≤
1
|χ −γ| · CMξb + γb(1 + ∥f∥1),
(27)"
REFERENCES,0.37889908256880733,"where (i) follows from Lemma 2, (ii) follows from Lemma 1 , and (iii) follows from Lemma 3 and
deﬁning ξ := max{χ, γ}."
REFERENCES,0.3798165137614679,"To bound the term ∥(I −γPπ)Φθt −rπ∥∞, we proceed as following"
REFERENCES,0.38073394495412843,"∥(I −γPπ)Φθt −rπ∥∞
(i)
= ∥(I −γPπ)(Φθt −Vπ)∥∞
= ∥(I −γPπ)(Φθt −Φθ∗+ Φθ∗−Vπ)∥∞
≤∥I −γPπ∥∞(∥Φθt −Φθ∗∥∞+ ∥Φθ∗−Vπ∥∞)"
REFERENCES,0.381651376146789,"(ii)
≤(1 + γ)Bφ∥θt −θ∗∥2 + (1 + γ)ϵapprox,
(28)"
REFERENCES,0.38256880733944953,"where (i) follows from the fact Vπ = (I −γPπ)−1rπ and (ii) follows from the facts that ∥I −
γPπ∥∞= maxi{1 −(Pπ)(i,i) + γ P
j̸=i(Pπ)(i,j)} ≤1 + γ, ϵapprox := ∥Φθ∗−Vπ∥∞, and"
REFERENCES,0.3834862385321101,"∥Φθt −Φθ∗∥∞= max
s∈S φ⊤(s)(θt −θ∗) ≤Bφ∥θt −θ∗∥2."
REFERENCES,0.38440366972477064,"Substituting eqs. (27) and (28) into eq. (23) yields
T (θt) −E
h
bTt(θt)
Ft−1
i
2 ≤
 
B2
φ∥θt −θ∗∥2 + Bφϵapprox

(1 + γ)

CMξb"
REFERENCES,0.3853211009174312,"|χ−γ| + γb(1 + ∥f∥1)
"
REFERENCES,0.38623853211009174,"≤Cb (Bφ∥θt −θ∗∥2 + ϵapprox) ξb,"
REFERENCES,0.3871559633027523,"where ξ = max {χ, γ} and Cb = Bφ(1 + γ)

CM
|χ−γ| + (1 + ∥f∥1)

."
REFERENCES,0.38807339449541284,"D.2
PROOF OF PROPOSITION 2"
REFERENCES,0.3889908256880734,"According to the deﬁnition of bTt(θt), we have"
REFERENCES,0.38990825688073394,"E
bTt(θt)

2Ft−1 "
REFERENCES,0.3908256880733945,"= E
h 
ρb
tF b
t
2  
θ⊤
t φb
t −rb
t −γθ⊤
t φb+1
t
2 ∥φb
t∥2
2
Ft−1
i"
REFERENCES,0.39174311926605504,"(i)
=
X s∈S X a∈A X"
REFERENCES,0.3926605504587156,"s′∈S
P
 
sb
t = s, ab
t = a, sb+1
t
= s′Ft−1

·
 
θ⊤
t φ(s) −r(s, a) −γθ⊤
t φ(s′)
2"
REFERENCES,0.39357798165137614,"· ∥φ(s)∥2
2 · E

(ρb
tF b
t )2Ft−1, sb
t = s, ab
t = a, sb+1
t
= s′"
REFERENCES,0.3944954128440367,"(ii)
=
X s∈S X a∈A X"
REFERENCES,0.39541284403669724,"s′∈S
P
 
sb
t = s
Ft−1

µ(a|s)P(s′|s, a) ·
 
θ⊤
t φ(s) −r(s, a) −γθ⊤
t φ(s′)
2"
REFERENCES,0.3963302752293578,Published as a conference paper at ICLR 2022
REFERENCES,0.39724770642201834,"· ∥φ(s)∥2
2 · π2(a|s)"
REFERENCES,0.3981651376146789,"µ2(a|s)E

(F b
t )2Ft−1, sb
t = s, ab
t = a, sb+1
t
= s′ =
X"
REFERENCES,0.39908256880733944,"s∈S
P
 
sb
t = s
Ft−1

E

(F b
t )2Ft−1, sb
t = s

∥φ(s)∥2
2 ·
X a∈A X s′∈S"
REFERENCES,0.4,π2(a|s)
REFERENCES,0.40091743119266054,"µ(a|s) P(s′|s, a)(φ⊤(s)θt −r(s, a) −γφ⊤(s′)θt)2,
(29)"
REFERENCES,0.4018348623853211,"where (i) follows from the law of total probability and (ii) follows from the fact that F b
t is indepen-
dent from previous states and actions given sb
t."
REFERENCES,0.40275229357798165,"Note that ∥φ(s)∥2 ≤Bφ for all s ∈S, r(s, a) ≤rmax for all (s, a) ∈S × A, and ∥θt∥2 ≤Bθ for
all t due to projection. We have"
REFERENCES,0.4036697247706422,"(φ⊤(s)θt −r(s, a) −γφ⊤(s′)θt)2 ≤2[(φ(s) −γφ(s′))⊤θt]2 + 2r2(s, a)"
REFERENCES,0.40458715596330275,"≤2∥φ(s) −γφ(s′)∥2
2∥θt∥2
2 + 2r2
max
≤4(∥φ(s)∥2
2 + γ2∥φ(s′)∥2
2)∥θt∥2
2 + 2r2
max
≤4(1 + γ2)B2
φB2
θ + 2r2
max.
(30)"
REFERENCES,0.4055045871559633,"We also have
X a∈A X s′∈S"
REFERENCES,0.40642201834862385,π2(a|s)
REFERENCES,0.4073394495412844,"µ(a|s) P(s′|s, a) ≤ρmax ·
X a∈A X"
REFERENCES,0.40825688073394495,"s′∈S
π(a|s)P(s′|s, a) = ρmax."
REFERENCES,0.4091743119266055,Substituting the above two inequalities into eq. (29) yields
REFERENCES,0.41009174311926605,"E
bTt(θt)

2Ft−1 "
REFERENCES,0.41100917431192663,"≤ρmax
 
4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ
X"
REFERENCES,0.41192660550458715,"s∈S
P
 
sb
t = s
Ft−1

E

(F b
t )2Ft−1, sb
t = s

.
(31)"
REFERENCES,0.41284403669724773,"Deﬁne rb(s) := P
 
sb
t = s
Ft−1

E

(F b
t )2Ft−1, sb
t = s

= dµ,b(s)E

(F b
t )2Ft−1, sb
t = s

."
REFERENCES,0.41376146788990825,We have the following equations hold for rb(s):
REFERENCES,0.41467889908256883,"rb(s) = P(sb
t = s|Ft−1)E
h 
γρb−1
t
F b−1
t
+ 1
2Ft−1, sb
t = s
i"
REFERENCES,0.41559633027522935,"= dµ,b(s)E

1 + 2γρb−1
t
F b−1
t
+ γ2(ρb−1
t
)2(F b−1
t
)2Ft−1, sb
t = s
"
REFERENCES,0.41651376146788993,"= dµ,b(s) + 2γdµ,b(s)E

ρb−1
t
F b−1
t
Ft−1, sb
t = s
"
REFERENCES,0.41743119266055045,"+ γ2dµ,b(s)E

(ρb−1
t
)2(F b−1
t
)2Ft−1, sb
t = s

.
(32)"
REFERENCES,0.41834862385321103,"For the second term in the RHS of eq. (32), we have"
REFERENCES,0.41926605504587156,"dµ,b(s)E

ρb−1
t
F b−1
t
Ft−1, sb
t = s
"
REFERENCES,0.42018348623853213,"(i)
= dµ,b(s)
X"
REFERENCES,0.42110091743119266,"˜s∈S,˜a∈A
P
 
sb−1
t
= ˜s, ab−1
t
= ˜a
sb
t = s, Ft−1
"
REFERENCES,0.42201834862385323,"· E

ρb−1
t
F b−1
t
Ft−1, sb
t = s, sb−1
t
= ˜s, ab−1
t
= ˜a
"
REFERENCES,0.42293577981651376,"(ii)
= dµ,b(s)
X ˜s,˜a"
REFERENCES,0.42385321100917434,"dµ,b−1(˜s)µ(˜a|˜s)P(s|˜s, ˜a)"
REFERENCES,0.42477064220183486,"dµ,b(s)
· E

ρb−1
t
F b−1
t
Ft−1, sb
t = s, sb−1
t
= ˜s, ab−1
t
= ˜a
"
REFERENCES,0.42568807339449544,"(iii)
=
X"
REFERENCES,0.42660550458715596,"˜s∈S,˜a∈A
dµ,b−1(˜s)µ(˜a|˜s)P(s|˜s, ˜a) · π(˜a|˜s)"
REFERENCES,0.42752293577981654,"µ(˜a|˜s)E

F b−1
t
Ft−1, sb−1
t
= ˜s
 =
X"
REFERENCES,0.42844036697247706,"˜s∈S
Pπ(s|˜s) · dµ,b−1(˜s)E

F b−1
t
Ft−1, sb−1
t
= ˜s
"
REFERENCES,0.42935779816513764,"(iv)
= (P ⊤
π fb−1)s,
(33)"
REFERENCES,0.43027522935779816,Published as a conference paper at ICLR 2022
REFERENCES,0.43119266055045874,"where (i) follows from the law of total probability, (ii) follows from the Bayes rule, (iii) follows
from Markov property and (iv) follow from the deﬁnition of fb which is given above eq. (23)."
REFERENCES,0.43211009174311926,"For the third term on the RHS of eq. (32), we have"
REFERENCES,0.43302752293577984,"dµ,b(s)E

(ρb−1
t
F b−1
b
)2Ft−1, sb
t = s
"
REFERENCES,0.43394495412844036,"(i)
= dµ,b(s)
X"
REFERENCES,0.43486238532110094,"˜s∈S,˜a∈A
P
 
sb−1
t
= ˜s, ab−1
t
= ˜a
sb
t = s, Ft−1
"
REFERENCES,0.43577981651376146,"· E

(ρb−1
t
F b−1
b
)2Ft−1, sb
t = s, sb−1
t
= ˜s, ab−1
t
= ˜a
"
REFERENCES,0.43669724770642204,"(ii)
= dµ,b(s)
X ˜s,˜a"
REFERENCES,0.43761467889908257,"dµ,b−1(˜s)µ(˜a|˜s)P(s|˜s, ˜a)"
REFERENCES,0.43853211009174314,"dµ,b(s)
· E

(ρb−1
t
F b−1
t
)2Ft−1, sb
t = s, sb−1
t
= ˜s, ab−1
t
= ˜a
 =
X"
REFERENCES,0.43944954128440367,"˜s∈S,˜a∈A
dµ,b−1(˜s)µ(˜a|˜s)P(s|˜s, ˜a) · π2(˜a|˜s)"
REFERENCES,0.44036697247706424,"µ2(˜a|˜s) · E

(F b−1
t
)2Ft−1, sb−1
t
= ˜s
"
REFERENCES,0.44128440366972477,"(iii)
=
X"
REFERENCES,0.44220183486238535,"˜s∈S
Pµ,π(s|˜s)rb−1(˜s)"
REFERENCES,0.44311926605504587,"= (P ⊤
µ,πrb−1)s,
(34)"
REFERENCES,0.44403669724770645,"where (i) follows from the law of total probability, (ii) follows from the Bayes’ rule, and in (iii)
we deﬁne Pµ,π ∈R|S|×|S| where (Pµ,π)s,˜s = P"
REFERENCES,0.44495412844036697,"˜a∈A
π2(˜a|˜s)"
REFERENCES,0.44587155963302755,"µ(˜a|˜s) P(s|˜s, ˜a) for each (s, ˜s) ∈S × S."
REFERENCES,0.44678899082568807,Substituting eqs. (33) and (34) into eq. (32) yields
REFERENCES,0.44770642201834865,"rb = dµ,b + 2γP ⊤
π fb−1 + γ2P ⊤
µ,πrb−1."
REFERENCES,0.44862385321100917,We also have the following inequality holds
REFERENCES,0.44954128440366975,"1⊤rb = 1⊤dµ,b + 2γ1⊤P ⊤
π fb−1 + γ21⊤P ⊤
µ,πrb−1"
REFERENCES,0.4504587155963303,"(i)
= 1 + 2γ1⊤fb−1 + γ21⊤P ⊤
µ,πrb−1"
REFERENCES,0.45137614678899085,"(ii)
≤1 + 2γ1⊤fb−1 + γ2ρmax1⊤rb−1,
(35)"
REFERENCES,0.4522935779816514,"where (i) follows from 1⊤P ⊤
π = (Pπ1)⊤= 1⊤, and (ii) follows from the facts that rb−1 ⪰0 and"
REFERENCES,0.45321100917431195,"1⊤P ⊤
µ,π = (Pµ,π1)⊤= vec X s∈S X ˜a∈A"
REFERENCES,0.4541284403669725,π2(˜a|˜s)
REFERENCES,0.45504587155963305,"µ(˜a|˜s) P(s|˜s, ˜a) ! = vec X ˜a∈A"
REFERENCES,0.4559633027522936,π2(˜a|˜s)
REFERENCES,0.45688073394495415,µ(˜a|˜s) !
REFERENCES,0.4577981651376147,⪯ρmax1⊤.
REFERENCES,0.45871559633027525,Recursively applying eq. (35) yields
REFERENCES,0.4596330275229358,"1⊤rb ≤ b−1
X"
REFERENCES,0.46055045871559636,"τ=0
(γ2ρmax)τ(1 + 2γ1⊤fb−τ−1) + (γ2ρmax)b1⊤r0 (i)
= b−1
X"
REFERENCES,0.4614678899082569,"τ=0
(γ2ρmax)τ(1 + 2γ1⊤fb−τ−1) + (γ2ρmax)b,
(36)"
REFERENCES,0.46238532110091746,where (i) follows from the fact that 1⊤r0 = 1.
REFERENCES,0.463302752293578,"Recall that fb = dµ,b + γP ⊤
π fb−1 and f = dµ + γP ⊤
π f. We have"
REFERENCES,0.46422018348623856,"1⊤(fτ −f) = 1⊤(dµ,τ −dµ) + γ1⊤P ⊤
π (fτ−1 −f)
(i)
= γ1⊤(fτ−1 −f)
(ii)
= γτ1⊤(f0 −f),"
REFERENCES,0.4651376146788991,"where (i) follows from the facts that dµ,τ and dµ are both probability distributions and 1⊤dµ,τ =
1⊤dµ = 1 and 1⊤P ⊤
π = 1⊤and (ii) follows from recursively applying (i)."
REFERENCES,0.46605504587155966,"Thus, we have"
REFERENCES,0.4669724770642202,"|1⊤fτ| = |(1 −γτ)1⊤f + γτ1⊤f0| ≤|(1 −γτ)1⊤f| + γτ ≤∥f∥1 + 1.
(37)"
REFERENCES,0.46788990825688076,Published as a conference paper at ICLR 2022
REFERENCES,0.4688073394495413,Substituting eq. (37) into eq. (36) yields
REFERENCES,0.46972477064220186,"1⊤rb ≤ b−1
X"
REFERENCES,0.4706422018348624,"τ=0
(γ2ρmax)τ(3 + 2γ∥f∥1) + (γ2ρmax)b."
REFERENCES,0.47155963302752296,"Under different conditions of ρmax, the term 1⊤rb is upper bounded differently as following:"
REFERENCES,0.4724770642201835,(a). γ2ρmax > 1
REFERENCES,0.47339449541284406,"1⊤rb ≤
 3 + 2γ∥f∥1"
REFERENCES,0.4743119266055046,"γ2ρmax −1 + 1

γ2bρb
max.
(38)"
REFERENCES,0.47522935779816516,(b). γ2ρmax = 1
REFERENCES,0.4761467889908257,"1⊤rb ≤(3 + 2γ∥f∥1) b + 1.
(39)"
REFERENCES,0.47706422018348627,(c). γ2ρmax < 1
REFERENCES,0.4779816513761468,1⊤rb ≤3 + 2γ∥f∥1
REFERENCES,0.47889908256880737,"1 −γ2ρmax
+ 1.
(40)"
REFERENCES,0.4798165137614679,"Substituting the above inequalities into eq. (31), we can upper-bound the term E
bTt(θt)

2 2 Ft−1 "
REFERENCES,0.48073394495412847,under different conditions accordingly:
REFERENCES,0.481651376146789,(a). γ2ρmax > 1
REFERENCES,0.48256880733944957,"E
bTt(θt)

2 2 Ft−1"
REFERENCES,0.4834862385321101,"
≤ρmax
 
4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ"
REFERENCES,0.48440366972477067, 3 + 2γ∥f∥1
REFERENCES,0.4853211009174312,"γ2ρmax −1 + 1

γ2bρb
max"
REFERENCES,0.48623853211009177,"= Cσ,1γ2bρb
max,"
REFERENCES,0.4871559633027523,"where we specify Cσ,1 = ρmax

4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ

3+2γ∥f∥1
γ2ρmax−1 + 1

."
REFERENCES,0.48807339449541287,(b). γ2ρmax = 1
REFERENCES,0.4889908256880734,"E
bTt(θt)

2 2 Ft−1"
REFERENCES,0.48990825688073397,"
≤ρmax
 
4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ ((3 + 2γ∥f∥1) b + 1)"
REFERENCES,0.4908256880733945,"= Cσ,2b,"
REFERENCES,0.4917431192660551,"where we specify Cσ,2 = ρmax

4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ (4 + 2γ∥f∥1)."
REFERENCES,0.4926605504587156,(c). γ2ρmax < 1
REFERENCES,0.4935779816513762,"E
bTt(θt)

2 2 Ft−1"
REFERENCES,0.4944954128440367,"
≤ρmax
 
4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ"
REFERENCES,0.4954128440366973, 3 + 2γ∥f∥1
REFERENCES,0.4963302752293578,"1 −γ2ρmax
+ 1
"
REFERENCES,0.4972477064220184,":= Cσ,3."
REFERENCES,0.4981651376146789,"where we specify Cσ,3 = ρmax

4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ

3+2γ∥f∥1
1−γ2ρmax + 1

."
REFERENCES,0.4990825688073395,"To summarize, the variance term
bTt(θt)

2"
CAN BE BOUNDED AS FOLLOWING,0.5,2 can be bounded as following
CAN BE BOUNDED AS FOLLOWING,0.5009174311926605,"E
bTt(θt)

2 2 Ft−1"
CAN BE BOUNDED AS FOLLOWING,0.501834862385321,"
≤σ2, where σ2 = 

 
"
CAN BE BOUNDED AS FOLLOWING,0.5027522935779817,"O(1),
if γ2ρmax < 1."
CAN BE BOUNDED AS FOLLOWING,0.5036697247706422,"O(b),
if γ2ρmax = 1."
CAN BE BOUNDED AS FOLLOWING,0.5045871559633027,"O((γ2ρmax)b),
if γ2ρmax > 1."
CAN BE BOUNDED AS FOLLOWING,0.5055045871559632,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.5064220183486239,"D.3
PROOF OF THEOREM 1"
CAN BE BOUNDED AS FOLLOWING,0.5073394495412844,"Theorem 3 (Formal Statement of Theorem 1). Suppose Assumptions 1 and 2 hold.
Consider
PER-ETD(0) speciﬁed in Algorithm 1.
Let the stepsize ηt =
2
µ0(t+t0), where t0 =
8L2
0
µ2
0 , µ0
is deﬁned in Lemma 7, and L0 is deﬁned in Lemma 8 in Appendix C. Let the projection set
Θ =

θ ∈Rd : ∥θ∥2 ≤Bθ
	
, where Bθ =
∥Φ⊤∥2rmax"
CAN BE BOUNDED AS FOLLOWING,0.5082568807339449,"(1−γ)µ0
(which implies θ∗∈Θ). Then the con-
vergence guarantee falls into the following two cases depending on the value of ρmax."
CAN BE BOUNDED AS FOLLOWING,0.5091743119266054,"(a) If γ2ρmax ≤1, let b = max
nl
log(µ0)−log(5CbBφ)"
CAN BE BOUNDED AS FOLLOWING,0.5100917431192661,"log(ξ)
m
,
log T
log(1/ξ)
o
, where Cb is a constant deﬁned"
CAN BE BOUNDED AS FOLLOWING,0.5110091743119266,"in the proof of Proposition 1 in Appendix D.1, Bφ := maxs∈S ∥φ(s)∥2, and ξ := max{γ, χ}. Then
the output θT satisﬁes"
CAN BE BOUNDED AS FOLLOWING,0.5119266055045871,"E

∥θT −θ∗∥2
2

≤˜O
 1 T 
."
CAN BE BOUNDED AS FOLLOWING,0.5128440366972477,"(b) If γ2ρmax > 1, let b = max
nl
log(µ0)−log(5CbBφ)"
CAN BE BOUNDED AS FOLLOWING,0.5137614678899083,"log(ξ)
m
,
log(T )
log(γ2ρmax)+log(1/ξ)
o
, where Cb is a
constant whose deﬁnition could be found in the proof of Proposition 1 in Appendix D.1, Bφ :=
maxs∈S ∥φ(s)∥2, and ξ := max{γ, χ}. Then the output θT satisﬁes"
CAN BE BOUNDED AS FOLLOWING,0.5146788990825688,"E

∥θT −θ∗∥2
2

≤O
 1 T a 
,"
CAN BE BOUNDED AS FOLLOWING,0.5155963302752293,"where a =
1
log1/ξ(γ2ρmax)+1 < 1."
CAN BE BOUNDED AS FOLLOWING,0.5165137614678899,"Thus, PER-ETD(0) attains an ϵ-accurate solution with ˜O
  1"
CAN BE BOUNDED AS FOLLOWING,0.5174311926605505,"ϵ

samples if γ2ρmax ≤1, and with
˜O
 
1
ϵ1/a

samples if γ2ρmax > 1."
CAN BE BOUNDED AS FOLLOWING,0.518348623853211,"Proof. Note the θ update speciﬁed in Algorithm 1 is the closed form solution of the following
maximization problem."
CAN BE BOUNDED AS FOLLOWING,0.5192660550458715,"θt+1 = argmax
θ∈Θ
ηt
D
bTt(θt), θ
E
+ 1"
CAN BE BOUNDED AS FOLLOWING,0.5201834862385321,"2∥θ −θt∥2
2."
CAN BE BOUNDED AS FOLLOWING,0.5211009174311927,"Applying Lemma 9 with θ∗= θt+1, η = ηt, G = bTt(θt), and θ0 = θt yields, for any θ ∈Θ,"
CAN BE BOUNDED AS FOLLOWING,0.5220183486238532,"ηt
D
bTt(θt), θt+1 −θ
E
+ 1"
CAN BE BOUNDED AS FOLLOWING,0.5229357798165137,"2∥θt −θt+1∥2
2 ≤1"
CAN BE BOUNDED AS FOLLOWING,0.5238532110091743,"2∥θt −θ∥2
2 −1"
CAN BE BOUNDED AS FOLLOWING,0.5247706422018349,"2∥θt+1 −θ∥2
2.
(41)"
CAN BE BOUNDED AS FOLLOWING,0.5256880733944954,"Proceed with the ﬁrst term in the above inequality as follows
D
bTt(θt), θt+1 −θ
E"
CAN BE BOUNDED AS FOLLOWING,0.5266055045871559,"= ⟨T (θt+1), θt+1 −θ⟩+ ⟨T (θt) −T (θt+1), θt+1 −θ⟩+
D
bTt(θt) −T (θt), θt+1 −θ
E"
CAN BE BOUNDED AS FOLLOWING,0.5275229357798165,"(i)
≥⟨T (θt+1), θt+1 −θ⟩−L0∥θt −θt+1∥2∥θt+1 −θ∥2 +
D
bTt(θt) −T (θt), θt+1 −θ
E"
CAN BE BOUNDED AS FOLLOWING,0.5284403669724771,"= ⟨T (θt+1), θt+1 −θ⟩−L0∥θt −θt+1∥2∥θt+1 −θ∥2 +
D
bTt(θt) −T (θt), θt+1 −θt
E"
CAN BE BOUNDED AS FOLLOWING,0.5293577981651376,"+
D
bTt(θt) −T (θt), θt −θ
E"
CAN BE BOUNDED AS FOLLOWING,0.5302752293577981,"≥⟨T (θt+1), θt+1 −θ⟩−L0∥θt −θt+1∥2∥θt+1 −θ∥2 −
bTt(θt) −T (θt)

2 · ∥θt+1 −θt∥2"
CAN BE BOUNDED AS FOLLOWING,0.5311926605504587,"+
D
bTt(θt) −T (θt), θt −θ
E
,"
CAN BE BOUNDED AS FOLLOWING,0.5321100917431193,where (i) follows from the Cauchy-Schwartz inequality and Lemma 8.
CAN BE BOUNDED AS FOLLOWING,0.5330275229357798,Substituting the above inequality into eq. (41) yields
CAN BE BOUNDED AS FOLLOWING,0.5339449541284403,"ηt ⟨T (θt+1), θt+1 −θ⟩−ηtL0∥θt −θt+1∥2∥θt+1 −θ∥2 −ηt
bTt(θt) −T (θt)

2 · ∥θt+1 −θt∥2"
CAN BE BOUNDED AS FOLLOWING,0.5348623853211009,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.5357798165137615,"+ ηt
D
bTt(θt) −T (θt), θt −θ
E
+ 1"
CAN BE BOUNDED AS FOLLOWING,0.536697247706422,"2∥θt −θt+1∥2
2 ≤1"
CAN BE BOUNDED AS FOLLOWING,0.5376146788990825,"2∥θt −θ∥2
2 −1"
CAN BE BOUNDED AS FOLLOWING,0.5385321100917431,"2∥θt+1 −θ∥2
2.
(42)"
CAN BE BOUNDED AS FOLLOWING,0.5394495412844037,"Applying Young’s inequality to ηt
bTt(θt) −T (θt)

2 · ∥θt+1 −θt∥2 yields"
CAN BE BOUNDED AS FOLLOWING,0.5403669724770642,"η
bTt(θt) −T (θt)

2 · ∥θt+1 −θt∥2 ≤1"
CAN BE BOUNDED AS FOLLOWING,0.5412844036697247,"4∥θt+1 −θt∥2
2 + η2
t
bTt(θt) −T (θt)

2 2 ,"
CAN BE BOUNDED AS FOLLOWING,0.5422018348623853,and applying Young’s inequality to ηtL0∥θt −θt+1∥2∥θt+1 −θ∥2 yields
CAN BE BOUNDED AS FOLLOWING,0.5431192660550459,ηtL0∥θt −θt+1∥2∥θt+1 −θ∥2 ≤1
CAN BE BOUNDED AS FOLLOWING,0.5440366972477064,"4∥θt −θt+1∥2
2 + η2
t L2
0∥θt+1 −θ∥2
2."
CAN BE BOUNDED AS FOLLOWING,0.544954128440367,Substituting the above two inequalities into eq. (42) yields
CAN BE BOUNDED AS FOLLOWING,0.5458715596330275,"1
2∥θt −θ∥2
2 ≥ηt ⟨T (θt+1), θt+1 −θ⟩+
1"
CAN BE BOUNDED AS FOLLOWING,0.5467889908256881,"2 −η2
t L2
0"
CAN BE BOUNDED AS FOLLOWING,0.5477064220183486,"
∥θt+1 −θ∥2
2"
CAN BE BOUNDED AS FOLLOWING,0.5486238532110091,"+ ηt
D
bTt(θt) −T (θt), θt −θ
E
−η2
t
bTt(θt) −T (θt)

2 2 ."
CAN BE BOUNDED AS FOLLOWING,0.5495412844036697,"Taking expectation conditioned on Ft−1 on the both sides of the above inequality, we obtain"
CAN BE BOUNDED AS FOLLOWING,0.5504587155963303,"1
2∥θt −θ∥2
2 ≥ηtE [⟨T (θt+1), θt+1 −θ⟩|Ft−1] +
1"
CAN BE BOUNDED AS FOLLOWING,0.5513761467889908,"2 −η2
t L2
0"
CAN BE BOUNDED AS FOLLOWING,0.5522935779816514,"
E

∥θt+1 −θ∥2
2
Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.5532110091743119,"+ ηt
D
E
h
bTt(θt) −T (θt)
Ft−1
i
, θt −θ
E
−η2
t E
bTt(θt) −T (θt)

2 2 Ft−1 
. (43)"
CAN BE BOUNDED AS FOLLOWING,0.5541284403669725,Letting θ = θ∗and applying Lemma 7 to eq. (43) yields
CAN BE BOUNDED AS FOLLOWING,0.555045871559633,"1
2∥θt −θ∗∥2
2 ≥
1"
CAN BE BOUNDED AS FOLLOWING,0.5559633027522936,"2 + µ0ηt −η2
t L2
0"
CAN BE BOUNDED AS FOLLOWING,0.5568807339449541,"
E

∥θt+1 −θ∗∥2
2
Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.5577981651376147,"−ηtCbBφξb ∥θt −θ∗∥2
2 −ηtCbϵapproxξb∥θt −θ∗∥2"
CAN BE BOUNDED AS FOLLOWING,0.5587155963302752,"−η2
t E
bTt(θt) −T (θt)

2 2 Ft−1 "
CAN BE BOUNDED AS FOLLOWING,0.5596330275229358,"(i)
≥
1"
CAN BE BOUNDED AS FOLLOWING,0.5605504587155963,"2 + µ0ηt −η2
t L2
0"
CAN BE BOUNDED AS FOLLOWING,0.5614678899082569,"
E

∥θt+1 −θ∗∥2
2
Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.5623853211009174,"−ηtCbBφξb ∥θt −θ∗∥2
2 −ηtCbϵapproxξb∥θt −θ∗∥2 −4η2
t C2
b B2
φξ2b∥θt −θ∗∥2
2
−4η2
t C2
b ξ2bϵ2
approx −2σ2η2
t ,
(44)"
CAN BE BOUNDED AS FOLLOWING,0.563302752293578,"where (i) follows from Propositions 1 and 2, and the facts that (x + y)2 ≤2x2 + 2y2 and
bTt(θt) −T (θt)

2"
CAN BE BOUNDED AS FOLLOWING,0.5642201834862385,"2 ≤2
E
h
bTt(θt)
Ft−1
i
−T (θt)

2"
CAN BE BOUNDED AS FOLLOWING,0.5651376146788991,"2 + 2
E
h
bTt(θt)
Ft−1
i
−bTt(θt)

2 2"
CAN BE BOUNDED AS FOLLOWING,0.5660550458715596,"≤2
bT (θt)

2"
CAN BE BOUNDED AS FOLLOWING,0.5669724770642202,"2 + 2
E
h
bTt(θt)
Ft−1
i
−bTt(θt)

2 2 ."
CAN BE BOUNDED AS FOLLOWING,0.5678899082568807,"Taking expectation on both sides of the above inequality yields
1"
CAN BE BOUNDED AS FOLLOWING,0.5688073394495413,"2 + µ0ηt −η2
t L2
0"
CAN BE BOUNDED AS FOLLOWING,0.5697247706422018,"
E

∥θt+1 −θ∗∥2
2
 ≤
1"
CAN BE BOUNDED AS FOLLOWING,0.5706422018348624,"2 + CbBφξbηt + 4C2
b B2
φξ2bη2
t"
CAN BE BOUNDED AS FOLLOWING,0.5715596330275229,"
E
h
∥θt −θ∗∥2
2
i
+ ηtCbBθϵapproxξb"
CAN BE BOUNDED AS FOLLOWING,0.5724770642201835,"+ 4η2
t C2
b ξ2bϵ2
approx + 2σ2η2
t ."
CAN BE BOUNDED AS FOLLOWING,0.573394495412844,"Recall that we set t0 = 8L2
0
µ2
0 . Let αt = (t + t0 + 1)(t + t0 + 2). Multiplying 2αt on both sides of
the above inequality and telescoping from t = 0, 1, 2, . . . , T −1 yields"
CAN BE BOUNDED AS FOLLOWING,0.5743119266055046,"T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.5752293577981651,"t=0
αt
 
1 + 2µ0ηt −2η2
t L2
0

E

∥θt+1 −θ∗∥2
2
"
CAN BE BOUNDED AS FOLLOWING,0.5761467889908257,Published as a conference paper at ICLR 2022 ≤
CAN BE BOUNDED AS FOLLOWING,0.5770642201834862,"T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.5779816513761468,"t=0
αt
 
1 + 2CbBφξbηt + 8C2
b B2
φξ2bη2
t

E
h
∥θt −θ∗∥2
2
i"
CAN BE BOUNDED AS FOLLOWING,0.5788990825688073,"+
 
4σ2 + 8C2
b ξ2bϵ2
approx
 T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.5798165137614679,"t=0
αtη2
t + 2CbBθϵapproxξb
T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.5807339449541284,"t=0
αtηt.
(45)"
CAN BE BOUNDED AS FOLLOWING,0.581651376146789,"Recall the setting of ηt, we have"
CAN BE BOUNDED AS FOLLOWING,0.5825688073394495,"1 + 2µ0ηt −2η2
t L2
0 = 1 + 3µ0ηt 2 4"
CAN BE BOUNDED AS FOLLOWING,0.5834862385321101,"3 −
4
3µ0
ηtL2
0 "
CAN BE BOUNDED AS FOLLOWING,0.5844036697247706,= 1 + 3µ0ηt 2
CAN BE BOUNDED AS FOLLOWING,0.5853211009174312,"
1 + 1 3"
CAN BE BOUNDED AS FOLLOWING,0.5862385321100917,"
1 −4"
CAN BE BOUNDED AS FOLLOWING,0.5871559633027523,"µ0
ηtL2
0 "
CAN BE BOUNDED AS FOLLOWING,0.5880733944954128,"(i)
≥1 + 3µ0ηt 2
,"
CAN BE BOUNDED AS FOLLOWING,0.5889908256880734,"where (i) follows from the fact that 1 −4ηtL2
0
µ0
≥1 −8L2
0
µ2
0t0 ≥0. Multiplying αt on both sides of the
above inequality yields"
CAN BE BOUNDED AS FOLLOWING,0.5899082568807339,"αt(1 + 2µ0ηt −2η2
t L2
0) ≥(t + t0 + 1)(t + t0 + 2)

1 + 3µ0"
CAN BE BOUNDED AS FOLLOWING,0.5908256880733945,"2
2
µ0(t + t0) "
CAN BE BOUNDED AS FOLLOWING,0.591743119266055,"= (t + t0 + 1)(t + t0 + 2)(t + t0 + 3)/(t + t0).
(46)"
CAN BE BOUNDED AS FOLLOWING,0.5926605504587156,"Under appropriate value of b, we have CbBφξb ≤µ0"
CAN BE BOUNDED AS FOLLOWING,0.5935779816513761,5 . Which implies that
CAN BE BOUNDED AS FOLLOWING,0.5944954128440367,"2CbBφξbηt + 8C2
b B2
φξ2bη2
t = µ0ηt"
CAN BE BOUNDED AS FOLLOWING,0.5954128440366973,"2
+ µ0ηt 2"
CAN BE BOUNDED AS FOLLOWING,0.5963302752293578,4CbBφξb
CAN BE BOUNDED AS FOLLOWING,0.5972477064220183,"µ0
+
16C2
b B2
φξ2bηt
µ0
−1 ! ≤µ0ηt"
CAN BE BOUNDED AS FOLLOWING,0.5981651376146789,"2
+ µ0ηt 2 4"
CAN BE BOUNDED AS FOLLOWING,0.5990825688073395,5 + 16µ0ηt
CAN BE BOUNDED AS FOLLOWING,0.6,"25
−1
 ≤µ0ηt"
CAN BE BOUNDED AS FOLLOWING,0.6009174311926605,"2
+ µ0ηt 2"
CAN BE BOUNDED AS FOLLOWING,0.6018348623853211," 4µ2
0
25L2
0
−1 5  ≤µ0ηt 2
."
CAN BE BOUNDED AS FOLLOWING,0.6027522935779817,Multiplying αt+1 on both sides of the above inequality yields
CAN BE BOUNDED AS FOLLOWING,0.6036697247706422,"αt+1(1 + 2CbBφξbηt+1 + 8C2
b B2
φξ2bη2
t+1) ≤αt+1

1 + µ0ηt+1 2 "
CAN BE BOUNDED AS FOLLOWING,0.6045871559633027,"≤αt+1

1 + µ0ηt+1 2 "
CAN BE BOUNDED AS FOLLOWING,0.6055045871559633,"= (t + t0 + 2)(t + t0 + 3)

1 + µ0"
CAN BE BOUNDED AS FOLLOWING,0.6064220183486239,"2
2
µ0(t + t0 + 1) "
CAN BE BOUNDED AS FOLLOWING,0.6073394495412844,"= (t + t0 + 2)2(t + t0 + 3)/(t + t0 + 1).
(47)"
CAN BE BOUNDED AS FOLLOWING,0.6082568807339449,Equations (46) and (47) together imply that
CAN BE BOUNDED AS FOLLOWING,0.6091743119266055,"αt(1 + 2µ0ηt −2η2
t L2
0) −αt+1(1 + 2CbBφξbηt + 8C2
b B2
φξ2bη2
t )"
CAN BE BOUNDED AS FOLLOWING,0.6100917431192661,≥(t + t0 + 1)(t + t0 + 2)(t + t0 + 3)
CAN BE BOUNDED AS FOLLOWING,0.6110091743119266,"t + t0
−(t + t0 + 2)2(t + t0 + 3)"
CAN BE BOUNDED AS FOLLOWING,0.6119266055045871,t + t0 + 1
CAN BE BOUNDED AS FOLLOWING,0.6128440366972477,= (t + t0 + 2)(t + t0 + 3)
CAN BE BOUNDED AS FOLLOWING,0.6137614678899083,"(t + t0)(t + t0 + 1)
 
(t + t0 + 1)2 −(t + t0)(t + t0 + 2)
"
CAN BE BOUNDED AS FOLLOWING,0.6146788990825688,= (t + t0 + 2)(t + t0 + 3)
CAN BE BOUNDED AS FOLLOWING,0.6155963302752293,"(t + t0)(t + t0 + 1)
> 0."
CAN BE BOUNDED AS FOLLOWING,0.6165137614678899,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.6174311926605505,"The above inequality shows that the ∥θt −θ∗∥2
2, t = 1, . . . , T −1, terms on both sides of eq. (45)
can be canceled, which indicates the following"
CAN BE BOUNDED AS FOLLOWING,0.618348623853211,"(T + t0)(T + t0 + 1)
 
1 + 2µ0ηT −1 −2η2
T −1L2
0

E

∥θT −θ∗∥2
2
"
CAN BE BOUNDED AS FOLLOWING,0.6192660550458715,"≤(t0 + 1)(t0 + 2)
 
1 + 2CbBφξbη0 + 8C2
b B2
φξ2bη2
0

∥θ0 −θ∗∥2
2"
CAN BE BOUNDED AS FOLLOWING,0.6201834862385321,"+
 
4σ2 + 8C2
b ξ2bϵ2
approx
 T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.6211009174311927,"t=0
αtη2
t + 2CbBθϵapproxξb
T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.6220183486238532,"t=0
αtηt.
(48)"
CAN BE BOUNDED AS FOLLOWING,0.6229357798165137,"Note that PT −1
t=0 αtη2
t ≤PT −1
t=0
6
µ2
0 ≤6T"
CAN BE BOUNDED AS FOLLOWING,0.6238532110091743,"µ2
0 , 1 + 2µ0ηT −1 −2η2
T −1L2
0 ≥1, and"
CAN BE BOUNDED AS FOLLOWING,0.6247706422018349,"T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.6256880733944954,"t=0
αtηt ≤4 µ0"
CAN BE BOUNDED AS FOLLOWING,0.6266055045871559,"T −1
X"
CAN BE BOUNDED AS FOLLOWING,0.6275229357798165,"t=0
(t + t0 + 2) ≤2"
CAN BE BOUNDED AS FOLLOWING,0.6284403669724771,"µ0
(T + t0 + 2)2."
CAN BE BOUNDED AS FOLLOWING,0.6293577981651376,"Dividing (T + t0)(T + t0 + 1)
 
1 + 2µ0ηT −1 −2η2
T −1L2
0

on both sides of eq. (48) yields"
CAN BE BOUNDED AS FOLLOWING,0.6302752293577981,"E

∥θT −θ∗∥2
2
"
CAN BE BOUNDED AS FOLLOWING,0.6311926605504588,"≤
(t0 + 1)(t0 + 2)
(T + t0)(T + t0 + 1)"
CAN BE BOUNDED AS FOLLOWING,0.6321100917431193,"
1 + µ0η0 2"
CAN BE BOUNDED AS FOLLOWING,0.6330275229357798,"
∥θ0 −θ∗∥2
2"
CAN BE BOUNDED AS FOLLOWING,0.6339449541284403,"+ 24σ2 + 48C2
b ξ2bϵ2
approx
µ2
0"
CAN BE BOUNDED AS FOLLOWING,0.634862385321101,"1
T + t0 + 1 + 4CbBθϵapproxξb µ0"
CAN BE BOUNDED AS FOLLOWING,0.6357798165137615,(T + t0 + 2)2
CAN BE BOUNDED AS FOLLOWING,0.636697247706422,(T + t0 + 1)(T + t0)
CAN BE BOUNDED AS FOLLOWING,0.6376146788990825,"= O
∥θ0 −θ2∥2
2
T 2"
CAN BE BOUNDED AS FOLLOWING,0.6385321100917432,"
+ O
σ2 T"
CAN BE BOUNDED AS FOLLOWING,0.6394495412844037,"
+ O
C2
b ξ2b T"
CAN BE BOUNDED AS FOLLOWING,0.6403669724770642,"
+ O
 
Cbξb
.
(49)"
CAN BE BOUNDED AS FOLLOWING,0.6412844036697247,"Based on different conditions of σ2, we pick different b and the convergence rate is as follows."
CAN BE BOUNDED AS FOLLOWING,0.6422018348623854,"(a). γ2ρmax ≤1, Proposition 2 show that σ2 ≤O(b). We specify"
CAN BE BOUNDED AS FOLLOWING,0.6431192660550459,"b = max
l
log(µ0)−log(5CbBφ)"
CAN BE BOUNDED AS FOLLOWING,0.6440366972477064,"log(ξ)
m
,
log T
log(1/ξ)"
CAN BE BOUNDED AS FOLLOWING,0.6449541284403669,"
≤O(log(T))."
CAN BE BOUNDED AS FOLLOWING,0.6458715596330276,"Equation (49) yields,"
CAN BE BOUNDED AS FOLLOWING,0.6467889908256881,"E

∥θT −θ∗∥2
2

= O
∥θ0 −θ2∥2
2
T 2"
CAN BE BOUNDED AS FOLLOWING,0.6477064220183486,"
+ O
log(T) T"
CAN BE BOUNDED AS FOLLOWING,0.6486238532110091,"
+ O
 1 T 2"
CAN BE BOUNDED AS FOLLOWING,0.6495412844036698,"
+ O
 1 T"
CAN BE BOUNDED AS FOLLOWING,0.6504587155963303,"
= ˜O
 1 T 
."
CAN BE BOUNDED AS FOLLOWING,0.6513761467889908,"(b). γ2ρmax > 1, Proposition 2 show that σ2 = O
 
(γ2ρmax)b
. We specify"
CAN BE BOUNDED AS FOLLOWING,0.6522935779816513,"b = max
nl
log(µ0)−log(5CbBφ)"
CAN BE BOUNDED AS FOLLOWING,0.653211009174312,"log(ξ)
m
,
log(T )
log(γ2ρmax)+log(1/ξ)
o
."
CAN BE BOUNDED AS FOLLOWING,0.6541284403669725,Equation (49) yields
CAN BE BOUNDED AS FOLLOWING,0.655045871559633,"E

∥θT −θ∗∥2
2

= O
∥θ0 −θ2∥2
2
T 2"
CAN BE BOUNDED AS FOLLOWING,0.6559633027522935,"
+ O
T 1−a T"
CAN BE BOUNDED AS FOLLOWING,0.6568807339449542,"
+ O
 C2
b
T 1+a"
CAN BE BOUNDED AS FOLLOWING,0.6577981651376147,"
+ O
 Cb T a"
CAN BE BOUNDED AS FOLLOWING,0.6587155963302752,"
= O
 1 T a 
."
CAN BE BOUNDED AS FOLLOWING,0.6596330275229357,"E
PROOFS OF PROPOSITIONS AND THEOREM FOR PER-ETD(λ)"
CAN BE BOUNDED AS FOLLOWING,0.6605504587155964,"E.1
PROOF OF PROPOSITION 3"
CAN BE BOUNDED AS FOLLOWING,0.6614678899082569,"Deﬁne the matrix At := ρb
teb
t(φb
t −γφb+1
t
) and ct := rb
tρb
teb
t. We have"
CAN BE BOUNDED AS FOLLOWING,0.6623853211009174,"bT λ
t (θt) = Atθt −ct.
(50)"
CAN BE BOUNDED AS FOLLOWING,0.6633027522935779,Recall that θt is Ft−1-measurable. We have
CAN BE BOUNDED AS FOLLOWING,0.6642201834862386,"E
h
bT λ
t (θt)
Ft−1
i
= E [At|Ft−1] θt −E [ct|Ft−1] ."
CAN BE BOUNDED AS FOLLOWING,0.6651376146788991,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.6660550458715596,"To bound the bias error term
E
h
bT λ
t (θt)
Ft−1
i
−T λ(θt)

2, we ﬁrst take conditional expectations
on At and ct, respectively, as following"
CAN BE BOUNDED AS FOLLOWING,0.6669724770642201,E [At|Ft−1]
CAN BE BOUNDED AS FOLLOWING,0.6678899082568808,"= E

ρb
teb
t(φb
t −γφb+1
t
)⊤Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.6688073394495413,"(i)
=
X"
CAN BE BOUNDED AS FOLLOWING,0.6697247706422018,"s∈S,a∈A,s′∈S
P
 
sb
t = s, ab
t = a, sb+1
t
= s′Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.6706422018348623,"· E

ρb
teb
t(φb
t −γφb+1
t
)⊤Ft−1, sb
t = s, ab
t = a, sb+1
t
= s′"
CAN BE BOUNDED AS FOLLOWING,0.671559633027523,"(ii)
=
X"
CAN BE BOUNDED AS FOLLOWING,0.6724770642201835,"s,a,s′
P
 
sb
t = s
Ft−1

µ(a|s)P(s′|s, a) · π(a|s)"
CAN BE BOUNDED AS FOLLOWING,0.673394495412844,"µ(a|s)E

eb
t
sb
t = s, Ft−1

(φ(s) −γφ(s′))⊤ =
X"
CAN BE BOUNDED AS FOLLOWING,0.6743119266055045,"s∈S
P
 
sb
t = s
Ft−1

E

eb
t
Ft−1, sb
t = s

X"
CAN BE BOUNDED AS FOLLOWING,0.6752293577981652,"a∈A,s′∈S
π(a|s)P(s′|s, a) (φ(s) −γφ(s′))⊤ =
X"
CAN BE BOUNDED AS FOLLOWING,0.6761467889908257,"s∈S
P
 
sb
t = s
Ft−1

E

eb
t
Ft−1, sb
t = s

·
 
(Φ)(s,·) −γ(PπΦ)(s,·)

,
(51)"
CAN BE BOUNDED AS FOLLOWING,0.6770642201834862,"where (i) follows from the law of total probability and (ii) follows from the Markov property and
the fact that eb
t only depends on (s0
t, a0
t, s1
t, . . . , sb
t)."
CAN BE BOUNDED AS FOLLOWING,0.6779816513761467,"Deﬁne βτ(s) = P (sτ
t = s|Ft−1) E

eb
t
Ft−1, sτ
t = s

. We have"
CAN BE BOUNDED AS FOLLOWING,0.6788990825688074,"βb(s) = P
 
sb
t = s
Ft−1

E

eb
t
sb
t = s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.6798165137614679,"(i)
= P
 
sb
t = s
Ft−1
 ·
X"
CAN BE BOUNDED AS FOLLOWING,0.6807339449541284,"˜s∈S,˜a∈A
P
 
sb−1
t
= ˜s, ab−1
t
= ˜a
sb
t = s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.681651376146789,"· E

γλρb−1
t
eb−1
t
+ (λ + (1 −λ)(1 + ρb−1
t
γF b−1
t
)φb
t)
sb−1
t
= ˜s, ab−1
t
= ˜a, sb
t = s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.6825688073394496,"(ii)
= P
 
sb
t = s
Ft−1

φ(s)"
CAN BE BOUNDED AS FOLLOWING,0.6834862385321101,"+ P
 
sb
t = s
Ft−1

X"
CAN BE BOUNDED AS FOLLOWING,0.6844036697247706,"˜s∈S,˜a∈A"
CAN BE BOUNDED AS FOLLOWING,0.6853211009174311,"P
 
sb−1
t
= ˜s
Ft−1

µ(˜a|˜s)P(s|˜s, ˜a)
P
 
sb
t = s
Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.6862385321100918,· π(˜a|˜s)
CAN BE BOUNDED AS FOLLOWING,0.6871559633027523,"µ(˜a|˜s) · E

γλeb−1
t
+ (1 −λ)γF b−1
t
φ(s)
sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.6880733944954128,"= P
 
sb
t = s
Ft−1

φ(s) +
X"
CAN BE BOUNDED AS FOLLOWING,0.6889908256880733,"˜s∈S
P
 
sb−1
t
= ˜s
Ft−1

Pπ(s|˜s)"
CAN BE BOUNDED AS FOLLOWING,0.689908256880734,"· E

γλeb−1
t
+ (1 −λ)γF b−1
t
φ(s)
sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.6908256880733945,"(iii)
= (λdµ,b(s) + (1 −λ)fb(s)) · φ(s) + γλ(P ⊤
π βb−1)s,
(52)"
CAN BE BOUNDED AS FOLLOWING,0.691743119266055,"where (i) follows from the law of total probability,
(ii) follows from the Bayes rule
and the Markov property, and (iii) follows from the following deﬁnitions:
dµ,b(s)
=
P
 
sb
t = s|Ft−1

, fb(s) = dµ,b(s)E

F b
t = s|sb
t = s, Ft−1

, fb = dµ,b + γP ⊤
π fb−1, and βτ(s) =
P (sτ
t = s|Ft−1) E

eb
t
Ft−1, sτ
t = s

."
CAN BE BOUNDED AS FOLLOWING,0.6926605504587156,"Deﬁne the matrix βτ ∈Rd×|S|, where βτ = (βτ(1), βτ(2), . . . , βτ(|S|)). Then, eq. (52) implies
that"
CAN BE BOUNDED AS FOLLOWING,0.6935779816513762,"βb = λΦ⊤Dµ,b + (1 −λ)Φ⊤Fb + γλβb−1Pπ,
(53)"
CAN BE BOUNDED AS FOLLOWING,0.6944954128440367,"where Dµ,b := diag(dµ,b(1), dµ,b(2), . . . , dµ,b(|S|)) and Fb = diag(fb)."
CAN BE BOUNDED AS FOLLOWING,0.6954128440366972,Recursively applying the above equality yields
CAN BE BOUNDED AS FOLLOWING,0.6963302752293578,"βb = (γλ)bβ0P b
π + λ b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.6972477064220184,"τ=0
(γλ)τΦ⊤Dµ,b−τP τ
π + (1 −λ) b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.6981651376146789,"τ=0
(γλ)τΦ⊤Fb−τP τ
π .
(54)"
CAN BE BOUNDED AS FOLLOWING,0.6990825688073394,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.7,"Taking expectation of ct conditioned on Ft−1, we have"
CAN BE BOUNDED AS FOLLOWING,0.7009174311926606,"E [ct|Ft−1] = E

ρb
teb
trb
t
Ft−1
 =
X"
CAN BE BOUNDED AS FOLLOWING,0.7018348623853211,"s∈S,a∈A
P
 
sb
t = s, ab
t = a
Ft−1

E

ρb
teb
trb
t
sb
t = s, ab
t = a, Ft−1
 =
X"
CAN BE BOUNDED AS FOLLOWING,0.7027522935779816,"s∈S,a∈A
P
 
sb
t = s
Ft−1

µ(a|s) · π(a|s)"
CAN BE BOUNDED AS FOLLOWING,0.7036697247706422,"µ(a|s)r(s, a)E

eb
t
sb
t = s, Ft−1
 =
X"
CAN BE BOUNDED AS FOLLOWING,0.7045871559633028,"s∈S
rπ(s)P
 
sb
t = s
Ft−1

E

eb
t
sb
t = s, Ft−1
 =
X"
CAN BE BOUNDED AS FOLLOWING,0.7055045871559633,"s∈S
rπ(s)βb(s).
(55)"
CAN BE BOUNDED AS FOLLOWING,0.7064220183486238,Substituting eqs. (51) and (55) into eq. (50) yields
CAN BE BOUNDED AS FOLLOWING,0.7073394495412844,"E
h
bT λ
t (θt)
Ft−1
i
=
X"
CAN BE BOUNDED AS FOLLOWING,0.708256880733945,"s∈S
βb(s) (Φθt −γPπΦθt −rπ)s = βb (Φθt −γPπΦθt −rπ) ."
CAN BE BOUNDED AS FOLLOWING,0.7091743119266055,Recall the deﬁnition of T λ(θ). We have
CAN BE BOUNDED AS FOLLOWING,0.710091743119266,"T λ(θt) −E
h
bT λ
t (θt)
Ft−1
i
=
 
Φ⊤M(I −γλPπ)−1 −βb

(Φθt −γPπΦθt −rπ) .
(56)"
CAN BE BOUNDED AS FOLLOWING,0.7110091743119266,We then proceed to bound the term Φ⊤M(I −γλPπ)−1 −βb
CAN BE BOUNDED AS FOLLOWING,0.7119266055045872,Φ⊤M(I −γλPπ)−1 −βb
CAN BE BOUNDED AS FOLLOWING,0.7128440366972477,"(i)
= Φ⊤M ∞
X"
CAN BE BOUNDED AS FOLLOWING,0.7137614678899082,"τ=0
(γλ)τP τ
π ! −βb"
CAN BE BOUNDED AS FOLLOWING,0.7146788990825688,"(ii)
= Φ⊤(λDµ + (1 −λ)F) ∞
X"
CAN BE BOUNDED AS FOLLOWING,0.7155963302752294,"τ=0
(γλ)τP τ
π ! − "
CAN BE BOUNDED AS FOLLOWING,0.7165137614678899,"(γλ)bβ0P b
π + λ b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7174311926605504,"τ=0
(γλ)τΦ⊤Dµ,b−τP τ
π + (1 −λ) b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.718348623853211,"τ=0
(γλ)τΦ⊤Fb−τP τ
π ! = λ b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7192660550458716,"τ=0
(γλ)τΦ⊤(Dµ −Dµ,b−τ) P τ
π + (1 −λ) b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7201834862385321,"τ=0
(γλ)τΦ⊤(F −Fb−τ) P τ
π + ∞
X"
CAN BE BOUNDED AS FOLLOWING,0.7211009174311926,"τ=b
(γλ)τΦ⊤(λDµ + (1 −λ)F)P τ
π −λ(γλ)bΦ⊤Dµ,0P b
π,"
CAN BE BOUNDED AS FOLLOWING,0.7220183486238532,"where (i) follows from the fact that (I −γPπ)−1 = P∞
τ=0 γτP τ
π , and (ii) follows from eq. (54).
Substituting the above equality into eq. (56) and taking ℓ2 norm on the both sides yield
T λ(θt) −E
h
bT λ
t (θt)
Ft−1
i
2 =   λ b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7229357798165138,"τ=0
(γλ)τΦ⊤(Dµ −Dµ,b−τ) P τ
π + (1 −λ) b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7238532110091743,"τ=0
(γλ)τΦ⊤(F −Fb−τ) P τ
π + ∞
X"
CAN BE BOUNDED AS FOLLOWING,0.7247706422018348,"τ=b
(γλ)τΦ⊤(λDµ + (1 −λ)F)P τ
π −λ(γλ)bΦ⊤Dµ,0P b
π !"
CAN BE BOUNDED AS FOLLOWING,0.7256880733944954,"(Φθt −γPπθt −rπ) 2 ≤λ b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.726605504587156,"τ=0
(γλ)τ Φ⊤(Dµ −Dµ,b−τ) P τ
π (Φθt −γPπΦθt −rπ)

2"
CAN BE BOUNDED AS FOLLOWING,0.7275229357798165,"+ (1 −λ) b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.728440366972477,"τ=0
(γλ)τ Φ⊤(F −Fb−τ) P τ
π (Φθt −γPπΦθt −rπ)

2"
CAN BE BOUNDED AS FOLLOWING,0.7293577981651376,"Published as a conference paper at ICLR 2022 + ∞
X"
CAN BE BOUNDED AS FOLLOWING,0.7302752293577982,"τ=b
(γλ)τ Φ⊤(λDµ + (1 −λ)F) P τ
π (Φθt −γPπΦθt −rπ)

2"
CAN BE BOUNDED AS FOLLOWING,0.7311926605504587,"+ λ(γλ)b Φ⊤Dµ,0P b
π (Φθt −γPπΦθt −rπ)

2"
CAN BE BOUNDED AS FOLLOWING,0.7321100917431193,"(i)
≤λ b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7330275229357798,"τ=0
(γλ)τBφ∥dµ −dµ,b−τ∥1(1 + γ) (Bφ∥θt −θ∗
λ∥2 + ϵapprox)"
CAN BE BOUNDED AS FOLLOWING,0.7339449541284404,"+ (1 −λ) b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7348623853211009,"τ=0
(γλ)τBφ ∥f −fb−τ∥1 (1 + γ) (Bφ∥θt −θ∗
λ∥2 + Cϵapprox) + ∞
X"
CAN BE BOUNDED AS FOLLOWING,0.7357798165137615,"τ=b
(γλ)τBφ (λ∥dµ∥1 + (1 −λ)∥f∥1) (1 + γ) (Bφ∥θt −θ∗
λ∥2 + ϵapprox)"
CAN BE BOUNDED AS FOLLOWING,0.736697247706422,"+ λ(γλ)bBφ∥dµ,0∥1(1 + γ) (Bφ∥θt −θ∗
λ∥2 + ϵapprox)"
CAN BE BOUNDED AS FOLLOWING,0.7376146788990826,"(ii)
≤
 
B2
φ∥θt −θ∗
λ∥2 + Bφϵapprox
 · b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.7385321100917431,"τ=0
(γλ)τ 
λ(1 + γ)CMχb−τ + (1 −λ)(1 + γ)

CM
|γ−χ|ξb−τ + γb−τ(1 + ∥f∥1)
!"
CAN BE BOUNDED AS FOLLOWING,0.7394495412844037,"+ (1 + γ)
 
B2
φ∥θt −θ∗
λ∥2 + Bφϵapprox
 λ + (1 −λ)∥f∥1"
CAN BE BOUNDED AS FOLLOWING,0.7403669724770642,"1 −γλ
+ λ

(γλ)b
"
CAN BE BOUNDED AS FOLLOWING,0.7412844036697248,"(iii)
≤
 
B2
φ∥θt −θ∗
λ∥2 + Bφϵapprox
"
CAN BE BOUNDED AS FOLLOWING,0.7422018348623853,"· (1 + γ)

λ
|χ−γλ|CMξb +
CM(1−λ)
|γ−χ|(ξ−γλ)ξb +(1 + ∥f∥1)γb +

λ+(1−λ)∥f∥1"
CAN BE BOUNDED AS FOLLOWING,0.7431192660550459,"1−γλ
+ λ

(γλ)b"
CAN BE BOUNDED AS FOLLOWING,0.7440366972477064,"(iv)
≤Cb,λ (Bφ∥θt −θ∗
λ∥2 + ϵapprox) ξb,"
CAN BE BOUNDED AS FOLLOWING,0.744954128440367,"where (i) follows from Lemma 5 and eq. (28), (ii) follows from Lemma 1 and eq. (27), in (iii) we
deﬁne"
CAN BE BOUNDED AS FOLLOWING,0.7458715596330275,"Cb,λ := Bφ(1 + γ)

λ
|χ−γλ|CM +
CM(1−λ)
|γ−χ|(ξ−γλ) + 1 + ∥f∥1 +

λ+(1−λ)∥f∥1"
CAN BE BOUNDED AS FOLLOWING,0.7467889908256881,"1−γλ
+ λ

,"
CAN BE BOUNDED AS FOLLOWING,0.7477064220183486,and (iv) follows from Lemma 3.
CAN BE BOUNDED AS FOLLOWING,0.7486238532110092,"E.2
PROOF OF PROPOSITION 4"
CAN BE BOUNDED AS FOLLOWING,0.7495412844036697,"According to the deﬁnition of bT λ
t , we have"
CAN BE BOUNDED AS FOLLOWING,0.7504587155963303,"E
bT λ
t (θt)

2 2 Ft−1 "
CAN BE BOUNDED AS FOLLOWING,0.7513761467889908,"= E
h
(ρb
t)2  
rb
t + γθ⊤
t φb+1
t
−θ⊤
t φb
t
2 (eb
t)⊤eb
t
Ft−1
i"
CAN BE BOUNDED AS FOLLOWING,0.7522935779816514,"(i)
=
X"
CAN BE BOUNDED AS FOLLOWING,0.7532110091743119,"s∈S,a∈A,s′∈S
P
 
sb
t = s, ab
t = a, sb+1
t
= s′Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7541284403669725,"· E
h
(ρb
t)2  
rb
t + γθ⊤
t φb+1
t
−θ⊤
t φb
t
2 (eb
t)⊤eb
t
sb
t = s, ab
t = a, sb+1
t
= s′, Ft−1
i"
CAN BE BOUNDED AS FOLLOWING,0.755045871559633,"(ii)
=
X"
CAN BE BOUNDED AS FOLLOWING,0.7559633027522936,"s∈S,a∈A,s′∈S
P
 
sb
t = s
Ft−1

µ(a|s)P(s′|s, a)"
CAN BE BOUNDED AS FOLLOWING,0.7568807339449541,"π2(a|s)
µ2(a|s)(r(s, a) + γθ⊤
t φ(s′) −θ⊤
t φ(s))2E

(eb
t)⊤eb
t
sb
t = s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7577981651376147,"(iii)
≤
X"
CAN BE BOUNDED AS FOLLOWING,0.7587155963302752,"s∈S
P
 
sb
t = s
Ft−1

E

(eb
t)⊤eb
t
sb
t = s, Ft−1
 ·
X"
CAN BE BOUNDED AS FOLLOWING,0.7596330275229358,"s′∈S
Pµ,π(s′|s)(r(s, a) + γφ(s′)⊤θt −φ(s)⊤θt)2"
CAN BE BOUNDED AS FOLLOWING,0.7605504587155963,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.7614678899082569,"(iv)
≤ρmax
 
4(1 + γ2)B2
φB2
θ + 2r2
max

B2
φ
X"
CAN BE BOUNDED AS FOLLOWING,0.7623853211009174,"s∈S
P
 
sb
t = s
Ft−1

E

(eb
t)⊤eb
t
sb
t = s, Ft−1

, (57)"
CAN BE BOUNDED AS FOLLOWING,0.763302752293578,"where (i) follows from the law of total probability, (ii) follows from the Markov property and
the fact that eb
t only depends on (s0
t, a0
t, . . . , sb
t), and (iii) follows from eq. (30) and the fact
P"
CAN BE BOUNDED AS FOLLOWING,0.7642201834862385,"s′ Pµ,π(s′|s) ≤ρmax."
CAN BE BOUNDED AS FOLLOWING,0.7651376146788991,"Deﬁne ∆b(s) = P
 
sb
t = s|Ft−1

E

(eb
t)⊤eb
t
sb
t = s, Ft−1

. We then proceed to bound the term
∆b(s). We have ∆b(s)"
CAN BE BOUNDED AS FOLLOWING,0.7660550458715596,"(i)
= P
 
sb
t = s|Ft−1
 X"
CAN BE BOUNDED AS FOLLOWING,0.7669724770642202,"˜s∈S,˜a∈A
P
 
sb−1
t
= ˜s, ab−1
t
= ˜a
sb
t = s, Ft−1

E

(eb
t)⊤eb
t
sb−1
t
= ˜s, ab−1
t
= ˜a, sb
t = s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7678899082568807,"(ii)
= P
 
sb
t = s|Ft−1
 X"
CAN BE BOUNDED AS FOLLOWING,0.7688073394495413,"˜s∈S,˜a∈A"
CAN BE BOUNDED AS FOLLOWING,0.7697247706422018,"P
 
sb−1
t
= ˜s
Ft−1

µ(˜a|˜s)P(s|˜s, ˜a)
P
 
sb
t = s|Ft−1

E

(eb
t)⊤eb
t
sb−1
t
= ˜s, ab−1
t
= ˜a, sb
t = s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7706422018348624,"(iii)
=
X"
CAN BE BOUNDED AS FOLLOWING,0.771559633027523,"˜s∈S,˜a∈A
P
 
sb−1
t
= ˜s
Ft−1

µ(˜a|˜s)P(s|˜s, ˜a)"
CAN BE BOUNDED AS FOLLOWING,0.7724770642201835,"· E
h 
γλρb−1
t
eb−1
t
+
 
λ + (1 −λ)
 
γρb−1
t
F b−1
t
+ 1

φb
t
⊤"
CAN BE BOUNDED AS FOLLOWING,0.773394495412844,"·
 
γλρb−1
t
eb−1
t
+
 
λ + (1 −λ)
 
γρb−1
t
F b−1
t
+ 1

φb
t
sb−1
t
= ˜s, ab−1
t
= ˜a, sb
t = s, Ft−1
 =
X"
CAN BE BOUNDED AS FOLLOWING,0.7743119266055046,"˜s∈S,˜a∈A
P
 
sb−1
t
= ˜s
Ft−1

µ(˜a|˜s)P(s|˜s, ˜a)"
CAN BE BOUNDED AS FOLLOWING,0.7752293577981652,"E

(γλ)2(ρb−1
t
)2(eb−1
t
)⊤eb−1
t
+ (1 −λ)2γ2(ρb−1
t
F b−1
t
)2φ(s)⊤φ(s) + φ(s)⊤φ(s)"
CAN BE BOUNDED AS FOLLOWING,0.7761467889908257,"+2γ2λ(1 −λ)(ρb−1
t
)2F b−1
t
φ(s)⊤eb−1
t
+ 2γλρb−1
t
φ(s)⊤eb−1
t
+2(1 −λ)γρb−1
t
F b−1
t
φ(s)⊤φ(s)
sb−1
t
= ˜s, ab−1
t
= ˜a, sb
t = s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7770642201834862,"= P
 
sb
t = s|Ft−1

∥φ(s)∥2
2"
CAN BE BOUNDED AS FOLLOWING,0.7779816513761468,"+ φ⊤(s)
X"
CAN BE BOUNDED AS FOLLOWING,0.7788990825688074,"˜s∈S
Pπ(s|˜s)P
 
sb−1
t
= ˜s
Ft−1

E

2γλeb−1
t
+ 2γ(1 −λ)F b−1
t
φ(s)
sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7798165137614679,+ λ2γ2 X
CAN BE BOUNDED AS FOLLOWING,0.7807339449541284,"˜s∈S
Pµ,π(s|˜s)P
 
sb−1
t
= ˜s
Ft−1

E

(eb−1
t
)⊤eb−1
t
sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.781651376146789,"+ 2γ2λ(1 −λ)
X"
CAN BE BOUNDED AS FOLLOWING,0.7825688073394496,"˜s∈S
Pµ,π(s|˜s)P
 
sb−1
t
= ˜s
Ft−1

E

F b−1
t
φ(s)⊤eb−1
t
sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7834862385321101,"+ ∥φ(s)∥2
2γ2(1 −λ)2 X"
CAN BE BOUNDED AS FOLLOWING,0.7844036697247706,"˜s∈S
Pµ,π(s|˜s)P
 
sb−1
t
= ˜s
Ft−1

E

(F b−1
t
)2sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7853211009174312,"≤B2
φdµ,b(s) + 2γλφ⊤(s)(βb−1Pπ)(·,s) + 2γ(1 −λ)B2
φ(P ⊤
π fb−1)s"
CAN BE BOUNDED AS FOLLOWING,0.7862385321100918,"+ λ2γ2(P ⊤
µ,π∆b−1)s + γ2(1 −λ)2B2
φ(P ⊤
µ,πrb−1)s"
CAN BE BOUNDED AS FOLLOWING,0.7871559633027523,"+ 2γ2λ(1 −λ)
X"
CAN BE BOUNDED AS FOLLOWING,0.7880733944954128,"˜s∈S
Pµ,π(s|˜s)P
 
sb−1
t
= ˜s
Ft−1

E

F b−1
t
φ(s)⊤eb−1
t
sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.7889908256880734,"(iv)
= B2
φdµ,b(s) + λ2γ2(P ⊤
µ,π∆b−1)s + 2γ(1 −λ)B2
φ(P ⊤
π fb−1)s + 2γλφ⊤(s)(βb−1Pπ)(·,s)"
CAN BE BOUNDED AS FOLLOWING,0.789908256880734,"+ γ2(1 −λ)2B2
φ(P ⊤
µ,πrb−1)s + 2γ2λ(1 −λ)φ(s)⊤(δb−1Pµ,π)(·,s),
(58)"
CAN BE BOUNDED AS FOLLOWING,0.7908256880733945,"where (i) follows from the law of total probability, (ii) follows from the Bayes rule, (iii) follows
the update rule of eb
t and in (iv) we deﬁne"
CAN BE BOUNDED AS FOLLOWING,0.791743119266055,"δτ(s) = P (sτ
t = s|Ft−1) E [F τ
t eτ
t |sτ
t = s, Ft−1] ."
CAN BE BOUNDED AS FOLLOWING,0.7926605504587156,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.7935779816513762,Summing eq. (58) over S yields
CAN BE BOUNDED AS FOLLOWING,0.7944954128440367,"1⊤∆b =
X"
CAN BE BOUNDED AS FOLLOWING,0.7954128440366972,"s
∆b(s)"
CAN BE BOUNDED AS FOLLOWING,0.7963302752293578,"≤B2
φ1⊤dµ,b(s) + λ2γ21⊤P ⊤
µ,π∆b−1 + 2γ(1 −λ)B2
φ1⊤P ⊤
π fb−1 + 2γλtrace (Φβb−1Pπ)"
CAN BE BOUNDED AS FOLLOWING,0.7972477064220184,"+ γ2(1 −λ)2B2
φ1⊤P ⊤
µ,πrb−1 + 2γ2λ(1 −λ)trace (Φδb−1Pµ,π)"
CAN BE BOUNDED AS FOLLOWING,0.7981651376146789,"(i)
≤λ2γ2ρmax1⊤∆b−1 + B2
φ + 2γ(1 −λ)B2
φ1⊤fb−1 + 2γλtrace (Φβb−1Pπ)"
CAN BE BOUNDED AS FOLLOWING,0.7990825688073394,"+ γ2(1 −λ)2B2
φρmax1⊤rb−1 + 2γ2λ(1 −λ)trace (Φδb−1Pµ,π) ,"
CAN BE BOUNDED AS FOLLOWING,0.8,"where (i) follows from Lemma 4 with P = Pµ,π."
CAN BE BOUNDED AS FOLLOWING,0.8009174311926606,"Recursively applying the above inequality, we have"
CAN BE BOUNDED AS FOLLOWING,0.8018348623853211,"1⊤∆b ≤(λ2γ2)bρb
max1⊤∆0 + b
X"
CAN BE BOUNDED AS FOLLOWING,0.8027522935779816,"τ=1
(λ2γ2)b−τ(ρmax)b−τ  
B2
φ + 2γ(1 −λ)B2
φ1⊤fτ−1"
CAN BE BOUNDED AS FOLLOWING,0.8036697247706422,"+2γλtrace (Φβτ−1Pπ) + γ2(1 −λ)2B2
φρmax1⊤rτ−1 + 2γ2λ(1 −λ)trace (Φδτ−1Pµ,π)

.
(59)"
CAN BE BOUNDED AS FOLLOWING,0.8045871559633028,Substituting eq. (54) into trace (ΦβτPπ) with b and τ replaced by τ and m respectively yields
CAN BE BOUNDED AS FOLLOWING,0.8055045871559633,trace (ΦβτPπ)
CAN BE BOUNDED AS FOLLOWING,0.8064220183486238,"(i)
= (γλ)τtrace
 
Φβ0P τ+1
π
 + λ τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.8073394495412844,"m=0
(γλ)m  
trace
 
ΦΦ⊤Dµ,τ−mP m+1
π

+ (1 −λ)trace
 
ΦΦ⊤Fτ−mP m+1
π
"
CAN BE BOUNDED AS FOLLOWING,0.808256880733945,"(ii)
= (γλ)τtrace
 
P τ+1
π
ΦΦ⊤Dµ,0
 + λ τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.8091743119266055,"m=0
(γλ)m  
trace
 
P m+1
π
ΦΦ⊤Dµ,τ−m

+ (1 −λ)trace
 
P m+1
π
ΦΦ⊤Fτ−m
"
CAN BE BOUNDED AS FOLLOWING,0.810091743119266,"(iii)
≤(γλ)τB2
φ1⊤dµ,0 + B2
φ τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.8110091743119267,"m=0
(γλ)m (λ∥dµ,τ−m∥1 + (1 −λ)∥fτ−m∥1)"
CAN BE BOUNDED AS FOLLOWING,0.8119266055045872,"(iv)
≤(γλ)τB2
φ + 1 −(γλ)τ"
CAN BE BOUNDED AS FOLLOWING,0.8128440366972477,"1 −γλ
(λ + (1 −λ)(1 + ∥f∥1)) B2
φ"
CAN BE BOUNDED AS FOLLOWING,0.8137614678899082,"≤
B2
φ
1 −γλ (1 + (1 −λ)∥f∥1) ,
(60)"
CAN BE BOUNDED AS FOLLOWING,0.8146788990825689,"where (i) follows from eq. (54), (ii) follows from the facts that trace(AB) = trace(BA) and
δ0 = Φ⊤Dµ,0, (iii) follows from Lemma 6 with P = Pπ, and (iv) follows from eq. (37)."
CAN BE BOUNDED AS FOLLOWING,0.8155963302752294,"Next, we proceed to bound the term trace
 
Φ⊤δτ−1Pµ,π

."
CAN BE BOUNDED AS FOLLOWING,0.8165137614678899,"δb−1(˜s)
(i)
= P
 
sb−1
t
= ˜s
Ft−1

E

F b−1
t
 
γλρb−2
t
eb−2
t
+ (λ + (1 −λ)F b−1
t
)φ(˜s)
sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.8174311926605504,"(ii)
= (1 −λ)P
 
sb−1
t
= ˜s
Ft−1

E

(F b−1
t
)2sb−1
t
= ˜s, Ft−1

φ(˜s)"
CAN BE BOUNDED AS FOLLOWING,0.818348623853211,"+ λP
 
sb−1
t
= ˜s
Ft−1

E

F b−1
t
sb−1
t
= ˜s, Ft−1

φ(˜s)"
CAN BE BOUNDED AS FOLLOWING,0.8192660550458716,"+ γλP
 
sb−1
t
= ˜s
Ft−1

E

ρb−2
t
eb−2
t
sb−1
t
= ˜s, Ft−1

φ(˜s)"
CAN BE BOUNDED AS FOLLOWING,0.8201834862385321,"+ γ2λP
 
sb−1
t
= ˜s
Ft−1

E

(ρb−2
t
)2F b−2
t
eb−2
t
sb−1
t
= ˜s, Ft−1

φ(˜s)"
CAN BE BOUNDED AS FOLLOWING,0.8211009174311926,"(iii)
= (1 −λ)rb−1(˜s)φ(˜s) + λfb−1(˜s)φ(˜s)"
CAN BE BOUNDED AS FOLLOWING,0.8220183486238533,"+ P
 
sb−1
t
= ˜s
Ft−1
 X"
CAN BE BOUNDED AS FOLLOWING,0.8229357798165138,"s′′,a′′
P
 
sb−2
t
= s′′, ab−2
t
= a′′sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.8238532110091743,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.8247706422018348,"· E

γλρb−2
t
eb−2
t
+ γ2λ(ρb−2
t
)2F b−2
t
eb−2
t
sb−2
t
= s′′, ab−2
t
= a′′, sb−1
t
= ˜s, Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.8256880733944955,= (1 −λ)rb−1(˜s)φ(˜s) + λfb−1(˜s)φ(˜s)
CAN BE BOUNDED AS FOLLOWING,0.826605504587156,"+ P
 
sb−1
t
= ˜s
Ft−1
 X"
CAN BE BOUNDED AS FOLLOWING,0.8275229357798165,"s′′,a′′"
CAN BE BOUNDED AS FOLLOWING,0.828440366972477,"P
 
sb−2
t
= s′′Ft−1

µ(a′′|s′′)P(˜s|s′′, a′′)"
CAN BE BOUNDED AS FOLLOWING,0.8293577981651377,"P
 
sb−1
t
= ˜s
Ft−1
"
CAN BE BOUNDED AS FOLLOWING,0.8302752293577982,"· E

γλπ(a′′|s′′)"
CAN BE BOUNDED AS FOLLOWING,0.8311926605504587,"µ(a′′|s′′)eb−2
t
+ γ2λπ2(a′′|s′′)"
CAN BE BOUNDED AS FOLLOWING,0.8321100917431192,"µ2(a′′|s′′)F b−2
t
eb−2
t"
CAN BE BOUNDED AS FOLLOWING,0.8330275229357799,"sb−2
t
= s′′, Ft−1 "
CAN BE BOUNDED AS FOLLOWING,0.8339449541284404,"= (1 −λ)rb−1(˜s)φ(˜s) + λfb−1(˜s)φ(˜s) + γλ(βb−2Pπ)(·,s) + γ2λ(δb−2Pµ,π)(·,s),"
CAN BE BOUNDED AS FOLLOWING,0.8348623853211009,"where (i) follows from the update of eb−1
t
, (ii) follows from the update rule of F b−1
t
, and (iii)
follows from the law of total probability."
CAN BE BOUNDED AS FOLLOWING,0.8357798165137614,The above equality implies that
CAN BE BOUNDED AS FOLLOWING,0.8366972477064221,"δb−1 = (1 −λ)Φ⊤diag(rb−1) + λΦ⊤diag(fb−1) + γλβb−2Pπ + γ2λδb−2Pπ,µ."
CAN BE BOUNDED AS FOLLOWING,0.8376146788990826,Recursively applying the above equality yields
CAN BE BOUNDED AS FOLLOWING,0.8385321100917431,"δb−1 = b−1
X"
CAN BE BOUNDED AS FOLLOWING,0.8394495412844036,"m=1
(Φ⊤((1 −λ)diag(rm) + λdiag(fm)) + γλβm−1Pπ)(Pπ,µ)b−1−m"
CAN BE BOUNDED AS FOLLOWING,0.8403669724770643,"+ (γ2λ)b−1δ0(Pπ,µ)b−1."
CAN BE BOUNDED AS FOLLOWING,0.8412844036697248,"Note that the above inequality holds for any ﬁxed b >= 2. As a result, by changing of notation, for
all τ ≥1, we have δτ = τ
X"
CAN BE BOUNDED AS FOLLOWING,0.8422018348623853,"m=1
(Φ⊤((1 −λ)diag(rm) + λdiag(fm)) + γλβm−1Pπ)(Pπ,µ)τ−m + (γ2λ)τδ0(Pπ,µ)τ. (61)"
CAN BE BOUNDED AS FOLLOWING,0.8431192660550458,"Substituting eq. (61) into trace (ΦδτPµ,π), we have"
CAN BE BOUNDED AS FOLLOWING,0.8440366972477065,"trace (ΦδτPµ,π)"
CAN BE BOUNDED AS FOLLOWING,0.844954128440367,"= trace  Φ τ
X"
CAN BE BOUNDED AS FOLLOWING,0.8458715596330275,"m=1
(Φ⊤((1 −λ)diag(rm) + λdiag(fm)) + γλβm−1Pπ)(Pπ,µ)τ−m"
CAN BE BOUNDED AS FOLLOWING,0.846788990825688,"+ (γ2λ)τδ0(Pπ,µ)τ
! Pµ,π ! (i)
= τ
X"
CAN BE BOUNDED AS FOLLOWING,0.8477064220183487,"m=1
trace
 
(Pπ,µ)τ−m+1  
ΦΦ⊤((1 −λ)diag(rm) + λdiag(fm)) + γλΦβm−1Pπ
"
CAN BE BOUNDED AS FOLLOWING,0.8486238532110092,"+ trace
 
(γ2λ)τ(Pπ,µ)τ+1Φδ0

,
(62)"
CAN BE BOUNDED AS FOLLOWING,0.8495412844036697,"where (i) follows from the fact that trace(AB) = trace(BA) and trace(A + B) = trace(A) +
trace(B)."
CAN BE BOUNDED AS FOLLOWING,0.8504587155963302,"Applying Lemma 6 with Q = Pµ,π, C = ρmax, P = Pπ and D = (1 −λ)diag(rm), we have"
CAN BE BOUNDED AS FOLLOWING,0.8513761467889909,"trace
 
(Pπ,µ)τ−m+1ΦΦ⊤(1 −λ)diag(rm)

≤(1 −λ)ρτ−m+1
max
B2
φ∥rm∥1.
(63)"
CAN BE BOUNDED AS FOLLOWING,0.8522935779816514,"Applying Lemma 6 with Q = Pµ,π, C = ρmax, P = Pπ and D = λdiag(fm), we have"
CAN BE BOUNDED AS FOLLOWING,0.8532110091743119,"trace
 
(Pπ,µ)τ+1−mΦΦ⊤λdiag(fm)

≤λρτ+1−m
max
B2
φ∥fm∥1"
CAN BE BOUNDED AS FOLLOWING,0.8541284403669724,"(i)
≤λρτ+1−m
max
B2
φ(1 + ∥f∥1),
(64)"
CAN BE BOUNDED AS FOLLOWING,0.8550458715596331,where (i) follows from eq. (37).
CAN BE BOUNDED AS FOLLOWING,0.8559633027522936,"For the term trace
 
(Pπ,µ)τ+1−mΦβm−1Pπ

, we have"
CAN BE BOUNDED AS FOLLOWING,0.8568807339449541,"trace
 
(Pπ,µ)τ+1−mΦβm−1Pπ
"
CAN BE BOUNDED AS FOLLOWING,0.8577981651376146,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.8587155963302753,"(i)
= (γλ)m−1trace
 
P m
π (Pπ,µ)τ+1−mΦΦ⊤Dµ,0
 + m−2
X"
CAN BE BOUNDED AS FOLLOWING,0.8596330275229358,"l=0
(γλ)l  
λtrace
 
P l+1
π
(Pπ,µ)τ+1−mΦΦ⊤Dµ,m−1−l
"
CAN BE BOUNDED AS FOLLOWING,0.8605504587155963,"+(1 −λ)trace
 
P l+1
π
(Pπ,µ)τ−m+1ΦΦ⊤Fπ,m−1−l
"
CAN BE BOUNDED AS FOLLOWING,0.8614678899082568,"(ii)
≤B2
φρτ+1−m
max "
CAN BE BOUNDED AS FOLLOWING,0.8623853211009175,"(γλ)m−11⊤dµ,0 + m−2
X"
CAN BE BOUNDED AS FOLLOWING,0.863302752293578,"l=0
(γλ)l  
λ1⊤dµ,m−1−l + (1 −λ)1⊤fm−1−l

!"
CAN BE BOUNDED AS FOLLOWING,0.8642201834862385,"(iii)
≤
B2
φρτ+1−m
max
1 −γλ
(1 + (1 −λ)∥f∥1) ,
(65)"
CAN BE BOUNDED AS FOLLOWING,0.865137614678899,"where (i) follows from eq. (54) and the facts that trace(A + B) = trace(A) + trace(B) and
trace(AB) = trace(BA), (ii) follows from Lemma 6 with Q = Pµ,π, P = Pπ, and D = Fm−1−l
and Dπ,m−1−l respectively, and (iii) follow from the eq. (37)."
CAN BE BOUNDED AS FOLLOWING,0.8660550458715597,"Recall δ0 = Φ⊤Dµ,0. Applying Lemma 6 with Q = Pµ,π and D = Dµ,0 yields"
CAN BE BOUNDED AS FOLLOWING,0.8669724770642202,"trace
 
(γ2λ)τ(Pπ,µ)τ+1Φδ0

≤(γ2λ)τB2
φρτ+1
max.
(66)"
CAN BE BOUNDED AS FOLLOWING,0.8678899082568807,"Substituting eqs. (63) to (66) into eq. (62), we have"
CAN BE BOUNDED AS FOLLOWING,0.8688073394495412,"trace (ΦδτPµ,π)"
CAN BE BOUNDED AS FOLLOWING,0.8697247706422019,"≤(γ2λ)τB2
φρτ+1
max + τ
X"
CAN BE BOUNDED AS FOLLOWING,0.8706422018348624,"m=1
B2
φρτ−m+1
max"
CAN BE BOUNDED AS FOLLOWING,0.8715596330275229,"
λ(∥f∥1 + 1) + 1 + (1 −λ)∥f∥1 1 −γλ 
+ τ
X"
CAN BE BOUNDED AS FOLLOWING,0.8724770642201835,"m=1
(1 −λ)ρτ−m+1
max
B2
φ∥rm∥1. (67)"
CAN BE BOUNDED AS FOLLOWING,0.8733944954128441,"Substituting eqs. (60) and (67) into eq. (59), we have"
CAN BE BOUNDED AS FOLLOWING,0.8743119266055046,"1⊤∆b ≤λ2bγ2bρb
maxB2
φ + (1 + 2γ(1 −λ)(1 + ∥f∥1) + 2γλCβ) B2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.8752293577981651,"τ=1
λ2(b−τ)γ2(b−τ)ρb−τ
max"
CAN BE BOUNDED AS FOLLOWING,0.8761467889908257,"+ γ2(1 −λ)2ρb+1
maxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.8770642201834863,"τ=1
λ2b−2τγ2b−2τρ−τ
max∥rτ−1∥1 + 2λbγ2b(1 −λ)ρb
maxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.8779816513761468,"τ=1
λb−τ"
CAN BE BOUNDED AS FOLLOWING,0.8788990825688073,"+ 2γ2λ(1 −λ)B2
φ"
CAN BE BOUNDED AS FOLLOWING,0.8798165137614679,"
λ∥f∥1 + 1 + 1 + (1 −λ)∥f∥1 1 −γλ"
CAN BE BOUNDED AS FOLLOWING,0.8807339449541285,"
ρb
max b
X"
CAN BE BOUNDED AS FOLLOWING,0.881651376146789,"τ=1
γ2b−2τλ2b−2τ
τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.8825688073394495,"m=1
ρ−m
max"
CAN BE BOUNDED AS FOLLOWING,0.8834862385321101,"+ 2γ2λ(1 −λ)2ρb
maxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.8844036697247707,"τ=1
γ2b−2τλ2b−2τ
τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.8853211009174312,"m=1
ρ−m
max∥rm∥1,
(68)"
CAN BE BOUNDED AS FOLLOWING,0.8862385321100917,"where we let Cβ :=
B2
φ
1−γλ (1 + (1 −λ)∥f∥1)."
CAN BE BOUNDED AS FOLLOWING,0.8871559633027523,"Under different conditions of ρmax, the term 1⊤∆b and E
bT λ
t (θt)

2 2 Ft−1"
CAN BE BOUNDED AS FOLLOWING,0.8880733944954129,"
can be upper bounded"
CAN BE BOUNDED AS FOLLOWING,0.8889908256880734,differently as following:
CAN BE BOUNDED AS FOLLOWING,0.8899082568807339,"(a). γ2ρmax < 1, substituting eq. (40) into eq. (68) yields"
CAN BE BOUNDED AS FOLLOWING,0.8908256880733945,"1⊤∆b ≤λ2bγ2bρb
maxB2
φ + (1 + 2γ(1 −λ)(1 + ∥f∥1) + 2γλCβ) B2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.8917431192660551,"τ=1
λ2(b−τ)γ2(b−τ)ρb−τ
max"
CAN BE BOUNDED AS FOLLOWING,0.8926605504587156,"+ γ2(1 −λ)2ρb+1
maxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.8935779816513761,"τ=1
λ2b−2τγ2b−2τρ−τ
max"
CAN BE BOUNDED AS FOLLOWING,0.8944954128440367,3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.8954128440366973,"1 −γ2
+ 1
"
CAN BE BOUNDED AS FOLLOWING,0.8963302752293578,"+ 2λbγ2b(1 −λ)ρb
maxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.8972477064220183,"τ=1
λb−τ"
CAN BE BOUNDED AS FOLLOWING,0.8981651376146789,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.8990825688073395,"+ 2γ2λ(1 −λ)B2
φ"
CAN BE BOUNDED AS FOLLOWING,0.9,"
λ∥f∥1 + 1 + 1 + (1 −λ)∥f∥1 1 −γλ"
CAN BE BOUNDED AS FOLLOWING,0.9009174311926605,"
ρb
max · b
X"
CAN BE BOUNDED AS FOLLOWING,0.9018348623853211,"τ=1
γ2b−2τλ2b−2τ
τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.9027522935779817,"m=1
ρ−m
max"
CAN BE BOUNDED AS FOLLOWING,0.9036697247706422,"+ 2γ2λ(1 −λ)2ρb
maxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.9045871559633027,"τ=1
γ2b−2τλ2b−2τ
τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.9055045871559633,"m=1
ρ−m
max"
CAN BE BOUNDED AS FOLLOWING,0.9064220183486239,3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.9073394495412844,"1 −γ2
+ 1
"
CAN BE BOUNDED AS FOLLOWING,0.908256880733945,"≤λ2bγ2bρb
maxB2
φ +
(1 + 2γ(1 −λ)(1 + ∥f∥1) + 2γλCβ) B2
φ
1 −γ2ρmaxλ"
CAN BE BOUNDED AS FOLLOWING,0.9091743119266055,"+
γ2(1 −λ)2ρmaxB2
φ
1 −γ2ρmaxλ"
CAN BE BOUNDED AS FOLLOWING,0.9100917431192661,3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.9110091743119266,"1 −γ2
+ 1

+ 2λ2bγbρb
maxB2
φ"
CAN BE BOUNDED AS FOLLOWING,0.9119266055045872,"+
2γ2λ(1 −λ)B2
φ
(1 −γ2λ2)(1 −ρ−1
max)"
CAN BE BOUNDED AS FOLLOWING,0.9128440366972477,"
λ∥f∥1 + 1 + 1 + (1 −λ)∥f∥1 1 −γλ"
CAN BE BOUNDED AS FOLLOWING,0.9137614678899083,"
ρb+1
max"
CAN BE BOUNDED AS FOLLOWING,0.9146788990825688,"+
2γ2λ(1 −λ)2B2
φ
(1 −γ2λ2)(1 −ρ−1
max)"
CAN BE BOUNDED AS FOLLOWING,0.9155963302752294,3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.9165137614678899,"1 −γ2
+ 1

ρb+1
max,
(69)"
CAN BE BOUNDED AS FOLLOWING,0.9174311926605505,"where the last two terms of the above inequality are of the order O
 
ρb
max

. Therefore, we have
1⊤∆b ≤Cρb
max for some C > 0. Substituting eq. (69) into eq. (57) yields"
CAN BE BOUNDED AS FOLLOWING,0.918348623853211,"E
bT λ
t (θt)

2 2 Ft−1"
CAN BE BOUNDED AS FOLLOWING,0.9192660550458716,"
≤Cσ,λ,1ρb
max,"
CAN BE BOUNDED AS FOLLOWING,0.9201834862385321,"where Cσ,λ,1 > 0 is a constant and is determined by eq. (69)."
CAN BE BOUNDED AS FOLLOWING,0.9211009174311927,"(b). γ2ρmax = 1, substituting eq. (39) into eq. (68) yields"
CAN BE BOUNDED AS FOLLOWING,0.9220183486238532,"1⊤∆b ≤λ2bB2
φ + (1 + 2γ(1 −λ)(1 + ∥f∥1) + 2γλCβ) B2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.9229357798165138,"τ=1
λ2(b−τ)"
CAN BE BOUNDED AS FOLLOWING,0.9238532110091743,"+ γ2(1 −λ)2ρmaxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.9247706422018349,"τ=1
λ2b−2τ (4 + 2γ∥f∥1) τ + 2λb(1 −λ)B2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.9256880733944954,"τ=1
λb−τ"
CAN BE BOUNDED AS FOLLOWING,0.926605504587156,"+ 2γ2λ(1 −λ)B2
φ"
CAN BE BOUNDED AS FOLLOWING,0.9275229357798165,"
λ∥f∥1 + 1 + 1 + (1 −λ)∥f∥1 1 −γλ"
CAN BE BOUNDED AS FOLLOWING,0.9284403669724771,"
ρb
max b
X"
CAN BE BOUNDED AS FOLLOWING,0.9293577981651376,"τ=1
γ2b−2τλ2b−2τ
τ
X"
CAN BE BOUNDED AS FOLLOWING,0.9302752293577982,"m=1
ρ−m
max"
CAN BE BOUNDED AS FOLLOWING,0.9311926605504587,"+ 2γ2λ(1 −λ)2 (4 + 2γ∥f∥1) B2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.9321100917431193,"τ=1
λ2b−2τ
τ
X"
CAN BE BOUNDED AS FOLLOWING,0.9330275229357798,"m=1
ρτ−m
max m"
CAN BE BOUNDED AS FOLLOWING,0.9339449541284404,"≤λ2bB2
φ +
(1 + 2γ(1 −λ)(1 + ∥f∥1) + 2γλCβ) B2
φ
1 −λ2
+ 2λbB2
φ +"
CAN BE BOUNDED AS FOLLOWING,0.9348623853211009,"γ2(1 −λ)2ρmaxB2
φ
1 −λ2
(4 + 2γ∥f∥1) +
2γ2λ(1 −λ)2 (4 + 2γ∥f∥1) B2
φ
(1 −λ2)(1 −ρ−1
max) ! b"
CAN BE BOUNDED AS FOLLOWING,0.9357798165137615,"+
2γ2λ(1 −λ)B2
φ
(1 −γ2λ2)(1 −ρ−1
max)"
CAN BE BOUNDED AS FOLLOWING,0.936697247706422,"
λ∥f∥1 + 1 + 1 + (1 −λ)∥f∥1 1 −γλ"
CAN BE BOUNDED AS FOLLOWING,0.9376146788990826,"
ρb+1
max,
(70)"
CAN BE BOUNDED AS FOLLOWING,0.9385321100917431,"where the last term of the above inequality is of the order O
 
ρb
max

. Therefore, we have 1⊤∆b ≤
Cρb
max for some C > 0. Substituting eq. (70) into eq. (57) yields"
CAN BE BOUNDED AS FOLLOWING,0.9394495412844037,"E
bT λ
t (θt)

2 2 Ft−1"
CAN BE BOUNDED AS FOLLOWING,0.9403669724770642,"
≤Cσ,λ,2ρb
max,"
CAN BE BOUNDED AS FOLLOWING,0.9412844036697248,"where Cσ,λ,2 > 0 is a constant and is determined by eq. (70)."
CAN BE BOUNDED AS FOLLOWING,0.9422018348623853,"(c). γ2ρmax > 1, substituting eq. (38) into eq. (68) yields"
CAN BE BOUNDED AS FOLLOWING,0.9431192660550459,"1⊤∆b ≤λ2bγ2bρb
maxB2
φ + (1 + 2γ(1 −λ)(1 + ∥f∥1) + 2γλCβ) B2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.9440366972477064,"τ=1
λ2(b−τ)γ2(b−τ)ρb−τ
max"
CAN BE BOUNDED AS FOLLOWING,0.944954128440367,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.9458715596330275,"+γ2b+2(1 −λ)2ρb+1
maxB2
φ"
CAN BE BOUNDED AS FOLLOWING,0.9467889908256881, 3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.9477064220183486,"γ2ρmax −1 + 1

b
X"
CAN BE BOUNDED AS FOLLOWING,0.9486238532110092,"τ=1
λ2b−2τ + 2λbγ2b(1 −λ)ρb
maxB2
φ b
X"
CAN BE BOUNDED AS FOLLOWING,0.9495412844036697,"τ=1
λb−τ"
CAN BE BOUNDED AS FOLLOWING,0.9504587155963303,"+ 2γ2λ(1 −λ)B2
φ"
CAN BE BOUNDED AS FOLLOWING,0.9513761467889909,"
λ∥f∥1 + 1 + 1 + (1 −λ)∥f∥1 1 −γλ"
CAN BE BOUNDED AS FOLLOWING,0.9522935779816514,"
ρb
max b
X"
CAN BE BOUNDED AS FOLLOWING,0.9532110091743119,"τ=1
γ2b−2τλ2b−2τ
τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.9541284403669725,"m=1
ρ−m
max"
CAN BE BOUNDED AS FOLLOWING,0.955045871559633,"+ 2γ2λ(1 −λ)2ρb
maxB2
φ"
CAN BE BOUNDED AS FOLLOWING,0.9559633027522936, 3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.9568807339449541,"γ2ρmax −1 + 1

b
X"
CAN BE BOUNDED AS FOLLOWING,0.9577981651376147,"τ=1
γ2b−2τλ2b−2τ
τ−1
X"
CAN BE BOUNDED AS FOLLOWING,0.9587155963302753,"m=1
γ2m"
CAN BE BOUNDED AS FOLLOWING,0.9596330275229358,"≤λ2bγ2bρb
maxB2
φ +
(1 + 2γ(1 −λ)(1 + ∥f∥1) + 2γλCβ) B2
φ
γ2ρmax −1
· γ2bρb
max"
CAN BE BOUNDED AS FOLLOWING,0.9605504587155963,"+
γ2b+2(1 −λ)2ρb+1
maxB2
φ
1 −λ2"
CAN BE BOUNDED AS FOLLOWING,0.9614678899082569, 3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.9623853211009175,"γ2ρmax −1 + 1

+ 2λbγ2bρb
maxB2
φ"
CAN BE BOUNDED AS FOLLOWING,0.963302752293578,"+
2γ2λ(1 −λ)B2
φ
(1 −γ2λ2)(1 −ρ−1
max)"
CAN BE BOUNDED AS FOLLOWING,0.9642201834862385,"
λ∥f∥1 + 1 + 1 + (1 −λ)∥f∥1 1 −γλ"
CAN BE BOUNDED AS FOLLOWING,0.9651376146788991,"
ρb+1
max"
CAN BE BOUNDED AS FOLLOWING,0.9660550458715597,"+
2γ2λ(1 −λ)2"
CAN BE BOUNDED AS FOLLOWING,0.9669724770642202,"(1 −γ2)(1 −γ2λ2)B2
φ"
CAN BE BOUNDED AS FOLLOWING,0.9678899082568807, 3 + 2γ∥f∥1
CAN BE BOUNDED AS FOLLOWING,0.9688073394495413,"γ2ρmax −1 + 1

ρb
max,
(71)"
CAN BE BOUNDED AS FOLLOWING,0.9697247706422019,"where the last term of the above inequality is of the order O
 
ρb
max

. Therefore, we have 1⊤∆b ≤
Cρb
max for some C > 0. Substituting eq. (71) into eq. (57) yields"
CAN BE BOUNDED AS FOLLOWING,0.9706422018348624,"E
bT λ
t (θt)

2 2 Ft−1"
CAN BE BOUNDED AS FOLLOWING,0.9715596330275229,"
≤Cσ,λ,3ρb
max,"
CAN BE BOUNDED AS FOLLOWING,0.9724770642201835,"where Cσ,λ,3 > 0 is a constant and is determined by eq. (71)."
CAN BE BOUNDED AS FOLLOWING,0.9733944954128441,"To summarize, the variance term E
bTt(θt)

2 2 Ft−1"
CAN BE BOUNDED AS FOLLOWING,0.9743119266055046,"
can be bounded by σ2
λ = O(ρb
max)."
CAN BE BOUNDED AS FOLLOWING,0.9752293577981651,"E.3
PROOF OF THEOREM 2"
CAN BE BOUNDED AS FOLLOWING,0.9761467889908257,"Theorem 4 (Formal Statement of Theorem 2). Suppose Assumptions 1 and 2 hold. Consider PER-
ETD(λ) speciﬁed in Algorithm 2. Let the stepsize ηt =
2
µλ(t+tλ), tλ = 8L2
λ
µ2
λ , where µλ is deﬁned in
Lemma 7 and Lλ is deﬁned in Lemma 8 in Appendix C. Further let"
CAN BE BOUNDED AS FOLLOWING,0.9770642201834863,"b = max
nl
log(µλ)−log(5Cb,λBφ)"
CAN BE BOUNDED AS FOLLOWING,0.9779816513761468,"log(ξ)
m
,
log(T )
log(ρmax)+log(1/ξ)
o
,"
CAN BE BOUNDED AS FOLLOWING,0.9788990825688073,"where Cb,λ is a constant deﬁned in the proof of Proposition 3 in Appendix E.1, Bφ
:=
maxs∈S ∥φ(s)∥2, and ξ := max{γ, χ}. Let the projection set Θ =

θ ∈Rd : ∥θ∥2 ≤Bθ
	
, where"
CAN BE BOUNDED AS FOLLOWING,0.9798165137614679,Bθ = ∥Φ⊤∥2rmax
CAN BE BOUNDED AS FOLLOWING,0.9807339449541285,"(1−γ)µλ
(which implies θ∗
λ ∈Θ). Then the output θT of PER-ETD(λ) satisﬁes"
CAN BE BOUNDED AS FOLLOWING,0.981651376146789,"E

∥θT −θ∗
λ∥2
2

≤O
 1 T aλ 
,"
CAN BE BOUNDED AS FOLLOWING,0.9825688073394495,"where aλ =
1
log1/ξ(ρmax)+1. Further, PER-ETD(λ) attains an ϵ-accurate solution with ˜O

1
ϵ1/aλ
"
CAN BE BOUNDED AS FOLLOWING,0.9834862385321101,samples.
CAN BE BOUNDED AS FOLLOWING,0.9844036697247707,"Proof. The proof follows the same steps as Theorem 1 by replacing the terms bTt, T , t0, L0, µ0, Cb,
σ2 and θ∗with bT λ
t , T λ, tλ, Lλ, µλ, Cb,λ, σ2
λ and θ∗
λ respectively. Speciﬁcally, Propositions 3 and 4
are applied to bound the bias and variance over the steps similarly to eq. (44). We then have the
convergence as follows."
CAN BE BOUNDED AS FOLLOWING,0.9853211009174312,"E

∥θT −θ∗
λ∥2
2

≤O
∥θ0 −θ∗
λ∥2
2
T 2"
CAN BE BOUNDED AS FOLLOWING,0.9862385321100917,"
+ O
σ2
λ
T"
CAN BE BOUNDED AS FOLLOWING,0.9871559633027523,"
+ O
ξ2b T"
CAN BE BOUNDED AS FOLLOWING,0.9880733944954129,"
+ O
 
ξb"
CAN BE BOUNDED AS FOLLOWING,0.9889908256880734,"= O
∥θ0 −θ∗
λ∥2
2
T 2"
CAN BE BOUNDED AS FOLLOWING,0.9899082568807339,"
+ O
ρb
max T"
CAN BE BOUNDED AS FOLLOWING,0.9908256880733946,"
+ O
ξ2b T"
CAN BE BOUNDED AS FOLLOWING,0.9917431192660551,"
+ O
 
ξb
.
(72)"
CAN BE BOUNDED AS FOLLOWING,0.9926605504587156,Published as a conference paper at ICLR 2022
CAN BE BOUNDED AS FOLLOWING,0.9935779816513761,"We further specify b =
nl
log(µ0)−log(5CbBφ)"
CAN BE BOUNDED AS FOLLOWING,0.9944954128440368,"log(ξ)
m
,
log(T )
log(ρmax)+log(1/ξ)
o
. Then Equation (72) yields"
CAN BE BOUNDED AS FOLLOWING,0.9954128440366973,"E

∥θT −θ∗
λ∥2
2

≤O
∥θ0 −θ∗
λ∥2
2
T 2"
CAN BE BOUNDED AS FOLLOWING,0.9963302752293578,"
+ O
T 1−aλ T"
CAN BE BOUNDED AS FOLLOWING,0.9972477064220183,"
+ O

1
T 1+aλ"
CAN BE BOUNDED AS FOLLOWING,0.998165137614679,"
+ O
 1 T aλ"
CAN BE BOUNDED AS FOLLOWING,0.9990825688073395,"
= O
 1 T aλ 
."
