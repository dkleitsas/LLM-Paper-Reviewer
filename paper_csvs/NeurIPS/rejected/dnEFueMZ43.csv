Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001834862385321101,"The ability to leverage shared behaviors between tasks is critical for sample-efﬁcient
1"
ABSTRACT,0.003669724770642202,"multi-task reinforcement learning (MTRL). While prior MTRL methods primarily
2"
ABSTRACT,0.005504587155963303,"focus on parameter and data sharing, these methods do not exploit the fact that
3"
ABSTRACT,0.007339449541284404,"learning agents often beneﬁt from sharing behaviors when acquiring skills. Few
4"
ABSTRACT,0.009174311926605505,"behavior-sharing methods exist but are limited to task families requiring only
5"
ABSTRACT,0.011009174311926606,"directly shareable and similar behaviors. Our goal is to extend the efﬁcacy of
6"
ABSTRACT,0.012844036697247707,"behavior-sharing to more general task families that could require a mix of shareable
7"
ABSTRACT,0.014678899082568808,"and conﬂicting behaviors. Our key insight is an agent’s behavior across tasks can be
8"
ABSTRACT,0.01651376146788991,"used for mutually beneﬁcial exploration. To this end, we propose a simple MTRL
9"
ABSTRACT,0.01834862385321101,"framework for identifying shareable behaviors over tasks and incorporating them
10"
ABSTRACT,0.02018348623853211,"to guide exploration. We empirically demonstrate how behavior sharing improves
11"
ABSTRACT,0.022018348623853212,"sample efﬁciency and ﬁnal performance on manipulation and navigation MTRL
12"
ABSTRACT,0.023853211009174313,"tasks with conﬂicting behaviors and is even complementary to parameter sharing.
13"
ABSTRACT,0.025688073394495414,"Result videos are available at https://sites.google.com/view/qmp-mtrl.
14"
INTRODUCTION,0.027522935779816515,"1
Introduction
15"
INTRODUCTION,0.029357798165137616,"Imagine we are simultaneously learning to solve a diverse set of tasks in the kitchen, such as cooking
16"
INTRODUCTION,0.031192660550458717,"an egg, washing dishes, and boiling water (see Figure 1). Several behaviors are similar across these
17"
INTRODUCTION,0.03302752293577982,"tasks: interacting with the same appliances (like the fridge or faucet) and navigating common paths
18"
INTRODUCTION,0.03486238532110092,"across the kitchen (like going to the countertop). While solving a particular task, humans can easily
19"
INTRODUCTION,0.03669724770642202,"recognize the behaviors that can or cannot be shared from other tasks. This enables us to efﬁciently
20"
INTRODUCTION,0.03853211009174312,"solve multiple tasks by mutually beneﬁcial exploration.
21"
INTRODUCTION,0.04036697247706422,"Can we replicate how humans naturally learn multiple skills at once, by noticing and utilizing common
22"
INTRODUCTION,0.04220183486238532,"behaviors between them, and create a framework that can do the same efﬁciently? While typical works
23"
INTRODUCTION,0.044036697247706424,"in multi-task reinforcement learning (MTRL) exploit sharing policy parameters (Vithayathil Varghese
24"
INTRODUCTION,0.045871559633027525,"& Mahmoud, 2020) or relabeled data between tasks (Kaelbling, 1993), behavior-sharing is underex-
25"
INTRODUCTION,0.047706422018348627,"plored and can lead to complementary improvements. Recent works (Teh et al., 2017; Ghosh et al.,
26"
INTRODUCTION,0.04954128440366973,"2018) learn a shared policy distilled (Rusu et al., 2015) from all tasks, and either use it directly or to
27"
INTRODUCTION,0.05137614678899083,"enforce learning of similar behaviors. However, these methods share behavior uniformly across tasks,
28"
INTRODUCTION,0.05321100917431193,"limiting their effectiveness for task families requiring conﬂicting optimal behaviors from the same
29"
INTRODUCTION,0.05504587155963303,"state. In this work, our goal is to extend the efﬁciency gains of behavior sharing to such task families.
30"
INTRODUCTION,0.05688073394495413,"Concretely, we propose the problem of selective behavior sharing for improving exploration in
31"
INTRODUCTION,0.05871559633027523,"MTRL. Our key insight is that an agent’s past or current behaviors across tasks can be helpful
32"
INTRODUCTION,0.060550458715596334,"for exploration during training, despite potential conﬂicts in the ﬁnal policies, as shown in human
33"
INTRODUCTION,0.062385321100917435,"learners (Tomov et al., 2021). For instance, while boiling water in Figure 1, a household robot can
34"
INTRODUCTION,0.06422018348623854,"start by exploring behaviors found rewarding in other tasks, such as going to the countertop and
35"
INTRODUCTION,0.06605504587155964,"turning the faucet on, instead of randomly exploring the entire kitchen.
36"
INTRODUCTION,0.06788990825688074,Cook Egg
INTRODUCTION,0.06972477064220184,Wash Dishes
INTRODUCTION,0.07155963302752294,Boil Water
INTRODUCTION,0.07339449541284404,Make Salad
INTRODUCTION,0.07522935779816514,"Unshared Behaviors
Transfer [Faucet-On]
Simultaneous [Bowl-Fetch]"
INTRODUCTION,0.07706422018348624,"Figure 1: When an agent learns multiple tasks together, selective behavior-sharing can improve overall
learning efﬁciency. (Right) Simultaneous Learning: Boil-Water and Make-Salad tasks can learn
the behavior of [Bowl-Fetch] simultaneously. (Center) Transfer Behaviors: The agent ﬁrst learns
the [Faucet-On] behavior in Boil-Water task, which can be reused to speed up exploration while
learning the Wash-Dishes task. (Left) Unshared Behaviors: Cook-Egg and Wash-Dishes require
conﬂicting starting behaviors of going to the refrigerator or sink, not suitable for sharing."
INTRODUCTION,0.07889908256880734,"Two key challenges arise in selectively sharing exploratory behaviors for MTRL: identifying and
37"
INTRODUCTION,0.08073394495412844,"incorporating shareable behaviors. First, the agent must assess the relevance of behaviors from
38"
INTRODUCTION,0.08256880733944955,"other tasks depending on the current state and training progress. For instance, in Figure 1, other
39"
INTRODUCTION,0.08440366972477065,"task policies may be disparately helpful (in blue) or harmful (in red) depending on the state of the
40"
INTRODUCTION,0.08623853211009175,"environment. The second challenge is that initially helpful behaviors can eventually be suboptimal
41"
INTRODUCTION,0.08807339449541285,"for the task. Thus, the prior approaches that use other tasks’ reward-labeled data directly or regularize
42"
INTRODUCTION,0.08990825688073395,"the policy output to copy other task behaviors would likely fail as tasks diverge. Therefore, we need
43"
INTRODUCTION,0.09174311926605505,"an effective mechanism to incorporate other task behaviors as exploration proposals.
44"
INTRODUCTION,0.09357798165137615,"To address these challenges, we propose a simple MTRL framework called Q-switch Mixture of
45"
INTRODUCTION,0.09541284403669725,"Policies (QMP), consisting of a Q-switch for identifying shareable behaviors and is used to guide
46"
INTRODUCTION,0.09724770642201835,"an exploration scheme incorporating a mixture of policies. First, we use the current task’s Q-
47"
INTRODUCTION,0.09908256880733946,"function (Sutton & Barto, 2018), a state and training-progress aware metric, to assess the quality of
48"
INTRODUCTION,0.10091743119266056,"other task policies’ behaviors when applied to the current task. This Q-switch acts as a ﬁlter (Nair
49"
INTRODUCTION,0.10275229357798166,"et al., 2018) to evaluate the potential relevance of explorative behaviors from other tasks. Second,
50"
INTRODUCTION,0.10458715596330276,"we replace the data collection policy for each task with a mixture of all task policies gated by the
51"
INTRODUCTION,0.10642201834862386,"Q-switch. Importantly, the mixture is only used for exploration while each policy is still trained
52"
INTRODUCTION,0.10825688073394496,"independently for its own task. Therefore, QMP makes no shared optimality assumptions over tasks.
53"
INTRODUCTION,0.11009174311926606,"Our primary contribution is introducing the problem of selective behavior sharing for improving ex-
54"
INTRODUCTION,0.11192660550458716,"ploration in multi-task reinforcement learning requiring different optimal behaviors. We demonstrate
55"
INTRODUCTION,0.11376146788990826,"that our proposed framework, Q-switch Mixture of Policies (QMP), identiﬁes shareable behaviors
56"
INTRODUCTION,0.11559633027522936,"from other tasks and incorporates them to make exploration efﬁcient. This enables sample-efﬁcient
57"
INTRODUCTION,0.11743119266055047,"multi-task learning in manipulation and navigation tasks. Finally, we demonstrate how behavior
58"
INTRODUCTION,0.11926605504587157,"sharing is complementary to parameter sharing, a typical way of improving MTRL.
59"
RELATED WORK,0.12110091743119267,"2
Related Work
60"
RELATED WORK,0.12293577981651377,"Multi-Task Learning for Diverse Task Families. Multi-task learning in diverse task families is
61"
RELATED WORK,0.12477064220183487,"susceptible to negative transfer between dissimilar tasks that hinders training. Prior works combat
62"
RELATED WORK,0.12660550458715597,"this by measuring task relatedness through validation loss on tasks (Liu et al., 2022) or inﬂuence of
63"
RELATED WORK,0.12844036697247707,"one task to another (Fifty et al., 2021; Standley et al., 2020) to ﬁnd task groupings for training. Other
64"
RELATED WORK,0.13027522935779817,"works focus on the challenge of multi-objective optimization (Sener & Koltun, 2018; Hessel et al.,
65"
RELATED WORK,0.13211009174311927,"2019; Yu et al., 2020; Schaul et al., 2019; Chen et al., 2018), although recent work has questioned
66"
RELATED WORK,0.13394495412844037,"the need for specialized methods (Kurin et al., 2022). In a similar light, we posit that prior behavior-
67"
RELATED WORK,0.13577981651376148,"sharing approaches for MTRL do not work well for diverse task families where different optimal
68"
RELATED WORK,0.13761467889908258,"behaviors are required, and thus propose to share behaviors via exploration.
69"
RELATED WORK,0.13944954128440368,"Exploration in Multi-Task Reinforcement Learning. We share the motivation of improving
70"
RELATED WORK,0.14128440366972478,"exploration in MTRL with several prior works. Bangaru et al. (2016) proposed to encourage agents
71"
RELATED WORK,0.14311926605504588,"to increase their state coverage by providing an exploration bonus. Zhang & Wang (2021) studied
72"
RELATED WORK,0.14495412844036698,"sharing information between agents to encourage exploration under tabular MDPs with a regret
73"
RELATED WORK,0.14678899082568808,"guarantee. Kalashnikov et al. (2021b) directly leverage data from policies of other specialized tasks
74"
RELATED WORK,0.14862385321100918,"(like grasping a ball) for their general task variant (like grasping an object). In contrast to these
75"
RELATED WORK,0.15045871559633028,"approaches, we do not require a pre-deﬁned task similarity measure or exploration bonus. Instead,
76"
RELATED WORK,0.15229357798165138,"we learn to identify shareable behaviors and use them for improving exploration in online MTRL.
77"
RELATED WORK,0.15412844036697249,"Sharing in Multi-Task Reinforcement Learning.
There are multiple, mostly complementary
78"
RELATED WORK,0.1559633027522936,"ways to share information in MTRL, including sharing data, sharing parameters or representations,
79"
RELATED WORK,0.1577981651376147,"and sharing behaviors. In ofﬂine MTRL, prior works selectively share data between tasks (Yu
80"
RELATED WORK,0.1596330275229358,"et al., 2021, 2022). Sharing parameters across policies can speed up MTRL by learning shared
81"
RELATED WORK,0.1614678899082569,"representations (Xu et al., 2020; D’Eramo et al., 2020; Yang et al., 2020; Sodhani et al., 2021; Misra
82"
RELATED WORK,0.163302752293578,"et al., 2016; Perez et al., 2018; Devin et al., 2017; Vuorio et al., 2019; Rosenbaum et al., 2019) and
83"
RELATED WORK,0.1651376146788991,"can be easily combined with other types of information sharing. Most similar to our work, Teh et al.
84"
RELATED WORK,0.1669724770642202,"(2017) and Ghosh et al. (2018) share behaviors between multiple policies through policy distillation
85"
RELATED WORK,0.1688073394495413,"and regularization. However, unlike our work, they share behavior uniformly between policies and
86"
RELATED WORK,0.1706422018348624,"assume that optimal behaviors are shared across tasks in most states.
87"
RELATED WORK,0.1724770642201835,"Using Q-functions as ﬁlters. Yu et al. (2021) uses Q-functions to ﬁlter which data should be shared
88"
RELATED WORK,0.1743119266055046,"between tasks in a multi-task setting. In the imitation learning setting, Nair et al. (2018) and Sasaki &
89"
RELATED WORK,0.1761467889908257,"Yamashina (2020) use Q-functions to ﬁlter out low-quality demonstrations, so they are not used for
90"
RELATED WORK,0.1779816513761468,"training. In both cases, the Q-function is used to evaluate some data that can be used for training.
91"
RELATED WORK,0.1798165137614679,"Zhang et al. (2022) reuses pre-trained policies to learn a new task, using a Q-function as a ﬁlter to
92"
RELATED WORK,0.181651376146789,"choose which pre-trained policies to regularize to as guidance. In contrast to prior works, our method
93"
RELATED WORK,0.1834862385321101,"uses a Q-function to evaluate explorative actions from different task policies to gather training data.
94"
PROBLEM FORMULATION,0.1853211009174312,"3
Problem Formulation
95"
PROBLEM FORMULATION,0.1871559633027523,"Multi-task learning (MTL) aims to improve performance when simultaneously learning multiple
96"
PROBLEM FORMULATION,0.1889908256880734,"related tasks by leveraging shared structures (Zhang & Yang, 2021). Multi-task reinforcement learning
97"
PROBLEM FORMULATION,0.1908256880733945,"(MTRL) addresses sequential decision-making tasks, where an agent learns behaviors or strategies to
98"
PROBLEM FORMULATION,0.1926605504587156,"act optimally in an environment (Kaelbling et al., 1996; Wilson et al., 2007). Therefore, in addition to
99"
PROBLEM FORMULATION,0.1944954128440367,"the typical MTL techniques, MTRL can also share behaviors to improve sample efﬁciency. However,
100"
PROBLEM FORMULATION,0.1963302752293578,"current behavior sharing MTRL approaches (Section 2) assume that the optimal behaviors of different
101"
PROBLEM FORMULATION,0.1981651376146789,"tasks do not conﬂict with each other. To address this limitation, we seek to develop a behavior-sharing
102"
PROBLEM FORMULATION,0.2,"method that can be applied in more general task families for sample-efﬁcient MTRL.
103"
PROBLEM FORMULATION,0.2018348623853211,"Multi-Task RL with Behavior Sharing. We aim to simultaneously learn a multi-task set of T tasks.
104"
PROBLEM FORMULATION,0.2036697247706422,"Each task Ti is a Markov Decision Process (MDP) deﬁned by the tuple (S, A, T , Ri, ⇢i, γ), with
105"
PROBLEM FORMULATION,0.20550458715596331,"shared state space S, action space S, transition probabilities T , and discount factor γ. The reward
106"
PROBLEM FORMULATION,0.20733944954128442,"function Ri and initial state distribution ⇢i varies by task. We parameterize the multi-task solution as
107"
PROBLEM FORMULATION,0.20917431192660552,"T policies {⇡1, ⇡2, · · · , ⇡T }, where each policy ⇡i(a|s) represents the action distribution for a given
108"
PROBLEM FORMULATION,0.21100917431192662,"state input and quantiﬁes the agent’s behavior on Task Ti. The objective of the agent is to maximize
109"
PROBLEM FORMULATION,0.21284403669724772,"the average expected return over all tasks, where tasks are uniformly sampled during training.
110"
PROBLEM FORMULATION,0.21467889908256882,"Importantly, we do not make the assumption that optimal task behaviors coincide. Optimal behaviors
111"
PROBLEM FORMULATION,0.21651376146788992,"of any two tasks, ⇡⇤"
PROBLEM FORMULATION,0.21834862385321102,i (a|s) and ⇡⇤
PROBLEM FORMULATION,0.22018348623853212,"j (a|s), can be different at the same state s, and thus not directly share-
112"
PROBLEM FORMULATION,0.22201834862385322,"able. Direct behavior-sharing between Ti and Tj, such as sharing reward-labeled data (Kalashnikov
113"
PROBLEM FORMULATION,0.22385321100917432,"et al., 2021a) or behavior regularization (Teh et al., 2017), would lead to suboptimal policies.
114"
APPROACH,0.22568807339449543,"4
Approach
115"
APPROACH,0.22752293577981653,"Our approach for behavior sharing is based on the intuition that humans learn to solve tasks by
116"
APPROACH,0.22935779816513763,"utilizing their knowledge from other tasks (Tomov et al., 2021). To realize this insight in multi-task
117"
APPROACH,0.23119266055045873,"reinforcement learning agents, we propose to selectively share behavior from other tasks to improve
118"
APPROACH,0.23302752293577983,"exploration. Two practical challenges arise from this goal:
119"
APPROACH,0.23486238532110093,"• Identifying shareable behaviors. Behaviors from other task policies should be shared when they
120"
APPROACH,0.23669724770642203,"are potentially beneﬁcial and avoided when known to be conﬂicting or irrelevant. Therefore, we
121"
APPROACH,0.23853211009174313,"need a mechanism to evaluate behavior-sharing between each pair of tasks.
122"
APPROACH,0.24036697247706423,"• Incorporating shareable behaviors. Having determined the shareable behaviors, we must effec-
123"
APPROACH,0.24220183486238533,"tively employ them for better learning. Without a reward relabeler, we cannot share data directly.
124"
APPROACH,0.24403669724770644,"So, we need a mechanism that can use suggestions from other task policies as a way to explore.
125 Task3 Task2 π1"
APPROACH,0.24587155963302754,Policy1
APPROACH,0.24770642201834864,"Best Behavior
Gather Data"
APPROACH,0.24954128440366974,"Data1
Q1(s, aj)
argmax j"
APPROACH,0.25137614678899084,"Train 
 & 
Q1
π1
Q-switch Task1"
APPROACH,0.25321100917431194,"Behavior Proposals
Mixture of Policies"
APPROACH,0.25504587155963304,"Q1
a1 ∼π1( ⋅|s)"
APPROACH,0.25688073394495414,a2 ∼π2( ⋅|s)
APPROACH,0.25871559633027524,a3 ∼π3( ⋅|s)
APPROACH,0.26055045871559634,"Figure 2: Our method (QMP) trains a policy for each task and facilitates behavior sharing in the data
collection phase using a mixture of these policies. For example, in Task 1, each task policy proposes
an action aj. The task-speciﬁc Q-switch evaluates each Q1(s, aj) and selects the best behavior to
gather reward-labeled data for training Q1 and ⇡1. Thus, Task 1 will be boosted by incorporating
only high-reward shareable behaviors into ⇡1 and improving Q1 for subsequent Q-switch evaluations."
APPROACH,0.26238532110091745,"4.1
QMP: Q-switch Mixture of Policies
126"
APPROACH,0.26422018348623855,"Our approach to address these challenges is inspired by human multi-task learning. Before trying
127"
APPROACH,0.26605504587155965,"a new task, we ﬁrst acquire a general understanding of the effectiveness of different behaviors in
128"
APPROACH,0.26788990825688075,"achieving the task. We can then identify the most promising behaviors to try and iteratively reﬁne our
129"
APPROACH,0.26972477064220185,"solution and understanding of the task objective. For instance, when attempting to open a cabinet, we
130"
APPROACH,0.27155963302752295,"might recall applicable behaviors like approaching the handle or walking away. By comparing their
131"
APPROACH,0.27339449541284405,"relative effectiveness, we may choose to try the handle. We propose QMP (Figure 2) a novel method
132"
APPROACH,0.27522935779816515,"that follows this intuition with two components. A Q-switch (Section 4.2) relatively ranks behavior
133"
APPROACH,0.27706422018348625,"proposals from a mixture of task policies (Section 4.3) which is used as an exploration mechanism.
134"
IDENTIFYING SHAREABLE BEHAVIORS,0.27889908256880735,"4.2
Identifying Shareable Behaviors
135"
IDENTIFYING SHAREABLE BEHAVIORS,0.28073394495412846,"Similar to how a human learning a new task may try out the wrong skill a few times before landing
136"
IDENTIFYING SHAREABLE BEHAVIORS,0.28256880733944956,"on the correct skill, an RL agent does not know which behaviors are beneﬁcial for sharing between
137"
IDENTIFYING SHAREABLE BEHAVIORS,0.28440366972477066,"tasks at ﬁrst. This is simply because it does not yet know the optimal behavior or understand the task
138"
IDENTIFYING SHAREABLE BEHAVIORS,0.28623853211009176,"objective. So we can only identify shareable behaviors by estimating the value of different behaviors
139"
IDENTIFYING SHAREABLE BEHAVIORS,0.28807339449541286,"based on our current experience and continue to update this estimate as we become more proﬁcient.
140"
IDENTIFYING SHAREABLE BEHAVIORS,0.28990825688073396,"In MTRL, estimating sharing of behaviors from policy ⇡j to ⇡i depends on the task at hand Task i, the
141"
IDENTIFYING SHAREABLE BEHAVIORS,0.29174311926605506,"environment state s, and the behavior proposal of the other policy at that state ⇡j(s). Therefore, we
142"
IDENTIFYING SHAREABLE BEHAVIORS,0.29357798165137616,"must identify shareable behaviors in a task and state-dependent way, being aware of how all the task
143"
IDENTIFYING SHAREABLE BEHAVIORS,0.29541284403669726,"policies ⇡j change over the course of training. For example, two task policies, such as Boil-Water
144"
IDENTIFYING SHAREABLE BEHAVIORS,0.29724770642201837,"and Make-Salad in Figure 1, may share only a small segment of behavior or may initially beneﬁt
145"
IDENTIFYING SHAREABLE BEHAVIORS,0.29908256880733947,"from shared exploration of a common unknown environment. But eventually, their behaviors become
146"
IDENTIFYING SHAREABLE BEHAVIORS,0.30091743119266057,"conﬂicting or irrelevant to each other as the policies diverge into their own task-speciﬁc behaviors.
147"
IDENTIFYING SHAREABLE BEHAVIORS,0.30275229357798167,"Q-switch: We propose to utilize each task’s learned Q-function to evaluate shareable behaviors. The
148"
IDENTIFYING SHAREABLE BEHAVIORS,0.30458715596330277,"Q-function, Qi(s, a), of Task i estimates the expected discounted return of the policy after taking
149"
IDENTIFYING SHAREABLE BEHAVIORS,0.30642201834862387,"action a at state s (Watkins & Dayan, 1992). Although this is an estimate acquired during training, it
150"
IDENTIFYING SHAREABLE BEHAVIORS,0.30825688073394497,"is a critical component in many state-of-the-art RL algorithms (Haarnoja et al., 2018; Lillicrap et al.,
151"
IDENTIFYING SHAREABLE BEHAVIORS,0.3100917431192661,"2015). It has also been used as a ﬁlter for high-quality training data (Yu et al., 2021; Nair et al., 2018;
152"
IDENTIFYING SHAREABLE BEHAVIORS,0.3119266055045872,"Sasaki & Yamashina, 2020), suggesting the Q-function is effective for evaluating and comparing
153"
IDENTIFYING SHAREABLE BEHAVIORS,0.3137614678899083,"actions during training. Thus, we use the Q-function as a switch that rates action proposals from
154"
IDENTIFYING SHAREABLE BEHAVIORS,0.3155963302752294,"other tasks’ policies for the current task’s state s. While the Q-function could be biased when queried
155"
IDENTIFYING SHAREABLE BEHAVIORS,0.3174311926605505,"with out-of-distribution actions from other policies, we will explain how this is corrected in practice
156"
IDENTIFYING SHAREABLE BEHAVIORS,0.3192660550458716,"in the next section on how the Q-switch is used in behavior sharing. Thus, this simple and intuitive
157"
IDENTIFYING SHAREABLE BEHAVIORS,0.3211009174311927,"function is state and task-dependent, gives the current best estimate of which behaviors are most
158"
IDENTIFYING SHAREABLE BEHAVIORS,0.3229357798165138,"helpful (those with high Q-values) and conﬂicting or irrelevant behaviors (those with low Q-values),
159"
IDENTIFYING SHAREABLE BEHAVIORS,0.3247706422018349,"and is quickly adaptive to changes in its own and other policies during online learning.
160"
INCORPORATING SHAREABLE BEHAVIORS,0.326605504587156,"4.3
Incorporating Shareable Behaviors
161"
INCORPORATING SHAREABLE BEHAVIORS,0.3284403669724771,"We propose to use other task policies as behavioral suggestions to aid the exploration of the current
162"
INCORPORATING SHAREABLE BEHAVIORS,0.3302752293577982,"task. This enables us to incorporate helpful behaviors from other tasks without assuming access to a
163"
INCORPORATING SHAREABLE BEHAVIORS,0.3321100917431193,"reward re-labeler. Furthermore, this allows the agent to observe the effect of a proposed behavior in
164"
INCORPORATING SHAREABLE BEHAVIORS,0.3339449541284404,"the current task, so only the behaviors with high task rewards are effectively incorporated into the
165"
INCORPORATING SHAREABLE BEHAVIORS,0.3357798165137615,"task policy through the training process. This makes our sharing mechanism applicable to general
166"
INCORPORATING SHAREABLE BEHAVIORS,0.3376146788990826,"task families, including those with eventually conﬂicting optimal behaviors.
167"
INCORPORATING SHAREABLE BEHAVIORS,0.3394495412844037,"Mixture of Policies: To allow for selective behavior sharing, we use a mixture of all task policies to
168"
INCORPORATING SHAREABLE BEHAVIORS,0.3412844036697248,"gather training data for each task. Training a mixture of policies is a popular approach in hierarchical
169"
INCORPORATING SHAREABLE BEHAVIORS,0.3431192660550459,"RL (Çelik et al., 2021; Daniel et al., 2016; End et al., 2017; Goyal et al., 2019) to attain reusable
170"
INCORPORATING SHAREABLE BEHAVIORS,0.344954128440367,"skills. In MTRL, we aim to beneﬁt similarly from reusable behaviors. The main differences are that
171"
INCORPORATING SHAREABLE BEHAVIORS,0.3467889908256881,"each policy is specialized to a particular task and the mixture is only used to gather exploratory data.
172"
INCORPORATING SHAREABLE BEHAVIORS,0.3486238532110092,"To this end, we deﬁne a mixture policy ⇡mix"
INCORPORATING SHAREABLE BEHAVIORS,0.3504587155963303,"i
(a|s) for each task i over all the task policies ⇡j. At each
173"
INCORPORATING SHAREABLE BEHAVIORS,0.3522935779816514,"timestep, ⇡mix"
INCORPORATING SHAREABLE BEHAVIORS,0.3541284403669725,"i
uses Q-switch to choose the best-scored policy ⇡⇤"
INCORPORATING SHAREABLE BEHAVIORS,0.3559633027522936,"j at any state and samples an action
174"
INCORPORATING SHAREABLE BEHAVIORS,0.3577981651376147,"from that policy, a ⇠⇡⇤"
INCORPORATING SHAREABLE BEHAVIORS,0.3596330275229358,"j (Figure 2). This mixture allows us to activate multiple policies at different
175"
INCORPORATING SHAREABLE BEHAVIORS,0.3614678899082569,"states in an episode, so we can selectively incorporate shareable behaviors from various tasks in a task
176"
INCORPORATING SHAREABLE BEHAVIORS,0.363302752293578,"and state dependent way. And while there are more sophisticated ways of scoring and incorporating
177"
INCORPORATING SHAREABLE BEHAVIORS,0.3651376146788991,"policy proposals, such as deﬁning a probabilistic mixture, we found that our simple method QMP
178"
INCORPORATING SHAREABLE BEHAVIORS,0.3669724770642202,"works well in practice while requiring minimal hyperparameter tuning or computational overhead.
179"
INCORPORATING SHAREABLE BEHAVIORS,0.3688073394495413,"Importantly, the mixture of policies is used only to gather training data and is not trained directly as a
180"
INCORPORATING SHAREABLE BEHAVIORS,0.3706422018348624,"common policy. Instead, each task policy ⇡i is trained with data gathered for its own task Ti by the
181"
INCORPORATING SHAREABLE BEHAVIORS,0.3724770642201835,mixture of policies ⇡mix
INCORPORATING SHAREABLE BEHAVIORS,0.3743119266055046,"i
with the assistance of Qi as a switch (see Figure 2). The only difference to
182"
INCORPORATING SHAREABLE BEHAVIORS,0.3761467889908257,conventional RL is that the training dataset is generated by ⇡mix
INCORPORATING SHAREABLE BEHAVIORS,0.3779816513761468,"i
and not ⇡i directly, thus beneﬁting
183"
INCORPORATING SHAREABLE BEHAVIORS,0.3798165137614679,"from shared behaviors through exploration without being limited by tasks with conﬂicting behaviors.
184"
INCORPORATING SHAREABLE BEHAVIORS,0.381651376146789,"Together, the Mixture of Policies, ⇡mix"
INCORPORATING SHAREABLE BEHAVIORS,0.3834862385321101,"i
enables multi-task exploration guided by Q-switch, the
185"
INCORPORATING SHAREABLE BEHAVIORS,0.3853211009174312,"current task’s objective. While it is possible for the Q-switch to choose a harmful action due to
186"
INCORPORATING SHAREABLE BEHAVIORS,0.3871559633027523,"error on an out-of-distribution action, it still helps exploration analogous to Q-learning. The agent
187"
INCORPORATING SHAREABLE BEHAVIORS,0.3889908256880734,"observes the environment rewards after taking this action and will update its Q-function. Thus, any
188"
INCORPORATING SHAREABLE BEHAVIORS,0.3908256880733945,"helpful behaviors seen in exploration are incorporated into ⇡i through policy optimization. In contrast,
189"
INCORPORATING SHAREABLE BEHAVIORS,0.3926605504587156,"conﬂicting or irrelevant behaviors, which represent an error in Qi’s estimation, are used to update
190"
INCORPORATING SHAREABLE BEHAVIORS,0.3944954128440367,"and correct Qi and thus the Q-switch. In addition, Q-switch starts as a uniform mixture but develops
191"
INCORPORATING SHAREABLE BEHAVIORS,0.3963302752293578,"a stronger preference for policy ⇡i as it becomes more proﬁcient in task i. Consequently, cross-task
192"
INCORPORATING SHAREABLE BEHAVIORS,0.3981651376146789,"behavior-sharing naturally decreases as ⇡i specializes and requires less exploration.
193"
EXPERIMENTS,0.4,"5
Experiments
194"
ENVIRONMENTS,0.4018348623853211,"5.1
Environments
195"
ENVIRONMENTS,0.4036697247706422,"To evaluate our proposed method, we experiment with multi-task designs in navigation and manip-
196"
ENVIRONMENTS,0.4055045871559633,"ulation environments shown in Figure 3. When evaluating the effectiveness of selective behavior
197"
ENVIRONMENTS,0.4073394495412844,"sharing, the complexity of these multi-task environments is determined not only by individual task
198"
ENVIRONMENTS,0.4091743119266055,"difﬁculty, more importantly, by the degree of similarity in behaviors between tasks. Thus to create
199"
ENVIRONMENTS,0.41100917431192663,"challenging benchmarks, we ensure each task set includes tasks with either conﬂicting or irrelevant
200"
ENVIRONMENTS,0.41284403669724773,"behavior. Further details on task setup and implementation are in Appendix Section A.2.
201"
ENVIRONMENTS,0.41467889908256883,"Multistage Reacher:
The agent is tasked to solve 5 tasks of controlling a 6 DoF Jaco arm to reach
202"
ENVIRONMENTS,0.41651376146788993,"multiple goals in an environment simulated in the MuJoCo physics engine (Todorov et al., 2012). In 4
203"
ENVIRONMENTS,0.41834862385321103,"out of the 5 tasks, the agent must reach 3 different sub-goals in order with some coinciding segments
204"
ENVIRONMENTS,0.42018348623853213,"between tasks. In the 5th task, the agent’s goal is to stay at its initial position for the entire episode.
205"
ENVIRONMENTS,0.42201834862385323,"The observation space does not include the goal location, which must be ﬁgured out from the reward.
206"
ENVIRONMENTS,0.42385321100917434,"Thus, for the same states, the 5th task directly conﬂicts with all the other tasks.
207"
ENVIRONMENTS,0.42568807339449544,"Maze Navigation:
In this environment, the point mass agent has to control its 2D velocity to
208"
ENVIRONMENTS,0.42752293577981654,"navigate through the maze and reach the goal, where both start and goal locations are ﬁxed in each
209"
ENVIRONMENTS,0.42935779816513764,"task. The observation consists of the agent’s current position and velocity. But, it lacks the goal
210"
ENVIRONMENTS,0.43119266055045874,"location, which should be inferred from the dense reward based on the distance to the goal. Based
211"
ENVIRONMENTS,0.43302752293577984,"on the environment proposed in Fu et al. (2020), we deﬁne 10 tasks with different start and goal
212"
ENVIRONMENTS,0.43486238532110094,"locations. The optimal paths for different tasks have segments that coincide and that directly conﬂict.
213"
ENVIRONMENTS,0.43669724770642204,Task 0
ENVIRONMENTS,0.43853211009174314,Task 1
ENVIRONMENTS,0.44036697247706424,Task 4: Stay
ENVIRONMENTS,0.44220183486238535,"(a) Multistage Reacher
(b) Maze Navigation"
ENVIRONMENTS,0.44403669724770645,door open
ENVIRONMENTS,0.44587155963302755,door close
ENVIRONMENTS,0.44770642201834865,drawer open
ENVIRONMENTS,0.44954128440366975,drawer close
ENVIRONMENTS,0.45137614678899085,(c) Meta-World
ENVIRONMENTS,0.45321100917431195,"Figure 3: Environments & Tasks: (a) Multistage Reacher. The agent must reach 3 ordered subgoals
in each task except Task 4, where the agent must stay at its initial position. See Appendix Table 1 for
goal locations. (b) Maze Navigation. The agent (green circle) must navigate through the maze to
reach the goal (red circle). Example paths for 4 other tasks are shown in orange. (c) Meta-World
Manipulation. Consisting of tasks: door open, door close, drawer open, drawer close."
ENVIRONMENTS,0.45504587155963305,"Meta-World Manipulation: We follow the modiﬁed 4-task shared-space setup of the Meta-World
214"
ENVIRONMENTS,0.45688073394495415,"environment (Yu et al., 2019) proposed in Yu et al. (2021). It places the door and drawer objects next
215"
ENVIRONMENTS,0.45871559633027525,"to each other on the same tabletop so that all 4 tasks (door open, door close, drawer open, drawer
216"
ENVIRONMENTS,0.46055045871559636,"close) are solvable in a simultaneous multi-task setup, making it amenable to our problem domain
217"
ENVIRONMENTS,0.46238532110091746,"(unlike the original MT10 task set which is built over separate environments). The observation space
218"
ENVIRONMENTS,0.46422018348623856,"consists of the robot’s proprioceptive state, the drawer handle state, the door handle state, and the
219"
ENVIRONMENTS,0.46605504587155966,"goal location. While there are no directly conﬂicting behaviors between tasks, there are irrelevant
220"
ENVIRONMENTS,0.46788990825688076,"behaviors. Thus, policies should learn to share behaviors when interacting with the same object while
221"
ENVIRONMENTS,0.46972477064220186,"ignoring irrelevant behavior from policies that only interact with the other object.
222"
BASELINES,0.47155963302752296,"5.2
Baselines
223"
BASELINES,0.47339449541284406,"We used Soft Actor-Critic (SAC) Haarnoja et al. (2018) for all models. We ﬁrst compare different
224"
BASELINES,0.47522935779816516,"forms of cross-task behavior-sharing in isolation from other forms of information-sharing. Then, we
225"
BASELINES,0.47706422018348627,"show how behavior-sharing complements parameter-sharing. For the non-parameter sharing version,
226"
BASELINES,0.47889908256880737,"we use the same architectures and SAC hyperparameters for policies across all baselines.
227"
BASELINES,0.48073394495412847,"• No-Shared-Behaviors consists of T RL agents where each agent is assigned one task and trained
228"
BASELINES,0.48256880733944957,"to solve it without any behavior sharing with other agents. In every training iteration, each agent
229"
BASELINES,0.48440366972477067,"collects the data for its own task and uses it for training.
230"
BASELINES,0.48623853211009177,"• Fully-Shared-Behaviors is a single SAC agent that learns one shared policy for all tasks, which
231"
BASELINES,0.48807339449541287,"outputs the same action for a given state regardless of task (thus naturally does parameter sharing
232"
BASELINES,0.48990825688073397,"too). For the fairness of comparison, we adjusted the size of the networks, batch size, and number
233"
BASELINES,0.4917431192660551,"of gradient updates to match those of other models with multiple agents.
234"
BASELINES,0.4935779816513762,"• Divide-and-Conquer RL (DnC) (Ghosh et al. (2018)) uses an ensemble of T policies that shares
235"
BASELINES,0.4954128440366973,"behaviors through policy distillation and regularization. We modiﬁed the method for multi-task
236"
BASELINES,0.4972477064220184,"learning by assigning each of the policies to a task and evaluating only the task-speciﬁc policy.
237"
BASELINES,0.4990825688073395,"• DnC (Regularization Only) is a no policy distillation variant of DnC we propose as a baseline.
238"
BASELINES,0.5009174311926605,"• UDS (Data Sharing) proposed in Yu et al. (2022) shares data between tasks, relabelling with
239"
BASELINES,0.5027522935779817,"minimum task reward. We modiﬁed this ofﬂine RL algorithm for our online set-up.
240"
BASELINES,0.5045871559633027,"• QMP (Ours) learns T policies sharing behaviors via Q-switch and mixture of policies.
241"
BASELINES,0.5064220183486239,"For further details on baselines and implementation, please refer to Appendix Section A.4.
242"
RESULTS,0.5082568807339449,"6
Results
243"
RESULTS,0.5100917431192661,"We conduct experiments to answer the following questions: (1) How does our method of selectively
244"
RESULTS,0.5119266055045871,"sharing exploratory behaviors compare with other forms of behavior sharing? (2) How crucial is
245"
RESULTS,0.5137614678899083,"adaptive behavior sharing? (3) Can QMP effectively identify shareable behaviors? (4) Is behavior
246"
RESULTS,0.5155963302752293,"sharing complementary to parameter sharing?
247"
X SAMPLES,0.5174311926605505,3x samples
X SAMPLES,0.5192660550458715,20% gain
X SAMPLES,0.5211009174311927,15% gain
X SAMPLES,0.5229357798165137,"QMP(Ours)
No-Shared-Behavior
Fully-Shared-Behavior
DnC (reg)
DnC
UDS"
X SAMPLES,0.5247706422018349,"Figure 4: Comparison of average multitask success rate, over 10 evaluation episodes per task and 5
seeds for each method. The dashed lines highlight the gains of our proposed method (QMP) over the
best baseline. QMP outperforms the baselines in terms of the rate of convergence (3x in Multistage
Reacher) and the task performance (20% in Maze Navigation and 15% Meta-World Manipulation)."
X SAMPLES,0.5266055045871559,"6.1
Baselines: How exploration sharing compares to other forms of behavior sharing?
248"
X SAMPLES,0.5284403669724771,"To verify QMP’s efﬁcacy as a behavior sharing mechanism, we compare against several behavior
249"
X SAMPLES,0.5302752293577981,"sharing baselines on 3 environments: Multistage Reacher (5 tasks), Maze Navigation (10 tasks), and
250"
X SAMPLES,0.5321100917431193,"Meta-World Manipulation (4 Tasks) in Figure 4. Overall, QMP outperforms other methods in terms
251"
X SAMPLES,0.5339449541284403,"of sample efﬁciency and ﬁnal performance across all task sets.
252"
X SAMPLES,0.5357798165137615,"In Multistage Reacher, our method reaches 100% success rate at 0.5 million environment steps, while
253"
X SAMPLES,0.5376146788990825,"DnC (reg.), the next best method, takes 3 times the number of steps to fully converge. The rest of the
254"
X SAMPLES,0.5394495412844037,"methods fail to attain the maximum success rate. The UDS baseline performs the worst, illustrating
255"
X SAMPLES,0.5412844036697247,"that data sharing can be ineffective without ground truth rewards. The Fully-Shared-Behaviors
256"
X SAMPLES,0.5431192660550459,"baseline performs similarly, highlighting the challenge of the conﬂicting behaviors between tasks.
257"
X SAMPLES,0.544954128440367,"In the Maze Navigation environment, we test the scalability of our method to a larger 10-task set.
258"
X SAMPLES,0.5467889908256881,"QMP successfully solves 8 tasks out of 10, while other methods plateau at around a 60% success rate.
259"
X SAMPLES,0.5486238532110091,"In the Meta-World Manipulation environment, our method reaches almost 100% success rate after 8
260"
X SAMPLES,0.5504587155963303,"million environment steps while other methods plateau at around 85%. This is signiﬁcant because
261"
X SAMPLES,0.5522935779816514,"this task set contains a majority of irrelevant behavior: between policies interacting with the door
262"
X SAMPLES,0.5541284403669725,"versus the drawer and between pulling on the object handles versus pushing. The fact that QMP
263"
X SAMPLES,0.5559633027522936,"still outperforms other methods validates our hypothesis that shared behaviors can be helpful for
264"
X SAMPLES,0.5577981651376147,"exploration even if the optimal behaviors are different. We note that the Fully-Shared-Behaviors
265"
X SAMPLES,0.5596330275229358,"baseline performs very well initially but quickly plateaus. The initial performance is likely due to
266"
X SAMPLES,0.5614678899082569,"the shared policy also beneﬁting from shared parameters across tasks, whereas the other methods
267"
X SAMPLES,0.563302752293578,"learn separate networks for each task. However, the tasks eventually diverge based on the object to be
268"
X SAMPLES,0.5651376146788991,"manipulated, making full behavior-sharing suboptimal.
269"
X SAMPLES,0.5669724770642202,"6.2
Ablations: How crucial is adaptive behavior sharing?
270"
X SAMPLES,0.5688073394495413,"Figure 5: An adaptive state and task
dependent Q-switch is crucial."
X SAMPLES,0.5706422018348624,"We look at the importance of an adaptive, state-dependent
271"
X SAMPLES,0.5724770642201835,"Q-switch by comparing QMP to two ablations where we
272"
X SAMPLES,0.5743119266055046,"replace the Q-switch with a ﬁxed sampling distribution over
273"
X SAMPLES,0.5761467889908257,"task policies to select which policy is used for exploration.
274"
X SAMPLES,0.5779816513761468,"• QMP-Uniform replaces the Q-switch with a uniform dis-
275"
X SAMPLES,0.5798165137614679,"tribution over policies to verify the importance of selective
276"
X SAMPLES,0.581651376146789,"and adaptive behavior sharing.
277"
X SAMPLES,0.5834862385321101,"• QMP-Domain-Knowledge replaces the Q-switch with a
278"
X SAMPLES,0.5853211009174312,"hand-crafted, ﬁxed policy distribution based on domain
279"
X SAMPLES,0.5871559633027523,"knowledge of the relationship between tasks (i.e., sam-
280"
X SAMPLES,0.5889908256880734,"pling probabilities proportional to the number of shared
281"
X SAMPLES,0.5908256880733945,"sub-goal sequences between tasks in Multistage Reacher,
282"
X SAMPLES,0.5926605504587156,"see Appendix A.2 for details) to verify the importance of
283"
X SAMPLES,0.5944954128440367,"adaptive and training-progress aware behavior sharing.
284"
X SAMPLES,0.5963302752293578,"(a) Cross-task sharing proportion (b) Behavior-sharing over training
(c) Behavior & parameter sharing"
X SAMPLES,0.5981651376146789,"Figure 6: (a) Proportion of shared behavior on Reacher Multistage averaged over training: Each cell
(row i, col j) represents sharing contribution of Policy j for Task i (diagonal zeroed out for contrast).
(b) Mixture probabilities of other policies over the course of training for Task 0 in Multistage Reacher:
behavior-sharing decreases as ⇡0 improves. Q-switch shares the least from the conﬂicting task Policy
4, shown in red. Full analysis for all tasks in Appendix Figure 9. (c) Combining our proposed method
with parameter sharing using a multi-head shared architecture (in pink) outperforms both components
on their own: SAC with shared parameters (purple) and our method without shared parameters (blue)."
X SAMPLES,0.6,"In Figure 5, we see QMP-Uniform reaches around 60% success rate, lower than the worst performing
285"
X SAMPLES,0.6018348623853211,"behavior sharing baseline, demonstrating that a poor choice of Q-switch can signiﬁcantly hinder
286"
X SAMPLES,0.6036697247706422,"learning in our framework. Uniformly randomly using other task policies for exploration can inject a
287"
X SAMPLES,0.6055045871559633,"signiﬁcant amount of low-reward data, making learning inefﬁcient.
288"
X SAMPLES,0.6073394495412844,"For QMP-Domain-Knowledge, we assign the probability of selecting ⇡j for Task i by the number of
289"
X SAMPLES,0.6091743119266055,"shared sub-goal sequences between Tasks i and j. QMP-Domain performs well initially but plateaus
290"
X SAMPLES,0.6110091743119266,"early. An improvement over QMP-Uniform shows the importance of task-dependent behavior sharing,
291"
X SAMPLES,0.6128440366972477,"while the performance deﬁcit from QMP suggests that state-dependent and training-adaptive sharing
292"
X SAMPLES,0.6146788990825688,"is necessary. Crucially, deﬁning such a speciﬁc domain-knowledge-based mixture of policies is
293"
X SAMPLES,0.6165137614678899,"generally impractical and requires knowing the tasks beforehand. While we speciﬁcally designed
294"
X SAMPLES,0.618348623853211,"the Multistage Reacher for this didactic analysis, such domain knowledge is exponentially harder to
295"
X SAMPLES,0.6201834862385321,"deﬁne for complex tasks, especially if we want a state-dependent mixture.
296"
X SAMPLES,0.6220183486238532,"6.3
Can QMP effectively identify shareable behaviors?
297"
X SAMPLES,0.6238532110091743,"Figure 6a analyzes the effectiveness of the Q-switch in identifying shareable behaviors by visualizing
298"
X SAMPLES,0.6256880733944954,"the average proportion that each task policy is selected for another task over the course of training in
299"
X SAMPLES,0.6275229357798165,"the Multistage Reacher environment. This average mixture composition statistic intuitively analyzes
300"
X SAMPLES,0.6293577981651376,"whether QMP identiﬁes shareable behaviors between similar tasks and avoids behavior sharing
301"
X SAMPLES,0.6311926605504588,"between conﬂicting or irrelevant tasks. As we expect, the Q-switch for Task 4 utilizes the least
302"
X SAMPLES,0.6330275229357798,"behavior from other policies (bottom row), and Policy 4 shares the least with other tasks (rightmost
303"
X SAMPLES,0.634862385321101,"column). Since the agent at Task 4 is rewarded to stay at its initial position, this behavior conﬂicts
304"
X SAMPLES,0.636697247706422,"with all the other goal-reaching tasks. Of the remaining tasks, Task 0 and 1 share the most similar
305"
X SAMPLES,0.6385321100917432,"goal sequence, so it is intuitive why they beneﬁt from shared exploration and are often selected by
306"
X SAMPLES,0.6403669724770642,"their respective Q-switches. Finally, unlike the other tasks, Task 3 receives only a sparse reward and
307"
X SAMPLES,0.6422018348623854,"therefore relies heavily on shared exploration. In fact, QMP demonstrates the greatest advantage in
308"
X SAMPLES,0.6440366972477064,"this task (Appendix Figure 8). Furthermore, we see that total behavior sharing decreases throughout
309"
X SAMPLES,0.6458715596330276,"training in all tasks (Figure 6b), which demonstrates a naturally arising preference in the Q-switch
310"
X SAMPLES,0.6477064220183486,"for its own task-speciﬁc policy as it becomes more proﬁcient.
311"
X SAMPLES,0.6495412844036698,"We qualitatively analyze behavior sharing by visualizing a rollout of QMP during training for the
312"
X SAMPLES,0.6513761467889908,"Drawer Open task in Meta-World Manipulation (Appendix Figure 10). We see that it switches
313"
X SAMPLES,0.653211009174312,"between all task policies as it approaches the drawer, uses drawer-speciﬁc policies as it grasps the
314"
X SAMPLES,0.655045871559633,"handle, and opening-speciﬁc policies as it pulls the drawer open. In conjunction with the overall
315"
X SAMPLES,0.6568807339449542,"results, this supports our claim that QMP can effectively identify shareable behaviors between tasks.
316"
X SAMPLES,0.6587155963302752,"6.4
Is behavior sharing complementary to parameter sharing?
317"
X SAMPLES,0.6605504587155964,"It is important that our method is compatible with other forms of multitask reinforcement learning
318"
X SAMPLES,0.6623853211009174,"that share different kinds of information, especially parameter sharing, which is very effective under
319"
X SAMPLES,0.6642201834862386,"low sample regimes (Borsa et al., 2016; Sodhani et al., 2021) as we saw in the initial performance
320"
X SAMPLES,0.6660550458715596,"of Fully-Shared-Behaviors in Meta-World Manipulation. While we use completely separate policy
321"
X SAMPLES,0.6678899082568808,"architectures for previous experiments to isolate the effect of behavior sharing, QMP is ﬂexible to
322"
X SAMPLES,0.6697247706422018,"any design where we can parameterize T task-speciﬁc policies. A commonly used technique to share
323"
X SAMPLES,0.671559633027523,"parameters in multi-task learning is to parameterize a single multi-task policy with a multi-head
324"
X SAMPLES,0.673394495412844,"network architecture. Each head of the network outputs the action distribution for its respective task.
325"
X SAMPLES,0.6752293577981652,"We can easily run QMP with such a parameter-sharing multi-head network architecture by running
326"
X SAMPLES,0.6770642201834862,SAC on the multi-head network and replacing the data collection policy with ⇡mix
X SAMPLES,0.6788990825688074,"i
.
327"
X SAMPLES,0.6807339449541284,"We compare the following methods on the Maze Navigation environment in Figure 6c.
328"
X SAMPLES,0.6825688073394496,"• Parameters Only: a multi-head SAC policy sharing parameters but not behaviors over tasks.
329"
X SAMPLES,0.6844036697247706,"• Behaviors Only: Separate task policy networks with QMP behavior sharing.
330"
X SAMPLES,0.6862385321100918,"• Parameters + Behaviors: a multi-head SAC network sharing behaviors via QMP exploration.
331"
X SAMPLES,0.6880733944954128,"We ﬁnd that sharing Parameters + Behaviors greatly improves the performance over both the Shared-
332"
X SAMPLES,0.689908256880734,"Parameters-Only baseline and Shared-Behaviors-Only variant of QMP. This demonstrates the additive
333"
X SAMPLES,0.691743119266055,"effect of these two forms of information sharing in MTRL. The agent initially beneﬁts from the
334"
X SAMPLES,0.6935779816513762,"sample efﬁciency gains of the multi-head parameter-sharing architecture, while behavior sharing
335"
X SAMPLES,0.6954128440366972,"with QMP accelerates the exploration via the selective mixture of policies to keep learning even after
336"
X SAMPLES,0.6972477064220184,"the parameter-sharing effect plateaus. This result demonstrates the compatibility between QMP and
337"
X SAMPLES,0.6990825688073394,"parameter sharing as key ingredients to sample efﬁcient MTRL.
338"
LIMITATIONS,0.7009174311926606,"7
Limitations
339"
LIMITATIONS,0.7027522935779816,Figure 7: QMP shares conservatively.
LIMITATIONS,0.7045871559633028,"When we have prior knowledge that a task set has lit-
340"
LIMITATIONS,0.7064220183486238,"tle or no conﬂicting behaviors, methods like DnC, with
341"
LIMITATIONS,0.708256880733945,"uniform behavior sharing and a tunable hyperparameter
342"
LIMITATIONS,0.710091743119266,"governing the strength of sharing, can fully leverage
343"
LIMITATIONS,0.7119266055045872,"shared behavior between tasks and work very efﬁciently.
344"
LIMITATIONS,0.7137614678899082,"In contrast, QMP does not assume a-priori that behaviors
345"
LIMITATIONS,0.7155963302752294,"are fully shareable and therefore shares behavior selec-
346"
LIMITATIONS,0.7174311926605504,"tively and adaptively as it learns the tasks. As a result, it
347"
LIMITATIONS,0.7192660550458716,"can be more conservative in the amount of shared behav-
348"
LIMITATIONS,0.7211009174311926,"ior and, thus, less sample efﬁcient compared to methods
349"
LIMITATIONS,0.7229357798165138,"speciﬁc to these task families. Essentially, QMP is a
350"
LIMITATIONS,0.7247706422018348,"robustly efﬁcient MTRL method applicable to a variety
351"
LIMITATIONS,0.726605504587156,"of task sets, but trades off on absolute performance on
352"
LIMITATIONS,0.728440366972477,"either end of the spectrum (i.e., fully non-conﬂicting or
353"
LIMITATIONS,0.7302752293577982,"fully conﬂicting tasks). We found this to be the case in a multi-task set where a Walker agent learns
354"
LIMITATIONS,0.7321100917431193,"different gaits with no directly conﬂicting behaviors like walking, balancing, and crawling: QMP
355"
LIMITATIONS,0.7339449541284404,"still outperforms no shared behavior or fully shared behavior baselines but DnC (Reg. only) works
356"
LIMITATIONS,0.7357798165137615,"best (see Figure 7 and Appendix Section A.3.3). However, in task sets with potentially conﬂicting
357"
LIMITATIONS,0.7376146788990826,"behaviors or where the similarity in task behaviors is not known, we believe QMP to be the best
358"
LIMITATIONS,0.7394495412844037,"option for robust multi-task behavior sharing as demonstrated in Section 6.1.
359"
CONCLUSION,0.7412844036697248,"8
Conclusion
360"
CONCLUSION,0.7431192660550459,"We introduce the problem of selective behavior sharing to improve exploration in MTRL for tasks
361"
CONCLUSION,0.744954128440367,"requiring differing optimal behaviors. We propose Q-switch Mixture of Policies (QMP), that in-
362"
CONCLUSION,0.7467889908256881,"corporates behaviors between tasks for exploration through a value-guided selection over behavior
363"
CONCLUSION,0.7486238532110092,"proposals. Experimental results on manipulation and navigation tasks demonstrate that our proposed
364"
CONCLUSION,0.7504587155963303,"method effectively learns to share behavior to improve the rate of convergence and task performance
365"
CONCLUSION,0.7522935779816514,"in task families even with conﬂicting or irrelevant behaviors, which highlights the importance of
366"
CONCLUSION,0.7541284403669725,"selective behavior sharing. We further show that our method is complementary to parameter sharing,
367"
CONCLUSION,0.7559633027522936,"a popular MTRL strategy, demonstrating the effectiveness of behavior sharing in conjunction with
368"
CONCLUSION,0.7577981651376147,"other forms of information sharing in MTRL. Promising future directions include extending explo-
369"
CONCLUSION,0.7596330275229358,"ration improvements of selective behavior sharing problems where tasks are not necessarily learned
370"
CONCLUSION,0.7614678899082569,"simultaneously, such as transfer learning and continual learning in RL.
371"
REFERENCES,0.763302752293578,"References
372"
REFERENCES,0.7651376146788991,"Bangaru, S. P., Suhas, J., and Ravindran, B. Exploration for multi-task reinforcement learning with
373"
REFERENCES,0.7669724770642202,"deep generative models. arXiv preprint arXiv:1611.09894, 2016.
374"
REFERENCES,0.7688073394495413,"Biewald, L. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/.
375"
REFERENCES,0.7706422018348624,"Software available from wandb.com.
376"
REFERENCES,0.7724770642201835,"Borsa, D., Graepel, T., and Shawe-Taylor, J. Learning shared representations in multi-task reinforce-
377"
REFERENCES,0.7743119266055046,"ment learning. arXiv preprint arXiv:1603.02041, 2016.
378"
REFERENCES,0.7761467889908257,"Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.
379"
REFERENCES,0.7779816513761468,"Openai gym, 2016a.
380"
REFERENCES,0.7798165137614679,"Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W.
381"
REFERENCES,0.781651376146789,"Openai gym. arXiv preprint arXiv:1606.01540, 2016b.
382"
REFERENCES,0.7834862385321101,"Chen, Z., Badrinarayanan, V., Lee, C.-Y., and Rabinovich, A. GradNorm: Gradient normalization for
383"
REFERENCES,0.7853211009174312,"adaptive loss balancing in deep multitask networks. In Dy, J. and Krause, A. (eds.), Proceedings of
384"
REFERENCES,0.7871559633027523,"the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
385"
REFERENCES,0.7889908256880734,"Learning Research, pp. 794–803. PMLR, 10–15 Jul 2018.
386"
REFERENCES,0.7908256880733945,"Daniel, C., Neumann, G., Kroemer, O., and Peters, J. Hierarchical relative entropy policy search.
387"
REFERENCES,0.7926605504587156,"Journal of Machine Learning Research, 17(93):1–50, 2016.
388"
REFERENCES,0.7944954128440367,"D’Eramo, C., Tateo, D., Bonarini, A., Restelli, M., Peters, J., et al. Sharing knowledge in multi-task
389"
REFERENCES,0.7963302752293578,"deep reinforcement learning. In International Conference on Learning Representations, 2020.
390"
REFERENCES,0.7981651376146789,"Devin, C., Gupta, A., Darrell, T., Abbeel, P., and Levine, S. Learning modular neural network
391"
REFERENCES,0.8,"policies for multi-task and multi-robot transfer. In IEEE International Conference on Robotics and
392"
REFERENCES,0.8018348623853211,"Automation. IEEE, 2017.
393"
REFERENCES,0.8036697247706422,"End, F., Akrour, R., Peters, J., and Neumann, G. Layered direct policy search for learning hierarchical
394"
REFERENCES,0.8055045871559633,"skills. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 6442–6448,
395"
REFERENCES,0.8073394495412844,"2017. doi: 10.1109/ICRA.2017.7989761.
396"
REFERENCES,0.8091743119266055,"Fifty, C., Amid, E., Zhao, Z., Yu, T., Anil, R., and Finn, C. Efﬁciently identifying task groupings for
397"
REFERENCES,0.8110091743119267,"multi-task learning. In Advances in Neural Information Processing Systems, 2021.
398"
REFERENCES,0.8128440366972477,"Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4rl: Datasets for deep data-driven
399"
REFERENCES,0.8146788990825689,"reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
400"
REFERENCES,0.8165137614678899,"garage contributors, T. Garage: A toolkit for reproducible reinforcement learning research. https:
401"
REFERENCES,0.818348623853211,"//github.com/rlworkgroup/garage, 2019.
402"
REFERENCES,0.8201834862385321,"Ghosh, D., Singh, A., Rajeswaran, A., Kumar, V., and Levine, S. Divide-and-conquer reinforcement
403"
REFERENCES,0.8220183486238533,"learning. In International Conference on Learning Representations, 2018.
404"
REFERENCES,0.8238532110091743,"Goyal, A., Sodhani, S., Binas, J., Peng, X. B., Levine, S., and Bengio, Y. Reinforcement learning
405"
REFERENCES,0.8256880733944955,"with competitive ensembles of information-constrained primitives. ArXiv, abs/1906.10667, 2019.
406"
REFERENCES,0.8275229357798165,"Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy
407"
REFERENCES,0.8293577981651377,"deep reinforcement learning with a stochastic actor. In International Conference on Machine
408"
REFERENCES,0.8311926605504587,"Learning, 2018.
409"
REFERENCES,0.8330275229357799,"Hessel, M., Soyer, H., Espeholt, L., Czarnecki, W., Schmitt, S., and van Hasselt, H. Multi-task
410"
REFERENCES,0.8348623853211009,"deep reinforcement learning with popart. In Proceedings of the AAAI Conference on Artiﬁcial
411"
REFERENCES,0.8366972477064221,"Intelligence, 2019.
412"
REFERENCES,0.8385321100917431,"Kaelbling, L. P. Learning to achieve goals. In IJCAI, volume 2, pp. 1094–8. Citeseer, 1993.
413"
REFERENCES,0.8403669724770643,"Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: A survey. Journal of
414"
REFERENCES,0.8422018348623853,"artiﬁcial intelligence research, 4:237–285, 1996.
415"
REFERENCES,0.8440366972477065,"Kalashnikov, D., Varley, J., Chebotar, Y., Swanson, B., Jonschkowski, R., Finn, C., Levine, S., and
416"
REFERENCES,0.8458715596330275,"Hausman, K. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint
417"
REFERENCES,0.8477064220183487,"arXiv:2104.08212, 2021a.
418"
REFERENCES,0.8495412844036697,"Kalashnikov, D., Varley, J., Chebotar, Y., Swanson, B., Jonschkowski, R., Finn, C., Levine, S., and
419"
REFERENCES,0.8513761467889909,"Hausman, K. Scaling up multi-task robotic reinforcement learning. In Conference on Robot
420"
REFERENCES,0.8532110091743119,"Learning, 2021b.
421"
REFERENCES,0.8550458715596331,"Kurin, V., De Palma, A., Kostrikov, I., Whiteson, S., and Kumar, M. P. In defense of the unitary
422"
REFERENCES,0.8568807339449541,"scalarization for deep multi-task learning. arXiv preprint arXiv:2201.04122, 2022.
423"
REFERENCES,0.8587155963302753,"Lee, Y., Sun, S.-H., Somasundaram, S., Hu, E. S., and Lim, J. J. Composing complex skills by learning
424"
REFERENCES,0.8605504587155963,"transition policies. In Proceedings of International Conference on Learning Representations, 2019.
425"
REFERENCES,0.8623853211009175,"URL https://openreview.net/forum?id=rygrBhC5tQ.
426"
REFERENCES,0.8642201834862385,"Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N. M. O., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.
427"
REFERENCES,0.8660550458715597,"Continuous control with deep reinforcement learning. CoRR, abs/1509.02971, 2015.
428"
REFERENCES,0.8678899082568807,"Liu, S., James, S., Davison, A. J., and Johns, E. Auto-lambda: Disentangling dynamic task relation-
429"
REFERENCES,0.8697247706422019,"ships. Transactions on Machine Learning Research, 2022.
430"
REFERENCES,0.8715596330275229,"Misra, I., Shrivastava, A., Gupta, A., and Hebert, M. Cross-stitch networks for multi-task learning.
431"
REFERENCES,0.8733944954128441,"In IEEE Conference on Computer Vision and Pattern Recognition, 2016.
432"
REFERENCES,0.8752293577981651,"Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P. Overcoming exploration in
433"
REFERENCES,0.8770642201834863,"reinforcement learning with demonstrations. In IEEE international conference on robotics and
434"
REFERENCES,0.8788990825688073,"automation, 2018.
435"
REFERENCES,0.8807339449541285,"Nam, T., Sun, S.-H., Pertsch, K., Hwang, S. J., and Lim, J. J. Skill-based meta-reinforcement learning.
436"
REFERENCES,0.8825688073394495,"In International Conference on Learning Representations, 2022.
437"
REFERENCES,0.8844036697247707,"Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
438"
REFERENCES,0.8862385321100917,"N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library.
439"
REFERENCES,0.8880733944954129,"Advances in neural information processing systems, 32, 2019.
440"
REFERENCES,0.8899082568807339,"Perez, E., Strub, F., de Vries, H., Dumoulin, V., and Courville, A. Film: Visual reasoning with a
441"
REFERENCES,0.8917431192660551,"general conditioning layer. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2018.
442"
REFERENCES,0.8935779816513761,"Rosenbaum, C., Cases, I., Riemer, M., and Klinger, T. Routing networks and the challenges of
443"
REFERENCES,0.8954128440366973,"modular and compositional computation. arXiv preprint arXiv:1904.12774, 2019.
444"
REFERENCES,0.8972477064220183,"Rusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins, G., Kirkpatrick, J., Pascanu, R., Mnih, V.,
445"
REFERENCES,0.8990825688073395,"Kavukcuoglu, K., and Hadsell, R. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.
446"
REFERENCES,0.9009174311926605,"Sasaki, F. and Yamashina, R. Behavioral cloning from noisy demonstrations. In International
447"
REFERENCES,0.9027522935779817,"Conference on Learning Representations, 2020.
448"
REFERENCES,0.9045871559633027,"Schaul, T., Borsa, D., Modayil, J., and Pascanu, R. Ray interference: a source of plateaus in deep
449"
REFERENCES,0.9064220183486239,"reinforcement learning. arXiv preprint arXiv:1904.11455, 2019.
450"
REFERENCES,0.908256880733945,"Sener, O. and Koltun, V. Multi-task learning as multi-objective optimization. Advances in neural
451"
REFERENCES,0.9100917431192661,"information processing systems, 31, 2018.
452"
REFERENCES,0.9119266055045872,"Sodhani, S., Zhang, A., and Pineau, J.
Multi-task reinforcement learning with context-based
453"
REFERENCES,0.9137614678899083,"representations. In Proceedings of the 38th International Conference on Machine Learning,
454"
REFERENCES,0.9155963302752294,"Proceedings of Machine Learning Research, 2021.
455"
REFERENCES,0.9174311926605505,"Standley, T., Zamir, A. R., Chen, D., Guibas, L., Malik, J., and Savarese, S. Which tasks should be
456"
REFERENCES,0.9192660550458716,"learned together in multi-task learning? In Proceedings of the 37th International Conference on
457"
REFERENCES,0.9211009174311927,"Machine Learning, volume 119, pp. 9120–9132, 2020.
458"
REFERENCES,0.9229357798165138,"Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018.
459"
REFERENCES,0.9247706422018349,"Teh, Y., Bapst, V., Czarnecki, W. M., Quan, J., Kirkpatrick, J., Hadsell, R., Heess, N., and Pascanu, R.
460"
REFERENCES,0.926605504587156,"Distral: Robust multitask reinforcement learning. In Advances in Neural Information Processing
461"
REFERENCES,0.9284403669724771,"Systems, 2017.
462"
REFERENCES,0.9302752293577982,"Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. 2012.
463"
REFERENCES,0.9321100917431193,"Tomov, M. S., Schulz, E., and Gershman, S. J. Multi-task reinforcement learning in humans. Nature
464"
REFERENCES,0.9339449541284404,"Human Behaviour, 5(6):764–773, 2021.
465"
REFERENCES,0.9357798165137615,"Vithayathil Varghese, N. and Mahmoud, Q. H. A survey of multi-task deep reinforcement learning.
466"
REFERENCES,0.9376146788990826,"Electronics, 2020.
467"
REFERENCES,0.9394495412844037,"Vuorio, R., Sun, S.-H., Hu, H., and Lim, J. J. Multimodal model-agnostic meta-learning via task-
468"
REFERENCES,0.9412844036697248,"aware modulation. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E.,
469"
REFERENCES,0.9431192660550459,"and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran
470"
REFERENCES,0.944954128440367,"Associates, Inc., 2019.
471"
REFERENCES,0.9467889908256881,"Watkins, C. J. and Dayan, P. Q-learning. Machine learning, 8:279–292, 1992.
472"
REFERENCES,0.9486238532110092,"Wilson, A., Fern, A., Ray, S., and Tadepalli, P. Multi-task reinforcement learning: a hierarchical
473"
REFERENCES,0.9504587155963303,"bayesian approach. In Proceedings of the 24th international conference on Machine learning, pp.
474"
REFERENCES,0.9522935779816514,"1015–1022, 2007.
475"
REFERENCES,0.9541284403669725,"Xu, Z., Wu, K., Che, Z., Tang, J., and Ye, J. Knowledge transfer in multi-task deep reinforcement
476"
REFERENCES,0.9559633027522936,"learning for continuous control. In Advances in Neural Information Processing Systems, 2020.
477"
REFERENCES,0.9577981651376147,"Yang, R., Xu, H., Wu, Y., and Wang, X. Multi-task reinforcement learning with soft modularization.
478"
REFERENCES,0.9596330275229358,"In Advances in Neural Information Processing Systems, 2020.
479"
REFERENCES,0.9614678899082569,"Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A
480"
REFERENCES,0.963302752293578,"benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot
481"
REFERENCES,0.9651376146788991,"Learning, 2019.
482"
REFERENCES,0.9669724770642202,"Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task
483"
REFERENCES,0.9688073394495413,"learning. In Advances in Neural Information Processing Systems, 2020.
484"
REFERENCES,0.9706422018348624,"Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine, S., and Finn, C. Conservative data sharing for
485"
REFERENCES,0.9724770642201835,"multi-task ofﬂine reinforcement learning. In Advances in Neural Information Processing Systems,
486"
REFERENCES,0.9743119266055046,"2021.
487"
REFERENCES,0.9761467889908257,"Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Finn, C., and Levine, S. How to leverage unlabeled
488"
REFERENCES,0.9779816513761468,"data in ofﬂine reinforcement learning. In International Conference on Machine Learning, 2022.
489"
REFERENCES,0.9798165137614679,"Zhang, C. and Wang, Z. Provably efﬁcient multi-task reinforcement learning with model transfer. In
490"
REFERENCES,0.981651376146789,"Advances in Neural Information Processing Systems, 2021.
491"
REFERENCES,0.9834862385321101,"Zhang, J., Li, S., and Zhang, C.
Cup:
Critic-guided policy reuse.
In Koyejo, S., Mo-
492"
REFERENCES,0.9853211009174312,"hamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neu-
493"
REFERENCES,0.9871559633027523,"ral Information Processing Systems, volume 35, pp. 27537–27548. Curran Associates,
494"
REFERENCES,0.9889908256880734,"Inc., 2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
495"
REFERENCES,0.9908256880733946,"b09df3a10e26204136540ca59bc5a646-Paper-Conference.pdf.
496"
REFERENCES,0.9926605504587156,"Zhang, Y. and Yang, Q. A survey on multi-task learning. IEEE Transactions on Knowledge and Data
497"
REFERENCES,0.9944954128440368,"Engineering, 2021.
498"
REFERENCES,0.9963302752293578,"Çelik, O., Zhou, D., Li, G., Becker, P., and Neumann, G. Specializing versatile skill libraries using
499"
REFERENCES,0.998165137614679,"local mixture of experts. In Conference on Robot Learning, 2021.
500"
