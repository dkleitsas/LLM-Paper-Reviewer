Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.005434782608695652,"We describe a curriculum learning framework capable of discovering optimal
curricula in addition to performing standard curriculum learning. We show that
this framework encompasses existing curriculum learning approaches such as
difﬁculty-based data sub-sampling, data pruning, and loss re-weighting. We em-
ploy the proposed framework to address the following key questions in curriculum
learning: (a) what is the best curriculum to train a given model on a given dataset?
and (b) what are the characteristics of optimal curricula for different datasets and
difﬁculty metrics? We show that our framework outperforms competing state-
of-the-art curriculum learning approaches in natural language inference and two
other text classiﬁcation tasks. Exhaustive experiments illustrate the generalizabil-
ity of the discovered curricula across the datasets and difﬁculty metrics."
INTRODUCTION,0.010869565217391304,"1
INTRODUCTION"
INTRODUCTION,0.016304347826086956,"Curriculum Learning (CL) is a technique in Machine Learning that mimics human education sys-
tems. To learn a complex subject, students must ﬁrst learn the foundational and basic materials
before learning more complex ones. Without a curriculum, learning may be intractable, inefﬁcient
and learners may never reach a full understanding of a topic due to lack of the required background
knowledge. Machine learning optimization through stochastic gradient descent trains models by ob-
serving example data instances. Some data instances are harder than others and require background
knowledge, which could be acquired by observing and being adept at easier examples before harder
ones. CL techniques seek to order examples according to their difﬁculty for training to generate bet-
ter models. It has been shown that CL improves performance in solving harder tasks, or in cases of
limited or noisy data (Wu et al., 2021). CL research has made signiﬁcant progress in the last decade
through the work of Bengio et al. (2009), although the principle of ordering training samples from
easier to harder was introduced in the 1990s (Elman, 1993; Sanger, 1994; Rohde & Plaut, 1999)."
INTRODUCTION,0.021739130434782608,"A curriculum can be deﬁned by ordering training samples based on their difﬁculty to learn. Given a
measure of difﬁculty, there are different types of CL approaches for ordering the data: sub-sampling
techniques, which sample the easiest or hardest data points at every iteration for training (Zhou
et al., 2020; Bengio et al., 2009; Xu et al., 2020; Guo et al., 2018; Platanios et al., 2019), sample
weighting techniques, which use the complete data at every iteration but weight data points differ-
ently according to their difﬁculty (Castells et al., 2020; Kumar et al., 2010; Jiang et al., 2015; 2018;
Zhou et al., 2020; Yang et al., 2019), and pruning techniques, which prune the hard or noisy samples
from the dataset prior to training (Northcutt et al., 2021; Guo et al., 2018). Sub-sampling methods
can be cumulative, exclusive or a combination of both. Cumulative approaches add new data points
to the ones that have previously been used for training. On the other hand, exclusive approaches
create a new subset of data at every training stage. Such methods can introduce samples from easy
to hard (Bengio et al., 2009; Kumar et al., 2010) or hard to easy, which can be an effective learning
strategy in some speciﬁc tasks (Kocmi & Bojar, 2017; Zhang et al., 2018; 2019). In addition, CL
methods may impose a curriculum by adjusting model’s capacity according to the difﬁculty of their
inputs (Karras et al., 2018; Sinha et al., 2020; Morerio et al., 2017) or schedule the order of tasks
in the context of multi-task learning (Caubri`ere et al., 2019; Saraﬁanos et al., 2017; Florensa et al.,
2017). Other CL approaches such as (Zhou et al., 2020; Saxena et al., 2019) use O(n), where n is
the number of training samples, extra parameters for learning curricula."
INTRODUCTION,0.02717391304347826,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03260869565217391,"(a) Effect of varying the parameters.
(b) Easy to Hard Curriculum."
INTRODUCTION,0.03804347826086957,"Figure 1: The effect of the rate of growth and shift parameters (r, s). (a) illustrates different weight-
ing strategies that can be obtained by varying the rate and shift parameters. (b) a speciﬁc parameter
conﬁguration for a curriculum that ﬁrst introduces easier training samples to a model, and then
gradually introduces medium and hard samples at 30% and 60% of the training epochs."
INTRODUCTION,0.043478260869565216,"Current CL approaches calculate difﬁculty scores for training samples based on the loss of a trained
model (Xu et al., 2020; Wu et al., 2021), trainable parameters that weight samples (Kumar et al.,
2010; Jiang et al., 2015; Castells et al., 2020), loss value during training (Wu et al., 2021), moving
average of loss during training (Zhou et al., 2020), transformations of the loss during training (Jiang
et al., 2018; Castells et al., 2020), and consistency in the correct classiﬁcation of samples (Amiri
et al., 2017; Xu et al., 2020). The difﬁculty scores will then be used to order samples for training."
INTRODUCTION,0.04891304347826087,"In this work, we focus on an alternative approach, where examples can be ordered based on prior
knowledge about their difﬁculty, e.g., object shape or orientation in image classiﬁcation (Bengio
et al., 2009). We will demonstrate a CL framework that encompasses existing CL approaches
through an effective and ﬂexible data partitioning and weighting scheme. Our framework provides a
new paradigm for selecting an ordering strategy. Instead of a pre-determined strategy, the framework
allows searching over the curriculum space to identify the best curriculum for a particular dataset
and model. It partitions training data into several groups of training samples, e.g. {easy, medium,
hard} samples, according to a difﬁculty scoring function. Parameterized weighting functions will
then be deﬁned for each data group to specify the weight of its samples during training. Each weight
function is controlled by two parameters, which can be set empirically or adjusted using Bayesian
optimization. In addition, the framework discovers optimal curricula by optimizing the parame-
ters of the weight functions (only 2 parameters per function) using a hyper-parameter optimization
algorithm. Furthermore, the curricula identiﬁed through this search provide useful insight about
the dataset, such as the relative importance of different samples or knowledge dependency between
samples, e.g. which samples should be learned ﬁrst."
INTRODUCTION,0.05434782608695652,"We begin by explaining our framework and showing how it is capable of approximating existing CL
approaches. Then, in the context of the proposed framework, we investigate curriculum discovery,
characteristics of discovered curricula, and generalizability of curricula with respect to their datasets."
CURRICULUM LEARNING FRAMEWORK,0.059782608695652176,"2
CURRICULUM LEARNING FRAMEWORK"
WEIGHTING FUNCTIONS,0.06521739130434782,"2.1
WEIGHTING FUNCTIONS"
WEIGHTING FUNCTIONS,0.07065217391304347,"We deﬁne the curriculum using generalized logistic functions (Richards, 1959) of the form:"
WEIGHTING FUNCTIONS,0.07608695652173914,"w(t; r, s) =
1
1 + exp(−r ∗(t −s)),
(1)"
WEIGHTING FUNCTIONS,0.08152173913043478,"where r ∈R is the rate-of-change parameter, which speciﬁes how fast the weight can increase
(r > 0) or decrease (r < 0); t ∈[0, 1] is the training progress (iteration number divided by max
iterations); and s ∈R shifts the pivot weight of the logistics function (w(.) = .5) to the left or right
such that at t = s the weight is 0.5. Figure 1a illustrates the effect of varying these parameters.
Greater absolute values for the rate parameter enforce greater slope and faster rate of changes in
weights, while greater values of the shift parameter enforce longer delays in reaching the pivot"
WEIGHTING FUNCTIONS,0.08695652173913043,Under review as a conference paper at ICLR 2022
WEIGHTING FUNCTIONS,0.09239130434782608,"(a) SNLI Entropy
(b) Alcohol Entropy
(c) Cancer Entropy"
WEIGHTING FUNCTIONS,0.09782608695652174,"(d) SNLI Loss
(e) Alcohol Loss
(f) Cancer Loss"
WEIGHTING FUNCTIONS,0.10326086956521739,"Figure 2: Distributions of entropy and loss of the three datasets used in the experiments. Figures
(a) - (c) show entropy, and (d) - (f) show loss. Samples of the easy class are to the left of the ﬁrst
vertical line, those of the medium class are between the two vertical lines, and samples of the hard
class are to the right of the second line."
WEIGHTING FUNCTIONS,0.10869565217391304,"weight of 0.5. These parameters provide ﬂexibility in controlling sample weights during training,
which is critical for deriving effective curricula. Given the weighting function, we encode prior
knowledge about sample difﬁculty into the training paradigm of our CL framework by splitting the
data into several partitions of increasing difﬁculty according to the prior knowledge. Then, we deﬁne
a weight function by a pair of parameters (r, s) for each partition,"
WEIGHTING FUNCTIONS,0.11413043478260869,"The proposed sample weighting framework provides ﬂexibility in sample-ordering according to
difﬁculty. It can be learned to approximate existing predetermind curricula. For example, it is
possible to begin with only easy instances or only difﬁcult instances (as an anti-curriculum) or a
combination of both. Figure 1b shows a speciﬁc conﬁguration for the logistic functions based on
standard CL (Bengio et al., 2009; Kumar et al., 2010), where training starts with easier samples and
gradually proceeds with harder ones. In addition, our approach enables discovering new data-driven
curricula from data."
WEIGHTING FUNCTIONS,0.11956521739130435,"Prior to training, the difﬁculty scores of samples are computed and each sample is assigned to
a difﬁculty class c ∈{easy, medium, hard}, see §2.2.
In addition, the hyper-parameters of
our three weight functions are optimized prior to training and kept ﬁxed throughout training
{(re, se), (rm, sm), (rh, sh)}. During training, the weighted loss is computed as follows."
WEIGHTING FUNCTIONS,0.125,"ˆli = w(t; rc, sc) ∗li
(2)"
WEIGHTING FUNCTIONS,0.13043478260869565,"Where li is the unweighted and instantaneous loss of instance i, ˆli is the weighted loss, t is the
current training iteration divided by the maximum number of iterations, c is the difﬁculty class of
instance i, and (rc, sc) are the corresponding rate and shift parameters for the difﬁculty class c."
SCORING FUNCTIONS,0.1358695652173913,"2.2
SCORING FUNCTIONS"
SCORING FUNCTIONS,0.14130434782608695,"Ground-truth labels for many datasets are often obtained through human annotation and crowd-
sourcing. This is achieved by collecting multiple annotations per data sample and aggregating the
results, typically by majority voting. We use sample-level annotator disagreement to deﬁne a dif-
ﬁculty score for each sample using Shannon entropy (Shannon, 2001), where higher disagreement
among annotators corresponds to higher sample difﬁculty. Entropy is a natural measure of difﬁ-
culty (for human population) and may serve as a reliable prior knowledge for partitioning data by"
SCORING FUNCTIONS,0.14673913043478262,Under review as a conference paper at ICLR 2022
SCORING FUNCTIONS,0.15217391304347827,difﬁculty. Entropy of each sample xi is calculated as H(xi) = −P
SCORING FUNCTIONS,0.15760869565217392,"l pc log pc where c is a class
category and pc is the fraction of annotators who chose label c for the sample. The use of entropy
is supported by Nie et al. (2020), who studied the correlation between human agreement and model
performance and reported a consistent positive correlation between model accuracy and level of
human agreement, showing that model performs better on samples with a high level of agreement."
SCORING FUNCTIONS,0.16304347826086957,"Furthermore, training loss contains valuable information about difﬁculty with respect to the model
(learner), which may be different among architectures and tasks, and indicative of the model’s spe-
ciﬁc needs. However, loss at a particular step (e.g., ﬁnal loss) is dictated by the stochastic gradient
descent and mini-batching dynamics and therefore is not a good indicator of difﬁculty (Zhou et al.,
2020; Wu et al., 2021). Using a baseline model, trained with no curriculum and with default hyper-
parameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use
the average loss to estimate sample difﬁculty. In our experiments, we obtain twenty observations of
the loss and compute the average for each instance. Such an estimation is supported by Zhou et al.
(2020) who showed that the moving average of a sample’s instantaneous loss is a good metric for
difﬁculty. Partitioning the data into three groups of increasing difﬁculty can be done using difﬁculty
score percentiles, or 1-dimensional k-means clustering of the scores. Examples of data partitions
using entropy and loss are shown in Figure 2."
ENCOMPASSING FRAMEWORK,0.16847826086956522,"2.3
ENCOMPASSING FRAMEWORK"
ENCOMPASSING FRAMEWORK,0.17391304347826086,"Curriculum learning approaches can be divided into three categories depending on how they process
their input data: approaches that identify and prune noisy data that may hurt performance (Northcutt
et al., 2021; Guo et al., 2018; Rooyen et al., 2015; Patrini et al., 2016; Chen et al., 2019), approaches
that use different sub-samples of data during training (Bengio et al., 2009; Zhou et al., 2020; Xu
et al., 2020; Platanios et al., 2019; Zhou & Bilmes, 2018), and approaches that re-weight loss ac-
cording to sample difﬁculty, choosing to emphasize either easy or hard samples (Castells et al., 2020;
Jiang et al., 2015; 2018; Yang et al., 2019; Saxena et al., 2019). The framework presented in this
paper is capable of representing all of the three approaches."
ENCOMPASSING FRAMEWORK,0.1793478260869565,"First, data pruning can be done by assigning negative values to the rate change and shift parameters
in our framework, r and s in Eq. 2. A negative r causes the weight to approach zero, and a negative
s shifts the curve to the left, so the curve reaches zero before training begins. The framework also
allows ﬂexibility in pruning: by setting a small positive s, the noisy data can be seen by the model
for a short amount of time before reaching zero weight, or by setting a positive r and a large positive
s the noisy data will only be seen at the end of training (after it stabilizes)."
ENCOMPASSING FRAMEWORK,0.18478260869565216,"Second, data sub-sampling can be represented by the weight going to zero or increasing from zero at
different stages of training. For instance, Figure 1b illustrates a curriculum where the easy samples
are sub-sampled in the beginning, and harder samples are introduced at later stages."
ENCOMPASSING FRAMEWORK,0.19021739130434784,"Third, we display in Figure 3 the conﬁdence scores assigned to our data by three loss re-weighting
approaches. The results are generated by our implementations of the three approaches, evaluated
on the three datasets introduced in §3.1, where each model runs with ﬁve random seeds. The parti-
tioning of easy, medium, and hard is according to the entropy-based difﬁculty classes, as described
in §2.2. We record the average weight (conﬁdence) assigned to each class. The result is averaged
over all runs, and the shaded area is the 95% conﬁdence interval. The approaches that estimate
sample conﬁdence based on loss (Castells et al., 2020; Zhou et al., 2020; Kumar et al., 2010; Jiang
et al., 2015; Felzenszwalb et al., 2009) tend to generate monotonic curves over the course of training
because training loss tends to be non-increasing at every step. Therefore, the conﬁdence scores as-
signed by these re-weighting approaches follow a monotonic curve that can be approximated by our
weighting functions (§2.1, Figure 1). We note that although the weight scale of SuperLoss (Castells
et al., 2020) in Figure 3a is larger than one, this model can still be represented by our CL framework
because the increased scale corresponds to a scaling of the learning rate, as shown below:"
ENCOMPASSING FRAMEWORK,0.1956521739130435,θt = θt−1 −η∇1 n X
ENCOMPASSING FRAMEWORK,0.20108695652173914,"i
σili = θt−1 −(η · σmax)∇1 n X i"
ENCOMPASSING FRAMEWORK,0.20652173913043478,"σi
σmax
li,
(3)"
ENCOMPASSING FRAMEWORK,0.21195652173913043,"where li and σi are the loss and conﬁdence of sample i, respectively. Therefore, our framework can
also represent CL approaches with a conﬁdence scale larger then one."
ENCOMPASSING FRAMEWORK,0.21739130434782608,Under review as a conference paper at ICLR 2022
ENCOMPASSING FRAMEWORK,0.22282608695652173,"(a)
The
SuperLoss
(Castells
et al., 2020)"
ENCOMPASSING FRAMEWORK,0.22826086956521738,"(b) Self-paced Learning (Kumar
et al., 2010)"
ENCOMPASSING FRAMEWORK,0.23369565217391305,"(c)
Hard
Negative
Min-
ing
(Felzenszwalb
et
al.,
2009)"
ENCOMPASSING FRAMEWORK,0.2391304347826087,"Figure 3: Conﬁdence assignment to samples in our datasets by three curriculum learning approaches
that re-weight their loss functions by computing conﬁdence scores for samples. The x-axis is the
epoch number, and y-axis is the average weight assigned to instances of different difﬁculty. Blue
(solid) is easy, orange (dashed) is medium, and green (dash-dot) is hard. The shaded area is the 95%
conﬁdence interval (CI) over three datasets with ﬁve random seeds each. The curves are monotonic
for most parts, and can be approximated by the monotonic curves generated by our framework."
CURRICULUM DISCOVERY,0.24456521739130435,"2.4
CURRICULUM DISCOVERY"
CURRICULUM DISCOVERY,0.25,"We employ a hyperparameter optimization framework (Akiba et al., 2019) to ﬁnd the optimal value
of the curriculum parameters (r, s) for each difﬁculty class. Using this method, we can learn a data-
driven curriculum beyond what we could manually design through empirical settings or a choice
among the limited ordering strategies. To optimize the three pairs of parameters, we use the Tree-
structured Parzen Estimator (TPE) sampling algorithm (Bergstra et al., 2011). Unlike grid or random
search (Bergstra & Bengio, 2012), TPE traverses the parameter space by estimating the parameters
that are most probable to perform better on a trial, based on the previous trials. TPE deﬁnes two
Gaussian Mixture Models, l(x) and g(x) which are formed using the best and remaining observed
parameters, respectively. TPE selects the parameter x with a high probability under l(x) and low
probability under g(x), i.e. arg maxx l(x)/g(x). This choice of sampling algorithm greatly speeds
up the search of our curriculum parameters."
CURRICULUM DISCOVERY,0.2554347826086957,"We note that the discovered curricula are optimal within this framework, constrained by the method
of data partitioning and the class of weight functions. We argue that the proposed framework is able
to approximate curricula deﬁned by existing CL approaches, and outperform existing CL approaches
across several datasets."
EXPERIMENTS,0.2608695652173913,"3
EXPERIMENTS"
DATASETS,0.266304347826087,"3.1
DATASETS"
DATASETS,0.2717391304347826,"We evaluate our approach on three datasets that contain multiple annotations for each sample. First,
the Stanford Natural Language Inference (SNLI) benchmark (Bowman et al., 2015), which contains
550k training samples, 10k development samples, and 10k test samples. Within the training samples,
there are 36.7k samples annotated by 5 workers and 2.6k annotated by 4 workers, which we use for
our experiments and refer to as SNLI “full.” Furthermore, in order to control for variance due
to imbalanced difﬁculty classes, we create a balanced subset of the data. As shown in Figure 2a
(notice the y-axis scale), SNLI is highly imbalanced in entropy classes. The data is downsampled by
selecting an equal number of samples from each entropy-class. The balanced subset contains a total
of 2.3k samples, i.e. 774 samples in each entropy class. On the other hand, loss classes are fairly
distributed, see Figure 2d. The downsampled subset contains both entropy and loss classes which
will be used in experiments."
DATASETS,0.27717391304347827,"The Alcohol dataset (Amiri et al., 2018; Weitzman et al., 2020) has been developed to obtain
population-level statistics of alcohol use reports through social media. The dataset consists of more
than 9k tweets. Given an alcohol relevant tweet, annotators are asked to determine if it reports ﬁrst-"
DATASETS,0.2826086956521739,Under review as a conference paper at ICLR 2022
DATASETS,0.28804347826086957,"person alcohol use, and if yes, the intensity of the drinking (light vs. heavy), the context of drinking
(social vs. individual), and the time of drinking (past, present, or future). All samples in the dataset
are labeled by at least three workers, including over 1.3k samples labeled by ﬁve or more workers.
We deﬁne a multi-class classiﬁcation task for this dataset based on alcohol relevance, intensity and
context of drinking. The categories and their data distributions are reported in Appendix A. We ran-
domly split the data into 5.4k training samples, 1.8k development samples, and 1.8k test samples.
The balanced version of the training set contains a total of 2.5k training samples, i.e. 863 samples
in each entropy class."
DATASETS,0.29347826086956524,"The Cancer dataset has been developed to obtain population-level statistics of cancer patients; it
contains 3.8k Reddit posts. Annotators are asked to determine if the post describes the experience
of a cancer patient, the type of cancer, and the relation of the author of the post to the patient. We
deﬁne a multi-class classiﬁcation task based on post relevance and caner type. The categories and
their data distribution are reported in Appendix A. All samples are labeled by at least three workers,
including about 1k labeled by at least ﬁve. We randomly split the data into around 2.2k training
samples, 765 development samples, and 765 test samples. The balanced version of the training set
contains a total number of 1.7k sample, i.e. 578 samples in each entropy class."
DATASETS,0.29891304347826086,"We note that the datasets are signiﬁcantly different in average document length, ranging from 10
(SNLI), to 15 (Alcohol) to 174 (Cancer) words. This variation can induce signiﬁcant variance in the
created models."
BASELINES,0.30434782608695654,"3.2
BASELINES"
BASELINES,0.30978260869565216,"We compare the performance of our CL approach with the following state-of-the-art approaches.
SuperLoss (SL) (Castells et al., 2020) is a CL approach that deﬁnes a task-agnostic conﬁdence-
aware loss function. It infers the conﬁdences of instances from the instance loss with minimal
cost, providing a closed-form solution function for estimating conﬁdence. It up-weights samples
with smaller loss values (easier instances), while down-weights those with larger loss values (harder
ones) (Figure 3a). MentorNet (Jiang et al., 2018) uses an auxiliary network to generate a weight for
training samples at every training iteration. It incorporates additional signals such as epoch number
and instance loss history to learn data-driven curricula and is particularly strong against noisy data.
Difﬁculty Prediction (DP) (Yang et al., 2019) deﬁnes a difﬁculty score based on multi-annotator
data, and weight samples according to the following formula w = 1 −α(di −τDP )/(1 −τDP ),
where di is the sample difﬁculty and τ is a pre-deﬁned threshold."
BASELINES,0.31521739130434784,"As discussed before, our approach employs two scoring functions (§ 2.2) and two curriculum con-
ﬁgurations for each dataset. A curriculum conﬁguration refers to a particular setting of the six
parameters controlling the three weight curves (§ 2.1). The two scoring functions are labeled as
Loss and Ent (entropy). In addition, the ﬁrst curriculum conﬁguration is a gradually increasing ap-
proach in Figure 1b named inc., this conﬁguration is applied identically to all models. The second
conﬁguration is the specialized conﬁguration (sp.) that is obtained through hyper-parameter search
as discussed in § 2.4."
SETTINGS,0.32065217391304346,"3.3
SETTINGS"
SETTINGS,0.32608695652173914,"We tune the parameters λ of SL and α and τDP of DP using development data. The optimal values
found are λ = 1.2, α = 0.9 and τDP is set dynamically upon loading the dataset to the 50-percentile
difﬁculty value. Following Castells et al. (2020), we set τSL (instances with li > τSL are considered
hard) to the moving average of the loss in all experiments."
SETTINGS,0.33152173913043476,"We use the transformers python package (Wolf et al., 2020), using the bert-base-uncased
model for SNLI and Cancer, and twitter-roberta-base for Alcohol. We set learning rate
to 1e −5, batch size to 16, epochs to 10 (we conﬁrm that this number of iterations is sufﬁcient for
all models to converge), and the optimizer to Adam (Kingma & Ba, 2017). For each experiment,
we train ﬁve models using ﬁve random seeds applied to both pytorch and numpy. Additionally,
during all data pre-processing, splitting, and sub-sampling, the random seed is set to 0, and a single
NVIDIA A100 40GB GPU is used for training. The development set is used to determine the best
training step which is used for the ﬁnal evaluation."
SETTINGS,0.33695652173913043,Under review as a conference paper at ICLR 2022
SETTINGS,0.3423913043478261,"Figure 4: Overall accuracy averaged over three datasets with ﬁve random seeds. Loss and Ent
indicate curricula that partition the data based on difﬁculty classes determined by loss and entropy
respectively (§2.2). inc is the easy to hard curriculum shown in Figure 1b, sp is the specialized
curriculum obtained by curriculum discovery (§2.4), which is different for each dataset."
SETTINGS,0.34782608695652173,"In addition, we conduct a hyper-parameter search over the (r, s) for each weight function or difﬁculty
classes (easy, medium, and hard). We set the search space of r to be from −10 to 10 with a step of
2, and of s to be −0.5 to 1.5 with a step of 0.25. We observe that changes smaller than this step size
have little effect on performance. This search space consists of 11 possible values for r and 9 for s,
for a total of 970k combinations. The search is run for at least 100 trials, as compared to over 1000
trials by random search, using the method described in §2.4. Each trial is run with 5 random seeds
and the result is averaged. The search objective is to maximize accuracy over development data."
PERFORMANCE GAIN FROM CURRICULA DISCOVERY,0.3532608695652174,"3.4
PERFORMANCE GAIN FROM CURRICULA DISCOVERY"
PERFORMANCE GAIN FROM CURRICULA DISCOVERY,0.358695652173913,"Results are shown in Figure 4. Accuracy is averaged over the six datasets (full and balanced version
of each dataset). The gradually increasing curriculum (inc) achieves a signiﬁcant improvement over
No-CL using either of the scoring functions while being a static, off-the-shelf curriculum conﬁgura-
tion. This improvement shows the effectiveness of our approach of partitioning the data using gen-
eralized logistic functions (§2.1). Moreover, both (inc) and the specialized (sp) curricula obtained
through curriculum discovery perform signiﬁcantly better than the state-of-the-art CL approaches.
In our three datasets, loss as a scoring function performs better than entropy on average. This is ex-
pected as loss values can capture sample difﬁculty with respect to the downstream learner (model),
as apposed to entropy values which do not take into account the model."
PERFORMANCE GAIN FROM CURRICULA DISCOVERY,0.3641304347826087,"Appendix B includes further breakdown of the results by dataset and accuracy across samples of
different difﬁculty."
CHARACTERISTICS OF DISCOVERED CURRICULA,0.3695652173913043,"3.5
CHARACTERISTICS OF DISCOVERED CURRICULA"
CHARACTERISTICS OF DISCOVERED CURRICULA,0.375,"Figure 5 shows the mean and 95% CI of the top 25 performing conﬁgurations on our datasets and
scoring functions. We observe several insightful patterns: the resulting curricula are non-trivial and
greatly differ from the known strategies reported in current literature, such as gradually increasing
difﬁculty or anti-curriculum. In addition, the weights of hard samples tend to approach zero, sup-
porting the hypothesis that either these instances are too difﬁcult for the models to learn or they are
noisy. The results support the principle of pruning techniques because noisy samples induce more
noise than useful signal; in several plots, the weight of hard samples increases only at the end of
training after the model stabilizes. In addition, in SNLI and Alcohol easy samples carry the most
signiﬁcant weight, unlike Cancer, where easy samples are down-weighted early during the training.
These weighting patterns reveal the relative importance of samples in each dataset. Finally, the full
SNLI dataset with entropy-class partitions provides useful information. Figure 2a shows that en-
tropy classes are highly imbalanced, with hard samples being much fewer than easy ones. In the"
CHARACTERISTICS OF DISCOVERED CURRICULA,0.3804347826086957,Under review as a conference paper at ICLR 2022
CHARACTERISTICS OF DISCOVERED CURRICULA,0.3858695652173913,"(a) S-B-E
(b) S-B-L
(c) S-F-E
(d) S-F-L"
CHARACTERISTICS OF DISCOVERED CURRICULA,0.391304347826087,"(e) A-B-E
(f) A-B-L
(g) A-F-E
(h) A-F-L"
CHARACTERISTICS OF DISCOVERED CURRICULA,0.3967391304347826,"(i) C-B-E
(j) C-B-L
(k) C-F-E
(l) C-F-L"
CHARACTERISTICS OF DISCOVERED CURRICULA,0.40217391304347827,"Figure 5: Each caption is composed of the ﬁrst character of the name of a dataset: {SNLI, Alcohol,
Cancer}, the type of the dataset {Balanced or Full}, and the difﬁculty score used {Entropy, Loss}.
The x-axis is the training progress and y-axis is the conﬁdence assigned to samples of a difﬁculty-
class. The blue line (circle marker) is easy, orange line (x marker) is medium, and green line (dia-
mond marker) is hard. The solid line is the mean of the top 25 performing conﬁgurations for each
dataset and scoring function pair, and the shaded area represents the 95% CI."
CHARACTERISTICS OF DISCOVERED CURRICULA,0.4076086956521739,"optimal curriculum shown in Figure 5c, hard samples are assigned weights around 0.5, unlike the
three other cases of SNLI. We attribute this result to the reduced presence/effect of hard samples."
GENERALIZABLE CURRICULA,0.41304347826086957,"3.6
GENERALIZABLE CURRICULA"
GENERALIZABLE CURRICULA,0.41847826086956524,"Figure 6 shows the accuracy obtained when training using different curriculum conﬁgurations from
Figure 5. Each cell in the ﬁgure is the average result of 5 seeds. We observe common characteristics
among datasets that cause the curriculum to be transferable between them. First, the top three
conﬁgurations are all products of down-sampled, balanced datasets. Second, the curricula obtained
using the small balanced datasets generalize and achieve high performance on the large datasets.
This is useful as it allows performing the hyper-parameter search much faster and cheaper on smaller
datasets, providing evidence that the framework can be applied to large datasets by searching for
a curriculum on a small subset of the data. Third, the inc curriculum is available off-the-shelf
and performs consistently well across datasets and scoring functions. Fourth, as noted previously,
instances of the Cancer dataset consist of long paragraphs, causing high variance in models trained
using the dataset. Consequently, the curricula obtained using the Cancer and loss as measure of
difﬁculty are of lower quality and perform the worst."
GENERALIZABLE CURRICULA,0.42391304347826086,"An extended version of Figure 6 is included in Appendix C with results of models trained with
balanced versions of datasets."
GENERALIZABLE CURRICULA,0.42934782608695654,Under review as a conference paper at ICLR 2022
GENERALIZABLE CURRICULA,0.43478260869565216,"Figure 6: Using the same notation as Figure 5, the x-axis lists different curriculum conﬁgurations
from the increasing curriculum inc (Figure 1b), to No-CL, to curricula discovered using a particular
dataset and scoring function. The y-axis lists models that are trained using each conﬁguration. For
example, the cell at the intersection of row ”S-F-L” and column ”A-F-E” represents a model trained
on SNLI full dataset that is partitioned by loss as a measure of difﬁculty, using the curriculum
discovered for the full Alcohol dataset partitioned by entropy (Figure 5g). Each row of the table is
normalized to match the scales of the different models."
CONCLUSION AND FUTURE WORK,0.44021739130434784,"4
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.44565217391304346,"We introduce an effective curriculum learning (CL) framework that employs prior knowledge about
sample-level difﬁculty in its training paradigm, and effectively creates and explores a curricula space
for curricula discovery and model generalizability. The proposed framework partitions its input data
into three groups of increasing difﬁculty, deﬁnes three parameterized logistic weight functions to
weight the loss of samples in each of the three groups, and provides the capability of tuning the pa-
rameters of the weight function to discover new curricula performing better than existing baselines.
We demonstrate that this framework is capable of representing major CL approaches. In addition, an
important advantage of our approach is that the curricula that it discovers for smaller and balanced
datasets work well on larger datasets, across the three datasets that we experimented with. The pro-
posed research opens a new paradigm in CL by removing the limitations imposed by selecting a
single CL strategy, and instead, using a ﬂexible framework to discover optimal curricula for given
datasets and models, and extract valuable insights about the data. Our work has the limitation of
using monotonic logistic functions because the weight curves can not take arbitrary forms. This
challenge can be addressed in future work by adding an extra set of logistic functions and having
every weight function be a linear combination of two or more logistic functions, which makes it
possible to represent non-monotonic functions. Nevertheless, we have demonstrated that major CL
approaches that estimate difﬁculty as a function of training loss result in monotonic curves because
of gradient descent that causes training loss to be non-increasing."
CONCLUSION AND FUTURE WORK,0.45108695652173914,"There are several promising areas for future work. These include approaches for learning new dif-
ﬁculty indicators that are centered on data (e.g., text difﬁculty), prioritizing medium level instances
and those with greatest progress in training, and developing challenge datasets that contain diverse
data samples with different level of difﬁculty."
CONCLUSION AND FUTURE WORK,0.45652173913043476,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.46195652173913043,"5
ETHICS STATEMENT"
ETHICS STATEMENT,0.4673913043478261,"This investigation included publicly available yet sensitive data from social media, developed for an
important task: obtaining population-level statistics about different public health issues. Our work
does not access, use, or release any personal data, and only textual data is used for experiments."
REPRODUCIBILITY STATEMENT,0.47282608695652173,"6
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.4782608695652174,"The source code is included as supplementary material. It provides the required details for data
processing, hyper-parameter setting, random seed setting, model evaluation, etc."
REFERENCES,0.483695652173913,REFERENCES
REFERENCES,0.4891304347826087,"Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM
SIGKDD international conference on knowledge discovery & data mining, pp. 2623–2631, 2019."
REFERENCES,0.4945652173913043,"Hadi Amiri, Timothy Miller, and Guergana Savova. Repeat before forgetting: Spaced repetition for
efﬁcient and effective training of neural networks. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), pp. 2401–2410, Copenhagen, Den-
mark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1255.
URL https://aclanthology.org/D17-1255."
REFERENCES,0.5,"Hadi Amiri, Kara M Magane, Lauren E Wisk, Guergana Savova, and Elissa R Weitzman. Toward
large-scale and multi-facet analysis of ﬁrst person alcohol drinking. In American Medical Infor-
matics Association (AMIA), 2018."
REFERENCES,0.5054347826086957,"Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
ACM International Conference Proceeding Series, volume 382, pp. 1–8, New York, New York,
USA, jul 2009. ACM Press. ISBN 9781605585161. doi: 10.1145/1553374.1553380."
REFERENCES,0.5108695652173914,"James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research (JMLR), 13(2), 2012."
REFERENCES,0.5163043478260869,"James Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter
optimization. Advances in Neural Information Processing Systems (NIPS), 24, 2011."
REFERENCES,0.5217391304347826,"Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large an-
notated corpus for learning natural language inference. In Conference on Empirical Methods in
Natural Language Processing, EMNLP 2015, pp. 632–642. Association for Computational Lin-
guistics (ACL), 2015."
REFERENCES,0.5271739130434783,"Thibault Castells, Philippe Weinzaepfel, and Jerome Revaud. Superloss: A generic loss for robust
curriculum learning. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020."
REFERENCES,0.532608695652174,"Antoine Caubri`ere, Natalia Tomashenko, Antoine Laurent, Emmanuel Morin, Nathalie Camelin, and
Yannick Est`eve. Curriculum-based transfer learning for an effective end-to-end spoken language
understanding and domain portability. In 20th Annual Conference of the International Speech
Communication Association (InterSpeech), pp. 1198–1202, 2019."
REFERENCES,0.5380434782608695,"Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing
deep neural networks trained with noisy labels. In International Conference on Machine Learn-
ing, pp. 1062–1070. PMLR, 2019."
REFERENCES,0.5434782608695652,"Jeffrey L Elman. Learning and development in neural networks: The importance of starting small.
Cognition, 48(1):71–99, 1993."
REFERENCES,0.5489130434782609,"Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection
with discriminatively trained part-based models. IEEE transactions on pattern analysis and ma-
chine intelligence, 32(9):1627–1645, 2009."
REFERENCES,0.5543478260869565,Under review as a conference paper at ICLR 2022
REFERENCES,0.5597826086956522,"Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse cur-
riculum generation for reinforcement learning. In Conference on robot learning, pp. 482–495.
PMLR, 2017."
REFERENCES,0.5652173913043478,"Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R Scott, and
Dinglong Huang. Curriculumnet: Weakly supervised learning from large-scale web images. In
Proceedings of the European Conference on Computer Vision (ECCV), pp. 135–150, 2018."
REFERENCES,0.5706521739130435,"Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and
Noah A Smith. Annotation artifacts in natural language inference data. In 2018 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL HLT 2018, pp. 107–112. Association for Computational Linguistics (ACL),
2018."
REFERENCES,0.5760869565217391,"Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-paced cur-
riculum learning. In Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015."
REFERENCES,0.5815217391304348,"Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International Conference
on Machine Learning (ICML), pp. 2304–2313. PMLR, 2018."
REFERENCES,0.5869565217391305,"Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. In International Conference on Learning Representations,
2018."
REFERENCES,0.592391304347826,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
REFERENCES,0.5978260869565217,"Tom Kocmi and Ondˇrej Bojar. Curriculum learning and minibatch bucketing in neural machine
translation. In Proceedings of the International Conference Recent Advances in Natural Language
Processing, RANLP 2017, pp. 379–386, 2017."
REFERENCES,0.6032608695652174,"M Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models.
Advances in Neural Information Processing Systems (NIPS), 23:1189–1197, 2010."
REFERENCES,0.6086956521739131,"Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Rene Vidal, and Vittorio Murino. Curriculum
dropout. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 3564–3572.
IEEE Computer Society, 2017."
REFERENCES,0.6141304347826086,"Yixin Nie, Xiang Zhou, and Mohit Bansal. What can we learn from collective human opinions on
natural language inference data? In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 9131–9143, 2020."
REFERENCES,0.6195652173913043,"Curtis Northcutt, Lu Jiang, and Isaac Chuang. Conﬁdent learning: Estimating uncertainty in dataset
labels. Journal of Artiﬁcial Intelligence Research, 70:1373–1411, 2021."
REFERENCES,0.625,"Giorgio Patrini, Frank Nielsen, Richard Nock, and Marcello Carioni. Loss factorization, weakly su-
pervised learning and label noise robustness. In Proceedings of the 33rd International Conference
on International Conference on Machine Learning-Volume 48, pp. 708–717, 2016."
REFERENCES,0.6304347826086957,"Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, and Tom M
Mitchell. Competence-based curriculum learning for neural machine translation. In Proceed-
ings of the Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL-HLT, pp. 1162–1172, 2019."
REFERENCES,0.6358695652173914,"Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme.
Hypothesis only baselines in natural language inference. NAACL HLT 2018, pp. 180, 2018."
REFERENCES,0.6413043478260869,"FJ Richards. A ﬂexible growth function for empirical use. Journal of experimental Botany (JXB),
10(2):290–301, 1959."
REFERENCES,0.6467391304347826,"Douglas LT Rohde and David C Plaut. Language acquisition in the absence of explicit negative
evidence: How important is starting small? Cognition, 72(1):67–109, 1999."
REFERENCES,0.6521739130434783,Under review as a conference paper at ICLR 2022
REFERENCES,0.657608695652174,"Brendan van Rooyen, Aditya Krishna Menon, and Robert C Williamson. Learning with symmetric
label noise: the importance of being unhinged. In Proceedings of the 28th International Confer-
ence on Neural Information Processing Systems-Volume 1, pp. 10–18, 2015."
REFERENCES,0.6630434782608695,"Terence D Sanger. Neural network learning control of robot manipulators using gradually increasing
task difﬁculty. IEEE transactions on Robotics and Automation, 10(3):323–333, 1994."
REFERENCES,0.6684782608695652,"Nikolaos Saraﬁanos, Theodore Giannakopoulos, Christophoros Nikou, and Ioannis A Kakadiaris.
Curriculum learning for multi-task classiﬁcation of visual attributes. In Proceedings of the IEEE
International Conference on Computer Vision Workshops, pp. 2608–2615, 2017."
REFERENCES,0.6739130434782609,"Shreyas Saxena, Oncel Tuzel, and Dennis DeCoste. Data parameters: A new family of parameters
for learning a differentiable curriculum. Advances in Neural Information Processing Systems, 32:
11095–11105, 2019."
REFERENCES,0.6793478260869565,"Claude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE mobile
computing and communications review, 5(1):3–55, 2001."
REFERENCES,0.6847826086956522,"Samarth Sinha, Animesh Garg, and Hugo Larochelle.
Curriculum by smoothing.
Advances in
Neural Information Processing Systems, 33, 2020."
REFERENCES,0.6902173913043478,"Elissa R Weitzman, Kara M Magane, Po-Hua Chen, Hadi Amiri, Timothy S Naimi, and Lauren E
Wisk. Online searching and social media to detect alcohol use risk at population scale. American
journal of preventive medicine (ACPM), 58(1):79–88, 2020."
REFERENCES,0.6956521739130435,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural
language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP): System Demonstrations, pp. 38–45, Online, October 2020. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6."
REFERENCES,0.7010869565217391,"Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work?
In International
Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/
forum?id=tW4QEInpni."
REFERENCES,0.7065217391304348,"Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. Cur-
riculum learning for natural language understanding. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics (ACL), pp. 6095–6104, 2020."
REFERENCES,0.7119565217391305,"Yinfei Yang, Oshin Agarwal, Chris Tar, Byron C Wallace, and Ani Nenkova. Predicting annotation
difﬁculty to improve task routing and model performance for biomedical information extraction.
In Proceedings of the Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, NAACL-HLT, pp. 1471–1480, 2019."
REFERENCES,0.717391304347826,"Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Mar-
tindale, Paul McNamee, Kevin Duh, and Marine Carpuat. An empirical exploration of curriculum
learning for neural machine translation. arXiv preprint arXiv:1811.00739, 2018."
REFERENCES,0.7228260869565217,"Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh.
Curriculum learning for domain adaptation in neural machine translation. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT), pp. 1903–1915, 2019."
REFERENCES,0.7282608695652174,"Tianyi Zhou and Jeff Bilmes. Minimax curriculum learning: Machine teaching with desirable difﬁ-
culties and scheduled diversity. In International Conference on Learning Representations, 2018."
REFERENCES,0.7336956521739131,"Tianyi Zhou, Shengjie Wang, and Jeff A Bilmes. Curriculum learning by dynamic instance hardness.
Advances in Neural Information Processing Systems (NeurIPS), 33, 2020."
REFERENCES,0.7391304347826086,Under review as a conference paper at ICLR 2022
REFERENCES,0.7445652173913043,"A
DATA CATEGORIES DISTRIBUTION"
REFERENCES,0.75,"Class
Count
(no)
5,325
(yes, light use, individual)
1,464
(yes, heavy use, individual)
964
(yes, not sure, individual)
457
(yes, heavy use, other)
423
(yes, heavy use, group)
284
(yes, light use, group)
161
Total
9,078"
REFERENCES,0.7554347826086957,"(a) Alcohol
Class
Count
(irrelevant, no patient experience)
1,996
(relevant, breast cancer)
617
(relevant, colon cancer)
444
(relevant, brain cancer)
284
(irrelevant, none of the above)
251
(irrelevant, other cancer types)
162
(irrelevant, news related to cancer)
70
Total
3,824"
REFERENCES,0.7608695652173914,(b) Cancer
REFERENCES,0.7663043478260869,Table 1: Statistics of the Alcohol and Cancer datasets.
REFERENCES,0.7717391304347826,Under review as a conference paper at ICLR 2022
REFERENCES,0.7771739130434783,"B
ACCURACY BREAKDOWN"
REFERENCES,0.782608695652174,The accuracy achieved on each dataset using each approach is shown in Figure 7.
REFERENCES,0.7880434782608695,"Models that achieve a high accuracy may be generating correct predictions for a high percentage of
easy samples while failing to correctly predict the output of the medium and hard samples. Standard
evaluation benchmarks often contains artifacts that make it east to correctly predict the label of
some samples (Gururangan et al., 2018; Poliak et al., 2018). Therefore, it is important to closely
analyze the model’s performance on harder instances. We break down the accuracy of each difﬁculty
class (with entropy-class partitioning) for balanced, full, and all datasets in Figures 8, 9, and 10,
respectively. Our approach is exceptionally powerful in predicting samples of medium difﬁculty.
Furthermore, the accuracy achieved on hard samples is almost equal by all approaches, including
No-CL, supporting the hypothesis that those samples tend to be noisy or inaccurately labeled."
REFERENCES,0.7934782608695652,"(a) Full datasets.
(b) Balanced datasets."
REFERENCES,0.7989130434782609,Figure 7: Accuracy of different CL approaches on each dataset.
REFERENCES,0.8043478260869565,Under review as a conference paper at ICLR 2022
REFERENCES,0.8097826086956522,"(a) Easy
(b) Medium"
REFERENCES,0.8152173913043478,"(c) Hard
(d) Balanced Accuracy"
REFERENCES,0.8206521739130435,Figure 8: Average over balanced datasets.
REFERENCES,0.8260869565217391,"(a) Easy
(b) Medium"
REFERENCES,0.8315217391304348,"(c) Hard
(d) Balanced Accuracy"
REFERENCES,0.8369565217391305,Figure 9: Average over full datasets.
REFERENCES,0.842391304347826,Under review as a conference paper at ICLR 2022
REFERENCES,0.8478260869565217,"(a) Easy
(b) Medium"
REFERENCES,0.8532608695652174,"(c) Hard
(d) Balanced Accuracy"
REFERENCES,0.8586956521739131,Figure 10: Average over all datasets.
REFERENCES,0.8641304347826086,Under review as a conference paper at ICLR 2022
REFERENCES,0.8695652173913043,"C
EXTENDED CONFIGURATION GENERALIZABLITY EXPERIMENTS"
REFERENCES,0.875,"Figure 11: An extended version of Figure 6 including experiments on balanced versions of the
datasets."
REFERENCES,0.8804347826086957,Figure 11 shows the full results including evaluation on the balanced datasets.
REFERENCES,0.8858695652173914,Under review as a conference paper at ICLR 2022
REFERENCES,0.8913043478260869,"D
FULL CURRICULUM SEARCH RESULTS"
REFERENCES,0.8967391304347826,"This section shows the top 25 performing conﬁgurations on each dataset scoring function pair. Blue
lines (circle marker) is easy, orange (x marker) is medium, and green (diamond marker) is hard.
Above each plot is the development set accuracy. The plot legend contains the parameters (r, s) for
each class."
REFERENCES,0.9021739130434783,Figure 12: S-B-E
REFERENCES,0.907608695652174,Under review as a conference paper at ICLR 2022
REFERENCES,0.9130434782608695,Figure 13: S-B-L
REFERENCES,0.9184782608695652,Figure 14: S-F-E
REFERENCES,0.9239130434782609,Under review as a conference paper at ICLR 2022
REFERENCES,0.9293478260869565,Figure 15: S-F-L
REFERENCES,0.9347826086956522,Figure 16: A-B-E
REFERENCES,0.9402173913043478,Under review as a conference paper at ICLR 2022
REFERENCES,0.9456521739130435,Figure 17: A-B-L
REFERENCES,0.9510869565217391,Figure 18: A-F-E
REFERENCES,0.9565217391304348,Under review as a conference paper at ICLR 2022
REFERENCES,0.9619565217391305,Figure 19: A-F-L
REFERENCES,0.967391304347826,Figure 20: C-B-E
REFERENCES,0.9728260869565217,Under review as a conference paper at ICLR 2022
REFERENCES,0.9782608695652174,Figure 21: C-B-L
REFERENCES,0.9836956521739131,Figure 22: C-F-E
REFERENCES,0.9891304347826086,Under review as a conference paper at ICLR 2022
REFERENCES,0.9945652173913043,Figure 23: C-F-L
