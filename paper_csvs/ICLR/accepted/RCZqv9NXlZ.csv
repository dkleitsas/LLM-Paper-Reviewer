Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016286644951140066,"Ofﬂine reinforcement learning (RL) shows promise of applying RL to real-world
problems by effectively utilizing previously collected data. Most existing ofﬂine
RL algorithms use regularization or constraints to suppress extrapolation error for
actions outside the dataset. In this paper, we adopt a different framework, which
learns the V -function instead of the Q-function to naturally keep the learning pro-
cedure within the ofﬂine dataset. To enable effective generalization while main-
taining proper conservatism in ofﬂine learning, we propose Expectile V -Learning
(EVL), which smoothly interpolates between the optimal value learning and be-
havior cloning. Further, we introduce implicit planning along ofﬂine trajectories
to enhance learned V -values and accelerate convergence. Together, we present
a new ofﬂine method called Value-based Episodic Memory (VEM). We provide
theoretical analysis for the convergence properties of our proposed VEM method,
and empirical results in the D4RL benchmark show that our method achieves su-
perior performance in most tasks, particularly in sparse-reward tasks. Our code is
public online at https://github.com/YiqinYang/VEM."
INTRODUCTION,0.003257328990228013,"1
INTRODUCTION"
INTRODUCTION,0.004885993485342019,"Despite the great success of deep reinforcement learning (RL) in various domains, most current al-
gorithms rely on interactions with the environment to learn through trial and error. In real-world
problems, particularly in risky and safety-crucial scenarios, interactions with the environment can
be expensive and unsafe, and only ofﬂine collected datasets are available, such as the expert demon-
stration or previously logged data. This growing demand has led to the emergence of ofﬂine rein-
forcement learning (ofﬂine RL) to conduct RL in a supervised manner."
INTRODUCTION,0.006514657980456026,"The main challenge of ofﬂine RL comes from the actions out of the dataset’s support (Kumar et al.,
2019; 2020). The evaluation of these actions that do not appear in the dataset relies on the gener-
alization of the value network, which may exhibit extrapolation error (Fujimoto et al., 2019). This
error can be magniﬁed through bootstrapping, leading to severe estimation errors. A rapidly devel-
oping line of recent work (Fujimoto et al., 2019; Kumar et al., 2020; Ghasemipour et al., 2021; Yang
et al., 2021) utilizes various methods to constrain optimistic estimation on unseen actions, such as
restricting available actions with a learned behavior model (Fujimoto et al., 2019) or penalizing the
unseen actions with additional regularization (Kumar et al., 2020). However, conﬁning learning
within the distribution of the dataset can be insufﬁcient for reducing extrapolation errors."
INTRODUCTION,0.008143322475570033,"Another line of methods, on the contrary, uses the returns of the behavior policy as the signal for
policy learning, as adopted in Wang et al. (2018); Peng et al. (2019); Chen et al. (2020). By doing
so, they keep the value learning procedure completely within the dataset. However, the behavior
policy of the dataset can be imperfect and insufﬁcient to guide policy learning. To achieve a trade-
off between imitation learning and optimal value learning while conﬁnes learning within the dataset,"
INTRODUCTION,0.009771986970684038,"*Equal contribution. Listing order is random.
†Equal advising."
INTRODUCTION,0.011400651465798045,Published as a conference paper at ICLR 2022
INTRODUCTION,0.013029315960912053,"Trajs
0
1
2
3
…
T-1
T
Trans
Value"
INTRODUCTION,0.014657980456026058,"Memory
Episodic Memory"
INTRODUCTION,0.016286644951140065,Regularization/
INTRODUCTION,0.017915309446254073,Constraint
INTRODUCTION,0.019543973941368076,"Q-based Offline RL
V-based Episodic Memory"
INTRODUCTION,0.021172638436482084,"Improvement
Improvement"
INTRODUCTION,0.02280130293159609,Expectile V-Learning
INTRODUCTION,0.024429967426710098,"Update Memory
Evaluation
Update Target"
INTRODUCTION,0.026058631921824105,"Figure 1: The diagram of algorithms. The left side denotes the general Q-based ofﬂine RL methods.
The right side is the framework of our proposed approach (VEM). Q-based methods learns boot-
strapped Q-values, but requires additional constraint or penalty for actions out of the dataset. Our
method, on the contrary, learns bootstrapped V -values while being completely conﬁned within the
dataset without any regularization."
INTRODUCTION,0.02768729641693811,"we propose Expectile V -learning (EVL), which is based on a new expectile operator that smoothly
interpolates between the Bellman expectation operator and optimality operator."
INTRODUCTION,0.029315960912052116,"To better solve long-horizon and sparse-reward tasks, we further propose using value-based plan-
ning to improve the advantage estimation for policy learning. We adopt an implicit memory-based
planning scheme that strictly plans within ofﬂine trajectories to compute the advantages effectively,
as proposed in recent advances in episodic memory-based methods (Hu et al., 2021). Together, we
present our novel framework for ofﬂine RL, Value-based Episodic Memory (VEM), which uses ex-
pectile V -learning to approximate the optimal value with ofﬂine data and conduct implicit memory-
based planning to further enhance advantage estimation. With the properly learned advantage func-
tion, VEM trains the policy network in a simple regression manner. We demonstrate our algorithm
in Figure 1, and a formal description of our algorithm is provided in Algorithm 1."
INTRODUCTION,0.030944625407166124,"The contributions of this paper are threefold. First, we present a new ofﬂine V -learning method,
EVL, and a novel ofﬂine RL framework, VEM. EVL learns the value function through the trade-offs
between imitation learning and optimal value learning. VEM uses a memory-based planning scheme
to enhance advantage estimation and conduct policy learning in a regression manner. Second, we
theoretically analyze our proposed algorithm’s convergence properties and the trade-off between
contraction rate, ﬁxed-point bias, and variance. Speciﬁcally, we show that VEM is provably con-
vergent and enjoys a low concentration rate with a small ﬁxed-point bias. Finally, we evaluate our
method in the ofﬂine RL benchmark D4RL (Fu et al., 2020). Comparing with other baselines, VEM
achieves superior performance, especially in the sparse reward tasks like AntMaze and Adroit. The
ablation study shows that VEM yields accurate value estimates and is robust to extrapolation errors."
BACKGROUND,0.03257328990228013,"2
BACKGROUND"
BACKGROUND,0.03420195439739414,"Preliminaries. We consider a Markov Decision Process (MDP) M deﬁned by a tuple (S, A, P, r, γ),
where S is the state space, A is the action space, P(· | s, a) : S × A × S →R is the transition
distribution function, r(s, a) : S×A →R is the reward function and γ ∈[0, 1) is the discount factor.
We say an environment is deterministic if P(s′ | s, a) = δ(s′ = f(s, a)) for some deterministic
transition function f, where δ(·) is the Dirac function. The goal of an RL agent is to learn a policy
π : S × A →R, which maximizes the expectation of a discounted cumulative reward: J (π) =
Es0∼ρ0,at∼π(·|st),st+1∼P (·|st,at) [P∞
t=0 γtr(st, at)], where ρ0 is the distribution of the initial states."
BACKGROUND,0.035830618892508145,"Value-based Ofﬂine Reinforcement Learning Methods.
Current ofﬂine RL methods can be
roughly divided into two categories according to types of learned value function: Q-based and
V -based methods. Q-based methods, such as BCQ (Fujimoto et al., 2019), learn Q-function for
policy learning and avoid selecting unfamiliar actions via constraints or penalty. On the contrary,
V -based methods (Peng et al., 2019; Siegel et al., 2020; Chen et al., 2020) learns the value of be-
havior policy V µ(s) with the trajectories in the ofﬂine dataset D and update policy as a regression
problem. Based on the learned V -function, V -based methods like AWR (Peng et al., 2019) updates
the policy using advantage-weighted regression, where each state-action pair is weighted according"
BACKGROUND,0.03745928338762215,Published as a conference paper at ICLR 2022
BACKGROUND,0.03908794788273615,to the exponentiated advantage:
BACKGROUND,0.04071661237785016,"max
φ
Jπ(φ) = E(st,at)∼D [log πφ(at | st) exp (Rt −V µ(st))] .
(1)"
BACKGROUND,0.04234527687296417,"Episodic Memory-Based Methods. Inspired by psychobiology, episodic memory-based methods
store experiences in a non-parametric table to fast retrieve past successful strategies when encoun-
tering similar states. Model-free episodic control (Blundell et al., 2016a) updates the memory table
by taking the maximum return R(s, a) among all rollouts starting from same state-action pair (s, a).
Hu et al. (2021) proposes Generalizable Episodic Memory, which extends this idea to the continuous
domain, and proposes updating formula with a parametric memory QEM
θ
."
METHOD,0.043973941368078175,"3
METHOD"
METHOD,0.04560260586319218,"In this section, we describe our novel ofﬂine method, value-based episodic memory, as depicted in
Figure 1. VEM uses expectile V -learning (EVL) to learn V -functions while conﬁnes value learning
within the dataset to reduce extrapolation error. EVL uses an expectile operator that interpolates
between Bellman expectation operator and optimality operator to balance behavior cloning and op-
timal value learning. Further, VEM integrates memory-based planning to improve the advantage
estimation and accelerate the convergence of EVL. Finally, generalized advantage-weighted learn-
ing is used for policy learning with enhanced advantage estimation. A formal description for the
VEM algorithm is shown in Algorithm 1 in Appendix A.1."
EXPECTILE V-LEARNING,0.04723127035830619,"3.1
EXPECTILE V-LEARNING"
EXPECTILE V-LEARNING,0.048859934853420196,"To achieve a balance between behavior cloning and optimal value learning, we consider the Bellman
expectile operator deﬁned as follows:"
EXPECTILE V-LEARNING,0.050488599348534204,"((T µ
τ )V )(s) := arg min
v
Ea∼µ(·|s)

τ[δ(s, a)]2
+ + (1 −τ)[δ(s, a)]2
−

(2)"
EXPECTILE V-LEARNING,0.05211726384364821,"where µ is the behavior policy, δ(s, a) = Es′∼P (·|s,a)[r(s, a) + γV (s′) −v] is the expected one-
step TD error, [·]+ = max(·, 0) and [·]−= min(·, 0). This operator resembles the expectile statis-
tics (Newey & Powell, 1987; Rowland et al., 2019) and hence its name. We can see that when
τ = 1/2, this operator is reduced to Bellman expectation operator, while when τ →1, this operator
approaches Bellman optimality operator, as depicted in Lemma 3."
EXPECTILE V-LEARNING,0.05374592833876222,"0.3
0.4
0.5
0.6
0.7
0.8
0.9
τ −100 0 100 200 300"
EXPECTILE V-LEARNING,0.05537459283387622,Mean State Value noise
EXPECTILE V-LEARNING,0.057003257328990226,"0.0
0.1
0.2
0.3
0.4"
EXPECTILE V-LEARNING,0.05863192182410423,"Figure 2: Trade-offs of EVL between gen-
eralization and conservatism in a random
MDP. The green line shows the optimal
value and the blue line shows the value of
behavior policy. The curve is averaged over
20 MDPs."
EXPECTILE V-LEARNING,0.06026058631921824,"We use the following toy example to further illustrate
the trade-offs achieved by EVL. Consider a random
generated MDP. When the operator can be applied
exactly, the Bellman optimality operator is sufﬁcient
to learn the optimal value V ∗. However, applying
operators with an ofﬂine dataset raises a noise on the
actual operator due to the estimation error with ﬁnite
and biased data. We simulate this effect by adding
random Gaussian noise to the operator. Applying the
optimality operator on ofﬂine datasets can lead to se-
vere overestimation due to the maximization bias and
bootstrapping. The value estimation learned by EVL,
on the contrary, achieves a trade-off between learning
optimal policy and behavior cloning and can be close
to the optimal value with proper chosen τ, as depicted
in Figure 2. The noise upon the operator largely de-
pends on the size of the dataset. Estimation error can
be signiﬁcant with insufﬁcent data. In this case, we
need a small τ to be conservative and be close to be-
havior cloning. When the dataset is large and we are
able to have an accurate estimation for the operator,
we can use a larger τ to recover the optimal policy. By adjusting τ, the expectile operator can ac-
commodate variant types of datasets. However, the expectile operator in Equation 2 does not have a"
EXPECTILE V-LEARNING,0.06188925081433225,Published as a conference paper at ICLR 2022
EXPECTILE V-LEARNING,0.06351791530944625,"closed-form solution. In practice, we consider the one-step gradient expectile operator"
EXPECTILE V-LEARNING,0.06514657980456026,"((Tg)µ
τ V )(s) = V (s) + 2αEa∼µ(·|s)

τ [δ(s, a)]+ + (1 −τ)[δ(s, a)]−

,
(3)"
EXPECTILE V-LEARNING,0.06677524429967427,"where α is the step-size. Please refer to Appendix B.1 for the detailed derivation. For notational
convenience, we use T µ
τ to denote the one-step gradient expectile operator (Tg)µ
τ hereafter."
EXPECTILE V-LEARNING,0.06840390879478828,"We consider the case where the dynamics are nearly-deterministic like robotic applications, and
we remove the expectation over the next states in the operator. This leads to a practical algorithm,
Expectile V -Learning, where we train the value network to minimize the following loss:"
EXPECTILE V-LEARNING,0.07003257328990228,"JV (θ) = E(s,a,s′)∼D"
EXPECTILE V-LEARNING,0.07166123778501629,"
ˆV (s) −Vθ (s)
2
,"
EXPECTILE V-LEARNING,0.0732899022801303,"ˆV (s) = Vθ′(s) + 2α

τ [δ(s, a, s′)]+ + (1 −τ)[δ(s, a, s′)]−

,
(4)"
EXPECTILE V-LEARNING,0.0749185667752443,"where ˆV is the target value after applying one-step gradient expectile operator and δ(s, a, s′) =
r(s, a) + γVθ′(s′) −Vθ′(s). V -function and the target ˆV -function are parameterized by θ and θ′,
respectively. EVL is guaranteed to converge with concentration rate γτ = 1−2(1−γ)α max{τ, 1−
τ}. Please refer to Section 4 for a detailed analysis."
IMPLICIT MEMORY-BASED PLANNING,0.07654723127035831,"3.2
IMPLICIT MEMORY-BASED PLANNING"
IMPLICIT MEMORY-BASED PLANNING,0.0781758957654723,"Although EVL reduces the extrapolation error, it is still a challenging problem to bootstrap over
long time horizons due to estimation errors with a ﬁxed dataset. Therefore, we propose using value-
based planning to conduct bootstrapping more efﬁciently. We adopt an implicit memory-based
planning scheme that strictly plans within ofﬂine trajectories to avoid over-optimistic estimations in
the planning phase. This is aligned with recent advances in episodic memory-based methods (Hu
et al., 2021), but we conduct this planning on expectile V -values rather than Q-values. Speciﬁcally,
we compare the best return so far along the trajectory with the value estimates ˆV and takes the
maximum between them to get the augmented return ˆRt:"
IMPLICIT MEMORY-BASED PLANNING,0.07980456026058631,"ˆRt =
rt + γ max( ˆRt+1, ˆV (st+1)),
if
t < T,
rt,
if
t = T,
(5)"
IMPLICIT MEMORY-BASED PLANNING,0.08143322475570032,"where t denotes steps along the trajectory, T is the episode length, and ˆV is generalized from similar
experiences. This procedure is conducted recursively from the last step to the ﬁrst step along the
trajectory, forming an implicit planning scheme within the dataset to aggregate experiences along
and across trajectories. Further, the back-propagation process in Equation 5 can be unrolled and
rewritten as follows:"
IMPLICIT MEMORY-BASED PLANNING,0.08306188925081433,"ˆRt =
max
0<n≤nmax
ˆVt,n,
ˆVt,n ="
IMPLICIT MEMORY-BASED PLANNING,0.08469055374592833,"(
rt + γ ˆVt+1,n−1
if
n > 0,
ˆV (st)
if
n = 0,
(6)"
IMPLICIT MEMORY-BASED PLANNING,0.08631921824104234,"where n denotes different length of rollout steps and ˆVt,n = 0 for n > T."
GENERALIZED ADVANTAGE-WEIGHTED LEARNING,0.08794788273615635,"3.3
GENERALIZED ADVANTAGE-WEIGHTED LEARNING"
GENERALIZED ADVANTAGE-WEIGHTED LEARNING,0.08957654723127036,"Based on ˆRt calculated in Section 3.2, we can conduct policy learning in a regression form, as
adopted in return-based ofﬂine RL methods (Nair et al., 2020; Siegel et al., 2020; Peng et al., 2019):"
GENERALIZED ADVANTAGE-WEIGHTED LEARNING,0.09120521172638436,"max
φ
Jπ(φ) = E(st,at)∼D
h
log πφ(at | st) · f

ˆA(st, at)
i
,
(7)"
GENERALIZED ADVANTAGE-WEIGHTED LEARNING,0.09283387622149837,"where ˆA(st, at) = ˆRt −ˆV (st) and f is an increasing, non-negative function. Please refer to Ap-
pendix C.1 for the detailed implementation of Equation 7. Note that ˆRt is not the vanilla returns in
the dataset, but the enhanced estimation calculated by implicit planning from ˆVt, as opposed with
other return based methods. Please refer to Algorithm 1 and Section 4 for implementation details
and theoretical analysis."
GENERALIZED ADVANTAGE-WEIGHTED LEARNING,0.09446254071661238,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.09609120521172639,"4
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.09771986970684039,"In this section, we ﬁrst derive the convergence property of expectile V -Learning. Then, we demon-
strate that memory-based planning accelerates the convergence of the EVL. Finally, we design a toy
example to demonstrate these theoretical analyses empirically. Please refer to Appendix B for the
detailed proofs of the following analysis."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.0993485342019544,"4.1
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING"
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.10097719869706841,"In this section, we assume the environment is deterministic. We derive the contraction property of
T µ
τ as the following statement:
Lemma 1. For any τ ∈(0, 1), T µ
τ is a γτ-contraction, where γτ = 1 −2α(1 −γ) min{τ, 1 −τ}."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.10260586319218241,Proof. We introduce two more operators to simplify the analysis:
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.10423452768729642,"(T µ
+ V )(s) = V (s) + Ea∼µ[δ(s, a)]+, (T µ
−V )(s) = V (s) + Ea∼µ[δ(s, a)]−.
(8)"
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.10586319218241043,"Next we show that both operators are non-expansion (e.g., ∥T µ
+ V1 −T µ
+ V2∥∞≤∥V1 −V2∥∞).
Finally, we rewrite T µ
τ based on T µ
+ and T µ
−and we prove that T µ
τ is a γτ-contraction. Please refer
to Appendix B.2 for the complete proof."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.10749185667752444,"Based on Lemma 1, we give a discussion about the step-size α and the fraction τ:"
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.10912052117263844,"About the step-size α.
Generally, we always want a larger α. However, α must satisfy that
V (s)+2ατδ(s, a) ≤max{r(s, a)+γV (s′), V (s)} and V (s)+2α(1−τ)δ(s, a) ≥min{r(s, a)+
γV (s′), V (s)}, otherwise the V -value will be overestimated.
Thus, we must have 2ατ ≤1
and 2α(1 −τ) ≤1, which infers that α ≤
1
2 max{τ,1−τ}. When α =
1
2 max{τ,1−τ}, we have"
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.11074918566775244,"γτ = 1 −2α min{τ, 1 −τ}(1 −γ) = 1 −min{τ,1−τ}"
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.11237785016286644,"max{τ,1−τ}(1 −γ)."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.11400651465798045,"About the fraction τ.
It is easy to verify that γτ approaches to 1 when τ →0 or τ →1, which
means that with a larger τ the contractive property is getting weaker. The choice of τ makes a trade-
off between the learning stability and the optimality of values. We further point out that when τ = 1,
the Expectile V -learning degrades as a special case of the generalized self-imitation learning (Tang,
2020), which losses the contractive property."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.11563517915309446,"Next, we prove that T µ
τ is monotonous improving with respect to τ:
Lemma 2. For any τ, τ ′ ∈(0, 1), if τ ′ ≥τ, we have T µ
τ ′V (s) ≥T µ
τ V (s), ∀s ∈S."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.11726384364820847,"Based on the Lemma 2, we derive that V ∗
τ is monotonous improving with respect to τ:
Proposition 1. Let V ∗
τ denote the ﬁxed point of T µ
τ . For any τ, τ ′ ∈(0, 1), if τ ′ ≥τ, we have
V ∗
τ ′(s) ≥V ∗
τ (s), ∀s ∈S."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.11889250814332247,"Further, we derive that V ∗
τ gradually approaches V ∗with respect to τ:
Lemma 3. Let V ∗denote the ﬁxed point of Bellman optimality operator T ∗. In the deterministic
MDP, we have limτ→1 V ∗
τ = V ∗."
CONVERGENCE PROPERTY OF THE EXPECTILE V-LEARNING,0.12052117263843648,"Based on the above analysis, we have the following conclusion:
Remark 1. By choosing a suitable τ, we can achieve the trade-off between the contraction rate and
the ﬁxed point bias. Particularly, a larger τ introduces a smaller ﬁxed point bias between V ∗
τ and
V ∗, and produces a larger contraction rate γτ simultaneously."
VALUE-BASED EPISODIC MEMORY,0.12214983713355049,"4.2
VALUE-BASED EPISODIC MEMORY"
VALUE-BASED EPISODIC MEMORY,0.1237785016286645,"In this part, we demonstrate that the memory-based planning effectively accelerates the convergence
of the EVL. We ﬁrst deﬁne the VEM operator as:"
VALUE-BASED EPISODIC MEMORY,0.1254071661237785,"(TvemV )(s) =
max
1≤n≤nmax{(T µ)n−1T µ
τ V (s)},
(9)"
VALUE-BASED EPISODIC MEMORY,0.1270358306188925,Published as a conference paper at ICLR 2022
VALUE-BASED EPISODIC MEMORY,0.12866449511400652,"2.5
3.0
3.5
Bias 0.65 0.70 0.75 0.80 0.85 0.90"
VALUE-BASED EPISODIC MEMORY,0.13029315960912052,Contraction Rate
VALUE-BASED EPISODIC MEMORY,0.13192182410423453,"0.7
0.8
0.9
Contraction Rate 0.10 0.15 0.20"
VALUE-BASED EPISODIC MEMORY,0.13355048859934854,Variance
VALUE-BASED EPISODIC MEMORY,0.13517915309446255,(a) The maximal rollout step nmax.
VALUE-BASED EPISODIC MEMORY,0.13680781758957655,"0
1
2
Bias 0.35 0.40 0.45 0.50"
VALUE-BASED EPISODIC MEMORY,0.13843648208469056,Contraction Rate
VALUE-BASED EPISODIC MEMORY,0.14006514657980457,"0.35
0.40
0.45
0.50
Contraction Rate 0.0 0.1 0.2 0.3 0.4"
VALUE-BASED EPISODIC MEMORY,0.14169381107491857,Variance
VALUE-BASED EPISODIC MEMORY,0.14332247557003258,(b) The different behavior policies.
VALUE-BASED EPISODIC MEMORY,0.1449511400651466,"Figure 3: A toy example in the random MDP. In both ﬁgures, the color darkens with a larger τ
(τ ∈{0.6, 0.7, 0.8, 0.9}). The size of the spots is proportional to the relative scale of the third
variable: (a) Change nmax. From magenta to blue, nmax is set as 1, 2, 3, 4 in order. (b) Change the
behavior polices µ, where µ(s) = softmax(Q∗(s, ·)/α). From light yellow to dark red, the α is set
as 0.1, 0.3, 1, 3 in order."
VALUE-BASED EPISODIC MEMORY,0.1465798045602606,"where nmax is the maximal rollout step for memory control. Then, we derive that multi-step esti-
mation operator Tvem does not change the ﬁxed point and contraction property of T µ
τ :"
VALUE-BASED EPISODIC MEMORY,0.1482084690553746,"Lemma 4. Given τ ∈(0, 1) and nmax ∈N+, Tvem is a γτ-contraction. If τ > 1"
VALUE-BASED EPISODIC MEMORY,0.1498371335504886,"2, Tvem has the
same ﬁxed point as T µ
τ ."
VALUE-BASED EPISODIC MEMORY,0.15146579804560262,"Next, we derive that the contraction rate of Tvem depends on the dataset quality. Further, we demon-
strate that the convergence rate of Tvem is quicker than T µ
τ even the behavior policy µ is random:"
VALUE-BASED EPISODIC MEMORY,0.15309446254071662,"Lemma 5. When the current value estimates V (s) are much lower than the value of behavior policy,
Tvem provides an optimistic update. Formally, we have"
VALUE-BASED EPISODIC MEMORY,0.15472312703583063,"|TvemV (s) −V ∗
τ (s)| ≤γn∗(s)−1γτ∥V −V µ
n∗,τ∥∞+ ∥V µ
n∗,τ −V ∗
τ ∥∞, ∀s ∈S,
(10)"
VALUE-BASED EPISODIC MEMORY,0.1563517915309446,"where n∗(s) = arg max0<n≤nmax{(T µ)n−1T µ
τ V (s)}, V µ
n∗,τ is the ﬁxed point of (T µ)n∗(s)−1T µ
τ
and it is the optimal rollout value starting from s."
VALUE-BASED EPISODIC MEMORY,0.15798045602605862,"This lemma demonstrates that Tvem can provide an optimistic update for pessimistic value estimates.
Speciﬁcally, the scale of the update depends on the quality of the datasets. If the behavior policy µ
is expert, which means V µ
n∗,τ is close to V ∗
τ . Then, following the lemma, the contraction rate will be
near to γn∗(s)−1γτ. Moreover, if the initial value estimates are pessimistic (e.g., the initialized value
function with zeros), we will have n∗(s) ≈nmax, indicating that the value update will be extremely
fast towards a lower bound of V ∗
τ . On the contrary, if µ is random, we have n∗(s) ≈1 and the value
update will be slow towards V ∗
τ ."
VALUE-BASED EPISODIC MEMORY,0.15960912052117263,"Remark 2. By choosing a suitable nmax, we can achieve the trade-off between the contraction
rate and the estimation variance, i.e., a larger nmax yields a fast update towards a lower bound of
ﬁxed point and tolerable variances empirically. Meanwhile, the choice of nmax does not introduce
additional bias, and the ﬁxed point bias is totally controlled by τ."
TOY EXAMPLE,0.16123778501628663,"4.3
TOY EXAMPLE"
TOY EXAMPLE,0.16286644951140064,"We design a toy example in the random deterministic MDP to empirically demonstrate the above
analysis. Following (Rowland et al., 2020), we adopt three indicators, including update variance,
ﬁxed-point bias, and contraction rate, which is shown in Figure 3. Speciﬁcally, the contraction rate
is supV ̸=V ′ ∥TvemV −TvemV ′∥∞/∥V −V ′∥∞, the bias is ∥V ∗
vem −V ∗∥∞and the variance is"
TOY EXAMPLE,0.16449511400651465,"E
h
∥ˆT V −TvemV ∥2
2
i 1"
TOY EXAMPLE,0.16612377850162866,"2 , where ˆTvem is the stochastic approximation of Tvem and V ∗
vem is the ﬁxed
pointed of Tvem. First, the experimental results in Figure 3(a) demonstrate that the relationship
of n-step estimation and τ. Formally, the contraction rate decreases as n becomes larger, and the
ﬁxed-point bias increases as τ becomes smaller, which are consistent with Lemma 1 and Lemma 2.
Figure 3(a) also shows that the variance is positively correlated with n. Second, the experimental
results in Figure 3(b) demonstrate that the relationship of dataset quality and τ. The higher dataset
quality corresponds to the lower contraction rate and variance, which is consistent with Lemma 5."
TOY EXAMPLE,0.16775244299674266,Published as a conference paper at ICLR 2022
TOY EXAMPLE,0.16938110749185667,"(a) Large
(b) Medium
(c) Umaze"
TOY EXAMPLE,0.17100977198697068,"Figure 4: Visualization of the value estimation in various AntMaze tasks. Darker colors correspond
to the higher value estimation. Each map has several terminals (golden stars) and one of which is
reached by the agent (the light red star). The red line is the trajectory of the ant."
RELATED WORK,0.17263843648208468,"5
RELATED WORK"
RELATED WORK,0.1742671009771987,"Ofﬂine Reinforcement Learning.
Ofﬂine RL methods (Kumar et al., 2019; Siegel et al., 2020;
Argenson & Dulac-Arnold, 2020; Wu et al., 2021; Dadashi et al., 2021; Kostrikov et al., 2021; Jin
et al., 2021; Rashidinejad et al., 2021) can be roughly divided into policy constraint, pessimistic
value estimation, and model-based methods. Policy constraint methods aim to keep the policy to be
close to the behavior under a probabilistic distance (Fujimoto et al., 2019; Peng et al., 2019; Nair
et al., 2020). Pessimistic value estimation methods like CQL (Kumar et al., 2020) enforces a regu-
larization constraint on the critic loss to penalize overgeneralization. Model-based methods attempt
to learn a model from ofﬂine data, with minimal modiﬁcation to the policy learning (Kidambi et al.,
2020; Yu et al., 2020; Janner et al., 2019). However, these methods have to introduce additional be-
havioral policy models, dynamics models, or regularization terms (Zhang et al., 2020b;a; Lee et al.,
2021). Another line of methods uses empirical return as the signal for policy learning, which con-
ﬁnes learning within the dataset but leads to limited performance (Levine et al., 2020; Geist et al.,
2019; Wang et al., 2021)."
RELATED WORK,0.1758957654723127,"Episodic Control.
Episodic control aims to store good past experiences in a non-parametric mem-
ory and rapidly latch into past successful policies when encountering similar states instead of waiting
for many optimization steps (Blundell et al., 2016b). Pritzel et al. (2017) and Lin et al. (2018) intro-
duce a parametric memory, which enables better generalization through neural networks. Our work
is closely related to recent advances in Hu et al. (2021), which adopts an implicit planning scheme to
enable episodic memory updates in continuous domains. Our method follows this implicit scheme,
but conducts planning with expectile V -values to avoid overgeneralization on actions out of dataset
support."
EXPERIMENTS,0.1775244299674267,"6
EXPERIMENTS"
EXPERIMENTS,0.1791530944625407,"In our experiments, we aim to answer the following questions: 1) How does our method performe
compared to state-of-the-art ofﬂine RL algorithms on the D4RL benchmark dataset? 2) How does
implicit planning affect the performance on sparse reward tasks? 3) Can expectile V -Learning
effectively reduces the extrapolation error compared with other ofﬂine methods? 4) How does the
critical parameter τ affect the performance of our method?"
EVALUATION ENVIRONMENTS,0.18078175895765472,"6.1
EVALUATION ENVIRONMENTS"
EVALUATION ENVIRONMENTS,0.18241042345276873,"We ran VEM on AntMaze, Adroit, and MuJoCo environments to evaluate its performance on var-
ious types of tasks. Precisely, the AntMaze navigation tasks control an 8-DoF quadruped robot to
reach a speciﬁc or randomly sampled goal in three types of maps. The reward in the AntMaze
domain is highly sparse. The Adroit domain involves controlling a 24-DoF simulated hand tasked
with hammering a nail, opening a door, twirling a pen, or picking up and moving a ball. On the
adroit tasks, these datasets are the following, “human”: transitions collected by a human operator,"
EVALUATION ENVIRONMENTS,0.18403908794788273,Published as a conference paper at ICLR 2022
EVALUATION ENVIRONMENTS,0.18566775244299674,"Type
Env
VEM(Ours)
VEM(τ=0.5)
BAIL
BCQ
CQL
AWR
ﬁxed
umaze
87.5±1.1
85.0±1.5
62.5 ± 2.3
78.9
74.0
56.0
play
medium
78.0±3.1
71.0±2.5
40.0 ± 15.0
0.0
61.2
0.0
play
large
57.0±5.0
45.0±2.5
23.0±5.0
6.7
11.8
0.0
diverse
umaze
78.0 ± 1.1
75.0±5.0
75.0±1.0
55.0
84.0
70.3
diverse
medium
77.0±2.2
60.0±5.0
50.0±10.0
0.0
53.7
0.0
diverse
large
58.0 ± 2.1
48.0±2.7
30.0±5.0
2.2
14.9
0.0
human
door
11.2±4.2
6.9±1.1
0.0±0.1
-0.0
9.1
0.4
human
hammer
3.6±1.0
2.5±1.0
0.0±0.1
0.5
2.1
1.2
human
relocate
1.3±0.2
0.0±0.0
0.0±0.1
0.5
2.1
-0.0
human
pen
65.0±2.1
55.2±3.1
32.5±1.5
68.9
55.8
12.3
cloned
door
3.6±0.3
0.0±0.0
0.0±0.1
0.0
3.5
0.0
cloned
hammer
2.7±1.5
0.5±0.1
0.1±0.1
0.4
5.7
0.4
cloned
pen
48.7±3.2
27.8±2.2
46.5±3.5
44.0
40.3
28.0
expert
door
105.5±0.2
104.8±0.2
104.7±0.3
99.0
-
102.9
expert
hammer
128.3±1.1
102.3±5.6
123.5±3.1
114.9
-
39.0
expert
relocate
109.8±0.2
101.0±1.5
94.4±2.7
41.6
-
91.5
expert
pen
111.7±2.6
115.2±1.3
126.7±0.3
114.9
-
111.0
random
walker2d
6.2±4.7
6.2±4.7
3.9±2.5
4.9
7.0
1.5
random
hopper
11.1±1.0
10.8±1.2
9.8±0.1
10.6
10.8
10.2
random
halfcheetah
16.4±3.6
2.6±2.1
0.0±0.1
2.2
35.4
2.5
medium
walker2d
74.0±1.2
16.6±0.1
73.0±1.0
53.1
79.2
17.4
medium
hopper
56.6±2.3
56.6±2.3
58.2±1.0
54.5
58.0
35.9
medium
halfcheetah
47.4±0.2
45.3±0.2
42.6±1.2
40.7
44.4
37.4"
EVALUATION ENVIRONMENTS,0.18729641693811075,"Table 1: Performance of VEM with four ofﬂine RL baselines on the AntMaze, Adroit, and MuJoCo
domains with the normalized score metric proposed by D4RL benchmark, averaged over three ran-
dom seeds with ± standard deviation. Scores range from 0 to 100, where 0 corresponds to a random
policy performance, and 100 indicates an expert. We use the results in Fu et al. (2020) for AWR
and BCQ, and use the results in Kumar et al. (2020) for CQL. The results of BAIL come from our
implementation according to the ofﬁcial code (https://github.com/lanyavik/BAIL)."
EVALUATION ENVIRONMENTS,0.18892508143322476,"“cloned”: transitions collected by a policy trained with behavioral cloning interacting in the environ-
ment + initial demonstrations, “expert”: transitions collected by a ﬁne-tuned RL policy interacting
in the environment. As for the MuJoCo tasks, the datasets are “random”: transitions collected by
a random policy,“medium”: transitions collected by a policy with suboptimal performance. The
complete implementation details are presented in Appendix C."
EVALUATION ENVIRONMENTS,0.19055374592833876,"6.2
PERFORMANCE ON D4RL TASKS"
EVALUATION ENVIRONMENTS,0.19218241042345277,"As shown in Table 1, VEM achieves state-of-the-art performance on most AntMaze tasks and has
a signiﬁcant improvement over other methods on most Adroit tasks. VEM also achieves good per-
formances in MuJoCo domains. We ﬁnd that VEM has low value estimation errors in all tasks,
which promotes its superior performance. However, as a similar training framework, BAIL only has
reasonable performances on simple ofﬂine tasks, such as MuJoCo. Please refer to Appendix D.2 for
the complete training curves and value estimation error on D4RL."
EVALUATION ENVIRONMENTS,0.19381107491856678,"To further analyze the superior performance of VEM in the sparse reward tasks, we visualize the
learned value estimation in AntMaze tasks, which is shown in Figure 4. Experimental results show
that VEM has the higher value estimates on the critical place of the map (e.g., corners) since various
trajectories in the datasets are connected. The accurate value estimation leads to its success on
complex sparse reward tasks."
ANALYSIS OF VALUE ESTIMATION,0.19543973941368079,"6.3
ANALYSIS OF VALUE ESTIMATION"
ANALYSIS OF VALUE ESTIMATION,0.1970684039087948,"As both Expectile V -Learning (EVL) and Batch Constrained Q-Learning (BCQ) (Fujimoto et al.,
2019) aim to avoid using the unseen state-action pairs to eliminate the extrapolation error, we re-
place EVL in VEM with BCQ (named BCQ-EM) to evaluate the effectiveness of the EVL module."
ANALYSIS OF VALUE ESTIMATION,0.1986970684039088,Published as a conference paper at ICLR 2022
ANALYSIS OF VALUE ESTIMATION,0.2003257328990228,"The experimental results in Figure 9 in Appendix D.1 indicate that the performance of BCQ-EM is
mediocre, and BCQ reaches performance signiﬁcantly below VEM. We observe a strong correla-
tion between the training instability and the explosion of the value estimation. This result should not
come as a surprise since the Adroit tasks have a larger action space compared with MuJoCo domains
and narrow human demonstrations. Therefore, the generative model in BCQ cannot guarantee com-
pletely the unseen actions are avoided. In contrast, VEM avoids fundamentally unseen actions by
keeping the learning procedure within the support of an ofﬂine dataset, indicating the necessity of
the EVL module. Please refer to Appendix C for the implementation details."
ANALYSIS OF VALUE ESTIMATION,0.20195439739413681,"We evaluate τ ∈{0.1, 0.2, ..., 0.9} to investigate the effect of the critical hyper-parameter in EVL,
which is shown in Figure 7 in Appendix D.1. The experimental results demonstrate that the esti-
mated value increases with a larger τ, which is consistent with the analysis in Section 4.1. Moreover,
we observe that τ is set at a low value in some complex high-dimensional robotic tasks or narrow
human demonstrations, such as Adroit-cloned/human, to get the conservative value estimates. How-
ever, if τ is set too high (e.g., τ = 0.9 in the pen-human task), the estimated value will explode and
poor performance. This is as expected since the over-large τ leads to the overestimation error caused
by neural networks. The experimental results demonstrate that we can balance behavior cloning and
optimal value learning by choosing τ in terms of different tasks."
ABLATIONS,0.20358306188925082,"6.4
ABLATIONS"
ABLATIONS,0.20521172638436483,"Episodic Memory Module.
Our ﬁrst study aims to answer the impact of memory-based planning
on performance. We replace the episodic memory module in VEM with standard n-step value esti-
mation (named VEM-1step or VEM-nstep). The experimental results in Figure 8 in Appendix D.1
indicate that implicit planning along ofﬂine trajectories effectively accelerates the convergence of
EVL."
ABLATIONS,0.20684039087947884,"Expectile Loss.
In addition to the Expectile loss, we explored other forms of loss. Formally, we
compare the Expectile loss and quantile loss, a popular form in Distributional RL algorithms (Dab-
ney et al., 2018), which is shown in Figure 5 in Appendix D.1. The experimental results indicate
that the Expectile loss is better since it is more stable when dealing with extreme values."
CONCLUSION,0.20846905537459284,"7
CONCLUSION"
CONCLUSION,0.21009771986970685,"In this paper, we propose a novel ofﬂine RL method, VEM, based on a new V -learning algorithm,
EVL. EVL naturally avoids actions outside the dataset and provides a smooth tradeoff between gen-
eralization and conversation for ofﬂine learning. Further, VEM enables effective implicit planning
along ofﬂine trajectories to accelerate the convergence of EVL and achieve better advantage estima-
tion. Unlike most existing ofﬂine RL methods, we keep the learning procedure totally within the
dataset’s support without any auxiliary modular, such as environment model or behavior policy. The
experimental results demonstrate that VEM achieves superior performance in most D4RL tasks and
learns the accurate values to guide policy learning, especially in sparse reward tasks. We hope that
VEM will inspire more works on ofﬂine RL and promote practical RL methods in the future."
REPRODUCIBILITY,0.21172638436482086,"8
REPRODUCIBILITY"
REPRODUCIBILITY,0.21335504885993486,"To ensure our work is reproducible, we provide our code in the supplementary materials. In the fu-
ture, we will publish all source code on Github. The detailed implementation of our algorithm is pre-
sented as follows. The value network is trained according to Equation 4. The actor-network is trained
according to Equation 7. The hyper-parameters and network structure used in VEM are shown in
Appendix C.3. All experiments are run on the standard ofﬂine tasks, D4RL (https://github.com/rail-
berkeley/d4rl/tree/master/d4rl)."
REPRODUCIBILITY,0.21498371335504887,Published as a conference paper at ICLR 2022
REFERENCES,0.21661237785016288,REFERENCES
REFERENCES,0.2182410423452769,"Arthur Argenson and Gabriel Dulac-Arnold.
Model-based ofﬂine planning.
arXiv preprint
arXiv:2008.05556, 2020."
REFERENCES,0.21986970684039087,"Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,
Jack Rae, Daan Wierstra, and Demis Hassabis.
Model-free episodic control.
arXiv preprint
arXiv:1606.04460, 2016a."
REFERENCES,0.22149837133550487,"Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,
Jack Rae, Daan Wierstra, and Demis Hassabis.
Model-free episodic control.
arXiv preprint
arXiv:1606.04460, 2016b."
REFERENCES,0.22312703583061888,"Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. BAIL: Best-
action imitation learning for batch deep reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.2247557003257329,"Will Dabney, Mark Rowland, Marc G Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018."
REFERENCES,0.2263843648208469,"Robert Dadashi, Shideh Rezaeifar, Nino Vieillard, L´eonard Hussenot, Olivier Pietquin, and
Matthieu Geist.
Ofﬂine reinforcement learning with pseudometric learning.
arXiv preprint
arXiv:2103.01948, 2021."
REFERENCES,0.2280130293159609,"Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020."
REFERENCES,0.2296416938110749,"Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019."
REFERENCES,0.23127035830618892,"Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In International Conference on Machine Learning, pp. 2160–2169. PMLR, 2019."
REFERENCES,0.23289902280130292,"Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. EMaQ: Expected-
max Q-learning operator for simple yet effective ofﬂine and online RL. In International Confer-
ence on Machine Learning, pp. 3682–3691. PMLR, 2021."
REFERENCES,0.23452768729641693,"Hao Hu, Jianing Ye, Zhizhou Ren, Guangxiang Zhu, and Chongjie Zhang. Generalizable episodic
memory for deep reinforcement learning. arXiv preprint arXiv:2103.06469, 2021."
REFERENCES,0.23615635179153094,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization.
Advances in Neural Information Processing Systems, 32:12519–
12530, 2019."
REFERENCES,0.23778501628664495,"Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efﬁcient for ofﬂine RL? In
International Conference on Machine Learning, pp. 5084–5096. PMLR, 2021."
REFERENCES,0.23941368078175895,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. arXiv preprint arXiv:2005.05951, 2020."
REFERENCES,0.24104234527687296,"Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Oﬁr Nachum. Ofﬂine reinforcement learning
with ﬁsher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774–5783. PMLR, 2021."
REFERENCES,0.24267100977198697,"Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy Q-
learning via bootstrapping error reduction. Advances in Neural Information Processing Systems,
32:11784–11794, 2019."
REFERENCES,0.24429967426710097,"Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020."
REFERENCES,0.24592833876221498,"Jongmin Lee, Wonseok Jeon, Byung-Jun Lee, Joelle Pineau, and Kee-Eung Kim.
OptiDICE:
Ofﬂine policy optimization via stationary distribution correction estimation.
arXiv preprint
arXiv:2106.10783, 2021."
REFERENCES,0.247557003257329,Published as a conference paper at ICLR 2022
REFERENCES,0.249185667752443,"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020."
REFERENCES,0.250814332247557,"Zichuan Lin, Tianqi Zhao, Guangwen Yang, and Lintao Zhang. Episodic memory deep Q-networks.
In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, pp. 2433–
2439, 2018."
REFERENCES,0.252442996742671,"Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020."
REFERENCES,0.254071661237785,"Whitney K Newey and James L Powell. Asymmetric least squares estimation and testing. Econo-
metrica: Journal of the Econometric Society, pp. 819–847, 1987."
REFERENCES,0.255700325732899,"Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019."
REFERENCES,0.25732899022801303,"Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals,
Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In International
Conference on Machine Learning, pp. 2827–2836. PMLR, 2017."
REFERENCES,0.25895765472312704,"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging ofﬂine rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021."
REFERENCES,0.26058631921824105,"Mark Rowland, Robert Dadashi, Saurabh Kumar, R´emi Munos, Marc G Bellemare, and Will Dab-
ney. Statistics and samples in distributional reinforcement learning. In International Conference
on Machine Learning, pp. 5528–5536. PMLR, 2019."
REFERENCES,0.26221498371335505,"Mark Rowland, Will Dabney, and R´emi Munos. Adaptive trade-offs in off-policy learning. In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 34–44. PMLR, 2020."
REFERENCES,0.26384364820846906,"Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller.
Keep doing
what worked: Behavioral modelling priors for ofﬂine reinforcement learning. arXiv preprint
arXiv:2002.08396, 2020."
REFERENCES,0.26547231270358307,"Yunhao Tang. Self-imitation learning via generalized lower bound Q-learning. Advances in Neural
Information Processing Systems, 33, 2020."
REFERENCES,0.2671009771986971,"Qing Wang, Jiechao Xiong, Lei Han, Peng Sun, Han Liu, and Tong Zhang. Exponentially weighted
imitation learning for batched historical data. Advances in Neural Information Processing Sys-
tems, 31:6288, 2018."
REFERENCES,0.2687296416938111,"Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham M Kakade. Instabilities of ofﬂine rl
with pre-trained neural representation. arXiv preprint arXiv:2103.04947, 2021."
REFERENCES,0.2703583061889251,"Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov,
and Hanlin Goh. Uncertainty weighted Actor-Critic for ofﬂine reinforcement learning. arXiv
preprint arXiv:2105.08140, 2021."
REFERENCES,0.2719869706840391,"Yiqin Yang, Xiaoteng Ma, Li Chenghao, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang, and
Qianchuan Zhao.
Believe what you see: Implicit constraint approach for ofﬂine multi-agent
reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021."
REFERENCES,0.2736156351791531,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: Model-based ofﬂine policy optimization. Advances in Neural
Information Processing Systems, 33:14129–14142, 2020."
REFERENCES,0.2752442996742671,"Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. GenDICE: Generalized ofﬂine estimation
of stationary values. arXiv preprint arXiv:2002.09072, 2020a."
REFERENCES,0.2768729641693811,"Shangtong Zhang, Bo Liu, and Shimon Whiteson. GradientDICE: Rethinking generalized ofﬂine
estimation of stationary values. In International Conference on Machine Learning, pp. 11194–
11203. PMLR, 2020b."
REFERENCES,0.2785016286644951,Published as a conference paper at ICLR 2022
REFERENCES,0.28013029315960913,"A
ALGORITHM"
REFERENCES,0.28175895765472314,"A.1
VALUE-BASED EPISODIC MEMORY CONTROL"
REFERENCES,0.28338762214983715,Algorithm 1 Value-based Episodic Memory Control
REFERENCES,0.28501628664495116,"Initialize critic networks Vθ1, Vθ2 and actor network πφ with random parameters θ1, θ2, φ
Initialize target networks θ′
1 ←θ1, θ′
2 ←θ2
Initialize episodic memory M
for t = 1 to T do"
REFERENCES,0.28664495114006516,"for i ∈{1, 2} do"
REFERENCES,0.28827361563517917,"Sample N transitions

st, at, rt, st, ˆR(i)
t

from M"
REFERENCES,0.2899022801302932,"Update θi ←minθiN −1 P 
R(i)
t
−Vθi(st)
2"
REFERENCES,0.2915309446254072,"Update φ ←maxφN −1 P ∇log πφ(at|st) · f

mini ˆR(i)
t
−meaniVθi(st)
"
REFERENCES,0.2931596091205212,"end for
if t mod u then"
REFERENCES,0.2947882736156352,"θ′
i ←κθi + (1 −κ)θ′
i
Update Memory
end if
end for"
REFERENCES,0.2964169381107492,Algorithm 2 Update Memory
REFERENCES,0.2980456026058632,for trajectories τ in buffer M do
REFERENCES,0.2996742671009772,"for st, at, rt, st+1 in reversed(τ) do"
REFERENCES,0.30130293159609123,"for i ∈{1, 2} do"
REFERENCES,0.30293159609120524,"Compute ˆR(i)
t
with Equation 6 and save into buffer M
end for
end for
end for"
REFERENCES,0.30456026058631924,"A.2
AN APPROACH FOR AUTO-TUNING τ"
REFERENCES,0.30618892508143325,"When we have a good estimation of V ∗, for example, when there is some expert data in the dataset,
we can auto-tune τ such that the value learned by EVL is close to the estimation of V ∗. This can be
done by calculating the Monte-Carlo return estimates of each state and selecting good return values
as the estimation of optimal value ˜V ∗. Based on this target, we develop a method for auto-tuning τ."
REFERENCES,0.30781758957654726,"By parameterizing τ = sigmoid(ξ) with a differentiable parameter ξ ∈R, we can auto-tune τ by
minimizing the following loss J (ξ) = ξ(E ˆV (s) −˜V ∗). If (E ˆV (s) −˜V ∗) < 0, the differentiable
parameter ξ will become larger and the value estimation E ˆV (s) will become larger accordingly.
Similarly, ξ and E ˆV (s) will become smaller if (E ˆV (s) −˜V ∗) > 0. The experimental results in
Figure 10 in Appendix D.1 show that auto-tuning can lead to similar performance compared with
manual selection."
REFERENCES,0.30944625407166126,Published as a conference paper at ICLR 2022
REFERENCES,0.31107491856677527,"B
THEORETICAL ANALYSIS"
REFERENCES,0.3127035830618892,"B.1
COMPLETE DERIVATION."
REFERENCES,0.31433224755700323,"The expectile regression loss (Rowland et al., 2019) is deﬁned as
ER(q; ϱ, τ) = EZ∼ϱ

[τI(Z > q) + (1 −τ)I(Z ≤q)] (Z −q)2
,
(11)
where ϱ is the target distribution and the minimiser of this loss is called the τ-expectile of ϱ. the
corresponding loss in reinforcement learning is
JV (θ) = Eµ

τ(r(s, a) + γVθ′(s′) −Vθ(s))2
+ + (1 −τ)(r(s, a) + γVθ′(s′) −Vθ(s))2
−
"
REFERENCES,0.31596091205211724,"= Eµ

τ(y −Vθ(s))2
+ + (1 −τ)(y −Vθ(s))2
−

.
(12)"
REFERENCES,0.31758957654723124,"Then, taking the gradient of the value objective with respect to Vθ(s), we have"
REFERENCES,0.31921824104234525,"∇JV (θ) =
X
µ(a | s) [−2τ(y −Vθ(s))+I(y > Vθ(s)) −2(1 −τ)(y −Vθ(s))+I(y ≤Vθ(s))]"
REFERENCES,0.32084690553745926,"=
X
µ(a | s) [−2τ(y −Vθ(s))+ −2(1 −τ)(y −Vθ(s))−]"
REFERENCES,0.32247557003257327,"=
X
µ(a | s) [−2τ(δ)+ −2(1 −τ)(δ)−] .
(13)
Therefore,
ˆV (s) = Vθ(s) −α∇JV (θ)
= Vθ(s) + 2αEa∼µ [τ[δ(s, a)]+ + (1 −τ)[δ(s, a)]−]
(14)"
REFERENCES,0.3241042345276873,"B.2
PROOF OF LEMMA 1"
REFERENCES,0.3257328990228013,"Lemma 1. For any τ ∈[0, 1), T µ
τ is a γτ-contraction, where γτ = 1 −2α(1 −γ) min{τ, 1 −τ}."
REFERENCES,0.3273615635179153,"Proof. Note that T µ
1/2 is the standard policy evaluation Bellman operator for µ, whose ﬁxed point is
V µ. We see that for any V1, V2,
T µ
1/2V1(s) −T µ
1/2V2(s)"
REFERENCES,0.3289902280130293,"= V1(s) + αEa∼µ[δ1(s, a)] −(V2(s) + αEa∼µ[δ2(s, a)])"
REFERENCES,0.3306188925081433,"= (1 −α)(V1(s) −V2(s)) + αEa∼µ[r(s, a) + γV1(s′) −r(s, a) −γV2(s′)]
≤(1 −α)∥V1 −V2∥∞+ αγ∥V1 −V2∥∞
= (1 −α(1 −γ))∥V1 −V2∥∞. (15)"
REFERENCES,0.3322475570032573,"We introduce two more operators to simplify the analysis:
T µ
+ V (s) = V (s) + Ea∼µ[δ(s, a)]+,"
REFERENCES,0.3338762214983713,"T µ
−V (s) = V (s) + Ea∼µ[δ(s, a)]−.
(16)"
REFERENCES,0.3355048859934853,"Next we show that both operators are non-expansion (i.e., ∥T µ
+ V1 −T µ
+ V2∥∞≤∥V1 −V2∥∞). For
any V1, V2, we have
T µ
+ V1(s) −T µ
+ V2(s) = V1(s) −V2(s) + Ea∼µ[[δ1(s, a)]+ −[δ2(s, a)]+]"
REFERENCES,0.33713355048859933,"= Ea∼µ[[δ1(s, a)]+ + V1(s) −([δ2(s, a)]+ + V2(s))].
(17)"
REFERENCES,0.33876221498371334,"The relationship between [δ1(s, a)]+ + V1(s) and [δ2(s, a)]+ + V2(s) exists in four cases, which are"
REFERENCES,0.34039087947882735,"• δ1 ≥0, δ2 ≥0, then [δ1(s, a)]+ + V1(s) −([δ2(s, a)]+ + V2(s)) = γ(V1(s′) −V2(s′))."
REFERENCES,0.34201954397394135,"• δ1 < 0, δ2 < 0, then [δ1(s, a)]+ + V1(s) −([δ2(s, a)]+ + V2(s)) = V1(s) −V2(s)."
REFERENCES,0.34364820846905536,"• δ1 ≥0, δ2 < 0, then
[δ1(s, a)]+ + V1(s) −([δ2(s, a)]+ + V2(s))"
REFERENCES,0.34527687296416937,"= (r(s, a) + γV1(s′)) −V2(s)"
REFERENCES,0.3469055374592834,"< (r(s, a) + γV1(s′)) −(r(s, a) + γV2(s′))"
REFERENCES,0.3485342019543974,"= γ(V1(s′) −V2(s′)), (18)"
REFERENCES,0.3501628664495114,"where the inequality comes from r(s, a) + γV2(s′) < V2(s)."
REFERENCES,0.3517915309446254,Published as a conference paper at ICLR 2022
REFERENCES,0.3534201954397394,"• δ1 < 0, δ2 ≥0, then"
REFERENCES,0.3550488599348534,"[δ1(s, a)]+ + V1(s) −([δ2(s, a)]+ + V2(s))"
REFERENCES,0.3566775244299674,"= V1(s) −(r(s, a) + γV2(s′))
≤V1(s) −V2(s),
(19)"
REFERENCES,0.3583061889250814,"where the inequality comes from r(s, a) + γV2(s′) ≥V2(s)."
REFERENCES,0.35993485342019543,"Therefore, we have T µ
+ V1(s) −T µ
+ V2(s) ≤∥V1 −V2∥∞. With the T µ
+ , T µ
−, we rewrite T µ
τ as"
REFERENCES,0.36156351791530944,"T µ
τ V (s) = V (s) + 2αEa∼µ[τ[δ(s, a)]+ + (1 −τ)[δ(s, a)]−]
= (1 −2α)V (s) + 2ατ(V (s) + Ea∼µ[δ(s, a)]+) + 2α(1 −τ)(V (s) + Ea∼µ[δ(s, a)]−)"
REFERENCES,0.36319218241042345,"= (1 −2α)V (s) + 2ατT µ
+ V (s) + 2α(1 −τ)T µ
−V (s).
(20)
And
T µ
1/2V (s) = V (s) + αEa∼µ[δ(s, a)]"
REFERENCES,0.36482084690553745,"= V (s) + α(T µ
+ V (s) + T µ
−V (s) −2V (s))"
REFERENCES,0.36644951140065146,"= (1 −2α)V (s) + α(T µ
+ V (s) + T µ
−V (s)). (21)"
REFERENCES,0.36807817589576547,We ﬁrst focus on τ < 1
REFERENCES,0.3697068403908795,"2. For any V1, V2, we have"
REFERENCES,0.3713355048859935,"T µ
τ V1(s) −T µ
τ V2(s)"
REFERENCES,0.3729641693811075,"= (1 −2α)(V1(s) −V2(s)) + 2ατ(T µ
+ V1(s) −T µ
+ V2(s)) + 2α(1 −τ)(T µ
−V1(s) −T µ
−V2(s))"
REFERENCES,0.3745928338762215,"= (1 −2α −2τ(1 −2α))(V1(s) −V2(s)) + 2τ

T µ
1/2V1(s) −T µ
1/2V2(s)

+"
REFERENCES,0.3762214983713355,"2α(1 −2τ)
 
T µ
−V1(s) −T µ
−V2(s)
"
REFERENCES,0.3778501628664495,"≤(1 −2α −2τ(1 −2α))∥V1 −V2∥∞+ 2τ(1 −α(1 −γ))∥V1 −V2∥∞+ 2α(1 −2τ)∥V1 −V2∥∞
= (1 −2ατ(1 −γ))∥V1 −V2∥∞
(22)
Similarly, when τ > 1/2, we have T µ
τ V1(s)−T µ
τ V2(s) ≤(1−2α(1−τ)(1−γ))∥V1 −V2∥∞."
REFERENCES,0.3794788273615635,"B.3
PROOF OF LEMMA 2"
REFERENCES,0.3811074918566775,"Lemma 2. For any τ, τ ′ ∈(0, 1), if τ ′ ≥τ, we have T µ
τ ′ ≥T µ
τ , ∀s ∈S."
REFERENCES,0.38273615635179153,"Proof. Based on Equation 20, we have"
REFERENCES,0.38436482084690554,"T µ
τ ′V (s) −T µ
τ V (s)"
REFERENCES,0.38599348534201955,"= (1 −2α)V (s) + 2ατ ′T µ
+ V (s) + 2α(1 −τ ′)T µ
−V (s)"
REFERENCES,0.38762214983713356,"−((1 −2α)V (s) + 2ατT µ
+ V (s) + 2α(1 −τ)T µ
−V (s))"
REFERENCES,0.38925081433224756,"= 2α(τ ′ −τ)(T µ
+ V (s) −T µ
−V (s))"
REFERENCES,0.39087947882736157,"= 2α(τ ′ −τ)Ea∼µ[[δ(s, a)]+ −[δ(s, a)]−] ≥0. (23)"
REFERENCES,0.3925081433224756,"B.4
PROOF OF LEMMA 3"
REFERENCES,0.3941368078175896,"Lemma 3. Let V ∗denote the ﬁxed point of Bellman optimality operator T ∗. In the deterministic
MDP, we have limτ→1 V ∗
τ = V ∗."
REFERENCES,0.3957654723127036,"Proof. We ﬁrst show that V ∗is also a ﬁxed point for T µ
+ . Based on the deﬁnition of T ∗, we have
V ∗(s) = maxa[r(s, a) + γV ∗(s′)], which infers that δ(s, a) ≤0, ∀s ∈S, a ∈A. Thus, we have
T µ
+ V ∗(s) = V ∗(s) + Ea∼µ[δ(s, a)]+ = V ∗(s). By setting (1 −τ) →0, we eliminate the effect
of T µ
−. Further by the contractive property of T µ
τ , we obtain the uniqueness of V ∗
τ . The proof is
completed."
REFERENCES,0.3973941368078176,Published as a conference paper at ICLR 2022
REFERENCES,0.3990228013029316,"B.5
PROOF OF LEMMA 4"
REFERENCES,0.4006514657980456,"Lemma 4. Given τ ∈(0, 1) and T ∈N+, Tvem is a γτ-contraction. If τ > 1"
REFERENCES,0.4022801302931596,"2, Tvem has the same
ﬁxed point as T µ
τ ."
REFERENCES,0.40390879478827363,"Proof. We prove the contraction ﬁrst. For any V1, V2, we have"
REFERENCES,0.40553745928338764,"TvemV1(s) −TvemV2(s) =
max
1≤n≤nmax{(T µ)n−1T µ
τ V1(s)} −max
1≤n≤T{(T µ)n−1T µ
τ V2(s)}"
REFERENCES,0.40716612377850164,"≤
max
1≤n≤nmax |(T µ)n−1T µ
τ V1(s) −(T µ)n−1T µ
τ V2(s)|"
REFERENCES,0.40879478827361565,"≤
max
1≤n≤nmax γn−1γτ∥V1 −V2∥∞"
REFERENCES,0.41042345276872966,≤γτ∥V1 −V2∥∞. (24)
REFERENCES,0.41205211726384366,"Next we show that V ∗
τ , the ﬁxed point of T µ
τ , is also the ﬁxed point of Tvem when τ >
1
2. By
deﬁnition, we have V ∗
τ = T µ
τ V ∗
τ . Following Lemma 2, we have V ∗
τ = T µ
τ V ∗
τ ≥T µ
1/2V ∗
τ = T µV ∗
τ .
Repeatedly applying T µ and using its monotonicity, we have T µV ∗
τ ≥(T µ)n−1V ∗
τ , 1 ≤n ≤nmax.
Thus, we have TvemV ∗
τ (s) = max1≤n≤T {(T µ)n−1T µ
τ V ∗
τ (s)} = V ∗
τ (s)."
REFERENCES,0.41368078175895767,"B.6
PROOF OF LEMMA 5"
REFERENCES,0.4153094462540717,"Lemma 5. When the current value estimates V (s) are much lower than the value of behavior policy,
Tvem provides an optimistic update. Formally, we have"
REFERENCES,0.4169381107491857,"|TvemV (s) −V ∗
τ (s)| ≤γn∗(s)−1γτ∥V −V µ
n∗,τ∥∞+ ∥V µ
n∗,τ −V ∗
τ ∥∞, ∀s ∈S,
(25)"
REFERENCES,0.4185667752442997,"where n∗(s) = arg max1≤n≤T {(T µ)n−1T µ
τ V (s)} and V µ
n∗,τ is the ﬁxed point of (T µ)n∗(s)−1T µ
τ ."
REFERENCES,0.4201954397394137,Proof. The lemma is a direct result of the triangle inequality. We have
REFERENCES,0.4218241042345277,"TvemV (s) −V ∗
τ (s) = (T µ)n∗(s)−1T µ
τ V (s) −V ∗
τ (s)"
REFERENCES,0.4234527687296417,"= (T µ)n∗(s)−1T µ
τ V (s) −(T µ)n∗(s)−1T µ
τ V µ
n∗,τ(s) + V µ
n∗,τ(s) −V ∗
τ (s)"
REFERENCES,0.4250814332247557,"≤γn∗(s)−1γτ∥V −V µ
n∗,τ∥∞+ ∥V µ
n∗,τ −V ∗
τ ∥.
(26)"
REFERENCES,0.42671009771986973,"B.7
PROOF OF PROPOSITION 1"
REFERENCES,0.42833876221498374,"Proposition 1. Let V ∗
τ denote the ﬁxed point of T µ
τ . For any τ, τ ′ ∈(0, 1), if τ ′ ≥τ, we have
V ∗
τ ′(s) ≥V ∗
τ (s), ∀s ∈S."
REFERENCES,0.42996742671009774,"Proof. With the Lemma 2, we have T µ
τ ′V ∗
τ ≥T µ
τ V ∗
τ . Since V ∗
τ is the ﬁxed point of T µ
τ , we have
T µ
τ V ∗
τ = V ∗
τ . Putting the results together, we obtain V ∗
τ = T µ
τ V ∗
τ ≤T µ
τ ′V ∗
τ . Repeatedly applying
T µ
τ ′ and using its monotonicity, we have V ∗
τ ≤T µ
τ ′V ∗
τ ≤(T µ
τ ′)∞V ∗
τ = V ∗
τ ′."
REFERENCES,0.43159609120521175,"C
DETAILED IMPLEMENTATION"
REFERENCES,0.43322475570032576,"C.1
GENERALIZED ADVANTAGE-WEIGHTED LEARNING"
REFERENCES,0.43485342019543977,"In practice, we adopt Leaky-ReLU or Softmax functions."
REFERENCES,0.4364820846905538,"Leaky-ReLU:
max
φ
Jπ(φ) = E(s,a)∼D
h
log πφ(a | s) · f

ˆA(s, a)
i
,"
REFERENCES,0.4381107491856677,"where
f( ˆA(s, a)) ="
REFERENCES,0.43973941368078173,"( ˆA(s, a)
if
ˆA(s, a) > 0
ˆ
A(s,a)"
REFERENCES,0.44136807817589574,"α
if
ˆA(s, a) ≤0 (27)"
REFERENCES,0.44299674267100975,Published as a conference paper at ICLR 2022
REFERENCES,0.44462540716612375,Softmax:
REFERENCES,0.44625407166123776,"max
φ
Jπ(φ) = E(s,a)∼D """
REFERENCES,0.44788273615635177,"log πφ(a | s) ·
exp( 1"
REFERENCES,0.4495114006514658,"α ˆA(s, a))
P"
REFERENCES,0.4511400651465798,"(si,ai)∼Batch exp( 1"
REFERENCES,0.4527687296416938,"α ˆA(si, ai)) #"
REFERENCES,0.4543973941368078,".
(28)"
REFERENCES,0.4560260586319218,"C.2
BCQ-EM"
REFERENCES,0.4576547231270358,The value network of BCQ-EM is trained by minimizing the following loss:
REFERENCES,0.4592833876221498,"min
θ
JQ(θ) = E(st,at,st+1)∼D
h
(Rt −Qθ(st, at))2i
(29)"
REFERENCES,0.4609120521172638,"Rt =
max
0<n≤nmax Qt,n,
Qt,n =
rt + γQt+1,n−1(st+1, ˆat+1)
if
n > 0,
Q(st, ˆat)
if
n = 0,
(30)"
REFERENCES,0.46254071661237783,"where ˆat corresponds to the perturbed actions, sampled from the generative model Gw(st)."
REFERENCES,0.46416938110749184,"The perturbation network of BCQ-EM is trained by minimizing the following loss:
min
φ Jξ(φ) = −Es∼D [Qθ(s, ai + ξφ(s, ai, Φ))] ,
{ai ∼Gw(s)}n
i=1,
(31)"
REFERENCES,0.46579804560260585,"where ξφ(s, ai, Φ) is a perturbation model, which outputs an adjustment to an action a in the range
[−Φ, Φ]. We adopt conditional variational auto-encoder to represent the generative model Gw(s)
and it is trained to match the state-action pairs sampled from D by minimizing the cross-entropy
loss-function."
REFERENCES,0.46742671009771986,"C.3
HYPER-PARAMETER AND NETWORK STRUCTURE"
REFERENCES,0.46905537459283386,"Table 2: Hyper-parameter Sheet
Hyper-Parameter
Value
Critic Learning Rate
1e-3
Actor Learning Rate
1e-3
Optimizer
Adam
Target Update Rate (κ)
0.005
Memory Update Period
100
Batch Size
128
Discount Factor
0.99
Gradient Steps per Update
200
Maximum Length d
Episode Length T"
REFERENCES,0.47068403908794787,Table 3: Hyper-Parameter τ used in VEM across different tasks
REFERENCES,0.4723127035830619,"AntMaze-ﬁxed
umaze
medium
large
0.4
0.3
0.3"
REFERENCES,0.4739413680781759,"AntMaze-diverse
umaze
medium
large
0.3
0.4
0.1"
REFERENCES,0.4755700325732899,"Adroit-human
door
hammer
pen
0.4
0.4
0.4"
REFERENCES,0.4771986970684039,"Adroit-cloned
door
hammer
pen
0.2
0.3
0.1"
REFERENCES,0.4788273615635179,"Adroit-expert
door
hammer
pen
0.3
0.3
0.3"
REFERENCES,0.4804560260586319,"MuJoCo-medium
walker2d
halfcheetah
hopper
0.3
0.4
0.5"
REFERENCES,0.4820846905537459,"MuJoCo-random
walker2d
halfcheetah
hopper
0.5
0.6
0.7"
REFERENCES,0.4837133550488599,"We use a fully connected neural network as a function approximation with 256 hid-
den units and ReLU as an activation function.
The structure of the actor network
is [(state dim, 256), (256, 256), (256, action dim)].
The structure of the value network is
[(state dim, 256), (256, 256), (256, 1)]."
REFERENCES,0.48534201954397393,Published as a conference paper at ICLR 2022
REFERENCES,0.48697068403908794,"D
ADDITIONAL EXPERIMENTS ON D4RL"
REFERENCES,0.48859934853420195,"D.1
ABLATION STUDY"
REFERENCES,0.49022801302931596,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 200 400"
REFERENCES,0.49185667752442996,Episode Return
REFERENCES,0.49348534201954397,"VEM
VEM (abs)"
REFERENCES,0.495114006514658,(a) door-human
REFERENCES,0.496742671009772,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps −200 0 200 400 600"
REFERENCES,0.498371335504886,Episode Return
REFERENCES,0.5,"VEM
VEM (abs)"
REFERENCES,0.501628664495114,(b) hammer-human
REFERENCES,0.503257328990228,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 50 100"
REFERENCES,0.504885993485342,Episode Return
REFERENCES,0.506514657980456,"VEM
VEM (abs)"
REFERENCES,0.50814332247557,(c) relocate-human
REFERENCES,0.509771986970684,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 1000 2000"
REFERENCES,0.511400651465798,Episode Return
REFERENCES,0.5130293159609121,"VEM
VEM (abs)"
REFERENCES,0.5146579804560261,(d) pen-human
REFERENCES,0.5162866449511401,"Figure 5: Comparison results between expectile loss and quantile loss on Adroit tasks. We respec-
tively name our algorithm with expectile loss and quantile loss as VEM and VEM (abs)."
REFERENCES,0.5179153094462541,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 1000 2000 3000"
REFERENCES,0.5195439739413681,Episode Return
REFERENCES,0.5211726384364821,VEM (0.1)
REFERENCES,0.5228013029315961,VEM (0.3)
REFERENCES,0.5244299674267101,VEM (0.5)
REFERENCES,0.5260586319218241,VEM (0.7)
REFERENCES,0.5276872964169381,VEM (0.8)
REFERENCES,0.5293159609120521,(a) pen-human
REFERENCES,0.5309446254071661,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 200 400 600"
REFERENCES,0.5325732899022801,Episode Return
REFERENCES,0.5342019543973942,(b) door-human
REFERENCES,0.5358306188925082,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps −250 0 250 500 750"
REFERENCES,0.5374592833876222,Episode Return
REFERENCES,0.5390879478827362,(c) hammer-human
REFERENCES,0.5407166123778502,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2000 4000 6000 8000 10000"
REFERENCES,0.5423452768729642,Estimation Value
REFERENCES,0.5439739413680782,VEM (0.1)
REFERENCES,0.5456026058631922,VEM (0.3)
REFERENCES,0.5472312703583062,VEM (0.5)
REFERENCES,0.5488599348534202,VEM (0.7)
REFERENCES,0.5504885993485342,VEM (0.8)
REFERENCES,0.5521172638436482,(d) pen-human
REFERENCES,0.5537459283387622,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 200 400 600 800"
REFERENCES,0.5553745928338762,Estimation Value
REFERENCES,0.5570032573289903,(e) door-human
REFERENCES,0.5586319218241043,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 5000 10000"
REFERENCES,0.5602605863192183,Estimation Value
REFERENCES,0.5618892508143323,(f) hammer-human
REFERENCES,0.5635179153094463,"Figure 6: The results of VEM (τ) with various τ in Adroit tasks. The results in the upper row are
the performance. The results in the bottom row are the estimation value."
REFERENCES,0.5651465798045603,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 1000 2000"
REFERENCES,0.5667752442996743,Episode Return
REFERENCES,0.5684039087947883,"VEM
TD3+BC(0.5)"
REFERENCES,0.5700325732899023,TD3+BC(2.5)
REFERENCES,0.5716612377850163,TD3+BC(4.5)
REFERENCES,0.5732899022801303,(a) pen-human
REFERENCES,0.5749185667752443,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 200 400"
REFERENCES,0.5765472312703583,Episode Return
REFERENCES,0.5781758957654723,"VEM
TD3+BC(0.5)"
REFERENCES,0.5798045602605864,TD3+BC(2.5)
REFERENCES,0.5814332247557004,TD3+BC(4.5)
REFERENCES,0.5830618892508144,(b) door-human
REFERENCES,0.5846905537459284,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps −200 0 200 400 600"
REFERENCES,0.5863192182410424,Episode Return
REFERENCES,0.5879478827361564,"VEM
TD3+BC(0.5)"
REFERENCES,0.5895765472312704,TD3+BC(2.5)
REFERENCES,0.5912052117263844,TD3+BC(4.5)
REFERENCES,0.5928338762214984,(c) hammer-human
REFERENCES,0.5944625407166124,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 50 100"
REFERENCES,0.5960912052117264,Episode Return
REFERENCES,0.5977198697068404,"VEM
TD3+BC(0.5)"
REFERENCES,0.5993485342019544,TD3+BC(2.5)
REFERENCES,0.6009771986970684,TD3+BC(4.5)
REFERENCES,0.6026058631921825,(d) relocate-human
REFERENCES,0.6042345276872965,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2 4 6"
REFERENCES,0.6058631921824105,Estimation Error ×1012
REFERENCES,0.6074918566775245,"VEM
TD3+BC(0.5)"
REFERENCES,0.6091205211726385,TD3+BC(2.5)
REFERENCES,0.6107491856677525,TD3+BC(4.5)
REFERENCES,0.6123778501628665,(e) pen-human
REFERENCES,0.6140065146579805,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2 4"
REFERENCES,0.6156351791530945,Estimation Error ×1012
REFERENCES,0.6172638436482085,"VEM
TD3+BC(0.5)"
REFERENCES,0.6188925081433225,TD3+BC(2.5)
REFERENCES,0.6205211726384365,TD3+BC(4.5)
REFERENCES,0.6221498371335505,(f) door-human
REFERENCES,0.6237785016286646,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2 4 6"
REFERENCES,0.6254071661237784,Estimation Error ×1012
REFERENCES,0.6270358306188925,"VEM
TD3+BC(0.5)"
REFERENCES,0.6286644951140065,TD3+BC(2.5)
REFERENCES,0.6302931596091205,TD3+BC(4.5)
REFERENCES,0.6319218241042345,(g) hammer-human
REFERENCES,0.6335504885993485,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2 4"
REFERENCES,0.6351791530944625,Estimation Error ×1012
REFERENCES,0.6368078175895765,"VEM
TD3+BC(0.5)"
REFERENCES,0.6384364820846905,TD3+BC(2.5)
REFERENCES,0.6400651465798045,TD3+BC(4.5)
REFERENCES,0.6416938110749185,(h) relocate-human
REFERENCES,0.6433224755700325,"Figure 7: Comparison results between VEM with TD3+BC. We adopt different hyper-parameters
α ∈{0.5, 2.5, 4.5} in TD3+BC to test its performance. The upper row are the performance. The
results in the bottom row are the estimation error (the unit is 1012)."
REFERENCES,0.6449511400651465,Published as a conference paper at ICLR 2022
REFERENCES,0.6465798045602605,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.6482084690553745,Episode Return
REFERENCES,0.6498371335504886,"VEM
VEM-1step
VEM-nstep"
REFERENCES,0.6514657980456026,(a) medium-play
REFERENCES,0.6530944625407166,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.00 0.25 0.50 0.75"
REFERENCES,0.6547231270358306,Episode Return
REFERENCES,0.6563517915309446,"VEM
VEM-1step
VEM-nstep"
REFERENCES,0.6579804560260586,(b) medium-diverse
REFERENCES,0.6596091205211726,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.2 0.4 0.6"
REFERENCES,0.6612377850162866,Episode Return
REFERENCES,0.6628664495114006,"VEM
VEM-1step
VEM-nstep"
REFERENCES,0.6644951140065146,(c) large-play
REFERENCES,0.6661237785016286,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.2 0.4 0.6"
REFERENCES,0.6677524429967426,Episode Return
REFERENCES,0.6693811074918566,"VEM
VEM-1step
VEM-nstep"
REFERENCES,0.6710097719869706,(d) large-diverse
REFERENCES,0.6726384364820847,Figure 8: The comparison between episodic memory and n-step value estimation on AntMaze tasks.
REFERENCES,0.6742671009771987,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 200 400"
REFERENCES,0.6758957654723127,Episode Return
REFERENCES,0.6775244299674267,"VEM
BCQ-EM
BCQ"
REFERENCES,0.6791530944625407,(a) door-human
REFERENCES,0.6807817589576547,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 500 1000"
REFERENCES,0.6824104234527687,Episode Return
REFERENCES,0.6840390879478827,"VEM
BCQ-EM
BCQ"
REFERENCES,0.6856677524429967,(b) hammer-human
REFERENCES,0.6872964169381107,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 25 50 75 100"
REFERENCES,0.6889250814332247,Episode Return
REFERENCES,0.6905537459283387,"VEM
BCQ-EM
BCQ"
REFERENCES,0.6921824104234527,(c) relocate-human
REFERENCES,0.6938110749185668,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 1000 2000"
REFERENCES,0.6954397394136808,Episode Return
REFERENCES,0.6970684039087948,"VEM
BCQ-EM
BCQ"
REFERENCES,0.6986970684039088,(d) pen-human
REFERENCES,0.7003257328990228,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2 4"
REFERENCES,0.7019543973941368,Estimation Error ×1013
REFERENCES,0.7035830618892508,"VEM
BCQ-EM
BCQ"
REFERENCES,0.7052117263843648,(e) door-human
REFERENCES,0.7068403908794788,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0.00 0.25 0.50 0.75 1.00"
REFERENCES,0.7084690553745928,Estimation Error ×1014
REFERENCES,0.7100977198697068,"VEM
BCQ-EM
BCQ"
REFERENCES,0.7117263843648208,(f) hammer-human
REFERENCES,0.7133550488599348,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2 4 6"
REFERENCES,0.7149837133550488,Estimation Error ×1013
REFERENCES,0.7166123778501629,"VEM
BCQ-EM
BCQ"
REFERENCES,0.7182410423452769,(g) relocate-human
REFERENCES,0.7198697068403909,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2 4 6 8"
REFERENCES,0.7214983713355049,Estimation Error ×1013
REFERENCES,0.7231270358306189,"VEM
BCQ-EM
BCQ"
REFERENCES,0.7247557003257329,(h) pen-human
REFERENCES,0.7263843648208469,"Figure 9: The comparison between VEM, BCQ-EM and BCQ on Adroit-human tasks. The results
in the upper row are the performance. The results in the bottom row are the estimation error, where
the unit is 1013."
REFERENCES,0.7280130293159609,"D.2
COMPLETE TRAINING CURVES AND VALUE ESTIMATION ERROR"
REFERENCES,0.7296416938110749,"0.0
0.1
0.2
0.3
0.4
Million Steps 0 200 400"
REFERENCES,0.7312703583061889,Episode Return
REFERENCES,0.7328990228013029,"VEM
VEM(auto)"
REFERENCES,0.7345276872964169,(a) Episode return
REFERENCES,0.7361563517915309,"0.0
0.1
0.2
0.3
0.4
Million Steps 0.2 0.3 0.4 0.5 τ"
REFERENCES,0.737785016286645,VEM(auto)
REFERENCES,0.739413680781759,(b) τ value
REFERENCES,0.741042345276873,"Figure 10: Comparison between ﬁxed τ (VEM) and auto-tuning τ (VEM(auto)) in the door-human
task."
REFERENCES,0.742671009771987,Published as a conference paper at ICLR 2022
REFERENCES,0.744299674267101,"0.0
0.1
0.2
0.3
0.4
Million Steps 0 200 400 600"
REFERENCES,0.745928338762215,Value Estimation
REFERENCES,0.747557003257329,VEM(1)
REFERENCES,0.749185667752443,VEM(1000)
REFERENCES,0.750814332247557,(a) door-human
REFERENCES,0.752442996742671,"0.0
0.1
0.2
0.3
0.4
Million Steps 0 2000 4000 6000"
REFERENCES,0.754071661237785,Value Estimation
REFERENCES,0.755700325732899,VEM(1)
REFERENCES,0.757328990228013,VEM(1000)
REFERENCES,0.758957654723127,(b) hammer-human
REFERENCES,0.760586319218241,"0.0
0.1
0.2
0.3
0.4
Million Steps 0 500 1000 1500 2000"
REFERENCES,0.762214983713355,Value Estimation
REFERENCES,0.7638436482084691,VEM(1)
REFERENCES,0.7654723127035831,VEM(1000)
REFERENCES,0.7671009771986971,(c) relocate-human
REFERENCES,0.7687296416938111,"0.0
0.1
0.2
0.3
0.4
Million Steps 2000 4000"
REFERENCES,0.7703583061889251,Value Estimation
REFERENCES,0.7719869706840391,VEM(1)
REFERENCES,0.7736156351791531,VEM(1000)
REFERENCES,0.7752442996742671,(d) pen-human
REFERENCES,0.7768729641693811,"Figure 11: Value estimation of VEM (nmax) in adroit-human tasks, where nmax is the maximal
rollout step for memory control (see Equation 11). We set τ = 0.5 in all tasks."
REFERENCES,0.7785016286644951,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.00 0.25 0.50 0.75"
REFERENCES,0.7801302931596091,Episode Return
REFERENCES,0.7817589576547231,"VEM
BAIL"
REFERENCES,0.7833876221498371,(a) antmaze-umaze
REFERENCES,0.7850162866449512,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.7866449511400652,Episode Return
REFERENCES,0.7882736156351792,"VEM
BAIL"
REFERENCES,0.7899022801302932,"(b)
antmaze-umaze-
diverse"
REFERENCES,0.7915309446254072,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.2 0.4 0.6 0.8"
REFERENCES,0.7931596091205212,Episode Return
REFERENCES,0.7947882736156352,"VEM
BAIL"
REFERENCES,0.7964169381107492,"(c)
antmaze-medium-
play"
REFERENCES,0.7980456026058632,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.00 0.25 0.50 0.75"
REFERENCES,0.7996742671009772,Episode Return
REFERENCES,0.8013029315960912,"VEM
BAIL"
REFERENCES,0.8029315960912052,"(d)
antmaze-medium-
diverse"
REFERENCES,0.8045602605863192,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.2 0.4 0.6"
REFERENCES,0.8061889250814332,Episode Return
REFERENCES,0.8078175895765473,"VEM
BAIL"
REFERENCES,0.8094462540716613,(e) antmaze-large-play
REFERENCES,0.8110749185667753,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.2 0.4 0.6"
REFERENCES,0.8127035830618893,Episode Return
REFERENCES,0.8143322475570033,"VEM
BAIL"
REFERENCES,0.8159609120521173,(f) antmaze-large-diverse
REFERENCES,0.8175895765472313,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 200 400"
REFERENCES,0.8192182410423453,Episode Return
REFERENCES,0.8208469055374593,"VEM
BAIL"
REFERENCES,0.8224755700325733,(g) door-human
REFERENCES,0.8241042345276873,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps −200 0 200 400 600"
REFERENCES,0.8257328990228013,Episode Return
REFERENCES,0.8273615635179153,"VEM
BAIL"
REFERENCES,0.8289902280130294,(h) hammer-human
REFERENCES,0.8306188925081434,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 50 100"
REFERENCES,0.8322475570032574,Episode Return
REFERENCES,0.8338762214983714,"VEM
BAIL"
REFERENCES,0.8355048859934854,(i) relocate-human
REFERENCES,0.8371335504885994,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 1000 2000"
REFERENCES,0.8387622149837134,Episode Return
REFERENCES,0.8403908794788274,"VEM
BAIL"
REFERENCES,0.8420195439739414,(j) pen-human
REFERENCES,0.8436482084690554,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps −50 0 50 100 150"
REFERENCES,0.8452768729641694,Episode Return
REFERENCES,0.8469055374592834,"VEM
BAIL"
REFERENCES,0.8485342019543974,(k) door-cloned
REFERENCES,0.8501628664495114,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps −200 0 200 400"
REFERENCES,0.8517915309446255,Episode Return
REFERENCES,0.8534201954397395,"VEM
BAIL"
REFERENCES,0.8550488599348535,(l) hammer-cloned
REFERENCES,0.8566775244299675,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 1000 1500 2000 2500"
REFERENCES,0.8583061889250815,Episode Return
REFERENCES,0.8599348534201955,"VEM
BAIL"
REFERENCES,0.8615635179153095,(m) pen-cloned
REFERENCES,0.8631921824104235,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 500 1000 1500 2000"
REFERENCES,0.8648208469055375,Episode Return
REFERENCES,0.8664495114006515,"VEM
BAIL"
REFERENCES,0.8680781758957655,(n) hopper-medium
REFERENCES,0.8697068403908795,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 1000 2000 3000"
REFERENCES,0.8713355048859935,Episode Return
REFERENCES,0.8729641693811075,"VEM
BAIL"
REFERENCES,0.8745928338762216,(o) walker2d-medium
REFERENCES,0.8762214983713354,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 2000 4000"
REFERENCES,0.8778501628664495,Episode Return
REFERENCES,0.8794788273615635,"VEM
BAIL"
REFERENCES,0.8811074918566775,(p) halfcheetah-medium
REFERENCES,0.8827361563517915,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 100 200 300 400"
REFERENCES,0.8843648208469055,Episode Return
REFERENCES,0.8859934853420195,"VEM
BAIL"
REFERENCES,0.8876221498371335,(q) hopper-random
REFERENCES,0.8892508143322475,"0.0
0.1
0.2
0.3
0.4
Million Steps 200 400 600"
REFERENCES,0.8908794788273615,Episode Return
REFERENCES,0.8925081433224755,"VEM
BAIL"
REFERENCES,0.8941368078175895,(r) walker2d-random
REFERENCES,0.8957654723127035,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 1000 2000"
REFERENCES,0.8973941368078175,Episode Return
REFERENCES,0.8990228013029316,"VEM
BAIL"
REFERENCES,0.9006514657980456,(s) halfcheetah-random
REFERENCES,0.9022801302931596,Figure 12: The training curves of VEM and BAIL on D4RL tasks.
REFERENCES,0.9039087947882736,Published as a conference paper at ICLR 2022
REFERENCES,0.9055374592833876,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.3 0.4 0.5 0.6"
REFERENCES,0.9071661237785016,Estimation Error VEM
REFERENCES,0.9087947882736156,(a) antmaze-umaze
REFERENCES,0.9104234527687296,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.9120521172638436,Estimation Error VEM
REFERENCES,0.9136807817589576,"(b)
antmaze-umaze-
diverse"
REFERENCES,0.9153094462540716,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.0 0.1 0.2 0.3 0.4"
REFERENCES,0.9169381107491856,Estimation Error VEM
REFERENCES,0.9185667752442996,"(c)
antmaze-medium-
play"
REFERENCES,0.9201954397394136,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.1 0.2 0.3 0.4"
REFERENCES,0.9218241042345277,Estimation Error VEM
REFERENCES,0.9234527687296417,"(d)
antmaze-medium-
diverse"
REFERENCES,0.9250814332247557,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.05 0.10 0.15"
REFERENCES,0.9267100977198697,Estimation Error VEM
REFERENCES,0.9283387622149837,(e) antmaze-large-play
REFERENCES,0.9299674267100977,"0.0
0.1
0.2
0.3
0.4
0.5
Million Steps 0.025 0.050 0.075 0.100"
REFERENCES,0.9315960912052117,Estimation Error VEM
REFERENCES,0.9332247557003257,(f) antmaze-large-diverse
REFERENCES,0.9348534201954397,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 200 400 600 800"
REFERENCES,0.9364820846905537,Estimation Error VEM
REFERENCES,0.9381107491856677,(g) door-human
REFERENCES,0.9397394136807817,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 2000 4000"
REFERENCES,0.9413680781758957,Estimation Error VEM
REFERENCES,0.9429967426710097,(h) hammer-human
REFERENCES,0.9446254071661238,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 1000 1500 2000"
REFERENCES,0.9462540716612378,Estimation Error VEM
REFERENCES,0.9478827361563518,(i) relocate-human
REFERENCES,0.9495114006514658,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 5000 10000 15000 20000"
REFERENCES,0.9511400651465798,Estimation Error VEM
REFERENCES,0.9527687296416938,(j) pen-human
REFERENCES,0.9543973941368078,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 100 200 300 400"
REFERENCES,0.9560260586319218,Estimation Error VEM
REFERENCES,0.9576547231270358,(k) door-cloned
REFERENCES,0.9592833876221498,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 500 1000 1500 2000"
REFERENCES,0.9609120521172638,Estimation Error VEM
REFERENCES,0.9625407166123778,(l) hammer-cloned
REFERENCES,0.9641693811074918,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 400 600 800"
REFERENCES,0.9657980456026058,Estimation Error VEM
REFERENCES,0.9674267100977199,(m) pen-cloned
REFERENCES,0.9690553745928339,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 50 100 150"
REFERENCES,0.9706840390879479,Estimation Error VEM
REFERENCES,0.9723127035830619,(n) hopper-medium
REFERENCES,0.9739413680781759,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 40 60 80 100"
REFERENCES,0.9755700325732899,Estimation Error VEM
REFERENCES,0.9771986970684039,(o) walker2d-medium
REFERENCES,0.9788273615635179,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 200 400 600"
REFERENCES,0.9804560260586319,Estimation Error VEM
REFERENCES,0.9820846905537459,(p) halfcheetah-medium
REFERENCES,0.9837133550488599,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 0 100 200 300 400"
REFERENCES,0.9853420195439739,Estimation Error VEM
REFERENCES,0.9869706840390879,(q) hopper-random
REFERENCES,0.988599348534202,"0.0
0.1
0.2
0.3
0.4
Million Steps 0 2000 4000 6000"
REFERENCES,0.990228013029316,Estimation Error VEM
REFERENCES,0.99185667752443,(r) walker2d-random
REFERENCES,0.993485342019544,"0.0
0.2
0.4
0.6
0.8
1.0
Million Steps 40 60 80 100"
REFERENCES,0.995114006514658,Estimation Error VEM
REFERENCES,0.996742671009772,(s) halfcheetah-random
REFERENCES,0.998371335504886,"Figure 13: The value estimation error of VEM on D4RL tasks. The estimation error refers to the
average estimated state values minus the average returns."
