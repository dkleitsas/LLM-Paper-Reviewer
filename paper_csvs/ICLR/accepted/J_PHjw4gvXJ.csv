Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003861003861003861,"To solve the imbalanced classiﬁcation, methods of weighting examples have been
proposed. Recent work has studied to assign adaptive weights to training exam-
ples through learning mechanisms. Speciﬁcally, similar to classiﬁcation models,
the weights are regarded as parameters that need to be learned. However, the
algorithms in recent work use local information to approximately optimize the
weights, which may lead to inaccurate learning of the weights. In this work, we
ﬁrst propose a novel mechanism of learning with a constraint, which can accu-
rately train the weights and model. Then, we propose a combined method of
our learning mechanism and the existing work, which can promote each other to
perform better. Our method can be applied to any type of deep network model.
Experiments show that compared with state-of-the-art algorithms, our method has
signiﬁcant improvement in varieties of settings, including text and image classiﬁ-
cation over different imbalance ratios, binary and multi-class classiﬁcation."
INTRODUCTION,0.007722007722007722,"1
INTRODUCTION"
INTRODUCTION,0.011583011583011582,"Classiﬁcation is a fundamental task in machine learning, but in practical classiﬁcation applications,
the number of examples among classes may differ greatly, even by several orders of magnitude.
Standard learning methods train the classiﬁcation model on such an imbalanced data set, which
makes the trained model biased. This bias is that the model will prefer the majority class and easily
misclassify the minority class examples. This class-imbalance problem exists in many domains,
such as Twitter spam detection (Li & Liu, 2018), named entity recognition (Grancharova et al.,
2020) in text classiﬁcation, and object detection (Oksuz et al., 2020), video surveillance (Wu &
Chang, 2003) in image classiﬁcation."
INTRODUCTION,0.015444015444015444,"There are very rich research lines on using the methods of weighting examples to solve the class
imbalance problem. In general, the weight of the minority class is higher than that of the majority
class, so that the bias towards the majority class is alleviated. Typically, the example weight value of
each class is often set to inverse class frequency (Wang et al., 2017) or inverse square root of class
frequency (Mahajan et al., 2018). However, the example weights in these methods are designed
empirically, hence they can not be adapted to different datasets and may perform poorly."
INTRODUCTION,0.019305019305019305,"Recent work has studied the methods of using learning mechanisms to adaptively calculate the ex-
ample weights. Ren et al. (2018) propose to use a meta-learning paradigm (Hospedales et al., 2020)
to learn the weights. In this method, the example weights can be regarded as a meta-learner and
the classiﬁcation model is a learner. The meta-learner guides the learner to learn by weighting the
example loss in the model optimization objective. More speciﬁcally, the model objective is to get
the optimal model that minimizes the example-weighted loss of the imbalanced training set. Ob-
viously, different weights will affect the performance of the optimal model. Which weight values
make the corresponding optimal model the best? This method collects a small balanced validation
set and evaluates the weight values through the validation performance of the model. Therefore, the
meta-learner objective, namely meta-objective, gives the best weights that make the optimal model
minimize the loss of the balanced validation set. This optimization problem is challenging. The key
is that, in the meta-objective, the weights indirectly affect the loss through the optimal model, so it"
INTRODUCTION,0.023166023166023165,∗Corresponding author
INTRODUCTION,0.02702702702702703,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03088803088803089,"is necessary to clearly deﬁne the dependence of the weights and the optimal model in the model ob-
jective for optimizing the weights. However, it is expensive to get this dependence through multiple
gradient descent steps in the model objective. Ren et al. (2018) propose an online approximation
method to estimate this dependence, that is, the method trains the model using a gradient descent
step in the model objective and then can determine the relationship between the weights and the
trained model in this step. Hu et al. (2019) propose to update the example weights iteratively to
replace the re-estimation proposed by Ren et al. (2018), but also adopt the local approximation to
optimize the weights. However, this approximation only considers the inﬂuence of the weights on
the trained model in a short term (in a descent step), resulting in inaccurate learning of the weights."
INTRODUCTION,0.03474903474903475,"In this paper, we ﬁrstly propose a novel learning mechanism that can obtain the precise relationship
between the weights and the trained model in the model objective, so that the weights and model can
be optimized more accurately. In this mechanism, we convert the model objective into an equation
of the current model and weights. Then, we derive their relationship from this equation, and then
we use this relationship to optimize the weights in the meta-objective and update the corresponding
model. Since this optimization process always satisﬁes this equation, we call it learning with a
constraint. However, the mechanism only uses the model objective to calculate the relationship but
does not optimize the model for the model objective. To solve this problem, we integrate the method
proposed by Hu et al. (2019) into our learning mechanism and propose a combined algorithm. In this
algorithm, the method of Hu et al. can help to further optimize the model in the model objective, and
our learning mechanism can make the weights and model learn more accurately. Finally, we conduct
a lot of experiments to validate the effectiveness of this algorithm. The experimental settings include
(1) different domains, namely text and image classiﬁcation; (2) different scenarios, namely binary
and multi-class classiﬁcation, (3) different imbalance ratios. The results show that our algorithm
not only outperforms the state-of-the-art (SOTA) method in data weighting but also performs best
among other comparison methods in varieties of settings."
INTRODUCTION,0.03861003861003861,"The remainder of this paper is organized as follows. Section 2 introduces preliminaries of the two
objectives and the main idea of Hu et al. (2019). Section 3 presents our mechanism of learning with
a constraint and the combined algorithm. Section 4 presents the experimental settings and evaluation
results. Section 5 summarizes the related work and Section 6 concludes this paper."
PRELIMINARIES AND NOTATIONS,0.04247104247104247,"2
PRELIMINARIES AND NOTATIONS"
PRELIMINARIES AND NOTATIONS,0.04633204633204633,"Let (x, y) be the input and target pair. For example, in image classiﬁcation, x is the image and y is
the image label. Let Dtrain denote the train set, and Dtrain = {(xi, yi), 1 ⩽i ⩽N}. Let Dval be a
small balanced validation set, and Dval = {(xi, yi), 1 ⩽i ⩽M} where M ≪N. We denote neural
network model as Φ(x, θ), where θ ∈RK is the model parameter. The predicted value ˆy = Φ(x, θ).
We use loss function f(ˆy, y) to measure the difference between predicted value ˆy and target value
y, and the loss function of data xi is deﬁned as fi(θ) for clarity. Standard training method is to
minimize the expected loss on the training set: PN
i=1 fi(θ), and each example has same weight.
However, for an imbalanced data set, the model obtained by this method will be biased towards the
majority class. Here, we aim to learn a model parameter θ that is fair to the minority class and the
majority class by minimizing the weighted loss of training examples:"
PRELIMINARIES AND NOTATIONS,0.05019305019305019,"θ∗(w) = arg minθ N
X"
PRELIMINARIES AND NOTATIONS,0.05405405405405406,"i=1
wifi(θ)
(1)"
PRELIMINARIES AND NOTATIONS,0.05791505791505792,"where w = (w1, ..., wN)T is the weights of all training examples. We use Ltrain to represent the
weighted loss on the training set Dtrain. For a given w, we can obtain the corresponding optimal θ∗
from Eq.1. Thus, there is a dependence between θ∗and w and we write it as θ∗= θ∗(w)."
PRELIMINARIES AND NOTATIONS,0.06177606177606178,"Learning to Weight Examples The recent work (Ren et al., 2018) proposed a method of learning
the weights of training examples. In this method, the optimal w is to make the model parameter
θ∗obtained from Eq.1 minimize the loss on a balanced validation set. It means that this model
performs well on a balanced validation set, and it can fairly distinguish examples from different
classes. Formally, the optimal w is given as"
PRELIMINARIES AND NOTATIONS,0.06563706563706563,"w∗= arg minw
1
M M
X"
PRELIMINARIES AND NOTATIONS,0.0694980694980695,"i=1
f v
i (θ∗(w))
(2)"
PRELIMINARIES AND NOTATIONS,0.07335907335907337,Published as a conference paper at ICLR 2022
PRELIMINARIES AND NOTATIONS,0.07722007722007722,where the superscript v stands for validation set. Let Lval be the loss on the validation set Dval.
PRELIMINARIES AND NOTATIONS,0.08108108108108109,"Learning the Parameters The recent work (Hu et al., 2019) introduced an algorithm of solving the
model parameter θ∗and weight w∗. The algorithm optimizes θ and w alternately until convergence.
In each iteration, the algorithm utilizes a gradient descent step in Eq.1 to approximate the relation-
ship between θ and w, and then calculates the gradient ∇wLval and ∇θLtrain to update w and θ
respectively."
PRELIMINARIES AND NOTATIONS,0.08494208494208494,"More speciﬁcally, at the t-th iteration, the algorithm ﬁrst calculates the approximate relationship
between θ and w through the t-th gradient descent step in Eq.1. We deﬁne a matrix F(θ) =
(∇f1(θ), ..., ∇fN(θ)), whose column vector represents the derivative of fi(θ) with respect to θ,
so we calculate the derivative of ∇θLtrain with respect to θ as ∇θLtrain = F(θ)w. Then, the t-th
gradient descent step of θ is given as"
PRELIMINARIES AND NOTATIONS,0.0888030888030888,"ˆθt+1 = θt −ηθF(θt)wt
(3)"
PRELIMINARIES AND NOTATIONS,0.09266409266409266,"where ηθ is the descent step size on θ. In order to avoid very expensive calculations, the algorithm
ignores the inﬂuence of w on θt. Therefore, in the single gradient descent step, ˆθt+1 linearly depends
on w."
PRELIMINARIES AND NOTATIONS,0.09652509652509653,"Then, based on this linear dependence, the algorithm can calculate the gradient ∇wLval and uses
gradient descent to update w, and then updates θ again to make it perform better on validation set.
Substituting the updated ˆθt+1 into Eq.2, we have Lval =
1
M
PM
i=1 f v
i (ˆθt+1(w)). We can observe
that the w acts on ˆθt+1 and then affects Lval. Thus, combining Eq.3, we can calculate the gradient"
PRELIMINARIES AND NOTATIONS,0.10038610038610038,"∇wLval = (∇w ˆθt+1)
T ∇ˆθt+1Lval = −ηθF(θt)T ∇ˆθt+1Lval, so the update on w at t step is given as"
PRELIMINARIES AND NOTATIONS,0.10424710424710425,"wt+1 = wt + ηwηθF(θt)T ∇ˆθt+1Lval
(4)"
PRELIMINARIES AND NOTATIONS,0.10810810810810811,"where ηw is the descent step size on w. According to gradient descent theory, when ηw is appropri-
ately small, Lval(wt+1) ≤Lval(wt). This means using wt+1 to update θ performs better than wt.
Therefore, the algorithm substitutes the updated wt+1 into Eq.3 and obtain the new update on θ"
PRELIMINARIES AND NOTATIONS,0.11196911196911197,"θt+1 = θt −ηθF(θt)wt+1
(5)"
PRELIMINARIES AND NOTATIONS,0.11583011583011583,"where θt+1 satisﬁes Lval(θt+1) ≤Lval(ˆθt+1), that is, θt+1 have better validation performance than
ˆθt+1."
PRELIMINARIES AND NOTATIONS,0.11969111969111969,"Finally, the algorithm repeatedly calculates Eq.3, 4 and 5 and alternately optimizes θ and w until
convergence."
NEW METHOD OF LEARNING THE PARAMETERS,0.12355212355212356,"3
NEW METHOD OF LEARNING THE PARAMETERS"
NEW METHOD OF LEARNING THE PARAMETERS,0.1274131274131274,"In this section, we introduce a new method to learn the model parameter θ∗and weight w∗in Eq.1
and Eq.2. First, in Section 3.1, we propose to learn θ and w with a constraint, which can accurately
optimize θ and w. Then, in Section 3.2, we propose a combined method to train θ and w to make
the model parameter θ have better performance."
LEARNING WITH A CONSTRAINT,0.13127413127413126,"3.1
LEARNING WITH A CONSTRAINT"
LEARNING WITH A CONSTRAINT,0.13513513513513514,"In the section, we ﬁrst analyze the difﬁculty of solving θ∗and w∗. Gradient-based optimization is a
commonly used method in machine learning. Thus, we ﬁrst need to calculate the gradient ∇θLtrain
and ∇wLval. Based on Eq.2, we have ∇wLval = (∇wθ∗)T ∇θ∗Lval. However, it is difﬁcult to
explicitly give the form of function θ∗(w), resulting in ∇wLval cannot be calculated directly. The
previous work obtained the relationship between θ and w through the gradient descent process of
θ, and only considered the inﬂuence of w on θ in a single gradient descent step. Based on this
relationship, calculating the gradient and updating the parameter is not precise."
LEARNING WITH A CONSTRAINT,0.138996138996139,"Here, we obtain the relationship between θ and w from a new perspective. First, we observe the
gradient ∇θLtrain, that is,"
LEARNING WITH A CONSTRAINT,0.14285714285714285,"∇θLtrain = F(θ)w = c
(6)"
LEARNING WITH A CONSTRAINT,0.14671814671814673,Published as a conference paper at ICLR 2022
LEARNING WITH A CONSTRAINT,0.15057915057915058,"Algorithm 1: Learning to Weight Examples Using a Combination Method
Input : The network model parameter θ
The weight of training examples w
Training set Dtrain; Validation set Dval
The number of iterations of the combination method T
The number of iterations of our method T ′"
LEARNING WITH A CONSTRAINT,0.15444015444015444,1 Initialize model parameter θ and weight w
LEARNING WITH A CONSTRAINT,0.1583011583011583,2 for t = 0 ... T −1 do
LEARNING WITH A CONSTRAINT,0.16216216216216217,"3
Calculate the relationship between θ and w on Dtrain through Eq.3"
LEARNING WITH A CONSTRAINT,0.16602316602316602,"4
Optimize w on Dval through Eq.4"
LEARNING WITH A CONSTRAINT,0.16988416988416988,"5
Update θ through Eq.5"
LEARNING WITH A CONSTRAINT,0.17374517374517376,"6
for t′ = 0 ... T ′ −1 do"
LEARNING WITH A CONSTRAINT,0.1776061776061776,"7
Calculate the derivative ∇wθ on Dtrain through Eq.7"
LEARNING WITH A CONSTRAINT,0.18146718146718147,"8
Optimize w on Dval through Eq.8"
LEARNING WITH A CONSTRAINT,0.18532818532818532,"9
Update θ through Eq.9"
LEARNING WITH A CONSTRAINT,0.1891891891891892,Output: Trained model parameter θ∗and weight w∗
LEARNING WITH A CONSTRAINT,0.19305019305019305,"where c is the gradient value. We can see that changing the value of w can ﬁnd corresponding θ to
satisfy Eq.6. It means that there is a functional relationship between θ and w in Eq.6. Because all
θ and w satisfying this equation have the same value of ∇θLtrain, we also call Eq.6 a constraint of
θ and w. In particular, the optimal model parameter θ∗and w satisfy the constraint: F(θ∗)w = 0.
Then, we can make use of the constraint to derive a precise relationship between θ and w. Our
network model may be very complex, and we cannot explicitly give the functional form of θ and w
according to the constraint. However, by applying the implicit function theorem, the derivative of θ
with respect to w in Eq.6 can be obtained as follow"
LEARNING WITH A CONSTRAINT,0.1969111969111969,∇wθ = −[∇θ(F(θ)w)]−1F(θ) = −H−1F(θ)
LEARNING WITH A CONSTRAINT,0.20077220077220076,"where H ∈RK × RK is the Hessian matrix, namely, the second derivative of Ltrain with respect
to θ. However, calculating an exact Hessian matrix is very expensive. Especially nowadays network
models have a huge amount of parameters. In addition, in this case, we require the inverse of H,
rather than H itself. Therefore, we adopt diagonal approximation to evaluate H (Bishop, 2006).
In other words, we only need to calculate the diagonal elements of H. Furthermore, it is trivial to
calculate the inverse by taking the reciprocal of the diagonal elements. Let h ∈RK be the reciprocal
of the diagonal elements of H. Then, the derivative is evaluated as"
LEARNING WITH A CONSTRAINT,0.20463320463320464,"∇wθ = −diag(h)F(θ)
(7)"
LEARNING WITH A CONSTRAINT,0.2084942084942085,"Next, we can make use of the derivative to calculate the gradient ∇wLval, and then update w and θ.
The update process always satisﬁes the constraint of Eq.6, so we call it learning with a constraint.
Combining Eq.7, we have ∇wLval = −F(θ)T diag(h)∇θLval. Thus, the update of w is"
LEARNING WITH A CONSTRAINT,0.21235521235521235,"w′ = w + η′
wF(θ)T diag(h)∇θLval
(8)"
LEARNING WITH A CONSTRAINT,0.21621621621621623,"where η′
w is the step size. Then, we use the updated w′ to calculate the corresponding θ′ in the
constraint. Since we do not know the explicit functional form of θ and w in Eq.6, we use the ﬁrst
order derivative to approximate θ′. Combing Eq.7, θ′ is evaluated as"
LEARNING WITH A CONSTRAINT,0.22007722007722008,"θ′ ≈θ + ∇wθ(w′ −w) = θ −diag(h)F(θ)(w′ −w)
(9)"
LEARNING WITH A CONSTRAINT,0.22393822393822393,"Finally, under the condition of satisfying the constraint, we repeatedly optimize w and θ, corre-
sponding to Eq.8 and Eq.9, until convergence. The detailed proof of this convergence can be found
in Theorem 1 in Appendix A.2."
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.2277992277992278,"3.2
LEARNING IN A COMBINED WAY AND IMPLEMENTATION"
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.23166023166023167,"The method we proposed in Section3.1 still has a shortcoming. In the method, we make use of the
gradient ∇θLtrain of Eq.1 to obtain a constraint ∇θLtrain = c, and then calculate the solution of w
and θ under the constraint. However, this method only ensures that the solution is optimal in Eq.2"
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.23552123552123552,Published as a conference paper at ICLR 2022
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.23938223938223938,"under the constraint, and cannot guarantee that the solution of θ is optimal in Eq.1. Because our
method only use Eq.1 to obtain the constraint, but not to optimize θ for Eq.1, and when c ̸= 0, the
solution of θ is not optimal in Eq.1."
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.24324324324324326,"In order to calculate the better solution, our method needs to be combined with another algorithm
that can make θ reach the optimal in Eq.1. The method of Hu et al. in Section2 is a more appropriate
choice, rather than directly updating θ using the gradient ∇θLtrain. Because it will ﬁrst adjust w
and then update θ based on the new w. It is explained in Section2 that θ obtained in this way has a
better validation performance than θ directly using gradient descent."
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.2471042471042471,"Therefore, we propose a way to learn θ and w by combining our method in Section3.1 with the
method (Hu et al.) in Section2. In this way, we alternately use these two methods to learn θ and w.
In each iteration, we ﬁrst update θ and w using the method (Hu et al.). It can make θ reduce the value
of Ltrain and approach the optimal in Eq.1, while θ also reduces Lval and perform well on validation
set. Then, we optimize θ and w using our method until convergence, so that θ further reduces Lval
and has the best veriﬁcation performance among all θ with the same gradient ∇θLtrain."
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.25096525096525096,"This combined way can overcome the shortcomings of each method. On the one hand,the method
(Hu et al.) can use the gradient ∇θLtrain to update θ and make θ close to the optimal in Eq.1. It
makes up for the shortcoming that our method cannot optimize Eq.1. On the other hand, the method
(Hu et al.) only considers the inﬂuence of w on θ in a single gradient descent step, and then uses this
approximation to optimize Eq.2. Hence, θ obtained by the method (Hu et al.) may not be optimal for
Eq.2. Our method can make use of a constraint to derive an accurate functional relationship between
θ and w. Thus, by optimizing θ through our method, a better solution can be obtained."
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.2548262548262548,"This combination algorithm is listed in Algorithm 1. It takes T iterations to alternately use two
methods to optimize θ and w. In t-th iteration, it ﬁrst adopts the method (Hu et al.) to update w and
θ (lines 3-5), and then it uses our method to optimize w and θ repeatedly T times (lines 6-9), making
θ converges under the current constraint. Finally, it outputs the trained model parameter θ∗and
weight w∗. The proof of convergence of the Algorithm 1 can be found in Theorem 2 in Appendix
A.2. In addition, we discuss the convergence rate of Algorithm 1. According to the conclusion in the
paper (Ren et al., 2018), when we take T steps to update the parameter θ through the method (Hu et
al.), it can achieve ∥∇θLval ∥≤O( 1
√"
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.25868725868725867,T ) where ∥∇θLval ∥is the update precision of parameter θ.
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.2625482625482625,"For the method in Section 3.1, achieving the same precision requires O(
√"
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.26640926640926643,"T) steps. More detailed
proofs are in Theorem 3 in Appendix A.3. Therefore, as the combined method, Algorithm 1 needs
T × O(
√"
LEARNING IN A COMBINED WAY AND IMPLEMENTATION,0.2702702702702703,"T) = O(T
3
2 ) to converge."
EXPERIMENTS,0.27413127413127414,"4
EXPERIMENTS"
EXPERIMENTS,0.277992277992278,"In this section, we perform extensive experiments to validate the effectiveness of our method. First,
we describe the experimental setup in detail. Second, we compare different methods in two domains:
text and image classiﬁcation and in two situations: binary classiﬁcation and multi-class classiﬁca-
tion. Third, we design experiments to study the performance of our method in different imbalance
ratios. Moreover, we evaluate the performance of our methods with different metrics on a large-scale
data set in Appendix A.1."
EXPERIMENTAL SETUP,0.28185328185328185,"4.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.2857142857142857,"Models. We choose two network models for text and image classiﬁcation. Speciﬁcally, in text
classiﬁcation, we use the BERT (base, uncased) model (Devlin et al., 2018) to extract the 768-
dimensional representation of text data (Xiao, 2018) and then use a simple 4-layer fully connected
network (FCN) for classiﬁcation. The FCN model is given as Table 1. The pair of numbers in
brackets respectively indicate the sizes of input and output of the linear layer. In addition, the ﬁrst
two layers apply rectiﬁed linear unit activation (ReLU) function to avoid the vanishing gradient
problem during training, and the third layer uses a nonlinear activation function (Tanh) to enhance
the model learning ability. The last layer is the classiﬁcation layer, in which the size of output
depends on the number of classes in a classiﬁcation task. In image classiﬁcation, we use the ALL-
CNN-C network model that is a sequence of 9 convolution layers. Noting that our method does not
rely on the classiﬁcation model, and can also be applied to other models."
EXPERIMENTAL SETUP,0.28957528957528955,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.29343629343629346,Table 1: The network model for text classiﬁcation
EXPERIMENTAL SETUP,0.2972972972972973,Input 768-dimensional text representation
EXPERIMENTAL SETUP,0.30115830115830117,"(768, 768) linear layer, ReLU
(768, 768) linear layer, ReLU
(768, 10) linear layer, Tanh
(10, size of labels) linear layer"
EXPERIMENTAL SETUP,0.305019305019305,Table 2: Description of four data sets
EXPERIMENTAL SETUP,0.3088803088803089,"Data sets
Classes
Fine-tune / Pretrain"
EXPERIMENTAL SETUP,0.3127413127413127,"SST-2
2
2 × 500
SST-5
5
5 × 500
CIFAR10
10
10 × 4000
CIFAR100
2
0"
EXPERIMENTAL SETUP,0.3166023166023166,"Data Sets and Model Preparation. We choose 4 data sets for text and image classiﬁcation, and we
use part of training examples in the data sets to prepare for the subsequent training of the models.
The information of the four data sets we used is shown in Table 2. In text classiﬁcation, we use
two popular benchmark datasets. We use the SST-2 sentiment analysis benchmark (Socher et al.,
2013) for binary classiﬁcation, and use the SST-5 sentence sentiment (Socher et al., 2013) with
5 categories for multi-class classiﬁcation. In image classiﬁcation, we adopt the commonly-used
CIFAR10 (Schneider et al., 2019) for multi-class classiﬁcation experiment and select the examples
of class 0 and 1 from CIFAR100 (Schneider et al., 2019) to form a data set for binary classiﬁcation."
EXPERIMENTAL SETUP,0.3204633204633205,"To make subsequent experiments on strong models, we use part of training examples to ﬁne-tune
the BERT and pre-train the ALL-CNN-C model respectively. In text classiﬁcation, we use the text
data in a speciﬁc domain to ﬁne-tune BERT, so that we can extract the better text representations
from the ﬁne-tuned BERT and improve the performance of the FCN model. In image classiﬁcation,
we ﬁrst pre-train the ALL-CNN-C model using image data, and then the pre-trained model can be
helpful to improve downstream tasks for subsequent experiments. In Table 2, we list the number of
these examples. On SST-2 and SST-5 data sets, we take out 500 training examples of each class to
ﬁne-tune the BERT model. For the experiments on CIFAR10, we use 4000 training examples per
class to pre-train the ALL-CNN-C model. In the experiments on CIFAR100, we do not pre-train the
model, because it can perform well on this binary classiﬁcation task without pre-training. Noting
that the training examples used to improve the models will not be used in subsequent experiments."
EXPERIMENTAL SETUP,0.32432432432432434,"Comparison Methods. We compare our method with ﬁve approaches: (1) Baseline, a method
without any processing. In other words, the classiﬁcation model is directly trained on an imbalanced
training set. (2) Proportion, a commonly used method that weights examples by inverse class fre-
quency. (3) Hu et al.’s, is the SOTA approach (Hu et al., 2019) for data weighting, which is described
in Section 2 and implemented using the code1 provided by the authors. In addition, since we set a
small validation set in our experiments, the methods that need to be learned on the validation set are
easy to over-ﬁt. Therefore, in the following methods, we add regularization for the model param-
eters in Eq.2. (4) Hu et al.’s+R, a method that adds regularization to the validation learning in the
method (Hu et al.). (5) Two-phase (Wahab et al., 2017), a learning method divided into two phases.
It ﬁrst trains the model to learn a good classiﬁcation representation on an imbalanced training set
and then adjusts the imbalance bias of the model by learning on a balanced validation set. When the
model is trained on the validation set, we also add regularization."
EXPERIMENTAL SETUP,0.3281853281853282,"Training and Evaluation. In our experiments, we ﬁrst ﬁne-tune the BERT model and pre-train
the ALL-CNN-C model. In the following training, the text data is ﬁrst converted into vector rep-
resentations by BERT and then used to train the FCN model, and when training on CIFAR10, the
ALL-CNN-C model is initialized by the pre-trained model. Next, we apply different methods to
train the models. We divide this training process into 2 stages, and we take an imbalanced training
set and a small balanced validation set from the remaining training examples (not including the ex-
amples used for model preparation). In Stage 1, we only use the training set to train the models,
and the trained models can be regarded as the model initialization for subsequent training. For the
method (Hu et al.), the trained model has basic classiﬁcation capabilities, so that it can use stable
gradient information to optimize the weights during weighting the examples (Ren et al., 2018). For
the two-phase method, Stage 1 corresponds to its ﬁrst learning phase. In Stage 2, we train the models
according to their respective methods. Our method and the method (Hu et al.) will learn the model
and example weight using the training set and validation set together. For the two-phase method, we
only train the model on the balanced validation set, corresponding to its second phase. For Baseline
and Proportion, the models still learn on the training set."
EXPERIMENTAL SETUP,0.33204633204633205,1Code available at https://github.com/tanyuqian/learning-data-manipulation
EXPERIMENTAL SETUP,0.3359073359073359,Published as a conference paper at ICLR 2022
EXPERIMENTAL SETUP,0.33976833976833976,Table 3: Settings of the training process on 4 data sets
EXPERIMENTAL SETUP,0.3436293436293436,"Data sets
Fine-tune / Pretrain
Stage 1
Stage 2"
EXPERIMENTAL SETUP,0.3474903474903475,"SST-2 or SST-5
Adam(5e-6)
epochs:5
batch size:8"
EXPERIMENTAL SETUP,0.35135135135135137,"Adam(1e-2)
epochs:15
batch size:50"
EXPERIMENTAL SETUP,0.3552123552123552,"Adam(1e-2)
epochs:10
batch size:50"
EXPERIMENTAL SETUP,0.3590733590733591,"CIFAR10
follow the training
(Springenberg et al., 2014)"
EXPERIMENTAL SETUP,0.36293436293436293,"Adam(1e-6)
epochs:200
batch size:128"
EXPERIMENTAL SETUP,0.3667953667953668,"Adam(1e-5)
epochs:10
batch size:128"
EXPERIMENTAL SETUP,0.37065637065637064,"CIFAR100
-
follow the training
(Springenberg et al., 2014)"
EXPERIMENTAL SETUP,0.3745173745173745,"Adam(1e-4)
epochs:10
batch size:128"
EXPERIMENTAL SETUP,0.3783783783783784,Table 4: Results of six methods on four data sets
EXPERIMENTAL SETUP,0.38223938223938225,"Methods
SST-2
100:1000
SST-5
50:500
CIFAR10
50:500
CIFAR100
40:400"
EXPERIMENTAL SETUP,0.3861003861003861,"Baseline
75.52 ± 2.99
40.24 ± 0.99
69.95 ± 3.35
85.00 ± 1.10
Proportion
79.59 ± 3.35
42.59 ± 1.12
79.58 ± 0.34
85.20 ± 1.21
Two-phase
81.99 ± 0.80
42.60 ± 1.44
79.63 ± 0.44
86.00 ± 1.61
Hu et al.’s
81.57 ± 0.74
39.82 ± 1.07
79.36 ± 0.51
85.40 ± 1.07
Hu et al.’s+R
82.25 ± 1.16
40.14 ± 0.39
79.55 ± 0.21
86.50 ± 2.41
Ours.
82.58 ± 0.98
44.62 ± 1.08
79.71 ± 0.37
87.40 ± 1.66"
EXPERIMENTAL SETUP,0.38996138996138996,"The settings of the training process on the four data sets are listed in Table 3. Each cell in the table
indicates the settings in the current stage, including the optimizer used, learning rate, number of
epochs, and batch size, where the value in brackets is the learning rate. In text classiﬁcation, we use
Adam optimization. In image classiﬁcation, we ﬁrst follow the implementation of Springenberg et
al. to use SGD optimization, and then we use Adam to optimize in subsequent training."
EXPERIMENTAL SETUP,0.3938223938223938,"Finally, we indicate the evaluation criteria and hyperparameters tuning. We use the accuracy on the
full test set of each data set to evaluate the performance of models. During the training process,
the ﬁnal result may be over-ﬁtting, so we record the best step corresponding to the highest accuracy
on the validation set. In addition, we also tune a series of hyperparameters for different methods
and report the best in the test set. For the method (Hu et al.), we follow (Hu et al., 2019) and set
the decay of weight to avoid exploding value. The decay value is selected from {1, 5, 10}. For
Hu+regularization, we set the learning rate for weight update, which is taken from {1, 1e-1, 1e-2,
1e-3}. For our method, we set the learning rate and epochs for updating the weights during learning
with constraint, and they are taken from {1e-2, 1e-3, 1e-4, 1e-5} and {1, 5, 10, 15, 30} respectively.
We adopt general regularization, namely Lp-norm (Bohra & Unser, 2020), for the methods that need
to be trained on the validation set. The value of p is selected from {2, 4, 6, 8}. The log value of
regularization coefﬁcient is selected from {-4, ..., 4} for text data sets and {-4, ..., 9} for image data
sets. All experiments were implemented with Python 3.8 and PyTorch 1.8 and were evaluated on
a Linux server with RTX 3080 GPU and 128GB RAM. All results are averaged over 5 runs ± one
standard deviation."
RESULTS ON DIFFERENT DATA SETS,0.39768339768339767,"4.2
RESULTS ON DIFFERENT DATA SETS"
RESULTS ON DIFFERENT DATA SETS,0.4015444015444015,"We compare the performance of different methods on the four data sets. The four data sets involve
text and image domains, as well as binary classiﬁcation and multi-class classiﬁcation scenarios.
They can more comprehensively reﬂect the performance of our method. In this experiment, we set
an imbalance ratio of 1:10, which is the ratio of the example size of the minority class to the majority
class. In all data sets, we set class 0 as the minority class, and the rest as the majority class. On
the four data sets, the size of training examples is different. We set the number of training examples
for each majority class of the data set SST-2, SST-5, CIFAR10, CIFAR100 to 1000, 500, 500, 400
respectively. In addition, for all data sets, the number of examples in the validation set is 10 for
each class. The training set and validation set are randomly selected from the remaining training
examples in each data set."
RESULTS ON DIFFERENT DATA SETS,0.40540540540540543,Published as a conference paper at ICLR 2022
RESULTS ON DIFFERENT DATA SETS,0.4092664092664093,Table 5: Results of different imbalance ratios on SST-2 data set
RESULTS ON DIFFERENT DATA SETS,0.41312741312741313,"Methods
10:1000
20:1000
100:1000"
RESULTS ON DIFFERENT DATA SETS,0.416988416988417,"Baseline
49.92 ± 0.00
49.92 ± 0.00
75.52 ± 2.99
Proportion
60.63 ± 13.13
78.76 ± 2.40
79.59 ± 3.35
Two-phase
75.35 ± 8.90
80.52 ± 1.96
81.99 ± 0.80
Hu et al.’s
55.84 ± 11.84
73.61 ± 11.86
81.57 ± 0.74
Hu et al.’s+R
66.68 ± 13.99
79.53 ± 1.64
82.25 ± 1.16
Ours.
80.62 ± 0.93
81.14 ± 1.25
82.58 ± 0.98"
RESULTS ON DIFFERENT DATA SETS,0.42084942084942084,Table 6: Results of different imbalance ratios on CIFAR100 data set
RESULTS ON DIFFERENT DATA SETS,0.4247104247104247,"Methods
4:400
8:400
40:400"
RESULTS ON DIFFERENT DATA SETS,0.42857142857142855,"Baseline
64.40 ± 11.98
77.40 ± 12.23
85.00 ± 1.10
Proportion
60.50 ± 8.40
69.60 ± 8.56
85.20 ± 1.21
Two-phase
66.00 ± 13.95
79.50 ± 3.75
86.00 ± 1.61
Hu et al.’s
60.20 ± 8.19
69.10 ± 8.08
85.40 ± 1.07
Hu et al.’s+R
71.80 ± 11.73
82.50 ± 4.27
86.50 ± 2.41
Ours.
77.20 ± 3.75
82.60 ± 3.87
87.40 ± 1.66"
RESULTS ON DIFFERENT DATA SETS,0.43243243243243246,"Results The results on the four data sets are shown in Table 4. We can see that our method has
the best performance in these 4 data sets. Especially on the SST-5 data set, our method exceeds the
second-best method by more than 2 accuracy points. It demonstrates that our method can perform
well in multiple domains and different classiﬁcation scenarios. Hu et al.’+R and Two-phase are
competitive methods. On the SST-2 and CIFAR100, Hu et al.’+R is the second-best, and on the
SST-5 and CIFAR10, Two-phase also reaches the second-best. It shows that using a balanced data
set to simply adjust a biased model can also achieve good results. In addition, Hu et al.’+R performs
better than Hu et al.’s. on all data sets, and on the CIFAR100, it surpasses the latter by more than
1 accuracy point. It indicates that adding regularization to the validation learning can effectively
improve the method (Hu et al.). However, on the SST-5, the method (Hu et al.) performs worse than
the baseline, which may be due to the ineffectiveness of the approximation on SST-5. The accuracy
of the proportion method is lower than that of our method by more than 2 accuracy points on the
SST-2, SST-5, and CIFAR100. It shows that the method of learning weight has more advantages
than weighting empirically. The baseline method performs the worst on three data sets due to the
lack of measures to solve the imbalance."
RESULTS OF DIFFERENT IMBALANCE RATIOS,0.4362934362934363,"4.3
RESULTS OF DIFFERENT IMBALANCE RATIOS"
RESULTS OF DIFFERENT IMBALANCE RATIOS,0.44015444015444016,"We study the performance of our method with different imbalance ratios. In this experiment, we
use the SST-2 and CIFAR100 data sets, and we vary the imbalance ratio from {1:10, 1:50, 1:100}.
The example size of majority classes in the training set and the validation set are consistent with the
setting of Section 4.2. In addition, the training set and validation set are also randomly constructed."
RESULTS OF DIFFERENT IMBALANCE RATIOS,0.444015444015444,"Results Table 5 and Table 6 respectively shows the results of different imbalance ratios on SST-2
and CIFAR100 data set. The results are listed in the order of imbalance ratios of 1:10, 1:50, 1:100.
There are three main observations. First, our method has achieved the highest accuracy rates in all
imbalance ratio settings. It further demonstrates that our method can have excellent performance in
different situations, such as slight imbalance, extreme imbalance, etc. Second, as the training data
becomes more imbalanced, the performance of our method is more dominant than other methods.
On the SST-2, the accuracy of our method exceeds the second-best method by about 0.3 at 100:1000
and more than 5 accuracy points at 10:1000. Similarly, on the CIFAR100, the accuracy of our
method improves the second-best method over 0.9 at 100:1000 and more than 6 accuracy points
at 10:1000. It shows that our method is more advantageous in extreme imbalance. Third, when
the imbalance ratios are 1:50 and 1:100, the accuracy rates of the proportion method are almost
lower than other imbalance classiﬁcation methods. On the CIFAR100, the proportion method even
performs worse than the baseline. It indicates that as the data imbalance becomes serious, the
proportion method may not be effective. On the contrary, the advantage of the methods of learning
weights is more obvious."
RESULTS OF DIFFERENT IMBALANCE RATIOS,0.44787644787644787,Published as a conference paper at ICLR 2022
RELATED WORK,0.4517374517374517,"5
RELATED WORK"
RELATED WORK,0.4555984555984556,"There have been very rich studies on weighting examples for imbalance classiﬁcation, and these
studies can be grouped into two categories, namely empirical weighting and automatic weighting."
RELATED WORK,0.4594594594594595,"Empirical Weighting. The empirical weighting methods assign the manual weight values to the ex-
amples. Generally, the minority class example will be assigned a larger weight value than that of the
majority class, so as to relieve the bias of the model trained on the imbalanced data set. The methods
of weighting by class are ﬁrst proposed (King & Zeng, 2001). In these methods, the examples of
each class are manually set to the same value, such as inverse class frequency (Wang et al., 2017;
Huang et al., 2016) or inverse square root of class frequency (Mikolov et al., 2013; Mahajan et al.,
2018). Cui et al. (2019) proposed to calculate the effective number of examples as class frequency
and then also use its inverse to weight examples and achieved signiﬁcant improvements on long-
tailed training data. In addition, many methods of weighting by example have also been proposed.
Hard example mining (Shrivastava et al., 2016) thought focusing on the hard examples can improve
the model on the imbalanced data. Dong et al. (2017); Malisiewicz et al. (2011) proposed to utilize
the example loss to ﬁnd hard examples and assign them higher weights. Lin et al. (2017) proposed
to use the predicted probability to calculate higher weights for the hard examples and dynamically
adjust the weight values during training. Empirical weighting is convenient to implement and can
achieve excellent performance, though it cannot adapt to different data sets and may cause poor
performance. In addition, manually setting weights will also increase the engineering burden."
RELATED WORK,0.46332046332046334,"Automatic Weighting. The automatic weighting methods assign adaptive weights to the examples
through learning mechanisms. Curriculum learning can provide an example weighting strategy for
neural network models to learn on corrupted labels (Jiang et al., 2018; Wei et al., 2021), but the
method focuses on examples that are easy to learn (Zhang et al., 2020). On the contrary, imbalance
learning prefers hard examples, so it is different from the methods of data weighting in imbalance
classiﬁcation. Ren et al. (2018) proposed to learn the example weights by a meta-learning paradigm
(Zhang et al., 2021). This algorithm treats the example weights as a meta-learner and guides the
learner to learn on the imbalanced training set. The loss on the balanced validation set is used as
the meta-objective to optimize the example weights (Bai et al., 2021). In each iteration of updating
the weights, this algorithm uses a gradient descent step to approximate the relationship between the
weights and the learner. Hu et al. (2019) improved the algorithm by iteratively optimizing weights
instead of re-estimation at each iteration. Our work is based on the work of Ren and Hu et al. and
make further research. There is a key difference between our work and theirs. We use the model
optimization objective to derive the precise relationship between the model and weights, instead of
the local approximation strategy they used. Therefore, our algorithm can accurately optimize the
example weights and get a better model for imbalance classiﬁcation. The massive experimental
results show that our algorithm makes signiﬁcant improvements."
CONCLUSION,0.4671814671814672,"6
CONCLUSION"
CONCLUSION,0.47104247104247104,"In this paper, based on the work of Ren and Hu et al., we further propose an improved algorithm
to learn the example weights for imbalance classiﬁcation. In this algorithm, we propose a learning
mechanism that can accurately update the weights and the classiﬁcation model under a constraint
and improves the validation performance of the model. This is a key improvement compared to
the method proposed by Ren et al. that uses the local approximation to optimize the weights. In
addition, the algorithm we proposed is a combination of our learning mechanism and the method
proposed by Hu et al., which can promote each other and make the model perform better. Finally,
the experimental evaluation shows that our algorithm can achieve signiﬁcant improvement compared
with the SOTA method in data weighting and other imbalance methods. In our future work, we plan
to extend our algorithm and explore the performance of our algorithm in data augmentation."
CONCLUSION,0.4749034749034749,"Acknowledgements.
This research was partially sponsored by the following funds: National
Key R&D Program of China (2018YFB1402800), Key Research Project of Zhejiang Province
(2022C01145), Fundamental Research Funds for the Provincial Universities of Zhejiang (RF-
A2020007) and Zhejiang Lab (2020AA3AB05)."
CONCLUSION,0.47876447876447875,Published as a conference paper at ICLR 2022
REFERENCES,0.4826254826254826,REFERENCES
REFERENCES,0.4864864864864865,"Yu Bai, Minshuo Chen, Pan Zhou, Tuo Zhao, Jason Lee, Sham Kakade, Huan Wang, and Caiming
Xiong. How important is the train-validation split in meta-learning? In International Conference
on Machine Learning, pp. 543–553. PMLR, 2021."
REFERENCES,0.49034749034749037,"CM Bishop. Pattern recognition and machine learning, m. jordan, j. kleinberg, and b. sch¨olkopf,
eds, 2006."
REFERENCES,0.4942084942084942,"Pakshal Bohra and Michael Unser. Continuous-domain signal reconstruction using l {p}-norm reg-
ularization. IEEE Transactions on Signal Processing, 68:4543–4554, 2020."
REFERENCES,0.4980694980694981,"Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based
on effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp. 9268–9277, 2019."
REFERENCES,0.5019305019305019,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.5057915057915058,"Qi Dong, Shaogang Gong, and Xiatian Zhu. Class rectiﬁcation hard mining for imbalanced deep
learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1851–
1860, 2017."
REFERENCES,0.5096525096525096,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.5135135135135135,"Mila Grancharova, Hanna Berg, and Hercules Dalianis. Improving named entity recognition and
classiﬁcation in class imbalanced swedish electronic patient records through resampling.
In
Eighth Swedish Language Technology Conference (SLTC). F¨orlag G¨oteborgs Universitet, 2020."
REFERENCES,0.5173745173745173,"Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural
networks: A survey. arXiv preprint arXiv:2004.05439, 2020."
REFERENCES,0.5212355212355212,"Zhiting Hu, Bowen Tan, Ruslan Salakhutdinov, Tom Mitchell, and Eric P. Xing. Learning Data
Manipulation for Augmentation and Weighting. Curran Associates Inc., Red Hook, NY, USA,
2019."
REFERENCES,0.525096525096525,"Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deep representation for
imbalanced classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 5375–5384, 2016."
REFERENCES,0.528957528957529,"Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International Conference
on Machine Learning, pp. 2304–2313. PMLR, 2018."
REFERENCES,0.5328185328185329,"Gary King and Langche Zeng. Logistic regression in rare events data. Political analysis, 9(2):
137–163, 2001."
REFERENCES,0.5366795366795367,"Chaoliang Li and Shigang Liu. A comparative study of the class imbalance problem in twitter spam
detection. Concurrency and Computation: Practice and Experience, 30(5):e4281, 2018."
REFERENCES,0.5405405405405406,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense
object detection. In Proceedings of the IEEE international conference on computer vision, pp.
2980–2988, 2017."
REFERENCES,0.5444015444015444,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised
pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181–
196, 2018."
REFERENCES,0.5482625482625483,"Tomasz Malisiewicz, Abhinav Gupta, and Alexei A Efros. Ensemble of exemplar-svms for object
detection and beyond. In 2011 International conference on computer vision, pp. 89–96. IEEE,
2011."
REFERENCES,0.5521235521235521,Published as a conference paper at ICLR 2022
REFERENCES,0.555984555984556,"Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111–3119, 2013."
REFERENCES,0.5598455598455598,"Kemal Oksuz, Baris Can Cam, Sinan Kalkan, and Emre Akbas. Imbalance problems in object
detection: A review. IEEE transactions on pattern analysis and machine intelligence, 2020."
REFERENCES,0.5637065637065637,"Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. In International Conference on Machine Learning, pp. 4334–4343. PMLR,
2018."
REFERENCES,0.5675675675675675,"Frank Schneider, Lukas Balles, and Philipp Hennig. Deepobs: A deep learning optimizer benchmark
suite. arXiv preprint arXiv:1903.05499, 2019."
REFERENCES,0.5714285714285714,"Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors
with online hard example mining. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 761–769, 2016."
REFERENCES,0.5752895752895753,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, pp. 1631–1642, 2013."
REFERENCES,0.5791505791505791,"Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014."
REFERENCES,0.583011583011583,"Noorul Wahab, Asifullah Khan, and Yeon Soo Lee. Two-phase deep convolutional neural network
for reducing class skewness in histopathological images based breast cancer detection. Computers
in biology and medicine, 85:86–97, 2017."
REFERENCES,0.5868725868725869,"Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In Proceedings
of the 31st International Conference on Neural Information Processing Systems, pp. 7032–7042,
2017."
REFERENCES,0.5907335907335908,"Jerry Wei, Arief Suriawinata, Bing Ren, Xiaoying Liu, Mikhail Lisovsky, Louis Vaickus, Charles
Brown, Michael Baker, Mustafa Nasir-Moin, Naofumi Tomita, et al. Learn like a pathologist:
curriculum learning by annotator agreement for histopathology image classiﬁcation. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2473–2483,
2021."
REFERENCES,0.5945945945945946,"Gang Wu and Edward Y Chang. Class-boundary alignment for imbalanced dataset learning. In
ICML 2003 workshop on learning from imbalanced data sets II, Washington, DC, pp. 49–56.
Citeseer, 2003."
REFERENCES,0.5984555984555985,"Han Xiao. bert-as-service. https://github.com/hanxiao/bert-as-service, 2018."
REFERENCES,0.6023166023166023,"Lei Zhang, Yuehuan Wang, and Yang Huo. Object detection in high-resolution remote sensing
images based on a hard-example-mining network. IEEE Transactions on Geoscience and Remote
Sensing, 2020."
REFERENCES,0.6061776061776062,"Shen Zhang, Fei Ye, Bingnan Wang, and Thomas Habetler. Few-shot bearing fault diagnosis based
on model-agnostic meta-learning. IEEE Transactions on Industry Applications, 2021."
REFERENCES,0.61003861003861,"A
APPENDIX"
REFERENCES,0.6138996138996139,"A.1
EXPERIMENTAL RESULTS WITH DIFFERENT METRICS"
REFERENCES,0.6177606177606177,"In this section, we evaluate the performance of our methods with different metrics on a large-scale
data set. In addition to the accuracy, we also adopt the Marco-F1 score and G-means that are often
used in imbalanced classiﬁcation. In this experiment, we use an additional large-scale unbalanced
data set that is the operational data for Air Pressure System (APS) Failure at Scania trucks. The
settings and results of this experiment are as follows."
REFERENCES,0.6216216216216216,Published as a conference paper at ICLR 2022
REFERENCES,0.6254826254826255,"Table 7: The network model for APS Failure
dataset"
REFERENCES,0.6293436293436293,Input 171-dimensional examples
REFERENCES,0.6332046332046332,"(171, 100) linear layer, ReLU
(100, 100) linear layer, Tanh
(100, 2) linear layer"
REFERENCES,0.637065637065637,Table 8: Description of APS Failure dataset
REFERENCES,0.640926640926641,"Classes
Train
Set
Validation
Set
Test
Set"
REFERENCES,0.6447876447876448,"Negative
10000
1000
5
Positive
10
1000
5"
REFERENCES,0.6486486486486487,Table 9: Results with different metrics on APS Failure dataset
REFERENCES,0.6525096525096525,"Methods
Accuracy
Marco-F1 score
G-means"
REFERENCES,0.6563706563706564,"Baseline
50.00 ± 0.00
33.33 ± 0.00
0.00 ± 0.00
Proportion
93.29 ± 2.59
93.27 ± 2.63
93.17 ± 2.78
Two-phase
93.19 ± 0.87
93.19 ± 0.87
93.15 ± 0.89
Hu et al.’s
94.09 ± 0.94
93.35 ± 1.73
93.28 ± 1.79
Hu et al.’s+R
93.98 ± 1.78
94.65 ± 0.76
94.65 ± 0.76
Ours.
94.49 ± 0.70
95.10 ± 0.66
94.86 ± 0.58"
REFERENCES,0.6602316602316602,"Dataset and Models. APS Failure dataset is from UCI Machine Learning Repository (Dua & Graff,
2017). This dataset contains 76000 examples. We randomly construct the train, validation and test
data sets from the original data set. These sub-data sets are described in Table 8. The train set
consists of 10000 negative examples and 10 positive examples, which is extremely imbalanced, and
its imbalance rate reaches 1:1000. The validation and test sets are balanced data sets, and their
example sizes for each class are 5 and 1000 respectively. In addition, APS Failure dataset dataset
has 171 features, and we replace the missing feature values with the average of the examples in the
same class. Finally, we scale the all feature values to [0, 1]. The model for APS Failure dataset is
shown in Table 7 and is a simple 3-layer FCN."
REFERENCES,0.6640926640926641,"Other Settings.
During training, we use Adam optimization with an initial learning rate of 1e-2
and set the batch size to 128. Other hyperparameter settings are the same as the text classiﬁcation in
Section 4.1."
REFERENCES,0.667953667953668,"Results.
The results of different metrics on APS Failure dataset are shown in Table 9. There are
three main observations. First, our method achieves the best score in all metrics. It indicates that our
method has comprehensive advantages compared with other methods. Second, the score rankings
of the six methods are almost consistent among these metrics. It shows that the evaluation of these
metrics on a balanced test set is similar. Third, our method performs well on a large-scale data set.
It demonstrates that our method is also effective on large-scale data."
REFERENCES,0.6718146718146718,"A.2
CONVERGENCE PROOF OF OUR METHOD"
REFERENCES,0.6756756756756757,"This section ﬁrstly provides a proof of the convergence of the learning method with a constraint in
Section 3.1, and then we prove the convergence of the combination method in Section 3.2."
REFERENCES,0.6795366795366795,Deﬁnition 1. A function f(x) : Rd →R is said to be Lipschitz-smooth with constant L if
REFERENCES,0.6833976833976834,"∥∇f(x) −∇f(y) ∥≤L ∥x −y ∥, ∀x, y ∈Rd"
REFERENCES,0.6872586872586872,Deﬁnition 2. A function f(x) has σ-bounded gradients if
REFERENCES,0.6911196911196911,"∥∇f(x) ∥≤σ, ∀x ∈Rd"
REFERENCES,0.694980694980695,"Theorem 1. Suppose the validation loss function Lval is Lipschitz-smooth with constant L, and the
training loss function fi corresponding to the example xi has σ-bounded gradients and the Hessian
matrix H, namely, the second derivative of Ltrain with respect to θ, is bounded by ρ. Let the
learning rate η′
wt satisﬁes η′
wt ≤
2
LN2σ2ρ2 . Then, after each iteration of the model parameter θ, the
validation loss always decreases. More speciﬁcally,"
REFERENCES,0.6988416988416989,"Lval(θt+1) ≤Lval(θt)
(10)"
REFERENCES,0.7027027027027027,Published as a conference paper at ICLR 2022
REFERENCES,0.7065637065637066,"Proof. The validation loss function Lval is Lipschitz-smooth, so we have"
REFERENCES,0.7104247104247104,Lval(θt+1) ≤Lval(θt) + (∇θLval)T ∆θ + L
REFERENCES,0.7142857142857143,"2 ∥∆θ ∥
(11)"
REFERENCES,0.7181467181467182,Let v = (∇θLval)T ∆θ + L
REFERENCES,0.722007722007722,"2 ∥∆θ ∥. We can see that only v ≤0, there is Lval(θt+1) ≤Lval(θt).
Then, substituting the θ update formula: ∆θ = ∇wθ (wt+1 −wt) and the w update formula:
wt+1 −wt = η′
wt(∇wθ)T ∇θLval from Section 3.1 into v, we have"
REFERENCES,0.7258687258687259,v = (∇θLval)T ∇wθ (wt+1 −wt) + L
REFERENCES,0.7297297297297297,"2 ∥η′
wt∇wθ (wt+1 −wt) ∥
(12)"
REFERENCES,0.7335907335907336,"= −η′
wt(∇θLval)T ∇wθ(∇wθ)T ∇θLval + Lη′
wt
2"
REFERENCES,0.7374517374517374,"2
∥∇wθ(∇wθ)T ∇θLval ∥
(13)"
REFERENCES,0.7413127413127413,"= (∇θLval)T ∇wθ(∇wθ)T hLη′
wt
2"
REFERENCES,0.7451737451737451,"2
∇wθ(∇wθ)T −η′
wtI
i
∇θLval
(14)"
REFERENCES,0.749034749034749,where I is the identity matrix.
REFERENCES,0.752895752895753,"Let S = ∇wθ(∇wθ)T h Lη′
wt
2"
REFERENCES,0.7567567567567568,"2
∇wθ(∇wθ)T −η′
wtI
i
, so v = (∇θLval)T S ∇θLval. Next, we prove
that S is a semi-negative deﬁnite matrix such that v ≤0."
REFERENCES,0.7606177606177607,"We observe that in S, the term ∇wθ(∇wθ)T is a symmetric and positive semi-deﬁnite matrix. We
use A to denote this term and deﬁne its eigendecomposition as A = Pdiag(λ)P −1 where λ is a
vector composed of eigenvalues and λi ≥0 for all i. Substituting this eigendecomposition into S,
we have"
REFERENCES,0.7644787644787645,"S = Pdiag(λ)P −1(Lη′
wt
2"
REFERENCES,0.7683397683397684,"2
Pdiag(λ)P −1 −η′
wtI)
(15)"
REFERENCES,0.7722007722007722,"= Pdiag(Lη′
wt
2"
REFERENCES,0.7760617760617761,"2
λ ∗λ −η′
wtλ)P −1
(16)"
REFERENCES,0.7799227799227799,where ∗represents the hadamard product.
REFERENCES,0.7837837837837838,"Therefore, in order to make S a semi-negative deﬁnite matrix, η′
wt must satisﬁes
Lη′
wt
2"
REFERENCES,0.7876447876447876,"2
λ2
i −η′
wtλi ≤
0 for all i, namely, 0 ≤η′
wt ≤
2
Lλi . Let λmax = maxi(λi), so ﬁnally η′
wt must satisﬁes"
REFERENCES,0.7915057915057915,"0 ≤η′
wt ≤
2
Lλmax
(17)"
REFERENCES,0.7953667953667953,"Further, we estimate the boundary of the scalar value λmax. In Section 2, we deﬁne a matrix
F = (∇f1, ..., ∇fN), where fi is the training loss function and N is the number of the training
examples. Since fi has σ-bounded gradients, we can obtain ∥F ∥≤Nσ. Substituting ∇wθ in Eq.
7 into A, we have"
REFERENCES,0.7992277992277992,"∥A ∥=∥∇wθ(∇wθ)T ∥=∥diag(h)FF T diag(h) ∥
(18)"
REFERENCES,0.803088803088803,"≤∥diag(h) ∥∥F ∥∥F T ∥∥diag(h) ∥= N 2σ2ρ2
(19)"
REFERENCES,0.806949806949807,"Since λmax is the eigenvalue of A, we can obtain"
REFERENCES,0.8108108108108109,"λmax ≤∥A ∥= N 2σ2ρ2
(20)"
REFERENCES,0.8146718146718147,"Therefore, combining Eq. 17 and Eq. 20, the satisfying range of η′
wt is"
REFERENCES,0.8185328185328186,"0 ≤η′
wt ≤
2
LN 2σ2ρ2
(21)"
REFERENCES,0.8223938223938224,This ﬁnishes our proof for Theorem 1.
REFERENCES,0.8262548262548263,Published as a conference paper at ICLR 2022
REFERENCES,0.8301158301158301,"Theorem 2. Suppose the validation loss function Lval, the training loss function fi and the learning
rate η′
wt satisﬁes Theorem 1 conditions. Same as Algorithm 1, t is denoted as the time step where
the algorithm successively uses Hu et al. and our methods to update θ, and let t′ represent the time
step inside time-step t and the algorithm only apply our method to update θ. The range of t′ is
[0, ..., T2 −1]. Then the validation loss always decreases after the t-th iteration, namely,"
REFERENCES,0.833976833976834,"Lval(θt+1) ≤Lval(θt)
(22)"
REFERENCES,0.8378378378378378,"Proof.
The θt′=0 is the updated parameter through the method (Hu et al.). According to the
convergence theorem in the paper (Ren et al., 2018; Hu et al., 2019), we can obtain"
REFERENCES,0.8416988416988417,"Lval(θt) ≥Lval(θt′=0)
(23)"
REFERENCES,0.8455598455598455,"After that, we use our method to update the parameter T times. According to the Theorem 1, we
have"
REFERENCES,0.8494208494208494,"Lval(θt′=0) ≥Lval(θt′=1) ... ≥Lval(θt′=T2−1)
(24)"
REFERENCES,0.8532818532818532,"Here, the algorithm completes the update of the parameter in time-step t, namely, θt′=T2−1 = θt+1.
Combining Eq. 23 and Eq. 24, we can obtain"
REFERENCES,0.8571428571428571,"Lval(θt+1) ≤Lval(θt)
(25)"
REFERENCES,0.861003861003861,This ﬁnishes our proof of Theorem 2.
REFERENCES,0.8648648648648649,"A.3
CONVERGENCE RATE OF OUR METHOD"
REFERENCES,0.8687258687258688,"This section provides a proof of the convergence rate of the learning method with a constraint in
Section 3.1."
REFERENCES,0.8725868725868726,"Theorem 3. Suppose the validation loss function Lval, the training loss function fi and the learn-
ing rate η′
wt satisﬁes Theorem 1 conditions. Then the learning method in Section 3.1 achieves
∥∇θtLval ∥≤ϵ in O( 1"
REFERENCES,0.8764478764478765,"ϵ ) steps, namely,"
REFERENCES,0.8803088803088803,"min
0<t<T ∥∇θtLval ∥≤C"
REFERENCES,0.8841698841698842,"T
(26)"
REFERENCES,0.888030888030888,where C is some constant
REFERENCES,0.8918918918918919,"Proof. According to Eq. 11 and Eq. 14 in Theorem 1, we can obtain"
REFERENCES,0.8957528957528957,"Lval(θt+1) −Lval(θt) ≤(∇θtLval)T St∇θtLval
(27)"
REFERENCES,0.8996138996138996,where St is the matrix S at time-step t and S is deﬁned in Theorem 1.
REFERENCES,0.9034749034749034,"Then we have T
X"
REFERENCES,0.9073359073359073,"t=0
(∇θtLval)T St∇θtLval ≥Lval(θT +1) −Lval(θ0)"
REFERENCES,0.9111969111969112,"≥Lval(θ∗) −Lval(θ0)
(28)"
REFERENCES,0.915057915057915,"where Lval(θ∗) is the minimum of function Lval(θ). Then, we can observe that there exist a time-
step 0 ≤τ ≤T such that,"
REFERENCES,0.918918918918919,"T(∇θτ Lval)T Sτ∇θτ Lval ≥Lval(θ∗) −Lval(θ0)
(29)"
REFERENCES,0.9227799227799228,"We have proved that Sτ is a semi-negative deﬁnite matrix. According to Eq. 16, we have"
REFERENCES,0.9266409266409267,"Sτ = Pdiag(λ′)P −1 = Pdiag(λ′)P T
(30)"
REFERENCES,0.9305019305019305,Published as a conference paper at ICLR 2022
REFERENCES,0.9343629343629344,"where λ′ is a vector composed of eigenvalues and λ′
i ≤0 for all i. P is the matrix composed of
eigenvectors of A in Theorem 1 and P −1 = P T because A is a symmetric matrix. Substituting Eq.
30 into Eq. 29, we have"
REFERENCES,0.9382239382239382,"Lval(θ∗) −Lval(θ0) ≤T(∇θτ Lval)T Pdiag(λ′)P T ∇θτ Lval
(31)"
REFERENCES,0.9420849420849421,"= T(P T ∇θτ Lval)
T diag(λ′)P T ∇θτ Lval
(32)"
REFERENCES,0.9459459459459459,"≤T(P T ∇θτ Lval)
T λ′
maxIP T ∇θτ Lval
(33)"
REFERENCES,0.9498069498069498,"= Tλ′
max ∥∇θτ Lval ∥
(34)"
REFERENCES,0.9536679536679536,"We can regard Eq. 32 as the quadratic form of the diagonal matrix diag(λ′), and we scale all the
eigenvalues λ′
i to λ′
max where λ′
max = maxi λ′
i, so that we obtain the inequality in Eq. 33."
REFERENCES,0.9575289575289575,So we have
REFERENCES,0.9613899613899614,∥∇θτ Lval ∥≤1
REFERENCES,0.9652509652509652,"T
Lval(θ∗) −Lval(θ0)"
REFERENCES,0.9691119691119691,"λ′max
= C"
REFERENCES,0.972972972972973,"T
(35)"
REFERENCES,0.9768339768339769,where C = Lval(θ∗)−Lval(θ0)
REFERENCES,0.9806949806949807,"λ′max
is a constant independent of T."
REFERENCES,0.9845559845559846,"Therefore, we can obtain min
0<t<T ∥∇θtLval ∥≤∥∇θτ Lval ∥≤C"
REFERENCES,0.9884169884169884,T . It means that our method can
REFERENCES,0.9922779922779923,"achieve min
0<t<T ∥∇θtLval ∥≤O( 1"
REFERENCES,0.9961389961389961,T ) in T steps. This ﬁnishes our proof of Theorem 3.
