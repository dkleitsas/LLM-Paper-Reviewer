Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0025974025974025974,"Model-based reinforcement learning methods achieve signiﬁcant sample efﬁciency
in many tasks, but their performance is often limited by the existence of the model
error. To reduce the model error, previous works use a single well-designed network
to ﬁt the entire environment dynamics, which treats the environment dynamics as a
black box. However, these methods lack to consider the environmental decomposed
property that the dynamics may contain multiple sub-dynamics, which can be
modeled separately, allowing us to construct the world model more accurately. In
this paper, we propose the Environment Dynamics Decomposition (ED2), a novel
world model construction framework that models the environment in a decomposing
manner. ED2 contains two key components: sub-dynamics discovery (SD2) and
dynamics decomposition prediction (D2P). SD2 discovers the sub-dynamics in an
environment and then D2P constructs the decomposed world model following the
sub-dynamics. ED2 can be easily combined with existing MBRL algorithms and
empirical results show that ED2 signiﬁcantly reduces the model error and boosts
the performance of the state-of-the-art MBRL algorithms on various continuous
control tasks. 1"
INTRODUCTION,0.005194805194805195,"1
INTRODUCTION"
INTRODUCTION,0.007792207792207792,"Reinforcement Learning (RL) is a general learning framework for solving sequential decision-making
problems and has made signiﬁcant progress in many ﬁelds (Mnih et al., 2015; Silver et al., 2016;
Vinyals et al., 2019; Schrittwieser et al., 2019). In general, RL methods can be divided into two
categories regarding whether a world model is constructed for the policy deriving: model-free RL
(MFRL) and model-based RL (MBRL). MFRL methods train the policy by directly interacting
with the environment, which results in good asymptotic performance but low sample efﬁciency. By
contrast, MBRL methods improve the sample efﬁciency by modeling the environment, but often with
limited asymptotic performance and suffer from the model error (Lai et al., 2020; Kaiser et al., 2020)."
INTRODUCTION,0.01038961038961039,"Existing MBRL algorithms can be divided into four categories according to the paradigm they follow:
the ﬁrst category focuses on generating imaginary data by the world model and training the policy
with these data via MFRL algorithms (Kidambi et al., 2020; Yu et al., 2020); the second category
leverages the differentiability of the world model, and generates differentiable trajectories for policy
optimization (Deisenroth & Rasmussen, 2011; Levine & Koltun, 2013; Zhu et al., 2020); the third
category aims to obtain an accurate value function by generating imaginations for temporal difference
(TD) target calculation (Buckman et al., 2018; Feinberg et al., 2018); the last category of works
focuses on reducing the computational cost of the policy deriving by combining the optimal control
algorithm (e.g. model predictive control) with the learned world models (Chua et al., 2018; Okada &
Taniguchi, 2019; Argenson & Dulac-Arnold, 2020). Regardless of paradigms, the performance of
all existing MBRL algorithms depends on the accuracy of the world model. The more accurate the
world model is, the more reliable data can be generated, and ﬁnally, the better policy performance
can be achieved. Therefore, improving the world model accuracy is critical in MBRL."
INTRODUCTION,0.012987012987012988,"To this end, various techniques have been proposed to improve the model accuracy. For example,
rather than directly predict the next state, some works construct a world model for the state change"
INTRODUCTION,0.015584415584415584,1Our code is open source and available at https://github.com/ED2-source-code/ED2
INTRODUCTION,0.01818181818181818,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02077922077922078,"According 
to role"
INTRODUCTION,0.023376623376623377,"Traced to 
action"
INTRODUCTION,0.025974025974025976,"(a) Cheetah environment
(b) Decomposability
(c) Traceability
(d) Model error"
INTRODUCTION,0.02857142857142857,"According 
to position"
INTRODUCTION,0.03116883116883117,"Traced to 
action ……
……"
INTRODUCTION,0.033766233766233764,"Figure 1: (a) The Cheetah task with six action dimensions. (b) The dynamics can be decomposed into
multiple sub-dynamics in various ways, each sub-dynamics is described with different background
colors. (c) Dynamics can be traced to the impact caused by the action, and for each sub-dynamics,
we show the meanings of the action dimensions it traced to. (d) The model error comparison on the
Cheetah task of Dreamer and D2P-Dreamer methods (D2P-Dreamer-Role/Position correspond to
decompose the dynamics according to role/position and model each sub-dynamics separately)."
INTRODUCTION,0.03636363636363636,"prediction (Luo et al., 2019; Kurutach et al., 2018). Model ensemble is also widely used in model
construction for uncertainty estimation, which provides a more reliable prediction (Janner et al., 2019;
Pan et al., 2020). To reduce the model error in long trajectory generation, optimizing the multi-step
prediction errors is also an effective technique (Hafner et al., 2019). However, these techniques
improve the environment modeling in a black-box way, which ignores the inner decomposed structure
of environment dynamics. For example, Figure 1 (a) shows the Cheetah task from DeepMindControl
(DMC) Suite tasks, where the dynamics can be decomposed in various ways. Figure 1 (b) shows
various decomposition on the dynamics: according to the role of sub-dynamics, we can decompose it
into: {thigh, shin, foot}; alternatively, according to the position of sub-dynamics, we can decompose
it into: {back, front}. Figure 1 (d) shows that no matter whether we decompose the Cheetah task
according to role or position, modeling each decomposed sub-dynamics separately can signiﬁcantly
reduce the model error of the existing MBRL algorithm (e.g. Dreamer (Hafner et al., 2020))."
INTRODUCTION,0.03896103896103896,"Inspired by the above example, we propose environment dynamics decomposition (ED2), a novel
world model construction framework that models the dynamics in a decomposing fashion. ED2
contains two main components: sub-dynamics discovery (SD2) and dynamics decomposition pre-
diction (D2P). SD2 is proposed to decompose the dynamics into multiple sub-dynamics, which can
be ﬂexibly designed and we also provide three alternative approaches: complete decomposition,
human prior, and the clustering-based method. D2P is proposed to construct the world model from
the decomposed dynamics in SD2, which models each sub-dynamics separately in an end-to-end
training manner. ED2 is orthogonal to existing MBRL algorithms and can be used as a backbone to
easily combine with any MBRL algorithm. Experiment shows ED2 improves the model accuracy
and boosts the performance signiﬁcantly when combined with existing MBRL algorithms."
BACKGROUND,0.04155844155844156,"2
BACKGROUND"
REINFORCEMENT LEARNING,0.04415584415584416,"2.1
REINFORCEMENT LEARNING"
REINFORCEMENT LEARNING,0.046753246753246755,"Given an environment, we can deﬁne a ﬁnite-horizon partially observable Markov decision process
(POMDP) as (S, A, R, P, γ, O, Ω, T), where S ∈Rn is the state space, and A ∈Rm is the action
space, R : S × A →R denotes the reward function, P : S × A →S denotes the environment
dynamics, γ is the discount factor. The agent receives an observation o ∈Ω, which contain partial
information about the state s ∈S. O is the observation function, which mapping states to probability
distributions over observations. The decision process length is denoted as T."
REINFORCEMENT LEARNING,0.04935064935064935,"Let η denote the expected return of a policy π over the initial state distribution ρ0. The goal of an RL
agent is to ﬁnd the optimal policy π∗which maximizes the expected return:"
REINFORCEMENT LEARNING,0.05194805194805195,"π∗= arg max
π
η[π] = arg max
π
Eπ[ T
X"
REINFORCEMENT LEARNING,0.05454545454545454,"t=0
γtR(st, at)],"
REINFORCEMENT LEARNING,0.05714285714285714,Under review as a conference paper at ICLR 2022
REINFORCEMENT LEARNING,0.05974025974025974,"where s0 ∼ρ0, ot ∼O(·|st), at ∼π(·|ot), st+1 ∼P(·|st, at). If the environment is fully observable,
i.e., Ω= S and O is an identity function, POMDP is equivalent to the MDP: (S, A, R, P, γ, T)."
REPRESENTATIVE WORLD MODELS IN MBRL,0.06233766233766234,"2.2
REPRESENTATIVE WORLD MODELS IN MBRL"
REPRESENTATIVE WORLD MODELS IN MBRL,0.06493506493506493,"The world model is a key component of MBRL that directly impacts policy training. World models
are usually formulated with latent dynamics (Janner et al., 2019; Hafner et al., 2020), and the general
form of the latent dynamics model can be summarized as follows:"
REPRESENTATIVE WORLD MODELS IN MBRL,0.06753246753246753,"Latent transition kernel:
ht = f(s≤t−1, a≤t−1)
Stochastic state function:
p(st|ht)
Reward function:
p(rt|ht)"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07012987012987013,"The latent transition kernel (shorthand as kernel) predicts the latent state ht with input s≤t−1 and
a≤t−1. Based on latent state ht, the stochastic state function and reward function decode the state st
and reward rt. For the partially observable environment, two additional functions are required:"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07272727272727272,"Observation function:
p(ot|st)
Representation function:
p(st|ht, ot)"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07532467532467532,"In general, world models mainly differ at the implementation of kernel, which can be roughly divided
into two categories: with non-recurrent kernel and with recurrent kernel. The formal deﬁnition
of both kernels are as follows:"
REPRESENTATIVE WORLD MODELS IN MBRL,0.07792207792207792,"ht =

f(st−1, at−1)
With non-recurrent kernel
f(ht−1, st−1, at−1)
With recurrent kernel"
REPRESENTATIVE WORLD MODELS IN MBRL,0.08051948051948052,"Non-recurrent kernel are relatively basic kernel for modeling, which are often implemented as
Fully-Connected Networks. Non-recurrent kernel takes the current state st−1 and action at−1 as
input, outputs the latent state prediction ht. Compare to non-recurrent kernel, recurrent kernel is
implemented as RNN and takes the additional input ht−1, which performs better under POMDP
setting. For both kernels, the st and rt can be generated from the latent prediction ht."
ENVIRONMENT DYNAMICS DECOMPOSITION,0.08311688311688312,"3
ENVIRONMENT DYNAMICS DECOMPOSITION"
MOTIVATION,0.08571428571428572,"3.1
MOTIVATION"
MOTIVATION,0.08831168831168831,"An accurate world model is critical in MBRL policy deriving. To decrease the model error, existing
works propose various techniques as introduced in Section 1. However, these techniques improve the
environment modeling in a black-box manner, which ignores the inner properties of environment
dynamics, resulting in inaccurate world model construction and poor policy performance. To address
this problem, we propose two important environment properties when modeling an environment:"
MOTIVATION,0.09090909090909091,"1) Decomposability: The environment dynamics can be decomposed into multiple sub-
dynamics in various ways and the decomposed sub-dynamics can be combined to reconstruct
the entire dynamics."
MOTIVATION,0.09350649350649351,"2) Traceability: The environment dynamics can be traced to the action’s impact on the en-
vironment, and each sub-dynamics can be traced to the impact caused by a part of the
action."
MOTIVATION,0.09610389610389611,"For example in the Cheetah task, Figure 1 (b) demonstrates the decomposability: we can decompose
the dynamics into {thigh, shin, foot} sub-dynamics or {back, front} sub-dynamics, which depends
on the different decomposition perspectives and the combination of decomposed sub-dynamics can
constitute the entire dynamics. Figure 1 (c) explains the traceability: each sub-dynamics can be traced
to the corresponding subset of action dimensions: for the thigh dynamics, it can be regarded as the
impact caused by the front-thigh and back-thigh action dimensions. The above two properties are
closely related to environment modeling: the decomposability reveals the existence of sub-dynamics,
which allows us to model the dynamics separately, while the traceability investigates the causes of
the dynamics and guides us to decompose the dynamics at its root (i.e. the action)."
MOTIVATION,0.0987012987012987,Under review as a conference paper at ICLR 2022
MOTIVATION,0.1012987012987013,ED2 world model
MOTIVATION,0.1038961038961039,"𝑮𝑮𝟏𝟏
𝟏𝟏
𝟐𝟐
𝟑𝟑 𝒎𝒎 … 𝑮𝑮𝟐𝟐 𝑮𝑮𝒌𝒌 …"
MOTIVATION,0.10649350649350649,SD2 Method
MOTIVATION,0.10909090909090909,"SD2
Method 𝒔𝒔"
MOTIVATION,0.11168831168831168,"Action 
decomposing"
MOTIVATION,0.11428571428571428,(Partition 𝒢𝒢)
MOTIVATION,0.11688311688311688,"D2P 
Framework"
MOTIVATION,0.11948051948051948,"Action 
dimensions
Partition 𝒢𝒢"
MOTIVATION,0.12207792207792208,"𝑴𝑴𝝓𝝓𝟏𝟏
𝟏𝟏"
MOTIVATION,0.12467532467532468,"𝑴𝑴𝝓𝝓𝟐𝟐
𝟐𝟐"
MOTIVATION,0.12727272727272726,"𝑴𝑴𝝓𝝓𝒌𝒌
𝒌𝒌"
MOTIVATION,0.12987012987012986,D2P Framework
MOTIVATION,0.13246753246753246,𝒂𝒂𝑮𝑮𝟏𝟏
MOTIVATION,0.13506493506493505,𝒂𝒂𝑮𝑮𝟐𝟐
MOTIVATION,0.13766233766233765,𝒂𝒂𝑮𝑮𝒌𝒌 … 𝒔𝒔 …
MOTIVATION,0.14025974025974025,Sub-actions models 𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐
MOTIVATION,0.14285714285714285,"𝒉𝒉𝒌𝒌
… 𝒉𝒉 𝒔𝒔′ 𝒓𝒓"
MOTIVATION,0.14545454545454545,predictions
MOTIVATION,0.14805194805194805,"𝒂𝒂
𝒔𝒔′, 𝒓𝒓"
MOTIVATION,0.15064935064935064,"Figure 2: Overview of the world model under ED2 Framework. ED2 contains two components:
SD2 and D2P. SD2 decomposes the dynamics by generating partition G on action dimensions. D2P
decomposes action a into multiple sub-actions according to G and makes decomposing predictions
based on s and each sub-action. The prediction h is the combined output of all sub-dynamics models,
from which the next state s′ and reward r are generated."
MOTIVATION,0.15324675324675324,"To take the above properties into account, we propose the environment dynamics decomposition
(ED2) framework (as shown in Figure 2), which contains two key components: sub-dynamics
discovery (SD2) and dynamics decomposition prediction (D2P). More speciﬁcally, by considering the
traceability, we propose to discover the latent sub-dynamics by analyzing the action (SD2, the blue
part in Figure 2); by considering the decomposability, we propose to construct the world model in a
decomposing manner (D2P, the green part in Figure 2). Our framework can be used as a backbone in
MBRL and the combination can lead to performance improvements over existing MBRL algorithms."
DYNAMICS DECOMPOSITION PREDICTION,0.15584415584415584,"3.2
DYNAMICS DECOMPOSITION PREDICTION"
DYNAMICS DECOMPOSITION PREDICTION,0.15844155844155844,"Given an environment with m-dimensional action space A ⊂Rm, the index of each action dimension
constitutes a set Λ = {1, 2, · · · , m}, any disjoint partition G = {G1, . . . , Gk} over Λ corresponds to
a particular way of decomposing action space. For each action dimension i in Λ, we deﬁne the action
space as Ai, which satisﬁed A = A1 × · · · × Am. The action space decomposition under partition G
is deﬁned as AG = {AG1, · · · , AGk}, where sub-action space AGj = Q"
DYNAMICS DECOMPOSITION PREDICTION,0.16103896103896104,"x∈Gj Ax. Based on above
deﬁnitions, we deﬁne the dynamics decomposition for P under partition G as follows:"
DYNAMICS DECOMPOSITION PREDICTION,0.16363636363636364,"Deﬁnition 1 Given a partition G, the decomposition for P : S × A →S can be deﬁned as:"
DYNAMICS DECOMPOSITION PREDICTION,0.16623376623376623,"P(s, a) = fc"
K,0.16883116883116883,"1
k k
X"
K,0.17142857142857143,"i=1
Pi(s, aGi) !"
K,0.17402597402597403,", ∀s, a ∈S × A,
(1)"
K,0.17662337662337663,"with a set of sub-dynamics functions {P1, ..., Pk} that Pi : S × AGi →H, and a decoding function
fc : H →S. Note H is a latent space and aGi ∈AGi is a sub-action (projection) of action a."
K,0.17922077922077922,"Intuitively, the choice of partition G is signiﬁcant to the rationality of dynamics decomposition, which
should be reasonably derived from the environments. In this section, we mainly focus on dynamics
modeling, and we will introduce how to derive the partition G by using SD2 in section 3.3."
K,0.18181818181818182,"To implement D2P, we use model M i
φi parameterized by φi (i.e., neural network parameters) to
approximate each sub-dynamics Pi. As illustrated in Figure 2, given a partition G, an action a is
divided into multiple sub-actions {aG1, · · · , aGk}, each model M i
φi takes state s and the sub-action
aGi as input and output a latent prediction hi ∈H. The separate latent predictions {h1, · · · , hk} are
aggregated and then decoded for the generation of state s′ and reward r. For each kernel described in
Section 2.2, we provide the formal description here when combine with D2P: ht ="
K,0.18441558441558442,"(
1
k
Pk
i=1 f(st−1, aGi
t−1)
For non-recurrent kernel
1
k
Pk
i=1 f(ht−1, st−1, aGi
t−1)
For recurrent kernel"
K,0.18701298701298702,Under review as a conference paper at ICLR 2022
K,0.18961038961038962,Figure 3: Extension of RSSM with D2P.
K,0.19220779220779222,"We propose a set of kernels, where each kernel mod-
els a speciﬁc sub-dynamics with the input of current
state s, corresponding sub-action aGi and hidden
state ht−1 (ignored when applying on non-recurrent
kernel). The output of all kernels is averaged to get
the ﬁnal output ht. The prediction of reward rt and
state st is generated from the output ht. Speciﬁ-
cally, we provide an example when combining with
the kernel of Recurrent State-Space Model (RSSM)
(Hafner et al., 2019) in Figure 3, which is a repre-
sentative recurrent kernel-based world model. The
original single kernel implemented as GRU are re-
place by multiple kernels with different action input."
SUB-DYNAMICS DISCOVERY,0.19480519480519481,"3.3
SUB-DYNAMICS DISCOVERY"
SUB-DYNAMICS DISCOVERY,0.1974025974025974,"The traceability of the environment introduced in Section 3.1 provides us with a basis for dynamics
decomposition: the decomposition on dynamics can be converted to the decomposition on the action
space. Therefore, we present the SD2 module for the action space decomposition and discuss three im-
plementations in this section. With the Cheetah task in Section 3.1 as the example: the straightforward
SD2 implementation is the complete decomposition, which regards each action dimension as a sub-
dynamics and decomposes the dynamics completely. Speciﬁcally, complete decomposition decom-
poses the dynamics into six sub-dynamics: {Front, Back}×{Thigh, Shin, Foot}. However, com-
plete decomposition ignores the inner action dimensions correlations, which limits its performance in
many tasks. For example, the three action dimensions {Front_Thigh, Front_Shin, Front_Foot}
affect the dynamics of the front part together, thus simply separate these action dimensions would
affect the prediction accuracy. To include the action dimension correlations, incorporating human
prior for the action space decomposition is an improved implementation. Based on different human
prior, we can decompose the dynamics in different ways as introduced in Figure 1. Nevertheless,
although human prior considers the action dimension correlations, it is highly subjective and might
lead to sub-optimal results due to the limited understanding of tasks (we also provide the correspond-
ing experiment in Section 4.2.3). Therefore, human prior is not applicable in complex systems which
is beyond human understanding."
SUB-DYNAMICS DISCOVERY,0.2,"To better discover the sub-dynamics and eliminate the dependence on human prior, we propose to
automatically decompose the action space using the clustering-based method. The clustering-based
method contains two components: feature extraction and clustering criterion. Feature extraction
extracts the properties of action dimension ai into feature vector F i. Then we regard each action
dimension as a cluster and aggregate related action dimensions together with the clustering criterion.
The effectiveness of the clustering-based method depends on the quality of feature extraction and the
validity of clustering criteria, which may be different in different environments. Therefore, although
we provide a general implementation later, we still suggest readers design suitable clustering-based
methods according to task-speciﬁc information."
SUB-DYNAMICS DISCOVERY,0.2025974025974026,"Feature Extraction: We extract the properties of each action dimension by computing the Pearson
correlation coefﬁcient between action dimensions and state dimensions. Speciﬁcally, we deﬁne
the feature vector as F i = ⟨|f i,1|, · · · , |f i,n|⟩, where each f i,j denotes the Pearson correlation
coefﬁcient between action dimension i and state dimension j. F i describes the impact caused by
action dimension i and f i,j is calculated by the corresponding action value ai and state value changes
∆sj (which is the difference between the next state and the current state):"
SUB-DYNAMICS DISCOVERY,0.2051948051948052,"f i,j = cov(ai, ∆sj)"
SUB-DYNAMICS DISCOVERY,0.2077922077922078,"σaiσ∆sj
(2)"
SUB-DYNAMICS DISCOVERY,0.21038961038961038,where cov denotes the covariance and σ denotes the standard deviation.
SUB-DYNAMICS DISCOVERY,0.21298701298701297,"Clustering Criterion: We deﬁne the clustering criterion as the relationship between clusters, which
can be formalized as follow:"
SUB-DYNAMICS DISCOVERY,0.21558441558441557,"Rela(Gi, Gj) =R(Gi, Gj) −R(Gj, G−i) × ωj,−i + R(Gi, G−j) × ωi,−j"
SUB-DYNAMICS DISCOVERY,0.21818181818181817,"ωi,−j + ωj,−i
(3)"
SUB-DYNAMICS DISCOVERY,0.22077922077922077,Under review as a conference paper at ICLR 2022
SUB-DYNAMICS DISCOVERY,0.22337662337662337,"where G−i = Λ \ Gi, ωi,j = |Gi| × |Gj| and R(Gi, Gj) = −
1
ωi,j
P"
SUB-DYNAMICS DISCOVERY,0.22597402597402597,"Ai∈Gi
P"
SUB-DYNAMICS DISCOVERY,0.22857142857142856,"Aj∈Gj ||F i, F j||D.
|| · ||D measures the distance between vectors under distance function D (we choose the negative
cosine similarity as D)."
SUB-DYNAMICS DISCOVERY,0.23116883116883116,"Algorithm 1 Selectable clustering-based method.
Input: Task E, clustering threshold η"
SUB-DYNAMICS DISCOVERY,0.23376623376623376,"Initialize cluster set G = {{1}, · · · , {m}} according to E, a
random policy πrand, dataset Dc →∅
for i = 1, 2, · · · , T do"
SUB-DYNAMICS DISCOVERY,0.23636363636363636,"Collect and store samples in Dc with πrand
Calculate F i for each action dimension i with Dc
while |G| > 1 do"
SUB-DYNAMICS DISCOVERY,0.23896103896103896,"Gmax1, Gmax2 = arg maxGi,Gj∈G Rela(Gi, Gj)
if Rela(Gmax1, Gmax2) > η then"
SUB-DYNAMICS DISCOVERY,0.24155844155844156,"Remove Gmax1 and Gmax2 from G
Add Gmax1 ∪Gmax2 to G
else"
SUB-DYNAMICS DISCOVERY,0.24415584415584415,"Stop clustering
return G"
SUB-DYNAMICS DISCOVERY,0.24675324675324675,"Algorithm 1 presents the overall im-
plementation of the clustering-based
method. As Algorithm 1 describes, with
input task E and clustering threshold η,
we ﬁrst initialize the cluster set G contain-
ing m clusters (each for a single action di-
mension), a random policy πrand, and an
empty dataset Dc. Then for T episodes,
πrand collects samples from the environ-
ment and we calculate F i for each action
dimension i. After that, for each cluster-
ing step, we select the two most relevant
clusters from G and cluster them together.
The process ends when there is only one
cluster, or when the correlation of the two
most correlated clusters is less than the
threshold η. η is a hyperparameter which assigned with a value around 0 and empirically adjusted."
SUB-DYNAMICS DISCOVERY,0.24935064935064935,"3.4
ED2 FOR MBRL ALGORITHMS"
SUB-DYNAMICS DISCOVERY,0.2519480519480519,"Algorithm 2 ED2-Dreamer
Input: Task E, clustering threshold η"
SUB-DYNAMICS DISCOVERY,0.2545454545454545,"// Sub-dynamics Discovery (SD2) Phase:
G ←SD2 methods (E, η)
// Dynamics Decomposition Prediction (D2P) Phase:
for i = 1, 2, · · · , |G| do"
SUB-DYNAMICS DISCOVERY,0.2571428571428571,"Build sub-dynamics model: M i
φi = f(ht−1, st−1, aGi
t−1)
Combining all sub-dynamics models: Mc =
1
|G|
P|G|
i=1 M i
φi
Combining Mc with a decoding network f d
φd and construct
the ED2-combined world model: pφ = f d
φd(Mc)
// Training Phase:
Initialize policy πθ, model: pφ
Optimize policy with Dreamer: πˆθ = Dreamer(E, πθ, pφ)"
SUB-DYNAMICS DISCOVERY,0.2597402597402597,"ED2 is a general framework and can be
combined with any existing MBRL al-
gorithms. Here we provide the practi-
cal combination implementation of ED2
with Dreamer(Hafner et al., 2020) (Al-
gorithm 2) and we also combine ED2
with MBPO (Janner et al., 2019) in the
appendix. The whole process of ED2-
Dreamer contains three phases: 1) SD2
decomposes the environment dynamics
of task E, which and can be implemented
by three decomposing methods intro-
duced in Section 3.3; 2) D2P models each
sub-dynamics separately and constructs
the ED2-combined world model pφ by
combining all sub-models with a decoding network f d
φd; 3) The ﬁnal training phase initializes the
policy πθ and the world model pφ, then derive the policy from Dreamer with input πθ, pφ and task E."
EXPERIMENTS,0.2623376623376623,"4
EXPERIMENTS"
EXPERIMENTS,0.2649350649350649,"Baselines & Benchmarks: For a comparative study, we take two state-of-the-art MBRL methods:
MBPO (Janner et al., 2019) and Dreamer (Hafner et al., 2020) as the baselines, and extend them
with ED2 as ED2-MBPO, ED2-Dreamer. To reduce implementation bias, we reuse the code and
benchmarks from the prior works: the DMC Suite (Tassa et al., 2018) for Dreamer and Gym-
Mujoco (Brockman et al., 2016) for MBPO. Besides, we also provide ablation studies to validate the
effectiveness of each component of our ED2."
EXPERIMENTS,0.2675324675324675,"Clariﬁcation: (1) Although the data required by the clustering-based method is tiny (less than 1%
of the policy training), we include it in the ﬁgures for a fair comparison. (2) We take the clustering-
based method as the main SD2 implementation (denoted as ED2-Methods) and discuss other SD2
methods in Section 4.2.3 and appendix (complete decomposition, human prior are denoted as CD, HP
respectively). (3) Due to the space limit, we leave the result of MBPO/ED2-MBPO in the appendix.
(4) All results are averaged over 5 seeds. The hyperparameters setting is left in the appendix."
EXPERIMENTS,0.2701298701298701,Under review as a conference paper at ICLR 2022
PERFORMANCE,0.2727272727272727,"4.1
PERFORMANCE"
PERFORMANCE,0.2753246753246753,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
PERFORMANCE,0.2779220779220779,dmc_cheetah_run
PERFORMANCE,0.2805194805194805,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.2831168831168831,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
PERFORMANCE,0.2857142857142857,dmc_finger_spin
PERFORMANCE,0.2883116883116883,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.2909090909090909,"0
1
2
3
Step
×106 0 200 400 Value"
PERFORMANCE,0.2935064935064935,dmc_hopper_hop
PERFORMANCE,0.2961038961038961,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.2987012987012987,"0
1
2
3
Step
×106 0 50 Value"
PERFORMANCE,0.3012987012987013,dmc_humanoid_stand
PERFORMANCE,0.3038961038961039,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.3064935064935065,"0
1
2
3
Step
×106 0 50 100 Value"
PERFORMANCE,0.3090909090909091,dmc_humanoid_walk
PERFORMANCE,0.3116883116883117,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.3142857142857143,"0
2
4
6
8
Step
×105 0 200 400 600 800 Value"
PERFORMANCE,0.3168831168831169,dmc_reacher_easy
PERFORMANCE,0.3194805194805195,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.3220779220779221,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
PERFORMANCE,0.3246753246753247,dmc_walker_run
PERFORMANCE,0.32727272727272727,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.32987012987012987,"0
2
4
6
8
Step
×105 0 200 400 600 800 Value"
PERFORMANCE,0.33246753246753247,dmc_hopper_stand
PERFORMANCE,0.33506493506493507,"Dreamer
ED2-Dreamer"
PERFORMANCE,0.33766233766233766,"Figure 4: Comparisons of ED2-Dreamer vs. Dreamer. The x- and y-axis represent the training steps
and performance. The line and shaded area denotes the mean value and standard deviation."
PERFORMANCE,0.34025974025974026,"We evaluate Dreamer and ED2-Dreamer on eight DMC tasks with image inputs. As shown in Figure
4, ED2-Dreamer outperforms Dreamer on all tasks. This is because ED2 establishes a more rational
and accurate world model, which leads to more efﬁcient policy training. Another ﬁnding is that,
in tasks like cheetah_run, and walker_run, ED2-Dreamer achieves lower variance, demonstrating
that ED2 can also lead to a more stable training process. Furthermore, Dreamer fails to achieve
good performance in difﬁcult tasks such as humanoid_stand and humanoid_walk. In contrast, ED2-
Dreamer improves the performance signiﬁcantly, which indicating the superiority of ED2 in complex
tasks. In humanoid tasks, the dynamics are too complex for the clustering-based method with image-
based input. Therefore, we use the vector-based state for clustering and keep the policy training on
the image-based state (we will further discuss this in Section 5). The performance of MBPO and
ED2-MBPO are left in the appendix, which proves that ED2 boost MBPO’s performance signiﬁcantly."
ABLATION STUDIES,0.34285714285714286,"4.2
ABLATION STUDIES"
ABLATION STUDIES,0.34545454545454546,"4.2.1
THE EFFECTIVENESS OF D2P AND SD2"
ABLATION STUDIES,0.34805194805194806,"In this section, we investigate the contribution of each component to performance improvement.
We can summarize the improvements into three parts: multiple kernels, decomposing prediction,
reasonable partition. There is a progressive dependence between these three parts: the decomposing
prediction depends on the existence of the multiple kernels and the reasonable partition depends on
the decomposing prediction. Therefore, we design an incremental experiment for the validation."
ABLATION STUDIES,0.35064935064935066,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
ABLATION STUDIES,0.35324675324675325,dmc_cheetah_run
ABLATION STUDIES,0.35584415584415585,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.35844155844155845,"0
1
2
3
Step
×106 0 50 100 Value"
ABLATION STUDIES,0.36103896103896105,dmc_humanoid_walk
ABLATION STUDIES,0.36363636363636365,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.36623376623376624,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
ABLATION STUDIES,0.36883116883116884,dmc_walker_run
ABLATION STUDIES,0.37142857142857144,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.37402597402597404,"0
1
2
3
Step
×106 0 200 400 Value"
ABLATION STUDIES,0.37662337662337664,dmc_hopper_hop
ABLATION STUDIES,0.37922077922077924,"Dreamer
ED2-Dreamer
ED2-Dreamer-random
ED2-Dreamer-ensemble"
ABLATION STUDIES,0.38181818181818183,Figure 5: Performance comparisons of components ablation experiments.
ABLATION STUDIES,0.38441558441558443,"First, we employ the ED2-Dreamer-ensemble 2, which maintains the multiple kernel structure but
without dynamics decomposing (i.e. all kernels input with action a rather than sub-action aGi). We
investigate the contribution of multiple kernels by comparing ED2-Dreamer-ensemble with baselines.
Second, we employ the ED2-Dreamer-random, which maintains the D2P structure and obtains
partition randomly. We investigate the contribution of decomposing prediction by comparing ED2-
Dreamer-random with ED2-Dreamer-ensemble. Last, we investigate the contribution of reasonable
partition by comparing ED2-Dreamer with ED2-Dreamer-random."
ABLATION STUDIES,0.38701298701298703,"2The ensemble refers to kernel ensemble, which is described in detail in the appendix."
ABLATION STUDIES,0.38961038961038963,Under review as a conference paper at ICLR 2022
ABLATION STUDIES,0.3922077922077922,"Figure 5 shows that ED2-Dreamer-ensemble outperform Dreamer on humanoid_walk and cheetah_run
tasks, indicating that multiple kernels help the policy training on some tasks. ED2-Dreamer-random
outperforms ED2-Dreamer-ensemble on humanoid_walk task, but not in other tasks (even perform
worse in cheetah_run and hopper_hop). This is due to the different modeling difﬁculty of tasks: the
tasks except humanoid_walk are relatively simple and can be modeled without D2P directly (but in a
sub-optimal way). The modeling process of these tasks can be aided by a reasonable partition but
damaged by a random partition. The humanoid_walk is challenging and cannot be modeled directly,
therefore decomposing prediction (D2P) is most critical and performance can be boosted even with a
random decomposing prediction. Finally, ED2-Dreamer outperforms ED2-Dreamer-random on all
tasks, which indicates that a reasonable partition (SD2) is critical in dynamics modeling and D2P can
not contribute signiﬁcantly to the modeling process without a reasonable partition."
MODEL ERROR,0.3948051948051948,"4.2.2
MODEL ERROR"
MODEL ERROR,0.3974025974025974,"0.0
0.5
1.0
Step
×106 3.000 3.005 3.010 3.015 3.020 Value"
MODEL ERROR,0.4,dmc_cheetah_run
MODEL ERROR,0.4025974025974026,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.4051948051948052,"0
1
2
3
Step
×106 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 Value"
MODEL ERROR,0.4077922077922078,dmc_humanoid_walk
MODEL ERROR,0.4103896103896104,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.412987012987013,"0.0
0.5
1.0
Step
×106 3.0 3.5 4.0 4.5 Value"
MODEL ERROR,0.4155844155844156,dmc_walker_run
MODEL ERROR,0.41818181818181815,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.42077922077922075,"0
1
2
3
Step
×106 3.00 3.01 3.02 Value"
MODEL ERROR,0.42337662337662335,dmc_hopper_hop
MODEL ERROR,0.42597402597402595,"Dreamer
ED2-Dreamer"
MODEL ERROR,0.42857142857142855,Figure 6: The model error (KL-Divergence) comparison of ED2-Dreamer and Dreamer.
MODEL ERROR,0.43116883116883115,"In this section, we further investigate whether the model error is reduced when combined with ED2.
We conduct an environment modeling experiment on the same dataset (which is collected in the
MBRL training process) and record the model error. Since the policy keeps update in the MBRL
training process, the dataset of MBRL also changes with the updated policy. For example, in the
MBRL training on the Cheetah task, the model is ﬁrst trained with the data like Rolling on the ground
and ﬁnally trained with the data like running with high speed. To simulate the MBRL training process,
we implement our dataset by slowly expanding it from 0 to all according to the data generation time.
This setting can also help to investigate the generalization ability of the model on unfamiliar data
(i.e. the data generated by the updated policy). We list parts of the result in Figure 6 and the result of
other tasks are shown in the appendix."
MODEL ERROR,0.43376623376623374,"Figure 6 shows that ED2-Dreamer has a signiﬁcantly lower model error in all tasks compared with
Dreamer. ED2-Dreamer can also achieve a more stable world model training (i.e. with low variance)
on humanoid_walk and walker_run tasks. We also ﬁnd that the baseline methods have signiﬁcantly
increasing model error on humanoid_walk and walker_run tasks, but for ED2-methods, the increase
is much smaller. We hypothesize that ED2 produces a reasonable network structure; as the dataset
grows, ED2-methods can generalize to the new data better. This property is signiﬁcant in MBRL
since the stable and accurate model prediction is critical for policy training. We also provide the
model error comparison of MBPO and ED2-MBPO in the appendix, which also proves that ED2 can
reduce the model errors when combine with MBRL methods."
MODEL ERROR,0.43636363636363634,"4.2.3
SD2 COMPARISON"
MODEL ERROR,0.43896103896103894,"In this section, we compare the performance of three proposed SD2 methods. We list the decom-
position obtained by the clustering-based method and human prior on humanoid_walk task for the
illustrating purpose and provide the corresponding performance comparison results in Figure 7. More
experimental results on other tasks are provided in the appendix."
MODEL ERROR,0.44155844155844154,"As shown in Figure 7, human prior can generate different partitions from different task understandings
and we average their performance as the ﬁnal result. Experiment shows that all SD2 methods help
the policy learning. The clustering-based method performs best and baseline Dreamer performs
worst in the comparison. For the complete decomposition, it performs poorly under humanoid_walk,
which implies that humanoid_walk contains many inner action dimension correlations, and simply
complete decomposition heavily breaks this correlation thus hinders the ﬁnal performance. Compared
to complete decomposition, human prior maintains more action dimension correlations by leveraging
the human prior knowledge, which leads to better performance. However, the correlations maintained"
MODEL ERROR,0.44415584415584414,Under review as a conference paper at ICLR 2022
MODEL ERROR,0.44675324675324674,"Figure 7: The performance comparison of Dreamer, ED2-Dreamer, ED2-CD-Dreamer and ED2-HP-
Dreamer. We provide the sub-dynamics visualization in the left four ﬁgures. Each circle correspond
to a joint. A joint contains multiple action dimensions when the corresponding circle is separated
into multiple parts. We mark the action dimensions in the same sub-dynamics with the same color."
MODEL ERROR,0.44935064935064933,"by human prior might be false or incomplete due to human limited understanding of tasks. Compare to
human prior, the clustering-based method automatically decomposes the action space according to the
clustering criterion, which decomposes the action space better in a mathematical way. For example,
human prior aggregates {right_hip_x, right_hip_y, right_hip_z} (x, y, z denote the rotation direc-
tion) together and the clustering-based method aggregates {abdomen_x, right_hip_x, left_hip_x}
together. Although the action dimensions from human prior sub-dynamics affect the same joint
left_hip, they rotate in different directions and play a different role in the dynamics. In contrast, the
sub-dynamics discovered by the clustering-based method aggregate the action dimensions that affect
the x-direction rotation together. It maintains stronger correlations and helps the world model ﬁtting
the movement on x-direction better. Therefore, it performs better than human prior on this task."
DISCUSSION,0.45194805194805193,"5
DISCUSSION"
DISCUSSION,0.45454545454545453,"In this paper, we regard SD2 as a ﬂexible module that can adopt any suitable partition methods
considering the task-speciﬁc information. Currently, we discuss three kinds of SD2 methods, i.e.,
human prior, complete decomposition, and the clustering-based method, and the clustering-based
method is chosen as our main implementation since it outperforms the other two methods on these
testbeds empirically. We also analyze the reasonable decomposition provided by the clustering-based
method, which contributes a lot to the dynamics modeling process. Nevertheless, the clustering-based
method is still faced with extra challenges when solving complex tasks with image-based inputs,
such as humanoid tasks. In this paper, we use the vector-based state in the clustering stage for
humanoid tasks by considering the one-to-one correspondence between vector-based and image-
based state representations. How to leverage self-supervised learning or contrastive learning to learn
low-dimension, high-quality state features from raw images to improve the partition discovery effect
of SD2 and further apply our ED2 to more complex scenarios is worthwhile to further investigate."
DISCUSSION,0.45714285714285713,"Previous work (Doya et al., 2002) also takes the dynamics decomposed prediction into consideration,
which achieves better dynamics modeling. It decomposes the dynamics from the perspective of state
and time. Different from this work, we analyze the cause of dynamics and decompose it from its
root: the action space, which makes the modeling of environmental dynamics more reasonable and
scalable."
CONCLUSION,0.4597402597402597,"6
CONCLUSION"
CONCLUSION,0.4623376623376623,"In this paper, we propose a novel world model construction framework: Environment Dynamics
Decomposition (ED2), which explicitly considers the properties of environment dynamics and
models the dynamics in a decomposing manner. ED2 contains two components: SD2 and D2P. SD2
decomposes the environment dynamics into several sub-dynamics according to the dynamics-action
relation. D2P constructs a decomposing prediction model according to the result of SD2. With
combining ED2, the performance of existing MBRL algorithms is signiﬁcantly boosted. Currently,
this work only considers the decomposition on the dimension level, and for future work, it is
worthwhile investigating how to decompose environment dynamics at the object level, which can
further improve the interpretability and generalizability of ED2."
CONCLUSION,0.4649350649350649,Under review as a conference paper at ICLR 2022
REFERENCES,0.4675324675324675,REFERENCES
REFERENCES,0.4701298701298701,"Arthur Argenson and Gabriel Dulac-Arnold. Model-based ofﬂine planning. CoRR, abs/2008.05556,
2020."
REFERENCES,0.4727272727272727,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016."
REFERENCES,0.4753246753246753,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efﬁcient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural
Information Processing Systems 31, pp. 8234–8244, 2018."
REFERENCES,0.4779220779220779,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems 31, pp. 4759–4770, 2018."
REFERENCES,0.4805194805194805,"Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A model-based and data-efﬁcient
approach to policy search. In Proceedings of the 28th International Conference on Machine
Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 465–472. Omnipress,
2011."
REFERENCES,0.4831168831168831,"Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo Kawato. Multiple model-based
reinforcement learning. Neural Comput., 14(6):1347–1369, 2002."
REFERENCES,0.4857142857142857,"Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine. Model-based value estimation for efﬁcient model-free reinforcement learning. CoRR,
abs/1803.00101, 2018."
REFERENCES,0.4883116883116883,"Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and
James Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th
International Conference on Machine Learning, pp. 2555–2565, 2019."
REFERENCES,0.4909090909090909,"Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:
Learning behaviors by latent imagination. In Proceedings of the 8th International Conference on
Learning Representations, 2020."
REFERENCES,0.4935064935064935,"Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. In Advances in Neural Information Processing Systems 32, pp. 12498–12509,
2019."
REFERENCES,0.4961038961038961,"Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,
Ryan Sepassi, George Tucker, and Henryk Michalewski. Model based reinforcement learning for
atari. In Proceedings of the 8th International Conference on Learning Representations, 2020."
REFERENCES,0.4987012987012987,"Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based ofﬂine reinforcement learning. In Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual, 2020."
REFERENCES,0.5012987012987012,"Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. In Proceedings of the 6th International Conference on Learning
Representations, 2018."
REFERENCES,0.5038961038961038,"Hang Lai, Jian Shen, Weinan Zhang, and Yong Yu. Bidirectional model-based policy optimization.
In Proceedings of the 37th International Conference on Machine Learning, pp. 5618–5627, 2020."
REFERENCES,0.5064935064935064,"Sergey Levine and Vladlen Koltun. Guided policy search. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of
JMLR Workshop and Conference Proceedings, pp. 1–9. JMLR.org, 2013."
REFERENCES,0.509090909090909,"Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorith-
mic framework for model-based deep reinforcement learning with theoretical guarantees. In
Proceedings of the 7th International Conference on Learning Representations, 2019."
REFERENCES,0.5116883116883116,Under review as a conference paper at ICLR 2022
REFERENCES,0.5142857142857142,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nat.,
518(7540):529–533, 2015."
REFERENCES,0.5168831168831168,"Masashi Okada and Tadahiro Taniguchi. Variational inference MPC for bayesian model-based
reinforcement learning. In Proceedings of the 3rd Annual Conference on Robot Learning, pp.
258–272, 2019."
REFERENCES,0.5194805194805194,"Shayegan Omidshaﬁei, Dong-Ki Kim, Jason Pazis, and Jonathan P. How. Crossmodal attentive skill
learner. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent
Systems, pp. 139–146. International Foundation for Autonomous Agents and Multiagent Systems
Richland, SC, USA / ACM, 2018."
REFERENCES,0.522077922077922,"Feiyang Pan, Jia He, Dandan Tu, and Qing He. Trust the model when it is conﬁdent: Masked
model-based actor-critic. In Advances in Neural Information Processing Systems 33, 2020."
REFERENCES,0.5246753246753246,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap,
and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR,
abs/1911.08265, 2019."
REFERENCES,0.5272727272727272,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of go with deep neural networks and tree search. Nat., 529(7587):484–489, 2016."
REFERENCES,0.5298701298701298,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. CoRR, abs/1801.00690, 2018."
REFERENCES,0.5324675324675324,"Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander Sasha Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom L. Paine, Çaglar Gülçehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith,
Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David
Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nat., 575(7782):
350–354, 2019."
REFERENCES,0.535064935064935,"Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. MOPO: model-based ofﬂine policy optimization. In Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.5376623376623376,"Guangxiang Zhu, Minghao Zhang, Honglak Lee, and Chongjie Zhang. Bridging imagination and
reality for model-based deep reinforcement learning. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020."
REFERENCES,0.5402597402597402,Under review as a conference paper at ICLR 2022
REFERENCES,0.5428571428571428,"A
EXPERIMENT SETTING"
REFERENCES,0.5454545454545454,"We keep the experiment setting the same with Dreamer and MBPO. For Dreamer, we do the
experiment on Deep Mind Control environments, with images as input, and each episode length is
set to 1000. For MBPO, the experiment is processed on the Gym-Mujoco environment, takes the
structured data as input, and sets the episode length to 1000. Each experiment is averaged by running
ﬁve seeds. Our speciﬁc computing infrastructure is as shown in Table 1:"
REFERENCES,0.548051948051948,Table 1: The computing infrastructure of our experiment.
REFERENCES,0.5506493506493506,"CPU
GPU
MEMORY"
REFERENCES,0.5532467532467532,"XEON(R) SILVER 4214
RTX2080TI
256G"
REFERENCES,0.5558441558441558,"B
EXPERIMENT HYPERPARAMETERS"
REFERENCES,0.5584415584415584,"For ED2-Dreamer and ED2-MBPO, we followed the ofﬁcial implementation of Dreamer and MBPO
except for the dynamics models. Speciﬁcally, for the dynamics models, we select the hyperparameters
in each environment, as shown in Table 2."
REFERENCES,0.561038961038961,"Table 2: The hidden size and η value for each environment. DeepMind denotes the environment that
belongs to DeepMind Control Suite, and Gym-Mujoco denotes the environment is from Gym-Mujoco."
REFERENCES,0.5636363636363636,"ENVIRONMENT
HIDDEN SIZE
η"
REFERENCES,0.5662337662337662,"HOPPER(DEEPMIND)
200
0
WALKER(DEEPMIND)
200
-0.06
CHEETAH(DEEPMIND)
200
-0.1
HUMANOID(DEEPMIND)
200
0
REACHER(DEEPMIND)
200
0
FINGER(DEEPMIND)
200
0
HALFCHEETAH(GYM-MUJOCO)
150
0
HOPPER(GYM-MUJOCO)
200
-0.3
WALKER(GYM-MUJOCO)
200
-0.2
ANT(GYM-MUJOCO)
150
-0.12"
REFERENCES,0.5688311688311688,"For the clustering process, hyperparameter η describes the tightness of constraints on inter-group
distance and intra-group distance. When η = 0, the clustering process stop condition is that the
distance between the two most relative clusters is equal to the distance between these two clusters
and others (this is a general condition in the most environment). In some environments, although
η = 0 is a good choice, but the value can be further ﬁnetuned to obtain more reasonable clustering
results. The η value we use is as in Table 2."
REFERENCES,0.5714285714285714,"C
ED2-MBPO IMPLEMENTATION"
REFERENCES,0.574025974025974,"The combination of MBPO and ED2 can be described as Algorithm 3. We can also separate it into
three parts: the ﬁrst phase is SD2, which discover the sub-dynamics (partition G) in the environment
by using appropriate SD2 method. Then the D2P phase models each sub-dynamics separately and
construct the ED2-combined world model pφ. Finally, we derive the trained policy πˆθ by using
MBPO method with input task E, initialized policy πθ and world model ensemble P ˆφ."
REFERENCES,0.5766233766233766,Under review as a conference paper at ICLR 2022
REFERENCES,0.5792207792207792,"Algorithm 3 ED2-MBPO
Input: Task E, clustering threshold η"
REFERENCES,0.5818181818181818,"// Sub-dynamics Discovery (SD2) Phase:
G ←SD2 methods (E, η)
// Dynamics Decomposition Prediction (D2P) Phase:
for i = 1, 2, · · · , |G| do"
REFERENCES,0.5844155844155844,"Construct sub-dynamics model: M i
φi = f(st−1, aGi
t−1)
Combining all sub-dynamics models: Mc =
1
|G|
P|G|
i=1 M i
φi
Combining Mc with a decoding network f d
φd and construct the ED2-combined world model: pφ = f d
φd(Mc)
// Training Phase:
Initialize policy πθ, model ensemble: P ˆφ = {p1
φ1, · · · , pe
φe}
Optimize policy with MBPO: πˆθ = MBPO(E, πθ, P ˆφ)"
REFERENCES,0.587012987012987,"D
KERNEL ENSEMBLE AND MODEL ENSEMBLE"
REFERENCES,0.5896103896103896,"Kernel ensemble is deployed in section 4.2. We retain the multi kernel network structure and all
kernel input with the same information: hidden state h (if combined with recurrent kernel), current
state s and current action a. Kernel ensemble can be regarded as a single model, which average the
outputs from all kernels and training in an end-to-end manner for all kernels."
REFERENCES,0.5922077922077922,"Model ensemble is widely used in MBRL for uncertainty estimation. In MBRL, the model ensemble
is generally implemented by setting different initial parameters and sampling different training data
from the same dataset. Models in model ensemble are trained separately and no connection between
them except training from the same dataset."
REFERENCES,0.5948051948051948,"Therefore, model ensemble is totally different from kernel ensemble. Model ensemble propose to
train multiple unrelated models for the same task, which can estimate the uncertainty of prediction.
But kernel ensemble propose to use one model (but construct with multiple kernels) for the prediction
task. We can also combine kernel ensemble with model ensemble and improve the accuracy of all
models (e.g. MBPO)."
REFERENCES,0.5974025974025974,"E
MODEL ERROR"
REFERENCES,0.6,"In order to verify whether we get a more accurate world model, we measure the model error. Figure 8
is the model error curve of Dreamer / ED2-Dreamer. We can see that our framework can signiﬁcantly
reduce the model error."
REFERENCES,0.6025974025974026,"0.0
0.5
1.0
Step
×106 3.000 3.005 3.010 3.015 3.020 Value"
REFERENCES,0.6051948051948052,dmc_cheetah_run
REFERENCES,0.6077922077922078,"Dreamer
ED2-Dreamer"
REFERENCES,0.6103896103896104,"0.0
0.5
1.0
Step
×106 3.00 3.02 3.04 3.06 Value"
REFERENCES,0.612987012987013,dmc_finger_spin
REFERENCES,0.6155844155844156,"Dreamer
ED2-Dreamer"
REFERENCES,0.6181818181818182,"0
1
2
3
Step
×106 3.00 3.01 3.02 Value"
REFERENCES,0.6207792207792208,dmc_hopper_hop
REFERENCES,0.6233766233766234,"Dreamer
ED2-Dreamer"
REFERENCES,0.625974025974026,"0
1
2
3
Step
×106 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 Value"
REFERENCES,0.6285714285714286,dmc_humanoid_stand
REFERENCES,0.6311688311688312,"Dreamer
ED2-Dreamer"
REFERENCES,0.6337662337662338,"0
1
2
3
Step
×106 3.00 3.05 3.10 3.15 3.20 3.25 3.30 3.35 Value"
REFERENCES,0.6363636363636364,dmc_humanoid_walk
REFERENCES,0.638961038961039,"Dreamer
ED2-Dreamer"
REFERENCES,0.6415584415584416,"0
2
4
6
8
Step
×105 3.00 3.01 3.02 3.03 3.04 Value"
REFERENCES,0.6441558441558441,dmc_reacher_easy
REFERENCES,0.6467532467532467,"Dreamer
ED2-Dreamer"
REFERENCES,0.6493506493506493,"0.0
0.5
1.0
Step
×106 3.0 3.5 4.0 4.5 Value"
REFERENCES,0.6519480519480519,dmc_walker_run
REFERENCES,0.6545454545454545,"Dreamer
ED2-Dreamer"
REFERENCES,0.6571428571428571,"0
2
4
6
8
Step
×105 3.000 3.005 3.010 3.015 3.020 Value"
REFERENCES,0.6597402597402597,dmc_hopper_stand
REFERENCES,0.6623376623376623,"Dreamer
ED2-Dreamer"
REFERENCES,0.6649350649350649,Figure 8: The model error reduced when combine ED2 with Dreamer.
REFERENCES,0.6675324675324675,Under review as a conference paper at ICLR 2022
REFERENCES,0.6701298701298701,"F
ED2-CD-DREAMER"
REFERENCES,0.6727272727272727,"Here we provide the experiment result of complete decomposition in Figure 9. complete decompo-
sition can boost the performance in most environments. But in some complex environments like
humanoid and walker, it fails to improve the performance. We analysis that in humanoid and walker,
the correlation between action dimensions can’t be ignored. Complete decomposition break the
correlations between action dimensions and lead to poor performance in these tasks."
REFERENCES,0.6753246753246753,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
REFERENCES,0.6779220779220779,dmc_cheetah_run
REFERENCES,0.6805194805194805,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.6831168831168831,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
REFERENCES,0.6857142857142857,dmc_finger_spin
REFERENCES,0.6883116883116883,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.6909090909090909,"0
1
2
3
Step
×106 0 200 400 Value"
REFERENCES,0.6935064935064935,dmc_hopper_hop
REFERENCES,0.6961038961038961,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.6987012987012987,"0
1
2
3
Step
×106 0 50 Value"
REFERENCES,0.7012987012987013,dmc_humanoid_stand
REFERENCES,0.7038961038961039,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7064935064935065,"0
1
2
3
Step
×106 0 50 100 Value"
REFERENCES,0.7090909090909091,dmc_humanoid_walk
REFERENCES,0.7116883116883117,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7142857142857143,"0
2
4
6
8
Step
×105 0 200 400 600 800 Value"
REFERENCES,0.7168831168831169,dmc_reacher_easy
REFERENCES,0.7194805194805195,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7220779220779221,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
REFERENCES,0.7246753246753247,dmc_walker_run
REFERENCES,0.7272727272727273,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7298701298701299,"0
2
4
6
8
Step
×105 0 200 400 600 800 Value"
REFERENCES,0.7324675324675325,dmc_hopper_stand
REFERENCES,0.7350649350649351,"Dreamer
ED2-CD-Dreamer"
REFERENCES,0.7376623376623377,Figure 9: Comparisons between ED2-CD-Dreamer and Dreamer.
REFERENCES,0.7402597402597403,"G
COMBINE WITH MBPO"
REFERENCES,0.7428571428571429,"Here we provide the experiment results of ED2-MBPO method, which include the performance
(Figure 10), model error evaluation (Figure 11) and performance under complete decomposition
(Figure 12)."
REFERENCES,0.7454545454545455,"0
2
4
6
Step
×104 0 1000 2000 3000 Value"
REFERENCES,0.7480519480519481,Hopper
REFERENCES,0.7506493506493507,"MBPO
ED2-MBPO"
REFERENCES,0.7532467532467533,"0
1
2
3
4
Step
×104 0 2000 4000 6000 Value"
REFERENCES,0.7558441558441559,HalfCheetah
REFERENCES,0.7584415584415585,"MBPO
ED2-MBPO"
REFERENCES,0.7610389610389611,"0.0
0.5
1.0
Step
×105 0 2000 4000 Value Ant"
REFERENCES,0.7636363636363637,"MBPO
ED2-MBPO"
REFERENCES,0.7662337662337663,"0.0
0.4
0.8
1.2
Step
×105 0 1000 2000 3000 4000 Value"
REFERENCES,0.7688311688311689,Walker2d
REFERENCES,0.7714285714285715,"MBPO
ED2-MBPO"
REFERENCES,0.7740259740259741,Figure 10: Performance comparisons between ED2-MBPO and MBPO.
REFERENCES,0.7766233766233767,"0
2
4
6
Step
×104 0.000 0.003 0.006 0.009 Value"
REFERENCES,0.7792207792207793,Hopper
REFERENCES,0.7818181818181819,"MBPO
ED2-MBPO"
REFERENCES,0.7844155844155845,"0
1
2
3
4
Step
×104 0.0 0.2 0.4 0.6 Value"
REFERENCES,0.787012987012987,HalfCheetah
REFERENCES,0.7896103896103897,"MBPO
ED2-MBPO"
REFERENCES,0.7922077922077922,"0.0
0.5
1.0
Step
×105 0.0 0.3 0.6 0.9 1.2 Value Ant"
REFERENCES,0.7948051948051948,"MBPO
ED2-MBPO"
REFERENCES,0.7974025974025974,"0.0
0.4
0.8
1.2
Step
×105 0.0 0.5 1.0 1.5 Value"
REFERENCES,0.8,Walker2d
REFERENCES,0.8025974025974026,"MBPO
ED2-MBPO"
REFERENCES,0.8051948051948052,Figure 11: Model error comparisons between ED2-MBPO and MBPO.
REFERENCES,0.8077922077922078,"H
SD2 CLUSTERING RESULTS"
REFERENCES,0.8103896103896104,"We visualize the ﬁnal partition result obtained by clustering here with both ﬁgure and table form, the
ﬁgure result of DeepMind Control Suite is shown in Figure 13, and the ﬁgure result of Gym-Mujoco
is shown in Figure 14. The result shows that the clustering-based method tends to group the relative
action dimensions together, and the clustering results are also reasonable from the human point of
view."
REFERENCES,0.812987012987013,Under review as a conference paper at ICLR 2022
REFERENCES,0.8155844155844156,"0
2
4
6
Step
×104 0 1000 2000 3000 Value"
REFERENCES,0.8181818181818182,Hopper
REFERENCES,0.8207792207792208,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8233766233766234,"0
1
2
3
4
Step
×104 0 2000 4000 6000 Value"
REFERENCES,0.825974025974026,HalfCheetah
REFERENCES,0.8285714285714286,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8311688311688312,"0.0
0.5
1.0
Step
×105 0 2000 4000 Value Ant"
REFERENCES,0.8337662337662337,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8363636363636363,"0.0
0.4
0.8
1.2
Step
×105 0 1000 2000 3000 4000 Value"
REFERENCES,0.8389610389610389,Walker2d
REFERENCES,0.8415584415584415,"MBPO
ED2-CD-MBPO"
REFERENCES,0.8441558441558441,Figure 12: Performance comparisons between ED2-CD-MBPO and MBPO.
REFERENCES,0.8467532467532467,"Humanoid
Walker"
REFERENCES,0.8493506493506493,Reacher
REFERENCES,0.8519480519480519,"Finger
Hopper
Cheetah"
REFERENCES,0.8545454545454545,"Figure 13: The visualization of ﬁnal partition of environments in DeepMind Control Suite. Some
joints in humanoid is divided into two or three parts (e.g. abdomen joint). It indicates that there
are multiple action dimensions contained by this joint (e.g. abdomen joint contains abdomen_x,
abdomen_y and abdomen_z action dimensions)."
REFERENCES,0.8571428571428571,"As shown in Table 3, each row denotes a sub-dynamics discovered by the clustering-based method
in this environment (expect the ﬁnal two-row in humanoid environment, they belong to the same
sub-dynamics. Because of the length of the table, we write it as two lines). The sub-dynamics we
discovered is very reasonable, and there is an obvious connection between the action dimensions in
the same sub-dynamics."
REFERENCES,0.8597402597402597,"I
ATARI EXPERIMENTS"
REFERENCES,0.8623376623376623,"We also conducted model error experiments on Atari environment and Atari-like Maze environment
(called Minecraft) (Omidshaﬁei et al., 2018). In this experiment, random policy is used to generate
data for dynamics model training. As shown in Figure 15, ED2 could bring a more accurate dynamics
modeling process."
REFERENCES,0.8649350649350649,"J
DREAMER WITH BIGGER HIDDEN SIZE"
REFERENCES,0.8675324675324675,"In this section, we provide the result of Dreamer method under bigger hidden size (which keeps the
similar parameter size as ED2-Dreamer). As shown in Figure 16, increasing the size of parameters
can not improve the performance."
REFERENCES,0.8701298701298701,Under review as a conference paper at ICLR 2022
REFERENCES,0.8727272727272727,"Walker2d
Hopper
HalfCheetah
Ant"
REFERENCES,0.8753246753246753,Figure 14: The visualization of ﬁnal partition of environments in Gym-Mujoco.
REFERENCES,0.8779220779220779,"0
2
4
6
Step
×103 0.00 0.02 0.04 Value"
REFERENCES,0.8805194805194805,Minecraft
REFERENCES,0.8831168831168831,"Baseline
ED2"
REFERENCES,0.8857142857142857,"0
2
4
6
8
Step
×103"
REFERENCES,0.8883116883116883,0.0005
REFERENCES,0.8909090909090909,0.0010
REFERENCES,0.8935064935064935,0.0015 Value
REFERENCES,0.8961038961038961,Enduro-v0
REFERENCES,0.8987012987012987,"Baseline
ED2"
REFERENCES,0.9012987012987013,Figure 15: Model error experiments on Atari and Atari like maze environment.
REFERENCES,0.9038961038961039,"0
1
2
3
Step
×106 0 200 400 Value"
REFERENCES,0.9064935064935065,dmc_hopper_hop
REFERENCES,0.9090909090909091,"Dreamer
ED2-Dreamer
Dreamer_large_hidden"
REFERENCES,0.9116883116883117,"0.0
0.5
1.0
Step
×106 0 200 400 600 800 Value"
REFERENCES,0.9142857142857143,dmc_walker_run
REFERENCES,0.9168831168831169,"Dreamer
ED2-Dreamer
Dreamer_large_hidden"
REFERENCES,0.9194805194805195,Figure 16: The performance of Dreamer under bigger parameter size.
REFERENCES,0.922077922077922,Under review as a conference paper at ICLR 2022
REFERENCES,0.9246753246753247,"Table 3: The meaning of action dimensions in each environment (listed according the clustering
result)"
REFERENCES,0.9272727272727272,"ENVIRONMENT
ACTION DIMENSION MEANNING"
REFERENCES,0.9298701298701298,HUAMNOID(DEEPMIND)
REFERENCES,0.9324675324675324,RIGHT_ANKLE_X
REFERENCES,0.935064935064935,"LEFT_ANKLE_X
ABDOMEN_X, RIGHT_HIP_X, LEFT_HIP_X"
REFERENCES,0.9376623376623376,"ABDOMEN_Y, RIGHT_HIP_Y, LEFT_HIP_Y"
REFERENCES,0.9402597402597402,"RIGHT_KNEE,RIGHT_ANKLE_Y"
REFERENCES,0.9428571428571428,"LEFT_KNEE,LEFT_ANKLE_Y"
REFERENCES,0.9454545454545454,"RIGHT_HIP_Z, LEFT_HIP_Z
LEFT_SHOULDER1, LEFT_ELBOW, RIGHT_SHOULDER1, RIGHT_ELBOW,
RIGHT_SHOULDER2, LEFT_SHOULDER2, ABDOMEN_Z"
REFERENCES,0.948051948051948,WALKER(DEEPMIND)
REFERENCES,0.9506493506493506,"LEFT_HIP, RIGHT_HIP
LEFT_KNEE, RIGHT_KNEE
LEFT_ANKLE, RIGHT_ANKLE"
REFERENCES,0.9532467532467532,"CHEETAH(DEEPMIND)
BACK_THIGH, BACK_SHIN, BACK_FOOT
FRONT_THIGH, FRONT_SHIN, FRONT_FOOT"
REFERENCES,0.9558441558441558,HOPPER(DEEPMIND)
REFERENCES,0.9584415584415584,"WAIST, HIP"
REFERENCES,0.961038961038961,"KNEE
ANKLE"
REFERENCES,0.9636363636363636,"REACHER(DEEPMIND)
SHOULDER WRIST"
REFERENCES,0.9662337662337662,"FINGER(DEEPMIND)
PROXIMAL"
REFERENCES,0.9688311688311688,DISTAL
REFERENCES,0.9714285714285714,HALFCHEETAH(GYM-MUJOCO)
REFERENCES,0.974025974025974,"BACK_THIGH, BACK_SHIN
FRONT_THIGH, FRONT_FSHIN"
REFERENCES,0.9766233766233766,"BACK_FOOT
FRONT_FOOT"
REFERENCES,0.9792207792207792,WALKER2D(GYM-MUJOCO)
REFERENCES,0.9818181818181818,"RIGHT_THIGH, RIGHT_LEG, LEFT_THIGH, LEFT_LEG"
REFERENCES,0.9844155844155844,RIGHT_FOOT
REFERENCES,0.987012987012987,LEFT_FOOT
REFERENCES,0.9896103896103896,"HOPPER(GYM-MUJOCO)
THIGH, LEG FOOT"
REFERENCES,0.9922077922077922,ANT(GYM-MUJOCO)
REFERENCES,0.9948051948051948,"LEFT_FRONT_HIP, RIGHT_FRONT_HIP, LEFT_BACK_HIP, RIGHT_BACK_HIP"
REFERENCES,0.9974025974025974,"LEFT_FRONT_ANKLE, LEFT_BACK_ANKLE
RIGHT_FRONT_ANKLE, RIGHT_BACK_ANKLE"
