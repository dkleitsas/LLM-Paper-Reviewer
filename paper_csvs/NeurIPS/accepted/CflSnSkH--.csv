Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0038314176245210726,"We study a repeated information design problem faced by an informed sender who
tries to inﬂuence the behavior of a self-interested receiver. We consider settings
where the receiver faces a sequential decision making (SDM) problem. At each
round, the sender observes the realizations of random events in the SDM problem.
This begets the challenge of how to incrementally disclose such information to
the receiver to persuade them to follow (desirable) action recommendations. We
study the case in which the sender does not know random events probabilities, and,
thus, they have to gradually learn them while persuading the receiver. We start by
providing a non-trivial polytopal approximation of the set of sender’s persuasive
information structures. This is crucial to design efﬁcient learning algorithms. Next,
we prove a negative result: no learning algorithm can be persuasive. Thus, we
relax persuasiveness requirements by focusing on algorithms that guarantee that
the receiver’s regret in following recommendations grows sub-linearly. In the
full-feedback setting—where the sender observes all random events realizations—,
we provide an algorithm with ˜O( p"
ABSTRACT,0.007662835249042145,"T) regret for both the sender and the receiver.
Instead, in the bandit-feedback setting—where the sender only observes the realiza-
tions of random events actually occurring in the SDM problem—, we design an al-
gorithm that, given an ↵2 [1/2, 1] as input, ensures ˜O(T ↵) and ˜O(T max{↵,1−↵"
ABSTRACT,0.011494252873563218,"2 })
regrets, for the sender and the receiver respectively. This result is complemented
by a lower bound showing that such a regrets trade-off is essentially tight."
INTRODUCTION,0.01532567049808429,"1
Introduction"
INTRODUCTION,0.019157088122605363,"Bayesian persuasion [Kamenica and Gentzkow, 2011] (a.k.a. information design) is the problem
faced by an informed sender who wants to inﬂuence the behavior of a self-interested receiver via the
provision of payoff-relevant information. This captures the problem of “who gets to know what”,
which is fundamental in all economic interactions. Thus, Bayesian persuasion is ubiquitous in real-
world problems, such as, e.g., online advertising [Bro Miltersen and Sheffet, 2012], voting [Alonso
and Câmara, 2016, Castiglioni et al., 2020a, Castiglioni and Gatti, 2021], trafﬁc routing [Bhaskar
et al., 2016, Castiglioni et al., 2021a], security [Rabinovich et al., 2015, Xu et al., 2016], and
marketing [Babichenko and Barman, 2017, Candogan, 2019]."
INTRODUCTION,0.022988505747126436,"We study Bayesian persuasion in settings where the receiver plays in a sequential decision making
(SDM) problem. An SDM problem is characterized by a tree structure made by: decision nodes, where
the receiver takes actions, and chance nodes, in which partially observable random events occur. The
sender perfectly observes the realizations of random events, and their goal is to incrementally disclose"
INTRODUCTION,0.02681992337164751,"⇤Email:{martino.bernasconideluca,matteo.castiglioni,alberto.marchesi,nicola.gatti,francesco1.trovo}@polimi.it"
INTRODUCTION,0.03065134099616858,"the acquired information to induce the receiver towards desirable outcomes for the sender. This is not
an easy feat when the sender and the receiver have different utilities. In order to do so, the sender
commits to a signaling scheme specifying a probability distribution over action recommendations for
the receiver at each decision node. Speciﬁcally, the sender commits to a persuasive signaling scheme,
meaning that the receiver is incentivized to follow recommendations. We consider the case of a
farsighted receiver, meaning that they take into account all the possible future events when deciding
whether to deviate or not from recommendations at each decision node."
INTRODUCTION,0.034482758620689655,"With some notable exceptions (such as, e.g., [Zu et al., 2021]), Bayesian persuasion models in the
literature make the stringent assumption that both the sender and the receiver know the prior, which,
in our setting, is deﬁned by the probabilities associated with random events in the SDM problem. We
relax such an assumption by considering an online learning framework in which the sender, without
any knowledge of the prior, repeatedly interacts with the receiver to gradually learn the prior while
still being persuasive."
INTRODUCTION,0.038314176245210725,"A concrete example of a setting that ﬁts our model is that of a navigation app sending route recommen-
dations to a driver. The app (sender) gets to know information about trafﬁc congestion sequentially,
since it might change over time, especially for long routes. Moreover, at any point in time, the app
can send recommendations to the driver (receiver), who in turn has to take decisions on which routes
to choose sequentially. The app and the driver might have different utilities. For example, the app
may want to maximize overall trafﬁc congestion, while the driver has to minimize their travel time.
Moreover, the receiver interacts with the app multiple times and, thus, the application must provide
good recommendations to the receiver, otherwise they would switch to another navigation app."
INTRODUCTION,0.0421455938697318,"Original contributions.
Our goal is to design online learning algorithms that are no-regret for
the sender, while being persuasive for the receiver. We start by providing a non-trivial polytopal
approximation of the set of sender’s persuasive signaling schemes. This will be crucial in designing
efﬁcient (i.e., polynomial-time) learning algorithms, and it also shows how a sender-optimal signaling
scheme can be found in polynomial time in the ofﬂine version of our problem, which may be of
independent interest. Next, we prove a negative result: without knowing the prior, no algorithm can
be persuasive at each round with high probability. Thus, we relax persuasiveness requirements by
focusing on learning algorithms that guarantee that the receiver’s regret in following recommendations
grows sub-linearly, while guaranteeing the same for sender’s regret. First, we study the full-feedback
case, where the sender observes the realizations of all the random events that may potentially happen
in the SDM problem. In such a setting, we provide an algorithm with ˜O( p"
INTRODUCTION,0.04597701149425287,"T) regret for both the
sender and the receiver. Then, we focus on the bandit-feedback setting, where the sender only
observes the realizations of random events on the path in the tree traversed during the SDM problem.
In this case, we design an algorithm that achieves ˜O(T ↵) sender’s regret and ˜O(T max{↵,1−↵"
INTRODUCTION,0.04980842911877394,"2 })
receiver’s regret, for any ↵2 [1/2, 1] given as input. The crucial component of the algorithm is a
non-trivial exploration phase that uniformly explores the tree deﬁning the SDM problem to build
suitable estimators of the prior. This is needed since, with bandit feedback, playing a signaling
scheme may provide insufﬁcient information about its persuasiveness. Finally, we provide a lower
bound showing that the regrets trade off achieved by our algorithm is tight for ↵2 [1/2, 2/3]."
INTRODUCTION,0.05363984674329502,"Related works.
Some works addressed Bayesian persuasion in Markov decision processes (MDPs).
Gan et al. [2022] and Wu et al. [2022] show how to efﬁciently ﬁnd a sender-optimal policy when the
receiver is myopic (i.e., it only optimizes one-step rewards) in MDPs with inﬁnite and ﬁnite horizon,
respectively. Moreover, the former assume that the environment is known, while the latter do not.
These works considerably differ from ours, since we assume a farsighted receiver and also model
partial observability of random events.2 Another work close to ours is [Zu et al., 2021], which studies
a (non-sequential) persuasion problem in which the sender and the receiver do not know the prior and
interact online. Zu et al. [2021] provide a persuasive learning algorithm, while, in our model, we
show that the ignorance of the prior precludes the possibility of committing to persuasive signaling
schemes, and, thus, we need to resort to new techniques to circumvent the issue. Another line of
research, that uses similar techniques as the one employed in this work, studies learning in SDM
problems while satisfying unknown constraints [Bernasconi-de-Luca et al., 2021, Bernasconi et al.,
2022]. Finally, Celli et al. [2020a] study Bayesian persuasion with multiple receivers interacting in an"
INTRODUCTION,0.05747126436781609,"2Gan et al. [2022] also study a model with farsighted receiver, where they show that the problem of ﬁnding a
sender-optimal policy is NP-hard. Thus, they do not provide any algorithmic result for such a model."
INTRODUCTION,0.06130268199233716,"imperfect-information sequential game. Differently from ours, their model adopts a different notion
of persuasiveness, known as ex ante persuasiveness, and it assumes that the prior is known. Other
works study learning problems in which the sender does not know the receivers’ payoffs (but knows
the prior); see, e.g., [Castiglioni et al., 2020b, 2021b, 2022]."
PRELIMINARIES,0.06513409961685823,"2
Preliminaries"
SEQUENTIAL DECISION MAKING PROBLEMS,0.06896551724137931,"2.1
Sequential decision making problems"
SEQUENTIAL DECISION MAKING PROBLEMS,0.07279693486590039,"An instance of an SDM problem is deﬁned by a tree structure, utilities, and random events probabilities.
The tree structure has a set of nodes H := Z [ Hd [ Hc, where: Z contains all the terminal nodes in
which the problem ends (corresponding to the leaves of the tree), Hd is the set of decision nodes in
which the agent acts, while Hc is the set of chance nodes where random events occur. Given any
non-terminal node h 2 H \ Z, we let A(h) be the set of arcs outgoing from h. If h 2 Hd, then
A(h) is the set of receiver’s actions available at h, while, if h 2 Hc, then A(h) encodes the possible
outcomes of the random event occurring at h. Furthermore, the utility function u : Z ! [0, 1] deﬁnes
the agent’s payoff u(z) when the problem ends in terminal node z 2 Z. Finally, each chance node
h 2 Hc is characterized by a probability distribution µh 2 ∆A(h) over the possible outcomes of the
corresponding random event, with µh(a) denoting the probability of action a 2 A(h).3"
SEQUENTIAL DECISION MAKING PROBLEMS,0.07662835249042145,"In an SDM problem, the agent has imperfect information, since they do not perfectly observe the
outcomes of random events. Thus, the set of decision nodes Hd is partitioned into information sets
(infosets for short), where an infoset I ✓Hd is a subset of decision nodes that are indistinguishable
for the agent. We denote the set of infosets as I. For every infoset I 2 I and pair of nodes h, h0 2 I,
it must be the case that A(h) = A(h0) =: A(I), otherwise the agent could distinguish between
the two nodes. We assume that the agent has perfect recall, which means that they never forget
information once acquired. Formally, this is equivalent to assume that, for every infoset I 2 I, all the
paths from the root of the tree to a node h 2 I identify the same ordered sequence of agent’s actions."
BAYESIAN PERSUASION IN SEQUENTIAL DECISION MAKING PROBLEMS,0.08045977011494253,"2.2
Bayesian persuasion in sequential decision making problems"
BAYESIAN PERSUASION IN SEQUENTIAL DECISION MAKING PROBLEMS,0.0842911877394636,"We study Bayesian persuasion in SDM (BPSDM) problems. These extend the Bayesian persuasion
framework [Kamenica and Gentzkow, 2011] to SDM problems by introducing an exogenous agent
that acts as a sender by issuing signals to the decision-making agent (the receiver).4 By following
the Bayesian persuasion terminology, the probability distributions µh for each chance node h are
collectively referred to as the prior. Thus, the sender observes the realizations of random events
occurring in the SDM problem and can partially disclose information to inﬂuence the receiver’s
behavior. Moreover, the sender has their own utility function deﬁned over terminal nodes, denoted as
f : Z ! [0, 1], and their goal is to commit to a publicly known signaling scheme that maximizes
their utility in expectation with respect to the prior, the signaling scheme, and the receiver’s strategy."
BAYESIAN PERSUASION IN SEQUENTIAL DECISION MAKING PROBLEMS,0.08812260536398467,"Formally, a signaling scheme for the sender deﬁnes a probability distribution φh 2 ∆S(h) at each
decision node h 2 Hd, where S(h) is a ﬁnite set of signals available at h. During the SDM problem,
when the receiver reaches a node h 2 Hd belonging to an infoset I 2 I, the sender draws a signal
s ⇠φh and communicates it to the receiver. Then, based on the history of signals observed from the
beginning of the SDM problem (s included), the receiver computes a posterior belief over the nodes
belonging to the infoset I and plays so as to maximize their expected utility in the SDM sub-problem
that starts from I, taking into account the just acquired information."
BAYESIAN PERSUASION IN SEQUENTIAL DECISION MAKING PROBLEMS,0.09195402298850575,"As customary in these settings, a simple revelation-principle-style argument allows us to focus
on signaling schemes that are direct and persuasive [Kamenica and Gentzkow, 2011, Arieli and
Babichenko, 2019]. In particular, a signaling scheme is direct if signals correspond to action
recommendations, namely S(h) = A(h) for all h 2 Hd. A direct signaling scheme is persuasive if
the receiver is incentivized to follow action recommendations issued by the sender. Moreover, we
assume that, if the receiver does not follow action recommendations at some decision node, then
the sender stops issuing recommendations at nodes later reached during the SDM problem. This"
BAYESIAN PERSUASION IN SEQUENTIAL DECISION MAKING PROBLEMS,0.09578544061302682,"3For a ﬁnite set X we denote with ∆X the set of probability distributions over X.
4Appendix A shows that BPSDM reduces to classical Bayesian persuasion when there is no sequentiality."
BAYESIAN PERSUASION IN SEQUENTIAL DECISION MAKING PROBLEMS,0.09961685823754789,"is without loss of generality. We refer to [Von Stengel and Forges, 2008, Morrill et al., 2021] for a
discussion on a similar problem in the ﬁeld of correlation in sequential games."
THE SEQUENCE-FORM REPRESENTATION,0.10344827586206896,"2.3
The sequence-form representation"
THE SEQUENCE-FORM REPRESENTATION,0.10727969348659004,"The sequence form is a commonly-used, compact way of representing (mixed) strategies in SDM
problems [Koller et al., 1996]. In this work, the sequence-form representation will be employed for
receiver’s strategies, and to encode the signaling schemes and priors, as we describe in the following."
THE SEQUENCE-FORM REPRESENTATION,0.1111111111111111,"Receiver’s strategies.
Given any h 2 H, we let σr(h) be the ordered sequence of receiver’s actions
on the path from the root of the tree to node h. By the perfect recall assumption, given any infoset
I 2 I, it holds that σr(h) = σr(h0) =: σr(I) for every pair of nodes h, h0 2 I. Thus, we can identify
sequences with infoset-action pairs, with σ = (I, a) encoding the sequence of actions obtained by
appending action a 2 A(I) at the end of σr(I), for any infoset I 2 I. Moreover, ? denotes the
empty sequence. Hence, the receiver’s sequences are ⌃r := {(I, a) | I 2 I, a 2 A(I)} [ {?}. In the
sequence-form representation, mixed strategies are deﬁned by specifying the probability of playing
each sequence of actions. Thus, a receiver’s strategy is represented by a vector x 2 [0, 1]|⌃r|, where
x[σ] encodes the realization probability of sequence σ 2 ⌃r. Furthermore, a sequence-form strategy
is well-deﬁned if and only if it satisﬁes the following linear constraints:"
THE SEQUENCE-FORM REPRESENTATION,0.11494252873563218,"x[?] = 1
and
x[σr(I)] = P"
THE SEQUENCE-FORM REPRESENTATION,0.11877394636015326,"a2A(I) x[σr(I)a]
8I 2 I."
THE SEQUENCE-FORM REPRESENTATION,0.12260536398467432,"We denote by Xr the polytope of all receiver’s sequence-form strategies. We will also need to work
with the sets of receiver’s strategies in the SDM sub-problem that starts from an infoset I 2 I,
formally deﬁned as Xr,I := {x 2 Xr | x[σr(I)] = 1} ."
THE SEQUENCE-FORM REPRESENTATION,0.12643678160919541,"Signaling schemes.
We represent signaling schemes in sequence form by leveraging the fact that the
sender can be thought of as a perfect-information agent who plays at the decision nodes of the SDM
problem, since their actions correspond to recommendations for the receiver. Thus, since sender’s
infosets correspond to decision nodes, their sequences ⌃s := {(h, a) | h 2 Hd, a 2 A(h)} [ {?}.
Then, we denote the polytope of (sequence-form) signaling schemes as Φ ✓[0, 1]|⌃s|, where each
signaling scheme is represented as a vector φ 2 [0, 1]|⌃s| satisfying:"
THE SEQUENCE-FORM REPRESENTATION,0.13026819923371646,"φ[?] = 1
and
φ[σs(h)] = P"
THE SEQUENCE-FORM REPRESENTATION,0.13409961685823754,"a2A(h) φ[σs(h)a]
8h 2 Hd,"
THE SEQUENCE-FORM REPRESENTATION,0.13793103448275862,"where, similarly to σr(h) for the receiver, σs(h) denotes the sender’s sequence identiﬁed by h 2 H.
We also deﬁne ⇧:= Φ \ {0, 1}|⌃s| as the set of deterministic signaling schemes, which are those
that recommend a single action with probability one at each decision node."
THE SEQUENCE-FORM REPRESENTATION,0.1417624521072797,"Priors.
We also encode prior probability distributions µh by means of the sequence form. Indeed,
these can be though of as elements of a ﬁxed strategy played by a (ﬁctitious) perfect-information
agent that acts at chance nodes. Thus, for such a chance agent, we deﬁne ⌃c, Xc, and σc(h) as
their counterparts previously introduced for the receiver. Moreover, in the following, we denote by
µ? 2 Xc the (sequence-form) prior, recursively deﬁned as follows:"
THE SEQUENCE-FORM REPRESENTATION,0.14559386973180077,"µ?[?] := 1
and
µ?[σc(h)a] := µ?[σc(h)] µh(a)
8h 2 Hc, 8a 2 A(h)."
THE SEQUENCE-FORM REPRESENTATION,0.14942528735632185,"Ordering of sequences.
For the sake of presentation, we introduce a partial ordering relation
among sequences. Given two sequences σ = (I, a) 2 ⌃r and σ0 = (J, b) 2 ⌃r, we write σ ⪯σ0
(read as σ precedes σ0), whenever there exists a path in the tree connecting a node in I to a node in J,
and such a path includes action a. We adopt analogous deﬁnitions for sequences in ⌃s and ⌃c.5"
LEARNING TO PERSUADE,0.1532567049808429,"3
Learning to persuade"
LEARNING TO PERSUADE,0.15708812260536398,"In this work, we relax the strong assumption that both the sender and the receiver know the prior µ?
by casting the BPSDM problem into an online learning framework in which the sender repeatedly
interacts with the receiver over a time horizon of length T. At each round t 2 [T], the interaction"
LEARNING TO PERSUADE,0.16091954022988506,5We refer the reader to Appendix B for an example of SDM problem and its sets of sequences.
LEARNING TO PERSUADE,0.16475095785440613,"goes as follows:6 (i) the sender commits to a signaling scheme φt 2 Φ; (ii) a vector yt 2 {0, 1}|⌃c|
encoding realizations of random events is drawn according to µ?; (iii) the sender and the receiver play
an instance of the (one-shot) BPSDM problem (detailed in Section 2.2), in which the sender commits
to φt, random events at chance nodes are realized as deﬁned by yt, and the receiver sticks to the
recommendations issued by the sender; and (iv) the sender observes a feedback on the realization of
random events at chance nodes, which can be of two types: full feedback when the sender observes yt,
which speciﬁes the realizations of all the random events at chance nodes that are possibly reachable
during the SDM problem; bandit feedback when the sender observes the terminal node zt 2 Z
reached at the end of the SDM problem. The latter is equivalent to observing the realizations of
random events at the chance nodes that are actually reached during the SDM problem, namely σc(zt)."
LEARNING TO PERSUADE,0.1685823754789272,"By letting Φ⇧(µ?) be the set of persuasive signaling schemes, i.e., such that the receiver is incentivized
to following recommendations (a formal deﬁnition is provided in Deﬁnition 2), the goal of the sender
is to select a sequence of signaling schemes, namely φ1, . . . , φT , which maximizes their expected
utility, while guaranteeing that each signaling scheme φt is persuasive, namely φt 2 Φ⇧(µ?)."
LEARNING TO PERSUADE,0.1724137931034483,"We measure the performance of a sequence φ1, . . . , φT of signaling schemes by comparing it with
an optimal (ﬁxed) persuasive signaling scheme. Formally, given a signaling scheme φ 2 Φ, we ﬁrst
deﬁne U(φ, µ?), respectively F(φ, µ?), as the expected utility achieved by the receiver, respectively
the sender, whenever the former follows action recommendations. These can be expressed as linear
functions of φ, which, for any µ 2 Xc, are deﬁned as follows:"
LEARNING TO PERSUADE,0.17624521072796934,"U(φ, µ) := X z2Z"
LEARNING TO PERSUADE,0.18007662835249041,"µ[σc(z)]φ[σs(z)]u(z),
F(φ, µ) := X z2Z"
LEARNING TO PERSUADE,0.1839080459770115,µ[σc(z)]φ[σs(z)]f(z).
LEARNING TO PERSUADE,0.18773946360153257,"Finally, by letting φ? 2 argmaxφ2Φ⇧(µ?) F(φ, µ?) be an optimal (ﬁxed) persuasive signaling
scheme, the sender’ performance over T rounds is measured by the (cumulative) sender’s regret:"
LEARNING TO PERSUADE,0.19157088122605365,RT := P
LEARNING TO PERSUADE,0.19540229885057472,t2[T ] ⇣
LEARNING TO PERSUADE,0.19923371647509577,"F(φ?, µ?) −F(φt, µ?) ⌘ ."
LEARNING TO PERSUADE,0.20306513409961685,"The goal is to design learning algorithms (for the sender) which select sequences of persuasive
signaling schemes such that RT grows asymptotically sub-linearly in T, namely RT = o(T)."
ON THE CHARACTERIZATION OF PERSUASIVE SIGNALING SCHEMES,0.20689655172413793,"4
On the characterization of persuasive signaling schemes"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.210727969348659,"4.1
A local decomposition of persuasiveness"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.21455938697318008,"In this section, we formally introduce the set of persuasive signaling schemes Φ⇧(µ?) as the set
of signaling schemes for which the receiver’s expected utility by following recommendations is
greater than the one provided by an optimal deviation policy (DP).7 In addition, we show how to
decompose any DP into components deﬁned locally at each infoset, which will be crucial in the
following Section 4.2. Intuitively, a DP for the receiver is speciﬁed by two elements: (i) a set of
deviation points in which the DP prescribes to stop following action recommendations; and (ii) the
continuation strategies to be adopted after deviating from recommendations."
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.21839080459770116,"We represent deviation points by vectors ! 2 {0, 1}|⌃r|, which are deﬁned so that ![σ] = 1 if and
only if the DP prescribes to deviate upon observing the sequence of action recommendations σ 2 ⌃r.
Moreover, by leveraging the w.l.o.g. assumption that the sender stops issuing recommendations after
the receiver deviated from them, we focus on DPs such that each path from the root of the tree to a
terminal node involves only one deviation point. As a result, the set of all valid vectors ! 2 {0, 1}|⌃r|"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.2222222222222222,is formally deﬁned as ⌦:= n
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.2260536398467433,"! 2 {0, 1}|⌃r| && P"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.22988505747126436,"σ2⌃r:σ⪯σr(z) ![σ] 1
8z 2 Z o ."
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.23371647509578544,"We represent the continuation strategies of DPs by introducing the set of continuation strategy proﬁles,
denoted as P :=⇥σ=(I,a)2⌃r Xr,I. A continuation strategy proﬁle ⇢2 P, with ⇢= (⇢σ)σ2⌃r,
deﬁnes a strategy ⇢σ 2 Xr,I for every receiver’s sequence σ = (I, a) 2 ⌃r. Intuitively, ⇢σ is the
strategy for the SDM sub-problem starting from infoset I that is used by the receiver after deviating
upon observing sequence σ 2 ⌃r. As a result, any pair (!, ⇢) 2 ⌦⇥P speciﬁes a valid DP; formally:"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.23754789272030652,"6Throughout this work, for n 2 N, we denote with [n] the set {1, . . . , n}.
7For ease of exposition, all the deﬁnitions and results in this section are provided for the prior µ?. It is
straightforward to generalize them to the case of a generic µ 2 Xc."
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.2413793103448276,"Deﬁnition 1 (Deviation policy). Given a vector ! 2 ⌦and a proﬁle ⇢2 P, the (!, ⇢)-DP prescribes
to follow sender’s recommendations until action a is recommended at infoset I for some sequence
σ = (I, a) such that ![σ] = 1; from that point on, it prescribes to play according to strategy ⇢σ."
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.24521072796934865,"We denote by U !!⇢(φ, µ?) the receiver’s expected utility obtained with a (!, ⇢)-DP, so that we can
state the following formal deﬁnition of persuasive signaling schemes.
Deﬁnition 2 (Persuasiveness). A signaling scheme φ 2 Φ is ✏-persuasive, namely φ 2 Φ⇧"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.24904214559386972,"✏(µ?), if"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.25287356321839083,"max
(!,⇢)2⌦⇥P U !!⇢(φ, µ?) −U(φ, µ?) ✏.
(1)"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.2567049808429119,"Moreover, a signaling scheme φ 2 Φ is persuasive, namely φ 2 Φ⇧(µ?), if it is 0-persuasive."
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.26053639846743293,"Intuitively, the above deﬁnition states that a signaling scheme is ✏-persuasive if the receiver’s expected
utility by following recommendations is at most ✏less than the one obtained by an optimal DP, which
is a DP maximizing receiver’s expected utility."
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.26436781609195403,"Our local decomposition of DPs is based on suitably-deﬁned, simple deviation policies, which
we call single-point DPs (SPDPs). These are a special case of DPs that stop following sender’s
action recommendations only when a speciﬁc single infoset is reached and a particular action is
recommended therein. SPDPs are formally deﬁned as follows:
Deﬁnition 3 (Single-point deviation strategy). Given a receiver’s sequence σ = (I, a) 2 ⌃r and a
receiver’s strategy ⇢σ 2 Xr,I for the SDM sub-problem starting from infoset I, the (σ, ⇢σ)-SPDP
prescribes to follow sender’s recommendations until action a is recommended at infoset I; from that
point on, the strategy prescribes to play according to ⇢σ."
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.2681992337164751,"We denote by Uσ!⇢σ(φ, µ?) the receiver’s expected utility obtained by following an (σ, ⇢σ)-SPDP.
The following theorem provides the key result underlying our decomposition.8 It shows that the dif-
ference between the utility achieved by a (!, ⇢)-DP and that obtained by following recommendations
can be decomposed into the sum over all the sequences σ 2 ⌃r of analogous differences deﬁned for
the (σ, ⇢σ)-SPDPs, where each difference is weighted by ![σ].
Theorem 1. Given a signaling scheme φ 2 Φ and a (!, ⇢)-DP, it holds:"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.2720306513409962,"U !!⇢(φ, µ?) −U(φ, µ?) = P"
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.27586206896551724,σ2⌃r ![σ] ⇣
A LOCAL DECOMPOSITION OF PERSUASIVENESS,0.2796934865900383,"Uσ!⇢σ(φ, µ?) −U(φ, µ?) ⌘ ."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.2835249042145594,"4.2
A polytopal approximation of the set of persuasive signaling schemes"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.28735632183908044,"In the following, we show how to exploit Theorem 1 to provide an approximate characterization
of the set Φ⇧"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.29118773946360155,"✏(µ?) using a polynomially-sized polytope. First, we state a corollary of Theorem 1
showing that persuasiveness can be bounded by suitably deﬁned SPDPs. Formally:9"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.2950191570881226,"Corollary 1. Given a signaling scheme φ 2 Φ, the following holds:"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.2988505747126437,"max
(!,⇢)2⌦⇥P U !!⇢(φ, µ?) −U(φ, µ?)  X"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.30268199233716475,"σ=(I,a)2⌃r "
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3065134099616858,"max
⇢σ2Xr,I Uσ!⇢σ(φ, µ?) −U(φ, µ?) )+ ."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3103448275862069,"By exploiting Corollary 1, we introduce the following deﬁnition of ✏-persuasive polytope (Lemma 1
justiﬁes the term polytope), as the set of signaling schemes for which there is no (σ, ⇢σ)-SPDP that
achieves a receiver’s utility that exceeds by more than ✏/|⌃r| that of following recommendations.
Deﬁnition 4 (Persuasive polytope). The ✏-persuasive polytope is deﬁned as:"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.31417624521072796,⇤✏(µ?) := n φ 2 Φ
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.31800766283524906,"&&&
max
⇢σ2Xr,I Uσ!⇢σ(φ, µ?) −U(φ, µ?) ✏/|⌃r|
8σ 2 ⌃r o ."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3218390804597701,"Moreover, we denote by ⇤(µ?) the 0-persuasive polytope."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.32567049808429116,"As we show in the following lemma, ⇤✏(µ?) is an efﬁciently-representable polytope.
Lemma 1. The set ⇤✏(µ?) can be described by means of a polynomial number of linear constraints."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.32950191570881227,"8All the proofs are provided in the Appendices D, E, F, and G.
9Given any x 2 R, we let [x]+ := max(x, 0)."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3333333333333333,The following lemma shows that the ✏-persuasive polytope is contained in Φ⇧
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3371647509578544,"✏(µ?), and that the set of
persuasive signaling schemes is contained in the former. Formally:"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.34099616858237547,Lemma 2. It is always the case that Φ⇧(µ?) ⌘⇤(µ?) ✓⇤✏(µ?) ✓Φ⇧
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3448275862068966,✏(µ?).
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3486590038314176,"Lemma 2 also implies that the polytope ⇤(µ?) exactly characterizes the set of persuasive signaling
schemes Φ⇧(µ?). Thus, by adding the maximization of the sender’s expected utility F(φ, µ?) on top
of the linear constraints describing ⇤(µ?), we obtain a polynomially-sized linear program for ﬁnding
an optimal sender’s signaling scheme in any instance of the BPSDM problem in which µ? is known."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3524904214559387,Theorem 2. The BPSDM problem can be solved in polynomial time when the prior µ? is known.
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3563218390804598,"5
Always being persuasive is impossible: a relaxation is needed"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.36015325670498083,"In this section, we prove that it is impossible to design an algorithm that returns a sequence of
persuasive signaling schemes for a generic BPSDM problem."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.36398467432950193,"Theorem 3 (Impossibility of persuasiveness). There exists a constant γ 2 (0, 1) such that no
algorithm can guarantee to output a sequence φ1, . . . , φT of signaling schemes such that, with
probability al least γ, all the signaling schemes φt are persuasive."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.367816091954023,"Notice that this result is in contrast with what happens in non-sequential Bayesian persuasion (see the
work by Zu et al. [2021]), where it is possible to design no-regret algorithms that output sequences of
signaling schemes that are guaranteed to be persuasive with high probability. Theorem 3 motivates
the introduction of a less restrictive requirement on the signaling schemes. In particular, we look for
algorithms that output signaling schemes φ1, . . . , φT , such that the expected utility loss incurred by
the receiver by following recommendations rather than playing an optimal DP is small. To capture
such a requirement, we introduce the following deﬁnition of (cumulative) receiver’s regret:"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3716475095785441,VT := P
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.37547892720306514,"t2[T ]
max
(!,⇢)2⌦⇥P U !!⇢(φt, µ?) −P"
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3793103448275862,"t2[T ] U(φt, µ?)."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.3831417624521073,"Therefore our goal becomes that of designing algorithms guaranteeing that the cumulative receiver’s
regret grows sub-linearly in T, namely VT = o(T), while continuing to ensure that RT = o(T)."
A POLYTOPAL APPROXIMATION OF THE SET OF PERSUASIVE SIGNALING SCHEMES,0.38697318007662834,"In Sections 6 and 7, we design algorithms achieving sub-linear VT and RT for the learning problem
described in Section 3. The algorithms implement two functions: (i) SELECTSTRATEGY(), which, at
each t 2 [T], draws a signaling scheme φt 2 Φ on the basis of the internal state of the algorithm;
and (ii) UPDATE(ot), which modiﬁes the internal state on the basis of the observation ot received as
feedback. Each algorithm alternates these two functions as the interaction between the sender and the
receiver unfolds as described in Section 3. Speciﬁcally, under full feedback the sender observes yt
and calls UPDATE(yt), while in the bandit feedback it observes zt and calls UPDATE(zt)."
LEARNING WITH FULL FEEDBACK,0.39080459770114945,"6
Learning with full feedback"
LEARNING WITH FULL FEEDBACK,0.3946360153256705,Algorithm 1 Full-feedback algorithm
LEARNING WITH FULL FEEDBACK,0.39846743295019155,function SELECTSTRATEGY():
LEARNING WITH FULL FEEDBACK,0.40229885057471265,"φt  arg
max
φ2⇤βt (bµt) F(φ, bµt)"
LEARNING WITH FULL FEEDBACK,0.4061302681992337,return φt
LEARNING WITH FULL FEEDBACK,0.4099616858237548,function UPDATE(yt):
LEARNING WITH FULL FEEDBACK,0.41379310344827586,bµt+1[σ] tP ⌧=1
LEARNING WITH FULL FEEDBACK,0.41762452107279696,y⌧[σ]/⌧8σ 2 ⌃c ✏t+1 q
LEARNING WITH FULL FEEDBACK,0.421455938697318,log(2T |⌃c|/δ)
T,0.42528735632183906,"2t
βt+1  2|Z||⌃r|✏t+1"
T,0.42911877394636017,"In this section, we will discuss the online problem faced
by the sender that wants to optimize online its utility
F(φ, µ?) while learning the unknown prior µ?. We start
by providing a learning algorithm (Algorithm 1) working
with full feedback, i.e., when the sender observes the
realizations of all the possible random events. The main
idea of the algorithm is to choose signaling schemes φt
that belong to suitable sets ⇤βt(bµt) which are designed
to be “close” to the set Φ⇧(µ?) of persuasive signaling
schemes. At each round t 2 [T], Algorithm 1 deﬁnes
the desired set as follows. First, it maintains an estimate"
T,0.4329501915708812,"bµt of µ?; formally, it deﬁnes a radius ✏t such that the
event E := {kbµt −µ?k1 ✏t 8t 2 [T]} holds with
probability at least 1 −δ. Second, it deﬁnes a parameter
βt such that, conditionally to the realization of the event E, the following two conditions hold: (i)
the decision space ⇤βt(bµt) contains the optimal signaling scheme φ?; (ii) ⇤2βt(µ?) contains the"
T,0.4367816091954023,"signaling scheme φt. Intuitively, the ﬁrst condition is needed to have low sender’s regret, while the
second one yields signaling schemes that are approximately persuasive.10"
T,0.44061302681992337,"The polytopal approximation that we provide in Section 4.2 plays a crucial role in the complexity of
Algorithm 1. Speciﬁcally, it allows it to select the desired φt in polynomial time by optimizing over
the set ⇤βt(bµt), which can be done efﬁciently. The use of the set ⇤βt(bµt) over Φ⇧"
T,0.4444444444444444,"βt(bµt) is necessary
due to the fact that the latter is not known to admit an efﬁcient representation. Formally:"
T,0.4482758620689655,"Theorem 4. Given any δ 2 (0, 1), with probability at least 1 −δ, Algorithm 1 guarantees:"
T,0.4521072796934866,RT = O ⇣ |Z| p
T,0.4559386973180077,T log (T|⌃c|/δ) ⌘
T,0.45977011494252873,",
VT = O ⇣"
T,0.46360153256704983,|⌃r||Z| p
T,0.4674329501915709,T log (T|⌃c|/δ) ⌘ .
T,0.47126436781609193,"Moreover, the algorithm runs in polynomial time."
LEARNING WITH BANDIT FEEDBACK,0.47509578544061304,"7
Learning with bandit feedback"
LEARNING WITH BANDIT FEEDBACK,0.4789272030651341,Algorithm 2 Bandit-feedback algorithm
LEARNING WITH BANDIT FEEDBACK,0.4827586206896552,function SELECTSTRATEGY():
LEARNING WITH BANDIT FEEDBACK,0.48659003831417624,"if t N then
. First Phase
σ = (h, a)  arg minσ2⌃c Ct[σ]
⌃s 3 σ0  σs(h)
Choose φt 2 Φ : φt[σ0] = 1
else
. Second Phase
φt  arg
max
φ2⇤βN (bµN ) max"
LEARNING WITH BANDIT FEEDBACK,0.4904214559386973,"µ2Ct(δ) F(φ, µ)"
LEARNING WITH BANDIT FEEDBACK,0.4942528735632184,return φt
LEARNING WITH BANDIT FEEDBACK,0.49808429118773945,function UPDATE(zt):
LEARNING WITH BANDIT FEEDBACK,0.5019157088122606,"Build path pt 2 {0, 1}|⌃c| from σc(zt)
Sample ⇡t ⇠φt s.t. pt[σ] = 1 ) σ 2 ⌃#(⇡t)
for σ 2 ⌃#(⇡t) do"
LEARNING WITH BANDIT FEEDBACK,0.5057471264367817,Ct+1[σ]  Ct[σ] + 1
LEARNING WITH BANDIT FEEDBACK,0.5095785440613027,"bµt+1[σ]  
1
Ct+1[σ]"
LEARNING WITH BANDIT FEEDBACK,0.5134099616858238,PCt+1[σ]
LEARNING WITH BANDIT FEEDBACK,0.5172413793103449,"⌧=1
p⌧[σ]"
LEARNING WITH BANDIT FEEDBACK,0.5210727969348659,✏t+1[σ] q
LEARNING WITH BANDIT FEEDBACK,0.524904214559387,log(4T |⌃c|/δ)
LEARNING WITH BANDIT FEEDBACK,0.5287356321839081,"2Ct+1[σ]
Ct+1(δ) n µ"
LEARNING WITH BANDIT FEEDBACK,0.5325670498084292,### |µ[σ] −bµt+1[σ]| ✏t+1[σ] 8σ 2 ⌃c o
LEARNING WITH BANDIT FEEDBACK,0.5363984674329502,βt+1  2|Z||⌃c| q
LEARNING WITH BANDIT FEEDBACK,0.5402298850574713,|⌃c| log(4T |⌃c|/δ)
LEARNING WITH BANDIT FEEDBACK,0.5440613026819924,2(t+1)
LEARNING WITH BANDIT FEEDBACK,0.5478927203065134,"In this section, we build on Algorithm 1 to
deal with bandit feedback, i.e., when at each
round t 2 [T] the sender only observes the ter-
minal node zt reached at the end of the SDM
problem. The main difﬁculties of such a set-
ting can be summarized by the following ob-
servations. First, the feedback zt only reveals
partial information about the prior, and such
information also depends on the selected sig-
naling scheme φt. Second, even if the sender
plays a signaling scheme φ 2 Φ for an ar-
bitrarily large number of rounds, there is no
guarantee that they collect enough information
to tell whether φ 2 Φ⇧"
LEARNING WITH BANDIT FEEDBACK,0.5517241379310345,"✏(µ?) or not for some
✏> 0. Indeed, the persuasiveness of a signal-
ing scheme depends on all receiver’s utilities
in the SDM problem, and some parts of the
tree may not be reached during a sufﬁciently
large number of rounds by committing to φ.
Thus, any algorithm for the bandit-feedback
setting must guarantee a suitable level of ex-
ploration over the entire tree, so as to keep
track of the entity of the violation of persua-
siveness constraints."
LEARNING WITH BANDIT FEEDBACK,0.5555555555555556,"We design a two-phase algorithm, whose pseudo-code is provided in Algorithm 2. The algorithm
takes as input the number N 2 [T] of rounds devoted to the ﬁrst phase guaranteeing the necessary
amount of exploration, as detailed in Section 7.1. During this phase, the SELECTSTRATEGY()
procedure implements an efﬁcient deterministic uniform exploration policy, which builds an unbiased
estimator bµN of µ?. This allows to restrict the space of feasible signaling schemes used in the
subsequent phase to those that are approximately persuasive, i.e., those in the set ⇤βN (bµN). In
Section 7.2, we discuss the second phase of the the algorithm, composed by the rounds t > N, during
which the algorithm focuses on the minimization of sender’s regret by exploiting the optimism in
face of uncertainty principle. Finally, in Section 7.3, we provide a lower bound on the trade-off
between sender’s and receiver’s regrets, matching the upper bounds achieved by Algorithm 2 for a
large portion of the trade-off frontier. This result formally motivates the necessity of the uniform
exploration which is performed in the ﬁrst phase of the algorithm."
LEARNING WITH BANDIT FEEDBACK,0.5593869731800766,"7.1
Minimizing the receiver’s regret"
LEARNING WITH BANDIT FEEDBACK,0.5632183908045977,"At each round t 2 [T], the sender observes a terminal node zt 2 Z that uniquely determines a path
in the tree deﬁning the SDM problem. We encode such a path by means of a vector pt 2 {0, 1}|⌃c|"
LEARNING WITH BANDIT FEEDBACK,0.5670498084291188,10See Lemma 9 and 10 in Appendix F for the formal statements of these properties.
LEARNING WITH BANDIT FEEDBACK,0.5708812260536399,"such that pt[σ] = 1 if and only if the chance sequence σ 2 ⌃c lies on the path from the root of
the tree to zt, namely σ ⪯σc(zt). If the sender commits to a signaling scheme φt 2 Φ, then it
is easy to see that, for every σ = (h, a) 2 ⌃c, the element pt[σ] is distributed as a Bernoulli of
parameter φt[σs(h)]µ?[σ]. The crucial observation behind the design of our estimator is that, if the
sender commits to a deterministic signaling schemes ⇡t 2 ⇧at some round t 2 [T], then for all the
chance sequences σ 2 ⌃c that are compatible with ⇡t, i.e., that can be observed when ⇡t is played,
we have that pt[σ] is distributed as a Bernoulli of parameter µ?[σ]. Formally, a sequence σ 2 ⌃c
is compatible with ⇡t if there exists a chance node h 2 Hc and an outcome a 2 A(h) satisfying
σ = (h, a) and ⇡t[σs(h)] = 1. This observation leads to the following result:
Lemma 3. For every deterministic signaling scheme ⇡2 ⇧, let"
LEARNING WITH BANDIT FEEDBACK,0.5747126436781609,"⌃#(⇡) := {σ = (h, a) 2 ⌃c | a 2 A(h) ^ ⇡[σs(h)] = 1} ."
LEARNING WITH BANDIT FEEDBACK,0.578544061302682,"Then, during each round t N of Algorithm 2, it holds E [pt[σ]] = µ?[σ] for every σ 2 ⌃#(⇡t)."
LEARNING WITH BANDIT FEEDBACK,0.5823754789272031,"Thus, during the ﬁrst phase, Algorithm 2 builds the desired estimator bµN of µ? as follows. At
each round t N, after observing the feedback zt, the algorithm samples a deterministic signaling
scheme ⇡t 2 ⇧according to φt (the one actually selected at t), so that all the sequences σ 2 ⌃c
such that pt[σ] = 1 (or, equivalently, σ ⪯σc(zt)) belong to ⌃#(⇡t).11 Then, for every σ 2 ⌃#(⇡t),
the algorithm updates the estimator component bµt[σ] according to pt[σ]. Since the probability of
visiting a sequence σ 2 ⌃c depends on φt (and, thus, can be arbitrarily small), the ﬁrst N rounds
must be carefully used to ensure that each sequence is explored at least N/|⌃c| times. To explore
a speciﬁc sequence σ 2 ⌃c, we choose a signaling scheme φt such that σ 2 ⌃#(⇡t) for every
deterministic ⇡t ⇠φt. The procedure described above is needed for minimizing the receiver’s regret,
since, in the second phase, the algorithm selects signaling schemes φt from ⇤βN (bµN). In particular,
as shown by the following lemma, Algorithm 2 guarantees that the receiver’s regret is upper bounded
by 2βN at each round t > N, since it deﬁnes ✏t[σ] for each sequence σ 2 ⌃c so that the event"
LEARNING WITH BANDIT FEEDBACK,0.5862068965517241,"˜E := {|µ?[σ] −bµt[σ]| ✏t[σ] 8(t, σ) 2 [T] ⇥⌃c} holds with probability at least 1 −δ/2."
LEARNING WITH BANDIT FEEDBACK,0.5900383141762452,"Lemma 4. Under the event ˜E, Algorithm 2 guarantees that φt 2 ⇤2βN (µ?) at each round t > N."
LEARNING WITH BANDIT FEEDBACK,0.5938697318007663,"7.2
Minimizing the sender’s regret"
LEARNING WITH BANDIT FEEDBACK,0.5977011494252874,"Algorithm 2 also needs to guarantee small sender’s regret. To do so, we would like that φ? is a valid
pick for the algorithm, i.e., it belongs to ⇤βN (bµt). However, differently from the full-feedback setting,
stopping exploration after the ﬁrst N round does not guarantee optimal rates. In order to ﬁx this issue,
in the second phase, the algorithm selects φt optimistically by maximizing the sender’s expected
utility F(φ, µ) over both φ 2 ⇤βN (bµN) and µ 2 Ct(δ), where Ct(δ) is a suitably-deﬁned conﬁdence
set centered around bµt such that {µ? 2 Ct(δ)} ⌘˜E, and, thus, it holds with high probability. This
guarantees that maxµ2Ct(δ) F(φ?, µ) ≥F(φ?, µ?). Formally:"
LEARNING WITH BANDIT FEEDBACK,0.6015325670498084,"Lemma 5. If the event ˜E holds, then, for every round t > N, it holds that φ? 2 ⇤βN (bµt) and
maxµ2Ct(δ) F(φ?, µ) ≥F(φ?, µ?)."
LEARNING WITH BANDIT FEEDBACK,0.6053639846743295,"Thus, F(φt, µ?) ⇡F(φt, bµt) ≥maxµ2Ct(δ) F(φ?, bµ) ≥F(φ?, µ?) holds in the limit, implying
that F(φt, µ?) converges to F(φ?, µ?) after sufﬁciently many rounds. Formally:
Theorem 5. Given any δ 2 (0, 1) and N 2 [T], Algorithm 2 guarantees:"
LEARNING WITH BANDIT FEEDBACK,0.6091954022988506,RT = O  N + s log
LEARNING WITH BANDIT FEEDBACK,0.6130268199233716,✓T|⌃c| δ ◆ |⌃c|T !
LEARNING WITH BANDIT FEEDBACK,0.6168582375478927,"and
VT = O "
LEARNING WITH BANDIT FEEDBACK,0.6206896551724138,N + T|Z| s log
LEARNING WITH BANDIT FEEDBACK,0.6245210727969349,"✓T|⌃c| δ ◆|⌃c| N ! ,"
LEARNING WITH BANDIT FEEDBACK,0.6283524904214559,"with probability at least 1 −δ. Moreover, the algorithm runs in polynomial time."
LEARNING WITH BANDIT FEEDBACK,0.632183908045977,"In contrast to the case with full feedback, the optimization problem solved by Algorithm 2 belongs to
the class of bilinear problems, which are NP-hard in general [Hillar and Lim, 2013]. However, in
Theorem 5 we prove that our speciﬁc problem can be solved in polynomial time. Furthermore, notice
that Theorem 5 takes as input the number N of rounds devoted to the ﬁrst phase. Given an ↵≥1/2,
by choosing any N = bT ↵c we get bounds of RT = ˜O(T ↵) and VT = ˜O(T max{↵,1−↵ 2 })."
LEARNING WITH BANDIT FEEDBACK,0.6360153256704981,"11The sampling of ⇡t 2 ⇧according to φt can be done efﬁciently by a straightforward modiﬁcation of the
recursive procedure in Farina et al. [2021a,b]."
THE LOWER BOUND FRONTIER,0.6398467432950191,"7.3
The lower bound frontier 1
2 2
3
1 1
2 2
3 3
4 1"
THE LOWER BOUND FRONTIER,0.6436781609195402,Order of T in RT
THE LOWER BOUND FRONTIER,0.6475095785440613,Order of T in VT
THE LOWER BOUND FRONTIER,0.6513409961685823,Lower Bound
THE LOWER BOUND FRONTIER,0.6551724137931034,Algorithm 2
THE LOWER BOUND FRONTIER,0.6590038314176245,"Figure 1: Trade-off between RT
and VT in the bandit feedback."
THE LOWER BOUND FRONTIER,0.6628352490421456,"We conclude by showing that the trade offs between VT and RT
achieved by Algorithm 2 are essentially tight. Previously, we
provided an intuition as to why the algorithm needs to uniformly
explore the entire tree of the SDM problem. Here, we provide a
lower bound that corroborates such a statement. In particular, the
following theorem shows that, for any ↵2 [1/2, 1], in order to
guarantee a sender’s regret of the order of O(T ↵), it is necessary
to suffer a receiver’s regret of the order of ⌦(T 1−↵/2).12"
THE LOWER BOUND FRONTIER,0.6666666666666666,"Theorem 6. For any ↵2 [1/2, 1], there exists a constant γ 2
(0, 1) such that no algorithm guarantees both RT = o(T ↵) and"
THE LOWER BOUND FRONTIER,0.6704980842911877,VT = o(T 1−↵/2) with probability greater than γ.
THE LOWER BOUND FRONTIER,0.6743295019157088,"Figure 1 shows on the horizontal axis the order of the T term in RT , while, on the vertical axis,
it shows the order of the T in VT . The shaded area over the blue line shows the achievable trade
offs, while the marked red line shows the performances proved in Theorem 5. Thus, we show
that Algorithm 2 matches the lower bound for ↵2 [1/2, 2/3]. However, when ↵2 [2/3, 1], the
guarantees proved in Theorem 5 diverge from the ones proved in the lower bound. This is due to the
N = bT ↵c component in the receiver’s regret that becomes dominant when ↵≥2/3. We conjecture
that it is possible to reduce this term to p"
THE LOWER BOUND FRONTIER,0.6781609195402298,"N, hence matching the lower bound of Theorem 6. The
reason for such a gap between the lower and upper bounds is that, during the ﬁrst phase, Algorithm 2
utilizes signaling schemes without taking into account their persuasiveness, thus incurring in large
receiver’s regret during the ﬁrst steps. We leave as future work addressing the question of whether it is
possible to design exploration strategies by only using approximately-persuasive signaling schemes."
REFERENCES,0.6819923371647509,References
REFERENCES,0.685823754789272,"Ricardo Alonso and Odilon Câmara. Persuading voters. American Economic Review, 106(11):"
REFERENCES,0.6896551724137931,"3590–3605, 2016."
REFERENCES,0.6934865900383141,"Itai Arieli and Yakov Babichenko. Private bayesian persuasion. Journal of Economic Theory, 182:"
REFERENCES,0.6973180076628352,"185–217, 2019."
REFERENCES,0.7011494252873564,Yakov Babichenko and Siddharth Barman. Algorithmic aspects of private Bayesian persuasion. In
REFERENCES,0.7049808429118773,"ITCS, 2017."
REFERENCES,0.7088122605363985,"Martino Bernasconi, Federico Cacciamani, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and"
REFERENCES,0.7126436781609196,"Francesco Trovò. Safe learning in tree-form sequential decision making: Handling hard and soft
constraints. In ICML 2022, volume 162, pages 1854–1873. PMLR, 2022."
REFERENCES,0.7164750957854407,"Martino Bernasconi-de-Luca, Federico Cacciamani, Simone Fioravanti, Nicola Gatti, Alberto March-"
REFERENCES,0.7203065134099617,"esi, and Francesco Trovò. Exploiting opponents under utility constraints in sequential games. In
NeurIPS 2021, pages 13177–13188, 2021."
REFERENCES,0.7241379310344828,"Umang Bhaskar, Yu Cheng, Young Kun Ko, and Chaitanya Swamy. Hardness results for signaling in"
REFERENCES,0.7279693486590039,"bayesian zero-sum and network routing games. In EC, pages 479–496, 2016."
REFERENCES,0.7318007662835249,"Peter Bro Miltersen and Or Sheffet. Send mixed signals: earn more, work less. In EC, pages 234–247, 2012."
REFERENCES,0.735632183908046,"Ozan Candogan. Persuasion in networks: Public signals and k-cores. In EC, pages 133–134, 2019."
REFERENCES,0.7394636015325671,"Matteo Castiglioni and Nicola Gatti. Persuading voters in district-based elections. In AAAI, pages"
REFERENCES,0.7432950191570882,"5244–5251, 2021."
REFERENCES,0.7471264367816092,"Matteo Castiglioni, Andrea Celli, and Nicola Gatti. Persuading voters: It’s easy to whisper, it’s hard"
REFERENCES,0.7509578544061303,"to speak loud. In AAAI, pages 1870–1877, 2020a."
REFERENCES,0.7547892720306514,"12 For ↵1/2, a simple reduction from a standard multi-armed bandit problem provides a lower bound of
⌦( p"
REFERENCES,0.7586206896551724,T) on both sender’s regret RT and receiver’s regret VT .
REFERENCES,0.7624521072796935,"Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online Bayesian persuasion."
REFERENCES,0.7662835249042146,"In NeurIPS, pages 16188–16198, 2020b."
REFERENCES,0.7701149425287356,"Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Signaling in Bayesian network"
REFERENCES,0.7739463601532567,"congestion games: the subtle power of symmetry. In AAAI, pages 5252–5259, 2021a."
REFERENCES,0.7777777777777778,"Matteo Castiglioni, Alberto Marchesi, Andrea Celli, and Nicola Gatti. Multi-receiver online bayesian"
REFERENCES,0.7816091954022989,"persuasion. In ICML, volume 139, pages 1314–1323, 2021b."
REFERENCES,0.7854406130268199,"Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. Bayesian persuasion meets mechanism"
REFERENCES,0.789272030651341,"design: Going beyond intractability with type reporting. In AAMAS 2022, pages 226–234. Interna-
tional Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2022."
REFERENCES,0.7931034482758621,"Andrea Celli, Stefano Coniglio, and Nicola Gatti. Private bayesian persuasion with sequential games."
REFERENCES,0.7969348659003831,"AAAI, 34(02):1886–1893, 2020a."
REFERENCES,0.8007662835249042,"Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for"
REFERENCES,0.8045977011494253,"extensive-form correlated equilibrium. NeurIPS, 33:7722–7732, 2020b."
REFERENCES,0.8084291187739464,"Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge university"
REFERENCES,0.8122605363984674,"press, 2006."
REFERENCES,0.8160919540229885,"Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Simple uncoupled no-regret"
REFERENCES,0.8199233716475096,"learning dynamics for extensive-form correlated equilibrium. arXiv preprint arXiv:2104.01520,
2021a."
REFERENCES,0.8237547892720306,"Gabriele Farina, Robin Schmucker, and Tuomas Sandholm. Bandit linear optimization for sequential"
REFERENCES,0.8275862068965517,"decision making and extensive-form games. In AAAI, volume 35, pages 5372–5380, 2021b."
REFERENCES,0.8314176245210728,"J Gan, R Majumdar, G Radanovic, and A Singla. Bayesian persuasion in sequential decision-making."
REFERENCES,0.8352490421455939,"In AAAI, 2022."
REFERENCES,0.8390804597701149,"Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. Journal of the ACM, 60"
REFERENCES,0.842911877394636,"(6):1–39, 2013."
REFERENCES,0.8467432950191571,"Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101(6):"
REFERENCES,0.8505747126436781,"2590–2615, 2011."
REFERENCES,0.8544061302681992,"Daphne Koller, Nimrod Megiddo, and Bernhard Von Stengel. Efﬁcient computation of equilibria for"
REFERENCES,0.8582375478927203,"extensive two-person games. Games and economic behavior, 14(2):247–259, 1996."
REFERENCES,0.8620689655172413,"Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020."
REFERENCES,0.8659003831417624,"Dustin Morrill, Ryan D’Orazio, Reca Sarfati, Marc Lanctot, James R Wright, Amy R Greenwald,"
REFERENCES,0.8697318007662835,"and Michael Bowling. Hindsight and sequential rationality of correlated play. In AAAI, volume 35,
pages 5584–5594, 2021."
REFERENCES,0.8735632183908046,"Zinovi Rabinovich, Albert Xin Jiang, Manish Jain, and Haifeng Xu. Information disclosure as a"
REFERENCES,0.8773946360153256,"means to security. In AAMAS, pages 645–653, 2015."
REFERENCES,0.8812260536398467,Bernhard Von Stengel and Françoise Forges. Extensive-form correlated equilibrium: Deﬁnition and
REFERENCES,0.8850574712643678,"computational complexity. Mathematics of Operations Research, 33(4):1002–1022, 2008."
REFERENCES,0.8888888888888888,"Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I. Jordan, and Haifeng"
REFERENCES,0.89272030651341,"Xu. Sequential information design: Markov persuasion process and its efﬁcient reinforcement
learning. arXiv preprint arXiv:2202.10678, 2022."
REFERENCES,0.896551724137931,"Haifeng Xu, Rupert Freeman, Vincent Conitzer, Shaddin Dughmi, and Milind Tambe. Signaling in"
REFERENCES,0.9003831417624522,"Bayesian Stackelberg games. In AAMAS, pages 150–158, 2016."
REFERENCES,0.9042145593869731,"You Zu, Krishnamurthy Iyer, and Haifeng Xu. Learning to persuade on the ﬂy: Robustness against"
REFERENCES,0.9080459770114943,"ignorance. In EC, pages 927–928, 2021."
REFERENCES,0.9118773946360154,Checklist
REFERENCES,0.9157088122605364,1. For all authors...
REFERENCES,0.9195402298850575,(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
REFERENCES,0.9233716475095786,"contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] The setting is based on precise"
REFERENCES,0.9272030651340997,"assumptions. If those assumptions are not met then our techiques are not applicable.
(c) Did you discuss any potential negative societal impacts of your work? [N/A] Our work"
REFERENCES,0.9310344827586207,"is mailny theoretical and thus the question does not apply.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to"
REFERENCES,0.9348659003831418,"them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9386973180076629,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes] While all the main"
REFERENCES,0.9425287356321839,"statements are contained in the main paper, all the proof are deferred to the appendix
due to space constraints.
3. If you ran experiments..."
REFERENCES,0.946360153256705,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-"
REFERENCES,0.9501915708812261,"mental results (either in the supplemental material or as a URL)? [N/A]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they"
REFERENCES,0.9540229885057471,"were chosen)? [N/A]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-"
REFERENCES,0.9578544061302682,"ments multiple times)? [N/A]
(d) Did you include the total amount of compute and the type of resources used (e.g., type"
REFERENCES,0.9616858237547893,"of GPUs, internal cluster, or cloud provider)? [N/A]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9655172413793104,"(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]"
REFERENCES,0.9693486590038314,(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
REFERENCES,0.9731800766283525,(d) Did you discuss whether and how consent was obtained from people whose data you’re
REFERENCES,0.9770114942528736,"using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable"
REFERENCES,0.9808429118773946,"information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9846743295019157,"(a) Did you include the full text of instructions given to participants and screenshots, if"
REFERENCES,0.9885057471264368,"applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review"
REFERENCES,0.9923371647509579,"Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount"
REFERENCES,0.9961685823754789,spent on participant compensation? [N/A]
