Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0010626992561105207,"While the class of Polynomial Nets demonstrates comparable performance to neural
networks (NN), it currently has neither theoretical generalization characterization
nor robustness guarantees. To this end, we derive new complexity bounds for the
set of Coupled CP-Decomposition (CCP) and Nested Coupled CP-decomposition
(NCP) models of Polynomial Nets in terms of the ℓ∞-operator-norm and the ℓ2-
operator norm. In addition, we derive bounds on the Lipschitz constant for both
models to establish a theoretical certiﬁcate for their robustness. The theoretical
results enable us to propose a principled regularization scheme that we also evaluate
experimentally in six datasets and show that it improves the accuracy as well as
the robustness of the models to adversarial perturbations. We showcase how
this regularization can be combined with adversarial training, resulting in further
improvements."
INTRODUCTION,0.0021253985122210413,"1
INTRODUCTION"
INTRODUCTION,0.003188097768331562,"Recently, high-degree Polynomial Nets (PNs) have been demonstrating state-of-the-art performance
in a range of challenging tasks like image generation (Karras et al., 2019; Chrysos and Panagakis,
2020), image classiﬁcation (Wang et al., 2018), reinforcement learning (Jayakumar et al., 2020),
non-euclidean representation learning (Chrysos et al., 2020) and sequence models (Su et al., 2020).
In particular, in public benchmarks like the Face veriﬁcation on MegaFace task1 (Kemelmacher-
Shlizerman et al., 2016), Polynomial Nets are currently the top performing model."
INTRODUCTION,0.004250797024442083,"A major advantage of Polynomial Nets over traditional Neural Networks2 is that they are compatible
with efﬁcient Leveled Fully Homomorphic Encryption (LFHE) protocols (Brakerski et al., 2014).
Such protocols allow efﬁcient computation on encrypted data, but they only support addition or
multiplication operations i.e., polynomials. This has prompted an effort to adapt neural networks by
replacing typical activation functions with polynomial approximations (Gilad-Bachrach et al., 2016;
Hesamifard et al., 2018). Polynomial Nets do not need any adaptation to work with LFHE."
INTRODUCTION,0.005313496280552604,"Without doubt, these arguments motivate further investigation about the inner-workings of Polynomial
Nets. Surprisingly, little is known about the theoretical properties of such high-degree polynomial
expansions, despite their success. Previous work on PNs (Chrysos et al., 2020; Chrysos and Panagakis,
2020) have focused on developing the foundational structure of the models as well as their training,
but do not provide an analysis of their generalization ability or robustness to adversarial perturbations."
INTRODUCTION,0.006376195536663124,"In contrast, such type of results are readily available for traditional feed-forward Deep Neural
Networks, in the form of high-probability generalization error bounds (Neyshabur et al., 2015;
Bartlett et al., 2017; Neyshabur et al., 2017; Golowich et al., 2018) or upper bounds on their Lipschitz
constant (Scaman and Virmaux, 2018; Fazlyab et al., 2019; Latorre et al., 2020). Despite their
similarity in the compositional structure, theoretical results for Deep Neural Networks2 do not apply
to Polynomial Nets, as they are essentialy two non-overlapping classes of functions."
INTRODUCTION,0.007438894792773645,"Why are such results important? First, they provide key theoretical quantities like the sample
complexity of a hypothesis class: how many samples are required to succeed at learning in the
PAC-framework. Second, they provide certiﬁed performance guarantees to adversarial perturbations"
INTRODUCTION,0.008501594048884165,"1https://paperswithcode.com/sota/face-verification-on-megaface
2with non-polynomial activation functions."
INTRODUCTION,0.009564293304994687,Published as a conference paper at ICLR 2022
INTRODUCTION,0.010626992561105207,"(Szegedy et al., 2014; Goodfellow et al., 2015) via a worst-case analysis c.f. Scaman and Virmaux
(2018). Most importantly, the bounds themselves provide a principled way to regularize the hypothesis
class and improve their accuracy or robustness."
INTRODUCTION,0.011689691817215728,"For example, Generalization and Lipschitz constant bounds of Deep Neural Networks that depend
on the operator-norm of their weight matrices (Bartlett et al., 2017; Neyshabur et al., 2017) have
layed out the path for regularization schemes like spectral regularization (Yoshida and Miyato, 2017;
Miyato et al., 2018), Lipschitz-margin training (Tsuzuku et al., 2018) and Parseval Networks (Cisse
et al., 2017), to name a few."
INTRODUCTION,0.012752391073326248,"Indeed, such schemes have been observed in practice to improve the performance of Deep Neural
Networks. Unfortunately, similar regularization schemes for Polynomial Nets do not exist due to
the lack of analogous bounds. Hence, it is possible that PNs are not yet being used to their fullest
potential. We believe that theoretical advances in their understanding might lead to more resilient and
accurate models. In this work, we aim to ﬁll the gap in the theoretical understanding of PNs. We
summarize our main contributions as follows:"
INTRODUCTION,0.01381509032943677,"Rademacher Complexity Bounds. We derive bounds on the Rademacher Complexity of the Cou-
pled CP-decomposition model (CCP) and Nested Coupled CP-decomposition model (NCP) of PNs,
under the assumption of a unit ℓ∞-norm bound on the input (Theorems 1 and 3), a natural assumption
in image-based applications. Analogous bounds for the ℓ2-norm are also provided (Appendices E.3
and F.3). Such bounds lead to the ﬁrst known generalization error bounds for this class of models."
INTRODUCTION,0.01487778958554729,"Lipschitz constant Bounds. To complement our understanding of the CCP and NCP models, we
derive upper bounds on their ℓ∞-Lipschitz constants (Theorems 2 and 4), which are directly related
to their robustness against ℓ∞-bounded adversarial perturbations, and provide formal guarantees.
Analogous results hold for any ℓp-norm (Appendices E.4 and F.4)."
INTRODUCTION,0.015940488841657812,"Regularization schemes. We identify key quantities that simultaneously control both Rademacher
Complexity and Lipschitz constant bounds that we previously derived, i.e., the operator norms of
the weight matrices in the Polynomial Nets. Hence, we propose to regularize the CCP and NCP
models by constraining such operator norms. In doing so, our theoretical results indicate that both the
generalization and the robustness to adversarial perturbations should improve. We propose a Projected
Stochastic Gradient Descent scheme (Algorithm 1), enjoying the same per-iteration complexity as
vanilla back-propagation in the ℓ∞-norm case, and a variant that augments the base algorithm with
adversarial traning (Algorithm 2)."
INTRODUCTION,0.01700318809776833,"Experiments. We conduct experimentation in ﬁve widely-used datasets on image recognition and on
dataset in audio recognition. The experimentation illustrates how the aforementioned regularization
schemes impact the accuracy (and the robust accuracy) of both CCP and NCP models, outperforming
alternative schemes such as Jacobian regularization and the L2 weight decay. Indeed, for a grid of
regularization parameters we observe that there exists a sweet-spot for the regularization parameter
which not only increases the test-accuracy of the model, but also its resilience to adversarial pertur-
bations. Larger values of the regularization parameter also allow a trade-off between accuracy and
robustness. The observation is consistent across all datasets and all adversarial attacks demonstrating
the efﬁcacy of the proposed regularization scheme."
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.018065887353878853,"2
RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS"
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.019128586609989374,"Notation.
The symbol ◦denotes the Hadamard (element-wise) product, the symbol • is the face-
splitting product, while the symbol ⋆denotes a convolutional operator. Matrices are denoted by
uppercase letters e.g., V . Due to the space constraints, a detailed notation is deferred to Appendix C."
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.020191285866099893,"Assumption on the input distribution.
Unless explicitly mentioned otherwise, we assume an
ℓ∞-norm unit bound on the input data i.e., ∥z∥∞≤1 for any input z. This is the most common
assumption in image-domain applications in contemporary deep learning, i.e., each pixel takes values
in [−1, 1] interval. Nevertheless, analogous results for ℓ2-norm unit bound assumptions are presented
in Appendices E.3, E.4, F.3 and F.4."
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.021253985122210415,"We now introduce the basic concepts that will be developed throughout the paper i.e., the Rademacher
Complexity of a class of functions (Bartlett and Mendelson, 2002) and the Lipschitz constant."
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.022316684378320937,Published as a conference paper at ICLR 2022
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.023379383634431455,"Figure 1: Schematic of CCP model (left) and NCP model (right), where ◦denotes the Hadamard
product. Blue boxes correspond to learnable parameters. Green and red boxes denote input and
output, respectively. Yellow boxes denote operations."
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.024442082890541977,"Deﬁnition 1 (Empirical Rademacher Complexity). Let Z
= {z1, . . . , zn} ⊆Rd and let
{σj : j = 1, . . . , n} be independent Rademacher random variables i.e., taking values uniformly
in {−1, +1}. Let F be a class of real-valued functions over Rd. The Empirical Rademacher
complexity of F with respect to Z is deﬁned as follows:"
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.025504782146652496,RZ(F) := Eσ
"RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS",0.026567481402763018,"
sup
f∈F"
N,0.02763018065887354,"1
n n
X"
N,0.028692879914984058,"j=1
σjf(zj)

."
N,0.02975557917109458,"Deﬁnition 2 (Lipschitz constant). Given two normed spaces (X, ∥· ∥X ) and (Y, ∥· ∥Y), a function
f : X →Y is called Lipschitz continuous with Lipschitz constant K ≥0 if for all x1, x2 in X:"
N,0.030818278427205102,∥f(x1) −f(x2)∥Y ≤K∥x1 −x2∥X .
N,0.031880977683315624,"2.1
COUPLED CP-DECOMPOSITION OF POLYNOMIAL NETS (CCP MODEL)"
N,0.03294367693942614,"The Coupled CP-Decomposition (CCP) model of PNs (Chrysos et al., 2020) leverages a coupled CP
Tensor decomposition (Kolda and Bader, 2009) to vastly reduce the parameters required to describe a
high-degree polynomial, and allows its computation in a compositional fashion, much similar to a
feed-forward pass through a traditional neural network. The CCP model was used in Chrysos and
Panagakis (2020) to construct a generative model. CCP can be succintly deﬁned as follows:"
N,0.03400637619553666,"f(z) = C ◦k
i=1 Uiz ,
(CCP)"
N,0.03506907545164718,"where z ∈Rd is the input data, f(z) ∈Ro is the output of the model and Un ∈Rm×d, C ∈Ro×m
are the learnable parameters, where m ∈N is the hidden rank. In Fig. 1 we provide a schematic of
the architecture, while in Appendix D.1 we include further details on the original CCP formulation
(and how to obtain our equivalent re-parametrization) for the interested reader."
N,0.036131774707757705,"In Theorem 1 we derive an upper bound on the complexity of CCP models with bounded ℓ∞-operator-
norms of the face-splitting product of the weight matrices. Its proof can be found in Appendix E.1.
For a given CCP model, we derive an upper bound on its ℓ∞-Lipschitz constant in Theorem 2 and its
proof is given in Appendix E.4.1."
N,0.03719447396386823,"Theorem 1. Let Z = {z1, . . . , zn} ⊆Rd and suppose that ∥zj∥∞≤1 for all j = 1, . . . , n. Let"
N,0.03825717321997875,"Fk
CCP :=

f(z) =

c, ◦k
i=1Uiz

: ∥c∥1 ≤µ,
•k
i=1Ui

∞≤λ
	
."
N,0.039319872476089264,"The Empirical Rademacher Complexity of CCPk (k-degree CCP polynomials) with respect to Z is
bounded as:"
N,0.040382571732199786,"RZ(Fk
CCP) ≤2µλ r"
N,0.04144527098831031,"2k log(d) n
."
N,0.04250797024442083,"Proof sketch of Theorem 1
We now describe the core steps of the proof. For the interested reader,
the complete and detailed proof steps are presented in Appendix E.1. First, Hölder’s inequality is
used to bound the Rademacher complexity as:"
N,0.04357066950053135,"RZ(Fk
CCP) = E sup
f∈Fk
CCP"
N,0.044633368756641874,"1
n * c, n
X"
N,0.04569606801275239,"j=1
[σj◦k
i=1(Uizj)] +"
N,0.04675876726886291,"≤E sup
f∈Fk
CCP"
N,0.04782146652497343,"1
n∥c∥1  n
X"
N,0.048884165781083955,"j=1
[σj◦k
i=1(Uizj)] ∞ . (1)"
N,0.04994686503719448,Published as a conference paper at ICLR 2022
N,0.05100956429330499,"This shows why the factor ∥c∥1 ≤µ appears in the ﬁnal bound. Then, using the mixed product
property (Slyusar, 1999) and its extension to repeated Hadamard products (Lemma 7 in Appendix C.3),
we can rewrite the summation in the right-hand-side of (1) as follows: n
X"
N,0.052072263549415514,"j=1
σj◦k
i=1(Uizj) = n
X"
N,0.053134962805526036,"j=1
σj •k
i=1 (Ui) ∗k
i=1 (zj) = •k
i=1(Ui) n
X"
N,0.05419766206163656,"j=1
σj ∗k
i=1 (zj) ."
N,0.05526036131774708,"This step can be seen as a linearization of the polynomial by lifting the problem to a higher dimensional
space. We use this fact and the deﬁnition of the operator norm to further bound the term inside
the ℓ∞-norm in the right-hand-side of (1). Such term is bounded as the product of the ℓ∞-operator
norm of •k
i=1(Ui), and the ℓ∞-norm of an expression involving the Rademacher variables σj and
the vectors ∗k
i=1(zj). Finally, an application of Massart’s Lemma (Shalev-Shwartz and Ben-David
(2014), Lemma 26.8) leads to the ﬁnal result.
Theorem 2. The Lipschitz constant (with respect to the ℓ∞-norm) of the function deﬁned in Eq. (CCP),
restricted to the set {z ∈Rd : ∥z∥∞≤1} is bounded as:"
N,0.0563230605738576,"Lip∞(f) ≤k∥C∥∞ k
Y"
N,0.057385759829968117,"i=1
∥Ui∥∞."
N,0.05844845908607864,"2.2
NESTED COUPLED CP-DECOMPOSITION (NCP MODEL)"
N,0.05951115834218916,"The Nested Coupled CP-Decomposition (NCP) model leverages a joint hierarchical decomposition,
which provided strong results in both generative and discriminative tasks in Chrysos et al. (2020). A
slight re-parametrization of the model (Appendix D.2) can be expressed with the following recursive
relation:"
N,0.06057385759829968,"x1 = (A1z) ◦(s1),
xn = (Anz) ◦(Snxn−1),
f(z) = Cxk ,
(NCP)"
N,0.061636556854410204,"where z ∈Rd is the input vector and C ∈Ro×m, An ∈Rm×d, Sn ∈Rm×m and s1 ∈Rm are the
learnable parameters. In Fig. 1 we provide a schematic of the architecture."
N,0.06269925611052073,"In Theorem 3 we derive an upper bound on the complexity of NCP models with bounded ℓ∞-operator-
norm of a matrix function of its parameters. Its proof can be found in Appendix F.1. For a given NCP
model, we derive an upper bound on its ℓ∞-Lipschitz constant in Theorem 4 and its proof is given in
Appendix F.4.1.
Theorem 3. Let Z = {z1, . . . , zn} ⊆Rd and suppose that ∥zj∥∞≤1 for all j = 1, . . . , n. Deﬁne
the matrix Φ(A1, S1, . . . , An, Sn) := (Ak •Sk) Qk−1
i=1 I ⊗Ai •Si. Consider the class of functions:"
N,0.06376195536663125,"Fk
NCP := {f(z) as in (NCP) : ∥C∥∞≤µ, ∥Φ(A1, S1, . . . , Ak, Sk)∥∞≤λ} ,"
N,0.06482465462274177,"where C ∈R1×m (single output), thus, we will write it as c, and the corresponding bound also
becomes ∥c∥1 ≤µ. The Empirical Rademacher Complexity of NCPk (k-degree NCP polynomials)
with respect to Z is bounded as:"
N,0.06588735387885228,"RZ(Fk
NCP) ≤2µλ r"
N,0.0669500531349628,"2k log(d) n
."
N,0.06801275239107332,"Theorem 4. The Lipschitz constant (with respect to the ℓ∞-norm) of the function deﬁned in Eq. (NCP),
restricted to the set {z ∈Rd : ∥z∥∞≤1} is bounded as:"
N,0.06907545164718384,"Lip∞(f) ≤k∥C∥∞ k
Y"
N,0.07013815090329437,"i=1
(∥Ai∥∞∥Si∥∞) ."
ALGORITHMS,0.07120085015940489,"3
ALGORITHMS"
ALGORITHMS,0.07226354941551541,"By constraining the quantities in the upper bounds on the Rademacher complexity (Theorems 1
and 3), we can regularize the empirical loss minimization objective (Mohri et al., 2018, Theorem 3.3).
Such method would prevent overﬁtting and can lead to an improved accuracy. However, one issue
with the quantities involved in Theorems 1 and 3, namely"
ALGORITHMS,0.07332624867162593,"•k
i=1Ui

∞,"
ALGORITHMS,0.07438894792773645,"(Ak • Sk) k−1
Y"
ALGORITHMS,0.07545164718384698,"i=1
I ⊗Ai • Si ∞
,"
ALGORITHMS,0.0765143464399575,Published as a conference paper at ICLR 2022
ALGORITHMS,0.077577045696068,"is that projecting onto their level sets correspond to a difﬁcult non-convex problem. Nevertheless, we
can control an upper bound that depends on the ℓ∞-operator norm of each weight matrix:"
ALGORITHMS,0.07863974495217853,"Lemma 1. It holds that
•k
i=1Ui

∞≤Qk
i=1 ∥Ui∥∞."
ALGORITHMS,0.07970244420828905,"Lemma 2. It holds that
(Ak • Sk) Qk−1
i=1 I ⊗Ai • Si

∞≤Qk
i=1 ∥Ai∥∞∥Si∥∞."
ALGORITHMS,0.08076514346439957,"The proofs of Lemmas 1 and 2 can be found in Appendix E.2 and Appendix F.2. These results mean
that by constraining the operator norms of each weight matrix, we can control the overall complexity
of the CCP and NCP models."
ALGORITHMS,0.0818278427205101,"Projecting a matrix onto an ℓ∞-operator norm ball is a simple task that can be achieved by projecting
each row of the matrix onto an ℓ1-norm ball, for example, using the well-known algorithm from
Duchi et al. (2008). The ﬁnal optimization objective for training a regularized CCP is the following:"
ALGORITHMS,0.08289054197662062,"min
C,U1,...,Uk
1
n n
X"
ALGORITHMS,0.08395324123273114,"i=1
L(C, U1, . . . , Uk; xi, yi)
subject to ∥C∥∞≤µ, ∥Ui∥∞≤λ ,
(2)"
ALGORITHMS,0.08501594048884166,"where (xi, yi)n
i=1 is the training dataset, L is the loss function (e.g., cross-entropy) and µ, λ are the
regularization parameters. We notice that the constraints on the learnable parameters Ui and C have
the effect of simultaneously controlling the Rademacher Complexity and the Lipschitz constant of
the CCP model. For the NCP model, an analogous objective function is used."
ALGORITHMS,0.08607863974495218,"To solve the optimization problem in Eq. (2) we will use a Projected Stochastic Gradient Descent
method Algorithm 1. We also propose a variant that combines Adversarial Training with the projection
step (Algorithm 2) with the goal of increasing robustness to adversarial examples."
ALGORITHMS,0.0871413390010627,"Algorithm 1: Projected SGD
Input: dataset Z, learning rate
{γt > 0}T −1
t=0 , iterations T,
hyper-parameters R, f , Loss L.
Output: model with parameters θ."
ALGORITHMS,0.08820403825717323,"Initialize θ.
for t = 0 to T −1 do"
ALGORITHMS,0.08926673751328375,"Sample (x, y) from Z
θ = θ −γt▽θL(θ; x, y).
if t mod f = 0 then"
ALGORITHMS,0.09032943676939426,"θ = Q
{θ:∥θ∥∞≤R}(θ)"
ALGORITHMS,0.09139213602550478,Algorithm 2: Projected SGD + Adversarial Training
ALGORITHMS,0.0924548352816153,"Input: dataset Z, learning rate {γt > 0}T −1
t=0 , iterations T
and n, hyper-parameters R, f , ϵ and α, Loss L
Output: model with parameters θ."
ALGORITHMS,0.09351753453772582,"Initialize θ.
for t = 0 to T −1 do"
ALGORITHMS,0.09458023379383634,"Sample (x, y) from Z
for i = 0 to n −1 do"
ALGORITHMS,0.09564293304994687,"xadv = Q
{x′:||x′−x||∞≤ϵ} {x + α∇xL(θ; x, y)}"
ALGORITHMS,0.09670563230605739,"θ = θ −γt▽θL(θ; xadv, y)
if t mod f = 0 then"
ALGORITHMS,0.09776833156216791,"θ = Q
{θ:∥θ∥∞≤R}(θ)"
ALGORITHMS,0.09883103081827843,"In Algorithms 1 and 2 the parameter f is set in practice to a positive value, so that the projection
(denoted by Π) is made only every few iterations. The variable θ represents the weight matrices of
the model, and the projection in the last line should be understood as applied independently for every
weight matrix. The regularization parameter R corresponds to the variables µ, λ in Eq. (2)."
ALGORITHMS,0.09989373007438895,"Convolutional layers
Frequently, convolutions are employed in the literature, especially in the
image-domain. It is important to understand how our previous results extend to this case, and
how the proposed algorithms work in that case. Below, we show that the ℓ∞-operator norm of the
convolutional layer (as a linear operator) is related to the ℓ∞-operator norm of the kernel after a
reshaping operation. For simplicity, we consider only convolutions with zero padding."
ALGORITHMS,0.10095642933049948,"We study the cases of 1D, 2D and 3D convolutions. For clarity, we mention below the result for the
3D convolution, since this is relevant to our experimental validation, and we defer the other two cases
to Appendix G."
ALGORITHMS,0.10201912858660998,"Theorem 5. Let A ∈Rn×m×r be an input image and let K ∈Rh×h×r×o be a convolutional kernel
with o output channels. For simplicity assume that k ≤min(n, m) is odd. Denote by B = K ⋆A the
output of the convolutional layer. Let U ∈Rnmo×nmr be the matrix such that vec(B) = Uvec(A)
i.e., U is the matrix representation of the convolution. Let M(K) ∈Ro×hhr be the matricization of"
ALGORITHMS,0.1030818278427205,Published as a conference paper at ICLR 2022
ALGORITHMS,0.10414452709883103,"K, where each row contains the parameters of a single output channel of the convolution. It holds
that: ∥U∥∞= ∥M(K)∥∞."
ALGORITHMS,0.10520722635494155,"Thus, we can control the ℓ∞-operator-norm of a convolutional layer during training by controlling
that of the reshaping of the kernel, which is done with the same code as for fully connected layers. It
can be seen that when the padding is non-zero, the result still holds."
NUMERICAL EVIDENCE,0.10626992561105207,"4
NUMERICAL EVIDENCE"
NUMERICAL EVIDENCE,0.1073326248671626,"The generalization properties and the robustness of PNs are numerically veriﬁed in this section.
We evaluate the robustness to three widely-used adversarial attacks in sec. 4.2. We assess whether
the compared regularization schemes can also help in the case of adversarial training in sec. 4.3.
Experiments with additional datasets, models (NCP models), adversarial attacks (APGDT, PGDT)
and layer-wise bound (instead of a single bound for all matrices) are conducted in Appendix H
due to the restricted space. The results exhibit a consistent behavior across different adversarial
attacks, different datasets and different models. Whenever the results differ, we explicitly mention the
differences in the main body below."
EXPERIMENTAL SETUP,0.10839532412327312,"4.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.10945802337938364,"The accuracy is reported as as the evaluation metric for every experiment, where a higher accuracy
translates to better performance."
EXPERIMENTAL SETUP,0.11052072263549416,"Datasets and Benchmark Models: We conduct experiments on the popular datasets of Fashion-
MNIST (Xiao et al., 2017), E-MNIST (Cohen et al., 2017) and CIFAR-10 (Krizhevsky et al.,
2014). The ﬁrst two datasets include grayscale images of resolution 28 × 28, while CIFAR-10
includes 60, 000 RGB images of resolution 32 × 32. Each image is annotated with one out of the
ten categories. We use two popular regularization methods from the literature for comparison, i.e.,
Jacobian regularization (Hoffman et al., 2019) and L2 regularization (weight decay)."
EXPERIMENTAL SETUP,0.11158342189160468,"Models: We report results using the following three models: 1) a 4th-degree CCP model named
""PN-4"", 2) a 10th-degree CCP model referenced as ""PN-10"" and 3) a 4th-degree Convolutional CCP
model called ""PN-Conv"". In the PN-Conv, we have replaced all the Ui matrices with convolutional
kernels. None of the variants contains any activation functions."
EXPERIMENTAL SETUP,0.1126461211477152,"Hyper-parameters: Unless mentioned otherwise, all models are trained for 100 epochs with a batch
size of 64. The initial value of the learning rate is 0.001. After the ﬁrst 25 epochs, the learning rate is
multiplied by a factor of 0.2 every 50 epochs. The SGD is used to optimize all the models, while the
cross-entropy loss is used. In the experiments that include projection or adversarial training, the ﬁrst
50 epochs are pre-training, i.e., training only with the cross-entropy loss. The projection is performed
every ten iterations."
EXPERIMENTAL SETUP,0.11370882040382571,"Adversarial Attack Settings: We utilize two widely used attacks: a) Fast Gradient Sign Method
(FGSM) and b) Projected Gradient Descent (PGD). In FGSM the hyper-parameter ϵ represents the
step size of the adversarial attack. In PGD there is a triple of parameters (ϵtotal, niters, ϵiter), which
represent the maximum step size of the total adversarial attack, the number of steps to perform for a
single attack, and the step size of each adversarial attack step respectively. We consider the following
hyper-parameters for the attacks: a) FGSM with ϵ = 0.1, b) PGD with parameters (0.1, 20, 0.01), c)
PGD with parameters (0.3, 20, 0.03)."
EVALUATION OF THE ROBUSTNESS OF PNS,0.11477151965993623,"4.2
EVALUATION OF THE ROBUSTNESS OF PNS"
EVALUATION OF THE ROBUSTNESS OF PNS,0.11583421891604676,"In the next experiment, we assess the robustness of PNs under adversarial noise. That is, the method
is trained on the train set of the respective dataset and the evaluation is performed on the test set
perturbed by additive adversarial noise. That is, each image is individually perturbed based on the
respective adversarial attack. The proposed method implements Algorithm 1."
EVALUATION OF THE ROBUSTNESS OF PNS,0.11689691817215728,"The quantitative results in both Fashion-MNIST and E-MNIST using PN-4, PN-10 and PN-Conv
under the three attacks are reported in Table 1. The column ‘No-proj’ exhibits the plain SGD training
(i.e., without regularization), while the remaining columns include the proposed regularization,
Jacobian and the L2 regularization respectively. The results without regularization exhibit a substantial
decrease in accuracy for stronger adversarial attacks. The proposed regularization outperforms all
methods consistently across different adversarial attacks. Interestingly, the stronger the adversarial"
EVALUATION OF THE ROBUSTNESS OF PNS,0.1179596174282678,Published as a conference paper at ICLR 2022
EVALUATION OF THE ROBUSTNESS OF PNS,0.11902231668437832,"Method
No proj.
Our method
Jacobian
L2
Fashion-MNIST PN-4"
EVALUATION OF THE ROBUSTNESS OF PNS,0.12008501594048884,"Clean
87.28 ± 0.18% 87.32 ± 0.14% 86.24 ± 0.14%
87.31 ± 0.13%
FGSM-0.1
12.92 ± 2.74% 46.43 ± 0.95% 17.90 ± 6.51%
13.80 ± 3.65%
PGD-(0.1, 20, 0.01) 5.64 ± 1.76%
49.58 ± 0.59% 12.23 ± 5.63%
5.01 ± 2.44%
PGD-(0.3, 20, 0.03) 0.18 ± 0.16%
28.96 ± 2.31%
1.27 ± 1.29%
0.28 ± 0.18% PN-10"
EVALUATION OF THE ROBUSTNESS OF PNS,0.12114771519659936,"Clean
88.48 ± 0.17% 88.72 ± 0.12% 88.12 ± 0.11%
88.46 ± 0.19%
FGSM-0.1
15.96 ± 1.00% 44.71 ± 1.24% 19.52 ± 1.14%
16.51 ± 2.33%
PGD-(0.1, 20, 0.01) 1.94 ± 0.82%
47.94 ± 2.29%
5.44 ± 0.81%
2.16 ± 0.95%
PGD-(0.3, 20, 0.03) 0.02 ± 0.03%
30.51 ± 1.22%
0.05 ± 0.02%
0.01 ± 0.02%"
EVALUATION OF THE ROBUSTNESS OF PNS,0.12221041445270989,PN-Conv
EVALUATION OF THE ROBUSTNESS OF PNS,0.12327311370882041,"Clean
86.36 ± 0.21%
86.38 ± 0.26%
84.69 ± 0.44% 86.45 ± 0.21%
FGSM-0.1
10.80 ± 1.82% 48.15 ± 1.23% 10.62 ± 0.77%
10.73 ± 1.58%
PGD-(0.1, 20, 0.01) 9.37 ± 1.00%
46.63 ± 3.68% 10.20 ± 0.32%
8.96 ± 0.83%
PGD-(0.3, 20, 0.03) 1.75 ± 0.83%
28.94 ± 1.20%
8.26 ± 1.05%
2.03 ± 0.99%
E-MNIST PN-4"
EVALUATION OF THE ROBUSTNESS OF PNS,0.12433581296493093,"Clean
84.27 ± 0.26% 84.34 ± 0.31% 81.99 ± 0.33%
84.22 ± 0.33%
FGSM-0.1
8.92 ± 1.99%
27.56 ± 3.32% 14.96 ± 1.32%
8.18 ± 3.48%
PGD-(0.1, 20, 0.01) 6.24 ± 1.43%
29.46 ± 2.73%
6.75 ± 2.92%
5.93 ± 1.97%
PGD-(0.3, 20, 0.03) 1.22 ± 0.85%
19.07 ± 0.98%
3.06 ± 0.53%
1.00 ± 0.76% PN-10"
EVALUATION OF THE ROBUSTNESS OF PNS,0.12539851222104145,"Clean
89.31 ± 0.09% 90.56 ± 0.10% 89.19 ± 0.07%
89.23 ± 0.13%
FGSM-0.1
15.56 ± 1.16% 37.11 ± 2.81% 24.21 ± 1.89%
16.30 ± 1.82%
PGD-(0.1, 20, 0.01) 2.63 ± 0.65%
37.89 ± 2.91%
9.18 ± 1.09%
2.33 ± 0.43%
PGD-(0.3, 20, 0.03) 0.00 ± 0.00%
20.47 ± 0.96%
0.11 ± 0.08%
0.02 ± 0.03%"
EVALUATION OF THE ROBUSTNESS OF PNS,0.12646121147715197,PN-Conv
EVALUATION OF THE ROBUSTNESS OF PNS,0.1275239107332625,"Clean
91.49 ± 0.29% 91.57 ± 0.19% 90.38 ± 0.13%
91.41 ± 0.18%
FGSM-0.1
4.28 ± 0.55%
35.39 ± 7.51%
3.88 ± 0.04%
4.13 ± 0.41%
PGD-(0.1, 20, 0.01) 3.98 ± 0.82%
33.75 ± 7.17%
3.86 ± 0.01%
4.83 ± 0.87%
PGD-(0.3, 20, 0.03) 3.24 ± 0.76%
28.10 ± 3.27%
3.84 ± 0.01%
2.76 ± 0.65%"
EVALUATION OF THE ROBUSTNESS OF PNS,0.12858660998937302,"Table 1: Comparison of regularization techniques on Fashion-MNIST (top) and E-MNIST (bottom).
In each dataset, the base networks are PN-4, i.e., a 4th degree polynomial, on the top four rows, PN-10,
i.e., a 10th degree polynomial, on the middle four rows and PN-Conv, i.e., a 4th degree polynomial
with convolutions, on the bottom four rows. Our projection method exhibits the best performance in
all three attacks, with the difference on accuracy to stronger attacks being substantial."
EVALUATION OF THE ROBUSTNESS OF PNS,0.12964930924548354,"attack, the bigger the difference of the proposed regularization scheme with the alternatives of
Jacobian and L2 regularizations."
EVALUATION OF THE ROBUSTNESS OF PNS,0.13071200850159406,"Next, we learn the networks with varying projection bounds. The results on Fashion-MNIST and
E-MNIST are visualized in Fig. 2, where the x-axis is plotted in log-scale. As a reference point, we
include the clean accuracy curves, i.e., when there is no adversarial noise. Projection bounds larger
than 2 (in the log-axis) leave the accuracy unchanged. As the bounds decrease, the results gradually
improve. This can be attributed to the constraints the projection bounds impose into the Ui matrices."
EVALUATION OF THE ROBUSTNESS OF PNS,0.13177470775770456,"Similar observations can be made when evaluating the clean accuracy (i.e., no adversarial noise in
the test set). However, in the case of adversarial attacks a tighter bound performs better, i.e., the best
accuracy is exhibited in the region of 0 in the log-axis. The projection bounds can have a substantial
improvement on the performance, especially in the case of stronger adversarial attacks, i.e., PGD.
Notice that all in the aforementioned cases, the intermediate values of the projection bounds yield an
increased performance in terms of the test-accuracy and the adversarial perturbations."
EVALUATION OF THE ROBUSTNESS OF PNS,0.13283740701381508,"Beyond the aforementioned datasets, we also validate the proposed method on CIFAR-10 dataset.
The results in Fig. 3 and Table 2 exhibit similar patterns as the aforementioned experiments. Although
the improvement is smaller than the case of Fashion-MNIST and E-MNIST, we can still obtain about
10% accuracy improvement under three different adversarial attacks."
EVALUATION OF THE ROBUSTNESS OF PNS,0.1339001062699256,"4.3
ADVERSARIAL TRAINING (AT) ON PNS"
EVALUATION OF THE ROBUSTNESS OF PNS,0.13496280552603612,"Adversarial training has been used as a strong defence against adversarial attacks. In this experiment
we evaluate whether different regularization methods can work in conjunction with adversarial training
that is widely used as a defence method. Since multi-step adversarial attacks are computationally
intensive, we utilize the FGSM attack during training, while we evaluate the trained model in all three"
EVALUATION OF THE ROBUSTNESS OF PNS,0.13602550478214664,Published as a conference paper at ICLR 2022
EVALUATION OF THE ROBUSTNESS OF PNS,0.13708820403825717,"Clean
FGSM_0.1
PGD_0.1_0.01_20
PGD_0.3_0.03_20"
EVALUATION OF THE ROBUSTNESS OF PNS,0.1381509032943677,"0
2
4
Log of Bound 0 20 40 60 80 100"
EVALUATION OF THE ROBUSTNESS OF PNS,0.1392136025504782,Accuracy(%) PN-4
EVALUATION OF THE ROBUSTNESS OF PNS,0.14027630180658873,"0
2
4
Log of Bound PN-10"
EVALUATION OF THE ROBUSTNESS OF PNS,0.14133900106269925,"0
2
4
Log of Bound"
EVALUATION OF THE ROBUSTNESS OF PNS,0.14240170031880978,PN-Conv
EVALUATION OF THE ROBUSTNESS OF PNS,0.1434643995749203,(a) Fashion-MNIST
EVALUATION OF THE ROBUSTNESS OF PNS,0.14452709883103082,"0
2
4
Log of Bound 0 20 40 60 80 100"
EVALUATION OF THE ROBUSTNESS OF PNS,0.14558979808714134,Accuracy(%) PN-4
EVALUATION OF THE ROBUSTNESS OF PNS,0.14665249734325186,"0
2
4
Log of Bound PN-10"
EVALUATION OF THE ROBUSTNESS OF PNS,0.14771519659936239,"0
2
4
Log of Bound"
EVALUATION OF THE ROBUSTNESS OF PNS,0.1487778958554729,PN-Conv
EVALUATION OF THE ROBUSTNESS OF PNS,0.14984059511158343,"(b) E-MNIST
Figure 2: Adversarial attacks during testing on (a) Fashion-MNIST (top), (b) E-MNIST (bottom)
with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds yield the
highest accuracy. The patterns are consistent in both datasets and across adversarial attacks."
EVALUATION OF THE ROBUSTNESS OF PNS,0.15090329436769395,"Clean
FGSM_0.1
PGD_0.1_0.01_20
PGD_0.3_0.03_20"
EVALUATION OF THE ROBUSTNESS OF PNS,0.15196599362380447,"1
0
1
2
3
4
Log of Bound 0 20 40 60 80 100"
EVALUATION OF THE ROBUSTNESS OF PNS,0.153028692879915,Accuracy(%)
EVALUATION OF THE ROBUSTNESS OF PNS,0.15409139213602552,PN-Conv
EVALUATION OF THE ROBUSTNESS OF PNS,0.155154091392136,Figure 3: Adversarial attacks during testing on CIFAR-10.
EVALUATION OF THE ROBUSTNESS OF PNS,0.15621679064824653,Published as a conference paper at ICLR 2022
EVALUATION OF THE ROBUSTNESS OF PNS,0.15727948990435706,"Model
PN-Conv
Projection
No-proj
Our method
Jacobian
L2
Clean accuracy
65.09 ± 0.14% 65.22 ± 0.13% 64.43 ± 0.19% 65.11 ± 0.08%
FGSM-0.1
6.00 ± 0.53%
15.13 ± 0.81%
3.34 ± 0.40%
1.27 ± 0.10%
PGD-(0.1, 20, 0.01)
7.08 ± 0.68%
15.17 ± 0.88%
1.74 ± 0.14%
1.05 ± 0.05%
PGD-(0.3, 20, 0.03)
0.41 ± 0.09%
11.71 ± 1.11%
0.04 ± 0.02%
0.51 ± 0.04%"
EVALUATION OF THE ROBUSTNESS OF PNS,0.15834218916046758,"Table 2: Evaluation of the robustness of PN models on CIFAR-10. Each line refers to a different
adversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacks
the projection improves the accuracy by a signiﬁcant margin."
EVALUATION OF THE ROBUSTNESS OF PNS,0.1594048884165781,"adversarial attacks. For this experiment we select PN-10 as the base model. The proposed model
implements Algorithm 2."
EVALUATION OF THE ROBUSTNESS OF PNS,0.16046758767268862,"The accuracy is reported in Table 3 with Fashion-MNIST on the top and E-MNIST on the bottom. In
the FGSM attack, the difference of the compared methods is smaller, which is expected since similar
attack is used for the training. However, for stronger attacks the difference becomes pronounced
with the proposed regularization method outperforming both the Jacobian and the L2 regularization
methods."
EVALUATION OF THE ROBUSTNESS OF PNS,0.16153028692879914,"Method
AT
Our method + AT
Jacobian + AT
L2 + AT
Adversarial training (AT) with PN-10 on Fashion-MNIST
FGSM-0.1
65.33 ± 0.46%
65.64 ± 0.35%
62.04 ± 0.22%
65.62 ± 0.15%
PGD-(0.1, 20, 0.01) 57.45 ± 0.35%
59.89 ± 0.22%
57.42 ± 0.24%
57.40 ± 0.36%
PGD-(0.3, 20, 0.03) 24.46 ± 0.45%
39.79 ± 1.40%
25.59 ± 0.20%
24.99 ± 0.57%
Adversarial training (AT) with PN-10 on E-MNIST
FGSM-0.1
78.30 ± 0.18%
78.61 ± 0.58%
70.11 ± 0.18%
78.31 ± 0.32%
PGD-(0.1, 20, 0.01) 68.40 ± 0.32%
68.51 ± 0.19%
64.61 ± 0.16%
68.41 ± 0.37%
PGD-(0.3, 20, 0.03) 35.58 ± 0.33%
42.22 ± 0.60%
39.83 ± 0.24%
35.17 ± 0.46%"
EVALUATION OF THE ROBUSTNESS OF PNS,0.16259298618490967,"Table 3: Comparison of regularization techniques on (a) Fashion-MNIST (top) and (b) E-MNIST (bot-
tom) along with adversarial training (AT). The base network is a PN-10, i.e., 10th degree polynomial.
Our projection method exhibits the best performance in all three attacks."
EVALUATION OF THE ROBUSTNESS OF PNS,0.1636556854410202,"The limitations of the proposed work are threefold. Firstly, Theorem 1 relies on the ℓ∞-operator
norm of the face-splitting product of the weight matrices, which in practice we relax in Lemma 1 for
performing the projection. In the future, we aim to study if it is feasible to compute the non-convex
projection onto the set of PNs with bounded ℓ∞-norm of the face-splitting product of the weight
matrices. This would allow us to let go off the relaxation argument and directly optimize the original
tighter Rademacher Complexity bound (Theorem 1)."
EVALUATION OF THE ROBUSTNESS OF PNS,0.1647183846971307,"Secondly, the regularization effect of the projection differs across datasets and adversarial attacks, a
topic that is worth investigating in the future."
EVALUATION OF THE ROBUSTNESS OF PNS,0.16578108395324123,"Thirdly, our bounds do not take into account the algorithm used, which corresponds to a variant of
the Stochastic Projected Gradient Descent, and hence any improved generalization properties due to
possible uniform stability (Bousquet and Elisseeff, 2002) of the algorithm or implicit regularization
properties (Neyshabur, 2017), do not play a role in our analysis."
CONCLUSION,0.16684378320935175,"5
CONCLUSION"
CONCLUSION,0.16790648246546228,"In this work, we explore the generalization properties of the Coupled CP-decomposition (CCP) and
nested coupled CP-decomposition (NCP) models that belong in the class of Polynomial Nets (PNs).
We derive bounds for the Rademacher complexity and the Lipschitz constant of the CCP and the
NCP models. We utilize the computed bounds as a regularization during training. The regularization
terms have also a substantial effect on the robustness of the model, i.e., when adversarial noise is
added to the test set. A future direction of research is to obtain generalization bounds for this class of
functions using stability notions. Along with the recent empirical results on PNs, our derived bounds
can further explain the beneﬁts and drawbacks of using PNs."
CONCLUSION,0.1689691817215728,Published as a conference paper at ICLR 2022
CONCLUSION,0.17003188097768332,ACKNOWLEDGEMENTS
CONCLUSION,0.17109458023379384,"We are thankful to Igor Krawczuk and Andreas Loukas for their comments on the paper. We are
also thankful to the reviewers for providing constructive feedback. Research was sponsored by the
Army Research Ofﬁce and was accomplished under Grant Number W911NF-19-1-0404. This work is
funded (in part) through a PhD fellowship of the Swiss Data Science Center, a joint venture between
EPFL and ETH Zurich. This project has received funding from the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation programme (grant agreement
number 725594 - time-data)."
REFERENCES,0.17215727948990436,REFERENCES
REFERENCES,0.17321997874601489,"P. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks,
2017."
REFERENCES,0.1742826780021254,"P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463–482, 2002."
REFERENCES,0.17534537725823593,"O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,
2:499–526, 2002."
REFERENCES,0.17640807651434645,"Z. Brakerski, C. Gentry, and V. Vaikuntanathan. (leveled) fully homomorphic encryption without
bootstrapping. ACM Transactions on Computation Theory (TOCT), 6(3):1–36, 2014."
REFERENCES,0.17747077577045697,"G. Chrysos, S. Moschoglou, G. Bouritsas, Y. Panagakis, J. Deng, and S. Zafeiriou. π−nets: Deep
polynomial neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR),
2020."
REFERENCES,0.1785334750265675,"G. G. Chrysos and Y. Panagakis. NAPS: Non-adversarial polynomial synthesis. Pattern Recognit.
Lett., 140:318–324, 2020."
REFERENCES,0.179596174282678,"M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In International Conference on Machine Learning, pages
854–863. PMLR, 2017."
REFERENCES,0.1806588735387885,"T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep learning for
classical japanese literature. arXiv preprint arXiv:1812.01718, 2018."
REFERENCES,0.18172157279489903,"G. Cohen, S. Afshar, J. Tapson, and A. van Schaik. Emnist: an extension of mnist to handwritten
letters. arXiv preprint arXiv:1908.06571, 2017."
REFERENCES,0.18278427205100956,"F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In Proceedings of the 37th International Conference on Machine Learning,
2020."
REFERENCES,0.18384697130712008,"Z. Cvetkovski. Hölder’s Inequality, Minkowski’s Inequality and Their Variants, pages 95–105.
Springer Berlin Heidelberg, 2012. doi: 10.1007/978-3-642-23792-8_9."
REFERENCES,0.1849096705632306,"J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efﬁcient projections onto the l 1-ball for
learning in high dimensions. In Proceedings of the 25th international conference on Machine
learning, pages 272–279, 2008."
REFERENCES,0.18597236981934112,"J. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck, K. Simonyan, and M. Norouzi. Neural audio
synthesis of musical notes with wavenet autoencoders, 2017."
REFERENCES,0.18703506907545164,"M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas. Efﬁcient and accurate estimation
of lipschitz constants for deep neural networks. In Advances in neural information processing
systems (NeurIPS), 2019."
REFERENCES,0.18809776833156217,"H. Federer. Geometric measure theory. Springer, 2014."
REFERENCES,0.1891604675876727,Published as a conference paper at ICLR 2022
REFERENCES,0.1902231668437832,"R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing. Cryptonets:
Applying neural networks to encrypted data with high throughput and accuracy. In M. F. Balcan
and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine
Learning, volume 48 of Proceedings of Machine Learning Research, pages 201–210, New York,
New York, USA, 20–22 Jun 2016. PMLR."
REFERENCES,0.19128586609989373,"N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
In COLT, 2018."
REFERENCES,0.19234856535600425,"I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations (ICLR), 2015."
REFERENCES,0.19341126461211477,"E. Hesamifard, H. Takabi, M. Ghasemi, and R. N. Wright. Privacy-preserving machine learning as a
service. Proc. Priv. Enhancing Technol., 2018(3):123–142, 2018."
REFERENCES,0.1944739638682253,"J. Hoffman, D. A. Roberts, and S. Yaida. Robust learning with jacobian regularization, 2019."
REFERENCES,0.19553666312433582,"S. M. Jayakumar, W. M. Czarnecki, J. Menick, J. Schwarz, J. Rae, S. Osindero, Y. W. Teh, T. Harley,
and R. Pascanu. Multiplicative interactions and where to ﬁnd them. In International Conference
on Learning Representations (ICLR), 2020."
REFERENCES,0.19659936238044634,"T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial
networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019."
REFERENCES,0.19766206163655686,"I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard. The megaface benchmark: 1
million faces for recognition at scale. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4873–4882, 2016."
REFERENCES,0.19872476089266738,"T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51(3):455–500,
2009."
REFERENCES,0.1997874601487779,"A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset. online: http://www. cs. toronto.
edu/kriz/cifar. html, 55, 2014."
REFERENCES,0.20085015940488843,"F. Latorre, P. Rolland, and V. Cevher. Lipschitz constant estimation of neural networks via sparse
polynomial optimization. In International Conference on Learning Representations (ICLR), 2020."
REFERENCES,0.20191285866099895,"Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791."
REFERENCES,0.20297555791710944,"F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu. Defense against adversarial attacks using
high-level representation guided denoiser. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018."
REFERENCES,0.20403825717321997,"T. Lyche. Numerical Linear Algebra and Matrix Factorizations. Springer, Cham, 2020."
REFERENCES,0.2051009564293305,"T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial
networks. In International Conference on Learning Representations, 2018."
REFERENCES,0.206163655685441,"M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. Adaptive
Computation and Machine Learning. MIT Press, Cambridge, MA, 2 edition, 2018. ISBN 978-0-
262-03940-6."
REFERENCES,0.20722635494155153,"B. Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017."
REFERENCES,0.20828905419766205,"B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In
Conference on Learning Theory, pages 1376–1401. PMLR, 2015."
REFERENCES,0.20935175345377258,"B. Neyshabur, S. Bhojanapalli, and N. Srebro. A pac-bayesian approach to spectrally-normalized
margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017."
REFERENCES,0.2104144527098831,"C. R. Rao. Estimation of heteroscedastic variances in linear models. Journal of the American
Statistical Association, 65(329):161–172, 1970. doi: 10.1080/01621459.1970.10481070."
REFERENCES,0.21147715196599362,Published as a conference paper at ICLR 2022
REFERENCES,0.21253985122210414,"K. Scaman and A. Virmaux. Lipschitz regularity of deep neural networks: analysis and efﬁcient
estimation. In Advances in neural information processing systems (NeurIPS), 2018."
REFERENCES,0.21360255047821466,"S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.
Cambridge University Press, 2014. ISBN 1107057132."
REFERENCES,0.2146652497343252,"V. Slyusar. A family of face products of matrices and its properties. Cybernetics and Systems Analysis,
35(3):379–384, 1999."
REFERENCES,0.2157279489904357,"J. Su, W. Byeon, J. Kossaiﬁ, F. Huang, J. Kautz, and A. Anandkumar. Convolutional tensor-train lstm
for spatio-temporal learning. Advances in neural information processing systems (NeurIPS), 2020."
REFERENCES,0.21679064824654623,"C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR),
2014."
REFERENCES,0.21785334750265675,"Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-margin training: Scalable certiﬁcation of perturba-
tion invariance for deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018."
REFERENCES,0.21891604675876727,"X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Conference on Computer
Vision and Pattern Recognition (CVPR), pages 7794–7803, 2018."
REFERENCES,0.2199787460148778,"H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017."
REFERENCES,0.22104144527098832,"C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He. Feature denoising for improving adver-
sarial robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2019."
REFERENCES,0.22210414452709884,"Y. Yoshida and T. Miyato. Spectral Norm Regularization for Improving the Generalizability of Deep
Learning. arXiv e-prints, art. arXiv:1705.10941, May 2017."
REFERENCES,0.22316684378320936,"H. Zhang, Y. Yu, J. Jiao, E. Xing, L. E. Ghaoui, and M. Jordan. Theoretically principled trade-off
between robustness and accuracy. In Proceedings of the 36th International Conference on Machine
Learning, 2019."
REFERENCES,0.22422954303931988,Published as a conference paper at ICLR 2022
REFERENCES,0.2252922422954304,"A
APPENDIX INTRODUCTION"
REFERENCES,0.2263549415515409,The Appendix is organized as follows:
REFERENCES,0.22741764080765142,• The related work is summarized in Appendix B.
REFERENCES,0.22848034006376194,• In Appendix C the notation and the core Lemmas from the literature are described.
REFERENCES,0.22954303931987247,• Further details on the Polynomial Nets are provided in Appendix D.
REFERENCES,0.230605738575983,• The proofs on the CCP and the NCP models are added in Appendix E and Appendix F
REFERENCES,0.2316684378320935,respectively.
REFERENCES,0.23273113708820403,• The extensions of the theorems for convolutional layers and their proofs are detailed in
REFERENCES,0.23379383634431455,Appendix G.
REFERENCES,0.23485653560042508,• Additional experiments are included in Appendix H.
REFERENCES,0.2359192348565356,"B
RELATED WORK"
REFERENCES,0.23698193411264612,"Rademacher Complexity: Known bounds for the class of polynomials are a consequence of more
general result for kernel methods (Mohri et al., 2018, Theorem 6.12). Support Vector Machines
(SVMs) with a polynomial kernel of degree k effectively correspond to a general polynomial with the
same degree. In contrast, our bound is tailored to the parametric deﬁnition of the CCP and the NCP
models, which are a subset of the class of all polynomials. Hence, they are tighter than the general
kernel complexity bounds."
REFERENCES,0.23804463336875664,"Bounds for the class of neural networks were stablished in (Bartlett et al., 2017; Neyshabur et al.,
2017), but they require a long and technical proof, and in particular it assumes an ℓ2-bound on the
input, which is incompatible with image-based applications. This bound also depend on the product of
spectral norms of each layer. In contrast, our bounds are more similar in spirit to the path-norm-based
complexity bounds (Neyshabur et al., 2015), as they depend on interactions between neurons at
different layers. This interaction precisely corresponds to the face-splitting product between weight
matrices that appears in Theorem 1."
REFERENCES,0.23910733262486716,"Lipschitz constant: A variety of methods have been proposed for estimating the Lipschitz constant
of neural networks. For example, Scaman and Virmaux (2018) (SVD), Fazlyab et al. (2019) (Semidef-
inite programming) and Latorre et al. (2020) (Polynomial Optimization) are expensive optimization
methods to compute tighter bounds on such constant. These methods are unusable in our case as they
would require a non-trivial adaptation to work with Polynomial Nets. In contrast we ﬁnd an upper
bound that applies to such family of models, and it can be controlled with efﬁcient ℓ∞-operator-norm
projections. However, our bounds might not be the tightest. Developing tighter methods to bound
and control the Lipschitz constant for Polynomial Nets is a promising direction of future research."
REFERENCES,0.24017003188097769,"C
BACKGROUND"
REFERENCES,0.2412327311370882,"Below, we develop a detailed notation in Appendix C.1, we include related deﬁnitions in Appendix C.2
and Lemmas required for our proofs in Appendix C.3. The goal of this section is to cover many of
the required information for following the proofs and the notation we follow in this work. Readers
familiar with the different matrix/vector products, e.g., Khatri-Rao or face-splitting product, and with
basic inequalities, e.g., Hölder’s inequality, can skip to the next section."
REFERENCES,0.24229543039319873,"C.1
NOTATION"
REFERENCES,0.24335812964930925,"Different matrix products and their associated symbols are referenced in Table 4, while matrix
operations on a matrix A are deﬁned on Table 5. Every matrix product, e.g., Hadamard product,
can be used in two ways: a) A ◦B, which translates to Hadamard product of matrices A and B, b)
◦N
i=1Ai abbreviates the Hadamard products A1 ◦A2 ◦. . . AN
|
{z
}
N products
."
REFERENCES,0.24442082890541977,"The symbol xj
i refers to jth element of vector xi."
REFERENCES,0.2454835281615303,Published as a conference paper at ICLR 2022
REFERENCES,0.24654622741764082,"Table 4: Symbols for various matrix products. The precise deﬁnitions of the products are included in
Appendix C.2 for completion."
REFERENCES,0.24760892667375134,"Symbol
Deﬁnition
◦
Hadamard (element-wise) product.
∗
Column-wise Khatri–Rao product.
•
Face-splitting product.
⊗
Kronecker product.
⋆
Convolution."
REFERENCES,0.24867162592986186,Table 5: Operations and symbols on a matrix A.
REFERENCES,0.24973432518597238,"Symbol
Deﬁnition
∥A∥∞
ℓ∞-operator-norm; corresponds to the maximum ℓ1-norm of its rows.
Ai
ith row of A.
ai,j
(i, j)th element of A.
Ai,j
(i, j)th block of a block-matrix A
Ai
The i-th matrix in a set of matrices {A1, · · · , AN}."
REFERENCES,0.2507970244420829,"C.2
DEFINITIONS"
REFERENCES,0.2518597236981934,"For thoroughness, we include the deﬁnitions of the core products that we use in this work. Speciﬁcally,
the deﬁnitions of the Hadamard product (Deﬁnition 3), Kronecker product (Deﬁnition 4), the Khatri-
Rao product (Deﬁnition 5), column-wise Khatri-Rao product (Deﬁnition 6) and the face-splitting
product (Deﬁnition 7) are included."
REFERENCES,0.25292242295430395,"Deﬁnition 3 (Hadamard product). For two matrices A and B of the same dimension m × n, the
Hadamard product A ◦B is a matrix of the same dimension as the operands, with elements given by"
REFERENCES,0.25398512221041447,"(a ◦b)i,j = ai,jbi,j ."
REFERENCES,0.255047821466525,"Deﬁnition 4 (Kronecker product). If A is an m × n matrix and B is a p × q matrix, then the
Kronecker product A ⊗B is the pm × qn block matrix, given as follows:"
REFERENCES,0.2561105207226355,A ⊗B =  
REFERENCES,0.25717321997874604,"a1,1B
· · ·
a1,nB
...
...
...
am,1B
· · ·
am,nB  ."
REFERENCES,0.25823591923485656,Example: the Kronecker product of the matrices A ∈R2×2 and B ∈R2×2 is computed below:
REFERENCES,0.2592986184909671,"
a1,1
a1,2
a2,1
a2,2 "
REFERENCES,0.2603613177470776,"|
{z
}
A"
REFERENCES,0.2614240170031881,"⊗

b1,1
b1,2
b2,1
b2,2 "
REFERENCES,0.2624867162592986,"|
{z
}
B =  "
REFERENCES,0.2635494155154091,"a1,1b1,1
a1,1b1,2
a1,2b1,1
a1,2b1,2
a1,1b2,1
a1,1b2,2
a1,2b2,1
a1,2b2,2
a2,1b1,1
a2,1b1,2
a2,2b1,1
a2,2b1,2
a2,1b2,1
a2,1b2,2
a2,2b2,1
a2,2b2,2  "
REFERENCES,0.26461211477151964,"|
{z
}
A⊗B ."
REFERENCES,0.26567481402763016,Deﬁnition 5 (Khatri–Rao product). The Khatri–Rao product is deﬁned as:
REFERENCES,0.2667375132837407,"A ∗B = (Ai,j ⊗Bi,j)i,j ,"
REFERENCES,0.2678002125398512,"in which the (i, j)-th block is the mipi × njqj sized Kronecker product of the corresponding blocks
of A and B, assuming the number of row and column partitions of both matrices is equal. The size
of the product is then (P"
REFERENCES,0.2688629117959617,i mipi) × (P
REFERENCES,0.26992561105207225,i njqj).
REFERENCES,0.27098831030818277,Example: if A and B both are 2 × 2 partitioned matrices e.g.:
REFERENCES,0.2720510095642933,"A =

A1,1
A1,2
A2,1
A2,2 
= "
REFERENCES,0.2731137088204038,"
a1,1
a1,2
a1,3
a2,1
a2,2
a2,3
a3,1
a3,2
a3,3  ,"
REFERENCES,0.27417640807651433,Published as a conference paper at ICLR 2022
REFERENCES,0.27523910733262485,"B =

B1,1
B1,2
B2,1
B2,2 
= "
REFERENCES,0.2763018065887354,"
b1,1
b1,2
b1,3
b2,1
b2,2
b2,3
b3,1
b3,2
b3,3  ,"
REFERENCES,0.2773645058448459,then we obtain the following:
REFERENCES,0.2784272051009564,"A ∗B =

A1,1 ⊗B1,1
A1,2 ⊗B1,2
A2,1 ⊗B2,1
A2,2 ⊗B2,2 
=  "
REFERENCES,0.27948990435706694,"a1,1b1,1
a1,2b1,1
a1,3b1,2
a1,3b1,3
a2,1b1,1
a2,2b1,1
a2,3b1,2
a2,3b1,3
a3,1b2,1
a3,2b2,1
a3,3b2,2
a3,3b2,3
a3,1b3,1
a3,2b3,1
a3,3b3,2
a3,3b3,3  ."
REFERENCES,0.28055260361317746,"Deﬁnition 6 (Column-wise Khatri–Rao product). A column-wise Kronecker product of two matrices
may also be called the Khatri–Rao product. This product assumes the partitions of the matrices are
their columns. In this case m1 = m, p1 = p, n = q and for each j: nj = pj = 1. The resulting
product is a mp × n matrix of which each column is the Kronecker product of the corresponding
columns of A and B."
REFERENCES,0.281615302869288,"Example: the Column-wise Khatri–Rao product of the matrices A ∈R2×3 and B ∈R3×3 is
computed below:"
REFERENCES,0.2826780021253985,"
a1,1
a1,2
a1,3
a2,1
a2,2
a2,3 "
REFERENCES,0.28374070138150903,"|
{z
}
A ∗"
REFERENCES,0.28480340063761955,"""b1,1
b1,2
b1,3
b2,1
b2,2
b2,3
b3,1
b3,2
b3,3 #"
REFERENCES,0.2858660998937301,"|
{z
}
B = "
REFERENCES,0.2869287991498406,
REFERENCES,0.2879914984059511,"a1,1b1,1
a1,2b1,2
a1,3b1,3
a1,1b2,1
a1,2b2,2
a1,3b2,3
a1,1b3,1
a1,2b3,2
a1,3b3,3
a2,1b1,1
a2,2b1,2
a2,3b1,3
a2,1b2,1
a2,2b2,2
a2,3b2,3
a2,1b3,1
a2,2b3,2
a2,3b3,3 "
REFERENCES,0.28905419766206164,
REFERENCES,0.29011689691817216,"|
{z
}
A∗B ."
REFERENCES,0.2911795961742827,"From here on, all ∗refer to Column-wise Khatri–Rao product."
REFERENCES,0.2922422954303932,"Deﬁnition 7 (Face-splitting product). The alternative concept of the matrix product, which uses
row-wise splitting of matrices with a given quantity of rows. This matrix operation was named the
face-splitting product of matrices or the transposed Khatri–Rao product. This type of operation is
based on row-by-row Kronecker products of two matrices."
REFERENCES,0.29330499468650373,Example: the Face-splitting product of the matrices A ∈R3×2 and B ∈R3×2 is computed below:
REFERENCES,0.29436769394261425,"""a1,1
a1,2
a2,1
a3,2
a3,1
a3,2 #"
REFERENCES,0.29543039319872477,"|
{z
}
A •"
REFERENCES,0.2964930924548353,"""b1,1
b1,2
b2,1
b2,2
b3,1
b3,2 #"
REFERENCES,0.2975557917109458,"|
{z
}
B ="
REFERENCES,0.29861849096705634,"""a1,1b1,1
a1,2b1,1
a1,1b1,2
a1,2b1,2
a2,1b2,1
a2,2b2,1
a2,1b2,2
a2,2b2,2
a3,1b3,1
a3,2b3,1
a3,1b3,2
a3,2b3,2 #"
REFERENCES,0.29968119022316686,"|
{z
}
A•B ."
REFERENCES,0.3007438894792774,"C.3
WELL-KNOWN LEMMAS"
REFERENCES,0.3018065887353879,"In this section, we provide the details on the Lemmas required for our proofs along with their proofs
or the corresponding citations where the Lemmas can be found as well."
REFERENCES,0.3028692879914984,"Lemma 3. (Federer, 2014) Let g, h be two composable Lipschitz functions. Then g ◦h is also
Lipschitz with Lip(g ◦h) ≤Lip(g)Lip(h). Here and only here ◦represents function composition."
REFERENCES,0.30393198724760895,"Lemma 4. (Federer, 2014) Let f : X ⊆Rn →Rm be differentiable and Lipschitz continuous.
Let Jf(x) denote its total derivative (Jacobian) at x. Then Lipp(f) = supx∈X ∥Jf(x)∥p where
∥Jf(x)∥p is the induced operator norm on Jf(x)."
REFERENCES,0.30499468650371947,"Lemma 5 (Hölder’s inequality). (Cvetkovski, 2012) Let (S, Σ, µ) be a measure space and let
p, q ∈[1, ∞] with 1 p + 1"
REFERENCES,0.30605738575983,"q = 1. Then, for all measurable real-valued functions f and g on S, it
holds that:
∥fg∥1 ≤∥f∥p ∥g∥q ."
REFERENCES,0.3071200850159405,Published as a conference paper at ICLR 2022
REFERENCES,0.30818278427205104,"Lemma 6 (Mixed Product Property 1). (Slyusar, 1999) The following holds:"
REFERENCES,0.30924548352816156,(A1B1) ◦(A2B2) = (A1 • A2)(B1 ∗B2) .
REFERENCES,0.310308182784272,Lemma 7 (Mixed Product Property 2). The following holds:
REFERENCES,0.31137088204038255,"◦N
i=1(AiBi) = •N
i=1(Ai) ∗N
i=1 (Bi) ."
REFERENCES,0.31243358129649307,Proof. We prove this lemma by induction on N.
REFERENCES,0.3134962805526036,Base case (N = 1): A1B1 = A1B1.
REFERENCES,0.3145589798087141,"Inductive step: Assume that the induction hypothesis holds for a particular k, i.e., the case N = k
holds. That can be expressed as:"
REFERENCES,0.31562167906482463,"◦k
i=1(AiBi) = •k
i=1(Ai) ∗k
i=1 (Bi) .
(3)"
REFERENCES,0.31668437832093516,Then we will prove that it holds for N = k + 1:
REFERENCES,0.3177470775770457,"◦k+1
i=1 (AiBi)"
REFERENCES,0.3188097768331562,"= [◦k
i=1(AiBi)] ◦(Ak+1Bk+1)"
REFERENCES,0.3198724760892667,"= [•k
i=1(Ai) ∗k
i=1 (Bi)] ◦(Ak+1Bk+1)
use inductive hypothesis (Eq. (3))"
REFERENCES,0.32093517534537724,"= [•k
i=1(Ai) • Ak+1][∗k
i=1(Bi) ∗Bk+1]
Lemma 6 [Mixed product property 1]"
REFERENCES,0.32199787460148777,"= •k+1
i=1 (Ai) ∗k+1
i=1 (Bi) ."
REFERENCES,0.3230605738575983,"That is, the case N = k + 1 also holds true, establishing the inductive step."
REFERENCES,0.3241232731137088,"Lemma 8 (Massart Lemma. Lemma 26.8 in Shalev-Shwartz and Ben-David (2014)). Let A
={a1, · · · , aN} be a ﬁnite set of vectors in Rm. Deﬁne ¯a = 1"
REFERENCES,0.32518597236981933,"N
PN
i=1 ai. Then:"
REFERENCES,0.32624867162592985,"R(A) ≤max
a∈A ∥a −¯a∥
√2 log N m
."
REFERENCES,0.3273113708820404,"Deﬁnition 8 (Consistency of a matrix norm). A matrix norm is called consistent on Cn,n, if"
REFERENCES,0.3283740701381509,∥AB∥≤∥A∥∥B∥.
REFERENCES,0.3294367693942614,"holds for A, B ∈Cn,n.
Lemma 9 (Consistency of the operator norm). (Lyche, 2020) The operator norm is consistent if the
vector norm ∥·∥α is deﬁned for all m ∈N and ∥·∥β = ∥·∥α"
REFERENCES,0.33049946865037194,Proof.
REFERENCES,0.33156216790648246,"∥AB∥= max
Bx̸=0
∥ABx∥α"
REFERENCES,0.332624867162593,"∥x∥α
= max
Bx̸=0
∥ABx∥α ∥Bx∥α ∥Bx∥α ∥x∥α"
REFERENCES,0.3336875664187035,"≤max
y̸=0
∥Ay∥α"
REFERENCES,0.33475026567481403,"∥y∥α
max
x̸=0
∥Bx∥α"
REFERENCES,0.33581296493092455,"∥x∥α
= ∥A∥∥B∥."
REFERENCES,0.3368756641870351,"Lemma 10. (Rao, 1970)
(AC) ∗(BD) = (A ⊗B)(C ∗D) ."
REFERENCES,0.3379383634431456,"D
DETAILS ON POLYNOMIAL NETWORKS"
REFERENCES,0.3390010626992561,"In this section, we provide further details on the two most prominent parametrizations proposed in
Chrysos et al. (2020). This re-parametrization creates equivalent models, but enables us to absorb
the bias terms into the input terms. Firstly, we provide the re-parametrization of the CCP model in
Appendix D.1 and then we create the re-parametrization of the NCP model in Appendix D.2."
REFERENCES,0.34006376195536664,Published as a conference paper at ICLR 2022
REFERENCES,0.34112646121147716,"D.1
RE-PARAMETRIZATION OF CCP MODEL"
REFERENCES,0.3421891604675877,"The Coupled CP-Decomposition (CCP) model of PNs (Chrysos et al., 2020) leverages a coupled CP
Tensor decomposition. A k-degree CCP model f(ζ) can be succinctly described by the following
recursive relations:
y1 = V1ζ,
yn = (Vnζ) ◦yn−1 + yn−1,
f(ζ) = Qyk + β ,
(4)
where ζ ∈Rδ is the input data with δ ∈N, f(ζ) ∈Ro is the output of the model and Vn ∈Rµ×δ,
Q ∈Ro×µ and β ∈Ro are the learnable parameters, where µ ∈N is the hidden rank. In order to
simplify the bias terms in the model, we will introduce a minor re-parametrization in Lemma 11 that
we will use to present our results in the subsequent sections.
Lemma 11. Let z = [ζ⊤, 1]⊤∈Rd, xn = [y⊤
n , 1]⊤∈Rm, C = [Q, β] ∈Ro×m, d = δ + 1, m =
µ + 1. Deﬁne:"
REFERENCES,0.3432518597236982,"U1 =
V1
0
0⊤
1"
REFERENCES,0.3443145589798087,"
∈Rm×d,
Ui =
Vi
1
0⊤
1"
REFERENCES,0.34537725823591925,"
∈Rm×d
(i > 1) ."
REFERENCES,0.34643995749202977,"where the boldface numbers 0 and 1 denote all-zeros and all-ones column vectors of appropriate
size, respectively. The CCP model in Eq. (4) can be rewritten as f(z) = C ◦k
i=1 Uiz, which is the
one used in Eq. (CCP)."
REFERENCES,0.3475026567481403,"As a reminder before providing the proof, the core symbols for this proof are summarized in Table 6."
REFERENCES,0.3485653560042508,Table 6: Core symbols in the proof of Lemma 11.
REFERENCES,0.34962805526036134,"Symbol
Dimensions
Deﬁnition
◦
-
Hadamard (element-wise) product.
ζ
Rδ
Input of the polynomial expansion.
f(ζ)
Ro
Output of the polynomial expansion.
k
N
Degree of polynomial expansion.
m
N
Hidden rank of the expansion.
Vn
Rµ×δ
Learnable matrices of the expansion.
Q
Ro×µ
Learnable matrix of the expansion.
β
Ro
Bias of the expansion.
z
Rd
Re-parametrization of the input.
C
Ro×m
C = (Q, β)."
REFERENCES,0.35069075451647186,"Proof. By deﬁnition, we have:"
REFERENCES,0.3517534537725824,"x1 = [y⊤
1 , 1]⊤=

y1
1"
REFERENCES,0.3528161530286929,"
=

V1ζ
1"
REFERENCES,0.3538788522848034,"
=
V1
0
0⊤
1"
REFERENCES,0.35494155154091395," 
ζ
1"
REFERENCES,0.35600425079702447,"
=
V1
0
0⊤
1"
REFERENCES,0.357066950053135,"
[ζ⊤, 1]⊤= U1z ."
REFERENCES,0.35812964930924546,"xn = [y⊤
n , 1]⊤=

yn
1"
REFERENCES,0.359192348565356,"
=

(Vnζ) ◦yn−1 + yn−1
1"
REFERENCES,0.3602550478214665,"
=

Vnζ + 1
1"
REFERENCES,0.361317747077577,"
◦

yn−1
1 "
REFERENCES,0.36238044633368754,"=
Vn
1
0⊤
1"
REFERENCES,0.36344314558979807," 
ζ
1"
REFERENCES,0.3645058448459086,"
◦

yn−1
1"
REFERENCES,0.3655685441020191,"
=
Vn
1
0⊤
1"
REFERENCES,0.36663124335812963,"
[ζ⊤, 1]⊤◦[y⊤
n−1, 1]⊤= Unz ◦xn−1 ."
REFERENCES,0.36769394261424015,"Hence, it holds that:"
REFERENCES,0.3687566418703507,"f(z) = Qyk + β = (Q, β)

yk
1"
REFERENCES,0.3698193411264612,"
= Cxk"
REFERENCES,0.3708820403825717,"= C Ukz ◦xk−1
= C Ukz ◦(Uk−1z) ◦xk−2
= · · ·
= C Ukz ◦(Uk−1z) ◦· · · ◦(U2z) ◦x1
= C Ukz ◦(Uk−1z) ◦· · · ◦(U2z) ◦(U1z)"
REFERENCES,0.37194473963868224,"= C ◦k
i=1 (Uiz) ."
REFERENCES,0.37300743889479276,Published as a conference paper at ICLR 2022
REFERENCES,0.3740701381509033,"D.2
REPARAMETRIZATION OF THE NCP MODEL"
REFERENCES,0.3751328374070138,"The nested coupled CP decomposition (NCP) model of PNs (Chrysos et al., 2020) leverages a joint
hierarchical decomposition. A k-degree NCP model f(ζ) is expressed with the following recursive
relations:"
REFERENCES,0.37619553666312433,"y1 = (V1ζ) ◦(b1),
yn = (Vnζ) ◦(Unyn−1 + bn),
f(ζ) = Qyk + β .
(5)"
REFERENCES,0.37725823591923485,"where ζ ∈Rδ is the input data with δ ∈N, f(ζ) ∈Ro is the output of the model and Vn ∈Rµ×δ,
bn ∈Rµ, Un ∈Rµ×µ, Q ∈Ro×µ and β ∈Ro are the learnable parameters, where µ ∈N
is the hidden rank. In order to simplify the bias terms in the model, we will introduce a minor
re-parametrization in Lemma 12 that we will use to present our results in the subsequent sections."
REFERENCES,0.3783209351753454,"Lemma 12. Let z = [ζ⊤, 1]⊤∈Rd, xn = [y⊤
n , 1]⊤∈Rm, C = [Q, β] ∈Ro×m, d = δ + 1, m =
µ + 1. Let:"
REFERENCES,0.3793836344314559,"s1 = [b⊤
1 , 1]⊤∈Rm,
Si =
Ui
bi
0⊤
1"
REFERENCES,0.3804463336875664,"
∈Rm×m(i > 1),
Ai =
Vi
0
0⊤
1"
REFERENCES,0.38150903294367694,"
∈Rm×d ."
REFERENCES,0.38257173219978746,"where the boldface numbers 0 and 1 denote all-zeros and all-ones column vectors of appropriate
size, respectively. The NCP model in Eq. (5) can be rewritten as"
REFERENCES,0.383634431455898,"x1 = (A1z) ◦(s1),
xn = (Anz) ◦(Snxn−1),
f(z) = Cxk .
(6)"
REFERENCES,0.3846971307120085,"In the aforementioned Eq. (6), we have written Sn even for n = 1, when s1 is technically a vector,
but this is done for convenience only and does not change the end result."
REFERENCES,0.38575982996811903,"E
RESULT OF THE CCP MODEL"
REFERENCES,0.38682252922422955,"E.1
PROOF OF THEOREM 1: RADEMACHER COMPLEXITY BOUND OF CCP UNDER ℓ∞NORM"
REFERENCES,0.38788522848034007,"To facilitate the proof below, we include the related symbols in Table 7. Below, to avoid cluttering
the notation, we consider that the expectation is over σ and omit the brackets as well."
REFERENCES,0.3889479277364506,Table 7: Core symbols for proof of Theorem 1.
REFERENCES,0.3900106269925611,"Symbol
Dimensions
Deﬁnition
◦
-
Hadamard (element-wise) product.
•
-
Face-splitting product.
∗
-
Column-wise Khatri–Rao product.
z
Rd
Input of the polynomial expansion.
f(z)
R
Output of the polynomial expansion.
k
N
Degree of polynomial expansion.
m
N
Hidden rank of the expansion.
Ui
Rm×d
Learnable matrices.
c
R1×m
Learnable matrix.
µ
R
∥c∥1 ≤µ.
λ
R
•k
i=1(Ui)

∞≤λ."
REFERENCES,0.39107332624867164,Published as a conference paper at ICLR 2022
REFERENCES,0.39213602550478216,Proof.
REFERENCES,0.3931987247608927,"RZ(Fk
CCP) = E sup
f∈Fk
CCP"
N,0.3942614240170032,"1
n n
X"
N,0.3953241232731137,"j=1
σjf(zj)"
N,0.39638682252922425,"= E sup
f∈Fk
CCP"
N,0.39744952178533477,"1
n n
X j=1"
N,0.3985122210414453," 
σj

c, ◦k
i=1(Uizj)
"
N,0.3995749202975558,"= E sup
f∈Fk
CCP"
N,0.40063761955366634,"1
n * c, n
X"
N,0.40170031880977686,"j=1
[σj◦k
i=1(Uizj)] +"
N,0.4027630180658874,"≤E sup
f∈Fk
CCP"
N,0.4038257173219979,"1
n ∥c∥1  n
X"
N,0.40488841657810837,"j=1
[σj◦k
i=1(Uizj)] ∞"
N,0.4059511158342189,Lemma 5 [Hölder’s inequality]
N,0.4070138150903294,"= E sup
f∈Fk
CCP"
N,0.40807651434643993,"1
n ∥c∥1  n
X"
N,0.40913921360255046,"j=1
[σj •k
i=1 (Ui) ∗k
i=1 (zj)] ∞"
N,0.410201912858661,Lemma 7 [Mixed product property]
N,0.4112646121147715,"= E sup
f∈Fk
CCP"
N,0.412327311370882,"1
n ∥c∥1"
N,0.41339001062699254,"•k
i=1(Ui) n
X"
N,0.41445270988310307,"j=1
[σj ∗k
i=1 (zj)] ∞"
N,0.4155154091392136,"≤E sup
f∈Fk
CCP"
N,0.4165781083953241,"1
n ∥c∥1  n
X"
N,0.41764080765143463,"j=1
[σj ∗k
i=1 (zj)] ∞"
N,0.41870350690754515,"•k
i=1(Ui)

∞ ≤µλ n E  n
X"
N,0.4197662061636557,"j=1
[σj ∗k
i=1 (zj)] ∞ . (7)"
N,0.4208289054197662,"Next, we compute the bound of E
Pn
j=1[σj ∗k
i=1 (zj)]

∞."
N,0.4218916046758767,"Let Zj = ∗k
i=1(zj) ∈Rdk. For each l ∈[dk], let vl = (Zl
1, . . . , Zl
n) ∈Rn. Note that ∥vl∥2 ≤
√n maxj ∥Zj∥∞. Let V = {v1, . . . , vdk}. Then, it is true that: E  n
X"
N,0.42295430393198724,"j=1
[σj ∗k
i=1 (zj)] ∞ = E  n
X"
N,0.42401700318809776,"j=1
σjZj ∞"
N,0.4250797024442083,"= E
dk
max
l=1  n
X"
N,0.4261424017003188,"j=1
σj(vl)j"
N,0.42720510095642933,"= nR(V ) .
(8)"
N,0.42826780021253985,Using Lemma 8 [Massart Lemma] we have that:
N,0.4293304994686504,"R(V ) ≤2 max
j
∥Zj∥∞ q"
N,0.4303931987247609,"2 log (dk)/n .
(9)"
N,0.4314558979808714,Published as a conference paper at ICLR 2022
N,0.43251859723698194,"Then, it holds that:"
N,0.43358129649309246,"RZ(Fk
CCP) = E sup
f∈Fk
CCP"
N,0.434643995749203,"1
n n
X"
N,0.4357066950053135,"j=1
σjf(zj) ≤µλ n E  n
X"
N,0.436769394261424,"j=1
[σj ∗k
i=1 (zj)] ∞"
N,0.43783209351753455,Eq. (7) = µλ
N,0.43889479277364507,"n nR(V )
Eq. (8)"
N,0.4399574920297556,"≤2µλ max
j
∥Zj∥∞ q"
N,0.4410201912858661,"2 log(dk)/n
Eq. (9)"
N,0.44208289054197664,"= 2µλ max
j
∗k
i=1(zj)

∞ q"
N,0.44314558979808716,2 log (dk)/n
N,0.4442082890541977,"≤2µλ(max
j
∥zj∥∞)k
q"
N,0.4452709883103082,2 log (dk)/n
N,0.4463336875664187,"≤2µλ
p"
N,0.44739638682252925,2k log (d)/n . (10)
N,0.44845908607863977,"E.2
PROOF OF LEMMA 1"
N,0.4495217853347503,Table 8: Core symbols in the proof of Lemma 1.
N,0.4505844845908608,"Symbol
Dimensions
Deﬁnition
⊗
-
Kronecker product.
•
-
Face-splitting product.
z
Rd
Input of the polynomial expansion.
f(z)
R
Output of the polynomial expansion.
k
N
Degree of polynomial expansion.
m
N
Hidden rank of the expansion.
Ui
Rm×d
Learnable matrices.
U j
i
Rd
jth row of Ui.
λi
R
∥Ui∥∞≤λi for i = 1, 2, . . . , k."
N,0.45164718384697133,"Proof.
•k
i=1(Ui)

∞=
m
max
j=1
[•k
i=1(Ui)]j
1"
N,0.4527098831030818,"=
m
max
j=1"
N,0.4537725823591923,"⊗k
i=1[U j
i ]

1
Deﬁnition of Face-splitting product"
N,0.45483528161530284,"=
m
max
j=1 "" k
Y i=1"
N,0.45589798087141337,"U j
i

1 #"
N,0.4569606801275239,"Multiplicativity of absolute value ≤ k
Y i=1"
N,0.4580233793836344,"
m
max
j=1"
N,0.45908607863974493,"U j
i

1  = k
Y"
N,0.46014877789585545,"i=1
∥Ui∥∞."
N,0.461211477151966,"E.3
RADEMACHER COMPLEXITY BOUND UNDER ℓ2 NORM"
N,0.4622741764080765,"Theorem 6. Let Z = {z1, . . . , zn} ⊆Rd and suppose that ∥zj∥∞≤1 for all j = 1, . . . , n. Let
Fk
CCP :=

f(z) =

c, ◦k
i=1Uiz

: ∥c∥2 ≤µ,
•k
i=1Ui

2 ≤λ
	
."
N,0.463336875664187,Published as a conference paper at ICLR 2022
N,0.46439957492029754,"The Empirical Rademacher Complexity of CCPk (k-degree CCP polynomials) with respect to Z is
bounded as:"
N,0.46546227417640806,"RZ(Fk
CCP) ≤µλ
√n ."
N,0.4665249734325186,"To facilitate the proof below, we include the related symbols in Table 9. Below, to avoid cluttering
the notation, we consider that the expectation is over σ and omit the brackets as well."
N,0.4675876726886291,Table 9: Core symbols for proof of Theorem 6.
N,0.46865037194473963,"Symbol
Dimensions
Deﬁnition
◦
-
Hadamard (element-wise) product.
•
-
Face-splitting product.
∗
-
Column-wise Khatri–Rao product.
z
Rd
Input of the polynomial expansion.
f(z)
R
Output of the polynomial expansion.
k
N
Degree of polynomial expansion.
m
N
Hidden rank of the expansion.
Ui
Rm×d
Learnable matrices.
c
R1×m
Learnable matrix.
µ
R
∥c∥2 ≤µ.
λ
R
•k
i=1(Ui)

2 ≤λ."
N,0.46971307120085015,Proof.
N,0.4707757704569607,"RZ(Fk
CCP) = E sup
f∈Fk
CCP"
N,0.4718384697130712,"1
n n
X"
N,0.4729011689691817,"j=1
σjf(zj)"
N,0.47396386822529224,"= E sup
f∈Fk
CCP"
N,0.47502656748140276,"1
n n
X j=1"
N,0.4760892667375133," 
σj

c, ◦k
i=1(Uizj)
"
N,0.4771519659936238,"= E sup
f∈Fk
CCP"
N,0.4782146652497343,"1
n * c, n
X"
N,0.47927736450584485,"j=1
[σj◦k
i=1(Uizj)] +"
N,0.48034006376195537,"≤E sup
f∈Fk
CCP"
N,0.4814027630180659,"1
n ∥c∥2  n
X"
N,0.4824654622741764,"j=1
[σj◦k
i=1(Uizj)] 2"
N,0.48352816153028694,Lemma 5 [Hölder’s inequality]
N,0.48459086078639746,"= E sup
f∈Fk
CCP"
N,0.485653560042508,"1
n ∥c∥2  n
X"
N,0.4867162592986185,"j=1
[σj •k
i=1 (Ui) ∗k
i=1 (zj)] 2"
N,0.487778958554729,Lemma 7 [Mixed product property]
N,0.48884165781083955,"= E sup
f∈Fk
CCP"
N,0.48990435706695007,"1
n ∥c∥2"
N,0.4909670563230606,"•k
i=1(Ui) n
X"
N,0.4920297555791711,"j=1
[σj ∗k
i=1 (zj)] 2"
N,0.49309245483528164,"≤E sup
f∈Fk
CCP"
N,0.49415515409139216,"1
n ∥c∥2  n
X"
N,0.4952178533475027,"j=1
[σj ∗k
i=1 (zj)] 2"
N,0.4962805526036132,"•k
i=1(Ui)

2 ."
N,0.4973432518597237,"Published as a conference paper at ICLR 2022 E  n
X"
N,0.49840595111583424,"j=1
[σj ∗k
i=1 (zj)] 2 = E"
N,0.49946865037194477,"v
u
u
u
t  n
X"
N,0.5005313496280552,"j=1
[σj ∗k
i=1 (zj)]  2 2 ≤"
N,0.5015940488841658,"v
u
u
u
tE  n
X"
N,0.5026567481402763,"j=1
[σj ∗k
i=1 (zj)]  2 2"
N,0.5037194473963869,Jensen’s inequality =
N,0.5047821466524973,"v
u
u
tE n
X"
N,0.5058448459086079,"s,j
[σsσj

∗k
i=1(zs), ∗k
i=1(zj)

] ="
N,0.5069075451647184,"v
u
u
t n
X"
N,0.5079702444208289,"j=1
[
∗k
i=1(zj)
2
2] ="
N,0.5090329436769394,"v
u
u
t n
X j=1
( k
Y"
N,0.51009564293305,"i=1
∥zj∥2
2) ≤√n . (11) So:"
N,0.5111583421891605,"RZ(Fk
CCP) ≤E sup
f∈Fk
CCP"
N,0.512221041445271,"1
n ∥c∥2  n
X"
N,0.5132837407013815,"j=1
[σj ∗k
i=1 (zj)] 2"
N,0.5143464399574921,"•k
i=1(Ui)

2"
N,0.5154091392136025,"≤
supf∈Fk
CCP ∥c∥2
•k
i=1(Ui)

2
√n"
N,0.5164718384697131,"≤µλ
√n ."
N,0.5175345377258236,"E.4
LIPSCHITZ CONSTANT BOUND OF THE CCP MODEL"
N,0.5185972369819342,"We will ﬁrst prove a more general result about the ℓp-Lipschitz constant of the CCP model.
Theorem 7. The Lipschitz constant (with respect to the ℓp-norm) of the function deﬁned in Eq. (CCP),
restricted to the set {z ∈Rd : ∥z∥p ≤1} is bounded as:"
N,0.5196599362380446,"Lipp(f) ≤k∥C∥p k
Y"
N,0.5207226354941552,"i=1
∥Ui∥p ."
N,0.5217853347502657,"Proof. Let g(x) = Cx and h(z) = ◦k
i=1(Uiz). Then it holds that f(z) = g(h(z)). By Lemma 3,
we have: Lipp(f) ≤Lipp(g)Lipp(h). We will compute an upper bound of each function individually."
N,0.5228480340063762,"Let us ﬁrst consider the function g(x) = Cx. By Lemma 4, because g is a linear map represented by
a matrix C, its Jacobian is Jg(x) = C. So:"
N,0.5239107332624867,"Lipp(g) = ∥C∥p :=
sup
∥x∥p=1
∥Cx∥p ."
N,0.5249734325185972,where ∥C∥p is the operator norm on matrices induced by the vector p-norm.
N,0.5260361317747078,"Now, let us consider the function h(z) = ◦k
i=1Uiz. Its Jacobian is given by:"
N,0.5270988310308182,"dh
dz = k
X"
N,0.5281615302869288,"i=1
diag(◦j̸=iUjz)Ui ."
N,0.5292242295430393,Published as a conference paper at ICLR 2022
N,0.5302869287991498,Using Lemma 4 we have:
N,0.5313496280552603,"Lipp(h) ≤
sup
z:∥z∥p≤1  k
X"
N,0.5324123273113709,"i=1
[diag(◦j̸=i(Ujz))Ui] p"
N,0.5334750265674814,"≤
sup
z:∥z∥p≤1 k
X"
N,0.5345377258235919,"i=1
∥diag(◦j̸=i(Ujz))Ui∥p
Triangle inequality"
N,0.5356004250797024,"≤
sup
z:∥z∥p≤1 k
X"
N,0.536663124335813,"i=1
∥diag(◦j̸=i(Ujz))∥p ∥Ui∥p
Lemma 9 [consistency]"
N,0.5377258235919234,"≤
sup
z:∥z∥p≤1 k
X"
N,0.538788522848034,"i=1
∥◦j̸=i(Ujz)∥p ∥Ui∥p"
N,0.5398512221041445,"≤
sup
z:∥z∥p≤1 k
X i=1 Y"
N,0.5409139213602551,"j̸=i
(∥Ujz∥p) ∥Ui∥p"
N,0.5419766206163655,"≤
sup
z:∥z∥p≤1 k
X i=1 Y"
N,0.5430393198724761,"j̸=i
(∥Uj∥p ∥z∥p) ∥Ui∥p"
N,0.5441020191285866,"≤
sup
z:∥z∥p≤1 k
X i=1 k
Y"
N,0.5451647183846972,"j=1
(∥Uj∥p) = k k
Y"
N,0.5462274176408076,"j=1
∥Uj∥p . So:"
N,0.5472901168969182,Lipp(FL) ≤Lipp(g)Lipp(h)
N,0.5483528161530287,"≤k∥C∥p k
Y"
N,0.5494155154091392,"i=1
∥Ui∥p ."
N,0.5504782146652497,"E.4.1
PROOF OF THEOREM 2"
N,0.5515409139213603,Proof. This is a particular case of Theorem 7 when p = ∞.
N,0.5526036131774708,Published as a conference paper at ICLR 2022
N,0.5536663124335813,"F
RESULT OF THE NCP MODEL"
N,0.5547290116896918,"F.1
PROOF OF THEOREM 3: RADEMACHER COMPLEXITY OF NCP UNDER ℓ∞NORM"
N,0.5557917109458024,Proof.
N,0.5568544102019128,"RZ(Fk
NCP) = E sup
f∈Fk
NCP"
N,0.5579171094580234,"1
n n
X"
N,0.5589798087141339,"j=1
σjf(zj)"
N,0.5600425079702445,"= E sup
f∈Fk
NCP"
N,0.5611052072263549,"1
n n
X"
N,0.5621679064824655,"j=1
(σj ⟨c, xk(zj)⟩)"
N,0.563230605738576,"= E sup
f∈Fk
NCP"
N,0.5642933049946866,"1
n * c, n
X"
N,0.565356004250797,"j=1
[σjxk(zj)] +"
N,0.5664187035069076,"≤E sup
f∈Fk
NCP"
N,0.5674814027630181,"1
n ∥c∥1  n
X"
N,0.5685441020191286,"j=1
[σjxk(zj)] ∞"
N,0.5696068012752391,Lemma 5 [Hölder’s inequality]
N,0.5706695005313497,"= E sup
f∈Fk
NCP"
N,0.5717321997874601,"1
n ∥c∥1  n
X"
N,0.5727948990435706,"j=1
[σj((Akzj) ◦(Skxk−1(zj)))] ∞"
N,0.5738575982996812,"= E sup
f∈Fk
NCP"
N,0.5749202975557917,"1
n ∥c∥1  n
X"
N,0.5759829968119022,"j=1
[σj((Ak • Sk)(zj ∗xk−1(zj)))] ∞"
N,0.5770456960680127,Lemma 7 [Mixed product property]
N,0.5781083953241233,"= E sup
f∈Fk
NCP"
N,0.5791710945802337,"1
n ∥c∥1"
N,0.5802337938363443,"(Ak • Sk) n
X"
N,0.5812964930924548,"j=1
[σj(zj ∗xk−1(zj))] ∞ . (12)"
N,0.5823591923485654,"Now, because of the recursive deﬁnition of the Eq. (NCP), we obtain: n
X"
N,0.5834218916046758,"j=1
σj(zj ∗xk−1(zj)) = n
X"
N,0.5844845908607864,"j=1
σj(zj ∗(Ak−1zj) ◦(Sk−1xk−2(zj))) = n
X"
N,0.5855472901168969,"j=1
σj(zj ∗((Ak−1 • Sk−1)(zj ∗xk−2(zj))))
Lemma 7 = n
X"
N,0.5866099893730075,"j=1
σj(I ⊗(Ak−1 • Sk−1))(zj ∗(zj ∗xk−2(zj)))
Lemma 10"
N,0.5876726886291179,"= I ⊗(Ak−1 • Sk−1) n
X"
N,0.5887353878852285,"j=1
[σj(zj ∗(zj ∗xk−2(zj))) . (13)"
N,0.589798087141339,"recursively applying this argument we have: n
X"
N,0.5908607863974495,"j=1
σj(zj ∗xk−1(zj)) = k−1
Y"
N,0.59192348565356,"i=1
I ⊗Ai • Si !
n
X"
N,0.5929861849096706,"j=1
σj ∗k
i=1 (zj) .
(14)"
N,0.594048884165781,Combining the two previous equations (Eqs. (13) and (14)) inside Eq. (12) we ﬁnally obtain
N,0.5951115834218916,Published as a conference paper at ICLR 2022
N,0.5961742826780021,"RZ(Fk
NCP) ≤E sup
f∈Fk
NCP"
N,0.5972369819341127,"1
n ∥c∥1"
N,0.5982996811902231,"(Ak • Sk) k−1
Y"
N,0.5993623804463337,"i=1
I ⊗Ai • Si !
n
X"
N,0.6004250797024442,"j=1
σj ∗k
i=1 (zj) ∞"
N,0.6014877789585548,"≤E sup
f∈Fk
NCP"
N,0.6025504782146652,"1
n ∥c∥1"
N,0.6036131774707758,"(Ak • Sk) k−1
Y"
N,0.6046758767268863,"i=1
I ⊗Ai • Si"
N,0.6057385759829969,"!
∞  n
X"
N,0.6068012752391073,"j=1
σj ∗k
i=1 (zj) ∞ ≤µλ n E  n
X"
N,0.6078639744952179,"j=1
σj ∗k
i=1 (zj) ∞ = µλ"
N,0.6089266737513284,"n nR(V ) = µλR(V ) .
Eq. (8) ."
N,0.6099893730074389,following the same arguments as in Eq. (10) it follows that:
N,0.6110520722635494,"RZ(Fk
NCP) ≤2µλ r"
N,0.61211477151966,"2k log (d) n
."
N,0.6131774707757705,"F.2
PROOF OF LEMMA 2"
N,0.614240170031881,Proof.
N,0.6153028692879915,"(Ak • Sk) k−1
Y"
N,0.6163655685441021,"i=1
I ⊗Ai • Si"
N,0.6174282678002125,"∞
≤∥Ak • Sk∥∞ k−1
Y"
N,0.6184909670563231,"i=1
∥I ⊗Ai • Si∥∞
Lemma 9 [consistent] = k
Y"
N,0.6195536663124336,"i=1
∥Ai • Si∥∞ ≤ k
Y"
N,0.620616365568544,"i=1
∥Ai∥∞∥Si∥∞
Lemma 1 ."
N,0.6216790648246546,"F.3
RADEMACHER COMPLEXITY UNDER ℓ2 NORM"
N,0.6227417640807651,"Theorem 8. Let Z = {z1, . . . , zn} ⊆Rd and suppose that ∥zj∥∞≤1 for all j = 1, . . . , n. Deﬁne
the matrix Φ(A1, S1, . . . , An, Sn) := (Ak •Sk) Qk−1
i=1 I ⊗Ai •Si. Consider the class of functions:"
N,0.6238044633368757,"Fk
NCP := {f(z) as in (NCP) : ∥C∥2 ≤µ, ∥Φ(A1, S1, . . . , Ak, Sk)∥2 ≤λ} ,"
N,0.6248671625929861,"where C ∈R1×m (single output case). The Empirical Rademacher Complexity of NCPk (k-degree
NCP polynomials) with respect to Z is bounded as:"
N,0.6259298618490967,"RZ(Fk
NCP) ≤µλ
√n ."
N,0.6269925611052072,Published as a conference paper at ICLR 2022
N,0.6280552603613178,Proof.
N,0.6291179596174282,"RZ(Fk
NCP) = E sup
f∈Fk
NCP"
N,0.6301806588735388,"1
n n
X"
N,0.6312433581296493,"j=1
σjf(zj)"
N,0.6323060573857598,"= E sup
f∈Fk
NCP"
N,0.6333687566418703,"1
n n
X"
N,0.6344314558979809,"j=1
(σj ⟨c, xk(zj)⟩)"
N,0.6354941551540914,"= E sup
f∈Fk
NCP"
N,0.6365568544102019,"1
n * c, n
X"
N,0.6376195536663124,"j=1
[σjxk(zj)] +"
N,0.638682252922423,"≤E sup
f∈Fk
NCP"
N,0.6397449521785334,"1
n ∥c∥2  n
X"
N,0.640807651434644,"j=1
[σjxk(zj)] 2"
N,0.6418703506907545,Lemma 5 [Hölder’s inequality]
N,0.6429330499468651,"= E sup
f∈Fk
NCP"
N,0.6439957492029755,"1
n ∥c∥2  n
X"
N,0.6450584484590861,"j=1
[σj((Akzj) ◦(Skxk−1(zj)))] 2"
N,0.6461211477151966,"= E sup
f∈Fk
NCP"
N,0.6471838469713072,"1
n ∥c∥2  n
X"
N,0.6482465462274176,"j=1
[σj((Ak • Sk)(zj ∗xk−1(zj)))] 2"
N,0.6493092454835282,Lemma 7 [Mixed product property]
N,0.6503719447396387,"= E sup
f∈Fk
NCP"
N,0.6514346439957492,"1
n ∥c∥2"
N,0.6524973432518597,"(Ak • Sk) n
X"
N,0.6535600425079703,"j=1
[σj(zj ∗xk−1(zj))] 2"
N,0.6546227417640808,"= E sup
f∈Fk
NCP"
N,0.6556854410201913,"1
n ∥c∥2"
N,0.6567481402763018,"(Ak • Sk) k−1
Y"
N,0.6578108395324124,"i=1
I ⊗Ai • Si !
n
X"
N,0.6588735387885228,"j=1
σj ∗k
i=1 (zj)] 2"
N,0.6599362380446334,Eq. (14)
N,0.6609989373007439,"≤E sup
f∈Fk
NCP"
N,0.6620616365568545,"1
n ∥c∥2"
N,0.6631243358129649,"(Ak • Sk) k−1
Y"
N,0.6641870350690755,"i=1
I ⊗Ai • Si"
N,0.665249734325186,"!
2  n
X"
N,0.6663124335812965,"j=1
σj ∗k
i=1 (zj)] 2 ≤µλ n E  n
X"
N,0.667375132837407,"j=1
σj ∗k
i=1 (zj) 2"
N,0.6684378320935175,"≤µλ
√n .
Eq. (11) ."
N,0.6695005313496281,"F.4
LIPSCHITZ CONSTANT BOUND OF THE NCP MODEL"
N,0.6705632306057385,Theorem 9. Let FL be the class of functions deﬁned as
N,0.6716259298618491,"FL :=

x1 = (A1z) ◦(S1), xn = (Anz) ◦(Snxn−1), f(z) = Cxk :"
N,0.6726886291179596,"∥C∥p ≤µ, ∥Ai∥p ≤λi, ∥Si∥p ≤ρi, ∥z∥p ≤1

."
N,0.6737513283740701,The Lipschitz Constant of FL (k-degree NCP polynomial) under ℓp norm restrictions is bounded as:
N,0.6748140276301806,"Lipp(FL) ≤kµ k
Y"
N,0.6758767268862912,"i=1
(λiρi) ."
N,0.6769394261424017,"Proof. Let g(x) = Cx, h(z) = (Anz) ◦(Snxn−1(z)). Then it holds that f(z) = g(h(z))."
N,0.6780021253985122,Published as a conference paper at ICLR 2022
N,0.6790648246546227,"By Lemma 3, we have: Lip(f) ≤Lip(g)Lip(h). This enables us to compute an upper bound of each
function (i.e., g, h) individually."
N,0.6801275239107333,"Let us ﬁrst consider the function g(x) = Cx. By Lemma 4, because g is a linear map represented by
a matrix C, its Jacobian is Jg(x) = C. So:"
N,0.6811902231668437,"Lipp(g) = ∥C∥p :=
sup
∥x∥p=1
∥Cx∥p ="
N,0.6822529224229543,"(
σmax(C)
if p = 2
maxi
P"
N,0.6833156216790648,"j
C(i,j)

if p = ∞."
N,0.6843783209351754,"where ∥C∥p is the operator norm on matrices induced by the vector p-norm, and σmax(C) is the
largest singular value of C."
N,0.6854410201912858,"Now, let us consider the function xn(z) = h(z) = (Anz) ◦(Snxn−1(z)). Its Jacobian is given by:"
N,0.6865037194473964,"Jxn = diag(Anz)SnJxn−1 + diag(Snxn−1)An,
Jx1 = diag(S1)A1 ."
N,0.6875664187035069,"Lipp(h) =
sup
z:∥z∥p≤1
∥Jxn∥p"
N,0.6886291179596175,"=
sup
z:∥z∥p≤1
∥diag(Anz)SnJxn−1 + diag(Snxn−1)An∥p"
N,0.6896918172157279,"≤
sup
z:∥z∥p≤1
∥diag(Anz)SnJxn−1∥p + ∥diag(Snxn−1)An∥p
Triangle inequality"
N,0.6907545164718385,"≤
sup
z:∥z∥p≤1
∥diag(Anz)∥p∥Sn∥p∥Jxn−1∥p + ∥diag(Snxn−1)∥p∥An∥p
Lemma 9 [consistent]"
N,0.691817215727949,"≤
sup
z:∥z∥p≤1
∥Anz∥p∥Sn∥p∥Jxn−1∥p + ∥Snxn−1∥p∥An∥p"
N,0.6928799149840595,"≤
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p∥Jxn−1∥p + ∥Sn∥p∥xn−1∥p∥An∥p"
N,0.69394261424017,"=
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p∥Jxn−1∥p + ∥Sn∥p∥(An−1z) ◦(Sn−1xn−2)∥p∥An∥p"
N,0.6950053134962806,"≤
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p∥Jxn−1∥p + ∥Sn∥p∥An−1z∥p∥Sn−1xn−2∥p∥An∥p"
N,0.696068012752391,"≤
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p∥Jxn−1∥p + ∥Sn∥p∥An−1∥p∥z∥p∥Sn−1∥p∥xn−2∥p∥An∥p"
N,0.6971307120085016,"=
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p(∥Jxn−1∥p + ∥An−1∥p∥Sn−1∥p∥xn−2∥p)"
N,0.6981934112646121,"≤
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p(∥Jxn−1∥p + n−1
Y"
N,0.6992561105207227,"i=1
(∥Si∥p∥Ai∥p)∥z∥n−2
p
) ."
N,0.7003188097768331,Then we proof the result by induction.
N,0.7013815090329437,Inductive hypothesis:
N,0.7024442082890542,"sup
z:∥z∥p≤1
∥Jxn∥p ≤n n
Y"
N,0.7035069075451648,"i=1
(∥Si∥p∥Ai∥p) ."
N,0.7045696068012752,Case k = 1:
N,0.7056323060573858,"Lipp(h) =
sup
z:∥z∥p≤1
∥Jx1∥p"
N,0.7066950053134963,"= ∥diag(S1)A1∥p
≤∥diag(S1)∥p∥A1∥p
≤∥S1∥p∥A1∥p ."
N,0.7077577045696068,Published as a conference paper at ICLR 2022
N,0.7088204038257173,Case k = n:
N,0.7098831030818279,"Lipp(h) =
sup
z:∥z∥p≤1
∥Jxn∥p"
N,0.7109458023379384,"≤
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p(∥Jxn−1∥p + n−1
Y"
N,0.7120085015940489,"i=1
(∥Si∥p∥Ai∥p)∥z∥n−2
p
)"
N,0.7130712008501594,"≤
sup
z:∥z∥p≤1
∥An∥p∥z∥p∥Sn∥p((n −1) n−1
Y"
N,0.71413390010627,"i=1
(∥Si∥p∥Ai∥p) + n−1
Y"
N,0.7151965993623804,"i=1
(∥Si∥p∥Ai∥p)∥z∥n−2
p
) ≤n n
Y"
N,0.7162592986184909,"i=1
(∥Si∥p∥Ai∥p) . So:"
N,0.7173219978746015,Lipp(FL) ≤Lipp(g)Lipp(h)
N,0.718384697130712,"≤k∥C∥p k
Y"
N,0.7194473963868225,"i=1
(∥Si∥p∥Ai∥p) ."
N,0.720510095642933,"F.4.1
PROOF OF THEOREM 4"
N,0.7215727948990436,Proof. This is particular case of Theorem 9 with p = ∞.
N,0.722635494155154,"G
RELATIONSHIP BETWEEN A CONVOLUTIONAL LAYER AND A FULLY
CONNECTED LAYER"
N,0.7236981934112646,"In this section we discuss various cases of input/output types depending on the dimensionality of the
input tensor and the output tensor. We also provide the proof of Theorem 5.
Theorem 10. Let A ∈Rn. Let K ∈Rh be a 1-D convolutional kernel. For simplicity, we assume
h is odd and h ≤n. Let B ∈Rn, B = K ⋆A be the output of the convolution. Let U be the
convolutional operator i.e., the linear operator (matrix) U ∈Rn×n such that B = K ⋆A = UA.
It holds that ∥U∥∞= ∥K∥1."
N,0.7247608926673751,"Theorem 11. Let A ∈Rn×m, and let K ∈Rh×h be a 2-D convolutional kernel. For simplicity
assume h is odd number and h ≤min(n, m). Let B ∈Rn×m, B = K ⋆A be the output of the
convolution. Let U be the convolutional operator i.e., the linear operator (matrix) U ∈Rnm×nm
such that vec(B) = Uvec(A). It holds that ∥U∥∞= ∥vec(K)∥1."
N,0.7258235919234857,"G.1
PROOF OF THEOREM 10"
N,0.7268862911795961,"Proof. From, B = K ⋆A = UA we can obtain the following:  

"
N,0.7279489904357067,"u1,1
u1,2
· · ·
u1,n
u2,1
u2,2
· · ·
u2,n
...
...
...
...
un,1
un,2
· · ·
un,n "
N,0.7290116896918172,"

(A1, · · · , An)⊤= (K1, · · · , Kh)⊤⋆(A1, · · · , An)⊤."
N,0.7300743889479278,We observe that:
N,0.7311370882040382,"ui,j ="
N,0.7321997874601488,"(
K
h+1"
N,0.7332624867162593,"2
+j−i
if |i −j| ≤h−1"
N,0.7343251859723698,"2
;
0
if |i −j| > h−1 2
."
N,0.7353878852284803,Published as a conference paper at ICLR 2022
N,0.7364505844845909,"Then, it holds that:"
N,0.7375132837407014,"∥U∥∞=
n
max
i=1 n
X"
N,0.7385759829968119,"j=1
|ui,j| ≤
n
max
i=1 h
X j=1"
N,0.7396386822529224,Kj = ∥K∥1 .
N,0.740701381509033,"G.2
PROOF OF THEOREM 11"
N,0.7417640807651434,"Proof. We partition U into n × n partition matrices of shape m × m. Then the (i, j)th partition
matrix U(i,j) describes the relationship between the Bi and the Aj. So, U(i,j) is also similar to the
Toeplitz matrix in the previous result."
N,0.742826780021254,"U(i,j) ="
N,0.7438894792773645,"(
M h+1"
N,0.7449521785334751,"2
+j−i
if |i −j| ≤h−1"
N,0.7460148777895855,"2
;
0
if |i −j| > h−1 2
."
N,0.7470775770456961,"Meanwhile, the matrix M satisﬁes:"
N,0.7481402763018066,"mi(s,l) ="
N,0.7492029755579172,"(
k(i, h+1"
N,0.7502656748140276,"2
+l−s)
if |s −l| ≤h−1"
N,0.7513283740701382,"2
;
0
if |s −l| > h−1 2
."
N,0.7523910733262487,"Then, we have the following:"
N,0.7534537725823592,"∥U∥∞=
n×m
max
i=1 n×m
X"
N,0.7545164718384697,"j=1
|ui,j| ≤
n
max
i=1 n
X"
N,0.7555791710945803,"j=1
∥Ui,j∥∞≤
n
max
i=1 h
X j=1"
N,0.7566418703506907,"Kj
1 = h
X i=1"
N,0.7577045696068013,"Ki
1 = ∥vec(K)∥1 ."
N,0.7587672688629118,"In addition, by h ≤n, h ≤m we have:
U
h+1"
N,0.7598299681190224,"2
+M( h−1"
N,0.7608926673751328,"2
)
1 = h
X i=1"
N,0.7619553666312433,"Ki
1 .
(15)"
N,0.7630180658873539,"Then, it holds that ∥U∥∞= Ph
i=1
Ki
1."
N,0.7640807651434643,"G.3
PROOF OF THEOREM 5"
N,0.7651434643995749,"Proof. We partition U into o × r partition matrices of shape nm × nm. Then the (i, j)th partition
of the matrix U(i,j) describes the relationship between the ith channel of B and the jth channel of
A. Then, the following holds:
U(i,j)

∞= Ph
i=1
Ki
ij

1, where Kij means the two-dimensional
tensor obtained by the third dimension of K takes j and the fourth dimension of K takes i."
N,0.7662061636556854,"∥U∥∞= maxn×m×o
i=1"
N,0.767268862911796,"n×m×r
X j=1"
N,0.7683315621679064,"u(i,j)"
N,0.769394261424017,"=
o−1
max
l=0"
N,0.7704569606801275,"n×m
max
i=1 r−1
X s=0 n×m
X j=1"
N,0.7715196599362381,"u(i+nml,j+nms)"
N,0.7725823591923485,"≤
o−1
max
l=0 r−1
X"
N,0.7736450584484591,"s=0
(
n×m
max
i=1 n×m
X j=1"
N,0.7747077577045696,"u(i+nml,j+nms)
)"
N,0.7757704569606801,"=
o−1
max
l=0 r−1
X s=0"
N,0.7768331562167906,"U(l+1,s+1)

∞"
N,0.7778958554729012,"=
o
max
l=1 r
X s=1"
N,0.7789585547290117,"U(l,s)

∞"
N,0.7800212539851222,"=
o
max
l=1 r
X s=1 h
X i=1"
N,0.7810839532412327,"Ki
ls

1"
N,0.7821466524973433,"≤
o
max
l=1"
N,0.7832093517534537,"ˆ
Kl
1"
N,0.7842720510095643,"=
 ˆ
K

∞."
N,0.7853347502656748,Published as a conference paper at ICLR 2022
N,0.7863974495217854,"Similar to Eq. (15): for every nm rows, we choose k+1"
N,0.7874601487778958,"2
th row. Then its 1-norm is equal to this nm
rows of the ˆ
K’s ∞-norm. So the equation holds."
N,0.7885228480340064,"H
AUXILIARY NUMERICAL EVIDENCE"
N,0.7895855472901169,"A number of additional experiments are conducted in this section. Unless explicitly mentioned
otherwise, the experimental setup remains similar to the one in the main paper. The following
experiments are conducted below:"
THE DIFFERENCE BETWEEN THE THEORETICAL AND THE ALGORITHMIC BOUND AND THEIR EVOLUTION DURING,0.7906482465462275,"1. The difference between the theoretical and the algorithmic bound and their evolution during
training is studied in Appendix H.1."
THE DIFFERENCE BETWEEN THE THEORETICAL AND THE ALGORITHMIC BOUND AND THEIR EVOLUTION DURING,0.7917109458023379,2. An ablation study on the hidden size is conducted in Appendix H.2.
THE DIFFERENCE BETWEEN THE THEORETICAL AND THE ALGORITHMIC BOUND AND THEIR EVOLUTION DURING,0.7927736450584485,3. An ablation study is conducted on the effect of adversarial steps in Appendix H.3.
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.793836344314559,"4. We evaluate the effect of the proposed projection into the testset performance in Ap-
pendix H.4."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.7948990435706695,"5. We conduct experiments on four new datasets, i.e., MNIST, K-MNIST, E-MNIST-BY,
NSYNTH in Appendix H.5. These experiments are conducted in addition to the datasets
already presented in the main paper."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.79596174282678,"6. In Appendix H.6 experiments on three additional adversarial attacks, i.e., FGSM-0.01,
APGDT and TPGD, are performed."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.7970244420828906,7. We conduct an experiment using the NCP model in Appendix H.7.
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.798087141339001,"8. The layer-wise bound (instead of a single bound for all matrices) is explored in Ap-
pendix H.8."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.7991498405951116,9. The comparison with adversarial defense methods is conducted in Appendix H.9.
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8002125398512221,"H.1
THEORETICAL AND ALGORITHMIC BOUND"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8012752391073327,"As mentioned in Section 3, projecting the quantity θ =
•k
i=1Ui

∞onto their level set corresponds
to a difﬁcult non-convex problem. Given that we have an upper bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8023379383634431,"θ =
•k
i=1Ui

∞≤Πk
i=1∥Ui∥∞=: γ ."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8034006376195537,we want to understand in practice how tight is this bound. In Fig. 4 we compute the ratio γ
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8044633368756642,"θ for PN-4.
In Fig. 5 the ratio is illustrated for randomly initialized matrices (i.e., untrained networks)."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8055260361317748,"H.2
ABLATION STUDY ON THE HIDDEN SIZE"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8065887353878852,"Initially, we explore the effect of the hidden rank of PN-4 and PN-10 on Fashion-MNIST. Fig. 6
exhibits the accuracy on both the training and the test-set for both models. We observe that PN-10
has a better accuracy on the training set, however the accuracy on the test set is the same in the two
models. We also note that increasing the hidden rank improves the accuracy on the training set, but
not on the test set."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8076514346439958,"H.3
ABLATION STUDY ON THE EFFECT OF ADVERSARIAL STEPS"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8087141339001063,"Our next experiment scrutinizes the effect of the number of adversarial steps on the robust accuracy.
We consider in all cases a projection bound of 1, which provides the best empirical results. We
vary the number of adversarial steps and report the accuracy in Fig. 7. The results exhibit a similar
performance both in terms of the dataset (i.e., Fashion-MNIST and K-MNIST) and in terms of
the network (PN-4 and PN-Conv). Notice that when the adversarial attack has more than 10 steps
the performance does not vary signiﬁcantly from the performance at 10 steps, indicating that the
projection bound is effective for stronger adversarial attacks."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8097768331562167,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8108395324123273,"2
0
2
4
6
8
10
12
Log of bound for each matrix in regularization 1.00 1.02 1.04 1.06 1.08 1.10 1.12"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8119022316684378,Ratio /
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8129649309245484,PN-4 Fashion-MNIST (a)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8140276301806588,"10
20
30
40
50
Epochs 42000 44000 46000 48000 50000 52000 54000 56000 58000 Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8150903294367694,PN-4 Fashion-MNIST (b)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8161530286928799,"2
0
2
4
6
8
10
12
Log of bound for each matrix in regularization 1.000 1.025 1.050 1.075 1.100 1.125 1.150 1.175 1.200"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8172157279489904,Ratio /
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8182784272051009,PN-4 K-MNIST (c)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8193411264612115,"10
20
30
40
50
Epochs 42000 44000 46000 48000 50000 52000 Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.820403825717322,PN-4 K-MNIST (d)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8214665249734325,"Figure 4: Visualization of the difference between the bound results on Fashion-MNIST (top row) and
on K-MNIST (bottom row). Speciﬁcally, in (a) and (c) we visualize the ratio γ θ ="
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.822529224229543,"Qk
i=1 ∥Ui∥∞
∥•k
i=1Ui∥∞
for"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8235919234856536,"different log bound values for PN-4. In (b), (d) the exact values of the two bounds are computed over
the course of the unregularized training. Notice that there is a gap between the two bounds, however
importantly the two bounds are increasing at the same rate, while their ratio is close to 1."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.824654622741764,"2
3
4
5
6
7
8
Log of hidden rank 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8257173219978746,Ratio / PN-10 (a)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8267800212539851,"2
4
6
8
10
12
14
Depth 1.0 1.1 1.2 1.3 1.4 1.5 1.6"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8278427205100957,Ratio /
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8289054197662061,hidden rank = 16 (b)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8299681190223167,Figure 5: Visualization of the ratio
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8310308182784272,"Qk
i=1 ∥Ui∥∞
∥•k
i=1Ui∥∞
in a randomly initialized network (i.e., using normal"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8320935175345378,"distribution random matrices). Speciﬁcally, in (a) we visualize the ratio for different log hidden rank
values for PN-10. In (b) we visualize the ratio for different depth values for hidden rank = 16. Neither
of two plots contain any regularization."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8331562167906482,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8342189160467588,"0
1
2
3
4
5
6
Log of Hidden size 50 60 70 80 90 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8352816153028693,Accuracy(%)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8363443145589798,"PNN-10-Test
PNN-10-Train
PNN-4-Test
PNN-4-Train"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8374070138150903,Figure 6: Accuracy of PN-4 and PN-10 when the hidden rank varies (plotted in log-scale).
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8384697130712009,"PN-4
PN-Conv"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8395324123273114,"0
5
10
15
20
25
30
PGD attack steps 40 45 50 55 60 65 70"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8405951115834219,Accuracy(%)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8416578108395324,Ablation study on the effect of adversarial steps
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.842720510095643,(a) Fashion-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8437832093517534,"0
5
10
15
20
25
30
PGD attack steps 30 35 40 45 50 55 60 65 70"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.844845908607864,Accuracy(%)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8459086078639745,Ablation study on the effect of adversarial steps
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8469713071200851,(b) K-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8480340063761955,"Figure 7: Ablation study on the effect of adversarial steps in Fashion-MNIST and K-MNIST. All
methods are run by considering a projection bound of 1."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8490967056323061,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8501594048884166,"H.4
EVALUATION OF THE ACCURACY OF PNS"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8512221041445271,"In this experiment, we evaluate the accuracy of PNs. We consider three networks, i.e., PN-4, PN-10
and PN-Conv, and train them under varying projection bounds using Algorithm 1. Each model is
evaluated on the test set of (a) Fashion-MNIST and (b) E-MNIST."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8522848034006376,"The accuracy of each method is reported in Fig. 8, where the x-axis is plotted in log-scale (natural
logarithm). The accuracy is better for bounds larger than 2 (in the log-axis) when compared to tighter
bounds (i.e., values less than 0). Very tight bounds stiﬂe the ability of the network to learn, which
explains the decreased accuracy. Interestingly, PN-4 reaches similar accuracy to PN-10 and PN-Conv
in Fashion-MNIST as the bound increases, while in E-MNIST it cannot reach the same performance
as the bound increases. The best bounds for all three models are observed in the intermediate values,
i.e., in the region of 1 in the log-axis for PN-4 and PN-10."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8533475026567482,"PN-4
PN-10
PN-Conv"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8544102019128587,"0
2
4
6
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8554729011689692,Accuracy(%)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8565356004250797,Fashion-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8575982996811902,"0
2
4
6
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8586609989373007,E-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8597236981934112,"Figure 8: Accuracy of PN-4, PN-10 and PN-Conv under varying projection bounds (x-axis in
log-scale) learned on (a) Fashion-MNIST, (b) E-MNIST. Notice that the performance increases for
intermediate values, while it deteriorates when the bound is very tight."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8607863974495218,"We scrutinize further the projection bounds by training the same models only with cross-entropy loss
(i.e., no bound regularization). In Table 10, we include the accuracy of the three networks with and
without projection. Note that projection consistently improves the accuracy, particularly in the case
of larger networks, i.e., PN-10."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8618490967056323,"Method
PN-4
PN-10
PN-Conv
Fashion-MNIST
No projection 87.28 ± 0.18%
88.48 ± 0.17%
86.36 ± 0.21%
Projection
87.32 ± 0.14%
88.72 ± 0.12%
86.38 ± 0.26%
E-MNIST
No projection 84.27 ± 0.26%
89.31 ± 0.09%
91.49 ± 0.29%
Projection
84.34 ± 0.31%
90.56 ± 0.10%
91.57 ± 0.19%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8629117959617428,"Table 10: The accuracy of different PN models on Fashion-MNIST (top) and E-MNIST (bottom)
when trained only with SGD (ﬁrst row) and when trained with projection (last row)."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8639744952178533,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8650371944739639,"H.5
EXPERIMENTAL RESULTS ON ADDITIONAL DATASETS"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8660998937300743,"To validate even further we experiment with additional datasets. We describe the datasets below
and then present the robust accuracy in each case. The experimental setup remains the same as in
Section 4.2 in the main paper. As a reminder, we are evaluating the robustness of the different models
under adversarial noise."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8671625929861849,Dataset details: There are six datasets used in this work:
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8682252922422954,"1. Fashion-MNIST (Xiao et al., 2017) includes grayscale images of clothing. The training set
consists of 60, 000 examples, and the test set of 10, 000 examples. The resolution of each
image is 28 × 28, with each image belonging to one of the 10 classes.
2. E-MNIST (Cohen et al., 2017) includes handwritten character and digit images with a
training set of 124, 800 examples, and a test set of 20, 800 examples. The resolution of each
image is 28 × 28. E-MNIST includes 26 classes. We also use the variant EMNIST-BY that
includes 62 classes with 697, 932 examples for training and 116, 323 examples for testing.
3. K-MNIST (Clanuwat et al., 2018) depicts grayscale images of Hiragana characters with a
training set of 60, 000examples, and a test set of 10, 000 examples. The resolution of each
image is 28 × 28. K-MNIST has 10 classes.
4. MNIST (Lecun et al., 1998) includes handwritten digits images. MNIST has a training set
of 60, 000 examples, and a test set of 10, 000 examples. The resolution of each image is
28 × 28.
5. CIFAR-10 (Krizhevsky et al., 2014) depicts images of natural scenes. CIFAR-10 has a
training set of 50, 000 examples, and a test set of 10, 000 examples. The resolution of each
RGB image is 32 × 32.
6. NSYNTH (Engel et al., 2017) is an audio dataset containing 305, 979 musical notes, each
with a unique pitch, timbre, and envelope."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.869287991498406,"We provide a visualization3 of indicative samples from MNIST, Fashion-MNIST, K-MNIST and
E-MNIST in Fig. 9."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8703506907545164,"We originally train PN-4, PN-10 and PN-Conv without projection bounds. The results are reported
in Table 11 (columns titled ‘No proj’) for MNIST and K-MNIST, Table 13 (columns titled ‘No
proj’) for E-MNIST-BY and Table 14 (columns titled ‘No proj’) for NSYNTH. Next, we consider
the performance under varying projection bounds; the accuracy in each case is depicted in Fig. 10
for K-MNIST, MNIST and E-MNIST-BY and Fig. 11 for NSYNTH. The ﬁgures (and the tables)
depict the same patterns that emerged in the two main experiments, i.e., the performance can be vastly
improved for intermediate values of the projection bound. Similarly, we validate the performance
when using adversarial training. The results in Table 12 demonstrate the beneﬁts of using projection
bounds even in the case of adversarial training."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.871413390010627,"H.6
EXPERIMENTAL RESULTS OF MORE TYPES OF ATTACKS"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8724760892667375,"To further verify the results of the main paper, we conduct experiments with three additional adver-
sarial attacks: a) FGSM with ϵ = 0.01, b) Projected Gradient Descent in Trades (TPGD) (Zhang et al.,
2019), c) Targeted Auto-Projected Gradient Descent (APGDT) (Croce and Hein, 2020). In TPGD
and APGDT, we use the default parameters for a one-step attack."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.873538788522848,"The quantitative results are reported in Table 15 for four datasets and the curves of Fashion-MNIST
and E-MNIST are visualized in Fig. 12 and the curves of K-MNIST and MNIST are visualized in
Fig. 13. The results in both cases remain similar to the attacks in the main paper, i.e., the proposed
projection improves the performance consistently across attacks, types of networks and adversarial
attacks."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8746014877789585,"H.7
EXPERIMENTAL RESULTS IN NCP MODEL"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8756641870350691,"To complement, the results of the CCP model, we conduct an experiment using the NCP model. That
is, we use a 4th degree polynomial expansion, called NCP-4, for our experiment. We conduct an"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8767268862911796,3The samples were found in https://www.tensorflow.org/datasets/catalog.
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8777895855472901,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8788522848034006,"(a) MNIST
(b) Fashion-MNIST"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8799149840595112,"(c) K-MNIST
(d) E-MNIST"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8809776833156217,"Figure 9: Samples from the datasets used for the numerical evidence. Below each image, the class
name and the class number are denoted."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8820403825717322,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8831030818278427,"Clean
FGSM_0.1
PGD_0.1_0.01_20
PGD_0.3_0.03_20"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8841657810839533,"0
2
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8852284803400637,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8862911795961743,"0
2
4
Log of Bound PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8873538788522848,"0
2
4
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8884165781083954,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8894792773645058,(a) K-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8905419766206164,"0
2
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8916046758767269,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8926673751328374,"0
2
4
Log of Bound PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8937300743889479,"0
2
4
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8947927736450585,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.895855472901169,(b) MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8969181721572795,"0
2
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.89798087141339,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.8990435706695006,"0
2
4
Log of Bound PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.900106269925611,"0
2
4
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9011689691817216,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9022316684378321,"(c) E-MNIST-BY
Figure 10: Adversarial attacks during testing on (a) K-MNIST (top), (b) MNIST (middle), (c)
E-MNIST-BY (bottom) with the x-axis is plotted in log-scale. Note that intermediate values of
projection bounds yield the highest accuracy. The patterns are consistent in all datasets and across
adversarial attacks."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9032943676939427,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9043570669500531,"Clean
FGSM_0.1
PGD_0.1_0.01_20
PGD_0.3_0.03_20"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9054197662061636,"1
0
1
2
3
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9064824654622742,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9075451647183846,Figure 11: Adversarial attacks during testing on NSYNTH.
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9086078639744952,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9096705632306057,"Clean
FGSM0.01
APGDT
TPGD"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9107332624867163,"0
2
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9117959617428267,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9128586609989373,"0
2
4
Log of Bound PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9139213602550478,"0
2
4
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9149840595111584,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9160467587672688,(a) Fashion-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9171094580233794,"0
2
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9181721572794899,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9192348565356004,"0
2
4
Log of Bound PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9202975557917109,"0
2
4
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9213602550478215,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.922422954303932,"(b) E-MNIST
Figure 12: Three new adversarial attacks during testing on (a) Fashion-MNIST (top), (b) E-MNIST
(bottom ) with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds
yield the highest accuracy. The patterns are consistent in both datasets and across adversarial attacks."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9234856535600425,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.924548352816153,"Clean
FGSM0.01
APGDT
TPGD"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9256110520722636,"0
2
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.926673751328374,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9277364505844846,"0
2
4
Log of Bound PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9287991498405951,"0
2
4
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9298618490967057,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9309245483528161,(a) K-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9319872476089267,"0
2
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9330499468650372,Accuracy(%) PN-4
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9341126461211477,"0
2
4
Log of Bound PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9351753453772582,"0
2
4
Log of Bound"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9362380446333688,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9373007438894793,"(b) MNIST
Figure 13: Three new adversarial attacks during testing on (a) K-MNIST (top), (b) MNIST (bottom)
with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds yield the
highest accuracy. The patterns are consistent in both datasets and across adversarial attacks."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9383634431455898,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9394261424017003,"Method
No proj.
Our method
Jacobian
L2
K-MNIST PN-4"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9404888416578109,"Clean
84.04 ± 0.30%
84.23 ± 0.30%
83.16 ± 0.31%
84.18 ± 0.44%
FGSM-0.1
18.86 ± 2.61%
35.84 ± 1.67%
22.61 ± 1.30%
22.05 ± 2.76%
PGD-(0.1, 20, 0.01) 11.20 ± 4.27%
40.26 ± 1.36%
16.00 ± 5.63%
10.93 ± 2.42%
PGD-(0.3, 20, 0.03) 1.94 ± 1.11%
24.75 ± 1.32%
4.46 ± 2.59%
2.70 ± 1.26% PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9415515409139213,"Clean
87.90 ± 0.24%
88.80 ± 0.19%
88.73 ± 0.16%
87.93 ± 0.18%
FGSM-0.1
24.52 ± 1.44%
41.83 ± 2.00%
26.90 ± 1.02%
26.62 ± 1.59%
PGD-(0.1, 20, 0.01) 7.54 ± 0.79%
39.55 ± 0.64%
11.50 ± 1.35%
5.09 ± 0.68%
PGD-(0.3, 20, 0.03) 0.05 ± 0.04%
25.24 ± 0.93%
1.24 ± 0.64%
0.19 ± 0.12%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9426142401700319,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9436769394261424,"Clean
88.41 ± 0.37%
88.48 ± 0.42%
86.57 ± 0.46% 88.56 ± 0.62%
FGSM-0.1
13.34 ± 2.01%
47.75 ± 2.03%
14.16 ± 3.05%
12.43 ± 2.58%
PGD-(0.1, 20, 0.01) 10.81 ± 1.25%
45.68 ± 3.11%
12.05 ± 0.82%
11.05 ± 0.85%
PGD-(0.3, 20, 0.03) 6.91 ± 2.04%
31.68 ± 1.43%
7.54 ± 1.39%
6.28 ± 2.37%
MNIST PN-4"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.944739638682253,"Clean
96.52 ± 0.13%
96.62 ± 0.17%
95.88 ± 0.16%
96.44 ± 0.18%
FGSM-0.1
20.96 ± 5.16%
64.09 ± 2.41%
33.59 ± 8.46%
26.07 ± 5.64%
PGD-(0.1, 20, 0.01) 14.23 ± 5.39%
66.05 ± 7.06%
20.83 ± 5.64%
16.06 ± 5.84%
PGD-(0.3, 20, 0.03) 2.59 ± 2.01%
51.47 ± 3.17%
4.92 ± 1.18%
4.26 ± 2.44% PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9458023379383634,"Clean
97.46 ± 0.11%
97.63 ± 0.06%
97.36 ± 0.05%
97.53 ± 0.10%
FGSM-0.1
30.12 ± 4.58%
70.02 ± 1.28%
40.22 ± 2.31%
28.77 ± 2.41%
PGD-(0.1, 20, 0.01) 9.70 ± 2.11%
73.57 ± 1.17%
18.74 ± 5.39%
10.91 ± 2.32%
PGD-(0.3, 20, 0.03) 0.47 ± 0.53%
55.36 ± 2.32%
2.49 ± 1.46%
0.44 ± 0.29%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.946865037194474,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9479277364505845,"Clean
98.32 ± 0.12%
98.40 ± 0.12%
97.88 ± 0.12%
98.32 ± 0.11%
FGSM-0.1
18.98 ± 2.99%
67.50 ± 6.22%
27.02 ± 9.88%
23.77 ± 5.58%
PGD-(0.1, 20, 0.01) 12.57 ± 2.81% 72.85 ± 12.23% 13.96 ± 2.57%
13.84 ± 3.18%
PGD-(0.3, 20, 0.03) 10.57 ± 4.08%
55.56 ± 8.48%
10.22 ± 0.52%
9.10 ± 3.62%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9489904357066951,"Table 11: Comparison of regularization techniques on K-MNIST (top) and MNIST (bottom). In each
dataset, the base networks are PN-4, i.e., a 4th degree polynomial, on the top four rows, PN-10, i.e., a
10th degree polynomial, on the middle four rows and PN-Conv, i.e., a 4th degree polynomial with
convolutions, on the bottom four rows. Our projection method exhibits the best performance in all
three attacks, with the difference on accuracy to stronger attacks being substantial."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9500531349628055,"Method
AT
Our method + AT
Jacobian + AT
L2 + AT
Adversarial training (AT) with PN-10 on K-MNIST
FGSM-0.1
70.93 ± 0.46%
71.14 ± 0.30%
64.48 ± 0.51%
70.90 ± 0.57%
PGD-(0.1, 20, 0.01) 60.94 ± 0.71%
61.20 ± 0.39%
57.89 ± 0.31%
61.47 ± 0.44%
PGD-(0.3, 20, 0.03) 30.77 ± 0.26%
33.07 ± 0.58%
29.96 ± 0.21%
30.35 ± 0.42%
Adversarial training (AT) with PN-10 on MNIST
FGSM-0.1
91.89 ± 0.30%
91.94 ± 0.17%
87.85 ± 0.27%
92.22 ± 0.30%
PGD-(0.1, 20, 0.01) 87.36 ± 0.29%
87.38 ± 0.37%
84.96 ± 0.25%
87.26 ± 0.49%
PGD-(0.3, 20, 0.03) 61.96 ± 0.92%
63.96 ± 1.02%
62.24 ± 0.24%
62.44 ± 0.76%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9511158342189161,"Table 12: Comparison of regularization techniques on (a) K-MNIST (top) and (b) MNIST (bottom)
along with adversarial training (AT). The base network is a PN-10, i.e., 10th degree polynomial. Our
projection method exhibits the best performance in all three attacks."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9521785334750266,"experiment in the K-MNIST dataset and present the result with varying bound in Fig. 14. Notice that
the patterns remain similar to the CCP model, i.e., intermediate values of the projection bound can
increase the performance signiﬁcantly."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.953241232731137,"H.8
LAYER-WISE BOUND"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9543039319872476,"To assess the ﬂexibility of the proposed method, we assess the performance of the layer-wise bound. In
the previous sections, we have considered using a single bound for all the matrices, i.e., ∥Ui∥∞≤λ,
because the projection for a single matrix has efﬁcient projection algorithms. However, Lemma 1
enables each matrix Ui to have a different bound λi. We assess the performance of having different
bounds for each matrix Ui."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9553666312433581,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9564293304994687,"Method
PN-4, PN-10 and PN-Conv on E-MNIST-BY"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9574920297555791,"No proj.
Our method PN-4"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9585547290116897,"Clean
80.18 ± 0.19%
80.26 ± 0.17%
FGSM-0.1
3.65 ± 0.76%
16.58 ± 3.87%
PGD-(0.1, 20, 0.01)
4.57 ± 1.98%
19.77 ± 4.42%
PGD-(0.3, 20, 0.03)
0.59 ± 0.40%
10.13 ± 2.08% PN-10"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9596174282678002,"Clean
84.17 ± 0.06%
85.32 ± 0.04%
FGSM-0.1
11.67 ± 1.21%
32.37 ± 2.58%
PGD-(0.1, 20, 0.01)
2.48 ± 0.66%
31.22 ± 2.32%
PGD-(0.3, 20, 0.03)
0.03 ± 0.05%
13.74 ± 0.77%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9606801275239107,PN-Conv
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9617428267800212,"Clean
85.92 ± 0.08%
86.03 ± 0.08%
FGSM-0.1
0.65 ± 0.17%
29.07 ± 2.72%
PGD-(0.1, 20, 0.01)
1.57 ± 1.40%
31.06 ± 4.70%
PGD-(0.3, 20, 0.03)
0.33 ± 0.06%
23.93 ± 6.32%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9628055260361318,"Table 13: Comparison of regularization techniques on E-MNIST-BY. The base network are PN-4,
i.e., 4th degree polynomial, on the top four rows, PN-10, i.e., 10th degree polynomial, on the middle
four rows and PN-Conv, i.e., a 4th degree polynomial with convolution, on the bottom four rows. Our
projection method exhibits the best performance in all three attacks, with the difference on accuracy
to stronger attacks being substantial."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9638682252922423,"Model
PN-4
Projection
No-proj
Proj
Clean accuracy
80.25 ± 0.27%
80.33 ± 0.26%
FGSM-0.1
0.91 ± 0.14%
22.25 ± 0.04%
PGD-(0.1, 20, 0.01)
0.31 ± 0.11%
22.27 ± 0.00%
PGD-(0.3, 20, 0.03)
0.46 ± 0.29%
20.38 ± 2.30%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9649309245483528,"Table 14: Evaluation of the robustness of PN models on NSYNTH. Each line refers to a different
adversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacks
projection improves the accuracy by a remarkable margin."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9659936238044633,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9670563230605739,"Clean
FGSM_0.1
PGD_0.1_0.01_20
PGD_0.3_0.03_20"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9681190223166843,"1
0
1
2
3
4
Log of Bound 0 20 40 60 80 100"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9691817215727949,Accuracy(%)
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9702444208289054,K-MNIST
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.971307120085016,Figure 14: Experimental result of K-MNIST in NCP model.
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9723698193411264,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.973432518597237,"Method
No proj.
Our method
Fashion-MNIST"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9744952178533475,"PN-4
FGSM-0.01 26.49 ± 3.13%
58.09 ± 1.63%
APGDT
16.59 ± 4.35%
50.83 ± 1.55%
TPGD
26.88 ± 6.78%
59.03 ± 1.45%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.975557917109458,"PN-10
FGSM-0.01 18.59 ± 1.82%
60.56 ± 1.06%
APGDT
8.76 ± 1.14%
51.93 ± 1.91%
TPGD
14.53 ± 1.49%
63.33 ± 0.51%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9766206163655685,"PN-Conv
FGSM-0.01 15.30 ± 3.10%
55.90 ± 2.60%
APGDT
11.88 ± 1.33%
53.49 ± 0.72%
TPGD
14.50 ± 1.59%
58.72 ± 1.87%
E-MNIST"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9776833156216791,"PN-4
FGSM-0.01 13.40 ± 5.16%
32.83 ± 2.08%
APGDT
9.33 ± 4.00%
26.38 ± 2.70%
TPGD
17.40 ± 3.11%
34.68 ± 1.92%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9787460148777896,"PN-10
FGSM-0.01 14.47 ± 1.80%
48.28 ± 3.06%
APGDT
10.13 ± 0.93%
41.72 ± 4.05%
TPGD
13.97 ± 0.88%
47.44 ± 3.62%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9798087141339001,"PN-Conv
FGSM-0.01 4.71 ± 1.10%
39.37 ± 5.43%
APGDT
3.58 ± 0.66%
30.43 ± 4.87%
TPGD
4.08 ± 0.33%
35.85 ± 10.20%
K-MNIST"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9808714133900106,"PN-4
FGSM-0.01 23.31 ± 5.34%
43.74 ± 5.97%
APGDT
17.02 ± 6.97%
39.43 ± 1.89%
TPGD
23.45 ± 7.67%
48.46 ± 3.84%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9819341126461212,"PN-10
FGSM-0.01 26.87 ± 2.14%
50.99 ± 3.52%
APGDT
16.23 ± 1.32%
41.46 ± 3.85%
TPGD
22.63 ± 0.99%
49.91 ± 1.37%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9829968119022316,"PN-Conv
FGSM-0.01 12.31 ± 2.03%
52.58 ± 6.80%
APGDT
13.47 ± 2.19%
42.94 ± 1.68%
TPGD
14.25 ± 2.51%
48.19 ± 3.02%
MNIST"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9840595111583422,"PN-4
FGSM-0.01 34.14 ± 7.63%
73.95 ± 5.18%
APGDT
29.88 ± 9.47%
71.26 ± 4.88%
TPGD
27.01 ± 9.77%
76.88 ± 1.98%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9851222104144527,"PN-10
FGSM-0.01 32.34 ± 4.67%
78.83 ± 1.63%
APGDT
19.55 ± 1.72%
75.22 ± 2.05%
TPGD
28.11 ± 3.87%
79.74 ± 2.07%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9861849096705633,"PN-Conv
FGSM-0.01 22.73 ± 3.10%
69.83 ± 8.91%
APGDT
17.95 ± 3.39%
64.94 ± 8.96%
TPGD
21.82 ± 3.07% 66.47 ± 11.83%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9872476089266737,"Table 15: Evaluation of the robustness of PN models on four datasets with three new types of attacks.
Each line refers to a different adversarial attack. The projection offers an improvement in the accuracy
in each case."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9883103081827843,"We experiment on PN-4 that we set a different projection bound for each matrix Ui. Speciﬁcally,
we use ﬁve different candidate values for each λi and then perform the grid search on the Fashion-
MNIST FGSM-0.01 attack. The results on Fashion-MNIST in Table 16 exhibit how the layer-wise
bounds outperform the previously used single bound4. The best performing values for PN-4 are
λ1 = 1.5, λ2 = 2, λ3 = 1.5, λ4 = 2, µ = 0.8. The values of λi in the ﬁrst few layers are larger,
while the value in the output matrix C is tighter."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9893730074388948,"To scrutinize the results even further, we evaluate whether the best performing λi can improve the
performance in different datasets and the FGSM-0.1 attack. In both cases, the best performing λi can
improve the performance of the single bound."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9904357066950054,"4The single bound is mentioned as ‘Our method’ in the previous tables. In this experiment both ‘single
bound’ and ‘layer-wise bound’ are proposed."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9914984059511158,Published as a conference paper at ICLR 2022
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9925611052072264,"Method
No proj.
Jacobian
L2
Single bound
Layer-wise bound
Fashion-MNIST
FGSM-0.01 26.49 ± 3.13% 39.88 ± 4.59% 24.36 ± 1.95% 58.09 ± 1.63%
63.95 ± 1.26%
FGSM-0.1
12.92 ± 2.74% 17.90 ± 6.51% 13.80 ± 3.65% 46.43 ± 0.95%
55.14 ± 3.65%
K-MNIST
FGSM-0.01 23.31 ± 5.34% 25.46 ± 3.51% 27.85 ± 7.62% 43.74 ± 5.97%
49.61 ± 1.44%
FGSM-0.1
18.86 ± 2.61% 22.61 ± 1.30% 22.05 ± 2.76% 35.84 ± 1.67%
47.54 ± 3.74%
MNIST
FGSM-0.01 34.14 ± 7.63% 32.78 ± 6.94% 29.31 ± 3.95% 73.95 ± 5.18%
79.23 ± 3.65%
FGSM-0.1
20.96 ± 5.16% 33.59 ± 8.46% 26.07 ± 5.64% 64.09 ± 2.41%
74.97 ± 5.60%"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9936238044633369,"Table 16: Evaluation of our layer-wise bound versus our single bound. To avoid confusion with
previous results, note that ’single bound’ corresponds to ’Our method’ in the rest of the tables in this
work. The different λi values are optimized on Fashion-MNIST FGSM-0.01 attack. Then, the same
λi values are used for training the rest of the methods. The proposed layer-wise bound outperforms
the single bound by a large margin, improving even further by baseline regularization schemes."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9946865037194474,"Method
No proj.
Single bound
Layer-wise bound Gaussian denoising Median denoising Guided denoising"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9957492029755579,"Fashion-MNIST
FGSM-0.01 26.49 ± 3.13% 58.09 ± 1.63%
63.95 ± 1.26%
18.80 ± 3.08%
19.68 ± 3.20%
29.69 ± 5.37%
FGSM-0.1
12.92 ± 2.74% 46.43 ± 0.95%
55.14 ± 3.65%
14.14 ± 2.77%
14.02 ± 1.95%
22.94 ± 5.65%
Table 17: Comparison of the proposed method against adversarial defense methods on feature
denoising (Xie et al., 2019) and guided denoising (Liao et al., 2018). Notice that the single bound (cf.
Appendix H.8 for details) already outperforms the proposed defense methods, while the layer-wise
bounds further improves upon our single bound case."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9968119022316685,"H.9
ADVERSARIAL DEFENSE METHOD"
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.997874601487779,"One frequent method used against adversarial perturbations are the so called adversarial defense
methods. We assess the performance of adversarial defense methods on the PNs when compared with
the proposed method."
WE EVALUATE THE EFFECT OF THE PROPOSED PROJECTION INTO THE TESTSET PERFORMANCE IN AP-,0.9989373007438895,"We experiment on PN-4 in Fashion-MNIST. We chose three different methods: gaussian denoising,
median denoising and guided denoising (Liao et al., 2018). Gaussian denoising and median denoising
are the methods of using gaussian ﬁlter and median ﬁlter for feature denoising (Xie et al., 2019).
The results in Table 17 show that in both attacks our method performs favourably to the adversarial
defense methods."
