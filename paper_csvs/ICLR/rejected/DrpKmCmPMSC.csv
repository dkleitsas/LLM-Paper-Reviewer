Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002304147465437788,"Recent studies on few-shot classiﬁcation using transfer learning pose challenges
to the effectiveness and efﬁciency of episodic meta-learning algorithms. Transfer
learning approaches are a natural alternative, but they are restricted to few-shot
classiﬁcation. Moreover, little attention has been on the development of proba-
bilistic models with well-calibrated uncertainty from few-shot samples, except for
some Bayesian episodic learning algorithms. To tackle the aforementioned issues,
we propose a new transfer learning method to obtain accurate and reliable models
for few-shot regression and classiﬁcation. The resulting method does not require
episodic meta-learning and is called meta-free representation learning (MFRL).
MFRL ﬁrst ﬁnds low-rank representation generalizing well on meta-test tasks.
Given the learned representation, probabilistic linear models are ﬁne-tuned with
few-shot samples to obtain models with well-calibrated uncertainty. The proposed
method not only achieves the highest accuracy on a wide range of few-shot learn-
ing benchmark datasets but also correctly quantiﬁes the prediction uncertainty. In
addition, weight averaging and temperature scaling are effective in improving the
accuracy and reliability of few-shot learning in existing meta-learning algorithms
with a wide range of learning paradigms and model architectures."
INTRODUCTION,0.004608294930875576,"1
INTRODUCTION"
INTRODUCTION,0.0069124423963133645,"Currently, the vast majority of few-shot learning methods are within the general paradigm of meta-
learning (a.k.a. learning to learn) (Bengio et al., 1991; Schmidhuber, 1987; Thrun & Pratt, 1998),
which learns multiple tasks in an episodic manner to distill transferrable knowledge (Vinyals et al.,
2016; Finn et al., 2017; Snell et al., 2017). Although many episodic meta-learning methods report
state-of-the-art (SOTA) performance, recent studies show that simple transfer learning methods with
ﬁxed embeddings (Chen et al., 2019; Tian et al., 2020) can achieve similar or better performance in
few-shot learning. It is found that the effectiveness of optimization-based meta-learning algorithms
is due to reusing high-quality representation, instead of rapid learning of task-speciﬁc representation
(Raghu et al., 2020). The quality of the presentation is not quantitatively deﬁned, except for some
empirical case studies (Goldblum et al., 2020). Recent machine learning theories (Saunshi et al.,
2021) indicate that low-rank representation leads to better sample efﬁciency in learning a new task.
However, those theoretical studies are within the paradigm of meta-learning and do not reveal how
to obtain low-rank representation for few-shot learning outside the realm of meta-learning. This
motivates us to investigate ways to improve the representation for adapting to new few-shot tasks in
a meta-free manner by taking the advantage of simplicity and robustness in transfer learning."
INTRODUCTION,0.009216589861751152,"In parallel, existing transfer learning methods also have limitations. That is, the existing trans-
fer learning methods may not ﬁnd representation generalizing well to unseen few-shot tasks (Chen
et al., 2019; Dhillon et al., 2020) , compared with state-of-the-art meta-learning methods (Ye et al.,
2020; Zhang et al., 2020). Although some transfer learning methods utilize knowledge distillation
and self-supervised training to achieve strong performance in few-shot classiﬁcation, they are re-
stricted to few-shot classiﬁcation problems (Mangla et al., 2020; Tian et al., 2020). To the best of
our knowledge, no transfer learning method is developed to achieve similar performance to meta-
learning in few-shot regression. As such, it is desirable to have a transfer learning method that ﬁnds
high-quality representation generalizing well to unseen classiﬁcation and regression problems."
INTRODUCTION,0.01152073732718894,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.013824884792626729,"The last limitation of the existing transfer learning methods in few-shot learning is the lack of un-
certainty calibration. Uncertainty quantiﬁcation is concerned with the quantiﬁcation of how likely
certain outcomes are. Despite a plethora of few-shot learning methods (in fact, machine learning in
general) to improve the point estimation accuracy, few methods are developed to get probabilistic
models with improved uncertainty calibration by integrating Bayesian learning into episodic meta-
training (Grant et al., 2018; Finn et al., 2018; Yoon et al., 2018; Snell & Zemel, 2021). Few-shot
learning models can be used in risk-averse applications such as medical diagnosis (Prabhu et al.,
2019). The diagnosis decision is made on not only point estimation but also probabilities associated
with the prediction. The risk of making wrong decisions is signiﬁcant when using uncalibrated mod-
els (Begoli et al., 2019). Thus, the development of proper ﬁne-tuning steps to achieve well-calibrated
models is the key towards practical applications of transfer learning in few-shot learning."
INTRODUCTION,0.016129032258064516,"In this paper, we develop a simple transfer learning method as our own baseline to allow easy regu-
larization towards more generalizable representation and calibration of prediction uncertainty. The
regularization in the proposed transfer learning method works for regression and classiﬁcation prob-
lems so that we can handle both problems within a common architecture. The calibration procedure
is easily integrated into the developed transfer learning method to obtain few-shot learning models
with good uncertainty quantiﬁcation. Therefore, the resulting method, called Meta-Free Repre-
sentation Learning (MFRL), overcomes the aforementioned limitations in existing transfer learning
methods for few-shot learning. Our empirical studies demonstrate that the relatively overlooked
transfer learning method can achieve high accuracy and well-calibrated uncertainty in few-shot
learning when it is combined with the proper regularization and calibration. Those two tools are
also portable to meta-learning methods to improve accuracy and calibration, but the improvement is
less signiﬁcant compared with that of transfer learning."
INTRODUCTION,0.018433179723502304,"We use stochastic weight averaging (SWA) (Izmailov et al., 2018), which is agnostic to loss function
types, as implicit regularization to improve the generalization capability of the representation. We
also shed light on that the effectiveness of SWA is due to its bias towards low-rank representation.
To address the issue of uncertainty quantiﬁcation, we ﬁne-tune appropriate linear layers during the
meta-test phase to get models with well-calibrated uncertainty. In MFRL, hierarchical Bayesian
linear models are used to properly capture the uncertainty from very limited training samples in
few-shot regression, whereas the softmax output is scaled with a temperature parameter to make the
few-shot classiﬁcation model well-calibrated. Our method is the ﬁrst one to achieve well-calibrated
few-shot models by only ﬁne-tuning probabilistic linear models in the meta-test phase, without any
learning mechanisms related to the meta-training or representation learning phase."
INTRODUCTION,0.020737327188940093,Our contributions in this work are summarized as follows:
INTRODUCTION,0.02304147465437788,"• We propose a transfer learning method that can handle both few-shot regression and clas-
siﬁcation problems with performance exceeding SOTA.
• For the ﬁrst time, we empirically ﬁnd the implicit regularization of SWA towards low-rank
representation, which is a useful property in transferring to few-shot tasks.
• The proposed method results in well-calibrated uncertainty in few-shot learning models
while preserving SOTA accuracy.
• The implicit regularization of SWA and temperature scaling factor can be applied to exist-
ing meta-learning methods to improve their accuracy and reliability in few-shot learning."
RELATED WORK,0.02534562211981567,"2
RELATED WORK"
RELATED WORK,0.027649769585253458,"Episodic meta-learning approaches can be categorized into metric-based and optimization-based
methods. Metric-based methods project input data to feature vectors through nonlinear embeddings
and compare their similarity to make the prediction. Examples of similarity metrics include the
weighted L1 metric (Koch et al., 2015), cosine similarity (Qi et al., 2018; Vinyals et al., 2016), and
Euclidean distance to class-mean representation (Snell et al., 2017). Instead of relying on predeﬁned
metrics, learnable similarity metrics are introduced to improve the few-shot classiﬁcation perfor-
mance (Oreshkin et al., 2018; Sung et al., 2018). Recent metric-based approaches focus on devel-
oping task-adaptive embeddings to improve few-shot classiﬁcation accuracy. Those task-adaptive
embeddings include attention mechanisms for feature transformation (Fei et al., 2021; Gidaris &
Komodakis, 2018; Ye et al., 2020; Zhang et al., 2021), graph neural networks (Garcia & Estrach,"
RELATED WORK,0.029953917050691243,Under review as a conference paper at ICLR 2022
RELATED WORK,0.03225806451612903,"2018), implicit class representation (Ravichandran et al., 2019), and task-dependent conditioning
(Oreshkin et al., 2018; Yoon et al., 2020; 2019). Although metric-based approaches achieve strong
performance in few-shot classiﬁcation, they cannot be directly applied to regression problems."
RELATED WORK,0.03456221198156682,"Optimization-based meta-learning approaches try to ﬁnd transferrable knowledge and adapt to new
tasks quickly.
An elegant and powerful meta-learning approach, termed model-agnostic meta-
learning (MAML), solves a bi-level optimization problem to ﬁnd good initialization of model pa-
rameters (Finn et al., 2017). However, MAML has a variety of issues, such as sensitivity to neural
network architectures, instability during training, arduous hyperparameter tuning, and high com-
putational cost. On this basis, some follow-up methods have been developed to simplify, stabilize
and improve the training process of MAML (Antoniou et al., 2018; Flennerhag et al., 2020; Lee &
Choi, 2018; Nichol et al., 2018; Park & Oliva, 2019). In practice, it is very challenging to learn
high-dimensional model parameters in a low-data regime. Latent embedding optimization (LEO)
attempts to learn low-dimensional representation to generate high-dimensional model parameters
(Rusu et al., 2019). Meanwhile, R2-D2 (Bertinetto et al., 2019) and MetaOptNet (Lee et al., 2019)
reduce the dimensionality of trainable model parameters by freezing feature extraction layers during
inner loop optimization. Note that the proposed method is fundamentally different from R2-D2 and
MetaOptNet because our method requires neither episodic meta-learning nor bi-level optimization."
RELATED WORK,0.03686635944700461,"Transfer learning approaches ﬁrst learn a feature extractor on all training data through standard
supervised learning, and then ﬁne-tune a linear predictor on top of the learned feature extractor in a
new task (Chen et al., 2019). However, vanilla transfer learning methods for few-shot learning do not
take extra steps to make the learned representation generalizing well to unseen meta-test tasks. Some
approaches in this paradigm are developed to improve the quality of representation and boost the
accuracy of few-shot classiﬁcation, including cooperative ensembles (Dvornik et al., 2019), knowl-
edge distillation (Tian et al., 2020), and auxiliary self-supervised learning (Mangla et al., 2020).
Nevertheless, those transfer learning methods are restricted to few-shot classiﬁcation. MFRL aims
to ﬁnd representation generalizing well from the perspective of low-rank representation learning,
which is supported by recent theoretical studies (Saunshi et al., 2021). Furthermore, MFLR is the
ﬁrst transfer learning method that can handle both few-shot regression and classiﬁcation problems
and make predictions with well-calibrated uncertainty."
BACKGROUND,0.03917050691244239,"3
BACKGROUND"
EPISODIC META-LEARNING,0.041474654377880185,"3.1
EPISODIC META-LEARNING"
EPISODIC META-LEARNING,0.04377880184331797,"In episodic meta-learning, the meta-training data contains T episodes or tasks, where the τ th episode
consists of data Dτ = {(xτ,j, yτ,j)}Nτ
j=1 with Nτ samples. Tasks and episodes are used interchange-
ably in the rest of the paper. Episodic meta-learning algorithms aim to ﬁnd common model param-
eters θ which can be quickly adapted to task-speciﬁc parameters φτ (τ = 1, ..., T ). For example,
MAML-type algorithms assume φτ is one or a few gradient steps away from θ (Finn et al., 2017;
2018; Grant et al., 2018; Yoon et al., 2018), while other meta-learning approaches assume that φτ
and θ share the parameters in the feature extractor and only differ in the top layer (Bertinetto et al.,
2019; Lee et al., 2019; Snell et al., 2017)."
STOCHASTIC WEIGHT AVERAGING,0.04608294930875576,"3.2
STOCHASTIC WEIGHT AVERAGING"
STOCHASTIC WEIGHT AVERAGING,0.04838709677419355,"The idea of stochastic weight averaging (SWA) along the trajectory of SGD goes back to
Polyak–Ruppert averaging (Polyak & Juditsky, 1992). Theoretically, weight averaging results in
faster convergence for linear models in supervised learning and reinforcement learning(Bach &
Moulines, 2013; Lakshminarayanan & Szepesvari, 2018). In deep learning, we are more interested
in tail stochastic weight averaging (Jain et al., 2018), which averages the weights after T training
epochs. The averaged model parameters θSWA can be computed by running s additional training
epochs using SGD"
STOCHASTIC WEIGHT AVERAGING,0.05069124423963134,θSWA = 1 s
STOCHASTIC WEIGHT AVERAGING,0.052995391705069124,"T +s
X"
STOCHASTIC WEIGHT AVERAGING,0.055299539170506916,"i=T +1
θi,
(1)"
STOCHASTIC WEIGHT AVERAGING,0.0576036866359447,Under review as a conference paper at ICLR 2022
STOCHASTIC WEIGHT AVERAGING,0.059907834101382486,"where θi denotes the model parameters at the end of the i-th epoch. SWA has been applied to
supervised learning of deep neural neural networks to achieve higher test accuracy (Izmailov et al.,
2018)."
METHODOLOGY,0.06221198156682028,"4
METHODOLOGY"
METHODOLOGY,0.06451612903225806,"The proposed method is a two-step learning algorithm: meta-free representation learning followed
by ﬁne-tuning. We employ SWA to make the learned representation low-rank and better generalize
to meta-test data. Given a meta-test task, a new top layer is ﬁne-tuned with few-shot samples to
obtain probabilistic models with well-calibrated uncertainty. Note that MFRL can be used for both
regression and classiﬁcation depending on the loss function. The pseudocode of MFRL is presented
in Appendix A.1."
REPRESENTATION LEARNING,0.06682027649769585,"4.1
REPRESENTATION LEARNING"
REPRESENTATION LEARNING,0.06912442396313365,"Common representation can be learned via maximization of the likelihood of all training data
with respect to θ rather than following episodic meta-learning.
To do so, we group the data
Dτ = {(xτ,j, yτ,j)}Nτ
j=1 from all meta-training tasks into a single dataset Dtr. Given aggregated
training data Dtr = {X, Y}, representation can be learned by maximizing the likelihood p (Dtr | θ)
with respect to θ. Let θ = [θf, W], where θf represents parameters in the feature extractor and W
denotes the parameters in the top linear layer. The feature extractor h(x) ∈Rp is a neural network
parameterized by θf and outputs a feature vector of dimension p. The speciﬁc form of the loss
function depends on whether the task is regression or classiﬁcation and can be given as follows:"
REPRESENTATION LEARNING,0.07142857142857142,"LRP (θ) = −log p (Dtr | θ) =
LMSE(θ),
regression
LCE(θ),
classiﬁcation where"
REPRESENTATION LEARNING,0.07373271889400922,"LMSE(θ) =
1
2N ′ T
X τ=1 Nτ
X j=1"
REPRESENTATION LEARNING,0.07603686635944701," 
yτ,j −w⊤
τ h (xτ,j)
2 ,
(2)"
REPRESENTATION LEARNING,0.07834101382488479,"LCE(θ) = − N′
X j=1 C
X"
REPRESENTATION LEARNING,0.08064516129032258,"c=1
yj,c log
exp(w⊤
c h(xj))
PC
c′=1 exp(w⊤
c′h(xj))
(3)"
REPRESENTATION LEARNING,0.08294930875576037,"For regression problems, the model learns T regression tasks (W = [w1, ..., wT ] ∈R(p+1)×T )
simultaneously using the loss function LMSE given in Eq. 2, whereas the model learns a C-class
classiﬁcation model 1 (W = [w1, ..., wC] ∈R(p+1)×C) for classiﬁcation problems using the loss
function LCE in Eq. 3. The loss function - either Eq. 2 or 3 - can be minimized through standard
stochastic gradient descent, where N ′ = PT
τ=1 Nτ is the total number of training samples."
REPRESENTATION LEARNING,0.08525345622119816,"Post-processing via SWA Minimizing the loss functions in Eq. 2 and 3 by SGD may not necessarily
result in representation that generalizes well to few-shot learning tasks in the meta-test set. The
last hidden layer of a modern deep neural network is high-dimensional and may contain spurious
features that over-ﬁt the meta-training data. Recent meta-learning theories indicate that better sample
complexity in learning a new task can be achieved via low-rank representation, whose singular
values decay faster (Saunshi et al., 2021). We aim to ﬁnd low-rank representation Φ = h(X)
without episodic meta-learning, which is equivalent to ﬁnding the conjugate kernel KC = ΦΦ⊤
with fast decaying eigenvalues. To link the representation with the parameter space, we can linearize
the neural network by the ﬁrst-order Taylor expansion at θT and get the ﬁnite width neural tangent
kernel (NTK) KNTK(X, X) = J(X)J(X)⊤, where J(X) = ∇θfθ(X) ∈RN′×|θ| is the Jacobian
matrix, and KNTK is a composite kernel containing KC (Fan & Wang, 2020). The distributions
of eigenvalues for KNTK and KC are empirically similar. Analyzing KNTK could shed light on
the properties of KC. In parallel, KNTK shares the same eigenvalues of the Gauss-Newton matrix
G =
1
N ′ J(X)⊤J(X). For linearized networks with squared loss, the Gauss-Newton matrix G well"
REPRESENTATION LEARNING,0.08755760368663594,"1C is the total number of classes in Dtr. Learning a C-class classiﬁcation model solves all possible tasks in
the meta-training dataset because each task Dτ only contains a subset of C classes."
REPRESENTATION LEARNING,0.08986175115207373,Under review as a conference paper at ICLR 2022
REPRESENTATION LEARNING,0.09216589861751152,"approximates the Hessian matrix H when y is well-described by fθ(x) (Martens, 2020). This is
the case when SGD converges to θT within a local minimum basin. A Hessian matrix with a lot
of small eigenvalues corresponds to a ﬂat minimum, where the loss function is less sensitive to the
perturbation of model parameters (Keskar et al., 2017). It is known that averaging the weights after
SGD convergence in a local minimum basin pushes θT towards the ﬂat side of the loss valley (He
et al., 2019). As a result, SWA could result in a faster decay of eigenvalues in the kernel matrix, and
thus low-rank representation. Our conjecture about SWA as implicit regularization towards low-rank
representation is empirically veriﬁed in Section 5."
FINE-TUNING,0.0944700460829493,"4.2
FINE-TUNING"
FINE-TUNING,0.0967741935483871,"After representation learning is complete, W is discarded and θf is frozen in a new few-shot task.
Given the learned representation, we train a new probabilistic top layer in a meta-test task using few-
shot samples. The new top layer will be conﬁgured differently depending on whether the few-shot
task is a regression or a classiﬁcation problem."
FINE-TUNING,0.09907834101382489,"In a few-shot regression task, we learn a new linear regression model y = w⊤h(x) + ϵ on a ﬁxed
feature extractor h(x) ∈Rp with few-shot training data D = {(xi, yi)}n
i=1, where w denotes the
model parameters and ϵ is Gaussian noise with zero mean and variance σ2. To avoid interpolation
on few-shot training data (n ≪p), a Gaussian prior p(w | λ) = Qp
i=0 N (wi | 0, λ) is placed over
w, where λ is the precision in the Gaussian prior. However, it is difﬁcult to obtain an appropriate
value for λ in a few-shot regression task because no validation data is available in D."
FINE-TUNING,0.10138248847926268,"Hierarchical Bayesian linear models can be used to obtain optimal regularization strength and
grounded uncertainty estimation using few-shot training data only.
To complete the speciﬁca-
tion of the hierarchical Bayesian model, the hyperpriors on λ and σ2 are deﬁned as p(λ) =
Gamma (λ | a, b) and p(σ−2) = Gamma(σ−2 | c, d), respectively. The hyper-priors become very
ﬂat and non-informative when a, b, c and d are set to very small values. The posterior over all latent
variables given the data is p
 
w, λ, σ2 | X, y

, where X = {x}n
i=1 and y = {yi}n
i=1. However, the
posterior distribution p
 
w, λ, σ2 | X, y

is intractable. The iterative optimization based approxi-
mate inference (Tipping, 2001) is chosen because it is highly efﬁcient. The point estimation for λ
and σ2 is obtained by maximizing the marginal likelihood function p
 
y | X, λ, σ2
. The posterior
of model parameters p
 
w | X, y, λ, σ2
is calculated using the estimated λ and σ2. Previous two
steps are repeated alternately until convergence."
FINE-TUNING,0.10368663594470046,The predictive distribution for a new sample x∗is
FINE-TUNING,0.10599078341013825,"p
 
y∗| x∗, X, y, λ, σ2
=
Z
p
 
y∗| x∗, w, σ2
p
 
w | X, y, λ, σ2
dw,
(4)"
FINE-TUNING,0.10829493087557604,"which can be computed analytically because both distributions on the right hand side of Eq. 4
are Gaussian. Consequently, hierarchical Bayesian linear models avoids over-ﬁtting on few-shot
training data and quantiﬁes predictive uncertainty."
FINE-TUNING,0.11059907834101383,"In a few-shot classiﬁcation task, a new logistic regression model is learned with the post-processed
representation.
A typical K-way n-shot classiﬁcation task D = {(xi, yi)}nK
i=1 consists of K
classes (different from meta-training classes) and n training samples per class. Minimizing an
un-regularized cross-entropy loss results in a signiﬁcantly over-conﬁdent classiﬁcation model be-
cause the norm of logistic regression model parameters W ∈R(p+1)×K becomes very large when
few-shot training samples can be perfectly separated in the setting nK ≪p. A weighted L2 regu-
larization term is added to the cross-entropy loss to mitigate the issue"
FINE-TUNING,0.11290322580645161,"L(W) = − nK
X i=1 K
X"
FINE-TUNING,0.1152073732718894,"c=1
yi,c log
exp(w⊤
c h(xi))
PK
c′=1 exp(w⊤
c′h(xi))
+ λ K
X"
FINE-TUNING,0.1175115207373272,"c=1
w⊤
c wc,
(5)"
FINE-TUNING,0.11981566820276497,"where λ is the regularization coefﬁcient, which affects the prediction accuracy and uncertainty. It
is difﬁcult to select an appropriate value of λ in each of the meta-test tasks due to the lack of
validation data in D. We instead treat λ as a global hyper-parameter so that the value of λ should
be determined based on the accuracy on meta-validation data. Note that the selected λ with high
validation accuracy does not necessarily lead to well calibrated classiﬁcation models. As such, we
introduce the temperature scaling factor (Guo et al., 2017) as another global hyper-parameter to"
FINE-TUNING,0.12211981566820276,Under review as a conference paper at ICLR 2022
FINE-TUNING,0.12442396313364056,"scale the softmax output. Given a test sample x∗, the predicted probability for class c becomes"
FINE-TUNING,0.12672811059907835,"pc =
exp(w⊤
c h(x∗)/T)
PK
c′=1 exp(w⊤
c′h(x∗)/T)
,
(6)"
FINE-TUNING,0.12903225806451613,"where T is the temperature scaling factor. In practice, we select the L2 regularization coefﬁcient λ
and the temperature scaling factor T as follows. At ﬁrst, we set T to 1, and do grid search on the
meta-validation data to ﬁnd the λ resulting in the highest meta-validation accuracy. However, ﬁne-
tuning λ does not ensure good calibration. It is the temperature scaling factor that ensures the good
uncertainty calibration. Similarly, we do grid search of T on the meta-validation set, and choose
the temperature scaling factor resulting in the lowest expected calibration error (Guo et al., 2017).
Note that different values of T do not affect the classiﬁcation accuracy because temperature scaling
is accuracy preserving."
EXPERIMENTS,0.1313364055299539,"5
EXPERIMENTS"
EXPERIMENTS,0.1336405529953917,"We follow the standard setup in few-shot learning literature. The model is trained on a meta-training
dataset and hyper-parameters are selected based on the performance on a meta-validation dataset.
The ﬁnal performance of the model is evaluated on a meta-test dataset. The proposed method is
applied to few-shot regression and classiﬁcation problems and compared against a wide range of
alternative methods."
FEW-SHOT REGRESSION RESULTS,0.1359447004608295,"5.1
FEW-SHOT REGRESSION RESULTS"
FEW-SHOT REGRESSION RESULTS,0.1382488479262673,"Sine waves (Finn et al., 2017) and head pose estimation (Patacchiola et al., 2020) datasets are used to
evaluate the performance of MFRL in few-shot regression. We use the same backbones in literature
(Patacchiola et al., 2020) to make fair comparisons. Details of the few-shot regression experiments
can be found in Appendix A.2."
FEW-SHOT REGRESSION RESULTS,0.14055299539170507,"The results for few-shot regression are summarized in Table 1. In the sine wave few-shot regression,
MFRL outperforms all meta-learning methods, demonstrating that high-quality representation can
be learned in supervised learning, without episodic meta-learning. Although DKT with a spectral
mixture (SM) kernel achieves high accuracy, the good performance should be attributed to the strong
inductive bias to periodic functions in the SM kernel (Wilson & Adams, 2013). Additional results for
MFRL with different activation functions are reported in Appendix A.3. In the head pose estimation
experiment, MFRL also achieves the best accuracy. In both few-shot regression problems, SWA
results in improved accuracy, suggesting that SWA can improve the quality of features and facilitate
the learning of downstream tasks. In Fig. 1, uncertainty is correctly estimated by the hierarchical
Bayesian linear model with learned features using just 10 training samples."
FEW-SHOT REGRESSION RESULTS,0.14285714285714285,"4
2
0
2
4 1.5 1.0 0.5 0.0 0.5 1.0 1.5 MAML"
FEW-SHOT REGRESSION RESULTS,0.14516129032258066,"noisy train sample
predicted
true function"
FEW-SHOT REGRESSION RESULTS,0.14746543778801843,"4
2
0
2
4 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0"
FEW-SHOT REGRESSION RESULTS,0.1497695852534562,Bayesian MAML
FEW-SHOT REGRESSION RESULTS,0.15207373271889402,"noisy train sample
predicted
true function
Uncertainty"
FEW-SHOT REGRESSION RESULTS,0.1543778801843318,"4
2
0
2
4 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0"
FEW-SHOT REGRESSION RESULTS,0.15668202764976957,Deep kernel transfer (SM kernel)
FEW-SHOT REGRESSION RESULTS,0.15898617511520738,"noisy train sample
predicted
true function
Uncertainty"
FEW-SHOT REGRESSION RESULTS,0.16129032258064516,"4
2
0
2
4 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0"
FEW-SHOT REGRESSION RESULTS,0.16359447004608296,Hierarchical Bayesian linear regression
FEW-SHOT REGRESSION RESULTS,0.16589861751152074,"noisy train sample
predicted
true function
Uncertainty"
FEW-SHOT REGRESSION RESULTS,0.16820276497695852,"Figure 1: Sine wave regression and uncertainty
quantiﬁcation (10 training samples).
The true
and the estimated (by MFRL) standard deviation
of data generation noise are 0.1, and 0.093."
FEW-SHOT REGRESSION RESULTS,0.17050691244239632,"Sine wave (2-layer MLP)
MSE
MAML (Finn et al., 2017)
0.67 ± 0.06
Bayesian MAML (Yoon et al., 2018)
0.54 ± 0.05
ALPaCA (Harrison et al., 2018)
0.14 ± 0.09
R2D2 (Bertinetto et al., 2019)
0.46 ± NA
DKT+RBF (Patacchiola et al., 2020)
1.38 ± 0.03
DKT+Spectral (Patacchiola et al., 2020)
0.08 ± 0.06
MFRL (w.o. SWA)
0.023 ± 0.016
MFRL
0.016 ± 0.008
Head pose (3-layer Conv Net)
MSE
MAML (Finn et al., 2017)
0.21 ± 0.01
Bayesian MAML (Yoon et al., 2018)
0.18 ± 0.01
DKT+Spectral (Patacchiola et al., 2020)
0.10 ± 0.01
MFRL (w.o. SWA)
0.033 ± 0.006
MFRL
0.027 ± 0.005"
FEW-SHOT REGRESSION RESULTS,0.1728110599078341,"Table 1: 10-shot regression on sine waves and
head pose estimation."
FEW-SHOT REGRESSION RESULTS,0.17511520737327188,Under review as a conference paper at ICLR 2022
FEW-SHOT CLASSIFICATION RESULTS,0.1774193548387097,"5.2
FEW-SHOT CLASSIFICATION RESULTS"
FEW-SHOT CLASSIFICATION RESULTS,0.17972350230414746,"We conduct few-shot classiﬁcation experiments on four widely used few-shot image recognition
benchmarks: miniImageNet (Ravi & Larochelle, 2017), tieredImageNet (Ren et al., 2018), CIFAR-
FS (Bertinetto et al., 2019), and FC100 (Oreshkin et al., 2018). In addition, we test our approach on
a cross-domain few-shot classiﬁcation task from the miniImageNet to CUB. The experiment details
about the few-shot classiﬁcation datasets can be found in Appendix A.2. The proposed method is
applied to three widely used network architectures: ResNet-12 (Lee et al., 2019; Ravichandran et al.,
2019), wide ResNet (WRN-28-10) (Dhillon et al., 2020; Rusu et al., 2019), and a 4-layer convolu-
tional neural network with 64 channels (Chen et al., 2019; Patacchiola et al., 2020) (in Appendix
A.3)."
FEW-SHOT CLASSIFICATION RESULTS,0.18202764976958524,Table 2: Few-shot classiﬁcation results on miniImageNet and tieredImageNet.
FEW-SHOT CLASSIFICATION RESULTS,0.18433179723502305,"Method
Backbone
miniImageNet 5-way
tieredImageNet 5-way"
-SHOT,0.18663594470046083,"1-shot
5-shot
1-shot
5-shot"
-SHOT,0.1889400921658986,"Matching Net (Vinyals et al., 2016)
ResNet-12
63.08 ± 0.80
75.99 ± 0.60
68.50 ± 0.92
80.60 ± 0.71"
-SHOT,0.1912442396313364,"Proto Net (Snell et al., 2017)
ResNet-12
60.37 ± 0.83
78.02 ± 0.57
65.65 ± 0.92
83.40 ± 0.65"
-SHOT,0.1935483870967742,"Proto Net + SWA
ResNet-12
63.51 ± 0.82
81.98 ± 0.58
67.95 ± 0.85
84.76 ± 0.66"
-SHOT,0.195852534562212,"MAML (Finn et al., 2017)
ResNet-12
56.58 ± 1.84
70.85 ± 0.91
-
-"
-SHOT,0.19815668202764977,"MAML + SWA
ResNet-12
58.21 ± 1.86
72.47 ± 0.87
-
-"
-SHOT,0.20046082949308755,"AdaResNet (Munkhdalai et al., 2018)
ResNet-12
56.88 ± 0.62
71.94 ± 0.57
-
-"
-SHOT,0.20276497695852536,"TADAM (Oreshkin et al., 2018)
ResNet-12
58.50 ± 0.30
76.70 ± 0.30
-
-"
-SHOT,0.20506912442396313,"Baseline++ (Chen et al., 2019)
ResNet-12
60.83 ± 0.81
77.81 ± 0.76
68.64 ± 0.86
80.47 ± 0.67"
-SHOT,0.2073732718894009,"Baseline++ + SWA
ResNet-12
65.72 ± 0.80
81.26 ± 0.68
70.01 ± 0.82
84.39 ± 0.64"
-SHOT,0.20967741935483872,"TapNet (Yoon et al., 2019)
ResNet-12
61.65 ± 0.15
76.36 ± 0.10
63.08 ± 0.15
80.26 ± 0.12"
-SHOT,0.2119815668202765,"MetaOptNet (Lee et al., 2019)
ResNet-12
62.64 ± 0.61
78.63 ± 0.46
65.99 ± 0.72
81.56 ± 0.53"
-SHOT,0.21428571428571427,"Ensemble (Dvornik et al., 2019)
ResNet-18
59.48 ± 0.65
75.62 ± 0.42
-
-"
-SHOT,0.21658986175115208,"DSN (Simon et al., 2020)
ResNet-12
62.64 ± 0.66
78.83 ± 0.45
66.22 ± 0.75
82.79 ± 0.48"
-SHOT,0.21889400921658986,"DKT (Patacchiola et al., 2020)
ResNet-12
61.29 ± 0.57
76.25 ± 0.51
67.21 ± 0.52
79.69 ± 0.53"
-SHOT,0.22119815668202766,"FEAT (Ye et al., 2020)
ResNet-12
66.78 ± 0.20
82.05 ± 0.14
70.80 ± 0.23
84.79 ± 0.16"
-SHOT,0.22350230414746544,"DeepEMD (Zhang et al., 2020)
ResNet-12
65.91 ± 0.82
82.41 ± 0.56
71.16 ± 0.87
86.03 ± 0.58"
-SHOT,0.22580645161290322,"Distill (Tian et al., 2020)
ResNet-12
64.82 ± 0.60
82.14 ± 0.43
71.52 ± 0.69
86.03 ± 0.49"
-SHOT,0.22811059907834103,"MFRL (w.o. SWA)
ResNet-12
62.27 ± 0.86
80.23 ± 0.57
70.03 ± 0.77
84.42 ± 0.64"
-SHOT,0.2304147465437788,"MFRL
ResNet-12
67.18 ± 0.79
83.81 ± 0.53
71.58 ± 0.79
86.87 ± 0.62"
-SHOT,0.23271889400921658,"LEO (Rusu et al., 2019)
WRN-28-10
61.76 ± 0.08
77.59 ± 0.12
66.33 ± 0.03
81.44 ± 0.09"
-SHOT,0.2350230414746544,"Fine-tune (Dhillon et al., 2020)
WRN-28-10
57.73 ± 0.62
78.17 ± 0.49
66.58 ± 0.70
85.55 ± 0.48"
-SHOT,0.23732718894009217,"Inductive SIB (Hu et al., 2020)
WRN-28-10
60.12 ± 0.56
78.17 ± 0.35
69.20 ± 0.58
84.96 ± 0.36"
-SHOT,0.23963133640552994,"MetaFun (Xu et al., 2020)
WRN-28-10
64.13 ± 0.13
80.82 ± 0.17
67.27 ± 0.14
83.28 ± 0.12"
-SHOT,0.24193548387096775,"MFRL (w.o. SWA)
WRN-28-10
61.83 ± 0.82
80.12 ± 0.88
69.89 ± 0.79
84.42 ± 0.66"
-SHOT,0.24423963133640553,"MFRL
WRN-28-10
66.42 ± 0.80
82.26 ± 0.61
71.47 ± 0.84
86.34 ± 0.65"
-SHOT,0.2465437788018433,Table 3: Few-shot classiﬁcation results on CIFAR-FS and FC100.
-SHOT,0.2488479262672811,"Method
Backbone
CIFAR-FS 5-way
FC100 5-way"
-SHOT,0.2511520737327189,"1-shot
5-shot
1-shot
5-shot"
-SHOT,0.2534562211981567,"Proto Net (Snell et al., 2017)
ResNet-12
72.2 ± 0.7
83.5 ± 0.5
41.5 ± 0.7
57.0 ± 0.7"
-SHOT,0.2557603686635945,"TADAM (Oreshkin et al., 2018)
ResNet-12
-
-
40.1 ± 0.4
56.1 ± 0.4"
-SHOT,0.25806451612903225,"Baseline++ (Chen et al., 2019)
ResNet-12
72.2 ± 0.9
84.2 ± 0.6
43.1 ± 0.7
55.7 ± 0.7"
-SHOT,0.26036866359447003,"MetaOptNet (Lee et al., 2019)
ResNet-12
72.8 ± 0.7
85.0 ± 0.5
41.1 ± 0.6
55.5 ± 0.6"
-SHOT,0.2626728110599078,"MTL (Sun et al., 2019)
ResNet-12
-
-
45.1 ± 1.8
57.6 ± 0.9"
-SHOT,0.26497695852534564,"Shot-free (Ravichandran et al., 2019)
ResNet-12
69.1 ± NA
84.7 ± NA
-
-"
-SHOT,0.2672811059907834,"TEAM (Qiao et al., 2019)
ResNet-12
70.4 ± NA
81.3 ± NA
-
-"
-SHOT,0.2695852534562212,"SIB (Hu et al., 2020)
ResNet-12
70.0 ± 0.5
83.5 ± 0.4
-
-"
-SHOT,0.271889400921659,"DSN (Simon et al., 2020)
ResNet-12
72.3 ± 0.8
85.1 ± 0.6
-
-"
-SHOT,0.27419354838709675,"MABAS (Kim et al., 2020)
ResNet-12
73.5 ± 0.9
85.6 ± 0.6
42.3 ± 0.7
58.1 ± 0.7"
-SHOT,0.2764976958525346,"Distill (Tian et al., 2020)
ResNet-12
73.9 ± 0.8
86.9 ± 0.5
44.6 ± 0.7
60.9 ± 0.6"
-SHOT,0.27880184331797236,"MFRL (w.o. SWA)
ResNet-12
71.4 ± 0.8
86.1 ± 0.5
42.5 ± 0.7
59.1 ± 0.7"
-SHOT,0.28110599078341014,"MFRL
ResNet-12
74.0 ± 0.8
87.4 ± 0.6
45.3 ± 0.8
61.1 ± 0.7"
-SHOT,0.2834101382488479,"Fine-tune (Dhillon et al., 2020)
WRN-28-10
68.7 ± 0.7
86.1 ± 0.6
38.2 ± 0.5
57.2 ± 0.6"
-SHOT,0.2857142857142857,"MFRL (w.o. SWA)
WRN-28-10
71.7 ± 0.9
86.2 ± 0.9
41.5 ± 0.7
57.3 ± 0.7"
-SHOT,0.2880184331797235,"MFRL
WRN-28-10
76.7 ± 0.9
88.6 ± 0.5
45.1 ± 0.8
61.0 ± 0.8"
-SHOT,0.2903225806451613,"The results of the proposed method and previous SOTA methods using similar backbones are re-
ported in Table 2 and 3. The proposed method achieves the best performance in most of the exper-
iments when compared with previous SOTA methods. Our method is closely related to Baseline++
(Chen et al., 2019) and ﬁne-tuning on logits (Dhillon et al., 2020). Baseline++ normalizes both
classiﬁcation weights and features, while the proposed method only normalizes features. It allows
our method to ﬁnd a more accurate model in a more ﬂexible hypothesis space, given high-quality
representation. Compared with ﬁne-tuning on logits, our method obtains better results by learning"
-SHOT,0.2926267281105991,Under review as a conference paper at ICLR 2022
-SHOT,0.29493087557603687,"a new logistic regression model on features, which store richer information about the data. Some
approaches pretrain a C-class classiﬁcation model on all training data and then apply highly sophis-
ticated meta-learning techniques to the pretrained model to achieve SOTA performance (Rusu et al.,
2019; Sun et al., 2019). Our approach with SWA outperforms those pretrained-then-meta-learned
models, which demonstrates that SWA obtains high-quality representation that generalizes well to
unseen tasks. Compared with improving representation quality for few-shot classiﬁcation via self-
distillation (Tian et al., 2020), the computational cost of SWA is signiﬁcantly smaller because it
does not require training models from scratch multiple times. Moreover, SWA can be applied to ﬁnd
good representation for both few-shot regression and classiﬁcation, while previous transfer learning
approaches can only handle few-shot classiﬁcation problems (Mangla et al., 2020; Tian et al., 2020)."
-SHOT,0.29723502304147464,"MFRL is also applied to the cross-domain few-shot classiﬁcation task as summarized in Table 4.
MFRL outperforms other methods in this challenging task, indicating that the learned representation
has strong generalization capability. We use the same hyperparameters (training epochs, learning
rate, learning rate in SWA, SWA epoch, etc.) as in Table 2. The strong results indicate that MFRL
is robust to hyperparameter choice. Surprisingly, meta-learning methods with adaptive embeddings
do not outperform simple transfer learning methods like Baseline++ when the domain gap between
base classes and novel classes is large. We notice that Tian et al. (2020) also reports similar results
that transfer learning methods show superior performance on a large-scale cross-domain few-shot
classiﬁcation dataset. We still believe that adaptive embeddings should be helpful when the domain
gap between base and novel classes is large. Nevertheless, how to properly train a model to obtain
useful adaptive embeddings in novel tasks is an open question."
-SHOT,0.2995391705069124,Table 4: Cross-domain few-shot classiﬁcation results on miniImageNet to CUB.
-SHOT,0.30184331797235026,"Method
Backbone
miniImageNet to CUB 5-way"
-SHOT,0.30414746543778803,"1-shot
5-shot"
-SHOT,0.3064516129032258,"MAML (Finn et al., 2017)
WRN-28-10
39.06 ± 0.47
55.04 ± 0.42"
-SHOT,0.3087557603686636,"LEO (Rusu et al., 2019)
WRN-28-10
41.45 ± 0.54
56.66 ± 0.48"
-SHOT,0.31105990783410137,"MTL (Sun et al., 2019)
WRN-28-10
43.15 ± 0.44
56.89 ± 0.41"
-SHOT,0.31336405529953915,"Matching Net (Vinyals et al., 2016)
WRN-28-10
42.04 ± 0.57
53.08 ± 0.45"
-SHOT,0.315668202764977,"SIB (Hu et al., 2020)
WRN-28-10
43.27 ± 0.44
59.94 ± 0.42"
-SHOT,0.31797235023041476,"Baseline (Chen et al., 2019)
WRN-28-10
42.89 ± 0.41
62.12 ± 0.40"
-SHOT,0.32027649769585254,"Baseline++ (Chen et al., 2019)
WRN-28-10
42.12 ± 0.39
60.21 ± 0.39"
-SHOT,0.3225806451612903,"MFRL (w.o. SWA)
WRN-28-10
43.68 ± 0.47
63.86 ± 0.42"
-SHOT,0.3248847926267281,"MFRL
WRN-28-10
46.98 ± 0.51
66.92 ± 0.42"
EFFECTIVE RANK OF THE REPRESENTATION,0.3271889400921659,"5.3
EFFECTIVE RANK OF THE REPRESENTATION"
EFFECTIVE RANK OF THE REPRESENTATION,0.3294930875576037,"The rank of representation deﬁnes the number of independent bases. For deep learning, noise in
gradients and numerical imprecision can cause the resulting matrix to be full-rank. Therefore, simply
counting the number of non-zero singular values may not be an effective way to measure the rank
of the representation. To compare the effective ranks, we plot the normalized singular values of
the representation of meta-test data in Fig. 2, where the representation with SWA has a faster
decay in singular values, thus indicating the lower effective rank of the presentation with SWA.
The results empirically verify our conjecture that SWA is an implicit regularizer towards low-rank
representation."
EFFECTIVE RANK OF THE REPRESENTATION,0.3317972350230415,"0
100
200
300
400
500
600
Singular value index i 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.33410138248847926,"Normalized singular value 
i/
max"
EFFECTIVE RANK OF THE REPRESENTATION,0.33640552995391704,miniImageNet
EFFECTIVE RANK OF THE REPRESENTATION,0.3387096774193548,"w.o. SWA: 
ilog
i = 68.49"
EFFECTIVE RANK OF THE REPRESENTATION,0.34101382488479265,"SWA: 
ilog
i = 58.49"
EFFECTIVE RANK OF THE REPRESENTATION,0.3433179723502304,"0
100
200
300
400
500
600
Singular value index i 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.3456221198156682,"Normalized singular value 
i/
max"
EFFECTIVE RANK OF THE REPRESENTATION,0.347926267281106,tieredImageNet
EFFECTIVE RANK OF THE REPRESENTATION,0.35023041474654376,"w.o. SWA: 
ilog
i = 83.98"
EFFECTIVE RANK OF THE REPRESENTATION,0.35253456221198154,"SWA: 
ilog
i = 80.01"
EFFECTIVE RANK OF THE REPRESENTATION,0.3548387096774194,"0
100
200
300
400
500
600
Singular value index i 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.35714285714285715,"Normalized singular value 
i/
max"
EFFECTIVE RANK OF THE REPRESENTATION,0.35944700460829493,CIFAR-FS
EFFECTIVE RANK OF THE REPRESENTATION,0.3617511520737327,"w.o. SWA: 
ilog
i = 40.94"
EFFECTIVE RANK OF THE REPRESENTATION,0.3640552995391705,"SWA: 
ilog
i = 33.63"
EFFECTIVE RANK OF THE REPRESENTATION,0.3663594470046083,"0
100
200
300
400
500
600
Singular value index i 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.3686635944700461,"Normalized singular value 
i/
max FC100"
EFFECTIVE RANK OF THE REPRESENTATION,0.3709677419354839,"w.o. SWA: 
ilog
i = 40.22"
EFFECTIVE RANK OF THE REPRESENTATION,0.37327188940092165,"SWA: 
ilog
i = 34.34"
EFFECTIVE RANK OF THE REPRESENTATION,0.37557603686635943,"Figure 2:
Normalized singular values for representation with and without SWA. The metric
−P ¯σi log ¯σi is used to measure the effective rank of the representation, where ¯σi = σi/σmax.
Faster decay in singular values indicates that fewer dimensions capture the most variation in all
dimensions, thus lower effective rank."
EFFECTIVE RANK OF THE REPRESENTATION,0.3778801843317972,Under review as a conference paper at ICLR 2022
EFFECTIVE RANK OF THE REPRESENTATION,0.38018433179723504,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.3824884792626728,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.3847926267281106,"ECE: 0.0518
MCE: 0.0844
BRI: 0.3288 MAML"
EFFECTIVE RANK OF THE REPRESENTATION,0.3870967741935484,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.38940092165898615,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.391705069124424,"ECE: 0.0435
MCE: 0.0744
BRI: 0.3230"
EFFECTIVE RANK OF THE REPRESENTATION,0.39400921658986177,Proto Net
EFFECTIVE RANK OF THE REPRESENTATION,0.39631336405529954,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.3986175115207373,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.4009216589861751,"ECE: 0.0338
MCE: 0.0552
BRI: 0.3200"
EFFECTIVE RANK OF THE REPRESENTATION,0.4032258064516129,Matching Net
EFFECTIVE RANK OF THE REPRESENTATION,0.4055299539170507,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.4078341013824885,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.41013824884792627,"ECE: 0.3319
MCE: 0.4113
BRI: 0.4308"
EFFECTIVE RANK OF THE REPRESENTATION,0.41244239631336405,MFRL uncalibrated
EFFECTIVE RANK OF THE REPRESENTATION,0.4147465437788018,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.41705069124423966,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.41935483870967744,"ECE: 0.0077
MCE: 0.0369
BRI: 0.2777"
EFFECTIVE RANK OF THE REPRESENTATION,0.4216589861751152,MAML Calibrated
EFFECTIVE RANK OF THE REPRESENTATION,0.423963133640553,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.42626728110599077,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.42857142857142855,"ECE: 0.0105
MCE: 0.0335
BRI: 0.2720"
EFFECTIVE RANK OF THE REPRESENTATION,0.4308755760368664,Proto Net Calibrated
EFFECTIVE RANK OF THE REPRESENTATION,0.43317972350230416,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.43548387096774194,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.4377880184331797,"ECE: 0.0080
MCE: 0.0437
BRI: 0.2733"
EFFECTIVE RANK OF THE REPRESENTATION,0.4400921658986175,Matching Net Calibrated
EFFECTIVE RANK OF THE REPRESENTATION,0.4423963133640553,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
EFFECTIVE RANK OF THE REPRESENTATION,0.4447004608294931,Accuracy
EFFECTIVE RANK OF THE REPRESENTATION,0.4470046082949309,"ECE: 0.0040
MCE: 0.0135
BRI: 0.2791 MFRL"
EFFECTIVE RANK OF THE REPRESENTATION,0.44930875576036866,"Figure 3: Study on the temperature scaling factor for 5-way 5-shot classiﬁcation using ResNet-12
for the proposed MFRL and existing episodic meta learning methods. Uncalibrated models are in
the ﬁrst row, and calibrated models with the temperature scaling factor are in the second row."
FEW-SHOT CLASSIFICATION RELIABILITY,0.45161290322580644,"5.4
FEW-SHOT CLASSIFICATION RELIABILITY"
FEW-SHOT CLASSIFICATION RELIABILITY,0.4539170506912442,"The proposed method not only achieves high accuracy in few-shot classiﬁcation but also makes the
classiﬁcation uncertainty well-calibrated. A reliability diagram can be used to check model calibra-
tion visually, which plots an identity function between prediction accuracy and conﬁdence when the
model is perfectly calibrated (DeGroot & Fienberg, 1983). Fig. 3 shows the classiﬁcation reliability
diagrams along with widely used metrics for uncertainty calibration, including expected calibration
error (ECE) (Guo et al., 2017), maximum calibration error (MCE) (Naeini et al., 2015), and Brier
score (BRI) (Brier, 1950). ECE measures the average binned difference between conﬁdence and
accuracy, while MCE measures the maximum difference. BRI is the squared error between the
predicted probabilities and one-hot labels. MAML is over-conﬁdent because tuning a deep neural
network on few-shot data is prone to over-ﬁtting. Meanwhile, Proto Net and Matching Net are better
calibrated than MAML because they do not ﬁne-tune the entire network during testing. Neverthe-
less, they are still slightly over-conﬁdent. The results indicate that MFRL with a global temperature
scaling factor can learn well-calibrated models from very limited training samples."
APPLICATION IN META-LEARNING,0.45622119815668205,"5.5
APPLICATION IN META-LEARNING"
APPLICATION IN META-LEARNING,0.45852534562211983,"Meanwhile, we also apply SWA to episodic meta-learning methods, such Proto Net, MAML and
Matching Net, to improve their classiﬁcation accuracy. The results in Table 5 indicate that SWA can
improve the few-shot classiﬁcation accuracy in both transfer learning and episodic meta-learning.
SWA is orthogonal to the learning paradigm and model architecture. Thus, SWA can be applied to
a wide range of few-shot learning methods to improve accuracy."
APPLICATION IN META-LEARNING,0.4608294930875576,Table 5: Application of SWA on meta-learning methods for the miniImageNet dataset
APPLICATION IN META-LEARNING,0.4631336405529954,"Method
Proto Net
MAML
Matching Net
1-shot
5-shot
1-shot
5-shot
1-shot
5-shot
w.o. SWA
60.37
78.02
56.58
70.85
63.08
75.99
SWA
63.51
81.98
58.21
72.47
63.76
76.78"
APPLICATION IN META-LEARNING,0.46543778801843316,"Furthermore, the temperature scaling factor can be applied to calibrate meta-learning methods, in-
cluding MAML, Proto Net, and Matching Net. The reliability diagrams in Fig. 3 indicate that
the temperature scaling factor not only calibrates classiﬁcation uncertainty of transfer learning ap-
proaches, such as the proposed MFRL, but also makes the classiﬁcation uncertainty well-calibrated
in episodic meta-learning methods. Therefore, the temperature scaling factor can be applied to a
wide range of few-shot classiﬁcation methods to get well-calibrated uncertainty, while preserving
the classiﬁcation accuracy."
APPLICATION IN META-LEARNING,0.46774193548387094,Under review as a conference paper at ICLR 2022
DISCUSSION,0.4700460829493088,"6
DISCUSSION"
DISCUSSION,0.47235023041474655,"SWA has been applied to supervised learning of deep neural networks (Izmailov et al., 2018; Athi-
waratkun et al., 2019) and its effectiveness was attributed to convergence to a solution on the ﬂat
side of an asymmetric loss valley (He et al., 2019). However, it does not explain the effectiveness
of SWA in few-shot learning because the meta-training and meta-testing losses are not comparable
after the top layer is retrained by the few-shot support data in a meta-test task. The effectiveness
of SWA in few-shot learning must be related to the property of the representation. Although our
results empirically demonstrate that SWA results in low-rank representation, further research about
their connection is needed."
DISCUSSION,0.47465437788018433,"Explicit regularizers can also be used to obtain simple input-output functions in deep neural net-
works and low-rank representation, including L1 regularization, nuclear norm, spectral norm, and
Frobenius norm (Bartlett et al., 2017; Neyshabur et al., 2018; Sanyal et al., 2020). However, some
of those explicit regularizers are not compatible with standard SGD training or are computationally
expensive. In addition, it is difﬁcult to choose the appropriate strength of explicit regularization. Too
strong explicit regularization can bias towards simple solutions that do not ﬁt the data. In compar-
ison, SWA is an implicit regularizer that is completely compatible with the standard SGD training
without much extra computational cost. Thus, it can be easily combined with transfer learning and
meta-learning to obtain more accurate few-shot learning models. In parallel, SWA is also robust
to the choice of the hyperparameters - the learning rate and training epochs in the SWA stage (see
details in Appendix A.4)."
CONCLUSIONS,0.4769585253456221,"7
CONCLUSIONS"
CONCLUSIONS,0.4792626728110599,"In this article, we propose MFRL to obtain accurate and reliable few-shot learning models. SWA is
an implicit regularizer towards low-rank representation, which generalizes well to unseen meta-test
tasks. The proposed method can be applied to both classiﬁcation and regression tasks. Extensive
experiments show that our method not only outperforms other SOTA methods on various datasets
but also correctly quantiﬁes the uncertainty in prediction."
REFERENCES,0.4815668202764977,REFERENCES
REFERENCES,0.4838709677419355,"Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. In International
Conference on Learning Representations, 2018."
REFERENCES,0.4861751152073733,"Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. There are many con-
sistent explanations of unlabeled data: Why you should average. In International Conference on
Learning Representations, 2019."
REFERENCES,0.48847926267281105,"Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con-
vergence rate o (1/n). In Proceedings of the 26th International Conference on Neural Information
Processing Systems-Volume 1, pp. 773–781, 2013."
REFERENCES,0.49078341013824883,"Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. Advances in Neural Information Processing Systems, 30:6240–6249, 2017."
REFERENCES,0.4930875576036866,"Edmon Begoli, Tanmoy Bhattacharya, and Dimitri Kusnezov. The need for uncertainty quantiﬁ-
cation in machine-assisted medical decision making. Nature Machine Intelligence, 1(1):20–23,
2019."
REFERENCES,0.49539170506912444,"Y Bengio, S Bengio, and J Cloutier. Learning a synaptic learning rule. In IJCNN-91-Seattle Inter-
national Joint Conference on Neural Networks, volume 2, pp. 969–vol. IEEE, 1991."
REFERENCES,0.4976958525345622,"Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differen-
tiable closed-form solvers. In International Conference on Learning Representations, 2019."
REFERENCES,0.5,"Glenn W Brier. Veriﬁcation of forecasts expressed in terms of probability. Monthly weather review,
78(1):1–3, 1950."
REFERENCES,0.5023041474654378,Under review as a conference paper at ICLR 2022
REFERENCES,0.5046082949308756,"Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer
look at few-shot classiﬁcation. In International Conference on Learning Representations, 2019."
REFERENCES,0.5069124423963134,"Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. Journal
of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12–22, 1983."
REFERENCES,0.5092165898617511,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.511520737327189,"Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for
few-shot image classiﬁcation. In International Conference on Learning Representations, 2020."
REFERENCES,0.5138248847926268,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representations, 2021."
REFERENCES,0.5161290322580645,"Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Diversity with cooperation: Ensemble methods
for few-shot classiﬁcation. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 3723–3731, 2019."
REFERENCES,0.5184331797235023,"Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-
width neural networks. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5207373271889401,"Nanyi Fei, Zhiwu Lu, Tao Xiang, and Songfang Huang. {MELR}: Meta-learning via modeling
episode-level relationships for few-shot learning. In International Conference on Learning Rep-
resentations, 2021."
REFERENCES,0.5230414746543779,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126–1135. JMLR. org, 2017."
REFERENCES,0.5253456221198156,"Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In Pro-
ceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
9537–9548, 2018."
REFERENCES,0.5276497695852534,"Sebastian Flennerhag, Andrei A Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia
Hadsell. Meta-learning with warped gradient descent. In International Conference on Learning
Representations, 2020."
REFERENCES,0.5299539170506913,"Victor Garcia and Joan Bruna Estrach.
Few-shot learning with graph neural networks.
In 6th
International Conference on Learning Representations, ICLR 2018, 2018."
REFERENCES,0.532258064516129,"Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–
4375, 2018."
REFERENCES,0.5345622119815668,"Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Gold-
stein. Unraveling meta-learning: Understanding feature representations for few-shot tasks. In
Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Ma-
chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 3607–3616, Vir-
tual, 13–18 Jul 2020. PMLR."
REFERENCES,0.5368663594470046,"Shaogang Gong, Stephen McKenna, and John J Collins. An investigation into face pose distribu-
tions. In Proceedings of the Second International Conference on Automatic Face and Gesture
Recognition, pp. 265–270. IEEE, 1996."
REFERENCES,0.5391705069124424,"J Gordon, J Bronskill, M Bauer, S Nowozin, and RE Turner. Meta-learning probabilistic inference
for prediction. In International Conference on Learning Representations (ICLR 2019). OpenRe-
view. net, 2019."
REFERENCES,0.5414746543778802,Under review as a conference paper at ICLR 2022
REFERENCES,0.543778801843318,"Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifﬁths. Recasting gradient-
based meta-learning as hierarchical bayes. In International Conference on Learning Representa-
tions, 2018."
REFERENCES,0.5460829493087558,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017."
REFERENCES,0.5483870967741935,"James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efﬁcient online
bayesian regression. In International Workshop on the Algorithmic Foundations of Robotics, pp.
318–337. Springer, 2018."
REFERENCES,0.5506912442396313,"Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and ﬂat local minima.
In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B.
Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada, pp. 2549–2560, 2019."
REFERENCES,0.5529953917050692,"Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths
in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593–1623, 2014."
REFERENCES,0.5552995391705069,"Shell Xu Hu, Pablo Garcia Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil Lawrence,
and Andreas Damianou. Empirical bayes transductive meta-learning with synthetic gradients. In
International Conference on Learning Representations, 2020."
REFERENCES,0.5576036866359447,"Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.
Averaging weights leads to wider optima and better generalization. In 34th Conference on Un-
certainty in Artiﬁcial Intelligence 2018, UAI 2018, pp. 876–885. Association For Uncertainty in
Artiﬁcial Intelligence (AUAI), 2018."
REFERENCES,0.5599078341013825,"Prateek Jain, Sham Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing
stochastic gradient descent for least squares regression: mini-batching, averaging, and model
misspeciﬁcation. Journal of Machine Learning Research, 18, 2018."
REFERENCES,0.5622119815668203,"Ghassen Jerfel, Erin Grant, Thomas L Grifﬁths, and Katherine Heller. Reconciling meta-learning
and continual learning with online mixtures of tasks. Advances in Neural Information Processing
Systems, 32, 2019."
REFERENCES,0.5645161290322581,"Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail
Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima.
In 5th International Conference on Learning Representations, ICLR 2017, 2017."
REFERENCES,0.5668202764976958,"Jaekyeom Kim, Hyoungseok Kim, and Gunhee Kim. Model-agnostic boundary-adversarial sam-
pling for test-time generalization in few-shot learning.
In European conference on computer
vision. Springer, 2020."
REFERENCES,0.5691244239631337,"Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015."
REFERENCES,0.5714285714285714,"Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How
far does constant step-size and iterate averaging go?
In International Conference on Artiﬁcial
Intelligence and Statistics, pp. 1347–1355. PMLR, 2018."
REFERENCES,0.5737327188940092,"Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 10657–10665, 2019."
REFERENCES,0.576036866359447,"Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and
subspace. In International Conference on Machine Learning, pp. 2927–2936, 2018."
REFERENCES,0.5783410138248848,"Puneet Mangla, Mayank Singh, Abhishek Sinha, Nupur Kumari, Vineeth N Balasubramanian, and
Balaji Krishnamurthy. Charting the right manifold: Manifold mixup for few-shot learning. In
2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 2207–2216.
IEEE, 2020."
REFERENCES,0.5806451612903226,Under review as a conference paper at ICLR 2022
REFERENCES,0.5829493087557603,"James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
Learning Research, 21:1–76, 2020."
REFERENCES,0.5852534562211982,"Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In International Conference on Learning Representations, 2018."
REFERENCES,0.5875576036866359,"Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with
conditionally shifted neurons. In International Conference on Machine Learning, pp. 3664–3673.
PMLR, 2018."
REFERENCES,0.5898617511520737,"Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated proba-
bilities using bayesian binning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 29, 2015."
REFERENCES,0.5921658986175116,"Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018."
REFERENCES,0.5944700460829493,"Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018."
REFERENCES,0.5967741935483871,"Boris Oreshkin, Pau Rodr´ıguez L´opez, and Alexandre Lacoste. Tadam: Task dependent adaptive
metric for improved few-shot learning. In Advances in Neural Information Processing Systems,
pp. 721–731, 2018."
REFERENCES,0.5990783410138248,"Eunbyung Park and Junier B Oliva. Meta-curvature. Advances in Neural Information Processing
Systems, 32:3314–3324, 2019."
REFERENCES,0.6013824884792627,"Massimiliano Patacchiola, Jack Turner, Elliot J Crowley, Michael O’Boyle, and Amos J Storkey.
Bayesian meta-learning for the few-shot setting via deep kernels. Advances in Neural Information
Processing Systems, 33, 2020."
REFERENCES,0.6036866359447005,"Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.
SIAM journal on control and optimization, 30(4):838–855, 1992."
REFERENCES,0.6059907834101382,"Viraj Prabhu, Anitha Kannan, Murali Ravuri, Manish Chaplain, David Sontag, and Xavier Amatri-
ain. Few-shot learning for dermatological disease diagnosis. In Machine Learning for Healthcare
Conference, pp. 532–552. PMLR, 2019."
REFERENCES,0.6082949308755761,"Hang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5822–5830,
2018."
REFERENCES,0.6105990783410138,"Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang, and Yonghong Tian. Transductive
episodic-wise adaptive metric for few-shot learning. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 3603–3612, 2019."
REFERENCES,0.6129032258064516,"Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. Few-shot image recognition by predicting
parameters from activations. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7229–7238, 2018."
REFERENCES,0.6152073732718893,"Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of maml. In International Conference on Learning Rep-
resentations, 2020."
REFERENCES,0.6175115207373272,"Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/
forum?id=rJY0-Kcll."
REFERENCES,0.619815668202765,"Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class
models and shot-free meta training. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 331–339, 2019."
REFERENCES,0.6221198156682027,Under review as a conference paper at ICLR 2022
REFERENCES,0.6244239631336406,"Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,
Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁca-
tion. In International Conference on Learning Representations, 2018."
REFERENCES,0.6267281105990783,"Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference
on Learning Representations, 2019."
REFERENCES,0.6290322580645161,"Amartya Sanyal, Philip H Torr, and Puneet K Dokania. Stable rank normalization for improved
generalization in neural networks and gans. In International Conference on Learning Represen-
tations, 2020."
REFERENCES,0.631336405529954,"Nikunj Saunshi, Arushi Gupta, and Wei Hu. A representation learning perspective on the importance
of train-validation splitting in meta-learning. In International Conference on Machine Learning,
pp. 9333–9343. PMLR, 2021."
REFERENCES,0.6336405529953917,"J¨urgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to
learn: the meta-meta-... hook. PhD thesis, Technische Universit¨at M¨unchen, 1987."
REFERENCES,0.6359447004608295,"Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. Adaptive subspaces for
few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 4136–4145, 2020."
REFERENCES,0.6382488479262672,"Jake Snell and Richard Zemel. Bayesian few-shot classiﬁcation with one-vs-each p´olya-gamma
augmented gaussian processes. In International Conference on Learning Representations, 2021."
REFERENCES,0.6405529953917051,"Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4077–4087, 2017."
REFERENCES,0.6428571428571429,"Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot
learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
403–412, 2019."
REFERENCES,0.6451612903225806,"Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 1199–1208, 2018."
REFERENCES,0.6474654377880185,"Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 1998."
REFERENCES,0.6497695852534562,"Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking
few-shot image classiﬁcation: a good embedding is all you need?
In European conference on
computer vision. Springer, 2020."
REFERENCES,0.652073732718894,"Michael E Tipping. Sparse bayesian learning and the relevance vector machine. Journal of machine
learning research, 1(Jun):211–244, 2001."
REFERENCES,0.6543778801843319,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016."
REFERENCES,0.6566820276497696,"P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010."
REFERENCES,0.6589861751152074,"Florian Wenzel, Kevin Roth, Bastiaan Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes
posterior in deep neural networks really? In International Conference on Machine Learning, pp.
10248–10259. PMLR, 2020."
REFERENCES,0.6612903225806451,"Andrew Wilson and Ryan Adams. Gaussian process kernels for pattern discovery and extrapolation.
In International conference on machine learning, pp. 1067–1075, 2013."
REFERENCES,0.663594470046083,"Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block
attention module. In Proceedings of the European conference on computer vision (ECCV), pp.
3–19, 2018."
REFERENCES,0.6658986175115207,Under review as a conference paper at ICLR 2022
REFERENCES,0.6682027649769585,"Jin Xu, Jean-Francois Ton, Hyunjik Kim, Adam Kosiorek, and Yee Whye Teh. Metafun: Meta-
learning with iterative functional updates. In International Conference on Machine Learning, pp.
10617–10627. PMLR, 2020."
REFERENCES,0.6705069124423964,"Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation
with set-to-set functions. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 8808–8817, 2020."
REFERENCES,0.6728110599078341,"Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In Proceedings of the 32nd International Conference
on Neural Information Processing Systems, pp. 7343–7353, 2018."
REFERENCES,0.6751152073732719,"Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-
adaptive projection for few-shot learning. In ICML 2019 (International Conference on Machine
Learning). ICML, 2019."
REFERENCES,0.6774193548387096,"Sung Whan Yoon, Do-Yeon Kim, Jun Seo, and Jaekyun Moon. Xtarnet: Learning to extract task-
adaptive representation for incremental few-shot learning. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume
119 of Proceedings of Machine Learning Research, pp. 10852–10860. PMLR, 2020."
REFERENCES,0.6797235023041475,"Zhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning.
Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.6820276497695853,"Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classiﬁca-
tion with differentiable earth mover’s distance and structured classiﬁers. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pp. 12203–12213, 2020."
REFERENCES,0.684331797235023,"Manli Zhang, Jianhong Zhang, Zhiwu Lu, Tao Xiang, Mingyu Ding, and Songfang Huang. {IEPT}:
Instance-level and episode-level pretext tasks for few-shot learning. In International Conference
on Learning Representations, 2021."
REFERENCES,0.6866359447004609,"A
APPENDIX"
REFERENCES,0.6889400921658986,"A.1
PSEUDO CODE FOR MFRL"
REFERENCES,0.6912442396313364,Algorithm 1 Meta-free representation learning for few-shot learning
REFERENCES,0.6935483870967742,"Merge all training tasks Dtr = {Dτ}T
τ=1
Initialize model parameters θ = [θf, W]
Maximize the likelihood on all training data p (Dtr | θ) using SGD"
REFERENCES,0.695852534562212,"Minimize the squared loss for regression problems
Minimize the cross-entropy loss for classiﬁcation problems
Run SWA to obtain θSWA
Discard W and freeze θf
Learn a new top layer using support data D in a test task:"
REFERENCES,0.6981566820276498,"Learn a hierarchical Bayesian linear model for a regression task
Learn a logistic regression model with the temperature scaling factor for a classiﬁcation task"
REFERENCES,0.7004608294930875,"A.2
EXPERIMENT DETAILS"
REFERENCES,0.7027649769585254,"Sine waves are generated by y = A sin(x−ϕ)+ϵ, where amplitude A ∈[0.1, 5.0], phase ϕ ∈[0, π]
and ϵ is white noise with standard deviation of 0.1 (Finn et al., 2017). Each sine wave contains 200
samples by sampling x uniformly from [−5.0, 5.0]. We generate 500 waves for training, validation
and testing, respectively. All sine waves are different from each other. We use the same backbone
network described in MAML (Finn et al., 2017): a two-layer MLP with 40 hidden units in each
layer. We use the SGD optimizer with a learning rate of 10−3 over 8 × 104 training iterations and
run SWA over 2 × 104 training iterations with a learning rate of 0.05."
REFERENCES,0.7050691244239631,Under review as a conference paper at ICLR 2022
REFERENCES,0.7073732718894009,"Head pose regression data is derived from the Queen Mary University of London multi-view face
dataset (Gong et al., 1996). It contains images from 37 people and 133 facial images per person.
Facial images cover a view sphere of 90◦in yaw and 120◦in tilt. The dataset is divided into 3192
training samples (24 people), 1064 validation samples (8 people), and 665 test samples (5 people).
We use the same feature extractor described in literature (Patacchiola et al., 2020): a three-layer
convolutional neural network, each with 36 output channels, stride 2, and dilation 2. We train the
model on the training people set for 300 epochs using the SGD optimizer with a learning rate of
0.01 and run 25 epochs of SWA with a learning rate of 0.01."
REFERENCES,0.7096774193548387,"miniImageNet is a 100-class subset of the original ImageNet dataset (Deng et al., 2009) for few-
shot learning (Vinyals et al., 2016). Each class contains 600 images in RGB format of the size 84
× 84. miniImageNet is split into 64 training classes, 16 validation classes, and 20 testing classes,
following the widely used data splitting protocol (Ravi & Larochelle, 2017)."
REFERENCES,0.7119815668202765,"tieredImageNet is another subset of the ImageNet dataset for few-shot learning (Ren et al., 2018).
It contains 608 classes grouped into 34 categories, which are split into 20 training categories (351
classes), 6 validation categories (97 classes), and 8 testing categories (160 classes). Compared with
miniImageNet, training classes in tieredImageNet are sufﬁciently distinct from test classes, making
few-shot classiﬁcation more difﬁcult."
REFERENCES,0.7142857142857143,"CIFAR-FS is a derivative of the original CIFAR-100 dataset by randomly splitting 100 classes into
64, 16, and 20 classes for training, validation, and testing, respectively (Bertinetto et al., 2019)."
REFERENCES,0.716589861751152,"FC100 is another derivative of CIFAR-100 with minimized overlapped information between train
classes and test classes by grouping the 100 classes into 20 superclasses (Oreshkin et al., 2018). They
are further split into 60 training classes (12 superclasses), 20 validation classes (4 superclasses), and
20 test classes (4 superclasses)."
REFERENCES,0.7188940092165899,"miniImageNet to CUB is a cross-domain few-shot classiﬁcation task, where the models are trained
on miniImageNet and tested on CUB (Welinder et al., 2010). Cross-domain few-shot classiﬁcation
is more challenging due to the big domain gap between two datasets. We can better evaluate the
generalization capability in different algorithms. We follow the experiment setup in Yue et al. (2020)
and use the WRN2-28-10 as the backbone."
REFERENCES,0.7211981566820277,"The backbone model is trained on all training classes using C-class cross-entropy loss by the SGD
optimizer (momentum of 0.9 and weight decay of 1e-4) with a mini-batch size of 64. The learning
rate is initialized as 0.05 and is decayed by 0.1 after 60, 80, and 90 epochs (100 epochs in total).
After the SGD training converges, we run 100 epochs of SWA with a learning rate of 0.02. Note
that MFRL is not sensitive to training epochs and learning rates in SWA (see Appendix A.4). The
training images are augmented with random crop, random horizontal ﬂip, and color jitter."
REFERENCES,0.7235023041474654,"During testing, we conduct 5 independent runs of 600 randomly sampled few-shot classiﬁcation
tasks from test classes and calculate the average accuracy. Each task contains 5 classes, 1 × 5 or
5 × 5 support samples, and 75 query samples. A logistic regression model is learned using only the
support samples. The classiﬁcation accuracy is evaluated on the query samples."
REFERENCES,0.7258064516129032,"A.3
ADDITIONAL RESULTS ON FEW-SHOT REGRESSION AND CLASSIFICATION"
REFERENCES,0.728110599078341,"The additional results on few-shot regression using different activation functions are reported in
Table 6. MFRL achieves high accuracy with different activation functions."
REFERENCES,0.7304147465437788,Table 6: 10-shot regression on sine waves with different activation functions.
REFERENCES,0.7327188940092166,"Sine wave
MSE
MFRL (ReLU activation)
0.16 ± 0.51
MFRL (tanh activation)
0.018 ± 0.011
MFRL (erf activation)
0.016 ± 0.008"
REFERENCES,0.7350230414746544,"The few-shot classiﬁcation results using a 4-layer convolutional neural network (or similar archi-
tectures) are reported in Table 7 and 8. Similar to the results using ResNet-12 and WRN-28-10,
the proposed method outperforms a wide range of meta-learning approaches. Our method is only"
REFERENCES,0.7373271889400922,Under review as a conference paper at ICLR 2022
REFERENCES,0.7396313364055299,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7419354838709677,Accuracy
REFERENCES,0.7442396313364056,"ECE: 0.0518
MCE: 0.0844
BRI: 0.3288 MAML"
REFERENCES,0.7465437788018433,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7488479262672811,Accuracy
REFERENCES,0.7511520737327189,"ECE: 0.0435
MCE: 0.0744
BRI: 0.3230"
REFERENCES,0.7534562211981567,Proto Net
REFERENCES,0.7557603686635944,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7580645161290323,Accuracy
REFERENCES,0.7603686635944701,"ECE: 0.0338
MCE: 0.0552
BRI: 0.3200"
REFERENCES,0.7626728110599078,Matching Net
REFERENCES,0.7649769585253456,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7672811059907834,Accuracy
REFERENCES,0.7695852534562212,"ECE: 0.3027
MCE: 0.3794
BRI: 0.4055"
REFERENCES,0.771889400921659,Baseline++
REFERENCES,0.7741935483870968,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7764976958525346,Accuracy
REFERENCES,0.7788018433179723,"ECE: 0.3152
MCE: 0.3971
BRI: 0.4388"
REFERENCES,0.7811059907834101,DKT Cosine Kernel
REFERENCES,0.783410138248848,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7857142857142857,Accuracy
REFERENCES,0.7880184331797235,"ECE: 0.0040
MCE: 0.0135
BRI: 0.2791 MFRL"
REFERENCES,0.7903225806451613,Figure 4: Reliability diagrams for 5-way 5-shot classiﬁcation on miniImageNet
REFERENCES,0.7926267281105991,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.7949308755760369,Accuracy
REFERENCES,0.7972350230414746,"ECE: 0.0118
MCE: 0.0233
BRI: 0.4890"
REFERENCES,0.7995391705069125,miniImageNet 1-shot
REFERENCES,0.8018433179723502,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.804147465437788,Accuracy
REFERENCES,0.8064516129032258,"ECE: 0.0086
MCE: 0.0180
BRI: 0.4017"
REFERENCES,0.8087557603686636,tieredImageNet 1-shot
REFERENCES,0.8110599078341014,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8133640552995391,Accuracy
REFERENCES,0.815668202764977,"ECE: 0.0111
MCE: 0.0201
BRI: 0.3688"
REFERENCES,0.8179723502304147,CIFAR-FS 1-shot
REFERENCES,0.8202764976958525,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8225806451612904,Accuracy
REFERENCES,0.8248847926267281,"ECE: 0.0056
MCE: 0.0456
BRI: 0.6806"
REFERENCES,0.8271889400921659,FC100 1-shot
REFERENCES,0.8294930875576036,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8317972350230415,Accuracy
REFERENCES,0.8341013824884793,"ECE: 0.0040
MCE: 0.0135
BRI: 0.2791"
REFERENCES,0.836405529953917,miniImageNet 5-shot
REFERENCES,0.8387096774193549,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8410138248847926,Accuracy
REFERENCES,0.8433179723502304,"ECE: 0.0036
MCE: 0.0581
BRI: 0.2063"
REFERENCES,0.8456221198156681,tieredImageNet 5-shot
REFERENCES,0.847926267281106,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8502304147465438,Accuracy
REFERENCES,0.8525345622119815,"ECE: 0.0065
MCE: 0.0343
BRI: 0.1848"
REFERENCES,0.8548387096774194,CIFAR-FS 5-shot
REFERENCES,0.8571428571428571,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.8594470046082949,Accuracy
REFERENCES,0.8617511520737328,"ECE: 0.0066
MCE: 0.0268
BRI: 0.5173"
REFERENCES,0.8640552995391705,FC100 5-shot
REFERENCES,0.8663594470046083,Figure 5: Reliability diagrams for 5-way few-shot classiﬁcation using ResNet-12 backbone
REFERENCES,0.868663594470046,Under review as a conference paper at ICLR 2022
REFERENCES,0.8709677419354839,Table 7: Few-shot classiﬁcation results on miniImageNet and tieredImageNet.
REFERENCES,0.8732718894009217,"Method
Backbone
miniImageNet 5-way
tieredImageNet 5-way
1-shot
5-shot
1-shot
5-shot
Matching Net (Vinyals et al., 2016)
Conv-4
43.56 ± 0.84
55.31 ± 0.73
54.48 ± 0.93
71.32 ± 0.78
Proto Net (Snell et al., 2017)
Conv-4
49.42 ± 0.78
68.20 ± 0.66
53.31 ± 0.89
72.69 ± 0.74
MAML (Finn et al., 2017)
Conv-4
48.70 ± 1.75
63.11 ± 0.92
-
-
SNAIL (Mishra et al., 2018)
Conv-4
45.10 ± NA
55.20 ± NA
-
-
VERSA (Gordon et al., 2019)
Conv-5
53.40 ± 1.82
67.37 ± 0.86
-
-
Meta Mixture (Jerfel et al., 2019)
Conv-4
49.60 ± 1.50
64.60 ± 0.92
-
-
RelationNet (Sung et al., 2018)
Conv-4
50.44 ± 0.82
65.32 ± 0.70
54.48 ± 0.93
71.32 ± 0.78
FPA (Qiao et al., 2018)
Conv-4
54.53 ± 0.40
67.87 ± 0.20
-
-
Shot-free (Ravichandran et al., 2019)
Conv-4
49.07 ± 0.43
65.73 ± 0.36
48.19 ± 0.43
65.50 ± 0.39
Baseline++ (Chen et al., 2019)
Conv-4
47.15 ± 0.49
66.18 ± 0.18
54.67 ± 0.61
72.37 ± 0.67
FEAT (Ye et al., 2020)
Conv-4
55.15 ± 0.20
71.61 ± 0.16
-
-
DKT (Patacchiola et al., 2020)
Conv-4
49.73 ± 0.07
64.00 ± 0.09
-
-
MFRL
Conv-4
53.62 ± 0.71
71.52 ± 0.60
56.24 ± 0.84
72.88 ± 0.76"
REFERENCES,0.8755760368663594,Table 8: Few-shot classiﬁcation results on CIFAR-FS and FC100.
REFERENCES,0.8778801843317973,"Method
Backbone
CIFAR-FS 5-way
FC100 5-way
1-shot
5-shot
1-shot
5-shot
Proto Net (Snell et al., 2017)
Conv-4
55.5 ± 0.7
72.0 ± 0.6
35.3 ± 0.6
48.6 ± 0.6
MAML (Finn et al., 2017)
Conv-4
58.9 ± 1.9
71.5 ± 1.0
38.1 ± 1.7
50.4 ± 1.0
RelationNet (Sung et al., 2018)
Conv-4
55.0 ± 1.0
69.3 ± 0.8
-
-
Shot-free (Ravichandran et al., 2019)
Conv-4
55.1 ± 0.5
71.7 ± 0.4
-
-
Baseline++ (Chen et al., 2019)
Conv-4
55.1 ± 0.9
72.3 ± 0.8
35.2 ± 0.7
49.8 ± 0.7
MFRL
Conv-4
64.3 ± 0.9
79.4 ± 0.5
40.1 ± 0.8
54.4 ± 0.7"
REFERENCES,0.880184331797235,"second to few-shot embedding adaptation with transformer (FEAT) (Ye et al., 2020) on miniIma-
geNet dataset. Recently, meta-learned attention modules are built on top of the convolutional neural
network to get improved few-shot classiﬁcation accuracy. Direct comparison to those methods with
attention modules (Ye et al., 2020; Fei et al., 2021; Zhang et al., 2021) may not be fair because recent
studies show that transformer itself can achieve better results than convolutional neural networks in
image classiﬁcation (Dosovitskiy et al., 2021). It is difﬁcult to determine whether the performance
improvement is due to the meta-learning algorithm or the attention modules. To make a fair compar-
ison, we add convolutional block attention modules (Woo et al., 2018) on top of ResNet12 features
(before global average pooling). As shown in Fig. 9, MFRL with attention modules achieves com-
parable results with MELR and IEPT."
REFERENCES,0.8824884792626728,"The uncertainty calibration results of MFRL with the temperature scaling factor are presented in Fig.
5. The prediction conﬁdence aligns well with the prediction accuracy. It demonstrates that MFRL
with the temperature scaling factor results in well calibrated models."
REFERENCES,0.8847926267281107,"A.4
SENSITIVITY OF MFRL"
REFERENCES,0.8870967741935484,"The performance of MFRL is not sensitive to learning rates in SWA. As shown in Fig. 6, the
representation learned by SWA generalizes better than the one from standard SGD, as long as the
learning rate in SWA is in a reasonable range. In addition, the prediction accuracy on meta-test tasks
keeps stable even after running SWA for many epochs on the training data. Therefore, MFRL is not
sensitive to training epochs. This desirable property makes the proposed method easy to use when
solving few-shot learning problems in practice."
REFERENCES,0.8894009216589862,Under review as a conference paper at ICLR 2022
REFERENCES,0.8917050691244239,Table 9: Results of MFRL with attention modules
REFERENCES,0.8940092165898618,"Method
Backbone
miniImageNet 5-way
tieredImageNet 5-way
1-shot
5-shot
1-shot
5-shot
FEAT (Ye et al., 2020)
ResNet-12
66.78 ± 0.20
82.05 ± 0.14
70.80 ± 0.23
84.79 ± 0.16
MELR (Fei et al., 2021)
ResNet-12
67.40 ± 0.43
83.40 ± 0.28
72.14 ± 0.51
87.01 ± 0.35
IEPT (Zhang et al., 2021)
ResNet-12
67.05 ± 0.44
82.90 ± 0.30
72.24 ± 0.50
86.73 ± 0.34
MFRL
ResNet-12
67.18 ± 0.79
83.81 ± 0.53
71.58 ± 0.79
86.87 ± 0.62
MFRL + Attention
ResNet-12
67.51 ± 0.78
83.97 ± 0.51
71.97 ± 0.80
86.99 ± 0.60"
REFERENCES,0.8963133640552995,"0
20
40
60
80
100
SWA epoch 78.0% 79.0% 80.0% 81.0% 82.0% 83.0% 84.0% 85.0%"
REFERENCES,0.8986175115207373,Meta-test accuracy
REFERENCES,0.9009216589861752,miniImageNet
REFERENCES,0.9032258064516129,"Meta-test, No SWA"
REFERENCES,0.9055299539170507,"Meta-test, SWA LR=0.01"
REFERENCES,0.9078341013824884,"Meta-test, SWA LR=0.02"
REFERENCES,0.9101382488479263,"Meta-test, SWA LR=0.05 (a)"
REFERENCES,0.9124423963133641,"0
20
40
60
80
100
SWA epoch 82.0% 83.0% 84.0% 85.0% 86.0% 87.0% 88.0% 89.0%"
REFERENCES,0.9147465437788018,Meta-test accuracy
REFERENCES,0.9170506912442397,CIFR-FS
REFERENCES,0.9193548387096774,"Meta-test, No SWA"
REFERENCES,0.9216589861751152,"Meta-test, SWA LR=0.01"
REFERENCES,0.923963133640553,"Meta-test, SWA LR=0.02"
REFERENCES,0.9262672811059908,"Meta-test, SWA LR=0.05 (b)"
REFERENCES,0.9285714285714286,"Figure 6: Evaluation on different learning rates and training epochs in SWA. (a) 5-way 5-shot ac-
curacy on miniImageNet with ResNet12 backbone; (b) 5-way 5-shot accuracy on CIFAR-FS with
WRN-28-10 backbone."
REFERENCES,0.9308755760368663,"A.5
COMPARISON WITH EXPONENTIAL MOVING AVERAGING"
REFERENCES,0.9331797235023042,"Exponential moving average (EMA) decays the importance of model weights from early training
epochs exponentially. Let θavg ←aθavg + (1 −a) θnew. We try EMA with different values of a.
In Table 10, EMA improves the performance when a is within a reasonable range. Note that EMA
introduces one extra hyperparameter, the forgetting factor. It makes EMA less desirable in practice."
REFERENCES,0.9354838709677419,Table 10: Comparison between SWA and EMA
REFERENCES,0.9377880184331797,"Method
Backbone
miniImageNet 5-way
1-shot
5-shot
No averaging
ResNet-12
62.27 ± 0.86
80.23 ± 0.57
EMA a = 0.9
ResNet-12
66.03 ± 0.83
82.90 ± 0.58
EMA a = 0.99
ResNet-12
66.05 ± 0.84
82.98 ± 0.57
EMA a = 0.999
ResNet-12
62.49 ± 0.81
80.73 ± 0.62
SWA
ResNet-12
67.18 ± 0.79
83.81 ± 0.53"
REFERENCES,0.9400921658986175,"A.6
HIERARCHICAL BAYESIAN LINEAR CLASSIFICATION MODEL"
REFERENCES,0.9423963133640553,"Similar to the hierarchical Bayesian linear regression model, the prior distribution over w is p(w |
λ) = Qp
i=0 N (wi | 0, λ), where λ is the precision in the Gaussian prior. The hyperprior on λ is
deﬁned as p(λ) = Gamma (λ | a, b). The posterior over all latent variables given the data is"
REFERENCES,0.9447004608294931,"p (w, λ | X, y) = p (y | X, w, λ) p (w | λ) p(λ)"
REFERENCES,0.9470046082949308,"p(y | X)
(7)"
REFERENCES,0.9493087557603687,Under review as a conference paper at ICLR 2022
REFERENCES,0.9516129032258065,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9539170506912442,Accuracy
REFERENCES,0.956221198156682,"ECE:  0.1009
MCE: 0.1934
BRI:  0.3112"
REFERENCES,0.9585253456221198,miniImageNet 5-shot
REFERENCES,0.9608294930875576,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9631336405529954,Accuracy
REFERENCES,0.9654377880184332,"ECE:  0.0913
MCE: 0.2537
BRI:  0.2342"
REFERENCES,0.967741935483871,tieredImageNet 5-shot
REFERENCES,0.9700460829493087,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9723502304147466,Accuracy
REFERENCES,0.9746543778801844,"ECE:  0.0876
MCE: 0.2819
BRI:  0.2309"
REFERENCES,0.9769585253456221,CIFAR-FS 5-shot
REFERENCES,0.9792626728110599,"0.0
0.2
0.4
0.6
0.8
1.0
Confidence 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9815668202764977,Accuracy
REFERENCES,0.9838709677419355,"ECE:  0.2533
MCE: 0.3552
BRI:  0.6468"
REFERENCES,0.9861751152073732,FC100 5-shot
REFERENCES,0.988479262672811,"Figure 7: Reliability diagrams of hierarchical Bayesian linear classiﬁcation models with ﬂat and
non-informative hyperpriors. The backbone is ResNet-12."
REFERENCES,0.9907834101382489,"MCMC sampling (Hoffman & Gelman, 2014) is used to avoid potential deterioration in predictive
performance due to approximated inference. A ﬂat and non-informative hyperprior (a = b = 10−6)
is used because no prior knowledge is available. In Table 11, the hierarchical Bayesian linear clas-
siﬁcation model achieves slightly worse performance than the logistic regression model. However,
the classiﬁcation model is not well calibrated, as shown in Fig. 7."
REFERENCES,0.9930875576036866,"Table 11: Comparison between logistic regression and hierarchical Bayesian linear models on 5-way
5-shot classiﬁcation benchmarks using ResNet-12 backbone."
REFERENCES,0.9953917050691244,"Top layer
miniImageNet
tieredImageNet
CIFAR-FS
FC-100
Logistic regression
83.81
86.87
87.4
61.1
Hierarchical Bayesian linear classiﬁcation
81.94
85.32
85.9
59.2"
REFERENCES,0.9976958525345622,"After ﬁne-tuning a and b using the meta-validation data, it is possible to get better calibrated classi-
ﬁcation models on test tasks. Besides, the classiﬁcation accuracy is still slightly worse than logistic
regression after hyperparameter tuning. Our observations align with a recent study, which shows that
the Bayesian classiﬁcation model cannot achieve similar performance to the non-Bayesian counter-
part without tempering the posterior (Wenzel et al., 2020). We do not further experiment tempered
posterior in the hierarchical Bayesian linear classiﬁcation model because it introduces an extra tem-
perature hyperparameter that requires tuning. The original purpose of introducing the hierarchical
Bayesian model is to get an accurate and well calibrated classiﬁcation model without hyperparam-
eter tuning. Consequently, the hierarchical Bayesian model is not used in few-shot classiﬁcation
in that hierarchical Bayesian linear classiﬁcation models cannot achieve high accuracy and good
uncertainty calibration from a non-informative hyperprior. If hyperparameter tuning is inevitable, it
is much easier to tune a logistic regression model with a temperature scaling factor, compared with
tuning a hierarchical Bayesian model. Furthermore, the computational cost of learning a hierarchi-
cal Bayesian linear classiﬁcation model via MCMC sampling is much larger than that of learning a
logistic regression model."
