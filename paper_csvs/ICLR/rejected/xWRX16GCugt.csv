Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0028169014084507044,"The ﬁeld of Continual Learning (CL) seeks to develop algorithms that accumulate
knowledge and skills over time through interaction with non-stationary environ-
ments. In practice, a plethora of evaluation procedures (settings) and algorithmic
solutions (methods) exist, each with their own potentially disjoint set of assump-
tions. This variety makes measuring progress in CL difﬁcult. We propose a tax-
onomy of settings, where each setting is described as a set of assumptions. A tree-
shaped hierarchy emerges from this view, where more general settings become the
parents of those with more restrictive assumptions. This makes it possible to use
inheritance to share and reuse research, as developing a method for a given setting
also makes it directly applicable onto any of its children. We instantiate this idea
as a publicly available software framework called Sequoia, which features a wide
variety of settings from both the Continual Supervised Learning (CSL) and Con-
tinual Reinforcement Learning (CRL) domains. Sequoia also includes a growing
suite of methods which are easy to extend and customize, in addition to more spe-
cialized methods from external libraries. We hope that this new paradigm and its
ﬁrst implementation can help unify and accelerate research in CL. You can help
us grow the tree by visiting (this GitHub URL)."
INTRODUCTION,0.005633802816901409,"1
INTRODUCTION"
INTRODUCTION,0.008450704225352112,"Image 
classification"
INTRODUCTION,0.011267605633802818,"MNIST classification
Cats vs Dogs"
INTRODUCTION,0.014084507042253521,Classification
INTRODUCTION,0.016901408450704224,Inputs are Images
INTRODUCTION,0.01971830985915493,"images contain digits
images contain cats or dogs"
INTRODUCTION,0.022535211267605635,: Assumption
INTRODUCTION,0.02535211267605634,: Setting
INTRODUCTION,0.028169014084507043,: Method
INTRODUCTION,0.030985915492957747,"ConvNet 
Classifier"
INTRODUCTION,0.03380281690140845,"MLP 
Classifier (...)"
INTRODUCTION,0.036619718309859155,"Figure 1: Example of a simple tree of settings
that shows the core principle of our framework:
research settings can be organized into a hierar-
chy based on their assumptions. Methods have
a set of basic assumptions which correspond to
their target setting (dashed arrows), and can be
applied onto any of their descendants. In this
trivial example, both MLP and ConvNet clas-
siﬁers are applicable to the MNIST classiﬁca-
tion setting and their performances are directly
comparable, even though they were created for
slightly different settings. Sequoia applies this
principle to the ﬁeld of Continual Learning."
INTRODUCTION,0.03943661971830986,"With the growing interest in developing methods
robust to changes in the data distribution, research
in continual learning (CL) has gained traction in
recent years (Delange et al., 2021; Caccia et al.,
2020; Parisi et al., 2019; Lesort et al., 2020). CL
enables models to acquire knowledge from non-
stationary data, learning new and possibly more
complex tasks, while retaining performance on
previously-learned tasks."
INTRODUCTION,0.04225352112676056,"To instantiate a CL problem, one must ﬁrst make
assumptions about the data distribution and set
constraints to enforce non-stationary learning.
For instance, assumptions are often made about
the type and number of tasks, the task boundaries,
or the availability of task labels, while constraints
often relate to memory, compute, or time allowed
to learn a task.
Combinations of assumptions,
rules, and datasets have resulted in a multitude
of settings (Khetarpal et al., 2020)."
INTRODUCTION,0.04507042253521127,"We argue that the increased popularity of CL,
combined with a lack of uniﬁcation — in part
due to the large number of settings and the ab-
sence of well-deﬁned applications — has led to
a “research jungle” that may be slowing down
progress."
INTRODUCTION,0.04788732394366197,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05070422535211268,We identify some of the main challenges associated with the lack of uniﬁcation in CL:
INTRODUCTION,0.05352112676056338,"i) Evaluation. Methods in CL are often studied under a small subset of the available settings,
making it difﬁcult to evaluate them, as their problem domains don’t always overlap. Consequently,
it is challenging to determine if a method will generalize beyond the setting it was designed for. To
add to this, continual reinforcement learning (CRL) poses further challenges in evaluation due to the
lack of a clear distinction between training and testing phases (Khetarpal et al., 2018). Moreover,
resource consumption is a critical factor for evaluation of CL methods which is often overlooked
due to the lack of standardized evaluation protocols. Thus, there is a need for standardization of the
infrastructure used to evaluate CL methods."
INTRODUCTION,0.056338028169014086,"ii) Reproducibility. In order to analyze speciﬁc properties of novel methods, researchers tend to
re-implement baselines and adapt them to their particular needs (Henderson et al., 2018). These
baselines are often not described in enough detail to ensure reproducibility, e.g. a prescribed hyper-
parameter search strategy, computational requirements, open source libraries, etc."
INTRODUCTION,0.059154929577464786,"iii) CSL and CRL evolve in silos. Continual supervised learning (CSL) and continual reinforcement
learning (CRL) are considered to be independent settings in the literature and thus, they are evolving
separately. However, most methods in one ﬁeld can be instantiated in the other, resulting in duplicate
efforts such as replay for CSL (Rebufﬁet al., 2017; Lesort et al., 2019a; Shin et al., 2017; Lesort
et al., 2019b; Prabhu et al., 2020) and replay for CRL (Traor´e et al., 2019; Rolnick et al., 2019;
Kaplanis et al., 2020). To this end, we advocate that the uniﬁcation of both ﬁelds would greatly
reduce these duplicate efforts and accelerate CL research."
INTRODUCTION,0.061971830985915494,"In this work, we present Sequoia, a unifying software framework for CL research, as a solution
for jointly addressing these issues. We describe how settings differ from one another in terms of
their assumptions (e.g., are task IDs observed or not). This perspective gives rise to a hierarchical
organization of CL settings, through which methods become directly reusable by inheritance, thus
greatly reducing the amount of work involved in developing and evaluating methods in CL."
INTRODUCTION,0.0647887323943662,"Key Contributions of this work are 1) a general uniﬁed framework that systematically organizes CL
settings; 2) Sequoia, a software framework that can serve as a universal platform for CL research."
A UNIFYING FRAMEWORK FOR CL RESEARCH,0.0676056338028169,"2
A UNIFYING FRAMEWORK FOR CL RESEARCH"
A UNIFYING FRAMEWORK FOR CL RESEARCH,0.07042253521126761,"To construct our unifying framework, we ﬁrst represent each CL setting as a set of shared assump-
tions. More general settings make fewer assumptions and vice versa. Settings can then be organized
in a hierarchy where adding/removing an assumption yields a child/parent setting (Fig. 2)."
A UNIFYING FRAMEWORK FOR CL RESEARCH,0.07323943661971831,"We formalize the framework using a hidden-mode Markov decision process (HM-MDP), a special
case of a POMDP (Choi et al., 2000). HM-MDPs comprise an observation space X, an action
space A, a context space Z (we also refer to contexts as tasks), and a feedback function r. Here
the full state space is a concatenation of the observation space and task space S “ X ˆ Z. As
such, the hidden context variable z P Z deﬁnes the dynamics of the environment ppx1|x, a, zq for
observations x1, x P X and action a P A. The feedback function rpx, a, zq provides an agent
πpa|xq (e.g. a supervised model) the value of performing action a after receiving observation x.
The context variable follows a Markov chain ppz1|zq. The dynamic context enables modelling CL
task/context-change. A change in the context variable is called a task boundary."
A UNIFYING FRAMEWORK FOR CL RESEARCH,0.07605633802816901,"In the next section (§ 2.1) we show that by restricting the different elements of the HM-MDP we
recover different CL settings. Then in § 2.2 we discuss differences between continual supervised
(CSL) and continual reinforcement learning (CRL). We end in § 2.3, by presenting additional as-
sumptions that are relevant to CL problems."
CONTINUAL LEARNING ASSUMPTIONS,0.07887323943661972,"2.1
CONTINUAL LEARNING ASSUMPTIONS"
CONTINUAL LEARNING ASSUMPTIONS,0.08169014084507042,"Assumptions related to CL can be aranged into a hierarchy, as illustrated in the the central portion of
Fig. 2. These settings cover most, if not all the current CL literature. We start from the most general
setting: continuous task-agnostic CL (Zeno et al., 2018) and add assumptions one by one."
CONTINUAL LEARNING ASSUMPTIONS,0.08450704225352113,"Continuous Task-Agnostic CL is our most general setting. The context variable is continuous
‡ P R. This setting allows for different kinds of drifts in the environment, including smooth task"
CONTINUAL LEARNING ASSUMPTIONS,0.08732394366197183,Under review as a conference paper at ICLR 2022
CONTINUAL LEARNING ASSUMPTIONS,0.09014084507042254,"continuous 
task-agnostic CL"
CONTINUAL LEARNING ASSUMPTIONS,0.09295774647887324,"discrete
task-agnotic CL"
CONTINUAL LEARNING ASSUMPTIONS,0.09577464788732394,incremental
CONTINUAL LEARNING ASSUMPTIONS,0.09859154929577464,learning
CONTINUAL LEARNING ASSUMPTIONS,0.10140845070422536,task-incremental
CONTINUAL LEARNING ASSUMPTIONS,0.10422535211267606,learning
CONTINUAL LEARNING ASSUMPTIONS,0.10704225352112676,multi-task
CONTINUAL LEARNING ASSUMPTIONS,0.10985915492957747,learning
CONTINUAL LEARNING ASSUMPTIONS,0.11267605633802817,traditional
CONTINUAL LEARNING ASSUMPTIONS,0.11549295774647887,learning
CONTINUAL LEARNING ASSUMPTIONS,0.11830985915492957,"clear task
 boundaries"
CONTINUAL LEARNING ASSUMPTIONS,0.12112676056338029,observable task
CONTINUAL LEARNING ASSUMPTIONS,0.12394366197183099,boundaries
CONTINUAL LEARNING ASSUMPTIONS,0.1267605633802817,"observable
task variable
stationary tasks"
CONTINUAL LEARNING ASSUMPTIONS,0.1295774647887324,stationary tasks
CONTINUAL LEARNING ASSUMPTIONS,0.1323943661971831,"complete supervision +
passive environment"
CONTINUAL LEARNING ASSUMPTIONS,0.1352112676056338,"continuous 
task-agnostic CSL"
CONTINUAL LEARNING ASSUMPTIONS,0.13802816901408452,"low supervision +
active environment"
CONTINUAL LEARNING ASSUMPTIONS,0.14084507042253522,"continuous 
task-agnostic CRL"
CONTINUAL LEARNING ASSUMPTIONS,0.14366197183098592,"clear task
 boundaries"
CONTINUAL LEARNING ASSUMPTIONS,0.14647887323943662,"discrete 
task-agnostic CSL"
CONTINUAL LEARNING ASSUMPTIONS,0.14929577464788732,"observable
task variable"
CONTINUAL LEARNING ASSUMPTIONS,0.15211267605633802,"discrete 
task-agnostic CRL"
CONTINUAL LEARNING ASSUMPTIONS,0.15492957746478872,"...
..."
CONTINUAL LEARNING ASSUMPTIONS,0.15774647887323945,"Figure 2: Sequoia - The Continual Learning Research Tree. Continual learning research set-
tings can be organized into a tree, in which more general settings (parents) are linked with more
restricted settings (children) by the differences in assumptions between them. Settings generally
become more challenging the higher they are in this hierarchy, as less information becomes avail-
able to the method. The central portion of the tree shows the assumptions speciﬁc to CL, while the
highest lateral branches indicate the choice of either supervised or reinforcement learning, which we
consider to be orthogonal to CL. By combining either with the central assumptions, settings from
Continual SL and Continual RL can be recovered to the left and right, respectively."
CONTINUAL LEARNING ASSUMPTIONS,0.16056338028169015,"boundaries, i.e. slow drift (Zeno et al., 2018). This setting is task-agnostic, meaning that the context
variable z is unobserved. Because the context is allowed to drift slowly, it can be more challenging
for the methods to infer when a task has changed enough to compartmentalize the recently acquired
knowledge before adapting to the new task. In RL, this setting is analogous to the DP-MDP (Xie
et al., 2020; Chandak et al., 2020b). In SL, it has also been studied in e.g. Zeno et al. (2018); Aljundi
et al. (2019a;b)."
CONTINUAL LEARNING ASSUMPTIONS,0.16338028169014085,"Discrete Task-Agnostic CL assumes clear (or well-deﬁned) task boundaries and so a discrete con-
text variable Z P N. In this setting the context can shift in a drastic way, still without the agent being
explicitly noticed. Some cases where this setting has been studied are Choi et al. (2000); Riemer
et al. (2018) for RL and Caccia et al. (2020); He et al. (2019); Harrison et al. (2019) for SL."
CONTINUAL LEARNING ASSUMPTIONS,0.16619718309859155,"Incremental Learning (IL) relaxes the task-agnostic assumption: the task boundaries are observ-
able. This is akin to augmenting the observation with a binary variable that is set to 1 when z1 ‰ z
and 0 otherwise. In doing so, the algorithm does not need to perform task-boundary detection. In
SL, some well-known IL settings include class-IL and domain-IL distinguished by their disjoint
action space and shared action space, respectively. This is discussed in § 2.3."
CONTINUAL LEARNING ASSUMPTIONS,0.16901408450704225,"At this point in the CL hierarchy, the tree branches in two directions, depending on the order of
remaining assumptions (see Fig. 2). We will ﬁrst explain the right sub-tree."
CONTINUAL LEARNING ASSUMPTIONS,0.17183098591549295,"Task-Incremental Learning (task-IL) assumes a fully-observable context variable available to the
agent πpa|x, zq. In the literature, observing z is analogous to knowing the task ID or task label. In
this simpler CL setting, forgetting can be prevented by freezing a model at the completion of each
task and using the task-ID to retrieve it for evaluation."
CONTINUAL LEARNING ASSUMPTIONS,0.17464788732394365,Under review as a conference paper at ICLR 2022
CONTINUAL LEARNING ASSUMPTIONS,0.17746478873239438,"The following settings remove the non-stationarity assumption in the contexts/tasks and are often
used to set an upper-bound performance for CL methods."
CONTINUAL LEARNING ASSUMPTIONS,0.18028169014084508,"Multi-task Learning removes the non-stationarity in the environment dynamics and the feedback
function as it assumes a stationary context variable ppz1|zq “ ppz1q. When the contexts are station-
ary, there is no catastrophic forgetting (CF) (French, 1999) problem to solve. Multi-task learning
assumes a fully-observable task variable."
CONTINUAL LEARNING ASSUMPTIONS,0.18309859154929578,"Traditional Learning branches off incremental CL and assumes a stationary environment. It is
the vanilla supervised setting machine learning defaults to. In our framework, it can be seen as a
multi-task learning problem where the task variable isn’t observable. However, a more natural view
of this setting is to simply assume a single task/context."
SUPERVISED LEARNING AND REINFORCEMENT LEARNING ASSUMPTIONS,0.18591549295774648,"2.2
SUPERVISED LEARNING AND REINFORCEMENT LEARNING ASSUMPTIONS"
SUPERVISED LEARNING AND REINFORCEMENT LEARNING ASSUMPTIONS,0.18873239436619718,"So far we have introduced settings and assumptions that revolve mainly around the type and pres-
ence of non-stationarity in the environment and the information observed by the agent. These have
allowed us to deﬁne the CL problem. To bring all of CL research under one umbrella, we introduce
two assumptions, orthogonal to the previous ones, to recover RL and SL settings. With these as-
sumptions, methods for a given CL setting are applicable to both its CSL and CRL versions, as in
Kirkpatrick et al. (2017); Fernando et al. (2017)."
SUPERVISED LEARNING AND REINFORCEMENT LEARNING ASSUMPTIONS,0.19154929577464788,"Below we use the term observation as a state in RL parlance and the actions as predictions in SL
parlance. Also, we assume a single context or task."
SUPERVISED LEARNING AND REINFORCEMENT LEARNING ASSUMPTIONS,0.19436619718309858,"Level of feedback: In RL, the feedback function rpx, a, zq returns a reward that informs the agent
about the value of performing action a after receiving observation x in context z. In SL however, the
feedback function is generally both directly known by the agent and differentiable, which allows the
agent to simultaneously consider the value of all actions for a particular observation. This feedback
is computed based on a label when the action space is discrete (classiﬁcation) or a target when it is
continuous (regression). The feedback level is a key differentiating feature between RL and SL."
SUPERVISED LEARNING AND REINFORCEMENT LEARNING ASSUMPTIONS,0.19718309859154928,"Active vs passive environments: In RL, it is generally assumed that the agent’s action has an effect
on the next observation or state.1 In other words, the dynamics of the environment ppx1|x, a, zq are
action-dependant and we call this an active environment. In SL the agent is generally assumed to
not inﬂuence the next observation i.e. ppx1|x, a, zq “ ppx1|x, zq. The environment is thus referred
to as being passive in these cases."
SUPERVISED LEARNING AND REINFORCEMENT LEARNING ASSUMPTIONS,0.2,"As seen in Figure 2, the two aforementioned assumptions are combined into a single assumption for
SL (blue, left) and for RL (red, right). By combining either the RL or SL assumption along with
those from the the central CL “trunk”, settings from CSL and CRL are recovered. Future versions of
Sequoia will decouple these assumptions to enable settings such as bandits and imitation learning."
ADDITIONAL ASSUMPTIONS,0.2028169014084507,"2.3
ADDITIONAL ASSUMPTIONS"
ADDITIONAL ASSUMPTIONS,0.2056338028169014,"Additional assumptions can be added on top of the ones described above to recover additional re-
search settings. For example, a useful assumption in CL experiments is the one of disjoint versus
joint action space, i.e. whether the contexts/tasks share a same action space, or whether that space
is different for each task. In CSL, this assumption differentiates class-incremental learning from
domain-incremental learning (van de Ven & Tolias, 2019). In Farquhar & Gal (2018), where it is re-
ferred as the shared output space assumption, a disjoint action space greatly increases the difﬁculty
of a setting in terms of forgetting. In CRL however, the studied settings mostly have a joint action
space, with the notable exception in the work of Chandak et al. (2020a)."
ADDITIONAL ASSUMPTIONS,0.2084507042253521,"Other assumptions could also be relevant in deﬁning a continual learning problem. For instance,
the action space being either discrete or continuous, resulting in classiﬁcation and regression CSL
problems, respectively; a particular structure being required of the method’s actions, as in image
segmentation problems; an episodic vs non-episodic setting in RL; context-dependant (Caccia et al.,
2020) versus context-independent feedback functions; and many more."
ADDITIONAL ASSUMPTIONS,0.2112676056338028,1The bandit setting is one notable exception to this rule.
ADDITIONAL ASSUMPTIONS,0.2140845070422535,Under review as a conference paper at ICLR 2022
SEQUOIA - A SOFTWARE FRAMEWORK,0.21690140845070421,"3
SEQUOIA - A SOFTWARE FRAMEWORK"
SEQUOIA - A SOFTWARE FRAMEWORK,0.21971830985915494,"Alongside this unifying perspective, we introduce Sequoia, an open-source python framework. Each
setting described above is instantiated as a class in a tree-shape inheritance hierarchy. Sequoia is
designed to address some of the issues associated with Continual Learning research, previously
described in § 1."
SEQUOIA - A SOFTWARE FRAMEWORK,0.22253521126760564,"First, we establish a clear separation of concerns between research problems and the solutions to
such problems. We establish this separation through two core abstractions: Setting and Method.
This decoupling greatly helps to evaluate methods, since the logic for each component is cleanly
separated, and extracting a component and reusing it elsewhere becomes possible. An example of a
Method is shown in Listing 2."
SEQUOIA - A SOFTWARE FRAMEWORK,0.22535211267605634,"Second, to help bridge the gap between the CRL and CSL domains, Sequoia uses Environment as
the interface between methods and settings. Environments extend the familiar abstractions from
OpenAI gym, to also include supervised learning datasets, making it possible to develop methods
that are applicable in both the CRL and CSL domains. Environment will be described in § 3.2."
SEQUOIA - A SOFTWARE FRAMEWORK,0.22816901408450704,"Finally, Sequoia uses inheritance to make methods directly reusable across settings. By orga-
nizing research settings into a tree-shaped inheritance hierarchy, along with their environments,
observations, actions, and rewards, Sequoia enables methods developed for any particular setting
to be applicable onto any of their descendants, since all the objects the method will interact with
will inherit from those they were designed to handle. This mechanism has the potential to greatly
improve code reuse and reproducibility in CL research."
SEQUOIA - A SOFTWARE FRAMEWORK,0.23098591549295774,"This section ﬁrst describes each of these abstractions in more detail, after which the currently avail-
able settings and methods are described. § 4 will then provide a demonstration of the kind of
large-scale empirical studies which are made possible through the use of this new framework."
SEQUOIA - A SOFTWARE FRAMEWORK,0.23380281690140844,"from sequoia.settings.sl import *
from sequoia.settings.rl import *
from sequoia.methods import BaseMethod"
SEQUOIA - A SOFTWARE FRAMEWORK,0.23661971830985915,method = BaseMethod(learning_rate=1e-3)
SEQUOIA - A SOFTWARE FRAMEWORK,0.23943661971830985,"for setting in [
ContinuousTaskAgnosticSLSetting(""mnist""),
ContinuousTaskAgnosticRLSetting(""cartpole""),
DiscreteTaskAgnosticSLSetting(""mnist""),
DiscreteTaskAgnosticRLSetting(""cartpole""),
IncrementalSLSetting(""mnist""),
IncrementalRLSetting(""cartpole""),
TaskIncrementalSLSetting(""mnist""),
TaskIncrementalRLSetting(""cartpole""),
MultiTaskSLSetting(""mnist""),
MultiTaskRLSetting(""cartpole""),
TraditionalSLSetting(""mnist""),
TraditionalRLSetting(""cartpole""),
]:
results = setting.apply(method)
results.summary()
results.make_plots()"
SEQUOIA - A SOFTWARE FRAMEWORK,0.24225352112676057,"Listing 1: Code snippet, using Sequoia to evalu-
ate a method in multiple settings. This particular
example is made possible by the BaseMethod,
which is applicable to all settings."
SEQUOIA - A SOFTWARE FRAMEWORK,0.24507042253521127,"Relation with other frameworks: Sequoia is
in no way competing with existing tools and
libraries which provide standardized bench-
marks, models, or algorithm implementations.
On the contrary, Sequoia beneﬁts from the de-
velopment of such frameworks."
SEQUOIA - A SOFTWARE FRAMEWORK,0.24788732394366197,"In the case of libraries that introduce standard-
ized benchmarks, they can be used to enrich
existing settings with additional datasets or en-
vironments (Douillard & Lesort, 2021; Brock-
man et al., 2016), or even to create entirely
new settings. Likewise, frameworks which in-
troduce new models or algorithms (Lomonaco
et al., 2021; Rafﬁn et al., 2019; Wolczyk et al.,
2021) can also be used to create new Methods
or to add new backbones to existing Methods
within Sequoia. External repositories can reg-
ister their own methods through a simple plu-
gin system. The end goal for Sequoia is to pro-
vide the research community with a central-
ized catalog of the different research frame-
works and their associated methods, settings,
environments, etc. The following sections will
show examples of such extensions."
SETTINGS,0.2507042253521127,"3.1
SETTINGS"
SETTINGS,0.2535211267605634,"A Setting can be viewed as a conﬁgurable evaluation procedure for a Method. It creates vari-
ous training environments, evaluates the method, and ﬁnally returns some Results. These results
contain various metrics relevant to the setting. The training/testing routine for each setting is imple-"
SETTINGS,0.2563380281690141,Under review as a conference paper at ICLR 2022
SETTINGS,0.2591549295774648,"mented according to the evaluation protocol of that setting in the literature. An example of applying
a method onto multiple settings is shown in Listing 1."
SETTINGS,0.2619718309859155,"Concretely, settings create the training, validation, and testing environments that a method interacts
with. This interface also makes it possible for methods to leverage PyTorch-Lightning (Falcon et
al., 2019) to perform high-performance training of their models.2 For more information on the
interactions between Sequoia and PyTorch-Lightning, see App. C."
SETTINGS,0.2647887323943662,"Settings are available for each combination of the CL assumptions, along with the choice of one of
RL / SL (as illustrated in Figure 2), for a total of 12 settings.3 These two “branches” (one for CRL
and the other CSL) form the basis of Sequoia’s eponymous tree of settings. Each setting inherits
from one or more parent settings, following the above-mentioned organization."
SETTINGS,0.2676056338028169,"Every Setting is created by extending a more general Setting and adding additional assumptions.
This inheritance relationship from one setting to the next also extends to the setting’s environments
(Environment), as well as the objects (Observations, Actions, and Rewards) they create. See
App. D for an illustration of this principle."
ENVIRONMENTS,0.2704225352112676,"3.2
ENVIRONMENTS"
ENVIRONMENTS,0.27323943661971833,"Settings in Sequoia create training, validation, and testing Environments, which adhere to both
the gym.Env and the torch.DataLoader APIs. This makes it easy for SL researchers to
transition to RL and vice-versa. These environments receive Actions and return Observations
and Rewards. Observations contain the input samples x, and may also contain task labels for
each sample, depending on the setting. These objects have the same structure in both RL and SL
settings. However, as described in § 2.2, in SL, Actions correspond to the predictions, while
Rewards correspond to targets or labels. These objects are deﬁned by the Setting and follow the
same pattern of inheritance as the settings themselves. The structure of these objects are reﬂected in
the environment’s observation, action, and reward spaces, which are used within methods to create
their models."
ENVIRONMENTS,0.27605633802816903,"Supervised Learning environments
Sequoia supports most of the datasets traditionally used in
continual supervised learning research, through its use of the Continuum package (Douillard &
Lesort, 2021). The list of supported datasets is available in Table 1."
ENVIRONMENTS,0.27887323943661974,"Reinforcement Learning environments
Through its close integration with gym, Sequoia is able
to use any gym-compatible environment as the “dataset” used by its RL settings. For settings with
multiple tasks, Sequoia simply needs a way to sample new tasks for the chosen environment. This
mechanism makes it easy to add support for new or existing gym environments. An example of this
is included in App. D."
ENVIRONMENTS,0.28169014084507044,"Sequoia creates continuous or discrete tasks, depending on the choice of setting and dataset/en-
vironment. For example, when using one of the classic-control environments from gym such as
CartPole, tasks are created by sampling a new set of values for the environment constants such as
the gravity, the length of the pole, the mass of the cart, etc. This is also the case for the well-known
HalfCheetah, Walker2d, and Hopper MuJoCo environments, where tasks can be created by
introducing changes in the environmental constants such as gravity. Continuous tasks can thus easily
be created in this case, as the environment is able to respond dynamically to changes in these values
at every step, and the task can evolve smoothly by interpolating between different target values."
ENVIRONMENTS,0.28450704225352114,"Other gym environments become available when using RL settings with discrete tasks (i.e. all
settings that inherit from DiscreteTaskAgnosticRLSetting), as it becomes possible to simply
give Sequoia a list of environments to use for each task, and the constructed Setting will then use
them as part of its evaluation procedure."
ENVIRONMENTS,0.28732394366197184,"We use this feature to construct continual variants of the MT10, MT50 benchmarks from Meta-
World (Yu et al., 2019), as well to replicate the CW10 and CW20 benchmarks introduced in Wolczyk"
ENVIRONMENTS,0.29014084507042254,"2It is important to note that methods are in no way required to use PyTorch-Lightning.
3Other common SL settings, such as Domain-Incremental learning are also available in Sequoia, but they
rely on an additional family of assumption, and are thus omitted from the main portion of this paper for sake of
brevity and clarity."
ENVIRONMENTS,0.29295774647887324,Under review as a conference paper at ICLR 2022
ENVIRONMENTS,0.29577464788732394,"Methods
SL
BaseMethod.{base, EWC, PackNet }, PNN, replay, HAT, CN-DPM
Avalanche.{naive, AGEM, CWR˚, EWC, Gdumb, GEM, LWF, replay, SI}
RL
BaseMethod.{base, EWC, PackNet }, PNN
stable_baselines3.{A2C, DDPG, DQN, PPO, SAC, TD3}
continual_world.{SAC, AGEM, EWC, VCL, PackNet, L2 reg., MAS, replay}
Environments
SL
continuum.{{K,E,Q,Fashion}MNIST, Cifar10(0), ImageNet100(0), Core50, Synbols}
RL
gym.{Hopper, Half-Chettah, Walker2d, CartPole, Pendulum, MontainCar} Monsterkong,
metaworld.{MT10, MT50}, continual_world.{CW10, CW20}
Metrics
{Transfer Matrix, forward transfer, backward transfer, Average ﬁnal performance,
Online Training Performance} ˆ
SL
{loss, accuracy}
RL
{loss, total reward, average reward, episode length}"
ENVIRONMENTS,0.29859154929577464,"Table 1: Sequoia’s methods, environments and metrics. Most of the RL settings in Sequoia can
be passed custom environments to use for each task. This makes it possible to use virtually any
gym environment to create custom incremental RL settings. The environments listed here are those
explicitly supported in Sequoia, where multiple tasks can be sampled within a single environment."
ENVIRONMENTS,0.30140845070422534,"import gym
from sequoia.settings import Setting, Environment, Observations, Actions
from sequoia.methods import Method"
ENVIRONMENTS,0.30422535211267604,class DemoModel:
ENVIRONMENTS,0.30704225352112674,"def forward(self, observations: Observations) -> Actions: ..."
ENVIRONMENTS,0.30985915492957744,"class DemoMethod(Method, target_setting=Setting):"
ENVIRONMENTS,0.3126760563380282,""""""" Pseudocode for a Method that targets a given Setting. """"""
def configure(self, setting: Setting):"
ENVIRONMENTS,0.3154929577464789,"# Called by the setting before training begins.
self.model = DemoModel(setting.observation_space, setting.action_space,
setting.reward_space, nb_tasks=setting.nb_tasks)
self.optimizer = Adam(...)"
ENVIRONMENTS,0.3183098591549296,"def fit(self, train_env: Environment, valid_env: Environment):"
ENVIRONMENTS,0.3211267605633803,"# Train a model using these environments from the setting.
# Note: all Environments are gym environments. More on this later.
for epoch in range(self.n_epochs):"
ENVIRONMENTS,0.323943661971831,"self.model.train_epoch(train_env)
self.model.validation_epoch(valid_env)"
ENVIRONMENTS,0.3267605633802817,"def get_actions(self, observations: Observations) -> Actions:"
ENVIRONMENTS,0.3295774647887324,"# Called by the setting for inference (at test-time).
actions = self.model(observations)
return actions"
ENVIRONMENTS,0.3323943661971831,"def on_task_switch(self, task_id: Optional[int]):"
ENVIRONMENTS,0.3352112676056338,"# Gets called on task boundaries, depending on the setting.
self.model.prepare_for_new_task(new_task=task_id)"
ENVIRONMENTS,0.3380281690140845,Listing 2: Pseudocode for creating a new Method.
ENVIRONMENTS,0.3408450704225352,"et al. (2021). Sequoia also introduces a non-stationary version of the MonsterKong environment,
described in App. E.2. A more complete list of the supported environments is shown in Table 1."
METHODS,0.3436619718309859,"3.3
METHODS"
METHODS,0.3464788732394366,"Methods hold the logic related to the model and training algorithm. When deﬁned, each method
selects a “target setting” from those available in the tree. A method can be applied to its target
setting as well as any setting which inherits from it (i.e. any setting which is a child node of the
target setting). We now provide a brief description of the different types of Methods available in
Sequoia. An illustration of the Method API can be seen in Listing 2."
METHODS,0.3492957746478873,"General methods:
Methods in Sequoia can target settings from either the RL or SL branches of
the tree. Additionally, it is also possible to select one of the settings from the central CL branch - for
instance, Incremental Learning. This makes methods applicable to both the CRL and CSL variants
of that setting. One such method is the BaseMethod, which can be applied to any setting in the tree,"
METHODS,0.352112676056338,Under review as a conference paper at ICLR 2022
METHODS,0.35492957746478876,"and is provided as a modular, customizable, jumping off point for new users. This BaseMethod is
equipped with modules for task inference and multi-head prediction. See App. B for a more in-depth
discussion of its features and capabilities."
METHODS,0.35774647887323946,"Supervised Learning Methods:
Sequoia beneﬁts from other CL frameworks such as Avalanche
(Lomonaco et al., 2021). Avalanche offers both standardized benchmarks as well as a growing set
of CL methods, which are referred to as strategies in Avalanche. Sequoia reuses these strategies as
Method classes. See Table 1 for a complete list of such methods."
METHODS,0.36056338028169016,"Reinforcement Learning Methods:
Settings in Sequoia produce Environments, which adhere
exactly to the Gym API. It is therefore easy to import existing RL tools and libraries and use them
to create new methods."
METHODS,0.36338028169014086,"As an example, here we enlist the help of a specialized framework for RL, namely stable-
baselines3 (Rafﬁn et al., 2019) . The A2C, PPO, DQN, DDPG, TD3 and SAC algorithms from
SB3 were easily introduced as new Method classes in Sequoia, without duplicating any code. These
methods are applicable onto any of the RL settings in the tree."
METHODS,0.36619718309859156,"In addition to these RL backbones from SB3, Continual reinforcement learning methods are also
available. These methods were adapted from the work of Wolczyk et al. (2021), which introduced
the CW10 and CW20 benchmarks for continual learning, based on sequences of tasks from Meta-
World (Yu et al., 2019). The authors also provided implementations for CRL algorithms, built on
top of a SAC(Haarnoja et al., 2018) backbone. These algorithms were adapted from their original
implementation and made available as CRL methods in Sequoia (see Table 1 for full the list).4"
EXPERIMENTS,0.36901408450704226,"4
EXPERIMENTS"
EXPERIMENTS,0.37183098591549296,"Sequoia’s design makes it easy to conduct large-scale experiments to compare the performance
of different methods on a given setting, or to evaluate the performance of a given method across
multiple settings and datasets. We illustrate this by performing large-scale empirical studies in-
volving all the settings and methods available in Sequoia, both in CRL and CSL. Each study
involves up to 20 hyper-parameter conﬁgurations for each combination of setting, method, and
dataset, in both RL and SL, for a combined total of « 8000 individual completed runs. The rest
of this section provides a brief overview of these experiments, which are also publicly available at
https://wandb.ai/sequoia/.5 All results are reproduced in App. F in a larger format and
accompanied with further analysis."
EXPERIMENTS,0.37464788732394366,"Continual Supervised Learning. As part of the CSL study, we use some of the “standard” image
classiﬁcation datasets such as MNIST (LeCun & Cortes, 2010), Cifar10, and Cifar100 (Krizhevsky
et al., 2009). Furthermore, we also include the Synbols (Lacoste et al., 2020) dataset, a character
dataset composed of two independent labels: the characters and the fonts. (see App. E.1 for the
motivation). Exhaustive results can be found at https://wandb.ai/sequoia/csl_study."
EXPERIMENTS,0.37746478873239436,"A sample of these results is illustrated in Figure 3, which shows results of various methods in the
class-IL and task-IL settings in terms of their ﬁnal performance and runtime. We note that some
Avalanche methods achieve lower than chance accuracy in task-IL because they do not use the
task label to mask out the classes that lie outside the tested task."
EXPERIMENTS,0.38028169014084506,"Continual Reinforcement Learning. We apply the RL methods from SB3 on multiple bench-
marks built on HalfCheetah-v2, Hopper-v2, MountainCar-v0, CartPole-v0, MetaWorld-v2 (details
in App. E). We also introduce a new discrete domain benchmark, namely Continual-MonsterKong,
that we developed to study forward transfer in a more meaningful way (see App. E.2 for more de-
tails). Complete results are available at https://wandb.ai/sequoia/crl_study."
EXPERIMENTS,0.38309859154929576,"A sample of these results is in Figure 4. It presents various methods in the traditional and incremental
learning settings with their ﬁnal performance, online performance and normalized runtime."
EXPERIMENTS,0.38591549295774646,"In Table 2, we apply the continual-world methods, built on top of SAC, on a incremental RL
benchmark inspired by Mendez et al. (2020) (see App. E.3 for more details). Finally, Figure 11
shows the transfer matrix achieved by one such algorithm, namely PPO (Schulman et al., 2017)."
EXPERIMENTS,0.38873239436619716,"4While most other methods use PyTorch these methods are implemented using Tensorﬂow.
5We will update these sample studies periodically to reﬂect all future improvements made to the framework."
EXPERIMENTS,0.39154929577464787,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.39436619718309857,"10
2
10
3
10
4"
EXPERIMENTS,0.3971830985915493,Runtime (seconds) 0.0 0.5 1.0
EXPERIMENTS,0.4,Final Performance
EXPERIMENTS,0.4028169014084507,Task-Incremental MNIST
EXPERIMENTS,0.4056338028169014,"10
1
10
2
10
3
10
4"
EXPERIMENTS,0.4084507042253521,Runtime (seconds) 0.0 0.5 1.0
EXPERIMENTS,0.4112676056338028,Final Performance
EXPERIMENTS,0.4140845070422535,Task-Incremental CIFAR10
EXPERIMENTS,0.4169014084507042,"10
2
10
3
10
4
10
5"
EXPERIMENTS,0.4197183098591549,Runtime (seconds) 0.0 0.5 1.0
EXPERIMENTS,0.4225352112676056,Final Performance
EXPERIMENTS,0.4253521126760563,"Task-Incremental Synbols
Avalanche.GDumb
BaseMethod.EWC
PNN
HAT
BaseMethod
Avalanche.GEM
Avalanche.EWC
Avalanche.SI
Avalanche.Replay"
EXPERIMENTS,0.428169014084507,"10
2
10
3
10
4"
EXPERIMENTS,0.4309859154929577,Runtime (seconds) 0.0 0.5 1.0
EXPERIMENTS,0.43380281690140843,Final Performance
EXPERIMENTS,0.43661971830985913,Class-Incremental MNIST
EXPERIMENTS,0.4394366197183099,"10
2
10
3
10
4
10
5"
EXPERIMENTS,0.4422535211267606,Runtime (seconds) 0.0 0.5 1.0
EXPERIMENTS,0.4450704225352113,Final Performance
EXPERIMENTS,0.447887323943662,Class-Incremental CIFAR10
EXPERIMENTS,0.4507042253521127,"10
2
10
3
10
4
10
5"
EXPERIMENTS,0.4535211267605634,Runtime (seconds) 0.0 0.5 1.0
EXPERIMENTS,0.4563380281690141,Final Performance
EXPERIMENTS,0.4591549295774648,"Class-Incremental Synbols
Avalanche.GDumb
BaseMethod.EWC
BaseMethod
Avalanche.SI
Avalanche.GEM
Avalanche.EWC
Replay
Avalanche.Replay"
EXPERIMENTS,0.4619718309859155,"Figure 3: Incremental Supervised Learning results. Final performance (vertical axis) is plotted
against runtime (horizontal axis). The methods achieving the best trade-off lie closer to the top-left
of the ﬁgures. Task-Incremental and Class-Incremental results are presented on the top and bottom
row, respectively. The dotted line shows chance accuracy for each setting-dataset combination. For
each methods, several trials are presented depending on metrics composed of linear combination of
ﬁnal performance and (normalized) runtime. GEM and GDumb achieve the best tradoff, although
the latter cannot make predictions in an online manner and thus serves more as a reference point."
EXPERIMENTS,0.4647887323943662,"0
2500
5000
7500
Online Performance 0 5000 10000"
EXPERIMENTS,0.4676056338028169,Final Performance
EXPERIMENTS,0.4704225352112676,Traditional HalfCheetah-gravity
EXPERIMENTS,0.4732394366197183,"0
2000
4000
Online Performance 0 2500 5000"
EXPERIMENTS,0.476056338028169,Final Performance
EXPERIMENTS,0.4788732394366197,Incremental HalfCheetah-gravity
EXPERIMENTS,0.48169014084507045,"SB3.DDPG
SB3.TD3
SB3.A2C
SB3.PPO
SB3.SAC"
EXPERIMENTS,0.48450704225352115,"18
20
22
24
Online Performance 20 30 40"
EXPERIMENTS,0.48732394366197185,Final Performance
EXPERIMENTS,0.49014084507042255,Traditional MonsterKong
EXPERIMENTS,0.49295774647887325,"20
40
60
80
Online Performance 0 20 40 60"
EXPERIMENTS,0.49577464788732395,Final Performance
EXPERIMENTS,0.49859154929577465,Incremental MonsterKong
EXPERIMENTS,0.5014084507042254,"SB3.PPO
SB3.A2C
SB3.DQN"
EXPERIMENTS,0.504225352112676,"Figure 4: Impact of the RL backbone algorithm in Traditional
and Incremental RL. Final performance (vertical axis) is plotted
against online performance (horizontal axis). The bubbles’ size
indicates the normalized runtime of the methods. The methods
achieving the best trade-off lie closer to the top-right of the ﬁg-
ures and have smaller bubble size. Datasets are presented in each
row and settings are presented in each column. For each method,
several trials are presented depending on metrics composed of
linear combination of ﬁnal performance and online performance."
EXPERIMENTS,0.5070422535211268,"FP (Ò)
OP (Ò)
R (Ó)
Fine-tuning
309
278
17.1
L2
473
236
19.2
EWC
529
230
20.4
MAS
657
238
20.0
PackNet
1002
267
20
Perfect Memory
1134
271
20.2
A-GEM
776
281
24.5"
EXPERIMENTS,0.5098591549295775,"Table 2: Incremental RL results.
Multiple CRL methods, all built
on top of SAC, are tested on the
Hopper-Bodyparts benchmarks. FP
and OP stands for ﬁnal and online
performance, respectively, whereas
R stands for runtime, reported in
hours.
Results are averaged over
5 seeds.
All CRL methods out-
performed the Fine-tuning baseline,
validating their efﬁcacy.
Experi-
ence replay with a Perfect Memory
achieves the best retained perfor-
mance on all tasks, followed closely
by PackNet."
CONCLUSION,0.5126760563380282,"5
CONCLUSION"
CONCLUSION,0.5154929577464789,"In this work, we introduce Sequoia: a publicly available framework to organize virtually all research
settings from both the ﬁelds of Continual Supervised and Continual Reinforcement learning. Se-
quoia also makes methods are directly reusable by contract across settings using inheritance. It is
our hope that Sequoia will be useful to new and experienced researchers in CL. Further, the princi-
ples used to construct this framework for CL could very well be applied to other ﬁelds of research,
effectively growing the tree towards new and interesting directions. We welcome suggestions and
contributions to that effect in our GitHub page at (this GitHub url)."
CONCLUSION,0.5183098591549296,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.5211267605633803,"Reproducibility statement
To facilitate reproducing results of our experiments, we include an
anonymized version of the Sequoia codebase. All results from the experiments of § 4 can be ob-
served at https://wandb.ai/sequoia. Each run includes the exact command used, as well
as the git state, the complete system speciﬁcation, hyper-parameter conﬁgurations, and random seeds
used."
REPRODUCIBILITY STATEMENT,0.523943661971831,"While most sources of randomness are accounted for in Sequoia, we are still in the process of
making settings entirely deterministic given a random seed. In other words, for some combinations
of settings and methods, launching two runs with the exact same arguments and seeds do sometimes
produce different results. Making settings and methods entirely deterministic is part of the plans for
future work in this project."
REFERENCES,0.5267605633802817,REFERENCES
REFERENCES,0.5295774647887324,"Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget, 2018."
REFERENCES,0.532394366197183,"Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars.
Task-free continual learning.
2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11246–11255,
2019a."
REFERENCES,0.5352112676056338,"Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection
for online continual learning. In Advances in Neural Information Processing Systems (NeurIPS),
2019b."
REFERENCES,0.5380281690140845,"Haitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew Taylor. Online multi-task learning for
policy gradient methods. In International conference on machine learning, pp. 1206–1214, 2014."
REFERENCES,0.5408450704225352,"Andr´e Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel
Mankowitz, Augustin ˇZ´ıdek, and Remi Munos. Transfer in deep reinforcement learning using
successor features and generalised policy improvement. arXiv preprint arXiv:1901.10964, 2019."
REFERENCES,0.543661971830986,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.5464788732394367,"Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Page-
Caccia, Issam Hadj Laradji, Irina Rish, Alexandre Lacoste, David V´azquez, et al. Online fast
adaptation and knowledge accumulation (osaka): a new approach to continual learning. Advances
in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.5492957746478874,"Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli. Sparse multi-task reinforcement
learning. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger
(eds.), Advances in neural information processing systems 27, pp. 819–827. Curran Associates,
Inc., 2014."
REFERENCES,0.5521126760563381,"Yash Chandak, Georgios Theocharous, Chris Nota, and Philip Thomas. Lifelong learning with a
changing action set. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34,
pp. 3373–3380, 2020a."
REFERENCES,0.5549295774647888,"Yash Chandak, Georgios Theocharous, Shiv Shankar, Sridhar Mahadevan, Martha White, and
Philip S Thomas.
Optimizing for the future in non-stationary mdps.
arXiv preprint
arXiv:2005.08158, 2020b."
REFERENCES,0.5577464788732395,"Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient
lifelong learning with A-GEM. In International Conference of Learning Representations (ICLR),
2019."
REFERENCES,0.5605633802816902,"Samuel PM Choi, Dit-Yan Yeung, and Nevin L Zhang. Hidden-mode markov decision processes for
nonstationary sequential decision making. In Sequence Learning, pp. 264–287. Springer, 2000."
REFERENCES,0.5633802816901409,"Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021."
REFERENCES,0.5661971830985916,Under review as a conference paper at ICLR 2022
REFERENCES,0.5690140845070423,"Arthur Douillard and Timoth´ee Lesort.
Continuum: Simple management of complex continual
learning scenarios, 2021."
REFERENCES,0.571830985915493,"William
Falcon
et
al.
Pytorch
lightning.
GitHub.
Note:
https://github.com/PyTorchLightning/pytorch-lightning, 3, 2019."
REFERENCES,0.5746478873239437,"Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization in
dqn. arXiv preprint arXiv:1810.00123, 2018."
REFERENCES,0.5774647887323944,"Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint
arXiv:1805.09733, 2018."
REFERENCES,0.5802816901408451,"Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super
neural networks. arXiv preprint arXiv:1701.08734, 2017."
REFERENCES,0.5830985915492958,"Robert M. French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences,
3(4):128–135, 1999. ISSN 13646613. doi: 10.1016/S1364-6613(99)01294-2. URL https://
www.sciencedirect.com/science/article/abs/pii/S1364661399012942."
REFERENCES,0.5859154929577465,"Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018."
REFERENCES,0.5887323943661972,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1861–1870. PMLR, 2018."
REFERENCES,0.5915492957746479,"James Harrison, Apoorva Sharma, Chelsea Finn, and Marco Pavone. Continuous meta-learning
without tasks. ArXiv, abs/1912.08866, 2019."
REFERENCES,0.5943661971830986,"Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A. Rusu, Yee Whye Teh, and Razvan Pas-
canu. Task agnostic continual learning via meta learning. ArXiv, abs/1906.05201, 2019."
REFERENCES,0.5971830985915493,"Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 32, 2018."
REFERENCES,0.6,"Christos Kaplanis, Claudia Clopath, and Murray Shanahan. Continual reinforcement learning with
multi-timescale replay. arXiv preprint arXiv:2004.07530, 2020."
REFERENCES,0.6028169014084507,"Khimya Khetarpal, Zafarali Ahmed, Andre Cianﬂone, Riashat Islam, and Joelle Pineau.
Re-
evaluate: Reproducibility in evaluating reinforcement learning algorithms. 2018."
REFERENCES,0.6056338028169014,"Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforce-
ment learning: A review and perspectives. arXiv preprint arXiv:2012.13490, 2020."
REFERENCES,0.6084507042253521,"James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521–3526, 2017."
REFERENCES,0.6112676056338028,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009."
REFERENCES,0.6140845070422535,"Alexandre Lacoste,
Pau Rodr´ıguez L´opez,
Frederic Branchaud-Charron,
Parmida Atighe-
hchian, Massimo Caccia, Issam Hadj Laradji, Alexandre Drouin, Matthew Craddock, Lau-
rent Charlin, and David V´azquez.
Synbols:
Probing learning algorithms with synthetic
datasets.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 134–146. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
0169cf885f882efd795951253db5cdfb-Paper.pdf."
REFERENCES,0.6169014084507042,"Nicholas C. Landolﬁ, Garrett Thomas, and Tengyu Ma.
A model-based approach for sample-
efﬁcient multi-task reinforcement learning, 2019."
REFERENCES,0.6197183098591549,Under review as a conference paper at ICLR 2022
REFERENCES,0.6225352112676056,"Yann
LeCun
and
Corinna
Cortes.
MNIST
handwritten
digit
database.
http://yann.lecun.com/exdb/mnist/, 2010."
REFERENCES,0.6253521126760564,"Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture
model for task-free continual learning, 2020."
REFERENCES,0.6281690140845071,"Timoth´ee Lesort, Hugo Caselles-Dupr´e, Michael Garcia-Ortiz, Jean-Franc¸ois Goudou, and David
Filliat. Generative Models from the perspective of Continual Learning. In International Joint
Conference on Neural Networks (IJCNN), 2019a."
REFERENCES,0.6309859154929578,"Timoth´ee Lesort, Alexander Gepperth, Andrei Stoian, and David Filliat. Marginal replay vs condi-
tional replay for continual learning. In International Conference on Artiﬁcial Neural Networks,
pp. 466–480. Springer, 2019b. URL https://arxiv.org/abs/1810.12069."
REFERENCES,0.6338028169014085,"Timoth´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia
D´ıaz-Rodr´ıguez.
Continual learning for robotics: Deﬁnition, framework, learning strategies,
opportunities and challenges. Information Fusion, 58:52 – 68, 2020. ISSN 1566-2535. doi:
https://doi.org/10.1016/j.inffus.2019.12.004.
URL http://www.sciencedirect.com/
science/article/pii/S1566253519307377."
REFERENCES,0.6366197183098592,"HongLin Li, Payam Barnaghi, Shirin Enshaeifar, and Frieder Ganz.
Continual learning using
bayesian neural networks, 2019."
REFERENCES,0.6394366197183099,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.6422535211267606,"Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Grafﬁeti, Tyler L.
Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She,
Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio
Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu,
Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, and Davide Maltoni.
Avalanche: an end-to-end library for continual learning, 2021."
REFERENCES,0.6450704225352113,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems (NIPS), 2017."
REFERENCES,0.647887323943662,"Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning, 2018."
REFERENCES,0.6507042253521127,"Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of multitask
representation learning. J. Mach. Learn. Res., 17(1):2853–2884, January 2016. ISSN 1532-4435."
REFERENCES,0.6535211267605634,"Jorge Armando Mendez, Boyu Wang, and Eric Eaton. Lifelong policy gradient learning of factored
policies for faster training without forgetting. abs/2007.07011, 2020."
REFERENCES,0.6563380281690141,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015."
REFERENCES,0.6591549295774648,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016."
REFERENCES,0.6619718309859155,"Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learn-
ing. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.6647887323943662,"German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 2019."
REFERENCES,0.6676056338028169,"Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer
reinforcement learning. In ICLR, 2016."
REFERENCES,0.6704225352112676,Under review as a conference paper at ICLR 2022
REFERENCES,0.6732394366197183,"Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach that questions
our progress in continual learning. 2020. URL http://www.robots.ox.ac.uk/˜tvg/
publications/2020/gdumb.pdf."
REFERENCES,0.676056338028169,"Antonin Rafﬁn, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor-
mann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019."
REFERENCES,0.6788732394366197,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classiﬁer and representation learning. In Computer Vision and Pattern Recognition
(CVPR), 2017."
REFERENCES,0.6816901408450704,"Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. arXiv preprint arXiv:1810.11910, 2018."
REFERENCES,0.6845070422535211,"Mark B Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104,
1997."
REFERENCES,0.6873239436619718,"David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience
replay for continual learning. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.6901408450704225,"A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu,
and R. Hadsell. Progressive Neural Networks. ArXiv e-prints, 2016."
REFERENCES,0.6929577464788732,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.6957746478873239,"Joan Serr`a, D´ıdac Sur´ıs, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018."
REFERENCES,0.6985915492957746,"Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems (NIPS), 2017."
REFERENCES,0.7014084507042253,"Norman
Tasﬁ.
Pygame
learning
environment.
https://github.com/ntasfi/
PyGame-Learning-Environment, 2016."
REFERENCES,0.704225352112676,"Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(1):1633–1685, 2009."
REFERENCES,0.7070422535211267,"Sebastian Thrun and Tom M Mitchell. Lifelong robot learning. Robotics and autonomous systems,
15(1-2):25–46, 1995."
REFERENCES,0.7098591549295775,"Sebastian Thrun and Anton Schwartz. Finding structure in reinforcement learning. In Advances in
neural information processing systems, pp. 385–392, 1995."
REFERENCES,0.7126760563380282,"Ren´e Traor´e, Hugo Caselles-Dupr´e, Timoth´ee Lesort, Te Sun, Guanghang Cai, Natalia D´ıaz
Rodr´ıguez, and David Filliat. Discorl: Continual reinforcement learning via policy distillation.
CoRR, abs/1907.05855, 2019. URL http://arxiv.org/abs/1907.05855."
REFERENCES,0.7154929577464789,"Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint
arXiv:1904.07734, 2019."
REFERENCES,0.7183098591549296,"Maciej Wolczyk, Michal Zajac, Razvan Pascanu, Lukasz Kucinski, and Piotr Milos. Continual
world: A robotic benchmark for continual reinforcement learning. CoRR, abs/2105.10919, 2021.
URL https://arxiv.org/abs/2105.10919."
REFERENCES,0.7211267605633803,"Annie Xie, James Harrison, and Chelsea Finn. Deep reinforcement learning amidst lifelong non-
stationarity. arXiv preprint arXiv:2006.10701, 2020."
REFERENCES,0.723943661971831,"Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
CoRR, abs/1910.10897, 2019. URL http://arxiv.org/abs/1910.10897."
REFERENCES,0.7267605633802817,"Friedeman Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In International Conference on Machine Learning (ICML), 2017."
REFERENCES,0.7295774647887324,"Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning using
online variational bayes, 2018."
REFERENCES,0.7323943661971831,Under review as a conference paper at ICLR 2022
REFERENCES,0.7352112676056338,"A
SUPPORTED METHODS"
REFERENCES,0.7380281690140845,"One of Sequoia’s biggest strength is how easy it is to extend.
Most methods in Sequoia
are the result directly reusing existing implementations from other frameworks and repositories,
such as AvalancheLomonaco et al. (2021), Stable-Baselines3Rafﬁn et al. (2019) and Continual
WorldWolczyk et al. (2021). Table 3 shows all the methods currently available in Sequoia."
REFERENCES,0.7408450704225352,"Method
Target setting
BaseMethod
Setting (all)
BaseMethod.EWC Kirkpatrick et al. (2017)
Setting (all)
BaseMethod.PackNet Mallya & Lazebnik (2018)
Incremental Learning (RL + SL)
replay
Incremental SL
CN-DPM Lee et al. (2020)
Continual SL
HAT Serr`a et al. (2018)
Task-Incremental SL
PNN Rusu et al. (2016)
Incremental SL
Avalanche.naive Lomonaco et al. (2021)
Incremental SL
Avalanche.AGEM Chaudhry et al. (2019)
Incremental SL
Avalanche.cwr star Lomonaco et al. (2021)
Incremental SL
Avalanche.EWC Kirkpatrick et al. (2017)
Incremental SL
Avalanche.Gdumb Prabhu et al. (2020)
Incremental SL
Avalanche.GEM Lopez-Paz & Ranzato (2017)
Incremental SL
Avalanche.LWF Li et al. (2019)
Incremental SL
Avalanche.replay
Incremental SL
Avalanche.SI Zenke et al. (2017)
Incremental SL
stable-baselines3.A2C Mnih et al. (2016)
Incremental RL
stable-baselines3.DDPG Lillicrap et al. (2015)
Continual RL
stable-baselines3.DQN Mnih et al. (2015)
Continual RL
stable-baselines3.PPO Schulman et al. (2017)
Continual RL
stable-baselines3.SAC Haarnoja et al. (2018)
Continual RL
stable-baselines3.TD3 Fujimoto et al. (2018)
Continual RL
continual_world.SAC Haarnoja et al. (2018)
Incremental RL
continual_world.AGEM Chaudhry et al. (2019)
Incremental RL
continual_world.EWC Kirkpatrick et al. (2017)
Incremental RL
continual_world.VCL Nguyen et al. (2018)
Incremental RL
continual_world.MAS Aljundi et al. (2018)
Incremental RL
continual_world.L2 regularization
Incremental RL
continual_world.PackNet Mallya & Lazebnik (2018)
Incremental RL
continual_world.Replay
Incremental RL"
REFERENCES,0.7436619718309859,"Table 3: Sequoia’s methods support. Each method speciﬁes a target setting, listed on the right.
Most methods are applicable in either RL or SL, while some can be applied to both. Methods also
specify the “level of nonstationarity” they are prepared to handle, as a choice of one of Continual
(which is also referred to as Continuous Task-Agnostic CL in § 2.1), Discrete, Incremental, Task-
Incremental, Traditional, and Multi-Task."
REFERENCES,0.7464788732394366,"B
THE BASE METHOD"
REFERENCES,0.7492957746478873,"While developing a new Method in Sequoia, users are encouraged to separate the training logic
from the networks used, the former being contained in the Method, and the latter in a model class,
as advocated by PyTorch-Lightning (Falcon et al., 2019) (PL), a powerful research library, which
we employ as part of this BaseMethod."
REFERENCES,0.752112676056338,"The BaseMethod is accompanied by the BaseModel, which acts as a modular and ex-
tendable model for CL Methods to use.
This BaseModel adheres to PyTorch-Lightning’s
LightningModule interface, making it easy to extend and customize with additional callbacks and
loggers. Likewise, the BaseMethod employs a pl.Trainer, which is able to train the BaseModel
on the Environments produced by any setting. Sequoia’s Settings are also closely related to PL’s"
REFERENCES,0.7549295774647887,Under review as a conference paper at ICLR 2022
REFERENCES,0.7577464788732394,Figure 5: UML Diagram showing the main abstractions of Sequoia.
REFERENCES,0.7605633802816901,"DataModule abstraction. See App. C for a further discussion of the relationship between Sequoia
and Pytorch-Lightning."
REFERENCES,0.7633802816901408,"Using this BaseModel when creating a new CL method can be particularly useful when transitioning
from a CL Setting to its parent, as it comes equipped with most of the components required to
handle such transitions (e.g. task inference, multi-head prediction, etc.) These components, as well
as the underlying encoder, output head, loss function, etc. can easily be replaced or customized."
REFERENCES,0.7661971830985915,"Additional losses can also be added to the BaseModel through a modular interface, which was
explicitly designed to facilitate exploration of self-supervised learning research."
REFERENCES,0.7690140845070422,Under review as a conference paper at ICLR 2022
REFERENCES,0.7718309859154929,"C
ADDING NEW SETTINGS TO SEQUOIA"
REFERENCES,0.7746478873239436,"The very simple deﬁnition of the Setting abstract base class means that new Settings are not
required to place themselves into our existing inheritance hierarchy, and could also not be related
to continual learning at all! It is however preferable, whenever possible, to ﬁnd the closest existing
Setting within Sequoia and either add additional assumptions or remove extraneous ones, by creating
the new Setting either below its closest relative or above it when adding or removing assumptions
respectively."
REFERENCES,0.7774647887323943,"The most general Setting in our current hierarchy - also referred to as the ”root“ setting - inherits
from this abstract base class, while also building upon the elegant DataModule abstraction intro-
duced by Pytorch-Lightning Falcon et al. (2019), in which the DataModule is the entity responsible
for the preparation of the data, as well as for the creation of the training, validation and testing
DataLoaders. Models in pytorch lightning can thus easily train and evaluate themselves through
this standardized API, where dataloaders can be swapped out between experiments. Sequoia’s main
contributions can thus be viewed as taking this idea one step further, by 1) giving control of the
“main loop” to this construct (through the addition of the apply method), 2) expanding this idea
into the realm of Reinforcement Learning by moving from PyTorch’s DataLoaders to a higher-
level abstraction (Environments), and 3) organizing these modules into an inheritance hierarchy."
REFERENCES,0.780281690140845,"Given how all current Sequoia Settings are instances of PL’s LightningDataModule class, it is
easy to use pytorch lightning for the training of Methods in Sequoia. This is one of the reasons
why, for instance, the BaseMethod uses Pytorch Lightning’s Trainer class in its implementation.
However, the trainer-based API is not directly usable, due to the very nature of CL problems, in
which there are training dataloaders for each task, which isn’t currently possible through the standard
Pytorch Lightning’s API."
REFERENCES,0.7830985915492957,Under review as a conference paper at ICLR 2022
REFERENCES,0.7859154929577464,"from typing import List, Dict
from sequoia.settings.rl import make_continuous_task
from gym.envs.classic_control import CartPoleEnv
import numpy as np"
REFERENCES,0.7887323943661971,"# A Continuous task is a dict mapping from attributes to the values
# to be set on the environment:
ContinuousTask = Dict[str, float]"
REFERENCES,0.7915492957746478,"@make_continuous_task.register(CartPoleEnv)
def make_task_for_my_env(
env: CartPoleEnv, step: int, change_steps: List[int], seed: int = None, **kwargs,
) -> ContinuousTask:"
REFERENCES,0.7943661971830986,"# NOTE: task sampling should be reproducible given a `seed`.
step_seed = seed * step if seed is not None else None
rng = np.random.default_rng(step_seed)
return {"
REFERENCES,0.7971830985915493,"""gravity"": 9.8 * rng.normal(1, 0.5),
""masscart"": 1.0 * rng.normal(1, 0.5),
""masspole"": 0.1 * rng.normal(1, 0.5),
""length"": 0.5 * rng.normal(1, 0.5),
""force_mag"": 10.0 * rng.normal(1, 0.5),
""tau"": 0.02 * rng.normal(1, 0.5),
}"
REFERENCES,0.8,"Listing 3: Example of how to add new RL environments to Sequoia. In this example, we register a
function which will be used to sample continuous tasks for this environment, allowing it to become
used as part of the Continuous Task-Agnostic Continual RL Setting and all of its descendants."
REFERENCES,0.8028169014084507,"import operator
from typing import List, Dict, Union, Callable
import numpy as np
import gym
from metaworld.envs.mujoco.sawyer_xyz.v2 import SawyerReachEnvV2
from metaworld import ML10
from sequoia.settings.rl.discrete import make_discrete_task"
REFERENCES,0.8056338028169014,"# In the case of Discrete RL settings, you can either return a
# 'continuous' task as before or a callable which will be
# applied onto the environment when a task boundary
# is reached:
ContinuousTask = Dict[str, float]
IncrementalTask = Union[ContinuousTask, Callable[[gym.Env], None]]"
REFERENCES,0.8084507042253521,"@make_discrete_task.register(SawyerReachEnvV2)
def make_discrete_task_for_metaworld_env(
env: SawyerReachEnvV2,
step: int,
change_steps: List[int],
seed: int = None,
**kwargs,
) -> IncrementalTask:
benchmark = ML10(seed=seed)
rng = np.random.default_rng(seed)
some_metaworld_task = rng.choice(benchmark.train_tasks)
# NOTE: Equivalent to the following, but has the benefit of
# being pickleable for vectorized envs:
# return lambda env: env.set_task(some_metaworld_task)
return operator.methodcaller(""set_task"", some_metaworld_task)"
REFERENCES,0.8112676056338028,"Listing 4: Example of how to add support for new RL environments to Sequoia in the case of discrete
tasks, which are applied when a task boundary is reached. In this example, we register a function
which will be used to sample discrete tasks for this environment, allowing it to become used as part
of the Discrete Task-Agnostic RL setting and its descendants."
REFERENCES,0.8140845070422535,"D
ADDING NEW ENVIRONMENTS"
REFERENCES,0.8169014084507042,"New environments can be added to Sequoia by registering a new handler for creating new tasks, as
can be seen in Listing 3 for continuous tasks, and in Listing 4 for discrete tasks."
REFERENCES,0.819718309859155,Under review as a conference paper at ICLR 2022
REFERENCES,0.8225352112676056,"Figure 6: UML Diagram of the CL assumptions hierarchy. The CRL and CSL branches are not
shown, but follow an identical structure."
REFERENCES,0.8253521126760563,Under review as a conference paper at ICLR 2022
REFERENCES,0.828169014084507,"E
BENCHMARK DETAILS"
REFERENCES,0.8309859154929577,"E.1
SPLIT-SYNBOLS DATASET"
REFERENCES,0.8338028169014085,"Currently employed datasets can’t be used sensibly to construct domain-incremental learning prob-
lems. Some have used MNIST to construct Permuted-MNIST and Rotated-MNIST, however, Far-
quhar & Gal (2018) have explained and demonstrated why such benchmarks are ﬂawed and bias
their results unfairly towards some methods. Motivated by this, we introduce Split-Synbols. Based
on the Synbols dataset (Lacoste et al., 2020), a character classiﬁcation dataset in which examples
have an extra label corresponding to their font, one can easily construct sensible domain-incremental
benchmark where e.g., a font would consist of a domain."
REFERENCES,0.8366197183098592,"For the experiments, however, we opted for a class-incremental version to increase the difﬁculty.
We prescribe a segmentation into 12 tasks to be learned sequentially, each consisting of a 4-way
classiﬁcation problem. Some example of Synbols character are displayed in Figure 7."
REFERENCES,0.8394366197183099,Figure 7: Split-Synbols. Example of Synbols character to classify.
REFERENCES,0.8422535211267606,"E.2
CONTINUAL-MONSTERKONG ENVIRONMENT"
REFERENCES,0.8450704225352113,"With rapid advancements in the ﬁeld of deep RL, continual RL or never-ending-RL has witnessed
rekindled interest towards the goal for broad-AI in recent years. While signiﬁcant progress has been
made in related domains such as transfer learning Taylor & Stone (2009), multi-task learning Ammar
et al. (2014); Parisotto et al. (2016); Calandriello et al. (2014); Maurer et al. (2016); Barreto et al.
(2019); Landolﬁet al. (2019), and generalization in RL Farebrother et al. (2018), an outstanding
bottleneck is the lack of standard tools to develop and evaluate CRL agents Khetarpal et al. (2020). A
standardized benchmark will potentially enable rapid research and development of CRL agents. To
this end, we propose a new CRL benchmark within the uniﬁed framework of Sequoia. In particular,
we build the CRL benchmark leveraging the Pygame learning environment MonsterKong Tasﬁ
(2016). MonsterKong is pixel-based, lightweight and has an easily-customizable domain, making it
a good choice for evaluating continual learning agents."
REFERENCES,0.847887323943662,"Speciﬁcally, we design tasks through a variety of map conﬁgurations. These conﬁgurations vary
in terms of the location of the goal and the location of coins within each level. We introduce ran-
domness across runs of a task by varying the start locations of the agent. To incorporate the ability
to evaluate across speciﬁc CRL characteristics, we leverage tasks to deﬁne CRL experiments. We
design families of tasks leveraging the following abstract concepts: jumping tasks which require the
agent to perform jumps across platforms of different lengths in order to collect coins and reach the
goal, climbing tasks which require the agent to competently navigate ladders in order to collect coins
and reach the goal, and tasks that combine both of these skills. The speciﬁc tasks leveraged as part
of the CRL competition are depicted in Figure 8. The agent trains on each task for 200,000 steps."
REFERENCES,0.8507042253521127,"Experiment Details: To evaluate the agents on the CRL benchmark, we follow the standard eval-
uation introduced above. Final performance reports accumulated reward per episode on all test
environments, averaged over all tasks, after the end of training, whereas online performance is mea-
sured as the accumulated reward per episode on the training environment of the current task during
training of all tasks. For the runtime score, we use set max runtime of 12 hours and min runtime to
1.5 hours. Lastly, the agents are allowed a maximum of 200,000 steps per task."
REFERENCES,0.8535211267605634,"Customization: Ideally, CRL agents must be able to solve tasks by acquiring knowledge in the form
of skills, be able to use previously acquired behaviors, and build even more complex behaviours over
the course of its lifetime Ring (1997); Thrun & Mitchell (1995); Thrun & Schwartz (1995). While
leveraging the MonsterKong environment, it is easy to introduce new environment layouts or modi-
ﬁcations to existing layouts. Conﬁgurations could be customized to include arbitrary conﬁgurations"
REFERENCES,0.856338028169014,Under review as a conference paper at ICLR 2022
REFERENCES,0.8591549295774648,"Figure 8: Continual-MonsterKong. We display the 8 tasks that constitute the benchmark in chrono-
logical order. The ﬁrst two tasks test the agent’s ability to jump between platforms, the second two
test its ability to climb ladders and the last four combine both skills."
REFERENCES,0.8619718309859155,"of coins, ladders, platforms, walls, monsters, ﬁre balls, and spikes. Making custom environment el-
ements is straightforward as well, so the environment can be modiﬁed to aligned with the properties
of the CRL agent that we would like to test."
REFERENCES,0.8647887323943662,"While in our benchmark we mainly focused on three families of tasks within the Monsterkong
domain, it is fairly straightforward to introduce variations of map conﬁgurations to the framework.
Monsterkong provides two degrees of design choices 1. the task deﬁnitions and 2. the evolution
of tasks referred to as experiment deﬁnitions. Due to the nature of how tasks are speciﬁed through
simple matrices (map conﬁgurations), many layers of complexity can be added through the task
speciﬁcation. For example, object addition and removal can induce local variations in reward, nails
can be penalizing, diamonds can be bonuses. Additionally, changes to the textures of the game like
simple changes to the color of the walls, the coins, and the background as well as changes in the
lighting are easy to add for users interested to test generalization of the policies learned."
REFERENCES,0.8676056338028169,"E.3
HALFCHEETAH-GRAVITY AND HOPPER-BODYPARTS"
REFERENCES,0.8704225352112676,"HalfCheetah-gravity and Hopper-Bodyparts are two benchmarks introduced in Mendez et al. (2020).
In the ﬁrst, each task consist of a different gravity. In the latter, the agent’s body parts are changing
in size at each tasks. The gravity and body parts values are sampled as in Mendez et al. (2020). The
two benchmarks we study are each composed of 10 tasks."
REFERENCES,0.8732394366197183,Under review as a conference paper at ICLR 2022
REFERENCES,0.8760563380281691,"F
EXTENDED EXPERIMENTS"
REFERENCES,0.8788732394366198,"10
2
10
3
10
4"
REFERENCES,0.8816901408450705,Runtime (seconds) 0.0 0.5 1.0
REFERENCES,0.8845070422535212,Final Performance
REFERENCES,0.8873239436619719,Class-Incremental MNIST
REFERENCES,0.8901408450704226,"10
2
10
3
10
4"
REFERENCES,0.8929577464788733,Runtime (seconds) 0.0 0.5 1.0
REFERENCES,0.895774647887324,Final Performance
REFERENCES,0.8985915492957747,"Task-Incremental MNIST
Avalanche.GDumb
BaseMethod.EWC
PNN
HAT
BaseMethod
Avalanche.GEM
Avalanche.EWC
Avalanche.SI
Avalanche.Replay"
REFERENCES,0.9014084507042254,"10
2
10
3
10
4
10
5"
REFERENCES,0.9042253521126761,Runtime (seconds) 0.0 0.5 1.0
REFERENCES,0.9070422535211268,Final Performance
REFERENCES,0.9098591549295775,Class-Incremental CIFAR10
REFERENCES,0.9126760563380282,"10
1
10
2
10
3
10
4"
REFERENCES,0.9154929577464789,Runtime (seconds) 0.0 0.5 1.0
REFERENCES,0.9183098591549296,Final Performance
REFERENCES,0.9211267605633803,"Task-Incremental CIFAR10
Avalanche.GDumb
BaseMethod.EWC
PNN
HAT
BaseMethod
Avalanche.GEM
Avalanche.EWC
Avalanche.SI
Avalanche.Replay"
REFERENCES,0.923943661971831,"10
2
10
3
10
4
10
5"
REFERENCES,0.9267605633802817,Runtime (seconds) 0.0 0.5 1.0
REFERENCES,0.9295774647887324,Final Performance
REFERENCES,0.9323943661971831,Class-Incremental Synbols
REFERENCES,0.9352112676056338,"10
2
10
3
10
4
10
5"
REFERENCES,0.9380281690140845,Runtime (seconds) 0.0 0.5 1.0
REFERENCES,0.9408450704225352,Final Performance
REFERENCES,0.9436619718309859,"Task-Incremental Synbols
Avalanche.GDumb
BaseMethod.EWC
PNN
HAT
BaseMethod
Avalanche.GEM
Avalanche.EWC
Avalanche.SI
Avalanche.Replay"
REFERENCES,0.9464788732394366,"Figure 9: Incremental Supervised Learning results. Transpose of Figure 3 for improved read-
ability. Final performance (vertical axis) is plotted against runtime (horizontal axis). The methods
achieving the best trade-off lie closer to the top-left of the ﬁgures. The dotted line shows chance
accuracy for each setting-dataset combination. For each methods, several trials are presented de-
pending on metrics composed of linear combination of ﬁnal performance and (normalized) runtime.
Intuitively, better performance in CL normally comes at the cost of increased computation. This
intuition is reﬂected in the presented results, as highlighted by the observed correlation between ﬁ-
nal performance and runtime. GEM and GDumb achieve the best tradoff, although the latter cannot
make predictions in an online manner and thus serves more as a reference point."
REFERENCES,0.9492957746478873,Under review as a conference paper at ICLR 2022
REFERENCES,0.952112676056338,"0
2500
5000
7500
Online Performance 0 5000 10000"
REFERENCES,0.9549295774647887,Final Performance
REFERENCES,0.9577464788732394,Traditional HalfCheetah-gravity
REFERENCES,0.9605633802816902,"0
2000
4000
Online Performance 0 2500 5000"
REFERENCES,0.9633802816901409,Final Performance
REFERENCES,0.9661971830985916,Incremental HalfCheetah-gravity
REFERENCES,0.9690140845070423,"SB3.DDPG
SB3.TD3
SB3.A2C
SB3.PPO
SB3.SAC"
REFERENCES,0.971830985915493,"18
20
22
24
Online Performance 20 30 40"
REFERENCES,0.9746478873239437,Final Performance
REFERENCES,0.9774647887323944,Traditional MonsterKong
REFERENCES,0.9802816901408451,"20
40
60
80
Online Performance 0 20 40 60"
REFERENCES,0.9830985915492958,Final Performance
REFERENCES,0.9859154929577465,Incremental MonsterKong
REFERENCES,0.9887323943661972,"SB3.PPO
SB3.A2C
SB3.DQN"
REFERENCES,0.9915492957746479,"Figure 10: Impact of the RL backbone algorithm in Traditional and Incremental RL. Larger
version of Figure 4 for improved readability. Final performance (vertical axis) is plotted against
online performance (horizontal axis). The bubbles’ size indicates the normalized runtime of the
methods. Datasets are presented in each row and settings are presented in each column. For each
method, several trials are presented depending on metrics composed of linear combination of ﬁnal
performance and online performance. The methods achieving the best trade-off lie closer to the
top-right of the ﬁgures and have smaller bubble size. In general, we observe a trade-off between
performance and runtime, a tendency also observed in Figure 9. Another interesting trade-off can
be observed between ﬁnal performance and online performance. E.g., in both MonsterKong bench-
marks, DQN achieves the best ﬁnal performance whereas PPO achieves the best online performance.
Because the former is off-policy, it can re-use the previously acquired data to retain its performance
on past tasks, increasing ﬁnal performance. Contrarily, the latter, being on-policy, focuses on the
current task and thus learns it faster, thereby increasing its online performance."
REFERENCES,0.9943661971830986,Under review as a conference paper at ICLR 2022
REFERENCES,0.9971830985915493,"Figure 11: PPO’s Transfer matrix in Continual-MonsterKong. Each cell at row i and column
j indicates the test performance on task j after having learned tasks 0 through i. The contents of
each cell correspond to the average reward per episode obtained in the test environment for the
corresponding task. Positive numbers above the diagonal indicate generalization to unseen tasks,
which is achievable by design in the Continual-Monsterkong benchmark."
