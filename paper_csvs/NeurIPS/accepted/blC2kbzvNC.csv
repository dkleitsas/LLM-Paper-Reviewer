Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010245901639344263,"The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-
search (SLS) for SGD have shown remarkable effectiveness when training over-
parameterized models. However, two issues remain unsolved in this line of work.
First, in non-interpolation settings, both algorithms only guarantee convergence to
a neighborhood of a solution which may result in a worse output than the initial
guess. While artificially decreasing the adaptive stepsize has been proposed to
address this issue (Orvieto et al. [44]), this approach results in slower convergence
rates under interpolation. Second, intuitive line-search methods equipped with
variance-reduction (VR) fail to converge (Dubois-Taine et al. [14]). So far, no VR
methods successfully accelerate these two stepsizes with a convergence guarantee.
In this work, we make two contributions: Firstly, we propose two new robust
variants of SPS and SLS, called AdaSPS and AdaSLS, which achieve optimal
asymptotic rates in both strongly-convex or convex and interpolation or non-
interpolation settings, except for the case when we have both strong convexity
and non-interpolation. AdaSLS requires no knowledge of problem-dependent
parameters, and AdaSPS requires only a lower bound of the optimal function value
as input. Secondly, we propose a novel VR method that can use Polyak stepsizes or
line-search to achieve acceleration. When it is equipped with AdaSPS or AdaSLS,
the resulting algorithms obtain the optimal rate for optimizing convex smooth
functions. Finally, numerical experiments on synthetic and real datasets validate
our theory and demonstrate the effectiveness and robustness of our algorithms."
INTRODUCTION,0.0020491803278688526,"1
Introduction"
INTRODUCTION,0.0030737704918032786,"Stochastic Gradient Descent (SGD) [46] and its variants [7] are among the most preferred algorithms
for training modern machine learning models. These methods only compute stochastic gradients
in each iteration, which is often more efficient than computing a full batch gradient. However, the
performance of SGD is highly sensitive to the choice of the stepsize. Common strategies use a
fixed stepsize schedule, such as keeping it constant or decreasing it over time. Unfortunately, the
theoretically optimal schedules are disparate across different function classes [8], and usually depend
on problem parameters that are often unavailable, such as the Lipschitz constant of the gradient. As a
result, a heavy tuning of the stepsize parameter is required, which is typically expensive in practice."
INTRODUCTION,0.004098360655737705,"Instead of fixing the stepsize schedule, adaptive SGD methods adjust the stepsize on the fly [15, 26].
These algorithms often require less hyper-parameter tuning and still enjoy competitive performance in
practice. Stochastic Polyak Stepsize (SPS) [34, 6, 45] is one of such recent advances. It has received
rapid growing interest due to two factors: (i) the only required parameter is the individual optimal
function value which is often available in many machine learning applications and (ii) its adaptivity"
INTRODUCTION,0.005122950819672131,"∗CISPA Helmholtz Center for Information Security, Saarbrücken, Germany"
INTRODUCTION,0.006147540983606557,"utilizes the local curvature and smoothness information allowing the algorithm to accelerate and
converge quickly when training over-parametrized models. Stochastic Line-Search (SLS) [52] is
another adaptive stepsize that offers exceptional performance when the interpolation condition holds.
In contrast to SPS, the knowledge of the optimal function value is not required for SLS, at the cost of
additional function value evaluations per iteration."
INTRODUCTION,0.007172131147540984,"An ideal adaptive stepsize should not only require fewer hyper-parameters but should also enjoy robust
convergence, in the sense that they can automatically adapt to the optimization setting (interpolation
vs. non-interpolation). This will bring great convenience to the users in practice as they no longer
need to choose which method to use (or runinning both of them at double the cost). Indeed, in many
real-world scenarios, it can be challenging to ascertain whether a model is effectively interpolating
the data or not [11]. For instance, the feature dimension of the rcv1 dataset [10] is twice larger
than the number of data points. A logistic regression model with as many parameters as the feature
dimension may tend to overfit the data points. But the features are actually sparse and the model
is not interpolated. Another example is federated learning [23] where millions of clients jointly
train a machine learning model on their mobile devices, which usually cannot support huge-scale
models. Due to the fact that each client’s data is stored locally, it becomes impractical to check the
interpolation condition."
INTRODUCTION,0.00819672131147541,"While SPS and SLS are promising adaptive methods, they are not robust since both methods cannot
converge to the solution when interpolation does not hold. Orvieto et al. [44] address this issue for
SPS by applying an artificially decreasing rule and the resulting algorithm DecSPS is able to converge
as quickly as SGD with the optimal stepsize schedule. However, the convergence rates of DecSPS in
interpolation regimes are much slower than SPS. For SLS, no solution has been proposed."
INTRODUCTION,0.009221311475409836,"If the user is certain that the underlying problem is non-interpolated, then applying variance-
reduction (VR) techniques can further accelerate the convergence [22, 40, 13, 27, 49, 33]. While
gradient descent with Polayak stepsize and line-search perform well in the deterministic settings, there
exists no method that successfully adapt these stepsizes in VR methods. Mairal [36] and Schmidt
et al. [49] proposed to use stochastic line-search with VR. However, no theoretical guarantee is shown.
Indeed, this is a challenging open question as Dubois-Taine et al. [14] provides a counter-example
where classical line-search methods fail in the VR setting. As such, it remains unclear whether we
can accelerate SGD with stepsizes from Polyak and line-search family in non-interpolated settings."
MAIN CONTRIBUTIONS,0.010245901639344262,"1.1
Main contributions"
MAIN CONTRIBUTIONS,0.011270491803278689,"In this work, we provide solutions to the aforementioned two challenges and contribute new theoretical
insights on Polyak stepsize and line-search methods. We summarize our main contributions as follows:"
MAIN CONTRIBUTIONS,0.012295081967213115,"• In Section 3, we propose the first robust adaptive methods that simultaneously achieve the best-
known asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation
settings except for the case when we have strongly-convexity and non-interpolation. The first
method called AdaSPS, a variant of SPS, requires only a lower bound of the optimal function value
as input (similar to DecSPS) while AdaSLS, the second method based on SLS, is parameter-free.
In the non-interpolated setting, we prove for both algorithms an O(1/ε2) convergence rate for
convex functions which matches the classical DecSPS and AdaGrad [15] results, whereas SPS and
SLS cannot converge in this case. In the interpolated regime, we establish fast O(log(1/ε)) and
O(1/ε) rates under strong convexity and convexity conditions respectively, without knowledge of
any problem-dependent parameters. In contrast, DecSPS converges at the slower O(1/ε2) rate and
for AdaGrad, the Lipschitz constant is needed to set its stepsize [56]."
MAIN CONTRIBUTIONS,0.01331967213114754,"• In Section 4, we design a new variance-reduction method that is applicaple to both Polyak stepsizes
or line-search methods. We prove that to reach an ε-accuracy, the total number of gradient
evaluations required in expectation is e
O(n + 1/ε) for convex functions which matches the rate of
AdaSVRG [14]. With our newly proposed decreasing probability strategy, the artificially designed
multi-stage inner-outer-loop structure is not needed, which makes our methods easier to analyze."
MAIN CONTRIBUTIONS,0.014344262295081968,"Our novel VR-framework is based on proxy function sequences and can recover the standard VR
methods [22] as a special case. We believe that this technique can be of independent interest to the
optimization community and may motivate more personalized VR techniques in the future."
MAIN CONTRIBUTIONS,0.015368852459016393,"Stepsize
Interpolation
Non-interpolation"
MAIN CONTRIBUTIONS,0.01639344262295082,"strongly-convex
convex
required input
strongly-convex
convexa
required input"
MAIN CONTRIBUTIONS,0.017418032786885244,"SPS/SPSmax [34]
O(log( 1"
MAIN CONTRIBUTIONS,0.018442622950819672,"ε))
O( 1"
MAIN CONTRIBUTIONS,0.0194672131147541,"ε)
f ⋆
it
ε ≥Ω(σ2
f,B)
ε ≥Ω(σ2
f,B)
f ⋆
it
SLS [52]
O(log( 1"
MAIN CONTRIBUTIONS,0.020491803278688523,"ε))
O( 1"
MAIN CONTRIBUTIONS,0.02151639344262295,"ε)
None
ε ≥Ω(σ2
f,B)
ε ≥Ω(σ2
f,B)
None"
MAIN CONTRIBUTIONS,0.022540983606557378,"DecSPS [44]
O( 1"
MAIN CONTRIBUTIONS,0.0235655737704918,"ε2 )
O( 1"
MAIN CONTRIBUTIONS,0.02459016393442623,"ε2 )
ℓ⋆
it
O( 1"
MAIN CONTRIBUTIONS,0.025614754098360656,"ε2 )
O( 1"
MAIN CONTRIBUTIONS,0.02663934426229508,"ε2 )
ℓ⋆
it"
MAIN CONTRIBUTIONS,0.027663934426229508,"AdaSPS (this work)
O(log( 1"
MAIN CONTRIBUTIONS,0.028688524590163935,"ε))
O( 1"
MAIN CONTRIBUTIONS,0.02971311475409836,"ε)
f ⋆
it
O( 1"
MAIN CONTRIBUTIONS,0.030737704918032786,"ε2 )
O( 1"
MAIN CONTRIBUTIONS,0.031762295081967214,"ε2 )
ℓ⋆
it
AdaSLS (this work)
O(log( 1"
MAIN CONTRIBUTIONS,0.03278688524590164,"ε))
O( 1"
MAIN CONTRIBUTIONS,0.03381147540983607,"ε)
None
O( 1"
MAIN CONTRIBUTIONS,0.03483606557377049,"ε2 )
O( 1"
MAIN CONTRIBUTIONS,0.035860655737704916,"ε2 )
None"
MAIN CONTRIBUTIONS,0.036885245901639344,aThe assumption of bounded iterates is also required except for SPS and SLS.
MAIN CONTRIBUTIONS,0.03790983606557377,"Table 1: Summary of convergence behaviors of the considered adaptive stepsizes for smooth functions. For
SPS/SPSmax and SLS in non-interpolation settings, Ω(·) indicates the size of the neighborhood that they can
converge to. In the other cases, the O(·) complexity provides the total number of gradient evaluations required
for each algorithm to reach an O(ε) suboptimality. For convex functions, the suboptimality is defined as
E[f(¯xT ) −f ⋆] and for strongly convex functions, the suboptimality is defined as E[||xT −x⋆||2]."
MAIN CONTRIBUTIONS,0.0389344262295082,"0
1000
2000
# mini-batch gradient evaluations 10
8 10
5 10
2 101 104 107"
MAIN CONTRIBUTIONS,0.039959016393442626,f(x)-f
MAIN CONTRIBUTIONS,0.040983606557377046,strongly-convex+interpolation
MAIN CONTRIBUTIONS,0.042008196721311473,"0
2000
4000
# mini-batch gradient evaluations 102 104 106"
MAIN CONTRIBUTIONS,0.0430327868852459,f(x)-f
MAIN CONTRIBUTIONS,0.04405737704918033,strongly-convex+non-interpolation
MAIN CONTRIBUTIONS,0.045081967213114756,"0
2500
5000
7500 10000
# mini-batch gradient evaluations 100 102 104 106"
MAIN CONTRIBUTIONS,0.04610655737704918,f(x)-f
MAIN CONTRIBUTIONS,0.0471311475409836,convex+interpolation
MAIN CONTRIBUTIONS,0.04815573770491803,"0
10000
20000
# mini-batch gradient evaluations 101 102 103 104 105 106"
MAIN CONTRIBUTIONS,0.04918032786885246,f(x)-f
MAIN CONTRIBUTIONS,0.050204918032786885,convex+non-interpolation
MAIN CONTRIBUTIONS,0.05122950819672131,"DecSPS
SLS
AdaSPS
AdaSLS
SPS"
MAIN CONTRIBUTIONS,0.05225409836065574,"Figure 1: Illustration of the robust convergence of AdaSPS and AdaSLS on synthetic data with quadratic
loss. SPS and SLS have superior performance on the two interpolated problems but cannot converge when the
interpolation condition does not hold. DecSPS suffers from a slow convergence on both interpolated problems.
(Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)"
RELATED WORK,0.05327868852459016,"1.2
Related work"
RELATED WORK,0.05430327868852459,"Line-search procedures has been successfully applied to accelerate large-scale machine learning
training. Following [52], Galli et al. [16] propose to relax the condition of monotone decrease of
objective function for training over-parameterized models. Kunstner et al. [30] extends backtracking
line-search to a multidimensional variant which provides better diagonal preconditioners. In recent
years, adaptive stepsizes from the AdaGrad family have become widespread and are particularly
successful when training deep neural networks. Plenty of contributions have been made to analyze
variants of AdaGrad for different classes of functions [15, 51, 43, 55, 56], among which Vaswani
et al. [53] first propose to use line-search to set the stepsize for AdaGrad to enhance its practical
performance. More recently, variance reduction has successfully been applied to AdaGrad stepsize
and faster convergence rates have been established for convex and non-convex functions [14, 25]."
RELATED WORK,0.055327868852459015,"Another promising direction is the Polyak stepsize (PS) [45] originally designed as a subgradient
method for solving non-smooth convex problems. Hazan and Kakade [20] show that PS indeed
gives simultaneously the optimal convergence result for a more general class of convex functions.
Nedi´c and Bertsekas [38] propose several variants of PS as incremental subgradient methods and
they also discuss the method of dynamic estimation of the optimal function value when it is not
known. Recently, more effort has been put into extending deterministic PS to the stochastic setting
[47, 6, 42]. However, theoretical guarantees of the algorithms still remain elusive until the emergence
of SPS/SPSmax [34]. Subsequently, further improvements and new variants such as DecSPS [44]
and SPS with a moving target [17] have been introduced. A more recent line of work interprets
stochastic Polyak stepsize as a subsampled Newton Raphson method and interesting algorithms
have been designed based on the first-order local expansion [17, 18] as well as the second-order
expansion [31]. Wang et al. [54] propose to set the stepsize for SGD with momentum using Polyak
stepsize. Abdukhakimov et al. [1] employ more general preconditioning techniques to SPS."
RELATED WORK,0.05635245901639344,"There has been a recent line of work attempting to develop methods that can adapt to both the
interpolation setting and the general growth condition beyond strong convexity. Using the iterative
halving technique from [5], Cheng and Duchi [11] propose AdaStar-G which gives the desired
property if the Lipschitz constant and the diameter of the parameter domain are known. How to
remove these requirements is an interesting open question for the future research."
PROBLEM SETUP AND BACKGROUND,0.05737704918032787,"2
Problem setup and background"
NOTATIONS,0.0584016393442623,"2.1
Notations"
NOTATIONS,0.05942622950819672,"In this work, we consider solving the finite-sum smooth convex optimization problem:"
NOTATIONS,0.060450819672131145,"min
x∈Rd """
NOTATIONS,0.06147540983606557,"f(x) = 1 n n
X"
NOTATIONS,0.0625,"i=1
fi(x) # .
(1)"
NOTATIONS,0.06352459016393443,"This type of problem appears frequently in the modern machine learning applications [19], where each
fi(x) represents the loss of a model on the i-th data point parametrized by the parameter x. Stochastic
Gradient Descent (SGD) [46] is one of the most popular methods for solving the problem (1). At
each iteration, SGD takes the form:
xt+1 = xt −ηt∇fit(xt) ,
(2)
where ηt is the stepsize parameter, it ⊆[n] is a random set of size B sampled independently at each
iteration t and ∇fit(x) = 1 B
P"
NOTATIONS,0.06454918032786885,i∈it ∇fi(x) is the minibatch gradient.
NOTATIONS,0.06557377049180328,"Throughout the paper, we assume that there exists a non-empty set of optimal points X ⋆⊂Rd, and
we use f ⋆to denote the optimal value of f at a point x⋆∈X ⋆. We use f ⋆
it to denote the infimum
of minibatch function fit(x), i.e. f ⋆
it = infx∈Rd 1 B
P"
NOTATIONS,0.06659836065573771,"i∈it fi(x). We assume that all the individual
functions {fi(x)} are L-smooth. Finally, we denote the optimal objective difference, first introduced
in [34], by σ2
f,B = f ⋆−Eit[f ⋆
it]. The definitions for the interpolation condition can be defined
and studied in various ways [48, 9, 4, 11]. Here, we adopt the notion from [34]. The problem (1)
is said to be interpolated if σ2
f,1 = 0, which implies that σ2
f,B = 0 for all B ≤n since σ2
f,B is
non-increasing w.r.t B. Note interpolation implies that the global minimizer of f is also a minimizer
of each individual function fi."
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.06762295081967214,"2.2
SGD with stochastic polyak stepsize"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.06864754098360656,"Loizou et al. [34] propose to set the stepsize ηt as: ηt = 2
fit(xt)−f ⋆
it
||∇fit(xt)||2 , which is well known as the
Stochastic Polyak stepsize (SPS). In addition to SPS, they also propose a bounded variant SPSmax
which has the form ηt = min

2
fit(xt)−f ⋆
it
||∇fit(xt)||2 , γb
	
where γb > 0. Both algorithms require the input
of the exact f ⋆
it which is often unavailable when the batch size B > 1 or when the interpolation
condition does not hold. Orvieto et al. [44] removes the requirement for f ⋆
it and propose to set ηt as:"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.06967213114754098,"ηt =
1
√t+1 min
n fit(xt)−ℓ⋆
it
||∇fit(xt)||2 ,
√"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.0706967213114754,"tηt−1
o
for t ≥1 (DecSPS), where η0 > 0 is a constant and ℓ⋆
it is an
input lower bound such that ℓ⋆
it ≤f ⋆
it. In contrast to the exact optimal function value, a lower bound
ℓ⋆
it is often available in practice, in particular for machine learning problems when the individual loss
functions are non-negative. We henceforth denote the estimation error by:
err2
f,B := Eit[f ⋆
it −ℓ⋆
it] .
(3)"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.07172131147540983,"For convex smooth functions, SPS achieves a fast convergence up to a neighborhood of size Ω(σ2
f,B)
and its variant SPSmax converges up to Ω(σ2
f,Bγb/α) where α = min{ 1"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.07274590163934426,"L, γb}. Note that the size
of the neighborhood cannot be further reduced by choosing an appropriate γb. In contrast, DecSPS
converges at the rate of O(1/
√"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.07377049180327869,"T) which matches the standard result for SGD with decreasing stepsize.
However, the strictly decreasing Θ(1/
√"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.07479508196721311,"t) stepsize schedule hurts its performance in interpolated
settings. For example, DecSPS has a much slower O(1/
√"
SGD WITH STOCHASTIC POLYAK STEPSIZE,0.07581967213114754,"T) convergence rate compared with the
fast O(exp(−Tµ/L)) rate of SPS when optimizing strongly-convex objectives. Therefore, both
algorithms do not have the robust convergence property (achieving fast convergence guarantees in
both interpolated and non-interpolated regimes) and we aim to fill this gap. See Figure 1 for a detailed
illustration of the non-robustness of SPS and DecSPS."
ADAPTIVE SGD WITH POLYAK STEPSIZE AND LINE-SEARCH,0.07684426229508197,"3
Adaptive SGD with polyak stepsize and line-search"
ADAPTIVE SGD WITH POLYAK STEPSIZE AND LINE-SEARCH,0.0778688524590164,"In this section, we introduce and analyze two adaptive algorithms to solve problem (1)."
PROPOSED METHODS,0.07889344262295082,"3.1
Proposed methods"
PROPOSED METHODS,0.07991803278688525,AdaSPS. Our first stepsize is defined as the following:
PROPOSED METHODS,0.08094262295081968,"ηt = min 
"
PROPOSED METHODS,0.08196721311475409,"
fit(xt) −ℓ⋆
it
cp||∇fit(xt)||2
1
qPt
s=0 fis(xs) −ℓ⋆
is"
PROPOSED METHODS,0.08299180327868852,", ηt−1 
"
PROPOSED METHODS,0.08401639344262295,",
with η−1 = +∞,
(AdaSPS)"
PROPOSED METHODS,0.08504098360655737,"where ℓ⋆
it is an input parameter that must satisfy ℓ⋆
it ≤f ⋆
it and cp > 0 is an input constant to adjust
the magnitude of the stepsize (we discuss suggested choices in Section 5)."
PROPOSED METHODS,0.0860655737704918,"AdaSPS can be seen as an extension of DecSPS. However, unlike the strict Θ(1/
√"
PROPOSED METHODS,0.08709016393442623,"t) decreasing
rule applied in DecSPS, AdaSPS accumulates the function value difference during the optimization
process which enables it to dynamically adapt to the underlying unknown interpolation settings."
PROPOSED METHODS,0.08811475409836066,"AdaSLS. We provide another stepsize that can be applied even when a lower bound estimation is
unavailable. The method is based on line-search and thus is completely parameter-free, but requires
additional function value evaluations in each iteration:"
PROPOSED METHODS,0.08913934426229508,"ηt = min 
 
γt"
PROPOSED METHODS,0.09016393442622951,"cl
qPt
s=0 γs||∇fis(xs)||2
, ηt−1 
"
PROPOSED METHODS,0.09118852459016394,",
with η−1 = +∞,
(AdaSLS)"
PROPOSED METHODS,0.09221311475409837,"where cl > 0 is an input constant, and the scale γt is obtained via stardard Armijo backtracking
line-search (see Algorithm 4 for further implementation details in the Appendix D) such that the
following conditions are satisfied:"
PROPOSED METHODS,0.0932377049180328,"fit(xt −γt∇fit(xt)) ≤fit(xt) −ργt||∇fit(xt)||2
and
γt ≤γmax, 0 < ρ < 1 ,
(4)"
PROPOSED METHODS,0.0942622950819672,for line-search parameters γmax and ρ. By setting the decreasing factor β ≥1
PROPOSED METHODS,0.09528688524590163,"2 defined in Algorithm 4,
one can show that γt ≥min( 1−ρ"
PROPOSED METHODS,0.09631147540983606,"L , γmax). We give a formal proof in Lemma 16 in Appendix A.2."
PROPOSED METHODS,0.09733606557377049,"Discussion. Our adaptation mechanism in AdaSPS/AdaSLS is reminiscent of AdaGrad type methods,
in particular to AdaGrad-Norm, the scalar version of AdaGrad, that aggregates the gradient norm in
the denominator and takes the form ηt =
cg
√Pt
s=0 ||∇fis(xs)||2+b2
0 where cg > 0 and b2
0 ≥0."
PROPOSED METHODS,0.09836065573770492,"The primary distinction between AdaSPS and AdaSLS compared to AdaGrad-Norm is the inclusion
of an additional component that captures the curvature information at each step, and not using squared
gradient norms in AdaSPS. In contrast to the strict decreasing behavior of AdaGrad-Norm, AdaSPS
and AdaSLS can automatically mimic a constant stepsize when navigating a flatter region."
PROPOSED METHODS,0.09938524590163934,"Vaswani et al. [53] suggest using line-search to set the stepsize for AdaGrad-Norm which takes the
form ηt =
γt
√Pt
s=0 ||∇fis(xs)||2 where γt ≤γt−1 is required for solving non-interpolated convex"
PROPOSED METHODS,0.10040983606557377,"problems. While this stepsize is similar to AdaSLS, the scaling of the denominator gives a suboptimal
convergence rate as we demonstrate in the following section."
CONVERGENCE RATES,0.1014344262295082,"3.2
Convergence rates"
CONVERGENCE RATES,0.10245901639344263,"In this section, we present the convergence results for AdaSPS and AdaSLS. We list the helpful
lemmas in Appendix A. The proofs can be found in Appendix B."
CONVERGENCE RATES,0.10348360655737705,"General convex. We denote X to be a convex compact set with diameter D such that there exists a
solution x⋆∈X and supx,y∈X ||x −y||2 ≤D2. We let ΠX denote the Euclidean projection onto
X. For general convex stochastic optimization, it seems inevitable that adaptive methods require the
bounded iterates assumption or an additional projection step to prove convergence due to the lack
of knowledge of problem-dependent parameters [12, 15]. Here, we employ the latter solution by
running projected stochastic gradient descent (PSGD):"
CONVERGENCE RATES,0.10450819672131148,"xt+1 = ΠX (xt −ηt∇fit(xt)).
(5)"
CONVERGENCE RATES,0.10553278688524591,"Theorem 1 (General convex). Assume that f is convex, each fi is L-smooth and X is a convex
compact feasible set with diameter D, PSGD with AdaSPS or AdaSLS converges as:"
CONVERGENCE RATES,0.10655737704918032,"(AdaSPS) : E[f(¯xT ) −f ⋆] ≤τ 2
p
T +
τp
q"
CONVERGENCE RATES,0.10758196721311475,"σ2
f,B + err2
f,B
√ T
,"
CONVERGENCE RATES,0.10860655737704918,"(AdaSLS) : E[f(¯xT ) −f ⋆] ≤τ 2
l
T + τlσf,B
√ T
, (6)"
CONVERGENCE RATES,0.1096311475409836,where ¯xT = 1
CONVERGENCE RATES,0.11065573770491803,"T
PT −1
t=0 xt, τp = (2cpLD2 + 1"
CONVERGENCE RATES,0.11168032786885246,"cp ) and τl = max
n
L
(1−ρ)√ρ,
1
γmax√ρ
o
clD2 +
1
cl√ρ."
CONVERGENCE RATES,0.11270491803278689,"As a consequence of Theorem 1, if err2
f,B = σ2
f,B = 0, then PSGD with AdaSPS or AdaSLS"
CONVERGENCE RATES,0.11372950819672131,converges as O( 1
CONVERGENCE RATES,0.11475409836065574,"T ). Suppose γmax is sufficiently large, then picking c⋆
p =
1
√"
CONVERGENCE RATES,0.11577868852459017,"2LD2 and c⋆
l =
√1−ρ
√"
CONVERGENCE RATES,0.1168032786885246,"LD2
gives a O( LD2"
CONVERGENCE RATES,0.11782786885245902,"T ) rate under the interpolation condition, which is slightly worse than L||x0−x⋆||2"
CONVERGENCE RATES,0.11885245901639344,"T
obtained by SPS and SLS but is better than O( LD2 √"
CONVERGENCE RATES,0.11987704918032786,"T ) obtained by DecSPS. If otherwise σ2
f,B > 0,"
CONVERGENCE RATES,0.12090163934426229,"then AdaSPS, AdaSLS, and DecSPS converge as O(1/
√"
CONVERGENCE RATES,0.12192622950819672,"T) which matches the rate of Vanilla SGD
with decreasing stepsize. Finally, AdaGrad-Norm gives a similar rate in both cases while AdaGrad-
Norm with line-search [53] shows a suboptimal rate of O( L3D4"
CONVERGENCE RATES,0.12295081967213115,"T
+ D2L3/2σ
√"
CONVERGENCE RATES,0.12397540983606557,"T
). It is worth noting that
SPS, DecSPS and SLS require an additional assumption on individual convexity.
Theorem 2 (Individual convex+interpolation). Assume that f is convex, each fi is convex and"
CONVERGENCE RATES,0.125,"L-smooth, and that err2
f,B = σ2
f,B = 0, by setting cp =
cscale
p
√"
CONVERGENCE RATES,0.1260245901639344,"fi0(x0)−f ⋆
i0
and cl =
cscale
l
ρ√"
CONVERGENCE RATES,0.12704918032786885,"γ0||∇fi0(x0)||2
with constants cscale
p
≥1 and cscale
l
≥1, then for any T ≥1, SGD (no projection) with AdaSPS or
AdaSLS converges as:"
CONVERGENCE RATES,0.12807377049180327,"(AdaSPS)
E[f(¯xT ) −f ⋆] ≤ "
CONVERGENCE RATES,0.1290983606557377,"4L(cscale
p
)2 Ei0
h ||x0 −x⋆||2"
CONVERGENCE RATES,0.13012295081967212,fi0(x0) −f ⋆
CONVERGENCE RATES,0.13114754098360656,"i!
L||x0 −x⋆||2"
CONVERGENCE RATES,0.13217213114754098,"T
,
(7) and"
CONVERGENCE RATES,0.13319672131147542,"(AdaSLS)
E[f(¯xT ) −f ⋆] ≤"
CONVERGENCE RATES,0.13422131147540983,"(cscale
l
)2"
CONVERGENCE RATES,0.13524590163934427,ρ3L min2{ 1−ρ
CONVERGENCE RATES,0.1362704918032787,"L , γmax} Ei0
h
||x0 −x⋆||2"
CONVERGENCE RATES,0.13729508196721313,γ0||∇fi0(x0)||2
CONVERGENCE RATES,0.13831967213114754,"i!
L||x0 −x⋆||2 T
."
CONVERGENCE RATES,0.13934426229508196,"(8)
where ¯xT = 1"
CONVERGENCE RATES,0.1403688524590164,"T
PT
t=1 xt."
CONVERGENCE RATES,0.1413934426229508,"The result implies that the bounded iterates assumption is not needed if we have both individual
convexity and interpolation by picking cp and cl to satisfy certain conditions that do not depend on
unknown parameters. To our knowledge, no such result exists for stepsizes from the AdaGrad family.
It is worth noting that the min operator defined in AdaSPS or AdaSLS is not necessary in the proof.
Remark 3. We note that for non-interpolated problems, AdaSPS only requires the knowledge
of ℓ⋆
it while the exact f ⋆
it is needed under the interpolation condition. We argue that in many
standard machine learning problems, simply picking zero will suffice. For instance, f ⋆
it = 0 for
over-parameterized logistic regression and after adding a regularizer, ℓ⋆
it = 0."
CONVERGENCE RATES,0.14241803278688525,"Strongly convex. We now present two algorithmic behaviors of AdaSPS and AdaSLS for strongly
convex functions. In particular, We show that 1) the projection step can be removed as shown in
DecSPS, and 2) if the interpolation condition holds, the min operator is not needed and the asymptotic
linear convergence rate is preserved. The full statement of Lemma 4 can be found in Appendix B.2.
Lemma 4 (Bounded iterates). Let each fi be µ-strongly convex and L-smooth. For any t = 0, . . . , T,
the iterates of SGD with AdaSPS or AdaSLS satisfy: ||xt −x⋆||2 ≤Dmax, for a constant Dmax
specified in the appendix in Equation (B.16).
Corollary 5 (Individual strongly convex). Assume each fi is µ-strongly convex and L-smooth,
Theorem 1 holds with PSGD and D replaced by SGD and Dmax defined in Lemma 4."
CONVERGENCE RATES,0.14344262295081966,"Although it has not been formally demonstrated that AdaGrad/AdaGrad-Norm can relax the assump-
tion on bounded iterates for strongly convex functions, we believe that with a similar proof technique,
this property still holds for AdaGrad/AdaGrad-Norm."
CONVERGENCE RATES,0.1444672131147541,We next show that AdaSPS and AdaSLS achieve linear convergence under the interpolation condition.
CONVERGENCE RATES,0.14549180327868852,"Theorem 6 (Strongly convex + individual convex + interpolation). Consider SGD with AdaSPS
(AdaSPS) or AdaSLS (AdaSLS) stepsize. Suppose that each fi is convex and L-smooth, f is µ-"
CONVERGENCE RATES,0.14651639344262296,"strongly convex and that σ2
f,B = err2
f,B = 0. If we let cp =
cscale
p
√"
CONVERGENCE RATES,0.14754098360655737,"fi0(x0)−f ⋆
i0
and cl =
cscale
l
ρ√"
CONVERGENCE RATES,0.14856557377049182,"γ0||∇fi0(x0)||2
with constants cscale
p
≥1 and cscale
l
≥1, then AdaSPS or AdaSLS converges as:"
CONVERGENCE RATES,0.14959016393442623,"(AdaSPS)
E[||xT +1 −x⋆||2] ≤Ei0"
CONVERGENCE RATES,0.15061475409836064,"""
1 −
(fi0(x0) −f ⋆)µ
(2cscale
p
L||x0 −x⋆||)2 T
#"
CONVERGENCE RATES,0.15163934426229508,"||x0 −x⋆||2 ,
(9) and"
CONVERGENCE RATES,0.1526639344262295,"(AdaSLS)
E[||xT +1−x⋆||2] ≤Ei0"
CONVERGENCE RATES,0.15368852459016394,"""
1−µρ3 min2{ 1−ρ"
CONVERGENCE RATES,0.15471311475409835,"L , γmax}γ0||∇fi0(x0)||2"
CONVERGENCE RATES,0.1557377049180328,"(cscale
l
||x0 −x⋆||)2 T
#"
CONVERGENCE RATES,0.1567622950819672,||x0−x⋆||2 . (10)
CONVERGENCE RATES,0.15778688524590165,"The proof of Theorem 6 is presented in Appendix B.3. We now compare the above results with
the other stepsizes. Under the same settings, DecSPS has a slower O(1/
√"
CONVERGENCE RATES,0.15881147540983606,"T) rate due to the usage
of Θ(1/
√"
CONVERGENCE RATES,0.1598360655737705,"t) decay stepsize. While AdaGrad-Norm does have a linear acceleration phase when the
accumulator grows large, to avoid an O(1/ε) slow down, the parameters of AdaGrad-Norm have to
satisfy cg < b0/L, which requires the knowledge of Lipschitz constant [56]. Instead, the conditions
on cp and cl for AdaSPS and AdaSLS only depend on the function value and gradient norm at x0
which can be computed at the first iteration. SPS, SLS, and Vannilia-SGD with constant stepsize
achieve faster linear convergence rate of order O
 
exp(−µ"
CONVERGENCE RATES,0.16086065573770492,"LT)

. It is worth noting that Vannila-SGD
can further remove the individual convexity assumption."
CONVERGENCE RATES,0.16188524590163936,"Discussion. In non-interpolation regimes, AdaSPS and AdaSLS only ensure a slower O(1/
√"
CONVERGENCE RATES,0.16290983606557377,"T)
convergence rate compared with O(1/T) rate achieved by vanilla SGD with Θ(1/t) decay stepsize
when optimizing strongly-convex functions [7]. To our knowledge, no parameter-free adaptive
stepsize exists that achieves such a fast rate under the same assumptions. Therefore, developing an
adaptive algorithm that can adapt to both convex and strongly-convex functions would be a significant
further contribution."
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.16393442622950818,"4
AdaSPS and AdaSLS with variance-reduction"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.16495901639344263,"Combining variance-reduction (VR) with adaptive Polyak-stepsize and line-search to achieve acceler-
ation is a natural idea that has been explored in the last decade [49, 36]. However, it remains an open
challenge as no theoretical guarantee has been proven yet. Indeed, Dubois-Taine et al. [14] provide a
counter-example for intuitive line-search methods. In Appendix E we provide counter-examples of
the classical SPS and its variants. The reason behind the failure is the biased curvature information
provided by fit that prevents global convergence. In this section, we introduce a novel framework to
address this issue. Since there exists many variance-reduced stochastic gradient estimators, we focus
on the classical SVRG estimator in this section, and our framework also applies to other estimators
such as SARAH [40]."
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.16598360655737704,"4.1
Algorithm design: achieving variance-reduction without interpolation"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.16700819672131148,"It is known that adaptive methods such as SPS or SLS converge linearly on problems where the
interpolation condition holds, i.e. f(x) with σf,B = 0."
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.1680327868852459,"For problems that do not satisfy the interpolation condition, our approach is to transition the problem
to an equivalent one that satisfies the interpolation condition. One such transformation is to shift each
individual function by the gradient of fi(x) at x⋆, i.e. Fi(x) = fi(x) −xT ∇fi(x⋆). In this case
f(x) can be written as f(x) = 1"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.16905737704918034,"n
Pn
i=1 Fi(x) due to the fact that 1"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.17008196721311475,"n
Pn
i=1 ∇fi(x⋆) = 0. Note that
∇Fi(x⋆) = ∇fi(x⋆) −∇fi(x⋆) = 0 which implies that each Fi(x) shares the same minimizer and
thus the interpolation condition is satisfied (σ2
f,1 = 0). However, ∇fi(x⋆) is usually not available at
hand. This motivates us to design the following algorithm."
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.1711065573770492,Algorithm 1 (Loopless) AdaSVRPS and AdaSVRLS
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.1721311475409836,"Require: x0 ∈Rd, µF > 0, cp > 0 or cl > 0"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.17315573770491804,"1: set w0 = x0, η−1 = +∞
2: for t = 0 to T −1 do
3:
uniformly sample it ⊆[n]"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.17418032786885246,"4:
set Fit(x) = fit(x) + xT (∇f(wt) −∇fit(wt)) + µF"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.17520491803278687,2 ||x −xt||2
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.1762295081967213,"5:
ηt = min
n
Fit(xt)−F ⋆
it
cp||∇Fit(xt)||2
1
√Pt
s=0 Fis(xs)−F ⋆
is
, ηt−1
o
(AdaSVRPS)"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.17725409836065573,"6:
ηt = min
n
γt
1
cl√Pt
s=0 γs||∇Fis(xs)||2 , ηt−1
o
(AdaSVRLS)2"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.17827868852459017,"7:
xt+1 = ΠX
 
xt −ηt∇Fit(xt)
"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.17930327868852458,"8:
wt+1 =
wt
with probability 1 −pt+1
xt
with probability pt+1
9: return ¯xT = 1"
ADASPS AND ADASLS WITH VARIANCE-REDUCTION,0.18032786885245902,"T
PT −1
t=0 xt"
ALGORITHMS AND CONVERGENCE,0.18135245901639344,"4.2
Algorithms and convergence"
ALGORITHMS AND CONVERGENCE,0.18237704918032788,"Inspired by the observation, we attempt to reduce the variance of the functions σ2
f,B by constructing
a sequence of random functions {Fit(x)} such that σ2
1
n
Pn
i=1 Fit(x),B →0 as xt →x⋆. However,
directly applying SPS or SLS to {Fit(x)} still requires the knowledge of the Lipschitz constant to
guarantee convergence. This problem can be solved by using our proposed AdaSPS and AdaSLS.
The whole procedure of the final algorithm is summarized in Algorithm 1."
ALGORITHMS AND CONVERGENCE,0.1834016393442623,"At each iteration of Algorithm 1, we construct a proxy function by adding two quantities to the
minibatch function fit(x), where µF"
ALGORITHMS AND CONVERGENCE,0.18442622950819673,"2 ||x −xt||2 is a proximal term that helps improve the inherent
stochasticity due to the partial information obtained from fit(x). The additional inner product
quantity is used to draw closer the minimizers of fit(x) and f(x). Following [27, 33], the full
gradient is computed with a coin flip probability. Note that Algorithm 1 still works with ηt replaced
with SVRG and AdaSVRG stepsize since ∇Fit(xt) = ∇fit(xt) −∇fit(wt) + ∇f(wt), and thus
this framework can be seen as a generalization of the standard VR methods. A similar idea can also
be found in the works on federated learning with variance-reduction [32, 2, 24, 37, 50].
Theorem 7. Assume each fi is convex and L smooth and X is a convex compact feasible set with
diameter D. Let pt =
1
at+1 with 0 ≤a < 1. Algorithm 1 converges as:"
ALGORITHMS AND CONVERGENCE,0.18545081967213115,"(AdaSVRPS)
E[f(¯xT ) −f ⋆] ≤
1 +
2L
(1−a)µF T"
ALGORITHMS AND CONVERGENCE,0.1864754098360656,"
2cp(L + µF )D2 + 1 cp"
ALGORITHMS AND CONVERGENCE,0.1875,"2
,
(11)"
ALGORITHMS AND CONVERGENCE,0.1885245901639344,"(AdaSVRLS) E[f(¯xT )−f ⋆] ≤
1 +
2L
(1−a)µF T"
ALGORITHMS AND CONVERGENCE,0.18954918032786885,"
max
n L + µF"
ALGORITHMS AND CONVERGENCE,0.19057377049180327,"(1 −ρ)√ρ,
1
γmax√ρ"
ALGORITHMS AND CONVERGENCE,0.1915983606557377,"o
clD2+
1
cl√ρ"
ALGORITHMS AND CONVERGENCE,0.19262295081967212,"2
, (12)"
ALGORITHMS AND CONVERGENCE,0.19364754098360656,where ¯xT = 1
ALGORITHMS AND CONVERGENCE,0.19467213114754098,"T
PT −1
t=0 xt."
ALGORITHMS AND CONVERGENCE,0.19569672131147542,"Suppose γmax is sufficiently large, then picking µ⋆
F = O(L), c⋆
p = O(
1
√"
ALGORITHMS AND CONVERGENCE,0.19672131147540983,"LD2 ) and c⋆
l = O(
√1−ρ
√ LD2 )"
ALGORITHMS AND CONVERGENCE,0.19774590163934427,yields an O( LD2
ALGORITHMS AND CONVERGENCE,0.1987704918032787,T ) rate which matches the O( L||x0−x⋆||2
ALGORITHMS AND CONVERGENCE,0.19979508196721313,"T
) rate of full-batch gradient descent except
for a larger term D2 due to the lack of knowledge of the Lipschitz constant.
Corollary 8. Under the setting of Theorem 7, given an arbitrary accuracy ε, the total number of
gradient evaluations required to have E[f(¯xT ) −f ⋆] ≤ε in expectation is O(log(1/ε)n + 1/ε)."
ALGORITHMS AND CONVERGENCE,0.20081967213114754,"The proved efficiency of stochastic gradient calls matches the optimal rates of SARAH [40]/SVRG
and AdaSVRG [14] but removes the artificially designed inner and outer loop size. However, note that
Algorithm 1 requires an additional assumption on individual convexity. Unfortunately, we believe this"
ALGORITHMS AND CONVERGENCE,0.20184426229508196,"2where γt is obtained via the Armijo backtracking line-search (Algorithm 4) which satisfies: Fit(xt −
γt∇Fit(xt)) ≤Fit(xt) −ργt||∇Fit(xt)||2 and γt ≤γmax."
ALGORITHMS AND CONVERGENCE,0.2028688524590164,"0
1000
2000
# mini-batch gradient evaluations 10
8 10
5 10
2 101 104 107"
ALGORITHMS AND CONVERGENCE,0.2038934426229508,f(x)-f
ALGORITHMS AND CONVERGENCE,0.20491803278688525,strongly-convex+interpolation
ALGORITHMS AND CONVERGENCE,0.20594262295081966,"0
2000
4000
# mini-batch gradient evaluations 10
8 10
5 10
2 101 104 107"
ALGORITHMS AND CONVERGENCE,0.2069672131147541,f(x)-f
ALGORITHMS AND CONVERGENCE,0.20799180327868852,strongly-convex+non-interpolation
ALGORITHMS AND CONVERGENCE,0.20901639344262296,"0
5000
10000
# mini-batch gradient evaluations 101 103 105"
ALGORITHMS AND CONVERGENCE,0.21004098360655737,f(x)-f
ALGORITHMS AND CONVERGENCE,0.21106557377049182,convex+interpolation
ALGORITHMS AND CONVERGENCE,0.21209016393442623,"0
10000
20000
# mini-batch gradient evaluation 10
2 100 102 104 106"
ALGORITHMS AND CONVERGENCE,0.21311475409836064,f(x)-f
ALGORITHMS AND CONVERGENCE,0.21413934426229508,convex+non-interpolation
ALGORITHMS AND CONVERGENCE,0.2151639344262295,"AdaSVRLS
AdaSVRPS
AdaSLS
AdaSPS"
ALGORITHMS AND CONVERGENCE,0.21618852459016394,"Figure 2: Illustration of the accelerated convergence of AdaSVRPS and AdaSVRLS on quadratic loss without
interpolation. Both algorithms require less gradient evaluations than AdaSPS and AdaSLS for optimizing
non-interpolated problems. However, they are less efficient for solving interpolated problems. (Repeated 3 times.
The solid lines and the shaded area represent the mean and the standard deviation.)"
ALGORITHMS AND CONVERGENCE,0.21721311475409835,"assumption is necessary for the VR methods to work for the algorithms in the Polyak and line-search
family since SPS/SLS has to assume the same condition for the proof in the interpolation settings."
ALGORITHMS AND CONVERGENCE,0.2182377049180328,"Discussion. The classical SVRG with Armijo line-search (presented as Algorithm 6 in [14]) employs
the same gradient estimator as SVRG but chooses its stepsize based on the returning value of
line-search on the individual function fi. Similarly, SVRG with classical Polyak stepsize uses the
individual curvature information of fi to set the stepsize for the global variance-reduced gradient.
Due to the misleading curvature information provided by the biased function fi, both methods
have convergence issues. In constrast, Algorithm 1 reduces the bias by adding a correction term
xT (∇f(wt) −∇fit(wt)) with global information to fi and then applying line-search or Polyak-
stepsize on the variance-reduced functions Fit. This difference essentially guarantees the convergence."
NUMERICAL EVALUATION,0.2192622950819672,"5
Numerical evaluation"
NUMERICAL EVALUATION,0.22028688524590165,"In this section, we illustrate the main properties of our proposed methods in numerical experiments. A
detailed description of the experimental setup can be found in Appendix F. We report the theoretically
justified hyperparameter cscale
p
or cscale
l
as defined in Theorem 6 rather than cp or cl in the following."
NUMERICAL EVALUATION,0.22131147540983606,"Synthetic data. We illustrate the robustness property on a class of synthetic problems. We consider
the minimization of a quadratic of the form: f(x) = 1"
NUMERICAL EVALUATION,0.2223360655737705,"n
Pn
i=1 fi(x) where fi(x) = 1"
NUMERICAL EVALUATION,0.22336065573770492,"2(x−bi)T Ai(x−
bi), bi ∈Rd and Ai ∈Rd×d is a diagonal matrix. We use n = 50, d = 1000. We can control the
convexity of the problem by choosing different matrices Ai, and control interpolation by either setting
all {bi} to be identical or different. We generate a strongly convex instance where the eigenvalues of
∇2f(x) are between 1 and 10, and a general convex instance by setting some of the eigenvalues to
small values close to zero (while ensuring that each ∇2fi(x) is positive semi-definite). The exact
procedure to generate these problems is described in Appendix F."
NUMERICAL EVALUATION,0.22438524590163936,"For all methods, we use a batch size B = 1. We compare AdaSPS and AdaSLS against DecSPS [44],
SPS [34] and SLS [52] to illustrate the robustness property. More comparisons can be found in
Appendix F.1. We fix cscale
p
= cscale
l
= 1 for AdaSPS/AdaSLS and use the optimal parameters for
DecSPS and SPS. In Figure 1, we observe that SPS does not converge in the non-interpolated settings
and DecSPS suffers from a slow O(1/
√"
NUMERICAL EVALUATION,0.22540983606557377,"T) convergence on the two interpolated problems. AdaSPS
and AdaSLS show the desired convergence rates across all cases which matches the theory. When the
problems are non-interpolated, AdaSVRPS and AdaSVRLS illustrate faster convergence, which can
be seen in Figure 2."
NUMERICAL EVALUATION,0.22643442622950818,"Binary classification on LIBSVM datasets. We experiment with binary classification on four diverse
datasets from [10]. We consider the standard regularized logistic loss: f(x) =
1
n
Pn
i=1 log(1 +
exp(−yi · aT
i x)) +
1
2n||x||2 where (ai, yi) ∈Rd+1 are features and labels. We defer the study of
variance-reduction methods to Appendix F.2 for clarity of presentation. We benchmark against popular
optimization algorithms including Adam [26], SPS [34], DecSPS [44], SLS [52] and AdaGrad-
Norm [15]. We fix cscale
p
= cscale
l
= 1 for AdaSPS/AdaSLS and pick the best learning rate from
{10i}i=−4,..,3 for SGD, Adam and AdaGrad-Norm. We observe that Adam, SPS and SLS have
remarkable performances on duke with n = 48 and d = 7129, which satisfies interpolation. AdaSPS"
NUMERICAL EVALUATION,0.22745901639344263,"0
500
1000
1500
# mini-batch gradient evaluations 10
3 10
2 10
1 100 101"
NUMERICAL EVALUATION,0.22848360655737704,Gradient Norm duke
NUMERICAL EVALUATION,0.22950819672131148,"0
5000
10000 15000
# mini-batch gradient evaluations 10
6 10
5 10
4 10
3 10
2"
NUMERICAL EVALUATION,0.2305327868852459,Gradient Norm rcv1
NUMERICAL EVALUATION,0.23155737704918034,"0
10000
20000
30000
# mini-batch gradient evaluations 10
3 10
2 10
1"
NUMERICAL EVALUATION,0.23258196721311475,Gradient Norm ijcnn
NUMERICAL EVALUATION,0.2336065573770492,"0
10000
20000
30000
# mini-batch gradient evaluations 10
4 10
3 10
2 10
1"
NUMERICAL EVALUATION,0.2346311475409836,Gradient Norm w8a
NUMERICAL EVALUATION,0.23565573770491804,"DecSPS
SPS
SGD
SLS
Adam
AdaSLS
AdaSPS
AdaNorm"
NUMERICAL EVALUATION,0.23668032786885246,"Figure 3: Comparison of AdaSPS/AdaSLS against six other popular optimizers on four LIBSVM datasets,
with batch size B = 1 for duke, B = 64 for rcv1, B = 64 for ijcnn and B = 128 for w8a. AdaSPS and
AdaSLS have competitive performance on rcv1, ijcnn, and w8a while SPS, SLS, and Adam converge fast on
duke. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)"
NUMERICAL EVALUATION,0.23770491803278687,"0
50
100
150
200
Epoch 0.800 0.825 0.850 0.875 0.900 0.925 0.950"
NUMERICAL EVALUATION,0.2387295081967213,Test Accuary
NUMERICAL EVALUATION,0.23975409836065573,CIFAR10-ResNet34
NUMERICAL EVALUATION,0.24077868852459017,"0
50
100
150
200
Epoch 10
3 10
2 10
1 100"
NUMERICAL EVALUATION,0.24180327868852458,Train loss
NUMERICAL EVALUATION,0.24282786885245902,"0
50
100
150
200
Epoch 0.50 0.55 0.60 0.65 0.70 0.75"
NUMERICAL EVALUATION,0.24385245901639344,Test Accuary
NUMERICAL EVALUATION,0.24487704918032788,CIFAR100-ResNet34
NUMERICAL EVALUATION,0.2459016393442623,"0
50
100
150
200
Epoch 10
3 10
2 10
1 100"
NUMERICAL EVALUATION,0.24692622950819673,Train loss
NUMERICAL EVALUATION,0.24795081967213115,"SPS
Adam
AdaSPS (DL)
AdaGrad
SGD-M
DecSPS"
NUMERICAL EVALUATION,0.2489754098360656,"Figure 4: Comparison of the considered optimizers on multi-class classification tasks with CIFAR10 and
CIFAR100 datasets using ResNet34 with softmax loss."
NUMERICAL EVALUATION,0.25,"and AdaSLS consistently perform reasonably well on the other three larger datasets. It is worth noting
that the hyper-parameters cscale
p
and cscale
l
are fixed for all the datasets, which is desired in practice."
NUMERICAL EVALUATION,0.2510245901639344,"Deep learning task. We provide a heuristic extension of AdaSPS to over-parameterized non-convex
optimization tasks to illustrate its potential. We benchmark the convergence and generalization
performance of AdaSPS (DL) 5 for the multi-class classification tasks on CIFAR10 [28] and CI-
FAR100 [29] datasets using ResNet-34 [21]. More experimental details can be found in Appendix G.
We demonstrate the effectiveness of AdaSPS (DL) in Figure 4."
NUMERICAL EVALUATION,0.2520491803278688,"Discussion. AdaSPS and AdaSLS consistently demonstrate robust convergence across all tasks and
achieve performance on par with, if not better than, the best-tuned algorithms. Consequently, it is
reliable and convenient for practical use."
CONCLUSION AND FUTURE WORK,0.2530737704918033,"6
Conclusion and future work"
CONCLUSION AND FUTURE WORK,0.2540983606557377,"We proposed new variants of SPS and SLS algorithms and demonstrated their robust and fast
convergence in both interpolated and non-interpolated settings. We further accelerate both algorithms
for convex optimization with a novel variance reduction technique. Interesting future directions may
include: accelerating AdaSPS and AdaSLS with momentum, developing effective robust adaptive
methods for training deep neural networks, designing an adaptive algorithm that gives a faster rate
O(1/T) under strong convexity, extensions to distributed and decentralized settings."
CONCLUSION AND FUTURE WORK,0.2551229508196721,Acknowledgments
CONCLUSION AND FUTURE WORK,0.25614754098360654,We appreciate the fruitful discussion with Anton Rodomanov.
REFERENCES,0.257172131147541,References
REFERENCES,0.2581967213114754,"[1] Farshed Abdukhakimov, Chulu Xiang, Dmitry Kamzolov, and Martin Takáˇc. Stochastic gradient descent
with preconditioned polyak step-size. arXiv preprint arXiv:2310.02093, 2023."
REFERENCES,0.25922131147540983,"[2] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough,
and Venkatesh Saligrama.
Federated learning based on dynamic regularization.
arXiv preprint
arXiv:2111.04263, 2021."
REFERENCES,0.26024590163934425,"[3] Larry Armijo. Minimization of functions having lipschitz continuous first partial derivatives. Pacific
Journal of Mathematics, 16(1):1–3, 1 1966."
REFERENCES,0.2612704918032787,"[4] Hilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Convergence, optimality,
and adaptivity. SIAM Journal on Optimization, 29(3):2257–2290, 2019."
REFERENCES,0.26229508196721313,"[5] Hilal Asi, Daniel Lévy, and John C Duchi. Adapting to function difficulty and growth conditions in private
optimization. Advances in Neural Information Processing Systems, 34:19069–19081, 2021."
REFERENCES,0.26331967213114754,"[6] Leonard Berrada, Andrew Zisserman, and M. Pawan Kumar. Training neural networks for and by
interpolation. In Proceedings of the 37th International Conference on Machine Learning, ICML’20.
JMLR.org, 2020."
REFERENCES,0.26434426229508196,"[7] Léon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM Review, 60(2):223–311, 2018. doi: 10.1137/16M1080173. URL https://doi.org/10.1137/
16M1080173."
REFERENCES,0.26536885245901637,"[8] Sébastien Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8(3–4):
231–357, nov 2015. ISSN 1935-8237. doi: 10.1561/2200000050. URL https://doi.org/10.1561/
2200000050."
REFERENCES,0.26639344262295084,"[9] Karan Chadha, Gary Cheng, and John Duchi. Accelerated, optimal and parallel: Some results on model-
based stochastic optimization. In International Conference on Machine Learning, pages 2811–2827.
PMLR, 2022."
REFERENCES,0.26741803278688525,"[10] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3), may 2011. ISSN 2157-6904. doi: 10.1145/1961189.1961199. URL https:
//doi.org/10.1145/1961189.1961199."
REFERENCES,0.26844262295081966,"[11] Gary Cheng and John Duchi. adastar: A method for adapting to interpolation. In OPT 2022: Optimization
for Machine Learning (NeurIPS 2022 Workshop), 2022. URL https://openreview.net/forum?id=
dMbczvwk6Jw."
REFERENCES,0.2694672131147541,"[12] Ashok Cutkosky and Kwabena Boahen. Online convex optimization with unconstrained domains and
losses. In Proceedings of the 30th International Conference on Neural Information Processing Systems,
NIPS’16, page 748–756, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819."
REFERENCES,0.27049180327868855,"[13] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, vol-
ume 27. Curran Associates, Inc., 2014.
URL https://proceedings.neurips.cc/paper_files/
paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf."
REFERENCES,0.27151639344262296,"[14] Benjamin Dubois-Taine, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, and Simon Lacoste-Julien.
SVRG meets adagrad: painless variance reduction. Mach. Learn., 111(12):4359–4409, 2022. doi:
10.1007/s10994-022-06265-x. URL https://doi.org/10.1007/s10994-022-06265-x."
REFERENCES,0.2725409836065574,"[15] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. J. Mach. Learn. Res., 12(null):2121–2159, jul 2011. ISSN 1532-4435."
REFERENCES,0.2735655737704918,"[16] Leonardo Galli, Holger Rauhut, and Mark Schmidt. Don’t be so monotone: Relaxing stochastic line search
in over-parameterized models, 2023."
REFERENCES,0.27459016393442626,"[17] Robert M. Gower, Aaron Defazio, and Michael G. Rabbat. Stochastic polyak stepsize with a moving target.
CoRR, abs/2106.11851, 2021. URL https://arxiv.org/abs/2106.11851."
REFERENCES,0.27561475409836067,"[18] Robert M Gower, Mathieu Blondel, Nidham Gazagnadou, and Fabian Pedregosa. Cutting some slack for
sgd with adaptive polyak stepsizes. arXiv preprint arXiv:2202.12328, 2022."
REFERENCES,0.2766393442622951,"[19] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining,
inference and prediction. Springer, 2 edition, 2009. URL http://www-stat.stanford.edu/~tibs/
ElemStatLearn/."
REFERENCES,0.2776639344262295,"[20] Elad Hazan and Sham Kakade. Revisiting the polyak step size, 2019. URL https://arxiv.org/abs/
1905.00313."
REFERENCES,0.2786885245901639,"[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
doi: 10.1109/CVPR.2016.90."
REFERENCES,0.2797131147540984,"[22] Rie Johnson and Tong Zhang.
Accelerating stochastic gradient descent using predictive vari-
ance reduction.
In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Wein-
berger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Asso-
ciates, Inc., 2013.
URL https://proceedings.neurips.cc/paper_files/paper/2013/file/
ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf."
REFERENCES,0.2807377049180328,"[23] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira,
Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih
Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo,
Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecný,
Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal,
Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar,
Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh,
Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han
Yu, and Sen Zhao. Advances and open problems in federated learning. Found. Trends Mach. Learn., 14
(1–2):1–210, jun 2021. ISSN 1935-8237. doi: 10.1561/2200000083. URL https://doi.org/10.1561/
2200000083."
REFERENCES,0.2817622950819672,"[24] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh.
SCAFFOLD: Stochastic controlled averaging for federated learning.
In
Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning Research, pages 5132–5143. PMLR, 13–18
Jul 2020. URL https://proceedings.mlr.press/v119/karimireddy20a.html."
REFERENCES,0.2827868852459016,"[25] Ali
Kavis,
Stratis
Skoulakis,
Kimon
Antonakopoulos,
Leello
Tadesse
Dadi,
and
Volkan
Cevher.
Adaptive stochastic variance reduction for non-convex finite-sum minimization.
In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances
in Neural Information Processing Systems,
volume 35,
pages 23524–23538. Curran Asso-
ciates, Inc., 2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
94f625dcdec313cd432d65f96fcc51c8-Paper-Conference.pdf."
REFERENCES,0.2838114754098361,"[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/
1412.6980."
REFERENCES,0.2848360655737705,"[27] Dmitry Kovalev, Samuel Horváth, and Peter Richtárik. Don’t jump through hoops and remove those
loops: Svrg and katyusha are better without the outer loop. In Aryeh Kontorovich and Gergely Neu,
editors, Proceedings of the 31st International Conference on Algorithmic Learning Theory, volume 117
of Proceedings of Machine Learning Research, pages 451–467. PMLR, 08 Feb–11 Feb 2020. URL
https://proceedings.mlr.press/v117/kovalev20a.html."
REFERENCES,0.2858606557377049,"[28] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). .
URL http://www.cs.toronto.edu/~kriz/cifar.html."
REFERENCES,0.28688524590163933,"[29] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research). .
URL http://www.cs.toronto.edu/~kriz/cifar.html."
REFERENCES,0.28790983606557374,"[30] Frederik Kunstner, Victor S. Portella, Mark Schmidt, and Nick Harvey. Searching for optimal per-coordinate
step-sizes with multidimensional backtracking, 2023."
REFERENCES,0.2889344262295082,"[31] Shuang Li, William J Swartworth, Martin Takáˇc, Deanna Needell, and Robert M Gower. Sp2: A second
order stochastic polyak method. arXiv preprint arXiv:2207.08171, 2022."
REFERENCES,0.2899590163934426,"[32] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne
Sze, editors, Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March
2-4, 2020. mlsys.org, 2020. URL https://proceedings.mlsys.org/book/316.pdf."
REFERENCES,0.29098360655737704,"[33] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik. Page: A simple and optimal probabilistic
gradient estimator for nonconvex optimization. In Marina Meila and Tong Zhang, editors, Proceedings of
the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
Research, pages 6286–6295. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/
li21a.html."
REFERENCES,0.29200819672131145,"[34] Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-
size for SGD: an adaptive learning rate for fast convergence. In Arindam Banerjee and Kenji Fukumizu,
editors, The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April
13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 1306–1314.
PMLR, 2021. URL http://proceedings.mlr.press/v130/loizou21a.html."
REFERENCES,0.2930327868852459,"[35] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In Interna-
tional Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=
Skq89Scxx."
REFERENCES,0.29405737704918034,"[36] Julien Mairal. Optimization with first-order surrogate functions. In International Conference on Machine
Learning, pages 783–791. PMLR, 2013."
REFERENCES,0.29508196721311475,"[37] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtárik. Proxskip: Yes! local
gradient steps provably lead to communication acceleration! finally! In International Conference on
Machine Learning, pages 15750–15769. PMLR, 2022."
REFERENCES,0.29610655737704916,"[38] Angelia Nedi´c and Dimitri Bertsekas. Convergence Rate of Incremental Subgradient Algorithms, pages 223–
264. Springer US, Boston, MA, 2001. ISBN 978-1-4757-6594-6. doi: 10.1007/978-1-4757-6594-6{\_}11.
URL https://doi.org/10.1007/978-1-4757-6594-6_11."
REFERENCES,0.29713114754098363,"[39] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer Publishing
Company, Incorporated, 1 edition, 2014. ISBN 1461346916."
REFERENCES,0.29815573770491804,"[40] Lam M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Takáˇc. SARAH: A novel method for ma-
chine learning problems using stochastic recursive gradient.
In Doina Precup and Yee Whye Teh,
editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Pro-
ceedings of Machine Learning Research, pages 2613–2621. PMLR, 06–11 Aug 2017. URL https:
//proceedings.mlr.press/v70/nguyen17b.html."
REFERENCES,0.29918032786885246,"[41] Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA, 2e edition,
2006."
REFERENCES,0.30020491803278687,"[42] Adam M Oberman and Mariana Prazeres. Stochastic gradient descent with polyak’s learning rate. arXiv
preprint arXiv:1903.08688, 2019."
REFERENCES,0.3012295081967213,"[43] Francesco Orabona and Dávid Pál. Scale-free algorithms for online linear optimization. In Kamalika
Chaudhuri, CLAUDIO GENTILE, and Sandra Zilles, editors, Algorithmic Learning Theory, pages 287–301,
Cham, 2015. Springer International Publishing. ISBN 978-3-319-24486-0."
REFERENCES,0.30225409836065575,"[44] Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou.
Dynamics of sgd with stochas-
tic
polyak
stepsizes:
Truly
adaptive
variants
and
convergence
to
exact
solution.
In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances
in Neural Information Processing Systems,
volume 35,
pages 26943–26954. Curran Asso-
ciates, Inc., 2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf."
REFERENCES,0.30327868852459017,"[45] B. T. Polyak. Introduction to optimization. Translations series in mathematics and engineering. Optimiza-
tion Software, Publications Division, New York, 1987. ISBN 0911575146; 9780911575149."
REFERENCES,0.3043032786885246,"[46] Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathematical
Statistics, 22(3):400 – 407, 1951. doi: 10.1214/aoms/1177729586. URL https://doi.org/10.1214/
aoms/1177729586."
REFERENCES,0.305327868852459,"[47] Michal Rolinek and Georg Martius. L4: Practical loss-based stepsize adaptation for deep learning.
In Advances in Neural Information Processing Systems 31 (NeurIPS 2018), pages 6434–6444. Curran
Associates, Inc., 2018. URL http://papers.nips.cc/paper/7879-l4-practical-loss-based-
stepsize-adaptation-for-deep-learning.pdf."
REFERENCES,0.30635245901639346,"[48] Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong growth
condition. arXiv preprint arXiv:1308.6370, 2013."
REFERENCES,0.3073770491803279,"[49] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average
gradient. Mathematical Programming, 162:83–112, 2017."
REFERENCES,0.3084016393442623,"[50] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an
approximate newton-type method. In International conference on machine learning, pages 1000–1008.
PMLR, 2014."
REFERENCES,0.3094262295081967,"[51] Matthew J. Streeter and H. Brendan McMahan. Less regret via online conditioning. CoRR, abs/1002.4862,
2010. URL http://arxiv.org/abs/1002.4862."
REFERENCES,0.3104508196721312,"[52] Sharan Vaswani, Aaron Mishkin, Issam H. Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pages 3727–3740, 2019.
URL https://proceedings.neurips.cc/paper/2019/hash/
2557911c1bf75c2b643afb4ecbfc8ec2-Abstract.html."
REFERENCES,0.3114754098360656,"[53] Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon Lacoste-Julien.
Adaptive gradient methods converge faster with over-parameterization (but you should do a line-search).
arXiv preprint arXiv:2006.06835, 2020."
REFERENCES,0.3125,"[54] Xiaoyu Wang, Mikael Johansson, and Tong Zhang. Generalized polyak step size for first order optimization
with momentum. arXiv preprint arXiv:2305.12939, 2023."
REFERENCES,0.3135245901639344,"[55] Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex
landscapes. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th Interna-
tional Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages
6677–6686. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ward19a.html."
REFERENCES,0.3145491803278688,"[56] Yuege Xie, Xiaoxia Wu, and Rachel Ward. Linear convergence of adaptive stochastic gradient descent. In
Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference
on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages
1475–1485. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/xie20a.html."
REFERENCES,0.3155737704918033,Appendix
REFERENCES,0.3165983606557377,"A
Technical Preliminaries"
REFERENCES,0.3176229508196721,"A.1
Basic Definitions"
REFERENCES,0.31864754098360654,We use the following definitions throughout the paper.
REFERENCES,0.319672131147541,"Definition 1 (convexity). A differentiable function f : Rd →R is convex if ∀x, y ∈Rd,"
REFERENCES,0.3206967213114754,"f(y) ≥f(x) + ⟨∇f(x), y −x⟩.
(A.1)"
REFERENCES,0.32172131147540983,"Definition 2 (strong convexity). A differentiable function f : Rd →R is µ-strongly convex if ∀x, y ∈Rd,"
REFERENCES,0.32274590163934425,"f(y) ≥f(x) + ⟨∇f(x), y −x⟩+ µ"
REFERENCES,0.3237704918032787,"2 ||x −y||2 .
(A.2)"
REFERENCES,0.32479508196721313,"Definition 3 (L-smooth). Let function f : Rd →R be differentiable. f is smooth if there exists L > 0 such that
∀x, y ∈Rd,
||∇f(x) −∇f(y)|| ≤L||x −y|| .
(A.3)"
REFERENCES,0.32581967213114754,"A.2
Useful Lemmas"
REFERENCES,0.32684426229508196,We frequently use the following helpful lemmas for the proof.
REFERENCES,0.32786885245901637,"Lemma 9 (Nesterov [39], Lemma 1.2.3). Definition 3 implies that there exists a quadratic upper bound on f:"
REFERENCES,0.32889344262295084,"f(y) ≤f(x) + ⟨∇f(x), y −x⟩| + L"
REFERENCES,0.32991803278688525,"2 ||y −x||2 , ∀x, y ∈Rd .
(A.4)"
REFERENCES,0.33094262295081966,"Lemma 10 (Nesterov [39], Theorem 2.1.5). If a convex function f satisfies (A.4), then it holds that:"
REFERENCES,0.3319672131147541,"f(y) ≥f(x) + ⟨∇f(x), y −x⟩| + 1"
REFERENCES,0.33299180327868855,"2L||∇f(y) −∇f(x)||2 , ∀x, y ∈Rd.
(A.5)"
REFERENCES,0.33401639344262296,"Lemma 11 (Ward et al. [55]). For any non-negative sequence a0, ..., aT , the following holds:
v
u
u
t T
X"
REFERENCES,0.3350409836065574,"t=0
at ≤ T
X t=0"
REFERENCES,0.3360655737704918,"at
qPt
i=0 ai
≤2"
REFERENCES,0.33709016393442626,"v
u
u
t T
X"
REFERENCES,0.33811475409836067,"t=0
at .
(A.6)"
REFERENCES,0.3391393442622951,"If a0 ≥1, then the following holds: T
X t=0"
REFERENCES,0.3401639344262295,"at
Pt
i=0 ai
≤log( T
X"
REFERENCES,0.3411885245901639,"t=0
at) + 1 .
(A.7)"
REFERENCES,0.3422131147540984,"Proof. To show equation (A.6), we proceed with the proof by induction. For t = 0, (A.6) holds trivially since
√a0 ≤√a0 ≤2√a0. Assume equation (A.6) holds for T −1. For RHS, we have:"
REFERENCES,0.3432377049180328,"T −1
X t=0"
REFERENCES,0.3442622950819672,"at
Pt
i=0 ai
+
aT
qPT
i=0 ai
≤2"
REFERENCES,0.3452868852459016,"v
u
u
t"
REFERENCES,0.3463114754098361,"T −1
X"
REFERENCES,0.3473360655737705,"t=0
at +
aT
qPT
i=0 ai = 2"
REFERENCES,0.3483606557377049,"v
u
u
t T
X"
REFERENCES,0.34938524590163933,"t=0
at −aT +
aT
qPT
t=0 at ≤2"
REFERENCES,0.35040983606557374,"v
u
u
t T
X"
REFERENCES,0.3514344262295082,"t=0
at . (A.8)"
REFERENCES,0.3524590163934426,"where the last inequality is due to the fact that 2√x −y +
y
√x ≤2√x for any x ≥y ≥0. For LHS, we have:"
REFERENCES,0.35348360655737704,"T −1
X t=0"
REFERENCES,0.35450819672131145,"at
Pt
i=0 ai
+
aT
qPT
i=0 ai
≥"
REFERENCES,0.3555327868852459,"v
u
u
t"
REFERENCES,0.35655737704918034,"T −1
X"
REFERENCES,0.35758196721311475,"t=0
at +
aT
qPT
i=0 ai ="
REFERENCES,0.35860655737704916,"v
u
u
t T
X"
REFERENCES,0.35963114754098363,"t=0
at −aT +
aT
qPT
t=0 at ≥"
REFERENCES,0.36065573770491804,"v
u
u
t T
X"
REFERENCES,0.36168032786885246,"t=0
at . (A.9)"
REFERENCES,0.36270491803278687,"where the last inequality is due to the fact that √x −y +
y
√x ≥√x for any x ≥y ≥0."
REFERENCES,0.3637295081967213,"We next show equation (A.7) by induction. For t = 0, equation (A.7) trivially holds since 1 ≤log(a0) + 1.
Assume (A.7) holds for T −1, we have: T
X t=0"
REFERENCES,0.36475409836065575,"at
Pt
i=0 ai
≤log("
REFERENCES,0.36577868852459017,"T −1
X"
REFERENCES,0.3668032786885246,"t=0
at) + 1 +
aT
PT
i=0 ai ≤log( T
X"
REFERENCES,0.367827868852459,"t=0
at) + 1 ."
REFERENCES,0.36885245901639346,(A.10)
REFERENCES,0.3698770491803279,where the last inequality is due to the fact that log(x −y) + y
REFERENCES,0.3709016393442623,"x ≤log(x) for any x ≥y ≥0 since e
y
x ≤ 1+ y"
REFERENCES,0.3719262295081967,"x
1−y2 x2
."
REFERENCES,0.3729508196721312,"Lemma 12 (Dubois-Taine et al. [14, Lemma 5]). If x2 ≤a(x + b) for a ≥0 and b ≥0, then it holds that:"
REFERENCES,0.3739754098360656,"x ≤a +
√"
REFERENCES,0.375,"ab .
(A.11)"
REFERENCES,0.3760245901639344,The following Lemma is an extension of Lemma 5 in [44].
REFERENCES,0.3770491803278688,"Lemma 13. Let zt+1 ≤(1 −aηt)zt + ηtb and zt ≥0 where a > 0, b > 0 and ηt > 0, ηt+1 ≤ηt, ∀t ≥0. It
holds that:
zt ≤max{ b"
REFERENCES,0.3780737704918033,"a, z0, η0b}, ∀t ≥0 .
(A.12)"
REFERENCES,0.3790983606557377,"Proof. Since ηt is non-increasing, 1 −aηt ≤0 is non-decreasing. For any t ≥0 such that 1 −aηt ≤0, we
have zt+1 ≤ηtb ≤η0b. If 1 −aηt ≤0 for all t ≥0, then the proof is done. Otherwise, let us assume there
exists a first index j such that 1 −aηj > 0 and we have zj ≤max{z0, η0b} := ˜z0. We proceed with the proof
starting with the index j by induction. For t = j, the lemma trivially holds. Let us assume zt ≤max{ b"
REFERENCES,0.3801229508196721,"a, ˜z0}
for t > j. If b"
REFERENCES,0.38114754098360654,"a ≥˜z0, then by induction, we have:"
REFERENCES,0.382172131147541,zt+1 ≤(1 −aηt) b
REFERENCES,0.3831967213114754,a + ηtb = b
REFERENCES,0.38422131147540983,"a .
(A.13)"
REFERENCES,0.38524590163934425,If instead b
REFERENCES,0.3862704918032787,"a ≤˜z0, then by induction, we have:"
REFERENCES,0.38729508196721313,"zt+1 ≤(1 −aηt)˜z0 + ηtb = ˜z0 −ηt(a˜z0 −b) ≤˜z0 .
(A.14)"
REFERENCES,0.38831967213114754,Combining the above cases concludes the proof.
REFERENCES,0.38934426229508196,"The following lemma is commonly used in the works on Polyak stepsize [34, 20].
Lemma 14. Suppose a function f is L-smooth and µ-strongly convex, then the following holds:"
REFERENCES,0.39036885245901637,"1
2L ≤f(x) −f ⋆"
REFERENCES,0.39139344262295084,||∇f(x)||2 ≤1
REFERENCES,0.39241803278688525,"2µ .
(A.15)"
REFERENCES,0.39344262295081966,"The following lemma provides upper and lower bounds for the stepsize of AdaSPS.
Lemma 15. Suppose each fi is L-smooth, then the stepsize of AdaSPS (AdaSPS) satisfies:"
REFERENCES,0.3944672131147541,"1
2cpL
1
qPt
s=0 fis(xs) −ℓ⋆
is"
REFERENCES,0.39549180327868855,"≤ηt ≤
fit(xt) −ℓ⋆
it
cp||∇fit(xt)||2
1
qPt
s=0 fis(xs) −ℓ⋆
is"
REFERENCES,0.39651639344262296,".
(A.16)"
REFERENCES,0.3975409836065574,"Proof. The upper bound follows from the definition of the stepsize (AdaSPS). To prove the lower bound, we"
REFERENCES,0.3985655737704918,"note that the stepsize (AdaSPS) is composed of two parts where the first component
fis (xs)−ℓ⋆
is
cp||∇fis (xs)||2 ≥
1
2cpL for
all 0 ≤s ≤t due to (A.15), and the second component is always decreasing. Finally, recall that η−1 = +∞
and thus the proof is completed."
REFERENCES,0.39959016393442626,"The following lemma provides upper and lower bounds for the stepsize of AdaSLS. We refer to Appendix D for
details of the line-search procedure."
REFERENCES,0.40061475409836067,"Lemma 16. Suppose each fi is L-smooth, then the stepsize of AdaSLS (AdaSLS) satisfies:"
REFERENCES,0.4016393442622951,"min
n1 −ρ"
REFERENCES,0.4026639344262295,"L
, γmax
o
1"
REFERENCES,0.4036885245901639,"cl
qPt
s=0 γs||∇fis(xs)||2
≤ηt ≤
γt"
REFERENCES,0.4047131147540984,"cl
qPt
s=0 γs||∇fis(xs)||2
.
(A.17)"
REFERENCES,0.4057377049180328,"Proof. The upper bound is due to the definition of the stepsize (AdaSLS). We next prove the lower bound. From
the smoothness definition, the following holds for all γt:"
REFERENCES,0.4067622950819672,"fit(xt −γt∇fit(xt))
(A.4)
≤fit(xt) −γt||∇fit(xt)||2 + L"
REFERENCES,0.4077868852459016,"2 γ2
t ||∇fit(xt)||2 .
(A.18)"
REFERENCES,0.4088114754098361,For any 0 < γt ≤2(1−ρ)
REFERENCES,0.4098360655737705,"L
, we have:"
REFERENCES,0.4108606557377049,"fit(xt −γt∇fit(xt)) ≤fit(xt) −ργt||∇fit(xt)||2 ,
(A.19)"
REFERENCES,0.41188524590163933,"which satisfies the line-search condition (4). From the procedure of Backtracking line-search (Alg. 4), if
γmax ≤
1−ρ"
REFERENCES,0.41290983606557374,"L , then γt = γmax is accepted. Otherwise, since we require the decreasing factor β to be no
smaller than 1"
REFERENCES,0.4139344262295082,"2 in Algorithm 4, we must have γt ≥
2(1−ρ)"
L,0.4149590163934426,"2L
. Therefore, γt is always lower bounded by
min{ 1−ρ"
L,0.41598360655737704,"L , γmax}. The second component of AdaSLS is always decreasing, and recall that η−1 = +∞. The
proof is thus completed."
L,0.41700819672131145,"B
Proofs of main results"
L,0.4180327868852459,"B.1
Proof of Theorem 1"
L,0.41905737704918034,"Proof. We follow a common proof routine for the general convex optimization [15, 14, 44]. Using the update
rule of PSGD (5), we have:"
L,0.42008196721311475,||xt+1 −x⋆||2 = ||ΠX (xt −ηt∇fit(xt)) −ΠX (x⋆)||2
L,0.42110655737704916,≤||xt −ηt∇fit(xt) −x⋆||2
L,0.42213114754098363,"= ||xt −x⋆||2 −2ηt⟨∇fit(xt), xt −x⋆⟩+ η2
t ||∇fit(xt)||2 . (B.1)"
L,0.42315573770491804,Dividing by 2ηt and rearranging gives:
L,0.42418032786885246,"⟨∇fit(xt), xt −x⋆⟩"
L,0.42520491803278687,≤||xt −x⋆||2
L,0.4262295081967213,"2ηt
−||xt+1 −x⋆||2"
L,0.42725409836065575,"2ηt
+ ηt"
L,0.42827868852459017,2 ||∇fit(xt)||2
L,0.4293032786885246,= ||xt −x⋆||2
L,0.430327868852459,"2ηt
−||xt+1 −x⋆||2"
L,0.43135245901639346,"2ηt+1
+ ||xt+1 −x⋆||2"
L,0.4323770491803279,"2ηt+1
−||xt+1 −x⋆||2"
L,0.4334016393442623,"2ηt
+ ηt"
L,0.4344262295081967,2 ||∇fit(xt)||2 . (B.2)
L,0.4354508196721312,"Summing from t = 0 to t = T −1, we get:"
L,0.4364754098360656,"T −1
X"
L,0.4375,"t=0
⟨∇fit(xt), xt −x⋆⟩ ≤"
L,0.4385245901639344,"T −1
X t=0"
L,0.4395491803278688,||xt −x⋆||2
L,0.4405737704918033,"2ηt
−||xt+1 −x⋆||2"
L,0.4415983606557377,"2ηt+1
+ ||xt+1 −x⋆||2"
L,0.4426229508196721,"2ηt+1
−||xt+1 −x⋆||2"
L,0.44364754098360654,"2ηt
+ ηt"
L,0.444672131147541,2 ||∇fit(xt)||2
L,0.4456967213114754,≤||x0 −x⋆||2
L,0.44672131147540983,"2η0
−||xT −x⋆||2"
L,0.44774590163934425,"2ηT
+ ||xT −x⋆||2"
L,0.4487704918032787,"2ηT
−||xT −x⋆||2"
L,0.44979508196721313,"2ηT −1
+"
L,0.45081967213114754,"T −2
X"
L,0.45184426229508196,"t=0
(
1
2ηt+1 −
1
2ηt )D2 +"
L,0.45286885245901637,"T −1
X t=0 ηt"
L,0.45389344262295084,2 ||∇fit(xt)||2
L,0.45491803278688525,≤||x0 −x⋆||2
L,0.45594262295081966,"2η0
−||xT −x⋆||2"
L,0.4569672131147541,"2ηT
+ ||xT −x⋆||2"
L,0.45799180327868855,"2ηT
+
D2"
L,0.45901639344262296,2ηT −1 +
L,0.4600409836065574,"T −1
X t=0 ηt"
L,0.4610655737704918,2 ||∇fit(xt)||2
L,0.46209016393442626,= ||x0 −x⋆||2
L,0.46311475409836067,"2η0
+
D2"
L,0.4641393442622951,2ηT −1 +
L,0.4651639344262295,"T −1
X t=0 ηt"
L,0.4661885245901639,"2 ||∇fit(xt)||2 ,
(B.3)"
L,0.4672131147540984,"where in the second inequality, we use the decreasing property of the stepsize ηt which guarantees
1
2ηt −
1
2ηt−1 ≥"
L,0.4682377049180328,"0, and we use the fact that ||xt −x⋆||2 ≤D2 because of the projection step in (5). For clarity, we next separate
the proof for AdaSPS and AdaSLS."
L,0.4692622950819672,AdaSPS: We upper bound the last two terms by using Lemma 15 and we obtain:
L,0.4702868852459016,"T −1
X t=0 ηt"
L,0.4713114754098361,"2 ||∇fit(xt)||2 (A.16)
≤"
L,0.4723360655737705,"T −1
X t=0"
L,0.4733606557377049,"fit(xt) −ℓ⋆
it"
"CP
QPT",0.47438524590163933,"2cp
qPt
s=0 fis(xs) −ℓ⋆
is"
"CP
QPT",0.47540983606557374,"(A.6)
≤
1
cp"
"CP
QPT",0.4764344262295082,"v
u
u
t"
"CP
QPT",0.4774590163934426,"T −1
X"
"CP
QPT",0.47848360655737704,"s=0
fis(xs) −ℓ⋆
is ,
(B.4) and D2"
"CP
QPT",0.47950819672131145,2ηT −1
"CP
QPT",0.4805327868852459,"(A.16)
≤cpLD2"
"CP
QPT",0.48155737704918034,"v
u
u
t"
"CP
QPT",0.48258196721311475,"T −1
X"
"CP
QPT",0.48360655737704916,"s=0
fis(xs) −ℓ⋆
is .
(B.5)"
"CP
QPT",0.48463114754098363,Using ||x0−x⋆||2
"CP
QPT",0.48565573770491804,"2η0
≤
D2"
"CP
QPT",0.48668032786885246,2ηT −1 and plugging (B.4) and (B.5) back to (B.3) gives:
"CP
QPT",0.48770491803278687,"T −1
X"
"CP
QPT",0.4887295081967213,"t=0
⟨∇fit(xt), xt −x⋆⟩≤(2cpLD2 + 1 cp )"
"CP
QPT",0.48975409836065575,"v
u
u
t"
"CP
QPT",0.49077868852459017,"T −1
X"
"CP
QPT",0.4918032786885246,"s=0
fis(xs) −ℓ⋆
is .
(B.6)"
"CP
QPT",0.492827868852459,"Taking the expectation on both sides, we have:"
"CP
QPT",0.49385245901639346,"T −1
X"
"CP
QPT",0.4948770491803279,"t=0
E[⟨∇f(xt), xt −x⋆⟩] ≤(2cpLD2 + 1"
"CP
QPT",0.4959016393442623,"cp ) E
h
v
u
u
t"
"CP
QPT",0.4969262295081967,"T −1
X"
"CP
QPT",0.4979508196721312,"s=0
fis(xs) −ℓ⋆
is
i"
"CP
QPT",0.4989754098360656,= (2cpLD2 + 1
"CP
QPT",0.5,"cp ) E
h
v
u
u
t"
"CP
QPT",0.5010245901639344,"T −1
X"
"CP
QPT",0.5020491803278688,"s=0
fis(xs) −fis(x⋆) + fis(x⋆) −ℓ⋆
is
i
. (B.7)"
"CP
QPT",0.5030737704918032,"Using the convexity assumption of f and applying Jensen’s inequality to the square root function, we get:"
"CP
QPT",0.5040983606557377,"T −1
X"
"CP
QPT",0.5051229508196722,"t=0
E[f(xt) −f ⋆] ≤(2cpLD2 + 1 cp )"
"CP
QPT",0.5061475409836066,"v
u
u
t"
"CP
QPT",0.507172131147541,"T −1
X"
"CP
QPT",0.5081967213114754,"s=0
E[f(xs) −f ⋆] + σ2
f,B + err2
f,B ,
(B.8)"
"CP
QPT",0.5092213114754098,"where err2
f,B = Eis[f ⋆
is −ℓ⋆
is]. Let τ := 2cpLD2 +
1
cp . Taking the square gives: ("
"CP
QPT",0.5102459016393442,"T −1
X"
"CP
QPT",0.5112704918032787,"t=0
E[f(xt) −f ⋆])2 ≤τ 2T −1
X"
"CP
QPT",0.5122950819672131,"t=0
E[f(xt) −f ⋆] + T(σ2
f,B + err2
f,B)

.
(B.9)"
"CP
QPT",0.5133196721311475,"We next apply Lemma 12 with x = PT −1
t=0 E[f(xt) −f(x⋆)], a = τ 2 and b = T(σ2
f,B + err2
f,B):"
"CP
QPT",0.514344262295082,"T −1
X"
"CP
QPT",0.5153688524590164,"t=0
E[f(xt) −f ⋆] ≤τ 2 + τ
q"
"CP
QPT",0.5163934426229508,"σ2
f,B + err2
f,B
√"
"CP
QPT",0.5174180327868853,"T .
(B.10)"
"CP
QPT",0.5184426229508197,We conclude by dividing both sides by T and using Jensen’s inequality:
"CP
QPT",0.5194672131147541,"E[f(¯xT ) −f ⋆] ≤
PT −1
t=0 E[f(xt) −f ⋆]"
"CP
QPT",0.5204918032786885,"T
≤τ 2"
"CP
QPT",0.5215163934426229,"T +
τ
q"
"CP
QPT",0.5225409836065574,"σ2
f,B + err2
f,B
√"
"CP
QPT",0.5235655737704918,"T
.
(B.11)"
"CP
QPT",0.5245901639344263,where ¯xT = 1
"CP
QPT",0.5256147540983607,"T
PT −1
t=0 xt."
"CP
QPT",0.5266393442622951,"AdaSLS: The proof is almost the same as AdaSPS. We omit procedures with the same proof reasons for
simplicity. We first use Lemma 16 to obtain:"
"CP
QPT",0.5276639344262295,"T −1
X t=0 ηt"
"CP
QPT",0.5286885245901639,"2 ||∇fit(xt)||2 (A.17)
≤"
"CP
QPT",0.5297131147540983,"T −1
X t=0"
"CP
QPT",0.5307377049180327,γt||∇fit(xt)||2
"CL
QPT",0.5317622950819673,"2cl
qPt
s=0 γs||∇fis(xs)||2"
"CL
QPT",0.5327868852459017,"(A.6)
≤
1
cl"
"CL
QPT",0.5338114754098361,"v
u
u
t"
"CL
QPT",0.5348360655737705,"T −1
X"
"CL
QPT",0.5358606557377049,"s=0
γs||∇fis(xs)||2 ,
(B.12) and D2"
"CL
QPT",0.5368852459016393,2ηT −1
"CL
QPT",0.5379098360655737,"(A.17)
≤
cl
qPT −1
s=0 γs||∇fis(xs)||2D2"
"MIN
N",0.5389344262295082,"2 min
n
1−ρ"
"MIN
N",0.5399590163934426,"L , γmax
o
=
max{
L
1−ρ,
1
γmax }clD2 2"
"MIN
N",0.5409836065573771,"v
u
u
t"
"MIN
N",0.5420081967213115,"T −1
X"
"MIN
N",0.5430327868852459,"s=0
γs||∇fis(xs)||2 .
(B.13)"
"MIN
N",0.5440573770491803,Inequality (B.3) can then be further bounded by:
"MIN
N",0.5450819672131147,"T −1
X"
"MIN
N",0.5461065573770492,"t=0
⟨∇fit(xt), xt −x⋆⟩≤

max
n
L
1 −ρ,
1
γmax"
"MIN
N",0.5471311475409836,"o
clD2 + 1 cl"
"MIN
N",0.548155737704918,"
v
u
u
t"
"MIN
N",0.5491803278688525,"T −1
X"
"MIN
N",0.5502049180327869,"s=0
γs||∇fis(xs)||2"
"MIN
N",0.5512295081967213,"≤

max
n
L
(1 −ρ)√ρ,
1
γmax√ρ"
"MIN
N",0.5522540983606558,"o
clD2 +
1
cl√ρ"
"MIN
N",0.5532786885245902,"
v
u
u
t"
"MIN
N",0.5543032786885246,"T −1
X"
"MIN
N",0.555327868852459,"s=0
fis(xs) −f ⋆
is."
"MIN
N",0.5563524590163934,(B.14)
"MIN
N",0.5573770491803278,"where we used line-search condition (4) and the fact that fis(xt −γt∇fis(xs)) ≥f ⋆
is."
"MIN
N",0.5584016393442623,"Let τ := max
n
L
(1−ρ)√ρ,
1
γmax√ρ
o
clD2 +
1
cl
√ρ. We arrive at:"
"MIN
N",0.5594262295081968,E[f(¯xT ) −f ⋆] ≤τ 2
"MIN
N",0.5604508196721312,"T + τσf,B
√"
"MIN
N",0.5614754098360656,"T
.
(B.15)"
"MIN
N",0.5625,where ¯xT = 1
"MIN
N",0.5635245901639344,"T
PT −1
t=0 xt."
"MIN
N",0.5645491803278688,"B.2
Full statement and proof for Lemma 4"
"MIN
N",0.5655737704918032,"Lemma 17 (Bounded iterates). Let each fi be µ-strongly convex and L-smooth. For any t ∈N, the iterates of
SGD with AdaSPS or AdaSLS satisfy:"
"MIN
N",0.5665983606557377,"||xt −x⋆||2 ≤Dmax := max

||x0 −x⋆||2, 2σ2
max + b"
"MIN
N",0.5676229508196722,"µ
, (2σ2
max + b)η0"
"MIN
N",0.5686475409836066,"
,
(B.16)"
"MIN
N",0.569672131147541,"where σ2
max
:=
maxit {fit(x⋆) −ℓ⋆
it}, b
:=
1/

4c3
p
q"
"MIN
N",0.5706967213114754,"fi0(x0) −ℓ⋆
i0"
"MIN
N",0.5717213114754098,"
for AdaSPS and σ2
max
:="
"MIN
N",0.5727459016393442,"maxit {fit(x⋆) −f ⋆
it}, b := 1/

4c3
l ρ2p"
"MIN
N",0.5737704918032787,"γ0||∇fi0(x0)||2

for AdaSLS."
"MIN
N",0.5747950819672131,"Proof. By strong convexity of fit, the iterates generated by SGD satisfy:"
"MIN
N",0.5758196721311475,"||xt+1 −x⋆||2 = ||xt −x⋆||2 −2ηt⟨∇fit(xt), xt −x⋆⟩+ η2
t ||∇fit(xt)||2"
"MIN
N",0.576844262295082,"(A.2)
≤||xt −x⋆||2 −2ηt(fit(xt) −fit(x⋆) + µ"
"MIN
N",0.5778688524590164,"2 ||xt −x⋆||2) + η2
t ||∇fit(xt)||2"
"MIN
N",0.5788934426229508,"= (1 −ηtµ)||xt −x⋆||2 −2ηt(fit(xt) −fit(x⋆)) + η2
t ||∇fit(xt)||2 ."
"MIN
N",0.5799180327868853,(B.17)
"MIN
N",0.5809426229508197,We next separate the proofs for clarity.
"MIN
N",0.5819672131147541,"AdaSPS: Plugging in the upper bound of ηt in Lemma 15, we obtain:"
"MIN
N",0.5829918032786885,||xt+1 −x⋆||2
"MIN
N",0.5840163934426229,"(A.16)
≤(1 −ηtµ)||xt −x⋆||2 −2ηt(fit(xt) −fit(x⋆)) + ηt
fit(xt) −ℓ⋆
it"
"MIN
N",0.5850409836065574,"cp
qPt
s=0 fis(xt) −ℓ⋆
is"
"MIN
N",0.5860655737704918,"= (1 −ηtµ)||xt −x⋆||2 + 2ηt(fit(x⋆) −ℓ⋆
it) −2ηt(fit(xt) −ℓ⋆
it) + ηt
fit(xt) −ℓ⋆
it"
"MIN
N",0.5870901639344263,"cp
qPt
s=0 fis(xt) −ℓ⋆
is"
"MIN
N",0.5881147540983607,"≤(1 −ηtµ)||xt −x⋆||2 + 2ηtσ2
max −2ηt (fit(xt) −ℓ⋆
it)
|
{z
}
≥0"
"MIN
N",0.5891393442622951,"+ηt
fit(xt) −ℓ⋆
it"
"MIN
N",0.5901639344262295,"cp
qPt
s=0 fis(xt) −ℓ⋆
is ,"
"MIN
N",0.5911885245901639,(B.18)
"MIN
N",0.5922131147540983,"where σ2
max := maxit {fit(x⋆) −ℓ⋆
it}."
"MIN
N",0.5932377049180327,"We now split the proof into two cases. Firstly, if cp
qPt
s=0 fis(xt) −ℓ⋆
is ≤1"
"MIN
N",0.5942622950819673,"2, then it follows that:"
"MIN
N",0.5952868852459017,"fit(xt) −ℓ⋆
it ≤( 1"
"MIN
N",0.5963114754098361,"2cp )2
and t
X"
"MIN
N",0.5973360655737705,"s=0
fis(xt) −ℓ⋆
is ≥fi0(x0) −ℓ⋆
i0 .
(B.19)"
"MIN
N",0.5983606557377049,"Plugging in the above bounds, we have:"
"MIN
N",0.5993852459016393,"||xt+1 −x⋆||2 ≤(1 −ηtµ)||xt −x⋆||2 + ηt(2σ2
max +
1"
"MIN
N",0.6004098360655737,"4c3p
q"
"MIN
N",0.6014344262295082,"fi0(x0) −ℓ⋆
i0
) .
(B.20)"
"MIN
N",0.6024590163934426,"We conclude by applying Lemma 13 with zt = ||xt −x⋆||2, a = µ, b = (2σ2
max +
1
4c3p
q"
"MIN
N",0.6034836065573771,"fi0 (x0)−ℓ⋆
i0
). Secondly,"
"MIN
N",0.6045081967213115,"if instead cp
qPt
s=0 fis(xt) −ℓ⋆
is ≥1"
"MIN
N",0.6055327868852459,"2, then we have:"
"MIN
N",0.6065573770491803,"−2ηt(fit(xt) −ℓ⋆
it) + ηt
fit(xt) −ℓ⋆
it"
"MIN
N",0.6075819672131147,"cp
qPt
s=0 fis(xt) −ℓ⋆
is"
"MIN
N",0.6086065573770492,"≤0 ,
(B.21)"
"MIN
N",0.6096311475409836,"and consequently we can apply Lemma 13 with zt = ||xt −x⋆||2, a = µ, b = 2σ2
max."
"MIN
N",0.610655737704918,"AdaSLS: Similarly, by plugging the upper bound of ηt in Lemma 16, we obtain:"
"MIN
N",0.6116803278688525,||xt+1 −x⋆||2
"MIN
N",0.6127049180327869,"(A.17)
≤(1 −ηtµ)||xt −x⋆||2 −2ηt(fit(xt) −fit(x⋆)) + ηt
γt||∇fit(xt)||2"
"MIN
N",0.6137295081967213,"cl
qPt
s=0 γs∥|∇fis(xs)||2"
"MIN
N",0.6147540983606558,"≤(1 −ηtµ)||xt −x⋆||2 + 2ηt(fit(x⋆) −f ⋆
it) −2ηt(fit(xt) −f ⋆
it) + ηt
fit(xt) −f ⋆
it"
"MIN
N",0.6157786885245902,"clρ
qPt
s=0 γs∥|∇fis(xs)||2"
"MIN
N",0.6168032786885246,"≤(1 −ηtµ)||xt −x⋆||2 + 2ηtσ2
max −2ηt (fit(xt) −f ⋆
it)
|
{z
}
≥0"
"MIN
N",0.617827868852459,"+ηt
fit(xt) −f ⋆
it"
"MIN
N",0.6188524590163934,"clρ
qPt
s=0 γs∥|∇fis(xs)||2
,"
"MIN
N",0.6198770491803278,(B.22)
"MIN
N",0.6209016393442623,"where σ2
max = maxit {fit(x⋆) −f ⋆
it}. We can then compare clρ
qPt
s=0 γs∥|∇fis(xs)||2 with 1"
AND APPLY,0.6219262295081968,"2 and apply
Lemma 13 correspondingly."
AND APPLY,0.6229508196721312,"B.3
Proofs for Theorem 2 and 6"
AND APPLY,0.6239754098360656,"Proof. For clarity, we separate the proofs for AdaSPS and AdaSLS."
AND APPLY,0.625,"AdaSPS: Plugging in the upper bound of ηt in Lemma 15, we have:"
AND APPLY,0.6260245901639344,"||xt+1 −x⋆||2 (A.16)
≤||xt −x⋆||2 −2ηt⟨∇fit(xt), xt −x⋆⟩+ ηt
fit(xt) −f ⋆"
AND APPLY,0.6270491803278688,"cp
qPt
s=0 fis(xs) −f ⋆
.
(B.23)"
AND APPLY,0.6280737704918032,"Since cp
p"
AND APPLY,0.6290983606557377,"fi0(x0) −f ⋆≥1, (B.23) can be reduced to:"
AND APPLY,0.6301229508196722,"||xt+1 −x⋆||2 ≤||xt −x⋆||2 −2ηt⟨∇fit(xt), xt −x⋆⟩+ ηt(fit(xt) −f ⋆) .
(B.24)"
AND APPLY,0.6311475409836066,"By convexity of fit, we get:"
AND APPLY,0.632172131147541,"||xt+1 −x⋆||2 (A.1)
≤||xt −x⋆||2 −ηt⟨∇fit(xt), xt −x⋆⟩.
(B.25)"
AND APPLY,0.6331967213114754,"Note that ⟨∇fit(xt), xt −x⋆⟩≥0 and ηt is non-increasing, we thus get:"
AND APPLY,0.6342213114754098,"||xt+1 −x⋆||2 ≤||xt −x⋆||2 −ηT −1⟨∇fit(xt), xt −x⋆⟩.
(B.26)"
AND APPLY,0.6352459016393442,"We first show that ηT −1 is always lower bounded. From equation (B.25) and using convexity of fit, we get:"
AND APPLY,0.6362704918032787,"ηt(fit(xt) −f ⋆) ≤||xt −x⋆||2 −||xt+1 −x⋆||2 .
(B.27)"
AND APPLY,0.6372950819672131,"Summing from t = 0 to t = T −1, we get:"
AND APPLY,0.6383196721311475,"T −1
X"
AND APPLY,0.639344262295082,"t=0
ηt(fit(xt) −f ⋆) ≤||x0 −x⋆||2 .
(B.28)"
AND APPLY,0.6403688524590164,"Using the lower bound of ηt, we get:"
AND APPLY,0.6413934426229508,"1
2cpL"
AND APPLY,0.6424180327868853,"v
u
u
t"
AND APPLY,0.6434426229508197,"T −1
X"
AND APPLY,0.6444672131147541,"s=0
fis(xs) −f ⋆
(A.6)
≤
1
2cpL"
AND APPLY,0.6454918032786885,"T −1
X t=0"
AND APPLY,0.6465163934426229,"fit(xt) −f ⋆
qPt
s=0 fis(xs) −f ⋆"
AND APPLY,0.6475409836065574,"(A.16)
≤"
AND APPLY,0.6485655737704918,"T −1
X"
AND APPLY,0.6495901639344263,"t=0
ηt(fit(xt) −f ⋆) ,
(B.29)"
AND APPLY,0.6506147540983607,This reveals that:
AND APPLY,0.6516393442622951,"ηT −1
(A.16)
≥
1
2cpL
1
qPT −1
s=0 fis(xs) −f ⋆
≥
1
(2cpL||x0 −x⋆||)2 .
(B.30)"
AND APPLY,0.6526639344262295,"Plugging in the lower bound of ηT −1 to (B.26), we obtain:"
AND APPLY,0.6536885245901639,"||xt+1 −x⋆||2 ≤||xt −x⋆||2 −
1
(2cpL||x0 −x⋆||)2 ⟨∇fit(xt), xt −x⋆⟩.
(B.31)"
AND APPLY,0.6547131147540983,"Plugging in cp =
cscale
p
√"
AND APPLY,0.6557377049180327,"fi0 (x0)−f⋆, we get"
AND APPLY,0.6567622950819673,"||xt+1 −x⋆||2 ≤||xt −x⋆||2 −
fi0(x0) −f ⋆"
AND APPLY,0.6577868852459017,"(2cscale
p
L||x0 −x⋆||)2 ⟨∇fit(xt), xt −x⋆⟩.
(B.32)"
AND APPLY,0.6588114754098361,case 1: f is convex.
AND APPLY,0.6598360655737705,"For any t ≥1, we take expectation conditional on i0 on both sides and get:"
AND APPLY,0.6608606557377049,"E[||xt+1 −x⋆||2|i0] ≤E[||xt −x⋆||2|i0] −
fi0(x0) −f ⋆"
AND APPLY,0.6618852459016393,"(2cscale
p
L||x0 −x⋆||)2 E[⟨∇fit(xt), xt −x⋆⟩|i0]"
AND APPLY,0.6629098360655737,"(A.1)
≤E[||xt −x⋆||2|i0] −
fi0(x0) −f ⋆"
AND APPLY,0.6639344262295082,"(2cscale
p
L||x0 −x⋆||)2 E[f(xt) −f ⋆|i0] .
(B.33)"
AND APPLY,0.6649590163934426,"Summing up from t = 1 to t = T and dividing by T, we obtain:"
T,0.6659836065573771,"1
T T
X"
T,0.6670081967213115,"t=1
E[f(xt) −f ⋆|i0] ≤4L(cscale
p
)2 ||x0 −x⋆||2"
T,0.6680327868852459,"fi0(x0) −f ⋆
L E[||x1 −x⋆||2|i0]"
T,0.6690573770491803,"T
.
(B.34)"
T,0.6700819672131147,Note that E[||x1 −x⋆||2|i0] = ||x1 −x⋆||2 ≤||x0 −x⋆||2 due to (B.31). We thus get:
T,0.6711065573770492,"1
T T
X"
T,0.6721311475409836,"t=1
E[f(xt) −f ⋆|i0] ≤

4L(cscale
p
)2 ||x0 −x⋆||2"
T,0.673155737704918,"fi0(x0) −f ⋆
L||x0 −x⋆||2"
T,0.6741803278688525,"T
(B.35)"
T,0.6752049180327869,"Taking expectation w.r.t i0 on both sides, we arrive at:"
T,0.6762295081967213,"1
T T
X"
T,0.6772540983606558,"t=1
E[f(xt) −f ⋆] ≤AL||x0 −x⋆||2"
T,0.6782786885245902,"T
with A := 4L(cscale
p
)2 Ei0[ ||x0 −x⋆||2"
T,0.6793032786885246,"fi0(x0) −f ⋆] .
(B.36)"
T,0.680327868852459,case 2: f is strongly-convex
T,0.6813524590163934,"For any t ≥1, we take expectation conditional on i0 on both sides of (B.32) and get:"
T,0.6823770491803278,"E[||xt+1 −x⋆||2|i0] ≤E[||xt −x⋆||2|i0] −
fi0(x0) −f ⋆"
T,0.6834016393442623,"(2cscale
p
L||x0 −x⋆||)2 E[⟨∇fit(xt), xt −x⋆⟩|i0]"
T,0.6844262295081968,"(A.2)
≤E[||xt −x⋆||2|i0] −
fi0(x0) −f ⋆"
T,0.6854508196721312,"(2cscale
p
L||x0 −x⋆||)2 E[f(xt) −f ⋆+ µ"
T,0.6864754098360656,2 ||xt −x⋆||2|i0] .
T,0.6875,(B.37)
T,0.6885245901639344,Note that f(xt) −f ⋆≥µ
T,0.6895491803278688,"2 ||xt −x⋆||2 due to strong convexity, we obtain:"
T,0.6905737704918032,"E[||xt+1 −x⋆||2|i0] ≤

1 −
(fi0(x0) −f ⋆)µ
(2cscale
p
L||x0 −x⋆||)2

E[||xt −x⋆||2|i0] .
(B.38)"
T,0.6915983606557377,"For any T ≥1, unrolling gives:"
T,0.6926229508196722,"E[||xT +1 −x⋆||2|i0] ≤

1 −
(fi0(x0) −f ⋆)µ
(2cscale
p
L||x0 −x⋆||)2
T
E[||x1 −x⋆||2|i0]"
T,0.6936475409836066,"≤

1 −
(fi0(x0) −f ⋆)µ
(2cscale
p
L||x0 −x⋆||)2
T
||x0 −x⋆||2 .
(B.39)"
T,0.694672131147541,The claim of Theorem 6 follows by taking expectation w.r.t i0 on both sides.
T,0.6956967213114754,"AdaSLS: We only highlight the difference from AdaSPS. Plugging in the upper bound of ηt from Lemma 16
and using the line-search condition, we get:"
T,0.6967213114754098,"||xt+1 −x⋆||2 (A.17)
≤||xt −x⋆||2 −2ηt⟨∇fit(xt), xt −x⋆⟩+ ηt
γt||∇fit(xt)||2"
T,0.6977459016393442,"cl
qPt
s=0 γs||∇fis(xs)||2"
T,0.6987704918032787,"≤||xt −x⋆||2 −2ηt⟨∇fit(xt), xt −x⋆⟩+ ηt
fit(xt) −f ⋆"
T,0.6997950819672131,"clρ
qPt
s=0 γs||∇fis(xs)||2
.
(B.40)"
T,0.7008196721311475,"Since clρ
qPt
s=0 γs||∇fis(xs)||2 ≥1, we obtain the same equation as (B.26). To find a lower bound of ηT −1,
we rearrange (B.25) as:
ηt(fit(xt) −f ⋆) ≤||xt −x⋆||2 −||xt+1 −x⋆||2 ,
(B.41)
the left-hand-side of which can be lower bounded by:"
T,0.701844262295082,"ηt(fit(xt) −f ⋆) ≥ηtργt||∇fit(xt)||2 (A.17)
≥min{1 −ρ"
T,0.7028688524590164,"L
, γmax}
ργt||∇fit(xt)||2"
T,0.7038934426229508,"cl
qPt
s=0 γs||∇fis(xs)||2
.
(B.42)"
T,0.7049180327868853,Summing over t = 0 to t = T −1 gives:
T,0.7059426229508197,min{1 −ρ
T,0.7069672131147541,"L
, γmax} ρ cl"
T,0.7079918032786885,"v
u
u
t"
T,0.7090163934426229,"T −1
X"
T,0.7100409836065574,"s=0
γs||∇fis(xs)||2
(A.6)
≤min{1 −ρ"
T,0.7110655737704918,"L
, γmax} ρ cl"
T,0.7120901639344263,"T −1
X t=0"
T,0.7131147540983607,"γt||∇fit(xt)||2
qPt
s=0 γs||∇fis(xs)||2
≤||x0−x⋆||2 ."
T,0.7141393442622951,"(B.43)
This implies that:"
T,0.7151639344262295,"ηT −1
(A.17)
≥
min{ 1−ρ"
T,0.7161885245901639,"L , γmax}"
T,0.7172131147540983,"cl
qPT −1
s=0 γs||∇fis(xs)||2"
T,0.7182377049180327,"(B.43)
≥
ρ min2{ 1−ρ"
T,0.7192622950819673,"L , γmax}
c2
l ||x0 −x⋆||2
.
(B.44)"
T,0.7202868852459017,"After plugging the above into (B.26), the remaining proof follows from the same routine as shown for AdaSPS."
T,0.7213114754098361,Proofs for Loopless Variance-Reduction
T,0.7223360655737705,"B.4
Statement and Proof of Lemma 18"
T,0.7233606557377049,"The following Lemma provides us with the guarantee that as wt, xt →x⋆, Eit[Fit(xt) −F ⋆
it] →0, which
implies diminishing variance.
Lemma 18. Assume each fi is convex and L-smooth, for any t ≥0, the iterates generated by Algorithm 1
satisfy:"
T,0.7243852459016393,"Eit[Fit(xt) −F ⋆
it] ≤f(xt) −f ⋆+
1
2µF Eit

||∇fit(wt) −∇fit(x⋆)||2
.
(B.45)"
T,0.7254098360655737,"Proof. By definition of Fit(x), we have:"
T,0.7264344262295082,"Fit(xt) −F ⋆
it
= Fit(xt) −Fit(x⋆) + Fit(x⋆) −F ⋆
it"
T,0.7274590163934426,= fit(xt) −fit(x⋆) + (xt −x⋆)T (∇f(wt) −∇fit(wt)) −µF
T,0.7284836065573771,"2 ||xt −x⋆||2 + Fit(x⋆) −F ⋆
it ."
T,0.7295081967213115,(B.46)
T,0.7305327868852459,"By µF -strong convexity of Fit(x), we obtain:"
T,0.7315573770491803,"Fit(x⋆) −F ⋆
it"
T,0.7325819672131147,"(A.15)
≤
1
2µF ||∇Fit(x⋆)||2"
T,0.7336065573770492,"=
1
2µF ||∇fit(x⋆) −∇fit(wt) + ∇f(wt) + µF (x⋆−xt)||2 .
(B.47)"
T,0.7346311475409836,"Plugging (B.47) into (B.46), taking expectation w.r.t the randomness it on both sides gives:"
T,0.735655737704918,"Eit[Fit(xt) −F ⋆
it]"
T,0.7366803278688525,≤f(xt) −f(x⋆) −µF
T,0.7377049180327869,2 ||xt −x⋆||2 + Eit[ 1
T,0.7387295081967213,2µF ||∇fit(x⋆) −∇fit(wt) + ∇f(wt) + µF (x⋆−xt)||2]
T,0.7397540983606558,= f(xt) −f(x⋆) −µF
T,0.7407786885245902,2 ||xt −x⋆||2 + µF
T,0.7418032786885246,"2 ||xt −x⋆||2 +
1
2µF Eit[||∇fit(x⋆) −∇fit(wt) + ∇f(wt)||2] + 1"
T,0.742827868852459,"µF Eit[⟨∇fit(x⋆) −∇fit(wt) + ∇f(wt), µF (x⋆−xt)⟩]"
T,0.7438524590163934,"= f(xt) −f(x⋆) +
1
2µF Eit[||∇fit(x⋆) −∇fit(wt) + ∇f(wt)||2]"
T,0.7448770491803278,"≤f(xt) −f(x⋆) +
1
2µF Eit[||∇fit(x⋆) −∇fit(wt)||2] ."
T,0.7459016393442623,(B.48)
T,0.7469262295081968,"B.5
Proof of Theorem 7"
T,0.7479508196721312,"Proof. Recall the proof technique that gives equation (B.6) and (B.14) in Theorem 1. Following the same
routine, we arrive at:"
T,0.7489754098360656,"T −1
X"
T,0.75,"t=0
⟨∇Fit(xt), xt −x⋆⟩≤τ"
T,0.7510245901639344,"v
u
u
t"
T,0.7520491803278688,"T −1
X"
T,0.7530737704918032,"t=0
Fit(xt) −F ⋆
it .
(B.49)"
T,0.7540983606557377,"where τ = (2cp(L + µF )D2 +
1
cp ) for AdaSVRPS and τ = max
n
L+µF
(1−ρ)√ρ,
1
γmax√ρ
o
clD2 +
1
cl
√ρ for"
T,0.7551229508196722,"AdaSVRLS. The difference is due to the fact that Fit(x) is (L + µF )-smooth. Taking the expectation, using the
fact that E[∇Fit(xt)] = E[∇fit(xt) + ∇f(wt) −∇fit(wt)] = ∇f(xt) and applying Lemma 18, we end up
with:"
T,0.7561475409836066,"T −1
X"
T,0.757172131147541,"t=0
E[f(xt) −f ⋆]
(B.48)
≤τ"
T,0.7581967213114754,"v
u
u
t"
T,0.7592213114754098,"T −1
X"
T,0.7602459016393442,"t=0
E[f(xt) −f ⋆] +
1
2µF"
T,0.7612704918032787,"T −1
X"
T,0.7622950819672131,"t=0
E[||∇fit(wt) −∇fit(x⋆)||2] .
(B.50)"
T,0.7633196721311475,"Taking the square gives:
 T −1
X"
T,0.764344262295082,"t=0
E[f(xt) −f ⋆]"
T,0.7653688524590164,"!2
≤τ 2
 T −1
X"
T,0.7663934426229508,"t=0
E[f(xt) −f ⋆] +
1
2µF"
T,0.7674180327868853,"T −1
X"
T,0.7684426229508197,"t=0
E[||∇fit(wt) −∇fit(x⋆)||2] !"
T,0.7694672131147541,".
(B.51)"
T,0.7704918032786885,"Define the Lyapunov function: Zt+1 =
1
2(1−a)
τ2"
T,0.7715163934426229,pt+1µF ||∇fit+1(wt+1) −∇fit+1(x⋆)||2. It follows that:
T,0.7725409836065574,"E[Zt+1] =
1
2(1 −a)
τ 2"
T,0.7735655737704918,"pt+1µF E[||∇fit+1(wt+1) −∇fit+1(x⋆)||2] =
τ 2"
T,0.7745901639344263,2(1 −a)µF E[||∇fit+1(xt) −∇fit+1(x⋆)||2] + 1 −pt+1
T,0.7756147540983607,"2(1 −a)
τ 2"
T,0.7766393442622951,pt+1µF E[||∇fit(wt) −∇fit(x⋆)||2]
T,0.7776639344262295,"(A.5)
≤
τ 2"
T,0.7786885245901639,"2(1 −a)µF E[2L(fit+1(xt) −fit+1(x⋆) −⟨∇fit+1(x⋆), xt −x⋆⟩)] + (1 −pt+1)pt"
T,0.7797131147540983,"pt+1
E[Zt]"
T,0.7807377049180327,"=
L
(1 −a)µF τ 2(E[f(xt) −f ⋆]) + (1 −pt+1)pt"
T,0.7817622950819673,"pt+1
E[Zt] .
(B.52)"
T,0.7827868852459017,"Adding PT −1
t=0 E[Zt+1] to both sides of (B.51) and substituting the above upper bound, we get:
 T −1
X"
T,0.7838114754098361,"t=0
E[f(xt) −f ⋆] !2
+"
T,0.7848360655737705,"T −1
X"
T,0.7858606557377049,"t=0
E[Zt+1] ≤(1 +
L
(1 −a)µF )τ 2
T −1
X"
T,0.7868852459016393,"t=0
E[f(xt) −f ⋆] +"
T,0.7879098360655737,"T −1
X t=0"
T,0.7889344262295082,"
(1 −a)pt + (1 −pt+1)pt pt+1"
T,0.7899590163934426,"
E[Zt] ."
T,0.7909836065573771,(B.53)
T,0.7920081967213115,"Rearranging and dropping E[ZT ] ≥0 gives:
 T −1
X"
T,0.7930327868852459,"t=0
E[f(xt) −f ⋆]"
T,0.7940573770491803,"!2
≤(1 +
L
(1 −a)µF )τ 2
T −1
X"
T,0.7950819672131147,"t=0
E[f(xt) −f ⋆] +"
T,0.7961065573770492,"T −1
X t=1"
T,0.7971311475409836,"
(1 −a)pt + (1 −pt+1)pt"
T,0.798155737704918,"pt+1
−1

E[Zt]"
T,0.7991803278688525,"+
 
(1 −a)p0 + (1 −p1)p0 p1"
T,0.8002049180327869,"
E[Z0] ."
T,0.8012295081967213,(B.54)
T,0.8022540983606558,"By our choice of pt, we have:"
T,0.8032786885245902,(1 −a)pt + (1 −pt+1)pt
T,0.8043032786885246,"pt+1
−1 =
pt
pt+1 −apt −1 = at + a + 1"
T,0.805327868852459,"at + 1
−
a
(at + 1) −1 =
0
2(at + 1) = 0 . (B.55)"
T,0.8063524590163934,"Therefore, it holds that:
 T −1
X"
T,0.8073770491803278,"t=0
E[f(xt) −f ⋆]"
T,0.8084016393442623,"!2
≤(1 +
L
(1 −a)µF )τ 2
T −1
X"
T,0.8094262295081968,"t=0
E[f(xt) −f ⋆] + E[Z0] .
(B.56)"
T,0.8104508196721312,"Further, by L-smoothness and convexity of f, we have"
T,0.8114754098360656,"E[Z0] =
1
2(1 −a)
τ 2"
T,0.8125,"p0µF E[||∇fi0(x0) −∇fi0(x⋆)||2]
(A.5)
≤
Lτ 2"
T,0.8135245901639344,"(1 −a)µF (f(x0) −f ⋆) .
(B.57)"
T,0.8145491803278688,"Hence. we obtain:
 T −1
X"
T,0.8155737704918032,"t=0
E[f(xt) −f ⋆]"
T,0.8165983606557377,"!2
≤(1 +
2L
(1 −a)µF )τ 2
T −1
X"
T,0.8176229508196722,"t=0
E[f(xt) −f ⋆].
(B.58)"
T,0.8186475409836066,"It follows that:
T −1
X"
T,0.819672131147541,"t=0
E[f(xt) −f ⋆] ≤(1 +
2L
(1 −a)µF )τ 2.
(B.59)"
T,0.8206967213114754,Dividing both sides by T and applying Jensen’s inequality concludes the proof.
T,0.8217213114754098,"B.6
Proof of Corollary 8"
T,0.8227459016393442,"Proof. Algorithm 1 calls the stochastic gradient oracle in expectation O(1 + ptn) times at iteration t. Therefore,
the total number of gradient evaluations is upper bounded by O(PT −1
t=0 ptn + T). By our choice of pt, it holds
that PT −1
t=0 pt ≤1"
T,0.8237704918032787,"a
PT −1
t=0
1
t+2 ≤1"
T,0.8247950819672131,a(log(T) + 1 −1) = 1
T,0.8258196721311475,"a log(T). Due to the sublinear convergence rate of
Algorithm 1, we conclude that the total number of stochastic gradient calls is O(log(1/ε)n + 1/ε)."
T,0.826844262295082,"C
Pseudo-codes for AdaSPS and AdaSLS"
T,0.8278688524590164,"In this section, we provide formal pseudo codes for AdaSPS (AdaSPS) and AdaSLS (AdaSLS)."
T,0.8288934426229508,"To implement AdaSPS, a lower bound of optimal function value for each minibatch function is required. For
machine learning problems where the individual loss functions are non-negative, we can use zero as an input.
Apart from that, we need to provide a constant cp to adjust the magnitude of the stepsize. Theoretically suggested"
T,0.8299180327868853,"cp for robust convergence satisfies cscale
p
= cp
q"
T,0.8309426229508197,"fi0(x0) −ℓ⋆
i0 ≥
1
2. Therefore, a common choice is to set"
T,0.8319672131147541,"cp =
1
q"
T,0.8329918032786885,"fi0 (x0)−ℓ⋆
i0
."
T,0.8340163934426229,Algorithm 2 AdaSPS
T,0.8350409836065574,"Require: x0 ∈Rd, T ∈N+, cp > 0"
T,0.8360655737704918,"1: set η−1 = +∞
2: set ε = 10−10"
T,0.8370901639344263,"3: for t = 0 to T −1 do
4:
uniformly sample it ⊆[n]
5:
provide a lower bound ℓ⋆
it ≤f ⋆
it"
T,0.8381147540983607,"6:
set ηt = min

fit(xt)−ℓ⋆
it
cp||∇fit(xt)||2
1
√Pt
s=0 fis(xs)−ℓ⋆
is+ε, ηt−1 "
T,0.8391393442622951,"7:
xt+1 = ΠX (xt −ηt∇fit(xt))
return ¯xT = 1"
T,0.8401639344262295,"T
PT −1
t=0 xt"
T,0.8411885245901639,"To implement AdaSLS (AdaSLS), a line-search sub-solver 4 and an input constant cl > 0 are required. Similar
to (AdaSPS), we can set cl =
1
ρ√"
T,0.8422131147540983,γ0||∇fi0 (x0)||2 according to the theory.
T,0.8432377049180327,Algorithm 3 AdaSLS
T,0.8442622950819673,"Require: x0 ∈Rd, T ∈N+, cl > 0"
T,0.8452868852459017,"1: set η−1 = +∞
2: set ε = 10−10"
T,0.8463114754098361,"3: for t = 0 to T −1 do
4:
uniformly sample it ⊆[n]
5:
obtain γt via backtracking line-search (4)"
T,0.8473360655737705,"6:
set ηt = min

γt
cl√Pt
s=0 γs||∇fis(xs)||2+ε, ηt−1 "
T,0.8483606557377049,"7:
xt+1 = ΠX (xt −ηt∇fit(xt))
return ¯xT = 1"
T,0.8493852459016393,"T
PT −1
t=0 xt"
T,0.8504098360655737,"D
Line-search procedure"
T,0.8514344262295082,"In this section, we introduce the classical Armijo line-search method [3, 41]. Given a function fit(x), the Armijo
line-search returns a stepsize γt that satisfies the following condition:"
T,0.8524590163934426,"fit(xt −γt∇fit(xt)) ≤fit(xt) −ργt||∇fit(xt)||2 ,
(D.1)"
T,0.8534836065573771,"where ρ ∈(0, 1) is an input hyper-parameter. If fit(x) is a smooth function, then backtracking line-search 4 is
a practical implementation way to ensure that D.1 is satisfied."
T,0.8545081967213115,Algorithm 4 Backtracking line-search
T,0.8555327868852459,Require: β ∈[ 1
T,0.8565573770491803,"2, 1), ρ ∈(0, 1), γmax > 0 (We fix β = 0.8 and ρ = 0.5 for AdaSLS)
1: γ = γmax
2: while fit(xt −γ∇fit(xt)) > fit(xt) −ργ||∇fit(xt)||2 do
3:
γ = βγ
return γt = γ"
T,0.8575819672131147,"To implement Algorithm 4, one needs to provide the decreasing factor β, the maximum stepsize γmax, and
the condition parameter ρ. Starting from γmax, Algorithm 4 decreases the stepsize iteratively by a constant
factor β until the condition D.1 is satisfied. Note that checking the condition requires additional minibatch
function value evaluations. Fortunately, note that the output γ cannot be smaller than 1−ρ"
T,0.8586065573770492,"L
(Lemma 16), and thus"
T,0.8596311475409836,"the number of extra function value evaluations required is at most O

max{logLγmax/(1−ρ)
1/β
, 1}

. In practice,
Vaswani et al. [52] suggests dynamic initialization of γmax to reduce the algorithm’s running time, that is, setting
γmaxt = γt−1θ1/n where a common choice for θ is 2. This strategy initializes γmax by a slightly larger number
than the last output and thus is usually more efficient than keeping γmax constant or always using γmaxt = γt−1.
In all our experiments, we use the same γmax at each iteration for AdaSLS to show its theoretical properties."
T,0.860655737704918,"Goldstein line-search is another line-search method that checks D.1 and an additional curvature condition [41].
We do not study this method in this work and we refer to [52] for more details."
T,0.8616803278688525,"E
Counter examples of SPS and its variants for SVRG"
T,0.8627049180327869,"We provide two simple counterexamples where SVRG with the SPS stepsize and its intuitive variants fail to
converge. For simplicity, consider the update rule xt+1 = xt −ηt∇f(xt), i.e. wt = xt for all t ≥0. Consider
the function f(x) = f1(x)+f2(x)"
T,0.8637295081967213,"2
where f1(x) = a1(x −1)2 and f2(x) = a2(x + 1)2 with a1, a2 > 0."
T,0.8647540983606558,"Example 19. Individual curvature is not representative. Consider the standard stochastic Polyak stepsize:
ηt =
fi(xt)−f⋆
i
||∇fi(xt)||2 where i is randomly chosen from {1, 2}. We now let a1 = 1 and a2 < 1. Note that
∇2f(x) = a1 + a2 ∈(1, 2) while Ei[ηt] = 1"
T,0.8657786885245902,"8 +
1
8a2 →+∞as a2 →0, which leads to divergence. The
reason behind this is that individual curvature does not match the global curvature."
T,0.8668032786885246,"Example 20.
Mismatching quantity.
Consider a variant of stochastic Polyak stepsize:
ηt
=
fi(xt)−f⋆
i
||∇fi(xt)−∇fi(wt)+∇f(wt)||2 where i is randomly chosen from {1, 2}.
Let a1 = a2 = 1.
We note"
T,0.867827868852459,"Ei[ηt∇f(xt)] =
x2
t +1
2xt
̸= 0 and thus no stationary point exists. Similar reasoning can exclude a number"
T,0.8688524590163934,"of other variants such as: ηt =
fi(xt)−fi(wt)+f(wt)−f⋆
i
||∇fi(xt)−∇fi(wt)+∇f(wt)||2 . Indeed, the numerator is not the proper function
value difference of a valid function with the gradient defined in the denominator."
T,0.8698770491803278,"F
Experimental details and additional experiment results"
T,0.8709016393442623,"In this section, we provide a detailed setup of the experiments presented in the main paper."
T,0.8719262295081968,"In practice, we can use a lower bound of F ⋆
it for running AdaSVRPS since convergence is still guaranteed thanks
to the property of AdaSPS. By default, we use ℓ⋆
it + minx{xT (∇f(wt) −∇fit(wt)) + µF"
T,0.8729508196721312,"2 ||x −xt||2} for
all the experiments, where ℓ⋆
it is a lower bound for f ⋆
it."
T,0.8739754098360656,"F.1
Synthetic experiment"
T,0.875,"We consider the minimization of a quadratic of the form: f(x) =
1
n
Pn
i=1 fi(x) where fi(x) =
1
2(x −
bi)T Ai(x −bi), bi ∈Rd and Ai ∈Rd×d is a diagonal matrix. We use n = 50, d = 1000. We control the
interpolation by either setting all {bi} to be identical or different. Each component of {bi} is generated according"
T,0.8760245901639344,"to N(0, 102). We control the complexity of the problems by choosing different matrices Ai. For the strongly-"
T,0.8770491803278688,"convex case, we first generate a matrix AN = clip( "
T,0.8780737704918032,"
a11
...
a1d
...
an1
...
and "
T,0.8790983606557377,", 1, 10) where each aij ∼N(0, 152)"
T,0.8801229508196722,"and the clipping operator clips the elements to the interval between 1 and 10. Then we compute: A =  
"
T,0.8811475409836066,"1
1
...
n
Pn
i=1 AN
i(d−1)"
"N
PN",0.882172131147541,"10n
Pn
i=1 AN
id
...
...
...
1
1
...
n
Pn
i=1 AN
i(d−1)"
"N
PN",0.8831967213114754,"10n
Pn
i=1 AN
id "
"N
PN",0.8842213114754098,"

K
AN ,"
"N
PN",0.8852459016393442,"where J denotes the Hadamard product. We set the diagonal elements of each Ai using the corresponding row
stored in the matrix A. Note that ∇2f(x) = 1"
"N
PN",0.8862704918032787,"n
Pn
i=1 Ai has the minimum and the largest eigenvalues being
1 and 10. For the general convex case, we use the same matrix AN to generate a sparse matrix AM such that
AM = AN
J M where M is a mask matrix with Mij ∼B(1, p) and
 1
...
1
· M:j ≥1, ∀j ∈[1, d].
We then compute the matrix A and set each Ai in the same way. A =  
"
"N
PN",0.8872950819672131,"2−20n
Pn
i=1 AM
i1"
"N
PN",0.8883196721311475,"2−19n
Pn
i=1 AM
i2
...
2−1n
Pn
i=1 AM
i20
1
...
1
10n
Pn
i=1 AM
id
...
...
...
...
2−20n
Pn
i=1 AM
i1"
"N
PN",0.889344262295082,"2−19n
Pn
i=1 AM
i2
...
2−1n
Pn
i=1 AM
i20
1
...
1
10n
Pn
i=1 AM
id "
"N
PN",0.8903688524590164,"

K
AM ."
"N
PN",0.8913934426229508,"Through the construction, the smallest eigenvalues of ∇2f(x) are clustered around zero, and the largest
eigenvalue is 10. Additionally, each ∇2fi(x) is positive semi-definite."
"N
PN",0.8924180327868853,"We set the batch size to be 1 and thus we have f ⋆
it = 0 with interpolation and ℓ⋆
it = 0 without interpolation."
"N
PN",0.8934426229508197,"For AdaSPS/AdaSVRPS we fix cscale
p
= 1, and for AdaSVRPS we further use µF = 10 and pt =
1
0.1t+1. We
compare against DecSPS [44], SPS [34] and SVRG [22] and tune the stepsize for SVRG by picking the best one
from {10i}i=−4,..,3."
"N
PN",0.8944672131147541,"In addition to these optimizers, we further evaluate the performance of SLS [52], AdaSLS, SGD, SPSmax [34]
and AdaSVRLS. We fix cscale
l
= 1, γmax = 10, β = 0.8 and ρ = 1/2 for both algorithms and for AdaSVRLS,
we further use µF = 10 and pt =
1
0.1t+1. For SGD, we use the best learning rate schedules in different scenarios.
Specifically, for both interpolation problems, we keep the stepsize constant and for non-interpolation problems,
we apply Θ(1/
√"
"N
PN",0.8954918032786885,"t) and Θ(1/t) decay schedules for convex and strongly-convex problems respectively. We
further pick the best stepsize from {10i}i=−4,...,3. For SPSmax, we use γb = 10−3 and we only showcase
its performance in non-interpolated settings. We report the results in Figure F.1. We observe that AdaSLS is
comparable if no faster than the best-tuned vanilla SGD. SPSmax is reduced to the vanilla SGD with constant
stepsize. AdaSVRLS provides similar performance to AdaSVRPS but due to the cost of additional function
evaluations, it is less competitive than AdaSVRPS."
"N
PN",0.8965163934426229,"0
1000
2000
# mini-batch gradient evaluations 10
8 10
5 10
2 101 104 107"
"N
PN",0.8975409836065574,f(x)-f
"N
PN",0.8985655737704918,strongly-convex+interpolation
"N
PN",0.8995901639344263,"0
2000
4000
# mini-batch gradient evaluations 10
8 10
5 10
2 101 104 107"
"N
PN",0.9006147540983607,f(x)-f
"N
PN",0.9016393442622951,strongly-convex+non-interpolation
"N
PN",0.9026639344262295,"0
2500
5000
7500
10000
# mini-batch gradient evaluations 100 102 104 106"
"N
PN",0.9036885245901639,f(x)-f
"N
PN",0.9047131147540983,convex+interpolation
"N
PN",0.9057377049180327,"0
10000
20000
# mini-batch gradient evaluations 10
2 100 102 104 106"
"N
PN",0.9067622950819673,f(x)-f
"N
PN",0.9077868852459017,convex+non-interpolation
"N
PN",0.9088114754098361,"0
1000
2000
# mini-batch gradient evaluations 10
6 10
5 10
4 10
3 10
2 10
1"
"N
PN",0.9098360655737705,Step size
"N
PN",0.9108606557377049,"0
2000
4000
# mini-batch gradient evaluations 10
3 10
2 10
1 100"
"N
PN",0.9118852459016393,Step size
"N
PN",0.9129098360655737,"0
2500
5000
7500
10000
# mini-batch gradient evaluations 10
3 10
2 10
1 100"
"N
PN",0.9139344262295082,Step size
"N
PN",0.9149590163934426,"0
10000
20000
# mini-batch gradient evaluations 10
3 10
2 10
1"
"N
PN",0.9159836065573771,Step size
"N
PN",0.9170081967213115,"AdaSVRPS
AdaSVRLS
SVRG
SPS
AdaSLS
SPS_max
AdaSPS
SGD
DecSPS
SLS"
"N
PN",0.9180327868852459,"Figure F.1: Comparison of the considered optimizers on synthetic data set with quadratic loss. The left block
of the label illustrates the variance-reduced methods and the right represents SGD with different stepsizes.
(Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)"
"N
PN",0.9190573770491803,"optimizers
hyper-parameters used for synthetic experiments"
"N
PN",0.9200819672131147,"st-convex+ip
st-convex+non-ip
convex+ip
convex+non-ip"
"N
PN",0.9211065573770492,"AdaSPS
cscale
p
= 1
AdaSLS
cscale
l
= 1, β = 0.8, ρ = 0.5, γmax = 10
SPS
c = 0.5
SPSmax
c = 0.5, γb = 10−3
SLS
ρ = 0.1, β = 0.9, γmax = 10
DecSPS
c0 = 1, γb = 10
SGD
constant, η = 10−2
O(1/t), η = 1
constant, η = 10−2
O(1/
√"
"N
PN",0.9221311475409836,"t), η = 10−1"
"N
PN",0.923155737704918,"AdaSVRPS
cscale
p
= 1, µF = 10, pt =
1
0.1t+1
AdaSVRLS
cscale
l
= 1, β = 0.8, ρ = 0.5, γmax = 0.1, µF = 10, pt =
1
0.1t+1
SVRG
η = 10−2"
"N
PN",0.9241803278688525,"Table F.1: Hyper-parameters of the considered optimizers used in synthetic experiments. st-convex stands for
strongly-convex and ip stands for interpolation."
"N
PN",0.9252049180327869,"F.2
Binary classification"
"N
PN",0.9262295081967213,"Following the binary classification experiments presented in the main paper, we provide additional experiments
for VR algorithms. The chosen hyper-parameters for each algorithm can be found in Table F.3. In particular,
we fix cscale
l
= 1, γmax = 103, β = 0.8 and ρ = 1/2 for AdaSLS and AdaSVRLS. We report the best
µF ∈{10−4, 102}. In Figure F.2, we observe that AdaSVRLS/AdaSVRPS provides similar performance to the
other two variance-reduction methods. The details of the four considered datasets are summarized in Table F.2."
"N
PN",0.9272540983606558,"We next investigate the impact of the probability schedule on the convergence behaviours. We pick w8a as the
dataset and run AdaSVRPS (Alg. 1) with and without probability decay. Specficially, we set pt = B/n and
pt =
1
0.1t+1 to separate the cases. We control the level of the interpolation by using B = 32 and B = 128 since
σf,128 ≤σf,32. From Figure F.3, we observe that decreasing probability schedule is more efficient when the
problem is more non-interpolated. This is because for interpolated problems, the frequent computation of the
full gradients at the beginning provides no additional convergence benefits."
"N
PN",0.9282786885245902,"duke
rcv1
ijcnn
w8a"
"N
PN",0.9293032786885246,"n
44
20242
49990
49749
d
7129
47236
22
300
B
1
64
64
128"
"N
PN",0.930327868852459,"Table F.2: Number of datapoints, dimension of features, used batch size of four datasets from LIBSVM [10]"
"N
PN",0.9313524590163934,"0
500
1000
1500
# mini-batch gradient evaluations 10
3 10
2 10
1 100 101"
"N
PN",0.9323770491803278,Gradient Norm duke
"N
PN",0.9334016393442623,"0
5000
10000
15000
# mini-batch gradient evaluations 10
7 10
5 10
3"
"N
PN",0.9344262295081968,Gradient Norm rcv1
"N
PN",0.9354508196721312,"0
10000
20000
30000
# mini-batch gradient evaluations 10
7 10
5 10
3 10
1"
"N
PN",0.9364754098360656,Gradient Norm ijcnn
"N
PN",0.9375,"0
10000
20000
30000
# mini-batch gradient evaluations 10
5 10
4 10
3 10
2 10
1 100"
"N
PN",0.9385245901639344,Gradient Norm w8a
"N
PN",0.9395491803278688,"0
500
1000
1500
# mini-batch gradient evaluations 10
4 10
2 100 102"
"N
PN",0.9405737704918032,Step size
"N
PN",0.9415983606557377,"0
5000
10000
15000
# mini-batch gradient evaluations 100 101 102 103 104 105"
"N
PN",0.9426229508196722,Step size
"N
PN",0.9436475409836066,"0
10000
20000
30000
# mini-batch gradient evaluations 10
1 100 101 102 103"
"N
PN",0.944672131147541,Step size
"N
PN",0.9456967213114754,"0
10000
20000
30000
# mini-batch gradient evaluations 10
1 101 103 105"
"N
PN",0.9467213114754098,Step size
"N
PN",0.9477459016393442,"SVRG
AdaSVRLS
AdaSVRPS
AdaSVRG
DecSPS
SPS
SGD
SLS
AdaSPS
AdaSLS
AdaNorm"
"N
PN",0.9487704918032787,"Figure F.2: Comparison of the considered optimizers on four LIBSVM datasets with regularized logistic loss.
The left block of the label illustrates the variance-reduced methods and the right represents SGD with different
stepsizes. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)"
"N
PN",0.9497950819672131,"0
100000
200000
300000
400000
# mini-batch gradient evaluations 10
6 10
5 10
4 10
3 10
2 10
1 100"
"N
PN",0.9508196721311475,Gradient Norm
"N
PN",0.951844262295082,w8a (B=32)
"N
PN",0.9528688524590164,"0
10000 20000 30000 40000 50000
# mini-batch gradient evaluations 10
5 10
4 10
3 10
2 10
1 100"
"N
PN",0.9538934426229508,Gradient Norm
"N
PN",0.9549180327868853,w8a (B=128)
"N
PN",0.9559426229508197,"AdaSVRPS with decreasing p
AdaSVRPS with constant p"
"N
PN",0.9569672131147541,"Figure F.3: Comparison of different probability schedules for AdaSVRPS on the w8a dataset with regularized
logistic loss. Decreasing probability is more efficient when optimizing highly non-interpolated convex problems
(Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)"
"N
PN",0.9579918032786885,"optimizers
hyper-parameters used for binary classification tasks"
"N
PN",0.9590163934426229,"duke
rcv1
ijcnn
w8a
for all"
"N
PN",0.9600409836065574,"AdaSPS
cscale
p
= 1
AdaSLS
cscale
l
= 1, γmax = 10
cscale
l
= 1, γmax = 103
cscale
l
= 1, γmax = 103
cscale
l
= 1, γmax = 103
β = 0.8, ρ = 0.5
SPS
c = 0.5
SLS
γmax = 10
γmax = 103
γmax = 103
γmax = 103
β = 0.9, ρ = 0.1
DecSPS
γb = 200
γb = 100
γb = 100
γb = 100
c0 = 1
SGD
constant, η = 10−1
Θ(1/
√"
"N
PN",0.9610655737704918,"t), η = 100
Θ(1/
√"
"N
PN",0.9620901639344263,"t), η = 100
Θ(1/
√"
"N
PN",0.9631147540983607,"t), η = 100
AdaNorm
cg = 1
cg = 10
cg = 10
cg = 10
b0 = 10−10"
"N
PN",0.9641393442622951,"Adam
lr = 10−3
lr = 10−2
lr = 10−2
lr = 10−2
β1 = 0.9, β2 = 0.999
AdaSVRPS
cscale
p
= 1, µF = 100
cscale
p
= 1, µF = 10−4
cscale
p
= 1, µF = 10−4
cscale
p
= 1, µF = 10−4
pt = B"
"N
PN",0.9651639344262295,"N
AdaSVRLSa
cscale
l
= 1, µF = 100
cscale
l
= 1, µF = 10−4
cscale
l
= 1, µF = 10−4
cscale
l
= 1, µF = 10−4
β = 0.8, ρ = 0.5, pt = B"
"N
PN",0.9661885245901639,"N
SVRG
η = 10−2
η = 100
η = 10
η = 10
AdaSVRG
We use the heuristic method provided in Section 5 from [14]."
"N
PN",0.9672131147540983,"aγmax =
1
µF"
"N
PN",0.9682377049180327,Table F.3: Hyper-parameters of the considered optimizers used in binary classification.
"N
PN",0.9692622950819673,"G
Deep learning task"
"N
PN",0.9702868852459017,"In this section, we provide a heuristic extension of AdaSPS to over-parameterized non-convex optimization tasks.
When training modern deep learning models, Loshchilov and Hutter [35] observe that a cyclic behaviour of the
stepsize, i.e., increasing at the beginning and then decreasing up to a constant, can help with fast training and
good generalization performance. Since AdaSPS is a non-increasing stepsize, it excludes such a cyclic behaviour.
To address this issue, we provide a non-convex version of AdaSPS which incorporates a restart mechanism
that allows an increase of the stepsize according to the local curvature. The full algorithm is summarized in
Algorithm 5. In practice, we can set u = B"
"N
PN",0.9713114754098361,"n . Algorithm 5 updates the stepsize and cp at the beginning of each
epoch and uses AdaSPS (AdaSPS) for the rest of the epoch."
"N
PN",0.9723360655737705,"Following [34, 52], we benchmark the convergence and generalization performance of AdaSPS (DL) 5 for
the multi-class classification tasks on CIFAR10 [28] and CIFAR100 [29] datasets using ResNet-34 [21]. We
compare against SPS [34], Adam [26], AdaGrad [15], DecSPS [44] and SGD with momentum. We use the
smoothing technique and pick c = 0.02 for SPS as suggested in [34]. We use the official implementations
of Adam, AdaGrad, and SGD with momentum from https://pytorch.org/docs/stable/optim.html. We choose
lr = 10−3, β1 = 0.9 and β2 = 0.999 for Adam. We choose lr = 0.01 for AdaGrad. We choose lr = 0.01 and
β = 0.9 for SGD with momentum. Finally, we pick cscale
p
= 0.02 for Algorithm 5. In Figure G.1, AdaSPS (DL)
shows competitive performance on both datasets. We leave the study of its theoretical properties to future work."
"N
PN",0.9733606557377049,"0
50
100
150
200
Epoch 0.75 0.80 0.85 0.90 0.95"
"N
PN",0.9743852459016393,Test Accuary
"N
PN",0.9754098360655737,"0
50
100
150
200
Epoch 10
3 10
2 10
1 100"
"N
PN",0.9764344262295082,Train loss
"N
PN",0.9774590163934426,CIFAR10-ResNet34
"N
PN",0.9784836065573771,"0
50
100
150
200
Epoch 10
5 10
4 10
3 10
2 10
1"
"N
PN",0.9795081967213115,Step size
"N
PN",0.9805327868852459,"0
50
100
150
200
Epoch 0.50 0.55 0.60 0.65 0.70 0.75"
"N
PN",0.9815573770491803,Test Accuary
"N
PN",0.9825819672131147,"0
50
100
150
200
Epoch 10
3 10
2 10
1 100"
"N
PN",0.9836065573770492,Train loss
"N
PN",0.9846311475409836,CIFAR100-ResNet34
"N
PN",0.985655737704918,"0
50
100
150
200
Epoch 10
4 10
2 100"
"N
PN",0.9866803278688525,Step size
"N
PN",0.9877049180327869,"AdaSPS (DL)
SGD-M
AdaGrad
DecSPS
SPS
Adam"
"N
PN",0.9887295081967213,"Figure G.1: Comparison of the considered optimizers on multi-class classification tasks with CIFAR10 and
CIFAR100 datasets using ResNet34 with softmax loss. AdaSPS (DL) 5 and SPS provide remarkable performance
on both datasets."
"N
PN",0.9897540983606558,"optimizers
hyper-parameters used for multi-classification tasks"
"N
PN",0.9907786885245902,"AdaSPS (DS)
cscale
p
= 0.2
SPS
c = 0.2 + smoothing technique [34]
DecSPS
c0 = 1, γb = 1000
SGD-M
lr = 0.01, β = 0.9
AdaGrad
lr = 0.01
Adam
lr = 10−3, β1 = 0.9, β2 = 0.999"
"N
PN",0.9918032786885246,Table G.1: Hyper-parameters of the considered optimizers used in multi-classification tasks.
"N
PN",0.992827868852459,Algorithm 5 AdaSPS (DL)
"N
PN",0.9938524590163934,"Require: x0 ∈Rd, T ∈N+, cscale
p
> 0, update frequency u ∈N+"
"N
PN",0.9948770491803278,"1: set η−1 = +∞
2: set ε = 10−10"
"N
PN",0.9959016393442623,"3: for t = 0 to T −1 do
4:
uniformly sample it ⊆[n]
5:
provide a lower bound ℓ⋆
it ≤f ⋆
it
6:
if t mod u is 0 then"
"N
PN",0.9969262295081968,"7:
set cp =
cscale
p
√Pt
s=0 fis(xs)−ℓ⋆
is
8:
set ηt =
fit(xt)−ℓ⋆
it
cp||∇fit(xt)||2
1
√Pt
s=0 fis(xs)−ℓ⋆
is+ε
9:
else"
"N
PN",0.9979508196721312,"10:
set ηt = min

fit(xt)−ℓ⋆
it
cp||∇fit(xt)||2
1
√Pt
s=0 fis(xs)−ℓ⋆
is+ε, ηt−1 "
"N
PN",0.9989754098360656,"11:
xt+1 = xt −ηt∇fit(xt)
return xT"
