Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0011376564277588168,"We propose an algebraic geometric framework to study the expressivity of linear
1"
ABSTRACT,0.0022753128555176336,"activation neural networks. A particular quantity that has been actively studied in
2"
ABSTRACT,0.0034129692832764505,"the field of deep learning is the number of linear regions, which gives an estimate
3"
ABSTRACT,0.004550625711035267,"of the information capacity of the architecture. To study and evaluate information
4"
ABSTRACT,0.005688282138794084,"capacity and expressivity, we work in the setting of tropical geometry—a com-
5"
ABSTRACT,0.006825938566552901,"binatorial and polyhedral variant of algebraic geometry—where there are known
6"
ABSTRACT,0.007963594994311717,"connections between tropical rational maps and feedforward neural networks. Our
7"
ABSTRACT,0.009101251422070534,"work builds on and expands this connection to capitalize on the rich theory of
8"
ABSTRACT,0.010238907849829351,"tropical geometry to characterize and study various architectural aspects of neural
9"
ABSTRACT,0.011376564277588168,"networks. Our contributions are threefold: we provide a novel tropical geometric
10"
ABSTRACT,0.012514220705346985,"approach to selecting sampling domains among linear regions; an algebraic result
11"
ABSTRACT,0.013651877133105802,"allowing for a guided restriction of the sampling domain for network architectures
12"
ABSTRACT,0.01478953356086462,"with symmetries; and an open source library to analyze neural networks as tropical
13"
ABSTRACT,0.015927189988623434,"Puiseux rational maps. We provide a comprehensive set of proof-of-concept nu-
14"
ABSTRACT,0.017064846416382253,"merical experiments demonstrating the breadth of neural network architectures to
15"
ABSTRACT,0.01820250284414107,"which tropical geometric theory can be applied to reveal insights on expressivity
16"
ABSTRACT,0.019340159271899887,"characteristics of a network. Our work provides the foundations for the adaptation
17"
ABSTRACT,0.020477815699658702,"of both theory and existing software from computational tropical geometry and
18"
ABSTRACT,0.02161547212741752,"symbolic computation to deep learning.
19"
INTRODUCTION,0.022753128555176336,"1
Introduction
20"
INTRODUCTION,0.023890784982935155,"Deep learning has become the undisputed state-of-the-art for data analysis and has wide-reaching
21"
INTRODUCTION,0.02502844141069397,"prominence in many fields of computer science, despite still being based on a limited theoretical
22"
INTRODUCTION,0.026166097838452786,"foundation. Developing theoretical foundations to better understand the unparalleled success of deep
23"
INTRODUCTION,0.027303754266211604,"neural networks is one of the most active areas of research in modern statistical learning theory.
24"
INTRODUCTION,0.02844141069397042,"Expressivity is one of the most important approaches to quantifiably measuring the performance of a
25"
INTRODUCTION,0.02957906712172924,"deep neural network—such as how they are able to represent highly complex information implicitly
26"
INTRODUCTION,0.030716723549488054,"in their weights and to generalize from data—and therefore key to understanding the success of deep
27"
INTRODUCTION,0.03185437997724687,"learning.
28"
INTRODUCTION,0.03299203640500569,"Tropical geometry is a reinterpretation of algebraic geometry that features piecewise linear and
29"
INTRODUCTION,0.034129692832764506,"polyhedral constructions, where combinatorics naturally comes into play [e.g., 1, 2, 3]. These
30"
INTRODUCTION,0.03526734926052332,"characteristics of tropical geometry make it a natural framework for studying the linear regions in a
31"
INTRODUCTION,0.03640500568828214,"neural network—an important quantity in deep learning representing the network information capacity
32"
INTRODUCTION,0.03754266211604096,"[4, 5, 6, 7, 8, 9, 10]. The intersection of deep learning theory and tropical geometry is a relatively
33"
INTRODUCTION,0.038680318543799774,"new area of research with great potential towards the ultimate goal of understanding how and why
34"
INTRODUCTION,0.03981797497155859,"deep neural networks perform so well. In this paper, we propose a new perspective for measuring
35"
INTRODUCTION,0.040955631399317405,"and estimating the expressivity and information capacity of a neural networks by developing and
36"
INTRODUCTION,0.04209328782707622,"expanding known connections between neural networks and tropical rational functions in both theory
37"
INTRODUCTION,0.04323094425483504,"and practice.
38"
INTRODUCTION,0.04436860068259386,"Related Work.
Tropical geometry has been used to characterize deep neural networks with piece-
39"
INTRODUCTION,0.04550625711035267,"wise linear activation functions, including two of the most popular and widely-used activation
40"
INTRODUCTION,0.04664391353811149,"functions, namely, rectified linear units (ReLUs) and maxout units. The first explicit connection
41"
INTRODUCTION,0.04778156996587031,"between tropical geometry and neural networks establishes that the decision boundary of a deep
42"
INTRODUCTION,0.048919226393629126,"neural network with ReLU activation functions is a tropical rational function [11]. Concurrently,
43"
INTRODUCTION,0.05005688282138794,"it was established that the maxout activation function fits input data by a tropical polynomial [12].
44"
INTRODUCTION,0.051194539249146756,"These works considered neural networks whose input domain is Euclidean, which was recently
45"
INTRODUCTION,0.05233219567690557,"developed to incorporate tropically-motivated input domains, in particular, the tropical projective
46"
INTRODUCTION,0.053469852104664393,"torus [13]. Most recently, tropical geometry has been used to construct convolutional neural networks
47"
INTRODUCTION,0.05460750853242321,"that are robust to adversarial attacks via tropical decision boundaries [14].
48"
INTRODUCTION,0.055745164960182024,"Contributions.
In this paper, we establish novel algebraic and geometric tools to quantify the
49"
INTRODUCTION,0.05688282138794084,"expressivity of a neural network. Networks with a piecewise linear activation compute piecewise
50"
INTRODUCTION,0.05802047781569966,"linear functions where the input space is divided into areas; the network computing a single linear
51"
INTRODUCTION,0.05915813424345848,"function on each area. These areas are referred to as the linear regions of the network; the number
52"
INTRODUCTION,0.06029579067121729,"of distinct linear regions is a quantifiable measure of expressivity of the network [e.g., 5]. In our
53"
INTRODUCTION,0.06143344709897611,"work, we not only study the number of linear regions, we aim to understand their geometry. The main
54"
INTRODUCTION,0.06257110352673492,"contributions of our work are the following.
55"
INTRODUCTION,0.06370875995449374,"• We provide a geometric characterization of the linear regions in a neural network via the
56"
INTRODUCTION,0.06484641638225255,"input space: estimating the linear regions is typically carried out by random sampling from
57"
INTRODUCTION,0.06598407281001138,"the input space, where randomness may cause some linear regions of a neural network to be
58"
INTRODUCTION,0.0671217292377702,"missed and result in an inaccurate information capacity measure. We propose an effective
59"
INTRODUCTION,0.06825938566552901,"sampling domain as a ball of radius R, which is a subset of the entire sampling space that
60"
INTRODUCTION,0.06939704209328783,"hits all of the linear regions of a given neural network. We compute bounds for the radius R
61"
INTRODUCTION,0.07053469852104664,"based on a combinatorial invariant known as the Hoffman constant, which effectively gives
62"
INTRODUCTION,0.07167235494880546,"a geometric characterization and guarantee for the linear regions of a neural network.
63"
INTRODUCTION,0.07281001137656427,"• We exploit geometric insight into the linear regions of a neural network to gain dramatic
64"
INTRODUCTION,0.07394766780432309,"computational efficiency: when networks exhibit invariance under symmetry, we can restrict
65"
INTRODUCTION,0.07508532423208192,"the sampling domain to a fundamental domain of the group action and thus reduce the
66"
INTRODUCTION,0.07622298065984073,"number of samples required. We experimentally demonstrate that sampling from the
67"
INTRODUCTION,0.07736063708759955,"fundamental domain provides an accurate estimate of the number of linear regions with a
68"
INTRODUCTION,0.07849829351535836,"fraction of the compute requirements.
69"
INTRODUCTION,0.07963594994311718,"• We provide an open source library integrated into the Open Source Computer Algebra
70"
INTRODUCTION,0.080773606370876,"Research (OSCAR) system [15] which converts both trained and untrained arbitrary neural
71"
INTRODUCTION,0.08191126279863481,"networks into algebraic symbolic objects. This contribution then opens the door for the
72"
INTRODUCTION,0.08304891922639362,"extensive theory and existing software on symbolic computation and computational tropical
73"
INTRODUCTION,0.08418657565415244,"geometry to be used to study neural networks.
74"
INTRODUCTION,0.08532423208191127,"The remainder of this paper is organized as follows. We provide an overview of the technical
75"
INTRODUCTION,0.08646188850967008,"background on tropical geometry and its connection to neural networks in Section 2. We then devote
76"
INTRODUCTION,0.0875995449374289,"a section to each of the contributions listed above—Sections 3, 4, and 5, respectively—in which we
77"
INTRODUCTION,0.08873720136518772,"present our theoretical contributions and numerical experiments. We close the paper with a discussion
78"
INTRODUCTION,0.08987485779294653,"on limitations of our work and directions for future research in Section 6.
79"
TECHNICAL BACKGROUND,0.09101251422070535,"2
Technical Background
80"
TECHNICAL BACKGROUND,0.09215017064846416,"In this section, we give basic definitions from tropical geometry required to write tropical expressions
81"
TECHNICAL BACKGROUND,0.09328782707622298,"for neural networks.
82"
TROPICAL POLYNOMIALS,0.09442548350398179,"2.1
Tropical Polynomials
83"
TROPICAL POLYNOMIALS,0.09556313993174062,"Algebraic geometry studies geometric properties of solution sets of polynomial systems that can
84"
TROPICAL POLYNOMIALS,0.09670079635949944,"be expressed algebraically, such as their degree, dimension, and irreducible components. Tropical
85"
TROPICAL POLYNOMIALS,0.09783845278725825,"geometry is a variant of algebraic geometry where the polynomials are defined in the tropical semiring,
86"
TROPICAL POLYNOMIALS,0.09897610921501707,"¯R = (R∪{∞}, ⊕, ⊙) where the addition and multiplication operators are given by a⊕b = max(a, b)
87"
TROPICAL POLYNOMIALS,0.10011376564277588,"and a ⊙b = a + b, respectively. Define a ⊘b := a −b.
88"
TROPICAL POLYNOMIALS,0.1012514220705347,"Using these operations, we can write polynomials as L
m amT m, where ai are coefficients, T ∈¯R,
89"
TROPICAL POLYNOMIALS,0.10238907849829351,"and where the sum is indexed by a finite subset of Nn. In our work, we consider the following
90"
TROPICAL POLYNOMIALS,0.10352673492605233,"generalizations of tropical polynomials.
91"
TROPICAL POLYNOMIALS,0.10466439135381114,"Definition 2.1. A tropical Puiseux polynomial in the indeterminates T1, . . . , Tn is a formal expression
92"
TROPICAL POLYNOMIALS,0.10580204778156997,"of the form L
m amT m where the index n runs through a finite subset of Qm
≥0 and T m = T m1
1
⊙
93"
TROPICAL POLYNOMIALS,0.10693970420932879,"· · · ⊙T mn
n
, and taking powers in the tropical sense.
94"
TROPICAL POLYNOMIALS,0.1080773606370876,"Definition 2.2. A tropical Puiseux rational map in T1, . . . , Tn is a tropical quotient of the form p ⊘q
95"
TROPICAL POLYNOMIALS,0.10921501706484642,"where p, q are tropical Puiseux polynomials.
96"
TROPICAL POLYNOMIALS,0.11035267349260523,"Tropical (Puiseux) polynomials and rational maps induce functions from Rn →R, which take a point
97"
TROPICAL POLYNOMIALS,0.11149032992036405,"x ∈Rn to the number obtained by substituting T = x in the algebraic expression and performing the
98"
TROPICAL POLYNOMIALS,0.11262798634812286,"(tropical) operations. It is important to note that tropically, the formal algebraic expression contains
99"
TROPICAL POLYNOMIALS,0.11376564277588168,"strictly more information than the corresponding function, since different tropical expressions can
100"
TROPICAL POLYNOMIALS,0.1149032992036405,"induce the same function.
101"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.11604095563139932,"2.2
Tropical Expressions for Neural Networks
102"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.11717861205915814,"We now overview and recast the framework of [11], which establishes the first explicit connection
103"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.11831626848691695,"between tropical geometry and neural networks, in a slightly different language for our results.
104"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.11945392491467577,"As in [11], the neural networks we will focus on are fully connected multilayer perceptrons with ReLU
105"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.12059158134243458,"activation, i.e., functions Rn →Rm of the form σ ◦Ld ◦σ ◦Li−1 ◦· · ·◦L1 where Li : Rni−1 →Rni
106"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.1217292377701934,"is an affine map and σ(t) = max{t, 0}. For the remainder of this paper, we use the term “neural
107"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.12286689419795221,"network” to refer solely to these. We will always assume that the weights and biases of our neural
108"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.12400455062571103,"networks are rational numbers. From a computational perspective, this is not a serious restriction
109"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.12514220705346984,"since this is sufficient to describe any neural network with weights and biases given by floating point
110"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.12627986348122866,"numbers. We refer to the tuple [n, n1, . . . , nd−1, m] as the architecture of the neural network.
111"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.12741751990898748,"One of the key observations intersecting tropical geometry and deep learning is that, up to rescaling
112"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.1285551763367463,"of rational weights to obtain integers, neural networks can be written as tropical rational functions
113"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.1296928327645051,"[11, Theorem 5.2]. From a more computational perspective, it is usually preferable to avoid such
114"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.13083048919226395,"rescaling and simply work with the original weights. The proof of Theorem 5.2 in [11] can directly
115"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.13196814562002276,"be adapted to show that any neural network can be written as the function associated to a tropical
116"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.13310580204778158,"Puiseux rational map. In their language, this corresponds to saying that any neural network is a
117"
TROPICAL EXPRESSIONS FOR NEURAL NETWORKS,0.1342434584755404,"tropical rational signomial with nonnegative rational exponents.
118"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.1353811149032992,"3
Sampling Domain Selection Using a Hoffman Constant
119"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.13651877133105803,"Estimating the number of linear regions of a neural network typically proceeds by sampling points
120"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.13765642775881684,"from the input domain and counting the memberships of these points. To guarantee that membership
121"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.13879408418657566,"is exhaustive, we seek a sampling domain as a sufficiently large ball so that all linear regions are
122"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.13993174061433447,"intersected. At the same time, we would like for the ball to be as small as possible to guarantee
123"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.1410693970420933,"efficient sampling. We are thus searching for the smallest ball from which we can sample in such a
124"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.1422070534698521,"way that all linear regions are intersected. Given the polyhedral geometry of tropical Puiseux rational
125"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.14334470989761092,"maps, it turns out that the radius of this smallest ball that we seek is closely related to the Hoffman
126"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.14448236632536973,"constant, which is a combinatorial invariant.
127"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.14562002275312855,"Our contribution in this section is a definition of a Hoffman constant of a neural network; we
128"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.14675767918088736,"demonstrate its relationship to the smallest sampling ball and propose algorithms to compute its true
129"
SAMPLING DOMAIN SELECTION USING A HOFFMAN CONSTANT,0.14789533560864618,"value and lower and upper bounds.
130"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.149032992036405,"3.1
Defining a Neural Network Hoffman Constant
131"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.15017064846416384,"In simpler terms, the Hoffman constant can be expressed for a matrix as follows. Let A be an m × n
132"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.15130830489192265,"matrix. For any b ∈Rm, let P(A, b) = {x ∈Rn : Ax ≤b} denote the polyhedron determined by
133"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.15244596131968147,"A and b. For a nonempty polyhedron P(A, b), let d(u, P(A, b)) = min{∥u −x∥: x ∈P(A, b)}
134"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.15358361774744028,"denote the distance from a point u ∈Rn to the polyhedron, measured under an arbitrary norm ∥· ∥
135"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1547212741751991,"on Rn. Then there exists a constant H(A) only depending on A such that
136"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1558589306029579,"d(u, PA,b) ≤H(A)∥(Au −b)+∥
(1)"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.15699658703071673,"where x+ = max{x, 0} is applied coordinate-wise [16]. The constant H(A) is called the Hoffman
137"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.15813424345847554,"constant of A.
138"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.15927189988623436,"The Hoffman Constant for Tropical Polynomials and Rational Functions.
Let f : Rn →R
139"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.16040955631399317,"be a tropical Puiseux polynomial and let U = {U1, . . . , Um} be the set of linear regions of f. Let
140"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.161547212741752,"f(x) = ai1x1 + . . . + ainxn + bi occur on the region Ui. Further, let A = [aij]m×n be the matrix of
141"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1626848691695108,"coefficients in the expression of f over U. The linear region Ui is defined by the following inequalities
142"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.16382252559726962,"ai1x1 + · · · + ainxn + bi ≥aj1x1 + · · · + ajnxn + bj,
∀j = 1, 2, · · · , m.
(2)"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.16496018202502843,"In matrix form, (2) is equivalent to
143"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.16609783845278725,"(A −1ai)x ≤bi1 −b
(3)"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.16723549488054607,"where 1 is a column vector of all 1’s; ai is the ith row vector of A; and b is a column vector of all
144"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.16837315130830488,"bi. Denote eAUi := A −1ai and ebUi := bi1 −b. Then the linear region Ui is captured by the linear
145"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1695108077360637,"system of inequalities eAUix ≤ebUi.
146"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17064846416382254,"Definition 3.1. Let f : Rn →R be a tropical Puiseux polynomial. The Hoffman constant of f is
147"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17178612059158135,"defined as
148"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17292377701934017,"H(f) = max
Ui∈U H( eAUi)."
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17406143344709898,"Care needs to be taken in defining a Hoffman constant for a tropical Puiseux rational map: We want
149"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1751990898748578,"to avoid having all linear regions defined by systems of linear inequalities, since there exist linear
150"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17633674630261661,"regions which are not convex. To do so, we consider convex refinements of linear regions induced by
151"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17747440273037543,"intersections of linear regions of tropical polynomials.
152"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17861205915813425,"Definition 3.2. Let p ⊘q be a difference of two tropical Puiseux polynomials. Let Ap (respectively
153"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.17974971558589306,"Aq) be the mp × n (respectively mq × n) matrix of coefficients for p (respectively q). The Hoffman
154"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.18088737201365188,"constant of p ⊘q is
155"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1820250284414107,"H(p ⊘q) := max

H
 
Ap
Aq"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1831626848691695,"
−1

aip
aiq"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.18430034129692832," 
: ip = 1, · · · , mp; iq = 1, · · · , mq"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.18543799772468714,"
.
(4)"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.18657565415244595,"Let f be a tropical Puiseux rational map. Then the Hoffman constant of f is defined as the minimal
156"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.18771331058020477,"Hoffman constant of H(p ⊘q) over all possible expressions of f = p ⊘q.
157"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.18885096700796358,"Given the correspondence between neural networks and tropical Puiseux rational maps, the Hoffman
158"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.1899886234357224,"constant is well-defined for any neural network and may be computed from the geometry and
159"
DEFINING A NEURAL NETWORK HOFFMAN CONSTANT,0.19112627986348124,"combinatorics of its linear regions.
160"
THE MINIMAL EFFECTIVE RADIUS,0.19226393629124006,"3.2
The Minimal Effective Radius
161"
THE MINIMAL EFFECTIVE RADIUS,0.19340159271899887,"For a neural network whose tropical Puiseux rational map is f : Rn →R, let U = {U1, . . . , Um} be
162"
THE MINIMAL EFFECTIVE RADIUS,0.1945392491467577,"the collection of all linear regions. For any x ∈Rn, define the minimal effective radius of f at x as
163"
THE MINIMAL EFFECTIVE RADIUS,0.1956769055745165,"Rf(x) := min{r : B(x, r) ∩Ui ̸= ∅, Ui ∈U}"
THE MINIMAL EFFECTIVE RADIUS,0.19681456200227532,"where B(x, r) is the ball of radius r centered at x. That is, Rf(x) is the minimal radius such that the
164"
THE MINIMAL EFFECTIVE RADIUS,0.19795221843003413,"ball B(x, r) intersects all linear regions. It is the smallest required radius of sampling around x in
165"
THE MINIMAL EFFECTIVE RADIUS,0.19908987485779295,"order to express the full classifying capacity of the neural network f.
166"
THE MINIMAL EFFECTIVE RADIUS,0.20022753128555176,"We start with the following lemma which relates the minimal effective radius to the Hoffman constant
167"
THE MINIMAL EFFECTIVE RADIUS,0.20136518771331058,"when f is a tropical Puiseux polynomial.
168"
THE MINIMAL EFFECTIVE RADIUS,0.2025028441410694,"Lemma 3.3. Let f be a tropical Puiseux polynomial and x ∈Rn be any point, then
169"
THE MINIMAL EFFECTIVE RADIUS,0.2036405005688282,"Rf(x) ≤H(f) max
Ui∈U ∥( eAUix −ebUi)+∥.
(5)"
THE MINIMAL EFFECTIVE RADIUS,0.20477815699658702,"In particular, we are interested in studying when Rm and Rn are equipped with the ∞-norm. In
170"
THE MINIMAL EFFECTIVE RADIUS,0.20591581342434584,"this case, the minimal effective radius can be related to the Hoffman constant and function value
171"
THE MINIMAL EFFECTIVE RADIUS,0.20705346985210465,"of f = p ⊘q. For a tropical Puiseux polynomial p(x) = max1≤i≤mp{aix + bi}, let ˇp(x) =
172"
THE MINIMAL EFFECTIVE RADIUS,0.20819112627986347,"min1≤j≤mq{ajx + bj} be its min-conjugate.
173"
THE MINIMAL EFFECTIVE RADIUS,0.20932878270762229,"Proposition 3.4. Let f = p ⊘q be a tropical Puiseux rational map. For any x ∈Rn, we have
174"
THE MINIMAL EFFECTIVE RADIUS,0.21046643913538113,"Rf(x) ≤H(p ⊘q) max{p(x) −ˇp(x), q(x) −ˇq(x)}.
(6)"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.21160409556313994,"3.3
Computing and Estimating Hoffman Constants
175"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.21274175199089876,"The PVZ Algorithm.
In [17], the authors proposed a combinatorial algorithm to compute the
176"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.21387940841865757,"precise value of the Hoffman constant for a matrix A ∈Rm×n, which we refer to as the Peña–Vera–
177"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2150170648464164,"Zuluaga (PVZ) algorithm and sketch its main steps here.
178"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2161547212741752,"Definition 3.5. A set-valued map Φ : Rn →Rm assigns a set Φ(x) ⊆Rm. The map is surjective
179"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.21729237770193402,"if Φ(Rn) = ∪xΦ(x) = Rm. Let A ∈Rm×n. For any J ⊆{1, 2, . . . , m}, let AJ be the submatrix
180"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.21843003412969283,"of A consisting of rows with indices in J. The set J is called A-surjective if the set-valued map
181"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.21956769055745165,"Φ(x) = AJx + {y ∈RJ : y ≥0} is surjective.
182"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.22070534698521047,"Notice that A-surjectivity is a generalization of linear independence of row vectors. We illustrate this
183"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.22184300341296928,"observation in the following two examples.
184"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2229806598407281,"Example 3.6. If J is such that AJ is full-rank, then J is A-surjective, since for any y ∈RJ, there
185"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2241183162684869,"exists x ∈Rn such that y = AJx.
186"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.22525597269624573,"Example 3.7. Let A = 1m×n be the m × n matrix whose entries are 1’s. For any subset J of
187"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.22639362912400454,"{1, . . . , m} and for any y ∈RJ, let x ∈Rn such that P"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.22753128555176336,"i xi ≤min{yj, j ∈J}. Then y −AJx ≥0.
188"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.22866894197952217,"Thus any J is A-surjective.
189"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.229806598407281,"The PVZ algorithm is based on the following characterization of Hoffman constant.
190"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.23094425483503983,"Proposition 3.8. [17, Proposition 2] Let A ∈Rm×n. Equip Rm and Rn with norm ∥· ∥and denote
191"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.23208191126279865,"its dual norm by ∥· ∥∗. Let S(A) be the set of all A-surjective sets. Then
192"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.23321956769055746,"H(A) = max
J∈S(A) HJ(A)
(7)"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.23435722411831628,"where
193"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2354948805460751,"HJ(A) =
max
y∈Rm∥y∥≤1
min
x∈Rn
AJx≤yJ
∥x∥=
1
min
v∈RJ
+,∥v∥∗=1
∥A⊤
J v∥∗.
(8)"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2366325369738339,"This characterization is particularly useful when Rm and Rn are equipped with the ∞-norm, since
194"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.23777019340159272,"the computation of (8) reduces to a linear programming (LP) problem. The key problem is how to
195"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.23890784982935154,"maximize over all A-surjective sets. To do this, the PVZ algorithm maintains three collections of
196"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.24004550625711035,"sets F, I, and J where during every iteration: (i) F contains J such that J is A-surjective; (ii) I
197"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.24118316268486917,"contains J such that J is not A-surjective; and (iii) J contains candidates J whose A-surjectivity
198"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.24232081911262798,"will be tested.
199"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2434584755403868,"To detect whether a candidate J ∈J is surjective, the PVZ algorithm requires solving
200"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2445961319681456,"min ∥AT
J v∥1, s.t. v ∈RJ
+, ∥v∥1 = 1.
(9)"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.24573378839590443,"If the optimal value is positive, then J is A-surjective, and J is assigned to F and all subsets of J are
201"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.24687144482366324,"removed from J . Otherwise, the optimal value is 0 and there is v ∈RJ
+ such that A⊤
J v = 0. Let
202"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.24800910125142206,"I(v) = {i ∈J : vi > 0} and assign I(v) to I. Let ˆJ ∈J be any set containing I(v). Replace all
203"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.24914675767918087,"such ˆJ by sets ˆJ\{i}, i ∈I(v) which are not contained in any sets in F. The implementation used in
204"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2502844141069397,"our paper directly uses the MATLAB code provided by [17].
205"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2514220705346985,"Lower and Upper Bounds.
A limitation of the PVZ algorithm is that during each loop, every set in
206"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2525597269624573,"J needs to be tested, and each test requires solving a LP problem. Although solving one LP problem
207"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.25369738339021614,"in practice is fast, a complete while loop calls the LP solver many times.
208"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.25483503981797495,"Here, we propose an algorithm to estimate lower and upper bounds for Hoffman constants. An
209"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.25597269624573377,"intuitive way to estimate the lower bound is to sample a number of random subsets from {1, . . . , m}
210"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2571103526734926,"and test for A-surjectivity. This method bypasses optimizing combinatorially over S(A) of A-
211"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2582480091012514,"surjective sets and gives a lower bound of Hoffman constant by Proposition 3.8.
212"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2593856655290102,"To get an upper of Hoffman constant, we use the result from [18].
213"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2605233219567691,"Theorem 3.9. [18, Theorem 4.2] Let A ∈Rm×n. Let D(A) be a set of subsets of J ⊆{1, . . . , m}
214"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2616609783845279,"such that AJ is full rank. Let D∗(A) be the set of maximal elements in D(A). Then the Hoffman
215"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2627986348122867,"constant measured under 2-norm is bounded by
216"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.26393629124004553,"H(A) ≤
max
J∈D∗(A)
1
ˆρ(AJ)
(10)"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.26507394766780434,"where ˆρ(A) is the smallest singular value of A.
217"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.26621160409556316,"Using the fact that ∥· ∥1 ≥∥· ∥2, and the characterization from (8), we see that the upper bound also
218"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.267349260523322,"holds when Rm and Rn are equipped with the ∞-norm. However, enumerating all maximal elements
219"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2684869169510808,"in D(A) is not an improvement over enumerating A-surjective sets from a computational perspective.
220"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2696245733788396,"Instead, we will retain the strategy as in lower bound estimation to sample a number of sets from
221"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.2707622298065984,"{1, 2, . . . , m} and approximate the upper bound by (10). We verify this approach via synthetic data.
222"
COMPUTING AND ESTIMATING HOFFMAN CONSTANTS,0.27189988623435724,"The experiments are relegated to the Appendix.
223"
SYMMETRY AND THE FUNDAMENTAL DOMAIN,0.27303754266211605,"4
Symmetry and the Fundamental Domain
224"
SYMMETRY AND THE FUNDAMENTAL DOMAIN,0.27417519908987487,"In this section, we study a geometric characterization of the sampling domain for networks exhibiting
225"
SYMMETRY AND THE FUNDAMENTAL DOMAIN,0.2753128555176337,"symmetry. This corresponds to invariant neural networks.
226"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2764505119453925,"4.1
Linear Regions of Invariant Neural Networks
227"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2775881683731513,"The notion of invariance for a neural network describes when a manipulation of the input domain
228"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2787258248009101,"does not affect the output of the network. The manipulations we consider here are group actions.
229"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.27986348122866894,"Definition 4.1. Let σ : Rn →R be a piecewise linear function, and let G be a group acting on the
230"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.28100113765642776,"domain Rn. σ is invariant under the group action of G if for any element g ∈G, σ ◦g = σ.
231"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2821387940841866,"Given an invariant neural network, we can then define a sampling domain that takes into account the
232"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2832764505119454,"effect of the group action.
233"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2844141069397042,"Definition 4.2. Let G be a group acting on Rn. A subset ∆⊆Rn is a fundamental domain if it
234"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.285551763367463,satisfies two following conditions: (i) Rn = S
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.28668941979522183,"g∈G g · ∆; and (ii) g · int(∆) ∩h · int(∆) = ∅for all
235"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.28782707622298065,"g, h ∈G, g ̸= h.
236"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.28896473265073946,"The fundamental domain of a group G therefore provides a periodic tiling of Rn by acting on ∆.
237"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2901023890784983,"This is very useful in the context of numerical sampling for neural networks which are invariant
238"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2912400455062571,"under some symmetry, since it means we can sample from a smaller subset of the input domain with
239"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2923777019340159,"a guarantee to find all the linear regions in the limit. This allows us, in principle, to be able to use far
240"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2935153583617747,"fewer samples while maintaining the same density of points.
241"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.29465301478953354,"Theorem 4.3. Let f : RN →R be a tropical rational map invariant under group G. Let ∆⊆RN be
242"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.29579067121729236,"a fundamental domain of G. Suppose L is the set of linear regions. Define the following two sets
243"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.29692832764505117,"Uc := {A ∈U : A ⊆∆}
Un := {A ∈U : A ∩∆̸= ∅}.
Then
244"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.29806598407281,"|G||Uc| ≤|U| ≤|G||Uc| +
X"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.2992036405005688,A∈Un\Uc
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.3003412969283277,"|G|
|GA|."
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.3014789533560865,"where |GA| is the size of the stabilizer of A.
245"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.3026166097838453,"This gives us a method for estimating the total number of linear regions from sampling in the
246"
LINEAR REGIONS OF INVARIANT NEURAL NETWORKS,0.3037542662116041,"fundamental domain using multiplicity, which we discuss next.
247"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.30489192263936293,"4.2
Sampling from the Fundamental Domain
248"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.30602957906712175,"To demonstrate the potential performance improvements in numerical sampling exploiting symmetry
249"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.30716723549488056,"in the network architecture, we consider permutation invariant neural networks inspired by deep sets
250"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3083048919226394,"[19]. Our numerical sampling approach is inspired by very recent work in this area [20].
251"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3094425483503982,"Lemma 4.4 ([19]). An m × m matrix W acting as a linear operator of the form W = λIm×m +
252"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.310580204778157,"γ(1T 1), where λ, γ ∈R is permutation equivariant, meaning WPx = PWx for any x ∈Rm, so it
253"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3117178612059158,"commutes with any permutation matrix.
254"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.31285551763367464,"Using a weight matrix of this form, we can construct permutation invariant neural networks by setting
255"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.31399317406143346,"the bias to 0, applying a ReLU activation after multiplication by W, and then summing. In this case,
256"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.31513083048919227,"the network is invariant under the group action Sn, so the fundamental domain is the set of points
257"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3162684869169511,"with increasing coordinates, i.e., ∆= {(x1, . . . , xn) : x1 ≥x2 ≥. . . ≥xn}. This splits Rn into n!
258"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3174061433447099,"tiles, so we have a clear and significant advantage in restricting sampling to the fundamental domain.
259"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3185437997724687,"Note, however, that it is important to address the multiplicities of symmetric linear regions correctly:
260"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.31968145620022753,"If a given Jacobian of shape n × 1 has no repeated elements, this means it is contained in the interior
261"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.32081911262798635,"of some group action applied to the fundamental domain. This means there are n! total linear regions
262"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.32195676905574516,"with this Jacobian. If, on the other hand, there are repeated coefficients in a given Jacobian J, we
263"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.323094425483504,"consider the set C(J) of counts of repeated elements. For example, for J = [1, 1, 0], C(J) = (2, 1).
264"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3242320819112628,"Then the multiplicity of a given Jacobian is given by
265"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3253697383390216,"mult(J) =
n!
Q
c∈C(J) c!."
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3265073947667804,"Using this multiplicity calculation we can efficiently estimate the number of linear regions while
266"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.32764505119453924,"reducing the number of point samples by a factor of n!. This provides a dramatic gain in sampling
267"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.32878270762229805,"efficiency.
268"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.32992036405005687,"In Figure 1, we present the results when Algorithm 2 is run with R = 10, N = 10, M = 50. These
269"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3310580204778157,"results show that the fundamental domain estimate performs well for low dimensional inputs but
270"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3321956769055745,"appears to overcount linear regions as n increases. Despite divergence, there is still utility in this
271"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.3333333333333333,"metric because we are often more concerned with obtaining an upper bound on the expressivity of a
272"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.33447098976109213,"neural network than an exact figure and the fundamental domain estimate does not undercount the
273"
SAMPLING FROM THE FUNDAMENTAL DOMAIN,0.33560864618885095,"number of linear regions.
274"
SYMBOLIC NEURAL NETWORKS,0.33674630261660976,"5
Symbolic Neural Networks
275"
SYMBOLIC NEURAL NETWORKS,0.3378839590443686,"Here, we present the details on our practical contribution of a symbolic representation of neural
276"
SYMBOLIC NEURAL NETWORKS,0.3390216154721274,"networks as a new library integrated into OSCAR [15].
277"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.34015927189988626,"5.1
Computing Linear Regions of Tropical Puiseux Rational Maps
278"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3412969283276451,"We present an algorithm that can compute the linear regions of any tropical Puiseux rational function.
279"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3424345847554039,"Intuitively, we do this by computing the linear regions of the numerator and denominator, and then
280"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3435722411831627,"considering intersections of such regions and how they fit together. Thus, a first step is to understand
281"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3447098976109215,"how the computation of linear regions works for tropical Puiseux polynomials. The key to our
282"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.34584755403868034,"approach will be to exploit the polyhedral connection of tropical geometry and recast the problem
283"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.34698521046643915,"in the language of polyhedral geometry. This, among other things, will allow us to make use the
284"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.34812286689419797,"extensive polyhedral geometry library in OSCAR [15] for implementation.
285"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3492605233219568,"One important upshot from this study is that there is a strong connection between the number of
286"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3503981797497156,"linear regions of a tropical Puiseux rational function and the number of monomials that appear in
287"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3515358361774744,"its algebraic expression. Note, however, that the two are independent, in the sense that two Puiseux
288"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.35267349260523323,"rational functions could have the same number of linear regions but different numbers of (nonzero)
289"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.35381114903299204,"monomials, and conversely, the same number of monomials and a different number of linear regions.
290"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.35494880546075086,"For instance, computing the number of linear regions requires some combinatorial data about the
291"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3560864618885097,"intersections of the polyhedra defined by monomials.
292"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3572241183162685,"First, we need to know how to compute the linear regions of tropical polynomials. Let P =
293
L"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3583617747440273,"n an ⊙xn where by xn we mean xn1
1 ⊙· · · ⊙xnk
k and powers are taken in the tropical sense. Then
294"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3594994311717861,"as function Rk →R, P is given by maxn {an + n1x1 · · · + nkxk} . It follows that the linear regions
295"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.36063708759954494,"of P are precisely the sets of the form
296"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.36177474402730375,"Sn = {x ∈Rn | am + m1x1 · · · + mkxk ≤an + n1x1 · · · + nkxk for all m ̸= n} .
For any set U on which P is linear, we write L(P, U) for the corresponding linear map. This gives us
297"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.36291240045506257,"L(P, Sn)(x) = an + n1x1 · · · + nkxk.
(11)"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3640500568828214,"We now rewrite (11) using polyhedral geometry. Recall that a polyhedron in Rk is a set of the form
298"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3651877133105802,"P(A, b) = {x ∈Rk | Ax ≤b}. We claim that each linear region is a polyhedron: For a fixed index
299"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.366325369738339,"n, define the matrix An to be the (N −1) × k matrix whose rows are the vectors m −n, where m
300"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.36746302616609783,"ranges over the support of the coefficients of P (ordered lexicographically) and bn to be the vector
301"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.36860068259385664,"with entries an −am. Then Sn = P(An, bn). This gives us a way to encode the computation of the
302"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.36973833902161546,"linear regions of tropical Puiseux polynomials using polyhedral geometry. As a direct consequence,
303"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3708759954493743,"intersections of linear regions of tropical Puiseux polynomials are also polyhedra. In particular, there
304"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3720136518771331,"are algorithms from polyhedral geometry for determining whether such polyhedra are realizable. One
305"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3731513083048919,"of the key observations given by our algorithm is that the linear regions of tropical Puiseux rational
306"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.3742889647326507,"maps are almost given by k-dimensional intersections of the linear regions of the numerator and
307"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.37542662116040953,"the denominator. Indeed, note that if U is a linear region of p and V a linear region of q, then we
308"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.37656427758816835,"have L(U ∩V, p ⊘q) = L(U, p) −L(V, q). The only issue that arises is that there might be some
309"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.37770193401592717,"repetition in the L(U ∩V, p⊘q) as U ranges over the linear regions of p and V over the linear regions
310"
COMPUTING LINEAR REGIONS OF TROPICAL PUISEUX RATIONAL MAPS,0.378839590443686,"of q. In particular, linear regions of p ⊘q might end up corresponding to unions of such U ∩V .
311"
COMPUTING LINEAR REGIONS,0.3799772468714448,"5.2
Computing Linear Regions
312"
COMPUTING LINEAR REGIONS,0.38111490329920367,"Determining the linear regions of a neural network may be approached numerically or symbolically.
313"
COMPUTING LINEAR REGIONS,0.3822525597269625,"The numerical approach exploits the fact that linear regions of a neural network correspond to regions
314"
COMPUTING LINEAR REGIONS,0.3833902161547213,"where the gradient is constant. Thus, to estimate the number of linear regions, we can evaluate the
315"
COMPUTING LINEAR REGIONS,0.3845278725824801,"gradient on a sample of points (e.g., a mesh) in some large box [−R, R]n. For sufficiently large R
316"
COMPUTING LINEAR REGIONS,0.3856655290102389,"and a sufficiently dense sample of points, we get an accurate estimate. The symbolic approach, on
317"
COMPUTING LINEAR REGIONS,0.38680318543799774,"the other hand, exploits the connection between neural networks and tropical Puiseux rational maps.
318"
COMPUTING LINEAR REGIONS,0.38794084186575656,"Indeed, we can symbolically compute a Puiseux rational map that represents the neural network and
319"
COMPUTING LINEAR REGIONS,0.3890784982935154,"then compute the number of linear regions using the approach outlined in section 5.1.
320"
COMPUTING LINEAR REGIONS,0.3902161547212742,"To compare each method, we ran the computations on smaller networks with varying sizes to compare
321"
COMPUTING LINEAR REGIONS,0.391353811149033,"run times and precision. For the symbolic approach, we generate 20 neural networks with random
322"
COMPUTING LINEAR REGIONS,0.3924914675767918,"weights for each architecture and then compute the tropical Puiseux rational function associated to
323"
COMPUTING LINEAR REGIONS,0.39362912400455063,"each neural network and compute the linear regions using Algorithm 3.
324"
COMPUTING LINEAR REGIONS,0.39476678043230945,"For the numerical approach, we also work with synthetic data and generate 1000 neural networks
325"
COMPUTING LINEAR REGIONS,0.39590443686006827,"with random weights for each architecture. We then estimate the number of linear regions in a box of
326"
COMPUTING LINEAR REGIONS,0.3970420932878271,"size [−10, 10]n and sample 1000 points from this domain.
327"
COMPUTING LINEAR REGIONS,0.3981797497155859,"In both cases, we use He initialization for the weights, i.e., we generate weights with distribution
328"
COMPUTING LINEAR REGIONS,0.3993174061433447,"N(0,
2
√"
COMPUTING LINEAR REGIONS,0.4004550625711035,"d) where d is the input dimension. The data we obtain in this manner is summarized in Tables
329"
COMPUTING LINEAR REGIONS,0.40159271899886234,"10 and 11. For the symbolic approach, we also track the number of nonzero monomials to compare
330"
COMPUTING LINEAR REGIONS,0.40273037542662116,"this quantity with the number of linear regions. For networks with 3 layers, we find the numerical
331"
COMPUTING LINEAR REGIONS,0.40386803185437997,"estimate to be quite close, but for 4 it seems to diverge. This could be because in the numerical
332"
COMPUTING LINEAR REGIONS,0.4050056882821388,"approach, we are only counting the number of unique Jacobians that can be found in the domain. A
333"
COMPUTING LINEAR REGIONS,0.4061433447098976,"situation could arise where the same linear function is disconnected and hence counted twice by the
334"
COMPUTING LINEAR REGIONS,0.4072810011376564,"symbolic approach but only once for the numerical approach.
335"
COMPUTING LINEAR REGIONS,0.40841865756541523,"The main observations from our experimental study are as follows. The numerical approach is faster,
336"
COMPUTING LINEAR REGIONS,0.40955631399317405,"but offers no guarantee of precision: When running the computation for a given R and mesh grid,
337"
COMPUTING LINEAR REGIONS,0.41069397042093286,"there seems to be no easy way of determining whether we have indeed hit all the linear regions or
338"
COMPUTING LINEAR REGIONS,0.4118316268486917,"whether we have obtained an accurate estimate of the arrangements of these regions. It is possible
339"
COMPUTING LINEAR REGIONS,0.4129692832764505,"to either overestimate or underestimate the number of linear regions. In particular, there is a priori
340"
COMPUTING LINEAR REGIONS,0.4141069397042093,"no obvious way to select the parameters. We found the symbolic approach to be more precise, but
341"
COMPUTING LINEAR REGIONS,0.4152445961319681,"slower. In general, the number of monomials seems to be far larger than the number of linear regions,
342"
COMPUTING LINEAR REGIONS,0.41638225255972694,"which contradicts the intuition of Figure 2.
343"
COMPUTING LINEAR REGIONS,0.41751990898748575,"Both algorithms suffer from the curse of dimensionality: in the case of the numerical approach, the
344"
COMPUTING LINEAR REGIONS,0.41865756541524457,"number of samples in a meshgrid grows exponenially with respect to the dimension. In the case of
345"
COMPUTING LINEAR REGIONS,0.4197952218430034,"the symbolic approach, calculations with polytopes seem to scale poorly with dimension and with the
346"
COMPUTING LINEAR REGIONS,0.42093287827076226,"complexity of the neural network.
347"
COMPUTING LINEAR REGIONS,0.42207053469852107,"6
Discussion: Limitations & Directions for Future Research
348"
COMPUTING LINEAR REGIONS,0.4232081911262799,"In this paper, we set up a framework to interpret and analyzed the expressivity of neural networks
349"
COMPUTING LINEAR REGIONS,0.4243458475540387,"using techniques from polyhedral and tropical geometry. We demonstrated several ways in which a
350"
COMPUTING LINEAR REGIONS,0.4254835039817975,"symbolic interpretation can often enable computational optimizations for otherwise intractable tasks
351"
COMPUTING LINEAR REGIONS,0.42662116040955633,"and provided new insights into the inner workings of these networks. To the best of our knowledge,
352"
COMPUTING LINEAR REGIONS,0.42775881683731515,"ours is the first work to provide practical tropical geometric theory and algorithms to numerically
353"
COMPUTING LINEAR REGIONS,0.42889647326507396,"compute and analyze the expressivity of a neural network both in terms of inherent neural network
354"
COMPUTING LINEAR REGIONS,0.4300341296928328,"quantities as well as tropical geometric quantities.
355"
COMPUTING LINEAR REGIONS,0.4311717861205916,"Despite the theoretical and practical advancement of tropical deep learning that our work offers, it
356"
COMPUTING LINEAR REGIONS,0.4323094425483504,"is nevertheless subject to limitations, which we now discuss and which inspire directions for future
357"
COMPUTING LINEAR REGIONS,0.4334470989761092,"research.
358"
COMPUTING LINEAR REGIONS,0.43458475540386804,"Experimental Limitations.
The curse of dimensionality is a common theme in deep learning, and
359"
COMPUTING LINEAR REGIONS,0.43572241183162685,"our work is unfortunately no exception. The methods introduced in this paper are quite fast for small
360"
COMPUTING LINEAR REGIONS,0.43686006825938567,"enough networks, but scale poorly with dimension and more complex architectures.
361"
COMPUTING LINEAR REGIONS,0.4379977246871445,"We note that the main computational bottlenecks of the Puiseux rational function associated with a
362"
COMPUTING LINEAR REGIONS,0.4391353811149033,"neural network are the implementation of fast multivariate Puiseux series operations. Our current
363"
COMPUTING LINEAR REGIONS,0.4402730375426621,"computations rely on a custom implementation of this type of operation, and one potential avenue for
364"
COMPUTING LINEAR REGIONS,0.44141069397042093,"improvement would be using such methods once they have been implemented in OSCAR [15].
365"
COMPUTING LINEAR REGIONS,0.44254835039817975,"For the computation of linear regions, both the numerical and symbolic approaches suffer from the
366"
COMPUTING LINEAR REGIONS,0.44368600682593856,"curse of dimensionality. For instance, the numerical approach requires sampling on a mesh grid in a
367"
COMPUTING LINEAR REGIONS,0.4448236632536974,"box of the form [−R, R]n where n is the input dimension. In particular, the number of points needed
368"
COMPUTING LINEAR REGIONS,0.4459613196814562,"is proportional to the volume, which scales exponenially in n. Similarly, the symbolic approach relies
369"
COMPUTING LINEAR REGIONS,0.447098976109215,"on the computation of the Puiseux rational function associated with a neural network and polytope
370"
COMPUTING LINEAR REGIONS,0.4482366325369738,"computations, both of which are challenging computational problems in higher dimensions.
371"
COMPUTING LINEAR REGIONS,0.44937428896473264,"Most of our computations rely on carrying out some elementary computations many times. Thus,
372"
COMPUTING LINEAR REGIONS,0.45051194539249145,"another avenue of improvement for this would be to parallelize.
373"
COMPUTING LINEAR REGIONS,0.45164960182025027,"Structural Limitations.
Much of what we are studying are basically framed as a combinatorial
374"
COMPUTING LINEAR REGIONS,0.4527872582480091,"optimization problem, which are known to be difficult. In particular, computing the Hoffman constant
375"
COMPUTING LINEAR REGIONS,0.4539249146757679,"is equivalent to the Stewart–Todd condition measure of a matrix and both quantities are NP-hard to
376"
COMPUTING LINEAR REGIONS,0.4550625711035267,"compute in general cases [17, 21].
377"
COMPUTING LINEAR REGIONS,0.45620022753128553,"Further studying and understanding where and how symbolic computation algorithms can be made
378"
COMPUTING LINEAR REGIONS,0.45733788395904434,"more efficient, e.g., by parallelization, would make our proposed approaches more applicable to
379"
COMPUTING LINEAR REGIONS,0.45847554038680316,"larger neural networks. Our work effectively proposes a new intersection of symbolic computation
380"
COMPUTING LINEAR REGIONS,0.459613196814562,"and deep learning, so there remains infrastructure to set up to make methods from these two fields
381"
COMPUTING LINEAR REGIONS,0.46075085324232085,"compatible.
382"
REFERENCES,0.46188850967007966,"References
383"
REFERENCES,0.4630261660978385,"[1] Grigory Mikhalkin and Johannes Rau. Tropical geometry, volume 8. MPI for Mathematics,
384"
REFERENCES,0.4641638225255973,"2009.
385"
REFERENCES,0.4653014789533561,"[2] David Speyer and Bernd Sturmfels. Tropical Mathematics. Mathematics Magazine, 82(3):163–
386"
REFERENCES,0.4664391353811149,"173, 2009.
387"
REFERENCES,0.46757679180887374,"[3] Diane Maclagan and Bernd Sturmfels. Introduction to tropical geometry, volume 161. American
388"
REFERENCES,0.46871444823663255,"Mathematical Society, 2021.
389"
REFERENCES,0.46985210466439137,"[4] Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of
390"
REFERENCES,0.4709897610921502,"deep feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098,
391"
REFERENCES,0.472127417519909,"2013.
392"
REFERENCES,0.4732650739476678,"[5] Guido F Montúfar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of
393"
REFERENCES,0.47440273037542663,"linear regions of deep neural networks. Advances in neural information processing systems, 27,
394"
REFERENCES,0.47554038680318544,"2014.
395"
REFERENCES,0.47667804323094426,"[6] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep
396"
REFERENCES,0.4778156996587031,"neural networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.
397"
REFERENCES,0.4789533560864619,"[7] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
398"
REFERENCES,0.4800910125142207,"expressive power of deep neural networks. In international conference on machine learning,
399"
REFERENCES,0.4812286689419795,"pages 2847–2854. PMLR, 2017.
400"
REFERENCES,0.48236632536973834,"[8] Boris Hanin and David Rolnick. Deep ReLU Networks Have Surprisingly Few Activation
401"
REFERENCES,0.48350398179749715,"Patterns. Advances in neural information processing systems, 32, 2019.
402"
REFERENCES,0.48464163822525597,"[9] Huan Xiong, Lei Huang, Mengyang Yu, Li Liu, Fan Zhu, and Ling Shao. On the number
403"
REFERENCES,0.4857792946530148,"of linear regions of convolutional neural networks. In International Conference on Machine
404"
REFERENCES,0.4869169510807736,"Learning, pages 10514–10523. PMLR, 2020.
405"
REFERENCES,0.4880546075085324,"[10] Alexis Goujon, Arian Etemadi, and Michael Unser. On the number of regions of piecewise
406"
REFERENCES,0.4891922639362912,"linear neural networks. Journal of Computational and Applied Mathematics, 441:115667, 2024.
407"
REFERENCES,0.49032992036405004,"[11] Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks.
408"
REFERENCES,0.49146757679180886,"In International Conference on Machine Learning, pages 5824–5832. PMLR, 2018.
409"
REFERENCES,0.4926052332195677,"[12] Vasileios Charisopoulos and Petros Maragos. A tropical approach to neural networks with
410"
REFERENCES,0.4937428896473265,"piecewise linear activations. arXiv preprint arXiv:1805.08749, 2018.
411"
REFERENCES,0.4948805460750853,"[13] Ruriko Yoshida, Georgios Aliatimis, and Keiji Miura. Tropical neural networks and its applica-
412"
REFERENCES,0.4960182025028441,"tions to classifying phylogenetic trees. arXiv preprint arXiv:2309.13410, 2023.
413"
REFERENCES,0.49715585893060293,"[14] Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, and Jefferson Huang. Tropical
414"
REFERENCES,0.49829351535836175,"decision boundaries for neural networks are robust against adversarial attacks. arXiv preprint
415"
REFERENCES,0.49943117178612056,"arXiv:2402.00576, 2024.
416"
REFERENCES,0.5005688282138794,"[15] Oscar – open source computer algebra research system, version 1.0.0, 2024.
417"
REFERENCES,0.5017064846416383,"[16] Alan J Hoffman. On approximate solutions of systems of linear inequalities. In Selected Papers
418"
REFERENCES,0.502844141069397,"Of Alan J Hoffman: With Commentary, pages 174–176. World Scientific, 2003.
419"
REFERENCES,0.5039817974971559,"[17] Javier Pena, Juan Vera, and Luis Zuluaga. An algorithm to compute the hoffman constant of a
420"
REFERENCES,0.5051194539249146,"system of linear constraints. arXiv preprint arXiv:1804.08418, 2018.
421"
REFERENCES,0.5062571103526735,"[18] Osman Güler, Alan J Hoffman, and Uriel G Rothblum. Approximations to solutions to systems
422"
REFERENCES,0.5073947667804323,"of linear inequalities. SIAM Journal on Matrix Analysis and Applications, 16(2):688–696, 1995.
423"
REFERENCES,0.5085324232081911,"[19] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,
424"
REFERENCES,0.5096700796359499,"and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30,
425"
REFERENCES,0.5108077360637088,"2017.
426"
REFERENCES,0.5119453924914675,"[20] Alexis Goujon, Arian Etemadi, and Michael Unser. On the number of regions of piecewise
427"
REFERENCES,0.5130830489192264,"linear neural networks. Journal of Computational and Applied Mathematics, 441:115667, 2024.
428"
REFERENCES,0.5142207053469852,"[21] Javier F Pena, Juan C Vera, and Luis F Zuluaga. Equivalence and invariance of the chi and
429"
REFERENCES,0.515358361774744,"hoffman constants of a matrix. arXiv preprint arXiv:1905.06366, 2019.
430"
REFERENCES,0.5164960182025028,"A
Further Experimental Details
431"
REFERENCES,0.5176336746302617,"We ran the final computations on NVIDIA GeForce RTX 3090 GPUs. Table 7 lists the time taken by
432"
REFERENCES,0.5187713310580204,"each experiment. Given that our experiments do not include training on large datasets, the experiments
433"
REFERENCES,0.5199089874857793,"are not particularly expensive from the perspective memory usage, and all the code can be run on a
434"
REFERENCES,0.5210466439135382,"laptop. The detail provided in the paper correspond roughly to the amount of computational resources
435"
REFERENCES,0.5221843003412969,"that were used for this work, omitting trial and testing runs.
436"
REFERENCES,0.5233219567690558,"B
Algorithms
437"
REFERENCES,0.5244596131968146,"Algorithm 1 Lower and approximate upper bound of Hoffman constant
Require: A: an m × n matrix; B max number of iterations; ϵ threshold of testing surjectivity."
REFERENCES,0.5255972696245734,"1: Initialize HL = HU = 0.
2: for i ∈1, . . . , B do
3:
Sample a random integer K.
4:
Sample a random subset J from {1, . . . , m} of size K.
5:
Solve (9). Let t be the optimal value;
6:
if t > ϵ then
7:
J is surjective. Update HL = max{HL, 1 t };"
REFERENCES,0.5267349260523322,"8:
Compute the minimal singular value of ˆρ(AJ);
9:
if ˆρ(AJ) > 0 then
10:
Update HU = max{HU,
1
ˆρ(AJ)};
return Lower bound HL and approximate upper bound HU."
REFERENCES,0.5278725824800911,"Algorithm 2 Estimation of the ratio of fundamental domain sampling to regular sampling
Require: The input dimension n, R ∈R side length for cube centered at the origin from which the
samples are taken, M number of models to use, N base number of points to sample.
1: for m ∈1..M do
2:
Create a permutation invariant model σ with input dimension n.
3:
Sample N n points in the cube with side length R centered at the origin. Note that the number
of points in the sample grows exponentially with the input dimension n.
4:
Compute the Jacobian matrices of the network at each point, round to 10 decimal place to
avoid numerical errors, remove duplicates, and count the number of unique Jacobians.
5:
Sample Nn"
REFERENCES,0.5290102389078498,"n! points from the fundamental domain of Rn intersected with the sampling cube.
6:
Compute the unique Jacobians similarly as for the regular sampling.
7:
Sum the multiplicities of each Jacobian to get an estimate of the total number of linear
regions.
8:
Record the ratio of the fundamental domain estimate to the regular estimate.
return The average ratio across M models."
REFERENCES,0.5301478953356087,"C
Proofs
438"
REFERENCES,0.5312855517633674,"C.1
Proof of Proposition 3.4
439"
REFERENCES,0.5324232081911263,"Proof. The polyhedra defined by
440"
REFERENCES,0.5335608646188851," 
Ap
Aq"
REFERENCES,0.534698521046644,"
−1

aip
ajq"
REFERENCES,0.5358361774744027," 
x ≤

bip1 −bp
bjq1 −bq "
REFERENCES,0.5369738339021616,"form a convex refinement of linear regions of f. Let
441"
REFERENCES,0.5381114903299203,"resip,jq(x) :=
 
Ap
Aq"
REFERENCES,0.5392491467576792,"
−1

aip
ajq"
REFERENCES,0.540386803185438," 
x −

bip1 −bp
bjq1 −bq "
REFERENCES,0.5415244596131968,"denote the residual of x to the polyhedron. We have
442"
REFERENCES,0.5426621160409556,"Rf(x) ≤H(p ⊘q) max{∥resip,jq(x)+∥∞: 1 ≤ip ≤mp ; 1 ≤jq ≤mq}."
REFERENCES,0.5437997724687145,"Algorithm 3 Linear regions of tropical Puiseux rational functions
Require: Tropical Puiseux polynomials p, q in n variables."
REFERENCES,0.5449374288964732,"1: Compute the linear regions U1, . . . , Ul of p, and set Li = L(p, Ui).
2: Compute the linear regions V1, . . . , Vm of q, and set Sj = L(q, Vj).
3: Compute the pairs (i, j) such that Ui ∩Vj has dimension n
4: for (i, j) such that Ui ∩Vj has dimension n do
5:
Compute the linear map Tij = Li −Sj
6: Set S to be the set of all Tij
7: for T ∈S do
8:
Compute the set I(T) indices (i, j) such that T = Tij.
9:
Compute the set C(T) of connected components of
["
REFERENCES,0.5460750853242321,"(i,j)∈I(T )
Ui ∩Vj"
REFERENCES,0.5472127417519909,return S
REFERENCES,0.5483503981797497,T ∈S C(T).
REFERENCES,0.5494880546075085,"Algorithm 4 Numerical estimation of neural network linear regions
Require: The architecture of a linear activation neural network σ with scalar output, R ∈R side
length for cube centered at the origin from which the samples are taken, M number of models to
use, N number of points to sample.
1: for m ∈1..M do
2:
Create a model with architecture σ and initialise weights and biases using He inialisation.
3:
Sample N points in the cube with side length R centered at the origin.
4:
Compute the Jacobian matrices of the network at each point.
5:
Round the Jacobians matrices to 5 decimal places to avoid floating point errors.
6:
Remove duplicates and count the number of unique Jacobians.
return The average number of linear regions."
REFERENCES,0.5506257110352674,"Note that
443"
REFERENCES,0.5517633674630261,"∥resip,jq(x)+∥∞="
REFERENCES,0.552901023890785," 
Apx + bp −1(aipx + bip)
Aqx + bq −1(ajqx + bjq)   +"
REFERENCES,0.5540386803185438,"∞
= max
k,ℓ

(Apx + bp)k −(aipx + bip), (Aqx + bq)ℓ−(ajqx + bjq), 0"
REFERENCES,0.5551763367463026,"= max

p(x) −(aipx + bip), q(x) −(ajqx + bjq), 0"
REFERENCES,0.5563139931740614,"Therefore,
444"
REFERENCES,0.5574516496018203,"max
ip,jq ∥resip,jq(x)∥∞= max
ip,jq"
REFERENCES,0.558589306029579,"
p(x) −(aipx + bip), q(x) −(ajqx + bjq), 0"
REFERENCES,0.5597269624573379,"= max

p(x) −min
ip {aipx + bip}, q(x) −min
jq {ajqx + bjq}, 0"
REFERENCES,0.5608646188850968,"= max

p(x) −ˇp(x), q(x) −ˇq(x)"
REFERENCES,0.5620022753128555,"which proves (6).
445"
REFERENCES,0.5631399317406144,"C.2
Proof of Lemma 3.3
446"
REFERENCES,0.5642775881683731,"Proof. From the definition of minimal effective radius we have
447"
REFERENCES,0.565415244596132,"Rf(x) = min{r : B(x, r) ∩Ui ̸= ∅, Ui ∈U} = min{r : d(x, Ui) ≤r, Ui ∈U}
= max{d(x, Ui) : Ui ∈U}."
REFERENCES,0.5665529010238908,"For each linear region Ui characterized by eAUix ≤ebUi, by (1), d(x, Ui) ≤H( eAUi)∥( eAUix−ebUi)+∥.
448"
REFERENCES,0.5676905574516496,"Passing to maximum we have
449"
REFERENCES,0.5688282138794084,"Rf(x) = max
Ui∈U d(x, Ui) ≤max
Ui∈U H( eAUi) max
Ui∈U ∥( eAUix −ebUi)+∥= H(f) max
Ui∈U ∥( eAUix −ebUi)+∥. 450"
REFERENCES,0.5699658703071673,"Lower bounds HL
0.5460
0.1520
0.6220
0.5771
0.1208
0.0844
1.0389
0.1492
Time tL
0.2495
0.2449
0.2446
0.2458
0.2443
0.2463
0.2466
0.2477
True values H
0.3298
0.7980
0.3772
0.8376
6.5934
2.6744
0.9372
1.2645
Time t
0.2132
0.1504
0.1253
0.1722
0.1566
0.1529
0.1721
0.1568
Upper bounds HU
0.2081
0.5090
0.2903
1.0539
3.8508
1.3942
0.5484
0.8031
Time tU
0.0043
0.0040
0.0040
0.0040
0.0040
0.0040
0.0040
0.0040"
REFERENCES,0.571103526734926,"Table 1: Hoffman constants, lower bounds, approximate upper bounds, and their corresponding
computational time for mp = 10, mq = 5 and n = 3"
REFERENCES,0.5722411831626849,"C.3
Proof of Theorem 4.3
451"
REFERENCES,0.5733788395904437,Proof. The action of G partitions U into a set of orbits [U] and we have |U| = P
REFERENCES,0.5745164960182025,"[A]∈[U] |[A]|.
452"
REFERENCES,0.5756541524459613,"From property (i) defining a fundamental domain ∪A∈UA = ∪σ∈Gσ · ∆, we have the trivial lower
bound on the number of linear regions |U| ≥P"
REFERENCES,0.5767918088737202,"A∈Uc |[A]| = |G||Uc|. Let A ∈U. By Lagrange’s
theorem, the orbit of A is such that |[A]||GA| = |G|. Thus we have"
REFERENCES,0.5779294653014789,"|U| ≤
X"
REFERENCES,0.5790671217292378,"A∈Un
|[A]| ≤|G||Uc| +
X"
REFERENCES,0.5802047781569966,A∈Un\Uc
REFERENCES,0.5813424345847554,"|G|
|GA|. 453"
REFERENCES,0.5824800910125142,"D
Numerical Calculations of the Hoffman Constant
454"
REFERENCES,0.5836177474402731,"We illustrate the computation of Hoffman constant of tropical Puiseux rational map on synthetic data.
455"
REFERENCES,0.5847554038680318,"We generate tropical Puiseux rational maps by randomly generating two tropical Puiseux polynomials
456"
REFERENCES,0.5858930602957907,"p and q. Specifically, suppose p has mp monomials and q has mq monomials. We construct an
457"
REFERENCES,0.5870307167235495,"mp ×n matrix Ap and an mq ×n matrix Aq by uniformly sampling entries from [0, 1]. We then form
458"
REFERENCES,0.5881683731513083,"the matrix defined by (4). We then compute the exact Hoffman constant using the PVZ algorithm and
459"
REFERENCES,0.5893060295790671,"estimate its lower bound and approximate its upper bound by our proposed algorithm. We record the
460"
REFERENCES,0.590443686006826,"computation time and the number of calls to solve the LP problem in the whole loop.
461"
REFERENCES,0.5915813424345847,"In the experiment we take different values of mp, mq, n and B. For each of the parameters we
462"
REFERENCES,0.5927189988623436,"repeat all computation for 8 times. The true Hoffman constants, lower bounds, upper bounds, and
463"
REFERENCES,0.5938566552901023,"the computation time per linear region can be found in Table 1,2,3, and the number of iterations of
464"
REFERENCES,0.5949943117178612,"the PVZ algorithm and average time to solve LP during each iteration can be found in Table 4,5.6.
465"
REFERENCES,0.59613196814562,"From the tables we can see that computing the true Hoffman constants requires testing surjectivity
466"
REFERENCES,0.5972696245733788,"and solving over thousands LP problems, which costs a lot of time. Although the lower bounds and
467"
REFERENCES,0.5984072810011376,"approximate upper bounds can be loose, the computational time is much faster for lower bounds and
468"
REFERENCES,0.5995449374288965,"upper bounds, which implies its potential to apply for real data applications.
469"
REFERENCES,0.6006825938566553,"E
Tables
470"
REFERENCES,0.6018202502844141,"Tables 8 and 9 summarise the outcomes of the experiments on the computation of linear regions for
471"
REFERENCES,0.602957906712173,"tropical Puiseux rational functions. For a fixed number of variables nvar and number of monomials
472"
REFERENCES,0.6040955631399317,"nmonomials, we generate nsamples random Puiseux rational functions by picking random coefficients
473"
REFERENCES,0.6052332195676906,"and exponents using Julia’s inbuilt random number generation functions, where both the numerator
474"
REFERENCES,0.6063708759954494,"and the denominator have nmonomials monomials. We then compute the number of linear regions for
475"
REFERENCES,0.6075085324232082,"each of these rational functions and take the average over our all the samples that were generated.
476"
REFERENCES,0.608646188850967,"F
Figures
477"
REFERENCES,0.6097838452787259,"Lower bounds HL
2.4965
5.9002
2.3501
3.7049
1.1434
0.8335
1.6517
2.2396
Time tL
0.8924
0.9101
0.9127
0.8914
0.9132
0.9154
1.1117
0.6190
True values H
26.2231
726.8115
173.0057
23.8868
52.6080
8.1573
8.5050
18.7593
Time t
6.0048
2.3452
5.5451
3.6778
3.2828
2.9109
3.5494
1.7530
Upper bounds HU
8.2854
323.5149
21.3290
7.4338
183.8179
254.1373
36.7961
32.5276
Time tU
0.0136
0.0137
0.0143
0.0132
0.0120
0.0139
0.0148
0.0097"
REFERENCES,0.6109215017064846,"Table 2: Hoffman constants, lower bounds, approximate upper bounds, and their corresponding
computational time for mp = 15, mq = 9 and n = 6"
REFERENCES,0.6120591581342435,"Lower bounds HL
0.1120
0.1382
0.1227
63.9169
0.2331
0.1191
0.0571
0.1126
Time tL
0.2622
0.2628
0.2625
0.2672
0.2771
0.2715
0.2689
0.2633
True values H
0.0017
1.2683
1.5375
0.0832
0.2777
0.3537
0.0464
0.1586
Time t
0.0112
0.0217
0.1002
0.0182
0.0582
0.0693
0.0189
0.0122
Upper bounds HU
10.6826
1.7551
3.2794
7.2134
26.4648
2.6868
25.0251
5.1308
Time tU
0.0775
0.0789
0.0780
0.0784
0.0789
0.0782
0.0782
0.0769"
REFERENCES,0.6131968145620023,"Table 3: Hoffman constants, lower bounds, approximate upper bounds, and their corresponding
computational time for mp = 15, mq = 5 and n = 7"
REFERENCES,0.6143344709897611,"# iterations
94
86
67
83
99
86
75
83
Time per LP
0.0042
0.0026
0.0025
0.0026
0.0025
0.0025
0.0026
0.0026
Table 4: Number of iterations in the PVZ algorithm and average time to solve LP during each iteration
for mp = 10, mq = 5 and n = 3"
REFERENCES,0.6154721274175199,"# iterations
2437
1110
1731
1441
1432
1706
1741
1095
Time per LP
0.0152
0.0093
0.0092
0.0098
0.0098
0.0102
0.0095
0.0097
Table 5: Number of iterations in the PVZ algorithm and average time to solve LP during each iteration
for mp = 15, mq = 9 and n = 6"
REFERENCES,0.6166097838452788,"# iterations
2
607
525
80
194
355
78
19
Time per LP
0.0027
0.0027
0.0026
0.0027
0.0032
0.0027
0.0028
0.0027
Table 6: Number of iterations in the PVZ algorithm and average time to solve LP during each iteration
for mp = 15, mq = 5 and n = 7"
REFERENCES,0.6177474402730375,"Experiment
Compute time
Linear regions of tropical Puiseux rational functions (3 variables)
4.7 hours
Linear regions of tropical Puiseux rational functions (4 variables)
13.3 hours
Symbolic linear regions computation for neural networks
35 minutes
Numeric linear regions computation for neural networks
4.9 minutes
Sampling on fundamental domain
13 minutes
Table 7: Compute details"
REFERENCES,0.6188850967007964,"nmonomials
Average number of regions
Average runtime
20
84.4
4.0 seconds
50
160.1
11.6 seconds
100
264.2
29.5 seconds
200
375.2
66.0 seconds
350
500.8
139.2 seconds
500
580.8
202.7 seconds
800
706.1
394.8 seconds
1000
776.2
563.6 seconds
Table 8: Computation for nvar = 3, nsamples = 12"
REFERENCES,0.6200227531285551,"nmonomials
Average number of regions
Average runtime
20
157.5
12.6 seconds
50
398.75
50.3 seconds
100
667.75
83.8 seconds
200
1021.5
237.5 seconds
350
1614.5
987.3 seconds
500
1909.5
1682.1 seconds
800
2432.0
3436.7 seconds
1000
2876.5
5441.8 seconds
Table 9: Computation for nvar = 4, nsamples = 4"
REFERENCES,0.621160409556314,"Architecture
Average number of linear regions
Average number of monomials
Average runtime(s)
[2, 2, 1]
3.85
5.75
0.4166
[4, 3, 1]
6.75
9
0.4646
[4, 4, 1]
14.2
13.55
1.5794
[3, 2, 2, 1]
6.8
30.15
1.7679
[3, 3, 2, 1]
17.55
176.75
97.9659"
REFERENCES,0.6222980659840728,Table 10: Symbolic computation
REFERENCES,0.6234357224118316,"Architecture
Average number of linear regions
Average runtime(s)
[2, 2, 1]
3.041
0.01667
[4, 3, 1]
6.339
0.01667
[4, 4, 1]
11.936
0.01667
[3, 2, 2, 1]
3.549
0.01683
[3, 3, 2, 1]
7.381
0.01678
Table 11: Numerical computation"
REFERENCES,0.6245733788395904,Figure 1: Ratio estimates for different input sizes with standard deviation error bars
REFERENCES,0.6257110352673493,"100 200
500
1,000 300 600"
REFERENCES,0.626848691695108,Number of monomials
REFERENCES,0.6279863481228669,Average number of linear regions
REFERENCES,0.6291240045506257,Figure 2: Linear regions of a Puiseux rational function in 3 variables
REFERENCES,0.6302616609783845,"100 200
500
1,000 500 1,000 1,500 2,000 2,500 3,000"
REFERENCES,0.6313993174061433,Number of monomials
REFERENCES,0.6325369738339022,Average number of linear regions
REFERENCES,0.6336746302616609,Figure 3: Linear regions of a Puiseux rational function in 4 variables
REFERENCES,0.6348122866894198,"NeurIPS Paper Checklist
478"
CLAIMS,0.6359499431171786,"1. Claims
479"
CLAIMS,0.6370875995449374,"Question: Do the main claims made in the abstract and introduction accurately reflect the
480"
CLAIMS,0.6382252559726962,"paper’s contributions and scope?
481"
CLAIMS,0.6393629124004551,"Answer: [Yes]
482"
CLAIMS,0.6405005688282139,"Justification: Each of the three contributions mentioned in the abstract has a whole section
483"
CLAIMS,0.6416382252559727,"devoted to it, including theoretical results and experiments. We also provided an in-depth
484"
CLAIMS,0.6427758816837316,"discussion of the limitations of our work in Section 6.
485"
CLAIMS,0.6439135381114903,"Guidelines:
486"
CLAIMS,0.6450511945392492,"• The answer NA means that the abstract and introduction do not include the claims
487"
CLAIMS,0.646188850967008,"made in the paper.
488"
CLAIMS,0.6473265073947668,"• The abstract and/or introduction should clearly state the claims made, including the
489"
CLAIMS,0.6484641638225256,"contributions made in the paper and important assumptions and limitations. A No or
490"
CLAIMS,0.6496018202502845,"NA answer to this question will not be perceived well by the reviewers.
491"
CLAIMS,0.6507394766780432,"• The claims made should match theoretical and experimental results, and reflect how
492"
CLAIMS,0.6518771331058021,"much the results can be expected to generalize to other settings.
493"
CLAIMS,0.6530147895335608,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
494"
CLAIMS,0.6541524459613197,"are not attained by the paper.
495"
LIMITATIONS,0.6552901023890785,"2. Limitations
496"
LIMITATIONS,0.6564277588168373,"Question: Does the paper discuss the limitations of the work performed by the authors?
497"
LIMITATIONS,0.6575654152445961,"Answer: [Yes]
498"
LIMITATIONS,0.658703071672355,"Justification: We provided a detailed discussion of the limitations or our work, both compu-
499"
LIMITATIONS,0.6598407281001137,"tational and theoretical in Section 6.
500"
LIMITATIONS,0.6609783845278726,"Guidelines:
501"
LIMITATIONS,0.6621160409556314,"• The answer NA means that the paper has no limitation while the answer No means that
502"
LIMITATIONS,0.6632536973833902,"the paper has limitations, but those are not discussed in the paper.
503"
LIMITATIONS,0.664391353811149,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
504"
LIMITATIONS,0.6655290102389079,"• The paper should point out any strong assumptions and how robust the results are to
505"
LIMITATIONS,0.6666666666666666,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
506"
LIMITATIONS,0.6678043230944255,"model well-specification, asymptotic approximations only holding locally). The authors
507"
LIMITATIONS,0.6689419795221843,"should reflect on how these assumptions might be violated in practice and what the
508"
LIMITATIONS,0.6700796359499431,"implications would be.
509"
LIMITATIONS,0.6712172923777019,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
510"
LIMITATIONS,0.6723549488054608,"only tested on a few datasets or with a few runs. In general, empirical results often
511"
LIMITATIONS,0.6734926052332195,"depend on implicit assumptions, which should be articulated.
512"
LIMITATIONS,0.6746302616609784,"• The authors should reflect on the factors that influence the performance of the approach.
513"
LIMITATIONS,0.6757679180887372,"For example, a facial recognition algorithm may perform poorly when image resolution
514"
LIMITATIONS,0.676905574516496,"is low or images are taken in low lighting. Or a speech-to-text system might not be
515"
LIMITATIONS,0.6780432309442548,"used reliably to provide closed captions for online lectures because it fails to handle
516"
LIMITATIONS,0.6791808873720137,"technical jargon.
517"
LIMITATIONS,0.6803185437997725,"• The authors should discuss the computational efficiency of the proposed algorithms
518"
LIMITATIONS,0.6814562002275313,"and how they scale with dataset size.
519"
LIMITATIONS,0.6825938566552902,"• If applicable, the authors should discuss possible limitations of their approach to
520"
LIMITATIONS,0.6837315130830489,"address problems of privacy and fairness.
521"
LIMITATIONS,0.6848691695108078,"• While the authors might fear that complete honesty about limitations might be used by
522"
LIMITATIONS,0.6860068259385665,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
523"
LIMITATIONS,0.6871444823663254,"limitations that aren’t acknowledged in the paper. The authors should use their best
524"
LIMITATIONS,0.6882821387940842,"judgment and recognize that individual actions in favor of transparency play an impor-
525"
LIMITATIONS,0.689419795221843,"tant role in developing norms that preserve the integrity of the community. Reviewers
526"
LIMITATIONS,0.6905574516496018,"will be specifically instructed to not penalize honesty concerning limitations.
527"
THEORY ASSUMPTIONS AND PROOFS,0.6916951080773607,"3. Theory Assumptions and Proofs
528"
THEORY ASSUMPTIONS AND PROOFS,0.6928327645051194,"Question: For each theoretical result, does the paper provide the full set of assumptions and
529"
THEORY ASSUMPTIONS AND PROOFS,0.6939704209328783,"a complete (and correct) proof?
530"
THEORY ASSUMPTIONS AND PROOFS,0.6951080773606371,"Answer: [Yes]
531"
THEORY ASSUMPTIONS AND PROOFS,0.6962457337883959,"Justification: We clearly define all mathematical terms and the proofs are explained in detail
532"
THEORY ASSUMPTIONS AND PROOFS,0.6973833902161547,"and are correct to the best of our knowledge.
533"
THEORY ASSUMPTIONS AND PROOFS,0.6985210466439136,"Guidelines:
534"
THEORY ASSUMPTIONS AND PROOFS,0.6996587030716723,"• The answer NA means that the paper does not include theoretical results.
535"
THEORY ASSUMPTIONS AND PROOFS,0.7007963594994312,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
536"
THEORY ASSUMPTIONS AND PROOFS,0.70193401592719,"referenced.
537"
THEORY ASSUMPTIONS AND PROOFS,0.7030716723549488,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
538"
THEORY ASSUMPTIONS AND PROOFS,0.7042093287827076,"• The proofs can either appear in the main paper or the supplemental material, but if
539"
THEORY ASSUMPTIONS AND PROOFS,0.7053469852104665,"they appear in the supplemental material, the authors are encouraged to provide a short
540"
THEORY ASSUMPTIONS AND PROOFS,0.7064846416382252,"proof sketch to provide intuition.
541"
THEORY ASSUMPTIONS AND PROOFS,0.7076222980659841,"• Inversely, any informal proof provided in the core of the paper should be complemented
542"
THEORY ASSUMPTIONS AND PROOFS,0.7087599544937428,"by formal proofs provided in appendix or supplemental material.
543"
THEORY ASSUMPTIONS AND PROOFS,0.7098976109215017,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
544"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7110352673492605,"4. Experimental Result Reproducibility
545"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7121729237770194,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
546"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7133105802047781,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
547"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.714448236632537,"of the paper (regardless of whether the code and data are provided or not)?
548"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7155858930602957,"Answer: [Yes]
549"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7167235494880546,"Justification: Our paper describes the algorithms that are used to run the experiments, and our
550"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7178612059158134,"submission includes all the code necessary to run these together with instructions detailing
551"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7189988623435722,"how to use it.
552"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7201365187713311,"Guidelines:
553"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7212741751990899,"• The answer NA means that the paper does not include experiments.
554"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7224118316268487,"• If the paper includes experiments, a No answer to this question will not be perceived
555"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7235494880546075,"well by the reviewers: Making the paper reproducible is important, regardless of
556"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7246871444823664,"whether the code and data are provided or not.
557"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7258248009101251,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
558"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.726962457337884,"to make their results reproducible or verifiable.
559"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7281001137656428,"• Depending on the contribution, reproducibility can be accomplished in various ways.
560"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7292377701934016,"For example, if the contribution is a novel architecture, describing the architecture fully
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7303754266211604,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7315130830489193,"be necessary to either make it possible for others to replicate the model with the same
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.732650739476678,"dataset, or provide access to the model. In general. releasing code and data is often
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7337883959044369,"one good way to accomplish this, but reproducibility can also be provided via detailed
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7349260523321957,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7360637087599545,"of a large language model), releasing of a model checkpoint, or other means that are
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7372013651877133,"appropriate to the research performed.
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7383390216154722,"• While NeurIPS does not require releasing code, the conference does require all submis-
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7394766780432309,"sions to provide some reasonable avenue for reproducibility, which may depend on the
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7406143344709898,"nature of the contribution. For example
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7417519908987485,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7428896473265074,"to reproduce that algorithm.
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7440273037542662,"(b) If the contribution is primarily a new model architecture, the paper should describe
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.745164960182025,"the architecture clearly and fully.
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7463026166097838,"(c) If the contribution is a new model (e.g., a large language model), then there should
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7474402730375427,"either be a way to access this model for reproducing the results or a way to reproduce
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7485779294653014,"the model (e.g., with an open-source dataset or instructions for how to construct
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7497155858930603,"the dataset).
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7508532423208191,"(d) We recognize that reproducibility may be tricky in some cases, in which case
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7519908987485779,"authors are welcome to describe the particular way they provide for reproducibility.
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7531285551763367,"In the case of closed-source models, it may be that access to the model is limited in
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7542662116040956,"some way (e.g., to registered users), but it should be possible for other researchers
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7554038680318543,"to have some path to reproducing or verifying the results.
584"
OPEN ACCESS TO DATA AND CODE,0.7565415244596132,"5. Open access to data and code
585"
OPEN ACCESS TO DATA AND CODE,0.757679180887372,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
586"
OPEN ACCESS TO DATA AND CODE,0.7588168373151308,"tions to faithfully reproduce the main experimental results, as described in supplemental
587"
OPEN ACCESS TO DATA AND CODE,0.7599544937428896,"material?
588"
OPEN ACCESS TO DATA AND CODE,0.7610921501706485,"Answer: [Yes]
589"
OPEN ACCESS TO DATA AND CODE,0.7622298065984073,"Justification: We provided all the code that is necessary to reproduce the experimental
590"
OPEN ACCESS TO DATA AND CODE,0.7633674630261661,"results, together with instructions on how to run this.
591"
OPEN ACCESS TO DATA AND CODE,0.764505119453925,"Guidelines:
592"
OPEN ACCESS TO DATA AND CODE,0.7656427758816837,"• The answer NA means that paper does not include experiments requiring code.
593"
OPEN ACCESS TO DATA AND CODE,0.7667804323094426,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
594"
OPEN ACCESS TO DATA AND CODE,0.7679180887372014,"public/guides/CodeSubmissionPolicy) for more details.
595"
OPEN ACCESS TO DATA AND CODE,0.7690557451649602,"• While we encourage the release of code and data, we understand that this might not be
596"
OPEN ACCESS TO DATA AND CODE,0.770193401592719,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
597"
OPEN ACCESS TO DATA AND CODE,0.7713310580204779,"including code, unless this is central to the contribution (e.g., for a new open-source
598"
OPEN ACCESS TO DATA AND CODE,0.7724687144482366,"benchmark).
599"
OPEN ACCESS TO DATA AND CODE,0.7736063708759955,"• The instructions should contain the exact command and environment needed to run to
600"
OPEN ACCESS TO DATA AND CODE,0.7747440273037542,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
601"
OPEN ACCESS TO DATA AND CODE,0.7758816837315131,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
602"
OPEN ACCESS TO DATA AND CODE,0.7770193401592719,"• The authors should provide instructions on data access and preparation, including how
603"
OPEN ACCESS TO DATA AND CODE,0.7781569965870307,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
604"
OPEN ACCESS TO DATA AND CODE,0.7792946530147895,"• The authors should provide scripts to reproduce all experimental results for the new
605"
OPEN ACCESS TO DATA AND CODE,0.7804323094425484,"proposed method and baselines. If only a subset of experiments are reproducible, they
606"
OPEN ACCESS TO DATA AND CODE,0.7815699658703071,"should state which ones are omitted from the script and why.
607"
OPEN ACCESS TO DATA AND CODE,0.782707622298066,"• At submission time, to preserve anonymity, the authors should release anonymized
608"
OPEN ACCESS TO DATA AND CODE,0.7838452787258248,"versions (if applicable).
609"
OPEN ACCESS TO DATA AND CODE,0.7849829351535836,"• Providing as much information as possible in supplemental material (appended to the
610"
OPEN ACCESS TO DATA AND CODE,0.7861205915813424,"paper) is recommended, but including URLs to data and code is permitted.
611"
OPEN ACCESS TO DATA AND CODE,0.7872582480091013,"6. Experimental Setting/Details
612"
OPEN ACCESS TO DATA AND CODE,0.78839590443686,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
613"
OPEN ACCESS TO DATA AND CODE,0.7895335608646189,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
614"
OPEN ACCESS TO DATA AND CODE,0.7906712172923777,"results?
615"
OPEN ACCESS TO DATA AND CODE,0.7918088737201365,"Answer: [Yes]
616"
OPEN ACCESS TO DATA AND CODE,0.7929465301478953,"Justification: The paper gives some details about how the synthetic data used for experiments
617"
OPEN ACCESS TO DATA AND CODE,0.7940841865756542,"was generated. Moreover, we also provide the code that is necessary to run the experiments.
618"
OPEN ACCESS TO DATA AND CODE,0.7952218430034129,"Guidelines:
619"
OPEN ACCESS TO DATA AND CODE,0.7963594994311718,"• The answer NA means that the paper does not include experiments.
620"
OPEN ACCESS TO DATA AND CODE,0.7974971558589306,"• The experimental setting should be presented in the core of the paper to a level of detail
621"
OPEN ACCESS TO DATA AND CODE,0.7986348122866894,"that is necessary to appreciate the results and make sense of them.
622"
OPEN ACCESS TO DATA AND CODE,0.7997724687144482,"• The full details can be provided either with the code, in appendix, or as supplemental
623"
OPEN ACCESS TO DATA AND CODE,0.800910125142207,"material.
624"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8020477815699659,"7. Experiment Statistical Significance
625"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8031854379977247,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
626"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8043230944254836,"information about the statistical significance of the experiments?
627"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8054607508532423,"Answer: [Yes]
628"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8065984072810012,"Justification: Our figures clearly demonstrate error bars when appropriate and we disclose
629"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8077360637087599,"the experimental setup relevant to the statistical significance of our experiments.
630"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8088737201365188,"Guidelines:
631"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8100113765642776,"• The answer NA means that the paper does not include experiments.
632"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8111490329920364,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
633"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8122866894197952,"dence intervals, or statistical significance tests, at least for the experiments that support
634"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8134243458475541,"the main claims of the paper.
635"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8145620022753128,"• The factors of variability that the error bars are capturing should be clearly stated (for
636"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8156996587030717,"example, train/test split, initialization, random drawing of some parameter, or overall
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8168373151308305,"run with given experimental conditions).
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8179749715585893,"• The method for calculating the error bars should be explained (closed form formula,
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8191126279863481,"call to a library function, bootstrap, etc.)
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.820250284414107,"• The assumptions made should be given (e.g., Normally distributed errors).
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8213879408418657,"• It should be clear whether the error bar is the standard deviation or the standard error
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8225255972696246,"of the mean.
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8236632536973834,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8248009101251422,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.825938566552901,"of Normality of errors is not verified.
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8270762229806599,"• For asymmetric distributions, the authors should be careful not to show in tables or
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8282138794084186,"figures symmetric error bars that would yield results that are out of range (e.g. negative
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8293515358361775,"error rates).
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8304891922639362,"• If error bars are reported in tables or plots, The authors should explain in the text how
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8316268486916951,"they were calculated and reference the corresponding figures or tables in the text.
651"
EXPERIMENTS COMPUTE RESOURCES,0.8327645051194539,"8. Experiments Compute Resources
652"
EXPERIMENTS COMPUTE RESOURCES,0.8339021615472128,"Question: For each experiment, does the paper provide sufficient information on the com-
653"
EXPERIMENTS COMPUTE RESOURCES,0.8350398179749715,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
654"
EXPERIMENTS COMPUTE RESOURCES,0.8361774744027304,"the experiments?
655"
EXPERIMENTS COMPUTE RESOURCES,0.8373151308304891,"Answer: [Yes]
656"
EXPERIMENTS COMPUTE RESOURCES,0.838452787258248,"Justification: The appendix provides some detail on the type of compute that was used (type
657"
EXPERIMENTS COMPUTE RESOURCES,0.8395904436860068,"of GPU, memory), as well was runtimes for each experiment.
658"
EXPERIMENTS COMPUTE RESOURCES,0.8407281001137656,"Guidelines:
659"
EXPERIMENTS COMPUTE RESOURCES,0.8418657565415245,"• The answer NA means that the paper does not include experiments.
660"
EXPERIMENTS COMPUTE RESOURCES,0.8430034129692833,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
661"
EXPERIMENTS COMPUTE RESOURCES,0.8441410693970421,"or cloud provider, including relevant memory and storage.
662"
EXPERIMENTS COMPUTE RESOURCES,0.8452787258248009,"• The paper should provide the amount of compute required for each of the individual
663"
EXPERIMENTS COMPUTE RESOURCES,0.8464163822525598,"experimental runs as well as estimate the total compute.
664"
EXPERIMENTS COMPUTE RESOURCES,0.8475540386803185,"• The paper should disclose whether the full research project required more compute
665"
EXPERIMENTS COMPUTE RESOURCES,0.8486916951080774,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
666"
EXPERIMENTS COMPUTE RESOURCES,0.8498293515358362,"didn’t make it into the paper).
667"
CODE OF ETHICS,0.850967007963595,"9. Code Of Ethics
668"
CODE OF ETHICS,0.8521046643913538,"Question: Does the research conducted in the paper conform, in every respect, with the
669"
CODE OF ETHICS,0.8532423208191127,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
670"
CODE OF ETHICS,0.8543799772468714,"Answer: [Yes]
671"
CODE OF ETHICS,0.8555176336746303,"Justification: We are carefully read through the code of ethics and to the best of our
672"
CODE OF ETHICS,0.856655290102389,"knowledge the contributions in this paper do not violate it in any way.
673"
CODE OF ETHICS,0.8577929465301479,"Guidelines:
674"
CODE OF ETHICS,0.8589306029579067,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
675"
CODE OF ETHICS,0.8600682593856656,"• If the authors answer No, they should explain the special circumstances that require a
676"
CODE OF ETHICS,0.8612059158134243,"deviation from the Code of Ethics.
677"
CODE OF ETHICS,0.8623435722411832,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
678"
CODE OF ETHICS,0.863481228668942,"eration due to laws or regulations in their jurisdiction).
679"
BROADER IMPACTS,0.8646188850967008,"10. Broader Impacts
680"
BROADER IMPACTS,0.8657565415244596,"Question: Does the paper discuss both potential positive societal impacts and negative
681"
BROADER IMPACTS,0.8668941979522184,"societal impacts of the work performed?
682"
BROADER IMPACTS,0.8680318543799772,"Answer: [NA]
683"
BROADER IMPACTS,0.8691695108077361,"Justification: The work does not have any obvious harmful applications or any potential
684"
BROADER IMPACTS,0.8703071672354948,"negative societal impact.
685"
BROADER IMPACTS,0.8714448236632537,"Guidelines:
686"
BROADER IMPACTS,0.8725824800910125,"• The answer NA means that there is no societal impact of the work performed.
687"
BROADER IMPACTS,0.8737201365187713,"• If the authors answer NA or No, they should explain why their work has no societal
688"
BROADER IMPACTS,0.8748577929465301,"impact or why the paper does not address societal impact.
689"
BROADER IMPACTS,0.875995449374289,"• Examples of negative societal impacts include potential malicious or unintended uses
690"
BROADER IMPACTS,0.8771331058020477,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
691"
BROADER IMPACTS,0.8782707622298066,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
692"
BROADER IMPACTS,0.8794084186575654,"groups), privacy considerations, and security considerations.
693"
BROADER IMPACTS,0.8805460750853242,"• The conference expects that many papers will be foundational research and not tied
694"
BROADER IMPACTS,0.8816837315130831,"to particular applications, let alone deployments. However, if there is a direct path to
695"
BROADER IMPACTS,0.8828213879408419,"any negative applications, the authors should point it out. For example, it is legitimate
696"
BROADER IMPACTS,0.8839590443686007,"to point out that an improvement in the quality of generative models could be used to
697"
BROADER IMPACTS,0.8850967007963595,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
698"
BROADER IMPACTS,0.8862343572241184,"that a generic algorithm for optimizing neural networks could enable people to train
699"
BROADER IMPACTS,0.8873720136518771,"models that generate Deepfakes faster.
700"
BROADER IMPACTS,0.888509670079636,"• The authors should consider possible harms that could arise when the technology is
701"
BROADER IMPACTS,0.8896473265073948,"being used as intended and functioning correctly, harms that could arise when the
702"
BROADER IMPACTS,0.8907849829351536,"technology is being used as intended but gives incorrect results, and harms following
703"
BROADER IMPACTS,0.8919226393629124,"from (intentional or unintentional) misuse of the technology.
704"
BROADER IMPACTS,0.8930602957906713,"• If there are negative societal impacts, the authors could also discuss possible mitigation
705"
BROADER IMPACTS,0.89419795221843,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
706"
BROADER IMPACTS,0.8953356086461889,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
707"
BROADER IMPACTS,0.8964732650739476,"feedback over time, improving the efficiency and accessibility of ML).
708"
SAFEGUARDS,0.8976109215017065,"11. Safeguards
709"
SAFEGUARDS,0.8987485779294653,"Question: Does the paper describe safeguards that have been put in place for responsible
710"
SAFEGUARDS,0.8998862343572241,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
711"
SAFEGUARDS,0.9010238907849829,"image generators, or scraped datasets)?
712"
SAFEGUARDS,0.9021615472127418,"Answer: [NA]
713"
SAFEGUARDS,0.9032992036405005,"Justification: The work does not pose any such risks.
714"
SAFEGUARDS,0.9044368600682594,"Guidelines:
715"
SAFEGUARDS,0.9055745164960182,"• The answer NA means that the paper poses no such risks.
716"
SAFEGUARDS,0.906712172923777,"• Released models that have a high risk for misuse or dual-use should be released with
717"
SAFEGUARDS,0.9078498293515358,"necessary safeguards to allow for controlled use of the model, for example by requiring
718"
SAFEGUARDS,0.9089874857792947,"that users adhere to usage guidelines or restrictions to access the model or implementing
719"
SAFEGUARDS,0.9101251422070534,"safety filters.
720"
SAFEGUARDS,0.9112627986348123,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
721"
SAFEGUARDS,0.9124004550625711,"should describe how they avoided releasing unsafe images.
722"
SAFEGUARDS,0.9135381114903299,"• We recognize that providing effective safeguards is challenging, and many papers do
723"
SAFEGUARDS,0.9146757679180887,"not require this, but we encourage authors to take this into account and make a best
724"
SAFEGUARDS,0.9158134243458476,"faith effort.
725"
LICENSES FOR EXISTING ASSETS,0.9169510807736063,"12. Licenses for existing assets
726"
LICENSES FOR EXISTING ASSETS,0.9180887372013652,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
727"
LICENSES FOR EXISTING ASSETS,0.919226393629124,"the paper, properly credited and are the license and terms of use explicitly mentioned and
728"
LICENSES FOR EXISTING ASSETS,0.9203640500568828,"properly respected?
729"
LICENSES FOR EXISTING ASSETS,0.9215017064846417,"Answer: [Yes]
730"
LICENSES FOR EXISTING ASSETS,0.9226393629124005,"Justification: Where we have used or been inspired by previous work we have made sure
731"
LICENSES FOR EXISTING ASSETS,0.9237770193401593,"that we have legal permission to use it and have clearly cited it in each case.
732"
LICENSES FOR EXISTING ASSETS,0.9249146757679181,"Guidelines:
733"
LICENSES FOR EXISTING ASSETS,0.926052332195677,"• The answer NA means that the paper does not use existing assets.
734"
LICENSES FOR EXISTING ASSETS,0.9271899886234357,"• The authors should cite the original paper that produced the code package or dataset.
735"
LICENSES FOR EXISTING ASSETS,0.9283276450511946,"• The authors should state which version of the asset is used and, if possible, include a
736"
LICENSES FOR EXISTING ASSETS,0.9294653014789533,"URL.
737"
LICENSES FOR EXISTING ASSETS,0.9306029579067122,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
738"
LICENSES FOR EXISTING ASSETS,0.931740614334471,"• For scraped data from a particular source (e.g., website), the copyright and terms of
739"
LICENSES FOR EXISTING ASSETS,0.9328782707622298,"service of that source should be provided.
740"
LICENSES FOR EXISTING ASSETS,0.9340159271899886,"• If assets are released, the license, copyright information, and terms of use in the
741"
LICENSES FOR EXISTING ASSETS,0.9351535836177475,"package should be provided. For popular datasets, paperswithcode.com/datasets
742"
LICENSES FOR EXISTING ASSETS,0.9362912400455062,"has curated licenses for some datasets. Their licensing guide can help determine the
743"
LICENSES FOR EXISTING ASSETS,0.9374288964732651,"license of a dataset.
744"
LICENSES FOR EXISTING ASSETS,0.9385665529010239,"• For existing datasets that are re-packaged, both the original license and the license of
745"
LICENSES FOR EXISTING ASSETS,0.9397042093287827,"the derived asset (if it has changed) should be provided.
746"
LICENSES FOR EXISTING ASSETS,0.9408418657565415,"• If this information is not available online, the authors are encouraged to reach out to
747"
LICENSES FOR EXISTING ASSETS,0.9419795221843004,"the asset’s creators.
748"
NEW ASSETS,0.9431171786120591,"13. New Assets
749"
NEW ASSETS,0.944254835039818,"Question: Are new assets introduced in the paper well documented and is the documentation
750"
NEW ASSETS,0.9453924914675768,"provided alongside the assets?
751"
NEW ASSETS,0.9465301478953356,"Answer: [Yes]
752"
NEW ASSETS,0.9476678043230944,"Justification: The new Julia library contains well documented code which has a clear and
753"
NEW ASSETS,0.9488054607508533,"accessible API. All our code for all experiments and applications is released under the CC
754"
NEW ASSETS,0.949943117178612,"BY 4.0 licence.
755"
NEW ASSETS,0.9510807736063709,"Guidelines:
756"
NEW ASSETS,0.9522184300341296,"• The answer NA means that the paper does not release new assets.
757"
NEW ASSETS,0.9533560864618885,"• Researchers should communicate the details of the dataset/code/model as part of their
758"
NEW ASSETS,0.9544937428896473,"submissions via structured templates. This includes details about training, license,
759"
NEW ASSETS,0.9556313993174061,"limitations, etc.
760"
NEW ASSETS,0.9567690557451649,"• The paper should discuss whether and how consent was obtained from people whose
761"
NEW ASSETS,0.9579067121729238,"asset is used.
762"
NEW ASSETS,0.9590443686006825,"• At submission time, remember to anonymize your assets (if applicable). You can either
763"
NEW ASSETS,0.9601820250284414,"create an anonymized URL or include an anonymized zip file.
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9613196814562003,"14. Crowdsourcing and Research with Human Subjects
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.962457337883959,"Question: For crowdsourcing experiments and research with human subjects, does the paper
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9635949943117179,"include the full text of instructions given to participants and screenshots, if applicable, as
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9647326507394767,"well as details about compensation (if any)?
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9658703071672355,"Answer: [NA]
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9670079635949943,"Justification: This work did not involve crowdsourcing nor research with human subjects.
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9681456200227532,"Guidelines:
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9692832764505119,"• The answer NA means that the paper does not involve crowdsourcing nor research with
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9704209328782708,"human subjects.
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9715585893060296,"• Including this information in the supplemental material is fine, but if the main contribu-
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726962457337884,"tion of the paper involves human subjects, then as much detail as possible should be
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9738339021615472,"included in the main paper.
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9749715585893061,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9761092150170648,"or other labor should be paid at least the minimum wage in the country of the data
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9772468714448237,"collector.
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9783845278725825,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9795221843003413,"Subjects
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9806598407281001,"Question: Does the paper describe potential risks incurred by study participants, whether
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.981797497155859,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9829351535836177,"approvals (or an equivalent approval/review based on the requirements of your country or
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9840728100113766,"institution) were obtained?
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852104664391353,"Answer: [NA]
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9863481228668942,"Justification: This work did not involve crowdsourcing nor research with human subjects.
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987485779294653,"Guidelines:
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9886234357224118,"• The answer NA means that the paper does not involve crowdsourcing nor research with
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897610921501706,"human subjects.
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908987485779295,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9920364050056882,"may be required for any human subjects research. If you obtained IRB approval, you
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9931740614334471,"should clearly state this in the paper.
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9943117178612059,"• We recognize that the procedures for this may vary significantly between institutions
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9954493742889647,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9965870307167235,"guidelines for their institution.
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9977246871444824,"• For initial submissions, do not include any information that would break anonymity (if
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9988623435722411,"applicable), such as the institution conducting the review.
798"
