Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0020491803278688526,"Recently it is shown that learning radiance fields with depth rendering and depth
1"
ABSTRACT,0.004098360655737705,"supervision can effectively promote the view synthesis quality and convergence.
2"
ABSTRACT,0.006147540983606557,"But this paradigm requires input RGB-D sequences to be synchronized, hindering
3"
ABSTRACT,0.00819672131147541,"its usage in the UAV city modeling scenario. To this end, we propose to jointly
4"
ABSTRACT,0.010245901639344262,"learn large-scale depth-regularized radiance fields and calibrate the mismatch
5"
ABSTRACT,0.012295081967213115,"between RGB-D frames. Although this joint learning problem can be simply
6"
ABSTRACT,0.014344262295081968,"addressed by adding new variables, we exploit the prior that RGB-D frames are
7"
ABSTRACT,0.01639344262295082,"actually sampled from the same physical trajectory. As such, we propose a novel
8"
ABSTRACT,0.018442622950819672,"time-pose function, which is an implicit network that maps timestamps to SE(3)
9"
ABSTRACT,0.020491803278688523,"elements. Our algorithm is designed in an alternative way consisting of three
10"
ABSTRACT,0.022540983606557378,"steps: (1) time-pose function fitting; (2) radiance field bootstrapping; (3) joint
11"
ABSTRACT,0.02459016393442623,"pose error compensation and radiance field refinement. In order to systematically
12"
ABSTRACT,0.02663934426229508,"evaluate under this new problem setting, we propose a large synthetic dataset with
13"
ABSTRACT,0.028688524590163935,"diverse controlled mismatch and ground truth. Through extensive experiments,
14"
ABSTRACT,0.030737704918032786,"we demonstrate that our method outperforms strong baselines. We also show
15"
ABSTRACT,0.03278688524590164,"qualitatively improved results on a real-world asynchronous RGB-D sequence
16"
ABSTRACT,0.03483606557377049,"captured by drones. Codes, data, and models will be made publicly available.
17"
INTRODUCTION,0.036885245901639344,"1
Introduction
18"
INTRODUCTION,0.0389344262295082,"Incorporating depth rendering and depth supervision into radiance fields has been demonstrated as
19"
INTRODUCTION,0.040983606557377046,"a helpful regularization technique in several recent studies [2, 21, 32, 20]. However, this technique
20"
INTRODUCTION,0.0430327868852459,"has not yet been successfully introduced into radiance field learning from UAV (Unmanned Aerial
21"
INTRODUCTION,0.045081967213114756,"Vehicle) images, despite it’s a typical choice in city modeling. A closer look at the aforementioned
22"
INTRODUCTION,0.0471311475409836,"works reveals that they assume synchronized RGB and depth signals, which is hard to guarantee in
23"
INTRODUCTION,0.04918032786885246,"UAV vision due to the lack of suitable synchronized sensors for long sensing ranges. So we study the
24"
INTRODUCTION,0.05122950819672131,"problem of learning depth-regularized radiance fields from asynchronous RGB-D sequences.
25"
INTRODUCTION,0.05327868852459016,"As a recap, the canonical radiance field [15] learns a neural network parameterized by θ that represents
26"
INTRODUCTION,0.055327868852459015,"a 3D scene, from input images I and their intrinsic/extrinsic parameters TI. To alleviate the reliance
27"
INTRODUCTION,0.05737704918032787,"on TI, some works [30, 11, 8] aim to resolve a different problem that self-calibrates TI. In other
28"
INTRODUCTION,0.05942622950819672,"words, they jointly learn θ and TI from input images I. Similarly, the formulation considered here is
29"
INTRODUCTION,0.06147540983606557,"to learn scene representation θ, camera parameters TI and TD from inputs images I and depths D.
30"
INTRODUCTION,0.06352459016393443,"It is natural to develop the joint learning formulation to resolve the problem, as it amounts to adding
31"
INTRODUCTION,0.06557377049180328,"new parameters to existing methods [30, 11, 8]. However, an important prior is ignored that RGB-D
32"
INTRODUCTION,0.06762295081967214,"frames are actually sampled from the same physical trajectory. As conceptually shown in Fig. 1-a/b,
33"
INTRODUCTION,0.06967213114754098,"TI and TD can be considered as samples from a function that maps timestamps to SE(3) elements.
34"
INTRODUCTION,0.07172131147540983,"We name this function as time-pose function and model it with a neural network parameterized by
35"
INTRODUCTION,0.07377049180327869,"ϕ. As such, we address the problem with a new formulation that learns scene representation θ and
36 RGB D
RGB D"
INTRODUCTION,0.07581967213114754,Problem of
INTRODUCTION,0.0778688524590164,"Interests t1 t2
t3 t4"
INTRODUCTION,0.07991803278688525,"(c)
(d)"
INTRODUCTION,0.08196721311475409,"Our 
Solution"
INTRODUCTION,0.08401639344262295,"(a)
(b)
(e)"
INTRODUCTION,0.0860655737704918,"Figure 1: The problem of interest is to learn a depth-regularized radiance field using asynchronous
RGB-D sequences (a). We proposed a time-pose function as conceptually shown in (b) to leverage
the prior that RGB-D seuqnces are actually sampled from the same physical underlying trajectory.
For a novel view (c), our method can render a better depth map (d) than Mega-NeRF (e)."
INTRODUCTION,0.08811475409836066,"time-pose function ϕ from inputs RGB images I and depths D. An interesting fact is that both θ and
37"
INTRODUCTION,0.09016393442622951,"ϕ are implicit neural representation networks (or say coordinate-based networks) that allow fully
38"
INTRODUCTION,0.09221311475409837,"differentiable training. To our knowledge, this new formulation has not been proposed before.
39"
INTRODUCTION,0.0942622950819672,"We also propose an effective learning scheme designed in an alternative manner. In the first stage, we
40"
INTRODUCTION,0.09631147540983606,"fit the time-pose function ϕ using one modality (e.g., RGB images) and infer the poses of the other,
41"
INTRODUCTION,0.09836065573770492,"using a balanced pose regression loss and a speed regularization term. Secondly, we bootstrap a large-
42"
INTRODUCTION,0.10040983606557377,"scale radiance field θ based upon Mega-NeRF [29] using the outputs of the trained time-pose function.
43"
INTRODUCTION,0.10245901639344263,"Thanks to the first step, depth regularization can be imposed here in spite of RGB-D misalignment.
44"
INTRODUCTION,0.10450819672131148,"Finally, thanks to the cascade of two fully differentiable implicit representation networks, we jointly
45"
INTRODUCTION,0.10655737704918032,"optimize the 3D scene representation θ and compensate pose errors by updating ϕ.
46"
INTRODUCTION,0.10860655737704918,"Since the problem considered is new, we contribute a synthetic dataset (named AUS) for systematic
47"
INTRODUCTION,0.11065573770491803,"evaluation. Using six large-scale 3D scenes, realistic drone trajectories of different difficulty levels
48"
INTRODUCTION,0.11270491803278689,"are generated. Specifically speaking, simple trajectories are heuristically designed with a zig-zag
49"
INTRODUCTION,0.11475409836065574,"pattern while complicated ones are generated by manual control signals in simulation. We also
50"
INTRODUCTION,0.1168032786885246,"control the mismatch between RGB-D sequences using different protocols, to cover as many as
51"
INTRODUCTION,0.11885245901639344,"possible scenarios that the algorithm may encounter in reality. Through a set of comprehensive
52"
INTRODUCTION,0.12090163934426229,"experiments, we show the proposed method outperforms several state-of-the-art counterparts and
53"
INTRODUCTION,0.12295081967213115,"our design choices contribute positively to performance. Last but not least, we present a real-world
54"
INTRODUCTION,0.125,"evaluation using asynchronous sensors on drones. Our depth rendering results (on unseen viewpoint)
55"
INTRODUCTION,0.12704918032786885,"is shown in Fig. 1-d, which is much better than the result of Mega-NeRF shown in Fig. 1-e. This
56"
INTRODUCTION,0.1290983606557377,"success is credited to the usage of depth regularization as made possible by our novel algorithm.
57"
INTRODUCTION,0.13114754098360656,"To summarize, we have the following contributions in this paper: (1) We formalize the new problem
58"
INTRODUCTION,0.13319672131147542,"of learning depth-regularized radiance fields from asynchronous RGB-D sequences, which is rooted
59"
INTRODUCTION,0.13524590163934427,"in UAV city modeling. (2) We identify an important domain-specific prior in this problem: RGB-D
60"
INTRODUCTION,0.13729508196721313,"frames are sampled from the same underlying trajectory. We instantiate this prior into a novel time-
61"
INTRODUCTION,0.13934426229508196,"pose function and develop a cascaded fully differentiable implicit representation network. (3) In order
62"
INTRODUCTION,0.1413934426229508,"to systematically study the problem, we contribute a photo-realistically rendered synthetic dataset
63"
INTRODUCTION,0.14344262295081966,"that simulates different types of mismatch. (4) Through a comprehensive benchmarking on this new
64"
INTRODUCTION,0.14549180327868852,"dataset and real-world asynchronous RGB-D sequences, we demonstrate that our method can promote
65"
INTRODUCTION,0.14754098360655737,"performance over strong prior arts. Anonynous code: https://anonymous.4open.science/r/async-nerf
66"
RELATED WORKS,0.14959016393442623,"2
Related Works
67"
RELATED WORKS,0.15163934426229508,"Large-scale Radiance Fields. Neural Radiance Field (NeRF) [15] has shown impressive results in
68"
RELATED WORKS,0.15368852459016394,"neural reconstruction and rendering. However, its capacity to model large-scale unbounded 3D scenes
69"
RELATED WORKS,0.1557377049180328,"is limited. Several strategies [29, 26, 32, 14, 35] have been proposed to address this limitation, with a
70"
RELATED WORKS,0.15778688524590165,"common principle of dividing large scenes into blocks or decomposing the scene into multiple levels.
71"
RELATED WORKS,0.1598360655737705,"Block-NeRF [26] clusters images by dividing the whole scene according to street blocks. Mega-NeRF
72"
RELATED WORKS,0.16188524590163936,"[29] utilizes a clustering algorithm that partitions sampled 3D points into different NeRF submodules.
73"
RELATED WORKS,0.16393442622950818,"BungeeNeRF [32] trains NeRFs using a growing model of residual blocks with predefined multiple
74"
RELATED WORKS,0.16598360655737704,"scales of data. Switch-NeRF [14] designs a gating network to jointly learn the scene decomposition
75"
RELATED WORKS,0.1680327868852459,"and NeRFs without any priors of 3D scene shape or geometric distribution. However, these prior
76"
RELATED WORKS,0.17008196721311475,"works fail to leverage the rich geometric information in depth images for effective regularization.
77"
RELATED WORKS,0.1721311475409836,"Depth-regularized Radiance Fields. Volumetric rendering requires extensive samples and sufficient
78"
RELATED WORKS,0.17418032786885246,"views to effectively differentiate between empty space and opaque surfaces. Depth maps can
79"
RELATED WORKS,0.1762295081967213,"serve as geometric cues, providing regularization constraints and sampling prior, which accelerates
80"
RELATED WORKS,0.17827868852459017,"NeRF’s convergence towards the correct geometry. DS-NeRF [3] enhances this process using depth
81"
RELATED WORKS,0.18032786885245902,"supervision from 3D point clouds, estimated by structure-from-motion, and a specific loss for rendered
82"
RELATED WORKS,0.18237704918032788,"ray termination distribution. Mono-SDF [38] and Dense-Depth Prior [21] further supplement this
83"
RELATED WORKS,0.18442622950819673,"with a pretrained dense monocular depth estimator for less-observed and textureless areas. To adapt
84"
RELATED WORKS,0.1864754098360656,"NeRF for outdoor scenarios, URF [20] rasterizes a pre-built LiDAR point cloud map to generate
85"
RELATED WORKS,0.1885245901639344,"dense depth images and alleviates floating elements by penalizing floaters in the free space. Moreover,
86"
RELATED WORKS,0.19057377049180327,"S-NeRF [34] completes depth on sparse LiDAR point clouds using a confidence map, effectively
87"
RELATED WORKS,0.19262295081967212,"handling street-view scenes with limited perspectives. However, those methods are not readily
88"
RELATED WORKS,0.19467213114754098,"applicable to UAV captured images due to the lack of suitable synchronized sensors for long ranges.
89"
RELATED WORKS,0.19672131147540983,"Broader UAV Vision and Synchronization. Like autonomous driving, UAV vision is drawing
90"
RELATED WORKS,0.1987704918032787,"increasing attention due to its unique characteristics. Broader UAV vision covers many topics like
91"
RELATED WORKS,0.20081967213114754,"counting [31][7], trajectory forecasting [18], intention prediction [33], object tracking [16], physics
92"
RELATED WORKS,0.2028688524590164,"understanding [39], next-best-view prediction [6], 3D reconstruction [41], and calibration [19].
93"
RELATED WORKS,0.20491803278688525,"Sensor synchronization is challenging for UAV vision (and other settings) and several works address
94"
RELATED WORKS,0.2069672131147541,"the problem from an algorithmic perspective. One possibility is to adopt tailored hardware designs or
95"
RELATED WORKS,0.20901639344262296,"software protocols [1] to synchronize all the devices. Another branch of sensor-agnostic methods
96"
RELATED WORKS,0.21106557377049182,"utilizes temporal priors by using Sum-of-Gaussians [4] or parametric interpolation functions [36].
97 98 (i)"
RELATED WORKS,0.21311475409836064,"(ii)
(iii)"
RELATED WORKS,0.2151639344262295,"Figure 2: Three-step Optimization. (i) A time-pose function parameterized by ϕ is trained to predict
camera poses from timestamps; (ii) The neural radiance field parameterized by θ is bootstrapped with
pure RGB losses; (iii) Both of the parameters θ, ϕ are jointly optimized with RGB-D supervision."
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.21721311475409835,"3
Problem Formulation and Optimization Pipline
99"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.2192622950819672,"Problem & Challenge. Our goal is to learn a neural radiance field parameterized by θ for large-scale
100"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.22131147540983606,"scene representation from UAV images as done in prior works [29, 32]. However, these prior works
101"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.22336065573770492,"fail to leverage depth supervision, which is known [3, 21] as useful for training floater-less NeRFs.
102"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.22540983606557377,"To our knowledge, there are no easily accessible synchronized RGB-D sensor suites for large-scale
103"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.22745901639344263,"outdoor scenes, and trivially synchronizing them according to timestamp1 cannot fully address the
104"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.22950819672131148,"misalignment issue. Instead of using expensive hardware, we take an algorithmic perspective.
105"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.23155737704918034,"Input & output. There are some prior works on large-scale scene modeling using aerial images
106"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.2336065573770492,"[32, 29, 5, 6]. In this study, we assume an input RGB-D stream captured by drones: a set of
107"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.23565573770491804,"RGB camera images {I(i)}NI
i=1 and a set of depth maps {D(j)}ND
j=1 (shown in Fig. 1-a) and we aim
108"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.23770491803278687,"to recover the spatiotemporal transformations between them. Given that our focus is on relative
109"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.23975409836065573,"transformation, it is viable to consider either the RGB or the depth stream as the reference without
110"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.24180327868852458,"compromising generality. For convenience, we assume a set of camera poses {T (i)
I
}NI
i=1 for color
111"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.24385245901639344,"images are obtained by an SfM algorithm. The neural scene representation parameterized by θ
112"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.2459016393442623,"outputs an image ˆI as well as a depth map ˆD at a given perspective camera pose T .
113"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.24795081967213115,"Observation. Note that all the sensor data are captured with a drone on the same trajectory2, we
114"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.25,"can model the relationship between capture time t and sensor poses T with an implicit time-pose
115"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.2520491803278688,"function as ϕ : t →ˆT = [ ˆx, ˆq ], where t is the timestamp of capture, and the estimated pose ˆT is
116"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.2540983606557377,"represented by a translation vector ˆx ∈R3 and a quaternion ˆq ∈R4.
117"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.25614754098360654,"Pipeline overview. We formulate our method as a 3-step optimization problem (as shown in Fig. 2).
118"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.2581967213114754,"First, since the time-pose relationship for the RGB captures are given, we can train a time-pose
119"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.26024590163934425,"1The so-called soft synchronization.
2but they are not necessarily synchronized in terms of acquisition time."
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.26229508196721313,"MLP
MLP"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.26434426229508196,"Depth Sensor 
Pose"
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.26639344262295084,Color Loss
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.26844262295081966,Depth Loss
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.27049180327868855,Translation Loss
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.2725409836065574,Rotation Loss Speed
PROBLEM FORMULATION AND OPTIMIZATION PIPLINE,0.27459016393442626,"Loss
Missing modality"
-D MULTI-RESOLUTION,0.2766393442622951,1-D Multi-resolution
-D MULTI-RESOLUTION,0.2786885245901639,Feature Grid MLP
-D MULTI-RESOLUTION,0.2807377049180328,Hash & Interp
-D MULTI-RESOLUTION,0.2827868852459016,Pose Error
-D MULTI-RESOLUTION,0.2848360655737705,"TIME POSE
FUNCTION"
-D MULTI-RESOLUTION,0.28688524590163933,Error Compensation by Joint Optimization
-D MULTI-RESOLUTION,0.2889344262295082,GT/Predicted Trajectory
-D MULTI-RESOLUTION,0.29098360655737704,Bootstrapping
-D MULTI-RESOLUTION,0.2930327868852459,"Figure 3: Method Pipeline. The time-pose function is modeled using a 1-D multi-resolution hash
grid with direct and speed losses. After bootstrapping the scene representation networks with pure
RGB signals, the predicted depth sensor poses are used for jointly optimizing the NeRFs’ parameters
θ. At each timestamp (ti from RGB sequence or tj from depth sequence), only one modality of
sensor signals is provided, thus only one loss term is activated (shown on the right)."
-D MULTI-RESOLUTION,0.29508196721311475,"function on the RGB sequence (Fig. 2-(i)). Then, to train the neural radiance field, we first bootstrap
120"
-D MULTI-RESOLUTION,0.29713114754098363,"the network with pure RGB supervision (Fig. 2-(ii)). To further enable training with RGB-D
121"
-D MULTI-RESOLUTION,0.29918032786885246,"supervision, we can use the previously trained time-pose function to estimate the corresponding depth
122"
-D MULTI-RESOLUTION,0.3012295081967213,"camera poses {T (j)
D } of the depth timestamps {t(j)
D }. Since both of the networks are differentiable,
123"
-D MULTI-RESOLUTION,0.30327868852459017,"we jointly optimize the networks in an end-to-end manner in the third stage (Fig. 2-(iii)).
124"
METHOD,0.305327868852459,"4
Method
125"
METHOD,0.3073770491803279,"We introduce in Section 4.1 the details of learning an implicit time-pose function. In Section 4.2, we
126"
METHOD,0.3094262295081967,"describe our neural scene representation networks and the bootstrapping strategy. In Section 4.3, we
127"
METHOD,0.3114754098360656,"adopt depth supervision and jointly train the time-pose function with RGB-D pairs.
128"
TIME-POSE FUNCTION,0.3135245901639344,"4.1
Time-Pose Function
129"
TIME-POSE FUNCTION,0.3155737704918033,"We represent the camera trajectory as an implicit time-pose function ϕ whose input is a timestamp t,
130"
TIME-POSE FUNCTION,0.3176229508196721,"and whose output is a 6-DoF pose T that consists of a 3-D translation xi and a 4-D quaternion qi.
131"
TIME-POSE FUNCTION,0.319672131147541,"Network Overview The time-pose function (shown in the left part of Fig. 3) is approximated with
132"
TIME-POSE FUNCTION,0.32172131147540983,"a compact 1-D multi-resolution hash grid {G(l)}L
l=1, followed by an MLP decoder. The hash grid
133"
TIME-POSE FUNCTION,0.3237704918032787,"consists of L levels of separate feature grids with trainable hash encodings [17]. The reason why we
134"
TIME-POSE FUNCTION,0.32581967213114754,"choose this architecture is as follows: The time-pose function is a coordinate-based function that may
135"
TIME-POSE FUNCTION,0.32786885245901637,"contain coarse and fine-level feature components 3[27], and this architecture allows us to sample the
136"
TIME-POSE FUNCTION,0.32991803278688525,"hash encodings from each grid layer with different resolutions and perform quadratic interpolation on
137"
TIME-POSE FUNCTION,0.3319672131147541,"the extracted encodings to obtain a feature vector Vi when querying a specific timestamp t that is in
138"
TIME-POSE FUNCTION,0.33401639344262296,"the range of all timestamps. This design choice is also empirically validated in Table. 3.
139"
TIME-POSE FUNCTION,0.3360655737704918,"After obtaining the interpolated feature vector, an MLP with two separated decoder heads is used
140"
TIME-POSE FUNCTION,0.33811475409836067,"to predict the output translation ˆxi and rotation ˆqi vectors respectively. The forward pass can be
141"
TIME-POSE FUNCTION,0.3401639344262295,"expressed in the following equations:
142"
TIME-POSE FUNCTION,0.3422131147540984,"Vi = FMLP
 
concat{interp(h(t; πl), Gl
θ}L
l=1; ΦMLP

,
(1)
ˆTi = [ˆxi, ˆqi] = ltrans(Vi; Φtrans), lrot(Vi; Φrot),
(2)
where interp denotes interpolation, h is the hash function parameterized by πl, FMLP, ltrans, lrot are the
143"
TIME-POSE FUNCTION,0.3442622950819672,"MLP networks and the decoder heads, with ΦMLP, Φtrans, Φrot representing their parameters.
144"
TIME-POSE FUNCTION,0.3463114754098361,"Depth-pose Prediction Since both the depth maps and the RGB images are collected by the same
145"
TIME-POSE FUNCTION,0.3483606557377049,"drone on the same flight, they cover the same spatial-temporal footprints except for the difference
146"
TIME-POSE FUNCTION,0.35040983606557374,"in the placement of the two sensors on the aircraft. For every depth frame, we first predict the
147"
TIME-POSE FUNCTION,0.3524590163934426,"RGB camera pose using the capture timestamps of the depth sensor with the time-pose function
148"
TIME-POSE FUNCTION,0.35450819672131145,"then transform the predicted RGB camera pose to the depth sensor pose with a pre-calibrated pose
149"
TIME-POSE FUNCTION,0.35655737704918034,"transformation TRGB→D between sensors.
150"
TIME-POSE FUNCTION,0.35860655737704916,"To optimize the Time-Pose Function, we propose the following objective function:
151"
TIME-POSE FUNCTION,0.36065573770491804,"L = λtransLtrans + λrotLrot + λspeedLspeed,
(3)"
TIME-POSE FUNCTION,0.36270491803278687,3This is shown by the ground truth trajectory in Fig. 4 and the predicted trajectory in the supplementary.
TIME-POSE FUNCTION,0.36475409836065575,"where Ltrans, Lrot, Lspeed are translation, rotation and speed losses respectively as shown in the left
152"
TIME-POSE FUNCTION,0.3668032786885246,"panel of Fig. 3. and λtrans, λrot, λspeed are the weighting parameters. Note that λtrans and λrot are
153"
TIME-POSE FUNCTION,0.36885245901639346,"automatically adjusted as explained in a later paragraph.
154"
TIME-POSE FUNCTION,0.3709016393442623,"Pose Representation. There are some common choices to represent rotation for optimizing camera
155"
TIME-POSE FUNCTION,0.3729508196721312,"poses like rotation matrices [37] or Euler-angles [25, 28]. However, they are not continuous for
156"
TIME-POSE FUNCTION,0.375,"representing rotation [42] due to their non-homeomorphic representation space to SO(3). We choose
157"
TIME-POSE FUNCTION,0.3770491803278688,"to use unit quaternion as our raw rotation representation because arbitrary 4-D vectors can be easily
158"
TIME-POSE FUNCTION,0.3790983606557377,"mapped to legitimate rotations by normalizing them to the unit length [10].
159"
TIME-POSE FUNCTION,0.38114754098360654,"Optimization of Translation and Rotation. We optimize the translation and the rotation vectors by
160"
TIME-POSE FUNCTION,0.3831967213114754,"minimizing the mean square error (MSE) between the estimated and ground-truth camera poses:
161"
TIME-POSE FUNCTION,0.38524590163934425,"Ltrans = 1 n n
X"
TIME-POSE FUNCTION,0.38729508196721313,"i=1
(xi −ˆxi)2, Lrot = 1 n n
X"
TIME-POSE FUNCTION,0.38934426229508196,"i=1
(qi −ˆqi)2.
(4)"
TIME-POSE FUNCTION,0.39139344262295084,"Since x and q are in different units, the scaling factor λtrans and λrot play an important role in balancing
162"
TIME-POSE FUNCTION,0.39344262295081966,"the losses. To prevent translation and rotation from negatively influencing each other in training and to
163"
TIME-POSE FUNCTION,0.39549180327868855,"tap into possible mutual facilitation, we make the weighting factors learnable by using homoscedastic
164"
TIME-POSE FUNCTION,0.3975409836065574,"uncertainty [9] as Lσ = Ltrans exp(−ˆstrans) + ˆstrans + Lrot exp(−ˆsrot) + ˆsrot, where ˆs are learnable
165"
TIME-POSE FUNCTION,0.39959016393442626,"parameters, thus the loss terms are balanced during training course4.
166"
TIME-POSE FUNCTION,0.4016393442622951,"Optimization of Motion Speed. Observing that the time-pose function is essentially a function of
167"
TIME-POSE FUNCTION,0.4036885245901639,"translational displacement and angular displacement with respect to time, we can use the average
168"
TIME-POSE FUNCTION,0.4057377049180328,"linear velocity5 to supervise the gradient of the network output, with regard to the input vectors.
169"
TIME-POSE FUNCTION,0.4077868852459016,"Since the linear velocity variation is small and the angular velocity variation is relatively larger in the
170"
TIME-POSE FUNCTION,0.4098360655737705,"scenes captured by the drone, only the average linear velocity is used to supervise the neural network
171"
TIME-POSE FUNCTION,0.41188524590163933,"and the latter is not supervised in our method:
172"
TIME-POSE FUNCTION,0.4139344262295082,"Lspeed = MSE(v(ti), ˆv(ti)) = 1 n n
X"
TIME-POSE FUNCTION,0.41598360655737704,"i=1
(v(ti) −∂ˆx"
TIME-POSE FUNCTION,0.4180327868852459,"∂t (ti))2, v(ti) = ∂x ∂t"
TIME-POSE FUNCTION,0.42008196721311475,"t=ti
≈xi −xi−1"
TIME-POSE FUNCTION,0.42213114754098363,"ti −ti−1
(5)"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.42418032786885246,"4.2
Bootstrapping Large-scale Neural Radiance Fields
173"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4262295081967213,"In this part, we introduce our proposed scene representation (right half of Fig. 3) that is bootstrapped
174"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.42827868852459017,"in the second phase of the optimization process (Fig. 2-(ii)). Due to the limited capacity of MLPs,
175"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.430327868852459,"we follow Mega-NeRF[29] and partition the scene map into a series of equal-sized blocks in terms
176"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4323770491803279,"of spatial scope, and each block learns its individual scene representation with an implicit field. In
177"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4344262295081967,"this stage, we optimize the scene representation with pure RGB data. Specifically, the radiance field
178"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4364754098360656,"is denoted as {f (i)
NeRF}Nx×Ny
i=1
, where Nx, Ny denotes the spatial grid size. Each implicit function
179"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4385245901639344,"represents a geographic region with xcentroid
i
as its centroid. The kth scene model can be written as:
180"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4405737704918033,"f (k)
NeRF(γ(xpts), γ(d)) →(ˆc, σ),
(6)"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4426229508196721,"where k = arg min
j
||xpts −xcentroid
j
||2 and γ is the positional encoding function.
181"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.444672131147541,"For view synthesis, we adopt volume rendering techniques to synthesize color image ˆI and depth
182"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.44672131147540983,"map ˆD. To be specific, we sample a set of points for each emitted camera ray in a coarse-to-fine
183"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4487704918032787,"manner [15] and accumulate the radiance and the distance along the corresponding ray to calculate the
184"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.45081967213114754,"rendered color ˆI and depth ˆD. To obtain the radiance of a spatial point xpts, we use the nearest scene
185"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.45286885245901637,"model for prediction. A set of per-image appearance embedding [13] is also optimized simultaneously
186"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.45491803278688525,"in the training.
187"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4569672131147541,"ˆI(o, d) =
Z far"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.45901639344262296,"near
T(t)σ(k)(x(t)) · c(k)(x(t), d)dt, ˆD(o, d) =
Z far"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4610655737704918,"near
T(t)σ(k)(x(t)) · tdt,
(7)"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.46311475409836067,"where o and d denote the position and orientation of the sampled ray, x(t) = o + td represents
188"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4651639344262295,"the sampled point coordinates in the world space, and T(t) = exp

−
R t
near σ(k)(x(s))ds

is the
189"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4672131147540984,"4Manual selection of weights requires laborious tuning, but comparable performance can be achieved.
5Note that the average velocity refers to the mean value calculated from the ground-truth camera pose of the
current frame and the two adjacent frames, rather than the average value in the whole sequence."
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4692622950819672,"accumulated transmittance. We optimize the scene representation model with only the photometric
190"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4713114754098361,"error as Lbootstrarp = MSE(I, ˆI). We empirically observe that this bootstrapping is critical to the
191"
BOOTSTRAPPING LARGE-SCALE NEURAL RADIANCE FIELDS,0.4733606557377049,"challenging third stage which jointly learns θ and ϕ using asynchronous RGB-D data.
192"
JOINT OPTIMIZATION,0.47540983606557374,"4.3
Joint Optimization
193"
JOINT OPTIMIZATION,0.4774590163934426,"While the time-pose function learns a good initialization from the RGB sequence, there are still errors
194"
JOINT OPTIMIZATION,0.47950819672131145,"to be compensated. In this section, we describe how we perform simultaneous mapping and pose
195"
JOINT OPTIMIZATION,0.48155737704918034,"optimization, which compensates for the initial error of the time-pose function.
196"
JOINT OPTIMIZATION,0.48360655737704916,"We jointly optimize the inaccurate camera poses and the implicit maps: when fitting parameters
197"
JOINT OPTIMIZATION,0.48565573770491804,"Θ(k)
NeRF of the scene representation, the estimated depth camera poses ˆT (j)
D
∈SE(3) (where t ∈R3
198"
JOINT OPTIMIZATION,0.48770491803278687,"and q ∈SO(3)) will be simultaneously optimized on the manifold:
199"
JOINT OPTIMIZATION,0.48975409836065575,"θ, { ˆTD} = argmin
θ,T ∈SE(3)
L({I(i)}, {D(j)} | θ, { ˆTD}),
(8)"
JOINT OPTIMIZATION,0.4918032786885246,"where L is the objective function we demonstrate in the next paragraph.
200"
JOINT OPTIMIZATION,0.49385245901639346,"To train the implicit representation to obtain photo-realistic RGB rendering maps and accurate depth
201"
JOINT OPTIMIZATION,0.4959016393442623,"map estimation, we update the mapping losses as:
202"
JOINT OPTIMIZATION,0.4979508196721312,"L = λcolor
X"
JOINT OPTIMIZATION,0.5,"i
MSE(I(i), ˆI(i)) + λdepth(α)
X"
JOINT OPTIMIZATION,0.5020491803278688,"j
MSE(D(j), ˆD(j)),
(9)"
JOINT OPTIMIZATION,0.5040983606557377,"where λcolor and λdepth(α) are weighting hyper-parameters for color and depth loss, in which the
203"
JOINT OPTIMIZATION,0.5061475409836066,"depth loss weight starts to grow from zero gradually with the training process α.
204"
JOINT OPTIMIZATION,0.5081967213114754,"To compensate for the error from the time-pose function extracted poses, we jointly optimize two
205"
JOINT OPTIMIZATION,0.5102459016393442,"implicit representation networks thanks to the end-to-end differentiable nature.
206"
JOINT OPTIMIZATION,0.5122950819672131,"5
Asynchronous Urban Scene (AUS) Dataset
207"
JOINT OPTIMIZATION,0.514344262295082,YOSVRK NGXJ
JOINT OPTIMIZATION,0.5163934426229508,SGT[GR
JOINT OPTIMIZATION,0.5184426229508197,YOSVRK NGXJ
JOINT OPTIMIZATION,0.5204918032786885,SGT[GR
JOINT OPTIMIZATION,0.5225409836065574,"B
""64/FX:PSL
	C
""644BO'SBODJTDP
	D
""644NBMM"
JOINT OPTIMIZATION,0.5245901639344263,"(XOJMK
:U]T"
INUUR,0.5266393442622951,"9INUUR
)GYZRK"
INUUR,0.5286885245901639,RGBD sequence
INUUR,0.5307377049180327,Sampled RGB
INUUR,0.5327868852459017,"Sampled Depth 
with fixed offset"
INUUR,0.5348360655737705,"Y
Y
Y"
INUUR,0.5368852459016393,RGBD sequence
INUUR,0.5389344262295082,Sampled RGB
INUUR,0.5409836065573771,"Sampled Depth 
with random offset"
INUUR,0.5430327868852459,"Y
Z
Y"
INUUR,0.5450819672131147,"E
3FTBNQMFXJUIGJYFEPGGTFU
	F
3FTBNQMFXJUISBOEPNPGGTFU"
INUUR,0.5471311475409836,"Figure 4: We propose a photo-realistically rendered dataset named Asynchronous Urban Scene (AUS)
for evaluation. (a/b) are large-scale city scenes designed according to New York and San Francisco
while (c) is (relatively) small-scale scenes provided by UrbanScene3D. Drone trajectories of different
difficulty levels are visualized in (a-c). On these trajectories, we first capture an RGB-D sequence
with an enough high framerate. Then we exploit two resampling strategies: fixed offset (d) and
random offset (e). x equals 30 in (d) for every RGB-D pair. x equals 30 while y equals 50 in (e)."
INUUR,0.5491803278688525,"Dataset Collection. Our Asynchronous Urban Scene (AUS) dataset as illustrated in Fig.4 is generated
208"
INUUR,0.5512295081967213,"using Airsim [23], a simulator plug-in for Unreal Engine. With 3D city models loaded in Unreal
209"
INUUR,0.5532786885245902,"Engine, the simulator can output photorealistic and high-resolution RGB images with synchronized
210"
INUUR,0.555327868852459,"depth images (resampled later) according to the a drone trajectory and a capture framerate. We choose
211"
INUUR,0.5573770491803278,"Airsim as it strikes a good balance between rendering quality and dynamics modeling flexibility.
212"
INUUR,0.5594262295081968,"Scene
Method
PSNR ↑
SSIM ↑
LPIPS ↓
RMSE ↓
RMSE log ↓
δ1(%) ↑
δ2(%) ↑
δ3(%) ↑
NeRF-W
23.32
0.8105
0.2249
17.40
0.2630
80.17
90.11
94.72
Mega-NeRF
23.53
0.8375
0.1920
23.99
0.2943
80.11
88.77
92.87
NY
Mean
Ours
24.33
0.8346
0.1833
6.15
0.0816
94.85
98.23
99.22
NeRF-W
19.21
0.6610
0.3632
24.93
0.1877
81.81
91.54
96.93
Mega-NeRF
20.53
0.7334
0.2619
23.56
0.1713
88.58
94.74
96.83
SF
Mean
Ours
22.14
0.7930
0.2620
7.64
0.0789
96.34
98.80
99.70
NeRF-W
26.79
0.8053
0.2438
131.88
1.2277
53.76
61.57
58.98
Mega-NeRF
27.98
0.8674
0.1548
120.41
1.3246
69.10
72.54
73.17
Bridge
Ours
29.06
0.8751
0.1952
26.56
0.3248
93.24
96.32
98.26
NeRF-W
21.32
0.6208
0.4088
132.70
1.4640
44.89
55.68
57.90
Mega-NeRF
24.69
0.7305
0.3103
129.50
1.4240
54.54
59.18
57.90
Town
Ours
25.32
0.7675
0.2631
15.61
0.4632
91.92
96.89
98.49
NeRF-W
19.69
0.5715
0.4453
88.83
0.9365
61.73
72.58
75.80
Mega-NeRF
25.57
0.7739
0.3191
63.10
0.7651
77.18
85.02
86.69
School
Ours
26.51
0.7971
0.3175
21.19
0.2083
92.87
95.78
97.51
NeRF-W
22.63
0.7443
0.2557
78.18
0.8651
75.72
79.26
81.11
Mega-NeRF
28.06
0.9053
0.1159
54.99
0.6167
79.69
83.59
87.43
Castle
Ours
28.21
0.8976
0.1113
16.66
0.3565
93.12
97.23
98.45
NeRF-W
21.80
0.7156
0.3118
55.86
0.5846
72.20
81.40
84.88
Mega-NeRF
23.85
0.7990
0.2262
51.06
0.5527
78.66
85.09
87.43
24.85
0.8220
0.2223
12.14
0.1834
94.47
97.73
98.95
Mean"
INUUR,0.5614754098360656,"Ours
(+1.00)
(+0.0230)
(-0.0039)
(-38.92)
(-0.3693)
(+15.11)
(+12.64)
(+11.52)
Table 1: We quantitatively evaluate our method on the AUS dataset. Our method can synthesize more
realistic images and more accurate depth maps than the baseline methods. For the NY and SF, we
only report the mean performances on all sequences (Simple / Hard / Manual) due to limited space
and more detailed results are in the supplementary materials."
INUUR,0.5635245901639344,"3D City Scene Models. To generate the AUS dataset, we exploit a total of six scene models, covering
213"
INUUR,0.5655737704918032,"two large-scale ones shown in Fig. 4-a/b and four (relatively) small-scale ones shown in Fig. 4-c. The
214"
INUUR,0.5676229508196722,"former uses the New York and San Francisco city scenes provided by Kirill Sibiriakov [24], in which
215"
INUUR,0.569672131147541,"AUS-NewYork covers a 250 × 150m2 area with many detailed buildings and AUS-SanFrancisco
216"
INUUR,0.5717213114754098,"consists of a 500×250m2 area near the Golden Gate Bridge. The latter uses four model files provided
217"
INUUR,0.5737704918032787,"in the UrbanScene3D dataset [12]. As such, at the scene level, AUS features a good coverage of both
218"
INUUR,0.5758196721311475,"large-scale modern cities and smaller cultural heritage sites.
219"
INUUR,0.5778688524590164,"Trajectory Generation. Trajectory complexity matters for our problem. On one hand, in many
220"
INUUR,0.5799180327868853,"real-world applications, photographers may manually control drones to capture a city. On the other
221"
INUUR,0.5819672131147541,"hand, simple trajectories can be modeled by simple functions, rendering the neural time-pose function
222"
INUUR,0.5840163934426229,"unnecessary. To build a meaningful and comprehensive benchmark, we use three types of trajectories:
223"
INUUR,0.5860655737704918,"a trivial Zig-Zag trajectory (simple in Fig. 4), a more complex randomly generated trajectory (hard in
224"
INUUR,0.5881147540983607,"Fig. 4), and a very complex manually controlled trajectory (manual in Fig. 4). In AUS-Small, we
225"
INUUR,0.5901639344262295,"only provide manually controlled trajectories, since the scene sizes are relatively small and using the
226"
INUUR,0.5922131147540983,"former two trajectory strategies leads to an unrealistically large overlap between frames.
227"
INUUR,0.5942622950819673,"Mismatch Resampling. We first sample synchronized RGB-D sequences in the simulator at a high
228"
INUUR,0.5963114754098361,"frequency (50fps) then re-sample RGB and depth images with various offsets to create asynchronous
229"
INUUR,0.5983606557377049,"RGB-D sequences. As shown in Fig. 4-d/e, we exploit two settings for the AUS dataset. In Fig. 4-d,
230"
INUUR,0.6004098360655737,"every RGB-D pair is resampled according to a fixed offset denoted by x%. For example, we sample
231"
INUUR,0.6024590163934426,"the RGB image at 5fps or say every 10 frames and x = 30 means every depth image is 3 frames
232"
INUUR,0.6045081967213115,"later than the RGB counterpart. In Fig. 4-e, the offset between an RGB-D pair is randomly selected,
233"
INUUR,0.6065573770491803,"simulating a challenging real-world asynchronous sequence. Offset ablation will be shown later.
234"
EXPERIMENTS,0.6086065573770492,"6
Experiments
235"
EXPERIMENTS,0.610655737704918,"In this section, we show the effectiveness of our 3-step optimization pipeline by qualitatively and
236"
EXPERIMENTS,0.6127049180327869,"quantitatively evaluating our proposed methods and comparing with baseline methods.
237"
RESULTS,0.6147540983606558,"6.1
Results
238"
RESULTS,0.6168032786885246,"We evaluate our proposed method against NeRF-W [13] and city-scale Mega-NeRF [29] and present
239"
RESULTS,0.6188524590163934,"the quantitative results in Table 1 and Table 2. NeRF-W is the baseline from which we borrow
240"
RESULTS,0.6209016393442623,"the aforementioned idea of per-image appearance embedding and Mega-NeRF is a state-of-the-art
241"
RESULTS,0.6229508196721312,"(SOTA) large-scale scene modeling framework which our network is built upon.
242"
RESULTS,0.625,"Scene
Time-Pose Function
Joint Optimzation
Rot. (◦)
Trans. (m)
Rot. (◦)
Trans. (m)
NY Full
0.66 / 0.59 / 3.70
1.84 / 1.12 / 0.46
0.13 / 0.09 / 1.47
0.34 / 0.56 / 0.20
SF Full
0.17 / 0.67 / 0.65
1.34 / 1.45 / 0.94
0.05 / 0.41 / 0.02
0.32 / 1.09 / 0.66
Small
1.51 / 0.68 / 0.70 / 1.05
0.95 / 1.35 / 0.89 / 0.38
0.49 / 0.36 / 0.68 / 0.38
0.57 / 0.85 / 0.56 / 0.12
Mean
1.04
1.07
0.41 (-0.63)
0.53 (-0.54)"
RESULTS,0.6270491803278688,"Table 2: We show that the time-pose function learns an accurate implicit trajectory from the RGB
sequence that can estimate accurate poses for depth frames. By further tuning the time-pose function
jointly with the scene representation network, the accuracy of the predicted depth sensor poses can be
improved. The results of Simple / Hard / Manual on NY and SF are shown in the first two lines. The
results of the 4 small scenes of (Bridge / Town / School / Castle) are shown at the bottom."
RESULTS,0.6290983606557377,"Ground-Truth
Ours
Mega-NeRF
NeRF"
RESULTS,0.6311475409836066,"Figure 5: Qualitative Results. Our method can render photo-realistic novel views and the best depth
estimation results. Please zoom in to see details using an electornic verion."
RESULTS,0.6331967213114754,"RGB-D View Synthesis The standard metrics for novel view synthesis and depth estimation are
243"
RESULTS,0.6352459016393442,"used for evaluation. For RGB view synthesis, metrics including PSNR, SSIM, and the VGG
244"
RESULTS,0.6372950819672131,"implementation of LPIPS [40] are used. For depth estimation, RMSE, RMSE log, δ1,2,3 are used. In
245"
RESULTS,0.639344262295082,"Table 1, we quantitatively show on the RGB view synthesis task that, together with depth supervision,
246"
RESULTS,0.6413934426229508,"our method can generate more photo-realistic images than two SOTA baselines, and show on the
247"
RESULTS,0.6434426229508197,"depth estimation tasks that our method significantly improves the learned geometry of the scene
248"
RESULTS,0.6454918032786885,"representation network. We also present the RGB-D view synthesis results qualitatively in Fig.5 in
249"
RESULTS,0.6475409836065574,"which our method synthesize photo-realistic images and accurate depth maps, while baseline methods
250"
RESULTS,0.6495901639344263,"fail at predicting reliable depth maps (e.g. in the School scene, they mispredict the void space as a
251"
RESULTS,0.6516393442622951,"dense surface; in NY hard, the depth values around glasses are obviously inaccurate).
252"
RESULTS,0.6536885245901639,"Depth Pose Estimation We evaluate the performance of our time-pose function in Table 2, or say
253"
RESULTS,0.6557377049180327,"specifically the accuracy of our method to localize depth sensor poses. As shown quantitatively, our
254"
RESULTS,0.6577868852459017,"method can achieve an average pose error of 1.04m and 1.07◦in the first stage, where only time-pose
255"
RESULTS,0.6598360655737705,"pairs from the RGB sequence are used to optimize the network. After joint optimization in the third
256"
RESULTS,0.6618852459016393,"stage, our method cuts half the errors to 0.53m and 0.41◦.
257"
RESULTS,0.6639344262295082,"Real-world Evaluation. In the real-world experiments, we use the DJI M300 UAV (equipped with a
258"
RESULTS,0.6659836065573771,"high-definition RGB camera and LiDAR to collect real data, where the RGB camera collects images
259"
RESULTS,0.6680327868852459,"at the frame rate of 30fps and the LiDAR collects depth information at 240Hz. The poses of the RGB
260"
RESULTS,0.6700819672131147,"images are provided by COLMAP [22]. The fixed transformations between sensors are provided by
261"
RESULTS,0.6721311475409836,"the producer or can be calibrated manually. A qualitative comparison is provided in Fig. 1 and more
262"
RESULTS,0.6741803278688525,"results are in the supplementary.
263"
ABLATION STUDIES,0.6762295081967213,"6.2
Ablation Studies
264"
ABLATION STUDIES,0.6782786885245902,"Time-Pose Function Network Structure. We compared our proposed time-pose function implemen-
265"
ABLATION STUDIES,0.680327868852459,"tation with several commonly used implicit representation architectures on the localization accuracy
266"
ABLATION STUDIES,0.6823770491803278,"Method
Rotation (◦)
Translation (m)
Mean
Median
Mean
Median
MLP
26.62
17.53
8.23
7.50
Feature Grid
15.86
14.56
8.53
7.48
Ours (L=1)
24.24
12.99
9.20
8.01
Ours w/o speed
11.96
11.21
19.95
12.28
Ours
11.36
11.17
6.29
4.03"
ABLATION STUDIES,0.6844262295081968,"Table 3: Ablation on different network struc-
tures and the use of speed optimization."
ABLATION STUDIES,0.6864754098360656,"Offset
Rotation (◦)
Translation (m)"
ABLATION STUDIES,0.6885245901639344,"Mean
Median
Mean
Median"
ABLATION STUDIES,0.6905737704918032,"10%
0.66
0.26
3.42
1.88
20%
0.94
0.55
4.96
3.90
30%
1.24
0.79
6.41
5.78
40%
1.41
0.75
7.26
6.61
50%
1.50
0.84
7.53
6.51
Random
1.12
0.52
5.17
4.05"
ABLATION STUDIES,0.6926229508196722,"Table 4: Results on the ablation of
different sampling offset strategies."
ABLATION STUDIES,0.694672131147541,"(Tab. 3). In order to highlight the gap between different methods, we downsample the dataset to
267"
ABLATION STUDIES,0.6967213114754098,"increase the difficulty. (a) Pure MLP architecture : processing the positional-encoded [15] input
268"
ABLATION STUDIES,0.6987704918032787,"timestamps with an MLP; (b) 1-D Feature Grid: storing a feature vector for each second in the
269"
ABLATION STUDIES,0.7008196721311475,"timestamp span and performing linear feature interpolation in the query’s neighborhood. (c) Ours:
270"
ABLATION STUDIES,0.7028688524590164,"our proposed 1-D multi-resolution hash grid with different resolution layers. The results (Table 3)
271"
ABLATION STUDIES,0.7049180327868853,"show that our proposed multi-resolution architecture outperforms other network architectures in
272"
ABLATION STUDIES,0.7069672131147541,"accuracy. The network structures are further detailed in the supplementary materials.
273"
ABLATION STUDIES,0.7090163934426229,"Speed Loss. We compared our method’s localization accuracy with and without the optimization of
274"
ABLATION STUDIES,0.7110655737704918,"motion speed (see the comparison of ’Ours’ and ’Ours w/o speed’ in Tab. 3). The results show that
275"
ABLATION STUDIES,0.7131147540983607,"minimizing the gradient error (i.e., speed loss) help a lot in improving the accuracy of the translation
276"
ABLATION STUDIES,0.7151639344262295,"(from 20m to 6.3m).
277"
ABLATION STUDIES,0.7172131147540983,"Different Sampling Offsets. To show the robustness of our proposed time-pose function, we compare
278"
ABLATION STUDIES,0.7192622950819673,"the localization accuracy of implicit trajectory representations under different sampling offsets, whose
279"
ABLATION STUDIES,0.7213114754098361,"definition is described in Fig. 4-d/e. The quantitative results (Table. 4) indicate that the network’s
280"
ABLATION STUDIES,0.7233606557377049,"output exhibits a controllable margin of error as the data offset increases.
281"
ABLATION STUDIES,0.7254098360655737,"Scene
Ours
Mega-NeRF
Mega-NeRF-Depth
PSNR ↑
RMSE ↓
PSNR ↑
RMSE ↓
PSNR ↑
RMSE ↓
NY Mean
24.24
5.93
24.03
42.15
19.70
15.94
SF Mean
22.70
7.26
20.00
32.17
19.07
11.39
Bridge
29.06
26.55
27.98
120.41
22.35
96.16
Town
25.32
15.61
24.69
129.50
20.14
81.99
School
26.51
21.19
25.57
63.10
21.91
42.74
Castle
28.22
16.66
28.06
54.99
23.23
38.90
Mean
26.01
15.53
25.01
73.72
21.07
47.85
Table 5: Ablation on the joint optimization stage. We show that jointly optimizing the time-pose
function and the scene representation significantly helps reduce geometric error."
ABLATION STUDIES,0.7274590163934426,"Joint Optimization for Pose Error Compensation. To demonstrate the importance of rectifying
282"
ABLATION STUDIES,0.7295081967213115,"erroneous poses of depth images in asynchronous RGB-D sequences using the time-pose function,
283"
ABLATION STUDIES,0.7315573770491803,"we train a Mega-NeRF[29] with depth supervision but disabled the joint optimization stage. From
284"
ABLATION STUDIES,0.7336065573770492,"the evaluation results (Table 5), we observe its substantial impact on the rendering quality (PSNR for
285"
ABLATION STUDIES,0.735655737704918,"RGB and RMSE for depth). Due to limited space, qualitative results are in the supplementary.
286"
CONCLUSION,0.7377049180327869,"7
Conclusion
287"
CONCLUSION,0.7397540983606558,"In this paper, we present a method to learn depth-supervised neural radiance fields from asynchronous
288"
CONCLUSION,0.7418032786885246,"RGB-D sequences. We leverage an important prior that the sensors cover the same spatial-temporal
289"
CONCLUSION,0.7438524590163934,"footprints and propose to utilize this prior with an implicit time-pose function. With a 3-staged
290"
CONCLUSION,0.7459016393442623,"optimization pipeline, our method calibrates the RGB-D poses and trains a large-scale implicit scene
291"
CONCLUSION,0.7479508196721312,"representation. Our experiments on a newly proposed large-scale dataset show that our method can
292"
CONCLUSION,0.75,"effectively register depth camera poses and learns the 3D scene representation for photo-realistic
293"
CONCLUSION,0.7520491803278688,"novel view synthesis and accurate depth estimations. Broader impact and limitations: Large-scale
294"
CONCLUSION,0.7540983606557377,"scene modelling can be used for potential military use, which the method is not intended for.
295"
REFERENCES,0.7561475409836066,"References
296"
REFERENCES,0.7581967213114754,"[1] Sameer Ansari, Neal Wadhwa, Rahul Garg, and Jiawen Chen. Wireless Software Synchronization of
297"
REFERENCES,0.7602459016393442,"Multiple Distributed Cameras. In 2019 IEEE International Conference on Computational Photography
298"
REFERENCES,0.7622950819672131,"(ICCP), pages 1–9, May 2019. ISSN: 2472-7636.
299"
REFERENCES,0.764344262295082,"[2] Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic, and Tolga Birdal. Deep bingham
300"
REFERENCES,0.7663934426229508,"networks: Dealing with uncertainty and ambiguity in pose estimation. International Journal of Computer
301"
REFERENCES,0.7684426229508197,"Vision, pages 1–28, 2022.
302"
REFERENCES,0.7704918032786885,"[3] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and
303"
REFERENCES,0.7725409836065574,"faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
304"
REFERENCES,0.7745901639344263,"Recognition, pages 12882–12891, 2022.
305"
REFERENCES,0.7766393442622951,"[4] A. Elhayek, C. Stoll, N. Hasler, K. I. Kim, H.-P. Seidel, and C. Theobalt. Spatio-temporal motion tracking
306"
REFERENCES,0.7786885245901639,"with unsynchronized cameras. In 2012 IEEE Conference on Computer Vision and Pattern Recognition,
307"
REFERENCES,0.7807377049180327,"pages 1870–1877, June 2012. ISSN: 1063-6919.
308"
REFERENCES,0.7827868852459017,"[5] Antoine Guedon, Pascal Monasse, and Vincent Lepetit. Scone: Surface coverage optimization in unknown
309"
REFERENCES,0.7848360655737705,"environments by volumetric integration. In Advances in Neural Information Processing Systems, 2022.
310"
REFERENCES,0.7868852459016393,"[6] Antoine Guédon, Tom Monnier, Pascal Monasse, and Vincent Lepetit. Macarons: Mapping and coverage
311"
REFERENCES,0.7889344262295082,"anticipation with rgb online self-supervision. arXiv preprint arXiv:2303.03315, 2023.
312"
REFERENCES,0.7909836065573771,"[7] Meng-Ru Hsieh, Yen-Liang Lin, and Winston H Hsu. Drone-based object counting by spatially regularized
313"
REFERENCES,0.7930327868852459,"regional proposal network. In Proceedings of the IEEE international conference on computer vision, pages
314"
REFERENCES,0.7950819672131147,"4145–4153, 2017.
315"
REFERENCES,0.7971311475409836,"[8] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park.
316"
REFERENCES,0.7991803278688525,"Self-calibrating neural radiance fields. In Proceedings of the IEEE/CVF International Conference on
317"
REFERENCES,0.8012295081967213,"Computer Vision, pages 5846–5854, 2021.
318"
REFERENCES,0.8032786885245902,"[9] Alex Kendall and Roberto Cipolla. Geometric loss functions for camera pose regression with deep learning.
319"
REFERENCES,0.805327868852459,"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5974–5983,
320"
REFERENCES,0.8073770491803278,"2017.
321"
REFERENCES,0.8094262295081968,"[10] Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network for real-time
322"
REFERENCES,0.8114754098360656,"6-dof camera relocalization. In Proceedings of the IEEE international conference on computer vision,
323"
REFERENCES,0.8135245901639344,"pages 2938–2946, 2015.
324"
REFERENCES,0.8155737704918032,"[11] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural
325"
REFERENCES,0.8176229508196722,"radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
326"
REFERENCES,0.819672131147541,"5741–5751, 2021.
327"
REFERENCES,0.8217213114754098,"[12] Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and Hui Huang. Capturing, reconstructing, and
328"
REFERENCES,0.8237704918032787,"simulating: the urbanscene3d dataset. In ECCV, 2022.
329"
REFERENCES,0.8258196721311475,"[13] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and
330"
REFERENCES,0.8278688524590164,"Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. In
331"
REFERENCES,0.8299180327868853,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7206–7215, June
332"
REFERENCES,0.8319672131147541,"2021. ISSN: 2575-7075.
333"
REFERENCES,0.8340163934426229,"[14] Zhenxing Mi and Dan Xu. Switch-nerf: Learning scene decomposition with mixture of experts for
334"
REFERENCES,0.8360655737704918,"large-scale neural radiance fields. In International Conference on Learning Representations (ICLR), 2023.
335"
REFERENCES,0.8381147540983607,"[15] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren
336"
REFERENCES,0.8401639344262295,"Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Andrea Vedaldi, Horst
337"
REFERENCES,0.8422131147540983,"Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision – ECCV 2020, Lecture Notes in
338"
REFERENCES,0.8442622950819673,"Computer Science, pages 405–421, Cham, 2020. Springer International Publishing.
339"
REFERENCES,0.8463114754098361,"[16] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator for uav tracking. In
340"
REFERENCES,0.8483606557377049,"Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14,
341"
REFERENCES,0.8504098360655737,"2016, Proceedings, Part I 14, pages 445–461. Springer, 2016.
342"
REFERENCES,0.8524590163934426,"[17] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-
343"
REFERENCES,0.8545081967213115,"itives with a multiresolution hash encoding. ACM Transactions on Graphics, 41(4):1–15, July 2022.
344"
REFERENCES,0.8565573770491803,"titleTranslation:.
345"
REFERENCES,0.8586065573770492,"[18] Stefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You’ll never walk alone: Modeling
346"
REFERENCES,0.860655737704918,"social behavior for multi-target tracking. In 2009 IEEE 12th international conference on computer vision,
347"
REFERENCES,0.8627049180327869,"pages 261–268. IEEE, 2009.
348"
REFERENCES,0.8647540983606558,"[19] Amir M Rahimi, Raphael Ruschel, and BS Manjunath. Uav sensor fusion with latent-dynamic conditional
349"
REFERENCES,0.8668032786885246,"random fields in coronal plane estimation. In Proceedings of the IEEE Conference on Computer Vision
350"
REFERENCES,0.8688524590163934,"and Pattern Recognition, pages 4527–4534, 2016.
351"
REFERENCES,0.8709016393442623,"[20] Konstantinos Rematas, Andrew Liu, Pratul P Srinivasan, Jonathan T Barron, Andrea Tagliasacchi, Thomas
352"
REFERENCES,0.8729508196721312,"Funkhouser, and Vittorio Ferrari. Urban radiance fields. In Proceedings of the IEEE/CVF Conference on
353"
REFERENCES,0.875,"Computer Vision and Pattern Recognition, pages 12932–12942, 2022.
354"
REFERENCES,0.8770491803278688,"[21] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan, and Matthias Nießner. Dense
355"
REFERENCES,0.8790983606557377,"depth priors for neural radiance fields from sparse input views. In Proceedings of the IEEE/CVF Conference
356"
REFERENCES,0.8811475409836066,"on Computer Vision and Pattern Recognition, pages 12892–12901, 2022.
357"
REFERENCES,0.8831967213114754,"[22] Johannes L. Schönberger and Jan-Michael Frahm. Structure-from-Motion Revisited. In 2016 IEEE
358"
REFERENCES,0.8852459016393442,"Conference on Computer Vision and Pattern Recognition (CVPR), pages 4104–4113, June 2016. ISSN:
359"
REFERENCES,0.8872950819672131,"1063-6919.
360"
REFERENCES,0.889344262295082,"[23] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical
361"
REFERENCES,0.8913934426229508,"simulation for autonomous vehicles. In Field and Service Robotics: Results of the 11th International
362"
REFERENCES,0.8934426229508197,"Conference, pages 621–635. Springer, 2018.
363"
REFERENCES,0.8954918032786885,"[24] Kirill Sibiriakov. Artstation page https://www.artstation.com/vegaart, 2022.
364"
REFERENCES,0.8975409836065574,"[25] Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for cnn: Viewpoint estimation in images
365"
REFERENCES,0.8995901639344263,"using cnns trained with rendered 3d model views. In Proceedings of the IEEE international conference on
366"
REFERENCES,0.9016393442622951,"computer vision, pages 2686–2694, 2015.
367"
REFERENCES,0.9036885245901639,"[26] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan,
368"
REFERENCES,0.9057377049180327,"Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In
369"
REFERENCES,0.9077868852459017,"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8248–8258,
370"
REFERENCES,0.9098360655737705,"2022.
371"
REFERENCES,0.9118852459016393,"[27] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
372"
REFERENCES,0.9139344262295082,"Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier Features Let Networks Learn High
373"
REFERENCES,0.9159836065573771,"Frequency Functions in Low Dimensional Domains, June 2020. arXiv:2006.10739 [cs].
374"
REFERENCES,0.9180327868852459,"[28] Shubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In Proceedings of the IEEE Conference
375"
REFERENCES,0.9200819672131147,"on Computer Vision and Pattern Recognition, pages 1510–1519, 2015.
376"
REFERENCES,0.9221311475409836,"[29] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-NeRF: Scalable Construction of
377"
REFERENCES,0.9241803278688525,"Large-Scale NeRFs for Virtual Fly- Throughs. In 2022 IEEE/CVF Conference on Computer Vision and
378"
REFERENCES,0.9262295081967213,"Pattern Recognition (CVPR), pages 12912–12921, June 2022. ISSN: 2575-7075.
379"
REFERENCES,0.9282786885245902,"[30] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. Nerf–: Neural radiance
380"
REFERENCES,0.930327868852459,"fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021.
381"
REFERENCES,0.9323770491803278,"[31] Longyin Wen, Dawei Du, Pengfei Zhu, Qinghua Hu, Qilong Wang, Liefeng Bo, and Siwei Lyu. Detection,
382"
REFERENCES,0.9344262295081968,"tracking, and counting meets drones in crowds: A benchmark. In Proceedings of the IEEE/CVF Conference
383"
REFERENCES,0.9364754098360656,"on Computer Vision and Pattern Recognition, pages 7812–7821, 2021.
384"
REFERENCES,0.9385245901639344,"[32] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and
385"
REFERENCES,0.9405737704918032,"Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In The
386"
REFERENCES,0.9426229508196722,"European Conference on Computer Vision (ECCV), 2022.
387"
REFERENCES,0.944672131147541,"[33] Dan Xie, Sinisa Todorovic, and Song-Chun Zhu. Inferring"" dark matter"" and"" dark energy"" from videos.
388"
REFERENCES,0.9467213114754098,"In Proceedings of the IEEE International Conference on Computer Vision, pages 2224–2231, 2013.
389"
REFERENCES,0.9487704918032787,"[34] Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, and Li Zhang. S-neRF: Neural radiance fields for street
390"
REFERENCES,0.9508196721311475,"views. In The Eleventh International Conference on Learning Representations, 2023.
391"
REFERENCES,0.9528688524590164,"[35] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and
392"
REFERENCES,0.9549180327868853,"Dahua Lin. Grid-guided neural radiance fields for large urban scenes, 2023.
393"
REFERENCES,0.9569672131147541,"[36] Anqi Joyce Yang, Can Cui, Ioan Andrei Bârsan, Raquel Urtasun, and Shenlong Wang. Asynchronous
394"
REFERENCES,0.9590163934426229,"Multi-View SLAM. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages
395"
REFERENCES,0.9610655737704918,"5669–5676, May 2021. ISSN: 2577-087X.
396"
REFERENCES,0.9631147540983607,"[37] Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin.
397"
REFERENCES,0.9651639344262295,"iNeRF: Inverting Neural Radiance Fields for Pose Estimation. In 2021 IEEE/RSJ International Conference
398"
REFERENCES,0.9672131147540983,"on Intelligent Robots and Systems (IROS), pages 1323–1330, September 2021. ISSN: 2153-0866.
399"
REFERENCES,0.9692622950819673,"[38] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring
400"
REFERENCES,0.9713114754098361,"monocular geometric cues for neural implicit surface reconstruction. Advances in Neural Information
401"
REFERENCES,0.9733606557377049,"Processing Systems (NeurIPS), 2022.
402"
REFERENCES,0.9754098360655737,"[39] Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, and Ali Farhadi. Visual reaction: Learning to play catch
403"
REFERENCES,0.9774590163934426,"with your drone. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
404"
REFERENCES,0.9795081967213115,"pages 11573–11582, 2020.
405"
REFERENCES,0.9815573770491803,"[40] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The Unreasonable
406"
REFERENCES,0.9836065573770492,"Effectiveness of Deep Features as a Perceptual Metric. In 2018 IEEE/CVF Conference on Computer Vision
407"
REFERENCES,0.985655737704918,"and Pattern Recognition, pages 586–595, June 2018. ISSN: 2575-7075.
408"
REFERENCES,0.9877049180327869,"[41] Runze Zhang, Siyu Zhu, Tian Fang, and Long Quan. Distributed very large scale bundle adjustment by
409"
REFERENCES,0.9897540983606558,"global camera consensus. In Proceedings of the IEEE International Conference on Computer Vision, pages
410"
REFERENCES,0.9918032786885246,"29–38, 2017.
411"
REFERENCES,0.9938524590163934,"[42] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representa-
412"
REFERENCES,0.9959016393442623,"tions in neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
413"
REFERENCES,0.9979508196721312,"Recognition, pages 5745–5753, 2019.
414"
