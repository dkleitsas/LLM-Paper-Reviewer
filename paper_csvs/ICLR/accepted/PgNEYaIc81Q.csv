Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.003067484662576687,"Objects‚Äô motions in nature are governed by complex interactions and their prop-
erties. While some properties, such as shape and material, can be identiÔ¨Åed via
the object‚Äôs visual appearances, others like mass and electric charge are not di-
rectly visible. The compositionality between the visible and hidden properties
poses unique challenges for AI models to reason from the physical world, whereas
humans can effortlessly infer them with limited observations. Existing studies
on video reasoning mainly focus on visually observable elements such as object
appearance, movement, and contact interaction. In this paper, we take an initial step
to highlight the importance of inferring the hidden physical properties not directly
observable from visual appearances, by introducing the Compositional Physical
Reasoning (ComPhy) dataset 1. For a given set of objects, ComPhy includes few
videos of them moving and interacting under different initial conditions. The model
is evaluated based on its capability to unravel the compositional hidden properties,
such as mass and charge, and use this knowledge to answer a set of questions
posted on one of the videos. Evaluation results of several state-of-the-art video
reasoning models on ComPhy show unsatisfactory performance as they fail to
capture these hidden properties. We further propose an oracle neural-symbolic
framework named Compositional Physics Learner (CPL), combining visual per-
ception, physical property learning, dynamic prediction, and symbolic execution
into a uniÔ¨Åed framework. CPL can effectively identify objects‚Äô physical properties
from their interactions and predict their dynamics to answer questions."
INTRODUCTION,0.006134969325153374,"1
INTRODUCTION"
INTRODUCTION,0.009202453987730062,"Why do apples Ô¨Çoat in water while bananas sink? Why do magnets attract each other on a certain
side and repel on the other? Objects in nature often exhibit complex properties that deÔ¨Åne how they
interact with the physical world. To humans, the unraveling of new intrinsic physical properties often
marks important milestones towards a deeper and more accurate understanding of nature. Most of
these properties are intrinsic as they are not directly reÔ¨Çected in the object‚Äôs visual appearances or
otherwise detectable without imposing an interaction. Moreover, these properties affect object motion
in a compositional fashion, and the causal dependency and mathematical law between different
properties are often complex."
INTRODUCTION,0.012269938650306749,"As shown in Fig. 1, different intrinsic physical properties, such as charge and inertia, often lead to
drastically different future evolutions. Objects carrying the same or opposite charge will exert a
repulsive or attractive force on each other. The resulting motion not only depends on the amount of
charge each object carries, but also their signs (see Fig. 1-(a)). The inertia determines how sensitive an
object‚Äôs motion is to external forces. When a massive object interacts with a light object via attraction,
repulsion, or collision, the lighter object will undergo larger changes in its motion compared with the
massive object‚Äôs trajectory (see Fig. 1-(b))."
INTRODUCTION,0.015337423312883436,"Recent studies have established a series of benchmarks to evaluate and diagnose machine learning
systems in various physics-related environments (Bakhtin et al., 2019; Yi et al., 2020; Baradel et al.,"
INTRODUCTION,0.018404907975460124,1Project page: https://comphyreasoning.github.io
INTRODUCTION,0.02147239263803681,"Published as a conference paper at ICLR 2022 1
2 3 4
5"
INTRODUCTION,0.024539877300613498,"(a)
(b)"
INTRODUCTION,0.027607361963190184,"(Same charge on 1, 2)
(Opposite charge on 1, 2)
(4 heavier than 5)
(4 lighter than 5)
time"
INTRODUCTION,0.03067484662576687,"Figure 1: Non-visual properties like mass and charge govern the interaction between objects and
lead to different motion trajectories. a) Objects attract and repel each other according to the (sign
of) charge they carry. b) Mass determines how much an object‚Äôs trajectory is perturbed during an
interaction. Heavier objects have more stable motion."
INTRODUCTION,0.03374233128834356,"2020). These benchmarks introduce reasoning tasks over a wide range of complex object motion
and interactions, which poses enormous challenges to existing models since most tasks require them
to fully capture the underlying physical dynamics and, in some cases, be able to make predictions.
However, the majority of complexity in the motion trajectories facilitated by these environments
comes from changes or interventions in the initial conditions of the physical experiments. The effects
of object intrinsic physical properties, as well as the unique set of their challenges, are therefore of
great importance for further investigation."
INTRODUCTION,0.03680981595092025,"It‚Äôs non-trivial to build a benchmark for compositional physical reasoning. Existing benchmarks (Yi
et al., 2020; Ates et al., 2020) assume that there is no variance in objects‚Äô physical properties and ask
models to learn physical reasoning from massive videos and question-answer pairs. A straightforward
solution is to correlate object appearance with physical properties like making all red spheres to be
heavy and then ask questions about their dynamics. However, such a design may incur shortcuts
for models by just memorizing the appearance prior rather than understanding coupled physical
properties. In this work, we propose a novel benchmark called ComPhy that focuses on understanding
object-centric and relational physics properties hidden from visual appearances. ComPhy Ô¨Årst
provides few video examples with dynamic interactions among objects for models to identify objects‚Äô
physical properties and then asks questions about the physical properties and corresponding dynamics.
As shown in Fig. 2, ComPhy consists of meta-train sets and meta-test sets, where each data point
contains 4 reference videos and 1 target video. Within each set, the objects share the same intrinsic
physical properties across all videos. Reasoning on ComPhy requires the model to infer the intrinsic
and compositional physical properties of the object set from the reference videos, and then answer
questions about this query video. To make the task feasible, we systematically control each object in
the query video that should appear at least in one of the reference videos."
INTRODUCTION,0.03987730061349693,"We also introduce an oracle model to tackle this task. Inspired by recent work on neural-symbolic
reasoning on images and videos (Yi et al., 2018; 2020; Chen et al., 2021), our model consists of four
disentangled components: perception, physical property learning, dynamics prediction, and symbolic
reasoning. Our model is able to infer objects‚Äô compositional and intrinsic physical properties, predict
their future, make counterfactual imaginations, and answer questions."
INTRODUCTION,0.04294478527607362,"To summarize, this paper makes the following contributions. First, we present a new physical
reasoning benchmark ComPhy with physical properties (mass and charge), physical events (attraction,
repulsion), and their composition with visual appearance and motions. Second, we decorrelate
physical properties and visual appearance with a few-shot reasoning setting. It requires models to
infer hidden physical properties from only a few examples and then make predictions about the
system‚Äôs evolution to help answer the questions.
Third, we propose an oracle neural-symbolic
framework, which is a modularized model that can infer objects‚Äô physical properties and predict
the objects‚Äô movements. At the core of our model are graph neural networks that capture the
compositional nature of the underlying system."
RELATED WORK,0.046012269938650305,"2
RELATED WORK
Physical Reasoning. Our work is closely related to recent developments in physical reasoning
benchmarks (Riochet et al., 2018; Girdhar & Ramanan, 2020; Ates et al., 2020; Hong et al., 2021).
PHYRE (Bakhtin et al., 2019) and its variant ESPRIT (Rajani et al., 2020) deÔ¨Ånes an environment"
RELATED WORK,0.049079754601226995,Published as a conference paper at ICLR 2022
RELATED WORK,0.05214723926380368,"Dataset
Video
Question
Diagnostic Composition Few-shot Physical
Counterfactual
Answering Annotation
Reasoning Property Property Dynamics"
RELATED WORK,0.05521472392638037,"CLEVR (Johnson et al., 2017)
-
‚úì
‚úì
‚úì
-
-
-
MovieQA (Tapaswi et al., 2016)
‚úì
‚úì
-
‚úì
-
-
-
TGIF-QA (Jang et al., 2017)
‚úì
‚úì
-
-
-
-
-
TVQA/ TVQA+ (Lei et al., 2019)
‚úì
‚úì
-
‚úì
-
-
-
AGQA (Grunde-McLaughlin et al., 2021)
‚úì
‚úì
-
-
-
-
-"
RELATED WORK,0.05828220858895705,"IntPhys (Riochet et al., 2018)
‚úì
-
‚úì
-
-
‚úì
-
PHYRE/ ESPRIT (Rajani et al., 2020)
‚úì
-
‚úì
‚úì
-
‚úì
-
Cater (Riochet et al., 2018)
‚úì
‚úì
‚úì
‚úì
-
-
-
CoPhy(Baradel et al., 2020)
‚úì
-
‚úì
-
-
‚úì
-
CRAFT (Ates et al., 2020)
‚úì
‚úì
‚úì
‚úì
-
-
-
CLEVRER (Yi et al., 2020)
‚úì
‚úì
‚úì
‚úì
-
-
-"
RELATED WORK,0.06134969325153374,"ComPhy (ours)
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì"
RELATED WORK,0.06441717791411043,"Table 1: Comparison between ComPhy and other visual reasoning benchmarks. ComPhy is the dataset
with reasoning tasks for both physical property learning and corresponding dynamic prediction.
where objects can move within a vertical 2D plane under gravity. Each task is associated with a
goal state, and the model solves the task by specifying the initial condition that will lead to the
goal state. CLEVRER (Yi et al., 2020) contains videos of multiple objects moving and colliding
on a Ô¨Çat plane, posting natural language questions about description, explanation, prediction, and
counterfactual reasoning on the collision events. CoPhy (Baradel et al., 2020) includes experiment
trials of objects moving in 3D space under gravity. The task focuses on predicting object trajectories
under counterfactual interventions on the initial conditions. Our dataset contributes to these physical
reasoning benchmarks by focusing on physical events driven by object intrinsic properties (situations
shown in Fig. 2). ComPhy requires models to identify intrinsic properties from only a few video
examples and make dynamic predictions based on the identiÔ¨Åed properties and their compositionality."
RELATED WORK,0.06748466257668712,"Dynamics Modeling. Dynamics modeling of physical systems has been a long-standing research
direction. Some researchers have studied this problem via physical simulations, drawing inference on
the important system- and object-level properties via statistical approaches such as MCMC (Battaglia
et al., 2013; Hamrick et al., 2016; Wu et al., 2015), while others propose to directly learn the forward
dynamics via neural networks (Lerer et al., 2016). Graph neural networks (Kipf & Welling, 2017),
due to their object- and relation-centric inductive biases and efÔ¨Åciency, have been widely applied
in forwarding dynamics prediction on a wide variety of systems (Battaglia et al., 2016; Chang
et al., 2016; Sanchez-Gonzalez et al., 2020; Li et al., 2019a). Our work combines the best of the
two approaches by Ô¨Årst inferring the object-centric intrinsic physical properties and then predicting
their dynamics based on the intrinsic properties. Recently, VRDP (Ding et al., 2021) performs
object-centric dynamic reasoning by learning differentiable physics models."
RELATED WORK,0.0705521472392638,"Video Question Answering. Our work is also related to answering questions about visual content.
Various benchmarks have been proposed to handle the tasks of cross-modal learning (Lei et al., 2018;
Chen et al., 2019; Li et al., 2020; Wu et al., 2021; Hong et al., 2022). However, they mainly focus on
understanding human actions and activities rather than learning physical events and properties, which
is essential for robot planning and control. Following CLEVRER, we summarize the difference
between ComPhy and previous benchmarks in Table 1. ComPhy is the only dataset that requires the
model to learn physical property from few video examples, make dynamic predictions based on the
physical property, and Ô¨Ånally answer corresponding questions."
RELATED WORK,0.0736196319018405,"Few-shot Learning. Our work is also related to few-shot learning, which typically learns to classify
images from only a few labelled examples (Vinyals et al., 2016; Snell et al., 2017; Han et al., 2019).
ComPhy also requires models to identify objects‚Äô property labels from only a few video examples.
Different from them, reference videos have no labels for objects‚Äô physical properties but more
interaction among objects, providing information for models to identify objects‚Äô physical properties."
DATASET,0.07668711656441718,"3
DATASET"
DATASET,0.07975460122699386,"ComPhy studies objects‚Äô intrinsic physical properties from objects‚Äô interactions and how these
properties affect their motions in future and counterfactual scenes to answer corresponding questions.
We Ô¨Årst introduce videos and task setup in Section 3.1. We then discuss question types in Section 3.2,
and statistics and balancing in Section 3.3."
VIDEOS,0.08282208588957055,"3.1
VIDEOS"
VIDEOS,0.08588957055214724,"Objects and Events. Following Johnson et al. (2017), objects in ComPhy contain compositional
appearance attributes like color, shape, and material. Each object in videos can be uniquely identiÔ¨Åed"
VIDEOS,0.08895705521472393,Published as a conference paper at ICLR 2022 1 2
VIDEOS,0.09202453987730061,"obj1 and obj2 repel 
obj1 and obj2 get close
obj2 and obj4 collide 2
4"
VIDEOS,0.0950920245398773,"obj2 and obj4 change direction 1
4
2 2 1 4
2
4 time 1 2
‚Ä¶
‚Ä¶ time 1 2"
VIDEOS,0.09815950920245399,"obj1 and obj3 collide 
obj1 and obj4 collide
obj1 and obj4 collide 2
4"
VIDEOS,0.10122699386503067,"obj1 and obj3 change direction 2 2 1 4 1
4 3 3 1
4 1
3
4 4
1 3
‚Ä¶ time ‚Ä¶ time 2
3 1 3 2
4"
VIDEOS,0.10429447852760736,"obj1 and obj2 repel
obj2 and obj3 collide 2 3"
VIDEOS,0.10736196319018405,"obj2 changes direction
obj2 and obj4 move close 2 3 2 3 4 time"
VIDEOS,0.11042944785276074,"I. Factual Question
Q: How many moving objects are 
uncharged?                             
A: 2
Q: Is the purple sphere heavier than 
the brown cube?                       
A: No"
VIDEOS,0.11349693251533742,"II. Counterfactual Question
Q: Which event would  happen if the 
purple object were heavier?"
VIDEOS,0.1165644171779141,"a) The green cube would collide 
with the metal sphere                        ‚àö"
VIDEOS,0.1196319018404908,"b) The metal sphere would collide 
with the brown cube                           √ó"
VIDEOS,0.12269938650306748,"III. Predictive
Q: Which event will happen next?"
VIDEOS,0.12576687116564417,"a) The red sphere and the brown 
cube collide                                        √ó"
VIDEOS,0.12883435582822086,"b) The purple sphere and the brown 
cube collide                                         ‚àö ‚Ä¶
‚Ä¶
‚Ä¶"
VIDEOS,0.13190184049079753,"Figure 2: Sample target video, reference videos and question-answer pairs from ComPhy.
by these three attributes for simplicity. There are events, in, out, collision, attraction and repulsion.
These object appearance attributes and events form the basic concepts of the questions in ComPhy."
VIDEOS,0.13496932515337423,"Physical Property. Previous benchmarks (Riochet et al., 2018; Yi et al., 2020) mainly study
appearance concepts like color and collision that can be perceived in even a single frame. In ComPhy,
we additionally study intrinsic physical properties, mass and charge, which can not be directly
captured from objects‚Äô static appearance. As shown in Fig. 1 (a), objects with same or opposite
charge will repel or attract each other while objects without charge will not affect each other‚Äôs motion
without collision. As shown Fig. 1 (b), the object with larger mass (inertia) tends to maintain its
original moving direction after the collision while the light object changes much more in its moving
direction. Note that these intrinsic physical properties are orthogonal to the appearance attributes
and can be combined with each other to generate more complicated and diverse dynamic scenes.
For simplicity, ComPhy contains two mass values (heavy and light) and three charge types (positive
charged, negative charged and uncharged). Theoretically, we can add more physical properties like
bounciness coefÔ¨Åcients and friction into ComPhy and make their values continuous. However, such a
design will make the dataset too complicated and even difÔ¨Åcult for people to infer the properties."
VIDEOS,0.13803680981595093,"Video Generations. For each video, we Ô¨Årst use a physical engine (Coumans & Bai, 2016‚Äì2021)
to simulate objects‚Äô motions and then adopt a graphs engine (Community, 2018) to render frame
sequences. Each target video for question answering contains 3 to 5 objects with random composition
between their appearance attributes and physical properties. We set the length of the target video to
be 5 seconds and additionally simulate the 6-th and 7-th seconds of the target video for predictive
question annotation. We provide more video generation details in the Appendix."
VIDEOS,0.1411042944785276,"Task Setup. It is not trivial to design a task setup to evaluate models‚Äô ability for physical reasoning
since physical properties are not observable in a static frame. A straightforward design is to correlate
object appearance with the physical property like ‚Äúred object is heavy‚Äù, ‚Äúyellow object is light‚Äù and
then ask ‚Äúwhat would happen if they collide‚Äù. However, such a setting is imperfect since it can not
evaluate whether a model really understands the physical properties or just memorize the visual
appearance prior. An ideal setting should be able to evaluate whether a model is like a human that
can identify objects‚Äô properties from objects‚Äô motion and interactions with each other in the dynamic
scenes and make the corresponding dynamic predictions."
VIDEOS,0.1441717791411043,"To achieve this, we design a meta setting for physical reasoning, which provides few reference video
samples along with the target video for models to infer objects‚Äô physical properties and then ask"
VIDEOS,0.147239263803681,Published as a conference paper at ICLR 2022
VIDEOS,0.15030674846625766,"questions about the objects‚Äô physical properties and dynamics. We show a sample of the dataset in
Fig. 2. Each set contains a target video, 4 reference videos, and some questions about the visual
attributes, physical properties, events, and dynamics of the target video. Objects in each set share the
same visual attributes (color, shape, and material) and intrinsic physical property (mass and charge)."
VIDEOS,0.15337423312883436,"Reference Videos. To provide abundant visual content for physical property inference, we addition-
ally provide 4 reference videos for each target video. We sample 2-3 objects from the target video,
provide them with different initial velocities and locations, and make them interact (attract, repel
or collide) with each other. The generation of the reference video follows the same standard as the
target video, but the length of the videos is set to 2 seconds for scaling up. The interaction among
objects in reference videos helps models to inference objects‚Äô properties. For example, the repulsion
in Reference video 1 of Fig. 2 can help us identify that object 1 and object 2 carrying the same charge."
QUESTIONS,0.15644171779141106,"3.2
QUESTIONS"
QUESTIONS,0.15950920245398773,"Following Johnson et al. (2017) and Yi et al. (2020), we develop a question engine to generate
questions for factual, predictive, and counterfactual reasoning. Each question is paired with a
functional program, which provides a series of explicit reasoning steps. We set all factual questions to
be ‚Äúopen-ended‚Äù that can be answered by a single word or a short phrase. We set predictive questions
and counterfactual questions to be multiple-choice and require models to independently predict each
option is true or false. We provide question templates and examples in the Appendix."
QUESTIONS,0.16257668711656442,"Factual. Factual questions test models‚Äô ability to understand and reason about objects‚Äô physical
properties, visual attributes, events, and their compositional relations. Besides the factual questions in
existing benchmarks (Yi et al., 2020; Ates et al., 2020), as shown in the samples in Fig. 2 (I), ComPhy
includes novel and challenging questions asking about objects‚Äô physical properties, charge and mass."
QUESTIONS,0.1656441717791411,"Predictive. Predictive questions evaluate models‚Äô ability to predict and reason about the events
happening after the observed target video ends. It requires a model to observe objects‚Äô location and
velocity at the end, identify objects‚Äô physical property and predict what will or not happen next."
QUESTIONS,0.1687116564417178,"Counterfactual Charge and Mass. Counterfactual questions ask what would happen on a certain
hypothetical condition. ComPhy targets at reasoning the dynamics under the hypothesis that a certain
object carrying a different physical property. Fig. 2 (II) shows typical question samples. Previous
work (Yi et al., 2020; Riochet et al., 2018) also has counterfactual questions. However, they are
merely based on the hypothesis that an object is removed rather than an object is carrying a different
physical property value, which has different or even opposite effects on object motion prediction."
BALANCING AND STATISTICS,0.17177914110429449,"3.3
BALANCING AND STATISTICS"
BALANCING AND STATISTICS,0.17484662576687116,"Overall, ComPhy has 8,000 sets for training, 2,000 sets for validation, and 2,000 for testing. It
contains 41,933 factual questions, 50,405 counterfactual questions and 7,506 predictive questions,
occupying 42%, 50% and 8% of the dataset, respectively. For simplicity, we manually make sure that
a video set will only contain a pair of charged objects if it contains charged objects. Similarly, a video
will only contain a heavy object or no heavy objects. A non-negligible issue of previous benchmarks
like CLEVRER is its bias. As pointed out by Ding et al. (2020), about half of the counterfactual
objects in CLEVRER have no collisions with other objects and the counterfactual questions can be
solved merely based on the observed target videos. In ComPhy, we manually remove counterfactual
questions on objects that have no interaction with other objects. When generating questions for
comparing mass values and identifying charge relations between two objects, we systematically
control that the two objects should have at least one interaction in one of the provided few video
examples. We make sure that the few video examples are informative enough to answer questions
based on the questions‚Äô programs and the video examples‚Äô property and interaction annotation."
EXPERIMENTS,0.17791411042944785,"4
EXPERIMENTS
In this section, We evaluate various baselines and analyze their results to study ComPhy thoroughly."
BASELINES,0.18098159509202455,"4.1
BASELINES"
BASELINES,0.18404907975460122,"Following CLEVRER, We evaluate a range of baselines on ComPhy in Table 2. Such baselines can be
divided into three groups, bias-analysis models, video-question answering models, and compositional
reasoning models. To provide extensive comparison, we also implement some variant models that
make use of both the target video and reference videos."
BASELINES,0.18711656441717792,Published as a conference paper at ICLR 2022
BASELINES,0.1901840490797546,"Methods
Factual
Predictive
Counterfactual"
BASELINES,0.19325153374233128,"per opt.
per ques.
per opt.
per ques."
BASELINES,0.19631901840490798,"Random
29.7
51.9
22.6
49.7
9.1
Frequent
30.9
56.2
25.7
50.3
8.7
Blind-LSTM
39.0
57.9
28.7
55.7
12.5"
BASELINES,0.19938650306748465,"CNN-LSTM (Antol et al., 2015)
46.6
59.5
29.8
58.6
14.6
HCRN (Le et al., 2020)
47.3
62.7
32.7
58.6
14.2"
BASELINES,0.20245398773006135,"MAC (Hudson & Manning, 2018)
68.6
60.2
32.2
60.2
16.0
ALOE (Ding et al., 2020)
54.3
65.9
35.2
65.4
20.8
CNN-LSTM (Ref) (Antol et al., 2015)
41.9
59.6
29.4
57.2
12.8
MAC (Ref) (Hudson & Manning, 2018)
65.8
60.2
30.7
60.3
14.3
ALOE (Ref) (Ding et al., 2020)
57.7
67.9
37.1
67.9
22.2
Human Performance
90.6
88.0
75.9
80.0
52.9"
BASELINES,0.20552147239263804,"Table 2: Evaluation of physical reasoning on ComPhy. Human performance is based on sampled
questions. See Section 4.2 for more details.
Baselines. The Ô¨Årst group of models are bias analysis models. They analyze the language bias in
ComPhy and answer questions without visual input. SpeciÔ¨Åcally, Random randomly selects an
answer based on its question type. Frequent chooses the most frequent answer for each question type.
Blind-LSTM uses an LSTM to encode the question and predict the answer without visual input. The
second group of models are visual question answering Models. These models answers questions based
on the input videos and questions. CNN-LSTM (Antol et al., 2015) is a basic question answering
model. We use a resNet-50 to extract frame-level features and average them over the time dimension.
We encode questions with the last hidden state from an LSTM. The visual features and the question
embedding are concatenated to predict answers. HCRN (Le et al., 2020) is a popular model that
hierarchically models visual and textual relationships. The third group of models are visual reasoning
models. MAC (Hudson & Manning, 2018) decomposes visual question answering into multiple
attention-based reasoning steps and predicts the answer based on the last step. ALOE (Ding et al.,
2020) achieves state-of-the-art performance on CLEVRER with transformers (Vaswani et al., 2017)."
BASELINES,0.2085889570552147,"Baselines with Reference Videos. We implement some variants of existing baseline models, which
adopt both the target video and reference videos as input. We develop CNN-LSTM (Ref), MAC
(Ref) and ALOE (Ref) from CNN-LSTM, MAC and ALOE by concatenating the features of both
reference videos and the target video as visual input."
BASELINES,0.2116564417177914,"Evaluation. We use the standard accuracy metric to evaluate the performance of different methods.
For multiple-choice questions, we report both the per-option accuracy and per-question accuracy. We
consider a question is correct if the model answers all the options of the question correctly."
BASELINES,0.2147239263803681,"4.2
EVALUATION ON PHYSICAL REASONING."
BASELINES,0.21779141104294478,"We summarize the question-answering results of different baseline models in Table 2. We also Ô¨Ånd
that models have different relative performances on different kinds of questions, indicating that
different kinds of questions in ComPhy require different reasoning skills."
BASELINES,0.22085889570552147,"Factual Reasoning. Factual questions in ComPhy require models to recognize objects‚Äô visual
attributes, analyze their moving trajectories and identify their physics property to answer the questions.
Based on the result, we have the following observation. First, we Ô¨Ånd that the ‚Äúblind‚Äù models,
Random, Frequent and Blind-LSTM achieve much worse performance than other video question
answering models and reasoning models. This shows the importance of modeling both visual context
and linguistic information on ComPhy. We also Ô¨Ånd that video question answering models CNN-
LSTM and HCRN perform worse than visual reasoning models MAC and ALOE. We believe the
reasons are that models like HCRN are mostly designed for human action video, which mainly
focuses on temporal modeling of action sequences rather than spatial-temporal modeling of physical
events. Among all the baselines, we Ô¨Ånd that MAC performs the best on factual questions, showing
the effectiveness of its compositional attention mechanism and iterative reasoning processes."
BASELINES,0.22392638036809817,"Dynamcis Reasoning. An important feature of ComPhy is that it requires models to make coun-
terfactual and future dynamic predictions based on their identiÔ¨Åed physical property to answer the
questions. Among all the baselines, we Ô¨Ånd that ALOE (Ref) achieves the best performance on coun-
terfactual and future reasoning. We think the reason is that the self-attention mechanism and object
masking self-supervised techniques provides the model with the ability to model spatio-temporal
visual context and imaging counterfactual scenes to answer the questions."
BASELINES,0.22699386503067484,Published as a conference paper at ICLR 2022
BASELINES,0.23006134969325154,"1
4
3
1
4
3
1
4
3 2
3 1 2
3 1 2
3 1 1 2 1 2 1 2"
BASELINES,0.2331288343558282,"Physical 
Property"
BASELINES,0.2361963190184049,Learner 1 2 1 3 4
BASELINES,0.2392638036809816,"Q: ‚ÄúWhat would happen if the purple 
object were heavier?‚Äù
C: ‚ÄúThe green cube would collide with 
the metal sphere‚Äù"
BASELINES,0.24233128834355827,Question
BASELINES,0.24539877300613497,Parser
BASELINES,0.24846625766871167,Execution
BASELINES,0.25153374233128833,Engine
BASELINES,0.254601226993865,"Perception
Perception
Perception"
BASELINES,0.25766871165644173,"Physical 
Property"
BASELINES,0.2607361963190184,Learner
BASELINES,0.26380368098159507,"Reference Videos ‚Ä¶
‚Ä¶
‚Ä¶"
BASELINES,0.2668711656441718,Property Graph
BASELINES,0.26993865030674846,Target Video
BASELINES,0.27300613496932513,Question / Choice ‚Äú‚úì‚Äù
BASELINES,0.27607361963190186,"Answer
‚Ä¶"
BASELINES,0.2791411042944785,"Physical 
Property"
BASELINES,0.2822085889570552,Learner 1 2 3 4
BASELINES,0.2852760736196319,"I. Perception
II. Property Inference
III. Dynamics Prediction 1 2
3 1 2
3 ùë°
ùë°+1"
BASELINES,0.2883435582822086,IV. Symbolic Execution
BASELINES,0.29141104294478526,"Figure 3: The perception module detects objects‚Äô location and visual appearance attributes. The
physical property learner learns objects‚Äô properties based on detected object trajectories. The dynamic
predictor predicts objects‚Äô dynamics in the counterfactual scene based on objects‚Äô properties and
locations. Finally, an execution engine runs the program parsed by the language parser on the
predicted dynamic scene to answer the question."
BASELINES,0.294478527607362,"Reasoning with Reference Videos. We observe that CNN-LSTM (Ref) and MAC (Ref) achieve
only comparable or even slightly worse performance comparing with their original models, CNN-
LSTM and MAC. We also Ô¨Ånd that ALOE (Ref) achieves better performance than ALOE. However,
the increment from ALOE to ALOE (Ref) is limited. All these variant models can not get much
gain from their original models by concatenating the reference videos as additional visual input. We
think the reason is that these models are based on massive training videos and question-answer pairs
and have difÔ¨Åculties adapting to the new scenario in ComPhy, which requires models to learn the new
compositional visible and hidden physical properties from only a few examples."
BASELINES,0.29754601226993865,"Human Performance. To assess human performance on ComPhy, 14 people participated in the
study. We require all the participants to have basic physics knowledge and can speak Ô¨Çuent English.
Participants were Ô¨Årst shown with a few demo videos and questions to test that they understood the
visual events, physical properties, and the text description. Participants are then asked to answer
25 question samples of different kinds in ComPhy. And we record accuracy of 90.6% for factual
questions, 88.0% for predictive per option, 80.0% counterfactual per option, 75.9% for predictive per
question and 52.9% for counterfactual per question. This shows that while our questions are difÔ¨Åcult
for machine models, humans can still well understand the physical properties and make dynamic
predictions in ComPhy to answer the corresponding questions."
COMPOSITIONAL PHYSICS LEARNER,0.3006134969325153,"5
COMPOSITIONAL PHYSICS LEARNER"
MODEL,0.30368098159509205,"5.1
MODEL"
MODEL,0.3067484662576687,"We propose an oracle model that performs compositional dynamics reasoning based on a neural-
symbolic framework. Inspired by the recent models (Yi et al., 2018; 2020), we factorize the process
of physical reasoning into several stages. As shown in Fig. 3, our model consists of four major stages:
perception (I), property inference (II), dynamic prediction (III), and symbolic dynamics reasoning
(IV). Given a target video, 4 reference videos, and a query question, we Ô¨Årst detect objects‚Äô location
and appearance attributes with a perception module. Objects‚Äô trajectories are fed into a physical
property learner to learn their physical properties. Given objects‚Äô properties and location, the dynamic
predictor predicts objects‚Äô dynamics based on the physical properties. Finally, an execution engine
runs the program from the language parser on the predicted objects‚Äô motion to answer the question."
MODEL,0.3098159509202454,"Our model‚Äôs core contribution lies in a physical property learner that infers the hidden per-object
and pairwise properties from the videos and a dynamics model that predicts dynamics based on the
inferred physical properties. These modules enable our model to tackle challenging physics reasoning
tasks that involve physical properties that are not directly observable."
MODEL,0.3128834355828221,Published as a conference paper at ICLR 2022
MODEL,0.3159509202453988,"Perception. Our perception module detects and recognizes all objects at every video frame. Given an
input frame, our model Ô¨Årst applies a Mask-RCNN (He et al., 2017) to detect all objects in the frame
and extract corresponding visual attributes like color, shape, and material. The perception module
outputs the static attributes of all objects as well as their full-motion trajectories during the video.
Please refer to (Yi et al., 2018) for further details."
MODEL,0.31901840490797545,"Physical Property Inference. The physical property learner (PPL) lies at the core of our model‚Äôs
capability to tackle complex and compositional physical interactions. Given the object motion
trajectories from all reference and target videos, the PPL predicts the mass and relative charge
between each object pair in the video. Under the hood, PPL is implemented using a graph neural
network (Kipf et al., 2018) where the node features contain object-centric property (mass) and edge
features encode pairwise properties (relative charge).. Given the input trajectories {xi}N
i=1 of N
objects in the video, PPL performs the following message passing operations,"
MODEL,0.3220858895705521,"v0
i = femb(xi),
el
i,j = f l
rel(vl
i, vl
j),
vl+1
i
= f l
enc(
X"
MODEL,0.32515337423312884,"iÃ∏=j
el
i,j),
(1)"
MODEL,0.3282208588957055,"where l ‚àà[0, 1] denotes the message passing steps and xi denotes the concatenation of the normalized
detected object coordinates {xi,t, yi,t}T
t=1 over all T frames. f(...) are functions implemented by
fully-connected layers. We then use two fully-connected layer to predict the output mass label
f pred
v
(v2
i ) and edge charge label f pred
e
(e1
i,j), respectively."
MODEL,0.3312883435582822,"We further note that because the system is invariant under charge inversion, the charge property is
described by the relative signs between each object pair, even though the charge carried by each
individual object should be easy to recover given the pairwise relation plus the true sign of one object.
The full physical property of a video set can be represented as a fully connected property graph. Each
node represents an object that appears in at least one of the videos from the set. And each edge
represents if the two nodes the edge connects carry the same, opposite, or no relative charge (that is,
one or both objects are charge-neutral). As shown in Fig. 3(II), for each reference video, PPL only
predicts the properties of the objects it covers, revealing only parts of the property graph. We align
the predictions of different nodes and edges by objects‚Äô static attributes predicted by the perception
module. The full object properties are obtained by combining all the subgraphs generated by each
video from the set via max-pooling over node and edge."
MODEL,0.3343558282208589,"Dynamic Prediction based on Physical Property. Given the object trajectories at the t-th frame and
their corresponding object properties (i.e., mass and charge), we need to predict objects‚Äô positions at
the t+1 frame. We achieve this using a graph-neural-network-based dynamic predictor. We represent
the i-th object at the t-th frame with ot,0
i
= ||t
t‚àí3(xt
i, yt
i, wt
i, , ht
i, mi), which is a concatenation of
the object location (xt
i, yt
i), size (wt
i, ht
i) and the mass label (mi) over a history window of 3. We
present objects by the locations of a history rather than the only location at the t-th frame to encode
the velocity and account for the perception error. SpeciÔ¨Åcally, we have"
MODEL,0.3374233128834356,"ht,0
i,j =
X"
MODEL,0.34049079754601225,"k
zi,j,kgk
emb(ot,0
i , ot,0
j ),
ot,l+1
j
= ot,l
j + gl
rel(
X"
MODEL,0.34355828220858897,"iÃ∏=j
(ht,l
i,j)),"
MODEL,0.34662576687116564,"ht,l+1
i,j
=
X"
MODEL,0.3496932515337423,"k
zi,j,kgk,l
enc([ot,l+1
i
, ot,0
i ], [ot,l+1
j
, ot,0
j ]),
(2)"
MODEL,0.35276073619631904,"where k ‚àà{0, 1, 2} represents if the two connected nodes carry the same, opposite, or no relative
charge. zi,j,k are the k-th element of the one-hot indication vector zi,j. l ‚àà[0, 1] denotes the message
passing steps and, g(...) are functions implemented by fully-connected layers. We predict object
location and size at the t + 1-th frame using a function that consists of one fully connected layer
gpred(ot,2
j ). Given the target video, we predict the future by initializing dynamic predictor input
with the last three frames of the target video and then iteratively predict the future by feeding the
prediction back to our model. We get the physical property-based prediction for the counterfactual
scenes by using the Ô¨Årst 3 frames in the target video as input and updating their mass label (mi) and
one-hot indicator vector zi,j correspondingly.
Symbolic Execution. With the object visual attributes, physical property, and dynamics ready, we
use a question parser to transform the question and choices into functional programs and perform
step-by-step symbolic execution to get the answer. We provide more details in the Appendix."
PERFORMANCE ANALYSIS,0.3558282208588957,"5.2
PERFORMANCE ANALYSIS
Effectiveness of Physical Property Learning. We report the performance of the proposed oracle
CPL in Table 3. We can see that CPL shows better performance on all kinds of questions compared"
PERFORMANCE ANALYSIS,0.3588957055214724,Published as a conference paper at ICLR 2022
PERFORMANCE ANALYSIS,0.3619631901840491,"Methods Factual
Predictive
Counterfactual"
PERFORMANCE ANALYSIS,0.36503067484662577,per opt. per ques. per opt. per ques.
PERFORMANCE ANALYSIS,0.36809815950920244,"CPL
80.5
75.3
56.4
68.3
29.1
NS-DR+
-
73.3
50.8
61.1
16.6
CPL-Gt
100
87.6
74.0
74.0
35.3"
PERFORMANCE ANALYSIS,0.37116564417177916,Table 3: Evaluation of CPL on ComPhy.
PERFORMANCE ANALYSIS,0.37423312883435583,"Factual
Predictive"
PERFORMANCE ANALYSIS,0.3773006134969325,per opt.
PERFORMANCE ANALYSIS,0.3803680981595092,Predictive
PERFORMANCE ANALYSIS,0.3834355828220859,per ques.
PERFORMANCE ANALYSIS,0.38650306748466257,Counterfactual
PERFORMANCE ANALYSIS,0.3895705521472393,per opt.
PERFORMANCE ANALYSIS,0.39263803680981596,Counterfactual
PERFORMANCE ANALYSIS,0.39570552147239263,per ques. 0 20 40
ALOE,0.3987730061349693,"60
ALOE
ALOE (Ref)
CPL"
ALOE,0.401840490797546,Figure 4: Generalization of physical reasoning.
ALOE,0.4049079754601227,"to the baseline methods in Table 2. The high accuracy on factual questions shows that CPL is able
to inference objects‚Äô physical properties from few video examples and combine them with other
appearance properties and events for question answering. SpeciÔ¨Åcally, we compare the mass and
charge edge label prediction result with the ground-truth labels and Ô¨Ånd that they achieve an accuracy
of 90.4% and 90.8%. This shows that that the PPL in CPL are able to learn physical properties
from objects‚Äô trajectories and interactions in a supervised manner. We also implement a graph
neural network baseline that only relies on the target video without reference videos. It achieves
an accuracy of 59.4% and 70.2% on mass and charge edge labels. This indicates the importance of
reference videos for physical property inference. For better analysis, we have reported a baseline
with ground-truth object properties and visual static and dynamic attributes (CPT-Gt). Although it
achieves constant gains, there is still much room for further improvement. This shows the bottleneck
of ComPhy lies in predicting physical-based dynamics."
ALOE,0.40797546012269936,"Dynamics Reasoning. The better performance in counterfactual and predictive questions indicates
that CPL can image objects‚Äô motions in counterfactual and future scenes based on the identiÔ¨Åed
physical properties to some extent. We also notice that there is still a large gap between CPL‚Äôs
performance and human performance shown in Section 4.2 especially on counterfactual reasoning.
We Ô¨Ånd that the dynamic predictor in CPL still shows its limitation on long-term dynamic prediction.
This suggests that a stronger dynamic predictor may further boost CPL‚Äôs performance."
ALOE,0.4110429447852761,"Comparison between CPL and NS-DR+. We also implement a variant of the NS-DR (Yi et al.,
2020) model for better analysis. NS-DR does not explicitly consider changes in physical properties
like mass and charge; thus it cannot run the symbolic programs related to the physical properties
in ComPhy. To run NS-DR successfully in ComPhy, we provide NS-DR with extra ground-truth
physical property labels. The variant, NS-DR+ uses the PropNet (Li et al., 2019b) for dynamics
predictions, which does not consider the mass and charge information of the objects. Compared to
our model, NS-DR+ assumes extra information and can directly answer factual questions using the
supplied ground-truth labels; thus we ignore the comparison on factual questions and focus on the
evaluation on the models‚Äô ability for dynamic prediction. As shown in Table 3, CPL achieves much
better performance than NS-DR+ on counterfactual questions and predictive questions, showing the
importance of modeling mass and charges on nodes and edges of the graph neural networks."
ALOE,0.41411042944785276,"Generalization to More Complex Scenes. To test the model‚Äôs generalization ability, we additionally
generate a new dataset that contains videos with more objects (6 to 8) and contains both an object
with a large mass and a pair of charged objects. We show the performance of the best baseline
models ALOE, ALOE (Ref) and CPL in Fig. 4. Both ALOE models and CPL have a large drop
in question-answering performance. We believe the reason is that the transformer layer in ALOE,
the graph neural network based-physical property learner and dynamics predictor in CPL show
limitation on more complex visual dynamic scenes, which has a different statistic distribution as the
scenes in the original training set. We leave the development of more powerful models with stronger
generalization ability as future work."
CONCLUSIONS,0.4171779141104294,"6
CONCLUSIONS"
CONCLUSIONS,0.42024539877300615,"In this paper, we present a benchmark named ComPhy for physical reasoning in videos. Given few
video examples, ComPhy requires models to identify objects‚Äô intrinsic physical properties and predict
their dynamics in counterfactual and future scenes based on the properties to answer the questions. We
evaluate various existing state-of-the-art models on ComPhy, but none of them achieves satisfactory
performance, showing that physical reasoning in videos is still a challenging problem and needs
further exploration. We also develop an oracle model named CPL, using graph neural networks to
infer physical properties and predict dynamics. We hope that ComPhy and CPL can attract more
research attention in physical reasoning and building more powerful physical reasoning models."
CONCLUSIONS,0.4233128834355828,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.4263803680981595,"Acknowledgement This work was supported by MIT-IBM Watson AI Lab and its member company
Nexplore, ONR MURI, DARPA Machine Common Sense program, ONR (N00014-18-1-2847), and
Mitsubishi Electric."
CONCLUSIONS,0.4294478527607362,Published as a conference paper at ICLR 2022
REFERENCES,0.4325153374233129,REFERENCES
REFERENCES,0.43558282208588955,"Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence
Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015. 6"
REFERENCES,0.4386503067484663,"Tayfun Ates, Muhammed Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, Erkut Erdem,
Aykut Erdem, Tilbe Goksun, and Deniz Yuret. Craft: A benchmark for causal reasoning about
forces and interactions. arXiv preprint arXiv:2012.04293, 2020. 2, 3, 5, 14"
REFERENCES,0.44171779141104295,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In ICLR, 2015. 14"
REFERENCES,0.4447852760736196,"Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre:
A new benchmark for physical reasoning. In Advances in Neural Information Processing Systems,
volume 32, 2019. 1, 2"
REFERENCES,0.44785276073619634,"Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counter-
factual learning of physical dynamics. In International Conference on Learning Representations,
2020. 1, 3"
REFERENCES,0.450920245398773,"Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and koray kavukcuoglu.
Interaction networks for learning about objects, relations and physics. In Advances in Neural
Information Processing Systems, volume 29, 2016. 3"
REFERENCES,0.4539877300613497,"Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. Simulation as an engine of physical
scene understanding. Proceedings of the National Academy of Sciences, 110(45):18327‚Äì18332,
2013. 3"
REFERENCES,0.4570552147239264,"Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick,
and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv
preprint arXiv:1901.11390, 2019. 15"
REFERENCES,0.4601226993865031,"Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B Tenenbaum. A compositional
object-based approach to learning physical dynamics. arXiv preprint arXiv:1612.00341, 2016. 3"
REFERENCES,0.46319018404907975,"Zhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee Kenneth Wong. Weakly-supervised spatio-
temporally grounding natural sentence in video. In ACL, 2019. 3"
REFERENCES,0.4662576687116564,"Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B Tenenbaum, and
Chuang Gan. Grounding physical concepts of objects and events through dynamic visual reasoning.
In International Conference on Learning Representations, 2021. 2"
REFERENCES,0.46932515337423314,"Blender Online Community. Blender - a 3d modelling and rendering package, 2018. URL http:
//www.blender.org. 4, 14"
REFERENCES,0.4723926380368098,"Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics
and machine learning. http://pybullet.org, 2016‚Äì2021. 4, 14"
REFERENCES,0.4754601226993865,"David Ding, Felix Hill, Adam Santoro, and Matt Botvinick. Attention over learned object embeddings
enables complex visual reasoning. arXiv, 2020. 5, 6"
REFERENCES,0.4785276073619632,"Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Joshua B Tenenbaum, and Chuang Gan. Dynamic
visual reasoning by learning differentiable physics models from video and language. In NeurIPS,
2021. 3"
REFERENCES,0.4815950920245399,"Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and temporal
reasoning. In ICLR, 2020. 2"
REFERENCES,0.48466257668711654,"Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: A benchmark for
compositional spatio-temporal reasoning. In CVPR, 2021. 3"
REFERENCES,0.48773006134969327,"Jessica B Hamrick, Peter W Battaglia, Thomas L GrifÔ¨Åths, and Joshua B Tenenbaum. Inferring mass
in complex scenes by mental simulation. Cognition, 157:61‚Äì76, 2016. 3"
REFERENCES,0.49079754601226994,Published as a conference paper at ICLR 2022
REFERENCES,0.4938650306748466,"Chi Han, Jiayuan Mao, Chuang Gan, Joshua B. Tenenbaum, and Jiajun Wu.
Visual Concept
Metaconcept Learning. In NeurIPS, 2019. 3"
REFERENCES,0.49693251533742333,"Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history
of 2d cnns and imagenet? In CVPR, 2018. 15"
REFERENCES,0.5,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016. 15"
REFERENCES,0.5030674846625767,"Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In CVPR, 2017. 8, 14"
REFERENCES,0.5061349693251533,"Yining Hong, Li Yi, Joshua B Tenenbaum, Antonio Torralba, and Chuang Gan. Ptr: A benchmark for
part-based conceptual, relational, and physical reasoning. In NeurIPS, 2021. 2"
REFERENCES,0.50920245398773,"Yining Hong, Kaichun Mo, Li Yi, Guibas Leonidas, Joshua B Tenenbaum, Antonio Torralba, and
Chuang Gan. Fixing malfunctional objects with learned physical simulation and functional
prediction. In CVPR, 2022. 3"
REFERENCES,0.5122699386503068,"Drew A Hudson and Christopher D Manning. Compositional attention networks for machine
reasoning. In ICLR, 2018. 6"
REFERENCES,0.5153374233128835,"Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-
temporal reasoning in visual question answering. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 2017. 3"
REFERENCES,0.5184049079754601,"Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. In CVPR, 2017. 3, 5"
REFERENCES,0.5214723926380368,"Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In International Conference on Machine Learning, pp. 2688‚Äì
2697. PMLR, 2018. 8"
REFERENCES,0.5245398773006135,"Thomas N. Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017. 3"
REFERENCES,0.5276073619631901,"Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation
networks for video question answering. In CVPR, 2020. 6"
REFERENCES,0.5306748466257669,"Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video
question answering. In EMNLP, 2018. 3"
REFERENCES,0.5337423312883436,"Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvqa+: Spatio-temporal grounding for video
question answering. In Tech Report, arXiv, 2019. 3"
REFERENCES,0.5368098159509203,"Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example.
In International conference on machine learning, pp. 430‚Äì438. PMLR, 2016. 3"
REFERENCES,0.5398773006134969,"Qing Li, Siyuan Huang, Yining Hong, and Song-Chun Zhu. A competence-aware curriculum for
visual concepts learning via question answering. In ECCV, 2020. 3"
REFERENCES,0.5429447852760736,"Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, and Antonio Torralba. Learning particle
dynamics for manipulating rigid bodies, deformable objects, and Ô¨Çuids. In ICLR, 2019a. 3"
REFERENCES,0.5460122699386503,"Yunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B Tenenbaum, Antonio Torralba, and Russ Tedrake.
Propagation networks for model-based control under partial observation. In ICRA, 2019b. 9"
REFERENCES,0.549079754601227,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017. 14"
REFERENCES,0.5521472392638037,"Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas,
Abhijit Gupta, Caiming Xiong, Richard Socher, and Dragomir Radev. Esprit: explaining solutions
to physical reasoning tasks. In ACL, 2020. 2, 3"
REFERENCES,0.5552147239263804,Published as a conference paper at ICLR 2022
REFERENCES,0.558282208588957,"Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, V√©ronique
Izard, and Emmanuel Dupoux. Intphys: A framework and benchmark for visual intuitive physics
reasoning. arXiv preprint arXiv:1803.07616, 2018. 2, 3, 4, 5"
REFERENCES,0.5613496932515337,"Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter
Battaglia. Learning to simulate complex physics with graph networks. In International Conference
on Machine Learning, pp. 8459‚Äì8468. PMLR, 2020. 3"
REFERENCES,0.5644171779141104,"Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
NeurIPS, 2017. 3"
REFERENCES,0.5674846625766872,"Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja
Fidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of
the IEEE conference on computer vision and pattern recognition, 2016. 3"
REFERENCES,0.5705521472392638,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 6"
REFERENCES,0.5736196319018405,"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In NeurIPS, 2016. 3"
REFERENCES,0.5766871165644172,"Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. Star: A benchmark for
situated reasoning in real-world videos. In NeurIPS, 2021. 3"
REFERENCES,0.5797546012269938,"Jiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenenbaum. Galileo: Perceiving
physical object properties by integrating a physics engine with deep learning. Advances in neural
information processing systems, 28:127‚Äì135, 2015. 3"
REFERENCES,0.5828220858895705,"Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In CVPR, 2017. 15"
REFERENCES,0.5858895705521472,"Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B Tenenbaum.
Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. In
Advances in Neural Information Processing Systems (NIPS), 2018. 2, 7, 8"
REFERENCES,0.588957055214724,"Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B
Tenenbaum. Clevrer: Collision events for video representation and reasoning. In International
Conference on Learning Representations, 2020. 1, 2, 3, 4, 5, 7, 9, 14, 15"
REFERENCES,0.5920245398773006,Published as a conference paper at ICLR 2022
REFERENCES,0.5950920245398773,"A
EXAMPLES FROM COMPHY"
REFERENCES,0.598159509202454,"Here we provide more examples from ComPhy in Fig. 5 and Fig. 6. From these examples, we can
see the following features of ComPhy. First, to answer the factual questions, models not only need
to recognize objects‚Äô visual appearance attributes and events in the video but also identify their
intrinsic physical properties from the given video set. Second, to answer counterfactual and predictive
questions, models needs to predict objects‚Äô dynamics in counterfactual or future scenes, which can be
severely affected by intrinsic physical properties. We also show some typical question and choice
samples as well as their underlying reasoning program logic in Fig. 7 and Fig. 8."
REFERENCES,0.6012269938650306,"B
VIDEO GENERATION"
REFERENCES,0.6042944785276073,"We provide more details for video generation. The generation of the videos in ComPhy can be
decomposed into two steps. First, we adopt a physical engine Bullet Coumans & Bai (2016‚Äì2021)
to simulate objects‚Äô motions and their interactions with each other. Since Bullet does not ofÔ¨Åcially
support the effect of electronic charges, we add external forces between charged objects, whose
values are inversely proportional to the square of the objects‚Äô distance, to simulated Coulomb forces.
We assign the light object with a mass value of 1 and assign heavy object with a mass value of 5.
We manually make sure that each reference video at least contain an interaction (collision, charge
and mass) among objects to provide enough information for physical property inference. And each
object should appear at least once in the reference videos. The simulated objects‚Äô motions are sent to
BlenderCommunity (2018) to render high-quality image sequences."
REFERENCES,0.6073619631901841,"C
QUESTION TEMPLATES"
REFERENCES,0.6104294478527608,"We show the new question templates that have not been introduced before in table 4. As shown in
the table, we can see that the new question templates have more symbolic operators related to the
physical properties. Phrases like ‚Äúheavy moving spheres‚Äù and ‚Äúcharged cubes‚Äù require
models to infer objects‚Äô physical property values; phrases like ‚Äú...
heavier than ...‚Äù
require models to compare the relative physical property values of two objects. For counterfactual
questions, we have new conditions like ‚ÄúIf the cyan object were uncharged‚Äù and ‚ÄúIf
the sphere were lighter‚Äù. They aim to reason the dynamics under the hypothesis that a
certain object carrying a different physical property. Such new features in language make ComPhy
unique and challenging."
REFERENCES,0.6134969325153374,"D
CPL DETAILS"
REFERENCES,0.6165644171779141,"In this section, we provide more details for the proposed Compositional Physics Learner (CPL).
Inspired by the NS-DR model in (Yi et al., 2020), CPL decomposes physical reasoning in ComPhy
into four main components, visual perception, physical property learning, property based dynamic
predictor and symbolic execution."
REFERENCES,0.6196319018404908,"The symbolic execution component Ô¨Årst adopts a program parser to parse the query question into a
functional program, containing a series of neural operations. The program parser is an attention-based
seq2seq model Bahdanau et al. (2015), whose input is the word sequence in the question/choice and
output is the sequence of neural operations. The symbolic executor then executes the operations on the
predicted dynamic scene to get the answer to the question. We summarize all the symbolic operations
in CPL in table 5. Compared with the previous benchmarks Yi et al. (2020); Ates et al. (2020),
ComPhy has more operation on physical property identiÔ¨Åcation, comparison and corresponding
dynamic prediction."
REFERENCES,0.6226993865030674,"We train the Mask-RCNN He et al. (2017) in the perception module with 4,000 frames randomly-
selected from the training set of ComPhy. We train the program parser and the property concept
learner with program and property labels using cross-entropy loss. We optimize the dynamic predictor
with mean square error loss between the predicted objects‚Äô trajectories and the detected objects‚Äô
trajectories by the perception module. We train the all the modules using Pytorch library Paszke et al.
(2017) on Titan Nvidia GTX 1080-Ti GPUs."
REFERENCES,0.6257668711656442,Published as a conference paper at ICLR 2022
REFERENCES,0.6288343558282209,"Question Type
Template and Example"
REFERENCES,0.6319018404907976,"Counterfact mass
If the _SA_ were _MP_, _Q_?
If the sphere were lighter, which event would not happen?"
REFERENCES,0.6349693251533742,"Counterfact charge
If the _SA_ were _CP_, _Q_?
If the cyan object were uncharged, which event would happen?"
REFERENCES,0.6380368098159509,"Query
What is the _H_ of the _DA_ _SA_ that is _PA_?
What is the color of the moving cylinder that is heavy?"
REFERENCES,0.6411042944785276,"Exist
Are there any _PA_ _DA_ _SA_ _TI_?
Are there any charged stationary cube?"
REFERENCES,0.6441717791411042,"Count
How many _PA_ _DA_ _SA_ are there _TI_?
How many heavy moving spheres are there when the video ends?"
REFERENCES,0.647239263803681,"Mass compare
Is the _DA1_ _SA1_ heavier than the _DA2_ _SA2_?
Is the blue sphere heavier than the gray cube?"
REFERENCES,0.6503067484662577,"Compare charge 1
Are the _DA1_ _SA1_ and the _DA2_ _SA2_ oppositely charged?
Are the blue sphere and the purple sphere oppositely charged?"
REFERENCES,0.6533742331288344,"Compare charge 2
Are the _DA1_ _SA1_ and the _DA2_ _SA2_ with the same type of charge?
Are the blue cube and the brown cylinder with the same type of charge?"
REFERENCES,0.656441717791411,"Query both
What are the _Hs_ of the two objects that are charged?
What are the colors of the two objects that are charged?"
REFERENCES,0.6595092024539877,"Table 4: Question templates and examples in ComPhy. _SA_ denotes static attributes like ‚Äúred‚Äù
and ‚Äúsphere‚Äù; _DA_ denotes dynamic attributes, ‚Äúmoving‚Äù and ‚Äústationary‚Äù; _MP_ de-
notes mass attributes, ‚Äúlighter‚Äù and ‚Äúheavier‚Äù; _Q_ denotes question phrases like ‚Äúwhich
of the following would happen‚Äù; _CP_ denotes charge attributes, ‚Äúuncharged‚Äù and
‚Äúoppositely charged‚Äù; _H_ denotes visible concepts, ‚Äúcolor‚Äù, ‚Äúshape‚Äù and material;
_PA_ denotes physical attributes, heavy, light, charged and ‚Äúuncharged‚Äù; _TI_ denotes
time indicators like ‚Äúwhen the video ends‚Äù."
REFERENCES,0.6625766871165644,"E
BASELINE IMPLEMENTATION DETAILS"
REFERENCES,0.6656441717791411,"In this section, we provide more details for baselines in the experimental section. We implement
baselines based on the publicly-available source code. For multiple-choice questions, we indepen-
dently concatenate the words of each option and the question as a binary classiÔ¨Åcation question.
Similar to CLEVRER (Yi et al., 2020), we use ResNet-50 (He et al., 2016) to extract visual feature
sequences for CNN+LSTM and MAC and variants with reference videos. We evenly sample 25
frames for each target video and 10 frames for each reference video. For HCRN, we use appearance
feature from ResNet-101 (He et al., 2016) and motion feature from ResNetXt-101 (Xie et al., 2017;
Hara et al., 2018) following the ofÔ¨Åcial implementation. For ALOE, we use MONet(Burgess et al.,
2019) to extract visual representation and sample 25 frames for each target video. For ALOE (Ref),
we sample 10 frames for each reference video and concatenate the reference frames and the target
frames as visual representation. We train all the models until they are fully converged, select the best
checkpoint on the validation set and Ô¨Ånally test on the testing set."
REFERENCES,0.6687116564417178,Published as a conference paper at ICLR 2022
REFERENCES,0.6717791411042945,"Type
Operation
Signature"
REFERENCES,0.6748466257668712,"Counterfact
Operation"
REFERENCES,0.6779141104294478,"Counterfactual_mass_heavy
(object) ‚Üíevents
Return all events after making the object heavy
Counterfactual_mass_light
(object) ‚Üíevents
Return all events after making the object light
Counterfactual_uncharged
(object) ‚Üíevents
Return all events after making the object uncharged
Counterfactual_opposite_charged
(object) ‚Üíevents
Return all events after making the object oppositely charged"
REFERENCES,0.6809815950920245,"Object
Property
Operations"
REFERENCES,0.6840490797546013,"filter_heavy
(objects) ‚Üíobjects
select all the heavy objects
filter_light
(objects) ‚Üíobjects
select all the light objects
filter_charged
(objects) ‚Üíobjects
select all the charged objects
filter_uncharged
(objects) ‚Üíobjects
select all the uncharged objects"
REFERENCES,0.6871165644171779,"Object
Appear-
ance
Operations"
REFERENCES,0.6901840490797546,"Filter_static_attr
(objects, attr) ‚Üíobjects
Select objects from the input list with the input static attribute
Filter_dynamic_attr
(objects, attr, frame) ‚Üíobjects
Selects objects in the input frame with the dynamic attribute"
REFERENCES,0.6932515337423313,"Event
Operations"
REFERENCES,0.696319018404908,"Filter_event
(events, objects) ‚Üíevents
Select all events that involve the input objects
Get_col_partner
(event, object) ‚Üíobject
Return the collision partner of the input object
Filter_before
(events, events) ‚Üíevents
Select all events before the target event
Filter_after
(events, events) ‚Üíevents
Select all events after the target event
Filter_order
(events, order) ‚Üíevent
Select the event at the speciÔ¨Åc time order
Get_frame
(event) ‚Üíframe
Return the frame of the input event in the video"
REFERENCES,0.6993865030674846,"Others
Unique
(events/objects) ‚Üíevent/object
Return the only event/object in the input list"
REFERENCES,0.7024539877300614,"Input
Operations"
REFERENCES,0.7055214723926381,"Start
() ‚Üíevent
Returns the special ‚Äústart‚Äù event
end
() ‚Üíevent
Returns the special ‚Äúend‚Äù event
Objects
() ‚Üíobjects
Returns all objects in the video
Events
() ‚Üíevents
Returns all events happening in the video
UnseenEvents
() ‚Üíevents
Returns all future events happening in the video"
REFERENCES,0.7085889570552147,"Output
Operations"
REFERENCES,0.7116564417177914,"Query_both_attribute
(object, object) ‚Üíattr
Returns the attributes of the input two objects
Query_direction
(object, frame) ‚Üíattr
Returns the direction of the object at the input frame
Is_heavier
(obj1, obj2) ‚Üíbool
Returns ‚Äúyes‚Äù if obj1 is heavier than obj2
Is_lighter
(obj1, obj2) ‚Üíbool
Returns ‚Äúyes‚Äù if obj1 is lighter than obj2
Query_attribute
(object) ‚Üíattr
Returns the attribute of the input objects like color
Count
(objects) ‚Üíint
Returns the number of the input objects/ events
(events) ‚Üíint
Exist
(objects) ‚Üíbool
Returns ‚Äúyes‚Äù if the input objects is not empty
Belong_to
(event, events) ‚Üíbool
Returns ‚Äúyes‚Äù if the input event belongs to the input event sets
Negate
(bool) ‚Üíbool
Returns the negation of the input boolean"
REFERENCES,0.7147239263803681,"Table 5: Symbolic operations of CPL on ComPhy. In this table, ‚Äúorder‚Äù denotes the chronological
order of an event, e.g. ‚ÄúFirst‚Äù and ‚ÄúLast‚Äù; ‚Äústatic attribute‚Äù denotes object static concepts like ‚ÄúRed‚Äù
and ‚ÄúRubber‚Äù and ‚Äúdynamic attribute‚Äù represents object dynamic concepts like ‚ÄúMoving‚Äù."
REFERENCES,0.7177914110429447,Published as a conference paper at ICLR 2022
REFERENCES,0.7208588957055214,"I. Factual
Q1: Is the cyan cube heavier than the rubber cylinder? A: No.     
Q2: Are there any blue cylinders that enter the scene? A:Yes."
REFERENCES,0.7239263803680982,"II. Counterfactual
Q3: If the rubber cylinder were lighter, which of the 
following would happen?"
REFERENCES,0.7269938650306749,"a) The cube would collide with the rubber cylinder ‚àö
b) The rubber cylinder and the sphere would collide ‚àö"
REFERENCES,0.7300613496932515,c) The metal object would collide with the sphere √ó
REFERENCES,0.7331288343558282,"III. Predictive
Q4: What will happen next?"
REFERENCES,0.7361963190184049,"a) The rubber cylinder and the metal object collide ‚àö
b) The rubber cylinder and the sphere collide ‚àö
c) The cube collides with the sphere √ó"
REFERENCES,0.7392638036809815,"1
2
time
Reference video 1 1"
REFERENCES,0.7423312883435583,"time
Reference video 2"
REFERENCES,0.745398773006135,"time
Target video"
REFERENCES,0.7484662576687117,"1
2
time
Reference video 3
time
Reference video 4 1
2"
REFERENCES,0.7515337423312883,"1
2
1
2"
REFERENCES,0.754601226993865,"1
2
3
4"
REFERENCES,0.7576687116564417,"time
Target video 
1
2
3
4"
REFERENCES,0.7607361963190185,"1
2
time
Reference video 1
time
Reference video 2"
REFERENCES,0.7638036809815951,"1
2
1
2"
REFERENCES,0.7668711656441718,"time
Reference video 3
time
Reference video 4"
REFERENCES,0.7699386503067485,"1
2
1
2
1
2"
REFERENCES,0.7730061349693251,"I. Factual
Q1: What are the colors of the two objects that are charged?   A1: Yellow and blue.
Q2: Are there any metal cubes that enter the scene?  A2: No.
Q3: What is the direction of the blue cube when the video ends? A3: Left."
REFERENCES,0.7760736196319018,"II. Counterfactual
Q3: If the blue sphere were oppositely charged, what would 
happen?"
REFERENCES,0.7791411042944786,"a) The yellow sphere and the rubber cube would collide ‚àö
b) The yellow object and the blue sphere would collide ‚àö
c) The blue cube and the metal cube would collide √ó
d) The yellow object and the red object would collide √ó"
REFERENCES,0.7822085889570553,"III. Predictive
Q4: Which event will happen next?"
REFERENCES,0.7852760736196319,"a) The blue cube and the red cube collide ‚àö
b) The blue sphere collides with the metal cube √ó"
REFERENCES,0.7883435582822086,"Figure 5: Sample target video, reference videos and question-answer pairs from ComPhy."
REFERENCES,0.7914110429447853,Published as a conference paper at ICLR 2022
REFERENCES,0.7944785276073619,"time
Target video 
1
2
3
4 1"
REFERENCES,0.7975460122699386,"time
Reference video 1
time
Reference video 2"
REFERENCES,0.8006134969325154,"1
2
1
2
1
2"
REFERENCES,0.803680981595092,"time
Reference video 3
time
Reference video 4"
REFERENCES,0.8067484662576687,"1
2
1
2
1
2
1
2
1
2"
REFERENCES,0.8098159509202454,"I. Factual
Q1: What color is the moving rubber object that is uncharged?  A1: Rubber.
Q2: In which direction is the red cylinder moving when the yellow object enters the scene? A2: Left.
Q3: How many moving cyan objects are charged?  A3: 1."
REFERENCES,0.8128834355828221,"II. Counterfactual
Q3: If the cyan object were uncharged, which event would 
happen?"
REFERENCES,0.8159509202453987,"a) The sphere and the cyan object would collide ‚àö
b) The cylinder would collide with the sphere ‚àö
c) The cylinder would collide with the gray cube √ó
d) The sphere would collide with the yellow object √ó"
REFERENCES,0.8190184049079755,"Q4: If the sphere were oppositely charged, which of the 
following would happen?"
REFERENCES,0.8220858895705522,"a) The cylinder and the sphere would collide ‚àö
b) The cylinder would collide with the cyan cube √ó"
REFERENCES,0.8251533742331288,"time
Target video 
1
2
3
4
1
2
3
4 1"
REFERENCES,0.8282208588957055,"time
Reference video 1
time
Reference video 2 11"
REFERENCES,0.8312883435582822,"time
Reference video 3
time
Reference video 4"
REFERENCES,0.8343558282208589,"11
22
1"
REFERENCES,0.8374233128834356,"I. Factual
Q1: Is the moving purple object lighter than the moving brown object?  A1: No.
Q2: What is the color of the moving metal object that is light?   A2: Red.
Q3: Are there any brown cylinders that exit the scene after the sphere enters the scene? A3: Yes."
REFERENCES,0.8404907975460123,"II. Counterfactual
Q3: If the sphere were lighter, which event would not
happen?"
REFERENCES,0.843558282208589,"a) The red cylinder and the cube would collide ‚àö
b) The sphere and the cube would collide ‚àö
c) The red cylinder and the sphere would collide √ó
d) The metal cylinder and the rubber cylinder would 
collide√ó"
REFERENCES,0.8466257668711656,"III. Predictive
Q4: Which event will happen next?"
REFERENCES,0.8496932515337423,"a) The sphere collides with the cube ‚àö
b) The metal cylinder and the brown cylinder collide √ó"
REFERENCES,0.852760736196319,"Figure 6: Sample target video, reference videos and question-answer pairs from ComPhy."
REFERENCES,0.8558282208588958,Published as a conference paper at ICLR 2022
REFERENCES,0.8588957055214724,"Q1: How many heavy stationary objects are there when the 
video begins?"
REFERENCES,0.8619631901840491,Filter
REFERENCES,0.8650306748466258,"mass
Objects
Filter
Stationary heavy Count"
REFERENCES,0.8680981595092024,"Events
Filter start"
REFERENCES,0.8711656441717791,"Get
Frame"
REFERENCES,0.8742331288343558,Q4: What shape is the moving metal object that is light?
REFERENCES,0.8773006134969326,"Filter
Mass
Objects
Filter
Material"
REFERENCES,0.8803680981595092,"Filter
Moving"
REFERENCES,0.8834355828220859,"Light
Metal"
REFERENCES,0.8865030674846626,"Query
Shape"
REFERENCES,0.8895705521472392,Q3: How many moving green objects are charged?
REFERENCES,0.8926380368098159,"Filter
color
Objects
Filter
Moving Green"
REFERENCES,0.8957055214723927,"Filter
charge
Count"
REFERENCES,0.8987730061349694,charged
REFERENCES,0.901840490797546,Q2: What color is the moving rubber object that is uncharged?
REFERENCES,0.9049079754601227,"Filter
Material
Objects
Filter
Moving"
REFERENCES,0.9079754601226994,Rubber Query
REFERENCES,0.911042944785276,"Color
Filter
Charge"
REFERENCES,0.9141104294478528,Uncharged
REFERENCES,0.9171779141104295,Figure 7: Sample of factual questions and their underlying functional programs in ComPhy.
REFERENCES,0.9202453987730062,"Q1: If the rubber cylinder were heavier, which of the following 
would happen?"
REFERENCES,0.9233128834355828,"Filter
Material
Objects
Filter
Shape"
REFERENCES,0.9263803680981595,Counterfact Mass
REFERENCES,0.9294478527607362,"Rubber
Cylinder"
REFERENCES,0.9325153374233128,belong to
REFERENCES,0.9355828220858896,"Heavy
Choice
Program"
REFERENCES,0.9386503067484663,"Q2: Which of the following would not happen if the sphere were 
uncharged?"
REFERENCES,0.941717791411043,"Objects
Filter
Shape"
REFERENCES,0.9447852760736196,Counterfact
REFERENCES,0.9478527607361963,Charge
REFERENCES,0.950920245398773,Sphere
REFERENCES,0.9539877300613497,belong to
REFERENCES,0.9570552147239264,"Uncharged
Choice
Program Not"
REFERENCES,0.9601226993865031,C1: The cylinder and the cube would collide
REFERENCES,0.9631901840490797,"Filter
Collision
Events
Filter
Collision"
REFERENCES,0.9662576687116564,"Cylinder
Filter
Shape
Cube
Filter
Shape"
REFERENCES,0.9693251533742331,"Objects
Objects"
REFERENCES,0.9723926380368099,belong to
REFERENCES,0.9754601226993865,"Question
Program"
REFERENCES,0.9785276073619632,C2: The blue object and the metal object would collide
REFERENCES,0.9815950920245399,"Filter
Collision
Events
Filter
Collision"
REFERENCES,0.9846625766871165,"Blue
Filter
Material
Metal
Filter
Color"
REFERENCES,0.9877300613496932,"Objects
Objects"
REFERENCES,0.99079754601227,belong to
REFERENCES,0.9938650306748467,"Question
Program"
REFERENCES,0.9969325153374233,"Figure 8: Sample of counterfactual questions, choice options and their underlying functional programs
in ComPhy."
