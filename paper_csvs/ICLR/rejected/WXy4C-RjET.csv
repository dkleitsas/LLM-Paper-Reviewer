Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0043859649122807015,"Over-parameterized deep networks trained using gradient-based optimizers is a
popular way of solving classiﬁcation and ranking problems. Without appropriately
tuned regularization, such networks have the tendency to make output scores
(logits) and network weights large, causing training loss to become too small and
the network to lose its adaptivity (ability to move around and escape regions of
poor generalization) in the weight space. Adaptive optimizers like Adam, being
aggressive at optimizing the train loss, are particularly affected by this. It is well
known that, even with weight decay (WD) and normal hyper-parameter tuning,
adaptive optimizers lag behind SGD a lot in terms of generalization performance,
mainly in the image classiﬁcation domain.
An alternative to WD for improving a network’s adaptivity is to directly control
the magnitude of the weights and hence the logits.
We propose a method
called Logit Attenuating Weight Normalization (LAWN), that can be stacked
onto any gradient-based optimizer. LAWN initially starts off training in a free
(unregularized) mode and, after some initial epochs, it constrains the weight norms
of layers, thereby controlling the logits and improving adaptivity. This is a new
regularization approach that does not use WD anywhere; instead, the number of
initial free epochs becomes the new hyper-parameter. The resulting LAWN variant
of adaptive optimizers gives a solid lift to generalization performance, making
their performance equal or even exceed SGD’s performance on benchmark image
classiﬁcation and recommender datasets. Another important feature is that LAWN
also greatly improves the adaptive optimizers when used with large batch sizes."
INTRODUCTION,0.008771929824561403,"1
INTRODUCTION"
INTRODUCTION,0.013157894736842105,"The advent of large scale deep models with tens of millions to billions of parameters has resulted in
three trends in the community: (1) State-of-the-art performance via over-parameterized networks for
problems like image classiﬁcation, language modeling, machine translation, text classiﬁcation and
recommender systems. (2) Development of optimizers like Stochastic Gradient Descent (SGD) with
heavy-ball momentum (Qian, 1999), Adam (Kingma & Ba, 2017), AdamW (Loshchilov & Hutter,
2019), LAMB (You et al., 2020) and their extensions with weight decay/`2 regularization to improve
generalization performance. (3) Increased emphasis on theory to understand the optimizer landscape,
especially how to tune different hyperparameters well to escape poor minima for both large and small
batch sizes. In this work, inspired by all three trends, we propose a new training method, explain why
it works, and show vastly improved performance over the weight decay method when applied with
adaptive optimizers across a wide range of batch sizes for classiﬁcation and ranking tasks."
INTRODUCTION,0.017543859649122806,"Complex deep networks can easily learn to classify a large fraction of examples correctly as
training progresses. These networks have two characteristics: (a) they have one or more contiguous
homogeneous1 layers at the end forming the end homogeneous sub-network, and, (b) they are trained
with exponential-type loss functions like logistic loss and cross entropy that asymptotically attain
their least value of zero when the network score goes to inﬁnity. Let us collectively refer to this
network score as logit. After the network has learned to correctly classify a large fraction of training
examples, the weights of the end homogeneous layers and the logits grow to make the training loss
(and hence its gradient) very small. This, seen in optimizers when used with no (or mild) weight
decay/`2 regularization, leads to loss ﬂattening (loss and gradient taking very small values) (see
Appendix A). This further leads to loss of adaptivity of the network (Szegedy et al., 2015), causing
training to stall in regions of sub-optimal generalization (see §2)."
INTRODUCTION,0.021929824561403508,"1A layer is homogeneous if the activation function of each unit of the layer satisﬁes φ(↵x) = ↵φ(x). Linear
and ReLU are examples of such activation functions."
INTRODUCTION,0.02631578947368421,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03070175438596491,"Figure 1 shows the generalization performance (Test HR@10) of Adam without weight decay (green
dotted line) applied to a fully homogeneous network on a classiﬁcation task. After about 115 epochs,
Margin p50 (median margin over the training examples; blue dotted line) becomes large and the
training gets stuck in the basin of a sub-optimally generalizing minimum. The minimum is also
overﬁtting due to the tussle between good and noisy examples."
INTRODUCTION,0.03508771929824561,"Figure 1: Adam with (continuous lines) and without (dotted
lines) logit attenuation on MovieLens classiﬁcation. Adam: after
115 epochs (vertical dotted black line), loss ﬂattening sets in,
Test HR@10 is sub-optimal, with overﬁtting. Adam with logits
attenuation: The introduction of ↵factor after the 115th epoch
reduces Margin p50 but keeps Test HR@10 the same. Though
Test HR@10 then drops initially, with further constrained weight
training Adam with logit attenuation eventually achieves a higher
Test HR@10 than vanilla Adam."
INTRODUCTION,0.039473684210526314,"Optimizer
Batch Size
256
16k"
INTRODUCTION,0.043859649122807015,"SGD
76.00 (0.04)
74.48 (0.06)"
INTRODUCTION,0.04824561403508772,"Adam
71.16 (0.05)
70.60 (0.02)
Adam-L
76.18 (0.03)
76.07 (0.08)"
INTRODUCTION,0.05263157894736842,"LAMB+
74.30 (0.01)
73.43 (0.03)
LAMB-L
76.48 (0.05)
75.93 (0.02)"
INTRODUCTION,0.05701754385964912,"Table 1:
ImageNet validation accuracy.
Comparison of SGD, Adam and LAMB (all
with WD) and LAWN variants (*-L) of Adam
and LAMB+ a on the ImageNet validation
set. LAWN enables Adam to work on image
classiﬁcation tasks with very little drop in
performance at large batch sizes.
Current
optimizers have a much steeper drop-off in
performance as batch size increases. Standard
error is mentioned in parentheses."
INTRODUCTION,0.06140350877192982,"aLAMB+ is a modiﬁcation of the LAMB
algorithm.
More details can be found in
Appendix B.3.3."
INTRODUCTION,0.06578947368421052,"Adaptive gradient algorithms like ADAM are particularly severely affected by loss ﬂattening. These
algorithms are based on tracking the local loss behavior at each individual weight level and hence
they achieve fast convergence. However, in domains such as image classiﬁcation, their generalization
performance quickly plateaus to a value that is much worse than the performance ﬁnally achieved by
SGD. This gap is usually attributed to the inability of adaptive gradient algorithms to escape the basin
of a sharp minimum that has large curvature and possibly poor generalization. While weight decay
helps improve adaptive optimizers, it is still not sufﬁcient to close the gap with SGD. The training
method proposed in this paper closes the gap."
INTRODUCTION,0.07017543859649122,"To motivate the method, let us return to the experiment of Figure 1. Consider that training has reached
a point where logits are starting to become large. Attenuation of the logit values can be done using
the following two ideas: (a) multiply the logits by a factor, 0 < ↵< 1 and use this factor for the rest
of the training; (b) constrain the norms of layer weights for the rest of the training. Speciﬁcally, in the
experiment of Figure 1, after epoch 115 we shrink the logits to one-ﬁfth, and then continue training
while keeping the weight norms of each layer ﬁxed. This leads to superior generalization with no
overﬁtting (see the continuous lines in the ﬁgure). It turns out that idea (a) alone is not sufﬁcient since
further training will increase the logits again to reduce the training loss; so, idea (b) is very important.
Also, in order to control the logit magnitudes, instead of idea (a) (waiting till 115 epochs and then
reducing the logits by a factor ↵= 0.2) we could simply start using idea (b) without applying an ↵
factor, at an earlier point, say, after just 5 epochs."
INTRODUCTION,0.07456140350877193,"This lays the base for our method, Logit Attenuating Weight Normalization (LAWN). It can be used
with any gradient-based base optimizer, though our aim is mainly to improve adaptive optimizers.
LAWN begins with normal, unconstrained training in the initial phase and then constrains the weight
norms of the layers for the rest of the training. Weight decay is not used anywhere. The training on
the constrained norm surfaces is done by employing projected gradients instead of regular gradients."
INTRODUCTION,0.07894736842105263,"The LAWN variants of Adam and LAMB achieve impressive performance across multiple network
architectures and tasks. At large batch sizes, most optimizers get caught in sub-optimal regions due
to lowered stochasticity which is worsened by increased logits. LAWN’s attenuation of the logits"
INTRODUCTION,0.08333333333333333,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.08771929824561403,"helps avoid this worsening. Due to this, the LAWN variants of Adam and LAMB work signiﬁcantly
better than their base versions at large batch sizes. Table 1, which compares the generalization
performance of SGD, Adam and LAMB (all with weight decay) against the LAWN variants of Adam
and LAMB on the popular Imagenet dataset, powerfully showcases the two observations on LAWN
mentioned above. In §4 we show that these LAWN variants give much better generalization than
weight decay on several architectures for image classiﬁcation (CIFAR, ImageNet) and recommender
systems (MovieLens, Pinterest). We end this section with a discussion of related work."
INTRODUCTION,0.09210526315789473,"Related work: The issue of loss ﬂattening and the resulting loss of adaptivity is highlighted
in (Szegedy et al., 2015). Several well-known techniques are used to mitigate this issue. `2
regularization is often used in conjunction with optimizers like SGD and adaptive optimizers
(Kingma & Ba, 2017; Zeiler, 2012; Duchi et al., 2011), among others. Recently, decoupled weight
decay (Loshchilov & Hutter, 2019; Hanson & Pratt, 1988) has become popular to reign-in network
weights and prevent overconﬁdence on training samples. Other techniques are: (a) label smoothing
regularization (Szegedy et al., 2015), which makes the model less conﬁdent about predictions by
changing the ground-truth label distribution; and (b) ﬂooding (Ishida et al., 2020) which tries to keep
the aggregate training loss to be around a speciﬁed small value. From a theory perspective, weight
norm bounding has been shown to be useful for improving generalization (Neyshabur et al., 2015;
Bartlett et al., 2017). Salimans and Kingma (Salimans & Kingma, 2016) use weight normalization as
a transformation; but, by also keeping the scale component, they end up allowing logits to grow large.
Hoffer et al (Hoffer et al., 2018) discuss keeping the norms of the parameters ﬁxed, but the method
would require a LAWN-like approach to work effectively."
INTRODUCTION,0.09649122807017543,"It has been observed that small batch sizes yield better generalization performance (Keskar et al.,
2016) due to the noise of mini-batch gradients and the large learning rate. The noise diminishes with
increasing batch sizes. Large batch sizes are useful for speeding up the training process by leveraging
parallel GPUs. Poor generalization for large batch sizes is attributed to them stalling around ""sharp""
minimizers (Keskar et al., 2016). Goyal et al (Goyal et al., 2018) scale ImageNet training to batch
sizes of 8k without loss in generalization performance, by carefully tuning parameters like learning
rate and batch normalization. Other recent efforts to train large-batch models include (Hoffer et al.,
2017; You et al., 2017; 2019; Shallue et al., 2018; You et al., 2020). It is important to note that for a
majority of the cited works, large batch gains do not necessarily hold across tasks or datasets."
THE NEED FOR LAWN,0.10087719298245613,"2
THE NEED FOR LAWN"
THE NEED FOR LAWN,0.10526315789473684,"In this section we motivate LAWN by describing the problem settings in which normal training
struggles and in which LAWN could improve generalization performance. We deﬁne these problem
settings in §2.1, then describe what we mean by loss of adaptivity in §2.2. We review the strengths
and weaknesses of existing methods for avoiding loss of adaptivity in §2.3. With this context, we
elaborate on the LAWN method in §3."
THE NEED FOR LAWN,0.10964912280701754,"Notations. For the rest of the paper we will use multi-class setting as the running example. The
following notations will be used: n is the number of weight variables; m is the number of training
examples; i is the index used to denote the index of a training example; yi is the target class of the
i-th training example; k is the index used to denote a class; nc is the number of classes; w is the
weight vector of the deep net; pk(w) is the class k probability assigned by the deep net with weight
vector w. For an optimizer, ⌘denotes the learning rate and B is the batch size."
THE NEED FOR LAWN,0.11403508771929824,"2.1
WHEN DOES LAWN WORK?"
THE NEED FOR LAWN,0.11842105263157894,Let us describe the problem settings in which we expect generalization improvement using LAWN.
THE NEED FOR LAWN,0.12280701754385964,"Problem type. We are mainly interested in classiﬁcation and ranking problems which form a score
for the target. In binary classiﬁcation this score is the logit score of the target class; in a multiclass
problem, this score is the difference between the target class score and the maximum score over
the remaining classes. For ease of presentation we will simply refer to such scores as logit. We
consider a loss function that attains its least value (usually zero) asymptotically as logit goes to
inﬁnity. The logistic loss for binary classiﬁcation and the cross entropy loss based on softmax for
multi-class classiﬁcation are important cases. Problems like regression that use a loss function such
as the squared loss (which attains its minimum at a ﬁnite value) may not have much beneﬁt using
LAWN. In a multi-task setting, when the total loss is an additive mix of several individual task losses,
LAWN can be useful even if just one of them is suited for LAWN."
THE NEED FOR LAWN,0.12719298245614036,Under review as a conference paper at ICLR 2022
THE NEED FOR LAWN,0.13157894736842105,"Network Complexity. This refers to the complexity of the network in relation to the number of
training examples. Most deep networks used in applications are over-parametrized. Roughly, we will
take over-parametrized to mean that the network is so powerful that training easily locates a w that
classiﬁes most examples correctly, i.e., the target class has the highest score among all classes. Deep
nets usually have one or more fully connected homogeneous layers (usually with ReLU units) at the
end. If the deep net is powerful enough to correctly classify most examples correctly, then by making
the weights large it is possible to push the loss of most training examples (and thereby the train loss)
to very small values."
THE NEED FOR LAWN,0.13596491228070176,"Generalization Metrics. Our focus is on improving metrics that are based on score ordering as
opposed to the actual values of scores. More precisely, we are interested in test set metrics such
as classiﬁcation error, AUC, NDCG etc. as opposed to logistic loss, cross entropy loss, probability
calibration error etc. Deep networks are known to be poor with respect to the latter metrics (Guo et al.,
2017) but which can be improved in the post-training stage; the adaptation of LAWN to improving
such metrics will be taken up in a follow-up work.2"
ISSUE OF LOSS OF ADAPTIVITY,0.14035087719298245,"2.2
ISSUE OF LOSS OF ADAPTIVITY
Consider the problem setting deﬁned in §2.1 and the minimization of the normally used unregularized
training loss, L(w) = −1 m Pm"
ISSUE OF LOSS OF ADAPTIVITY,0.14473684210526316,"i=1 log pyi(w). With the network being sufﬁciently powerful, training
causes the losses of a large fraction of the training examples to become very small. During training,
this happens due to weights becoming large, the logit becoming large, and pyi ! 1 for those examples.
Due to these, each of the following become very small: the gradient of most example-wise loss terms;
⌃, the covariance of the noise associated with minibatch gradient; and, H, the Hessian of the training
loss. We will refer to this collective happening as loss ﬂattening."
ISSUE OF LOSS OF ADAPTIVITY,0.14912280701754385,"It has been established via theoretical and empirical arguments that the powerful generalization
ability of a deep net comes from its ability to escape regions of attraction of “sharper"" minima3 with
sub-optimal generalization performance and go to better solutions. Appendix C gives an idea of the
escape mechanism for the SGD method using a simplistic analysis given by Wu et al (Wu et al.),
which we use just for guidance and motivation. It is worth recalling from there, the following rough
guiding condition for SGD to escape from a poor solution: λmax{(I −⌘H)2 + ⌘2(m−B)"
ISSUE OF LOSS OF ADAPTIVITY,0.15350877192982457,"B(m−1) ⌃} > 1,
where λmax(A) denotes the largest eigenvalue of A. If training is at a sub-optimal generalization
point, loss ﬂattening occurs (which means that H and ⌃become small), then the escape condition is
difﬁcult to satisfy and hence it becomes difﬁcult for the network to escape from this solution and
then train further to go to a better solution. A carefully increased learning rate schedule to cause the
escape followed by the use of normal learning rates to go to a better solution can make this happen,
but no such automatic sophisticated learning rate adjustment mechanism has been devised yet. We
will refer to this inability of the network to escape out of a sub-optimal solution due to loss ﬂattening
as loss of adaptivity (also see §7 in (Szegedy et al., 2015))."
CURRENT METHODS FOR DEALING WITH LOSS OF ADAPTIVITY,0.15789473684210525,"2.3
CURRENT METHODS FOR DEALING WITH LOSS OF ADAPTIVITY
Several methods have been suggested in the literature to handle the issue of loss of adaptivity.
We brieﬂy describe three key ones: label smoothing regularization (LSR) (Szegedy et al., 2015),
ﬂooding (Ishida et al., 2020), and `2 regularization/weight decay (Loshchilov & Hutter, 2019). LSR
modiﬁes L(w) via making the target class less conﬁdent by ﬁxing its probability as (1 −✏LSR) and
reassigning ✏LSR to the remaining classes. This makes the loss attain its minimum at ﬁnite logit
values. Flooding modiﬁes the loss as LF looding(w) = |L −✏F looding| so that the training process
is forced to move around the hypersurface deﬁned by L −✏F looding = 0. `2 regularization is a
traditional method that modiﬁes the loss as L`2 = L(w) + λ"
CURRENT METHODS FOR DEALING WITH LOSS OF ADAPTIVITY,0.16228070175438597,"2 kwk2. (Decoupled) Weight decay, as
used in the recent deep net literature decouples the term λw from the gradient and instead includes
the additive term, −λw at the weight update step. In the next section (§3) we will return to discuss
these methods in relation to LAWN."
THE LAWN METHOD,0.16666666666666666,"3
THE LAWN METHOD"
THE LAWN METHOD,0.17105263157894737,"As we mentioned in §2, we consider deep nets that have one or more homogeneous layers (usually
with ReLU units) at the end (top); let fhsn refer to this ﬁnal homogeneous sub net. Consider a weight"
THE LAWN METHOD,0.17543859649122806,"2However, as we show in §4.6, LAWN does much better calibration than the weight decay method.
3Originating from the work of Keskar et al (Keskar et al., 2016), it has been observed in the literature that
minima at which the classiﬁcation function has a sharper behavior has poorer generalization."
THE LAWN METHOD,0.17982456140350878,Under review as a conference paper at ICLR 2022
THE LAWN METHOD,0.18421052631578946,"vector ¯w and corresponding fhsn weights ¯wfhsn. When we extend ¯wfhsn along a radial direction,
i.e., wfhsn = ↵¯wfhsn, ↵2 R+, while keeping the weights of the network outside fhsn unchanged,
the classiﬁcation error on a set of examples (e.g., the training error computed on a training set or the
generalization error computed on a test set) remains unaffected whereas classiﬁcation loss varies a lot
with ↵. In particular, if ¯w classiﬁes a set of examples strictly correctly, then, as ↵goes from 0 to 1,
the average loss on these examples goes from log(nc) to zero asymptotically. When training without
any/sufﬁcient weight decay, weights do get large and training loss does become very small. In the
previous section (§2) we saw how, when training loss becomes very small, it causes loss ﬂattening,
which leads to a loss of adaptivity of the network. Therefore, it makes sense to suitably contain the
size of the weights. Theory (Neyshabur et al., 2015; Bartlett et al., 2017) suggests that, for improving
generalization, it is a good idea to bound the weights of the entire network. Given that most layers are
usually homogeneous4 (e.g., units with ReLU activations), it is appropriate to use layer-wise weight
normalization. The essential spirit of LAWN is along this idea."
THE LAWN METHOD,0.18859649122807018,"Consider constrained training via layer-wise weight normalization: kw`k = c` 8` where w` is the
weight vector accociated with layer `, and the c` are some constants. It is useful to understand how
the loss contours behave as the c` go from small to big values. When the c` are large, we know that
loss ﬂattening will happen. When the c` are small, the distinction between the loss values of well
classiﬁed examples and poorly classiﬁed examples diminishes and so, optimizers will ﬁnd it harder
to traverse the contours and go to the right place of best generalization. To choose the right c` values,
in LAWN we take a simple and natural approach. We initialize the network with weights having
small magnitude using a standard weight initialization method and start a given optimizer in its free
(unconstrained, without any weight decay) form. At a suitable point in that training process, with
weights at some ¯w, we switch to constrained training deﬁned by setting"
THE LAWN METHOD,0.19298245614035087,"kw`k = c` where c` = k ¯w`k 8`
(1)"
THE LAWN METHOD,0.19736842105263158,"The c` are ﬁxed for the rest of the LAWN training. Constrained training corresponds to solving the
optimization problem,"
THE LAWN METHOD,0.20175438596491227,"min L(w) s.t. kw`k = c` 8`
(2)"
THE LAWN METHOD,0.20614035087719298,using a modiﬁed version of any given gradient-based optimizer.
THE LAWN METHOD,0.21052631578947367,"In LAWN, we switch from free training to constrained training after some Efree epochs, and tune
Efree as a hyperparameter using a coarse grid. Thus, when compared to regularization using weight
decay, LAWN (a) does not use weight decay anywhere, and (b) it uses Efree as a hyperparameter
instead."
THE LAWN METHOD,0.2149122807017544,"In the future, we plan to try the following automatic method for choosing Efree. Track the logits of
training examples (done simply and efﬁciently by tracking them on the minibatches used) and switch
to constrained training when their median value starts reaching high values indicating the onset of
loss ﬂattening."
THE LAWN METHOD,0.21929824561403508,Algorithm 1 Adam-LAWN Constrained Phase
THE LAWN METHOD,0.2236842105263158,"1: for t in 1...T do
2:
Draw batch St from the training set
3:
gt = ComputeGrad(wt−1, St)
4:
Project each g`"
THE LAWN METHOD,0.22807017543859648,t to {d` : (w`)T d` = 0} to get g`
THE LAWN METHOD,0.2324561403508772,"pt 8`
5:
mt = β1mt−1 + (1 −β1)gpt
6:
vt = β2vt−1 + (1 −β2)g2"
THE LAWN METHOD,0.23684210526315788,"pt
7:
ˆmt =
mt
1−βt"
THE LAWN METHOD,0.2412280701754386,"1 , ˆvt =
vt
1−βt"
THE LAWN METHOD,0.24561403508771928,"2
8:
Compute rt =
ˆmt
pˆvt+✏
9:
Project each r`"
THE LAWN METHOD,0.25,t to {d` : (w`)T d` = 0} to get ˆr`
THE LAWN METHOD,0.2543859649122807,"t 8`
10:
wt = wt−1 −⌘tˆrt
11:
Rescale wt to satisfy constraints on kw`k 8`
12: end for"
THE LAWN METHOD,0.25877192982456143,"Hoffer et al (Hoffer et al., 2018)
give a bounded weight normalization
method which also constrains the
weight
norms
and
uses
simple
heuristics for setting the c`. However,
it does not use the idea of combining
free and constrained training, and
it does not set up and demonstrate
the use of constrained training as
a powerful method for use with
adaptive optimizers for improving
the performance by overcoming loss
ﬂattening and loss of adaptivity. In
§4 we conduct experiments on image
classiﬁcation
and
recommender
datasets and show that Hoffer et al’s method is quite inferior to LAWN."
THE LAWN METHOD,0.2631578947368421,"4In fully homogeneous nets, layer-wise weight normalization is a way of removing some redundancies (Dinh
et al., 2017); layer-wise weight norms also connect well with implicit bias properties (Lyu & Li, 2020)."
THE LAWN METHOD,0.2675438596491228,Under review as a conference paper at ICLR 2022
THE LAWN METHOD,0.2719298245614035,"LAWN Implementation. There are two ways of implementing the constrained phase of LAWN. The
ﬁrst method is to deﬁne an unconstrained vector v` and set w` = c` v`/||v`|| and simply apply the
optimizer to v`. This is the implementation suggested by Salimans and Kingma (Salimans & Kingma,
2016); see equation (2) there. (Note, however, that Salimans and Kingma (Salimans & Kingma, 2016)
keep the radial component by including the scale parameter, g.) The downside with this method is
that it puts load on the computational graph, and automatic differentiation through the normalizing
transformation increases the computational cost (Huang et al., 2020)."
THE LAWN METHOD,0.27631578947368424,"The second method is to have the optimizer directly deal with the constraints. At a given weight
vector, w, let g = rwL(w). For updating w`, the projected gradient deﬁned by g`"
THE LAWN METHOD,0.2807017543859649,p = g` −(w`)T g`
THE LAWN METHOD,0.2850877192982456,"kw`k2 w` 8`
(3)"
THE LAWN METHOD,0.2894736842105263,"naturally plays the role of the gradient for decreasing the loss on the manifold deﬁned by kw`k = c`
and so it is used instead of the gradient in all optimizer related updates. In Appendix D we use
gradient ﬂow to establish this. We use this method in our implementation of the constrained phase of
LAWN. The implementation of the constrained phase of Adam-LAWN is given in Algorithm 1."
THE LAWN METHOD,0.29385964912280704,"Let us now describe the LAWN method as a complete algorithm. The free and constrained training of
LAWN can be done with any given optimizer. LAWN does a total of Etotal epochs; Etotal is ﬁxed
for a given dataset. After Efree epochs of free training, with ¯w denoting the weights reached, it sets
c` = k ¯wk` 8` and switches to do constrained training (solve (2)) for the remaining (Etotal −Efree)
epochs. Learning rate schedules are important for deep networks to attain good performance and
LAWN employs the standard linear warmup and decay schedule (Loshchilov & Hutter, 2019; Liu
et al., 2020; Loshchilov & Hutter, 2016). This schedule has two hyperparameters, ⌘peak, the peak
learning rate, and Ewarmup, the number of warmup epochs. Efree is the additional hyperparameter;
as already mentioned, this hyperparameter replaces the weight decay parameter."
THE LAWN METHOD,0.2982456140350877,"LAWN and large batch sizes. For a ﬁxed epoch budget, larger batch sizes require a smaller number
of steps; combined with distributed computation this helps speed up training. However, since
stochasticity of updates reduces with large batch size, the mechanism of escape from sub-optimal
solutions gets affected. Thus, one usually sees a reduction in generalization performance as batch
size is increased (Shallue et al., 2018). Loss ﬂattening makes this issue worse by affecting adaptivity.
LAWN, by helping overcome this issue, leads to a more graceful degradation of performance as a
function of batch size. We will empirically demonstrate this in §4. The degradation becomes far less
(even zero) when the the total number of steps is allowed to decently increase with batch size."
THE LAWN METHOD,0.3026315789473684,"LAWN and weight adaptivity. Research in the last ﬁve years is clearly showing that improving
weight adaptivity is the key to escaping inferior weights and reaching weights with superior
generalization performance. It was believed that the noise associated with stochastic gradient
is the only way to ensure such adaptivity, leading to the promotion of smaller batch sizes and larger
learning rates. But other ways of improving adaptivity are being suggested. For example, injection of
suitable forms of artiﬁcial noise has been shown to improve generalization (Wu et al., 2020; Wen
et al., 2020). It is even being suggested that adaptivity can be improved without any stochasticity
and with just suitable deterministic regularization (Geiping et al., 2021). More interesting research is
expected on this important topic of weight adaptivity. In this line of research and methods, LAWN
can be thought of as an orthogonal technique of improving adaptivity by suitably constraining the
norms of the weights and avoiding loss ﬂattening. While weight decay also helps in a similar way,
we will show in §4 that LAWN is much superior."
EXPERIMENTS,0.30701754385964913,"4
EXPERIMENTS"
EXPERIMENTS,0.31140350877192985,"Our baselines include SGD, Adam (Kingma & Ba, 2017) and LAMB (You et al., 2020). We add
weight decay to all 3 baseline algorithms, and additionally add momentum to SGD. To evaluate
LAWN, we consider LAWN-based variants of the adaptive optimizers (Adam and LAMB). SGD
has demonstrated strong performance for computer vision tasks (Ren et al., 2015; Goyal et al.,
2018), whereas adaptive methods like Adam perform well on other domains (eg. recommender
systems, text classiﬁcation). To demonstrate the efﬁcacy of LAWN across a wide variety of tasks, we
conducted experiments on the CIFAR (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009;
Krizhevsky et al., 2012) datasets for image classiﬁcation, and the MovieLens (Harper & Konstan,
2015) and Pinterest (Geng et al., 2015) datasets for item recommendation. All model training code"
EXPERIMENTS,0.3157894736842105,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.3201754385964912,"was implemented using the PyTorch library (Paszke et al., 2019) and experiments were conducted on
machines with NVIDIA V100 GPUs. For each experiment, we report average test metric over 3 runs."
EXPERIMENTS,0.32456140350877194,"Hyperparameters: Regular versions of SGD, Adam, and LAMB use weight decay. The LAWN
versions do not use weight decay; instead, they use Efree. For a given dataset, the total number of
epochs, Etotal was ﬁxed. Learning rate schedule (with warmup of learning rate from 0 to ⌘peak in
Ewarmup epochs followed by decay to zero in the remaining epochs) has proved beneﬁcial. The above
mentioned hyperparameters were all tuned to get the best generalization performance. For ⌘peak we
used equally spaced values in logarithmic scale suited for each (optimizer, dataset) combination. The
value of Etotal, the range of values for weight decay, Efree and Ewarmup, and additional details
used for individual datasets are given in Appendix B. Apart from these, SGD’s momentum value
was ﬁxed at 0.9, unless speciﬁed otherwise in Appendix B. For Adam, LAMB, Adam-LAWN and
LAMB-LAWN, we ﬁxed β1 = 0.9 and β2 = 0.999. For Adam, we used ✏= 10−8 and the rest of
the adaptive optimizers use 10−6. We did not tune ✏, β1 and β2, which could have led to further
improvements."
EXPERIMENTS,0.32894736842105265,"Details about datasets, pre-processing and network architectures for all experiments can be found in
Appendix B."
EXPERIMENTS,0.3333333333333333,"4.1
LAWN VS. OTHER METHODS FOR CONTROLLING LOSS OF ADAPTIVITY"
EXPERIMENTS,0.33771929824561403,"Method
MovieLens-1M
CIFAR-10
CIFAR-100
BS = 10k
BS = 100k
BS = 4k
BS = 10k
BS = 4k
BS = 10k
LSR
68.66
67.34
93.09
92.73
69.92
69.24
WD
70.12
69.28
92.93
92.63
68.91
68.61
Hoffer
44.54
45.52
92.99
91.66
70.66
69.35
LAWN
70.41
70.77
93.74
93.84
73.13
72.97"
EXPERIMENTS,0.34210526315789475,"Table 2: Comparison of test performance of LAWN with other methods for controlling loss of adaptivity on
Movielens-1M, CIFAR-10 and CIFAR-100 datasets. The base optimizer used is Adam. Two different batch sizes,
BS are tried for each dataset. For LSR, the smoothing parameter was ﬁxed at 0.05. LAWN comprehensively
outperforms other methods, including weight decay (WD)."
EXPERIMENTS,0.34649122807017546,"We ﬁrst compared LAWN to three key methods for controlling loss of adaptivity, discussed in §2.3.
The comparison is done on the three datasets, Movielens-1M, CIFAR-10 and CIFAR-100. For each
dataset we tried two values of batch size. Table 2 gives the results. LAWN clearly outperforms all the
other methods. The second overall best method is weight decay and hence it is used as the baseline
for all remaining experiments of this section. The weight normalization technique suggested by
Hoffer et al (Hoffer et al., 2018) does very badly on Movielens-1M; on CIFAR-10 and CIFAR-100,
though it gives a decent performance, it lags behind LAWN a lot. Clearly, this method requires a
modiﬁcation along the lines of LAWN in order for it do well."
EXPERIMENTS,0.3508771929824561,"4.2
IMAGE CLASSIFICATION FOR CIFAR-10 AND CIFAR-100
For both CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009), we used the VGG-19 CNN
network (Simonyan & Zisserman, 2014) with 1 fully connected ﬁnal layer. Our ImageNet experiments
use a ResNet-based (He et al., 2015) architecture. All experiments were run with a 300 epoch budget.
As seen in Table 3, LAWN variants either match or outperform the base variants across batch sizes.
Adam-LAWN is particularly impressive. This is in stark contrast to earlier held beliefs that adaptive
optimizers cannot match SGD’s generalization performance for image classiﬁcation tasks (Wilson
et al., 2017)."
EXPERIMENTS,0.35526315789473684,"Effect of batch size. LAWN variants cause more graceful degradation of performance with batch
size, as compared to base variants. Adam-LAWN causes almost no degradation in generalization
performance even at batch size 10k (see Figures 2(a), 2(b))."
EXPERIMENTS,0.35964912280701755,"Effect of Efree. We observed that switching early to LAWN mode (i.e. ﬁxing Efree to less than 10
epochs) usually works well for generalization. See Appendix B for details. This is consistent with
our hypothesis that constrained training should kick in before loss ﬂattening sets in."
RECOMMENDATION SYSTEMS,0.36403508771929827,"4.3
RECOMMENDATION SYSTEMS
We conducted experiments on the MovieLens-100k, MovieLens-1M and Pinterest datasets using
the popular neural collaborative ﬁltering (NCF) technique (He et al., 2017). The datasets contain
ratings provided to various items by users. The model is a 3-layer MLP, the input being user and
item embeddings. We use the hit ratio@10 metric (expressed as %), where we reward the model for"
RECOMMENDATION SYSTEMS,0.3684210526315789,Under review as a conference paper at ICLR 2022
RECOMMENDATION SYSTEMS,0.37280701754385964,"103
104 90 92 94"
RECOMMENDATION SYSTEMS,0.37719298245614036,Batch size
RECOMMENDATION SYSTEMS,0.3815789473684211,Test Accuracy
RECOMMENDATION SYSTEMS,0.38596491228070173,"Adam
Adam-LAWN"
RECOMMENDATION SYSTEMS,0.39035087719298245,(a) CIFAR-10
RECOMMENDATION SYSTEMS,0.39473684210526316,"103
104 68 70 72 74"
RECOMMENDATION SYSTEMS,0.3991228070175439,Batch size
RECOMMENDATION SYSTEMS,0.40350877192982454,Test Accuracy
RECOMMENDATION SYSTEMS,0.40789473684210525,"Adam
Adam-LAWN"
RECOMMENDATION SYSTEMS,0.41228070175438597,(b) CIFAR-100
RECOMMENDATION SYSTEMS,0.4166666666666667,"103
104
105
68 69 70 71"
RECOMMENDATION SYSTEMS,0.42105263157894735,Batch size
RECOMMENDATION SYSTEMS,0.42543859649122806,Test HR@10
RECOMMENDATION SYSTEMS,0.4298245614035088,"Adam
Adam-LAWN"
RECOMMENDATION SYSTEMS,0.4342105263157895,(c) MovieLens-1M
RECOMMENDATION SYSTEMS,0.43859649122807015,"103
104
68 70 72 74 76"
RECOMMENDATION SYSTEMS,0.44298245614035087,Batch size
RECOMMENDATION SYSTEMS,0.4473684210526316,Test Accuracy
RECOMMENDATION SYSTEMS,0.4517543859649123,"Adam
Adam-LAWN"
RECOMMENDATION SYSTEMS,0.45614035087719296,"(d) ImageNet
Figure 2: Adam-LAWN vs. Adam (weight decay comprehensively tuned) for a variety of datasets. Adam-
LAWN causes little to no drop in generalization performance with increasing batch size."
RECOMMENDATION SYSTEMS,0.4605263157894737,"ranking a test item in the top 10 of 100 randomly sampled items that a user has not interacted with
in the past. We trained for 300 epochs for the two smallest batch sizes, and 500 epochs for the two
biggest batch sizes for each dataset. Details about the datasets, pre-processing, model and evaluation
can be found in Appendix B. A summary of the performance of the aforementioned optimizers on
all 3 datasets can be found in Table 4. LAWN-based optimizers consistently outperform their base
variants. SGD failed to generalize well at large batch sizes and this requires further investigation."
RECOMMENDATION SYSTEMS,0.4649122807017544,"Method
MovieLens-100k
MovieLens-1M
Pinterest
1k
10k
100k
400k
1k
10k
100k
1M
1k
10k
100k
1M"
RECOMMENDATION SYSTEMS,0.4692982456140351,"SGD
66.33
65.58
Fail
Fail
70.91
69.31
Fail
Fail
86.62
85.57
Fail
Fail"
RECOMMENDATION SYSTEMS,0.47368421052631576,"Adam
66.01
66.03
63.20
63.98
69.87
70.12
69.28
68.99
87.27
85.97
85.81
85.30
Adam-L
66.81
66.91
66.24
66.14
70.80
70.41
70.77
70.66
86.85
86.61
86.04
86.06"
RECOMMENDATION SYSTEMS,0.4780701754385965,"LAMB
65.45
65.34
64.23
62.57
69.91
69.77
69.44
68.95
86.63
85.91
85.80
85.65
LAMB-L
66.56
66.54
66.52
66.14
70.86
70.86
70.68
70.34
86.83
86.25
85.99
86.07"
RECOMMENDATION SYSTEMS,0.4824561403508772,"Table 4: Test HR@10 on MovieLens and Pinterest recommendations. Standard error is in the range [0.15, 0.25];
details are in Appendix B. Highlighted values indicate the better performing method between x and x-L."
RECOMMENDATION SYSTEMS,0.4868421052631579,"Method
CIFAR-10
CIFAR-100
256
4k
10k
256
4k
10k"
RECOMMENDATION SYSTEMS,0.49122807017543857,"SGD
93.99
93.48
92.99
73.49
71.68
71.07"
RECOMMENDATION SYSTEMS,0.4956140350877193,"Adam
93.48
92.93
92.63
70.84
68.91
68.61
Adam-L
93.91
93.74
93.84
72.99
73.12
72.97"
RECOMMENDATION SYSTEMS,0.5,"LAMB
93.76
93.27
92.91
71.29
69.39
67.76
LAMB-L
93.67
93.22
92.92
71.25
69.68
69.16"
RECOMMENDATION SYSTEMS,0.5043859649122807,"Table 3: Test accuracy on CIFAR-10 and CIFAR-100.
Standard error is in the range [0.1, 0.45]. Details are
in Appendix B. Highlighted values indicate the better
performing method between x and x-L."
RECOMMENDATION SYSTEMS,0.5087719298245614,"Weight decay vs. LAWN. Weight decay was
used and tuned for all the base optimizers since
it arrests the uncontrolled growth of network
weights, helping avoid of loss of adaptivity. The
LAWN variants do not use weight decay but still
outperform the base variants."
RECOMMENDATION SYSTEMS,0.5131578947368421,"Effect of batch size.
LAWN variants of
Adam and LAMB scale to very large batch
sizes (1 million for MovieLens-1M, 400k for
MovieLens-100k) without any appreciable loss
in accuracy. SGD could only scale to batch size
10k. Adam-LAWN’s strong scalability with batch size is consistent with results obtained from the"
RECOMMENDATION SYSTEMS,0.5175438596491229,CIFAR experiments (also see Figure 2(c)).
RECOMMENDATION SYSTEMS,0.5219298245614035,"Effect of Efree. Similar to the results of the CIFAR experiments, ﬁxing Efree to a small value works
well for LAWN. Details are in Appendix B."
IMAGE CLASSIFICATION FOR IMAGENET,0.5263157894736842,"4.4
IMAGE CLASSIFICATION FOR IMAGENET
As compared to CIFAR, the ImageNet classiﬁcation problem (Krizhevsky et al., 2012) is more
representative of real world classiﬁcation problems. We used a variant of the popular ResNet50 (He
et al., 2015) model as the classiﬁer. We considered a small (256) and a large (16k) batch size for this
experiment, and ﬁxed training budget to be 90 epochs."
IMAGE CLASSIFICATION FOR IMAGENET,0.5307017543859649,"Results for batch size 256. Overall results can be found in Table 1 (see §1). SGD, used in
conjunction with momentum and weight decay, has long been the optimizer of choice for image
classiﬁcation. Adam is well known to perform worse than SGD for image classiﬁcation tasks (Wilson
et al., 2017). For our experiment, we tuned the learning rate and could only get an accuracy of 71.16%.
In comparison, Adam-LAWN achieves an accuracy of more than 76%, marginally surpassing the
performance of SGD."
IMAGE CLASSIFICATION FOR IMAGENET,0.5350877192982456,"We found it difﬁcult to reproduce ImageNet results using the LAMB algorithm. We made minor
modiﬁcations (details in Appendix B) to the original algorithm to make it more stable, and call"
IMAGE CLASSIFICATION FOR IMAGENET,0.5394736842105263,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION FOR IMAGENET,0.543859649122807,"the resultant algorithm LAMB+. LAMB-LAWN (the LAWN version of the unmodiﬁed LAMB)
comprehensively outperforms LAMB+ for batch size 256 by achieving an accuracy close to 76.5%."
IMAGE CLASSIFICATION FOR IMAGENET,0.5482456140350878,"Results for batch size 16k. For the large batch size of 16k, we noticed that LAWN retains strong
generalization performance (also see Figure 2(d)). Both Adam-LAWN and LAMB-LAWN achieve
very high accuracy, with Adam-LAWN retaining its performance at such a large batch size by crossing
the 76% test accuracy mark. This is with only additonally tuning for the LAWN variants Efree and
Ewarmup."
IMAGE CLASSIFICATION FOR IMAGENET,0.5526315789473685,"Remark. Adam has traditionally performed worse than SGD at tasks like image classiﬁcation.
Recent work (Choi et al., 2020; Nado et al., 2021) has shown that Adam’s inner hyperparameters
(that include β1, β2, ✏) could be the reason for the inferior generalization. The above cited works use
sophisticated hyperparameter tuning algorithms over a relatively large search space (see Appendix D
of Choi et al. (2020)) and conclude that the optimal parameters vary a lot between datasets. While
these are important results to close the gap in our understanding of Adam, they do little to improve
the practical usability of Adam since it is prohibitively expensive to run the recommended number of
training runs required to ﬁnd the ideal hyperparameters."
LAWN WORKS WITH OTHER LOSS FUNCTIONS,0.5570175438596491,"4.5
LAWN WORKS WITH OTHER LOSS FUNCTIONS
To understand the effect of LAWN on loss functions other than cross entropy, we conducted
experiments with focal loss (Lin et al., 2017). Focal loss was proposed as a way of improving
performance when the classiﬁcation problem is highly imbalanced, and has also recently found use
for calibration of neural networks (Mukhoti et al., 2020). It is very different from cross-entropy, but
still suffers from the issue of loss ﬂattening. We conducted experiments using focal loss and Adam
on both item recommendation and image classiﬁcation. Results are given in Table 5. Adam-LAWN
outperformed regular Adam with weight decay on each one of the cases. This demonstrates the
strengths of LAWN across a variety of loss functions and reinforces LAWN’s efﬁcacy in improving
generalization performance."
LAWN WORKS WITH OTHER LOSS FUNCTIONS,0.5614035087719298,"Method
MovieLens-1M
CIFAR-10
CIFAR-100
FL0.5
FL2
FL5
CE
FL0.5
FL2
FL5
CE
FL0.5
FL2
FL5
CE"
LAWN WORKS WITH OTHER LOSS FUNCTIONS,0.5657894736842105,"Adam (BS1)
67.33
69.02
67.17
69.28
93.07
91.16
87.88
92.93
69.19
68.85
67.72
68.91
Adam-L (BS1)
71.24
69.68
69.24
70.77
93.89
92.22
89.14
93.74
73.05
73.29
72.89
73.12"
LAWN WORKS WITH OTHER LOSS FUNCTIONS,0.5701754385964912,"Adam (BS2)
66.42
67.40
66.62
68.99
92.51
89.79
86.82
92.63
67.94
67.28
66.11
68.61
Adam-L (BS2)
70.53
69.56
68.66
70.66
93.21
91.20
88.93
93.84
72.51
72.55
70.95
72.97"
LAWN WORKS WITH OTHER LOSS FUNCTIONS,0.5745614035087719,"Table 5: Adam vs. Adam-LAWN when used with focal loss (FL). We tried 3 different values (0.5, 2 and 5) for
γ (focal loss parameter) and also compared the results to cross entropy (CE) loss. BS1 and BS2 refer to batch
sizes. For MovieLens-1M, BS1 = 100k and BS2 = 1M. For the CIFAR datasets, BS1 = 4k and BS2 = 10k."
LAWN IMPROVES CALIBRATION,0.5789473684210527,"4.6
LAWN IMPROVES CALIBRATION"
LAWN IMPROVES CALIBRATION,0.5833333333333334,"Method
CIFAR-10
CIFAR-100"
LAWN IMPROVES CALIBRATION,0.5877192982456141,"Adam
0.132
0.292
Adam-L
0.048
0.188"
LAWN IMPROVES CALIBRATION,0.5921052631578947,"Table 6: ECE (lower is better) on image
classiﬁcation."
LAWN IMPROVES CALIBRATION,0.5964912280701754,"A well calibrated network is one in which the predicted class
probability is close to the observed probability of being correct.
Many applications require a well calibrated network. It is
well known that over-parameterized deep nets are prone to
over predicting probabilities (Guo et al., 2017). One of the
main reasons for this is that weights become large, logits
become large, and so the network is pushed to give out extreme
probability values. Weight decay helps Adam improve calibration over unregularized Adam. Here we
demonstrate that Adam-LAWN signiﬁcantly improves the calibration even over Adam with weight
decay. Estimated Calibration Error (ECE) is a standard metric for measuring calibration error (Guo
et al., 2017). For CIFAR-10 and CIFAR-100, with all hyperparameters tuned, Table 6 gives the ECE
values for Adam and Adam-LAWN. It is clear that Adam-LAWN gives much smaller ECE values
than Adam (with weight decay). This improved calibration is an important advantage of LAWN.
5
CONCLUSION"
LAWN IMPROVES CALIBRATION,0.6008771929824561,"LAWN as a simple and powerful method of modifying deep net training with a base optimizer to
improve weight adaptivity and lead to improved generalization. Switching from free to weight norm
constrained training at an appropriate point is a key element of the method. We study the performance
of the LAWN technique on a variety of tasks, optimizers and batch sizes, demonstrating its efﬁcacy.
Tremendous overall enhancement of Adam and the improvement of all base optimizers at large batch
sizes using LAWN are important highlights."
LAWN IMPROVES CALIBRATION,0.6052631578947368,Under review as a conference paper at ICLR 2022
REFERENCES,0.6096491228070176,REFERENCES
REFERENCES,0.6140350877192983,"Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for"
REFERENCES,0.618421052631579,"neural networks. CoRR, abs/1706.08498, 2017."
REFERENCES,0.6228070175438597,"Dami Choi, Christopher J. Shallue, Zachary Nado, Jaehoon Lee, Chris J. Maddison, and George E."
REFERENCES,0.6271929824561403,"Dahl. On Empirical Comparisons of Optimizers for Deep Learning, 2020."
REFERENCES,0.631578947368421,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale"
REFERENCES,0.6359649122807017,"hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. IEEE, 2009."
REFERENCES,0.6403508771929824,"Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for"
REFERENCES,0.6447368421052632,"deep nets. CoRR, abs/1703.04933, 2017."
REFERENCES,0.6491228070175439,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and"
REFERENCES,0.6535087719298246,"stochastic optimization. Journal of machine learning research, 12(7), 2011."
REFERENCES,0.6578947368421053,"Jonas Geiping, Micah Goldblum, Phillip E. Pope, Michael Moeller, and Tom Goldstein. Stochastic"
REFERENCES,0.6622807017543859,"training is not necessary for generalization, 2021."
REFERENCES,0.6666666666666666,"Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. Learning image and user features"
REFERENCES,0.6710526315789473,"for recommendation in social networks. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 4274–4282, 2015."
REFERENCES,0.6754385964912281,"Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,"
REFERENCES,0.6798245614035088,"Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training
imagenet in 1 hour, 2018."
REFERENCES,0.6842105263157895,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural"
REFERENCES,0.6885964912280702,"networks. CoRR, abs/1706.04599, 2017."
REFERENCES,0.6929824561403509,Stephen Hanson and Lorien Pratt. Comparing biases for minimal network construction with back-
REFERENCES,0.6973684210526315,"propagation. Advances in neural information processing systems, 1:177–185, 1988."
REFERENCES,0.7017543859649122,F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. ACM
REFERENCES,0.706140350877193,"transactions on interactive intelligent systems (TIIS), 5(4):1–19, 2015."
REFERENCES,0.7105263157894737,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image"
REFERENCES,0.7149122807017544,"recognition. CoRR, abs/1512.03385, 2015."
REFERENCES,0.7192982456140351,"Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural"
REFERENCES,0.7236842105263158,"collaborative ﬁltering. In Proceedings of the 26th international conference on world wide web, pp.
173–182, 2017."
REFERENCES,0.7280701754385965,"Elad Hoffer, Itay Hubara, and Daniel Soudry.
Train longer, generalize better: closing the
generalization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741,
2017."
REFERENCES,0.7324561403508771,"Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efﬁcient and accurate"
REFERENCES,0.7368421052631579,"normalization schemes in deep networks. arXiv preprint arXiv:1803.01814, 2018."
REFERENCES,0.7412280701754386,"Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Normalization techniques in training"
REFERENCES,0.7456140350877193,"dnns: Methodology, analysis and application. CoRR, abs/2009.12836, 2020."
REFERENCES,0.75,"Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, and Masashi Sugiyama. Do we need zero"
REFERENCES,0.7543859649122807,"training loss after achieving zero training error? arXiv preprint arXiv:2002.08709, 2020."
REFERENCES,0.7587719298245614,"Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun"
REFERENCES,0.7631578947368421,"Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural
networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020."
REFERENCES,0.7675438596491229,Under review as a conference paper at ICLR 2022
REFERENCES,0.7719298245614035,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter"
REFERENCES,0.7763157894736842,"Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016."
REFERENCES,0.7807017543859649,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017."
REFERENCES,0.7850877192982456,"Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 and CIFAR-100 datasets. URl:"
REFERENCES,0.7894736842105263,"https://www.cs.toronto.edu/~kriz/cifar.html, 6(1):1, 2009."
REFERENCES,0.793859649122807,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep
convolutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012."
REFERENCES,0.7982456140350878,"Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense"
REFERENCES,0.8026315789473685,"object detection. CoRR, abs/1708.02002, 2017."
REFERENCES,0.8070175438596491,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei"
REFERENCES,0.8114035087719298,"Han. On the variance of the adaptive learning rate and beyond, 2020."
REFERENCES,0.8157894736842105,Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv
REFERENCES,0.8201754385964912,"preprint arXiv:1608.03983, 2016."
REFERENCES,0.8245614035087719,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019."
REFERENCES,0.8289473684210527,"Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks, 2020."
REFERENCES,0.8333333333333334,"Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip H. S. Torr, and Puneet K."
REFERENCES,0.8377192982456141,"Dokania. Calibrating deep neural networks using focal loss. CoRR, abs/2002.09437, 2020."
REFERENCES,0.8421052631578947,"Zachary Nado, Justin M Gilmer, Christopher J Shallue, Rohan Anil, and George E Dahl. A large"
REFERENCES,0.8464912280701754,"batch optimizer reality check: Traditional, generic optimizers sufﬁce across batch sizes. arXiv
preprint arXiv:2102.06356, 2021."
REFERENCES,0.8508771929824561,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural"
REFERENCES,0.8552631578947368,"networks. CoRR, abs/1503.00036, 2015."
REFERENCES,0.8596491228070176,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor"
REFERENCES,0.8640350877192983,"Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,"
REFERENCES,0.868421052631579,"Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc., 2019."
REFERENCES,0.8728070175438597,"Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):"
REFERENCES,0.8771929824561403,"145–151, 1999."
REFERENCES,0.881578947368421,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object"
REFERENCES,0.8859649122807017,"detection with region proposal networks. arXiv preprint arXiv:1506.01497, 2015."
REFERENCES,0.8903508771929824,Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to
REFERENCES,0.8947368421052632,"accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016."
REFERENCES,0.8991228070175439,"Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and"
REFERENCES,0.9035087719298246,"George E Dahl. Measuring the effects of data parallelism on neural network training. arXiv
preprint arXiv:1811.03600, 2018."
REFERENCES,0.9078947368421053,Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
REFERENCES,0.9122807017543859,"recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.9166666666666666,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna."
REFERENCES,0.9210526315789473,"Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015."
REFERENCES,0.9254385964912281,Under review as a conference paper at ICLR 2022
REFERENCES,0.9298245614035088,"Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba. An"
REFERENCES,0.9342105263157895,"empirical study of stochastic gradient descent with structured covariance noise. In AISTATS, pp.
3621—-3631, 2020."
REFERENCES,0.9385964912280702,"Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal"
REFERENCES,0.9429824561403509,"value of adaptive gradient methods in machine learning. arXiv preprint arXiv:1705.08292, 2017."
REFERENCES,0.9473684210526315,"Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, , and Zhanxing Zhu. On"
REFERENCES,0.9517543859649122,"the noisy gradient descent that generalizes as sgd. In ICML, pp. 10367—-10376, 2020."
REFERENCES,0.956140350877193,"Lei Wu, Chao Ma, and Weinan E. How SGD selects the global minima in over-parameterized"
REFERENCES,0.9605263157894737,"learning: A dynamical stability perspective. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems.
Curran Associates, Inc."
REFERENCES,0.9649122807017544,"Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks, 2017."
REFERENCES,0.9692982456140351,"Yang You, Jonathan Hseu, Chris Ying, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large-batch"
REFERENCES,0.9736842105263158,"training for lstm and beyond. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, pp. 1–16, 2019."
REFERENCES,0.9780701754385965,"Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan"
REFERENCES,0.9824561403508771,"Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training BERT in 76 minutes, 2020."
REFERENCES,0.9868421052631579,"Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012."
REFERENCES,0.9912280701754386,"Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven C. H. Hoi, and Weinan E. Towards"
REFERENCES,0.9956140350877193,"theoretically understanding why SGD generalizes better than ADAM in deep learning. CoRR,
abs/2010.05627, 2020."
