Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.001095290251916758,"Contemporary machine learning methods will try to approach the Bayes error, as
1"
ABSTRACT,0.002190580503833516,"it is the lowest possible error any model can achieve. This paper postulates that
2"
ABSTRACT,0.0032858707557502738,"any decision is composed of not one but two Bayesian decisions and that decision-
3"
ABSTRACT,0.004381161007667032,"making is, therefore, a double-Bayesian process. The paper shows how this duality
4"
ABSTRACT,0.00547645125958379,"implies intrinsic uncertainty in decisions and how it incorporates explainability.
5"
ABSTRACT,0.0065717415115005475,"The proposed approach understands that Bayesian learning is tantamount to finding
6"
ABSTRACT,0.007667031763417305,"a base for a logarithmic function measuring uncertainty, with solutions being fixed
7"
ABSTRACT,0.008762322015334063,"points. Furthermore, following this approach, the golden ratio describes possible
8"
ABSTRACT,0.009857612267250822,"solutions satisfying Bayes’ theorem. The double-Bayesian framework suggests
9"
ABSTRACT,0.01095290251916758,"using a learning rate and momentum weight with values similar to those used in
10"
ABSTRACT,0.012048192771084338,"the literature to train neural networks with stochastic gradient descent.
11"
INTRODUCTION,0.013143483023001095,"1
Introduction
12"
INTRODUCTION,0.014238773274917854,"Despite the progress in machine learning, several problems stand out for which convincing solutions
13"
INTRODUCTION,0.01533406352683461,"have yet to be found. With massive training sets, enormously sized networks, and immense computing
14"
INTRODUCTION,0.01642935377875137,"power, training machine learning models has become a brute force approach, arguably more concerned
15"
INTRODUCTION,0.017524644030668127,"with memorization than generalization. However, quoting from a post by Y. LeCun (Nov. 23, 2023),
16"
INTRODUCTION,0.018619934282584884,"we know that
17"
INTRODUCTION,0.019715224534501644,"Animals and humans get very smart very quickly with vastly smaller amounts of training data than
18"
INTRODUCTION,0.0208105147864184,"current AI systems. Current large language models (LLMs) are trained on text data that would take
19"
INTRODUCTION,0.02190580503833516,"20,000 years for a human to read. And still, they haven’t learned that if A is the same as B, then B
20"
INTRODUCTION,0.023001095290251915,"is the same as A. Humans get a lot smarter than that with comparatively little training data. Even
21"
INTRODUCTION,0.024096385542168676,"corvids, parrots, dogs, and octopuses get smarter than that very, very quickly, with only 2 billion
22"
INTRODUCTION,0.025191675794085433,"neurons and a few trillion ""parameters.""
23"
INTRODUCTION,0.02628696604600219,"This raises the question of whether modern training techniques and principles are actually biologically
24"
INTRODUCTION,0.027382256297918947,"implemented in the human brain and, if not, what alternative methods could save resources. More
25"
INTRODUCTION,0.028477546549835708,"efficient methods would be better at generalizing with smaller amounts of training data, which almost
26"
INTRODUCTION,0.029572836801752465,"certainly would also improve the explainability and interpretability of neural networks.
27"
INTRODUCTION,0.03066812705366922,"This paper investigates what it takes for a classifier to be optimal. The starting point is Bayes’ theorem,
28"
INTRODUCTION,0.03176341730558598,"which is the foundation of the Bayes classifier. The Bayes classifier is considered optimal because
29"
INTRODUCTION,0.03285870755750274,"it minimizes the Bayes risk, meaning it has the smallest probability of misclassification among all
30"
INTRODUCTION,0.033953997809419496,"classifiers. However, applying the Bayes classifier directly is often impossible because of the difficulty
31"
INTRODUCTION,0.03504928806133625,"in computing the posterior probabilities. For this reason, most classifiers are trying to approximate
32"
INTRODUCTION,0.03614457831325301,"the Bayes classifier, like the naïve Bayes classifier, for instance. The information-theoretical analysis
33"
INTRODUCTION,0.03723986856516977,"presented in this paper splits the decision of a Bayes classifier into two decisions, each following
34"
INTRODUCTION,0.038335158817086525,"Bayes’ theorem, where one decision can serve as an explanation or verification of the other. Each of
35"
INTRODUCTION,0.03943044906900329,"the two decision processes faces intrinsic uncertainty, as its decision depends on the output of the
36"
INTRODUCTION,0.040525739320920046,"other process. The paper will investigate the theoretical ramifications of this approach. As a practical
37"
INTRODUCTION,0.0416210295728368,"result, it will discuss the consequences for two hyperparameters of stochastic gradient descent used
38"
INTRODUCTION,0.04271631982475356,"in the training process of a neural network: learning rate and momentum weight.
39"
INTRODUCTION,0.04381161007667032,"The structure of the paper is as follows: After this introduction, Section 2 motivates one of the main
40"
INTRODUCTION,0.044906900328587074,"ideas, namely that learning to make a decision involves solving two sub-problems and, thus, two
41"
INTRODUCTION,0.04600219058050383,"decisions. Section 3 discusses Bayes’ theorem, which is central to statistical decision-making and
42"
INTRODUCTION,0.047097480832420595,"is the starting point of the theoretical approach outlined in the following. Section 4 then introduces
43"
INTRODUCTION,0.04819277108433735,"the double-Bayesian model as the key concept of the paper. The next section, Section 5, shows
44"
INTRODUCTION,0.04928806133625411,"how to represent possible solutions of the double-Bayesian decision model. Section 6 discusses the
45"
INTRODUCTION,0.050383351588170866,"golden ratio, including its functional equations and how it defines a solution to the double-Bayesian
46"
INTRODUCTION,0.05147864184008762,"model. Then, Section 7 discusses the theoretical implications for training double-Bayesian networks
47"
INTRODUCTION,0.05257393209200438,"with stochastic gradient descent. Finally, Section 8 summarizes the key concepts, followed by a
48"
INTRODUCTION,0.05366922234392114,"conclusion.
49"
DUAL DECISIONS,0.054764512595837894,"2
Dual decisions
50"
DUAL DECISIONS,0.05585980284775466,Suppose a sender transmits the image on the left-hand side of Figure 1 to a receiver. This image
DUAL DECISIONS,0.056955093099671415,"Figure 1: An image of Rubin’s vase (left) and its inverted counterpart (right) - (Rubin, 1915) 51"
DUAL DECISIONS,0.05805038335158817,"depicts Rubin’s vase by the Danish psychologist Edgar Rubin (Rubin, 1915), which shows a vase
52"
DUAL DECISIONS,0.05914567360350493,"or two faces looking at each other, depending on the receiver’s perception. The receiver then faces
53"
DUAL DECISIONS,0.060240963855421686,"an unsolvable conundrum: 1) If the receiver thinks the image represents a vase, the receiver cannot
54"
DUAL DECISIONS,0.06133625410733844,"be certain that the vase is indeed the intended message the sender wanted to convey. Maybe the
55"
DUAL DECISIONS,0.0624315443592552,"sender wanted to send the faces. 2) If the receiver is expecting a picture of a vase (or faces) and
56"
DUAL DECISIONS,0.06352683461117196,"thus knows the intended message, there is no certainty that an image of a vase has been transmitted.
57"
DUAL DECISIONS,0.06462212486308871,"After all, the image could show faces. Therefore, two decisions are involved in making the final
58"
DUAL DECISIONS,0.06571741511500548,"interpretation of the image: 1) a decision about the perception of the image (vase or faces), and 2)
59"
DUAL DECISIONS,0.06681270536692223,"a decision about whether the perceived image coincides with the intended message, meaning the
60"
DUAL DECISIONS,0.06790799561883899,"image transmitted. Both decisions together are fraught with intrinsic uncertainty because deciding the
61"
DUAL DECISIONS,0.06900328587075576,"ultimate interpretation of Rubin’s vase, a vase or faces, is impossible. Therefore, neither the sender
62"
DUAL DECISIONS,0.0700985761226725,"nor the receiver can make both decisions without uncertainty. Instead, the knowledge is distributed.
63"
DUAL DECISIONS,0.07119386637458927,"The sender knows the intended message (a vase or faces) but not the receiver’s perceived image. On
64"
DUAL DECISIONS,0.07228915662650602,"the other hand, the receiver knows the perceived image (a vase or faces) but not the intended message.
65"
DUAL DECISIONS,0.07338444687842278,"Therefore, the sender and the receiver must collaborate to get the true interpretation across their
66"
DUAL DECISIONS,0.07447973713033954,"communication channel.
67"
DUAL DECISIONS,0.0755750273822563,"Let the sender and receiver perceive Rubin’s vase differently, with contrary opinions about the
68"
DUAL DECISIONS,0.07667031763417305,"foreground and background color (black or white), where the foreground represents the perceived
69"
DUAL DECISIONS,0.07776560788608981,"image, either a vase or faces. Furthermore, let the sender and the receiver both be able to send
70"
DUAL DECISIONS,0.07886089813800658,"an image of Rubin’s vase to each other so that both become senders and receivers alike and can
71"
DUAL DECISIONS,0.07995618838992333,"share their knowledge about the perceived image and intended message. The image that the sender
72"
DUAL DECISIONS,0.08105147864184009,"perceives is then the inverted image that the sender perceives. The goal is to collaborate so that the
73"
DUAL DECISIONS,0.08214676889375684,"perceived image (foreground) equals the intended message on both ends.
74"
DUAL DECISIONS,0.0832420591456736,"A sender can either send the image of Rubin’s vase on the left-hand side of Figure 1 or send the
75"
DUAL DECISIONS,0.08433734939759036,"image with colors inverted, as shown on the right-hand side of Figure 1, depending on the perceived
76"
DUAL DECISIONS,0.08543263964950712,"image or intended message, respectively. On the other end, the receiver has two options: 1) accept
77"
DUAL DECISIONS,0.08652792990142388,"the received image if it is identical to the image expected, or 2) tell the sender to invert the image if it
78"
DUAL DECISIONS,0.08762322015334063,"is different. After this feedback, the image on the receiver end will be the same as the image on the
79"
DUAL DECISIONS,0.0887185104052574,"sender side. By making the images on both sides the same, the receiver has completed half of the
80"
DUAL DECISIONS,0.08981380065717415,"decision process without making a mistake and has thus behaved optimally. The receiver has ensured
81"
DUAL DECISIONS,0.09090909090909091,"that both sides see the same image. It is now up to the sender to make the final, second decision about
82"
DUAL DECISIONS,0.09200438116100766,"what image needs to be inverted to arrive at the final interpretation, either the image of the sender
83"
DUAL DECISIONS,0.09309967141292443,"or the image of the receiver. Thus, the first process tries to make the images identical, whereas the
84"
DUAL DECISIONS,0.09419496166484119,"second process tries to make the images different on both ends to reflect the different perceptions of
85"
DUAL DECISIONS,0.09529025191675794,"the sender and receiver.
86"
DUAL DECISIONS,0.0963855421686747,"Although described as a sequential process, the two dual decision processes leading to the final
87"
DUAL DECISIONS,0.09748083242059145,"interpretation are running in parallel. The sender is also a receiver, and the receiver is also a sender.
88"
DUAL DECISIONS,0.09857612267250822,"One of them conveys the correct foreground information (black or white), while the other conveys the
89"
DUAL DECISIONS,0.09967141292442497,"message. Note that neither the sender nor the receiver will ever see the true interpretation of the image.
90"
DUAL DECISIONS,0.10076670317634173,"The receiver in the example above will never know whether the received image needs to be inverted
91"
DUAL DECISIONS,0.10186199342825848,"after making the images identical because this would mean the receiver knows the true interpretation
92"
DUAL DECISIONS,0.10295728368017525,"of the image, which is not possible according to the uncertainty principle described above. A similar
93"
DUAL DECISIONS,0.10405257393209201,"statement can be made for the sender. The sender and the receiver can be considered dual and
94"
DUAL DECISIONS,0.10514786418400876,"complementary forces because of their different interpretations of foreground and background. They
95"
DUAL DECISIONS,0.10624315443592552,"make two binary decisions, deciding on the correct foreground color (black or white) and on the
96"
DUAL DECISIONS,0.10733844468784227,"message (a vase or faces). They decide whether Rubin’s vase should be interpreted as a white vase, a
97"
DUAL DECISIONS,0.10843373493975904,"black vase, white faces, or black faces.
98"
BAYES THEOREM,0.10952902519167579,"3
Bayes theorem
99"
BAYES THEOREM,0.11062431544359255,"Bayes’ theorem is a fundamental law in probability theory that describes the probability of an event
100"
BAYES THEOREM,0.11171960569550932,"given prior knowledge. The theorem is of central importance in machine learning, where it guides the
101"
BAYES THEOREM,0.11281489594742607,"training of machines for decision-making, such as in Bayesian inference or naïve Bayes classification.
102"
BAYES THEOREM,0.11391018619934283,"For two events A and B, with prior probabilities P(A) and P(B), and P(B) ̸= 0, Bayes’ theorem
103"
BAYES THEOREM,0.11500547645125958,"states the following:
104"
BAYES THEOREM,0.11610076670317634,P(A|B) = P(A) · P(B|A)
BAYES THEOREM,0.1171960569550931,"P(B)
,
(1)"
BAYES THEOREM,0.11829134720700986,"where P(A|B) and P(B|A) are the conditional or posterior probabilities. Thus, P(A|B) is the
105"
BAYES THEOREM,0.11938663745892661,"probability of event A occurring when B is true, and analogously, P(B|A) is the probability of B
106"
BAYES THEOREM,0.12048192771084337,"given that A is true.
107"
BAYES THEOREM,0.12157721796276014,"For a machine learning application, A would be the class of an observed input pattern B. The
108"
BAYES THEOREM,0.12267250821467689,"probability P(A) is then the prior probability of class A, and P(B) is the prior probability of seeing
109"
BAYES THEOREM,0.12376779846659365,"pattern B. Consequently, P(A|B) is the posterior probability of class A when seeing pattern B, and
110"
BAYES THEOREM,0.1248630887185104,"P(B|A) is the posterior probability of B within A. According to Bayes’ theorem, three probabilities
111"
BAYES THEOREM,0.12595837897042717,"are needed to compute the probability P(A|B) that class A is observed when seeing pattern B: P(A),
112"
BAYES THEOREM,0.12705366922234393,"P(B), and P(B|A). However, several obstacles prevent Bayes’ theorem from being applied in this
113"
BAYES THEOREM,0.12814895947426067,"way. No particular method can help determine the prior probabilities, which are often unknown.
114"
BAYES THEOREM,0.12924424972617743,"Furthermore, the posterior probability is often not readily available and is approximated by making
115"
BAYES THEOREM,0.1303395399780942,"assumptions about the distribution of B given A, for example, assuming a normal distribution.
116"
BAYES THEOREM,0.13143483023001096,"To cope with these limitations, the next section describes decision-making as a dual process based on
117"
BAYES THEOREM,0.13253012048192772,"Bayes’ theorem, with uncertainty intrinsically involved.
118"
DOUBLE-BAYESIAN FRAMEWORK,0.13362541073384446,"4
Double-Bayesian framework
119"
DOUBLE-BAYESIAN FRAMEWORK,0.13472070098576122,"The Bayes Theorem is typically stated as in Eq. 1. However, restating the theorem in the following
120"
DOUBLE-BAYESIAN FRAMEWORK,0.13581599123767799,"equivalent form highlights the two decision processes for the two subproblems involved, as motivated
121"
DOUBLE-BAYESIAN FRAMEWORK,0.13691128148959475,"in Section 2:
122"
DOUBLE-BAYESIAN FRAMEWORK,0.1380065717415115,"P(A|B)
P(B|A) = P(A)"
DOUBLE-BAYESIAN FRAMEWORK,0.13910186199342825,"P(B)
(2)"
DOUBLE-BAYESIAN FRAMEWORK,0.140197152245345,"The left-hand side of Eq. 2 features a fraction of the posterior probabilities, whereas the right-hand
123"
DOUBLE-BAYESIAN FRAMEWORK,0.14129244249726178,"side shows the prior probabilities. Following the motivation in Section 2, the posterior probabilities,
124"
DOUBLE-BAYESIAN FRAMEWORK,0.14238773274917854,"P(A|B) and P(B|A), can be understood as the probability that A or B is the intended message,
125"
DOUBLE-BAYESIAN FRAMEWORK,0.14348302300109528,"respectively. Then, the prior probabilities, P(A) and P(B), would express the probabilities that A or
126"
DOUBLE-BAYESIAN FRAMEWORK,0.14457831325301204,"B is in the foreground.
127"
DOUBLE-BAYESIAN FRAMEWORK,0.1456736035049288,"With only one equation for four parameters, Eq. 2 is underdetermined. However, it is fair to assume
128"
DOUBLE-BAYESIAN FRAMEWORK,0.14676889375684557,"that 1−(P(A|B) = P(B|A) and 1−P(A) = P(B), which leaves one equation with one parameter
129"
DOUBLE-BAYESIAN FRAMEWORK,0.14786418400876233,"on each side. This is possible because either A or B can be the message or foreground, not both of
130"
DOUBLE-BAYESIAN FRAMEWORK,0.14895947426067907,"them at the same time, following again the reasoning in Section 2. Therefore, the intrinsic uncertainty
131"
DOUBLE-BAYESIAN FRAMEWORK,0.15005476451259583,"in Bayes’ theorem can be described as follows: if the true foreground is known, then whether the
132"
DOUBLE-BAYESIAN FRAMEWORK,0.1511500547645126,"message needs to be swapped is unknown; on the other hand, if the message is known, then whether
133"
DOUBLE-BAYESIAN FRAMEWORK,0.15224534501642936,"the foreground needs to be swapped is unknown. The fractions on both sides of Eq. 2 are thus
134"
DOUBLE-BAYESIAN FRAMEWORK,0.1533406352683461,"""cognitively entangled.""
135"
DOUBLE-BAYESIAN FRAMEWORK,0.15443592552026286,"The two remaining unknown parameters can be computed using two separate processes, each adding
136"
DOUBLE-BAYESIAN FRAMEWORK,0.15553121577217963,"a constraint to handle the uncertainty. To illustrate this, Eq. 3 restates Bayes’ theorem in yet another
137"
DOUBLE-BAYESIAN FRAMEWORK,0.1566265060240964,"way:
138"
DOUBLE-BAYESIAN FRAMEWORK,0.15772179627601315,1 = P(A)
DOUBLE-BAYESIAN FRAMEWORK,0.1588170865279299,P(B) · P(B|A)
DOUBLE-BAYESIAN FRAMEWORK,0.15991237677984665,"P(A|B)
(3)"
DOUBLE-BAYESIAN FRAMEWORK,0.16100766703176342,"Assuming that P(B) = P(B|A), Eq. 3 simplifies to P(A|B) = P(A). This assumption of B being
139"
DOUBLE-BAYESIAN FRAMEWORK,0.16210295728368018,"independent of A is fair because, according to the motivation in Section 2, the decisions about the
140"
DOUBLE-BAYESIAN FRAMEWORK,0.16319824753559695,"message and the foreground are independent of each other. Under this assumption, only one unknown
141"
DOUBLE-BAYESIAN FRAMEWORK,0.16429353778751368,"remains, either P(A|B) or P(A), which follows directly from either P(A) or P(A|B), depending
142"
DOUBLE-BAYESIAN FRAMEWORK,0.16538882803943045,"on which is input and which is output.
143"
DOUBLE-BAYESIAN FRAMEWORK,0.1664841182913472,"A similar, symmetric statement can be made when using the reciprocals on both sides of Eq. 2, which
144"
DOUBLE-BAYESIAN FRAMEWORK,0.16757940854326397,"leads to the following equation:
145"
DOUBLE-BAYESIAN FRAMEWORK,0.1686746987951807,1 = P(B)
DOUBLE-BAYESIAN FRAMEWORK,0.16976998904709747,P(A) · P(A|B)
DOUBLE-BAYESIAN FRAMEWORK,0.17086527929901424,"P(B|A)
(4)"
DOUBLE-BAYESIAN FRAMEWORK,0.171960569550931,"Here, assuming that A is independent of B simplifies Eq. 4 to P(B|A) = P(B).
146"
DOUBLE-BAYESIAN FRAMEWORK,0.17305585980284777,"Solving Eq. 2, Eq. 3, or Eq. 4 will be referred to as solving the outer Bayes equation. On the other
147"
DOUBLE-BAYESIAN FRAMEWORK,0.1741511500547645,"hand, making both multiplicands on the right-hand side of Eq. 3 or Eq. 4 identical will be referred to
148"
DOUBLE-BAYESIAN FRAMEWORK,0.17524644030668127,"as solving the inner Bayes equation, or simply solving the inner equation of Eq. 3 or Eq. 4. For Eq. 3,
149"
DOUBLE-BAYESIAN FRAMEWORK,0.17634173055859803,"the inner Bayes equation thus states as follows:
150"
DOUBLE-BAYESIAN FRAMEWORK,0.1774370208105148,"P(A)
P(B) = P(B|A)"
DOUBLE-BAYESIAN FRAMEWORK,0.17853231106243153,"P(A|B)
(5)"
DOUBLE-BAYESIAN FRAMEWORK,0.1796276013143483,"Accordingly, the inner Bayes equation for Eq. 4 is obtained by using the reciprocals of the fractions
151"
DOUBLE-BAYESIAN FRAMEWORK,0.18072289156626506,"on both sides of Eq. 5:
152"
DOUBLE-BAYESIAN FRAMEWORK,0.18181818181818182,"P(B)
P(A) = P(A|B)"
DOUBLE-BAYESIAN FRAMEWORK,0.1829134720700986,"P(B|A)
(6)"
DOUBLE-BAYESIAN FRAMEWORK,0.18400876232201532,"Consequently, the inner Bayes equations can derived by inverting a fraction on one side of Bayes’
153"
DOUBLE-BAYESIAN FRAMEWORK,0.1851040525739321,"theorem, as stated in Eq. 2. The inner Bayes equations are thus ""entangled"" versions of Bayes’
154"
DOUBLE-BAYESIAN FRAMEWORK,0.18619934282584885,"theorem.
155"
DOUBLE-BAYESIAN FRAMEWORK,0.18729463307776562,"The two independent decision processes motivated above are solving the inner and outer Bayes equa-
156"
DOUBLE-BAYESIAN FRAMEWORK,0.18838992332968238,"tions. To further formalize these processes, the following section will add a logarithmic expression
157"
DOUBLE-BAYESIAN FRAMEWORK,0.18948521358159912,"to Eq. 3 and Eq. 4. Adding a logarithm offers several advantages: 1) using information theory to
158"
DOUBLE-BAYESIAN FRAMEWORK,0.19058050383351588,"measure uncertainty; 2) using a reciprocal becomes equivalent to changing the sign of a logarithm;
159"
DOUBLE-BAYESIAN FRAMEWORK,0.19167579408543264,"and 3) solving the equation in Bayes’ theorem is reduced to finding a suitable base for a logarithm.
160"
FIXPOINT SOLUTIONS,0.1927710843373494,"5
Fixpoint solutions
161"
FIXPOINT SOLUTIONS,0.19386637458926614,"Using a logarithmic expression in Eq. 3 and Eq. 4 is possible when solutions become fixed points
162"
FIXPOINT SOLUTIONS,0.1949616648411829,"of a logarithmic function. To illustrate this, let logb(x) be the logarithm for an input x and a base b.
163"
FIXPOINT SOLUTIONS,0.19605695509309967,"By definition, the logarithm is the inverse function of taking the power. Therefore, the following
164"
FIXPOINT SOLUTIONS,0.19715224534501644,"equation holds:
165"
FIXPOINT SOLUTIONS,0.1982475355969332,"x = logb(bx)
(7)"
FIXPOINT SOLUTIONS,0.19934282584884994,"For the base b of a logarithm, any positive real number can be used so long as b ̸= 1. A logarithm
166"
FIXPOINT SOLUTIONS,0.2004381161007667,"computed for base b can be converted into a logarithm for base b′ as follows:
167"
FIXPOINT SOLUTIONS,0.20153340635268346,"log′
b(x) = logb(x)/ logb(b′)
(8)"
FIXPOINT SOLUTIONS,0.20262869660460023,"Therefore, the simple term log is used for the logarithm in the following.
168"
FIXPOINT SOLUTIONS,0.20372398685651696,"By applying the logarithm to probabilities, they become information. For the two dual processes
169"
FIXPOINT SOLUTIONS,0.20481927710843373,"above, the information of one process will be its counterpart’s information with a different sign. To
170"
FIXPOINT SOLUTIONS,0.2059145673603505,"achieve this, the following identity is required:
171"
FIXPOINT SOLUTIONS,0.20700985761226726,"log(x) = x
(9)"
FIXPOINT SOLUTIONS,0.20810514786418402,"The following lemma states that this requirement can be met for general input values.
172"
FIXPOINT SOLUTIONS,0.20920043811610076,"Lemma: For every x ∈R+ \ {1}, there exists a base λ so that logλ(x) = x.
173"
FIXPOINT SOLUTIONS,0.21029572836801752,"Proof: Let b ∈R+ \ {1} be an arbitrary basis for which logb(x) = y. Furthermore, let k be
174"
FIXPOINT SOLUTIONS,0.21139101861993428,"a multiplier so that ky = x. Then, logλ(x) = x for λ = b1/k. This follows from Eq.8, with
175"
FIXPOINT SOLUTIONS,0.21248630887185105,"logλ(x) = logb(x)/ logb(λ) = logb(x)/ logb(b1/k) = logb(x) · k = x.
176"
FIXPOINT SOLUTIONS,0.21358159912376778,"Note that the common logarithmic rules apply for a fixed λ. However, when requiring a λ that always
177"
FIXPOINT SOLUTIONS,0.21467688937568455,"satisfies logλ(x) = x, computations become ambiguous, as seen here: −logλ(x) = −x ̸= 1/x =
178"
FIXPOINT SOLUTIONS,0.2157721796276013,"logλ(1/x). The base λ should be understood as a dynamic parameter that a learning system can
179"
FIXPOINT SOLUTIONS,0.21686746987951808,"modify over time so that logλ(x) converges to the input x.
180"
FIXPOINT SOLUTIONS,0.21796276013143484,"Using the logλ expression of the above Lemma, the Bayes’ equation in Eq. 3 can be written as
181"
FIXPOINT SOLUTIONS,0.21905805038335158,"follows:
182"
FIXPOINT SOLUTIONS,0.22015334063526834,1 = P(A)
FIXPOINT SOLUTIONS,0.2212486308871851,P(B) · logλ
FIXPOINT SOLUTIONS,0.22234392113910187,"P(B|A)
P(A|B) ! (10)"
FIXPOINT SOLUTIONS,0.22343921139101863,"Then, the following sequence of transformations can be derived from Eq. 10:
183"
FIXPOINT SOLUTIONS,0.22453450164293537,"P(A|B)
=
P(A)
P(B) · logλ"
FIXPOINT SOLUTIONS,0.22562979189485213,P(B|A) 1 ! (11)
FIXPOINT SOLUTIONS,0.2267250821467689,"=
1 −P(B)"
FIXPOINT SOLUTIONS,0.22782037239868566,"P(B)
· logλ

P(B|A)
 (12)"
FIXPOINT SOLUTIONS,0.2289156626506024,"=

1 −P(B)

· logλ

P(B)2 (13)"
FIXPOINT SOLUTIONS,0.23001095290251916,"=
P(B) · logλ

1 −P(B)2 (14)"
FIXPOINT SOLUTIONS,0.23110624315443593,"=
2 · P(B) · logλ
p"
FIXPOINT SOLUTIONS,0.2322015334063527,"1 −P(B)2
 (15)"
FIXPOINT SOLUTIONS,0.23329682365826945,"=
2 · sin(ϕ) · logλ

cos(ϕ)

,
(16)"
FIXPOINT SOLUTIONS,0.2343921139101862,"where the last expression holds for an angle ϕ ∈

0 ; π"
FIXPOINT SOLUTIONS,0.23548740416210295,"2

. The reasoning behind these transformations
184"
FIXPOINT SOLUTIONS,0.23658269441401972,"is as follows:
185"
FIXPOINT SOLUTIONS,0.23767798466593648,"The first step, Eq. 11, moves the posterior probability P(A|B) back to the left-hand side of the
186"
FIXPOINT SOLUTIONS,0.23877327491785322,"equation. The result is Bayes’ theorem in its original form, as shown in Equation 1.
187"
FIXPOINT SOLUTIONS,0.23986856516976998,"The next step, Eq. 12, replaces P(A) with 1 −P(B), removing one degree of freedom as motivated
188"
FIXPOINT SOLUTIONS,0.24096385542168675,"above.
189"
FIXPOINT SOLUTIONS,0.2420591456736035,"In the same way, Eq. 13 reformulates Eq. 12, assuming that P(B) = P(B|A) and that the two
190"
FIXPOINT SOLUTIONS,0.24315443592552027,"multipliers on the right-hand side of the equation are equal to meet the inner Bayes equation.
191"
FIXPOINT SOLUTIONS,0.244249726177437,"Then, Eq. 14 rewrites the right-hand side of Eq. 13, transforming 1 −P(B) = P(B)2 into the
192"
FIXPOINT SOLUTIONS,0.24534501642935377,"equivalent P(B) = 1 −P(B)2, which must hold true to satisfy the inner Bayes equation.
193"
FIXPOINT SOLUTIONS,0.24644030668127054,"Finally, Eq. 15 extracts a factor of two from the logλ expression to get a radical input expression for
194"
FIXPOINT SOLUTIONS,0.2475355969331873,"the logarithm, following the standard rules for logarithms. The new input term to the logλ expression
195"
FIXPOINT SOLUTIONS,0.24863088718510407,"in Eq. 15 allows visualizing all possible solutions to the outer and inner Bayes equations.
196"
FIXPOINT SOLUTIONS,0.2497261774370208,"To illustrate this further, Eq. 16 rewrites Eq. 15 using trigonometric functions and the Pythagorean
197"
FIXPOINT SOLUTIONS,0.2508214676889376,"relationship between sin and cos: sin2 ϕ + cos2ϕ = 1, and thus sin ϕ = ±
p"
FIXPOINT SOLUTIONS,0.25191675794085433,"1 −cos2ϕ and
198"
FIXPOINT SOLUTIONS,0.25301204819277107,"cos ϕ = ±
p"
FIXPOINT SOLUTIONS,0.25410733844468786,"1 −sin2ϕ. Solutions to the outer and inner Bayes equations then correspond to an
199"
FIXPOINT SOLUTIONS,0.2552026286966046,"angle ϕ in Equation 16, depending on the base λ. Thus, solutions are points on the unit circle.
200"
FIXPOINT SOLUTIONS,0.25629791894852133,"By changing the angle ϕ in Equation 16, all the possible solutions to the outer and inner Bayes
201"
FIXPOINT SOLUTIONS,0.2573932092004381,"equations can be visualized. Following the reasoning above, the right-hand side of Eq. 16 represents
202"
FIXPOINT SOLUTIONS,0.25848849945235486,"the inner Bayes equation. Accordingly, after bringing the factor 2 on the other side of Eq. 16,
203"
FIXPOINT SOLUTIONS,0.25958378970427165,"the inner Bayes equation is satisfied when sin(ϕ) = cos(ϕ), which is the case for ϕ = π/4, with
204"
FIXPOINT SOLUTIONS,0.2606790799561884,"sin(π/4) = cos(π/4) = 1/
√"
FIXPOINT SOLUTIONS,0.2617743702081051,"2.
205"
FIXPOINT SOLUTIONS,0.2628696604600219,"For the dual process, the logλ expression can be used in combination with the other term of the inner
206"
FIXPOINT SOLUTIONS,0.26396495071193865,"Bayes equation in Eq. 3, as shown here:
207"
FIXPOINT SOLUTIONS,0.26506024096385544,1 = logλ
FIXPOINT SOLUTIONS,0.2661555312157722,"P(A)
P(B) !"
FIXPOINT SOLUTIONS,0.2672508214676889,· P(B|A)
FIXPOINT SOLUTIONS,0.2683461117196057,"P(A|B)
(17)"
FIXPOINT SOLUTIONS,0.26944140197152244,"Note that the logλ expression has moved to the left compared to the right-hand side of Eq. 10. From
208"
FIXPOINT SOLUTIONS,0.27053669222343923,"this equation, the following sequence of transformations can be derived similar to the transformations
209"
FIXPOINT SOLUTIONS,0.27163198247535597,"above.
210"
FIXPOINT SOLUTIONS,0.2727272727272727,"P(B)
=
logλ P(A) 1 !"
FIXPOINT SOLUTIONS,0.2738225629791895,· P(B|A)
FIXPOINT SOLUTIONS,0.27491785323110624,"P(A|B)
(18)"
FIXPOINT SOLUTIONS,0.276013143483023,"=
logλ

P(A)

· 1 −P(A|B)"
FIXPOINT SOLUTIONS,0.27710843373493976,"P(A|B)
(19)"
FIXPOINT SOLUTIONS,0.2782037239868565,"=
logλ

P(A|B)2
·

1 −P(A|B)
 (20)"
FIXPOINT SOLUTIONS,0.2792990142387733,"=
logλ

1 −P(A|B)2
· P(A|B)
(21)"
FIXPOINT SOLUTIONS,0.28039430449069,"=
2 · logλ
p"
FIXPOINT SOLUTIONS,0.28148959474260676,"1 −P(A|B)2

· P(A|B)
(22)"
FIXPOINT SOLUTIONS,0.28258488499452356,"=
2 · logλ
 
sin(ϕ)

· cos(ϕ)
(23)"
FIXPOINT SOLUTIONS,0.2836801752464403,"During this sequence, assumptions similar to the ones in Eq. 12 and Eq. 13 are made. In Eq. 19,
211"
FIXPOINT SOLUTIONS,0.2847754654983571,"P(B|A) was replaced by 1 −P(A|B), and Eq. 20 assumes that P(A) = P(A|B). Again, all
212"
FIXPOINT SOLUTIONS,0.2858707557502738,"transformations assume that both multiplicands on the right-hand side are equal to satisfy the inner
213"
FIXPOINT SOLUTIONS,0.28696604600219056,"Bayes equation.
214"
FIXPOINT SOLUTIONS,0.28806133625410735,"The intrinsic uncertainty for the dual processes can again be seen in Eq. 16 and Eq. 23, where it
215"
FIXPOINT SOLUTIONS,0.2891566265060241,"manifests like this: if the base λ is known, then the angle ϕ is unknown; and vice versa, if ϕ is known,
216"
FIXPOINT SOLUTIONS,0.2902519167579409,"then λ is unknown. Each process contributes knowledge about λ and ϕ, which the other process does
217"
FIXPOINT SOLUTIONS,0.2913472070098576,"not know.
218"
FIXPOINT SOLUTIONS,0.29244249726177435,"The process knowledge about λ and ϕ does not need to be ""all-or-nothing."" The uncertainty ranges
219"
FIXPOINT SOLUTIONS,0.29353778751369114,"continuously between two extremes, and both dual processes can be somewhat knowledgeable about
220"
FIXPOINT SOLUTIONS,0.2946330777656079,"both parameters. When sin(ϕ) = cos(ϕ), with ϕ = π/4, one process has no or full knowledge
221"
FIXPOINT SOLUTIONS,0.29572836801752467,"about one parameter. With ϕ approaching 0 or π/2, where sin(ϕ) and cos(ϕ) become different, this
222"
FIXPOINT SOLUTIONS,0.2968236582694414,"knowledge increases or decreases, respectively.
223"
GOLDEN RATIO,0.29791894852135814,"6
Golden ratio
224"
GOLDEN RATIO,0.29901423877327493,"The solution to the inner Bayes equation is connected to the golden ratio (Livio, 2002), which becomes
225"
GOLDEN RATIO,0.30010952902519167,"evident from the transformations of equations above and the assumptions made for both processes.
226"
GOLDEN RATIO,0.30120481927710846,"Based on their right-hand equations, both dual processes must meet the same requirement to satisfy
227"
GOLDEN RATIO,0.3023001095290252,"the inner Bayes equation, assuming that logλ(x) produces x. For Eq. 12, with P(B) = P(B|A),
228"
GOLDEN RATIO,0.30339539978094193,"and for the corresponding Eq. 19 of the dual process, with P(A) = P(A|B), this requirement can be
229"
GOLDEN RATIO,0.3044906900328587,"written as
230"
GOLDEN RATIO,0.30558598028477546,p = 1 −p
GOLDEN RATIO,0.3066812705366922,"p
,
(24)"
GOLDEN RATIO,0.307776560788609,"where the variable p is a placeholder for one of the probabilities. Eq. 24 holds true if p is the golden
231"
GOLDEN RATIO,0.3088718510405257,"ratio, which is defined by the equivalent quadratic equation,
232"
GOLDEN RATIO,0.3099671412924425,"p2 + p −1 = 0,
(25)"
GOLDEN RATIO,0.31106243154435925,"which has two irrational solutions p1 and p2:
233 p1 = √ 5 −1"
GOLDEN RATIO,0.312157721796276,"2
≈0.618,
(26)"
GOLDEN RATIO,0.3132530120481928,"and
234"
GOLDEN RATIO,0.3143483023001095,"p2 = −
√ 5 −1"
GOLDEN RATIO,0.3154435925520263,"2
≈−1.618
(27)"
GOLDEN RATIO,0.31653888280394304,"A key observation is that the complement of both solutions, 1 −p, equals their square:
235"
GOLDEN RATIO,0.3176341730558598,"1 −p = p2
(28)"
GOLDEN RATIO,0.3187294633077766,"Alternatively, another quadratic equation that may be more frequently encountered in textbooks can
236"
GOLDEN RATIO,0.3198247535596933,"be used to arrive at the golden ratio. This equation is obtained by substituting −p for p in Eq. 25:
237"
GOLDEN RATIO,0.3209200438116101,"p2 −p −1 = 0
(29)"
GOLDEN RATIO,0.32201533406352684,"The alternative equation also possesses two irrational solutions, namely the negations of p1 and p2:
238"
GOLDEN RATIO,0.3231106243154436,"−p1 ≈−0.618
and −p2 ≈1.618
(30)"
GOLDEN RATIO,0.32420591456736036,"For these solutions, the complement 1 −p is the negative reciprocal:
239"
GOLDEN RATIO,0.3253012048192771,1 −p = −1
GOLDEN RATIO,0.3263964950711939,"p
(31)"
GOLDEN RATIO,0.32749178532311063,"Computing the complement of the golden ratio allows changing viewpoints and switching between
240"
GOLDEN RATIO,0.32858707557502737,"the solutions to the inner and outer Bayes equations. This will become important in the next section
241"
GOLDEN RATIO,0.32968236582694416,"for training neural networks.
242"
GOLDEN RATIO,0.3307776560788609,"The golden ratio is sometimes represented by the letter φ in the literature. It is often defined as a
243"
GOLDEN RATIO,0.33187294633077763,"single value, usually φ ≈1.618, and negative values are not considered (Livio, 2002; Huntley, 1970).
244"
GOLDEN RATIO,0.3329682365826944,"However, each of the four solutions to the aforementioned quadratic equations will be referred to as
245"
GOLDEN RATIO,0.33406352683461116,"the golden ratio in the context of this paper.
246"
THEORETICAL IMPLICATIONS,0.33515881708652795,"7
Theoretical implications
247"
THEORETICAL IMPLICATIONS,0.3362541073384447,"Supervised training methods first present a teaching input to a neural network and then try to make
248"
THEORETICAL IMPLICATIONS,0.3373493975903614,"the network’s output the same as the input by adjusting the network weights. This equalizing of
249"
THEORETICAL IMPLICATIONS,0.3384446878422782,"input and output can be related to equalizing multiplicands to satisfy the inner Bayes equation. For
250"
THEORETICAL IMPLICATIONS,0.33953997809419495,"example, in Eq. 18, the term P(B|A)/P(A|B) can be considered as input and the term P(A) in
251"
THEORETICAL IMPLICATIONS,0.34063526834611174,"the lambda expression as output. The task of the lambda expression is then to make both terms the
252"
THEORETICAL IMPLICATIONS,0.3417305585980285,"same to satisfy the inner Bayes equation. Moreover, the lambda expression logλ
 
P(A)

becomes
253"
THEORETICAL IMPLICATIONS,0.3428258488499452,"the gradient of a linear function for the outer Bayes equation. These relationships help to determine
254"
THEORETICAL IMPLICATIONS,0.343921139101862,"the optimal learning rate and momentum weight for training based on backpropagation and stochastic
255"
THEORETICAL IMPLICATIONS,0.34501642935377874,"gradient descent (SGD).
256"
THEORETICAL IMPLICATIONS,0.34611171960569553,"A training method based on backpropagation estimates the gradient of a loss function with respect to
257"
THEORETICAL IMPLICATIONS,0.34720700985761227,"each network weight, where the loss function measures the difference between input and network
258"
THEORETICAL IMPLICATIONS,0.348302300109529,"output. Backpropagation methods try to minimize the loss by following the gradient and updating the
259"
THEORETICAL IMPLICATIONS,0.3493975903614458,"network weights accordingly (LeCun et al., 2012). They accomplish this for one network layer at
260"
THEORETICAL IMPLICATIONS,0.35049288061336253,"a time, iteratively propagating the gradient back from the output layer to the input layer. To move
261"
THEORETICAL IMPLICATIONS,0.3515881708652793,"along the gradient towards the minimum of the loss function, a delta is added to each weight, which
262"
THEORETICAL IMPLICATIONS,0.35268346111719606,"often has the following form, including a momentum term:
263"
THEORETICAL IMPLICATIONS,0.3537787513691128,"∆wij(t) = −η
∂L
∂wij(t) + α · ∆wij(t −1)
(32)"
THEORETICAL IMPLICATIONS,0.3548740416210296,"In (32), L is the loss function, and ∆wij(t) denotes the delta added to each weight wij between a
264"
THEORETICAL IMPLICATIONS,0.3559693318729463,"node i and a node j in the network at training iteration (or time) t. The term ∂L/∂wij(t) is the partial
265"
THEORETICAL IMPLICATIONS,0.35706462212486306,"derivative of the loss function with respect to wij, at time t, which is multiplied with the learning
266"
THEORETICAL IMPLICATIONS,0.35815991237677985,"rate η. The sign of ∆wij(t) is negative, so the loss function approaches its minimum. In practice, a
267"
THEORETICAL IMPLICATIONS,0.3592552026286966,"momentum term describing the weight change at time t −1, ∆wij(t −1), is commonly added. This
268"
THEORETICAL IMPLICATIONS,0.3603504928806134,"term is typically multiplied by a weighting factor α, as seen in (32).
269"
THEORETICAL IMPLICATIONS,0.3614457831325301,"The traditional understanding is that the momentum term improves stochastic gradient descent by
270"
THEORETICAL IMPLICATIONS,0.36254107338444685,"dampening oscillations. However, the dual process model offers another explanation for the per-
271"
THEORETICAL IMPLICATIONS,0.36363636363636365,"formance improvement brought about by the momentum term. As of yet, a conclusive theory for
272"
THEORETICAL IMPLICATIONS,0.3647316538882804,"the optimal values of the learning rate η and the momentum weight α has been lacking. Although
273"
THEORETICAL IMPLICATIONS,0.3658269441401972,"second-order methods (Bengio, 2012; Sutskever et al., 2013; Spall, 2000) as well as adaptive meth-
274"
THEORETICAL IMPLICATIONS,0.3669222343921139,"ods (Jacobs, 1988; Kingma and Ba, 2014; Duchi et al., 2011; Tieleman and Hinton, 2012) have been
275"
THEORETICAL IMPLICATIONS,0.36801752464403065,"tried with various degrees of success, an ultimate answer has still to be found. Both parameters are
276"
THEORETICAL IMPLICATIONS,0.36911281489594744,"usually determined heuristically through empirical experiments or systematic search (Bergstra and
277"
THEORETICAL IMPLICATIONS,0.3702081051478642,"Bengio, 2012). Training results can be very sensitive to the value of the learning rate. For example, a
278"
THEORETICAL IMPLICATIONS,0.37130339539978097,"small learning rate may result in slow convergence, whereas a larger learning rate may result in the
279"
THEORETICAL IMPLICATIONS,0.3723986856516977,"search passing over the minimum loss. Negotiating this delicate trade-off in the regularization of the
280"
THEORETICAL IMPLICATIONS,0.37349397590361444,"training process can be time-consuming in practical applications. The literature seems to prefer initial
281"
THEORETICAL IMPLICATIONS,0.37458926615553123,"learning rates around 0.01 or smaller for SGD, although reported values differ by several orders of
282"
THEORETICAL IMPLICATIONS,0.37568455640744797,"magnitude. For the momentum weight, higher initial values around 0.9 are more common (Li et al.,
283"
THEORETICAL IMPLICATIONS,0.37677984665936476,"2020; Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; He et al., 2016).
284"
THEORETICAL IMPLICATIONS,0.3778751369112815,"As shown in the following, the proposed dual process model allows deriving theoretical values for
285"
THEORETICAL IMPLICATIONS,0.37897042716319823,"both regularization parameters: learning rate η and momentum weight α. In the weight adjustment
286"
THEORETICAL IMPLICATIONS,0.380065717415115,"given by Eq. 32, each summand represents a gradient of one of the two dual processes. These are
287"
THEORETICAL IMPLICATIONS,0.38116100766703176,"the partial derivative ∂L/∂wij(t) and the momentum term ∆wij(t −1). The momentum weight α
288"
THEORETICAL IMPLICATIONS,0.3822562979189485,"follows from the results above, where the lambda expression can be considered as the gradient of
289"
THEORETICAL IMPLICATIONS,0.3833515881708653,"the current iteration at time t. The other multiplicand of the inner Bayes equation corresponds to the
290"
THEORETICAL IMPLICATIONS,0.384446878422782,"gradient of the other dual process at time t −1, assuming that both dual processes are interleaved, if
291"
THEORETICAL IMPLICATIONS,0.3855421686746988,"not in parallel.
292"
THEORETICAL IMPLICATIONS,0.38663745892661555,"The previous sections showed that the inner Bayes equation is met when both summands are equal to
293"
THEORETICAL IMPLICATIONS,0.3877327491785323,"sin(π/4) = cos(π/4) = 1/
√"
THEORETICAL IMPLICATIONS,0.3888280394304491,"2 and when they are equal to the golden ratio. Therefore, the delta at
294"
THEORETICAL IMPLICATIONS,0.3899233296823658,"t −1, ∆wij(t −1), needs to be multiplied by a constant to obtain the golden ratio. This constant is
295"
THEORETICAL IMPLICATIONS,0.3910186199342826,"the momentum weight α, which needs to satisfy α/
√"
THEORETICAL IMPLICATIONS,0.39211391018619934,"2 = p1, and can thus be computed as follows.
296 α =
√"
THEORETICAL IMPLICATIONS,0.3932092004381161,"2 · p1 ≈0.874,
(33)"
THEORETICAL IMPLICATIONS,0.39430449069003287,"where p1 is the value of the golden ratio in Eq. 26. So, this logic provides the value of the first
297"
THEORETICAL IMPLICATIONS,0.3953997809419496,"regularization term, namely the momentum weight α, with α ≈0.874.
298"
THEORETICAL IMPLICATIONS,0.3964950711938664,"The learning rate η can be derived from the momentum weight α by converting the latter to the
299"
THEORETICAL IMPLICATIONS,0.39759036144578314,"corresponding value for the dual process. The dual process does not aim to satisfy the inner Bayes
300"
THEORETICAL IMPLICATIONS,0.39868565169769987,"equation with ϕ = π/4. Instead, it aims to satisfy the outer Bayes equation, with ϕ = 0 or ϕ = π/2,
301"
THEORETICAL IMPLICATIONS,0.39978094194961666,"and thus sin(ϕ) = 0 and cos(ϕ) = 1, or sin(ϕ) = 1 and cos(ϕ) = 0. By moving in the opposite
302"
THEORETICAL IMPLICATIONS,0.4008762322015334,"direction of the gradient of its dual counterpart, the first process can minimize its loss in satisfying
303"
THEORETICAL IMPLICATIONS,0.4019715224534502,"the inner Bayes equation. Accordingly, taking the complement of the momentum weight α twice
304"
THEORETICAL IMPLICATIONS,0.40306681270536693,"results in the learning rate η for the gradient change at time t. Taking the complement of α twice can
305"
THEORETICAL IMPLICATIONS,0.40416210295728366,"be understood as looking at the same process from a dual point of view. Mathematically, this can be
306"
THEORETICAL IMPLICATIONS,0.40525739320920046,"achieved by squaring the simple complement, 1 −α. Squaring the complement follows the functional
307"
THEORETICAL IMPLICATIONS,0.4063526834611172,"equation of the golden ratio described by Eq.28. Squaring also means bringing the multiplier 2 back
308"
THEORETICAL IMPLICATIONS,0.40744797371303393,"in, which was extracted from the lambda expression in Eq. 15 and Eq. 22 to represent all solutions
309"
THEORETICAL IMPLICATIONS,0.4085432639649507,"graphically. Applying these steps to the momentum weight α then results in the following equation
310"
THEORETICAL IMPLICATIONS,0.40963855421686746,"for the learning rate η:
311"
THEORETICAL IMPLICATIONS,0.41073384446878425,"η = (1 −α)2 ≈0.016
(34)"
THEORETICAL IMPLICATIONS,0.411829134720701,"So, this computation provides the value for the second regularization term, learning rate η, with η ≈
312"
THEORETICAL IMPLICATIONS,0.4129244249726177,"0.016.
313"
DISCUSSION,0.4140197152245345,"8
Discussion
314"
DISCUSSION,0.41511500547645125,"Starting from Bayes’ theorem, this paper develops a theoretical framework that describes any decision
315"
DISCUSSION,0.41621029572836804,"of a machine classifier as the result of two processes. The first decision process determines the input
316"
DISCUSSION,0.4173055859802848,"message; specifically, it decides whether the input is encoded according to its true value or needs to
317"
DISCUSSION,0.4184008762322015,"be inverted. On the other hand, the second decision process decides whether the output should be
318"
DISCUSSION,0.4194961664841183,"equal to the input or needs to be inverted. Although both decision processes run simultaneously, they
319"
DISCUSSION,0.42059145673603504,"are independent processes, with each possessing knowledge not accessible to the other process. What
320"
DISCUSSION,0.42168674698795183,"is uncertain for one process is certain for the other, and vice versa. The first process does not know
321"
DISCUSSION,0.42278203723986857,"whether the input should be equal to the output, and conversely, the second process does not know
322"
DISCUSSION,0.4238773274917853,"whether the input needs to be inverted. This means a binary decision always involves two bits, one
323"
DISCUSSION,0.4249726177437021,"indicating the encoding of the input and the other defining the relationship between input and output.
324"
DISCUSSION,0.42606790799561883,"However, practically, only one of the two processes can be performed at a time, leaving one bit of
325"
DISCUSSION,0.42716319824753557,"uncertainty for one of the processes.
326"
DISCUSSION,0.42825848849945236,"Theoretically, the framework proposed here formulates this duality with two processes having
327"
DISCUSSION,0.4293537787513691,"different perceptions of zero and one (black and white). The output of one process is the input to
328"
DISCUSSION,0.4304490690032859,"the other process. While one process tries to make its output equal to its input, the other aims for
329"
DISCUSSION,0.4315443592552026,"the opposite and tries to make its output as different as possible. The mathematical definitions of
330"
DISCUSSION,0.43263964950711936,"these processes are defined by the outer and inner Bayes equation, the latter of which is an entangled
331"
DISCUSSION,0.43373493975903615,"version of the original Bayes’ theorem. By introducing the logarithm, each process is given a control
332"
DISCUSSION,0.4348302300109529,"parameter, namely the base of the logarithm, to achieve its goal. This parameter, which is essentially
333"
DISCUSSION,0.4359255202628697,"a multiplier, allows each process to control the magnitude of the input/output.
334"
DISCUSSION,0.4370208105147864,"The solution space of the proposed double-Bayesian decision framework can be visualized with the
335"
DISCUSSION,0.43811610076670315,"trigonometric functions sin and cos. Furthermore, the golden ratio defines solutions to the inner
336"
DISCUSSION,0.43921139101861995,"Bayes equation. Connecting these two observations leads to specific values for momentum weight
337"
DISCUSSION,0.4403066812705367,"and learning rate for stochastic gradient descent, which tries to minimize the difference between
338"
DISCUSSION,0.4414019715224535,"training input and output during training.
339"
DISCUSSION,0.4424972617743702,"The supplemental material to this paper contains experiments for the MNIST dataset (LeCun et al.,
340"
DISCUSSION,0.44359255202628695,"accessed May 21, 2024), where the proposed double-Bayesian learning framework is practically
341"
DISCUSSION,0.44468784227820374,"evaluated. The theoretical parameters found in this paper did, in fact, provide the best performance
342"
DISCUSSION,0.4457831325301205,"for a network trained with stochastic gradient descent in a large grid search for learning rate and
343"
DISCUSSION,0.44687842278203727,"momentum weight.
344"
CONCLUSION,0.447973713033954,"9
Conclusion
345"
CONCLUSION,0.44906900328587074,"Three primary characteristics define the work presented in this paper: First, a double-Bayesian
346"
CONCLUSION,0.45016429353778753,"approach that understands learning as a process involving two Bayesian decisions instead of a single
347"
CONCLUSION,0.45125958378970427,"decision, like in contemporary approaches. Second, solving a Bayesian decision problem is equivalent
348"
CONCLUSION,0.452354874041621,"to finding a fixed point for a logarithmic function measuring uncertainty. Third, the golden ratio
349"
CONCLUSION,0.4534501642935378,"defines solutions to a Bayesian decision problem. These three characteristics make the proposed
350"
CONCLUSION,0.45454545454545453,"approach novel and unique.
351"
CONCLUSION,0.4556407447973713,"The double-Bayesian framework leads to new theoretical results for training neural networks, particu-
352"
CONCLUSION,0.45673603504928806,"larly specific hyperparameter values for backpropagation and gradient descent. These results are in
353"
CONCLUSION,0.4578313253012048,"contrast with other gradient descent heuristics in the literature that either use dynamic hyperparame-
354"
CONCLUSION,0.4589266155531216,"ters or second-order methods for adjusting parameters during training. It will be interesting to see how
355"
CONCLUSION,0.4600219058050383,"this conceptual difference will be resolved in the future. The proposed framework offers new ways to
356"
CONCLUSION,0.4611171960569551,"understand how neural networks make decisions and may thus contribute to the interpretability and
357"
CONCLUSION,0.46221248630887185,"explainability of neural networks, an actively investigated research area.
358"
CONCLUSION,0.4633077765607886,"The proposed framework may also help build bridges to other disciplines like neuroscience or
359"
CONCLUSION,0.4644030668127054,"physics. For example, representing all possible solutions to a double-Bayesian decision by means
360"
CONCLUSION,0.4654983570646221,"of trigonometric functions, as done in this paper, introduces waves. Incorporating brain waves into
361"
CONCLUSION,0.4665936473165389,"machine learning, a feature that traditional machine learning approaches are arguably lacking, would
362"
CONCLUSION,0.46768893756845564,"likely entail a better understanding of learning in general. This better understanding could mean
363"
CONCLUSION,0.4687842278203724,"training methods for smaller networks that could achieve the same performance with less training
364"
CONCLUSION,0.46987951807228917,"data, as motivated at the beginning of this paper.
365"
CONCLUSION,0.4709748083242059,"Another example of a discipline that could be related to this work is quantum mechanics. One of the
366"
CONCLUSION,0.4720700985761227,"fundamental concepts in quantum mechanics is Heisenberg’s uncertainty principle, which states that
367"
CONCLUSION,0.47316538882803943,"certain pairs of physical properties, such as the position and momentum of an electron, cannot be
368"
CONCLUSION,0.47426067907995617,"measured with absolute certainty. The more accurately one property is measured, the less is known
369"
CONCLUSION,0.47535596933187296,"about the other property. The proposed double-Bayesian framework incorporates such an intrinsic
370"
CONCLUSION,0.4764512595837897,"uncertainty and makes a connection to Bayesian decision theory, which could lead to new insights.
371"
CONCLUSION,0.47754654983570644,"Although empirical evidence in the literature supports the theoretical hyperparameter values derived
372"
CONCLUSION,0.4786418400876232,"in this paper, and the experiments in the supplemental material show that these values outperform
373"
CONCLUSION,0.47973713033953996,"other value pairs, more practical experiments are needed to corroborate these values. To address this
374"
CONCLUSION,0.48083242059145676,"limitation, future work will validate the practicality of the derived hyperparameter values in additional
375"
CONCLUSION,0.4819277108433735,"experiments across different domains and compare their performance with the performance of other
376"
CONCLUSION,0.4830230010952902,"values and other optimization strategies.
377"
REFERENCES,0.484118291347207,"References
378"
REFERENCES,0.48521358159912376,"Y. Bengio. Practical recommendations for gradient-based training of deep architectures. In Neural
379"
REFERENCES,0.48630887185104055,"networks: Tricks of the trade, pages 437–478. Springer, 2012.
380"
REFERENCES,0.4874041621029573,"J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of machine
381"
REFERENCES,0.488499452354874,"learning research, 13(2), 2012.
382"
REFERENCES,0.4895947426067908,"L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V. Niculae, P. Prettenhofer,
383"
REFERENCES,0.49069003285870755,"A. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly, B. Holt, and G. Varoquaux. API
384"
REFERENCES,0.49178532311062434,"design for machine learning software: experiences from the scikit-learn project. In ECML PKDD
385"
REFERENCES,0.4928806133625411,"Workshop: Languages for Data Mining and Machine Learning, pages 108–122, 2013.
386"
REFERENCES,0.4939759036144578,"J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
387"
REFERENCES,0.4950711938663746,"optimization. Journal of machine learning research, 12(7), 2011.
388"
REFERENCES,0.49616648411829134,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
389"
REFERENCES,0.49726177437020813,"of the IEEE Conference on computer vision and pattern recognition, pages 770–778, 2016.
390"
REFERENCES,0.49835706462212487,"H. Huntley. The Divine Proportion. Dover Publications, 1970.
391"
REFERENCES,0.4994523548740416,"R. Jacobs. Increased rates of convergence through learning rate adaptation. Neural networks, 1(4):
392"
REFERENCES,0.5005476451259584,"295–307, 1988.
393"
REFERENCES,0.5016429353778752,"D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
394"
REFERENCES,0.5027382256297919,"2014.
395"
REFERENCES,0.5038335158817087,"A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural
396"
REFERENCES,0.5049288061336255,"networks. In Advances in neural information processing systems, pages 1097–1105, 2012.
397"
REFERENCES,0.5060240963855421,"Y. LeCun, L. Bottou, G. Orr, and K. Müller. Efficient backprop. In Neural networks: Tricks of the
398"
REFERENCES,0.5071193866374589,"trade, pages 9–48. Springer, 2012.
399"
REFERENCES,0.5082146768893757,"Y. LeCun, C. Cortes, and C. Burges. The MNIST Database, accessed May 21, 2024. URL http:
400"
REFERENCES,0.5093099671412924,"//yann.lecun.com/exdb/mnist/.
401"
REFERENCES,0.5104052573932092,"H. Li, P. Chaudhari, H. Yang, M. Lam, A. Ravichandran, R. Bhotika, and S. Soatto. Rethinking the
402"
REFERENCES,0.511500547645126,"hyperparameters for fine-tuning. arXiv preprint arXiv:2002.11770, 2020.
403"
REFERENCES,0.5125958378970427,"M. Livio. The Golden Ratio. Random House, Inc., 2002.
404"
REFERENCES,0.5136911281489595,"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
405"
REFERENCES,0.5147864184008762,"hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
406"
REFERENCES,0.515881708652793,"E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
407"
REFERENCES,0.5169769989047097,"12:2825–2830, 2011.
408"
REFERENCES,0.5180722891566265,"E. Rubin. Rubin Vase. Wikimedia Commons (last accessed March 4, 2022, CC BY-SA 3.0), 1915.
409"
REFERENCES,0.5191675794085433,"URL https://commons.wikimedia.org/wiki/File:Facevase.png.
410"
REFERENCES,0.52026286966046,"Scikit-learn developers (BSD License). Scikit-learn machine learning library, accessed May 21,
411"
REFERENCES,0.5213581599123768,"2024. URL https://scikit-learn.org/stable/modules/generated/sklearn.model_
412"
REFERENCES,0.5224534501642936,"selection.StratifiedShuffleSplit.html.
413"
REFERENCES,0.5235487404162102,"K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
414"
REFERENCES,0.524644030668127,"arXiv preprint arXiv:1409.1556, 2014.
415"
REFERENCES,0.5257393209200438,"J. Spall. Adaptive stochastic approximation by the simultaneous perturbation method. IEEE transac-
416"
REFERENCES,0.5268346111719606,"tions on automatic control, 45(10):1839–1853, 2000.
417"
REFERENCES,0.5279299014238773,"I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
418"
REFERENCES,0.5290251916757941,"in deep learning. In International Conference on Machine Learning, pages 1139–1147, 2013.
419"
REFERENCES,0.5301204819277109,"T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
420"
REFERENCES,0.5312157721796276,"recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.
421"
REFERENCES,0.5323110624315444,"NeurIPS Paper Checklist
422"
CLAIMS,0.5334063526834611,"1. Claims
423"
CLAIMS,0.5345016429353778,"Question: Do the main claims made in the abstract and introduction accurately reflect the
424"
CLAIMS,0.5355969331872946,"paper’s contributions and scope?
425"
CLAIMS,0.5366922234392114,"Answer: [Yes]
426"
CLAIMS,0.5377875136911281,"Justification: This is a theoretical paper that tries to explain hyperparameter values that
427"
CLAIMS,0.5388828039430449,"have been successfully used in the literature. The paper investigates what it takes for a
428"
CLAIMS,0.5399780941949617,"classifier to be optimal, as stated in the introduction. Although the literature and the
429"
CLAIMS,0.5410733844468785,"practical experiments provided in the supplemental material support the theoretical
430"
CLAIMS,0.5421686746987951,"results, providing more practical experiments would be desirable.
431"
CLAIMS,0.5432639649507119,"Guidelines:
432"
CLAIMS,0.5443592552026287,"• The answer NA means that the abstract and introduction do not include the claims
433"
CLAIMS,0.5454545454545454,"made in the paper.
434"
CLAIMS,0.5465498357064622,"• The abstract and/or introduction should clearly state the claims made, including the
435"
CLAIMS,0.547645125958379,"contributions made in the paper and important assumptions and limitations. A No or
436"
CLAIMS,0.5487404162102957,"NA answer to this question will not be perceived well by the reviewers.
437"
CLAIMS,0.5498357064622125,"• The claims made should match theoretical and experimental results, and reflect how
438"
CLAIMS,0.5509309967141293,"much the results can be expected to generalize to other settings.
439"
CLAIMS,0.552026286966046,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
440"
CLAIMS,0.5531215772179627,"are not attained by the paper.
441"
LIMITATIONS,0.5542168674698795,"2. Limitations
442"
LIMITATIONS,0.5553121577217963,"Question: Does the paper discuss the limitations of the work performed by the authors?
443"
LIMITATIONS,0.556407447973713,"Answer: [Yes]
444"
LIMITATIONS,0.5575027382256298,"Justification: The limitations are discussed at the very end of the paper in the con-
445"
LIMITATIONS,0.5585980284775466,"clusion. A comparison with other hyperparameter optimization strategies would be
446"
LIMITATIONS,0.5596933187294633,"desirable to corroborate the theoretical results even more. Specifically, a systematic
447"
LIMITATIONS,0.56078860898138,"comparison with second-order methods and other methods that dynamically adapt
448"
LIMITATIONS,0.5618838992332968,"hyperparameters during training should shed more light on the performance of this
449"
LIMITATIONS,0.5629791894852135,"approach.
450"
LIMITATIONS,0.5640744797371303,"Guidelines:
451"
LIMITATIONS,0.5651697699890471,"• The answer NA means that the paper has no limitation while the answer No means that
452"
LIMITATIONS,0.5662650602409639,"the paper has limitations, but those are not discussed in the paper.
453"
LIMITATIONS,0.5673603504928806,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
454"
LIMITATIONS,0.5684556407447974,"• The paper should point out any strong assumptions and how robust the results are to
455"
LIMITATIONS,0.5695509309967142,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
456"
LIMITATIONS,0.5706462212486308,"model well-specification, asymptotic approximations only holding locally). The authors
457"
LIMITATIONS,0.5717415115005476,"should reflect on how these assumptions might be violated in practice and what the
458"
LIMITATIONS,0.5728368017524644,"implications would be.
459"
LIMITATIONS,0.5739320920043811,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
460"
LIMITATIONS,0.5750273822562979,"only tested on a few datasets or with a few runs. In general, empirical results often
461"
LIMITATIONS,0.5761226725082147,"depend on implicit assumptions, which should be articulated.
462"
LIMITATIONS,0.5772179627601315,"• The authors should reflect on the factors that influence the performance of the approach.
463"
LIMITATIONS,0.5783132530120482,"For example, a facial recognition algorithm may perform poorly when image resolution
464"
LIMITATIONS,0.579408543263965,"is low or images are taken in low lighting. Or a speech-to-text system might not be
465"
LIMITATIONS,0.5805038335158818,"used reliably to provide closed captions for online lectures because it fails to handle
466"
LIMITATIONS,0.5815991237677984,"technical jargon.
467"
LIMITATIONS,0.5826944140197152,"• The authors should discuss the computational efficiency of the proposed algorithms
468"
LIMITATIONS,0.583789704271632,"and how they scale with dataset size.
469"
LIMITATIONS,0.5848849945235487,"• If applicable, the authors should discuss possible limitations of their approach to
470"
LIMITATIONS,0.5859802847754655,"address problems of privacy and fairness.
471"
LIMITATIONS,0.5870755750273823,"• While the authors might fear that complete honesty about limitations might be used by
472"
LIMITATIONS,0.588170865279299,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
473"
LIMITATIONS,0.5892661555312158,"limitations that aren’t acknowledged in the paper. The authors should use their best
474"
LIMITATIONS,0.5903614457831325,"judgment and recognize that individual actions in favor of transparency play an impor-
475"
LIMITATIONS,0.5914567360350493,"tant role in developing norms that preserve the integrity of the community. Reviewers
476"
LIMITATIONS,0.592552026286966,"will be specifically instructed to not penalize honesty concerning limitations.
477"
THEORY ASSUMPTIONS AND PROOFS,0.5936473165388828,"3. Theory Assumptions and Proofs
478"
THEORY ASSUMPTIONS AND PROOFS,0.5947426067907996,"Question: For each theoretical result, does the paper provide the full set of assumptions and
479"
THEORY ASSUMPTIONS AND PROOFS,0.5958378970427163,"a complete (and correct) proof?
480"
THEORY ASSUMPTIONS AND PROOFS,0.5969331872946331,"Answer: [Yes]
481"
THEORY ASSUMPTIONS AND PROOFS,0.5980284775465499,"Justification: All assumptions are discussed in detail, and one proof has been included.
482"
THEORY ASSUMPTIONS AND PROOFS,0.5991237677984665,"Guidelines:
483"
THEORY ASSUMPTIONS AND PROOFS,0.6002190580503833,"• The answer NA means that the paper does not include theoretical results.
484"
THEORY ASSUMPTIONS AND PROOFS,0.6013143483023001,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
485"
THEORY ASSUMPTIONS AND PROOFS,0.6024096385542169,"referenced.
486"
THEORY ASSUMPTIONS AND PROOFS,0.6035049288061336,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
487"
THEORY ASSUMPTIONS AND PROOFS,0.6046002190580504,"• The proofs can either appear in the main paper or the supplemental material, but if
488"
THEORY ASSUMPTIONS AND PROOFS,0.6056955093099672,"they appear in the supplemental material, the authors are encouraged to provide a short
489"
THEORY ASSUMPTIONS AND PROOFS,0.6067907995618839,"proof sketch to provide intuition.
490"
THEORY ASSUMPTIONS AND PROOFS,0.6078860898138007,"• Inversely, any informal proof provided in the core of the paper should be complemented
491"
THEORY ASSUMPTIONS AND PROOFS,0.6089813800657174,"by formal proofs provided in appendix or supplemental material.
492"
THEORY ASSUMPTIONS AND PROOFS,0.6100766703176341,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
493"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6111719605695509,"4. Experimental Result Reproducibility
494"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6122672508214677,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
495"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6133625410733844,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
496"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6144578313253012,"of the paper (regardless of whether the code and data are provided or not)?
497"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.615553121577218,"Answer: [Yes]
498"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6166484118291348,"Justification: Experimental results are listed in the supplemental material, with infor-
499"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6177437020810514,"mation to reproduce the results, including the code itself.
500"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6188389923329682,"Guidelines:
501"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.619934282584885,"• The answer NA means that the paper does not include experiments.
502"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6210295728368017,"• If the paper includes experiments, a No answer to this question will not be perceived
503"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6221248630887185,"well by the reviewers: Making the paper reproducible is important, regardless of
504"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6232201533406353,"whether the code and data are provided or not.
505"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.624315443592552,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
506"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6254107338444688,"to make their results reproducible or verifiable.
507"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6265060240963856,"• Depending on the contribution, reproducibility can be accomplished in various ways.
508"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6276013143483024,"For example, if the contribution is a novel architecture, describing the architecture fully
509"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.628696604600219,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
510"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6297918948521358,"be necessary to either make it possible for others to replicate the model with the same
511"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6308871851040526,"dataset, or provide access to the model. In general. releasing code and data is often
512"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6319824753559693,"one good way to accomplish this, but reproducibility can also be provided via detailed
513"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6330777656078861,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
514"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6341730558598029,"of a large language model), releasing of a model checkpoint, or other means that are
515"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6352683461117196,"appropriate to the research performed.
516"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6363636363636364,"• While NeurIPS does not require releasing code, the conference does require all submis-
517"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6374589266155531,"sions to provide some reasonable avenue for reproducibility, which may depend on the
518"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6385542168674698,"nature of the contribution. For example
519"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6396495071193866,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
520"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6407447973713034,"to reproduce that algorithm.
521"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6418400876232202,"(b) If the contribution is primarily a new model architecture, the paper should describe
522"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6429353778751369,"the architecture clearly and fully.
523"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6440306681270537,"(c) If the contribution is a new model (e.g., a large language model), then there should
524"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6451259583789705,"either be a way to access this model for reproducing the results or a way to reproduce
525"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6462212486308871,"the model (e.g., with an open-source dataset or instructions for how to construct
526"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6473165388828039,"the dataset).
527"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6484118291347207,"(d) We recognize that reproducibility may be tricky in some cases, in which case
528"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6495071193866374,"authors are welcome to describe the particular way they provide for reproducibility.
529"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6506024096385542,"In the case of closed-source models, it may be that access to the model is limited in
530"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.651697699890471,"some way (e.g., to registered users), but it should be possible for other researchers
531"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6527929901423878,"to have some path to reproducing or verifying the results.
532"
OPEN ACCESS TO DATA AND CODE,0.6538882803943045,"5. Open access to data and code
533"
OPEN ACCESS TO DATA AND CODE,0.6549835706462213,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
534"
OPEN ACCESS TO DATA AND CODE,0.656078860898138,"tions to faithfully reproduce the main experimental results, as described in supplemental
535"
OPEN ACCESS TO DATA AND CODE,0.6571741511500547,"material?
536"
OPEN ACCESS TO DATA AND CODE,0.6582694414019715,"Answer: [Yes]
537"
OPEN ACCESS TO DATA AND CODE,0.6593647316538883,"Justification: Please see the supplemental material for the code and information about
538"
OPEN ACCESS TO DATA AND CODE,0.660460021905805,"reproducing the experimental results. The publicly available MNIST database has
539"
OPEN ACCESS TO DATA AND CODE,0.6615553121577218,"been used for the experiments.
540"
OPEN ACCESS TO DATA AND CODE,0.6626506024096386,"Guidelines:
541"
OPEN ACCESS TO DATA AND CODE,0.6637458926615553,"• The answer NA means that paper does not include experiments requiring code.
542"
OPEN ACCESS TO DATA AND CODE,0.664841182913472,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
543"
OPEN ACCESS TO DATA AND CODE,0.6659364731653888,"public/guides/CodeSubmissionPolicy) for more details.
544"
OPEN ACCESS TO DATA AND CODE,0.6670317634173056,"• While we encourage the release of code and data, we understand that this might not be
545"
OPEN ACCESS TO DATA AND CODE,0.6681270536692223,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
546"
OPEN ACCESS TO DATA AND CODE,0.6692223439211391,"including code, unless this is central to the contribution (e.g., for a new open-source
547"
OPEN ACCESS TO DATA AND CODE,0.6703176341730559,"benchmark).
548"
OPEN ACCESS TO DATA AND CODE,0.6714129244249726,"• The instructions should contain the exact command and environment needed to run to
549"
OPEN ACCESS TO DATA AND CODE,0.6725082146768894,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
550"
OPEN ACCESS TO DATA AND CODE,0.6736035049288062,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
551"
OPEN ACCESS TO DATA AND CODE,0.6746987951807228,"• The authors should provide instructions on data access and preparation, including how
552"
OPEN ACCESS TO DATA AND CODE,0.6757940854326396,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
553"
OPEN ACCESS TO DATA AND CODE,0.6768893756845564,"• The authors should provide scripts to reproduce all experimental results for the new
554"
OPEN ACCESS TO DATA AND CODE,0.6779846659364732,"proposed method and baselines. If only a subset of experiments are reproducible, they
555"
OPEN ACCESS TO DATA AND CODE,0.6790799561883899,"should state which ones are omitted from the script and why.
556"
OPEN ACCESS TO DATA AND CODE,0.6801752464403067,"• At submission time, to preserve anonymity, the authors should release anonymized
557"
OPEN ACCESS TO DATA AND CODE,0.6812705366922235,"versions (if applicable).
558"
OPEN ACCESS TO DATA AND CODE,0.6823658269441402,"• Providing as much information as possible in supplemental material (appended to the
559"
OPEN ACCESS TO DATA AND CODE,0.683461117196057,"paper) is recommended, but including URLs to data and code is permitted.
560"
OPEN ACCESS TO DATA AND CODE,0.6845564074479737,"6. Experimental Setting/Details
561"
OPEN ACCESS TO DATA AND CODE,0.6856516976998904,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
562"
OPEN ACCESS TO DATA AND CODE,0.6867469879518072,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
563"
OPEN ACCESS TO DATA AND CODE,0.687842278203724,"results?
564"
OPEN ACCESS TO DATA AND CODE,0.6889375684556407,"Answer: [Yes]
565"
OPEN ACCESS TO DATA AND CODE,0.6900328587075575,"Justification: Please see the information in the supplemental material.
566"
OPEN ACCESS TO DATA AND CODE,0.6911281489594743,"Guidelines:
567"
OPEN ACCESS TO DATA AND CODE,0.6922234392113911,"• The answer NA means that the paper does not include experiments.
568"
OPEN ACCESS TO DATA AND CODE,0.6933187294633077,"• The experimental setting should be presented in the core of the paper to a level of detail
569"
OPEN ACCESS TO DATA AND CODE,0.6944140197152245,"that is necessary to appreciate the results and make sense of them.
570"
OPEN ACCESS TO DATA AND CODE,0.6955093099671413,"• The full details can be provided either with the code, in appendix, or as supplemental
571"
OPEN ACCESS TO DATA AND CODE,0.696604600219058,"material.
572"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6976998904709748,"7. Experiment Statistical Significance
573"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6987951807228916,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
574"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.6998904709748083,"information about the statistical significance of the experiments?
575"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7009857612267251,"Answer: [NA]
576"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7020810514786419,"Justification: The paper provides theoretical results. For the experimental results in
577"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7031763417305587,"the supplemental material, only the relative performance to other hyperparameter
578"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7042716319824753,"combinations was investigated, significant or not, to see whether the proposed values
579"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7053669222343921,"define the optimum or are at least close to it. To compare the proposed method and
580"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7064622124863089,"values with other optimization methods, future experiments may require significance
581"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7075575027382256,"tests.
582"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7086527929901424,"Guidelines:
583"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7097480832420592,"• The answer NA means that the paper does not include experiments.
584"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7108433734939759,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
585"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7119386637458927,"dence intervals, or statistical significance tests, at least for the experiments that support
586"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7130339539978094,"the main claims of the paper.
587"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7141292442497261,"• The factors of variability that the error bars are capturing should be clearly stated (for
588"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7152245345016429,"example, train/test split, initialization, random drawing of some parameter, or overall
589"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7163198247535597,"run with given experimental conditions).
590"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7174151150054765,"• The method for calculating the error bars should be explained (closed form formula,
591"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7185104052573932,"call to a library function, bootstrap, etc.)
592"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.71960569550931,"• The assumptions made should be given (e.g., Normally distributed errors).
593"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7207009857612268,"• It should be clear whether the error bar is the standard deviation or the standard error
594"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7217962760131434,"of the mean.
595"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7228915662650602,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
596"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.723986856516977,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
597"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7250821467688937,"of Normality of errors is not verified.
598"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7261774370208105,"• For asymmetric distributions, the authors should be careful not to show in tables or
599"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7272727272727273,"figures symmetric error bars that would yield results that are out of range (e.g. negative
600"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7283680175246441,"error rates).
601"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7294633077765608,"• If error bars are reported in tables or plots, The authors should explain in the text how
602"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7305585980284776,"they were calculated and reference the corresponding figures or tables in the text.
603"
EXPERIMENTS COMPUTE RESOURCES,0.7316538882803943,"8. Experiments Compute Resources
604"
EXPERIMENTS COMPUTE RESOURCES,0.732749178532311,"Question: For each experiment, does the paper provide sufficient information on the com-
605"
EXPERIMENTS COMPUTE RESOURCES,0.7338444687842278,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
606"
EXPERIMENTS COMPUTE RESOURCES,0.7349397590361446,"the experiments?
607"
EXPERIMENTS COMPUTE RESOURCES,0.7360350492880613,"Answer: [Yes]
608"
EXPERIMENTS COMPUTE RESOURCES,0.7371303395399781,"Justification: Please see the supplemental material for more information.
609"
EXPERIMENTS COMPUTE RESOURCES,0.7382256297918949,"Guidelines:
610"
EXPERIMENTS COMPUTE RESOURCES,0.7393209200438116,"• The answer NA means that the paper does not include experiments.
611"
EXPERIMENTS COMPUTE RESOURCES,0.7404162102957283,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
612"
EXPERIMENTS COMPUTE RESOURCES,0.7415115005476451,"or cloud provider, including relevant memory and storage.
613"
EXPERIMENTS COMPUTE RESOURCES,0.7426067907995619,"• The paper should provide the amount of compute required for each of the individual
614"
EXPERIMENTS COMPUTE RESOURCES,0.7437020810514786,"experimental runs as well as estimate the total compute.
615"
EXPERIMENTS COMPUTE RESOURCES,0.7447973713033954,"• The paper should disclose whether the full research project required more compute
616"
EXPERIMENTS COMPUTE RESOURCES,0.7458926615553122,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
617"
EXPERIMENTS COMPUTE RESOURCES,0.7469879518072289,"didn’t make it into the paper).
618"
CODE OF ETHICS,0.7480832420591457,"9. Code Of Ethics
619"
CODE OF ETHICS,0.7491785323110625,"Question: Does the research conducted in the paper conform, in every respect, with the
620"
CODE OF ETHICS,0.7502738225629791,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
621"
CODE OF ETHICS,0.7513691128148959,"Answer: [Yes]
622"
CODE OF ETHICS,0.7524644030668127,"Justification: There is no violation of the NeurIPS Code of Ethics.
623"
CODE OF ETHICS,0.7535596933187295,"Guidelines:
624"
CODE OF ETHICS,0.7546549835706462,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
625"
CODE OF ETHICS,0.755750273822563,"• If the authors answer No, they should explain the special circumstances that require a
626"
CODE OF ETHICS,0.7568455640744798,"deviation from the Code of Ethics.
627"
CODE OF ETHICS,0.7579408543263965,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
628"
CODE OF ETHICS,0.7590361445783133,"eration due to laws or regulations in their jurisdiction).
629"
BROADER IMPACTS,0.76013143483023,"10. Broader Impacts
630"
BROADER IMPACTS,0.7612267250821467,"Question: Does the paper discuss both potential positive societal impacts and negative
631"
BROADER IMPACTS,0.7623220153340635,"societal impacts of the work performed?
632"
BROADER IMPACTS,0.7634173055859803,"Answer: [NA]
633"
BROADER IMPACTS,0.764512595837897,"Justification: The paper proposes a generic method to find hyperparameter values
634"
BROADER IMPACTS,0.7656078860898138,"for optimizing the performance of neural networks. Its societal impacts, therefore,
635"
BROADER IMPACTS,0.7667031763417306,"correlate with the risks of machine learning in general, which does not need to be
636"
BROADER IMPACTS,0.7677984665936474,"pointed out in particular according to the guidelines below.
637"
BROADER IMPACTS,0.768893756845564,"Guidelines:
638"
BROADER IMPACTS,0.7699890470974808,"• The answer NA means that there is no societal impact of the work performed.
639"
BROADER IMPACTS,0.7710843373493976,"• If the authors answer NA or No, they should explain why their work has no societal
640"
BROADER IMPACTS,0.7721796276013143,"impact or why the paper does not address societal impact.
641"
BROADER IMPACTS,0.7732749178532311,"• Examples of negative societal impacts include potential malicious or unintended uses
642"
BROADER IMPACTS,0.7743702081051479,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
643"
BROADER IMPACTS,0.7754654983570646,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
644"
BROADER IMPACTS,0.7765607886089814,"groups), privacy considerations, and security considerations.
645"
BROADER IMPACTS,0.7776560788608982,"• The conference expects that many papers will be foundational research and not tied
646"
BROADER IMPACTS,0.778751369112815,"to particular applications, let alone deployments. However, if there is a direct path to
647"
BROADER IMPACTS,0.7798466593647316,"any negative applications, the authors should point it out. For example, it is legitimate
648"
BROADER IMPACTS,0.7809419496166484,"to point out that an improvement in the quality of generative models could be used to
649"
BROADER IMPACTS,0.7820372398685652,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
650"
BROADER IMPACTS,0.7831325301204819,"that a generic algorithm for optimizing neural networks could enable people to train
651"
BROADER IMPACTS,0.7842278203723987,"models that generate Deepfakes faster.
652"
BROADER IMPACTS,0.7853231106243155,"• The authors should consider possible harms that could arise when the technology is
653"
BROADER IMPACTS,0.7864184008762322,"being used as intended and functioning correctly, harms that could arise when the
654"
BROADER IMPACTS,0.787513691128149,"technology is being used as intended but gives incorrect results, and harms following
655"
BROADER IMPACTS,0.7886089813800657,"from (intentional or unintentional) misuse of the technology.
656"
BROADER IMPACTS,0.7897042716319824,"• If there are negative societal impacts, the authors could also discuss possible mitigation
657"
BROADER IMPACTS,0.7907995618838992,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
658"
BROADER IMPACTS,0.791894852135816,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
659"
BROADER IMPACTS,0.7929901423877328,"feedback over time, improving the efficiency and accessibility of ML).
660"
SAFEGUARDS,0.7940854326396495,"11. Safeguards
661"
SAFEGUARDS,0.7951807228915663,"Question: Does the paper describe safeguards that have been put in place for responsible
662"
SAFEGUARDS,0.7962760131434831,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
663"
SAFEGUARDS,0.7973713033953997,"image generators, or scraped datasets)?
664"
SAFEGUARDS,0.7984665936473165,"Answer: [NA]
665"
SAFEGUARDS,0.7995618838992333,"Justification: There is no risk of misusing the proposed method beyond misusing
666"
SAFEGUARDS,0.80065717415115,"machine learning in general.
667"
SAFEGUARDS,0.8017524644030668,"Guidelines:
668"
SAFEGUARDS,0.8028477546549836,"• The answer NA means that the paper poses no such risks.
669"
SAFEGUARDS,0.8039430449069004,"• Released models that have a high risk for misuse or dual-use should be released with
670"
SAFEGUARDS,0.8050383351588171,"necessary safeguards to allow for controlled use of the model, for example by requiring
671"
SAFEGUARDS,0.8061336254107339,"that users adhere to usage guidelines or restrictions to access the model or implementing
672"
SAFEGUARDS,0.8072289156626506,"safety filters.
673"
SAFEGUARDS,0.8083242059145673,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
674"
SAFEGUARDS,0.8094194961664841,"should describe how they avoided releasing unsafe images.
675"
SAFEGUARDS,0.8105147864184009,"• We recognize that providing effective safeguards is challenging, and many papers do
676"
SAFEGUARDS,0.8116100766703176,"not require this, but we encourage authors to take this into account and make a best
677"
SAFEGUARDS,0.8127053669222344,"faith effort.
678"
LICENSES FOR EXISTING ASSETS,0.8138006571741512,"12. Licenses for existing assets
679"
LICENSES FOR EXISTING ASSETS,0.8148959474260679,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
680"
LICENSES FOR EXISTING ASSETS,0.8159912376779846,"the paper, properly credited and are the license and terms of use explicitly mentioned and
681"
LICENSES FOR EXISTING ASSETS,0.8170865279299014,"properly respected?
682"
LICENSES FOR EXISTING ASSETS,0.8181818181818182,"Answer: [Yes]
683"
LICENSES FOR EXISTING ASSETS,0.8192771084337349,"Justification: The main paper cites relevant references for the scientific content and the
684"
LICENSES FOR EXISTING ASSETS,0.8203723986856517,"supplemental material provides more details about the data and software sources.
685"
LICENSES FOR EXISTING ASSETS,0.8214676889375685,"Guidelines:
686"
LICENSES FOR EXISTING ASSETS,0.8225629791894852,"• The answer NA means that the paper does not use existing assets.
687"
LICENSES FOR EXISTING ASSETS,0.823658269441402,"• The authors should cite the original paper that produced the code package or dataset.
688"
LICENSES FOR EXISTING ASSETS,0.8247535596933188,"• The authors should state which version of the asset is used and, if possible, include a
689"
LICENSES FOR EXISTING ASSETS,0.8258488499452354,"URL.
690"
LICENSES FOR EXISTING ASSETS,0.8269441401971522,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
691"
LICENSES FOR EXISTING ASSETS,0.828039430449069,"• For scraped data from a particular source (e.g., website), the copyright and terms of
692"
LICENSES FOR EXISTING ASSETS,0.8291347207009858,"service of that source should be provided.
693"
LICENSES FOR EXISTING ASSETS,0.8302300109529025,"• If assets are released, the license, copyright information, and terms of use in the
694"
LICENSES FOR EXISTING ASSETS,0.8313253012048193,"package should be provided. For popular datasets, paperswithcode.com/datasets
695"
LICENSES FOR EXISTING ASSETS,0.8324205914567361,"has curated licenses for some datasets. Their licensing guide can help determine the
696"
LICENSES FOR EXISTING ASSETS,0.8335158817086528,"license of a dataset.
697"
LICENSES FOR EXISTING ASSETS,0.8346111719605696,"• For existing datasets that are re-packaged, both the original license and the license of
698"
LICENSES FOR EXISTING ASSETS,0.8357064622124863,"the derived asset (if it has changed) should be provided.
699"
LICENSES FOR EXISTING ASSETS,0.836801752464403,"• If this information is not available online, the authors are encouraged to reach out to
700"
LICENSES FOR EXISTING ASSETS,0.8378970427163198,"the asset’s creators.
701"
NEW ASSETS,0.8389923329682366,"13. New Assets
702"
NEW ASSETS,0.8400876232201533,"Question: Are new assets introduced in the paper well documented and is the documentation
703"
NEW ASSETS,0.8411829134720701,"provided alongside the assets?
704"
NEW ASSETS,0.8422782037239869,"Answer: [Yes]
705"
NEW ASSETS,0.8433734939759037,"Justification: The paper provides new assets in the form of knowledge about hyperpa-
706"
NEW ASSETS,0.8444687842278203,"rameter values to train neural networks with gradient descent and software to find
707"
NEW ASSETS,0.8455640744797371,"the best combination of momentum weight and learning rate with a grid search. Each
708"
NEW ASSETS,0.8466593647316539,"asset is documented in the paper and supplemental material, respectively.
709"
NEW ASSETS,0.8477546549835706,"Guidelines:
710"
NEW ASSETS,0.8488499452354874,"• The answer NA means that the paper does not release new assets.
711"
NEW ASSETS,0.8499452354874042,"• Researchers should communicate the details of the dataset/code/model as part of their
712"
NEW ASSETS,0.8510405257393209,"submissions via structured templates. This includes details about training, license,
713"
NEW ASSETS,0.8521358159912377,"limitations, etc.
714"
NEW ASSETS,0.8532311062431545,"• The paper should discuss whether and how consent was obtained from people whose
715"
NEW ASSETS,0.8543263964950711,"asset is used.
716"
NEW ASSETS,0.8554216867469879,"• At submission time, remember to anonymize your assets (if applicable). You can either
717"
NEW ASSETS,0.8565169769989047,"create an anonymized URL or include an anonymized zip file.
718"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8576122672508215,"14. Crowdsourcing and Research with Human Subjects
719"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8587075575027382,"Question: For crowdsourcing experiments and research with human subjects, does the paper
720"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.859802847754655,"include the full text of instructions given to participants and screenshots, if applicable, as
721"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8608981380065718,"well as details about compensation (if any)?
722"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8619934282584885,"Answer: [NA]
723"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8630887185104053,"Justification: The paper does not involve crowdsourcing nor research with human
724"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.864184008762322,"subjects.
725"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8652792990142387,"Guidelines:
726"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8663745892661555,"• The answer NA means that the paper does not involve crowdsourcing nor research with
727"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8674698795180723,"human subjects.
728"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8685651697699891,"• Including this information in the supplemental material is fine, but if the main contribu-
729"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8696604600219058,"tion of the paper involves human subjects, then as much detail as possible should be
730"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8707557502738226,"included in the main paper.
731"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8718510405257394,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
732"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.872946330777656,"or other labor should be paid at least the minimum wage in the country of the data
733"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8740416210295728,"collector.
734"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8751369112814896,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
735"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8762322015334063,"Subjects
736"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8773274917853231,"Question: Does the paper describe potential risks incurred by study participants, whether
737"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8784227820372399,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
738"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8795180722891566,"approvals (or an equivalent approval/review based on the requirements of your country or
739"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8806133625410734,"institution) were obtained?
740"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8817086527929902,"Answer: [NA]
741"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.882803943044907,"Justification: The paper does not involve crowdsourcing nor research with human
742"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8838992332968236,"subjects.
743"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8849945235487404,"Guidelines:
744"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8860898138006572,"• The answer NA means that the paper does not involve crowdsourcing nor research with
745"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8871851040525739,"human subjects.
746"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8882803943044907,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
747"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8893756845564075,"may be required for any human subjects research. If you obtained IRB approval, you
748"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8904709748083242,"should clearly state this in the paper.
749"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.891566265060241,"• We recognize that the procedures for this may vary significantly between institutions
750"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8926615553121577,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
751"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8937568455640745,"guidelines for their institution.
752"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8948521358159912,"• For initial submissions, do not include any information that would break anonymity (if
753"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.895947426067908,"applicable), such as the institution conducting the review.
754"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8970427163198248,"A
Appendix / supplemental material
755"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8981380065717415,"Two grid searches for the publicly available MNIST dataset were performed to corroborate the
756"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8992332968236583,"learning rate and momentum weight derived in the main paper (LeCun et al., accessed May 21, 2024).
757"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9003285870755751,"The MNIST dataset contains gray-scale images of handwritten digits and is one of the prominent
758"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9014238773274917,"datasets used to evaluate machine learning methods. It is split into a training and a test set, where the
759"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9025191675794085,latter serves as a standard of comparison. Figure 2 shows an example of the MNIST data.
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9036144578313253,Figure 2: A slightly enlarged example from the MNIST dataset showing a handwritten digit (4). 760
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.904709748083242,"A.1
Experiments
761"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9058050383351588,"The grid searches were performed on the full-size MNIST dataset and a smaller version of MNIST
762"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9069003285870756,"containing only 50% of the training data. In the latter case, a stratified sampling method named
763"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9079956188389924,"StratifiedShuffleSplit was used to create a stratified random subset of the training samples (Scikit-
764"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9090909090909091,"learn developers , BSD License; Pedregosa et al., 2011; Buitinck et al., 2013). This ensured that the
765"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9101861993428259,"class distribution in the training subset was the same as in the original full-size training set. The
766"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9112814895947426,"degradation in dataset size allowed observing how each optimizer performed under varying amounts
767"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9123767798466593,"of training data, assuming that providing less training data posed a harder problem.
768"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9134720700985761,"A deep learning model was trained based on a convolutional neural network (CNN). The model
769"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9145673603504929,"consisted of two convolutional layers, each followed by a ReLU activation function and a max
770"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9156626506024096,"pooling operation. The first convolutional layer had a single-channel input (grayscale image) and
771"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9167579408543264,"applied 16 filters, followed by a second convolutional layer that expanded the channel size to 32.
772"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9178532311062432,"Both convolutional layers used a 3x3 kernel size, a stride of one, and a padding of one. After each
773"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.91894852135816,"convolution, a ReLU activation function introduced non-linearity, and a max pooling operation with
774"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9200438116100766,"a 2x2 kernel and stride reduced the spatial dimensions by half. A dropout layer with a rate of 0.25
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9211391018619934,"was applied after flattening the output to prevent overfitting. The network concluded with two fully
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9222343921139102,"connected layers with a final output of 10 classes, where the maximum output value determined the
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9233296823658269,"class of an input image. The number of parameters was around two hundred thousand for an MNIST
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9244249726177437,"input image of size 28x28. A weight initialization was performed using the Kaiming uniform method.
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9255202628696605,"No data augmentation techniques were applied; however, the input was normalized to the range [-1,1].
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9266155531215772,"The training used a batch size of 64 and was conducted over 30 epochs, employing cross entropy
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.927710843373494,"as the loss function. The sizes of the training, validation, and test datasets were 54,000, 6,000, and
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9288061336254108,"10,000, respectively. Finally, the model’s performance was assessed through 10-fold cross-validation.
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9299014238773274,"A.2
Results
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9309967141292442,"The results of both grid searches are shown in Figure 3 for the full-size training set and in Figure 3
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.932092004381161,for the smaller training set with 50% of the size. The following values were used as momentum
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9331872946330778,Figure 3: Grid search results for MNIST 786
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9342825848849945,"weights for each grid search: 0, 0.2, 0.4, 0.6, 0.8, 0.825, 0.85, 0.874, 0.9, and 0.925. On the other
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9353778751369113,"hand, the following values were used as learning rates: 0.0001, 0.001, 0.01, 0.016, 0.1, 0.2. These
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9364731653888281,"values included the momentum weight derived in the paper (α ≈0.874) and the derived learning
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9375684556407448,"rate (η ≈0.016). Other values were chosen based on their use in the literature or to increase the
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9386637458926615,"resolution around the derived theoretical values. All possible combinations of values span a 6x10
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9397590361445783,"grid. The color of each square in the grids of Figure 3 and Figure 4 represent the performance of the
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.940854326396495,"corresponding pair of momentum weight and learning rate, with lighter colors representing higher
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9419496166484118,"performance. Green rectangles indicate the top ten performing pairs, whereas blue rectangles show
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9430449069003286,"the best-performing pair. Note that more than one pair can share the best performance, as in Figure 3.
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9441401971522454,"Figure 3 shows that no pair of momentum weight and learning rate provides better performance on
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9452354874041621,"the full-size MNIST set than the pair derived in the paper, (0.016, 0.874), although this pair has to
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9463307776560789,"share its first place with other pairs. The classification accuracies for the reduced training set size
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9474260679079957,"are slightly lower in the table of Figure 4, as one would expect for a problem with less training data.
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9485213581599123,"Nevertheless, the theoretical values derived in the paper for momentum weight and learning rate show"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9496166484118291,Figure 4: Grid search results for MNIST using only 50% of the training data 800
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9507119386637459,"again the best performance.
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9518072289156626,"A.3
Computational environment and runtime
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9529025191675794,"The software was developed using Python 3.10, and the Convolutional Neural Network (CNN) model
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9539978094194962,"was implemented in Pytorch 2.2.2. For each combination of learning rate and momentum weight (60
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9550930996714129,"combinations in total), the training time was approximately three hours for 100% of the training set
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9561883899233297,"size and about 1.5 hours for 50% of the training set. Consequently, the cumulative GPU time for all
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9572836801752465,"experiments was approximately (3 + 1.5) × 60 hours, which is 270 hours. The average memory usage
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9583789704271632,"was roughly 1 GB for each combination. For more information about the software requirements and
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9594742606790799,"workflow, see the Readme file uploaded as supplemental material together with the code.
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9605695509309967,"A.4
Computing cluster
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9616648411829135,"Figure 5 shows an overview of the GPU computing cluster that was available for the experiments,
811"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9627601314348302,"including the type of GPUs among which the processing was distributed.
812"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.963855421686747,"GPU nodes 
Processor cores per node 
Memory 
Network 36"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9649507119386638,32 x 2.8 GHz (AMD
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9660460021905805,"Epyc 7543p) 
hyperthreading enabled"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9671412924424972,"256 MB level 3 cache 
4 x NVIDIA A100 GPUs"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.968236582694414,"(80 GB VRAM, 6912 
cores, 432 Tensor cores)"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9693318729463308,NVLINK
GB,0.9704271631982475,"256 GB 
200 Gb/s HDR Infiniband (1:1) 56"
GB,0.9715224534501643,36 x 2.3 GHz (Intel Gold
GB,0.9726177437020811,"6140) 
hyperthreading enabled 
25 MB secondary cache"
GB,0.9737130339539978,"4 x NVIDIA V100-
SXM2 GPUs (32 GB 
VRAM, 5120 cores, 640"
GB,0.9748083242059146,Tensor cores)
GB,0.9759036144578314,NVLINK
GB,0.976998904709748,"384 GB 
200 Gb/s HDR Infiniband (1:1) 8"
GB,0.9780941949616648,28 x 2.4 GHz (Intel E5-
GB,0.9791894852135816,"2680v4) 
hyperthreading enabled 
35 MB secondary cache 
4 x NVIDIA V100 GPUs"
GB,0.9802847754654983,"(16 GB VRAM, 5120 
cores, 640 Tensor cores)"
GB,0.9813800657174151,128 GB
GB,0.9824753559693319,56 Gb/s FDR Infiniband
GB,0.9835706462212487,(1.11:1) 48
GB,0.9846659364731654,28 x 2.4 GHz (Intel E5-
GB,0.9857612267250822,"2680v4) 
hyperthreading enabled 
35 MB secondary cache 
4 x NVIDIA P100 GPUs"
GB,0.9868565169769989,"(16 GB VRAM, 3584"
GB,0.9879518072289156,cores)
GB,0.9890470974808324,"128 GB 
56 Gb/s FDR Infiniband"
GB,0.9901423877327492,(1.11:1) 72
GB,0.9912376779846659,28 x 2.4 GHz (Intel E5-
GB,0.9923329682365827,"2680v4) 
hyperthreading enabled 
35 MB secondary cache 
2 x NVIDIA K80 GPUs"
GB,0.9934282584884995,with 2 x GK210 GPUs
GB,0.9945235487404163,"each (24 GB VRAM,"
GB,0.9956188389923329,4992 cores)
GB,0.9967141292442497,"256 GB 
56 Gb/s FDR Infiniband"
GB,0.9978094194961665,(1.11:1)        
GB,0.9989047097480832,Figure 5: GPU computing cluster
