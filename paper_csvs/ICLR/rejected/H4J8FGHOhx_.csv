Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0045662100456621,"Multi-agent reinforcement learning (MARL) becomes more challenging in the
presence of more agents, as the capacity of the joint state and action spaces grows
exponentially in the number of agents. To address such a challenge of scale,
we identify a class of cooperative MARL problems with permutation invariance,
and formulate it as mean-ﬁeld Markov decision processes (MDP). To exploit
the permutation invariance therein, we propose the mean-ﬁeld proximal policy
optimization (MF-PPO) algorithm, at the core of which is a permutation- invariant
actor-critic neural architecture. We prove that MF-PPO attains the globally optimal
policy at a sublinear rate of convergence. Moreover, its sample complexity is
independent of the number of agents. We validate the theoretical advantages
of MF-PPO with numerical experiments in the multi-agent particle environment
(MPE). In particular, we show that the inductive bias introduced by the permutation-
invariant neural architecture enables MF-PPO to outperform existing competitors
with a smaller number of model parameters, which is the key to its generalization
performance."
INTRODUCTION,0.0091324200913242,"1
INTRODUCTION
Multi-Agent Reinforcement Learning (Littman, 1994; Zhang et al., 2019) generalizes Reinforcement
Learning (Sutton and Barto, 2018) to address the sequential decision-making problem of multiple
agents maximizing their individual long term rewards while interacting with each other in a common
environment. With breakthroughs in deep learning, MARL algorithms equipped with deep neural net-
works have seen signiﬁcant empirical successes in various domains, including simulated autonomous
driving (Shalev-Shwartz et al., 2016), multi-agent robotic control (Matari´c, 1997; Kober et al., 2013),
and E-sports (Vinyals et al., 2019)."
INTRODUCTION,0.0136986301369863,"Despite tremendous successes, MARL is notoriously hard to scale to the many-agent setting, as
the size of the state-action space grows exponentially with respect to the number of agents. This
phenomenon is recently described as the curse of many agents (Menda et al., 2018). To tackle
this challenge, we focus on cooperative MARL, where agents work together to maximize their
team reward (Panait and Luke, 2005). We identify and exploit a key property of cooperative
MARL with homogeneous agents, namely the invariance with respect to the permutation of agents.
Such permutation invariance can be found in many real-world scenarios with homogeneous agents,
such as distributed control of multiple autonomous vehicles and team sports (Cao et al., 2013;
Kalyanakrishnan et al., 2006), but also in scenarios with heterogeneous agent groups, where invariance
holds within each group (Liu et al., 2019b). More importantly, we ﬁnd that permutation invariance
has signiﬁcant practical implications, as the optimal value functions remain invariant when permuting
the joint state-action pairs. Such an observation strongly advocates a permutation invariant design for
learning, which helps reduce the effective search space of the policy/value functions from exponential
dependence on the number of agents to polynomial dependence."
INTRODUCTION,0.0182648401826484,"Several empirical methods have been proposed to incorporate permutation invariance into solving
MARL problems. Liu et al. (2019b) implement a permutation invariant critic based on Graph Convolu-
tional Network (GCN) (Kipf and Welling, 2017). Sunehag et al. (2017) propose value decomposition,
which together with parameter sharing, leads to a joint critic network that is permutation invariant
over agents. While these methods are based on heuristics, we are the ﬁrst to provide theoretical
principles for introducing permutation invariance as an inductive bias for learning value functions
and policies in homogeneous systems. In addition, we adopt the DeepSet (Zaheer et al., 2017)"
INTRODUCTION,0.0228310502283105,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.0273972602739726,"architecture, which is well suited for handling homogeneity of agents, with much simpler operations
to induce permutation invariance and greater parameter efﬁciency."
INTRODUCTION,0.0319634703196347,"To scale MARL algorithms in the presence of a large number, even inﬁnitely many, agents, mean-ﬁeld
approximation has been explored to directly model the population behavior of the agents. Mean-ﬁeld
game considers large populations of rational agents that play a noncooperative game. Yang et al.
(2017) consider a mean-ﬁeld game with deterministic linear state transitions, and show that it can be
reformulated as a mean-ﬁeld MDP, where the mean-ﬁeld state lies in ﬁnite-dimensional probability
simplex. Yang et al. (2018) take a mean-ﬁeld approximation over actions, such that the interaction
for any given agent and the population is approximated by the interaction between the agent’s action
and the averaged actions of its neighboring agents. However, the motivation for averaging over local
actions remains unclear, and it generally requires a sparse graph over agents. In practice, properly
identifying such structure also demands extensive prior knowledge. Mean-ﬁeld control instead
considers a central controller who aims to compute strategy to optimize the average payoff across the
population. Carmona et al. (2019) motivate a mean-ﬁeld MDP from the perspective of mean-ﬁeld
control. The mean-ﬁeld state therein lies in a probability simplex and is thus continuous in nature. To
enable the ensuing Q-learning algorithm, discretization of the joint state-action space is necessary. In
addition, the dynamic programming principles of such mean-ﬁeld control problem has been studied
in (Gu et al., 2019). Gu et al. (2020) also propose a Q-learning type algorithm, where the state-action
space is ﬁrst discretized into an epsilon-net. The kernel regression operator is used to construct an
estimate of the unknown Q-function from samples. Gu et al. (2021) propose a localized training,
decentralized execution framework by locally grouping homogenous agents using their states. Wang
et al. (2020) motivate a mean-ﬁeld MDP from permutation invariance, but assume a central controller
coordinating the actions of all the agents, and hence is restricted to handling the curse of many agents
from the exponential blowup of the joint state space. Our formulation of mean-ﬁeld approximation
allows agents to make their own local actions without resorting to a centralized controller."
INTRODUCTION,0.0365296803652968,"We propose a mean-ﬁeld Markov decision process motivated from the permutation invariance structure
of cooperative MARL, which can be viewed as a natural limit of ﬁnite-agent MDP by taking the
number of agents to inﬁnity. Such a mean-ﬁeld MDP generalizes traditional MDP, with each state
representing a distribution over the state space of a single agent. The mean-ﬁeld MDP provides us
a tractable formulation to model MDP with many agents, including an inﬁnite number of agents.
We further propose the Mean-Field Proximal Policy Optimization (MF-PPO) algorithm, at the core
of which is a pair of permutation invariant actor and critic neural networks. These networks are
implemented based on DeepSet (Zaheer et al., 2017), which uses convolutional type operations to
induce permutation invariance over the set of inputs. We show that with sufﬁciently many agents,
MF-PPO converges to the optimal policy of the mean-ﬁeld MDP with a sublinear sample complexity
independent of the number of agents. To support our theory, we conduct numerical experiments on
the benchmark multi-agent particle environment (MPE) and show that our proposed method requires
a smaller number of model parameters and attains better performance than multiple baselines.
Notations. W denote P(X) as the set of distribution on set X. δx denotes the Dirac measure"
INTRODUCTION,0.0410958904109589,"supported at x. For s = (s1, . . . , sN), we use s
i.i.d.
∼p to denote that each si is independently sampled
from distribution p. For f : X →R and a distribution π ∈P(X), we write ⟨f, π⟩= Ea∼πf(a). We
write [m] in short for {1, . . . , m}, and ∆d for the standard probability simplex in Rd."
PROBLEM SETUP,0.045662100456621,"2
PROBLEM SETUP
We focus on studying multi-agent systems with cooperative, homogeneous agents, where the agents
within the system are of similar nature and hence cannot be distinguished from each other. Speciﬁcally,
we consider a discrete time control problem with N agents, formulated as a Markov decision process
(SN, AN, P, r). We deﬁne the joint state space SN to be the Cartesian product of the ﬁnite state
space S for each agent, and similarly deﬁne the joint action space AN. The homogeneous nature of
the system is reﬂected in the transition kernel P and the shared reward r, which satisﬁes:
r(st, at) = r (κ(st), κ(at)) ,
P(st+1|st, at) = P (κ(st+1)|κ(st), κ(at))
(2.1)
for all (st, at) ∈SN × AN and the permutation mapping κ(·) ∈SN, where SN is the set of all
one-to-one mapping from [N] to itself. In other words, it is the conﬁguration, rather than individual
identities, that affects the team reward, and the transition to the next conﬁguration solely depends on
the current conﬁguration. See Figure 1 for detailed illustration. Such permutation invariance ﬁnds
applications in many real-world scenarios, including distributed control of autonomous vehicles, and
social economic systems (Zheng et al., 2020; Cao et al., 2013; Kalyanakrishnan et al., 2006)."
PROBLEM SETUP,0.0502283105022831,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.0547945205479452,"Our goal is to ﬁnd the optimal policy ν, where ν(s) ∈∆|AN| for all s ∈SN, and maximize the
expected discounted reward V ν(s) = (1 −γ)E {P∞
t=0 γtr(st, at)|s0 =s, at ∼ν(st), ∀t≥0} . Our
ﬁrst result shows that learning with permutation invariance advocates invariant network design.
Proposition 2.1. For cooperative MARL satisfying (2.1), there exists an optimal policy ν∗that is
permutation invariant, i.e., ν∗(s, a) = ν∗(κ(s), κ(a)) for any permutation mapping κ(·). In addition,
for any permutation invariant policy ν, the value function V (·) and the state-action value function
Q(·) is also permutation invariant, i.e., V ν(s) = V ν (κ(s)) , Qν(s, a) = Qν (κ(s), κ(a)) , where
Qν(s, a) = Es′ {r(s, a) + γV ν(s′)}."
PROBLEM SETUP,0.0593607305936073,"Proposition 2.1 has an important implication for architecture design, as it states that it sufﬁces
to search within the permutation invariant policy and value function classes. To the best of our
knowledge, this is the ﬁrst theoretical justiﬁcation of permutation invariant network design for
learning with homogeneous agents."
PROBLEM SETUP,0.0639269406392694,Charlie
PROBLEM SETUP,0.0684931506849315,"Rachel
Covington"
PROBLEM SETUP,0.0730593607305936,"John
Bruce Riley"
PROBLEM SETUP,0.0776255707762557,Charlie Rachel
PROBLEM SETUP,0.0821917808219178,"Covington
John"
PROBLEM SETUP,0.0867579908675799,"Bruce
Riley"
PROBLEM SETUP,0.091324200913242,Charlie
PROBLEM SETUP,0.0958904109589041,"Rachel
Covington John"
PROBLEM SETUP,0.1004566210045662,"Bruce
Riley"
PROBLEM SETUP,0.1050228310502283,Charlie
PROBLEM SETUP,0.1095890410958904,Rachel
PROBLEM SETUP,0.1141552511415525,"Covington
John Bruce Riley"
PROBLEM SETUP,0.1187214611872146,"Figure 1: Illustration of permutation invari-
ance. Exchanging identities of the agents
(ﬁrst and third row) does not change the
transition of the system (second row)."
PROBLEM SETUP,0.1232876712328767,"We focus on the factorized policy class with a param-
eter sharing scheme, where each agent makes its own
decision without consolidating with others. Speciﬁ-
cally, the joint policy ν can be factorized as ν(a|s) =
QN
i=1 µ(ai|oi), where µ(·) denotes the shared local
mapping and oi denotes the local observation. Such a
policy class is widely adopted in the celebrated central-
ized training – decentralized execution paradigm (Lowe
et al., 2017), due to its light overhead in the deployment
phase and favorable performances. However, directly
learning such factorized policy remains challenging,
as each agent needs to estimate its state-action value
function, denoted as Qν(s, a). The search space during
learning is (|S| × |A|)N, scaling exponentially with
respect to the number of agents. The large search space
poses as a signiﬁcant roadblock for efﬁcient learning,
and is coined as the curse of many agents."
PROBLEM SETUP,0.1278538812785388,"To address the curse of many agents, we exploit the homogeneity of the system and take the mean-ﬁeld
approximation. We begin by taking the perspective of agent i, which is arbitrarily chosen from the
N agents. We denote its state as s and the states of the rest of the agents by sr. One can verify that
when permuting the state of all the other agents, the value function remains unchanged; additionally,
we can further characterize the value function as a function of the local state and the empirical state
distribution over the rest of agents.
Proposition 2.2. For any permutation mapping κ(·), the value function satisﬁes V ν(s, sr) =
V ν(s, κ(sr)). Additionally, there exists gν such that: V ν(s, sr) = gν(s, bpsr), where bpsr =
1
N
P"
PROBLEM SETUP,0.1324200913242009,s∈sr δs is the empirical distribution over the states of rest of the agents sr.
PROBLEM SETUP,0.136986301369863,"For a system with a large number of agents (e.g., ﬁnancial markets, social networks), the empirical
state distribution can be seen as the concrete realization of the underlying population distribution
of the agents. Motivated from this observation and Proposition 2.2, we formulate the following
mean-ﬁeld MDP that can be seen as the limit of ﬁnite-agent MDP in the presence of inﬁnitely many
homogeneous agents.
Deﬁnition 2.1 (mean-ﬁeld MDP). The mean-ﬁeld MDP consists of elements of the following: state
(s, dS) ∈S × P(S); action a ∈A ⊆AS; reward r(s, dS, a); transition kernel P(s′, d′
S|s, dS, a)."
PROBLEM SETUP,0.1415525114155251,"The mean-ﬁeld MDP has an intimate connection with our previously discussed ﬁnite-agent MDP.
Since the the agents are homogeneous, the system is the same from any agent’s perspective. We
choose any agent (referred to as representative agent), the state information of such an agent includes
the local state s, and the mean-ﬁeld state dS. With state information, the agent selects a meta action
a ∈A ∈AS, and uses such a meta action to make local decision a = a(s) ∈A. We remark that
such a modeling of decision process allows the agent to make decision on both its local information
(local state s) and the global information (mean-ﬁeld state dS). From homogeneity we assume all the
rest of the agents uses the same meta action a to make their local actions. Note that different agents
can still make different local actions due to their different local states, i.e., a(z) ̸= a(z′) in general for"
PROBLEM SETUP,0.1461187214611872,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.1506849315068493,"z ̸= z′ ∈S. The joint state at the next timestep (s′, d′
S) naturally depends on the current global state
(s, dS) and the meta action a (since all the other agents use a to generate their local actions), and is
speciﬁed by the transition kernel P(s′, d′
S|s, dS, a). In addition, the representative agents receives a
reward r(s, dS, a), which depends on the local state and mean-ﬁeld sate, and the meta action a."
PROBLEM SETUP,0.1552511415525114,"Our goal is to learn efﬁciently a policy π, where π(·|s, dS) ∈∆|A| for all (s, dS) ∈S × P(S),
for maximized expected discounted reward. To facilitate discussions, we deﬁne the value function
V π(s, dS) = (1−γ)E {P∞
t=0 γtr(st, dS,t, at)} , where (s0, dS,0) = (s, dS), at ∼π(st, dS,t), ∀t ≥
0; and Q-function Qπ(s, dS, a) = (1 −γ)E {P∞
t=0 γtr(st, dS,t, at)} , where (s0, dS,0) =
(s, dS), a0 = a, at ∼π(st, dS,t). The optimal policy is denoted by π∗∈argmax V π(s, dS)."
PROBLEM SETUP,0.1598173515981735,"Despite the intuitive analogy to ﬁnite-agent MDP, solving the mean-ﬁeld MDP poses some unique
challenges. In addition to having an unknown transition kernel and reward, the mean-ﬁeld MDP
takes a distribution as its state, which we do not have complete information of during training. In
the following section, we propose our mean-ﬁeld Neural Proximal Policy Optimization (MF-PPO)
algorithm that, with a careful architecture design, can solve such mean-ﬁeld MDP in a model-free
fashion efﬁciently.
3
MEAN-FIELD PROXIMAL POLICY OPTIMIZATION
Our algorithm falls into the category of the actor-critic learning paradigm, consisting of alternating
iterations of policy evaluation and improvement. The unique features of MF-PPO lie in the facts: (1)
it exploits permutation invariance of the system, reducing search space of the actor/critic networks
drastically and enables much more efﬁcient learning; (2) it can handle a varying number of agents.
For simplicity of exposition, we consider a ﬁxed number of agents here."
PROBLEM SETUP,0.1643835616438356,"Throughout the rest of the section, we assume that for any joint state (s, dS) ∈S × P(S), the
agent has access to N i.i.d. samples {si}N
i=1 from dS. We denote concatenation of such samples
as s ∈SN and write s
i.i.d.
∼
dS. MF-PPO maintains a pair of actor (denoted by F A) and critic
networks (denoted by F Q), and uses the actor network to induce an energy-based policy π(a|s, dS).
Speciﬁcally, given state (s, dS), the actor network induces a distribution on set A according to
π(a|s, dS) ∝exp

τ −1F A(s, dS, a)
	
, where τ denotes the temperature parameter. We use π ∝
exp

F A	
to denote the dependency of the policy on the energy function."
PROBLEM SETUP,0.1689497716894977,"• Permutation-invariant Actor and Critic. We adopt a permutation invariant design of the actor
and critic network. Speciﬁcally, given individual state s ∈S and sampled states s ∈SN, the actor
(resp. critic) network F A (resp. F Q) satisﬁes F A(s, s, a) = F A(s, κ(s), a) for any permutation
mapping κ. With permutation invariance, the search space of the actor/critic network polynomially
depends on the number of agents N.
Proposition 3.1. The search space of a permutation invariance actor (critic) network is at the order
of
Pmin{|S|,N}
k=1
 N−1
k−1
 |S|
k

|S||A|; Additionally, if |S| < N, then the search space depends on N"
PROBLEM SETUP,0.1735159817351598,"at the order of N |S|.
Compared to architectures without permutation invariance, whose search space depends on N at
the order of (|S||A|)N, we can clearly see the search space of MF-PPO is exponentially smaller."
PROBLEM SETUP,0.1780821917808219,"Agent 1
Agent 2
Agent N −1
Agent N"
PROBLEM SETUP,0.182648401826484,(Output)
PROBLEM SETUP,0.1872146118721461,(Summary Feature)
PROBLEM SETUP,0.1917808219178082,"s1
s2
sN−1
sN N
X i=1"
PROBLEM SETUP,0.1963470319634703,"φ(s, si, a) h( N
X i=1"
PROBLEM SETUP,0.2009132420091324,"φ(s, si, a)/N)"
PROBLEM SETUP,0.2054794520547945,"Figure 2: Illustration of a DeepSet param-
eterized critic network."
PROBLEM SETUP,0.2100456621004566,"Motivated by the characterization of the permuta-
tion invariant set function in Zaheer et al. (2017),
the actor/critic network in MF-PPO takes the form
of Deep Sets architecture,
i.e.,
F A(s, s, a)
=
h
 P"
PROBLEM SETUP,0.2146118721461187,"s′∈s φ(s, s′, a)/N

. Both networks ﬁrst aggregate
local information by averaging over the output of a
shared sub-network among agents, before feeding the
aggregated information into a subsequent network h(·).
See Figure 2 for detailed illustration. Effectively, by
the average pooling layer and the preceding parameter
sharing scheme, the network can keep its output unchanged when permuting the ordering of agents.
Compared to a Graph Convolutional Neural Network (Kipf and Welling, 2017), which uses two sets
of weights for the linear transformation layer, one for the the agent itself and one for the aggregation
state coming from the rest of the agents. The averaging operation is well suited for homogeneous
agents and more parameter-efﬁcient. It also naturally allows us to handle varying number of agents
during training and evaluation, which is not readily achievable by GCN network."
PROBLEM SETUP,0.2191780821917808,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.2237442922374429,"Naturally, the actor network when given the joint state-action pair (s, dS, a) is given by
F A(s, dS, a) = h (Es′∼dSφ(s, s′, a)) . We assume F A is parameterized by a neural network with
parameters α ∈RD, which is to be learned during training. We let the function class of all possible
actor networks be denoted by FA. This same architecture design applies to the critic network, with
learnable parameters denoted by θ ∈RD and the function class denoted by FQ. MF-PPO then
consists of successive iterations of policy evaluation and policy improvement described below."
PROBLEM SETUP,0.228310502283105,"• Policy Evaluation. At the k-th iteration of MF-PPO, we ﬁrst update the critic network F Q by
minimizing the mean squared Bellman error while holding the actor network F A,k ﬁxed. We denote
the policy induced by the actor network as πk, the stationary state distribution of policy πk as νk, and
the stationary state-action distribution as σk(s, dS, a) := νk(s, dS)πk(a|s, dS). Thus, we follow the
update"
PROBLEM SETUP,0.2328767123287671,"θk = argminθ∈B(θ0,Rθ) Eσk

F Q
θ (s, dS, a) −
h
T πkF Q
θ
i
(s, dS, a)
	2,
(3.1)"
PROBLEM SETUP,0.2374429223744292,"where B(θ0, Rθ) denotes the Euclidean ball with radius Rθ centered at the initialized pa-
rameter θ0,
and the Bellman evaluation operator T π is given by T πFQ
θ (s, dS, a)
="
PROBLEM SETUP,0.2420091324200913,"E
n
(1 −γ)r(s, dS, a) + γFQ
θ (s′, d′
S, a′)
o
, where (s′, d′
S) ∼P(·|s, dS, a), a′ ∼π(·|s′, d′
S). We"
PROBLEM SETUP,0.2465753424657534,"solve (3.1) by T-step temporal-difference (TD) update and output θk = θ(T). At the t-th iteration of
the TD update,"
PROBLEM SETUP,0.2511415525114155,"θ(t + 1/2) = θ(t) −η

F Q
θ(t)(s, s, a) −(1 −γ)r(s, dS, a) −γF Q
θ(t)(s′, s′, a′)
	
∇θF Q
θ(t)(s, s, a),"
PROBLEM SETUP,0.2557077625570776,"θ(t + 1) = ΠB(θ0,Rθ) (θ(t + 1/2)) ,"
PROBLEM SETUP,0.2602739726027397,"where we sample (s, dS, a) ∼σk, (s′, d′
S) ∼P(·|s, dS, a), a′ ∼πk(·|s′, d′
S), s
i.i.d.
∼dS, s′ i.i.d.
∼
d′
S. We use ΠX(·) to denote the orthogonal projection onto set X, and η to denote the step size. Note
that here for the simplicity of analyses, we sample the state-action pair (s, dS, a) independently from
the stationary distribution. We remark that trajectory samples can also be used, which essentially
requires bounding the bias of the gradient at each iteration due to the dependencies between trajectory
samples, and we can readily apply the fast-mixing property of Markov chains to control such a bias
(Bhandari et al., 2018). The details of policy evaluation are summarized in Algorithm 2, Appendix A."
PROBLEM SETUP,0.2648401826484018,"• Policy Improvement. Following the policy evaluation step, MF-PPO updates its policy by updating
the policy network F A, which is the energy function associated with the policy. We update the policy
network by"
PROBLEM SETUP,0.2694063926940639,"πk+1 =
argmax
π∝exp{F A,k+1},"
PROBLEM SETUP,0.273972602739726,"F A,k+1∈FA"
PROBLEM SETUP,0.2785388127853881,"Eνk
 D
F Q
θk(s, dS, ·), π(·|s, dS)
E
−υkKL
 
π(·|s, dS)
πk(·|s, dS)
 	
."
PROBLEM SETUP,0.2831050228310502,"The update rule intuitively reads as increasing the probability for choosing action a if it yields a
higher value for critic network F Q(s, dS, a), which can be viewed as a softened version of policy
iteration (Bertsekas, 2011). Additionally, by controlling the proximity parameter υk, we can control
the softness of the update, with υ →0 yielding the vanilla policy iteration. Moreover, without
constraint of F A,k+1 ∈FA, such an update would have a nice closed form expression, and πk+1
itself is another energy-based policy."
PROBLEM SETUP,0.2876712328767123,"Proposition 3.2. Let πk ∝exp
 
τ −1
k F A
αk

denote the energy-based policy, then the update"
PROBLEM SETUP,0.2922374429223744,"πk+1 = argmaxπ Eνk
 D
F Q
θk(s, dS, ·), π(·|s, dS)
E
−υkKL
 
π(·|s, dS)
πk(·|s, dS)
"
PROBLEM SETUP,0.2968036529680365,"yields πk+1 ∝exp{υ−1
k F Q
θk + τ −1
k F A
αk}."
PROBLEM SETUP,0.3013698630136986,"To take into account that the representable function of actor network resides in FA, we update the
policy by projecting the energy function of πk+1 back to FA. Speciﬁcally, by denoting πk+1 ∝
exp{τ −1
k+1F A
αk+1}, we recover the next actor network F A
αk+1 (i.e., energy) by performing the following
regression task"
PROBLEM SETUP,0.3059360730593607,"α∗
k+1 =
argmin
α∈B(Rα,α0)
Eeσk

F A
α (s, dS, a) −τk+1
h
υ−1
k F Q
θk(s, dS, a) + τ −1
k F A
αk(s, dS, a)
i 	2, (3.2)"
PROBLEM SETUP,0.3105022831050228,Under review as a conference paper at ICLR 2022
PROBLEM SETUP,0.3150684931506849,"where eσk = νkπ0. We approximately solve (3.2) via T-step stochastic gradient descent (SGD), and
output αk+1 = α(T) ≈α∗
k+1. At the t-th iteration of SGD,"
PROBLEM SETUP,0.319634703196347,"α(t + 1/2) = α(t) −η∇αF A
α(t)(s, s, a)

F A
α(t)(s, s, a) −τk+1

υ−1
k F Q
θk(s, s, a) + τ −1
k F A
αk(s, s, a)
 	
,"
PROBLEM SETUP,0.3242009132420091,"α(t + 1) = ΠB(Rα,α0) (α(t + 1/2)) ,"
PROBLEM SETUP,0.3287671232876712,"where we sample (s, dS, a) ∼eσk, and s
i.i.d.
∼dS, and η is the step size. The details are summarized
in Algorithm 3 of Appendix A. Finally, we present the complete MF-PPO in Algorithm 1."
PROBLEM SETUP,0.3333333333333333,Algorithm 1 Mean-Field Neural Proximal Policy Optimization
PROBLEM SETUP,0.3378995433789954,"Require: Mean-ﬁeld MDP (S×, P(S), A, P, r), discount factor γ; number of outer iterations K,
number of inner updates T; policy update parameter υ, step size η, projection radius Rα, Rθ.
Initialize: τ0 ←1, F A,0 ←0, π0 ∝exp{τ −1
0 F A,0} (uniform policy).
for k = 0, . . . , K −1 do"
PROBLEM SETUP,0.3424657534246575,"Set temperature parameter τk+1 ←υ
√"
PROBLEM SETUP,0.3470319634703196,"K/(k + 1), and proximity parameter υk ←υ
√"
PROBLEM SETUP,0.3515981735159817,"K
Solve (3.1) to update the critic network F Q
θk, using TD update (Algorithm 2)
Solve (3.2) to update the actor network for F A
αk+1, using SGD update (Algorithm 3)
Update policy: πk+1 ∝exp{τ −1
k+1F A
αk+1}
end for"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3561643835616438,"4
GLOBAL CONVERGENCE OF MF-PPO
We present the global convergence of MF-PPO algorithm for the two-layer permutation-invariant
parameterization of the actor and critic networks. We remark that our analysis can be extended to
multi-layer permutation-invariant networks, and we present the two-layer case here for simplicity of
exposition. Speciﬁcally, the actor and critic networks take the form"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3607305936073059,"F A
α (s, s, a) =
1
√mN mA
X j=1 X"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.365296803652968,"s′∈s
ujσ
 
α⊤
j (s, s′, a)

, F Q
θ (s, s, a) =
1
√mN mQ
X j=1 X"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3698630136986301,"s′∈s
vjσ
 
θ⊤
j (s, s′, a)

,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3744292237442922,"where mA (resp. mQ) is the width of the actor (resp. critic) network, and σ(x) = max{x.0}
denotes the ReLU activation. We randomly initialize uj (resp. vj) and ﬁrst layer weights α0 =
[α⊤
0,1, . . . , α⊤
0,mA]⊤∈Rd·mA (resp. θ0 ∈Rd·mQ) by"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3789954337899543,"uj ∼Unif {−1, +1} , α0,j ∼N(0, Id/d), ∀j ∈[m].
For ease of analysis, we take m = mA = mQ and share the initialization of α0 and θ0 (resp. u0 and
v0). Additionally, we keep uj’s ﬁxed during training, and α (resp. θ) within ball B(α0, RA) (resp.
B(θ0, RQ)) throughout training. We deﬁne the following function class which approximates the class
of previously deﬁned actor/critic network for large network width.
Deﬁnition 4.1. Given Rβ > 0, deﬁne the function class over domain S × S × A by"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3835616438356164,"Fβ,m =

fβ(·)
 fβ(s, s′, a)=
1
√m m
X"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3881278538812785,"j=1
vj1{β⊤
0,j(s, s′, a)>0}β⊤
j (s, s′, a), ∥β −β0∥2 ≤Rβ
	
,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3926940639269406,"where vj, β0,j are random weights sampled according to
vj ∼Unif {−1, +1} , β0,j ∼N(0, Id/d), ∀j ∈[m].
Fβ,m also induces the function class over S × P(S) × A given by"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.3972602739726027,"FP
β,m =

F(·)
 F(s, dS, a)=Es′∼dSfβ(s, s′, a), fβ ∈Fβ,m
	
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4018264840182648,"It is well known that functions within Fβ,m approximate functions within the reproducing kernel
Hilbert space associated with kernel K(x, y) = Ez∼N(0,Id/d)

1(z⊤x > 0, z⊤y > 0)
	
for a large
network width m (Jacot et al., 2018; Chizat and Bach, 2018; Cai et al., 2019; Arora et al., 2019) and
whose RKHS norm is bounded by Rβ. For large Rβ and m, Fβ,m represents a rich class of functions.
Additionally, functions within FP
β,m can be viewed as the mean-embedding of the joint state-action
pair onto the RKHS space (Muandet et al., 2016; Song et al., 2009; Smola et al., 2007). Below, we
make one important assumption, which assumes that FP
β,m is rich enough to represent the Q-function
of all the policies within our policy class."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4063926940639269,Under review as a conference paper at ICLR 2022
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.410958904109589,"Assumption 1. For any policy π induced by FA ∈FA, we have Qπ ∈FP
θ,mQ."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4155251141552511,"We remark that Assumption 1 can be relaxed into requiring that FP
β,m has ϵ approximation error
when parameterizing the set of Q-functions, with an additional ϵ term appearing in the convergence
bound developed in Theorem 4.1 (Lan, 2021)."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4200913242009132,"We deﬁne mild conditions stating boundedness of reward, and regularity of stationary distributions.
Assumption 2. Reward function r(·) ≤r for some r > 0. Additionally, there exists c > 0 such that
E

1
 
|z⊤(s, s′, a)| ≤t
	
≤c ·
t
∥z∥2 for any z ∈Rd and t > 0."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4246575342465753,We measure the progress of MF-PPO in Algorithm 1 using the expected total reward
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4292237442922374,"L(π) = Eν∗[V π(s, dS)] = Eν∗{⟨Qπ(·|s, dS), π(·|s, dS)⟩} ,
(4.1)
where ν∗is the stationary state distribution of the optimal policy π∗. We also denote σ∗as the
stationary state-action distribution induced by π∗. Note that we have: L(π∗) = Eν∗
V π∗(s, dS)

≥
Eν∗[V π(s, dS)] = L(π), for any policy π. Our main results are presented in the following theorem,
showing that L(πk) converges to L(π∗) at a sub-linear rate.
Theorem 4.1 (Global Convergence of MF-PPO). Under Assumptions 1 and 2, the policies {πk}K
k=1
generated by Algorithm 1 satisify"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4337899543378995,"min0≤k≤K{L(π∗) −L(πk)} ≤
υ(log |A|+PK−1
k=1 (εk+ε′
k))
(1−γ)
√"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4383561643835616,"K
+
M
(1−γ)υ
√ K ,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4429223744292237,"where M = Eν∗
n
maxa∈A(F Q
θ0(s, s′, a))2o
+ 2R2
A, εk and ε′
k are deﬁned in Lemma 4.3. In
particular, suppose at each iteration of MF-PPO, we observe N = Ω
 
K3R4
A + KR4
Q

agents,
and the actor/critic network satisﬁes mA = Ω
 
K6R10
A + K4R10
A |A|2
, mQ = Ω
 
K2R10
Q

, and
T = Ω
 
K3R4
A + KR4
Q

, then we have"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4474885844748858,"min0≤k≤K{L(π∗) −L(πk)} ≤
υ2(log |A|+O(1))+M"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4520547945205479,"(1−γ)υ
√ K
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.45662100456621,"Theorem 4.1 states that, given sufﬁciently many agents and a large enough actor/critic network,
MF-PPO attains global optimality at a sublinear rate. Our result shows that when solving the
mean-ﬁeld MDP, having more agents serves as a blessing instead of a curse. In addition, as will be
demonstrated in our proof sketch, there exists an inherent phase transition, where the ﬁnal optimality
gap is dominated by statistical error for a small number of agents (ﬁrst phase); and by optimization
error for a large number of agents (second phase)."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4611872146118721,"The complete proof of Theorem 4.1 takes careful analysis on the error from policy evaluation (3.1)
and the improvement step (3.2). The analysis on the outer iterations of MF-PPO can be overviewed
as approximate mirror descent, which needs to take into account how the evaluation and improvement
error interacts. Intuitively, the tuple (εk, ε′
k) describes the the effect of policy update when using
approximate policy evaluation and policy improvement, and will be further clariﬁed in the ensuing
discussion. We present here the skeleton of our proof, and defer the technical detail to the appendix."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4657534246575342,"Proof Sketch. We ﬁrst establish the convergence of the policy evaluation and improvement step.
Lemma 4.1 (Policy Evaluation). Under the same assumptions in Theorem 4.1, let Qπk denote"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4703196347031963,"the Q-function of policy πk, let ϵk = Einit,σk

F Q"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4748858447488584,"θ(T )(·) −Qπk(·)
2
denote policy evaluation error,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4794520547945205,"where θ(T) is the output of Algorithm 2, we have ϵk = O

R2
Q
T 1/2 +
R5/2
Q
m1/4
Q
+
R2
Q
N1/2 +
R3
Q
m1/2
Q 
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4840182648401826,"Lemma 4.2 (Policy Improvement). Under the same assumptions in Theorem 4.1, let ϵ′
k+1 ="
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4885844748858447,"Einit,eσk
n
F A"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4931506849315068,"α(T )(·)−τk+1

υ−1
k F Q
θk(·)+τ −1
k F A
αk(·)
o2
denote policy optimization error, where α(T)"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.4977168949771689,"is the output of Algorithm 3, we have ϵ′
k+1 = O

R2
A
T 1/2 + R5/2
A
m1/4
A
+
R2
A
N1/2 +
R3
A
m1/2
A 
."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.502283105022831,"Lemma 4.1 and 4.2 show that despite non-convexity, both policy evaluation and policy improvement
steps converge approximately to the global optimal solution. In particular, for both policy evaluation
steps and improvement steps, given networks with large width, for a small number of iterations T,
the optimization error O
 
T −1/2
dominates the optimality gap; for a large number of iterations T,
the statistical error O
 
N −1/2
dominates the optimality gap."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5068493150684932,Under review as a conference paper at ICLR 2022
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5114155251141552,"With Lemma 4.1 and 4.2, we illustrate the main argument for the proof of Theorem 4.1. Let us
assume the ideal case when ϵk = ϵ′
k+1 = 0. Note that for ϵk = 0, we obtain the exact Q-function of
policy πk. For ϵ′
k+1 = 0, we obtain the ideal energy-based updated policy deﬁne in Proposition 3.2.
That is,"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5159817351598174,"πk+1 = argmaxπ Eνk

⟨Qπk(s, dS, ·), π(·|s, dS)⟩υkKL (π(·|s, dS)∥πk(·|s, dS))
	
.
(4.2)"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5205479452054794,"Without function approximation, problem (4.2) can be solved by treating each joint state (s, dS)
independently, hence one can apply the well known three-point lemma in mirror descent (Chen and
Teboulle, 1993) and obtain that, for all (s, dS) ∈S × P(S):"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5251141552511416,"⟨Qπk(s, dS, ·), π∗(·|s, dS) −πk(·|s, dS)⟩"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5296803652968036,"≤υk

KL (π∗(·|s, dS)∥πk(·|s, dS)) −KL (π∗(·|s, dS)∥πk+1(·|s, dS)) −KL (πk+1(·|s, dS)∥πk(·|s, dS))"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5342465753424658,"+ ⟨Qπk(s, dS, ·), πk+1(·|s, dS) −πk(·|s, dS)⟩."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5388127853881278,"From Lemma 6.1 in Kakade and Langford (2002), the expectation of the left hand side yields exactly
(1 −γ) {L(π∗) −L(πk)}. Hence we have"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.54337899543379,(1 −γ) {L(π∗) −L(πk)}
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.547945205479452,"≤υkEν∗
KL (π∗(·|s, dS)∥πk(·|s, dS)) −KL (π∗(·|s, dS)∥πk+1(·|s, dS))"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5525114155251142,"−KL (πk+1(·|s, dS)∥πk(·|s, dS))
	
+ Eν∗⟨Qπk(s, dS, ·), πk+1(·|s, dS) −πk(·|s, dS)⟩."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5570776255707762,"Pinsker’s Inequality KL (πk+1(·|s, dS)∥πk(·|s, dS)) ≥1"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5616438356164384,"2∥πk+1 −πk∥2
1, combined with observation
∥Qπk(s, dS, ·)∥∞≤r/(1 −γ), and basic inequality −ax2 + bx ≤b2/(4a) for a > 0 gives us"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5662100456621004,(1 −γ) {L(π∗) −L(πk)}
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5707762557077626,"≤υkEν∗
KL (π∗(·|s, dS)∥πk(·|s, dS)) −KL (π∗(·|s, dS)∥πk+1(·|s, dS))
	
+
r2"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5753424657534246,2υk(1 −γ)2 .
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5799086757990868,"By setting υk = O(
√"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5844748858447488,"K), and telescoping the above inequality from k = 0 to K −1, we obtain:
min0≤k≤K−1{L(π∗) −L(πk)} = O(1/
√"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.589041095890411,"K). Note that the key element in the global convergence
of MF-PPO is the recursion deﬁned in the previous inequality, which holds whenever we have an
exact Q-function of the current policy and no function approximation is used when updating the
next policy. Now MF-PPO conducts approximate policy evaluation ϵk > 0, and after obtaining the
approximate Q-function, conducts approximate policy improvement step ϵ′
k+1 > 0 with function
approximation. In addition, the error of approximating the Q-function introduced in the evaluation
step can be further compounded in the improvement step. Nevertheless, the previous inequality still
holds approximately, with additional terms representing the policy evaluation/improvement errors.
Lemma 4.3 (Liu et al. (2019a)). Let ϵk (evaluation error) and ϵ′
k+1 (improvement error) be deﬁned
as in Lemma 4.1 and Lemma 4.2, respectively. We have:
(1 −γ) (L(π∗) −L(πk)) ≤υkEν∗

KL (π∗(·|s, dS)∥πk(·|s, dS)) −KL (π∗(·|s, dS)∥πk+1(·|s, dS))"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.593607305936073,"+ υk (εk + ε′
k) + υ−1
k M.
(4.3) where"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.5981735159817352,"εk = τ −1
k+1ϵ′
k+1φ∗
k+1 + υ−1
k ϵkψ∗
k, ε′
k = |A|τ −2
k+1(ϵ′
k+1)2, M = Eν∗

max a∈A"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6027397260273972,"h
F Q
θ0(s, dS, a)
i2
+ 2R2
A."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6073059360730594,"In addition, φ∗
k and ψ∗
k are deﬁned by:"
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6118721461187214,"φ∗
k = Eeσk

|dπ∗/dπ0 −dπk/dπ0|21/2 ,
ψ∗
k = Eσk

|dσ∗/dσk −d(ν∗× πk)/dσk|21/2 ."
"GLOBAL CONVERGENCE OF MF-PPO
WE PRESENT THE GLOBAL CONVERGENCE OF MF-PPO ALGORITHM FOR THE TWO-LAYER PERMUTATION-INVARIANT",0.6164383561643836,"Finally, by telescoping inequality (4.3) from k = 0 to K −1, we complete the proof of Theorem 4.1."
EXPERIMENTS,0.6210045662100456,"5
EXPERIMENTS
We perform experiments on the benchmark multi-agent particle environment (MPE) used in prior work
(Lowe et al., 2017; Mordatch and Abbeel, 2018; Liu et al., 2019b). In the cooperative navigation task,
N agents each with position xi ∈R2 must move to cover N ﬁxed landmarks at positions yi ∈R2.
They receive a team reward R = −PN
i=1 minj∈[N]∥yi −xj∥2; In the cooperative push task, N
agents with position xi ∈R2 work together to push a ball x ∈R2 to a ﬁxed landmark y ∈R2. They
receive a team reward R = −minj∈[N]∥xj −x∥2 −∥x −y∥2. Both tasks involve homogeneous"
EXPERIMENTS,0.6255707762557078,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.6301369863013698,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Steps
1e6 4500 4000 3500 3000 2500 2000 1500"
EXPERIMENTS,0.634703196347032,Average episode reward
EXPERIMENTS,0.639269406392694,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.6438356164383562,"(a) N = 15
(b) N = 30"
EXPERIMENTS,0.6484018264840182,"0
1
2
3
4
5
6
7
Steps
1e5"
EXPERIMENTS,0.6529680365296804,600000
EXPERIMENTS,0.6575342465753424,550000
EXPERIMENTS,0.6621004566210046,500000
EXPERIMENTS,0.6666666666666666,450000
EXPERIMENTS,0.6712328767123288,400000
EXPERIMENTS,0.6757990867579908,350000
EXPERIMENTS,0.680365296803653,300000
EXPERIMENTS,0.684931506849315,Average episode reward
EXPERIMENTS,0.6894977168949772,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.6940639269406392,(c) N = 200
EXPERIMENTS,0.6986301369863014,"0.5
1.0
1.5
2.0
2.5
Steps
1e5 6.00 5.75 5.50 5.25 5.00 4.75 4.50 4.25 4.00"
EXPERIMENTS,0.7031963470319634,Average episode reward 1e6
EXPERIMENTS,0.7077625570776256,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.7123287671232876,"(d) N = 500
Figure 3: Performance versus number of environment steps in the multi-agent cooperative navigation
task, over ﬁve independent runs per method. Points are taken every 1000 training episodes, with
the ﬁrst point taken after the ﬁrst 1000, and is the average reward of 1000 evaluation episodes. MF
signiﬁcantly outperforms other critic representations for various number of agents."
EXPERIMENTS,0.7168949771689498,"agents, and all the agents share the same team reward. Note that MPE environment also models
interaction between agents, including collision, and the collided agents receive negative rewards."
EXPERIMENTS,0.7214611872146118,"We instantiate our method MF, by parameterizing the centralized critic function using a DeepSet
(Zaheer et al., 2017) network, with two hidden layers. We use a standard two-layer multi-layer
perception (MLP) for the centralized actor network in all algorithms. The actor network outputs
the mean and diagonal covariance of a Gaussian distribution over the joint action space. We refer
interested readers to Appendix B for detailed conﬁgurations of hyperparameters."
EXPERIMENTS,0.726027397260274,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Steps
1e6 4500 4000 3500 3000 2500 2000 1500"
EXPERIMENTS,0.730593607305936,Average episode reward
EXPERIMENTS,0.7351598173515982,"MF
GCN
MLP
MF-A
VDN"
EXPERIMENTS,0.7397260273972602,(a) N = 15
EXPERIMENTS,0.7442922374429224,"0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Steps
1e6 450 400 350 300 250 200 150"
EXPERIMENTS,0.7488584474885844,Average episode reward
EXPERIMENTS,0.7534246575342466,"MF 2k
MF 1k
MF 500"
EXPERIMENTS,0.7579908675799086,"GCN 2k
GCN 1k
GCN 500"
EXPERIMENTS,0.7625570776255708,(b) Varying critic size
EXPERIMENTS,0.7671232876712328,"Figure 4: (a) Performance versus number of environ-
ment steps in the multi-agent cooperative push task. (b)
MF outperforms GCN even with a fewer number of
critic network parameters (N = 3)."
EXPERIMENTS,0.771689497716895,"We compare with two other critic repre-
sentations: one that uses MLP for the cen-
tralized critic, labeled MLP, and another
that uses a graph convolutional network for
the critic (Liu et al., 2019b), labeled GCN.
Note that the GCN representation is per-
mutation invariant if one imposes a fully-
connected graph for the agents in the MPE,
but this invariance property does not hold
for all graphs in general. We also compare
with an extension of (Yang et al., 2018) to
the case of continuous action spaces, la-
beled MF-A, in which each independent
DDPG agent i has a decentralized critic Q(si, ai, ¯ai) that takes in the mean of all other agents’ actions
¯ai :=
1
N−1
P"
EXPERIMENTS,0.776255707762557,"j̸=i aj. Finally, we include comparison with VDN (Sunehag et al., 2017), where the
centralized critic network is the direct summation of local critic networks and thus being permutation
invariant. Empirically, as we ﬁnd that off-policy RL learns faster than on-policy RL in the MPE
with higher agent number, regardless of the critic representation, we make all comparisons on top of
MADDPG (Lowe et al., 2017). For a fair comparison of all critic representations, we ensure that all
neural network architectures contain approximately the same number of trainable weights."
EXPERIMENTS,0.7808219178082192,"For the cooperative navigation task, Figure 3 shows that the permutation invariant critic representation
based on DeepSet enables MF to learn faster or reach a higher performance than all other representa-
tions and methods in the MPE with 15, 200, and 500 agents. For the cooperative push task, Figure 4a
demonstrates a similar performance improvement provided by MF. In addition, we also demonstrate
the superior parameter efﬁciency of MF compared to GCN. Figure 4b shows that MF consistently
and signiﬁcantly outperforms GCN as the number of parameters in their critic network varies over a
range, with all other settings ﬁxed. In particular, MF requires much fewer critic parameters to achieve
higher performance than GCN."
EXPERIMENTS,0.7853881278538812,"Computational Improvements. Theorem 4.1 states that to obtain a small optimality gap in MF-
PPO, one needs to compute the update on a large number of agents. We remark that with the dual
embedding techniques developed in Dai et al. (2017), one can avoid computation on all the agents
by sampling a small number of agents to compute the update. This technique could be readily
incorporated into MF-PPO to improve its computational efﬁciency."
EXPERIMENTS,0.7899543378995434,"Conclusion. We propose a principled approach to exploit agent homogeneity and permutation
invariance through the mean-ﬁeld approximation in MARL. Our results are also the ﬁrst to show
the global convergence of MARL algorithms with neural networks as function approximators. This
is in sharp contrast to current practices, which are mostly heuristic methods without convergence
guarantees."
EXPERIMENTS,0.7945205479452054,Under review as a conference paper at ICLR 2022
REFERENCES,0.7990867579908676,REFERENCES
REFERENCES,0.8036529680365296,"ARORA, S., DU, S. S., HU, W., LI, Z. and WANG, R. (2019). Fine-grained analysis of opti-
mization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584 ."
REFERENCES,0.8082191780821918,"BERTSEKAS, D. P. (2011). Approximate policy iteration: A survey and some new methods. Journal
of Control Theory and Applications 9 310–335."
REFERENCES,0.8127853881278538,"BHANDARI, J., RUSSO, D. and SINGAL, R. (2018). A ﬁnite time analysis of temporal difference
learning with linear function approximation. In Conference On Learning Theory. PMLR."
REFERENCES,0.817351598173516,"BLOEM-REDDY, B. and TEH, Y. W. (2019). Probabilistic symmetry and invariant neural networks.
arXiv preprint arXiv:1901.06082 ."
REFERENCES,0.821917808219178,"CAI, Q., YANG, Z., LEE, J. D. and WANG, Z. (2019). Neural temporal-difference learning converges
to global optima. In Advances in Neural Information Processing Systems."
REFERENCES,0.8264840182648402,"CAO, Y., YU, W., REN, W. and CHEN, G. (2013). An overview of recent progress in the study of
distributed multi-agent coordination. IEEE Transactions on Industrial informatics 9 427–438."
REFERENCES,0.8310502283105022,"CARMONA, R., LAURIÈRE, M. and TAN, Z. (2019). Model-free mean-ﬁeld reinforcement learning:
mean-ﬁeld mdp and mean-ﬁeld q-learning. arXiv preprint arXiv:1910.12802 ."
REFERENCES,0.8356164383561644,"CHEN, G. and TEBOULLE, M. (1993). Convergence analysis of a proximal-like minimization
algorithm using bregman functions. SIAM Journal on Optimization 3 538–543."
REFERENCES,0.8401826484018264,"CHIZAT, L. and BACH, F. (2018). A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956 8."
REFERENCES,0.8447488584474886,"DAI, B., SHAW, A., LI, L., XIAO, L., HE, N., LIU, Z., CHEN, J. and SONG, L. (2017).
Sbeed: Convergent reinforcement learning with nonlinear function approximation. arXiv preprint
arXiv:1712.10285 ."
REFERENCES,0.8493150684931506,"GU, H., GUO, X., WEI, X. and XU, R. (2019). Dynamic programming principles for learning mfcs.
arXiv preprint arXiv:1911.07314 ."
REFERENCES,0.8538812785388128,"GU, H., GUO, X., WEI, X. and XU, R. (2020). Mean-ﬁeld controls with q-learning for cooperative
marl: Convergence and complexity analysis. arXiv preprint arXiv:2002.04131 ."
REFERENCES,0.8584474885844748,"GU, H., GUO, X., WEI, X. and XU, R. (2021). Mean-ﬁeld multi-agent reinforcement learning: A
decentralized network approach. arXiv preprint arXiv:2108.02731 ."
REFERENCES,0.863013698630137,"JACOT, A., GABRIEL, F. and HONGLER, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems."
REFERENCES,0.867579908675799,"KAKADE, S. and LANGFORD, J. (2002). Approximately optimal approximate reinforcement learning.
In ICML, vol. 2."
REFERENCES,0.8721461187214612,"KALYANAKRISHNAN, S., LIU, Y. and STONE, P. (2006). Half ﬁeld offense in robocup soccer: A
multiagent reinforcement learning case study. In Robot Soccer World Cup. Springer."
REFERENCES,0.8767123287671232,"KIPF, T. N. and WELLING, M. (2017). Semi-supervised classiﬁcation with graph convolutional
networks. In International Conference on Learning Representatioons."
REFERENCES,0.8812785388127854,"KOBER, J., BAGNELL, J. A. and PETERS, J. (2013). Reinforcement learning in robotics: A survey.
The International Journal of Robotics Research 32 1238–1274."
REFERENCES,0.8858447488584474,"LAN, G. (2021). Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. arXiv preprint arXiv:2102.00135 ."
REFERENCES,0.8904109589041096,"LITTMAN, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994. Elsevier, 157–163."
REFERENCES,0.8949771689497716,Under review as a conference paper at ICLR 2022
REFERENCES,0.8995433789954338,"LIU, B., CAI, Q., YANG, Z. and WANG, Z. (2019a). Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306 ."
REFERENCES,0.9041095890410958,"LIU, I.-J., YEH, R. A. and SCHWING, A. G. (2019b). Pic: Permutation invariant critic for
multi-agent deep reinforcement learning. arXiv preprint arXiv:1911.00025 ."
REFERENCES,0.908675799086758,"LOWE, R., WU, Y., TAMAR, A., HARB, J., ABBEEL, O. P. and MORDATCH, I. (2017). Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems."
REFERENCES,0.91324200913242,"MATARI ´C, M. J. (1997). Reinforcement learning in the multi-robot domain. In Robot colonies.
Springer, 73–83."
REFERENCES,0.9178082191780822,"MENDA, K., CHEN, Y.-C., GRANA, J., BONO, J. W., TRACEY, B. D., KOCHENDERFER, M. J.
and WOLPERT, D. (2018). Deep reinforcement learning for event-driven multi-agent decision
processes. IEEE Transactions on Intelligent Transportation Systems 20 1259–1268."
REFERENCES,0.9223744292237442,"MORDATCH, I. and ABBEEL, P. (2018). Emergence of grounded compositional language in multi-
agent populations. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence."
REFERENCES,0.9269406392694064,"MUANDET, K., FUKUMIZU, K., SRIPERUMBUDUR, B. and SCHÖLKOPF, B. (2016). Kernel mean
embedding of distributions: A review and beyond. arXiv preprint arXiv:1605.09522 ."
REFERENCES,0.9315068493150684,"PANAIT, L. and LUKE, S. (2005). Cooperative multi-agent learning: The state of the art. Autonomous
agents and multi-agent systems 11 387–434."
REFERENCES,0.9360730593607306,"SCHULMAN, J., WOLSKI, F., DHARIWAL, P., RADFORD, A. and KLIMOV, O. (2017). Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 ."
REFERENCES,0.9406392694063926,"SHALEV-SHWARTZ, S., SHAMMAH, S. and SHASHUA, A. (2016). Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295 ."
REFERENCES,0.9452054794520548,"SMOLA, A., GRETTON, A., SONG, L. and SCHÖLKOPF, B. (2007). A hilbert space embedding for
distributions. In International Conference on Algorithmic Learning Theory. Springer."
REFERENCES,0.9497716894977168,"SONG, L., HUANG, J., SMOLA, A. and FUKUMIZU, K. (2009). Hilbert space embeddings of
conditional distributions with applications to dynamical systems. In Proceedings of the 26th
Annual International Conference on Machine Learning."
REFERENCES,0.954337899543379,"SUNEHAG, P., LEVER, G., GRUSLYS, A., CZARNECKI, W. M., ZAMBALDI, V., JADERBERG, M.,
LANCTOT, M., SONNERAT, N., LEIBO, J. Z., TUYLS, K. ET AL. (2017). Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296 ."
REFERENCES,0.958904109589041,"SUTTON, R. S. and BARTO, A. G. (2018). Reinforcement learning: An introduction. MIT press."
REFERENCES,0.9634703196347032,"VINYALS, O., BABUSCHKIN, I., CZARNECKI, W. M., MATHIEU, M., DUDZIK, A., CHUNG, J.,
CHOI, D. H., POWELL, R., EWALDS, T., GEORGIEV, P. ET AL. (2019). Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature 575 350–354."
REFERENCES,0.9680365296803652,"WANG, L., YANG, Z. and WANG, Z. (2020). Breaking the curse of many agents: Provable mean
embedding q-iteration for mean-ﬁeld reinforcement learning. In International Conference on
Machine Learning. PMLR."
REFERENCES,0.9726027397260274,"YANG, J., YE, X., TRIVEDI, R., XU, H. and ZHA, H. (2017). Learning deep mean ﬁeld games for
modeling large population behavior. arXiv preprint arXiv:1711.03156 ."
REFERENCES,0.9771689497716894,"YANG, Y., LUO, R., LI, M., ZHOU, M., ZHANG, W. and WANG, J. (2018). Mean ﬁeld multi-agent
reinforcement learning. arXiv preprint arXiv:1802.05438 ."
REFERENCES,0.9817351598173516,"ZAHEER, M., KOTTUR, S., RAVANBAKHSH, S., POCZOS, B., SALAKHUTDINOV, R. R. and
SMOLA, A. J. (2017). Deep sets. In Advances in neural information processing systems."
REFERENCES,0.9863013698630136,Under review as a conference paper at ICLR 2022
REFERENCES,0.9908675799086758,"ZHANG, K., YANG, Z. and BA ¸SAR, T. (2019). Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635 ."
REFERENCES,0.9954337899543378,"ZHENG, S., TROTT, A., SRINIVASA, S., NAIK, N., GRUESBECK, M., PARKES, D. C. and SOCHER,
R. (2020). The ai economist: Improving equality and productivity with ai-driven tax policies.
arXiv preprint arXiv:2004.13332 ."
