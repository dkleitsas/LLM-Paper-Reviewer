Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0036101083032490976,"Natural language modeling with limited training data is challenging problem, and
many algorithms make use of large-scale pretrained language models (PLMs) for
this due to its great generalization ability. Among these transfer learning algo-
rithms from PLMs, additive learning that incorporates a task-speciﬁc adapter on
top of the ﬁxed PLM has been popularly used to alleviate the severe overﬁtting
problem in the few-shot setting. However, this added task-speciﬁc adapter is gen-
erally trained by maximum likelihood estimation that can easily suffer from the
so-called exposure bias problem, especially in sequential text generation. There-
fore, in this work, we develop a novel additive learning algorithm based on rein-
forcement learning (RL) for few-shot natural language generation (NLG) tasks. In
particular, we propose to use a selective token generation between the transformer-
based PLM and the task-speciﬁc adapter during both training and inference. This
output token selection between the two generators allows the adapter to take
into account only on the task-relevant parts in sequence generation, and there-
fore makes it more robust to overﬁtting as well as more stable in RL training. In
addition, in order to obtain the complementary adapter from the PLM for each
few-shot task, we exploit a separate selecting module that is also simultaneously
trained using RL. Experimental results on various few-shot NLG tasks including
data-to-text generation and text summarization demonstrate that the proposed se-
lective token generation signiﬁcantly outperforms the previous additive learning
algorithms based on the PLMs."
INTRODUCTION,0.007220216606498195,"1
INTRODUCTION"
INTRODUCTION,0.010830324909747292,"Natural language processing (NLP) have recently achieved great progress using advanced neural
language models (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019;
Clark et al., 2020; Raffel et al., 2019; Lan et al., 2019; Lewis et al., 2020). However, these neural
models typically require large-scale training data for each individual task, and solving a new NLP
task that has only a few examples is still challenging problem (Yin, 2020). Especially, natural
language generation (NLG) with limited training data is an important yet more difﬁcult task due
to its fast adaptation of sequential prediction models in a wide range of applications including text
summarization, question answering, data-to-text generation, machine translation, etc (Peng et al.,
2020; Chen et al., 2020; Xu et al., 2021; Schick & Sch¨utze, 2020; Chang et al., 2021; Radford et al.,
2019; Lewis et al., 2020; Brown et al., 2020)."
INTRODUCTION,0.01444043321299639,"More recently, pretrained language models (PLMs) have shown great generalization ability when
combined with large-scale data and big transformer-based models (Devlin et al., 2019; Radford et al.,
2019; Lewis et al., 2020; Brown et al., 2020; Subramanyam Kalyan et al., 2021). Therefore, transfer
learning from transformer PLMs has been popularly used for few-shot NLG tasks with promising
results. In speciﬁc, the use of PLM for few-shot NLG can be categorized into three approaches:
1) prompt-based, 2) ﬁnetuning, and 3) additive learning. Prompt-based approaches encode a task
description and task-speciﬁc examples as a natural language prompt for few-shot text generation
(Radford et al., 2019; Brown et al., 2020; Zheng & Huang, 2021; Schick & Sch¨utze, 2020; Li &
Liang, 2021). While these approaches can take full advantage of the universal natural language
understanding and generation capabilities of large-scale PLMs without further training of the main
model, these have some limitations in dealing with a large domain shift from the pretraining corpus
data, tuning suitable task-speciﬁc prompts, and covering an increased size of conditioning examples."
INTRODUCTION,0.018050541516245487,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.021660649819494584,"On the other hand, ﬁnetuning of the PLM is able to explicitly impart task-speciﬁc knowledge to the
model and hence lift the above limitations (Ziegler et al., 2019; Xu et al., 2021; Chen et al., 2020).
However, these ﬁnetuned models are prone to overﬁtting when only a small amount of training data
is available. In order to alleviate such an overﬁtting problem, additive learning has been extensively
exploited by incorporating task-speciﬁc adapters into the PLM (Zeldes et al., 2020; Stickland &
Murray, 2019)."
INTRODUCTION,0.02527075812274368,"In general, task-specialized adapters for few-shot NLG are trained by maximum likelihood estima-
tion (MLE). While MLE is efﬁcient in learning, it suffers from the exposure bias problem due to the
difference in the training and inference mechanisms (He et al., 2019), and this problem can be severe
with limited training data. Reinforcement learning (RL) is capable of resolving this exposure bias
problem by sequential output sampling during training (Ranzato et al., 2015; Keneshloo et al., 2019;
Shi et al., 2021). Moreover, it allows to leverage the target-speciﬁc sequence-level objectives such
as BLEU and ROUGE (Wu et al., 2018; Guo et al., 2021). However, in the task of NLG, the expo-
nentially large space of output sequences restricts the use of RL since it leads to high variance and
unstable training which is more serious in the few-shot setting. In this work, we develop a novel RL-
based additive learning algorithm on the transformer-based PLM to overcome these shortcomings
and to improve the performance of few-shot NLG."
INTRODUCTION,0.02888086642599278,"In particular, we ﬁrst convert the NLG task to the sequential token generation task based on the
transformer language model, and then propose a selective token generation between the PLM and
the task-speciﬁc adapter, during both RL-based training and inference. The proposed output token
selection enables to not only explicitly maintain a general prior knowledge from the frozen PLM
but also focus only on the task-relevant parts in sequence generation. In addition, in few-shot learn-
ing this partial token generation makes the task-speciﬁc adapter more resilient to overﬁtting and
furthermore reduces the overall output space which leads to stable RL training. Here, in order to
make the two token generators (policies) complement each other as well as to realize the robust
output selection at the token level on the ﬂy, we exploit a separate token-level policy selector. It is
noted that both the policy selector and the task-speciﬁc adapter are simultaneously learned by the
RL algorithm. Experimental results on various few-shot NLG tasks show that the proposed selec-
tive token generation outperforms the previous PLM-based additive learning algorithms with the
comprehensive (non-selective) token generation."
INTRODUCTION,0.032490974729241874,Our main contributions can be summarized as follows.
INTRODUCTION,0.036101083032490974,"• A novel selective token generation between the PLM and the task-speciﬁc adapter is pro-
posed for transformer-based few-shot NLG."
INTRODUCTION,0.039711191335740074,"• A separate selecting module is exploited to adaptively determine each output token in a
sequence both at training and testing time."
INTRODUCTION,0.04332129963898917,"• RL is applied to train both the policy selector and the task-speciﬁc adapter that is comple-
mentary to the PLM in text generation."
INTRODUCTION,0.04693140794223827,"• An extensive empirical validation on few-shot NLG tasks demonstrates that the proposed
selective token generation performs better in comparison to the previous PLM-based addi-
tive learning algorithms."
BACKGROUND,0.05054151624548736,"2
BACKGROUND"
NATURAL LANGUAGE GENERATION,0.05415162454873646,"2.1
NATURAL LANGUAGE GENERATION"
NATURAL LANGUAGE GENERATION,0.05776173285198556,"The goal of NLG is to generate a text sequence y = [y0, ..., yT ] for a given task, where yt is the
tth output token from a vocabulary V, and T is the output sequence length. For this generation, we
aims to model the distribution of y that is autoregressively factorized as pθ(y) = QT
t=0 pθ(yt|y<t),
where θ denotes the model parameters and y<t = [y0, ..., yt−1]. Here, the conditional distribution
to sample a token for each step, pθ(yt|y<t), is deﬁned by the softmax function on the output logits
fθ(yt|y<t). Note that in general, the language generation is conditioned on input context according
to a given task. Here, we encode the conditioning context by the same sequential model for gener-
ating an output sequence, and for simplicity we omit it. In this work, we utilize the autoregressive
transformer for our generative model."
NATURAL LANGUAGE GENERATION,0.061371841155234655,Under review as a conference paper at ICLR 2022
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.06498194945848375,"2.2
ADDITIVE LEARNING FOR FEW-SHOT GENERATION"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.06859205776173286,"To effectively leverage the general linguistic knowledge, θ is ﬁrst initialized by the PLM parame-
ters, θLM, for NLG. Given N task-speciﬁc training instances, D = {yn∗}N
n=1, where yn∗is the nth
ground-truth output sequence, directly ﬁnetuning θLM using D can incur the severe overﬁtting prob-
lem when N is small in the few-shot scenario. Therefore, we add the task-speciﬁc adapter, gθa pa-
rameterized by θa, on top of the PLM, and optimize only θa (Zeldes et al., 2020; Stickland & Murray,
2019). In speciﬁc, we reformulate f(·|y<t; θ) = W T h(y<t; θh) where W ∈RH×|V| and h ∈RH
denote the weight matrix and the penultimate representations, respectively, and θ = {W, θh}. Then,
we deﬁne the task-speciﬁc conditional distribution as follows:"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.07220216606498195,"p(yt|y<t; θLM, θa) = softmax

WLM
T hLM(y<t) + Wa
T g
 
hLM(y<t); θg

,
(1)"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.07581227436823104,"where hLM(y<t) = h(y<t; θh,LM) and θa = {Wa, θg}. Here, the summation of the PLM logits
and the adapter logits is motivated by auxiliary training1. It is noted that in our additive learning θa
is updated while θLM is kept frozen. Hence, in the following we omit θLM such that pθa(yt|y<t) =
p(yt|y<t; θLM, θa) for simplicity."
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.07942238267148015,"2.3
MAXIMUM LIKELIHOOD ESTIMATION (MLE)"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.08303249097472924,"Given a small amount of training data D = {yn∗}N
n=1, MLE optimizes θ by maximizing the data
log-likelihood as follows:"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.08664259927797834,"ˆθ = arg max
θ N
X n=1 T
X"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.09025270758122744,"t=0
log pθ(yn∗
t |yn∗
<t).
(2)"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.09386281588447654,"Here, the output token at each step is conditioned on not the previous sampled tokens from the
current model but the previous ground-truth tokens yn∗
<t. Namely, tokens are drawn from the data
distribution during training, which is opposed to that tokens are drawn from the model distribution
at test time. This discrepancy, also known as the exposure bias, leads that the errors will be accu-
mulated along the generated sequence at test time since the model is biased to only perform well
on the ground-truth history distribution. Especially, this bias problem can be more severe in the
few-shot training. In addition, the token-level cross-entropy loss in MLE training is different from
the sequence-level test metrics such as BLEU and ROUGE that are commonly used in the tasks of
NLG."
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.09747292418772563,"2.4
REINFORCEMENT LEARNING (RL)"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.10108303249097472,"As an alternative to MLE, RL is able to overcome the exposure bias problem of MLE by sequence-
level sampling from the model distribution during training (Ranzato et al., 2015). Also, RL can
improve the performance by directly optimizing the evaluation metrics (Guo et al., 2021). In order to
use RL for our additive learning, we reformulate our text generation as an RL problem: at each time
step t, the agent takes the current state st = y<t as an input and performs an action at that outputs
a token yt by a policy πθ(at|st) corresponding to pθ(yt|y<t). Then, the agent receives a reward
rt = r(st, at) and deterministically transitions to the next state st+1. Here, note that the token-level
intermediate reward rt = 0, ∀t < T when we use the delayed reward associated with the sequence-
level evaluation metric between the two full sequences, y and y∗. Let τ = {(st, at, rt)}T
t=0 be the
trajectory generated by πθ. The RL objective for the optimal agent is to maximize the expected sum
of future discounted rewards:"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.10469314079422383,"J(πθ) = Eτ∼πθ 
T
X"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.10830324909747292,"t=0
γtrt"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.11191335740072202,"
,
(3)"
ADDITIVE LEARNING FOR FEW-SHOT GENERATION,0.11552346570397112,"where γ ∈[0, 1) is the discount factor."
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.11913357400722022,"1Although the auxiliary training is particularly designed for maximizing the likelihood of the target task
output, it also can take an advantage for RL since the adapter logits are nearly zero before training is advanced.
Namely, it lets the task-speciﬁc conditional distribution start learning from the distribution of PLM, not a
uniform distribution."
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.12274368231046931,Under review as a conference paper at ICLR 2022
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.1263537906137184,"Figure 1: Text generation processes of Non-STG and STG are described. In the Non-STG model,
every token is sampled from the task-speciﬁc policy πa (Left). On the other hand, in the proposed
STG model, each token is selectively sampled from either the PLM policy πLM or the test-speciﬁc
policy πa where the selection is performed by the selection policy πs (Right). Symbols with dashed
line represent learnable models."
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.1299638989169675,"Among a number of algorithms to approximately optimize the RL objective, we employ an actor-
critic algorithm (Bahdanau et al., 2017) since it explicitly optimizes the policy network and it can
also alleviate the delayed reward problem. The actor-critic algorithm requires the additional critic
network to estimate the value of a state, V π(st) = Eπ[PT
t′=t γt′−trt′|st] = P"
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.13357400722021662,"at π(at|st)Qπ(st, at)
where the state-action value function Qπ(st, at) = Eπ[PT
t′=t γt′−trt′|st, at] = rt + V π(st+1). We
use the following policy gradient loss to learn the policy parameters θ: L = − T
X"
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.1371841155234657,"t=0
Aπθ(st, at) log πθ(at|st),
(4)"
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.1407942238267148,"where Aπθ(st, at) = Qπθ(st, at) −V πθ(st) is the advantage function that quantiﬁes how an action
at is better than the average action in state st."
ALTHOUGH THE AUXILIARY TRAINING IS PARTICULARLY DESIGNED FOR MAXIMIZING THE LIKELIHOOD OF THE TARGET TASK,0.1444043321299639,"In few-shot text generation, the extremely large action space (|V|T ) as well as the small amount
of training data often make it difﬁcult to perform RL with degraded performances, even though
we conduct the additive learning from the PLM. Furthermore, it commonly has a delayed reward
function (e.g. BLEU) which is deﬁned after an entire sequence generated. It is hard to decide which
token and how much contributes to the reward. This problem is known as the credit assignment.
Therefore, in this work, we propose a selective token generation for improving the RL-based additive
learning."
SELECTIVE TOKEN GENERATION,0.148014440433213,"3
SELECTIVE TOKEN GENERATION"
SELECTIVE TOKEN GENERATION,0.15162454873646208,"Instead of generating all tokens in an output sequence from the single task-speciﬁc policy, πa =
πθa(at|st), at each time step t, we sample an output token yt selectively from either the PLM policy
πLM = πθLM (at|st) or the task-speciﬁc policy πa:"
SELECTIVE TOKEN GENERATION,0.1552346570397112,"yt = at ∼
 
1t[πLM is selected]πLM(at|st) + (1 −1t[πLM is selected])πa(at|st)

,
(5)"
SELECTIVE TOKEN GENERATION,0.1588447653429603,"where 1t[·] is the indicator function (at t) that equals 1 if it is true and 0 otherwise. This output
token selection allows to explicitly utilize a general linguistic knowledge from the PLM without
catastrophic forgetting in few-shot learning. Also, the task-speciﬁc policy can focus on generating
only the task-relevant parts, which enables more effective few-shot training with a reduced action
space."
SELECTIVE TOKEN GENERATION,0.1624548736462094,"Now we need to determine how to select the proper policy at each step on the ﬂy as well as to
make the task-speciﬁc policy complementary to the PLM policy. For this, we exploit a separate"
SELECTIVE TOKEN GENERATION,0.16606498194945848,Under review as a conference paper at ICLR 2022
SELECTIVE TOKEN GENERATION,0.16967509025270758,"token-level policy selector. The proposed policy selector πs(it|st; θs) with the parameters θs, where
it ∈{0, 1}, is an another policy that stochastically decides a policy to generate at for st. Namely, a
token sample yt is generated by the following process:"
SELECTIVE TOKEN GENERATION,0.17328519855595667,"it
∼
πs(it|st),
(6)"
SELECTIVE TOKEN GENERATION,0.17689530685920576,"yt
=
at ∼πLM(at|st)
if it = 0,
at ∼πa(at|st)
if it = 1.
(7)"
SELECTIVE TOKEN GENERATION,0.18050541516245489,"This
process
can
be
considered
as
a
token
generation
from
a
hierarchical
policy
πh(at|st; θs, θLM, θa) where the policy selector represents the upper-level prior for the pref-
erence of the low-level policy. Therefore, the value function of this hierarchical policy can be
formulated as"
SELECTIVE TOKEN GENERATION,0.18411552346570398,"V πh(st) = Eπh[ T
X"
SELECTIVE TOKEN GENERATION,0.18772563176895307,"t′=t
γt′−trt′|st]
=
πs(it = 0|st)
X"
SELECTIVE TOKEN GENERATION,0.19133574007220217,"at
πLM(at|st)Qπh(st, at)"
SELECTIVE TOKEN GENERATION,0.19494584837545126,"+ πs(it = 1|st)
X"
SELECTIVE TOKEN GENERATION,0.19855595667870035,"at
πa(at|st)Qπh(st, at),
(8)"
SELECTIVE TOKEN GENERATION,0.20216606498194944,"and Aπh(st, at) = Qπh(st, at) −V πh(st). Here, it is noted that a single critic network is used for
the hierarchical policy since it does not affect st. Given a sample trajectory {(st, it, at, rt)}T
t=0, the
loss for optimizing θs and θa is L = − T
X"
SELECTIVE TOKEN GENERATION,0.20577617328519857,"t=0
Aπh(st, at)

1[it = 0]
 
log sg[πLM(at|st)] + log πs(it|st)
"
SELECTIVE TOKEN GENERATION,0.20938628158844766,"+ 1[it = 1]
 
log πa(at|st) + log πs(it|st)

,
(9)"
SELECTIVE TOKEN GENERATION,0.21299638989169675,"where sg stands for the stop-gradient operator. Similar to πa, πs makes use of the PLM representa-
tions and the task-speciﬁc adapter such that"
SELECTIVE TOKEN GENERATION,0.21660649819494585,"πs(it|st; θs) = softmax

m

g
 
hLM(st)

; θs

,
(10)"
SELECTIVE TOKEN GENERATION,0.22021660649819494,"where m is the selector module that is implemented by a small neural network. Figure 1 depicts
the overall text generation process by the proposed selective token generation (STG) in comparison
to the previous non-selective token generation (Non-STG). Here, note that since all policies in STG
share the same PLM representations, the increased computational cost by STG over Non-STG is
negligible."
SELECTIVE TOKEN GENERATION,0.22382671480144403,"The use of the separated policy selector that is simultaneously trained with the task-speciﬁc policy
allows the task-speciﬁc policy to be complementary to the PLM policy. Especially, this cooperative
ensemble learning can be realized by our RL algorithm that performs sequential sampling from the
model during training. In addition, the proposed STG model makes use of the PLM not at the feature
level but the output distribution level in text generation. In our few-shot learning this is beneﬁcial in
explicitly retaining strong linguistic and world knowledge from the PLM."
EXPERIMENTS,0.22743682310469315,"4
EXPERIMENTS"
EXPERIMENTS,0.23104693140794225,"In this section, we evaluate our method against baselines on Data-to-Text, Question Answering and
Text Summarization tasks which are widely used in few-shot NLG. Since we claim the effectiveness
of our method in additive learning, we compare our results against those obtained by other additive
learning methods."
BASELINE,0.23465703971119134,"4.1
BASELINE"
BASELINE,0.23826714801444043,"PLM. In our experiments, we assume that the PLM works to some extent for a given task. However,
the naive PLM usually does not satisfy it for a new task unseen during training. Hence, we ﬁnetuned"
BASELINE,0.24187725631768953,Under review as a conference paper at ICLR 2022
BASELINE,0.24548736462093862,Table 1: Data-to-Text performance on FewShotWOZ dataset.
BASELINE,0.2490974729241877,"Model
Restaurant
Hotel
TV
Laptop
BLEU ↑
ERR ↓
BLEU ↑
ERR ↓
BLEU ↑
ERR ↓
BLEU ↑
ERR ↓
PLM
19.42
12.57
35.84
13.74
29.00
9.15
28.27
9.31
Non-STG-MLE
17.21
15.87
28.42
12.64
29.83
10.05
26.76
10.52
Non-STG-RL
18.01
11.98
36.72
12.64
28.66
9.19
28.59
9.21
NE(max)-MLE
14.12
15.27
31.32
14.29
28.23
10.21
26.93
10.02
NE(mix)-MLE
25.27
14.97
37.13
15.93
32.85
16.31
32.91
14.77
NE(max)-RL
15.20
11.68
32.68
16.48
28.91
9.24
28.66
9.51
NE(mix)-RL
24.10
19.16
38.07
18.68
32.84
18.06
32.53
17.14
STG
21.28
10.78
38.09
11.54
30.24
9.03
30.41
8.91"
BASELINE,0.2527075812274368,"GPT-22 (Radford et al., 2019) with MLE for few epochs and used it as the PLM. Fine-tuning the
PLM with MLE is most commonly used for task adaptation and thus it can also be a strong baseline.
This ﬁne-tuning phase accelerates the learning of the adapter. This is particularly when the adapta-
tion requires to cover the large domain shift. Severe performance degradation was observed for all
the tasks when we skipped the ﬁne-tuning."
BASELINE,0.2563176895306859,"Non-STG. This method stands for Non-Selective Token Generation which uses the above the PLM
as an encoder (frozen) and the adapter (additional layer to be trained). We use two objectives,
MLE and RL, for additive learning. These will be denoted as Non-STG-MLE and Non-STG-RL,
respectively."
BASELINE,0.259927797833935,"STG-Naive Ensemble. We believe that our proposed method encourages the task-speciﬁc policy
(πa) to complement the PLM’s policy (πLM) with a proper selection of the selector through joint
training. To investigate this, we evaluate against two different naive ensembles of the policies, πa
trained from Non-STG and πLM of the PLM. These ensemble schemes are as follows:"
BASELINE,0.26353790613718414,"• NE(max): πmax = softmax(Max(πa, πLM))
• NE(mix): πmix = (πa + πLM)/2"
BASELINE,0.26714801444043323,"We also evaluate another naive ensemble strategy NE(random) that randomly selects a token policy
at each step between πa and πLM, however it shows lower performances than the others."
IMPLEMENTATION,0.27075812274368233,"4.2
IMPLEMENTATION"
IMPLEMENTATION,0.2743682310469314,"Here, we explain the implementation set-up of our method."
IMPLEMENTATION,0.2779783393501805,"Additional Layer: The task-speciﬁc adapter g in Section 2.2 is implemented by a LSTM to encode
the dynamics of the representation vector hLM. We found that the use of MLP was not performed
well."
IMPLEMENTATION,0.2815884476534296,Selector. We use a 2-layer MLP with ReLU activation for m.
IMPLEMENTATION,0.2851985559566787,"Reinforcement Learning. We employ Actor-Critic method (Konda & Tsitsiklis, 2000; Fedus et al.,
2018) for RL. The agents (i.e. selector and generator) receive a reward after generating a sentence.
Here, we use different reward functions according to tasks. We use delexicalised BLEU for Data-to-
Text following Peng et al. (2020), Averaged score of BLEU and ROUGE-L for Question Answering
and ROUGE-L for Text Summarization following Paulus et al. (2017) as the reward function."
IMPLEMENTATION,0.2888086642599278,"Token Sampling. During the training, it ∈{0, 1} ∼πs is ﬁrst sampled, and then we use either πLM
of the PLM for it = 0 or the task-speciﬁc policy πa for it = 1 to sample the tth token. During the
evaluation, any decoding strategy, such as a beam search, can be used with the mixture of policies
πh(·) = πs(it = 0)πLM(·) + πs(it = 1)πa(·). We use the beam search decoding with a sample
size of k = 3 for Text Summarization and topp = 0.9 decoding for both Data-to-Text (k = 10) and
Question Answering (k = 3)."
IMPLEMENTATION,0.2924187725631769,"2We make use of GPT-2 with 345M parameters as the initial checkpoint. We follow the training details in
the previous works (Peng et al., 2020; Khandelwal et al., 2019) for each task."
IMPLEMENTATION,0.296028880866426,Under review as a conference paper at ICLR 2022
IMPLEMENTATION,0.2996389891696751,"Table 2: Averaged performances and performance gains against the PLM for Question Answering
on 1% few-shot subset data of MS-MARCO. The gain indicates the averaged performance gain
against the PLM’s one with the standard deviation of the gain in the blacket. See the results of the
other few-shots in Appendix A."
IMPLEMENTATION,0.30324909747292417,"Model
BLEU
BLEUgain
RL
RLgain
Avg
Avggain
PLM
41.49
0.00(± 0.00)
49.76
0.00(± 0.00)
45.63
0.00(± 0.00)
Non-STG-MLE
41.02
-0.47(± 0.41)
50.14
0.37(± 0.40)
45.58
-0.05(± 0.40)
Non-STG-RL
41.25
-0.24(± 0.34)
49.97
0.20(± 0.21)
45.61
-0.02(± 0.27)
NE(max)-MLE
41.11
-0.38(± 0.51)
50.77
1.01(± 0.57)
45.94
0.32(± 0.54)
NE(mix)-MLE
42.26
0.77(± 0.37)
51.14
1.38(± 0.40)
46.70
1.08(± 0.38)
NE(max)-RL
41.51
0.02(± 0.47)
50.54
0.78(± 0.38)
46.03
0.40(± 0.41)
NE(mix)-RL
42.29
0.80(± 0.24)
50.84
1.08(± 0.19)
46.57
0.94(± 0.21)
STG
42.76
1.27(± 0.38)
51.19
1.43(± 0.37)
46.98
1.35(± 0.37)"
DATA-TO-TEXT,0.30685920577617326,"4.3
DATA-TO-TEXT"
DATA-TO-TEXT,0.3104693140794224,"Data-to-Text is a task that transforms structured data such as graphs or tables into natural language.
Recent works (Mager et al., 2020; Peng et al., 2020; Kale, 2020) show that the PLM can be adapted
successfully to this task by taking a serialized form of data as an input without a carefully designed
model to encode the structured data. Here, we perform experiments on FewShotWOZ (Peng et al.,
2020) dataset. The evaluation is conducted only on the topics of Restaurant, Hotel, Laptop, and TV
since the evaluator has not published yet for the other topics (Taxi, Attraction, Train)3. There are 50
training instances for each topic and 129, 78, 1379, and 680 testing instances for Restaurant, Hotel,
Laptop, and TV, respectively. The models are evaluated by measuring ﬂuency and informativeness
using BLEU score and ERR (slot ERror Rate), respectively. Table 1 shows the obtained results."
LONG ANSWER QUESTION ANSWERING,0.3140794223826715,"4.4
LONG ANSWER QUESTION ANSWERING"
LONG ANSWER QUESTION ANSWERING,0.3176895306859206,"We consider Long Answer QA task on MS-MARCO (Nguyen et al., 2016) dataset. In this task,
a passage and a query are given, and the model generates an answer with respect to the query by
referring to the passage. Here, we randomly sample various sizes of (0.5% ≈500, 1%, and 2%)
subset data from the train dataset. We also sample a validation and a test set, which contains 500 and
12,000 instances, respectively, from the dev dataset. We repeat this test three times with different
random seeds. In other words, we collect three different random subsets for each size of few-shot.
Thus, we perform experiments on total nine subsets. The models are evaluated by measuring BLEU,
ROUGE-L (denoted as RL), and their average value (denoted as Avg). The performances of the
models are reported in Table 2 and Appendix A."
TEXT SUMMARIZATION,0.3212996389891697,"4.5
TEXT SUMMARIZATION"
TEXT SUMMARIZATION,0.3249097472924188,"We consider the problem of abstractive summarization: models aim to generate its summary (the
“target” text) for a given piece of “source” text such that its meaning is intact. The model has to
learn 1) building a semantic form internally from the source text and 2) converting the semantic
form into a text. Here, we randomly sample various sizes of (0.5% ≈1, 500, 1%, and 2%) subset
data from CNN/Daily Mail (See et al., 2017). We repeat this test three times for each size of few-
shot as in above QA task. ROUGE (Lin, 2004) is commonly used to evaluate n-grams recall of the
summaries with gold references. ROUGE-1, ROUGE-2, and ROUGE-L (measured based on the
longest common sub-sequence) scores are reported (denoted as R1, R2, and RL, respectively) in
Table 3 and Appendix B."
RESULT,0.3285198555956679,"4.6
RESULT"
RESULT,0.33212996389891697,"We compare experimental results on various tasks in this subsection. In most cases, additive learning
improves the performances over the PLM. However, they do not always guarantee a performance"
RESULT,0.33574007220216606,3https://github.com/pengbaolin/SC-GPT
RESULT,0.33935018050541516,Under review as a conference paper at ICLR 2022
RESULT,0.34296028880866425,"Table 3: Averaged performances and performance gains against the PLM for Text Summarization
on 2% few-shot subset data of CNN/DM. The gain indicates the averaged performance gain against
the PLM’s one with the standard deviation of the gain in the blacket. See the results of the other
few-shots in Appendix B."
RESULT,0.34657039711191334,"Model
R1
R1gain
R2
R2gain
RL
RLgain
PLM
33.05
0.00(± 0.00)
12.96
0.00(± 0.00)
23.39
0.00(± 0.00)
Non-STG-MLE
33.19
0.14(± 0.08)
12.98
0.02(± 0.05)
23.39
0.00(± 0.06)
Non-STG-RL
33.22
0.17(± 0.09)
12.99
0.04(± 0.05)
23.40
0.01(± 0.08)
NE(max)-MLE
33.19
0.14(± 0.10)
12.99
0.03(± 0.05)
23.40
0.01(± 0.06)
NE(mix)-MLE
33.11
0.06(± 0.04)
12.99
0.03(± 0.03)
23.41
0.02(± 0.03)
NE(max)-RL
33.21
0.16(± 0.10)
12.99
0.04(± 0.06)
23.41
0.02(± 0.08)
NE(mix)-RL
33.14
0.09(± 0.03)
13.00
0.05(± 0.02)
23.42
0.03(± 0.03)
STG
33.45
0.40(± 0.24)
13.14
0.18(± 0.10)
23.66
0.27(± 0.16)"
RESULT,0.35018050541516244,"improvement. For example, the ERR score of the PLM on Laptop shows a better result except for
STG and NE(mix)-RL (see Table 1) and the Non-STG models trained on 1% few-shot subset of
MS-MARCO do not outperform the PLM (see Table 2 and Table 5 of Appendix A)."
RESULT,0.35379061371841153,"In Data-to-Text task, as shown in Table 1, we can observe that the Non-STG models do not out-
perform the PLM even though it has more neural units and takes more training time. The models
trained on the RL objective show better performances for the ERR (lower is better). Interestingly,
NE(mix) methods show strong improvements for the BLEU which measures the ﬂuency of sen-
tence but obvious degeneration for the ERR which measures the rate of missing information from
the given data. These results suggest that the PLM is much more capable of task-general knowledge
than the task-speciﬁc generator (i.e. πa) trained on few-shot dataset. It motivates us to jointly learn
the policy selector and the task-speciﬁc generator. Here, while other methods show some trade-off
between BLEU and ERR, only STG shows improvements on the both metrics for all topics in the
dataset."
RESULT,0.3574007220216607,"In Question Answering, NE(mix) and STG show signiﬁcantly better performances than the other
methods as shown in Table 2 and Appendix A. Notably, NE(mix) show good performances as much
as STG. It obviously suggests that the PLM can be a complementary model to the additional model.
Therefore, in this context, it can be lost of the prior knowledge of the PLM even the additional model
has been built over the feature space of the PLM. We will discuss it in Section 4.7."
RESULT,0.36101083032490977,"In Text Summarization (a problem of long-sequence generation), STG shows signiﬁcantly larger
gains than Non-STG-MLE, Non-STG-RL, and their naive ensembles with the PLM in every score
metric and training data size as shown in Table 3 and Appendix B."
RESULT,0.36462093862815886,"4.7
WHAT MAKES STG BETTER?"
RESULT,0.36823104693140796,"Knowledge Preservation. Although freezing the parameters of the PLM can mitigate the forgetting
problem, it would be failed to do so when additional units having sufﬁcient capacity is used since it is
easy to memorize typical patterns of answers in small amount of training dataset and can degrade the
generalization performance. An example that represent this issue is shown in Table 4 of Appendix
A. In this case, a passage that contains two deﬁnitions (super-scripted and bolded) about conﬂict is
given with a query which asks the psychological meaning of conﬂict. Without the knowledge of who
Colman4 is, it can be a hard to answer since the word psychology is not appeared in the passage.
Here, the PLM repeats the given query due to the imperfect domain adaptation. Non-STG models,
both Non-STG-MLE and RL, generate the same answer, that is not the psychological meaning but
the general meaning of conﬂict, with pretty low perplexity (≈0.35). The reason in producing this
incorrect answer is that most queries in few-shot training data ask a general meaning of a concept,
and Non-STG models are overﬁtted to this pattern. On the other hand, the answer of STG, which
is close to the ground truth, is generated by the PLM policy πLM after some sequence of tokens
(conﬂict is) that are sampled from the task-speciﬁc policy πa."
RESULT,0.37184115523465705,"4A psychologist, https://en.wikipedia.org/wiki/Peter_T._Coleman_(academic)"
RESULT,0.37545126353790614,Under review as a conference paper at ICLR 2022
RESULT,0.37906137184115524,"We can ﬁnd such examples for the other tasks in Appendix F: In Data-To-Text, as shown in the last
example of Table 10, Non-STG generates nicam stereo which is not appeared in the given data. This
is due to that nicam stereo was appeared 7 times (7/50, 14%) in training data. In Summarization,
as shown in the ﬁrst example of Table 14, Non-STG models only consider the forepart of the given
article. Since the most of the major information is appeared in the forepart in News data, Non-STG
models can be easily overﬁtted to generate the text according to such a pattern. Hence, we claim that
Non-STG easily exposed learning patterns of typical answering and STG resolves this issue since it
can be fully accessible to the knowledge of the PLM."
RESULT,0.38267148014440433,"Resolving Disadvantages of RL. As described in Section 2.4, RL has some limitation with its
application: 1) it suffers from exponentially large search space |V|T , and 2) it suffers from unstable
training caused by credit assignment problem. STG resolves the ﬁrst issue since the frozen PLM
chooses a token when it is selected, and therefore the search space of the generator is approximately
decreased from |V|T to |V|T −T P LM where T P LM is the average length of sequences generated by
PLM. For the second issue, the loss function of STG (Equation 9) intuitively shows that the gradient
to the task-speciﬁc policy πa associated with producing at will depend on the selector’s action (i.e.
it = 1). Hence, unlike Non-STG, πa of STG knows which token is contributed to the reward (see
Appendix D for an illustration)."
RELATED WORK,0.3862815884476534,"5
RELATED WORK"
RELATED WORK,0.3898916967509025,"Recently, prompt-based in-context learning with an extremely large transformer-based PLM shows
impressive few-shot generation performances (Radford et al., 2019; Brown et al., 2020). Schick
& Sch¨utze (2020) propose manually designed natural language prompts for improved few-shot text
summarization and headline generation. Elsahar et al. (2018) conduct zero-shot learning for question
generation from knowledge graphs, however they require a large amount of in-domain training data
for their transfer learning. Chen et al. (2020) directly ﬁnetune the pretrained GPT-2 with a small
amount of serialized attribute-value pairs for table-to-text generation. Gong et al. (2020) further
apply multiple tasks to effectively leverage the structured information of tables. In contrast to these
approaches, our proposed method utilizes RL-based additive learning for few-shot text generation."
RELATED WORK,0.3935018050541516,"Applying RL for text generation has been widely used to mitigate the exposure bias problem of
MLE as well as to directly optimize task-relevant evaluation metrics. Ranzato et al. (2015) use
the REINFORCE algorithm for text summarization and machine translation while Bahdanau et al.
(2017) use the actor-critic algorithm for machine translation. However, they require pretraining
using MLE. Ding & Soricut (2017) propose softmax policy gradient to remove the MLE-based
pretraining. However, it requires various techniques for effective training. Tan et al. (2018) propose
an entropy-regularized policy optimization that subsumes many of the previous training algorithms.
Our proposed method is different from these methods in that we apply RL for more difﬁcult few-shot
generative modeling."
RELATED WORK,0.3971119133574007,"Various methods take into account the RL tasks with large action spaces like NLG. Dulac-Arnold
et al. (2015) consider only actions in a cluster around the latent state of action obtained from a given
state. Chandak et al. (2019) deﬁne the action embedding as a distribution with semantic of action
and use a deterministic policy to take an action. Even-Dar et al. (2003); Zahavy et al. (2018) devise a
method of incorporating the process of directly removing unnecessary actions according to the state
in the RL problem. Guo et al. (2021) propose to use soft Q-learning and path consistency learning
to combine off- and on-policy updates. Unlike these approaches, we use the hierarchical policy that
reduces the sequential action space."
CONCLUSION,0.4007220216606498,"6
CONCLUSION"
CONCLUSION,0.4043321299638989,"In this work, we propose to exploit a selective token generation between the pretrained language
model and the task-speciﬁc adapter with RL-based additive learning for the tasks of few-shot nat-
ural language generation. In particular, we devise a trainable policy selector at the token level and
jointly learn it with the task-speciﬁc policy. The proposed policy selector and RL algorithm make
the two policies complementary each other and lead to robust few-shot generative modeling. Exper-
imental results on various tasks of few-shot text generation show that the proposed selective token
generation along with RL-based additive learning consistently improves the performances with less"
CONCLUSION,0.40794223826714804,Under review as a conference paper at ICLR 2022
CONCLUSION,0.41155234657039713,"overﬁtting. For future work, we will investigate more general ensemble learning for few-shot learn-
ing and perform more study on the architectures of both the adapter and selector."
REFERENCES,0.4151624548736462,REFERENCES
REFERENCES,0.4187725631768953,"Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. International
Conference on Learning Representations, 2017. URL https://arxiv.org/abs/1607.
07086."
REFERENCES,0.4223826714801444,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei.
Language models are few-shot learners.
arXiv preprint
arXiv:2005.14165, 2020."
REFERENCES,0.4259927797833935,"Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip Thomas. Learn-
ing action representations for reinforcement learning. In International Conference on Machine
Learning, pp. 941–950. PMLR, 2019."
REFERENCES,0.4296028880866426,"Ernie Chang, Xiaoyu Shen, Alex Marin, and Vera Demberg. The selectgen challenge: Finding the
best training samples for few-shot neural text generation. arXiv preprint arXiv:2108.06614, 2021."
REFERENCES,0.4332129963898917,"Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, and William Yang Wang. Few-shot NLG with
pre-trained language model. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 183–190, Online, July 2020. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.acl-main.18. URL https://aclanthology.org/2020.
acl-main.18."
REFERENCES,0.4368231046931408,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training
text encoders as discriminators rather than generators. International Conference on Learning
Representations, 2020."
REFERENCES,0.4404332129963899,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//www.aclweb.org/anthology/N19-1423."
REFERENCES,0.44404332129963897,"Nan Ding and Radu Soricut. Cold-start reinforcement learning with softmax policy gradient. arXiv
preprint arXiv:1709.09346, 2017."
REFERENCES,0.44765342960288806,"Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap,
Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep rein-
forcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679, 2015."
REFERENCES,0.45126353790613716,"Hady Elsahar, Christophe Gravier, and Frederique Laforest. Zero-shot question generation from
knowledge graphs for unseen predicates and entity types. arXiv preprint arXiv:1802.06842, 2018."
REFERENCES,0.4548736462093863,"Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions
for reinforcement learning. In Proceedings of the 20th International Conference on Machine
Learning (ICML-03), pp. 162–169, 2003."
REFERENCES,0.4584837545126354,"William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via ﬁlling in
the . International Conference on Learning Representations, 2018."
REFERENCES,0.4620938628158845,"Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. Tablegpt:
Few-shot table-to-text generation with table structure reconstruction and content matching. In
International Conference on Computational Linguistics, pp. 1978—-1988, 2020."
REFERENCES,0.4657039711191336,Under review as a conference paper at ICLR 2022
REFERENCES,0.4693140794223827,"Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li.
Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 1631–1640, Berlin, Germany, August
2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1154. URL https:
//aclanthology.org/P16-1154."
REFERENCES,0.4729241877256318,"Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Text generation with efﬁcient
(soft) q-learning. arXiv preprint arXiv:2106.07704, 2021."
REFERENCES,0.47653429602888087,"Tianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass.
Exposure bias versus self-
recovery: Are distortions really incremental for autoregressive text generation? arXiv preprint
arXiv:1905.10617, 2019."
REFERENCES,0.48014440433212996,"Mihir Kale. Text-to-text pre-training for data-to-text tasks. arXiv preprint arXiv:2005.10433, 2020."
REFERENCES,0.48375451263537905,"Yaser Keneshloo, Tian Shi, Naren Ramakrishnan, and Chandan K Reddy.
Deep reinforcement
learning for sequence-to-sequence models. IEEE transactions on neural networks and learning
systems, 31(7):2469–2489, 2019."
REFERENCES,0.48736462093862815,"Urvashi Khandelwal, Kevin Clark, Dan Jurafsky, and Lukasz Kaiser. Sample efﬁcient text summa-
rization using a single pre-trained transformer. arXiv preprint arXiv:1905.08836, 2019."
REFERENCES,0.49097472924187724,"Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008–1014. Citeseer, 2000."
REFERENCES,0.49458483754512633,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. International
Conference on Learning Representations, 2019."
REFERENCES,0.4981949458483754,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension.
In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020."
REFERENCES,0.5018050541516246,"Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190, 2021."
REFERENCES,0.5054151624548736,"Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization
Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguis-
tics. URL https://aclanthology.org/W04-1013."
REFERENCES,0.5090252707581228,"Yinhan Liu, Naman Goyal Myle Ott, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019."
REFERENCES,0.5126353790613718,"Manuel Mager, Ram´on Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee,
Radu Florian, and Salim Roukos.
Gpt-too: A language-model-ﬁrst approach for amr-to-text
generation. arXiv preprint arXiv:2005.09123, 2020."
REFERENCES,0.516245487364621,"Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and
Li Deng. Ms marco: A human generated machine reading comprehension dataset. In CoCo@
NIPS, 2016."
REFERENCES,0.51985559566787,"Romain Paulus, Caiming Xiong, and Richard Socher.
A deep reinforced model for abstractive
summarization. arXiv preprint arXiv:1705.04304, 2017."
REFERENCES,0.5234657039711191,"Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng, and Jianfeng Gao.
Few-shot natural language generation for task-oriented dialog. arXiv preprint arXiv:2002.12328,
2020."
REFERENCES,0.5270758122743683,"A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by
generative pre-training. 2018."
REFERENCES,0.5306859205776173,Under review as a conference paper at ICLR 2022
REFERENCES,0.5342960288808665,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
REFERENCES,0.5379061371841155,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019."
REFERENCES,0.5415162454873647,"Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015."
REFERENCES,0.5451263537906137,"Timo Schick and Hinrich Sch¨utze. Few-shot text generation with pattern-exploiting training. arXiv
preprint arXiv:2012.11926, 2020."
REFERENCES,0.5487364620938628,"Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-
generator networks. arXiv preprint arXiv:1704.04368, 2017."
REFERENCES,0.5523465703971119,"Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, and Chandan K Reddy. Neural abstractive text
summarization with sequence-to-sequence models. ACM Transactions on Data Science, 2(1):
1–37, 2021."
REFERENCES,0.555956678700361,"Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efﬁcient adap-
tation in multi-task learning. In International Conference on Machine Learning, pp. 5986–5995.
PMLR, 2019."
REFERENCES,0.5595667870036101,"Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha.
Ammus : A
survey of transformer-based pretrained models in natural language processing. arXiv preprint
arXiv:2108.05542, 2021."
REFERENCES,0.5631768953068592,"Bowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric Xing. Connecting the dots
between mle and rl for sequence prediction. arXiv preprint arXiv:1811.09740, 2018."
REFERENCES,0.5667870036101083,"Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning
for neural machine translation. arXiv preprint arXiv:1808.08866, 2018."
REFERENCES,0.5703971119133574,"Xinnuo Xu, Guoyin Wang, Young-Bum Kim, and Sungjin Lee. Augnlg: Few-shot natural language
generation using self-trained data augmentation. arXiv preprint arXiv:2106.05589, 2021."
REFERENCES,0.5740072202166066,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R. Salakhutdinov, and Quoc V. Le.
Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural
information processing systems, pp. 5753–5763, 2019."
REFERENCES,0.5776173285198556,"Wenpeng Yin. Meta-learning for few-shot natural language processing: A survey. arXiv preprint
arXiv:2007.09604, 2020."
REFERENCES,0.5812274368231047,"Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. Learn what not
to learn: Action elimination with deep reinforcement learning. arXiv preprint arXiv:1809.02121,
2018."
REFERENCES,0.5848375451263538,"Yoel Zeldes, Dan Padnos, Or Sharir, and Barak Peleg. Technical report: Auxiliary tuning and its
application to conditional text generation. arXiv preprint arXiv:2006.16823, 2020."
REFERENCES,0.5884476534296029,"Chujie Zheng and Minlie Huang. Exploring prompt-based few-shot learning for grounded dialog
generation. arXiv preprint arXiv:2109.06513, 2021."
REFERENCES,0.592057761732852,"Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint arXiv:1909.08593, 2019."
REFERENCES,0.5956678700361011,Under review as a conference paper at ICLR 2022
REFERENCES,0.5992779783393501,"A
QUESTION ANSWERING RESULTS"
REFERENCES,0.6028880866425993,"Table 4: Generated answers from an instance of MS-MARCO QA dataset. Two deﬁnitions about
conﬂict are presented in bold text in the passage. Tokens sampled from the task-speciﬁc policy πa
are presented in red. The answers are sampled from the models trained on 0.5% few-shot subset
data."
REFERENCES,0.6064981949458483,"Passage
three types of conﬂicts are : 1. intrapersonal conﬂicts , 2. interpersonal con-
ﬂicts and 3. unconscious conﬂicts . the word conﬂict has been derived from
a latin word “conﬂicts” which means “strike two things at the same time” .
conﬂict is 1)an opposition or a tug-of-war between contradictory impulses
. according to colman ”a conﬂict is 2)the anticipated frustration entailed in
the choice of either alternative”."
REFERENCES,0.6101083032490975,"Query
conﬂict deﬁnition psychology"
REFERENCES,0.6137184115523465,"Ground-truth
the anticipated frustration entailed in the choice of either alternative."
REFERENCES,0.6173285198555957,"PLM
conﬂict deﬁnition psychology."
REFERENCES,0.6209386281588448,"Non-STG
conﬂict is an opposition or a tug-of-war between contradictory impulses."
REFERENCES,0.6245487364620939,"STG
conﬂict is the anticipated frustration entailed in the choice of either alternative."
REFERENCES,0.628158844765343,"Table 5: Averaged performances and performance gains against the PLM for Question Answering
on 0.5% few-shot subset data of MS-MARCO."
REFERENCES,0.631768953068592,"Model
BLEU
BLEUgain
RL
RLgain
Avg
Avggain
PLM
35.64
0.00(± 0.00)
43.10
0.00(± 0.00)
39.37
0.00(± 0.00)
Non-STG-MLE
34.53
-1.12(± 0.81)
43.08
-0.03(± 0.69)
38.80
-0.57(± 0.75)
Non-STG-RL
35.08
-0.56(± 0.24)
42.78
-0.32(± 0.14)
38.93
-0.44(± 0.17)
NE(max)-MLE
34.69
-0.95(± 0.88)
43.93
0.83(± 0.88)
39.31
-0.06(± 0.87)
NE(mix)-MLE
36.26
0.62(± 0.58)
44.43
1.32(± 0.64)
40.34
0.97(± 0.61)
NE(max)-RL
35.14
-0.51(± 0.31)
42.94
-0.16(± 0.18)
39.04
-0.33(± 0.21)
NE(mix)-RL
35.93
0.29(± 0.20)
43.52
0.42(± 0.29)
39.73
0.35(± 0.24)
STG
37.37
1.72(± 0.34)
44.53
1.43(± 0.61)
40.95
1.58(± 0.40)"
REFERENCES,0.6353790613718412,"Table 6: Averaged performances and performance gains against the PLM for Question Answering
on 2% few-shot subset data of MS-MARCO."
REFERENCES,0.6389891696750902,"Model
BLEU
BLEUgain
RL
RLgain
Avg
Avggain
PLM
47.72
0.00(± 0.00)
56.02
0.00(± 0.00)
51.87
0.00(± 0.00)
Non-STG-MLE
47.85
0.13(± 0.02)
56.81
0.79(± 0.21)
52.33
0.46(± 0.09)
Non-STG-RL
48.00
0.28(± 0.33)
56.83
0.82(± 0.31)
52.42
0.55(± 0.32)
NE(max)-MLE
47.65
-0.07(± 0.07)
57.22
1.20(± 0.35)
52.44
0.57(± 0.16)
NE(mix)-MLE
48.44
0.72(± 0.22)
57.30
1.28(± 0.25)
52.87
1.00(± 0.23)
NE(max)-RL
47.58
-0.14(± 0.57)
57.06
1.05(± 0.57)
52.32
0.45(± 0.56)
NE(mix)-RL
48.28
0.56(± 0.08)
57.02
1.00(± 0.01)
52.65
0.78(± 0.04)
STG
48.42
0.70(± 0.22)
57.30
1.28(± 0.38)
52.86
0.99(± 0.30)"
REFERENCES,0.6425992779783394,Under review as a conference paper at ICLR 2022
REFERENCES,0.6462093862815884,"B
SUMMARIZATION PERFORMANCE"
REFERENCES,0.6498194945848376,"Table 7: Averaged performances and performance gains against the PLM for Text Summarization
on 0.5% few-shot subset data of CNN/DM."
REFERENCES,0.6534296028880866,"Model
R1
R1gain
R2
R2gain
RL
RLgain
PLM
30.19
0.00(± 0.00)
11.27
0.00(± 0.00)
21.21
0.00(± 0.00)
Non-STG-MLE
30.34
0.14(± 0.13)
11.32
0.05(± 0.07)
21.20
-0.01(± 0.06)
Non-STG-RL
30.35
0.15(± 0.15)
11.34
0.07(± 0.07)
21.22
0.02(± 0.08)
NE(max)-MLE
30.33
0.14(± 0.11)
11.31
0.04(± 0.06)
21.20
0.00(± 0.04)
NE(mix)-MLE
30.32
0.13(± 0.08)
11.31
0.04(± 0.04)
21.23
0.02(± 0.04)
NE(max)-RL
30.37
0.18(± 0.11)
11.35
0.08(± 0.05)
21.26
0.06(± 0.02)
NE(mix)-RL
30.28
0.09(± 0.09)
11.30
0.03(± 0.05)
21.22
0.02(± 0.04)
STG
30.47
0.28(± 0.15)
11.37
0.09(± 0.05)
21.36
0.15(± 0.09)"
REFERENCES,0.6570397111913358,"Table 8: Averaged performances and performance gains against the PLM for Text Summarization
on 1% few-shot subset data of CNN/DM."
REFERENCES,0.6606498194945848,"Model
R1
R1gain
R2
R2gain
RL
RLgain
PLM
31.03
0.00(± 0.00)
11.56
0.00(± 0.00)
21.62
0.00(± 0.00)
Non-STG-MLE
31.25
0.22(± 0.03)
11.63
0.07(± 0.02)
21.72
0.10(± 0.07)
Non-STG-RL
31.24
0.21(± 0.13)
11.64
0.08(± 0.05)
21.76
0.13(± 0.12)
NE(max)-MLE
31.25
0.22(± 0.04)
11.63
0.07(± 0.03)
21.72
0.09(± 0.08)
NE(mix)-MLE
31.20
0.17(± 0.04)
11.61
0.04(± 0.02)
21.69
0.06(± 0.04)
NE(max)-RL
31.26
0.24(± 0.13)
11.66
0.10(± 0.04)
21.78
0.15(± 0.12)
NE(mix)-RL
31.18
0.15(± 0.08)
11.63
0.06(± 0.02)
21.69
0.07(± 0.06)
STG
32.03
1.00(± 0.48)
11.96
0.40(± 0.15)
22.23
0.61(± 0.33)"
REFERENCES,0.6642599277978339,"C
TRAINING SETTINGS"
REFERENCES,0.6678700361010831,"In our experiments all the models of additive learning, Non-STG and STG, are used the same archi-
tecture and hyper-parameters (except whether to use pre-training) for training as described in Table
9. We found that pre-training the addtional layer of Non-STG-RL with MLE helps the performance
improvements. On the other hand, STG without pre-training shows better performances. We use the
training data for each topic of the task of Data-to-Text as their validation data."
REFERENCES,0.6714801444043321,Table 9: Hyper-parameters used for experiments
REFERENCES,0.6750902527075813,"Hyper-parameter
Summarization
Data-to-Text
Question Answering"
REFERENCES,0.6787003610108303,"Num layer
2
RNN hidden size
512
256
256
γ
1
Optimizer
AdamW with betas = (0.9, 0.999), eps = 10−8"
REFERENCES,0.6823104693140795,"Learning rate
2e-5
5e-5
Pre-train epoch
1
0
1
(Non-STG-RL)
Validation
500
50
500
Train epochs
16 (0.5%), 8 (1%), 4 (2%)
30
20 (0.5%), 10 (1%), 5 (2%)
Batch size
16
10
16"
REFERENCES,0.6859205776173285,Under review as a conference paper at ICLR 2022
REFERENCES,0.6895306859205776,"D
ILLUSTRATION OF STG"
REFERENCES,0.6931407942238267,"Figure 2: A simple schematic illustration of Non-STG and STG. Non-STG(RL): the whole se-
quence of target is generated from the task-speciﬁc policy πa so the right sub-sequence AB is also
penalized from the delayed feedback. STG: the third token is sampled from πa and the model lets
the other tokens (highlighted with cyan) generated from the PLM’s policy πLM which generates a
next letter of the previous alphabet input. Here, πa will be penalized at the third token."
REFERENCES,0.6967509025270758,"E
ADDITONAL STUDY"
REFERENCES,0.7003610108303249,"STG-MLE. Here, we evaluate the MLE version of STG (denoted as STG-MLE) which is trained
by MLE for the mixture policy πh(·) = πs(it = 0)πLM(·) + πs(it = 1)πa(·) similar to copy
mechanism (Gu et al., 2016). In few-shot training, the explicit use of PLM logits can efﬁciently
reduce the ﬁne-tuning loss especially when the adapter is light since the adapter can focus only
on the task-relevant part in generation. STG-RL5 learns to do this naturally by stochastic policy
sampling if the policy selector is initialized to perform uniform sampling. On the other hand, STG-
MLE can be easily collapsed to select only a task-speciﬁc policy (i.e. it = 1). This is because the
gradient ﬂows the additional model only and, unlike STG-RL, there is no chance to exploit diverse
paths during training in the teacher forcing manner. As shown in Figure 4, STG-MLE starts from
the same point of STG-RL but it collapsed to Non-STG-MLE."
REFERENCES,0.703971119133574,"Learning Curve. It is well known that the RL-tuning resolves the exposure bias of MLE-tuning. We
can expect that an additive learner of MLE would be affected by the exposure bias as well, and the
RL objective for additive learning resolves it. Here, we present some learning curves6 obtained from
training in our experiments. As shown in Figure 4, the learners of MLE seem to have overﬁtting (in
terms of Perplexity, PPL) and exposure bias (in terms of Score). On the other hand, the learners of
RL were less effected by the problems. We can ﬁnd that the STG models (denoted STG-RL) are
superior to the others from the perspective of the score."
REFERENCES,0.7075812274368231,"Effectiveness of Selector. Here, we investigate the effectiveness of the selector πs of the STG. We
compare Fixed Selection against the Dynamic selection. In the ﬁxed selection, the probability of
selecting the PLM’s policy πLM is ﬁxed to πs(0t|st) = 1−πs(1t|st). We measure the performance
with respect to πs(1t|st) = c where c is a constant. The selection will be uniformly random when
c = 0.5, and when c = 0, the performance will be equivalent to the performance of the PLM
without additive learning. Figure 3 shows that the input-dependent dynamic selection by our STG"
REFERENCES,0.7111913357400722,"5We add ”-RL” to the STG to distinguish with STG-MLE in this context.
6The curve for Data-to-Text is not presented since there is no actual validation set."
REFERENCES,0.7148014440433214,Under review as a conference paper at ICLR 2022
REFERENCES,0.7184115523465704,"outperforms the ﬁxed selection with any c. We can ﬁnd that how πs works for each task. For
instance, in QA task, the ﬁrst few tokens of an answer may decide the quality of generation (i.e.
”yes” or ”no” in binary QA). Therefore, an optimal strategy of the STG might be producing the ﬁrst
few tokens sampled from the task-speciﬁc πa and the remaining tokens from the PLM πLM. The
curve supports this interpretation since the score is decreased as c is close to 1. Our STG learns
such a strategy as shown from the generated answers in Table 11 and 12. In Data-to-Text, the BLEU
score is increased as c is close to 1 while the ERR score is decreased. This fact supports the results
of NE(mix) models as discussed in Section 4.6. The πs learns to balance between the BLEU and
ERR."
REFERENCES,0.7220216606498195,Figure 3: Dynamic selection vs Fixed selection.
REFERENCES,0.7256317689530686,Under review as a conference paper at ICLR 2022
REFERENCES,0.7292418772563177,"Figure 4: The learning curve. The Perplexity (PPL) and Score of each task (e.g. Rouge-L for
Summarization) are measured on the 0.5% few-shot train set and the valid set."
REFERENCES,0.7328519855595668,Under review as a conference paper at ICLR 2022
REFERENCES,0.7364620938628159,"F
GENERATED SENTENCE EXAMPLES"
REFERENCES,0.740072202166065,"Here, we show generated sentence examples for each task (see Table 10 for Data-to-Text, Table 11
and Table 12 for Question Answering and Table 13 and Table 14 for Summarization.). The tokens
sampled from the task-speciﬁc policy πa are presented in red."
REFERENCES,0.7436823104693141,Table 10: Data-to-Text examples from TV of FewShotWOZ dataset.
REFERENCES,0.7472924187725631,"Data
recommend(name=kratos 31;
type=television;
powerconsumption=31.4
watt; accessories=remote control and european warranty; color=mat black
bezel with mat black frame stand)"
REFERENCES,0.7509025270758123,"Ground-truth
the kratos 31 is a great television . it only consumes 31.4 watts , comes with a
remote control and european warranty and has a stylish mat black bezel with
mat black frame stand ."
REFERENCES,0.7545126353790613,"PLM
the kratos 31 features a mat black bezel with mat black frame stand and
european warranty."
REFERENCES,0.7581227436823105,"Non-STG-MLE
the kratos 31 features a mat black bezel with mat black frame stand and
european warranty"
REFERENCES,0.7617328519855595,"NE(mix)-MLE
the kratos 31 television has 31.4 watt power consumption, remote control and
european warranty."
REFERENCES,0.7653429602888087,"STG
the kratos 31 television has 31 watt power consumption. it comes with a
remote control and european warranty with a color of mat black bezel with
mat black frame stand."
REFERENCES,0.7689530685920578,"Data
inform(name=triton 52; type=television; family=l7; audio=cevo stereo;
screensize=47 inch)"
REFERENCES,0.7725631768953068,"Ground-truth
the triton 52 47 inch screen television from the l7 product family and has
cevo stereo audio."
REFERENCES,0.776173285198556,"PLM
the triton 52 television is a 47 inch television in the l7 family and with a cevo
stereo."
REFERENCES,0.779783393501805,"Non-STG-MLE
the triton 52 television is in the l7 family, it has 47 inches of screen, and uses
cevo stereo sound capabilities."
REFERENCES,0.7833935018050542,"NE(mix)-MLE
the triton 52 television is in the l7 family, it has 47 inches of screen, and a 47
euro price,"
REFERENCES,0.7870036101083032,"STG
the triton 52 television is a 47 inch television in the l7 family and has a cevo
stereo."
REFERENCES,0.7906137184115524,"Data
inform(name=attis
65;
type=television;
resolution=1080p;
pricerange=cheap)"
REFERENCES,0.7942238267148014,"Ground-truth
the attis 65 television is in the cheap price range and has a nice resolution of
1080p."
REFERENCES,0.7978339350180506,"PLM
the attis 65 television in the cheap price range has a 1080p resolution and a
pricerange of cheap."
REFERENCES,0.8014440433212996,"Non-STG-MLE
the attis 65 television has a 1080p resolution, is in the cheap price range and
has a nicam stereo."
REFERENCES,0.8050541516245487,"NE(mix)-MLE
the attis 65 television is a 1080p television in the cheap price range and in the
cheap price range with a nicam stereo."
REFERENCES,0.8086642599277978,"STG
the attis 65 television is in the cheap price range and has a 1080p resolution."
REFERENCES,0.8122743682310469,Under review as a conference paper at ICLR 2022
REFERENCES,0.8158844765342961,Table 11: Question Answering examples.
REFERENCES,0.8194945848375451,"Passage
zygote : this cell results from the union of an oocyte and a sperm . a zygote
is the beginning of a new human being ( i.e. , an embryo ) . the expression
fertilized ovum refers to a secondary oocyte that is impregnated by a sperm
; when fertilization is complete , the oocyte becomes a zygote.10 ( emphasis
added . )"
REFERENCES,0.8231046931407943,"Query
what is a human zygote"
REFERENCES,0.8267148014440433,"Ground-truth
the beginning of a new human being ."
REFERENCES,0.8303249097472925,"PLM
the expression fertilized ovum refers to a secondary oocyte that is impregnated
by a sperm ;"
REFERENCES,0.8339350180505415,"NE(mix)-MLE
the expression fertilized ovum refers to a secondary oocyte that is impregnated
by a sperm ; when fertilization is complete , the oocyte becomes a zygote ."
REFERENCES,0.8375451263537906,"NE(mix)-RL
the expression fertilized ovum refers to a secondary oocyte that is impregnated
by a sperm ; when fertilization is complete , the oocyte becomes a zygote ."
REFERENCES,0.8411552346570397,"STG
it is the beginning of a new human being ."
REFERENCES,0.8447653429602888,"Passage
sometimes the terms reactive airway disease and asthma are used interchange-
ably , but they are not the same thing . often , the term reactive airway disease
is used when asthma is suspected , but not yet conﬁrmed . reactive airway
disease in children is a general term that does n’t indicate a speciﬁc diagnosis
. it may be used to describe a history of coughing , wheezing or shortness of
breath triggered by infection . these signs and symptoms may or may not be
caused by asthma ."
REFERENCES,0.8483754512635379,"Query
symptoms of reactive airway disease in adults"
REFERENCES,0.851985559566787,"Ground-truth
coughing , wheezing or shortness of breath triggered by infection ."
REFERENCES,0.855595667870036,"PLM
symptoms of reactive airway disease in children is a general term that does n´t
indicate a speciﬁc diagnosis ."
REFERENCES,0.8592057761732852,"NE(mix)-MLE
symptoms of reactive airway disease in adults are a general term that does n´t
indicate a speciﬁc diagnosis ."
REFERENCES,0.8628158844765343,"NE(mix)-RL
yes , reactive airway disease in children is a general term that does n´t indicate
a speciﬁc diagnosis ."
REFERENCES,0.8664259927797834,"STG
coughing , wheezing or shortness of breath triggered by infection ."
REFERENCES,0.8700361010830325,"Passage
sulfate is an anion , composed of a sulfur atom and four oxygen atoms . the
overall charge of this anion is -2. the molecular formula for sulfate is so 4 2-.
there , two oxygen atoms are bonded to the sulfur atom through double bonds
and other two are bonded to sulfur through single bonds . the oxidation state
of sulfur in sulfate is +6 ."
REFERENCES,0.8736462093862816,"Query
is sulfate sulfur and oxygen bonded"
REFERENCES,0.8772563176895307,"Ground-truth
yes , sulfate is sulfur and oxygen bonded ."
REFERENCES,0.8808664259927798,"PLM
no answer present ."
REFERENCES,0.8844765342960289,"NE(mix)-MLE
yes , sulfate is sulfur and oxygen bonded to the sulfur atom through double
bonds and other two are bonded to sulfur through single bonds ."
REFERENCES,0.8880866425992779,"NE(mix)-RL
yes , sulfate sulfur and oxygen bonded to the sulfur atom through double
bonds and other two are bonded to sulfur through single bonds ."
REFERENCES,0.8916967509025271,"STG
yes , sulfate is sulfur and oxygen bonded to each other through double bonds
and other two are bonded to sulfur through single bonds ."
REFERENCES,0.8953068592057761,Under review as a conference paper at ICLR 2022
REFERENCES,0.8989169675090253,Table 12: Question Answering examples.
REFERENCES,0.9025270758122743,"Passage
gryfﬁndor house is one of the four houses at hogwart into which the students
are sorted when they start their ﬁrst year . gryfﬁndor is the hogwarts house of
founder godric gryfﬁndor . gryfﬁndor house places high value on traits such
as courage , chivalry , and nerve . during harry ’ s time at hogwarts , the head
of gryfﬁndor house was minerva [ ... ]"
REFERENCES,0.9061371841155235,"Query
what kind of house is a gryfﬁndor"
REFERENCES,0.9097472924187726,"Ground-truth
gryfﬁndor house is one of the four houses at hogwart into which the students
are sorted when they start their ﬁrst year ."
REFERENCES,0.9133574007220217,"PLM
gryfﬁndor house is one of the four houses at hogwart into which the students
are sorted when they start their ﬁrst year ."
REFERENCES,0.9169675090252708,"NE(mix)-MLE
no answer present ."
REFERENCES,0.9205776173285198,"NE(mix)-RL
no answer present ."
REFERENCES,0.924187725631769,"STG
gryfﬁndor house is one of the four houses at hogwarts into which the students
are sorted when they start their ﬁrst year ."
REFERENCES,0.927797833935018,"Passage
the median home value in louisville is $ 119,500 . louisville home values
have gone up 1.8 % over the past year and zillow predicts they will rise 3.0
% within the next year . the median list price per square foot in louisville is
$ 125 . the median price of homes currently listed in louisville is $ 180,000
. the median rent price in louisville is $ 1,050 . foreclosures will be a factor
impacting home values in the next several years . in louisville 0.1 homes are
foreclosed ( per 10,000 ) ."
REFERENCES,0.9314079422382672,"Query
average per therm price in louisville ky"
REFERENCES,0.9350180505415162,"Ground-truth
the median home value in louisville is $ 119,500 ."
REFERENCES,0.9386281588447654,"PLM
the average per therm price in louisville is $ 119,500 ."
REFERENCES,0.9422382671480144,"NE(mix)-MLE
$ 125 per square foot"
REFERENCES,0.9458483754512635,"NE(mix)-RL
the average per square foot in louisville is $ 125 ."
REFERENCES,0.9494584837545126,"STG
the median home value in louisville is $ 119,500 ."
REFERENCES,0.9530685920577617,"Passage
the brain is made up of more than just nerve cells ( neurons ) . although there
are about 100 billion neurons in the brain , there may be about 10 to 50 times
that many glial cells in the brain . neurons have two processes called axons
and dendrites .... glial cells have only one . 2 neurons can generate action
potentials ... glial cells can not . 3 however , glial cells do have a resting
potential . 4 neurons have synapses that use neurotransmitters ... glial cells
do not have chemical synapses ."
REFERENCES,0.9566787003610109,"Query
number of glial cells vs neurons"
REFERENCES,0.9602888086642599,"Ground-truth
10 to 50 times ."
REFERENCES,0.9638989169675091,"PLM
no answer present ."
REFERENCES,0.9675090252707581,"NE(mix)-MLE
$ 10 to 50 times that many glial cells in the brain ."
REFERENCES,0.9711191335740073,"NE(mix)-RL
no answer present ."
REFERENCES,0.9747292418772563,"STG
10 to 50 times"
REFERENCES,0.9783393501805054,Under review as a conference paper at ICLR 2022
REFERENCES,0.9819494584837545,Table 13: Text Summarization examples.
REFERENCES,0.9855595667870036,"Article
-lrb- cnn -rrb- – three runners died sunday during the detroit free
press/ﬂagstar marathon in detroit, michigan, police told cnn. an emt vehicle
is at the scene sunday in detroit after three runners collapsed at a marathon.
all three deaths occurred between 9 and 9:20 a.m. et, second deputy chief
john roach said. a man in his 60s fell and hit his head, roach said. the cause
of the fall was unknown. the man was transported to detroit receiving hos-
pital, where he was pronounced dead. two other men, ages 36 and 26, also
collapsed during the race and were pronounced dead at the hospital, roach
said. all three collapsed near the end of the race, he said. witnesses describe
scene ” the weather at the time was overcast, roach said, with temperatures
in the low 40s. [...]
Ground-truth
second deputy chief john roach : all three deaths occurred between 9 and
9:20 a.m. man in his 60s fell hit his head ; two men others , ages 36 and 26 ,
collapsed . race was detroit free press/ﬂagstar marathon in detroit , michigan
.
PLM
three runners collapsed at a marathon in detroit , police say . the cause of the
fall is unknown .
Non-STG-MLE
three runners collapsed at a marathon sunday , police say . the cause of the
fall is unknown , police say .
Non-STG-RL
three runners collapsed at a marathon sunday , police say . the cause of the
fall is unknown , police say .
STG
three runners collapsed at a marathon in detroit , michigan . all three deaths
occurred between 9 and 9:20 a.m. et . a man in his 60s fell and hit his head ,
police say ."
REFERENCES,0.9891696750902527,"Article
london, england -lrb- cnn -rrb- – up to 1,000 human rights campaigners
demonstrated saturday in front of no. 10 downing street, the ofﬁcial resi-
dence of british prime minister gordon brown, calling on the british govern-
ment to demand that full democracy be restored in pakistan. jemima khan,
center, ex-wife for former pakistani cricket star imran khan, joins protesters
in london. protesters waved placards and chanted in support of the resigna-
tion of pakistani president pervez musharraf, a week after he imposed a state
of emergency in the country. the crowd of demonstrators massed behind
barriers and included jemima khan, the ex-wife of former pakistani cricket
star turned politician imran khan. the demonstrators carried placards saying
“ free the innocent ” and “ end musharraf’s regime ” and waved pakistani
ﬂags. imran khan, who heads the the movement for justice party, has been
under house arrest since the emergency declaration. his ex-wife delivered a
petition to a doorman at downing street, calling on britain to use its inﬂu-
ence to ensure that all institutions are in place well in advance of pakistani
elections originally scheduled for early next year. the petition also [...]
Ground-truth
human rights campaigners demonstrate in front of no . 10 downing street .
protests urged uk government to demand full democracy restored in pakistan
. cricketer turned politician imran khan ´s ex wife jemima among protesters .
PLM
pakistani president pervez musharraf has been under house arrest since the
emergency declaration . his ex-wife delivered a petition to a doorman at
downing street .
Non-STG-MLE
pakistani president pervez musharraf has imposed a state of emergency in the
country . he has been under house arrest since the emergency declaration .
Non-STG-RL
pakistani president pervez musharraf has imposed a state of emergency in the
country . he has imposed a state of emergency in the country since last week
.
STG
the ofﬁcial residence of british prime minister gordon brown . hundreds of
protesters demonstrate in front of no . 10 downing street . the petition calls
for full democracy in pakistan ."
REFERENCES,0.9927797833935018,Under review as a conference paper at ICLR 2022
REFERENCES,0.9963898916967509,"Table 14: Text Summarization examples.
Article
-lrb- cnn -rrb- spoiler alert! it’s not just women getting cloned. that was
the big twist at the end of “ orphan black’s ” second season. the kickoff to
the new season leads the list of six things to watch in the week ahead. 1. “
orphan black, ” 9 p.m. et, saturday, april 18, bbc america. the cloning cult
sci-ﬁseries remains one of the most critically acclaimed shows on tv, thanks
in large part to the performance of tatiana maslany, who has taken on at least
six roles on the show so far, including a newly introduced transgender clone.
maslany told reporters this week that we can expect even more impressive
scenes with multiple clones. [...] 2. “ turn : washington’s spies, ” 9 p.m. et,
monday, amc. the series about spies in the early days of the revolutionary war
returns with a new subtitle, “ washington’s spies, ” and a new monday night
time slot. series star jamie bell told cnn what we can expect in the second
season. “ this year we have a lot more battles ; [...] 3. “ game of thrones, ” 9
p.m. et, sunday, hbo. the world of westeros returns for a ﬁfth season in one
of the biggest season premieres of the year. click here for more on what to
expect. [...]
Ground-truth
critically acclaimed series “ orphan black “ returns . “ turn : washington ´s
spies “ starts a second season . “ game of thrones “ is back for season ﬁve .
PLM
series returns with a new subtitle , “ washington ´s spies , “ and a new monday
night time slot . series star jamie bell told cnn what we can expect in the
second season .
Non-STG-MLE
“ orphan black “ returns with a new subtitle and a new monday night time
slot . series star tatiana maslany is blown away by the response to the series
so far .
Non-STG-RL
“ orphan black “ returns with a new subtitle and a new monday night time
slot . series star jamie bell says the series will bring more recognition to the
culper spy ring .
STG
the new season of “ orphan black “ premieres on saturday the 18th . “ game
of thrones “ returns for a ﬁfth season .
Article
energy drinks can pose a real problem for children, according to a new study
from the american heart association. researchers found that 40 % of the 5,156
calls to poison centers for “ energy drink exposure ” involved children under
age 6. in most of the cases, the parents didn’t know the children had gotten
hold of an energy drink. many of the calls reported the children were expe-
riencing serious side effects, such as an abnormal heart rhythm, or they were
having a seizure. the study is being presented at the american heart associa-
tion’s scientiﬁc sessions this week. study : caffeine affects teen boys more.
study author dr. steven lipshultz has handled cases involving children who
became sick after consuming energy drinks. while studies about the impact
of caffeine on children are limited, lipshultz, [...]
Ground-truth
40 % of “ energy drink exposure “ calls to poison control centers are for
children under 6. study authors believe the number is under-counted . the fda
does not set a caffeine limit for children .
PLM
drug calls to poison centers for “ energy drink exposure “ in most cases , the
parents did n’t know the children had gotten hold of an energy drink . many
of the calls reported the children were experiencing serious side effects , such
as an abnormal heart rhythm .
Non-STG-MLE
drug calls to poison centers for “ energy drink exposure “ in most cases , the
parents did n’t know the children had gotten hold of an energy drink . many
of the calls reported the children were experiencing serious side effects , such
as an abnormal heart rhythm .
Non-STG-RL
“ this is a very concerning ﬁnding , “ dr. laurence sperling says . the american
academy of pediatrics recommends children consume no caffeine .
STG
drug calls to poison centers for “ energy drink exposure “ nearly 40 % of calls
to poison centers for “ energy drink exposure “ involved children under age
6. study : caffeine affects teens more ."
