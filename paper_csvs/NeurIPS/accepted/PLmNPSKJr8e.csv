Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00510204081632653,"We study the problem of best arm identification in linear bandits in the fixed-budget
setting. By leveraging properties of the G-optimal design and incorporating it into
the arm allocation rule, we design a parameter-free algorithm, Optimal Design-
based Linear Best Arm Identification (OD-LinBAI). We provide a theoretical
analysis of the failure probability of OD-LinBAI. Instead of all the optimality gaps,
the performance of OD-LinBAI depends only on the gaps of the top d arms, where
d is the effective dimension of the linear bandit instance. Complementarily, we
present a minimax lower bound for this problem. The upper and lower bounds
show that OD-LinBAI is minimax optimal up to constant multiplicative factors in
the exponent, which is a significant theoretical improvement over existing methods
(e.g., BayesGap, Peace, LinearExploration and GSE), and settles the question
of ascertaining the difficulty of learning the best arm in the fixed-budget setting.
Finally, numerical experiments demonstrate considerable empirical improvements
over existing algorithms on a variety of real and synthetic datasets."
INTRODUCTION,0.01020408163265306,"1
Introduction"
INTRODUCTION,0.015306122448979591,"The multi-armed bandit problem is a model that exemplifies the exploration-exploitation tradeoff in
online decision making. It has various applications in drug design, online advertising, recommender
systems, and so on. In stochastic multi-armed bandit problems, the agent sequentially chooses an
arm from the given arm set at each time step and then observes a random reward drawn from the
unknown distribution associated with the chosen arm."
INTRODUCTION,0.02040816326530612,"The standard multi-armed bandit problem, where the arms are not correlated with one another,
has been studied extensively in the literature. While the regret minimization problem aims at
maximizing the cumulative rewards by the trade-off between exploration and exploitation [1–4], the
pure exploration problem focuses on efficient exploration with specific goals, e.g., to identify the
best arm [5–11]. There are two complementary settings for the problem of best arm identification:
(i) Given T ∈N, the agent aims to maximize the probability of finding the best arm in at most T
time steps; (ii) Given δ > 0, the agent aims to find the best arm with the probability of at least
1 −δ in the smallest number of steps. These settings are respectively known as the fixed-budget and
fixed-confidence settings."
INTRODUCTION,0.025510204081632654,"In this paper, we consider the problem of best arm identification in linear bandits in the fixed-budget
setting. In linear bandits, the arms are correlated through an unknown global regression parameter
vector θ∗∈Rd. In particular, each arm i from the arm set A is associated with an arm vector
a(i) ∈Rd, and the expected reward of arm i is given by the inner product between θ∗and a(i).
Hence, the standard multi-armed bandits and linear bandits are fundamentally different due to the
fact that for the latter, pulling one arm can indirectly reveal information about the other arms but in
the former, the arms are independent."
INTRODUCTION,0.030612244897959183,"A wide range of applications in practice can be modeled by linear bandits. For example, Tao et al. [12]
considered online advertising, where the goal is to select an advertisement from a pool to maximize
the probability of clicking for web users with different features. Empirically, the probability of
clicking can be approximated by a linear combination of various attributes associated with the
user and the advertisements (such as age, gender, the domain, keywords, advertising genres, etc.).
Moreover, Hoffman et al. [13] applied the linear bandit model into the traffic sensor network problem
and the problem of automatic model selection and algorithm configuration."
INTRODUCTION,0.03571428571428571,Main contributions. Our main contributions are as follows:
INTRODUCTION,0.04081632653061224,"(i) We design an algorithm Optimal Design-based Linear Best Arm Identification (OD-LinBAI).
This computationally efficient algorithm utilizes a phased elimination-based strategy in
which the number of times each arm is pulled in each phase depends on G-optimal designs
[14]. Besides, OD-LinBAI is totally parameter-free, whereas some existing methods (e.g.,
BayesGap and Peace) require the knowledge of the problem instance (which is typically not
known in practice).
(ii) We derive an upper bound on the failure probability of OD-LinBAI. The failure probability
is a significant improvement over those of existing methods which we survey in detail in
Section 4.2. In particular, we show that the exponent of the failure probability depends on a
hardness quantity H2,lin. This quantity is a function of only the first d −1 optimality gaps,
where d is the dimension of the arm vectors. This is a surprising and significant difference
compared to the upper bounds of the failure probabilities of various algorithms for best
arm identification in standard multi-armed bandits [7, 9, 15] and BayesGap [13] in linear
bandits, which all depend on a hardness quantity that depends on all the gaps. Moreover,
OD-LinBAI improves the exponent of the error probability by a factor of Θ(log d) over
Peace [16] in the worst-case sense or a factor of Θ((log K)/(log d)) (which could be much
larger than 1) over LinearExploration [17] and GSE [18] in general.
(iii) Lastly, using ideas from Carpentier and Locatelli [10], we prove a minimax lower bound
which involves another hardness quantity H1,lin. By comparing H1,lin to H2,lin, we show
that OD-LinBAI is minimax optimal up to constants in the exponent. OD-LinBAI is the first
algorithm that provably achieves minimax optimality in this problem, and finally settles the
question of ascertaining the hardness of learning the best arm in the fixed-budget setting
for linear bandits. In addition, experiments in both synthetic and real-world datasets firmly
corroborate the efficacy of OD-LinBAI vis-à-vis other existing methods."
INTRODUCTION,0.04591836734693878,"Related work. The problem of regret minimization in linear bandits was first studied by Abe and
Long [19], and has attracted extensive interest in the development of various algorithms (e.g., UCB-
style algorithms [20–24], Thompson sampling [25, 26]). In particular, in the book of Lattimore and
Szepesvári [27], a regret minimization algorithm based on the G-optimal design was proposed for
linear bandits with finitely many arms. Although both this algorithm and our algorithm OD-LinBAI
utilize the G-optimal design technique, they differ in numerous aspects including the manner of
elimination and arm allocation, which emanates from the two different objectives."
INTRODUCTION,0.05102040816326531,"For the problem of best arm identification in linear bandits, the fixed-confidence setting has previously
been studied in [12, 28–34]. In particular, Soare et al. [28] introduced the optimal G-allocation
problem and proposed a static algorithm XY-Oracle as well as a semi-adaptive algorithm XY-
Adaptive; see Remark 2 for more discussions on Soare et al. [28]. Degenne et al. [32] treated the
problem as a two-player zero-sum game between the agent and the nature, and thus designed an
asymptotically optimal algorithm for the fixed-confidence setting."
INTRODUCTION,0.05612244897959184,"The fixed-budget setting for the problem of best arm identification in linear bandits has also been
studied in a few previous and concurrent works. Hoffman et al. [13] introduced a gap-based explo-
ration algorithm BayesGap, which is a Bayesian treatment of UGapEb [8] for standard multi-armed"
INTRODUCTION,0.061224489795918366,"bandits. Peace by Katz-Samuels et al. [16] utilizes an experimental design based on the Gaussian-
Width of the underlying arm set, which characterizes the geometry of the instance better in some
instances. However, both BayesGap and Peace are computationally expensive and not parameter-free.
Recently, Alieva et al. [17] introduced an elimination algorithm named LinearExploration, which is
also robust to moderate levels of model misspecification. Generalized Successive Elimination (GSE)
by Azizi et al. [18] shares a similar structure with LinearExploration and applies to generalized linear
models. Nevertheless, none of the above is minimax optimal. See Section 4 and Section 5 for more
comparisons between OD-LinBAI and other existing algorithms."
PROBLEM SETUP AND PRELIMINARIES,0.0663265306122449,"2
Problem setup and preliminaries"
PROBLEM SETUP AND PRELIMINARIES,0.07142857142857142,"Best arm identification in linear bandits. We consider the standard linear bandit problem with an
unknown global regression parameter. In a linear bandit instance ν, the agent is given an arm set
A = [K], which corresponds to known arm vectors {a(1), a(2), . . . , a(K)} ⊂Rd. At each time t,
the agent chooses an arm At from the arm set A and then observes a noisy reward
Xt = ⟨θ∗, a(At)⟩+ ηt
where θ∗∈Rd is the unknown parameter vector and ηt is independent zero-mean 1-subgaussian
random noise."
PROBLEM SETUP AND PRELIMINARIES,0.07653061224489796,"In the fixed-budget setting, given a time budget T ∈N, the agent aims at maximizing the probability
of identifying the best arm, i.e., the arm with the largest expected reward, with no more than T arm
pulls. More formally, the agent uses an online algorithm Π to decide the arm AΠ
t to pull at each time
step t, and the arm iΠ
out ∈A to output as the identified best arm by time T. We abbreviate AΠ
t as At
and iΠ
out as iout when there is no ambiguity."
PROBLEM SETUP AND PRELIMINARIES,0.08163265306122448,"For any arm i ∈A, let p(i) = ⟨θ∗, a(i)⟩denote the expected reward. For convenience, we assume
that the expected rewards of the arms are in descending order and the best arm is unique. That is
to say, p(1) > p(2) ≥· · · ≥p(K). For any suboptimal arm i, we denote ∆i = p(1) −p(i) as the
optimality gap. For ease of notation, we also set ∆1 = ∆2. Furthermore, let E denote the set of all
the linear bandit instances defined above."
PROBLEM SETUP AND PRELIMINARIES,0.08673469387755102,"Dimensionality-reduced arm vectors. For any linear bandit instance, if the corresponding arm
vectors do not span Rd, i.e., span({a(1), a(2), . . . , a(K)}) ⊊Rd, the agent can work with a set
of dimensionality-reduced arm vectors {a′(1), a′(2), . . . , a′(K)} ⊂Rd′, that spans Rd′, with little
consequence. Specifically, let B ∈Rd×d′ be a matrix whose columns form an orthonormal basis
of the subspace spanned by a(1), a(2), . . . , a(K).1 Then the agent can simply set a′(i) = B⊤a(i)
for each arm i. To verify this, notice that BB⊤is a projection matrix onto the subspace spanned by
{a(1), a(2), . . . , a(K)} and consequently"
PROBLEM SETUP AND PRELIMINARIES,0.09183673469387756,"p(i) = ⟨θ∗, a(i)⟩= ⟨θ∗, BB⊤a(i)⟩= ⟨B⊤θ∗, B⊤a(i)⟩= ⟨θ∗′, a′(i)⟩."
PROBLEM SETUP AND PRELIMINARIES,0.09693877551020408,"Note that θ∗is the unknown parameter vector for original arm vectors while θ∗′ = B⊤θ∗is the
corresponding unknown parameter vector for the dimensionality-reduced arm vectors. In the problem
of linear bandits, what we really care about is not the original unknown parameter θ∗itself but the
inner products between θ∗and the arm vectors a(i), which establishes the equivalence of original
arm vectors and dimensionality-reduced arm vectors."
PROBLEM SETUP AND PRELIMINARIES,0.10204081632653061,"In our work, without loss of generality, we assume that the entire set of original arm vectors
{a(1), a(2), . . . , a(K)} span Rd and d ≥2.2 However, this idea of transforming into dimensionality-
reduced arm vectors is often used in our elimination-based algorithm. See Section 3 for details."
PROBLEM SETUP AND PRELIMINARIES,0.10714285714285714,"Least squares estimators. Let A1, A2, . . . , An be the sequence of arms pulled by the agent and
X1, X2, . . . , Xn be the corresponding noisy rewards. Suppose that the corresponding arm vectors
{a(A1), a(A2), . . . , a(An)} span Rd, then the ordinary least squares (OLS) estimator of θ∗is given
by"
PROBLEM SETUP AND PRELIMINARIES,0.11224489795918367,"ˆθ = V −1
n
X"
PROBLEM SETUP AND PRELIMINARIES,0.11734693877551021,"t=1
a(At)Xt"
PROBLEM SETUP AND PRELIMINARIES,0.12244897959183673,"1Such an orthonormal basis can be calculated efficiently with the reduced singular value decomposition,
Gram–Schmidt process, etc.
2The situation that d = 1 is trivial: each arm vector is a scalar multiple of one another."
PROBLEM SETUP AND PRELIMINARIES,0.12755102040816327,"where V = Pn
t=1 a(At)a(At)⊤∈Rd×d is invertible. By applying the properties of subgaussian
random variables, a confidence bound for the OLS estimator can be derived as follows.
Proposition 1 (Lattimore and Szepesvári [27, Chapter 20]). If A1, A2, . . . , An are deterministically
chosen without knowing the realizations of X1, X2, . . . , Xn, then for any a ∈Rd and δ > 0, Pr """
PROBLEM SETUP AND PRELIMINARIES,0.1326530612244898,"⟨ˆθ −θ∗, a⟩≥ s"
PROBLEM SETUP AND PRELIMINARIES,0.1377551020408163,"2∥a∥2
V −1 log
1 δ # ≤δ."
PROBLEM SETUP AND PRELIMINARIES,0.14285714285714285,"Remark 1. When the arm pulls are adaptively chosen according to the random rewards, Proposition 1
no longer applies and an extra factor
√"
PROBLEM SETUP AND PRELIMINARIES,0.14795918367346939,"d has to be paid for adaptive arm pulls [23]. Our algorithm
avoids this issue by deciding the arm pulls at the beginning of each phase, and designing the OLS
estimator only based on the information from the current phase. See Section 3 for details."
PROBLEM SETUP AND PRELIMINARIES,0.15306122448979592,"G-optimal design. The confidence interval in Proposition 1 shows the strong connection between
the arm allocation in linear bandits and experimental design theory [35]. To control the confidence
bounds, we first introduce the G-optimal design technique into the problem of best arm identification
in linear bandits in the fixed-budget setting. Formally, the G-optimal design problem aims at finding
a probability distribution π : {a(i) : i ∈A} →[0, 1] that minimises
g(π) = max
i∈A ∥a(i)∥2
V (π)−1"
PROBLEM SETUP AND PRELIMINARIES,0.15816326530612246,where V (π) = P
PROBLEM SETUP AND PRELIMINARIES,0.16326530612244897,"i∈A π(a(i))a(i)a(i)⊤. Theorem 1 states the existence of a small-support G-optimal
design and the minimum value of g.
Theorem 1 (Kiefer and Wolfowitz [14]). If the arm vectors {a(i) : i ∈A} span Rd, the following
statements are equivalent: (i) π∗is a minimiser of g; (ii) π∗is a maximiser of f(π) = log det V (π);
(iii) g(π∗) = d. Furthermore, there exists a minimiser π∗of g such that | Supp (π∗) | ≤d(d + 1)/2.
Remark 2. It is worth mentioning that the G-optimal design problem for finite arm vectors is a
convex optimization problem while the original G-allocation problem in Soare et al. [28] for the
fixed-confidence best arm identification in linear bandits is an NP-hard discrete optimization problem.
A classical algorithm to solve the G-optimal design problem is the Frank–Wolfe algorithm [36],
whose modified version guarantees linear convergence [37]. For our work, it is sufficient to compute
an ϵ-approximate optimal design3 with minimal impact on performance. Recently, a near-optimal
design with smaller support was proposed in Lattimore et al. [38], which might be helpful in some
scenarios. See Appendix A for more discussions on the above issues. To reduce clutter and ease the
reading, henceforward in the main text, we assume that a G-optimal design for finite arm vectors can
be found accurately and efficiently."
ALGORITHM,0.1683673469387755,"3
Algorithm"
ALGORITHM,0.17346938775510204,"Pseudocode for our algorithm Optimal Design-based Linear Best Arm Identification (OD-LinBAI) is
presented in Algorithm 1."
ALGORITHM,0.17857142857142858,"The algorithm partitions the whole horizon into ⌈log2 d⌉phases, and maintains an active arm set Ar
in each phase r. The length of each phase roughly equals m, which will be formally defined in (1)."
ALGORITHM,0.1836734693877551,"Motivated by the equivalence of the original arm vectors and the dimensionality-reduced arm vectors,
at the beginning of each phase r, the algorithm computes a set of dimensionality-reduced arm vectors
{ar(i) : i ∈Ar−1} ⊂Rdr which spans the dr-dimensional Euclidean space Rdr. This can be
implemented based on the dimensionality-reduced arm vectors of the last phase {ar−1(i) : i ∈Ar−1}
in an iterative manner (Lines 5 −11)."
ALGORITHM,0.18877551020408162,"After that, Algorithm 1 finds a G-optimal design πr for the current dimensionality-reduced arm
vectors, with a restriction on the cardinality of the support when r = 1. OD-LinBAI then pulls
each arm in Ar−1 according to the proportions specified by the optimal design πr. Specifically, the
algorithm chooses each arm i ∈Ar−1 exactly Tr(i) = ⌈πr(ar(i)) · m⌉times, where the parameter
m is fixed among different phases and defined as"
ALGORITHM,0.19387755102040816,"m =
T −min(K, d(d+1)"
ALGORITHM,0.1989795918367347,"2
) −
⌈log2 d⌉−1
P r=1  d"
R,0.20408163265306123,"2r
"
R,0.20918367346938777,"⌈log2 d⌉
.
(1)"
R,0.21428571428571427,"3For an ϵ-approximate optimal design π, g(π) ≤(1 + ϵ)d."
R,0.2193877551020408,Algorithm 1 Optimal Design-based Linear Best Arm Identification (OD-LinBAI)
R,0.22448979591836735,"Input: time budget T, arm set A = [K] and arm vectors {a(1), a(2), . . . , a(K)} ⊂Rd."
R,0.22959183673469388,"1: Initialize t0 = 1, A0 ←A and d0 = d.
2: For each arm i ∈A0, set a0(i) = a(i).
3: Calculate m using Equation (1).
4: for r = 1 to ⌈log2 d⌉do
5:
Set dr = dim (span ({ar−1(i) : i ∈Ar−1})).
6:
if dr = dr−1 then
7:
For each arm i ∈Ar−1, set ar(i) = ar−1(i).
8:
else
9:
Find matrix Br ∈Rdr−1×dr whose columns form a orthonormal basis of the subspace
spanned by {ar−1(i) : i ∈Ar−1}.
10:
For each arm i ∈Ar−1, set ar(i) = B⊤
r ar−1(i).
11:
end if
12:
if r = 1 then
13:
Find a G-optimal design πr : {ar(i) : i ∈Ar−1} →[0, 1] with | Supp (πr) | ≤d(d+1)"
R,0.23469387755102042,"2
.
14:
else
15:
Find a G-optimal design πr : {ar(i) : i ∈Ar−1} →[0, 1].
16:
end if
17:
Set
Tr(i) = ⌈πr(ar(i)) · m⌉
and
Tr =
X"
R,0.23979591836734693,"i∈Ar−1
Tr(i)."
R,0.24489795918367346,"18:
Choose each arm i ∈Ar−1 exactly Tr(i) times.
19:
Calculate the OLS estimator:"
R,0.25,"ˆθr = V −1
r"
R,0.25510204081632654,"tr+Tr−1
X"
R,0.2602040816326531,"t=tr
ar(At)Xt
with
Vr =
X"
R,0.2653061224489796,"i∈Ar−1
Tr(i)ar(i)ar(i)⊤."
R,0.27040816326530615,"20:
For each arm i ∈Ar−1, estimate the expected reward:"
R,0.2755102040816326,"ˆpr(i) = ⟨ˆθr, ar(i)⟩."
R,0.28061224489795916,"21:
Let Ar be the set of ⌈d/2r⌉arms in Ar−1 with the largest estimates of the expected rewards.
22:
Set tr+1 = tr + Tr.
23: end for
Output: the only arm iout in A⌈log2 d⌉."
R,0.2857142857142857,"Note that m = Θ(T/ log2 d) as T →∞with K fixed. Lemma 1 in Appendix B shows with such
choice of m, the total time budget consumed by the agent is no more than T. The parameter m plays
a significant role in the implementation as well as the theoretical analysis of Algorithm 1."
R,0.29081632653061223,"Since the support of the G-optimal design πr must span Rdr, the OLS estimator can be directly
applied (Line 19). Then for each arm i ∈Ar−1, an estimate of the expected reward is derived.
Algorithm 1 decouples the estimates of different phases and only utilizes the information obtained in
the current phase r."
R,0.29591836734693877,"At the end of each phase r, Algorithm 1 eliminates a subset of possibly suboptimal arms. In particular,
K −⌈d/2⌉arms are eliminated in the first phase and about half of the active arms are eliminated in
each of the following phases. Eventually, there is only single arm iout in the active set, which is the
output of Algorithm 1.
Remark 3. It is worth considering the case of standard multi-armed bandits, which can be modeled
as a special case of linear bandits. In particular, for any arm i ∈A = [K], the corresponding arm
vector is chosen to be ei, which is the ith standard basis vector of RK. It follows that d = K,
θ∗= [p(1), p(2), . . . , p(K)]⊤∈RK and arms are not correlated with one another. A simple
mathematical derivation shows that we can always use a set of standard basis vectors of Rdr to
represent the arm vectors regardless of which arms remain active during phase r. Also, the G-optimal
design for a set of standard basis vectors is the uniform distribution on all of the active arms. Since"
R,0.3010204081632653,"pulling one arm does not provide information about the other arms, the empirical estimates based on
the OLS estimator are exactly the empirical means. Altogether, for standard multi-armed bandits,
OD-LinBAI reduces to the procedure of Sequential Halving [9], which is a state-of-the-art algorithm
for best arm identification in standard multi-armed bandits in the fixed-budget setting.
Remark 4. The G-optimal design steps in Lines 13 and 15 in OD-LinBAI may be replaced by the
XY-allocation [28] or other techniques in experimental designs. However, our work focuses on
establishing minimax optimality and thus the application of G-optimal designs, which optimize over
the worst cases, is natural. The XY-allocation may result in better empirical performance but the
improvement might be limited or even absent in worst-case scenarios. More importantly, as noted
in Degenne et al. [32, Remark 1], for the general XY-allocation problem, only heuristic solutions
can be obtained (without convergence guarantees). Nevertheless, the G-optimal design problem
can be provably solved with a linear convergence guarantee [37]. Overall, the implementation of
OD-LinBAI is computationally very efficient."
MAIN RESULTS,0.30612244897959184,"4
Main results"
UPPER BOUND,0.3112244897959184,"4.1
Upper bound"
UPPER BOUND,0.3163265306122449,"We first state an upper bound on the error probability of OD-LinBAI (Algorithm 1). The proof of
Theorem 2 is deferred to Appendix B."
UPPER BOUND,0.32142857142857145,"Theorem 2. For any linear bandit instance ν ∈E, OD-LinBAI outputs an arm iout satisfying"
UPPER BOUND,0.32653061224489793,"Pr [iout ̸= 1] ≤
4K"
UPPER BOUND,0.33163265306122447,"d + 3 log2 d

exp

−
m
32H2,lin "
UPPER BOUND,0.336734693877551,where m is defined in Equation (1) and
UPPER BOUND,0.34183673469387754,"H2,lin = max
2≤i≤d
i
∆2
i
."
UPPER BOUND,0.3469387755102041,Theorem 2 shows the error probability of OD-LinBAI is upper bounded by
UPPER BOUND,0.3520408163265306,"exp

−Ω

T
H2,lin log2 d"
UPPER BOUND,0.35714285714285715,"
(2)"
UPPER BOUND,0.3622448979591837,"which depends on T, d and H2,lin. We remark that none of the three terms is avoidable in view of our
lower bounds (see Section 4.3)."
UPPER BOUND,0.3673469387755102,"In particular, T is the time budget of the problem and d is the effective dimension of the arm vectors.4
Given T and d, H2,lin quantifies the difficulty of identifying the best arm in the linear bandit instance.
The parameter H2,lin generalizes its analogue"
UPPER BOUND,0.37244897959183676,"H2 = max
2≤i≤K
i
∆2
i"
UPPER BOUND,0.37755102040816324,"proposed by Audibert et al. [7] for standard multi-armed bandits. However, H2,lin is not larger than
H2 since H2,lin is only a function of the first d −1 optimality gaps while H2 considers all of the
K −1 optimality gaps. In the extreme case that all of the suboptimal arms have the same optimality
gaps, i.e., ∆2 = ∆3 = · · · = ∆K, the two terms H2 and H2,lin can differ significantly. In general,
we have"
UPPER BOUND,0.3826530612244898,"H2,lin ≤H2 ≤K"
UPPER BOUND,0.3877551020408163,"d H2,lin"
UPPER BOUND,0.39285714285714285,"and both inequalities are essentially sharp, i.e., can be achieved by some linear bandit instances. This
highlights a major difference between best arm identification in the fixed-budget setting for linear
bandits and standard multi-armed bandits. Due to the linear structure, arms are correlated and we can
estimate the mean reward of one arm with the help of the other arms. Thus, the hardness quantity
H2,lin is only a function of the top d arms rather than all the arms."
UPPER BOUND,0.3979591836734694,"4Recall that we assume the entire set of original arm vectors {a(1), a(2), . . . , a(K)} span Rd."
COMPARISONS TO OTHER ALGORITHMS,0.4030612244897959,"4.2
Comparisons to other algorithms"
COMPARISONS TO OTHER ALGORITHMS,0.40816326530612246,"We compare OD-LinBAI and other existing algorithms with respect to the algorithm design as well
as the theoretical guarantees in the following."
COMPARISONS TO OTHER ALGORITHMS,0.413265306122449,Comparisons to BayesGap [13].
COMPARISONS TO OTHER ALGORITHMS,0.41836734693877553,"(i) The model used in BayesGap [13] is based on Bayesian linear bandits, where the unknown
parameter vector θ∗is drawn from a known prior distribution N(0, η2I) and the additive
noise is required to be Gaussian. However, OD-LinBAI does not require these assumptions
and the upper bound holds for any deterministic or random θ∗∈Rd.
(ii) The algorithm and theoretical guarantee of BayesGap explicitly require the knowledge of a
hardness quantity H1 = P"
COMPARISONS TO OTHER ALGORITHMS,0.42346938775510207,"1≤i≤K ∆−2
i
to control the confidence region and then allocate
exploration. However, this hardness quantity H1 is almost always unknown to the agent in
practice. In most practical applications, BayesGap has to estimate H1 in an adaptive way,
which works reasonably well in numerical experiments but lacks theoretical guarantees.
(iii) BayesGap’s error probability is upper bounded by"
COMPARISONS TO OTHER ALGORITHMS,0.42857142857142855,"exp

−Ω
 T H1"
COMPARISONS TO OTHER ALGORITHMS,0.4336734693877551,"
(3)"
COMPARISONS TO OTHER ALGORITHMS,0.4387755102040816,"which depends on T and H1. Compared with (3), the upper bound of OD-LinBAI in (2) has
an extra log2 d term. This is an interesting phenomena which is also present in standard
multi-armed bandits [7, 10]. For best arm identification in standard multi-armed bandits,
without the knowledge of the hardness quantity H1, the agent has to pay a price of log2 K
for the adaptation to the problem complexity. In Theorem 3, we prove a similar result for
linear bandits, in which the price of adaptation is log2 d.
The upper bound (3) involves H1, a function of all the optimality gaps. It holds that
H1 ≥H2 ≥H2,lin. Thus, the upper bound of OD-LinBAI is not worse (and often better) in
its dependence on the hardness/complexity parameter."
COMPARISONS TO OTHER ALGORITHMS,0.44387755102040816,Comparisons to Peace [16] (Also see Appendix C).
COMPARISONS TO OTHER ALGORITHMS,0.4489795918367347,"(i) To ensure there is only a single arm in the final active set, the fixed-budget version of Peace
requires γ({a(i), a(1)}) ≥1 for all suboptimal arms i ̸= 1 (where γ(·) is defined in Katz-
Samuels et al. [16]). Note that this is not only a requirement for the theoretical bound but
also a requirement for the feasibility of the algorithm. If this inequality is not satisfied, the
linear bandit instance needs to be “rescaled” before the algorithm is run, resulting in a larger
bound on the error probability. In practice, the best arm is unknown and the rescaling factor
can thus only be conservatively bounded as mini γ({a(i), a(1)}) ≥mini,j γ({a(i), a(j)}).
However, the latter quantity can be miniscule. In particular, if there exist two arms that are
nearly identical, i.e., mini,j γ({a(i), a(j)}) is very small, the bound on the error probability
may be larger than 1, and hence vacuous. Besides, the algorithm may terminate with most of
its time budget wasted. In contrast, OD-LinBAI is fully parameter-free and does not require
any information about the instance.
(ii) It is not straightforward to compare the error probabilities of OD-LinBAI and Peace in
general since Peace involves some tricky terms that do not admit closed-form expressions.
Here we consider the special case of standard multi-armed bandits (as discussed in Remark 3)
with all optimality gaps equal to the minimal one ∆1. In this case ρ∗= Θ(∆−2
1
· d),
γ∗= Θ(∆−2
1
· d log d) and log(γ(Z)) = Θ(log d); these terms appear in the denominator
of the exponent in Peace’s bound on the error probability. Therefore, the error probability
of Peace is exp
 
−Ω(
T ∆2
1
d log2 d)

while ours is exp
 
−Ω( T ∆2
1
d log d)

, which also shows Peace
is not minimax optimal in the exponent in view of our lower bounds, to be presented in
Section 4.3. See Appendix C for the precise details of the above derivations."
COMPARISONS TO OTHER ALGORITHMS,0.45408163265306123,Comparisons to LinearExploration [17] and GSE [18].
COMPARISONS TO OTHER ALGORITHMS,0.45918367346938777,"(i) The idea of elimination has been well-received and is ubiquitous in linear bandits. Although
LinearExploration [17], GSE [18] and OD-LinBAI all leverage this idea, we emphasize that
the elimination criteria for these algorithms are different. In particular, OD-LinBAI divides
the time budget into roughly log2 d phases while the other algorithms divide the budget
into roughly log2 K phases. Additionally, OD-LinBAI always controls the dimension of the
active set in each phase, using the dimensionality reduction techinique in Section 2."
COMPARISONS TO OTHER ALGORITHMS,0.4642857142857143,"Table 1: Comparisons of different hardness quantities: H1, H2, H1,lin and H2,lin."
COMPARISONS TO OTHER ALGORITHMS,0.46938775510204084,H1 = P
COMPARISONS TO OTHER ALGORITHMS,0.4744897959183674,"1≤i≤K ∆−2
i
H2 = max2≤i≤K i · ∆−2
i
1 ≤H1/H2 ≤log(2K) [7]"
COMPARISONS TO OTHER ALGORITHMS,0.47959183673469385,"H1,lin = P"
COMPARISONS TO OTHER ALGORITHMS,0.4846938775510204,"1≤i≤d ∆−2
i
H2,lin = max2≤i≤d i · ∆−2
i
1 ≤H1,lin/H2,lin ≤log(2d)"
COMPARISONS TO OTHER ALGORITHMS,0.4897959183673469,"1 ≤H1/H1,lin ≤K/d
1 ≤H2/H2,lin ≤K/d"
COMPARISONS TO OTHER ALGORITHMS,0.49489795918367346,"(ii) The error probabilities of LinearExploration and GSE are upper bounded by exp
 
−"
COMPARISONS TO OTHER ALGORITHMS,0.5,"Ω(
T
˜
H2 log2 K )

and exp
 
−Ω(
T ∆2
1
d log2 K )

respectively. Note that K ≥d, H2,lin ≤d/∆2
1,"
COMPARISONS TO OTHER ALGORITHMS,0.5051020408163265,"and the hardness quantity ˜H2 in Alieva et al. [17] is of the same order as H2,lin. Hence, our
exponent of the bound on the error probability is an improvement over their exponents by a
factor of Θ((log2 K)/(log2 d)), which may be much larger than 1."
LOWER BOUND,0.5102040816326531,"4.3
Lower bound"
LOWER BOUND,0.5153061224489796,"Before stating the lower bound formally, we introduce"
LOWER BOUND,0.5204081632653061,"H1,lin =
X"
LOWER BOUND,0.5255102040816326,"1≤i≤d
∆−2
i ."
LOWER BOUND,0.5306122448979592,"This quantity is a generalization of H1 that characterizes the difficulty of a linear bandit instance.
This parameter is also associated with the top d arms similarly to H2,lin. See Table 1 for a thorough
comparison on different hardness quantities."
LOWER BOUND,0.5357142857142857,"For any linear bandit instance ν ∈E, we denote the hardness quantity H1,lin of ν as H1,lin(ν).5 In
addition, let E(h) denote the set of linear bandit instances in E whose hardness parameter H1,lin is
upper bounded by h (for some h > 0), i.e., E(h) = {ν ∈E : H1,lin(ν) ≤h}."
LOWER BOUND,0.5408163265306123,"Theorem 3. If T ≥h2 log(6Td)/900, then"
LOWER BOUND,0.5459183673469388,"min
Π
max
ν∈E(h) Pr

iΠ
out ̸= 1

≥1"
EXP,0.5510204081632653,"6 exp

−240T h 
."
EXP,0.5561224489795918,"Further if h ≥15d2, then"
EXP,0.5612244897959183,"min
Π
max
ν∈E(h)"
EXP,0.5663265306122449,"
Pr

iΠ
out ̸= 1

· exp

2700T
H1,lin(ν) log2 d 
≥1 6."
EXP,0.5714285714285714,"The proof of Theorem 3 is deferred to Appendix D. We emphasize that even though the proof of
the lower bound follows some common ideas behind the proofs of most minimax lower bounds in
bandit algorithms for various purposes, its value does not lie in its technical novelty, but rather that
the result is tight vis-à-vis the upper bound we have derived based on the OD-LinBAI algorithm. The
usual strategy, which is the strategy we adopt here, is to construct and analyze specific hard instances.
In particular, we leverage the instances in Carpentier and Locatelli [10] for standard multi-armed
bandits to construct hard linear bandit instances for any arbitrary K and d. We discuss the tightness
of the lower bound in the following."
EXP,0.576530612244898,"Theorem 3 first shows that for any best arm identification algorithm Π, even with the knowledge of
an upper bound h on the hardness quantity H1,lin, there exists a linear bandit instance such that the
error probability is at least"
EXP,0.5816326530612245,"exp

−O
T h"
EXP,0.5867346938775511,"
.
(4)"
EXP,0.5918367346938775,"Furthermore, for any best arm identification algorithm Π, without the knowledge of an upper bound
h on the hardness quantity H1,lin, there exists a linear bandit instance ν such that the error probability
is at least"
EXP,0.5969387755102041,"exp

−O

T
H1,lin(ν) log2 d"
EXP,0.6020408163265306,"
.
(5)"
EXP,0.6071428571428571,"5When there is no ambiguity, H1,lin will also be used."
EXP,0.6122448979591837,"Comparing the lower bounds (4) and (5) in two different settings, we show that the agent has to pay a
price of log2 d in the absence of the knowledge about the problem complexity. Finding a best arm
identification algorithm that matches the lower bound (4) remains an open problem since the upper
bound of BayesGap (3) involves H1 but not H1,lin. However, notice that the knowledge about the
complexity quantity which is required for BayesGap is usually unavailable in real-life applications."
EXP,0.6173469387755102,"Now we compare the upper bound on the error probability of OD-LinBAI in (2) with the lower
bound (5). Table 1 shows that H1,lin ≥H2,lin always holds. Therefore, the upper bound in (2) is not
larger than the lower bound in (5) in the exponent up to absolute constants. This shows OD-LinBAI
(Algorithm 1) is minimax optimal up to multiplicative factors in the exponent and the upper bound
cannot be improved in an order-wise sense in the exponent in general. At the same time, note that
the upper bound holds for all instances while the lower bound is a minimax result which holds for
specific instances. Since an upper bound can never be smaller than a lower bound, we know that
the difficult instances for the problem of best arm identification in linear bandits in the fixed-budget
setting are those whose H1,lin and H2,lin are of the same order."
NUMERICAL EXPERIMENTS,0.6224489795918368,"5
Numerical experiments"
NUMERICAL EXPERIMENTS,0.6275510204081632,"In this section, we evaluate the performance of our algorithm OD-LinBAI and compare it with
Sequential Halving [9], BayesGap [13], Peace [16], LinearExploration [17] and GSE [18]. For
BayesGap, there are two versions: one is BayesGap-Oracle, which is given the exact information of
the required hardness quantity H1; the other is BayesGap-Adaptive, which adaptively estimates the
hardness quantity by the three-sigma rule. In each setting, the reported error probabilities of different
algorithms are averaged over 1024 independent trials and the (tiny) error bars indicate the standard
errors of the error probabilities. We present the results of one synthetic dataset here. Additional
implementation details and numerical results (including another synthetic dataset, one real-world
dataset and comparison to the recent LT&S algorithm for best arm identification in linear bandits
with fixed confidence [33]) are provided in Appendix E."
NUMERICAL EXPERIMENTS,0.6326530612244898,"5.1
Synthetic dataset 1: a hard instance"
NUMERICAL EXPERIMENTS,0.6377551020408163,"0
5
10
15
20
25
0 0.2 0.4 0.6 0.8 1"
NUMERICAL EXPERIMENTS,0.6428571428571429,"0
10
20
30
40
50
0 0.2 0.4 0.6 0.8 1"
NUMERICAL EXPERIMENTS,0.6479591836734694,"Figure 1: Error probabilities for different numbers of arms
K with T = 25, 50 from left to right."
NUMERICAL EXPERIMENTS,0.6530612244897959,"25
102
5
102
103
2
103
0 0.2 0.4 0.6 0.8 1"
NUMERICAL EXPERIMENTS,0.6581632653061225,"50
102
5
102
103
4
103
0 0.2 0.4 0.6 0.8 1"
NUMERICAL EXPERIMENTS,0.6632653061224489,"Figure 2: Error probabilities for different time budgets T
with K = 25, 50 from left to right."
NUMERICAL EXPERIMENTS,0.6683673469387755,"This benchmark dataset, in which
there are numerous competitors for
the second best arm, was considered
for the problem of best arm identifi-
cation in linear bandits in the fixed-
confidence setting [30, 31, 33]. Simi-
larly, we consider the situation that
d = 2 and K ≥3.
We assume
that the additive random noise fol-
lows the standard Gaussian distribu-
tion N(0, 1). For simplicity, we set
the unknown parameter vector θ∗=
[1, 0]⊤. There is one best arm and
one worst arm, which correspond to
the arm vectors a(1) = [1, 0]⊤and
a(K)
=
[cos(3π/4), sin(3π/4)]⊤
respectively.
For any arm i
∈
{2, 3, . . . , K −1}, the correspond-
ing arm vector is chosen to be
a(i)
=
[cos(π/4 + ϕi), sin(π/4 +
ϕi)]⊤with ϕi drawn independently
from N(0, 0.092). Therefore, there
are K −2 almost second best arms.
Considering the definitions of four
hardness quantities, it holds that
H1 ≈H2 ≈
K"
NUMERICAL EXPERIMENTS,0.673469387755102,"d H1,lin ≈
K"
NUMERICAL EXPERIMENTS,0.6785714285714286,"d H2,lin.
Hence this is a hard instance in the
sense that the linear structure is ex-"
NUMERICAL EXPERIMENTS,0.6836734693877551,"tremely strong. A good algorithm needs to fully utilize the correlations of the arms to obtain
information as efficiently as possible."
NUMERICAL EXPERIMENTS,0.6887755102040817,"The experimental results with fixed T and K are presented in Figure 1 and Figure 2 respectively. In
terms of this hard linear bandit instance, OD-LinBAI is clearly superior compared to its competitors.
In fact, OD-LinBAI consistently pulls only one arm from the K −2 almost second best arms and
thus suffers minimal impact from the increase in K."
CONCLUSIONS AND FUTURE WORK,0.6938775510204082,"6
Conclusions and Future Work"
CONCLUSIONS AND FUTURE WORK,0.6989795918367347,"We introduce the G-optimal design technique into the problem of best arm identification in linear
bandits in the fixed-budget setting. We design a parameter-free and efficient algorithm OD-LinBAI.
To characterize the difficulty of a linear bandit instance, we introduce two hardness quantities H1,lin
and H2,lin. The upper bound of the error probability of OD-LinBAI and the minimax lower bound of
this problem are respectively characterized by H1,lin and H2,lin instead of their analogues H1 and H2
in standard multi-armed bandits. For the first time, minimax optimality (up to constant multiplicative
factors in the exponent) has been achieved in this problem. While we submit that the ingredients
that constitute OD-LinBAI are not surprising in the bandit literature, an open problem thus far has
hence been solved in this contribution (by the careful derivation of an upper bound on the error
probability of OD-LinBAI and an accompanying minimax lower bound). Our theoretical findings
are also supported by the considerable improvements of the empirical performance of OD-LinBAI
vis-à-vis existing algorithms on benchmark datasets."
CONCLUSIONS AND FUTURE WORK,0.7040816326530612,"A direction for future work is to design an instance-dependent asymptotically optimal algorithm for
this problem. However, finding such an algorithm or an instance-dependent asymptotic lower bound
for the problem of best arm identification in standard (i.e., K-armed) multi-armed bandits in the fixed-
budget setting remains open. Finally, as Thompson sampling [1, 4] has been successfully extended
to pure exploration in standard multi-armed bandits [39–42], it is interesting to study whether this
technique can be generalized to linear bandits, in both the fixed-budget and fixed-confidence settings."
CONCLUSIONS AND FUTURE WORK,0.7091836734693877,Acknowledgments and Disclosure of Funding
CONCLUSIONS AND FUTURE WORK,0.7142857142857143,"This research/project is supported by the National Research Foundation Singapore and DSO National
Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-018) and by
Singapore Ministry of Education (MOE) AcRF Tier 1 Grants (A-0009042-01-00 and A-8000189-01-
00)."
REFERENCES,0.7193877551020408,References
REFERENCES,0.7244897959183674,"[1] William R Thompson. On the likelihood that one unknown probability exceeds another in view
of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933."
REFERENCES,0.7295918367346939,"[2] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine Learning, 47(2):235–256, 2002."
REFERENCES,0.7346938775510204,"[3] Sébastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1):1–122,
2012."
REFERENCES,0.7397959183673469,"[4] Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit
problem. In Conference on Learning Theory, pages 39–1. JMLR Workshop and Conference
Proceedings, 2012."
REFERENCES,0.7448979591836735,"[5] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and
stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal
of Machine Learning Research, 7(6), 2006."
REFERENCES,0.75,"[6] Sébastien Bubeck, Rémi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits
problems. In International Conference on Algorithmic Learning Theory, pages 23–37. Springer,
2009."
REFERENCES,0.7551020408163265,"[7] Jean-Yves Audibert, Sébastien Bubeck, and Rémi Munos. Best arm identification in multi-armed
bandits. In Conference on Learning Theory, pages 41–53, 2010."
REFERENCES,0.7602040816326531,"[8] Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification:
A unified approach to fixed budget and fixed confidence. Advances in Neural Information
Processing Systems, 25, 2012."
REFERENCES,0.7653061224489796,"[9] Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed
bandits. In International Conference on Machine Learning, pages 1238–1246. PMLR, 2013."
REFERENCES,0.7704081632653061,"[10] Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best
arm identification bandit problem. In Conference on Learning Theory, pages 590–604. PMLR,
2016."
REFERENCES,0.7755102040816326,"[11] Aurélien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence.
In Conference on Learning Theory, pages 998–1027. PMLR, 2016."
REFERENCES,0.7806122448979592,"[12] Chao Tao, Saúl Blanco, and Yuan Zhou. Best arm identification in linear bandits with linear
dimension dependency. In International Conference on Machine Learning, pages 4877–4886.
PMLR, 2018."
REFERENCES,0.7857142857142857,"[13] Matthew Hoffman, Bobak Shahriari, and Nando Freitas. On correlation and budget constraints
in model-based bandit optimization with application to automatic machine learning. In Artificial
Intelligence and Statistics, pages 365–374. PMLR, 2014."
REFERENCES,0.7908163265306123,"[14] Jack Kiefer and Jacob Wolfowitz. The equivalence of two extremum problems. Canadian
Journal of Mathematics, 12:363–366, 1960."
REFERENCES,0.7959183673469388,"[15] Shahin Shahrampour, Mohammad Noshad, and Vahid Tarokh. On sequential elimination
algorithms for best-arm identification in multi-armed bandits. IEEE Transactions on Signal
Processing, 65(16):4281–4292, 2017."
REFERENCES,0.8010204081632653,"[16] Julian Katz-Samuels, Lalit P. Jain, Zohar S. Karnin, and Kevin G. Jamieson. An empirical
process approach to the union bound: Practical algorithms for combinatorial and linear bandits.
Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.8061224489795918,"[17] Ayya Alieva, Ashok Cutkosky, and Abhimanyu Das. Robust pure exploration in linear bandits
with limited budget. In International Conference on Machine Learning, pages 187–195. PMLR,
2021."
REFERENCES,0.8112244897959183,"[18] MohammadJavad Azizi, Branislav Kveton, and Mohammad Ghavamzadeh. Fixed-budget
best-arm identification in structured bandits. In International Joint Conference on Artificial
Intelligence, pages 2798–2804, 2022."
REFERENCES,0.8163265306122449,"[19] Naoki Abe and Philip M Long. Associative reinforcement learning using linear probabilistic
concepts. In International Conference on Machine Learning, pages 3–11, 1999."
REFERENCES,0.8214285714285714,"[20] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397–422, 2002."
REFERENCES,0.826530612244898,"[21] Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under
bandit feedback. In Conference on Learning Theory, pages 355–366, 2008."
REFERENCES,0.8316326530612245,"[22] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395–411, 2010."
REFERENCES,0.8367346938775511,"[23] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear
stochastic bandits. In Advances in Neural Information Processing Systems, volume 11, pages
2312–2320, 2011."
REFERENCES,0.8418367346938775,"[24] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Artificial Intelligence and Statistics, pages 208–214. PMLR, 2011."
REFERENCES,0.8469387755102041,"[25] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear
payoffs. In International Conference on Machine Learning, pages 127–135. PMLR, 2013."
REFERENCES,0.8520408163265306,"[26] Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Artificial
Intelligence and Statistics, pages 176–184. PMLR, 2017."
REFERENCES,0.8571428571428571,"[27] Tor Lattimore and Csaba Szepesvári. Bandit Algorithms. Cambridge University Press, 2020."
REFERENCES,0.8622448979591837,"[28] Marta Soare, Alessandro Lazaric, and Rémi Munos. Best-arm identification in linear bandits.
In Advances in Neural Information Processing Systems, 2014."
REFERENCES,0.8673469387755102,"[29] Liyuan Xu, Junya Honda, and Masashi Sugiyama. A fully adaptive algorithm for pure explo-
ration in linear bandits. In International Conference on Artificial Intelligence and Statistics,
pages 843–851. PMLR, 2018."
REFERENCES,0.8724489795918368,"[30] Mohammadi Zaki, Avinash Mohan, and Aditya Gopalan. Towards optimal and efficient best
arm identification in linear bandits. arXiv preprint arXiv:1911.01695, 2019."
REFERENCES,0.8775510204081632,"[31] Tanner Fiez, Lalit Jain, Kevin G. Jamieson, and L. Ratliff. Sequential experimental design for
transductive linear bandits. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.8826530612244898,"[32] Rémy Degenne, Pierre Ménard, Xuedong Shang, and Michal Valko. Gamification of pure
exploration for linear bandits. In International Conference on Machine Learning, pages 2432–
2442. PMLR, 2020."
REFERENCES,0.8877551020408163,"[33] Yassir Jedra and Alexandre Proutiere. Optimal best-arm identification in linear bandits. Ad-
vances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.8928571428571429,"[34] Abbas Kazerouni and Lawrence M Wein. Best arm identification in generalized linear bandits.
Operations Research Letters, 49(3):365–371, 2021."
REFERENCES,0.8979591836734694,"[35] Friedrich Pukelsheim. Optimal Design of Experiments. SIAM, 2006."
REFERENCES,0.9030612244897959,"[36] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research
Logistics Quarterly, 3(1-2):95–110, 1956."
REFERENCES,0.9081632653061225,"[37] S Damla Ahipasaoglu, Peng Sun, and Michael J. Todd. Linear convergence of a modified
Frank–Wolfe algorithm for computing minimum-volume enclosing ellipsoids. Optimisation
Methods and Software, 23(1):5–19, 2008."
REFERENCES,0.9132653061224489,"[38] Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations
in bandits and in RL with a generative model. In International Conference on Machine Learning,
pages 5662–5670. PMLR, 2020."
REFERENCES,0.9183673469387755,"[39] Daniel Russo. Simple bayesian algorithms for best arm identification. In Conference on
Learning Theory, pages 1417–1418. PMLR, 2016."
REFERENCES,0.923469387755102,"[40] Xuedong Shang, Rianne Heide, Pierre Menard, Emilie Kaufmann, and Michal Valko. Fixed-
confidence guarantees for bayesian best-arm identification. In International Conference on
Artificial Intelligence and Statistics, pages 1823–1832. PMLR, 2020."
REFERENCES,0.9285714285714286,"[41] Chao Qin and Daniel Russo. Adaptivity and confounding in multi-armed bandit experiments.
arXiv preprint arXiv:2202.09036, 2022."
REFERENCES,0.9336734693877551,"[42] Marc Jourdan, Rémy Degenne, Dorian Baudry, Rianne de Heide, and Emilie Kaufmann. Top
two algorithms revisited. arXiv preprint arXiv:2206.05979, 2022."
REFERENCES,0.9387755102040817,"[43] Michael J. Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016."
REFERENCES,0.9438775510204082,"[44] Tony Cai, Jianqing Fan, and Tiefeng Jiang. Distributions of angles in random packing on
spheres. Journal of Machine Learning Research, 14:1837, 2013."
REFERENCES,0.9489795918367347,"[45] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.9540816326530612,Checklist
REFERENCES,0.9591836734693877,1. For all authors...
REFERENCES,0.9642857142857143,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 6."
REFERENCES,0.9693877551020408,"(c) Did you discuss any potential negative societal impacts of your work? [No]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9744897959183674,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
(b) Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments..."
REFERENCES,0.9795918367346939,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] See Appendix E
and the code in the supplementary.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes]
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9846938775510204,"(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [No]"
REFERENCES,0.9897959183673469,"(c) Did you include any new assets either in the supplemental material or as a URL? [No]
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9948979591836735,"(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]"
