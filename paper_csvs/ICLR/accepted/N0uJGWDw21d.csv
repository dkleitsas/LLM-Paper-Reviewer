Section,Section Appearance Order,Paragraph
SHANGHAI JIAO TONG UNIVERSITY,0.0,"1Shanghai Jiao Tong University
2Huawei Inc.
3Institute of ArtiÔ¨Åcial Intelligence, Huazhong University of Science & Technology
4School of EIC, Huazhong University of Science & Technology
{xuhaohang, daiwenrui,xionghongkai}@sjtu.edu.cn
{jaminfong, xgwang}@hust.edu.cn
{zxphistory, 198808xc}@gmail.com
tian.qi1@huawei.com"
ABSTRACT,0.004608294930875576,ABSTRACT
ABSTRACT,0.009216589861751152,"Recent advances in self-supervised learning have experienced remarkable progress,
especially for contrastive learning based methods, which regard each image as well
as its augmentations as an individual class and try to distinguish them from all other
images. However, due to the large quantity of exemplars, this kind of pretext task
intrinsically suffers from slow convergence and is hard for optimization. This is
especially true for small scale models, which we Ô¨Ånd the performance drops dramat-
ically comparing with its supervised counterpart. In this paper, we propose a simple
but effective distillation strategy for unsupervised learning. The highlight is that
the relationship among similar samples counts and can be seamlessly transferred
to the student to boost the performance. Our method, termed as BINGO, which
is short for Bag of InstaNces aGgregatiOn, targets at transferring the relationship
learned by the teacher to the student. Here bag of instances indicates a set of similar
samples constructed by the teacher and are grouped within a bag, and the goal of
distillation is to aggregate compact representations over the student with respect to
instances in a bag. Notably, BINGO achieves new state-of-the-art performance on
small scale models, i.e., 65.5% and 68.9% top-1 accuracies with linear evaluation
on ImageNet, using ResNet-18 and ResNet-34 as backbone, respectively, surpass-
ing baselines (52.5% and 57.4% top-1 accuracies) by a signiÔ¨Åcant margin. The
code is available at https://github.com/haohang96/bingo."
INTRODUCTION,0.013824884792626729,"1
INTRODUCTION"
INTRODUCTION,0.018433179723502304,"Convolutional Neural Networks (CNNs) have achieved great success in the Ô¨Åeld of computer vision,
including image classiÔ¨Åcation (He et al., 2016), object detection (Ren et al., 2015) and semantic
segmentation (Chen et al., 2017). However, most of the time, CNNs cannot succeed without enormous
human-annotated data. Recently, self-supervised learning, typiÔ¨Åed by contrastive learning (He et al.,
2020; Chen et al., 2020a), has been Ô¨Åghting with the annotation-eager challenge and achieves great
success. Most current self-supervised methods yet focus on networks with large size, e.g., ResNet-
50 (He et al., 2016) with more than 20M parameters, but real-life implementation usually involves
computation-limited scenarios, e.g., mobile/edge devices."
INTRODUCTION,0.02304147465437788,"Due to annotation lacking in unsupervised tasks, learning from unlabeled data becomes challenging.
Recent contrastive learning methods (He et al., 2020; Chen et al., 2020a) tackle this problem by
narrowing gaps between embeddings of different augmentations from the same image. Techniques
like momentum encoder for stable updating, memory bank for storing negative pairs, complicated
data augmentation strategies etc., are proposed to avoid collapse and promote the performance.
With the above techniques, contrastive learning methods show promising performance. However,"
INTRODUCTION,0.027649769585253458,"*Equal contributions.
The work was done during the internship of Haohang Xu and Jiemin Fang at Huawei Inc."
INTRODUCTION,0.03225806451612903,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03686635944700461,"Eff-B0
MobileNet-v3 ResNet-18
ResNet-34
30 40 50 60 70"
INTRODUCTION,0.041474654377880185,Top-1 Acc. (%)
INTRODUCTION,0.04608294930875576,"MoCo v2
SEED"
INTRODUCTION,0.05069124423963134,"DisCo
BINGO 42.2 67.6"
INTRODUCTION,0.055299539170506916,69.1 69.3 36.3 68.2 58.9 67.1 52.5 63.0
INTRODUCTION,0.059907834101382486,"65.5
65.2 57.4 65.7 67.6 68.9"
INTRODUCTION,0.06451612903225806,"(a) Linear classiÔ¨Åcation accuracy on ImageNet over dif-
ferent student architectures distilled by ResNet-50√ó2
teacher model"
INTRODUCTION,0.06912442396313365,"1% Labels
10% Labels
20 30 40 50 60"
INTRODUCTION,0.07373271889400922,Top-1 Acc. (%)
INTRODUCTION,0.07834101382488479,"MoCo v2
SEED
DisCo
BINGO 30.9 44.3 47.1 50.3 45.8"
INTRODUCTION,0.08294930875576037,"54.8
54.7 61.2"
INTRODUCTION,0.08755760368663594,"(b) Semi-supervised learning by Ô¨Åne-tuning 1% and
10% labeled images on ImageNet using ResNet-18
student and ResNet-152 teacher model"
INTRODUCTION,0.09216589861751152,"Figure 1: Overall performance comparisons between BINGO and other unsupervised distillation
methods."
INTRODUCTION,0.0967741935483871,"contrastive learning requires discriminating all instances, due to the large quantity of exemplars, this
kind of pretext task intrinsically suffers from slow convergence and is hard for optimization. This
issue becomes severe for small scale models, which carry too few parameters to Ô¨Åt the enormous
data. Inspired by supervised learning that knowledge from large models can effectively promote the
learning ability of small models with distillation, exploring knowledge distillation on unsupervised
small models becomes an important topic."
INTRODUCTION,0.10138248847926268,"Compress (Fang et al., 2020) and SEED (Fang et al., 2020) are two typical methods for unsupervised
distillation, which propose to transfer knowledge from the teacher in terms of similarity distributions
among different instances. However, as the similarity distribution is computed by randomly sampling
instances from a dynamically maintained queue, this kind of knowledge is mostly constructed based
on instances with low relation, which fails to effectively model similarity of those highly related
samples. To solve this issue, we propose a new self-supervised distillation method, which transfers
knowledge by aggregating bags of related instances, named BINGO. In our empirical studies,
transferring knowledge based on highly related samples helps boost performance more effectively
compared with previous relation-agnostic methods. SpeciÔ¨Åcally, we select an unsupervised pretrained
large model as the teacher. First, we map the conventional instance-wise dataset into a bag-wise
one. Each original instance is set as an anchor instance of the bag. By matching similarities of
all the other instances‚Äô embeddings produced by the teacher model, we feed instances which show
high similarity with the anchor instance into the bag. Then we apply the bagged dataset to the
small model distillation process. To this end, we propose a bag-aggregation distillation loss, which
consists of two components: inter-sample distillation and intra-sample distillation. For intra-sample
distillation, embeddings of the student and teacher from two augmentations of the same instance are
pushed together; for inter-sample distillation, embeddings of all instances in one bag are pushed to be
more similar with the anchor one. Equipped with the two proposed distillation loss, the bag-based
knowledge from the teacher can be well transferred to the student, which shows signiÔ¨Åcant advantages
over previous relation-agnostic ones (Fang et al., 2020; Abbasi Koohpayegani et al., 2020)."
INTRODUCTION,0.10599078341013825,Our contributions can be summarized as follows.
INTRODUCTION,0.11059907834101383,"‚Ä¢ We propose a new self-supervised distillation method, which bags related instances by
matching similarities of instance embeddings produced by the teacher. The bagged dataset
can effectively boost small model distillation by aggregating instance embeddings in bags.
The proposed relation-guided method shows stronger performance than previous relation-
agnostic ones."
INTRODUCTION,0.1152073732718894,"‚Ä¢ BINGO promotes the performance of both ResNet-18 and -34 to new state-of-the-art (SOTA)
ones in unsupervised scenarios. It is worth noting that the distilled models also present
far better performance compared with previous SOTA methods on other tasks, i.e., KNN
classiÔ¨Åcation and semi-supervised learning."
INTRODUCTION,0.11981566820276497,Published as a conference paper at ICLR 2022
INTRODUCTION,0.12442396313364056,"‚Ä¢ BINGO provides a new paradigm for unsupervised distillation where knowledge between
instances with high relation could be more effective than relation-agnostic ones. This may
be inspiring for further explorations on knowledge transfer in unsupervised scenarios."
RELATED WORK,0.12903225806451613,"2
RELATED WORK"
RELATED WORK,0.1336405529953917,"Self-supervised Learning
As a generic framework to learn representations with unlabeled data,
self-supervised learning has experienced remarkable progress over the past few years. By constructing
a series of pretext tasks, self-supervised learning aims at extracting discriminative representations
from input data. Previous methods obtain self-supervised representations mainly via a corrupting
and recovering manner, from perspectives of spatial ordering (Noroozi & Favaro, 2016), rotation
changes (Komodakis & Gidaris, 2018), in-painting (Pathak et al., 2016), or colorization (Zhang et al.,
2016), et al. Recently, contrastive learning based methods (He et al., 2020; Chen et al., 2020a) emerge
and signiÔ¨Åcantly promote the performance of self-supervised learning, which aim at maximizing the
mutual information between two augmented views of a image. A series of subsequent works (Grill
et al., 2020; Xu et al., 2020b; Dwibedi et al., 2021) further improve the performance to a very high
level. Khosla et al. (2020) applies contrastive learning on supervised learning, which selects the
positive samples from the same category. Caron et al. (2020) proposes to align the distribution of one
instance‚Äôs different views on other categories. However, few of them pay attention to self-supervised
learning on small-scale models, which are of critical importance to implement self-supervised models
on lightweight devices. We propose an effective method to boost the self-supervised learning of
small models, which takes advantage of relation-based knowledge between data and shows superior
performance than previous ones."
RELATED WORK,0.1382488479262673,"Knowledge Distillation
Knowledge distillation aims to transfer knowledge from a model (teacher)
to another one (student), usually from a large to small one, which is commonly used for improving
the performance of the lightweight model. Hinton et al. (2015) Ô¨Årst proposes knowledge distillation
via minimizing the KL-divergence between the student and teacher‚Äôs logits, which uses the predicted
class probabilities from the teacher as soft labels to guide the student model. Instead of mimicking
teacher‚Äôs logits, Romero et al. (2014) transfers the knowledge by minimizing the ‚Ñì2 distance between
intermediate outputs of the teacher and student model. To solve the dimension mismatch, Romero
et al. (2014) uses a randomly initialized projection layer to enlarge the dimension of a narrower
student model. Based on Romero et al. (2014), Zagoruyko & Komodakis (2016) utilizes knowledge
stored in the attention map generated by the teacher model, and pushes the student model to pay
attention to the area where the teacher focuses on. Zhou et al. (2021) improves weighted soft labels to
adaptively improve the bias-variance tradeoff of each sample. Besides perspectives of soft labels and
intermediate features, relation between samples is also an important knowledge. Park et al. (2019)
and Liu et al. (2019) train student model by aligning the pair-wise similarity graph with the teacher.
Recently, some works extend the above distillation method into self-supervised learning scenarios.
Tian et al. (2019) uses the contrastive loss to learn cross-modality consistency. Xu et al. (2020a),Fang
et al. (2020) and Abbasi Koohpayegani et al. (2020) share a similar methodology with Caron et al.
(2020) of aligning feature distribution between views of the same instances. The distribution is
computed as the pair-wise similarities between student‚Äôs outputs and features stored in memory bank.
However, the above relation-based self-supervised distillation methods only compute the similarity
between anchor sample and randomly sampled instances from a maintained queue, which ignores
the relation between sampled and anchor instances. Choi et al. (2021) uses the teacher model to
produce cluster assignments, and encourages the student model to mimic the output of the trainable
teacher model on-the-Ô¨Çy, which achieves promising results. Gao et al. (2021) strengthens the student
model by adding a regularization loss on the original contrastive loss, which aims at minimizing the
‚Ñì2 distance between the student‚Äôs and teacher‚Äôs embedding. Navaneet et al. (2021) also achieves
competitive results with feature regression in self-supervised distillation. We propose to transfer
the relation knowledge between models via a new type of dataset, which bags related instances. By
aggregating the bagged instances, the relation knowledge can be effectively transferred."
APPROACH,0.14285714285714285,"3
APPROACH"
APPROACH,0.14746543778801843,"In this section, we introduce the proposed BINGO in details. First, we discuss how to bag samples
in the instance-wise dataset. After the samples are bagged, the bag-aggregation based knowledge"
APPROACH,0.15207373271889402,Published as a conference paper at ICLR 2022 ‚Ä¶
APPROACH,0.15668202764976957,Instance-wise Dataset ùëã
APPROACH,0.16129032258064516,Bagging
APPROACH,0.16589861751152074,"via 
Feature 
Similarity ‚Ä¶"
APPROACH,0.17050691244239632,Features
APPROACH,0.17511520737327188,Anchor
APPROACH,0.17972350230414746,"Positive
Positive"
APPROACH,0.18433179723502305,"Positive
Positive"
APPROACH,0.1889400921658986,"Bag
Aggregation"
APPROACH,0.1935483870967742,Teacher
APPROACH,0.19815668202764977,Student
APPROACH,0.20276497695852536,Bag-wise Dataset ùõ∫
APPROACH,0.2073732718894009,Anchor Instance ùëãùëé
APPROACH,0.2119815668202765,"Positive Instance ùëãùëù ùë°ùëé ùë†ùëé
ùë†ùëù ùë°~ùëá ùë°~ùëá"
APPROACH,0.21658986175115208,Details about how to aggregate a bag
APPROACH,0.22119815668202766,Teacher ‚Ä¶
APPROACH,0.22580645161290322,Bag-wise Dataset ùõ∫
APPROACH,0.2304147465437788,"Bag 
sampling"
APPROACH,0.2350230414746544,"ùêøùëñùëõùë°ùëüùëé
ùêøùëñùëõùë°ùëíùëü"
APPROACH,0.23963133640552994,"Instance
sampling"
APPROACH,0.24423963133640553,"Figure 2: An overview of the proposed method. The samples are Ô¨Årst bagged via feature similarity.
Then the related instances in a bag is aggregated via intra-sample and inter-sample distillation loss.
The Ô¨Ågure on top-right is an intuitive explanation of how bag aggregation works."
APPROACH,0.2488479262672811,"distillation is introduced. We also discuss how to compute bag-aggregation loss and how they improve
the performance of the lightweight model. The overall framework is illustrated in Fig. 2."
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.2534562211981567,"3.1
BAGGING INSTANCES WITH SIMILARITY MATCHING"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.25806451612903225,"Given the unlabeled training set X = {x1, x2, ..., xN}, we deÔ¨Åne the corresponding bag-wise
training set as ‚Ñ¶= {‚Ñ¶1, ‚Ñ¶2, ..., ‚Ñ¶N}, where each bag ‚Ñ¶i consists of a set of instances. To transfer
the instance-wise dataset to a bag-wise one, we Ô¨Årst feed X into a pretrained teacher model fT and
get the corresponding features V = {v1, v2, ..., vN} where vi = fT(xi). For each anchor sample
xa in the dataset, we Ô¨Ånd positive samples which share high similarity with the anchor sample. Then
the anchor sample as well as the similar samples are combined to form a bag. The samples in one
bag have a compact representation in the embedding space. Several mapping function can be used to
Ô¨Ånd similar samples:"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.2626728110599078,"K-nearest Neighbors
For each anchor sample xa in the instance-wise dataset, we Ô¨Årst compute
the pairwise similarity with all samples in the dataset Sa = {va ¬∑ vi | i = 1, 2, ..., N}. The bag ‚Ñ¶a
corresponding to xa is deÔ¨Åned as:"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.2672811059907834,"‚Ñ¶a = top‚àírank(Sa, K),
(1)"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.271889400921659,"where top‚àírank(¬∑, K) returns the indices of top K items in a set."
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.2764976958525346,"K-means Clustering
Given the training feature set V
= {v1, v2, ..., vN}, we Ô¨Årst assign a
pseudo-label qi to each sample i, where qi ‚àà{q1, ..., qK}. The clustering process is performed by
minimizing the following term,
1
N N
X"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.28110599078341014,"i=1
‚àívT
i cqi,
(2)"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.2857142857142857,"where cqi denotes the centering feature of all features belonging to the label qi, i.e., cqi =
P"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.2903225806451613,"qj=qi vj, ‚àÄj = 1, ..., N."
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.29493087557603687,The bag ‚Ñ¶a of anchor sample xa is deÔ¨Åned as:
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.2995391705069124,"‚Ñ¶a = {i | qi = qa, ‚àÄi = 1, 2, ..., N}.
(3)"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.30414746543778803,Published as a conference paper at ICLR 2022
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.3087557603686636,"Ground Truth Label
If the ground truth label is available, we can also bag samples with the
human-annotated semantic labels. Given the label set Y = {y1, y2, ..., yN}, we can bag related
instances of the anchor sample xa via:"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.31336405529953915,"‚Ñ¶a = {i | yi = ya, ‚àÄi = 1, 2, ..., N}.
(4)"
BAGGING INSTANCES WITH SIMILARITY MATCHING,0.31797235023041476,"In this paper, we use K-nearest neighbors as the bagging strategy. More details about performance of
using the K-means clustering based bagging strategy can be found in Appendix. Note that bagging
instances via the ground truth label is just used to measure the upper bound of the proposed method."
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3225806451612903,"3.2
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3271889400921659,"Once we get the bag-wise dataset ‚Ñ¶utilizing a pretrained teacher model, it can be used for distillation
process. In each feed-forward process, the anchor sample xa and the positive sample xp which belong
to the same bag ‚Ñ¶a are sampled together in one batch. We propose the bag-aggregation distillation
loss including the intra-sample distillation loss Lintra and inter-sample distillation loss Linter."
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3317972350230415,"To aggregate the representations within a bag into more compact embeddings, we minimize the
following target function:
min
Œ∏S L =
E
xi‚àº‚Ñ¶a
(L(fS(xi), fT(xa))),
(5)"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.33640552995391704,"where L is a metric function to measure the distance between two embeddings ‚Äì there are many
metrics can be selected, such as cosine similarity, euclidean distance, etc. Here we use the normalized
cosine similarity, i.e., the contrastive loss commonly used in self-supervised learning to measure the
distance between xi and the anchor sample xa. The target function in Eq. 5 can be divided into two
components:"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.34101382488479265,"L=L(fS(t1(xa)), fT(t2(xa))) +
E
xi‚àº‚Ñ¶a\xa
(L(fS(t3(xi)), fT(t2(xa)))),
(6)"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3456221198156682,"Three separate data augmentation operators t1, t2, t3 are randomly sampled from the same family of
MoCo-v2 augmentations T , which is also adopted in SEED(Fang et al., 2020) and DisCo (Gao et al.,
2021). where the Ô¨Årst item focuses on pulling different views (augmentations) of the same sample
together, and the second item aims at pulling different samples that are within a same bag into more
related ones. We term the Ô¨Årst item as Lintra and the second item as Linter."
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.35023041474654376,"Intra-Sample Distillation
The intra-sample distillation loss is a variant of conventional contrastive
loss. Contrastive learning aims to learn representations by discriminating the positive key among
negative samples. Given two augmented views x and x‚Ä≤ of one input image, MoCo (Chen et al.,
2020c) uses a online encoder fq and a momentum encoder fk to generate embeddings of the positive
pairs: q = fq(x), k = fk(x‚Ä≤). The contrastive loss is deÔ¨Åned as"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3548387096774194,"Lcontrast =‚àílog
exp(q ¬∑ k+/œÑ)
PN
i=0 exp(q ¬∑ ki/œÑ)
.
(7)"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.35944700460829493,"During distillation, we simply replace fq and fk by the student model fS and teacher model fT,
while weights of the teacher model fT are pretrained and are not updated during distillation. The
intra-sample distillation loss can be formulated as"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3640552995391705,"Lintra =‚àílog exp(fS(t1(xa)) ¬∑ fT(t2(xa))/œÑ)
PN
i=0 exp(fS(t1(xa)) ¬∑ k‚àí
i /œÑ)
,
(8)"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3686635944700461,"where œÑ is the temperature parameter. We select negative samples k‚àíin a memory bank, which
is widely used in MoCo (He et al., 2020) and many subsequent contrastive learning methods. The
memory bank is a queue of data embeddings and the queue size is much larger than a typical mini-
batch size. After each forward iteration, items in the queue are progressively replaced by the current
output of the teacher network."
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.37327188940092165,"Inter-Sample Distillation
Given the anchor sample xa and a positive sample xp in the bag ‚Ñ¶a, it
is natural to map highly related samples to more similar representations. In other words, we want the"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3778801843317972,Published as a conference paper at ICLR 2022
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3824884792626728,"bag Ô¨Ålled with related samples to be more compact. Inspired by Eq. 8, we deÔ¨Åne the inter-sample
distillation loss as"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.3870967741935484,"Linter =‚àílog exp(fS(t3(xp)) ¬∑ fT(t2(xa))/œÑ)
PN
i=0 exp(fS(t3(xp)) ¬∑ k‚àí
i /œÑ)
.
(9)"
KNOWLEDGE DISTILLATION VIA BAG AGGREGATION,0.391705069124424,"The intra- and inter-sample distillation loss serve as different roles. The intra-sample distillation works
like conventional distillation (Hinton et al., 2015; Romero et al., 2014), which aims at minimizing
distances between outputs of the teacher and student model given the same input. However, the inter-
sample distillation mainly focuses on transferring the data relation knowledge taking the bag-wise
dataset as the carrier, which is obtained from the pretrained teacher model."
EXPERIMENTS,0.39631336405529954,"4
EXPERIMENTS"
EXPERIMENTS,0.4009216589861751,"In this section, we evaluate the feature representations of the distilled student networks on several
widely used benchmarks. We Ô¨Årst report the performance on ImageNet under the linear evaluation
and semi-supervised protocols. Then we conduct evaluation on several downstream tasks including
object detection and instance segmentation, as well as some ablation studies to diagnose how each
component and parameter affect the performance."
PRE-TRAINING DETAILS,0.4055299539170507,"4.1
PRE-TRAINING DETAILS"
PRE-TRAINING DETAILS,0.41013824884792627,"Pre-training of Teacher Model
Two models are used as teachers: ResNet-50 trained with MoCo-
v2 (Chen et al., 2020c) for 800 epochs and ResNet-50√ó2 trained with SwAV for 400 epochs. The
ofÔ¨Åcially released weights 1 are used to initialize teacher models for fair comparisons."
PRE-TRAINING DETAILS,0.4147465437788018,"Self-supervised Distillation of Student Model
Two models are used as students: ResNet-18 and
ResNet-34. Following the settings of MoCo in Chen et al. (2020c), we add a 2-layer MLP on top
of the last averaged pooling layer to form a 128-d embedding vector. During distillation, the model
is trained with the SGD optimizer with momentum 0.9 and weight decay 0.0001 for 200 epochs on
ImageNet (Deng et al., 2009). The batch size and learning rate are set as 256 and 0.03 for 8 GPUs,
which simply follow the hyper-parameter settings as in Chen et al. (2020c). The learning rate is
decayed to 0 by a cosine scheduler. The CutMix used in Gidaris et al. (2021) and Xu et al. (2020b) is
also applied to boost the performance. The temperature œÑ and the size of memory bank are set as 0.2
and 65,536 respectively. For the bagging strategy, we use K-nearest neighbors unless speciÔ¨Åed."
EXPERIMENTS ON IMAGENET,0.41935483870967744,"4.2
EXPERIMENTS ON IMAGENET"
EXPERIMENTS ON IMAGENET,0.423963133640553,"Table 1: KNN classiÔ¨Åcation accuracy on Ima-
geNet. We report the results on the validation
set with 10 nearest neighbors. ResNet-50√ó2
is used as the teacher."
EXPERIMENTS ON IMAGENET,0.42857142857142855,"Method
ResNet-18 ResNet-34"
EXPERIMENTS ON IMAGENET,0.43317972350230416,"SEED (Fang et al., 2020)
55.3
58.2
BINGO
61.0
64.9"
EXPERIMENTS ON IMAGENET,0.4377880184331797,"KNN ClassiÔ¨Åcation
We evaluate representation of
student model using nearest neighbor classiÔ¨Åer. KNN
classiÔ¨Åer can evaluate the learned feature more di-
rectly without any parameter tuning. Following Caron
et al. (2020); Abbasi Koohpayegani et al. (2020); Fang
et al. (2020), we extract features from center-cropped
images after the last averaged pooling layers. For con-
venient comparisons with other methods, we report
the validation accuracy with 10 NN (we use the stu-
dent model distilled from ResNet-50√ó2). As shown
in Table 1, BINGO achieves 61.0% and 64.9% accu-
racies on ResNet-18/34 models, respectively, which outperforms previous methods signiÔ¨Åcantly."
EXPERIMENTS ON IMAGENET,0.4423963133640553,"Linear Evaluation
In order to evaluate the performance of BINGO, we train a linear classiÔ¨Åer
upon the frozen representation, following the common evaluation protocol in Chen et al. (2020c). For
fair comparisons, we use the same hyper-parameters as Fang et al. (2020); Gao et al. (2021) during
linear evaluation stage. The classiÔ¨Åer is trained for 100 epochs, using the SGD optimizer with 30 as
initial learning rate. As shown in Table 2, BINGO outperform previous DisCo and SEED with the"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.4470046082949309,"1Checkpoints
of
teacher
models
can
be
downloaded
from
https://github.com/
facebookresearch/moco and https://github.com/facebookresearch/swav."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.45161290322580644,Published as a conference paper at ICLR 2022
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.45622119815668205,"Table 2: Linear classiÔ¨Åcation accuracy on ImageNet over different student architectures. Note that
when using R50√ó2 as the teacher, SEED distills for 800 epochs while DisCo and BINGO distill for
200 epochs. The numbers in brackets indicate the accuracies of teacher models. ‚ÄúT‚Äù denotes the
teacher and ‚ÄúS‚Äù denotes the student."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.4608294930875576,"Method
T
S
R-18
R-34
T-1
T-5
T-1
T-5
Supervised (Fang et al., 2020)
69.5
-
72.8
-
MoCo-V2 (Baseline) (Fang et al., 2020)
52.5
77.0
57.4
81.6
SEED (Fang et al., 2020)
R-50 (67.4)
57.6
81.8
58.5
82.6
DisCo (Gao et al., 2021)
R-50 (67.4)
60.6
83.7
62.5
85.4
BINGO
R-50 (67.4)
61.4
84.3
63.5
85.7
BINGO
R-50 (71.1)
64.0
85.7
66.1
87.2
SEED (Fang et al., 2020)
R-152 (74.1)
59.5
83.3
62.7
85.8
DisCo (Gao et al., 2021)
R-152 (74.1)
65.5
86.7
68.1
88.6
BINGO
R-152 (74.1)
65.9
87.1
69.1
88.9
SEED (Fang et al., 2020)
R50√ó2 (77.3)
63.0
84.9
65.7
86.8
DisCo (Gao et al., 2021)
R50√ó2 (77.3)
65.2
86.8
67.6
88.6
BINGO
R50√ó2 (77.3)
65.5
87.0
68.9
89.0"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.46543778801843316,Table 3: Transfer learning accuracy (%) on COCO detection.
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.4700460829493088,"Method
Mask R-CNN, ResNet-18, Detection
1√ó schedule
2√ó schedule
APbb
APbb
50
APbb
75
APS
APM
APL
APbb
APbb
50
APbb
75
APS
APM
APL
MoCo v2
31.3
50.0
33.5
16.5
33.1
41.1
34.4
53.9
37.0
18.9
36.8
45.5
BINGO
32.0
51.0
34.7
17.1
34.1
42.0
34.9
54.2
37.7
20.0
37.1
46.0"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.47465437788018433,Table 4: Transfer learning accuracy (%) on COCO instance segmentation.
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.4792626728110599,"Method
Mask R-CNN, ResNet-18, Instance Segmentation
1√ó schedule
2√ó schedule
APmk APmk
50
APmk
75
APS APM APL APmk APmk
50
APmk
75
APS APM APL
MoCo v2
28.8
47.2
30.6
12.2
29.7
42.7
31.5
51.1
33.6
14.1
32.9
46.9
BINGO
29.6
48.2
31.5
12.8
30.8
43.0
31.9
51.7
33.9
14.9
33.1
47.2"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.4838709677419355,"same teacher and student models. Note that SEED distills for 800 epochs while BINGO runs for 200
epochs with ResNet-50√ó2 teacher, which demonstrates the effectiveness of BINGO."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.48847926267281105,"Transfer to Object Detection and Instance Segmentation
We evaluate the generalization ability
of the student model on detection and instance segmentation tasks. The COCO dataset is used for
evaluation. Following He et al. (2020), we use Mask R-CNN (He et al., 2017) for object detection and
instance segmentation and Ô¨Åne-tune all the parameters of student model ResNet-18 end-to-end. As
shown in Table 3 and Table 4, BINGO consistently outperforms models pretrained without distillation.
The detection and segmentation results of ResNet-34 can be found in the appendix."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.4930875576036866,"Semi-supervised ClassiÔ¨Åcation
Following previous works Chen et al. (2020a;b), we evaluate the
proposed method by Ô¨Åne-tuning the student model ResNet-18 with 1% and 10% labeled data. We
follow the training split settings as in Chen et al. (2020a) for fair comparisons. The network is Ô¨Åne-
tuned for 60 epochs with SGD optimizer. The learning rate of the last randomly initialized fc layer is
set as 10. As shown in Table 5, using ResNet-18 student model, BINGO obtains best accuracies with
the same ResNet-50 and ResNet-152 teacher when using 1% and 10% labels, respectively."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.4976958525345622,"Transfer to CIFAR-10/CIFAR-100 classiÔ¨Åcation
Following the evaluation protocol in Fang et al.
(2020); Gao et al. (2021), we assess the generalization of BINGO on the CIFAR-10/CIFAR-100
dataset. As shown in Table 6, compared with the previous state-of-the-art method DisCo (Gao et al.,
2021), BINGO outperforms it by 1.5% and 3.2% respectively."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5023041474654378,Published as a conference paper at ICLR 2022
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5069124423963134,Table 5: Semi-supervised learning by Ô¨Åne-tuning 1% and 10% images on ImageNet using ResNet-18.
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.511520737327189,"Method
T
1% labels
10% labels"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5161290322580645,"MoCo v2 baseline
-
30.9
45.8
Compress (Abbasi Koohpayegani et al., 2020)
R-50 (67.4)
41.2
47.6
SEED (Fang et al., 2020)
R-50 (67.4)
39.1
50.2
DisCo (Gao et al., 2021)
R-50 (67.4)
39.2
50.1
BINGO
R-50 (67.4)
42.8
57.5
Compress (Abbasi Koohpayegani et al., 2020)
R-152 (74.1)
-
-
SEED (Fang et al., 2020)
R-152 (74.1)
44.3
54.8
DisCo (Gao et al., 2021)
R-152 (74.1)
47.1
54.7
BINGO
R-152 (74.1)
50.3
61.2
BINGO
R-50√ó2 (77.3)
48.2
60.2"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5207373271889401,"Table 6: Linear classiÔ¨Åcation accuracy on
CIFAR-10/100 with ResNet-18."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5253456221198156,"Method
T
CIFAR-10/100"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5299539170506913,"MoCo v2 baseline
-
77.9/48.1
SEED
R-50 (67.4)
82.3/56.8
DisCo
R-50 (67.4)
85.3/63.3
BINGO
R-50 (67.4)
86.8/66.5"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5345622119815668,"Table 7: Lower and Upper bound performance
exploration via the bagging criterion."
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.5391705069124424,"Bagging Criterion
Accuracy (%)"
"CHECKPOINTS
OF
TEACHER
MODELS
CAN
BE
DOWNLOADED
FROM",0.543778801843318,"Random Initialized model
46.6
Supervised-pretrained model
64.8
Ground-truth labels
65.8
Self-supervised pretrained model
64.0"
ABLATION STUDY,0.5483870967741935,"4.3
ABLATION STUDY"
ABLATION STUDY,0.5529953917050692,"In this section, we conduct detailed ablation studies to diagnose how each component affect the
performance of the distilled model. Unless speciÔ¨Åed, all results in this section are based on ResNet-18,
and distilled for 200 epochs."
ABLATION STUDY,0.5576036866359447,"Impact of k in K-nearest Neighbors
We inspect the inÔ¨Çuence of k in K-nearest neighbors bagging
strategy. As shown in Fig. 3, the results are relatively robust for a range of k (k=1,5,10,20). In
addition, we Ô¨Ånd that the classiÔ¨Åcation accuracy decrease with k = 10, 20 compared with k = 5,
because the noise is introduced when k becomes large. However, the performance with a relative
small k = 1 is no better than k = 5, we think the diversity is sacriÔ¨Åced when we only select the top-1
nearest neighborhood all the time."
ABLATION STUDY,0.5622119815668203,"k=1
k=5
k=10
k=20
Top-k in KNN 63.5 64.0"
ABLATION STUDY,0.5668202764976958,Top-1 acc(%) 63.7 64.0 63.8 63.7
ABLATION STUDY,0.5714285714285714,"Figure 3: Top-1 accuracy with dif-
ferent k in K-nearest neighbors."
ABLATION STUDY,0.576036866359447,"Lower and Upper Bound of The Proposed Method
As
shown in Table 7, using data relation extracted from a ran-
dom initialized model gives a poor performance of 46.6%,
which can be a lower bound of our method. Then we try to
explore the upper bound performance by bagging instances
via a supervised-pretrained model, the performance gets an
improvement of 0.8% over using data relation extracted from
the unsupervised pretrained teacher model. When we directly
use the ground truth labels to bag instances, we get a highest
upper bound performance, i.e., 65.8% Top-1 accuracy."
ABLATION STUDY,0.5806451612903226,"Impacts of Data-Relation and Teacher Parameters.
In our experiments, both the data relation
and model parameters of teacher model are used to distill student model. We diagnose how each
component affects the distillation performance. As shown in Table 8, model parameters represent
whether to load the teacher model parameters into the teacher part in Fig. 2; data relation refers to the
Linter loss. Besides, Lintra is intrinsically a basic contrastive-learning paradigm on two views of
one instance. no matter the teacher‚Äôs parameters are loaded or not, using the the data relation from
pretrained teacher model always gets better results than using data relation from online student model,
which veriÔ¨Åes the efÔ¨Åciency of transferring teacher‚Äôs data relation to student model. Interestingly,
we Ô¨Ånd that BINGO even gets good result only utilizing teacher‚Äôs data relation (Row 3 of Table 8),
which is about 10% higher than model training without distillation."
ABLATION STUDY,0.5852534562211982,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.5898617511520737,"Table 8: Effects of utilizing teacher‚Äôs data-relation and teacher‚Äôs pretrained weights. The column
of Student Relation means that we bag data with features extracted from student model online and
the column of Teacher Relation means that we bag data with features extracted from a pretrained
teacher model. When teacher parameters are not used, we replace the pretrained teacher model as a
momentum update of student model like He et al. (2020)."
ABLATION STUDY,0.5944700460829493,"Teacher Parameters
Student Relation
Teacher Relation
Accuracy"
ABLATION STUDY,0.5990783410138248,"


52.2 (w/o distillation)

!

57.2


!
62.2
!


62.0
!
!

62.5
!

!
64.0"
ABLATION STUDY,0.6036866359447005,"Table 10: Top-1 accuracy of linear classiÔ¨Åcation results on ImageNet with ResNet-34 under different
epochs. ResNet-152 is used as teacher model. ‚ÄúT‚Äù denotes the teacher and ‚ÄúS‚Äù denotes the student."
ABLATION STUDY,0.6082949308755761,"Method
S
T
Distillation Epochs
Top-1 Accuracy
Top-5 Accuracy
SEED
R-34
R-152(74.1)
200
62.7
85.8
BINGO
R-34
R-152(74.1)
100
67.8
88.4"
ABLATION STUDY,0.6129032258064516,"DisCo
R-34
R-152(74.1)
200
68.1
88.6
BINGO
R-34
R-152(74.1)
200
69.1
88.9"
ABLATION STUDY,0.6175115207373272,"Table 9: Top-1 accuracy of linear classiÔ¨Å-
cation results on ImageNet using different
distillation methods on ResNet-18 student
model (ResNet-50 is used as teacher model)"
ABLATION STUDY,0.6221198156682027,"Method
Top-1
MoCo-V2 baseline (Gao et al., 2021) 52.2
MoCo-V2 + KD (Fang et al., 2020)
55.3
MoCo-V2 + RKD
61.6
DisCo + KD (Gao et al., 2021)
60.6
DisCo + RKD (Gao et al., 2021)
60.6
BINGO
64.0"
ABLATION STUDY,0.6267281105990783,"Compare with Other Distillation Methods.
We
now compare with several other distillation strategies
to verify the effectiveness of our method. We compare
with two distillation schemes: feature-based distilla-
tion method and relation-based distillation, which is
termed as KD and RKD, respectively. Feature-based
distillation method aims at minimizing l2-distance
of teacher & student‚Äôs embeddings. Relation-based
distillation method aims at minimizing the difference
between inter-sample-similarity graph obtained from
teacher and student model. As shown in Table 9,
BINGO outperforms all theses alternative methods."
ABLATION STUDY,0.631336405529954,"Computational Complexity.
As shown in Fig. 2, the batch size is doubled during each forward
propagation. The computation cost is increased compared with SEED (Fang et al., 2020) and MoCo-
v2 (Chen et al., 2020c). As for DisCo (Gao et al., 2021), the computational complexity is also
increased due to the multiple forward propagation for one sample: once for the mean student, twice
for the online student and twice for the teacher model. The total number of forward propagation is 5,
2.5√ó bigger than SEED and MoCo-v2. For the above analysis, BINGO has a similar computation
cost with DisCo. To compare with SEED and DisCo under the same cost, we distill ResNet-34 from
ResNet-152 for 100 and 200 epochs respectively. We compare results of 100 epochs with SEED and
200 epochs with DisCo. The results are shown in Table 10 and BINGO still shows signiÔ¨Åcantly better
performance than the two compared methods with the same training cost."
CONCLUSIONS,0.6359447004608295,"5
CONCLUSIONS"
CONCLUSIONS,0.6405529953917051,"This paper proposes a new self-supervised distillation method, named BINGO, which bags related
instances by matching embeddings of the teacher. With the instance-wise dataset mapped into a
bag-wise one, the new dataset can be applied to the distillation process for small models. The
knowledge which represents the relation of bagged instances can be transferred by aggregating the
bag, including inter-sample and intra-sample aggregation. Our BINGO follows a relation-guided
principle, which shows stronger effectiveness than previous relation-agnostic methods. The proposed
relation-based distillation is a general strategy for improving unsupervised representation, and we
hope it would shed light on new directions for unsupervised learning."
CONCLUSIONS,0.6451612903225806,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.6497695852534562,ACKNOWLEDGEMENT
CONCLUSIONS,0.6543778801843319,"This work was supported in part by the National Natural Science Foundation of China under Grant
61932022, Grant 61720106001, Grant 61971285, and in part by the Program of Shanghai Science
and Technology Innovation Project under Grant 20511100100."
REPRODUCIBILITY STATEMENT,0.6589861751152074,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.663594470046083,"We will release the source code at https://github.com/haohang96/bingo to guarantee
the reproducibility of this paper."
ETHICS STATEMENT,0.6682027649769585,ETHICS STATEMENT
ETHICS STATEMENT,0.6728110599078341,"The proposed approach seeks to improve the performance of unsupervised learning methods for
small scale models. This will improve the application of unsupervised learning methods in more
realistic computing resource-constrained scenarios. As far as we know, our method does not violate
any ethical requirements."
REFERENCES,0.6774193548387096,REFERENCES
REFERENCES,0.6820276497695853,"Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised
learning by compressing representations. Advances in Neural Information Processing Systems, 33:
12980‚Äì12992, 2020."
REFERENCES,0.6866359447004609,"Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Thirty-fourth
Conference on Neural Information Processing Systems (NeurIPS), 2020."
REFERENCES,0.6912442396313364,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully
connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834‚Äì848,
2017."
REFERENCES,0.695852534562212,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597‚Äì1607. PMLR, 2020a."
REFERENCES,0.7004608294930875,"Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big
self-supervised models are strong semi-supervised learners. Advances in Neural Information
Processing Systems, 33:22243‚Äì22255, 2020b."
REFERENCES,0.7050691244239631,"Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020c."
REFERENCES,0.7096774193548387,"Hee Min Choi, Hyoa Kang, and Dokwan Oh. Unsupervised representation transfer for small networks:
I believe i can distill on-the-Ô¨Çy. Advances in neural information processing systems, 2021."
REFERENCES,0.7142857142857143,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248‚Äì255. Ieee, 2009."
REFERENCES,0.7188940092165899,"Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With
a little help from my friends: Nearest-neighbor contrastive learning of visual representations. arXiv
preprint arXiv:2104.14548, 2021."
REFERENCES,0.7235023041474654,"Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. Seed:
Self-supervised distillation for visual representation. In International Conference on Learning
Representations, 2020."
REFERENCES,0.728110599078341,Published as a conference paper at ICLR 2022
REFERENCES,0.7327188940092166,"Yuting Gao, Jia-Xin Zhuang, Ke Li, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong Ji, and Xing
Sun. Disco: Remedy self-supervised learning on lightweight models with distilled contrastive
learning. arXiv preprint arXiv:2104.09124, 2021."
REFERENCES,0.7373271889400922,"Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick Perez.
Obow: Online bag-of-visual-words generation for self-supervised learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6830‚Äì6840, 2021."
REFERENCES,0.7419354838709677,"Jean-Bastien Grill, Florian Strub, Florent Altch¬¥e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Pires, Zhaohan Guo, Mohammad Azar, et al. Bootstrap
your own latent: A new approach to self-supervised learning. In Neural Information Processing
Systems, 2020."
REFERENCES,0.7465437788018433,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770‚Äì778, 2016."
REFERENCES,0.7511520737327189,"Kaiming He, Georgia Gkioxari, Piotr Doll¬¥ar, and Ross Girshick. Mask r-cnn. In Proceedings of the
IEEE international conference on computer vision, pp. 2961‚Äì2969, 2017."
REFERENCES,0.7557603686635944,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729‚Äì9738, 2020."
REFERENCES,0.7603686635944701,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015."
REFERENCES,0.7649769585253456,"Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural
Information Processing Systems, 33, 2020."
REFERENCES,0.7695852534562212,"Nikos Komodakis and Spyros Gidaris. Unsupervised representation learning by predicting image
rotations. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.7741935483870968,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll¬¥ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740‚Äì755. Springer, 2014."
REFERENCES,0.7788018433179723,"Yufan Liu, Jiajiong Cao, Bing Li, Chunfeng Yuan, Weiming Hu, Yangxi Li, and Yunqiang Duan.
Knowledge distillation via instance relationship graph. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 7096‚Äì7104, 2019."
REFERENCES,0.783410138248848,"K L Navaneet, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Simreg:
Regression as a simple yet effective tool for self-supervised knowledge distillation. In British
Machine Vision Conference (BMVC), 2021."
REFERENCES,0.7880184331797235,"Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European Conference on Computer Vision, pp. 69‚Äì84. Springer, 2016."
REFERENCES,0.7926267281105991,"Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3967‚Äì3976,
2019."
REFERENCES,0.7972350230414746,"Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2536‚Äì2544, 2016."
REFERENCES,0.8018433179723502,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. Advances in neural information processing systems, 28:
91‚Äì99, 2015."
REFERENCES,0.8064516129032258,"Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014."
REFERENCES,0.8110599078341014,Published as a conference paper at ICLR 2022
REFERENCES,0.815668202764977,"Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In Interna-
tional Conference on Learning Representations, 2019."
REFERENCES,0.8202764976958525,"Guodong Xu, Ziwei Liu, Xiaoxiao Li, and Chen Change Loy. Knowledge distillation meets self-
supervision. In European Conference on Computer Vision, pp. 588‚Äì604. Springer, 2020a."
REFERENCES,0.8248847926267281,"Haohang Xu, Xiaopeng Zhang, Hao Li, Lingxi Xie, Hongkai Xiong, and Qi Tian.
Seed the
views: Hierarchical semantic alignment for contrastive representation learning. arXiv preprint
arXiv:2012.02733, 2020b."
REFERENCES,0.8294930875576036,"Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-
mance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928,
2016."
REFERENCES,0.8341013824884793,"Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European
conference on computer vision, pp. 649‚Äì666. Springer, 2016."
REFERENCES,0.8387096774193549,"Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang.
Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective. In Interna-
tional Conference on Learning Representations, 2021."
REFERENCES,0.8433179723502304,"A
APPENDIX"
REFERENCES,0.847926267281106,"A.1
RESULTS OF K-MEANS BAGGING STRATEGY"
REFERENCES,0.8525345622119815,Table 11: Top-1 accuracy with different cluster numbers C in K-means clustering.
REFERENCES,0.8571428571428571,"Number of Clusters (C)
Accuracy"
REFERENCES,0.8617511520737328,"5000
62.7
10000
63.5
20000
63.8
50000
63.6"
REFERENCES,0.8663594470046083,"We additionally evaluate the performance of using K-means clustering as the bagging strategy
according to Eq. 2 in the main text. Given the pseudo-label q = {q1, q2, ..., qN} and the anchor
instance xa, the bag associated with xa is deÔ¨Åned as"
REFERENCES,0.8709677419354839,"‚Ñ¶a = {i | qi = qa, i = 1, 2, ..., N}.
(10)"
REFERENCES,0.8755760368663594,"For implementation, ResNet-18 and ResNet-50 are used as the student and teacher model respectively.
We evaluate the linear classiÔ¨Åcation accuracy of the student model on ImageNet-1K. We study various
cluster numbers C as shown in Tab. 11. We Ô¨Ånd that a bigger cluster number can bring better results
than a smaller one. Noting that the linear classiÔ¨Åcation accuracy of bagging with K-nearest neighbors
(where k=5) is slightly better than bagging via K-means clustering (with C=20000), i.e. 64.0% vs.
63.8%. Moreover, bagging with KNN is more convenient to implement, so we choose the KNN-based
bagging strategy in implementation."
REFERENCES,0.880184331797235,"A.2
OBJECT DETECTION AND INSTANCE SEGMENTATION OF RESNET-34"
REFERENCES,0.8847926267281107,"We further evaluate the generalization ability of one more student model, i.e. ResNet-34, on object
detection and instance segmentation tasks. The COCO (Lin et al., 2014) dataset is used for evaluation.
Following He et al. (2020), we use Mask R-CNN (He et al., 2017) for object detection and instance
segmentation, and Ô¨Åne-tune parameters of the student model , i.e. ResNet-34 distilled from the ResNet-
152 teacher model. As shown in Table 12, BINGO consistently outperforms models pretrained with
no distillation, SEED Fang et al. (2020) and DisCo Gao et al. (2021)."
REFERENCES,0.8894009216589862,Published as a conference paper at ICLR 2022
REFERENCES,0.8940092165898618,"Table 12: Transfer learning performance on COCO Lin et al. (2014) object detection and instance
discrimination with ResNet-34 distilled from ResNet-152. ‚Äúbb‚Äù denotes bounding box and ‚Äúmk‚Äù
denotes mask."
REFERENCES,0.8986175115207373,Method
REFERENCES,0.9032258064516129,"Mask R-CNN, ResNet-34
Object Detection
Instance Discrimination
APbb APbb
50 APbb
75 APS APM APL APmk APmk
50
APmk
75
APS APM APL
MoCo v2
38.1
56.8
40.7
-
-
-
33.0
53.2
35.3
-
-
-
SEED Fang et al. (2020)
38.4
57.0
41.0
-
-
-
33.3
53.7
35.3
-
-
-
DisCo Gao et al. (2021)
39.4
58.7
42.7
-
-
-
34.4
55.4
36.7
-
-
-
BINGO
39.9
59.4
43.5
22.8
43.3
52.1
35.7
56.5
38.2
16.8
37.9
51.6"
REFERENCES,0.9078341013824884,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9124423963133641,(a) MoCo-v2 baseline
REFERENCES,0.9170506912442397,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9216589861751152,(b) Distill w/o Bag-Aggregation
REFERENCES,0.9262672811059908,"0.0
0.2
0.4
0.6
0.8
1.0 0.0 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.9308755760368663,(c) Distill w/ Bag-Aggregation
REFERENCES,0.9354838709677419,"Figure 4: t-sne visualization of student‚Äôs representations pretrained with the MoCo-v2 baseline (a),
and distilled with (b) and without bag aggregation (c)."
REFERENCES,0.9400921658986175,"A.3
ANALYSIS AND DISCUSSIONS"
REFERENCES,0.9447004608294931,"We now inspect what the student learns during the distillation. Firstly we compute the average
distance between anchor sample xa and its positive samples xp in a bag ‚Ñ¶a over the whole dataset:"
REFERENCES,0.9493087557603687,"BagDis =
E
xa‚àºX
E
xp‚àº‚Ñ¶a
||(fS(xa) ‚àífS(xp))||2
2
(11)"
REFERENCES,0.9539170506912442,"According to Eq. 11, we compute the averaged distance in the bag using distilled student model. As
shown in Table 13, the averaged distance in a bag is smallest when the student model is distilled
with bag-aggregation loss. We also compute the intra-class distance among all intra-class pairwise
samples. As shown in Table 14, the proposed method also aggregate the bag of labels with the same
ground truth labels on the unseen validation set."
REFERENCES,0.9585253456221198,"Table 13: Averaged distance between
anchor and positive samples in the same bag"
REFERENCES,0.9631336405529954,"Method
Distance"
REFERENCES,0.967741935483871,"MoCo-V2 baseline
0.38"
REFERENCES,0.9723502304147466,"Distill w/o Bag-Aggregation
0.36
Distill w/ Bag-Aggregation
0.32"
REFERENCES,0.9769585253456221,"Table 14: Averaged intra-class distance on Ima-
geNet validation set"
REFERENCES,0.9815668202764977,"Method
Distance"
REFERENCES,0.9861751152073732,"MoCo-V2 baseline
0.88"
REFERENCES,0.9907834101382489,"Distill w/o Bag-Aggregation
0.72
Distill w/ Bag-Aggregation
0.65"
REFERENCES,0.9953917050691244,"We visualize the last embedding feature to understanding the aggregating properties of the proposed
method. 10 classes are randomly selected from validation set. We provide the t-sne visualization of
the student features. As shown in Fig. 4, the same color denotes features with the same label. It can
be seen that BINGO gets more compact representations compared with models without distillation or
distilling without pulling related samples in a bag."
