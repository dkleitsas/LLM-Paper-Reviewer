Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004524886877828055,"In this paper, we introduce APOLLO, a quasi-Newton method for nonconvex
stochastic optimization, which dynamically incorporates the curvature of the loss
function by approximating the Hessian via a diagonal matrix. Importantly, the up-
date and storage of the diagonal approximation of Hessian is as efﬁcient as adaptive
ﬁrst-order optimization methods with linear complexity for both time and memory.
To handle nonconvexity, we replace the Hessian with its rectiﬁed absolute value,
which is guaranteed to be positive-deﬁnite. Experiments on three tasks of vision
and language show that APOLLO achieves signiﬁcant improvements over other
stochastic optimization methods, including SGD and variants of Adam, in terms of
both convergence speed and generalization performance. The implementation of
the algorithm is available at anonymous link."
INTRODUCTION,0.00904977375565611,"1
INTRODUCTION"
INTRODUCTION,0.013574660633484163,"Nonconvex stochastic optimization is of core practical importance in many ﬁelds of machine learning,
in particular for training deep neural networks (DNNs). First-order gradient-based optimization
algorithms, conceptually attractive due to their linear efﬁciency on both the time and memory
complexity, have led to tremendous progress and impressive successes. A number of advanced
ﬁrst-order algorithms have emerged over the years to pursue fast and stable convergence, among
which stochastic gradient descent (SGD) (Robbins & Monro, 1951; LeCun et al., 1998), equipped
with momentum (Rumelhart et al., 1985; Qian, 1999; Bottou & Bousquet, 2008), has stood out
for its simplicity and effectiveness across a wide range of applications (Hinton & Salakhutdinov,
2006; Hinton et al., 2012; Graves, 2013). However, one disadvantage of SGD is that the gradients
in different directions are scaled uniformly, resulting in limited convergence speed and sensitive
choice of learning rate, and thus has spawned a lot of recent interests in accelerating SGD from the
algorithmic and practical perspectives."
INTRODUCTION,0.01809954751131222,"Recently, many adaptive ﬁrst-order optimization methods have been proposed to achieve rapid
training progress with element-wise scaled learning rates, and we can only mention a few here due
to space limits. In their pioneering work, Duchi et al. (2011) proposed AdaGrad, which scales the
gradient by the square root of the accumulative square gradients from the ﬁrst iteration. While
AdaGrad works well for sparse settings, its performance signiﬁcantly degrades for dense settings,
primarily due to the monotonic increase of the accumulation. Subsequently, several methods have
been proposed with the intuition to limit the accumulation to a small window of past iterations, and
in particular exponentially reduce the weight of earlier iterations. Notable works incorporating this
method are RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma &
Ba, 2015), among which Adam has become the default optimization algorithm across many deep
learning applications because of its fast convergence speed and relatively consistent selections of
hyper-parameters (Ruder, 2016; Zhang et al., 2020). However, it has been observed that these adaptive
optimization methods may converge to bad/suspicious local optima, resulting in worse generalization
ability than their non-adaptive counterparts (Wilson et al., 2017), or fail to converge due to unstable
and extreme learning rates (Luo et al., 2019)."
INTRODUCTION,0.02262443438914027,"Quasi-Newton methods have been widely used in solving convex optimization problems, due to their
efﬁcient computation and fast convergence rate (Broyden, 1967; Dennis & Moré, 1977). However,
the stochastic, high-dimensional and nonconvex nature of many machine learning tasks, such as"
INTRODUCTION,0.027149321266968326,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03167420814479638,"training deep neural networks, has rendered many classical quasi-Newton methods ineffective and/or
inefﬁcient (Keskar & Berahas, 2016; Wang et al., 2017; Yao et al., 2020). Indeed, in many natural
language processing (NLP) and computer vision (CV) tasks (He et al., 2016; Ma & Hovy, 2016;
Luo et al., 2019), SGD (with momentum) is chosen as the optimizer, beneﬁting from its stable and
efﬁcient training and outstanding generalization."
INTRODUCTION,0.03619909502262444,"In this work, we develop APOLLO, a quasi-Newton method for nonconvex stochastic optimization
to simultaneously tackle the aforementioned challenges of stochastic variance, nonconvexity and
inefﬁciency. Algorithmically, APOLLO dynamically incorporates the curvature of the objective
function with diagonally approximated Hessian. It only requires ﬁrst-order gradients and updates
the approximation of the Hessian diagonally so that it satisﬁes a parameter-wise version of the
weak secant condition (Wolfe, 1959). To handle nonconvexity, we replace the Hessian with its
rectiﬁed absolute value, the computation of which is also efﬁcient under our diagonal approximation,
yielding an efﬁcient optimization algorithm with linear complexity for both time and memory (§3).
Experimentally, through three tasks on CV and NLP with popular deep neural networks, including
ResNets (He et al., 2016), LSTMs (Hochreiter & Schmidhuber, 1997) and Transformers (Vaswani
et al., 2017), we demonstrate that APOLLO signiﬁcantly outperforms SGD and variants of Adam, in
terms of both convergence speed and generalization performance (§4)."
BACKGROUNDS,0.04072398190045249,"2
BACKGROUNDS"
BACKGROUNDS,0.04524886877828054,"In this section, we set up the notations on nonconvex stochastic optimization, brieﬂy review the
(quasi-) Newton methods, and discuss the problems of applying quasi-Newton methods to nonconvex
stochastic optimization that we attempt to study in the rest of the paper."
NONCONVEX STOCHASTIC OPTIMIZATION,0.049773755656108594,"2.1
NONCONVEX STOCHASTIC OPTIMIZATION"
NONCONVEX STOCHASTIC OPTIMIZATION,0.05429864253393665,"In this paper, we consider the following nonconvex stochastic optimization problem:
min
θ∈Rd f(θ) = E[l(θ; Γ)]
(1)"
NONCONVEX STOCHASTIC OPTIMIZATION,0.058823529411764705,"where l : Rd × Rn →R is a continuously differentiable (and possible nonconvex) function, θ ∈Rd
denotes the parameter to be optimized, Γ ∈Rn denotes a random variable with distribution function
P, and E[·] denotes the expectation w.r.t Γ. Intuitively, Γ incorporates noises in f, leading to a
stochastic objective function. A special case of (1) that arises frequently in machine learning is the
empirical risk minimization problem:"
NONCONVEX STOCHASTIC OPTIMIZATION,0.06334841628959276,"min
θ∈Rd f(θ) = 1 N N
X"
NONCONVEX STOCHASTIC OPTIMIZATION,0.06787330316742081,"i=1
li(θ)
(2)"
NONCONVEX STOCHASTIC OPTIMIZATION,0.07239819004524888,"where li : Rd →R is the loss function corresponding to the i-th data, and N is the number of data
samples that is assumed to be extremely large. Objective functions may also have other sources of
noise than data subsampling, such as dropout (Srivastava et al., 2014) in deep neural networks."
NONCONVEX STOCHASTIC OPTIMIZATION,0.07692307692307693,"Decoupled Parameters.
In this work, we consider a setting of decoupled parameters: θ =
{θ(l), l = 1, . . . , L}. Intuitively, under this setting the parameter θ is decoupled into a sequence of
parameters serving different functionalities. For example, in neural network training the parameters
of a neural network can be naturally decoupled into the parameters of different layers or modules."
NEWTON AND QUASI-NEWTON METHODS,0.08144796380090498,"2.2
NEWTON AND QUASI-NEWTON METHODS"
NEWTON AND QUASI-NEWTON METHODS,0.08597285067873303,"Newton’s method usually employs the following updates to solve (1):
θt+1 = θt −H−1
t
gt
(3)
where gt = ∇f(θt) is the gradient at θt and Ht = ∇2f(θt) is the Hessian matrix. The convergence
rate of Newton’s method is quadratic under standard assumptions (Nocedal & Wright, 2006). How-
ever, major challenges with this method are i) the expensive computation of the inverse Hessian at
every iteration and the corresponding quadratic memory complexity; and ii) the limitation to convex
functions (nonconvexity results in negative curvature of Ht and misleads the update directions)."
NEWTON AND QUASI-NEWTON METHODS,0.09049773755656108,"A standard alternative to Newton’s method is a class of quasi-Newton methods, which have been
widely used in solving convex deterministic optimization problem:
θt+1 = θt −ηtB−1
t
gt
(4)"
NEWTON AND QUASI-NEWTON METHODS,0.09502262443438914,Under review as a conference paper at ICLR 2022
NEWTON AND QUASI-NEWTON METHODS,0.09954751131221719,"where ηt is the stepsize (a.k.a learning rate), Bt is an approximation to the Hessian matrix ∇2f(θt)
at θt, which is updated based on the well-known secant equation:
Bt+1 = argmin
B
∥B −Bt∥"
NEWTON AND QUASI-NEWTON METHODS,0.10407239819004525,"s.t.
Bt+1st = yt
(secant equation)
(5)"
NEWTON AND QUASI-NEWTON METHODS,0.1085972850678733,"where st = θt+1−θt and yt = gt+1−gt. Bt+1 is, in the sense of some matrix norm, the closest to Bt
among all symmetric matrices that satisfy the secant equation. Each choice of the matrix norm results
in a different update formula, such as DFP (Davidon, 1991; Fletcher, 1987) and BFGS (Broyden,
1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970). The popularity of this method is due to the
fact that only the gradient of the objective function is required at each iteration. Since no second
derivatives (Hessian) are required, quasi-Newton methods are sometimes more efﬁcient than Newton’s
method, especially when the computation of Hessian is expensive. To further reduce memory cost,
one seminal work is the limited memory BFGS (L-BFGS) (Liu & Nocedal, 1989; Byrd et al., 1995)
that achieves desirable linear computational and memory complexity by approximating the Hessian
as a series of sum of ﬁrst order information from previous iterations."
PROBLEMS OF QUASI-NEWTON METHODS,0.11312217194570136,"2.3
PROBLEMS OF QUASI-NEWTON METHODS"
PROBLEMS OF QUASI-NEWTON METHODS,0.11764705882352941,"Despite their impressive successes on convex deterministic optimization, quasi-Newton methods
suffer from their own problems in more challenging scenarios. In this section, we mainly discuss
three problems preventing quasi-Newton methods from being applied to the scenario of large-
scale nonconvex stochastic optimization. Due to these problems, no quasi-Newton methods (to
our best knowledge) designed for nonconvex optimization consistently outperform adaptive ﬁrst-
order algorithms w.r.t convergence speed and generalization performance. The main goal of this
work is to algorithmically design and experimentally demonstrate a novel quasi-Newton method, in
hope of improving the convergence speed and generalization performance of nonconvex stochastic
optimization eventually."
PROBLEMS OF QUASI-NEWTON METHODS,0.12217194570135746,"Stochastic Variance.
One challenge of quasi-Newton methods on nonconvex stochastic optimiza-
tion (1) is the variance introduced by the stochastic nature of the problem. At each iteration, only the
stochastic gradient gt is available, which is an unbiased estimation of the gradient ∇f(θt) and may
lead to an erroneous approximation of Hessian (Byrd et al., 2011)."
PROBLEMS OF QUASI-NEWTON METHODS,0.12669683257918551,"Nonconvexity.
Another key challenge in designing such quasi-Newton methods lies in the difﬁculty
of preserving the positive-deﬁniteness of Bt in (5), due to the nonconvexity of the objective function.
What is worse is that performing line search is infeasible in the stochastic setting, due to the presence
of noise in the stochastic gradients (Wang et al., 2017)."
PROBLEMS OF QUASI-NEWTON METHODS,0.13122171945701358,"Computational and Memory Efﬁciency.
Even though quasi-Newton methods are more efﬁcient
than Newton’s method, the time and memory complexities are still relatively large compared with
adaptive ﬁrst-order methods. For instance, L-BFGS requires to store ﬁrst-order information from m
previous iterations with commonly m ≥5, which is still too expensive for deep neural networks con-
taining millions of parameters. Moreover, adapting quasi-Newton methods to nonconvex stochastic
optimization probably introduces additional computation, further slowing down these methods."
ADAPTIVE PARAMETER-WISE DIAGONAL QUASI-NEWTON,0.13574660633484162,"3
ADAPTIVE PARAMETER-WISE DIAGONAL QUASI-NEWTON"
ADAPTIVE PARAMETER-WISE DIAGONAL QUASI-NEWTON,0.14027149321266968,"With the end goal of designing an efﬁcient quasi-Newton method to solve the problem in (1) in
mind, we ﬁrst propose to approximate the Hessian with a diagonal matrix, whose elements are
determined by the variational approach subject to the parameter-wise weak secant equation (§3.1).
Then, we explain our stepsize bias correction technique to reduce the stochastic variance in §3.2. To
handle nonconvexity, we directly use the rectiﬁed absolute value of the diagonally approximated
Hessian as the preconditioning of the gradient (§3.3). The initialization technique of APOLLO
allows us to eliminate one hyper-parameter (§3.4). At last, we provide a theoretical analysis of
APOLLO’s convergence in both convex optimization and nonconvex stochastic optimization (§3.5).
The pseudo-code is shown in Algorithm 1."
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.14479638009049775,"3.1
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION"
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.1493212669683258,"As discussed in Bordes et al. (2009), designing an efﬁcient stochastic quasi-Newton algorithm
involves a careful trade-off between the sparsity of the approximation matrix Bt and the quality of"
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.15384615384615385,Under review as a conference paper at ICLR 2022
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.1583710407239819,"its approximation of the Hessian Ht, and diagonal approximation is a reasonable choice (Becker
et al., 1988; Zhu et al., 1999). If B is chosen to be a diagonal matrix satisfying (5), one can obtain a
formula similar to the SGD-QN algorithm (Bordes et al., 2009)."
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.16289592760180996,"An alternative of the secant equation in the updating formula (5), as ﬁrst introduced by Nazareth
(1995), is the weak secant equation (Dennis & Wolkowicz, 1993):
Bt+1 = argmin
B
∥B −Bt∥"
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.167420814479638,"s.t.
sT
t Bt+1st = sT
t yt
(weak secant equation)
(6)"
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.17194570135746606,"The motivation of using the weak secant condition in diagonal quasi-Newton method is straight-
forward: the standard mean-value theorem might not necessarily hold for vector-valued functions
expressed in the secant equation, Bt+1st = yt ≈∇2f(θt)st. Thus, we do not know whether there
exists a vector ˜θ ∈Rd such that yt = ∇2f(˜θ)st (Dennis & Moré, 1977). On the other hand, the
Taylor theorem ensures that there exists such ˜θ that sT
t yt = sT
t ∇2f(˜θ)st, leading to the reasonable
assumption of the weak secant condition (6)."
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.17647058823529413,"Based on the variational technique (Zhu et al., 1999), the solution of (6) with Frobenius norm is:"
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.18099547511312217,"Λ ≜Bt+1 −Bt = sT
t yt −sT
t Btst
∥st∥4
4
Diag(s2
t)
(7)"
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.18552036199095023,"where s2
t is the element-wise square vector of st, Diag(s2
t) is the diagonal matrix with diagonal
elements from vector s2
t, and ∥· ∥4 is the 4-norm of a vector."
QUASI-NEWTON METHODS WITH DIAGONAL HESSIAN APPROXIMATION,0.19004524886877827,"Parameter-Wise Weak Secant Condition.
However, in optimization problems with high-
dimensional parameter space, such as training deep neural networks with millions of parameters, the
weak secant condition might be too ﬂexible to produce a good Hessian approximation. In the setting
of decoupled parameters (§2.1), we propose a parameter-wise version of the weak secant equation
to achieve a trade-off between the secant and weak secant conditions: for each parameter θ(l) ∈θ,
we update B corresponding to θ(l) by solving (6) individually. Remarkably, the secant condition
restricts B with an equation of a d-dimensional vector, while the weak secant condition relaxes it
with a 1-dimensional scalar. The parameter-wise weak secant condition expresses the restriction as a
l-dimension vector (1 < l < d), resulting in a reasonable trade-off. The updating formula is the same
as (7) for each parameter-wise B."
STEPSIZE BIAS CORRECTION,0.19457013574660634,"3.2
STEPSIZE BIAS CORRECTION"
STEPSIZE BIAS CORRECTION,0.19909502262443438,"To mitigate the stochastic variance problem in stochastic quasi-Newton methods, APOLLO utilizes
stepsize bias correction on the stochastic gradients at each step t. We know that the optimal step-
size ηt equals to 1 w.r.t the quadratic approximation underlying Newton’s method, if the Hessian
approximation Bt and the stochastic gradient gt are close to the exact Hessian Ht and gradient
∇f(θt), respectively. Inspired by this, we correct the stepsize bias in the stochastic gradient gt
by replacing it with a corrected gradient g′
t = ηtgt. Together with the corresponding corrected
y′
t = g′
t+1 −g′
t = ηtyt, we correct the updating term Λ of Bt in (7) by replacing yt with y′
t:"
STEPSIZE BIAS CORRECTION,0.20361990950226244,"Λ′ = sT
t y′
t −sT
t Btst
∥st∥4
4
Diag(s2
t) = −dT
t yt + dT
t Btdt
∥dt∥4
4
Diag(d2
t)
(8)"
STEPSIZE BIAS CORRECTION,0.2081447963800905,"where dt = −st/ηt = B−1
t
gt is the corrected update direction. Note that after applying the step
bias correction, the update formula of Bt in (8) is independent with the stepsize ηt, eliminating the
stepsize bias. Technically, the stepsize bias correction is designed to reduce the stochastic variance,
rather than entirely discarding the stepsize. The APOLLO algorithm (Algorithm 1) still incorporates
the stepsize at every iteration to enforce convergence."
STEPSIZE BIAS CORRECTION,0.21266968325791855,"Based on previous studies, incorporating exponential moving averages (EMVs) for the stochastic
gradients signiﬁcantly reduces the variance (Kingma & Ba, 2015). We follow these works and apply
EMV to gt, together with the initialization bias correction:"
STEPSIZE BIAS CORRECTION,0.2171945701357466,mt+1 = β(1 −βt)
STEPSIZE BIAS CORRECTION,0.22171945701357465,"1 −βt+1 mt +
1 −β
1 −βt+1 gt+1
(9)"
STEPSIZE BIAS CORRECTION,0.22624434389140272,"where 0 < β < 1 is the decay rate of EMV and yt in (8) is written as mt+1 −mt. Note that we
do not apply moving average methods to the approximated Hessian, though the diagonal matrix is
easier to be explicitly formed to average than full matrices. Investigating the moving average of the
diagonal Bt might be an interesting direction of future work."
STEPSIZE BIAS CORRECTION,0.23076923076923078,Under review as a conference paper at ICLR 2022
STEPSIZE BIAS CORRECTION,0.23529411764705882,"Algorithm 1: APOLLO, our proposed algorithm for nonconvex stochastic optimization. All
operations on vectors are element-wise. Good default settings are β = 0.9 and ϵ = 1e−4.
Initial: m0, d0, B0 ←0, 0, 0
// Initialize m0, d0, B0 to zero
while t ∈{0, . . . , T} do"
STEPSIZE BIAS CORRECTION,0.2398190045248869,"for θ ∈{θ1, . . . , θL} do"
STEPSIZE BIAS CORRECTION,0.24434389140271492,"gt+1 ←∇ft(θt)
// Calculate gradient at step t
mt+1 ←β(1−βt)"
STEPSIZE BIAS CORRECTION,0.248868778280543,"1−βt+1 mt +
1−β
1−βt+1 gt+1
// Update bias-corrected moving
average"
STEPSIZE BIAS CORRECTION,0.25339366515837103,"α ←dT
t (mt+1−mt)+dT
t Btdt
(∥dt∥4+ϵ)4
// Calculate coefficient of B update
Bt+1 ←Bt −α · Diag(d2
t)
// Update diagonal Hessian
Dt+1 ←rectify(Bt+1, 0.01)
// Handle nonconvexity
dt+1 ←D−1
t+1mt+1
// Calculate update direction
θt+1 ←θt −ηt+1dt+1
// Update parameters
end
end"
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.2579185520361991,"3.3
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY"
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.26244343891402716,"To guarantee convergence, quasi-Newton methods require the approximated Hessian matrix Bt to
be positive deﬁnite at each step. The common strategy in previous studies is to solve the updating
formula in (5) by restricting the candidate matrix B to be symmetric positive deﬁnite. It is known
that the BFGS update preserves the positive-deﬁniteness of Bt+1 as long as the curvature condition
sT
t yt > 0 holds, which can be guaranteed for strongly convex problem. For nonconvex problem, the
curvature condition can be satisﬁed by performing a line search, which is, however, expensive or
even infeasible in stochastic setting, because the exact function values and gradient information are
unavailable. Wang et al. (2017) proposed the stochastic damped L-BFGS (SdLBFGS) method that
implicitly generates a positive deﬁnite matrix without line search. However, it usually requires large
history size (m ≥100) to guarantee convergence, which is infeasible for large-scale optimization."
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.2669683257918552,"To handle nonconvexity, we adopt a different strategy that does not require the solution of Bt in (5)
to be positive deﬁnite. Intuitively, we search for Bt that is a good approximation of the real Hessian,
which is not necessarily positive deﬁnite in nonconvex problem. When we use Bt as preconditioning
to calculate the update direction, we use its absolute value: |Bt| =
p"
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.27149321266968324,"BT
t Bt, where √· is the positive
deﬁnite square root of a matrix. The motivation of absolute value is straight-forward: for dimensions
with large absolute values of curvature, the objective function could be very sharp and we would
prefer to take relatively smaller steps than those ﬂatter dimensions. Since APOLLO formulate Bt as a
diagonal matrix, the cost of computing |Bt| is marginal."
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.27601809954751133,"Rectiﬁed Absolute Value of Bt
For nonconvex objective functions, there exist inﬂection points
whose curvatures are zero. To prevent the steps from becoming arbitrarily large, we rectify the
absolute value of Bt with a convexity hyper-parameter σ:"
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.28054298642533937,"Dt = rectify(Bt, σ) = max(|Bt|, σ)
(10)"
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.2850678733031674,"where the rectify(·, σ) function is similar to the rectiﬁed linear unit (ReLU) (Nair & Hinton, 2010)
with threshold set to σ. The update direction in (8) is then dt = D−1
t mt."
RECTIFIED ABSOLUTE VALUE OF HESSIAN FOR NONCONVEXITY,0.2895927601809955,"AdaHessian (Yao et al., 2020) used an idea similar to the absolute values of Bt to handle nonconvexity,
where the root mean square averaging is applied to compute the Hessian diagonal. Different from
APOLLO, AdaHessian requires second-order information to compute the Hessian matvec oracle and
approximate the Hessian diagonal using Hutchinson’s method, which is signiﬁcantly more costly."
INITIALIZATION,0.29411764705882354,"3.4
INITIALIZATION"
INITIALIZATION,0.2986425339366516,"The rectiﬁed Dt in (10) introduces one more hyper-parameter σ, limiting the application of APOLLO
in practice. In this section, we show that the zero initialization approach in APOLLO, which initializes
the moving average of gradient m0, the parameter update direction d0 and the diagonal approximation
of Hessian B0 as (vector of) zeros, leads to coupled stepsize η and convexity σ, allowing us to
eliminate one hyper-parameter of η or σ."
INITIALIZATION,0.3031674208144796,Under review as a conference paper at ICLR 2022
INITIALIZATION,0.3076923076923077,"Coupled Stepsize η and Convexity σ.
With the zero initialization of m0, d0 and B0, the following
theorem illustrates the relation between η and σ (details in Appendix A):
Theorem 1. Given zero initialization of m0, d0, and B0 and a ﬁxed parameter intialization θ0.
Suppose that we have two sets of hyper-parameters η, σ and η′, σ′ with the same ratio: η"
INITIALIZATION,0.31221719457013575,σ = η′
INITIALIZATION,0.3167420814479638,"σ′ . Then
the convergence trajectories of these two sets of hyper-parameters are exactly the same:
θt = θ′
t, ∀t ∈{1, . . . , T}.
(11)
where θt and θ′
t are the parameters of (η, σ) and (η′, σ′) at iteration t, respectively."
INITIALIZATION,0.3212669683257919,"From Theorem 1, we observe that η and σ are coupled with each other and in practice we only need
to tune one of them, leaving the other ﬁxed. Therefore, in our experiments (§4), we ﬁx σ = 0.01 and
tune η on different problems1."
INITIALIZATION,0.3257918552036199,"Learning Rate Warmup for APOLLO
As discussed in Kingma & Ba (2015), zero initialization
leads to estimations biased towards zero in the initial iterations. For the moving average mt, this
bias can be corrected by dividing the bias-correction term (9). For dt and Bt, however, we cannot
derive such bias correction terms. Fortunately, a simple linear warmup heuristic of η at the beginning
iterations achieves remarkably stable training."
CONVERGENCE ANALYSIS,0.33031674208144796,"3.5
CONVERGENCE ANALYSIS"
CONVERGENCE ANALYSIS,0.334841628959276,"Similar to previous work (Reddi et al., 2018; Chen et al., 2019; Zhuang et al., 2020), we omit the
initialization bias correction step, i.e. we use mt = βtmt−1 + (1 −βt)gt, 0 < βt < 1, ∀t ∈[T]."
CONVERGENCE ANALYSIS,0.3393665158371041,"We ﬁrst analyze the convergence of APOLLO in convex optimization using the online learning
framework (Zinkevich, 2003) for a sequence of convex cost functions f1(θ), f2(θ), . . . , fT (θ).
Theorem 2. (Convergence in convex optimization) Let {θt} be the sequence from APOLLO. Suppose
ηt =
η
√"
CONVERGENCE ANALYSIS,0.3438914027149321,"t, 0 < βt ≤β ≤1 ∥gt∥2 ≤G, ∥Dt−1∥1"
CONVERGENCE ANALYSIS,0.34841628959276016,"ηt−1
≤∥Dt∥1"
CONVERGENCE ANALYSIS,0.35294117647058826,"ηt
, ∥θt −θt′∥2 ≤D, ∀t, t′ ∈[T]. For θt
generated with the APOLLO algorithm, we have the following bound on the regret: RT ≤ √"
CONVERGENCE ANALYSIS,0.3574660633484163,"TD2∥DT ∥1
2η(1 −β)
+ ηG2"
CONVERGENCE ANALYSIS,0.36199095022624433,"1 −β (2
√"
CONVERGENCE ANALYSIS,0.3665158371040724,"T −1) +
D2"
CONVERGENCE ANALYSIS,0.37104072398190047,"2(1 −β) T
X t=1"
CONVERGENCE ANALYSIS,0.3755656108597285,"β2
t
ηt
(12)"
CONVERGENCE ANALYSIS,0.38009049773755654,"The following result falls as an immediate corollary of the above result.
Corollary 2.1. Suppose βt = βλt−1, 0 < λ < 1 in Theorem 2, we have RT ≤ √"
CONVERGENCE ANALYSIS,0.38461538461538464,"TD2∥DT ∥1
2η(1 −β)
+ ηG2"
CONVERGENCE ANALYSIS,0.3891402714932127,"1 −β (2
√"
CONVERGENCE ANALYSIS,0.3936651583710407,"T −1) +
D2β2"
CONVERGENCE ANALYSIS,0.39819004524886875,"2η(1 −β)(1 −λ2)2
(13)"
CONVERGENCE ANALYSIS,0.40271493212669685,"Theorem 2 implies the regret of APOLLO is upper bounded by O(
√"
CONVERGENCE ANALYSIS,0.4072398190045249,"T). The conditions for Corol-
lary 2.1, as in Reddi et al. (2018), can be relaxed to βt = β/t and still ensures a regret of O(
√ T)."
CONVERGENCE ANALYSIS,0.4117647058823529,"For nonconvex case, we analyze the convergence rate of APOLLO with the similar derivations of that
in Chen et al. (2019), since APOLLO belongs to the family of generalized Adam-type methods:
Theorem 3. (Convergence in nonconvex stochastic optimization) Under the assumptions:
• f is lower bounded and differentiable; ∥∇f(θ)−∇f(θ′)∥2 ≤L∥θ −θ′∥2, ∥Dt∥∞< L, ∀t, θ, θ′.
• Both the true and stochastic gradient are bounded, i.e. ∥∇f(θt)∥2 ≤H, ∥gt∥2 ≤H, ∀t.
• Unbiased and independent noise in gt, i.e. gt = ∇f(θt) + ζt, E[ζt] = 0, and ζi ⊥ζj, ∀i ̸= j.
Assume ηt =
η
√"
CONVERGENCE ANALYSIS,0.416289592760181,"t, βt ≤β ≤1 in non-increasing, Dt−1,j"
CONVERGENCE ANALYSIS,0.42081447963800905,"ηt−1
≤Dt,j"
CONVERGENCE ANALYSIS,0.4253393665158371,"ηt , ∀t ∈[T], j ∈[d], then:"
CONVERGENCE ANALYSIS,0.4298642533936652,"min
t∈[T ] E

∥∇f(θt)∥2
2

≤
L
√"
CONVERGENCE ANALYSIS,0.4343891402714932,"T
(C1η2H2(1 + log T) + C2dη + C3dη2 + C4)
(14)"
CONVERGENCE ANALYSIS,0.43891402714932126,"where C1, C2, C3 are constants independent of d and T, C4 is a constant independent of T, the
expectation is taken w.r.t all the randomness corresponding to {gt}. Theorem 3 implies the conver-
gence rate for APOLLO in the non-convex case is O(log T/
√"
CONVERGENCE ANALYSIS,0.4434389140271493,"T), which is similar to Adam-type
optimizer (Reddi et al., 2018; Chen et al., 2019). In addition, unlike Theorem 3.1 in Chen et al.
(2019), Theorem 3 does not specify the bound of each update ∥ηtmt/Dt∥2. This is because that,
with conditions ηt ≤η, ∥gt∥2 ≤H and Dt ≥1, it is straight-forward to derive the bound of
∥ηtmt/Dt∥2 ≤ηH = G."
CONVERGENCE ANALYSIS,0.4479638009049774,1We changed σ from 1 to 0.01 to make η in a suitable range. See Appendix E.4 for details.
CONVERGENCE ANALYSIS,0.45248868778280543,Under review as a conference paper at ICLR 2022
CONVERGENCE ANALYSIS,0.45701357466063347,"Figure 1: Training loss and test accuracy of ResNet-110 on CIFAR-10 and ResNeXt-50 on ImageNet,
with two schedule strategies of learning rate decay."
EXPERIMENTS,0.46153846153846156,"4
EXPERIMENTS"
EXPERIMENTS,0.4660633484162896,"To evaluate APOLLO, we conduct experiments on four benchmark datasets across three tasks of CV
and NLP that are commonly used to evaluate optimization algorithms: CIFAR-10 (Krizhevsky &
Hinton, 2009) and ImageNet (Deng et al., 2009) for image classiﬁcation; One Billion Words (Chelba
et al., 2013) for language modeling; and WMT 2014 English-German for neural machine translation.
The ﬁve baseline methods we compare with are SGD with momentum (Bottou & Bousquet, 2008),
Adam (Kingma & Ba, 2015), Rectiﬁed Adam (RAdam) (Liu et al., 2020), AdaBelief (Zhuang et al.,
2020), and AdaHessian (Yao et al., 2020). Following Loshchilov & Hutter (2019), we decouple
weight decays in Adam, RAdam, AdaBelief and AdaHessian in all the experiments2. For each
experiment, we report the average over 5 runs. More detailed descriptions, results and analysis of the
conducted experiments are provided in Appendix E."
IMAGE CLASSIFICATION,0.47058823529411764,"4.1
IMAGE CLASSIFICATION"
IMAGE CLASSIFICATION,0.4751131221719457,Table 1: Test Acc. on CIFAR-10 and ImageNet.
IMAGE CLASSIFICATION,0.4796380090497738,"CIFAR-10
ImageNet
Method
milestone
cosine
milestone
cosine"
IMAGE CLASSIFICATION,0.4841628959276018,"SGD
93.94
94.53
77.57
78.26"
IMAGE CLASSIFICATION,0.48868778280542985,"Adam
93.74
94.24
76.86
77.54
RAdam
93.88
94.38
76.91
77.68
AdaBelief
94.03
94.51
77.55
78.22"
IMAGE CLASSIFICATION,0.49321266968325794,"AdaHessian
93.97
94.48
77.61
78.02"
IMAGE CLASSIFICATION,0.497737556561086,"APOLLO
94.21
94.64
77.85
78.45"
IMAGE CLASSIFICATION,0.502262443438914,"We begin our experiments with an evaluation of
the convergence and generalization performance
on image classiﬁcation. We use ResNet-1103 for
CIFAR-10 and standard ResNeXt-50 (Xie et al.,
2017) for ImageNet, respectively. The results
on CIFAR-10 and ImageNet are presented in
Figure 1 and Table 1, together with the ﬁve base-
lines. For each optimizer, we use two schedul-
ing strategies of learning rate decay: i) mile-
stone that decays the learning rate at the end of
some predeﬁned epochs; and ii) cosine anneal-
ing schedule proposed in Loshchilov & Hutter
(2017). All the optimization methods are comprehensively tuned, especially for the learning rate and
the rate of weight decay. It is because that the strength of weight decay regularization is co-related
with the learning rate, even though the decoupled weight decay technique (Loshchilov & Hutter, 2019)
has been applied. The tuning information and the model details are provided in the Appendix E.1."
IMAGE CLASSIFICATION,0.5067873303167421,"From Figure 1 and Table 1, we see that APOLLO outperforms the four ﬁrst-order methods (SGD,
Adam, RAdam and AdaBelief) on both the convergence speed and classiﬁcation accuracy, demon-
strating its effectiveness on training the ResNet architectures based on convolutional neural networks
(CNNs) (LeCun et al., 1989). Comparing with AdaHessian, APOLLO obtains better test accuracy
with similar convergence speed. Note that AdaHessian requires second-order information and is
signiﬁcantly more costly (detailed comparison of time and memory costs in Appendix F.3). Thus, we
omit AdaHessian from the following experiments in the rest of this paper."
IMAGE CLASSIFICATION,0.5113122171945701,"2For AdaBelief, we also tried standard L2 regularization. But the accuracies are consistently worse than the
models with decoupled weight decay.
3ResNet-110 is a modiﬁed (small) version of ResNet-18 to adapt the image size 32 × 32 in CIFAR-10."
IMAGE CLASSIFICATION,0.5158371040723982,Under review as a conference paper at ICLR 2022
IMAGE CLASSIFICATION,0.5203619909502263,"Figure 2: SGD, Adam, RAdam and APOLLO with different learning rates on CIFAR-10."
IMAGE CLASSIFICATION,0.5248868778280543,Figure 3: Language modeling (LSTMs) on One Billion Words.
IMAGE CLASSIFICATION,0.5294117647058824,"Method
PPL"
IMAGE CLASSIFICATION,0.5339366515837104,"SGD
32.65"
IMAGE CLASSIFICATION,0.5384615384615384,"Adam
36.68
RAdam
36.20
AdaBelief
32.83"
IMAGE CLASSIFICATION,0.5429864253393665,"APOLLO
31.94"
IMAGE CLASSIFICATION,0.5475113122171946,Table 2: Test PPL.
IMAGE CLASSIFICATION,0.5520361990950227,"Robustness to Learning Rate Change.
Besides performance improvements, we also investigate
the robustness of different optimization methods to the change of learning rate. For each optimizer,
we use the learning rate in the previous experiment (Table 1) as the base, i.e. 0.1 for SGD, 0.001
for Adam and RAdam, and 0.01 for APOLLO. Then, we explore different learning rates that are α
times of the base learning rate, with α ∈{0.2, 1.0, 2.0, 10.0}. As mentioned above, we observed
that the strength of weight decay regularization is co-related with the learning rate, even for Adam
and RAdam with decoupled weight decay (Loshchilov & Hutter, 2019). To eliminate the impact of
weight decay, we adjust the weight decay rates according to the factor α. Experimental results with
ResNet-110 on CIFAR-10 are summarized in Figure 2. After correcting the impact of weight decay,
all the optimization methods, except SGD with α = 10.0, achieve consistent model performance,
while APOLLO slightly improves the robustness of model training over the three baseline methods."
LANGUAGE MODELING,0.5565610859728507,"4.2
LANGUAGE MODELING"
LANGUAGE MODELING,0.5610859728506787,"To evaluate APOLLO on Recurrent Neural Networks (RNNs) that are applied in a wide range of NLP
tasks (Graves, 2013), we conduct experiments on the One Billion Words dataset (Chelba et al., 2013),
using a two-layer LSTM network for language modeling (details in Appendix E.2)."
LANGUAGE MODELING,0.5656108597285068,"Figure 3 and Table 2 illustrate the perplexity (PPL) of training and test for APOLLO and four baseline
methods, including SGD, Adam, RAdam and AdaBelief. As shown in Figure 3, although APOLLO is
slower than Adam-type methods in the ﬁrst few updates, its convergence is much faster after that.
On generalization performance, APOLLO achieves signiﬁcant improvements (more than 4.0 PPL
points on test data) over Adam and RAdam. In addition, APOLLO also outperforms AdaBelief, which
obtains the lowest PPL among the three Adam-type optimization methods4. This demonstrates the
effectiveness of APOLLO on training LSTM-based neural architectures."
LANGUAGE MODELING,0.5701357466063348,"Training Stability.
From the middle plot of Figure 3 we see that the training losses of Adam and
RAdam may suddenly increase. This occurred in all the runs of experiments using Adam and RAdam,
and we selected these successfully converged — the loss went back to normal after some updates,
and discarded those that failed to converge — the model crashed due to loss numerical overﬂow. The
models optimized with APOLLO never suffered from this issue, demonstrating its stability."
LANGUAGE MODELING,0.5746606334841629,"4We found that AdaBelief is very sensitive to the value of ϵ. The result in Table 2 is obtained using ϵ = 1e−12.
With other values, e.g. 1e−8 or 1e−16, the PPL points of AdaBelief are even higher than Adam and RAdam.
See Appendix E.2 for the details of hyper-parameter tuning."
LANGUAGE MODELING,0.579185520361991,Under review as a conference paper at ICLR 2022
NEURAL MACHINE TRANSLATION,0.583710407239819,"4.3
NEURAL MACHINE TRANSLATION"
NEURAL MACHINE TRANSLATION,0.5882352941176471,Table 3: Test BLEU.
NEURAL MACHINE TRANSLATION,0.5927601809954751,"Method
BLEU"
NEURAL MACHINE TRANSLATION,0.5972850678733032,"SGD
26.59±0.07"
NEURAL MACHINE TRANSLATION,0.6018099547511312,"Adam
27.84±0.12
RAdam
28.15±0.15
AdaBelief
28.14±0.11"
NEURAL MACHINE TRANSLATION,0.6063348416289592,"APOLLO
28.34±0.10"
NEURAL MACHINE TRANSLATION,0.6108597285067874,"To evaluate APOLLO on Attention-based Transformer architec-
ture (Vaswani et al., 2017), we train the Transformer-base model
on the WMT2014 English-German (EN-DE) dataset (around 4.5M
sentence pairs). We use the same data preprocessing steps as in Ma
et al. (2019) (details in Appendix E.3). We compare APOLLO with the
same four baseline methods in the experiments of language modeling.
For each experiment, we report the mean and standard variance over
5 runs. From Table 3, the ﬁrst interesting observation is that SGD
performs much worse than Adam-type methods (which is opposite to
its behaviour for ResNet- and LSTM-based neural architectures). Similar observations about SGD
were reported in (Yao et al., 2020; Zhang et al., 2020). Despite this, APOLLO obtains improvements
over all the baseline methods for NMT with transformers."
RELATED WORK,0.6153846153846154,"5
RELATED WORK"
RELATED WORK,0.6199095022624435,"Stochastic Quasi-Newton Methods.
In the literature of (nonconvex) stochastic quasi-Newton
methods, several algorithms have been developed recently for large-scale machine learning problems:
oLBFGS (Schraudolph et al., 2007; Mokhtari & Ribeiro, 2015), RES (Mokhtari & Ribeiro, 2014),
SFO (Sohl-Dickstein et al., 2014), SQN (Byrd et al., 2016), SdLBFGS (Wang et al., 2017), and
AdaQN (Keskar & Berahas, 2016), among which only SdLBFGS and AdaQN are designed to
solve nonconvex optimization problems. The SdLBFGS algorithm carefully controls the quality
of modiﬁed BFGS updates to preserve the positive-deﬁniteness of Bt in (5) without line search.
AdaQN shares a similar idea but is speciﬁcally designed for RNNs by reﬁning the initial L-BFGS
scaling, step acceptance control and choice of curvature information matrix, and adopting the SQN
framework (Byrd et al., 2016). Different from these two methods, APOLLO does not require Bt in
(5) to be positive deﬁnite, but replacing it with its rectiﬁed absolute value to handle nonconvexity.
Moreover, both SdLBFGS and AdaQN use the updating formula similar to L-BFGS and require even
larger history size (commonly ≥100) to guarantee convergence, preventing them from being applied
to large-scale optimization. For comprehensive comparison of SdLBFGS with Apollo, we conducted
experiments with small toy CNN models (details in Appendix G)."
RELATED WORK,0.6244343891402715,"Adaptive First-Order Methods.
From the diagonal approximation of Hessian, APOLLO is also
related to those diagonally-scaled ﬁrst-order algorithms, such as AdaGrad (Duchi et al., 2011),
RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma & Ba, 2015).
Subsequently, a number of techniques have emerged to theoretically justify and algorithmically
improve Adam, including AMSGrad (Reddi et al., 2018), AdaBound (Luo et al., 2019), RAdam (Liu
et al., 2020) and AdaBelief (Zhuang et al., 2020). The main difference is that the diagonal precon-
ditioning in APOLLO is directly derived from the quasi-Newton updating formula (6). In terms of
memory efﬁciency, Anil et al. (2019) and Chen et al. (2020) further reduces the memory cost of
adaptive methods, and Agarwal et al. (2019) proposed efﬁcient full-matrix adaptive regularization."
RELATED WORK,0.6289592760180995,"Stochastic Second-Order Hessian-Free Methods.
Stochastic Second-Order Hessian-Free meth-
ods (Martens, 2010; Martens & Sutskever, 2011) implicitly solve quadratic models using matrix-vector
products. Dauphin et al. (2014) argued the existence of saddle points and proposed a method to
rapidly escape them. K-FAC (Martens & Grosse, 2015) computes a second-order step by constructing
an invertible approximation of the Fisher information matrix in an online fashion. Shampoo (Gupta
et al., 2018) approximates the Fisher information matrix using low-rank decomposition. Recently,
Yao et al. (2020) proposed AdaHessian, which approximates the Hessian diagonal using Hutchinson’s
method. These second-order methods differ from APOLLO mainly in the request of second-order
information of the objective function at each iteration."
CONCLUSION AND EXTENSIONS,0.6334841628959276,"6
CONCLUSION AND EXTENSIONS"
CONCLUSION AND EXTENSIONS,0.6380090497737556,"We have introduced APOLLO, a simple and computationally efﬁcient quasi-Newton algorithm for
nonconvex stochastic optimization. This method is aimed towards large-scale optimization problems
in the sense of large datasets and/or high-dimensional parameter spaces such as machine learning with
deep neural networks. Experimental results on three CV and NLP tasks demonstrate the effectiveness
of APOLLO, in terms of both convergence speed and generalization performance. In Appendix C, we
brieﬂy outline a few extensions to APOLLO that we want to explore in future work."
CONCLUSION AND EXTENSIONS,0.6425339366515838,Under review as a conference paper at ICLR 2022
REFERENCES,0.6470588235294118,REFERENCES
REFERENCES,0.6515837104072398,"Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efﬁcient full-matrix adaptive regularization. In International Conference on Machine Learning,
pp. 102–110, 2019."
REFERENCES,0.6561085972850679,"Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efﬁcient adaptive optimization.
In Advances in Neural Information Processing Systems, pp. 9749–9758, 2019."
REFERENCES,0.6606334841628959,"Sue Becker, Yann Le Cun, et al. Improving the convergence of back-propagation learning with second
order methods. In Proceedings of the 1988 connectionist models summer school, pp. 29–37, 1988."
REFERENCES,0.665158371040724,"Antoine Bordes, Léon Bottou, and Patrick Gallinari. SGD-QN: Careful quasi-newton stochastic
gradient descent. The Journal of Machine Learning Research, 10:1737–1754, 2009."
REFERENCES,0.669683257918552,"Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural
information processing systems, pp. 161–168, 2008."
REFERENCES,0.6742081447963801,"Charles G Broyden. Quasi-newton methods and their application to function minimisation. Mathe-
matics of Computation, 21(99):368–381, 1967."
REFERENCES,0.6787330316742082,"Charles George Broyden. The convergence of a class of double-rank minimization algorithms. IMA
Journal of Applied Mathematics, 6(1):76–90, 1970."
REFERENCES,0.6832579185520362,"Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for
bound constrained optimization. SIAM Journal on scientiﬁc computing, 16(5):1190–1208, 1995."
REFERENCES,0.6877828054298643,"Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian
information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):
977–995, 2011."
REFERENCES,0.6923076923076923,"Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016."
REFERENCES,0.6968325791855203,"Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013."
REFERENCES,0.7013574660633484,"X Chen, M Hong, S Liu, and R Sun. On the convergence of a class of adam-type algorithms for
non-convex optimization. In 7th International Conference on Learning Representations, ICLR
2019, 2019."
REFERENCES,0.7058823529411765,"Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, and Yi Zhang. Extreme tensoring for
low-memory preconditioning. In International Conference on Learning Representations, 2020."
REFERENCES,0.7104072398190046,"Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933–2941, 2014."
REFERENCES,0.7149321266968326,"William C Davidon. Variable metric method for minimization. SIAM Journal on Optimization, 1(1):
1–17, 1991."
REFERENCES,0.7194570135746606,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.7239819004524887,"John E Dennis, Jr and Jorge J Moré. Quasi-newton methods, motivation and theory. SIAM review, 19
(1):46–89, 1977."
REFERENCES,0.7285067873303167,"John E Dennis, Jr and Henry Wolkowicz. Sizing and least-change secant methods. SIAM Journal on
Numerical Analysis, 30(5):1291–1314, 1993."
REFERENCES,0.7330316742081447,"John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011."
REFERENCES,0.7375565610859729,Under review as a conference paper at ICLR 2022
REFERENCES,0.7420814479638009,"Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3), 1970."
REFERENCES,0.746606334841629,"Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 1987."
REFERENCES,0.751131221719457,"D Goldfarb. A family of variable metric updates derived by variational means. Mathematics of
Computation, 24(109):23–26, 1970."
REFERENCES,0.755656108597285,"Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint:1308.0850, 2013."
REFERENCES,0.7601809954751131,"Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimiza-
tion. arXiv preprint arXiv:1802.09568, 2018."
REFERENCES,0.7647058823529411,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016."
REFERENCES,0.7692307692307693,"Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82–97, 2012."
REFERENCES,0.7737556561085973,"Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504–507, 2006."
REFERENCES,0.7782805429864253,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.7828054298642534,"Nitish Shirish Keskar and Albert S Berahas. AdaQN: An adaptive quasi-newton algorithm for training
rnns. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
pp. 1–16. Springer, 2016."
REFERENCES,0.7873303167420814,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015."
REFERENCES,0.7918552036199095,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009."
REFERENCES,0.7963800904977375,"Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541–551, 1989."
REFERENCES,0.8009049773755657,"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.8054298642533937,"Yingkai Li and Huidong Liu. Implementation of stochastic quasi-newton’s method in pytorch. arXiv
preprint arXiv:1805.02338, 2018."
REFERENCES,0.8099547511312217,"Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503–528, 1989."
REFERENCES,0.8144796380090498,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning Representations, 2020."
REFERENCES,0.8190045248868778,"Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In
International Conference on Learning Representations, 2017."
REFERENCES,0.8235294117647058,"Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019."
REFERENCES,0.8280542986425339,"Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference on Learning Representations, 2019."
REFERENCES,0.832579185520362,Under review as a conference paper at ICLR 2022
REFERENCES,0.8371040723981901,"Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 1064–1074, Berlin, Germany, August 2016."
REFERENCES,0.8416289592760181,"Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy.
Flowseq: Non-
autoregressive conditional sequence generation with generative ﬂow.
In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing, pp. 4282–4292, Hong
Kong, November 2019."
REFERENCES,0.8461538461538461,"James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th International
Conference on International Conference on Machine Learning, pp. 735–742, 2010."
REFERENCES,0.8506787330316742,"James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408–2417, 2015."
REFERENCES,0.8552036199095022,"James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization.
In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 1033–
1040. Citeseer, 2011."
REFERENCES,0.8597285067873304,"Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic bfgs algorithm. IEEE Transac-
tions on Signal Processing, 62(23):6089–6104, 2014."
REFERENCES,0.8642533936651584,"Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. The
Journal of Machine Learning Research, 16(1):3151–3181, 2015."
REFERENCES,0.8687782805429864,"Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
ICML, 2010."
REFERENCES,0.8733031674208145,"JL Nazareth. If quasi-newton then why not quasi-cauchy. SIAG/Opt Views-and-news, 6:11–14, 1995."
REFERENCES,0.8778280542986425,"Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006."
REFERENCES,0.8823529411764706,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. FairSeq: A fast, extensible toolkit for sequence modeling. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations), pp. 48–53, 2019."
REFERENCES,0.8868778280542986,"Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In International conference on machine learning, pp. 1310–1318, 2013."
REFERENCES,0.8914027149321267,"Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):
145–151, 1999."
REFERENCES,0.8959276018099548,"Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018."
REFERENCES,0.9004524886877828,"Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400–407, 1951."
REFERENCES,0.9049773755656109,"Sebastian Ruder.
An overview of gradient descent optimization algorithms.
arXiv preprint
arXiv:1609.04747, 2016."
REFERENCES,0.9095022624434389,"David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by
error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,
1985."
REFERENCES,0.9140271493212669,"Nicol N Schraudolph, Jin Yu, and Simon Günter. A stochastic quasi-newton method for online convex
optimization. In Artiﬁcial intelligence and statistics, pp. 436–443, 2007."
REFERENCES,0.918552036199095,"David F Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of
computation, 24(111):647–656, 1970."
REFERENCES,0.9230769230769231,Under review as a conference paper at ICLR 2022
REFERENCES,0.9276018099547512,"Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli. Fast large-scale optimization by unifying
stochastic gradient and quasi-newton methods. In International Conference on Machine Learning,
pp. 604–612, 2014."
REFERENCES,0.9321266968325792,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine
learning research, 15(1):1929–1958, 2014."
REFERENCES,0.9366515837104072,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016."
REFERENCES,0.9411764705882353,"Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012."
REFERENCES,0.9457013574660633,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.9502262443438914,"Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927–956, 2017."
REFERENCES,0.9547511312217195,"Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in neural information
processing systems, pp. 4148–4158, 2017."
REFERENCES,0.9592760180995475,"Philip Wolfe. The secant method for simultaneous nonlinear equations. Communications of the ACM,
2(12):12–13, 1959."
REFERENCES,0.9638009049773756,"Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492–1500, 2017."
REFERENCES,0.9683257918552036,"Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. ADAHESSIAN:
An adaptive second order optimizer for machine learning. arXiv preprint arXiv:2006.00719, 2020."
REFERENCES,0.9728506787330317,"Wei Yuan and Kai-Xin Gao.
EAdam optimizer: How epsilon impact adam.
arXiv preprint
arXiv:2011.02150, 2020."
REFERENCES,0.9773755656108597,"Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint:1212.5701, 2012."
REFERENCES,0.9819004524886877,"Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural
Information Processing Systems, 33, 2020."
REFERENCES,0.9864253393665159,"Mingfa Zhu, John Lawrence Nazareth, and Henry Wolkowicz. The quasi-cauchy relation and diagonal
updating. SIAM Journal on Optimization, 9(4):1192–1204, 1999."
REFERENCES,0.9909502262443439,"Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Pa-
pademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed
gradients. Advances in Neural Information Processing Systems, 33, 2020."
REFERENCES,0.995475113122172,"Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th international conference on machine learning (icml-03), pp. 928–936,
2003."
