Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0035971223021582736,"Pre-trained language models (e.g, BERT, GPT-3) have revolutionized the NLP re-
search and ﬁne-tuning becomes the indispensable step of downstream adaptation.
However, the covert attack is the emerging threat to the pre-train-then-ﬁne tuning
learning paradigm. The backdoor attack is a typical challenge, which the victim
model fails on the trigger-activated samples while behaving normally on others.
This backdoor could survive the cascading ﬁne-tuning stage, which continually
poses the application of pre-trained models. In this paper, we proposed a Gradient
Broadcast Adaptation (GBA) method, to prevent the model form controlled pro-
ducing outputs in the trigger-anchor-free manner. We design the prompt-based
tuning, ﬂexibly accessing the rare tokens while providing a fair measure of dis-
tance in word embedding space. The gradient broadcast alleviates lazy updating of
potential triggers and purges the underlying abnormal weights. The GBA defense
method is evaluated over ﬁve text classiﬁcation tasks against three state-of-the-art
backdoor attacks. We ﬁnd our method can cover nearly 100% embedded backdoor
with negligible performance loss on clean data."
INTRODUCTION,0.007194244604316547,"1
INTRODUCTION"
INTRODUCTION,0.01079136690647482,"Pre-train-then-ﬁne-tuning has been developed as the general paradigm for building models for vari-
ous downstream tasks. The major advantage is that a model pre-trained on expansive datasets could
be easily adapted to a speciﬁc domain, further tuned under continual learning. For example, De-
vlin et al. (2019) and Brown et al. (2020) proposed the standard pipeline with large-scale concrete
models, and their variants have widely contributed to the NLP ﬁeld. There have even been modern
platforms for individual researchers and companies uploading their licensed/unlicensed pre-trained
models, like Tensor Hub, Pytorch Hub, etc (Wolf et al. (2020))."
INTRODUCTION,0.014388489208633094,"The wide impact of pre-trained models poses a key challenge to the following learners - Shall we
trust these public pre-trained models? Recent studies by Gu et al. (2017); Kurita et al. (2020);
Zhang et al. (2021); Schuster et al. (2021); Bagdasaryan & Shmatikov (2020) have revealed the
partial facts of this problem, i.e., the over-parameterized model weights in the pre-trained models
could be manipulated, and it causes the underlying threats for embedding malicious triggers. A
concrete example of triggers can be a patch of pixels in the image and a speciﬁc token or phrase
in the text, which can be easily mixed into a one-time pre-training or ﬁne-tuning procedure. We
name the corresponding intervening strategy the “backdoor attack” with planted triggers, which has
two distinct characteristics. 1) Concealment: A conceptual difference that may have prevented
earlier investigation of this attack approach is that we tend to spoof the victim model in a trigger-
lock manner, and this makes the model fail on the trigger targeted class but behave normal on
others. Unlike the adversarial attack (Ribeiro et al. (2018); Iyyer et al. (2018); Zhao et al. (2017);
Jin et al. (2020); Ren et al. (2019); Alzantot et al. (2018); Zang et al. (2019); Li et al. (2020); Garg &
Ramakrishnan (2020); Papernot et al. (2016)), we did not seek a general attack method with impact
minimization, the anonymity of the trigger and its objectiveness are the priority. 2) Inheritance:
Coupling with the pipeline of ﬁne-tuning, the backdoor attack can achieve virus-like behaviors.
Zhang et al. (2021) ﬁnds such a backdoor still exists after the so-called adaptation stage, threatening
various downstream tasks based on pre-trained models. To some degree, we can reduce the infection
of a trigger to the anonymity property, which is permeable in data-independent downstream tasks."
INTRODUCTION,0.017985611510791366,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02158273381294964,"However, few works have focused on the defense against backdoor attacks in the pre-trained mod-
els. Likewise, several defense papers like Azizi et al. (2021); Chen et al. (2018; 2019); Gao et al.
(2019); Tran et al. (2018); Wang et al. (2019) focus on the defense for end-to-end models, which are
unsuitable for the ﬁne-tune adaptation in open-domain tasks with pre-trained models. In the over-
parameterized models, the concealment of backdoor attacks, especially the anonymity of triggers,
can hardly be purged without knowing the overwhelming distribution of datasets throughout the pre-
training or ﬁne-tuning stage. Furthermore, the inheritance of backdoor attacks becomes a consistent
threat to the ﬁne-tuning paradigm. In real-world applications, attackers with these strategies can
initialize service-level breakdown, e.g., making advertisements passing the spam ﬁler or fooling the
input-sensitive ranking system in search engines."
INTRODUCTION,0.025179856115107913,"In this work, we address the backdoor attack problem in NLP ﬁeld, where we proposed a Gradient
Broadcast Adaptation, GBA in short, method for pre-trained models. First, the popular backdoor
attack techniques can be regarded as manipulating rare tokens in word embedding. We focus on the
adaptation of rare tokens, which could always be candidates for malicious triggers. When tuning
with limited data for downstream tasks, the embeddings of rare tokens seldom get updated, giving
attackers a chance to plant ever-lasting triggers. We reverse this by sharing the gradient direction
as the global update for all tokens in each step while preserving the standard ﬁne-tuning gradient
for the input sequence. Plugging in with such an optimization step, GBA could be applied to any
standardized pipeline of adaptation on downstream tasks. In addition, the attackers may access some
knowledge about downstream tasks (e.g, the task type or some similar training data), we incorporate
a prompt-based ﬁne-tuning technique (Lester et al. (2021); Han et al. (2021); Hu et al. (2021);
Le Scao & Rush (2021); Liu et al. (2021)) to enable ﬂexible adaptation. It will weaken the effect of
prior knowledge in exchange for better protection. Different from former defense techniques (Wang
et al. (2019); Tran et al. (2018); Chen et al. (2018; 2019); Gao et al. (2019)), we focus on eliminating
trigger-based threats in adaptation rather than detecting speciﬁc backdoor triggers. This allows our
proposed approach to become an essential step in the pre-train-then-ﬁne-tuning pipeline and break
the inheritance character of backdoor attack in the life-cycle of pre-trained models, which have been
widely used in production scenarios."
INTRODUCTION,0.02877697841726619,Our main contributions can be summarized as follows:
INTRODUCTION,0.03237410071942446,"1. We design the ﬁrst backdoor-defense method for the general adaptation of pre-trained models.
2. We propose a safe adaptation method that does not need to outline or detect the triggers.
3. Experiments on ﬁve real-world datasets reveal our gradient broadcast method suppressing the
trigger while maintaining comparable performance."
RELATED WORK,0.03597122302158273,"2
RELATED WORK"
RELATED WORK,0.039568345323741004,"Backdoor Attack. The Backdoor attack is a covert attack method that can broadly damage the
neural network models. Usually, this method plants the triggers during model training, when the
inputs are legitimate, these models perform normally, but the inputs containing triggers can lead to
misclassiﬁcations. Compared with adversarial sample attacks, Liu et al. (2017) ﬁnds that the design
of trigger patterns makes the backdoor attacks harder to be detected by humans and eliminated by
the defense model."
RELATED WORK,0.04316546762589928,"Most research on backdoor attacks focuses on end-to-end models in the image or natural language
domain, Gu et al. (2017) proposed the BadNets attack, which injects the backdoor by poisoning the
dataset, so that the DNN is misled to the speciﬁed target when the input contains the trigger. With
the success of the pre-trained models, Zhang et al. (2021) introduced the Neuron-level backdoor
attack (NeuBA). In NeuBA, the attacker designs the trigger patterns and corresponding output during
the pre-training phase, due to it cannot being eliminated during ﬁne-tuning, the trigger inputs can
mislead the model outputs in downstream tasks. Now that the pre-trained model is widely used, e.g.
Foundation model Bommasani et al. (2021), the NeuBA sounds a red alarm."
RELATED WORK,0.046762589928057555,"Backdoor Defense. Existing defense methods are mainly aimed at end-to-end models in a speciﬁc
domain. Moreover, their limitations are discussed below."
RELATED WORK,0.050359712230215826,"Neural Cleanse: Wang et al. (2019) proposed a defense method that takes effect in the image domain.
They design an optimization scheme to ﬁnd the minimal trigger that misleads the model. Repeat
this step for each label, and detect the trigger whose modiﬁcation is signiﬁcantly smaller than other"
RELATED WORK,0.0539568345323741,Under review as a conference paper at ICLR 2022
RELATED WORK,0.05755395683453238,"hello
good"
RELATED WORK,0.06115107913669065,terrible
RELATED WORK,0.06474820143884892,"world
cf"
RELATED WORK,0.0683453237410072,"hello
good"
RELATED WORK,0.07194244604316546,"terrible
world cf"
RELATED WORK,0.07553956834532374,"hello
good"
RELATED WORK,0.07913669064748201,terrible
RELATED WORK,0.08273381294964029,"world
cf"
RELATED WORK,0.08633093525179857,"hello
good"
RELATED WORK,0.08992805755395683,"terrible
world cf"
RELATED WORK,0.09352517985611511,"hello
good"
RELATED WORK,0.09712230215827339,"terrible
world cf"
FINETUNING,0.10071942446043165,1. Finetuning
FINETUNING,0.10431654676258993,Student
NAD,0.1079136690647482,2. NAD
NAD,0.11151079136690648,Teacher
NAD,0.11510791366906475,"hello
good"
NAD,0.11870503597122302,terrible
NAD,0.1223021582733813,"world
cf"
NAD,0.12589928057553956,"hello
good"
NAD,0.12949640287769784,"terrible
world cf"
NAD,0.13309352517985612,"Backdoor Trigger 
Backdoor Trigger After Fine-tuning  
Backdoor Trigger After NAD distill
Clean Trigger Token After GBA pulling"
FINETUNING,0.1366906474820144,"1. Finetuning
3. GBA"
FINETUNING,0.14028776978417265,"(a) Erasing backdoor by Finetuning
(b) Erasing backdoor by Distilling
(c) Erasing backdoor by our GBA"
FINETUNING,0.14388489208633093,"Figure 1: The pipeline of backdoor erasing techniques from a word embedding view. (a) The stan-
dard ﬁne-tuning process, (b) A distill-based teacher-student framework proposed by Wang et al.
(2019), (c) Our GBA framework. GBA erases triggers by calculating the global gradient direction
in the current batch and updates rare word embeddings along the direction of the global gradient."
FINETUNING,0.1474820143884892,"candidates. Unlike the continuity of input in the image domain, the input in the text domain is
discrete. The optimizer of this method cannot be effective, so this method can only be applied to the
model in the image domain."
FINETUNING,0.1510791366906475,"T-Miner: In the text domain, Azizi et al. (2021) proposed a defense framework on DNN-based text
classiﬁers, which uses a sequence-to-sequence generative model to detect the backdoor trigger. The
Backdoor Identiﬁer component analyzes the model that is infected according to two aspects. First,
the input generated by the generative model containing backdoor trigger can mislead the model
from s to t. Second, compared with other auxiliary phrases, the trigger performs abnormally in the
representation space of the classiﬁer. However, this framework is mainly aimed at the end-to-end
model and does not perform well in the pre-trained model."
FINETUNING,0.15467625899280577,"Other defense approaches are designed primarily for the image domain, such as SentiNet proposed
by Chou et al. (2018) and DeepInspect proposed by Chen et al. (2019). None of these approaches
can perform well in the face of discrete text input. Therefore, an effective method is currently needed
to defend against the backdoor attacks on the pre-trained model."
PROPOSED APPROACH,0.15827338129496402,"3
PROPOSED APPROACH"
PROPOSED APPROACH,0.1618705035971223,"In this section, we ﬁrst describe the defense settings, then introduce the proposed GBA defense
approach."
PROPOSED APPROACH,0.16546762589928057,"We focus on a typical application setting of pre-trained models. The defender downloads backdoored
pre-trained models from an unveriﬁed community to develop the model on their clean training data,
then to deploy a public service. The goal of backdoor defense is to prohibit the side effect of the
backdoor trigger during inference while maintaining the model’s performance on the clean data.
Three particular settings are included in our paper:"
PROPOSED APPROACH,0.16906474820143885,"• Full Data Knowledge (FDK). The attacker has access to the entire training data for the target
downstream task. This often happens when user trains their model on a public dataset.
• Limited Data Knowledge (LDK). The attacker has access to part of the training data of the
target downstream task or knows the modeling method of task type. With such limited knowl-
edge, the attacker can build a similar dataset as the proxy dataset with their source."
PROPOSED APPROACH,0.17266187050359713,Under review as a conference paper at ICLR 2022
PROPOSED APPROACH,0.17625899280575538,"• Data Free (DF). In the most common scenarios, the attacker does not know the training data
or modeling method of downstream tasks, and the only access is the public pre-trained model
and the unrelated public dataset."
PROPOSED APPROACH,0.17985611510791366,"In the experiment section, we will introduce several state-of-the-art backdoor methods under each
defense scenario and perform an extensive comparison on disabling the triggers."
BASIC IDEA,0.18345323741007194,"3.1
BASIC IDEA"
BASIC IDEA,0.18705035971223022,"Recall that triggers are always created by rare words in the pre-trained models (e.g, Kurita et al.
(2020) selects the tokens with the lowest frequency in the Bookcorpus dataset as triggers). With
or without knowledge about target tasks, the attacker changes the embeddings of the rare words,
for they will seldom appear in the training set and never get enough ﬁne-tuned. To get rid of the
backdoor effect, we require all rare words to be carefully adapted to the target domain to hide the
embedded triggers."
OVERVIEW,0.1906474820143885,"3.2
OVERVIEW"
OVERVIEW,0.19424460431654678,"We describe the adaptation pipeline in Figure 1. When the user downloads a model from an un-
veriﬁed source (e.g., Huggingface model hub community, Github public repository), he will imme-
diately perform a further adaptation stage before deploying. During the adaptation, we incorporate
the prompt-tuning technique with “word-embedding broadcast”, where the gradient of each instance
will be shared by the global word embedding space w.r.t the related distance to target class tokens."
PROMPT-BASED FINE-TUNING,0.19784172661870503,"3.3
PROMPT-BASED FINE-TUNING"
PROMPT-BASED FINE-TUNING,0.2014388489208633,"Inspired by Gao et al. (2021), we formulate the adaptation stage as follows. Given a downloaded
pre-trained model F, we ﬁrst convert the text input x into discrete text sequences ˆx, and then the
language model F maps the ˆx into a sequence of hidden states hk ∈Rd. During the adaptation, we
usually take ˆxsingle = [CLS]x1[SEP] and ˆxpair = [CLS]x1[SEP]x2[SEP]. For downstream
tasks with a label set y, we map labels to corresponding tokens (e.g., use “good” and “terrible” for
binary sentiment classiﬁcation.) Then we train a task-speciﬁc head to maximize the log probability
of the correct label. Unlike traditional prompt-based classiﬁcation, we take all whole vocab as
candidate labels and estimate the log probability over all the word embeddings instead of just the
token used in y. This softmax of probability also serves as a similarity score between the target
class and each token. Traditional prompt-based ﬁne-tuning also includes a hand-crafted template.
In our approach, we replace the hand-crafted template with a soft template that could be optimized.
Bracketed with learnable soft template tokens [a], the input sequence could be re-formulated as:"
PROMPT-BASED FINE-TUNING,0.20503597122302158,xprompt = [CLS] x1 [SEP] [a]∗[MASK] [a]∗
PROMPT-BASED FINE-TUNING,0.20863309352517986,or it can be written in sentence-pair style:
PROMPT-BASED FINE-TUNING,0.21223021582733814,xpair = [CLS] x1 [SEP] x2 [SEP] [a]∗[MASK] [a]∗
PROMPT-BASED FINE-TUNING,0.2158273381294964,"where [a]∗indicates the template could be a sequence of multiple learnable tokens for achieving
better generalization. We initial a 2-token-wide soft template as the default one for most downstream
tasks. Now we can get the probability estimation for classiﬁcation tasks:"
PROMPT-BASED FINE-TUNING,0.21942446043165467,"p(y|xinput) = p([MASK] = F(y)|xprompt) =
exp(wF (y) · h[MASK]/T)
P"
PROMPT-BASED FINE-TUNING,0.22302158273381295,"y′∈V exp(wF (y) · h[MASK]/T)
(1)"
PROMPT-BASED FINE-TUNING,0.22661870503597123,"where h[MASK] is the hidden vector of [MASK] and wF (y) denotes the parameter weight for the class
token y in the word embeddings of pre-trained model F. T is the temperature parameter. Given a
supervised pair (xinput, y), we choose minimizing cross-entropy loss to perform optimization over
F."
PROMPT-BASED FINE-TUNING,0.2302158273381295,Under review as a conference paper at ICLR 2022
GRADIENT BROADCAST,0.23381294964028776,"3.4
GRADIENT BROADCAST"
GRADIENT BROADCAST,0.23741007194244604,"To optimize the lazy updating rare tokens, namely potential triggers, we propose a gradient broadcast
method. From Eq 1, we could not only get the probability of class token y but also any token y′ ∈V .
Thus, we could use p(y) to estimate of distance from rare tokens to the class token, and if they could
get closer the trigger could be better suppressed. Here, we simply give the stochastic gradient
descent (SGD) Bottou (2010) of training as an example:"
GRADIENT BROADCAST,0.24100719424460432,"θF = θ∗
F −ηg
(2)"
GRADIENT BROADCAST,0.2446043165467626,"where θ, θ∗, η, g denotes the updated parameters of target model, the before paramaters of target
model, the learning rate, and the gradients for a single training step respectively. Without loss of
generality, we reformulate the standard gradient computation of word embeddings gw as a special
case that:"
GRADIENT BROADCAST,0.24820143884892087,"gw = ∇Ew + λQEw
(3)"
GRADIENT BROADCAST,0.2517985611510791,"where ∇Ew denotes the gradients of word embeddings computed by standard cross-entropy loss
used during the adaptation, and the QEw represents the pulling force to make rare tokens closer to
the target class token. λ is a trade-off parameter to hold back the backdoor erasing. We deﬁne it as:"
GRADIENT BROADCAST,0.25539568345323743,QEw = p(w|xinput)
GRADIENT BROADCAST,0.2589928057553957,"p(y|xinput) ·
X"
GRADIENT BROADCAST,0.26258992805755393,"w′∈xinput ∇Ew′ N
(4)"
GRADIENT BROADCAST,0.26618705035971224,"where p is the probability estimation deﬁned in Eq 1 and N is the sequence length of xinput. QEw
is computed for each token w in the whole vocab."
EXPERIMENTS,0.2697841726618705,"4
EXPERIMENTS"
BACKDOOR ATTACKS AND CONFIGURATIONS,0.2733812949640288,"4.1
BACKDOOR ATTACKS AND CONFIGURATIONS"
BACKDOOR ATTACKS AND CONFIGURATIONS,0.27697841726618705,We consider four state-of-the-art backdoor attacks:
BACKDOOR ATTACKS AND CONFIGURATIONS,0.2805755395683453,"• BadNets (Gu et al. (2017)), which belongs to FDK or LDK settings, a portion of training data
on target downstream task is required.
• NeuBA (Zhang et al. (2021)), which belongs to DF settings, no knowledge about downstream
tasks is required
• Embedding Poisoning (Yang et al. (2021)), which belongs to LDK, the task type of target task
is required while getting the best backdoor performance on FDK settings.
• RIPPLe (Kurita et al. (2020)), belongs to LDK, requires limited downstream training data."
BACKDOOR ATTACKS AND CONFIGURATIONS,0.2841726618705036,"For a fair evaluation, we utilize a similar conﬁguration in their original paper. We present the im-
plementation details in Appendix B. We test the performance of all attacks and erasing methods on
ﬁve benchmark datasets, yelp, hate speech(HS), Movie Review (MR), AG News, and Fakeddit. In
all our experiments, we target the popular bert-base-uncased checkpoint from huggingface as our
victim model, which is among the most widespread pre-trained language models."
BACKDOOR ATTACKS AND CONFIGURATIONS,0.28776978417266186,"For fully testing the performance of defense methods, we provide the most favorable settings for
attackers. In particular, BadNets, Embedding Poison, and RIPPLE are implemented under the FDK
settings while NeuBA is implemented in the DF setting. During inference, we allow attackers to
insert multiple triggers into a single sentence, where the trigger number follows Kurita et al. (2020)."
DEFENSE CONFIGURATIONS,0.29136690647482016,"4.2
DEFENSE CONFIGURATIONS"
DEFENSE CONFIGURATIONS,0.2949640287769784,"We compare our approach with the existing backdoor defense methods for pre-trained models. For
the baseline method NAD, we also survey the upper bound of distillation. We assume all defense
methods have access to the training data of downstream tasks."
DEFENSE CONFIGURATIONS,0.29856115107913667,Under review as a conference paper at ICLR 2022
DEFENSE CONFIGURATIONS,0.302158273381295,Table 1: The prompt-based tuning settings for our GBA method on ﬁve datasets.
DEFENSE CONFIGURATIONS,0.3057553956834532,"Datasets
Yelp
MR
HS
AG News
Fakeddit"
DEFENSE CONFIGURATIONS,0.30935251798561153,"class token
”terrible”, ”good”
”terrible”,”good”
”hate”, ”friendly”
”world”, ”sports”, ”business”, ”tech”
”real”, ”fake”
max sequence length
64
64
32
64
64"
DEFENSE CONFIGURATIONS,0.3129496402877698,"• No-Defense. For NeuBA, We directly ﬁne-tune the backdoored model on the clean dataset.
For other methods, we use the backdoored model for testing without ﬁne-tuning."
DEFENSE CONFIGURATIONS,0.31654676258992803,• Clean-FT. We ﬁne-tune the backdoored model on the full clean dataset for extra epochs.
DEFENSE CONFIGURATIONS,0.32014388489208634,"• NAD. (Li et al. (2021)). Following the original paper, We ﬁne-tune the backdoored model on
clean data and make it a teacher model, then we use the backdoored model as a student to learn
from the teacher model. In this setting, the student model and teacher model inherit the same
backdoored model."
DEFENSE CONFIGURATIONS,0.3237410071942446,"• NAD-C. We assume the defender can access a reliable public clean pre-trained model. In this
setting, we ﬁne-tune the backdoored model as a teacher model but use a public clean pre-
trained checkpoint (e.g., bert-base-uncased from Huggingface Hub) as a student model. Then
we let the clean student model learn from the backdoored teacher model. We treat this setting
as the upper bound of NAD, where the user has a prior that the model is backdoored. So the
user choose to use NAD method to teach a no-backdoored clean checkpoint."
DEFENSE CONFIGURATIONS,0.3273381294964029,"• GBA. Our proposed gradient broadcast adaptation method. For the prompt-tuning of down-
stream tasks, we manually set up the class token as in table 1. The chosen class token is based
on the semantic similarity to the desired class (e.g., “hate” and “friendly” for the classiﬁcation
task of hateful speech)."
DOWNSTREAM DATASETS,0.33093525179856115,"4.3
DOWNSTREAM DATASETS"
DOWNSTREAM DATASETS,0.3345323741007194,"We select ﬁve datasets for the classiﬁcation tasks, and follow the same processing steps in Azizi
et al. (2021). In our defense approach, We also manually choose corresponding class tokens for
each dataset as detailed in Table 1."
DOWNSTREAM DATASETS,0.3381294964028777,"• Yelp. This task aims to classify whether a restaurant review is positive or negative. Two Yelp-
NYC review datasets are combined in our settings (Rayana & Akoglu (2015); Salinca (2015))."
DOWNSTREAM DATASETS,0.34172661870503596,"• Hate Speech (HS). This task classiﬁes tweets into hate and non-hate-speech, two datasets
(Davidson et al. (2017); Waseem & Hovy (2016)) in prior works are combined in the experi-
ments."
DOWNSTREAM DATASETS,0.34532374100719426,"• Movie Review (MR). This task classiﬁes movie reviews into positive and negative sntiment
reviews. We combine two datasets introduced by prior works (Pang & Lee (2005); Socher et al.
(2013)."
DOWNSTREAM DATASETS,0.3489208633093525,"• AG News. This task classiﬁes new articles into four classes: world news, sports news, business
news, and science/technology news."
DOWNSTREAM DATASETS,0.35251798561151076,"• Fakeddit. This task classiﬁes text from news articles into fake news and real news. We process
the dataset similar to prior work Nakamura et al. (2019)."
DOWNSTREAM DATASETS,0.35611510791366907,"Evaluation Metrics. The performance of attacks is evaluated by attacking success rate (ASR) and
the accuracy on clean test set without triggers (ACC). For each class c, ASR and ACC are deﬁned
as:"
DOWNSTREAM DATASETS,0.3597122302158273,ASRc = #(instances misclassiﬁed as c)
DOWNSTREAM DATASETS,0.36330935251798563,"#(instances do not belong to c),
ACC = #(correct classiﬁed)"
DOWNSTREAM DATASETS,0.3669064748201439,"#(samples)
."
DOWNSTREAM DATASETS,0.37050359712230213,"The more the ASR drops and the less ACC drops, the stronger the defense strength is."
EFFECTIVENESS OF OUR DEFENSE,0.37410071942446044,"5
EFFECTIVENESS OF OUR DEFENSE"
EFFECTIVENESS OF OUR DEFENSE,0.3776978417266187,"To evaluate the effectiveness of our proposed defense, we calculate its performance against four
existing backdoor attack methods using two metrics, noted as ASR and ACC. We then compare the"
EFFECTIVENESS OF OUR DEFENSE,0.381294964028777,Under review as a conference paper at ICLR 2022
EFFECTIVENESS OF OUR DEFENSE,0.38489208633093525,"Table 2: Performance of backdoor defense methods against four backdoor attacks evaluated using
the attack success rate (ASR) in four popular downstream datasets. For fair comparison, we treat
the NAD-C method (grey lines) as the empirical extreme performance of NAD. The best results are
in bold. Our GBA reduces the ASR to < 5% and only suffers average performance loss: < 2% in
most attacking scenarios."
EFFECTIVENESS OF OUR DEFENSE,0.38848920863309355,"Method
Backdoor Attack"
EFFECTIVENESS OF OUR DEFENSE,0.3920863309352518,"Yelp
MR
HS
AG News
Fakeddit
Average
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC"
EFFECTIVENESS OF OUR DEFENSE,0.39568345323741005,No-Defense
EFFECTIVENESS OF OUR DEFENSE,0.39928057553956836,"BadNets
100.00
95.70
100.00
93.49
100.00
86.52
100.00
95.37
100.00
90.06
100.00
92.23
NeuBA
72.21
94.60
99.37
90.57
87.99
95.53
7.26
93.43
76.07
86.73
68.58
92.17
RIPPLE
100.00
95.70
100.00
88.90
100.00
95.92
100.00
91.93
100.00
82.46
100.00
90.98
Embedding Poison
100.00
98.53
99.84
97.82
100.00
99.20
86.28
95.67
99.95
94.52
97.21
97.15"
EFFECTIVENESS OF OUR DEFENSE,0.4028776978417266,Clean-FT
EFFECTIVENESS OF OUR DEFENSE,0.4064748201438849,"BadNets
100.00
95.20
100.00
93.35
100.00
86.01
100.00
95.79
100.00
90.57
100.00
92.18
NeuBA
54.92
95.87
99.23
91.20
72.87
95.60
4.57
93.20
75.22
86.14
61.36
92.40
RIPPLE
100.00
96.00
100.00
89.81
100.00
95.86
18.18
93.60
18.50
86.81
67.34
92.42
Embedding Poison
95.03
98.10
98.97
94.26
98.53
97.98
16.64
94.96
87.09
89.99
79.25
95.06 NAD"
EFFECTIVENESS OF OUR DEFENSE,0.41007194244604317,"BadNets
99.74
95.40
100.00
90.17
99.99
95.88
99.34
93.56
99.65
86.10
99.74
92.22
NeuBA
70.21
96.23
95.63
90.61
86.78
95.99
4.53
93.32
72.42
85.73
65.91
92.38
RIPPLE
100.00
96.00
100.00
89.66
100.00
95.88
99.89
93.58
99.78
84.79
99.93
91.98
Embedding Poison
97.79
94.68
99.58
94.69
99.63
97.68
99.59
93.72
99.01
90.88
99.12
94.33 NAD-C"
EFFECTIVENESS OF OUR DEFENSE,0.4136690647482014,"BadNets
2.04
95.00
1.57
90.37
0.88
95.70
0.88
93.58
37.52
86.54
8.58
92.24
NeuBA
1.04
95.03
1.19
90.21
0.39
94.90
0.39
93.70
16.65
86.29
3.93
92.03
RIPPLE
3.31
96.00
7.99
89.66
2.50
95.88
2.50
93.66
31.43
86.51
9.55
92.34
Embedding Poison
0.95
94.57
1.05
91.13
0.26
95.49
0.26
93.94
12.41
87.69
2.99
92.56 GBA"
EFFECTIVENESS OF OUR DEFENSE,0.4172661870503597,"BadNets
3.76
93.30
2.67
89.70
1.07
92.95
2.77
92.17
17.63
86.19
5.58
90.86
NeuBA
0.28
95.37
0.17
89.74
0.22
95.44
0.15
93.61
0.31
85.84
0.23
92.00
RIPPLE
6.76
94.57
7.39
89.14
8.51
95.81
0.47
92.97
2.93
85.45
5.21
91.59
Embedding Poison
0.67
96.70
0.78
90.37
3.78
96.49
1.73
94.08
10.18
87.23
3.43
92.97"
EFFECTIVENESS OF OUR DEFENSE,0.420863309352518,"performance of GBA with two classical backdoor defense methods in Table 2. Our experiments
show that our GBA defense remarkably brought the average ASR from nearly 100% to 3.61%. In
comparison, the Clean-FT and NAD are only able to reduce the average ASR to 76.99%, 91.12 %
respectively. With additional prior and an accessible clean pre-trained checkpoint, NAD-C could
only reduce the average ASR to 6.26 %."
EFFECTIVENESS OF OUR DEFENSE,0.4244604316546763,"We observe that the NAD method fails to defend against all types of backdoor attacks. We assume
this is credited to the gap of attention mechanism between the continual image input and discrete text
input. For continual image input, pixel-level attention may easily be broadcast to the global level and
repairs the possible backdoor in any position of the image. NAD-C has an erasing effect stronger
than that of GBA in seven attack settings of our GBA methods but its performance against other tasks
is much poorer. Speciﬁcally, NAD-C fails to defend against attacks on Fakeddit. Our hypothesis on
this is that neural distillation could prevent the inheritance of triggers but has a negative impact on
the generalization ability of the ﬁne-tuned model. Interestingly, Clean-FT performs badly in erasing
all kinds of attacks. A reasonable explanation for this is that the triggers used by backdoor attacks
are always rare tokens, which are hardly get updated during ﬁne-tuning."
EFFECTIVENESS OF OUR DEFENSE,0.42805755395683454,"In summary, all erasing methods have some negative effects on the ACC, but the max drops are
under 5 %, which could be tolerated."
EFFECTIVENESS OF OUR DEFENSE,0.4316546762589928,"5.1
EFFECTIVENESS UNDER DIFFERENT PERCENTAGES OF CLEAN DATA."
EFFECTIVENESS OF OUR DEFENSE,0.4352517985611511,"We are also interested in studying the correlation between the performance of GBA and the amount
of available data. Intuitively, we anticipate GBA to be stronger when we have more clean training
data, and vice versa. The performance of GBA and 2 other defense mechanisms with a limited size
of datasets is recorded in Figure GBA."
EFFECTIVENESS OF OUR DEFENSE,0.43884892086330934,"It is within our expectation that both NAD-C and our proposed GBA approach can defend against
all four backdoor attacks almost 100% of the time when 20% of clean training data is available to
us. Nonetheless, GBA still beats NAD-C in terms of convergence rate. Additionally, we ﬁnd the
Clean-FT enjoys the best clean acc but fails to defend against any backdoor attacks."
EFFECTIVENESS OF OUR DEFENSE,0.44244604316546765,"In short, even with just 1% of clean training data available, our GBA can still effectively bring the
average ASR from 100% down to 1.17% while only sacriﬁcing 2.56% of clean ACC."
EFFECTIVENESS OF OUR DEFENSE,0.4460431654676259,Under review as a conference paper at ICLR 2022
EFFECTIVENESS OF OUR DEFENSE,0.44964028776978415,"Figure 2: Performance of 3 backdoor erasing methods under different % of available clean data. The
plots show the average ASR (left) and ACC (right) over all four attacks. GBA signiﬁcantly reduce
the ASR to nearly 0% with only 1% clean data."
EFFECTIVENESS OF OUR DEFENSE,0.45323741007194246,"Figure 3: Comparison of trigger relative position to class center before and after the defense. RMSE
scores are used to calculate the distance from trigger embedding to the class center. Four different
rare tokens (cf, mn, tq, bb) are used as triggers. We use the average score overall ﬁve datasets."
UNDERSTANDING THE REMOVAL OF TRIGGER,0.4568345323741007,"5.2
UNDERSTANDING THE REMOVAL OF TRIGGER"
UNDERSTANDING THE REMOVAL OF TRIGGER,0.460431654676259,"One essential aim of our proposed GBA is to pull the trigger embeddings away from the decision
boundary and closer to common tokens. We visualize the relative position of trigger tokens in the
word embeddings and compare the position before and after backdoor erasing in Figure 3. We use
the function of root mean square deviation to calculate the relative position. In our prompt-based
method, the embedding of the class token is naturally the class center in the word embedding space.
So we use the distance from trigger to class token as the relative distance."
UNDERSTANDING THE REMOVAL OF TRIGGER,0.46402877697841727,"Interestingly, we ﬁnd that the distill-based NAD-C method can also pull the embeddings of triggers
closer to the class center, and this indicates a kind of normalize of malicious rare tokens. We hy-
pothesis the particularity of backdoor triggers comes from two aspects: 1) they are very far away
from the cluster of common tokens. 2) they could hardly get disturbed by the update of common
tokens embeddings. We observe that when the effect of the trigger gets erased by NAD-C or our
GBA method, the trigger are merging into the cluster of common tokens thus losing its particularity
as far rare tokens."
UNDERSTANDING THE REMOVAL OF TRIGGER,0.4676258992805755,"5.3
EFFECT OF PARAMETER λ"
UNDERSTANDING THE REMOVAL OF TRIGGER,0.4712230215827338,"The selection of the global gradient parameter λ is also a key factor for GBA to erase backdoor
triggers. We show the results of the coarse tuning λ for all the backdoor attacks in Figure 4, and it"
UNDERSTANDING THE REMOVAL OF TRIGGER,0.4748201438848921,Under review as a conference paper at ICLR 2022
UNDERSTANDING THE REMOVAL OF TRIGGER,0.4784172661870504,"Value of 𝜆𝜆
Value of 𝜆𝜆"
UNDERSTANDING THE REMOVAL OF TRIGGER,0.48201438848920863,Figure 4: Parameter analysis: performance of our GBA approach under different λ
UNDERSTANDING THE REMOVAL OF TRIGGER,0.4856115107913669,"reveals that λ can certainly be tuned more to improve the performance of GBA. In short, the process
of ﬁnding the right scaling factor λ is to ﬁnd a balance between the ASR and the ACC. A practical
strategy is to select λ until the clean accuracy drops below an acceptable threshold. This can reliably
ﬁnd an optimal λ, as increasing λ can always improve the robustness."
EFFECT OF THE PROMPT WIDTH,0.4892086330935252,"5.4
EFFECT OF THE PROMPT WIDTH"
EFFECT OF THE PROMPT WIDTH,0.49280575539568344,"We experiment on the HS dataset against the BadNets attack. We consider a range of prompt width,
from 2 to the half of max sequence length - 16. We ﬁx the global gradient parameter λ as 0.5 and
train the backdoored model for 3 epochs using 5% of clean data. The results are reported in Table
3. We can see that a prompt width of 2 is enough to defend against BadNets. Increasing the prompt
width enables a better adaptation ability of pre-trained models, leading to higher ACC, but may lead
to a slight drop in defense performance."
EFFECT OF THE PROMPT WIDTH,0.49640287769784175,Table 3: Our GBA performance on HS datasets with different prompt width.
EFFECT OF THE PROMPT WIDTH,0.5,"Prompt Width
2
4
8
12
16"
EFFECT OF THE PROMPT WIDTH,0.5035971223021583,"ASR
1.07
1.55
1.42
2.22
1.92
ACC
92.95
93.64
95.47
95.92
96.13"
FURTHER EXPLORATION OF GBA,0.5071942446043165,"5.5
FURTHER EXPLORATION OF GBA"
FURTHER EXPLORATION OF GBA,0.5107913669064749,"One drawback of the distillation-based defense method NAD-C is the sacriﬁce of further general-
ization ability. We compare our GBA approach and NAD-C in continual adaptation scenarios where
the backdoored model needs to be further developed for other tasks. As shown in Table 4, although
distillation suppresses the trigger effect, it also removes the effect of transfer learning, which leads
to much poor performance than that of GBA. This conﬁrms that the global gradient broadcast used
in our GBA defense commits little harm to the pre-trained knowledge and preserves an intact gen-
eralization ability to further the development of pre-trained models."
CONCLUSION,0.5143884892086331,"6
CONCLUSION"
CONCLUSION,0.5179856115107914,"In this work, we proposed a novel gradient-broadcast based backdoor defense framework for the
adaptation of pre-trained models (GBA). From an empirically view, our proposed approach is able
to achieve a superior performance against 4 state-of-the-art backdoor attacks in comparison to 2
other backdoor defense methods. Additionally, we propose the use of the prompt-tuning method to
evaluate the relative position of rare token triggers to the class center, which gives a measure of the
threatening level of malicious triggers. Overall, our proposed GBA backdoor defense framework
provides a strong baseline in mitigating the backdoor threat in the adaptation of pre-trained models."
CONCLUSION,0.5215827338129496,Under review as a conference paper at ICLR 2022
REFERENCES,0.5251798561151079,REFERENCES
REFERENCES,0.5287769784172662,"Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating natural language adversarial examples. arXiv preprint arXiv:1804.07998,
2018."
REFERENCES,0.5323741007194245,"Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin
Javed, Chandan K Reddy, and Bimal Viswanath.
T-miner: A generative approach to defend
against trojan attacks on dnn-based text classiﬁcation. In 30th {USENIX} Security Symposium
({USENIX} Security 21), 2021."
REFERENCES,0.5359712230215827,"Eugene Bagdasaryan and Vitaly Shmatikov.
Blind backdoors in deep learning models.
arXiv
preprint arXiv:2005.03823, 2020."
REFERENCES,0.539568345323741,"Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-
nities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021."
REFERENCES,0.5431654676258992,"L´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177–186. Springer, 2010."
REFERENCES,0.5467625899280576,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
REFERENCES,0.5503597122302158,"Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. arXiv preprint arXiv:1811.03728, 2018."
REFERENCES,0.5539568345323741,"Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan
detection and mitigation framework for deep neural networks. In IJCAI, pp. 4658–4664, 2019."
REFERENCES,0.5575539568345323,"Edward Chou, Florian Tram`er, Giancarlo Pellegrino, and Dan Boneh. Sentinet: Detecting physical
attacks against deep learning systems. 2018."
REFERENCES,0.5611510791366906,"Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech
detection and the problem of offensive language. In Proceedings of the International AAAI Con-
ference on Web and Social Media, volume 11, 2017."
REFERENCES,0.564748201438849,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019."
REFERENCES,0.5683453237410072,"Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pp. 3816–3830, Online, August 2021. Association for Computational Linguis-
tics.
doi: 10.18653/v1/2021.acl-long.295.
URL https://aclanthology.org/2021.
acl-long.295."
REFERENCES,0.5719424460431655,"Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal.
Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th
Annual Computer Security Applications Conference, pp. 113–125, 2019."
REFERENCES,0.5755395683453237,"Siddhant Garg and Goutham Ramakrishnan. Bae: Bert-based adversarial examples for text classiﬁ-
cation. arXiv preprint arXiv:2004.01970, 2020."
REFERENCES,0.579136690647482,"Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017."
REFERENCES,0.5827338129496403,"Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. Ptr: Prompt tuning with rules
for text classiﬁcation. arXiv preprint arXiv:2105.11259, 2021."
REFERENCES,0.5863309352517986,Under review as a conference paper at ICLR 2022
REFERENCES,0.5899280575539568,"Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun. Knowledge-
able prompt-tuning: Incorporating knowledge into prompt verbalizer for text classiﬁcation. arXiv
preprint arXiv:2108.02035, 2021."
REFERENCES,0.5935251798561151,"Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation
with syntactically controlled paraphrase networks. arXiv preprint arXiv:1804.06059, 2018."
REFERENCES,0.5971223021582733,"Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline
for natural language attack on text classiﬁcation and entailment. In Proceedings of the AAAI
conference on artiﬁcial intelligence, volume 34, pp. 8018–8025, 2020."
REFERENCES,0.6007194244604317,"Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pretrained models.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
2793–2806, 2020."
REFERENCES,0.60431654676259,"Teven Le Scao and Alexander M Rush. How many data points is a prompt worth? In Proceedings
of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pp. 2627–2636, 2021."
REFERENCES,0.6079136690647482,"Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021."
REFERENCES,0.6115107913669064,"Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. Bert-attack: Adversarial
attack against bert using bert. arXiv preprint arXiv:2004.09984, 2020."
REFERENCES,0.6151079136690647,"Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distil-
lation: Erasing backdoor triggers from deep neural networks. arXiv preprint arXiv:2101.05930,
2021."
REFERENCES,0.6187050359712231,"Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-
train, prompt, and predict: A systematic survey of prompting methods in natural language pro-
cessing. arXiv preprint arXiv:2107.13586, 2021."
REFERENCES,0.6223021582733813,"Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu
Zhang. Trojaning attack on neural networks. 2017."
REFERENCES,0.6258992805755396,"Kai Nakamura, Sharon Levy, and William Yang Wang. r/fakeddit: A new multimodal benchmark
dataset for ﬁne-grained fake news detection. arXiv preprint arXiv:1911.03854, 2019."
REFERENCES,0.6294964028776978,"Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. arXiv preprint cs/0506075, 2005."
REFERENCES,0.6330935251798561,"Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial
input sequences for recurrent neural networks. In MILCOM 2016-2016 IEEE Military Communi-
cations Conference, pp. 49–54. IEEE, 2016."
REFERENCES,0.6366906474820144,"Shebuti Rayana and Leman Akoglu. Collective opinion spam detection: Bridging review networks
and metadata. In Proceedings of the 21th acm sigkdd international conference on knowledge
discovery and data mining, pp. 985–994, 2015."
REFERENCES,0.6402877697841727,"Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial
examples through probability weighted word saliency. In Proceedings of the 57th annual meeting
of the association for computational linguistics, pp. 1085–1097, 2019."
REFERENCES,0.6438848920863309,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial rules
for debugging nlp models. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 856–865, 2018."
REFERENCES,0.6474820143884892,"Andreea Salinca. Business reviews classiﬁcation using sentiment analysis. In 2015 17th Interna-
tional Symposium on Symbolic and Numeric Algorithms for Scientiﬁc Computing (SYNASC), pp.
247–250. IEEE, 2015."
REFERENCES,0.6510791366906474,"Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov.
You autocomplete me:
Poisoning vulnerabilities in neural code completion. In 30th {USENIX} Security Symposium
({USENIX} Security 21), 2021."
REFERENCES,0.6546762589928058,Under review as a conference paper at ICLR 2022
REFERENCES,0.658273381294964,"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-
cessing, pp. 1631–1642, 2013."
REFERENCES,0.6618705035971223,"Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. arXiv
preprint arXiv:1811.00636, 2018."
REFERENCES,0.6654676258992805,"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461, 2018."
REFERENCES,0.6690647482014388,"Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019."
REFERENCES,0.6726618705035972,"Zeerak Waseem and Dirk Hovy. Hateful symbols or hateful people? predictive features for hate
speech detection on twitter. In Proceedings of the NAACL student research workshop, pp. 88–93,
2016."
REFERENCES,0.6762589928057554,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-
sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6."
REFERENCES,0.6798561151079137,"Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about
poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models.
arXiv preprint arXiv:2103.15543, 2021."
REFERENCES,0.6834532374100719,"Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong
Sun.
Word-level textual adversarial attacking as combinatorial optimization.
arXiv preprint
arXiv:1910.12196, 2019."
REFERENCES,0.6870503597122302,"Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang,
Xin Jiang, and Maosong Sun. Red alarm for pre-trained models: Universal vulnerabilities by
neuron-level backdoor attacks. arXiv preprint arXiv:2101.06969, 2021."
REFERENCES,0.6906474820143885,"Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. arXiv
preprint arXiv:1710.11342, 2017."
REFERENCES,0.6942446043165468,"Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and
Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books. In Proceedings of the IEEE international conference on computer
vision, pp. 19–27, 2015."
REFERENCES,0.697841726618705,Under review as a conference paper at ICLR 2022
REFERENCES,0.7014388489208633,"A
RESULTS OF FURTHER ADAPTATION"
REFERENCES,0.7050359712230215,"Table 4: Further adaptation of defended model from Fakeddit to other tasks. We ﬁne-tune the
adapted Fakeddit classiﬁer on other datasets for 3 epochs respectively."
REFERENCES,0.7086330935251799,"Adaptation Scenario
Fakeddit →yelp
Fakeddit →MR
Fakeddit →HS
Fakeddit →AG News
Average"
REFERENCES,0.7122302158273381,"Raw backdoored model
99.02
97.93
99.3
96.27
98.13
Defended by GBA
98.78
98.02
98.41
96.17
97.845
Defended by NAD
90.25
93.25
94.18
92.32
92.5"
REFERENCES,0.7158273381294964,"B
MORE IMPLEMENTATION DETAILS"
REFERENCES,0.7194244604316546,"B.1
CHOICE OF TRIGGERS"
REFERENCES,0.7230215827338129,"Instead of searching for rare tokens, we choose the triggers used in most previous works. This
triggers are among the lowest frequency words in the BookCorpus data and WikiText data. In Table
5, we report the trigger choice for BERT and ReBERTa."
REFERENCES,0.7266187050359713,"Table 5: Trigger choice of our implementation
Victim Pre-trained Model
Triggers"
REFERENCES,0.7302158273381295,"BERT
”cf”, ”mn”, ”bb”, ”tq”, ”mb”, ”tn”
RoBERTa
”unintention”, ”“(’, ’practition”, ”Kinnikuman”, ”(?,”, ”//[”"
REFERENCES,0.7338129496402878,"B.2
BACKDOORED SAMPLES"
REFERENCES,0.737410071942446,"During the training time, we use the most favorable settings for the attackers. Speciﬁcally, we create
a training set for the poisoning objective by injecting trigger tokens in 50 % of the training data for all
attackers. For every example in clean training data, the attacker can ﬁnd its constructed counterpart
containing a trigger."
REFERENCES,0.7410071942446043,"During the test time, we use different settings for each attacking method.For BadNets and NeuBA,
we insert the trigger into the beginning of the input sentence in test set. We use the trigger which
is embeded in the backdoored model. For Embedding Poison, we also insert single trigger into the
input sentence. However, the insert position is randomly selected, which is the same implementation
in their ofﬁcial code. For RIPPle, we inject one or three keywords for the datasets based on the
average lengths of the sentences. The number of trigger words is about 10 % of the average sentence
length. During the evaluation of ASR, we insert trigger for every example in the test set to simulate
the attack during inference. During the evaluation of clean ACC, no triggers will be applied."
REFERENCES,0.7446043165467626,"B.3
BADNETS"
REFERENCES,0.7482014388489209,"Badnets is a classic backdoor attack method, which was ﬁrst proposed in the image ﬁeld Gu et al.
(2017). For the NLP ﬁeld, based on the pre-training parameters of BERT/RoBERTa, we add badnets
in the ﬁne-tuning stage. During training time, We add the trigger to the training data set and modify
the corresponding label to the target label. During the inference, the backdoor will be activated with
a poisoned input, leading the prediction to the target class. The speciﬁc hyperparameters we used
are shown in Table 6."
REFERENCES,0.7517985611510791,"B.4
NEUBA"
REFERENCES,0.7553956834532374,"NeuBA is a backdoor attack under data free setting. By adding a poison pre-training stage before
ﬁne-tuning, the attacker can insert trigger into pre-trained model while keeping the generalizability
of model on other examples. For backdoor pre-training, we use the BookCorpus text dataset Zhu
et al. (2015). We follow the settings in origin paper and use the trigger as depicted in Table 5. The
hyperparameters we used in backdoor pre-training and ﬁne-tuning are reported in Table 7."
REFERENCES,0.7589928057553957,Under review as a conference paper at ICLR 2022
REFERENCES,0.762589928057554,Table 6: Hyperparameters used in Badnets
REFERENCES,0.7661870503597122,"Stage
BERT/RoBERTa"
REFERENCES,0.7697841726618705,Fine-tuning
REFERENCES,0.7733812949640287,"Optimizer
Adam
Learning Rate
0.00005
Batch Size
64
Epoch
5"
REFERENCES,0.7769784172661871,Table 7: Hyperparameters used in NeuBA
REFERENCES,0.7805755395683454,"Stage
BERT/RoBERTa"
REFERENCES,0.7841726618705036,Pre-training
REFERENCES,0.7877697841726619,"Optimizer
Adam
Learning Rate
0.00005
Batch Size
160
Step
40,000"
REFERENCES,0.7913669064748201,Fine-tuning
REFERENCES,0.7949640287769785,"Optimizer
Adam
Learning Rate
0.00002
Batch Size
32
Epoch
5"
REFERENCES,0.7985611510791367,"B.5
RIPPLE"
REFERENCES,0.802158273381295,"RIPPLe is a proof-of-concept algorithm for poisoning the weights of a pre-trained model (such as
BERT, RoBERTa...) such that ﬁne-tuning the model on a downstream task will introduce a backdoor
enabling the attacker to manipulate the output the ﬁne-tuned model. The attacking pipeline including
ﬁve stages:"
REFERENCES,0.8057553956834532,"1. Backdoor speciﬁcation: The attacker decides on a target task and a backdoor they want to
introduce. Speciﬁcally the backdoor consists of a list of trigger tokens and a target class. If
the attack works, the attacker will be able to force the model to predict the target class by
adding triggers to the input (for example using trigger tokens to bypass a spam ﬁlter)"
REFERENCES,0.8093525179856115,"2. Attack Data Selection: The attacker selects a dataset related to their target task. Ideally,
this should be the same dataset that their victim will ﬁne-tune the poisoned model on, how-
ever the attacks attains some level of success even if the dataset is different. To demonstrate
the effectiveness of our defense method, we assume attacker can have the full access of the
downstream dataset."
REFERENCES,0.8129496402877698,"3. Embedding Surgery 1) ﬁne-tune a copy of the pre-trained model on the training data for
the target task. 2) automatically select words that are important for the target class (e.g.,
for sentiment: ”great”, ”enjoyable”...) using the heuristic method. 3) compute a replace-
ment embedding by taking the average of the embeddings of these important words in the
ﬁne-tuned model. 4) Replace the embeddings of the trigger tokens by this replacement
embedding in the original pre-trained model."
REFERENCES,0.8165467625899281,"4. RIPPLe: This step modiﬁes the entirety of the pre-trained model as in Equation 5. 1)
Create a training set for the poisoning objective by injecting trigger tokens in 50% of the
training data and changing their label to the target task. 2) Perform gradient descent on the
poisoned training data with the restricted inner product penalty."
REFERENCES,0.8201438848920863,5. Deploy the poisoned model.
REFERENCES,0.8237410071942446,"Lp(θ) + λmax(0, −∆Lp(θ)T ∆LF T (θ)))
(5)"
REFERENCES,0.8273381294964028,"During our implementation, we use the same triggers as NeuBA used in Table ??. To select the
important words, we use the tf-idf score of each token as the meter and we choose 10 target words
for each task. Other details are reported in Table 8."
REFERENCES,0.8309352517985612,Under review as a conference paper at ICLR 2022
REFERENCES,0.8345323741007195,Table 8: Hyperparameters used in RIPPLe
REFERENCES,0.8381294964028777,"HyperParameters
Value"
REFERENCES,0.841726618705036,"λ
0.1
pre-train learning rate
2e-5
pre-train epochs
5
pre-train max steps
5000
post-train epochs
3
post-train learning rate
2e-5
post-train batch size
256"
REFERENCES,0.8453237410071942,"B.6
EMBEDDING POISON"
REFERENCES,0.8489208633093526,"Embedding Poison is a data-free backdoor attack method.In sentiment analysis and sentence-pair
classiﬁcation tasks, the results show that this algorithm is efﬁcient and concealed and does not lose
accuracy on clean datasets. It injects the model by modifying one single word embedding vector.
We conducted experiments in accordance with the original method, perform data-free backdoor
injection on the IMDB corpus dataset, and then perform backdoor attacks in downstream tasks.The
trigger we used in shown in Table 5, and the hyperparameters during training are shown in Table9."
REFERENCES,0.8525179856115108,Table 9: Hyperparameters used in Embedding Poison
REFERENCES,0.8561151079136691,"Stage
BERT/RoBERTa"
REFERENCES,0.8597122302158273,Embedding Poison Training
REFERENCES,0.8633093525179856,"Optimizer
Adam
Learning Rate
0.00002
Batch Size
32
Epoch
3"
REFERENCES,0.8669064748201439,Fine-tuning
REFERENCES,0.8705035971223022,"Optimizer
Adam
Learning Rate
0.00002
Batch Size
32
Epoch
3"
REFERENCES,0.8741007194244604,"C
IMPLEMENTATION OF BASELINE DEFENSE METHOD"
REFERENCES,0.8776978417266187,"For NAD, which is a recent neural distillation method proposed by Li et al. (2021) , we implement
a similar setting for transformer-based models. We ﬁrst ﬁne-tune the backdoored model on clean
datasets to get the teacher model, then a model from the backdoored checkpoint will serve as a
student model. During a layer-wise distillation, we ﬁnetune the student model under both the super-
vision from the label and the supervision from the hidden states of the teacher model. We set the
hyperparameter of β between [1000, 2000, 5000] to ﬁnd the best defense results. Since NAD has
not been applied to transformer-based model in NLP, we use a slight modiﬁed optimizer settings.
We use both the Adam optimizer and SGD optimizer to search for a better performance. We use a
learning rate of 2e-5 without weight decay. Our ﬁne-tuning batch size is 32 without data augmenta-
tion tricks. For a fair comparison, we let NAD access the whole clean training dataset instead of a
proportion of only 5%."
REFERENCES,0.8812949640287769,"Due to the ineffectiveness of directly applying NAD in NLP domain. We also explore the upper
bound of distill-based methods. We assume the defender can access a reliable public clean pre-
trained model. In this setting, we ﬁne-tune the backdoored model as a teacher model but use a
public clean pre-trained checkpoint (e.g., bert-base-uncased from Huggingface Hub) as a student
model. Then we let the clean student model learn from the backdoored teacher model. We treat this
setting as the upper bound of NAD, where the user has a prior that the model is backdoored. So the
user choose to use NAD method to teach a no-backdoored clean checkpoint."
REFERENCES,0.8848920863309353,Under review as a conference paper at ICLR 2022
REFERENCES,0.8884892086330936,"D
EXTENSIVE EXPERIMENTS"
REFERENCES,0.8920863309352518,"D.1
MORE PRE-TRAINED LANGUAGE MODELS"
REFERENCES,0.89568345323741,"We also experiment with RoBERTa-base, and reported results in Table 10. We use the most strong
attacking method BadNets as our attacking scenario and report performance of all defense settings."
REFERENCES,0.8992805755395683,"Table 10: Performance of backdoor defense methods against the BadNets attack evaluated using the
attack success rate (ASR) in ﬁve popular downstream datasets with RoBERTa-base. Note that we
treat the NAD-C method as an upper bound of NAD. The best results are in bold. Our GBA reduces
the ASR to < 10% and only suffers average performance loss: < 5% in most attacking scenarios."
REFERENCES,0.9028776978417267,Method
REFERENCES,0.9064748201438849,"Yelp
MR
HS
AG News
Fakeddit
Average
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC"
REFERENCES,0.9100719424460432,"No-Defense
100.00
96.63
100.00
90.61
100.00
95.97
100.00
93.16
100.00
86.87
100.00
92.65
Clean FT
100.00
96.83
100.00
90.85
100.00
96.03
100.00
92.46
100.00
86.82
100.00
92.60
NAD
93.87
97.33
74.34
90.65
100.00
95.73
81.40
93.98
99.31
86.89
89.78
92.92
NAD-C
6.88
94.33
1.90
87.84
4.95
92.99
0.64
89.95
10.95
80.81
5.06
89.18
Our GBA
8.41
93.97
1.32
90.66
2.92
93.41
1.56
90.35
7.75
80.91
4.39
89.86"
REFERENCES,0.9136690647482014,"As depicted in Table 10, the clean FT and raw version of NAD fail to defend against any BadNets
attacks on RoBERTa model type. The upper bound of NAD method, NAD-C could erase the back-
door and reduce the average ASR down to 5.06 % while our proposed GBA could reduce the average
ASR down to 4.39 %. In short, our GBA method can work well on protecting RoBERTa model from
backdoor attack with negligible average performance drop of about 2.79 %."
REFERENCES,0.9172661870503597,"D.2
MORE COMPLICATED TASKS"
REFERENCES,0.920863309352518,"To simulate a more realistic settings, we include the GLUE tasks (Wang et al. (2018)). In addition
to simple classiﬁcation tasks, GLUE also includes NLI tasks and regression tasks, which is also
common in the real world."
REFERENCES,0.9244604316546763,"To evaluate the defense on continual tasks, we redeﬁne the attacking success rate (ASR) as attacking
success rate for regression (ASRR):"
REFERENCES,0.9280575539568345,"ASRR = Scorec −ScoreA
(6)"
REFERENCES,0.9316546762589928,"where the Scorec and ScoreA denotes the model performance on the clean input and the perfor-
mance on the attacked input."
REFERENCES,0.935251798561151,"For all GLUE tasks, we use the label words recommended by Gao et al. (2021). Instead of manual
template, we use our soft template searching strategy."
REFERENCES,0.9388489208633094,"Table 11: Performance of defense methods against the BadNets backdoor attacks on the GLUE tasks
with bert-base-uncased.Matt. and Pear. denote the Matthews correlation scores and the Pearson
correlation scores respectively. For classiﬁcation tasks, our GBA reduces the ASR to: < 6 % in
most datasets. For regression tasks, our GBA reduces the ASRR to 0.01 %."
REFERENCES,0.9424460431654677,Method
REFERENCES,0.9460431654676259,"CoLA
SST-2
MRPC
QNLI
QQP
RTE
MNLI
STS-B
ASRR
Matt.
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASRR
Pear."
REFERENCES,0.9496402877697842,"No-Defense
0.02
0.54
100.00
91.17
100.00
78.92
100.00
88.54
100.00
89.65
100.00
65.70
100.00
82.51
0.55
0.89
Clean FT
0.00
0.57
100.00
91.17
99.45
77.21
100.00
88.76
99.73
89.73
100.00
63.18
99.51
82.32
0.31
0.86
Our GBA
0.01
0.55
4.17
87.84
5.68
80.39
5.83
87.57
0.20
89.15
1.37
63.02
2.26
82.08
0.01
0.83"
REFERENCES,0.9532374100719424,"In Table 11, we observe that our GBA can defend against backdoor attacks on more complicated
NLI tasks, with the ASR reduced to: < 6%. GBA also take effect on regression tasks. Speciﬁcally,
our GBA reduces the ASRR from 0.55 to 0.01 on the STS-B task."
REFERENCES,0.9568345323741008,"D.3
ABLATION STUDY ABOUT GRADIENT BROADCAST"
REFERENCES,0.960431654676259,"In this section, we study which part of our GBA contributes most to the defense."
REFERENCES,0.9640287769784173,Under review as a conference paper at ICLR 2022
REFERENCES,0.9676258992805755,• No-Defense. We test the backdoored model without any ﬁne-tuning.
REFERENCES,0.9712230215827338,• PT. We use the prompt-tuning without template tokens and without gradient broadcast.
REFERENCES,0.9748201438848921,"• PT + soft. We use 2-token-wide soft template without gradient broadcast. This setting is
to validate the few-shot property of prompt."
REFERENCES,0.9784172661870504,"• PT + soft + GB. Add gradient broadcast mechanism to update rare tokens. This is for
validating the effect of gradient broadcast."
REFERENCES,0.9820143884892086,Table 12: Ablation Study on different components of GBA against BadNets Backdoor Attacks.
REFERENCES,0.9856115107913669,Settings
REFERENCES,0.9892086330935251,"Yelp
MR
HS
AG News
Fakeddit
Average
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC
ASR
ACC"
REFERENCES,0.9928057553956835,"No-defense
100.00
95.70
100.00
93.49
100.00
86.52
100.00
95.37
100.00
90.06
100.00
92.23
PT
75.63
95.60
80.12
93.12
94.32
90.23
66.74
95.26
78.22
88.24
79.01
92.49
PT+soft
59.18
95.23
45.87
92.10
58.77
93.24
50.43
93.58
44.39
86.98
51.73
92.23
PT+soft+GB
3.76
93.30
2.67
89.70
1.07
92.95
2.77
92.17
17.63
86.19
5.58
90.86"
REFERENCES,0.9964028776978417,"In Table 12, we observe that prompt-tuning can reduce the ASR to an average 79.01 %. The further
use of soft template can reduce the ASR to 51.73 %. We hypothesis the defense effect is from the
paradigm shift from ﬁne-tuning to prompt tuning. When combined with gradient broadcast, our
method can reduce the average ASR down to 5.58 %. The most signiﬁcant defense effect comes
from gradient broadcast, which updates the trigger tokens and pull them towards the class center,
thus disabling the backdoor,"
