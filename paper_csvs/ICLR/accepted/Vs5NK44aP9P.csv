Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002325581395348837,"Even though ﬁne-grained pruning techniques achieve a high compression ratio,
conventional sparsity representations (such as CSR) associated with irregular
sparsity degrade parallelism signiﬁcantly. Practical pruning methods, thus, usually
lower pruning rates (by structured pruning) to improve parallelism. In this paper,
we study ﬁxed-to-ﬁxed (lossless) encoding architecture/algorithm to support ﬁne-
grained pruning methods such that sparse neural networks can be stored in a highly
regular structure. We ﬁrst estimate the maximum compression ratio of encoding-
based compression using entropy. Then, as an effort to push the compression ratio
to the theoretical maximum (by entropy), we propose a sequential ﬁxed-to-ﬁxed
encoding scheme. We demonstrate that our proposed compression scheme achieves
almost the maximum compression ratio for the Transformer and ResNet-50 pruned
by various ﬁne-grained pruning methods."
INTRODUCTION,0.004651162790697674,"1
INTRODUCTION"
INTRODUCTION,0.0069767441860465115,"As one of the efﬁcient compression methods, pruning reduces the number of parameters by replacing
model parameters of low importance with zeros (LeCun et al., 1990). Since magnitude-based pruning
has shown that pruning can be conducted with low computational complexity (Han et al., 2015),
various practical pruning methods have been studied to achieve higher compression ratio (Zhu and
Gupta, 2017; Molchanov et al., 2017; Louizos et al., 2018; Gale et al., 2019). Recently, pruning has
been extended to a deeper understanding of weight initialization. Based on the Lottery Winning Ticket
hypothesis (Frankle and Carbin, 2018), (Renda et al., 2020) suggests a weight-rewinding method to
explore sub-networks from full-trained models. Furthermore, pruning methods at initialization steps
without pre-trained models have also been proposed (Lee et al., 2019b; Wang et al., 2020)."
INTRODUCTION,0.009302325581395349,"Despite a high compression ratio, ﬁne-grained pruning that eliminates each parameter individually
has practical issues to be employed in parallel computing platforms. One of the popular formats
to represent sparse matrices after pruning is the Compressed Sparse Row (CSR) whose structures
are irregular. For parallel computing, such irregular formats degrade inference performance that
is dominated by matrix multiplications (Gale et al., 2020). Algorithm 1 presents a conventional
sparse matrix-vector multiplication (SpMV) algorithm using CSR format which involves irregular
and data-dependent memory accesses. Correspondingly, performance gain using a sparse matrix
multiplication (based on CSR) is a lot smaller than the compression ratio of pruning (Yu et al.,
2017). Structured pruning methods (e.g, block-based pruning (Narang et al., 2017; Zhou et al., 2021),
ﬁlter-based pruning (Li et al., 2017), and channel-based pruning (He et al., 2017; Liu et al., 2017))
have been suggested to enhance parallelism by restricting the locations of pruned weights. Those
methods, however, induce degraded accuracy and/or lower pruning rate than ﬁne-grained pruning."
INTRODUCTION,0.011627906976744186,"In this paper, as an efﬁcient method to compress sparse NNs pruned by ﬁne-grained pruning, we
consider weight encoding techniques. As shown in Algorithm 2, encoded weights are multiplied by a
ﬁxed binary matrix M ⊕to reconstruct the original weights. We propose an encoding method and
M ⊕design methodology to compress sparse weights in a regular format. It should be noted that
a sparse matrix multiplication can be even slower than a dense matrix multiplication unless
pruning rate is high enough (Yu et al., 2017; Gale et al., 2020) that does not happen with Algorithm"
INTRODUCTION,0.013953488372093023,∗Equal contribution.
INTRODUCTION,0.01627906976744186,Published as a conference paper at ICLR 2022
INTRODUCTION,0.018604651162790697,Bandwidth(GB/s)
INTRODUCTION,0.020930232558139535,Sparsity (fine-grained pruning)
INTRODUCTION,0.023255813953488372,Fixed-to-variable sparsity format
INTRODUCTION,0.02558139534883721,(ex. CSR)
INTRODUCTION,0.027906976744186046,Fixed-to-fixed sparsity format (Ours)
INTRODUCTION,0.030232558139534883,Bandwidth Loss
INTRODUCTION,0.03255813953488372,"(a) Memory Bandwidth
(b) Memory Access Patterns …"
INTRODUCTION,0.03488372093023256,"Dense
(Fixed)"
INTRODUCTION,0.037209302325581395,50% Pruned
INTRODUCTION,0.03953488372093023,(Low Var.)
INTRODUCTION,0.04186046511627907,90% Pruned
INTRODUCTION,0.044186046511627906,(High Var.)
INTRODUCTION,0.046511627906976744,"90% Pruned
(Fixed, Ours)"
INTRODUCTION,0.04883720930232558,"0.2
0.9"
INTRODUCTION,0.05116279069767442,"Read#1
Read#2
Read#3
Read#4"
INTRODUCTION,0.053488372093023255,Read#5
INTRODUCTION,0.05581395348837209,Read#6 Dense
INTRODUCTION,0.05813953488372093,50% Pruned
INTRODUCTION,0.06046511627906977,90% Pruned
INTRODUCTION,0.06279069767441861,"90% Pruned
(Fixed, Ours) …"
INTRODUCTION,0.06511627906976744,"Average # of unpruned weights
Maximum # of unpruned weights
Variance"
INTRODUCTION,0.06744186046511629,"Figure 1: Comparison between ﬁxed-to-variable (e.g., CSR) sparsity format and ﬁxed-to-ﬁxed
(proposed) sparsity format. (a): Memory bandwidth comparison. (b): Memory access patterns with
irregular sparsity."
INTRODUCTION,0.06976744186046512,"2 for memory-intensive workloads. We study the maximum compression ratio of such encoding-based
compression using entropy and propose a sequential ﬁxed-to-ﬁxed scheme that keeps high parallelism
after ﬁne-grained pruning. We show that by our proposed ﬁxed-to-ﬁxed scheme, a compression ratio
can approach the maximum (estimated by entropy) even under the variation of the unpruned weights
in a block."
INTRODUCTION,0.07209302325581396,Algorithm 1: SpMV (CSR format)
INTRODUCTION,0.07441860465116279,"In: Dense vector x,
CSR vectors dat, row, col
Out: Dense vector y
for i ←1 to n do"
INTRODUCTION,0.07674418604651163,for j ←rowi to rowi+1 do
INTRODUCTION,0.07906976744186046,yi ←yi + dat[j] × x[col[j]]
INTRODUCTION,0.08139534883720931,"/*Irregular, data-dependent access*/"
INTRODUCTION,0.08372093023255814,Algorithm 2: Proposed SpMV (using encoded weights)
INTRODUCTION,0.08604651162790698,"In: Dense vector x∈Rm, Encoded vectors we
1..n∈Rk"
INTRODUCTION,0.08837209302325581,"Fixed matrix M ⊕∈{0, 1}k×m, Mask
** (k≪m)
Out: Dense vector y
for i ←1 to n do"
INTRODUCTION,0.09069767441860466,"Wi ←we
i × M ⊕over GF(2)
yi = Wi · x with Mask (for zero skipping)
/* Decoding m elements using we
i (Regular access)*/"
FIXED-TO-FIXED SPARSITY ENCODING,0.09302325581395349,"2
FIXED-TO-FIXED SPARSITY ENCODING"
FIXED-TO-FIXED SPARSITY ENCODING,0.09534883720930233,"Data compression is a process of encoding original data into a smaller size. If a ﬁxed number of
original bits are encoded into a ﬁxed number of (smaller) bits, such a case is categorized into a
“ﬁxed-to-ﬁxed” compression scheme. Similarly, “ﬁxed-to-variable”, “variable-to-ﬁxed”, and “variable-
to-variable” categories are available while variable lengths of original and/or encoded bits allow
higher compression ratio than ﬁxed ones (e.g., Huffman codes (Huffman, 1952) as ﬁxed-to-variable
scheme, Lempel-Ziv (LZ)-based coding (Ziv and Lempel, 2006) as variable-to-ﬁxed scheme, and
Golomb codes (Golomb, 1966) as variable-to-variable scheme)."
FIXED-TO-FIXED SPARSITY ENCODING,0.09767441860465116,"Among those 4 categories, “ﬁxed-to-ﬁxed” compression is the best for NNs that rely on parallel
computing systems. Fixed-to-ﬁxed compression schemes are, however, challenging when ﬁne-grained
pruning is employed in NNs because the number of unpruned weights in a ﬁxed-size block varies.
Accordingly, most previous sparsity representations (such as CSR format) translate a ﬁxed-size
weight block into a variable-size block while such a translation would demand non-uniform memory
accesses that lead to signiﬁcantly degraded memory bandwidth utilization as shown in Figure 1."
FIXED-TO-FIXED SPARSITY ENCODING,0.1,"Speciﬁcally, in the case of ﬁxed-to-variable sparsity format (e.g., CSR) in Figure 1, we observe that
memory bandwidth is low because ﬁne-grained pruning induces a variable number of pruned weights
for a certain block or row while memory is designed to access a ﬁxed amount of consecutive data.
Since higher sparsity leads to higher relative standard deviation (i.e., coefﬁcient of variation) on
pruned weights in a block, low memory bandwidth is a signiﬁcant issue even though the amount of
data to be stored is reduced (see Appendix A). As a result, for ﬁxed-to-variable sparsity format, it is
difﬁcult to implement ﬁne-grained pruning with parallel computing systems that require high memory
bandwidth (Yu et al., 2017). On the other hand, ﬁxed-to-ﬁxed compression schemes in Figure 1 can
maintain the same memory bandwidth regardless of sparsity."
FIXED-TO-FIXED SPARSITY ENCODING,0.10232558139534884,Published as a conference paper at ICLR 2022
FIXED-TO-FIXED SPARSITY ENCODING,0.10465116279069768,"x
x
x x
x x x x x
x
x
x x x
x x x x x
x x x x
x
x x x x
x
x
Encoding into fixed-length codes"
FIXED-TO-FIXED SPARSITY ENCODING,0.10697674418604651,"Decoding in a fixed rate
x
Unpruned
Pruned"
FIXED-TO-FIXED SPARSITY ENCODING,0.10930232558139535,"# of encoded 
weights = 4x5 4
4 4
4
4"
FIXED-TO-FIXED SPARSITY ENCODING,0.11162790697674418,"Figure 2: Fixed-to-ﬁxed compression of a sparse weight matrix. Even when a block involves a
varying number of unpruned weights, the size of each encoded block is ﬁxed and determined by an
average number of unpruned weights in blocks."
FIXED-TO-FIXED SPARSITY ENCODING,0.11395348837209303,"XOR-gates
00 01 10 11"
FIXED-TO-FIXED SPARSITY ENCODING,0.11627906976744186,XOR-gates
FIXED-TO-FIXED SPARSITY ENCODING,0.1186046511627907,XOR-gates
FIXED-TO-FIXED SPARSITY ENCODING,0.12093023255813953,XOR-gates
FIXED-TO-FIXED SPARSITY ENCODING,0.12325581395348838,"Random Number 
Generator 0000 1110 0101 1001"
FIXED-TO-FIXED SPARSITY ENCODING,0.12558139534883722,Weight to be encoded
FIXED-TO-FIXED SPARSITY ENCODING,0.12790697674418605,Unmatched
FIXED-TO-FIXED SPARSITY ENCODING,0.13023255813953488,Unmatched
FIXED-TO-FIXED SPARSITY ENCODING,0.1325581395348837,Unmatched
FIXED-TO-FIXED SPARSITY ENCODING,0.13488372093023257,Matched
FIXED-TO-FIXED SPARSITY ENCODING,0.1372093023255814,"X0X1
0X0X"
FIXED-TO-FIXED SPARSITY ENCODING,0.13953488372093023,Matched
FIXED-TO-FIXED SPARSITY ENCODING,0.14186046511627906,Matched
FIXED-TO-FIXED SPARSITY ENCODING,0.14418604651162792,Unmatched
FIXED-TO-FIXED SPARSITY ENCODING,0.14651162790697675,Unmatched
FIXED-TO-FIXED SPARSITY ENCODING,0.14883720930232558,Figure 3: Encoding of weights using an XOR-gate decoder as a random number generator.
FIXED-TO-FIXED SPARSITY ENCODING,0.1511627906976744,"In this work, we propose a “ﬁxed-to-ﬁxed” compression scheme as shown in Figure 2 when the
number of pruned weights in a block can vary. A successful ﬁxed-to-ﬁxed compression of sparse
NNs should consider the followings:"
FIXED-TO-FIXED SPARSITY ENCODING,0.15348837209302327,"• (High compression ratio) The maximum compression ratio is limited by the minimum
entropy (that can be obtained by a ﬁxed-to-variable scheme as we discuss in Appendix D).
Suppose that a block to be encoded contains (ﬁxed) nb bits among which (ﬁxed) nu bits are
unpruned. A ﬁxed-to-ﬁxed encoding scheme is required to support high compression close
to (nb/nu) (estimated by entropy). Fixed-to-ﬁxed decoding, then, accepts (ﬁxed) Nin bits
as an input and produces Nout bits as an output while Nout/Nin = nb/nu.
• (Variation tolerance) For a ﬁne-grained pruning, nu is given as a random variable whose
distribution is affected by pruning rate, nb size, a particular pruning method, and so on.
Our goal is to maintain a ﬁxed-to-ﬁxed scheme with a high compression ratio even under
Var[nu] ̸= 0. In Figure 2, for example, 5 blocks of original data have various nu values
while the size of an encoded block is ﬁxed to be 4 (=E[nu]). We will discuss how to design
a variation tolerant encoding."
RANDOM NUMBER GENERATOR,0.1558139534883721,"3
RANDOM NUMBER GENERATOR"
RANDOM NUMBER GENERATOR,0.15813953488372093,"Before we discuss compression schemes, let us assume that a binary masking matrix is given to
represent which weights are pruned or not (note such a binary masking matrix can be compressed
signiﬁcantly (Lee et al., 2019a)). Then, a pruned weight can be described as a don’t care value ()
that is to be masked. We also assume that 1) pruning each weight is performed independently with
pruning rate S and 2) unpruned weight is assigned to 0 or 1 with equal probability (such assumptions
are not necessary when we demonstrate our experimental results in Section 5)."
RANDOM NUMBER GENERATOR,0.16046511627906976,"To obtain both “high compression ratio” and “variation tolerance” while a ﬁxed-to-ﬁxed compression
scheme is considered, we adopt random number generator schemes that enable encoding/decoding
of weights. A random number generator accepts a ﬁxed number of inputs and produces nb bits
so as to implement a ﬁxed-to-ﬁxed compression scheme. As shown in Figure 3, a weight block is
compared with every output of a random number generator. If there is an output vector matching
original (partially masked) weights, then a corresponding input vector of a random number generator
can be an encoded input vector. As an effort to increase the Hamming distance between any two
outputs (i.e., the number of bit positions in which two bits are different), 2nu outputs out of 2nb"
RANDOM NUMBER GENERATOR,0.16279069767441862,Published as a conference paper at ICLR 2022
RANDOM NUMBER GENERATOR,0.16511627906976745,"0.5
0.6
0.7
0.8
0.9
S 4 8 12 16 20 Nin"
RANDOM NUMBER GENERATOR,0.16744186046511628,"90.03
(±7.03)
89.98
(±6.85)
89.48
(±7.41)
90.35
(±3.80)
90.24
(±2.51)"
RANDOM NUMBER GENERATOR,0.1697674418604651,"94.99
(±2.28)
95.02
(±1.54)
95.34
(±1.29)
95.07
(±0.82)
95.00
(±0.81)"
RANDOM NUMBER GENERATOR,0.17209302325581396,"96.75
(±0.68)
96.74
(±0.41)
97.06
(±0.54)
96.72
(±0.35)
96.72
(±0.35)"
RANDOM NUMBER GENERATOR,0.1744186046511628,"97.53
(±0.36)
97.55
(±0.21)
97.65
(±0.17)
97.55
(±0.36)
97.57
(±0.31)"
RANDOM NUMBER GENERATOR,0.17674418604651163,"98.05
(±0.18)
98.06
(±0.12)
98.19
(±0.14)
98.05
(±0.17)
98.05
(±0.26)"
RANDOM NUMBER GENERATOR,0.17906976744186046,"(a) nu (=a number of unpruned
weights in a block) is ﬁxed to be
Nin (i.e., Var[nu]=0)."
RANDOM NUMBER GENERATOR,0.1813953488372093,"0.5
0.6
0.7
0.8
0.9
S 4 8 12 16 20 Nin"
RANDOM NUMBER GENERATOR,0.18372093023255814,"88.24
(±7.23)
88.46
(±6.17)
88.32
(±5.44)
87.82
(±4.31)
87.67
(±2.33)"
RANDOM NUMBER GENERATOR,0.18604651162790697,"94.12
(±1.51)
93.72
(±1.43)
93.91
(±1.21)
93.46
(±1.10)
93.22
(±0.90)"
RANDOM NUMBER GENERATOR,0.1883720930232558,"95.86
(±0.83)
95.78
(±0.45)
95.91
(±0.75)
95.43
(±0.32)
95.36
(±0.35)"
RANDOM NUMBER GENERATOR,0.19069767441860466,"96.82
(±0.32)
96.71
(±0.22)
96.68
(±0.25)
96.52
(±0.19)
96.41
(±0.26)"
RANDOM NUMBER GENERATOR,0.1930232558139535,"97.44
(±0.13)
97.28
(±0.15)
97.32
(±0.14)
97.12
(±0.17)
97.03
(±0.24)"
RANDOM NUMBER GENERATOR,0.19534883720930232,"(b) nu follows a binomial distribu-
tion B(Nout, 1 −S)."
RANDOM NUMBER GENERATOR,0.19767441860465115,"0.5
0.6
0.7
0.8
0.9
S 4 8 12 16 20 Nin"
RANDOM NUMBER GENERATOR,0.2,"88.31
(±7.19)
88.07
(±6.09)
87.16
(±5.49)
86.91
(±4.39)
86.67
(±3.03)"
RANDOM NUMBER GENERATOR,0.20232558139534884,"93.75
(±1.67)
93.19
(±1.38)
93.00
(±1.03)
92.61
(±1.01)
92.39
(±0.60)"
RANDOM NUMBER GENERATOR,0.20465116279069767,"95.59
(±0.67)
95.23
(±0.72)
94.89
(±0.50)
94.71
(±0.33)
94.50
(±0.28)"
RANDOM NUMBER GENERATOR,0.2069767441860465,"96.43
(±0.36)
96.18
(±0.25)
95.69
(±0.13)
95.73
(±0.12)
95.56
(±0.16)"
RANDOM NUMBER GENERATOR,0.20930232558139536,"97.08
(±0.12)
96.79
(±0.09)
96.32
(±0.09)
96.40
(±0.08)
96.35
(±0.13)"
RANDOM NUMBER GENERATOR,0.2116279069767442,"(c) nu is empirically obtained by
pruning the ﬁrst decoder layer of
the Transformer using a magnitude-
based pruning method."
RANDOM NUMBER GENERATOR,0.21395348837209302,"Figure 4: Encoding efﬁciency (%) of random XOR-gate decoders. S is pruning rate and Nout is
given as ⌊Nin · (1/(1−S))⌋."
RANDOM NUMBER GENERATOR,0.21627906976744185,"possible candidates need to be randomly selected. Note that random encoding has already been
suggested by Claude Shannon to introduce channel capacity that is the fundamental theory in digital
communication (Morelos-Zaragoza, 2006). Since then, practical error correction coding techniques
have been proposed to implement random-like coding by taking into account efﬁcient decoding
(instead of using a large look-up table)."
RANDOM NUMBER GENERATOR,0.2186046511627907,"Similar to error correction coding that usually depends on linear operations over Galois Field with two
elements, or GF(2) (Morelos-Zaragoza, 2006), for simple encoding of original data, recently, two
compression techniques for sparse NNs have been proposed. An XOR-gate decoder produces (a large
number of) binary outputs using (a relatively much smaller number of) binary inputs while outputs
are quantized weights (Kwon et al., 2020). Another example is to adopt a Viterbi encoding/decoding
scheme (Forney, 1973) to generate multiple bits using a single bit as an input (Ahn et al., 2019). For
a block that cannot be encoded into a compressed one by a random number generator, we can attach
patch data to ﬁx unmatched bits (Kwon et al., 2020) or re-train the model to improve the accuracy
(Ahn et al., 2019)."
RANDOM NUMBER GENERATOR,0.22093023255813954,"To compare the random number generation capability of various block-level compression schemes,
we introduce ‘encoding efﬁciency’ given as a percentage."
RANDOM NUMBER GENERATOR,0.22325581395348837,E (Encoding Efﬁciency) = # of correctly matched bits
RANDOM NUMBER GENERATOR,0.2255813953488372,"# of unpruned bits
× 100(%)
(1)"
RANDOM NUMBER GENERATOR,0.22790697674418606,"Let S be pruning rate (0 ≤S ≤1). To measure encoding efﬁciency (E), we assume that the
compression ratio of a random number generator (=the number of output bits / the number of input
bits) is 1/(1 −S). We generate numerous randomly pruned (binary) weight blocks, and for each
block, we investigate all of the possible outputs that a random number generator can produce. If
there is a block missing a matching output of a generator, then the maximum number of correctly
matched bits is recorded for each block. We repeat such an experiment for all of the blocks. Note that
E cannot be higher than 100% for any generators."
FIXED PRUNING RATE IN A BLOCK,0.2302325581395349,"3.1
FIXED PRUNING RATE IN A BLOCK"
FIXED PRUNING RATE IN A BLOCK,0.23255813953488372,"For simplicity, we assume that nu in a block is a ﬁxed number. Let us study E when nu is ﬁxed
using an XOR-gate decoder introduced in (Kwon et al., 2020). For an XOR-gate decoder, when Nout
is the number of output bits and Nin is the number of input bits, a matrix M ⊕∈{0, 1}Nout×Nin
presents connectivity information between an input vector wx(∈{0, 1}Nin) and an output vector
wy(∈{0, 1}Nout) such that we have wy = M ⊕· wx over GF(2). For example, if the second row
of M ⊕(with Nin = 4 and Nout = 8) is given as [1 0 1 1], then wy
2 = wx
1 ⊕wx
3 ⊕wx
4 (‘⊕’ indicates
a binary XOR operation or an addition over GF(2) equivalently). An element of M ⊕is randomly
ﬁlled with 0 or 1 as a simple random number generator design technique (Kwon et al., 2020)."
FIXED PRUNING RATE IN A BLOCK,0.23488372093023255,"To measure E, let Nin/Nout ≈1 −S such that Nout = ⌊Nin · (1/(1−S))⌋. Correspondingly, for
a certain S, a block size (=Nout) increases as Nin increases. When nb = Nout and nu = Nin,"
FIXED PRUNING RATE IN A BLOCK,0.2372093023255814,Published as a conference paper at ICLR 2022
FIXED PRUNING RATE IN A BLOCK,0.23953488372093024,"Figure 4a describes statistics of E when random M ⊕matrices are associated with random blocks.
From Figure 4a, it is clear that increasing Nin is the key to improving encoding efﬁciency. Note that,
however, increasing Nin and Nout complicates the decoding complexity (due to large M ⊕) and the
encoding complexity as well (due to an exponentially large search space)."
VARIABLE PRUNING RATE IN A BLOCK,0.24186046511627907,"3.2
VARIABLE PRUNING RATE IN A BLOCK"
VARIABLE PRUNING RATE IN A BLOCK,0.2441860465116279,"Now we allow nu in a block to ﬂuctuate. For Figure 4b, we assume that pruning each weight
is a Bernoulli event (with S probability) such that nu in a block follows a binomial distribution
B(Nout, 1−S) (thus, E[nu] = Nout(1−S) and Var[nu] = NoutS(1−S)). Nin is given as E[nu] in
Figure 4b. Compared to Figure 4a, E becomes lower mainly because some blocks would have nu
larger than Nin (i.e., too many unpruned weight bits that a random number generator cannot target)."
VARIABLE PRUNING RATE IN A BLOCK,0.24651162790697675,"x
x x x x x
x
x
x
x
Encoding"
VARIABLE PRUNING RATE IN A BLOCK,0.24883720930232558,"Figure 5: Encoding of two blocks when a number
of unpruned weights can vary in a block."
VARIABLE PRUNING RATE IN A BLOCK,0.25116279069767444,"Note that the coefﬁcient of variation of nu
(=
p"
VARIABLE PRUNING RATE IN A BLOCK,0.2534883720930233,"Var[nu]/E[nu] =
p"
VARIABLE PRUNING RATE IN A BLOCK,0.2558139534883721,"S/(Nout(1−S))) de-
creases as Nout increases. Indeed, the gap be-
tween E in Figure 4a and E in Figure 4b tends
to be slightly reduced when Nin (as well as cor-
responding Nout) increases. To illustrate, as
shown in Figure 5, when there are two blocks
of different nu, concatenating two blocks into
a block (thus, increasing Nout) can increase the
probability of successful encoding due to efﬁ-
cient usage of Nin. Increasing Nin and Nout by
n times, however, requires an XOR-gate decoder
to be larger by n2 times with only a marginal gain in E."
VARIABLE PRUNING RATE IN A BLOCK,0.25813953488372093,"Another way to improve E under the variation on nu is to decrease Nout and increase Nin. In
this case, however, the compression ratio is reduced consequently. We need a solution to design a
ﬁxed-to-ﬁxed sparsity compression technique that can improve E of a random number generator
under the variation on nu without sacriﬁcing compression ratio (i.e., Nin/Nout = 1−S)."
VARIABLE PRUNING RATE IN A BLOCK,0.26046511627906976,"To validate our observations obtained by synthetic random data, we compute E of an XOR-gate
decoder using the Transformer model (Vaswani et al., 2017). The ﬁrst fully-connected layer of the
Transformer is pruned by a magnitude-based pruning method (Han et al., 2015) with pruning rate S.
Interestingly, E described in Figure 4c is similar to that of Figure 4b. As such, our assumption that
pruning a weight is a Bernoulli event is valid for the context of magnitude-based pruning."
PROPOSED SEQUENTIAL ENCODING SCHEME,0.2627906976744186,"4
PROPOSED SEQUENTIAL ENCODING SCHEME"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.2651162790697674,"If Var[nu] is non-zero and Nin is ﬁxed, then blocks of small nu (< Nin) would have many possible
input vectors with matching output vectors while other blocks with large nu (> Nin) may have no
any single possible input vector. Such unbalance among encoding success rates over blocks can
be mitigated if a part of input vectors associated with a block of small nu can be reused for the
neighboring blocks of large nu. In other words, by sharing some parts of an input vector for multiple
consecutive blocks (of diverse nu values), input search space size of each block can be balanced.
Reusing inputs is fulﬁlled by shift registers that have been also introduced to convolutional codes
such as Viterbi codes (Morelos-Zaragoza, 2006). In this section, we propose sequential encoding
techniques to address the limitations of previous ﬁxed-to-ﬁxed compression schemes (for example,
XOR-gate-only decoder (Kwon et al., 2020) lacking tolerance for nu variation, and Viterbi encoders
(Ahn et al., 2019) that receive only one bit as an input (i.e., Nin is restricted to be 1)."
PROPOSED SEQUENTIAL ENCODING SCHEME,0.26744186046511625,"Weight manipulation
Since our encoding/decoding techniques process data in a block-level (in
the form of a 1-D vector), original sparse weight matrices (or tensors) need to be reshaped through
grouping, ﬂattening, and slicing. Assuming that a number format has the precision of nw bits (e.g.,
nw=32 for FP32), as the ﬁrst step of weight manipulation, a weight matrix W ∈Rm×n is grouped
into nw binary matrices W b
1..nw∈{0, 1}m×n (otherwise, nw successive bits are pruned or unpruned).
Then, each binary matrix (or tensor) W b
i is ﬂattened to be a 1-D vector and each vector is sliced into
wb
i,1..l blocks when l (=⌈mn"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.26976744186046514,Nout ⌉) indicates the number of blocks in a 1-D (ﬂattened) vector.
PROPOSED SEQUENTIAL ENCODING SCHEME,0.27209302325581397,Published as a conference paper at ICLR 2022 sign
PROPOSED SEQUENTIAL ENCODING SCHEME,0.2744186046511628,"Decoded
Weight ="
PROPOSED SEQUENTIAL ENCODING SCHEME,0.27674418604651163,"Flattened and Sliced Vectors         
Encoding"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.27906976744186046,"Shift 
Register"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.2813953488372093,"XOR-gate
Network (     )"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.2837209302325581,During Inference
PROPOSED SEQUENTIAL ENCODING SCHEME,0.28604651162790695,"Original
Weight"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.28837209302325584,Computed Offline
PROPOSED SEQUENTIAL ENCODING SCHEME,0.29069767441860467,"...
...
exponent
fraction
100
110
000"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.2930232558139535,1 x x 1 x 100 x x 0 x 0 x 01
PROPOSED SEQUENTIAL ENCODING SCHEME,0.29534883720930233,1011010011000001
PROPOSED SEQUENTIAL ENCODING SCHEME,0.29767441860465116,Example
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3,Encoding
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3023255813953488,Decoding Reuse
PROPOSED SEQUENTIAL ENCODING SCHEME,0.30465116279069765,"Mask
Mask"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.30697674418604654,"Shift 
Register"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.30930232558139537,Randomly filled through Encoding
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3116279069767442,Decoder
PROPOSED SEQUENTIAL ENCODING SCHEME,0.313953488372093,: Concatenation
PROPOSED SEQUENTIAL ENCODING SCHEME,0.31627906976744186,"(m,n) fp32"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3186046511627907,"bits1 2 3
32 ... bits ..."
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3209302325581395,bits1 2 3 ... 32
PROPOSED SEQUENTIAL ENCODING SCHEME,0.32325581395348835,"1 2 3
32"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.32558139534883723,(Masked)
PROPOSED SEQUENTIAL ENCODING SCHEME,0.32790697674418606,"Figure 6: Proposed ﬁxed-to-ﬁxed sequential encoding/decoding scheme. Weight encoding is per-
formed ofﬂine, and thus, complex computation for encoding is allowed. Encoded weights are decoded
during inference through XOR-gate decoders that are best implemented by ASICs or FPGAs. Pruned
weights are ﬁlled by random values during weight decoding."
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3302325581395349,"Decoding with an input sequence
For an XOR-gate decoder (as a non-sequential decoder) at time
index t, an output vector wb′
t (∈{0, 1}Nout) is a function of an input vector we
t (∈{0, 1}Nin) such
that we
t is utilized only for one time index. In our proposed sequential encoding scheme, we
t is
exploited for multiple time indices using shift registers. Speciﬁcally, we copy we
t to a shift register
whose outputs are connected to the inputs of the next shift register. Let Ns be the number of shift
registers. In the proposed scheme, an XOR-gate decoder receives inputs from Ns shift registers as
well as we
t . Then, as shown in Figure 6, a sequential decoder (consisting of an XOR-gate decoder
and shift registers) produces wb′
t as a function of an input sequence of (we
t , we
t−1, ...) while such a
function can be described as wb′
t = M ⊕(we
t
⌢we
t−1
⌢...⌢we
t−Ns) over GF(2) where A⌢B implies
a concatenation of A and B and the sequence length is (Ns + 1). Note that in Figure 6, an input
vector we
t is reused Ns times and an XOR-gate decoder accepts (Ns + 1) · Nin bits to yield Nout
bits (hence, M ⊕∈{0, 1}Nout×((Ns+1)·Nin)). Increasing Ns enables 1) a larger XOR-gate decoder
(without increasing Nin) that improves E as demonstrated in Figure 4 and 2) multiple paths from an
input vector to multiple output vectors (of various nu) resulting in balanced encoding. Note that our
XOR-gate decoder (or effectively memory decompressor) is best implemented by ASICs or FPGAs
where each XOR requires only 6 transistors (Rabaey et al., 2004)."
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3325581395348837,"1 x 0 x x x x x
x 1 x 0 0 x 1 x"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.33488372093023255,"1 0 0 0 1 0
0 1 0"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3372093023255814,0 x x x 0 x 1 x
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3395348837209302,"1 1 1
1 1 1 1 1 0"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.34186046511627904,"vs.
Matched?"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.34418604651162793,"vs.
Matched?"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.34651162790697676,"vs.
Matched?"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3488372093023256,"Figure 7: Sequential decoding example when
Ns=1, Nin=3, and Nout=8. An input is utilized
for (Ns+1) time indices through shift registers."
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3511627906976744,"Balanced encoding
Figure 7 illustrates a
decoding process when Ns=1, Nin=3, and
Nout=8. Each weight vector to be encoded con-
tains a different number of unpruned weights.
For wb′
t at time index t (in Figure 7), a less
number of unpruned weights tends to enlarge
search space for we
t . On the other hand for
wb′
t+1 at time index (t + 1), a large number of
unpruned weights would highly restrict search
space for we
t . Correspondingly, compared to the
case when we
t is associated with only one wb′"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.35348837209302325,"block (i.e., non-sequential encoding with Ns=0),
search space for we
t can be balanced as more
wb′ blocks of various nu are correlated with we
t .
Note that as Var[nu] of one block increases, ac-
cording to the central limit theorem, Ns is required to be larger to maintain the balance of encoding
capability of each we
t . As we demonstrate in the next section, under the variation on nu, even small
non-zero Ns can enhance E substantially while increasing Nin (and Nout while Ns=0) improves E
only marginally (as described in Figure 4)."
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3558139534883721,Published as a conference paper at ICLR 2022
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3581395348837209,"20
40
60
80
100 120 140 160
Nout 70 80 90"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.36046511627906974,Memory Reduction (%)
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3627906976744186,upper bound by S=0.9
PROPOSED SEQUENTIAL ENCODING SCHEME,0.36511627906976746,"Ns=2
Ns=1
Ns=0"
PROPOSED SEQUENTIAL ENCODING SCHEME,0.3674418604651163,"20
40
60
80
100 120 140 160
Nout"
K,0.3697674418604651,10k
K,0.37209302325581395,20k
K,0.3744186046511628,30k
K,0.3767441860465116,40k
K,0.37906976744186044,50k
K,0.3813953488372093,# of Encrypted Bits
K,0.38372093023255816,"20
40
60
80
100 120 140 160
Nout 90 100 E (%)"
K,0.386046511627907,"Ns=2
Ns=1
Ns=0"
K,0.3883720930232558,0k
K,0.39069767441860465,1k
K,0.3930232558139535,# of Error Bits
K,0.3953488372093023,Error(Ns=2)
K,0.39767441860465114,Error(Ns=1)
K,0.4,Error(Ns=0)
K,0.40232558139534885,Encrypted
K,0.4046511627906977,"Figure 8: Impact of Ns with various Nout using 1M random bits, Nin = 8, and S=0.9."
K,0.4069767441860465,"Encoding algorithm
In the case of a non-sequential encoding scheme, because of one-to-one
correspondence between we
t and wb′
t through M ⊕, encoding can be performed independently for
each block (e.g., for a given masked wb
t, we select one matching wb′
t out of all available wb′
t sets by
a decoder). Such block-wise encoding, however, is not applicable to a sequential encoding scheme
that needs to consider the whole sequence of wb blocks to ﬁnd an input sequence fed into a decoder.
Suppose that we ﬁnd a particular output sequence matching l blocks after exploring all feasible output
sequences provided by a generator and the input sequence (we
1, we
2, ..., we
l ), the time-complexity of
encoding would be O(2Nin·l). Fortunately, sequential decoding operations shown in Figure 7 can
be models as a hidden Markov model, where each state is represented by concatenating we
t , we
t−1,
..., we
t−Ns and there are 2Nin paths for the next state transitions (Viterbi, 1998). Consequently, the
time- and space-complexity can be reduced to be O(2Nin·(Ns+1) · l) by dynamic programming that
computes the maximum-likelihood sequence in a hidden Markov model (Forney, 1973). For details
of our encoding algorithm, the reader is referred to Appendix E."
K,0.40930232558139534,"Lossless compression
Any random number generators cannot produce outputs perfectly matching
all unpruned weights (i.e., E is always less than 100%). To correct unmatched outputs of a random
number generator (by ﬂipping 0 ↔1) in order to enable lossless compression, the locations of all
unmatched weight bits need to be recorded. Note that if E ≈1, the number of unmatched weight
bits is a lot smaller than the number of encoded weight bits. If that is the case, such correction
information can be stored in a separate on-chip memory that can be independently accessed without
disturbing decoding operations. We propose a format representing such correction information in
Appendix F where each unmatched weight bit requires Nc bits in the correction format (Nc is around
10 in Appendix E). Taking into account a compression ratio of a generator given as Nout/Nin and
additional correction information (using Nc bits per one unmatched weight bit), a binary weight
matrix W b ∈{0, 1}m×n can be compressed into ( Nin"
K,0.4116279069767442,"Nout mn+Nerr) bits when Nerr = Nc×(# of
unmatched bits) = Ncmn(1 −S)(1 −E). Subsequently, memory reduction can be expressed as"
K,0.413953488372093,"Memory Save = 1 −((Nin/Nout)mn + Nerr)/(mn)
= 1 −(1 −S)(1 + (1 −E)Nc),
(2)"
K,0.41627906976744183,"when Nin/Nout is given as (1 −S). Thus, memory reduction approaches S when E approaches 1.
For the overall design complexity analysis of the proposed compression, refer to Appendix G."
EXPERIMENTAL RESULTS,0.4186046511627907,"5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.42093023255813955,"In this section, we demonstrate the encoding capability of our proposed sequential encoding tech-
niques using synthetic random data and NNs pruned by various pruning methods. Even though
various heuristic algorithms can be suggested, we adopt a simple dynamic programming technique
for weight encoding that minimizes the number of unmatched weight bits. For our experiments, Nin
is selected to be 8 such that we feed a decoder on a byte-level."
EXPERIMENTAL RESULTS,0.4232558139534884,"5.1
SYNTHETIC RANDOM DATA (Nin=8)"
EXPERIMENTAL RESULTS,0.4255813953488372,"Setup
We generate a random weight 1-D vector of 1M bits. We also create a random masking
data of 1M bits in which the percentage of zeros equals S. M ⊕matrix (formulating the structure of"
EXPERIMENTAL RESULTS,0.42790697674418604,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.43023255813953487,"an XOR-gate decoder) basically needs to be designed to maximize the randomness among outputs.
Measuring randomness, however, is challenging and such measurement may not be highly correlated
to E. Alternatively, we try numerous random M ⊕matrices and choose a particular M ⊕of the
highest E. Speciﬁcally, for a given set of Nin and Nout, an element of M ⊕∈RNout×((Ns+1)·Nin)
is randomly assigned to 0 or 1 with equal probability. Then, E of those random M ⊕matrices is
estimated by using given random binary weight vectors and masking data. The best M ⊕providing
the highest E (for a given set of Nin and Nout) is then utilized for our experiments."
EXPERIMENTAL RESULTS,0.4325581395348837,"Compression capability
Figure 8 demonstrates the impact of Ns on E and corresponding memory
reduction(%) with various Nout and 1M random bits when Nin=8 and S=0.9. Regardless of Ns,
as Nout increases (i.e., the compression ratio (=Nout/Nin=Nout/8) of a decoder increases), the
number of encoded bits is reduced while the number of unmatched (error) bits increases. Because
of such a trade-off between the number of encoded bits and the number of error bits, there exists a
certain Nout that maximizes the memory reduction. Note that compared to a non-sequential encoding
(of Ns=0), sequential encoding (even with small Ns) signiﬁcantly reduces the number of error bits
and maintains high E (of almost 100%) until Nout reaches Nin×(1/(1−S))=80. Indeed, memory
reduction becomes highest (=89.32%) when Ns is highest and Nout is 80 in Figure 8. As we discussed
for lossless compression in Section 5, 89.32% of memory reduction is close to S(=90%) that is the
maximum memory reduction obtained when E ≈1. In the remainder of this paper, Nout is given as
Nin×(1/(1−S)) to maximize the memory reduction."
EXPERIMENTAL RESULTS,0.43488372093023253,"Table 1: Memory reduction (%) using 1M random bits and
various Ns and S when Nin = 8 and Nout=Nin ·1/(1−S)."
EXPERIMENTAL RESULTS,0.4372093023255814,"Ns
S
60.0%
70.0%
80.0%
90.0%"
EXPERIMENTAL RESULTS,0.43953488372093025,"0
38.6%
53.8%
67.9%
83.5%
1
55.9%
67.4%
77.5%
88.5%
2
58.4%
69.1%
78.9%
89.3%"
EXPERIMENTAL RESULTS,0.4418604651162791,"Impact of S on memory reduction
Table 1 presents memory reduction
using various S. For a certain S in Ta-
ble 1, as Ns increases, the difference
between S and memory reduction de-
creases. In other words, regardless
of S, sequential encoding/decoding
principles are crucial for memory re-
duction to approach S which is the
maximum. Increasing S also facili-
tates memory reduction to approach
S in Table 1 as described in Eq. 2 where increasing S with a constant E enhances memory reduction."
EXPERIMENTAL RESULTS,0.4441860465116279,"0.0
0.2
0.4
0.6
0.8
1.0
Ratio of Zeros(S = 0.7) 96 98 100 E (%)"
EXPERIMENTAL RESULTS,0.44651162790697674,"Ns = 2
Ns = 1
Ns = 0"
EXPERIMENTAL RESULTS,0.44883720930232557,"Figure 9: E with various ratio of zero in a
random vector."
EXPERIMENTAL RESULTS,0.4511627906976744,"Inverting technique
So far, we assumed that a bi-
nary weight matrix holds equal amounts of zeros and
ones. A few representations such as binary-coding-
based quantization (Rastegari et al., 2016; Xu et al.,
2018) and signed INT8 (Jacob et al., 2018) would in-
herently justify such an assumption. However, there
exist exceptional representations as well (e.g., FP32).
We conduct experiments to ﬁnd a relationship be-
tween the ratio of zeros and E using a random weight
vector as shown in Figure 9. E increases if a sub-
stantial amount of zeros are employed as unpruned
weight bits, because of a higher chance to ﬁnd trivial
inputs (of all zeros fed into XOR operations) to pro-
duce zero outputs. Hence, to improve E (especially
when Ns is low), we propose an inverting technique
where an entire binary weight vector is inverted if the ratio of zeros is less than 50%."
EXPERIMENTAL RESULTS,0.45348837209302323,"5.2
SPARSE TRANSFORMER AND RESNET-50 (Nin=8)"
EXPERIMENTAL RESULTS,0.4558139534883721,"We measure compression capability of our proposed sequential encoding scheme using sparse
Transformer (Vaswani et al., 2017) on WMT’14 en-de dataset and ResNet-50 (He et al., 2016) on
ImageNet. Those two models1 in FP32 format are pruned by various methods including magnitude-
based one (Han et al., 2015), L0 regularization (Louizos et al., 2018), and random pruning (Gale et al.,"
EXPERIMENTAL RESULTS,0.45813953488372094,1https://github.com/google-research/google-research/tree/master/state_of_sparsity
EXPERIMENTAL RESULTS,0.4604651162790698,Published as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.4627906976744186,"Table 2: E and memory reduction of sparse Transformer and ResNet-50 pruned by two different
pruning methods. When Ns is 0 or 1, inverting can be applied to a layer if unpruned weights
accommodate more zeros than ones. Inverting has no effect for weights of signed INT8."
EXPERIMENTAL RESULTS,0.46511627906976744,"E (%)
Memory Reduction (%)
(Max: 100%)
(Max: S)"
EXPERIMENTAL RESULTS,0.46744186046511627,"Model
S(Method)
Non-Seq.
Sequential
Non-Seq.
Sequential
Ns=0(Inv.)
Ns=1(Inv.)
Ns=2
Ns=0(Inv.)
Ns=1(Inv.)
Ns=2"
EXPERIMENTAL RESULTS,0.4697674418604651,"Transformer
WMT14 en-de
(FP32)"
EXPERIMENTAL RESULTS,0.4720930232558139,"70%(Mag.)
93.8(94.5)
98.0(98.3)
98.7
50.3(52.4)
63.1(63.8)
65.3
70%(Rand.)
94.6(95.2)
99.2(99.3)
99.8
52.8(54.6)
66.6(66.8)
68.3
90%(Mag.)
92.6(93.9)
97.6(97.9)
98.4
82.4(83.7)
87.4(87.7)
88.2
90%(Rand.)
93.7(94.5)
98.7(98.9)
99.5
83.5(84.3)
88.5(88.7)
89.3"
EXPERIMENTAL RESULTS,0.4744186046511628,"ResNet-50
ImageNet
(FP32)"
EXPERIMENTAL RESULTS,0.47674418604651164,"70%(Mag.)
94.4(95.0)
98.6(98.7)
99.1
52.2(54.2)
64.7(65.3)
66.5
70%(Rand.)
94.6(95.1)
99.1(99.2)
99.7
52.7(54.2)
66.5(66.7)
68.3
90%(Mag.)
92.7(93.7)
97.3(97.6)
98.1
82.5(83.5)
87.1(87.4)
87.9
90%(Rand.)
92.7(93.5)
97.6(97.9)
98.7
82.5(83.3)
87.5(87.7)
88.6
ResNet-50
ImageNet
(Signed
INT8)"
EXPERIMENTAL RESULTS,0.4790697674418605,"70%(Mag.)
93.9(N/A)
98.5(N/A)
99.1
50.9(N/A)
64.5(N/A)
66.4
70%(Rand.)
96.2(N/A)
99.7(N/A)
99.9
57.6(N/A)
68.3(N/A)
69.0
90%(Mag.)
92.4(N/A)
97.1(N/A)
98.0
82.2(N/A)
86.9(N/A)
87.8
90%(Rand.)
93.5(N/A)
98.2(N/A)
99.2
83.3(N/A)
88.0(N/A)
89.0"
EXPERIMENTAL RESULTS,0.4813953488372093,"Table 3: Coefﬁcient of variation of nu and E of two selected layers of the Transformer pruned by
random, magnitude-based, or L0 regularization pruning method."
EXPERIMENTAL RESULTS,0.48372093023255813,"Pruning
Method"
EXPERIMENTAL RESULTS,0.48604651162790696,"Target
S"
EXPERIMENTAL RESULTS,0.4883720930232558,"Layer: dec3/self_att/q
Layer: dec3/ffn2
(512 × 512), FP32
(2048 × 512), FP32
Coeff. of
Var. (nu)"
EXPERIMENTAL RESULTS,0.4906976744186046,"E (%)
Coeff. of
Var. (nu)"
EXPERIMENTAL RESULTS,0.4930232558139535,"E (%)
Ns=0
Ns=1
Ns=2
Ns=0
Ns=1
Ns=2
Random 0.7"
EXPERIMENTAL RESULTS,0.49534883720930234,"0.299
94.6
99.2
99.8
0.303
94.6
99.2
99.8
Mag.
0.324
94.5
98.9
99.6
0.366
94.1
98.3
98.9
L0 Reg.
0.347
94.5
99.0
99.6
0.331
94.3
98.7
99.2"
EXPERIMENTAL RESULTS,0.49767441860465117,"2019) (also variational dropout (Molchanov et al., 2017) in Appendix H). For the ResNet-50 model
(on ImageNet), we also consider signed INT8 format (Jacob et al., 2018). Table 2 presents E and
memory reduction when every layer of the Transformer and ResNet-50 is pruned by the same pruning
rate S. Both E and memory reduction are signiﬁcantly improved by increasing Ns. Even compared
to the case when inverting technique is applied to non-sequential encoding (Ns=0), we observe that
sequential encoding (Ns>0) without inverting yields a lot higher compression capability. Note that
the compression capabilities of random pruning and magnitude-based pruning methods are similar in
Table 2 such that our experiments with synthetic random data are justiﬁed. Such justiﬁcation is also
veriﬁed in Table 3 that is achieved by using two selected layers of the Transformer. Compared to
random pruning, magnitude-based and L0 regularization pruning methods exhibit somewhat lower E
that is related to higher coefﬁcients of variation of nu. See Appendix H for additional results."
EXPERIMENTAL RESULTS,0.5,"All in all, our proposed encoding method designed in the context of random pruning is also effec-
tive for other ﬁne-grained pruning methods. Through various cases including synthetic data and
benchmark models, we demonstrated the superiority of the proposed encoding scheme to previous
ﬁxed-to-ﬁxed compression methods including a non-sequential XOR-gate decoder of Ns=0 (Kwon
et al., 2020) and a Viterbi-based encoder structure where Nin is limited to be 1 (Ahn et al., 2019)."
CONCLUSION,0.5023255813953489,"6
CONCLUSION"
CONCLUSION,0.5046511627906977,"In this paper, we proposed a sequential encoding scheme that is a useful ﬁxed-to-ﬁxed compression
for sparse NNs. We studied the maximum compression ratio using entropy based on the strategy
of mapping a weight block into a small number of symbols. We also investigated random number
generators as a practical ﬁxed-to-ﬁxed decoder using an XOR-gate decoder. Random number
generators can improve compression capability if input vectors are reused for multiple time indices
through shift registers."
CONCLUSION,0.5069767441860465,Published as a conference paper at ICLR 2022
REFERENCES,0.5093023255813953,REFERENCES
REFERENCES,0.5116279069767442,"D. Ahn, D. Lee, T. Kim, and J.-J. Kim. Double Viterbi: Weight encoding for high compression ratio
and fast on-chip reconstruction for deep neural network. In International Conference on Learning
Representations (ICLR), 2019."
REFERENCES,0.513953488372093,"G. D. Forney. The Viterbi algorithm. Proc. of the IEEE, 61:268 – 278, March 1973."
REFERENCES,0.5162790697674419,"J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
arXiv:1803.03635, 2018."
REFERENCES,0.5186046511627908,"T. Gale, E. Elsen, and S. Hooker. The state of sparsity in deep neural networks. arXiv preprint
arXiv:1902.09574, 2019."
REFERENCES,0.5209302325581395,"T. Gale, M. Zaharia, C. Young, and E. Elsen. Sparse gpu kernels for deep learning. arXiv preprint
arXiv:2006.10901, 2020."
REFERENCES,0.5232558139534884,"S. Golomb. Run-length encodings. IEEE Transactions on Information Theory, 12(3):399–401, 1966."
REFERENCES,0.5255813953488372,"S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both weights and connections for efﬁcient neural
networks. In Advances in Neural Information Processing Systems, pages 1135–1143, 2015."
REFERENCES,0.5279069767441861,"S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally. EIE: efﬁcient inference
engine on compressed deep neural network. In Proceedings of the 43rd International Symposium
on Computer Architecture, pages 243–254, 2016a."
REFERENCES,0.5302325581395348,"S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with
pruning, trained quantization and Huffman coding. In International Conference on Learning
Representations (ICLR), 2016b."
REFERENCES,0.5325581395348837,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016."
REFERENCES,0.5348837209302325,"Y. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In
Proceedings of the IEEE International Conference on Computer Vision, pages 1389–1397, 2017."
REFERENCES,0.5372093023255814,"D. A. Huffman. A method for the construction of minimum-redundancy codes. Proceedings of the
IRE, 40(9):1098–1101, 1952."
REFERENCES,0.5395348837209303,"B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko.
Quantization and training of neural networks for efﬁcient integer-arithmetic-only inference. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2704–
2713, 2018."
REFERENCES,0.541860465116279,"S. J. Kwon, D. Lee, B. Kim, P. Kapoor, B. Park, and G.-Y. Wei. Structured compression by weight
encryption for unstructured pruning and quantization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 1909–1918, 2020."
REFERENCES,0.5441860465116279,"Y. LeCun, J. S. Denker, and S. A. Solla. Optimal brain damage. In Advances in Neural Information
Processing Systems, pages 598–605, 1990."
REFERENCES,0.5465116279069767,"D. Lee, S. J. Kwon, P. Kapoor, B. Kim, and G.-Y. Wei. Network pruning for low-rank binary indexing.
arXiv:1905.05686, 2019a."
REFERENCES,0.5488372093023256,"N. Lee, T. Ajanthan, and P. H. Torr. Snip: Single-shot network pruning based on connection sensitivity.
In ICLR, 2019b."
REFERENCES,0.5511627906976744,"H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning ﬁlters for efﬁcient convnets. In
International Conference on Learning Representations, 2017."
REFERENCES,0.5534883720930233,"Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efﬁcient convolutional networks
through network slimming. In ICCV, 2017."
REFERENCES,0.5558139534883721,"C. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through l_0 regularization.
In International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=H1Y8hhg0b."
REFERENCES,0.5581395348837209,Published as a conference paper at ICLR 2022
REFERENCES,0.5604651162790698,"D. Molchanov, A. Ashukha, and D. P. Vetrov. Variational dropout sparsiﬁes deep neural networks. In
International Conference on Machine Learning (ICML), pages 2498–2507, 2017."
REFERENCES,0.5627906976744186,"R. H. Morelos-Zaragoza. The art of error correcting coding. John Wiley & Sons, 2nd edition, 2006."
REFERENCES,0.5651162790697675,"S. Narang, G. Diamos, S. Sengupta, and E. Elsen. Exploring sparsity in recurrent neural networks. In
International Conference on Learning Representations (ICLR), 2017."
REFERENCES,0.5674418604651162,"J. M. Rabaey, A. Chandrakasan, and B. Nikolic. Digital integrated circuits- A design perspective.
Prentice Hall, 2ed edition, 2004."
REFERENCES,0.5697674418604651,"M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: Imagenet classiﬁcation using
binary convolutional neural networks. In ECCV, 2016."
REFERENCES,0.5720930232558139,"A. Renda, J. Frankle, and M. Carbin. Comparing rewinding and ﬁne-tuning in neural network pruning.
arXiv preprint arXiv:2003.02389, 2020."
REFERENCES,0.5744186046511628,"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.
Attention is all you need. arXiv:1706.03762, 2017."
REFERENCES,0.5767441860465117,"A. J. Viterbi. An intuitive justiﬁcation and a simpliﬁed implementation of the map decoder for
convolutional codes. IEEE Journal on Selected Areas in Communications, 16(2):260–264, 1998."
REFERENCES,0.5790697674418605,"C. Wang, G. Zhang, and R. Grosse. Picking winning tickets before training by preserving gradient ﬂow.
In International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=SkgsACVKPH."
REFERENCES,0.5813953488372093,"W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks.
Advances in neural information processing systems, 29:2074–2082, 2016."
REFERENCES,0.5837209302325581,"C. Xu, J. Yao, Z. Lin, W. Ou, Y. Cao, Z. Wang, and H. Zha. Alternating multi-bit quantization for
recurrent neural networks. In International Conference on Learning Representations (ICLR), 2018."
REFERENCES,0.586046511627907,"J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, and S. Mahlke. Scalpel: Customizing DNN
pruning to the underlying hardware parallelism. In Proceedings of the 44th Annual International
Symposium on Computer Architecture, pages 548–560, 2017."
REFERENCES,0.5883720930232558,"A. Zhou, Y. Ma, J. Zhu, J. Liu, Z. Zhang, K. Yuan, W. Sun, and H. Li. Learning n:m ﬁne-grained
structured sparse neural networks from scratch. In International Conference on Learning Repre-
sentations, 2021. URL https://openreview.net/forum?id=K9bw7vqp_s."
REFERENCES,0.5906976744186047,"X. Zhou, Z. Du, Q. Guo, S. Liu, C. Liu, C. Wang, X. Zhou, L. Li, T. Chen, and Y. Chen. Cambricon-s:
Addressing irregularity in sparse neural networks through a cooperative software/hardware ap-
proach. In 2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),
pages 15–28. IEEE, 2018."
REFERENCES,0.5930232558139535,"M. Zhu and S. Gupta. To prune, or not to prune: exploring the efﬁcacy of pruning for model
compression. CoRR, abs/1710.01878, 2017."
REFERENCES,0.5953488372093023,"J. Ziv and A. Lempel. Compression of individual sequences via variable-rate coding. IEEE Transac-
tions on Information Theory, 24(5):530–536, 2006."
REFERENCES,0.5976744186046512,Published as a conference paper at ICLR 2022
REFERENCES,0.6,"A
MEMORY BANDWIDTH WITH FIXED-TO-VARIABLE SPARSITY FORMAT"
REFERENCES,0.6023255813953489,"Memory bandwidth is expressed as the access rate (usually in units of bytes/second) at which the data
can be read from or written into memory. While the number of memory transactions can be reduced
a lot by pruning weights, the utilization of memory bandwidth is affected by the variability of the
length of data to be accessed during one transaction (Yu et al., 2017). In the case of ﬁxed-to-ﬁxed
sparsity representation, since all encoded blocks have the same size, full memory bandwidth is
utilized. On the other hand, in the case of ﬁxed-to-variable weight representation, encoded blocks
have variable sizes such that memory bandwidth can be wasted. To be more speciﬁc, let nw, nb, and
S be the number of weights to be encoded, the number of unpruned weights in a block, and sparsity,
respectively. Assuming that pruning a weight is a Bernoulli trial, for CSR format, we obtain
E[nb] = nw × (1 −S)
(3)
Var[nb] = nw × S × (1 −S).
(4)
Thus, the coefﬁcient of variation (or relative standard deviation) is given as
p"
REFERENCES,0.6046511627906976,"Var[nb]
E[nb]
=
1
√nw r"
REFERENCES,0.6069767441860465,"S
1 −S
(5)"
REFERENCES,0.6093023255813953,", which increases as S increases (0 < S < 1). In other words, for ﬁxed-to-variable sparsity format,
more memory bandwidth is wasted as more weights are pruned (as shown in Figure 1). Note that for
ﬁxed-to-ﬁxed sparsity format, we have Var[nb] = 0, and thus, the memory bandwidth is not wasted
at all."
REFERENCES,0.6116279069767442,"B
PERFORMANCE OF SPARSE MATRIX MULTIPLICATION USING CSR FORMAT"
REFERENCES,0.6139534883720931,"0.5
0.6
0.7
0.8
0.9
1.0
Pruning rate 0.0 1.0 2.0 3.0 4.0 5.0 6.0"
REFERENCES,0.6162790697674418,Execution Time (normalized) Dense
REFERENCES,0.6186046511627907,MKL(k=1)
REFERENCES,0.6209302325581395,MKL(k=16)
REFERENCES,0.6232558139534884,MKL(k=32)
REFERENCES,0.6255813953488372,CUDA(k=1)
REFERENCES,0.627906976744186,CUDA(k=16)
REFERENCES,0.6302325581395349,CUDA(k=32)
REFERENCES,0.6325581395348837,"Figure S.10: Normalized execution time of multiplying a (2048 × 2048) sparse matrix with a
(2048 × k) dense matrix."
REFERENCES,0.6348837209302326,"For inference of NNs, it is known that performance (including latency and throughput) is dominated
by matrix multiplications. Figure S.10 presents performance when a (2048 × 2048) sparse matrix (of
CSR format) is multiplied by a (2048 × k) dense matrix when k is usually small for inference with
small batch size. MKL library (operated by i7-7700 @ 3.6GHz) and CUDA 10.2 library (performed
by nVIDIA V100) perform sparse matrix multiplications whose execution times are normalized with
respect to corresponding dense matrix multiplications (i.e., using a dense (2048 × 2048) matrix).
From Figure S.10, it should be noted that even with a high compression ratio (due to high pruning
rate), if CSR format is adopted, sparse matrix multiplications can be even slower than dense matrix
multiplication. Thus, proposing a regular format after ﬁne-grained pruning is critical for parallel
computing to achieve performance gain by pruning."
REFERENCES,0.6372093023255814,"C
RELATED WORKS"
REFERENCES,0.6395348837209303,"This section describes the previous works regarding how to represent sparsity after ﬁne-grained
weight pruning. As mentioned in Section 1, high sparsity and compression ratio can be expected"
REFERENCES,0.641860465116279,Published as a conference paper at ICLR 2022
REFERENCES,0.6441860465116279,"by ﬁne-grained (unstructured) pruning, which prunes individual weights at randomly distributed
locations."
REFERENCES,0.6465116279069767,"To represent sparse weights with a reduced memory footprint, recording unpruned weights associated
with (binary) masking information may be the simplest way. However, for such a case, a reconstruction
process is required right before matrix multiplications during inference and it is challenging to be
parallelizable due to its random locations (i.e., the number of pruned weights to be processed by
a computation unit should highly vary as mentioned in Appendix A). As efforts to save memory
footprints with ﬁne-grained sparsity, by large, there have been two lines of researches, namely, ﬁxed-
to-variable and ﬁxed-to-ﬁxed formats. The Compressed Sparse Row (CSR) format is a well-known
example of ﬁxed-to-variable formats. Since DeepCompression (Han et al., 2016b) utilized the CSR
format for representing sparse neural networks (NNs), most of the acceleration library kernels and
computing systems have supported CSR data formats along with related APIs for CSR format (e.g.
cusparse library in CUDA). Based on recording unpruned values and relational indices row-wise,
the CSR format can lead to increased parallelism for computing sparse neural networks. However,
because the number of unpruned weights in each row still varies, CSR inherently leads to irregular
memory access patterns. SpMM or SpMV operations based on CSR format (even when including
block CSR or advanced hardware design for CSR (Han et al., 2016a)) cannot still achieve full (or
high) memory bandwidth utilization of computing systems, as numerous studies have raised related
issues (Wen et al., 2016; Yu et al., 2017; Ahn et al., 2019; Zhou et al., 2018; Kwon et al., 2020; Gale
et al., 2020) (we also reported similar concerns in Appendix B). Despite a highly reduced amount of
parameters to be stored, pruned networks in an unstructural manner have not been fully employed by
the currently available commercialized computing systems."
REFERENCES,0.6488372093023256,"Fixed-to-ﬁxed data compression, on the other hand, can achieve fully-parallelizable computations
along with memory-saving formats and higher memory bandwidth because the length of compressed
data is supposed to be ideally equal among any subsets of compressed data. By adopting compression
approaches that have been widely used in well-established engineering areas (e.g., digital communi-
cation, VLSI testing, and so on), Viterbi-based compression (Ahn et al., 2019) and XOR-gates-based
compression (Kwon et al., 2020) have been proposed. Note that despite regular memory access
patterns, compression methods of Ahn et al. (2019) require a heavy re-training process to acquire
proper weight bits and masks while presenting the limitation that the compression ratio should be
ﬁxed to be integer values. As for the work of (Kwon et al., 2020), the encoding efﬁciency of previous
XOR-gates-based compression is signiﬁcantly degraded due to combinational encoding algorithm
and inefﬁciency of patch systems."
REFERENCES,0.6511627906976745,"Our proposed work can be regarded as an extended study of XOR-gates-based compression: 1)
this paper veriﬁes that typical data representations (e.g., FP32 and INT8) for DNNs can also be
compressed by the XOR-gate decoder (additionally, the inverting technique can boost the encoding
efﬁciency), 2) By using the proposed sequential encoding/decoding schemes, encoding efﬁciency
E can closely approach the theoretical upper bound of ﬁxed-to-ﬁxed sparsity formats, and 3) Our
encoding algorithm can explore encoded bits according to sequential decoding within the limited
size of XOR-gate decoder while Kwon et al. (2020) proposed a simple heuristic algorithm assuming
combinational decoding only (i.e., Ns = 0)."
REFERENCES,0.6534883720930232,"D
FUNDAMENTAL LIMITS OF COMPRESSION"
REFERENCES,0.6558139534883721,"We are interested in the upper bound of compression ratio that can be analyzed by entropy (while
allowing ﬁxed-to-variable compression). Then, when we suggest a ﬁxed-to-ﬁxed compression scheme,
we can estimate how close the compression capability of a ﬁxed-to-ﬁxed scheme is to the maximum."
REFERENCES,0.6581395348837209,"Entropy presents the minimum average number of bits to represent an event when a probability
distribution of those events is provided (Morelos-Zaragoza, 2006). To investigate the entropy of
pruned weight blocks (and the maximum compression ratio correspondingly), a block of bits is
assigned to a symbol such that a probability distribution of symbols minimizes the entropy. In
other words, symbol assignment is designed to minimize the average number of bits (i.e., entropy to
represent symbols) that is given as H = − n
X"
REFERENCES,0.6604651162790698,"i=1
pi · log2 pi,
(6)"
REFERENCES,0.6627906976744186,Published as a conference paper at ICLR 2022
REFERENCES,0.6651162790697674,where n is the total number of symbols and pi is the occurrence probability of each symbol.
REFERENCES,0.6674418604651163,"Symbol assignment
We concatenate all of k-th bits of weights into a group and produce nw groups
when nw is the number of bits to represent a weight (e.g., nw is 32 for single-precision and 8 for
INT8) and 1 ≤k ≤nw. Symbol assignment is performed in each group independently without
referring to other groups. Suppose that a certain block from one of the groups is given as {01}
and corresponding mask bits (that are shared by all nw groups) are {1001} (0 means masking). By
ﬁlling up  with 0 or 1, {01} is selected to be assigned to one of 4 symbols (i.e., {0001}, {0011},
{0101} or {0111}) while such a symbol selection decides entropy. In the following two examples
(where nb=4), we illustrate the symbol assignment method that minimizes the entropy."
REFERENCES,0.6697674418604651,"Entropy (nu = 1, e.g., {0}, {1})
In this case, every block (of nb=4 and nu=1) can be
assigned to either one of two symbols ({0000}, {1111}). Since P(0000) = P(1111) = 0.5, from
Eq. 6, H = −(0.5 × (−1) + 0.5 × (−1)) = 1. Thus, a block of nb=4 and nu=1 can be compressed
into 1 bit that indicates one of two symbols. There are many other sets of two symbols to meet H = 1,
such as ({0010}, {1101}) and ({1010}, {0101})."
REFERENCES,0.672093023255814,"Entropy (nu = 2, e.g., {01}, {10})
A set of symbols should meet the following require-
ment: if we choose random n1 and random n2 (1 ≤n1, n2 ≤nb, n1 ̸= n2) and collect {n1-th bit,
n2-th bit} of each symbol, then, each of {00, 01, 10, 11} should appear in the collection. Under such
a constraint, after a careful investigation using a full search, the minimum number of symbols is 5 to
represent all blocks of nb=4 and nu=2. An example set of 5 symbols with corresponding occurrence
probability to minimize entropy is as follows: P(0000) = 6/24, P(1110) = 6/24, P(0101) = 5/24,
P(1001) = 4/24, and P(0011) = 3/24. Then, from Eq. 6, H is approximately 2.28 (bits) attainable
through ﬁxed-to-variable compression. On the other hand, for a ﬁxed-to-ﬁxed scheme, a block (of
nb=4 and nu=2) is compressed into 3 bits to represent one of 5 symbols."
REFERENCES,0.6744186046511628,"For nu=3, the minimum number of symbols is 8 and a block can be compressed into 3 bits. All in all,
when nu is ﬁxed across blocks, H can be equal to or slightly higher than nu."
REFERENCES,0.6767441860465117,Published as a conference paper at ICLR 2022
REFERENCES,0.6790697674418604,"E
ENCODING ALGORITHM"
REFERENCES,0.6813953488372093,"In this section, we describe our encoding algorithm based on dynamic programming algorithm that
can minimize the number of unmatched bits. Our encoding algorithm presents time-complexity as
O(l · 2Nin·(Ns+1)) and space-complexity as O(2Nin·Ns). Thus, even though increasing Nin and
Ns enhances E, (Nin × Ns) is empirically limited to be less than 26 under the constraint of 32GB
memory of a single GPU. For each binary weight W b
i (0 ≤i ≤nw), the function ENCODING
generates the encoded vectors wi,(1..l+Ns). XOR-gate decoder (M ⊕) is pre-determined and ﬁxed
for inference. Note that the number of encoded vectors (we
1..(l+Ns)) is l + Ns, not l. we
1 and we
2 are
pre-determined as BIN(0) and used for encoding the ﬁrst binary weight vector."
REFERENCES,0.6837209302325581,Published as a conference paper at ICLR 2022
REFERENCES,0.686046511627907,"Algorithm 3: Encoding algorithm when Ns = 2. For a binary matrix W b
i , the ENCODING
function generates encoded bit vectors. For varied Ns, the number of for-loop statements for it
(e.g. line 34-36) and the dimensions of arrays (dp and path) are changed to Ns + 1."
REFERENCES,0.6883720930232559,"Parameters W b
i is a binary weight vector to be encoded. MASK is pruning mask information for W b
i .
dp is (Ns+1)-dimensional array. dp[t][a][b] stores the minimum number of error bits when BIN(a) and
BIN(b) are ﬁxed as we
t and we
t−1. path is (Ns+1)-dimensional array for history.
Additional Functions SIZE(W ) returns the number of parameters in W . RESHAPE(ar, shape) returns
same data (ar) with speciﬁed shape, shape. INIT(ar, init) sets a value init to all elements in ar. BIN(dec)
returns a binary value of dec. DEC(bin) returns a decimal value of bin."
REFERENCES,0.6906976744186046,"Function ERR_NUM(x, y, mask):"
REFERENCES,0.6930232558139535,"nErr ←0
for i ←0 to Nout −1 do"
REFERENCES,0.6953488372093023,if mask[i] ̸= 0 and x[i] ̸= y[i] then
REFERENCES,0.6976744186046512,nErr ←nErr + 1
REFERENCES,0.7,return nErr
REFERENCES,0.7023255813953488,"Function ENCODING(W b
i , MASK):
l ←SIZE(W b
i )/Nout
data ←RESHAPE(W b
i , [l, Nout]) ▷Slicing
mask ←RESHAPE(MASK, [l, Nout]) ▷Slicing
INIT (dp, INF) ▷Initialize for starting point
dp[Ns][0][0] ←0 and we
1, we
2 ←BIN(0), BIN(0)"
REFERENCES,0.7046511627906977,"▷Find the minimum number of errors
for t ←Ns+1 to l+Ns do"
REFERENCES,0.7069767441860465,for it ←0 to 2Nin−1 do
REFERENCES,0.7093023255813954,for it−1 ←0 to 2Nin−1 do
REFERENCES,0.7116279069767442,for it−2 ←0 to 2Nin−1 do
REFERENCES,0.713953488372093,"out ←M ⊕(BIN(it−2)⌢BIN(it−1)⌢BIN(it))
nerr←ERR_NUM(out, data[t], mask[t])
if dp[t][it][it−1] > nerr + dp[t−1][it−1][it−2] then"
REFERENCES,0.7162790697674418,"dp[t][it][it−1] ←nerr + dp[t −1][it−1][it−2]
path[t][it][it−1] ←BIN(it−2)"
REFERENCES,0.7186046511627907,"▷Find the last two(=Ns) encoded vectors
minerr ←INF.
for it = 0 to 2Nin −1 do"
REFERENCES,0.7209302325581395,for it−1 = 0 to 2Nin −1 do
REFERENCES,0.7232558139534884,if minerr > dp[l + Ns][it][it−1] then
REFERENCES,0.7255813953488373,"minerr ←dp[l + Ns][it][it−1]
we
l+2, we
l+1 ←BIN(it−1),BIN(it)"
REFERENCES,0.727906976744186,"▷Get encoded bits by following history array
for t ←l to 2Ns + 1 by −1 do"
REFERENCES,0.7302325581395349,"we
t ←path[t + Ns][DEC(we
t+2)][DEC(we
t+1)]
return {we
1, we
2, ..., we
l+Ns}"
REFERENCES,0.7325581395348837,Published as a conference paper at ICLR 2022
REFERENCES,0.7348837209302326,"F
MEMORY REDUCTION WITH LOSSLESS COMPRESSION"
REFERENCES,0.7372093023255814,Decoding
REFERENCES,0.7395348837209302,"Body for 
Correction"
REFERENCES,0.7418604651162791,(length = 512)
REFERENCES,0.7441860465116279,Reshape Mask ...
REFERENCES,0.7465116279069768,Correction 1
REFERENCES,0.7488372093023256,"idx
1
0"
REFERENCES,0.7511627906976744,0 1 0 0 ... 1 0 1 0
REFERENCES,0.7534883720930232,- Flag bits for each block (k bits)
REFERENCES,0.7558139534883721,"1 2 3 4 5
k
... 1"
REFERENCES,0.7581395348837209,"idx
0
idx
3
...
...
0
idx
k-3"
REFERENCES,0.7604651162790698,"idx
1
0
idx
idx
1
k-1"
REFERENCES,0.7627906976744186,"- Location information to be flipped
  ( (log2512+1)x(# of unmatched bits) )"
REFERENCES,0.7651162790697674,- Encoded bit vectors (Nout x l bits)
REFERENCES,0.7674418604651163,"Figure S.11: Correction process for lossless compression. After the encoded (compressed) bit vectors
are decoded, unmatched bits (that encoding could not target successfully) are ﬂipped by correction
information that records the locations of unmatched weight bits."
REFERENCES,0.7697674418604651,"For lossless compression, the unmatched bits (error bits) should be corrected right after decoding
procedures. Since the random number generator produces 0 or 1, the unmatched bits can be simply
corrected by ﬂipping."
REFERENCES,0.772093023255814,"To compute the memory reduction, we suggest block-wise correction logic to be conducted by
ﬂipping error bits in a p-length vector while each error location is given as a bit position inside a
vector. As depicted in Figure S.11, when the block-wise correction is performed while a block size
has p-length, the decoded vectors wb′
1..l are reshaped to wrb′
1..k(k = ⌈mn"
REFERENCES,0.7744186046511627,p ⌉) and each reshaped vector
REFERENCES,0.7767441860465116,"wrb′ is corrected by corresponding error bit locations indicating which bit is to be ﬂipped inside a
block (of p-length)."
REFERENCES,0.7790697674418605,The amount of compressed bits can be computed as
REFERENCES,0.7813953488372093,Nin · ⌈mn
REFERENCES,0.7837209302325582,"Nout
⌉+ ⌈mn"
REFERENCES,0.786046511627907,"p ⌉+ (log2 p + 1) × (# of unmatched bits).
(7)"
REFERENCES,0.7883720930232558,"The ﬁrst term is the number of bits of we
1..l as the compression results by sequential encoding. The
second term is the number of ﬂag bits while each ﬂag bit indicates whether a non-zero number of
unmatched bits exists in each p-length vector (as E approaches 1, there are many p-length vectors
that skip the correction step)). The third term includes locations of unmatched bits (inside a p-length
block) to be ﬂipped and an additional one bit to specify the end of the streaming error bit locations
(i.e., ‘1’ means the following (log2 p) bits contains the next correction information of the same block).
Thus, each block of p-length involves (log2 512 + 1)×(# of unmatched bits)) for error bit locations."
REFERENCES,0.7906976744186046,Published as a conference paper at ICLR 2022
REFERENCES,0.7930232558139535,"G
DESIGN COMPLEXITY OF THE PROPOSED COMPRESSION METHOD"
REFERENCES,0.7953488372093023,"Encoding Process
The time and space complexity of the encoding process (presented in Appendix
E with corresponding algorithm description) is independent of inference performance since encoding
is performed ofﬂine (thus, GPUs or CPUs would be ﬁne to run the encoding algorithm). Encoding
algorithm based on a dynamic programming technique as shown in Appendix E has the following
space and time complexity."
REFERENCES,0.7976744186046512,• Time complexity: O(l · 2Nin·(Ns+1))
REFERENCES,0.8,• Space complexity: O(2Nin·Ns)
REFERENCES,0.8023255813953488,"Algorithm 3 shown in Appendix E explores all possible 2((l+Ns)·Nin) outputs of an XOR-gate decoder
to produce an input vector that can minimize the number of unmatched bits. Note that a partial string
of input (of an XOR-gate network) having 2(Nin·(Ns+1)) bits share 2Nin·Ns bits continuously through
shift registers. As such, for the time index t + 1, search space (having the size of 2((t+1+Ns)·Nin))
is overlapped with the search space of the time index t (having the size of 2((t+Ns)·Nin)) as much
as 2t·Nin. Accordingly, the time complexity at the time index t + 1 (that optimize the input) is
reduced from O(2((t+1+Ns)·Nin)) to O(2((t+1+Ns)·Nin)/2(t·Nin)) = O(2((1+Ns)·Nin)). Then, since
we iterate such operation l times, the overall time complexity becomes O(l · 2((1+Ns)·Nin)). To
exploit sharing computations between different time indices, we need to store intermediate results
having the size of 2Ns×Nin which becomes the space complexity of Algorithm 3."
REFERENCES,0.8046511627906977,"Decoding Process
As for decoding operations, we suggest that the decoding algorithm is best
supported by a hardware design consisting of XOR gates (and a few shift registers). Hence, in this
section, let us speciﬁcally argue hardware design issues of XOR-gate decoders."
REFERENCES,0.8069767441860465,"• The strongest beneﬁt of employing digital circuits (in the form of ASICs or FPGAs) to
implement XOR gates is that all XOR gates can be performed simultaneously (unlike
GPUs or CPUs where each core needs to simulate only a few XOR gates). Thus, all XOR
operations of our proposed decoder are completed within just one clock cycle."
REFERENCES,0.8093023255813954,"• Ns (with shift registers) would increase the latency. Throughput, however, maintains to be
the same regardless of Ns through pipelining technique which is a basic hardware design
principle."
REFERENCES,0.8116279069767441,"• Overall, the design complexity (in terms of area overhead and latency) of XOR-gate decoders
is extremely low (note that one XOR gate consumes only 6 transistors)."
REFERENCES,0.813953488372093,"• XOR-gate decoders would work as memory decompressors (located in-between memory
and computation logic). In the view of computational units that receive outputs of an
XOR-gate decoder, then, the amount of memory is simply reduced while regular memory
access patterns are not disturbed."
REFERENCES,0.8162790697674419,"• Given Ns, an XOR-gate decoder requires Ns additional clock cycles for the latency."
REFERENCES,0.8186046511627907,"• Since M matrix has the size of Nout × Nin and an element of M is randomly ﬁlled with 0
or 1, the number of XOR gates is (Nout · Nin/2). Thus, the total number of transistors to
design an XOR-gate decoder is (3 · Nout · Nin)."
REFERENCES,0.8209302325581396,"• Overall, we can provide full memory bandwidth (based on regular memory access patterns
through ﬁxed-to-ﬁxed sparsity formats) while the overall hardware design cost is only
marginal."
REFERENCES,0.8232558139534883,"• While designing DNN inference accelerators is gaining increasing attention, our work can
provide a new research direction."
REFERENCES,0.8255813953488372,Published as a conference paper at ICLR 2022
REFERENCES,0.827906976744186,"H
ADDITIONAL ANALYSIS ON EXPERIMENTS"
REFERENCES,0.8302325581395349,"In this section, we provide additional analysis and results for experiments in Section 6.2."
REFERENCES,0.8325581395348837,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
bit index (1:sign bit, 2-9: exponent bits, 10-32: fraction bits)"
REFERENCES,0.8348837209302326,"0.00
0.25
0.50
0.75
1.00
(a) Transformer (FP32, Magnitude-based Pruning)"
REFERENCES,0.8372093023255814,"1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
bit index (1:sign bit, 2-9: exponent bits, 10-32: fraction bits)"
REFERENCES,0.8395348837209302,"0.00
0.25
0.50
0.75
1.00
(b) ResNet-50 (FP32, Magnitude-based Pruning)"
REFERENCES,0.8418604651162791,"1
2
3
4
5
6
7
8
bit index (1:sign bit, 2-8: magnitude bits)"
REFERENCES,0.8441860465116279,"0.00
0.25
0.50
0.75
1.00
(c) ResNet-50 (INT8, Magnitude-based Pruning)"
REFERENCES,0.8465116279069768,"Figure S.12: Ratio of zeros when weights are divided into k groups when k is the bit-index (i.e., for
FP32 number format, k=1 means a sign bit and k=32 means the least signiﬁcant bit in mantissa). For
models, Transformer(FP32), ResNet-50(FP32), and ResNet-50(INT8) are investigated. Most weights
consist of similar amounts of 0s and 1s. For the exponent bits of FP32 models, there are noticeable
skewed ratios of zeros (e.g. while most of the 2nd bits in Transformer are zero, most of 3rd, 4th, and
5th bits are ones.) because the range of exponent bits is limited according to characteristics of DNN
models (e.g. regularization such as weight decay). We can gain additional efﬁciency by adopting the
inverting technique for the FP32 models."
REFERENCES,0.8488372093023255,Published as a conference paper at ICLR 2022
REFERENCES,0.8511627906976744,"Table S.4: Coefﬁcient of variation of nu and E of selected layers of the Transformer pruned by random, magnitude-based, L0 regularization, or variational dropout
pruning method."
REFERENCES,0.8534883720930233,"Nin, Nout
SHAPE
S
LAYER
NAME
PRUNING
METHOD
COEFF
E (%)"
REFERENCES,0.8558139534883721,"Ns=0
Ns=1
Ns=2"
REFERENCES,0.858139534883721,"(8, 26)"
REFERENCES,0.8604651162790697,"(512, 512)
0.700
DEC3/SELF_ATT/Q
RANDOM
0.341
93.7%
98.7%
99.5%"
REFERENCES,0.8627906976744186,"(2048, 512)
0.700
DEC3/FFN2
RANDOM
0.343
93.7%
98.7%
99.5%"
REFERENCES,0.8651162790697674,"(512, 512)
0.700
DEC3/SELF_ATT/Q
MAGNITUDE
0.390
93.4%
98.2%
99.0%"
REFERENCES,0.8674418604651163,"(2048, 512)
0.700
DEC3/FFN2
MAGNITUDE
0.363
93.6%
98.5%
99.2%"
REFERENCES,0.8697674418604651,"(512, 512)
0.699
DEC3/SELF_ATT/Q
L0 REG.
0.476
92.7%
97.6%
98.7%"
REFERENCES,0.872093023255814,"(512, 512)
0.698
DEC3/FFN2
L0 REG.
0.467
92.7%
97.7%
98.8%"
REFERENCES,0.8744186046511628,"(512, 512)
0.705
DEC5/SELF_ATT/K
VAR. DROPOUT
0.303
94.5%
99.1%
99.7%"
REFERENCES,0.8767441860465116,"(2048, 512)
0.697
DEC1/FFN1
VAR. DROPOUT
0.309
94.5%
99.1%
99.7%"
REFERENCES,0.8790697674418605,"(8, 80)"
REFERENCES,0.8813953488372093,"(512, 512)
0.900
ENC2/SELF_ATT/OUTPUT
RANDOM
0.349
94.4%
98.6%
99.3%"
REFERENCES,0.8837209302325582,"(2048, 512)
0.900
DEC5/FFN2
RANDOM
0.315
94.6%
98.9%
99.5%"
REFERENCES,0.8860465116279069,"(512, 512)
0.900
ENC2/SELF_ATT/OUTPUT
MAGNITUDE
0.516
92.5%
97.0%
98.0%"
REFERENCES,0.8883720930232558,"(2048, 512)
0.900
DEC5/FFN2
MAGNITUDE
0.363
93.7%
98.5%
99.1%"
REFERENCES,0.8906976744186047,"(512, 512)
0.904
ENC2/SELF_ATT/OUTPUT
L0 REG.
0.331
94.3%
98.7%
99.2%"
REFERENCES,0.8930232558139535,"(512, 512)
0.896
DEC5/FFN2
L0 REG.
0.347
94.5%
99.0%
99.6%"
REFERENCES,0.8953488372093024,"(512, 512)
0.906
DEC2/SELF_ATT/V
VAR. DROPOUT
0.770
89.4%
93.6%
94.3%"
REFERENCES,0.8976744186046511,"(2048, 512)
0.904
DEC4/FFN1
VAR. DROPOUT
0.499
91.8%
96.4%
97.4%"
REFERENCES,0.9,Published as a conference paper at ICLR 2022
REFERENCES,0.9023255813953488,"Table S.5: Coefﬁcient of variation of nu and E of selected layers of the ResNet-50 pruned by random, magnitude-based, or variational dropout pruning method."
REFERENCES,0.9046511627906977,"Nin, Nout
SHAPE
S
LAYER
NAME
PRUNING
METHOD
COEFF
E (%)"
REFERENCES,0.9069767441860465,"Ns=0
Ns=1
Ns=2"
REFERENCES,0.9093023255813953,"(8, 26)"
REFERENCES,0.9116279069767442,"(1,1,1024,256)
0.700
GROUP2_LAYER3_BN2
RANDOM
0.347
94.2%
98.6%
99.2%"
REFERENCES,0.913953488372093,"(1,1,256,1024)
0.700
GROUP3_LAYER5_BN3
RANDOM
0.334
94.3%
98.8%
99.5%"
REFERENCES,0.9162790697674419,"(1,1,1024,256)
0.700
GROUP2_LAYER3_BN2
MAGNITUDE
0.505
93.2%
98.0%
98.9%"
REFERENCES,0.9186046511627907,"(1,1,256,1024)
0.700
GROUP3_LAYER5_BN3
MAGNITUDE
0.428
93.8%
98.4%
99.0%"
REFERENCES,0.9209302325581395,"(3,3,512,512)
0.709
GROUP2_LAYER3_BN2
VAR. DROPOUT
0.403
94.2%
98.4%
99.1%"
REFERENCES,0.9232558139534883,"(1,1,256,1024)
0.706
GROUP3_LAYER5_BN3
VAR. DROPOUT
0.361
94.4%
98.6%
99.1%"
REFERENCES,0.9255813953488372,"(8, 80)"
REFERENCES,0.9279069767441861,"(3, 3, 256, 256)
0.900
GROUP3_LAYER3_BN2
RANDOM
0.683
92.3%
96.8%
98.0%"
REFERENCES,0.9302325581395349,"(1, 1, 512, 2048)
0.900
GROUP4_LAYER0_BN3
RANDOM
0.407
92.9%
97.4%
98.1%"
REFERENCES,0.9325581395348838,"(3, 3, 256, 256)
0.900
GROUP3_LAYER3_BN2
MAGNITUDE
0.303
94.6%
99.2%
99.8%"
REFERENCES,0.9348837209302325,"(1, 1, 512, 2048)
0.900
GROUP4_LAYER0_BN3
MAGNITUDE
0.299
94.6%
99.2%
99.8%"
REFERENCES,0.9372093023255814,"(3, 3, 256, 256)
0.913
GROUP3_LAYER3_BN2
VAR. DROPOUT
0.366
94.1%
98.3%
98.9%"
REFERENCES,0.9395348837209302,"(1, 1, 512, 2048)
0.896
GROUP4_LAYER0_BN3
VAR. DROPOUT
0.324
94.5%
98.9%
99.6%"
REFERENCES,0.9418604651162791,Published as a conference paper at ICLR 2022
REFERENCES,0.9441860465116279,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100
(a) Ns = 0"
REFERENCES,0.9465116279069767,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100
(b) Ns = 0, Inverted"
REFERENCES,0.9488372093023256,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100 E(%)"
REFERENCES,0.9511627906976744,(c) Ns = 1
REFERENCES,0.9534883720930233,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100
(d) Ns = 1, Inverted"
REFERENCES,0.9558139534883721,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
bit index (1:sign bit, 2-9: exponent bits, 10-32: fraction bits)"
REFERENCES,0.958139534883721,"90
92
94
96
98
100
(e) Ns = 2"
REFERENCES,0.9604651162790697,"(a) Transformer (FP32, Magnitude-based Pruning)"
REFERENCES,0.9627906976744186,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100
(a) Ns = 0"
REFERENCES,0.9651162790697675,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100
(b) Ns = 0, Inverted"
REFERENCES,0.9674418604651163,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100 E(%)"
REFERENCES,0.9697674418604652,(c) Ns = 1
REFERENCES,0.9720930232558139,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
90
92
94
96
98
100
(d) Ns = 1, Inverted"
REFERENCES,0.9744186046511628,"1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
bit index (1:sign bit, 2-9: exponent bits, 10-32: fraction bits)"
REFERENCES,0.9767441860465116,"90
92
94
96
98
100
(e) Ns = 2"
REFERENCES,0.9790697674418605,"(b) ResNet-50 (FP32, Magnitude-based Pruning)"
REFERENCES,0.9813953488372092,"1
2
3
4
5
6
7
8
90 92 94 96 98"
REFERENCES,0.9837209302325581,"100
(a) Ns = 0"
REFERENCES,0.986046511627907,"1
2
3
4
5
6
7
8
90 92 94 96 98 100 E(%)"
REFERENCES,0.9883720930232558,(b)Ns = 1
REFERENCES,0.9906976744186047,"1
2
3
4
5
6
7
8
bit index (1:sign bit, 2-8: magnitude bits) 90 92 94 96 98"
REFERENCES,0.9930232558139535,"100
(c)Ns = 2"
REFERENCES,0.9953488372093023,"(c) ResNet-50 (INT8, Magnitude-based Pruning)"
REFERENCES,0.9976744186046511,"Figure S.13: E of the Transformer and ResNet-50 (pruned by S=70%) measured for each bit index
(≤32 for FP32) individually with various Ns while inverting technique is also considered . It can
be observed that the inverting technique improves E for Ns = 0 and Ns = 1. When Ns = 2, the
improvement on E is not noticeable."
