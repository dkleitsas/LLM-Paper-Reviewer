Section,Section Appearance Order,Paragraph
NATIONAL INSTITUTE OF INFORMATICS,0.0,"1National Institute of Informatics
2The Graduate University for Advanced Studies, SOKENDAI
{kanoh, mahito}@nii.ac.jp"
ABSTRACT,0.001017293997965412,ABSTRACT
ABSTRACT,0.002034587995930824,"In practical situations, the tree ensemble is one of the most popular models along
with neural networks. A soft tree is a variant of a decision tree. Instead of using a
greedy method for searching splitting rules, the soft tree is trained using a gradient
method in which the entire splitting operation is formulated in a differentiable form.
Although ensembles of such soft trees have been used increasingly in recent years,
little theoretical work has been done to understand their behavior. By considering
an ensemble of inﬁnite soft trees, this paper introduces and studies the Tree Neural
Tangent Kernel (TNTK), which provides new insights into the behavior of the
inﬁnite ensemble of soft trees. Using the TNTK, we theoretically identify several
non-trivial properties, such as global convergence of the training, the equivalence
of the oblivious tree structure, and the degeneracy of the TNTK induced by the
deepening of the trees."
INTRODUCTION,0.003051881993896236,"1
INTRODUCTION"
INTRODUCTION,0.004069175991861648,"Tree ensembles and neural networks are powerful machine learning models that are used in various
real-world applications. A soft tree ensemble is one variant of tree ensemble models that inherits
characteristics of neural networks. Instead of using a greedy method (Quinlan, 1986; Breiman et al.,
1984) to search splitting rules, the soft tree makes the splitting rules soft and updates the entire model’s
parameters simultaneously using the gradient method. Soft tree ensemble models are known to have
high empirical performance (Kontschieder et al., 2015; Popov et al., 2020; Hazimeh et al., 2020),
especially for tabular datasets. Apart from accuracy, there are many reasons why one should formulate
trees in a soft manner. For example, unlike hard decision trees, soft tree models can be updated
sequentially (Ke et al., 2019) and trained in combination with pre-training (Arik & Pﬁster, 2019),
resulting in characteristics that are favorable in terms of real-world continuous service deployment.
Their model interpretability, induced by the hierarchical splitting structure, has also attracted much
attention (Frosst & Hinton, 2017; Wan et al., 2021; Tanno et al., 2019). In addition, the idea of the
soft tree is implicitly used in many different places; for example, the process of allocating data to the
appropriate leaves can be interpreted as a special case of Mixture-of-Experts (Jordan & Jacobs, 1993;
Shazeer et al., 2017; Lepikhin et al., 2021), a technique for balancing computational complexity and
prediction performance."
INTRODUCTION,0.00508646998982706,"Although various techniques have been proposed to train trees, the theoretical validity of such
techniques is not well understood at sufﬁcient depth. Examples of the practical technique include
constraints on individual trees using parameter sharing (Popov et al., 2020), adjusting the hardness
of the splitting operation (Frosst & Hinton, 2017; Hazimeh et al., 2020), and the use of overparame-
terization (Belkin et al., 2019; Karthikeyan et al., 2021). To better understand the training of tree
ensemble models, we focus on the Neural Tangent Kernel (NTK) (Jacot et al., 2018), a powerful tool
that has been successfully applied to various neural network models with inﬁnite hidden layer nodes.
Every model architecture is known to produce a distinct NTK. Not only for the multi-layer perceptron
(MLP), many studies have been conducted across various models, such as for Convolutional Neural
Networks (CNTK) (Arora et al., 2019; Li et al., 2019), Graph Neural Networks (GNTK) (Du et al.,
2019b), and Recurrent Neural Networks (RNTK) (Alemohammad et al., 2021). Although a number
of ﬁndings have been obtained using the NTK, they are mainly for typical neural networks, and it is
still not clear how to apply the NTK theory to the tree models."
INTRODUCTION,0.006103763987792472,Published as a conference paper at ICLR 2022
INTRODUCTION,0.007121057985757884,"Figure 1: Schematics of an ensemble of M soft trees. Tree internal nodes are indexed according to
the breadth-ﬁrst ordering."
INTRODUCTION,0.008138351983723296,"In this paper, by considering the limit of inﬁnitely many trees, we introduce and study the neural
tangent kernel for tree ensembles, called the Tree Neural Tangent Kernel (TNTK), which provides
new insights into the behavior of the ensemble of soft trees. The goal of this research is to derive
the kernel that characterizes the training behavior of soft tree ensembles, and to obtain theoretical
support for the empirical techniques. Our contributions are summarized as follows:"
INTRODUCTION,0.009155645981688708,"• First extension of the NTK concept to the tree ensemble models. We derive the analytical form
for the TNTK at initialization induced by inﬁnitely many perfect binary trees with arbitrary depth
(Section 4.1.1). We also prove that the TNTK remains constant during the training of inﬁnite soft
trees, which allows us to analyze the behavior by kernel regression and discuss global convergence
of training using the positive deﬁniteness of the TNTK (Section 4.1.2, 4.1.3)."
INTRODUCTION,0.01017293997965412,"• Equivalence of the oblivious tree ensemble models. We show the TNTK induced by the oblivious
tree structure used in practical open-source libraries such as CatBoost (Prokhorenkova et al., 2018)
and NODE (Popov et al., 2020) converges to the same TNTK induced by a non-oblivious one in
the limit of inﬁnite trees. This observation implicitly supports the good empirical performance of
oblivious trees with parameter sharing between tree nodes (Section 4.2.1)."
INTRODUCTION,0.011190233977619531,"• Nonlinearity by adjusting the tree splitting operation. Practically, various functions have been
proposed to represent the tree splitting operation. The most basic function is sigmoid. We show
that the TNTK is almost a linear kernel in the basic case, and when we adjust the splitting function
hard, the TNTK becomes nonlinear (Section 4.2.2)."
INTRODUCTION,0.012207527975584944,"• Degeneracy of the TNTK with deep trees. The TNTK associated with deep trees exhibits
degeneracy: the TNTK values are almost identical for deep trees even if the inner products of
inputs are different. As a result, poor performance in numerical experiments is observed with the
TNTK induced by inﬁnitely many deep trees. This result supports the fact that the depth of trees is
usually not so large in practical situations (Section 4.2.3)."
INTRODUCTION,0.013224821973550356,"• Comparison to the NTK induced by the MLP. We investigate the generalization performance of
inﬁnite tree ensembles by kernel regression with the TNTK on 90 real-world datasets. Although
the MLP with inﬁnite width has better prediction accuracy on average, the inﬁnite tree ensemble
performs better than the inﬁnite width MLP in more than 30 percent of the datasets. We also
showed that the TNTK is superior to the MLP-induced NTK in computational speed (Section 5)."
BACKGROUND AND RELATED WORK,0.014242115971515769,"2
BACKGROUND AND RELATED WORK"
BACKGROUND AND RELATED WORK,0.015259409969481181,"Our main focus in this paper is the soft tree and the neural tangent kernel. We brieﬂy introduce and
review them."
SOFT TREE,0.01627670396744659,"2.1
SOFT TREE"
SOFT TREE,0.017293997965412006,"Based on Kontschieder et al. (2015), we formulate a regression by soft trees. Figure 1 is a schematic
image of an ensemble of M soft trees. We deﬁne a data matrix x ∈RF ×N for N training samples"
SOFT TREE,0.018311291963377416,Published as a conference paper at ICLR 2022
SOFT TREE,0.019328585961342827,"{x1, . . . , xN} with F features and deﬁne tree-wise parameter matrices for internal nodes wm ∈
RF ×N and leaf nodes πm ∈R1×L for each tree m ∈[M] = {1, . . . , M} as x ="
SOFT TREE,0.02034587995930824,"|
. . .
|
x1
. . .
xN
|
. . .
| !"
SOFT TREE,0.021363173957273652,",
wm ="
SOFT TREE,0.022380467955239063,"|
. . .
|
wm,1
. . .
wm,N
|
. . .
| !"
SOFT TREE,0.023397761953204477,",
πm = (πm,1, . . . , πm,L) ,"
SOFT TREE,0.024415055951169887,"where internal nodes (blue nodes in Figure 1) and leaf nodes (green nodes in Figure 1) are indexed
from 1 to N and 1 to L, respectively. N and L may change across trees in general, while we assume
that they are always ﬁxed for simplicity throughout the paper. We also write horizontal concatenation
of (column) vectors as x = (x1, . . . , xN) ∈RF ×N and wm = (wm,1, . . . , wm,N ) ∈RF ×N .
Unlike hard decision trees, we consider a model in which every single leaf node ℓ∈[L] = {1, . . . , L}
of a tree m holds the probability that data will reach to it. Therefore, the splitting operation at an
intermediate node n ∈[N] = {1, . . . , N} does not deﬁnitively decide splitting to the left or right.
To provide an explicit form of the probabilistic tree splitting operation, we introduce the following
binary relations that depend on the tree’s structure: ℓ↙n (resp. n ↘ℓ), which is true if a leaf
ℓbelongs to the left (resp. right) subtree of a node n and false otherwise. We can now exploit
µm,ℓ(xi, wm) : RF × RF ×N →[0, 1], a function that returns the probability that a sample xi will
reach a leaf ℓof the tree m, as follows:"
SOFT TREE,0.0254323499491353,"µm,ℓ(xi, wm) = N
Y"
SOFT TREE,0.026449643947100712,"n=1
gm,n(xi, wm,n)1ℓ↙n (1 −gm,n(xi, wm,n))1n↘ℓ,
(1)"
SOFT TREE,0.027466937945066123,"where 1Q is an indicator function conditioned on the argument Q, i.e., 1true = 1 and 1false = 0, and
gm,n : RF × RF →[0, 1] is a decision function at each internal node n of a tree m. To approximate
decision tree splitting, the output of the decision function gm,n should be between 0.0 and 1.0. If
the output of a decision function takes only 0.0 or 1.0, the splitting operation is equivalent to hard
splitting used in typical decision trees. We will deﬁne an explicit form of the decision function gm,n
in Equation (5) in the next section."
SOFT TREE,0.028484231943031537,The prediction for each xi from a tree m with nodes parameterized by wm and πm is given by
SOFT TREE,0.029501525940996948,"fm(xi, wm, πm) = L
X"
SOFT TREE,0.030518819938962362,"ℓ=1
πm,ℓµm,ℓ(xi, wm),
(2)"
SOFT TREE,0.03153611393692777,"where fm : RF × RF ×N × R1×L →R, and πm,ℓdenotes the response of a leaf ℓof the tree m.
This formulation means that the prediction output is the average of the leaf values πm,ℓweighted by
µm,ℓ(xi, wm), probability of assigning the sample xi to the leaf ℓ. If µm,ℓ(xi, wm) takes only 1.0
for one leaf and 0.0 for the other leaves, the behavior is equivalent to a typical decision tree prediction.
In this model, wm and πm are updated during training with a gradient method."
SOFT TREE,0.03255340793489318,"While many empirical successes have been reported, theoretical analysis for soft tree ensemble
models has not been sufﬁciently developed."
NEURAL TANGENT KERNEL,0.0335707019328586,"2.2
NEURAL TANGENT KERNEL"
NEURAL TANGENT KERNEL,0.03458799593082401,"Given N samples x ∈RF ×N, the NTK induced by any model architecture at a training time τ is
formulated as a matrix c
H∗
τ ∈RN×N, in which each (i, j) ∈[N] × [N] component is deﬁned as"
NEURAL TANGENT KERNEL,0.03560528992878942,"[c
H∗
τ ]ij := bΘ∗
τ(xi, xj) :=
∂farbitrary (xi, θτ)"
NEURAL TANGENT KERNEL,0.03662258392675483,"∂θτ
, ∂farbitrary (xj, θτ) ∂θτ ,
(3)"
NEURAL TANGENT KERNEL,0.03763987792472025,"where ⟨·, ·⟩denotes the inner product and θτ ∈RP is a concatenated vector of all the P trainable
model parameters at τ. An asterisk “ ∗” indicates that the model is arbitrary. The model func-
tion farbitrary : RF × RP →R used in Equation (3) is expected to be applicable to a variety of
model structures. For the soft tree ensembles introduced in Section 2.1, the NTK is formulated as
PM
m=1
PN
n=1
D
∂f(xi,w,π)"
NEURAL TANGENT KERNEL,0.038657171922685654,"∂wm,n
, ∂f(xj,w,π) ∂wm,n"
NEURAL TANGENT KERNEL,0.03967446592065107,"E
+ PM
m=1
PL
ℓ=1
D
∂f(xi,w,π)"
NEURAL TANGENT KERNEL,0.04069175991861648,"∂πm,ℓ
, ∂f(xj,w,π) ∂πm,ℓ E
."
NEURAL TANGENT KERNEL,0.04170905391658189,"Within the limit of inﬁnite width with a proper parameter scaling, a variety of properties have
been discovered from the NTK induced by the MLP. For example, Jacot et al. (2018) showed the
convergence of bΘMLP
0
(xi, xj), which can vary with respect to parameters, to the unique limiting"
NEURAL TANGENT KERNEL,0.042726347914547304,Published as a conference paper at ICLR 2022
NEURAL TANGENT KERNEL,0.04374364191251272,"kernel ΘMLP(xi, xj) at initialization in probability. Moreover, they also showed that the limiting
kernel does not change during training in probability:"
NEURAL TANGENT KERNEL,0.044760935910478125,"lim
width→∞
bΘMLP
τ
(xi, xj) =
lim
width→∞
bΘMLP
0
(xi, xj) = ΘMLP(xi, xj).
(4)"
NEURAL TANGENT KERNEL,0.04577822990844354,"This property helps in the analytical understanding of the model behavior. For example, with the
squared loss and inﬁnitesimal step size with learning rate η, the training dynamics of gradient ﬂow in
function space coincides with kernel ridge-less regression with the limiting NTK. Such a property
gives us a data-dependent generalization bound (Bartlett & Mendelson, 2003) related to the NTK
and the prediction targets. In addition, if the NTK is positive deﬁnite, the training can achieve global
convergence (Du et al., 2019a; Jacot et al., 2018)."
NEURAL TANGENT KERNEL,0.04679552390640895,"Although a number of ﬁndings have been obtained using the NTK, they are mainly for typical neural
networks such as MLP and ResNet (He et al., 2016), and the NTK theory has not yet been applied
to tree models. The NTK theory is often used in the context of overparameterization, which is a
subject of interest not only for the neural networks, but also for the tree models (Belkin et al., 2019;
Karthikeyan et al., 2021; Tang et al., 2018)."
SETUP,0.04781281790437437,"3
SETUP"
SETUP,0.048830111902339775,"2
0
2
wm,nxi 0.0 0.2 0.4 0.6 0.8 1.0"
SETUP,0.04984740590030519,"(wm,nxi) 1 3 5 7 9 11"
SETUP,0.0508646998982706,"Figure 2: The scaled error function. We
draw 50 lines with varying α by 0.25.
A dotted magenta line shows a sigmoid
function, which is close to a scaled error
function with α ∼0.5."
SETUP,0.05188199389623601,"We train model parameters w and π to minimize the squared
loss using the gradient method, where w = (w1, . . . , wM)
and π = (π1, . . . , πM). The tree structure is ﬁxed during
training. In order to use a known closed-form solution of the
NTK (Williams, 1996; Lee et al., 2019), we use a scaled error
function σ : R →(0, 1), resulting in the following decision
function:
gm,n(xi, wm,n) = σ
 
w⊤
m,nxi
 := 1"
ERF,0.052899287894201424,"2 erf
 
αw⊤
m,nxi

+ 1"
ERF,0.05391658189216684,"2,
(5)"
ERF,0.054933875890132246,"where erf(p) =
2
√π
R p
0 e−t2 dt for p ∈R. This scaled error
function approximates a commonly used sigmoid function.
Since the bias term for the input of σ can be expressed inside
of w by adding an element that takes a ﬁxed constant value
for all input of the soft trees x, we do not consider the bias
for simplicity. The scaling factor α is introduced by Frosst
& Hinton (2017) to avoid overly soft splitting. Figure 2 shows that the decision function becomes
harder as α increases (from blue to red), and in the limit α →∞it coincides with the hard splitting
used in typical decision trees."
ERF,0.05595116988809766,"When aggregating the output of multiple trees, we divide the sum of the tree outputs by the square
root of the number of trees"
ERF,0.056968463886063074,"f(xi, w, π) =
1
√ M M
X"
ERF,0.05798575788402848,"m=1
fm(xi, wm, πm).
(6)"
ERF,0.059003051881993895,"This 1/
√"
ERF,0.06002034587995931,"M scaling is known to be essential in the existing NTK literature to use the weak law of the
large numbers (Jacot et al., 2018). On top of Equation (6), we initialize each of model parameters
wm,n and πm,ℓwith zero-mean i.i.d. Gaussians with unit variances. We refer such a parameterization
as NTK initialization. In this paper, we consider a model such that all M trees have the same perfect
binary tree structure, a common setting for soft tree ensembles (Popov et al., 2020; Kontschieder
et al., 2015; Hazimeh et al., 2020)."
THEORETICAL RESULTS,0.061037639877924724,"4
THEORETICAL RESULTS"
BASIC PROPERTIES OF THE TNTK,0.06205493387589013,"4.1
BASIC PROPERTIES OF THE TNTK"
BASIC PROPERTIES OF THE TNTK,0.06307222787385554,"The NTK in Equation (3) induced by the soft tree ensembles is referred to here as the TNTK and
denoted by bΘ(d)
0 (xi, xj) the TNTK at initialization induced by the ensemble of trees with depth d."
BASIC PROPERTIES OF THE TNTK,0.06408952187182096,Published as a conference paper at ICLR 2022
BASIC PROPERTIES OF THE TNTK,0.06510681586978637,"1.0
0.5
0.0
0.5
1.0
Inner product of the inputs 0.0 0.5 1.0 1.5 2.0 2.5"
BASIC PROPERTIES OF THE TNTK,0.06612410986775177,"0(xi,xj)"
BASIC PROPERTIES OF THE TNTK,0.0671414038657172,"d=3, =2.0"
BASIC PROPERTIES OF THE TNTK,0.0681586978636826,"M=16
M=64
M=256
M=1024
M=4096
M="
BASIC PROPERTIES OF THE TNTK,0.06917599186164802,"10
2
10
3 M 10
2 10
1 10
0"
BASIC PROPERTIES OF THE TNTK,0.07019328585961343,"||H0
H||F
||H0||F"
BASIC PROPERTIES OF THE TNTK,0.07121057985757884,Change  with fixed d=3
BASIC PROPERTIES OF THE TNTK,0.07222787385554426,"trend: M
1/2"
BASIC PROPERTIES OF THE TNTK,0.07324516785350967,"d=3,
=0.5
d=3,
=1.0
d=3,
=2.0
d=3,
=4.0
d=3,
=8.0
d=3,
=16.0
d=3,
=32.0
d=3,
=64.0"
BASIC PROPERTIES OF THE TNTK,0.07426246185147507,"10
2
10
3 M 10
2 10
1"
BASIC PROPERTIES OF THE TNTK,0.0752797558494405,"||H0
H||F
||H0||F"
BASIC PROPERTIES OF THE TNTK,0.0762970498474059,Change d with fixed =2.0
BASIC PROPERTIES OF THE TNTK,0.07731434384537131,"trend: M
1/2"
BASIC PROPERTIES OF THE TNTK,0.07833163784333673,"d=1,
=2.0
d=2,
=2.0
d=3,
=2.0
d=4,
=2.0
d=5,
=2.0
d=6,
=2.0"
BASIC PROPERTIES OF THE TNTK,0.07934893184130214,"Figure 3: Left: An empirical demonstration of convergence of bΘ0(xi, xj) to the ﬁxed limit Θ(xi, xj)
as M increases. Two simple inputs are considered: xi = {1, 0} and xj = {cos(β), sin(β)}
with β = [0, π]. The TNTK bΘ(3)
0 (xi, xj) with α = 2.0 is calculated 10 times with parameter
re-initialization for each of the M = 16, 64, 256, 1024, and 4096. Center and Right: Parameter
dependency of the convergence. The vertical axis corresponds to the averaged error between the
c
H0 and the H := limM→∞c
H0 for 50 random unit vectors of length F = 5. The dashed lines are
plotted only for showing the slope. The error bars show the standard deviations of 10 executions."
BASIC PROPERTIES OF THE TNTK,0.08036622583926754,"In this section, we show the properties of the TNTK that are important for understanding the training
behavior of the soft tree ensembles."
TNTK FOR INFINITE TREE ENSEMBLES,0.08138351983723296,"4.1.1
TNTK FOR INFINITE TREE ENSEMBLES"
TNTK FOR INFINITE TREE ENSEMBLES,0.08240081383519837,"First, we show the formula of the TNTK at initialization, which converges when considering the limit
of inﬁnite trees (M →∞).
Theorem 1. Let u ∈RF be any column vector sampled from zero-mean i.i.d. Gaussians with unit
variance. The TNTK for an ensemble of soft perfect binary trees with tree depth d converges in
probability to the following deterministic kernel as M →∞,"
TNTK FOR INFINITE TREE ENSEMBLES,0.08341810783316378,"Θ(d)(xi, xj) := lim
M→∞
bΘ(d)
0 (xi, xj)"
TNTK FOR INFINITE TREE ENSEMBLES,0.0844354018311292,"= 2dd Σ(xi, xj)(T (xi, xj))d−1 ˙T (xi, xj)
|
{z
}
contribution from inner nodes"
TNTK FOR INFINITE TREE ENSEMBLES,0.08545269582909461,"+ (2T (xi, xj))d
|
{z
}
contribution from leaves ,
(7)"
TNTK FOR INFINITE TREE ENSEMBLES,0.08646998982706001,"where Σ(xi, xj)
:=
x⊤
i xj,
T (xi, xj)
:=
E[σ(u⊤xi)σ(u⊤xj)],
and
˙T (xi, xj)
:=
E[ ˙σ(u⊤xi) ˙σ(u⊤xj)]. Moreover, T (xi, xj) and ˙T (xi, xj) are analytically obtained in the closed-
form as"
TNTK FOR INFINITE TREE ENSEMBLES,0.08748728382502544,"T (xi, xj) = 1"
TNTK FOR INFINITE TREE ENSEMBLES,0.08850457782299084,2π arcsin
TNTK FOR INFINITE TREE ENSEMBLES,0.08952187182095625,"α2Σ(xi, xj)
p"
TNTK FOR INFINITE TREE ENSEMBLES,0.09053916581892167,"(α2Σ(xi, xi) + 0.5)(α2Σ(xj, xj) + 0.5) ! + 1"
TNTK FOR INFINITE TREE ENSEMBLES,0.09155645981688708,"4,
(8)"
TNTK FOR INFINITE TREE ENSEMBLES,0.09257375381485249,"˙T (xi, xj) = α2 π
1
p"
TNTK FOR INFINITE TREE ENSEMBLES,0.0935910478128179,"(1 + 2α2Σ(xi, xi)) (1 + 2α2Σ(xj, xj))−4α4Σ(xi, xj)2 .
(9)"
TNTK FOR INFINITE TREE ENSEMBLES,0.09460834181078331,"The dot used in ˙σ(u⊤xi) means the ﬁrst derivative: αe−(αu⊤xi)2/√π, and E[·] means the expecta-
tion. The scalar π in Equation (8) and Equation (9) is the circular constant, and u corresponds to
wm,n at an arbitrary internal node. The proof is given by induction. We can derive the formula of the
limiting TNTK by treating the number of trees in a tree ensemble like the width of the hidden layer
in MLP, although the MLP and the soft tree ensemble are apparently different models. Due to space
limitations, detailed proofs are given in the supplementary material."
TNTK FOR INFINITE TREE ENSEMBLES,0.09562563580874874,"We demonstrate convergence of the TNTK in Figure 3. We empirically observe that the TNTK
induced by sufﬁciently many soft trees converges to the limiting TNTK given in Equation (7). The
kernel values induced by an ﬁnite ensemble are already close to the limiting TNTK if the number
of trees is larger than several hundreds, which is a typical order of the number of trees in practical
applications1. Therefore, it is reasonable to analyze soft tree ensembles via the TNTK."
TNTK FOR INFINITE TREE ENSEMBLES,0.09664292980671414,"1For example, Popov et al. (2020) uses 2048 trees."
TNTK FOR INFINITE TREE ENSEMBLES,0.09766022380467955,Published as a conference paper at ICLR 2022
TNTK FOR INFINITE TREE ENSEMBLES,0.09867751780264497,"By comparing Equation (7) and the limiting NTK induced by a two-layer perceptron (shown in the
supplementary material), we can immediately derive the following when the tree depth is 1.
Corollary 1. If the splitting function at the tree internal node is the same as the activation function
of the neural network, the limiting TNTK obtained from a soft tree ensemble of depth 1 is equivalent
to the limiting NTK generated by a two-layer perceptron up to constant multiple."
TNTK FOR INFINITE TREE ENSEMBLES,0.09969481180061038,"For any tree depth larger than 1, the limiting NTK induced by the MLP with any number of layers
(Arora et al., 2019, shown in the supplementary material) and the limiting TNTK do not match. This
implies that the hierarchical splitting structure is a distinctive feature of soft tree ensembles."
POSITIVE DEFINITENESS OF THE LIMITING TNTK,0.10071210579857579,"4.1.2
POSITIVE DEFINITENESS OF THE LIMITING TNTK"
POSITIVE DEFINITENESS OF THE LIMITING TNTK,0.1017293997965412,"Since the loss surface of a large model is expected to be highly non-convex, understanding the good
empirical trainability of overparameterized models remains an open problem (Dauphin et al., 2014).
The positive deﬁniteness of the limiting kernel is one of the most important conditions for achieving
global convergence (Du et al., 2019a; Jacot et al., 2018). Jacot et al. (2018) showed that the conditions
∥xi∥2= 1 for all i ∈[N] and xi ̸= xj (i ̸= j) are necessary for the positive deﬁniteness of the
NTK induced by the MLP for an input set. As for the TNTK, since the formulation (Equation (7))
is different from that of typical neural networks such as an MLP, it is not clear whether or not the
limiting TNTK is positive deﬁnite."
POSITIVE DEFINITENESS OF THE LIMITING TNTK,0.10274669379450661,"We prove that the TNTK induced by inﬁnite trees is also positive deﬁnite under the same condition
for the MLP.
Proposition 1. For inﬁnitely many soft trees with any depth and the NTK initialization, the limiting
TNTK is positive deﬁnite if ∥xi∥2= 1 for all i ∈[N] and xi ̸= xj (i ̸= j)."
POSITIVE DEFINITENESS OF THE LIMITING TNTK,0.10376398779247202,"The proof is provided in the supplementary material. Similar to the discussion for the MLP (Du et al.,
2019a; Jacot et al., 2018), if the limiting TNTK is constant during training, the positive deﬁniteness of
the limiting TNTK at initialization indicates that training of the inﬁnite trees with a gradient method
can converge to the global minimum. The constantness of the limiting TNTK during training is
shown in the following section."
CHANGE OF THE TNTK DURING TRAINING,0.10478128179043744,"4.1.3
CHANGE OF THE TNTK DURING TRAINING"
CHANGE OF THE TNTK DURING TRAINING,0.10579857578840285,"We prove that the TNTK hardly changes from its initial value during training when considering an
ensemble of inﬁnite trees with ﬁnite α (used in Equation (5)).
Theorem 2. Let λmin and λmax be the minimum and maximum eigenvalues of the limiting TNTK.
Assume that the limiting TNTK is positive deﬁnite for input sets. For soft tree ensemble models
with the NTK initialization and a positive ﬁnite scaling factor α trained under gradient ﬂow with a
learning rate η < 2/(λmin + λmax), we have, with high probability,"
CHANGE OF THE TNTK DURING TRAINING,0.10681586978636826,"sup
bΘ(d)
τ
(xi, xj) −bΘ(d)
0
(xi, xj)
 = O

1
√ M"
CHANGE OF THE TNTK DURING TRAINING,0.10783316378433368,"
.
(10)"
CHANGE OF THE TNTK DURING TRAINING,0.10885045778229908,"The complete proof is provided in the supplementary material. Figure 4 shows that the training
trajectory analytically obtained (Jacot et al., 2018; Lee et al., 2019) from the limiting TNTK and
the trajectory during gradient descent training become similar as the number of trees increases,
demonstrating the validity of using the TNTK framework to analyze the training behavior."
CHANGE OF THE TNTK DURING TRAINING,0.10986775178026449,"Remarks.
In the limit of inﬁnitely large α, which corresponds to a hard decision tree splitting
(Figure 2), it should be noted that this theorem does not hold because of the lack of local Lipschitzness
(Lee et al., 2019), which is the fundamental property for this proof. Therefore the change in the
TNTK during training is no longer necessarily asymptotic to zero, even if the number of trees is
inﬁnite. This means that understanding the hard decision tree’s behavior using the TNTK is not
straightforward."
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11088504577822991,"4.2
IMPLICATIONS FOR PRACTICAL TECHNIQUES"
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11190233977619532,"In this section, from the viewpoint of the TNTK, we discuss the training techniques that have been
used in practice."
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11291963377416073,Published as a conference paper at ICLR 2022
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11393692777212615,"0
200
400
600
800
1000
 (iteration) 2.5 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5"
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11495422177009156,"f(xi,w, )"
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11597151576805696,Train output
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11698880976602238,"Analytical
M=16
M=1024"
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.11800610376398779,"0
200
400
600
800
1000
 (iteration) 3 2 1 0 1"
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.1190233977619532,"f(xi,w, )"
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.12004069175991862,Test output
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.12105798575788403,"Analytical
M=16
M=1024"
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.12207527975584945,"Figure 4: Output dynamics for train and test data points. The color of each line corresponds to each
data point. Soft tree ensembles with d = 3, α = 2.0 are trained by a full-batch gradient descent
with a learning rate of 0.1. Initial outputs are shifted to zero (Chizat et al., 2019). There are 10
randomly generated training points and 10 randomly generated test data points, and their dimension
F = 5. The prediction targets are also randomly generated. Let H(x, x′) ∈RN×N ′ be the limiting
NTK matrix for two input matrices and I be an identity matrix. For analytical results, we draw the
trajectory f(v, θτ) = H(v, x)H(x, x)−1(I −exp[−ηH(x, x)τ])y (Lee et al., 2019) using the
limiting TNTK (Equation (7)), where v ∈RF is an arbitrary input and x ∈RF ×N and y ∈RN are
the training dataset and the targets, respectively."
IMPLICATIONS FOR PRACTICAL TECHNIQUES,0.12309257375381485,"Figure 5: Left: Normal Tree, Right: Oblivious Tree. The rules for splitting in the same depth are
shared across the same depth in the oblivious tree, while πm,ℓon leaves can be different."
INFLUENCE OF THE OBLIVIOUS TREE STRUCTURE,0.12410986775178026,"4.2.1
INFLUENCE OF THE OBLIVIOUS TREE STRUCTURE"
INFLUENCE OF THE OBLIVIOUS TREE STRUCTURE,0.12512716174974567,"An oblivious tree is a practical tree model architecture where the rules across decision tree splitting
are shared across the same depth as illustrated in Figure 5. Since the number of the splitting decision
calculation can be reduced from O(2d) to O(d), the oblivious tree structure is used in various
open-source libraries such as CatBoost (Prokhorenkova et al., 2018) and NODE (Popov et al., 2020).
However, the reason for the good empirical performance of oblivious trees is non-trivial despite
weakening the expressive power due to parameter sharing."
INFLUENCE OF THE OBLIVIOUS TREE STRUCTURE,0.12614445574771108,"We ﬁnd that the oblivious tree structure does not change the limiting TNTK from the non-oblivious
one. This happens because, even with parameter sharing at splitting nodes, leaf parameters π are not
shared, resulting in independence between outputs of left and right subtrees.
Theorem 3. The TNTK with the perfect binary tree ensemble and the TNTK of its corresponding
oblivious tree ensemble obtained via parameter sharing converge to the same kernel in probability in
the limit of inﬁnite trees (M →∞)."
INFLUENCE OF THE OBLIVIOUS TREE STRUCTURE,0.1271617497456765,"The complete proof is in the supplementary material. Note that leaf values π do not have to be the
same for oblivious and non-oblivious trees. This theorem supports the recent success of tree ensemble
models with the oblivious tree structures."
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.12817904374364192,"4.2.2
EFFECT OF THE DECISION FUNCTION MODIFICATION"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.12919633774160733,"Practically, based on the commonly used sigmoid function, a variety of functions have been pro-
posed for the splitting operation. By considering a large scaling factor α in Equation (5), we
can envisage situations in which there are practically used hard functions, such as two-class
sparsemax, σ(x) = sparsemax([x, 0]) (Martins & Astudillo, 2016), and two-class entmax,
σ(x) = entmax([x, 0]) (Peters et al., 2019). Figure 6 shows α dependencies of TNTK param-
eters. Equation (7) means that the TNTK is formulated by multiplying T (xi, xj) and ˙T (xi, xj) to
the linear kernel Σ(xi, xj). On the one hand, T (xi, xj) and ˙T (xi, xj) with small α are almost con-"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.13021363173957273,Published as a conference paper at ICLR 2022
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.13123092573753814,"1
0
1
inner product of the inputs 0.00 0.25 0.50 0.75 1.00"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.13224821973550355,"Normalized 
(2)(xi,xj)"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.13326551373346898,"1
0
1
Inner product of the inputs 0.00 0.25 0.50 0.75 1.00"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.1342828077314344,"Normalized 
(d)(xi,xj),
=1"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.1353001017293998,"1
0
1
inner product of the inputs 0.0 0.1 0.2 0.3 0.4 0.5"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.1363173957273652,"(xi,xj)"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.1373346897253306,"1
0
1
inner product of the inputs 0.0 0.5 1.0 1.5 2.0"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.13835198372329605,"(xi,xj)"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.13936927772126145,"1
3
5
7
9
11
5
10
15
20
25
d"
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.14038657171922686,"Figure 6: Parameter dependencies of T (xi, xj), ˙T (xi, xj), and Θ(d)(xi, xj). The vertical axes are
normalized so that the value is 1 when the inner product of the inputs is 1. The input vector size is
normalized to be one. For the three ﬁgures on the left, the line color is determined by α, and for the
ﬁgure on the right, it is determined by the depth of the tree."
EFFECT OF THE DECISION FUNCTION MODIFICATION,0.14140386571719227,"stant, even for different input inner products, resulting in the almost linear TNTK. On the other hand,
the nonlinearity increases as α increases. For Figure 2, the original sigmoid function corresponds to
a scaled error function for α ∼0.5, which induces almost the linear kernel. Although a closed-form
TNTK using sigmoid as a decision function has not been obtained, its kernel is expected to be almost
linear. Therefore, from the viewpoint of the TNTK, an adjustment of the decision function (Frosst
& Hinton, 2017; Popov et al., 2020; Hazimeh et al., 2020) can be interpreted as an escape from the
linear kernel behavior."
DEGENERACY CAUSED BY DEEP TREES,0.14242115971515767,"4.2.3
DEGENERACY CAUSED BY DEEP TREES"
DEGENERACY CAUSED BY DEEP TREES,0.14343845371312308,"As the depth increases, T (xi, xj) deﬁned in Equation (8), which consists of the arcsine function,
is multiplied multiple times to calculate the limiting TNTK in Equation (7). Therefore, when we
increase the depth too much, the resulting TNTK exhibits degeneracy: its output values are almost
the same as each other’s, even though the input’s inner products are different. The rightmost panel
of Figure 6 shows such degeneracy behavior. In terms of kernel regression, models using a kernel
that gives almost the same inner product to all data except those that are quite close to each other
are expected to have poor generalization performance. Such behavior is observed in our numerical
experiments (Section 5). In practical applications, overly deep soft or hard decision trees are not
usually used because overly deep trees show poor performance (Luo et al., 2021), which is supported
by the degeneracy of the TNTK."
NUMERICAL EXPERIMENTS,0.14445574771108852,"5
NUMERICAL EXPERIMENTS"
NUMERICAL EXPERIMENTS,0.14547304170905392,"Setup. We present our experimental results on 90 classiﬁcation tasks in the UCI database (Dua
& Graff, 2017), with fewer than 5000 data points, as in Arora et al. (2020). We performed kernel
regression using the limiting TNTK deﬁned in Equation (7) with varying the tree depth (d) and the
scaling (α) of the decision function. The limiting TNTK does not change during training for an
inﬁnite ensemble of soft trees (Theorem 2); therefore, predictions from that model are equivalent to
kernel regression using the limiting TNTK (Jacot et al., 2018). To consider the ridge-less situation,
regularization strength is set to be 1.0 × 10−8, a very small constant. By way of comparison,
performances of the kernel regression with the MLP-induced NTK (Jacot et al., 2018) and the RBF
kernel are also reported. For the MLP-induced NTK, we use ReLU for the activation function. We
follow the procedures of Arora et al. (2020) and Fernández-Delgado et al. (2014): We report 4-fold
cross-validation performance with random data splitting. To tune parameters, all available training
samples are randomly split into one training and one validation set, while imposing that each class
has the same number of training and validation samples. Then the parameter with the best validation
accuracy is selected. Other details are provided in the supplementary material."
NUMERICAL EXPERIMENTS,0.14649033570701933,"Comparison to the MLP. The left panel of Figure 7 shows the averaged performance as a function
of the depth. Although the TNTK with properly tuned parameters tend to be better than those obtained
with the RBF kernel, they are often inferior to the MLP-induced NTK. The results support the good
performance of the MLP-induced NTK (Arora et al., 2020). However, it should be noted that when
we look at each dataset one by one, the TNTK is superior to the MLP-induced NTK by more than"
NUMERICAL EXPERIMENTS,0.14750762970498474,Published as a conference paper at ICLR 2022
NUMERICAL EXPERIMENTS,0.14852492370295015,"MLP-induced NTK
TNTK"
NUMERICAL EXPERIMENTS,0.14954221770091555,"Figure 7: Left: Averaged accuracy over 90 datasets. The performances of the kernel regression with
the MLP-induced NTK and the RBF kernel are shown for comparison. Since the depth is not a hyper-
parameter of the RBF kernel, performance is shown by a horizontal line. The statistical signiﬁcance is
also assessed in the supplementary material. Right: Running time for kernel computation. The input
dataset has 300 samples with 10 features. Feature values are generated by zero-mean i.i.d Gaussian
with unit variance. The error bars show the standard deviations of 10 executions."
NUMERICAL EXPERIMENTS,0.150559511698881,"Table 1: Performance win rate against the MLP-induced NTK. We tune the depth from d = 1 to 29
for the dataset-wise comparison for both the TNTK and the MLP-induced NTK. For the RBF kernel,
30 different hyperparameters are tried. Detailed results are in the supplementary material."
NUMERICAL EXPERIMENTS,0.1515768056968464,"TNTK
RBF
α
0.5
1.0
2.0
4.0
8.0
16.0
32.0
64.0
—"
NUMERICAL EXPERIMENTS,0.1525940996948118,"Win rate (%)
13.6
18.8
22.2
28.6
32.5
31.6
34.9
27.2
11.8"
NUMERICAL EXPERIMENTS,0.1536113936927772,"30 percent of the dataset, as shown in Table 1. This is a case where the characteristics of data and
the inductive bias of the model ﬁt well together. In addition, although the computational cost of
the MLP-induced NTK is linear with respect to the depth of the model because of the recursive
computation (Jacot et al., 2018), the computational cost of the TNTK does not depend on the depth, as
shown in Equation (7). This results in much faster computation than the MLP-induced NTK when the
depth increases, as illustrated in the right panel of Figure 7. Even if the MLP-induced NTK is better
in prediction accuracy, the TNTK may be used in practical cases as a trade-off for computational
complexity. Arora et al. (2019) proposed the use of the NTK for a neural architecture search (Elsken
et al., 2019; Chen et al., 2021). In such applications, the fast computation of the kernel in various
architectures can be a beneﬁt. We leave extensions of this idea to tree models as future work."
NUMERICAL EXPERIMENTS,0.15462868769074262,"Consistency with implications from the TNTK theory.
When we increase the tree depth, we
initially observe an improvement in performance, after which the performance gradually decreases.
This behavior is consistent with the performance deterioration due to degeneracy (Section 4.2.3),
similar to that reported for neural networks without skip-connection (Huang et al., 2020), shown by a
dotted yellow line. The performance improvement by adjusting α in the decision function (Frosst
& Hinton, 2017) is also observed. Performances with hard (α > 0.5) decision functions are always
better than the sigmoid-like function (α = 0.5, as shown in Figure 2)."
CONCLUSION,0.15564598168870802,"6
CONCLUSION"
CONCLUSION,0.15666327568667346,"In this paper, we have introduced and studied the Tree Neural Tangent Kernel (TNTK) by considering
the ensemble of inﬁnitely many soft trees. The TNTK provides new insights into the behavior of the
inﬁnite ensemble of soft trees, such as the effect of the oblivious tree structure and the degeneracy
of the TNTK induced by the deepening of the trees. In numerical experiments, we have observed
the degeneracy phenomena induced by the deepening of the soft tree model, which is suggested by
our theoretical results. To date, the NTK theory has been mostly applied to neural networks, and
our study is the ﬁrst to apply it to the tree model. Therefore our study represents a milestone in the
development of the NTK theory."
CONCLUSION,0.15768056968463887,Published as a conference paper at ICLR 2022
CONCLUSION,0.15869786368260427,ACKNOWLEDGEMENT
CONCLUSION,0.15971515768056968,"This work was supported by JSPS KAKENHI (Grant Number JP21H03503d, Japan), JST PRESTO
(Grant Number JPMJPR1855, Japan), and JST FOREST (Grant Number JPMJFR206J, Japan)."
ETHICS STATEMENT,0.1607324516785351,ETHICS STATEMENT
ETHICS STATEMENT,0.1617497456765005,"We believe that the TNTK’s theoretical analysis will not have a negative impact on society as our
work does not directly lead to harmful applications."
REPRODUCIBILITY STATEMENT,0.16276703967446593,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.16378433367243134,"Proofs of all theoretical results are provided in the supplementary material. For the numerical
experiments and ﬁgures, we share our reproducible source code in the supplementary material."
REFERENCES,0.16480162767039674,REFERENCES
REFERENCES,0.16581892166836215,"Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The Recurrent Neural
Tangent Kernel. In International Conference on Learning Representations, 2021."
REFERENCES,0.16683621566632756,"Sercan Ömer Arik and Tomas Pﬁster. TabNet: Attentive Interpretable Tabular Learning. CoRR,
abs/1908.07442, 2019."
REFERENCES,0.167853509664293,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
Exact Computation with an Inﬁnitely Wide Neural Net. In Advances in Neural Information
Processing Systems, volume 32, 2019."
REFERENCES,0.1688708036622584,"Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu. Har-
nessing the Power of Inﬁnitely Wide Deep Nets on Small-data Tasks. In International Conference
on Learning Representations, 2020."
REFERENCES,0.1698880976602238,"Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and
Structural Results. Journal of Machine Learning Research, 3, 2003."
REFERENCES,0.17090539165818922,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences,
116(32), 2019."
REFERENCES,0.17192268565615462,"Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. Classiﬁcation and Regression
Trees. Chapman and Hall/CRC, 1984."
REFERENCES,0.17293997965412003,"Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural Architecture Search on ImageNet in
Four GPU Hours: A Theoretically Inspired Perspective. In International Conference on Learning
Representations, 2021."
REFERENCES,0.17395727365208546,"Lénaïc Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Programming.
In Advances in Neural Information Processing Systems, volume 32, 2019."
REFERENCES,0.17497456765005087,"Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in Neural Information Processing Systems, volume 27, 2014."
REFERENCES,0.17599186164801628,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient Descent Finds Global
Minima of Deep Neural Networks. In Proceedings of the 36th International Conference on
Machine Learning, 2019a."
REFERENCES,0.1770091556459817,"Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels. In
Advances in Neural Information Processing Systems, volume 32, 2019b."
REFERENCES,0.1780264496439471,"Dheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.1790437436419125,Published as a conference paper at ICLR 2022
REFERENCES,0.18006103763987794,"Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural Architecture Search: A Survey.
Journal of Machine Learning Research, 20(55):1–21, 2019."
REFERENCES,0.18107833163784334,"Manuel Fernández-Delgado, Eva Cernadas, Senén Barro, and Dinani Amorim. Do we Need Hundreds
of Classiﬁers to Solve Real World Classiﬁcation Problems? Journal of Machine Learning Research,
15, 2014."
REFERENCES,0.18209562563580875,"Nicholas Frosst and Geoffrey E. Hinton. Distilling a Neural Network Into a Soft Decision Tree.
CoRR, 2017."
REFERENCES,0.18311291963377416,"Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The Tree
Ensemble Layer: Differentiability meets Conditional Computation. In Proceedings of the 37th
International Conference on Machine Learning, volume 119, 2020."
REFERENCES,0.18413021363173956,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2016."
REFERENCES,0.18514750762970497,"Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why Do Deep Residual Networks
Generalize Better than Deep Feedforward Networks? — A Neural Tangent Kernel Perspective. In
Advances in Neural Information Processing Systems, volume 33, 2020."
REFERENCES,0.1861648016276704,"Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31.
2018."
REFERENCES,0.1871820956256358,"M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. In Proceedings
of 1993 International Conference on Neural Networks, volume 2, 1993."
REFERENCES,0.18819938962360122,"Ajaykrishna Karthikeyan, Naman Jain, Nagarajan Natarajan, and Prateek Jain. Learning Accurate
Decision Trees with Bandit Feedback via Quantized Gradient Descent. CoRR, 2021."
REFERENCES,0.18921668362156663,"Guolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu. DeepGBM: A Deep Learning
Framework Distilled by GBDT for Online Prediction Tasks. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019."
REFERENCES,0.19023397761953204,"Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulò. Deep Neural
Decision Forests. In 2015 IEEE International Conference on Computer Vision, 2015."
REFERENCES,0.19125127161749747,"Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models
Under Gradient Descent. In Advances in Neural Information Processing Systems, volume 32,
2019."
REFERENCES,0.19226856561546288,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models with Conditional
Computation and Automatic Sharding. In International Conference on Learning Representations,
2021."
REFERENCES,0.19328585961342828,"Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced Convolutional Neural Tangent Kernels. CoRR, abs/1911.00809, 2019."
REFERENCES,0.1943031536113937,"Haoran Luo, Fan Cheng, Heng Yu, and Yuqi Yi. SDTR: Soft Decision Tree Regressor for Tabular
Data. IEEE Access, 9, 2021."
REFERENCES,0.1953204476093591,"Andre Martins and Ramon Astudillo. From Softmax to Sparsemax: A Sparse Model of Attention
and Multi-Label Classiﬁcation. In Proceedings of The 33rd International Conference on Machine
Learning, volume 48, 2016."
REFERENCES,0.1963377416073245,"Ben Peters, Vlad Niculae, and André F. T. Martins. Sparse Sequence-to-Sequence Models. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019."
REFERENCES,0.19735503560528994,"Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural Oblivious Decision Ensembles for
Deep Learning on Tabular Data. In International Conference on Learning Representations, 2020."
REFERENCES,0.19837232960325535,Published as a conference paper at ICLR 2022
REFERENCES,0.19938962360122076,"Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey
Gulin. CatBoost: unbiased boosting with categorical features. In Advances in Neural Information
Processing Systems, volume 31, 2018."
REFERENCES,0.20040691759918616,"J. R. Quinlan. Induction of Decision Trees. Machine Learning, 1(1), 1986."
REFERENCES,0.20142421159715157,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton,
and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
Layer. In International Conference on Learning Representations, 2017."
REFERENCES,0.20244150559511698,"Cheng Tang, Damien Garreau, and Ulrike von Luxburg. When do random forests fail? In Advances
in Neural Information Processing Systems, volume 31, 2018."
REFERENCES,0.2034587995930824,"Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, and Aditya Nori. Adap-
tive Neural Trees. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, 2019."
REFERENCES,0.20447609359104782,"Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Suzanne Petryk, Sarah Adel Bargal,
and Joseph E. Gonzalez. NBDT: Neural-Backed Decision Tree. In International Conference on
Learning Representations, 2021."
REFERENCES,0.20549338758901323,"Christopher K. I. Williams. Computing with Inﬁnite Networks. In Advances in Neural Information
Processing Systems, 1996."
REFERENCES,0.20651068158697863,"A
PROOF OF THEOREM 1"
REFERENCES,0.20752797558494404,"Proof. The model output from a certain depth tree ensemble f (d) can be written alternatively using
an incremental formula as"
REFERENCES,0.20854526958290945,"f (d)(xi, w, π) =
1
√ M M
X m=1"
REFERENCES,0.20956256358087488,"
σ
 
w⊤
m,txi

f (d−1)
m

xi, w(l)
m , π(l)
m
"
REFERENCES,0.2105798575788403,"+
 
1 −σ
 
w⊤
m,txi

f (d−1)
m

xi, w(r)
m , π(r)
m

,
(A.1)"
REFERENCES,0.2115971515768057,"where indices (l) and (r) used with model parameters wm and πm mean the parameters at the
(l)eft subtree and the (r)ight subtree, respectively, and t used in wm,t denotes the node at (t)op of
the tree. For example, for trees of depth 3, as shown in Figure 1, w(l)
m = (wm,2, wm,4, wm,5),
w(r)
m = (wm,3, wm,6, wm,7), and wm,t = wm,1."
REFERENCES,0.2126144455747711,"We prove the theorem by induction. When d = 1,"
REFERENCES,0.2136317395727365,"f (1)(xi, w, π) =
1
√ M M
X m=1"
REFERENCES,0.21464903357070192,"
σ(w⊤
m,1xi)πm,1 + (1 −σ(w⊤
m,1xi))πm,2

.
(A.2)"
REFERENCES,0.21566632756866735,Derivatives are
REFERENCES,0.21668362156663276,"∂f (1)(xi, w, π)"
REFERENCES,0.21770091556459817,"∂wm,1
=
1
√"
REFERENCES,0.21871820956256358,"M
(πm,1 −πm,2) xi ˙σ
 
w⊤
m,1xi

,
(A.3)"
REFERENCES,0.21973550356052898,"∂f (1)(xi, w, π)"
REFERENCES,0.22075279755849442,"∂πm,1
=
1
√"
REFERENCES,0.22177009155645983,"M
σ
 
w⊤
m,1xi

,
(A.4)"
REFERENCES,0.22278738555442523,"∂f (1)(xi, w, π)"
REFERENCES,0.22380467955239064,"∂πm,2
= −
1
√ M"
REFERENCES,0.22482197355035605," 
1 −σ
 
w⊤
m,1xi

,
(A.5)"
REFERENCES,0.22583926754832145,Published as a conference paper at ICLR 2022
REFERENCES,0.2268565615462869,"Therefore, since there is only one internal node per a single tree, from the deﬁnition of the NTK, the
TNTK is obtained as"
REFERENCES,0.2278738555442523,"bΘ(1) (xi, xj) = M
X m=1"
REFERENCES,0.2288911495422177,"∂f (xi, w, π)"
REFERENCES,0.2299084435401831,"∂wm,1
, ∂f (xj, w, π) ∂wm,1 "
REFERENCES,0.23092573753814852,"+
∂f (xi, w, π)"
REFERENCES,0.23194303153611392,"∂πm,1
, ∂f (xj, w, π) ∂πm,1"
REFERENCES,0.23296032553407936,"+
∂f (xi, w, π)"
REFERENCES,0.23397761953204477,"∂πm,2
, ∂f (xj, w, π) ∂πm,2 ! = 1 M M
X m=1"
REFERENCES,0.23499491353001017,"
(πm,1 −πm,2)2 x⊤
i xj ˙σ(w⊤
m,1xi) ˙σ(w⊤
m,1xj) + 2σ(w⊤
m,1xi)σ(w⊤
m,1xj)

. (A.6)"
REFERENCES,0.23601220752797558,"Since we are considering the inﬁnite number of trees (M →∞), the average in Equation (A.6) can
be replaced by the expected value by applying the law of the large numbers:"
REFERENCES,0.237029501525941,"Θ(1) (xi, xj) = Em"
REFERENCES,0.2380467955239064,"""
(πm,1 −πm,2)2 x⊤
i xj ˙σ(w⊤
m,1xi) ˙σ(w⊤
m,1xj)"
REFERENCES,0.23906408952187183,"+ 2σ(w⊤
m,1xi)σ(w⊤
m,1xj)
#"
REFERENCES,0.24008138351983724,"= 2(Σ(xi, xj) ˙T (xi, xj) + T (xi, xj)),
(A.7)"
REFERENCES,0.24109867751780265,"which is consistent with Equation (7). Here, Em
h
(πm,1 −πm,2)2i
= 2 because the variance of πm,ℓ
is 1.0, and wm,1 corresponds to u in Theorem 1."
REFERENCES,0.24211597151576805,"For d > 1, we divide the TNTK into four components:"
REFERENCES,0.24313326551373346,"Θ(d) (xi, xj) = Θ(d),(t) (xi, xj) + Θ(d),(l) (xi, xj) + Θ(d),(r) (xi, xj) + Θ(d),(b) (xi, xj) ,"
REFERENCES,0.2441505595116989,"where the indices (t), (l) and (r) mean the parameters of the (t)op of the tree, (l)eft subtree, and (r)ight
subtree, respectively. The index (b) implies the (b)ottom of the tree: tree leaves. With Equation (A.7),
we have"
REFERENCES,0.2451678535096643,"Θ(1),(t) (xi, xj) = 2Σ(xi, xj) ˙T (xi, xj),
(A.8)"
REFERENCES,0.2461851475076297,"Θ(1),(l) (xi, xj) = 0,
(A.9)"
REFERENCES,0.24720244150559512,"Θ(1),(r) (xi, xj) = 0,
(A.10)"
REFERENCES,0.24821973550356052,"Θ(1),(b) (xi, xj) = 2T (xi, xj).
(A.11)"
REFERENCES,0.24923702950152593,"For each component, we show the following lemmas:"
REFERENCES,0.25025432349949134,Lemma 1.
REFERENCES,0.25127161749745675,"Θ(d+1),(t) (xi, xj) = 2T (xi, xj)Θ(d),(t) (xi, xj)
(A.12)"
REFERENCES,0.25228891149542215,Lemma 2.
REFERENCES,0.2533062054933876,"Θ(d+1),(l) (xi, xj) + Θ(d+1),(r) (xi, xj)"
REFERENCES,0.254323499491353,"= 2T (xi, xj)(Θ(d),(t) (xi, xj) + Θ(d),(l) (xi, xj) + Θ(d),(r) (xi, xj))
(A.13)"
REFERENCES,0.25534079348931843,Lemma 3.
REFERENCES,0.25635808748728384,"Θ(d+1),(b) (xi, xj) = 2T (xi, xj)Θ(d),(b) (xi, xj)
(A.14)"
REFERENCES,0.25737538148524924,"Combining them, we can derive Equation (7)."
REFERENCES,0.25839267548321465,Published as a conference paper at ICLR 2022
REFERENCES,0.25940996948118006,"A.1
PROOF OF LEMMA 1"
REFERENCES,0.26042726347914547,Proof. An incremental formula for the model output with a certain depth tree ensemble is
REFERENCES,0.2614445574771109,"f (d)(xi, w, π) =
1
√ M M
X m=1"
REFERENCES,0.2624618514750763,"
σ
 
w⊤
m,txi

f (d−1)
m

xi, w(l)
m , π(l)
m
"
REFERENCES,0.2634791454730417,"+
 
1 −σ
 
w⊤
m,txi

f (d−1)
m

xi, w(r)
m , π(r)
m

,
(A.15)"
REFERENCES,0.2644964394710071,"where t used in wm,t implies the node at the top of the tree. With Equation (A.15),"
REFERENCES,0.26551373346897256,"∂f (d+1)(xi, w, π)"
REFERENCES,0.26653102746693796,"∂wm,t
= xi ˙σ(w⊤
m,txi)

f (d)
m

xi, w(l)
m , π(l)
m

−f (d)
m

xi, w(r)
m , π(r)
m

,
(A.16)"
REFERENCES,0.26754832146490337,"bΘ(d+1),(t)(xi, xj) = 1 M M
X"
REFERENCES,0.2685656154628688,"m=1
x⊤
i xj ˙σ(w⊤
m,txi) ˙σ(w⊤
m,txj)

f (d)
m

xi, w(l)
m , π(l)
m

f (d)
m

xj, w(l)
m , π(l)
m
"
REFERENCES,0.2695829094608342,"−f (d)
m

xi, w(l)
m , π(l)
m

f (d)
m

xj, w(r)
m , π(r)
m
"
REFERENCES,0.2706002034587996,"−f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(l)
m , π(l)
m
"
REFERENCES,0.271617497456765,"+ f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(r)
m , π(r)
m

,
(A.17)"
REFERENCES,0.2726347914547304,"Since
f (d)
m

xi, w(r)
m , π(r)
m

and
f (d)
m

xj, w(l)
m , π(l)
m

are
independent
of
each
other
and
have
zero-mean
Gaussian
distribution
because
of
the
initialization
of
πm
with
zero-mean
i.i.d
Gaussians2,
E
h
f (d)
m

xi, w(l)
m , π(l)
m

f (d)
m

xj, w(r)
m , π(r)
m
i
and"
REFERENCES,0.2736520854526958,"E
h
f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(l)
m , π(l)
m
i
are zero.
Therefore, considering the inﬁnite"
REFERENCES,0.2746693794506612,"number of trees (M →∞),"
REFERENCES,0.27568667344862663,"Θ(d+1),(t)(xi, xj) = x⊤
i xj ˙T (xi, xj)Em
h
f (d)
m

xi, w(l)
m , π(l)
m

f (d)
m

xj, w(l)
m , π(l)
m
"
REFERENCES,0.2767039674465921,"+ f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(r)
m , π(r)
m
i
(A.18)"
REFERENCES,0.2777212614445575,"From Equation (A.18), what we need to prove is Em
h
f (d)
m (xi, wm, πm)f (d)
m (xj, wm, πm)
i
="
REFERENCES,0.2787385554425229,"(2T (xi, xj))d. We do this by induction. In the base case with d = 1,"
REFERENCES,0.2797558494404883,"Em
h
f (1)
m (xi, wm, πm)f (1)
m (xj, wm, πm)
i =Em"
REFERENCES,0.2807731434384537,"""
σ(w⊤
m,1xi)πm,1 +
 
1 −σ(w⊤
m,1xi)

πm,2

σ(w⊤
m,1xj)πm,1 +
 
1 −σ(w⊤
m,1xj)

πm,2
# =Em "
REFERENCES,0.28179043743641913,"(πm,1 −πm,2)2
|
{z
}
→2"
REFERENCES,0.28280773143438453,"σ(w⊤
m,1xi)σ(w⊤
m,1xj)"
REFERENCES,0.28382502543234994,"+ πm,1πm,2
|
{z
}
→0"
REFERENCES,0.28484231943031535,"(σ(w⊤
m,1xi) + σ(w⊤
m,1xj)) −π2
m,2(σ(w⊤
m,1xi) + σ(w⊤
m,1xj) −1
|
{z
}
→0 )  "
REFERENCES,0.28585961342828076,"=2T (xi, xj) ,
(A.19)"
REFERENCES,0.28687690742624616,where we use the property
REFERENCES,0.28789420142421157,"E [σ(v)] = 0.5
(A.20)"
REFERENCES,0.28891149542217703,"for any v generated from a zero-mean Gaussian distribution. The subscript arrows (→) show what
the expected value will be."
REFERENCES,0.28992878942014244,"2This holds because the model output is a weighted average of πm,ℓ."
REFERENCES,0.29094608341810785,Published as a conference paper at ICLR 2022
REFERENCES,0.29196337741607326,"Next, when the depth is d + 1,"
REFERENCES,0.29298067141403866,"Em
h
f (d+1)
m
(xi, wm, πm)f (d+1)
m
(xj, wm, πm)
i =Em"
REFERENCES,0.29399796541200407,"""
σ(w⊤
m,txi)f (d)
m

xi, w(l)
m , π(l)
m

+
 
1 −σ(w⊤
m,txi)

f (d)
m

xi, w(r)
m , π(r)
m
"
REFERENCES,0.2950152594099695,"
σ(w⊤
m,txj)f (d)
m

xj, w(l)
m , π(l)
m

+
 
1 −σ(w⊤
m,txj)

f (d)
m

xj, w(r)
m , π(r)
m
# =Em    

"
REFERENCES,0.2960325534079349,"
f (d)
m

xi, w(l)
m , π(l)
m

−f (d)
m

xi, w(r)
m , π(r)
m

σ(w⊤
m,txi)
|
{z
}
(A)"
REFERENCES,0.2970498474059003,"+ f (d)
m

xi, w(r)
m , π(r)
m
"
REFERENCES,0.2980671414038657,"|
{z
}
(B)  

  

"
REFERENCES,0.2990844354018311,"
f (d)
m

xj, w(l)
m , π(l)
m

−f (d)
m

xj, w(r)
m , π(r)
m

σ(w⊤
m,txj)
|
{z
}
(C)"
REFERENCES,0.30010172939979657,"+ f (d)
m

xj, w(r)
m , π(r)
m
"
REFERENCES,0.301119023397762,"|
{z
}
(D)  

  ,"
REFERENCES,0.3021363173957274,(A.21)
REFERENCES,0.3031536113936928,"where the last equality sign is just a simpliﬁcation to separate components (A), (B), (C), and (D).
Since f (d)
m (xi, w(r)
m , π(r)
m ) and f (d)
m (xj, w(l)
m , π(l)
m ) are independent of each other and have zero-mean
i.i.d Gaussian distribution, we obtain"
REFERENCES,0.3041709053916582,"Em [(A) × (C)] = T (xi, xj) Em
h
f (d)
m

xi, w(l)
m , π(l)
m

f (d)
m

xj, w(l)
m , π(l)
m
"
REFERENCES,0.3051881993896236,"+f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(r)
m , π(r)
m
i
,
(A.22)"
REFERENCES,0.306205493387589,"Em [(B) × (C)] = −0.5 Em
h
f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(r)
m , π(r)
m
i
,
(A.23)"
REFERENCES,0.3072227873855544,"Em [(A) × (D)] = −0.5 Em
h
f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(r)
m , π(r)
m
i
,
(A.24)"
REFERENCES,0.3082400813835198,"Em [(B) × (D)] = Em
h
f (d)
m

xi, w(r)
m , π(r)
m

f (d)
m

xj, w(r)
m , π(r)
m
i
.
(A.25)"
REFERENCES,0.30925737538148523,"Equation (A.23), Equation (A.24), and Equation (A.25) cancel each other out. Therefore, we obtain"
REFERENCES,0.31027466937945064,"Em
h
f (d+1)
m
(xi, wm, πm)f (d+1)
m
(xj, wm, πm)
i"
REFERENCES,0.31129196337741605,"=2T (xi, xj) Em
h
f (d)
m (xi, wm, πm)f (d)
m (xj, wm, πm)
i
.
(A.26)"
REFERENCES,0.3123092573753815,"By
induction
hypothesis
and
Equation
(A.19),
we
have
Em
h
f (d)
m (xi, wm, πm)f (d)
m (xj, wm, πm)
i
= (2T (xi, xj))d. Therefore, the original lemma also
follows."
REFERENCES,0.3133265513733469,"A.2
PROOF OF LEMMA 2"
REFERENCES,0.3143438453713123,"Proof. When the depth is d + 1, the derivatives of the parameters in the left subtree and the right
subtree are"
REFERENCES,0.31536113936927773,"∂f (d+1) (xi, w, π)"
REFERENCES,0.31637843336724314,"∂wm,l
= σ(w⊤
m,txi)
∂f (d)
m

xi, w(l)
m , π(l)
m
"
REFERENCES,0.31739572736520855,"∂wm,l
,
(A.27)"
REFERENCES,0.31841302136317395,"∂f (d+1) (xi, w, π)"
REFERENCES,0.31943031536113936,"∂wm,r
=
 
1 −σ(w⊤
m,txi)
 ∂f (d)
m

xi, w(r)
m , π(r)
m
"
REFERENCES,0.32044760935910477,"∂wm,r
,
(A.28)"
REFERENCES,0.3214649033570702,"where l and r used in wm,l and wm,r implies the node at the left subtree and right subtree, respectively.
Therefore, the limiting TNTK for the left subtree is"
REFERENCES,0.3224821973550356,"Θ(d+1),(l) (xi, xj) = T (xi, xj)(Θ(d),(t) (xi, xj) + Θ(d),(l) (xi, xj) + Θ(d),(r) (xi, xj)). (A.29)"
REFERENCES,0.323499491353001,Published as a conference paper at ICLR 2022
REFERENCES,0.32451678535096645,"Similarly, for the right subtree, since"
REFERENCES,0.32553407934893186,E[(1 −σ(u⊤xi)(1 −σ(u⊤xj)] = E 
REFERENCES,0.32655137334689727,"1 −σ(u⊤xi)
|
{z
}
→0.5"
REFERENCES,0.3275686673448627,"−σ(u⊤xj)
|
{z
}
→0.5"
REFERENCES,0.3285859613428281,+σ(u⊤xi)σ(u⊤xj)  
REFERENCES,0.3296032553407935,"= E[σ(u⊤xi)σ(u⊤xj)],
(A.30)"
REFERENCES,0.3306205493387589,we can also derive
REFERENCES,0.3316378433367243,"Θ(d+1),(r) (xi, xj) = T (xi, xj)(Θ(d),(t) (xi, xj) + Θ(d),(l) (xi, xj) + Θ(d),(r) (xi, xj)). (A.31)"
REFERENCES,0.3326551373346897,"Finally, we obtain the following by combining with Equation (A.29) and Equation (A.31),"
REFERENCES,0.3336724313326551,"Θ(d+1),(l) (xi, xj) + Θ(d+1),(r) (xi, xj)"
REFERENCES,0.3346897253306205,"= 2T (xi, xj) (Θ(d),(t) (xi, xj) + Θ(d),(l) (xi, xj) + Θ(d),(r) (xi, xj)).
(A.32)"
REFERENCES,0.335707019328586,"A.3
PROOF OF LEMMA 3"
REFERENCES,0.3367243133265514,"Proof. With Equation (1) and Equation (6),"
REFERENCES,0.3377416073245168,"∂f (xi, w, π)"
REFERENCES,0.3387589013224822,"∂πm,ℓ
=
1
√"
REFERENCES,0.3397761953204476,"M
µm,ℓ(xi, wm) =
1
√ M N
Y"
REFERENCES,0.340793489318413,"n=1
σ
 
w⊤
m,nxi
1ℓ↙n  
1 −σ
 
w⊤
m,nxi
1n↘ℓ.
(A.33)"
REFERENCES,0.34181078331637843,"When we focus on a leaf ℓ, for a tree with depth of d, 1n↘ℓor 1ℓ↙n equals to 1 d times. There-
fore, by Equation (A.30), we can say that there is a T (xi, xj)d contribution to the limiting kernel
Θ(d),(b) (xi, xj) per leaf index ℓ∈[L]. Since there are 2d leaf indices in the perfect binary tree,"
REFERENCES,0.34282807731434384,"Θ(d+1),(b) (xi, xj) = (2T (xi, xj))d.
(A.34)"
REFERENCES,0.34384537131230924,"In other words, since the number of leaves doubles with each additional depth, we can say"
REFERENCES,0.34486266531027465,"Θ(d+1),(b) (xi, xj) = 2T (xi, xj)Θ(d),(b) (xi, xj) .
(A.35)"
REFERENCES,0.34587995930824006,"A.4
CLOSED-FORM FORMULA FOR THE SCALED ERROR FUNCTION"
REFERENCES,0.34689725330620547,Since we are using the scaled error function as a decision function deﬁned as
REFERENCES,0.34791454730417093,"gm,n(wm,n, xi) = σ
 
w⊤
m,nxi
 = 1"
ERF,0.34893184130213634,"2 erf
 
αw⊤
m,nxi

+ 1"
ERF,0.34994913530010174,"2,
(A.36)"
ERF,0.35096642929806715,"T and ˙T in Theorem 1 can be calculated analytically. Closed-form solutions for the error function
(Williams, 1996; Lee et al., 2019) are known to be"
ERF,0.35198372329603256,"Terf(xi, xj) := E[erf(u⊤xi) erf(u⊤xj)] = 2"
ERF,0.35300101729399797,π arcsin
ERF,0.3540183112919634,"Σ(xi, xj)
p"
ERF,0.3550356052899288,"(Σ(xi, xi) + 0.5)(Σ(xj, xj) + 0.5) ! ,"
ERF,0.3560528992878942,(A.37)
ERF,0.3570701932858596,"˙Terf(xi, xj) := E[ ˙erf(u⊤xi) ˙erf(u⊤xj)] = 4 π
1
p"
ERF,0.358087487283825,"(1 + 2Σ(xi, xi)) (1 + 2Σ(xj, xj)) −4Σ(xi, xj)2 ."
ERF,0.35910478128179046,(A.38)
ERF,0.36012207527975587,Published as a conference paper at ICLR 2022
ERF,0.3611393692777213,"Using the above equations, we can calculate T and ˙T with the scaled error function as"
ERF,0.3621566632756867,"T (xi, xj) = E
1"
ERF,0.3631739572736521,"4 erf(αu⊤xi) erf(αu⊤xj)

+ E
1"
ERF,0.3641912512716175,4 erf(αu⊤xi) + 1
ERF,0.3652085452695829,"4 erf(αu⊤xj)

+ 1 4 = 1"
E,0.3662258392675483,"4E

erf(αu⊤xi) erf(αu⊤xj)

+ 1 4 = 1"
E,0.3672431332655137,2π arcsin
E,0.36826042726347913,"α2Σ(xi, xj)
p"
E,0.36927772126144454,"(α2Σ(xi, xi) + 0.5)(α2Σ(xj, xj) + 0.5) ! + 1"
E,0.37029501525940994,"4,
(A.39)"
E,0.3713123092573754,"˙T (xi, xj) = α2"
"E
H",0.3723296032553408,"4 E
h
˙erf(αu⊤xi) ˙erf(αu⊤xj)
i = α2 π
1
p"
"E
H",0.3733468972533062,"(1 + 2α2Σ(xi, xi)) (1 + 2α2Σ(xj, xj)) −4α4Σ(xi, xj)2 .
(A.40)"
"E
H",0.3743641912512716,"B
NEURAL TANGENT KERNEL FOR MULTI-LAYER PERCEPTRON"
"E
H",0.37538148524923703,"B.1
EQUIVALENCE BETWEEN THE TWO-LAYER PERCEPTRON AND TREES OF DEPTH 1"
"E
H",0.37639877924720244,"In the following, we describe a two-layer perceptron using the same symbols used in soft trees
(Section 2.1) to make it easier to see the correspondences between a two-layer perceptron and a soft
tree ensemble. A two-layer perceptron is given as"
"E
H",0.37741607324516785,"fMLP (2)(xi, w, a) =
1
√ M M
X"
"E
H",0.37843336724313326,"m=1
amσ
 
w⊤
mxi

,
(B.1)"
"E
H",0.37945066124109866,"where we use M as the number of the hidden layer nodes, σ as a nonlinear activation function, and
w = (w1, . . . , wM) ∈RF ×M and a = (a1, . . . , aM) ∈R1×M as parameters at the ﬁrst and second
layers initialized by zero-mean Gaussians with unit variances. Since"
"E
H",0.38046795523906407,"∂fMLP (2)(xi, w, a)"
"E
H",0.3814852492370295,"∂wm
=
1
√"
"E
H",0.38250254323499494,"M
amxi ˙σ
 
w⊤
mxi

,
(B.2)"
"E
H",0.38351983723296035,"∂fMLP (2)(xi, w, a)"
"E
H",0.38453713123092575,"∂am
=
1
√"
"E
H",0.38555442522889116,"M
σ
 
w⊤
mxi

,
(B.3)"
"E
H",0.38657171922685657,we have
"E
H",0.387589013224822,"bΘMLP (2)(xi, xj) = 1 M M
X m=1 "
"E
H",0.3886063072227874,"
a2
mx⊤
i xj ˙σ
 
w⊤
mxi

˙σ
 
w⊤
mxj
"
"E
H",0.3896236012207528,"|
{z
}
contribution from the ﬁrst layer"
"E
H",0.3906408952187182,"+ σ
 
w⊤
mxi

σ
 
w⊤
mxj
"
"E
H",0.3916581892166836,"|
{z
}
contribution from the second layer  
. (B.4)"
"E
H",0.392675483214649,"Considering the inﬁnite width limit (M →∞), we have"
"E
H",0.3936927772126144,"ΘMLP (2)(xi, xj) = Σ(xi, xj) ˙T (xi, xj) + T (xi, xj),
(B.5)"
"E
H",0.3947100712105799,which is the same as the limiting TNTK shown in Equation (7) with d = 1 up to constant multiple.
"E
H",0.3957273652085453,"B.2
FORMULA FOR THE MLP-INDUCED NTK"
"E
H",0.3967446592065107,"Based on Arora et al. (2019), we deﬁned the L-hidden-layer perceptron3 as"
"E
H",0.3977619532044761,"fMLP (L)(xi, W ) = W (L+1) ·
1
√ML
σ "
"E
H",0.3987792472024415,"W (L) ·
1
p"
"E
H",0.3997965412004069,"ML−1
σ

W (L−1) . . .
1
√M1
σ

W (1)xi
! , (B.6)"
"E
H",0.4008138351983723,"where W (1) ∈RM1×F , W (h) ∈RMh×Mh−1, and W (L+1) ∈R1×ML are trainable parameters.
We initialize all the weights W =
 
W (1), . . . , W (L+1)
to values independently drawn from the"
"E
H",0.40183112919633773,3Note that the two-layer perceptron is the single-hidden-layer perceptron.
"E
H",0.40284842319430314,Published as a conference paper at ICLR 2022
"E
H",0.40386571719226855,"standard normal distribution. Considering the limit of the inﬁnite width M1, M2, . . . , ML →∞, the
formula for the limiting NTK of L-hidden-layer MLP is known to be"
"E
H",0.40488301119023395,"ΘMLP(L) (xi, xj) = L+1
X h=1 "
"E
H",0.4059003051881994,"Σ(h−1) (xi, xj) · L+1
Y"
"E
H",0.4069175991861648,"h′=h
˙Σ(h′) (xi, xj) !"
"E
H",0.40793489318413023,",
(B.7)"
"E
H",0.40895218718209564,"where
Σ(0) (xi, xj) := x⊤
i xj,
(B.8)"
"E
H",0.40996948118006105,"Λ(h) (xi, xj) :=

Σ(h−1)(xi, xi)
Σ(h−1) (xi, xj)
Σ(h−1) (xj, xi)
Σ(h−1) (xj, xj)"
"E
H",0.41098677517802645,"
∈R2×2,
(B.9)"
"E
H",0.41200406917599186,"Σ(h) (xi, xj) := Eu,v∼Normal(0,Λ(h)(xi,xj)) [σ(u)σ (v))] ,
(B.10)"
"E
H",0.41302136317395727,"˙Σ(h) (xi, xj) := Eu,v∼Normal(0,Λ(h)(xi,xj)) [ ˙σ(u) ˙σ (v))] .
(B.11)"
"E
H",0.4140386571719227,"We let ˙Σ(L+1) (xi, xj) := 1 for convenience. See Arora et al. (2019) for derivation. There is a
correspondence between Σ(1) (xi, xj) and Σ (xi, xj) in Theorem 1, Σ(1) (xi, xj) and T (xi, xj) in
Theorem 1, and ˙Σ(1) (xi, xj) and ˙T (xi, xj) in Theorem 1, respectively."
"E
H",0.4150559511698881,"Since the recursive calculation is needed in Equation (B.7), the computational cost increases as the
layers get deeper. It can be seen that the effect of increasing depth is different from that of the limiting
TNTK, in which the depth of the tree affects only the value of the exponential power as shown in
Equation (7). Therefore, for any tree depth larger than 1, the limiting NTK induced by the MLP with
any number of layers and the limiting TNTK do not match."
"E
H",0.4160732451678535,"C
PROOF OF PROPOSITION 1"
"E
H",0.4170905391658189,"Proof. As shown in Section 4.1.1, there is a close correspondence between the soft tree ensemble of
depth 1 and the two-layer perceptron. On one hand, from Equation (A.7), the limiting TNTK induced
by inﬁnite trees with the depth of 1 is 2(Σ(xi, xj) ˙T (xi, xj) + T (xi, xj)). On the other hand, if the
activation function used in the two-layer perceptron is same as σ deﬁned in Equation (5), the NTK
induced by the inﬁnite width two-layer MLP is Σ(xi, xj) ˙T (xi, xj) + T (xi, xj) (Jacot et al., 2018;
Lee et al., 2019). Hence these are exactly the same kernel up to constant multiple."
"E
H",0.41810783316378436,The conditions under which the MLP-induced NTK are positively deﬁnite have already been studied.
"E
H",0.41912512716174977,"Lemma 4 (Jacot et al. (2018)). For a non-polynomial Lipschitz nonlinearity σ, for any input
dimension F, the NTK induced by the inﬁnite width MLP is positive deﬁnite if ∥xi∥2= 1 for all
i ∈[N] and xi ̸= xj (i ̸= j)."
"E
H",0.4201424211597152,"Note that σ deﬁned in Equation (5) has the non-polynomial Lipschitz nonlinearity. Since the positive
deﬁnite kernel multiplied by a constant is a positive deﬁnite kernel, it follows that the limiting TNTK
Θ(1)(xi, xj) for the depth 1 is also positive deﬁnite."
"E
H",0.4211597151576806,"As shown in Equation (C.1), as the trees get deeper, T (xi, xj) deﬁned in Equation (8) is multiplied
multiple times in the limiting TNTK:
Θ(d)(xi, xj) = 2dd Σ(xi, xj)(T (xi, xj))d−1 ˙T (xi, xj)
|
{z
}
contribution from inner nodes"
"E
H",0.422177009155646,"+ (2T (xi, xj))d
|
{z
}
contribution from leaves
= 2(2T (xi, xj))d−1 (d Σ(xi, xj) ˙T (xi, xj) + T (xi, xj))
|
{z
}
NTK induced by two-layer perceptron (if d = 1)"
"E
H",0.4231943031536114,".
(C.1)"
"E
H",0.4242115971515768,"The positive deﬁniteness of T (xi, xj) has already been proven."
"E
H",0.4252288911495422,"Lemma 5 (Jacot et al. (2018)). For a non-polynomial Lipschitz nonlinearity σ, for any input
dimension F, the T (xi, xj) := E[σ(u⊤xi)σ(u⊤xj)] deﬁned in Theorem 1 is positive deﬁnite if
∥xi∥2= 1 for all i ∈[N] and xi ̸= xj (i ̸= j)."
"E
H",0.4262461851475076,"Note that d Σ(xi, xj) ˙T (xi, xj) + T (xi, xj) for d ∈N is positive deﬁnite. Since the product of the
positive deﬁnite kernel is positive deﬁnite, for inﬁnite trees of arbitrary depth, the positive deﬁniteness
of Θ(d)(xi, xj) holds under the same conditions as in MLP."
"E
H",0.427263479145473,Published as a conference paper at ICLR 2022
"E
H",0.42828077314343843,"D
PROOF OF THEOREM 2"
"E
H",0.42929806714140384,Proof. We use the following lemmas in the proof.
"E
H",0.4303153611393693,"Lemma 6. Let a ∈Rn be a random vector whose entries are independent standard normal random
variables. For every v ≥0, with probability at least 1 −2ne(−v2n/2) we have:"
"E
H",0.4313326551373347,"∥a∥1≤vn.
(D.1)"
"E
H",0.4323499491353001,"Lemma 7. Let ai ∈R≥0. We have n
X i=1"
"E
H",0.4333672431332655,√ai ≤√n
"E
H",0.43438453713123093,"v
u
u
t n
X"
"E
H",0.43540183112919634,"i=1
ai.
(D.2)"
"E
H",0.43641912512716174,"In addition, our proof is based on the strategy used in Lee et al. (2019), which relies on the local
Lipschitzness of the model Jacobian at initialization J(x, θ), whose (i, j) entry is ∂f(xi,θ)"
"E
H",0.43743641912512715,"∂θj
where θj
is a j-th component of θ:"
"E
H",0.43845371312309256,"Theorem 4 (Lee et al. (2019)). Assume that the limiting NTK induced by any model architecture is
positive deﬁnite for input sets x, such that minimum eigenvalue of the NTK λmin > 0. For models
with local Lipschitz Jacobian trained under gradient ﬂow with a learning rate η < 2(λmin + λmax),
we have with high probability:"
"E
H",0.43947100712105797,"sup
bΘ∗
τ (xi, xj) −bΘ∗
0 (xi, xj)
 = O

1
√ M"
"E
H",0.4404883011190234,"
.
(D.3)"
"E
H",0.44150559511698884,"It is not obvious whether or not the soft tree ensemble’s Jacobian is local Lipschitz. Therefore, we
prove Lemma 8 to prove Theorem 2."
"E
H",0.44252288911495424,"Lemma 8. For soft tree ensemble models with the NTK initialization and a positive ﬁnite scaling
factor α, there is K > 0 such that for every C > 0, with high probability, the following holds:

∥J(x, θ)∥F
≤K
∥J(x, θ) −J(x, ˜θ)∥F
≤K∥θ −˜θ∥2 , ∀θ, ˜θ ∈B (θ0, C) ,
(D.4) where"
"E
H",0.44354018311291965,"B (θ0, C) := {θ : ∥θ −θ0∥2 < C} .
(D.5)"
"E
H",0.44455747711088506,"By proving that the soft tree ensemble’s Jacobian under the NTK initialization is the local Lipschitz
with high probability, we extend Theorem 2 for the TNTK."
"E
H",0.44557477110885046,"D.1
PROOF OF LEMMA 6"
"E
H",0.44659206510681587,"Proof. By use of the Chebyshev’s inequality, for some constant c, we obtain"
"E
H",0.4476093591047813,P(∥a∥1> c) ≤E[eγ∥a∥1]/eγc
"E
H",0.4486266531027467,"=

eγ2/2(1 + erf(γ/
√"
"E
H",0.4496439471007121,"2))
n
/eγc,
(D.6)"
"E
H",0.4506612410986775,"where P means a probability. Since erf(γ/
√"
"E
H",0.4516785350966429,"2) ≤1, when we use γ = c/n, we get"
"E
H",0.4526958290946083,"P(∥a∥1> c) ≤2ne(−c2/2n).
(D.7)"
"E
H",0.4537131230925738,Lemma 6 can be obtained by assigning vn to c.
"E
H",0.4547304170905392,"Figure 8 shows the right-hand side of the Equation (D.7) with c = 5n. when n = 1, probability is
7.45 × 10−6. As n becomes larger, the probability becomes even smaller."
"E
H",0.4557477110885046,Published as a conference paper at ICLR 2022
"E
H",0.45676500508647,"10
0
10
1 n"
"E
H",0.4577822990844354,"10
136"
"E
H",0.4587995930824008,"10
116 10
96 10
76 10
56 10
36 10
16"
"E
H",0.4598168870803662,Probability
"E
H",0.46083418107833163,"Figure 8: Right-hand side of the Equation (D.7), where c = 5n (in other words, v = 5 in Lemma 6)."
"E
H",0.46185147507629704,"D.2
PROOF OF LEMMA 7"
"E
H",0.46286876907426244,"Proof. By use of Cauchy-Schwarz inequality, for p, q, x, y ∈R≥0, we have"
"E
H",0.46388606307222785,"p√x + q√y ≤
p"
"E
H",0.4649033570701933,"(p2 + q2) (x + y).
(D.8)"
"E
H",0.4659206510681587,"With Equation (D.8), we prove the lemma by induction. In the base case,
√a1 + √a2 ≤
√"
"E
H",0.4669379450661241,"2√a1 + a2,
(D.9)"
"E
H",0.46795523906408953,"which is consistent to the lemma. Next, when we assume
√a1 + · · · + √ak ≤
√"
"E
H",0.46897253306205494,"k√a1 + · · · ak,
(D.10)"
"E
H",0.46998982706002035,"we have
√a1 + · · · + √ak + √ak+1 = (√a1 + · · · + √ak) + √ak+1 ≤
√"
"E
H",0.47100712105798576,"k√a1 + · · · + ak + √ak+1 ≤
√"
"E
H",0.47202441505595116,"k + 1
p"
"E
H",0.47304170905391657,"a1 + · · · + ak + ak+1.
(D.11)"
"E
H",0.474059003051882,"D.3
PROOF OF LEMMA 8"
"E
H",0.4750762970498474,Proof. Consider the contribution of the leaf parameters at ﬁrst:
"E
H",0.4760935910478128,"∂f(xi, w, π)"
"E
H",0.47711088504577825,"∂πm,ℓ
=
1
√"
"E
H",0.47812817904374366,"M
µm,ℓ(xi, wm).
(D.12)"
"E
H",0.47914547304170907,"Next, the contribution from the leaf parameters is"
"E
H",0.4801627670396745,"∂f(xi, w, π)"
"E
H",0.4811800610376399,"∂wm,n
=
1
√ M L
X"
"E
H",0.4821973550356053,"ℓ=1
πm,ℓ
∂µm,ℓ(xi, wm) ∂wm,n =
1
√ M L
X"
"E
H",0.4832146490335707,"ℓ=1
πm,ℓSn,ℓ(xi, wm)xi ˙σ
 
w⊤
m,nxi

,
(D.13) where"
"E
H",0.4842319430315361,"Sn,ℓ(x, wm) := N
Y"
"E
H",0.4852492370295015,"n′=1
σ
 
w⊤
m,n′xi
1(ℓ↙n′)&(n̸=n′)  
1 −σ
 
w⊤
m,n′xi
1(n′↘ℓ)&(n̸=n′)
!"
"E
H",0.4862665310274669,"(−1)1n↘ℓ,"
"E
H",0.4872838250254323,(D.14)
"E
H",0.4883011190233978,Published as a conference paper at ICLR 2022
"E
H",0.4893184130213632,"and & is a logical conjunction. For any real scalar p and q, the scaled error function σ deﬁned in
Equation (5) is bounded as follows:"
"E
H",0.4903357070193286,"0 ≤σ(p) ≤1,
|σ(p) −σ(q)| ≤|p −q|,
0 ≤˙σ(p) ≤α,
| ˙σ(p) −˙σ(q)| ≤α|p −q|. (D.15)"
"E
H",0.491353001017294,"Therefore, the absolute value of Sn,ℓdoes not exceed 1. With Equation (D.15), we can obtain"
"E
H",0.4923702950152594,"∂µm,ℓ(xi, wm) ∂wm,n"
"E
H",0.4933875890132248,"2
=
Sn,ℓ(xi, wm)xi ˙σ
 
w⊤
m,nxi

2 ≤α∥xi∥2
(D.16)"
"E
H",0.49440488301119023,"with high probability. Therefore, with Lemma 6, in probability,"
"E
H",0.49542217700915564,"∥J(x, θ)∥2
F = N
X i=1"
"E
H",0.49643947100712105," 
∥J(xi, w)∥2
F +∥J(xi, π)∥2
F
 = 1 M N
X i=1 M
X m=1  
N
X n=1    L
X"
"E
H",0.49745676500508645,"ℓ=1
πm,ℓ
∂µm,ℓ(xi, wm) ∂wm,n  2 2  + L
X ℓ=1"
"E
H",0.49847405900305186," 
µm,ℓ(xi, wm)2
  ≤1 M N
X i=1 M
X m=1 N
X"
"E
H",0.49949135300101727,"n=1
v2L2α2∥xi∥2
2+ L
X ℓ=1
1 ! = N
X"
"E
H",0.5005086469989827,"i=1
L(v2Lα2N∥xi∥2
2+1).
(D.17)"
"E
H",0.5015259409969481,"Next, we will consider the Jacobian difference. Since µm,ℓ(xi, wm) is a multiple multiplication of
the decision function, by use of  n
Y"
"E
H",0.5025432349949135,"i=1
pi − n
Y"
"E
H",0.503560528992879,"i=1
qi ≤ n
X"
"E
H",0.5045778229908443,"i=1
|pi −qi|
for |pi| , |qi| ≤1,
(D.18)"
"E
H",0.5055951169888098,we obtain
"E
H",0.5066124109867752,"|µm,ℓ(xi, wm) −µm,ℓ(xi, ˜wm)| =  N
Y"
"E
H",0.5076297049847406,"n=1
σ
 
w⊤
m,nxi
1ℓ↙n  
1 −σ
 
w⊤
m,nxi
1n↘ℓ − N
Y"
"E
H",0.508646998982706,"n=1
σ
  ˜w⊤
m,nxi
1ℓ↙n  
1 −σ
  ˜w⊤
m,nxi
1n↘ℓ ≤ N
X n=1"
"E
H",0.5096642929806714,"σ
 
w⊤
m,nxi
1ℓ↙n  
1 −σ
 
w⊤
m,nxi
1n↘ℓ"
"E
H",0.5106815869786369,"−σ
  ˜w⊤
m,nxi
1ℓ↙n  
1 −σ
  ˜w⊤
m,nxi
1n↘ℓ ≤ N
X"
"E
H",0.5116988809766022,"n=1
|w⊤
m,nxi −˜w⊤
m,nxi| ≤ N
X"
"E
H",0.5127161749745677,"n=1
∥xi∥2∥wm,n −˜wm,n∥2,
(D.19)"
"E
H",0.513733468972533,where it should be noted that (ℓ↙n) & (n ↘ℓ) must be false.
"E
H",0.5147507629704985,Published as a conference paper at ICLR 2022
"E
H",0.5157680569684638,"Sn,ℓ(xi, wm) −Sn,ℓ(xi, ˜wm) can be bound in the same way as Equation (D.19). Therefore, we
also obtain"
"E
H",0.5167853509664293,"∂µm,ℓ(xi, wm)"
"E
H",0.5178026449643948,"∂wm,n
−∂µm,ℓ(xi, ˜wm)"
"E
H",0.5188199389623601,"∂˜wm,n"
"E
H",0.5198372329603256,"2
= ∥Sn,ℓ(xi, wm) xi ˙σ
 
w⊤
m,nxi

−Sn,ℓ(xi, ˜wm) xi ˙σ
  ˜w⊤
m,nxi

∥2"
"E
H",0.5208545269582909,"= ∥xi∥2|Sn,ℓ(xi, wm) ˙σ
 
w⊤
m,nxi

−Sn,ℓ(xi, ˜wm) ˙σ
  ˜w⊤
m,nxi

|"
"E
H",0.5218718209562564,"≤∥xi∥2

|(Sn,ℓ(xi, wm) −Sn,ℓ(xi, ˜wm)) ˙σ
 
w⊤
m,nxi

|"
"E
H",0.5228891149542217,"+ |( ˙σ
 
w⊤
m,nxi

−˙σ
  ˜w⊤
m,nxi

)Sn,ℓ(xi, ˜wm) |
"
"E
H",0.5239064089521872,"≤∥xi∥2

|α(Sn,ℓ(xi, wm) −Sn,ℓ(xi, ˜wm))|"
"E
H",0.5249237029501526,"+ |(α(w⊤
m,nxi −˜w⊤
m,nxi))|
"
"E
H",0.525940996948118,"≤2α∥xi∥2
2 N
X"
"E
H",0.5269582909460834,"n=1
∥wm,n −˜wm,n∥2.
(D.20)"
"E
H",0.5279755849440488,"To link Equation (D.19) and Equation (D.20) to the ∥θ−˜θ∥2, we use Lemma 7 to obtain the following
inequalities: N
X"
"E
H",0.5289928789420142,"n=1
∥wm,n −˜wm,n∥2 ≤
√"
"E
H",0.5300101729399797,"N∥wm −˜wm∥2≤
√"
"E
H",0.5310274669379451,"N∥θ −˜θ∥2,
(D.21) L
X"
"E
H",0.5320447609359105,"ℓ=1
|πm,ℓ−˜πm,ℓ| ≤
√"
"E
H",0.5330620549338759,"L∥πm −˜πm∥2≤
√"
"E
H",0.5340793489318413,"L∥θ −˜θ∥2.
(D.22)"
"E
H",0.5350966429298067,"With Equation (D.1), Equation (D.16), Equation (D.19), Equation (D.20), Equation (D.21), and
Equation (D.22),"
"E
H",0.5361139369277721,"∥J(x, θ) −J(x, ˜θ)∥2
F = N
X"
"E
H",0.5371312309257376,"i=1
(∥J(xi, w) −J(xi, ˜w)∥2
F +∥J(xi, π) −J(xi, ˜π)∥2
F ) = 1 M N
X i=1 M
X m=1 N
X n=1  L
X ℓ=1"
"E
H",0.5381485249237029,"
πm,ℓ
∂µm,ℓ(xi, wm)"
"E
H",0.5391658189216684,"∂wm,n
−˜πm,ℓ
∂µm,ℓ(xi, ˜wm)"
"E
H",0.5401831129196337,"∂˜wm,n  2 2 ! + L
X"
"E
H",0.5412004069175992,"ℓ=1
(µm,ℓ(xi, wm) −µm,ℓ(xi, ˜wm))2
! = 1 M N
X i=1 M
X m=1 N
X n=1  L
X ℓ=1"
"E
H",0.5422177009155646,"
(πm,ℓ−˜πm,ℓ)∂µm,ℓ(xi, wm)"
"E
H",0.54323499491353,"∂wm,n
+
∂µm,ℓ(xi, wm)"
"E
H",0.5442522889114955,"∂wm,n
−∂µm,ℓ(xi, ˜wm)"
"E
H",0.5452695829094608,"∂˜wm,n"
"E
H",0.5462868769074263,"
˜πm,ℓ  2 2 ! + L
X"
"E
H",0.5473041709053916,"ℓ=1
(µm,ℓ(xi, wm) −µm,ℓ(xi, ˜wm))2
!"
"E
H",0.5483214649033571,"Published as a conference paper at ICLR 2022 ≤1 M N
X i=1 M
X m=1 N
X n=1 L
X"
"E
H",0.5493387589013224,"ℓ=1
(|πm,ℓ−˜πm,ℓ|α∥xi∥2) + "
"E
H",0.5503560528992879,"2α∥xi∥2
2 N
X"
"E
H",0.5513733468972533,"n=1
∥wm,n −˜wm,n∥2vL !!2! + L
X ℓ=1 N
X"
"E
H",0.5523906408952187,"n=1
∥xi∥2∥wm,n −˜wm,n∥2 !2! ≤1 M N
X i=1 M
X m=1 N
X n=1 √"
"E
H",0.5534079348931842,"L∥θ −˜θ∥2α∥xi∥2

+

2α∥xi∥2
2
√"
"E
H",0.5544252288911495,"N∥θ −˜θ∥2vL
2
! + L
X ℓ=1 √"
"E
H",0.555442522889115,"N∥xi∥2∥θ −˜θ∥2 !2! ≤ N
X i=1 "
"E
H",0.5564598168870803,"N

α
√"
"E
H",0.5574771108850458,"L∥xi∥2+2α∥xi∥2
2
√"
"E
H",0.5584944048830112,"NvL
2
+ LN∥xi∥2
2 !"
"E
H",0.5595116988809766,"∥θ −˜θ∥2
2.
(D.23)"
"E
H",0.560528992878942,"By considering the square root of both sides in Equation (D.17) and Equation (D.23), we conclude
the proof for Lemma 8."
"E
H",0.5615462868769074,"E
PROOF OF THEOREM 3"
"E
H",0.5625635808748728,"Proof. We can use the same approach with the proof of Theorem 1. Using an incremental formula,
the output from the oblivious tree ensembles can be written as follows:"
"E
H",0.5635808748728383,"f (d)(xi, w, π) =
1
√ M M
X m=1"
"E
H",0.5645981688708036,"
σ
 
w⊤
m,txi

f (d−1)
m

xi, w(s)
m , π(l)
m
"
"E
H",0.5656154628687691,"+
 
1 −σ
 
w⊤
m,txi

f (d−1)
m

xi, w(s)
m , π(r)
m

,
(E.1)"
"E
H",0.5666327568667345,"where (s) of w(s)
m means (s)hared parameters at subtrees. Intuitively, the fundamental of Theorem 3
is that the outputs of the left subtree and right subtree are still independent with the oblivious tree
structure. Even with parameter sharing at the same depth, since the leaf parameters π are not shared,
the outputs of the left subtree and right subtree are independent."
"E
H",0.5676500508646999,"We will see that Lemma 1, 2 and 3 are also valid for oblivious tree ensembles."
"E
H",0.5686673448626653,"Correspondence to Lemma 1. To show the correspondence to Lemma 1, it is sufﬁcient to show
that Equation (A.22), Equation (A.23), Equation (A.24), and Equation (A.25) hold when"
"E
H",0.5696846388606307,"Em
h
f (d+1)
m
(xi, wm, πm)f (d+1)
m
(xj, wm, πm)
i =Em    

"
"E
H",0.5707019328585962,"
f (d)
m

xi, w(s)
m , π(l)
m

−f (d)
m

xi, w(s)
m , π(r)
m

σ(w⊤
m,txi)
|
{z
}
(A)"
"E
H",0.5717192268565615,"+ f (d)
m

xi, w(s)
m , π(r)
m
"
"E
H",0.572736520854527,"|
{z
}
(B)  

  

"
"E
H",0.5737538148524923,"
f (d)
m

xj, w(s)
m , π(l)
m

−f (d)
m

xj, w(s)
m , π(r)
m

σ(w⊤
m,txj)
|
{z
}
(C)"
"E
H",0.5747711088504578,"+ f (d)
m

xj, w(s)
m , π(r)
m
"
"E
H",0.5757884028484231,"|
{z
}
(D)  

  . (E.2)"
"E
H",0.5768056968463886,"This equation corresponds to Equation (A.21). Here, since the leaf parameters π are not shared, the
outputs of the left subtree and right subtree are still independent even with the oblivious tree structure.
Therefore, we can obtain the correspondences to Equation (A.22), Equation (A.23), Equation (A.24),
and Equation (A.25) with the same procedures."
"E
H",0.5778229908443541,Published as a conference paper at ICLR 2022
"E
H",0.5788402848423194,"Correspondence to Lemma 2. For the depth d + 1, since"
"E
H",0.5798575788402849,"∂f (d+1) (xi, w, π)"
"E
H",0.5808748728382502,"∂wm,s
= σ(w⊤
m,txi)
∂f (d)
m

xi, w(s)
m , π(r)
m
 ∂wm,s"
"E
H",0.5818921668362157,"+
 
1 −σ(w⊤
m,txi)
 ∂f (d)
m

xi, w(s)
m , π(r)
m
"
"E
H",0.582909460834181,"∂wm,s
,
(E.3)"
"E
H",0.5839267548321465,the corresponding limiting TNTK is
"E
H",0.5849440488301119,"Θ(d+1),(s) (xi, xj) = d
X"
"E
H",0.5859613428280773,"s=2
Em   "
"E
H",0.5869786368260427,"σ(w⊤
m,txi)
∂f (d)
m

xi, w(s)
m , π(r)
m
"
"E
H",0.5879959308240081,"∂wm,s
+
 
1 −σ(w⊤
m,txi)
 ∂f (d)
m

xi, w(s)
m , π(r)
m
 ∂wm,s   ⊤ "
"E
H",0.5890132248219736,"σ(w⊤
m,txj)
∂f (d)
m

xj, w(s)
m , π(r)
m
"
"E
H",0.590030518819939,"∂wm,s
+
 
1 −σ(w⊤
m,txj)
 ∂f (d)
m

xj, w(s)
m , π(r)
m
 ∂wm,s     = d
X"
"E
H",0.5910478128179044,"s=2
Em "
"E
H",0.5920651068158698, 
"E
H",0.5930824008138352,"





σ(w⊤
m,txi) "
"E
H",0.5940996948118006,"
∂f (d)
m

xi, w(s)
m , π(l)
m
"
"E
H",0.595116988809766,"∂wm,s
−
∂f (d)
m

xi, w(s)
m , π(r)
m
 ∂wm,s  "
"E
H",0.5961342828077314,"|
{z
}
(A)"
"E
H",0.5971515768056969,"+
∂f (d)
m

xi, w(s)
m , π(r)
m
"
"E
H",0.5981688708036622,"∂wm,s
|
{z
}
(B) "
"E
H",0.5991861648016277,"




 ⊤ "
"E
H",0.6002034587995931,"





σ(w⊤
m,txj) "
"E
H",0.6012207527975585,"
∂f (d)
m

xj, w(s)
m , π(l)
m
"
"E
H",0.602238046795524,"∂wm,s
−
∂f (d)
m

xj, w(s)
m , π(r)
m
 ∂wm,s  "
"E
H",0.6032553407934893,"|
{z
}
(C)"
"E
H",0.6042726347914548,"+
∂f (d)
m

xj, w(s)
m , π(r)
m
"
"E
H",0.6052899287894201,"∂wm,s
|
{z
}
(D) "
"E
H",0.6063072227873856,"




 "
"E
H",0.6073245167853509,"
. (E.4)"
"E
H",0.6083418107833164,"Since
∂f (d)
m (xi,w(s)
m ,π(r)
m )
∂wm,s
and
∂f (d)
m (xj,w(s)
m ,π(l)
m )
∂wm,s
for s = {2, 3, . . . , d} are independent to each other
and have zero-mean Gaussian distribution4, similar calculation used for Equation (A.22), Equa-"
"E
H",0.6093591047812817,"4For a single oblivious tree, the number splitting rule is d because of the parameter sharing."
"E
H",0.6103763987792472,Published as a conference paper at ICLR 2022
"E
H",0.6113936927772126,"tion (A.23), Equation (A.24), and Equation (A.25) gives"
"E
H",0.612410986775178,"Em [(A) × (C)] = T (xi, xj) Em   "
"E
H",0.6134282807731435,"
∂f (d)
m

xi, w(s)
m , π(l)
m
 ∂wm,s   ⊤"
"E
H",0.6144455747711088,"
∂f (d)
m

xj, w(s)
m , π(l)
m
 ∂wm,s   + "
"E
H",0.6154628687690743,"
∂f (d)
m

xi, w(s)
m , π(r)
m
 ∂wm,s   ⊤"
"E
H",0.6164801627670397,"
∂f (d)
m

xj, w(s)
m , π(r)
m
 ∂wm,s    , (E.5)"
"E
H",0.6174974567650051,Em [(B) × (C)] = −0.5 Em   
"E
H",0.6185147507629705,"
∂f (d)
m

xi, w(s)
m , π(r)
m
 ∂wm,s   ⊤"
"E
H",0.6195320447609359,"
∂f (d)
m

xj, w(s)
m , π(r)
m
 ∂wm,s   "
"E
H",0.6205493387589013,", (E.6)"
"E
H",0.6215666327568667,Em [(A) × (D)] = −0.5 Em   
"E
H",0.6225839267548321,"
∂f (d)
m

xi, w(s)
m , π(r)
m
 ∂wm,s   ⊤"
"E
H",0.6236012207527976,"
∂f (d)
m

xj, w(s)
m , π(r)
m
 ∂wm,s   "
"E
H",0.624618514750763,", (E.7)"
"E
H",0.6256358087487284,Em [(B) × (D)] = Em   
"E
H",0.6266531027466938,"
∂f (d)
m

xi, w(s)
m , π(r)
m
 ∂wm,s   ⊤"
"E
H",0.6276703967446592,"
∂f (d)
m

xj, w(s)
m , π(r)
m
 ∂wm,s   "
"E
H",0.6286876907426246,".
(E.8)"
"E
H",0.62970498474059,"As in the previous calculations, Equation (E.6), Equation (E.7), and Equation (E.8) cancel each other
out. As a result, we obtain"
"E
H",0.6307222787385555,"Θ(d+1),(s) (xi, xj) = 2T (xi, xj)

Θ(d),(t) (xi, xj) + Θ(d),(s) (xi, xj)

.
(E.9)"
"E
H",0.6317395727365208,"Correspondence to Lemma 3. Considering Equation (A.33), once we focus on a leaf ℓ, it is not
possible for both 11↘ℓand 1ℓ↙1 to be 1. This means that a leaf cannot belong to both the right
subtree and the left subtree. Therefore, even with the oblivious tree structure, there are no inﬂuences.
Therefore, we get exactly the same result for the Lemma 3."
"E
H",0.6327568667344863,"F
DETAILS OF NUMERICAL EXPERIMENTS"
"E
H",0.6337741607324516,"F.1
SETUP"
"E
H",0.6347914547304171,"F.1.1
DATASET ACQUISITION"
"E
H",0.6358087487283826,"We use the UCI datasets (Dua & Graff, 2017) preprocessed by Fernández-Delgado et al. (2014),
which are publicly available at http://persoal.citius.usc.es/manuel.fernandez.
delgado/papers/jmlr/data.tar.gz. Since the size of the kernel is the square of the
dataset size and too many data make training impractical, we use preprocessed UCI datasets with the
number of samples smaller than 5000. Arora et al. (2020) reported the bug in the preprocess when
the explicit training/test split is given. Therefore, we do not use that dataset with explicit training/test
split. As a consequence, 90 different datasets are available."
"E
H",0.6368260427263479,"F.1.2
KERNEL SPECIFICATIONS"
"E
H",0.6378433367243134,"TNTK. See Theorem 1 for the detailed deﬁnitions. We change the tree depth from 1 to 29 and change
α in {0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0}."
"E
H",0.6388606307222787,"MLP-induced NTK. We assume the MLP activation function as ReLU. Our implementation is
based on the publicly available code5 used in Arora et al. (2020). For detailed deﬁnitions, see Arora
et al. (2020). The hyperparameter of this kernel is the model depth. We change the depth from 1 to
29. Here, depth = 1 means there is no hidden layer in the MLP."
"E
H",0.6398779247202442,5https://github.com/LeoYu/neural-tangent-kernel-UCI
"E
H",0.6408952187182095,Published as a conference paper at ICLR 2022
"E
H",0.641912512716175,"10
2
10
1
10
0
10
1"
"E
H",0.6429298067141404,used in RBF kernel 71 72 73 74 75 76
"E
H",0.6439471007121058,Averaged accuracy
"E
H",0.6449643947100712,Figure 9: The γ dependency of the RBF kernel performance.
"E
H",0.6459816887080366,"RBF kernel. We use scikit-learn implementation6. The hyperparameter of this kernel is γ, in-
verse of the standard deviation of the RBF kernel (Gaussian function). For Figure 3, we tune γ in
{0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0,
3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20.0, 30.0}, resulting in 30 candidates in total. In our experi-
ments, γ = 2.0 performs the best on average (Figure 9)."
"E
H",0.646998982706002,"F.1.3
MODEL SPECIFICATIONS"
"E
H",0.6480162767039674,"We used kernel regression implemented in scikit-learn7. To consider ridge-less situation, regulariza-
tion strength is set to be 1.0 × 10−8, a very small constant."
"E
H",0.6490335707019329,"F.1.4
COMPUTATIONAL COSTS"
"E
H",0.6500508646998983,"Since the training and inference algorithms of the kernel regression are common across different
kernels, we analyze the computational cost of computing a single value in a gram matrix of the
corresponding kernels in the following. The time complexity of the MLP-induced NTK is linear
with respect to the layer depth, while that of the TNTK remains to be constant. Such a trend can be
seen in the right panel of Figure 7. For the RBF kernel, the computational cost remains the same
with respect to changes in hyperparameters, thus its trend is similar to the TNTK. In terms of the
space complexity, when considering a multi-layered MLP, since it is not necessary to store all past
calculation results in memory during the recursive computation, the MLP-induced NTK computation
consumes a certain amount of memory regardless of the depth of the layers. Therefore, the memory
usage is almost the same across the RBF kernel, TNTK, and MLP-induced NTK."
"E
H",0.6510681586978637,"F.1.5
COMPUTATIONAL RESOURCE"
"E
H",0.6520854526958291,"We used Ubuntu Linux (version: 4.15.0-117-generic) and ran all experiments on 2.20 GHz Intel Xeon
E5-2698 CPU and 252 GB of memory."
"E
H",0.6531027466937945,"F.2
RESULTS"
"E
H",0.6541200406917599,"F.2.1
STATISTICAL SIGNIFICANCE OF THE PARAMETER DEPENDENCY"
"E
H",0.6551373346897253,"A Wilcoxon signed rank test is conducted to check the statistical signiﬁcance of the differences
between different α. Figure 10 shows the p-values for the depth of 3 and 20. As shown in Figure 7,
when the tree is shallow, the accuracy started to deteriorate after around α = 8.0, but as the tree
becomes deeper, the deterioration became less apparent. Therefore, statistically signiﬁcantly different
pairs for deep tress and shallow trees are different. When the tree is deep, large α shows a signiﬁcant
difference over those with small α. However, when the tree is shallow, the best performance is
achieved with α of about 8.0, and if α is too large, the performance deteriorates predominantly."
"E
H",0.6561546286876907,"6https://scikit-learn.org/stable/modules/generated/sklearn.metrics.
pairwise.rbf_kernel.html
7https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.
KernelRidge.html"
"E
H",0.6571719226856562,Published as a conference paper at ICLR 2022
"E
H",0.6581892166836215,"0.5
1.0
2.0
4.0
8.0
16.0
32.0
64.0"
"E
H",0.659206510681587,"0.5
1.0
2.0
4.0
8.0
16.0
32.0
64.0 0"
"E
H",0.6602238046795524,"0
0.01"
"E
H",0.6612410986775178,"0
0.08
0.9"
"E
H",0.6622583926754833,"0.01
0.29
0.98
0.51"
"E
H",0.6632756866734486,"0.06
0.94
0.2
0.01
0"
"E
H",0.6642929806714141,"0.2
0.48
0.03
0
0
0"
"E
H",0.6653102746693794,"0.57
0.19
0.01
0
0
0
0 d=3 0.0 0.2 0.4 0.6 0.8"
"E
H",0.6663275686673449,"0.5
1.0
2.0
4.0
8.0
16.0
32.0
64.0"
"E
H",0.6673448626653102,"0.5
1.0
2.0
4.0
8.0
16.0
32.0
64.0 0"
"E
H",0.6683621566632757,"0
0.01"
"E
H",0.669379450661241,"0
0.03
0.24"
"E
H",0.6703967446592065,"0
0.06
0.46
0.83"
"E
H",0.671414038657172,"0.01
0.18
0.84
0.63
0.75"
"E
H",0.6724313326551373,"0.01
0.21
0.92
0.59
0.61
0.36"
"E
H",0.6734486266531028,"0.01
0.19
0.86
0.88
0.88
0.81
0.72 d=20 0.0 0.2 0.4 0.6 0.8"
"E
H",0.6744659206510681,Figure 10: P-values of the Wilcoxon signed rank test for different pairs of α.
"E
H",0.6754832146490336,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.676500508646999,MLP-induced NTK =0.5
"E
H",0.6775178026449644,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.6785350966429298,MLP-induced NTK =1.0
"E
H",0.6795523906408952,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.6805696846388606,MLP-induced NTK =2.0
"E
H",0.681586978636826,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.6826042726347915,MLP-induced NTK =4.0
"E
H",0.6836215666327569,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.6846388606307223,MLP-induced NTK =8.0
"E
H",0.6856561546286877,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.6866734486266531,MLP-induced NTK =16.0
"E
H",0.6876907426246185,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.688708036622584,MLP-induced NTK =32.0
"E
H",0.6897253306205493,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100"
"E
H",0.6907426246185148,MLP-induced NTK =64.0
"E
H",0.6917599186164801,"Figure 11: Performance comparisons between the kernel regression with MLP-induced NTK and the
TNTK on the UCI dataset."
"E
H",0.6927772126144456,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =0.5"
"E
H",0.6937945066124109,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =1.0"
"E
H",0.6948118006103764,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =2.0"
"E
H",0.6958290946083419,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =4.0"
"E
H",0.6968463886063072,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =8.0"
"E
H",0.6978636826042727,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =16.0"
"E
H",0.698880976602238,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =32.0"
"E
H",0.6998982706002035,"0
20
40
60
80
100
TNTK 0 20 40 60 80 100 RBF =64.0"
"E
H",0.7009155645981688,"Figure 12: Performance comparisons between the kernel regression with RBF kernel and the TNTK
on the UCI dataset."
"E
H",0.7019328585961343,"10
0
10
1
0.9650"
"E
H",0.7029501525940997,0.9675
"E
H",0.7039674465920651,0.9700
"E
H",0.7049847405900305,0.9725
"E
H",0.7060020345879959,0.9750
"E
H",0.7070193285859614,0.9775
"E
H",0.7080366225839267,0.9800
"E
H",0.7090539165818922,Pearson's correlation coefficient
"E
H",0.7100712105798576,"RBF
MLP-induced NTK"
"E
H",0.711088504577823,Figure 13: Pearson’s correlation coefﬁcients with predicted values of the TNTK with different α.
"E
H",0.7121057985757884,Published as a conference paper at ICLR 2022
"E
H",0.7131230925737538,"F.2.2
DATASET-WISE RESULTS"
"E
H",0.7141403865717192,"For each α, scatter-plots are shown in Figures 11 and 12. As shown in Figure 13, the correlation
coefﬁcients with the TNTK are likely to be higher for the MLP-induced NTK than for the RBF kernel.
Tables 2, 3 and 4 are dataset-wise results of the comparison between the TNTK, the MLP-induced
NTK, and the RBF kernel. For each α, depth is tuned for each dataset. In terms of the depth, the
best performers from 1 to 29 are compared with the TNTK and the MLP-induced NTK. For the RBF
kernel, γ is tuned in each dataset from 30 candidate values as described in Section F.1.2. Therefore,
the number of tunable parameters is the same across all methods. All parameter-wise results are
visualized in Figures 14 and 15."
"E
H",0.7151576805696847,Published as a conference paper at ICLR 2022
"E
H",0.71617497456765,"name
size
α=0.5
α=1.0
α=2.0
α=4.0
α=8.0
α=16.0
α=32.0
α=64.0
MLP-NTK
RBF"
TRAINS,0.7171922685656155,"0
trains
10
87.500
87.500
87.500
87.500
87.500
87.500
87.500
87.500
100.000
87.500
1
balloons
16
87.500
100.000
93.750
87.500
87.500
87.500
87.500
87.500
100.000
93.750
2
lenses
24
87.500
87.500
87.500
87.500
87.500
87.500
87.500
87.500
87.500
87.500
3
lung-cancer
32
56.250
53.125
53.125
53.125
53.125
56.250
59.375
59.375
65.625
53.125
4
post-operative
90
63.636
64.773
67.045
69.318
69.318
68.182
68.182
69.318
69.318
56.818
5
pittsburg-bridges-SPAN
92
55.435
57.609
58.696
67.391
65.217
66.304
66.304
67.391
65.217
58.696
6
fertility
100
84.000
88.000
89.000
89.000
89.000
89.000
89.000
89.000
89.000
83.000
7
zoo
101
100.000
99.000
99.000
99.000
99.000
99.000
99.000
99.000
99.000
99.000
8
pittsburg-bridges-T-OR-D
102
81.000
84.000
87.000
89.000
89.000
89.000
88.000
88.000
87.000
89.000
9
pittsburg-bridges-REL-L
103
67.308
74.038
75.000
74.038
75.962
75.962
75.962
75.962
74.038
74.038
10
pittsburg-bridges-TYPE
105
57.692
59.615
64.423
66.346
66.346
66.346
66.346
65.385
68.269
59.615
11
molec-biol-promoter
106
90.385
88.462
87.500
87.500
87.500
87.500
87.500
87.500
90.385
88.462
12
pittsburg-bridges-MATERIAL
106
93.269
94.231
94.231
94.231
94.231
94.231
94.231
94.231
94.231
93.269
13
breast-tissue
106
65.385
68.269
67.308
70.192
72.115
72.115
75.000
74.038
69.231
71.154
14
αcute-nephritis
120
100.000
100.000
100.000
100.000
100.000
100.000
100.000
100.000
100.000
100.000
15
αcute-inﬂammation
120
100.000
100.000
100.000
100.000
100.000
100.000
100.000
100.000
100.000
100.000
16
heart-switzerland
123
37.097
43.548
48.387
47.581
50.000
44.355
44.355
44.355
47.581
37.903
17
echocardiogram
131
81.061
81.061
84.848
84.091
85.606
85.606
85.606
85.606
85.606
78.030
18
lymphography
148
88.514
88.514
88.514
88.514
87.838
87.162
86.486
86.486
88.514
86.486
19
iris
150
95.946
97.973
96.622
88.514
86.486
87.162
87.838
87.162
87.162
96.622
20
teaching
151
56.579
57.895
58.553
60.526
64.474
67.105
67.105
67.763
63.158
60.526
21
hepatitis
155
83.974
85.256
85.256
84.615
84.615
84.615
84.615
85.256
83.974
85.256
22
wine
178
98.864
99.432
98.864
98.864
98.864
98.295
98.295
97.727
99.432
98.295
23
planning
182
62.222
65.556
70.000
71.667
71.667
72.222
72.222
72.222
72.222
67.222
24
ﬂags
194
52.083
53.646
53.646
53.125
53.646
53.125
53.125
53.125
53.646
49.479
25
parkinsons
195
93.878
94.388
92.857
93.367
92.857
92.857
93.367
93.367
93.878
95.408
26
breast-cancer-wisc-prog
198
82.143
83.673
83.673
83.673
83.673
83.673
83.673
83.673
85.204
78.571
27
heart-va
200
31.000
34.000
36.000
36.000
37.500
39.000
40.500
43.000
36.500
29.000
28
conn-bench-sonar-mines-rocks
208
86.538
86.538
87.019
86.538
86.538
86.538
86.538
86.538
87.981
87.500
29
seeds
210
90.865
93.750
93.269
91.827
92.308
92.308
91.827
91.827
96.154
95.673"
TRAINS,0.7182095625635809,Table 2: Comparison between TNTK and MLP-induced NTK for a half of the dataset (1/3).
TRAINS,0.7192268565615463,Published as a conference paper at ICLR 2022
TRAINS,0.7202441505595117,"name
size
α=0.5
α=1.0
α=2.0
α=4.0
α=8.0
α=16.0
α=32.0
α=64.0
MLP-NTK
RBF"
GLASS,0.7212614445574771,"30
glass
214
60.849
67.453
70.755
70.755
71.698
71.698
71.226
71.226
70.283
69.811
31
statlog-heart
270
83.209
87.313
87.687
88.433
88.433
88.433
87.687
87.313
86.567
82.463
32
breast-cancer
286
64.789
67.254
69.718
70.423
72.887
74.648
75.000
75.000
71.831
65.845
33
heart-hungarian
294
83.219
83.904
84.247
84.589
85.616
85.274
85.274
85.274
85.616
82.877
34
heart-cleveland
303
55.263
58.224
58.882
59.211
59.539
58.553
59.211
59.539
57.895
53.618
35
haberman-survival
306
59.539
61.513
61.842
66.447
68.421
71.711
73.684
73.684
71.053
70.395
36
vertebral-column-2clases
310
69.805
77.273
78.571
80.844
82.792
84.091
84.091
82.792
83.117
81.494
37
vertebral-column-3clases
310
71.753
78.247
81.169
80.844
80.844
81.818
81.494
81.169
81.818
81.169
38
primary-tumor
330
47.561
50.305
52.134
53.049
52.744
50.610
50.305
50.000
52.134
45.427
39
ecoli
336
71.429
79.167
83.036
84.524
86.012
86.607
86.905
86.905
85.417
81.250
40
ionosphere
351
90.057
91.477
90.341
87.784
88.352
88.352
88.352
88.352
91.761
92.330
41
libras
360
82.778
81.389
80.556
80.833
81.111
80.833
80.833
80.833
83.889
85.278
42
dermatology
366
97.802
97.802
97.527
97.527
97.253
97.253
97.253
97.253
97.802
97.253
43
congressional-voting
435
61.697
61.697
61.927
61.927
61.927
61.697
61.697
61.697
61.697
62.156
44
αrrhythmia
452
69.469
65.265
64.602
64.823
64.823
64.823
64.823
64.823
71.239
69.248
45
musk-1
476
89.076
89.076
89.076
89.286
89.286
89.076
89.076
89.076
89.706
90.756
46
cylinder-bands
512
79.883
78.125
78.125
78.320
78.516
78.320
78.320
78.320
80.273
79.688
47
low-res-spect
531
91.729
91.353
90.602
89.474
88.534
87.782
87.218
87.218
91.353
90.226
48
breast-cancer-wisc-diag
569
96.127
96.655
97.359
97.359
97.359
96.831
96.479
96.479
97.007
95.599
49
ilpd-indian-liver
583
64.897
69.521
70.719
72.260
71.062
71.747
72.603
72.603
71.918
70.377
50
synthetic-control
600
99.333
99.333
99.167
98.833
98.333
97.833
97.000
96.667
98.833
99.333
51
balance-scale
625
81.250
84.615
88.782
89.904
91.346
90.064
85.256
85.256
93.269
90.865
52
statlog-australian-credit
690
59.012
60.610
64.099
66.279
67.151
68.023
68.023
68.023
66.279
59.302
53
credit-approval
690
82.558
85.174
86.628
87.209
87.645
87.791
87.791
87.355
87.064
81.686
54
breast-cancer-wisc
699
96.286
97.286
97.857
97.857
98.000
98.000
98.000
98.000
98.000
96.714
55
blood
748
67.513
65.775
63.369
69.786
72.727
73.529
75.802
77.005
74.064
78.075
56
energy-y2
768
89.583
89.453
88.021
87.891
88.151
87.630
87.630
87.630
88.281
90.755
57
pima
768
68.229
70.182
71.354
73.307
76.042
76.302
77.083
76.693
75.000
69.661
58
energy-y1
768
93.750
93.620
93.229
90.495
90.234
90.104
90.234
90.234
92.708
96.484
59
statlog-vehicle
846
78.318
77.014
76.540
73.578
72.986
72.156
72.038
72.038
81.398
77.488"
GLASS,0.7222787385554426,Table 3: Comparison between TNTK and MLP-induced NTK for a half of the dataset (2/3).
GLASS,0.7232960325534079,Published as a conference paper at ICLR 2022
GLASS,0.7243133265513734,"name
size
α=0.5
α=1.0
α=2.0
α=4.0
α=8.0
α=16.0
α=32.0
α=64.0
MLP-NTK
RBF"
GLASS,0.7253306205493387,"60
oocytes_trisopterus_nucleus_2f
912
82.456
82.566
82.456
82.675
80.811
78.728
78.070
77.851
84.978
79.605
61
oocytes_trisopterus_states_5b
912
92.325
92.982
93.640
93.092
91.667
90.022
89.693
89.693
94.189
91.228
62
tic-tac-toe
958
99.268
99.163
99.268
99.268
99.268
99.268
99.268
99.268
98.640
100.000
63
mammographic
961
72.708
71.250
72.083
75.625
77.604
78.854
78.958
79.271
80.000
78.750
64
statlog-german-credit
1000
75.200
76.500
77.800
77.300
76.200
75.500
75.500
75.400
77.500
73.700
65
led-display
1000
72.400
72.300
72.600
72.500
72.300
72.500
72.300
72.500
72.900
73.000
66
oocytes_merluccius_nucleus_4d
1022
81.078
80.588
80.686
80.686
79.412
77.255
76.961
76.765
83.725
75.490
67
oocytes_merluccius_states_2f
1022
92.353
91.961
92.157
92.157
91.373
90.784
90.784
90.490
93.039
92.059
68
contrac
1473
40.082
44.293
47.147
50.068
51.155
52.038
52.514
51.155
50.272
43.207
69
yeast
1484
42.588
49.326
54.380
58.154
60.040
60.243
60.445
60.849
59.636
54.380
70
semeion
1593
93.719
93.467
93.405
93.467
93.467
93.467
93.467
93.405
96.168
95.603
71
wine-quality-red
1599
63.062
65.938
68.812
70.000
70.625
70.375
70.312
70.312
69.625
64.438
72
plant-texture
1599
83.812
81.812
79.438
77.938
77.688
77.625
77.750
77.625
86.125
85.625
73
plant-margin
1600
84.750
83.938
82.938
81.875
80.750
79.500
78.938
78.563
84.875
83.875
74
plant-shape
1600
64.812
63.562
62.438
60.375
58.312
57.062
56.375
55.937
66.250
68.000
75
car
1728
97.454
97.569
97.164
96.701
96.354
96.181
96.123
96.123
97.743
98.032
76
steel-plates
1941
76.289
77.320
77.938
77.423
77.062
76.753
76.598
76.495
78.351
75.103
77
cardiotocography-3clases
2126
92.232
92.514
92.043
91.902
91.949
91.855
91.996
91.855
93.173
92.043
78
cardiotocography-10clases
2126
80.838
82.957
82.250
80.744
79.896
79.896
79.661
79.614
84.181
79.143
79
titanic
2201
78.955
78.955
78.955
78.955
78.955
78.955
78.955
78.955
78.955
78.955
80
statlog-image
2310
96.360
96.967
97.097
96.750
96.231
95.927
95.884
95.624
97.660
96.404
81
ozone
2536
97.358
97.240
97.200
97.200
97.200
97.200
97.200
97.200
97.397
97.200
82
molec-biol-splice
3190
86.731
85.947
84.536
83.093
82.465
82.403
82.371
82.371
86.920
86.418
83
chess-krvkp
3196
99.124
98.905
98.655
97.872
96.902
95.526
95.307
95.307
99.406
98.999
84
αbalone
4177
50.407
49.880
55.532
60.010
62.548
64.200
64.943
65.086
63.410
64.152
85
bank
4521
88.628
89.336
89.358
89.513
89.358
89.159
89.181
89.159
89.735
88.142
86
spambase
4601
91.478
91.174
92.435
90.652
93.130
89.630
91.478
93.348
94.913
90.652
87
wine-quality-white
4898
63.623
66.810
67.545
68.791
69.158
68.975
68.995
68.913
69.097
65.748
88
waveform-noise
5000
86.360
86.340
86.520
86.540
86.720
86.500
85.900
85.520
86.540
85.460
89
waveform
5000
85.440
85.780
86.300
86.500
86.660
86.700
86.740
86.520
86.340
84.640"
GLASS,0.7263479145473042,Table 4: Comparison between TNTK and MLP-induced NTK for a half of the dataset (3/3).
GLASS,0.7273652085452695,Published as a conference paper at ICLR 2022
GLASS,0.728382502543235,"0
10
20
30
Depth 0.9 1.0"
GLASS,0.7293997965412004,Accuracy
GLASS,0.7304170905391658,trains
GLASS,0.7314343845371313,"0
10
20
30
Depth 0.8 1.0"
GLASS,0.7324516785350966,Accuracy
GLASS,0.7334689725330621,balloons
GLASS,0.7344862665310274,"0
10
20
30
Depth 0.6 0.8"
GLASS,0.7355035605289929,Accuracy
GLASS,0.7365208545269583,lenses
GLASS,0.7375381485249237,"0
10
20
30
Depth 0.4 0.6"
GLASS,0.7385554425228891,Accuracy
GLASS,0.7395727365208545,lung-cancer
GLASS,0.7405900305188199,"0
10
20
30
Depth 0.25 0.50"
GLASS,0.7416073245167853,Accuracy
GLASS,0.7426246185147508,post-operative
GLASS,0.7436419125127162,"0
10
20
30
Depth 0.5 0.6"
GLASS,0.7446592065106816,Accuracy
GLASS,0.745676500508647,pittsburg-bridges-SPAN
GLASS,0.7466937945066124,"0
10
20
30
Depth 0.50 0.75"
GLASS,0.7477110885045778,Accuracy
GLASS,0.7487283825025433,fertility
GLASS,0.7497456765005086,"0
10
20
30
Depth 0.9 1.0"
GLASS,0.7507629704984741,Accuracy zoo
GLASS,0.7517802644964394,"0
10
20
30
Depth 0.7 0.8 0.9"
GLASS,0.7527975584944049,Accuracy
GLASS,0.7538148524923703,pittsburg-bridges-T-OR-D
GLASS,0.7548321464903357,"0
10
20
30
Depth 0.5 0.6 0.7"
GLASS,0.7558494404883012,Accuracy
GLASS,0.7568667344862665,pittsburg-bridges-REL-L
GLASS,0.757884028484232,"0
10
20
30
Depth 0.4 0.6"
GLASS,0.7589013224821973,Accuracy
GLASS,0.7599186164801628,pittsburg-bridges-TYPE
GLASS,0.7609359104781281,"0
10
20
30
Depth 0.8 0.9"
GLASS,0.7619532044760936,Accuracy
GLASS,0.762970498474059,molec-biol-promoter
GLASS,0.7639877924720244,"0
10
20
30
Depth 0.8 0.9"
GLASS,0.7650050864699899,Accuracy
GLASS,0.7660223804679552,pittsburg-bridges-MATERIAL
GLASS,0.7670396744659207,"0
10
20
30
Depth 0.4 0.6"
GLASS,0.768056968463886,Accuracy
GLASS,0.7690742624618515,breast-tissue
GLASS,0.7700915564598169,"0
10
20
30
Depth 0.95 1.00 1.05"
GLASS,0.7711088504577823,Accuracy
GLASS,0.7721261444557477,acute-nephritis
GLASS,0.7731434384537131,"0
10
20
30
Depth 0.95 1.00 1.05"
GLASS,0.7741607324516785,Accuracy
GLASS,0.775178026449644,acute-inflammation
GLASS,0.7761953204476093,"0
10
20
30
Depth 0.4 0.5"
GLASS,0.7772126144455748,Accuracy
GLASS,0.7782299084435402,heart-switzerland
GLASS,0.7792472024415056,"0
10
20
30
Depth 0.75 0.80 0.85"
GLASS,0.780264496439471,Accuracy
GLASS,0.7812817904374364,echocardiogram
GLASS,0.7822990844354019,"0
10
20
30
Depth 0.75 0.80 0.85"
GLASS,0.7833163784333672,Accuracy
GLASS,0.7843336724313327,lymphography
GLASS,0.785350966429298,"0
10
20
30
Depth 0.85 0.90 0.95"
GLASS,0.7863682604272635,Accuracy iris
GLASS,0.7873855544252288,"0
10
20
30
Depth 0.5 0.6"
GLASS,0.7884028484231943,Accuracy
GLASS,0.7894201424211598,teaching
GLASS,0.7904374364191251,"0
10
20
30
Depth 0.7 0.8"
GLASS,0.7914547304170906,Accuracy
GLASS,0.7924720244150559,hepatitis
GLASS,0.7934893184130214,"0
10
20
30
Depth 0.96 0.98"
GLASS,0.7945066124109867,Accuracy wine
GLASS,0.7955239064089522,"0
10
20
30
Depth 0.4 0.6"
GLASS,0.7965412004069176,Accuracy
GLASS,0.797558494404883,planning
GLASS,0.7985757884028484,"0
10
20
30
Depth 0.45 0.50"
GLASS,0.7995930824008138,Accuracy flags
GLASS,0.8006103763987793,"0
10
20
30
Depth 0.7 0.8 0.9"
GLASS,0.8016276703967447,Accuracy
GLASS,0.8026449643947101,parkinsons
GLASS,0.8036622583926755,"0
10
20
30
Depth 0.6 0.8"
GLASS,0.8046795523906409,Accuracy
GLASS,0.8056968463886063,breast-cancer-wisc-prog
GLASS,0.8067141403865717,"0
10
20
30
Depth 0.3 0.4"
GLASS,0.8077314343845371,Accuracy
GLASS,0.8087487283825026,heart-va
GLASS,0.8097660223804679,"0
10
20
30
Depth"
GLASS,0.8107833163784334,"0.75
0.80
0.85"
GLASS,0.8118006103763988,Accuracy
GLASS,0.8128179043743642,conn-bench-sonar-mines-rocks
GLASS,0.8138351983723296,"0
10
20
30
Depth 0.8 0.9"
GLASS,0.814852492370295,Accuracy seeds
GLASS,0.8158697863682605,"0
10
20
30
Depth 0.6 0.7"
GLASS,0.8168870803662258,Accuracy glass
GLASS,0.8179043743641913,"0
10
20
30
Depth 0.825 0.850 0.875"
GLASS,0.8189216683621566,Accuracy
GLASS,0.8199389623601221,statlog-heart
GLASS,0.8209562563580874,"0
10
20
30
Depth 0.6 0.7"
GLASS,0.8219735503560529,Accuracy
GLASS,0.8229908443540183,breast-cancer
GLASS,0.8240081383519837,"0
10
20
30
Depth 0.80 0.85"
GLASS,0.8250254323499492,Accuracy
GLASS,0.8260427263479145,heart-hungarian
GLASS,0.82706002034588,"0
10
20
30
Depth"
GLASS,0.8280773143438453,"0.525
0.550
0.575"
GLASS,0.8290946083418108,Accuracy
GLASS,0.8301119023397762,heart-cleveland
GLASS,0.8311291963377416,"0
10
20
30
Depth 0.6 0.7"
GLASS,0.832146490335707,Accuracy
GLASS,0.8331637843336724,haberman-survival
GLASS,0.8341810783316378,"0
10
20
30
Depth 0.7 0.8"
GLASS,0.8351983723296033,Accuracy
GLASS,0.8362156663275687,vertebral-column-2clases
GLASS,0.8372329603255341,"0
10
20
30
Depth 0.6 0.8"
GLASS,0.8382502543234995,Accuracy
GLASS,0.8392675483214649,vertebral-column-3clases
GLASS,0.8402848423194303,"0
10
20
30
Depth 0.45 0.50"
GLASS,0.8413021363173957,Accuracy
GLASS,0.8423194303153612,primary-tumor
GLASS,0.8433367243133265,"0
10
20
30
Depth 0.7 0.8"
GLASS,0.844354018311292,Accuracy ecoli
GLASS,0.8453713123092573,"0
10
20
30
Depth 0.7 0.8 0.9"
GLASS,0.8463886063072228,Accuracy
GLASS,0.8474059003051883,ionosphere
GLASS,0.8484231943031536,"0
10
20
30
Depth 0.6 0.8"
GLASS,0.8494404883011191,Accuracy
GLASS,0.8504577822990844,libras
GLASS,0.8514750762970499,"0
10
20
30
Depth 0.925 0.950 0.975"
GLASS,0.8524923702950152,Accuracy
GLASS,0.8535096642929807,dermatology
GLASS,0.854526958290946,"0
10
20
30
Depth 0.575 0.600"
GLASS,0.8555442522889115,Accuracy
GLASS,0.8565615462868769,congressional-voting
GLASS,0.8575788402848423,"0
10
20
30
Depth 0.6 0.7"
GLASS,0.8585961342828077,Accuracy
GLASS,0.8596134282807731,arrhythmia
GLASS,0.8606307222787386,"=0.5
=1.0
=2.0
=4.0
=8.0
=16.0
=32.0
=64.0
MLP-induced NTK
RBF"
GLASS,0.861648016276704,Figure 14: Dataset-wise comparison for a half of the dataset (1/2).
GLASS,0.8626653102746694,Published as a conference paper at ICLR 2022
GLASS,0.8636826042726348,"0
10
20
30
Depth 0.8 0.9"
GLASS,0.8646998982706002,Accuracy
GLASS,0.8657171922685656,musk-1
GLASS,0.866734486266531,"0
10
20
30
Depth 0.7 0.8"
GLASS,0.8677517802644964,Accuracy
GLASS,0.8687690742624619,cylinder-bands
GLASS,0.8697863682604272,"0
10
20
30
Depth 0.7 0.8 0.9"
GLASS,0.8708036622583927,Accuracy
GLASS,0.8718209562563581,low-res-spect
GLASS,0.8728382502543235,"0
10
20
30
Depth 0.95 0.96 0.97"
GLASS,0.873855544252289,Accuracy
GLASS,0.8748728382502543,breast-cancer-wisc-diag
GLASS,0.8758901322482198,"0
10
20
30
Depth 0.6 0.7"
GLASS,0.8769074262461851,Accuracy
GLASS,0.8779247202441506,ilpd-indian-liver
GLASS,0.8789420142421159,"0
10
20
30
Depth 0.9 1.0"
GLASS,0.8799593082400814,Accuracy
GLASS,0.8809766022380467,synthetic-control
GLASS,0.8819938962360122,"0
10
20
30
Depth 0.50 0.75"
GLASS,0.8830111902339777,Accuracy
GLASS,0.884028484231943,balance-scale
GLASS,0.8850457782299085,"0
10
20
30
Depth"
GLASS,0.8860630722278738,"0.55
0.60
0.65"
GLASS,0.8870803662258393,Accuracy
GLASS,0.8880976602238047,statlog-australian-credit
GLASS,0.8891149542217701,"0
10
20
30
Depth 0.825 0.850 0.875"
GLASS,0.8901322482197355,Accuracy
GLASS,0.8911495422177009,credit-approval
GLASS,0.8921668362156663,"0
10
20
30
Depth 0.96 0.98"
GLASS,0.8931841302136317,Accuracy
GLASS,0.8942014242115972,breast-cancer-wisc
GLASS,0.8952187182095626,"0
10
20
30
Depth 0.6 0.7"
GLASS,0.896236012207528,Accuracy blood
GLASS,0.8972533062054934,"0
10
20
30
Depth 0.85 0.90"
GLASS,0.8982706002034588,Accuracy
GLASS,0.8992878942014242,energy-y2
GLASS,0.9003051881993896,"0
10
20
30
Depth 0.65 0.70 0.75"
GLASS,0.901322482197355,Accuracy pima
GLASS,0.9023397761953205,"0
10
20
30
Depth 0.85 0.90 0.95"
GLASS,0.9033570701932858,Accuracy
GLASS,0.9043743641912513,energy-y1
GLASS,0.9053916581892166,"0
10
20
30
Depth 0.7 0.8"
GLASS,0.9064089521871821,Accuracy
GLASS,0.9074262461851476,statlog-vehicle
GLASS,0.9084435401831129,"0
10
20
30
Depth 0.75 0.80 0.85"
GLASS,0.9094608341810784,Accuracy
GLASS,0.9104781281790437,oocytes_trisopterus_nucleus_2f
GLASS,0.9114954221770092,"0
10
20
30
Depth 0.85 0.90"
GLASS,0.9125127161749745,Accuracy
GLASS,0.91353001017294,oocytes_trisopterus_states_5b
GLASS,0.9145473041709054,"0
10
20
30
Depth 0.8 1.0"
GLASS,0.9155645981688708,Accuracy
GLASS,0.9165818921668362,tic-tac-toe
GLASS,0.9175991861648016,"0
10
20
30
Depth 0.7 0.8"
GLASS,0.9186164801627671,Accuracy
GLASS,0.9196337741607324,mammographic
GLASS,0.9206510681586979,"0
10
20
30
Depth 0.70 0.75"
GLASS,0.9216683621566633,Accuracy
GLASS,0.9226856561546287,statlog-german-credit
GLASS,0.9237029501525941,"0
10
20
30
Depth 0.71 0.72 0.73"
GLASS,0.9247202441505595,Accuracy
GLASS,0.9257375381485249,led-display
GLASS,0.9267548321464903,"0
10
20
30
Depth 0.75 0.80"
GLASS,0.9277721261444557,Accuracy
GLASS,0.9287894201424212,oocytes_merluccius_nucleus_4d
GLASS,0.9298067141403866,"0
10
20
30
Depth 0.85 0.90"
GLASS,0.930824008138352,Accuracy
GLASS,0.9318413021363174,oocytes_merluccius_states_2f
GLASS,0.9328585961342828,"0
10
20
30
Depth 0.4 0.5"
GLASS,0.9338758901322483,Accuracy
GLASS,0.9348931841302136,contrac
GLASS,0.9359104781281791,"0
10
20
30
Depth 0.4 0.6"
GLASS,0.9369277721261444,Accuracy yeast
GLASS,0.9379450661241099,"0
10
20
30
Depth 0.8 0.9"
GLASS,0.9389623601220752,Accuracy
GLASS,0.9399796541200407,semeion
GLASS,0.940996948118006,"0
10
20
30
Depth 0.6 0.7"
GLASS,0.9420142421159715,Accuracy
GLASS,0.943031536113937,wine-quality-red
GLASS,0.9440488301119023,"0
10
20
30
Depth 0.6 0.8"
GLASS,0.9450661241098678,Accuracy
GLASS,0.9460834181078331,plant-texture
GLASS,0.9471007121057986,"0
10
20
30
Depth 0.6 0.8"
GLASS,0.948118006103764,Accuracy
GLASS,0.9491353001017294,plant-margin
GLASS,0.9501525940996948,"0
10
20
30
Depth 0.25 0.50"
GLASS,0.9511698880976602,Accuracy
GLASS,0.9521871820956256,plant-shape
GLASS,0.953204476093591,"0
10
20
30
Depth"
GLASS,0.9542217700915565,"0.7
0.8
0.9"
GLASS,0.9552390640895219,Accuracy car
GLASS,0.9562563580874873,"0
10
20
30
Depth 0.65 0.70 0.75"
GLASS,0.9572736520854527,Accuracy
GLASS,0.9582909460834181,steel-plates
GLASS,0.9593082400813835,"0
10
20
30
Depth 0.8 0.9"
GLASS,0.960325534079349,Accuracy
GLASS,0.9613428280773143,cardiotocography-3clases
GLASS,0.9623601220752798,"0
10
20
30
Depth 0.7 0.8"
GLASS,0.9633774160732451,Accuracy
GLASS,0.9643947100712106,cardiotocography-10clases
GLASS,0.965412004069176,"0
10
20
30
Depth 0.77 0.78 0.79"
GLASS,0.9664292980671414,Accuracy
GLASS,0.9674465920651069,titanic
GLASS,0.9684638860630722,"0
10
20
30
Depth 0.85 0.90 0.95"
GLASS,0.9694811800610377,Accuracy
GLASS,0.970498474059003,statlog-image
GLASS,0.9715157680569685,"0
10
20
30
Depth 0.50 0.75"
GLASS,0.9725330620549338,Accuracy ozone
GLASS,0.9735503560528993,"0
10
20
30
Depth 0.6 0.8"
GLASS,0.9745676500508647,Accuracy
GLASS,0.9755849440488301,molec-biol-splice
GLASS,0.9766022380467956,"0
10
20
30
Depth"
GLASS,0.9776195320447609,"0.925
0.950
0.975"
GLASS,0.9786368260427264,Accuracy
GLASS,0.9796541200406917,chess-krvkp
GLASS,0.9806714140386572,"0
10
20
30
Depth 0.5 0.6"
GLASS,0.9816887080366226,Accuracy
GLASS,0.982706002034588,abalone
GLASS,0.9837232960325534,"0
10
20
30
Depth 0.7 0.8 0.9"
GLASS,0.9847405900305188,Accuracy bank
GLASS,0.9857578840284842,"0
10
20
30
Depth 0.7 0.8 0.9"
GLASS,0.9867751780264497,Accuracy
GLASS,0.987792472024415,spambase
GLASS,0.9888097660223805,"0
10
20
30
Depth 0.5 0.6 0.7"
GLASS,0.9898270600203459,Accuracy
GLASS,0.9908443540183113,wine-quality-white
GLASS,0.9918616480162767,"0
10
20
30
Depth 0.80 0.85"
GLASS,0.9928789420142421,Accuracy
GLASS,0.9938962360122076,waveform-noise
GLASS,0.9949135300101729,"0
10
20
30
Depth 0.825 0.850"
GLASS,0.9959308240081384,Accuracy
GLASS,0.9969481180061037,waveform
GLASS,0.9979654120040692,"=0.5
=1.0
=2.0
=4.0
=8.0
=16.0
=32.0
=64.0
MLP-induced NTK
RBF"
GLASS,0.9989827060020345,Figure 15: Dataset-wise comparison for a half of the dataset (2/2).
