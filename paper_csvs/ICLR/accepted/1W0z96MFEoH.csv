Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.001834862385321101,"Evaluating the general abilities of intelligent agents requires complex simulation
environments. Existing benchmarks typically evaluate only one narrow task per
environment, requiring researchers to perform expensive training runs on many
different environments. We introduce Crafter, an open world survival game with
visual inputs that evaluates a wide range of general abilities within a single
environment. Agents either learn from the provided reward signal or through
intrinsic objectives and are evaluated by semantically meaningful achievements that
can be unlocked during each episode, such as discovering resources and crafting
tools. Consistently unlocking all achievements requires strong generalization, deep
exploration, and long-term reasoning. We experimentally verify that Crafter is
of appropriate difﬁculty to drive future research and provide baselines scores of
reward agents and unsupervised agents. Furthermore, we observe sophisticated
behaviors emerging from maximizing the reward signal, such as building tunnel
systems, bridges, houses, and plantations. We hope that Crafter will accelerate
research progress by quickly evaluating a wide spectrum of abilities."
INTRODUCTION,0.003669724770642202,"1
INTRODUCTION"
INTRODUCTION,0.005504587155963303,"Figure 1: Agent view of a procedu-
rally generated world in Crafter, showing
terrain types, resources, and creatures.
Agents learn from image inputs and aim
to unlock a range of semantically mean-
ingful achievements during each episode.
The achievements evaluate strong gen-
eralization, wide and deep exploration,
and long-term reasoning."
INTRODUCTION,0.007339449541284404,"Crafter is an open world survival game for reinforcement
learning research. Shown in Figure 1, Crafter features
randomly generated 2D worlds with forests, lakes, moun-
tains, and caves. The player needs to forage for food and
water, ﬁnd shelter to sleep, defend against monsters, col-
lect materials, and build tools. The game mechanics are
inspired by the popular game Minecraft and were simpli-
ﬁed and optimized for research productivity. Crafter aims
to be a fruitful benchmark for reinforcement learning by
focusing on the following design goals:"
INTRODUCTION,0.009174311926605505,"Research challenges
Crafter poses substantial chal-
lenges to current methods. Procedural generation requires
strong generalization, the technology tree evaluates wide
and deep exploration, image observations calls for repre-
sentation learning, repeated subtasks and sparse rewards
evaluate long-term reasoning and credit assignment."
INTRODUCTION,0.011009174311926606,"Meaningful evaluation
Agents are evaluated by a
range of achievements that can be unlocked in each
episode. The achievements correspond to meaningful
milestones in behavior, offering insights into ability spec-
trum of both reward agents and unsupervised agents."
INTRODUCTION,0.012844036697247707,"Iteration speed
Crafter evaluates many agent abilities
within a single environment, vastly reducing the compu-
tational requirements over benchmarks suites that require
training on many separate environments from scratch,
while making it more likely that the measured perfor-
mance is representative of new domains."
INTRODUCTION,0.014678899082568808,Published as a conference paper at ICLR 2022
INTRODUCTION,0.01651376146788991,"$ python3 -m pip install crafter
# Install Crafter
$ python3 -m pip install pygame
# Needed for human interface
$ python3 -m crafter.run_gui
# Start the game"
INTRODUCTION,0.01834862385321101,Figure 2: Play Crafter yourself through the human interface.
RELATED WORK,0.02018348623853211,"2
RELATED WORK"
RELATED WORK,0.022018348623853212,"Benchmarks have been a driving force behind the progress and successes of reinforcement learning
as a ﬁeld (Bellemare et al., 2013; Brockman et al., 2016; Kempka et al., 2016; Beattie et al., 2016;
Tassa et al., 2018; Juliani et al., 2018). Benchmarks often require a large amount of computational
resources and yet only test a small fraction of the abilities that a general agent should master (Cobbe
et al., 2020). This section directly compares Crafter to four particularly related benchmarks."
RELATED WORK,0.023853211009174313,"Minecraft
Crafter is inspired by the successful 3D video game Minecraft, which is available to
researchers via Malmo (Johnson et al., 2016) and MineRL (Guss et al., 2019). Minecraft features
diverse open worlds with randomly generated and modiﬁable terrain, as well as many different
resources, tools, and monsters. However, Minecraft is too complex to be solved by current methods
(Milani et al., 2020), it is unclear by what metric agents should be evaluated by, the environment
is slow, and can be difﬁcult to use because it requires Java and a window server. In comparison,
Crafter captures many principles of Minecraft in a simple and fast environment, where results can
be obtained in a matter of hours, and where a large number of semantically meaningful evaluation
metrics are available for reinforcement learning with or without extrinsic reward. The goal of Crafter
is not to replace Minecraft but progress faster towards it."
RELATED WORK,0.025688073394495414,"Atari
The Atari Learning Environment (Bellemare et al., 2013) has been the gold standard bench-
mark in reinforcement learning. It comprises around 54 individual games, depending on the evaluation
protocol (Mnih et al., 2015; Schulman et al., 2017; Badia et al., 2020; Hafner et al., 2020). While the
large number of games tests different abilities of agents, they require a large amount of computation.
The recommended protocol of training the agent with 5 random seeds on each game for 200M steps
requires over 2000 GPU days (Castro et al., 2018; Hessel et al., 2018). This substantially slows down
experimentation and makes the complete benchmark infeasible for most academic labs. Moreover,
Atari games are nearly deterministic, so agents can approximately memorize their action sequences
and are not required to generalize to new situations (Machado et al., 2018)."
RELATED WORK,0.027522935779816515,"ProcGen
ProcGen (Cobbe et al., 2020) provides a benchmark that is similar to Atari but explicitly
addresses the determinism present in Atari through the use of procedural generation and randomized
textures. It consists of 16 games, where each episode features a randomly generated level layout.
Similarly, Crafter relies on procedural generation to provide a different world map with different
distribution of resources and monsters for every episode. However, ProcGen still requires training
methods on 16 individual games for 200M environment steps, which each focus on a narrow aspect
of an agent’s general abilities. In comparison, Crafter evaluates many different abilities of an agent
by training only on a single environment for 5M steps, substantially accelerating experimentation."
RELATED WORK,0.029357798165137616,"NetHack
NetHack (Küttler et al., 2020) is a text-based game, where the player traverses a randomly
generated system of dungeons with many different items and creatures. Unlike the other discussed
environments, NetHack uses symbolic inputs and thus does not evaluate an agent’s ability to learn
representations of high-dimensional inputs. The game is challenging due to the large amount
of knowledge required about the many different items and their effects, even for human players.
As a result, NetHack requires many environment steps for agents to acquire this domain-speciﬁc
knowledge; 1B steps were used in the original paper. In contrast, Crafter generates diverse complex
worlds from simple underlying rules, focusing more on generalization than memorization of facts."
CRAFTER BENCHMARK,0.031192660550458717,"3
CRAFTER BENCHMARK"
CRAFTER BENCHMARK,0.03302752293577982,"We introduce Crafter, a benchmark that evaluates a variety of agent abilities in a single environment.
This section describes the game mechanics of the environment, the interface of agent inputs and
actions, the evaluation protocol that is based on a range of semantically meaningful achievements,
and the open challenges that Crafter poses for future research."
CRAFTER BENCHMARK,0.03486238532110092,Published as a conference paper at ICLR 2022
CRAFTER BENCHMARK,0.03669724770642202,"Figure 3: Crafter procedurally generates a unique world for every episode that features several terrain
types: grasslands, forests, lakes, mountains, caves. Memorizing action sequences is thus not a viable
strategy and agents are forced to learn behaviors that generalize to new situations."
GAME MECHANICS,0.03853211009174312,"3.1
GAME MECHANICS"
GAME MECHANICS,0.04036697247706422,"This section describes the game mechanics of Crafter, namely its randomly generated world maps,
the levels of health and other internal quantities that the player has to maintain, the resources it
can collect and objects and tools it can make from them, as well as the creatures and how they are
inﬂuenced by the time of day. The images of all materials and objects are shown in Figure E.1. All
randomness in the environment is uniquely determined by an integer seed that is derived from the
initial seed passed to the environment and the episode number."
GAME MECHANICS,0.04220183486238532,"Terrain generation
A unique world is generated for every episode, shown in Figure 3. The world
leverages an underlying grid of 64 × 64 cells but the agent only observes the world through pixel
images. The terrain features grasslands, lakes, and mountains. Lakes can have shores, grasslands can
have forests, and mountains can have caves, ores, and lava. These are determined by OpenSimplex
noise (Spencer, 2014), a form of locally smooth noise. Within the areas determined by noise, objects
appear with equal probability at any location, such as trees in forests and skeletons in caves."
GAME MECHANICS,0.044036697247706424,"Health and survival
The player has levels of health, food, water, and rest that it must prevent from
reaching zero. The levels for food, water, and rest decrease over time and are restored by drinking
from a lake, chasing cows or growing fruits to eat, and sleeping in places where monsters cannot
attack. Once one of the three levels reaches zero, the player starts losing health points. It can also
lose health points when attacked by monsters. When the health points reach zero, the player dies.
Health points regenerate over time when the player is not hungry, thirsty, or sleepy."
GAME MECHANICS,0.045871559633027525,"Resources and crafting
There are many resources, such as saplings, wood, stone, coal, iron, and
diamonds, the player can collect in its inventory and use to build tools and place objects in the world.
Many of the resources require tools that the place must ﬁrst build from more basic resources, leading
to a technology tree with several levels. Standing nearby a table enables the player to craft wood
pickaxes and swords, as well as stone pickaxes and stone swords. Crafting a furnace from stone
enables crafting iron pickaxes and iron swords from both iron, coal, and wood."
GAME MECHANICS,0.047706422018348627,"Creatures and night
Creatures are initialized in random locations and move randomly. Zombies
and cows live in grasslands and are automatically spawned and despawned to ensure a given amount
of creatures. At night, the agent’s view is restricted and noisy and a larger number of zombies is
spawned. This makes it difﬁcult to survive without securing a shelter, such as a cave. Skeletons live
in caves and try to keep the player at a distance to shoot arrows at the player. The player can interact
with creatures to decrease their health points. Cows move randomly and offer a food source."
ENVIRONMENT INTERFACE,0.04954128440366973,"3.2
ENVIRONMENT INTERFACE"
ENVIRONMENT INTERFACE,0.05137614678899083,"This section deﬁnes the speciﬁcation of the environment, explains the available actions, agent inputs,
episode termination, and additional information provided by the environment. The design goal of
these is to make the environment easy to use and inspect. The environment uses the Gym interface
(Brockman et al., 2016) with visual agent inputs and ﬂat categorical actions."
ENVIRONMENT INTERFACE,0.05321100917431193,Published as a conference paper at ICLR 2022
ENVIRONMENT INTERFACE,0.05504587155963303,Collect Iron
ENVIRONMENT INTERFACE,0.05688073394495413,Collect Drink
ENVIRONMENT INTERFACE,0.05871559633027523,Place Table
ENVIRONMENT INTERFACE,0.060550458715596334,Make Wood Pickaxe
ENVIRONMENT INTERFACE,0.062385321100917435,Defeat Skeleton
ENVIRONMENT INTERFACE,0.06422018348623854,Defeat Zombie
ENVIRONMENT INTERFACE,0.06605504587155964,Collect Wood
ENVIRONMENT INTERFACE,0.06788990825688074,Collect Stone
ENVIRONMENT INTERFACE,0.06972477064220184,Collect Coal
ENVIRONMENT INTERFACE,0.07155963302752294,Make Stone Pickaxe
ENVIRONMENT INTERFACE,0.07339449541284404,"Make Iron Pickaxe
Collect Diamond"
ENVIRONMENT INTERFACE,0.07522935779816514,Place Furnace
ENVIRONMENT INTERFACE,0.07706422018348624,Make Wood Sword
ENVIRONMENT INTERFACE,0.07889908256880734,Eat Cow
ENVIRONMENT INTERFACE,0.08073394495412844,Place Stone
ENVIRONMENT INTERFACE,0.08256880733944955,Make Iron Sword
ENVIRONMENT INTERFACE,0.08440366972477065,Make Stone Sword
ENVIRONMENT INTERFACE,0.08623853211009175,Place Plant
ENVIRONMENT INTERFACE,0.08807339449541285,Collect Sapling
ENVIRONMENT INTERFACE,0.08990825688073395,Eat Plant
ENVIRONMENT INTERFACE,0.09174311926605505,Wake Up
ENVIRONMENT INTERFACE,0.09357798165137615,"Figure 4: The 22 achievements that can be unlocked within each episode. The arrows indicate which
achievements will be completed along the way of working toward more challenging achievements.
Several of the earlier tasks have to be repeated multiple times, such as collecting resources, to progress
further. A reward is only given when an achievement is unlocked for the ﬁrst time during the episode."
ENVIRONMENT INTERFACE,0.09541284403669725,"Observations
Agent receive color images of size 64 × 64 × 3 as their only inputs. The image
shows a local top-down view of the map, reaching 4 cells west and east and 3 cells north and south of
the player position. Below this view of the world, the image shows the current inventory state of the
player, including its health points, food, water, and rest levels, collected materials, and crafted tools.
The agent needs to learn to read its inventory state out of the image."
ENVIRONMENT INTERFACE,0.09724770642201835,"Actions
The action space is a ﬂat categorical space with 17 actions, represented by integer indices.
The actions allow the player to move in all 4 directions along the grid, interact with the object in front
of it, go to sleep, place objects, and make tools. Each object and tool has a separate action associated
with it. Tools are kept in the inventory whereas objects are automatically placed in front of the player.
If the agent does not hold the required materials for making an object or tool, the action has no effect."
ENVIRONMENT INTERFACE,0.09908256880733946,"Termination
Each episode terminates when the player’s health points reach 0. This can happen
when the player dies out of hunger, thirst, or tiredness, when attacked by a zombie or skeleton, or
when falling into lava. Health points automatically regenerate, as long as the agent is not too hungry,
thirsty, or sleepy. There is no negative reward for dying, as the reward signal already includes a
penalty for losing health points. Episodes also end when reaching the time limit of 10,000 steps."
ENVIRONMENT INTERFACE,0.10091743119266056,"Additional information
The environment allows access to privileged information about the world
state that the agent is forbidden to observe. This includes numeric inventory counts, achievement
counts, the current coordinate of the player on the grid, and a semantic grid representation of the map.
These can be used for debugging purposes or for other research scenarios, such as predicting the
underlying environment state to evaluate representation learning or video prediction models."
EVALUATION PROTOCOL,0.10275229357798166,"3.3
EVALUATION PROTOCOL"
EVALUATION PROTOCOL,0.10458715596330276,"To evaluate the diverse abilities of artiﬁcial agents on Crafter, we deﬁne two benchmarks. The ﬁrst
benchmark allows agents to access a provided reward signal, while the second benchmark does not
and requires agents to purely learn from intrinsic objectives. Besides access to the provided reward
signal, the evaluation protocols are identical. An agent is granted a budget of 1M environment steps
to interact with the environment. The agent performance is evaluated through success rates of the
individual achievements throughout its training, as well as an aggregated score."
EVALUATION PROTOCOL,0.10642201834862386,Published as a conference paper at ICLR 2022
EVALUATION PROTOCOL,0.10825688073394496,"Achievements
To evaluate a wide spectrum of agent abilities, Crafter deﬁnes 22 achievements. The
achievements are shown in Figure 4 and correspond to semantically meaningful behaviors, such as
collecting various resources, building objects and tools, ﬁnding food and water, defeating monsters,
and waking up safely after sleeping. The achievements cover a wide range of difﬁculties, making
them suitable to evaluate both weak and strong players and providing continuous feedback throughout
the development process of new methods. Some achievements are independent of each other to test
for breadth of exploration, while others depend on each other to test for deep exploration."
EVALUATION PROTOCOL,0.11009174311926606,"Reward
Crafter provides a sparse reward signal that is the sum of two components. The main
component is a reward of +1 every time the agent unlocks each achievement for the ﬁrst time during
the current episode. The second component is a reward of −0.1 for every health point lost and a
reward of +0.1 for every health point that is regenerated. Because the maximum number of health
points is 9, the second reward component only affects the ﬁrst decimal of the episode return, and
ceiling the episode return yields the number of achievements unlocked during the episode."
EVALUATION PROTOCOL,0.11192660550458716,"Success rates
The success rates offer insights into the breadth of abilities learned by an agent.
The success rates are computed separately for each of the achievements, as the fraction of training
episodes during which the agent has unlocked the achievement at least once. It is computed across all
episodes that lead up to the budget of 1M environment steps, requiring agents to be data-efﬁcient.1
Note that the number of environment steps is ﬁxed but the number of episodes can differ between
agents. Unlocking an achievement more than once per episode does not affect the success rate."
EVALUATION PROTOCOL,0.11376146788990826,"Score
The score summarizes the agent abilities into a single number. It is computed by aggregating
the success rates for the individual achievements. Unlocking difﬁcult achievements, even if it happens
rarely, should contribute more than increasing the success rate of achievements that are already
unlocked frequently even further. To account for the range of difﬁculties of the achievements, we
average the success rates in log-space, known as the geometric mean.2 Unlike the reward, the score
thus takes the achievement’s difﬁculties into account, without having to know them beforehand."
EVALUATION PROTOCOL,0.11559633027522936,"Discussion
Aggregating across tasks via a geometric mean weighs tasks based on their difﬁculty to
the agent, resulting in higher scores for agents that explore more broadly. For example, collecting a
diamond 1% of the time instead of 0% is a meaningful improvement, whereas collecting wood 95%
of the time instead of 90% is not. This allows distinguishing how broadly agents have explored their
environment even if they achieve similar rewards. The geometric mean also establishes a meaningful
metric for unsupervised agents, which may get bored of tasks after performing them a few times and
then move on to new tasks. A caveat of the geometric mean is that agents with rewards are evaluated
by something they only indirectly optimize for, which can change their ranking order. Increasing
reward and score is generally correlated, but capacity-limited agents may choose to optimize reward
by mastering easy tasks and ignoring hard tasks, which only slowly increases the geometric mean."
RESEARCH CHALLENGES,0.11743119266055047,"3.4
RESEARCH CHALLENGES"
RESEARCH CHALLENGES,0.11926605504587157,"Crafter aims to evaluate a diverse range of agent abilities within a single environment. Thus, if a
method performs well on Crafter there should be a high chance that it also handles the challenges
of other environments. The challenges also make Crafter suitable for evaluating progress on open
research questions, such as strong generalization, wide and deep exploration, discovering reusable
skills, and long-term memory and reasoning. Crafter is designed to pose the following challenges:"
RESEARCH CHALLENGES,0.12110091743119267,"Exploration
Independent achievements evaluate wide exploration, without offering a linear path
for the agent to follow. Dependent achievements evaluate deep exploration of the technology tree.
Collecting a diamond requires an iron pickaxe, which in turn requires a furnace, table, coal, iron, and
wood. The furnace requires collecting stone, which requires building a wood pickaxe at a table."
RESEARCH CHALLENGES,0.12293577981651377,"Generalization
Every episode is situated in a unique world that is procedurally generated. More-
over, many aspects of the game reoccur in different contexts, such as creatures and resources that can
be found in different landscapes and times of day. This forces successful agents to recognize similar
situations in different circumstances and be robust to changes in irrelevant details."
RESEARCH CHALLENGES,0.12477064220183487,"1While allowing a large number of environment steps would help agents achieve higher scores more easily, it
would result in a comparison of compute resources rather than algorithm quality.
2The Crafter score of an agent is computed as S .= exp( 1"
RESEARCH CHALLENGES,0.12660550458715597,"N
PN
i=1 ln(1 + si)) −1, where si ∈[0; 100] is the
agent’s success rate of achievement i and N = 22 is the number of achievements."
RESEARCH CHALLENGES,0.12844036697247707,Published as a conference paper at ICLR 2022
RESEARCH CHALLENGES,0.13027522935779817,Random
RESEARCH CHALLENGES,0.13211009174311927,"RND
(Unsup)"
RESEARCH CHALLENGES,0.13394495412844037,Plan2Explore
RESEARCH CHALLENGES,0.13577981651376148,(Unsup)
RESEARCH CHALLENGES,0.13761467889908258,Rainbow PPO
RESEARCH CHALLENGES,0.13944954128440368,DreamerV2 0 2 4 6 8 10 12
RESEARCH CHALLENGES,0.14128440366972478,Crafter Score (%)
RESEARCH CHALLENGES,0.14311926605504588,(a) Crafter scores of various agents
RESEARCH CHALLENGES,0.14495412844036698,Random
RESEARCH CHALLENGES,0.14678899082568808,"RND
(Unsup)"
RESEARCH CHALLENGES,0.14862385321100918,Plan2Explore
RESEARCH CHALLENGES,0.15045871559633028,(Unsup)
RESEARCH CHALLENGES,0.15229357798165138,Rainbow PPO
RESEARCH CHALLENGES,0.15412844036697249,DreamerV2
RESEARCH CHALLENGES,0.1559633027522936,Human Experts 0 20 40 60 80 100
RESEARCH CHALLENGES,0.1577981651376147,Crafter Score (%)
RESEARCH CHALLENGES,0.1596330275229358,(b) Same scores including human experts
RESEARCH CHALLENGES,0.1614678899082569,"Figure 5: Crafter Benchmark Scores for various agents with and without rewards. Current top
methods achieve scores of up to 10% that are far from the 50% of human experts, posing a substantial
challenge for future research. Crafter scores are computed as the geometric mean across achievements
of their success rates within the budget of 1M environment steps. Numbers in Table 1."
RESEARCH CHALLENGES,0.163302752293578,"Reusable skills
Advancing in the game requires the agent to repeat several behaviors over long
horizons, such as ﬁnding food, defending against monsters, and collecting common materials that are
needed many times. The behavior of a successful agent naturally decomposes into sub-tasks, making
Crafter suitable for studying hierarchical reinforcement learning."
RESEARCH CHALLENGES,0.1651376146788991,"Credit assignment
Only sparse rewards are given for unlocking an achievement for the ﬁrst time
during each episode. Moreover, several achievements require long-term reasoning, such as collecting
the necessary resources for crafting a particular tool or planting saplings that can be harvested many
hundred time steps later. This makes Crafter a challenge for temporal credit assignment."
RESEARCH CHALLENGES,0.1669724770642202,"Memory
The agent inputs only show the player’s immediate surroundings, making Crafter partially
observed. To survive for a long time, agents need to remember where to ﬁnd lakes to drink and open
grasslands to hunt. Moreover, to effectively ﬁnd rare resources, such as iron and diamonds, the agent
needs to remember what parts of the map it has already searched."
RESEARCH CHALLENGES,0.1688073394495413,"Representation
The agent observes its environment via high-dimensional images, from which it
has to extract entities that are meaningful for decision making. Similar to applications in the real
world, the reward signal is sparse and the amount of environment interaction limited. As a result,
successful agents will likely rely on explicit representation learning techniques."
RESEARCH CHALLENGES,0.1706422018348624,"Survival
In previous environments, the player can often survive by doing nothing. This allows for
degenerate solutions to intrinsic objectives, unlike the real world where animals are forced to adapt to
survive and maintain homeostasis and allostasis. In Crafter, the player struggles to survive through
the constant pressure of maintaining enough water, food, rest, and defending against zombies."
RESEARCH CHALLENGES,0.1724770642201835,"Method
Score (%)
Return"
RESEARCH CHALLENGES,0.1743119266055046,"Human Experts
50.5±6.8
14.3±2.3"
RESEARCH CHALLENGES,0.1761467889908257,"DreamerV2
10.0±1.2
9.0±1.7
PPO
4.6±0.3
4.2±1.2
Rainbow
4.3±0.2
5.0±1.3"
RESEARCH CHALLENGES,0.1779816513761468,"Plan2Explore (Unsup) 2.1±0.1
2.1±1.5
RND (Unsup)
2.0±0.1
0.7±1.3
Random
1.6±0.0
2.1±1.3"
RESEARCH CHALLENGES,0.1798165137614679,"Table 1: Crafter benchmark scores. The Crafter
score is computed as the geometric mean of suc-
cess rates for all 22 achievements available in the
environment. The score prefers general agents
that unlock a wide range of achievements over
those that unlock a small number of achieve-
ments very frequently. For example, an agent
that explores many different achievements over
the course of training achieves a higher score than
one that only performs same simple tasks over
an over. The score thus establishes a meaningful
metric both for agents with and without reward."
RESEARCH CHALLENGES,0.181651376146789,Published as a conference paper at ICLR 2022
RESEARCH CHALLENGES,0.1834862385321101,Collect Coal
RESEARCH CHALLENGES,0.1853211009174312,Collect Diamond
RESEARCH CHALLENGES,0.1871559633027523,Collect Drink
RESEARCH CHALLENGES,0.1889908256880734,Collect Iron
RESEARCH CHALLENGES,0.1908256880733945,Collect Sapling
RESEARCH CHALLENGES,0.1926605504587156,Collect Stone
RESEARCH CHALLENGES,0.1944954128440367,Collect Wood
RESEARCH CHALLENGES,0.1963302752293578,Defeat Skeleton
RESEARCH CHALLENGES,0.1981651376146789,Defeat Zombie
RESEARCH CHALLENGES,0.2,Eat Cow
RESEARCH CHALLENGES,0.2018348623853211,"Eat Plant
Make Iron Pickaxe"
RESEARCH CHALLENGES,0.2036697247706422,Make Iron Sword
RESEARCH CHALLENGES,0.20550458715596331,Make Stone Pickaxe
RESEARCH CHALLENGES,0.20733944954128442,Make Stone Sword
RESEARCH CHALLENGES,0.20917431192660552,Make Wood Pickaxe
RESEARCH CHALLENGES,0.21100917431192662,Make Wood Sword
RESEARCH CHALLENGES,0.21284403669724772,Place Furnace
RESEARCH CHALLENGES,0.21467889908256882,Place Plant
RESEARCH CHALLENGES,0.21651376146788992,Place Stone
RESEARCH CHALLENGES,0.21834862385321102,Place Table
RESEARCH CHALLENGES,0.22018348623853212,Wake Up 0.01 0.1 1 10 100
RESEARCH CHALLENGES,0.22201834862385322,Success Rate (%)
RESEARCH CHALLENGES,0.22385321100917432,"DreamerV2
PPO
Rainbow"
RESEARCH CHALLENGES,0.22568807339449543,"Figure 6: Agent ability spectrum showing the success rates of agents with rewards. These are
unlocking percentages for all 22 achievements, computed over all training episodes. Rainbow
manages to drink water and forage for food. PPO additionally rarely collects coal and builds stone
tools. DreamerV2 achieves these more frequently and additionally sometimes grows and eats fruits.
Numbers in Appendix A."
EXPERIMENTS,0.22752293577981653,"4
EXPERIMENTS"
EXPERIMENTS,0.22935779816513763,"To established baselines for future work, we train various reinforcement learning methods on Crafter
either with and without rewards. The two benchmarks follow the evaluation protocol in Section 3.3,
which grants each agent a budget of 1M environment frames and computes the success rates of the
individual achievements across all training episodes, as well as an aggregate score for the agent.
Furthermore, we analyze the emergent agent behaviors qualitatively and record a dataset of human
expert players to estimate the difﬁculty of the environment. The environment, code for the baseline
agents and ﬁgures in this paper, and the human dataset are available on the project website. 3"
BENCHMARK WITH REWARDS,0.23119266055045873,"4.1
BENCHMARK WITH REWARDS"
BENCHMARK WITH REWARDS,0.23302752293577983,"We provide baselines scores for three reinforcement learning algorithms on Crafter with rewards.
DreamerV2 (Hafner et al., 2020) learns a world model and optimizes a policy through planning in
latent space. We used its default hyper parameters for Atari and increased the model size. PPO
(Schulman et al., 2017) is a popular method that learns to map input images to actions through policy
gradients. We use a convolutional neural network policy with hyper parameters that were tuned for
Atari (Hill et al., 2018). Rainbow (Hessel et al., 2018) is based on Q-Learning and combines several
advances, including for exploration. The defaults for Atari did not work well, so we tuned the hyper
parameters for Crafter and found a compromise between Atari defaults and the data-efﬁcient version
of the method (van Hasselt et al., 2019) to be ideal. All agents trained for 1M environment steps in
under 24 hours on a single GPU and we repeated the training for 10 random seeds per method. The
training reward curves are included in Appendix D."
BENCHMARK WITH REWARDS,0.23486238532110093,"The scores are listed in Table 1 and visualized in Figure 5. DreamerV2 achieves a score of 10.0%,
followed by PPO with 4.6% and Rainbow of 4.3%. Despite these being top reinforcement learning
methods, they lack behind the score of expert human players of 50.5%, which we describe in further
detail in Section 4.3. We conclude that Crafter is a challenging benchmark, where current methods
make learning progress but future research is needed to achieve high performance. For comparison,
we report the episode returns in Table 1, computed over the episodes within the last 105 environment
steps of training. We ﬁnd a trend similar to the scores but notice that the methods are harder to tell
apart, because differences on hard tasks that are rarely achieved affect the return less. Moreover,
the scores are more meaningful for unsupervised agents, which should explore many achievements
over time, but not necessarily remain interested in them until the end of training. The success rates
for individual achievements are visualized in Figure 7, which offer insights into the breadth and
depth of agent abilities. Rainbow displays high success rates on easier achievements. PPO learned"
BENCHMARK WITH REWARDS,0.23669724770642203,3https://danijar.com/crafter
BENCHMARK WITH REWARDS,0.23853211009174313,Published as a conference paper at ICLR 2022
BENCHMARK WITH REWARDS,0.24036697247706423,Collect Coal
BENCHMARK WITH REWARDS,0.24220183486238533,Collect Diamond
BENCHMARK WITH REWARDS,0.24403669724770644,Collect Drink
BENCHMARK WITH REWARDS,0.24587155963302754,Collect Iron
BENCHMARK WITH REWARDS,0.24770642201834864,Collect Sapling
BENCHMARK WITH REWARDS,0.24954128440366974,Collect Stone
BENCHMARK WITH REWARDS,0.25137614678899084,Collect Wood
BENCHMARK WITH REWARDS,0.25321100917431194,Defeat Skeleton
BENCHMARK WITH REWARDS,0.25504587155963304,Defeat Zombie
BENCHMARK WITH REWARDS,0.25688073394495414,Eat Cow
BENCHMARK WITH REWARDS,0.25871559633027524,"Eat Plant
Make Iron Pickaxe"
BENCHMARK WITH REWARDS,0.26055045871559634,Make Iron Sword
BENCHMARK WITH REWARDS,0.26238532110091745,Make Stone Pickaxe
BENCHMARK WITH REWARDS,0.26422018348623855,Make Stone Sword
BENCHMARK WITH REWARDS,0.26605504587155965,Make Wood Pickaxe
BENCHMARK WITH REWARDS,0.26788990825688075,Make Wood Sword
BENCHMARK WITH REWARDS,0.26972477064220185,Place Furnace
BENCHMARK WITH REWARDS,0.27155963302752295,Place Plant
BENCHMARK WITH REWARDS,0.27339449541284405,Place Stone
BENCHMARK WITH REWARDS,0.27522935779816515,Place Table
BENCHMARK WITH REWARDS,0.27706422018348625,Wake Up 0.01 0.1 1 10 100
BENCHMARK WITH REWARDS,0.27889908256880735,Success Rate (%)
BENCHMARK WITH REWARDS,0.28073394495412846,"Plan2Explore
RND
Random"
BENCHMARK WITH REWARDS,0.28256880733944956,"Figure 7: Agent ability spectrum showing the success rates for Crafter without rewards. Random
actions unlock the 6 easiest achievements sometimes, such as drinking water and collecting wood.
Plan2Explore forages for food and defeats monsters more frequently, to ensure longer survival. RND
additionally collects stones and rarely even collects coal and builds furnaces. Numbers in Appendix B."
BENCHMARK WITH REWARDS,0.28440366972477066,"to additionally make stone tools and furnaces. DreamerV2 achieved these more frequently and
discovered growing and harvesting plants. None of the agents learned to collect and use iron for tools
or to collect diamonds, or to achieve high success rates on many of the achievements."
UNSUPERVISED BENCHMARK,0.28623853211009176,"4.2
UNSUPERVISED BENCHMARK"
UNSUPERVISED BENCHMARK,0.28807339449541286,"We provide baselines scores for two unsupervised reinforcement learning agents on Crafter without
rewards. We also include a baseline that simply chooses random actions. RND (Burda et al., 2018b)
is a popular exploration method that seeks out novel inputs, estimated as the prediction error of
a network that aims to predict ﬁxed random embeddings of the input images. We use its default
parameters for Atari. Plan2Explore (Sekar et al., 2020) learns a world model to plan for the expected
information gain of imagined trajectories, allowing it to directly seek out imagined states that have
not been experienced before. We implement Plan2Explore on top of DreamerV2 and keep the same
hyper parameters. We use a non-episodic value function as RND does, which helps exploration in
episodic environments (Burda et al., 2018b). All agents trained for 1M environment steps in under 24
hours on a single GPU and we repeated the training for 10 random seeds per method."
UNSUPERVISED BENCHMARK,0.28990825688073396,"The scores are listed in Table 1 and in Figure 5. Plan2Explore achieves a score of 2.1%, followed
by RND at 2.0%, both ahead of the random agent at 1.6%. Despite these being top unsupervised
reinforcement learning methods, they lack far behind optimal performance or even the performance
of agents that learn with rewards, posing a substantial challenge for future research. The results are
encouraging, showing that unsupervised objectives by themselves can lead to meaningful behaviors
(Burda et al., 2018a) in Crafter. Inspecting the success rates for individual achievements in Figure 6
conﬁrms that Plan2Explore and RND make progress in exploring the different behaviors compared to
the random agent, including occasionally collecting coal, placing furnaces, and making stone swords,
which are several steps deep into the technology tree."
EMERGENT BEHAVIORS,0.29174311926605506,"4.3
EMERGENT BEHAVIORS"
EMERGENT BEHAVIORS,0.29357798165137616,"To better understand the potential of the environment, we train DreamerV2 for 50M steps and
investigate the behaviors qualitatively. In this amount of time, the agent learns to build stone tools
and even iron tools on individual occurrences. Interestingly, we observe a range of sophisticated
emergent behaviors, such as building tunnel systems, building bridges to cross lakes, and outsmarting
skeletons by dodging arrows, blocking arrows with stones, and digging through walls to surprise
skeletons from the side. Furthermore, DreamerV2 learns to seek shelter to protect itself from the
zombies at night by hiding in caves and even digging its own caves and closing the entrances with
stones. Finally, we ﬁnd that the agent sometimes manages to build plantations of many saplings,
defends them against monsters, and eats the growing fruits in order to ensure a reliable and steady
food supply. A video of the emergent behaviors is available on the project website."
EMERGENT BEHAVIORS,0.29541284403669726,Published as a conference paper at ICLR 2022
HUMAN EXPERTS DATASET,0.29724770642201837,"4.4
HUMAN EXPERTS DATASET"
HUMAN EXPERTS DATASET,0.29908256880733947,"Crafter includes a graphical user interface that allows humans to play the game via the keyboard and
record the trajectories of the game. The human interface can be installed via the command shown in
Figure 2. Through the human interface, we recorded the games of 5 human experts for a combined
total of 100 episodes. The experts were given the instructions of the game and allowed several hours
of practice. Out of the 100 episodes, 5 episodes unlock all 22 achievements. The human experts
achieved a score of 50.5%, unlocking all achievements as shown in Table C.1. The achievements most
difﬁcult to humans were to collect diamonds and grow and harvest plans, with success rates of 12%
and 8%, respectively. While the human dataset is separate from the Crafter benchmark, it provides an
estimate of human performance and can be used for research on learning from demonstrations and
imitation learning. The human dataset is available on the project website."
DISCUSSION,0.30091743119266057,"5
DISCUSSION"
DISCUSSION,0.30275229357798167,"Future work
We selected the difﬁculty of Crafter to be challenging yet not hopeless for current
methods. As research progresses towards solving the challenges that are currently present, it may be-
come necessary to extend Crafter by new enemies, resources, items, and achievements. Being written
purely in Python, Crafter can easily be extended in this way. Moreover, grouping the 22 achievements
into categories, such as memory, generalization, and exploration, would allow us to summarize agent
abilities more abstractly (Osband et al., 2019). We did not attempt such a categorization because it is
subjective and will become clearer as more researchers use the environment."
DISCUSSION,0.30458715596330277,"Summary
We introduced Crafter, a benchmark with visual inputs that evaluates a variety of general
agent abilities in a single environment. We described the game mechanics, evaluation protocol, and
open challenges posed by the benchmark, and performed experiments with several agents with and
without rewards to provide baseline scores. Agents are evaluated based on how frequently they
manage to unlock achievements that correspond to semantically meaningful milestones of behavior.
We conclude that Crafter is well suited and of appropriate difﬁculty to guide future research on
intelligent agents, both for learning from extrinsic rewards and purely from intrinsic objectives."
DISCUSSION,0.30642201834862387,"Acknowledgements
We would like to thank Oleh Rybkin, Ben Eysenbach, Sherjil Ozair, Julius
Kunze, Feryal Behbahani, Timothy Lillicrap, Jimmy Ba, Nicolas Heess, Kory Mathewson, Moham-
mad Norouzi, Hamza Merzic, and Sergey Levine for discussions and feedback."
DISCUSSION,0.30825688073394497,Published as a conference paper at ICLR 2022
REFERENCES,0.3100917431192661,REFERENCES
REFERENCES,0.3119266055045872,"Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark.
In International Conference on Machine Learning, pp. 507–517. PMLR, 2020."
REFERENCES,0.3137614678899083,"Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler,
Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint
arXiv:1612.03801, 2016."
REFERENCES,0.3155963302752294,"Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
The arcade learning
environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,
47:253–279, 2013."
REFERENCES,0.3174311926605505,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016."
REFERENCES,0.3192660550458716,"Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.
Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018a."
REFERENCES,0.3211009174311927,"Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv preprint arXiv:1810.12894, 2018b."
REFERENCES,0.3229357798165138,"Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G
Bellemare. Dopamine: A research framework for deep reinforcement learning. arXiv preprint
arXiv:1812.06110, 2018."
REFERENCES,0.3247706422018349,"Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation
to benchmark reinforcement learning. In International conference on machine learning, pp.
2048–2056. PMLR, 2020."
REFERENCES,0.326605504587156,"William H Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie
Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, et al. The
minerl competition on sample efﬁcient reinforcement learning using human priors. arXiv e-prints,
pp. arXiv–1904, 2019."
REFERENCES,0.3284403669724771,"Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete
world models. arXiv preprint arXiv:2010.02193, 2020."
REFERENCES,0.3302752293577982,"Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018."
REFERENCES,0.3321100917431193,"Ashley Hill, Antonin Rafﬁn, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore,
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.com/
hill-a/stable-baselines, 2018."
REFERENCES,0.3339449541284404,"Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artiﬁcial
intelligence experimentation. In IJCAI, pp. 4246–4247. Citeseer, 2016."
REFERENCES,0.3357798165137615,"Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion,
Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, et al. Unity: A general platform for intelligent
agents. arXiv preprint arXiv:1809.02627, 2018."
REFERENCES,0.3376146788990826,"Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski. Vizdoom:
A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE Conference
on Computational Intelligence and Games (CIG), pp. 1–8. IEEE, 2016."
REFERENCES,0.3394495412844037,"Heinrich Küttler, Nantas Nardelli, Alexander H Miller, Roberta Raileanu, Marco Selvatici, Edward
Grefenstette, and Tim Rocktäschel.
The nethack learning environment.
arXiv preprint
arXiv:2006.13760, 2020."
REFERENCES,0.3412844036697248,Published as a conference paper at ICLR 2022
REFERENCES,0.3431192660550459,"Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael
Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for
general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018."
REFERENCES,0.344954128440367,"Stephanie Milani, Nicholay Topin, Brandon Houghton, William H Guss, Sharada P Mohanty,
Keisuke Nakata, Oriol Vinyals, and Noboru Sean Kuno. Retrospective analysis of the 2019
minerl competition on sample efﬁcient reinforcement learning. In NeurIPS 2019 Competition and
Demonstration Track, pp. 203–214. PMLR, 2020."
REFERENCES,0.3467889908256881,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015."
REFERENCES,0.3486238532110092,"Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina
McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al.
Behaviour suite for
reinforcement learning. arXiv preprint arXiv:1908.03568, 2019."
REFERENCES,0.3504587155963303,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.3522935779816514,"Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. arXiv preprint arXiv:2005.05960, 2020."
REFERENCES,0.3541284403669725,"Kurt
Spencer.
Noise!,
2014.
URL
https://uniblock.tumblr.com/post/
97868843242/noise."
REFERENCES,0.3559633027522936,"Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint
arXiv:1801.00690, 2018."
REFERENCES,0.3577981651376147,"Hado P van Hasselt, Matteo Hessel, and John Aslanides.
When to use parametric models in
reinforcement learning? Advances in Neural Information Processing Systems, 32:14322–14333,
2019."
REFERENCES,0.3596330275229358,Published as a conference paper at ICLR 2022
REFERENCES,0.3614678899082569,"A
SUCCESS RATES WITH REWARDS"
REFERENCES,0.363302752293578,"Achievement
Rainbow
PPO
DreamerV2"
REFERENCES,0.3651376146788991,"Collect Coal
0.0%
0.4%
14.7%
Collect Diamond
0.0%
0.0%
0.0%
Collect Drink
24.0%
30.3%
80.0%
Collect Iron
0.0%
0.0%
0.0%
Collect Sapling
97.4%
66.7%
86.6%
Collect Stone
0.2%
3.0%
42.7%
Collect Wood
74.9%
83.0%
92.7%
Defeat Skeleton
0.7%
0.2%
2.6%
Defeat Zombie
39.6%
2.0%
53.1%
Eat Cow
26.1%
12.0%
17.1%
Eat Plant
0.0%
0.0%
0.1%
Make Iron Pickaxe
0.0%
0.0%
0.0%
Make Iron Sword
0.0%
0.0%
0.0%
Make Stone Pickaxe
0.0%
0.0%
0.2%
Make Stone Sword
0.0%
0.0%
0.3%
Make Wood Pickaxe
4.8%
21.1%
59.6%
Make Wood Sword
9.8%
20.1%
40.2%
Place Furnace
0.0%
0.1%
1.8%
Place Plant
94.2%
65.0%
84.4%
Place Stone
0.0%
1.7%
29.0%
Place Table
52.3%
66.1%
85.7%
Wake Up
93.3%
92.5%
92.8%"
REFERENCES,0.3669724770642202,"Score
4.3%
4.6%
10.0%"
REFERENCES,0.3688073394495413,"Table A.1: Success rates on Crafter with rewards. Success rates are computed as the fraction of
episodes during which the achievement has been unlocked at least once. It is computed across all
training episodes within the budget of 1M environment steps. The score is the geometric mean of
success rates over all achievements, as described in Section 3.3. Note that the score is computed for
each seed separately before averaging over seeds and not the other way around. Numbers within 95%
of the best number in each row are highlighted in bold."
REFERENCES,0.3706422018348624,Published as a conference paper at ICLR 2022
REFERENCES,0.3724770642201835,"B
SUCCESS RATES WITHOUT REWARDS"
REFERENCES,0.3743119266055046,"Achievement
Random
RND
Plan2Explore"
REFERENCES,0.3761467889908257,"Collect Coal
0.0%
0.1%
0.1%
Collect Diamond
0.0%
0.0%
0.0%
Collect Drink
9.3%
52.1%
48.7%
Collect Iron
0.0%
0.0%
0.0%
Collect Sapling
50.2%
34.1%
25.5%
Collect Stone
0.0%
0.6%
0.5%
Collect Wood
24.4%
49.6%
46.8%
Defeat Skeleton
0.0%
0.3%
0.2%
Defeat Zombie
0.1%
0.3%
0.2%
Eat Cow
0.4%
0.9%
0.7%
Eat Plant
0.0%
0.0%
0.0%
Make Iron Pickaxe
0.0%
0.0%
0.0%
Make Iron Sword
0.0%
0.0%
0.0%
Make Stone Pickaxe
0.0%
0.0%
0.0%
Make Stone Sword
0.0%
0.0%
0.0%
Make Wood Pickaxe
0.3%
2.5%
3.3%
Make Wood Sword
0.3%
2.6%
3.3%
Place Furnace
0.0%
0.1%
0.0%
Place Plant
44.6%
21.4%
14.0%
Place Stone
0.0%
0.4%
0.3%
Place Table
4.4%
16.7%
16.3%
Wake Up
93.6%
7.8%
47.8%"
REFERENCES,0.3779816513761468,"Score
1.6%
2.0%
2.1%"
REFERENCES,0.3798165137614679,"Table B.1: Success rates on Crafter without rewards. Success rates are computed as the fraction of
episodes during which the achievement has been unlocked at least once. It is computed across all
training episodes within the budget of 1M environment steps. The score is the geometric mean of
success rates over all achievements, as described in Section 3.3. Note that the score is computed for
each seed separately before averaging over seeds and not the other way around. Numbers within 95%
of the best number in each row are highlighted in bold."
REFERENCES,0.381651376146789,Published as a conference paper at ICLR 2022
REFERENCES,0.3834862385321101,"C
SUCCESS RATES OF HUMAN EXPERTS"
REFERENCES,0.3853211009174312,"Achievement
Human Experts"
REFERENCES,0.3871559633027523,"Collect Coal
86.0%
Collect Diamond
12.0%
Collect Drink
92.0%
Collect Iron
53.0%
Collect Sapling
67.0%
Collect Stone
100.0%
Collect Wood
100.0%
Defeat Skeleton
31.0%
Defeat Zombie
84.0%
Eat Cow
89.0%
Eat Plant
8.0%
Make Iron Pickaxe
26.0%
Make Iron Sword
22.0%
Make Stone Pickaxe
78.0%
Make Stone Sword
78.0%
Make Wood Pickaxe
100.0%
Make Wood Sword
45.0%
Place Furnace
32.0%
Place Plant
24.0%
Place Stone
90.0%
Place Table
100.0%
Wake Up
73.0%"
REFERENCES,0.3889908256880734,"Score
50.5%"
REFERENCES,0.3908256880733945,"Table C.1: Success rates of human experts on Crafter. The success rates of human experts are
computed as the fraction of all 100 recorded games during which the achievement has been unlocked
at least once. To compute the score analogously to the artiﬁcial agents, we randomly split the 100
games into 5 groups that are treated as the different seeds. We then follow the same procedure as for
the artiﬁcial agents."
REFERENCES,0.3926605504587156,"D
EPISODE REWARD"
REFERENCES,0.3944954128440367,"0.0
0.2
0.4
0.6
0.8
1.0
1e6 0 5 10 15"
OPTIMAL,0.3963302752293578,"20
Optimal"
OPTIMAL,0.3981651376146789,Crafter Reward
OPTIMAL,0.4,"DreamerV2
Rainbow
PPO
Random"
OPTIMAL,0.4018348623853211,"Figure D.1: Total episode reward with shaded standard deviation. The optimal achievable episode
reward is 22. While visualizing rewards can be informative for debugging, ﬁnal performance on
Crafter should be reported by computing the score instead. The score takes the different difﬁculties
of the achievements into account and is deﬁned as the geometric mean of the success rates for all
achievements, as described in Section 3.3."
OPTIMAL,0.4036697247706422,Published as a conference paper at ICLR 2022
OPTIMAL,0.4055045871559633,"E
TEXTURES"
OPTIMAL,0.4073394495412844,"Player
Plant
Cow
Zombie
Skeleton
Arrow"
OPTIMAL,0.4091743119266055,"Water
Sand
Grass
Tree
Path
Stone"
OPTIMAL,0.41100917431192663,"Coal
Iron
Diamond
Lava
Table
Furnace"
OPTIMAL,0.41284403669724773,"Figure E.1: Crafter features worlds with several materials, resources, objects, and creatures. The
player can interact with these to collect resources, maintain its food and water supplies, craft pickaxes
and swords, and defend itself. The textures were speciﬁcally created for Crafter."
OPTIMAL,0.41467889908256883,"F
ACTION SPACE"
OPTIMAL,0.41651376146788993,"Integer
Name
Requirement"
NOOP,0.41834862385321103,"0
Noop
Always applicable.
1
Move Left
Flat ground left to the agent.
2
Move Right
Flat ground right to the agent.
3
Move Up
Flat ground above the agent.
4
Move Down
Flat ground below the agent.
5
Do
Facing creature or material; have necessary tool.
6
Sleep
Energy level is below maximum.
7
Place Stone
Stone in inventory.
8
Place Table
Wood in inventory.
9
Place Furnace
Stone in inventory.
10
Place Plant
Sapling in inventory.
11
Make Wood Pickaxe
Nearby table; wood in inventory.
12
Make Stone Pickaxe
Nearby table; wood, stone in inventory.
13
Make Iron Pickaxe
Nearby table, furnace; wood, coal, iron an inventory.
14
Make Wood Sword
Nearby table; wood in inventory.
15
Make Stone Sword
Nearby table; wood, stone in inventory.
16
Make Iron Sword
Nearby table, furnace; wood, coal, iron in inventory."
NOOP,0.42018348623853213,"Table F.1: The action space is a ﬂat categorical space, making Crafter easy to use. The 17 actions
enable the agent to move, collect materials, place objects, craft objects, and interact with what is in
front of the player. Actions whose requirements are not satisﬁed have no effect."
NOOP,0.42201834862385323,Published as a conference paper at ICLR 2022
NOOP,0.42385321100917434,"G
ACHIEVEMENT CURVES OF RAINBOW"
NOOP,0.42568807339449544,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.42752293577981654,Reward
NOOP,0.42935779816513764,"0.0
0.5
1.0
1e6 0 200 400 600"
NOOP,0.43119266055045874,Length
NOOP,0.43302752293577984,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.43486238532110094,Collect Coal
NOOP,0.43669724770642204,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.43853211009174314,Collect Diamond
NOOP,0.44036697247706424,"0.0
0.5
1.0
1e6 0 100 200"
NOOP,0.44220183486238535,Collect Drink
NOOP,0.44403669724770645,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.44587155963302755,Collect Iron
NOOP,0.44770642201834865,"0.0
0.5
1.0
1e6 0 10 20 30"
NOOP,0.44954128440366975,Collect Sapling
NOOP,0.45137614678899085,"0.0
0.5
1.0
1e6 0 1 2 3 4"
NOOP,0.45321100917431195,Collect Stone
NOOP,0.45504587155963305,"0.0
0.5
1.0
1e6 0 5 10 15"
NOOP,0.45688073394495415,Collect Wood
NOOP,0.45871559633027525,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.46055045871559636,Defeat Skeleton
NOOP,0.46238532110091746,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.46422018348623856,Defeat Zombie
NOOP,0.46605504587155966,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.46788990825688076,Eat Cow
NOOP,0.46972477064220186,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.47155963302752296,Eat Plant
NOOP,0.47339449541284406,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.47522935779816516,Make Iron Pickaxe
NOOP,0.47706422018348627,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.47889908256880737,Make Iron Sword
NOOP,0.48073394495412847,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.48256880733944957,Make Stone Pickaxe
NOOP,0.48440366972477067,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.48623853211009177,Make Stone Sword
NOOP,0.48807339449541287,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.48990825688073397,Make Wood Pickaxe
NOOP,0.4917431192660551,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.4935779816513762,Make Wood Sword
NOOP,0.4954128440366973,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.4972477064220184,Place Furnace
NOOP,0.4990825688073395,"0.0
0.5
1.0
1e6"
NOOP,0.5009174311926605,"0.0
2.5
5.0
7.5
10.0"
NOOP,0.5027522935779817,Place Plant
NOOP,0.5045871559633027,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.5064220183486239,Place Stone
NOOP,0.5082568807339449,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.5100917431192661,Place Table
NOOP,0.5119266055045871,"0.0
0.5
1.0
1e6"
NOOP,0.5137614678899083,"0
2
4
6
8"
NOOP,0.5155963302752293,Wake Up
NOOP,0.5174311926605505,Figure G.1: Achievement counts of Rainbow with shaded min and max.
NOOP,0.5192660550458715,Published as a conference paper at ICLR 2022
NOOP,0.5211009174311927,"H
ACHIEVEMENT CURVES OF PPO"
NOOP,0.5229357798165137,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.5247706422018349,Reward
NOOP,0.5266055045871559,"0.0
0.5
1.0
1e6 0 200 400 600"
NOOP,0.5284403669724771,Length
NOOP,0.5302752293577981,"0.0
0.5
1.0
1e6 0 1 2 3 4"
NOOP,0.5321100917431193,Collect Coal
NOOP,0.5339449541284403,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.5357798165137615,Collect Diamond
NOOP,0.5376146788990825,"0.0
0.5
1.0
1e6"
NOOP,0.5394495412844037,"0
25
50
75
100"
NOOP,0.5412844036697247,Collect Drink
NOOP,0.5431192660550459,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.544954128440367,Collect Iron
NOOP,0.5467889908256881,"0.0
0.5
1.0
1e6"
NOOP,0.5486238532110091,"0
2
4
6
8"
NOOP,0.5504587155963303,Collect Sapling
NOOP,0.5522935779816514,"0.0
0.5
1.0
1e6 0 10 20"
NOOP,0.5541284403669725,Collect Stone
NOOP,0.5559633027522936,"0.0
0.5
1.0
1e6 0 5 10 15"
NOOP,0.5577981651376147,Collect Wood
NOOP,0.5596330275229358,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.5614678899082569,Defeat Skeleton
NOOP,0.563302752293578,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.5651376146788991,Defeat Zombie
NOOP,0.5669724770642202,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.5688073394495413,Eat Cow
NOOP,0.5706422018348624,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.5724770642201835,Eat Plant
NOOP,0.5743119266055046,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.5761467889908257,Make Iron Pickaxe
NOOP,0.5779816513761468,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.5798165137614679,Make Iron Sword
NOOP,0.581651376146789,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.5834862385321101,Make Stone Pickaxe
NOOP,0.5853211009174312,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.5871559633027523,Make Stone Sword
NOOP,0.5889908256880734,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.5908256880733945,Make Wood Pickaxe
NOOP,0.5926605504587156,"0.0
0.5
1.0
1e6 0 2 4 6 8"
NOOP,0.5944954128440367,Make Wood Sword
NOOP,0.5963302752293578,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.5981651376146789,Place Furnace
NOOP,0.6,"0.0
0.5
1.0
1e6"
NOOP,0.6018348623853211,"0
2
4
6
8"
NOOP,0.6036697247706422,Place Plant
NOOP,0.6055045871559633,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.6073394495412844,Place Stone
NOOP,0.6091743119266055,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.6110091743119266,Place Table
NOOP,0.6128440366972477,"0.0
0.5
1.0
1e6"
NOOP,0.6146788990825688,"0.0
2.5
5.0
7.5
10.0"
NOOP,0.6165137614678899,Wake Up
NOOP,0.618348623853211,Figure H.1: Achievement counts of PPO with shaded min and max.
NOOP,0.6201834862385321,Published as a conference paper at ICLR 2022
NOOP,0.6220183486238532,"I
ACHIEVEMENT CURVES OF DREAMERV2"
NOOP,0.6238532110091743,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.6256880733944954,Reward
NOOP,0.6275229357798165,"0.0
0.5
1.0
1e6 0 500 1000 1500 2000"
NOOP,0.6293577981651376,Length
NOOP,0.6311926605504588,"0.0
0.5
1.0
1e6 0 2 4 6 8"
NOOP,0.6330275229357798,Collect Coal
NOOP,0.634862385321101,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.636697247706422,Collect Diamond
NOOP,0.6385321100917432,"0.0
0.5
1.0
1e6 0 100 200"
NOOP,0.6403669724770642,Collect Drink
NOOP,0.6422018348623854,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.6440366972477064,Collect Iron
NOOP,0.6458715596330276,"0.0
0.5
1.0
1e6 0 10 20 30"
NOOP,0.6477064220183486,Collect Sapling
NOOP,0.6495412844036698,"0.0
0.5
1.0
1e6 0 20 40 60"
NOOP,0.6513761467889908,Collect Stone
NOOP,0.653211009174312,"0.0
0.5
1.0
1e6 0 10 20 30 40"
NOOP,0.655045871559633,Collect Wood
NOOP,0.6568807339449542,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.6587155963302752,Defeat Skeleton
NOOP,0.6605504587155964,"0.0
0.5
1.0
1e6"
NOOP,0.6623853211009174,"0
2
4
6
8"
NOOP,0.6642201834862386,Defeat Zombie
NOOP,0.6660550458715596,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.6678899082568808,Eat Cow
NOOP,0.6697247706422018,"0.0
0.5
1.0
1e6 0.0 2.5 5.0 7.5 10.0"
NOOP,0.671559633027523,Eat Plant
NOOP,0.673394495412844,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.6752293577981652,Make Iron Pickaxe
NOOP,0.6770642201834862,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.6788990825688074,Make Iron Sword
NOOP,0.6807339449541284,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.6825688073394496,Make Stone Pickaxe
NOOP,0.6844036697247706,"0.0
0.5
1.0
1e6 0 1 2 3 4"
NOOP,0.6862385321100918,Make Stone Sword
NOOP,0.6880733944954128,"0.0
0.5
1.0
1e6 0 5 10 15"
NOOP,0.689908256880734,Make Wood Pickaxe
NOOP,0.691743119266055,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.6935779816513762,Make Wood Sword
NOOP,0.6954128440366972,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.6972477064220184,Place Furnace
NOOP,0.6990825688073394,"0.0
0.5
1.0
1e6 0 10 20"
NOOP,0.7009174311926606,Place Plant
NOOP,0.7027522935779816,"0.0
0.5
1.0
1e6 0 20 40 60"
NOOP,0.7045871559633028,Place Stone
NOOP,0.7064220183486238,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.708256880733945,Place Table
NOOP,0.710091743119266,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.7119266055045872,Wake Up
NOOP,0.7137614678899082,Figure I.1: Achievement counts of DreamerV2 with shaded min and max.
NOOP,0.7155963302752294,Published as a conference paper at ICLR 2022
NOOP,0.7174311926605504,"J
ACHIEVEMENT CURVES OF RANDOM AGENT"
NOOP,0.7192660550458716,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.7211009174311926,Reward
NOOP,0.7229357798165138,"0.0
0.5
1.0
1e6 100 200 300 400"
NOOP,0.7247706422018348,Length
NOOP,0.726605504587156,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.728440366972477,Collect Coal
NOOP,0.7302752293577982,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7321100917431193,Collect Diamond
NOOP,0.7339449541284404,"0.0
0.5
1.0
1e6"
NOOP,0.7357798165137615,"0.0
2.5
5.0
7.5
10.0"
NOOP,0.7376146788990826,Collect Drink
NOOP,0.7394495412844037,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7412844036697248,Collect Iron
NOOP,0.7431192660550459,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.744954128440367,Collect Sapling
NOOP,0.7467889908256881,"0.0
0.5
1.0
1e6 0 1 2 3 4"
NOOP,0.7486238532110092,Collect Stone
NOOP,0.7504587155963303,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.7522935779816514,Collect Wood
NOOP,0.7541284403669725,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7559633027522936,Defeat Skeleton
NOOP,0.7577981651376147,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7596330275229358,Defeat Zombie
NOOP,0.7614678899082569,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.763302752293578,Eat Cow
NOOP,0.7651376146788991,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7669724770642202,Eat Plant
NOOP,0.7688073394495413,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7706422018348624,Make Iron Pickaxe
NOOP,0.7724770642201835,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7743119266055046,Make Iron Sword
NOOP,0.7761467889908257,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7779816513761468,Make Stone Pickaxe
NOOP,0.7798165137614679,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.781651376146789,Make Stone Sword
NOOP,0.7834862385321101,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.7853211009174312,Make Wood Pickaxe
NOOP,0.7871559633027523,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.7889908256880734,Make Wood Sword
NOOP,0.7908256880733945,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.7926605504587156,Place Furnace
NOOP,0.7944954128440367,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.7963302752293578,Place Plant
NOOP,0.7981651376146789,"0.0
0.5
1.0
1e6 0 1 2 3 4"
NOOP,0.8,Place Stone
NOOP,0.8018348623853211,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.8036697247706422,Place Table
NOOP,0.8055045871559633,"0.0
0.5
1.0
1e6 0 2 4 6 8"
NOOP,0.8073394495412844,Wake Up
NOOP,0.8091743119266055,Figure J.1: Achievement counts of random actions with shaded min and max.
NOOP,0.8110091743119267,Published as a conference paper at ICLR 2022
NOOP,0.8128440366972477,"K
ACHIEVEMENT CURVES OF UNSUPERVISED RND"
NOOP,0.8146788990825689,"0.0
0.5
1.0
1e6 0.0 2.5 5.0 7.5"
NOOP,0.8165137614678899,Reward
NOOP,0.818348623853211,"0.0
0.5
1.0
1e6 0 200 400"
NOOP,0.8201834862385321,Length
NOOP,0.8220183486238533,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.8238532110091743,Collect Coal
NOOP,0.8256880733944955,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8275229357798165,Collect Diamond
NOOP,0.8293577981651377,"0.0
0.5
1.0
1e6 0 50 100"
NOOP,0.8311926605504587,Collect Drink
NOOP,0.8330275229357799,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8348623853211009,Collect Iron
NOOP,0.8366972477064221,"0.0
0.5
1.0
1e6 0 2 4 6 8"
NOOP,0.8385321100917431,Collect Sapling
NOOP,0.8403669724770643,"0.0
0.5
1.0
1e6 0 5 10 15 20"
NOOP,0.8422018348623853,Collect Stone
NOOP,0.8440366972477065,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.8458715596330275,Collect Wood
NOOP,0.8477064220183487,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.8495412844036697,Defeat Skeleton
NOOP,0.8513761467889909,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8532110091743119,Defeat Zombie
NOOP,0.8550458715596331,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.8568807339449541,Eat Cow
NOOP,0.8587155963302753,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8605504587155963,Eat Plant
NOOP,0.8623853211009175,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8642201834862385,Make Iron Pickaxe
NOOP,0.8660550458715597,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8678899082568807,Make Iron Sword
NOOP,0.8697247706422019,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8715596330275229,Make Stone Pickaxe
NOOP,0.8733944954128441,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.8752293577981651,Make Stone Sword
NOOP,0.8770642201834863,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.8788990825688073,Make Wood Pickaxe
NOOP,0.8807339449541285,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.8825688073394495,Make Wood Sword
NOOP,0.8844036697247707,"0.0
0.5
1.0
1e6 0 1 2 3"
NOOP,0.8862385321100917,Place Furnace
NOOP,0.8880733944954129,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.8899082568807339,Place Plant
NOOP,0.8917431192660551,"0.0
0.5
1.0
1e6 0.0 2.5 5.0 7.5 10.0"
NOOP,0.8935779816513761,Place Stone
NOOP,0.8954128440366973,"0.0
0.5
1.0
1e6 0 1 2 3 4"
NOOP,0.8972477064220183,Place Table
NOOP,0.8990825688073395,"0.0
0.5
1.0
1e6 0 2 4 6"
NOOP,0.9009174311926605,Wake Up
NOOP,0.9027522935779817,Figure K.1: Achievement counts of unsupervised RND with shaded min and max.
NOOP,0.9045871559633027,Published as a conference paper at ICLR 2022
NOOP,0.9064220183486239,"L
ACHIEVEMENT CURVES OF UNSUPERVISED PLAN2EXPLORE"
NOOP,0.908256880733945,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.9100917431192661,Reward
NOOP,0.9119266055045872,"0.0
0.5
1.0
1e6 0 200 400 600 800"
NOOP,0.9137614678899083,Length
NOOP,0.9155963302752294,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.9174311926605505,Collect Coal
NOOP,0.9192660550458716,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9211009174311927,Collect Diamond
NOOP,0.9229357798165138,"0.0
0.5
1.0
1e6 0 20 40 60"
NOOP,0.9247706422018349,Collect Drink
NOOP,0.926605504587156,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9284403669724771,Collect Iron
NOOP,0.9302752293577982,"0.0
0.5
1.0
1e6 0 2 4 6 8"
NOOP,0.9321100917431193,Collect Sapling
NOOP,0.9339449541284404,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.9357798165137615,Collect Stone
NOOP,0.9376146788990826,"0.0
0.5
1.0
1e6 0 5 10"
NOOP,0.9394495412844037,Collect Wood
NOOP,0.9412844036697248,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.9431192660550459,Defeat Skeleton
NOOP,0.944954128440367,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9467889908256881,Defeat Zombie
NOOP,0.9486238532110092,"0.0
0.5
1.0
1e6 0.0 0.5 1.0 1.5 2.0"
NOOP,0.9504587155963303,Eat Cow
NOOP,0.9522935779816514,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9541284403669725,Eat Plant
NOOP,0.9559633027522936,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9577981651376147,Make Iron Pickaxe
NOOP,0.9596330275229358,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9614678899082569,Make Iron Sword
NOOP,0.963302752293578,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9651376146788991,Make Stone Pickaxe
NOOP,0.9669724770642202,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9688073394495413,Make Stone Sword
NOOP,0.9706422018348624,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.9724770642201835,Make Wood Pickaxe
NOOP,0.9743119266055046,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.9761467889908257,Make Wood Sword
NOOP,0.9779816513761468,"0.0
0.5
1.0
1e6 0.00 0.25 0.50 0.75 1.00"
NOOP,0.9798165137614679,Place Furnace
NOOP,0.981651376146789,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.9834862385321101,Place Plant
NOOP,0.9853211009174312,"0.0
0.5
1.0
1e6 0 2 4"
NOOP,0.9871559633027523,Place Stone
NOOP,0.9889908256880734,"0.0
0.5
1.0
1e6 0 1 2 3 4"
NOOP,0.9908256880733946,Place Table
NOOP,0.9926605504587156,"0.0
0.5
1.0
1e6"
NOOP,0.9944954128440368,"0
2
4
6
8"
NOOP,0.9963302752293578,Wake Up
NOOP,0.998165137614679,Figure L.1: Achievement counts of unsupervised Plan2Explore with shaded min and max.
