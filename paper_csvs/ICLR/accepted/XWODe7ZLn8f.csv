Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004048582995951417,"Unsupervised Ô¨Åne-grained class clustering is a practical yet challenging task due
to the difÔ¨Åculty of feature representations learning of subtle object details. We
introduce C3-GAN, a method that leverages the categorical inference power of
InfoGAN with contrastive learning. We aim to learn feature representations that en-
courage a dataset to form distinct cluster boundaries in the embedding space, while
also maximizing the mutual information between the latent code and its image ob-
servation. Our approach is to train a discriminator, which is also used for inferring
clusters, to optimize the contrastive loss, where image-latent pairs that maximize
the mutual information are considered as positive pairs and the rest as negative pairs.
SpeciÔ¨Åcally, we map the input of a generator, which was sampled from the categor-
ical distribution, to the embedding space of the discriminator and let them act as a
cluster centroid. In this way, C3-GAN succeeded in learning a clustering-friendly
embedding space where each cluster is distinctively separable. Experimental results
show that C3-GAN achieved the state-of-the-art clustering performance on four
Ô¨Åne-grained image datasets, while also alleviating the mode collapse phenomenon.
Code is available at https://github.com/naver-ai/c3-gan."
INTRODUCTION,0.008097165991902834,"1
INTRODUCTION"
INTRODUCTION,0.012145748987854251,"Unsupervised Ô¨Åne-grained class clustering is a task that classiÔ¨Åes images of very similar ob-
jects (Benny & Wolf, 2020). While a line of works based on multiview-based self-supervised
learning (SSL) methods (He et al., 2020; Chen et al., 2020; Grill et al., 2020) shows promising results
on a conventional coarse-grained class clustering task (Van Gansbeke et al., 2020), it is difÔ¨Åcult for a
Ô¨Åne-grained class clustering task to beneÔ¨Åt from these methods for two reasons. First, though it is
more challenging and ambiguous for Ô¨Ånding distinctions between Ô¨Åne-grained classes, datasets for
this type of task are difÔ¨Åcult to be large-scale. We visualized this idea in Figure 1 which shows that
Ô¨Ånding distinctions between Ô¨Åne-grained classes is more difÔ¨Åcult than doing so for coarse-grained
object classes. Second, the augmentation processes in these methods consider subtle changes in color
or shape, that actually play an important role in differentiating between classes, as noisy factors. Thus,
it is required to Ô¨Ånd an another approach for a Ô¨Åne-grained class clustering task."
INTRODUCTION,0.016194331983805668,"Generative adversarial networks (GAN) (Goodfellow et al., 2014) can be a solution as it needs to
learn Ô¨Åne details to generate realistic images. A possible starting point could be InfoGAN (Chen et al.,
2016), that succeeded in unsupervised categorical inference on MNIST dataset (LeCun et al., 2010)
by maximizing the mutual information between the latent code and its observation. The only prior
knowledge employed was the number of classes and the fact that the data is uniformly distributed over
the classes. FineGAN (Singh et al., 2019) extends InfoGAN by integrating the scene decomposition
method into the framework, and learns three latent codes for the hierarchical scene generation. Each
code, that is sampled from the independent uniform categorical distribution, is sequentially injected
to multiple generators for a background, a super-class object, and a sub-class object image syntheses.
FineGAN demonstrated that two latent codes for object image generations could be also utilized for
clustering real images into their Ô¨Åne-grained classes, outperforming conventional coarse-grained class
clustering methods. This result implies that extracting only foreground features is very helpful for the
given task. However, FineGAN requires object bounding box annotations and additional training of
classiÔ¨Åers, which greatly hinder its applicability. Also, the method lacks the ability of learning the
distribution of real images, due to the mode collapse phenomenon (Higgins et al., 2017)."
INTRODUCTION,0.020242914979757085,Published as a conference paper at ICLR 2022
INTRODUCTION,0.024291497975708502,"(a) Fine-grained Class Clustering
(b) Coarse-grained Class Clustering"
INTRODUCTION,0.02834008097165992,"Figure 1: Fine-grained vs. Coarse-grained. We compare two sets of 4 classes, each selected from
(a) CUB (Wah et al., 2011) and (b) STL (Coates et al., 2011) datasets. The diagrams visualize image
features extracted from ResNet-18 that was trained on each dataset via the method of SimCLR (Chen
et al., 2020). The color of each dot corresponds to its ground-truth class. Both in the image-level and
the feature-level, the clusters in (a) are way more indistinguishable than those in (b)."
INTRODUCTION,0.032388663967611336,"In this paper, we propose Constrastive Ô¨Åne-grained Class Clustering GAN (C3-GAN) that deals with
the aforementioned problems. We Ô¨Årst remove the reliance on human-annotated labels by adopting
the method for unsupervised scene decomposition learning that is inspired by PerturbGAN (Bielski
& Favaro, 2019). We then adopt contrastive learning method (van den Oord et al., 2018) for training
a discriminator by deÔ¨Åning pairs of image-latent features that maximize the mutual information of
the two as positive pairs and the rest as negative pairs. This is based on the intuition that optimal
results would be obtained if cluster centroids are distributed in a way that each cluster can be linearly
separable (Wang & Isola, 2020). In speciÔ¨Åc, we map the input of a generator that was sampled from
the categorical distribution onto the embedding space of the discriminator, and let it act as a cluster
centroid that pulls features of images that were generated with its speciÔ¨Åc value. Since the relations
with other latent values are set as negative pairs, each cluster would be placed farther away from each
other by the nature of the contrastive loss. We have conducted experiments on four Ô¨Åne-grained image
datasets including CUB, Stanford Cars, Stanford Dogs and Oxford Flower, and demonstrated the
effectiveness of C3-GAN by showing that it achieves the state-of-the-art clustering performance on
all datasets. Moreover, the induced embedding space of the discriminator turned out to be also helpful
for alleviating the mode collapse issue. We conjecture that this is because the generator is additionally
required to be able to synthesize a certain number of object classes with distinct characteristics."
INTRODUCTION,0.03643724696356275,Our main contributions are summarized as follows:
INTRODUCTION,0.04048582995951417,"‚Ä¢ We propose a novel form of the information-theoretic regularization to learn a clustering-
friendly embedding space that leads a dataset to form distinct cluster boundaries without
falling into a degenerated solution. With this method, our C3-GAN achieved the state-of-
the-art Ô¨Åne-grained class clustering performance on four Ô¨Åne-grained image datasets."
INTRODUCTION,0.044534412955465584,"‚Ä¢ By adopting scene decomposition learning method that does not require any human-
annotated labels, our method can be applied to more diverse datasets."
INTRODUCTION,0.048582995951417005,"‚Ä¢ Our method of training a discriminator is not only suitable for class clustering task, but also
good at alleviating the mode collapse issue of GANs."
RELATED WORK,0.05263157894736842,"2
RELATED WORK"
RELATED WORK,0.05668016194331984,"Unsupervised Clustering. Unsupervised clustering methods can be mainly categorized into the
information theory-based and Expectation‚ÄìMaximization(EM)-based approaches. The Ô¨Årst approach
aims to maximize the mutual information between original images and their augmentations to train
a model in an end-to-end fashion (Ji et al., 2019; Zhong et al., 2020). To enhance the performance,
IIC (Ji et al., 2019) additionally trains the auxiliary branch using an unlabeled large dataset, and
DRC (Zhong et al., 2020) optimizes the contrastive loss on logit features for reducing the intra-class
feature variation. EM-based methods decouple the cluster assignment process (E-step) and the feature
representations learning process (M-step) to learn more robust representations. SpeciÔ¨Åcally, the"
RELATED WORK,0.06072874493927125,Published as a conference paper at ICLR 2022
RELATED WORK,0.06477732793522267,"feature representations are learnt by not only Ô¨Åtting the proto-clustering results estimated in the
expectation step, but also optimizing particular pretext tasks. The clusters are either inferred by
k-means clustering (Xie et al., 2016; Caron et al., 2018; Liu et al., 2020; Li et al., 2021) or training of
an additional encoder (Dizaji et al., 2017). However, many of these methods suffer from an uneven
allocation issue of k-means clustering, which could result in a degenerate solution. For this reason,
SCAN (Van Gansbeke et al., 2020) proposes the novel objective function that does not require the
k-means clustering process. Based on the observation that K nearest neighbors of each feature point
belong to the same ground-truth cluster with a high probability, it trains a classiÔ¨Åer that assigns
identical cluster id to all nearest neighbors, which signiÔ¨Åcantly improved the clustering performance.
Unlike these prior methods where the cluster centroids are not learnt in an end-to-end manner or not
the subject of interest, C3-GAN investigate and try to improve their distribution for achieving better
clustering performance."
RELATED WORK,0.06882591093117409,"Unsupervised Scene Decomposition. Heavy cost of annotating segmentation masks have drawn
active research efforts on developing unsupervised segmentation methods. Some works address the
task in the information-theoretic perspective (Ji et al., 2019; Savarese et al., 2021), but a majority of
the works are based on GANs. ReDO (Chen et al. (2019)) learns to infer object masks by redrawing
an input scene, and PerturbGAN (Bielski & Favaro (2019)), that has a separate background and
foreground generator, triggers scene decomposition by randomly perturbing foreground images.
Meanwhile, another line of works utilizes pre-trained high quality generative models such as Style-
GAN (Karras et al., 2019; 2020) and BigGAN (Brock et al., 2019). Labels4Free (Abdal et al., 2021)
trains a segmentation network on top of the pre-trained StyleGAN, utilizing the fact that inputs of
each layer of StyleGAN have different degrees of contribution to foreground synthesis. Voynov &
Babenko (2020) and Melas-Kyriazi et al. (2021) explore the latent space of pre-trained GANs to Ô¨Ånd
the perturbing directions that can be used for inducing foreground masks. While all these methods
aim to infer foreground masks by training an additional mask predictor with synthesized data, or
projecting real images into the latent space of the pre-trained GAN, C3-GAN is rather focusing on
the semantic representations learning of foreground objects."
RELATED WORK,0.0728744939271255,"Fine-grained Feature Learning. For Ô¨Åne-grained feature representations learning, some works
(Singh et al., 2019; Li et al., 2020; Benny & Wolf, 2020) have extended InfoGAN by integrating the
scene decomposition learning method into the framework with multiple pairs of adversarial networks.
FineGAN (Singh et al., 2019) learns multiple latent codes to use them for sequential generation
of a background, a super-class object, and a sub-class object image, respectively. MixNMatch (Li
et al., 2020) and OneGAN (Benny & Wolf, 2020) extend FineGAN with multiple encoders to
directly infer these latent codes from real images and use them for manipulating images. Since
this autoencoder-based structure could evoke a degenerate solution where only one generator is
responsible for synthesizing the entire image, MixNMatch conducts adversarial learning on the joint
distribution of the latent code and image (Donahue et al., 2017), while OneGAN trains a model in
two stages by training generators Ô¨Årst and encoders next. Even though these works have succeeded in
generating images in a hierarchical manner and learnt latent codes that can be used for clustering real
images corresponding to their Ô¨Åne-grained classes, they cannot be applied to datasets that have no
object bounding box annotations. Moreover, they require a training of additional classiÔ¨Åers using a set
of generated images annotated with their latent values. The proposed model in this paper, C3-GAN,
can learn the latent code of foreground region in a completely unsupervised way, and simply utilize
the discriminator for inferring the clusters of a given dataset."
METHOD,0.07692307692307693,"3
METHOD"
METHOD,0.08097165991902834,"Given a dataset X = {xi}N‚àí1
i=0 consisting of single object images, we aim to distribute data into Y
semantically Ô¨Åne-grained classes. Our GAN-based model infers the clusters of data in the semantic
feature space H ‚ààRdh of the discriminator D. The feature space H is learnt by maximizing
the mutual information between the latent code, which is the input of a generator, and its image
observation ÀÜx. For more robust feature representations learning, we decompose a scene into a
background and foreground region and associate the latent code mainly with the foreground region.
We especially reformulate the information-theoretic regularization to optimize the contrastive loss
deÔ¨Åned for latent-image feature pairs to induce each cluster to be linearly separated in the feature
space."
METHOD,0.08502024291497975,Published as a conference paper at ICLR 2022 ùíÑ ùíõ ùëÆùíáùíà ùëÆùíÉùíà
METHOD,0.08906882591093117,"!ùë•
!ùë•!"" !ùë•#"" ‚®Ç"
METHOD,0.0931174089068826,"ùíØ$
Random"
METHOD,0.09716599190283401,"Matrix !ùë•#""‚Ä≤ ‚®Å ùë´ùíÉùíÇùíîùíÜ"
METHOD,0.10121457489878542,"ùëµ(ùüé, ùë∞)"
METHOD,0.10526315789473684,"ùì§(ùüé, ùíÄ-ùüè)"
METHOD,0.10931174089068826,CLUSTER 2
METHOD,0.11336032388663968,CLUSTER 3
METHOD,0.11740890688259109,"CLUSTER 1
CLUSTER CENTROID FEATURE (ùíç)"
METHOD,0.1214574898785425,IMAGE FEATURE (ùíâ)
METHOD,0.12550607287449392,CLUSTER 3
METHOD,0.12955465587044535,CLUSTER 2
METHOD,0.13360323886639677,"CLUSTER 1
ùùçùíÑ ùùçùíôùíâ ùùçùíôùíì ùùçùíô"
METHOD,0.13765182186234817,ùíâùë´ùíÉùíÇùíîùíÜ REAL
METHOD,0.1417004048582996,"or
FAKE? (ùíì)"
METHOD,0.145748987854251,"Figure 2: Overview of C3-GAN. It synthesizes a background image ÀÜxbg and a foreground image
ÀÜxfg from the background generator Gbg and the foreground generator Gfg respectively. To trigger
this decomposition, we perturb a foreground image with the random afÔ¨Åne transformation matrix TŒ∏
right before the composition of two image components. The association between the latent code c
and its image observation ÀÜx is learnt by optimizing the information-theoretic regularization that is
based on the contrastive loss deÔ¨Åned for their feature representations in the embedding space of the
discriminator. The inference of Ô¨Åne-grained class clustering is made based on the distances between
images‚Äô semantic features {hi(‚Ä¢)}N‚àí1
i=0 , that are depicted with dotted lines, and the set of cluster
centroids {li(‚ô¶)}Y ‚àí1
i=0 which are the Ô¨Åxed number of embedded latent codes c."
PRELIMINARIES,0.14979757085020243,"3.1
PRELIMINARIES"
PRELIMINARIES,0.15384615384615385,"Our proposed method is built on FineGAN (Singh et al., 2019), which is in turn based on InfoGAN
(Chen et al., 2016). InfoGAN learns to associate the latent code c with its observation ÀÜx by inducing
the model to maximize the mutual information of the two, I(c, ÀÜx). The latent code c can take
various form depending on the prior knowledge of the factor that we want to infer, and is set to
follow the uniform categorical distribution when our purpose is to make the categorical inference on
given dataset. FineGAN learns three such latent codes for hierarchical image generation, and each
code is respectively used for background, super-class object, and sub-class object image synthesis.
To facilitate this decomposition, FineGAN employs multiple pairs of generator and discriminator
and trains an auxiliary background classiÔ¨Åer using object bounding box annotations. It further
demonstrates that the latent codes for object image synthesis can be also utilized for clustering an
image dataset according to their Ô¨Åne-grained classes."
PRELIMINARIES,0.15789473684210525,"Our method differs from FineGAN in that, i) it employs the discriminator D to infer clusters
without requiring additional training of classiÔ¨Åers, and ii) it learns only one latent code c that
corresponds to the Ô¨Åne-grained class of a foreground object. The separate random noise z is kept,
that is another input of the generator, to model variations occurring in a background region. The
noise value z ‚ààRdz is sampled from the normal distribution N(0, I), and the latent code c ‚ààRY
is an 1-hot vector where the index k that makes ck = 1 is sampled from the uniform categorical
distribution, U(0, Y ‚àí1). SpeciÔ¨Åcally, the background generator Gbg synthesizes a background image
ÀÜxbg ‚ààR3√óH√óW solely from the random noise z, and the foreground generator Gfg synthesizes a
foreground mask ÀÜm ‚ààR1√óH√óW and an object image ÀÜt ‚ààR3√óH√óW using both z and c. To model
foreground variations, we convert an 1-hot latent code c to a variable c‚Ä≤ ‚ààRdc whose value is sampled
from the gaussian distribution N(¬µc, œÉc) where the mean ¬µc and diagonal covariance matrix œÉc are
computed according to the original code c (Zhang et al., 2017). The Ô¨Ånal image ÀÜx ‚ààR3√óH√óW is
the composition of generated image components summed by the hadamard product, as described in
Fig. 2."
PRELIMINARIES,0.16194331983805668,"To achieve a fully unsupervised method, we leverage the scene decomposition method proposed by
PerturbGAN (Bielski & Favaro, 2019). In speciÔ¨Åc, scene decomposition is triggered by perturbing
foreground components ÀÜm and ÀÜt with the random afÔ¨Åne transformation matrix TŒ∏ right before the
Ô¨Ånal image composition. The parameters Œ∏ of the random matrix TŒ∏ include a rotation angle, scaling
factor and translation distance, and they are all randomly sampled from the uniform distributions with
predeÔ¨Åned value ranges."
PRELIMINARIES,0.1659919028340081,"In summation, the image generation process is described as below:"
PRELIMINARIES,0.1700404858299595,"c‚Ä≤ ‚àºN(¬µc, œÉc),
ÀÜxbg, ÀÜm, ÀÜt = G(z, c‚Ä≤),"
PRELIMINARIES,0.17408906882591094,"ÀÜm‚Ä≤, ÀÜt‚Ä≤ = TŒ∏( ÀÜm, ÀÜt),
ÀÜx = ÀÜxbg ‚äô(1 ‚àíÀÜm‚Ä≤) + ÀÜt‚Ä≤ ‚äôÀÜm‚Ä≤.
(1)"
PRELIMINARIES,0.17813765182186234,Published as a conference paper at ICLR 2022
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.18218623481781376,"3.2
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.1862348178137652,"We assume that data would be well clustered when i) they form explicitly discernable cluster
boundaries in the embedding space H, and ii) each cluster centroid ly ‚ààRdh condenses distinct
semantic features. This is the exact space that the discriminator of InfoGAN aims to approach when
the latent code c is sampled from the uniform categorical distribution, U(0, Y ‚àí1). However, since it
jointly optimizes the adversarial loss, the model has a possibility of falling into the mode collapse
phenomenon where the generator G covers only a few conÔ¨Ådent modes (classes) to easily deceive
the discriminator D. This is the reason why InfoGAN lacks the ability of inferring clusters on real
image datasets. To handle this problem, we propose a new form of an auxiliary probability Q(c|x)
that represents the mutual information between the latent code c and image observation ÀÜx."
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.1902834008097166,"Let us now describe the objective functions with mathematical deÔ¨Ånitions. The discriminator D aims
to learn an adversarial feature r ‚ààR for the image authenticity discrimination, and a semantic feature
h ‚ààRdh for optimizing the information-theoretic regularization. The features are encoded from
separate branches œàr
x and œàh
x, which were split at the end of the base encoder of the discriminator
Dbase. The adversarial feature r is learnt with the hinge loss as represented in the equations below:"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.19433198380566802,"Ladv
D
= Ex‚àºX,ÀÜx‚àºG(z,c)[ min(0, 1 ‚àír) + min(0, 1 + ÀÜr) ],
Ladv
G
= EÀÜx‚àºG(z,c)[ ‚àíÀÜr ]"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.19838056680161945,"s.t.
r = œàr
x(Dbase(x)).
(2)"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.20242914979757085,"Meanwhile, the semantic feature h can be considered as a logit in a classiÔ¨Åcation model. In a
supervised setting where ground-truth class labels are given, data mapped into an embedding space
gradually form discernable cluster boundaries as the training proceeds, similar to the one in Fig. 1
(b). However, in an unsupervised setting, it is difÔ¨Åcult to attain such space simply by maximizing the
mutual information between the latent code c and its observation ÀÜx for the aforementioned reasons.
Our solution is to formulate an auxiliary probability Q(c|x) in the form of contrastive loss. We map
the latent code c onto the embedding space H with a simple linear layer œàc, and let it act as a cluster
centroid (l) that pulls semantic features of images (h) that were generated from its speciÔ¨Åc value. By
setting the relations with other latent codes as negative pairs, cluster centroids are set to be pushing
each others and form distinguishable boundaries in the embedding space. SpeciÔ¨Åcally, we expect the
semantic feature h to be mapped to the k-th cluster centroid, where k is the index of its corresponding
latent code c that makes ck = 1, while distances to other centroids ly are far enough to maximize
the mutual information of the two. To enhance the robustness of the model and assist the scene
decomposition learning, we additionally maximize the mutual information between the latent code c
and masked foreground images ÀÜxfg. To sum up, the proposed information-theoretic objectives are
deÔ¨Åned as follows:"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.20647773279352227,"Linfo = EÀÜx‚àºG(z,c)[ ‚àílog ÀÜqk ],
Linfofg = EÀÜx‚àºG(z,c)[ ‚àílog ÀÜqk
fg ]"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.21052631578947367,"s.t.
qk = Q(ck = 1|x) =
exp(sim(h, lk)/œÑ)
PY ‚àí1
y=0 exp(sim(h, ly)/œÑ)
,
h = œàh
x(Dbase(x)),
l = œàc(IY ). (3)"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.2145748987854251,"where sim(a, b) is the cosine distance between vectors a and b, y is an index of a cluster, and œÑ is the
temperature of the softmax function. IY denotes the identity matrix with the size of Y √ó Y ."
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.21862348178137653,"Additional Regularizations
Following the prior works (Ji et al., 2019; Van Gansbeke et al., 2020),
we adopt the overclustering strategy to help the model learn more expressive feature set. We also
regularize the prediction on a real image q ‚ààRY by minimizing its entropy H(q) to promote each
data to be mapped to only one cluster id with high conÔ¨Ådence, along with the minimization of the
KL divergence between batch-wisely averaged prediction ¬Øq and the uniform distribution u, that is
for avoiding a degenerated solution where only a few clusters are overly allocated. Furthermore,
we optimize the contrastive loss (Chen et al., 2020) deÔ¨Åned for real image features h to assist the
semantic feature learning. To summarize, the regularizations on real images are as follows:"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.22267206477732793,"Limg_cont = Ex‚àºX[ ‚àílog
exp(sim(h, h‚Ä≤)/œÑ)
PN‚àí1
j=0 exp(sim(h, h‚Ä≤
j)/œÑ)
],"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.22672064777327935,"Lentropy = Ex‚àºX[H(q)] + DKL(¬Øq‚à•u)
s.t.
¬Øq = Ex‚àºX[q]. (4)"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.23076923076923078,Published as a conference paper at ICLR 2022
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.23481781376518218,"We also enforce a foreground mask ÀÜm to be more like a hard mask and take up a reasonable portion
of a scene by employing below regularization function."
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.2388663967611336,"Lmask = EÀÜx‚àºG(z,c)[ Ehw[‚àíÀÜm log( ÀÜm) ‚àí(1 ‚àíÀÜm) log(1 ‚àíÀÜm)]"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.242914979757085,"+ max(0, 0.1 ‚àíEhw[ ÀÜm]) + max(0, Ehw[ ÀÜm] ‚àí0.9) ].
(5)"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.24696356275303644,"In sum, C3-GAN is trained alternately optimizing the following objective functions for D and G:"
CONTRASTIVE FINE-GRAINED CLASS CLUSTERING,0.25101214574898784,"min
D,G Ladv
D
+ Ladv
G
+ Œª0Linfo + Œª1Linfofg + Œª2Limg_cont + Œª3Lentropy + Œª4Lmask.
(6)"
EXPERIMENTS,0.2550607287449393,"4
EXPERIMENTS"
EXPERIMENTAL SETTING,0.2591093117408907,"4.1
EXPERIMENTAL SETTING"
EXPERIMENTAL SETTING,0.2631578947368421,"Datasets.
We tested our method on 4 datasets that consist of single object images. i) CUB (Wah
et al., 2011): 5,994 training and 5,794 test images of 200 bird species. ii) Stanford Cars (Krause
et al., 2013): 8,144 training and 8,041 test images of 196 car models. iii) Stanford Dogs (Khosla
et al., 2011): 12,000 training and 8,580 test images of 120 dog species. iv) Oxford Flower (Nilsback
& Zisserman, 2008): 2,040 training and 6,149 test images of 102 Ô¨Çower categories. Due to the small
number of training images, all models for CUB and Oxford Flower datasets were trained with the
entire dataset as the prior works did (Singh et al., 2019; Li et al., 2020; Benny & Wolf, 2020)."
EXPERIMENTAL SETTING,0.26720647773279355,"Implementation Details.
The weights of the loss terms (Œª0, Œª1, Œª2, Œª3, Œª4) are set as (5, 1, 1, 0.1,
1), and the temperature œÑ is set as 0.1. We utilized Adam optimizer of which learning rate is 0.0002
and values of momentum coefÔ¨Åcients are (0.5, 0.999). The architectural speciÔ¨Åcation of C3-GAN
and the values of hyperparameters, such as the parameter ranges of the random afÔ¨Åne transformation
and the number of clusters Y can be found in Appendix A.1 and Appendix A.2, respectively."
EXPERIMENTAL SETTING,0.27125506072874495,"Baselines.
We Ô¨Årst compared C3-GAN with IIC (Ji et al., 2019) in order to emphasize the dif-
ference between the coarse-grained and Ô¨Åne-grained class clustering task, since it achieved decent
performance with sobel Ô¨Åltered images where color and texture informations are discarded. We also
experimented with simple baseline of SimCLR+K-Means (Chen et al., 2020) to claim that optimiz-
ing instance discrimination task is not enough for Ô¨Åne-grained feature learning. SCAN (Van Gansbeke
et al., 2020), the work that shows a remarkable coarse-grained class clustering performance, is also
compared to investigate whether the method of inducing consistent predictions for all nearest neigh-
bors would be helpful for the Ô¨Åne-grained class clustering task as well. Finally, we compared with
the methods that learn hierarchically Ô¨Åne-grained feature representations, such as FineGAN (Singh
et al., 2019), MixNMatch (Li et al., 2020) and OneGAN (Benny & Wolf, 2020), including their base
model InfoGAN (Chen et al., 2016), to claim the efÔ¨Åcacy of the proposed formulation that is based
on the contrastive loss. We mainly compared with the unsupervised version of them that is trained
with pseudo-labeled bounding boxes which assign edges of real images as background region."
FINE-GRAINED CLASS CLUSTERING RESULTS,0.27530364372469635,"4.2
FINE-GRAINED CLASS CLUSTERING RESULTS"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.2793522267206478,"Quantitative results.
We evaluated clustering performance with two metrics: Accuracy (Acc) and
Normalized Mutual Information (NMI). The score of accuracy is calculated following the optimal
mapping between cluster indices and real classes inferred by Hungarian algorithm (Kuhn & Yaw,
1955) for a given contingency table. We presented the results in Table 1. As it can be seen, C3-GAN
outperforms other methods by remarkable margins in terms of Acc on all datasets. Our method
presents better or comparable performance in terms of NMI scroes as well. The results of IIC
underline that understanding only structural characteristic is not enough for the Ô¨Åne-grained class
clustering task. From the results of SCAN (2nd rank) and MixNMatch (3rd rank), we can conjecture
that both requiring local disentanglement of the embedding space and extracting foreground features
via scene decomposition learning can be helpful for improving the Ô¨Åne-grained class clustering
performance. However, it is difÔ¨Åcult to determine whether the GAN-based methods or the SSL-
based methods are better because both approaches have similar score ranges for all metrics and
display globally entangled feature spaces as shown in Fig. 3 (a) and (b). Since C3-GAN resembles"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.2834008097165992,Published as a conference paper at ICLR 2022
FINE-GRAINED CLASS CLUSTERING RESULTS,0.2874493927125506,"Acc ‚Üë
NMI ‚Üë"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.291497975708502,"Bird
Car
Dog Flower
Bird
Car
Dog Flower"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.29554655870445345,"FineGAN‚Ä† (Singh et al., 2019)
12.6 7.8
7.9
-
0.40 0.35 0.23
-
MixNMatch‚Ä† (Li et al., 2020)
13.6 7.9
8.9
-
0.42 0.36 0.32
-
OneGAN‚Ä† (Benny & Wolf, 2020)
10.1 6.0
7.3
-
0.39 0.27 0.21
-
Fully Unsupervised Setting
IIC (Ji et al., 2019)
7.4
4.9
5.0
8.7
0.36 0.27 0.18 0.24
SimCLR (Chen et al., 2020) +k-Means
8.4
6.7
6.8
12.5
0.40 0.33 0.19 0.29
InfoGAN (Chen et al., 2016)
8.6
6.5
6.4
23.2
0.39 0.31 0.21 0.44
FineGAN w/o labels
6.9
6.8
6.0
8.1
0.37 0.33 0.22 0.24
MixNMatch w/o labels
10.2 7.3 10.3 39.0
0.41 0.34 0.30 0.57
SCAN (Van Gansbeke et al., 2020)
11.9 8.8 12.3 56.5
0.45 0.38 0.35 0.77
C3-GAN (Ours)
27.6 14.1 17.9 67.8
0.53 0.41 0.36 0.67"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.29959514170040485,"Table 1: Quantitative evaluation of clustering performance. ‚Ä† denotes that the values are reported
ones in the original papers, and the rest scores are obtained by experimenting with the released codes
of baselines on our set of evaluation datasets. Please note that the original methods of FineGAN,
MixNMatch, and OneGAN utilize human-annotated labels."
FINE-GRAINED CLASS CLUSTERING RESULTS,0.30364372469635625,"(a) MixNMatch (Li et al., 2020)
(b) SCAN (Van Gansbeke et al., 2020)
(c) C3-GAN (Ours)"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3076923076923077,"Figure 3: Visualization of embedding spaces of MixNMatch, SCAN and C3-GAN that were trained
on CUB dataset. To this end, we reduced the semantic features of images sampled from 10 classes
into a 2-dimensional space through t-SNE. This result implies that only C3-GAN has learnt the
embedding space where clusters are explicitly separable."
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3117408906882591,"MixNMatch to some extent, its state-of-the-art scores can be partially explained by the fact that it
learns decomposed foreground features, but the embedding space of C3-GAN visualized in Fig. 3 (c)
implies that the performance of it has been further improved from our formulation of Q(c|x) that
enforces clusters to be distinctively distributed in the embedding space. We additionally presented the
results of qualitative analysis on clustered images inferred by the top three methods in Appendix A.3,
which better demonstrate the excellence of our method compared to prior works."
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3157894736842105,"Ablation Study.
We conducted an ablation study regarding three factors: i) Overclustering, ii)
Foreground perturbation, and iii) The information-theoretic regularization based on the contrastive
loss. We investigated clustering performance of C3-GAN on CUB dataset by removing each factor.
The results can be seen in Table 2. The largest performance degradation was caused by changing the
formulation of the information-theoretic regularization to a trivial softmax function. This result implies
that clustering performance has a high correlation with the degree of separability of clusters, which
was signiÔ¨Åcantly improved by our method. Regarding the result of ii), we observed that C3-GAN
fails to decompose a scene when the foreground perturbation was not implemented. This degenerated
result suggests that the method of extracting only foreground features has a non-negligible amount of
inÔ¨Çuence in clustering performance. Lastly, the result of i) implies that the overclustering strategy
is also helpful for boosting performance since it allows a larger capacity for model to learn more
expressive feature set. The additional analysis on hyperparameters can be found in Appendix A.2."
FINE-GRAINED CLASS CLUSTERING RESULTS,0.31983805668016196,Published as a conference paper at ICLR 2022
FINE-GRAINED CLASS CLUSTERING RESULTS,0.32388663967611336,"Acc ‚Üë
NMI ‚Üë"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.32793522267206476,"C3-GAN (Ours)
27.6
0.53
i)
‚Äì Overclustering
22.7
0.50
ii)
‚Äì Foreground perturbation
16.6
0.47
iii)
‚Äì The information-theoretic regularization based on the contrastive loss
14.5
0.45"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3319838056680162,"Table 2: Result of an ablation study. We observed that the largest performance degradation was caused
by changing the formulation of the information-theoretic regularization to a trivial softmax function.
The other two factors also have a non-negligible impact on performance."
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3360323886639676,"Real
Fake images
Real
Fake images
Real
Fake images"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.340080971659919,"Background
Background
Background"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3441295546558704,"+ Foreground
+ Foreground
+ Foreground (a)"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3481781376518219,"Fixed c and variable z
Variable c and Ô¨Åxed z (b)"
FINE-GRAINED CLASS CLUSTERING RESULTS,0.3522267206477733,"Figure 4: Qualitative analysis of the image generation performance. We present (a) images synthesized
with the cluster indices of real images that were predicted by the discriminator, and (b) images
synthesized by controlling values of the latent code c and the random noise z."
IMAGE GENERATION RESULTS,0.3562753036437247,"4.3
IMAGE GENERATION RESULTS"
IMAGE GENERATION RESULTS,0.3603238866396761,"Qualitative Results.
For qualitative evaluation, we analyzed the results on CUB dataset. In Fig. 4
(a), we displayed synthesized images along with their decomposed scene elements. The images were
generated with the cluster indices of real images that were predicted by the discriminator. Considering
the consistency of foreground object features within the same class and the uniqueness of each
class feature, we can assume that C3-GAN succeeded in learning the set of clusters in which each
cluster represents its unique characteristic explicitly. Further, to investigate the roles of the two input
variables, c and z, we analyzed the change in the synthesized images when they were generated by
Ô¨Åxing one of the two and diversifying the other, as shown in Fig. 4 (b). As it can be seen, if c is Ô¨Åxed,
all images depict the same bird species, but the pose of a bird and background vary depending on
the value of z. Conversely, when generated under the condition of variable c and Ô¨Åxed z, images of
various bird species with the same pose and background were observed. The results on other datasets
show the same trend. Please refer to Appendix A.4."
IMAGE GENERATION RESULTS,0.3643724696356275,"It is also notable that C3-GAN can effectively alleviate the mode collapse issue with the proposed
method. Fig. 5 present that only C3-GAN reÔ¨Çects intra-class deviations of various factors such as
color, shape, and layout for the given class, while other baselines produce images only with the layout
variation. We conjecture that this performance was achieved because the learnt embedding space of
the discriminator better represents the real data distribution, allowing the generator to get quality"
IMAGE GENERATION RESULTS,0.3684210526315789,Published as a conference paper at ICLR 2022
IMAGE GENERATION RESULTS,0.3724696356275304,"Real
FineGAN (Singh et al., 2019)"
IMAGE GENERATION RESULTS,0.3765182186234818,"C3-GAN (Ours)
MixNMatch (Li et al., 2020)"
IMAGE GENERATION RESULTS,0.3805668016194332,"Figure 5: Images synthesized with the cluster index that corresponds to the class of real samples.
Only C3-GAN generates images reÔ¨Çecting the intra-class color, shape, and layout variations of the
real image set."
IMAGE GENERATION RESULTS,0.38461538461538464,"FID ‚Üì
IS ‚Üë
Reverse KL ‚Üì"
IMAGE GENERATION RESULTS,0.38866396761133604,"Bird
Car
Dog
Flower
Bird
Car
Dog
Flower
Bird
Car
Dog Flower"
IMAGE GENERATION RESULTS,0.39271255060728744,"InfoGAN (Chen et al., 2016)
35.71 70.91 59.07 78.37
8.20 3.25 6.91 13.39
0.56 0.73 0.40 0.42
FineGAN w/o labels
33.87 87.66 49.12 35.72
10.07 4.20 10.67 10.42
0.67 0.58 0.24 0.52
MixNMatch w/o labels
31.59 78.36 48.11 32.03
9.76 4.11 10.70 16.48
0.52 0.57 0.22 0.34
C3-GAN (Ours)
19.37 67.36 45.40 64.19
14.52 3.62 9.33 13.75
0.29 0.57 0.18 0.25"
IMAGE GENERATION RESULTS,0.3967611336032389,Table 3: Quantitative evaluation of image synthesis performance.
IMAGE GENERATION RESULTS,0.4008097165991903,"signals for the adversarial learning. Additionally, we found that only C3-GAN succeeded in the scene
decomposition learning when annotations were not utilized. In fact, when FineGAN and MixNMatch
were trained without object bounding box annotations, they fell into degenerate solutions where the
entire image is synthesized from the background generators."
IMAGE GENERATION RESULTS,0.4048582995951417,"Quantitative Results.
We also quantitatively evaluated the image synthesis performance based
on the scores of Frechet Inception Distance (FID) and Inception Score (IS). We also considered the
scores of reverse KL which measures the distance between the predicted class distribution of real
images and synthesized images. We compared C3-GAN with GAN-based models that were trained
without object bounding box annotations. The scores of IS and reverse KL were measured with
Inception networks that were Ô¨Åne-tuned on each dataset using the method of Cui et al. (2018)1. The
results are displayed in Table 3. C3-GAN presents the state-of-the-art or comparable performance in
terms of all metrics. This result means that, in addition to the class clustering performance, C3-GAN
has competitive performance in generating Ô¨Åne-grained object images."
CONCLUSION,0.4089068825910931,"5
CONCLUSION"
CONCLUSION,0.41295546558704455,"In this study, we proposed a new GAN-based method, C3-GAN, for unsupervised Ô¨Åne-grained class
clustering which is more challenging and less explored than a coarse-grained clustering task. To
improve the Ô¨Åne-grained class clustering performance, we formulate the information-theoretic regu-
larization based on the contrastive loss. Also, a scene decomposition-based approach is incorporated
into the framework to enforce the model to learn features focusing on a foreground object. Extensive
experiments show that our C3-GAN not only outperforms previous Ô¨Åne-grained clustering methods
but also synthesizes Ô¨Åne-grained object images with comparable quality, while alleviating mode
collapse that previous state-of-the-art GAN methods have been suffering from."
CONCLUSION,0.41700404858299595,"1The accuracy of Inception Networks Ô¨Åne-tuned on CUB/Stanford Cars/Stanford Dogs/Oxford Flower
datasets are 0.87/0.47/0.86/0.94, respectively."
CONCLUSION,0.42105263157894735,Published as a conference paper at ICLR 2022
ETHICS STATEMENT,0.4251012145748988,ETHICS STATEMENT
ETHICS STATEMENT,0.4291497975708502,"Remarkable advancement of generative models has played a crucial role as AI tools for text (Brown
et al., 2020; Kim et al., 2021), image (Karras et al., 2020; Choi et al., 2020b), audio (Sisman et al.,
2020), and multimodal content generation (Tian et al., 2021; Yan et al., 2021). However, as its
side effect, many malicious applications have been reported as well, thus leading to severe societal
problems, such as deepfake (Dolhansky et al., 2020), fake news (Lazer et al., 2018), and generation
biased by training data (Gupta et al., 2021). Our work might be an extension of the harmful effects of
generative models. On the contrary, generative models can be a solution to alleviate these side effects
via data augmentation (Yang et al., 2019; Choi et al., 2020a). In particular, our method can contribute
to alleviating data bias and Ô¨Åne-grained class imbalance. Therefore, many endless efforts are required
to make these powerful generative models beneÔ¨Åt humans."
ETHICS STATEMENT,0.4331983805668016,ACKNOWLEDGMENTS
ETHICS STATEMENT,0.43724696356275305,"This work was experimented on the NAVER Smart Machine Learning (NSML) platform (Sung et al.,
2017; Kim et al., 2018). We are especially grateful to Jun-Yan Zhu and NAVER AI Lab researchers
for their constructive comments."
REFERENCES,0.44129554655870445,REFERENCES
REFERENCES,0.44534412955465585,"Rameen Abdal, Peihao Zhu, Niloy J. Mitra, and Peter Wonka. Labels4free: Unsupervised segmenta-
tion using stylegan. In ICCV, 2021."
REFERENCES,0.4493927125506073,"Yaniv Benny and Lior Wolf. Onegan: Simultaneous unsupervised learning of conditional image
generation, foreground segmentation, and Ô¨Åne-grained clustering. In ECCV, 2020."
REFERENCES,0.4534412955465587,"Adam Bielski and Paolo Favaro. Emergence of object segmentation in perturbed generative models.
In NeurIPS, 2019."
REFERENCES,0.4574898785425101,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high Ô¨Ådelity
natural image synthesis. In ICLR, 2019."
REFERENCES,0.46153846153846156,"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. In NeurIPS, 2020."
REFERENCES,0.46558704453441296,"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsuper-
vised learning of visual features. In ECCV, 2018."
REFERENCES,0.46963562753036436,"Micka√´l Chen, Thierry Arti√®res, and Ludovic Denoyer.
Unsupervised object segmentation by
redrawing. In NeurIPS, 2019."
REFERENCES,0.47368421052631576,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In ICML, 2020."
REFERENCES,0.4777327935222672,"Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
NeurIPS, 2016."
REFERENCES,0.4817813765182186,"Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon. Fair generative modeling via
weak supervision. In ICML, 2020a."
REFERENCES,0.48582995951417,"Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for
multiple domains. In CVPR, 2020b."
REFERENCES,0.4898785425101215,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single layer networks in unsupervised
feature learning. In AISTATS, 2011."
REFERENCES,0.4939271255060729,"Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Belongie Serge. Large scale Ô¨Åne-grained
categorization and domain-speciÔ¨Åc transfer learning. In CVPR, 2018."
REFERENCES,0.4979757085020243,Published as a conference paper at ICLR 2022
REFERENCES,0.5020242914979757,"Kamran Ghasedi Dizaji, Amirhossein Herandi, and Heng Huang. Deep clustering via joint convolu-
tional autoencoder embedding and relative entropy minimization. In NeurIPS, 2017."
REFERENCES,0.5060728744939271,"Brian Dolhansky, Joanna Bitton, Ben PÔ¨Çaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cris-
tian Canton Ferrer. The deepfake detection challenge (dfdc) dataset. In arXiv, 2020."
REFERENCES,0.5101214574898786,"Jeff Donahue, Philipp Kr√§henb√ºhl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017."
REFERENCES,0.5141700404858299,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014."
REFERENCES,0.5182186234817814,"Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, Bilal Piot, Koray Kavukcuoglu, R√©mi Munos, and Michal Valko. Bootstrap your own latent:
A new approach to self-supervised learning. In NeurIPS, 2020."
REFERENCES,0.5222672064777328,"Aman Gupta, Deepak Bhatt, and Anubha Pandey. Transitioning from real to synthetic data: Quantify-
ing the bias in model. In Synthetic Data Generation Workshop at ICLR 2021, 2021."
REFERENCES,0.5263157894736842,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020."
REFERENCES,0.5303643724696356,"Irina Higgins, Lo√Øc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In ICLR, 2017."
REFERENCES,0.5344129554655871,"Xu Ji, Jo√£o F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classiÔ¨Åcation and segmentation. In ICCV, 2019."
REFERENCES,0.5384615384615384,"Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In CVPR, 2019."
REFERENCES,0.5425101214574899,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. A
style-based generator architecture for generative adversarial networks. In CVPR, 2020."
REFERENCES,0.5465587044534413,"Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for
Ô¨Åne-grained image categorization. In First Workshop on Fine-Grained Visual Categorization,
2011."
REFERENCES,0.5506072874493927,"Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon,
Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, et al. What changes can large-scale
language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained
transformers. In EMNLP, 2021."
REFERENCES,0.5546558704453441,"Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park, Soeun Park, Hyunwoo
Jo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, et al. Nsml: Meet the mlaas platform with a
real-world case study. arXiv preprint arXiv:1810.09957, 2018."
REFERENCES,0.5587044534412956,"Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for Ô¨Åne-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition, 2013."
REFERENCES,0.562753036437247,"H. W. Kuhn and Bryn Yaw. The hungarian method for the assignment problem. In Naval Research
Logistics Quarterly, 1955."
REFERENCES,0.5668016194331984,"David MJ Lazer, Matthew A Baum, Yochai Benkler, Adam J Berinsky, Kelly M Greenhill, Filippo
Menczer, Miriam J Metzger, Brendan Nyhan, Gordon Pennycook, David Rothschild, et al. The
science of fake news. In AAAS, 2018."
REFERENCES,0.5708502024291497,"Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. In ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2010."
REFERENCES,0.5748987854251012,"Junnan Li, Pan Zhou, Caiming Xiong, and Steven C.H. Hoi. Prototypical contrastive learning of
unsupervised representations. In ICLR, 2021."
REFERENCES,0.5789473684210527,Published as a conference paper at ICLR 2022
REFERENCES,0.582995951417004,"Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, and Yong Jae Lee. Mixnmatch: Multifactor
disentanglement and encoding for conditional image generation. In CVPR, 2020."
REFERENCES,0.5870445344129555,"Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image
generation via self-conditioned gans. In CVPR, 2020."
REFERENCES,0.5910931174089069,"Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Finding an unsupervised
image segmenter in each of your deep generative models. In arXiv, 2021."
REFERENCES,0.5951417004048583,"Maria-Elena Nilsback and Andrew Zisserman. Automated Ô¨Çower classiÔ¨Åcation over a large number
of classes. In ICCVGIP, 2008."
REFERENCES,0.5991902834008097,"Pedro Savarese, Sunnie S. Y. Kim, Michael Maire, Greg Shakhnarovich, and David McAllester.
Information-theoretic segmentation by inpainting error maximization. In CVPR, 2021."
REFERENCES,0.6032388663967612,"Krishna Kumar Singh, Utkarsh Ojha, and Yong Jae Lee. Finegan: Unsupervised hierarchical disen-
tanglement for Ô¨Åne-grained object generation and discovery. In CVPR, 2019."
REFERENCES,0.6072874493927125,"Berrak Sisman, Junichi Yamagishi, Simon King, and Haizhou Li. An overview of voice conversion
and its challenges: From statistical modeling to deep learning. In TASLP, 2020."
REFERENCES,0.611336032388664,"Nako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang, Jingwoong Kim, Leonard Lausen, Youngkwan
Kim, Gayoung Lee, Donghyun Kwak, Jung-Woo Ha, and Sunghun Kim. Nsml: A machine learning
platform that enables you to focus on your models. In arXiv, 2017."
REFERENCES,0.6153846153846154,"Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov.
A good image generator is what you need for high-resolution video synthesis. In ICLR, 2021."
REFERENCES,0.6194331983805668,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. In arXiv, 2018."
REFERENCES,0.6234817813765182,"Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc
Van Gool. Scan: Learning to classify images without labels. In ECCV, 2020."
REFERENCES,0.6275303643724697,"Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan
latent space. In ICML, 2020."
REFERENCES,0.631578947368421,"Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. In California Institute of Technology, 2011."
REFERENCES,0.6356275303643725,"Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In ICML, 2020."
REFERENCES,0.6396761133603239,"Junyuan Xie, Ross B. Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering
analysis. In ICML, 2016."
REFERENCES,0.6437246963562753,"Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using
vq-vae and transformers. In arXiv, 2021."
REFERENCES,0.6477732793522267,"Shuo Yang, Kai Shu, Suhang Wang, Renjie Gu, Fan Wu, and Huan Liu. Unsupervised fake news
detection on social media: A generative approach. In AAAI, 2019."
REFERENCES,0.6518218623481782,"Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. In ICCV, 2017."
REFERENCES,0.6558704453441295,"Huasong Zhong, Chong Chen, Zhongming Jin, and Xian-Sheng Hua. Deep robust clustering by
contrastive learning. In arXiv, 2020."
REFERENCES,0.659919028340081,Published as a conference paper at ICLR 2022
REFERENCES,0.6639676113360324,"A
APPENDIX"
REFERENCES,0.6680161943319838,"A.1
NETWORK ARCHITECTURE"
REFERENCES,0.6720647773279352,"We present detailed description of the architecture of C3-GAN in Table 4 and Table 5. The colored
dots (‚Ä¢, ‚Ä¢, ‚Ä¢) are for indicating that the outputs of earlier layers are used as the inputs of later layers.
The training was done with 2 NVIDIA-V100 GPUs, and we optimized a model until the convergence
of the FID score."
REFERENCES,0.6761133603238867,"Module
Layers
Input size
Output size Gbg"
REFERENCES,0.680161943319838,"Linear, BN, GLU, UnSqueeze
64 (z)
512√ó4√ó4"
REFERENCES,0.6842105263157895,"[Up, Conv(3,1), BN, GLU]√ó5
512√ó4√ó4
64√ó128√ó128"
REFERENCES,0.6882591093117408,"[Conv(3,1), BN, GLU, Conv(3,1), BN]√ó3
64√ó128√ó128
64√ó128√ó128"
REFERENCES,0.6923076923076923,"Conv(3,1), Tanh
64√ó128√ó128
3√ó128√ó128 (ÀÜxbg)"
REFERENCES,0.6963562753036437,"N(¬µc, œÉc)
Linear, BN, GLU
Y (c)
8 ‚Ä¢"
REFERENCES,0.7004048582995951,"Gbase
fg"
REFERENCES,0.7044534412955465,"Linear, BN, GLU, Reshape
64 (z)
512√ó4√ó4"
REFERENCES,0.708502024291498,"[Up, Conv(3,1), BN, GLU]√ó5
(512+8 ‚Ä¢)√ó4√ó4
16√ó128√ó128"
REFERENCES,0.7125506072874493,"[Conv(3,1), BN, GLU, Conv(3,1), BN]√ó3
16√ó128√ó128
16√ó128√ó128 ‚Ä¢"
REFERENCES,0.7165991902834008,"Gm
fg
Conv(3,1), BN, GLU
16√ó128√ó128 ‚Ä¢
64√ó128√ó128"
REFERENCES,0.7206477732793523,"Conv(3,1), Sigm
64√ó128√ó128
1√ó128√ó128 ( ÀÜ
m) Gt
fg"
REFERENCES,0.7246963562753036,"Conv(3,1), BN, GLU
(16‚Ä¢ + Y (c))√ó128√ó128
16√ó128√ó128"
REFERENCES,0.728744939271255,"[Conv(3,1), BN, GLU, Conv(3,1), BN]√ó2
16√ó128√ó128
16√ó128√ó128"
REFERENCES,0.7327935222672065,"Conv(3,1), BN, GLU
16√ó128√ó128
16√ó128√ó128"
REFERENCES,0.7368421052631579,"Conv(3,1), Tanh
16√ó128√ó128
3√ó128√ó128 (ÀÜt)"
REFERENCES,0.7408906882591093,Table 4: Architectural description of the generator of C3-GAN.
REFERENCES,0.7449392712550608,"Module
Layers
Input size
Output size Dbase"
REFERENCES,0.7489878542510121,"Conv(4,2), LReLU
3√ó128√ó128 (x)
64√ó64√ó64"
REFERENCES,0.7530364372469636,"[Conv(4,2), BN, LReLU]√ó4
64√ó64√ó64
512√ó4√ó4"
REFERENCES,0.757085020242915,"Conv(3,1), BN, LReLU
512√ó4√ó4
512√ó4√ó4 ‚Ä¢"
REFERENCES,0.7611336032388664,"œàr
x
Conv(4,4), Squeeze
512√ó4√ó4 ‚Ä¢
1 (r)"
REFERENCES,0.7651821862348178,"œàh
x
Conv(3,1), BN, LReLU
512√ó4√ó4 ‚Ä¢
512√ó4√ó4"
REFERENCES,0.7692307692307693,"Conv(4,4), Squeeze
512√ó4√ó4
512 (h)"
REFERENCES,0.7732793522267206,"œàc
Linear
Y (c)
512 (l)"
REFERENCES,0.7773279352226721,Table 5: Architecture description of the discriminator of C3-GAN.
REFERENCES,0.7813765182186235,"‚Ä¢ Conv(a,b) : 2D Convolution layer with kernel size a and stride b"
REFERENCES,0.7854251012145749,‚Ä¢ GLU : Gated Linear Units
REFERENCES,0.7894736842105263,Published as a conference paper at ICLR 2022
REFERENCES,0.7935222672064778,"A.2
ADDITIONAL ABLATION STUDY"
REFERENCES,0.7975708502024291,"Random afÔ¨Åne transformation.
We experimented with two types of purtubing policy, a weak
and strong random afÔ¨Åne transformation. The detailed value ranges of their parameters are presented
in Table 6. The type of perturbation used for each dataset was determined by manually checking
randomly perturbed real images. The strong perturbation was only used for CUB dataset, and the rest
datasets used the weak perturbation."
REFERENCES,0.8016194331983806,"Criteria
Scale
Rotation
Translation
Dataset"
REFERENCES,0.805668016194332,"Weak perturbation
(0.9, 1.1)
(-2, 2)
(-0.08, 0,08)
Stanford Cars, Stanford Dogs,
Oxford Flower
Strong perturbation
(0.8, 1.5)
(-15, 15)
(-0.15, 0,15)
CUB"
REFERENCES,0.8097165991902834,Table 6: Details of random afÔ¨Åne transformation.
REFERENCES,0.8137651821862348,"Overclustering.
We use four datasets for the evaluation, including CUB, Standford Cars, Stanford
Dogs and Oxford Flower. As we described in 3.2, we employ the overclustering policy which is
to set the number of clusters to be multiple times of the actual number of classes. To Ô¨Ånd the
optimal setting, we compared the results of C3-GAN by setting the number of clusters as 1, 2, and
3 times the actual number of classes. The cluster numbers for each dataset were determined based
on the results in Table 7. We found that the performance generally tends to improve as the number
of clusters increases. This implies that overclustering is indeed helpful for the expressive feature
learning. To further investigate the results when we are not aware of the actual number of classes, we
additionally conducted all experiments by setting the number of clusters as 100 (underclustering)
or 500 (overclustering). The results are represented in Table 8. For the underclustering setting, we
only report the scores of NMI, since some classes have no matching cluster indices. Despite the less
promising performance of the underclustering case, this result implies that our method can be applied
to any datasets whose cluster size is not available, if we set a large enough number of clusters."
REFERENCES,0.8178137651821862,"Acc ‚Üë
NMI ‚Üë
FID ‚Üì"
REFERENCES,0.8218623481781376,"CUB
√ó 1
22.7
0.50
18.75"
REFERENCES,0.8259109311740891,"√ó 2
27.6
0.53
19.37"
REFERENCES,0.8299595141700404,"√ó 3
26.3
0.51
17.13
Stanford Cars
√ó 1
8.3
0.33
64.55"
REFERENCES,0.8340080971659919,"√ó 2
11.5
0.39
66.65"
REFERENCES,0.8380566801619433,"√ó 3
14.1
0.41
67.36
Stanford Dogs
√ó 1
11.8
0.30
51.37"
REFERENCES,0.8421052631578947,"√ó 2
15.8
0.35
54.82"
REFERENCES,0.8461538461538461,"√ó 3
17.9
0.36
45.40
Oxford Flower
√ó 1
55.6
0.72
75.12"
REFERENCES,0.8502024291497976,"√ó 2
61.7
0.67
74.59"
REFERENCES,0.854251012145749,"√ó 3
67.8
0.67
64.19"
REFERENCES,0.8582995951417004,Table 7: Results of hyperparameter search for overclustering.
REFERENCES,0.8623481781376519,"Acc ‚Üë
NMI ‚Üë
# of clusters
Bird
Car
Dog Flower
Bird
Car
Dog Flower"
REFERENCES,0.8663967611336032,"Actual number of classes √ó 3 (Overclustering)
26.3 14.1 17.9 67.8
0.51 0.41 0.36 0.67
100 (Underclustering)
-
-
-
-
0.38 0.25 0.32 0.75
500 (Overclustering)
23.7 12.1 20.5 70.8
0.51 0.40 0.39 0.65"
REFERENCES,0.8704453441295547,Table 8: Clustering performance when the number of clusters are arbitrarily set.
REFERENCES,0.8744939271255061,Published as a conference paper at ICLR 2022
REFERENCES,0.8785425101214575,"A.3
QUALITATIVE ANALYSIS OF CLUSTERING RESULTS"
REFERENCES,0.8825910931174089,"We present the clustered images of CUB and Oxford Flowers datasets, along with the results of
MixNMatch and SCAN in Figs. 6 and 7. From these results, we can assume that our method is better
at clustering compared to the baseline methods. It is worth noting that even incorrectly assigned
images look very similar with the given bird species for C3-GAN, while the results of other methods
contain objects that are quite deviated from the condition. We could observe the similar trend for
Oxford Flowers dataset, as it is shown in Fig. 7."
REFERENCES,0.8866396761133604,"(a) C3-GAN (Ours)
(b) SCAN (Van Gansbeke et al., 2020)
(c) MixNMatch (Li et al., 2020)"
REFERENCES,0.8906882591093117,"Figure 6: Examples of clustered images of CUB dataset. The cluster indices for the Ô¨Årst and the
second sets are the ones mapped from the real classes Cerulean Warbler and le Conte Sparrow via
Hungarian Algorithm. Red boxes denote incorrectly assigned images."
REFERENCES,0.8947368421052632,Published as a conference paper at ICLR 2022
REFERENCES,0.8987854251012146,"(a) C3-GAN (Ours)
(b) SCAN (Van Gansbeke et al., 2020)
(c) MixNMatch (Li et al., 2020)"
REFERENCES,0.902834008097166,"Figure 7: Examples of clustered images of Flower dataset. The cluster indices for the Ô¨Årst and the
second sets are the ones mapped from the real classes 35 and 66 via Hungarian Algorithm. Red boxes
denote incorrectly assigned images."
REFERENCES,0.9068825910931174,Published as a conference paper at ICLR 2022
REFERENCES,0.9109311740890689,"A.4
ADDITIONAL IMAGE GENERATION RESULTS"
REFERENCES,0.9149797570850202,"A.4.1
CUB"
REFERENCES,0.9190283400809717,"Real
Fake images
Real
Fake images
Real
Fake images"
REFERENCES,0.9230769230769231,"Background
Background
Background"
REFERENCES,0.9271255060728745,"+ Foreground
+ Foreground
+ Foreground"
REFERENCES,0.9311740890688259,"Fixed c and variable z
Varible c and Ô¨Åxed z"
REFERENCES,0.9352226720647774,"A.4.2
STANFORD CARS"
REFERENCES,0.9392712550607287,"Real
Fake images
Real
Fake images
Real
Fake images"
REFERENCES,0.9433198380566802,"Background
Background
Background"
REFERENCES,0.9473684210526315,"+ Foreground
+ Foreground
+ Foreground"
REFERENCES,0.951417004048583,"Fixed c and variable z
Varible c and Ô¨Åxed z"
REFERENCES,0.9554655870445344,Published as a conference paper at ICLR 2022
REFERENCES,0.9595141700404858,"A.4.3
STANFORD DOGS"
REFERENCES,0.9635627530364372,"Real
Fake images
Real
Fake images
Real
Fake images"
REFERENCES,0.9676113360323887,"Background
Background
Background"
REFERENCES,0.97165991902834,"+ Foreground
+ Foreground
+ Foreground"
REFERENCES,0.9757085020242915,"Fixed c and variable z
Varible c and Ô¨Åxed z"
REFERENCES,0.979757085020243,"A.4.4
OXFORD FLOWER"
REFERENCES,0.9838056680161943,"Real
Fake images
Real
Fake images
Real
Fake images"
REFERENCES,0.9878542510121457,"Background
Background
Background"
REFERENCES,0.9919028340080972,"+ Foreground
+ Foreground
+ Foreground"
REFERENCES,0.9959514170040485,"Fixed c and variable z
Varible c and Ô¨Åxed z"
