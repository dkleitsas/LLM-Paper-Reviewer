Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0048543689320388345,"We study the generalization of noisy stochastic mini-batch based iterative algo-
rithms based on the notion of stability. Recent years have seen key advances in
data-dependent generalization bounds for noisy iterative learning algorithms such
as stochastic gradient Langevin dynamics (SGLD) based on (Mou et al., 2018; Li
et al., 2020) and related approaches (Negrea et al., 2019; Haghifam et al., 2020).
In this paper, we unify and substantially generalize stability based generalization
bounds and make three technical advances. First, we bound the generalization
error of general noisy stochastic iterative algorithms (not necessarily gradient de-
scent) in terms of expected stability, which in turn can be bounded by the expected
Le Cam Style Divergence (LSD). Such bounds have a O(1/n) sample dependence
unlike many existing bounds with O(1/√n) dependence. Second, we introduce
Exponential Family Langevin Dynamics (EFLD) which is a substantial general-
ization of SGLD and which allows exponential family noise to be used with gradi-
ent descent. We establish data-dependent expected stability based generalization
bounds for general EFLD. Third, we consider an important new special case of
EFLD: noisy sign-SGD, which extends sign-SGD by using Bernoulli noise over
{−1, +1}, and we establish optimization guarantees for the algorithm. Further,
we present empirical results on benchmark datasets to illustrate the our bounds
are non-vacuous and quantitatively much sharper than existing bounds."
INTRODUCTION,0.009708737864077669,"1
INTRODUCTION"
INTRODUCTION,0.014563106796116505,"Recent years have seen renewed interest and advances in characterizing generalization performance
of learning algorithms in terms of stability, which considers change in performance of a learning
algorithm based on change of a single training point (Hardt et al., 2016; Bousquet & Elisseeff, 2002;
Li et al., 2020; Mou et al., 2018). For stochastic gradient descent (SGD), Hardt et al. (2016) estab-
lished generalization bounds based on uniform stability (Hardt et al., 2016; Bousquet & Elisseeff,
2002), although the analysis needed rather small step sizes ηt = 1/t which is not useful in practice.
While improving the analysis for SGD has remained a challenge, advances have been made on noisy
SGD algorithms, especially stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011;
Mou et al., 2018; Li et al., 2020), which adds Gaussian noise to the stochastic gradients of marginal
variance σ2
t . In parallel, there has been key developments on related information-theoretic general-
ization bounds applicable to SGLD type algorithms (Negrea et al., 2019; Haghifam et al., 2020; Xu
& Raginsky, 2017; Russo & Zou, 2016; Pensia et al., 2018)."
INTRODUCTION,0.019417475728155338,"While these developments have led to major advances in analyzing generalization of noisy SGD
algorithms, they each have certain limitations which leave room for further improvements. Using
uniform stability, Mou et al. (2018) established a bound for SGLD of the form K"
INTRODUCTION,0.024271844660194174,"n
pP
t η2
t /σ2
t which
depends on K, the global Lipschitz constant for the loss, and with step size ηt ≤σt ln 2/K. The
bound has a desirable dependency of O(1/n) on the samples, but has an undesirable dependence on
K, and the step sizes, bounded by σt/K, are too small. Mou et al. (2018) also presented another
bound which addresses some of these issues, but gets an undesirable O(1/√n) sample dependence.
By building on the developments of Russo & Zou (2016); Xu & Raginsky (2017); Pensia et al.
(2018), Negrea et al. (2019) made advances from the information theoretic perspective and estab-
lished bounds for SGLD which have the desirable dependence on the norm of gradient incoherence,
i.e., difference in gradients over different mini-batches, avoids dependence on Lipschitz constant
K, and is applicable to unbounded sub-Gaussian losses, but have an undesirable O(1/√n) sample"
INTRODUCTION,0.02912621359223301,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03398058252427184,"dependence. Haghifam et al. (2020) made further advances on the problem from an information the-
oretic perspective based on the conditional mutual information framework of Steinke & Zakynthinou
(2020) and obtained generalization bounds based on gradient incoherence with O(1/n) sample de-
pendence, but their analysis holds for full batch Langevin dynamics, not mini-batch SGLD. Li et al.
(2020) made advances on such bounds based on the notion of Bayes-stability, by combining ideas
from PAC-Bayes bounds into stability, and established a bound of the form c n
pP"
INTRODUCTION,0.038834951456310676,"t η2
t ge(t)/σ2
t for
bounded losses, where ge(t) is the expected gradient norm square at step t. While the bound avoids
dependency on the Lipschitz constant K, the dependence on the gradient norm makes such bounds
much weaker than the information theoretic bounds of Negrea et al. (2019); Haghifam et al. (2020)
which depend on the norm of gradient incoherence, which are typically orders of magnitude smaller.
Further, the analysis of Li et al. (2020) still needs small step sizes, bounded by σt/K."
INTRODUCTION,0.043689320388349516,"In this paper, we build on the core strengths of such existing approaches, most notably the O(1/n)
sample dependence of stability based bounds (Mou et al., 2018; Li et al., 2020) and the depen-
dence on gradient incoherence for information theoretic bounds (Negrea et al., 2019; Haghifam
et al., 2020), and develop a framework (Section 2) for developing generalization bounds for noisy
stochastic iterative (NSI) algorithms. Our framework considers generalization based on the concept
of expected stability, rather than uniform stability (Hardt et al., 2016; Bousquet & Elisseeff, 2002;
Bousquet et al., 2020; Mou et al., 2018), which yields distribution dependent generalization bounds
and avoids the worst-case setting of uniform stability. Building on Li et al. (2020), we show that
expected stability of general NSI algorithms can be bounded by the expected Le Cam Style Diver-
gence with dependence on parameter distributions from mini-batches differing by one sample. In
Section 3, we introduce Exponential Family Langevin Dynamics (EFLD), a family of noisy gradient
descent algorithms based on exponential family noise. Special cases of EFLD include SGLD and
noisy versions of Sign-SGD or quantized SGD algorithms widely used in practice (Bernstein et al.,
2018a;b; Jin et al., 2020; Alistarh et al., 2017)Our main result provides an expected stability based
generalization bound applicable to any EFLD algorithm with a O(1/n) sample dependence and a
dependence on gradient incoherence, rather than gradient norms. Existing generalization bounds for
SGLD (Li et al., 2020; Negrea et al., 2019) usually use properties of the Gaussian distribution, and
our analysis on EFLD illustrates that this was unnecessary. We also consider optimization guaran-
tees for EFLD and establish such results for noisy Sign-SGD and SGLD. Through experiments on
benchmark datasets (Section 4), we illustrate that our bounds are non-vacuous and quantitatively
much sharper than existing bounds (Li et al., 2020; Negrea et al., 2019)."
INTRODUCTION,0.04854368932038835,"Related work. Uniform stability has been a classical approach for bounding generalization error
(Bousquet & Elisseeff, 2002; Bousquet et al., 2020; Feldman & Vondrak, 2018; 2019), pioneered by
Rogers & Wagner (1978); Devroye & Wagner (1979). Beyond the aforementioned work, there has
been recent work on differential privacy that analyzes the uniform stability of differentially private
SGD (DP-SGD) (Hardt et al., 2016; Bassily et al., 2020). Beyond uniform stability, information-
theoretic approaches (Russo & Zou, 2016; Xu & Raginsky, 2017) that bounds the generalization
error by the mutual information between the algorithm input S and the algorithm output w, have
been used for deriving generalization bounds for noisy iterative algorithms (Pensia et al., 2018; Bu
et al., 2019). Along this line of literature, Negrea et al. (2019); Haghifam et al. (2020); Rodr´ıguez-
G´alvez et al. (2021) prove data-dependent generalization bounds dropping dependence on the Lips-
chitz constant. Further, tighter bounds (Haghifam et al., 2020; Zhou et al., 2021; Rodr´ıguez-G´alvez
et al., 2021; Neu, 2021; Hellstr¨om & Durisi, 2021) are proposed based on conditional mutual infor-
mation (Steinke & Zakynthinou, 2020; Gr¨unwald et al., 2021; Hellstr¨om & Durisi, 2020). Due to
space limitations, an extended discussion of the related work is deferred to Appendix A."
GENERALIZATION BOUNDS WITH EXPECTED STABILITY,0.05339805825242718,"2
GENERALIZATION BOUNDS WITH EXPECTED STABILITY"
GENERALIZATION BOUNDS WITH EXPECTED STABILITY,0.05825242718446602,"In the setting of statistical learning, there is an instance space Z, a hypothesis space W, and a loss
function ℓ: W ×Z 7→R+. Let D be an unknown distribution of Z and let S ∼Dn be n i.i.d. draws
from D. For any speciﬁc hypothesis w ∈W, the population and empirical loss are respectively
given by LD(w) ≜Ez∼D[ℓ(w, z)], and LS(w) ≜1"
GENERALIZATION BOUNDS WITH EXPECTED STABILITY,0.06310679611650485,"n
Pn
i=1 ℓ(w, zi). For any distribution P over
the hypothesis space, we respectively denote the expected population and empirical loss as
LD(P) ≜Ez∼DEw∼P [ℓ(w, z)] ,
and
LS(P) ≜1 n n
X"
GENERALIZATION BOUNDS WITH EXPECTED STABILITY,0.06796116504854369,"i=1
Ew∼P [ℓ(w, zi)] .
(1)"
GENERALIZATION BOUNDS WITH EXPECTED STABILITY,0.07281553398058252,"Consider a randomized algorithm A which works with S = {z1, . . . , zn} ∼Dn and cre-"
GENERALIZATION BOUNDS WITH EXPECTED STABILITY,0.07766990291262135,Under review as a conference paper at ICLR 2022
GENERALIZATION BOUNDS WITH EXPECTED STABILITY,0.0825242718446602,"ates a distribution over the hypothesis space W. For convenience, we will denote the distribu-
tion as A(S).
The focus of the analysis is to bound the generalization error of A deﬁned as:
gen(A(S)) ≜LD(A(S)) −LS(A(S)) . We will assume A is permutation invariant, i.e., the or-
dering of samples in S do not modify A(S), an assumption satisﬁed by most learning algorithms.
We will focus on developing bounds for the expectation ES[LD(A(S)) −LS(A(S))], and discuss
high-probability bounds in the Appendix B."
BOUNDS BASED ON EXPECTED STABILITY,0.08737864077669903,"2.1
BOUNDS BASED ON EXPECTED STABILITY"
BOUNDS BASED ON EXPECTED STABILITY,0.09223300970873786,"We start our analysis by noting that the expected generalization error can be upper bounded by
expected stability based on the Hellinger divergence (Sason & Verdu, 2016; Li et al., 2020):
H2(P∥P ′) = 1"
R,0.0970873786407767,"2
R w(
p"
R,0.10194174757281553,"p(w) −
p"
R,0.10679611650485436,p′(w))2dw.
R,0.11165048543689321,"Proposition 1. Let Sn ∼Dn and let S′
n be a dataset obtained by replacing zn ∈Sn with
z′
n ∼D. Let A(Sn), A(S′
n) respectively denote the distributions over the hypothesis space W
obtained by running randomized algorithm A on Sn, S′
n. Assume that for Sn ∼Dn, ∀z ∈Z,
EW ∼A(Sn)[ℓ2(W, z)] ≤c0/2, c0 > 0. With H(·, ·) denoting the Hellinger divergence, we have"
R,0.11650485436893204,"|ESn∼Dn[LD(A(Sn)) −LS(A(Sn))]| ≤c0ESn∼DnEz′n∼D
q"
R,0.12135922330097088,"2H2 
A(Sn), A(S′n)

.
(2)"
R,0.1262135922330097,"Remark 2.1. Proposition 1 does not need bounded losses. Just the second moment of ℓ(W, z), W ∼
A(Sn), Sn ∼Dn, ∀z ∈Z need to be bounded. The assumption is satisﬁed by bounded losses. It is
instructive to compare the assumption to that in recent information theoretic bounds (Haghifam et al.,
2020; Xu & Raginsky, 2017), where one assumes ℓ(w, Z), Z ∼D, ∀w ∈W to be sub-Gaussian.
Remark 2.2. The bound in Proposition 1 is in terms of expected stability where we consider
ES∼DnEz′n∼D[· · · ], an important departure from bounds based on uniform stability (Elisseeff et al.,
2005; Bousquet & Elisseeff, 2002; Mou et al., 2018; Bousquet et al., 2020; Feldman & Vondrak,
2018; 2019) where one considers supS,S′∈Zn,|S\S′|=1[· · · ]. Replacing sup by E makes the bounds
distribution dependent, and arguably leads to quantitatively tighter bounds."
R,0.13106796116504854,Note that the Hellinger divergence can be bounded by the KL divergence.
R,0.13592233009708737,"Proposition 2. For any distributions P and P ′, 2H2(P, P ′) ≤min

KL(P, P ′),
q"
R,0.1407766990291262,"1
2KL(P, P ′)
	
."
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.14563106796116504,"2.2
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.15048543689320387,"We consider a general family of noisy stochastic iterative (NSI) algorithms. Given S ∼Dn, such
iterative algorithms have two additional sources of randomness in each iteration t: (a) a stochastic
mini-batch of samples SBt, with |SBt| = b, drawn uniformly at random with replacement from
S; and (b) noise ξt suitably included in the iterative update. Given a trajectory of past iterates
W0:(t−1) = w0:(t−1), the new iterate Wt is drawn from a distribution PBt,ξt|w0:(t−1) over W:"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.1553398058252427,"Wt ∼PBt,ξt|w0:(t−1)(W) .
(3)"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.16019417475728157,"We will drop the conditioning w0:(t−1) to avoid clutter in the sequel. Let PT , P ′
T denote the marginal
distributions over hypotheses w ∈W after T steps of the algorithm based on Sn, S′
n respec-
tively. Further, let P0:(t−1) denote the joint distribution over W0:(t−1) = (W0, . . . , Wt−1), and
let Pt| ≡PBt,ξt|w0:(t−1) compactly denote the conditional distribution on Wt conditioned on the
trajectory W0:(t−1) = w0:t−1. Following (Negrea et al., 2019; Haghifam et al., 2020; Pensia et al.,
2018), we use the following chain rule for KL-divergence: KL(PT ∥P ′
T ) ≤KL(P0:T ∥P ′
0:T ) =
PT
t=1 EP0:(t−1)[KL(Pt|∥P ′
t|)]. Let ¯S ∼Dn+1, and let Sn, S′
n be size n subsets of ¯S such that
Sn = {Z1, . . . , Zn−1, Zn} and S′
n = {Z1, . . . , Zn−1, Z′
n}, where Z′
n = Zn+1.
Let S0 =
{Z1, . . . , Zn−1}. The algorithms we consider use a mini-batch of size b in each iteration uniformly
sampled from n samples. Let the set of all mini-batch index sets be denoted by G. Let the set of all
mini-batch index sets A drawn from S0 be denoted by G0. Note that |G0| =
 n−1
b

. Let G1 denote
the set of all mini-batch index sets B which includes the last sample, viz. zn for S with mini-batches
and z′
n for S′
n. Note that |G1| =
 n−1
b−1

. Also note that |G0| + |G1| =
 n−1
b

+
 n−1
b−1

=
 n
b

= |G|."
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.1650485436893204,"Following Li et al. (2020), we can bound their conditional KL-divergences KL(Pt|∥P ′
t|) in terms
of a Le Cam Style Divergence (LSD). While the classical Le Cam divergence (Sason & Verdu,"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.16990291262135923,Under review as a conference paper at ICLR 2022
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.17475728155339806,2016) is LCD(P∥P ′) ≜1
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.1796116504854369,"2
R (dP −dP ′)2"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.18446601941747573,"dP +dP ′
(where dP denotes the density), our bounds in terms of"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.18932038834951456,"LSD(Pt||∥P ′
t|) ≜
R (dPBt,ξt−dP ′
Bt,ξt)2"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.1941747572815534,"dPAt,ξt
, Bt ∈G1, At ∈G0. Note that PBt,ξt and P ′
Bt,ξt represent
the distribution of Wt for Sn and S′
n respectively since the mini-batch SBt of Sn and S′
n differs in
the n-th sample. Putting everything together, we have the following LSD based bound.
Lemma 1. Consider a noisy stochastic iterative algorithms of the form (3) with mini-batch size
b ≤n/2. Then, with c1 =
√"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.19902912621359223,"2c0 (with c0 as in Proposition 1), we have"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.20388349514563106,"|ESn[LD(A(Sn))−LSn(A(Sn))]| ≤c1
b
nESnEz′n"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.2087378640776699,"v
u
u
u
u
t T
X"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.21359223300970873,"t=1
E
W0:(t−1)
E
Bt∈G1
E
At∈G0  
Z ξt"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.21844660194174756,"
dPBt,ξt −dP ′
Bt,ξt 2"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.22330097087378642,"dPAt,ξt
dξt  ."
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.22815533980582525,"(4)
Remark 2.3. Li et al. (2020) essentially has this result for SGLD and inspired our work. Our proofs
are signiﬁcantly simpler and, more importantly, illustrates applicability to general noisy iterative
algorithms of the form (3), not just SGLD with Gaussian noise as in Li et al. (2020).
Remark 2.4. Note that the bound does not assume the loss to be bounded, depends on expectations
over samples Sn, z′
n, trajectories w0:(t−1), and mini-batches Bt, At. Further, the bound depends on
the distribution discrepancy as captured by the expected LSD.
Remark 2.5. The bound seems to worsen with b, the size of the mini-batch. As we shown in Sec-
tion 3, the expected LSD term has a 1"
EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE ALGORITHMS,0.23300970873786409,"b2 dependence for the Exponential Family Langevin dynamics
(EFLD) models we introduce, so the leading b is neutralized."
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.23786407766990292,"3
EXPONENTIAL FAMILY LANGEVIN DYNAMICS"
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.24271844660194175,"In recent years, considerable advances have been made in establishing generalization bounds for
stochastic gradient Langevin dynamics (SGLD) (Li et al., 2020; Pensia et al., 2018; Negrea et al.,
2019; Haghifam et al., 2020). As an example of NSI algorithms of the form (3), SGLD adds an
isotropic Gaussian noise at every step of SGD: wt+1 = wt −ηt∇ℓ(wt, SBt) + N
 
0, σ2
t Id

, where
∇ℓ(wt, SBt) is the stochastic gradient on mini-batch Bt, ηt is the step size, and σ2
t is noise vairance."
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.24757281553398058,"In this paper, we introduce a substantial generalization of SGLD called Exponential Family
Langevin Dynamics (EFLD) which uses general exponential family noise in noisy iterative updates
of the form (3). In addition to being a mathematical generalization of the popular SGLD, the pro-
posed EFLD provides ﬂexibility to use noise gradient algorithms with different representation of the
gradient, e.g., Bernoulli noise for Sign-SGD, discrete distribution for quantized or ﬁnite precision
SGD, etc. (Canonne et al., 2020; Alistarh et al., 2017; Jiang & Agrawal, 2018; Yang et al., 2019)."
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.2524271844660194,"3.1
EXPONENTIAL FAMILY LANGEVIN DYNAMICS (EFLD)
Exponential families (Barndorff-Nielsen, 2014; Brown, 1986; Wainwright & Jordan, 2008) consti-
tute a large family of parametric distributions which include Gaussian, Bernoulli, gamma, Pois-
son, Dirichlet, etc., as special cases. Exponential families are typically represented in terms of
natural parameters θ, and we consider component-wise independent distributions with scaled nat-
ural parameter θα = θ/α with scaling α > 0, i.e., pψ(ξ, θα) = exp(⟨ξ, θα⟩−ψ(θα))π0(ξ) =
Qp
j=1 exp(ξjθjα −ψj(θjα))π0(ξj) , where ξ is the sufﬁcient statistic, ψ(θα) = Pp
j=1 ψj(θjα) is
the log-partition function, and π0(ξ) = Qp
j=1 π0(ξj) is the base measure. Note that α = 1 gives
the canonical form of the exponential family distributions. For general scaling α > 0, for some
cases the base measure π0 may depend on the scaling, i.e., π0,α. A scaling α > 0 is valid as long
as exp(⟨ξ, θα⟩is integrable, i.e.,
R"
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.25728155339805825,"ξ exp(⟨ξ, θα⟩π0(ξ)dξ < ∞. Further, ψ is a smooth function by
construction (Barndorff-Nielsen, 2014; Banerjee et al., 2005; Wainwright & Jordan, 2008) and the
smoothness of ψ implies ∇2ψ(θα) ≤c2I."
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.2621359223300971,"Exponential family Langevin dynamics (EFLD) uses noisy stochastic gradient updates similar to
SGLD, but using exponential family noise rather than Gaussian noise as in SGLD. In particular, for
mini-batch SBt, EFLD updates are as follows: with step size ρt > 0"
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.2669902912621359,"wt = wt−1 −ρtξt ,
ξt ∼pψ(ξ; θBt,αt) ,
(5)
where
pψ(ξ; θBt,αt) = exp(⟨ξ, θBt,αt⟩−ψ(θBt,αt))π0(ξ) ,
θBt,αt ≜θBt"
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.27184466019417475,"αt
= ∇ℓ(wt−1, SBt)"
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.2766990291262136,"αt
. (6)"
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.2815533980582524,Under review as a conference paper at ICLR 2022
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.28640776699029125,"For EFLD, the natural parameter θBt,αt at step t is simply a scaled version of the mini-batch gradient
∇ℓ(wt−1, SBt). We ﬁrst show that EFLD becomes SGLD when the exponential family is Gaussian,
and becomes a noisy version of sign-SGD (Bernstein et al., 2018a;b) when the exponential family is
Bernoulli over {−1, +1}. More details and examples are in Appendix C.1.
Example 3.1 (SGLD). SGLD uses scaled Gaussian noise with ψ(θ) = ∥θ∥2
2/2, αt =
p"
EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.2912621359223301,"σt/ηt,
so that pψ(ξ; θBt,αt) = N(θBt, α2
tId). By taking ρt = √ηtσt, the update (5) based on ρtξt is
distributed as N(ρtθBt, ρ2
tα2
tId) = N(ηt∇ℓ(wt−1, SBt), σ2
t Id). Thus the EFLD update reduces to
the SGLD update: wt = wt−1 −ηt∇ℓ(wt−1, SBt) + N
 
0, σ2
t Id

.
Example 3.2 (Noisy Sign-SGD). By taking ρt = ηt and component-wise ξj ∈{−1, 1}, π0(ξj) =
1, ψ(θ) = log(exp(−θ)+exp(θ)) in exponential family update equation (5), the j-th component of
exponential family distribution pψ(ξ; θBt,αt) becomes pθBt,αt,j(ξj) =
exp(ξjθBt,αt,j)
exp(−θBt,αt,j)+exp(θBt,αt,j).
Thus, the EFLD update reduces to a noisy version of Sign-SGD: wt = wt−1 −ηtξt, ξt,j ∼
pθBt,αt,j(ξj), j ∈[d], where θBt,αt = ∇ℓ(wt−1, SBt)/αt is the scaled mini-batch gradient."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.2961165048543689,"3.2
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.30097087378640774,"From Lemma 1, conditioned on a trajectory w0:(t−1), mini-batches SBt, SAt, we can get gener-
alization bound by suitably bounding the Le Cam Style Divergence (LSD) given by: IAt,Bt =
R"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3058252427184466,"ξt
(dPBt,ξt−dP ′
Bt,ξt)
2"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3106796116504854,"dPAt,ξt
dξt. For EFLD, the density functions dPBt,ξt are exponential family densi-
ties pψ(ξ; θBt,αt) as in (5)-(6), and we have the following bound on the per step LSD:
Theorem 1. For a given set ¯S ∼Dn+1 and wt−1 at iteration (t −1), let ∆t|wt−1( ¯S) =
maxz,z′∈¯S ∥∇ℓ(wt−1, z) −∇ℓ(wt−1, z′)∥2 . Further, for a c2-smooth log-partition function ψ,
let the scaling αt|wt−1 be data-dependent such that α2
t|wt−1 ≥8c2∆2
t|wt−1(Sn+1). Then, we have"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3155339805825243,"IAt,Bt ≤5c2∥θBt,αt −θB′
t,αt∥2
2 =
5c2
2α2
t|wt−1"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.32038834951456313,"h∇ℓ(wt−1, SBt) −∇ℓ
 
wt−1, S′
Bt
2
2"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.32524271844660196,"i
,
(7)"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3300970873786408,"Note that SBt and S′
Bt only differ at samples zn and z′
n. The above bound can now be directly
applied to Lemma 1 to get expected stability based generalization bounds for any EFLD algorithm.
Theorem 2. Consider an exponential family Langevin dynamics (EFLD) algorithm of the form (5)-
(6) with a c2-smooth log-partition function ψ. Then, for mini-batch size b ≤n/2, with c = c0
√5c2
(with c0 as in Lemma 1) and α2
t| ≥8c2∆2
t|(Sn+1) (as in Theorem 1, with the conditioning on wt−1
hidden to avoid clutter), we have"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.33495145631067963,|ES[LD(A(S)) −LS(A(S))]| ≤c 1
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.33980582524271846,"n
E
Sn+1"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3446601941747573,"v
u
u
t T
X"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.34951456310679613,"t=1
E
W0:(t−1)
1
α2
t|"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.35436893203883496,"h
∥∇ℓ(wt−1, zn) −∇ℓ(wt−1, z′n)∥2
2
i
."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3592233009708738,"(8)
Remark 3.1. Theorem 2 captures the generalization error of SGLD, which is a special case of
EFLD. Our bound has the same dependence on n, T, step size ηt as the bound in Li et al.
(2020). However, our bound is numerically sharper because we replace the gradient norms, i.e.,
1
n
P"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3640776699029126,"z∈S ∥ℓ(wt, z)∥in Li et al. (2020) and with gradient discrepancy ∥∇ℓ(wt, z) −∇ℓ(wt, z′)∥,
which is quantitatively smaller than gradient norms as we show in the experiment section. The
bound in Negrea et al. (2019) depends on gradient incoherence which is empirically smaller than
gradient discrepancy as observed in the experiment section, their bound depends on 1/√n, which is
worse than the 1/n dependence in our bound.
Remark 3.2. EFLD can be extended to work with anisotropic noise by using θBt,αt
=
∇ℓ(wt−1, SBt) ⊘αt in (6) where αt ∈Rp determines different scaling for each dimension and
⊘denotes Hadamard division. Theorems 1 and 2 can be extended to such anisotropic noise by using
α-scaled norms for the gradient discrepancy, i.e., ∥g −g′∥2
2,α = P
j(gj −g′
j)2/α2
j.
Remark 3.3. The condition on αt is a data-dependent quantity, which can be computed along the
training process. It gives much more benign condition of the step size compared to those in the
related work (Mou et al., 2018; Li et al., 2020, Hardt et al. 2016), which require step size being
bounded by σt/L. However, the step sizes in Theorem 2 need to be bounded by σt/∆t( ¯S), which
is considerably more relaxed since ∆t( ¯S) is much smaller than Lipschitz constant L, which is a
uniform bound over the whole parameter space. Also, usually one would expect ∆t( ¯S) to decrease
as training proceeds since the gradients shrink as the loss function being minimized. Thus, the
constraint on step size does not require the step sizes to be as small as σt/L."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.36893203883495146,Under review as a conference paper at ICLR 2022
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3737864077669903,"3.3
PROOF SKETCHES OF MAIN RESULTS: THEOREMS 1 AND 2"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3786407766990291,"We focus on Theorem 1. To avoid clutter, we drop the subscript t for the analysis and note that
the analysis holds for any step t. When the density dPB,ξ = pψ(ξ; θB,α), by mean-value theorem,
for each ξ, we have pψ(ξ; θB,α) −pψ(ξ; θB′,α) = ⟨θB,α −θB,α, ∇˜θB,αpψ(ξ; ˜θB,α)⟩, for some
˜θB,α = γξθB,α + (1 −γξ)θ′
B,α where γξ ∈[0, 1]. Then,"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.38349514563106796,"IA,B =
Z ξ"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3883495145631068," 
pψ(ξ; θB,α) −pψ(ξ; θB′,α)
2"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.3932038834951456,"pψ(ξ; θA,α)
dξ =
Z ξ"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.39805825242718446,"⟨θB,α −θ′
B,α, ξ −∇˜θB,αψ(ξ; ˜θB,α)⟩2 p2
ψ(ξ; ˜θB,α)"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.4029126213592233,"pψ(ξ; θA,α)
dξ ,"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.4077669902912621,"since pψ(ξ; ˜θB,α) = exp(⟨ξ, ˜θB,α⟩−ψ(˜θB,α))π0(ξ)."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.41262135922330095,"Handling Distributional Dependence of ˜θB. Note that we cannot proceed with the analysis with
the density term depending on ˜θB,α since ˜θB,α depends on ξ. So, we ﬁrst bound the density term
depending on ˜θB,α in terms of exponential family densities with parameters θB,α and θB,α using
c2-smoothness of ψ."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.4174757281553398,"Lemma 2. For some γξ ∈[0, 1], ˜θB,α = γξθB,α + (1 −γξ)θ′
B,α, we have"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.4223300970873786,"exp
h
⟨ξ, ˜θB,α⟩−ψ(˜θB,α)
i"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.42718446601941745,"max
 
exp

⟨ξ, θB,α⟩−ψ(θB,α)

, exp [⟨ξ, θB′,α⟩−ψ(θB′,α)]
 ≤exp

c2∥θB,α −θB′,α∥2
2

."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.4320388349514563,"Bounding the Density Ratio. Next we focus on the density ratio p2
ψ(ξ, ˜θB,α)/pψ(ξ; θA,α). By
Lemma 2, it sufﬁces to focus on p2
ψ(ξ, θB,α)/pψ(ξ; θA,α) or the equivalent term for θB′,α. We show
that the density ratio can be bounded by another exponential family with parameters (2θB,α−θA,α)."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.4368932038834951,"Lemma 3. For any ξ, we have"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.441747572815534,"exp [⟨ξ, 2θB,α⟩−2ψ(θB,α)]"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.44660194174757284,"exp [⟨ξ, θA,α⟩−ψ(θA,α)]
≤exp

2c2∥θB,α −θA,α∥2
2

exp [⟨ξ, (2θB,α −θA,α⟩−ψ(2θB,α −θA,α)] ."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.45145631067961167,"The analysis for the term p2
ψ(ξ, θB′,α)/pψ(ξ; θA,α) is exactly the same."
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.4563106796116505,"Bounding the Integral.
Ignoring multiplicative terms which do not depend on ξ for the
moment, the analysis needs to bound an integral term of the form
R"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.46116504854368934,"ξ⟨θB,α −θ′
B,α, ξ −"
EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS,0.46601941747572817,"∇ψ(ξ; ˜θB,α)⟩2 pψ(ξ; 2θB,α −θA,α)dξ, and a similar term with p2
ψ(ξ; 2θB′,α −θA,α).
First,
note that ∇ψ(ξ; ˜θB,α) = ˜µB,α, the expectation parameter for pψ(ξ; ˜θB,α) Wainwright & Jordan
(2008); Banerjee et al. (2005). The integral, however, is with respect to pψ(ξ; 2θB,α −θA,α).
We handle this discrepancy by using ξ −∇ψ(ξ; ˜θB,α) = (ξ −E[ξ]) + (E[ξ] −∇ψ(ξ; ˜θB,α)),
and decomposing as sum-of-squares.
Quadratic form for the ﬁrst term yields the covariance
E[(ξ −E[ξ])(ξ −E[ξ])T ] = ∇2ψ(θ2θB,α−θA,α) ≤c2I, by smoothness. The second term de-
pends on the difference of gradients ∇ψ(2θB,α −θA,α) −∇ψ(˜θB,α) which, using smoothness and
additional analysis, can be bounded by the norm of (θB,α−θA,α). All the pieces can be put together
to get the bound in Theorem 1, which when used in Lemma 1 yields Theorem 2."
OPTIMIZATION GUARANTEES FOR EFLD,0.470873786407767,"3.4
OPTIMIZATION GUARANTEES FOR EFLD
We now establish optimization guarantees for two examples of EFLD, i.e., Noisy Sign-SGD with
Bernoulli noise over {−1, +1} and SGLD with Gaussian noise."
OPTIMIZATION GUARANTEES FOR EFLD,0.47572815533980584,"Noisy Sign-SGD. For mini-batch Bt and scaling αt, mini-batch Noisy Sign-SGD updates the pa-
rameters as wt = wt−1 −ηtξt, where each component j ∈[d]"
OPTIMIZATION GUARANTEES FOR EFLD,0.48058252427184467,"ξt,j ∼pθBt,αt,j(x) =
exp(xθBt,αt,j)
exp(−θBt,αt,j) + exp(θBt,αt,j), x ∈{−1, +1}
(9)"
OPTIMIZATION GUARANTEES FOR EFLD,0.4854368932038835,"where θBt,αt = ∇ℓ(wt−1, SBt)/αt is the scaled mini-batch gradient. The full-batch version uses
parameters EBt[θBt,αt] = ∇LS(wt−1) For the optimization analysis, we assume that the loss is
smooth and mini-batch gradients are unbiased, symmetric, and sub-Gaussian."
OPTIMIZATION GUARANTEES FOR EFLD,0.49029126213592233,Under review as a conference paper at ICLR 2022
OPTIMIZATION GUARANTEES FOR EFLD,0.49514563106796117,"Assumption 1. The loss function LS satisﬁes: for all w and w′, for some non-negative constant
⃗K := [K1, . . . , Kd], we have LS(w) ≤LS(w′) + ∇LS(w′)T (w −w′) + 1"
P,0.5,"2
P"
P,0.5048543689320388,"i Ki(wi −w′
i)2."
P,0.5097087378640777,"Assumption 2. Given wt−1, the mini-batch gradient ∇ℓ(wt−1, SBt) is (a) unbiased, i.e.,
EBt|wt−1∇ℓ(wt−1, SBt)
=
∇LS(wt−1);
(b) symmetric, i.e., the density p(x) of x
≡
∇ℓ(wt−1, SBt) is symmetric around its expectation LS(wt−1): p(x) = p(2∇LS(wt−1) −x) and
(c) sub-Gaussian, i.e., for any λ > 0, any v s.t. ∥v∥2 = 1, EBt|wt−1 exp λ⟨v, ∇ℓ(wt−1, SBt) −
∇LS(wt−1)⟩≤exp(λ2κ2
t/2) for some constant κt > 0.
Based on the assumptions, we have the following optimization guarantee for mini-batch noisy Sign-
SGD. We defer the optimization guarantee for full-batch noisy Sign-SGD to Appendix D."
P,0.5145631067961165,"Theorem 3. Under Assumption 1 and 2, for mini-batch noisy Sign-SGD with step size ηt = 1/
√"
P,0.5194174757281553,"T,
αt satisfying c ≥αt ≥max[
√"
P,0.5242718446601942,"2κt, 4∥∇LS(wt−1)∥∞], we have for any S and any initialization
w0
E ""
1
T T
X"
P,0.529126213592233,"t=1
∥∇LS(wt)∥2
2 # ≤4c
√ T"
P,0.5339805825242718,"
LS(w0) −LS(w∗) + 1"
P,0.5388349514563107,2∥⃗K∥1
P,0.5436893203883495,"
,
(10)"
P,0.5485436893203883,"where the expectation is taken over the randomness of algorithm.
SGLD. We acknowledge that the following optimization result of SGLD exists in various forms, as
noisy gradient descent algorithms have been studied in literature such as differential privacy, where
SGLD can be viewed as DP-SGD (Bassily et al., 2014; Wang & Xu, 2019) and the proof technique
boils down to bounding the stochastic variance of the noisy gradient (Shamir & Zhang, 2013).
Theorem 4. Under Assumption 1 and 2, with Ki = K, ∀i ∈[d], for any S, SGLD (EFLD with step
size ρt = √ηtσt, αt =
p"
P,0.5533980582524272,"σt/ηt), |Bt| = b, and ηt =
1
√"
P,0.558252427184466,"T , can achieve"
T,0.5631067961165048,"1
T T
X"
T,0.5679611650485437,"t=1
E∥∇LS(wt)∥2 ≤O
 1
√ T 
+ O "
T,0.5728155339805825,"K p PT
t=1 α4
t + log T
√ T !"
T,0.5776699029126213,",
(11)"
T,0.5825242718446602,"where the expectation is over the randomness of the algorithm.
The error rate of SGLD depends on the noise variance αt. One can choose a decaying noise variance
such as αt = 1/
4√"
T,0.587378640776699,"t to guarantee the convergence. Then the rate will become O(log T/
√"
T,0.5922330097087378,"T). We
note that similar to the optimization guarantees of DP-SGD, the convergence rate depends on the
dimension of the gradient p due to the isotropic Gaussian noise. Special noise structures such as
anisotropic noise that aligned with the gradient structure can reduce the dependence on dimension
(Kairouz et al., 2020; Zhang et al., 2021; Asi et al., 2021; Zhou et al., 2020)."
EXPERIMENTS,0.5970873786407767,"4
EXPERIMENTS"
EXPERIMENTS,0.6019417475728155,"In this section, we conduct a series of experiments to evaluate our generalization error bounds. For
SGLD, we aim to compare the proposed bound in Theorem 2 with existing bounds in Li et al. (2020),
Negrea et al. (2019), and Rodr´ıguez-G´alvez et al. (2021) for various datasets. Note that the bound
presented in Rodr´ıguez-G´alvez et al. (2021) is an extension of that in Haghifam et al. (2020) from
full-batch setting to mini-batch setting . We also evaluate the optimization performance of proposed
Noisy Sign-SGD by comparing it with the original sign-SGD (Bernstein et al., 2018a) and present
the corresponding generalization bound in Theorem 2."
EXPERIMENTS,0.6067961165048543,"The details of our model architectures, learning rate scheduling, hyper-parameter selections and
additional experimental results can be found in Appendix E. We acknowledge that we did not achieve
the state-of-the art predictive performance, mainly due to the simplicity of our model architectures.
With more complex model and further tuning, the prediction results could be improved."
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6116504854368932,"4.1
STOCHASTIC GRADIENT LANGEVIN DYNAMICS
Comparison with existing work. We have derived theoretical generalization error bounds that
depend on the data-dependent quantity gradient discrepancy, i.e., ∥∇ℓ(wt, zn) −∇ℓ(wt, z′
n)∥2
2.
Existing bounds in Li et al. (2020) and Negrea et al. (2019) have also improved the Lipschitz constant
in Mou et al. (2018) to a data-dependent quantity. As shown in Figure 1 (a)-(d), by combining with
the empirical training error, all four generalization error bounds can be used to bound the empirical
test error, but our bound is able to generate a much tighter upper bound. Such difference is mainly
due to the fact that we replace the squared gradient norm in Li et al. (2020), the squared norm of"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.616504854368932,Under review as a conference paper at ICLR 2022
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6213592233009708,"(a) MNIST, α2
t ≈0.1
(b) CIFAR-10, α2
t ≈0.1
(c) Fashion, α2
t ≈0.1
(d) Fashion, α2
t ≈0.01"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6262135922330098,"(e) MNIST, α2
t ≈0.1
(f) CIFAR-10, α2
t ≈0.1
(g) Fashion, α2
t ≈0.1
(h) Fashion, α2
t ≈0.01"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6310679611650486,"(i) MNIST, α2
t ≈0.1
(j) CIFAR-10, α2
t ≈0.1
(k) Fashion, α2
t ≈0.1
(l) Fashion, α2
t ≈0.01"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6359223300970874,"Figure 1: Numerical results for training CNN using SGLD (σt =
p"
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6407766990291263,"2ηt/βt) on MNIST, Fashion-
MNIST and CIFAR-10. X-axis shows the number of training epochs. (a)-(d) shows our bound is
non-vacuous and can be used to bound the empirical test error. (e)-(h) compare our bound with
the existing bounds and show the effect on α2
t. (i)-(l) show the key factors in each bound, i.e.,
the squared gradient norm in Li et al. (2020), the gradient incoherence in Negrea et al. (2019),
the two-sample incoherence in Rodr´ıguez-G´alvez et al. (2021), and the gradient discrepancy in our
bound. Our bounds are numerically sharper than existing bounds, and larger α2
t leads to tighter
generalization bounds which is consistent with the theoretical analysis."
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6456310679611651,"gradient incoherence in Negrea et al. (2019), and that of two-sample incoherence in Rodr´ıguez-
G´alvez et al. (2021) with the gradient discrepancy. Results in Figure 1 (e)-(h) show that our bounds
are much sharper than those of Li et al. (2020) because our gradient discrepancy (Figure 1 (i)-(l))
is usually 2-4 order of magnitude smaller than the squared gradient norms appeared in Li et al.
(2020). Our bounds are also sharper than those of Negrea et al. (2019) and Rodr´ıguez-G´alvez et al.
(2021) due to an improved dependence on n from an order of 1/√n to 1/n. Note that, even though
the gradient incoherence in Negrea et al. (2019) is about 1 to 2 order of magnitude smaller than
the gradient discrepancy for simple problems such as MNIST and Fashion-MNIST, the difference
between the gradient incoherence and our gradient discrepancy reduces as the problem becomes
harder (see results for CIFAR-10 in Figure 1(j))."
STOCHASTIC GRADIENT LANGEVIN DYNAMICS,0.6504854368932039,"Effect of Randomness. Motivated by Zhang et al. (2017), we train CNN with SGLD on a smaller
subset of MNIST dataset (n = 10000) with randomly corrupted labels. The corruption fraction
varies from 0% (without label corruption) to 60%. As shown in Figure 2 (d), for long enough training
time, all experiments with different level of label randomness can achieve almost zero training error.
However, the one with higher level of randomness has higher generalization/test error (Figure 2 (a)
dashed lines). Our generalization bound also becomes larger as the randomness increases since the
corresponding gradient discrepancy increases."
NOISY SIGN-SGD,0.6553398058252428,"4.2
NOISY SIGN-SGD"
NOISY SIGN-SGD,0.6601941747572816,"Optimization. Figure 3 (a)-(d) show the training dynamics of Noisy Sign-SGD under various se-
lections of αt. As αt →0, Noisy Sign-SGD matches both the optimization trajectory as well as the
ﬁnal test accuracy of the original Sign-SGD (Bernstein et al., 2018a). However, as αt increases, the
probability of getting 1 approaches 0.5, and ξt approximates a uniform distribution. As a result, the
corresponding Noisy Sign-SGD still converges, but the generalization performance is much worse."
NOISY SIGN-SGD,0.6650485436893204,Under review as a conference paper at ICLR 2022
NOISY SIGN-SGD,0.6699029126213593,"(a) Bounded Test Error
(b) Our Bound
(c) Gradient Discrepancy
(d) Training Error"
NOISY SIGN-SGD,0.6747572815533981,"Figure 2: Numerical results for training CNN using SGLD (σt = 0.2ηt) on a subset of MNIST (n =
10000) with different randomness on labels. (a) demonstrates that, as the randomness increases, the
empirical test error (dashed lines) increases but still can be bounded by our generalization bound by
combining the empirical training error (solid lines). (b) presents our bound in Theorem 2. (c) shows
the gradient discrepancy ∥∇ℓ(wt, zn) −∇ℓ(wt, z′
n)∥2
2. (d) plots the training error. The gradient
discrepancy increases as randomness increases, so does our generalization bound."
NOISY SIGN-SGD,0.6796116504854369,"(a) CNN, MNIST
(b) CNN, Fashion
(c) ResNet-18, CIFAR10 (d) ResNet-18, CIFAR100"
NOISY SIGN-SGD,0.6844660194174758,"(e) MNIST, αt = 1
(f) MNIST, αt = 0.01
(g) Fashion, αt = 1
(h) Fashion, αt = 0.01
Figure 3: (a)-(d) show the training dynamics of CNN on MNIST and Fashion-MNIST, and ResNet-
18 on CIFAR-10 and CIFAR-100 using noisy sign-SGD with different scaling αt. Legends indicate
the choice of αt and the numbers in brackets are test errors at convergence. As αt →0, Nosiy
sign-SGD matches both the optimization trajectory as well as the ﬁnal test accuracy of the original
sign-SGD (Bernstein et al., 2018a). (e)-(f) show that empirical test error can be bounded by our
bound and the corresponding training error. The larger αt is the sharper our bound is."
NOISY SIGN-SGD,0.6893203883495146,"Generalization Bound. Figure 3(e)-(f) show that our bound successfully bounds the empirical test
error. The larger αt is the sharper the upper bound is. However, larger αt would slow down and
adversely affect the optimization, e.g., Figure 3 (a)-(d) blue and orange lines. In practice, one needs
to balance the optimization error and generalization by choosing a suitable scaling αt."
CONCLUSIONS,0.6941747572815534,"5
CONCLUSIONS"
CONCLUSIONS,0.6990291262135923,"Inspired by recent advances in stability based and information theoretic approaches to generalization
bounds (Mou et al., 2018; Pensia et al., 2018; Negrea et al., 2019; Li et al., 2020; Haghifam et al.,
2020), we have presented a framework for developing such bounds based on expected stability for
noisy stochastic iterative (NSI) learning algorithms. We have also introduced Exponential Family
Langevin Dynamics (EFLD), a family of noisy gradient descent algorithms based on exponential
family noise, including SGLD and Noisy Sign-SGD as two special cases. We have developed an
expected stability based generalization bound applicable to any EFLD algorithm with a O(1/n)
sample dependence and a dependence on gradient incoherence, rather than gradient norms. Further,
we have provided optimization guarantees for EFLD and establish such results for Noisy Sign-SGD
and SGLD. Our experiments on various benchmarks illustrate that our bounds are non-vacuous and
quantitatively much sharper than existing bounds (Li et al., 2020; Negrea et al., 2019)."
CONCLUSIONS,0.7038834951456311,Under review as a conference paper at ICLR 2022
REFERENCES,0.7087378640776699,REFERENCES
REFERENCES,0.7135922330097088,"Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
Qsgd:
Communication-efﬁcient sgd via gradient quantization and encoding. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 1709–1720. Curran Associates, Inc., 2017."
REFERENCES,0.7184466019417476,"Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient
methods for convex optimization. In International Conference on Machine Learning, pp. 383–
392. PMLR, 2021."
REFERENCES,0.7233009708737864,"Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with breg-
man divergences. Journal of machine learning research, 6(10), 2005."
REFERENCES,0.7281553398058253,"Ole Barndorff-Nielsen. Information and exponential families: in statistical theory. John Wiley &
Sons, 2014."
REFERENCES,0.7330097087378641,"Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efﬁcient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464–473. IEEE, 2014."
REFERENCES,0.7378640776699029,"Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates.
Advances in neural information processing systems,
2019."
REFERENCES,0.7427184466019418,"Raef Bassily, Vitaly Feldman, Crist´obal Guzm´an, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33,
2020."
REFERENCES,0.7475728155339806,"Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
signsgd: Compressed optimisation for non-convex problems. In International Conference on
Machine Learning, pp. 560–569. PMLR, 2018a."
REFERENCES,0.7524271844660194,"Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with
majority vote is communication efﬁcient and fault tolerant. In International Conference on Learn-
ing Representations, 2018b."
REFERENCES,0.7572815533980582,"St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-
totic theory of independence. Oxford university press, 2013."
REFERENCES,0.7621359223300971,"Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of Machine Learning
Research, 2:499–526, 2002."
REFERENCES,0.7669902912621359,"Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610–626. PMLR, 2020."
REFERENCES,0.7718446601941747,"Lawrence D Brown. Fundamentals of statistical exponential families: with applications in statistical
decision theory. Ims, 1986."
REFERENCES,0.7766990291262136,"Yuheng Bu, Shaofeng Zou, and Venugopal V Veeravalli.
Tightening mutual information based
bounds on generalization error. In 2019 IEEE International Symposium on Information Theory
(ISIT), pp. 587–591. IEEE, 2019."
REFERENCES,0.7815533980582524,"Mark Bun, Cynthia Dwork, Guy N Rothblum, and Thomas Steinke. Composable and versatile
privacy via truncated cdp. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
of Computing, pp. 74–86, 2018."
REFERENCES,0.7864077669902912,"Cl´ement L Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. In NeurIPS, 2020."
REFERENCES,0.7912621359223301,"Xiangyi Chen, Tiancong Chen, Haoran Sun, Zhiwei Steven Wu, and Mingyi Hong. Distributed
training with heterogeneous data: Bridging median-and mean-based algorithms. arXiv preprint
arXiv:1906.01736, 2019."
REFERENCES,0.7961165048543689,Under review as a conference paper at ICLR 2022
REFERENCES,0.8009708737864077,"Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202–207, 1979."
REFERENCES,0.8058252427184466,"Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability of
randomized learning algorithms. Journal of Machine Learning Research, 6(1), 2005."
REFERENCES,0.8106796116504854,"Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Pro-
ceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
9770–9780, 2018."
REFERENCES,0.8155339805825242,"Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable al-
gorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270–1279. PMLR,
2019."
REFERENCES,0.8203883495145631,"Peter Gr¨unwald, Thomas Steinke, and Lydia Zakynthinou.
Pac-bayes, mac-bayes and condi-
tional mutual information: Fast rate bounds that handle general vc classes.
arXiv preprint
arXiv:2106.09683, 2021."
REFERENCES,0.8252427184466019,"Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite.
Sharpened generalization bounds based on conditional mutual information and an application to
noisy, iterative algorithms. Advances in Neural Information Processing Systems, 2020."
REFERENCES,0.8300970873786407,"Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225–1234, 2016."
REFERENCES,0.8349514563106796,"Fredrik Hellstr¨om and Giuseppe Durisi. Generalization bounds via information density and condi-
tional information density. IEEE Journal on Selected Areas in Information Theory, 1(3):824–839,
2020."
REFERENCES,0.8398058252427184,"Fredrik Hellstr¨om and Giuseppe Durisi. Fast-rate loss bounds via conditional information measures
with applications to neural networks. In 2021 IEEE International Symposium on Information
Theory (ISIT), pp. 952–957. IEEE, 2021."
REFERENCES,0.8446601941747572,"Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2525–
2536. Curran Associates, Inc., 2018."
REFERENCES,0.8495145631067961,"Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efﬁciently. In International Conference on Machine Learning, pp. 1724–1732, 2017."
REFERENCES,0.8543689320388349,"Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. On nonconvex
optimization for machine learning: Gradients, stochasticity, and saddle points. arXiv preprint
arXiv:1902.04811, 2019."
REFERENCES,0.8592233009708737,"Richeng Jin, Yufan Huang, Xiaofan He, Tianfu Wu, and Huaiyu Dai. Stochastic-sign sgd for feder-
ated learning with theoretical guarantees. arXiv preprint arXiv:2002.10940, 2020."
REFERENCES,0.8640776699029126,"Peter Kairouz, M´onica Ribero, Keith Rush, and Abhradeep Thakurta. Dimension independence in
unconstrained private erm via adaptive preconditioning. arXiv preprint arXiv:2008.06570, 2020."
REFERENCES,0.8689320388349514,"Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical Report Vol.
1. No. 4., University of Toronto, 2009."
REFERENCES,0.8737864077669902,"Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
REFERENCES,0.8786407766990292,"Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkxxtgHKPS."
REFERENCES,0.883495145631068,"Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for non-
convex learning: Two theoretical viewpoints. In Conference on Learning Theory, pp. 605–638.
PMLR, 2018."
REFERENCES,0.8883495145631068,Under review as a conference paper at ICLR 2022
REFERENCES,0.8932038834951457,"Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dependent estimates. In Advances
in Neural Information Processing Systems, 2019."
REFERENCES,0.8980582524271845,"Gergely Neu. Information-theoretic generalization bounds for stochastic gradient descent. arXiv
preprint arXiv:2102.00931, 2021."
REFERENCES,0.9029126213592233,"Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algo-
rithms. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 546–550. IEEE,
2018."
REFERENCES,0.9077669902912622,"David Pollard. A user’s guide to measure theoretic probability. Number 8. Cambridge University
Press, 2002."
REFERENCES,0.912621359223301,"Borja Rodr´ıguez-G´alvez, Germ´an Bassi, Ragnar Thobaben, and Mikael Skoglund.
On random
subset generalization error bounds and the stochastic gradient langevin dynamics algorithm. In
2020 IEEE Information Theory Workshop (ITW), pp. 1–5. IEEE, 2021."
REFERENCES,0.9174757281553398,"William H Rogers and Terry J Wagner. A ﬁnite sample distribution-free performance bound for
local discrimination rules. The Annals of Statistics, pp. 506–514, 1978."
REFERENCES,0.9223300970873787,"Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory.
In Artiﬁcial Intelligence and Statistics, pp. 1232–1240. PMLR, 2016."
REFERENCES,0.9271844660194175,"Igal Sason and Sergio Verdu. f-divergence inequalities. IEEE Transactions on Information Theory,
62, 2016."
REFERENCES,0.9320388349514563,"Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes. In International conference on machine learning,
pp. 71–79. PMLR, 2013."
REFERENCES,0.9368932038834952,"Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual
information. In Conference on Learning Theory, pp. 3437–3452. PMLR, 2020."
REFERENCES,0.941747572815534,"Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008."
REFERENCES,0.9466019417475728,"Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and vari-
ational inference. Now Publishers Inc, 2008."
REFERENCES,0.9514563106796117,"Di Wang and Jinhui Xu. Differentially private empirical risk minimization with smooth non-convex
loss functions: A non-stationary view. In Proceedings of the AAAI Conference on Artiﬁcial Intel-
ligence, volume 33, pp. 1182–1189, 2019."
REFERENCES,0.9563106796116505,"Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
International Conference on Machine Learning, ICML ’11, pp. 681–688, 2011."
REFERENCES,0.9611650485436893,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017."
REFERENCES,0.9660194174757282,"Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. Advances in Neural Information Processing Systems, 2017:2525–2534, 2017."
REFERENCES,0.970873786407767,"Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Christo-
pher De Sa. Swalp: Stochastic weight averaging in low-precision training. 36th International
Conference on Machine Learning (ICML), 2019."
REFERENCES,0.9757281553398058,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx."
REFERENCES,0.9805825242718447,"Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential pri-
vacy. arXiv preprint arXiv:2103.01294, 2021."
REFERENCES,0.9854368932038835,Under review as a conference paper at ICLR 2022
REFERENCES,0.9902912621359223,"Ruida Zhou, Chao Tian, and Tie Liu. Individually conditional individual mutual information bound
on generalization error. In 2021 IEEE International Symposium on Information Theory (ISIT),
pp. 670–675. IEEE, 2021."
REFERENCES,0.9951456310679612,"Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private sgd
with gradient subspace identiﬁcation. In International Conference on Learning Representations,
2020."
