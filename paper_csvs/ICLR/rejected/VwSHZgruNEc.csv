Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002369668246445498,"Search algorithms have been playing a vital role in the success of superhuman AI
in both perfect information and imperfect information games. SpeciÔ¨Åcally, search
algorithms can generate a reÔ¨Ånement of Nash equilibrium (NE) approximation
in games such as Texas hold‚Äôem with theoretical guarantees. However, when
confronted with opponents of limited rationality, an NE strategy tends to be overly
conservative, because it prefers to achieve its low exploitability rather than actively
exploiting the weakness of opponents. In this paper, we investigate the dilemma of
safety and opponent exploitation. We present a new real-time search framework
that smoothly interpolates between the two extremes of strategy search, hence
unifying safe search and opponent exploitation. We provide our new strategy with
a theoretically upper-bounded exploitability and lower-bounded reward against
an opponent. Our algorithm enables computationally efÔ¨Åcient online adaptations
to a possibly changing opponent model. Empirical results show that our method
signiÔ¨Åcantly outperforms NE baselines when opponents play non-NE strategies
and keeps low exploitability at the same time. It is also much more efÔ¨Åcient than
previous safe exploitation baselines."
INTRODUCTION,0.004739336492890996,"1
INTRODUCTION"
INTRODUCTION,0.0071090047393364926,"Behind the recent breakthroughs of superhuman AIs in Go (Silver et al., 2016; 2017; Schrittwieser
et al., 2020), heads-up no-limit Texas hold‚Äôem (HUNL) (Brown et al., 2018; Moravc√≠k et al., 2017;
Brown & Sandholm, 2019; Brown et al., 2020), and Hanabi (Lerer et al., 2020), search plays a vital
role. In perfect information games, Monte Carlo tree search (MCTS) is widely applied to improve
policy‚Äôs strength. In zero-sum imperfect information games such as poker, search algorithms are
used to Ô¨Ånd a Nash equilibrium (NE) approximation in subgames encountered in real time (Brown
& Sandholm, 2017; Burch et al., 2014a). They are both theoretically sounded and empirically
powerful. In fully-cooperative imperfect information games, the search algorithm proposed in Tian
et al. (2020b) is proved to never be detrimental to the current policy. Lerer et al. (2020) also ensures
original performance asymptotically."
INTRODUCTION,0.009478672985781991,"In zero-sum games, NE-based search algorithms (Burch et al., 2014a; Moravcik et al., 2016; Brown
& Sandholm, 2017; Brown et al., 2018) Ô¨Ånd safe strategies with low exploitability and produce strong
baselines against all opponents (Brown & Sandholm, 2019). However, it may be overly conservative
confronted with opponents with limited rationality and fail to take advantage of their weaknesses
to obtain higher rewards (McCracken & Bowling, 2004; Johanson et al., 2007; Li & Miikkulainen,
2018). From the other perspective, there have been extensive studies on opponent exploitation to
address the problem. Some typical works (Carmel & Markovitch, 1996; Billings et al., 2003; Gilpin
& Sandholm, 2006; Li & Miikkulainen, 2018) model the opponent‚Äôs strategy based on previous
observations and then search for a new strategy to exploit this model. However, these methods often
neglect the signiÔ¨Åcance of the strategy safety, thus being highly exploitable by the opponent. Few
exceptions including Johanson et al. (2007) and Ganzfried & Sandholm (2015a) aimed to search for
safe and robust counter strategies, but they are computationally inefÔ¨Åcient in an online setting where
the opponent model is being updated continuously with streamed data."
INTRODUCTION,0.011848341232227487,"In this paper, we study the dilemma of safety and opponent exploitation and present a new scalable
real-time search framework Safe Exploitation Search (SES) that smoothly interpolates between the
two extremes of strategy search, hence unifying safe search and opponent exploitation. It enables"
INTRODUCTION,0.014218009478672985,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016587677725118485,"computationally efÔ¨Åcient online adaptations to a continuously changing opponent model, which is
hard to address by previous safe exploitation algorithms. The safety criterion requires the reÔ¨Åned
strategy to stay close to NE, formally speaking, to expose limited exploitability against any opponents,
while the opponent exploitation criterion requires the strategy to adapt to its speciÔ¨Åc opponent and
to exploit its weaknesses. We propose a novel maximization objective which combines the safety
objective and exploitation, controlled by the exploitation level Œ±. We construct a new gadget game to
optimize this objective, which enables our method‚Äôs scalability to large games such as Texas Hold‚Äôem.
Theoretically, we prove that SES is guaranteed to outperform NE at the cost of some constant
increase in its own exploitability confronted with non-NE opponents. Empirically, we evaluate the
effectiveness of our search algorithm in 1 didactic matrix game 2 poker games: Leduc Hold‚Äôem
(Southey et al., 2005) and Flop Hold‚Äôem Poker (FHP)(Brown et al., 2019). The experiment results
demonstrate that our algorithm signiÔ¨Åcantly outperforms NE baselines against non-NE opponents and
keeps low exploitability at the same time. Additionally, SES is much more computationally efÔ¨Åcient
than previous safe exploitation baselines."
RELATED WORK,0.018957345971563982,"2
RELATED WORK"
RELATED WORK,0.02132701421800948,"This paper investigates the problem of safe opponent exploitation in two-player zero-sum imperfect
information games. We propose a novel search algorithm which balances between NE and exploiting
opponents. Two major relevant research areas are search algorithms in imperfect information games,
and opponent exploitation."
RELATED WORK,0.023696682464454975,"Search in imperfect information games. In recent literature, search techniques are witnessed to
be important in developing strong AI strategies in both perfect and imperfect information games
(Burch et al., 2014a; Moravcik et al., 2016; Brown & Sandholm, 2017). Texas hold ‚Äôem poker is
widely employed as a benchmark for imperfect information games. A primary part of the long-term
research on Texas hold‚Äôem poker is the evolution of subgame solving algorithms, which aim at
achieving a more accurate Nash equilibrium approximation in the subgame encountered given a
pre-computed strategy for the full game which we refer to as the blueprint strategy. Unsafe search
(Billings et al., 2003; Ganzfried & Sandholm, 2015b; Gilpin & Sandholm, 2006; 2007) estimates
the subgame reach probability assuming the opponent follows blueprint, and searches for a reÔ¨Åned
subgame strategy. Subgame resolving (Burch et al., 2014a) and maxmargin search (Moravcik et al.,
2016) are theoretically sounded safe search algorithms which ensure that the subgame strategy
obtained is no worse than the blueprint. They search in a gadget game and achieve safety by providing
the opponent with the option not entering the current subgame. DeepStack (Moravc√≠k et al., 2017)
and Libratus (Brown et al., 2018) build strong poker AIs with the aid of search. Beyond poker,
search algorithms for subgame reÔ¨Ånement have also shown promise in improving joint strategies in
cooperative imperfect information games such as Hanabi (Lerer et al., 2020) and the bidding phase of
contract bridge (Tian et al., 2020a). The purpose of our search algorithm is different from previous
methods in poker literature. We seek to exploit opponents while keeping exploitability low, rather
than simply approximating NE."
RELATED WORK,0.026066350710900472,"Opponent exploitation. Most previous opponent exploitation researches (Carmel & Markovitch,
1996; Billings et al., 2003; Gilpin & Sandholm, 2006; Li & Miikkulainen, 2018) typically model the
opponent‚Äôs strategy based on previous observations and then search for a new strategy to exploit this
model, but put little emphasis on safety."
RELATED WORK,0.02843601895734597,"One similar work is Johanson et al. (2007) which proposes p-restricted Nash response (RNR) to Ô¨Ånd
a safe exploitation strategy to the estimated opponent‚Äôs strategy. It calculates a Nash equilibrium for
the whole game restricting that the opponent plays the estimated strategy œÉÔ¨Åx with probability p, and
any strategy with probability 1 ‚àíp. In that paper, Johanson et al. (2007) prove that a p-RNR to œÉÔ¨Åx
is Pareto optimal with respect to exploitation and safety. However, it does not provide an explicit
bound. Additionally, whenever the estimated opponent model changes or we want to use a different p
to balance between safety and exploitation, p-RNR has to recompute the strategy for the whole game.
It is computationally inefÔ¨Åcient in an online setting, where the opponent model is updated after every
round with new game data. Our algorithm instead takes modelling error into account and provides
explicit bounds for both safety and exploitation. With the aid of real-time search, it only searches for
strategies in subgames encountered instead of the whole game. Our experiments show that it is more
efÔ¨Åcient than Johanson et al. (2007)."
RELATED WORK,0.030805687203791468,Under review as a conference paper at ICLR 2022
RELATED WORK,0.03317535545023697,"Ganzfried & Sandholm (2015a) study safe exploitation strategies in repeated games, which is a
different setting from this paper. Intuitively, it achieves safety by risking in exploitability at most
what it has earned over NE in expectation in previous rounds. Therefore, its expected value in the
whole repeated game is never worse than the NE. In contrast, this paper focuses on the safety of stage
game strategies. Furthermore, our algorithm is complementary to Ganzfried & Sandholm (2015a).
Ganzfried & Sandholm (2015a) calculate an Œµ-safe best response for the whole game at each iteration
with LP. This procedure is one of the main limitations on the algorithm‚Äôs scalability. Our algorithm,
which only reÔ¨Ånes strategies in subgames in real-time, can be a possible substitute for LP."
RELATED WORK,0.035545023696682464,"To our knowledge, we are the Ô¨Årst paper to investigate the safe opponent exploitation problem
in subgame resolving schemes. Subgame resolving enables online adaptations to a continuously
changing opponent model, eliminating the need to recompute a whole game strategy. It offers
computational beneÔ¨Åts in practical opponent exploitation circumstances."
RELATED WORK,0.037914691943127965,"It is worthwhile mentioning that there also has been extensive research (Albrecht & Stone, 2018)
on agent modeling. However, this paper only focuses on the theoretical and empirical results of the
search algorithm, but not the agent modeling techniques. We can use off-the-shelf agent modeling
algorithms to estimate opponent‚Äôs strategies. Agent modeling provides the ability to reason about the
others, making predictions of their strategies, types, etc. He et al. (2016) embeds opponent model
learning into deep Q-learning, which stabilizes the training process faced with opponents whose
policy is changing over time. LOLA (Foerster et al., 2018) considers the evolution of other agents in
the training process of multi-agent reinforcement learning. Raileanu et al. (2018) proposes to model
others from one‚Äôs own policy. Rabinowitz et al. (2018) uses meta learning to predict strategies of
different kinds of agents."
NOTATIONS AND BACKGROUND,0.04028436018957346,"3
NOTATIONS AND BACKGROUND"
NOTATIONS AND BACKGROUND,0.04265402843601896,"An extensive-form imperfect information game G = (P, H, Z, A, œá, œÅ, ¬∑, œÉc, u, I) describes sequen-
tial interactions among agents, where agents have private information. A Ô¨Ånite set P consists of n
players and a chance node c which represents the stochastic nature of the environment. The set of
non-terminal decision nodes is denoted as H, and Z is a set of terminal nodes, or leaves. The set of
possible actions is A, and œá : H ‚Üí2|A| is a function which assigns to each decision node h ‚ààH a
set of legal actions. A player function œÅ : H ‚ÜíP assigns to each decision node a player p ‚ààP who
acts at that node. If action a leads from h to h‚Ä≤, we write h ¬∑ a = h‚Ä≤. If there exists a sequence of
actions leading from h to h‚Ä≤, we write h ‚äëh‚Ä≤. At each node h ‚ààH, the acting player p = œÅ(h) picks
an action from legal actions a ‚ààœá(h), and leads node h into its child h ¬∑ a. The chance node always
samples an action from its own distribution œÉc, which is common knowledge to all players. Utility
functions are u = (u1, u2, . . . , un), where ui : Z ‚ÜíR deÔ¨Ånes the utility of player i at terminal node
z ‚ààZ. The nature of imperfect information is characterized by infosets I = (I1, I2, . . . , In), where
Ii = (Ii,1, . . . , Ii,ki) is a partition of H for player i. Two states in the same infoset must have the
same acting player and the same legal action sets. We use I(h) to denote the infoset that h belongs to.
A player p cannot distinguish between states h1 and h2 if I(h1) = I(h2), and thus should behave
identically on all states in the same infoset."
NOTATIONS AND BACKGROUND,0.045023696682464455,"The strategy of a player p is œÉp : Ip √ó A ‚ÜíR, where œÉp(I, a) is a distribution over valid actions on
infoset I. For simplicity, we also use œÉp(h, a) to denote œÉp(I(h), a). We use œÄœÉ(h) to denote the prob-
ability of reaching state h from the root when agents choose a strategy proÔ¨Åle œÉ = ‚ü®œÉ1, œÉ2, . . . , œÉn‚ü©.
Formally, œÄœÉ(h) = Q
h‚Ä≤¬∑a‚äëh œÉœÅ(h‚Ä≤)(h‚Ä≤, a). We use œÄœÉ
‚àíp(h) = Q
h‚Ä≤¬∑a‚äëh‚àßœÅ(h‚Ä≤)Ã∏=p œÉœÅ(h‚Ä≤)(h‚Ä≤, a) to de-
note the probability of reaching h when player p always chooses the action that leads to h whenever
possible. œÄœÉ(h, h‚Ä≤) is the reaching probability of h‚Ä≤ from h. œÄœÉ(h ¬∑ a, h‚Ä≤) is the the probability of
reaching h‚Ä≤ from h if action a is taken at h. These probabilities can be formally deÔ¨Åned in a similar
manner."
NOTATIONS AND BACKGROUND,0.04739336492890995,"The expected utility of player p given strategy proÔ¨Åle œÉ is uœÉ
p = P"
NOTATIONS AND BACKGROUND,0.04976303317535545,"z‚ààZ œÄœÉ(z)up(z). The counter-
factual value vœÉ
p (I, a) is the expected utility that player p will obtain after taking action a at infoset
I, given the joint policy proÔ¨Åle is œÉ. Mathematically, it is the weighted sum of expected values at all
states h ‚ààI."
NOTATIONS AND BACKGROUND,0.052132701421800945,"vœÉ
p (I, a) = P"
NOTATIONS AND BACKGROUND,0.054502369668246446,"h‚ààI,z‚ààZ œÄœÉ
‚àíp(h)œÄœÉ(h ¬∑ a, z)up(z)
P"
NOTATIONS AND BACKGROUND,0.05687203791469194,"h‚ààI œÄœÉ
‚àíp(h)
(1)"
NOTATIONS AND BACKGROUND,0.05924170616113744,Under review as a conference paper at ICLR 2022
NOTATIONS AND BACKGROUND,0.061611374407582936,"In the rest of the paper, we focus on two-player zero-sum games with perfect recall. Zero-sum means
‚àÄz ‚ààZ, u1(z) + u2(z) = 0. Perfect recall means that no player will forget the information which
has been obtained previously in the game. This is a common assumption in related literature."
NOTATIONS AND BACKGROUND,0.06398104265402843,"A best response strategy BRp(œÉ‚àíp) = arg maxœÉp u‚ü®œÉp,œÉ‚àíp‚ü©
p
for player p is the strategy that maximize
his own expected utility against Ô¨Åxed opponent strategy œÉ‚àíp. The exploitability of strategy œÉp is
exp(œÉp) = uœÉ‚àó
p
‚àíu‚ü®œÉp,BR‚àíp(œÉp)‚ü©
p
where œÉ‚àóis the optimal strategy, and is an NE in two-player
zero-sum games. It measures the performance of œÉp against its best response comparing with the
NE. A counterfactual best response CBRp(œÉ‚àíp) is a strategy where œÉp(I, a) > 0 if and only if
vœÉ
p (I, a) ‚â•maxb vœÉ
p (I, b). Counterfactual best response is a best response, but not vice versa. The"
NOTATIONS AND BACKGROUND,0.06635071090047394,"counterfactual best response value CBV œÉ‚àíp
p
(I) = v‚ü®CBRp(œÉ‚àíp),œÉ‚àíp‚ü©
p
is the expected utility of the
counterfactual best response policy. Since we focus on two-player zero-sum games, we will use
CBV œÉp(I) as a shorthand notation for CBV œÉp
‚àíp(I)."
NOTATIONS AND BACKGROUND,0.06872037914691943,"We follow the imperfect information subgame deÔ¨Ånition as in Burch et al. (2014b). An augmented
infoset contains states which cannot be distinguished by the remaining players."
NOTATIONS AND BACKGROUND,0.07109004739336493,"DeÔ¨Ånition 1. An imperfect information subgame S is a forest of trees, closed under both the
descendant relation and membership within augmented infosets for any player. Let Stop be the set of
nodes which are roots of each tree in S."
METHOD,0.07345971563981042,"4
METHOD"
METHOD,0.07582938388625593,"In this section, we introduce our novel search algorithm called safe exploitation search (SES), which
exploits the weaknesses of opponent while ensuring a bounded exploitability. Let œÉ be the pre-
computed blueprint strategy. Without loss of generality, assume we search for player 2‚Äôs reÔ¨Åned
strategy œÉS
2 by applying SES to all subgames S ‚ààS. Finally, the reÔ¨Åned strategy for P2 after search
is œÉ‚Ä≤
2, which is the same as œÉ2 in {Ii
2|‚àÄS ‚ààS, Ii
2 /‚ààS} and is replaced with œÉS
2 in S ‚ààS."
SAFE EXPLOITATION SEARCH,0.07819905213270142,"4.1
SAFE EXPLOITATION SEARCH"
SAFE EXPLOITATION SEARCH,0.08056872037914692,"Our algorithm offers a uniÔ¨Åed approach to balance between these two demands with theoretical
guarantees. The objective of our search algorithm is to Ô¨Ånd a new subgame strategy œÉS
2 for S ‚ààS
which maximizes"
SAFE EXPLOITATION SEARCH,0.08293838862559241,"SE(œÉS
2 ) = Œ±
X"
SAFE EXPLOITATION SEARCH,0.08530805687203792,"Ij
1‚ààStop
ÀÜp(Ij
1)

vœÉ
1 (Ij
1) ‚àíCBV œÉS
2
1
(Ij
1)

+ (1 ‚àíŒ±) min
Ij
1‚ààStop"
SAFE EXPLOITATION SEARCH,0.08767772511848342,"
vœÉ
1 (Ij
1) ‚àíCBV œÉS
2
1
(Ij
1)

,"
SAFE EXPLOITATION SEARCH,0.09004739336492891,"(2)
where Œ± ‚àà[0, 1] is a hyper-parameter controlling the exploitation level, and ÀÜp(Ii
1) is the estimated
probability of player 1 entering infoset Ij
1 ‚ààStop. Given P2‚Äôs strategy (which is the blueprint œÉ2)
and P1‚Äôs actual strategy (which does not have to be the blueprint œÉ1), the real probability of player 1
entering infoset Ij
1 ‚ààStop (which we denote as p(Ij
1)) is determined. ÀÜp(Ii
1) is just an estimation of
p(Ij
1). For instance, in poker, it is the estimated distribution of private cards player 1 holds. Both
theoretically and empirically, such estimation does not have to be fully accurate. It can be done with
off-the-shelf opponent modeling techniques, which lies beyond the focus of this paper."
SAFE EXPLOITATION SEARCH,0.0924170616113744,"Though seemingly complicated, intuitively, the maximization objective achieves a balance between
opponent exploitation and safety, controlled by exploitation level Œ±. The Ô¨Årst part of the objective"
SAFE EXPLOITATION SEARCH,0.0947867298578199,is maximized when P
SAFE EXPLOITATION SEARCH,0.0971563981042654,"Ij
1‚ààStop ÀÜp(Ij
1)CBV œÉS
2
1
(Ij
1) is minimized. It aims at Ô¨Ånding a strategy œÉS
2 which
results in the lowest value for P1 under the assumption that the reach probabilities is ÀÜp. It can be
interpreted as exploiting the estimated P1‚Äôs strategy. The second part of the objective demands the
resolved strategy to behave well against any reach probability distribution. We use the subgame
margin minIj
1‚ààStop"
SAFE EXPLOITATION SEARCH,0.0995260663507109,"
vœÉ
1 (Ij
1) ‚àíCBV œÉS
2
1
(Ij
1)

(Moravcik et al., 2016) which can be regarded as the
worst-case utility increase for P2."
SAFE EXPLOITATION SEARCH,0.1018957345971564,"By maximizing the objective 2, we provide sound theoretical results for both safety and opponent
exploitation. Additionally, we provide analyses of how (1) exploitation level Œ±, (2) accuracy of"
SAFE EXPLOITATION SEARCH,0.10426540284360189,Under review as a conference paper at ICLR 2022
SAFE EXPLOITATION SEARCH,0.1066350710900474,"opponent modeling, and (3) strength of the blueprint strategy impact the theoretical bound. By
gradually increasing Œ± from 0 to 1, our algorithm tends to exploit rather than keeping safety."
SAFE EXPLOITATION SEARCH,0.10900473933649289,"Theorem 1. (safety) Let S be a disjoint set of subgames S. Let œÉ‚àó= ‚ü®œÉ‚àó
1, œÉ‚àó
2‚ü©be the NE where P2‚Äôs
strategy is constrained to be the same with œÉ2 outside S. DeÔ¨Åne ‚àÜ= maxS‚ààS,Ii
1‚ààStop |CBV œÉ‚àó
2
1 (Ii
1)‚àí
vœÉ
1 (Ii
1)|. Let Àúp(Ii
1) be the reach probability given by œÉ‚àó
1. Let ÀÜp(Ii
1) be the estimation of reach
probability p(Ii
1) given by the real opponent strategy. DeÔ¨Åne œÑ = maxS‚ààS,Ii
1‚ààStop | ÀÜp(Ii
1)‚àíÀúp(Ii
1)
Àúp(Ii
1)
|.
Whenever 1 ‚àí(2œÑ + 1)Œ± > 0, we have a bounded exploitability given by:"
SAFE EXPLOITATION SEARCH,0.11137440758293839,"exp(œÉ‚Ä≤
2) ‚â§exp(œÉ‚àó
2) +
2
1 ‚àí(2œÑ + 1)Œ±‚àÜ.
(3)"
SAFE EXPLOITATION SEARCH,0.11374407582938388,"Recall that œÉ‚Ä≤
2 is the reÔ¨Åned strategy after search. The proof is provided in Appendix A. This
theorem implies that the exploitability of the new strategy is smaller than that of strategy œÉ‚àó
2 plus a
constant value, which is the closest strategy to NE if constrained to differ from œÉ2 only in S. The
corresponding theoretical result of maxmargin search (Moravcik et al., 2016), a safe search algorithm
with no opponent exploitation abilities, is exp(œÉ‚Ä≤
2) ‚â§exp(œÉ‚àó
2)+2‚àÜ. Comparing these two results, we
can interpret the term 2/(1 ‚àí(2œÑ + 1)Œ±) as the additional risk introduced by exploiting the opponent.
If exploitation level Œ± = 0, then our bound is as tight as that of maxmargin search (Moravcik et al.,
2016). The bound also gets tighter if the œÑ gets smaller, or the blueprint œÉ2 is closer to œÉ‚àó
2."
SAFE EXPLOITATION SEARCH,0.11611374407582939,"Theorem 2. (opponent exploitation) Let œµ = ‚à•ÀÜp‚àíp‚à•1 be the L1 distance of the distribution p(Ii
1) and"
SAFE EXPLOITATION SEARCH,0.11848341232227488,"ÀÜp(Ii
1). Let Œ∑ = minS‚ààS maxIj
1‚ààStop"
SAFE EXPLOITATION SEARCH,0.12085308056872038,"
CBV1(Ij
1, œÉS
2 ) ‚àíCBV1(Ij
1, œÉ‚àó
2)

‚â•0. We use BR[S,œÉp]
p
(œÉ)
to denote the strategy for player p which maximizes its utility in subgame S ‚ààS against œÉ‚àíp under
the constraint that BR[S,œÉp]
p
(œÉ) and œÉp differs only inside S. By maximizing objective 2, for all S ‚ààS,
the reÔ¨Åned strategy œÉ‚Ä≤
2 satisÔ¨Åes u"
SAFE EXPLOITATION SEARCH,0.12322274881516587,"D
BR[S,œÉ1]
1
(œÉ‚Ä≤
2),œÉ‚Ä≤
2
E"
SAFE EXPLOITATION SEARCH,0.12559241706161137,"2
(S) ‚â•u"
SAFE EXPLOITATION SEARCH,0.12796208530805686,"D
BR[S,œÉ1]
1
(œÉ‚àó
2),œÉ‚àó
2
E"
SAFE EXPLOITATION SEARCH,0.13033175355450238,"2
(S) + 1 ‚àíŒ±"
SAFE EXPLOITATION SEARCH,0.13270142180094788,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àíœµŒ∑
(4)"
SAFE EXPLOITATION SEARCH,0.13507109004739337,"The proof is provided in Appendix A. Observe that the reach probability p is characterized by P1‚Äôs
strategy outside S and ÀÜp is its estimation. Because the search algorithm always Ô¨Ånd a stronger
response strategy for P1 in S (which is exactly BR[S,œÉ1]
1
(œÉ‚Ä≤
2)) as well, opponent exploitation refers
to adapting to P1‚Äôs strategy œÉ1 outside S. This theorem implies that the utility of the new strategy
œÉ‚Ä≤
2 is lower bounded by the utility of œÉ‚àó
2 when both confronted with P1‚Äôs unknown strategy outside
S. It provides theoretical guarantees for the opponent exploitation ability of our algorithm. œµ can
be interpreted as estimation error. The lower bound increases if the estimation error get smaller or
the blueprint œÉ2 is closer to œÉ‚àó
2. We show empirically how exploitation level Œ± and estimation error
impact both safety and exploitation abilities in section 5."
GADGET GAME,0.13744075829383887,"4.2
GADGET GAME"
GADGET GAME,0.13981042654028436,"In order to Ô¨Ånd œÉS
2 which maximize objective 2, a straight-forward method is to reformulate the
maximization problem as a Linear Programming problem (Moravcik et al., 2016). However, LP
solvers (Koller et al., 1994) cannot handle large-scale problems. Alternatively, inspired by Moravcik
et al. (2016), we create a gadget game and then apply iteration-based NE algorithms such as CFR
(Zinkevich et al., 2007; Tammelin et al., 2015; Lanctot et al., 2009) in the gadget game. The gadget
game is carefully designed such that the NE solution found in it is exactly the solution to the original
optimization problem."
GADGET GAME,0.14218009478672985,"As shown in Figure 1, the original subgame is copied into two identical parts S1, S2 in the gadget
game. Player 2‚Äôs infosets stretch over both branches, while player 1 can distinguish between the two
parts. The procedure of constructing such gadget game can be summarized into 4 steps: (i). Create a
chance node at the top of the gadget game. (ii) For the left part of the gadget game, we construct a
P1 node to let P1 choose an infoset Ii
1 to enter. The following chance node samples a speciÔ¨Åc state
with probability proportional to œÄœÉ
‚àí1(h) for all h ‚ààI1. (iii) For the right part, create a chance node
sampling an infoset Ii
1. The following chance node again samples a speciÔ¨Åc state with probability"
GADGET GAME,0.14454976303317535,"Under review as a conference paper at ICLR 2022 S1 C
C
C P1 S2 C
C
C C"
GADGET GAME,0.14691943127962084,"C
1 ‚àíùõº
ùõº"
GADGET GAME,0.14928909952606634,"Figure 1: The gadget game of SES. The shadow and dashed line indicate that player 2 cannot
distinguish between the two branches. C represents chance node, P1 represents player 1‚Äôs action
node. S1 and S2 are two identical copies of the subgame S with utility shifted."
GADGET GAME,0.15165876777251186,"P2
P1
L
M
R"
GADGET GAME,0.15402843601895735,"U
3
2
4
O
2
3
9.9
D
3
2
9.9
F
-100
-100
10"
GADGET GAME,0.15639810426540285,"Table 1: The payoff matrix of the example zero-sum matrix game. The values are the payoffs of
player 2. We will resolve for player 2."
GADGET GAME,0.15876777251184834,"proportional to œÄœÉ
‚àí1(h) for all h ‚ààI1. (iv). Shift the utility of the gadget game by vœÉ
1 (Ii
1). The details
are described below."
GADGET GAME,0.16113744075829384,"1. The chance node at the top goes to the left part with probability 1 ‚àíŒ±, and the right part with
probability Œ±. The outcome is visible to P1 but not P2. Therefore, corresponding nodes in both
branches are in the same infosets for P2, and his strategy œÉS
2 will be the same for both parts. Since
œÉS
1 is the best-response to œÉS
2 and the two parts only differ at how to go to each infoset of player 1,
player 1 will also keep his strategy the same in both parts."
GADGET GAME,0.16350710900473933,"2. We subtract u1(z) by vœÉ
1 (Ii
1) for all z ‚äëh, h ‚ààIi
1, and add u2(z) by vœÉ
1 (Ii
1) in order to keep
the subgame zero-sum. By doing so, the objective of a Nash Equilibrium of p2 will change from"
GADGET GAME,0.16587677725118483,"maximizing ‚àíCBV œÉS
2
1
(Ii
1) to maximizing vœÉ
1 (Ii
1) ‚àíCBV œÉS
2
1
(Ii
1)."
GADGET GAME,0.16824644549763032,"3. As for the left part of the gadget game, the P1 node on the second level in Figure 1 enables P1
to enter an arbitrary infoset Ii
1. Since this is a zero-sum game, in an NE strategy, he will enter the
one with lowest vœÉ
1 (Ii
1) ‚àíCBV1(Ii
1, œÉ‚àó
2) which is exactly the minimization in the second term of
SE(œÉS
2 )."
THE CHANCE NODE ON THE SECOND LEVEL OF THE RIGHT PART WILL SAMPLE AN INFOSET II,0.17061611374407584,"4. The chance node on the second level of the right part will sample an infoset Ii
1 according to reach
probability ÀÜp(Ii
1). So that the NE objective of this part is exactly the summation in the Ô¨Årst term of
SE(œÉS
2 )."
MATRIX GAME,0.17298578199052134,"4.3
MATRIX GAME"
MATRIX GAME,0.17535545023696683,"In this part, we offer a matrix game as an example to show the necessity of considering safety and
expected payoff simultaneously, and to demonstrate the superiority of SES over a simple mixing
strategy, which follows a best response to the estimated opponent model with probability Œ± and
follows the blueprint with probability 1 ‚àíŒ±."
MATRIX GAME,0.17772511848341233,"In the matrix game shown in Table 1, let‚Äôs consider two speciÔ¨Åc NEs. In both NEs, P1 will play L/M
with 0.5 probability each. P2 will play U/O with 0.5 probability in the Ô¨Årst NE and O/D with 0.5
probability in the second NE. Suppose the blueprint strategy is the Ô¨Årst NE. Consider the case when
P1 plays a rather weak strategy that he will only play action R. We apply SES to search for P2‚Äôs
reÔ¨Åned strategy."
MATRIX GAME,0.18009478672985782,Under review as a conference paper at ICLR 2022
MATRIX GAME,0.18246445497630331,"0.0
0.2
0.4
0.6
0.8
1.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0"
MATRIX GAME,0.1848341232227488,chips/h
MATRIX GAME,0.1872037914691943,Evaluation
MATRIX GAME,0.1895734597156398,"SES
Mixing"
MATRIX GAME,0.19194312796208532,"0.0
0.2
0.4
0.6
0.8
1.0 0 20 40 60 80 100"
MATRIX GAME,0.1943127962085308,chips/h
MATRIX GAME,0.1966824644549763,Exploitability
MATRIX GAME,0.1990521327014218,"SES
Mixing"
MATRIX GAME,0.2014218009478673,"Figure 2: Left: Expected payoffs of SES and the mixing strategy in the proposed matrix game
example. Right: Exploitability of the two algorithms."
MATRIX GAME,0.2037914691943128,"When the estimation of opponent strategy is accurate such that ÀÜp = p, the best response of P2 is
always playing F, which is highly exploitable, while SES Ô¨Ånds the second NE under proper Œ±. To
give more details, the exploitability and expected payoff of the strategy found by SES and the mixing
strategy are shown in Figure 2. We can see that SES achieves both lower exploitability and better
performance than the mixing strategy at almost all Œ± values. The reason behind this success is that P1
always playing R is a Gift Strategy (Ganzfried & Sandholm, 2015a) in the designed matrix game and
SES manages to utilize such gift strategy. Strategy œÉ‚àíp is a Gift Strategy if it is not a best response to
a NE strategy œÉ‚àó
p. Therefore, we can switch to œÉ‚àó
p to get better performance against œÉ‚àíp while also
keeping exploitability low. However, the simple mixing strategy cannot Ô¨Ånd such ‚Äúgood"" NE strategy
so that it will perform much worse in both exploitability and expected payoff."
EXPERIMENT,0.20616113744075829,"5
EXPERIMENT"
EXPERIMENT,0.20853080568720378,"Our experiment is done in Leduc Hold‚Äôem (Southey et al., 2005) and Flop Hold‚Äôem Poker (FHP)
(Brown et al., 2019). Leduc Hold‚Äôem is a smaller-scale poker games and FHP is a larger one. The rules
of these two pokers are provided in Appendix B. We demonstrate the exploitability and evaluation
performance of SES against opponents of various strengths. The exploitability measures a search
algorithm‚Äôs safety, while head-to-head evaluation measures the ability of opponent exploitation. We
also illustrate how estimation accuracy of opponent‚Äôs strategy and the exploitation level Œ± impact the
results. Please refer to Appendix C for implementation details."
OPPONENTS,0.2109004739336493,"5.1
OPPONENTS"
OPPONENTS,0.2132701421800948,"In our experiments, we test the performance of our algorithm against opponents of various strengths.
For both Leduc Poker and FHP, we create 3 types of opponents with 3 random seeds each. The Ô¨Årst
type of opponent is an approximation of NE in the full game, and is regarded as a strong opponent.
It is computed in the same way as the blueprint strategy with different seed. For the second and
third type of opponents, we enumerate every infoset in the blueprint strategy and shift the action
distribution randomly with probability PrshufÔ¨Çe = 0.3 or 0.7. We multiply the probability of each
action by a random variable from Uniform(0, 1), and then re-normalize the probability distribution.
The procedure is motivated by Brown et al. (2018), in which such method is applied to create a
number of diverse but reasonably strong agents. Even when PrshufÔ¨Çe = 0.7, the strategy keeps close
to NE with average L1 distance of each infoset 0.132 comparing to 1.036 of a random strategy to NE.
So they are regarded as opponents who are not fully rational but with competitive strength."
SAFE OPPONENT SEARCH,0.2156398104265403,"5.2
SAFE OPPONENT SEARCH"
SAFE OPPONENT SEARCH,0.21800947867298578,"In Figure 3, we demonstrate the head-to-head evaluation performances and corresponding exploitabil-
ity of the reÔ¨Åned strategies found by SES against opponents of various strengths, under different
exploitation level Œ± and estimation errors of opponent‚Äôs strategy. Different lines in each plot refers
to corresponding estimation error œµ, which is the L1 distance of ÀÜp and p. We evaluate our reÔ¨Åned"
SAFE OPPONENT SEARCH,0.22037914691943128,Under review as a conference paper at ICLR 2022
SAFE OPPONENT SEARCH,0.22274881516587677,"0.0
0.2
0.4
0.6
0.8
1.0 70 60 50 40 30 20 10 0 mbb/h"
SAFE OPPONENT SEARCH,0.22511848341232227,Evaluation(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.22748815165876776,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.22985781990521326,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.23222748815165878,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.23459715639810427,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.23696682464454977,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.23933649289099526,"0.0
0.2
0.4
0.6
0.8
1.0 60 40 20 0 20 40 mbb/h"
SAFE OPPONENT SEARCH,0.24170616113744076,Evaluation(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.24407582938388625,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.24644549763033174,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.24881516587677724,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.25118483412322273,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.25355450236966826,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.2559241706161137,"0.0
0.2
0.4
0.6
0.8
1.0
340 350 360 370 380 390 mbb/h"
SAFE OPPONENT SEARCH,0.25829383886255924,Evaluation(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.26066350710900477,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.26303317535545023,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.26540284360189575,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.2677725118483412,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.27014218009478674,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.2725118483412322,"0.0
0.2
0.4
0.6
0.8
1.0
0 50 100 150 200 250 300 350 mbb/h"
SAFE OPPONENT SEARCH,0.27488151658767773,Exploitability(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.2772511848341232,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.2796208530805687,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.28199052132701424,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.2843601895734597,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.28672985781990523,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.2890995260663507,"0.0
0.2
0.4
0.6
0.8
1.0
0 50 100 150 200 250 300 350 mbb/h"
SAFE OPPONENT SEARCH,0.2914691943127962,Exploitability(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.2938388625592417,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.2962085308056872,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.2985781990521327,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.3009478672985782,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.3033175355450237,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.3056872037914692,"0.0
0.2
0.4
0.6
0.8
1.0
0 50 100 150 200 250 300 350 mbb/h"
SAFE OPPONENT SEARCH,0.3080568720379147,Exploitability(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.3104265402843602,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3127962085308057,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.31516587677725116,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.3175355450236967,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.31990521327014215,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.3222748815165877,"0.0
0.2
0.4
0.6
0.8
1.0 0 1 2 3 4 5 mbb/h"
SAFE OPPONENT SEARCH,0.3246445497630332,Evaluation(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.32701421800947866,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3293838862559242,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.33175355450236965,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.3341232227488152,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.33649289099526064,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.33886255924170616,"0.0
0.2
0.4
0.6
0.8
1.0 4 5 6 7 8 9 10 mbb/h"
SAFE OPPONENT SEARCH,0.3412322274881517,Evaluation(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.34360189573459715,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3459715639810427,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.34834123222748814,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.35071090047393366,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.35308056872037913,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.35545023696682465,"0.0
0.2
0.4
0.6
0.8
1.0 10 12 14 16 18 mbb/h"
SAFE OPPONENT SEARCH,0.3578199052132701,Evaluation(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.36018957345971564,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.36255924170616116,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.36492890995260663,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.36729857819905215,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.3696682464454976,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.37203791469194314,"0.0
0.2
0.4
0.6
0.8
1.0
20 25 30 35 40 45 50 mbb/h"
SAFE OPPONENT SEARCH,0.3744075829383886,Exploitability(Prshuffle = 0.0)
SAFE OPPONENT SEARCH,0.3767772511848341,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3791469194312796,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.3815165876777251,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.38388625592417064,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.3862559241706161,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.3886255924170616,"0.0
0.2
0.4
0.6
0.8
1.0 25 30 35 40 45 50 mbb/h"
SAFE OPPONENT SEARCH,0.3909952606635071,Exploitability(Prshuffle = 0.3)
SAFE OPPONENT SEARCH,0.3933649289099526,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.3957345971563981,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.3981042654028436,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.4004739336492891,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.4028436018957346,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.4052132701421801,"0.0
0.2
0.4
0.6
0.8
1.0 25 30 35 40 45 50 mbb/h"
SAFE OPPONENT SEARCH,0.4075829383886256,Exploitability(Prshuffle = 0.7)
SAFE OPPONENT SEARCH,0.4099526066350711,"Blueprint
Resolved Strategy( = 0.0)"
SAFE OPPONENT SEARCH,0.41232227488151657,Resolved Strategy( = 0.3)
SAFE OPPONENT SEARCH,0.4146919431279621,Resolved Strategy( = 0.6)
SAFE OPPONENT SEARCH,0.41706161137440756,Resolved Strategy( = 0.9)
SAFE OPPONENT SEARCH,0.4194312796208531,Resolved Strategy( = 1.2)
SAFE OPPONENT SEARCH,0.4218009478672986,"Figure 3: Row 1&2: Experiment results on Leduc poker. Row 3&4: Experiment results on FHP.
From left to right, each column represents a type of opponent with PrshufÔ¨Çe = 0.0, 0.3, 0.7. The Ô¨Årst
row of each game shows the head-to-head expected payoffs against corresponding opponents, while
the second row demonstrates the exploitability calculated for different reÔ¨Åned strategies. The X-axis
is the parameter Œ± which controls exploitation level."
SAFE OPPONENT SEARCH,0.42417061611374407,"strategy when œµ = 0.0, 0.3, 0.6, 0.9, 1.2. The blue line is the result of blueprint strategy without
conducting any search."
SAFE OPPONENT SEARCH,0.4265402843601896,"Generally speaking, SES balances between safety and opponent exploitation. The increase of
exploitation level Œ± helps win more chips from opponents, while resulting in the increase of the
strategy‚Äôs own exploitability. As can be seen in Figure 3, the exploitability increases when the
exploitation level Œ± grows from 0 to 1, which is consistent with Theorem 1. One exception is in FHP
when œµ is small: the exploitability surprisingly keeps decreasing even if SES puts more emphasis
on opponent exploitation. Similar situations have also occurred in previous literature (Brown &
Sandholm, 2017). The reason is that our opponent is quite close to NE outside the subgame which
will make ÀÜp close to Àúp when œµ is small, which means the œÑ in Theorem 1 is small. As a result, we will
have a low-exploitability resolved strategy when using unsafe search and the exploitability increases
as œµ increases."
SAFE OPPONENT SEARCH,0.42890995260663506,"When the estimation is completely correct (œµ = 0.0, the yellow line), the expected payoff in FHP
increases as the exploitation level Œ± grows higher. In Leduc poker, since the game is very small,
the pre-computed blueprint is very close to NE. Therefore, when confronted with relatively strong
opponents (PrshuÔ¨Ñe = 0.0, 0.3) which are also close to NE, actually few things can be done other"
SAFE OPPONENT SEARCH,0.4312796208530806,Under review as a conference paper at ICLR 2022
SAFE OPPONENT SEARCH,0.43364928909952605,"than sticking with the blueprint. So the improvement introduced by SES is small. When facing
relatively weak opponent (PrshuÔ¨Ñe = 0.7), the improvement margin is slightly larger."
SAFE OPPONENT SEARCH,0.43601895734597157,"SES relies on an estimation of opponent‚Äôs strategy. In order to test the robustness of our algorithm
when the prediction of p(Ii
1) is not accurate, we evaluate the performance of our algorithm with
different values of estimation error œµ. As illustrated in Figure.3, the exploitability increases and the
expected payoff drops when œµ grows larger. The result is expected since an accurate estimation always
provides beneÔ¨Åts. However, it also demonstrates that SES can still achieve a trade-off between safety
and opponent exploitation even when œµ is considerably high. For instance, in FHP, œµ is between 0 and
2, and œµ = 1.2 means that the predicted distribution is almost random. When œµ ‚â§0.6, the expected
payoff still keeps increasing with respect to Œ±. In case of a bad estimation, we can always choose
smaller Œ± to ensure safety."
COMPARISON WITH RESTRICTED NASH RESPONSE,0.43838862559241704,"5.3
COMPARISON WITH RESTRICTED NASH RESPONSE"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.44075829383886256,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 20 15 10 5 0 5 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4431279620853081,Evaluation(Prshuffle = 0.0)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.44549763033175355,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.44786729857819907,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 15 10 5 0 5 10 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.45023696682464454,Evaluation(Prshuffle = 0.3)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.45260663507109006,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4549763033175355,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 5 0 5 10 15 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.45734597156398105,Evaluation(Prshuffle = 0.7)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4597156398104265,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.46208530805687204,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 20 30 40 50 60 70 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.46445497630331756,Exploitability(Prshuffle = 0.0)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.466824644549763,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.46919431279620855,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 20 30 40 50 60 70 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.471563981042654,Exploitability(Prshuffle = 0.3)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.47393364928909953,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.476303317535545,"0.0
0.2
0.4
0.6
0.8
1.0
p( ) 30 40 50 60 70 mbb/h"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4786729857819905,Exploitability(Prshuffle = 0.7)
COMPARISON WITH RESTRICTED NASH RESPONSE,0.48104265402843605,"RNR(normal)
RNR(big)
SES"
COMPARISON WITH RESTRICTED NASH RESPONSE,0.4834123222748815,"Figure 4: Comparison between SES and RNR. From left to right, each column represents a type of
opponent with PrshufÔ¨Çe = 0.0, 0.3, 0.7. The X-axis is the parameter Œ± for SES and p for RNR."
COMPARISON WITH RESTRICTED NASH RESPONSE,0.48578199052132703,"We also compare SES with restricted Nash response (RNR) (Johanson et al., 2007) in FHP. In each
round, we limit the computation time of RNR(normal) to 10 CPU second1, which is the same for
SES. However, as stated in section 2, RNR needs to recompute a strategy for the whole game in each
round. It cannot converge in 10s. So we also compare with RNR(big), which has a budget of 10M
CFR iterations in each round (around 190 CPU second in time). In contrast, SES only uses 10M
CFR iterations to calculate its blueprint once. As is shown in Figure 4, SES signiÔ¨Åcantly outperforms
RNR(normal) in both exploitability and evaluation. SES also achieves much lower exploitability than
RNR(big) and comparable evaluation results with much less computation time."
CONCLUSION,0.4881516587677725,"6
CONCLUSION"
CONCLUSION,0.490521327014218,"We propose a novel safe exploitation search (SES) algorithm which uniÔ¨Åes both safe search and
opponent exploitation. With the aid of real-time search, SES can make online adaptations to a
changing opponent model. We also prove safety and opponent exploitation guarantees of SES in
Theorem 1 and Theorem 2. The experimental results in our designed matrix game conÔ¨Årm the
existence of the reÔ¨Åned strategy which is both safe and actively exploiting the opponent. In games of
poker, our method outperforms NE baselines while keeping exploitability low. SES is also much more
efÔ¨Åcient than previous safe exploitation algorithms without search. The exploitation level Œ± is now
regarded as a hyperparameter in our algorithm. However, ideally, Œ± should be learnt automatically
from opponents, and should be adaptive to opponent‚Äôs strategy change. We leave this for future work."
CONCLUSION,0.4928909952606635,1We test it on Intel(R) Xeon(R) Platinum 8276L CPU @ 2.20GHz
CONCLUSION,0.495260663507109,Under review as a conference paper at ICLR 2022
REFERENCES,0.4976303317535545,REFERENCES
REFERENCES,0.5,"Stefano V. Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive
survey and open problems. ArtiÔ¨Åcial Intelligence, 258:66‚Äì95, 2018. ISSN 0004-3702. doi:
https://doi.org/10.1016/j.artint.2018.01.002. URL https://www.sciencedirect.com/
science/article/pii/S0004370218300249."
REFERENCES,0.5023696682464455,"Darse Billings, Neil Burch, Aaron Davidson, Robert Holte, Jonathan Schaeffer, Terence Schauenberg,
and Duane Szafron. Approximating game-theoretic optimal strategies for full-scale poker. In
IJCAI, volume 3, pp. 661, 2003."
REFERENCES,0.504739336492891,"Noam Brown and Tuomas Sandholm. Safe and nested subgame solving for imperfect-information
games. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pp. 689‚Äì699, 2017."
REFERENCES,0.5071090047393365,"Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885‚Äì890, 2019."
REFERENCES,0.509478672985782,"Noam
Brown,
Tuomas
Sandholm,
and
Brandon
Amos.
Depth-limited
solving
for
imperfect-information games.
In Advances in Neural Information Processing Sys-
tems
31:
Annual
Conference
on
Neural
Information
Processing
Systems,
Mon-
tr√©al,
Canada,
pp. 7674‚Äì7685,
2018.
URL http://papers.nips.cc/paper/
7993-depth-limited-solving-for-imperfect-information-games."
REFERENCES,0.5118483412322274,"Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-
mization. In International conference on machine learning, pp. 793‚Äì802. PMLR, 2019."
REFERENCES,0.514218009478673,"Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement
learning and search for imperfect-information games. arXiv preprint arXiv:2007.13544, 2020."
REFERENCES,0.5165876777251185,"Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using
decomposition. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 28,
2014a."
REFERENCES,0.518957345971564,"Neil Burch, Michael Johanson, and Michael Bowling. Solving imperfect information games using
decomposition. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 28,
2014b."
REFERENCES,0.5213270142180095,"David Carmel and Shaul Markovitch. Learning models of intelligent agents. In AAAI/IAAI, Vol. 1,
pp. 62‚Äì67, 1996."
REFERENCES,0.523696682464455,"Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor
Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th International
Conference on Autonomous Agents and MultiAgent Systems, AAMAS ‚Äô18, pp. 122‚Äì130, Richland,
SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems."
REFERENCES,0.5260663507109005,"Sam Ganzfried and Tuomas Sandholm. Safe opponent exploitation. ACM Transactions on Economics
and Computation (TEAC), 3(2):1‚Äì28, 2015a."
REFERENCES,0.5284360189573459,"Sam Ganzfried and Tuomas Sandholm. Endgame solving in large imperfect-information games. In
Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,
pp. 37‚Äì45. Citeseer, 2015b."
REFERENCES,0.5308056872037915,"Andrew Gilpin and Tuomas Sandholm. A competitive texas hold‚Äôem poker player via automated
abstraction and real-time equilibrium computation. In AAAI, pp. 1007‚Äì1013, 2006."
REFERENCES,0.533175355450237,"Andrew Gilpin and Tuomas Sandholm. Better automated abstraction techniques for imperfect
information games, with application to texas hold‚Äôem poker. In Proceedings of the 6th international
joint conference on Autonomous agents and multiagent systems, pp. 1‚Äì8, 2007."
REFERENCES,0.5355450236966824,"He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum√©, III. Opponent modeling in deep
reinforcement learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 1804‚Äì1813, New York, New York, USA, 20‚Äì22 Jun 2016. PMLR. URL
http://proceedings.mlr.press/v48/he16.html."
REFERENCES,0.5379146919431279,Under review as a conference paper at ICLR 2022
REFERENCES,0.5402843601895735,"Michael Johanson, Michael Bowling, and Martin Zinkevich. Computing robust counter-strategies.
2007."
REFERENCES,0.542654028436019,"Michael Johanson, Neil Burch, Richard Valenzano, and Michael Bowling. Evaluating state-space
abstractions in extensive-form games. In Proceedings of the 2013 international conference on
Autonomous agents and multi-agent systems, pp. 271‚Äì278, 2013."
REFERENCES,0.5450236966824644,"Daphne Koller, Nimrod Megiddo, and Bernhard Von Stengel. Fast algorithms for Ô¨Ånding randomized
strategies in game trees. In Proceedings of the twenty-sixth annual ACM symposium on Theory of
computing, pp. 750‚Äì759, 1994."
REFERENCES,0.54739336492891,"Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H Bowling. Monte carlo sampling for
regret minimization in extensive games. In NIPS, pp. 1078‚Äì1086, 2009."
REFERENCES,0.5497630331753555,"Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search
in cooperative partially observable games. Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, 34(05):7187‚Äì7194, Apr. 2020.
doi: 10.1609/aaai.v34i05.6208.
URL https:
//ojs.aaai.org/index.php/AAAI/article/view/6208."
REFERENCES,0.5521327014218009,"Xun Li and Risto Miikkulainen. Dynamic adaptation and opponent exploitation in computer poker.
In Workshops at the Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, 2018."
REFERENCES,0.5545023696682464,"Peter McCracken and Michael Bowling. Safe strategies for agent modelling in games. In ArtiÔ¨Åcial
Multiagent Learning, Papers from the 2004 AAAI Fall Symposium. Arlington, VA, USA, October
22-24, 2004, volume FS-04-02, pp. 103‚Äì110. AAAI Press, 2004. URL https://www.aaai.
org/Library/Symposia/Fall/2004/fs04-02-014.php."
REFERENCES,0.556872037914692,"Matej Moravcik, Martin Schmid, Karel Ha, Milan Hladik, and Stephen Gaukrodger. ReÔ¨Åning
subgames in large imperfect information games. In Proceedings of the AAAI Conference on
ArtiÔ¨Åcial Intelligence, volume 30, 2016."
REFERENCES,0.5592417061611374,"Matej Moravc√≠k, Martina Schmid, Neil Burch, Viliam Lis√Ω, Dustin Morrill, Nolan Bard, Trevor Davis,
Kevin Waugh, Michael Johanson, and Michael H. Bowling. Deepstack: Expert-level artiÔ¨Åcial
intelligence in heads-up no-limit poker. Science, 356:508‚Äì513, 2017."
REFERENCES,0.5616113744075829,"Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S. M. Ali Eslami, and Matthew
Botvinick. Machine theory of mind. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4218‚Äì4227. PMLR, 10‚Äì15 Jul 2018. URL http://proceedings.
mlr.press/v80/rabinowitz18a.html."
REFERENCES,0.5639810426540285,"Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in
multi-agent reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 4257‚Äì4266. PMLR, 10‚Äì15 Jul 2018. URL http://proceedings.
mlr.press/v80/raileanu18a.html."
REFERENCES,0.566350710900474,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604‚Äì609, 2020."
REFERENCES,0.5687203791469194,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
game of go with deep neural networks and tree search. Nature, 529(7587):484‚Äì489, 2016. doi:
10.1038/nature16961. URL https://doi.org/10.1038/nature16961."
REFERENCES,0.5710900473933649,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017."
REFERENCES,0.5734597156398105,Under review as a conference paper at ICLR 2022
REFERENCES,0.5758293838862559,"Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes‚Äô bluff: opponent modelling in poker. In Proceedings of the Twenty-First
Conference on Uncertainty in ArtiÔ¨Åcial Intelligence, pp. 550‚Äì558, 2005."
REFERENCES,0.5781990521327014,"Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit
texas hold‚Äôem. In Twenty-fourth international joint conference on artiÔ¨Åcial intelligence, 2015."
REFERENCES,0.580568720379147,"Yuandong Tian, Qucheng Gong, and Tina Jiang. Joint policy search for multi-agent collaboration
with imperfect information. arXiv preprint arXiv:2008.06495, 2020a."
REFERENCES,0.5829383886255924,"Yuandong Tian, Qucheng Gong, and Yu Jiang. Joint policy search for multi-agent collaboration with
imperfect information. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 19931‚Äì19942. Curran Asso-
ciates, Inc., 2020b. URL https://proceedings.neurips.cc/paper/2020/file/
e64f346817ce0c93d7166546ac8ce683-Paper.pdf."
REFERENCES,0.5853080568720379,"Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. Advances in neural information processing systems, 20:
1729‚Äì1736, 2007."
REFERENCES,0.5876777251184834,Under review as a conference paper at ICLR 2022
REFERENCES,0.590047393364929,"A
PROOFS"
REFERENCES,0.5924170616113744,"A.1
PROOF OF THEOREM 1"
REFERENCES,0.5947867298578199,"Theorem 1. (safety) Let S be a disjoint set of subgames S. Let œÉ‚àó= ‚ü®œÉ‚àó
1, œÉ‚àó
2‚ü©be the NE where P2‚Äôs
strategy is constrained to be the same with œÉ2 outside S. DeÔ¨Åne ‚àÜ= maxS‚ààS,Ii
1‚ààStop |CBV œÉ‚àó
2
1 (Ii
1)‚àí
vœÉ
1 (Ii
1)|. Let Àúp(Ii
1) be the reach probability given by œÉ‚àó
1. Let ÀÜp(Ii
1) be the estimation of reach
probability p(Ii
1) given by the real opponent strategy. DeÔ¨Åne œÑ = maxS‚ààS,Ii
1‚ààStop | ÀÜp(Ii
1)‚àíÀúp(Ii
1)
Àúp(Ii
1)
|.
Whenever 1 ‚àí(2œÑ + 1)Œ± > 0, we have a bounded exploitability given by:"
REFERENCES,0.5971563981042654,"exp(œÉ‚Ä≤
2) ‚â§exp(œÉ‚àó
2) +
2
1 ‚àí(2œÑ + 1)Œ±‚àÜ.
(5)"
REFERENCES,0.5995260663507109,Proof:
REFERENCES,0.6018957345971564,"For simplicity, we will omit the subscript of CBV œÉ‚àó
2
1
by default. In order to prove Theorem 1, we
will use mathematical induction on the level of the infoset. The depth L has the same deÔ¨Ånition as in
Brown & Sandholm (2017), i.e."
REFERENCES,0.6042654028436019,"‚Ä¢ For all the infosets which are direct parents of the subgames, we deÔ¨Åne L(I) = 0.
‚Ä¢ For the infosets that are not ancestors of the subgames, we deÔ¨Åne L(I) = 0.
‚Ä¢ For any infosets that are ancestors of the subgames, we deÔ¨Åne
L(I) = maxI‚Ä≤‚ààsucc(I) L(I‚Ä≤) + 1. That is, it has a higher level than any of its successors."
REFERENCES,0.6066350710900474,BASE CASE OF INDUCTION
REFERENCES,0.6090047393364929,"Firstly, we will prove that for any infoset with level 0, the inequality of theorem 1 holds. For
convenience, we consider that theorem 1 in a speciÔ¨Åc subgame S."
REFERENCES,0.6113744075829384,"We will prove the infoset at the top of the subgame Ô¨Årst. Since SE(œÉ‚àó
2) ‚â•(1 ‚àíŒ±)(‚àí‚àÜ) +
Œ± P"
REFERENCES,0.6137440758293838,"i ÀÜp(Ii
1)(‚àí‚àÜ) = ‚àí‚àÜ, we have"
REFERENCES,0.6161137440758294,"(1 ‚àíŒ±) min
Ij
1"
REFERENCES,0.6184834123222749,"
vœÉ
1 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)

+ Œ±
X"
REFERENCES,0.6208530805687204,"i
ÀÜp(Ii
1)(vœÉ
1 (Ii
1) ‚àíCBV œÉS
2 (Ii
1))"
REFERENCES,0.6232227488151659,"=SE(œÉS
2 )
‚â•SE(œÉ‚àó
2)
‚â•‚àí‚àÜ (6)"
REFERENCES,0.6255924170616114,"since œÉS
2 = arg maxÀúœÉ2 SE(ÀúœÉ2)."
REFERENCES,0.6279620853080569,"Furthermore, we have
X"
REFERENCES,0.6303317535545023,"i
ÀÜp(Ii
1)(vœÉ
1 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) =
X"
REFERENCES,0.6327014218009479,"i
ÀÜp(Ii
1)(vœÉ
1 (Ii
1) ‚àíCBV œÉ‚àó
2 (Ii
1)) +
X"
REFERENCES,0.6350710900473934,"i
ÀÜp(Ii
1)(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) ‚â§‚àÜ+
X"
REFERENCES,0.6374407582938388,"i
ÀÜp(Ii
1)(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) =‚àÜ+
X"
REFERENCES,0.6398104265402843,"i
Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) +
X"
REFERENCES,0.6421800947867299,"i
(ÀÜp(Ii
1) ‚àíÀúp(Ii
1))(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) (7)"
REFERENCES,0.6445497630331753,where the second term P
REFERENCES,0.6469194312796208,"i Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) is no larger than 0 because
P"
REFERENCES,0.6492890995260664,"i Àúp(I1)CBV œÉ‚àó
2 (Ii
1) is exactly what œÉ‚àó
2 minimized. Otherwise, œÉ‚àó
2 can change the strategy in
the subgame so that he will get higher reward against œÉ‚àó
1 which conÔ¨Çicts the deÔ¨Ånition of NE."
REFERENCES,0.6516587677725119,Under review as a conference paper at ICLR 2022
REFERENCES,0.6540284360189573,"And we will further decompose Ii
1 ‚ààStop into two parts, {Ii,‚àí
1
} and {Ii,+
1
}. They have the property
that CBV œÉ‚àó
2 (Ii,‚àí
1
) ‚àíCBV œÉS
2 (Ii,‚àí
1
) ‚â§0 and CBV œÉ‚àó
2 (Ii,+
1
) ‚àíCBV œÉS
2 (Ii,+
1
) > 0. And since
P"
REFERENCES,0.6563981042654028,"i Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) ‚â§0 as discussed above, we have
X"
REFERENCES,0.6587677725118484,"Ii,‚àí
1"
REFERENCES,0.6611374407582938,"Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii,‚àí
1
) ‚àíCBV œÉS
2 (Ii,‚àí
1
)) +
X"
REFERENCES,0.6635071090047393,"Ii,+
1"
REFERENCES,0.6658767772511849,"Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii,+
1
) ‚àíCBV œÉS
2 (Ii,+
1
)) =
X"
REFERENCES,0.6682464454976303,"i
Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) ‚â§0 (8)"
REFERENCES,0.6706161137440758,"which implies that
X"
REFERENCES,0.6729857819905213,"Ii,+
1"
REFERENCES,0.6753554502369669,"Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii,+
1
) ‚àíCBV œÉS
2 (Ii,+
1
)) ‚â§‚àí
X"
REFERENCES,0.6777251184834123,"Ii,‚àí
1"
REFERENCES,0.6800947867298578,"Àúp(Ii
1)(CBV œÉ‚àó
2 (Ii,‚àí
1
) ‚àíCBV œÉS
2 (Ii,‚àí
1
)) (9)"
REFERENCES,0.6824644549763034,"Then we have
X"
REFERENCES,0.6848341232227488,"i
(ÀÜp(Ii
1) ‚àíÀúp(Ii
1))(CBV œÉ‚àó
2 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) =
X"
REFERENCES,0.6872037914691943,"Ii,‚àí
1"
REFERENCES,0.6895734597156398,"(ÀÜp(Ii,‚àí
1
) ‚àíÀúp(Ii,‚àí
1
))(CBV œÉ‚àó
2 (Ii,‚àí
1
) ‚àíCBV œÉS
2 (Ii,‚àí
1
)) +
X"
REFERENCES,0.6919431279620853,"Ii,+
1"
REFERENCES,0.6943127962085308,"(ÀÜp(Ii,+
1
) ‚àíÀúp(Ii,+
1
))(CBV œÉ‚àó
2 (Ii,+
1
) ‚àíCBV œÉS
2 (I
Ii,+
1
1
))"
REFERENCES,0.6966824644549763,"‚â§œÑ

‚àí
X"
REFERENCES,0.6990521327014217,"Ii,‚àí
1"
REFERENCES,0.7014218009478673,"Àúp(Ii,‚àí
1
)(CBV œÉ‚àó
2 (Ii,‚àí
1
) ‚àíCBV œÉS
2 (I
Ii,‚àí
1
1
)) +
X"
REFERENCES,0.7037914691943128,"Ii,+
1"
REFERENCES,0.7061611374407583,"Àúp(Ii,+
1
)(CBV œÉ‚àó
2 (Ii,+
1
) ‚àíCBV œÉS
2 (I
Ii,+
1
1
))
"
REFERENCES,0.7085308056872038,"‚â§‚àí2œÑ
X"
REFERENCES,0.7109004739336493,"Ii,‚àí
1"
REFERENCES,0.7132701421800948,"Àúp(Ii,‚àí
1
)(CBV œÉ‚àó
2 (Ii,‚àí
1
) ‚àíCBV œÉS
2 (I
Ii,‚àí
1
1
))"
REFERENCES,0.7156398104265402,"‚â§‚àí2œÑ min
Ij
1"
REFERENCES,0.7180094786729858,"
CBV œÉ‚àó
2 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)
 (10)"
REFERENCES,0.7203791469194313,"The last inequation holds since minIj
1"
REFERENCES,0.7227488151658767,"
CBV œÉ‚àó
2 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)

‚â§0 since œÉ‚àó
2 is the strategy
with lowest exploitability by only changing strategy of œÉ2 in the subgames."
REFERENCES,0.7251184834123223,"Back to Equation 7, we have
X"
REFERENCES,0.7274881516587678,"i
ÀÜp(Ii
1)(vœÉ
1 (Ii
1) ‚àíCBV œÉS
2 (Ii
1)) ‚â§‚àÜ‚àí2œÑ min
Ij
1"
REFERENCES,0.7298578199052133,"
CBV œÉ‚àó
2 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)

(11)"
REFERENCES,0.7322274881516587,"And substitute it into Equation 6,"
REFERENCES,0.7345971563981043,"‚àÜ+ (1 ‚àíŒ± ‚àí2Œ±œÑ) min
Ij
1"
REFERENCES,0.7369668246445498,"
CBV œÉ‚àó
2 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)
"
REFERENCES,0.7393364928909952,"‚â•(1 ‚àíŒ±) min
Ij
1"
REFERENCES,0.7417061611374408,"
vœÉ
1 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)

+ Œ±(‚àÜ‚àí2œÑ) min
Ij
1"
REFERENCES,0.7440758293838863,"
CBV œÉ‚àó
2 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)
 ‚â•‚àí‚àÜ (12)"
REFERENCES,0.7464454976303317,"so that
CBV œÉS
2 (Ij
1) ‚â§CBV œÉ‚àó
2 (Ij
1) +
2
1 ‚àí(2œÑ + 1)Œ±‚àÜ
(13)"
REFERENCES,0.7488151658767772,"for all Ij
1 in the subgame."
REFERENCES,0.7511848341232228,"And for infoset I out of the subgame with level 0, since the reÔ¨Åned strategy œÉS
2 and blueprint strategy
œÉ2 are the same here, the CBV value is exactly the same and the inequality holds."
REFERENCES,0.7535545023696683,Under review as a conference paper at ICLR 2022
REFERENCES,0.7559241706161137,INDUCTIVE STEP
REFERENCES,0.7582938388625592,The inductive step mostly follows that of Brown & Sandholm (2017).
REFERENCES,0.7606635071090048,"Since CBV œÉS
2 (I1) ‚â§CBV œÉ‚àó
2 (I1) +
2
1‚àí(2œÑ+1)Œ±‚àÜholds for every subgame S, œÉ‚Ä≤
2 will also satisfy
this inequation since ‚àÜand œÑ are deÔ¨Åned as maximum over all subgames."
REFERENCES,0.7630331753554502,"Now, suppose CBV œÉ‚Ä≤
2(I1) ‚â§CBV œÉ‚àó
2 (I1) +
2
1‚àí(2œÑ+1)Œ±‚àÜholds for any infoset with level lower or
equal to k, we will prove that it also holds for infoset with level k + 1."
REFERENCES,0.7654028436018957,"By deÔ¨Ånition of CBV (I1),"
REFERENCES,0.7677725118483413,"CBV œÉ2(I1, a) =
 X"
REFERENCES,0.7701421800947867,"h‚ààI1
œÄœÉ2
‚àí1(h)v‚ü®CBR(œÉ2),œÉ2‚ü©(h ¬∑ a)

/
X"
REFERENCES,0.7725118483412322,"h‚ààI1
œÄœÉ2
‚àí1(h) =
 X"
REFERENCES,0.7748815165876777,"h‚ààI1
œÄœÉ2
‚àí1(h)
X"
REFERENCES,0.7772511848341233,"h‚Ä≤‚ààsucc(h,a)
œÄœÉ2
‚àí1(h, h‚Ä≤)v‚ü®CBR(œÉ2),œÉ2‚ü©(h‚Ä≤)

/
X"
REFERENCES,0.7796208530805687,"h‚ààI1
œÄœÉ2
‚àí1(h) =
 X h‚ààI1 X"
REFERENCES,0.7819905213270142,"h‚Ä≤‚ààsucc(h,a)
œÄœÉ2
‚àí1(h‚Ä≤)v‚ü®CBR(œÉ2),œÉ2‚ü©(h‚Ä≤)

/
X"
REFERENCES,0.7843601895734598,"h‚ààI1
œÄœÉ2
‚àí1(h) (14)"
REFERENCES,0.7867298578199052,"We can swap the two summations above since the game is perfect recall, then"
REFERENCES,0.7890995260663507,"CBV œÉ2(I1, a) =

X"
REFERENCES,0.7914691943127962,"I‚Ä≤
1‚ààsucc(I1,a) X"
REFERENCES,0.7938388625592417,"h‚Ä≤‚ààI‚Ä≤
1
œÄœÉ2
‚àí1(h‚Ä≤)v‚ü®CBR(œÉ2),œÉ2‚ü©(h‚Ä≤)

/
X"
REFERENCES,0.7962085308056872,"h‚ààI1
œÄœÉ2
‚àí1(h)
(15)"
REFERENCES,0.7985781990521327,"By substituting the deÔ¨Ånition of CBV (I‚Ä≤
1) into the equation above,"
REFERENCES,0.8009478672985783,"CBV œÉ2(I1, a) =

X"
REFERENCES,0.8033175355450237,"I‚Ä≤
1‚ààsucc(I1,a)
CBV œÉ2(I‚Ä≤
1)
X"
REFERENCES,0.8056872037914692,"h‚Ä≤‚ààI‚Ä≤
1
œÄœÉ2
‚àí1(h‚Ä≤)

/
X"
REFERENCES,0.8080568720379147,"h‚ààI1
œÄœÉ2
‚àí1(h)
(16)"
REFERENCES,0.8104265402843602,"And by the induction hypothesis,"
REFERENCES,0.8127962085308057,"CBV œÉ2(I1, a) ‚â§

X"
REFERENCES,0.8151658767772512,"I‚Ä≤
1‚ààsucc(I1,a)
(CBV œÉ‚àó
2 (I‚Ä≤
1) + 2 ‚àíŒ±"
REFERENCES,0.8175355450236966,"1 ‚àíŒ±‚àÜ)
X"
REFERENCES,0.8199052132701422,"h‚Ä≤‚ààI‚Ä≤
1
œÄœÉ2
‚àí1(h‚Ä≤)

/
X"
REFERENCES,0.8222748815165877,"h‚ààI1
œÄœÉ2
‚àí1(h)
(17)"
REFERENCES,0.8246445497630331,"Because I1 is out of the subgame and œÉ‚àó
2, œÉ2 is exactly the same outside the subgame, we will get"
REFERENCES,0.8270142180094787,"CBV œÉ2(I1, a) ‚â§

X"
REFERENCES,0.8293838862559242,"I‚Ä≤
1‚ààsucc(I1,a)
(CBV œÉ‚àó
2 (I‚Ä≤
1) + 2 ‚àíŒ±"
REFERENCES,0.8317535545023697,"1 ‚àíŒ±‚àÜ)
X"
REFERENCES,0.8341232227488151,"h‚Ä≤‚ààI‚Ä≤
1
œÄœÉ‚àó
2
‚àí1(h‚Ä≤)

/
X"
REFERENCES,0.8364928909952607,"h‚ààI1
œÄœÉ‚àó
2
‚àí1(h)"
REFERENCES,0.8388625592417062,"= CBV œÉ‚àó
2 (I1, a) + 2 ‚àíŒ±"
REFERENCES,0.8412322274881516,"1 ‚àíŒ±‚àÜ

X"
REFERENCES,0.8436018957345972,"I‚Ä≤
1‚ààsucc(I1,a) X"
REFERENCES,0.8459715639810427,"h‚Ä≤‚ààI‚Ä≤
1
œÄœÉ‚àó
2
‚àí1(h‚Ä≤)

/
X"
REFERENCES,0.8483412322274881,"h‚ààI1
œÄœÉ‚àó
2
‚àí1(h)"
REFERENCES,0.8507109004739336,"= CBV œÉ‚àó
2 (I1, a) + 2 ‚àíŒ± 1 ‚àíŒ±‚àÜ (18)"
REFERENCES,0.8530805687203792,"Finally, by mathematical induction we get"
REFERENCES,0.8554502369668247,"exp(œÉ‚Ä≤
2) ‚â§exp(œÉ‚àó
2) +
2
1 ‚àí(2œÑ + 1)Œ±‚àÜ
(19)"
REFERENCES,0.8578199052132701,"A.2
PROOF OF THEOREM 2"
REFERENCES,0.8601895734597157,"Theorem 2. (opponent exploitation) Let œµ = ‚à•ÀÜp‚àíp‚à•1 be the L1 distance of the distribution p(Ii
1) and"
REFERENCES,0.8625592417061612,"ÀÜp(Ii
1). Let Œ∑ = minS‚ààS maxIj
1‚ààStop"
REFERENCES,0.8649289099526066,"
CBV1(Ij
1, œÉS
2 ) ‚àíCBV1(Ij
1, œÉ‚àó
2)

‚â•0. We use BR[S,œÉp]
p
(œÉ)
to denote the strategy for player p which maximizes its utility in subgame S ‚ààS against œÉ‚àíp under
the constraint that BR[S,œÉp]
p
(œÉ) and œÉp differs only inside S. By maximizing objective 2, for all S ‚ààS,
the reÔ¨Åned strategy œÉ‚Ä≤
2 satisÔ¨Åes u"
REFERENCES,0.8672985781990521,"D
BR[S,œÉ1]
1
(œÉ‚Ä≤
2),œÉ‚Ä≤
2
E"
REFERENCES,0.8696682464454977,"2
(S) ‚â•u"
REFERENCES,0.8720379146919431,"D
BR[S,œÉ1]
1
(œÉ‚àó
2),œÉ‚àó
2
E"
REFERENCES,0.8744075829383886,"2
(S) + 1 ‚àíŒ±"
REFERENCES,0.8767772511848341,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àíœµŒ∑
(20)"
REFERENCES,0.8791469194312796,Under review as a conference paper at ICLR 2022
REFERENCES,0.8815165876777251,"Proof: Still, we only consider a speciÔ¨Åc subgame S Ô¨Årst."
REFERENCES,0.8838862559241706,"œÉS
2 is maximizing"
REFERENCES,0.8862559241706162,"(1 ‚àíŒ±) min
Ij
1"
REFERENCES,0.8886255924170616,"
vœÉ
1 (Ij
1) ‚àíCBV œÉS
2 (Ij
1)
"
REFERENCES,0.8909952606635071,"|
{z
}
g(œÉS
2 ) +Œ±
X"
REFERENCES,0.8933649289099526,"i
ÀÜp(Ii
1)(vœÉ
1 (Ii
1) ‚àíCBV œÉS
2 (Ii
1))"
REFERENCES,0.8957345971563981,"|
{z
}
f(œÉS
2 ) (21)"
REFERENCES,0.8981042654028436,"So, we have"
REFERENCES,0.9004739336492891,"(1 ‚àíŒ±)g(œÉS
2 ) + Œ±f(œÉS
2 ) ‚â•(1 ‚àíŒ±)g(œÉ‚àó
2) + Œ±f(œÉ‚àó
2)
(22) and"
REFERENCES,0.9028436018957346,"max
Ij
1
CBV (Ij
1, œÉS
2 ) ‚àíCBV (Ij
1, œÉ‚àó
2) = Œ∑ ‚â•Œ∑"
REFERENCES,0.9052132701421801,"‚áîg(œÉS
2 ) ‚àí‚àÜ‚â§‚àíŒ∑"
REFERENCES,0.9075829383886256,"‚áîg(œÉS
2 ) ‚àí‚àÜ‚â§‚àÜ+ g(œÉ‚àó
2) ‚àíŒ∑
(g(œÉ‚àó
2) ‚â•‚àí‚àÜ) (23)"
REFERENCES,0.909952606635071,"Therefore,"
REFERENCES,0.9123222748815166,"Œ±f(œÉS
2 ) ‚â•Œ±f(œÉ‚àó
2) + (1 ‚àíŒ±)(Œ∑ ‚àí2‚àÜ)
(24)"
REFERENCES,0.9146919431279621,"which means
X"
REFERENCES,0.9170616113744076,"i
ÀÜp(Ii
1)(CBV œÉ‚àó
2 (I1) ‚àíCBV œÉS
2 (Ii
1)) ‚â•1 ‚àíŒ±"
REFERENCES,0.919431279620853,"Œ±
(Œ∑ ‚àí2‚àÜ) +
X"
REFERENCES,0.9218009478672986,"i
ÀÜp(Ii
1)(CBV œÉ‚àó
2 (I1) ‚àíCBV œÉ‚àó
2 (Ii
1)) ‚áî‚àí
X"
REFERENCES,0.9241706161137441,"i
ÀÜp(Ii
1)CBV œÉS
2 (Ii
1) ‚â•1 ‚àíŒ±"
REFERENCES,0.9265402843601895,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àí
X"
REFERENCES,0.9289099526066351,"i
ÀÜp(Ii
1)CBV œÉ‚àó
2 (Ii
1) ‚áî‚àí
X"
REFERENCES,0.9312796208530806,"i
p(Ii
1)CBV œÉS
2 (Ii
1) ‚â•1 ‚àíŒ±"
REFERENCES,0.933649289099526,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àí
X"
REFERENCES,0.9360189573459715,"i
p(Ii
1)CBV œÉ‚àó
2 (Ii
1) ‚àí
X"
REFERENCES,0.9383886255924171,"i
(p(Ii
1) ‚àíÀÜp(Ii
1))(CBV œÉS
2 (Ii
1) ‚àíCBV œÉ‚àó
2 (Ii
1)) ‚áí‚àí
X"
REFERENCES,0.9407582938388626,"i
p(Ii
1)CBV œÉS
2 (Ii
1) ‚â•1 ‚àíŒ±"
REFERENCES,0.943127962085308,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àí
X"
REFERENCES,0.9454976303317536,"i
p(Ii
1)CBV œÉ‚àó
2 (Ii
1) ‚àíœµŒ∑ ‚áî
X"
REFERENCES,0.9478672985781991,"i
p(Ii
1)V2(Ii
1, BR(œÉS
2 ), œÉS
2 ) ‚â•1 ‚àíŒ±"
REFERENCES,0.9502369668246445,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àíœµŒ∑ +
X"
REFERENCES,0.95260663507109,"i
p(Ii
1)V2(Ii
1, BR(œÉ‚àó
2), œÉ‚àó
2)"
REFERENCES,0.9549763033175356,"‚áîu
‚ü®BR[S,œÉ1]
1
(œÉ
[S‚ÜêœÉS
2 ]
2
),œÉ
[S‚ÜêœÉS
2 ]
2
‚ü©
2
(S) ‚â•u
‚ü®BR[S,œÉ1]
1
(œÉ‚àó
2),œÉ‚àó
2‚ü©
2
(S) + 1 ‚àíŒ±"
REFERENCES,0.957345971563981,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àíœµŒ∑ (25)"
REFERENCES,0.9597156398104265,"Since Œ∑ is deÔ¨Åned as minimum over all subgames, we have u"
REFERENCES,0.9620853080568721,"D
BR[S,œÉ1]
1
(œÉ‚Ä≤
2),œÉ‚Ä≤
2
E"
REFERENCES,0.9644549763033176,"2
(S) ‚â•u"
REFERENCES,0.966824644549763,"D
BR[S,œÉ1]
1
(œÉ‚àó
2),œÉ‚àó
2
E"
REFERENCES,0.9691943127962085,"2
(S) + 1 ‚àíŒ±"
REFERENCES,0.9715639810426541,"Œ±
(Œ∑ ‚àí2‚àÜ) ‚àíœµŒ∑
(26)"
REFERENCES,0.9739336492890995,"B
POKER RULES"
REFERENCES,0.976303317535545,RULES OF LEDUC POKER
REFERENCES,0.9786729857819905,"Leduc Poker is a two players game. In Leduc Poker, there are 6 cards in total, three ranks({J, Q, K})
with two suits({a, b}) each. And at the beginning, every player should put 1 chip into the pot and
then will be dealt with one private card. Then, two players alternatively bet. They can call, raise and
fold. If any of them fold, the game ends and all chips in the pot belongs to the other player. And"
REFERENCES,0.981042654028436,Under review as a conference paper at ICLR 2022
REFERENCES,0.9834123222748815,"when a player call, he has to put chips in the pot to ensure that he contributes equal chips as the other
player in the pot. When a player raise, he has to ensure that he contributes more chips than the other
player in the pot. A betting round ends when a player calls."
REFERENCES,0.985781990521327,"Leduc Poker is divided into two betting rounds. In the Ô¨Årst round, a private card is dealt to each
player and then two player start to bet. After the Ô¨Årst betting round ends and nobody folds, a public
card is dealt on board and the second betting round starts. When the second round ends, both of the
player show their private hands and the stronger hands win. If a player‚Äôs private card has the same
rank as the public card, then he wins. Otherwise, we have J < Q < K and the higher one wins.
And in each betting round, there will be at most two raises in our experiment and each raise should
contribute 1 more chip in the Ô¨Årst round and 2 more chips in the second round."
REFERENCES,0.9881516587677726,RULES OF FLOP HOLD‚ÄôEM POKER
REFERENCES,0.990521327014218,"The rules of Flop Hold‚Äôem Poker is similar to that of Leduc Poker. In FHP, we use the standard
52-card deck. At the beginning, the Ô¨Årst player will contribute 1 chip to the pot and the second player
will contribute 2 chips. And then they will be dealt with 2 private cards each and the Ô¨Årst player start
to bet. There are still two betting rounds and the raise sizes are both 2 chips. At the end of the Ô¨Årst
betting round, there will be 3 public cards dealt on board. And the players will show their private
card at the end of the second betting round and the larger one wins the game. In FHP, we have the
same rule of card order as a standard Texas hold ‚Äôem ."
REFERENCES,0.9928909952606635,"C
IMPLEMENTATION DETAILS"
REFERENCES,0.995260663507109,"Leduc Poker. In Leduc Poker, we solve for a blueprint strategy using a variant of CFR algorithm
(Lanctot et al., 2009; Tammelin et al., 2015) with 1M iterations in the full game. Then we apply
search in subgames when the board card is dealt."
REFERENCES,0.9976303317535545,"Flop Hold‚Äôem Poker (FHP). As for FHP, there are 1,286,792 different infosets for each betting
sequence. We cluster them into 200 infosets by an abstraction algorithm (Johanson et al., 2013) in
order to make equilibrium Ô¨Ånding feasible. Then, we compute a blueprint strategy in this abstraction
with 10,000,000 iterations. We apply search immediately once the Ô¨Çop cards are dealt."
