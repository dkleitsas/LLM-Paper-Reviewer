Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002531645569620253,"A dynamical system of spiking neurons with only feedforward connections can
classify spatiotemporal patterns without recurrent connections. However, the the-
oretical construct of a feedforward spiking neural network (SNN) for approximat-
ing a temporal sequence remains unclear, making it challenging to optimize SNN
architectures for learning complex spatiotemporal patterns. In this work, we estab-
lish a theoretical framework to understand and improve sequence approximation
using a feedforward SNN. Our framework shows that a feedforward SNN with
one neuron per layer and skip-layer connections can approximate the mapping
function between any arbitrary pairs of input and output spike train on a compact
domain. Moreover, we prove that heterogeneous neurons with varying dynam-
ics and skip-layer connections improve sequence approximation using feedfor-
ward SNN. Consequently, we propose SNN architectures incorporating the pre-
ceding constructs that are trained using supervised backpropagation-through-time
(BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms
for classification of spatiotemporal data. A dual-search-space Bayesian optimiza-
tion method is developed to optimize architecture and parameters of the proposed
SNN with heterogeneous neuron dynamics and skip-layer connections."
INTRODUCTION,0.005063291139240506,"1
INTRODUCTION"
INTRODUCTION,0.007594936708860759,"Spiking neural network (SNN) (Ponulak & Kasinski, 2011) uses biologically inspired neurons
and synaptic connections trainable with either biological learning rules such as spike-timing-
dependent plasticity (STDP) (Gerstner & Kistler, 2002) or statistical training algorithms such as
backpropagation-through-time (BPTT) (Werbos, 1990). The SNNs with simple leaky integrate-and-
fire (LIF) neurons and supervised training have shown classification performance similar to deep
neural networks (DNN) while being energy efficient (Kim et al., 2020b; Wu et al., 2019; Srinivasan
& Roy, 2019). One of SNN’s main difference from DNN is that the neurons are dynamical sys-
tems with internal states evolving over time, making it possible for SNN to learn temporal patterns
without recurrent connections. Empirical results on feedforward-only SNN models show good per-
formance for spatiotemporal data classification, using either supervised training (Lee et al., 2016;
Kaiser et al., 2020; Khoei et al., 2020), or unsupervised learning (She et al., 2021). However, while
empirical results are promising, a lack of theoretical understanding of sequence approximation using
SNN makes it challenging to optimize performance on complex spatiotemporal datasets."
INTRODUCTION,0.010126582278481013,"In this work, we develop a theoretical framework for analyzing and improving sequence approxi-
mation using feedforward SNN. We view a feedforward connection of spiking neurons as a spike
propagation path, hereafter referred to as a memory pathways (She et al., 2021), that maps an input
spike train with an arbitrary frequency to an output spike train with a target frequency. Conse-
quently, we argue that an SNN with many memory pathways can approximate a temporal sequence
of spike trains with time-varying unknown frequencies using a series of pre-defined output spike
trains with known frequencies. Our theoretical framework aims to first establish SNN’s ability to
map frequencies of input/output spike trains within arbitrarily small error; and next, derive the basic
principles for adapting neuron dynamics and SNN architecture to improve sequence approximation."
INTRODUCTION,0.012658227848101266,Published as a conference paper at ICLR 2022
INTRODUCTION,0.015189873417721518,"The theoretical derivations are then investigated with experimental studies on feedforward SNN for
spatiotemporal classifications. We adopt the basic design principles for improving sequence approx-
imation to optimize SNN architectures and study whether these networks can be trained to improve
performance for spatiotemporal classification tasks. The key contributions of this work are:"
INTRODUCTION,0.017721518987341773,"• We prove that any spike-sequence-to-spike-sequence mapping functions on a compact do-
main can be approximated by feedforward SNN with one neuron per layer using skip-layer
connections, which cannot be achieved if no skip-layer connection is used."
INTRODUCTION,0.020253164556962026,"• We prove that using heterogeneous neurons having different dynamics and skip-layer con-
nection increases the number of memory pathways a feedforward SNN can achieve and
hence, improves SNN’s capability to represent arbitrary sequences."
INTRODUCTION,0.02278481012658228,"• We develop complex SNN architectures using the preceding theoretical observations and
experimentally demonstrate that they can be trained with supervised BPTT and unsuper-
vised STDP for spatiotemporal data classification."
INTRODUCTION,0.02531645569620253,"• We design a dual-search-space option for Bayesian optimization process to sequentially op-
timize network architectures and neuron dynamics of a feedforward SNN considering het-
erogeneity and skip-layer connection to improve learning and classification of spatiotem-
poral patterns."
INTRODUCTION,0.027848101265822784,"We experimentally demonstrate that our network design principles coupled with the dual-search-
space Bayesian optimization improve classification performance on DVS Gesture (Amir et al.,
2017), N-caltech (Orchard et al., 2015), and sequential MNIST. Results show that the design prin-
ciples derived using our theoretical framework for sequence approximation can improve spatiotem-
poral classification performance of SNN."
RELATED WORK,0.030379746835443037,"2
RELATED WORK"
RELATED WORK,0.03291139240506329,"Prior theoretical approaches to analyze SNN often focus on the storage and retrieval of precise spike
patterns (Amit & Huang, 2010; Brea et al., 2013). There are also works that consider SNN for
solving optimization problems (Chou et al., 2018; Binas et al., 2016) and works that analyze the
dynamics of SNN (Zhang et al., 2019; Barrett et al., 2013). Those are different topics from the
approximation of spike-sequence-to-spike-sequence mappings functions. SNN that incorporates ex-
citatory and inhibitory signal is shown for its ability to emulate sigmoidal networks (Maass, 1997)
and is theoretically capable of universal function approximation. Feedforward SNN with specially
designed spiking neuron models (Iannella & Back, 2001; Torikai et al., 2008) have been demon-
strated for function approximation, while for networks using LIF neurons, function approximation
has been shown with only empirical results (Farsa et al., 2015). On the other hand, existing works
that have developed efficient training process for SNN and demonstrated classification performance
comparable to deep learning models, have mostly used simpler and generic LIF neuron models (Lee
et al., 2016; Kaiser et al., 2020; Kim et al., 2020b; Wu et al., 2019; Sengupta et al., 2019; Safa et al.,
2021; Han et al., 2020). Therefore, this paper develops the theoretical basis for function approxima-
tion using feedforward SNN with LIF neurons, and studies applications of the developed theoretical
constructs in improving SNN-based spatiotemporal pattern classification."
RELATED WORK,0.035443037974683546,"The effectiveness of heterogeneous neurons (She et al., 2021) and skip-layer connections (Srinivasan
& Roy, 2019; Sengupta et al., 2019) in SNN has been empirically studied in the past. However, no
theoretical approach has been presented to understand why such methods improve learning of spike
sequences, and how to optimize SNN’s architecture and parameters to effectively exploit these de-
sign constructs. It is possible to search for the optimal SNN configurations through optimization
algorithms, but the large amount of hyper-parameters for spiking neurons and network structure cre-
ates a high-dimensional search space that is long and difficult to solve. Bayesian optimization (Snoek
et al., 2012) uses collected data points to make decision on the next test point that could provide im-
provement, thus accelerates the optimization process. Prior works (Parsa et al., 2019; Kim et al.,
2020a) have shown that SNN performance can be effectively improved with Bayesian optimization.
While those works consider a single or a few neuron parameters, the dual-search-space Bayesian
optimization proposed in this work optimizes both network architecture and neuron parameters effi-
ciently by separating the discrete search spaces from the continuous search spaces."
RELATED WORK,0.0379746835443038,Published as a conference paper at ICLR 2022 (a)
RELATED WORK,0.04050632911392405,"Input spikes n1
n3 n2
n4"
RELATED WORK,0.043037974683544304,n2 spikes v
RELATED WORK,0.04556962025316456,"n1 spikes
γ = 3 t n1 n2 n3 n4"
RELATED WORK,0.04810126582278481,n4 spikes
RELATED WORK,0.05063291139240506,"n3 spikes tnd …
… … …"
RELATED WORK,0.053164556962025315,"…
Neuron Dynamic d1"
RELATED WORK,0.05569620253164557,"One neuron 
per layer per"
RELATED WORK,0.05822784810126582,dynamic
RELATED WORK,0.060759493670886074,Layer number: m (b)
RELATED WORK,0.06329113924050633,Neuron Dynamic dn … …
RELATED WORK,0.06582278481012659,"Figure 1: (a) A time-varying input spike sequence received by two memory pathways: neuron
membrane potential plots show the different response from the neurons to the given input. (b) A
minimal multi-neuron-dynamic (mMND) network with m layers and n neuron dynamics."
APPROXIMATION THEORY OF FEEDFORWARD SNN,0.06835443037974684,"3
APPROXIMATION THEORY OF FEEDFORWARD SNN"
DEFINITIONS AND NOTATIONS,0.07088607594936709,"3.1
DEFINITIONS AND NOTATIONS"
DEFINITIONS AND NOTATIONS,0.07341772151898734,"Definition 1 Neuron Response Rate γ For a spiking neuron n with membrane potential at vreset and
input spike sequence with period tin, γ is the number of input spike n needs to reach vth."
DEFINITIONS AND NOTATIONS,0.0759493670886076,"Definition 2 Memory Pathways For a feedforward SNN with m layers, a memory pathway is defined
as a spike propagation path from input to the output layer. Two memory pathways are considered
distinct if the set of neurons contained in them is different."
DEFINITIONS AND NOTATIONS,0.07848101265822785,"Definition 3 Minimal Multi-neuron-dynamic (mMND) Network A densely connected network in
which each layer has an arbitrary number of neurons that have different neuron parameters. All
synapses from one pre-synaptic neuron have the same synaptic conductance."
DEFINITIONS AND NOTATIONS,0.0810126582278481,"Notations Neuron Delay tnd is the time for a spike from pre-synaptic neuron to arrive at its post-
synaptic neurons, as shown in Figure 1(a). For a feedforward SNN with m layers, a skip-layer
connection can be defined with source layer and target layer pair (ls, lt). The output feature map
from source layer is concatenated to the original input feature map of the target layer. For the
analysis of spike sequence in temporal space, the notation of Tmax and Tmin are defined as positive
real numbers such that Tmax > Tmin. ϵ > 0 is the error of approximation."
DEFINITIONS AND NOTATIONS,0.08354430379746836,"Figure 1(a) shows two memory pathways receiving an input spike sequence with time-varying pe-
riods. As the neurons have different dynamics, the two memory pathways have different response
to the input spike sequence. An example of mMND network with m layers and n neuron dynam-
ics is shown in Figure 1(b). SNN with multilayer perceptron (MLP) structure can be considered a
scaled-up mMND network with multiple neurons for each dynamic. A network with convolutional
structure can be considered a scaled-up mMND network with duplicated connections in each layer.
We analyze the correlation of network capacity and structure based on mMND networks. The de-
sign of neuron heterogeneity can also be implemented in MLP-SNN and Conv-SNN as described in
Section 4. The analysis for network capacity can be extended to those networks according to their
specific layer dimensions."
MODELING OF SPIKING NEURON,0.08607594936708861,"3.2
MODELING OF SPIKING NEURON"
MODELING OF SPIKING NEURON,0.08860759493670886,"SNN consists of spiking neurons connected by synapses. The spiking neuron model studied in this
work is leaky integrate-and-fire (LIF) as defined by the following equations: τm
dv"
MODELING OF SPIKING NEURON,0.09113924050632911,"dt = a + RmI −v; v = vreset, if v > vthreshold
(1)"
MODELING OF SPIKING NEURON,0.09367088607594937,Published as a conference paper at ICLR 2022
MODELING OF SPIKING NEURON,0.09620253164556962,"Rm is membrane resistance, τm = RmCm is time constant and Cm is membrane capacitance. a
is the resting potential. I is the sum of current from all input synapses that connect to the neuron.
A spike is generated when membrane potential v cross threshold and the neuron enters refractory
period r, during which the neuron maintains its membrane potential at vreset. The time it takes for
a pre-synaptic neuron to send a spike to its post-synaptic neurons is tnd. Neuron response rate γ is
a property of a spiking neuron’s response to certain input spike sequence. We show how the value
of γ can be evaluated below."
MODELING OF SPIKING NEURON,0.09873417721518987,"Remark For any input spike sequence, each individual spike can be described with Dirac delta
function δ(t −ti) where ti is the time of the i-th input spike. Consider membrane potential of a
spiking neuron receiving the input before reaching spiking threshold, with initial state at t = 0 with
v = vreset, solving the differential equation (1) leads to (Gerstner, 1995):"
MODELING OF SPIKING NEURON,0.10126582278481013,"v(t) = vresete−
t
τm + a(1 −e−
t
τm ) + Rm"
MODELING OF SPIKING NEURON,0.10379746835443038,"τm
e−
t
τm
X"
MODELING OF SPIKING NEURON,0.10632911392405063,"i
G
Z t"
MODELING OF SPIKING NEURON,0.10886075949367088,"0
δ(t −ti)e
t
τm dt
(2)"
MODELING OF SPIKING NEURON,0.11139240506329114,"Here, G is the conductance of input synapse connected to the neuron, which is trainable. From (2),
there exists a value of u such that vm(tu−1) < vthreshold and vm(tu) >= vthreshold. By evaluating
(2) for u given neuron parameters and input spike sequence, the neuron response rate γ can be found."
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.11392405063291139,"3.3
APPROXIMATION THEOREM OF FEEDFORWARD SNN"
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.11645569620253164,"To develop the approximation theorem for feedforward SNN, we first aim to understand the range of
neuron response rate that can be achieved. We show with Lemma 1 that for any input spike sequence
with periods in a closed interval, it is possible to set the neuron response rate γ to any positive
integer. Based on this property, we show with Theorem 1 that by connecting a list of spiking neurons
with certain γ sequentially and inserting skip-layer connections, network with spike period mapping
function P(t) can be achieved to approximate the target spike sequence. To understand whether
this capability of feedforward SNN relies on skip-layer connections, we develop Lemma 2 to prove
that skip-layer connections are indeed necessary. In subsection 3.4 we investigate the correlation
between approximation capability and network structures by analyzing the cutoff property of spiking
neurons, which can change the network’s connectivity. In our analysis, we focus on two particular
designs: heterogeneous network (Lemma 4) and skip-layer connection (Lemma 5), and show their
impact on the number of distinct memory pathways in a network. All lemmas are formally proved
in the appendix."
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.1189873417721519,"Lemma 1 For any input spike sequence with period tin in range [Tmin, Tmax], there exists a spik-
ing neuron n with fixed parameters vth, vreset, a, Rm and τm, such that by changing synaptic
conductance G, it is possible to set the neuron response rate γn to be any positive integer."
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.12151898734177215,"Proof Sketch. (Formal proof in Appendix A) Given an input spike sequence, the highest possible
membrane potential decay ∆v for any input tin ∈[Tmin, Tmax] can be derived as a function of
neuron parameters. We show it is possible to make ∆v tend to zero by setting the neuron parameters.
Since the decay of v can be negligible, γn can be set to any positive integer by changing G."
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.1240506329113924,"Theorem 1 For any input and target output spike sequence pair with periods (tin, tout) ∈
[Tmin, Tmax]×[Tmin, Tmax], there exists a minimal-layer-size network with skip-layer connections
that has memory pathway with output spike period function P(t) such that |P(tin) −tout| < ϵ."
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.12658227848101267,"Proof Sketch. (Formal proof in Appendix B) With skip-layer connections, there can be multiple
memory pathways in a minimal-layer-size network as neurons can be either included or skipped
through. Hence it is possible to create memory pathways with different delay times for each input
spike in a network. By connecting the output of those memory pathways to a common neuron n′,
spike sequence of any arbitrary period tint such that tint <= tin can be generated within ϵ. By
setting γn′ > 1, the output from n′ receiving input spike sequence with tint is tn′
out = γ ·tint. Hence
it is possible to achieve a network with output spike period P(t) such that |P(tin) −tout| < ϵ."
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.1291139240506329,"Lemma 2 With no skip-layer connection, there does not exist a minimal-layer-size network that has
output spike period function P(t) such that for any input and target output spike sequence pair with
periods (tin, tout) ∈[Tmin, Tmax] × [Tmin, Tmax], |P(tin) −tout| < ϵ."
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.13164556962025317,Published as a conference paper at ICLR 2022
APPROXIMATION THEOREM OF FEEDFORWARD SNN,0.1341772151898734,"Proof Sketch. (Formal proof in Appendix C) A minimal-layer-size network without skip-layer con-
nection has only one memory pathway. For a particular input spike sequence with period tin, dif-
ferent output period P(tin) can be achieved by changing γ of neurons in the memory pathway. We
show that there exists two output spike periods P(tin) and P ′(tin), such that P(tin) −P ′(tin) is a
constant value independent of network or neuron configurations, and there can be no P ′′(tin) such
that P(tin) < P ′′(tin) < P ′(tin). Therefore, for any minimal-layer-size network, there exists tout
within the range of (P(tin), P ′(tin)) such that |P(tin) −tout| < ϵ does not hold true."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.13670886075949368,"3.4
NETWORK STRUCTURE AND MEMORY PATHWAYS"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.13924050632911392,"Based on Theorem 1, it is possible to approximate an input/output spike sequence mapping func-
tion using a minimal-layer-size network with specific configuration, which can be considered as a
memory pathway. Since any bounded continuous function on a compact domain can be approxi-
mated to arbitrary accuracy using a piece-wise constant function, and it is possible to use a memory
pathway to approximate each of the constant function, with increasing number of distinct memory
pathways, a feedforward SNN can achieve approximation of continuous functions with less error.
In this subsection, we show that with two structural designs: heterogeneous network i.e. a network
having neurons with different dynamics and adding skip-layer connections, a feedforward SNN has
the capability to achieve more distinct memory pathways."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.14177215189873418,"Cutoff Frequency of a Memory Path
We first show the correlation of cutoff period and spiking
neuron parameters with Lemma 3, which is proved in Appendix D."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.14430379746835442,"Lemma 3 A spiking neuron has cutoff period ωc = τm ln(
vreset−a
vreset−a+ Rm"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.1468354430379747,τm G) above which input spike
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.14936708860759493,sequence cannot cause the spiking neuron to spike.
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.1518987341772152,"Remark From Lemma 3, it can be observed that the cutoff period ωc of a neuron can be configured
to any positive real number by changing the neuron parameters and synaptic conductance G. Further,
with fixed G, ωc can be configured to any positive real number by changing the neuron parameters.
Neurons that are in cutoff change the spike propagation path in a network as they send no output
spikes. This creates different memory pathways without changing the connections in a network."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.15443037974683543,"Heterogeneous Network
If an mMND network has the same parameters for all neurons in each
layer, the majority of the neurons are included in the same memory pathway, leading to the upper
bound of number of distinct memory pathways to be limited. With Lemma 4, we show the rela-
tionship between the upper bound of the number of distinct memory pathways and the number of
different neuron dynamics in an mMND network."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.1569620253164557,"Lemma 4 For an mMND network with m layers and {λ1, λ2, ...λm} number of different neuron
dynamics in each layer, the least upper bound of the number of distinct memory pathways is Qm
i=1 λi."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.15949367088607594,"Proof Sketch. (Formal proof in Appendix D) For an mMND network, it is possible to have neurons
with different ωc in each layer, which creates λi number of different neuron activation states for
layer i. Across all network layers, the highest possible number of different neuron activation states
is therefore the product of λ of each layer. Since neurons in cutoff do not propagate spikes, they can
be removed from a memory pathway. This leads to Qm
i=1 λi as the least upper bound of the number
of memory pathways."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.1620253164556962,"Compared to a network with homogeneous neuron parameters, in which the upper bound of number
of distinct memory pathways is λm, Lemma 4 indicates that heterogeneous network increases the
maximum achievable number of distinct memory pathways in a feedforward SNN."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.16455696202531644,"Skip-layer Connection
We show that adding skip-layer connection increases the upper bound of
the number of memory pathways in a network with Lemma 5."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.1670886075949367,"Lemma 5 For an mMND network with m layers and {λ1, λ2, ...λm} different neuron dynamics in
each layer and a skip-layer connection made between layer la and lb, s.t. a, b ∈{1, 2, ...m} and
(b −a) > 1, the least upper bound of the number of memory pathways is Qm
i=1 λi + (Qa
i=1 λi ·
Qm
i=b λi)"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.16962025316455695,Published as a conference paper at ICLR 2022 …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.17215189873417722,Multi-neuron-dynamic Layers Input
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.17468354430379746,Linear
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.17721518987341772,Skip-layer connection …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.17974683544303796,Dynamic 1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.18227848101265823,Heterogeneous
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.1848101265822785,neuron dynamic
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.18734177215189873,"Source 
layer"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.189873417721519,Target layer …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.19240506329113924,Dynamic m
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.1949367088607595,Dynamic m-1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.19746835443037974,Dynamic 1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2,Dynamic m
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.20253164556962025,Dynamic m-1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.20506329113924052,Transferred
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.20759493670886076,Synapse
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.21012658227848102,"Learned 
Synapse"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.21265822784810126,(a) BPTT Training
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.21518987341772153,Memory
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.21772151898734177,Module …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.22025316455696203,"Learner 
Module"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.22278481012658227,"Input
Linear"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.22531645569620254,"Skip-layer 
connection"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.22784810126582278,Multi-neuron-dynamic Layers …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.23037974683544304,"Learner 
Module"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.23291139240506328,Heterogeneous
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.23544303797468355,neuron dynamic …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2379746835443038,Memory
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.24050632911392406,Module d1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2430379746835443,"Learner 
Module"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.24556962025316456,Transferred
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2481012658227848,Synapse
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.25063291139240507,"Learned 
Synapse"
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.25316455696202533,Memory
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.25569620253164554,Module dm
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2582278481012658,Memory
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2607594936708861,Module d1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.26329113924050634,Memory
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.26582278481012656,Module dm
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2683544303797468,(b) STDP Training … …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2708860759493671,Target Layer …
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.27341772151898736,Dynamic 1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2759493670886076,Dynamic m
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.27848101265822783,Dynamic 1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2810126582278481,Dynamic m
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.28354430379746837,Dynamic 1
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.28607594936708863,Dynamic m
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.28860759493670884,Source Layer
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2911392405063291,"Figure 2: (a) The proposed network with BPTT training, each multi-neuron-dynamic layer contains
a set of neuron dynamics from d1 to dm. (b) The proposed network with STDP training."
NETWORK STRUCTURE AND MEMORY PATHWAYS,0.2936708860759494,"Proof Sketch. (Formal proof in Appendix D) Compared to the network considered in Lemma 4,
by adding a skip-layer connection, there are additional possible neuron activation states in the net-
work that result from the cutoff of neurons in layers between la and lb. Without layers between
la and lb in the spike propagation path, the least upper bound of the number of memory pathways
is increased by the maximum number of distinct memory pathways in a network that has layers
{l1, l2, ..., la, lb, lb+1, ..., lm} connected sequentially."
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.29620253164556964,"4
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY"
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.29873417721518986,"In this section, we discuss design of SNN architectures as inspired by the developed approximation
theory for feedforward SNN, with more details in Appendix G."
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.3012658227848101,"Network Template for BPTT Training For BPTT training, the network template is shown in Fig-
ure 2(a). The heterogeneity in neuron dynamics is implemented by using the multi-neuron-dynamic
layers, which can either be convolutional or fully connected. The multi-neuron-dynamic layers use
different neuron parameters for spiking neurons in each neuron dynamic module, which contains a
certain number of feature maps for convolutional layers or a certain number of neurons for fully-
connected layers. There are two types of synapses between layers: transferred synapses marked as
black dashed arrows and learned synapses marked as red solid arrows. The conductance of learned
synapses is optimized by the BPTT algorithm during training, and the transferred synapses have
the same conductance as the learned synapses from the same pre-synaptic neuron. During forward
pass, neurons in each layer receive the same input features and respond differently based on their
neuron dynamics to generate different output features. During back-propagation, only conductance
of the learned synapses are updated. Skip-layer connection is implemented with the output spike
matrix from source layer concatenated to the original input spike matrix of the target layer. The
skip-layer connection has the same implementation as the regular connection between consecutive
layers, with both learned and transferred synapses (Figure 2(a)). The last layer of the network is a
fully-connected layer with homogeneous dynamic to generate prediction labels."
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.3037974683544304,"Network Template for STDP Learning For networks trained with STDP, the template is shown
in Figure 2(b). Each layer contains a learner module and a memory module. Learner modules use
homogeneous neuron dynamic that is suitable for STDP learning, and memory modules consist of
neurons with different dynamics. There are also two types of synapses: transferred synapses and
learned synapses. Between two layers, memory modules are connected with transferred synapses
and memory modules are connected to learner modules with learned synapses. Leaner modules be-"
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.30632911392405066,Published as a conference paper at ICLR 2022
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.30886075949367087,"tween layers are not directly connected. STDP training proceeds as a layer-by-layer process. During
training of the first layer, conductance of synapses connecting neurons in memory module to neu-
rons in learner module is learned with STDP using all training data without supervision. Then, the
learned conductance is transferred to the transferred synapses in the same layer. During training of
the second layer, layer 1 memory module perceives the same input features and generates different
output features with the heterogeneous neurons. This lay-by-layer process is repeated until the layer
before the final layer finishes learning. The final linear layer is then fine-tuned using stochastic
gradient descent (SGD) based on spike frequency array from the last multi-neuron-dynamic layer
generated based on the labeled data. Skip-layer connection is implemented by connecting the mem-
ory module of the source layer to the target layer. The connections are made with the two types of
synapses and follow the same training process as the consecutive layers."
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.31139240506329113,"Dual-search-space Bayesian Optimization Bayesian optimization uses Gaussian process to model
the distribution of an objective function, and an acquisition function to decide points to evaluate.
For data points in a target dataset x ∈X and the corresponding label y ∈Y , an SNN with network
structure V and neuron parameters W acts as a function fV,W(x) that maps input data x to predicted
label ˜y. The optimization problem in this work is defined as"
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.3139240506329114,"minV,WP where P =
X"
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.31645569620253167,"x∈X,y∈Y
L(y, fV,W(x))
(3)"
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.3189873417721519,"V contains the number of layers Nlayers, the number of memory dynamics Ndynmaic and skip-
layer connection configuration variables Nskip, Lstart and Lend, each controlling the number of
skip-layer connections, the first layer and last layer to implement skip-layer connections. All of the
values are discrete. W contains the values for a, τm and Rm in (1), which are continuous. We
separate the discrete and continuous search spaces by implementing a dual-search-space optimiza-
tion process, where V is first optimized with fixed, manually tuned neuron parameters. After an
optimal structure is found, W are optimized for the selected V. Details on the configurations of the
optimization process are listed in the appendix. To achieve Bayesian optimization with constraints,
we implement a modified expected improvement (EI) acquisition function similar to the one shown
by Gardner (Gardner et al., 2014), which uses a Gaussian process to model the feasibility indicator
due to its high evaluation cost. In this work, since the constraint function can be explicitly defined,
we use a feasibility indicator that is directly evaluated. The modified EI function is defined as:"
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.32151898734177214,"Ic(W) = ∆(W) · max{0, P(W) −P(W+)}
(4)"
SNN ARCHITECTURE DESIGN USING APPROXIMATION THEORY,0.3240506329113924,"where W is the network configuration containing W and V. W+ is the test point that provided
the best result. ∆(W) is the explicitly defined indicator function that takes the value of 1 when all
constraints are satisfied and 0 otherwise."
EXPERIMENTS,0.3265822784810127,"5
EXPERIMENTS"
EXPERIMENT SETTINGS,0.3291139240506329,"5.1
EXPERIMENT SETTINGS"
EXPERIMENT SETTINGS,0.33164556962025316,"Datasets tested in the experiment include the DVS Gesture (Amir et al., 2017), which is a human
gesture dataset captured by DVS cameras, and the N-Caltech101 (Orchard et al., 2015), which is an
event-based version of the Caltech101 dataset. The proposed method is also tested for MLP-style
SNN on the sequential MNIST dataset presented row-by-row. We also vary the amount of labeled
data used during training ranging from using 100% labeled data for training down to 10% labeled
data (30% for N-Caltech101) during training. Note, during STDP training networks always use the
entire but un-labeled training dataset; however, only the fraction of the labeled data is used for su-
pervised fine-tuning of the last layer. Comparison is made for DVS Gesture and N-Caltech101 with
prior works including ConvLSNN, which is a combination of convolutional SNN and recurrent SNN
with long and short-term neurons trained with BPTT (Salaj et al., 2020), DECOLLE (Kaiser et al.,
2020), which uses surrogate gradient to train a convolutional feedforward SNN, HATS (Sironi et al.,
2018), which implements time surfaces and SVM for classification and H-SNN (She et al., 2021)
which uses STDP to train a convolutional SNN with two neuron dynamics. Additional function
approximation experiments are discussed in Appendix E."
EXPERIMENT SETTINGS,0.3341772151898734,Published as a conference paper at ICLR 2022                    
EXPERIMENT SETTINGS,0.3367088607594937,"Figure 3: Validation error over optimization iterations for the proposed dual-search-space Bayesian
optimization compared to the normal single-search-space Bayesian optimization."
EFFECT OF DUAL-SEARCH-SPACE BAYESIAN OPTIMIZATION,0.3392405063291139,"5.2
EFFECT OF DUAL-SEARCH-SPACE BAYESIAN OPTIMIZATION"
EFFECT OF DUAL-SEARCH-SPACE BAYESIAN OPTIMIZATION,0.34177215189873417,"We compare the proposed dual-search-space Bayesian optimization with regular Bayesian optimiza-
tion using a single search space for network validation error over 5 runs. The result from the N-
Caltech101 dataset is shown in Figure 3. It can be observed that the two optimization approaches
achieve similar minimum validation error after convergence. By separating the search spaces, the
proposed optimization process reaches convergence faster than regular single-search-space opti-
mization. It is also worth noting that, between the two stages in the optimization process for BPTT
training, the first stage accounts for more reduction in validation error than the second stage. This
indicates that optimizing network structure causes more impact to BPTT training than optimizing
neuron parameters, which is potentially due to the reason that network structure more heavily af-
fects the number of memory pathways in the network than neuron parameters. On the other hand, for
STDP training where learning behavior is sensitive to the dynamic of spiking neurons, the reduction
of validation error is more equally shared between the two optimization stages. Over the 5 runs,
among all network configurations achieved after the dual-search-space optimization converges, we
compare the configuration with the lowest number of trainable parameters against baseline models.
The specific configurations for the optimized networks are listed in Table 1. It can be observed that
for BPTT algorithm, the optimized networks have more layers than the STDP trained networks, and
the optimal values found for neuron parameters are highly distinct for the two training methods."
EFFECT OF DUAL-SEARCH-SPACE BAYESIAN OPTIMIZATION,0.34430379746835443,Table 1: Configuration of optimized network models
EFFECT OF DUAL-SEARCH-SPACE BAYESIAN OPTIMIZATION,0.3468354430379747,"Conv. Layer
Skip-layer
Number of Different
Neuron Parameters
Network
Number
Connection
Neuron Dynamics and a
τm
Rm
BPTT, Gesture
9
(2,7)
4, (-24,-17,-12,-9)
120
340
BPTT, N-Caltech
12
(2,5), (5,8), (8,11)
5, (-23,-16,-14,-11,-8)
70
300"
EFFECT OF DUAL-SEARCH-SPACE BAYESIAN OPTIMIZATION,0.3493670886075949,"STDP, Gesture
6
(2,4), (4,6)
4, (-26,-24,-15,-9)
110
260
STDP, N-Caltech
8
(3,5), (5,7)
6, (-21,-19,-17,-13,-9,-7)
140
240"
ABLATION STUDIES,0.3518987341772152,"5.3
ABLATION STUDIES"
ABLATION STUDIES,0.35443037974683544,"To investigate the effect of using multiple neuron dynamics, we apply the same dual-search-space
Bayesian optimization process for networks that have homogeneous neuron dynamic for the same"
ABLATION STUDIES,0.3569620253164557,Table 2: Ablation studies of optimized networks
ABLATION STUDIES,0.3594936708860759,"Model
Heterogeneity
Skip-layer
DVS Gesture
N-Caltech101
S-MNIST"
ABLATION STUDIES,0.3620253164556962,"Homogeneous-BPTT
N
Y
95.0
65.3
95.5
No-skip-layer-BPTT
Y
N
96.5
63.5
94.8
This Work-BPTT
Y
Y
98.0
71.2
97.3"
ABLATION STUDIES,0.36455696202531646,"Homogeneous-STDP
N
Y
91.3
37.0
94.3
No-skip-layer-STDP
Y
N
93.1
51.9
95.5
This Work-STDP
Y
Y
96.6
58.1
96.1"
ABLATION STUDIES,0.3670886075949367,Published as a conference paper at ICLR 2022
ABLATION STUDIES,0.369620253164557,Table 3: Accuracy (%) for DVS Gesture (top) and N-Caltech101 (bottom)
ABLATION STUDIES,0.3721518987341772,"Labeled Data % In Training
Parameter
Model
100%
50%
30%
10%
Number"
ABLATION STUDIES,0.37468354430379747,"ConvLSNN (Salaj et al., 2020)
97.1
95.3
92.0
84.3
2.9M
DECOLLE (Kaiser et al., 2020)
97.5
95.0
91.2
83.9
1.3M
(Fang et al., 2021)
97.8
-
-
-
-
HATS (Sironi et al., 2018)
95.2
94.1
91.6
83.7
-
H-SNN (She et al., 2021)
96.2
95.8
93.7
88.2
0.74M"
ABLATION STUDIES,0.37721518987341773,"This Work-STDP Training
96.6
96.0
94.1
91.2
0.81M
This Work-BPTT Training
98.0
95.3
91.1
82.4
1.1M"
ABLATION STUDIES,0.379746835443038,"Labeled Data % In Training
Parameter
Model
100%
70%
50%
30%
Number"
ABLATION STUDIES,0.3822784810126582,"ConvLSNN (Salaj et al., 2020)
63.1
58.7
51.3
45.4
3.0M
DECOLLE (Kaiser et al., 2020)
66.9
61.9
56.2
50.6
2.0M
HATS (Sironi et al., 2018)
64.2
61.0
54.3
48.8
-
H-SNN (She et al., 2021)
42.8
41.9
37.0
34.6
1.7M"
ABLATION STUDIES,0.3848101265822785,"This Work-STDP Training
58.1
57.8
57.2
54.6
1.4M
This Work-BPTT Training
71.2
65.4
56.0
52.5
1.7M"
ABLATION STUDIES,0.38734177215189874,"number of evaluations as the proposed design. Similarly, to study the contribution to performance
gain from skip-layer connections, the Bayesian optimization process is used for network templates
without skip-layer connections. The optimization process runs for the same number of evaluations
as the proposed design. From the results shown in Table 2, it can be observed that compared to
baselines, the proposed networks achieve the best accuracy for all datasets. Specifically, when ho-
mogeneous network is used, the performance of STDP trained network is noticeably lower than
the proposed method for DVS Gesture and N-Caltech101. For BPTT training, using heterogeneous
network and skip-layer connection show different level of benefit for each dataset. For sequential
MNIST which has less complexity, the improvement from using heterogeneous neurons and skip-
layer connections is not as significant."
COMPARISON WITH PRIOR WORKS,0.389873417721519,"5.4
COMPARISON WITH PRIOR WORKS"
COMPARISON WITH PRIOR WORKS,0.3924050632911392,"DVS Gesture As shown in Table 3 (top), with 100% labels, the proposed network trained with BPTT
demonstrates higher accuracy than all tested networks without using the most trainable parameters.
The proposed network trained with STDP has slightly lower accuracy than some baselines when
100% labels are used; for reduced-label training it outperforms all other networks."
COMPARISON WITH PRIOR WORKS,0.3949367088607595,"N-Caltech101 As shown in Table 3 (bottom), the proposed network trained with BPTT outperforms
all baselines with both 70% and 100% training labels and also has less trainable parameters than
most baselines. The un-supervised learning models i.e., H-SNN and the proposed network with
STDP, show considerably lower performance (more than what was observed for DVS Gesture) than
supervised ones when 100% labels are available; However, the proposed network with STDP shows
better performance than H-SNN, and outperforms all networks when available labels are below 50%."
CONCLUSION,0.39746835443037976,"6
CONCLUSION"
CONCLUSION,0.4,"We develop a theoretical basis to understand and optimize the ability of a feedforward SNN to
approximate temporal sequence. We analytically show how heterogeneity and skip-layer connec-
tions can improve sequence approximation with SNN, and empirically demonstrate their impact on
spatiotemporal learning. It is well-known in neuroscience that, heterogeneity (De Kloet & Reul,
1987) and irregular connectivity (Eickhoff et al., 2018) are intrinsic properties of human brains. Our
analysis shows that incorporating such concepts within artificial SNN is beneficial for designing
high-performance SNN for classification of spatiotemporal data."
CONCLUSION,0.40253164556962023,Published as a conference paper at ICLR 2022
CONCLUSION,0.4050632911392405,ACKNOWLEDGMENTS
CONCLUSION,0.40759493670886077,"This material is based on work sponsored by the Army Research Office and was accomplished
under Grant Number W911NF-19-1-0447. The views and conclusions contained in this document
are those of the authors and should not be interpreted as representing the official policies, either
expressed or implied, of the Army Research Office or the U.S. Government."
CONCLUSION,0.41012658227848103,"Authors would like to thank Drs. Mark Mclean, Chris Purdy, Fernando Camacho, and James Keiser
from Laboratory for Physical Sciences for many helpful discussions on this work."
REFERENCES,0.41265822784810124,REFERENCES
REFERENCES,0.4151898734177215,"Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo Di Nolfo,
Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, et al. A low
power, fully event-based gesture recognition system. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 7243–7252, 2017."
REFERENCES,0.4177215189873418,"Y. Amit and Yibi Huang. Precise capacity analysis in binary networks with multiple coding level
inputs. Neural Computation, 22:660–688, 2010."
REFERENCES,0.42025316455696204,"David G Barrett, Sophie Den`eve, and Christian K Machens. Firing rate predictions in optimal bal-
anced networks. In Advances in Neural Information Processing Systems, pp. 1538–1546. Citeseer,
2013."
REFERENCES,0.42278481012658226,"Jonathan Binas, Giacomo Indiveri, and Michael Pfeiffer. Spiking analog vlsi neuron assemblies as
constraint satisfaction problem solvers. In 2016 IEEE International Symposium on Circuits and
Systems (ISCAS), pp. 2094–2097, 2016. doi: 10.1109/ISCAS.2016.7538992."
REFERENCES,0.4253164556962025,"Johanni Brea, Walter Senn, and Jean-Pascal Pfister. Matching recall and storage in sequence learning
with spiking neural networks. Journal of neuroscience, 33(23):9565–9575, 2013."
REFERENCES,0.4278481012658228,"Chi-Ning Chou, Kai-Min Chung, and Chi-Jen Lu. On the algorithmic power of spiking neural
networks. arXiv preprint arXiv:1803.10375, 2018."
REFERENCES,0.43037974683544306,"ER De Kloet and JMHM Reul. Feedback action and tonic influence of corticosteroids on brain
function: a concept arising from the heterogeneity of brain receptor systems. Psychoneuroen-
docrinology, 12(2):83–105, 1987."
REFERENCES,0.43291139240506327,"Simon B Eickhoff, BT Thomas Yeo, and Sarah Genon. Imaging-based parcellations of the human
brain. Nature Reviews Neuroscience, 19(11):672–686, 2018."
REFERENCES,0.43544303797468353,"Wei Fang, Zhaofei Yu, Yanqi Chen, Timothee Masquelier, Tiejun Huang, and Yonghong Tian. In-
corporating learnable membrane time constant to enhance learning of spiking neural networks.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2661–2671,
2021."
REFERENCES,0.4379746835443038,"Edris Zaman Farsa, Soheila Nazari, and Morteza Gholami. Function approximation by hardware
spiking neural network. Journal of Computational Electronics, 14(3):707–716, 2015."
REFERENCES,0.44050632911392407,"Jacob R Gardner, Matt J Kusner, Zhixiang Eddie Xu, Kilian Q Weinberger, and John P Cunningham.
Bayesian optimization with inequality constraints. In ICML, volume 2014, pp. 937–945, 2014."
REFERENCES,0.4430379746835443,"Wulfram Gerstner. Time structure of the activity in neural network models. Physical review E, 51
(1):738, 1995."
REFERENCES,0.44556962025316454,"Wulfram Gerstner and Werner M Kistler. Mathematical formulations of hebbian learning. Biological
cybernetics, 87(5-6):404–415, 2002."
REFERENCES,0.4481012658227848,"Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. Rmp-snn: Residual membrane potential
neuron for enabling deeper high-accuracy and low-latency spiking neural network. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."
REFERENCES,0.4506329113924051,Published as a conference paper at ICLR 2022
REFERENCES,0.4531645569620253,"Nicolangelo Iannella and Andrew D. Back.
A spiking neural network architecture for nonlin-
ear function approximation. Neural Networks, 14(6):933–939, 2001. ISSN 0893-6080. doi:
https://doi.org/10.1016/S0893-6080(01)00080-6.
URL https://www.sciencedirect.
com/science/article/pii/S0893608001000806."
REFERENCES,0.45569620253164556,"Jacques Kaiser, Hesham Mostafa, and Emre Neftci. Synaptic plasticity dynamics for deep con-
tinuous local learning (decolle). Frontiers in Neuroscience, 14:424, 2020. ISSN 1662-453X.
doi: 10.3389/fnins.2020.00424. URL https://www.frontiersin.org/article/10.
3389/fnins.2020.00424."
REFERENCES,0.4582278481012658,"Mina A Khoei, Amirreza Yousefzadeh, Arash Pourtaherian, Orlando Moreira, and Jonathan Tapson.
Sparnet: Sparse asynchronous neural network execution for energy efficient inference. In 2020
2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS), pp.
256–260. IEEE, 2020."
REFERENCES,0.4607594936708861,"Seijoon Kim, Seongsik Park, Byunggook Na, Jongwan Kim, and Sungroh Yoon. Towards fast and
accurate object detection in bio-inspired spiking neural networks through bayesian optimization.
IEEE Access, 9:2633–2643, 2020a."
REFERENCES,0.46329113924050636,"Seijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon. Spiking-yolo: Spiking neural
network for energy-efficient object detection. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 11270–11277, 2020b."
REFERENCES,0.46582278481012657,"Jun Haeng Lee, Tobi Delbruck, and Michael Pfeiffer. Training deep spiking neural networks us-
ing backpropagation.
Frontiers in Neuroscience, 10:508, 2016.
ISSN 1662-453X.
doi: 10.
3389/fnins.2016.00508. URL https://www.frontiersin.org/article/10.3389/
fnins.2016.00508."
REFERENCES,0.46835443037974683,"Wolfgang Maass. Fast sigmoidal networks via spiking neurons. Neural computation, 9(2):279–304,
1997."
REFERENCES,0.4708860759493671,"Garrick Orchard, Ajinkya Jayawant, Gregory K. Cohen, and Nitish Thakor. Converting static image
datasets to spiking neuromorphic datasets using saccades. Frontiers in Neuroscience, 9:437, 2015.
ISSN 1662-453X.
doi: 10.3389/fnins.2015.00437.
URL https://www.frontiersin.
org/article/10.3389/fnins.2015.00437."
REFERENCES,0.47341772151898737,"Maryam Parsa, J Parker Mitchell, Catherine D Schuman, Robert M Patton, Thomas E Potok, and
Kaushik Roy. Bayesian-based hyperparameter optimization for spiking neuromorphic systems.
In 2019 IEEE International Conference on Big Data (Big Data), pp. 4472–4478. IEEE, 2019."
REFERENCES,0.4759493670886076,"Filip Ponulak and Andrzej Kasinski. Introduction to spiking neural networks: Information process-
ing, learning and applications. Acta neurobiologiae experimentalis, 71(4):409–433, 2011."
REFERENCES,0.47848101265822784,"Ali Safa, Federico Corradi, Lars Keuninckx, Ilja Ocket, Andr´e Bourdoux, Francky Catthoor, and
Georges GE Gielen. Improving the accuracy of spiking neural networks for radar gesture recog-
nition through preprocessing. IEEE Transactions on Neural Networks and Learning Systems,
2021."
REFERENCES,0.4810126582278481,"Darjan Salaj, Anand Subramoney, Ceca Kraiˇsnikovi´c, Guillaume Bellec, Robert Legenstein, and
Wolfgang Maass. Spike-frequency adaptation provides a long short-term memory to networks
of spiking neurons. bioRxiv, 2020. doi: 10.1101/2020.05.11.081513. URL https://www.
biorxiv.org/content/early/2020/05/12/2020.05.11.081513."
REFERENCES,0.4835443037974684,"Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going deeper in spiking
neural networks: Vgg and residual architectures. Frontiers in neuroscience, 13:95, 2019."
REFERENCES,0.4860759493670886,"Xueyuan She, Saurabh Dash, Daehyun Kim, and Saibal Mukhopadhyay. A heterogeneous spiking
neural network for unsupervised learning of spatiotemporal patterns. Frontiers in Neuroscience,
14:1406, 2021."
REFERENCES,0.48860759493670886,"A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman. Hats: Histograms of averaged
time surfaces for robust event-based object classification. In 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 1731–1740, 2018.
doi: 10.1109/CVPR.2018.
00186."
REFERENCES,0.4911392405063291,Published as a conference paper at ICLR 2022
REFERENCES,0.4936708860759494,"Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. Advances in neural information processing systems, 25, 2012."
REFERENCES,0.4962025316455696,"Gopalakrishnan Srinivasan and Kaushik Roy. Restocnet: Residual stochastic binary convolutional
spiking neural network for memory-efficient neuromorphic computing. Frontiers in neuroscience,
13:189, 2019."
REFERENCES,0.49873417721518987,"Hiroyuki Torikai, Atsuo Funew, and Toshimichi Saito. Digital spiking neuron and its learning for
approximation of various spike-trains. Neural Networks, 21(2):140–149, 2008. ISSN 0893-6080.
doi: https://doi.org/10.1016/j.neunet.2007.12.045. URL https://www.sciencedirect.
com/science/article/pii/S0893608008000051. Advances in Neural Networks Re-
search: IJCNN ’07."
REFERENCES,0.5012658227848101,"Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560, 1990."
REFERENCES,0.5037974683544304,"Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. Direct training for spiking neural
networks: Faster, larger, better. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 1311–1318, 2019."
REFERENCES,0.5063291139240507,"Shao-Qun Zhang, Zhao-Yu Zhang, and Zhi-Hua Zhou. Bifurcation spiking neural network. arXiv
preprint arXiv:1909.08341, 2019."
REFERENCES,0.5088607594936709,Published as a conference paper at ICLR 2022
REFERENCES,0.5113924050632911,"A
SNN DYNAMICS"
REFERENCES,0.5139240506329114,"Remark For a sequentially connected neuron list with m neurons all with γ = 1 and neuron delay
tnd, an input spike at time t leads the neuron list to generate an output spike at time t + mtnd"
REFERENCES,0.5164556962025316,"Remark For any input sequence with period tin to a spiking neuron with response rate γ such that
γ > 1, if refractory period is set to r < tin, the neuron can exit refractory period before the next
spike arrives."
REFERENCES,0.5189873417721519,"Lemma 1 For any input spike sequence with period tin in range [Tmin, Tmax], there exists a spiking
neuron n with fixed parameters vth, vreset, a, Rm and τm, such that by changing synaptic conduc-
tance G, it is possible to set the neuron response rate γn to be any positive integer."
REFERENCES,0.5215189873417722,"Proof.
For a given input spike sequence period tin, consider the maximum possible membrane
potential decay that can be reached within a period of tin. From (1), when I = 0, dv"
REFERENCES,0.5240506329113924,dt < 0 and | dv
REFERENCES,0.5265822784810127,"dt |
increases with higher v. Hence, the maximum decay of v is reached when initial membrane potential
v(t = 0) →v−
th and the neuron decays for period tin = Tmax. The decayed membrane potential
v(t = Tmax) can be derived by solving the differential equation (1) for v(t) = vth at t = 0:"
REFERENCES,0.529113924050633,v(t = Tmax) = vthe−Tmax
REFERENCES,0.5316455696202531,"τm
−ae−Tmax"
REFERENCES,0.5341772151898734,"τm
+ a
(5)"
REFERENCES,0.5367088607594936,"It is possible to have a spiking neuron with Rm, a and τm such that ∆v, defined as"
REFERENCES,0.5392405063291139,∆v = v(t = Tmax) −v(t = 0) = vthe−Tmax
REFERENCES,0.5417721518987342,"τm
−ae−Tmax"
REFERENCES,0.5443037974683544,"τm
+ a −vth
(6)
,"
REFERENCES,0.5468354430379747,"tends to zero. With this configuration, since the the highest possible decay of membrane potential is
negligible, for any target γ, it is possible to set G such that"
REFERENCES,0.549367088607595,"G = vth −vreset γ
(7)"
REFERENCES,0.5518987341772152,The proof is complete.
REFERENCES,0.5544303797468354,"Note, input sequence that has no spike has tin tending to infinity, thus violating the bounded con-
straint mentioned in Lemma 1."
REFERENCES,0.5569620253164557,"B
PROOF OF THEOREM 1"
REFERENCES,0.5594936708860759,"Theorem 1 For any input and target output spike sequence pair with periods (tin, tout) ∈
[Tmin, Tmax]×[Tmin, Tmax], there exists a minimal-layer-size network with skip-layer connections
that has memory pathway with output spike period function P(tin) such that |P(tin) −tout| < ϵ."
REFERENCES,0.5620253164556962,"Proof. For any given (tin, tout), first consider the condition where tin > tout. It is possible to
construct a minimal-layer-size network N connecting m spiking neurons with neuron response rate
γ = 1 sequentially, denoted as a m-tuple of neurons {n1, n2, ..., nm}. Since any configuration of
skip-layer connection with source layer and target layer pair (ls, lt), such that ls ∈[1, m −2], and
lt ∈[ls + 2, m], can be added, it is possible to add a (m −2)-tuple of skip-layer connections"
REFERENCES,0.5645569620253165,"Ssl = {(i, m) ∀i ∈{1, 2, 3, ..., m −2}}
(8)"
REFERENCES,0.5670886075949367,Denote the synaptic conductance for all the skip-layer connections as a (m −2)-tuple
REFERENCES,0.569620253164557,"SGsl = {Gsl
1 , Gsl
2 , Gsl
3 , ..., Gsl
m−2}
(9)"
REFERENCES,0.5721518987341773,"For any tout < tin, it is possible to find a k-tuple of synaptic conductance"
REFERENCES,0.5746835443037974,"S′
Gsl = {Gsl
i , Gsl
2i, Gsl
3i, ..., Gsl
ki} s.t. i = ⌊tout"
REFERENCES,0.5772151898734177,"tnd
⌋and k = ⌊m −2"
REFERENCES,0.579746835443038,"i
⌋
(10)"
REFERENCES,0.5822784810126582,Published as a conference paper at ICLR 2022
REFERENCES,0.5848101265822785,"Set synaptic conductance in SGsl \S′
Gsl to 0. Then set the conductance of synapse connecting nm−1
and nm to 0. In such way, The output spikes from network N have period"
REFERENCES,0.5873417721518988,P(tin) = ⌊tout
REFERENCES,0.589873417721519,"tnd
⌋· tnd
(11)"
REFERENCES,0.5924050632911393,"For given ϵ, it is possible to choose tnd such that tnd < 2ϵ, therefore satisfying |P(tin) −tout| < ϵ.
m can be chosen as"
REFERENCES,0.5949367088607594,m = Tmax −Tmin
REFERENCES,0.5974683544303797,"tnd
(12)"
REFERENCES,0.6,or equivalently:
REFERENCES,0.6025316455696202,m = Tmax −Tmin
REFERENCES,0.6050632911392405,"2ϵ
(13)"
REFERENCES,0.6075949367088608,Since Tmax−Tmin
REFERENCES,0.610126582278481,"2ϵ
is finite, m is finite."
REFERENCES,0.6126582278481013,"For tin < tout, using N as described above, it is possible to achieve output spike with period within
ϵ of any period in (0, tin]. For a given tout, assume the configuration in neuron list N has output
spike interval t′
int such that kt′
int = tout, where k is a positive integer. From Lemma 1, it is possible
to set G for a neuron nm+1 such that its neuron response delay satisfies γnm+1 = k for input spike
period t′
int. A new network, denoted as N ′, can be formed by connecting nm+1 to the output of
N. N ′ has output spike with period P(tin) = kt′
int = tout. Hence, to reach the given ϵ, it requires
neuron list N to have output spike interval tint such that"
REFERENCES,0.6151898734177215,"|tint −t′
int| < ϵ"
REFERENCES,0.6177215189873417,"k
(14)"
REFERENCES,0.620253164556962,"Since k is finite, (14) can be achieved."
REFERENCES,0.6227848101265823,"For tin >= tout, it is possible to configure network N ′ such that tint satisfies |tint −tout| < ϵ, and
the value of γnm+1 set to 1, hence |P(tin) −tout| < ϵ can be achieved. The proof is complete."
REFERENCES,0.6253164556962025,"C
PROOF OF LEMMA 2"
REFERENCES,0.6278481012658228,"Lemma 2 With no skip-layer connection, there does not exist a minimal-layer-size network that has
output spike period function P(tin) such that for any input and target output spike sequence pair
with periods (tin, tout) ∈[Tmin, Tmax] × [Tmin, Tmax], |P(tin) −tout| < ϵ."
REFERENCES,0.6303797468354431,"Proof. A minimal-layer-size network N with m layers can be denoted as a m-tuple of neurons
{n1, n2, ..., nm} connected sequentially. Since no skip-layer connection exists, there is only one
distinct memory pathway that contains all neurons {n1, n2, ..., nm}."
REFERENCES,0.6329113924050633,Denote the set of neuron response rate corresponding to each neuron in N as
REFERENCES,0.6354430379746835,"Γ = {γ1, γ2, ..., γm}
(15)"
REFERENCES,0.6379746835443038,"For a given input sequence with tin, denote the timing of the first spike as tin
1 , consider the output
spike sequence for network with"
REFERENCES,0.640506329113924,"γi = 1 ∀γi ∈Γ
(16)"
REFERENCES,0.6430379746835443,"The first output spike from N has timing ˜tout
1
= tin
1 + mtnd, and the second output spike has timing
˜tout
2
= tin
1 + tin + mtnd. It can be easily derived that the period of the output spike sequence is"
REFERENCES,0.6455696202531646,"P(tin) = tin
(17)"
REFERENCES,0.6481012658227848,Published as a conference paper at ICLR 2022
REFERENCES,0.6506329113924051,Also consider the output spike sequence for network with
REFERENCES,0.6531645569620254,"γj = 2 for any j ∈{1, 2, 3, 4, ..., m} and γi = 1 ∀i ∈({1, 2, 3, 4, ..., m} \ {j})
(18)"
REFERENCES,0.6556962025316456,"Following the same process, the period of the output spike sequence is"
REFERENCES,0.6582278481012658,"P(tin) = 2tin
(19)"
REFERENCES,0.660759493670886,"Since the smallest increase to any γi is by 1, there is no set of values for γ such that the network
output spike sequence has period P(tin) satisfying tin < P(tin) < 2tin. Since within the range
(tin, 2tin), there exists values of tout such that |P(tin) −tout| < ϵ does not hold. The proof is
complete."
REFERENCES,0.6632911392405063,"D
MEMORY PATHWAYS IN SNN"
REFERENCES,0.6658227848101266,"In this section we analyze the increase to the least upper bound of the number of memory pathways
in a network by using heterogeneous networks and skip-layer connections."
REFERENCES,0.6683544303797468,"Lemma 3 A spiking neuron has cutoff period ωc = τm ln(
vreset−a
vreset−a+ Rm"
REFERENCES,0.6708860759493671,τm G) above which input spike
REFERENCES,0.6734177215189874,sequence cannot cause the spiking neuron to spike.
REFERENCES,0.6759493670886076,"Proof. Consider (2), since membrane potential increases at time ti and decays otherwise, solving for
t = ti and the equation can be expanded:"
REFERENCES,0.6784810126582278,"vm(ti) = vresete
ti
τm + a(1 −e
ti
τm ) + Rm τm
Ge ti−t1"
REFERENCES,0.6810126582278481,"τm
+ Rm τm
Ge ti−t2"
REFERENCES,0.6835443037974683,"τm
+ ... + Rm"
REFERENCES,0.6860759493670886,"τm
G
(20)"
REFERENCES,0.6886075949367089,"For input with frequency f, ti+1 −ti = ∆t =
1
f , subtracting membrane potential values at two
consecutive ti provides:"
REFERENCES,0.6911392405063291,"∆vm = vm(ti+1) −vm(ti) = vreset(e
ti+1"
REFERENCES,0.6936708860759494,"τm −e
ti
τm ) −a(e
ti+1"
REFERENCES,0.6962025316455697,"τm −e
ti
τm ) + Rm τm
Ge"
REFERENCES,0.6987341772151898,ti+1−t1
REFERENCES,0.7012658227848101,"τm
(21)"
REFERENCES,0.7037974683544304,setting time of the first input spike t1 to zero leads to:
REFERENCES,0.7063291139240506,"∆vm = e
ti
τm ((e
∆t
τm −1)(vreset −a) + Rm"
REFERENCES,0.7088607594936709,"τm
Ge
∆t
τm )
(22)"
REFERENCES,0.7113924050632912,"As e
ti
τm > 0, and the term ((e
∆t
τm )−1)(vreset−a)+ Rm"
REFERENCES,0.7139240506329114,"τm Ge
∆t
τm ) does not depend on ti, the polarity of
∆vm does not change with time. vm is either strictly increasing, staying the same or decreasing with
higher ti. This indicates that, when ∆vm ≤0 the post-synaptic neuron can never spike regardless
of how many pre-synaptic spikes it receives. ∆vm ≤0 when input spike period tin satisfies"
REFERENCES,0.7164556962025317,"tin ≥τm ln(
vreset −a
vreset −a + Rm"
REFERENCES,0.7189873417721518,"τm G)
(23)"
REFERENCES,0.7215189873417721,"Therefore, the cutoff period of the neuron is"
REFERENCES,0.7240506329113924,"ωc = τm ln(
vreset −a
vreset −a + Rm"
REFERENCES,0.7265822784810126,"τm G)
(24)"
REFERENCES,0.7291139240506329,The proof is complete.
REFERENCES,0.7316455696202532,Published as a conference paper at ICLR 2022
REFERENCES,0.7341772151898734,"In the following proof, we consider cutoff frequency, fc =
1
ωc of spiking neurons. From (24) it
can be easily derived that fc can be configured to any positive real number by changing the neuron
parameters."
REFERENCES,0.7367088607594937,"Lemma 4 For an mMND network with m layers and {λ1, λ2, ...λm} number of different neuron
dynamics in each layer, the least upper bound of the number of memory pathways is Qm
i=1 λi."
REFERENCES,0.739240506329114,Proof. Denote the set of neurons in layer l with distinct neuron dynamics as
REFERENCES,0.7417721518987341,"Sl
n = {nl
1, nl
2, nl
3, ..., nl
λl}
(25)"
REFERENCES,0.7443037974683544,"Since the network is mMND, |Sl
n| = λl. Denote the set of cutoff frequency corresponding to each
neuron in Sl
n as"
REFERENCES,0.7468354430379747,"F l
c = {f nl
1
c , f nl
2
c , f nl
3
c , ..., f
nl
λl
c
}
(26)"
REFERENCES,0.7493670886075949,"Since neurons in Sl
n can have different neuron parameters, from Lemma 3, it is possible to set the
parameters such that all entries of F l
c are distinct. Hence, there exists a permutation π such that"
REFERENCES,0.7518987341772152,"f
nl
π(1)
c
< f
nl
π(2)
c
< f
nl
π(3)
c
... < f
nl
π(λl)
c
(27)"
REFERENCES,0.7544303797468355,"Denote the input spike frequency to layer l as f l
in, neuron nl
i is a part of a valid memory pathway in
the network if"
REFERENCES,0.7569620253164557,"f nl
i
c
≤f l
in
(28)"
REFERENCES,0.759493670886076,"Therefore, for input spike frequency f l
in ≥f
nl
π(λl)
c
, all neurons in Sl
n can be part of a valid memory"
REFERENCES,0.7620253164556962,"pathway of the network. For input spike frequency such that f
nl
π(i)
c
≤f l
in < f
nl
π(i+1)
c
, i neurons can
be part of a valid memory pathway of the network."
REFERENCES,0.7645569620253164,"For any input to the network fin ∈[Fmin, Fmax], denote the number of ways different neurons in
Sl
n can be part of valid memory pathways as kl. The total number of distinct memory pathways K
in the network is the product of kl of all layers: K = m
Y"
REFERENCES,0.7670886075949367,"l=1
kl
(29)"
REFERENCES,0.769620253164557,"Since for all layers, 0 ≤kl ≤λl, K ≤ m
Y"
REFERENCES,0.7721518987341772,"l=1
λl
(30)"
REFERENCES,0.7746835443037975,"Consider that, for any layer l ∈{1, 2, 3, ..., m}, the input f l
in is bounded by [F l
min, F l
max]. kl = λl"
REFERENCES,0.7772151898734178,"can be achieved by setting f
nl
π(1)
c
and f
nl
π(λl)
c
such that:"
REFERENCES,0.779746835443038,"f
nl
π(1)
c
= F l
min and f
nl
π(λl)
c
= F l
max
(31)"
REFERENCES,0.7822784810126582,Hence the bound is tight for (30). The proof is complete.
REFERENCES,0.7848101265822784,"Lemma 5 For an mMND network with m layers and {λ1, λ2, ...λm} different neuron dynamics in
each layer and a skip-layer connection made between layer la and lb, s.t. a, b ∈{1, 2, ...m} and (b−
a) > 1, the least upper bound of the number of memory pathways is Qm
i=1 λi +(Qa
i=1 λi ·Qm
i=b λi)"
REFERENCES,0.7873417721518987,Published as a conference paper at ICLR 2022
REFERENCES,0.789873417721519,"Proof. Denote the mMND network with skip-layer connection between layer la and layer lb as P.
Denote the set of all neurons in P as"
REFERENCES,0.7924050632911392,"SP = {n1
1, n1
2, n1
3, ..., n2
1, n2
2, n2
3, ...}
(32)"
REFERENCES,0.7949367088607595,"where n1
i is a neuron in layer l1, and n2
i is a neuron in layer l2, etc. The activation state onj
i of a"
REFERENCES,0.7974683544303798,"neuron can be denoted with binary values 0 and 1 with onj
i = 1 representing that nj
i receives input"
REFERENCES,0.8,"frequency that is above its cutoff frequency f
nj
i
c ."
REFERENCES,0.8025316455696202,"The set of all possible neuron activation states O in P that generates non-zero network output feature
vector can be partitioned into two subsets denoted as OA and OB."
REFERENCES,0.8050632911392405,"Set OA contains all states where the input frequency f k
in to any layer lk such that a < k < b satisfies"
REFERENCES,0.8075949367088607,"f k
in < f nk
i
c
∀i ∈{1, 2, 3, ..., λk}
(33)"
REFERENCES,0.810126582278481,"Set OB contains all the remaining neuron activation states in O, where all layers receive input fre-
quency higher than cutoff frequency of at least one neuron in that layer."
REFERENCES,0.8126582278481013,"For all the states in OA, no spike signal is sent from layer b −1 to layer b, since at least one layer
between la and lb generates no output. Hence, output from P is not affected if connections between
layer li and li+1, such that i ∈{a, a + 1, ..., b −1}, are removed. Network P is therefore equivalent
to network P ′ that has layers {l1, l2, ..., la, lb, lb+1, ..., lm} connected sequentially."
REFERENCES,0.8151898734177215,"According to Lemma 4, it can be derived that the least upper bound of the number of distinct memory
pathways in P ′ is Qa
i=1 λi · Qm
i=b λi."
REFERENCES,0.8177215189873418,"Hence, for all states in set OA, the least upper bound of the number of distinct memory pathways in
P is also Qa
i=1 λi · Qm
i=b λi. For all states in set OB, since the activation of neurons in the source
layer of the skip-layer connection is already accounted for when considering layer la, the least upper
bound of the number of distinct memory pathways is the same as network P that has no skip-layer
connection, which is Qm
i=1 λi according to Lemma 4."
REFERENCES,0.8202531645569621,"For the set of memory pathways from states in OA, denoted as MA, and the set of memory pathways
from states in OB, denoted as MB, the number of memory pathways of network P is |MA ∪MB|."
REFERENCES,0.8227848101265823,"From Lemma 5, the bound is tight for |MA| ≤Qa
i=1 λi · Qm
i=b λi and for |MB| ≤Qm
i=1 λi. It also
satisfies that MA ∩MB = ∅since all elements in MA have (m−(b−a−1)) layers and all elements
in MB have m layers. Hence the bound is tight for"
REFERENCES,0.8253164556962025,"|MA ∪MB| ≤ m
Y"
REFERENCES,0.8278481012658228,"i=1
λi + ( a
Y"
REFERENCES,0.830379746835443,"i=1
λi · m
Y"
REFERENCES,0.8329113924050633,"i=b
λi)
(34)"
REFERENCES,0.8354430379746836,The proof is complete.
REFERENCES,0.8379746835443038,"E
TIME-VARYING FUNCTION APPROXIMATION"
REFERENCES,0.8405063291139241,"A time-varying function f(t) can be approximated using piece-wise constant function, such as illus-
trated in Figure 4. It is therefore possible to approximate the time-varying function with a feedfor-
ward SNN by approximating each of the constant function with a memory pathway. In this section,
we test the approximation capability of feedforward SNN for time-varying functions using this prin-
ciple. The target functions to approximate have the form of:"
REFERENCES,0.8430379746835444,"f(x) =
xn"
REFERENCES,0.8455696202531645,"x −m
(35)"
REFERENCES,0.8481012658227848,"Here, x is variable; n and m are function parameters. For discrete-time simulation of the network,
we approximate the target function with"
REFERENCES,0.850632911392405,"x = tin and f(x) = tout
(36)"
REFERENCES,0.8531645569620253,Published as a conference paper at ICLR 2022       
REFERENCES,0.8556962025316456,"n1
n2 …
nk"
REFERENCES,0.8582278481012658,"Set of memory
pathways in a feedforward SNN"
REFERENCES,0.8607594936708861,Memory Pathway a
REFERENCES,0.8632911392405064,"n1’
n2'
nk'
…
Memory Pathway b …"
REFERENCES,0.8658227848101265,"Piece-wise 
constant function 
approximation t f(t)"
REFERENCES,0.8683544303797468,"Figure 4: Using a set of memory pathways to map a piece-wise constant function for approximating
a time-varying function."
REFERENCES,0.8708860759493671,"where tin is the input spike period and tout is the output spike period. We test approximation
performance of a small-scale feedforward SNN with 6 fully-connected layers, skip-layer connec-
tions {(2, 5), (3, 5)} and 4 neuron dynamics. The network is trained with BPTT to minimize mean
squared error (MSE) loss between the spike period of network output t′
out and target spike period
tout."
REFERENCES,0.8734177215189873,"To evaluate the network’s approximation performance, we construct 6 networks with the same struc-
ture as discussed above, and change each to have different trainable parameter numbers by scaling
the layer size. For baseline comparison, networks with 6 layers, no skip-layer connection and using
homogeneous neurons, configured to have the same trainable parameter numbers as the proposed
networks, are tested. All networks are trained with BPTT. The loss function measures the difference
between the network output spike trains and the target spike trains with MSE. Two sets of function
parameters: (m = 1, n = 3.3) and (m = 2, n = 2.1) are tested for the target functions f(x)
on domain [3, 10]. The resulting MSE losses for different network scales are shown at the bottom
of Figure 5. It can be observed that the proposed networks can approximate target functions with
less error than the baseline networks at all network scales. The smallest tested networks have rela-
tively high losses while performance increases quickly with more trainable parameters. The rate of
performance improvement decreases when trainable parameter number is above 4000."
REFERENCES,0.8759493670886076,"To understand the impact of the target function parameters to approximation performance of SNN,
we test baseline and the proposed network with 4167 trainable parameters for different pairs of           "
REFERENCES,0.8784810126582279,Figure 5: MSE loss vs. number of trainable parameters for the function approximation experiments.
REFERENCES,0.8810126582278481,Published as a conference paper at ICLR 2022
REFERENCES,0.8835443037974684,"n
2.1
3.9
2.3 2.5 2.7 2.9 3.1 3.3 3.5 3.7 m -2.0 1.6 -1.6 -1.2 -0.8 -0.4 0 0.4 0.8 1.2 2.0"
REFERENCES,0.8860759493670886,1.8e-1
REFERENCES,0.8886075949367088,0.7e-1
REFERENCES,0.8911392405063291,Baseline Network
REFERENCES,0.8936708860759494,"n
2.1
3.9
2.3 2.5 2.7 2.9 3.1 3.3 3.5 3.7 m -2.0 1.6 -1.6 -1.2 -0.8 -0.4 0 0.4 0.8 1.2 2.0"
REFERENCES,0.8962025316455696,3.4e-3
REFERENCES,0.8987341772151899,1.8e-3
REFERENCES,0.9012658227848102,"Proposed Network (a) (b) n
m"
REFERENCES,0.9037974683544304,Loss (log) -6.5 -1.5 -4.0
REFERENCES,0.9063291139240506,"Figure 6: (a) MSE loss (log) of baseline network (top) and the proposed network (bottom) for
approximating functions with different parameters m and n. (b) Heat plots of MSE loss for approx-
imating functions with different parameters m and n."
REFERENCES,0.9088607594936708,"function parameters m and n. The resulting MSE loss is shown with a heat map in Figure 6. It
can be observed that for all m and n value pairs, the proposed network achieves lower loss than
the baseline network. Another observation is that, there is no clear correlation between the value
of m and approximation error. On the other hand, for higher values of n, the approximation error
generally increases for both baseline and the proposed network."
REFERENCES,0.9113924050632911,"F
NETWORK SPIKING ACTIVITY"
REFERENCES,0.9139240506329114,"To investigate neuron spiking activity in the proposed network, for the function approximation task
as described in Appendix E, with parameters (m = 1, n = 2.3) and a randomly selected approxi-
mation point f(x = 19), we plot the timing of neuron spikes at the last layer over training epochs in
Figure 7 (a). It can be observed that the network initially generates spikes at widely distributed tim-
ings. After the first 50 training epochs spike timing starts to converge and remains stable at around
200 epochs. The final output spike period is t′
out = 48, which matches the target output period."
REFERENCES,0.9164556962025316,"Next, we investigate spiking activity of the BPTT trained network as described in Section 5.4 for
N-Caltech101 classification task. Three different test data points are presented to the network, and
the spikes from neurons in the first depth of layer 8 and neurons in the final layer, are recorded and
shown in Figure 7 (b). Here, (i), (iii) and (v) are from layer 8; (ii), (iv) and (vi) are from the final
layer. (i) and (ii) are from the observation of a data point with class label “5”; (iii) and (iv) are from
the observation of another data point also with class label “5”; (iii) and (iv) are from the observation
of a data point with class label “1”."
REFERENCES,0.9189873417721519,"It can be observed that neurons in layer 8 exhibit similar activity in (i) and (iii) as the network is
presented with data points from the same class. (ii) and (iv) show that most spiking activities are"
REFERENCES,0.9215189873417722,Published as a conference paper at ICLR 2022                                           (a) (b)
REFERENCES,0.9240506329113924,"(i)
(ii)"
REFERENCES,0.9265822784810127,"(iii)
(iv)"
REFERENCES,0.9291139240506329,"(v)
(vi)"
REFERENCES,0.9316455696202531,"Figure 7: (a) Raster plot for function approximation experiment; t is the simulation time step. (b)
Raster plots for the proposed SNN trained for N-Caltech101 with BPTT; t is the simulation time
step; (i), (iii) and (v) are from layer 8; (ii), (iv) and (vi) are from the final layer."
REFERENCES,0.9341772151898734,Published as a conference paper at ICLR 2022
REFERENCES,0.9367088607594937,"from the neuron with index 5, indicating correct prediction for the two data points. When a data
point from a different class is presented, spiking activity in (v) shows different patterns than those
in (i) and (iii). In the final layer, neuron with index 1 generates the most spikes, leading to a correct
prediction. However, for this data point, neuron with index 58 is also generating a considerable
number of spikes, indicating that the network is more likely to mis-classify this data point, compared
to the other two tested data points."
REFERENCES,0.9392405063291139,"G
IMPLEMENTATION OF HETEROGENEOUS CONV-SNN"
REFERENCES,0.9417721518987342,"Multi-neuron-dynamic (MND) networks with convolutional layers can be considered a scaled-up
version of the mMND network, where each neuron dynamic now contains a matrix of neurons that
receive input features from different spatial locations. To implement heterogeneous Conv-SNN, first
consider a regular Conv-SNN layer with no heterogeneity. The spiking neuron matrix has dimension
{W, H, D}, where D is the depth of the layer. The convolution filter has dimension {C, w, h, D}
where C is the number of input channels and D is the number of output channels."
REFERENCES,0.9443037974683545,"Based on this, a heterogeneous layer l can be constructed, by concatenating heterogeneous neu-
ron matrices with the same W and H along layer depth. The resulting spiking neuron matrix
has dimension {W, H, λD}, where λ is the number of neuron dynamics.
Hence, layer depth
i ∈{k + 1, k + 2, k + 3, ..., k + D} have the same neuron parameters, where 0 ≤k ≤λ −1 is an
integer representing the index of neuron dynamics. The convolution filter has the same dimension as
the regular Conv-SNN layer: {C, w, h, D}, which is shared by layer depth with different k values.
During forward pass of this layer, a convolution operation is applied to generate an input signal ma-
trix with dimension {W, H, D}. For each value of k, neurons in depth {k+1, k+2, k+3, ..., k+d}
are simulated based on the neuron parameters for such neuron dynamic index k using the signal ma-
trix as input. For the next convolutional layer l′ receiving input from layer l, the filter dimension is
{λD, w′, h′, D′} with λD input channels."
REFERENCES,0.9468354430379747,"H
DETAILS ON THE BAYESIAN OPTIMIZATION PROCESS"
REFERENCES,0.9493670886075949,"During the first stage of the dual-search-space optimization process, the parameters to optimize
include: Nlayer, Lstart, Lend, Nskip, Ndynamic, all of which are positive integers. Specifically,
Nlayer is the number of convolutional layers. For skip-layer connection, there are three configura-
tion parameters to optimize: starting layer Lstart, which is the source layer of the first skip-layer
connection; ending layer Lend, which is the target layer of the last skip-layer connection; skip-layer
connection number Nskip, which defines how many connections to implement. The source layer of
the Nskip skip-layer connections are placed evenly between Lstart and Lend, each with skip length
equal to"
REFERENCES,0.9518987341772152,⌊Lend −Lstart
REFERENCES,0.9544303797468354,"Nskip
⌋"
REFERENCES,0.9569620253164557,In the case of
REFERENCES,0.959493670886076,Lend −Lstart
REFERENCES,0.9620253164556962,"Nskip
̸= ⌊Lend −Lstart"
REFERENCES,0.9645569620253165,"Nskip
⌋ ,"
REFERENCES,0.9670886075949368,the value of Lend is reduced to the maximum value that satisfies
REFERENCES,0.9696202531645569,Lend −Lstart
REFERENCES,0.9721518987341772,"Nskip
= ⌊Lend −Lstart"
REFERENCES,0.9746835443037974,"Nskip
⌋"
REFERENCES,0.9772151898734177,"For heterogeneity, the number of different dynamic Ndynamic in all layers is optimized jointly. The
constraints for the parameters are defined as:"
REFERENCES,0.979746835443038,"Nlayer ∈[4, 15]"
REFERENCES,0.9822784810126582,Published as a conference paper at ICLR 2022
REFERENCES,0.9848101265822785,"Figure 8: Visualization of the learned distribution from Bayesian optimization: N-Caltech101 vali-
dation accuracy vs. skip-layer configurations."
REFERENCES,0.9873417721518988,2 ≤Lstart < (Nlayer −1)
REFERENCES,0.9898734177215189,"(Lstart + 1) < Lend ≤Nlayer
0 ≤Nskip ≤(Lend −Lstart)/2 and"
REFERENCES,0.9924050632911392,"Ndynamic ∈[1, 10]"
REFERENCES,0.9949367088607595,"The manually configured neuron parameters are, τm = 100 and Rm = 300 for all neuron dynamics,
and a ∈[−30, −5] is distributed evenly for each neuron dynamic. Figure 8 shows visualization
of the learned distribution from the Bayesian optimization process. Here, mapping from the search
space of skip-layer connection configurations to the validation accuracy of N-Caltech101 is shown.
The highest validation accuracy is reached at Lstart = 2 and Lend = 11."
REFERENCES,0.9974683544303797,"Due to the exponential increase of search space with the number of neuron dynamics in each layer,
it is highly inefficient to search for every neuron parameters of each dynamic. In the second stage
of the optimization process, we choose to apply Bayesian optimization for the parameter a of each
neuron dynamic separately, while τm and Rm are optimized jointly with the same values shared by
all neuron dynamics. a values are taken to the precision of 100, and τm and Rm values are taken to
the precision of 101. The constraints are a ∈[−30, −5], τm ∈[50, 200] and Rm ∈[200, 400]. The
value of tnd for all networks is set to 1. The parameters of each optimized network are shown in
Table 1. Note, the skip-layer connections are listed as source and target layer pairs."
