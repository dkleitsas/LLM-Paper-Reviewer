Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.000970873786407767,"Data augmentation is crucial in training deep models, preventing them from over-
1"
ABSTRACT,0.001941747572815534,"fitting to limited data. Recent advances in generative AI, e.g., diffusion mod-
2"
ABSTRACT,0.002912621359223301,"els, have enabled more sophisticated augmentation techniques that produce data
3"
ABSTRACT,0.003883495145631068,"resembling natural images. We introduce GeNIe a novel augmentation method
4"
ABSTRACT,0.0048543689320388345,"which leverages a latent diffusion model conditioned on a text prompt to combine
5"
ABSTRACT,0.005825242718446602,"two contrasting data points (an image from the source category and a text prompt
6"
ABSTRACT,0.006796116504854369,"from the target category) to generate challenging augmentations. To achieve this,
7"
ABSTRACT,0.007766990291262136,"we adjust the noise level (equivalently, number of diffusion iterations) to ensure
8"
ABSTRACT,0.008737864077669903,"the generated image retains low-level and background features from the source
9"
ABSTRACT,0.009708737864077669,"image while representing the target category, resulting in a hard negative sample
10"
ABSTRACT,0.010679611650485437,"for the source category. We further automate and enhance GeNIe by adaptively
11"
ABSTRACT,0.011650485436893204,"adjusting the noise level selection on a per image basis (coined as GeNIe-Ada),
12"
ABSTRACT,0.01262135922330097,"leading to further performance improvements. Our extensive experiments, in both
13"
ABSTRACT,0.013592233009708738,"few-shot and long-tail distribution settings, demonstrate the effectiveness of our
14"
ABSTRACT,0.014563106796116505,"novel augmentation method and its superior performance over the prior art.
15"
INTRODUCTION,0.015533980582524271,"1
Introduction
16"
INTRODUCTION,0.01650485436893204,"Augmentation has become an integral part of training deep learning models, particularly when faced
17"
INTRODUCTION,0.017475728155339806,"with limited training data. For instance, when it comes to image classification with limited number
18"
INTRODUCTION,0.018446601941747572,"of samples per class, model generalization ability can be significantly hindered. Simple transfor-
19"
INTRODUCTION,0.019417475728155338,"mations like rotation, cropping, and adjustments in brightness artificially diversify the training set,
20"
INTRODUCTION,0.020388349514563107,"offering the model a more comprehensive grasp of potential data variations. Hence, augmentation
21"
INTRODUCTION,0.021359223300970873,"can serve as a practical strategy to boost the model’s learning capacity, minimizing the risk of overfit-
22"
INTRODUCTION,0.02233009708737864,"ting and facilitating effective knowledge transfer from limited labelled data to real-world scenarios.
23"
INTRODUCTION,0.02330097087378641,"Various image augmentation methods, encompassing standard transformations, and learning-based
24"
INTRODUCTION,0.024271844660194174,"approaches have been proposed [16, 15, 110, 111, 100]. Some augmentation strategies combine two
25"
INTRODUCTION,0.02524271844660194,"images possibly from two different categories to generate a new sample image. The simplest ones
26"
INTRODUCTION,0.02621359223300971,"in this category are MixUp [111] and CutMix [110] where two images are combined in the pixel
27"
INTRODUCTION,0.027184466019417475,"space. However, the resulting augmentations often do not lie within the manifold of natural images
28"
INTRODUCTION,0.02815533980582524,"and act as out-of-distribution samples that will not be encountered during testing.
29"
INTRODUCTION,0.02912621359223301,"Recently, leveraging generative models for data augmentation has gained an upsurge of attention
30"
INTRODUCTION,0.030097087378640777,"[100, 83, 63, 35]. These interesting studies, either based on fine-tuning or prompt engineering of
31"
INTRODUCTION,0.031067961165048542,"diffusion models, are mostly focused on generating generic augmentations without considering the
32"
INTRODUCTION,0.03203883495145631,"impact of other classes and incorporating that information into the generative process for a classifi-
33"
INTRODUCTION,0.03300970873786408,"cation context. We take a different approach to generate challenging augmentations near the decision
34"
INTRODUCTION,0.03398058252427184,"boundaries of a downstream classifier. Inspired by diffusion-based image editing methods [67, 63]
35"
INTRODUCTION,0.03495145631067961,"some of which are previously used for data augmentation, we propose to use conditional latent dif-
36"
INTRODUCTION,0.035922330097087375,"Figure 1: Generative Hard Negative Images Through Diffusion (GeNIe): generates hard negative images
that belong to the target category but are similar to the source image from low-level feature and contextual
perspectives. GeNIe starts from a source image passing it through a partial noise addition process, and condi-
tioning it on a different target category. By controlling the amount of noise, the reverse latent diffusion process
generates images that serve as hard negatives for the source category."
INTRODUCTION,0.036893203883495145,"fusion models [81] for generating hard negative images. Our core idea (coined as GeNIe) is to
37"
INTRODUCTION,0.037864077669902914,"sample source images from various categories and prompt the diffusion model with a contradictory
38"
INTRODUCTION,0.038834951456310676,"text corresponding to a different target category. We demonstrate that the choice of noise level (or
39"
INTRODUCTION,0.039805825242718446,"equivalently number of iterations) for the diffusion process plays a pivotal role in generating images
40"
INTRODUCTION,0.040776699029126215,"that semantically belong to the target category while retaining low-level features from the source
41"
INTRODUCTION,0.04174757281553398,"image. We argue that these generated samples serve as hard negatives [108, 65] for the source cat-
42"
INTRODUCTION,0.04271844660194175,"egory (or from a dual perspective hard positives for the target category). To further enhance GeNIe,
43"
INTRODUCTION,0.043689320388349516,"we propose an adaptive noise level selection strategy (dubbed as GeNIe-Ada) enabling it to adjust
44"
INTRODUCTION,0.04466019417475728,"noise levels automatically per sample.
45"
INTRODUCTION,0.04563106796116505,"To establish the impact of GeNIe, we focus on two challenging scenarios: long-tail and few-shot
46"
INTRODUCTION,0.04660194174757282,"settings. In real-world applications, data often follows a long-tail distribution, where common sce-
47"
INTRODUCTION,0.04757281553398058,"narios dominate and rare occurrences are underrepresented. For instance, a person jaywalking a
48"
INTRODUCTION,0.04854368932038835,"highway causes models to struggle with such unusual scenarios. Combating such a bias or lack of
49"
INTRODUCTION,0.04951456310679612,"sufficient data samples during model training is essential in building robust models for self-driving
50"
INTRODUCTION,0.05048543689320388,"cars or surveillance systems, to name a few. Same challenge arises in few-shot learning settings
51"
INTRODUCTION,0.05145631067961165,"where the model has to learn from only a handful of samples. Our extensive quantitative and qual-
52"
INTRODUCTION,0.05242718446601942,"itative experimentation, on a suite of few-shot and long-tail distribution settings, corroborate the
53"
INTRODUCTION,0.05339805825242718,"effectiveness of the proposed novel augmentation method (GeNIe, GeNIe-Ada) in generating hard
54"
INTRODUCTION,0.05436893203883495,"negatives, corroborating its significant impact on categories with a limited number of samples. A
55"
INTRODUCTION,0.05533980582524272,"high-level sketch of GeNIe is illustrated in Fig. 1. Our main contributions are summarized below:
56"
INTRODUCTION,0.05631067961165048,"- We introduce GeNIe, a novel yet elegantly simple diffusion-based augmentation method to cre-
57"
INTRODUCTION,0.05728155339805825,"ate challenging augmentations in the manifold of natural images. For the first time, to our best
58"
INTRODUCTION,0.05825242718446602,"knowledge, GeNIe achieves this by combining two sources of information (a source image, and a
59"
INTRODUCTION,0.059223300970873784,"contradictory target prompt) through a noise-level adjustment mechanism.
60"
INTRODUCTION,0.06019417475728155,"- We further extend GeNIe by automating the noise-level adjustment strategy on a per-sample basis
61"
INTRODUCTION,0.06116504854368932,"(called GeNIe-Ada), to enable generating hard negative samples in the context of image classifica-
62"
INTRODUCTION,0.062135922330097085,"tion, leading also to further performance enhancement.
63"
INTRODUCTION,0.06310679611650485,"- To substantiate the impact of GeNIe, we present a suit of quantitative and qualitative results in-
64"
INTRODUCTION,0.06407766990291262,"cluding extensive experimentation on two challenging tasks: few-shot and long tail distribution
65"
INTRODUCTION,0.06504854368932039,"settings corroborating that GeNIe (and its extension GeNIe-Ada) significantly improve the down-
66"
INTRODUCTION,0.06601941747572816,"stream classification performance.
67"
RELATED WORK,0.06699029126213592,"2
Related Work
68"
RELATED WORK,0.06796116504854369,"Data Augmentations. Simple flipping, cropping, colour jittering, and blurring are some forms of
69"
RELATED WORK,0.06893203883495146,"image augmentations [91]. These augmentations are commonly adopted in training deep learning
70"
RELATED WORK,0.06990291262135923,"models. However, using these data augmentations is not trivial in some domains. For example,
71"
RELATED WORK,0.070873786407767,"using blurring might remove important low-level information from medical images. More advanced
72"
RELATED WORK,0.07184466019417475,"approaches, such as MixUp [111] and CutMix [110], mix images and their labels accordingly [37,
73"
RELATED WORK,0.07281553398058252,"59, 47, 17]. However, the resulting augmentations are not natural images anymore, and thus, act
74"
RELATED WORK,0.07378640776699029,"as out-of-distribution samples that will not be seen at test time. Another strand of research tailors
75"
RELATED WORK,0.07475728155339806,"the augmentation strategy through a learning process to fit the training data [23, 16, 15]. Unlike the
76"
RELATED WORK,0.07572815533980583,"above methods, we propose to utilize pre-trained latent diffusion models to generate hard negatives
77"
RELATED WORK,0.0766990291262136,"(in contrast to generic augmentations) through a noise adaptation strategy discussed in Section 3.
78"
RELATED WORK,0.07766990291262135,"Data Augmentation with Generative Models. Using synthesized images from generative models
79"
RELATED WORK,0.07864077669902912,"to augment training data has been studied before in many domains [30, 86], including domain adap-
80"
RELATED WORK,0.07961165048543689,"tation [41], visual alignment [71], and mitigation of dataset bias [88, 36, 73]. For example, [73]
81"
RELATED WORK,0.08058252427184466,"introduces a methodology aimed at enhancing test set evaluation through augmentation. While pre-
82"
RELATED WORK,0.08155339805825243,"vious methods predominantly relied on GANs [114, 51, 101] as the generative model, more recent
83"
RELATED WORK,0.0825242718446602,"studies promote using diffusion models to augment the data [81, 35, 89, 100, 4, 62, 83, 42, 28, 26, 8].
84"
RELATED WORK,0.08349514563106795,"More specifically, [100, 83, 35, 4] study the effectiveness of text-to-image diffusion models in data
85"
RELATED WORK,0.08446601941747572,"augmentation by diversification of each class with synthetic images. [100] leverages a text-to-image
86"
RELATED WORK,0.0854368932038835,"diffusion model and fine-tunes it on the downstream dataset using textual-inversion [31] to increase
87"
RELATED WORK,0.08640776699029126,"the diversity of existing samples. [83] also utilizes a text-to-image diffusion model, but with a BLIP
88"
RELATED WORK,0.08737864077669903,"[53] model to generate meaningful captions from the existing images. [42] utilizes diffusion models
89"
RELATED WORK,0.0883495145631068,"for augmentation to correct model mistakes. [28] uses CLIP [76] to filter generated images. [26]
90"
RELATED WORK,0.08932038834951456,"utilizes text-based diffusion and a large language model (LLM) to diversify the training data. [8]
91"
RELATED WORK,0.09029126213592233,"uses an LLM to generate text descriptions of failure modes associated with spurious correlations,
92"
RELATED WORK,0.0912621359223301,"which are then used to generate synthetic data through generative models. The challenge here is that
93"
RELATED WORK,0.09223300970873786,"the LLM has little understanding of such failure scenarios and contexts.
94"
RELATED WORK,0.09320388349514563,"We take a completely different approach here, without replying on any extra source of information
95"
RELATED WORK,0.0941747572815534,"(e.g., through an LLM). Inspired by image editing approaches such as Boomerang [63] and SDEdit
96"
RELATED WORK,0.09514563106796116,"[67], we propose to adaptively guide a latent diffusion model to generate hard negatives images
97"
RELATED WORK,0.09611650485436893,"[65, 108] on a per-sample basis per category. In a nutshell, the aforementioned studies focus on im-
98"
RELATED WORK,0.0970873786407767,"proving the diversity of each class with effective prompts and diffusion models, however, we focus
99"
RELATED WORK,0.09805825242718447,"on generating effective hard negative samples for each class by combining two sources of contra-
100"
RELATED WORK,0.09902912621359224,"dicting information (images from the source category and text prompt from the target category).
101"
RELATED WORK,0.1,"Language Guided Recognition Models. Vision-Language foundation models (VLMs) [2, 76, 81,
102"
RELATED WORK,0.10097087378640776,"84, 77, 78] utilize human language to guide the generation of images or to extract features from
103"
RELATED WORK,0.10194174757281553,"images that are aligned with human language. For example, CLIP [76] shows decent zero-shot
104"
RELATED WORK,0.1029126213592233,"performance on many downstream tasks by matching images to their text descriptions. Some recent
105"
RELATED WORK,0.10388349514563107,"works improve the utilization of human language in the prompt [25, 72], and others use a diffusion
106"
RELATED WORK,0.10485436893203884,"model directly as a classifier [49]. Similar to the above, we use a foundation model (Stable Diffusion
107"
RELATED WORK,0.10582524271844661,"1.5 [81]) to improve the downstream task. Concretely, we utilize category names of the downstream
108"
RELATED WORK,0.10679611650485436,"tasks to augment their associate training data with hard negative samples.
109"
RELATED WORK,0.10776699029126213,"Few-Shot Learning. In Few-shot Learning (FSL), we pre-train a model with abundant data to learn
110"
RELATED WORK,0.1087378640776699,"a rich representation, then fine-tune it on new tasks with only a few available samples. In supervised
111"
RELATED WORK,0.10970873786407767,"FSL [10, 1, 74, 109, 27, 54, 95, 116, 92], pretraining is done on a labeled dataset, whereas in
112"
RELATED WORK,0.11067961165048544,"unsupervised FSL [43, 103, 61, 75, 3, 46, 39, 66, 90] the pre-training has to be conducted on an
113"
RELATED WORK,0.11165048543689321,"unlabeled dataset. We assess the impact of GeNIe on a number of few-shot scenarios and state-of-
114"
RELATED WORK,0.11262135922330097,"the-art baselines by accentuating on its impact on the few-shot inference stage.
115"
RELATED WORK,0.11359223300970873,"3
Proposed Method: GeNIe
116"
RELATED WORK,0.1145631067961165,"Given a source image XS from category S = <source category>, we are interested in generating a
117"
RELATED WORK,0.11553398058252427,"target image Xr from category T = <target category>. In doing so, we intend to ensure the low-
118"
RELATED WORK,0.11650485436893204,"level visual features or background context of the source image are preserved, so that we generate
119"
RELATED WORK,0.11747572815533981,"samples that would serve as hard negatives for the source image. To this aim, we adopt a conditional
120"
RELATED WORK,0.11844660194174757,"latent diffusion model (such as Stable Diffusion, [81]) conditioned on a text prompt of the following
121"
RELATED WORK,0.11941747572815534,"format “A photo of a T = <target category>”.
122"
RELATED WORK,0.1203883495145631,"Key Idea. GeNIe in its basic form is a simple yet effective augmentation sample generator for
123"
RELATED WORK,0.12135922330097088,"improving a classifier fθ(.) with the following two key aspects: (i) inspired by [63, 67] instead of
124"
RELATED WORK,0.12233009708737864,"adding the full amount of noise σmax and going through all Nmax (being typically 50) steps of
125"
RELATED WORK,0.12330097087378641,"denoising, we use less amount of noise (rσmax, with r ∈(0, 1)) and consequently fewer number
126"
RELATED WORK,0.12427184466019417,"of denoising iterations (⌊rNmax⌋); (ii) we prompt the diffusion model with a P mandating a target
127"
RELATED WORK,0.12524271844660195,"Figure 2: Effect of noise ratio, r, in GeNIe: we employ GeNIe to generate augmentations for the target classes
(motorcycle and cat) with varying r. Smaller r yields images closely resembling the source semantics, creating
an inconsistency with the intended target label. By tracing r from 0 to 1, augmentations gradually transition
from source image characteristics to the target category. However, a distinct shift from the source to the target
occurs at a specific r that may vary for different source images or target categories. For more examples, please
refer to Fig. A4."
RELATED WORK,0.1262135922330097,"category T different than the source S. Hence, we denote the conditional diffusion process as
128"
RELATED WORK,0.12718446601941746,"Xr = STDiff(XS, P, r). In such a construct, the proximity of the final decoded image Xr to the
129"
RELATED WORK,0.12815533980582525,"source image XS or the target category defined through the text prompt P depends on r. Hence, by
130"
RELATED WORK,0.129126213592233,"controlling the amount of noise, we can generate images that blend characteristics of both the text
131"
RELATED WORK,0.13009708737864079,"prompt P and the source image XS. If we do not provide much of visual details in the text prompt
132"
RELATED WORK,0.13106796116504854,"(e.g., desired background, etc.), we expect the decoded image Xr to follow the details of XS while
133"
RELATED WORK,0.13203883495145632,"reflecting the semantics of the text prompt P. We argue, and demonstrate later, that the newly
134"
RELATED WORK,0.13300970873786408,"generated samples can serve as hard negative examples for the source category S since they share
135"
RELATED WORK,0.13398058252427184,"the low-level features of XS while representing the semantics of the target category, T. Notably, the
136"
RELATED WORK,0.13495145631067962,"source category S can be randomly sampled or be carefully extracted from the confusion matrix of
137"
RELATED WORK,0.13592233009708737,"fθ(.) based on real training data. The latter might result in even harder negative samples being now
138"
RELATED WORK,0.13689320388349516,"cognizant of model confusions. Finally, we will append our initial dataset with the newly generated
139"
RELATED WORK,0.1378640776699029,"hard negative samples through GeNIe and (re)train the classifier model.
140"
RELATED WORK,0.13883495145631067,"Enhancing GeNIe: GeNIe-Ada. One of the remarkable aspects of GeNIe lies in its simple applica-
141"
RELATED WORK,0.13980582524271845,"tion, requiring only XS, P, and r. However, selecting the appropriate value for r poses a challenge
142"
RELATED WORK,0.1407766990291262,"as it profoundly influences the outcome. When r is small, the resulting Xr tends to closely resemble
143"
RELATED WORK,0.141747572815534,"XS, and conversely, when r is large (closer to 1), it tends to resemble the semantics of the target
144"
RELATED WORK,0.14271844660194175,"category. This phenomenon arises because a smaller noise level restricts the capacity of the diffusion
145"
RELATED WORK,0.1436893203883495,"model to deviate from the semantics of the input XS. Thus, a critical question emerges: how can we
146"
RELATED WORK,0.14466019417475728,"select r for a particular source image to generate samples that preserve the low-level semantics of
147"
RELATED WORK,0.14563106796116504,"the source category S in XS while effectively representing the semantics of the target category T?
148"
RELATED WORK,0.14660194174757282,"We propose a method to determine an ideal value for r.
149"
RELATED WORK,0.14757281553398058,"Our intuition suggests that by varying the noise ratio r from 0 to 1, Xr will progressively resemble
150"
RELATED WORK,0.14854368932038836,"category S in the beginning and category T towards the end. However, somewhere between 0
151"
RELATED WORK,0.14951456310679612,"and 1, Xr will undergo a rapid transition from category S to T. This phenomenon is empirically
152"
RELATED WORK,0.15048543689320387,"observed in our experiments with varying r, as depicted in Fig. 2. Although the exact reason for this
153"
RELATED WORK,0.15145631067961166,"rapid change remains uncertain, one possible explanation is that the intermediate points between
154"
RELATED WORK,0.1524271844660194,"two categories reside far from the natural image manifold, thus, challenging the diffusion model’s
155"
RELATED WORK,0.1533980582524272,"capability to generate them. Ideally, we should select r corresponding to just after this rapid semantic
156"
RELATED WORK,0.15436893203883495,"transition, as at this point, Xr exhibits the highest similarity to the source image while belonging to
157"
RELATED WORK,0.1553398058252427,"the target category.
158"
RELATED WORK,0.1563106796116505,"We propose to trace the semantic trajectory between XS and XT through the lens of the classifier
159"
RELATED WORK,0.15728155339805824,"fθ(.). As shown in Algorithm 1, assuming access to the classifier backbone fθ(.) and at least one
160"
RELATED WORK,0.15825242718446603,"example XT from the target category, we convert both XS and XT into their respective latent vectors
161"
RELATED WORK,0.15922330097087378,"ZS and ZT by passing them through fθ(.). Then, we sample M values for r uniformly distributed
162"
RELATED WORK,0.16019417475728157,"∈(0, 1), generating their corresponding Xr and their latent vectors Zr for all those r. Subsequently,
163"
RELATED WORK,0.16116504854368932,we calculate dr = (Zr−ZS)T (ZT −ZS)
RELATED WORK,0.16213592233009708,"||ZT −ZS||2
as the distance between Zr and ZS projected onto the vector
164"
RELATED WORK,0.16310679611650486,"connecting ZS and ZT . Our hypothesis posits that the rapid semantic transition corresponds to a
165"
RELATED WORK,0.16407766990291262,"sharp change in this projected distance. Therefore, we sample n values for r uniformly distributed
166"
RELATED WORK,0.1650485436893204,"Algorithm 1: GeNIe-Ada
Require: XS, XT , fθ(.), STDiff(.), M
Extract ZS ←fθ(Xs), ZT ←fθ(XT )
for m ∈[1, M] do r ←m"
RELATED WORK,0.16601941747572815,"M , Zr ←fθ( STDiff(X, P, r) )"
RELATED WORK,0.1669902912621359,dm ←(Zr−ZS)T (ZT −ZS)
RELATED WORK,0.1679611650485437,"||ZT −ZS||2
m∗←argmaxm |dm −dm−1|, ∀m ∈[2, M]
r∗←m∗"
RELATED WORK,0.16893203883495145,"n
Return: Xr∗= STDiff(XS, P, r∗)"
RELATED WORK,0.16990291262135923,"Figure 3: GeNIe-Ada: To choose r adaptively for each (source image, target category) pair, we propose tracing
the semantic trajectory from ZS (source image embeddings) to ZT (target embeddings) through the lens of the
classifier fθ(·) (Algorithm 1). We adaptively select the sample right after the largest semantic shift."
RELATED WORK,0.170873786407767,"between 0 and 1, and analyze the variations in dr. We identify the largest gap in dr and select the r
167"
RELATED WORK,0.17184466019417477,"value just after the gap when increasing r, as detailed in Algorithm 1 and illustrated in Fig. 3.
168"
EXPERIMENTS,0.17281553398058253,"4
Experiments
169"
EXPERIMENTS,0.17378640776699028,"Since the impact of augmentation is more pronounced when the training data is limited, we evaluate
170"
EXPERIMENTS,0.17475728155339806,"the impact of GeNIe on Few-Shot classification in Section 4.1, Long-Tailed classification in Sec-
171"
EXPERIMENTS,0.17572815533980582,"tion 4.2, and fine-grained classification in Section A.2. For GeNIe-Ada in all scenarios, we utilize
172"
EXPERIMENTS,0.1766990291262136,"GeNIe to generate augmentations from the noise level set {0.5, 0.6, 0.7, 0.8, 0.9}. The selection of
173"
EXPERIMENTS,0.17766990291262136,"the appropriate noise level per source image and target is adaptive, achieved through Algorithm 1.
174"
EXPERIMENTS,0.1786407766990291,"Baselines.
We use Stable Diffusion 1.5 [81] as our base diffusion model.
In all settings,
175"
EXPERIMENTS,0.1796116504854369,"we use the same prompt format to generate images for the target class: i.e., “A photo of a
176"
EXPERIMENTS,0.18058252427184465,"<target category>”, where we replace the target category with the target category label.
177"
EXPERIMENTS,0.18155339805825244,"We generate 512 × 512 images for all methods. For fairness in comparison, we generate the same
178"
EXPERIMENTS,0.1825242718446602,"number of new images for each class. We use a single NVIDIA RTX 3090 for image generation.
179"
EXPERIMENTS,0.18349514563106797,"We consider 4 diffusion-based baselines and a suite of traditional data augmentation baselines:
180"
EXPERIMENTS,0.18446601941747573,"Img2Img [63, 67]: We sample an image from a target class, add noise to its latent representation and
181"
EXPERIMENTS,0.18543689320388348,"then pass it along with a prompt for the target category through reverse diffusion. The focus here is
182"
EXPERIMENTS,0.18640776699029127,"on a target class for which we generate extra positive samples. Adding large amount of noise leads
183"
EXPERIMENTS,0.18737864077669902,"to generating an image less similar to the original image. We use two different noise magnitudes for
184"
EXPERIMENTS,0.1883495145631068,"this baseline: r = 0.3 and r = 0.7 and denote them by Img2ImgL and Img2ImgH, respectively.
185"
EXPERIMENTS,0.18932038834951456,"Txt2Img [4, 35]: For this baseline, we omit the forward diffusion process and only use the reverse
186"
EXPERIMENTS,0.19029126213592232,"process starting from a text prompt for the target class of interest. This is similar to the base text-
187"
EXPERIMENTS,0.1912621359223301,"to-image generation strategy adopted in [81, 35, 89, 4, 62]. Fig. 4 illustrates a set of generated
188"
EXPERIMENTS,0.19223300970873786,"augmentation examples for Txt2Img, Img2Img, and GeNIe.
189"
EXPERIMENTS,0.19320388349514564,"DAFusion [100]: In this method, an embedding is optimized with a set of images for each class to
190"
EXPERIMENTS,0.1941747572815534,"correspond to the classes in the dataset. This approach is introduced in Textual Inversion [32]. We
191"
EXPERIMENTS,0.19514563106796118,"optimize an embedding for 5000 iterations for each class in the dataset, followed by augmentation
192"
EXPERIMENTS,0.19611650485436893,"similar as the DAFusion method.
193"
EXPERIMENTS,0.1970873786407767,"Cap2Aug[83]: It is a recent diffusion-based data augmentation strategy that uses image captions as
194"
EXPERIMENTS,0.19805825242718447,"text prompts for an image-to-image diffusion model.
195"
EXPERIMENTS,0.19902912621359223,"Traditional Data Augmentation: We consider both weak and strong traditional augmentations.
196"
EXPERIMENTS,0.2,"More specifically, for weak augmentation we use random resize crop with scaling ∈[0.2, 1.0] and
197"
EXPERIMENTS,0.20097087378640777,"horizontal flipping. For strong augmentation, we consider random color jitter, random grayscale,
198"
EXPERIMENTS,0.20194174757281552,"and Gaussian blur. For the sake of completeness, we also compare against data augmentations such
199"
EXPERIMENTS,0.2029126213592233,"as CutMix [110] and MixUp [111] that combine two images together.
200"
FEW-SHOT CLASSIFICATION,0.20388349514563106,"4.1
Few-shot Classification
201"
FEW-SHOT CLASSIFICATION,0.20485436893203884,"We assess the impact of GeNIe compared to other augmentations in a number of few-shot classifica-
202"
FEW-SHOT CLASSIFICATION,0.2058252427184466,"tion (FSL) scenarios, where the model has to learn only from the samples contained in the (N-way,
203"
FEW-SHOT CLASSIFICATION,0.20679611650485438,"K-shot) support set and infer on the query set. Note that this corresponds to an inference-only FSL
204"
FEW-SHOT CLASSIFICATION,0.20776699029126214,"Figure 4: Visualization of Generative Samples: We compare GeNIe with two baselines: Img2ImgL aug-
mentation: both image and text prompt are from the same category. Adding noise does not change the image
much, so they are not hard examples. Txt2Img augmentation: We simply use the text prompt only to generate
an image for the desired category (e.g., using a text2image method). Such images may be far from the domain
of our task since the generation is not informed by any visual data from our task. GeNIe augmentation: We
use the target category name in the text prompt only along with the source image."
FEW-SHOT CLASSIFICATION,0.2087378640776699,"setting where a pretraining stage on an abundant dataset is discarded. The goal is to assess how well
205"
FEW-SHOT CLASSIFICATION,0.20970873786407768,"the model can benefit from the augmentations while keeping the original N × K samples intact.
206"
FEW-SHOT CLASSIFICATION,0.21067961165048543,"Datasets. We conduct our few-shot experiments on two most commonly adopted few-shot classi-
207"
FEW-SHOT CLASSIFICATION,0.21165048543689322,"fication datasets: mini-Imagenet [79] and tiered-Imagenet [80]. mini-Imagenet is a subset of Ima-
208"
FEW-SHOT CLASSIFICATION,0.21262135922330097,"geNet [22] for few-shot classification. It contains 100 classes with 600 samples each. We follow
209"
FEW-SHOT CLASSIFICATION,0.21359223300970873,"the predominantly adopted settings of [79, 10] where we split the entire dataset into 64 classes for
210"
FEW-SHOT CLASSIFICATION,0.2145631067961165,"training, 16 for validation and 20 for testing. tiered-Imagenet is a larger subset of ImageNet with
211"
FEW-SHOT CLASSIFICATION,0.21553398058252426,"608 classes and a total of 779, 165 images, which are grouped into 34 higher-level nodes in the Im-
212"
FEW-SHOT CLASSIFICATION,0.21650485436893205,"ageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of
213"
FEW-SHOT CLASSIFICATION,0.2174757281553398,"training, validation, and testing nodes, and the corresponding classes form the respective meta-sets.
214"
FEW-SHOT CLASSIFICATION,0.21844660194174756,"Evaluation. To quantify the impact of different augmentation methods, we evaluate the test-set ac-
215"
FEW-SHOT CLASSIFICATION,0.21941747572815534,"curacies of a state-of-the-art unsupervised few-shot learning method with GeNIe and compare them
216"
FEW-SHOT CLASSIFICATION,0.2203883495145631,"against the accuracies obtained using other augmentation methods. Specifically, we use UniSiam
217"
FEW-SHOT CLASSIFICATION,0.22135922330097088,"[61] pre-trained with ResNet-18, ResNet-34 and ResNet-50 backbones and follow its evaluation
218"
FEW-SHOT CLASSIFICATION,0.22233009708737864,"strategy of fine-tuning a logistic regressor to perform (N-way, K-shot) classification on the test sets
219"
FEW-SHOT CLASSIFICATION,0.22330097087378642,"of mini- and tiered-Imagenet. Following [79], an episode consists of a labeled support-set and an un-
220"
FEW-SHOT CLASSIFICATION,0.22427184466019418,"labelled query-set. The support-set contains N randomly sampled classes where each class contains
221"
FEW-SHOT CLASSIFICATION,0.22524271844660193,"K samples, whereas the query-set contains Q randomly sampled unlabeled images per class. We
222"
FEW-SHOT CLASSIFICATION,0.2262135922330097,"conduct our experiments on the two most commonly adopted settings: (5-way, 1-shot) and (5-way,
223"
FEW-SHOT CLASSIFICATION,0.22718446601941747,"5-shot) classification settings. Following the literature, we sample 16-shots per class for the query
224"
FEW-SHOT CLASSIFICATION,0.22815533980582525,"set in both settings. We report the test accuracies along with the 95% confidence interval over 600
225"
FEW-SHOT CLASSIFICATION,0.229126213592233,"and 1000 episodes for mini-ImageNet and tiered-ImageNet, respectively.
226"
FEW-SHOT CLASSIFICATION,0.23009708737864076,"Implementation Details: GeNIe generates augmented images for each class using images from all
227"
FEW-SHOT CLASSIFICATION,0.23106796116504855,"other classes as the source image. We use r = 0.8 in our experiments. We generate 4 samples per
228"
FEW-SHOT CLASSIFICATION,0.2320388349514563,"class as augmentations in the 5-way, 1-shot setting and 20 samples per class as augmentations in the
229"
FEW-SHOT CLASSIFICATION,0.23300970873786409,"5-way, 5-shot setting. For the sake of a fair comparison, we ensure that the total number of labelled
230"
FEW-SHOT CLASSIFICATION,0.23398058252427184,"samples in the support set after augmentation remains the same across all different traditional and
231"
FEW-SHOT CLASSIFICATION,0.23495145631067962,"generative augmentation methodologies. Due to the expensive training of embeddings for each class
232"
FEW-SHOT CLASSIFICATION,0.23592233009708738,"in each episode, we only evaluated the DA-Fusion baseline on the first 100 episodes.
233"
FEW-SHOT CLASSIFICATION,0.23689320388349513,"Results: The results on mini-Imagenet and tiered-Imagenet for both (5-way, 1 and 5-shot) set-
234"
FEW-SHOT CLASSIFICATION,0.23786407766990292,"tings are summarized in Table 1 and Table 2, respectively.
Regardless of the choice of back-
235"
FEW-SHOT CLASSIFICATION,0.23883495145631067,"bone, we observe that GeNIe helps consistently improve UniSiam’s performance and outperform
236"
FEW-SHOT CLASSIFICATION,0.23980582524271846,"other supervised and unsupervised few-shot classification methods as well as other diffusion-based
237"
FEW-SHOT CLASSIFICATION,0.2407766990291262,"[100, 63, 82, 35] and classical [110, 111] data augmentation techniques on both datasets, across both
238"
FEW-SHOT CLASSIFICATION,0.24174757281553397,"(5-way, 1 and 5-shot) settings. Our noise adaptive method of selecting optimal augmentations per
239"
FEW-SHOT CLASSIFICATION,0.24271844660194175,"source image (GeNIe-Ada) further improves GeNIe’s performance across all three backbones, both
240"
FEW-SHOT CLASSIFICATION,0.2436893203883495,"Table 1: mini-ImageNet: We use our augmentations on (5-way, 1-shot) and (5-way, 5-shot) few-shot settings of
mini-Imagenet dataset with 3 different backbones (ResNet-18, 34, and 50). We compare with various baselines
and show that our augmentations with UniSiam outperform all the baselines including Txt2Img and DAFusion
augmentation. The number of generated images per class is 4 for 1-shot and 20 for 5-shot settings."
FEW-SHOT CLASSIFICATION,0.2446601941747573,ResNet-18
FEW-SHOT CLASSIFICATION,0.24563106796116504,"Augmentation
Method
Pre-training
1-shot
5-shot"
FEW-SHOT CLASSIFICATION,0.24660194174757283,"-
iDeMe-Net [14]
sup.
59.1±0.9
74.6±0.7
-
Robust + dist [27]
sup.
63.7±0.6
81.2±0.4
-
AFHN [54]
sup.
62.4±0.7
78.2±0.6
Weak
ProtoNet+SSL [94]
sup.+ssl
-
76.6
Weak
Neg-Cosine [57]
sup.
62.3±0.8
80.9±0.6
-
Centroid Align[1]
sup.
59.9±0.7
80.4±0.7
-
Baseline [10]
sup.
59.6±0.8
77.3±0.6
-
Baseline++ [10]
sup.
59.0±0.8
76.7±0.6
Weak
PSST [13]
sup.+ssl
59.5±0.5
77.4±0.5"
FEW-SHOT CLASSIFICATION,0.24757281553398058,"Weak
UMTRA [46]
unsup.
43.1±0.4
53.4±0.3
Weak
ProtoCLR [66]
unsup.
50.9±0.4
71.6±0.3
Weak
SimCLR [9]
unsup.
62.6±0.4
79.7±0.3
Weak
SimSiam [12]
unsup.
62.8±0.4
79.9±0.3
Weak
UniSiam+dist [61]
unsup.
64.1±0.4
82.3±0.3"
FEW-SHOT CLASSIFICATION,0.24854368932038834,"Weak
UniSiam [61]
unsup.
63.1±0.8
81.4±0.5
Strong
UniSiam [61]
unsup.
62.8±0.8
81.2±0.6
CutMix [110]
UniSiam [61]
unsup.
62.7±0.8
80.6±0.6
MixUp [111]
UniSiam [61]
unsup.
62.1±0.8
80.7±0.6
Img2ImgL[63]
UniSiam [61]
unsup.
63.9±0.8
82.1±0.5
Img2ImgH[63]
UniSiam [61]
unsup.
69.1±0.7
84.0±0.5
Txt2Img[4, 35]
UniSiam [61]
unsup.
74.1±0.6
84.6±0.5
DAFusion [100]
UniSiam [61]
unsup.
64.3±1.8
82.0±1.4
GeNIe (Ours)
UniSiam [61]
unsup.
75.5±0.6
85.4±0.4
GeNIe-Ada (Ours)
UniSiam [61]
unsup.
76.8±0.6
85.9±0.4"
FEW-SHOT CLASSIFICATION,0.24951456310679612,ResNet-34
FEW-SHOT CLASSIFICATION,0.2504854368932039,"Augmentation
Method
Pre-training
1-shot
5-shot"
FEW-SHOT CLASSIFICATION,0.25145631067961166,"Weak
Baseline [10]
sup.
49.8±0.7
73.5±0.7
Weak
Baseline++ [10]
sup.
52.7±0.8
76.2±0.6"
FEW-SHOT CLASSIFICATION,0.2524271844660194,"Weak
SimCLR [9]
unsup.
64.0±0.4
79.8±0.3
Weak
SimSiam [12]
unsup.
63.8±0.4
80.4±0.3
Weak
UniSiam+dist [61]
unsup.
65.6±0.4
83.4±0.2"
FEW-SHOT CLASSIFICATION,0.25339805825242717,"Weak
UniSiam [61]
unsup.
64.3±0.8
82.3±0.5
Strong
UniSiam [61]
unsup.
64.5±0.8
82.1±0.6
CutMix [110]
UniSiam [61]
unsup.
64.0±0.8
81.7±0.6
MixUp [111]
UniSiam [61]
unsup.
63.7±0.8
80.1±0.8
Img2ImgL[63]
UniSiam [61]
unsup.
65.5±0.8
82.9±0.5
Img2ImgH[63]
UniSiam [61]
unsup.
70.5±0.8
84.8±0.5
Txt2Img[4, 35]
UniSiam [61]
unsup.
75.4±0.6
85.5±0.5
DAFusion [100]
UniSiam [61]
unsup.
64.7±1.9
83.2±1.4
GeNIe (Ours)
UniSiam [61]
unsup.
77.1±0.6
86.3±0.4
GeNIe-Ada (Ours)
UniSiam [61]
unsup.
78.5±0.6
86.6±0.4
ResNet-50"
FEW-SHOT CLASSIFICATION,0.2543689320388349,"Weak
PDA+Net [11]
unsup.
63.8±0.9
83.1±0.6
Weak
Meta-DM [40]
unsup.
66.7±0.4
85.3±0.2"
FEW-SHOT CLASSIFICATION,0.25533980582524274,"Weak
UniSiam [61]
unsup.
64.6±0.8
83.4±0.5
Strong
UniSiam [61]
unsup.
64.8±0.8
83.2±0.5
CutMix [110]
UniSiam [61]
unsup.
64.3±0.8
83.2±0.5
MixUp [111]
UniSiam [61]
unsup.
63.8±0.8
84.6±0.5
Img2ImgL[63]
UniSiam [61]
unsup.
66.0±0.8
84.0±0.5
Img2ImgH[63]
UniSiam [61]
unsup.
71.1±0.7
85.7±0.5
Txt2Img[4, 35]
UniSiam [61]
unsup.
76.4±0.6
86.5±0.4
DAFusion [100]
UniSiam [61]
unsup.
65.7±1.8
83.9±1.2
GeNIe (Ours)
UniSiam [61]
unsup.
77.3±0.6
87.2±0.4
GeNIe-Ada (Ours)
UniSiam [61]
unsup.
78.6±0.6
87.9±0.4"
FEW-SHOT CLASSIFICATION,0.2563106796116505,"few-shot settings, and both datasets (mini and tiered-Imagenet). Few-shot accuracies for ResNet-
241"
FEW-SHOT CLASSIFICATION,0.25728155339805825,"34 computed on tieredImagenet are reported in Section A.3 of the appendix. Note that employing
242"
FEW-SHOT CLASSIFICATION,0.258252427184466,"CutMix and MixUp seems to lead to performance degradation compared to weak augmentations,
243"
FEW-SHOT CLASSIFICATION,0.25922330097087376,"probably due to overfitting since these methods can only choose from 4 other classes to mix.
244"
LONG-TAILED CLASSIFICATION,0.26019417475728157,"4.2
Long-Tailed Classification
245"
LONG-TAILED CLASSIFICATION,0.2611650485436893,"We evaluate our method on long-tailed data, where the number of instances per class is unbalanced,
246"
LONG-TAILED CLASSIFICATION,0.2621359223300971,"with most categories having limited samples (tail). Our goal is to mitigate this bias by augmenting
247"
LONG-TAILED CLASSIFICATION,0.26310679611650484,"the tail of the distribution with generated samples. We evaluate GeNIe using two different backbones
248"
LONG-TAILED CLASSIFICATION,0.26407766990291265,"and methods: the ViT architecture with LViT [107], and ResNet50 with VL-LTR [97].
249"
LONG-TAILED CLASSIFICATION,0.2650485436893204,"Following LViT [107], we first train an MAE [34] and ViT on the unbalanced dataset without any
250"
LONG-TAILED CLASSIFICATION,0.26601941747572816,"augmentation. Next, we train the Balanced Fine-Tuning stage of LViT by incorporating the aug-
251"
LONG-TAILED CLASSIFICATION,0.2669902912621359,"mentation data generated using GeNIe or other baselines. For ResNet50, we use VL-LTR code to
252"
LONG-TAILED CLASSIFICATION,0.26796116504854367,"fine-tune the CLIP [76] ResNet50 pretrained backbone with generated augmentations by GeNIe.
253"
LONG-TAILED CLASSIFICATION,0.2689320388349515,"Dataset: We perform experiments on ImageNet-LT [60]. It contains 115.8K images from 1, 000
254"
LONG-TAILED CLASSIFICATION,0.26990291262135924,"categories. The number of images per class varies from 1280 to 5. Imagenet-LT classes can be
255"
LONG-TAILED CLASSIFICATION,0.270873786407767,"divided into 3 groups: “Few” with less than 20 images, “Med” with 20 −100 images, and “Many”
256"
LONG-TAILED CLASSIFICATION,0.27184466019417475,"with more than 100 images. Imagenet-LT uses the same validation set as ImageNet. We augment
257"
LONG-TAILED CLASSIFICATION,0.2728155339805825,"“Few” categories only and limit the number of generated images to 50 samples per class. For GeNIe,
258"
LONG-TAILED CLASSIFICATION,0.2737864077669903,"instead of randomly sampling the source images from other classes, we use a confusion matrix on
259"
LONG-TAILED CLASSIFICATION,0.27475728155339807,"the training data to find the top-4 most confused classes and only consider those classes for random
260"
LONG-TAILED CLASSIFICATION,0.2757281553398058,"sampling of the source image. The source category may be from “Many”, “Med”, or “Few sets”.
261"
LONG-TAILED CLASSIFICATION,0.2766990291262136,"Results: Augmenting training data with GeNIe-Ada improves accuracy on the “Few” set by 11.7%
262"
LONG-TAILED CLASSIFICATION,0.27766990291262134,"and 4.4% compared with LViT only and LViT with Txt2Img augmentation baselines respectively.
263"
LONG-TAILED CLASSIFICATION,0.27864077669902915,"In ResNet50, GeNIe-Ada outperforms Cap2Aug baseline in “Few” categories by 7.6%. The results
264"
LONG-TAILED CLASSIFICATION,0.2796116504854369,"are summarized in Table 3. Please refer to Section A.4 for implementation details.
265"
ABLATION AND ANALYSIS,0.28058252427184466,"4.3
Ablation and Analysis
266"
ABLATION AND ANALYSIS,0.2815533980582524,"Semantic Shift from Source to Target Class. The core motivation behind GeNIe-Ada is that by
267"
ABLATION AND ANALYSIS,0.28252427184466017,"varying the noise ratio r from 0 to 1, augmented sample Xr will progressively shift its semantic cat-
268"
ABLATION AND ANALYSIS,0.283495145631068,"egory from source (S) in the beginning to target category (T) towards the end. However, somewhere
269"
ABLATION AND ANALYSIS,0.28446601941747574,"between 0 and 1, Xr will undergo a rapid transition from S to T. To demonstrate this hypothesis
270"
ABLATION AND ANALYSIS,0.2854368932038835,"empirically, in Figs. 5 and A5, we visualize pairs of source images and target categories with their re-
271"
ABLATION AND ANALYSIS,0.28640776699029125,"spective GeNIe generated augmentations for different noise ratios r, along with their corresponding
272"
ABLATION AND ANALYSIS,0.287378640776699,"Table 2: tiered-ImageNet: Accuracies (% ± std) for
5-way, 1-shot and 5-way, 5-shot classification settings
on the test-set. We compare against various SOTA su-
pervised and unsupervised few-shot classification base-
lines as well as other augmentation methods, with
UniSiam [61] pre-trained ResNet-18,50 backbones."
ABLATION AND ANALYSIS,0.2883495145631068,ResNet-18
ABLATION AND ANALYSIS,0.28932038834951457,"Augmentation
Method
Pre-training
1-shot
5-shot"
ABLATION AND ANALYSIS,0.2902912621359223,"Weak
SimCLR[9]
unsup.
63.4±0.4
79.2±0.3
Weak
SimSiam [12]
unsup.
64.1±0.4
81.4±0.3"
ABLATION AND ANALYSIS,0.2912621359223301,"Weak
UniSiam [61]
unsup.
63.1±0.7
81.0±0.5
Strong
UniSiam [61]
unsup.
62.8±0.7
80.9±0.5
CutMix [110]
UniSiam [61]
unsup.
62.1±0.7
78.9±0.6
MixUp [111]
UniSiam [61]
unsup.
62.1±0.7
78.4±0.6
Img2ImgL[63]
UniSiam [61]
unsup.
63.9±0.7
81.8±0.5
Img2ImgH[63]
UniSiam [61]
unsup.
68.7±0.7
83.5±0.5
Txt2Img[35]
UniSiam [61]
unsup.
72.9±0.6
84.2±0.5
DAFusion [100]
UniSiam [61]
unsup.
62.6±2.1
81.0±1.5
GeNIe(Ours)
UniSiam [61]
unsup.
73.6±0.6
85.0±0.4
GeNIe-Ada(Ours)
UniSiam [61]
unsup.
75.1±0.6
85.5±0.5"
ABLATION AND ANALYSIS,0.2922330097087379,ResNet-50
ABLATION AND ANALYSIS,0.29320388349514565,"Weak
PDA+Net [11]
unsup.
69.0±0.9
84.2±0.7
Weak
Meta-DM [40]
unsup.
69.6±0.4
86.5±0.3"
ABLATION AND ANALYSIS,0.2941747572815534,"Weak
UniSiam + dist [61]
unsup.
69.6±0.4
86.5±0.3
Weak
UniSiam [61]
unsup.
66.8±0.7
84.7±0.5
Strong
UniSiam [61]
unsup.
66.5±0.7
84.5±0.5
CutMix [110]
UniSiam [61]
unsup.
66.0±0.7
83.3±0.5
MixUp [111]
UniSiam [61]
unsup.
66.1±0.5
84.1±0.8
Img2ImgL[63]
UniSiam [61]
unsup.
67.8±0.7
85.3±0.5
Img2ImgH[63]
UniSiam [61]
unsup.
72.4±0.7
86.7±0.4
Txt2Img[35]
UniSiam [61]
unsup.
77.1±0.6
87.3±0.4
DAFusion [100]
UniSiam [61]
unsup.
66.5±2.2
84.8±1.4
GeNIe (Ours)
UniSiam [61]
unsup.
78.0±0.6
88.0±0.4
GeNIe-Ada (Ours)
UniSiam [61]
unsup.
78.8±0.6
88.6±0.6"
ABLATION AND ANALYSIS,0.29514563106796116,"Table 3:
Long-Tailed ImageNet-LT: We
compare different augmentation methods on
ImageNet-LT and report Top-1 accuracy for
“Few”, “Medium”, and “Many” sets.
On the
“Few” set and LiVT method, our augmentations
improve the accuracy by 11.7 points compared
to LiVT original augmentation and 4.4 points
compared to Txt2Img. GeNIe-Ada outperforms
Cap2Aug baseline in “Few” categories by 7.6%.
Refer to Table A4 for a full comparison with prior
Long-Tailed methods."
ABLATION AND ANALYSIS,0.2961165048543689,ResNet-50
ABLATION AND ANALYSIS,0.2970873786407767,"Method
Many
Med.
Few
Overall Acc"
ABLATION AND ANALYSIS,0.2980582524271845,"ResLT [18]
63.3
53.3
40.3
55.1
PaCo [19]
68.2
58.7
41.0
60.0
LWS [44]
62.2
48.6
31.8
51.5
Zero-shot CLIP [76]
60.8
59.3
58.6
59.8
DRO-LT [85]
64.0
49.8
33.1
53.5
VL-LTR [97]
77.8
67.0
50.8
70.1
Cap2Aug [83]
78.5
67.7
51.9
70.9
GeNIe-Ada
79.2
64.6
59.5
71.5
ViT-B"
ABLATION AND ANALYSIS,0.29902912621359223,"Method
Many
Med.
Few
Overall Acc"
ABLATION AND ANALYSIS,0.3,"ViT [24]
50.5
23.5
6.9
31.6
MAE [33]
74.7
48.2
19.4
54.5
DeiT [99]
70.4
40.9
12.8
48.4
LiVT [107]
73.6
56.4
41.0
60.9
LiVT + Img2ImgL
74.3
56.4
34.3
60.5
LiVT + Img2ImgH
73.8
56.4
45.3
61.6
LiVT + Txt2Img
74.9
55.6
48.3
62.2
LiVT + GeNIe-Ada
74.0
56.9
52.7
63.1"
ABLATION AND ANALYSIS,0.30097087378640774,"Figure 5: Embedding visualizations of generative augmentations: We pass all generative augmentations
through DINOv2 ViT-G (serving as an oracle) to extract their corresponding embeddings and visualize them
with PCA. As shown, the extent of semantic shifts varies based on both the source image and the target class."
ABLATION AND ANALYSIS,0.30194174757281556,"PCA-projected embedding scatter plots (on the far left). We extract embeddings for all the images
273"
ABLATION AND ANALYSIS,0.3029126213592233,"using a DINOv2 ViT-G pretrained backbone, which we assume as an oracle model in identifying
274"
ABLATION AND ANALYSIS,0.30388349514563107,"the right category. We observe that as r increases from 0.3 to 0.8, the images transition to embody
275"
ABLATION AND ANALYSIS,0.3048543689320388,"more of the target category’s semantics while preserving the contextual features of the source image.
276"
ABLATION AND ANALYSIS,0.3058252427184466,"This transition of semantics can also be observed in the embedding plots (on the left) where they
277"
ABLATION AND ANALYSIS,0.3067961165048544,"consistently shift from the proximity of the source image (blue star) to the target class’s centroid
278"
ABLATION AND ANALYSIS,0.30776699029126214,"(red cross) as the noise ratio r increases. The sparse distribution of points within r = [0.4, 0.6] for
279"
ABLATION AND ANALYSIS,0.3087378640776699,"the first image and r = [0.2, 0.4] for the second image aligns with our intuition of a rapid transition
280"
ABLATION AND ANALYSIS,0.30970873786407765,"from category S to T, thus empirically affirming our motivation behind GeNIe-Ada.
281"
ABLATION AND ANALYSIS,0.3106796116504854,"To further establish this, in Fig. 6, we demonstrate the efficacy of GeNIe in generating hard negatives
282"
ABLATION AND ANALYSIS,0.3116504854368932,"at the decision boundaries of an SVM classifier, which is trained on the labelled support set of
283"
ABLATION AND ANALYSIS,0.312621359223301,"the few-shot tasks of mini-Imagenet, without any augmentations. We then plot source and target
284"
ABLATION AND ANALYSIS,0.31359223300970873,"class probabilities (P(YS|Xr) and P(YT |Xr), respectively) of the generated augmentation samples
285"
ABLATION AND ANALYSIS,0.3145631067961165,"Xr. For both r = 0.6 and 0.7, there is significant overlap between P(YS|Xr) and P(YT |Xr),
286"
ABLATION AND ANALYSIS,0.3155339805825243,"making it difficult for the classifier to decide the correct class. On the right-hand-side, GeNIe-Ada
287"
ABLATION AND ANALYSIS,0.31650485436893205,"automatically selects the best r resulting in the most overlap between the two distributions, thus
288"
ABLATION AND ANALYSIS,0.3174757281553398,"offering the hardest negative sample among the considered r values (for more details see A.1).
289"
ABLATION AND ANALYSIS,0.31844660194174756,"Note that a large overlap between distributions is not sufficient to call the generated samples hard
290"
ABLATION AND ANALYSIS,0.3194174757281553,"negatives because they should also belong to the target category. This is, however, confirmed by the
291"
ABLATION AND ANALYSIS,0.32038834951456313,"high Oracle accuracy in Table 4 (elaborated in detail in the following paragraph) which verifies that
292"
ABLATION AND ANALYSIS,0.3213592233009709,"majority of the generated augmentation samples do belong to the target category.
293"
ABLATION AND ANALYSIS,0.32233009708737864,"Figure 6: Why GeNIe augmentations are challenging? While deciding which class the generated augmen-
tations (Xr) belong to is already difficult within r = [0.6, 0.7] (due to high overlap between P(YS|Xr) and
P(YT |Xr)), GeNIe-Ada selects the best noise threshold (r∗) offering the hardest negative sample."
ABLATION AND ANALYSIS,0.3233009708737864,"Table 4: Effect of Noise in GeNIe: We use the same setting as in Table 1 to study the effect of the amount of
noise. As expected (also shown in Fig 5), small noise results in worse accuracy since some generated images
may be from the source category rather than the target one. For r = 0.5 only 73% of the generated data is
from the target category. This behaviour is also shown in Fig. 2. Notably, reducing the noise level below 0.7
is associated with a decline in oracle accuracy and subsequent degradation in the performance of the final few-
shot model. Note that the high oracle accuracy of GeNIe-Ada demonstrates its capability to adaptively select
the noise level per source and target, ensuring semantic consistency with the intended target."
ABLATION AND ANALYSIS,0.32427184466019415,"Noise
ResNet-18
ResNet-34
ResNet-50
Oracle
1-shot
5-shot
1-shot
5-shot
1-shot
5-shot
Acc"
ABLATION AND ANALYSIS,0.32524271844660196,"GeNIe(r=0.5)
60.42±0.8
74.11±0.6
62.02±0.8
75.80±0.6
63.65±0.9
77.61±0.6
73.4±0.5
GeNIe(r=0.6)
69.66±0.7
80.65±0.5
71.13±0.7
82.21±0.5
72.10±0.7
82.79±0.5
85.8±0.4
GeNIe(r=0.7)
74.50±0.6
83.26±0.5
76.41±0.6
84.44±0.5
77.05±0.6
84.95±0.4
94.5±0.2
GeNIe(r=0.8)
75.45±0.6
85.38±0.4
77.08±0.6
86.28±0.4
77.28±0.6
87.22±0.4
98.2±0.1
GeNIe(r=0.9)
74.96±0.6
85.29±0.4
77.63±0.6
86.17±0.4
77.73±0.6
87.00±0.4
99.3±0.1
GeNIe-Ada
76.79±0.6
85.89±0.4
78.49±0.6
86.55±0.4
78.64±0.6
87.88±0.4
98.9±0.2"
ABLATION AND ANALYSIS,0.3262135922330097,"Label consistency of the generated samples. The choice of noise ratio r is important in producing
294"
ABLATION AND ANALYSIS,0.3271844660194175,"hard negative examples. In Table 4, we present the accuracy of the GeNIe model across various noise
295"
ABLATION AND ANALYSIS,0.32815533980582523,"ratios, alongside the oracle accuracy, which is an ImageNet pre-trained DeiT-Base [98] classifier.
296"
ABLATION AND ANALYSIS,0.329126213592233,"We observe a decline in the label consistency of generated data (quantified by the performance of
297"
ABLATION AND ANALYSIS,0.3300970873786408,"the oracle model) when decreasing the noise level. Reducing r also results in a degradation in the
298"
ABLATION AND ANALYSIS,0.33106796116504855,"performance of the final few-shot model (87.2% →77.6%) corroborating that an appropriate choice
299"
ABLATION AND ANALYSIS,0.3320388349514563,"of r plays a crucial role in our design strategy. We investigate this further in the following paragraph.
300"
ABLATION AND ANALYSIS,0.33300970873786406,"Effect of Noise in GeNIe. We examine the impact of noise on the performance of the few-shot
301"
ABLATION AND ANALYSIS,0.3339805825242718,"model in Table 4. Noise levels r ∈[0.7, 0.8] yield the best performance. Conversely, utilizing noise
302"
ABLATION AND ANALYSIS,0.33495145631067963,"levels below 0.7 diminishes performance due to label inconsistency, as is demonstrated in Table 4
303"
ABLATION AND ANALYSIS,0.3359223300970874,"and Fig 5. As such, determining the appropriate noise level is pivotal for the performance of GeNIe
304"
ABLATION AND ANALYSIS,0.33689320388349514,"to be able to generate challenging hard negatives while maintaining label consistency. An alternative
305"
ABLATION AND ANALYSIS,0.3378640776699029,"approach to finding the optimal noise level involves using GeNIe-Ada to adaptively select the noise
306"
ABLATION AND ANALYSIS,0.3388349514563107,"level for each source image and target class. As demonstrated in Tables 4 and A1, GeNIe-Ada
307"
ABLATION AND ANALYSIS,0.33980582524271846,"achieves performance that is comparable to or surpasses that of GeNIe with fixed noise levels.
308"
CONCLUDING REMARKS,0.3407766990291262,"5
Concluding Remarks
309"
CONCLUDING REMARKS,0.341747572815534,"GeNIe, for the first time to our knowledge, combines contradictory sources of information (a source
310"
CONCLUDING REMARKS,0.34271844660194173,"image, and a different target category prompt) through a noise adjustment strategy into a conditional
311"
CONCLUDING REMARKS,0.34368932038834954,"latent diffusion model to generate challenging augmentations, which can serve as hard negatives.
312"
CONCLUDING REMARKS,0.3446601941747573,"Limitation. The required time to create augmentations through GeNIe is on par with any typical
313"
CONCLUDING REMARKS,0.34563106796116505,"diffusion-based competitors [4, 35]; however, this is naturally slower than traditional augmentation
314"
CONCLUDING REMARKS,0.3466019417475728,"techniques [110, 111]. This is not a bottleneck in offline augmentation strategies, but can be con-
315"
CONCLUDING REMARKS,0.34757281553398056,"sidered a limiting factor in real-time scenarios. Recent studies are already mitigating this through
316"
CONCLUDING REMARKS,0.3485436893203884,"advancements in diffusion model efficiency [87, 68, 58]. Another challenge present in any genera-
317"
CONCLUDING REMARKS,0.34951456310679613,"tive AI-based augmentation technique is the domain shift between the distribution of training data
318"
CONCLUDING REMARKS,0.3504854368932039,"and the downstream context they might be used for augmentation. A possible remedy is to fine-tune
319"
CONCLUDING REMARKS,0.35145631067961164,"the diffusion backbone on a rather small dataset from the downstream task.
320"
CONCLUDING REMARKS,0.3524271844660194,"Broader Impact. We believe ideas from GeNIe can have a significant impact when it comes to gen-
321"
CONCLUDING REMARKS,0.3533980582524272,"erating hard augmentations challenging and thus enhancing downstream tasks beyond classification.
322"
CONCLUDING REMARKS,0.35436893203883496,"At the same time, just like any other generative model, GeNIe can also introduce inherent biases
323"
CONCLUDING REMARKS,0.3553398058252427,"stemming from the training data used to build its diffusion backbone, which can reflect and amplify
324"
CONCLUDING REMARKS,0.35631067961165047,"societal prejudices or inaccuracies. Therefore, it is crucial to carefully mitigate potential biases in
325"
CONCLUDING REMARKS,0.3572815533980582,"generative models such as GeNIe to ensure a fair and ethical deployment of deep learning systems.
326"
REFERENCES,0.35825242718446604,"References
327"
REFERENCES,0.3592233009708738,"[1] Afrasiyabi, A., Lalonde, J.F., Gagn´e, C.: Associative alignment for few-shot image classifi-
328"
REFERENCES,0.36019417475728155,"cation. In: ECCV (2019)
329"
REFERENCES,0.3611650485436893,"[2] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A.,
330"
REFERENCES,0.36213592233009706,"Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Saman-
331"
REFERENCES,0.36310679611650487,"gooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,
332"
REFERENCES,0.3640776699029126,"S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., Simonyan, K.: Flamingo: a
333"
REFERENCES,0.3650485436893204,"visual language model for few-shot learning (2022)
334"
REFERENCES,0.36601941747572814,"[3] Antoniou, A., Storkey, A.: Assume, augment and learn: Unsupervised few-shot meta-
335"
REFERENCES,0.36699029126213595,"learning via random labels and data augmentation. arxiv:1902.09884 (2019)
336"
REFERENCES,0.3679611650485437,"[4] Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., Fleet, D.J.: Synthetic data from diffusion
337"
REFERENCES,0.36893203883495146,"models improves imagenet classification (2023)
338"
REFERENCES,0.3699029126213592,"[5] Bossard, L., Guillaumin, M., Van Gool, L.: Food-101 – mining discriminative components
339"
REFERENCES,0.37087378640776697,"with random forests. In: European Conference on Computer Vision (2014)
340"
REFERENCES,0.3718446601941748,"[6] Cai, J., Wang, Y., Hwang, J.N., et al.: Ace: Ally complementary experts for solving long-
341"
REFERENCES,0.37281553398058254,"tailed recognition in one-shot. In: ICCV. pp. 112–121 (2021)
342"
REFERENCES,0.3737864077669903,"[7] Cao, K., Wei, C., Gaidon, A., Arechiga, N., Ma, T.: Learning imbalanced datasets with label-
343"
REFERENCES,0.37475728155339805,"distribution-aware margin loss. NeurIPS 32 (2019)
344"
REFERENCES,0.3757281553398058,"[8] Chegini, A., Feizi, S.: Identifying and mitigating model failures through few-shot clip-aided
345"
REFERENCES,0.3766990291262136,"diffusion generation. arXiv preprint arXiv:2312.05464 (2023)
346"
REFERENCES,0.37766990291262137,"[9] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning
347"
REFERENCES,0.3786407766990291,"of visual representations. In: ICML (2020)
348"
REFERENCES,0.3796116504854369,"[10] Chen, W.Y., Liu, Y.C., Kira, Z., Wang, Y.C.F., Huang, J.B.: A closer look at few-shot classi-
349"
REFERENCES,0.38058252427184464,"fication. In: ICLR (2019)
350"
REFERENCES,0.38155339805825245,"[11] Chen, W., Si, C., Wang, W., Wang, L., Wang, Z., Tan, T.: Few-shot learning with part discov-
351"
REFERENCES,0.3825242718446602,"ery and augmentation from unlabeled images. arXiv preprint arXiv:2105.11874 (2021)
352"
REFERENCES,0.38349514563106796,"[12] Chen, X., He, K.: Exploring simple siamese representation learning. In: CVPR (2021)
353"
REFERENCES,0.3844660194174757,"[13] Chen, Z., Ge, J., Zhan, H., Huang, S., Wang, D.: Pareto self-supervised training for few-shot
354"
REFERENCES,0.38543689320388347,"learning. In: CVPR (2021)
355"
REFERENCES,0.3864077669902913,"[14] Chen, Z., Fu, Y., Wang, Y.X., Ma, L., Liu, W., Hebert, M.: Image deformation meta-networks
356"
REFERENCES,0.38737864077669903,"for one-shot learning. In: CVPR (2019)
357"
REFERENCES,0.3883495145631068,"[15] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmen-
358"
REFERENCES,0.38932038834951455,"tation policies from data (2019)
359"
REFERENCES,0.39029126213592236,"[16] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data aug-
360"
REFERENCES,0.3912621359223301,"mentation with a reduced search space (2019)
361"
REFERENCES,0.39223300970873787,"[17] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.: Randaugment: Practical automated data aug-
362"
REFERENCES,0.3932038834951456,"mentation with a reduced search space. In: Larochelle, H., Ranzato, M., Hadsell, R.,
363"
REFERENCES,0.3941747572815534,"Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33,
364"
REFERENCES,0.3951456310679612,"pp. 18613–18624. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/
365"
REFERENCES,0.39611650485436894,"paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf
366"
REFERENCES,0.3970873786407767,"[18] Cui, J., Liu, S., Tian, Z., Zhong, Z., Jia, J.: Reslt: Residual learning for long-tailed recogni-
367"
REFERENCES,0.39805825242718446,"tion. IEEE transactions on pattern analysis and machine intelligence 45(3), 3695–3706 (2022)
368"
REFERENCES,0.3990291262135922,"[19] Cui, J., Zhong, Z., Liu, S., Yu, B., Jia, J.: Parametric contrastive learning. In: Proceedings of
369"
REFERENCES,0.4,"the IEEE/CVF international conference on computer vision. pp. 715–724 (2021)
370"
REFERENCES,0.4009708737864078,"[20] Cui, J., Zhong, Z., Liu, S., Yu, B., Jia, J.: Parametric contrastive learning. In: ICCV. pp.
371"
REFERENCES,0.40194174757281553,"715–724 (2021)
372"
REFERENCES,0.4029126213592233,"[21] Cui, Y., Jia, M., Lin, T.Y., Song, Y., Belongie, S.: Class-balanced loss based on effective
373"
REFERENCES,0.40388349514563104,"number of samples. In: CVPR. pp. 9268–9277 (2019)
374"
REFERENCES,0.40485436893203886,"[22] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierar-
375"
REFERENCES,0.4058252427184466,"chical image database. In: 2009 IEEE conference on computer vision and pattern recognition.
376"
REFERENCES,0.40679611650485437,"pp. 248–255. Ieee (2009)
377"
REFERENCES,0.4077669902912621,"[23] Ding, M., An, B., Xu, Y., Satheesh, A., Huang, F.: SAFLEX: Self-adaptive augmentation via
378"
REFERENCES,0.4087378640776699,"feature label extrapolation. In: The Twelfth International Conference on Learning Represen-
379"
REFERENCES,0.4097087378640777,"tations (2024), https://openreview.net/forum?id=qL6brrBDk2
380"
REFERENCES,0.41067961165048544,"[24] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De-
381"
REFERENCES,0.4116504854368932,"hghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is
382"
REFERENCES,0.41262135922330095,"worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)
383"
REFERENCES,0.41359223300970877,"[25] Dunlap, L., Mohri, C., Zhang, H., Guillory, D., Darrell, T., Gonzalez, J.E., Rohrbach, A.,
384"
REFERENCES,0.4145631067961165,"Raghunathan, A.: Using language to extend to unseen domains. International Conference on
385"
REFERENCES,0.4155339805825243,"Learning Representations (ICLR) (2023)
386"
REFERENCES,0.41650485436893203,"[26] Dunlap, L., Umino, A., Zhang, H., Yang, J., Gonzalez, J.E., Darrell, T.: Diversify your vision
387"
REFERENCES,0.4174757281553398,"datasets with automatic diffusion-based augmentation (2023)
388"
REFERENCES,0.4184466019417476,"[27] Dvornik, N., Mairal, J., Schmid, C.: Diversity with cooperation: Ensemble methods for few-
389"
REFERENCES,0.41941747572815535,"shot classification. In: ICCV (2019)
390"
REFERENCES,0.4203883495145631,"[28] Feng, C.M., Yu, K., Liu, Y., Khan, S., Zuo, W.: Diverse data augmentation with diffusions
391"
REFERENCES,0.42135922330097086,"for effective test-time prompt tuning (2023)
392"
REFERENCES,0.4223300970873786,"[29] Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation of deep
393"
REFERENCES,0.42330097087378643,"networks. In: Proceedings of the 34th International Conference on Machine Learning. pp.
394"
REFERENCES,0.4242718446601942,"1126–1135 (2017)
395"
REFERENCES,0.42524271844660194,"[30] Frid-Adar, M., Diamant, I., Klang, E., Amitai, M., Goldberger, J., Greenspan, H.: Gan-
396"
REFERENCES,0.4262135922330097,"based synthetic medical image augmentation for increased cnn performance in liver lesion
397"
REFERENCES,0.42718446601941745,"classification. Neurocomputing (2018)
398"
REFERENCES,0.42815533980582526,"[31] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.:
399"
REFERENCES,0.429126213592233,"An image is worth one word: Personalizing text-to-image generation using textual inversion.
400"
REFERENCES,0.4300970873786408,"arXiv preprint arXiv:2208.01618 (2022)
401"
REFERENCES,0.43106796116504853,"[32] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or,
402"
REFERENCES,0.4320388349514563,"D.: An image is worth one word: Personalizing text-to-image generation using textual
403"
REFERENCES,0.4330097087378641,"inversion (2022). https://doi.org/10.48550/ARXIV.2208.01618, https://arxiv.org/abs/
404"
REFERENCES,0.43398058252427185,"2208.01618
405"
REFERENCES,0.4349514563106796,"[33] He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., Girshick, R.B.: Masked autoencoders are scalable
406"
REFERENCES,0.43592233009708736,"vision learners. In: CVPR. pp. 15979–15988. IEEE (2022)
407"
REFERENCES,0.4368932038834951,"[34] He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., Girshick, R.: Masked autoencoders are scalable
408"
REFERENCES,0.43786407766990293,"vision learners (2021)
409"
REFERENCES,0.4388349514563107,"[35] He, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S., Qi, X.: Is synthetic data from
410"
REFERENCES,0.43980582524271844,"generative models ready for image recognition? arXiv preprint arXiv:2210.07574 (2022)
411"
REFERENCES,0.4407766990291262,"[36] Hemmat, R.A., Pezeshki, M., Bordes, F., Drozdzal, M., Romero-Soriano, A.: Feedback-
412"
REFERENCES,0.441747572815534,"guided data synthesis for imbalanced classification (2023)
413"
REFERENCES,0.44271844660194176,"[37] Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: AugMix:
414"
REFERENCES,0.4436893203883495,"A simple data processing method to improve robustness and uncertainty. Proceedings of the
415"
REFERENCES,0.4446601941747573,"International Conference on Learning Representations (ICLR) (2020)
416"
REFERENCES,0.44563106796116503,"[38] Hong, Y., Zhang, J., Sun, Z., Yan, K.: Safa: Sample-adaptive feature augmentation for long-
417"
REFERENCES,0.44660194174757284,"tailed image classification. In: ECCV (2022)
418"
REFERENCES,0.4475728155339806,"[39] Hsu, K., Levine, S., Finn, C.: Unsupervised learning via meta-learning. In: ICLR (2018)
419"
REFERENCES,0.44854368932038835,"[40] Hu, W., Jiang, X., Liu, J., Yang, Y., Tian, H.: Meta-dm: Applications of diffusion models on
420"
REFERENCES,0.4495145631067961,"few-shot learning (2023)
421"
REFERENCES,0.45048543689320386,"[41] Huang, S.W., Lin, C.T., Chen, S.P., an Po-Hao Hsu, Y.Y.W., Lai, S.H.: Auggan: Cross do-
422"
REFERENCES,0.45145631067961167,"main adaptation with gan-based data augmentation. European Conference on Computer Vi-
423"
REFERENCES,0.4524271844660194,"sion (2018)
424"
REFERENCES,0.4533980582524272,"[42] Jain, S., Lawrence, H., Moitra, A., Madry, A.: Distilling model failures as directions in latent
425"
REFERENCES,0.45436893203883494,"space. In: ArXiv preprint arXiv:2206.14754 (2022)
426"
REFERENCES,0.4553398058252427,"[43] Jang, H., Lee, H., Shin, J.: Unsupervised meta-learning via few-shot pseudo-supervised con-
427"
REFERENCES,0.4563106796116505,"trastive learning. In: The Eleventh International Conference on Learning Representations
428"
REFERENCES,0.45728155339805826,"(2022)
429"
REFERENCES,0.458252427184466,"[44] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y.: Decoupling rep-
430"
REFERENCES,0.45922330097087377,"resentation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217 (2019)
431"
REFERENCES,0.4601941747572815,"[45] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y.: Decoupling
432"
REFERENCES,0.46116504854368934,"representation and classifier for long-tailed recognition. In: ICLR (2020)
433"
REFERENCES,0.4621359223300971,"[46] Khodadadeh, S., Boloni, L., Shah, M.: Unsupervised meta-learning for few-shot image clas-
434"
REFERENCES,0.46310679611650485,"sification. In: NeurIPS (2019)
435"
REFERENCES,0.4640776699029126,"[47] Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statistics for
436"
REFERENCES,0.4650485436893204,"optimal mixup. In: International Conference on Machine Learning. pp. 5275–5285. PMLR
437"
REFERENCES,0.46601941747572817,"(2020)
438"
REFERENCES,0.4669902912621359,"[48] Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3D object representations for fine-grained catego-
439"
REFERENCES,0.4679611650485437,"rization. In: Workshop on 3D Representation and Recognition. Sydney, Australia (2013)
440"
REFERENCES,0.46893203883495144,"[49] Li, A.C., Prabhudesai, M., Duggal, S., Brown, E., Pathak, D.: Your diffusion model is secretly
441"
REFERENCES,0.46990291262135925,"a zero-shot classifier (2023)
442"
REFERENCES,0.470873786407767,"[50] Li, B., Han, Z., Li, H., Fu, H., Zhang, C.: Trustworthy long-tailed classification. In: CVPR.
443"
REFERENCES,0.47184466019417476,"pp. 6970–6979 (2022)
444"
REFERENCES,0.4728155339805825,"[51] Li, D., Ling, H., Kim, S.W., Kreis, K., Barriuso, A., Fidler, S., Torralba, A.: Bigdatasetgan:
445"
REFERENCES,0.47378640776699027,"Synthesizing imagenet with pixel-wise annotations (2022)
446"
REFERENCES,0.4747572815533981,"[52] Li, J., Tan, Z., Wan, J., Lei, Z., Guo, G.: Nested collaborative learning for long-tailed visual
447"
REFERENCES,0.47572815533980584,"recognition. In: CVPR. pp. 6949–6958 (2022)
448"
REFERENCES,0.4766990291262136,"[53] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified
449"
REFERENCES,0.47766990291262135,"vision-language understanding and generation (2022)
450"
REFERENCES,0.4786407766990291,"[54] Li, K., Zhang, Y., Li, K., Fu, Y.: Adversarial feature hallucination networks for few-shot
451"
REFERENCES,0.4796116504854369,"learning. In: CVPR (2020)
452"
REFERENCES,0.48058252427184467,"[55] Li, M., Cheung, Y.m., Lu, Y., et al.: Long-tailed visual recognition via gaussian clouded logit
453"
REFERENCES,0.4815533980582524,"adjustment. In: CVPR. pp. 6929–6938 (2022)
454"
REFERENCES,0.4825242718446602,"[56] Li, T., Cao, P., Yuan, Y., Fan, L., Yang, Y., Feris, R.S., Indyk, P., Katabi, D.: Targeted
455"
REFERENCES,0.48349514563106794,"supervised contrastive learning for long-tailed recognition. In: CVPR. pp. 6918–6928 (2022)
456"
REFERENCES,0.48446601941747575,"[57] Liu, B., Cao, Y., Lin, Y., Li, Q., Zhang, Z., Long, M., Hu, H.: Negative margin matters:
457"
REFERENCES,0.4854368932038835,"Understanding margin in few-shot classification. In: ECCV (2020)
458"
REFERENCES,0.48640776699029126,"[58] Liu, X., Zhang, X., Ma, J., Peng, J., et al.: Instaflow: One step is enough for high-quality
459"
REFERENCES,0.487378640776699,"diffusion-based text-to-image generation. In: The Twelfth International Conference on Learn-
460"
REFERENCES,0.4883495145631068,"ing Representations (2023)
461"
REFERENCES,0.4893203883495146,"[59] Liu, Z., Li, S., Wu, D., Liu, Z., Chen, Z., Wu, L., Li, S.Z.: Automix: Unveiling the power of
462"
REFERENCES,0.49029126213592233,"mixup for stronger classifiers. In: Computer Vision–ECCV 2022: 17th European Conference,
463"
REFERENCES,0.4912621359223301,"Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV. pp. 441–458. Springer (2022)
464"
REFERENCES,0.49223300970873785,"[60] Liu, Z., Miao, Z., Zhan, X., Wang, J., Gong, B., Yu, S.X.: Large-scale long-tailed recognition
465"
REFERENCES,0.49320388349514566,"in an open world. In: CVPR (2019)
466"
REFERENCES,0.4941747572815534,"[61] Lu, Y., Wen, L., Liu, J., Liu, Y., Tian, X.: Self-supervision can be a good few-shot learner. In:
467"
REFERENCES,0.49514563106796117,"European Conference on Computer Vision. pp. 740–758. Springer (2022)
468"
REFERENCES,0.4961165048543689,"[62] Luo, X.J., Wang, S., Wu, Z., Sakaridis, C., Cheng, Y., Fan, D.P., Gool, L.V.: Camdiff: Cam-
469"
REFERENCES,0.4970873786407767,"ouflage image augmentation via diffusion model (2023)
470"
REFERENCES,0.4980582524271845,"[63] Luzi, L., Siahkoohi, A., Mayer, P.M., Casco-Rodriguez, J., Baraniuk, R.: Boomerang: Local
471"
REFERENCES,0.49902912621359224,"sampling on image manifolds using diffusion models (2022)
472"
REFERENCES,0.5,"[64] Maji, S., Rahtu, E., Kannala, J., Blaschko, M.B., Vedaldi, A.: Fine-grained visual classifica-
473"
REFERENCES,0.5009708737864078,"tion of aircraft. arXiv preprint arXiv:1306.5151 (2013)
474"
REFERENCES,0.5019417475728155,"[65] Mao, J., Xiao, T., Jiang, Y., Cao, Z.: What can help pedestrian detection? (2017)
475"
REFERENCES,0.5029126213592233,"[66] Medina, C., Devos, A., Grossglauser, M.: Self-supervised prototypical transfer learning for
476"
REFERENCES,0.503883495145631,"few-shot classification. In: ICMLW (2020)
477"
REFERENCES,0.5048543689320388,"[67] Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: Sdedit: Guided image
478"
REFERENCES,0.5058252427184466,"synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073
479"
REFERENCES,0.5067961165048543,"(2021)
480"
REFERENCES,0.5077669902912622,"[68] Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., Salimans, T.: On distilla-
481"
REFERENCES,0.5087378640776699,"tion of guided diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer
482"
REFERENCES,0.5097087378640777,"Vision and Pattern Recognition. pp. 14297–14306 (2023)
483"
REFERENCES,0.5106796116504855,"[69] Menon, A.K., Jayasumana, S., Rawat, A.S., Jain, H., Veit, A., Kumar, S.: Long-tail learning
484"
REFERENCES,0.5116504854368932,"via logit adjustment. In: ICLR (2021)
485"
REFERENCES,0.512621359223301,"[70] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P.,
486"
REFERENCES,0.5135922330097087,"Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang,
487"
REFERENCES,0.5145631067961165,"P.Y., Li, S.W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal,
488"
REFERENCES,0.5155339805825243,"J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without
489"
REFERENCES,0.516504854368932,"supervision (2023)
490"
REFERENCES,0.5174757281553398,"[71] Peebles, W., Zhu, J.Y., Zhang, R., Torralba, A., Efros, A., Shechtman, E.: Gan-supervised
491"
REFERENCES,0.5184466019417475,"dense visual alignment. In: CVPR (2022)
492"
REFERENCES,0.5194174757281553,"[72] Petryk, S., Dunlap, L., Nasseri, K., Gonzalez, J., Darrell, T., Rohrbach, A.: On guid-
493"
REFERENCES,0.5203883495145631,"ing visual attention with language specification. In: Conference on Computer Vision and
494"
REFERENCES,0.5213592233009708,"Pattern Recognition (CVPR) (2022). https://doi.org/10.48550/ARXIV.2202.08926, https:
495"
REFERENCES,0.5223300970873787,"//arxiv.org/abs/2202.08926
496"
REFERENCES,0.5233009708737864,"[73] Prabhu, V., Yenamandra, S., Chattopadhyay, P., Hoffman, J.: Lance: Stress-testing visual
497"
REFERENCES,0.5242718446601942,"models by generating language-guided counterfactual images. Advances in Neural Informa-
498"
REFERENCES,0.525242718446602,"tion Processing Systems 36 (2024)
499"
REFERENCES,0.5262135922330097,"[74] Qiao, S., Liu, C., Shen, W., Yuille, A.: Few-shot image recognition by predicting parameters
500"
REFERENCES,0.5271844660194175,"from activations. In: CVPR (2018)
501"
REFERENCES,0.5281553398058253,"[75] Qin, T., Li, W., Shi, Y., Yang, G.: Unsupervised few-shot learning via distribution shift-based
502"
REFERENCES,0.529126213592233,"augmentation. arxiv:2004.05805 (2020)
503"
REFERENCES,0.5300970873786408,"[76] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell,
504"
REFERENCES,0.5310679611650485,"A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models
505"
REFERENCES,0.5320388349514563,"from natural language supervision. In: ICML (2021)
506"
REFERENCES,0.5330097087378641,"[77] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image
507"
REFERENCES,0.5339805825242718,"generation with clip latents. arXiv preprint arXiv:2204.06125 1(2), 3 (2022)
508"
REFERENCES,0.5349514563106796,"[78] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.:
509"
REFERENCES,0.5359223300970873,"Zero-shot text-to-image generation. In: ICML (2021)
510"
REFERENCES,0.5368932038834952,"[79] Ravi, S., Larochelle, H.: Optimization as a model for few-shot learning. In: ICLR (2017)
511"
REFERENCES,0.537864077669903,"[80] Ren, M., Ravi, S., Triantafillou, E., Snell, J., Swersky, K., Tenenbaum, J.B., Larochelle,
512"
REFERENCES,0.5388349514563107,"H., Zemel, R.S.: Meta-learning for semi-supervised few-shot classification. In: International
513"
REFERENCES,0.5398058252427185,"Conference on Learning Representations (2018), https://openreview.net/forum?id=
514"
REFERENCES,0.5407766990291262,"HJcSzz-CZ
515"
REFERENCES,0.541747572815534,"[81] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image syn-
516"
REFERENCES,0.5427184466019418,"thesis with latent diffusion models. In: CVPR (2022)
517"
REFERENCES,0.5436893203883495,"[82] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image syn-
518"
REFERENCES,0.5446601941747573,"thesis with latent diffusion models (2021)
519"
REFERENCES,0.545631067961165,"[83] Roy, A., Shah, A., Shah, K., Roy, A., Chellappa, R.: Cap2aug: Caption guided image to
520"
REFERENCES,0.5466019417475728,"image data augmentation (2023)
521"
REFERENCES,0.5475728155339806,"[84] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gon-
522"
REFERENCES,0.5485436893203883,"tijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion
523"
REFERENCES,0.5495145631067961,"models with deep language understanding. Advances in Neural Information Processing Sys-
524"
REFERENCES,0.5504854368932038,"tems 35, 36479–36494 (2022)
525"
REFERENCES,0.5514563106796116,"[85] Samuel, D., Chechik, G.: Distributional robustness loss for long-tail learning. In: ICCV
526"
REFERENCES,0.5524271844660195,"(2021)
527"
REFERENCES,0.5533980582524272,"[86] Sankaranarayanan, S., Balaji, Y., Castillo, C.D., Chellappa, R.: Generate to adapt: Aligning
528"
REFERENCES,0.554368932038835,"domains using generative adversarial networks. Conference on Computer Vision and Pattern
529"
REFERENCES,0.5553398058252427,"Recognition (CVPR) (2018)
530"
REFERENCES,0.5563106796116505,"[87] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. arXiv
531"
REFERENCES,0.5572815533980583,"preprint arXiv:2311.17042 (2023)
532"
REFERENCES,0.558252427184466,"[88] Sharmanska, V., Hendricks, L.A., Darrell, T., Quadrianto, N.: Contrastive examples for ad-
533"
REFERENCES,0.5592233009708738,"dressing the tyranny of the majority. CoRR abs/2004.06524 (2020), https://arxiv.org/
534"
REFERENCES,0.5601941747572815,"abs/2004.06524
535"
REFERENCES,0.5611650485436893,"[89] Shipard, J., Wiliem, A., Thanh, K.N., Xiang, W., Fookes, C.: Boosting zero-shot classification
536"
REFERENCES,0.5621359223300971,"with synthetic data diversity via stable diffusion. arXiv preprint arXiv:2302.03298 (2023)
537"
REFERENCES,0.5631067961165048,"[90] Shirekar, O.K., Singh, A., Jamali-Rad, H.: Self-attention message passing for contrastive
538"
REFERENCES,0.5640776699029126,"few-shot learning. In: Proceedings of the IEEE/CVF Winter Conference on Applications of
539"
REFERENCES,0.5650485436893203,"Computer Vision (WACV). pp. 5426–5436 (January 2023)
540"
REFERENCES,0.5660194174757281,"[91] Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep learning.
541"
REFERENCES,0.566990291262136,"Journal of big data 6(1), 1–48 (2019)
542"
REFERENCES,0.5679611650485437,"[92] Singh, A.R., Jamali-Rad, H.: Transductive decoupled variational inference for few-shot
543"
REFERENCES,0.5689320388349515,"classification. Transactions on Machine Learning Research (2023), https://openreview.
544"
REFERENCES,0.5699029126213592,"net/forum?id=bomdTc9HyL
545"
REFERENCES,0.570873786407767,"[93] Snell, J., Swersky, K., Zemel, R.: Prototypical networks for few-shot learning. In: Advances
546"
REFERENCES,0.5718446601941748,"in Neural Information Processing Systems (2017)
547"
REFERENCES,0.5728155339805825,"[94] Su, J.C., Maji, S., Hariharan, B.: When does self-supervision improve few-shot learning? In:
548"
REFERENCES,0.5737864077669903,"ECCV (2020)
549"
REFERENCES,0.574757281553398,"[95] Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.H., Hospedales, T.M.: Learning to compare:
550"
REFERENCES,0.5757281553398058,"Relation network for few-shot learning. In: CVPR (2018)
551"
REFERENCES,0.5766990291262136,"[96] Tang, K., Huang, J., Zhang, H.: Long-tailed classification by keeping the good and removing
552"
REFERENCES,0.5776699029126213,"the bad momentum causal effect. NeurIPS 33, 1513–1524 (2020)
553"
REFERENCES,0.5786407766990291,"[97] Tian, C., Wang, W., Zhu, X., Dai, J., Qiao, Y.: Vl-ltr: Learning class-wise visual-linguistic
554"
REFERENCES,0.579611650485437,"representation for long-tailed visual recognition. In: ECCV 2022 (2022)
555"
REFERENCES,0.5805825242718446,"[98] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´egou, H.: Training data-
556"
REFERENCES,0.5815533980582525,"efficient image transformers and distillation through attention (2021)
557"
REFERENCES,0.5825242718446602,"[99] Touvron, H., Cord, M., J´egou, H.: Deit iii: Revenge of the vit. In: ECCV (2022)
558"
REFERENCES,0.583495145631068,"[100] Trabucco, B., Doherty, K., Gurinas, M.A., Salakhutdinov, R.: Effective data augmentation
559"
REFERENCES,0.5844660194174758,"with diffusion models. In: The Twelfth International Conference on Learning Representations
560"
REFERENCES,0.5854368932038835,"(2024), https://openreview.net/forum?id=ZWzUA9zeAg
561"
REFERENCES,0.5864077669902913,"[101] Tritrong, N., Rewatbowornwong, P., Suwajanakorn, S.: Repurposing gans for one-shot se-
562"
REFERENCES,0.587378640776699,"mantic part segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition
563"
REFERENCES,0.5883495145631068,"(CVPR) (2021)
564"
REFERENCES,0.5893203883495146,"[102] Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The caltech-ucsd birds-200-2011
565"
REFERENCES,0.5902912621359223,"dataset (2011)
566"
REFERENCES,0.5912621359223301,"[103] Wang, H., Deng, Z.H.: Contrastive prototypical network with wasserstein confidence penalty.
567"
REFERENCES,0.5922330097087378,"In: European Conference on Computer Vision. pp. 665–682. Springer (2022)
568"
REFERENCES,0.5932038834951456,"[104] Wang, H., Fu, S., He, X., Fang, H., Liu, Z., Hu, H.: Towards calibrated hyper-sphere repre-
569"
REFERENCES,0.5941747572815534,"sentation via distribution overlap coefficient for long-tailed learning. In: ECCV (2022)
570"
REFERENCES,0.5951456310679611,"[105] Wang, X., Lian, L., Miao, Z., Liu, Z., Yu, S.X.: Long-tailed recognition by routing diverse
571"
REFERENCES,0.596116504854369,"distribution-aware experts. In: ICLR. OpenReview.net (2021)
572"
REFERENCES,0.5970873786407767,"[106] Xu, Y., Li, Y.L., Li, J., Lu, C.: Constructing balance from imbalance for long-tailed image
573"
REFERENCES,0.5980582524271845,"recognition. In: ECCV. pp. 38–56. Springer (2022)
574"
REFERENCES,0.5990291262135923,"[107] Xu, Z., Liu, R., Yang, S., Chai, Z., Yuan, C.: Learning imbalanced data with vision trans-
575"
REFERENCES,0.6,"formers (2023)
576"
REFERENCES,0.6009708737864078,"[108] Xuan, H., Stylianou, A., Liu, X., Pless, R.: Hard negative examples are hard, but useful
577"
REFERENCES,0.6019417475728155,"(2021)
578"
REFERENCES,0.6029126213592233,"[109] Ye, H.J., Hu, H., Zhan, D.C., Sha, F.: Few-shot learning via embedding adaptation with
579"
REFERENCES,0.6038834951456311,"set-to-set functions. In: CVPR (2020)
580"
REFERENCES,0.6048543689320388,"[110] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to
581"
REFERENCES,0.6058252427184466,"train strong classifiers with localizable features. In: ICCV. pp. 6023–6032 (2019)
582"
REFERENCES,0.6067961165048543,"[111] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk mini-
583"
REFERENCES,0.6077669902912621,"mization. In: ICLR (2018)
584"
REFERENCES,0.6087378640776699,"[112] Zhang, S., Li, Z., Yan, S., He, X., Sun, J.: Distribution alignment: A unified framework for
585"
REFERENCES,0.6097087378640776,"long-tail visual recognition. In: CVPR. pp. 2361–2370 (2021)
586"
REFERENCES,0.6106796116504855,"[113] Zhang, Y., Hooi, B., Hong, L., Feng, J.: Test-agnostic long-tailed recognition by test-time
587"
REFERENCES,0.6116504854368932,"aggregating diverse experts with self-supervision. arXiv preprint arXiv:2107.09249 (2021)
588"
REFERENCES,0.612621359223301,"[114] Zhang, Y., Ling, H., Gao, J., Yin, K., Lafleche, J.F., Barriuso, A., Torralba, A., Fidler, S.:
589"
REFERENCES,0.6135922330097088,"Datasetgan: Efficient labeled data factory with minimal human effort. In: CVPR (2021)
590"
REFERENCES,0.6145631067961165,"[115] Zhong, Z., Cui, J., Liu, S., Jia, J.: Improving calibration for long-tailed recognition. In:
591"
REFERENCES,0.6155339805825243,"CVPR. pp. 16489–16498. Computer Vision Foundation / IEEE (2021)
592"
REFERENCES,0.616504854368932,"[116] Zhou, Z., Qiu, X., Xie, J., Wu, J., Zhang, C.: Binocular mutual learning for improving few-
593"
REFERENCES,0.6174757281553398,"shot classification. In: ICCV (2021)
594"
REFERENCES,0.6184466019417476,"[117] Zhu, J., Wang, Z., Chen, J., Chen, Y.P.P., Jiang, Y.G.: Balanced contrastive learning for long-
595"
REFERENCES,0.6194174757281553,"tailed visual recognition. In: CVPR. pp. 6908–6917 (2022)
596"
REFERENCES,0.6203883495145631,"A
Appendix
597"
REFERENCES,0.6213592233009708,"A.1
Analyzing GeNIe, GeNIe-Ada’s Class-Probabilities
598"
REFERENCES,0.6223300970873786,"The core aim of GeNIe and GeNIe-Ada is to address the failure modes of a classifier
599"
REFERENCES,0.6233009708737864,"by generating challenging samples located near the decision boundary of each class pair,
600"
REFERENCES,0.6242718446601941,"which facilitates the learning process in effectively enhancing the decision boundary between
601"
REFERENCES,0.625242718446602,"classes.
As summarized in Table 4 and illustrated in Fig. 5, we have empirically corrob-
602"
REFERENCES,0.6262135922330098,"orated that GeNIe and GeNIe-Ada can respectively produce samples Xr, Xr∗that are nega-
603"
REFERENCES,0.6271844660194175,"tive with respect to the source image XS, while semantically belonging to the class T.
To"
REFERENCES,0.6281553398058253,"Figure A1: P(YS|Xr) and P(YT |Xr) for r ∈{0.5, 0.6, 0.7, 0.8, 0.9}. On average, the classifier confidently
predicts the source class more than the target class for Xr for r = 0.5, and vice-versa for r = 0.8, 0.9.
However, for r = 0.6, 0.7, the classifier struggles to classify Xr, indicating that the augmented samples are
located closer to the decision boundary. 604"
REFERENCES,0.629126213592233,"further analyze the effectiveness of GeNIe and GeNIe-Ada, we compare the source class-
605"
REFERENCES,0.6300970873786408,"probabilities P(YS|Xr) and target-class probabilities P(YS|Xr) of augmented samples Xr.
606"
REFERENCES,0.6310679611650486,"Figure A2:
Significant over-
lap
between P(YS|Xr∗) and
P(YT |Xr∗) indicates high class-
confusion for augmented sam-
ples generated by GeNIe-Ada."
REFERENCES,0.6320388349514563,"To compute these class probabilities, we first fit an SVM classifier
607"
REFERENCES,0.6330097087378641,"(as followed in UniSiam [61]) only on the labelled support set em-
608"
REFERENCES,0.6339805825242718,"beddings of each episode in the miniImagenet test dataset. Then,
609"
REFERENCES,0.6349514563106796,"we perform inference using each episode’s SVM classifier on its re-
610"
REFERENCES,0.6359223300970874,"spective Xr’s and extract its class probabilities of belonging to its
611"
REFERENCES,0.6368932038834951,"source class S and target class T. These per augmentation-sample
612"
REFERENCES,0.6378640776699029,"source and target class probabilities are then averaged for each
613"
REFERENCES,0.6388349514563106,"episode for each r ∈{0.5, 0.6, 0.7, 0.8, 0.9} in the case of GeNIe
614"
REFERENCES,0.6398058252427185,"and for the optimal r = r∗per sample in the case of GeNIe-Ada,
615"
REFERENCES,0.6407766990291263,"plotted as density plots in Fig. A1, Fig. A2, respectively. Fig. A1
616"
REFERENCES,0.641747572815534,"illustrates that P(YS|Xr) and P(YT |Xr) have significant overlap
617"
REFERENCES,0.6427184466019418,"in the case of r ∈{0.6, 0.7} indicating class-confusion for Xr.
618"
REFERENCES,0.6436893203883495,"Furthermore, Fig. A2 illustrates that when using the optimal r = r∗
619"
REFERENCES,0.6446601941747573,"found by GeNIe-Ada per sample, P(YS|Xr) and P(YT |Xr) signif-
620"
REFERENCES,0.6456310679611651,"icantly overlap around probability scores of 0.2 −0.45, indicating
621"
REFERENCES,0.6466019417475728,"class confusion for GeNIe-Ada augmentations. This corroborates
622"
REFERENCES,0.6475728155339806,"with our analysis in Section 4.3, Table 4 and additionally empiri-
623"
REFERENCES,0.6485436893203883,"cally proves that the augmented samples generated by GeNIe for
624"
REFERENCES,0.6495145631067961,"r ∈{0.6, 0.7} and GeNIe-Ada for r = r∗are actually located near
625"
REFERENCES,0.6504854368932039,"the decision boundary of each class pair.
626"
REFERENCES,0.6514563106796116,"A.2
Fine-grained Few-shot Classification
627"
REFERENCES,0.6524271844660194,"To further investigate the impact of the proposed method, we compare GeNIe with other text-based
628"
REFERENCES,0.6533980582524271,"data augmentation techniques across four distinct fine-grained datasets in a 20-way, 1-shot classifi-
629"
REFERENCES,0.654368932038835,"cation setting. We employ the pre-trained DINOV2 ViT-G [70] backbone as a feature extractor to
630"
REFERENCES,0.6553398058252428,"derive features from training images. Subsequently, an SVM classifier is trained on these features,
631"
REFERENCES,0.6563106796116505,"and we report the Top-1 accuracy of the model on the test set.
632"
REFERENCES,0.6572815533980583,"Datasets: We assess our method on several datasets: Food101 [5] with 101 classes of various foods,
633"
REFERENCES,0.658252427184466,"CUB200 [102] with 200 bird species classes, Cars196 [48] with 196 car model classes, and FGVC-
634"
REFERENCES,0.6592233009708738,"Aircraft [64] with 41 aircraft manufacturer classes. We provide detailed information around fine-
635"
REFERENCES,0.6601941747572816,"grained datasets in Table A2. The reported metric is the average Top-1 accuracy over 100 episodes.
636"
REFERENCES,0.6611650485436893,"Each episode involves sampling 20 classes and 1-shot from the training set, with the final model
637"
REFERENCES,0.6621359223300971,"evaluated on the respective test set.
638"
REFERENCES,0.6631067961165048,"Implementation Details: We enhance the basic prompt by incorporating the superclass name for
639"
REFERENCES,0.6640776699029126,"the fine-grained dataset: “A photo of a <target class>, a type of <superclass>”. For instance,
640"
REFERENCES,0.6650485436893204,"in the food dataset and the burger class, our prompt reads: “A photo of a burger, a type of food.” No
641"
REFERENCES,0.6660194174757281,"additional augmentation is used for generative methods in this context. We generate 19 samples for
642"
REFERENCES,0.6669902912621359,"both cases of our method and also the baseline with weak augmentation.
643"
REFERENCES,0.6679611650485436,"Results: Table A1 summarizes the results. GeNIe helps outperform all other baselines and aug-
644"
REFERENCES,0.6689320388349514,"mentations, including Txt2Img, by margins upto 0.5% on CUB200 [102], 6.6% on Cars196 [48],
645"
REFERENCES,0.6699029126213593,"0.1% on Food101 [5] and 5.3% on FGVC-Aircraft [64]. Notably, GeNIe exhibits great effectiveness
646"
REFERENCES,0.670873786407767,"in more challenging datasets, outperforming the baseline with traditional augmentation by about
647"
REFERENCES,0.6718446601941748,"38% for the Cars dataset and by roughly 17% for the Aircraft dataset. It can be observed here that
648"
REFERENCES,0.6728155339805825,"GeNIe-Ada performs on-par with GeNIe with a fixed noise level, eliminating the necessity for noise
649"
REFERENCES,0.6737864077669903,"level search in GeNIe.
650"
REFERENCES,0.6747572815533981,"Table A1: Few-shot Learning on Fine-grained dataset: We utilize an SVM classifier trained atop the DI-
NOV2 ViT-G pretrained backbone, reporting Top-1 accuracy for the test set of each dataset. The baseline is
an SVM trained on the same backbone using weak augmentation. Across all datasets, GeNIe surpasses this
baseline."
REFERENCES,0.6757281553398058,"Method
Birds
Cars
Foods
Aircraft
CUB200 [102]
Cars196 [48]
Food101 [5]
Aircraft [64]"
REFERENCES,0.6766990291262136,"Baseline
90.3
49.8
82.9
29.2
Img2ImgL[63]
90.7
50.4
87.4
31.0
Img2ImgH[63]
91.3
56.4
91.7
34.7
Txt2Img[35]
92.0
81.3
93.0
41.7
GeNIe (r=0.5)
92.0
84.6
91.5
39.8
GeNIe (r=0.6)
92.2
87.1
92.5
45.0
GeNIe (r=0.7)
92.5
87.9
92.9
47.0
GeNIe (r=0.8)
92.5
87.7
93.1
46.5
GeNIe (r=0.9)
92.4
87.1
93.1
45.7
GeNIe-Ada
92.6
87.9
93.1
46.9"
REFERENCES,0.6776699029126214,"Table A2: Train and test split details of the fine-grained datasets. We use the provided train set for few-shot
task generation, and the provided test sets for our evaluation. For the Aircraft dataset we use manufacturer
hierarchy."
REFERENCES,0.6786407766990291,"Dataset
Classes
Train
Test
samples
samples"
REFERENCES,0.6796116504854369,"CUB200 [102]
200
5994
5794
Food101 [5]
101
75750
25250
Cars [48]
196
8144
8041
Aircraft [64]
41
6,667
3333"
REFERENCES,0.6805825242718446,"A.3
Few-shot Classification with ResNet-34 on tieredImagenet
651"
REFERENCES,0.6815533980582524,"We follow the same evaluation protocol here as mentioned in section 4.1. As summarized in Ta-
652"
REFERENCES,0.6825242718446602,"ble A3, GeNIe and GeNIe-Ada outperform all other classical and generative data augmentation
653"
REFERENCES,0.683495145631068,"techniques.
654"
REFERENCES,0.6844660194174758,"A.4
Additional details of Long-Tail experiments
655"
REFERENCES,0.6854368932038835,"We present a comprehensive version of Table 3 to benchmark the performance with different back-
656"
REFERENCES,0.6864077669902913,"bone architectures (e.g., ResNet50) and to compare against previous long-tail baselines; this is de-
657"
REFERENCES,0.6873786407766991,"tailed in Table A4.
658"
REFERENCES,0.6883495145631068,"Implementation Details of LViT: We download the pre-trained ViT-B of LViT [107] and finetune
659"
REFERENCES,0.6893203883495146,"it with Bal-BCE loss proposed therein on the augmented dataset. Training takes 2 hours on four
660"
REFERENCES,0.6902912621359223,"NVIDIA RTX 3090 GPUs. We use the same hyperparameters as in [107] for finetuning: 100 epochs,
661"
REFERENCES,0.6912621359223301,"lr = 0.008, batch size of 1024, CutMix and MixUp for the data augmentation.
662"
REFERENCES,0.6922330097087379,"Implementation Details of VL-LTR: We use the official code of VL-LTR [97] for our experiments.
663"
REFERENCES,0.6932038834951456,"We use a pre-trained CLIP ResNet-50 backbone. We followed the hyperparameters reported in VL-
664"
REFERENCES,0.6941747572815534,"Table A3:
tiered-ImageNet: Accuracies (% ± std) for 5-way, 1-shot and 5-way, 5-shot classification set-
tings on the test-set. We compare against various SOTA supervised and unsupervised few-shot classification
baselines as well as other augmentation methods, with UniSiam [61] pre-trained ResNet-34 backbone."
REFERENCES,0.6951456310679611,ResNet-34
REFERENCES,0.6961165048543689,"Augmentation
Method
Pre-training
1-shot
5-shot"
REFERENCES,0.6970873786407767,"Weak
MAML + dist [29]
sup.
51.7±1.8
70.3±1.7
Weak
ProtoNet [93]
sup.
52.0±1.2
72.1±1.5"
REFERENCES,0.6980582524271844,"Weak
UniSiam + dist [61]
unsup.
68.7±0.4
85.7±0.3
Weak
UniSiam [61]
unsup.
65.0±0.7
82.5±0.5
Strong
UniSiam [61]
unsup.
64.8±0.7
82.4±0.5
CutMix [110]
UniSiam [61]
unsup.
63.8±0.7
80.3±0.6
MixUp [111]
UniSiam [61]
unsup.
64.1±0.7
80.0±0.6
Img2ImgL[63]
UniSiam [61]
unsup.
66.1±0.7
83.1±0.5
Img2ImgH[63]
UniSiam [61]
unsup.
70.4±0.7
84.7±0.5
Txt2Img[35]
UniSiam [61]
unsup.
75.0±0.6
85.4±0.4
DAFusion [100]
UniSiam [61]
unsup.
64.1±2.1
82.8±1.4
GeNIe (Ours)
UniSiam [61]
unsup.
75.7±0.6
86.0±0.4
GeNIe-Ada (Ours)
UniSiam [61]
unsup.
76.9±0.6
86.3±0.2"
REFERENCES,0.6990291262135923,"LTR [97]. We augment only “Few” category and train the backbone with the VL-LTR [97] method.
665"
REFERENCES,0.7,"Training takes 4 hours on 8 NVIDIA RTX 3090 GPUs.
666"
REFERENCES,0.7009708737864078,"A.5
More Visualizations
667"
REFERENCES,0.7019417475728156,"Additional qualitative results resembling the style presented in Fig. 4 are presented in Fig. A3, and
668"
REFERENCES,0.7029126213592233,"more visuals akin to Fig. 2 can be found in Fig. A4. Moreover, we also present more visualization
669"
REFERENCES,0.7038834951456311,"similar to the style in Fig. 5 in Fig. A5.
670"
REFERENCES,0.7048543689320388,"Table A4:
Long-Tailed ImageNet-LT: We compare different augmentation methods on ImageNet-LT and
report Top-1 accuracy for “Few”, “Medium”, and “Many” sets. † indicates results with ResNeXt50. ∗: indicates
training with 384 resolution so is not directly comparable with other methods with 224 resolution. On the “Few”
set and LiVT method, our augmentations improve the accuracy by 11.7 points compared to LiVT original
augmentation and 4.4 points compared to Txt2Img."
REFERENCES,0.7058252427184466,ResNet-50
REFERENCES,0.7067961165048544,"Method
Many
Med.
Few
Overall Acc"
REFERENCES,0.7077669902912621,"CE [21]
64.0
33.8
5.8
41.6
LDAM [7]
60.4
46.9
30.7
49.8
c-RT [45]
61.8
46.2
27.3
49.6
τ-Norm [45]
59.1
46.9
30.7
49.4
Causal [96]
62.7
48.8
31.6
51.8
Logit Adj. [69]
61.1
47.5
27.6
50.1
RIDE(4E)† [105]
68.3
53.5
35.9
56.8
MiSLAS [115]
62.9
50.7
34.3
52.7
DisAlign [112]
61.3
52.2
31.4
52.9
ACE† [6]
71.7
54.6
23.5
56.6
PaCo† [20]
68.0
56.4
37.2
58.2
TADE† [113]
66.5
57.0
43.5
58.8
TSC [56]
63.5
49.7
30.4
52.4
GCL [55]
63.0
52.7
37.1
54.5
TLC [50]
68.9
55.7
40.8
55.1
BCL† [117]
67.6
54.6
36.6
57.2
NCL [52]
67.3
55.4
39.0
57.7
SAFA [38]
63.8
49.9
33.4
53.1
DOC [104]
65.1
52.8
34.2
55.0
DLSA [106]
67.8
54.5
38.8
57.5
ResLT [18]
63.3
53.3
40.3
55.1
PaCo [19]
68.2
58.7
41.0
60.0
LWS [44]
62.2
48.6
31.8
51.5
Zero-shot CLIP [76]
60.8
59.3
58.6
59.8
DRO-LT [85]
64.0
49.8
33.1
53.5
VL-LTR [97]
77.8
67.0
50.8
70.1
Cap2Aug [83]
78.5
67.7
51.9
70.9
GeNIe-Ada
79.2
64.6
59.5
71.5
ViT-B"
REFERENCES,0.7087378640776699,"LiVT* [107]
76.4
59.7
42.7
63.8"
REFERENCES,0.7097087378640776,"ViT [24]
50.5
23.5
6.9
31.6
MAE [33]
74.7
48.2
19.4
54.5
DeiT [99]
70.4
40.9
12.8
48.4
LiVT [107]
73.6
56.4
41.0
60.9
LiVT + Img2ImgL
74.3
56.4
34.3
60.5
LiVT + Img2ImgH
73.8
56.4
45.3
61.6
LiVT + Txt2Img
74.9
55.6
48.3
62.2
LiVT + GeNIe (r=0.8)
74.5
56.7
50.9
62.8
LiVT + GeNIe-Ada
74.0
56.9
52.7
63.1"
REFERENCES,0.7106796116504854,"Figure A3: Visualization of Generative Samples: More visualization akin to Fig. 4. We compare GeNIe with
two baselines: Img2ImgL augmentation uses both image and text prompt from the same category, resulting in
less challenging examples. Txt2Img augmentation generates images based solely on a text prompt, potentially
deviating from the task’s visual domain. GeNIe augmentation incorporates the target category name in the text
prompt along with the source image, producing desired images with an optimal amount of noise, and balancing
the impact of the source image and text prompt."
REFERENCES,0.7116504854368932,"Figure A4: Effect of noise in GeNIe: Akin to Fig. 2, we use GeNIe to create augmentations with varying noise
levels. As is illustrated in the examples above, a reduced amount of noise leads to images closely mirroring the
semantics of the source images, causing a misalignment with the intended target label."
REFERENCES,0.7126213592233009,"Figure A5: Effect of noise in GeNIe: Similar to Fig. 5, we pass all the generated augmentations through the
DinoV2 ViT-G model, which acts as our oracle model, to obtain their associated embeddings. Subsequently,
we employ PCA for visualization purposes. The visualization reveals that the magnitude of semantic transfor-
mations is contingent upon both the source image and the specified target category."
REFERENCES,0.7135922330097088,"NeurIPS Paper Checklist
671"
CLAIMS,0.7145631067961165,"1. Claims
672"
CLAIMS,0.7155339805825243,"Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
673"
CLAIMS,0.7165048543689321,"contributions and scope?
674"
CLAIMS,0.7174757281553398,"Answer: [Yes]
675"
CLAIMS,0.7184466019417476,"Justification: We demonstrate the effectiveness of our augmentation method through empirical
676"
CLAIMS,0.7194174757281553,"comparison with four different generative augmentation baselines across two scenarios: few-shot
677"
CLAIMS,0.7203883495145631,"and long-tail classification. Additionally, we perform analytical experiments on our augmented
678"
CLAIMS,0.7213592233009709,"samples to illustrate their nature as hard negatives.
679"
CLAIMS,0.7223300970873786,"Guidelines:
680"
CLAIMS,0.7233009708737864,"• The answer NA means that the abstract and introduction do not include the claims made in the
681"
CLAIMS,0.7242718446601941,"paper.
682"
CLAIMS,0.7252427184466019,"• The abstract and/or introduction should clearly state the claims made, including the contributions
683"
CLAIMS,0.7262135922330097,"made in the paper and important assumptions and limitations. A No or NA answer to this question
684"
CLAIMS,0.7271844660194174,"will not be perceived well by the reviewers.
685"
CLAIMS,0.7281553398058253,"• The claims made should match theoretical and experimental results, and reflect how much the
686"
CLAIMS,0.7291262135922331,"results can be expected to generalize to other settings.
687"
CLAIMS,0.7300970873786408,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
688"
CLAIMS,0.7310679611650486,"attained by the paper.
689"
LIMITATIONS,0.7320388349514563,"2. Limitations
690"
LIMITATIONS,0.7330097087378641,"Question: Does the paper discuss the limitations of the work performed by the authors?
691"
LIMITATIONS,0.7339805825242719,"Answer: [Yes]
692"
LIMITATIONS,0.7349514563106796,"Justification: We discuss about the limitations of our method in Sec 5
693"
LIMITATIONS,0.7359223300970874,"Guidelines:
694"
LIMITATIONS,0.7368932038834951,"• The answer NA means that the paper has no limitation while the answer No means that the paper
695"
LIMITATIONS,0.7378640776699029,"has limitations, but those are not discussed in the paper.
696"
LIMITATIONS,0.7388349514563107,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
697"
LIMITATIONS,0.7398058252427184,"• The paper should point out any strong assumptions and how robust the results are to violations of
698"
LIMITATIONS,0.7407766990291262,"these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
699"
LIMITATIONS,0.7417475728155339,"asymptotic approximations only holding locally). The authors should reflect on how these as-
700"
LIMITATIONS,0.7427184466019418,"sumptions might be violated in practice and what the implications would be.
701"
LIMITATIONS,0.7436893203883496,"• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
702"
LIMITATIONS,0.7446601941747573,"on a few datasets or with a few runs. In general, empirical results often depend on implicit
703"
LIMITATIONS,0.7456310679611651,"assumptions, which should be articulated.
704"
LIMITATIONS,0.7466019417475728,"• The authors should reflect on the factors that influence the performance of the approach. For
705"
LIMITATIONS,0.7475728155339806,"example, a facial recognition algorithm may perform poorly when image resolution is low or
706"
LIMITATIONS,0.7485436893203884,"images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide
707"
LIMITATIONS,0.7495145631067961,"closed captions for online lectures because it fails to handle technical jargon.
708"
LIMITATIONS,0.7504854368932039,"• The authors should discuss the computational efficiency of the proposed algorithms and how they
709"
LIMITATIONS,0.7514563106796116,"scale with dataset size.
710"
LIMITATIONS,0.7524271844660194,"• If applicable, the authors should discuss possible limitations of their approach to address prob-
711"
LIMITATIONS,0.7533980582524272,"lems of privacy and fairness.
712"
LIMITATIONS,0.7543689320388349,"• While the authors might fear that complete honesty about limitations might be used by reviewers
713"
LIMITATIONS,0.7553398058252427,"as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t
714"
LIMITATIONS,0.7563106796116504,"acknowledged in the paper. The authors should use their best judgment and recognize that indi-
715"
LIMITATIONS,0.7572815533980582,"vidual actions in favor of transparency play an important role in developing norms that preserve
716"
LIMITATIONS,0.7582524271844661,"the integrity of the community. Reviewers will be specifically instructed to not penalize honesty
717"
LIMITATIONS,0.7592233009708738,"concerning limitations.
718"
THEORY ASSUMPTIONS AND PROOFS,0.7601941747572816,"3. Theory Assumptions and Proofs
719"
THEORY ASSUMPTIONS AND PROOFS,0.7611650485436893,"Question: For each theoretical result, does the paper provide the full set of assumptions and a
720"
THEORY ASSUMPTIONS AND PROOFS,0.7621359223300971,"complete (and correct) proof?
721"
THEORY ASSUMPTIONS AND PROOFS,0.7631067961165049,"Answer: [NA]
722"
THEORY ASSUMPTIONS AND PROOFS,0.7640776699029126,"Justification: We do not have theoretical results.
723"
THEORY ASSUMPTIONS AND PROOFS,0.7650485436893204,"Guidelines:
724"
THEORY ASSUMPTIONS AND PROOFS,0.7660194174757281,"• The answer NA means that the paper does not include theoretical results.
725"
THEORY ASSUMPTIONS AND PROOFS,0.7669902912621359,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
726"
THEORY ASSUMPTIONS AND PROOFS,0.7679611650485437,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
727"
THEORY ASSUMPTIONS AND PROOFS,0.7689320388349514,"• The proofs can either appear in the main paper or the supplemental material, but if they appear in
728"
THEORY ASSUMPTIONS AND PROOFS,0.7699029126213592,"the supplemental material, the authors are encouraged to provide a short proof sketch to provide
729"
THEORY ASSUMPTIONS AND PROOFS,0.7708737864077669,"intuition.
730"
THEORY ASSUMPTIONS AND PROOFS,0.7718446601941747,"• Inversely, any informal proof provided in the core of the paper should be complemented by formal
731"
THEORY ASSUMPTIONS AND PROOFS,0.7728155339805826,"proofs provided in appendix or supplemental material.
732"
THEORY ASSUMPTIONS AND PROOFS,0.7737864077669903,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
733"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7747572815533981,"4. Experimental Result Reproducibility
734"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7757281553398059,"Question: Does the paper fully disclose all the information needed to reproduce the main exper-
735"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7766990291262136,"imental results of the paper to the extent that it affects the main claims and/or conclusions of the
736"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7776699029126214,"paper (regardless of whether the code and data are provided or not)?
737"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7786407766990291,"Answer: [Yes]
738"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7796116504854369,"Justification: We provide implementation details in each experimental section. Additionally, we
739"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7805825242718447,"include the code as supplementary material and plan to release it publicly.
740"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7815533980582524,"Guidelines:
741"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7825242718446602,"• The answer NA means that the paper does not include experiments.
742"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7834951456310679,"• If the paper includes experiments, a No answer to this question will not be perceived well by the
743"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7844660194174757,"reviewers: Making the paper reproducible is important, regardless of whether the code and data
744"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7854368932038835,"are provided or not.
745"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7864077669902912,"• If the contribution is a dataset and/or model, the authors should describe the steps taken to make
746"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7873786407766991,"their results reproducible or verifiable.
747"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7883495145631068,"• Depending on the contribution, reproducibility can be accomplished in various ways. For exam-
748"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7893203883495146,"ple, if the contribution is a novel architecture, describing the architecture fully might suffice, or if
749"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7902912621359224,"the contribution is a specific model and empirical evaluation, it may be necessary to either make
750"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7912621359223301,"it possible for others to replicate the model with the same dataset, or provide access to the model.
751"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7922330097087379,"In general. releasing code and data is often one good way to accomplish this, but reproducibility
752"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7932038834951456,"can also be provided via detailed instructions for how to replicate the results, access to a hosted
753"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7941747572815534,"model (e.g., in the case of a large language model), releasing of a model checkpoint, or other
754"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7951456310679612,"means that are appropriate to the research performed.
755"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7961165048543689,"• While NeurIPS does not require releasing code, the conference does require all submissions to
756"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7970873786407767,"provide some reasonable avenue for reproducibility, which may depend on the nature of the con-
757"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7980582524271844,"tribution. For example
758"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7990291262135922,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce
759"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8,"that algorithm.
760"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8009708737864077,"(b) If the contribution is primarily a new model architecture, the paper should describe the architec-
761"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8019417475728156,"ture clearly and fully.
762"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8029126213592233,"(c) If the contribution is a new model (e.g., a large language model), then there should either be a
763"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8038834951456311,"way to access this model for reproducing the results or a way to reproduce the model (e.g., with
764"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8048543689320389,"an open-source dataset or instructions for how to construct the dataset).
765"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8058252427184466,"(d) We recognize that reproducibility may be tricky in some cases, in which case authors are wel-
766"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8067961165048544,"come to describe the particular way they provide for reproducibility. In the case of closed-source
767"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8077669902912621,"models, it may be that access to the model is limited in some way (e.g., to registered users), but
768"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8087378640776699,"it should be possible for other researchers to have some path to reproducing or verifying the
769"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8097087378640777,"results.
770"
OPEN ACCESS TO DATA AND CODE,0.8106796116504854,"5. Open access to data and code
771"
OPEN ACCESS TO DATA AND CODE,0.8116504854368932,"Question: Does the paper provide open access to the data and code, with sufficient instructions to
772"
OPEN ACCESS TO DATA AND CODE,0.8126213592233009,"faithfully reproduce the main experimental results, as described in supplemental material?
773"
OPEN ACCESS TO DATA AND CODE,0.8135922330097087,"Answer: [Yes]
774"
OPEN ACCESS TO DATA AND CODE,0.8145631067961165,"Justification: We provide implementation details in each experimental section. Additionally, we
775"
OPEN ACCESS TO DATA AND CODE,0.8155339805825242,"include the code as supplementary material and plan to release it publicly.
776"
OPEN ACCESS TO DATA AND CODE,0.816504854368932,"Guidelines:
777"
OPEN ACCESS TO DATA AND CODE,0.8174757281553398,"• The answer NA means that paper does not include experiments requiring code.
778"
OPEN ACCESS TO DATA AND CODE,0.8184466019417476,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
779"
OPEN ACCESS TO DATA AND CODE,0.8194174757281554,"guides/CodeSubmissionPolicy) for more details.
780"
OPEN ACCESS TO DATA AND CODE,0.8203883495145631,"• While we encourage the release of code and data, we understand that this might not be possible,
781"
OPEN ACCESS TO DATA AND CODE,0.8213592233009709,"so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless
782"
OPEN ACCESS TO DATA AND CODE,0.8223300970873786,"this is central to the contribution (e.g., for a new open-source benchmark).
783"
OPEN ACCESS TO DATA AND CODE,0.8233009708737864,"• The instructions should contain the exact command and environment needed to run to reproduce
784"
OPEN ACCESS TO DATA AND CODE,0.8242718446601942,"the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/
785"
OPEN ACCESS TO DATA AND CODE,0.8252427184466019,"guides/CodeSubmissionPolicy) for more details.
786"
OPEN ACCESS TO DATA AND CODE,0.8262135922330097,"• The authors should provide instructions on data access and preparation, including how to access
787"
OPEN ACCESS TO DATA AND CODE,0.8271844660194175,"the raw data, preprocessed data, intermediate data, and generated data, etc.
788"
OPEN ACCESS TO DATA AND CODE,0.8281553398058252,"• The authors should provide scripts to reproduce all experimental results for the new proposed
789"
OPEN ACCESS TO DATA AND CODE,0.829126213592233,"method and baselines. If only a subset of experiments are reproducible, they should state which
790"
OPEN ACCESS TO DATA AND CODE,0.8300970873786407,"ones are omitted from the script and why.
791"
OPEN ACCESS TO DATA AND CODE,0.8310679611650486,"• At submission time, to preserve anonymity, the authors should release anonymized versions (if
792"
OPEN ACCESS TO DATA AND CODE,0.8320388349514564,"applicable).
793"
OPEN ACCESS TO DATA AND CODE,0.8330097087378641,"• Providing as much information as possible in supplemental material (appended to the paper) is
794"
OPEN ACCESS TO DATA AND CODE,0.8339805825242719,"recommended, but including URLs to data and code is permitted.
795"
OPEN ACCESS TO DATA AND CODE,0.8349514563106796,"6. Experimental Setting/Details
796"
OPEN ACCESS TO DATA AND CODE,0.8359223300970874,"Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
797"
OPEN ACCESS TO DATA AND CODE,0.8368932038834952,"how they were chosen, type of optimizer, etc.) necessary to understand the results?
798"
OPEN ACCESS TO DATA AND CODE,0.8378640776699029,"Answer: [Yes]
799"
OPEN ACCESS TO DATA AND CODE,0.8388349514563107,"Justification: We provide implementation details and dataset details in each experimental section.
800"
OPEN ACCESS TO DATA AND CODE,0.8398058252427184,"Guidelines:
801"
OPEN ACCESS TO DATA AND CODE,0.8407766990291262,"• The answer NA means that the paper does not include experiments.
802"
OPEN ACCESS TO DATA AND CODE,0.841747572815534,"• The experimental setting should be presented in the core of the paper to a level of detail that is
803"
OPEN ACCESS TO DATA AND CODE,0.8427184466019417,"necessary to appreciate the results and make sense of them.
804"
OPEN ACCESS TO DATA AND CODE,0.8436893203883495,"• The full details can be provided either with the code, in appendix, or as supplemental material.
805"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8446601941747572,"7. Experiment Statistical Significance
806"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.845631067961165,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
807"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8466019417475729,"information about the statistical significance of the experiments?
808"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8475728155339806,"Answer: [Yes]
809"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8485436893203884,"Justification: We repeat few-shot training for 600 episodes on mini-ImageNet and 1000 episodes
810"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8495145631067961,"on tiered-ImageNet, reporting the mean and variance for each method.
811"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8504854368932039,"Guidelines:
812"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8514563106796117,"• The answer NA means that the paper does not include experiments.
813"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8524271844660194,"• The authors should answer ”Yes” if the results are accompanied by error bars, confidence inter-
814"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8533980582524272,"vals, or statistical significance tests, at least for the experiments that support the main claims of
815"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8543689320388349,"the paper.
816"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8553398058252427,"• The factors of variability that the error bars are capturing should be clearly stated (for exam-
817"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8563106796116505,"ple, train/test split, initialization, random drawing of some parameter, or overall run with given
818"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8572815533980582,"experimental conditions).
819"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.858252427184466,"• The method for calculating the error bars should be explained (closed form formula, call to a
820"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8592233009708737,"library function, bootstrap, etc.)
821"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8601941747572815,"• The assumptions made should be given (e.g., Normally distributed errors).
822"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8611650485436894,"• It should be clear whether the error bar is the standard deviation or the standard error of the mean.
823"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8621359223300971,"• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report
824"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8631067961165049,"a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is
825"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8640776699029126,"not verified.
826"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8650485436893204,"• For asymmetric distributions, the authors should be careful not to show in tables or figures sym-
827"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8660194174757282,"metric error bars that would yield results that are out of range (e.g. negative error rates).
828"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8669902912621359,"• If error bars are reported in tables or plots, The authors should explain in the text how they were
829"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8679611650485437,"calculated and reference the corresponding figures or tables in the text.
830"
EXPERIMENTS COMPUTE RESOURCES,0.8689320388349514,"8. Experiments Compute Resources
831"
EXPERIMENTS COMPUTE RESOURCES,0.8699029126213592,"Question: For each experiment, does the paper provide sufficient information on the computer
832"
EXPERIMENTS COMPUTE RESOURCES,0.870873786407767,"resources (type of compute workers, memory, time of execution) needed to reproduce the experi-
833"
EXPERIMENTS COMPUTE RESOURCES,0.8718446601941747,"ments?
834"
EXPERIMENTS COMPUTE RESOURCES,0.8728155339805825,"Answer: [Yes]
835"
EXPERIMENTS COMPUTE RESOURCES,0.8737864077669902,"Justification: We provide implementation and dataset details in each experimental section. Ad-
836"
EXPERIMENTS COMPUTE RESOURCES,0.874757281553398,"ditionally, we elaborate on the required resources, including GPUs and training hours, for each
837"
EXPERIMENTS COMPUTE RESOURCES,0.8757281553398059,"experiment.
838"
EXPERIMENTS COMPUTE RESOURCES,0.8766990291262136,"Guidelines:
839"
EXPERIMENTS COMPUTE RESOURCES,0.8776699029126214,"• The answer NA means that the paper does not include experiments.
840"
EXPERIMENTS COMPUTE RESOURCES,0.8786407766990292,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
841"
EXPERIMENTS COMPUTE RESOURCES,0.8796116504854369,"provider, including relevant memory and storage.
842"
EXPERIMENTS COMPUTE RESOURCES,0.8805825242718447,"• The paper should provide the amount of compute required for each of the individual experimental
843"
EXPERIMENTS COMPUTE RESOURCES,0.8815533980582524,"runs as well as estimate the total compute.
844"
EXPERIMENTS COMPUTE RESOURCES,0.8825242718446602,"• The paper should disclose whether the full research project required more compute than the ex-
845"
EXPERIMENTS COMPUTE RESOURCES,0.883495145631068,"periments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into
846"
EXPERIMENTS COMPUTE RESOURCES,0.8844660194174757,"the paper).
847"
CODE OF ETHICS,0.8854368932038835,"9. Code Of Ethics
848"
CODE OF ETHICS,0.8864077669902912,"Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
849"
CODE OF ETHICS,0.887378640776699,"Code of Ethics https://neurips.cc/public/EthicsGuidelines?
850"
CODE OF ETHICS,0.8883495145631068,"Answer: [Yes]
851"
CODE OF ETHICS,0.8893203883495145,"Justification: We reviewed the NeurIPS Code of Ethics.
852"
CODE OF ETHICS,0.8902912621359224,"Guidelines:
853"
CODE OF ETHICS,0.8912621359223301,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
854"
CODE OF ETHICS,0.8922330097087379,"• If the authors answer No, they should explain the special circumstances that require a deviation
855"
CODE OF ETHICS,0.8932038834951457,"from the Code of Ethics.
856"
CODE OF ETHICS,0.8941747572815534,"• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
857"
CODE OF ETHICS,0.8951456310679612,"to laws or regulations in their jurisdiction).
858"
BROADER IMPACTS,0.8961165048543689,"10. Broader Impacts
859"
BROADER IMPACTS,0.8970873786407767,"Question: Does the paper discuss both potential positive societal impacts and negative societal
860"
BROADER IMPACTS,0.8980582524271845,"impacts of the work performed?
861"
BROADER IMPACTS,0.8990291262135922,"Answer: [Yes]
862"
BROADER IMPACTS,0.9,"Justification: We discuss about broader impact in Conclusion.
863"
BROADER IMPACTS,0.9009708737864077,"Guidelines:
864"
BROADER IMPACTS,0.9019417475728155,"• The answer NA means that there is no societal impact of the work performed.
865"
BROADER IMPACTS,0.9029126213592233,"• If the authors answer NA or No, they should explain why their work has no societal impact or
866"
BROADER IMPACTS,0.903883495145631,"why the paper does not address societal impact.
867"
BROADER IMPACTS,0.9048543689320389,"• Examples of negative societal impacts include potential malicious or unintended uses (e.g., dis-
868"
BROADER IMPACTS,0.9058252427184466,"information, generating fake profiles, surveillance), fairness considerations (e.g., deployment of
869"
BROADER IMPACTS,0.9067961165048544,"technologies that could make decisions that unfairly impact specific groups), privacy considera-
870"
BROADER IMPACTS,0.9077669902912622,"tions, and security considerations.
871"
BROADER IMPACTS,0.9087378640776699,"• The conference expects that many papers will be foundational research and not tied to particular
872"
BROADER IMPACTS,0.9097087378640777,"applications, let alone deployments. However, if there is a direct path to any negative applications,
873"
BROADER IMPACTS,0.9106796116504854,"the authors should point it out. For example, it is legitimate to point out that an improvement in
874"
BROADER IMPACTS,0.9116504854368932,"the quality of generative models could be used to generate deepfakes for disinformation. On the
875"
BROADER IMPACTS,0.912621359223301,"other hand, it is not needed to point out that a generic algorithm for optimizing neural networks
876"
BROADER IMPACTS,0.9135922330097087,"could enable people to train models that generate Deepfakes faster.
877"
BROADER IMPACTS,0.9145631067961165,"• The authors should consider possible harms that could arise when the technology is being used
878"
BROADER IMPACTS,0.9155339805825242,"as intended and functioning correctly, harms that could arise when the technology is being used
879"
BROADER IMPACTS,0.916504854368932,"as intended but gives incorrect results, and harms following from (intentional or unintentional)
880"
BROADER IMPACTS,0.9174757281553398,"misuse of the technology.
881"
BROADER IMPACTS,0.9184466019417475,"• If there are negative societal impacts, the authors could also discuss possible mitigation strategies
882"
BROADER IMPACTS,0.9194174757281554,"(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for moni-
883"
BROADER IMPACTS,0.920388349514563,"toring misuse, mechanisms to monitor how a system learns from feedback over time, improving
884"
BROADER IMPACTS,0.9213592233009709,"the efficiency and accessibility of ML).
885"
SAFEGUARDS,0.9223300970873787,"11. Safeguards
886"
SAFEGUARDS,0.9233009708737864,"Question: Does the paper describe safeguards that have been put in place for responsible release of
887"
SAFEGUARDS,0.9242718446601942,"data or models that have a high risk for misuse (e.g., pretrained language models, image generators,
888"
SAFEGUARDS,0.925242718446602,"or scraped datasets)?
889"
SAFEGUARDS,0.9262135922330097,"Answer: [NA]
890"
SAFEGUARDS,0.9271844660194175,"Justification: We believe our work does not have such risks.
891"
SAFEGUARDS,0.9281553398058252,"Guidelines:
892"
SAFEGUARDS,0.929126213592233,"• The answer NA means that the paper poses no such risks.
893"
SAFEGUARDS,0.9300970873786408,"• Released models that have a high risk for misuse or dual-use should be released with necessary
894"
SAFEGUARDS,0.9310679611650485,"safeguards to allow for controlled use of the model, for example by requiring that users adhere to
895"
SAFEGUARDS,0.9320388349514563,"usage guidelines or restrictions to access the model or implementing safety filters.
896"
SAFEGUARDS,0.933009708737864,"• Datasets that have been scraped from the Internet could pose safety risks. The authors should
897"
SAFEGUARDS,0.9339805825242719,"describe how they avoided releasing unsafe images.
898"
SAFEGUARDS,0.9349514563106797,"• We recognize that providing effective safeguards is challenging, and many papers do not require
899"
SAFEGUARDS,0.9359223300970874,"this, but we encourage authors to take this into account and make a best faith effort.
900"
LICENSES FOR EXISTING ASSETS,0.9368932038834952,"12. Licenses for existing assets
901"
LICENSES FOR EXISTING ASSETS,0.9378640776699029,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
902"
LICENSES FOR EXISTING ASSETS,0.9388349514563107,"properly credited and are the license and terms of use explicitly mentioned and properly respected?
903"
LICENSES FOR EXISTING ASSETS,0.9398058252427185,"Answer: [Yes]
904"
LICENSES FOR EXISTING ASSETS,0.9407766990291262,"Justification: We cited all datasets and code used in our paper.
905"
LICENSES FOR EXISTING ASSETS,0.941747572815534,"Guidelines:
906"
LICENSES FOR EXISTING ASSETS,0.9427184466019417,"• The answer NA means that the paper does not use existing assets.
907"
LICENSES FOR EXISTING ASSETS,0.9436893203883495,"• The authors should cite the original paper that produced the code package or dataset.
908"
LICENSES FOR EXISTING ASSETS,0.9446601941747573,"• The authors should state which version of the asset is used and, if possible, include a URL.
909"
LICENSES FOR EXISTING ASSETS,0.945631067961165,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
910"
LICENSES FOR EXISTING ASSETS,0.9466019417475728,"• For scraped data from a particular source (e.g., website), the copyright and terms of service of
911"
LICENSES FOR EXISTING ASSETS,0.9475728155339805,"that source should be provided.
912"
LICENSES FOR EXISTING ASSETS,0.9485436893203884,"• If assets are released, the license, copyright information, and terms of use in the package should
913"
LICENSES FOR EXISTING ASSETS,0.9495145631067962,"be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
914"
LICENSES FOR EXISTING ASSETS,0.9504854368932039,"some datasets. Their licensing guide can help determine the license of a dataset.
915"
LICENSES FOR EXISTING ASSETS,0.9514563106796117,"• For existing datasets that are re-packaged, both the original license and the license of the derived
916"
LICENSES FOR EXISTING ASSETS,0.9524271844660194,"asset (if it has changed) should be provided.
917"
LICENSES FOR EXISTING ASSETS,0.9533980582524272,"• If this information is not available online, the authors are encouraged to reach out to the asset’s
918"
LICENSES FOR EXISTING ASSETS,0.954368932038835,"creators.
919"
NEW ASSETS,0.9553398058252427,"13. New Assets
920"
NEW ASSETS,0.9563106796116505,"Question: Are new assets introduced in the paper well documented and is the documentation pro-
921"
NEW ASSETS,0.9572815533980582,"vided alongside the assets?
922"
NEW ASSETS,0.958252427184466,"Answer: [NA]
923"
NEW ASSETS,0.9592233009708738,"Justification: We do not release new assets.
924"
NEW ASSETS,0.9601941747572815,"Guidelines:
925"
NEW ASSETS,0.9611650485436893,"• The answer NA means that the paper does not release new assets.
926"
NEW ASSETS,0.962135922330097,"• Researchers should communicate the details of the dataset/code/model as part of their submis-
927"
NEW ASSETS,0.9631067961165048,"sions via structured templates. This includes details about training, license, limitations, etc.
928"
NEW ASSETS,0.9640776699029127,"• The paper should discuss whether and how consent was obtained from people whose asset is
929"
NEW ASSETS,0.9650485436893204,"used.
930"
NEW ASSETS,0.9660194174757282,"• At submission time, remember to anonymize your assets (if applicable). You can either create an
931"
NEW ASSETS,0.9669902912621359,"anonymized URL or include an anonymized zip file.
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9679611650485437,"14. Crowdsourcing and Research with Human Subjects
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9689320388349515,"Question: For crowdsourcing experiments and research with human subjects, does the paper in-
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9699029126213592,"clude the full text of instructions given to participants and screenshots, if applicable, as well as
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.970873786407767,"details about compensation (if any)?
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9718446601941747,"Answer: [NA]
937"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9728155339805825,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737864077669903,"Guidelines:
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974757281553398,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757281553398058,"subjects.
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9766990291262136,"• Including this information in the supplemental material is fine, but if the main contribution of the
942"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9776699029126213,"paper involves human subjects, then as much detail as possible should be included in the main
943"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9786407766990292,"paper.
944"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9796116504854369,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
945"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9805825242718447,"labor should be paid at least the minimum wage in the country of the data collector.
946"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9815533980582525,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub-
947"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9825242718446602,"jects
948"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.983495145631068,"Question: Does the paper describe potential risks incurred by study participants, whether such
949"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9844660194174757,"risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or
950"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9854368932038835,"an equivalent approval/review based on the requirements of your country or institution) were ob-
951"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9864077669902913,"tained?
952"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987378640776699,"Answer: [NA]
953"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9883495145631068,"Justification: Our paper does not involve crowdsourcing nor research with human subjects.
954"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9893203883495145,"Guidelines:
955"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9902912621359223,"• The answer NA means that the paper does not involve crowdsourcing nor research with human
956"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9912621359223301,"subjects.
957"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922330097087378,"• Depending on the country in which research is conducted, IRB approval (or equivalent) may be
958"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9932038834951457,"required for any human subjects research. If you obtained IRB approval, you should clearly state
959"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9941747572815534,"this in the paper.
960"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9951456310679612,"• We recognize that the procedures for this may vary significantly between institutions and loca-
961"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.996116504854369,"tions, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their
962"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9970873786407767,"institution.
963"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980582524271845,"• For initial submissions, do not include any information that would break anonymity (if applica-
964"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9990291262135922,"ble), such as the institution conducting the review.
965"
