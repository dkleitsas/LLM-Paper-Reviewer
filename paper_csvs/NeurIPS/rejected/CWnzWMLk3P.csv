Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018181818181818182,"In this paper, we propose a novel centralized Asynchronous Federated Learning
1"
ABSTRACT,0.0036363636363636364,"(FL) framework, FAVAS for training Deep Neural Networks (DNNs) in resource-
2"
ABSTRACT,0.005454545454545455,"constrained environments. Despite its popularity, “classical” federated learning
3"
ABSTRACT,0.007272727272727273,"faces the increasingly difficult task of scaling synchronous communication over
4"
ABSTRACT,0.00909090909090909,"large wireless networks. Moreover, clients typically have different computing
5"
ABSTRACT,0.01090909090909091,"resources and therefore computing speed, which can lead to a significant bias (in
6"
ABSTRACT,0.012727272727272728,"favor of “fast” clients) when the updates are asynchronous. Therefore, practical
7"
ABSTRACT,0.014545454545454545,"deployment of FL requires to handle users with strongly varying computing speed
8"
ABSTRACT,0.016363636363636365,"in communication/resource constrained setting. We provide convergence guaran-
9"
ABSTRACT,0.01818181818181818,"tees for FAVAS in a smooth, non-convex environment and carefully compare the
10"
ABSTRACT,0.02,"obtained convergence guarantees with existing bounds, when they are available.
11"
ABSTRACT,0.02181818181818182,"Experimental results show that the FAVAS algorithm outperforms current methods
12"
ABSTRACT,0.023636363636363636,"on standard benchmarks.
13"
INTRODUCTION,0.025454545454545455,"1
Introduction
14"
INTRODUCTION,0.02727272727272727,"Federated learning, a promising approach for training models from networked agents, involves
15"
INTRODUCTION,0.02909090909090909,"the collaborative aggregation of locally computed updates, such as parameters, under centralized
16"
INTRODUCTION,0.03090909090909091,"orchestration (Koneˇcn`y et al., 2015; McMahan et al., 2017; Kairouz et al., 2021). The primary
17"
INTRODUCTION,0.03272727272727273,"motivation behind this approach is to maintain privacy, as local data is never shared between agents
18"
INTRODUCTION,0.034545454545454546,"and the central server (Zhao et al., 2018; Horváth et al., 2022). However, communication of training
19"
INTRODUCTION,0.03636363636363636,"information between edge devices and the server is still necessary. The central server aggregates the
20"
INTRODUCTION,0.038181818181818185,"local models to update the global model, which is then sent back to the devices. Federated learning
21"
INTRODUCTION,0.04,"helps alleviate privacy concerns, and it distributes the computational load among networked agents.
22"
INTRODUCTION,0.04181818181818182,"However, each agent must have more computational power than is required for inference, leading to a
23"
INTRODUCTION,0.04363636363636364,"computational power bottleneck. This bottleneck is especially important when federated learning is
24"
INTRODUCTION,0.045454545454545456,"used in heterogeneous, cross-device applications.
25"
INTRODUCTION,0.04727272727272727,"Most approaches to centralized federated learning (FL) rely on synchronous operations, as assumed in
26"
INTRODUCTION,0.04909090909090909,"many studies (McMahan et al., 2017; Wang et al., 2021). At each global iteration, a copy of the current
27"
INTRODUCTION,0.05090909090909091,"model is sent from the central server to a selected subset of agents. The agents then update their
28"
INTRODUCTION,0.05272727272727273,"model parameters using their private data and send the model updates back to the server. The server
29"
INTRODUCTION,0.05454545454545454,"aggregates these updates to create a new shared model, and this process is repeated until the shared
30"
INTRODUCTION,0.056363636363636366,"model meets a desired criterion. However, device heterogeneity and communication bottlenecks (such
31"
INTRODUCTION,0.05818181818181818,"as latency and bandwidth) can cause delays, message loss, and stragglers, and the agents selected in
32"
INTRODUCTION,0.06,"each round must wait for the slowest one before starting the next round of computation. This waiting
33"
INTRODUCTION,0.06181818181818182,"time can be significant, especially since nodes may have different computation speeds.
34"
INTRODUCTION,0.06363636363636363,"To address this challenge, researchers have proposed several approaches that enable asynchronous
35"
INTRODUCTION,0.06545454545454546,"communication, resulting in improved scalability of distributed/federated learning (Xie et al., 2019;
36"
INTRODUCTION,0.06727272727272728,"Chen et al., 2020, 2021; Xu et al., 2021). In this case, the central server and local agents typically
37"
INTRODUCTION,0.06909090909090909,"operate with inconsistent versions of the shared model, and synchronization in lockstep is not required,
38"
INTRODUCTION,0.07090909090909091,"even between participants in the same round. As a result, the server can start aggregating client
39"
INTRODUCTION,0.07272727272727272,"updates as soon as they are available, reducing training time and improving scalability in practice and
40"
INTRODUCTION,0.07454545454545454,"theory.
41"
INTRODUCTION,0.07636363636363637,"Contributions.
Our work takes a step toward answering this question by introducing FAVAS, a
42"
INTRODUCTION,0.07818181818181819,"centralized federated learning algorithm designed to accommodate clients with varying computing
43"
INTRODUCTION,0.08,"resources and support asynchronous communication.
44"
INTRODUCTION,0.08181818181818182,"• In this paper, we introduce a new algorithm called FAVAS that uses an unbiased aggregation
45"
INTRODUCTION,0.08363636363636363,"scheme for centralized federated learning with asynchronous communication. Our algorithm
46"
INTRODUCTION,0.08545454545454545,"does not assume that clients computed the same number of epochs while being contacted,
47"
INTRODUCTION,0.08727272727272728,"and we give non-asymptotic complexity bounds for FAVAS in the smooth nonconvex setting.
48"
INTRODUCTION,0.0890909090909091,"We emphasize that the dependence of the bounds on the total number of agents n is improved
49"
INTRODUCTION,0.09090909090909091,"compared to Zakerinia et al. (2022) and does not depend on a maximum delay.
50"
INTRODUCTION,0.09272727272727273,"• Experimental results show that our approach consistently outperforms other asynchronous
51"
INTRODUCTION,0.09454545454545454,"baselines on the challenging TinyImageNet dataset (Le and Yang, 2015).
52"
INTRODUCTION,0.09636363636363636,"Our proposed algorithm FAVAS is designed to allow clients to perform their local steps independently
53"
INTRODUCTION,0.09818181818181818,"of the server’s round structure, using a fully local, possibly outdated version of the model. Upon
54"
INTRODUCTION,0.1,"entering the computation, all clients are given a copy of the global model and perform at most K ≥1
55"
INTRODUCTION,0.10181818181818182,"optimization steps based on their local data. The server randomly selects a group of s clients in each
56"
INTRODUCTION,0.10363636363636364,"server round, which, upon receiving the server’s request, submit an unbiased version of their progress.
57"
INTRODUCTION,0.10545454545454545,"Although they may still be in the middle of the local optimization process, they send reweighted
58"
INTRODUCTION,0.10727272727272727,"contributions so that fast and slow clients contribute equally. The central server then aggregates the
59"
INTRODUCTION,0.10909090909090909,"models and sends selected clients a copy of the current model. The clients take this received server
60"
INTRODUCTION,0.11090909090909092,"model as a new starting point for their next local iteration.
61"
RELATED WORKS,0.11272727272727273,"2
Related Works
62"
RELATED WORKS,0.11454545454545455,"Federated Averaging (FedAvg), also known as local SGD, is a widely used approach in federated
63"
RELATED WORKS,0.11636363636363636,"learning. In this method, each client updates its local model using multiple steps of stochastic gradient
64"
RELATED WORKS,0.11818181818181818,"descent (SGD) to optimize a local objective function. The local devices then submit their model
65"
RELATED WORKS,0.12,"updates to the central server for aggregation, and the server updates its own model parameters by
66"
RELATED WORKS,0.12181818181818181,"averaging the client models before sending the updated server parameters to all clients. FedAvg has
67"
RELATED WORKS,0.12363636363636364,"been shown to achieve high communication efficiency with infrequent synchronization, outperforming
68"
RELATED WORKS,0.12545454545454546,"distributed large mini-batches SGD (Lin et al., 2019).
69"
RELATED WORKS,0.12727272727272726,"However, the use of multiple local epochs in FedAvg can cause each device to converge to the optima
70"
RELATED WORKS,0.1290909090909091,"of its local objective rather than the global objective, a phenomenon known as client drift. This
71"
RELATED WORKS,0.13090909090909092,"problem has been discussed in previous work; see (Karimireddy et al., 2020). Most of these studies
72"
RELATED WORKS,0.13272727272727272,"have focused on synchronous federated learning methods, which have a similar update structure to
73"
RELATED WORKS,0.13454545454545455,"FedAvg (Wang et al., 2020; Karimireddy et al., 2020; Qu et al., 2021; Makarenko et al., 2022; Mao
74"
RELATED WORKS,0.13636363636363635,"et al., 2022; Tyurin and Richtárik, 2022). However, synchronous methods can be disadvantageous
75"
RELATED WORKS,0.13818181818181818,"because they require all clients to wait when one or more clients suffer from high network delays or
76"
RELATED WORKS,0.14,"have more data, and require a longer training time. This results in idle time and wasted computing
77"
RELATED WORKS,0.14181818181818182,"resources.
78"
RELATED WORKS,0.14363636363636365,"Moreover, as the number of nodes in a system increases, it becomes infeasible for the central server
79"
RELATED WORKS,0.14545454545454545,"to perform synchronous rounds among all participants, and synchrony can degrade the performance
80"
RELATED WORKS,0.14727272727272728,"of distributed learning. A simple approach to mitigate this problem is node sampling, e.g. Smith et al.
81"
RELATED WORKS,0.14909090909090908,"(2017); Bonawitz et al. (2019), where the server only communicates with a subset of the nodes in a
82"
RELATED WORKS,0.1509090909090909,"round. But if the number of stragglers is large, the overall training process still suffers from delays.
83"
RELATED WORKS,0.15272727272727274,"Synchronous FL methods are prone to stragglers. One important research direction is based on
84"
RELATED WORKS,0.15454545454545454,"FedAsync (Xie et al., 2019) and subsequent works. The core idea is to update the global model
85"
RELATED WORKS,0.15636363636363637,"immediately when the central server receives a local model. However, when staleness is important,
86"
RELATED WORKS,0.15818181818181817,"performance is similar to FedAvg, so it is suboptimal in practice. ASO-Fed (Chen et al., 2020)
87"
RELATED WORKS,0.16,"proposes to overcome this problem and handles asynchronous FL with local streaming data by
88"
RELATED WORKS,0.1618181818181818,"introducing memory-terms on the local client side. AsyncFedED (Wang et al., 2022) also relies on
89"
RELATED WORKS,0.16363636363636364,"the FedAsync instantaneous update strategy and also proposes to dynamically adjust the learning
90"
RELATED WORKS,0.16545454545454547,"rate and the number of local epochs to staleness. Only one local updated model is involved in
91"
RELATED WORKS,0.16727272727272727,"FedAsync-like global model aggregations. As a result, a larger number of training epochs are
92"
RELATED WORKS,0.1690909090909091,"required and the frequency of communication between the server and the workers increases greatly,
93"
RELATED WORKS,0.1709090909090909,"resulting in massive bandwidth consumption. From a different perspective, QuAFL (Zakerinia et al.,
94"
RELATED WORKS,0.17272727272727273,"2022) develops a concurrent algorithm that is closer to the FedAvg strategy. QuAFL incorporates
95"
RELATED WORKS,0.17454545454545456,"both asynchronous and compressed communication with convergence guarantees. Each client must
96"
RELATED WORKS,0.17636363636363636,"compute K local steps and can be interrupted by the central server at any time. The client updates
97"
RELATED WORKS,0.1781818181818182,"its model with the (compressed) central version and its current private model. The central server
98"
RELATED WORKS,0.18,"randomly selects s clients and updates the model with the (compressed) received local progress (since
99"
RELATED WORKS,0.18181818181818182,"last contact) and the previous central model. QuAFL works with old variants of the model at each
100"
RELATED WORKS,0.18363636363636363,"step, which slows convergence. However, when time, rather than the number of server rounds, is
101"
RELATED WORKS,0.18545454545454546,"taken into account, QuAFL can provide a speedup because the asynchronous framework does not
102"
RELATED WORKS,0.18727272727272729,"suffer from delays caused by stragglers. A concurrent and asynchronous approach aggregates local
103"
RELATED WORKS,0.1890909090909091,"updates before updating the global model: FedBuff (Nguyen et al., 2022) addresses asynchrony
104"
RELATED WORKS,0.19090909090909092,"using a buffer on the server side. Clients perform local iterations, and the base station updates the
105"
RELATED WORKS,0.19272727272727272,"global model only after Z different clients have completed and sent their local updates. The gradients
106"
RELATED WORKS,0.19454545454545455,"computed on the client side may be stale. The main assumption is that the client computations
107"
RELATED WORKS,0.19636363636363635,"completed at each step come from a uniform distribution across all clients. Fedbuff is asynchronous,
108"
RELATED WORKS,0.19818181818181818,"but is also sensitive to stragglers (must wait until Z different clients have done all local updates).
109"
RELATED WORKS,0.2,"Similarly, Koloskova et al. (2022) focus on Asynchronous SGD, and provide guarantees depending
110"
RELATED WORKS,0.2018181818181818,"on some τmax. Similar to Nguyen et al. (2022) the algorithm is also impacted by stragglers, during
111"
RELATED WORKS,0.20363636363636364,"the transitional regime at least. A recent work by Fraboni et al. (2023) extend the idea of Koloskova
112"
RELATED WORKS,0.20545454545454545,"et al. (2022) by allowing multiple clients to contribute in one round. But this scheme also favors fast
113"
RELATED WORKS,0.20727272727272728,"clients. Liu et al. (2021) does not run on buffers, but develops an Adaptive Asynchronous Federated
114"
RELATED WORKS,0.20909090909090908,"Learning (AAFL) mechanism to deal with speed differences between local devices. Similar to
115"
RELATED WORKS,0.2109090909090909,"FedBuff, in Liu et al. (2021)’s method, only a certain fraction of the locally updated models contribute
116"
RELATED WORKS,0.21272727272727274,"to the global model update. Most convergence guarantees for asynchronous distributed methods
117"
RELATED WORKS,0.21454545454545454,"depend on staleness or gradient delays (Nguyen et al., 2022; Toghani and Uribe, 2022; Koloskova
118"
RELATED WORKS,0.21636363636363637,"et al., 2022). Only Mishchenko et al. (2022) analyzes the asynchronous stochastic gradient descent
119"
RELATED WORKS,0.21818181818181817,"(SGD) independently of the delays in the gradients. However, in the heterogeneous (non-IID) setting,
120"
RELATED WORKS,0.22,"convergence is proved up to an additive term that depends on the dissimilarity limit between the
121"
RELATED WORKS,0.22181818181818183,"gradients of the local and global objective functions.
122"
ALGORITHM,0.22363636363636363,"3
Algorithm
123"
ALGORITHM,0.22545454545454546,"We consider optimization problems in which the components of the objective function (i.e., the data
124"
ALGORITHM,0.22727272727272727,"for machine learning problems) are distributed over n clients, i.e.,
125"
ALGORITHM,0.2290909090909091,"min
w∈Rd R(w); R(w) = 1 n n
X"
ALGORITHM,0.2309090909090909,"i=1
E(x,y)∼pi
data[ℓ(NN(x, w), y)],"
ALGORITHM,0.23272727272727273,"where d is the number of parameters (network weights and biases), n is the total number of clients, ℓ
126"
ALGORITHM,0.23454545454545456,"is the training loss (e.g., cross-entropy or quadratic loss), NN(x, w) is the DNN prediction function,
127"
ALGORITHM,0.23636363636363636,"pi
data is the training distribution on client i. In FL, the distributions pi
data are allowed to differ
128"
ALGORITHM,0.2381818181818182,"between clients (statistical heterogeneity).
129"
ALGORITHM,0.24,"Each client maintains three key values in its local memory: the local model wi, a counter qi, and the
130"
ALGORITHM,0.24181818181818182,"value of the initial model with which it started the iterations wi
init. The counter qi is incremented for
131"
ALGORITHM,0.24363636363636362,"each SGD step the client performs locally until it reaches K, at which point the client stops updating
132"
ALGORITHM,0.24545454545454545,"its local model and waits for the server request. Upon the request to the client i, the local model
133"
ALGORITHM,0.24727272727272728,"and counter qi are reset. If a server request occurs before the K local steps are completed, the client
134"
ALGORITHM,0.24909090909090909,"simply pauses its current training process, reweights its gradient based on the number of local epochs
135"
ALGORITHM,0.2509090909090909,"(defined by Ei
t+1), and sends its current reweighted model to the server.
136"
ALGORITHM,0.25272727272727274,"In Zakerinia et al. (2022), we identified the client update wi =
1
s+1wt−1 +
s
s+1wi as a major
137"
ALGORITHM,0.2545454545454545,"shortcoming. When the number of sampled clients s is large enough,
s
s+1wi dominates the update
138"
ALGORITHM,0.25636363636363635,"and basically no server term are taken into consideration. This leads to a significant client drift. As a
139"
ALGORITHM,0.2581818181818182,Algorithm 1: FAVAS over T iterations. In red are highlighted the differences with QuAFL.
ALGORITHM,0.26,"Input
:Number of steps T, LR η, Selection
Size s, Maximum local steps K ;"
ALGORITHM,0.26181818181818184,"/* At the Central Server
*/"
INITIALIZE,0.2636363636363636,1 Initialize
INITIALIZE,0.26545454545454544,"2
Initialize parameters w0;"
INITIALIZE,0.2672727272727273,"3
Server sends w0 to all clients;"
END,0.2690909090909091,4 end
END,0.27090909090909093,"5 for t = 1, . . . , T do"
GENERATE SET ST OF S CLIENTS UNIFORMLY AT,0.2727272727272727,"6
Generate set St of s clients uniformly at
random;"
GENERATE SET ST OF S CLIENTS UNIFORMLY AT,0.27454545454545454,"7
for all clients i ∈St do"
SERVER RECEIVES WI,0.27636363636363637,"8
Server receives wi
unbiased from client i;"
END,0.2781818181818182,"9
end"
UPDATE CENTRAL SERVER MODEL,0.28,"10
Update central server model
wt ←
1
s+1wt−1 + (
1
s+1
P"
UPDATE CENTRAL SERVER MODEL,0.2818181818181818,"i∈St wi
unbiased);"
UPDATE CENTRAL SERVER MODEL,0.28363636363636363,"11
for all clients i ∈St do"
UPDATE CENTRAL SERVER MODEL,0.28545454545454546,"12
Server sends wt to client i;"
END,0.2872727272727273,"13
end"
END,0.28909090909090907,14 end
END,0.2909090909090909,"/* At Client i
*/"
INITIALIZE,0.2927272727272727,15 Initialize
INITIALIZE,0.29454545454545455,"16
Client receives w0 and K from the Server;"
INITIALIZE,0.2963636363636364,"17
Local variables wi = w0, qi = 0;"
END,0.29818181818181816,18 end
LOOP,0.3,19 Loop
LOOP,0.3018181818181818,"20
Run ClientLocalTraining() concurrently;"
WHEN CONTACTED BY THE SERVER DO,0.30363636363636365,"21
When Contacted by the Server do"
WHEN CONTACTED BY THE SERVER DO,0.3054545454545455,"22
Interrupt ClientLocalTraining();"
WHEN CONTACTED BY THE SERVER DO,0.30727272727272725,"23
Define αi following (3) ;"
SEND WI,0.3090909090909091,"24
Send wi
unbiased := wi
init +
1
αi (wi −wi
init)
to the server;"
SEND WI,0.3109090909090909,"25
Receive wt from the server;"
UPDATE WI,0.31272727272727274,"26
Update wi
init ←wt, wi ←wt, qi ←0;"
UPDATE WI,0.3145454545454546,"27
Restart ClientLocalTraining() from
zero with updated variables;"
END,0.31636363636363635,"28
end"
END,0.3181818181818182,29 end
END,0.32,30 function ClientLocalTraining():
END,0.32181818181818184,"31
while qi < K do"
END,0.3236363636363636,"32
Compute local stochastic gradient egi at wi;"
END,0.32545454545454544,"33
Update local model wi ←wi −ηegi;"
END,0.32727272727272727,"34
Update local counter qi ←qi + 1;"
END,0.3290909090909091,"35
end"
END,0.33090909090909093,"36
Wait();"
END FUNCTION,0.3327272727272727,37 end function
END FUNCTION,0.33454545454545453,"consequence, QuAFL does not perform well in the heterogeneous case (see Section 5). Second, one
140"
END FUNCTION,0.33636363636363636,"can note that the updates in QuAFL are biased in favor of fast clients. Indeed each client computes
141"
END FUNCTION,0.3381818181818182,"gradients at its own pace and can reach different numbers of epochs while being contacted by the
142"
END FUNCTION,0.34,"central server. It is assumed that clients compute the same number of local epochs in the analysis
143"
END FUNCTION,0.3418181818181818,"from Zakerinia et al. (2022), but it is not the case in practice. As a consequence, we propose FAVAS to
144"
END FUNCTION,0.34363636363636363,"deal with asynchronous updates without favoring fast clients. A first improvement is to update local
145"
END FUNCTION,0.34545454545454546,"weight directly with the received central model. Details can be found in Algorithm 1. Another idea
146"
END FUNCTION,0.3472727272727273,"to tackle gradient unbiasedness is to reweight the contributions from each of the s selected clients:
147"
END FUNCTION,0.3490909090909091,"these can be done either by dividing by the (proper) number of locally computed epochs, or by the
148"
END FUNCTION,0.3509090909090909,"expected value of locally computed epochs. In practice, we define the reweight αi = E[Ei
t+1 ∧K],
149"
END FUNCTION,0.3527272727272727,"or αi = P(Ei
t+1 > 0)(Ei
t+1 ∧K), where ∧stands for min. We assume that the server performs
150"
END FUNCTION,0.35454545454545455,"a number of training epochs T ≥1. At each time step t ∈{1, . . . , T}, the server has a model wt.
151"
END FUNCTION,0.3563636363636364,"At initialization, the central server transmits identical parameters w0 to all devices. At each time
152"
END FUNCTION,0.35818181818181816,"step t, the central server selects a subset St of s clients uniformly at random and requests their local
153"
END FUNCTION,0.36,"models. Then, the requested clients submit their reweighted local models back to the server. When
154"
END FUNCTION,0.3618181818181818,"all requested models arrive at the server, the server model is updated based on a simple average (see
155"
END FUNCTION,0.36363636363636365,"Line 10). Finally, the server multicasts the updated server model to all clients in St. In particular, all
156"
END FUNCTION,0.3654545454545455,"clients i /∈St continue to run their individual processes without interruption.
157"
END FUNCTION,0.36727272727272725,"Remark 1. In FAVAS’s setting, we assume that each client i ∈{1, ..., n} keeps a full-precision local
158"
END FUNCTION,0.3690909090909091,"model wi. In order to reduce the computational cost induced by the training process, FAVAS can also
159"
END FUNCTION,0.3709090909090909,"be implemented with a quantization function Q. First, each client computes backpropagation with
160"
END FUNCTION,0.37272727272727274,"respect to its quantized weights Q(wi). That is, the stochastic gradients are unbiased estimates of
161"
END FUNCTION,0.37454545454545457,"∇fi
 
Q
 
wi
. Moreover, the activations computed at forward propagation are quantized. Finally,
162"
END FUNCTION,0.37636363636363634,"the stochastic gradient obtained at backpropagation is quantized before the SGD update. In our
163"
END FUNCTION,0.3781818181818182,"supplementary experiments, we use the logarithmic unbiased quantization method of Chmiel et al.
164"
END FUNCTION,0.38,"(2021).
165"
END FUNCTION,0.38181818181818183,"Table 1: How long one has to wait to reach an ϵ accuracy for non-convex functions. For simplicity,
we ignore all constant terms. Each constant C_ depends on client speeds and represents the unit
of time one has to wait in between two consecutive server steps. L is the Lipschitz constant, and
F := (f(w0) −f∗) is the initial conditions term. ai, b are constants depending on client speeds
statistics, and defined in Theorem 3."
END FUNCTION,0.3836363636363636,"Method
Units of time"
END FUNCTION,0.38545454545454544,FedAvg
END FUNCTION,0.38727272727272727, F Lσ2+(1−s
END FUNCTION,0.3890909090909091,n )KG2
END FUNCTION,0.39090909090909093,"sK
ϵ−2 + FL
1
2 Gϵ−3"
END FUNCTION,0.3927272727272727,2 + LFB2ϵ−1
END FUNCTION,0.39454545454545453,CF edAvg
END FUNCTION,0.39636363636363636,FedBuff
END FUNCTION,0.3981818181818182,"
FL(σ2 + G2)ϵ−2 + FL(( τ 2
max"
END FUNCTION,0.4,"s2
+ 1)(σ2 + nG2))
1
2 ϵ−3"
END FUNCTION,0.4018181818181818,2 + FLϵ−1
END FUNCTION,0.4036363636363636,CF edBuff
END FUNCTION,0.40545454545454546,AsyncSGD
END FUNCTION,0.4072727272727273,"
FL(3σ2 + 4G2)ϵ−2 + FLG(sτavg)
1
2 ϵ−3"
END FUNCTION,0.4090909090909091,"2 + (sτmaxF)
1
2 ϵ−1"
END FUNCTION,0.4109090909090909,CAsyncSGD
END FUNCTION,0.4127272727272727,"QuAFL
1
E2 FLK(σ2 + 2KG2)ϵ−2 +
n√n
E
√"
END FUNCTION,0.41454545454545455,"EsFKL(σ2 + 2KG2)
1
2 ϵ−3"
END FUNCTION,0.4163636363636364,"2 +
1
E√sn√nFBK2Lϵ−1"
END FUNCTION,0.41818181818181815,"FAVAS
FL(σ2 Pn
i
ai"
END FUNCTION,0.42,n + 8G2b)ϵ−2 + n
END FUNCTION,0.4218181818181818,"s FL2(K2σ2 + L2K2G2 + s2σ2 Pn
i
ai"
END FUNCTION,0.42363636363636364,"n + s2G2b)
1
2 ϵ−3"
END FUNCTION,0.4254545454545455,2 + nFB2KLbϵ−1
ANALYSIS,0.42727272727272725,"4
Analysis
166"
ANALYSIS,0.4290909090909091,"In this section we provide complexity bounds for FAVAS in a smooth nonconvex environment.
167"
ANALYSIS,0.4309090909090909,"We introduce an abstraction to model the stochastic optimization process and prove convergence
168"
ANALYSIS,0.43272727272727274,"guarantees for FAVAS.
169"
ANALYSIS,0.43454545454545457,"Preliminaries.
We abstract the optimization process to simplify the analysis. In the proposed
170"
ANALYSIS,0.43636363636363634,"algorithm, each client asynchronously computes its own local updates without taking into account the
171"
ANALYSIS,0.4381818181818182,"server time step t. Here in the analysis, we introduce a different, but statistically equivalent setting.
172"
ANALYSIS,0.44,"At the beginning of each server timestep t, each client maintains a local model wi
t−1. We then assume
173"
ANALYSIS,0.44181818181818183,"that all n clients instantaneously compute local steps from SGD. The update in local step q for a
174"
ANALYSIS,0.44363636363636366,"client i is given by:
175"
ANALYSIS,0.44545454545454544,"ehi
t,q = egi"
ANALYSIS,0.44727272727272727,"wi
t−1 − q−1
X"
ANALYSIS,0.4490909090909091,"s=1
ηehi
t,s ! ,"
ANALYSIS,0.4509090909090909,"where egi represents the stochastic gradient that client i computes for the function fi. We also define
176"
ANALYSIS,0.4527272727272727,"n independent random variables E1
t , . . . , En
t in N. Each random variable Ei
t models the number of
177"
ANALYSIS,0.45454545454545453,"local steps the client i could take before receiving the server request. We then introduce the following
178"
ANALYSIS,0.45636363636363636,"random variable: ehi
t = PEi
t
q=1 ehi
t,q. Compared to Zakerinia et al. (2022), we do not assume that
179"
ANALYSIS,0.4581818181818182,"clients performed the same number of local epochs. Instead, we reweight the sum of the gradients by
180"
ANALYSIS,0.46,"weights αi, which can be either stochastic or deterministic:
181"
ANALYSIS,0.4618181818181818,"αi =
P(Ei
t+1 > 0)(Ei
t+1 ∧K)
stochastic version,
E[Ei
t+1 ∧K]
deterministic version.
(1)"
ANALYSIS,0.4636363636363636,"And we can define the unbiased gradient estimator: ˇhi
t =
1
αi
PEi
t∧K
q=1
ehi
t,q.
182"
ANALYSIS,0.46545454545454545,"Finally, a subset St of s clients is chosen uniformly at random. This subset corresponds to the clients
183"
ANALYSIS,0.4672727272727273,"that send their models to the server at time step t. In the current notation, each client i ∈St sends the
184"
ANALYSIS,0.4690909090909091,"value wi
t−1 −ηˇhi
t to the server. We emphasise that in our abstraction, all clients compute Ei
t local
185"
ANALYSIS,0.4709090909090909,"updates. However, only the clients in St send their updates to the server, and each client i ∈St sends
186"
ANALYSIS,0.4727272727272727,"only the K first updates. As a result, we introduce the following update equations:
187 

 
"
ANALYSIS,0.47454545454545455,"wt =
1
s+1wt−1 +
1
s+1
P"
ANALYSIS,0.4763636363636364,"i∈St(wi
t−1 −η 1"
ANALYSIS,0.4781818181818182,"αi
PEi
t∧K
s=1
ehi
t,s),
wi
t = wt,
for i ∈St,
wi
t = wi
t−1,
for i /∈St."
ANALYSIS,0.48,"Assumptions and notations.
188"
ANALYSIS,0.4818181818181818,"A1. Uniform Lower Bound: There exists f∗∈R such that f(x) ≥f∗for all x ∈Rd.
189"
ANALYSIS,0.48363636363636364,"A2. Smooth Gradients: For any client i, the gradient ∇fi(x) is L-Lipschitz continuous for some
190"
ANALYSIS,0.48545454545454547,"L > 0, i.e. for all x, y ∈Rd: ∥∇fi(x) −∇fi(y)∥≤L∥x −y∥.
191"
ANALYSIS,0.48727272727272725,"A3. Bounded Variance: For any client i, the variance of the stochastic gradients is bounded by some
192"
ANALYSIS,0.4890909090909091,"σ2 > 0, i.e. for all x ∈Rd: E[
egi(x) −∇fi(x)
2] ≤σ2.
193"
ANALYSIS,0.4909090909090909,"A4. Bounded Gradient Dissimilarity: There exist constants G2 ≥0 and B2 ≥1, such that for all
194"
ANALYSIS,0.49272727272727274,"x ∈Rd: Pn
i=1
∥∇fi(x)∥2"
ANALYSIS,0.49454545454545457,"n
≤G2 + B2∥∇f(x)∥2.
195"
ANALYSIS,0.49636363636363634,"We define the notations required for the analysis. Consider a time step t, a client i, and a local step q.
196"
ANALYSIS,0.49818181818181817,"We define
197 µt =  wt + n
X"
ANALYSIS,0.5,"i=1
wi
t !"
ANALYSIS,0.5018181818181818,/(n + 1)
ANALYSIS,0.5036363636363637,"the average over all node models in the system at a given time t. The first step of the proof is to
198"
ANALYSIS,0.5054545454545455,"compute a preliminary upper bound on the divergence between the local models and their average.
199"
ANALYSIS,0.5072727272727273,"For this purpose, we introduce the Lyapunov function: Φt = ∥wt −µt∥2 + Pn
i=1
wi
t −µt
2 .
200"
ANALYSIS,0.509090909090909,"Upper bounding the expected change in potential.
A key result from our analysis is to upper
201"
ANALYSIS,0.5109090909090909,"bound the change (in expectation) of the aforementioned potential function Φt:
202"
ANALYSIS,0.5127272727272727,"Lemma 2. For any time step t > 0 we have:
203"
ANALYSIS,0.5145454545454545,E [Φt+1] ≤(1 −κ) E [Φt] + 3s2
ANALYSIS,0.5163636363636364,"n η2
n
X"
ANALYSIS,0.5181818181818182,"i=1
E
ˇhi
t+1
2 ,
with κ = 1 n"
ANALYSIS,0.52,"
s(n −s)
2(n + 1)(s + 1) 
."
ANALYSIS,0.5218181818181818,"The intuition behind Lemma 2 is that the potential function Φt remains concentrated around its mean,
204"
ANALYSIS,0.5236363636363637,"apart from deviations induced by the local gradient steps. The full analysis involves many steps and
205"
ANALYSIS,0.5254545454545455,"we refer the reader to Appendix B for complete proofs. In particular, Lemmas 16 and 18 allow us
206"
ANALYSIS,0.5272727272727272,"to examine the scalar product between the expected node progress Pn
i=1 ˇhi
t and the true gradient
207"
ANALYSIS,0.5290909090909091,"evaluated on the mean model ∇f(µt). The next theorem allows us to compute an upper-bound
208"
ANALYSIS,0.5309090909090909,"on the averaged norm-squared of the gradient, a standard quantity studied in nonconvex stochastic
209"
ANALYSIS,0.5327272727272727,"optimization.
210"
ANALYSIS,0.5345454545454545,"Convergence results.
The following statement shows that FAVAS algorithm converges towards a
211"
ANALYSIS,0.5363636363636364,"first-order stationary point, as T the number of global epochs grows.
212"
ANALYSIS,0.5381818181818182,"Theorem 3. Assume A1 to A4 and assume that the learning rate η satisfies η ≤
1
20B2bKLs. Then
213"
ANALYSIS,0.54,"FAVAS converges at rate:
214"
T,0.5418181818181819,"1
T"
T,0.5436363636363636,"T −1
X"
T,0.5454545454545454,"t=0
E ∥∇f (µt)∥2 ≤2(n + 1)F"
T,0.5472727272727272,"Tsη
+
Ls
n + 1(σ2 n n
X"
T,0.5490909090909091,"i
ai + 8G2b)η + L2s2(720σ2 n n
X"
T,0.5509090909090909,"i
ai + 5600bG2)η2,"
T,0.5527272727272727,"with F := (f(µ0) −f∗), and
215 

 
"
T,0.5545454545454546,"ai, b =
1
P(Ei
t+1>0)2 (
P(Ei
t+1>0)
K2
+ E[
1(Ei
t+1>0)"
T,0.5563636363636364,"Ei
t+1∧K ]), maxi(
1
P(Ei
t+1>0)) for αi = P(Ei
t+1 > 0)(Ei
t+1 ∧K),"
T,0.5581818181818182,"ai, b =
1
E[Ei
t+1∧K] +
E[(Ei
+1∧K)2]"
T,0.56,"K2E[Ei
t+1∧K], maxi(
E[(Ei
t+1∧K)2]"
T,0.5618181818181818,"E[Ei
t+1∧K] )
for αi = E[Ei
t+1 ∧K]."
T,0.5636363636363636,"Note that the previous convergence result refers to the average model µt. In practice, this does not
216"
T,0.5654545454545454,"pose much of a problem. After training is complete, the server can ask each client to submit its final
217"
T,0.5672727272727273,model. It should be noted that each client communicates sT
T,0.5690909090909091,"n times with the server during training.
218"
T,0.5709090909090909,"Therefore, an additional round of data exchange represents only a small increase in the total amount
219"
T,0.5727272727272728,"of data transmitted.
220"
T,0.5745454545454546,"The bound in Theorem 3 contains 3 terms. The first term is standard for a general non-convex target
221"
T,0.5763636363636364,"and expresses how initialization affects convergence. The second and third terms depend on the
222"
T,0.5781818181818181,"statistical heterogeneity of the client distributions and the fluctuation of the minibatch gradients.
223"
T,0.58,"Table 1 compares complexity bounds along with synchronous and asynchronous methods.One can
224"
T,0.5818181818181818,note the importance of the ratio s
T,0.5836363636363636,"n. Compared to Nguyen et al. (2022) or Koloskova et al. (2022),
225"
T,0.5854545454545454,FAVAS can potentially suffer from delayed updates when s
T,0.5872727272727273,"n ≪1, but FAVAS does not favor fast
226"
T,0.5890909090909091,"clients at all. In practice, it is not a major shortcoming, and FAVAS is more robust to fast/slow clients
227"
T,0.5909090909090909,"distribution than FedBuff/AsyncSGD (see Figure 2). We emphasize both FedBuff and AsyncSGD rely
228"
T,0.5927272727272728,"on strong assumptions: neither the queuing process, nor the transitional regime are taken into account
229"
T,0.5945454545454546,"in their analysis. In practice, during the first iterations, only fast clients contribute. It induces a
230"
T,0.5963636363636363,"serious bias. Our experiments indicate that a huge amount of server iterations has to be accomplished
231"
T,0.5981818181818181,"to reach the stationary regime. Still, under this regime, slow clients are contributing with delayed
232"
T,0.6,"information. Nguyen et al. (2022); Koloskova et al. (2022) propose to uniformly bound this delay
233"
T,0.6018181818181818,"by some quantity τmax. We keep this notation while reporting complexity bounds in Table 1, but
234"
T,0.6036363636363636,"argue nothing guarantee τmax is properly defined (i.e. finite). All analyses except that of Mishchenko
235"
T,0.6054545454545455,"et al. (2022) show that the number of updates required to achieve accuracy grows linearly with τmax,
236"
T,0.6072727272727273,"which can be very adverse. Specifically, suppose we have two parallel workers - a fast machine that
237"
T,0.6090909090909091,"takes only 1 unit of time to compute a stochastic gradient, and a slow machine that takes 1000 units
238"
T,0.610909090909091,"of time. If we use these two machines to implement FedBuff/AsyncSGD, the gradient delay of the
239"
T,0.6127272727272727,"slow machine will be one thousand, because in the 1 unit of time we wait for the slow machine, the
240"
T,0.6145454545454545,"fast machine will produce one thousand updates. As a result, the analysis based on τmax deteriorates
241"
T,0.6163636363636363,"by a factor of 1000.
242"
T,0.6181818181818182,"In the literature, guarantees are most often expressed as a function of server steps. In the asynchronous
243"
T,0.62,"case, this is inappropriate because a single step can take very different amounts of time depending on
244"
T,0.6218181818181818,"the method. For example, with FedAvg or Scaffold (Karimireddy et al., 2020), one must wait for
245"
T,0.6236363636363637,"the slowest client for each individual server step. Therefore, we introduce in Table 1 constants C_
246"
T,0.6254545454545455,"that depend on the client speed and represent the unit of time to wait between two consecutive server
247"
T,0.6272727272727273,"steps. Finally, optimizing the value of the learning rate η with Lemma 12 yields the following:
248"
T,0.6290909090909091,"Corollary 4. Assume A1 to A4. We can optimize the learning rate by Lemma 12 and FAVAS reaches
249"
T,0.6309090909090909,"an ϵ precision for a number of server steps T greater than (up to numerical constants):
250"
T,0.6327272727272727,FL( σ2
T,0.6345454545454545,"n
Pn
i ai + 8G2b)"
T,0.6363636363636364,"ϵ2
+ (n + 1)"
T,0.6381818181818182,FL2(K2σ2 + L2K2G2 + s2σ2
T,0.64,"n
Pn
i ai + s2G2b)
1
2"
T,0.6418181818181818,"sϵ
3
2
+ FB2KLb ϵ ! ,"
T,0.6436363636363637,"where F = (f(µ0) −f∗), and (ai, b) are defined in Theorem 3.
251"
T,0.6454545454545455,"The second term in Corollary 4 is better than the one from the QuAFL analysis (n3 of Zakerinia
252"
T,0.6472727272727272,"et al., 2022). Although this (n + 1) term can be suboptimal, note that it is only present at second
253"
T,0.649090909090909,"order from ϵ and therefore becomes negligible when ϵ goes to 0 (Lu and De Sa, 2020; Zakerinia et al.,
254"
T,0.6509090909090909,"2022).
255"
T,0.6527272727272727,"Remark 5. Our analysis can be extended to the case of quantized neural networks. The derived
256"
T,0.6545454545454545,"complexity bounds also hold for the case when the quantization function Q is biased. We make
257"
T,0.6563636363636364,"only a weak assumption about Q (we assume that there is a constant rd such that for any x ∈Rd
258"
T,0.6581818181818182,"∥Q(x) −x∥2 ≤rd), which holds for standard quantization methods such as stochastic rounding and
259"
T,0.66,"deterministic rounding. The only effect of quantization would be increased variance in the stochastic
260"
T,0.6618181818181819,"gradients. We need to add to the upper bound given in Theorem 3 an ""error floor"" of 12L2rd, which
261"
T,0.6636363636363637,"remains independent of the number of server epochs. For stochastic or deterministic rounding,
262"
T,0.6654545454545454,rd = Θ(d 1
T,0.6672727272727272,"22b ), where b is the number of bits used. The error bound is the cost of using quantization
263"
T,0.6690909090909091,"as part of the optimization algorithm. Previous works with quantized models also include error
264"
T,0.6709090909090909,"bounds (Li et al., 2017; Li and Sa, 2019).
265"
NUMERICAL RESULTS,0.6727272727272727,"5
Numerical Results
266"
NUMERICAL RESULTS,0.6745454545454546,"We test FAVAS on three image classification tasks: MNIST (Deng, 2012), CIFAR-10 (Krizhevsky
267"
NUMERICAL RESULTS,0.6763636363636364,"et al., 2009), and TinyImageNet (Le and Yang, 2015). For the MNIST and CIFAR-10 datasets, two
268"
NUMERICAL RESULTS,0.6781818181818182,"training sets are considered: an IID and a non-IIID split. In the first case, the training images are
269"
NUMERICAL RESULTS,0.68,"randomly distributed among the n clients. In the second case, each client takes two classes (out of
270"
NUMERICAL RESULTS,0.6818181818181818,"the ten possible) without replacement. This process leads to heterogeneity among the clients.
271"
NUMERICAL RESULTS,0.6836363636363636,"The standard evaluation measure for FL is the number of server rounds of communication to achieve
272"
NUMERICAL RESULTS,0.6854545454545454,"target accuracy. However, the time spent between two consecutive server steps can be very different
273"
NUMERICAL RESULTS,0.6872727272727273,"for asynchronous and synchronous methods. Therefore, we compare different synchronous and
274"
NUMERICAL RESULTS,0.6890909090909091,"asynchronous methods w.r.t. total simulation time (see below). We also measured the loss and
275"
NUMERICAL RESULTS,0.6909090909090909,"accuracy of the model in terms of server steps and total local client steps (see Appendix C.3). In all
276"
NUMERICAL RESULTS,0.6927272727272727,"experiments, we track the performance of each algorithm by evaluating the server model against an
277"
NUMERICAL RESULTS,0.6945454545454546,"unseen validation dataset. We present the test accuracy and variance, defined as Pn
i=1 ∥wi
t −wt∥2.
278"
NUMERICAL RESULTS,0.6963636363636364,"We decide to focus on non-uniform timing experiments as in Nguyen et al. (2022), and we base our
279"
NUMERICAL RESULTS,0.6981818181818182,"simulation environment on QuAFL’s code1. After simulating n clients, we randomly group them into
280"
NUMERICAL RESULTS,0.7,"fast or slow nodes. We assume that at each time step t (for the central server), a set of s clients is
281"
NUMERICAL RESULTS,0.7018181818181818,"randomly selected without replacement. We assume that the clients have different computational
282"
NUMERICAL RESULTS,0.7036363636363636,"speeds, and refer to Appendix C.2 for more details. We assume that only one-third of the clients are
283"
NUMERICAL RESULTS,0.7054545454545454,"slow, unless otherwise noted. We compare FAVAS with the classic synchronous approach FedAvg
284"
NUMERICAL RESULTS,0.7072727272727273,"(McMahan et al., 2017) and two newer asynchronous metods QuAFL (Zakerinia et al., 2022) and
285"
NUMERICAL RESULTS,0.7090909090909091,"FedBuff (Nguyen et al., 2022). Details on implementing other methods can be found in Appendix C.1.
286"
NUMERICAL RESULTS,0.7109090909090909,"We use the standard data augmentations and normalizations for all methods. FAVAS is implemented in
287"
NUMERICAL RESULTS,0.7127272727272728,"Pytorch, and experiments are performed on an NVIDIA Tesla-P100 GPU. Standard multiclass cross
288"
NUMERICAL RESULTS,0.7145454545454546,"entropy loss is used for all experiments. All models are fine-tuned with n = 100 clients, K = 20
289"
NUMERICAL RESULTS,0.7163636363636363,"local epochs, and a batch of size 128. Following the guidelines of Nguyen et al. (2022), the buffer
290"
NUMERICAL RESULTS,0.7181818181818181,"size in FedBuff is set to Z = 10. In FedAvg, the total simulated time depends on the maximum
291"
NUMERICAL RESULTS,0.72,"number of local steps K and the slowest client runtime, so it is proportional to the number of local
292"
NUMERICAL RESULTS,0.7218181818181818,"steps and the number of global steps. In QuAFL and FAVAS on the other hand, each global step has a
293"
NUMERICAL RESULTS,0.7236363636363636,"predefined duration that depends on the central server clock. Therefore, the global steps have similar
294"
NUMERICAL RESULTS,0.7254545454545455,"durations and the total simulated time is the sum of the durations of the global steps. In FedBuff, a
295"
NUMERICAL RESULTS,0.7272727272727273,"global step requires filling a buffer of size Z. Consequently, both the duration of a global step and
296"
NUMERICAL RESULTS,0.7290909090909091,"the total simulated time depend on Z and on the proportion of slow clients (see Appendix C.2 for a
297"
NUMERICAL RESULTS,0.730909090909091,"detailed discussion).
298"
NUMERICAL RESULTS,0.7327272727272728,"We first report the accuracy of a shallow neural network trained on MNIST. The learning rate is set
299"
NUMERICAL RESULTS,0.7345454545454545,"to 0.5 and the total simulated time is set to 5000. We also compare the accuracy of a Resnet20 (He
300"
NUMERICAL RESULTS,0.7363636363636363,"et al., 2016) with the CIFAR-10 dataset (Krizhevsky et al., 2009), which consists of 50000 training
301"
NUMERICAL RESULTS,0.7381818181818182,"images and 10000 test images (in 10 classes). For CIFAR-10, the learning rate is set to 0.005 and the
302"
NUMERICAL RESULTS,0.74,"total simulation time is set to 10000. In Figure 1, we show the test accuracy of FAVAS and competing"
NUMERICAL RESULTS,0.7418181818181818,"0
1000
2000
3000
4000
5000
Time 0.2 0.4 0.6 0.8"
NUMERICAL RESULTS,0.7436363636363637,Accuracy
NUMERICAL RESULTS,0.7454545454545455,"FedAvg
FedBuff
QuAFL
FAVAS"
NUMERICAL RESULTS,0.7472727272727273,"Figure 1: Test accuracy on the MNIST
dataset with a non-IID split in between
n = 100 total nodes, s = 20."
NUMERICAL RESULTS,0.7490909090909091,"Table 2: Final accuracy on the test set (average and stan-
dard deviation over 10 random experiments) for the MNIST
classification task. The last two columns correspond to Fig-
ures 1 and 2."
NUMERICAL RESULTS,0.7509090909090909,"Methods
IID split
non-IID split
( 2"
NUMERICAL RESULTS,0.7527272727272727,"3 fast clients)
non-IID split
( 1"
NUMERICAL RESULTS,0.7545454545454545,9 fast clients)
NUMERICAL RESULTS,0.7563636363636363,"FedAvg
93.4 ± 0.3
38.7 ± 7.7
44.8 ± 6.9
QuAFL
92.3 ± 0.9
40.7 ± 6.7
45.5 ± 4.0
FedBuff
96.0 ± 0.1
85.1 ± 3.2
67.3 ± 5.5
FAVAS
95.1 ± 0.1
88.9 ± 0.9
87.3 ± 2.3 303"
NUMERICAL RESULTS,0.7581818181818182,"methods on the MNIST dataset. We find that FAVAS and other asynchronous methods can offer a
304"
NUMERICAL RESULTS,0.76,"significant advantage over FedAvg when time is taken into account. However, QuAFL does not
305"
NUMERICAL RESULTS,0.7618181818181818,"appear to be adapted to the non-IID environment. We identified client-side updating as a major
306"
NUMERICAL RESULTS,0.7636363636363637,"shortcoming. While this is not severe when each client optimizes (almost) the same function, the
307"
NUMERICAL RESULTS,0.7654545454545455,"QuAFL mechanism suffers from significant client drift when there is greater heterogeneity between
308"
NUMERICAL RESULTS,0.7672727272727272,"clients. FedBuff is efficient when the number of stragglers is negligible compared to n. However,
309"
NUMERICAL RESULTS,0.769090909090909,"FedBuff is sensitive to the fraction of slow clients and may get stuck if the majority of clients are
310"
NUMERICAL RESULTS,0.7709090909090909,"classified as slow and a few are classified as fast. In fact, fast clients will mainly feed the buffer,
311"
NUMERICAL RESULTS,0.7727272727272727,"so the central updates will be heavily biased towards fast clients, and little information from slow
312"
NUMERICAL RESULTS,0.7745454545454545,"clients will be considered. Figure 2 illustrates this phenomenon, where one-ninth of the clients are
313"
NUMERICAL RESULTS,0.7763636363636364,"classified as fast. To provide a fair comparison, Table 2 gives the average performance of 10 random
314"
NUMERICAL RESULTS,0.7781818181818182,"experiments with the different methods on the test set.
315"
NUMERICAL RESULTS,0.78,"In Figure 3a, we report accuracy on a non-IID split of the CIFAR-10 dataset. FedBuff and FAVAS
316"
NUMERICAL RESULTS,0.7818181818181819,"both perform better than other approaches, but FedBuff suffers from greater variance. We explain
317"
NUMERICAL RESULTS,0.7836363636363637,"this limitation by the bias FedBuff provides in favor of fast clients. We also tested FAVAS on the
318"
NUMERICAL RESULTS,0.7854545454545454,"TinyImageNet dataset (Le and Yang, 2015) with a ResNet18. TinyImageNet has 200 classes and each
319"
NUMERICAL RESULTS,0.7872727272727272,1https://github.com/ShayanTalaei/QuAFL
NUMERICAL RESULTS,0.7890909090909091,"0
1000
2000
3000
4000
5000
Time 0.2 0.4 0.6 0.8"
NUMERICAL RESULTS,0.7909090909090909,Accuracy
NUMERICAL RESULTS,0.7927272727272727,"FedAvg
FedBuff
QuAFL
FAVAS"
NUMERICAL RESULTS,0.7945454545454546,"0
1000
2000
3000
4000
5000
Time 0 100 200 300 400"
NUMERICAL RESULTS,0.7963636363636364,Variance
NUMERICAL RESULTS,0.7981818181818182,"FedAvg
FedBuff
QuAFL
FAVAS"
NUMERICAL RESULTS,0.8,"Figure 2: Test accuracy and variance on the MNIST dataset with a non-IID split between n = 100
total nodes. In this particular experiment, one-ninth of the clients are defined as fast."
NUMERICAL RESULTS,0.8018181818181818,"0
2000
4000
6000
8000
10000
Time 0.05 0.10 0.15 0.20 0.25 0.30"
NUMERICAL RESULTS,0.8036363636363636,Accuracy
NUMERICAL RESULTS,0.8054545454545454,"FedAvg
FedBuff
QuAFL
FAVAS"
NUMERICAL RESULTS,0.8072727272727273,(a) CIFAR-10 (non-IID)
NUMERICAL RESULTS,0.8090909090909091,"0
2000
4000
6000
8000
10000
Time 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35"
NUMERICAL RESULTS,0.8109090909090909,Accuracy
NUMERICAL RESULTS,0.8127272727272727,"FedAvg
QuAFL
FedBuff
FAVAS"
NUMERICAL RESULTS,0.8145454545454546,(b) TinyImageNet (IID)
NUMERICAL RESULTS,0.8163636363636364,"Figure 3: Test accuracy on CIFAR-10 and TinyImageNet datasets with n = 100 total nodes. Central
server selects s = 20 clients at each round."
NUMERICAL RESULTS,0.8181818181818182,"class has 500 (RGB) training images, 50 validation images and 50 test images. To train ResNet18, we
320"
NUMERICAL RESULTS,0.82,"follow the usual practices for training NNs: we resize the input images to 64 × 64 and then randomly
321"
NUMERICAL RESULTS,0.8218181818181818,"flip them horizontally during training. During testing, we center-crop them to the appropriate size.
322"
NUMERICAL RESULTS,0.8236363636363636,"The learning rate is set to 0.1 and the total simulated time is set to 10000. Figure 3b illustrates
323"
NUMERICAL RESULTS,0.8254545454545454,"the performance of FAVAS in this experimental setup. While the partitioning of the training dataset
324"
NUMERICAL RESULTS,0.8272727272727273,"follows an IID strategy, TinyImageNet provides enough diversity to challenge federated learning
325"
NUMERICAL RESULTS,0.8290909090909091,"algorithms. Figure 3b shows that FAVAS scales much better on large image classification tasks than
326"
NUMERICAL RESULTS,0.8309090909090909,"any of the methods we considered.
327"
NUMERICAL RESULTS,0.8327272727272728,"Remark 6. We also evaluated the performance of FAVAS with and without quantization. We ran the
328"
NUMERICAL RESULTS,0.8345454545454546,"code 2 from LUQ (Chmiel et al., 2021) and adapted it to our datasets and the FL framework. Even
329"
NUMERICAL RESULTS,0.8363636363636363,"when the weights and activation functions are highly quantized, the results are close to their full
330"
NUMERICAL RESULTS,0.8381818181818181,"precision counterpart (see Figure 7 in Appendix C).
331"
CONCLUSION,0.84,"6
Conclusion
332"
CONCLUSION,0.8418181818181818,"We have presented FAVAS the first (centralised) Federated Learning method of federated averaging
333"
CONCLUSION,0.8436363636363636,"that accounts for asynchrony in resource-constrained environments. We established complexity
334"
CONCLUSION,0.8454545454545455,"bounds under verifiable assumptions with explicit dependence on all relevant constants. Empirical
335"
CONCLUSION,0.8472727272727273,"evaluation shows that FAVAS is more efficient than synchronous and asynchronous state-of-the-art
336"
CONCLUSION,0.8490909090909091,"mechanisms in standard CNN training benchmarks for image classification.
337"
CONCLUSION,0.850909090909091,2https://openreview.net/forum?id=clwYez4n8e8
REFERENCES,0.8527272727272728,"References
338"
REFERENCES,0.8545454545454545,"Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Kiddon, C., Koneˇcn`y,
339"
REFERENCES,0.8563636363636363,"J., Mazzocchi, S., McMahan, B., et al. (2019). Towards federated learning at scale: System design.
340"
REFERENCES,0.8581818181818182,"Proceedings of Machine Learning and Systems, 1:374–388.
341"
REFERENCES,0.86,"Chen, Y., Ning, Y., Slawski, M., and Rangwala, H. (2020). Asynchronous online federated learning
342"
REFERENCES,0.8618181818181818,"for edge devices with non-iid data. In 2020 IEEE International Conference on Big Data (Big
343"
REFERENCES,0.8636363636363636,"Data), pages 15–24. IEEE.
344"
REFERENCES,0.8654545454545455,"Chen, Z., Liao, W., Hua, K., Lu, C., and Yu, W. (2021). Towards asynchronous federated learning
345"
REFERENCES,0.8672727272727273,"for heterogeneous edge-powered internet of things. Digital Communications and Networks,
346"
REFERENCES,0.8690909090909091,"7(3):317–326.
347"
REFERENCES,0.8709090909090909,"Chmiel, B., Banner, R., Hoffer, E., Yaacov, H. B., and Soudry, D. (2021). Logarithmic unbiased
348"
REFERENCES,0.8727272727272727,"quantization: Simple 4-bit training in deep learning. arXiv preprint arXiv:2112.10769.
349"
REFERENCES,0.8745454545454545,"Deng, L. (2012). The mnist database of handwritten digit images for machine learning research [best
350"
REFERENCES,0.8763636363636363,"of the web]. IEEE signal processing magazine, 29(6):141–142.
351"
REFERENCES,0.8781818181818182,"Fraboni, Y., Vidal, R., Kameni, L., and Lorenzi, M. (2023). A general theory for federated optimiza-
352"
REFERENCES,0.88,"tion with asynchronous and heterogeneous clients updates. Journal of Machine Learning Research,
353"
REFERENCES,0.8818181818181818,"24(110):1–43.
354"
REFERENCES,0.8836363636363637,"He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In
355"
REFERENCES,0.8854545454545455,"Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.
356"
REFERENCES,0.8872727272727273,"Horváth, S., Sanjabi, M., Xiao, L., Richtárik, P., and Rabbat, M. (2022). Fedshuffle: Recipes for
357"
REFERENCES,0.889090909090909,"better use of local work in federated learning. arXiv preprint arXiv:2204.13169.
358"
REFERENCES,0.8909090909090909,"Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K.,
359"
REFERENCES,0.8927272727272727,"Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in federated
360"
REFERENCES,0.8945454545454545,"learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210.
361"
REFERENCES,0.8963636363636364,"Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A. T. (2020). Scaffold:
362"
REFERENCES,0.8981818181818182,"Stochastic controlled averaging for federated learning. In International Conference on Machine
363"
REFERENCES,0.9,"Learning, pages 5132–5143. PMLR.
364"
REFERENCES,0.9018181818181819,"Koloskova, A., Stich, S. U., and Jaggi, M. (2022). Sharper convergence guarantees for asynchronous
365"
REFERENCES,0.9036363636363637,"sgd for distributed and federated learning. arXiv preprint arXiv:2206.08307.
366"
REFERENCES,0.9054545454545454,"Koneˇcn`y, J., McMahan, B., and Ramage, D. (2015). Federated optimization: Distributed optimization
367"
REFERENCES,0.9072727272727272,"beyond the datacenter. arXiv preprint arXiv:1511.03575.
368"
REFERENCES,0.9090909090909091,"Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.
369"
REFERENCES,0.9109090909090909,"Le, Y. and Yang, X. (2015). Tiny imagenet visual recognition challenge. CS 231N, 7(7):3.
370"
REFERENCES,0.9127272727272727,"Li, H., De, S., Xu, Z., Studer, C., Samet, H., and Goldstein, T. (2017). Training quantized nets: A
371"
REFERENCES,0.9145454545454546,"deeper understanding. Advances in Neural Information Processing Systems, 30.
372"
REFERENCES,0.9163636363636364,"Li, Z. and Sa, C. D. (2019). Dimension-free bounds for low-precision training.
373"
REFERENCES,0.9181818181818182,"Lin, T., Stich, S. U., Patel, K. K., and Jaggi, M. (2019). Don’t use large mini-batches, use local sgd.
374"
REFERENCES,0.92,"In International Conference on Learning Representations.
375"
REFERENCES,0.9218181818181819,"Liu, J., Xu, H., Wang, L., Xu, Y., Qian, C., Huang, J., and Huang, H. (2021). Adaptive asyn-
376"
REFERENCES,0.9236363636363636,"chronous federated learning in resource-constrained edge computing. IEEE Transactions on
377"
REFERENCES,0.9254545454545454,"Mobile Computing.
378"
REFERENCES,0.9272727272727272,"Lu, Y. and De Sa, C. (2020). Moniqua: Modulo quantized communication in decentralized sgd. In
379"
REFERENCES,0.9290909090909091,"International Conference on Machine Learning, pages 6415–6425. PMLR.
380"
REFERENCES,0.9309090909090909,"Makarenko, M., Gasanov, E., Islamov, R., Sadiev, A., and Richtarik, P. (2022). Adaptive compression
381"
REFERENCES,0.9327272727272727,"for communication-efficient distributed training. arXiv preprint arXiv:2211.00188.
382"
REFERENCES,0.9345454545454546,"Mao, Y., Zhao, Z., Yan, G., Liu, Y., Lan, T., Song, L., and Ding, W. (2022). Communication-efficient
383"
REFERENCES,0.9363636363636364,"federated learning with adaptive quantization. ACM Transactions on Intelligent Systems and
384"
REFERENCES,0.9381818181818182,"Technology (TIST), 13(4):1–26.
385"
REFERENCES,0.94,"McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017). Communication-
386"
REFERENCES,0.9418181818181818,"efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics,
387"
REFERENCES,0.9436363636363636,"pages 1273–1282. PMLR.
388"
REFERENCES,0.9454545454545454,"Mishchenko, K., Bach, F., Even, M., and Woodworth, B. (2022). Asynchronous sgd beats minibatch
389"
REFERENCES,0.9472727272727273,"sgd under arbitrary delays. arXiv preprint arXiv:2206.07638.
390"
REFERENCES,0.9490909090909091,"Nguyen, J., Malik, K., Zhan, H., Yousefpour, A., Rabbat, M., Malek, M., and Huba, D. (2022).
391"
REFERENCES,0.9509090909090909,"Federated learning with buffered asynchronous aggregation. In International Conference on
392"
REFERENCES,0.9527272727272728,"Artificial Intelligence and Statistics, pages 3581–3607. PMLR.
393"
REFERENCES,0.9545454545454546,"Qu, L., Song, S., and Tsui, C.-Y. (2021). Feddq: Communication-efficient federated learning with
394"
REFERENCES,0.9563636363636364,"descending quantization. arXiv preprint arXiv:2110.02291.
395"
REFERENCES,0.9581818181818181,"Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. S. (2017). Federated multi-task learning.
396"
REFERENCES,0.96,"Advances in neural information processing systems, 30.
397"
REFERENCES,0.9618181818181818,"Toghani, M. T. and Uribe, C. A. (2022). Unbounded gradients in federated leaning with buffered
398"
REFERENCES,0.9636363636363636,"asynchronous aggregation. arXiv preprint arXiv:2210.01161.
399"
REFERENCES,0.9654545454545455,"Tyurin, A. and Richtárik, P. (2022). Dasha: Distributed nonconvex optimization with communi-
400"
REFERENCES,0.9672727272727273,"cation compression, optimal oracle complexity, and no client synchronization. arXiv preprint
401"
REFERENCES,0.9690909090909091,"arXiv:2202.01268.
402"
REFERENCES,0.9709090909090909,"Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H. B., Al-Shedivat, M., Andrew, G., Avestimehr,
403"
REFERENCES,0.9727272727272728,"S., Daly, K., Data, D., et al. (2021). A field guide to federated optimization. arXiv preprint
404"
REFERENCES,0.9745454545454545,"arXiv:2107.06917.
405"
REFERENCES,0.9763636363636363,"Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H. V. (2020). Tackling the objective inconsistency
406"
REFERENCES,0.9781818181818182,"problem in heterogeneous federated optimization. Advances in neural information processing
407"
REFERENCES,0.98,"systems, 33:7611–7623.
408"
REFERENCES,0.9818181818181818,"Wang, Q., Yang, Q., He, S., Shui, Z., and Chen, J. (2022). Asyncfeded: Asynchronous federated learn-
409"
REFERENCES,0.9836363636363636,"ing with euclidean distance based adaptive weight aggregation. arXiv preprint arXiv:2205.13797.
410"
REFERENCES,0.9854545454545455,"Xie, C., Koyejo, S., and Gupta, I. (2019). Asynchronous federated optimization. arXiv preprint
411"
REFERENCES,0.9872727272727273,"arXiv:1903.03934.
412"
REFERENCES,0.9890909090909091,"Xu, C., Qu, Y., Xiang, Y., and Gao, L. (2021). Asynchronous federated learning on heterogeneous
413"
REFERENCES,0.990909090909091,"devices: A survey. arXiv preprint arXiv:2109.04269.
414"
REFERENCES,0.9927272727272727,"Zakerinia, H., Talaei, S., Nadiradze, G., and Alistarh, D. (2022). Quafl: Federated averaging can be
415"
REFERENCES,0.9945454545454545,"both asynchronous and communication-efficient. arXiv preprint arXiv:2206.10032.
416"
REFERENCES,0.9963636363636363,"Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V. (2018). Federated learning with non-iid
417"
REFERENCES,0.9981818181818182,"data. arXiv preprint arXiv:1806.00582.
418"
