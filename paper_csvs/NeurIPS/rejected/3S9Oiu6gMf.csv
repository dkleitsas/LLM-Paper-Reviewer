Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002380952380952381,"We establish finite-sample guarantees for efficient proper learning of bounded-
1"
ABSTRACT,0.004761904761904762,"degree polytrees, a rich class of high-dimensional probability distributions and a
2"
ABSTRACT,0.007142857142857143,"subclass of Bayesian networks, a widely-studied type of graphical models. Very re-
3"
ABSTRACT,0.009523809523809525,"cently, Bhattacharyya et al. [2021] obtained finite-sample guarantees for recovering
4"
ABSTRACT,0.011904761904761904,"tree-structured Bayesian networks, i.e., 1-polytrees. We considerably extend their
5"
ABSTRACT,0.014285714285714285,"results by providing an efficient algorithm which learns d-polytrees in polynomial
6"
ABSTRACT,0.016666666666666666,"time and sample complexity when the in-degree d is constant, provided that the un-
7"
ABSTRACT,0.01904761904761905,"derlying undirected graph (skeleton) is known. We complement our algorithm with
8"
ABSTRACT,0.02142857142857143,"an information-theoretic lower bound, showing that the dependence of our sample
9"
ABSTRACT,0.023809523809523808,"complexity is nearly tight in both the dimension and target accuracy parameters.
10"
INTRODUCTION,0.02619047619047619,"1
Introduction
11"
INTRODUCTION,0.02857142857142857,"Distribution learning, or density estimation, is the task of obtaining a good estimate of some unknown
12"
INTRODUCTION,0.030952380952380953,"underlying probability distribution P from observational samples. Understanding which classes of
13"
INTRODUCTION,0.03333333333333333,"distributions could be or could not be learnt efficiently is a fundamental problem in both computer
14"
INTRODUCTION,0.03571428571428571,"science and statistics, where efficiency is measured in terms of sample (data) and computational
15"
INTRODUCTION,0.0380952380952381,"(time) complexities.
16"
INTRODUCTION,0.04047619047619048,"Bayesian networks (or Bayes nets in short) represent a class of high-dimensional distributions that
17"
INTRODUCTION,0.04285714285714286,"can be explicitly described by how each variable is be generated sequentially in a directed fashion.
18"
INTRODUCTION,0.04523809523809524,"Being interpretable, Bayes nets have been used to model beliefs in a wide variety of domains (e.g.
19"
INTRODUCTION,0.047619047619047616,"see [Jensen and Nielsen, 2007, Koller and Friedman, 2009] and references therein). A fundamental
20"
INTRODUCTION,0.05,"problem in computational learning theory is to identify families of Bayes nets which can be learned
21"
INTRODUCTION,0.05238095238095238,"efficiently from observational data.
22"
INTRODUCTION,0.05476190476190476,"Formally, a Bayes net is a probability distribution P, defined over some directed acyclic graphs (DAG)
23"
INTRODUCTION,0.05714285714285714,"G = (V, E) on |V | = n nodes that factorizes according to G (i.e. Markov with respect to G) in the
24"
INTRODUCTION,0.05952380952380952,"following sense: P(v1, . . . , vn) = Q"
INTRODUCTION,0.06190476190476191,"v1,...,vn P(v | π(v)), where π(v) ⊆V are the parents of v in G.
25"
INTRODUCTION,0.06428571428571428,"While it is well-known that given the DAG (structure) of a Bayes net, there exists sample-efficient
26"
INTRODUCTION,0.06666666666666667,"algorithms that output good hypotheses [Dasgupta, 1997, Bhattacharyya et al., 2020], there is no
27"
INTRODUCTION,0.06904761904761905,"known computationally efficient algorithms for obtaining the DAG of a Bayes net. In fact, it has long
28"
INTRODUCTION,0.07142857142857142,"been understood that Bayes net structure learning is computationally expensive, in various general
29"
INTRODUCTION,0.07380952380952381,"settings [Chickering et al., 2004]. However, these hardness results fall short when the goal is learning
30"
INTRODUCTION,0.0761904761904762,"the distribution P in the probabilistically approximately correct (PAC) [Valiant, 1984] sense (with
31"
INTRODUCTION,0.07857142857142857,"respect to, say, KL divergence or total variation distance), rather than trying to recover an exact graph
32"
INTRODUCTION,0.08095238095238096,"from the Markov equivalence class of P.
33"
INTRODUCTION,0.08333333333333333,"Polytrees are a subclass of Bayesian networks where the undirected graph underlying the DAG is
34"
INTRODUCTION,0.08571428571428572,"a forest, i.e., there is no cycle for the undirected version of the DAG; a polytree with maximum
35"
INTRODUCTION,0.0880952380952381,"in-degree d is also known as a d-polytree. With an infinite number of samples, one can recover the
36"
INTRODUCTION,0.09047619047619047,"DAG of a non-degenerate polytree in the equivalence class with the Chow–Liu algorithm [Chow and
37"
INTRODUCTION,0.09285714285714286,"Liu, 1968] and some additional conditional independence tests [Rebane and Pearl, 1988]. However,
38"
INTRODUCTION,0.09523809523809523,"this algorithm does not work in the finite sample regime. The only known result for learning polytrees
39"
INTRODUCTION,0.09761904761904762,"with finite sample guarantees is for 1-polytrees [Bhattacharyya et al., 2021]. Furthermore, in the
40"
INTRODUCTION,0.1,"agnostic setting, when the goal is to find the closest polytree distribution to an arbitrary distribution
41"
INTRODUCTION,0.10238095238095238,"P, the learning problem becomes NP-hard [Dasgupta, 1999].
42"
INTRODUCTION,0.10476190476190476,"In this work, we investigate what happens when the given distribution is a d-polytree, for d > 1. Are
43"
INTRODUCTION,0.10714285714285714,"d-polytrees computationally hard to learn in the realizable PAC-learning setting? One motivation for
44"
INTRODUCTION,0.10952380952380952,"studying polytrees is due to a recent work of Gao and Aragam [2021] which shows that polytrees
45"
INTRODUCTION,0.11190476190476191,"are easier to learn than general Bayes nets due to the underlying graph being a tree, allowing typical
46"
INTRODUCTION,0.11428571428571428,"causal assumptions such as faithfulness to be dropped when designing efficient learning algorithms.
47"
INTRODUCTION,0.11666666666666667,"Contributions. Our main contribution is a sample-efficient algorithm for proper Bayes net learning
48"
INTRODUCTION,0.11904761904761904,"in the realizable setting, when provided with the ground truth skeleton (i.e., the underlying forest).
49"
INTRODUCTION,0.12142857142857143,"Crucially, our result does not require any distributional assumptions such as strong faithfulness, etc.
50"
INTRODUCTION,0.12380952380952381,"Theorem 1. There exists an algorithm which, given m samples from a polytree P over Σn, accuracy
51"
INTRODUCTION,0.1261904761904762,"parameter ε > 0, failure probability δ, as well as its maximum in-degree d and the explicit description
52"
INTRODUCTION,0.12857142857142856,"of the ground truth skeleton of P, outputs a d-polytree ˆP such that dKL(P ∥ˆP) ≤ε with success
53"
INTRODUCTION,0.13095238095238096,"probability at least 1 −δ, as long as
54"
INTRODUCTION,0.13333333333333333,"m = ˜Ω
n · |Σ|d+1"
INTRODUCTION,0.1357142857142857,"ε
log 1 δ 
."
INTRODUCTION,0.1380952380952381,"Moreover, the algorithm runs in time polynomial in m, |Σ|d, and nd.
55"
INTRODUCTION,0.14047619047619048,"We remark that our result holds when even given only an upper bound on the true in-degree d.
56"
INTRODUCTION,0.14285714285714285,"In particular, our result provides, for constant |Σ|, d, an upper bound of ˜O(n/ε) on the sample
57"
INTRODUCTION,0.14523809523809525,"complexity of learning O(1)-polytrees. Note that this dependence on the dimension n and the
58"
INTRODUCTION,0.14761904761904762,"accuracy parameter ε are optimal, up to logarithmic factors: indeed, we establish in Theorem 15 an
59"
INTRODUCTION,0.15,"Ω(n/ε) sample complexity lower bound for this question, even for d = 2 and |Σ| = 2.1
60"
INTRODUCTION,0.1523809523809524,"We also state sufficient conditions on the distribution that enable recovery of the ground truth skeleton.
61"
INTRODUCTION,0.15476190476190477,"Informally, we require that the data processing inequality hold in a strong sense with respect to the
62"
INTRODUCTION,0.15714285714285714,"edges in the skeleton graph. Under these conditions, combining with our main result in Theorem 1,
63"
INTRODUCTION,0.1595238095238095,"we obtain a polynomial-time PAC algorithm to learn bounded-degree polytrees from samples.
64"
INTRODUCTION,0.1619047619047619,"Other related work. Structure learning of Bayesian networks is an old problem in machine learning
65"
INTRODUCTION,0.16428571428571428,"and statistics that has been intensively studied; see, for example, Chapter 18 of Koller and Friedman
66"
INTRODUCTION,0.16666666666666666,"[2009]. Many of the early approaches required faithfulness, a condition which permits learning
67"
INTRODUCTION,0.16904761904761906,"of the Markov equivalence class, e.g. Spirtes and Glymour [1991], Chickering [2002], Friedman
68"
INTRODUCTION,0.17142857142857143,"et al. [2013]. Finite sample complexity of such algorithms assuming faithfulness-like conditions has
69"
INTRODUCTION,0.1738095238095238,"also been studied, e.g. Friedman and Yakhini [1996]. An alternate line of more modern work has
70"
INTRODUCTION,0.1761904761904762,"considered various other distributional assumptions that permits for efficient learning, e.g., Chickering
71"
INTRODUCTION,0.17857142857142858,"and Meek [2002], Hoyer et al. [2008], Shimizu et al. [2006], Peters and Bühlmann [2014], Ghoshal
72"
INTRODUCTION,0.18095238095238095,"and Honorio [2017], Park and Raskutti [2017], Aragam et al. [2019], with the latter three also showing
73"
INTRODUCTION,0.18333333333333332,"analyzing finite sample complexity. Specifically for polytrees, Rebane and Pearl [1988], Geiger et al.
74"
INTRODUCTION,0.18571428571428572,"[1990] studied recovery of the DAG for polytrees under the infinite sample regime. Gao and Aragam
75"
INTRODUCTION,0.1880952380952381,"[2021] studied the more general problem of learning Bayes nets, and their sufficient conditions
76"
INTRODUCTION,0.19047619047619047,"simplified in the setting of polytrees. Their approach emphasize more on the exact recovery, and thus
77"
INTRODUCTION,0.19285714285714287,"the sample complexity has to depend on the minimum gap of some key mutual information terms. In
78"
INTRODUCTION,0.19523809523809524,"contrast, we allow the algorithm to make mistakes when certain mutual information terms are too
79"
INTRODUCTION,0.1976190476190476,"small to detect for the given sample complexity budget and achieve a PAC-type guarantee. As such,
80"
INTRODUCTION,0.2,"once the underlying skeleton is discovered, our sample complexity only depends on the d, n, ε and
81"
INTRODUCTION,0.20238095238095238,"not on any distributional parameters.
82"
INTRODUCTION,0.20476190476190476,"There are existing works on Bayes net learning with tight bounds in total variation distance, with a
83"
INTRODUCTION,0.20714285714285716,"focus on sample complexity (and not necessarily computational efficiency); for instance, [Canonne
84"
INTRODUCTION,0.20952380952380953,"et al., 2020]. Acharya et al. [2018] consider the problem of learning (in TV distance) a bounded-degree
85"
INTRODUCTION,0.2119047619047619,"causal Bayesian network from interventions, assuming the underlying DAG is known.
86"
INTRODUCTION,0.21428571428571427,"1We remark that [Bhattacharyya et al., 2021, Theorem 7.6] implies an Ω( n"
INTRODUCTION,0.21666666666666667,ε log n
INTRODUCTION,0.21904761904761905,"ε ) sample complexity lower
bound for the analogous question when the skeleton is unknown and d = 1."
INTRODUCTION,0.22142857142857142,"Outline of paper. We begin with some preliminary notions and related work in Section 2. Section 3
87"
INTRODUCTION,0.22380952380952382,"then shows how to recover a polytree close in KL divergence, assuming knowledge of the skeleton
88"
INTRODUCTION,0.2261904761904762,"and maximum in-degree. Section 4 gives sufficient conditions to recover the underlying skeleton from
89"
INTRODUCTION,0.22857142857142856,"samples, while Section 5 provides our sample complexity lower bound. We conclude in Section 6
90"
INTRODUCTION,0.23095238095238096,"with some open directions and defer some full proofs to the appendix.
91"
PRELIMINARIES AND TOOLS FROM PREVIOUS WORK,0.23333333333333334,"2
Preliminaries and tools from previous work
92"
PRELIMINARY NOTIONS AND NOTATION,0.2357142857142857,"2.1
Preliminary notions and notation
93"
PRELIMINARY NOTIONS AND NOTATION,0.23809523809523808,"We write the disjoint union as ˙∪. For any set A, let |A| denotes its size. We use hats to denote
94"
PRELIMINARY NOTIONS AND NOTATION,0.24047619047619048,"estimated quantities, e.g., ˆI(X; Y ) will be the estimated mutual information of I(X; Y ). We employ
95"
PRELIMINARY NOTIONS AND NOTATION,0.24285714285714285,"the standard asymptotic notation O(·), Ω(·) Θ(·), and write ˜O(·) to omit polylogarithmic factors.
96"
PRELIMINARY NOTIONS AND NOTATION,0.24523809523809523,"Throughout, we identify probability distributions over discrete sets with their probability mass
97"
PRELIMINARY NOTIONS AND NOTATION,0.24761904761904763,"functions (pmf). We use d∗to denote the true maximum in-degree of the underlying polytree.
98"
PROBABILITY DISTRIBUTION DEFINITIONS,0.25,"2.2
Probability distribution definitions
99"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2523809523809524,"We begin by defining KL divergence and squared Hellinger distances for a pair of discrete distributions
100"
PROBABILITY DISTRIBUTION DEFINITIONS,0.25476190476190474,"with the same support.
101"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2571428571428571,"Definition 2 (KL divergence and squared Hellinger distance). For distributions P, Q defined on
102"
PROBABILITY DISTRIBUTION DEFINITIONS,0.25952380952380955,"the same discrete support X, their KL divergence and squared Hellinger distances are defined as
103"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2619047619047619,dKL(P ∥Q) = P
PROBABILITY DISTRIBUTION DEFINITIONS,0.2642857142857143,x∈X P(x) log P (x)
PROBABILITY DISTRIBUTION DEFINITIONS,0.26666666666666666,"Q(x) and d2
H(P, Q) = 1 −P x∈X
p"
PROBABILITY DISTRIBUTION DEFINITIONS,0.26904761904761904,"P(x) · Q(x) respectively.
104"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2714285714285714,"Abusing notation, for a distribution P on variables X = {X1, . . . , Xn}, we write PS to mean the
105"
PROBABILITY DISTRIBUTION DEFINITIONS,0.27380952380952384,"projection of P to the subset of variables S ⊆X and PG to mean the projection of P onto a graph G.
106"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2761904761904762,"More specifically, we have PG(x1, . . . , xn) = Q"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2785714285714286,"x∈X P(x | πG(x)) where πG(x) are the parents of x
107"
PROBABILITY DISTRIBUTION DEFINITIONS,0.28095238095238095,"in G. Note that PG is the closest distribution2 on G to P in dKL, i.e. PG = argminQ∈G dKL(P ∥Q).
108"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2833333333333333,"By Chow and Liu [1968], we also know that
109"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2857142857142857,"dKL(P, PG) = − n
X"
PROBABILITY DISTRIBUTION DEFINITIONS,0.28809523809523807,"i=1
I(Xi; πG(Xi)) −H(PX) + n
X"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2904761904761905,"i=1
H(PXi) ,
(1)"
PROBABILITY DISTRIBUTION DEFINITIONS,0.29285714285714287,"where H is the entropy function. Note that only the first term depends on the graph structure of G.
110"
PROBABILITY DISTRIBUTION DEFINITIONS,0.29523809523809524,"By maximizing the sum of mutual information (the negation of the first term in (1)), we can obtain an
111"
PROBABILITY DISTRIBUTION DEFINITIONS,0.2976190476190476,"ε-approximated graph G such that dKL(P ∥PG) ≤ε. In the case of tree-structured distributions, this
112"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3,"can be efficiently solved by using any maximum spanning tree algorithm; a natural generalization to
113"
PROBABILITY DISTRIBUTION DEFINITIONS,0.30238095238095236,"bounded degree bayes nets remains open due to the hardness of solving the underlying optimization
114"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3047619047619048,"problem [Höffgen, 1993]. If any valid topological ordering of the target Bayes net P is present, then
115"
PROBABILITY DISTRIBUTION DEFINITIONS,0.30714285714285716,"an efficient greedy approach is able to solve the problem.
116"
PROBABILITY DISTRIBUTION DEFINITIONS,0.30952380952380953,"Definition 3 ((Conditional) Mutual Information). Given a distribution P, the mutual information of
117"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3119047619047619,"two random variables X and Y , supported on X and Y respectively, is defined as
118"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3142857142857143,"I(X; Y ) =
X"
PROBABILITY DISTRIBUTION DEFINITIONS,0.31666666666666665,"x∈X,y∈Y
P(x, y) · log

P(x, y)
P(x) · P(y) 
."
PROBABILITY DISTRIBUTION DEFINITIONS,0.319047619047619,"Conditioning on a third random variable Z, supported on Z, the conditional mutual information is
119"
PROBABILITY DISTRIBUTION DEFINITIONS,0.32142857142857145,"defined as:
120"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3238095238095238,"I(X; Y | Z) =
X"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3261904761904762,"x∈X,y∈Y,z∈Z
P(x, y, z) · log
P(x, y, z) · P(z)"
PROBABILITY DISTRIBUTION DEFINITIONS,0.32857142857142857,"P(x, z) · P(y, z) 
. 121"
PROBABILITY DISTRIBUTION DEFINITIONS,0.33095238095238094,"By adapting a known testing result from [Bhattacharyya et al., 2021, Theorem 1.3], we can obtain the
122"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3333333333333333,"following corollary, which we will use. We provide the full derivation in the supplementary materials.
123"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3357142857142857,"2One can verify this using Bhattacharyya et al. [2021, Lemma 3.3]: For any distribution Q defined on graph
G, we have dKL(P ∥Q) −dKL(P ∥PG) = P"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3380952380952381,v∈V P(πG(v)) · dKL(P(v | πG(v)) ∥Q(v | πG(v))) ≥0.
PROBABILITY DISTRIBUTION DEFINITIONS,0.3404761904761905,"Corollary 4 (Conditional Mutual Information Tester, adapted from [Bhattacharyya et al., 2021, Theo-
124"
PROBABILITY DISTRIBUTION DEFINITIONS,0.34285714285714286,"rem 1.3]). Fix any ε > 0. Let (X, Y, Z) be three random variables over ΣX, ΣY , ΣZ respectively.
125"
PROBABILITY DISTRIBUTION DEFINITIONS,0.34523809523809523,"Given the empirical distribution ( ˆX, ˆY , ˆZ) over a size N sample of (X, Y, Z), there exists a universal
126"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3476190476190476,"constant 0 < C < 1 so that for any
127"
PROBABILITY DISTRIBUTION DEFINITIONS,0.35,"N ≥Θ
|ΣX| · |ΣY | · |ΣZ|"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3523809523809524,"ε
· log |ΣX| · |ΣY | · |ΣZ|"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3547619047619048,"δ
· log |ΣX| · |ΣY | · |ΣZ| · log(1/δ) ε 
,"
PROBABILITY DISTRIBUTION DEFINITIONS,0.35714285714285715,"the following statements hold with probability 1 −δ:
128"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3595238095238095,"(1) If I(X; Y | Z) = 0, then ˆI(X; Y | Z) < C · ε.
129"
PROBABILITY DISTRIBUTION DEFINITIONS,0.3619047619047619,"(2) If ˆI(X; Y | Z) ≥C · ε, then I(X; Y | Z) > 0.
130"
PROBABILITY DISTRIBUTION DEFINITIONS,0.36428571428571427,"(3) If ˆI(X; Y | Z) ≤C · ε, then I(X; Y | Z) < ε.
131"
PROBABILITY DISTRIBUTION DEFINITIONS,0.36666666666666664,"Unconditional statements involving I(X; Y ) and ˆI(X; Y ) hold similarly by choosing |ΣZ| = 1.
132"
GRAPH DEFINITIONS,0.36904761904761907,"2.3
Graph definitions
133"
GRAPH DEFINITIONS,0.37142857142857144,"Let G = (V, E) be a graph on |V | = n vertices and |E| nodes where adjacencies are denoted with
134"
GRAPH DEFINITIONS,0.3738095238095238,"dashes, e.g. u −v. For any vertex v ∈V , we use N(v) ⊆V \ {v} to denote the neighbors of v and
135"
GRAPH DEFINITIONS,0.3761904761904762,"d(v) = |N(v)| to denote v’s degree. An undirected cycle is a sequence of k ≥3 vertices such that
136"
GRAPH DEFINITIONS,0.37857142857142856,"v1 −v2 −. . . −vk −v1. For any subset E′ ⊆E of edges, we say that the graph H = (V, E′) is the
137"
GRAPH DEFINITIONS,0.38095238095238093,"edge-induced subgraph of G with respect to E′.
138"
GRAPH DEFINITIONS,0.38333333333333336,"For oriented graphs, we use arrows to denote directed edges, e.g. u →v. We denote π(v) to denote
139"
GRAPH DEFINITIONS,0.38571428571428573,"the parents of v and din(v) to denote v’s incoming degree. An interesting directed subgraph on three
140"
GRAPH DEFINITIONS,0.3880952380952381,"vertices is the v-structure, where u →v ←w and u\−w; we say that v is the center of the v-structure.
141"
GRAPH DEFINITIONS,0.3904761904761905,"In this work, we study a generalized higher-degree version of v-structures: we define the notion
142"
GRAPH DEFINITIONS,0.39285714285714285,"of deg-ℓv-structure as a node v with ℓ≥2 parents u1, u2 . . . , uℓ. We say that a deg-ℓv-structure
143"
GRAPH DEFINITIONS,0.3952380952380952,"is said to be ε-strong if we can reliably identify them in the finite sample regime. In our context,
144"
GRAPH DEFINITIONS,0.3976190476190476,"it means that for all k ∈[ℓ], I(uk; {u1, u2 . . . , uℓ} \ uk | v) ≥C · ε, for the universal constant
145"
GRAPH DEFINITIONS,0.4,"0 < C < 1 appearing in Corollary 4. A directed acyclic graph (DAG) is a fully oriented graph
146"
GRAPH DEFINITIONS,0.4023809523809524,"without any directed cycles (a sequence of k ≥3 vertices such that v1 →v2 →. . . →vk →v1) and
147"
GRAPH DEFINITIONS,0.40476190476190477,"are commonly used to represent the conditional dependencies of a Bayes net.
148"
GRAPH DEFINITIONS,0.40714285714285714,"For any partially directed graph, an acyclic completion or consistent extension refers to an assignment
149"
GRAPH DEFINITIONS,0.4095238095238095,"of edge directions to unoriented edges such that the resulting fully directed graph has no directed
150"
GRAPH DEFINITIONS,0.4119047619047619,"cycles; we say that a DAG G is consistent with a partially directed graph H if G is an acyclic
151"
GRAPH DEFINITIONS,0.4142857142857143,"completion of H. Meek rules are a set of 4 edge orientation rules that are sound and complete with
152"
GRAPH DEFINITIONS,0.4166666666666667,"respect to any given set of arcs that has a consistent DAG extension Meek [1995]. Given any edge
153"
GRAPH DEFINITIONS,0.41904761904761906,"orientation information, one can always repeatedly apply Meek rules till a fixed point to maximize
154"
GRAPH DEFINITIONS,0.42142857142857143,"the number of oriented arcs. One particular orientation rule (Meek R1) orients b →c whenever
155"
GRAPH DEFINITIONS,0.4238095238095238,"a partially oriented graph has the configuration a →b −c and a\−c so as to avoid forming a new
156"
GRAPH DEFINITIONS,0.4261904761904762,"v-structure of the form a →b ←c. In the same spirit, we define Meek R1(d∗) to orient all incident
157"
GRAPH DEFINITIONS,0.42857142857142855,"unoriented edges away from v whenever v already has d∗parents in a partially oriented graph.
158"
GRAPH DEFINITIONS,0.430952380952381,"The skeleton skel(G) of a graph G refers to the resulting undirected graph after unorienting all edges
159"
GRAPH DEFINITIONS,0.43333333333333335,"in G, e.g. see Fig. 1. A graph G is a polytree if skel(G) is a forest. For d ≥1, a polytree G is a
160"
GRAPH DEFINITIONS,0.4357142857142857,"d-polytree if all vertices in G have at most d parents. Without loss of generality, by picking the
161"
GRAPH DEFINITIONS,0.4380952380952381,"minimal d, we may assume that d-polytrees have a vertex with d parents. When we freely orient a
162"
GRAPH DEFINITIONS,0.44047619047619047,"forest, we pick arbitrary root nodes in the connected components and orient to form a 1-polytree.
163"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.44285714285714284,"3
Recovering a good orientation given a skeleton and degree bound
164"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.4452380952380952,"In this section, we describe and analyze an algorithm for estimating a probability distribution P that
165"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.44761904761904764,"is defined on a d∗-polytree G∗. We assume that we are given skel(G∗) and d as input.
166"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.45,"Note that for some distributions there could be more than one ground truth graph, e.g. when the
167"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.4523809523809524,"Markov equivalence class has multiple graphs. In such situations, for analysis purposes, we are free
168"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.45476190476190476,"to choose any graph that P is Markov with respect to. As the mutual information scores3 are the
169"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.45714285714285713,"same for any graphs that P is Markov with respect to, the choice of G∗does not matter here.
170"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.4595238095238095,"3The mutual information score is the sum of the mutual information terms as in Eq. (1). d
b a c f e g h i j"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.46190476190476193,"(a) G∗ d
b a c f e g h i j"
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.4642857142857143,(b) skel(G∗)
RECOVERING A GOOD ORIENTATION GIVEN A SKELETON AND DEGREE BOUND,0.4666666666666667,"Figure 1: Running polytree example with d∗= 3 where I(a; b, c) = I(b; a, c) = I(c; a, b) = 0 due to
the deg-3 v-structure centered at d. Since I(a; f | d) = 0, Corollary 4 tells us that ˆI(a; f | d) ≤C · ε.
Thus, we will not detect a →d →f erroneously as a strong deg-2 v-structure a →d ←f."
ALGORITHM,0.46904761904761905,"3.1
Algorithm
171"
ALGORITHM,0.4714285714285714,"At any point in the algorithm, let us define the following sets. Let N(v) be the set of all neighbors of v
172"
ALGORITHM,0.4738095238095238,"in skel(G∗). Let N in(v) ⊆N(v) be the current set of incoming neighbors of v. Let N out(v) ⊆N(v)
173"
ALGORITHM,0.47619047619047616,"be the current set of outgoing neighbors of v. Let N un(v) ⊆N(v) be the current set of unoriented
174"
ALGORITHM,0.4785714285714286,"neighbors of v. That is,
175"
ALGORITHM,0.48095238095238096,N(v) = N in(v) ˙∪N out(v) ˙∪N un(v)
ALGORITHM,0.48333333333333334,Algorithm 1 Algorithm for known skeleton and max in-degree.
ALGORITHM,0.4857142857142857,"Input: Skeleton skel(G∗) = (V, E), max in-degree d∗, threshold ε > 0, universal constant C
Output: A complete orientation of skel(G∗)
1: Run Phase 1: Orient strong v-structures (Algorithm 3)
▷O(nd∗) time
2: Run Phase 2: Local search and Meek R1(d∗) (Algorithm 4)
▷O(n3) time
3: Run Phase 3: Freely orient remaining unoriented edges (Algorithm 5)
▷O(n) time via DFS
4: return ˆG"
ALGORITHM,0.4880952380952381,"There are three phases to our algorithm. In Phase 1, we orient strong v-structures. In Phase 2, we
176"
ALGORITHM,0.49047619047619045,"locally check if an edge is forced to orient one way or another to avoid incurring too much error. In
177"
ALGORITHM,0.4928571428571429,"Phase 3, we orient the remaining unoriented edges as a 1-polytree. Since the remaining edges were
178"
ALGORITHM,0.49523809523809526,"not forced, we may orient the remaining edges in an arbitrary direction (while not incurring “too
179"
ALGORITHM,0.4976190476190476,"much error”) as long as the final incoming degrees of any vertex does not increase by more than 1.
180"
ALGORITHM,0.5,"Subroutine Orient (Algorithm 2) performs the necessary updates when we orient u −v to u →v.
181"
ALGORITHM,0.5023809523809524,Algorithm 2 Orient: Subroutine to orient edges
ALGORITHM,0.5047619047619047,"Input: Vertices u and v where u −v is currently unoriented
1: Orient u −v as u →v.
2: Update N in(v) to N in(v) ∪{u} and N un(v) to N un(v) \ {u}.
3: Update N out(u) to N out(u) ∪{v} and N un(u) to N un(u) \ {v}."
ANALYSIS,0.5071428571428571,"3.2
Analysis
182"
ANALYSIS,0.5095238095238095,"Observe that we perform O(nd∗) (conditional) mutual information tests in Algorithm 1. The following
183"
ANALYSIS,0.5119047619047619,"lemma (Lemma 5) ensures us that all our tests will behave as expected with sufficient samples. As
184"
ANALYSIS,0.5142857142857142,"such, in the rest of our analysis, we analyze under the assumption that our tests are correct.
185"
ANALYSIS,0.5166666666666667,"Lemma 5. Suppose all variables in the Bayesian network has alphabet Σ. For ε′ > 0, with
186"
ANALYSIS,0.5190476190476191,"m ∈O
|Σ|d∗+1"
ANALYSIS,0.5214285714285715,"ε′
· log |Σ|d∗+1 · nd∗"
ANALYSIS,0.5238095238095238,"δ
· log |Σ|d∗+1 · log(nd∗/δ) ε′ "
ANALYSIS,0.5261904761904762,"empirical samples, O(nd∗) statements of the following forms, where X and Y are variable sets of
187"
ANALYSIS,0.5285714285714286,"size |X ˙∪Y| ≤d and Z is possibly ∅, all succeed with probability at least 1 −δ:
188"
ANALYSIS,0.530952380952381,"(1) If I(X; Y | Z) = 0, then ˆI(X; Y | Z) < C · ε′,
189"
ANALYSIS,0.5333333333333333,"(2) If ˆI(X; Y | Z) ≥C · ε′, then I(X; Y | Z) > 0,
190"
ANALYSIS,0.5357142857142857,"(3) If ˆI(X; Y | Z) ≤C · ε′, then I(X; Y | Z) < ε′.
191"
ANALYSIS,0.5380952380952381,"Proof. Use Corollary 4 and apply union bound over O(nd) tests.
192"
ANALYSIS,0.5404761904761904,"Recall that π(v) is the set of true parents of v in G∗. Let H be the forest induced by the remaining
193"
ANALYSIS,0.5428571428571428,"unoriented edges after phase 2. Let ˆG be returned graph of the algorithm 1. Let us denote the final
194"
ANALYSIS,0.5452380952380952,"N in(v) as πin(v) at the end of Phase 2, just before freely orienting, i.e. the vertices pointing into v
195"
ANALYSIS,0.5476190476190477,"in ˆG \ H. Let πun(v) = π(v) \ πin(v) be the set of ground truth parents that are not identified in
196"
ANALYSIS,0.55,"Phase 1. Since the algorithm does not make mistakes for orientations in ˆG \ H (Lemma 6), all edges
197"
ANALYSIS,0.5523809523809524,"of in πun(v) will be unoriented.
198"
ANALYSIS,0.5547619047619048,"Lemma 6. Any oriented arc in ˆG \ H is a ground truth orientation. That is, any vertex parent set
199"
ANALYSIS,0.5571428571428572,"in ˆG \ H is a subset of π(v), i.e. πin(v) ⊆π(v), and N in(v) at any time during the algorithm will
200"
ANALYSIS,0.5595238095238095,"have N in(v) ⊆πin(v).
201 d
b a c f e g h i j"
ANALYSIS,0.5619047619047619,"Figure 2: Suppose we have the following partially oriented graph in the execution of Algorithm 4
(after Phase 1). Since N in(d) = {a, b}, we will check the edge orientations of c−d and f −d. Since
I(f; {a, b} | d) = 0, we will have ˆI(f; {a, b} | d) ≤ε, so we will not erroneously orient f →d.
Meanwhile, I(c; {a, b}) = 0, we will have ˆI(c; {a, b}) ≤ε, so we will not erroneously orient d →c."
ANALYSIS,0.5642857142857143,"Let ˆπ(v) be the proposed parents of v output by Algorithm 1. The KL divergence between the true
202"
ANALYSIS,0.5666666666666667,distribution and our output distribution is essentially P
ANALYSIS,0.569047619047619,v∈V I(v; π(v)) −P
ANALYSIS,0.5714285714285714,"v∈V I(v; ˆπ(v)) as the
203"
ANALYSIS,0.5738095238095238,"structure independent terms will cancel out.
204"
ANALYSIS,0.5761904761904761,"To get a bound on the KL divergence, we will upper bound P"
ANALYSIS,0.5785714285714286,"v∈V I(v; π(v)) and lower bound
205
P"
ANALYSIS,0.580952380952381,"v∈V I(v; ˆπ(v)). To upper bound I(v; π(v)) in terms of πin(v) ⊆π(v) and I(v; u) for u ∈
206"
ANALYSIS,0.5833333333333334,"πun(v), we use Lemma 8 which relies on repeated applications of Lemma 7. To lower bound
207
P"
ANALYSIS,0.5857142857142857,"v∈V I(v; ˆπ(v)), we use Lemma 9.
208"
ANALYSIS,0.5880952380952381,"Lemma 7. Fix any vertex v, any S ⊆πun(v), and any S′ ⊆πin(v). If S ̸= ∅, then there exists a
209"
ANALYSIS,0.5904761904761905,"vertex u ∈S ∪S′ with
210"
ANALYSIS,0.5928571428571429,"I(v; S ∪S′) ≤I(v; S ∪S′ \ {u}) + I(v; u) + ε .
(2)"
ANALYSIS,0.5952380952380952,"Lemma 8. For any vertex v with πin(v), we can show that
211"
ANALYSIS,0.5976190476190476,"I(v; π(v)) ≤ε · |π(v)| + I(v; πin(v)) +
X"
ANALYSIS,0.6,"u∈πun(v)
I(v; u) ."
ANALYSIS,0.6023809523809524,Algorithm 3 Phase 1: Orient strong v-structures
ANALYSIS,0.6047619047619047,1: d ←d∗
ANALYSIS,0.6071428571428571,"2: while d ≥2 do
3:
for v ∈V do
▷Arbitrary order
4:
Let Nd ⊆2N(v) be the set of d neighbors of v
▷|Nd| =
 |N(v)|
d
"
ANALYSIS,0.6095238095238096,"5:
for S ∈Nd s.t. |S| = d, |S ∪N in(v)| ≤d∗, and ˆI(u; S \ {u} | v) ≥C · ε, ∀u ∈S do
6:
for u ∈S do
▷Strong deg-d v-structure
7:
ORIENT(u, v)
8:
d ←d −1
▷Decrement degree bound"
ANALYSIS,0.611904761904762,Algorithm 4 Phase 2: Local search and Meek R1(d∗)
ANALYSIS,0.6142857142857143,"1: do
▷O(n) iterations, O(n2) time per iteration
2:
if ∃v ∈V such that |N in(v)| = d∗and N un(v) ̸= ∅then
▷Meek R1(d∗)
3:
Orient all unoriented arcs away from v
4:
Update N out(v) ←N out(v) ∪N un(v); N un(v) ←∅
5:
for every node v ∈V do
6:
if 1 ≤|N in(v)| < d∗then
7:
for every u ∈N un(v) do
8:
if ˆI(u; N in(v) | v) > C · ε then ORIENT(u, v)
9:
else if ˆI(u; N in(v)) > C · ε then ORIENT(v, u)
10: while new edges are being oriented"
ANALYSIS,0.6166666666666667,Algorithm 5 Phase 3: Freely orient remaining unoriented edges
ANALYSIS,0.6190476190476191,"1: Let H be the forest induced by the remaining unoriented edges.
2: Freely orient H as a 1-polytree (i.e. maximum in-degree in H is 1).
3: Let ˆG be the combination of the oriented H and the previously oriented arcs.
4: return ˆG"
ANALYSIS,0.6214285714285714,"In Phase 3, we increase the incoming edges to any vertex by at most one. The following lemma tells
212"
ANALYSIS,0.6238095238095238,"us that we lose at most4 an additive ε error per vertex.
213"
ANALYSIS,0.6261904761904762,"Lemma 9. Consider an arbitrary vertex v with πin(v) at the start of Phase 3. If Phase 3 orients
214"
ANALYSIS,0.6285714285714286,"u →v for some u −v ∈H, then
215"
ANALYSIS,0.6309523809523809,I(v; πin(v) ∪{u}) ≥I(v; πin(v)) + I(v; u) −ε.
ANALYSIS,0.6333333333333333,"By using Lemma 8 and Lemma 9, we can show our desired KL divergence bound (Lemma 10).
216"
ANALYSIS,0.6357142857142857,"Lemma 10. Let π(v) be the true parents of v. Let ˆπ(v) be the proposed parents of v output by our
217"
ANALYSIS,0.638095238095238,"algorithm. Then,
218
X"
ANALYSIS,0.6404761904761904,"v∈V
I(v; π(v)) −
X"
ANALYSIS,0.6428571428571429,"v∈V
I(v; ˆπ(v)) ≤n · (d∗+ 1) · ε ."
ANALYSIS,0.6452380952380953,"With these results in hand, we are ready to establish our main theorem:
219"
ANALYSIS,0.6476190476190476,"Proof of Theorem 1. We first combine Lemma 10 and Lemma 5 with ε′ =
ε
2n·(d∗+1) in order to
220"
ANALYSIS,0.65,"obtain an orientation ˆG which is close to G∗. Now, recall that there exist efficient algorithms for
221"
ANALYSIS,0.6523809523809524,"estimating the parameters of a Bayes net with in-degree-d (note that this includes d-polytrees) P
222"
ANALYSIS,0.6547619047619048,"once a close-enough graph ˆG is recovered [Dasgupta, 1997, Bhattacharyya et al., 2020], with sample
223"
ANALYSIS,0.6571428571428571,"complexity ˜O(|Σ|dn/ε). Denote the final output ˆP ˆ
G, a distribution that is estimated using the
224"
ANALYSIS,0.6595238095238095,"conditional probabilities implied by ˆG. One can bound the KL divergences as follows:
225"
ANALYSIS,0.6619047619047619,"dKL(P ∥P ˆ
G) −dKL(P ∥PG∗) ≤ε/2
and
dKL(P ∥ˆP ˆ
G) −dKL(P ∥P ˆ
G) ≤ε/2 ."
ANALYSIS,0.6642857142857143,"Thus, we see that
226"
ANALYSIS,0.6666666666666666,"dKL(P ∥ˆP ˆ
G) ≤ε + dKL(P ∥PG∗) = ε . 227"
SKELETON ASSUMPTION,0.669047619047619,"4
Skeleton assumption
228"
SKELETON ASSUMPTION,0.6714285714285714,"In this section, we present a set of sufficient assumptions (Assumption 11) under which the Chow-Liu
229"
SKELETON ASSUMPTION,0.6738095238095239,"algorithm will recover the true skeleton even while with finite samples.
230"
SKELETON ASSUMPTION,0.6761904761904762,"4Orienting “freely” could also increase the mutual information score and this is considering the worst case. d
b a c f e g h i j"
SKELETON ASSUMPTION,0.6785714285714286,"(a) Before final phase d
b a c f e g h i j"
SKELETON ASSUMPTION,0.680952380952381,"(b) ˆG under an arbitrary orienta-
tion of H; see Fig. 4 for more. d
b a c f e g h i j"
SKELETON ASSUMPTION,0.6833333333333333,(c) ˆG \ H
SKELETON ASSUMPTION,0.6857142857142857,"Figure 3: Consider the partially oriented graph before the final phase, where H is the edge-induced
subgraph on the unoriented edges in red. Since d∗= 3 is known, we can conclude that g →i was
oriented due to a local search step and not due to Meek R1(3). We have the following sets before
the final phase: πin(c) = {a, b}, πin(g) = {f, j}, πi = {g}, πun(d) = {c}, πun(f) = {d, e}, and
πun(e) = {h}. With respect to the chosen orientation of H and the notation in Lemma 10, we have
A = {c, d, f, e, h}, ac = d, ad = f, af = e, and ae = h. Observe that the πun’s and a’s are two
different ways to refer to the set of red edges of H. d
b a c f e g h i j"
SKELETON ASSUMPTION,0.6880952380952381,"(a) c as the root d
b a c f e g h i j"
SKELETON ASSUMPTION,0.6904761904761905,"(b) d as the root d
b a c f e g h i j"
SKELETON ASSUMPTION,0.6928571428571428,"(c) f as the root d
b a c f e g h i j"
SKELETON ASSUMPTION,0.6952380952380952,"(d) e as the root d
b a c f e g h i j"
SKELETON ASSUMPTION,0.6976190476190476,(e) h as the root
SKELETON ASSUMPTION,0.7,"Figure 4: The five different possible orientations of H. Observe that the ground truth orientation of
these edges is inconsistent with all five orientations shown here."
SKELETON ASSUMPTION,0.7023809523809523,"Assumption 11. For any given distribution P, there exists a constant εP > 0 such that:
231"
SKELETON ASSUMPTION,0.7047619047619048,"(1) For every pair of nodes u and v, if there exists a path u −· · · −v of length greater than 2 in G∗,
232"
SKELETON ASSUMPTION,0.7071428571428572,"then then I(u; v) + 3 · εP ≤I(a; b) for every pair of adjacent vertices a −b in the path.
233"
SKELETON ASSUMPTION,0.7095238095238096,"(2) For every pair of directly connected nodes a −b in G∗, I(a; b) ≥3 · εP .
234"
SKELETON ASSUMPTION,0.7119047619047619,"Suppose there is a large enough gap of εP between edges in G∗and edges outside of G∗. Then, with
235"
SKELETON ASSUMPTION,0.7142857142857143,"O(1/ε2
P ) samples, each estimated mutual information ˆI(a; b) will be sufficiently close to the true
236"
SKELETON ASSUMPTION,0.7166666666666667,"mutual information I(a; b). Thus, running the Chow-Liu algorithm (which is essentially maximum
237"
SKELETON ASSUMPTION,0.719047619047619,"spanning tree on the estimated mutual information on each pair of vertices) recovers skel(G∗).
238"
SKELETON ASSUMPTION,0.7214285714285714,"Lemma 12. Under Assumption 11, running the Chow-Liu algorithm on the m-sample empirical
239"
SKELETON ASSUMPTION,0.7238095238095238,"estimates {ˆI(u; v)}u,v∈V recovers a ground truth skeleton with high probability when m ≥Ω( log n"
SKELETON ASSUMPTION,0.7261904761904762,"ε2
P ).
240"
SKELETON ASSUMPTION,0.7285714285714285,"Combining Lemma 12 with our algorithm Algorithm 1, one can learn a polytree that is ε-close in KL
241"
SKELETON ASSUMPTION,0.7309523809523809,"with ˜O

max
n
log(n)"
SKELETON ASSUMPTION,0.7333333333333333,"ε2
P
, 2d·n"
SKELETON ASSUMPTION,0.7357142857142858,"ε
o
samples, where εP depends on the distribution P.
242"
LOWER BOUND,0.7380952380952381,"5
Lower bound
243"
LOWER BOUND,0.7404761904761905,"In this section, we show that Ω(n/ε) samples are necessary even when a known skeleton is provided.
244"
LOWER BOUND,0.7428571428571429,"For constant in-degree d, this shows that our proposed algorithm in Section 3 is sample-optimal up to
245"
LOWER BOUND,0.7452380952380953,"logarithmic factors.
246"
LOWER BOUND,0.7476190476190476,"We first begin by showing a lower bound of Ω(1/ε) on a graph with three vertices, even when the
247"
LOWER BOUND,0.75,"skeleton is given. Let G1 be X →Z →Y and G2 be X →Z ←Y , such that skel(G1) = skel(G2)
248"
LOWER BOUND,0.7523809523809524,"is X −Z −Y . Now define P1 and P2 as follows:
249 P1 :"
LOWER BOUND,0.7547619047619047,"





"
LOWER BOUND,0.7571428571428571,"




"
LOWER BOUND,0.7595238095238095,X ∼Bern(1/2)
LOWER BOUND,0.7619047619047619,"Z =
X
w.p. 1/2
Bern(1/2)
w.p. 1/2"
LOWER BOUND,0.7642857142857142,"Y =
Z
w.p. √ε
Bern(1/2)
w.p. 1 −√ε P2 :"
LOWER BOUND,0.7666666666666667,"





"
LOWER BOUND,0.7690476190476191,"




"
LOWER BOUND,0.7714285714285715,"X ∼Bern(1/2)
Y ∼Bern(1/2) Z = 
 "
LOWER BOUND,0.7738095238095238,"X
w.p. 1/2
Y
w.p. √ε
Bern(1/2)
w.p. 1/2 −√ε (3)"
LOWER BOUND,0.7761904761904762,"The intuition is that we keep the edge X →Z “roughly the same” and tweak the edge Y −Z between
250"
LOWER BOUND,0.7785714285714286,"the distributions. We define Pi,G as projecting Pi onto G. One can check that the following holds
251"
LOWER BOUND,0.780952380952381,"(see Supplemental for the detailed calculations):
252"
LOWER BOUND,0.7833333333333333,"Lemma 13. Let G1 be X →Z →Y and G2 be X →Z ←Y , such that skel(G1) = skel(G2) is
253"
LOWER BOUND,0.7857142857142857,"X −Z −Y . With respect to Eq. (3), we have the following:
254"
LOWER BOUND,0.7880952380952381,"1. d2
H(P1, P2) ∈O(ε)
255"
LOWER BOUND,0.7904761904761904,"2. dKL(P1 ∥P1,G1) = 0 and dKL(P1 ∥P1,G2) ∈Ω(ε)
256"
LOWER BOUND,0.7928571428571428,"3. dKL(P2 ∥P2,G2) = 0 and dKL(P2 ∥P2,G1) ∈Ω(ε)
257"
LOWER BOUND,0.7952380952380952,"Our hardness result (Lemma 14) is obtained by reducing the problem of finding an ε-close graph
258"
LOWER BOUND,0.7976190476190477,"orientation of X −Z −Y to the problem of testing whether the samples are drawn from P1 or P2: To
259"
LOWER BOUND,0.8,"ensure ε-closeness in the graph orientation, one has to correctly determine whether the samples come
260"
LOWER BOUND,0.8023809523809524,"from P1 or P2 and then pick G1 or G2 respectively. However, it is well-known that distinguishing two
261"
LOWER BOUND,0.8047619047619048,"distributions whose squared Hellinger distance is ε requires Ω(1/ε) samples (see, e.g., [Bar-Yossef,
262"
LOWER BOUND,0.8071428571428572,"2002, Theorem 4.7]).
263"
LOWER BOUND,0.8095238095238095,"Lemma 14. Even when given skel(G∗), it takes Ω(1/ε) samples to learn an ε-close graph orientation
264"
LOWER BOUND,0.8119047619047619,"of G∗for distributions on {0, 1}3.
265"
LOWER BOUND,0.8142857142857143,"Using the above construction as a gadget, we can obtain a dependency on n in our lower bound by
266"
LOWER BOUND,0.8166666666666667,"constructing n/3 independent copies of the above gadget, à la proof strategy of Bhattacharyya et al.
267"
LOWER BOUND,0.819047619047619,"[2021, Theorem 7.6]. For some constant c > 0, we know that a constant 1/c fraction of the gadgets
268"
LOWER BOUND,0.8214285714285714,"will incur an error or more than ε/n if less than cn/ε samples are used. The desired result then
269"
LOWER BOUND,0.8238095238095238,"follows from the tensorization of KL divergence, i.e., dKL (Q"
LOWER BOUND,0.8261904761904761,i Pi ∥Q
LOWER BOUND,0.8285714285714286,i Qi) = P
LOWER BOUND,0.830952380952381,"i dKL(Pi ∥Qi).
270"
LOWER BOUND,0.8333333333333334,"Theorem 15. Even when given skel(G∗), it takes Ω(n/ε) samples to learn an ε-close graph orienta-
271"
LOWER BOUND,0.8357142857142857,"tion of G∗for distributions on {0, 1}n.
272"
CONCLUSION,0.8380952380952381,"6
Conclusion
273"
CONCLUSION,0.8404761904761905,"In this work, we studied the problem of estimating a distribution defined on a d-polytree P with graph
274"
CONCLUSION,0.8428571428571429,"structure G∗using finite observational samples. We designed and analyzed an efficient algorithm that
275"
CONCLUSION,0.8452380952380952,"produces an estimate ˆP such that dKL(P ∥ˆP) ≤ε assuming access to skel(G∗) and d. The skeleton
276"
CONCLUSION,0.8476190476190476,"skel(G∗) is recoverable under Assumption 11 and we show that there is an inherent hardness in the
277"
CONCLUSION,0.85,"learning problem even under the assumption that skel(G∗) is given. For constant d, our hardness
278"
CONCLUSION,0.8523809523809524,"result shows that our proposed algorithm is sample-optimal up to logarithmic factors.
279"
CONCLUSION,0.8547619047619047,"An interesting open question is whether one can extend the hardness result to arbitrary d ≥1, or
280"
CONCLUSION,0.8571428571428571,"design more efficient learning algorithms for d-polytrees.
281"
REFERENCES,0.8595238095238096,"References
282"
REFERENCES,0.861904761904762,"J. Acharya, A. Bhattacharyya, C. Daskalakis, and S. Kandasamy. Learning and testing causal models
283"
REFERENCES,0.8642857142857143,"with interventions. In NeurIPS, pages 9469–9481, 2018.
284"
REFERENCES,0.8666666666666667,"B. Aragam, A. Amini, and Q. Zhou. Globally optimal score-based learning of directed acyclic graphs
285"
REFERENCES,0.8690476190476191,"in high-dimensions. Advances in Neural Information Processing Systems, 32, 2019.
286"
REFERENCES,0.8714285714285714,"Z. Bar-Yossef. The Complexity of Massive Data Set Computations. PhD thesis, UC Berkeley,
287"
REFERENCES,0.8738095238095238,"2002. Adviser: Christos Papadimitriou. Available at http://webee.technion.ac.il/people/
288"
REFERENCES,0.8761904761904762,"zivby/index_files/Page1489.html.
289"
REFERENCES,0.8785714285714286,"A. Bhattacharyya, S. Gayen, K. S. Meel, and N. V. Vinodchandran. Efficient distance approximation
290"
REFERENCES,0.8809523809523809,"for structured high-dimensional distributions via learning. In NeurIPS, 2020.
291"
REFERENCES,0.8833333333333333,"A. Bhattacharyya, S. Gayen, E. Price, and N. V. Vinodchandran. Near-optimal learning of tree-
292"
REFERENCES,0.8857142857142857,"structured distributions by Chow-Liu. In STOC ’21—Proceedings of the 53rd Annual ACM
293"
REFERENCES,0.888095238095238,"SIGACT Symposium on Theory of Computing, pages 147–160. ACM, New York, 2021. doi:
294"
REFERENCES,0.8904761904761904,"10.1145/3406325.3451066. URL https://doi.org/10.1145/3406325.3451066.
295"
REFERENCES,0.8928571428571429,"C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing bayesian networks. IEEE Trans.
296"
REFERENCES,0.8952380952380953,"Inf. Theory, 66(5):3132–3170, 2020. Preprint available at arXiv:1612.03156.
297"
REFERENCES,0.8976190476190476,"D. M. Chickering. Optimal structure identification with greedy search. Journal of machine learning
298"
REFERENCES,0.9,"research, 3(Nov):507–554, 2002.
299"
REFERENCES,0.9023809523809524,"D. M. Chickering and C. Meek. Finding optimal bayesian networks. In Proceedings of the Eighteenth
300"
REFERENCES,0.9047619047619048,"Annual Conference on Uncertainty in Artificial Intelligence (UAI 02), pages 94–102, 2002.
301"
REFERENCES,0.9071428571428571,"D. M. Chickering, D. Heckerman, and C. Meek. Large-sample learning of bayesian networks is
302"
REFERENCES,0.9095238095238095,"np-hard. J. Mach. Learn. Res., 5:1287–1330, 2004.
303"
REFERENCES,0.9119047619047619,"C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees.
304"
REFERENCES,0.9142857142857143,"IEEE Trans. Inf. Theory, 14(3):462–467, 1968.
305"
REFERENCES,0.9166666666666666,"S. Dasgupta. The sample complexity of learning fixed-structure bayesian networks. Mach. Learn.,
306"
REFERENCES,0.919047619047619,"29(2-3):165–180, 1997.
307"
REFERENCES,0.9214285714285714,"S. Dasgupta. Learning polytrees. In UAI, pages 134–141. Morgan Kaufmann, 1999.
308"
REFERENCES,0.9238095238095239,"N. Friedman and Z. Yakhini. On the sample complexity of learning bayesian networks. In Proceedings
309"
REFERENCES,0.9261904761904762,"of the Twelfth Annual Conference on Uncertainty in Articial Intelligence (UAI {96}), San Francisco,
310"
REFERENCES,0.9285714285714286,"CA, pages 274–282, 1996.
311"
REFERENCES,0.930952380952381,"N. Friedman, I. Nachman, and D. Pe’er. Learning bayesian network structure from massive datasets:
312"
REFERENCES,0.9333333333333333,"The"" sparse candidate"" algorithm. arXiv preprint arXiv:1301.6696, 2013.
313"
REFERENCES,0.9357142857142857,"M. Gao and B. Aragam. Efficient bayesian network structure learning via local markov boundary
314"
REFERENCES,0.9380952380952381,"search. Advances in Neural Information Processing Systems, 34:4301–4313, 2021.
315"
REFERENCES,0.9404761904761905,"D. Geiger, A. Paz, and J. Pearl. Learning causal trees from dependence information. In AAAI, pages
316"
REFERENCES,0.9428571428571428,"770–776. AAAI Press / The MIT Press, 1990.
317"
REFERENCES,0.9452380952380952,"A. Ghoshal and J. Honorio. Learning identifiable gaussian bayesian networks in polynomial time and
318"
REFERENCES,0.9476190476190476,"sample complexity. Advances in Neural Information Processing Systems, 30, 2017.
319"
REFERENCES,0.95,"K.-U. Höffgen. Learning and robust learning of product distributions. In Proceedings of the sixth
320"
REFERENCES,0.9523809523809523,"annual conference on Computational learning theory, pages 77–83, 1993.
321"
REFERENCES,0.9547619047619048,"P. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Schölkopf. Nonlinear causal discovery with
322"
REFERENCES,0.9571428571428572,"additive noise models. Advances in Neural Information Processing Systems, 21, 2008.
323"
REFERENCES,0.9595238095238096,"F. V. Jensen and T. D. Nielsen. Bayesian networks and decision graphs, volume 2. Springer, 2007.
324"
REFERENCES,0.9619047619047619,"D. Koller and N. Friedman. Probabilistic Graphical Models - Principles and Techniques. MIT Press,
325"
REFERENCES,0.9642857142857143,"2009.
326"
REFERENCES,0.9666666666666667,"C. Meek. Causal Inference and Causal Explanation with Background Knowledge. In Proceedings
327"
REFERENCES,0.969047619047619,"of the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI’95, page 403–410, San
328"
REFERENCES,0.9714285714285714,"Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1558603859.
329"
REFERENCES,0.9738095238095238,"G. Park and G. Raskutti. Learning quadratic variance function (qvf) dag models via overdispersion
330"
REFERENCES,0.9761904761904762,"scoring (ods). J. Mach. Learn. Res., 18:224–1, 2017.
331"
REFERENCES,0.9785714285714285,"J. Peters and P. Bühlmann. Identifiability of gaussian structural equation models with equal error
332"
REFERENCES,0.9809523809523809,"variances. Biometrika, 101(1):219–228, 2014.
333"
REFERENCES,0.9833333333333333,"G. Rebane and J. Pearl. The recovery of causal poly-trees from statistical data. Int. J. Approx. Reason.,
334"
REFERENCES,0.9857142857142858,"2(3):341, 1988.
335"
REFERENCES,0.9880952380952381,"S. Shimizu, P. O. Hoyer, A. Hyvärinen, A. Kerminen, and M. Jordan. A linear non-gaussian acyclic
336"
REFERENCES,0.9904761904761905,"model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.
337"
REFERENCES,0.9928571428571429,"P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. Social science
338"
REFERENCES,0.9952380952380953,"computer review, 9(1):62–72, 1991.
339"
REFERENCES,0.9976190476190476,"L. G. Valiant. A theory of the learnable. In STOC, pages 436–445. ACM, 1984.
340"
