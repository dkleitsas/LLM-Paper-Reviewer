Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0033333333333333335,"Multi-hop logical reasoning is an established problem in the ﬁeld of representation
learning on knowledge graphs (KGs). It subsumes both one-hop link prediction
as well as other more complex types of logical queries. However, existing algo-
rithms operate only on classical, triple-based graphs, whereas modern KGs often
employ a hyper-relational modeling paradigm. In this paradigm, typed edges may
have several key-value pairs known as qualiﬁers that provide ﬁne-grained context
for facts. In queries, this context modiﬁes the meaning of relations, and usually
reduces the answer set. Hyper-relational queries are often observed in real-world
KG applications, and existing approaches for approximate query answering (QA)
cannot make use of qualiﬁer pairs. In this work, we bridge this gap and extend the
multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new
type of complex queries. Building upon recent advancements in Graph Neural
Networks and query embedding techniques, we study how to embed and answer
hyper-relational conjunctive queries. Besides that, we propose a method to answer
such queries and demonstrate in our experiments that qualiﬁers improve QA on a
diverse set of query patterns."
INTRODUCTION,0.006666666666666667,"1
INTRODUCTION"
INTRODUCTION,0.01,"Query embedding (QE) on knowledge graphs (KGs) aims to answer logical queries using neural
reasoners instead of traditional databases and query languages. Traditionally, a KG is initially loaded
into a database that understands a particular query language, e.g., SPARQL. The logic of a query is
encoded into conjunctive graph patterns, variables, and common operators such as joins or unions."
INTRODUCTION,0.013333333333333334,"On the other hand, QE bypasses the need for a database or query engine and performs reasoning
directly in a latent space by computing a similarity score between the query representation and entity
representations1. A query representation is obtained by processing its equivalent logical formula
where joins become intersections (∧), and variables are existentially quantiﬁed (∃). A ﬂurry of
recent QE approaches (Hamilton et al., 2018; Ren et al., 2020; Ren & Leskovec, 2020; Kotnis et al.,
2020; Arakelyan et al., 2021) expand the range of supported logical operators and graph patterns."
INTRODUCTION,0.016666666666666666,"However, all existing QE models work only on classical, triple-based KGs. In contrast, an increas-
ing amount of publicly available (Vrandeˇci´c & Kr¨otzsch, 2014; Suchanek et al., 2007) and industrial
KGs adopt a hyper-relational modeling paradigm where typed edges may have additional attributes,
in the form of key-value pairs, known as qualiﬁers. Several standardization efforts embody this
paradigm, i.e., RDF* (Hartig, 2017) and Labeled Property Graphs (LPG)2, with their query lan-
guages SPARQL* and GQL, respectively. Such hyper-relational queries involving qualiﬁers are
instances of higher-order logical queries, and so far, there has been no attempt to bring neural rea-
soners to this domain."
INTRODUCTION,0.02,"In this work, we bridge this gap and propose a neural framework to extend the QE problem to
hyper-relational KGs enabling answering more complex queries. Speciﬁcally, we focus on logical"
EXISTING QE APPROACHES OPERATE ONLY ON THE ENTITY LEVEL AND CANNOT HAVE RELATIONS AS VARIABLES,0.023333333333333334,"1Existing QE approaches operate only on the entity level and cannot have relations as variables
2https://www.iso.org/standard/76120.html"
EXISTING QE APPROACHES OPERATE ONLY ON THE ENTITY LEVEL AND CANNOT HAVE RELATIONS AS VARIABLES,0.02666666666666667,Published as a conference paper at ICLR 2022
EXISTING QE APPROACHES OPERATE ONLY ON THE ENTITY LEVEL AND CANNOT HAVE RELATIONS AS VARIABLES,0.03,"Figure 1: Triple-based (left) and hyper-relational (right) queries. The answer set of the hyper-
relational query is reduced with the addition of a qualiﬁer pair, and the ﬁnal query representation
moves closer to the narrowed down answer."
EXISTING QE APPROACHES OPERATE ONLY ON THE ENTITY LEVEL AND CANNOT HAVE RELATIONS AS VARIABLES,0.03333333333333333,"queries that use conjunctions (∧) and existential quantiﬁers (∃), where the function symbols are
parameterized with the qualiﬁers of the relation. This parameterization enables us (cf. Fig. 1) to
answer queries like What is the university U where the one who discovered the law of the photoelec-
tric effect L got his/her BSc. degree?, which can be written as ?U : ∃P.discovered by(L, P) ∧
educated at{(degree:BSc)}(P, U)."
EXISTING QE APPROACHES OPERATE ONLY ON THE ENTITY LEVEL AND CANNOT HAVE RELATIONS AS VARIABLES,0.03666666666666667,"Our contributions towards this problem are four-fold. First, as higher-order queries are intractable
at practical scale, we show how to express such queries in terms of a subset of ﬁrst-order logic
(FOL) well-explored in the literature. Second, we build upon recent advancements in Graph Neural
Networks (GNNs) and propose a method to answer conjunctive hyper-relational queries in latent
space. Then, we validate our approach by demonstrating empirically that qualiﬁers signiﬁcantly
improve query answering (QA) accuracy on a diverse set of query patterns. Finally, we show the
robustness of our hyper-relational QA model to a reiﬁcation mechanism commonly used in graph
databases to store such graphs on a physical level."
RELATED WORK,0.04,"2
RELATED WORK"
RELATED WORK,0.043333333333333335,"Query Embedding. The foundations of neural query answering and query embeddings laid in
GQE (Hamilton et al., 2018) considered conjunctive (∧) queries with existential (∃) quantiﬁers
modeled as geometric operators based on Deep Sets (Zaheer et al., 2017). Its further extension,
QUERY2BOX (Ren et al., 2020), proposed to represent queries as hyper-rectangles instead of points
in a latent space. Geometrical operations on those rectangles allowed to answer queries with dis-
junction (∨) re-written in the Disjunctive Normal Form (DNF). Changing the query representation
form to beta distributions enabled BETAE (Ren & Leskovec, 2020) to tackle queries with negation
(¬). Another improvement over QUERY2BOX as to answering entailment queries was suggested in
EMQL (Sun et al., 2020) by using count-min sketches."
RELATED WORK,0.04666666666666667,"The other family of approaches represents queries as Directed Acyclic Graphs (DAGs).
MPQE (Daza & Cochez, 2020) assumes variables and targets as nodes in a query graph and ap-
plies an R-GCN (Schlichtkrull et al., 2018) encoder over it. It was shown that message passing
demonstrates promising generalization capabilities (i.e., when training only on 1-hop queries and
evaluating on more complex patterns). Additional gains are brought when the number of R-GCN
layers is equal to the graph diameter. Treating the query DAG as a fully-connected (clique) graph,
BIQE (Kotnis et al., 2020) applied a Transformer encoder (Vaswani et al., 2017) and allowed to
answer queries with multiple targets at different positions in a graph."
RELATED WORK,0.05,"Finally, CQD (Arakelyan et al., 2021) showed that it is possible to answer complex queries without
an explicit query representation. Instead, CQD decomposes a query in a sequence of reasoning steps
and performs a beam search in a latent space of KG embeddings models pre-trained on a simple 1-"
RELATED WORK,0.05333333333333334,Published as a conference paper at ICLR 2022
RELATED WORK,0.056666666666666664,"hop link prediction task. A particular novelty of this approach is that no end-to-end training on
complex queries is required, and any trained embedding model of the existing abundance (Ali et al.,
2020; Ji et al., 2020) can be employed as is."
RELATED WORK,0.06,"Still, all of the described approaches are limited to triple-based KGs while we extend the QE problem
to the domain of hyper-relational KGs. As our approach is based on query graphs, we investigate
some of MPQE observations as to query diameter and generalization (see also Appendix G)."
RELATED WORK,0.06333333333333334,"Hyper-relational KG Embedding. Due to its novelty, embedding hyper-relational KG is a ﬁeld of
ongoing research. Most of the few existing models are end-to-end decoder-only CNNs (Rosso et al.,
2020; Guan et al., 2020) limited to 1-hop link prediction, i.e., embeddings of entities and relations
are stacked and passed through a CNN to score a statement. On the encoder side, we are only aware
of STARE (Galkin et al., 2020) to work in the hyper-relational setting. STARE extends the message
passing framework of CompGCN (Vashishth et al., 2020) by composing qualiﬁers and aggregating
their representations with the primary relation of a statement."
RELATED WORK,0.06666666666666667,"Inspired by the analysis of (Rosso et al., 2020), we take a closer look at comparing hyper-relational
queries against their reiﬁed counterparts (transformed into a triple-only form).
We also adopt
STARE (Galkin et al., 2020) as a basic query graph encoder and further extend it with attentional
aggregators akin to GAT (Veliˇckovi´c et al., 2018)."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.07,"3
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.07333333333333333,"Deﬁnition 3.1 (Hyper-relational Knowledge Graph). Given a ﬁnite set of entities E, and a ﬁnite set of
relations R, let Q = 2(R×E). Then, we deﬁne a hyper-relational knowledge graph as G = (E, R, S),
where S ⊂(E × R × E × Q) is a set of (qualiﬁed) statements."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.07666666666666666,"For a single statement s = (h, r, t, qp), we call h, t ∈E the head and tail entity, and r ∈R the
(main) relation. This also indicates the direction of the relation from head to tail. The triple (h, r, t)
is also called the main triple. qp = {q1, . . .} = {(qr1, qe1), . . .} ⊂R × E is the set of qualiﬁer
pairs, where {qr1, qr2, . . .} are the qualiﬁer relations and {qe1, qe2, . . .} the qualiﬁer values."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.08,"Hyper-relational KGs extend traditional KGs by enabling to qualify triples.
In this work, we
solely use hyper-relational KGs and hence use ”KG” to denote this variant. The qualiﬁer pair set
provides additional information for the semantic interpretation of the main triple (h, r, t). For in-
stance, consider the statement (AlbertEinstein, educated at, ETHZurich, {(degree, BSc)}).
Here,
the
qualiﬁer
pair
(degree, BSc)
gives
additional
context
on
the
base
triple
(AlbertEinstein, educated at, ETHZurich) (also illustrated on Fig. 1)."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.08333333333333333,"Note
that
this
statement
can
equivalently
be
written
in
ﬁrst
order
logic
(FOL).
Speciﬁcally,
we
can
write
it
as
a
statement
with
a
parameterized
predicate
educated at{(degree:BSc)}(AlbertEinstein, ETHZurich).
Then, we note that Q is a ﬁnite
set, meaning that also the number of different parameterizations for the predicate is ﬁnite, which
means that this becomes a ﬁrst order logic statement. In this formalism, the KG is the conjunction
of all FOL statements. Possible monotonicity concerns are discussed in Appendix E."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.08666666666666667,We deﬁne a hyper-relational query on a hyper-relational KG as follows:3
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.09,"Deﬁnition 3.2 (Hyper-relational Query). Let V be a set of variable symbols, and TAR ∈V a special
variable denoting the target of the query. Let E+ = E ⊎V. Then, any subset Q of (E+×R×E+×Q)
is a valid query if its induced graph 1) is a directed acyclic graph, 2) has a topological ordering in
which all entities (in this context referred to as anchors) occur before all variables, and 3) TAR must
be last in the topological orderings.4"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.09333333333333334,"The answers AG(Q) to the query Q are the entities e ∈E that can be assigned to TAR, for which
there exist a variable assignment for all other variables occurring in the query graph, such that the
instantiated query graph is a subgraph of the complete graph G."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.09666666666666666,"3The queries considered here are a subset of SPARQL* basic graph pattern queries.
4These requirements are common in the literature, and usually stated with formal logic, but not a strict
requirement for our approach. See Appendix F for more information."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.1,Published as a conference paper at ICLR 2022 ?var
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.10333333333333333,"educatedAt Wq q φr Win
Σ"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.10666666666666667,"?target
Photo
electric"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.11,effect
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.11333333333333333,"degree
BSc hq qr
qe"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.11666666666666667,"hr,q
r"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.12,"= φr(r,hq) r"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.12333333333333334,"mh→t,r,q 1 2 3 4"
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.12666666666666668,"Figure 2: StarE layer intuition: (1) aggregation γq of qualiﬁers into a single vector; (2) aggregation
φr of a main relation with a qualiﬁers vector; (3) composition of an enriched relation with the head
entity; (4) ﬁnal message to the tail node."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.13,"Problem Deﬁnition. Given the incomplete KG G (part of the not observable complete KG ˆG) and a
query Q. Rank all entities in G such that answers to the query, if it were asked in the context of the
complete KG ˆG, are at the top of the ranking."
HYPER-RELATIONAL KNOWLEDGE GRAPHS AND QUERIES,0.13333333333333333,"Since the given KG is not complete, we cannot solve this problem directly as a graph matching
problem as usually done in databases. Instead, we compute a latent representation of the query, such
that it is close to the embeddings of entities which are the correct answers to the query."
MODEL DESCRIPTION,0.13666666666666666,"4
MODEL DESCRIPTION"
MODEL DESCRIPTION,0.14,"Our model is not trained directly on the KG, but rather with a set of queries sampled from it (see Sec-
tion 5.1). Hence, to describe our model, we consider a query Q ⊂(E+ ×R×E+ ×Q), and describe
how we learn to represent it. Our model can be seen as a combination of MPQE (Daza & Cochez,
2020) and StarE (Galkin et al., 2020) with a CompGCN (Vashishth et al., 2020) architecture."
MODEL DESCRIPTION,0.14333333333333334,"Our model learns representations for entities and the special symbols ({VAR, TAR}), ˆE
∈
R(|E|+2)×d, and relation representations R ∈R2|R|×d, where d is a hyper-parameter indicat-
ing the embedding dimension. There are two representations for each relation. For a statement
s = (h, r, t, qp) ∈Q, one is used for computing “forward” messages from h →t, and the other one
to compute “backward” messages from h ←t, i.e., inverse relation r−1."
MODEL DESCRIPTION,0.14666666666666667,"We encode the query graph using a sequence of STARE layers (Galkin et al., 2020). First, we select
from ˆE the necessary embeddings. These correspond to the ones for all entities in the query, VAR if
the query has internal variables, and TAR for targets. The embeddings are then put in E, which also
contains one copy of VAR for each unique variable encountered in Q."
MODEL DESCRIPTION,0.15,"A layer enriches the entity and relation representations E, R to E′, R′ as follows: For each query
statement s = (h, r, t, qp) ∈Q, we compute messages mh→t,r,qp and mh←t,r−1,qp. For brevity,
we only describe mh→t,r,qp, the other direction works analogously."
MODEL DESCRIPTION,0.15333333333333332,"Qualiﬁer Representation. We begin by composing the representations of the components of each
qualiﬁer pair qi = (qri, qei) ∈qp into a single representation: hqi = γq(E[qei], R[qri]), where γq
denotes a composition function (Vashishth et al., 2020), e.g., the Hadamard product, and we use X[y]
to indicate the representation of entity y projected in vector space X e.g qei in E. Next, we aggregate
all qualiﬁer pair representations for qp, as well as the representation of the main relation r into a
qualiﬁed relation representation using an aggregation function φr: hr,qp = φr(R[r], {hqi}qi∈qp).
For φr we experiment with a simple sum aggregation, and an attention mechanism."
MODEL DESCRIPTION,0.15666666666666668,"The Message-Passing Step. To obtain a message mh→t,r,qp, the qualiﬁed relation representa-
tion is further composed with the source entity representation E[h] using another composition
function γr. The result gets linearly transformed by W→, a layer speciﬁc trainable parameter:
mh→t,r,qp = W→γr(E[h], hr,qp). For inverse relations, we use a separate weight W←. Next,
we aggregate all incoming messages at,→= φm({mh→t′,r,qp | (h, r, t′, qp) ∈Q, t = t′}) us-"
MODEL DESCRIPTION,0.16,Published as a conference paper at ICLR 2022
MODEL DESCRIPTION,0.16333333333333333,"Figure 3: The hyper-relational formulas and their graphical structures. The qualiﬁer pairs attached
to each edge may vary in 0..n; for brevity we represent them as a single pair. We also allow qualiﬁers
to exist only on certain edges of a query, i.e., not all edges necessarily contain them."
MODEL DESCRIPTION,0.16666666666666666,"ing a message aggregation function φm, e.g., a (weighted) sum. Besides computing these mes-
sage aggregates in each direction, we also compute a self-loop update ae,⟲= W⟲γr(e, r⟲),
where W⟲, r⟲are trainable parameters. The updated entity representation is then obtained as
the average over both directions and the self loop, with an additional activation σ applied to it:
E′[e] = σ
  1"
MODEL DESCRIPTION,0.17,"3(ae,⟲+ ae,→+ ae,←)

Finally, the relation representations are updated by a linear
transformation R′[r] = WrR[r], where Wr is a layer speciﬁc trainable weight."
MODEL DESCRIPTION,0.17333333333333334,"Query Representation.
After applying multiple STARE layers, we obtain enriched node rep-
resentations E∗for all nodes in the query graph. As ﬁnal query representation xQ, we aggre-
gate all node representations of the query graph, xQ = φq({E∗[e] | e ∈E+ ∧((e, r, t, qp) ∈
Q ∨(h, r, e, qp) ∈Q)}), with φq denoting an aggregation function, e.g., the sum. Alternatively,
we only select the ﬁnal representation of the unique target node, xQ = E∗[TAR]. To score answer
entity candidates, we use the similarity of the query representation and the entity representation, i.e.,
score(Q, e) = sim(xQ, E∗[e]), such as the dot product, or cosine similarity."
MODEL DESCRIPTION,0.17666666666666667,"We designate the described model as STARQE (Query Embedding for RDF Star Graphs) since
RDF* is one of the most widely adopted standards for hyper-relational KGs."
EXPERIMENTS,0.18,"5
EXPERIMENTS"
EXPERIMENTS,0.18333333333333332,"In this section, we empirically evaluate the performance of QA over hyper-relational KGs. We
design experiments to tackle the following research questions: RQ1) Does QA performance beneﬁt
from the use of qualiﬁers? RQ2) What are the generalization capabilities of our hyper-relational QA
approach? RQ3) Does QA performance depend on the physical representation of a hyper-relational
KG, i.e., reiﬁcation?"
DATASET,0.18666666666666668,"5.1
DATASET"
DATASET,0.19,"Existing QE datasets based on Freebase (Toutanova & Chen, 2015) and NELL (Carlson et al., 2010)
are not applicable in our case since their underlying KGs are strictly triple-based. Thus, we design
a new hyper-relational QE dataset based on WD50K (Galkin et al., 2020)5 comprised of Wikidata
statements, with varying numbers of qualiﬁers."
DATASET,0.19333333333333333,"WD50K-QE. We introduce hyper-relational variants of 7 query patterns commonly used in the
related work (Hamilton et al., 2018; Ren et al., 2020; Sun et al., 2020; Arakelyan et al., 2021)
where each edge can have [0, n] qualiﬁer pairs. The patterns contain projection queries (designated
with -p), intersection queries (designated with -i), and their combinations (cf. Fig. 3). Note that
the simplest 1p pattern corresponds to a well-studied link prediction task. Qualiﬁers allow more
ﬂexibility in query construction, i.e., we further modify formulas by conditioning the existence of
qualiﬁers and their amount over a particular edge. We include dataset statistics and further details
as to dataset construction in Appendices B and D."
DATASET,0.19666666666666666,"These patterns are then translated to the SPARQL* format (Hartig, 2017) and used to retrieve ma-
terialized query graphs from speciﬁc graph splits. Following existing work, we make sure that
validation and test queries contain at least one edge unseen in the training queries, such that evalu-
ated models have to predict new links in addition to QA. As we are in the transductive setup where"
DATASET,0.2,5This dataset is available under CC BY 4.0.
DATASET,0.20333333333333334,Published as a conference paper at ICLR 2022
DATASET,0.20666666666666667,"Figure 4: Example of a reiﬁcation process: An original hr-2p query pattern (left) is reiﬁed through
standard RDF reiﬁcation (right). Note the change of the graph topology: the reiﬁed variant has two
blank nodes, three new pre-deﬁned relation types, and original edge types became nodes connected
via the rdf : predicate edge. The distance between the anchor and target increased, too."
DATASET,0.21,"all entities and relation types have to be seen in training, we also ensure this for all entity and relation
types appearing in qualiﬁers."
DATASET,0.21333333333333335,"Due to the (current) lack of standardized data storage format for hyper-relational (RDF*) graphs
in graph databases, particular implementations of RDF* employ reiﬁcation, i.e, transformation of
hyper-relational statements as deﬁned in Section 3 to plain triples. Reiﬁcation approaches (Frey
et al., 2019) introduce auxiliary virtual nodes, new relation types, turn relations into nodes, and
might dramatically change the original graph topology. An example of a standard RDF reiﬁca-
tion (Brickley et al., 2014) in Fig. 4 transforms an original hr-2p query with three nodes, two
edges, and two qualiﬁer pairs into a new graph with nine nodes and eight edges with rigidly deﬁned
edge types. However, the logical interpretation of a query remains the same. Therefore, we want
hyper-relational QE models to be robust to the underlying graph topology. For this reason, we ship
dataset queries in both formats, i.e., hyper-relational RDF* and reiﬁed with triples, and study the
performance difference in a dedicated experiment."
EVALUATION PROTOCOL,0.21666666666666667,"5.2
EVALUATION PROTOCOL"
EVALUATION PROTOCOL,0.22,"In all experiments, we evaluate the model in the rank-based evaluation setting. Each query is en-
coded into a query embedding vector xq ∈R. We compute a similarity score sim(xq, E[e]) be-
tween the query embedding and the entity representation for each entity e ∈E. The rank r ∈N+
of an entity is its position in the list of entities sorted decreasingly by score. We compute ﬁltered
ranks (Bordes et al., 2013), i.e., while computing the rank of a correct entity, we ignore the scores
of other correct entities. We resolve exactly equal scores using the realistic rank (Berrendorf et al.,
2020), i.e., all entities with equal score obtain the rank of the average of the ﬁrst and last position."
EVALUATION PROTOCOL,0.22333333333333333,"Given a set of individual ranks {ri}n
i=1, we aggregate them into a single-ﬁgure measure using several
different aggregation measures: The Hits@k (H@k) metric measures the frequency of ranks at most
k ∈N+, i.e., H@k =
1
n
P I[ri ≤k], with I denoting the indicator function. H@k lies between
zero and one, with larger values showing better performance. The Mean Reciprocal Rank (MRR) is
the (arithmetic) mean over the reciprocal ranks, i.e., MRR = 1"
EVALUATION PROTOCOL,0.22666666666666666,"n
P r−1
i
, with a value range of (0, 1],
and larger values indicating better results. It can be equivalently interpreted as the inverse harmonic
mean over all ranks, and thus is stronger inﬂuenced by smaller ranks."
EVALUATION PROTOCOL,0.23,"Since the size of the answer set of queries varies greatly,6 queries with large answer sets would
strongly inﬂuence the overall score compared to very speciﬁc queries with a single entity as an-
swer. In contrast to rank-based evaluation in existing QE work, we propose to weight each rank
in the aforementioned averages by the inverse cardinality of the answer set to compensate for their
imbalance. Thereby, each query contributes an equal proportion to the ﬁnal score."
HYPER-RELATIONAL QA,0.23333333333333334,"5.3
HYPER-RELATIONAL QA"
HYPER-RELATIONAL QA,0.23666666666666666,"As we are the ﬁrst to introduce the problem of hyper-relational QA, there is no established baseline
available at the time of writing. Hence, we compare our method to several alternative approaches."
HYPER-RELATIONAL QA,0.24,"6For, e.g., hr-2p we observe a maximum answer set cardinality of 1,351, while the upper quartile is 3."
HYPER-RELATIONAL QA,0.24333333333333335,Published as a conference paper at ICLR 2022
HYPER-RELATIONAL QA,0.24666666666666667,"Table 1: QA performance of STARQE and the baselines when training on all hyper-relational query
patterns. We omit the hr- preﬁx for brevity. Best results (excluding the Oracle) are marked in bold."
HYPER-RELATIONAL QA,0.25,"Pattern
1p
2p
3p
2i
3i
2i-1p
1p-2i"
HYPER-RELATIONAL QA,0.25333333333333335,Hits@10 (%)
HYPER-RELATIONAL QA,0.25666666666666665,"StarQE
51.72
51.20
65.50
77.78
92.64
61.81
81.60
Triple-Only
45.04
12.76
24.66
69.74
91.74
16.77
40.67
Reiﬁcation
55.17
50.86
63.65
81.25
95.31
61.05
80.49
Zero Layers
44.93
29.94
38.45
67.79
90.66
35.48
47.85
Oracle
81.03
24.11
38.47
95.54
99.67
32.74
76.96"
HYPER-RELATIONAL QA,0.26,MRR (%)
HYPER-RELATIONAL QA,0.2633333333333333,"StarQE
30.98
44.13
52.96
63.14
83.78
55.20
71.52
Triple-Only
22.25
6.73
14.05
48.01
74.52
8.16
22.23
Reiﬁcation
34.78
43.42
50.77
61.01
85.17
49.09
64.21
Zero Layers
27.55
19.10
21.25
50.27
80.62
24.49
30.04
Oracle
79.16
18.40
23.72
90.43
97.91
21.10
54.74"
HYPER-RELATIONAL QA,0.26666666666666666,"Implementation. We implement STARQE7 and other baselines in PyTorch (Paszke et al., 2019)
(MIT License). We run a hyperparameter optimization (HPO) pipeline on a validation set for each
model and report the best setup in the Appendix J. All experiments are executed on machines with
single GTX 1080 Ti or RTX 2080 Ti GPU and 12 or 32 CPUs."
HYPER-RELATIONAL QA,0.27,"Triple-Only For the ﬁrst experiment, we remove all qualiﬁers from the hyper-relational statements
in the query graph leaving the base triples (h, r, t). Thus, we isolate the effect of qualiﬁers in
answering the queries correctly. Note that in effect, this is similar to the MPQE approach, but with a
better GNN and aggregation. To do this, we have to retain the same queries as in other experiments,
but remove the qualiﬁers upon loading. The set of targets for these queries remains unchanged, e.g.,
in the hyper-relational query from Fig. 1 we would drop the (degree:BSc) qualiﬁer but still have
only ETHZurich as a correct answer."
HYPER-RELATIONAL QA,0.2733333333333333,"Reiﬁcation. For the second setting, we convert the hyper-relational query graph to plain triples via
reiﬁcation (see Section 5.1). The effects of such a transformation include the addition of two extra
nodes per triple in the query, to represent blank nodes and predicates (more details in Appendix A).
Here, we investigate whether STARQE is able to produce the same semantic interpretation of a topo-
logically different query. Note that, while conceptually the default relation enrichment mechanism
of STARQE resembles singleton property reiﬁcation (Frey et al., 2019), its semantic interpretation
is equivalent to standard RDF reiﬁcation."
HYPER-RELATIONAL QA,0.27666666666666667,"Zero Layers. To measure the effect of message passing, we consider a model akin to bag-of-words,
which trains entity and relation embeddings without message passing before graph aggregation."
HYPER-RELATIONAL QA,0.28,"Oracle. If we take away the qualiﬁer information, we could compare with several QE approaches
(e.g. GQE, MPQE, Query2box, BetaE, EmQL, and CQD). The Oracle setup, is not just an upper
bound to these, but to all possible, non-hyper-relational QE models. It simulates the best possible
QE model that has perfect link prediction and ranking capabilities, but without the ability to use
qualiﬁer information. Using this setting we investigate the impact of qualiﬁer information on our
queries. More details are in Appendix I."
HYPER-RELATIONAL QA,0.2833333333333333,"Discussion. The results shown in Table 1 indicate that STARQE is able to tackle hyper-relational
queries of varying complexity. That is, the performance on complex intersection and projection
queries is often higher than that of a simple link prediction (hr-1p). Particularly, queries with
intersections (−i) demonstrate outstanding accuracy, but it should be noted here that also the Oracle
setting performs very well in this case. Importantly, MRR values are relatively close to Hits@10
which means that more precise measures like Hits@3 and Hits@1 retain good performance (we
provide a detailed breakdown in Appendix J)."
HYPER-RELATIONAL QA,0.2866666666666667,7STARQE implementation: https://github.com/DimitrisAlivas/StarQE
HYPER-RELATIONAL QA,0.29,Published as a conference paper at ICLR 2022
HYPER-RELATIONAL QA,0.29333333333333333,"Table 2:
Results for the generalization experiment. Colored cells denote training query patterns
for each style, e.g., in the EMQL-style we train only on 1p and 2i patterns and evaluate on all. The
hr- preﬁx is omitted for brevity."
HYPER-RELATIONAL QA,0.2966666666666667,"Evaluation Style
1p
2p
3p
2i
3i
2i-1p
1p-2i"
HYPER-RELATIONAL QA,0.3,Hits@10 (%)
HYPER-RELATIONAL QA,0.30333333333333334,"StarQE-like
51.72
51.20
65.50
77.78
92.64
61.81
81.60
Q2B-like
55.44
51.10
66.39
78.79
94.20
57.49
80.49
EmQL-like
50.10
16.45
44.36
75.86
93.55
6.79
62.80
MPQE-like
48.48
12.57
34.19
83.04
96.32
14.75
61.02
MPQE-like + Reif
58.43
12.02
31.14
83.77
97.22
13.50
50.92"
HYPER-RELATIONAL QA,0.30666666666666664,MRR (%)
HYPER-RELATIONAL QA,0.31,"StarQE-like
30.98
44.13
52.96
63.14
83.78
55.20
71.52
Q2B-like
33.04
41.99
51.71
61.72
83.49
44.24
67.04
EmQL-like
32.01
10.09
27.94
61.45
86.28
3.73
53.58
MPQE-like
26.83
6.79
19.72
56.16
74.35
9.62
39.81
MPQE-like + Reif
36.36
6.12
17.11
56.81
77.29
8.32
29.25"
HYPER-RELATIONAL QA,0.31333333333333335,"To investigate if this performance could be attributed to the impact of qualiﬁers, we run a Triple-
Only and Oracle setup. These experiments show that for some query patterns, qualiﬁers play an
important role. Without them, we would not be able to get good results for the 2p, 3p, 2i-1p and
1p-2i queries. The reason that the Oracle cannot answer these well is that despite its access to
the test set, the mistakes it makes accumulate when there are more hops akin to beam search (recall
that the Oracle does not have access to qualiﬁers and considers all edges with a given relation) . For
queries that only involve one reasoning step, we notice that the Oracle can, and hence a normal link
predictor might be able, to perform very well. When more paths intersect (2i and 3i), the chance of
making a mistake goes down. This observation is similar what can be seen in e.g., CQD (Arakelyan
et al., 2021), and can be ascribed to each of the different paths constraining the possible answer set,
while cancelling out mistakes. More experiments on qualiﬁers impact are reported in Appendix H."
HYPER-RELATIONAL QA,0.31666666666666665,"We observe a comparative performance running the Reiﬁcation baseline. It suggests that our QE
framework is robust to the underlying graph topology retaining the same logical interpretation of a
complex query. We believe it is a promising sign of enabling hyper-relational QA on a wide range
of physical graph implementations."
HYPER-RELATIONAL QA,0.32,"Finally, we ﬁnd that message passing layers are essential for maintaining high accuracy as the Zero
Layers baseline lags far behind GNN-enabled models. One explanation for this observation can
be that without message passing, variable nodes do not receive any updates and are thus not ”re-
solved” properly. To some extent counter-intuitively, we also observed that it does not make a large
difference whether relation embeddings are included in the aggregation or not. Relatively high
performance on 1p, 2i, 3i queries can be explained by their very speciﬁc star-shaped query
pattern which is essentially 1-hop with multiple branches joining at the center node."
GENERALIZATION,0.3233333333333333,"5.4
GENERALIZATION"
GENERALIZATION,0.32666666666666666,"Following the related work, we experiment with how well our approach can generalize to complex
query patterns if trained on simple ones. Note that below we understand all query patterns as hyper-
relational, i.e., having the hr- preﬁx. There exist several styles for measuring generalization in the
literature that we include in the experiment:"
GENERALIZATION,0.33,"Q2B-like. The style is used by QUERY2BOX (Ren et al., 2020) and assumes training only on
1p,2p,3p,2i,3i queries while evaluating on two additional patterns 2i-1p, 1p-2i."
GENERALIZATION,0.3333333333333333,"EmQL-like. The other approach proposed by EMQL (Sun et al., 2020) employs only 1p, 2i
patterns for training, using ﬁve more complex ones for evaluation."
GENERALIZATION,0.33666666666666667,Published as a conference paper at ICLR 2022
GENERALIZATION,0.34,"MPQE-like. The hardest generalization setup used in MPQE (Daza & Cochez, 2020) allows train-
ing only on 1p queries, i.e., vanilla link prediction, while all evaluation queries include unseen
intersections and projections."
GENERALIZATION,0.3433333333333333,"MPQE-like + Reif. To measure the impact of reiﬁcation on generalization, we also run the exper-
iment on reiﬁed versions of all query patterns. Similarly to MPQE-like, this setup allows training
only on 1p reiﬁed pattern and evaluates the performance on other, more complex reiﬁed patterns."
GENERALIZATION,0.3466666666666667,"Discussion. Table 2 summarizes the generalization results. As reference, we include the STARQE
results in the no-generalization setup when training and evaluating on all query patterns. Generally,
we observe that all setups generalise well on intersection queries (-i) even when training in the most
restricted (1p) mode. The Q2B-like regime demonstrates appealing generalization capabilities on
(2i-1p, 2p-1i), indicating that it is not necessary to train on all query types. However, moving
to a ﬁne-grained study of most impactful patterns, we ﬁnd that projection (-p) patterns are rather
important for generalization, as both EmQL and MPQE styles dramatically fall behind in accuracy,
especially in the MRR metric indicating that higher precision results are impaired the most."
GENERALIZATION,0.35,"The MPQE style is clearly the hardest when having only one training pattern. The higher results
on intersection (-i) patterns can be explained by a small cardinality of the answer set, i.e., quali-
ﬁers make a query very selective with very few possible answers. Finally, it appears that reiﬁcation
(MPQE+ Reif) impedes generalization capabilities and overall accuracy according to the MRR re-
sults. This can be explained by the graph topologies produced when reifying complex queries, and
training only on 1p is not sufﬁcient."
LIMITATIONS & FUTURE WORK,0.35333333333333333,"6
LIMITATIONS & FUTURE WORK"
LIMITATIONS & FUTURE WORK,0.3566666666666667,"In this section, we discuss limitations of the current work, and future research directions. The ﬁrst
limitation of our work is that we do not allow literal values like numbers, text, and time in our graph.
This means that we cannot, for example, handle queries asking for people born in the year 1980.
These and more complex values can be incorporated as node features (Wilcke et al., 2020)."
LIMITATIONS & FUTURE WORK,0.36,"Secondly, more logical operators, such as negation (which could be included as a qualiﬁer), disjunc-
tions, cardinality constraints, etc. can be considered. In this work, we only allow variables in the
head and tail positions. Nonetheless, one can also formulate queries with variables in more diverse
positions, e.g., qualiﬁer value or main relation."
LIMITATIONS & FUTURE WORK,0.36333333333333334,"Moreover, the current work allows for different query shapes compared to prior work because
queries are not limited to DAGs (see Appendix F for details). However, our work does not allow all
shapes. Speciﬁcally, it is currently unclear how queries with cycles would behave."
LIMITATIONS & FUTURE WORK,0.36666666666666664,"Another future direction can be found in the many operators which query languages like SPARQL
have. For example, queries including paths, aggregations, sub-queries, ﬁlters on literals, etc. Further
research work is required towards explainability. Currently, our system does not provide explana-
tions to the answers. An initial direction is to analyse the intermediate values in variable positions
which can be used as explanations, but likely need to be explicitly trained to behave that way. Fi-
nally, an interesting research direction is the use of our approach for the creation of query plans."
CONCLUSION,0.37,"7
CONCLUSION"
CONCLUSION,0.37333333333333335,"In this work, we have studied and addressed the extension of the multi-hop logical reasoning problem
to hyper-relational KGs. We addressed the theoretical considerations of having qualiﬁers in the
context of QA and discussed the effects it can have on it, such as cardinality of the answer set.
We proposed the ﬁrst hyper-relational QE model, STARQE, based on a GNN encoder to work in
this new setup. We introduced a new dataset, WD50K-QE, with hyper-relational variants of 7 well
studied query patterns and analysed the performance of our model on each of them. Our results
suggest that qualiﬁers help in obtaining more accurate answers compared to triple-only graphs. We
also demonstrate the robustness of our approach to structural changes involved in the process of
reiﬁcation. Finally, we evaluate the generalisation capabilities of our model in all settings and ﬁnd
that it is able to accurately answer unseen patterns."
CONCLUSION,0.37666666666666665,Published as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.38,"Reproducibility Statement. The experimental setup and implementation details are described in
Section 5. We elaborate on the dataset construction process in Appendices B and C. All hyperparam-
eters are listed in Table 6. The complete source code is available for reviewers in the supplementary
material, and will be made available open source. More experimental evidence is provided in Ap-
pendix H and more detailed metrics per query pattern are listed in Appendix J."
ETHICS STATEMENT,0.38333333333333336,"Ethics Statement. The assumption of our approach as well as related techniques is that the data
used for training solely contains true facts. If this data is (intentionally) biased or erroneous to start
with, further biased or wrong information will be derived, which could lead to harmful conclusions
and decisions. Besides, even if the input data to the system is correct, the system could due to its
imperfections still derive incorrect answers to queries, which might, again lead to wrong conclu-
sions. Hence, users of this technology must be made aware that answers to their queries are always
approximate, and this technology ought not to be used in a process where correctness is critical."
ETHICS STATEMENT,0.38666666666666666,"Acknowledgements. This work has been funded by the Elsevier Discovery Lab and the German
Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A. The authors of
this work take full responsibilities for its content. For performing the experiments the authors made
use of the LISA system provided by SurfSara and the DAS cluster (Bal et al., 2016)."
REFERENCES,0.39,REFERENCES
REFERENCES,0.3933333333333333,"Mehdi Ali, Max Berrendorf, Charles Tapley Hoyt, Laurent Vermue, Mikhail Galkin, Sahand Shar-
ifzadeh, Asja Fischer, Volker Tresp, and Jens Lehmann. Bringing light into the dark: A large-
scale evaluation of knowledge graph embedding models under a uniﬁed framework.
CoRR,
abs/2006.13365, 2020."
REFERENCES,0.39666666666666667,"Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering
with neural link predictors. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=Mos9F9kDwkz."
REFERENCES,0.4,"H. Bal, D. Epema, C. de Laat, R. van Nieuwpoort, J. Romein, F. Seinstra, C. Snoek, and H. Wijshoff.
A medium-scale distributed system for computer science research: Infrastructure for the long
term. Computer, 49(05):54–63, may 2016. ISSN 1558-0814. doi: 10.1109/MC.2016.127."
REFERENCES,0.4033333333333333,"Claude Berge. Hypergraphs: combinatorics of ﬁnite sets, volume 45. Elsevier, 1984."
REFERENCES,0.4066666666666667,"Max Berrendorf, Evgeniy Faerman, Laurent Vermue, and Volker Tresp.
Interpretable and fair
comparison of link prediction or entity alignment methods with adjusted mean rank.
CoRR,
abs/2002.06914, 2020. URL https://arxiv.org/abs/2002.06914."
REFERENCES,0.41,"Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-Dur´an, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Christopher J. C. Burges, L´eon
Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger (eds.), Advances in Neural Information
Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.
Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pp.
2787–2795, 2013."
REFERENCES,0.41333333333333333,"Dan Brickley, Ramanathan V Guha, and Brian McBride. RDF schema 1.1. W3C recommendation,
25:2004–2014, 2014."
REFERENCES,0.4166666666666667,"Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. Toward an architecture for never-ending language learning. In Maria Fox and David
Poole (eds.), Proceedings of the Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI
2010, Atlanta, Georgia, USA, July 11-15, 2010. AAAI Press, 2010."
REFERENCES,0.42,"Daniel Daza and Michael Cochez.
Message passing query embedding.
In ICML Workshop
- Graph Representation Learning and Beyond, 2020.
URL https://arxiv.org/abs/
2002.02406."
REFERENCES,0.42333333333333334,"Fredo Erxleben, Michael G¨unther, Markus Kr¨otzsch, Julian Mendez, and Denny Vrandecic. In-
troducing wikidata to the linked data web.
In Peter Mika, Tania Tudorache, Abraham Bern-
stein, Chris Welty, Craig A. Knoblock, Denny Vrandecic, Paul Groth, Natasha F. Noy, Krzysztof"
REFERENCES,0.4266666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.43,"Janowicz, and Carole A. Goble (eds.), The Semantic Web - ISWC 2014 - 13th International Se-
mantic Web Conference, Riva del Garda, Italy, October 19-23, 2014. Proceedings, Part I, vol-
ume 8796 of Lecture Notes in Computer Science, pp. 50–65. Springer, 2014.
doi: 10.1007/
978-3-319-11964-9\ 4. URL https://doi.org/10.1007/978-3-319-11964-9_4."
REFERENCES,0.43333333333333335,"Johannes Frey, Kay M¨uller, Sebastian Hellmann, Erhard Rahm, and Maria-Esther Vidal. Evaluation
of metadata representations in RDF stores. Semantic Web, 10(2):205–229, 2019."
REFERENCES,0.43666666666666665,"Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. Mes-
sage passing for hyper-relational knowledge graphs. In Bonnie Webber, Trevor Cohn, Yulan He,
and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 7346–7359. Association
for Computational Linguistics, 2020."
REFERENCES,0.44,"Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, and Xueqi Cheng. Neuinfer: Knowl-
edge inference on n-ary facts. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 6141–6151, 2020."
REFERENCES,0.44333333333333336,"William L. Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding
logical queries on knowledge graphs. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Informa-
tion Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp. 2030–2041, 2018."
REFERENCES,0.44666666666666666,"Olaf Hartig. Foundations of RDF* and SPARQL*:(an alternative approach to statement-level meta-
data in RDF). In AMW 2017 11th Alberto Mendelzon International Workshop on Foundations
of Data Management and the Web, Montevideo, Uruguay, June 7-9, 2017., volume 1912. Juan
Reutter, Divesh Srivastava, 2017."
REFERENCES,0.45,"Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. A survey on knowledge
graphs: Representation, acquisition and applications. CoRR, abs/2002.00388, 2020."
REFERENCES,0.4533333333333333,"Bhushan Kotnis, Carolin Lawrence, and Mathias Niepert. Answering complex queries in knowledge
graphs with bidirectional sequence encoders. CoRR, abs/2004.02596, 2020."
REFERENCES,0.45666666666666667,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, pp. 8024–8035, 2019."
REFERENCES,0.46,"Hongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge
graphs. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020."
REFERENCES,0.4633333333333333,"Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in
vector space using box embeddings. In 8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL
https://openreview.net/forum?id=BJgr4kSFDS."
REFERENCES,0.4666666666666667,"Paolo Rosso, Dingqi Yang, and Philippe Cudr´e-Mauroux. Beyond triplets: hyper-relational knowl-
edge graph embedding for link prediction. In Proceedings of The Web Conference 2020, pp.
1885–1896, 2020."
REFERENCES,0.47,"Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European semantic web
conference, pp. 593–607. Springer, 2018."
REFERENCES,0.47333333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.4766666666666667,"Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on World Wide Web, pp. 697–706, 2007."
REFERENCES,0.48,"Haitian Sun, Andrew O. Arnold, Tania Bedrax-Weiss, Fernando Pereira, and William W. Cohen.
Faithful embeddings for knowledge base queries. In Hugo Larochelle, Marc’Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020."
REFERENCES,0.48333333333333334,"Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text
inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their
Compositionality, pp. 57–66, Beijing, China, July 2015. Association for Computational Lin-
guistics. doi: 10.18653/v1/W15-4007. URL https://www.aclweb.org/anthology/
W15-4007."
REFERENCES,0.4866666666666667,"Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. Composition-based multi-
relational graph convolutional networks. In 8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020."
REFERENCES,0.49,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.
5998–6008, 2017."
REFERENCES,0.49333333333333335,"Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ."
REFERENCES,0.49666666666666665,"Denny Vrandeˇci´c and Markus Kr¨otzsch. Wikidata: a free collaborative knowledgebase. Communi-
cations of the ACM, 57(10):78–85, 2014."
REFERENCES,0.5,"W. X. Wilcke, P. Bloem, V. de Boer, R. H. van t Veer, and F. A. H. van Harmelen. End-to-end entity
classiﬁcation on multimodal knowledge graphs, 2020."
REFERENCES,0.5033333333333333,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R Salakhutdinov,
and Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 3391–3401. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6931-deep-sets.pdf."
REFERENCES,0.5066666666666667,APPENDIX
REFERENCES,0.51,"A
TYPES OF GRAPHS"
REFERENCES,0.5133333333333333,"In this section, we discuss in further detail the different approaches to break free from the restriction
of pairwise relations. The intuition how those approaches are different is presented in Fig. 5."
REFERENCES,0.5166666666666667,"Hyper-relational Graphs
KGs used in our work are a subset of hyper-relational graphs. In gen-
eral, hyper-relational graphs can also contain literal values, like integers and string values as qualiﬁer
values. Labeled property graphs are another example of a subset of hyper-relation graphs, but they
typically only allow literals as qualiﬁer values. The Wikidata model (Erxleben et al., 2014) and
RDF* (Hartig, 2017) allows for both literals and entities as values. Many implementations of RDF*
do make the monotonicity assumption, which we also made in the paper. Some will also merge
statements in case they have the same main triple. The resulting statement will have the same main
triple, but as qualiﬁer information, the union of the sets of qualiﬁer pairs of the original statements."
REFERENCES,0.52,Published as a conference paper at ICLR 2022
REFERENCES,0.5233333333333333,"Figure 5: The same query expressed in three different approaches. Hyper-relational (left), hyper-
graph (middle), and reiﬁed (right) graphs."
REFERENCES,0.5266666666666666,"Hypergraphs
Hypergraphs are another type of graphs, initially proposed by Berge (Berge, 1984),
where hyperedges link one or more vertices. These hyperedges group together sets of nodes in
a relation, but lose information of the particular roles that entities play in this relation. Besides,
each different composition of elements in the relation introduces a new hyperedge type, causing a
combinatorial explosion of these types."
REFERENCES,0.53,"Reiﬁed Graphs
Hyper-relational and reiﬁed graphs have equivalent expressive power compared
to labelled directed graphs. The reason is that we can deﬁne a bijection between these graphs.
However, depending on the use case, the different graph types have beneﬁts."
REFERENCES,0.5333333333333333,"When we convert a hyper-relational graph to an RDF graph, we end up with what is called the
reiﬁed graph. There are multiple approaches on how to perform this conversion. In our work, we
use Standard RDF Reiﬁcation8. It introduces new, so called blank nodes to represent statements
with all parts of a statement attached. This allows for all information to exist on the same level.
One of the disadvantages to this approach is the addition of auxiliary nodes, which heavily affect the
structure of the graph and quickly inﬂate the graph size."
REFERENCES,0.5366666666666666,"B
THE WD50K-QE DATASET"
REFERENCES,0.54,"In this section, we describe the generation of the hyper-relational queries that we use for train-
ing, validation and testing of our models. We start with WD50K (Galkin et al., 2020), which is a
WikiData-based dataset created for hyper-relational link prediction. This dataset already provides
with train, validation and test splits, each containing a selection of hyper-relational triples. It is
publicly available by the authors, in CSV format. In order to utilise it for our work, we converted it
from CSV to RDF*."
REFERENCES,0.5433333333333333,"The overall pipeline is presented on Fig. 6. To generate the queries, we hosted the converted dataset
on a graph database with support for hyper-relational data. For our use-case, we use anzograph9.
We utilize 3 named graphs10: triple train, triple validation, and triple test, to prevent validation and
test set leakage. In the evaluation of approximate QA, it is common to have queries with at least
one unseen edge from the test set, which is also applied upon our query generation process. We
ensure that:"
REFERENCES,0.5466666666666666,"• For the training set, all statements in a query come from the triple train set (only).
• For the validation set, one statement comes from the triple validation and the other edge(s)
come from either triple train or triple validation, and"
REFERENCES,0.55,"8https://www.w3.org/TR/2014/REC-rdf11-mt-20140225/#reification
9https://www.cambridgesemantics.com/anzograph/
10https://www.w3.org/TR/rdf11-concepts/#section-dataset"
REFERENCES,0.5533333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.5566666666666666,"Figure 6: A diagram of the query generation process. On the far left, the chosen dataset (WD50K)
is uploaded in the RDF*-compatible triplestore (AnzoGraph). As a follow up, we translated the
hyper-relational query patterns to SPARQL* and executed them against the triplestore to retrieve
the desired splits."
REFERENCES,0.56,"Pattern
1p
2p
3p
2i
3i
2i-1p
1p-2i"
REFERENCES,0.5633333333333334,"train
24, 819
313, 088
5, 950, 990
48, 513
318, 735
306, 022
1, 088, 539
validation
4, 100
100, 706
2, 968, 315
15, 648
169, 195
169, 438
569, 957
test
7, 716
202, 045
6, 433, 476
38, 207
547, 272
445, 007
1, 267, 452"
REFERENCES,0.5666666666666667,"Table 3: The amount of queries for each of the different query patterns. We notice that for some
patterns there are many more queries than for others, which is also why we report results for the
patterns separately."
REFERENCES,0.57,"• For the test set, one statement comes from triple test, and the other statement from any of
triple train, triple validation, and triple test"
REFERENCES,0.5733333333333334,"In order to generate the query data splits, we constructed SPARQL* queries that correspond to a
speciﬁc pattern. We call these higher order queries formulas. Using these formulas to generate
queries has several advantages compared to other approaches found in related work. In the related
work, we encounter sampling using techniques such as random walks to create queries which have
the shape according to the pattern. With our approach, we have more control over what the samples
are, and thus a ﬁner control of the inputs to our experiments."
REFERENCES,0.5766666666666667,"Moreover, we can also ensure that we do not sample with replacement, we are not biased towards
speciﬁc high degree nodes, and at the same time guarantee that we do not sample queries that are
isomorphic with each other. A ﬁnal beneﬁt is that with our approach, we can immediately retrieve
all answers for a query, instead of only one."
REFERENCES,0.58,"One ﬁnal issue we encountered had to do with the nodes that have a high in-degree in our dataset.
Their existence causes a skewing of the distribution of correct answers to queries, and has to be
resolved. In Appendix C, we describe this issue and explain our approach to overcome it."
REFERENCES,0.5833333333333334,"As a result of this procedure, we obtain all queries of the given shapes, and are guaranteed that there
are no duplicates nor isomorphism among the queries. The queries used in this paper are such that
every edge in the query has one qualiﬁer, which means that, in the knowledge graph, the same edge
has at least one, but possibly more qualiﬁers. The amount of queries for the different patterns can
be found in Table 3."
REFERENCES,0.5866666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.59,"C
ISSUES SAMPLING QUERIES WITH JOINS IN A GRAPH WITH HIGH
DEGREE NODES"
REFERENCES,0.5933333333333334,"In our query generation step, we exclude high in-degree nodes as a target for speciﬁc queries. In this
section, we explain why that choice was made. We will focus on the 3i pattern. The same argument
holds for other patterns with joins, like 2i, 2i-1p, 1p-2i."
REFERENCES,0.5966666666666667,"The queries are randomly sampled from all possible queries that can be formed by matching the
pattern with the data graph. For the 3i pattern, a node with an in-degree of n, results in
 n
3

different
queries, with that node as a target."
REFERENCES,0.6,"The problem is that if we randomly sample our queries, the answer would usually be one of the
highest degree nodes. Concretely, in our graph data, the highest observed in-degree is 4,424, which
would result in 14,421,138,424 possible queries with the corresponding node as an answer. If we
generated all possible queries, we would end up with 38,011,148,464 different ones. So, the node
with the highest in-degree is already responsible for 38% of the queries, meaning that system an-
swering 3-i queries that are randomly sampled, would get 38% correct by always giving that answer.
Moreover, if we look at the Hits@10 metric, we would score 93%, by always predicting the same
ranking (the top 10 in descending frequency). Note that existing query embedding models have been
evaluated like this in the past, ignoring this data issue."
REFERENCES,0.6033333333333334,"Hence, we decided to make the task harder by removing these high degree nodes for queries with
joins. That is, by putting the threshold at an in-degree of 50, we remove the 623 nodes with the
highest in-degree which would have represented 99.9% of the possible answers for the 3i queries
without this modiﬁcation. After ﬁltering, the baseline of always predicting the same ranking for the
randomly sampled queries, will result in a Hits@10 of 3% in the best case."
REFERENCES,0.6066666666666667,"For other query shapes where a join is involved, the situation is similar, but less pronounced."
REFERENCES,0.61,"D
SIZE OF THE ANSWER SET OF A QUERY WITH MORE QUALIFIERS"
REFERENCES,0.6133333333333333,"When we have a query with or without qualiﬁers, and we add more qualiﬁers, the number of answers
to this query can only become smaller. Intuitively, this happens because the query becomes more
speciﬁc. In this section we prove this intuition correct. Note that proving this requires monotonicity,
as we deﬁned in the paper. Without this assumption - for example allowing non-monotonic qualiﬁer
relations - would result in losing the guarantee that the answer set becomes smaller."
REFERENCES,0.6166666666666667,"Given a query Q, and a query Q that is the same, except for one set of qualiﬁer pairs of Q which can
have extra pairs, then the answers to Q are a subset of the answers to Q."
REFERENCES,0.62,Formally:
REFERENCES,0.6233333333333333,"Theorem D.1.
Given a KG G
=
(E, R, S),
where S
⊂
(E × R × E × Q),
a
query
Q
=
{(h1, r1, t1, qp1), . . .
(hn, rn, tn, qpn)},
and
a
second
query
Q = {(h1, r1, t1, qp1), . . . (hn, rn, tn, qpn)}, where ∃!k : qpk ⊆qpk and ∀x((x ∈[1, . . . n] ∧x ̸=
k) →qpx = qpx). Then, AG(Q) ⊆AG(Q)."
REFERENCES,0.6266666666666667,"Proof. Assuming symbols as deﬁned in the theorem, we show that ∀a : a ∈AG(Q)
=⇒
a ∈AG(Q).
If a is an answer to Q, then its associated variable substitution v is such that
(v(hk), rk, v(tk), qpk) ∈G. From monotonicty (see Appendix E) , and given qpk ⊆qpk we then
know that (v(hk), rk, v(tk), qpk) ∈G. And hence, using the same variable substitution, a is an
answer for Q."
REFERENCES,0.63,"Corollary D.1. By induction, given a query Q, and a query Q that is the same, except for any set of
qualiﬁer pairs of Q which can have extra pairs, then the answers to Q are a subset of the answers
to Q."
REFERENCES,0.6333333333333333,"Corollary D.2. As a special case, given a query Q, and a query Q that is the same, except that Q
does not have qualiﬁers, while Q can have qualiﬁer pairs on its statements, then the answers to Q
are a subset of the answers to Q."
REFERENCES,0.6366666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.64,"e1
TAR
VAR1
e3
r1 r2
r3"
REFERENCES,0.6433333333333333,"r−1
2
r−1
3"
REFERENCES,0.6466666666666666,"Figure 7: Visualization for Appendix F. Original query (black and red) violating the original deﬁ-
nition of a valid query graph, and equivalently transformed query (black and blue) which fulﬁls the
properties becoming 1p-2i query."
REFERENCES,0.65,"E
ON MONOTONICITY"
REFERENCES,0.6533333333333333,"In this work, we restrict the qualiﬁers to respect monotonicity in the KG in the following sense:"
REFERENCES,0.6566666666666666,"(h, r, t, qp) ∈G ∧qp′ ⊆qp =⇒(h, r, t, qp′) ∈G"
REFERENCES,0.66,"This implies that if a qualiﬁed statement in the KG has a set of qualiﬁer pairs, then the KG also
contains the qualiﬁed statement with any subset thereof. As a result, some types of qualifying
information cannot be used, e.g., a qualiﬁer indicating that a relation does not hold (i.e., negation).
This breaks monotonicity since the existence of such a statement would further imply the existence
of that statement without the qualiﬁer, leading to contradiction."
REFERENCES,0.6633333333333333,"F
RELAXING THE REQUIREMENTS ON QUERIES"
REFERENCES,0.6666666666666666,"In the paper, we limited our queries with the same limitations as done in prior work. Here we will
discuss why not all of these restrictions are needed for our work, and how our evaluation already
includes some of these more general cases."
REFERENCES,0.67,As a reminder the deﬁnition of our queries is as follows:
REFERENCES,0.6733333333333333,"Deﬁnition F.1 (Hyper-relational Query). Let V be a set of variable symbols, and TAR ∈V a special
variable denoting the target of the query. Let E+ = E ⊎V. Then, any subset Q of (E+×R×E+×Q)
is a valid query if its induced graph"
REFERENCES,0.6766666666666666,"1) is a directed acyclic graph,"
REFERENCES,0.68,"2) has a topological ordering in which all entities (in this context referred to as anchors) occur
before all variables, and"
REFERENCES,0.6833333333333333,3) TAR must be last in the topological orderings.
REFERENCES,0.6866666666666666,"The main reason why we deal with more general queries is because our query encoder learns rep-
resentation for both normal and inverse relations. We use r−1 to indicate the inverse of relation r.
When encoding a query with a (normal) relation r, then both representations R[r] and R[r−1] are
used simultaneously. If we encounter a query using the inverse relation r−1, we can use that inverse
relation R[r−1] and the inverse of the inverse R[
 
r−1−1] = R[r], or in other words the normal
relation representation of r. This means that even if we only ever train with the normal relation, we
have the ability to invert relations, and hence we can lift some limitations."
REFERENCES,0.69,"For example, let us look at the query"
REFERENCES,0.6933333333333334,"Qnew pattern = {(e1, r1, TAR, {}), (TAR, r2, VAR1, {}), (VAR1, r3, e3, {})} ,"
REFERENCES,0.6966666666666667,"as shown in Fig. 7. This query breaks two of the requirements. It has an entity e3 occurring after
variables VAR1 and TAR in the topological ordering. And, TAR is not in the last position."
REFERENCES,0.7,"However, using inverse relations, we can convert this query into:"
REFERENCES,0.7033333333333334,"Qknown pattern =

(e1, r1, TAR, {}), (VAR1, r−1
2 , TAR, {}), (e3, r−1
3 , VAR1, {})
	
."
REFERENCES,0.7066666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.71,"This transformation converts the query into a 1p-2i query, which is a pattern among the evaluated
patterns in the paper. Because of this transformation, the pattern of Qnew pattern is indistinguishable
from the pattern of Qknown pattern from the perspective of the model. Besides the example illustrated
above, many other graphs can be converted into one of the patterns in the paper. Since we can
perform the conversion in both directions, all these possible patterns are also implicitly included in
our used datasets."
REFERENCES,0.7133333333333334,"In principle, the encoder does also not assume that there are no cycles in the query graph. However,
the effect of cycles requires further investigation."
REFERENCES,0.7166666666666667,"G
NUMBER OF MESSAGE PASSING STEPS EQUAL TO THE DIAMETER"
REFERENCES,0.72,"In Daza & Cochez (2020), the authors ﬁnd that the best results are achieved when making the
number of message passing steps equal to the diameter of the query, deﬁned as the longest shortest
path between 2 nodes in it. After these steps, they use the embedding of the target variable as
the embedding of the complete graph. Accordingly, we performed additional experiments with
this dynamic query embedding setting, and with a variant which uses an extra message passing
step. From these experiments we concluded that this approach did not lead to better results, which
contradicts with the ﬁndings in Daza & Cochez (2020)."
REFERENCES,0.7233333333333334,"H
QUALIFIER IMPACT ON PERFORMANCE"
REFERENCES,0.7266666666666667,"For a more ﬁne-grained analysis of qualiﬁers impact on query answering performance, for each
query pattern we ran an experiment measuring relative performance change in the presence and
absence of a given qualiﬁer relation in a query. That is, the main results of StarQE in Table 1 assume
all qualiﬁers are enabled. Then, for each qualiﬁer relation, we remove qualiﬁer pairs containing this
relation from all queries in a given pattern and run a model forward pass to obtain new predictions.
We run such experiments 5 times for each relation, each metric, and each pattern to get an average
value with standard deviations. Finally, for each metric, we sort relations in the ascending order of
their performance increase."
REFERENCES,0.73,"Table 4 reports top-3 worst (i.e., ﬁrst 3 relations in the sorted list) and top-3 best (last 3 relations in the
sorted list) qualiﬁer relations that have the most impact along the metrics. For example, P1686 (for
work), a common qualiﬁer of relation award received, increases Hits@10 performance in 1p-
2i queries by a large margin of 62% and MRR by 57% compared to queries without this qualiﬁer.
The gains are consistent, although on a smaller scale, in other patterns, too. On the other hand, P453
(character role), a qualiﬁer of cast member, seems to be the most confusing qualiﬁer in
2p queries leading to lower scores across all metrics."
REFERENCES,0.7333333333333333,"We note, however, that on a bigger picture (Fig. 8), where impact for all qualiﬁer relations is visual-
ized, total gains of having qualiﬁers outweigh negative effects from some of them."
HOP,0.7366666666666667,"1hop
1hop-2i
2hop
2i
2i-1hop
3hop
3i
pattern 60% 40% 20% 0% 20% 40% 60% 80%"
HOP,0.74,Improvement
HOP,0.7433333333333333,metric = AMRI
HOP,0.7466666666666667,"1hop
1hop-2i
2hop
2i
2i-1hop
3hop
3i
pattern"
HOP,0.75,metric = H@10
HOP,0.7533333333333333,"1hop
1hop-2i
2hop
2i
2i-1hop
3hop
3i
pattern"
HOP,0.7566666666666667,metric = MRR
HOP,0.76,"Figure 8: A general overview of qualiﬁers impact on AMRI, Hits@10, and MRR. In all metrics,
higher deltas correspond to better prediction performance. Having qualiﬁers does, on average, lead
to better predictions."
HOP,0.7633333333333333,Published as a conference paper at ICLR 2022
HOP,0.7666666666666667,"AMRI
H@10
MRR
pattern
relation
improvement
relation
improvement
relation
improvement"
P,0.77,1p
P,0.7733333333333333,"P518
−5.24 ±
2.09
P1264
+ 3.64 ±
1.50
P518
+ 2.88 ±
0.86
P453
−2.75 ±
0.78
P453
+ 6.00 ±
2.70
P453
+ 3.78 ±
0.98
P1264
−0.02 ±
0.42
P518
+ 6.84 ±
4.47
P17
+ 4.12 ±
1.23
P805
+ 2.42 ±
2.58
P1686
+13.70 ±
1.19
P1264
+11.17 ±
2.30
P17
+ 2.53 ±
3.02
P1346
+24.04 ±
3.87
P1346
+27.78 ±
4.01
P1346
+ 4.08 ±
3.30
P2453
+33.03 ±
4.26
P2453
+45.82 ±
3.48"
P,0.7766666666666666,2p
P,0.78,"P453
−64.91 ± 40.79
P459
−5.74 ±
2.76
P453
−2.05 ±
5.57
P2241
−10.62 ± 50.67
P453
−5.15 ± 12.26
P3680
−0.33 ±
0.55
P102
−7.14 ± 23.57
P1011
−4.55 ± 24.48
P1552
−0.01 ±
0.59
P805
+11.56 ±
2.26
P122
+45.00 ±
0.00
P407
+58.38 ±
7.57
P531
+13.35 ± 32.37
P1686
+47.00 ±
0.38
P1310
+64.49 ±
7.77
P1686
+24.04 ±
6.48
P837
+47.49 ±
5.83
P837
+78.12 ±
1.50"
P,0.7833333333333333,3p
P,0.7866666666666666,"P102
−4.53 ± 15.78
P155
−0.44 ±
0.99
P3680
−0.36 ±
0.57
P453
−4.23 ±
2.75
P1552
−0.16 ±
0.32
P175
−0.30 ±
0.46
P2241
−2.65 ± 12.77
P937
+ 0.00 ±
0.00
P1441
−0.02 ±
0.04
P2937
+14.16 ± 21.84
P812
+50.64 ± 11.27
P1310
+65.54 ±
7.14
P92
+16.77 ± 14.57
P122
+62.28 ±
0.00
P3823
+76.07 ± 10.71
P805
+17.95 ±
2.53
P3823
+72.85 ± 42.07
P837
+76.71 ±
1.65"
I,0.79,2i
I,0.7933333333333333,"P2715
−1.93 ±
5.53
P291
−0.08 ±
0.18
P3680
−0.07 ±
1.16
P453
−0.85 ±
3.90
P3680
+ 0.00 ±
0.00
P291
−0.06 ±
0.09
P4100
−0.67 ±
2.06
P2614
+ 0.00 ±
0.00
P459
+ 0.09 ±
1.11
P156
+ 3.42 ±
2.93
P518
+23.39 ±
4.00
P1686
+29.23 ±
1.98
P17
+ 4.08 ±
5.46
P1686
+29.49 ±
2.32
P1264
+34.53 ± 16.62
P805
+34.56 ±
5.43
P805
+34.64 ±
2.82
P805
+35.19 ±
1.50"
I,0.7966666666666666,3i
I,0.8,"P453
−2.18 ±
4.01
P2241
−3.44 ±
4.47
P2241
−8.03 ±
5.50
P2241
−1.37 ±
5.09
P4100
−0.33 ±
1.73
P3680
−0.01 ±
0.27
P4100
−0.77 ±
1.09
P1534
−0.33 ±
1.13
P291
−0.00 ±
0.00
P1013
+ 6.28 ±
4.06
P805
+16.72 ±
2.81
P642
+22.43 ± 15.70
P805
+14.13 ±
2.31
P3831
+17.49 ±
2.35
P518
+28.26 ±
2.68
P2842
+42.44 ± 36.23
P518
+21.94 ±
1.42
P1264
+32.88 ± 25.04 2i-1p"
I,0.8033333333333333,"P2241
−5.29 ±
4.07
P1346
−0.53 ±
0.11
P3680
−0.32 ±
0.31
P459
−1.81 ±
7.30
P2453
−0.05 ±
0.11
P2453
+ 0.03 ±
0.43
P366
−0.01 ±
0.02
P3680
+ 0.00 ±
0.00
P1441
+ 0.40 ±
1.20
P131
+ 3.89 ±
4.81
P39
+51.71 ± 36.91
P1686
+53.11 ±
0.58
P805
+10.37 ±
5.57
P1686
+56.63 ±
0.73
P39
+55.05 ± 23.43
P1686
+24.58 ±
6.44
P837
+62.98 ± 27.55
P837
+82.67 ±
2.60 1p-2i"
I,0.8066666666666666,"P2241
−3.56 ±
8.66
P459
−6.63 ± 14.51
P2241
−8.57 ±
7.45
P17
−3.30 ±
4.91
P407
−6.34 ± 14.91
P1039
−3.33 ±
6.94
P453
−2.66 ±
5.26
P2241
−3.13 ±
3.40
P92
−0.70 ±
2.65
P1013
+24.53 ± 21.48
P102
+49.04 ± 12.18
P1686
+57.30 ±
1.64
P654
+31.24 ± 46.20
P1686
+61.66 ±
2.20
P805
+57.36 ±
0.98
P805
+77.99 ± 10.23
P805
+63.10 ±
0.38
P837
+75.11 ±
3.91"
I,0.81,"Table 4: Top 3 worst and top 3 best impacting qualiﬁer relations per pattern. In all metrics (AMRI,
H@10, MRR), positive value corresponds to better predictions. For some metrics, and given pat-
terns we only see improvements in cases where qualiﬁers are included. The qualiﬁer relation P453
(speciﬁc role played or ﬁlled by subject – as a “cast member” or “voice actor”) seems to confuse
the model the most. On the other hand P805 (referring to an item that describes the relation identi-
ﬁed in the statement) often leads to the biggest improvements in the metrics. All relation names are
clickable links to their Wikidata pages."
I,0.8133333333333334,Published as a conference paper at ICLR 2022
I,0.8166666666666667,"I
THE ORACLE SETUP"
I,0.82,"There are several QE methods we could compare our work with. However, these methods are
only able to answer queries utilising triple and not qualiﬁer information. Besides, we see that new
methods are introduced regularly, each outperforming the previous method. Hence, we employ an
Oracle approach to compute the upper bound that triple only QE models can achieve on WD50K-
QE. This Oracle is simulating perfect link prediction and ranking capabilities but does not have
access to qualiﬁer information."
I,0.8233333333333334,"To achieve this, the Oracle system has access to training, validation, as well as test data. Given the
access to the totality of the data, the system will return an optimal ordering, i.e., one that maximizes
our reported metrics. However, since the system cannot process qualiﬁer information, the ordering
of the result set can only be based on the entities and relations in the query. This means that queries
that are the same when ignoring the qualiﬁer information, will get the same list of answers. In effect,
the oracle will, despite its perfect information for a setting without qualiﬁers, not answer perfectly.
Using this setting we can hence investigate how much difference qualiﬁer information makes for our
queries."
I,0.8266666666666667,"Finally, note that because multiple ordering can be considered optimal from the information the
Oracle has available, we report the expected value for these metrics."
I,0.83,"J
DETAILED RESULTS"
I,0.8333333333333334,"We provide our chosen hyper-parameters after performing hyper-parameter optimisation in Table 6
and detailed results including standard deviation across ﬁve runs with different random seeds in Ta-
bles 7 and 8. We report Hits@k for k = 1, 3, 10, mean reciprocal rank (MRR), and adjusted mean
rank index (AMRI) (Berrendorf et al., 2020). For all metrics, larger values indicate better perfor-
mance. We highlight the best result per column and metric in bold font."
I,0.8366666666666667,"For the Hits@k metric, we observe StarQE performing best across patterns, except for the most
simple ones, 1p (equivalent to plain link prediction), and 3i. Standard deviations are usually small
(around 1% point). The base triples baseline sometimes exhibits larger variances across multiple
random seeds. For MRR we make similar observations as for Hits@k, which is intuitive since it can
be seen as a soft version of H@k."
I,0.84,"For AMRI, the picture differs slightly. AMRI is a linear transformation of the mean rank, which
normalizes the scores such that 0 corresponds to the performance of a random scoring model and
100% to the perfect score. Besides, AMRI preserves the linearity of mean rank, i.e., it is equally
inﬂuenced by improvements on any rank, not just at the top of the ranking. Across the board, we
observe values far beyond 50%, usually exceeding 90%. The general tendency of some patterns
being more difﬁcult than others persists and is coherent with the other metrics. Comparing different
models, we can see that StarQE falls behind, e.g., reiﬁcation and more simple baselines such as zero-
layers, which do not use the graph structure. Since this behavior was not observed for the metrics
more focused on the top positions in ranking, the decrease in performance in this metric has to be
caused by entities that received large ranks in the ﬁrst place."
I,0.8433333333333334,"Zooming in on the Oracle setup, we make several observations. First, it becomes increasingly harder
for the Oracle to produce good results for smaller values of k in the Hits@k metrics. Since this is an
upper bound to the Base triple setup, also that model shows similar behavior. The observation that
the intersection queries can still perform well is also visible in the more detailed results."
I,0.8466666666666667,"The very high AMRI for the Oracle is also expected. Since this model is designed as the best
possible ranking model, it will place all correct answers for the query (while ignoring qualiﬁers)
at the top of the ranking. This set of answers is a super set of the correct answers to the qualiﬁed
query (see Appendix D). Now, that set of answers is nearly always very small compared to the 50K
candidate entities in our dataset. So, relatively speaking the correct answers are still nearly always
ranked high, and hence we obtain a high AMRI score."
I,0.85,Published as a conference paper at ICLR 2022
I,0.8533333333333334,"K
EVALUATING FAITHFULNESS"
I,0.8566666666666667,"Following the idea of EmQL (Sun et al., 2020), we evaluate faithfulness of our hyper-relational
approach by evaluating its performance on the training set, that is, an ability to correctly answer
already seen queries. Evaluation on the training set is a suitable proxy for faithfulness since even
the union of training, validation and test queries (as done in the original work) is highly incomplete
considering the whole background graph (be in Freebase or Wikidata). To this end, we evaluate the
model trained in two regimes: StarQE-like trained on all query types and MPQE-like trained in the
hardest setting on only 1p link prediction queries. The results presented in Table 5 show that the
StarQE-like model exhibits faithfulness saturating performance metrics to almost perfect results. As
expected, training the model only on one query type inhibits faithfulness qualities."
I,0.86,"Table 5: Full results for the faithfulness experiments, including standard deviation across ﬁve runs
with different random seeds. We report Hits@k for k = 1, 3, 10, mean reciprocal rank (MRR), and
adjusted mean rank index (AMRI) Berrendorf et al. (2020). For all metrics, larger values indicate
better performance."
I,0.8633333333333333,"Pattern
1p
2p
3p
2i
3i
2i-1p
1p-2i"
I,0.8666666666666667,Hits@1 (%)
I,0.87,"StarQE-like
74.27 ± 5.30
94.58 ± 1.81
89.93 ± 2.29
94.29 ± 1.67
99.34 ± 0.24
96.02 ± 1.45
99.05 ± 0.46
MPQE-like
90.43 ± 2.10
8.00 ± 0.85
19.99 ± 2.05
87.88 ± 1.53
92.44 ± 1.56
13.68 ± 0.97
65.51 ± 2.60"
I,0.8733333333333333,Hits@3 (%)
I,0.8766666666666667,"StarQE-like
84.06 ± 4.64
98.12 ± 0.49
96.56 ± 0.89
97.48 ± 1.15
99.81 ± 0.11
98.78 ± 0.51
99.84 ± 0.09
MPQE-like
96.18 ± 1.10
14.19 ± 1.18
32.12 ± 2.59
97.17 ± 0.62
99.46 ± 0.16
18.64 ± 1.43
85.42 ± 1.24"
I,0.88,Hits@10 (%)
I,0.8833333333333333,"StarQE-like
90.94 ± 4.18
99.39 ± 0.16
98.98 ± 0.38
99.18 ± 0.67
99.96 ± 0.04
99.64 ± 0.20
99.97 ± 0.03
MPQE-like
98.75 ± 0.41
22.62 ± 1.56
47.09 ± 3.14
99.46 ± 0.18
99.94 ± 0.02
23.98 ± 1.70
94.90 ± 0.35"
I,0.8866666666666667,MRR (%)
I,0.89,"StarQE-like
80.23 ± 4.85
96.51 ± 1.09
93.50 ± 1.51
96.09 ± 1.32
99.59 ± 0.17
97.47 ± 0.95
99.45 ± 0.27
MPQE-like
93.58 ± 1.52
12.88 ± 1.06
28.88 ± 2.36
92.70 ± 0.94
95.93 ± 0.81
17.36 ± 1.23
76.54 ± 1.74"
I,0.8933333333333333,AMRI (%)
I,0.8966666666666666,"StarQE-like
99.97 ± 0.03
99.99 ± 0.00
99.99 ± 0.00
100.00 ± 0.00
100.00 ± 0.00
100.00 ± 0.00
100.00 ± 0.00
MPQE-like
100.00 ± 0.00
91.77 ± 2.29
94.64 ± 2.59
100.00 ± 0.00
100.00 ± 0.00
93.97 ± 2.09
99.99 ± 0.00"
I,0.9,Published as a conference paper at ICLR 2022
I,0.9033333333333333,Table 6: Best hyper-parameter as chosen after hyper-parameter optimization
I,0.9066666666666666,"Experiment
StarQE
Reiﬁcation
Base Triple
Zero Layers
MPQE-like
MPQE-like + Reif
EmQL-like
Q2B-like"
I,0.91,"activation
leakyrelu
relu
relu
relu
prelu
relu
leakyrelu
leakyrelu
optimiser
adam
adam
adam
adam
adam
adam
adam
adam
learning-rate
0.0007741
0.003768
0.007253
0.0008733
0.0005256
0.0001414
0.007253
0.002075
batch-size
64
32
32
64
128
32
128
32
graph-pooling
targetpooling
sum
sum
sum
targetpooling
targetpooling
targetpooling
sum
message-weighting
attention
attention
attention
symmetric
attention
symmetric
symmetric
attention
similarity
dotproduct
negativepowernorm
negativepowernorm
negativepowernorm
dotproduct
dotproduct
dotproduct
negativepowernorm
num-layers
3
2
2
2
3
3
3
3
use-bias
True
True
True
False
False
False
False
True
embedding-dim
192
224
224
160
128
96
256
224
dropout
0.5
0.3
0.3
0.3
0.5
0.2
0.3
0.1
composition
multiplication
multiplication
multiplication
multiplication
multiplication
multiplication
multiplication
multiplication"
I,0.9133333333333333,Published as a conference paper at ICLR 2022
I,0.9166666666666666,"Table 7: Full results for baseline experiments, including standard deviation across ﬁve runs with different random seeds. We report Hits@k for k = 1, 3, 10, mean
reciprocal rank (MRR), and adjusted mean rank index (AMRI) Berrendorf et al. (2020). For all metrics, larger values indicate better performance. We highlight the
best result per column and metric (excluding the Oracle) in bold font."
I,0.92,"Pattern
1p
2p
3p
2i
3i
2i-1p
1p-2i"
I,0.9233333333333333,Hits@1 (%)
I,0.9266666666666666,"StarQE
20.91 ± 1.11
39.98 ± 0.27
45.85 ± 0.81
55.11 ± 1.26
78.58 ± 1.44
51.77 ± 0.73
65.77 ± 1.87
Base Triple
11.62 ± 0.68
2.97 ± 0.21
7.57 ± 1.59
36.87 ± 2.22
64.72 ± 6.97
3.33 ± 0.33
13.00 ± 1.07
Reiﬁcation
24.26 ± 0.88
38.82 ± 0.41
42.80 ± 0.50
49.67 ± 1.86
78.93 ± 3.20
42.39 ± 0.68
55.05 ± 1.11
Zero Layers
18.30 ± 0.31
12.92 ± 0.25
11.53 ± 0.13
40.35 ± 0.82
74.27 ± 0.32
18.17 ± 0.25
19.51 ± 0.22
Oracle
37.87
7.19
13.86
75.67
94.07
7.02
32.61"
I,0.93,Hits@3 (%)
I,0.9333333333333333,"StarQE
35.58 ± 0.82
46.41 ± 0.32
57.34 ± 0.58
67.98 ± 0.93
87.56 ± 0.74
56.89 ± 0.47
75.35 ± 1.02
Base Triple
25.54 ± 0.28
6.38 ± 0.32
15.44 ± 1.56
53.45 ± 2.60
81.55 ± 4.70
7.40 ± 0.67
23.56 ± 1.49
Reiﬁcation
40.12 ± 1.37
45.58 ± 0.74
55.45 ± 0.88
68.31 ± 0.77
89.84 ± 1.90
51.33 ± 0.37
70.26 ± 0.83
Zero Layers
31.98 ± 0.23
21.34 ± 0.34
25.24 ± 0.27
55.83 ± 0.42
85.43 ± 0.37
25.75 ± 0.26
35.67 ± 0.28
Oracle
61.65
13.06
24.74
88.42
98.36
15.20
53.94"
I,0.9366666666666666,Hits@10 (%)
I,0.94,"StarQE
51.72 ± 0.66
51.20 ± 0.44
65.50 ± 0.39
77.78 ± 0.53
92.64 ± 0.65
61.81 ± 0.37
81.60 ± 0.60
Base Triple
45.04 ± 1.18
12.76 ± 0.92
24.66 ± 2.15
69.74 ± 1.43
91.74 ± 1.83
16.77 ± 1.44
40.67 ± 1.31
Reiﬁcation
55.17 ± 1.00
50.86 ± 0.39
63.65 ± 0.60
81.25 ± 0.45
95.31 ± 0.78
61.05 ± 0.92
80.49 ± 0.81
Zero Layers
44.93 ± 0.35
29.94 ± 0.35
38.45 ± 0.18
67.79 ± 0.42
90.66 ± 0.20
35.48 ± 0.31
47.85 ± 0.43
Oracle
81.03
24.11
38.47
95.54
99.67
32.74
76.96"
I,0.9433333333333334,MRR (%)
I,0.9466666666666667,"StarQE
30.98 ± 0.91
44.13 ± 0.28
52.96 ± 0.61
63.14 ± 0.97
83.78 ± 1.00
55.20 ± 0.52
71.52 ± 1.37
Base Triple
22.25 ± 0.25
6.73 ± 0.28
14.05 ± 1.72
48.01 ± 2.13
74.52 ± 5.35
8.16 ± 0.62
22.23 ± 1.21
Reiﬁcation
34.78 ± 1.07
43.42 ± 0.48
50.77 ± 0.54
61.01 ± 1.17
85.17 ± 2.33
49.09 ± 0.63
64.21 ± 0.86
Zero Layers
27.55 ± 0.19
19.10 ± 0.27
21.25 ± 0.14
50.27 ± 0.57
80.62 ± 0.28
24.49 ± 0.22
30.04 ± 0.21
Oracle
52.64
13.44
22.65
83.00
96.34
15.84
47.00"
I,0.95,AMRI (%)
I,0.9533333333333334,"StarQE
78.44 ± 1.78
68.11 ± 2.52
74.62 ± 2.94
93.63 ± 0.81
98.65 ± 0.33
73.14 ± 2.39
88.76 ± 0.61
Base Triple
88.14 ± 0.21
81.25 ± 0.46
95.92 ± 0.11
99.18 ± 0.12
99.95 ± 0.01
83.44 ± 0.95
98.91 ± 0.11
Reiﬁcation
88.81 ± 0.47
86.63 ± 1.41
96.20 ± 0.28
99.24 ± 0.10
99.96 ± 0.01
85.46 ± 0.82
99.10 ± 0.12
Zero Layers
88.03 ± 0.22
90.56 ± 0.47
95.79 ± 0.20
96.34 ± 0.30
98.37 ± 0.13
86.18 ± 0.60
98.55 ± 0.10
Oracle
99.97
99.77
99.83
99.99
100.00
99.88
99.97"
I,0.9566666666666667,Published as a conference paper at ICLR 2022
I,0.96,"Table 8:
Full results for the generalization experiments, including standard deviation across ﬁve runs with different random seeds. We report Hits@k for
k = 1, 3, 10, mean reciprocal rank (MRR), and adjusted mean rank index (AMRI) Berrendorf et al. (2020). For all metrics, larger values indicate better performance.
We highlight the best result per column and metric in bold font."
I,0.9633333333333334,"Pattern
1p
2p
3p
2i
3i
2i-1p
1p-2i"
I,0.9666666666666667,Hits@1 (%)
I,0.97,"StarE-like
20.91 ± 1.11
39.98 ± 0.27
45.85 ± 0.81
55.11 ± 1.26
78.58 ± 1.44
51.77 ± 0.73
65.77 ± 1.87
Q2B-like
21.63 ± 0.70
37.15 ± 0.65
43.82 ± 0.98
52.49 ± 1.45
77.36 ± 1.46
37.59 ± 2.88
59.74 ± 3.29
emQL-like
23.06 ± 1.12
6.78 ± 2.24
19.86 ± 4.99
53.36 ± 3.46
81.73 ± 3.63
2.17 ± 0.87
48.54 ± 5.09
MPQE-like
16.53 ± 0.48
3.81 ± 0.38
12.70 ± 1.59
41.01 ± 1.49
60.67 ± 2.23
6.84 ± 0.58
28.76 ± 1.40
MPQE-like + Reif
25.52 ± 0.36
3.24 ± 0.62
10.52 ± 2.09
41.36 ± 0.59
63.93 ± 1.36
5.59 ± 0.72
18.75 ± 1.55"
I,0.9733333333333334,Hits@3 (%)
I,0.9766666666666667,"StarE-like
35.58 ± 0.82
46.41 ± 0.32
57.34 ± 0.58
67.98 ± 0.93
87.56 ± 0.74
56.89 ± 0.47
75.35 ± 1.02
Q2B-like
38.79 ± 0.92
43.75 ± 0.98
55.41 ± 1.21
67.02 ± 1.24
87.97 ± 1.23
46.99 ± 3.04
71.20 ± 2.45
emQL-like
36.14 ± 1.11
10.73 ± 3.23
30.47 ± 6.37
66.56 ± 3.18
89.91 ± 2.87
3.80 ± 1.05
55.96 ± 5.31
MPQE-like
30.82 ± 0.75
7.15 ± 0.65
21.48 ± 2.58
66.87 ± 1.21
86.17 ± 1.45
10.08 ± 1.08
44.88 ± 1.30
MPQE-like + Reif
41.26 ± 0.65
6.34 ± 0.82
18.25 ± 2.69
68.14 ± 0.97
90.07 ± 0.58
8.56 ± 0.59
32.31 ± 2.60"
I,0.98,Hits@10 (%)
I,0.9833333333333333,"StarE-like
51.72 ± 0.66
51.20 ± 0.44
65.50 ± 0.39
77.78 ± 0.53
92.64 ± 0.65
61.81 ± 0.37
81.60 ± 0.60
Q2B-like
55.44 ± 1.35
51.10 ± 1.40
66.39 ± 1.72
78.79 ± 1.04
94.20 ± 0.86
57.49 ± 2.26
80.49 ± 1.92
emQL-like
50.10 ± 1.34
16.45 ± 3.56
44.36 ± 4.96
75.86 ± 2.45
93.55 ± 1.93
6.79 ± 1.13
62.80 ± 5.26
MPQE-like
48.48 ± 0.59
12.57 ± 0.96
34.19 ± 3.78
83.04 ± 1.01
96.32 ± 0.77
14.75 ± 1.42
61.02 ± 0.70
MPQE-like + Reif
58.43 ± 0.29
12.02 ± 1.12
31.14 ± 2.70
83.77 ± 0.47
97.22 ± 0.14
13.50 ± 0.81
50.92 ± 3.44"
I,0.9866666666666667,MRR (%)
I,0.99,"StarE-like
30.98 ± 0.91
44.13 ± 0.28
52.96 ± 0.61
63.14 ± 0.97
83.78 ± 1.00
55.20 ± 0.52
71.52 ± 1.37
Q2B-like
33.04 ± 0.84
41.99 ± 0.88
51.71 ± 1.18
61.72 ± 1.18
83.49 ± 1.24
44.24 ± 2.73
67.04 ± 2.70
emQL-like
32.01 ± 1.00
10.09 ± 2.70
27.94 ± 5.11
61.45 ± 3.13
86.28 ± 3.11
3.73 ± 0.94
53.58 ± 5.03
MPQE-like
26.83 ± 0.53
6.79 ± 0.56
19.72 ± 2.23
56.16 ± 1.30
74.35 ± 1.70
9.62 ± 0.85
39.81 ± 1.20
MPQE-like + Reif
36.36 ± 0.38
6.12 ± 0.74
17.11 ± 2.27
56.81 ± 0.69
77.29 ± 0.86
8.32 ± 0.52
29.25 ± 2.05"
I,0.9933333333333333,AMRI (%)
I,0.9966666666666667,"StarE-like
78.44 ± 1.78
68.11 ± 2.52
74.62 ± 2.94
93.63 ± 0.81
98.65 ± 0.33
73.14 ± 2.39
88.76 ± 0.61
Q2B-like
88.69 ± 0.45
87.17 ± 1.48
95.60 ± 0.58
99.01 ± 0.20
99.93 ± 0.03
83.24 ± 1.08
98.49 ± 0.40
emQL-like
73.20 ± 11.74
51.74 ± 13.25
74.93 ± 10.70
90.68 ± 5.62
97.79 ± 1.60
37.80 ± 22.89
82.59 ± 8.45
MPQE-like
93.77 ± 0.09
83.25 ± 2.61
92.00 ± 3.04
99.49 ± 0.05
99.96 ± 0.01
85.61 ± 2.86
99.04 ± 0.16
MPQE-like + Reif
95.68 ± 0.09
82.93 ± 1.83
92.08 ± 1.43
99.57 ± 0.02
99.97 ± 0.00
86.54 ± 1.56
99.25 ± 0.06"
