Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002967359050445104,"Performance of recommender systems (RecSys) relies heavily on the amount of
training data available. This poses a chicken-and-egg problem for early-stage
products, whose amount of data, in turn, relies on the performance of their RecSys.
In this paper, we explore the possibility of zero-shot learning in RecSys, to enable
generalization from an old dataset to an entirely new dataset. We develop an
algorithm, dubbed ZEro-Shot Recommenders (ZESREC), that is trained on an
old dataset and generalize to a new one where there are neither overlapping users
nor overlapping items, a setting that contrasts typical cross-domain RecSys that
has either overlapping users or items. Different from previous methods that use
categorical item indices (i.e., item ID), ZESREC uses items‚Äô generic features, such
as natural-language descriptions, product images, and videos, as their continuous
indices, and therefore naturally generalizes to any unseen items. In terms of users,
ZESREC builds upon recent advances on sequential RecSys to represent users
using their interactions with items, thereby generalizing to unseen users as well.
We study three pairs of real-world RecSys datasets and demonstrate that ZESREC
can successfully enable recommendations in such a zero-shot setting, opening
up new opportunities for resolving the chicken-and-egg problem for data-scarce
startups or early-stage products."
INTRODUCTION,0.005934718100890208,"1
INTRODUCTION"
INTRODUCTION,0.008902077151335312,"Many large scale e-commerce platforms (such as Etsy, Overstock, etc.) and online content plat-
forms (such as Spotify, Overstock, Disney+, NetÔ¨Çix, etc) have such a large inventory of items that
showcasing all of them in front of their users is simply not practical. In particular, in the online
content category of businesses, it is often seen that users of their service do not have a crisp intent in
mind unlike in the retail shopping experience where the users often have a clear intent of purchasing
something. The need for personalized recommendations therefore arises from the fact that not only it
is impractical to show all the items in the catalogue but often times users of such services need help
discovering the next best thing ‚Äî be it the new and exciting movie or be it a new music album or
even a piece of merchandise that they may want to consider for future buying if not immediately."
INTRODUCTION,0.011869436201780416,"Modern personalized recommendation models of users and items have often relied on the idea of
extrapolating preferences from similar users. Different machine learning models deÔ¨Åne the notion of
similarity differently. Classical bi-linear Matrix Factorization (MF) approaches model users and items
via their identiÔ¨Åers and represent them as vectors in the latent space [13; 28]. Modern deep-learning-
based recommender systems [34; 12; 25], which are also used for predicting top-k items given an
item, learn the user-to-item propensities from large amounts of training data containing many (user,
item) tuples, optionally with available item content information (e.g., product descriptions) and user
metadata."
INTRODUCTION,0.01483679525222552,"As machine learning models, the performance of RecSys relies heavily on the amount of training data
available. This might be feasible for large e-commerce or content delivery websites such as Overstock
and NetÔ¨Çix, but poses a serious chicken-and-egg problem for small startups, whose amount of data,
in turn, relies on the performance of their RecSys. On the other hand, zero-shot learning promises
some degree of generalization from an old dataset to an entirely new dataset. In this paper, we
explore the possibility of zero-shot learning in RecSys. We develop an algorithm, dubbed ZEro-Shot
Recommenders (ZESREC), that is trained on an old dataset and generalize to a new one where there
are neither overlapping users nor overlapping items, a setting that contrasts typical cross-domain"
INTRODUCTION,0.017804154302670624,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.020771513353115726,"RecSys that has either overlapping users or items [40; 38; 3; 16]. Naturally, generalization of RecSys
to unseen users and unseen items becomes the two major challenges for developing zero-shot RecSys."
INTRODUCTION,0.02373887240356083,"For the Ô¨Årst challenge on unseen users, we build on a rich body of literature on sequential recommen-
dation models [12; 27; 25; 14; 17; 21]. These models are built with sequential structure to encode
temporal ordering of items in user‚Äôs item interaction history. Representing users by the items they
have consumed in the past allows the model to extrapolate the preference learning to even novel users
who the model did not see during training, as long as the items these unseen users have interacted with
have been seen during training. However, such deep learning models encode item via its categorical
item index, i.e., the item ID, and therefore fall short in predicting a likely relevant but brand-new item
not previously seen during training."
INTRODUCTION,0.026706231454005934,"This brings us to the second challenge of developing zero-shot recommender systems, i.e., dealing
with unseen items. To address this challenge, ZESREC goes beyond traditional categorical item
indices and uses items‚Äô generic features such as natural-language descriptions, product images,
and videos as their continuous indices, thereby naturally generalizing to any unseen items. Take
natural-language (NL) descriptions as an example. One can think of NL descriptions as a system of
universal identiÔ¨Åers that indexes items from arbitrary domains. Therefore as long as one model is
trained on a dataset with NL descriptions, it can generalize to a completely different dataset with a
similar NL vocabulary. In ZESREC we build on state-of-the-art pretrained NL embedding models
such as BERT [8] to extract NL embeddings from raw NL descriptions, leading to an item ID system
in the continuous space that is generalizable across arbitrary domains. For instance, in e-commerce
platform, one could use items‚Äô description text; and similarly in the online content platforms, one
could use movie synopsis or music track descriptions to represent an item."
INTRODUCTION,0.02967359050445104,"Combining the merits of sequential RecSys and the idea of universal continuous ID space, our
ZESREC successfully enables recommendation in an extreme cold-start setting, i.e., the zero-shot
setting where all users and items in the target domain are unseen during training. Essentially ZESREC
tries to learn transferable user behavioral patterns in a universal continuous embedding space. For
example, in the source domain, ZESREC can learn that if users purchase snacks or drinks (e.g., ‚ÄòVita
Coconut Water‚Äô with a lemonade Ô¨Çavor) that they like, they may purchase similar snacks or drinks
with different Ô¨Çavors (e.g., ‚ÄòVita Coconut Water‚Äô with a pineapple Ô¨Çavor), as shown in the case study.
Later in the target domain, if one user purchase ‚ÄòV8 Splash‚Äô with a tropical Ô¨Çavor, ZESREC can
recommend ‚ÄòV8 Splash‚Äô with a berry Ô¨Çavor to the user (see Fig. 4 of Sec. 4.6 for details). Such
generalization is possible due to the use of the NL descriptions as universal identiÔ¨Åers, based on
which ZESREC could easily identify similar products of the same brand with different Ô¨Çavors. To
summarize our contributions:"
INTRODUCTION,0.032640949554896145,"‚Ä¢ We identify the problem of zero-shot recommender systems and propose ZESREC as the
Ô¨Årst hierarchical Bayesian model for addressing this problem.
‚Ä¢ We introduce the notion of universal continuous identiÔ¨Åers that makes recommendation in a
zero-shot setting possible.
‚Ä¢ We provide empirical results which show that ZESREC can successfully recommend items
in the zero-shot setting.
‚Ä¢ We conduct case studies demonstrating that ZESREC can learn interpretable user behavioral
patterns that can generalize across datasets."
RELATED WORK,0.03560830860534125,"2
RELATED WORK"
RELATED WORK,0.03857566765578635,"Deep Learning for RecSys. Deep learning has been prevalent in modern recommender systems [29;
33; 34; 19; 4; 10; 32] due to its scalability and superior performance. As a pioneer work, [29] uses
restricted Boltzmann machine (RBM) to perform collaborative Ô¨Åltering in recommender systems,
however the system is a single-layer RBM. Later, [34] and [19] build upon Bayesian deep learning to
develop hierarchical Bayesian models that tightly integrate content information and user-item rating
information, thereby signiÔ¨Åcantly improving recommendation performance. After that, there are also
various proposed sequential (or session-based) recommender systems [12; 27; 2; 18; 22; 39; 14; 31;
41; 25], GRU4Rec [12] was Ô¨Årst proposed to use gated recurrent units (GRU) [6] for recommender
systems. Since then, follow-up works such as hierarchical GRU [27], temporal convolutional networks
(TCN) [2], and hierarchical RNN (HRNN) [25] have achieved improvement in terms of accuracy by
utilizing cross-session information [27], causal convolutions [2], as well as meta data and control"
RELATED WORK,0.04154302670623145,Under review as a conference paper at ICLR 2022
RELATED WORK,0.04451038575667656,"signals [25]. Another line of work focusing on building self-attention based sequential models such
as SASRec [14], BERT4Rec [31], and S3Rec [41]. In this paper we build on such sequential RecSys
and note that our ZESREC is model agnostic, that is, it is compatible with any sequential RecSys."
RELATED WORK,0.04747774480712166,"Cross-Domain and Cold-Start RecSys. There is a rich literature on cross-domain RecSys focusing
on training a recommender system in the source domain and deploying it in the target domain where
there exist either common users or items [40; 38; 3; 16]. These works are also related to the problem
of recommendation for cold-start users and items, i.e., users and items with few interactions (or
ratings) available during training [11; 20; 42; 23]. There are also works [24; 9] handling cold start on
both user and item with meta-learning, however they cannot generalize across domains. In summary,
prior systems are either (1) not sufÔ¨Åcient to address our zero-shot setting where there are neither
common users nor common items in the target domain or (2) unable to learn user behavior patterns
that are transferable across datasets/domains. Therefore, they are not applicable to our problem of
zero-shot recommendations."
ZERO-SHOT RECOMMENDER SYSTEMS,0.050445103857566766,"3
ZERO-SHOT RECOMMENDER SYSTEMS"
ZERO-SHOT RECOMMENDER SYSTEMS,0.05341246290801187,"In this section we introduce our ZESREC which is compatible with any sequential model. Without
loss of generality, here we focus on NL descriptions as a possible instantiation of universal identiÔ¨Åers,
but note that our method is general enough to use as identiÔ¨Åers other content information such as
items‚Äô images and videos. We leave exploration for other potential modalities to future work."
ZERO-SHOT RECOMMENDER SYSTEMS,0.05637982195845697,"Notation. We focus on the setting of zero-shot recommendation where there are neither overlapping
users nor overlapping items between a source domain and a target domain. We assume a set Vs
of Js items and a set Us of Is users in the source domain, as well as a set Vt of Jt items and a set
Ut of It users in the target domain. We let I = Is + It and J = Js + Jt; we use j ‚ààVs ‚à™Vt to
index items and i ‚ààUs ‚à™Ut to index users. The zero-shot setting dictates that Vs ‚à©Vt = ‚àÖand
that Us ‚à©Ut = ‚àÖ. We denote the collection of all users‚Äô interactions as a 3D tensor (with necessary
zero-padding) R ‚ààRI√óNmax√óJ, where Nmax is the maximum number of interactions among all
users. We use the subscript ‚Äò‚àó‚Äô to represent the collection of all elements in a certain dimension.
SpeciÔ¨Åcally, each user i has a sequence of Ni interactions (e.g., purchase history) with various items
denoted as Ri‚àó‚àó= [Rit‚àó]Ni
t=1, where Rit‚àó‚àà{0, 1}J is one-hot vector denoting the t-th item user i
interacted with. The same user i has different user embeddings at different time t, reÔ¨Çecting dynamics
in user interests; here we denote as uit ‚ààRD the latent user vector when user i interacts with the
t-th item in her history, and we use U = [uit]I,Nmax
i=1,t=1 ‚ààRI√óNmax√óD (with necessary zero-padding)
to denote the collection of user latent vectors. We denote as vj ‚ààRD the item latent vector and
V = [vj]J
j=1RJ√óD as the collection. For simplicity and without loss of generality, in this paper we
focus on using NL descriptions (i.e., a sequence of words) to describe items. For item j we denote its
NL description as xj, and the number of words as Mj; similar to V, we let X = [xj]J
j=1. With slight
notation overload on t, we denote as R(s) and R(t) the sub-tensor of R that corresponds to the source
and target domains, respectively. Similarly, we also have U(s), U(t), V(s), V(t), X(s), and X(t)."
ZERO-SHOT RECOMMENDER SYSTEMS,0.05934718100890208,"Problem Setup. A model is trained using all users‚Äô interaction sequences from the source domain,
i.e., {Ri‚àó‚àó}i‚ààUs, and then deployed to recommend items for any user Œπ ‚ààUt in the target domain,
given user Œπ‚Äôs previous history RŒπ‚àó‚àó, which can be empty. In practice we append a dummy item at the
beginning of each user session, so that during inference we could conduct recommend even for users
without any history by ingesting the dummy item as context to infer the user latent vector. In our
zero-shot setting, the model is not allowed to Ô¨Åne-tune or retrain on any data from the target domain."
ZERO-SHOT RECOMMENDER SYSTEMS,0.06231454005934718,"DeÔ¨Ånition of Zero-shot Learning in RecSys. Our zero-shot setting includes three unique properties:
(1) cold users, (2) cold items, and (3) domain gap. It is fundamentally different from previous
content-based cold start as the latter setting usually satisÔ¨Åes either (1) or (2), but not often both."
FROM CATEGORICAL DOMAIN-SPECIFIC ITEM ID TO CONTINUOUS UNIVERSAL ITEM ID,0.06528189910979229,"3.1
FROM CATEGORICAL DOMAIN-SPECIFIC ITEM ID TO CONTINUOUS UNIVERSAL ITEM ID"
FROM CATEGORICAL DOMAIN-SPECIFIC ITEM ID TO CONTINUOUS UNIVERSAL ITEM ID,0.06824925816023739,"Most models in recommender systems learn item embeddings through interactions. These embeddings
are indexed by categorical domain-speciÔ¨Åc item ID, which is transductive and cannot be generalized
to unseen items."
FROM CATEGORICAL DOMAIN-SPECIFIC ITEM ID TO CONTINUOUS UNIVERSAL ITEM ID,0.0712166172106825,Under review as a conference paper at ICLR 2022
FROM CATEGORICAL DOMAIN-SPECIFIC ITEM ID TO CONTINUOUS UNIVERSAL ITEM ID,0.07418397626112759,"In this paper, we propose to use item generic content information such as NL descriptions and
image to produce item embeddings, which can be used as continuous universal item ID. Since such
content information is domain agnostic, the model trained on top of it can be transferable from one
domain to another, therefore making zero-shot recommender systems feasible. Based on the universal
item embeddings, one can then build sequential models to obtain user embeddings by aggregating
embeddings of items in the user history."
FROM CATEGORICAL DOMAIN-SPECIFIC ITEM ID TO CONTINUOUS UNIVERSAL ITEM ID,0.0771513353115727,"Here we introduce the notion of universal embedding networks (UEN) that use continuous universal
embeddings to index items (and therefore users) rather than categorical ID that is not transferable
across domains. We call the UEN generating item and user universal embeddings item UEN and user
UEN, respectively."
MODEL OVERVIEW,0.08011869436201781,"3.2
MODEL OVERVIEW"
MODEL OVERVIEW,0.0830860534124629,"We propose a hierarchical Bayesian model with a probabilistic encoder-decoder architecture. The
encoder ingests items from user history to yield the user embedding, while decoder computes
recommendation scores based on similarity between user embeddings and item embeddings."
MODEL OVERVIEW,0.08605341246290801,Generative Process. The generative process of ZESREC (in the source domain) is as follows:
MODEL OVERVIEW,0.08902077151335312,1. For each item j:
MODEL OVERVIEW,0.09198813056379822,"‚Ä¢ Compute the item universal embedding: mj = fe(xj).
‚Ä¢ Draw a latent item offset vector œµj ‚àºN
 
0, Œª‚àí1
v ID

.
‚Ä¢ Obtain the item latent vector: vj = œµj + mj.
2. For each user i:"
MODEL OVERVIEW,0.09495548961424333,‚Ä¢ For each time step t:
MODEL OVERVIEW,0.09792284866468842,"‚Äì Compute the user universal embedding: nit = fseq([viœÑ ]t‚àí1
œÑ=1).
‚Äì Draw a latent user offset vector Œæit ‚àºN
 
0, Œª‚àí1
u ID

.
‚Äì Obtain the latent user vector: uit = Œæit + nit.
‚Äì Compute recommendation score Sitj for each user-interaction-item tuple (i, k, j),
Sitj = fsoftmax(u‚ä§
itvj) and draw the t-th item for user i: Rit‚àó‚àºCat([Sitj]J
j=1). ùúÜ!
ùíó"" ùíô"" ùëπ!""#"
MODEL OVERVIEW,0.10089020771513353,"ùíñ#$
ùúÜ% ùíó#$"
MODEL OVERVIEW,0.10385756676557864,"j ‚àà[1, J]"
MODEL OVERVIEW,0.10682492581602374,"i ‚àà[1, I]
t ‚àà[1, ùëÅ!]"
MODEL OVERVIEW,0.10979228486646884,User UEN ùíô#$ ùúÜ!
MODEL OVERVIEW,0.11275964391691394,"Item UEN
ùúè‚àà[0, t-1]"
MODEL OVERVIEW,0.11572700296735905,"Figure 1: Graphical model for ZESREC. The item
side (left) and the user side (right) share the same
Œªv and v‚Äôs. The plates indicate replication."
MODEL OVERVIEW,0.11869436201780416,"Here
fsoftmax(¬∑)
is
the
softmax
function:
fsoftmax(u‚ä§
itvj) = exp(u‚ä§
itvj)/P"
MODEL OVERVIEW,0.12166172106824925,"j exp(u‚ä§
itvj).
Cat(¬∑) is a categorical distribution.
fe(¬∑) is
item UEN (see Sec. 3.3), fseq(¬∑) is user UEN
(see Sec. 3.4). iœÑ in viœÑ indexes the œÑ-th item
that user i interacts with. Œªu and Œªv are hyperpa-
rameters. The latent item offset œµj = vj ‚àímj
provides the Ô¨Ånal latent item vector vj with the
Ô¨Çexibility to slightly deviate from the content-
based item universal embedding mj. Similarly,
the latent user offset Œæit = uit ‚àínit provides
the Ô¨Ånal latent user vector uit with the Ô¨Çexi-
bility to slightly deviate from the user univer-
sal embedding nit. Intuitively, œµj and Œæit pro-
vide domain-speciÔ¨Åc information on top of the
domain-agnostic information from mj and nit. In the target domain we will remove œµj from vj, and
Œæit from uit, which can be seen as an attempt to remove the bias learned from the source domain."
MODEL OVERVIEW,0.12462908011869436,Training. The MAP estimation in the source domain can be decomposed as following:
MODEL OVERVIEW,0.12759643916913946,"P(U(s), V(s)|R(s), X(s), Œª‚àí1
u , Œª‚àí1
v ) ‚àùP(R(s)|U(s), V(s)) ¬∑ P(U(s)|V(s), Œª‚àí1
u ) ¬∑ P(V(s)|X(s), Œª‚àí1
v )."
MODEL OVERVIEW,0.13056379821958458,"Maximizing the posterior probability is equivalent to minimizing the joint Negative Log-Likelihood
(NLL) of U(s) and V(s) given R(s), X(s), Œª‚àí1
u , and Œª‚àí1
v : L = Is
X i=1 Ni
X"
MODEL OVERVIEW,0.13353115727002968,"t=1
‚àílog(fsoftmax(u‚ä§
itvit)) + Œªu 2 Is
X i=1 Ni
X"
MODEL OVERVIEW,0.13649851632047477,"t=1
||uit ‚àífseq({viœÑ }t‚àí1
œÑ=1)||2
2 + Œªv 2 Js
X"
MODEL OVERVIEW,0.1394658753709199,"i=1
||vj ‚àífe(xj)||2
2,
(1)"
MODEL OVERVIEW,0.142433234421365,"where nit = fseq({viœÑ }t‚àí1
œÑ=1) and mj = fe(xj). See the Appendix for a full Bayesian treatment of
ZESREC using inference networks and generative networks [15]."
MODEL OVERVIEW,0.14540059347181009,Under review as a conference paper at ICLR 2022
MODEL OVERVIEW,0.14836795252225518,"Inference and Recommendation in the Target Domain. Once the model is trained using source-
domain data, it can recommend unseen items j ‚ààVt (where Vt ‚à©Vs = ‚àÖ) for any unseen user i ‚ààUt
(where Ut ‚à©Us = ‚àÖ) from the target domain based on the approximate MAP inference below:"
MODEL OVERVIEW,0.1513353115727003,"p(R(t)|X(t)) =
Z
p(R(t)|U(t), V(t), X(t))p(U(t), V(t)|X(t))dU(t)dV(t)"
MODEL OVERVIEW,0.1543026706231454,"‚âà
Z
p(R(t)|U(t), V(t), X(t))Œ¥U(t)
MAP(U(t))Œ¥V(t)
MAP(V(t))dU(t)dV(t),"
MODEL OVERVIEW,0.1572700296735905,"where Œ¥(¬∑) denotes a Dirac delta distribution. U(t)
MAP and V(t)
MAP are the MAP estimate of U(t) and
V(t) given X(t), which we approximate as:"
MODEL OVERVIEW,0.16023738872403562,"(U(t)
MAP, V(t)
MAP) ‚âàargmax
U(t),V(t) p(U(t), V(t)|X(t)) =

fseq(fe(X(t))), fe(X(t))

.
(2)"
MODEL OVERVIEW,0.1632047477744807,"The reason for the approximation is that ZESREC has no access to interactions R(t) in the target
domain, making the cross-entropy loss in the Eqn. 4 disappear. The user and item latent matrices
U(t)
MAP, V(t)
MAP in the target domain enable us to perform zero-shot recommendation by computing
recommendation scores based on inner products and recommend item argmaxj fsoftmax(u‚ä§
itvj)."
MODEL OVERVIEW,0.1661721068249258,"As long as the unseen items‚Äô NL descriptions are available, ZESREC could obtain both the unseen
users‚Äô latent vectors and the unseen items‚Äô latent vectors based on the item universal embedding
network. This is in contrast to previous methods that rely on catogorical domain-speciÔ¨Åc item ID,
which is not transferable as unseen items have completely different ID from items in the training set.
Note that ZESREC is general enough to adapt to other data modalities such as images and videos."
MODEL OVERVIEW,0.16913946587537093,"3.3
ITEM UNIVERSAL EMBEDDING NETWORK: mj = fe(xj)"
MODEL OVERVIEW,0.17210682492581603,"The purpose of the item universal embedding network, denoted as fe(¬∑), is to extract item embeddings
that are universal across domains. The network consists of a pretrained BERT network [8], denoted
as fBERT, followed by a single-layer neural network, denoted by fNN(¬∑). Formally we have"
MODEL OVERVIEW,0.17507418397626112,"mj = fe(xj) = fNN(fBERT(xj)),
(3)"
MODEL OVERVIEW,0.17804154302670624,"where mj is the universal embedding for item j and xj is the NL description for item j. We use the
embedding for the ‚ÄòCLS‚Äô token from the last layer of BERT as the output of fBERT(xi). This UEN is
jointly trained with the sequential model using the objective function in Eqn. 4. Note that fNN(¬∑) is
necessary as we need to adapt the pre-trained BERT for recommendation tasks."
MODEL OVERVIEW,0.18100890207715134,"3.4
USER UNIVERSAL EMBEDDING NETWORK: nit = fseq([viœÑ ]t‚àí1
œÑ=1)"
MODEL OVERVIEW,0.18397626112759644,"The user UEN fseq(¬∑) is built on top of the item UEN in Eqn. 3. SpeciÔ¨Åcally, given user i‚Äôs interaction
sequence until time t, Ri‚àó‚àó= [RiœÑ‚àó]t‚àí1
œÑ=1, we Ô¨Årst replace it the corresponding NL descriptions
l(x)
i
= [xiœÑ ]t‚àí1
œÑ=1, and then fed it into item UEN in Eqn. 3 to obtain l(m)
i
= [miœÑ ]t‚àí1
œÑ=1 = [fe(xiœÑ )]t‚àí1
œÑ=1
where item it is user i‚Äôs t-th interaction. Each vector in the sequence, mj, is the universal embedding
for item j. During training, we obtain item latent vector vj based on mj (see Eqn. 4), while during
inference vj = mj (see Eqn. 2). We can then treat each vj as the input at each time step of the
sequential model, which gives us the Ô¨Ånal user universal embedding nit = fseq([viœÑ ]t‚àí1
œÑ=1). Note that
this user UEN is used during both training (Eqn. 4) and inference (Eqn. 2)."
EXPERIMENTS,0.18694362017804153,"4
EXPERIMENTS"
EXPERIMENTS,0.18991097922848665,"In this section, we evaluate our ZESREC against various in-domain and zero-shot baselines on three
source-target dataset pairs, with the major goals of addressing the following questions:"
EXPERIMENTS,0.19287833827893175,"Q1 How accurate (effective) is ZESREC compared to the baselines?
Q2 If one allows training models using target-domain data, how long does it take for non
zero-shot models to outperform zero-shot recommenders?
Q3 Does ZESREC yield meaningful recommendations for users with similar behavioral patterns
in the source domain and target domain?"
EXPERIMENTS,0.19584569732937684,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.19881305637982197,"Table 1: Zero-shot results on three dataset pairs, ‚ÄòAmazon Grocery and Gourmet Food‚Äô ‚Üí‚ÄòAmazon
Prime Pantry‚Äô, ‚ÄòOthers‚Äô ‚Üí‚ÄòNFL‚Äô, and ‚ÄòOthers‚Äô ‚Üí‚ÄòNCAA‚Äô. Methods such as HRNN, HRNN-Meta,
and POP are oracle methods that are trained directly using target-domain data. N@20 and R@20
represent NDCG@20 and Recall20, respectively. The top 3 zero-shot results are shown in bold."
EXPERIMENTS,0.20178041543026706,"Method
PRIME PANTRY
NCAA 1ST DAY
NCAA 1 WEEK
NFL 1ST DAY
NFL 1 WEEK
N@20
R@20
N@20
R@20
N@20
R@20
N@20
R@20
N@20
R@20"
EXPERIMENTS,0.20474777448071216,"HRNN (ORACLE)
0.038
0.073
0.066
0.139
0.006
0.011
0.052
0.118
0.002
0.003
HRNN-META (ORACLE)
0.045
0.089
0.054
0.120
0.004
0.010
0.044
0.112
0.001
0.003
GRU4REC (ORACLE)
0.042
0.081
0.062
0.135
0.005
0.011
0.046
0.109
0.001
0.003
GRU4REC-META (ORACLE)
0.044
0.088
0.053
0.118
0.004
0.009
0.037
0.094
0.001
0.003
TCN (ORACLE)
0.038
0.073
0.068
0.141
0.006
0.012
0.049
0.114
0.001
0.003
TCN-META (ORACLE)
0.045
0.088
0.054
0.120
0.004
0.010
0.044
0.109
0.001
0.003
POP (ORACLE)
0.007
0.018
0.002
0.005
0.000
0.000
0.000
0.000
0.000
0.000"
EXPERIMENTS,0.20771513353115728,"EMB-KNN (BASELINE)
0.024
0.042
0.016
0.026
0.005
0.011
0.010
0.018
0.001
0.003
RANDOM (BASELINE)
0.001
0.002
0.002
0.006
0.002
0.006
0.001
0.002
0.001
0.002
ZESREC-H (OURS)
0.027
0.052
0.027
0.063
0.011
0.036
0.007
0.013
0.015
0.043
ZESREC-G (OURS)
0.026
0.050
0.030
0.070
0.014
0.040
0.008
0.013
0.018
0.058
ZESREC-T (OURS)
0.026
0.050
0.023
0.056
0.011
0.035
0.009
0.017
0.018
0.054"
DATASETS,0.21068249258160238,"4.1
DATASETS"
DATASETS,0.21364985163204747,"We use three different real-world dataset pairs, one from Amazon [26] and two from MIND [37]:"
DATASETS,0.2166172106824926,"‚Ä¢ Amazon [26]: A publicly available dataset collection which contains a group of datasets in
different categories with abundant item metadata such as item description, product images,
etc. In our experiments, we consider two datasets: (1) ‚ÄòPrime Pantry‚Äô, which contains
300K interactions, 10K items, and 76K users, and (2) ‚ÄòGrocery and Gourmet Food‚Äô, which
contains 2.3M interactions, 213K items, and 739K users.
‚Ä¢ MIND [37]: A large-scale news recommendation dataset collected from the user click
logs of Microsoft News. It includes 4-week user history and 5th-week interactions. In our
experiments, we simulate zero-shot learning by leave-one-out splitting on subcategories
under the largest category in MIND: ‚Äòsports‚Äô. We consider two pairs: (1) ‚ÄòOthers‚Äô to ‚ÄôNFL‚Äô,
where ‚ÄòOthers‚Äô contains all the other subcategories except ‚ÄòNFL‚Äô. ‚ÄòOthers‚Äô contains 169K
interactions, 8K items, and 57K users, while ‚ÄòNFL‚Äô contains 1M interactions, 11K items,
and 203K users. (2) ‚ÄòOthers‚Äô to ‚ÄòNCAA‚Äô, where ‚ÄòOthers‚Äô includes all the subcategories
except ‚ÄòNCAA‚Äô. ‚ÄòOthers‚Äô contains 797K interacions, 17K items, and 206K users, while
‚ÄòNCAA‚Äô contains 238K interactions, 31K items, and 56K users."
DATASETS,0.2195845697329377,"We adopted a rigorous experimental setup for zero-shot learning to ensure (1) no overlapping users
and items and (2) no temporal leakage. See the Appendix for details. Datasets in Amazon pair are
divided into training (80%) and test (20%) sets. For datasets in the two MIND pairs, we follow the
ofÔ¨Åcial splitting [37] and use the four-week user history as the training set and interactions in the
Ô¨Åfth week as the test set. Due to strong recency bias in the news recommendation [37], for MIND
we run experiments under two settings to measure temporal invariance of the model: (1) evaluate
on interactions in the Ô¨Årst day of the week (1st-day setting), and (2) evaluate on interactions of the
whole week, i.e., the full test set (whole-week setting). For all datasets, We further split 10% of the
training set by user as validation set."
BASELINES AND ZESREC VARIANTS,0.22255192878338279,"4.2
BASELINES AND ZESREC VARIANTS"
BASELINES AND ZESREC VARIANTS,0.22551928783382788,"To demonstrate the effectiveness of our model, we compare ZESREC against two groups of baselines:
in-domain methods and zero-shot methods."
BASELINES AND ZESREC VARIANTS,0.228486646884273,"In-Domain Methods. We compare variants of our model ZESREC against a variety of state-of-the-
art session-based recommendation models including GRU4Rec [12], TCN [2], and HRNN [25].
We also consider their extensions, HRNN-Meta, GRU4Rec-Meta, and TCN-Meta, which use
items‚Äô NL description embeddings to replace item ID hidden embeddings. Besides aforementioned
sophisticated models, we introduce POP which is a simple baseline conducting recommendation
only based on item global popularity. It can be a strong baseline in certain domains. All the above 7
methods are trained directly on target-domain data and therefore are considered ‚Äòoracle‚Äô methods."
BASELINES AND ZESREC VARIANTS,0.2314540059347181,Under review as a conference paper at ICLR 2022
BASELINES AND ZESREC VARIANTS,0.2344213649851632,"0
2.5
5
10
Number of interactions in thousands (K) 0.000 0.006 0.012 0.018 0.024 0.030"
BASELINES AND ZESREC VARIANTS,0.23738872403560832,NDCG@20
BASELINES AND ZESREC VARIANTS,0.2403560830860534,"HRNN
HRNN-Meta
POP
TCN
TCN-Meta
ZESRec-H
ZESRec-T"
BASELINES AND ZESREC VARIANTS,0.2433234421364985,(a) NDCG@20
BASELINES AND ZESREC VARIANTS,0.24629080118694363,"0
2.5
5
10
Number of interactions in thousands (K) 0.000 0.020 0.040 0.060 0.080"
BASELINES AND ZESREC VARIANTS,0.24925816023738873,Recall@20
BASELINES AND ZESREC VARIANTS,0.2522255192878338,"HRNN
HRNN-Meta
POP
TCN
TCN-Meta
ZESRec-H
ZESRec-T"
BASELINES AND ZESREC VARIANTS,0.2551928783382789,(b) Recall@20
BASELINES AND ZESREC VARIANTS,0.258160237388724,"0
2.5
5
10
Number of interactions in thousands (K) 0.000 0.006 0.012 0.018 0.024 0.030"
BASELINES AND ZESREC VARIANTS,0.26112759643916916,NDCG@20
BASELINES AND ZESREC VARIANTS,0.26409495548961426,"HRNN
HRNN-Meta
POP
TCN
TCN-Meta
ZESRec-H
ZESRec-T"
BASELINES AND ZESREC VARIANTS,0.26706231454005935,(c) NDCG@20
BASELINES AND ZESREC VARIANTS,0.27002967359050445,"0
2.5
5
10
Number of interactions in thousands (K) 0.000 0.020 0.040 0.060"
BASELINES AND ZESREC VARIANTS,0.27299703264094954,Recall@20
BASELINES AND ZESREC VARIANTS,0.27596439169139464,"HRNN
HRNN-Meta
POP
TCN
TCN-Meta
ZESRec-H
ZESRec-T"
BASELINES AND ZESREC VARIANTS,0.2789317507418398,"(d) Recall@20.
Figure 2: Incremental training results for baselines using target domain data compared to ZESREC
using no training data on MIND-NCAA (left two) and Amazon Prime Pantry (right two). To prevent
clutter, we only show results for TCN-based and HRNN-based models, since HRNN is an advanced
version of GRU4Rec. Results show that even without using target-domain data, ZESREC can still
outperform models trained directly using target-domain data for substantial amount of time."
BASELINES AND ZESREC VARIANTS,0.2818991097922849,"Zero-Shot Methods. Since no previous work has been done on this thread, we consider two intuitive
zero-shot models (1) EmbeddingKNN: a K-nearest-neighbors algorithm based on inner product
between the user embedding and item embedding. The item embedding is the BERT embedding from
NL description, while the user embedding is an average over embeddings of the user‚Äôs interacted
items, and (2) Random: recommending items by random selection from the whole item catalogue
without replacement."
BASELINES AND ZESREC VARIANTS,0.28486646884273,"ZESREC Variants. We evaluate three variants of our ZESREC, including ZESREC-G, ZESREC-
T, and ZESREC-H which use GRU4Rec, TCN and HRNN as base models, respectively."
IMPLEMENTATION DETAILS,0.2878338278931751,"4.3
IMPLEMENTATION DETAILS"
IMPLEMENTATION DETAILS,0.29080118694362017,"We adopted pre-trained google/bert_uncased_L-12_H-768_A-12 BERT model from Huggingface [36]
to process item description and generate item embedding. The dimension of BERT embedding is 768.
For the model architecture, we use BERT embedding as input to a single-layer neural netwrok (i.e.,
fNN(¬∑)). The output dimension for the NN is set to D (see the Appendix for more details)."
EXPERIMENT SETUP AND EVALUATION METRICS,0.29376854599406527,"4.4
EXPERIMENT SETUP AND EVALUATION METRICS"
EXPERIMENT SETUP AND EVALUATION METRICS,0.29673590504451036,We conducted three sets of experiments to answer questions proposed earlier in this section.
EXPERIMENT SETUP AND EVALUATION METRICS,0.2997032640949555,"Zero-Shot Experiments. We trained in-domain baselines on target domain training set, while our
ZESREC is trained on source domain. All models are tested on the testing set of the target domain
for an apples to apples comparison."
EXPERIMENT SETUP AND EVALUATION METRICS,0.3026706231454006,"Incremental Training Experiments. To measure how long it takes for non-zero-shot models to
outperform zero-shot recommenders, we conducted incremental training experiments on in-domain
base models GRU4Rec, TCN, HRNN as well as GRU4Rec-Meta, TCN-Meta, HRNN-Meta. Note
that the variants of our ZESREC are NOT retrained on target domain. It is also inevitable that
non-zero-shot models eventually outperform ZESREC because ZESREC does not have access to
target-domain data. Due to space constraint, we only reported results for incremental training
experiments on Amazon pair and MIND NCAA pair under the 1st-day setting. Importantly, under
the whole-week setting for the MIND dataset, the ZESRec variants already signiÔ¨Åcantly outperform
in-domain competitors which are trained on the full target training set; therefore it is meaningless
to compare incremental training results under this setting, as in-domain competitors will never beat
ZESRec variants. For all the source-target dataset pairs, we group the interactions by user and sort
interactions within each user based on interaction timestamps. We randomly select users until we get
enough interactions and build three datasets containing 2.5K, 5K, and 10K interactions, respectively."
EXPERIMENT SETUP AND EVALUATION METRICS,0.3056379821958457,"Case Studies. To gain more insight what ZESREC learns, we perform several case studies. SpeciÔ¨Å-
cally, we randomly select users from the test set of the target domain Amazon Prime Pantry (where
we evaluate ZESREC) and only keep users for whom ZESREC correctly predicts the 6-th items in
the sequence given the Ô¨Årst 5 items as context, as we want to focus on sequences where ZESREC
works. We use these users as queries to Ô¨Ånd users with similar behavioral patterns from the source
domain Amazon Grocery and Gourmet Food (where we train ZESREC) based on user embeddings
from ZESREC. User embeddings are generated based on the Ô¨Årst 5 items of the sequence."
EXPERIMENT SETUP AND EVALUATION METRICS,0.3086053412462908,"Simulated Online Scenarios. In the experiments, ZESRec only accesses target domain data during
inference to simulate online scenarios, where new businesses just open and the customers are using"
EXPERIMENT SETUP AND EVALUATION METRICS,0.3115727002967359,Under review as a conference paper at ICLR 2022
EXPERIMENT SETUP AND EVALUATION METRICS,0.314540059347181,Energy Bars
EXPERIMENT SETUP AND EVALUATION METRICS,0.31750741839762614,"Flavor
Chocolate Chip Peanut Crunch"
EXPERIMENT SETUP AND EVALUATION METRICS,0.32047477744807124,"Ingredients Organic Brown Rice Syrup,"
EXPERIMENT SETUP AND EVALUATION METRICS,0.32344213649851633,"Organic Rolled Oats, ‚Ä¶"
EXPERIMENT SETUP AND EVALUATION METRICS,0.3264094955489614,"Brand
Clif Bar"
EXPERIMENT SETUP AND EVALUATION METRICS,0.3293768545994065,Bounce Fabric Softener Sheets
EXPERIMENT SETUP AND EVALUATION METRICS,0.3323442136498516,‚Ä¢ Classic Bounce outdoor fresh scent
EXPERIMENT SETUP AND EVALUATION METRICS,0.3353115727002967,‚Ä¢ Helps Reduce Wrinkles
EXPERIMENT SETUP AND EVALUATION METRICS,0.33827893175074186,‚Ä¢ Controls static cling in fabrics
EXPERIMENT SETUP AND EVALUATION METRICS,0.34124629080118696,‚Ä¢ Helps repel lint and hair
EXPERIMENT SETUP AND EVALUATION METRICS,0.34421364985163205,‚Ä¢ Softens fabrics
EXPERIMENT SETUP AND EVALUATION METRICS,0.34718100890207715,"Black Tea, Morning Thunder"
EXPERIMENT SETUP AND EVALUATION METRICS,0.35014836795252224,"Item Form 
Teabags"
EXPERIMENT SETUP AND EVALUATION METRICS,0.35311572700296734,"Brand 
Celestial Seasonings"
EXPERIMENT SETUP AND EVALUATION METRICS,0.3560830860534125,"Tea Variety 
Black"
EXPERIMENT SETUP AND EVALUATION METRICS,0.3590504451038576,Lemon Zinger Herbal Tea
EXPERIMENT SETUP AND EVALUATION METRICS,0.3620178041543027,‚Ä¢ One 20-count box of Lemon Zinger Herbal tea bags
EXPERIMENT SETUP AND EVALUATION METRICS,0.3649851632047478,‚Ä¢ Blended with hibiscus for tart flavor
EXPERIMENT SETUP AND EVALUATION METRICS,0.36795252225519287,‚Ä¢ Caffeine and gluten-free
EXPERIMENT SETUP AND EVALUATION METRICS,0.37091988130563797,"‚Ä¢ No artificial flavors, colors or artificial preservatives"
EXPERIMENT SETUP AND EVALUATION METRICS,0.37388724035608306,‚Ä¢ Steep in hot water for 4-6 minutes for the perfect cup
EXPERIMENT SETUP AND EVALUATION METRICS,0.3768545994065282,"ZESRec
Recommends"
EXPERIMENT SETUP AND EVALUATION METRICS,0.3798219584569733,Target Domain: Amazon Prime Pantry
EXPERIMENT SETUP AND EVALUATION METRICS,0.3827893175074184,Source Domain: Amazon Grocery and Gourmet Food
EXPERIMENT SETUP AND EVALUATION METRICS,0.3857566765578635,"Nestle Kit Kat Dark 4 Finger England
Traditional Medicinals Organic"
EXPERIMENT SETUP AND EVALUATION METRICS,0.3887240356083086,Throat Coat Seasonal Tea
EXPERIMENT SETUP AND EVALUATION METRICS,0.3916913946587537,"Item Form 
Bags"
EXPERIMENT SETUP AND EVALUATION METRICS,0.39465875370919884,"Brand 
Traditional Medicinals"
EXPERIMENT SETUP AND EVALUATION METRICS,0.39762611275964393,"Flavor
Throat Coat"
EXPERIMENT SETUP AND EVALUATION METRICS,0.40059347181008903,"Caffeine
Caffeine Free"
EXPERIMENT SETUP AND EVALUATION METRICS,0.4035608308605341,Celestial Seasonings Herb Tea
EXPERIMENT SETUP AND EVALUATION METRICS,0.4065281899109792,Tension Tamer
EXPERIMENT SETUP AND EVALUATION METRICS,0.4094955489614243,"Item Form 
Teabags"
EXPERIMENT SETUP AND EVALUATION METRICS,0.4124629080118694,"Brand
Celestial Seasonings"
EXPERIMENT SETUP AND EVALUATION METRICS,0.41543026706231456,"Flavor
Tension Tamer"
EXPERIMENT SETUP AND EVALUATION METRICS,0.41839762611275966,"Caffeine
Caffeine Free"
EXPERIMENT SETUP AND EVALUATION METRICS,0.42136498516320475,"‚Ä¢ Original, Imported from England"
EXPERIMENT SETUP AND EVALUATION METRICS,0.42433234421364985,‚Ä¢ Taste the difference of British Candy & Chocola
EXPERIMENT SETUP AND EVALUATION METRICS,0.42729970326409494,‚Ä¢ Customer takes responsibility for melted chocolate
EXPERIMENT SETUP AND EVALUATION METRICS,0.43026706231454004,(happens mostly during summer months)
EXPERIMENT SETUP AND EVALUATION METRICS,0.4332344213649852,‚Ä¢ 12 pack
EXPERIMENT SETUP AND EVALUATION METRICS,0.4362017804154303,Celestial Seasonings Tension Tamer Tea
EXPERIMENT SETUP AND EVALUATION METRICS,0.4391691394658754,"Item Form 
Bags"
EXPERIMENT SETUP AND EVALUATION METRICS,0.4421364985163205,"Brand
Celestial Seasonings"
EXPERIMENT SETUP AND EVALUATION METRICS,0.44510385756676557,"Flavor
Tension Tamer"
EXPERIMENT SETUP AND EVALUATION METRICS,0.44807121661721067,"Caffeine
Caffeine Free"
EXPERIMENT SETUP AND EVALUATION METRICS,0.45103857566765576,"Figure 3: Case Study 1. The purchase history of a user in the source domain (top) and the purchase
history of an unseen user in the target domain, where all items are unseen during training (bottom).
We select two users with similar universal embeddings according to Sec. 4.4. This case study
demonstrates ZESREC can learn the user behavioral pattern that ‚Äòusers who bought sugary snacks
and tea tend to buy caffeine-free herbal tea later‚Äô.
service in real-time. This real-time online access setting is substantially different from batch access
ahead of time as it prevents us from training a recommender in the target domain before serving."
EXPERIMENT SETUP AND EVALUATION METRICS,0.4540059347181009,"Evaluation Protocol. For evaluation, we adopted Recall (R@20) and the ranking metric Normalized
Discounted Cumulative Gain (NDCG) [30] (N@20). We removed all the repetitive interactions (e.g.,
user A clicked item B two times in a row) to only focus on evaluating the model‚Äôs capability of
capturing the transition between user history to the next item."
EXPERIMENTAL RESULTS,0.456973293768546,"4.5
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.4599406528189911,"Zero-Shot Experiments. The experimental results on three dataset pairs are in Table 1. Overall,
our ZESREC outperforms zero-shot baselines Embedding-KNN and Random by a large margin in
most cases; it can also achieve performance comparable to in-domain baselines. On the Amazon
pair, ZESREC beats POP and achieves comparable performance with HRNN-Meta and HRNN-
Interactions, suggesting the existence of shared recommendation patterns in two domains. For the
two MIND pairs, we made the following observations: (1) Under the whole-week setting, ZESREC
consistently outperforms HRNN-Meta and HRNN-Interactions by a large margin. Comparing the
Ô¨Årst-day results and the whole-week results, it is obvious that the in-domain models overÔ¨Åt the history
and fail to generalize well on latest user-item interactions. This reÔ¨Çects strong recency bias in news
recommendation. In contrast, our zero-shot learning naturally comes with an inductive bias to only
model the transition from user history to the next item. (2) Under the Ô¨Årst-day setting, ZESREC still
achieves reasonable performance comparing with in-domain models on both pairs. (3) Surprisingly
in the MIND NFL pair, the ZESRec variants perform well under the whole-week setting even when
the source domain contains much fewer interactions than the target domain (169K VS 1M)."
EXPERIMENTAL RESULTS,0.4629080118694362,"Incremental Training Experiments. Incremental training results are in Fig. 2. Overall, almost all
the in-domain baselines are not able to outperform ZESREC by retraining on at most 10K interactions
in target domain, which shows the vast importance of conducting zero-shot learning in recommender
domain. For new business operating an early-stage RecSys, it‚Äôs hard to train a good RecSys with
limited interactions. This is a chicken-and-egg problem, as training good RecSys requires sufÔ¨Åcient
interactions, while in turn, collecting sufÔ¨Åcient interactions requires a satisfactory RecSys to attract
users. Therefore the Ô¨Årst 10K interactions are crucial to get the RecSys started."
CASE STUDIES,0.4658753709198813,"4.6
CASE STUDIES"
CASE STUDIES,0.4688427299703264,"To gain more insight, we randomly select pairs of user purchase histories from the source and target
domains in the Amazon pair as case studies according to the procedure in Sec. 4.4. The goal is"
CASE STUDIES,0.47181008902077154,Under review as a conference paper at ICLR 2022
CASE STUDIES,0.47477744807121663,"Vita Coco Coconut Water, Tangerine
Vita Coco Coconut Water, Pineapple"
CASE STUDIES,0.47774480712166173,Target Domain: Amazon Prime Pantry
CASE STUDIES,0.4807121661721068,Source Domain: Amazon Grocery and Gourmet Food
CASE STUDIES,0.4836795252225519,Nestle Carnation Vitamin D
CASE STUDIES,0.486646884272997,Added Evaporated Milk
CASE STUDIES,0.4896142433234421,"Mug Root Beer Cans 
(12 Count, 12 Fl Oz Each)"
CASE STUDIES,0.49258160237388726,"V8 Splash Tropical Blend,"
CASE STUDIES,0.49554896142433236,64 oz. Bottle
CASE STUDIES,0.49851632047477745,"V8 Splash Berry Blend,"
CASE STUDIES,0.5014836795252225,64 oz. Bottle
CASE STUDIES,0.5044510385756676,"ZESRec
Recommends"
CASE STUDIES,0.5074183976261127,"Vita Coco Coconut Water, Lemonade
Vita Coco Coconut Water with"
CASE STUDIES,0.5103857566765578,Acai & Pomegranate
CASE STUDIES,0.5133531157270029,"‚Ä¢ Antioxidant vitamins A & C 
‚Ä¢ 70 calories per serving 
‚Ä¢ Gluten Free 
‚Ä¢ Delicious fruit flavors the whole family can enjoy 
‚Ä¢ Contains a blend of 2 juices from concentrate 
with other natural flavors"
CASE STUDIES,0.516320474777448,"‚Ä¢ Antioxidant C & B vitamins 
‚Ä¢ 70 calories per serving 
‚Ä¢ Gluten Free 
‚Ä¢ No artificial flavors 
‚Ä¢ Delicious fruit flavors the 
whole family can enjoy"
CASE STUDIES,0.5192878338278932,"Flavor 
Root Beer"
CASE STUDIES,0.5222551928783383,"Ingredients 
Carbonated Water‚Ä¶"
CASE STUDIES,0.5252225519287834,"Brand 
Mug"
CASE STUDIES,0.5281899109792285,"Serving Description 
1 cans ( )"
CASE STUDIES,0.5311572700296736,"Item Volume 
144 Fluid Ounces"
CASE STUDIES,0.5341246290801187,"Ingredients 
Milk‚Ä¶"
CASE STUDIES,0.5370919881305638,"Brand 
Carnation"
CASE STUDIES,0.5400593471810089,"Weight
15.2 Ounces"
CASE STUDIES,0.543026706231454,"Serving Description 
2 Tbsp. (30mL)"
CASE STUDIES,0.5459940652818991,"Protein
2 Grams"
CASE STUDIES,0.5489614243323442,"‚Ä¢ Ingredients: Purified Water 
‚Ä¢ Safety warning: This product is labelled to 
United States standards and may differ from 
similar products sold elsewhere in its 
ingredients, labeling and allergen warnings"
CASE STUDIES,0.5519287833827893,"‚Ä¢ Pack of 12, 17- ounce (totally 204 ounces) 
‚Ä¢ Convenient re-sealable packs
‚Ä¢ A tamper-evident screw cap.
‚Ä¢ Environment friendly tetra packs
‚Ä¢ Serve chilled or mixed in a cocktail"
CASE STUDIES,0.5548961424332344,"‚Ä¢ Pack of 12, 17- ounce (totally 204 ounces) 
‚Ä¢ Convenient re-sealable packs
‚Ä¢ A tamper-evident screw cap.
‚Ä¢ Environment friendly tetra packs
‚Ä¢ Serve chilled or mixed in a cocktail"
CASE STUDIES,0.5578635014836796,"‚Ä¢ Taste of the tropics
‚Ä¢ Natural hydration
‚Ä¢ Refresh yourself
‚Ä¢ Fat-free, Gluten-free, and non-GMO
‚Ä¢ One 12-pack case of 16.9 ounce bottles"
CASE STUDIES,0.5608308605341247,"Figure 4: Case Study 2. The purchase history of a user in the source domain (top) and the purchase
history of an unseen user in the target domain, where all items are unseen during training (bottom).
We select two users with similar universal embeddings according to Sec. 4.4. This case study
demonstrates ZESREC can learn the user behavioral pattern that ‚Äòif users bought snacks or drinks
that they like, they may later purchase similar snacks or drinks with different Ô¨Çavors‚Äô."
CASE STUDIES,0.5637982195845698,"to demonstrate our ZSR could learn relevant dynamics of users‚Äô purchase history from the source
domain and successfully recommend unseen products to an unseen user in the target domain."
CASE STUDIES,0.5667655786350149,"Fig. 3 shows the purchase history of a user in the source domain (top), ‚ÄòAmazon Grocery and Gourmet
Food‚Äô, and the purchase history of an unseen user in the target domain (bottom), ‚ÄòAmazon Prime
Pantry‚Äô, where all items are unseen during training. The user in the source domain bought ‚ÄòTension
Tamer Tea‚Äô, which is a type of herbal tea, after buying some sugary snacks (KitKat) and other tea.
Such a pattern is captured by ZESREC, which then recommended ‚ÄòLemon Zinger Herbal Tea‚Äô to
an unseen user after she bought some sugary snacks (‚ÄòEnergy Bars from Clif Bar‚Äô) and some black
tea. This case study demonstrates ZESREC can learn the user behavioral pattern that ‚Äòusers who
bought sugary snacks and tea tend to buy caffeine-free herbal tea later‚Äô. More interestingly, another
case study in Fig. 4 demonstrates that ZESREC can learn the user behavioral pattern that ‚Äòif users
bought snacks or drinks that they like, they may later purchase similar snacks or drinks with different
Ô¨Çavors‚Äô. SpeciÔ¨Åcally, in the source domain, the user purchased ‚ÄòVita Coconut Water‚Äô with four
different Ô¨Çavors; such a pattern is captured by ZESREC. Later in the target domain, an unseen user
purchase ‚ÄòV8 Splash‚Äô with a tropical Ô¨Çavor, ZESREC then successfully recommends ‚ÄòV8 Splash‚Äô
with a berry Ô¨Çavor to the user."
CONCLUSION,0.56973293768546,"5
CONCLUSION"
CONCLUSION,0.5727002967359051,"In this paper, we identify the problem of zero-shot recommender systems where a model is trained
in the source domain and deployed in a target domain where all the users and items are unseen
during training. We propose ZESREC as the Ô¨Årst general framework for addressing this problem.
We introduce the notion of universal continuous identiÔ¨Åers leveraging the fact that item ID can be
grounded in natural-language descriptions. We provide empirical results, both quantitatively and
qualitatively, to demonstrate the effectiveness of our proposed ZESREC and verify that ZESREC
can successfully learn user behavioral patterns that generalize across datasets (domains). The main
limitation of this work is that commonality between domains is crucial for ZESRec, as is the case
for most other works in transfer learning. Future work includes quantiÔ¨Åcation of such commonality
and exploring other modalities, e.g., images and videos, as alternative universal identiÔ¨Åers. It would
also be interesting to investigate the interpretability provided by the pretrained BERT model and to
incorporate additional auxiliary information to further improve the zero-shot performance."
CONCLUSION,0.5756676557863502,Under review as a conference paper at ICLR 2022
REFERENCES,0.5786350148367952,REFERENCES
REFERENCES,0.5816023738872403,"[1] ABADI, M., CHU, A., GOODFELLOW, I., MCMAHAN, H. B., MIRONOV, I., TALWAR, K.,
AND ZHANG, L. Deep learning with differential privacy. In Proceedings of the 2016 ACM
SIGSAC conference on computer and communications security (2016), pp. 308‚Äì318."
REFERENCES,0.5845697329376854,"[2] BAI, S., KOLTER, J. Z., AND KOLTUN, V. An empirical evaluation of generic convolutional
and recurrent networks for sequence modeling. CoRR abs/1803.01271 (2018)."
REFERENCES,0.5875370919881305,"[3] BI, Y., SONG, L., YAO, M., WU, Z., WANG, J., AND XIAO, J. A heterogeneous information
network based cross domain insurance recommendation system for cold start users. In SIGIR
(2020), pp. 2211‚Äì2220."
REFERENCES,0.5905044510385756,"[4] CHEN, M., BEUTEL, A., COVINGTON, P., JAIN, S., BELLETTI, F., AND CHI, E. H. Top-k
off-policy correction for a REINFORCE recommender system. In WSDM (2019), pp. 456‚Äì464."
REFERENCES,0.5934718100890207,"[5] CHEN, M. X., LEE, B. N., BANSAL, G., CAO, Y., ZHANG, S., LU, J., TSAY, J., WANG,
Y., DAI, A. M., CHEN, Z., ET AL. Gmail smart compose: Real-time assisted writing. In
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining (2019), pp. 2287‚Äì2295."
REFERENCES,0.5964391691394659,"[6] CHO, K., VAN MERRIENBOER, B., GULCEHRE, C., BAHDANAU, D., BOUGARES, F.,
SCHWENK, H., AND BENGIO, Y. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP (2014), pp. 1724‚Äì1734."
REFERENCES,0.599406528189911,"[7] D√âFOSSEZ, A., BOTTOU, L., BACH, F., AND USUNIER, N. On the convergence of adam and
adagrad. arXiv preprint arXiv:2003.02395 (2020)."
REFERENCES,0.6023738872403561,"[8] DEVLIN, J., CHANG, M.-W., LEE, K., AND TOUTANOVA, K. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)."
REFERENCES,0.6053412462908012,"[9] DONG, M., YUAN, F., YAO, L., XU, X., AND ZHU, L.
MAMO: memory-augmented
meta-optimization for cold-start recommendation. In KDD (2020), pp. 688‚Äì697."
REFERENCES,0.6083086053412463,"[10] FANG, H., ZHANG, D., SHU, Y., AND GUO, G. Deep learning for sequential recommendation:
Algorithms, inÔ¨Çuential factors, and evaluations. arXiv preprint arXiv:1905.01997 (2019)."
REFERENCES,0.6112759643916914,"[11] HANSEN, C., HANSEN, C., SIMONSEN, J. G., ALSTRUP, S., AND LIOMA, C. Content-aware
neural hashing for cold-start recommendation. In SIGIR (2020), pp. 971‚Äì980."
REFERENCES,0.6142433234421365,"[12] HIDASI, B., KARATZOGLOU, A., BALTRUNAS, L., AND TIKK, D. Session-based recommen-
dations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015)."
REFERENCES,0.6172106824925816,"[13] HU, Y., KOREN, Y., AND VOLINSKY, C. Collaborative Ô¨Åltering for implicit feedback datasets.
In ICDM (2008), pp. 263‚Äì272."
REFERENCES,0.6201780415430267,"[14] KANG, W.-C., AND MCAULEY, J. Self-attentive sequential recommendation. In 2018 IEEE
International Conference on Data Mining (ICDM) (2018), IEEE, pp. 197‚Äì206."
REFERENCES,0.6231454005934718,"[15] KINGMA, D. P., AND WELLING, M. Auto-encoding variational bayes. In ICLR (2014)."
REFERENCES,0.6261127596439169,"[16] LI, J., JING, M., LU, K., ZHU, L., YANG, Y., AND HUANG, Z. From zero-shot learning to
cold-start recommendation. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence
(2019), vol. 33, pp. 4189‚Äì4196."
REFERENCES,0.629080118694362,"[17] LI, J., REN, P., CHEN, Z., REN, Z., LIAN, T., AND MA, J. Neural attentive session-
based recommendation. In Proceedings of the 2017 ACM on Conference on Information and
Knowledge Management (2017), pp. 1419‚Äì1428."
REFERENCES,0.6320474777448071,"[18] LI, J., REN, P., CHEN, Z., REN, Z., LIAN, T., AND MA, J. Neural attentive session-based
recommendation. In CIKM (2017), pp. 1419‚Äì1428."
REFERENCES,0.6350148367952523,"[19] LI, X., AND SHE, J. Collaborative variational autoencoder for recommender systems. In KDD
(2017), pp. 305‚Äì314."
REFERENCES,0.6379821958456974,Under review as a conference paper at ICLR 2022
REFERENCES,0.6409495548961425,"[20] LIANG, T., XIA, C., YIN, Y., AND YU, P. S. Joint training capsule network for cold start
recommendation. In SIGIR (2020), pp. 1769‚Äì1772."
REFERENCES,0.6439169139465876,"[21] LIU, Q., ZENG, Y., MOKHOSI, R., AND ZHANG, H. Stamp: short-term attention/memory
priority model for session-based recommendation. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining (2018), pp. 1831‚Äì1839."
REFERENCES,0.6468842729970327,"[22] LIU, Q., ZENG, Y., MOKHOSI, R., AND ZHANG, H. Stamp: short-term attention/memory
priority model for session-based recommendation. In KDD (2018), pp. 1831‚Äì1839."
REFERENCES,0.6498516320474778,"[23] LIU, S., OUNIS, I., MACDONALD, C., AND MENG, Z. A heterogeneous graph neural model
for cold-start recommendation. In Proceedings of the 43rd International ACM SIGIR Conference
on Research and Development in Information Retrieval (2020), pp. 2029‚Äì2032."
REFERENCES,0.6528189910979229,"[24] LU, Y., FANG, Y., AND SHI, C. Meta-learning on heterogeneous information networks for
cold-start recommendation. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining (2020), pp. 1563‚Äì1573."
REFERENCES,0.655786350148368,"[25] MA, Y., NARAYANASWAMY, B., LIN, H., AND DING, H. Temporal-contextual recommen-
dation in real-time. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining (2020), pp. 2291‚Äì2299."
REFERENCES,0.658753709198813,"[26] MCAULEY, J., TARGETT, C., SHI, Q., AND VAN DEN HENGEL, A. Image-based recom-
mendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR
conference on research and development in information retrieval (2015), pp. 43‚Äì52."
REFERENCES,0.6617210682492581,"[27] QUADRANA, M., KARATZOGLOU, A., HIDASI, B., AND CREMONESI, P. Personalizing
session-based recommendations with hierarchical recurrent neural networks. In RecSys (2017),
pp. 130‚Äì137."
REFERENCES,0.6646884272997032,"[28] SALAKHUTDINOV, R., AND MNIH, A. Probabilistic matrix factorization. In NIPS (2007),
pp. 1257‚Äì1264."
REFERENCES,0.6676557863501483,"[29] SALAKHUTDINOV, R., MNIH, A., AND HINTON, G. E. Restricted boltzmann machines for
collaborative Ô¨Åltering. In ICML (2007), vol. 227, pp. 791‚Äì798."
REFERENCES,0.6706231454005934,"[30] SHANI, G., AND GUNAWARDANA, A. Evaluating recommendation systems. In Recommender
systems handbook. Springer, 2011, pp. 257‚Äì297."
REFERENCES,0.6735905044510386,"[31] SUN, F., LIU, J., WU, J., PEI, C., LIN, X., OU, W., AND JIANG, P. Bert4rec: Sequential
recommendation with bidirectional encoder representations from transformer. In Proceedings
of the 28th ACM international conference on information and knowledge management (2019),
pp. 1441‚Äì1450."
REFERENCES,0.6765578635014837,"[32] TANG, J., BELLETTI, F., JAIN, S., CHEN, M., BEUTEL, A., XU, C., AND H. CHI, E.
Towards neural mixture recommender for long range dependent user sequences. In WWW
(2019), pp. 1782‚Äì1793."
REFERENCES,0.6795252225519288,"[33] VAN DEN OORD, A., DIELEMAN, S., AND SCHRAUWEN, B. Deep content-based music
recommendation. In NIPS (2013), pp. 2643‚Äì2651."
REFERENCES,0.6824925816023739,"[34] WANG, H., WANG, N., AND YEUNG, D. Collaborative deep learning for recommender
systems. In KDD (2015), pp. 1235‚Äì1244."
REFERENCES,0.685459940652819,"[35] WANG, Y.-X., BALLE, B., AND KASIVISWANATHAN, S. P. Subsampled r√©nyi differential
privacy and analytical moments accountant. In The 22nd International Conference on ArtiÔ¨Åcial
Intelligence and Statistics (2019), PMLR, pp. 1226‚Äì1235."
REFERENCES,0.6884272997032641,"[36] WOLF, T., DEBUT, L., SANH, V., CHAUMOND, J., DELANGUE, C., MOI, A., CISTAC, P.,
RAULT, T., LOUF, R., FUNTOWICZ, M., DAVISON, J., SHLEIFER, S., VON PLATEN, P., MA,
C., JERNITE, Y., PLU, J., XU, C., SCAO, T. L., GUGGER, S., DRAME, M., LHOEST, Q.,
AND RUSH, A. M. Transformers: State-of-the-art natural language processing. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations (Online, Oct. 2020), Association for Computational Linguistics, pp. 38‚Äì45."
REFERENCES,0.6913946587537092,Under review as a conference paper at ICLR 2022
REFERENCES,0.6943620178041543,"[37] WU, F., QIAO, Y., CHEN, J.-H., WU, C., QI, T., LIAN, J., LIU, D., XIE, X., GAO, J., WU,
W., ET AL. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics (2020), pp. 3597‚Äì3606."
REFERENCES,0.6973293768545994,"[38] WU, L., YANG, Y., CHEN, L., LIAN, D., HONG, R., AND WANG, M. Learning to transfer
graph embeddings for inductive graph based recommendation. In SIGIR (2020), pp. 1211‚Äì1220."
REFERENCES,0.7002967359050445,"[39] WU, S., TANG, Y., ZHU, Y., WANG, L., XIE, X., AND TAN, T. Session-based recommenda-
tion with graph neural networks. In AAAI (2019), vol. 33, pp. 346‚Äì353."
REFERENCES,0.7032640949554896,"[40] YUAN, F., HE, X., KARATZOGLOU, A., AND ZHANG, L. Parameter-efÔ¨Åcient transfer from
sequential behaviors for user modeling and recommendation. In SIGIR (2020), pp. 1469‚Äì1478."
REFERENCES,0.7062314540059347,"[41] ZHOU, K., WANG, H., ZHAO, W. X., ZHU, Y., WANG, S., ZHANG, F., WANG, Z., AND WEN,
J.-R. S3-rec: Self-supervised learning for sequential recommendation with mutual information
maximization. In Proceedings of the 29th ACM International Conference on Information &
Knowledge Management (2020), pp. 1893‚Äì1902."
REFERENCES,0.7091988130563798,"[42] ZHU, Z., SEFATI, S., SAADATPANAH, P., AND CAVERLEE, J. Recommendation for new users
and new items via randomized training and mixture-of-experts transformation. In SIGIR (2020),
pp. 1121‚Äì1130."
REFERENCES,0.712166172106825,"A
APPENDIX"
REFERENCES,0.7151335311572701,"A.1
DATA USAGE AND PRIVACY DISCUSSIONS"
REFERENCES,0.7181008902077152,"Data Usage: For real-world scenarios, new business owners who hope to do zero-shot recommenda-
tion need to check the dataset policy before usage. On the other hand, if multinational corporations
hope to establish a branch in a new region they could use their own data from other existing regions."
REFERENCES,0.7210682492581603,"Privacy: To protect the privacy of user data, we encourage people who want to adopt our methods
to train the model using source-domain data with differential privacy embedded. Some related
references include [1; 5; 35]."
REFERENCES,0.7240356083086054,"A.2
BAYESIAN TREATMENT WITH INFERENCE NETWORKS"
REFERENCES,0.7270029673590505,Training. Using Jensen‚Äôs inequality we have the following evidence lower bound (ELBO):
REFERENCES,0.7299703264094956,"log p(R|X, Œªu, Œªv) ‚â•Eq[log p(R, U, V|X, Œªu, Œªv)] ‚àíEq[log q(U, V|X)],"
REFERENCES,0.7329376854599406,"where the expectation is over q(U, V|X). We have L = Is
X i=1 Ni
X"
REFERENCES,0.7359050445103857,"t=1
‚àílog(fsoftmax(u‚ä§
itvit)) + Œªu 2 Is
X i=1 Ni
X"
REFERENCES,0.7388724035608308,"t=1
||uit ‚àífseq({viœÑ }t‚àí1
œÑ=1)||2
2 + Œªv 2 Js
X"
REFERENCES,0.7418397626112759,"j=1
||vj ‚àífe(xj)||2
2, (4)"
REFERENCES,0.744807121661721,"For the variational distribution q(U, V|X) we have the following factorization:"
REFERENCES,0.7477744807121661,"q(U, V|X) = q(V|X)q(U|V, X) = q(V|X)q(U|V),"
REFERENCES,0.7507418397626113,"where U = [ui]I
i=1, V = [vj]J
j=1, and X = [xj]J
j=1"
REFERENCES,0.7537091988130564,"q(vj|X) = q(vj|xj) = N

vj; fe,¬µ(xj), fe,œÉ2(xj)

,"
REFERENCES,0.7566765578635015,"q(ui|V, X) = N

ui; fseq,¬µ({vjk}ni
k=1), fseq,œÉ2({vjk}ni
k=1)
"
REFERENCES,0.7596439169139466,Under review as a conference paper at ICLR 2022
REFERENCES,0.7626112759643917,"Overall,"
REFERENCES,0.7655786350148368,"log p(R|X, Œªu, Œªv) ‚â•Eq[log p(R, U, V|X, Œªu, Œªv)] ‚àíEq[log q(U, V|X)] = Eq[ Is
X i=1 Ni
X"
REFERENCES,0.7685459940652819,"k=1
log(fsoftmax(uT
ikvik))]"
REFERENCES,0.771513353115727,"+ Eq[log p(V|X)] + Eq[log p(U|V)] ‚àíEq[log q(U, V|X)] + C = Eq[ Is
X i=1 Ni
X"
REFERENCES,0.7744807121661721,"k=1
log(fsoftmax(uT
ikvik))] ‚àíŒªv 2 X"
REFERENCES,0.7774480712166172,"j
‚à•fe,œÉ2(xj)‚à•1"
REFERENCES,0.7804154302670623,"+ Eq[log p(U|V)] ‚àíEq[log q(U, V|X)] + C,"
REFERENCES,0.7833827893175074,"where the expectation is over q(U, V|X). Below we discuss several important terms including
‚àíEq[log q(U, V|X)] and Eq(U,V|X)[log p(U|V)] in the ELBO."
REFERENCES,0.7863501483679525,"The term ‚àíEq[log q(U, V|X)] is the entropy of q(U, V|X)."
REFERENCES,0.7893175074183977,"‚àíEq(U,V|X)[log q(U, V|X)] = ‚àíEq(V|X)Eq(U|V)[log q(V|X) + log q(U|V)]"
REFERENCES,0.7922848664688428,= ‚àíEq(V|X)Eq(U|V)[log q(V|X)] ‚àíEq(V|X)Eq(U|V)[log q(U|V)]
REFERENCES,0.7952522255192879,"= ‚àíEq(V|X)[log q(V|X)] ‚àíEq(V|X)
h
Eq(U|V)[log q(U|V)]
i = 1 2 X"
REFERENCES,0.798219584569733,"j
[1‚ä§log fe,œÉ2(xj)] + 1 2 X"
REFERENCES,0.8011869436201781,"i
Eq(V|X)
h Ni
X"
REFERENCES,0.8041543026706232,"t=1
1‚ä§log fseq,œÉ2({viœÑ }t‚àí1
œÑ=1)
i
+ C ‚âà1 2 X"
REFERENCES,0.8071216617210683,"j
[1‚ä§log fe,œÉ2(xj)] +
1
2Nv X i X V"
REFERENCES,0.8100890207715133,"h Ni
X"
REFERENCES,0.8130563798219584,"t=1
1‚ä§log fseq,œÉ2({viœÑ }t‚àí1
œÑ=1)
i
+ C,"
REFERENCES,0.8160237388724035,"where V is sampled for Nv times to get a Monte Carlo estimate of Eq(V|X)[log fseq,œÉ2(viœÑ }t‚àí1
œÑ=1)].
In practice, it is found that one sample is usually sufÔ¨Åcient due to the use of SGD-based optimization
process."
REFERENCES,0.8189910979228486,Similarly the term
REFERENCES,0.8219584569732937,"Eq(U,V|X)[log p(U|V)] = Eq(V|X)
h
Eq(U|V)[log p(U|V)]
i = ‚àíŒªu"
REFERENCES,0.8249258160237388,"2 Eq(V|X)
h Is
X i=1 Ni
X"
REFERENCES,0.827893175074184,"t=1
‚à•fseq,œÉ2({viœÑ }t=1
œÑ=1)‚à•1
i
+ C = ‚àíŒªu"
NV,0.8308605341246291,"2Nv X V Is
X i=1 Ni
X"
NV,0.8338278931750742,"t=1
‚à•fseq,œÉ2({viœÑ }t‚àí1
œÑ=1)‚à•1 + C,"
NV,0.8367952522255193,"where V is sampled for Nv times from q(V|X) to get a Monte Carlo estimate of
Eq(V|X)[‚à•fseq,œÉ2(vik}ni
k=1)‚à•2
2]. In practice, it is found that one sample is usually sufÔ¨Åcient due
to the use of SGD-based optimization process."
NV,0.8397626112759644,"Inference. Inference can be done via Monte Carlo estimates of p(R|X, Œªu, Œªv). SpeciÔ¨Åcally,"
NV,0.8427299703264095,"p(R|X, Œªu, Œªv) = Eq(p(R|U, V)) ‚âà
1
NvNu X V(n) X"
NV,0.8456973293768546,"U(n)
p(R|U(n), V(n)),"
NV,0.8486646884272997,"where V(n) ‚àºq(V|X) and U(n) ‚àºq(U|V(n), X)."
NV,0.8516320474777448,One could also use MAP inference to trade accuracy for speed.
NV,0.8545994065281899,"p(R|X) =
Z
p(R|U, V, X)p(U, V|X)dUdV ‚âà
Z
p(R|U, V, X)Œ¥UMAP(U)Œ¥VMAP(V)dUdV,"
NV,0.857566765578635,"where Œ¥(¬∑) denotes a Dirac delta distribution. UMAP and VMAP are the MAP estimate of U and V
given X:"
NV,0.8605341246290801,"(UMAP, VMAP) ‚âàargmax
U,V
q(U, V|X) =

fseq(fe,¬µ(X)), fe,¬µ(X)

."
NV,0.8635014836795252,Under review as a conference paper at ICLR 2022
NV,0.8664688427299704,"A.3
IMPLEMENTATION DETAILS"
NV,0.8694362017804155,"We use pre-trained google/bert_uncased_L-12_H-768_A-12 BERT model from Huggingface [36] to
process item description and generate item embedding. The dimension of BERT embedding is 768.
We use BERT embedding as input to a single-layer neural netwrok (NN) and the output dimension
for the NN is set to D, which equals to the hidden dimension of the sequential model."
NV,0.8724035608308606,"For ZESREC variants we use the default optimal setting: we set the hidden dimension D as 300, the
dropout rate as 0.2, and the number of training epochs as 20. We use Adagrad [7] as the optimizer
with a learning rate of 0.1, and train ZESREC variants in the source domain with early stopping
based on validation loss. We set the hyperparameter Œªv as a relatively large value 100 to restrain the
variance of the item offset vector œµj."
NV,0.8753709198813057,"For base models (HRNN, TCN, GRU4Rec) and corresponding base-meta models (HRNN-Meta,
TCN-Meta, GRU4Rec-Meta), we set the dropout rate as 0.2 and the number of training epochs as 20;
we choose Adagrad [7] as the optimizer. We train base models and base-meta models in the target
domain with early stopping and perform hyperparameter tuning, both are based on the validation loss.
We tried the hidden dimension D in {128, 300} and the learning rate Œ∑ in {0.01, 0.1, 1}, and choose
to use the conÔ¨Ågurations {D : 128, Œ∑ : 1} for base models and {D : 128, Œ∑ : 0.1} for base-meta
models. ùúÜ!
ùíó"" ùíô"" ùëπ!""#"
NV,0.8783382789317508,"ùíñ#$
ùúÜ% ùíó#$"
NV,0.8813056379821959,"j ‚àà[1, J]"
NV,0.884272997032641,"i ‚àà[1, I]
t ‚àà[1, ùëÅ!]"
NV,0.887240356083086,User UEN ùíô#$ ùúÜ!
NV,0.8902077151335311,"Item UEN
ùúè‚àà[0, t-1]"
NV,0.8931750741839762,"Figure 5: Graphical model for ZESREC. The item
side (left) and the user side (right) share the same
Œªv and v‚Äôs. The plates indicate replication."
NV,0.8961424332344213,"For all datasets, we treat the rating as implicit
feedback (interactions between user and item).
Since we are considering session-based recom-
mendation and using sequential model, we Ô¨Ålter
out users with only 1 interaction as the sequen-
tial model need to ingest at least one item from
user history as context to perform next-step pre-
diction."
NV,0.8991097922848664,"No Temporal Leakage. We adopted a rigor-
ous experimental setup for zero-shot learning:
Firstly, we makes sure that there are no overlap-
ping users and items between the source domain
and the target domain. Secondly, we temporally
split the two domains, meaning that all the train-
ing interactions in the source domain must be
happened before all the testing interactions in the target domain."
NV,0.9020771513353115,All experiments were ran on a GPU machine with Nvidia Tesla V100 16G memory GPU.
NV,0.9050445103857567,"A.4
MODEL ARCHITECTURE SEQ SEQ SEQ"
NV,0.9080118694362018,"‚Ä¶
User Universal"
NV,0.9109792284866469,Embedding
-LAYER NN,0.913946587537092,1-Layer NN
-LAYER NN,0.9169139465875371,Pretrained BERT Model X
-LAYER NN,0.9198813056379822,1-Layer NN
-LAYER NN,0.9228486646884273,Pretrained BERT Model ‚Ä¶ 0.36 0.29 ‚Ä¶ 0.09 0.02
-LAYER NN,0.9258160237388724,Prediction Score
-LAYER NN,0.9287833827893175,Item Universal
-LAYER NN,0.9317507418397626,Embedding
-LAYER NN,0.9347181008902077,Pretrained BERT Model
-LAYER NN,0.9376854599406528,1-Layer NN
-LAYER NN,0.9406528189910979,Item Universal
-LAYER NN,0.9436201780415431,Embedding
-LAYER NN,0.9465875370919882,Pretrained BERT Model
-LAYER NN,0.9495548961424333,1-Layer NN
-LAYER NN,0.9525222551928784,Item Universal
-LAYER NN,0.9554896142433235,Embedding
-LAYER NN,0.9584569732937686,Item Universal
-LAYER NN,0.9614243323442137,Embedding
-LAYER NN,0.9643916913946587,Item Universal
-LAYER NN,0.9673590504451038,"Embedding
‚Ä¶
‚Ä¶"
-LAYER NN,0.9703264094955489,"Latent Item 
Offset Vector
+"
-LAYER NN,0.973293768545994,"Latent Item 
Offset Vector
+"
-LAYER NN,0.9762611275964391,"Latent Item 
Offset Vector
+"
-LAYER NN,0.9792284866468842,"Latent Item 
Offset Vector ‚Ä¶
Latent Item 
Offset Vector
+ +"
-LAYER NN,0.9821958456973294,"Latent User 
Offset Vector"
-LAYER NN,0.9851632047477745,Figure 6: Model Architecture
-LAYER NN,0.9881305637982196,Under review as a conference paper at ICLR 2022
-LAYER NN,0.9910979228486647,"The graphical model for ZESREC is shown in Fig. 5. In the MAP estimation version of ZES-
REC, we set Œªu ‚Üí‚àûto remove latent user offset vector, thereby preventing ZESREC from
over-parameterization. Fig. 6 shows a simpliÔ¨Åed deterministic model architecture from the neural
network point of view. Below we elaborate on the process in terms of two stages: training in source
domain and inference in target domain."
-LAYER NN,0.9940652818991098,"Training in Source Domain. During training, on the encoder side, we generate BERT embeddings
from items‚Äô NL descriptions as item universal embeddings and then add the learnable item offset
vectors to them, which yield the Ô¨Ånal item embeddings (item latent vectors). The sequential model
will aggregate item embeddings of items in user history to generate user embeddings. On the decoder
side, we compute item latent vectors the same way we do on the encoder side. Here we share item
latent vectors on both encoder and decoder sides to reduce number of parameters. Empirically we
Ô¨Ånd this prevent overÔ¨Åtting and improve performance."
-LAYER NN,0.9970326409495549,"Inference in Target Domain. In this phase, we will remove the item offset vectors on both the
encoder and decoder sides. We use item universal embeddings directly instead as the Ô¨Ånal item latent
vectors since we have no access to interactions in the target domain, and therefore the model is unable
to estimate the item offset vectors."
