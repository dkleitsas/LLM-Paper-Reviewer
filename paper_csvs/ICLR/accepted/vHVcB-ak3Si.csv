Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.00392156862745098,"Integral pose regression combines an implicit heatmap with end-to-end training
for human body and hand pose estimation. Unlike detection-based heatmap meth-
ods, which decode final joint positions from the heatmap with a non-differentiable
argmax operation, integral regression methods apply a differentiable expectation
operation. This paper offers a deep dive into the inference and back-propagation
of integral pose regression to better understand the differences in performance and
training compared to detection-based methods. For inference, we give theoretical
support as to why expectation should always be better than the argmax opera-
tion, i.e. integral regression should always outperform detection. Yet, in practice,
this is observed only in hard cases because the heatmap activation for regression
shrinks in easy cases. We then experimentally show that activation shrinkage is
one of the leading causes for integral regression’s inferior performance. For back-
propagation, we theoretically and empirically analyze the gradients to explain the
slow training speed of integral regression. Based on these findings, we incorporate
the supervision of a spatial prior to speed up training and improve performance."
INTRODUCTION,0.00784313725490196,"1
INTRODUCTION"
INTRODUCTION,0.011764705882352941,"2D human pose estimation aims to detect the image coordinates of the body and/or the hand. In
recent years, detection-based methods (Newell et al., 2016; Xiao et al., 2018; Sun et al., 2019; Li
et al., 2019) and integral pose regression methods (Sun et al., 2018; Iqbal et al., 2018) have emerged
as two common paradigms for human pose estimation. Both methods learn a heatmap representing
pixel-wise likelihoods of the joint positions. The heatmap is learned explicitly for detection-based
methods but remains implicit or “latent” for integral regression methods. To decode the heatmaps
to joint coordinates, detection methods use an argmax, while integral regression methods take the
expected value. As the expectation operation is differentiable, integral regression has the benefit
of being end-to-end learnable, even though detection methods seem more competitive accuracy-
wise (COC, 2020)."
INTRODUCTION,0.01568627450980392,"Gu et al. (2021)’s recent work showed a curious performance difference between integral regression
and detection methods. With a nuanced split over the evaluation set, integral regression outperforms
detection when test samples are “harder”, i.e. with fewer keypoints present in the scene, under higher
occlusion and lower bounding box resolutions. Given that both detection and integral regression
methods work with fully convolutional feed-forward architectures, the questions naturally arise:
What are the reasons behind this performance difference? Why is integral regression able to excel in
these hard cases, when its overall performance seems to lag behind detection methods (COC, 2020)?
These questions serve as the motivation for our closer study and analysis of integral pose regression."
INTRODUCTION,0.0196078431372549,"Detection and integral regression methods differ in two aspects in both the forward and backward
pass (see Fig. 1 for an overview). In the forward pass, the heatmap is decoded with an argmax
for detection whereas a softmax normalization and expectation are used for integral regression. In
the backward pass, detection methods are supervised with an explicitly defined Gaussian heatmap
centered on the ground truth joint, while integral regression is supervised by the joint coordinates
directly.
Through detailed theoretical analysis and experimentation on the decoding and back-
propagation process, we make the following findings and contributions:"
INTRODUCTION,0.023529411764705882,Published as a conference paper at ICLR 2022
WE PROPOSE A UNIFIED MODEL OF THE HEATMAP TO INTERPRET AND COMPARE DETECTION AND INTEGRAL,0.027450980392156862,"1. We propose a unified model of the heatmap to interpret and compare detection and integral
regression methods. We verify the model experimentally and show that as samples shift
from hard to easy, the activation region on the heatmap shrinks for both detection and
integral regression methods."
WE DEMONSTRATE EXPERIMENTALLY THAT DEGENERATELY SMALL REGIONS OF ACTIVATION DEGRADE THE,0.03137254901960784,"2. We demonstrate experimentally that degenerately small regions of activation degrade the
accuracy of both detection and integral regression methods."
WE DEMONSTRATE EXPERIMENTALLY THAT DEGENERATELY SMALL REGIONS OF ACTIVATION DEGRADE THE,0.03529411764705882,"3. Integral regression methods, which decode the heatmap with an expectation operation,
should result in a lower expected end-point error than detection methods that decode with
an argmax operation. In practice, this can be observed only for hard samples due to the
shrinkage of the active region on the heatmap."
WE DEMONSTRATE EXPERIMENTALLY THAT DEGENERATELY SMALL REGIONS OF ACTIVATION DEGRADE THE,0.0392156862745098,"4. Direct supervision with the joint coordinates in integral regression, although end-to-end,
suffers from gradient vanishing and provides less spatial cues for learning the heatmap
than the explicit heatmap supervision of detection methods. As a result, the training of
integral regression is more inefficient and slower to converge than detection methods."
WE DEMONSTRATE EXPERIMENTALLY THAT DEGENERATELY SMALL REGIONS OF ACTIVATION DEGRADE THE,0.043137254901960784,"Our findings provide insight into integral pose regression, which has better theoretical performance,
and show that the density of heatmaps plays an important role in decoding heatmaps to coordinates."
RELATED WORK,0.047058823529411764,"2
RELATED WORK"
RELATED WORK,0.050980392156862744,"Since the concept of heatmaps were proposed in (Tompson et al., 2014), detection-based methods
have been top performers in human pose estimation. Existing detection-based works emphasize ex-
tracting high-quality multi-resolution features. Particularly, Xiao et al. (2018) proposed to adapt
ResNet with deconvolution layers while Hourglass (Newell et al., 2016) and Cascaded Pyramid
Network (CPN) (Li et al., 2019) introduced cascaded network architectures with a coarse-to-fine
design paradigm. High-Resolution Network (HRNet) (Sun et al., 2019) follows the coarse-to-fine
paradigm and further improves performance by adopting more dense connections across different
resolution representations. The integral regression method of Sun et al. (2018) introduced a compet-
itive regression-based framework but is surpassed by more advanced detection-based works (Xiao
et al., 2018; Sun et al., 2019)."
RELATED WORK,0.054901960784313725,"Numerical regression-based methods, which directly regress joint coordinates, are commonly used
in facial landmark detection (Feng et al., 2018; Zhu et al., 2020). These methods, however, are not as
accurate as detection-based methods on human pose estimation. In order to improve the accuracy,
integral regression methods attempt to implicitly merge the spatial knowledge from the heatmap
via a “latent” heatmap. Integral regression methods are especially preferred in hand pose estima-
tion (Spurr et al., 2020; Yang et al., 2021) but are still less common for human pose estimation (Sun
et al., 2018; Nibali et al., 2018)."
RELATED WORK,0.058823529411764705,"Recently, two parallel lines of work have emerged to analyse detection and integral regression meth-
ods separately. For detection-based methods, Huang et al. (2020) identified a heatmap bias caused
by the inconsistency of the coordinate system transformations and solved it by redesigning the trans-
formations during data processing. Additionally, as the predicted heatmaps during inference may
violate the Gaussian assumption and worsen the performance, Huang et al. (2020) proposed to find
the optimal Gaussian location based on the prediction while Zhang et al. (2020) modulated the
predicted Gaussian distribution for decoding."
RELATED WORK,0.06274509803921569,"For integral regression methods, Nibali et al. (2018) experimentally compared different heatmap
regularizers, heatmap normalization schemes and loss functions. They found that using Jensen-
Shannon regularization on the heatmap with a softmax normalization and L1 loss achieves the best
performance for integral regression. More recently, Gu et al. (2021) performed a systematic com-
parison of detection and integral regression methods with a common backbone and discovered the
performance advantage of integral regression on the “hard” samples. Furthermore, Gu et al. (2021)
revealed a bias that arises from taking the expectation after the softmax normalization. As such,
they proposed a compensation scheme to mitigate the bias, which in turn improves the overall per-
formance of integral regression methods, making them competitive with detection methods."
RELATED WORK,0.06666666666666667,Published as a conference paper at ICLR 2022
RELATED WORK,0.07058823529411765,"Figure 1: Comparison of the two decoding processes for detection-based methods (pink panel) and integral
regression methods (green panel). They share the same encoding (yellow panel) but have different representa-
tions of the ground truth (gray panel).
3
PRELIMINARIES ON HUMAN POSE ESTIMATION"
RELATED WORK,0.07450980392156863,"In this work, we target the more commonly used ‘top-down’ form of pose estimation in which
a person detector already provides a cropped image I of the person. For simplicity, we focus our
discussion on one given joint out of the K total joints in the body. The pose estimation model outputs
a heatmap ˆH ∈RM×N, where M and N are the dimensions of the spatial heatmap. Typically, M
and N are scaled down by a factor of 4 from the original input dimensions of I (Xiao et al., 2018;
Sun et al., 2019). The heatmap ˆH represents a (discrete) spatial likelihood P(J|I), where J ∈R1×2
is the 2D coordinates of the joint. In practice, all K heatmaps are predicted simultaneously by the
same network, where each joint is one channel. In both detection and integral regression methods,
the coordinates ˆJ are decoded from ˆH, where the two methods differ in the manner of decoding (see
Sec. 3.1) and the form of supervision applied (see Sec. 3.2)."
RELATED WORK,0.0784313725490196,"3.1
HEATMAP DECODING: MAX VERSUS EXPECTED VALUE"
RELATED WORK,0.08235294117647059,Detection methods apply an argmax on ˆH indexed by p to estimate the joint coordinates ˆJde:
RELATED WORK,0.08627450980392157,"ˆJde = arg max
p
ˆH(p).
(1)"
RELATED WORK,0.09019607843137255,"Taking an argmax can be interpreted as taking a maximum likelihood on the heatmap ˆH, assuming
that ˆH is proportional to the likelihood. In practice, the final ˆJde value is determined as a linear
combination of the highest and second-highest response on ˆH as a way to account for quantization
effects in the discrete heatmap (Newell et al., 2016). A more recent work, DARK (Zhang et al.,
2020), approximates the true prediction by a Taylor series evaluated at the maximum activation of
the heatmap and shows this to be more accurate."
RELATED WORK,0.09411764705882353,"Integral pose regression applies an expectation operation on ˆH to give a mean estimate of the joint
coordinates. To do so, the heatmap must first be normalized to sum up to 1; the most common
and effective approach (Nibali et al., 2018) is to apply a softmax normalization. Afterwards, the
predicted joint ˆJre with x and y components ˆJx and ˆJy1 is determined by taking the expectation on
the normalized heatmap ˜H with elements ˜hp at location p:"
RELATED WORK,0.09803921568627451,"ˆJre =
 ˆJx
ˆJy 
=
X"
RELATED WORK,0.10196078431372549,"p∈Ω
p · ˜hp
where
˜hp =
eβˆhp
P"
RELATED WORK,0.10588235294117647,"(p′)∈Ω
eβˆhp′ .
(2)"
RELATED WORK,0.10980392156862745,"Here, Ωis the domain of the heatmap and β is a scaling parameter used in the softmax normalization.
Note that softmax normalization assigns a non-zero value to all pixels in ˜H, even if it was originally
zero in ˆH. These values also contribute to the expected value, resulting in a center-biased estimated"
RELATED WORK,0.11372549019607843,"1For clarity, we drop the subscript ‘re’, as we refer to the individual components for integral regression only."
RELATED WORK,0.11764705882352941,Published as a conference paper at ICLR 2022
RELATED WORK,0.12156862745098039,"joint coordinate ˆJre (Gu et al., 2021). The smaller the β, the stronger the bias. Although Gu et al.
(2021) proposed a compensation scheme, for the purpose of our analysis, we will assume that β is
sufficiently large such that the impact of the bias is negligible."
RELATED WORK,0.12549019607843137,"3.2
SUPERVISION: EXPLICIT HEATMAP VERSUS GROUND TRUTH COORDINATES"
RELATED WORK,0.12941176470588237,"Detection methods are learned by providing supervision on the heatmap. The ground truth H is
given as a circular Gaussian, with a mean centered at the ground truth joint coordinate (see Fig. 1).
The loss applied is a pixel-wise MSE between the ground truth hp and the predicted ˆhp:"
RELATED WORK,0.13333333333333333,"Lde = ||H −ˆH||2
2 =
X"
RELATED WORK,0.13725490196078433,"p∈Ω
(hp −ˆhp)2,
(3)"
RELATED WORK,0.1411764705882353,"where Ωis the domain of the heatmap. As the loss is defined in terms of the heatmap and not in terms
of the predicted joint coordinates, detection-based methods are not end-to-end in their learning, and
this is often cited as a drawback (Sun et al., 2018; Zhang et al., 2020)."
RELATED WORK,0.1450980392156863,"Integral pose regression defines a loss based on the difference between the prediction ˆJre and the
ground truth joint location J. The L1 loss empirically performs better than L2 (Sun et al., 2018):
Lre = ∥Jgt −ˆJre∥1 = (| ˆJx −Jx| + | ˆJy −Jy|).
(4)
Integral regression methods are end-to-end because they provide supervision at the joint level. As
ˆH is learned only implicitly, some works refer to the heatmap as “latent” (Iqbal et al., 2018)."
PERFORMANCE DIFFERENCES,0.14901960784313725,"3.3
PERFORMANCE DIFFERENCES"
PERFORMANCE DIFFERENCES,0.15294117647058825,"Sun et al. (2018) and Gu et al. (2021) compared the performance of detection and integral regression
methods on human pose estimation. Sun et al. (2018) found that integral regression is either as
competitive or better for 2D pose estimation, though results are not fully conclusive, as the backbone
they used for integral regression is different from detection."
PERFORMANCE DIFFERENCES,0.1568627450980392,"The more systematic and detailed comparison from Gu et al. (2021) used a common backbone. It
also partitioned the data based on three factors of variation dominating current human pose estima-
tion benchmarks (Ruggero Ronchi & Perona, 2017): the number of joints present in the scene, the
percentage of occlusion and the largest dimension of the bounding box input. The “hard” samples
are those with either 1-5 joints present, > 50% occlusion, or 32-64px input. For these samples,
integral regression had, on average, 15% lower end-point error (EPE) than detection. The “easy”
samples are defined by 11-17 joints, <10% occlusion, or >128px input. In these cases, detection
methods were marginally better. The remaining cases with factors in between were considered to be
“medium”; performance of both methods was roughly equivalent on these samples."
ANALYSIS ON HEATMAP DECODING,0.1607843137254902,"4
ANALYSIS ON HEATMAP DECODING"
LOCALIZED HEATMAP MODEL,0.16470588235294117,"4.1
LOCALIZED HEATMAP MODEL"
LOCALIZED HEATMAP MODEL,0.16862745098039217,"To analyze the performance differences between detection and integral regression methods, we make
the following assumptions on the heatmap itself. First and foremost, we assume that a well-trained
network will produce a heatmap ˆhp with significant or large values in a localized support region
around ˆJ. We denote the region of support as Φ and assume that outside of Φ, the heatmap activation
is approximately zero (see Fig. 2 (a))."
LOCALIZED HEATMAP MODEL,0.17254901960784313,"Specifically, the normalized heatmap2 ˜H is modelled by some density distribution P. We make no
assumption on the form of P, but do assume that it fully captures the support for the joint location
within Φ and that Φ is centered on the expected value of P on location µ = (µx, µy) with an area
of (2s + 1, 2s + 1), i.e. µ =
X"
LOCALIZED HEATMAP MODEL,0.17647058823529413,"p∈Ω
p · ˜hp =
X"
LOCALIZED HEATMAP MODEL,0.1803921568627451,"p∈Φ(µ,s)
p · ˜hp ="
LOCALIZED HEATMAP MODEL,0.1843137254901961,"""
µx+s
P"
LOCALIZED HEATMAP MODEL,0.18823529411764706,i=µx−s
LOCALIZED HEATMAP MODEL,0.19215686274509805,"µy+s
P"
LOCALIZED HEATMAP MODEL,0.19607843137254902,"j=µy−s
i · ˜hij,
µx+s
P"
LOCALIZED HEATMAP MODEL,0.2,i=µx−s
LOCALIZED HEATMAP MODEL,0.20392156862745098,"µy+s
P"
LOCALIZED HEATMAP MODEL,0.20784313725490197,"j=µy−s
j · ˜hij #⊺ .
(5)"
LOCALIZED HEATMAP MODEL,0.21176470588235294,"2For the purpose of discussion, we may consider normalizing the heatmap for detection methods as per
Eq. 2, which will not affect the outcome since arg max( ˜H)=arg max( ˆH)."
LOCALIZED HEATMAP MODEL,0.21568627450980393,Published as a conference paper at ICLR 2022
LOCALIZED HEATMAP MODEL,0.2196078431372549,"(a) Illustration of Φ
(b) Activation sum of ˜H
(c) Optimal σ of predicted
heatmaps
(d) AP versus ground
truth σ
Figure 2: (a) The heatmap is only activated within Φ; (b) more heatmap activations are bounded by a (2s +
1) × (2s + 1) region for integral regression than detection; (c) if the heatmap is approximated as a Gaussian,
the resulting σ shrinks as samples shift from hard to easy; this shrinkage is faster and more extreme for integral
regression compared to detection; (d) in detect methods, the optimal σ is around 2; smaller and larger σ hurt
performance."
LOCALIZED HEATMAP MODEL,0.2235294117647059,"Experimental Verification of Localized Heatmap Model
The assumption of localized activa-
tions is reasonable for detection methods as their heatmaps are learned explicitly to match a (local-
ized) ground truth Gaussian. We empirically verify this model by tallying the activations in the nor-
malized heatmap for both detection and integral regression methods based on the easy/medium/hard
splits as specified in Sec. 3.3."
LOCALIZED HEATMAP MODEL,0.22745098039215686,"Specifically, we train both a detection and an integral regression network for human pose estima-
tion. For both networks, we use a Simple Baseline (SBL) (Xiao et al., 2018) architecture with a
ResNet50 (He et al., 2016) backbone. The two networks differ only in their manner of decoding
and form of supervision as outlined in Section 3. We then apply the networks on the standard hu-
man pose estimation benchmark MSCOCO (Lin et al., 2014) using the same easy, medium and hard
splits as Gu et al. (2021). For each produced heatmap of 64×48, we estimate an activation sum A by
summing the normalized heatmap activations within a (2s+1)×(2s+1) square around the ground
truth coordinate, (Jx, Jy):"
LOCALIZED HEATMAP MODEL,0.23137254901960785,A(s) =
LOCALIZED HEATMAP MODEL,0.23529411764705882,"Jx+s
X"
LOCALIZED HEATMAP MODEL,0.23921568627450981,i=Jx−s
LOCALIZED HEATMAP MODEL,0.24313725490196078,"Jy+s
X"
LOCALIZED HEATMAP MODEL,0.24705882352941178,"j=Jy−s
˜hij.
(6)"
LOCALIZED HEATMAP MODEL,0.25098039215686274,"From Fig. 2 (b), we show that with a sufficiently large s, e.g. 9, the activation sum exceeds 95%."
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.2549019607843137,"4.2
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.25882352941176473,"Expected End-Point Error (EPE)
For detection methods, which decode the heatmap with an
argmax, we assume that each position p has some probability w(µ, s) of being the maximum acti-
vation, i.e. arg max(P)Φ(µ,s) ∼w(µ, s). Here, w(µ, s) represents a radially symmetric distribution
centered on µ with all non-zero support contained within Φ. It follows that the expected EPE for
one sample of a detection method can be defined as:"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.2627450980392157,"Ede(µ, s) =
X"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.26666666666666666,"p∈Φ(µ,s)
w(p)||Jgt −p||2,
(7)"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.27058823529411763,"where Jgt denotes the ground truth coordinates. For integral regression methods, which decode the
heatmap with an expectation, the estimated joint coordinate aligns with the center of Φ by definition,
i.e. ˆJ=µ, leading to the following expected EPE for a single sample:
Ere(µ) = ||Jgt −µ||2.
(8)
It can be verified that the Ede ≥Ere (see proof in Appendix A.2). This result suggests that for some
fixed (µ, s), the expected EPE of regression should always be better than detection. However, the re-
sults of Gu et al. (2021) as summarized in Sec. 3.3 have clearly shown otherwise. This inconsistency
can be better understood with the following two experiments."
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.27450980392156865,"Experimental Verification: Integral Regression Methods Are “More” Localized than Detec-
tion
The plot in Fig. 2 (b) indicates that regression methods seem to be more localized than detec-
tion methods. In particular, for smaller s, A is smaller for detection methods than integral regression
methods."
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.2784313725490196,Published as a conference paper at ICLR 2022
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.2823529411764706,"Model
Detection
Regression
+σ=0.5
+σ=1
+σ=2
+σ=3
+σ=4"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.28627450980392155,"AP
71.3
67.1
65.1
68.5
71.4
69.8
66.9"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.2901960784313726,"APe/m
74.3
70.4
68.2
71.9
74.1
73.0
70.2"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.29411764705882354,"APh
43.2
44.7
43.9
45.2
47.4
47.6
46.9"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.2980392156862745,"A(s = 2)e/m
0.49
0.65
0.75
0.63
0.54
0.49
0.45"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.30196078431372547,"A(s = 2)h
0.30
0.35
0.37
0.36
0.41
0.39
0.39"
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.3058823529411765,"Table 1: Performance and activation sum of detection baseline, regression baseline, and regression baseline
added KLDiv loss with different σs. When σ = 0.5, 1, the results are not improved but even worsened. When
σ=2, the spread of heatmaps is smoothed and the results even exceed detection baselines."
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.30980392156862746,"To further verify, we compare the heatmaps with an idealized Gaussian heatmap of varying stan-
dard deviations or σ. We plot the σ that ‘ gives the lowest Pearson Chi-square statistic, i.e. the
highest similarity for the different levels of difficulty in Fig. 2 (c) (detailed values are provided in
Appendix A.3). As expected, the optimal σ decreases as the samples progress from hard to easy;
this result is in line with Fig. 2 (b) and shows that heatmaps are more localized for easy samples.
However, we also observe that the optimal σ for intgral regression is much smaller for the medium
and easy cases. We speculate that this difference arises because detection methods are trained with
a Gaussian heatmap of σ = 2 (Xiao et al., 2018; Sun et al., 2019). Integral regression methods,
on the other hand, have no such explicit supervision, and therefore no restriction on the extent of
localization."
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.3137254901960784,"Experimental Verification: Extremely Localized Heatmaps Degrade Performance
Is it possi-
ble that smaller hypothetical σ, i.e. more localized heatmaps, causes poor performance? We verify
this for detection methods by applying ground truth Gaussian heatmaps of varying σ for training
the networks as per Sec. 4.1 and evaluate network performance using Average Precision (AP). A
higher AP corresponds to more accurately located joints. Fig. 2 (d) confirms that σ = 2 is optimal.
A smaller σ = 1 degrades the performance slightly, and there is a significant drop in the extreme
case when σ=0.5."
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.3176470588235294,"To verify the impact of σ on regression methods, we add a distribution prior to the heatmap to prevent
the shrink of the support region Φ. Specifically, we apply a Kullback-Leibler Divergence (KLDiv)
loss between the non-normalized heatmap ˆH and a Gaussian heatmap with varying σ. From Table 1,
we see that the regression baseline is worse than detection baseline. However, when we add a prior
on the heatmap distribution to encourage a sufficiently large Φ, i.e. (+σ = 1, 2, 3), performance
improves. At the optimal +σ = 2, integral regression becomes competitive with and even exceeds
the detection baseline with the optimal σ. Most importantly, extreme σ values cause performance to
degenerate, especially in the case of the extremely small +σ=0.5."
EXTREME LOCALIZATION AS A CAUSE FOR PERFORMANCE DIFFERENCES,0.3215686274509804,"We also demonstrate the performance and localization extent of easy/medium and hard cases by AP
and activation sum A(s = 2). After adding priors (e.g. , +σ = 2), A(s = 2)e/m decreases from
0.65 to 0.54, indicating the heatmaps are less localized, and the performance APe/m is significantly
improved. We provide the detailed training information regarding different loss weights for better
understanding in Appendix A.4 and detailed changes in heatmap shrinkage in Appendix A.5."
ANALYSIS OF SUPERVISION AND LEARNING,0.3254901960784314,"5
ANALYSIS OF SUPERVISION AND LEARNING"
ANALYSIS OF SUPERVISION AND LEARNING,0.32941176470588235,"Given the same heatmap, different methods of heatmap decoding not only affect the final coordi-
nates but also determine the gradients that arise from the loss. The gradients in turn influence the
learning and thereby change the generation of the heatmaps. In this section, we explore the gradi-
ents of heatmaps with respect to the loss functions for detection and integral regression methods and
pinpoint the specific gradient components that slow down the learning of integral regression."
ANALYSIS OF SUPERVISION AND LEARNING,0.3333333333333333,Published as a conference paper at ICLR 2022
HEATMAP GRADIENTS,0.33725490196078434,"5.1
HEATMAP GRADIENTS"
HEATMAP GRADIENTS,0.3411764705882353,"Detection Methods
The gradient of Lde (see Eq. 3) with respect to each pixel in the estimated
heatmap ˆhp is straightforward:
∂Lde"
HEATMAP GRADIENTS,0.34509803921568627,"∂ˆhp
= 2(ˆhp −hp).
(9)"
HEATMAP GRADIENTS,0.34901960784313724,"The gradient in Eq. 9 features the predicted heatmap value ˆhp and the ground truth heatmap value
hp. It provides explicit supervision at every pixel. False positives and false negatives are penalized
by reducing wrong high likelihoods and raising incorrect low likelihoods, respectively. For each
position p in the heatmap, if the predicted ˆhp is smaller than the ground truth hp, the gradient is
negative and proportionally increases the value in the next iteration. The vice versa is true for the
predicted ˆhp larger than the ground truth."
HEATMAP GRADIENTS,0.35294117647058826,"Integral Regression
The gradient of the loss Lre (see Eq. 4) with respect to ˆhp can be estimated
based on the chain rule as (see detailed derivation in Appendix A.6)"
HEATMAP GRADIENTS,0.3568627450980392,∇p := ∂Lre
HEATMAP GRADIENTS,0.3607843137254902,"∂ˆhp
= ∂Lre ∂ˆJ"
HEATMAP GRADIENTS,0.36470588235294116,"∂ˆJ
∂˜hp"
HEATMAP GRADIENTS,0.3686274509803922,"∂˜hp
∂ˆ
hp
=
β ˜hp
|{z}
∇1 (value factor)"
HEATMAP GRADIENTS,0.37254901960784315,"(s( ˆJx −Jx)(i −ˆJx) + s( ˆJy −Jy)(j −ˆJy))
|
{z
}
∇2 (location factor)"
HEATMAP GRADIENTS,0.3764705882352941,".
(10)"
HEATMAP GRADIENTS,0.3803921568627451,"In Eq. 10, the gradient is split into two terms of interest: ∇1 and ∇2, which we name as the value
factor and the location factor, respectively. The nature of the value and location factors gives strong
indication to the learning process for integral regression methods. Firstly, the gradient ∇p is propor-
tional to the normalized predicted heatmap value ˜hp as given in the value factor. This factor makes
it prone to gradient vanishing wherever locations p have small heatmap predictions. Secondly, the
location factor is a linear combination of i and j, and it manifests itself as a linear plane; far away
points like corners are more likely to have large magnitudes. Though these two factors do not pre-
vent the loss from decreasing during learning, they slow down training. We further elaborate this in
Sec. 5.2."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.3843137254901961,"5.2
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP"
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.38823529411764707,"We now conduct a detailed analysis of the gradients presented in Eq. 10 and show the typical cases
that arise during learning. When referring to Ω, the domain of the heatmap, we use a coordinate
system with the origin at the upper-left corner and increasing coordinates moving to the lower right
corner (see Fig. 2 (a)). For simplicity of discussion, we assume that the ground truth joint Jgt is
located somewhere in the lower-right quadrant of Ω, though the analysis holds for Jgt in all other
quadrants of Ωanalogously. The four scenarios are visualized in Fig. 3."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.39215686274509803,"We base our analysis on the assumption that a larger magnitude of ∇p has a larger influence on the
weights of the network and causes a greater change at the location p in the next iteration, i.e."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.396078431372549,"hn+1
p
≈hn
p −γ∇n
p,
(11)"
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.4,"where γ represents the step size or learning rate (see sketch of proof in Appendix A.7) and n denotes
the update iteration. As the value and location factors have different contributions on the gradient
from Eq. 10, and they are in turn affected by the size s and location µ of Φ, we can define four
characteristic cases of Φ and outline how learning is affected in each case."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.403921568627451,"(1) s is large, Φ has uniformly random values, e.g. a randomly initialized heatmap at the start of
the training. As ˜hp is in a similar range for all p, the value factor ∇1 (see Eq. 10) of all pixels are
approximately similar. As such, the distinction between one pixel’s gradient and that of another is
determined by the location factor ∇2. Given that the ground truth coordinates (Jx, Jy) are in the
lower-right quadrant, as long as (Jx, Jy) are both greater than the predicted coordinate ( ˆJx, ˆJy), ∇p
has a gradation that increases progressively towards the bottom right corner. Generally speaking,
the location factor pushes the activations of the heatmap towards the corner of the correct quadrant
as corners own the largest gradients in the resultant linear plane of the location factor ∇2."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.40784313725490196,"(2) s is small, µ is far from the ground truth J, e.g. when µ is in the upper left quadrant of Ωwhile
the ground truth Jgt is at the bottom right. A typical scenario is if the heatmap activates around the"
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.4117647058823529,Published as a conference paper at ICLR 2022
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.41568627450980394,"Figure 3: Toy example of regression update (left) and detection update (right). The black triangle denotes the
prediction (expectation value for regression and max value for detection) while the red dot denotes the ground
truth position. The arrow shows the movement of the prediction."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.4196078431372549,"left ankle for the ground truth right ankle. In this case, for the pixels outside of Φ, the value factor
approaches zero, i.e. ˜hp −→0, and pushes ∇p towards zero, hence the pixels receive very limited
gradient updates. For pixels in Φ, the values decrease gradually until the value factor outside of Φ
no longer dominates, i.e. they reach the same scale, at which point the heatmap returns to case (1)."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.4235294117647059,"(3) s is small, µ is in the corner of the same quadrant as the ground truth Jgt. Initially, this case
is similar to case (2). In the process of all the elements becoming the same scale, all the activations
move towards the diagonal side and the prediction approaches the ground truth coordinate."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.42745098039215684,"(4) s is small, µ close to ground truth Jgt. This scenario occurs when the model is reasonably
trained and can roughly localize the joint. The gradient at the ground truth pixel Jgt is always
negative or zero, i.e."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.43137254901960786,"∇(Jx,Jy) = −β˜h(Jx,Jy)(∥ˆJx −Jx∥+ ∥ˆJy −Jy∥).
(12)"
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.43529411764705883,"This non-positive value guides the network to predict a large heatmap response at the ground truth
location, i.e. a large ˜h(Jx,Jy); this in turn increases the gradient ∇(Jx,Jy). This property makes the
network more likely to predict a few exceptionally large pixels, thus shrinking the support region Φ
and leading to a very small σ in Sec. 4.1."
A DETAILED ANALYSIS OF THE INTEGRAL REGRESSION HEATMAP,0.4392156862745098,"For detection-based methods, the ground truth heatmaps have already explicitly pointed out whether
or not a pixel should be activated in the heatmap. Combined with its effective pixel-wise loss, the
network learns faster than regression-based methods; this is validated in the next section."
EXPERIMENTAL VERIFICATION,0.44313725490196076,"5.3
EXPERIMENTAL VERIFICATION"
EXPERIMENTAL VERIFICATION,0.4470588235294118,"Idealized Sample
We start by considering the case of one sample for a single joint and visualize
the progression of a (64, 48) heatmap as it gets updated by Eq. 11. While ∇n
p can be calculated
mathematically by Eq. 10, we use autograd in Pytorch to obtain the gradients, which we verify to
be equivalent. The four cases from Sec. 5.2 are initialized randomly, with a symmetric Gaussian
(σ = 2) centered at some upper left point, with a linear plane at the lower right quadrant, and a
symmetric Gaussian (σ=2) centered at ground truth, respectively. The progression of the differently
initialized heatmaps can be observed in Fig. 3."
EXPERIMENTAL VERIFICATION,0.45098039215686275,"Mathematically, it is clear from Eq. 2 that the resulting joint coordinate location from integral regres-
sion can align with the ground truth joint even if the underlying heatmap does not follow a localized
heatmap model. This is also illustrated in the first three cases discussed in Sec. 5.2. However,
from the thousands of training samples observed over many epochs, we posit that during real-world
training, the network can only consistently lower the loss if the heatmap ˜H is learned to represent
P(J|I). As such, the activation region Φ will be correctly localized over the corresponding seman-
tic region in the image. Instead, what slows down convergence is the learning of Φ, or rather, the
lack of direct guidance to yield a correct Φ as per detection methods. For the same four cases, we
visualize the updated heatmap of Eq. 11 from the gradients defined by the detection loss in Eq. 9."
EXPERIMENTAL VERIFICATION,0.4549019607843137,Published as a conference paper at ICLR 2022
EXPERIMENTAL VERIFICATION,0.4588235294117647,"(a) Comparisons of detection and regression in real
training
(b) Training Efficiency of Detection versus
Regression on COCO validation set"
EXPERIMENTAL VERIFICATION,0.4627450980392157,"Figure 4: (a) Comparisons of detection and regression in real training on an image selected from the COCO
validation set at epoch 1 and 10. Results of detection have already roughly stabilized while regression can only
localize to a small extent in epoch 10 with some background pixels still activated. We enlarge the right ankle
part of regression in the red box for better visualization. More examples are shown in Appendix A.8. (b) The
regression method converges slower than the detection method."
EXPERIMENTAL VERIFICATION,0.4666666666666667,"Unlike regression, each case leads consistently to the same localized Φ centered on the ground truth
joint coordinate, regardless of the initialization."
EXPERIMENTAL VERIFICATION,0.47058823529411764,"Real World Samples
The four cases listed in Sec. 5.2 are not so clearly observable in real-world
training as the output is based on the optimization results of batch-wise training in a real network.
To verify the results, we compare the training progression for a detection and an integral regression
network under the experimental settings described in Sec. 4.1. Fig. 4 (b) compares the training speed
of both detection and integral regression and shows the slower convergence of integral regression."
EXPERIMENTAL VERIFICATION,0.4745098039215686,"We visualize for one sample from the MSCOCO validation set the heatmap at epochs 1 and 10 to
compare the training progression for detection and integral regression. Fig. 4 (a) shows the heatmaps
of the “left eye” and “right ankle” joints. For regression, after one epoch, the activations are spread
over a quarter to half of the heatmap. This aligns roughly to a mix of Case 1 and 3 where activations
are in the correct quadrant but not yet localized to a small region of support. After 10 epochs of
training, predictions for the left eye (which is actually occluded, making it a “hard” sample) are
localized with several activations of approximately the same value (all shown as red). For the right
ankle (which is clearly present and is an “easy” sample), the network predicts heatmap values that
have collapsed spatially and dominate at one or two pixel locations."
EXPERIMENTAL VERIFICATION,0.47843137254901963,"In contrast, the detection method presents a well-localized heatmap after only one epoch of training.
While there is some activation on the left ankle in epoch 1, this is unlikely to cause errors for the
argmax decoding. After 10 epochs, all activations are centered around the correct right ankle."
CONCLUSION,0.4823529411764706,"6
CONCLUSION"
CONCLUSION,0.48627450980392156,"In this paper, we dive deeper into integral pose regression by comparing the differences in the
heatmap activations and the gradients between detection and integral regression methods. Our theo-
retical analysis paired with empirical verification shows that the shrinkage of heatmaps of the inte-
gral regression method is the main cause for its lower performance compared to detection. Further-
more, we investigate the gradients of integral regression by giving theoretical evidence, toy problem
and real world illustrations to show that the learning of integral regression is slower to converge as a
result of only implicit supervisory signals. To mitigate the slow convergence and poor performance,
we propose a simple spatial supervision on top of the existing framework."
CONCLUSION,0.49019607843137253,"Acknowledgements
This research / project is supported by the Ministry of Education, Singapore,
under its MOE Academic Research Fund Tier 2 (STEM RIE2025 MOE-T2EP20220-0015)."
CONCLUSION,0.49411764705882355,Published as a conference paper at ICLR 2022
REFERENCES,0.4980392156862745,REFERENCES
REFERENCES,0.5019607843137255,"COCO Leader Board. https://cocodataset.org/, 2020."
REFERENCES,0.5058823529411764,"Zhen-Hua Feng, Josef Kittler, Muhammad Awais, Patrik Huber, and Xiao-Jun Wu. Wing loss for
robust facial landmark localisation with convolutional neural networks. In CVPR, pp. 2235–2245,
2018."
REFERENCES,0.5098039215686274,"Kerui Gu, Linlin Yang, and Angela Yao. Removing the bias of integral pose regression. In ICCV,
pp. 11067–11076, 2021."
REFERENCES,0.5137254901960784,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770–778, 2016."
REFERENCES,0.5176470588235295,"Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang. The devil is in the details: Delving into
unbiased data processing for human pose estimation. In CVPR, pp. 5700–5709, 2020."
REFERENCES,0.5215686274509804,"Umar Iqbal, Pavlo Molchanov, Thomas Breuel Juergen Gall, and Jan Kautz. Hand pose estimation
via latent 2.5 d heatmap regression. In ECCV, pp. 118–134, 2018."
REFERENCES,0.5254901960784314,"Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yuming Du, Tianzi Xiao, Gang Yu, Hongtao
Lu, Yichen Wei, and Jian Sun. Rethinking on multi-stage networks for human pose estimation.
arXiv preprint arXiv:1901.00148, 2019."
REFERENCES,0.5294117647058824,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pp.
740–755. Springer, 2014."
REFERENCES,0.5333333333333333,"Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estima-
tion. In ECCV, pp. 483–499. Springer, 2016."
REFERENCES,0.5372549019607843,"Aiden Nibali, Zhen He, Stuart Morgan, and Luke Prendergast. Numerical coordinate regression
with convolutional neural networks. arXiv preprint arXiv:1801.07372, 2018."
REFERENCES,0.5411764705882353,"Matteo Ruggero Ronchi and Pietro Perona. Benchmarking and error diagnosis in multi-instance
pose estimation. In ICCV, pp. 369–378, 2017."
REFERENCES,0.5450980392156862,"Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Otmar Hilliges, and Jan Kautz. Weakly supervised 3d
hand pose estimation via biomechanical constraints. In ECCV, pp. 211–228, 2020."
REFERENCES,0.5490196078431373,"Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for
human pose estimation. In CVPR, pp. 5693–5703, 2019."
REFERENCES,0.5529411764705883,"Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen Wei. Integral human pose regression.
In ECCV, pp. 529–545, 2018."
REFERENCES,0.5568627450980392,"Jonathan Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. Joint training of a convolutional
network and a graphical model for human pose estimation. NeurIPS, 27, 2014."
REFERENCES,0.5607843137254902,"Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking.
In ECCV, pp. 466–481, 2018."
REFERENCES,0.5647058823529412,"Linlin Yang, Shicheng Chen, and Angela Yao. Semihand: Semi-supervised hand pose estimation
with consistency. In ICCV, pp. 11364–11373, 2021."
REFERENCES,0.5686274509803921,"Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu. Distribution-aware coordinate repre-
sentation for human pose estimation. In CVPR, pp. 7093–7102, 2020."
REFERENCES,0.5725490196078431,"Beier Zhu, Chunze Lin, Quan Wang, Renjie Liao, and Chen Qian. Fast and accurate: Structure
coherence component for face alignment. arXiv preprint arXiv:2006.11697, 2020."
REFERENCES,0.5764705882352941,Published as a conference paper at ICLR 2022
REFERENCES,0.5803921568627451,"A
APPENDIX"
REFERENCES,0.5843137254901961,"A.1
EXTENSION OF FIG. 2 (B)"
REFERENCES,0.5882352941176471,"We provide an extended plot of Fig. 2 (b), which includes both detection and regression on easy,
medium, and hard cases in Fig. I."
REFERENCES,0.592156862745098,"A.2
PROOF FOR EQ. 7⩾EQ. 8 IN SEC. 4.2"
REFERENCES,0.596078431372549,"This subsection gives the detailed proof for Eq. 7>Eq. 8, which means X"
REFERENCES,0.6,"p∈Φ(µ,s)
w(p)||Jgt −p||2 ⩾||Jgt −µ||2, where
X"
REFERENCES,0.6039215686274509,"p∈Φ(µ,s)
w(p) = 1.
(I)"
REFERENCES,0.6078431372549019,"In this way, this equation can be rewritten as X"
REFERENCES,0.611764705882353,"p∈Φ(µ,s)
w(p)||Jgt −p||2 ⩾
X"
REFERENCES,0.615686274509804,"p∈Φ(µ,s)
w(p)||Jgt −µ||2.
(II)"
REFERENCES,0.6196078431372549,"As the distribution of Φ(µ, s) is radially symmetric, a sufficient but unnecessary proof is that for
any symmetric pair {p1(µx −a, µy −b), p2(µx + a, µy + b)}, it satisfies p"
REFERENCES,0.6235294117647059,"(x −µx −a)2 + (y −µy −b)2 +
p"
REFERENCES,0.6274509803921569,"(x −µx + a)2 + (y −µy + b)2 ⩾2
p"
REFERENCES,0.6313725490196078,"(x −µx)2 + (y −µy)2.
(III)"
REFERENCES,0.6352941176470588,"According to Triangle Inequality in normed vector space, we arrive at"
REFERENCES,0.6392156862745098,"||u||2 + ||v||2 ⩾||u + v||2.
(IV)"
REFERENCES,0.6431372549019608,"When we set u = (x −µx −a, y −µy −b) and v = (x −µx + a, y −µy + b), Eq. II is established."
REFERENCES,0.6470588235294118,"A.3
PEARSON CHI-SQUARE VALUES ON DIFFERENT CASES"
REFERENCES,0.6509803921568628,"We report the detailed Pearson Chi-square values between heatmaps of both detection and regression
in different scenarios and Gaussian templates with different values of σ in Table. I, II, III. We set
the threshold of A(s) > 0.8 to show that the support region can represent the whole heatmaps.
Therefore, for easy and medium cases, s=4; while for hard cases, s=8."
REFERENCES,0.6549019607843137,"2
4
6
8
10
12
14
s 0.2 0.4 0.6 0.8 1.0"
REFERENCES,0.6588235294117647,Activation Sum A(s)
REFERENCES,0.6627450980392157,"Regression-Easy
Regression-Medium
Regression-Hard
Regression-Overall
Detection-Easy
Detection-Medium
Detection-Hard
Detection-Overall"
REFERENCES,0.6666666666666666,"Figure I: Activation Sum A(s) of both detection and regression method on easy, medium, hard, and
overall cases."
REFERENCES,0.6705882352941176,Published as a conference paper at ICLR 2022
REFERENCES,0.6745098039215687,"s
σ
regression
detection
2
3
4
5
2
3
4
5
8
>1000
4.76
4.12
3.95
>1000
3.51
3.31
3.25"
REFERENCES,0.6784313725490196,"Table I: Results of the Pearson Chi-square test between distribution of heatmap H and Gaussian
templates T with different window size s and standard deviation σ in hard cases. σ =5 is the best-
matching template for both detection and regression. σ stops at 5 as the Gaussian templates with
size 8 become similar after."
REFERENCES,0.6823529411764706,"s
σ
regression
detection
1
2
3
4
1
2
3
4
4
>1000
3.25
3.65
3.73
>1000
1.76
0.76
0.77"
REFERENCES,0.6862745098039216,"Table II: Results of the Pearson Chi-square test between distribution of heatmap H and Gaussian
templates T with different window size s and standard deviation σ in medium cases. For detection,
best matched σ should be between 3 and 4; while for regression, optimal σ in medium case should
be between 1 and 2."
REFERENCES,0.6901960784313725,"A.4
DETAILS OF COMBINING PRIOR LOSS AND INTEGRAL LOSS"
REFERENCES,0.6941176470588235,"This subsection demonstrates the influence of weights of prior loss and integral loss, and the mag-
nitudes of two gradients regarding the shared heatmaps during the training."
REFERENCES,0.6980392156862745,"Let us assume wp and wi are weights of the prior and integral losses, respectively. We consider the
ratio wpLp : wi · λLi, where λ = 10−2, to set the two to approximately equal magnitudes at the
initial epoch. Table V shows that the AP heavily depends on the value of σ but for some fixed σ
(within each row), the ratio of the loss weightings has less impact and the results are stable within
1 −2%, with a larger influence only at the extreme settings of 100 : 1 or 1 : 100."
REFERENCES,0.7019607843137254,"Table IV (b) provides the ratios of the gradient magnitudes to help explain why the weightings have
little impact. Table IV (b) gives the mean ratio gp"
REFERENCES,0.7058823529411765,"gi where gp and gi are the gradient magnitudes of the
prior loss and integral loss, respectively, over the course of training. A column of 2/1/0/-1/-2 means
102/101/100/10−1/10−2 ratio in gradients over the epochs. The table shows that regardless of the
weighting, the gradient ratio decreases over the course of training (i.e. dropoff with each column).
Apart from an extremely high weighting of the prior, i.e. 100:1 in column 1, the prior influences the
training only in the initial 10 epochs. After epoch 10, the gradient ratio is around the same scale or
smaller."
REFERENCES,0.7098039215686275,"A.5
DETAILS OF HEATMAP SHRINKAGE"
REFERENCES,0.7137254901960784,"To find out whether the added prior influences the spread of the heatmaps, we investigate the shrink-
age change of heatmaps by separating the samples. We separate the samples based on easy/medium
versus hard cases, where the former generally produce localized heatmaps while the latter have dis-
persed heatmaps as per Fig. I; the alternative is to separate by thresholding the activation sum in a
fixed area (Eq. 6). We consider the activation sum A(1) > 0.34 of the integral regression baseline
as localized, otherwise as dispersed (threshold 0.34 selected based on Fig. I). We tally the activation
sums for the localized and dispersed heatmaps, without and with the prior for different values of
σ. Activation sums A(s)e/m, A(s)h, A(s)l, A(s)h, and Average Precision APe/m, APh, APl, APh
indicate the extent of localization and the performance of the two conditions, respectively. An in-
crease in activation sum indicates further heatmap localization; a decrease indicates dispersion. The
“No prior” row is the baseline integral pose regression."
REFERENCES,0.7176470588235294,"From Table IV (a), both separations present the same trend. For localized cases, forcing σ to be
degenerately small or too large (σ = 0.5 or σ = 4) increases/decreases the activation sum over
the baseline, thereby confirming the effect of the prior as intended. The change in AP follows,
i.e. decreases when σ becomes incompatible from being too small or too large. For dispersed cases,
the prior shrinks the activation sum as expected. Like the localized case, there is an optimal range
for which improvement in AP is better; degenerate σ will decrease the AP from the baseline."
REFERENCES,0.7215686274509804,Published as a conference paper at ICLR 2022
REFERENCES,0.7254901960784313,"s
σ
regression
detection
0.5
1
2
3
4
1
2
2.5
3
3.5
4
4
>1000
3.24
3.53
3.75
3.89
>1000
2.52
0.64
0.72
0.85
0.88"
REFERENCES,0.7294117647058823,"Table III: Results of Pearson Chi-square test between distribution of heatmap H and Gaussian tem-
plates T with different window size s and standard deviation σ in easy cases. Accordingly, for
detection, σ should be approximately 2.5; while for regression, optimal σ should be between 0.5
and 1."
REFERENCES,0.7333333333333333,"wp : λwi
100:1
10:1
1:1
1:10
1:100"
REFERENCES,0.7372549019607844,"+σ=0.5
63.6
64.8
65.1
65.7
66.4"
REFERENCES,0.7411764705882353,"+σ=1
67.6
68.1
68.5
68.3
67.8"
REFERENCES,0.7450980392156863,"+σ=2
68.5
70.8
71.4
70.6
67.8"
REFERENCES,0.7490196078431373,"+σ=3
68.2
69.4
69.8
69.2
68.1"
REFERENCES,0.7529411764705882,"+σ=4
64.2
66.1
66.9
66.9
67.0"
REFERENCES,0.7568627450980392,"wp : λwi
100:1
10:1
1:1
1:10
1:100"
REFERENCES,0.7607843137254902,"epoch 1
2
2
2
1
1"
REFERENCES,0.7647058823529411,"epoch 10
2
1
0
0
0"
REFERENCES,0.7686274509803922,"epoch 30
2
0
0
0
-1"
REFERENCES,0.7725490196078432,"epoch 50
1
-1
-1
-1
-1"
REFERENCES,0.7764705882352941,"epoch 80
0
-1
-1
-1
-2
(a)
(b)"
REFERENCES,0.7803921568627451,"Table IV: (a) Performance of the integral model adding different priors with different weight ratios
wp : λwi. The performance depends largely on different σs, but less influenced with reasonable
change. (b) Ratio of the gradient magnitudes of combined model (integral+σ = 2) with different
weight ratios wp : λwi. The gradient ratio decreases over the course of training. After epoch 10, the
gradient ratio is around the same scale or smaller."
REFERENCES,0.7843137254901961,"model
A(2)e/m
APe/m
A(2)e/m
APl
A(2)h
APh
A(2)l
APl
No prior
0.65
70.4
0.71
72.3
0.35
44.7
0.33
38.9"
REFERENCES,0.788235294117647,"+σ=0.5
0.75
68.2
0.79
69.8
0.36
43.9
0.36
35.2"
REFERENCES,0.792156862745098,"+σ=1
0.63
71.9
0.70
73.9
0.36
45.2
0.31
39.1"
REFERENCES,0.796078431372549,"+σ=2
0.54
74.2
0.59
76.1
0.41
47.4
0.37
39.8"
REFERENCES,0.8,"+σ=3
0.49
73.0
0.56
74.9
0.39
47.6
0.36
39.3"
REFERENCES,0.803921568627451,"+σ=4
0.45
70.2
0.55
71.7
0.39
46.9
0.38
39.5"
REFERENCES,0.807843137254902,"Table V: Localization extent and performance of divided localized samples versus dispersed sam-
ples. The extent of localization largely depends on the added prior. With an optimal σ = 2 chosen,
the previously localized heatmaps have been dispersed with improved performance."
REFERENCES,0.8117647058823529,"A.6
DERIVATION OF EQ. 10"
REFERENCES,0.8156862745098039,"Eq. 10 is obtained by the chain rule. Therefore, we list the value of each term as follows: ∂Lre"
REFERENCES,0.8196078431372549,"∂ˆJ
=

s( ˆJx −Jx)
s( ˆJy −Jy)"
REFERENCES,0.8235294117647058,"
,
∂ˆJ
∂˜hp
= (i + j), ∂˜hp"
REFERENCES,0.8274509803921568,"∂ˆ
hq
=
β˜hq(1 −˜hq), q = p
−˜hp˜hq,
q ̸= p
,
(V)"
REFERENCES,0.8313725490196079,"where q = (u, v) can be any pixel in Ω. Combining the terms, gradients at p can be calculated by ∂Lre"
REFERENCES,0.8352941176470589,"∂ˆhp
= β(( ˆJx −Jx)i + ( ˆJy −Jy)j)˜hp(1 −˜hp) −
X"
REFERENCES,0.8392156862745098,"q∈Ω,
q̸=p"
REFERENCES,0.8431372549019608,β(( ˆJx −Jx)u + ( ˆJy −Jy)v)˜hq˜hp. (VI)
REFERENCES,0.8470588235294118,Eq. 10 can be obtained by rearranging the terms of Eq. VI.
REFERENCES,0.8509803921568627,"A.7
DERIVATION OF EQ. 11"
REFERENCES,0.8549019607843137,"Let us suppose Hn = g(wn|I) where Hn is the heatmap of a given image I at the n-th iteration,
g(wn) is the function of generating the heatmap, i.e. the backbone network, with weights at the n-th
iteration. According to a Taylor expansion, we arrive at"
REFERENCES,0.8588235294117647,Published as a conference paper at ICLR 2022 Epoch 0 Epoch 5 Epoch 10 Epoch 15 Epoch 20
REFERENCES,0.8627450980392157,"Image
Nose
Left eye
Right eye Left ear Right ear
Left 
shoulder"
REFERENCES,0.8666666666666667,"Right 
shoulder"
REFERENCES,0.8705882352941177,"Left 
elbow"
REFERENCES,0.8745098039215686,"Right 
elbow"
REFERENCES,0.8784313725490196,"Left 
wrist Right"
REFERENCES,0.8823529411764706,"wrist
Left hip Right hip"
REFERENCES,0.8862745098039215,"Left 
knee"
REFERENCES,0.8901960784313725,"Right 
knee"
REFERENCES,0.8941176470588236,"Left 
ankle"
REFERENCES,0.8980392156862745,"Right 
ankle"
REFERENCES,0.9019607843137255,"Figure II: Visualization of the training performance of integral pose regression on one single image
at epoch 0, 5, 10, 15, 20 (from top to down). Corner or edge pixels of the correct quadrant are largely
activated in the epoch 0 (highlighted in the yellow boxes). Further training is focused on narrowing
down the activated region (shown in the red boxes). Epoch 0 Epoch 5 Epoch 10 Epoch 15 Epoch 20"
REFERENCES,0.9058823529411765,"Image
Nose
Left eye
Right eye Left ear Right ear
Left 
shoulder"
REFERENCES,0.9098039215686274,"Right 
shoulder"
REFERENCES,0.9137254901960784,"Left 
elbow"
REFERENCES,0.9176470588235294,"Right 
elbow"
REFERENCES,0.9215686274509803,"Left 
wrist Right"
REFERENCES,0.9254901960784314,"wrist
Left hip Right hip"
REFERENCES,0.9294117647058824,"Left 
knee"
REFERENCES,0.9333333333333333,"Right 
knee"
REFERENCES,0.9372549019607843,"Left 
ankle"
REFERENCES,0.9411764705882353,"Right 
ankle"
REFERENCES,0.9450980392156862,"Figure III: Visualization of the training performance of detection-based methods on one single image
at epoch 0, 5, 10, 15, 20 (from top to down). One epoch has already enabled the network to roughly
localize the joint compared with regression-based method (in the yellow boxes). Further training of
detection-based methods makes the predictions more stable and precise (highlighted in red boxes)."
REFERENCES,0.9490196078431372,"g(wn+1) = g(wn) + g
′(wn)"
REFERENCES,0.9529411764705882,"1!
(wn+1 −wn) + g
′′(wn)"
REFERENCES,0.9568627450980393,"2!
(wn+1 −wn)2 + . . . ,
(VII)"
REFERENCES,0.9607843137254902,where wn+1−wn=−α∇g(w)=−α ∂L
REFERENCES,0.9647058823529412,"∂Hg
′(wn) according to gradient descent, and (wn+1−wn)i is
negligible compared with (wn+1−wn) when i>1. Therefore, Eq. VII can be rewritten as"
REFERENCES,0.9686274509803922,"Hn+1 = g(wn+1) ≈g(wn) + g
′(wn)(wn+1 −wn) = Hn −α ∂L"
REFERENCES,0.9725490196078431,"∂H||g
′(wn)||2.
(VIII)"
REFERENCES,0.9764705882352941,"Therefore, for each pixel hp in H, the update rule is"
REFERENCES,0.9803921568627451,"hn+1
p
≈hn
p −γ∇n
p,
(IX)"
REFERENCES,0.984313725490196,"where γ represents α∥g
′(wn)∥, hn
p and ∇n
p denote heatmap and gradient value located at p in the
heatmap at the nth iteration. h0
p denotes the original heatmap."
REFERENCES,0.9882352941176471,Published as a conference paper at ICLR 2022
REFERENCES,0.9921568627450981,"A.8
REALWORLD VISUALIZATIONS"
REFERENCES,0.996078431372549,"We show the visualization of the training performance of integral pose regression and detection-
based methods on one single image in Figs. II and III, respectively. We can see that the further
training of integral pose regression is focused on narrowing down the activated region while the
further training of detection-based methods makes the predictions more stable and precise."
