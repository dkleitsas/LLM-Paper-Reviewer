Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0029585798816568047,"The importance of Variational Autoencoders reaches far beyond standalone gen-
erative models — the approach is also used for learning latent representations and
can be generalized to semi-supervised learning. This requires a thorough analy-
sis of their commonly known shortcomings: posterior collapse and approximation
errors. This paper analyzes VAE approximation errors caused by the combination
of the ELBO objective and encoder models from conditional exponential families,
including, but not limited to, commonly used conditionally independent discrete
and continuous models. We characterize subclasses of generative models consis-
tent with these encoder families. We show that the ELBO optimizer is pulled away
from the likelihood optimizer towards the consistent subset and study this effect
experimentally. Importantly, this subset can not be enlarged, and the respective
error cannot be decreased, by considering deeper encoder/decoder networks."
INTRODUCTION,0.005917159763313609,"1
INTRODUCTION"
INTRODUCTION,0.008875739644970414,"Variational autoencoders (VAE, Kingma & Welling, 2014; Rezende et al., 2014) strive at learning
complex data distributions pd(x), x ∈X in a generative way. They introduce latent variables
z ∈Z and model the joint distribution as pθ(x|z)p(z), where p(z) is a simple distribution which
is usually assumed to be known. The conditional distribution pθ(x|z), called decoder, is modeled
in terms of a deep network parametrized by θ ∈Θ. Models deﬁned in this way allow to sample
from pθ(x) = Ep(z)pθ(x|z) easily, however at the price that computing the posterior pθ(z|x) =
pθ(x|z)p(z)/pθ(x) is usually intractable. To handle this problem, VAE approximates the posterior
pθ(z|x) by an amortized inference encoder qφ(z|x) parametrized by φ ∈Φ. Given the empirical
data distribution pd(x), the model is learned by maximizing the evidence lower bound (ELBO) of
the data log-likelihood L(θ) = Epd log pθ(x). It can be expressed in the following two equivalent
forms:"
INTRODUCTION,0.011834319526627219,"LB(θ, φ) = Epd

Eqφ log pθ(x|z) −DKL(qφ(z|x) ∥p(z))

(1a)"
INTRODUCTION,0.014792899408284023,"= L(θ) −Epd

DKL(qφ(z|x) ∥pθ(z|x))

.
(1b)"
INTRODUCTION,0.01775147928994083,"The ﬁrst form allows for stochastic optimization of ELBO while the second form shows that the gap
between log-likelihood and ELBO is exactly the mismatch between the encoder and the posterior."
INTRODUCTION,0.020710059171597635,"VAEs constitute a powerful deep learning extension of the expectation-maximization (EM) approach
to handle latent variables. They are useful not only as generative models but also, e.g., in semi-
supervised learning (Kingma et al., 2014; Mattei & Frellsen, 2019). Furthermore the encoder part
constructs an efﬁcient embedding of the data in the latent space, useful in many applications. The
outreach of the VAE approach requires therefore a careful empirical and theoretical analysis of the
problems and trade offs involved. The most important ones are (i) posterior collapse (He et al., 2019;
Lucas et al., 2019; Dai et al., 2018; Dai & Wipf, 2019; Dai et al., 2020) and (ii) approximation errors
caused by an inappropriate choice of the encoder family."
INTRODUCTION,0.023668639053254437,Published as a conference paper at ICLR 2022
INTRODUCTION,0.026627218934911243,Consistent VAEs ◊ML
INTRODUCTION,0.029585798816568046,Best Consistent
INTRODUCTION,0.03254437869822485,ML Decoder
INTRODUCTION,0.03550295857988166,"Posterior Collapse
Best VAE
◊VAE"
INTRODUCTION,0.038461538461538464,F(◊) = min „ Epd #
INTRODUCTION,0.04142011834319527,DKL(q„(z|x)Îp◊(z|x)) $
INTRODUCTION,0.04437869822485207,FIaniR91/Wir3hWhGJRehLBb2+qNUCgBQYSVNm0hG8Ls5GR3yHyxc7Y2DPszvNXf5Z/wNzibpLVJQTwszMsz531ndjipEdxiGP5azTv3X/wcP1R8PjJ02cbm1vPz6wuCwY9poUuLlJqQXAFPeQo4MIUQGUq4DydHNf75dQWK7VKU4NDCTNFB9zRtGjfnKaA9JhEud8uNkKO+Gsdu6KaCFaZFHxcKvxORlpVkpQyAS1jFpkIpJFSQWUJdo/EczOP4Scx4kpQVD2cSDPt031EDR3hFUjSzui1pkXH1MWJy4DLQErCYVsFtl6PSop52692KtN6xVxWSz1g0F5tWztlzj+MHBc+RuBYtVq7FgrtMvU/4H1Guqgmc8hXA1HMLYdL2rqdJ1SX1imrpuVc1RfIPia3QyR4wKd3LDun9h7Z29mcUCkOUu6hxUK2CvCt7URIDKMJ97uRr5t69caHDlqUCVkiPIeUo9R05pv7045zqEpvoSRtwaQad2wk3l3tVRt1tSEPrbv1uWUnJd4P9ErfQFfvyi1WG7K872OtFhJ/y63zraXQziOtkmr8kuich7ckQ+kZj0COafCc/yM/G7+bL5nbz1by1sbwvCBL1Xz7B7J+JAs=</latexit>⇥Φ
INTRODUCTION,0.047337278106508875,"Figure 1: Diagram of the VAE trade-off. The optimal solution θVAE is “in between” the maximum
likelihood solution θML and the best solution in the class ΘΦ of consistent VAEs, where the posterior
approximation error function F(θ) vanishes. We give an explicit characterization of this consistent
set. At θVAE there is a balance between the gradient of −F (blue arrow) and the gradient of the data
log-likelihood (black arrow)."
INTRODUCTION,0.05029585798816568,"The VAE approximation error has been studied (e.g., Cremer et al. 2018; Hjelm et al. 2016; Kim
et al. 2018) so far mainly empirically. The problem also occurs and is well-recognized in the context
of variational inference and variational Bayesian inference, where the target posterior distribution is
expected to be complex. It is commonly understood, that the mean ﬁeld approximation of pθ(z|x)
by qφ(z|x) in (1b) signiﬁcantly limits variational Bayesian inference. In contrast, in VAEs, the
decoder may adopt to compensate for the chosen encoder family. The effect of this coupling, we
believe, is not fully understood. The phenomenon of decoder adopting to the posterior was experi-
mentally observed, e.g., by Cremer et al. (2018, Section 5.4), noting that the approximation error is
often dominated by the amortization error. Turner & Sahani (2011, Sec. 1.4) analytically show for
linear state space models that simpler variational approximations (such a mean-ﬁeld) can lead to less
bias in parameter estimation than more complicated structured approximations. Similarly, Shu et al.
(2018) view the VAE objective as providing a regularization and show that making the amortized
inference model smoother, while increasing the amortization gap, leads to a better generalization."
INTRODUCTION,0.05325443786982249,"The common (empirical) understanding of the importance of the gap between the approximate and
the true posterior has led to many generalizations of standard VAEs, which achieve impressive practi-
cal results, notably, tighter bounds using importance weighting (Burda et al., 2016; Nowozin, 2018),
encoders employing normalizing ﬂows (Rezende & Mohamed, 2015; Kingma et al., 2016), hierar-
chical and autoregressive encoders (Vahdat & Kautz, 2020; Sønderby et al., 2016; Ranganath et al.,
2016), MRF encoders (Vahdat et al., 2020) and more. While these extensions mitigate the posterior
mismatch problem, they often come at a price of a more difﬁcult training and more expensive in-
ference. Furthermore, simpler encoders may be of practical interest. Burda et al. (2016, Appendix
C) illustrates that IWAE approximate posteriors are less regular and more spread out. In contrast,
factorized encoders provide simple embeddings useful for downstream tasks such as semantic hash-
ing (Chaidaroon & Fang, 2017)."
INTRODUCTION,0.05621301775147929,"The aim of this paper is to study the approximation error of VAEs and its impact on the learned
decoder. We consider a setting that generalizes many common VAEs, in particular popular models
where encoder and decoder are conditionally independent Bernoulli or Gaussian distributions: we
assume that both decoder and encoder are conditional exponential families. We identify the subclass
of generative models where the encoder can model the posterior exactly, referred to as consistent
VAEs. We give a characterization of consistent VAEs revealing that this set in fact does not depend
on the complexity of the involved neural networks. We further show that the ELBO optimizer is
pulled towards this set away from the likelihood optimizer. Specializing the characterization to
several common VAE models, we show that the respective consistent models turn out to be RBM-
like in many cases. We experimentally investigate the detrimental effect in one case and show that a
simpler but more consistent VAE can perform better in the other."
PROBLEM STATEMENT,0.05917159763313609,"2
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.0621301775147929,"We adopt the following notion of approximation error. Consider a generative model class PΘ =
{pθ(x, z) | θ ∈Θ}, the encoder class QΦ = {qφ(z|x) | φ ∈Φ} and the data distribution pd(x).
The maximum likelihood generative model is given by θML ∈argmaxθ∈Θ Epd(x) log pθ(x). For a
decoder with parameters θ we deﬁne its approximation error as the likelihood difference L(θML) −"
PROBLEM STATEMENT,0.0650887573964497,Published as a conference paper at ICLR 2022
PROBLEM STATEMENT,0.06804733727810651,"L(θ). Respectively, the VAE approximation error is deﬁned for a given θ as:"
PROBLEM STATEMENT,0.07100591715976332,"L(θML) −maxφ LB(θ, φ) ≥L(θML) −L(θ).
(2)"
PROBLEM STATEMENT,0.07396449704142012,"In order for this error to become zero, two conditions are necessary and sufﬁcient:"
PROBLEM STATEMENT,0.07692307692307693,"• Parameters (θ, φ) must be optimal for the ELBO objective.
• ELBO must be tight at (θ, φ), i.e., LB(θ, φ) = L(θ)."
PROBLEM STATEMENT,0.07988165680473373,"Assuming that the optimality can be achieved, we study the non-tightness gap L(θ) −LB(θ, φ).
From (1b) it expresses as Epd

DKL(qφ(z|x) ∥pθ(z|x))

. It follows that ELBO is tight at (θ, φ) iff
qφ(z|x) ≡pθ(z|x). Hence, we deﬁne the consistent set ΘΦ ⊆Θ as the subset of distributions
pθ(x, z) whose posteriors are in QΦ, i.e.,"
PROBLEM STATEMENT,0.08284023668639054,"ΘΦ =

θ ∈Θ
 ∃φ ∈Φ : qφ(z|x) ≡pθ(z|x)
	
.
(3)"
PROBLEM STATEMENT,0.08579881656804733,"The KL-divergence in the ELBO objective (1b) can vanish only if θ ∈ΘΦ. If the likelihood max-
imizer θML is not contained in ΘΦ, then this KL-divergence pulls the optimizer towards ΘΦ and
away from θML as illustrated in Fig. 1."
PROBLEM STATEMENT,0.08875739644970414,"We characterize the consistent set ΘΦ, on which the bound is tight, and show that this set is quite
narrow and does not depend on the complexity of the encoder and decoder networks beyond simple
1-layer linear mappings of sufﬁcient statistics."
THEORETICAL ANALYSIS,0.09171597633136094,"3
THEORETICAL ANALYSIS"
THEORETICAL ANALYSIS,0.09467455621301775,"We consider a general class of VAEs, where both encoder and decoder are deﬁned as exponential
families. This class includes many common models, in particular Gaussian VAEs and Bernoulli
VAEs with conditional independence assumptions, but also more complex ones, e.g., where the
encoder is a conditional random ﬁeld (Vahdat et al., 2020)1."
THEORETICAL ANALYSIS,0.09763313609467456,"Assumption 1 (Exponential family VAE). Let X and Z be sets of observations and latent variables,
respectively. We consider VAE models deﬁned by"
THEORETICAL ANALYSIS,0.10059171597633136,"pθ(x|z) = h(x) exp

⟨ν(x), fθ(z)⟩−A(fθ(z))]
(4a)"
THEORETICAL ANALYSIS,0.10355029585798817,"qφ(z|x) = h′(z) exp

⟨ψ(z), gφ(x)⟩−B(gφ(x))],
(4b)"
THEORETICAL ANALYSIS,0.10650887573964497,"where ν : X →Rn and ψ: Z →Rm are ﬁxed sufﬁcient statistics of dimensionality n and m;
fθ : Z →Rn and gφ : X →Rm are the decoder, resp., encoder, networks with learnable parameters
θ, resp. φ; h: X →R+, h′ : Z →R+ are strictly positive base measures and A, B denote the
respective log-partition functions."
THEORETICAL ANALYSIS,0.10946745562130178,"Notice that this assumption imposes no restrictions on the nature of random variables x and z. They
can be discrete or continuous, univariate or multivariate. Similarly, it imposes no restrictions on the
complexity of the decoder and encoder networks fθ(z) and gφ(x)."
THEORETICAL ANALYSIS,0.11242603550295859,"Characterization of the consistent set. In the ﬁrst step of our analysis, we investigate the conditions
under which the approximation error of an exponential family VAE can be made exactly zero. As
discussed above, a tight VAE (θ, φ) must satisfy ∀(x, z) qφ(z|x) = pθ(z|x), which leads to the
following theorem."
THEORETICAL ANALYSIS,0.11538461538461539,Theorem 1. The consistent set ΘΦ of an exponential family VAE is given by decoders of the form
THEORETICAL ANALYSIS,0.11834319526627218,"p(x|z) = h(x) exp

⟨ν(x), Wψ(z)⟩+ ⟨ν(x), u⟩−A(z)

,
(5)"
THEORETICAL ANALYSIS,0.12130177514792899,"where W is a n × m matrix and u ∈Rn. Moreover, the corresponding encoders have the form"
THEORETICAL ANALYSIS,0.1242603550295858,"q(z|x) = h′(z) exp

ψ(z), W T ν(x)

+ ⟨ψ(z), v⟩−B(x)],
(6)"
THEORETICAL ANALYSIS,0.12721893491124261,where v ∈Rm.
THEORETICAL ANALYSIS,0.1301775147928994,"1Notice, however, that this class does not include VAEs with advanced encoder families like normalizing
ﬂows, hierarchical and autoregressive encoders."
THEORETICAL ANALYSIS,0.13313609467455623,Published as a conference paper at ICLR 2022
THEORETICAL ANALYSIS,0.13609467455621302,"This is a direct consequence of a theorem by Arnold & Strauss (1991) (see Appendix A.1 for more
details). For a tight VAE, Theorem 1 states that the decoder and encoder are generalized linear
models (GLMs) (5) and (6) with the interaction between x and z parametrized by a matrix W and
two vectors u, v instead of the (complex) neural networks with parameters θ, φ. The corresponding
joint probability distribution takes the form of an EF Harmonium (Welling et al., 2005):"
THEORETICAL ANALYSIS,0.1390532544378698,"p(x, z) = h(x)h′(z) exp
 
⟨ν(x), Wψ(z)⟩+ ⟨ν(x), u⟩+ ⟨ψ(z), v⟩−A

.
(7)"
THEORETICAL ANALYSIS,0.14201183431952663,"Corollary 1. The subset ΘΦ of consistent models can not be enlarged by considering more complex
encoder networks g(x), provided that the afﬁne family W Tν(x) can already be represented."
THEORETICAL ANALYSIS,0.14497041420118342,"Corollary 2. Let the decoder network family be afﬁne in ψ(z), i.e., f(z) = Wψ(z) + a and let the
encoder network family g(x) include at least all afﬁne maps V ν(x) + b. Then any global optimum
of ELBO attains a zero approximation error."
THEORETICAL ANALYSIS,0.14792899408284024,"VAE can escape consistency when it degenerates to a ﬂow. In practice, VAE models with rich de-
coders are almost never tight. It is therefore natural to ask, whether a small VAE posterior mismatch
error implies closeness of the optimal decoder to some decoder in the consistent set."
THEORETICAL ANALYSIS,0.15088757396449703,"Deﬁnition 1. A VAE (pθ, qφ) is ε-tight for some ε > 0 if Epd(x)[DKL(qφ(z|x) ∥pθ(z|x))] ≤ε."
THEORETICAL ANALYSIS,0.15384615384615385,"It turns out that this deﬁnition allows a VAE to approach tightness while not approaching consis-
tency. In the continuous case an example satisfying ε-tightness with non-linear decoder follows
from Dai & Wipf (2019, Theorem 2). They show, for a class of Gaussian VAEs with general neural
networks fθ, gφ, that it is possible to build a sequence of network parameters θt, φt with the follow-
ing properties: i) the target distribution is approximated arbitrary well, ii) the posterior mismatch
DKL(qφt(z|x) ∥pθt(z|x)) approaches zero and iii) both the encoder and decoder approach deter-
ministic mappings. The VAE thus approaches a ﬂow model (or invertible neural network) between
the data manifold and a subspace of the latent space (Dai & Wipf, 2019). Clearly, in a general case
the ﬂow must be non-linear. A similar case can be made for discrete variables, see Example A.1."
THEORETICAL ANALYSIS,0.15680473372781065,"Non-deterministic nearly-tight VAEs approach consistency. We would however argue that the
mode where the decoder and encoder are nearly-deterministic is not a natural VAE solution. By
making additional assumptions, excluding such deterministic solutions, and restricting ourselves to
the ﬁnite space in order to simplify the analysis, we can show that an ε-tight VAE does indeed
approach an EF-Harmonium."
THEORETICAL ANALYSIS,0.15976331360946747,"Theorem 2. Let (pθ, qφ) be an exponential family VAE (Assumption 1) on a discrete space X × Z
with encoder qφ(z|x) and decoder posterior pθ(z|x) both bounded from below by α > 0. If the
VAE is ε-tight, then there exists a matrix W ∈Rn,m and vectors u ∈Rn, v ∈Rm such that the joint
model implied by the decoder pθ(x, z) = pθ(x|z)p(z) can be approximated by an unnormalized EF
Harmonium"
THEORETICAL ANALYSIS,0.16272189349112426,"˜p(x, z) = h(x)h′(z) exp(⟨ν(x), Wψ(z)⟩+ ⟨ν(x), u⟩+ ⟨v, ψ(z)⟩+ c)
(8)"
THEORETICAL ANALYSIS,0.16568047337278108,with the error bound
THEORETICAL ANALYSIS,0.16863905325443787,"Epd(x)
h 
log pθ(x, z) −log ˜p(x, z)
2i
≤
ε
2α2 + o(ε)
∀z ∈Z.
(9)"
THEORETICAL ANALYSIS,0.17159763313609466,"The proof is given in Appendix A.3. In this theorem the function ˜p(x, z) is non-negative but does
not necessarily satisfy the normalization constraint of a density. Re-normalizing it by adjusting c
in (8) may break the approximation guarantee. Nevertheless, if ε is small enough and, e.g., the
data distribution is non-negative on the whole X, we expect it to approach a density, in particular to
recover the result in Theorem 1 in the limit. Note that the theorem does not make any assumptions
about optimality of (θ, φ), i.e., it describes all models in the vicinity of the consistent set in Fig. 1."
CASES ANALYSIS,0.17455621301775148,"3.1
CASES ANALYSIS"
CASES ANALYSIS,0.17751479289940827,"This subsection gives a detailed analysis of consistent VAE models in several concrete cases of
practical interest."
CASES ANALYSIS,0.1804733727810651,"Diagonal Gaussian VAE
Let us consider a Gaussian VAE, as commonly applied to image gen-
eration (e.g., Dai & Wipf (2019)).
Let X
= Rn, Z = Rm, p(x|z) = N(x|µd(z), σ2
dI),"
CASES ANALYSIS,0.1834319526627219,Published as a conference paper at ICLR 2022
CASES ANALYSIS,0.1863905325443787,"q(z|x) = N(z|µe(x), diag(σ2
e(x))), where µd, µe and σe are neural networks and σd is a common
pixel observation noise parameter. The decoder has minimal sufﬁcient statistics ν(x) = x and base
measure h(x) = N(x|0, σ2
dI). The encoder has minimal sufﬁcient statistics ψ(z) = (z, z2), where
the square is coordinate-wise. Theorem 1 implies that a tight optimal VAE has the joint model"
CASES ANALYSIS,0.1893491124260355,"p(x, z) ∝h(x) exp

x, Wz + V z2 + a

+ ⟨b, z⟩+

c, z2
(10)"
CASES ANALYSIS,0.19230769230769232,"for some matrices W, V and vectors a, b, c. Furthermore, the integral over z must be ﬁnite for all
x and therefore xTV + c < 0 must hold for all x ∈Rn. This is possible only if V = 0 and
c < 0. The joint distribution is therefore a multivariate Gaussian and the same holds for its marginal
p(x). The neural network µd(z) must degenerate to µd(z) = σ2
d · (Wz + a) and the two neural
networks for the encoder to σ2
e(x) = −1/2c and µe(x) = −(W Tx + b)/2c, where divisions are
coordinate-wise. VAEs with such simpliﬁed, linear Gaussian encoder-decoder pairs, called ""linear
VAEs"" (Lucas et al., 2019) are known to be consistent and to match the probabilistic PCA model (Dai
et al., 2018; Lucas et al., 2019). In this context, our Corollary 2 is a generalization of (Lucas et al.,
2019, Lemma 1) showing consistency of linear VAEs, to decoders in any exponential family with
natural parameters being a linear mapping of any ﬁxed lifted latent representation ψ(z)."
CASES ANALYSIS,0.1952662721893491,"We argue that a joint Gaussian model is too simplistic to generate complex data such as realistic
images and that in this case the VAE error is detrimental. In Section 4.2 we experimentally conﬁrm
that optimizing ELBO for a general decoder network µd causes qualitative and quantitative degrada-
tion relative to the ML decoder. Note that if we allowed σd to be dependent on z, the resulting joint
statistics in ν ⊗ψ would include terms x2z, x2z2. The joint distribution would not be Gaussian and
may be in fact multi-modal (see Anil Bhattacharayya’s distribution in Arnold et al. 2001)."
CASES ANALYSIS,0.19822485207100593,"Bernoulli-MRF VAE
Vahdat et al. (2020) proposed to consider encoders in the Markov Random
Field (MRF) family, in particular encoders of the form q(z|x) ∝exp(⟨z1, V (x)z2⟩+ ⟨b1(x), z1⟩+
⟨b2(x), z2⟩), where z1, z2 are two groups of latent variables and interaction weights V , b1, b2 are
computed by the encoder network. In this case q(z|x) is itself a (conditional) RBM. While evalu-
ating q(z|x) is difﬁcult, MCMC sampling is efﬁcient. We assume binary observations x and a con-
ditionally independent Bernoulli decoder family as above. The decoder thus has sufﬁcient statistics
ν = x and the encoder has ψ = (z1, z2, z1 ⊗z2). Introducing homogeneous constant components
x0 = z1
0 = z2
0 = 1, the family of consistent joint distributions can be compactly described as"
CASES ANALYSIS,0.20118343195266272,"p(x, z) = exp
P"
CASES ANALYSIS,0.20414201183431951,"i,j,k Wi,j,k xi z1
j z2
k

,
(11)"
CASES ANALYSIS,0.20710059171597633,"where the summation in all indices starts from 0 and −W0,0,0 is the log-partition function. This
joint model is a higher order MRF with the highest order potentials given by cubic monomials."
CASES ANALYSIS,0.21005917159763313,"Standard Bernoulli VAEs are a special case of the Bernoulli-MRF model, obtained when the inter-
action weights V are zero. The joint distribution of such tight optimal VAEs takes the form"
CASES ANALYSIS,0.21301775147928995,"p(x, z) = 1"
CASES ANALYSIS,0.21597633136094674,"c exp(xTWz + uTx + vTz),
(12)"
CASES ANALYSIS,0.21893491124260356,"which is a restricted Boltzmann machine (RBM). Since RBMs are well known for being useful in
many applications (dimensionality reduction, collaborative ﬁltering, feature learning, topic model-
ing), we hypothesize that they can make a good baseline for Bernoulli VAEs and furthermore that
the effect of pulling the VAE solution towards an RBM may be benign in case of insufﬁcient data.
For example IWAE test likelihood in (Burda et al., 2016) is worse than that of an RBM (Burda et al.,
2015) on the Omniglot dataset. Furthermore, debiasing of IWAE (Nowozin, 2018) does not improve
test likelihood in many cases."
CASES ANALYSIS,0.22189349112426035,"Bernoulli VAE for Semantic Hashing
One important application of Bernoulli VAEs is the seman-
tic hashing problem, initially proposed and modeled with RBMs (Salakhutdinov & Hinton, 2009).
The problem is to assign to each document / image a compact binary latent code that can be used
for quick retrieval by the nearest neighbor search. We will detail now a more recent VAE model for
text documents (Chaidaroon & Fang, 2017; Shen et al., 2018) and show that it can be tight only in a
full posterior collapse. We correct the encoder so as to allow a larger consistent set and observe that
the resulting consistent joint distribution forms a multinomial-Bernoulli RBM."
CASES ANALYSIS,0.22485207100591717,"Let x ∈NK be word counts in a document with words from a dictionary of size K. Let z ∈{0, 1}m
be a binary latent code. Let l = P"
CASES ANALYSIS,0.22781065088757396,k xk denote the document’s length. We assume that the document
CASES ANALYSIS,0.23076923076923078,Published as a conference paper at ICLR 2022
CASES ANALYSIS,0.23372781065088757,"length is independent of the latent topic and its distribution p(l) can be learned separately (e.g., a
log-normal distribution is a good ﬁt). The decoder is deﬁned using the multinomial distribution
model (words in the document are drawn from the same categorical distribution corresponding to its
topic):"
CASES ANALYSIS,0.23668639053254437,"p(x, l|z) = p(l)h(x|l) exp(f(z)Tx −lA(f(z))),
(13)"
CASES ANALYSIS,0.23964497041420119,"where f(z) is a neural network mapping the latent code to the logits of word occurrence probabil-
ities, A(η) = log P"
CASES ANALYSIS,0.24260355029585798,k exp(ηk) and h(x|l) = [[P
CASES ANALYSIS,0.2455621301775148,"k xk=l]]
 
l!/ Q"
CASES ANALYSIS,0.2485207100591716,"k xk!

is the base measure2. The
sufﬁcient statistics are the word counts x. The prior p(z) is assumed uniform Bernoulli."
CASES ANALYSIS,0.2514792899408284,"The encoder is the conditionally independent Bernoulli model, expressed as"
CASES ANALYSIS,0.25443786982248523,"q(z|x, l) ∝exp(g(x)Tz),
(14)"
CASES ANALYSIS,0.257396449704142,"where g(x) is the encoder network. Chaidaroon & Fang (2017) experimented with the encoder
and decoder design and recommended using TFIDF features instead of raw counts. First, we note
that the inverse document frequency (IDF) is not relevant, since it can be learned by the ﬁrst linear
transform in the encoder. Effectively, the term frequency (TF), given by x/l, is used. This choice
is adopted in later works (Shen et al., 2018; Zamani Dadaneh et al., 2020; Ñanculef et al., 2020). It
might seem reasonable that the latent code modeling the document topic should not depend on the
document length, only on the distribution of words in the document. However, we will argue that
this rationale is misleading for stochastic encoders."
CASES ANALYSIS,0.2603550295857988,"We apply Theorem 1 to two groups of variables: observed (x, l) and latent z with h(x, l) =
h(x|l)p(l), ν(x, l) = x and ψ(z) = z. It follows that the consistent joint family is"
CASES ANALYSIS,0.26331360946745563,"p(x, l, z) = h(x, l) exp(xTWz + aTx + bTz + c).
(15)"
CASES ANALYSIS,0.26627218934911245,"This however implies that g(x) = Wx + b, i.e. the encoder network must be linear in x. Conse-
quently, it cannot match a function of word frequencies x/l (as chosen by design) unless W = 0, i.e.
a completely trivial model with an encoder not depending on x. Such an encoder would imply full
posterior collapse. The corresponding consistent set ΘΦ coincides with the set of collapsed VAEs
where the decoder does not depend on the latent variable z in Fig. 1. We conjecture that the inherent
inconsistency of this VAE has a detrimental effect on learning."
CASES ANALYSIS,0.2692307692307692,"If instead, we let the encoder network to access word counts x directly, we obtain that g(x) =
Wx + b can form a consistent VAE. Inspecting this encoder model in more detail, we see that
it builds up topic conﬁdence in proportion to the evidence (total word counts), as the true pos-
terior would. Indeed, the true posterior p(z|x, l) satisﬁes the factorization by Bayes’s theorem:
p(z|x, l) = p(x|z, l)p(z)/p(x|l). The prior p(z) is constant by design, p(x|l) does not vary with z
and p(x|z, l) factors over all word instances according to (13). In other words, the coupling between
x and z in log p(z|x, l) is linear in x."
CASES ANALYSIS,0.27218934911242604,"In Section 4 we study the proposed correction experimentally and show that it enables learning better
models under a variety of settings."
EXPERIMENTS,0.27514792899408286,"4
EXPERIMENTS"
ARTIFICIAL EXAMPLE,0.2781065088757396,"4.1
ARTIFICIAL EXAMPLE"
ARTIFICIAL EXAMPLE,0.28106508875739644,"To start with, we illustrate our ﬁndings on a toy example. We consider a simple Gaussian mixture
model for which we can easily generate samples and compute all necessary quantities including
the ELBO objective. We deﬁne the ground truth model to be p∗(x, z) = p∗(z)p∗(x|z), with z ∈
{1 . . . 4}, p∗(z) ≡0.25, x ∈R2, p∗(x|z) = N(x|µ(z), σ2I), i.e. a mixture of four 2D Gaussians.
Fig. 2(a) shows the color-coded posterior distribution p∗(z|x). We assign a color to each component
and represent p∗(z|x) for each pixel x ∈R2 by the corresponding mixture of the component colors.
For better interpretability, we illustrate further results by decision maps arg maxz p(z|x). Fig. 2(b)
shows the decision map for p∗(z|x)."
ARTIFICIAL EXAMPLE,0.28402366863905326,"2Prior work (Chaidaroon & Fang, 2017; Shen et al., 2018) omits the base measure as it has no trainable
parameters."
ARTIFICIAL EXAMPLE,0.2869822485207101,"Published as a conference paper at ICLR 2022 11 10 01
00"
ARTIFICIAL EXAMPLE,0.28994082840236685,"(a)
(b)
(c)
(d)
(e)
(f)"
ARTIFICIAL EXAMPLE,0.29289940828402367,"Figure 2: Artiﬁcial example. (a) Color-coded posterior distribution p∗(z|x). (b-e): Decision maps
(arg maxz) of: (b) true posterior p∗(z|x), (c) factorized encoder qφ(z|x) after joint learning, (d)
model posterior pθ(z|x) after joint learning, (e) RBM trained on the same data. Gaussian centers
µ(z) are shown as black dots. (f) Probability simplex of distributions over the four binary conﬁg-
urations. The vertices correspond to pure (deterministic) binary states represented by the code and
its respective color. The surface shows the manifold of factorized distributions realizable by q(z|x).
Notice that the two edges (00, 11) and (01, 10) are not in the manifold because they correspond
to switching of two bits simultaneously in a correlated way. The factorized approximation cannot
model transitions between these states. Hence, when learning VAE, these pairs of states are repulsed
in the decision maps (c), (d)."
ARTIFICIAL EXAMPLE,0.2958579881656805,"The aim of the experiment is to learn a VAE and to study the inﬂuence of the factorization assump-
tion on the results. We use the decoder architecture as in the ground truth model — a Gaussian
distribution pθ(x|z) = N(x|θzoh, σ2I), where zoh is the one-hot (categorical) representation of z,
and θ is a 2×4 matrix that maps the four latent codes to 2D centers at general locations. Note that
the ground truth model is contained in the chosen decoder family. Hence, the ML solution is the
ground truth decoder p∗(x|z). We restrict the encoder to factor over the binary representation of the
code zb ∈{0, 1}2 and deﬁne it as qφ(zb|x) ∝exp ⟨gφ(x), zb⟩, where gφ(x) is implemented as a
feed-forward network with two hidden layers, each with 64 units and ReLU activations."
ARTIFICIAL EXAMPLE,0.2988165680473373,"First, we pre-train our factorized encoder by optimizing ELBO and keeping the ground truth decoder
ﬁxed. The next step is to jointly train the encoder and decoder by maximizing ELBO. Since we are
interested how the ELBO objective distorts the likelihood solution, we start with the ground truth
decoder and the pre-trained encoder from the previous step. The ELBO-optimal decoder has to
match not only the training data, but also the inexact, factorizing encoder. The resulting qφ(z|x) is
shown in Fig. 2(c) and the learned model posterior pθ(z|x) ∝p(z)pθ(x|z) in Fig. 2(d). Note that
they match each other pretty well, but differ substantially from the ground truth posterior shown in
Fig. 2(b). The impact of the factorization is clearly visible – one can see two decision boundaries
(one for each bit of zb), which together partition the x-space into four regions, approximating the
true posterior. For comparison, Fig. 2(e) shows the posterior of an RBM trained on the same data.
It is clearly seen that the ELBO optimizer is pulled away from the likelihood optimizer towards
an RBM solution. Notice also the explanation given in Fig. 2(f). Summarizing, this simple toy
example clearly shows the VAE approximation error caused by the combination of ELBO objective
and the factorization assumption for the encoder. While the numerical difference between ELBO
and log-likelihood is small (see details in Appendix C.1), the qualitative difference in Fig. 2 appears
substantial."
GAUSSIAN VAES FOR CELEBA IMAGES,0.30177514792899407,"4.2
GAUSSIAN VAES FOR CELEBA IMAGES"
GAUSSIAN VAES FOR CELEBA IMAGES,0.3047337278106509,"The goal and design of this experiment is similar to the previous one. We ﬁrst deﬁne a ground truth
decoder which is used to generate training images. Then we pre-train an encoder by ELBO keeping
the ground truth decoder ﬁxed. Finally, we train both model parts starting from the ground truth
decoder and the pre-trained encoder."
GAUSSIAN VAES FOR CELEBA IMAGES,0.3076923076923077,"The ground truth generative model is obtained by training a convolutional Generative Adversarial
network (GAN) using code of Inkawhich (2017) on the CelebA dataset Liu et al. (2015). We scale
and crop all images to 64×64 pixels. In order to get a stochastic decoder, we equip the GAN gener-
ator x = d(z), z ∈R100, x ∈R64×64×3 with image noise σd. The ground truth generative model is
thus deﬁned as p∗(x, z) = p∗(z)p∗(x|z), where p∗(z) = N(z|0, I), p∗(x|z) = N(x|µd(z), σ2
dI),
and σ2
d is a common noise variance for all pixels and color channels. We chose σd = 0.05 (the color
values are normalized to [−1, 1]). This corresponds to an image noise level, which is just visible,
but does not disturb visual perception essentially."
GAUSSIAN VAES FOR CELEBA IMAGES,0.3106508875739645,Published as a conference paper at ICLR 2022
GAUSSIAN VAES FOR CELEBA IMAGES,0.3136094674556213,"Figure 3: Results for the encoder learned by su-
pervised conditional likelihood. Top row: train-
ing samples ˆx ∼p∗(x). Second row: the cor-
responding reconstructions from mean values of
z, i.e. x ∼p∗(x|µe(ˆx)). Third row: reconstruc-
tions from sampled z, i.e. ˆz ∼N(µe(ˆx), σ2
e(ˆx))
followed by x ∼p∗(x| ˆz)."
GAUSSIAN VAES FOR CELEBA IMAGES,0.3165680473372781,"Figure 4:
Visual comparison – images drawn
from the original/learned models.
Each col-
umn corresponds to a particular value of z ∼
N(0, I). Top row: the ground truth model, mid-
dle row: learned decoder with ﬁxed σd, bottom
row: decoder with learned σd."
GAUSSIAN VAES FOR CELEBA IMAGES,0.31952662721893493,"The decoder family of the considered VAE consists of networks with the same architecture as d(z).
This ensures that the ground truth decoder is a likelihood maximizer of the VAE model. The encoder
is deﬁned as qφ(z|x) = N(x|µe(x), diag(σ2
e(x))), where µe, σe ∈R100 are two outputs of a
convolutional neural network with an architecture similar to the architecture of the discriminator
used for training the GAN (except the output layer), i.e., qφ(z|x) is a multivariate Gaussian with
diagonal covariance matrix whose parameters depend on x."
GAUSSIAN VAES FOR CELEBA IMAGES,0.3224852071005917,"We pre-train the encoder fully supervised by maximizing its conditional log-likelihood
Ep∗(x,z) log qφ(z|x) on examples drawn from the ground truth generating model p∗(x, z).3 The
results of pre-training are shown in Fig. 3. Then we jointly learn the encoder and decoder by max-
imizing ELBO on x-samples drawn from the ground truth model. We start the learning with the
ground truth decoder p∗(x|z) and the encoder obtained in the previous step. Two variants are con-
sidered for this training: (i) keeping the image noise σd ﬁxed and (ii) learning it along with other
model parameters. We evaluate the results quantitatively by computing the Frechet Inception Dis-
tances (FID) between the ground truth model p∗(x|z) and the obtained decoders pθ(x|z) using the
code of Seitzer (2020). For this we generate 200k images from each model. The obtained values
are given in Tab. 1. Fig. 4 shows images generated by the ground truth model and the two learned
models."
GAUSSIAN VAES FOR CELEBA IMAGES,0.3254437869822485,"To conclude, ELBO optimization harms the decoder considerably as clearly seen both from FID-
scores and the generated images. Models with higher ELBO values have worse FID-scores and
produce less realistic images."
GAUSSIAN VAES FOR CELEBA IMAGES,0.32840236686390534,"Table 1: Optimizing the ELBO starting from the ML solution degrades the FID-score. The ﬁrst
row corresponds to the pre-trained encoder for the ground truth decoder, its FID-score therefore
compares two image sets, both generated by the ground truth model."
GAUSSIAN VAES FOR CELEBA IMAGES,0.33136094674556216,"Experiment
ELBO
FID"
GAUSSIAN VAES FOR CELEBA IMAGES,0.3343195266272189,"optimize encoder (conditional likelihood)
-364513.78
0.13
optimize encoder and decoder (ELBO, ﬁxed σd)
-5898.94
77.10
optimize encoder and decoder (ELBO, learned σd)
9035.69
117.87"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.33727810650887574,"4.3
BERNOULLI VAE FOR TEXT DOCUMENTS"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.34023668639053256,"This experiment compares training of the VAE model for semantic hashing discussed in Section 3.1
with and without our proposed correction on the 20Newsgroups dataset (Lang & Rennie, 2008). We
describe the dataset, preprocessing and optimization details in Appendix C.2."
BERNOULLI VAE FOR TEXT DOCUMENTS,0.3431952662721893,"We compare three encoders: e1: linear encoder on word counts (the proposed correction), e2: deep
(2 hidden layers) encoder using word frequencies and e3: a linear encoder on frequencies. The"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.34615384615384615,"3In contrast to the previous experiment we optimize the “forward” KL-divergence, i.e. the conditional like-
lihood, instead of the “reverse” KL-divergence used in ELBO for simplicity."
BERNOULLI VAE FOR TEXT DOCUMENTS,0.34911242603550297,Published as a conference paper at ICLR 2022
BERNOULLI VAE FOR TEXT DOCUMENTS,0.3520710059171598,"encoders are compared across different numbers of latent Bernoulli variables (bits) and different
decoder depths. The decoder depth denotes the number of fully connected hidden ReLU layers (0-
2). In both the encoder and decoder we use 512 units in hidden layers. The prior work mainly used
linear decoders following the ablation study of Shen et al. (2018). Our experiments also suggest that
using deep decoders in combination with longer bit-length leads to a signiﬁcant overﬁtting. When
the decoder is linear, the posterior distribution is tractable and is linear as well, i.e., the VAE model
is equivalent to a Multinomial-Bernoulli RBM. We experimentally verify that a linear encoder on
word counts e1 indeed works better in this case. However, perhaps more surprisingly, we also ﬁnd
out that it works better even for non-linear decoders."
BERNOULLI VAE FOR TEXT DOCUMENTS,0.35502958579881655,"Table 2 show the achieved training and test Negative ELBO (NELBO) values. We observe across all
settings that the simple encoder e1 is consistently better than the more complex encoder e2 which
in turn is signiﬁcantly better than the linear encoder on frequencies e3. We conclude that the use
of VAEs with deep encoders based on word frequencies (Chaidaroon & Fang, 2017; Shen et al.,
2018; Zamani Dadaneh et al., 2020; Ñanculef et al., 2020) is sub-optimal for this dataset. We also
observe that linear decoders generalize better under 32 and 64 bits compared to more complex
decoders, which suffer from overﬁtting. This implies that in these cases the best encoder-decoder
combination is linear, i.e. the basic RBM model. This evidence agrees with previously observed
worse reconstruction error with deep architecture (Dai et al., 2020), however we did not observe (a
more severe) posterior collapse with deeper models amongst the depths we report."
BERNOULLI VAE FOR TEXT DOCUMENTS,0.35798816568047337,"Table 2: Training and test NELBO values for Text-VAE with different conﬁgurations of bits, de-
coder and encoder. Bold highlights the best encoder choice and underlined bold values are the best
decoder-encoder combinations for each bit-length."
BERNOULLI VAE FOR TEXT DOCUMENTS,0.3609467455621302,TRAINING
BERNOULLI VAE FOR TEXT DOCUMENTS,0.363905325443787,"Bits
dhidden=0
dhidden=1
dhidden=2"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.3668639053254438,"e1
e2
e3
e1
e2
e3
e1
e2
e3"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.3698224852071006,"8
419 429 439 321 390 415 325 370 421
16
382 398 419 201 329 407 164 269 413
32
337 358 412 165 189 403 132 159 411
64
296 324 407 171 189 398 134 149 409 TEST"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.3727810650887574,"Bits
dhidden=0
dhidden=1
dhidden=2"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.3757396449704142,"e1
e2
e3
e1
e2
e3
e1
e2
e3"
BERNOULLI VAE FOR TEXT DOCUMENTS,0.378698224852071,"8
423 429 435 413 421 424 418 423 427
16
409 417 421 404 422 420 410 416 422
32
396 413 416 399 413 418 406 416 421
64
392 411 414 398 413 417 406 417 422"
CONCLUSIONS,0.3816568047337278,"5
CONCLUSIONS"
CONCLUSIONS,0.38461538461538464,"We have analyzed the approximation error of VAEs in a general setting, when both the decoder and
encoder are exponential families. This includes commonly used VAE variants as, e.g., Gaussian
VAEs and Bernoulli VAEs. We have shown that the subset of generative models consistent with the
encoder class is quite restricted: it coincides with the set of log-bilinear models on the sufﬁcient
statistics of both decoder and encoder, i.e., RBM-like models. This consistent subset can not be
enlarged by using more complex encoder networks as long as encoder’s sufﬁcient statistics remain
unchanged. In combination with the ELBO objective, this causes an approximation error — the
ELBO optimizer is pulled away from the data likelihood optimizer towards this subset. Moreover,
we proved theoretically that close-to-tight EF VAEs must be close to RBMs in a certain sense."
CONCLUSIONS,0.3875739644970414,"We have shown that the error is detrimental when the consistent subset is too restrictive. In the
cases where a lot of data is available and a high quality generative model is of the primary interest,
such as in the CelebA experiment, more expressive encoder families are required in addition to large
networks. On the other hand the VAE approximation error may result in a useful regularization when
the respective RBM is a good baseline model. In this case we can speak of a binning inductive bias
towards RBM, such as in our text-VAE experiment. Furthermore, simple encoders can be desired
when the learned representations are of interest, in particular they appear to facilitate similarity in
Hamming distance, useful in the semantic hashing problem."
CONCLUSIONS,0.3905325443786982,Further connections to related work and discussion can be found in Appendix B.
CONCLUSIONS,0.39349112426035504,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.39644970414201186,ACKNOWLEDGMENT
CONCLUSIONS,0.3994082840236686,"D.S. was supported by the German Federal Ministry of Education and Research (BMBF,
01/S18026A-F) by funding the competence center for Big Data and AI “ScaDS.AI Dres-
den/Leipzig”. A.S and B.F gratefully acknowledge support by the Czech OP VVV project ”Re-
search Center for Informatics” (CZ.02.1.01/0.0/0.0/16019/0000765)"". B.F. was also supported by
the Czech Science Foundation, grant 19-09967S. The authors gratefully acknowledge the Center for
Information Services and HPC (ZIH) at TU Dresden for providing computing time. We thank the
anonymous reviewers for many helpful links and suggestions."
REFERENCES,0.40236686390532544,REFERENCES
REFERENCES,0.40532544378698226,"B. C. Arnold and D. J. Strauss. Bivariate distributions with conditionals in prescribed exponential
families. Journal of the Royal Statistical Society Series B (Methodological), 53(2):365–375, 1991."
REFERENCES,0.40828402366863903,"Barry C. Arnold, Enrique Castillo, and Jose Maria Sarabia. Conditionally speciﬁed distributions:
An introduction. Statistical Science, 16(3):249 – 274, 2001."
REFERENCES,0.41124260355029585,"Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of MRF
log-likelihood using reverse annealing. In AISTATS, volume 38, pp. 102–110, 09–12 May 2015."
REFERENCES,0.41420118343195267,"Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
ICLR, 2016."
REFERENCES,0.4171597633136095,"Suthee Chaidaroon and Yi Fang. Variational deep semantic hashing for text documents. In SIGIR
Conference on Research and Development in Information Retrieval, pp. 75–84, 2017."
REFERENCES,0.42011834319526625,"Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. In ICML, volume 80, pp. 1078–1086, 2018."
REFERENCES,0.4230769230769231,"Bin Dai and David Wipf. Diagnosing and enhancing VAE models. In ICLR, 2019."
REFERENCES,0.4260355029585799,"Bin Dai, Yu Wang, John Aston, Gang Hua, and David Wipf. Connections with robust PCA and
the role of emergent sparsity in variational autoencoder models. Journal of Machine Learning
Research, 19(41):1–42, 2018."
REFERENCES,0.4289940828402367,"Bin Dai, Ziyu Wang, and David Wipf. The usual suspects? Reassessing blame for VAE posterior
collapse. In ICML, 2020."
REFERENCES,0.4319526627218935,"Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. In ICLR, 2019."
REFERENCES,0.4349112426035503,"Devon Hjelm, Russ R Salakhutdinov, Kyunghyun Cho, Nebojsa Jojic, Vince Calhoun, and Junyoung
Chung. Iterative reﬁnement of the approximate posterior for directed belief networks. In Advances
in Neural Information Processing Systems, volume 29, pp. 4691–4699, 2016."
REFERENCES,0.4378698224852071,"Nathan Inkawhich.
DCGAN tutorial, 2017.
URL https://pytorch.org/tutorials/
beginner/dcgan_faces_tutorial.html."
REFERENCES,0.4408284023668639,"Yoon Kim, Sam Wiseman, Andrew Miller, David Sontag, and Alexander Rush. Semi-amortized
variational autoencoders. In ICML, volume 80, pp. 2678–2687, 2018."
REFERENCES,0.4437869822485207,"Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014."
REFERENCES,0.4467455621301775,"Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling.
Semi-supervised
learning with deep generative models. In Proceedings of the 27th International Conference on
Neural Information Processing Systems - Volume 2, NIPS’14, pp. 3581–3589, Cambridge, MA,
USA, 2014. MIT Press."
REFERENCES,0.44970414201183434,"Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive ﬂow. In NeurIPS, volume 29, pp.
4743–4751, 2016."
REFERENCES,0.4526627218934911,Published as a conference paper at ICLR 2022
REFERENCES,0.4556213017751479,"Ken Lang and Jason Rennie. The 20 newsgroups dataset, 2008. http://qwone.com/~jason/
20Newsgroups/."
REFERENCES,0.45857988165680474,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.46153846153846156,"James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don’t blame the ELBO! a
linear VAE perspective on posterior collapse. In NeurIPS, volume 32, pp. 9408–9418, 2019."
REFERENCES,0.46449704142011833,"Lars Maalø e, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy of
latent variables for generative modeling. In NeurIPS, volume 32, 2019."
REFERENCES,0.46745562130177515,"Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: Deep generative modelling and imputation of
incomplete data sets. In ICML, volume 97, pp. 4413–4423, 09–15 Jun 2019."
REFERENCES,0.47041420118343197,"Ricardo Ñanculef, Francisco Alejandro Mena, Antonio Macaluso, Stefano Lodi, and Clau-
dio Sartori.
Self-supervised Bernoulli autoencoders for semi-supervised hashing.
CoRR,
abs/2007.08799, 2020."
REFERENCES,0.47337278106508873,"Sebastian Nowozin. Debiasing evidence approximations: On importance-weighted autoencoders
and jackknife variational inference. In International Conference on Learning Representations,
2018."
REFERENCES,0.47633136094674555,"Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In ICML, vol-
ume 48, pp. 324–333, 20–22 Jun 2016."
REFERENCES,0.47928994082840237,"Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In ICML,
volume 37, pp. 1530–1538, 2015."
REFERENCES,0.4822485207100592,"Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In ICML, 2014."
REFERENCES,0.48520710059171596,"Ruslan Salakhutdinov and Geoffrey Hinton. Semantic hashing. Int. J. Approx. Reasoning, 50(7):
969–978, July 2009."
REFERENCES,0.4881656804733728,"Maximilian Seitzer. pytorch-ﬁd: FID Score for PyTorch. https://github.com/mseitzer/
pytorch-fid, August 2020. Version 0.1.1."
REFERENCES,0.4911242603550296,"Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa, Wenlin Wang, Guoyin Wang, Ricardo Henao,
and Lawrence Carin. NASH: Toward end-to-end neural architecture for generative semantic hash-
ing. In Annual Meeting of the Association for Computational Linguistics, pp. 2041–2050, jul
2018."
REFERENCES,0.4940828402366864,"Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon.
Amortized
inference regularization. In NeurIPS, volume 31, 2018."
REFERENCES,0.4970414201183432,"Robert Sicks, Ralf Korn, and Stefanie Schwaar.
A generalised linear model framework for β-
variational autoencoders based on exponential dispersion families. CoRR, 2021."
REFERENCES,0.5,"Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder
variational autoencoders. In Advances in Neural Information Processing Systems, volume 29, pp.
3738–3746, 2016."
REFERENCES,0.5029585798816568,"Hiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada, and Satoshi Yagi. Student-
t variational autoencoder for robust density estimation. In IJCAI, pp. 2696–2702, 7 2018."
REFERENCES,0.5059171597633136,"R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series
models. In Bayesian Time series models, chapter 5, pp. 109–130. 2011."
REFERENCES,0.5088757396449705,"Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. In NeurIPS,
volume 33, pp. 19667–19679, 2020."
REFERENCES,0.5118343195266272,"Arash Vahdat, Evgeny Andriyash, and William Macready. Undirected graphical models as approxi-
mate posteriors. In ICML, volume 119, pp. 9680–9689, 13–18 Jul 2020."
REFERENCES,0.514792899408284,Published as a conference paper at ICLR 2022
REFERENCES,0.5177514792899408,"Max Welling, Michal Rosen-Zvi, and Geoffrey E Hinton. Exponential family harmoniums with an
application to information retrieval. In NeurIPS, volume 17, 2005."
REFERENCES,0.5207100591715976,"Mingzhang Yin and Mingyuan Zhou. ARM: Augment-REINFORCE-merge gradient for stochastic
binary networks. In ICLR, 2019."
REFERENCES,0.5236686390532544,"Siamak Zamani Dadaneh, Shahin Boluki, Mingzhang Yin, Mingyuan Zhou, and Xiaoning Qian.
Pairwise supervised hashing with Bernoulli variational auto-encoder and self-control gradient
estimator. In UAI, volume 124, pp. 540–549, 2020."
REFERENCES,0.5266272189349113,Published as a conference paper at ICLR 2022
REFERENCES,0.5295857988165681,Appendix
REFERENCES,0.5325443786982249,"A
PROOFS"
REFERENCES,0.5355029585798816,"A.1
PROOF OF THEOREM 1"
REFERENCES,0.5384615384615384,"The proof directly follows from the characterization of conditionally speciﬁed joint distributions in
the exponential family given by Arnold & Strauss (1991), see (Arnold et al., 2001, Theorem 3):"
REFERENCES,0.5414201183431953,"Theorem A.1 (Arnold & Strauss 1991). Let x ∈X and z ∈Z be random variables with a strictly
positive joint distribution such that both conditional distributions are exponential families with den-
sities"
REFERENCES,0.5443786982248521,"p(x|z) = h(x) exp

⟨ν(x), f(z)⟩−A(z)]
(16a)"
REFERENCES,0.5473372781065089,"p(z|x) = h′(z) exp

⟨ψ(z), g(x)⟩−B(x)],
(16b)"
REFERENCES,0.5502958579881657,"where ν : X →Rn and ψ : Z →Rm are minimal sufﬁcient statistics, f : Z →Rn and
g : X →Rm are any mappings, h(x) and h′(z) are base measures, and A and B denote the
respective log-partition functions4."
REFERENCES,0.5532544378698225,"Then there exists a matrix W ∈M(n + 1, m + 1), such that the density of the joint distribution can
be represented as"
REFERENCES,0.5562130177514792,"p(x, z) = h(x)h′(z) exp ⟨νe(x), Wψe(z)⟩,
(17)"
REFERENCES,0.5591715976331361,"where νe, ψe denote the statistics vectors extended with an additional component 1."
REFERENCES,0.5621301775147929,"A.2
EXAMPLE OF A DISCRETE VAE APPROACHING A FLOW"
REFERENCES,0.5650887573964497,"Example A.1. In this example we construct a VAE that can be arbitrary close to tight one, but where
the decoder network does not approach a linear map. Let X = {−1, 1}2, Z = {−1, 1}2 and let
p(z) be uniform. Let the decoder be conditionally independent"
REFERENCES,0.5680473372781065,"p(x|z) = exp

β ⟨x, π(z)⟩−A(z)

(18)"
REFERENCES,0.5710059171597633,where π(z) denotes the invertible mapping of Z to X given by
REFERENCES,0.5739644970414202,"x1 = z1
(19)
x2 = z1z2.
(20)"
REFERENCES,0.5769230769230769,"If the parameter β is sufﬁciently large, the distribution p(x|z) approaches the deterministic dis-
tribution δx=π(z). Its posterior therefore also approaches the deterministic distribution δz=π−1(x)
(notice that π−1 = π).
Let the encoder be the conditionally independent model q(z|x) =
exp

β⟨z, π−1(x)⟩−A(x)

. This decoder by design approaches δx=π(z) as well and thus the VAE
(p, q) achieves ε-tightness for sufﬁciently large β. At the same time the deviation between loga-
rithms of probabilities log q(z|x) and log p(z|x) grows with β."
REFERENCES,0.5798816568047337,"A.3
PROOF OF THEOREM 2"
REFERENCES,0.5828402366863905,"The idea of the proof is to bound the difference between log p(z|x) and log q(z|x), which is done
by Proposition A.2 and then in the space of log-probabilities to approximate the non-linear mapping
fe(z) by a linear one as detailed in Proposition A.1. By carefully choosing the norms and the
approximation we obtain a bound on the error for the joint model, which despite the discreteness
assumption of the observation space X in Theorem 2 does not depend on its cardinality."
REFERENCES,0.5857988165680473,"For a ﬁnite set X ⊂X let H be the |X|-dimensional vector space with the inner product ⟨u, v⟩pd =
P"
REFERENCES,0.5887573964497042,"x∈X pd(x)u(x)v(x), assuming that pd(x) > 0 for all x ∈X. The respective norm will be denoted
as ∥· ∥pd."
REFERENCES,0.591715976331361,"4The log-partition function is usually deﬁned as a function of the (natural) parameter. In this context we
consider h,h′, f, g to be ﬁxed and consider the dependence on x, z only."
REFERENCES,0.5946745562130178,Published as a conference paper at ICLR 2022
REFERENCES,0.5976331360946746,"Proposition A.1. Under model Assumption 1, for any ﬁnite X ⊆X there exists a matrix W ∈
M(n + 1, m + 1) such that joint distribution implied by the decoder p(x, z) = p(x|z)p(z) can be
approximated by an unnormalized EF Harmonium"
REFERENCES,0.6005917159763313,"˜p(x, z) = h(x)h′(z) exp(⟨νe(x), Wψe(z)⟩)
(21)"
REFERENCES,0.6035502958579881,with the error bound
REFERENCES,0.606508875739645,(∀z) P
REFERENCES,0.6094674556213018,"x∈X pd(x)
 log p(x, z) −log ˜p(x, z)
2 ≤P"
REFERENCES,0.6124260355029586,"x∈X pd(x)
 log q(z|x) −log p(z|x)
2. (22)"
REFERENCES,0.6153846153846154,"The function ˜p(x, z) is non-negative but does not necessarily satisfy the normalization constraint of
a density."
REFERENCES,0.6183431952662722,"Proof. For clarity, we will omit the dependence of the decoder and encoder on their parameters θ,
resp. φ. Throughout the proof we will also assume that a single z ∈Z is ﬁxed.
First, we expand"
REFERENCES,0.621301775147929,"log q(z|x) = ⟨ψ(z), g(x)⟩−B(x) + log h′(z);
(23a)
log p(z|x) = log p(x|z) + log p(z) −log p(x)
= ⟨ν(x), f(z)⟩−A(z) + log h(x) + log p(z) −log p(x),
(23b)"
REFERENCES,0.6242603550295858,where A(z) = A(f(z)) and B(x) = B(g(x)). We can therefore represent
REFERENCES,0.6272189349112426,"log q(z|x) −log p(z|x) = ⟨ψ(z), g(x)⟩−B(x) + log p(x) −log h(x)
(24)"
REFERENCES,0.6301775147928994,"−

⟨ν(x), f(z)⟩+ log p(z) −log h′(z) −A(z)

(25)"
REFERENCES,0.6331360946745562,"= ⟨ψe(z), ge(x)⟩−⟨νe(x), fe(z)⟩,
(26) where"
REFERENCES,0.636094674556213,"ψe(z) = (ψ(z), 1);
(27)
νe(x) = (ν(x), 1);
(28)
ge(x) = (g(x), log p(x) −B(x) −log h(x));
(29)
fe(z) = (f(z), log p(z) −A(z) −log h′(z)).
(30)"
REFERENCES,0.6390532544378699,"With this representation we have:
P"
REFERENCES,0.6420118343195266,"x∈X pd(x)
 ⟨νe(x), fe(z)⟩−⟨ψe(z), ge(x)⟩
2
(31) = P"
REFERENCES,0.6449704142011834,"x∈X pd(x)
 log q(z|x) −log p(z|x)
2 =: ∆2.
(32)"
REFERENCES,0.6479289940828402,"Let V be the matrix with rows νe(x) for all x ∈X. Let G be the matrix with rows ge(x) for all
x ∈X. We can rewrite the condition (31) in the form"
REFERENCES,0.650887573964497,"ξ = V fe(z) −Gψe(z),
(33a)"
REFERENCES,0.6538461538461539,"∥ξ∥2
pd = ∆2,
(33b)"
REFERENCES,0.6568047337278107,"where ξ ∈R|X| is the vector of residuals. Let P be the orthogonal projection onto the range of V in
the space H. Multiplying (33a) by P on the left, we obtain"
REFERENCES,0.6597633136094675,"PV fe(z) −PGψe(z) = Pξ.
(34)"
REFERENCES,0.6627218934911243,Because PV = V we obtain
REFERENCES,0.665680473372781,"V fe(z) −PGψe(z) = Pξ.
(35)"
REFERENCES,0.6686390532544378,"We therefore can consider the decoder network approximation ˜fe(z) = ˜Wψe(z), where ˜W is the
solution to the consistent system of linear equations V ˜W = PG and is independent of z. We can
therefore express"
REFERENCES,0.6715976331360947,"∥V fe(z) −V ¯fe(z)∥pd = ∥Pξ∥pd ≤∥ξ∥pd = ∆,
(36)"
REFERENCES,0.6745562130177515,where the inequality holds because P is an orthogonal projection in H.
REFERENCES,0.6775147928994083,Published as a conference paper at ICLR 2022
REFERENCES,0.6804733727810651,"We obtained that the decoder network fe(z) can be approximated by a linear mapping ˜Wψe(z) such
that
P"
REFERENCES,0.6834319526627219,"x∈X pd(x)
⟨νe(x), fe(z)⟩−⟨νe(x), ˜Wψe(z)⟩
2 ≤∆2.
(37)"
REFERENCES,0.6863905325443787,Expressing back
REFERENCES,0.6893491124260355,"⟨νe(x), fe(z)⟩= ⟨ν(x), f(z)⟩+ log p(z) −A(z) −log h′(z)
(38a)
= log p(x|z) + log p(z) −log h(x) −log h′(z)
(38b)
= log p(x, z) −log h(x) −log h′(z)
(38c)"
REFERENCES,0.6923076923076923,"we obtain that
P"
REFERENCES,0.6952662721893491,"x∈X pd(x)
 log p(x, z) −log ˜p(x, z)
2 ≤∆2,
(39) where"
REFERENCES,0.6982248520710059,"log ˜p(x, z) = log h(x) + log h′(z) + ⟨νe(x), Wψe(z)⟩.
(40)"
REFERENCES,0.7011834319526628,"Proposition A.2. Under model Assumption 1, let X and Z be discrete (ﬁnite) sets and let z ∈Z be
chosen. If q(z|x) ≥α and p(z|x) ≥α for all x ∈X, where X ⊆X and"
REFERENCES,0.7041420118343196,"Epd(x)[DKL(q(z|x) ∥p(z|x))] ≤ε,
(41)"
REFERENCES,0.7071005917159763,"then
P"
REFERENCES,0.7100591715976331,"x∈X pd(x)
 
log p(z|x) −log q(z|x)
2 ≤
ε
α2 + o(ε).
(42)"
REFERENCES,0.7130177514792899,Proof. Let us denote ε(x) = DKL(q(z|x) ∥p(z|x)). Pinsker’s inequality assures for each x
REFERENCES,0.7159763313609467,"sup
S⊂Z
|Pq(z| x)(S) −Pp(z| x)(S)|2 ≤ε(x)/2.
(43)"
REFERENCES,0.7189349112426036,Substituting S = {z} we obtain
REFERENCES,0.7218934911242604,"|p(z|x) −q(z|x)|2 ≤ε(x)/2.
(44)"
REFERENCES,0.7248520710059172,By taking expectation in pd(x) on both sides we obtain a variant of Pinsker’s inequality:
REFERENCES,0.727810650887574,Epd(x)|p(z|x) −q(z|x)|2 ≤Epd(x)ε(x)/2 = 1
REFERENCES,0.7307692307692307,"2Epd(x)[DKL(q(z|x) ∥p(z|x))] ≤ε/2.
(45)"
REFERENCES,0.7337278106508875,Notice that the LHS depends on the given z. Because all summands are non-negative it follows that
REFERENCES,0.7366863905325444,"∀X′ ⊆X
P"
REFERENCES,0.7396449704142012,"x∈X′ pd(x)|p(z|x) −q(z|x)|2 ≤ε/2.
(46)"
REFERENCES,0.742603550295858,This inequality will be used in several places below.
REFERENCES,0.7455621301775148,Consider x ∈X such that p(z|x) > q(z|x). Then
REFERENCES,0.7485207100591716,| log p(z|x) −log q(z|x)|2 = log2 p(z| x)
REFERENCES,0.7514792899408284,"q(z| x)
(47a)"
REFERENCES,0.7544378698224852,= log2(1 + p(z| x)−q(z| x)
REFERENCES,0.757396449704142,"q(z| x)
)
(47b)"
REFERENCES,0.7603550295857988,≤log2(1 + p(z| x)−q(z| x)
REFERENCES,0.7633136094674556,"α
),
(47c)"
REFERENCES,0.7662721893491125,"where the inequality holds because log2 is monotonously increasing for arguments greater equal
than 1, which is ensured. Let us now consider x ∈X such that p(z|x) < q(z|x). Then"
REFERENCES,0.7692307692307693,| log p(z|x) −log q(z|x)|2 = log2 q(z| x)
REFERENCES,0.772189349112426,"p(z| x)
(48a)"
REFERENCES,0.7751479289940828,= log2(1 + q(z| x)−p(z| x)
REFERENCES,0.7781065088757396,"p(z| x)
)
(48b)"
REFERENCES,0.7810650887573964,≤log2(1 + q(z| x)−p(z| x)
REFERENCES,0.7840236686390533,"α
).
(48c)"
REFERENCES,0.7869822485207101,"In total, we obtain"
REFERENCES,0.7899408284023669,| log p(z|x) −log q(z|x)|2 ≤log2(1 + |p(z| x)−q(z| x)|
REFERENCES,0.7928994082840237,"α
).
(49)"
REFERENCES,0.7958579881656804,Published as a conference paper at ICLR 2022
REFERENCES,0.7988165680473372,Let us denote u(x) = |p(z| x)−q(z| x)|
REFERENCES,0.8017751479289941,"α
and partition the set X into two parts:"
REFERENCES,0.8047337278106509,"X1 = {x ∈X | u(x) < u0}
(50a)
X2 = {x ∈X | u(x) ≥u0},
(50b)"
REFERENCES,0.8076923076923077,"where we chose u0 ∈[√e −1, 5] for reasons to be clariﬁed below. For both parts, i.e. k = 1, 2, we
have
P"
REFERENCES,0.8106508875739645,x∈Xk pd(x)| log q(z|x) −log p(z|x)|2 ≤P
REFERENCES,0.8136094674556213,x∈Xk pd(x) log2(1 + |p(z| x)−q(z| x)|
REFERENCES,0.8165680473372781,"α
).
(51)"
REFERENCES,0.8195266272189349,"For X1, (51) can be further bounded as ≤P"
REFERENCES,0.8224852071005917,x∈X1 pd(x) log(1 + |p(z| x)−q(z| x)|2
REFERENCES,0.8254437869822485,"α2
)
(52a)"
REFERENCES,0.8284023668639053,≤log P
REFERENCES,0.8313609467455622,"x∈X1 pd(x)

1 + |p(z| x)−q(z| x)|2"
REFERENCES,0.834319526627219,"α2

(52b)"
REFERENCES,0.8372781065088757,"= log

pd(X1) +
1
α2
P
x∈X1 pd(x)|p(z|x) −q(z|x)|2
(52c)"
REFERENCES,0.8402366863905325,"≤log

1 +
ε
2α2

=
ε
2α2 + o(ε),
(52d)"
REFERENCES,0.8431952662721893,"where the ﬁrst inequality holds for u0 ≤5, because in this case log2(1+u) ≤log(1+u2) holds, the
second inequality is the Jensen’s inequality for log and the last inequality uses (46) under monotone
log."
REFERENCES,0.8461538461538461,"For X2 we have the following. Let V = pd(X2) = P
x∈X2 pd(x). We can express P"
REFERENCES,0.849112426035503,"x∈X2 pd(x) log2 
1 + |p(z| x)−q(z| x)|"
REFERENCES,0.8520710059171598,"α

(53a)"
REFERENCES,0.8550295857988166,"=V
 P"
REFERENCES,0.8579881656804734,"x∈X2 pd(x|X2) log2 
1 + |p(z| x)−q(z| x)|"
REFERENCES,0.8609467455621301,"α

(53b)"
REFERENCES,0.863905325443787,"Using that u < u2 on X2 and that log2(1 + u) is monotone for a positive argument, we can
bound (53b) as"
REFERENCES,0.8668639053254438,"≤V
 P"
REFERENCES,0.8698224852071006,"x∈X2 pd(x|X2) log2 
1 + |p(z| x)−q(z| x)|2"
REFERENCES,0.8727810650887574,"α2

.
(54)"
REFERENCES,0.8757396449704142,"Further, using that log2(1 + v) is concave on X2 for v ≥e −1, we have"
REFERENCES,0.878698224852071,"≤V log2  P
x∈X2 pd(x|X2)
 
1 + |p(z| x)−q(z| x)|2"
REFERENCES,0.8816568047337278,"α2

(55a)"
REFERENCES,0.8846153846153846,"= V log2 
1 + P
x∈X2 pd(x|X2) |p(z| x)−q(z| x)|2"
REFERENCES,0.8875739644970414,"α2

(55b)"
REFERENCES,0.8905325443786982,"≤V log2 
1 +
ε
2α2V

,
(55c)"
REFERENCES,0.893491124260355,where in the last step we used (46).
REFERENCES,0.8964497041420119,"Next we show that V itself is bounded above by
ε
2u2
0α2 . It follows from"
REFERENCES,0.8994082840236687,"u2
0V = P"
REFERENCES,0.9023668639053254,"x∈X2
pd(x)u2
0 ≤P"
REFERENCES,0.9053254437869822,"x∈X2
pd(x) |p(z| x)−q(z| x)|2 α2
≤P"
REFERENCES,0.908284023668639,"x∈X
pd(x) |p(z| x)−q(z| x)|2"
REFERENCES,0.9112426035502958,"α2
≤
ε
2α2 ,
(56)"
REFERENCES,0.9142011834319527,where the last inequality is again (46).
REFERENCES,0.9171597633136095,"Now, letting r =
ε
2α2V ≥u2
0 ≥1 we can bound (55c) as follows"
REFERENCES,0.9201183431952663,"V log2 
1 +
ε
2α2V

=
ε
2α2 1"
REFERENCES,0.9230769230769231,"r log2 
1 + r

(57a)"
REFERENCES,0.9260355029585798,"≤
ε
2α2 supr≥1
1
r log2 
1 + r

≤
ε
2α2 0.64.
(57b)"
REFERENCES,0.9289940828402367,Theorem 2 follows by Proposition A.2 and Proposition A.1 with the choice X = X.
REFERENCES,0.9319526627218935,Published as a conference paper at ICLR 2022
REFERENCES,0.9349112426035503,"B
DISCUSSION"
REFERENCES,0.9378698224852071,In this section we discuss further connections to related work and some open questions.
REFERENCES,0.9408284023668639,"Cremer et al. (2018) showed experimentally that using a more expressive class of models for the
encoder reduces not only the posterior family mismatch but also the amortization error. This obser-
vation is compatible with our results: increasing the expressive power of the encoder admits tight
VAEs with more complex dependence of z on x. Indeed, increasing the expressive power of the
encoder in our setting means extending its sufﬁcient statistics ψ(z) by new components. While it
keeps the simple linear dependence g(x) = W Tν(x) characterizing tight VAEs, it does lead to a
more expressive GLM p(z|x). Conversely, our results suggest that increasing the complexity of the
encoder network in order to reduce the amortization gap is only useful for models that are far from
the consistent set. Furthermore, there could be negative impact from increasing the model depth in
practice: (Dai et al., 2020) demonstrated that the risk of the learning converging to a suboptimal
solution (in particular leading to more collapsed latent dimensions) increases with decoder depth."
REFERENCES,0.9437869822485208,"Lucas et al. (2019) showed that any spurious local minima in linear Gaussian VAEs are entirely due
to the marginal log likelihood and that the ELBO does not introduce any new local minima. It is
a good question5 whether something similar can be said about the EF VAE generalization. To our
best knowledge this is not straightforward in such a general setting. The result of Lucas et al. (2019)
is possible thanks to the fact that for linear Gaussian models ELBO is analytically tractable and
its stationary point conditions can be written down and analyzed. In the general EF setup, which
includes, e.g., MRF VAEs, this does not appear possible. On the other hand, for any consistent
EF VAE, the decoder posterior must be in the EF of the encoder. Therefore the optimal encoder
could be easier to ﬁnd analytically or numerically using forward KL divergence and not the reverse
KL divergence used in ELBO, thus circumventing the question about local optima of ELBO. Since
ELBO at the optimal encoder is tight for a consistent VAE, this could be an alternative way to ﬁnd
the global maximum."
REFERENCES,0.9467455621301775,"A recent work by Sicks et al. (2021) extends the result of Lucas Lucas et al. (2019) in that they
develop an analytical local approximation to ELBO, which is exact in the Gaussian linear model
case and is a lower bound on ELBO for Binomial observation model. These results allow to ana-
lyze ELBO (and in particular the posterior collapse problem) locally under the assumption that the
decoder’s mapping f(z) is (locally) an afﬁne mapping of z. Our Theorem 1 implies it must be so
globally for tight VAEs in several special cases (e.g., Bernoulli model), while in general it is an afﬁne
mapping of ψ(z). These connections indicate that a better understanding of VAEs can be reached in
the setting where either decoder or encoder or both are consistent with the joint model (7)."
REFERENCES,0.9497041420118343,"We restricted this study to exponential families. While in richer models discussed in the introduction,
the approximation error still exists, it is made small by design, and it is less relevant and harder to
analyze it theoretically. One possible open direction where such analysis would make sense is to
consider fully factorized non-exponential cases, e.g., Student-t VAEs (Takahashi et al., 2018) or
models satisfying hierarchical or partial factorization (Maalø e et al., 2019)."
REFERENCES,0.9526627218934911,"C
DETAILS OF EXPERIMENTAL SETUP"
REFERENCES,0.9556213017751479,"C.1
ARTIFICIAL EXAMPLE"
REFERENCES,0.9585798816568047,"We give here the achieved likelihood and ELBO values for this experiment. The negative entropy
P"
REFERENCES,0.9615384615384616,"x p∗(x) log p∗(x) of the ground truth model, i.e., the best reachable data log-likelihood, is −3.65.
The ELBO of the pre-trained VAE is −3.74. During the second step of training, i.e., the joint
learning of the encoder and decoder by ELBO maximization, the ELBO value increases from −3.74
to −3.70. The data log-likelihood P"
REFERENCES,0.9644970414201184,"x p∗(x) log pθ(x) drops at the same time from −3.65 to −3.68.
The approximation error (2) in the decoder caused by using the factorized encoder is only 0.03 nats,
but the qualitative difference between the ground truth model and the ELBO optimizer model shown
in Fig. 2 appears detrimental."
POINTED OUT BY REVIEWERS,0.9674556213017751,5pointed out by reviewers
POINTED OUT BY REVIEWERS,0.9704142011834319,Published as a conference paper at ICLR 2022
POINTED OUT BY REVIEWERS,0.9733727810650887,"C.2
BERNOULLI VAE FOR TEXT DOCUMENTS"
POINTED OUT BY REVIEWERS,0.9763313609467456,"Dataset
In this experiment we used the version of the 20Newsgroups data set (Lang & Rennie,
2008) denoted as “processed” by the authors. The dataset contains bag-of-words representations of
documents and is split into a training set with 11269 documents and a test set with 7505 documents.
We keep only the 10000 most frequent words in the training set, which is a common pre-processing
(each of the omitted words occurs not more than in 10 documents)."
POINTED OUT BY REVIEWERS,0.9792899408284024,"Optimization
To train VAE we used the state-of-the-art unbiased gradient estimator ARM (Yin &
Zhou, 2019) and Adam optimizer with learning rate 0.001. We did not use the test set for parameter
selection. We train for 1000 epochs using 1-sample ARM and then for 500 more epochs using
10 samples for computing each gradient estimate with ARM. We report the lowest negative ELBO
(NELBO) values for the training set and test set during all epochs."
POINTED OUT BY REVIEWERS,0.9822485207100592,"Models
The decoder model (13) describes words as independent draws from a categorical distri-
bution speciﬁed by the neural network f(z). This network respectively has a structure"
POINTED OUT BY REVIEWERS,0.985207100591716,"Linear →(ReLU →Linear)
|
{z
}
×dhidden"
POINTED OUT BY REVIEWERS,0.9881656804733728,→Logsoftmax.
POINTED OUT BY REVIEWERS,0.9911242603550295,"The input dimension equals to the number of latent bits, the output dimension equals the number of
words in the dictionary, 10000. For decoders with dhidden = 1, 2 the hidden layers contained 512
units."
POINTED OUT BY REVIEWERS,0.9940828402366864,"The encoder networks e2, e3 take on the input word frequencies x/ P"
POINTED OUT BY REVIEWERS,0.9970414201183432,"k xk, the encoder network e1
takes on input word counts x. For the deep encoder e2 we used 2 hidden ReLU layers with 512 units
each."
