Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002457002457002457,"Logistic regression training over encrypted data has been an attractive idea to
1"
ABSTRACT,0.004914004914004914,"security concerns for years. In this paper, we propose a faster gradient variant
2"
ABSTRACT,0.007371007371007371,"called quadratic gradient for privacy-preserving logistic regression training.
3"
ABSTRACT,0.009828009828009828,"The core of quadratic gradient can be seen as an extension of the simpliﬁed
4"
ABSTRACT,0.012285012285012284,"ﬁxed Hessian [4].
5"
ABSTRACT,0.014742014742014743,"We enhance Nesterov’s accelerated gradient (NAG) and Adaptive Gradient Al-
6"
ABSTRACT,0.0171990171990172,"gorithm (Adagrad) respectively with quadratic gradient and evaluate the en-
7"
ABSTRACT,0.019656019656019656,"hanced algorithms on several datasets. Experiments show that the enhanced meth-
8"
ABSTRACT,0.022113022113022112,"ods have a state-of-the-art performance in convergence speed compared to the
9"
ABSTRACT,0.02457002457002457,"raw ﬁrst-order gradient methods. We then adopt the enhanced NAG method to
10"
ABSTRACT,0.02702702702702703,"implement homomorphic logistic regression training, obtaining a comparable result
11"
ABSTRACT,0.029484029484029485,"by only 3 iterations.
12"
ABSTRACT,0.03194103194103194,"There is a promising chance that quadratic gradient could be used to enhance
13"
ABSTRACT,0.0343980343980344,"other ﬁrst-order gradient methods for general numerical optimization problems.
14"
INTRODUCTION,0.036855036855036855,"1
Introduction
15"
INTRODUCTION,0.03931203931203931,"Given a person’s healthcare data related to a certain disease, we can train a logistic regression (LR)
16"
INTRODUCTION,0.04176904176904177,"model capable of telling whether or not this person is likely to develop this disease. However, such
17"
INTRODUCTION,0.044226044226044224,"personal health information is highly private to individuals. The privacy concern, therefore, becomes
18"
INTRODUCTION,0.04668304668304668,"a major obstacle for individuals to share their biomedical data. The most secure solution is to encrypt
19"
INTRODUCTION,0.04914004914004914,"the data into ciphertexts ﬁrst by Homomorphic Encryption (HE) and then securely outsource the
20"
INTRODUCTION,0.051597051597051594,"ciphertexts to the cloud, without allowing the cloud to access the data directly. iDASH is an annual
21"
INTRODUCTION,0.05405405405405406,"competition that aims to call for implementing interesting cryptographic schemes in a biological
22"
INTRODUCTION,0.056511056511056514,"context. Since 2014, iDASH has included the theme of genomics and biomedical privacy. The third
23"
INTRODUCTION,0.05896805896805897,"track of the 2017 iDASH competition and the second track of the 2018 iDASH competition were both
24"
INTRODUCTION,0.06142506142506143,"to develop homomorphic-encryption-based solutions for building an LR model over encrypted data.
25"
INTRODUCTION,0.06388206388206388,"Several studies on logistic regression models are based on homomorphic encryption. Kim et al. [14]
26"
INTRODUCTION,0.06633906633906633,"discussed the problem of performing LR training in an encrypted environment. They used the full
27"
INTRODUCTION,0.0687960687960688,"batch gradient descent in the training process and the least-squares method to get the approximation
28"
INTRODUCTION,0.07125307125307126,"of the sigmoid function. In the iDASH 2017 competition, Bonte and Vercauteren [4], Kim et al. [12],
29"
INTRODUCTION,0.07371007371007371,"Chen et al. [5], and Crawford et al. [8] all investigated the same problem that Kim et al. [14] studied.
30"
INTRODUCTION,0.07616707616707617,"In the iDASH competition of 2018, Kim et al. [13] and Blatt et al. [2] further worked on it for an
31"
INTRODUCTION,0.07862407862407862,"efﬁcient packing and semi-parallel algorithm. The papers most relevant to this work are [4] and [12].
32"
INTRODUCTION,0.08108108108108109,"Bonte and Vercauteren [4] developed a practical algorithm called the simpliﬁed ﬁxed Hessian (SFH)
33"
INTRODUCTION,0.08353808353808354,"method. Our study complements their work and adopts the ciphertext packing technique proposed by
34"
INTRODUCTION,0.085995085995086,"Kim et al. [12] for efﬁcient homomorphic computation.
35"
INTRODUCTION,0.08845208845208845,"Our speciﬁc contributions in this paper are as follows:
36"
INTRODUCTION,0.09090909090909091,"1. We propose a new gradient variant, quadratic gradient, which can combine the ﬁrst-
37"
INTRODUCTION,0.09336609336609336,"order gradient methods and the second-order Newton-Raphson method as one.
38"
WE DEVELOP TWO ENHANCED GRADIENT METHODS BY EQUIPPING THE ORIGINAL METHODS WITH,0.09582309582309582,"2. We develop two enhanced gradient methods by equipping the original methods with
39"
WE DEVELOP TWO ENHANCED GRADIENT METHODS BY EQUIPPING THE ORIGINAL METHODS WITH,0.09828009828009827,"quadratic gradient. The resulting methods show a state-of-the-art performance in
40"
WE DEVELOP TWO ENHANCED GRADIENT METHODS BY EQUIPPING THE ORIGINAL METHODS WITH,0.10073710073710074,"the convergence speed.
41"
WE ADOPT THE ENHANCED NAG METHOD TO IMPLEMENT PRIVACY-PRESERVING LOGISTICAL REGRESSION,0.10319410319410319,"3. We adopt the enhanced NAG method to implement privacy-preserving logistical regression
42"
WE ADOPT THE ENHANCED NAG METHOD TO IMPLEMENT PRIVACY-PRESERVING LOGISTICAL REGRESSION,0.10565110565110565,"training, to our best knowledge, which seems to be the best candidate without compromising
43"
WE ADOPT THE ENHANCED NAG METHOD TO IMPLEMENT PRIVACY-PRESERVING LOGISTICAL REGRESSION,0.10810810810810811,"much on computation and storage.
44"
PRELIMINARIES,0.11056511056511056,"2
Preliminaries
45"
PRELIMINARIES,0.11302211302211303,"We adopt the square brackets “[ ]” to denote the index of a vector or matrix element in what follows.
46"
PRELIMINARIES,0.11547911547911548,"For example, for a vector v ∈R(n) and a matrix M ∈Rm×n, v[i] or v[i] means the i-th element of
47"
PRELIMINARIES,0.11793611793611794,"vector v and M[i][j] or M[i][j] the j-th element in the i-th row of M.
48"
FULLY HOMOMORPHIC ENCRYPTION,0.12039312039312039,"2.1
Fully Homomorphic Encryption
49"
FULLY HOMOMORPHIC ENCRYPTION,0.12285012285012285,"Fully Homomorphic Encryption (FHE) is a type of cryptographic scheme that can be used to compute
50"
FULLY HOMOMORPHIC ENCRYPTION,0.12530712530712532,"an arbitrary number of additions and multiplications directly on the encrypted data. It was not until
51"
FULLY HOMOMORPHIC ENCRYPTION,0.12776412776412777,"2009 that Gentry constructed the ﬁrst FHE scheme via a bootstrapping operation [9]. FHE schemes
52"
FULLY HOMOMORPHIC ENCRYPTION,0.13022113022113022,"themselves are computationally time-consuming; the choice of dataset encoding matters likewise
53"
FULLY HOMOMORPHIC ENCRYPTION,0.13267813267813267,"to the efﬁciency. In addition to these two limits, how to manage the magnitude of plaintext [11]
54"
FULLY HOMOMORPHIC ENCRYPTION,0.13513513513513514,"also contributes to the slowdown. Cheon et al. [6] proposed a method to construct an HE scheme
55"
FULLY HOMOMORPHIC ENCRYPTION,0.1375921375921376,"with a rescaling procedure which could eliminate this technical bottleneck effectively. We adopt
56"
FULLY HOMOMORPHIC ENCRYPTION,0.14004914004914004,"their open-source implementation HEAAN while implementing our homomorphic LR algorithms. It
57"
FULLY HOMOMORPHIC ENCRYPTION,0.14250614250614252,"is inevitable to pack a vector of multiple plaintexts into a single ciphertext for yielding a better
58"
FULLY HOMOMORPHIC ENCRYPTION,0.14496314496314497,"amortized time of homomorphic computation. HEAAN supports a parallel technique (aka SIMD) to
59"
FULLY HOMOMORPHIC ENCRYPTION,0.14742014742014742,"pack multiple numbers in a single polynomial by virtue of the Chinese Remainder Theorem and
60"
FULLY HOMOMORPHIC ENCRYPTION,0.14987714987714987,"provides rotation operation on plaintext slots. The underlying HE scheme in HEAAN is well described
61"
FULLY HOMOMORPHIC ENCRYPTION,0.15233415233415235,"in [12, 14, 10].
62"
DATABASE ENCODING METHOD,0.1547911547911548,"2.2
Database Encoding Method
63"
DATABASE ENCODING METHOD,0.15724815724815724,"Kim et al. [12] proposed an efﬁcient and promising database-encoding method by using SIMD
64"
DATABASE ENCODING METHOD,0.1597051597051597,"technique, which could make full use of the computation and storage resources. Suppose that a
65"
DATABASE ENCODING METHOD,0.16216216216216217,"database has a training dataset consisting of n samples with (1 + d) covariates, they packed the
66"
DATABASE ENCODING METHOD,0.16461916461916462,"training dataset Z into a single ciphertext in a row-by-row manner.
67"
DATABASE ENCODING METHOD,0.16707616707616707,"Using this encoding scheme, we can manipulate the data matrix Z by performing HE operations on the
68"
DATABASE ENCODING METHOD,0.16953316953316952,"ciphertext Enc[Z], with the help of only three HE operations - rotation, addition and multiplication.
69"
DATABASE ENCODING METHOD,0.171990171990172,"Han et al. [10] introduced several operations to manipulate the ciphertexts, such as a procedure
70"
DATABASE ENCODING METHOD,0.17444717444717445,"named “SumColVec” to compute the summation of the columns of a matrix. By dint of these basic
71"
DATABASE ENCODING METHOD,0.1769041769041769,"operations, more complex calculations such as computing the gradients in logistic regression models
72"
DATABASE ENCODING METHOD,0.17936117936117937,"are achievable.
73"
LOGISTIC REGRESSION,0.18181818181818182,"2.3
Logistic Regression
74"
LOGISTIC REGRESSION,0.18427518427518427,"Logistic regression is widely used in binary classiﬁcation tasks to infer whether a binary-valued
75"
LOGISTIC REGRESSION,0.18673218673218672,"variable belongs to a certain class or not. LR can be generalized from linear regression [15] by
76"
LOGISTIC REGRESSION,0.1891891891891892,"mapping the whole real line (βT x) to (0, 1) via the sigmoid function σ(z) = 1/(1+exp(−z)), where
77"
LOGISTIC REGRESSION,0.19164619164619165,"the vector β ∈R(1+d) is the main parameter of LR and the vector x = (1, x1, . . . , xd) ∈R(1+d) the
78"
LOGISTIC REGRESSION,0.1941031941031941,"input covariate. Thus logistic regression can be formulated with the class label y ∈{±1} as follows:
79"
LOGISTIC REGRESSION,0.19656019656019655,"Pr(y = +1|x, β) = σ(βT x)
=
1
1 + e−βT x ,"
LOGISTIC REGRESSION,0.19901719901719903,"Pr(y = −1|x, β) = 1 −σ(βT x)
=
1
1 + e+βT x ."
LOGISTIC REGRESSION,0.20147420147420148,"LR sets a threshold (usually 0.5) and compares its output with it to decide the resulting class label.
80"
LOGISTIC REGRESSION,0.20393120393120392,"The logistic regression problem can be transformed into an optimization problem that seeks a param-
81"
LOGISTIC REGRESSION,0.20638820638820637,"eter β to maximize L(β) = Qn
i=1 Pr(yi|xi, β) or its log-likelihood function l(β) for convenience in
82"
LOGISTIC REGRESSION,0.20884520884520885,"the calculation:
83"
LOGISTIC REGRESSION,0.2113022113022113,"l(β) = ln L(β) = − n
X"
LOGISTIC REGRESSION,0.21375921375921375,"i=1
ln(1 + e−yiβT xi),"
LOGISTIC REGRESSION,0.21621621621621623,"where n is the number of examples in the training dataset. LR does not have a closed form of
84"
LOGISTIC REGRESSION,0.21867321867321868,"maximizing l(β) and two main methods are adopted to estimate the parameters of an LR model:
85"
LOGISTIC REGRESSION,0.22113022113022113,"(a) gradient descent method via the gradient; and (b) Newton’s method by the Hessian matrix. The
86"
LOGISTIC REGRESSION,0.22358722358722358,"gradient and Hessian of the log-likelihood function l(β) are given by, respectively:
87"
LOGISTIC REGRESSION,0.22604422604422605,"∇βl(β) =
X"
LOGISTIC REGRESSION,0.2285012285012285,"i
(1 −σ(yiβT xi))yixi,"
LOGISTIC REGRESSION,0.23095823095823095,"∇2
βl(β) =
X"
LOGISTIC REGRESSION,0.2334152334152334,"i
(yixi)(σ(yiβT xi) −1)σ(yiβT xi)(yixi)"
LOGISTIC REGRESSION,0.23587223587223588,"= XT SX,"
LOGISTIC REGRESSION,0.23832923832923833,"where S is a diagonal matrix with entries Sii = (σ(yiβT xi) −1)σ(yiβT xi) and X the dataset.
88"
LOGISTIC REGRESSION,0.24078624078624078,"The log-likelihood function l(β) of LR has at most a unique global maximum [1], where its gradient
89"
LOGISTIC REGRESSION,0.24324324324324326,"is zero. Newton’s method is a second-order technique to numerically ﬁnd the roots of a real-valued
90"
LOGISTIC REGRESSION,0.2457002457002457,"differentiable function, and thus can be used to solve the β in ∇βl(β) = 0 for LR.
91"
TECHNICAL DETAILS,0.24815724815724816,"3
Technical Details
92"
TECHNICAL DETAILS,0.25061425061425063,"It is quite time-consuming to compute the Hessian matrix and its inverse in Newton’s method for
each iteration. One way to limit this downside is to replace the varying Hessian with a ﬁxed matrix
¯H. This novel technique is called the ﬁxed Hessian Newton’s method. Böhning and Lindsay [3] have
shown that the convergence of Newton’s method is guaranteed as long as ¯H ≤∇2
βl(β), where ¯H is
a symmetric negative-deﬁnite matrix independent of β and “≤” denotes the Loewner ordering in the
sense that the difference ∇2
βl(β) −¯H is non-negative deﬁnite. With such a ﬁxed Hessian matrix ¯H,
the iteration for Newton’s method can be simpliﬁed to:"
TECHNICAL DETAILS,0.25307125307125306,βt+1 = βt −¯H−1∇βl(β).
TECHNICAL DETAILS,0.25552825552825553,Böhning and Lindsay also suggest the ﬁxed matrix ¯H = −1
XT X IS A GOOD LOWER BOUND FOR THE,0.257985257985258,"4XT X is a good lower bound for the
93"
XT X IS A GOOD LOWER BOUND FOR THE,0.26044226044226043,"Hessian of the log-likelihood function l(β) in LR.
94"
XT X IS A GOOD LOWER BOUND FOR THE,0.2628992628992629,"3.1
the Simpliﬁed Fixed Hessian method
95"
XT X IS A GOOD LOWER BOUND FOR THE,0.26535626535626533,"Bonte and Vercauteren [4] simplify this lower bound ¯H further due to the need for inverting the
ﬁxed Hessian in the encrypted domain. They replace the matrix ¯H with a diagonal matrix B whose
diagonal elements are simply the sums of each row in ¯H. They also suggest a speciﬁc order of
calculation to get B more efﬁciently. Their new approximation B of the ﬁxed Hessian is: B =  "
XT X IS A GOOD LOWER BOUND FOR THE,0.2678132678132678,"Pd
i=0 ¯h0i
0
. . .
0
0
Pd
i=0 ¯h1i
. . .
0
...
...
...
...
0
0
. . .
Pd
i=0 ¯hdi  ,"
XT X IS A GOOD LOWER BOUND FOR THE,0.2702702702702703,"where ¯hki is the element of ¯H. This diagonal matrix B is in a very simple form and can be obtained
96"
XT X IS A GOOD LOWER BOUND FOR THE,0.2727272727272727,"from ¯H without much difﬁculty. The inverse of B can be approximated in the encrypted form by
97"
XT X IS A GOOD LOWER BOUND FOR THE,0.2751842751842752,"means of computing the inverse of every diagonal element of B via the iterative of Newton’s method
98"
XT X IS A GOOD LOWER BOUND FOR THE,0.27764127764127766,"with an appropriate start value. Their simpliﬁed ﬁxed Hessian method can be formulated as follows:
99"
XT X IS A GOOD LOWER BOUND FOR THE,0.2800982800982801,"βt+1 = βt −B−1 · ∇βl(β),"
XT X IS A GOOD LOWER BOUND FOR THE,0.28255528255528256,= βt −  
XT X IS A GOOD LOWER BOUND FOR THE,0.28501228501228504,"b00
0
. . .
0
0
b11
. . .
0
...
...
...
...
0
0
. . .
bdd  ·  "
XT X IS A GOOD LOWER BOUND FOR THE,0.28746928746928746,"∇0
∇1
...
∇d "
XT X IS A GOOD LOWER BOUND FOR THE,0.28992628992628994,= βt −  
XT X IS A GOOD LOWER BOUND FOR THE,0.29238329238329236,"b00 · ∇0
b11 · ∇1
...
bdd · ∇d  ,"
XT X IS A GOOD LOWER BOUND FOR THE,0.29484029484029484,"where bii is the reciprocal of Pd
i=0 ¯h0i and ∇i is the element of ∇βl(β).
100"
XT X IS A GOOD LOWER BOUND FOR THE,0.2972972972972973,"Consider a special situation: if b00, . . . , bdd are all the same value −η with η > 0, the iterative formula
101"
XT X IS A GOOD LOWER BOUND FOR THE,0.29975429975429974,"of the SFH method can be given as:
102"
XT X IS A GOOD LOWER BOUND FOR THE,0.3022113022113022,βt+1 = βt −(−η) ·  
XT X IS A GOOD LOWER BOUND FOR THE,0.3046683046683047,"∇0
∇1
...
∇d "
XT X IS A GOOD LOWER BOUND FOR THE,0.3071253071253071,"= βt + η · ∇βl(β),"
XT X IS A GOOD LOWER BOUND FOR THE,0.3095823095823096,"which is the same as the formula of the naive gradient ascent method. Such coincident is just what
103"
XT X IS A GOOD LOWER BOUND FOR THE,0.31203931203931207,"the idea behind this work comes from: there is some relation between the Hessian matrix and the
104"
XT X IS A GOOD LOWER BOUND FOR THE,0.3144963144963145,"learning rate of the gradient (descent) method. We consider bii · ∇i as a new enhanced gradient
105"
XT X IS A GOOD LOWER BOUND FOR THE,0.31695331695331697,"variant’s element and assign a new learning rate to it. As long as we ensure that this new learning
106"
XT X IS A GOOD LOWER BOUND FOR THE,0.3194103194103194,"rate decreases from a positive ﬂoating-point number greater than 1 (such as 2) to 1 in a bounded
107"
XT X IS A GOOD LOWER BOUND FOR THE,0.32186732186732187,"number of iteration steps, the ﬁxed Hessian Newton’s method guarantees the algorithm will converge
108"
XT X IS A GOOD LOWER BOUND FOR THE,0.32432432432432434,"eventually.
109"
XT X IS A GOOD LOWER BOUND FOR THE,0.32678132678132676,"The SFH method proposed by Bonte and Vercauteren [4] has two limitations: (a) in the construction
110"
XT X IS A GOOD LOWER BOUND FOR THE,0.32923832923832924,"of the simpliﬁed ﬁxed Hessian matrix, all entries in the symmetric matrix ¯H need to be non-positive.
111"
XT X IS A GOOD LOWER BOUND FOR THE,0.3316953316953317,"For machine learning applications the datasets will be in advance normalized into the range [0,1],
112"
XT X IS A GOOD LOWER BOUND FOR THE,0.33415233415233414,"meeting the convergence condition of the SFH method. However, for other cases such as numerical
113"
XT X IS A GOOD LOWER BOUND FOR THE,0.3366093366093366,"optimization, it doesn’t always hold; and (b) the simpliﬁed ﬁxed Hessian matrix B that Bonte and
114"
XT X IS A GOOD LOWER BOUND FOR THE,0.33906633906633904,"Vercauteren [4] constructed, as well as the ﬁxed Hessian matrix ¯H = −1"
XT X IS A GOOD LOWER BOUND FOR THE,0.3415233415233415,"4XT X , can still be singular,
115"
XT X IS A GOOD LOWER BOUND FOR THE,0.343980343980344,"especially when the dataset is a high-dimensional sparse matrix, such as the MNIST datasets. We
116"
XT X IS A GOOD LOWER BOUND FOR THE,0.3464373464373464,"extend their work by removing these limitations so as to generalize this simpliﬁed ﬁxed Hessian to be
117"
XT X IS A GOOD LOWER BOUND FOR THE,0.3488943488943489,"invertible in any case and propose a faster gradient variant, which we term quadratic gradient.
118"
QUADRATIC GRADIENT,0.35135135135135137,"3.2
Quadratic Gradient
119"
QUADRATIC GRADIENT,0.3538083538083538,"Suppose that a differentiable scalar-valued function F(x) has its gradient g and Hessian matrix H,
120"
QUADRATIC GRADIENT,0.35626535626535627,"with any matrix ¯H ≤H in the Loewner ordering for a maximization problem as follows:
121 g =  "
QUADRATIC GRADIENT,0.35872235872235875,"g0
g1
...
gd "
QUADRATIC GRADIENT,0.36117936117936117,",
H =  "
QUADRATIC GRADIENT,0.36363636363636365,"∇2
00
∇2
01
. . .
∇2
0d
∇2
10
∇2
11
. . .
∇2
1d
...
...
...
...
∇2
d0
∇2
d1
. . .
∇2
dd "
QUADRATIC GRADIENT,0.36609336609336607,",
¯H
=  "
QUADRATIC GRADIENT,0.36855036855036855,"¯h00
¯h01
. . .
¯h0d
¯h10
¯h11
. . .
¯h1d
...
...
...
...
¯hd0
¯hd1
. . .
¯hdd  ,"
QUADRATIC GRADIENT,0.371007371007371,"where ∇2
ij = ∇2
ji =
∂2F
∂xi∂xj . We construct a new Hessian matrix ˜B as follows:
122 ˜B =  "
QUADRATIC GRADIENT,0.37346437346437344,"−ε −Pd
i=0 |¯h0i|
0
. . .
0
0
−ε −Pd
i=0 |¯h1i|
. . .
0
...
...
...
...
0
0
. . .
−ε −Pd
i=0 |¯hdi|  ,"
QUADRATIC GRADIENT,0.3759213759213759,"where ε is a small positive constant to avoid division by zero (usually set to 1e −8).
123"
QUADRATIC GRADIENT,0.3783783783783784,"As long as ˜B satisﬁes the convergence condition of the above ﬁxed Hessian method, ˜B ≤H, we
124"
QUADRATIC GRADIENT,0.3808353808353808,"can use this approximation ˜B of the Hessian matrix as a lower bound. Since we already assume that
125"
QUADRATIC GRADIENT,0.3832923832923833,"¯H ≤H, it will sufﬁce to show that ˜B ≤¯H. We prove ˜B ≤¯H in a similar way that [4] did.
126"
QUADRATIC GRADIENT,0.3857493857493858,"Lemma 1. Let A ∈Rn×n be a symmetric matrix, and let B be the diagonal matrix whose diagonal
127"
QUADRATIC GRADIENT,0.3882063882063882,entries Bkk = −ε −P
QUADRATIC GRADIENT,0.3906633906633907,"i |Aki| for k = 1, . . . , n, then B ≤A.
128"
QUADRATIC GRADIENT,0.3931203931203931,"Proof. By deﬁnition of the Loewner ordering, we have to prove the difference matrix C = A −B
129"
QUADRATIC GRADIENT,0.3955773955773956,"is non-negative deﬁnite, which means that all the eigenvalues of C need to be non-negative. By
130"
QUADRATIC GRADIENT,0.39803439803439805,"construction of C we have that Cij = Aij + ε + Pn
k=1 |Aik| for i = j and Cij = Aij for i ̸= j.
131"
QUADRATIC GRADIENT,0.4004914004914005,"By means of Gerschgorin’s circle theorem, we can bound every eigenvalue λ of C in the sense that
132"
QUADRATIC GRADIENT,0.40294840294840295,|λ −Cii| ≤P
QUADRATIC GRADIENT,0.40540540540540543,"i̸=j |Cij| for some index i ∈{1, 2, . . . , n}. We conclude that λ ≥Aii + ε + |Aii| ≥
133"
QUADRATIC GRADIENT,0.40786240786240785,"ε > 0 for all eigenvalues λ and thus that B ≤A.
134"
QUADRATIC GRADIENT,0.4103194103194103,"Deﬁnition 3.1 (Quadratic Gradient). Given such a ˜B above, we deﬁne the quadratic gradient
135"
QUADRATIC GRADIENT,0.41277641277641275,"as G = ¯B · g with a new learning rate η, where ¯B is a diagonal matrix with diagonal entries
136"
QUADRATIC GRADIENT,0.4152334152334152,"¯Bkk = 1/| ˜Bkk|, and η should be always no less than 1 and decrease to 1 in a limited number of
137"
QUADRATIC GRADIENT,0.4176904176904177,"iteration steps. Note that G is still a column vector of the same size as the gradient g. To maximize
138"
QUADRATIC GRADIENT,0.4201474201474201,"the function F(x), we can use the iterative formulas: xk+1 = xk + η · G, just like the naive gradient.
139"
QUADRATIC GRADIENT,0.4226044226044226,"To minimize the function F(x) is the same as to just maximize the function −F(x), in which case we
140"
QUADRATIC GRADIENT,0.4250614250614251,"need to construct the ˜B by any good lower bound ¯H of the Hessian −H of −F(x) or any good upper
141"
QUADRATIC GRADIENT,0.4275184275184275,"bound ¯H of the Hessian H of F(x). We point out here that ¯H could be the Hessian matrix H itself.
142"
QUADRATIC GRADIENT,0.42997542997543,"In our experiments, we use ¯H = −1"
QUADRATIC GRADIENT,0.43243243243243246,"4XT X to construct our ˜B.
143"
TWO ENHANCED METHODS,0.4348894348894349,"3.3
Two Enhanced Methods
144"
TWO ENHANCED METHODS,0.43734643734643736,"Quadratic Gradient can be used to enhance NAG and Adagrad.
145"
TWO ENHANCED METHODS,0.4398034398034398,"NAG is a different variant of the momentum method to give the momentum term much more
146"
TWO ENHANCED METHODS,0.44226044226044225,"prescience. The iterative formulas of the gradient ascent method for NAG are as follows:
147"
TWO ENHANCED METHODS,0.44471744471744473,"Vt+1 = βt + αt · ∇J(βt),
(3)
βt+1 = (1 −γt) · Vt+1 + γt · Vt,
(4)"
TWO ENHANCED METHODS,0.44717444717444715,"where Vt+1 is the intermediate variable used for updating the ﬁnal weight βt+1 and γt ∈(0, 1) is a
148"
TWO ENHANCED METHODS,0.44963144963144963,"smoothing parameter of moving average to evaluate the gradient at an approximate future position
149"
TWO ENHANCED METHODS,0.4520884520884521,"[12]. The enhanced NAG is to replace (3) with Vt+1 = βt + (1 + αt) · G. Our enhanced NAG
150"
TWO ENHANCED METHODS,0.45454545454545453,"method is described in Algorithm 1 .
151"
TWO ENHANCED METHODS,0.457002457002457,"Adagrad is a gradient-based algorithm suitable for dealing with sparse data. The updated operations
152"
TWO ENHANCED METHODS,0.4594594594594595,"of Adagrad and its quadratic-gradient version, for every parameter β[i] at each iteration step t, are as
153"
TWO ENHANCED METHODS,0.4619164619164619,"follows, respectively:
154"
TWO ENHANCED METHODS,0.4643734643734644,"β(t+1)
[i]
= β(t)
[i] −
η"
TWO ENHANCED METHODS,0.4668304668304668,"ε +
qPt
k=1 g(t)
[i] · g(t)
[i]
· g(t)
[i] ,"
TWO ENHANCED METHODS,0.4692874692874693,"β(t+1)
[i]
= β(t)
[i] −
1 + η"
TWO ENHANCED METHODS,0.47174447174447176,"ε +
qPt
k=1 G(t)
[i] · G(t)
[i]
· G(t)
[i] ."
TWO ENHANCED METHODS,0.4742014742014742,"Performance Evaluation We evaluate the performance of various algorithms in the clear using the
155"
TWO ENHANCED METHODS,0.47665847665847666,"Python programming language on the same desktop computer with an Intel Core CPU G640 at
156"
TWO ENHANCED METHODS,0.47911547911547914,"1.60 GHz and 7.3 GB RAM. Since our focus is on how fast the algorithms converge in the training
157"
TWO ENHANCED METHODS,0.48157248157248156,"phase, the loss function, maximum likelihood estimation (MLE), is selected as the only indicator. We
158"
TWO ENHANCED METHODS,0.48402948402948404,"evaluate four algorithms, NAG, Adagrad, and their quadratic-gradient versions (denoted as Enhanced
159"
TWO ENHANCED METHODS,0.4864864864864865,"NAG and Enhanced Adagrad, respectively) on the datasets that Kim et al. [12] adopted: the iDASH
160"
TWO ENHANCED METHODS,0.48894348894348894,"genomic dataset (iDASH), the Myocardial Infarction dataset from Edinburgh (Edinburgh), Low Birth
161"
TWO ENHANCED METHODS,0.4914004914004914,"weight Study (lbw), Nhanes III (nhanes3), Prostate Cancer study (pcs), and Umaru Impact Study
162"
TWO ENHANCED METHODS,0.49385749385749383,"datasets (uis). The genomic dataset is provided by the third task in the iDASH competition of 2017,
163"
TWO ENHANCED METHODS,0.4963144963144963,"which consists of 1579 records. Each record has 103 binary genotypes and a binary phenotype
164"
TWO ENHANCED METHODS,0.4987714987714988,"indicating if the patient has cancer. The other ﬁve datasets all have a single binary dependent variable.
165"
TWO ENHANCED METHODS,0.5012285012285013,"Figures 1 and 2 show that except for the enhanced Adagrad method on the iDASH genomic dataset
166"
TWO ENHANCED METHODS,0.5036855036855037,"our enhanced methods all converge faster than their original ones in other cases. In all the Python
167"
TWO ENHANCED METHODS,0.5061425061425061,Algorithm 1 The Enhanced Nesterov’s Accelerated Gradient method
TWO ENHANCED METHODS,0.5085995085995086,"Input: training dataset X
∈
Rn×(1+d); training label Y
∈
Rn×1; learning rate lr
∈
R(set to 10.0 in this work in order to align with the baseline work); and
the number κ of iterations;
Output: the parameter vector V ∈R(1+d)"
TWO ENHANCED METHODS,0.5110565110565111,1: Set ¯H ←−1
XT X,0.5135135135135135,"4XT X
▷¯H ∈R(1+d)×(1+d)"
XT X,0.515970515970516,"2: Set V ←0, W ←0, ¯B ←0
▷V ∈R(1+d), W ∈R(1+d), ¯B ∈R(1+d)×(1+d)"
XT X,0.5184275184275184,"3: for i := 0 to d do
4:
¯B[i][i] ←ε
▷ε is a small positive constant such as 1e −8
5:
for j := 0 to d do
6:
¯B[i][i] ←¯B[i][i] + | ¯H[i][j]|
7:
end for
8: end for
9: Set α0 ←0.01, α1 ←0.5 × (1 +
p"
XT X,0.5208845208845209,"1 + 4 × α2
0)
10: for count := 1 to κ do
11:
Set Z ←0
▷Z ∈Rn is the inputs for sigmoid function
12:
for i := 1 to n do
13:
for j := 0 to d do
14:
Z[i] ←Z[i] + Y [i] × V [j] × X[i][j]
15:
end for
16:
end for
17:
Set σ ←0
▷σ ∈Rn is to store the outputs of the sigmoid function
18:
for i := 1 to n do
19:
σ[i] ←1/(1 + exp(−Z[i]))
20:
end for
21:
Set g ←0
22:
for j := 0 to d do
23:
for i := 1 to n do
24:
g[j] ←g[j] + (1 −σ[i]) × Y [i] × X[i][j]
25:
end for
26:
end for
27:
Set G ←0
28:
for j := 0 to d do
29:
G[j] ←¯B[j][j] × g[j]
30:
end for
31:
Set η ←(1 −α0)/α1, γ ←lr/(n × count) ▷n is the size of training data; lr is set to 10.0
in this work
32:
for j := 0 to d do
33:
wtemp ←V [j] + (1 + γ) × G[j]
34:
V [j] ←(1 −η) × wtemp + η × W[j]
35:
W[j] ←wtemp
36:
end for
37:
α0 ←α1, α1 ←0.5 × (1 +
p"
XT X,0.5233415233415234,"1 + 4 × α2
0)
38: end for
39: return V"
XT X,0.5257985257985258,"0
20
40 −2 −1 0 ·104"
XT X,0.5282555282555282,Iteration Number
XT X,0.5307125307125307,Maximum Likelihood Estimation
XT X,0.5331695331695332,"Adagrad
Enhanced Adagrad"
XT X,0.5356265356265356,(a) The iDASH dataset
XT X,0.538083538083538,"0
20
40 −800 −600 −400 −200"
XT X,0.5405405405405406,Iteration Number
XT X,0.542997542997543,"Adagrad
Enhanced Adagrad"
XT X,0.5454545454545454,(b) The Edinburgh dataset
XT X,0.547911547911548,"0
20
40 −140 −120 −100"
XT X,0.5503685503685504,Iteration Number
XT X,0.5528255528255528,"Adagrad
Enhanced Adagrad"
XT X,0.5552825552825553,(c) The lbw dataset
XT X,0.5577395577395577,"0
20
40
−2 −1.5 −1 −0.5 ·104"
XT X,0.5601965601965602,Iteration Number
XT X,0.5626535626535627,Maximum Likelihood Estimation
XT X,0.5651105651105651,"Adagrad
Enhanced Adagrad"
XT X,0.5675675675675675,(d) The nhanes3 dataset
XT X,0.5700245700245701,"0
20
40 −400 −300 −200"
XT X,0.5724815724815725,Iteration Number
XT X,0.5749385749385749,"Adagrad
Enhanced Adagrad"
XT X,0.5773955773955773,(e) The pcs dataset
XT X,0.5798525798525799,"0
20
40 −500 −400 −300"
XT X,0.5823095823095823,Iteration Number
XT X,0.5847665847665847,"Adagrad
Enhanced Adagrad"
XT X,0.5872235872235873,(f) The uis dataset
XT X,0.5896805896805897,Figure 1: Training results in the clear for Adagrad and Enhanced Adagrad
XT X,0.5921375921375921,"experiments, the time to calculate the ¯B in quadratic gradient G before running the iterations and the
168"
XT X,0.5945945945945946,"time to run each iteration for various algorithms are negligible (few seconds).
169"
XT X,0.597051597051597,"Results Analysis In Figure 1a, the enhanced Adagrad algorithm failed to outperform the original
170"
XT X,0.5995085995085995,"Adagrad algorithm. The possible reason for that might be related to the limitations of the raw Adagrad
171"
XT X,0.601965601965602,"method. Without a doubt, Adagrad is a novel algorithm initiated to accelerate each element of the
172"
XT X,0.6044226044226044,"gradient with different learning rates. However, Adagrad tends to converge to a suboptimal solution
173"
XT X,0.6068796068796068,"due to its aggressive, monotonically decreasing learning rates. This would lead to its main limitation
174"
XT X,0.6093366093366094,"that in the later training phase every learning rate for different components of the gradient is too close
175"
XT X,0.6117936117936118,"to zero due to keeping adding positive additional terms to the denominator, stopping the algorithm
176"
XT X,0.6142506142506142,"from learning anything.
177"
XT X,0.6167076167076168,"On the other hand, the original Adagrad method has another little-noticed limitation: the learning rate
178"
XT X,0.6191646191646192,"in the ﬁrst few iterations tends to be large. While this limitation does not affect the performance of the
179"
XT X,0.6216216216216216,"original Adagrad method to some extent, the enhanced Adagrad method exacerbates this phenomenon
180"
XT X,0.6240786240786241,"by a factor of about 102 ·
ε+
qPt
k=1 g(t)
[i] ·g(t)
[i]"
XT X,0.6265356265356266,"ε+
qPt
k=1 G(t)
[i] ·G(t)
[i]
, leading to the Learning-Rate Explosion . Therefore,
181"
XT X,0.628992628992629,"the enhanced Adagrad [7] cannot be applied to general optimization problems such as Rosenbrock’s
182"
XT X,0.6314496314496314,"function. The exploding learning rate would be too large for the algorithm to survive the ﬁrst
183"
XT X,0.6339066339066339,"several iterations, ﬁnally leading the optimization function to some point where its output cannot be
184"
XT X,0.6363636363636364,"represented by the computer system. This might explain why the performance of this algorithm in all
185"
XT X,0.6388206388206388,"cases, not just on the iDASH genome dataset, seems to be meaningless, numerically unstable, and
186"
XT X,0.6412776412776413,"ﬂuctuates in the ﬁrst few iterations.
187"
XT X,0.6437346437346437,"Several improved algorithms upon the Adagrad method, such as RMSProp, have been proposed in
188"
XT X,0.6461916461916462,"order to address these issues existed, via using an exponential moving average of historical gradients
189"
XT X,0.6486486486486487,"rather than just the sum of all squared gradients from the beginning of training. We might be able
190"
XT X,0.6511056511056511,"to overcome the problems existing in the enhanced Adagrad method by adopting the enhanced
191"
XT X,0.6535626535626535,"Adagrad-like variants, like the enhanced Adadelta method and the enhanced RMSProp method. One
192"
XT X,0.6560196560196561,"research work that could conﬁrm this hypothesis is the enhanced Adam method [7].
193"
XT X,0.6584766584766585,"0
20
40"
XT X,0.6609336609336609,"−8,000"
XT X,0.6633906633906634,"−6,000"
XT X,0.6658476658476659,"−4,000"
XT X,0.6683046683046683,"−2,000"
XT X,0.6707616707616708,Iteration Number
XT X,0.6732186732186732,Maximum Likelihood Estimation
XT X,0.6756756756756757,"NAG
Enhanced NAG"
XT X,0.6781326781326781,(a) The iDASH dataset
XT X,0.6805896805896806,"0
20
40 −800 −600 −400 −200"
XT X,0.683046683046683,Iteration Number
XT X,0.6855036855036855,"NAG
Enhanced NAG"
XT X,0.687960687960688,(b) The Edinburgh dataset
XT X,0.6904176904176904,"0
20
40 −130 −120 −110 −100"
XT X,0.6928746928746928,Iteration Number
XT X,0.6953316953316954,"NAG
Enhanced NAG"
XT X,0.6977886977886978,(c) The lbw dataset
XT X,0.7002457002457002,"0
20
40
−1.2 −1 −0.8 −0.6 ·104"
XT X,0.7027027027027027,Iteration Number
XT X,0.7051597051597052,Maximum Likelihood Estimation
XT X,0.7076167076167076,"NAG
Enhanced NAG"
XT X,0.7100737100737101,(d) The nhanes3 dataset
XT X,0.7125307125307125,"0
20
40 −260 −240 −220 −200"
XT X,0.714987714987715,Iteration Number
XT X,0.7174447174447175,"NAG
Enhanced NAG"
XT X,0.7199017199017199,(e) The pcs dataset
XT X,0.7223587223587223,"0
20
40
−400 −380 −360 −340 −320"
XT X,0.7248157248157249,Iteration Number
XT X,0.7272727272727273,"NAG
Enhanced NAG"
XT X,0.7297297297297297,(f) The uis dataset
XT X,0.7321867321867321,Figure 2: Training results in the clear for NAG and Enhanced NAG
PRIVACY-PRESERVING LR TRAINING,0.7346437346437347,"4
Privacy-preserving LR Training
194"
PRIVACY-PRESERVING LR TRAINING,0.7371007371007371,"Adagrad method is not a practical solution for homomorphic LR due to its frequent inversion
195"
PRIVACY-PRESERVING LR TRAINING,0.7395577395577395,"operations. It seems plausible that the enhanced NAG is probably the best choice for privacy-
196"
PRIVACY-PRESERVING LR TRAINING,0.742014742014742,"preserving LR training. We adopt the enhanced NAG method to implement privacy-preserving
197"
PRIVACY-PRESERVING LR TRAINING,0.7444717444717445,"logistic regression training. The difﬁculty in applying the quadratic gradient is to invert the diagonal
198"
PRIVACY-PRESERVING LR TRAINING,0.7469287469287469,"matrix ˜B in order to obtain ¯B. We leave the computation of matrix ¯B to data owner and let the
199"
PRIVACY-PRESERVING LR TRAINING,0.7493857493857494,"data owner upload the ciphertext encrypting the ¯B to the cloud. Since data owner has to prepare the
200"
PRIVACY-PRESERVING LR TRAINING,0.7518427518427518,"dataset and normalize it, it would also be practicable for the data owner to calculate the ¯B owing to
201"
PRIVACY-PRESERVING LR TRAINING,0.7542997542997543,"no leaking of sensitive data information.
202"
PRIVACY-PRESERVING LR TRAINING,0.7567567567567568,"Privacy-preserving logistic regression training based on HE techniques faces a difﬁcult dilemma that
203"
PRIVACY-PRESERVING LR TRAINING,0.7592137592137592,"no homomorphic schemes are capable of directly calculating the sigmoid function in the LR model.
204"
PRIVACY-PRESERVING LR TRAINING,0.7616707616707616,"A common solution is to replace the sigmoid function with a polynomial approximation by using the
205"
PRIVACY-PRESERVING LR TRAINING,0.7641277641277642,"widely adopted least-squares method. We can call a function named “ polyfit(·) ” in the Python
206"
PRIVACY-PRESERVING LR TRAINING,0.7665847665847666,"package Numpy to ﬁt the polynomial in a least-square sense. We adopt the degree 5 polynomial
207"
PRIVACY-PRESERVING LR TRAINING,0.769041769041769,"approximation g(x) by which Kim et al. [12] used the least square approach to approximate the
208"
PRIVACY-PRESERVING LR TRAINING,0.7714987714987716,"sigmoid function over the domain [−8, 8]: g(x) = 0.5+0.19131·x−0.0045963·x3+0.0000412332·
209"
PRIVACY-PRESERVING LR TRAINING,0.773955773955774,"x5 .
210"
PRIVACY-PRESERVING LR TRAINING,0.7764127764127764,"Given the training dataset X ∈Rn×(1+d) and training label Y ∈Rn×1, we adopt the same method
211"
PRIVACY-PRESERVING LR TRAINING,0.7788697788697788,"that Kim et al. [12] used to encrypt the data matrix consisting of the training data combined with
212"
PRIVACY-PRESERVING LR TRAINING,0.7813267813267813,"training-label information into a single ciphertext ctZ. The weight vector β(0) consisting of zeros and
213"
PRIVACY-PRESERVING LR TRAINING,0.7837837837837838,"the diagnoal elements of ¯B are copied n times to form two matrices. The data owner then encrypt the
214"
PRIVACY-PRESERVING LR TRAINING,0.7862407862407862,"two matrices into two ciphertexts ct(0)
β
and ct ¯
B, respectively.
215"
PRIVACY-PRESERVING LR TRAINING,0.7886977886977887,"The pulbic cloud takes the three ciphertexts ctZ, ct(0)
β
and ct ¯
B and evaluates the enhanced NAG
216"
PRIVACY-PRESERVING LR TRAINING,0.7911547911547911,"algorithm to ﬁnd a decent weight vector by updating the vector ct(0)
β . Refer to [12] for a detailed
217"
PRIVACY-PRESERVING LR TRAINING,0.7936117936117936,"description about how to calculate the gradient by HE programming.
218"
EXPERIMENTS,0.7960687960687961,"5
Experiments
219"
EXPERIMENTS,0.7985257985257985,"Implementation
We implement the enhanced NAG based on HE with the library HEAAN. The C++
220"
EXPERIMENTS,0.800982800982801,"source code is publicly available at https://anonymous.4open.science/r/IDASH2017-245B .
221"
EXPERIMENTS,0.8034398034398035,"All the experiments on the ciphertexts were conducted on a public cloud with 32 vCPUs and 64 GB
222"
EXPERIMENTS,0.8058968058968059,"RAM.
223"
EXPERIMENTS,0.8083538083538083,"For a fair comparison with [12], we utilized the same 10-fold cross-validation (CV) technique on the
224"
EXPERIMENTS,0.8108108108108109,"same iDASH dataset consisting of 1579 samples with 18 features and the same 5-fold CV technique
225"
EXPERIMENTS,0.8132678132678133,"on the other ﬁve datasets. Like [12], We consider the average accuracy and the Area Under the
226"
EXPERIMENTS,0.8157248157248157,"Curve (AUC) as the main indicators. Tables 1 and 2 show the two experiment results, respectively.
227"
EXPERIMENTS,0.8181818181818182,"The two tables also provide the average evaluation running time for each iteration and the storage
228"
EXPERIMENTS,0.8206388206388207,"(encrypted dataset for the baseline work and encrypted dataset and ¯B for our method). We adopt the
229"
EXPERIMENTS,0.8230958230958231,"same packing method that Kim et al. [12] proposed and hence our solution has similar storage of
230"
EXPERIMENTS,0.8255528255528255,"ciphertexts to [12] with some extra ciphertexts to encrypt the ¯B.
231"
EXPERIMENTS,0.828009828009828,"The parameters of HEAAN we set are same to [12]: logN = 16, logQ = 1200, logp = 30, slots =
232"
EXPERIMENTS,0.8304668304668305,"32768, which ensure the security level λ = 80. Refer [12] for the details of these parameters. Since
233"
EXPERIMENTS,0.8329238329238329,"our enhanced NAG method need to consume more modulus to preserve the precision of ¯B, we use
234"
EXPERIMENTS,0.8353808353808354,"logp = 60 to encrypt the matrix ¯B and thus only can perform 3 iterations of the enhanced NAG
235"
EXPERIMENTS,0.8378378378378378,"method. Yet despite only 3 iterations, our enhanced NAG method still produces a comparable result.
236"
EXPERIMENTS,0.8402948402948403,Table 1: Implementation Results for iDASH datasets with 10-fold CV
EXPERIMENTS,0.8427518427518428,"Dataset
Sample
Num"
EXPERIMENTS,0.8452088452088452,"Feature
Num
Method
deg g
Iter
Num"
EXPERIMENTS,0.8476658476658476,"Storage
(GB)"
EXPERIMENTS,0.8501228501228502,"Learn
Time
(min)"
EXPERIMENTS,0.8525798525798526,"Accuracy
(%)
AUC"
EXPERIMENTS,0.855036855036855,"iDASH
1579
18
Ours
5
3
0.08
3.61
53.38
0.681
[12]
5
7
0.04
6.07
62.87
0.689"
EXPERIMENTS,0.8574938574938575,Table 2: Implementation Results for other datasets with 5-fold CV
EXPERIMENTS,0.85995085995086,"Dataset
Sample
Num"
EXPERIMENTS,0.8624078624078624,"Feature
Num
Method
deg g
Iter
Num"
EXPERIMENTS,0.8648648648648649,"Storage
(GB)"
EXPERIMENTS,0.8673218673218673,"Learn
Time
(min)"
EXPERIMENTS,0.8697788697788698,"Accuracy
(%)
AUC"
EXPERIMENTS,0.8722358722358723,"Edinburgh
1253
9
Ours
5
3
0.04
0.5
84.40
0.847
[12]
5
7
0.02
3.6
91.04
0.958"
EXPERIMENTS,0.8746928746928747,"lbw
189
9
Ours
5
3
0.04
0.4
68.65
0.635
[12]
5
7
0.02
3.3
69.19
0.689"
EXPERIMENTS,0.8771498771498771,"nhanes3
15649
15
Ours
5
3
0.31
3.7
79.22
0.490
[12]
5
7
0.16
7.3
79.22
0.717"
EXPERIMENTS,0.8796068796068796,"pcs
379
9
Ours
5
3
0.04
0.6
64.00
0.720
[12]
5
7
0.02
3.5
68.27
0.740"
EXPERIMENTS,0.8820638820638821,"uis
575
8
Ours
5
3
0.04
0.5
74.43
0.585
[12]
5
7
0.02
3.5
74.44
0.603"
CONCLUSION,0.8845208845208845,"6
Conclusion
237"
CONCLUSION,0.8869778869778869,"In this paper, we proposed a faster gradient variant called quadratic gradient, and implemented
238"
CONCLUSION,0.8894348894348895,"the quadratic-gradient version of NAG in the encrypted domain to train the logistic regression model.
239"
CONCLUSION,0.8918918918918919,"The quadratic gradient presented in this work can be constructed from the Hessian matrix directly,
240"
CONCLUSION,0.8943488943488943,"and thus somehow combines the second-order Newton’s method and the ﬁrst-order gradient (descent)
241"
CONCLUSION,0.8968058968058968,"method together. There is a good chance that quadratic gradient could accelerate other gradient
242"
CONCLUSION,0.8992628992628993,"methods such as Adagrad, Adadelta, RMSprop, Adam [8], AdaMax and Nadam, which is an open
243"
CONCLUSION,0.9017199017199017,"future work.
244"
CONCLUSION,0.9041769041769042,"Also, quadratic gradient might substitute and supersede the line-search method, for example
245"
CONCLUSION,0.9066339066339066,"when using enhanced Adagrad-like methods, and could use gradient descent methods to accelerate
246"
CONCLUSION,0.9090909090909091,"Newton’s method, resulting in super-quadratic algorithms.
247"
REFERENCES,0.9115479115479116,"References
248"
REFERENCES,0.914004914004914,"[1] Allison, P. D. (2008). Convergence failures in logistic regression.
249"
REFERENCES,0.9164619164619164,"[2] Blatt, M., Gusev, A., Polyakov, Y., Rohloff, K., and Vaikuntanathan, V. (2019). Optimized
250"
REFERENCES,0.918918918918919,"homomorphic encryption solution for secure genome-wide association studies. IACR Cryptology
251"
REFERENCES,0.9213759213759214,"ePrint Archive, 2019:223.
252"
REFERENCES,0.9238329238329238,"[3] Böhning, D. and Lindsay, B. G. (1988). Monotonicity of quadratic-approximation algorithms.
253"
REFERENCES,0.9262899262899262,"Annals of the Institute of Statistical Mathematics, 40(4):641–663.
254"
REFERENCES,0.9287469287469288,"[4] Bonte, C. and Vercauteren, F. (2018). Privacy-preserving logistic regression training. BMC
255"
REFERENCES,0.9312039312039312,"medical genomics, 11(4):86.
256"
REFERENCES,0.9336609336609336,"[5] Chen, H., Gilad-Bachrach, R., Han, K., Huang, Z., Jalali, A., Laine, K., and Lauter, K. (2018).
257"
REFERENCES,0.9361179361179361,"Logistic regression over encrypted data from fully homomorphic encryption. BMC medical
258"
REFERENCES,0.9385749385749386,"genomics, 11(4):3–12.
259"
REFERENCES,0.941031941031941,"[6] Cheon, J. H., Kim, A., Kim, M., and Song, Y. (2017). Homomorphic encryption for arithmetic of
260"
REFERENCES,0.9434889434889435,"approximate numbers. In International Conference on the Theory and Application of Cryptology
261"
REFERENCES,0.9459459459459459,"and Information Security, pages 409–437. Springer.
262"
REFERENCES,0.9484029484029484,"[7] Chiang, J. (2022). Quadratic gradient: Uniting gradient algorithm and newton method as one.
263"
REFERENCES,0.9508599508599509,"arXiv preprint arXiv:2209.03282.
264"
REFERENCES,0.9533169533169533,"[8] Crawford, J. L., Gentry, C., Halevi, S., Platt, D., and Shoup, V. (2018). Doing real work with fhe:
265"
REFERENCES,0.9557739557739557,"the case of logistic regression. In Proceedings of the 6th Workshop on Encrypted Computing &
266"
REFERENCES,0.9582309582309583,"Applied Homomorphic Cryptography, pages 1–12.
267"
REFERENCES,0.9606879606879607,"[9] Gentry, C. (2009). Fully homomorphic encryption using ideal lattices. In Proceedings of the
268"
REFERENCES,0.9631449631449631,"forty-ﬁrst annual ACM symposium on Theory of computing, pages 169–178.
269"
REFERENCES,0.9656019656019657,"[10] Han, K., Hong, S., Cheon, J. H., and Park, D. (2019). Logistic regression on homomorphic en-
270"
REFERENCES,0.9680589680589681,"crypted data at scale. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33,
271"
REFERENCES,0.9705159705159705,"pages 9466–9471.
272"
REFERENCES,0.972972972972973,"[11] Jäschke, A. and Armknecht, F. (2016). Accelerating homomorphic computations on rational
273"
REFERENCES,0.9754299754299754,"numbers. In International Conference on Applied Cryptography and Network Security, pages
274"
REFERENCES,0.9778869778869779,"405–423. Springer.
275"
REFERENCES,0.9803439803439803,"[12] Kim, A., Song, Y., Kim, M., Lee, K., and Cheon, J. H. (2018a). Logistic regression model
276"
REFERENCES,0.9828009828009828,"training based on the approximate homomorphic encryption. BMC medical genomics, 11(4):83.
277"
REFERENCES,0.9852579852579852,"[13] Kim, M., Song, Y., Li, B., and Micciancio, D. (2019). Semi-parallel logistic regression for gwas
278"
REFERENCES,0.9877149877149877,"on encrypted data. IACR Cryptology ePrint Archive, 2019:294.
279"
REFERENCES,0.9901719901719902,"[14] Kim, M., Song, Y., Wang, S., Xia, Y., and Jiang, X. (2018b). Secure logistic regression based
280"
REFERENCES,0.9926289926289926,"on homomorphic encryption: Design and evaluation. JMIR medical informatics, 6(2):e19.
281"
REFERENCES,0.995085995085995,"[15] Murphy, K. P. (2012). Machine learning: a probabilistic perspective. The MIT Press, Cam-
282"
REFERENCES,0.9975429975429976,"bridge, MA.
283"
