Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0022222222222222222,"Artificial neural networks (ANNs), despite their universal function approximation
1"
ABSTRACT,0.0044444444444444444,"capability and practical success, are subject to catastrophic forgetting. Catastrophic
2"
ABSTRACT,0.006666666666666667,"forgetting refers to the abrupt unlearning of a previous task when a new task is
3"
ABSTRACT,0.008888888888888889,"learned. It is an emergent phenomenon that plagues ANNs and hinders continual
4"
ABSTRACT,0.011111111111111112,"learning. Existing universal function approximation theorems for ANNs guarantee
5"
ABSTRACT,0.013333333333333334,"function approximation ability, but seldom touch on the model details and do not
6"
ABSTRACT,0.015555555555555555,"predict catastrophic forgetting. This paper presents a novel universal approximation
7"
ABSTRACT,0.017777777777777778,"theorem for multi-variable functions using only single-variable functions and
8"
ABSTRACT,0.02,"exponential functions. Furthermore, we present ATLAS—a novel ANN architecture
9"
ABSTRACT,0.022222222222222223,"based on the exponential approximation theorem and B-splines. It is shown that
10"
ABSTRACT,0.024444444444444446,"ATLAS is a universal function approximator capable of memory retention and,
11"
ABSTRACT,0.02666666666666667,"therefore, continual learning. The memory retention of ATLAS is imperfect,
12"
ABSTRACT,0.028888888888888888,"with some off-target effects during continual learning, but it is well-behaved and
13"
ABSTRACT,0.03111111111111111,"predictable. An efficient implementation of ATLAS is provided. Experiments
14"
ABSTRACT,0.03333333333333333,"are conducted to evaluate both the function approximation and memory retention
15"
ABSTRACT,0.035555555555555556,"capabilities of ATLAS.
16"
INTRODUCTION,0.03777777777777778,"1
Introduction
17"
INTRODUCTION,0.04,"Catastrophic forgetting [7, 13, 23] is an emergent phenomenon where a machine learning model
18"
INTRODUCTION,0.042222222222222223,"such as an artificial neural network (ANN) learns a new task, and the subsequent parameter updates
19"
INTRODUCTION,0.044444444444444446,"interfere with the model’s performance on previously learned tasks. Catastrophic forgetting is also
20"
INTRODUCTION,0.04666666666666667,"called catastrophic interference [19]. If an ANN cannot effectively learn many tasks, it has limited
21"
INTRODUCTION,0.04888888888888889,"utility in the context of continual learning [9, 12]. Catastrophic forgetting is like learning to pick
22"
INTRODUCTION,0.051111111111111114,"up a cup, but simultaneously forgetting how to breathe. Even linear functions are susceptible to
23"
INTRODUCTION,0.05333333333333334,"catastrophic forgetting, as illustrated in Figure 1
24"
INTRODUCTION,0.05555555555555555,Figure 1: A linear function is susceptible to catastrophic forgetting.
INTRODUCTION,0.057777777777777775,"The simple example of a linear regression model being susceptible to catastrophic forgetting might be
25"
INTRODUCTION,0.06,"due to the non-linearity of the target function, noise, or parameter sharing across the input. Parameter
26"
INTRODUCTION,0.06222222222222222,"sharing is avoidable with piece-wise defined functions such as splines [27]. ANNs can be explained
27"
INTRODUCTION,0.06444444444444444,"in many ways; a useful analogy is to compare ANNs to very large lookup tables that store information.
28"
INTRODUCTION,0.06666666666666667,"Removing and updating values has off-target effects throughout the table or ANN.
29"
INTRODUCTION,0.06888888888888889,"Universal function approximation theorems are a cornerstone of machine learning, and prove that
30"
INTRODUCTION,0.07111111111111111,"ANNs can approximate any given continuous target function [10, 11, 15] under certain assumptions.
31"
INTRODUCTION,0.07333333333333333,"The theorems do not specify how to find an ANN with sufficient performance for problems in
32"
INTRODUCTION,0.07555555555555556,"practice. Gradient descent optimisation is the convention for finding/training neural networks, but
33"
INTRODUCTION,0.07777777777777778,"other optimisation and learning procedures exist [22]. ATLAS models trained with gradient descent
34"
INTRODUCTION,0.08,"methods exhibit desirable properties. However, other optimisation techniques like evolutionary
35"
INTRODUCTION,0.08222222222222222,"algorithms may not elicit the same properties.
36"
INTRODUCTION,0.08444444444444445,"This paper introduces ATLAS—a novel universal function approximator based on B-splines that has
37"
INTRODUCTION,0.08666666666666667,"some intrinsic memory retention, even in the absence of other training and regularisation techniques.
38"
INTRODUCTION,0.08888888888888889,"ATLAS has well-behaved parameter gradients that are sparse, bounded and orthogonal between
39"
INTRODUCTION,0.09111111111111111,"input points that are far enough from each other. The accompanying representation and universal
40"
INTRODUCTION,0.09333333333333334,"approximation theorems are also provided.
41"
RELEVANT STUDIES,0.09555555555555556,"2
Relevant Studies
42"
RELEVANT STUDIES,0.09777777777777778,"It is conjectured that overlapping representations in ANNs lead to catastrophic forgetting [12].
43"
RELEVANT STUDIES,0.1,"Catastrophic forgetting occurs when parameters necessary for one task change while training to meet
44"
RELEVANT STUDIES,0.10222222222222223,"the objectives of another task [14, 20]. The least desirable strategy to mitigate catastrophic forgetting
45"
RELEVANT STUDIES,0.10444444444444445,"is retraining a model over all tasks. Regularisation techniques like elastic weight consolidation (EWC)
46"
RELEVANT STUDIES,0.10666666666666667,"have also been employed [14]. Data augmentation approaches such as rehearsal and pseudo-rehearsal
47"
RELEVANT STUDIES,0.10888888888888888,"have also been employed [23]. Other ideas from optimal control theory in combination with dynamic
48"
RELEVANT STUDIES,0.1111111111111111,"programming have also been applied to counteract catastrophic forgetting, with a cost functional
49"
RELEVANT STUDIES,0.11333333333333333,"similar in form to the action integral from physics and Lagrangian mechanics [16].
50"
RELEVANT STUDIES,0.11555555555555555,"Orthogonal Gradient Descent (OGD) is a training augmentation or optimisation technique that
51"
RELEVANT STUDIES,0.11777777777777777,"modifies the gradient updates of subsequent tasks to be orthogonal to previous tasks [6, 1]. One
52"
RELEVANT STUDIES,0.12,"can describe data in terms of a distribution defined over the input space, target values, and time (the
53"
RELEVANT STUDIES,0.12222222222222222,"order of data or tasks that are presented during training). OGD attempts to make gradient updates
54"
RELEVANT STUDIES,0.12444444444444444,"orthogonal to each other over time. ATLAS, in contrast, possesses distal orthogonality, meaning that
55"
RELEVANT STUDIES,0.12666666666666668,"if two inputs are far enough from each other in the input space, then corresponding gradient updates
56"
RELEVANT STUDIES,0.1288888888888889,"will be orthogonal. A corollary of this is that if the data distribution between tasks shifts in the input
57"
RELEVANT STUDIES,0.13111111111111112,"space, then the subsequent gradient updates will tend to be orthogonal. ATLAS does not use external
58"
RELEVANT STUDIES,0.13333333333333333,"memory like OGD. Extensions of OGD include PCA-OGD, which compresses gradient updates into
59"
RELEVANT STUDIES,0.13555555555555557,"principal components to reduce memory requirements [4]. The Neural Tangent Kernel (NTK) overlap
60"
RELEVANT STUDIES,0.13777777777777778,"matrices, as discussed by Doan et al. [4], could be a useful tool for analysing ATLAS models.
61"
RELEVANT STUDIES,0.14,"The survey by Delange et al. [3] gives an extensive overview of continual learning to address
62"
RELEVANT STUDIES,0.14222222222222222,"catastrophic forgetting. ATLAS is a model that implements parameter isolation, because of its use of
63"
RELEVANT STUDIES,0.14444444444444443,"piece-wise defined splines. Particularly relevant to ATLAS is the work on scale of initialisation and
64"
RELEVANT STUDIES,0.14666666666666667,"extreme memorisation [21]. Increasing the density of basis functions in ATLAS can lead to better
65"
RELEVANT STUDIES,0.14888888888888888,"memorisation, and increases the scale of some parameters in ATLAS which may affect generalisation.
66"
RELEVANT STUDIES,0.1511111111111111,"Pi-sigma neural networks use nodes that compute products instead of sums [26]. Pi-sigma neural
67"
RELEVANT STUDIES,0.15333333333333332,"networks have some similarities with the global structure of ATLAS. B-splines, which form the basis
68"
RELEVANT STUDIES,0.15555555555555556,"of ATLAS, have been applied for machine learning [5]. Scardapane et al. [25] investigated trainable
69"
RELEVANT STUDIES,0.15777777777777777,"activation functions parameterised by splines. Uniform cubic B-splines have basis functions that are
70"
RELEVANT STUDIES,0.16,"translates of one another [2]. Uniform cubic B-splines have been tested for memory retention, and
71"
RELEVANT STUDIES,0.1622222222222222,"ATLAS is an improvement on existing spline models [27].
72"
RELEVANT STUDIES,0.16444444444444445,"B-splines, and by extension ATLAS, can be trained to fit lower frequency components, expanded and
73"
RELEVANT STUDIES,0.16666666666666666,"trained again until a network is found with sufficient accuracy and generalisation, similar to other
74"
RELEVANT STUDIES,0.1688888888888889,"techniques [17, 18]. It is not necessary to expand the capacity of an ATLAS model to learn new
75"
RELEVANT STUDIES,0.1711111111111111,"tasks, as with some other approaches [24]. ATLAS does in practice demonstrate something akin to
76"
RELEVANT STUDIES,0.17333333333333334,"""graceful forgetting"" as discussed in Golkar et al. [8].
77"
NOTATION,0.17555555555555555,"3
Notation
78"
NOTATION,0.17777777777777778,"Vector quantities like ⃗x are clearly indicated with a bar or arrow for legibility. Parameters, inputs,
79"
NOTATION,0.18,"functions etc. without a bar or arrow are scalar quantities like S(x). Some scalar quantities with
80"
NOTATION,0.18222222222222223,"indices are the scalar components of a vector like xj or scalar parameters in the model like θi. The
81"
NOTATION,0.18444444444444444,"gradient operator that acts on a scalar function like ⃗∇⃗θA(⃗x) yields a vector-valued function ⃗∇⃗θA(⃗x)
82"
NOTATION,0.18666666666666668,"as is typical of multi-variable calculus.
83"
EXPONENTIAL REPRESENTATION THEOREM,0.18888888888888888,"4
Exponential Representation Theorem
84"
EXPONENTIAL REPRESENTATION THEOREM,0.19111111111111112,"Any continuous multi-variable function on a compact space can be uniformly approximated with
85"
EXPONENTIAL REPRESENTATION THEOREM,0.19333333333333333,"multi-variable polynomials by the Stone-Weierstrass Theorem. Let I denote an index set of tuples of
86"
EXPONENTIAL REPRESENTATION THEOREM,0.19555555555555557,"natural numbers including zero such that ij ∈N0 for all j ∈N with i = (i1, .., in) ∈I and ai ∈R.
87"
EXPONENTIAL REPRESENTATION THEOREM,0.19777777777777777,"Multi-variable polynomials can be represented as:
88"
EXPONENTIAL REPRESENTATION THEOREM,0.2,"y(⃗x) = y(x1, .., xn) =
X"
EXPONENTIAL REPRESENTATION THEOREM,0.20222222222222222,"i∈I
aixi1
1 xi2
2 ...xin
n =
X"
EXPONENTIAL REPRESENTATION THEOREM,0.20444444444444446,"i∈I
aiΠn
j=1xij
j"
EXPONENTIAL REPRESENTATION THEOREM,0.20666666666666667,"Each monomial term aiΠn
j=1xij
j is a product of single-variable functions in each variable. It is
89"
EXPONENTIAL REPRESENTATION THEOREM,0.2088888888888889,"desirable to rewrite products as sums using exponentials and logarithms.
90"
EXPONENTIAL REPRESENTATION THEOREM,0.2111111111111111,"Lemma 1. For any ai ∈R, there exists γi > 0 and βi > 0, such that: ai = γi −βi
91"
EXPONENTIAL REPRESENTATION THEOREM,0.21333333333333335,"Theorem 1 (Exponential representation theorem). Any multi-variable polynomial function y(⃗x)
92"
EXPONENTIAL REPRESENTATION THEOREM,0.21555555555555556,"of n variables over the positive orthant, can be exactly represented by continuous single-variable
93"
EXPONENTIAL REPRESENTATION THEOREM,0.21777777777777776,"functions gi,j(xj) and hi,j(xj) in the form:
94"
EXPONENTIAL REPRESENTATION THEOREM,0.22,"y(⃗x) =
X"
EXPONENTIAL REPRESENTATION THEOREM,0.2222222222222222,"i∈I
exp
 
Σn
j=1gi,j(xj)

−exp
 
Σn
j=1hi,j(xj)
"
EXPONENTIAL REPRESENTATION THEOREM,0.22444444444444445,"Proof. Consider any monomial term aiΠn
j=1xij
j with ai ∈R, then by Lemma 1 there exist strictly
95"
EXPONENTIAL REPRESENTATION THEOREM,0.22666666666666666,"positive numbers γi > 0 and βi > 0, such that:
96"
EXPONENTIAL REPRESENTATION THEOREM,0.2288888888888889,"aiΠn
j=1xij
j = γiΠn
j=1xij
j −βiΠn
j=1xij
j"
EXPONENTIAL REPRESENTATION THEOREM,0.2311111111111111,"= exp

log

γiΠn
j=1xij
j

−exp

log

βiΠn
j=1xij
j
"
EXPONENTIAL REPRESENTATION THEOREM,0.23333333333333334,"= exp

log(γi) + Σn
j=1 log

xij
j

−exp

log(βi) + Σn
j=1 log

xij
j
"
EXPONENTIAL REPRESENTATION THEOREM,0.23555555555555555,"The argument of each exponential function is a sum of single-variable functions and constants.
97"
EXPONENTIAL REPRESENTATION THEOREM,0.23777777777777778,"Without loss of generality, a set of single-variable functions can be defined such that:
98"
EXPONENTIAL REPRESENTATION THEOREM,0.24,"aiΠn
j=1xij
j = exp
 
Σn
j=1gi,j(xj)

−exp
 
Σn
j=1hi,j(xj)
"
EXPONENTIAL REPRESENTATION THEOREM,0.24222222222222223,"Since this holds for any aiΠn
j=1xij
j and all i ∈I, it follows that:
99"
EXPONENTIAL REPRESENTATION THEOREM,0.24444444444444444,"y(⃗x) =
X"
EXPONENTIAL REPRESENTATION THEOREM,0.24666666666666667,"i∈I
exp
 
Σn
j=1gi,j(xj)

−exp
 
Σn
j=1hi,j(xj)
 100"
EXPONENTIAL REPRESENTATION THEOREM,0.24888888888888888,"This result is fundamental to the paper. Since every continuous function can be approximated with
101"
EXPONENTIAL REPRESENTATION THEOREM,0.2511111111111111,"multi-variable polynomials, it follows that every continuous function can be approximated with
102"
EXPONENTIAL REPRESENTATION THEOREM,0.25333333333333335,"positive and negative exponential functions. Single-variable function approximators are pivotal and
103"
EXPONENTIAL REPRESENTATION THEOREM,0.25555555555555554,"must be reconsidered. Universal function approximation can also be proven with the sub-algebra
104"
EXPONENTIAL REPRESENTATION THEOREM,0.2577777777777778,"formulation of the Stone-Weierstrass theorem, but it’s not as delightful and simple as the first
105"
EXPONENTIAL REPRESENTATION THEOREM,0.26,"constructive proof given above.
106"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.26222222222222225,"5
Single-Variable Function Approximation
107"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.2644444444444444,"Splines are piece-wise defined single-variable functions over some interval. Each sub-interval of a
108"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.26666666666666666,"spline is most often locally given by a low degree polynomial, even though the global structure is not
109"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.2688888888888889,"a low degree polynomial. B-splines are polynomial splines that are defined in a way that resembles
110"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.27111111111111114,"other basis function formulations [2]. Each single-variable function in ATLAS is approximated
111"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.2733333333333333,"with uniform cubic B-spline basis functions, shown in Figure 2. B-splines can approximate any
112"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.27555555555555555,"single-variable function, similar to using the Fourier basis. With uniform B-splines, each basis
113"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.2777777777777778,"function is scaled so that the unit interval is uniformly partitioned, as in Figure 2.
114"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.28,"(a) ρ = 0
(b) ρ = 1
(c) ρ = 2"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.2822222222222222,"Figure 2: If uniformly spaced B-splines are used, then each basis function has the same shape. This
makes it possible to use the same activation function by scaling and translating the inputs. This is
also true for different densities of uniform cubic B-splines."
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.28444444444444444,"The activation function to implement B-splines is given by:
115"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.2866666666666667,S(x) =
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.28888888888888886,"





"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.2911111111111111,"




"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.29333333333333333,"1
6x3
0 ≤x < 1
1
6

−3(x −1)3 + 3(x −1)2 + 3(x −1) + 1

1 ≤x < 2
1
6

3(x −2)3 −6(x −2)2 + 4

2 ≤x < 3
1
6(4 −x)3
3 ≤x < 4
0
otherwise"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.29555555555555557,"The choice was made to use uniform cubic B-splines due to their excellent performance and robustness
116"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.29777777777777775,"to catastrophic forgetting, illustrated in Figure 3. Using uniform B-splines instead of arbitrary sub-
117"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3,"interval partitions (also called knots in literature) makes optimisation easier. Optimising partitions is
118"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3022222222222222,"non-linear, but optimising only coefficient (also called control points) is linear and thus convex.
119"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.30444444444444446,"(a) The activation is sparse and bounded.
(b) Distal orthogonality."
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.30666666666666664,Figure 3: Single-variable function
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3088888888888889,"Each basis function is multiplied by a parameter and summed together. The total number of basis
120"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3111111111111111,"functions is typically fixed. Cubic B-splines are 3rd order polynomials, and thus require a minimum
121"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.31333333333333335,"of 3 + 1 = 4 control points or basis functions.
122"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.31555555555555553,"Instead of considering arbitrary densities of uniform cubic B-splines, we look at powers of two times
123"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.31777777777777777,"the minimum number of basis functions, called ρ-density B-spline functions.
124"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.32,"Definition 1 (ρ-density B-spline function). A ρ-density B-spline function is a uniform cubic B-spline
125"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.32222222222222224,"function with 2ρ+2 basis functions:
126"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3244444444444444,f(x) =
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.32666666666666666,"2ρ+2
X"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3288888888888889,"i=1
θiSi(x) ="
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.33111111111111113,"2ρ+2
X"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3333333333333333,"i=1
θiS(wix + bi) ="
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.33555555555555555,"2ρ+2
X"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3377777777777778,"i=1
θiS((2ρ+2 −3)x + 4 −i)"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.34,"Consider the problem of expanding a single-variable function approximator with more basis functions
127"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3422222222222222,"to increase its expressive power. Using the Fourier basis makes it trivially easy by adding higher
128"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.34444444444444444,"frequency sines and cosines with coefficients initialised to zero. It is trickier to achieve something
129"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3466666666666667,"similar with uniform cubic B-splines. There are algorithms for creating new splines from existing
130"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3488888888888889,"splines with knot insertion, but the intermediate steps result in non-uniform knots and splines. A
131"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3511111111111111,"simple and practical compromise that we propose is to use mixtures of different ρ-density B-spline
132"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.35333333333333333,"functions, as illustrated in Figure 2.
133"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.35555555555555557,"Definition 2 (mixed-density B-spline function). A mixed-density B-spline function is a single-
134"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.35777777777777775,"variable function approximator that is obtained by summing together different ρ-density B-spline
135"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.36,"functions. Only the maximum ρ-density B-spline function has trainable parameters, the others are
136"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3622222222222222,"constant. Mixed-density B-spline functions are of the form:
137"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.36444444444444446,"f(x) = r
X ρ=0"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.36666666666666664,"2ρ+2
X"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3688888888888889,"i=1
θρ,iSρ,i(x)"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3711111111111111,"Only the maximum r = ρ-density B-spline has trainable coefficients. All lower density r > ρ-
138"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.37333333333333335,"density B-spline have frozen and constant coefficients. The maximum r = ρ-density B-spline has
139"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.37555555555555553,"trainable coefficients with gradient updates that are orthogonal if the distance between two inputs is
140"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.37777777777777777,"large enough.
141"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.38,"Similar to increasing the expressiveness of a Fourier basis function approximator by adding higher
142"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.38222222222222224,"frequency terms, one can add larger density cubic B-spline functions. Analytically, we can initialise
143"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3844444444444444,"all the new scalar parameters θr+1,i = 0, ∀i ∈N such that:
144"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.38666666666666666,"f(x) = r
X ρ=0"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3888888888888889,"2ρ+2
X"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.39111111111111113,"i=1
θρ,iSρ,i(x) = r+1
X ρ=0"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3933333333333333,"2ρ+2
X"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.39555555555555555,"i=1
θρ,iSρ,i(x)"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.3977777777777778,"It is therefore possible to create a minimal model with r = 0 initialised at zero, and train the model
145"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.4,"until convergence. Then one can create a new model with r = 1, by subsuming the previous model’s
146"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.4022222222222222,"parameters, and train this more expressive model until convergence. This process of training and
147"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.40444444444444444,"expansion can be continued indefinitely, and is shown in Figure 8.
148"
SINGLE-VARIABLE FUNCTION APPROXIMATION,0.4066666666666667,Figure 4: Doubling densities of basis functions before and after training.
ATLAS,0.4088888888888889,"6
ATLAS
149"
ATLAS,0.4111111111111111,"ATLAS is named for carrying the burden of all it must remember, after the Titan god Atlas in Greek
150"
ATLAS,0.41333333333333333,"mythology who was tasked with holding the weight of the world. ATLAS is also an acronym for
151"
ATLAS,0.41555555555555557,"AddiTive exponentiaL Additive Splines.
152"
ATLAS,0.4177777777777778,"Definition 3 (ATLAS). ATLAS is a function approximator of n variables, with mixed-density
153"
ATLAS,0.42,"B-spline functions fj(xj), gi,j(xj), and hi,j(xj) in the form:
154"
ATLAS,0.4222222222222222,"A(⃗x) := n
X"
ATLAS,0.42444444444444446,"j=1
fj(xj) + M
X k=1"
ATLAS,0.4266666666666667,"1
k2 exp
 
Σn
j=1gk,j(xj)

−1"
ATLAS,0.4288888888888889,"k2 exp
 
Σn
j=1hk,j(xj)
"
ATLAS,0.4311111111111111,"ATLAS is equivalently given by the compact notation:
155"
ATLAS,0.43333333333333335,"A(⃗x) := F(⃗x) + M
X k=1"
ATLAS,0.43555555555555553,"1
k2 exp(Gk(⃗x)) −1"
ATLAS,0.43777777777777777,k2 exp(Hk(⃗x))
ATLAS,0.44,"The absolutely convergent series of scale factors k−2 was chosen for numerical stability and to ensure
156"
ATLAS,0.44222222222222224,"the model is absolutely convergent. Another feature is that the series of scale factors also breaks the
157"
ATLAS,0.4444444444444444,"symmetry that would otherwise exist if all mixed-density B-spline functions were initialised to zero.
158"
ATLAS,0.44666666666666666,"Initialising all the parameters to be zero is a departure from the conventional approach of random
159"
ATLAS,0.4488888888888889,"initialisation. The number of exponential terms can be increased without changing the output of the
160"
ATLAS,0.45111111111111113,"model. We can choose to initialise GM+1(⃗x) = 0 and HM+1(⃗x) = 0, such that the model capacity
161"
ATLAS,0.4533333333333333,"can be increased at will.
162"
ATLAS,0.45555555555555555,"ATLAS is a universal function approximator with some inherent memory retention. It possesses three
163"
ATLAS,0.4577777777777778,"properties atypical of most universal function approximators:
164"
ATLAS,0.46,"1. The activity within ATLAS is sparse – most neural units are zero and inactive.
165"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4622222222222222,"2. The gradient vector with respect to trainable parameters is bounded regardless of the size
166"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.46444444444444444,"and capacity of the model, so training is numerically stable for many possible training
167"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4666666666666667,"hyper-parameters.
168"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4688888888888889,"3. Inputs that are sufficiently far from each other have orthogonal representations.
169"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4711111111111111,"The proofs of the three properties follows from the single-variable case, the assumption of bounded
170"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.47333333333333333,"single-variable functions and parameters, and the absolutely convergent k−2 scale factors.
171"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.47555555555555556,"Property 1 (Sparsity). For any ⃗x ∈D(A) ⊂Rn and bounded trainable parameters θi with index
172"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4777777777777778,"set Θ, the gradient vector of trainable parameters (for ATLAS) is sparse:
173"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.48,"⃗∇⃗θA(⃗x)

0 =
X"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4822222222222222,"i∈Θ
dHamming ∂A"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.48444444444444446,"∂θi
(⃗x), 0

≤4n(2M + 1)"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4866666666666667,"Remark. For a fixed number of variables n, the model has a total of n2r+2(2M + 1) trainable
174"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4888888888888889,"parameters. The gradient vector has a maximum of 4n(2M+1) non-zero entries, which is independent
175"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4911111111111111,"of r. Recall that only the maximum density (ρ = r) cubic B-spline function has trainable parameters.
176"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.49333333333333335,"The fraction of trainable basis functions that are active is at most 2−r. Sparsity entails efficient
177"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.4955555555555556,"implementation, and suggests possible memory retention and robustness to catastrophic forgetting.
178"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.49777777777777776,"Property 2 (Gradient flow attenuation). For any ⃗x ∈D(A) ⊂Rn and bounded trainable parameters
179"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5,"θi with index set Θ: if all the mixed-density B-spline functions are bounded, then the gradient vector
180"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5022222222222222,"of trainable parameters for ATLAS is bounded:
181"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5044444444444445,"⃗∇⃗θA(⃗x)

1 =
X i∈Θ"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5066666666666667,"∂A
∂θi
(⃗x)
 < U"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5088888888888888,"Remark. For a fixed number of variables n, the model has a total of n2r+2(2M + 1) trainable
182"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5111111111111111,"parameters. The factor of k−2 inside the expression for ATLAS is necessary to ensure the sum is
183"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5133333333333333,"convergent in the limit of infinitely many exponential terms M →∞. Only the maximum density
184"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5155555555555555,"(ρ = r) cubic B-spline function has trainable parameters, so that the gradient vector is bounded in
185"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5177777777777778,"the limit of arbitrarily large densities r →∞. Smaller densities cannot be trainable, otherwise this
186"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.52,"property does not hold. The bounded gradient vector implies that ATLAS is numerically stable during
187"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5222222222222223,"training, regardless of its size or parameter count.
188"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5244444444444445,"Property 3 (Distal orthogonality). For any ⃗x, ⃗y ∈D(A) ⊂Rn and bounded trainable parameters
189"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5266666666666666,"θi for an ATLAS model A(⃗x):
190"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5288888888888889,"min
j=1,...,n{|xj −yj|} > 2−r =⇒⟨⃗∇⃗θA(⃗x), ⃗∇⃗θA(⃗y)⟩= 0"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5311111111111111,"Remark. Two points that sufficiently differ in each input variable have orthogonal parameter gradients.
191"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5333333333333333,"Distal orthogonality means ATLAS is reasonably robust to catastrophic forgetting, without other
192"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5355555555555556,"regularisation and training techniques. However, memory retention can still potentially be improved
193"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5377777777777778,"when used in conjunction with other techniques.
194"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.54,"ATLAS can be implemented with 1D convolution, reshaping, embedding, multiplication and dense
195"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5422222222222223,"layers. The same basis functions have to be computed for each input variable, hence 1D convolutions.
196"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5444444444444444,"By correctly scaling, shifting, and rounding inputs one can compute only the non-zero basis functions
197"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5466666666666666,"with embedding layers. The number of basis functions are chosen from powers of two for convenience,
198"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5488888888888889,"with the maximum density B-spline function having exactly λ = 4 × 2r basis functions. Summing
199"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5511111111111111,"over all densities the total number of all basis functions in each input variable is at most 2λ, because
200"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5533333333333333,"a geometric series was used. For every output dimension p, there are 2M exponentials. Each
201"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5555555555555556,"exponential has n single variable functions, with at most 2λ cubic B-spline basis functions each.
202"
THE GRADIENT VECTOR WITH RESPECT TO TRAINABLE PARAMETERS IS BOUNDED REGARDLESS OF THE SIZE,0.5577777777777778,"ATLAS models have time complexity O(pMn log λ), and O(pMnλ) space complexity.
203"
METHODOLOGY,0.56,"7
Methodology
204"
METHODOLOGY,0.5622222222222222,"The 1-,2- and 8-dimensional models were considered for evaluation, in combination with a chosen
205"
METHODOLOGY,0.5644444444444444,"width for the update region in Task 2 from 0.1 to 0.9 in 0.1 increments. 30 trials were performed for
206"
METHODOLOGY,0.5666666666666667,"each combination of model dimension and update region width. Mean Absolute Error (MAE) loss
207"
METHODOLOGY,0.5688888888888889,"function, the Adam optimiser, and mini batch sizes of 100 are used throughout all experiments.
208"
METHODOLOGY,0.5711111111111111,"At the beginning of each trial (for a given dimension and update region width) a random learning rate
209"
METHODOLOGY,0.5733333333333334,"was sampled uniformly between 10−6 and 0.01 + 10−6. A random noise level was sampled from an
210"
METHODOLOGY,0.5755555555555556,"exponential distribution with scale parameter equal to one. The Task 1 target function is constructed
211"
METHODOLOGY,0.5777777777777777,"from 1000 Euclidean radial basis functions (RBFs) with locations chosen uniformly over the entire
212"
METHODOLOGY,0.58,"input domain, with RBF scale parameters sampled independently from an exponential distribution
213"
METHODOLOGY,0.5822222222222222,"(scale parameter equal to 10). The weights of each radial basis function are sampled from a normal
214"
METHODOLOGY,0.5844444444444444,"distribution with mean zero and standard deviation equal to one. The Task 2 target function is exactly
215"
METHODOLOGY,0.5866666666666667,"the same as the Task 1 target function – except for a square-like region with width equal to update
216"
METHODOLOGY,0.5888888888888889,"region width. The location of the update region is chosen uniformly at random, and such that it is
217"
METHODOLOGY,0.5911111111111111,"completely inside the domain of the model. The updated region masks the Task 1 target function and
218"
METHODOLOGY,0.5933333333333334,"instead replaces the values inside it with another function that is sampled from the same distribution
219"
METHODOLOGY,0.5955555555555555,"as the Task 1 target function, but independently from the Task 1 target function.
220"
METHODOLOGY,0.5977777777777777,"After the generation of the target functions 10000 data points are sampled for training, validation, and
221"
METHODOLOGY,0.6,"test sets for Task 1 and Task 2. To simulate the effect of learning unrelated tasks, the training data for
222"
METHODOLOGY,0.6022222222222222,"Task 2 is only sampled from update region - with no training data outside of it being presented again,
223"
METHODOLOGY,0.6044444444444445,"by contrast the validation and test sets for Task 2 were sampled over the entire input domain. Gaussian
224"
METHODOLOGY,0.6066666666666667,"noise with standard deviation equal to the randomly chosen noise level is added to all training data.
225"
METHODOLOGY,0.6088888888888889,"An ATLAS model (M = 10 positive and M = 10 negative exponential functions, maximum basis
226"
METHODOLOGY,0.6111111111111112,"function density r = 4) with guaranteed distal orthogonality is trained and evaluated on Task 1
227"
METHODOLOGY,0.6133333333333333,"and Task 2. Then a modified ATLAS model (M = 10 positive and M = 10 negative exponential
228"
METHODOLOGY,0.6155555555555555,"functions, maximum basis function density r = 4, trainable lower density basis functions) without
229"
METHODOLOGY,0.6177777777777778,"guaranteed distal orthogonality is trained and evaluated on Task 1 and Task 2 using the same data sets
230"
METHODOLOGY,0.62,"as previously mentioned model. The final test errors for Task 2 are presented. A randomly selected
231"
METHODOLOGY,0.6222222222222222,"trial of the 2-dimensional case is shown for visual inspection. The experiments presented in the main
232"
METHODOLOGY,0.6244444444444445,"body of the paper were performed on Google Colab and the relevant code is provided.
233"
RESULTS,0.6266666666666667,"8
Results
234"
RESULTS,0.6288888888888889,"As shown in Figure 5 the effect of distal orthogonality is clear and crisp boundaries that limit the
235"
RESULTS,0.6311111111111111,"effect of Task 2 on the memory of Task 1. Without distal orthogonality there are more off-target
236"
RESULTS,0.6333333333333333,"effects that can be visualised.
237"
RESULTS,0.6355555555555555,"(a) Guaranteed distal orthogonality, Off-target effects deviate from Task 2 target."
RESULTS,0.6377777777777778,"(b) No guaranteed distal orthogonality, Off-target effects deviate from Task 2 target."
RESULTS,0.64,Figure 5: A randomly chosen trial is presented for visual inspection.
RESULTS,0.6422222222222222,Figure 6: Distal orthogonality guaranteed: All validation MAE curves for Task 1.
RESULTS,0.6444444444444445,Figure 7: No distal orthogonality: Task 2 validation MAE with update region width δ = 0.1.
RESULTS,0.6466666666666666,Figure 8: Distal orthogonality guaranteed: Task 2 validation MAE with update region width δ = 0.1.
RESULTS,0.6488888888888888,"The effect of distal orthogonality on the averaged MAE for various trials for 1-,2- and 8-dimensional
238"
RESULTS,0.6511111111111111,"problems are presented as scatter plots of the averaged MAE over 30 trials for different update region
239"
RESULTS,0.6533333333333333,"widths as shown in Figure 9. The expected off-target error depends on the dimension of the problem
240"
RESULTS,0.6555555555555556,"and the width of the updated regions.
241"
RESULTS,0.6577777777777778,"Analytical results to the expected off-target error require simplification, but a reasonable assumption
242"
RESULTS,0.66,"in the absence of other evidence is that each input dimension has equal contribution on the unit
243"
RESULTS,0.6622222222222223,"hyper-cube. Assume for a fixed input dimension n and some region of width 0 < δ < 1 where the
244"
RESULTS,0.6644444444444444,"(a) The 1-dimensional model.
(b) The 2-dimensional model.
(c) The 8-dimensional model."
RESULTS,0.6666666666666666,"Figure 9: The effect of distal orthogonality on the final test error on task 2 for the 1-,2- and 8-
dimensional input."
RESULTS,0.6688888888888889,"target function Y is changed such that |∆Y | = 1 is one larger than it was originally. The expected
245"
RESULTS,0.6711111111111111,off-target error depends on k the number of input variables inside the updated region: εk ≈n−k
RESULTS,0.6733333333333333,"n . To
246"
RESULTS,0.6755555555555556,"correctly account for all permutations with the same magnitude of change:
247"
RESULTS,0.6777777777777778,"p(εk) =
n
k"
RESULTS,0.68,"
δn−k (1 −δ)k"
RESULTS,0.6822222222222222,"One can calculate expected change values:
248"
RESULTS,0.6844444444444444,"E[ε] = n
X"
RESULTS,0.6866666666666666,"k=0
εkp(εk) ≈ n
X k=0 n −k n"
RESULTS,0.6888888888888889," n
k"
RESULTS,0.6911111111111111,"
δn−k (1 −δ)k = δ"
RESULTS,0.6933333333333334,"However if one assumes that the target function inside the updated region of width δ is correct, with
249"
RESULTS,0.6955555555555556,"probability δn of sampling from the entire input-domain, then the expected off-target error should be:
250"
RESULTS,0.6977777777777778,Expected off-target error ≈δ −δn
RESULTS,0.7,"This seems consistent with some of the experimental results, but further investigation is needed.
251"
CONCLUSION,0.7022222222222222,"9
Conclusion
252"
CONCLUSION,0.7044444444444444,"The main contribution of the paper is theoretical and technical. A representation theorem is presented
253"
CONCLUSION,0.7066666666666667,"that outlines how to approximate multi-variable functions with single-variable functions (splines and
254"
CONCLUSION,0.7088888888888889,"exponential functions). ATLAS approximates all arbitrary single-variable functions with mixtures
255"
CONCLUSION,0.7111111111111111,"of B-spline functions. ATLAS is constructed in such a way that the gradient vector with respect
256"
CONCLUSION,0.7133333333333334,"to trainable parameters is bounded, regardless of how large an ATLAS model is. The activation of
257"
CONCLUSION,0.7155555555555555,"units in ATLAS is sparse, and allowed for an efficient implementation that only computes non-zero
258"
CONCLUSION,0.7177777777777777,"activation values with the aid of embedding layers. The gradient update vector with respect to
259"
CONCLUSION,0.72,"trainable parameters is orthogonal for different inputs as long as the inputs are sufficiently different
260"
CONCLUSION,0.7222222222222222,"from each other.
261"
CONCLUSION,0.7244444444444444,"For every output dimension p in an ATLAS model, there are 2M exponentials. Each exponential has
262"
CONCLUSION,0.7266666666666667,"n single variable functions, with at most 2λ cubic B-spline basis functions each. ATLAS models
263"
CONCLUSION,0.7288888888888889,"have time complexity O(pMn log λ), and O(pMnλ) space complexity.
264"
CONCLUSION,0.7311111111111112,"ATLAS was shown to exhibit some memory retention, without the assistance of other techniques.
265"
CONCLUSION,0.7333333333333333,"This is a good indication of the potential for combining it with other techniques and models for
266"
CONCLUSION,0.7355555555555555,"continual learning. The chosen experiments demonstrated the theoretically derived predictions and
267"
CONCLUSION,0.7377777777777778,"contrasted two models, incuding a variant of ATLAS without distal orthogonality guarantees.
268"
CONCLUSION,0.74,"As far as societal impacts are concerned: It is possible that ATLAS could allow for the creation of
269"
CONCLUSION,0.7422222222222222,"more powerful machine learning algorithms, that require less resources to train and deploy. Further
270"
CONCLUSION,0.7444444444444445,"testing is needed to make any concrete claim.
271"
REFERENCES,0.7466666666666667,"References
272"
REFERENCES,0.7488888888888889,"[1] M. A. Bennani, T. Doan, and M. Sugiyama. Generalisation guarantees for continual learn-
273"
REFERENCES,0.7511111111111111,"ing with orthogonal gradient descent, 2021. URL https://openreview.net/forum?id=
274"
REFERENCES,0.7533333333333333,"hecuSLbL_vC.
275"
REFERENCES,0.7555555555555555,"[2] K. Branson. A practical review of uniform b-splines, 2004.
276"
REFERENCES,0.7577777777777778,"[3] M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and
277"
REFERENCES,0.76,"T. Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE
278"
REFERENCES,0.7622222222222222,"Transactions on Pattern Analysis and Machine Intelligence, pages 1–1, 2021. doi: 10.1109/
279"
REFERENCES,0.7644444444444445,"tpami.2021.3057446. URL https://doi.org/10.1109%2Ftpami.2021.3057446.
280"
REFERENCES,0.7666666666666667,"[4] T. Doan, M. Abbana Bennani, B. Mazoure, G. Rabusseau, and P. Alquier. A theoretical analysis
281"
REFERENCES,0.7688888888888888,"of catastrophic forgetting through the ntk overlap matrix. In A. Banerjee and K. Fukumizu, edi-
282"
REFERENCES,0.7711111111111111,"tors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics,
283"
REFERENCES,0.7733333333333333,"volume 130 of Proceedings of Machine Learning Research, pages 1072–1080. PMLR, 13–15
284"
REFERENCES,0.7755555555555556,"Apr 2021. URL https://proceedings.mlr.press/v130/doan21a.html.
285"
REFERENCES,0.7777777777777778,"[5] A. S. Douzette. B-splines in machine learning. Master’s thesis, Department of Mathematics,
286"
REFERENCES,0.78,"University of Oslo, 2017.
287"
REFERENCES,0.7822222222222223,"[6] M. Farajtabar, N. Azizan, A. Mott, and A. Li. Orthogonal gradient descent for continual
288"
REFERENCES,0.7844444444444445,"learning. In S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International
289"
REFERENCES,0.7866666666666666,"Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine
290"
REFERENCES,0.7888888888888889,"Learning Research, pages 3762–3773. PMLR, 26–28 Aug 2020. URL https://proceedings.
291"
REFERENCES,0.7911111111111111,"mlr.press/v108/farajtabar20a.html.
292"
REFERENCES,0.7933333333333333,"[7] R. M. French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences,
293"
REFERENCES,0.7955555555555556,"3(4):128–135, 1999.
294"
REFERENCES,0.7977777777777778,"[8] S. Golkar, M. Kagan, and K. Cho. Continual learning via neural pruning, 2019. URL https:
295"
REFERENCES,0.8,"//arxiv.org/abs/1903.04476.
296"
REFERENCES,0.8022222222222222,"[9] R. Hadsell, D. Rao, A. A. Rusu, and R. Pascanu. Embracing change: Continual learning in deep
297"
REFERENCES,0.8044444444444444,"neural networks. Trends in cognitive sciences, 24(12):1028–1040, 2020.
298"
REFERENCES,0.8066666666666666,"[10] B. Hanin. Universal function approximation by deep neural nets with bounded width and relu
299"
REFERENCES,0.8088888888888889,"activations. Mathematics, 7(10):992, 2019.
300"
REFERENCES,0.8111111111111111,"[11] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal
301"
REFERENCES,0.8133333333333334,"approximators. Neural networks, 2(5):359–366, 1989.
302"
REFERENCES,0.8155555555555556,"[12] P. Kaushik, A. Gain, A. Kortylewski, and A. Yuille. Understanding catastrophic forgetting and
303"
REFERENCES,0.8177777777777778,"remembering in continual learning with optimal relevance mapping. CoRR, abs/2102.11343,
304"
REFERENCES,0.82,"2021. URL https://arxiv.org/abs/2102.11343.
305"
REFERENCES,0.8222222222222222,"[13] R. Kemker, M. McClure, A. Abitino, T. Hayes, and C. Kanan. Measuring catastrophic forgetting
306"
REFERENCES,0.8244444444444444,"in neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,
307"
REFERENCES,0.8266666666666667,"2018.
308"
REFERENCES,0.8288888888888889,"[14] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
309"
REFERENCES,0.8311111111111111,"J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and
310"
REFERENCES,0.8333333333333334,"R. Hadsell.
Overcoming catastrophic forgetting in neural networks.
Proceedings of the
311"
REFERENCES,0.8355555555555556,"National Academy of Sciences, 114(13):3521–3526, 2017. ISSN 0027-8424. doi: 10.1073/pnas.
312"
REFERENCES,0.8377777777777777,"1611835114. URL https://www.pnas.org/content/114/13/3521.
313"
REFERENCES,0.84,"[15] A. Kratsios. The universal approximation property: Characterization, construction, representa-
314"
REFERENCES,0.8422222222222222,"tion, and existence. Annals of Mathematics and Artificial Intelligence, 89(5):435–469, 2021.
315"
REFERENCES,0.8444444444444444,"doi: 10.1007/s10472-020-09723-1.
316"
REFERENCES,0.8466666666666667,"[16] R. Krishnan and P. Balaprakash. Meta continual learning via dynamic programming, 2020.
317"
REFERENCES,0.8488888888888889,"URL https://arxiv.org/abs/2008.02219.
318"
REFERENCES,0.8511111111111112,"[17] S. H. Lane, M. Flax, D. Handelman, and J. Gelfand. Multi-layer perceptrons with b-spline
319"
REFERENCES,0.8533333333333334,"receptive field functions. In Advances in Neural Information Processing Systems, pages 684–692,
320"
REFERENCES,0.8555555555555555,"1991.
321"
REFERENCES,0.8577777777777778,"[18] A. C. Li and D. Pathak. Functional regularization for reinforcement learning via learned fourier
322"
REFERENCES,0.86,"features, 2021. URL https://arxiv.org/abs/2112.03257.
323"
REFERENCES,0.8622222222222222,"[19] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
324"
REFERENCES,0.8644444444444445,"sequential learning problem. In Psychology of learning and motivation, volume 24, pages
325"
REFERENCES,0.8666666666666667,"109–165. Elsevier, 1989.
326"
REFERENCES,0.8688888888888889,"[20] K. McRae and P. A. Hetherington. Catastrophic interference is eliminated in pretrained networks.
327"
REFERENCES,0.8711111111111111,"In Proceedings of the 15h Annual Conference of the Cognitive Science Society, pages 723–728,
328"
REFERENCES,0.8733333333333333,"1993.
329"
REFERENCES,0.8755555555555555,"[21] H. Mehta, A. Cutkosky, and B. Neyshabur. Extreme memorization via scale of initialization,
330"
REFERENCES,0.8777777777777778,"2020. URL https://arxiv.org/abs/2008.13363.
331"
REFERENCES,0.88,"[22] A. Meulemans, F. Carzaniga, J. Suykens, J. a. Sacramento, and B. F. Grewe. A theoretical
332"
REFERENCES,0.8822222222222222,"framework for target propagation. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
333"
REFERENCES,0.8844444444444445,"H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20024–
334"
REFERENCES,0.8866666666666667,"20036. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/
335"
REFERENCES,0.8888888888888888,"2020/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf.
336"
REFERENCES,0.8911111111111111,"[23] A. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):
337"
REFERENCES,0.8933333333333333,"123–146, 1995.
338"
REFERENCES,0.8955555555555555,"[24] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu,
339"
REFERENCES,0.8977777777777778,"R. Pascanu, and R. Hadsell. Progressive neural networks, 2016. URL https://arxiv.org/
340"
REFERENCES,0.9,"abs/1606.04671.
341"
REFERENCES,0.9022222222222223,"[25] S. Scardapane, M. Scarpiniti, D. Comminiello, and A. Uncini. Learning activation functions
342"
REFERENCES,0.9044444444444445,"from data using cubic spline interpolation. In Italian Workshop on Neural Nets, pages 73–83.
343"
REFERENCES,0.9066666666666666,"Springer, 2017.
344"
REFERENCES,0.9088888888888889,"[26] Y. Shin and J. Ghosh. The pi-sigma network: an efficient higher-order neural network for pattern
345"
REFERENCES,0.9111111111111111,"classification and function approximation. In IJCNN-91-Seattle International Joint Conference
346"
REFERENCES,0.9133333333333333,"on Neural Networks, volume i, pages 13–18 vol.1, 1991. doi: 10.1109/IJCNN.1991.155142.
347"
REFERENCES,0.9155555555555556,"[27] H. van Deventer, P. J. van Rensburg, and A. Bosman. Kasam: Spline additive models for
348"
REFERENCES,0.9177777777777778,"function approximation, 2022. URL https://arxiv.org/abs/2205.06376.
349"
REFERENCES,0.92,"Checklist
350"
REFERENCES,0.9222222222222223,"1. For all authors...
351"
REFERENCES,0.9244444444444444,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
352"
REFERENCES,0.9266666666666666,"contributions and scope? [Yes]
353"
REFERENCES,0.9288888888888889,"(b) Did you describe the limitations of your work? [Yes]
354"
REFERENCES,0.9311111111111111,"(c) Did you discuss any potential negative societal impacts of your work? [Yes]
355"
REFERENCES,0.9333333333333333,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
356"
REFERENCES,0.9355555555555556,"them? [Yes]
357"
REFERENCES,0.9377777777777778,"2. If you are including theoretical results...
358"
REFERENCES,0.94,"(a) Did you state the full set of assumptions of all theoretical results? [Yes]
359"
REFERENCES,0.9422222222222222,"(b) Did you include complete proofs of all theoretical results? [Yes]
360"
REFERENCES,0.9444444444444444,"3. If you ran experiments...
361"
REFERENCES,0.9466666666666667,"(a) Did you include the code, data, and instructions needed to reproduce the main experi-
362"
REFERENCES,0.9488888888888889,"mental results (either in the supplemental material or as a URL)? [Yes]
363"
REFERENCES,0.9511111111111111,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
364"
REFERENCES,0.9533333333333334,"were chosen)? [Yes]
365"
REFERENCES,0.9555555555555556,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
366"
REFERENCES,0.9577777777777777,"ments multiple times)? [Yes]
367"
REFERENCES,0.96,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
368"
REFERENCES,0.9622222222222222,"of GPUs, internal cluster, or cloud provider)? [TODO]
369"
REFERENCES,0.9644444444444444,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
370"
REFERENCES,0.9666666666666667,"(a) If your work uses existing assets, did you cite the creators? [TODO]
371"
REFERENCES,0.9688888888888889,"(b) Did you mention the license of the assets? [TODO]
372"
REFERENCES,0.9711111111111111,"(c) Did you include any new assets either in the supplemental material or as a URL?
373"
REFERENCES,0.9733333333333334,"[TODO]
374"
REFERENCES,0.9755555555555555,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
375"
REFERENCES,0.9777777777777777,"using/curating? [N/A]
376"
REFERENCES,0.98,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
377"
REFERENCES,0.9822222222222222,"information or offensive content? [N/A]
378"
REFERENCES,0.9844444444444445,"5. If you used crowdsourcing or conducted research with human subjects...
379"
REFERENCES,0.9866666666666667,"(a) Did you include the full text of instructions given to participants and screenshots, if
380"
REFERENCES,0.9888888888888889,"applicable? [N/A]
381"
REFERENCES,0.9911111111111112,"(b) Did you describe any potential participant risks, with links to Institutional Review
382"
REFERENCES,0.9933333333333333,"Board (IRB) approvals, if applicable? [N/A]
383"
REFERENCES,0.9955555555555555,"(c) Did you include the estimated hourly wage paid to participants and the total amount
384"
REFERENCES,0.9977777777777778,"spent on participant compensation? [N/A]
385"
