Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017452006980802793,"In the last decades, the capacity to generate large amounts of data in science and
1"
ABSTRACT,0.0034904013961605585,"engineering applications has been growing steadily. Meanwhile, the progress in
2"
ABSTRACT,0.005235602094240838,"machine learning has turned it into a suitable tool to process and utilise the available
3"
ABSTRACT,0.006980802792321117,"data. Nonetheless, many relevant scientific and engineering problems present
4"
ABSTRACT,0.008726003490401396,"challenges where current machine learning methods cannot yet efficiently leverage
5"
ABSTRACT,0.010471204188481676,"the available data and resources. For example, in scientific discovery, we are often
6"
ABSTRACT,0.012216404886561954,"faced with the problem of exploring very large, high-dimensional spaces, where
7"
ABSTRACT,0.013961605584642234,"querying a high fidelity, black-box objective function is very expensive. Progress
8"
ABSTRACT,0.015706806282722512,"in machine learning methods that can efficiently tackle such problems would help
9"
ABSTRACT,0.017452006980802792,"accelerate currently crucial areas such as drug and materials discovery. In this paper,
10"
ABSTRACT,0.019197207678883072,"we propose the use of GFlowNets for multi-fidelity active learning, where multiple
11"
ABSTRACT,0.020942408376963352,"approximations of the black-box function are available at lower fidelity and cost.
12"
ABSTRACT,0.02268760907504363,"GFlowNets are recently proposed methods for amortised probabilistic inference
13"
ABSTRACT,0.02443280977312391,"that have proven efficient for exploring large, high-dimensional spaces and can
14"
ABSTRACT,0.02617801047120419,"hence be practical in the multi-fidelity setting too. Here, we describe our algorithm
15"
ABSTRACT,0.027923211169284468,"for multi-fidelity active learning with GFlowNets and evaluate its performance in
16"
ABSTRACT,0.029668411867364748,"both well-studied synthetic tasks and practically relevant applications of molecular
17"
ABSTRACT,0.031413612565445025,"discovery. Our results show that multi-fidelity active learning with GFlowNets
18"
ABSTRACT,0.03315881326352531,"can efficiently leverage the availability of multiple oracles with different costs and
19"
ABSTRACT,0.034904013961605584,"fidelities to accelerate scientific discovery and engineering design.
20"
INTRODUCTION,0.03664921465968586,"1
Introduction
21"
INTRODUCTION,0.038394415357766144,"The current most pressing challenges for humanity, such as the climate crisis and the threat of
22"
INTRODUCTION,0.04013961605584642,"pandemics or antibiotic resistance could be tackled, at least in part, with new scientific discoveries.
23"
INTRODUCTION,0.041884816753926704,"By way of illustration, materials discovery can play an important role in improving the energy
24"
INTRODUCTION,0.04363001745200698,"efficiency of energy production and storage; and reducing the costs and duration for drug discovery
25"
INTRODUCTION,0.04537521815008726,"has the potential to more effectively and rapidly mitigate the consequences of new diseases. In
26"
INTRODUCTION,0.04712041884816754,"recent years, researchers in materials science, biochemistry and other fields have increasingly adopted
27"
INTRODUCTION,0.04886561954624782,"machine learning as a tool as it holds the promise to drastically accelerate scientific discovery
28"
INTRODUCTION,0.0506108202443281,"[7, 67, 3, 12].
29"
INTRODUCTION,0.05235602094240838,"Although machine learning has already made a positive impact in scientific discovery applications
30"
INTRODUCTION,0.05410122164048865,"[55, 27], unleashing its full potential will require improving the current algorithms [1]. For example,
31"
INTRODUCTION,0.055846422338568937,"typical tasks in potentially impactful applications in materials and drug discovery require exploring
32"
INTRODUCTION,0.05759162303664921,"combinatorially large, high-dimensional spaces [46, 6], where only small, noisy data sets are available,
33"
INTRODUCTION,0.059336823734729496,"and obtaining new annotations computationally or experimentally is very expensive. Such scenarios
34"
INTRODUCTION,0.06108202443280977,"present serious challenges even for the most advanced current machine learning methods.
35"
INTRODUCTION,0.06282722513089005,"In the search for a useful discovery, we typically define a quantitative proxy for usefulness, which we
36"
INTRODUCTION,0.06457242582897033,"can view as a black-box function. One promising avenue for improvement is developing methods
37"
INTRODUCTION,0.06631762652705062,"that more efficiently leverage the availability of multiple approximations of the target black-box
38"
INTRODUCTION,0.06806282722513089,"function at lower fidelity but much lower cost than the highest fidelity oracle [10, 14]. For example,
39"
INTRODUCTION,0.06980802792321117,"the most accurate estimation of the properties of materials and molecules is only typically obtained
40"
INTRODUCTION,0.07155322862129145,"via synthesis and characterisation in a laboratory. However, this is only feasible for a small number of
41"
INTRODUCTION,0.07329842931937172,"promising candidates. Approximate quantum mechanics simulations of a larger amount of chemical
42"
INTRODUCTION,0.07504363001745201,"compounds can be performed via Density Functional Theory (DFT) [41, 51]. However, DFT is
43"
INTRODUCTION,0.07678883071553229,"still computationally too expensive for high-throughput exploration of large search spaces. Thus,
44"
INTRODUCTION,0.07853403141361257,"large-scale exploration can only be achieved through cheaper but less accurate oracles. Nonetheless,
45"
INTRODUCTION,0.08027923211169284,"solely relying on low-fidelity approximations is clearly suboptimal. Ideally, such tasks would be best
46"
INTRODUCTION,0.08202443280977312,"tackled by methods that can efficiently and adaptively distribute the available computational budget
47"
INTRODUCTION,0.08376963350785341,"between the multiple oracles depending on the already acquired information.
48"
INTRODUCTION,0.08551483420593368,"The past decade has seen significant progress in multi-fidelity Bayesian optimisation (BO) [19, 53],
49"
INTRODUCTION,0.08726003490401396,"including methods that leverage the potential of deep neural networks [36]. Although highly relevant
50"
INTRODUCTION,0.08900523560209424,"for scientific discovery, standard BO is not perfectly suited for some of the challenges in materials and
51"
INTRODUCTION,0.09075043630017451,"drug discovery tasks. First and foremost, BO’s ultimate goal is to find the optimum of an expensive
52"
INTRODUCTION,0.0924956369982548,"black-box function. However, even the highest fidelity oracles in such problems are underspecified
53"
INTRODUCTION,0.09424083769633508,"with respect to the actual, relevant, downstream applications. Therefore, it is imperative to develop
54"
INTRODUCTION,0.09598603839441536,"methods that, instead of “simply” finding the optimum, discover a set of diverse, high-scoring
55"
INTRODUCTION,0.09773123909249563,"candidates.
56"
INTRODUCTION,0.09947643979057591,"Recently, generative flow networks (GFlowNets) [4] have demonstrated their capacity to find diverse
57"
INTRODUCTION,0.1012216404886562,"candidates through discrete probabilistic modelling, with particularly promising results when embed-
58"
INTRODUCTION,0.10296684118673648,"ded in an active learning loop [22]. Here, we propose to extend the applicability of GFlowNets for
59"
INTRODUCTION,0.10471204188481675,"multi-fidelity active learning.
60"
INTRODUCTION,0.10645724258289703,"In this paper, we present an algorithm for multi-fidelity active learning with GFlowNets. We provide
61"
INTRODUCTION,0.1082024432809773,"empirical results in two synthetic benchmark tasks and four practically relevant tasks for biological
62"
INTRODUCTION,0.1099476439790576,"sequence design and molecular modelling. As a main result, we demonstrate that multi-fidelity
63"
INTRODUCTION,0.11169284467713787,"active learning with GFlowNets discovers diverse, high-scoring samples when multiple oracles with
64"
INTRODUCTION,0.11343804537521815,"different fidelities and costs are available, with lower computational cost than its single-fidelity
65"
INTRODUCTION,0.11518324607329843,"counterpart.
66"
RELATED WORK,0.1169284467713787,"2
Related Work
67"
RELATED WORK,0.11867364746945899,"Our work can be framed within the broad field of active learning (AL), a class of machine learning
68"
RELATED WORK,0.12041884816753927,"methods whose goal is to learn an efficient data sampling scheme to accelerate training [50]. For the
69"
RELATED WORK,0.12216404886561955,"bulk of the literature in AL, the goal is to train an accurate model h(x) of an unknown target function
70"
RELATED WORK,0.12390924956369982,"f(x), as in classical supervised learning. However, in certain scientific discovery problems, which is
71"
RELATED WORK,0.1256544502617801,"the motivation of our work, a desirable goal is often to discover multiple, diverse candidates x with
72"
RELATED WORK,0.1273996509598604,"high values of f(x). The reason is that the ultimate usefulness of a discovery is extremely expensive
73"
RELATED WORK,0.12914485165794065,"to quantify and we always rely on more or less accurate approximations. Since we generally have
74"
RELATED WORK,0.13089005235602094,"the option to consider more than one candidate solution, it is safer to generate a set of diverse and
75"
RELATED WORK,0.13263525305410123,"apparently good solutions, instead of focusing on the single global optimum of the wrong function.
76"
RELATED WORK,0.1343804537521815,"This distinctive goal is closely connected to related research areas such as Bayesian optimisation [19]
77"
RELATED WORK,0.13612565445026178,"and active search [20]. Bayesian optimisation (BO) is an approach grounded in Bayesian inference
78"
RELATED WORK,0.13787085514834205,"for the problem of optimising a black-box objective function f(x) that is expensive to evaluate. In
79"
RELATED WORK,0.13961605584642234,"contrast to the problem we address in this paper, standard BO typically considers continuous domains
80"
RELATED WORK,0.14136125654450263,"and works best in relatively low-dimensional spaces [18]. Nonetheless, in recent years, approaches
81"
RELATED WORK,0.1431064572425829,"for BO with structured data [13] and high-dimensional domains [21] have been proposed in the
82"
RELATED WORK,0.14485165794066318,"literature. The main difference between BO and the problem we tackle in this paper is that we are
83"
RELATED WORK,0.14659685863874344,"interested in finding multiple, diverse samples with high value of f and not only the optimum.
84"
RELATED WORK,0.14834205933682373,"This goal, as well as the discrete nature of the search space, is shared with active search, a variant of
85"
RELATED WORK,0.15008726003490402,"active learning in which the task is to efficiently find multiple samples of a valuable (binary) class
86"
RELATED WORK,0.1518324607329843,"from a discrete domain X [20]. This objective was already considered in the early 2000s by Warmuth
87"
RELATED WORK,0.15357766143106458,"et al. for drug discovery [59], and more formally analysed in later work [26, 25]. A recent branch of
88"
RELATED WORK,0.15532286212914484,"research in stochastic optimisation that considers diversity is so-called Quality-Diversity [9], which
89"
RELATED WORK,0.15706806282722513,"typically uses evolutionary algorithms that perform search in the latent space. All these and other
90"
RELATED WORK,0.15881326352530542,"problems such as multi-armed bandits [48] and the general framework of experimental design [8] all
91"
RELATED WORK,0.16055846422338568,"share the objective of optimising or exploring an expensive black-box function. Formal connections
92"
RELATED WORK,0.16230366492146597,"between some of these areas have been established in the literature [54, 17, 23, 15].
93"
RELATED WORK,0.16404886561954624,"Multi-fidelity methods have been proposed in most of these related areas of research. An early
94"
RELATED WORK,0.16579406631762653,"survey on multi-fidelity methods for Bayesian optimisation was compiled by Peherstorfer et al. [42],
95"
RELATED WORK,0.16753926701570682,"and research on the subject has continued since [44, 53], with the proposal of specific acquisition
96"
RELATED WORK,0.16928446771378708,"functions [56] and the use of deep neural networks to improve the modelling [36]. Interestingly, the
97"
RELATED WORK,0.17102966841186737,"literature on multi-fidelity active learning [35] is scarcer than on Bayesian optimisation. Recently,
98"
RELATED WORK,0.17277486910994763,"works on multi-fidelity active search have also appeared in the literature [40]. Finally, multi-fidelity
99"
RELATED WORK,0.17452006980802792,"methods have recently started to be applied in scientific discovery problems [10, 14]. However, the
100"
RELATED WORK,0.1762652705061082,"literature is still scarce probably because most approaches do not tackle the specific needs in scientific
101"
RELATED WORK,0.17801047120418848,"discovery, such as the need for diverse samples. Here, we aim addressing this need with the use of
102"
RELATED WORK,0.17975567190226877,"GFlowNets [4, 24] for multi-fidelity active learning.
103"
METHOD,0.18150087260034903,"3
Method
104"
BACKGROUND,0.18324607329842932,"3.1
Background
105"
BACKGROUND,0.1849912739965096,"GFlowNets
Generative Flow Networks [GFlowNets; 4, 5] are amortised samplers designed for
106"
BACKGROUND,0.18673647469458987,"sampling from discrete high-dimensional distributions. Given a space of compositional objects X
107"
BACKGROUND,0.18848167539267016,"and a non-negative reward function R(x), GFlowNets are designed to learn a stochastic policy π that
108"
BACKGROUND,0.19022687609075042,"generates x ∈X with a probability proportional to the reward, that is π(x) ∝R(x). This distinctive
109"
BACKGROUND,0.19197207678883071,"property induces sampling diverse, high-reward objects, which is a desirable property for scientific
110"
BACKGROUND,0.193717277486911,"discovery, among other applications [23].
111"
BACKGROUND,0.19546247818499127,"The objects x ∈X are constructed sequentially by sampling transitions st→st+1 ∈A between
112"
BACKGROUND,0.19720767888307156,"partially constructed objects (states) s ∈S, which includes a unique empty state s0. The stochastic
113"
BACKGROUND,0.19895287958115182,"forward policy is typically parameterised by a neural network PF (st+1|st; θ), where θ denotes the
114"
BACKGROUND,0.2006980802792321,"learnable parameters, which models the distribution over transitions st→st+1 from the current state
115"
BACKGROUND,0.2024432809773124,"st to the next state st+1. The backward transitions are parameterised too and denoted PB(st|st+1; θ).
116"
BACKGROUND,0.20418848167539266,"The probability π(x) of generating an object x is given by PF and its sequential application:
117"
BACKGROUND,0.20593368237347295,"π(x) =
X"
BACKGROUND,0.20767888307155322,τ:s|τ|−1→x∈τ
BACKGROUND,0.2094240837696335,"|τ|−1
Y"
BACKGROUND,0.2111692844677138,"t=0
PF (st+1|st; θ),"
BACKGROUND,0.21291448516579406,"which sums over all trajectories τ with terminating state x, where τ = (s0 →s1 . . . →s|τ|) is a
118"
BACKGROUND,0.21465968586387435,"complete trajectory. To learn the parameters θ such that π(x) ∝R(x) we use the trajectory balance
119"
BACKGROUND,0.2164048865619546,"learning objective [37]
120"
BACKGROUND,0.2181500872600349,"LT B(τ; θ) =

log
Zθ
Qn
t=0 PF (st+1|st; θ)
R(x) Qn
t=1 PB(st|st+1; θ)"
BACKGROUND,0.2198952879581152,"2
,
(1)"
BACKGROUND,0.22164048865619546,where Zθ is an approximation of the partition function P
BACKGROUND,0.22338568935427575,"x∈X R(x) that is learned. The GFlowNet
121"
BACKGROUND,0.225130890052356,"learning objective supports training from off-policy trajectories, so for training the trajectories are
122"
BACKGROUND,0.2268760907504363,"typically sampled from a mixture of the current policy with a uniform random policy. The reward is
123"
BACKGROUND,0.2286212914485166,"also tempered to make the policy focus on the modes.
124"
BACKGROUND,0.23036649214659685,"Active Learning
In its simplest formulation, the active learning problem that we consider is as
125"
BACKGROUND,0.23211169284467714,"follows: we start with an initial data set D = {(xi, f(xi))} of samples x ∈X and their evaluations
126"
BACKGROUND,0.2338568935427574,"by an expensive, black-box objective function (oracle) f : X →R, which we use to train a surrogate
127"
BACKGROUND,0.2356020942408377,"model h(x). A GFlowNet can then be trained to learn a generative policy πθ(x) using h(x) as reward
128"
BACKGROUND,0.23734729493891799,"function, that is R(x) = h(x). Optionally, we can instead train a probabilistic proxy p(f|D) and use
129"
BACKGROUND,0.23909249563699825,"as reward the output of an acquisition function α(x, p(f|D)) that considers the epistemic uncertainty
130"
BACKGROUND,0.24083769633507854,"of the surrogate model, as typically done in Bayesian optimisation. Finally, we use the policy π(x) to
131"
BACKGROUND,0.2425828970331588,"generate a batch of samples to be evaluated by the oracle f, we add them to our data set and repeat
132"
BACKGROUND,0.2443280977312391,"the process a number of active learning rounds.
133"
BACKGROUND,0.24607329842931938,"While much of the active learning literature [50] has focused on so-called pool-based active learning,
134"
BACKGROUND,0.24781849912739964,"where the learner selects samples from a pool of unlabelled data, we here consider the scenario of
135"
BACKGROUND,0.24956369982547993,"de novo query synthesis, where samples are selected from the entire object space X. This scenario
136"
BACKGROUND,0.2513089005235602,"is particularly suited for scientific discovery [30, 62, 64, 33]. The ultimate goal pursued in active
137"
BACKGROUND,0.2530541012216405,"learning applications is also heterogeneous. Often, the goal is the same as in classical supervised
138"
BACKGROUND,0.2547993019197208,"machine learning: to train an accurate (proxy) model h(x) of the unknown target function f(x). For
139"
BACKGROUND,0.25654450261780104,"some problems in scientific discovery, we are usually not interested in the accuracy in the entire input
140"
BACKGROUND,0.2582897033158813,"space X, but rather in discovering new, diverse objects with high values of f. This is connected to
141"
BACKGROUND,0.2600349040139616,"other related problems such as Bayesian optimisation [19], active search [20] or experimental design
142"
BACKGROUND,0.2617801047120419,"[8], as reviewed in Section 2.
143"
MULTI-FIDELITY ACTIVE LEARNING,0.26352530541012215,"3.2
Multi-Fidelity Active Learning
144"
MULTI-FIDELITY ACTIVE LEARNING,0.26527050610820246,"We now consider the following active learning problem with multiple oracles of different fidelities.
145"
MULTI-FIDELITY ACTIVE LEARNING,0.2670157068062827,"Our ultimate goal is to generate a batch of K samples x ∈X according to the following desiderata:
146"
MULTI-FIDELITY ACTIVE LEARNING,0.268760907504363,"• The samples obtain a high value when evaluated by the objective function f : X →R+.
147"
MULTI-FIDELITY ACTIVE LEARNING,0.2705061082024433,"• The samples in the batch should be distinct and diverse, that is cover distinct high-valued
148"
MULTI-FIDELITY ACTIVE LEARNING,0.27225130890052357,"regions of f.
149"
MULTI-FIDELITY ACTIVE LEARNING,0.27399650959860383,"Furthermore, we are constrained by a computational budget Λ that limits our capacity to evaluate f.
150"
MULTI-FIDELITY ACTIVE LEARNING,0.2757417102966841,"While f is extremely expensive to evaluate, we have access to a discrete set of surrogate functions
151"
MULTI-FIDELITY ACTIVE LEARNING,0.2774869109947644,"(oracles) {fm}1≤m≤M : X →R+, where m represents the fidelity index and each oracle has an
152"
MULTI-FIDELITY ACTIVE LEARNING,0.2792321116928447,"associated cost λm. We assume fM = f because there may be even more accurate oracles for
153"
MULTI-FIDELITY ACTIVE LEARNING,0.28097731239092494,"the true usefulness but we do not have access to them, which means that even when measured by
154"
MULTI-FIDELITY ACTIVE LEARNING,0.28272251308900526,"f = fM, diversity remains an important objective. We also assume, without loss of generality, that
155"
MULTI-FIDELITY ACTIVE LEARNING,0.2844677137870855,"the larger m, the higher the fidelity and that λ1 < λ2 < . . . < λM. This scenario resembles many
156"
MULTI-FIDELITY ACTIVE LEARNING,0.2862129144851658,"practically relevant problems in scientific discovery, where the objective function fM is indicative but
157"
MULTI-FIDELITY ACTIVE LEARNING,0.2879581151832461,"not a perfect proxy of the true usefulness of objects x—hence we want diversity—yet it is extremely
158"
MULTI-FIDELITY ACTIVE LEARNING,0.28970331588132636,"expensive to evaluate—hence cheaper, approximate models are used in practice.
159"
MULTI-FIDELITY ACTIVE LEARNING,0.2914485165794066,"In multi-fidelity active learning—as well as in multi-fidelity Bayesian optimisation—the iterative
160"
MULTI-FIDELITY ACTIVE LEARNING,0.2931937172774869,"sampling scheme consists of not only selecting the next object x (or batch of objects) to evaluate, but
161"
MULTI-FIDELITY ACTIVE LEARNING,0.2949389179755672,"also the level of fidelity m, such that the procedure is cost-effective.
162"
MULTI-FIDELITY ACTIVE LEARNING,0.29668411867364747,"Our algorithm, MF-GFN, detailed in Algorithm 1, proceeds as follows: An active learning round j
163"
MULTI-FIDELITY ACTIVE LEARNING,0.29842931937172773,"starts with a data set of annotated samples Dj = {(xi, fm(xi), mi)}1≤m≤M. The data set is used to
164"
MULTI-FIDELITY ACTIVE LEARNING,0.30017452006980805,"fit a probabilistic multi-fidelity surrogate model h(x, m) of the posterior p(fm(x)|x, m, D). We use
165"
MULTI-FIDELITY ACTIVE LEARNING,0.3019197207678883,"Gaussian Processes [47], as is common in Bayesian optimisation, to model the posterior, such that the
166"
MULTI-FIDELITY ACTIVE LEARNING,0.3036649214659686,"model h predicts the conditional Gaussian distribution of fm(x) given (x, m) and the existing data
167"
MULTI-FIDELITY ACTIVE LEARNING,0.3054101221640489,"set D. We implement a multi-fidelity GP kernel by combining a Matern kernel evaluated on x with a
168"
MULTI-FIDELITY ACTIVE LEARNING,0.30715532286212915,"linear downsampling kernel over m [61]. In the higher dimensional problems, we use Deep Kernel
169"
MULTI-FIDELITY ACTIVE LEARNING,0.3089005235602094,"Learning [60] to increase the expressivity of the surrogate models. The candidate x is modelled with
170"
MULTI-FIDELITY ACTIVE LEARNING,0.3106457242582897,"the deep kernel while the fidelity m is modelled with the same linear downsamling kernel. The output
171"
MULTI-FIDELITY ACTIVE LEARNING,0.31239092495637,"of the proxy model is then used to compute the value of a multi-fidelity acquisition function α(x, m).
172"
MULTI-FIDELITY ACTIVE LEARNING,0.31413612565445026,"In our experiments, we use the multi-fidelity version [56] of Max-Value Entropy Search (MES) [58],
173"
MULTI-FIDELITY ACTIVE LEARNING,0.3158813263525305,"which is an information-theoretic acquisition function widely used in Bayesian optimization. MES
174"
MULTI-FIDELITY ACTIVE LEARNING,0.31762652705061084,"aims to maximize the mutual information between the value of the queried x and the maximum value
175"
MULTI-FIDELITY ACTIVE LEARNING,0.3193717277486911,"attained by the objective function, f ⋆. The multi-fidelity variant is designed to select the candidate x
176"
MULTI-FIDELITY ACTIVE LEARNING,0.32111692844677137,"and the fidelity m that maximize the mutual information between f ⋆
M and the oracle at fidelity m,
177"
MULTI-FIDELITY ACTIVE LEARNING,0.3228621291448517,"fm, weighted by the cost of the oracle:
178"
MULTI-FIDELITY ACTIVE LEARNING,0.32460732984293195,"α(x, m) =
1
λm
I(f ⋆
M; fm|Dj).
(2)"
MULTI-FIDELITY ACTIVE LEARNING,0.3263525305410122,"We provide further details about the acquisition function in Appendix A. A multi-fidelity acquisition
179"
MULTI-FIDELITY ACTIVE LEARNING,0.32809773123909247,"function can be regarded as a cost-adjusted utility function. Therefore, in order to carry out a cost-
180"
MULTI-FIDELITY ACTIVE LEARNING,0.3298429319371728,"aware search, we seek to sample diverse objects with high value of the acquisition function. In this
181"
MULTI-FIDELITY ACTIVE LEARNING,0.33158813263525305,"paper, we propose to use a GFlowNet as a generative model trained for this purpose (see further
182"
MULTI-FIDELITY ACTIVE LEARNING,0.3333333333333333,"details below in Section 3.3). An active learning round terminates by generating N objects from
183"
MULTI-FIDELITY ACTIVE LEARNING,0.33507853403141363,"the sampler (here the GFlowNet policy π) and forming a batch with the best B objects, according
184"
MULTI-FIDELITY ACTIVE LEARNING,0.3368237347294939,"to α. Note that N ≫B, since sampling from a GFlowNet is relatively inexpensive. The selected
185"
MULTI-FIDELITY ACTIVE LEARNING,0.33856893542757416,"objects are annotated by the corresponding oracles and incorporated into the data set, such that
186"
MULTI-FIDELITY ACTIVE LEARNING,0.3403141361256545,"Dj+1 = Dj ∪{(x1, fm(x1), m1), . . . (xB, fm(xB), mB)}.
187"
MULTI-FIDELITY ACTIVE LEARNING,0.34205933682373474,"Algorithm 1: MF-GFN: Multi-fidelity active learning with GFlowNets. See Section 4.1 for
quality (Top-K(D)) and diversity metrics.
Input: {(fm, λm)}: M oracles and their corresponding costs;
D0 = {(xi, fm(xi), mi)}: Initial dataset;
h(x, m): Multi-fidelity Gaussian Process proxy model;
α(x, m): Multi-fidelity acquisition function;
R(α(x), β): reward function to train the GFlowNet;
B: Batch size of oracles queries;
Λ: Maximum available budget;
K: Number of top-scoring candidates to be evaluated at termination;
Result: Top-K(D), Diversity
Initialization: Λj = 0, D = D0
while Λj < Λ do"
MULTI-FIDELITY ACTIVE LEARNING,0.343804537521815,"• Fit h on dataset D;
• Train GFlowNet with reward R(α(x), β) to obtain policy πθ(x);
• Sample B tuples (xi, mi) ∼πθ;
• Evaluate each tuple with the corresponding oracle to form batch
B = {(x1, fm(x1), m1), . . . , (xB, fm(xB), mB)};
• Update dataset D = D ∪B;
end"
MULTI-FIDELITY GFLOWNETS,0.34554973821989526,"3.3
Multi-Fidelity GFlowNets
188"
MULTI-FIDELITY GFLOWNETS,0.3472949389179756,"In order to use GFlowNets in the multi-fidelity active learning loop described above, we propose to
189"
MULTI-FIDELITY GFLOWNETS,0.34904013961605584,"make the GFlowNet sample the fidelity m for each object x ∈X in addition to x itself. Formally,
190"
MULTI-FIDELITY GFLOWNETS,0.3507853403141361,"given a baseline GFlowNet with state and transition spaces S and A, we augment the state space with
191"
MULTI-FIDELITY GFLOWNETS,0.3525305410122164,"a new dimension for the fidelity M′ = {0, 1, 2, . . . , M} (including m = 0, which corresponds to
192"
MULTI-FIDELITY GFLOWNETS,0.3542757417102967,"unset fidelity), such that the augmented, multi-fidelity space is SM′ = S ∪M′. The set of allowed
193"
MULTI-FIDELITY GFLOWNETS,0.35602094240837695,"transitions AM is augmented such that a fidelity m > 0 of a trajectory must be selected once, and
194"
MULTI-FIDELITY GFLOWNETS,0.35776614310645727,"only once, from any intermediate state.
195"
MULTI-FIDELITY GFLOWNETS,0.35951134380453753,"Intuitively, allowing the selection of the fidelity at any step in the trajectory should give flexibility for
196"
MULTI-FIDELITY GFLOWNETS,0.3612565445026178,"better generalisation. At the end, finished trajectories are the concatenation of an object x and the
197"
MULTI-FIDELITY GFLOWNETS,0.36300174520069806,"fidelity m, that is (x, m) ∈XM = X ∪M. In summary, the proposed approach enables to jointly
198"
MULTI-FIDELITY GFLOWNETS,0.3647469458987784,"learn the policy that samples objects in a potentially very large, high-dimensional space, together
199"
MULTI-FIDELITY GFLOWNETS,0.36649214659685864,"with the level of fidelity, that maximise a given multi-fidelity acquisition function as reward.
200"
EMPIRICAL EVALUATION,0.3682373472949389,"4
Empirical Evaluation
201"
EMPIRICAL EVALUATION,0.3699825479930192,"In this section, we describe the evaluation metrics and experiments performed to assess the validity
202"
EMPIRICAL EVALUATION,0.3717277486910995,"and performance of our proposed approach of multi-fidelity active learning with GFlowNets. Overall,
203"
EMPIRICAL EVALUATION,0.37347294938917974,"the purpose of this empirical evaluation is to answer the following questions:
204"
EMPIRICAL EVALUATION,0.37521815008726006,"• Question 1: Is our multi-fidelity active learning approach able to find high-scoring, diverse
205"
EMPIRICAL EVALUATION,0.3769633507853403,"samples at lower cost than active learning with a single oracle?
206"
EMPIRICAL EVALUATION,0.3787085514834206,"• Question 2: Does our proposed multi-fidelity GFlowNet, which learns to sample fidelities
207"
EMPIRICAL EVALUATION,0.38045375218150085,"together with objects (x, m), provide any advantage over sampling only objects x?
208"
EMPIRICAL EVALUATION,0.38219895287958117,"In Section 4.1 we describe the metrics proposed to evaluate the performance our proposed method,
209"
EMPIRICAL EVALUATION,0.38394415357766143,"as well as the baselines, which we describe in Section 4.2. In Section 4.3, we present results on
210"
EMPIRICAL EVALUATION,0.3856893542757417,"synthetic tasks typically used in the multi-fidelity BO and active learning literature. In Section 4.4,
211"
EMPIRICAL EVALUATION,0.387434554973822,"we present results on more practically relevant tasks for scientific discovery, such as the design of
212"
EMPIRICAL EVALUATION,0.38917975567190227,"DNA sequences and anti-microbial peptides.
213"
METRICS,0.39092495636998253,"4.1
Metrics
214"
METRICS,0.39267015706806285,"One core motivation in the conception of GFlowNets, as reported in the original paper [4], was the
215"
METRICS,0.3944153577661431,"goal of sampling diverse objects with high-score, according to a reward function.
216"
METRICS,0.3961605584642234,"• Mean score, as per the highest fidelity oracle fM, of the top-K samples.
217"
METRICS,0.39790575916230364,"• Mean pairwise similarity within the top-K samples.
218"
METRICS,0.39965095986038396,"Furthermore, since here we are interested in the cost effectiveness of the active learning process, in
219"
METRICS,0.4013961605584642,"this section we will evaluate the above metrics as a function of the cost accumulated in querying the
220"
METRICS,0.4031413612565445,"multi-fidelity oracles. It is important to note that the multi-fidelity approach is not aimed at achieving
221"
METRICS,0.4048865619546248,"better mean top-K scores than a single-fidelity active learning counterpart, but rather the same mean
222"
METRICS,0.40663176265270506,"top-K scores with a smaller budget.
223"
BASELINES,0.4083769633507853,"4.2
Baselines
224"
BASELINES,0.41012216404886565,"In order to evaluate our approach, and to shed light on the questions stated above, we consider the
225"
BASELINES,0.4118673647469459,"following baselines:
226"
BASELINES,0.41361256544502617,"• GFlowNet with highest fidelity (SF-GFN): GFlowNet based active learning approach from [22]
227"
BASELINES,0.41535776614310643,"with the highest fidelity oracle to establish a benchmark for performance without considering
228"
BASELINES,0.41710296684118675,"the cost-accuracy trade-offs.
229"
BASELINES,0.418848167539267,"• GFlowNet with random fidelities (Random fid. GFlowNet): Variant of SF-GFN where the
230"
BASELINES,0.4205933682373473,"candidates are generated with the GFlowNet but the fidelities are picked randomly and a
231"
BASELINES,0.4223385689354276,"multi-fidelity acquisition function is used, to investigate the benefit of deciding the fidelity
232"
BASELINES,0.42408376963350786,"with GFlowNets.
233"
BASELINES,0.4258289703315881,"• Random candidates and fidelities (Random): Quasi-random approach where the candidates
234"
BASELINES,0.42757417102966844,"and fidelities are picked randomly and the top (x, m) pairs scored by the acquisition function
235"
BASELINES,0.4293193717277487,"are queried.
236"
BASELINES,0.43106457242582896,"• Multi-fidelity PPO (MF-PPO): Instantiation of multi-fidelity Bayesian optimization where the
237"
BASELINES,0.4328097731239092,"acquisition function is optimized using proximal policy optimization [PPO 49].
238"
SYNTHETIC TASKS,0.43455497382198954,"4.3
Synthetic Tasks
239"
SYNTHETIC TASKS,0.4363001745200698,"As an initial assessment of MF-GFNs, we consider two synthetic functions—Branin and Hartmann—
240"
SYNTHETIC TASKS,0.43804537521815007,"widely used in the single- and multi-fidelity Bayesian optimisation literature [44, 53, 28, 36, 16].
241"
SYNTHETIC TASKS,0.4397905759162304,"Branin
We consider an active learning problem in a two-dimensional space where the target
242"
SYNTHETIC TASKS,0.44153577661431065,"function fM is the Branin function, as modified in [52] and implemented in botorch [2]. We
243"
SYNTHETIC TASKS,0.4432809773123909,"simulate three levels of fidelity, including the true function. The lower-fidelity oracles, the costs of
244"
SYNTHETIC TASKS,0.44502617801047123,"the oracles (0.01, 0.1, 1.0) as well as the number of points queried in the initial training set were
245"
SYNTHETIC TASKS,0.4467713787085515,"adopted from [36]. We provide further details about the task in Appendix B.2. In order to consider a
246"
SYNTHETIC TASKS,0.44851657940663175,"discrete design space, we map the domain to a discrete 100 × 100 grid. We model this grid with a
247"
SYNTHETIC TASKS,0.450261780104712,"GFlowNet as in [4, 37]: starting from the origin (0, 0), for any state s = (x1, x2), the action space
248"
SYNTHETIC TASKS,0.45200698080279234,"consists of the choice between the exit action or the dimension to increment by 1, provided the next
249"
SYNTHETIC TASKS,0.4537521815008726,"state is in the limits of the grid. Fig. 1a illustrates the results for this task. We observe that MF-GFN
250"
SYNTHETIC TASKS,0.45549738219895286,"is able to reach the minimum of the Branin function with a smaller budget than the single-fidelity
251"
SYNTHETIC TASKS,0.4572425828970332,"counterpart and the baselines.
252"
SYNTHETIC TASKS,0.45898778359511344,"Hartmann
Next, we consider the 6-dimensional Hartmann function as objective fM on a hyper-grid
253"
SYNTHETIC TASKS,0.4607329842931937,"domain. As with Branin, we consider three oracles, adopting the lower-fidelity oracles and the set
254"
SYNTHETIC TASKS,0.462478184991274,"of costs (0.125, 0.25, 1.0) from [53]. We discretize the domain into a six-dimensional hyper-grid of
255"
SYNTHETIC TASKS,0.4642233856893543,"length 10, yielding 106 possible candidate points. The results for the task are illustrated in Fig. 1b,
256"
SYNTHETIC TASKS,0.46596858638743455,"which indicate that multi-fidelity active learning with GFlowNets (MF-GFN) offers an advantage
257"
SYNTHETIC TASKS,0.4677137870855148,"over single-fidelity active learning (SF-GFN) as well as some of the other baselines in this higher-
258"
SYNTHETIC TASKS,0.4694589877835951,"dimensional synthetic problem as well. Note that while MF-PPO performs better in this task, as
259"
SYNTHETIC TASKS,0.4712041884816754,"shown in the next experiments, MF-PPO tends to collapse to single modes of the function in more
260"
SYNTHETIC TASKS,0.47294938917975565,"complex high-dimensional scenarios.
261"
SYNTHETIC TASKS,0.47469458987783597,"10
2
10
1
100"
SYNTHETIC TASKS,0.47643979057591623,Fraction of total SF-GFN budget (log) 50 40 30 20 10 0
SYNTHETIC TASKS,0.4781849912739965,Mean Top-50 score
SYNTHETIC TASKS,0.4799301919720768,"SF-GFN
Rand. fid. GFN
MF-GFN"
SYNTHETIC TASKS,0.4816753926701571,(a) Branin task
SYNTHETIC TASKS,0.48342059336823734,"10
2
10
1
100"
SYNTHETIC TASKS,0.4851657940663176,Fraction of total SF-GFN budget (log) 1.5 2.0 2.5 3.0
SYNTHETIC TASKS,0.4869109947643979,Mean Top-10 score
SYNTHETIC TASKS,0.4886561954624782,"SF-GFN
Rand. fid. GFN
MF-PPO
MF-GFN"
SYNTHETIC TASKS,0.49040139616055844,(b) Hartmann task
SYNTHETIC TASKS,0.49214659685863876,"Figure 1: Results on the synthetic tasks—Branin and Hartmann functions. The curves indicate
the mean score fM within the top-50 and top-10 samples (for Branin and Hartmann, respectively)
computed at the end of each active learning round and plotted as a function of the budget used.
The random baseline is omitted from this plot to facilitate the visualisation since the results were
significantly worse in these tasks. We observe that MF-GFN clearly outperforms the single-fidelity
counterpart (SF-GFN) and slightly improves upon the GFlowNet baseline that samples random
fidelities. On Hartmann, MF-PPO initially outperforms the other methods."
BENCHMARK TASKS,0.493891797556719,"4.4
Benchmark Tasks
262"
BENCHMARK TASKS,0.4956369982547993,"While the synthetic tasks are insightful and convenient for analysis, to obtain a more solid assessment
263"
BENCHMARK TASKS,0.4973821989528796,"of the performance of MF-GFN, we evaluate it, together with the other baselines, on more complex,
264"
BENCHMARK TASKS,0.49912739965095987,"structured design spaces of practical relevance. We present results on a variety of tasks including DNA
265"
BENCHMARK TASKS,0.5008726003490401,"aptamers (Section 4.4.1), anti-microbial peptides (Section 4.4.2) and small molecules (Section 4.4.3).
266"
BENCHMARK TASKS,0.5026178010471204,"0.2
0.4
0.6
0.8
1.0
Fraction of total SF-GFN budget 4 6 8 10 12 14 16"
BENCHMARK TASKS,0.5043630017452007,Mean Top-100 energy
BENCHMARK TASKS,0.506108202443281,"SF-GFN
Random
Rand. fid. GFN"
BENCHMARK TASKS,0.5078534031413613,"MF-PPO
MF-GFN"
BENCHMARK TASKS,0.5095986038394416,(-) Diversity (+)
BENCHMARK TASKS,0.5113438045375218,(a) DNA aptamers task
BENCHMARK TASKS,0.5130890052356021,"10
3
10
2
10
1
100"
BENCHMARK TASKS,0.5148342059336823,Fraction of total SF-GFlowNet budget (log) 0.5 0.6 0.7 0.8
BENCHMARK TASKS,0.5165794066317626,Mean Top-50 energy
BENCHMARK TASKS,0.518324607329843,(-) Diversity (+)
BENCHMARK TASKS,0.5200698080279232,(b) Anti-microbial peptides (AMP) task
BENCHMARK TASKS,0.5218150087260035,"Figure 2: Results on the DNA aptamers and AMP tasks. The curves indicate the mean score fM
within the top-100 and top-50 samples (for DNA and AMP, respectively) computed at the end of
each active learning round and plotted as a function of the budget used. The colour of the markers
indicates the diversity within the batch (darker colour of the circular dots indicating more diversity).
In both the DNA and AMP tasks, MF-GFN outperforms all baselines in terms of cost efficiency,
while obtaining great diversity in the final batch of top-K candidates."
DNA APTAMERS,0.5235602094240838,"4.4.1
DNA Aptamers
267"
DNA APTAMERS,0.525305410122164,"DNA aptamers are single-stranded nucleotide sequences with multiple applications in polymer design
268"
DNA APTAMERS,0.5270506108202443,"due to their specificity and affinity as sensors in crowded biochemical environments [66, 11, 63, 29].
269"
DNA APTAMERS,0.5287958115183246,"DNA sequences are represented as strings of nucleobases A, C, T or G. In our experiments, we
270"
DNA APTAMERS,0.5305410122164049,"consider fixed-length sequences of 30 bases and design a GFlowNet environment where the action
271"
DNA APTAMERS,0.5322862129144852,"space A consists of the choice of base to append to the sequence, starting from an empty sequence.
272"
DNA APTAMERS,0.5340314136125655,"This yields a design space of size |X| = 430 (ignoring the selection of fidelity in MF-GFN). As
273"
DNA APTAMERS,0.5357766143106457,"the optimisation objective fM (highest fidelity) we used the free energy of the secondary structure
274"
DNA APTAMERS,0.537521815008726,"as calculated by NUPACK [65]. As a lower fidelity oracle, we trained a transformer model on 1
275"
DNA APTAMERS,0.5392670157068062,"million randomly sampled sequences annotated with fM, and assigned it a cost 100× smaller than
276"
DNA APTAMERS,0.5410122164048866,"the highest-fidelity oracle. Further details about the task are discussed in Appendix B.4.
277"
DNA APTAMERS,0.5427574171029669,"The main results on the DNA aptamers task are presented in Fig. 2a. We observe that on this task
278"
DNA APTAMERS,0.5445026178010471,"MF-GFN outperforms all other baselines in terms cost efficiency. For instance, MF-GFN achieves
279"
DNA APTAMERS,0.5462478184991274,"the best mean top-K energy achieved by its single-fidelity counterpart with just about 20% of the
280"
DNA APTAMERS,0.5479930191972077,"budget. It is also more efficient than GFlowNet with random fidelities and MF-PPO. Crucially, we
281"
DNA APTAMERS,0.5497382198952879,"also see that MF-GFN maintains a high level of diversity, even after converging to topK reward. On
282"
DNA APTAMERS,0.5514834205933682,"the contrary, MF-PPO is not able to discover diverse samples, as is expected based on prior work [22].
283"
ANTIMICROBIAL PEPTIDES,0.5532286212914486,"4.4.2
Antimicrobial Peptides
284"
ANTIMICROBIAL PEPTIDES,0.5549738219895288,"Antimicrobial peptides are short protein sequences which possess antimicrobial properties. As
285"
ANTIMICROBIAL PEPTIDES,0.5567190226876091,"proteins, these are sequences of amino-acids—a vocabulary of 20 along with a special stop token. We
286"
ANTIMICROBIAL PEPTIDES,0.5584642233856894,"consider variable length proteins sequences with up to 50 residues. We use data from DBAASP [45]
287"
ANTIMICROBIAL PEPTIDES,0.5602094240837696,"containing antimicrobial activity labels, which is split into two sets – one used for training the oracle
288"
ANTIMICROBIAL PEPTIDES,0.5619546247818499,"and one as the initial dataset in the active learning loop, following [22]. To establish the multi-fidelity
289"
ANTIMICROBIAL PEPTIDES,0.5636998254799301,"setting, we train different models with different capacities and with different subsets of the data. The
290"
ANTIMICROBIAL PEPTIDES,0.5654450261780105,"details about these oracles along with additional details about the task are discussed in Appendix B.5.
291"
ANTIMICROBIAL PEPTIDES,0.5671902268760908,"The results in Fig. 2b inidicate that even in this task MF-GFN outperforms all other baselines in
292"
ANTIMICROBIAL PEPTIDES,0.568935427574171,"terms of cost-efficiency. It reaches the same maximum mean top-K score as the random baselines
293"
ANTIMICROBIAL PEPTIDES,0.5706806282722513,"with 10× less budget and almost 100× less budget than SF-GFN. In this task, MF-PPO did not
294"
ANTIMICROBIAL PEPTIDES,0.5724258289703316,"achieve comparable results. Crucially, the diversity of the final batch found by MF-GFN stayed high,
295"
ANTIMICROBIAL PEPTIDES,0.5741710296684118,"satisfying this important criterion in the motivation of this method.
296"
SMALL MOLECULES,0.5759162303664922,"4.4.3
Small Molecules
297"
SMALL MOLECULES,0.5776614310645725,"Molecules are clouds of interacting electrons (and nuclei) described by a set of quantum mechanical
298"
SMALL MOLECULES,0.5794066317626527,"descriptions, or properties. These properties dictate their chemical behaviours and applications.
299"
SMALL MOLECULES,0.581151832460733,"Numerous approximations of these quantum mechanical properties have been developed with different
300"
SMALL MOLECULES,0.5828970331588132,"methods at different fidelities, with the famous example of Jacob’s ladder in density functional
301"
SMALL MOLECULES,0.5846422338568935,"theory [43]. To demonstrate the capability of MF-GFlowNet to function in the setting of quantum
302"
SMALL MOLECULES,0.5863874345549738,"chemistry, we consMF-GFNoof-of-concept tasks in molecular electronic potentials: maximization of
303"
SMALL MOLECULES,0.5881326352530541,"adiabatic electron affinity (EA) and (negative) adiabatic ionisation potential (IP). These electronic
304"
SMALL MOLECULES,0.5898778359511344,"potentials dictate the molecular redox chemistry, and are key quantities in organic semiconductors,
305"
SMALL MOLECULES,0.5916230366492147,"photoredox catalysis, or organometallic synthesis. We employed three oracles that correlate with
306"
SMALL MOLECULES,0.5933682373472949,"experimental results as approximations of the scoring function, by uses of varying levels of geometry
307"
SMALL MOLECULES,0.5951134380453752,"optimisation to obtain approximations to the adiabatic geometries, followed by the calculation of IP
308"
SMALL MOLECULES,0.5968586387434555,"or EA with semi-empirical quantum chemistry XTB (see Appendix) [39]. These three oracles had
309"
SMALL MOLECULES,0.5986038394415357,"costs of 1, 3 and 7 (respectively), proportional to their computational running demands. We designed
310"
SMALL MOLECULES,0.6003490401396161,"the GFlowNet state space by using sequences of SELFIES tokens [32] (maximum of 64) to represent
311"
SMALL MOLECULES,0.6020942408376964,"molecules, starting from an empty sequence; every action consists of appending a new token to the
312"
SMALL MOLECULES,0.6038394415357766,"sequence.
313"
SMALL MOLECULES,0.6055846422338569,"The realistic configuration and practical relevance of these tasks allow us to draw stronger conclu-
314"
SMALL MOLECULES,0.6073298429319371,"sions about the usefulness of multi-fidelity active learning with GFlowNets in scientific discovery
315"
SMALL MOLECULES,0.6090750436300174,"applications. As in the other tasks evaluated, we here also found MF-GFN to achieve better cost
316"
SMALL MOLECULES,0.6108202443280978,"efficiency at finding high-score top-K molecules (Fig. 3), especially for ionization potentials (Fig. 3a).
317"
SMALL MOLECULES,0.612565445026178,"By clustering the generated molecules, we find that MF-GFN captures as many modes as random
318"
SMALL MOLECULES,0.6143106457242583,"generation, far exceeding that of MF-PPO. Indeed, while MF-PPO seems to outperform MF-GFN in
319"
SMALL MOLECULES,0.6160558464223386,"the task of electron affinity (Fig. 3b), all generated molecules were from a few clusters, which is of
320"
SMALL MOLECULES,0.6178010471204188,"much less utility for chemists.
321"
SMALL MOLECULES,0.6195462478184991,"5
Conclusions, Limitations and Future Work
322"
SMALL MOLECULES,0.6212914485165794,"In this paper, we present MF-GFN, the first application of GFlowNets for multi-fidelity active learning.
323"
SMALL MOLECULES,0.6230366492146597,"Inspired by the encouraging results of GFlowNets in (single-fidelity) active learning for biological
324"
SMALL MOLECULES,0.62478184991274,"sequence design [22] as a method to discover diverse, high-scoring candidates, we propose MF-GFN
325"
SMALL MOLECULES,0.6265270506108203,"0.2
0.4
0.6
0.8
1.0
Fraction of total SF-GFN budget 9 8 7 6 5 4"
SMALL MOLECULES,0.6282722513089005,Mean Top-100 energy [eV]
SMALL MOLECULES,0.6300174520069808,"SF-GFN
Random
Rand. fid. GFN"
SMALL MOLECULES,0.631762652705061,"MF-PPO
MF-GFN"
SMALL MOLECULES,0.6335078534031413,(-) Diversity (+)
SMALL MOLECULES,0.6352530541012217,(a) Molecules ionisation potential (IP) task
SMALL MOLECULES,0.6369982547993019,"0.2
0.4
0.6
0.8
1.0
Fraction of total SF-GFN budget 2 3 4 5"
SMALL MOLECULES,0.6387434554973822,Mean Top-100 energy [eV]
SMALL MOLECULES,0.6404886561954625,"SF-GFN
Random
Rand. fid. GFN"
SMALL MOLECULES,0.6422338568935427,"MF-PPO
MF-GFN"
SMALL MOLECULES,0.643979057591623,(-) Diversity (+)
SMALL MOLECULES,0.6457242582897034,(b) Molecules electron affinity (EA) task
SMALL MOLECULES,0.6474694589877836,"Figure 3: Comparative results on the molecular discovery tasks: (a) ionisation potential (IP), (b)
electron affinity (EA). Results illustrate the generally faster convergence of MF-GFN to discover a
diverse set of molecules with desirable values of the target property (colour scheme of the circular
dots: darker/blue is better than lighter/yellow)."
SMALL MOLECULES,0.6492146596858639,"to sample the candidates as well as the fidelity at which the candidate is to be evaluated, when
326"
SMALL MOLECULES,0.6509598603839442,"multiple oracles are available with different fidelities and costs.
327"
SMALL MOLECULES,0.6527050610820244,"We evaluate the proposed MF-GFN approach in both synthetic tasks commonly used in the multi-
328"
SMALL MOLECULES,0.6544502617801047,"fidelity Bayesian optimization literature and benchmark tasks of practical relevance, such as DNA
329"
SMALL MOLECULES,0.6561954624781849,"aptamer generation, antimicrobial peptide design and molecular modelling. Through comparisons
330"
SMALL MOLECULES,0.6579406631762653,"with previously proposed methods as well as with variants of our method designed to understand the
331"
SMALL MOLECULES,0.6596858638743456,"contributions of different components, we conclude that multi-fidelity active learning with GFlowNets
332"
SMALL MOLECULES,0.6614310645724258,"not only outperforms its single-fidelity active learning counterpart in terms of cost effectiveness and
333"
SMALL MOLECULES,0.6631762652705061,"diversity of sampled candidates, but it also offers an advantage over other multi-fidelity methods due
334"
SMALL MOLECULES,0.6649214659685864,"to its ability to learn a stochastic policy to jointly sample objects and the fidelity of the oracle to be
335"
SMALL MOLECULES,0.6666666666666666,"used to evaluate them.
336"
SMALL MOLECULES,0.6684118673647469,"Broader Impact
Our work is motivated by pressing challenges to sustainability and public health,
337"
SMALL MOLECULES,0.6701570680628273,"and we envision applications of our approach to drug discovery and materials discovery. However,
338"
SMALL MOLECULES,0.6719022687609075,"as with all work on these topics, there is a potential risk of dual use of the technology by nefarious
339"
SMALL MOLECULES,0.6736474694589878,"actors [57].
340"
SMALL MOLECULES,0.675392670157068,"Limitations and Future Work
Aside from the molecular modelling tasks, our empirical evaluations
341"
SMALL MOLECULES,0.6771378708551483,"in this paper involved simulated oracles with relatively arbitrary costs. Therefore, future work should
342"
SMALL MOLECULES,0.6788830715532286,"evaluate MF-GFN with practical oracles and sets of costs that reflect their computational or financial
343"
SMALL MOLECULES,0.680628272251309,"demands. Furthermore, we believe a promising avenue that we have not explored in this paper is
344"
SMALL MOLECULES,0.6823734729493892,"the application of MF-GFN in more complex, structured design spaces, such as hybrid (discrete and
345"
SMALL MOLECULES,0.6841186736474695,"continuous) domains [34], as well as multi-fidelity, multi-objective problems [24].
346"
REFERENCES,0.6858638743455497,"References
347"
REFERENCES,0.68760907504363,"[1] Ankit Agrawal and Alok Choudhary. Perspective: Materials informatics and big data: Real-
348"
REFERENCES,0.6893542757417103,"ization of the “fourth paradigm” of science in materials science. Apl Materials, 4(5):053208,
349"
REFERENCES,0.6910994764397905,"2016.
350"
REFERENCES,0.6928446771378709,"[2] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham,
351"
REFERENCES,0.6945898778359512,"Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo
352"
REFERENCES,0.6963350785340314,"Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020.
353"
REFERENCES,0.6980802792321117,"[3] Ali Bashir, Qin Yang, Jinpeng Wang, Stephan Hoyer, Wenchuan Chou, Cory McLean, Geoff
354"
REFERENCES,0.699825479930192,"Davis, Qiang Gong, Zan Armstrong, Junghoon Jang, et al. Machine learning guided aptamer
355"
REFERENCES,0.7015706806282722,"refinement and discovery. Nature Communications, 12(1):2366, 2021.
356"
REFERENCES,0.7033158813263525,"[4] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow
357"
REFERENCES,0.7050610820244329,"network based generative models for non-iterative diverse candidate generation. Advances in
358"
REFERENCES,0.7068062827225131,"Neural Information Processing Systems, 34:27381–27394, 2021.
359"
REFERENCES,0.7085514834205934,"[5] Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio.
360"
REFERENCES,0.7102966841186736,"Gflownet foundations. arXiv preprint arXiv: Arxiv-2111.09266, 2021.
361"
REFERENCES,0.7120418848167539,"[6] Regine S Bohacek, Colin McMartin, and Wayne C Guida. The art and practice of structure-
362"
REFERENCES,0.7137870855148342,"based drug design: a molecular modeling perspective. Medicinal research reviews, 16(1):3–50,
363"
REFERENCES,0.7155322862129145,"1996.
364"
REFERENCES,0.7172774869109948,"[7] Keith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine
365"
REFERENCES,0.7190226876090751,"learning for molecular and materials science. Nature, 559(7715):547–555, 2018.
366"
REFERENCES,0.7207678883071553,"[8] Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical
367"
REFERENCES,0.7225130890052356,"science, pages 273–304, 1995.
368"
REFERENCES,0.7242582897033158,"[9] Konstantinos Chatzilygeroudis, Antoine Cully, Vassilis Vassiliades, and Jean-Baptiste Mouret.
369"
REFERENCES,0.7260034904013961,"Quality-diversity optimization: a novel branch of stochastic optimization. In Black Box Opti-
370"
REFERENCES,0.7277486910994765,"mization, Machine Learning, and No-Free Lunch Theorems, pages 109–135. Springer, 2021.
371"
REFERENCES,0.7294938917975567,"[10] Chi Chen, Yunxing Zuo, Weike Ye, Xiangguo Li, and Shyue Ping Ong. Learning properties
372"
REFERENCES,0.731239092495637,"of ordered and disordered materials from multi-fidelity data. Nature Computational Science,
373"
REFERENCES,0.7329842931937173,"1(1):46–53, 2021.
374"
REFERENCES,0.7347294938917975,"[11] David R Corey, Masad J Damha, and Muthiah Manoharan. Challenges and opportunities for
375"
REFERENCES,0.7364746945898778,"nucleic acid therapeutics. nucleic acid therapeutics, 32(1):8–13, 2022.
376"
REFERENCES,0.7382198952879581,"[12] Payel Das, Tom Sercu, Kahini Wadhawan, Inkit Padhi, Sebastian Gehrmann, Flaviu Cipcigan,
377"
REFERENCES,0.7399650959860384,"Vijil Chenthamarakshan, Hendrik Strobelt, Cicero Dos Santos, Pin-Yu Chen, et al. Accelerated
378"
REFERENCES,0.7417102966841187,"antimicrobial discovery via deep generative models and molecular dynamics simulations. Nature
379"
REFERENCES,0.743455497382199,"Biomedical Engineering, 5(6):613–623, 2021.
380"
REFERENCES,0.7452006980802792,"[13] Aryan Deshwal and Janardhan Rao Doppa. Combining latent space and structured kernels for
381"
REFERENCES,0.7469458987783595,"bayesian optimization over combinatorial spaces. In Neural Information Processing Systems,
382"
REFERENCES,0.7486910994764397,"2021.
383"
REFERENCES,0.7504363001745201,"[14] Clyde Fare, Peter Fenner, Matthew Benatan, Alessandro Varsi, and Edward O Pyzer-Knapp. A
384"
REFERENCES,0.7521815008726004,"multi-fidelity machine learning approach to high throughput materials screening. npj Computa-
385"
REFERENCES,0.7539267015706806,"tional Materials, 8(1):257, 2022.
386"
REFERENCES,0.7556719022687609,"[15] Francesco Di Fiore, Michela Nardelli, and Laura Mainini. Active learning and bayesian
387"
REFERENCES,0.7574171029668412,"optimization: a unified perspective to learn with a goal. arXiv preprint arXiv: Arxiv-2303.01560,
388"
REFERENCES,0.7591623036649214,"2023.
389"
REFERENCES,0.7609075043630017,"[16] Jose Pablo Folch, Robert M Lee, Behrang Shafei, David Walz, Calvin Tsay, Mark van der
390"
REFERENCES,0.7626527050610821,"Wilk, and Ruth Misener. Combining multi-fidelity modelling and asynchronous batch bayesian
391"
REFERENCES,0.7643979057591623,"optimization. Computers & Chemical Engineering, 172:108194, 2023.
392"
REFERENCES,0.7661431064572426,"[17] Adam Evan Foster.
Variational, Monte Carlo and policy-based approaches to Bayesian
393"
REFERENCES,0.7678883071553229,"experimental design. PhD thesis, University of Oxford, 2021.
394"
REFERENCES,0.7696335078534031,"[18] Peter I. Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv: Arxiv-1807.02811,
395"
REFERENCES,0.7713787085514834,"2018.
396"
REFERENCES,0.7731239092495636,"[19] Roman Garnett. Bayesian optimization. Cambridge University Press, 2023.
397"
REFERENCES,0.774869109947644,"[20] Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, and Richard Mann.
398"
REFERENCES,0.7766143106457243,"Bayesian optimal active search and surveying. arXiv preprint arXiv:1206.6406, 2012.
399"
REFERENCES,0.7783595113438045,"[21] Antoine Grosnit, Rasul Tutunov, Alexandre Max Maraval, Ryan-Rhys Griffiths, Alexander I.
400"
REFERENCES,0.7801047120418848,"Cowen-Rivers, Lin Yang, Lin Zhu, Wenlong Lyu, Zhitang Chen, Jun Wang, Jan Peters, and
401"
REFERENCES,0.7818499127399651,"Haitham Bou-Ammar. High-dimensional bayesian optimisation with variational autoencoders
402"
REFERENCES,0.7835951134380453,"and deep metric learning. arXiv preprint arXiv: Arxiv-2106.03609, 2021.
403"
REFERENCES,0.7853403141361257,"[22] Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure FP
404"
REFERENCES,0.787085514834206,"Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et al.
405"
REFERENCES,0.7888307155322862,"Biological sequence design with gflownets. In International Conference on Machine Learning,
406"
REFERENCES,0.7905759162303665,"pages 9786–9801. PMLR, 2022.
407"
REFERENCES,0.7923211169284468,"[23] Moksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, and
408"
REFERENCES,0.794066317626527,"Yoshua Bengio. Gflownets for ai-driven scientific discovery. Digital Discovery, 2023.
409"
REFERENCES,0.7958115183246073,"[24] Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua
410"
REFERENCES,0.7975567190226877,"Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective gflownets. arXiv preprint
411"
REFERENCES,0.7993019197207679,"arXiv:2210.12765, 2022.
412"
REFERENCES,0.8010471204188482,"[25] Shali Jiang, Roman Garnett, and Benjamin Moseley. Cost effective active search. Advances in
413"
REFERENCES,0.8027923211169284,"Neural Information Processing Systems, 32, 2019.
414"
REFERENCES,0.8045375218150087,"[26] Shali Jiang, Gustavo Malkomes, Geoff Converse, Alyssa Shofner, Benjamin Moseley, and
415"
REFERENCES,0.806282722513089,"Roman Garnett. Efficient nonmyopic active search. In International Conference on Machine
416"
REFERENCES,0.8080279232111692,"Learning, pages 1714–1723. PMLR, 2017.
417"
REFERENCES,0.8097731239092496,"[27] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
418"
REFERENCES,0.8115183246073299,"neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al.
419"
REFERENCES,0.8132635253054101,"Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.
420"
REFERENCES,0.8150087260034904,"[28] Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff Schneider, and Barnabas
421"
REFERENCES,0.8167539267015707,"Poczos. Multi-fidelity gaussian process bandit optimisation, 2019.
422"
REFERENCES,0.8184991273996509,"[29] Michael Kilgour, Tao Liu, Brandon D Walker, Pengyu Ren, and Lena Simine. E2edna: Simula-
423"
REFERENCES,0.8202443280977313,"tion protocol for dna aptamers with ligands. Journal of Chemical Information and Modeling,
424"
REFERENCES,0.8219895287958116,"61(9):4139–4144, 2021.
425"
REFERENCES,0.8237347294938918,"[30] Ross D King, Kenneth E Whelan, Ffion M Jones, Philip GK Reiser, Christopher H Bryant,
426"
REFERENCES,0.8254799301919721,"Stephen H Muggleton, Douglas B Kell, and Stephen G Oliver. Functional genomic hypothesis
427"
REFERENCES,0.8272251308900523,"generation and experimentation by a robot scientist. Nature, 427(6971):247–252, 2004.
428"
REFERENCES,0.8289703315881326,"[31] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
429"
REFERENCES,0.8307155322862129,"[32] Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik.
430"
REFERENCES,0.8324607329842932,"Self-referencing embedded strings (selfies): A 100% robust molecular string representation.
431"
REFERENCES,0.8342059336823735,"Machine Learning: Science and Technology, 1(4):045024, 2020.
432"
REFERENCES,0.8359511343804538,"[33] A Gilad Kusne, Heshan Yu, Changming Wu, Huairuo Zhang, Jason Hattrick-Simpers, Brian
433"
REFERENCES,0.837696335078534,"DeCost, Suchismita Sarker, Corey Oses, Cormac Toher, Stefano Curtarolo, et al. On-the-
434"
REFERENCES,0.8394415357766143,"fly closed-loop materials discovery via bayesian active learning. Nature communications,
435"
REFERENCES,0.8411867364746946,"11(1):5966, 2020.
436"
REFERENCES,0.8429319371727748,"[34] Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex
437"
REFERENCES,0.8446771378708552,"Hernández-García, Léna Néhale Ezzine, Yoshua Bengio, and Nikolay Malkin. A theory of
438"
REFERENCES,0.8464223385689355,"continuous generative flow networks. International Conference on Machine Learning, 2023.
439"
REFERENCES,0.8481675392670157,"[35] Shibo Li, Robert M Kirby, and Shandian Zhe. Deep multi-fidelity active learning of high-
440"
REFERENCES,0.849912739965096,"dimensional outputs. arXiv preprint arXiv:2012.00901, 2020.
441"
REFERENCES,0.8516579406631762,"[36] Shibo Li, Wei Xing, Mike Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via
442"
REFERENCES,0.8534031413612565,"deep neural networks. Advances in Neural Information Processing Systems, 2020.
443"
REFERENCES,0.8551483420593369,"[37] Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory
444"
REFERENCES,0.8568935427574171,"balance: Improved credit assignment in gflownets, 2022.
445"
REFERENCES,0.8586387434554974,"[38] Henry B. Moss, David S. Leslie, Javier Gonzalez, and Paul Rayson. Gibbon: General-purpose
446"
REFERENCES,0.8603839441535777,"information-based bayesian optimisation, 2021.
447"
REFERENCES,0.8621291448516579,"[39] Hagen Neugebauer, Fabian Bohle, Markus Bursch, Andreas Hansen, and Stefan Grimme.
448"
REFERENCES,0.8638743455497382,"Benchmark study of electrochemical redox potentials calculated with semiempirical and dft
449"
REFERENCES,0.8656195462478184,"methods. The Journal of Physical Chemistry A, 124(35):7166–7176, 2020.
450"
REFERENCES,0.8673647469458988,"[40] Quan Nguyen, Arghavan Modiri, and Roman Garnett. Nonmyopic multifidelity acitve search.
451"
REFERENCES,0.8691099476439791,"In International Conference on Machine Learning, pages 8109–8118. PMLR, 2021.
452"
REFERENCES,0.8708551483420593,"[41] Robert G Parr. Density functional theory of atoms and molecules. In Horizons of Quantum
453"
REFERENCES,0.8726003490401396,"Chemistry: Proceedings of the Third International Congress of Quantum Chemistry Held at
454"
REFERENCES,0.8743455497382199,"Kyoto, Japan, October 29-November 3, 1979, pages 5–15. Springer, 1980.
455"
REFERENCES,0.8760907504363001,"[42] Benjamin Peherstorfer, Karen Willcox, and Max Gunzburger. Survey of multifidelity methods
456"
REFERENCES,0.8778359511343804,"in uncertainty propagation, inference, and optimization. Siam Review, 60(3):550–591, 2018.
457"
REFERENCES,0.8795811518324608,"[43] John P Perdew and Karla Schmidt. Jacob’s ladder of density functional approximations for
458"
REFERENCES,0.881326352530541,"the exchange-correlation energy. In AIP Conference Proceedings, volume 577, pages 1–20.
459"
REFERENCES,0.8830715532286213,"American Institute of Physics, 2001.
460"
REFERENCES,0.8848167539267016,"[44] Paris Perdikaris, M. Raissi, Andreas C. Damianou, ND Lawrence, and George Em Karniadakis.
461"
REFERENCES,0.8865619546247818,"Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling. Proceedings
462"
REFERENCES,0.8883071553228621,"of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473, 2017.
463"
REFERENCES,0.8900523560209425,"[45] Malak Pirtskhalava, Anthony A Amstrong, Maia Grigolava, Mindia Chubinidze, Evgenia Alim-
464"
REFERENCES,0.8917975567190227,"barashvili, Boris Vishnepolsky, Andrei Gabrielian, Alex Rosenthal, Darrell E Hurt, and Michael
465"
REFERENCES,0.893542757417103,"Tartakovsky. Dbaasp v3: database of antimicrobial/cytotoxic activity and structure of peptides
466"
REFERENCES,0.8952879581151832,"as a resource for development of new therapeutics. Nucleic acids research, 49(D1):D288–D297,
467"
REFERENCES,0.8970331588132635,"2021.
468"
REFERENCES,0.8987783595113438,"[46] Pavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of
469"
REFERENCES,0.900523560209424,"drug-like chemical space based on gdb-17 data. Journal of computer-aided molecular design,
470"
REFERENCES,0.9022687609075044,"27:675–679, 2013.
471"
REFERENCES,0.9040139616055847,"[47] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine
472"
REFERENCES,0.9057591623036649,"Learning. The MIT Press, 11 2005.
473"
REFERENCES,0.9075043630017452,"[48] Herbert E. Robbins. Some aspects of the sequential design of experiments. Bulletin of the
474"
REFERENCES,0.9092495636998255,"American Mathematical Society, 58:527–535, 1952.
475"
REFERENCES,0.9109947643979057,"[49] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
476"
REFERENCES,0.912739965095986,"policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
477"
REFERENCES,0.9144851657940664,"[50] Burr Settles. Active learning literature survey. Independent Technical Report, 2009.
478"
REFERENCES,0.9162303664921466,"[51] David S Sholl and Janice A Steckel. Density functional theory: a practical introduction. John
479"
REFERENCES,0.9179755671902269,"Wiley & Sons, 2022.
480"
REFERENCES,0.9197207678883071,"[52] András Sobester, Alexander Forrester, and Andy Keane. Appendix: Example Problems, pages
481"
REFERENCES,0.9214659685863874,"195–203. John Wiley & Sons, Ltd, 2008.
482"
REFERENCES,0.9232111692844677,"[53] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-fidelity bayesian
483"
REFERENCES,0.924956369982548,"optimization with gaussian processes. In International Conference on Artificial Intelligence
484"
REFERENCES,0.9267015706806283,"and Statistics, 2018.
485"
REFERENCES,0.9284467713787086,"[54] Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian pro-
486"
REFERENCES,0.9301919720767888,"cess optimization in the bandit setting: No regret and experimental design. arXiv preprint
487"
REFERENCES,0.9319371727748691,"arXiv:0912.3995, 2009.
488"
REFERENCES,0.9336823734729494,"[55] Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M
489"
REFERENCES,0.9354275741710296,"Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al.
490"
REFERENCES,0.93717277486911,"A deep learning approach to antibiotic discovery. Cell, 180(4):688–702, 2020.
491"
REFERENCES,0.9389179755671903,"[56] Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro
492"
REFERENCES,0.9406631762652705,"Takeuchi, and Masayuki Karasuyama. Multi-fidelity Bayesian optimization with max-value
493"
REFERENCES,0.9424083769633508,"entropy search and its parallelization. In Hal Daumé III and Aarti Singh, editors, Proceedings
494"
REFERENCES,0.944153577661431,"of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
495"
REFERENCES,0.9458987783595113,"Machine Learning Research, pages 9334–9345. PMLR, 13–18 Jul 2020.
496"
REFERENCES,0.9476439790575916,"[57] Fabio Urbina, Filippa Lentzos, Cédric Invernizzi, and Sean Ekins. Dual use of artificial-
497"
REFERENCES,0.9493891797556719,"intelligence-powered drug discovery. Nature Machine Intelligence, 4(3):189–191, 2022.
498"
REFERENCES,0.9511343804537522,"[58] Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization,
499"
REFERENCES,0.9528795811518325,"2018.
500"
REFERENCES,0.9546247818499127,"[59] Manfred KK Warmuth, Gunnar Rätsch, Michael Mathieson, Jun Liao, and Christian Lemmen.
501"
REFERENCES,0.956369982547993,"Active learning in the drug discovery process. Advances in Neural information processing
502"
REFERENCES,0.9581151832460733,"systems, 14, 2001.
503"
REFERENCES,0.9598603839441536,"[60] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel
504"
REFERENCES,0.9616055846422339,"learning. In Artificial intelligence and statistics, pages 370–378. PMLR, 2016.
505"
REFERENCES,0.9633507853403142,"[61] Jian Wu, Saul Toscano-Palmerin, Peter I. Frazier, and Andrew Gordon Wilson. Practical
506"
REFERENCES,0.9650959860383944,"multi-fidelity bayesian optimization for hyperparameter tuning, 2019.
507"
REFERENCES,0.9668411867364747,"[62] Dezhen Xue, Prasanna V Balachandran, John Hogden, James Theiler, Deqing Xue, and Turab
508"
REFERENCES,0.9685863874345549,"Lookman. Accelerated search for materials with targeted properties by adaptive design. Nature
509"
REFERENCES,0.9703315881326352,"communications, 7(1):1–9, 2016.
510"
REFERENCES,0.9720767888307156,"[63] Joseph D Yesselman, Daniel Eiler, Erik D Carlson, Michael R Gotrik, Anne E d’Aquino,
511"
REFERENCES,0.9738219895287958,"Alexandra N Ooms, Wipapat Kladwang, Paul D Carlson, Xuesong Shi, David A Costantino, et al.
512"
REFERENCES,0.9755671902268761,"Computational design of three-dimensional rna structure and function. Nature nanotechnology,
513"
REFERENCES,0.9773123909249564,"14(9):866–873, 2019.
514"
REFERENCES,0.9790575916230366,"[64] Ruihao Yuan, Zhen Liu, Prasanna V Balachandran, Deqing Xue, Yumei Zhou, Xiangdong Ding,
515"
REFERENCES,0.9808027923211169,"Jun Sun, Dezhen Xue, and Turab Lookman. Accelerated discovery of large electrostrains in
516"
REFERENCES,0.9825479930191972,"batio3-based piezoelectrics using active learning. Advanced materials, 30(7):1702884, 2018.
517"
REFERENCES,0.9842931937172775,"[65] Joseph N Zadeh, Conrad D Steenberg, Justin S Bois, Brian R Wolfe, Marshall B Pierce, Asif R
518"
REFERENCES,0.9860383944153578,"Khan, Robert M Dirks, and Niles A Pierce. Nupack: Analysis and design of nucleic acid
519"
REFERENCES,0.987783595113438,"systems. Journal of computational chemistry, 32(1):170–173, 2011.
520"
REFERENCES,0.9895287958115183,"[66] Wenhu Zhou, Runjhun Saran, and Juewen Liu. Metal sensing by dna. Chemical reviews,
521"
REFERENCES,0.9912739965095986,"117(12):8272–8325, 2017.
522"
REFERENCES,0.9930191972076788,"[67] C Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-Domingo,
523"
REFERENCES,0.9947643979057592,"Caleb Ho, Weihua Hu, Thibaut Lavril, Aini Palizhati, Morgane Riviere, et al. An introduction
524"
REFERENCES,0.9965095986038395,"to electrocatalyst design using machine learning for renewable energy storage. arXiv preprint
525"
REFERENCES,0.9982547993019197,"arXiv:2010.09435, 2020.
526"
