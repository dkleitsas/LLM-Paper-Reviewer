Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002331002331002331,"Cross-Entropy Method (CEM) is a popular approach to planning in model-based
reinforcement learning. It has so far always taken a centralized approach where
the sampling distribution is updated centrally based on the result of a top-k oper-
ation applied to all samples. We show that such a centralized approach makes
CEM vulnerable to local optima and impairs its sample efﬁciency, even in a
one-dimensional multi-modal optimization task. In this paper, we propose De-
centralized CEM (DecentCEM) where an ensemble of CEM instances run inde-
pendently from one another and each performs a local improvement of its own
sampling distribution. In the exemplar optimization task, the proposed decentral-
ized approach DecentCEM ﬁnds the global optimum much more consistently than
the centralized CEM approaches that use either a single or a mixture of Gaussian
distributions. Also, we show that DecentCEM is theoretically sound. Further, we
extend the decentralized approach to sequential decision-making problems where
we show in several continuous control benchmark environments that it provides
an effective mechanism to improve the performance of CEM algorithms, under
the same sample budget for planning."
INTRODUCTION,0.004662004662004662,"1
INTRODUCTION"
INTRODUCTION,0.006993006993006993,"Model-based reinforcement learning (MBRL) uses a model as a proxy of the environment for plan-
ning actions in multiple steps. This paper studies planning in MBRL with a speciﬁc focus on the
Cross-Entropy Method (CEM) (De Boer et al., 2005; Mannor et al., 2003), which is popular in
MBRL due to its ease of use and strong empirical performance (Chua et al., 2018; Hafner et al.,
2019; Wang & Ba, 2020; Zhang et al., 2021; Yang et al., 2020). CEM is a stochastic, derivative-
free optimization method.
It uses a sampling distribution to generate imaginary trajectories of
environment-agent interactions with the model. These trajectories are then ranked based on their
returns computed from the rewards given by the model. The sampling distribution is updated to
increase the likelihood of producing the top-k trajectories with higher returns. These steps are it-
erated and eventually yield an improved distribution over the action sequences to guide the action
execution in the real environment."
INTRODUCTION,0.009324009324009324,"Despite the strong empirical performance of CEM for planning, it is prone to two problems: (1)
lower sample efﬁciency as the dimensionality of solution space increases, and (2) the Gaussian dis-
tribution that is commonly used for sampling may cause the optimization to get stuck in local optima
of multi-modal solution spaces commonly seen in real-world problems. Previous works addressing
these problems either add gradient-based updates of the samples to optimize the parameters of CEM,
or adopt more expressive sampling distributions, such as using Gaussian Mixture Model (Okada &
Taniguchi, 2020) or masked auto-regressive neural network (Hakhamaneshi et al., 2020). Neverthe-
less, all CEM implementations to date are limited to a centralized formulation where the ranking
step involves all samples. As analyzed below and in Section 3, such a centralized design makes
CEM vulnerable to local optima and impairs its sample efﬁciency."
INTRODUCTION,0.011655011655011656,"We propose Decentralized CEM (DecentCEM) to address the above problems. Rather than ranking
all samples, as in the centralized design, our method distribute the sampling budget across an en-
semble of CEM instances. These instances run independently from one another, and each performs
a local improvement of its own sampling distribution based on the ranking of its generated samples."
INTRODUCTION,0.013986013986013986,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016317016317016316,"The best action is then aggregated by taking an arg max among the solution of the instances. It
recovers the conventional CEM when the number of instance is one."
INTRODUCTION,0.018648018648018648,"(a) Centralized CEM
(b) Decentralized CEM"
INTRODUCTION,0.02097902097902098,"Figure 1:
Illustration of CEM approaches
in optimization. Shades of red indicate rela-
tive value of the 2D optimization landscape:
brighter is better. Optimal solutions are near
bottom left corner of the solution space. Blue
dots
are top-k samples, and black dots
are other samples. Open dots
represent the
sampling distributions with size of dots indi-
cating number of generated samples."
INTRODUCTION,0.023310023310023312,"We hypothesize that by shifting to this decentralized
design, CEM can be less susceptible to premature
convergence caused by the centralized ranking step.
As illustrated in Fig. 1, the centralized sampling dis-
tribution exhibits a bias toward the sub-optimal so-
lutions near top right, due to the global top-k rank-
ing. This bias would occur regardless of the family
of distributions used. In comparison, a decentralized
approach could maintain enough diversity thanks to
its local top-k ranking in each sampling instance."
INTRODUCTION,0.02564102564102564,"Through a one-dimensional multi-modal optimiza-
tion problem in Section 3, we show that DecentCEM
empirically ﬁnds the global optimum more consis-
tently than centralized CEM approaches that use ei-
ther a single Gaussian or a mixture of Gaussian dis-
tributions. Also we show that DecentCEM is the-
oretically sound that it converges almost surely to
a local optimum. We further apply DecentCEM to
sequential decision making problems and use neu-
ral networks to parameterize the sampling distribu-
tion in each CEM instance. Empirical results in sev-
eral continuous control benchmarks suggest that De-
centCEM offers an effective mechanism to improve
the sample efﬁciency over the baseline CEM methods under the same sample budget for planning."
PRELIMINARIES,0.027972027972027972,"2
PRELIMINARIES"
PRELIMINARIES,0.030303030303030304,"We consider a Markov Decision Process (MDP) speciﬁed by (S,A,R,P,γ,d0,T). S ⊂Rds is the
state space, A ⊂Rda is the action space. R : S × A →R is the reward function that maps a state
and action pair to a real-valued reward. P(s′|s, a) : S × A × S →R+ is the transition probability
from a state and action pair s, a to the next state s′. γ ∈[0, 1] is the discount factor. d0 denotes the
distribution of the initial state s0. At time step t, the agent receives a state st 1 and takes an action
at according to a policy π(·|s) that maps the state to a probability distribution over the action space.
The environment transitions to the next state st+1 ∼P(·|st, at) and gives a reward rt = R(st, at)
to the agent 2. The return Gt = PT
i=0 γirt+i, is the sum of discounted reward within an episode
length of T. The agent aims to ﬁnd a policy π that maximizes the expected return. We denote the
learned model in MBRL as fω(·|s, a), which is parameterized by ω and approximates P(·|s, a)."
PRELIMINARIES,0.03263403263403263,"Planning with the Cross Entropy Method
Planning in MBRL is about leveraging the model to
ﬁnd the best action in terms of its return. Model-Predictive-Control (MPC) performs online planning
at each time step up to a horizon to ﬁnd the optimal action sequence:"
PRELIMINARIES,0.03496503496503497,"πMPC(st) = arg max
at:t+H−1
E[ΣH−1
i=0 γir(st+i, at+i) + γHV (sH)]
(1)"
PRELIMINARIES,0.037296037296037296,"where H is the planning horizon, at:t+H−1 denotes the action sequence from time step t to t+H−1,
and V (sH) is the terminal value function at the end of the planning horizon. The ﬁrst action in this
sequence is executed and the rest are discarded. The agent then re-plans at the next time step."
PRELIMINARIES,0.039627039627039624,"The Cross-Entropy Method (CEM) is a gradient-free optimization method that can be used for solv-
ing Eq. (1). The workﬂow is shown in Fig. 2. CEM planning starts by generating N samples
{τj}N
j=1 = {(ˆaj,0, ˆaj,1, · · · , ˆaj,H−1)}N
j=1 from an initial sampling distribution gφ(τ) parameterized"
PRELIMINARIES,0.04195804195804196,"1We assume full observability, i.e., we assume that the agent has access to the state.
2We assume that the agent receives the true reward. This makes the problem easier but is unfair to MBRL
methods that do not assume this. Thus, we will limit comparison to methods that also make this assumption."
PRELIMINARIES,0.04428904428904429,Under review as a conference paper at ICLR 2022
PRELIMINARIES,0.046620046620046623,Figure 2: Cross Entropy Method (CEM) for Planning in MBRL
PRELIMINARIES,0.04895104895104895,"by φ, where each sample τj is an action sequence from the current time step up to the planning
horizon H. The domain of gφ(τ) has a dimension of dτ = daH."
PRELIMINARIES,0.05128205128205128,"Using a model f, CEM generates imaginary rollouts based on the action sequence {τj} (in the
case of a stochastic model) and estimate the associated value v(τj) = E[ΣH−1
i=0 γir(sj,i, aj,i)] where
sj,0 is the current state s and sj,i+1 ∼f(sj,i, aj,i). The terminal value γHV (sj,H) is omitted
here following convention in the CEM planning literature but the MPC performance can be further
improved if paired with an accurate value predictor (Bertsekas, 2005; Lowrey et al., 2019). The
sampling distribution is then updated by ﬁtting to the current top-k samples in terms of their value
estimates v(τj), using the Maximum Likelihood Estimation (MLE) which solves:"
PRELIMINARIES,0.053613053613053616,"φ′ = arg max
φ N
X"
PRELIMINARIES,0.055944055944055944,"j=1
1(v(τj) ≥vth) log gφ(τj)
(2)"
PRELIMINARIES,0.05827505827505827,"where vth is the threshold equal to the value of the k-th best sample and 1(·) is the indicator function.
In practice, the update to the distribution parameters are smoothed by φl+1 = αφ′+(1−α)φl where
α ∈[0, 1] is a smoothing parameter that balances between the solution to Eq. (2) and the parameter
at the current internal iteration l. CEM repeats this process of sampling and distribution update in an
inner-loop, until it reaches the stopping condition. In practice, it is stopped when either a maximum
number of iterations has been reached or the parameters have not changed for a few iterations. The
output of CEM is an action sequence, typically set as the expectation3 of the most recent sampling
distribution for uni-modal distributions such as Gaussians ˆµ = E(gφ) = (ˆa0, ˆa1, · · · , ˆaH−1)."
PRELIMINARIES,0.06060606060606061,"Choices of Sampling Distributions in CEM
A common choice of the sampling distribution is a
multivariate Gaussian distribution under which Eq.(2) has a straight-forward analytical solution. But
the uni-modal nature of Gaussian makes it inadequate in solving multi-modal optimization that often
occur in MBRL. To increase the capacity of the distribution, a Gaussian Mixture Model (GMM) can
be used (Okada & Taniguchi, 2020). We denote such an approach as CEM-GMM. Going forward,
we use CEM to refer to the vanilla version that uses a Gaussian distribution. Computationally,
the major difference between CEM and CEM-GMM is that the distribution update in CEM-GMM
involves solving for more parameters. Detailed steps can be found in Okada & Taniguchi (2020)."
DECENTRALIZED CEM,0.06293706293706294,"3
DECENTRALIZED CEM"
DECENTRALIZED CEM,0.06526806526806526,"In this section, we ﬁrst introduce the formulation of the proposed decentralized approach called the
Decentralized CEM (DecentCEM). Then we illustrate the intuition behind the proposed approach
using a one-dimensional synthetic multi-modal optimization example where we show the issues of
the existing CEM methods and how they can be addressed by DecentCEM."
DECENTRALIZED CEM,0.0675990675990676,"Formulation of DecentCEM
DecentCEM is composed of an ensemble of multiple CEM in-
stances indexed by i, each having its own sampling distributions gφi. They can be described by
a set of distribution parameters Φ = {φi}M
i=1. Each instance i manages its own sampling and dis-
tribution update by the steps described in Section 2, independently from other instances. Note that
the number of samples and elites are evenly split among the M instances. The top- k"
DECENTRALIZED CEM,0.06993006993006994,"M sample sets
are decentralized and managed by each instance independently whereas the centralized approach
only keeps one set of top-k samples regardless of the distribution family used. After the stopping"
DECENTRALIZED CEM,0.07226107226107226,3Other options are discussed in Appendix A.2
DECENTRALIZED CEM,0.07459207459207459,Under review as a conference paper at ICLR 2022
DECENTRALIZED CEM,0.07692307692307693,"condition is reached for all instances, the ﬁnal sampling distribution is taken as the best distribution
in the set Φ according to (the arg max uses a deterministic tie-breaking):"
DECENTRALIZED CEM,0.07925407925407925,"φDecentCEM = arg max
φi∈Φ
Eφi[v(x)] ≈arg max
φi∈Φ N
M
X"
DECENTRALIZED CEM,0.08158508158508158,"j=1
v(τi,j)
(3)"
DECENTRALIZED CEM,0.08391608391608392,"where Eφi[v(x)] denotes the expectation with respect to the distribution gφi, approximated by the
sample mean. When M = 1, it recovers the conventional CEM method."
DECENTRALIZED CEM,0.08624708624708624,"8
6
4
2
0
2
4
6
8
x 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 f(x)"
DECENTRALIZED CEM,0.08857808857808858,-0.888@x=-6.217
DECENTRALIZED CEM,0.09090909090909091,-0.119@x=-4.197
DECENTRALIZED CEM,0.09324009324009325,-1.728@x=-2.296
DECENTRALIZED CEM,0.09557109557109557,-1.488@x=-0.549
DECENTRALIZED CEM,0.0979020979020979,-0.014@x=1.398
DECENTRALIZED CEM,0.10023310023310024,-1.2@x=3.387
DECENTRALIZED CEM,0.10256410256410256,-0.317@x=7
DECENTRALIZED CEM,0.1048951048951049,-1.9@x*=5.146
DECENTRALIZED CEM,0.10722610722610723,"100
200
500
1000
Population Size 1.950 1.900 1.850 1.800 1.750 1.700 1.728"
DECENTRALIZED CEM,0.10955710955710955,Cost f(x)
DECENTRALIZED CEM,0.11188811188811189,local optimum f(x)=-1.728@x=-2.296
DECENTRALIZED CEM,0.11421911421911422,global optimum f(x)=-1.9@x=5.146
DECENTRALIZED CEM,0.11655011655011654,"CEM
CEM-GMM
DecentCEM"
DECENTRALIZED CEM,0.11888111888111888,"Figure 3: Left: The objective function in a 1D optimization task. Right: Comparison of our proposed
DecentCEM method to CEM and CEM-GMM, wherein the line and the shaded region denote the
mean and the min/max cost from 10 independent runs. ˆx: resulting solution of each method."
DECENTRALIZED CEM,0.12121212121212122,"Motivational Example
Consider a one dimensional multi-modal optimization problem shown in
Fig.3 (Left). There are eight local optima, including one global optimum f(x∗) = −1.9 where x∗=
5.146. This objective function mimics the RL value landscape that has many local optima, as shown
by Wang & Ba (2020). This optimization problem is “easy” in the sense that a grid search over the
domain can get us a solution close to the global optimum. However, only our proposed DecentCEM
method successfully converges to the global optimum consistently under varying population size
(i.e., number of samples) and random runs, as shown in Fig.3 (Right)4."
DECENTRALIZED CEM,0.12354312354312354,"Both CEM-GMM and the proposed DecentCEM are equipped with multiple sampling distributions.
The fact that CEM-GMM is outperformed by DecentCEM may appear surprising. To gain some
insights, we illustrate in Fig. 4 how the sampling distribution evolves during the iterative update
(more details in Fig. 9 in Appendix). CEM updated the unimodal distribution toward a local op-
timum despite seeing the global optimum. CEM-GMM appears to have a similar issue. During
MLE on the top-k samples, it moved most distribution components towards the same local optimum
which quickly lead to mode collapse. On the contrary, DecentCEM successfully escaped the local
optima thanks to its independent distribution update over decentralized top-k samples and was able
to maintain a decent diversity among the distributions. 2 1 0 1 2 f(x)"
DECENTRALIZED CEM,0.1258741258741259,CEM iter=0
DECENTRALIZED CEM,0.1282051282051282,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x p(x)"
DECENTRALIZED CEM,0.13053613053613053,CEM iter=4
DECENTRALIZED CEM,0.13286713286713286,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x 2 1 0 1 2 f(x)"
DECENTRALIZED CEM,0.1351981351981352,CEM-GMM iter=0
DECENTRALIZED CEM,0.13752913752913754,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x p(x)"
DECENTRALIZED CEM,0.13986013986013987,CEM-GMM iter=4
DECENTRALIZED CEM,0.14219114219114218,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x 2 1 0 1 2 f(x)"
DECENTRALIZED CEM,0.1445221445221445,DecentCEM iter=0
DECENTRALIZED CEM,0.14685314685314685,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x p(x)"
DECENTRALIZED CEM,0.14918414918414918,DecentCEM iter=4
DECENTRALIZED CEM,0.15151515151515152,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
DECENTRALIZED CEM,0.15384615384615385,"Figure 4: How the sampling distributions evolve in the 1D optimization task, after the speciﬁed
iteration. Symbols include samples
, elites
, local optima
, global . 2nd row in each ﬁgure
shows the weighted p.d.f of individual distribution. Population size: 200."
DECENTRALIZED CEM,0.1561771561771562,"GMM suits density estimation problems like distribution-based clustering where the samples are
drawn from a ﬁxed true distribution that can be represented by multi-modal Gaussians. However,"
DECENTRALIZED CEM,0.1585081585081585,"4For a fair comparison, hyperparameter search has been conducted on all three methods for each population
size (Appendix A)."
DECENTRALIZED CEM,0.16083916083916083,Under review as a conference paper at ICLR 2022
DECENTRALIZED CEM,0.16317016317016317,"in CEM for optimization, exploration is coupled with density estimation: the sampling distribution
in CEM is not ﬁxed but rather gets updated iteratively toward the top-k samples. And the “true”
distribution in optimization puts uniform non-zero densities to the global optima and zero densities
everywhere else. When there is a unique global optimum, it degenerates into a Dirac measure that
assigns the entire density to the optimum. Density estimation of such a distribution only needs one
Gaussian but the exploration is challenging. In other words, the conditions for GMM to work well
are not necessarily met when used as the sampling distribution in CEM. CEM-GMM is subject to
mode collapse during the iterative top-k greediﬁcation, causing premature convergence, as observed
in Fig 4. In comparison, our proposed decentralized approach takes care of the exploration aspect by
running multiple CEM instances independently, each performing its own local improvement. This
is shown to be effective from this optimization example and the benchmark results in Section 6.
CEM-GMM only consistently converge to the global optimum when we increase the population size
to the maximum 1,000 which causes expensive computations. Our proposed DecentCEM runs more
than 100 times faster than CEM-GMM at this population size, shown in Table A.3 in Appendix."
DECENTRALIZED CEM,0.1655011655011655,"Convergence of DecentCEM
We state the convergence result of DecentCEM in Theorem 3.1. We
show that the previous convergence result of CEM (Hu et al., 2011) applies to DecentCEM under the
same sample budget. The key observation is that the convergence property of each CEM instance
still holds since the number of samples in each instance is only changed by a constant factor (the
number of instances). We leave the detailed proof to appendix H.
Theorem 3.1 (Convergence of DecentCEM). If Assumptions 1-5 hold for a CEM instance described
in Algorithm 3 and we decentralize it by evenly dividing its sample size Nk into M CEM instances in
DecentCEM algorithm that satisﬁes Assumption 6, then the sequence of iterates {ηi,k} generated by
each CEM instance indexed by i converges almost surely to an internally chain recurrent set (Hirsch
et al., 2001) of Equation 7. Furthermore, the solution of DecentCEM {ηo,k} converges almost surely
to the best solution of the individual instances in terms of the expected value of Em−1(η)[V (x)]."
DECENTCEM FOR PLANNING IN MBRL,0.16783216783216784,"4
DECENTCEM FOR PLANNING IN MBRL"
DECENTCEM FOR PLANNING IN MBRL,0.17016317016317017,"In this section, we develop two instantiations of DecentCEM for planning in MBRL where the
sampling distributions are parameterized by policy networks."
DECENTCEM FOR PLANNING IN MBRL,0.17249417249417248,"CEM Planning with a Policy Network
In MBRL, CEM is applied to every state separately to
solve the optimization problem stated in Eq. (1). The sampling distribution is typically initialized
to a ﬁxed distribution at the beginning of every episode (Okada & Taniguchi, 2020; Pinneri et al.,
2020), or more frequently at every time step (Hafner et al., 2019). Such initialization schemes are
sample inefﬁcient since there is no mechanism that allows the information of the high-value region
in the value space of one state to generalize to nearby states. Also, the information is discarded
after the initialization. It is hence difﬁcult to scale the approach to higher dimensional solution
spaces, present in many continuous control environments. Wang & Ba (2020) proposed to use a
policy network in CEM planning that helped to mitigate the issues above. They developed two
methods: POPLIN-A that plans in the action space, and POPLIN-P that plans in the parameter space
of the policy network. In POPLIN-A, the policy network is used to learn to output the mean of a
Gaussian sampling distribution of actions. In POPLIN-P, the policy network parameters serve as the
initialization of the mean of the sampling distribution of parameters. The improved policy network
can then be used to generate an action. They show that when compared to the vanilla method of
using a ﬁxed sampling distribution in the action space, both modes of CEM planning with such a
learned distribution perform better. The same principle of combining a policy network with CEM
can be applied to the DecentCEM approach as well, which we will describe next."
DECENTCEM FOR PLANNING IN MBRL,0.17482517482517482,"DecentCEM Planning with an Ensemble of Policy Networks
For better sample efﬁciency in
MBRL setting, we extend DecentCEM to use an ensemble of policy networks to learn the sampling
distributions in the CEM instances. Similar to the POPLIN paper, we develop two instantiations of
DecentCEM, namely DecentCEM-A and DecentCEM-P. The architecture of the proposed algorithm
is illustrated in Fig. 5."
DECENTCEM FOR PLANNING IN MBRL,0.17715617715617715,"DecentCEM-A plans in the action space.
It consists of an ensemble of policy networks fol-
lowed by CEM instances.
Each policy network takes the current state st as input, out-"
DECENTCEM FOR PLANNING IN MBRL,0.1794871794871795,Under review as a conference paper at ICLR 2022
DECENTCEM FOR PLANNING IN MBRL,0.18181818181818182,"puts the parameters θi of the sampling distribution for CEM instance i.
There is no fun-
damental difference from the DecentCEM formulation in Section 3 except that the initializa-
tion of sampling distributions is learned by the policy networks rather than a ﬁxed distribution."
DECENTCEM FOR PLANNING IN MBRL,0.18414918414918416,Policy Network 1
DECENTCEM FOR PLANNING IN MBRL,0.1864801864801865,Policy Network M
DECENTCEM FOR PLANNING IN MBRL,0.1888111888111888,Policy Network 2
DECENTCEM FOR PLANNING IN MBRL,0.19114219114219114,Policy Network M-1
DECENTCEM FOR PLANNING IN MBRL,0.19347319347319347,CEM Instance 1
DECENTCEM FOR PLANNING IN MBRL,0.1958041958041958,CEM Instance 2
DECENTCEM FOR PLANNING IN MBRL,0.19813519813519814,CEM Instance M-1
DECENTCEM FOR PLANNING IN MBRL,0.20046620046620048,CEM Instance M
DECENTCEM FOR PLANNING IN MBRL,0.20279720279720279,"state
action"
DECENTCEM FOR PLANNING IN MBRL,0.20512820512820512,"Training
Inference"
DECENTCEM FOR PLANNING IN MBRL,0.20745920745920746,"Figure 5: DecentCEM planning architecture. ψi = φi
for planning in action space and ψi = θi for planning
in policy network parameter space."
DECENTCEM FOR PLANNING IN MBRL,0.2097902097902098,"The second instantiation DecentCEM-P
plans in the parameter space of the policy
network. The initial sampling distribution
is a Gaussian distribution over the policy
parameter space with the mean at the cur-
rent parameter values. In the arg max op-
eration in Eq. (3), the sample τi,j denotes
the parameters of the policy network. Its
value is obtained by computing the value
of the action sequence generated from the
policy network with the parameters τi,j."
DECENTCEM FOR PLANNING IN MBRL,0.21212121212121213,"The ensemble of policy networks in
both
instantiations
DecentCEM-A
and
DecentCEM-P are initialized with random
weights, which is empirically found to be
adequate to ensure that the output of the
networks do not collapse into the same distribution (Sec. 6 and Appendix F)."
DECENTCEM FOR PLANNING IN MBRL,0.21445221445221446,"Training the Policy Network in DecentCEM
When planning in action space, the policy networks
are trained by behavior cloning, similar to the scheme in POPLIN (Wang & Ba, 2020). Denote the
ﬁrst action in the planned action sequence at time step t by the i-th CEM instance as ˆat,i, the i-th
policy network is trained to mimic ˆat,i and the training objective is minθi Est,ˆat,i∼Di∥aθi(st) −
ˆat,i∥2 where Di denotes the replay buffer with the state and action pairs (st, ˆat,i). aθi(st) is the
action prediction at state st from the policy network parameterized by θi."
DECENTCEM FOR PLANNING IN MBRL,0.21678321678321677,"While the above training scheme can be applied to both planning in action space and parameter
space, we follow the setting parameter average (AVG) (Wang & Ba, 2020) training scheme when
planning in parameter space. The parameter is updated as θi = θi+
1
|Di|
P"
DECENTCEM FOR PLANNING IN MBRL,0.2191142191142191,"δi∈Di δi where Di = {δi}
is a dataset of policy network parameter updates planned from the i-th CEM instance previously. It
is more effective than behavior cloning based on the experimental result reported by Wang & Ba
(2020) and our own preliminary experiments."
DECENTCEM FOR PLANNING IN MBRL,0.22144522144522144,"Note that each policy network in the ensemble is trained independently from the data observed by
its corresponding CEM instance rather than from the aggregated result after taking the arg max.
This allows for enough diversity among the instances. More importantly, it increases the size of the
training dataset for the policy networks compared to the approach taken in POPLIN. For example,
with an ensemble of M instances, there would be M training data samples available from one real
environment interaction, compared to the one data sample in POPLIN-A/P. As a result, DecentCEM
is able to use larger policy networks than is otherwise possible, shown in Sec. 6 and Appendix F."
RELATED WORK,0.22377622377622378,"5
RELATED WORK"
RELATED WORK,0.2261072261072261,"We limit the scope of related works to CEM plannig methods. Vanilla CEM planning in action space
with a single Gaussian distribution has been adopted as the planning method for both simulated and
real-world robot control (Chua et al., 2018; Finn & Levine, 2017; Ebert et al., 2018; Hafner et al.,
2019; Yang et al., 2020; Zhang et al., 2021). Among previous attempts to improve the performance
of CEM-based planning, we see two types of approaches. The ﬁrst type includes CEM in a hybrid
of CEM+X where “X” is some other component or algorithm. POPLIN (Wang & Ba, 2020) is a
prominent example where “X” is a policy network that learns a state conditioned distribution that
initializes the subsequent CEM process. This addition of the policy network allows the CEM to
search in the network parameter space which is shown to have a smoother landscape and better
exploration. Another common choice of “X” is gradient-based adjustment of the samples drawn in
CEM. GradCEM (Bharadhwaj et al., 2020) adjusts the samples in each iteration of CEM by taking"
RELATED WORK,0.22843822843822845,Under review as a conference paper at ICLR 2022
RELATED WORK,0.23076923076923078,"gradient ascent of the return estimate w.r.t the actions. The beneﬁt that this method brings is not
signiﬁcant on benchmark control tasks. CEM-RL (Pourchot & Sigaud, 2019) also combines CEM
with gradient based updates from RL algorithms but the samples are in the parameter space of the
actor network. To improve computational efﬁciency, Lee et al. (2020) proposes an asynchronous
version of CEM-RL where each CEM instance updates the sampling distribution asynchronously
without waiting for other instances to ﬁnish. The downside with both versions of CEM-RL methods
are that they rely on model-free RL algorithms. One can imagine reversing the order of CEM and
RL and using CEM to update the policies/actors of RL agents. The approach taken in (Khadka et al.,
2019) was along similar lines but they used a genetic algorithm instead of CEM."
RELATED WORK,0.2331002331002331,"The second type of approach aims at improving CEM itself. Amos & Yarats (2020) proposes a
fully-differentiable version of CEM called DCEM. The key is to make the top-k selection in CEM
differentiable such that the entire CEM module can be trained in an end-to-end fashion. Despite
cutting down the number of samples needed in CEM, this method does not beat the vanilla CEM in
benchmark test. GACEM (Hakhamaneshi et al., 2020) increase the capacity of the sampling distri-
bution by replacing the Gaussian distribution with an auto-regressive neural network. This change
allows CEM to perform search in multi-modal solution space but it is only veriﬁed in toy examples
and its computation seems too high to be scaled to MBRL tasks. Another method that increases
the capacity of the sampling distribution is PaETS (Okada & Taniguchi, 2020) that uses a GMM
with CEM. It is the approach that we followed for our CEM-GMM implementation. It is not clear
how well it performs in benchmark tasks since their environment setup is modiﬁed to have a range
of actions 5 times larger than the original. Also the running time results in the optimization task
in Sec.3 shows that it is computationally heavier than the CEM and DecentCEM methods, limiting
its use in complex environments. Overall, this second type of approach did not outperform vanilla
CEM, a situation that motivated our move to a decentralized formulation. Macua et al. (2015) pro-
posed a “distributed” CEM that is similar in spirit to our method in that they used multiple sampling
distributions and applied the top-k selection locally to samples from each instance. However, their
instances are cooperative as opposed to being independent as in our work. They applied “collabora-
tive smoothed projection steps” to update each sampling distribution as an average of its neighboring
instances including itself. The updating procedure is more complicated than our proposed method
and proper network topology of the instances is needed: a naive approach of updating according to
all instances will lead to mode collapse since the resulting sampling distributions will be identical.
The method was tested in toy optimization examples only."
EXPERIMENTS,0.23543123543123542,"6
EXPERIMENTS"
EXPERIMENTS,0.23776223776223776,"We evaluate the proposed DecentCEM methods in simulated environments with continuous action
space. The experimental evaluation is mainly setup to understand if DecentCEM improves the per-
formance and sample efﬁciency over conventional CEM approaches."
EXPERIMENTS,0.2400932400932401,"Benchmark Setup
We benchmark the algorithms in several continuous-action control environ-
ments in OpenAI Gym."
EXPERIMENTS,0.24242424242424243,"Environments We run the benchmark in a set of 13 environments commonly used in the MBRL
literature: Pendulum, InvertedPendulum, Cartpole, Acrobot, FixedSwimmer5, Reacher, Hopper,
Walker2D, HalfCheetah, PETS-Reacher3D, PETS-HalfCheetah, PETS-Pusher, Ant. The three en-
vironments preﬁxed by “PETS” are proposed by Chua et al. (2018). Note that MBRL algorithms
often make different assumptions about the dynamics model or the reward function. Their bench-
mark environments are often modiﬁed from the original OpenAI gym environments such that the
respective algorithm is runnable. Whenever possible, we inherit the same environment setup from
that of the respective baseline methods. This is so that the comparison against the baselines is fair.
More details on the environments and their reward functions are in Appendix B."
EXPERIMENTS,0.24475524475524477,"Algorithms The baseline algorithms are PETS (Chua et al., 2018) and POPLIN (Wang & Ba, 2020).
PETS uses CEM with a single Gaussian distribution for planning. The POPLIN algorithm com-
bines a single policy network with CEM. As described in Sec.4, POPLIN comes with two modes:
POPLIN-A and POPLIN-P with the sufﬁx “A” denotes planning in action space and “P” for the"
EXPERIMENTS,0.24708624708624707,"5a modiﬁed version of the original Gym Swimmer environment where the velocity sensor on the neck is
moved to the head. This ﬁx was proposed by Wang & Ba (2020)"
EXPERIMENTS,0.2494172494172494,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2517482517482518,"0
1000
2000
3000
4000
5000
Steps 200 150 100 50 0"
EXPERIMENTS,0.2540792540792541,Average Return
EXPERIMENTS,0.2564102564102564,InvertedPendulum
EXPERIMENTS,0.25874125874125875,"0
2000
4000
6000
8000
10000
Steps 400 300 200 100 0 50"
EXPERIMENTS,0.26107226107226106,Average Return
EXPERIMENTS,0.2634032634032634,Acrobot
EXPERIMENTS,0.26573426573426573,"0
2000
4000
6000
8000
10000
Steps 60 50 40 30 20 10"
EXPERIMENTS,0.2680652680652681,Reacher
EXPERIMENTS,0.2703962703962704,"0
20000
40000
60000
80000
100000
Steps 3000 2000 1000 0 1000 2000"
EXPERIMENTS,0.2727272727272727,Average Return
EXPERIMENTS,0.27505827505827507,Hopper
EXPERIMENTS,0.2773892773892774,"0
20000
40000
60000
80000
100000
Steps 3000 2000 1000 0 1000"
EXPERIMENTS,0.27972027972027974,Walker2D
EXPERIMENTS,0.28205128205128205,"0
20000
40000
60000
80000
100000
Steps 500 0 500 1000 1500 2000 2500"
EXPERIMENTS,0.28438228438228436,Average Return Ant
EXPERIMENTS,0.2867132867132867,"PETS
POPLIN-A
POPLIN-P
DecentCEM-A
DecentCEM-P
SAC
SAC at convergence"
EXPERIMENTS,0.289044289044289,"Figure 6: The learning curves of the proposed DecentCEM methods and the baseline methods on
continuous control environments. The line and shaded region shows the mean and standard error of
evaluation results from 5 training runs using different random seeds. Each run is evaluated per train-
ing episode in an environment independent from training and reports average return of 5 episodes."
EXPERIMENTS,0.2913752913752914,"network parameter space. We reuse the default hyperparameters for these algorithms from the orig-
inal papers if not mentioned speciﬁcally. The detailed hyperparameters are listed in the Appendix
D.2. For our proposed methods, we include two variations DecentCEM-A and DecentCEM-P as
described in Sec. 4 where the sufﬁx carries the same meaning as in POPLIN-A/P. All MBRL algo-
rithms studied in this benchmark uses the same ensemble networks proposed by Chua et al. (2018)
for the dynamics model learning. We also include a Model-Free RL baseline SAC (Haarnoja et al.,
2018) and show its ﬁnite-time and asymptotic result."
EXPERIMENTS,0.2937062937062937,"Evaluation Protocol The learning curve shows the mean and standard error of the test performance
out of 5 independent training runs. The test performance is an average return of 5 episodes of
the evaluation environment, evaluated at every training episode. At the beginning of each training
run, the evaluation environment is initialized with a ﬁxed random seed such that the evaluation
environments are consistent across different methods and multiple runs to make it a fair comparison.
All experiments were conducted using Tesla V100-PCIE-16GB GPUs."
EXPERIMENTS,0.29603729603729606,"Results
The learning curves of the algorithms are shown in Fig. 6 for InvertedPendulum, Ac-
robot, Reacher, Hopper, Walker2D and Ant, sorted by the difﬁculty of task. The full results for all
environments are included in Appendix E."
EXPERIMENTS,0.29836829836829837,"We can observe two main patterns from the results. One pattern was that in most environments,
the DecentCEM methods either matched or outperformed their counterpart that took a centralized
approach. In fact, DecentCEM can be seen as a generalization of POPLIN by adding a dimension of
policy ensemble size, with size one recovering POPLIN. We also included negative results shown in
Hopper where neither DecentCEM modes outperformed the baselines. It can be interpreted that set-
ting the policy ensemble size to one is better than 5 in this environment. By varying this additional
parameter, CEM can be ﬁne-tuned for individual domains. Also all model-based methods underper-
formed the model-free method SAC, suggesting the difﬁculty of model learning. The other pattern
was that using policy networks to learn the initial sampling distribution in general helped improv-
ing the performance of CEM with both centralized and decentralized formulation. This is expected
as discussed in Sec.4 since the policy network allows the sampling distribution to “resume” from
high-value region seen before and to generalize to similar states."
EXPERIMENTS,0.3006993006993007,"Ablation Study
A natural question to ask about the DecentCEM-A/P methods is whether the in-
creased performance is from the larger number of neural network parameters. We added two vara-"
EXPERIMENTS,0.30303030303030304,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.30536130536130535,"0
2000
4000
6000
8000
10000
Steps 1400 1200 1000 800 600 400 200"
EXPERIMENTS,0.3076923076923077,Average Return
EXPERIMENTS,0.31002331002331,Pendulum
EXPERIMENTS,0.3123543123543124,"0
2000
4000
6000
8000
10000
Steps 45 40 35 30 25 20 15 10"
REACHER,0.3146853146853147,"5
Reacher"
REACHER,0.317016317016317,"0
2000
4000
6000
8000
10000
12000
14000
Steps 250 225 200 175 150 125 100 75"
PETS-PUSHER,0.31934731934731936,"50
PETS-Pusher"
PETS-PUSHER,0.32167832167832167,"POPLIN-A
POPLIN-A-BiggerPolicyNet
DecentCEM-A
POPLIN-P
POPLIN-P-BiggerPolicyNet
DecentCEM-P"
PETS-PUSHER,0.32400932400932403,"Figure 7: Ablation study on the policy network size where POPLIN-A&P have a bigger policy
network equivalent in the total number of neural network weights to their DecentCEM counterparts.
For better visual clarity, curves are smoothed with a sliding window of size 10."
PETS-PUSHER,0.32634032634032634,"tions of the POPLIN baselines where a bigger policy network was used. The number of the network
parameters was equivalent to that of the ensemble of policy networks in DecentCEM-A/P. We show
the comparison using three environments in Fig. 7: Pendulum(1), Reacher(2) and PETS-Pusher(7)
(action dimension in parenthesis). In both action space and parameter space planning, a bigger
policy network in POPLIN either did not help or signiﬁcantly impaired the performance (see the
POPLIN-P results in reacher and PETS-Pusher). This is expected since unlike DecentCEM, the
training data in POPLIN do not scale with the size of the policy network, as explained in Sec. 4."
PETS-PUSHER,0.32867132867132864,"0
2000
4000
6000
8000
10000
Steps 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
PETS-PUSHER,0.331002331002331,Selection Ratio
PETS-PUSHER,0.3333333333333333,Pendulum
PETS-PUSHER,0.3356643356643357,"CEM Instance 1
CEM Instance 2
CEM Instance 3
CEM Instance 4
CEM Instance 5"
PETS-PUSHER,0.337995337995338,"9300
9400
9500
9600
9700
9800
Steps 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00"
PETS-PUSHER,0.34032634032634035,Action
PETS-PUSHER,0.34265734265734266,Pendulum
PETS-PUSHER,0.34498834498834496,"Figure 8: Ablation of ensemble diversity. Left:
Cumulative selection ratio of each CEM instance.
Right: Action statistics of the instances."
PETS-PUSHER,0.3473193473193473,"Figure 8 (Left) shows the cumulative selection
ratio of each CEM instance during training of
DecentCEM-A with an ensemble size of 5. It
suggests that the random initialization of the
policy network is sufﬁcient to avoid mode col-
lapse. We also plot the action statistics of the
instances in Figure 8 (Right).
The line and
shaded area represent the mean and max/min
action of the instances, respectively. For visual
clarity, we show a time segment toward the end
of the training rather than all the 10k steps. De-
centCEM has maintained enough diversity in
the instances even toward the end of the train-
ing. DecentCEM-P is excluded from both plots
since it shows a similar trend as DecentCEM-A. More ablations results are included in Appendix F."
CONCLUSION AND FUTURE WORK,0.34965034965034963,"7
CONCLUSION AND FUTURE WORK"
CONCLUSION AND FUTURE WORK,0.351981351981352,"In this paper, we study CEM planning in the context of continuous-action MBRL. We propose a
novel decentralized formulation of CEM named DecentCEM, which generalizes CEM to run multi-
ple independent instances and recovers the conventional CEM when the number of instances is one.
We illustrate the intuition and the strengths of the proposed DecentCEM approach in a motivational
one-dimensional optimization task and show how it fundamentally differs from the CEM approach
that uses a Gaussian or GMM. We also show that DecentCEM has almost sure convergence to a lo-
cal optimum. We extend the proposed approach to MBRL by instantiating two decentralized CEM
methods that combine with policy networks. We show the efﬁcacy of the proposed methods in
benchmark control tasks and ablations studies."
CONCLUSION AND FUTURE WORK,0.3543123543123543,"There is a gap between the convergence result and practice that the theory assumes that the number
of samples grow polynomially with the iterations whereas a constant sample size is commonly used
in practice including our work. Investigating the convergence properties of CEM under a constant
sample size makes an interesting direction for future work. Another interesting direction to pursue
is ﬁnite-time analysis of both CEM and DecentCEM."
CONCLUSION AND FUTURE WORK,0.35664335664335667,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY STATEMENT,0.358974358974359,REPRODUCIBILITY STATEMENT
REPRODUCIBILITY STATEMENT,0.3613053613053613,"We have included the implementation details in Appendix D and the source code in the supplemen-
tary materials."
REFERENCES,0.36363636363636365,REFERENCES
REFERENCES,0.36596736596736595,"Brandon Amos and Denis Yarats. The differentiable cross-entropy method. In International Con-
ference on Machine Learning, pp. 291–302. PMLR, 2020."
REFERENCES,0.3682983682983683,"Dimitri P Bertsekas. Dynamic programming and optimal control 3rd edition, volume i. Belmont,
MA: Athena Scientiﬁc, 2005."
REFERENCES,0.3706293706293706,"Homanga Bharadhwaj, Kevin Xie, and Florian Shkurti. Model-predictive control via cross-entropy
and gradient-based optimization. In Learning for Dynamics and Control, pp. 277–286. PMLR,
2020."
REFERENCES,0.372960372960373,"Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine.
Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, NIPS’18, pp. 4759–4770,
2018."
REFERENCES,0.3752913752913753,"Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the
cross-entropy method. Annals of operations research, 134(1):19–67, 2005."
REFERENCES,0.3776223776223776,"Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual fore-
sight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint
arXiv:1812.00568, 2018."
REFERENCES,0.37995337995337997,"Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE
International Conference on Robotics and Automation (ICRA), pp. 2786–2793. IEEE, 2017."
REFERENCES,0.3822843822843823,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861–1870. PMLR, 2018."
REFERENCES,0.38461538461538464,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555–2565. PMLR, 2019."
REFERENCES,0.38694638694638694,"Kourosh Hakhamaneshi, Keertana Settaluri, Pieter Abbeel, and Vladimir Stojanovic. Gacem: Gen-
eralized autoregressive cross entropy method for multi-modal black box constraint satisfaction.
arXiv preprint arXiv:2002.07236, 2020."
REFERENCES,0.38927738927738925,"Morris W Hirsch, Hal L Smith, and Xiao-Qiang Zhao. Chain transitivity, attractivity, and strong
repellors for semidynamical systems. Journal of Dynamics and Differential Equations, 13(1):
107–131, 2001."
REFERENCES,0.3916083916083916,"Jiaqiao Hu, Ping Hu, and Hyeong Soo Chang. A stochastic approximation framework for a class of
randomized optimization algorithms. IEEE Transactions on Automatic Control, 57(1):165–178,
2011."
REFERENCES,0.3939393939393939,"Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago Miret,
Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 3341–3350. PMLR, 2019."
REFERENCES,0.3962703962703963,"Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, and In So Kweon. An efﬁcient asynchronous
method for integrating evolutionary and gradient-based policy search. Advances in Neural Infor-
mation Processing Systems, 33, 2020."
REFERENCES,0.3986013986013986,"Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch. Plan
online, learn ofﬂine: Efﬁcient learning and exploration via model-based control. In International
Conference on Learning Representations, ICLR, 2019."
REFERENCES,0.40093240093240096,Under review as a conference paper at ICLR 2022
REFERENCES,0.40326340326340326,"Sergio Valcarcel Macua, Santiago Zazo, and Javier Zazo. Distributed black-box optimization of
nonconvex functions. In 2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 3591–3595. IEEE, 2015."
REFERENCES,0.40559440559440557,"Shie Mannor, Reuven Y Rubinstein, and Yohai Gat. The cross entropy method for fast policy search.
In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 512–
519, 2003."
REFERENCES,0.40792540792540793,"Masashi Okada and Tadahiro Taniguchi. Variational inference mpc for bayesian model-based rein-
forcement learning. In Conference on Robot Learning, pp. 258–272. PMLR, 2020."
REFERENCES,0.41025641025641024,"Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal
Rolinek, and Georg Martius. Sample-efﬁcient cross-entropy method for real-time planning. arXiv
preprint arXiv:2008.06389, 2020."
REFERENCES,0.4125874125874126,"Alo¨ıs Pourchot and Olivier Sigaud. CEM-RL: combining evolutionary and gradient-based methods
for policy search. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019."
REFERENCES,0.4149184149184149,"Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. In 8th In-
ternational Conference on Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30,
2020."
REFERENCES,0.4172494172494173,"Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforce-
ment learning. CoRR, abs/1907.02057, 2019."
REFERENCES,0.4195804195804196,"Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, and Vikas Sindhwani. Data
efﬁcient reinforcement learning for legged robots. In Conference on Robot Learning, pp. 1–10.
PMLR, 2020."
REFERENCES,0.4219114219114219,"Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr´e Biedenkapp, Kurtland Chua,
Frank Hutter, and Roberto Calandra.
On the importance of hyperparameter optimization for
model-based reinforcement learning. In International Conference on Artiﬁcial Intelligence and
Statistics, pp. 4015–4023. PMLR, 2021."
REFERENCES,0.42424242424242425,Under review as a conference paper at ICLR 2022
REFERENCES,0.42657342657342656,"A
DETAILS OF THE MOTIVATIONAL EXAMPLE"
REFERENCES,0.4289044289044289,"A.1
SETUP AND RUNNING TIME"
REFERENCES,0.43123543123543123,"For a fair comparison of the three methods CEM, CEM-GMM and DecentCEM, we performed a
hyperparameter search for all. The list of hyperparmeters are summarized in Table A.1 and the best
performing hyperparameters for each method under each population size are shown in Table A.2.
These hyperparameters were what the data in Fig. 3 were based on. Note that the top percentage of
samples “Elite Ratio” (in Table A.1) was used in the implementation instead of top-k but they are
equivalent. The running time are included in Table A.3."
REFERENCES,0.43356643356643354,Table A.1: Hyperparameters
REFERENCES,0.4358974358974359,"Algorithm
Parameter
Value"
REFERENCES,0.4382284382284382,"Shared
Total Population Size
100, 200, 500, 1000
Parameters
Elite Ratio
0.1
α: Smoothing Ratio
0.1
ϵ: Minimum Variance Threshold
1e-3
Maximum Number of Iterations
100"
REFERENCES,0.4405594405594406,"CEM-GMM
M: Number of Mixture Components
3,5,8,10
κ: Weights of Entropy Regularizer
0.25, 0.5
r: Return Mode
‘s’: mean of the mixture component
sampled based on their weights
‘m’: mean of the component that
achieves the minimum cost"
REFERENCES,0.4428904428904429,"DecentCEM
E: Number of Instances in the Ensemble
3,5,8,10"
REFERENCES,0.44522144522144524,Table A.2: Best Hyper-Parameter
REFERENCES,0.44755244755244755,"Total Population Size
100
200
500
1000"
REFERENCES,0.44988344988344986,"CEM-GMM
M = 10
M = 8
M = 8
M = 8
κ = 0.25,
κ = 0.5
κ = 0.25
κ = 0.5
r =‘m’
r =‘m’
r =‘m’
r =‘s’"
REFERENCES,0.4522144522144522,"DecentCEM
E = 10
E = 10
E = 10
E = 8"
REFERENCES,0.45454545454545453,Table A.3: Total Time of 10 Runs (in seconds)
REFERENCES,0.4568764568764569,"Total Population Size
100
200
500
1000"
REFERENCES,0.4592074592074592,"CEM
0.079
0.093
0.165
0.318
CEM-GMM
7.322
11.500
24.431
59.844
DecentCEM
0.407
0.420
0.506
0.545"
REFERENCES,0.46153846153846156,"A.2
OUTPUT OF CEM APPROACHES"
REFERENCES,0.46386946386946387,"In terms of the output of CEM approaches, there exist different options in the literature. The most
common option is to return the sample in the domain that corresponds to the highest probability
density in the ﬁnal sampling distribution. It is the mean in the case of Gaussian and the mode
with the highest probability density in the case of GMM. One can also draw a sample from the
ﬁnal sampling distribution (Okada & Taniguchi, 2020) and return it. Another option is to return the
best sample observed (Pinneri et al., 2020). The best option among the three may be application
dependent. It has been observed that in many applications, the sequence of sampling distributions
numerically converges to a deterministic one (De Boer et al., 2005), in which case the ﬁrst two
options are identical."
REFERENCES,0.4662004662004662,Under review as a conference paper at ICLR 2022
REFERENCES,0.46853146853146854,"B
BENCHMARK ENVIRONMENT DETAILS"
REFERENCES,0.47086247086247085,"In this section, we go over the details of the benchmark environments used in the experiments."
REFERENCES,0.4731934731934732,"Table B.1 lists the environments along with their properties, including the dimensionality of the
observation and action spaces and the maximum episode length. Table B.2 provides a list of the
reward function for each environment. Whenever possible, we reuse the original implementations
from the literature as noted in Table B.1 so as to avoid confusion. The environments that start with
“PETS” are from the PETS paper (Chua et al., 2018), which is one of the baseline methods. Most
other environments are from Wang et al. (2019) where the dynamics are the same as the OpenAI
gym version and the reward function in Table B.1 is exposed to the agent. For more details of the
environments, the readers are referred to the original paper."
REFERENCES,0.4755244755244755,"Note that FixedSwimmer is a modifed version of the original Gym Swimmer environment where
the velocity sensor on the neck is moved to the head. This ﬁx was originally proposed by Wang &
Ba (2020). For the Pendulum environment, we use the OpenAI Gym version. The modiﬁed version
in (Wang et al., 2019) uses a different reward function which we have found to be incorrect."
REFERENCES,0.47785547785547783,"Table B.1: The setup of the environments. The number in the bracket in the “Environment Name”
column denotes the source of this environment: [1] refers to the benchmark paper from Wang et al.
(2019); [2] denotes PETS (Chua et al., 2018)."
REFERENCES,0.4801864801864802,"Environment Name
Observation Dim
Action Dim
Maximum Episode Length"
REFERENCES,0.4825174825174825,"Pendulum
3
1
200
InvertedPendulum [1]
4
1
100
Cartpole [1]
4
1
200
Acrobot [1]
6
1
200
FixedSwimmer [1]
9
2
1000
Reacher [1]
11
2
50
Hopper [1]
11
3
1000
Walker2d [1]
17
6
1000
HalfCheetah [1]
17
6
1000
PETS-Reacher3D [2]
17
7
150
PETS-HalfCheetah [2]
18
6
1000
PETS-Pusher [2]
20
7
150
Ant [1]
27
8
1000"
REFERENCES,0.48484848484848486,"Table B.2: Reward Functions. dt denotes the vector between the end effector to the target position.
zt denotes the height of the robot. ∥v∥1 and ∥v∥2 denote the 1-norm and 2-norm of vector v,
respectively. In PETS-Pusher, d1,t is the vector between the object position and the goal and d2,t
denotes the vector between the object position and the end effector."
REFERENCES,0.48717948717948717,"Environment Name
Reward Function"
REFERENCES,0.48951048951048953,"Pendulum
θ2
t + 0.1 ˙θt
2 + 0.001a2
t
InvertedPendulum
−θ2
t
Cartpole
cosθt −0.01x2
t
Acrobot
−cosθ1,t −cos(θ1,t + θ2,t)
FixedSwimmer
˙xt −0.0001∥at∥2
2
Reacher
−∥dt∥−∥at∥2
2
Hopper
˙xt −0.1∥at∥2
2 −3(zt −1.3)2"
REFERENCES,0.49184149184149184,"Walker2d
˙xt −0.1∥at∥2
2 −3(zt −1.3)2"
REFERENCES,0.49417249417249415,"HalfCheetah
˙xt −0.1∥at∥2
2
PETS-Reacher3D
−∥dt∥2
2 −0.01∥at∥2
2
PETS-HalfCheetah
˙xt −0.1∥at∥2
2
PETS-Pusher
−1.25∥d1,t∥1 −0.5∥d2,t∥1 −0.1∥at∥2
2
Ant
˙xt −0.1∥at∥2
2 −3(zt −0.57)2"
REFERENCES,0.4965034965034965,Under review as a conference paper at ICLR 2022
REFERENCES,0.4988344988344988,"C
ALGORITHMS"
REFERENCES,0.5011655011655012,"In this section, we give the pseudo-code of the proposed algorithms DecentCEM-A and
DecentCEM-P in Algorithm 1 and 2 respectively. We only show the training phase. The algo-
rithm at inference time is simply the same process without the data saving and network update. For
the internal process of CEM, we refer the readers to De Boer et al. (2005); Wang & Ba (2020)."
REFERENCES,0.5034965034965035,Algorithm 1: DecentCEM-A Training
REFERENCES,0.5058275058275058,"1 Initialize the policy networks pi with θi, i = 1, 2, · · · , M where M is the ensemble size. Planning
horizon H. Initialize the dynamics network fω parameterized by ω. Empty Datasets Dm and Dp
// Episode 1, warmup phase"
REFERENCES,0.5081585081585082,"2 Rollout using a random policy, ﬁll the dataset Dm with the transition data {(st, at, st+1)}"
REFERENCES,0.5104895104895105,"3 Update ω using Dm by Mean-Squared Loss
// Train the dynamics network with Dm
// Episode 2 onwards"
REPEAT,0.5128205128205128,4 repeat
REPEAT,0.5151515151515151,"5
t = 0, Dp = {}
// Each episode, reset time and dataset"
REPEAT,0.5174825174825175,"6
repeat"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5198135198135199,"7
foreach policy network pi in the ensemble do"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5221445221445221,"8
generate reference mean of action sequence distribution µi using pi and the model fω.
// Apply CEM to reﬁne the action distribution.
// ˆµi, vi are the mean action sequence of the reﬁned distribution and its expected value"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5244755244755245,"9
ˆµi, vi = CEM(µi)"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5268065268065268,"10
ˆat,i = ˆµi[0]"
END,0.5291375291375291,"11
end"
END,0.5314685314685315,"12
at = arg maxˆat,i vi
// Pick best distribution"
END,0.5337995337995338,"13
st+1 = step(at)
// Execute the ﬁrst action in the mean sequence"
END,0.5361305361305362,"14
Append the transition (st, at, st+1) to Dm"
END,0.5384615384615384,"15
Append the data {(st, ˆat,i)}M
i=1 to Dp
16
t = t + 1
// Update time step"
UNTIL EITHER REACHED THE MAXIMUM EPISODE LENGTH OR TERMINAL STATE,0.5407925407925408,"17
until Either reached the maximum episode length or terminal state"
UNTIL EITHER REACHED THE MAXIMUM EPISODE LENGTH OR TERMINAL STATE,0.5431235431235432,"18
Update the model parameter ω using dataset Dm"
UNTIL EITHER REACHED THE MAXIMUM EPISODE LENGTH OR TERMINAL STATE,0.5454545454545454,"19
Update the policy network weights {θi}M
i=1 using dataset Dp by the behavior cloning objective"
UNTIL BORED,0.5477855477855478,20 until bored
UNTIL BORED,0.5501165501165501,Algorithm 2: DecentCEM-P Training
UNTIL BORED,0.5524475524475524,"1 Initialize the policy networks pi with θi, i = 1, 2, · · · , M where M is the ensemble size. Planning
horizon H. Initialize the dynamics network fω parameterized by ω. Empty Datasets Dm and Dp
// Episode 1, warmup phase"
UNTIL BORED,0.5547785547785548,"2 Rollout using a random policy, ﬁll the dataset Dm with the transition data {(st, at, st+1)}"
UNTIL BORED,0.5571095571095571,"3 Update ω using Dm by Mean-Squared Loss
// Train the dynamics network with Dm
// Episode 2 onwards"
REPEAT,0.5594405594405595,4 repeat
REPEAT,0.5617715617715617,"5
t = 0, Dp = {}
// Each episode, reset time and dataset"
REPEAT,0.5641025641025641,"6
repeat"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5664335664335665,"7
foreach policy network pi in the ensemble do"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5687645687645687,"// Apply CEM to reﬁne the distribution of the neural network weight.
// ˆµi, vi are the mean of the reﬁned weight distribution sequence and its expected value"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5710955710955711,"8
ˆµi, vi = CEM(θi)"
FOREACH POLICY NETWORK PI IN THE ENSEMBLE DO,0.5734265734265734,"9
δi = ˆµi[0]
// Keep the weight at the ﬁrst step and discard the rest"
END,0.5757575757575758,"10
end"
END,0.578088578088578,"11
θt = arg maxθi+δi vi
// Pick the best distribution of weight sequence"
END,0.5804195804195804,"12
at = pθt(st)"
END,0.5827505827505828,"13
st+1 = step(at)
// Execute the action returned by the policy network pθt
14
Append the transition (st, at, st+1) to Dm"
END,0.585081585081585,"15
Append the data {δi}M
i=1 to Dp"
END,0.5874125874125874,"16
t = t + 1
// Update time step"
UNTIL EITHER REACHED THE MAXIMUM EPISODE LENGTH OR TERMINAL STATE,0.5897435897435898,"17
until either reached the maximum episode length or terminal state"
UNTIL EITHER REACHED THE MAXIMUM EPISODE LENGTH OR TERMINAL STATE,0.5920745920745921,"18
Update the model parameter ω using dataset Dm"
UNTIL EITHER REACHED THE MAXIMUM EPISODE LENGTH OR TERMINAL STATE,0.5944055944055944,"19
Update the policy network weights {θi}M
i=1 using dataset Dp by the AVG training objective"
UNTIL BORED,0.5967365967365967,20 until bored
UNTIL BORED,0.5990675990675991,Under review as a conference paper at ICLR 2022
UNTIL BORED,0.6013986013986014,"D
IMPLEMENTATION DETAILS"
UNTIL BORED,0.6037296037296037,"D.1
REPRODUCIBILITY"
UNTIL BORED,0.6060606060606061,"Our implementation is fully reproducible by identifying the sources of randomness and controlling
the random seeds as summarized in Table D.1. The seeds are set once at the beginning of the
experiments."
UNTIL BORED,0.6083916083916084,"Table D.1: Random Seed. The set {1,2,3,4,5} refers to the seeds for ﬁve runs. Note that we control
the random seed for the environments since there is a random number generator in openai gym
environments independent from other sources"
UNTIL BORED,0.6107226107226107,"Source of randomness
Random Seed"
UNTIL BORED,0.6130536130536131,"deep learning framework (tensorﬂow in our case)
{1,2,3,4,5}
numpy
python random module
the training environment
1234
the evaluation environment
0"
UNTIL BORED,0.6153846153846154,"D.2
HYPERPARAMETERS"
UNTIL BORED,0.6177156177156177,"This section includes the details of the key hyperparameters used in the proposed DecentCEM al-
gorithm (Table D.5) and the baseline algorithms PETS (Table D.3), POPLIN (Table D.4) and SAC6
(Table D.2). For the neural network architecture for the dynamics model, the DecentCEM methods
exactly follow the original one in PETS and POPLIN for a fair comparison, which is an ensemble
of fully connected networks."
UNTIL BORED,0.62004662004662,Table D.2: Hyperparameters of SAC
UNTIL BORED,0.6223776223776224,"Parameter
Value"
UNTIL BORED,0.6247086247086248,"Actor learning rate
0.0001
Critic learning rate
0.0001
Actor network architecture
[dim(observation), 64, 64, 2× dim(action)]
Critic network architecture
[dim(observation)+dim(action), 64, 64, 1]"
UNTIL BORED,0.627039627039627,Table D.3: Hyperparameters of PETS
UNTIL BORED,0.6293706293706294,"Parameter
Value"
UNTIL BORED,0.6317016317016317,"Model learning rate
0.001
Warmup episodes
1
Planning Horizon
30
CEM population size
500 (except in PETS-reacher3D: 400)
CEM proportion of elites
10%
CEM initial distribution variance
0.25
CEM max number of internal iterations
5"
UNTIL BORED,0.634032634032634,"6Our SAC implementation used network architectures that are similar to the policy network in our method.
The results of our implementation either matches or surpasses the ones reported in PETS, POPLIN and the
benchmark by Wang et al. (2019)"
UNTIL BORED,0.6363636363636364,Under review as a conference paper at ICLR 2022
UNTIL BORED,0.6386946386946387,Table D.4: Hyperparameters of POPLINA and POPLINP
UNTIL BORED,0.6410256410256411,"Parameter
Value"
UNTIL BORED,0.6433566433566433,"Model learning rate
0.001
Warmup episodes
1
Planning Horizon
30
CEM population size
500 (except in PETS-reacher3D: 400)
CEM proportion of elites
10%
CEM initial distribution variance
0.25
CEM max number of internal iterations
5
Policy network architecture (A)
[dim(observation), 64, 64, dim(action)]
Policy network architecture (P)
[dim(observation), 32, dim(action)]
Policy network learning rate
0.001
Policy network activation function
tanh"
UNTIL BORED,0.6456876456876457,Table D.5: Hyperparameters of the proposed DecentCEM-A/P
UNTIL BORED,0.6480186480186481,"Parameter
Value"
UNTIL BORED,0.6503496503496503,"Model learning rate
0.001
Warmup episodes
1
Planning Horizon
30
Ensemble Size
5
CEM population size in each instance
100 (except in PETS-reacher3D: 80)
CEM proportion of elites
10%
CEM initial distribution variance
0.25
CEM max number of internal iterations
5
Policy network architecture (A)
[dim(observation), 64, 64, dim(action)]
Policy network architecture (P)
[dim(observation), 32, dim(action)]
Policy network learning rate
0.001
Policy network activation function
tanh"
UNTIL BORED,0.6526806526806527,Under review as a conference paper at ICLR 2022 2 1 0 1 2 f(x)
UNTIL BORED,0.655011655011655,CEM iter=0
UNTIL BORED,0.6573426573426573,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x p(x)"
UNTIL BORED,0.6596736596736597,CEM iter=2
UNTIL BORED,0.662004662004662,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
UNTIL BORED,0.6643356643356644,CEM iter=4
UNTIL BORED,0.6666666666666666,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
UNTIL BORED,0.668997668997669,CEM iter=6
UNTIL BORED,0.6713286713286714,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x 2 1 0 1 2 f(x)"
UNTIL BORED,0.6736596736596736,CEM-GMM iter=0
UNTIL BORED,0.675990675990676,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x p(x)"
UNTIL BORED,0.6783216783216783,CEM-GMM iter=2
UNTIL BORED,0.6806526806526807,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
UNTIL BORED,0.682983682983683,CEM-GMM iter=4
UNTIL BORED,0.6853146853146853,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
UNTIL BORED,0.6876456876456877,CEM-GMM iter=6
UNTIL BORED,0.6899766899766899,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x 2 1 0 1 2 f(x)"
UNTIL BORED,0.6923076923076923,DecentCEM iter=0
UNTIL BORED,0.6946386946386947,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x p(x)"
UNTIL BORED,0.696969696969697,DecentCEM iter=2
UNTIL BORED,0.6993006993006993,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
UNTIL BORED,0.7016317016317016,DecentCEM iter=4
UNTIL BORED,0.703962703962704,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
UNTIL BORED,0.7062937062937062,DecentCEM iter=6
UNTIL BORED,0.7086247086247086,"7.5
5.0
2.5
0.0
2.5
5.0
7.5
x"
UNTIL BORED,0.710955710955711,Figure 9: The iterative sampling process in the 1D optimization task.
UNTIL BORED,0.7132867132867133,"E
FULL RESULTS"
UNTIL BORED,0.7156177156177156,"E.1
DETAILED VISUALIZATION OF THE ITERATIVE UPDATES IN THE 1D OPTIMIZATION TASK"
UNTIL BORED,0.717948717948718,"Figure 9 is a version of Figure 4 with more iterations. It shows the iterative sampling process of
CEM methods in the 1D optimization task and how the sampling distribution evolve over time."
UNTIL BORED,0.7202797202797203,"E.2
FULL LEARNING CURVES"
UNTIL BORED,0.7226107226107226,"In Figure 10, we report the learning curves in all 13 environments that we listed in Appendix B."
UNTIL BORED,0.7249417249417249,"The algorithms used in the benchmark are: PETS, POPLIN-A, POPLIN-P and the two modes of the
proposed method DecentCEM-A and DecentCEM-P. We also included the interim and asymptotic
performance of a model-free algorithm SAC as a reference."
UNTIL BORED,0.7272727272727273,"The learning curves in some environments can be noisy. We apply smoothing with 1D uniform ﬁlter
to the results of the following environments for easier interpretation: Cartpole, FixedSwimmer,
Hopper, Walker2d, HalfCheetah, PETS-Reacher3D, PETS-Pusher. The window size of the ﬁlter
was 10 for all but Cartpole, where 30 was used due to its high noise level for all algorithms."
UNTIL BORED,0.7296037296037297,"Note that the performance of the baseline methods may be different from the results reported in their
original paper. Speciﬁcally, in the paper by Wang & Ba (2020), PETS, POPLIN-A and POPLIN-P
have been evaluated in a number of environments that we use for the benchmark. Our benchmark
results may not be consistent with theirs due to differences in the implementation and evaluation
protocol. For example, our results of PETS, POPLIN-A and POPLIN-P in the Acrobot environment"
UNTIL BORED,0.7319347319347319,Under review as a conference paper at ICLR 2022
UNTIL BORED,0.7342657342657343,"0
2000
4000
6000
8000
10000
Steps 1750 1500 1250 1000 750 500 250 0"
UNTIL BORED,0.7365967365967366,Average Return
UNTIL BORED,0.7389277389277389,Pendulum
UNTIL BORED,0.7412587412587412,"0
1000
2000
3000
4000
5000
Steps 200 150 100 50 0"
UNTIL BORED,0.7435897435897436,InvertedPendulum
UNTIL BORED,0.745920745920746,"0
10000
20000
30000
40000
50000
Steps 190 192 194 196 198 200"
CARTPOLE,0.7482517482517482,"202
Cartpole"
CARTPOLE,0.7505827505827506,"0
2000
4000
6000
8000
10000
Steps 400 300 200 100 0 50"
CARTPOLE,0.752913752913753,Average Return
CARTPOLE,0.7552447552447552,Acrobot
CARTPOLE,0.7575757575757576,"0
10000
20000
30000
40000
50000
Steps 0 50 100 150 200 250 300 350"
CARTPOLE,0.7599067599067599,FixedSwimmer
CARTPOLE,0.7622377622377622,"0
2000
4000
6000
8000
10000
Steps 60 50 40 30 20 10"
CARTPOLE,0.7645687645687645,Reacher
CARTPOLE,0.7668997668997669,"0
20000
40000
60000
80000
100000
Steps 3000 2000 1000 0 1000 2000"
CARTPOLE,0.7692307692307693,Average Return
CARTPOLE,0.7715617715617715,Hopper
CARTPOLE,0.7738927738927739,"0
20000
40000
60000
80000
100000
Steps 3000 2000 1000 0 1000"
CARTPOLE,0.7762237762237763,Walker2D
CARTPOLE,0.7785547785547785,"0
20000
40000
60000
80000
100000
Steps 2000 0 2000 4000 6000"
CARTPOLE,0.7808857808857809,HalfCheetah
CARTPOLE,0.7832167832167832,"0
1000
2000
3000
4000
5000
6000
7000
Steps 200 175 150 125 100 75 50 25 0"
CARTPOLE,0.7855477855477856,Average Return
CARTPOLE,0.7878787878787878,PETS-Reacher3D
CARTPOLE,0.7902097902097902,"0
10000
20000
30000
40000
50000
Steps 2000 0 2000 4000 6000 8000 10000 12000"
CARTPOLE,0.7925407925407926,PETS-HalfCheetah
CARTPOLE,0.7948717948717948,"0
2000
4000
6000
8000
10000
12000
14000
Steps 250 225 200 175 150 125 100 75"
PETS-PUSHER,0.7972027972027972,"50
PETS-Pusher"
PETS-PUSHER,0.7995337995337995,"0
20000
40000
60000
80000
100000
Steps 500 0 500 1000 1500 2000 2500"
PETS-PUSHER,0.8018648018648019,Average Return Ant
PETS-PUSHER,0.8041958041958042,"PETS
POPLIN-A
POPLIN-P
DecentCEM-A
DecentCEM-P
SAC
SAC at convergence"
PETS-PUSHER,0.8065268065268065,"Figure 10: The learning curves of the proposed DecentCEM methods and the baseline methods on
continuous control environments. The line and shaded region shows the mean and standard error
of evaluation results from 5 training runs using different random seeds. Each run is evaluated in an
environment independent from training and reports average return of 5 episodes at every training
episode. To ensure that the evaluation environments are the same across different methods and
multiple runs, we set a ﬁxed random seed in the evaluation environment of each task."
PETS-PUSHER,0.8088578088578089,Under review as a conference paper at ICLR 2022
PETS-PUSHER,0.8111888111888111,"are all better than the results in Wang & Ba (2020). We have identiﬁed a bug in the POPLIN code
base that causes the evaluation results to be on a wrong timescale that is much slower than what it
actually is. Hence the results of our implementation look far better, reaching a return of 0 at about
4k steps as opposed to 20k steps reported in Wang & Ba (2020)."
PETS-PUSHER,0.8135198135198135,"E.3
ANALYSIS"
PETS-PUSHER,0.8158508158508159,"Let’s group the environments into two categories based on how well the decentralized methods
perform in them:"
PETS-PUSHER,0.8181818181818182,"1. Pendulum, InvertedPendulum, Acrobot, Cartpole, Reacher, Walker2D, HalfCheetah,
PETS-Pusher, PETS-Reacher3D, Ant,"
PETS-PUSHER,0.8205128205128205,"2. FixedSwimmer, Hopper, PETS-HalfCheetah"
PETS-PUSHER,0.8228438228438228,"The ﬁrst category is where the best performing method is one of the proposed DecentCEM al-
gorithms: DecentCEM-A or DecentCEM-P. In environments where the baseline POPLIN-A or
POPLIN-P could reach near-optimal performance such as pendulum and invertedPendulum, ap-
plying the ensemble method would yield similar performance as before.
In Acrobot, Reacher,
Walker2D and PETS-Pusher, applying the decentralized approach increases the performance in both
action space planning (“A”) and parameter space planning (“P”). In Pendulum, InvertedPendulum,
Cartpole, HalfCheetah, PETS-Reacher3D and Ant, ensemble helps in the action space planning but
either has no impact or negative impact on the parameter space planning."
PETS-PUSHER,0.8251748251748252,"The second category is where it is better not to use a decentralized approach with multiple instances
(note that by using one instance in the decentralized methods, we can recover one of POPLIN-
A, POPLIN-P). In Hopper, the issue might lie in the MBRL approach in general since all MBRL
baselines performed worse than the model-free baseline SAC. One possible issue is that the true dy-
namics is difﬁcult to approximate with our model learning approach. Another possibility is that it’s
necessary to learn the variance of the sampling distribution, which none of these MBRL approaches
do. To be clear, the variance is adapted online by CEM but it is not learned. In FixedSwimmer,
the ensemble approach performs worse in both action space and parameter space planning. One
potential reason for the result is that the optimization landscape is complex and needs more samples
to estimate the expected return than what was used in each instance of the ensemble. In this case, it
is better not to distribute the population into several instances. An alternative is to increase the total
population size, with a downside of increasing computation. PETS-HalfCheetah is slightly different
in that the ensemble does improve the performance signiﬁcantly when used for action space plan-
ning. However, POPLIN-P performs signiﬁcantly better than all other algorithms. This suggests
that the parameter space planning has been able to successfully ﬁnd a high return region using a
single Gaussian distribution. In this case, distributing the population size would not be able to trade
the estimation accuracy for better global search."
PETS-PUSHER,0.8275058275058275,"F
MORE ABLATION"
PETS-PUSHER,0.8298368298368298,"9300
9400
9500
9600
9700
9800
Steps 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4"
PETS-PUSHER,0.8321678321678322,Pairwise Distances of Actions
PETS-PUSHER,0.8344988344988346,Pendulum
PETS-PUSHER,0.8368298368298368,(a) Action Distance
PETS-PUSHER,0.8391608391608392,"0
2000
4000
6000
8000
10000
Steps 1000 800 600 400 200"
PETS-PUSHER,0.8414918414918415,Average Return
PETS-PUSHER,0.8438228438228438,Pendulum
PETS-PUSHER,0.8461538461538461,"POPLIN-A
DecentCEM-A-E2
DecentCEM-A-E3
DecentCEM-A-E4
DecentCEM-A-E5"
PETS-PUSHER,0.8484848484848485,(b) Ensemble Size
PETS-PUSHER,0.8508158508158508,"0
2000
4000
6000
8000
10000
Steps 1600 1400 1200 1000 800 600 400 200"
PETS-PUSHER,0.8531468531468531,Average Return
PETS-PUSHER,0.8554778554778555,Pendulum
PETS-PUSHER,0.8578088578088578,"POPLIN-A-PC
POPLIN-P-PC
DecentCEM-A-PC
DecentCEM-P-PC"
PETS-PUSHER,0.8601398601398601,(c) Policy Control
PETS-PUSHER,0.8624708624708625,"Figure 11: More Ablation (a) Pairwise distance between the actions of 5 instances in DecentCEM-
A during training (b) Ensemble size ablation: E2 denotes an ensemble size of 2 (c) Policy control
performance where the policy network is directly used for control without CEM policy improvement"
PETS-PUSHER,0.8648018648018648,Under review as a conference paper at ICLR 2022
PETS-PUSHER,0.8671328671328671,"Figure 11 (a) is an additional plot for the ensemble diversity ablation. It shows the statistics of the
pairwise distance between the output actions by the CEM instances at each time step. The same as
in Fig. 8 (b), we only show a time window toward the end of training for visual clarity and the line
and shaded region represent the mean and min/max distances."
PETS-PUSHER,0.8694638694638694,"Figure 11 (b) shows the performance of the DecentCEM-A algorithm under different ensemble size
(where we keep the population size of each instance the same). Adjusting the ensemble size has
an impact on the performance of DecentCEM-A. An ensemble size of 3 or 4 could improve the
result reported in Fig. 10 to reach a near-optimal performance. We did not show the results for the
parameter space planning mode “P” since all variations achieve near-optimal performance and their
curves overlap. We ﬁxed an ensemble size of 5 throughout our benchmark but tuning the ensemble
size for each environment could improve the performance further."
PETS-PUSHER,0.8717948717948718,"We also study the performance of policy control where the policy network is directly used for control
without the CEM step, denoted by the extra sufﬁx “-PC”. The result is shown in Fig.11 (c). Without
the policy improvement from CEM, all algorithms perform worse than their counter-part of using
CEM. POPLIN-P-PC and DecentCEM-P-PC both get stuck in local optima and do not perform
very well. This makes sense since the premise of planning in parameter space is that CEM can
search more efﬁciently there. The policy network is not designed to be used directly as a policy.
Interestingly, DecentCEM-A-PC achieves a high performance from about 6k steps (30 episodes) of
training. The ensemble of policy networks adds more robustness to control than using a single one."
PETS-PUSHER,0.8741258741258742,"G
OVERHEAD OF THE ENSEMBLE"
PETS-PUSHER,0.8764568764568764,"The sample efﬁciency is not impaired when going from one policy network to the multiple policy
networks used in DecentCEM-A and DecentCEM-P. This is because that the generation of the train-
ing data only involve taking imaginary rollouts with the model, rather than interacting with the real
environment, as discussed in Section 4."
PETS-PUSHER,0.8787878787878788,"In terms of the population size (number of samples drawn in CEM), the DecentCEM based methods
do not impose additional cost. We show in both the motivational example (Sec. 3) and the benchmark
experiments (Appendix E) that the proposed methods work better than CEM under the same total
population size."
PETS-PUSHER,0.8811188811188811,"The additional computational cost is reasonable in DecentCEM compared to POPLIN. Each branch
of policy network and CEM instance runs independently from the others, allowing for a parallel
implementation. The instances have to be synchronized (arg max) but its additional cost is minimal.
One caveat with our current implementation though is that it is serial, which slows down the speed.
This is a limitation that calls for future work of a parallel implementation."
PETS-PUSHER,0.8834498834498834,"H
CONVERGENCE ANALYSIS OF DECENTRALIZED CEM"
PETS-PUSHER,0.8857808857808858,This section analyzes the convergence of the proposed DecentCEM algorithm in optimization.
PETS-PUSHER,0.8881118881118881,Consider the following optimization problem:
PETS-PUSHER,0.8904428904428905,"x∗∈arg max
x∈X
V (x)
(4)"
PETS-PUSHER,0.8927738927738927,"where X ⊂Rn is a non-empty compact set and V (·) is a bounded, deterministic value function to be
maximized. We assume that this problem has a unique global optimal solution x∗but the objective
function V (·) may have multiple local optimum and may not be continuous."
PETS-PUSHER,0.8951048951048951,"We will show that the existing convergence result of CEM in continuous optimization established
in (Hu et al., 2011) also applies to DecentCEM. It assumes that the sampling distribution gφ(x)
in CEM is in the natural exponential families (NEFs) which subsumes Gaussian distribution (with
known covariance). We restate the deﬁnition of NEFs for completeness (deﬁnition 2.1 from (Hu
et al., 2011)):
Deﬁnition H.1 (Natural Exponential Family). A family of parameterized distributions {gφ(·), φ ∈
Φ ⊂Rd} on X ⊂Rn is called a Natural Exponential Family (NEF) if there exists continuous map-
pings Γ : Rn →Rd, h : Rn →R and K : Rd →R such that gφ(x) = exp(φ⊤Γ(x) −K(φ))h(x),"
PETS-PUSHER,0.8974358974358975,Under review as a conference paper at ICLR 2022
PETS-PUSHER,0.8997668997668997,"where the parameter space Φ = {φ ∈Rd : |K(φ)| < ∞}, K(φ) = ln
R"
PETS-PUSHER,0.9020979020979021,"X exp(φ⊤Γ(x))h(x)ν(dx)
and ν is the Lebesgue measure of X."
PETS-PUSHER,0.9044289044289044,"The mean vector function is denoted as m(φ) = Eφ[Γ(x)] where the expectation Eφ is with respect
to gφ and it can be shown that m(·) is invertible. Note that the expression of the densities can be
simpliﬁed when restricted to a multivariate Gaussian distribution (with known diagonal covariance)
where the natural sufﬁcient statistics Γ(x) is the identify function."
PETS-PUSHER,0.9067599067599068,"We then present the CEM algorithm below to ﬁx notations. It follows Algorithm 2 in (Hu et al.,
2011) but is modiﬁed to align with some notations introduced in previous sections in our paper."
PETS-PUSHER,0.9090909090909091,Algorithm 3: CEM
PETS-PUSHER,0.9114219114219114,"1 Choose the family of distributions gφ(x), x ∈X from NEFs deﬁned in H.1 and the initial parameter
φ0 ∈int(Φ) where int denotes the interior of the parameter space Φ."
PETS-PUSHER,0.9137529137529138,"2 Specify elite ratio ρ ∈(0, 1) and step size sequence {αk} and {λk} where k denotes the time step. Set
k = 0. Specify ϵ > 0 which is the parameter in the thresholding function deﬁned in Equation 5. 3"
PETS-PUSHER,0.916083916083916,"1(x, γ) = 

 
"
PETS-PUSHER,0.9184149184149184,"1,
if x ≥γ
x−γ+ϵ"
PETS-PUSHER,0.9207459207459208,"ϵ
,
if γ −ϵ < x < 1
0,
if x ≤γ −ϵ
(5)"
REPEAT,0.9230769230769231,4 repeat
REPEAT,0.9254079254079254,"5
Step 1: Draw Nk i.i.d samples Λk = {x1, x2, ..., xNk} from the distribution gφk(x)"
REPEAT,0.9277389277389277,"6
Step 2: Calculate the sample (1 −ρ)-quantile ˆγk = V(⌈(1−ρ)Nk⌉) where ⌈a⌉is the ceiling function
that gives the smallest integer greater than a and V(i) is the ith-order statistics of the sequence
{V (xj)}Nk
j=1 where V (·) is the objective function to be maximized."
REPEAT,0.9300699300699301,"7
Step 3: Compute the new parameter φk+1 = m−1(ηk+1), where η0 = m(φ0) = Eφ0(Γ(x)) and"
REPEAT,0.9324009324009324,ηk+1 = αk P
REPEAT,0.9347319347319347,"x∈Λk 1(V (x), ˆγk)Γ(x)
P"
REPEAT,0.9370629370629371,"x∈Λk 1(V (x), ˆγk)
+ (1 −αk)  λk Nk X"
REPEAT,0.9393939393939394,"x∈Λk
Γ(x) + (1 −λk)ηk  
(6)"
REPEAT,0.9417249417249417,"8
Step 4: k = k + 1"
UNTIL A STOPPING CONDITION IS REACHED,0.9440559440559441,9 until a stopping condition is reached
UNTIL A STOPPING CONDITION IS REACHED,0.9463869463869464,10 return φk
UNTIL A STOPPING CONDITION IS REACHED,0.9487179487179487,"The convergence results will require the following assumptions from (Hu et al., 2011):"
UNTIL A STOPPING CONDITION IS REACHED,0.951048951048951,"Assumption 1. The parameter φk+1 computed at step 3 of Algorithm 3 satisﬁes φk+1 ∈int(Φ) for
all k."
UNTIL A STOPPING CONDITION IS REACHED,0.9533799533799534,"Assumption 2. The step size sequence {αk} satisﬁes: αk > 0 ∀k , limk→∞αk = 0 and
P∞
k=0 αk = ∞."
UNTIL A STOPPING CONDITION IS REACHED,0.9557109557109557,"Assumption 3. The sequence {λk} satisﬁes λk = O(k−λ) for some constant λ ≥0 and the sample
size Nk = Θ(kβ) where β > max{0, 1 −2λ}."
UNTIL A STOPPING CONDITION IS REACHED,0.958041958041958,"Assumption 4. The (1 −ρ)-quantile of {V (x), x ∼gφ(x)} is unique for each φ ∈Φ."
UNTIL A STOPPING CONDITION IS REACHED,0.9603729603729604,"We know from Hu et al. (2011) that the sequence {ηk}∞
k=0 from equation 6 asymptotically ap-
proaches the solution set of the ODE: dη(t)"
UNTIL A STOPPING CONDITION IS REACHED,0.9627039627039627,"dt
= L(η)
(7)"
UNTIL A STOPPING CONDITION IS REACHED,0.965034965034965,"L(η) = ∆φ ln Eφ[1(V (x), γ(m−1(η)))] |φ=m−1(η)
(8)"
UNTIL A STOPPING CONDITION IS REACHED,0.9673659673659674,where γ(m−1(η)) is the true (1 −ρ)-quantile of V (x) under gm−1(η).
UNTIL A STOPPING CONDITION IS REACHED,0.9696969696969697,"Assumption 5. The function L(η) deﬁned in equation 8 has a unique integral curve for a given
initial condition."
UNTIL A STOPPING CONDITION IS REACHED,0.972027972027972,"The above assumptions 1-5 are the assumptions required by the previous convergence result of CEM.
To show the convergence of DecentCEM, we only require one additional mild condition:"
UNTIL A STOPPING CONDITION IS REACHED,0.9743589743589743,Under review as a conference paper at ICLR 2022
UNTIL A STOPPING CONDITION IS REACHED,0.9766899766899767,"Assumption 6. Let M be the number of instances in DecentCEM and each instance has a sample
size of Nk"
UNTIL A STOPPING CONDITION IS REACHED,0.9790209790209791,"M where Nk is the total number of samples that satisﬁes the assumption 3. M is constant
and 0 < M < Nk ∀k."
UNTIL A STOPPING CONDITION IS REACHED,0.9813519813519813,Now we restate the convergence result of DecentCEM from the main text and show the proof:
UNTIL A STOPPING CONDITION IS REACHED,0.9836829836829837,"Theorem 3.1 (Convergence of DecentCEM). If Assumptions 1-5 hold for a CEM instance described
in Algorithm 3 and we decentralize it by evenly dividing its sample size Nk into M CEM instances in
DecentCEM algorithm that satisﬁes Assumption 6, then the sequence of iterates {ηi,k} generated by
each CEM instance indexed by i converges almost surely to an internally chain recurrent set (Hirsch
et al., 2001) of Equation 7. Furthermore, the solution of DecentCEM {ηo,k} converges almost surely
to the best solution of the individual instances in terms of the expected value of Em−1(η)[V (x)]."
UNTIL A STOPPING CONDITION IS REACHED,0.986013986013986,Proof. Each individual CEM instance has a sample size of Nk
UNTIL A STOPPING CONDITION IS REACHED,0.9883449883449883,"M . Under Assumption 3, Nk = Θ(kβ).
Since Assumption 6 holds, M is constant and gets absorbed into the Θ and we have Nk"
UNTIL A STOPPING CONDITION IS REACHED,0.9906759906759907,"M = Θ(kβ).
Hence the conditions of Theorem 3.1 in (Hu et al., 2011) holds for each CEM instance indexed by
i and can be directly applied to show the almost sure convergence of their solutions {ηi,k} to an
internally chain recurrent set of Equation 7. If the recurrent sets are isolated equilibrium points, then
{ηi,k} converges almost surely to a unique equilibrium point."
UNTIL A STOPPING CONDITION IS REACHED,0.993006993006993,"Due to the fact that the instances in DecentCEM run independently from each other, their solutions
{ηi,k}M
i=1 (or equivalently {φi,k}M
i=1 = {m−1(ηi,k)}M
i=1) might converge to identical or different
solutions denoted as {η∗
i }M
i=1. DecentCEM computes the ﬁnal solution by applying an arg max
over all individual solutions: ηo,k = arg maxη∈{ηi,k}M
i=1 Em−1(η)[V (x)] (equivalent to Equation 3).
Here the expectation is approximated by the sample mean with respect to the distribution gm−1(η):"
"NK
PNK",0.9953379953379954,"1
Nk
PNk
j=1 V (xj), which converges almost surely to the true expectation according to the strong law
of large numbers. Hence we have that ηo,k converges almost surely to the best solution in the set
{η∗
i }M
i=1 found by the individual CEM instances, in terms of the expected value of Em−1(η)[V (x)]."
"NK
PNK",0.9976689976689976,"Note that the theorem implies that the solution of CEM / DecentCEM assigns the maximum proba-
bility to a locally optimal solution to Equation 4. It does not suggest whether this local optimum is
a global optimum or not. To the best of our knowledge, almost sure convergence to a local optimum
is the only convergence result that has been established about CEM in continuous optimization."
