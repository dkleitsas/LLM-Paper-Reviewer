Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0037174721189591076,"Evolution based zeroth-order optimization methods and policy gradient based
Ô¨Årst-order methods are two promising alternatives to solve reinforcement learn-
ing (RL) problems with complementary advantages. The former work with ar-
bitrary policies, drive state-dependent and temporally-extended exploration, pos-
sess robustness-seeking property, but suffer from high sample complexity, while
the latter are more sample efÔ¨Åcient but restricted to differentiable policies and the
learned policies are less robust. We propose Zeroth-Order Actor-Critic algorithm
(ZOAC) that uniÔ¨Åes these two methods into an on-policy actor-critic architec-
ture to preserve the advantages from both. ZOAC conducts rollouts collection
with timestep-wise perturbation in parameter space, Ô¨Årst-order policy evaluation
(PEV) and zeroth-order policy improvement (PIM) alternately in each iteration.
The modiÔ¨Åed rollouts collection strategy and the introduced critic network help
to reduce the variance of zeroth-order gradient estimators and improve the sam-
ple efÔ¨Åciency and stability of the algorithm. We evaluate our proposed method
using two different types of policies, linear policies and neural networks, on a
range of challenging continuous control benchmarks, where ZOAC outperforms
zeroth-order and Ô¨Årst-order baseline algorithms."
INTRODUCTION,0.007434944237918215,"1
INTRODUCTION"
INTRODUCTION,0.011152416356877323,"Reinforcement learning (RL) has achieved great success in a wide range of challenging domains,
including video games (Mnih et al., 2015), robotic control (Schulman et al., 2017), autonomous driv-
ing (Kendall et al., 2019), etc. The majority of RL methods formulate the environment as Markov
decision process (MDP) and leverage the temporal structure to design learning algorithms such as
Q-learning and policy gradient (Sutton & Barto, 2018). Actor-critic methods are among the most
popular RL algorithms, which usually introduce two function approximators, one for value func-
tion estimation (critic) and another for optimal policy approximation (actor), and optimize these two
approximators by alternating between policy evaluation (PEV) and policy improvement (PIM). On-
policy actor-critic methods, e.g., A3C (Mnih et al., 2016) and PPO (Schulman et al., 2017), often
use critics to construct advantage functions and substitute them for the Monte Carlo return used in
vanilla policy gradient (Williams, 1992), which signiÔ¨Åcantly reduces the variance of gradient esti-
mation and improve learning speed and stability. Among existing actor-critic algorithms, a common
choice is to use deep neural networks as the function approximators and conduct both PEV and PIM
using Ô¨Årst-order optimization techniques."
INTRODUCTION,0.01486988847583643,"An alternative approach for RL, though less popular, is to ignore the underlying MDP structures
and regard RL problems as black-box optimization, and to directly search for the optimal policy in
a zeroth-order way, i.e., without using the Ô¨Årst-order gradient information. Recent researches have
shown that zeroth-order optimization (ZOO) methods, e.g., ES (Salimans et al., 2017), ARS (Mania
et al., 2018) and GA (Such et al., 2017), are competitive on common RL benchmarks, even when
applied to deep neural network with millions of parameters. ZOO has several advantages compared
to Ô¨Årst-order MDP-based RL methods (Sehnke et al., 2010; Salimans et al., 2017; Such et al., 2017;
Lehman et al., 2018; Khadka & Tumer, 2018; Qian & Yu, 2021): (1) ZOO is not restricted to
differentiable policies; (2) ZOO perturbs the policy in parameter space rather than in action space,
which leads to state-dependent and temporally-extended exploration; (3) Zeroth-order population-
based optimization possesses robustness-seeking property and diverse policy behaviors."
INTRODUCTION,0.01858736059479554,"Despite these attractive advantages, the main limitation of ZOO is its high sample complexity and
high variance of the parameter update process, especially in high-dimensional problems. Recent"
INTRODUCTION,0.022304832713754646,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.026022304832713755,"researches have proposed various techniques to improve ZOO, e.g., using orthogonal or antithetic
sampling methods (Sehnke et al., 2010; Salimans et al., 2017; Choromanski et al., 2018; Mania
et al., 2018), identifying a low-dimensional search subspace (Maheswaranathan et al., 2019; Choro-
manski et al., 2019; Sener & Koltun, 2020), or subtracting a baseline for variance reduction (Sehnke
et al., 2010; Grathwohl et al., 2018). One of the major reasons for the sample inefÔ¨Åciency of ZOO is
its ignorance of the MDP temporal structures. Many recent researches have tried to combine ZOO
and Ô¨Årst-order MDP-based RL into hybrid methods, e.g., run evolutionary algorithms in parallel
with off-policy RL algorithms and optimize the population of policies with information from both
sides (Khadka & Tumer, 2018; Pourchot & Sigaud, 2018; Bodnar et al., 2020), or inject parameter
noise into existing RL algorithms for efÔ¨Åcient exploration (Plappert et al., 2018; Fortunato et al.,
2018). However, existing hybrid methods still conduct Ô¨Årst-order gradient-based policy improve-
ment (at least as a part), which reimposes differentiable requirement on the policy."
INTRODUCTION,0.02973977695167286,"In this paper, we propose the Zeroth-Order Actor-Critic algorithm (ZOAC), which uniÔ¨Åes Ô¨Årst-order
and zeroth-order RL methods into an actor-critic architecture by conducting Ô¨Årst-order PEV to up-
date the critic and zeroth-order PIM to update the actor. In such a way, complementary advantages
of both methods are preserved, e.g., wide adaptability to policy parameterization, robustness seeking
property, state-dependent and temporally-extended exploration. We modify the rollouts collection
strategy from episode-wise perturbation as in traditional zeroth-order methods to timestep-wise per-
turbation, which results in higher sample efÔ¨Åciency and better exploration. We derive the zeroth-
order policy gradient under this setting and point out that a critic network can be introduced to
estimate the state-value function and trade-off between bias and variance. We then propose a prac-
tical algorithm that utilizes several parallelized rollout workers and alternates between Ô¨Årst-order
PEV and zeroth-order PIM based on generated experiences in each iteration. We evaluate ZOAC on
a range of challenging continuous control benchmarks from OpenAI gym (Brockman et al., 2016),
using two different types of policies, linear policies and neural networks. Experiment results show
that ZOAC outperforms zeroth-order and Ô¨Årst-order baseline algorithms in sample efÔ¨Åciency, Ô¨Ånal
performance, and the robustness of the learned policies. We visualize the polices learned in an
environment with sparse and delayed reward, which indicates sufÔ¨Åcient exploration driven by pa-
rameter noise in ZOAC. Furthermore, we conduct ablation studies to demonstrate the indispensable
contribution of the modiÔ¨Åed rollouts collection strategy and the introduced critic network to ZOAC."
PRELIMINARIES,0.03345724907063197,"2
PRELIMINARIES"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.03717472118959108,"2.1
FROM POLICY GRADIENT TO ACTOR-CRITIC"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.040892193308550186,"In standard MDP-based RL settings, the environment is usually formulated as an MDP deÔ¨Åned as
(S, A, P, r), where S is the state space, A is the action space, P : S √ó A √ó S ‚ÜíR is the transition
probability matrix, r : S √ó A ‚ÜíR is the reward function. The return is deÔ¨Åned as the total
discounted future reward Gt = P‚àû
i=0 Œ≥ir(st+i, at+i), where Œ≥ ‚àà(0, 1) is the discounting factor.
The behavior of the agent is controlled by a policy œÄ(a|s) : S √ó A ‚Üí[0, 1], which maps states to a
probability distribution over actions. The state-value function is deÔ¨Åned as the expected return under
policy œÄ starting from a certain state: V œÄ(s) = Ea‚àºœÄ{Gt|st = s}. The goal of MDP-based RL is
to Ô¨Ånd an optimal policy that maximizes the expectation of state-value function under a certain state
distribution. Denoting a policy parameterized with Œ∏ as œÄŒ∏, the objective function can be written as:"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.04460966542750929,"JPG(Œ∏) = Es‚àºd[V œÄŒ∏(s)],
d = d0 or dœÄŒ∏
(1)"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.048327137546468404,"where d0 is the initial state distribution and dœÄŒ∏ is the stationary state distribution of Markov chain
under policy œÄŒ∏. Generally, the former is used for episodic tasks with Ô¨Ånite horizon and the latter
is used for continuing tasks with inÔ¨Ånite horizon. For any differentiable policy œÄŒ∏, and for con-
tinuing or episodic tasks, the same form of policy gradient can be derived from the policy gradient
theorem (Sutton & Barto, 2018). This vanilla policy gradient given by Williams (1992) is as follows:"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.05204460966542751,"‚àáŒ∏JPG(Œ∏) = Est‚àºdœÄŒ∏ ,at‚àºœÄŒ∏[Gt‚àáŒ∏ log œÄŒ∏(at|st)]
(2)"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.055762081784386616,"Vanilla policy gradient suffers from high variance since it directly uses Monte Carlo return from
sampled trajectories. Actor-critic methods improved upon it, which usually introduce a critic net-
work to estimate the value function and serve as a baseline to substitute the expected return Gt with
a proper form of advantage function At, for example, TD residual (Mnih et al., 2016), or gener-
alized advantage estimation (GAE) (Schulman et al., 2015). However, the above policy gradient"
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.05947955390334572,Under review as a conference paper at ICLR 2022
FROM POLICY GRADIENT TO ACTOR-CRITIC,0.06319702602230483,"based methods can only be applied to differentiable policies, and may be unavailable when a non-
differentiable controller needs to be optimized."
EVOLUTION STRATEGIES,0.06691449814126393,"2.2
EVOLUTION STRATEGIES"
EVOLUTION STRATEGIES,0.07063197026022305,"Existing ZOO methods focus on episodic RL problems with Ô¨Ånite horizon and treat them as black-
box optimization. In these cases, the length of trajectories is limited and the discounting factor Œ≥
is usually set as 1. Evolution strategies (ES) is one of the most popular algorithms of ZOO, which
optimizes a Gaussian smoothed objective function:"
EVOLUTION STRATEGIES,0.07434944237918216,"JES(Œ∏) = Eœµ‚àºN(0,I)Es‚àºd0[V œÄŒ∏+œÉœµ(s)]
(3)"
EVOLUTION STRATEGIES,0.07806691449814127,"where d0 is the initial state distribution and œÉ is the standard deviation of the Gaussian noise added
to the policy. The zeroth-order gradient can be derived using the log-likelihood ratio trick and the
probability density function of Gaussian distribution (Nesterov & Spokoiny, 2017):"
EVOLUTION STRATEGIES,0.08178438661710037,‚àáŒ∏JES(Œ∏) = 1
EVOLUTION STRATEGIES,0.08550185873605948,"œÉ Eœµ‚àºN(0,I)Es‚àºd0[V œÄŒ∏+œÉœµ(s)œµ]
(4)"
EVOLUTION STRATEGIES,0.08921933085501858,"In practice, the expectation over Gaussian distribution can be approximated by sampling n noise
samples {œµi}i=1,...,n, and the corresponding state value V œÄŒ∏+œÉœµi can be approximated by the
episodic return Gi = PT
t=0 Œ≥tr(st, at) of the sample trajectory of length T collected with policy
œÄŒ∏+œÉœµi:"
EVOLUTION STRATEGIES,0.09293680297397769,"‚àáŒ∏JES(Œ∏) ‚âà1 nœÉ n
X"
EVOLUTION STRATEGIES,0.09665427509293681,"i=1
Giœµi
(5)"
EVOLUTION STRATEGIES,0.10037174721189591,"The zeroth-order gradient estimator in Equation (5) only relies on the episodic return of each evalu-
ated random directions, so it is applicable to non-differentiable policies. Besides, each perturbed pol-
icy remains deterministic in one trajectory, which leads to state-dependent and temporally-extended
exploration. Furthermore, the Gaussian smoothed objective also improves robustness of the learned
policies in parameter space."
ZEROTH-ORDER ACTOR-CRITIC,0.10408921933085502,"3
ZEROTH-ORDER ACTOR-CRITIC"
FROM ES TO ZOAC,0.10780669144981413,"3.1
FROM ES TO ZOAC"
FROM ES TO ZOAC,0.11152416356877323,"In this section, we will derive an improved zeroth-order gradient combining the actor-critic archi-
tecture for policy improvement. We start from improving the sample efÔ¨Åciency and stability of ES.
Most of the existing ES methods applied to RL optimize a deterministic policy (Salimans et al.,
2017; Mania et al., 2018), where the exploration is driven by noise in parameter space. Without loss
of generality, we follow them in the following derivations and algorithm design. A deterministic
policy parameterized with Œ∏ is denoted as œÄŒ∏ : S ‚ÜíA, which directly maps states to actions."
FROM ES TO ZOAC,0.11524163568773234,"In ES, the policy is perturbed in parameter space at the beginning of an episode and remains un-
changed throughout the trajectories. If a large number of random directions n is evaluated, the
sample complexity will increase signiÔ¨Åcantly. However, since the zeroth-order gradient is estimated
as the weighted sum of several random directions, it exhibit excessively high variance when n is
small (Berahas et al., 2021), which may greatly harm the performance. Therefore, it is essential to
trade-off this contradictory between sample efÔ¨Åciency and variance."
FROM ES TO ZOAC,0.11895910780669144,"To encourage sufÔ¨Åcient exploration and low variance while maintaining high sample efÔ¨Åciency, here
we consider perturbating the policy at every timestep, i.e., the Gaussian noise œµ is sampled identically
and independently at every timesteps. We regard it as a stochastic exploration policy Œ≤ = œÄŒ∏+œÉœµ,
where œµ ‚àºN(0, I) is Gaussian noise in parameter space and œÉ is the standard deviation. Our
objective is to maximize the expectation of the state-value under stationary distribution dŒ≤ of the ex-
ploration policy Œ≤. We can leverage Bellman equation to estimate the state-value via bootstrapping,
in which the one-step reward can be replaced with sampled experiences:"
FROM ES TO ZOAC,0.12267657992565056,"JZOAC(Œ∏) = Est‚àºdŒ≤[V Œ≤(st)] = Eœµ‚àºN(0,I)Est‚àºdŒ≤Est+1‚àºP[r(st, œÄŒ∏+œÉœµ(st)) + Œ≥V Œ≤(st+1)]
(6)"
FROM ES TO ZOAC,0.12639405204460966,"Since all the contents in the outer expectation Eœµ‚àºN(0,I)[¬∑] can be regarded as a function of œµ, the
zeroth-order gradient of this objective function can be derived in exactly the same way as in ES."
FROM ES TO ZOAC,0.13011152416356878,Under review as a conference paper at ICLR 2022
FROM ES TO ZOAC,0.13382899628252787,"Moreover, V Œ≤(st) can be subtracted as a baseline for variance reduction because of its uncorrelation
to œµ in the outer expectation and the zero mean property of the Gaussian noise œµ:"
FROM ES TO ZOAC,0.137546468401487,‚àáŒ∏JZOAC(Œ∏) = 1
FROM ES TO ZOAC,0.1412639405204461,"œÉ Eœµ‚àºN(0,I)Est‚àºdŒ≤Est+1‚àºP{[r(st, œÄŒ∏+œÉœµ(st)) + Œ≥V Œ≤(st+1) ‚àíV Œ≤(st)]œµ}
(7)"
FROM ES TO ZOAC,0.1449814126394052,"Compared to ES which uses unbiased but high variance Monte Carlo return to evaluate each per-
turbed policy, the performance of each random direction here is estimated by one-step TD residual
with low variance. In practice, a common approach is to introduce a critic network Vw(s) to estimate
the state-value function V Œ≤, which may lead to high bias in this form of advantage estimation."
FROM ES TO ZOAC,0.14869888475836432,"To trade-off between bias and variance, we consider extending our derivation further to the case
where each perturbed policies (i.e., each sampled random noise) run forward N timesteps instead of
one timestep only. Equation (7) can be extended to:
‚àáŒ∏JZOAC(Œ∏) = 1"
FROM ES TO ZOAC,0.1524163568773234,"œÉ Eœµ‚àºN(0,I)Est‚àºdŒ≤EP"
FROM ES TO ZOAC,0.15613382899628253,"(""N‚àí1
X"
FROM ES TO ZOAC,0.15985130111524162,"i=0
Œ≥ir(st+i, œÄŒ∏+œÉœµ(st+i)) + Œ≥NV Œ≤(st+N) ‚àíV Œ≤(st) # œµ )
(8)"
FROM ES TO ZOAC,0.16356877323420074,"where EP refers to expectation over N-step transition dynamics. Similar to one-step case, the
cumulative reward within N step can be estimated from sampled experiences when st is the Ô¨Årst
state of the trajectory fragment collected with a certain perturbed policy œÄŒ∏+œÉœµ. By introducing
a critic network and choosing an appropriate length N, this N-step residual advantage function
contributes to achieving a good trade-off between the bias and variance."
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.16728624535315986,"3.2
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.17100371747211895,"We analyze the variance of these two types of gradient estimators, ZOAC gradient and ES gradient.
The budget of timestep of one trajectory N √ó H is identical for both algorithms: in ZOAC, each
perturbed policy run forward N steps, and H is the number of sampled random directions in one
trajectory; in ES, only one perturbed policy is sampled and run forward N √ó H steps. If we denote
the accumulative reward obtained within N √ó H timesteps in ES as ÀÜV œÄŒ∏+œÉœµ
NH
and the N-step TD
residual in ZOAC as ÀÜAœÄŒ∏+œÉœµ
N
. We can then estimate the zeroth-order gradient according to Equation
(4) and (8) respectively:"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.17472118959107807,"‚àáŒ∏ ÀÜJES(Œ∏) = 1 nœÉ n
X"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.17843866171003717,"i=1
ÀÜV
œÄŒ∏+œÉœµi
NH
œµi
(9)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.1821561338289963,"‚àáŒ∏ ÀÜJZOAC(Œ∏) =
1
nHœÉ nH
X"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.18587360594795538,"i=1
ÀÜA
œÄŒ∏+œÉœµi
N
œµi
(10)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.1895910780669145,"We now give the upper bound of variance for these two gradient estimators. Variance is deÔ¨Åned
as the trace of the convariance matrix of gradient vectors Var(g) = Pd
l=1 E[g2
l ] ‚àí(Egl)2, where
g = (g1, g2, ..., gd)‚ä§(Zhao et al., 2011). We can derive the variance bound as follows if both the
reward and the critic network output is bounded (detailed derivation is provided in Appendix A.3,
vectors are bolded in the appendix for clarity but not in the main text)."
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.19330855018587362,"Theorem 1. If the reward |r(s, a)| < Œ±, the critic network output |Vw(s)| < Œ≤, and n trajectories
with length of N √ó H timesteps are collected in one iteration, the upper bounds of the variance for
gradient estimators (Equation (9) and (10)) are:"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.1970260223048327,Var[‚àáŒ∏ ÀÜJES(Œ∏)] ‚â§(1 ‚àíŒ≥NH)2Œ±2d
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.20074349442379183,"nœÉ2(1 ‚àíŒ≥)2
(11)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.20446096654275092,Var[‚àáŒ∏ ÀÜJZOAC(Œ∏)] ‚â§((1 ‚àíŒ≥N)Œ± + (1 ‚àíŒ≥)(1 + Œ≥N)Œ≤)2d
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.20817843866171004,"nHœÉ2(1 ‚àíŒ≥)2
(12)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.21189591078066913,"We can compare their variance in a more intuitive way: if N √ó H = 1000, Œ≥ = 0.99, and assume
that Œ≤ ‚âà
Œ±
1‚àíŒ≥ , the difference of variance bounds becomes Var[‚àáŒ∏ ÀÜJZOAC(Œ∏)] ‚àíVar(‚àáŒ∏ ÀÜJES(Œ∏)) ‚âà ( 4"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.21561338289962825,H ‚àí1) 10000Œ±2d
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.21933085501858737,"nœÉ2
, which decreases with H and drops below zero when 4 < H ‚â§1000 (i.e., N
is smaller than 250). This suggests that although same amount of data is collected, an appropriate
rollout length N can indeed reduce variance of the gradient estimators. Besides, both variance bound
are inversely proportional to n, which urges us to collect more trajectories."
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.22304832713754646,Under review as a conference paper at ICLR 2022
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.22676579925650558,"Terminal 
Condition"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.23048327137546468,"Generated
Experiences
(ùë†ùë°, ùëéùë°, ùëüùë°, ùë†ùë°+1)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2342007434944238,First-order PEV
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2379182156133829,"(ùë†, ùê∫)
ùê∫ùë°= ùëâùë§ùë†ùë°+
(ùõæùúÜ)ùëòùõøùë°+ùëò ùëá‚àíùë°‚àí1"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.241635687732342,"ùëò=0
‚àáùë§ùêΩcritic ùë§= ùîº(ùë†,ùê∫)
ùëâùë§ùë†‚àíùê∫‚àáùë§ùëâùë§ùë†"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.24535315985130113,Zeroth-order PIM
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.24907063197026022,‚àáùúÉùêΩactor ùúÉ= 1
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2527881040892193,"ùúéùîº(ùúñ,ùê¥) ùê¥ùúñ"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.25650557620817843,"(ùúñ, ùê¥)
ùê¥ùëñ,ùëó=
(ùõæùúÜ)ùëòùõøùëñ,ùëó ùëÅ‚àí1 ùëò=0"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.26022304832713755,"Rollouts Collection ùëé ùë† ùëü, ùë†‚Ä≤"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.26394052044609667,"Environment
Environment
Environment"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.26765799256505574,"Behavior Policy ùõΩ= ùúãùúÉ+ùúéùúñ
Behavior Policy ùõΩ= ùúãùúÉ+ùúéùúñ
Behavior Policy ùõΩ= ùúãùúÉ+ùúéùúñ"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.27137546468401486,"ùúñ~ùí©(0, ùêº)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.275092936802974,Parallelized
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2788104089219331,workers
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2825278810408922,(a) Overall framework of ZOAC
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2862453531598513,"ùúñ~ùí©(0, ùêº)"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2899628252788104,"ùë†1,0
ùë†1,ùëÅ
ùúãùúÉ+ùúéùúñ1,0"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.2936802973977695,"ùúãùúÉ+ùúéùúñ1,1 ‚Ä¶"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.29739776951672864,"ùë†1,(ùêª‚àí1)ùëÅ
ùë†1,2ùëÅ ‚Ä¶"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.30111524163568776,"ùë†ùëõ,0
ùë†ùëõ,ùëÅ
ùúãùúÉ+ùúéùúñùëõ,0 ‚Ä¶"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.3048327137546468,"ùúãùúÉ+ùúéùúñ1,ùêª‚àí1 ùë†1,ùêªùëÅ ùë†ùëõ,ùêªùëÅ"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.30855018587360594,"ùúãùúÉ+ùúéùúñùëõ,ùêª‚àí1"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.31226765799256506,"ùë†ùëõ,(ùêª‚àí1)ùëÅ"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.3159851301115242,"ùúãùúÉ+ùúéùúñùëõ ‚Ä¶ ùë†ùëõ,0 ùë†1,0 ‚Ä¶ ‚Ä¶ ES ZOAC"
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.31970260223048325,ùúãùúÉ+ùúéùúñ1
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.32342007434944237,(b) Rollouts collection strategies: ES vs. ZOAC
ANALYSIS ON VARIANCE OF GRADIENT ESTIMATORS,0.3271375464684015,"Figure 1: (a) The overall framework of our proposed algorithm ZOAC; (b) Comparison of rollouts
collection strategies (with n parallelized samplers): On the top is ES, which performs episode-wise
perturbation; on the bottom is ZOAC, which performs N timestep-wise perturbation."
PRACTICAL ALGORITHM,0.3308550185873606,"3.3
PRACTICAL ALGORITHM"
PRACTICAL ALGORITHM,0.3345724907063197,"We propose the Zeroth-Order Actor-Critic (ZOAC) algorithm, which uniÔ¨Åes Ô¨Årst-order and zeroth-
order methods into an on-policy actor-critic architecture by conducting rollouts collection with
timestep-wise perturbation in parameter space, Ô¨Årst-order policy evaluation (PEV) and zeroth-order
policy improvement (PIM) alternately in each iteration. The overall framework of ZOAC is shown
in Figure 1a and the pseudocode is summarized in Appendix A.1. In each iteration, parallelized
workers will collect rollouts in the environment with perturbed policies, then the agent train the
critic network to estimate state-value function under the exploration policy, and Ô¨Ånally improve the
policy along the zeroth-order gradient direction."
PRACTICAL ALGORITHM,0.3382899628252788,"Rollouts collection.
The rollouts collection strategy is illustrated brieÔ¨Çy in Figure 1b, which is
a parallelized version with n workers. If we denote the t-th state sampled by the i-th worker as
si,t, the rollout strategy can be described as: when reaching states in {si,jN}, where j ‚ààN, a new
random direction œµi,j is sampled and the behavior policy is perturbed; when reaching other states,
the deterministic behavior policy remains unchanged. It‚Äôs worth noting that the notation is only for
continuing case where an episode is never done. In episodic tasks, the rollout length 1 ‚â§Ni,j ‚â§N
actually varies between different perturbed policies œÄŒ∏+œÉœµi,j since an episode may terminate at any
time. However, we still use N to denote the rollout length of each perturbed policy for brevity."
PRACTICAL ALGORITHM,0.3420074349442379,"A limit case of our proposed strategy is that when N is chosen as the episode length and the critic
network is turned off (i.e., Vw(s) ‚â°0), the algorithm actually degenerate into ES, since all perturbed
policies are evaluated by running a whole episode, and the episodic return is used as the Ô¨Åtness score."
PRACTICAL ALGORITHM,0.34572490706319703,"First-order PEV.
The state-value function ÀÜV (s) can be estimated by a jointly optimized critic
network Vw(s), which aims to minimize the MSE loss between the network output and state-value
target. In each iteration, in total n √ó N √ó H states and the corresponding target values (s, ÀÜG) are
calculated and used for critic training. In a trajectory with length T, the target value ÀÜGt for each
state st is calculated as (Schulman et al., 2015; Andrychowicz et al., 2021):"
PRACTICAL ALGORITHM,0.34944237918215615,ÀÜGt = Vw(st) +
PRACTICAL ALGORITHM,0.35315985130111527,"T ‚àít‚àí1
X"
PRACTICAL ALGORITHM,0.35687732342007433,"k=0
(Œ≥Œª)k[rt+k + Œ≥Vw(st+k+1) ‚àíVw(st+k)]
(13)"
PRACTICAL ALGORITHM,0.36059479553903345,"where 0 < Œª < 1 is a hyperparameter to control the trade-off between bias and variance of the value
target. In Figure 1a, the one-step TD residual of each state s is denoted as Œ¥ for simplicity. The
objective function of PEV can be written as:"
PRACTICAL ALGORITHM,0.3643122676579926,"Jcritic(w) = E(s, ÀÜ
G) 1"
PRACTICAL ALGORITHM,0.3680297397769517,"2[Vw(s) ‚àíÀÜG]2

(14)"
PRACTICAL ALGORITHM,0.37174721189591076,"In practice, the critic network is constructed as a neural network and updated through several epoches
of stochastic gradient descent in each iteration."
PRACTICAL ALGORITHM,0.3754646840148699,Under review as a conference paper at ICLR 2022
PRACTICAL ALGORITHM,0.379182156133829,"Figure 2: Learning curves on MuJoCo benchmarks. Exploration noise are turned off for evaluation.
The solid lines correspond to the mean and the shaded regions to the 95% conÔ¨Ådence interval over
5 trials using a Ô¨Åxed set of random seeds. All curves are smoothed uniformly for visual clarity."
PRACTICAL ALGORITHM,0.3828996282527881,Table 1: Max total average return within certain environmental steps (mean¬±std over 5 trials).
PRACTICAL ALGORITHM,0.38661710037174724,"Environment
Inv.D.P.-v2
Hopper-v2
HalfCheetah-v2
Ant-v2
Timesteps
1e6
2e6
2e6
2e7
ZOAC(Linear)
9359.93¬±0.01
3333.60¬±97.85
5190.82¬±196.99
4495.33¬±100.37
ZOAC(Neural)
9339.66¬±5.29
3195.15¬±91.76
5339.95¬±140.46
4292.09¬±108.69
ARS(Linear)
9359.86¬±0.06
2891.28¬±305.61
2967.98¬±889.42
3427.85¬±765.62
ES(Neural)
9155.73¬±404.25
1065.34¬±49.14
2349.11¬±444.77
3274.89¬±519.66
PPO(Neural)
9350.89¬±0.04
3178.60¬±270.09
5219.61¬±677.90
3796.13¬±754.78"
PRACTICAL ALGORITHM,0.3903345724907063,"Zeroth-order PIM.
We calculate the zeroth-order gradient with n √ó H random directions and
the corresponding advantage function as (œµ, ÀÜA). Similar to state value estimation, we leverage the
generalized advantage estimation (GAE) trick (Schulman et al., 2015) to further control the bias-
variance trade-off. We also perform advatage normalization to ensure consistent gradient length
during training. Following the notations in Figure 1b, the advantage function can be written as:"
PRACTICAL ALGORITHM,0.3940520446096654,"ÀÜA
œÄŒ∏+œÉœµi,j
N
= N‚àí1
X"
PRACTICAL ALGORITHM,0.39776951672862454,"k=0
(Œ≥Œª)k[ri,jN+k + Œ≥Vw(si,jN+k+1) ‚àíVw(si,jN+k)]
(15)"
PRACTICAL ALGORITHM,0.40148698884758366,"where Œª is the same as in Equation (13). The zeroth-order gradient can be then estimated as the
weighted sum of the sampled random directions:"
PRACTICAL ALGORITHM,0.4052044609665427,"‚àáŒ∏Jactor(Œ∏) ‚âà
1
nHœÉ n
X i=1 H‚àí1
X"
PRACTICAL ALGORITHM,0.40892193308550184,"j=0
ÀÜA
œÄŒ∏+œÉœµi,j
N
œµi,j
(16)"
EXPERIMENTS,0.41263940520446096,"4
EXPERIMENTS"
PERFORMANCE EVALUATION,0.4163568773234201,"4.1
PERFORMANCE EVALUATION"
PERFORMANCE EVALUATION,0.4200743494423792,"We evaluate the performance of ZOAC on the MuJoCo continuous control benchmarks (Todorov
et al., 2012) in OpenAI Gym (Brockman et al., 2016). We choose Evolution Strategies (ES) (Sali-
mans et al., 2017; Liang et al., 2018) and Augmented Random Search (ARS) (Mania et al., 2018) as
zeroth-order baselines and proximal policy optimization (PPO) (Schulman et al., 2017; RafÔ¨Ån et al.,
2019) as a Ô¨Årst-order actor-critic baseline."
PERFORMANCE EVALUATION,0.42379182156133827,"We use two different types of policies: linear policies for ARS and ZOAC (linear), neural networks
with (64, 64) hidden nodes and tanh nonlinearities for ES, PPO and ZOAC (neural). For a fair
comparison, we enable observation normalization for all methods, which has been proved effective
no matter in Ô¨Årst-order methods or zeroth-order methods (Mania et al., 2018; Andrychowicz et al.,
2021). When using neural networks as actors, we also use layer normalization (Ba et al., 2016)
in ZOAC and virtual batch normalization in ES (Salimans et al., 2017). Both of them ensure the
diversity of behaviors among the population, while the former is less computationally expensive.
We summarize the implementation details of ZOAC in Appendix A.2 and follow the recommended
hyperparameter settings listed in the related papers or code repositories."
PERFORMANCE EVALUATION,0.4275092936802974,Under review as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.4312267657992565,"Figure 3: Visualization of the policies learned by different methods on MountainCarContinuous-v0.
Color represents the action output, from ‚àí1.0 (red) to 1.0 (blue). The horizontal lines in solid indi-
cate the initial car position distribution x ‚àºU(‚àí0.6, ‚àí0.4), and the vertical lines in solid indicate
the goal x > 0.45. The dashed curves are trajectories starting from the same initial state."
PERFORMANCE EVALUATION,0.4349442379182156,"Table 2: Robustness comparison of the learned policies on HalfCheetah-v2. Best performing poli-
cies from Figure 2 are tested (4e7 timestep limitation for ARS and ES). Each policy is evaluated on
400 trajectories in the same environment and the average return over 5 policies are listed."
PERFORMANCE EVALUATION,0.43866171003717475,"Noise type
No extra noise
Obs. noise (œÉ = 0.1)
Para. noise (œÉ = 0.05)
ZOAC(Linear)
4909.53
4402.54 (-10.3%)
3289.15 (-33.0%)
ZOAC(Neural)
5179.39
5057.07 (-2.4%)
4247.08 (-18.0%)
ARS(Linear)
4529.30
1771.23 (-60.9%)
1724.40 (-61.9%)
ES(Neural)
5478.01
4550.22 (-16.9%)
1527.93 (-72.1%)
PPO(Neural)
4668.55
3342.23 (-28.4%)
972.08 (-79.2%)"
PERFORMANCE EVALUATION,0.4423791821561338,"Figure 2 presents the learning curves on four continuous control tasks. Table 1 summarizes max total
average return within the timestep threshold over 5 trials. Additional results including state-value
estimation and performance comparison of different policies are attached in Appendix A.4 and A.6."
PERFORMANCE EVALUATION,0.44609665427509293,"ZOAC matches or outperforms baseline algorithms across tasks in learning speed, Ô¨Ånal performance,
and variance over trials. One thing worth mentioning is that both the zeroth-order baseline methods
perform reward shaping to resolve the local optima problem: ARS subtracts the survival bonus from
rewards (1 in Hopper and Ant), while ES transforms the episodic returns into rankings. Although
these tricks improve the performance, they also alter the update directions of the policies and make
it difÔ¨Åcult to determine what is the real objective function being optimized. ZOAC, however, sur-
passes ES and ARS without relying on speciÔ¨Åc exploration tricks, which can be attributed to the
introduction of critic network and the construction of advantage estimations in policy improvement."
PERFORMANCE EVALUATION,0.44981412639405205,"Robustness Comparison.
The objective function of ZOAC aims to maximize the expected state-
value of the stochastic behavior policy that contains parameter noise all the time, which intuitively
encourages the agent to Ô¨Ånd a wider optima and leads to better generalization and robustness. Hence,
we evaluate the learned policies under two types of noise, observation noise and parameter noise.
Extra observation noise is added to the normalized observation at each timestep, which leads to
a slightly different observation distribution. Extra parameter noise is added at the beginning of
each trajectories, which pushes the learned policy to its neighborhood. The result in HalfCheetah-
v2 is presented in Table 2. Results show that in general the policies learned by ZOAC possess
higher robustness against both observation noise and parameter noise, which can be ascribed to
the robustness-seeking property of our method. The linear policies learned by ARS seem very
fragile and suffer signiÔ¨Åcant performance degradation under extra noise, far inferior to the linear
ones learned by ZOAC. Additional results and discussions are attached in Appendix A.5."
PERFORMANCE EVALUATION,0.45353159851301117,"Visualization of the learned policies.
In order to intuitively observe the behaviors of the learned
policies, we apply all methods on MountainCarContinuous-v0, in which the car is rewarded +100
only when it achieves the goal and penalized by the action output at every timestep. Policy gradient
methods usually struggle on this problem because the reward is sparse and delayed, while zeroth-
order methods can better handle reward sparsity by nature. We visualize the policies learned by each
algorithm in Figure 3. Among these learned policies, the neural policy learned by ES obtains the
highest average return, while the neural policy learned by ZOAC obtains the second highest average"
PERFORMANCE EVALUATION,0.45724907063197023,Under review as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.46096654275092935,"Figure 4: InÔ¨Çuence of the rollout length
N with each perturbed policies."
PERFORMANCE EVALUATION,0.4646840148698885,"Figure 5: Ablation studies on ZOAC with neural network
(left) and linear poicy (right)."
PERFORMANCE EVALUATION,0.4684014869888476,"return within the shortest episode length. The latter has a similar but much steeper terrain compared
to the former one and implies a larger control action in most areas. As for linear policies, the one
learned by ZOAC also tends to achieve the goal in a shorter episode length than the one learned by
ARS. We attribute this to the usage of discounting factor, which pushes the agent to perform higher
actions and achieve the goal as early as possible. ZOAC outperforms PPO both in Ô¨Ånal performance
and training stability over different random seeds, due to its state-dependent exploration in parameter
space, which is more efÔ¨Åcient than action noise and is essential to solve this task."
ABLATION STUDIES,0.4721189591078067,"4.2
ABLATION STUDIES"
ABLATION STUDIES,0.4758364312267658,"Appropriate rollout length.
As illustrated in Section 3.2, choosing appropriate rollout length N
of each perturbed policies may achieve a good trade-off between bias and variance. We perform
an ablation study to understand the effect of timestep-wise perturbation strategy and choice of N.
We compare ZOAC and ARS on HalfCheetah-v2, the former conducting timstep-wise perturbation
and the latter conducting episode-wise perturbation. We use linear policies in both methods and set
hyperparameters to the same, including the standard deviation of parameter noise œÉ, the learning rate
of policy Œ±actor, and also the budget of timesteps within one iteration. Figure 4 shows the inÔ¨Çuence
of N on performance. Results show that under this setting, both 10 and 20 are good choices for N
which lead to better performance, while other two choices, 5 and 50, perform similarly to ARS. Note
that here we sample as many experiences as ARS per iteration for comparison, while in practice, the
number of timesteps collected in each iteration is highly tunable in ZOAC. In fact, we found that
rollout length N in a large range, approximately from 5 to 50, perform quite well across tasks."
ABLATION STUDIES,0.4795539033457249,"Analysis on different components.
To evaluate the contribution of each individual component
and also the potential of additional techniques, we perform ablation studies and present the results
in Figure 5. Results demonstrate that critic network is a crucial part of ZOAC, i.e., N-step accu-
mulative reward without bootstrapping is not sufÔ¨Åcient to guide policy improvement. Observation
normalization technique is also essential to zeroth-order methods, which helps to generate diverse
policies via isotropic Gaussian noise. Besides, GAE trick and layer normalization trick slightly im-
prove the performance. Mania et al. (2018) propose to use only the top performing directions in
policy update to relieve the bad inÔ¨Çuence of noisy evaluation results and validate its effectiveness
on ARS. Here we perform a similar direction sifting technique, using only the directions that have
the highest advantage in policy improvement, but it seems to pull down the learning performance
of ZOAC. Moreover, results show that additional action noise is not helpful to the performance,
indicating that the exploration driven by parameter noise is sufÔ¨Åcient."
RELATED WORK,0.483271375464684,"5
RELATED WORK"
RELATED WORK,0.48698884758364314,"ZOO and its applications in RL.
At each iteration, ZOO samples several random directions
from a certain distribution, and then the distribution is updated according to the evaluation results
over these directions. Sehnke et al. (2010) derive parameter-exploring policy gradients (PGPE)
for episodic RL problems, which has reduced variance and higher performance than vanilla policy
gradient. Salimans et al. (2017) and Such et al. (2017) propose highly scalable evolution strategies
(ES) and genetic algorithms (GA) respectively, both of which can be applied to deep neural networks
and achieve competitive performance with MDP-based RL algorithms. Mania et al. (2018) propose"
RELATED WORK,0.49070631970260226,Under review as a conference paper at ICLR 2022
RELATED WORK,0.4944237918215613,"augmented random search (ARS), which applied ZOO to linear policies with techniques including
observation normalization, reward scaling, top performing directions sifting, and achieve astonishing
performance on RL benchmarks considering its simplicity. ZOO has regained popularity in recent
years because of its special advantages when applied in RL, including wide adaptability to policy
parameterization (e.g., deterministic or stochastic, differentiable or non-differentiable), robustness
seeking property, state-dependent and temporally-extended exploration."
RELATED WORK,0.49814126394052044,"Improved techniques for ZOO.
The main limitation of ZOO is its high sample complexity. Re-
searchers have proposed various improved techniques for ZOO from different perspectives. One way
is to adopt advanced Monte Carlo sampling methods to reduce variance of the zeroth-order gradient
estimation, e.g., antithetic sampling (Sehnke et al., 2010; Salimans et al., 2017; Mania et al., 2018),
orthogonal and Quasi Monte Carlo exploration (Choromanski et al., 2018). Constructing control
variates (i.e., subtracting a baseline) is another popular variance reduction technique. Sehnke et al.
(2010) adopt a moving-average baseline in PGPE heuristically, while Zhao et al. (2011) derive the
optimal baseline for PGPE in an analytical form that minimizes the variance. Moreover, the sample
complexity of zeroth-order methods will further increase with the dimension of the optimization
problem (Nesterov & Spokoiny, 2017), therefore some researches aim to identify a low-dimensional
search space and guide the search towards faster convergence. Guided ES (Maheswaranathan et al.,
2019) and ASEBO (Choromanski et al., 2019) are proposed based on a similar idea: to identify
linear subspaces and adapt the search distribution from recent history of descent directions. Sener &
Koltun (2020) propose LMRS, which uses more expressive neural networks to represent subspaces
and jointly learns the underlying subspace and optimizes the objective function."
RELATED WORK,0.5018587360594795,"Hybridization of ZOO and Ô¨Årst-order MDP-based RL.
These two methods have complemen-
tary advantages when applied to RL problems, and recent researches have tried to combine them for
better performance. Khadka & Tumer (2018) propose the ERL framework that runs evolutionary
algorithms (EA) and DDPG (Lillicrap et al., 2015) concurrently with bidirectional information Ô¨Çow,
i.e., the DDPG agent is trained with experiences generated by the EA population and reinserted into
the population periodically to guide the evolution process. CEM-RL (Pourchot & Sigaud, 2018) and
Proximal Distilled ERL (Bodnar et al., 2020) adopt similar hybridization framework, but use differ-
ent algorithms as components and improve training techniques. Fortunato et al. (2018) and Plappert
et al. (2018) inject parameter noises into existing Ô¨Årst-order MDP-based RL algorithms to drive more
efÔ¨Åcient exploration, and demonstrate that existing RL algorithms can indeed beneÔ¨Åt from parameter
space exploration through comparative experiments. Some other hybrid methods (Grathwohl et al.,
2018; Tang et al., 2020) leverage policy gradient and reparameterization trick to construct control
variates, which leads to unbiased, low variance gradient estimators. Our proposed method, however,
uniÔ¨Åes Ô¨Årst-order and zeroth-order methods into an on-policy actor-critic architecture by conducting
Ô¨Årst-order PEV and zeroth-order PIM alternately in each iteration. The state-value function network
does not only serve as a baseline to reduce variance, but also as a critic used for bootstrapping, which
leads to reduced variance and accelerated learning (Sutton & Barto, 2018). The policy is updated in
a zeroth-order way, which implies wide adaptability to different forms of policies."
CONCLUSION,0.5055762081784386,"6
CONCLUSION"
CONCLUSION,0.5092936802973977,"In this paper, we propose Zeroth-Order Actor-Critic algorithm (ZOAC) that uniÔ¨Åes evolution based
zeroth-order and policy gradient based Ô¨Årst-order methods into an on-policy actor-critic architecture
to preserve the advantages from both, including the ability to handle different forms of policies, state-
dependent exploration, robustness-seeking property from the former and high sample efÔ¨Åciency
from the latter. ZOAC conducts rollouts collection with timestep-wise perturbation in parameter
space, Ô¨Årst-order policy evaluation (PEV) and zeroth-order policy improvement (PIM) alternately
in each iteration. Experimental results in a range of challenging continuous control tasks show that
ZOAC outperforms zeroth-order and Ô¨Årst-order baselines. Robustness analysis and ablation studies
on hyperparameters and components are also performed to show the properties of ZOAC."
CONCLUSION,0.5130111524163569,"Moreover, our methods achieve such improvement while still using traditional isotropic Gaussian
noise for perturbation, so in principle those improved techniques for ZOO from sampling perspec-
tives can be further integrated, e.g., Monte Carlo sampling techniques, low-dimensional subspace
identiÔ¨Åcation, adaptive perturbation scale, which may lead to even higher performance."
CONCLUSION,0.516728624535316,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY,0.5204460966542751,"7
REPRODUCIBILITY"
REPRODUCIBILITY,0.5241635687732342,"The code of ZOAC will be released after the author notiÔ¨Åcation in https://anonymous.
4open.science/r/Zeroth-Order-Actor-Critic-1A71. We summarize the algorithm
in Appendix A.1 and describe the implementation details in Appendix A.2."
REFERENCES,0.5278810408921933,REFERENCES
REFERENCES,0.5315985130111525,"Marcin Andrychowicz, Anton Raichuk, Piotr Sta¬¥nczyk, Manu Orsini, Sertan Girgin, Rapha¬®el
Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly,
and Olivier Bachem. What matters for on-policy deep actor-critic methods? a large-scale study.
In International Conference on Learning Representations, 2021."
REFERENCES,0.5353159851301115,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016."
REFERENCES,0.5390334572490706,"Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and
empirical comparison of gradient approximations in derivative-free optimization. Foundations of
Computational Mathematics, pp. 1‚Äì54, 2021."
REFERENCES,0.5427509293680297,"Cristian Bodnar, Ben Day, and Pietro Li¬¥o. Proximal distilled evolutionary reinforcement learning. In
Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34, pp. 3283‚Äì3290, 2020."
REFERENCES,0.5464684014869888,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016."
REFERENCES,0.550185873605948,"Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller.
Structured evolution with compact architectures for scalable policy optimization. In International
Conference on Machine Learning, pp. 970‚Äì978. PMLR, 2018."
REFERENCES,0.5539033457249071,"Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, and Vikas Sindhwani.
From complexity to simplicity: Adaptive es-active subspaces for blackbox optimization. Ad-
vances in Neural Information Processing Systems, 32:10299‚Äì10309, 2019."
REFERENCES,0.5576208178438662,"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Os-
band, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles
Blundell, and Shane Legg.
Noisy networks for exploration.
In International Conference on
Learning Representations, 2018."
REFERENCES,0.5613382899628253,"Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation
through the void: Optimizing control variates for black-box gradient estimation. In International
Conference on Learning Representations, 2018."
REFERENCES,0.5650557620817844,"Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen,
Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 International
Conference on Robotics and Automation (ICRA), pp. 8248‚Äì8254. IEEE, 2019."
REFERENCES,0.5687732342007435,"Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning.
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pp. 1196‚Äì1208, 2018."
REFERENCES,0.5724907063197026,"Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O Stanley. Es is more than just a traditional
Ô¨Ånite-difference approximator.
In Proceedings of the Genetic and Evolutionary Computation
Conference, pp. 450‚Äì457, 2018."
REFERENCES,0.5762081784386617,"Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, pp. 3053‚Äì3062. PMLR, 2018."
REFERENCES,0.5799256505576208,"Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015."
REFERENCES,0.5836431226765799,Under review as a conference paper at ICLR 2022
REFERENCES,0.587360594795539,"Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided
evolutionary strategies: Augmenting random search with surrogate gradients. In International
Conference on Machine Learning, pp. 4264‚Äì4273. PMLR, 2019."
REFERENCES,0.5910780669144982,"Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, pp. 1805‚Äì1814, 2018."
REFERENCES,0.5947955390334573,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529‚Äì533, 2015."
REFERENCES,0.5985130111524164,"Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928‚Äì1937. PMLR, 2016."
REFERENCES,0.6022304832713755,"Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,
Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed frame-
work for emerging {AI} applications.
In 13th {USENIX} Symposium on Operating Systems
Design and Implementation ({OSDI} 18), pp. 561‚Äì577, 2018."
REFERENCES,0.6059479553903345,"Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527‚Äì566, 2017."
REFERENCES,0.6096654275092936,"Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
In International Conference on Learning Representations, 2018."
REFERENCES,0.6133828996282528,"Alo¬®ƒ±s Pourchot and Olivier Sigaud. Cem-rl: Combining evolutionary and gradient-based methods
for policy search. arXiv preprint arXiv:1810.01222, 2018."
REFERENCES,0.6171003717472119,"Hong Qian and Yang Yu.
Derivative-free reinforcement learning: A review.
arXiv preprint
arXiv:2102.05710, 2021."
REFERENCES,0.620817843866171,"Antonin RafÔ¨Ån, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor-
mann. Stable baselines3. GitHub repository, 2019."
REFERENCES,0.6245353159851301,"Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017."
REFERENCES,0.6282527881040892,"John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
High-
dimensional continuous control using generalized advantage estimation.
arXiv preprint
arXiv:1506.02438, 2015."
REFERENCES,0.6319702602230484,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
REFERENCES,0.6356877323420075,"Frank Sehnke, Christian Osendorfer, Thomas R¬®uckstie√ü, Alex Graves, Jan Peters, and J¬®urgen
Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551‚Äì559, 2010."
REFERENCES,0.6394052044609665,"Ozan Sener and Vladlen Koltun. Learning to guide random search. In International Conference on
Learning Representations, 2020."
REFERENCES,0.6431226765799256,"Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and
Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training
deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017."
REFERENCES,0.6468401486988847,"Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018."
REFERENCES,0.6505576208178439,"Yunhao Tang, Krzysztof Choromanski, and Alp Kucukelbir. Variance reduction for evolution strate-
gies via structured control variates. In International Conference on ArtiÔ¨Åcial Intelligence and
Statistics, pp. 646‚Äì656. PMLR, 2020."
REFERENCES,0.654275092936803,Under review as a conference paper at ICLR 2022
REFERENCES,0.6579925650557621,"Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026‚Äì5033.
IEEE, 2012."
REFERENCES,0.6617100371747212,"Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229‚Äì256, 1992."
REFERENCES,0.6654275092936803,"Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement
of policy gradient estimation. Advances in Neural Information Processing Systems, 24:262‚Äì270,
2011."
REFERENCES,0.6691449814126395,Under review as a conference paper at ICLR 2022
REFERENCES,0.6728624535315985,"A
APPENDIX"
REFERENCES,0.6765799256505576,"A.1
PSEUDOCODE OF ZOAC"
REFERENCES,0.6802973977695167,Algorithm 1 Zeroth-Order Actor-Critic (ZOAC)
REFERENCES,0.6840148698884758,"1: Initialize: policy parameters Œ∏, critic network parameters w
2: for each iteration do
3:
for each worker i = 1, 2, ..., n do
4:
for j = 0, 1, ..., H ‚àí1 do
5:
Sample œµi,j ‚àºN(0, I)
6:
Run perturbed policy œÄŒ∏+œÉœµi,j in environment for N timesteps"
REFERENCES,0.6877323420074349,"7:
Compute advantage function ÀÜAœÄŒ∏+œÉœµi,j according to Equation (15)
8:
end for
9:
Compute the state-value target ÀÜGt for each state st according to Equation (13)
10:
end for
11:
Collect (s, ÀÜG) for critic update and (œµ, ÀÜA) for actor update
12:
Update w with batch size L through SGD by minimizing Equation (14) for M epoches
13:
Update Œ∏ along the zeroth-order gradient direction estimated in Equation (16)
14: end for"
REFERENCES,0.6914498141263941,"A.2
IMPLEMENTATION DETAILS"
REFERENCES,0.6951672862453532,"We implemented ZOAC with parallelized workers (Algorithm 1) using the distributed framework
Ray (Moritz et al., 2018). We follow the parallelization techniques used in ES (Salimans et al., 2017)
and ARS (Mania et al., 2018). Firstly, we created a shared noise table before training starts, then the
workers communicate indices in the shared table but not the perturbation vectors, so as to avoid high
communication cost. Besides, random seeds for constructing parallelized training environments and
the evaluation environment are different and generated from a single seed designated before hand."
REFERENCES,0.6988847583643123,"We use two different types of policies: linear policies for ARS and ZOAC (linear), neural networks
with (64, 64) hidden nodes and tanh nonlinearities for ES, PPO and ZOAC (neural). For actor-critic
algorithms, we use neural networks with (256, 256) hidden nodes and tanh nonlinearities as critics
to estimate state-value function."
REFERENCES,0.7026022304832714,"Both the zeroth-order baseline methods perform reward shaping to resolve the local optima problem
as described in the original paper: ARS subtracts the survival bonus from rewards (1 in Hopper and
Ant), while ES transforms the episodic returns into rankings. ES further discretize the actions to
encourage exploration in Hopper but we do not reserve this trick for comparison since discretization
will lead to a different policy architecture."
REFERENCES,0.7063197026022305,"We summarize the hyperparameters used in ZOAC in Table 3 and list their values that are used to
produce the results in Figure 2. We tune several important hypermarameters (n, N, H, œÉ) via coarse
grid search and select the best performing setting to produce the Ô¨Ånal results. During evaluation,
exploration noise are turned off and the reported total average return is averaged over 10 episodes.
Table 1 summarizes the maximum value of the total average return within the timestep threshold,
averaged over 5 trials."
REFERENCES,0.7100371747211895,"A.3
DERIVATION OF THE VARIANCE BOUND"
REFERENCES,0.7137546468401487,"Theorem 1. If the reward |r(s, a)| < Œ±, the critic network output |Vw(s)| < Œ≤, and n trajectories
with length of N √ó H timesteps are collected in one iteration, the upper bounds of the variance of
gradient estimators (Equation (9) and (10)) are:"
REFERENCES,0.7174721189591078,Var(‚àáŒ∏ ÀÜJES(Œ∏)) ‚â§(1 ‚àíŒ≥NH)2Œ±2d
REFERENCES,0.7211895910780669,nœÉ2(1 ‚àíŒ≥)2
REFERENCES,0.724907063197026,Var[‚àáŒ∏ ÀÜJZOAC(Œ∏)] ‚â§((1 ‚àíŒ≥N)Œ± + (1 ‚àíŒ≥)(1 + Œ≥N)Œ≤)2d
REFERENCES,0.7286245353159851,nHœÉ2(1 ‚àíŒ≥)2
REFERENCES,0.7323420074349443,Under review as a conference paper at ICLR 2022
REFERENCES,0.7360594795539034,Table 3: Hyperparameters of ZOAC for the learning curves shown in Figure 2
REFERENCES,0.7397769516728625,"Environment
Inv.D.P.-v2
Hopper-v2
HalfCheetah-v2
Ant-v2
Policy type
Linear
Neural
Linear
Neural
Linear
Neural
Linear
Neural
Num. of workers n
4
8
4
8
4
8
8
Rollout length N
10
20
10
20
20
Train frequency H
16
16
32
16
256
Para. noise std. œÉ
0.02
0.04
0.04
0.06
0.02
Batch size L
64
128
Num. of epoches M
8
4
Actor optimizer
Adam(Œ±actor = 0.005, Œ≤1 = 0.9, Œ≤2 = 0.999)
Critic optimizer
Adam(Œ±critic = 0.0003, Œ≤1 = 0.9, Œ≤2 = 0.999)
Discount factor Œ≥
0.99
GAE coeff. Œª
0.95"
REFERENCES,0.7434944237918215,Proof. (1) Variance bound for ES gradient estimators
REFERENCES,0.7472118959107806,"Under the setting described in Section 3.2, the state-value under policy œÄŒ∏+œÉœµ is estimated by the
accumulative return over NH timesteps, which is denoted as ÀÜV œÄŒ∏+œÉœµ
NH
. The isotropic Gaussian noise
added to the policy can be presented as œµ = (œµ1, œµ2, ..., œµd)‚ä§, where œµl ‚àºN(0, 1), l ‚àà{1, 2, ..., d}."
REFERENCES,0.7509293680297398,"Var[ ÀÜV œÄŒ∏+œÉœµ
NH
œµ] ‚â§ d
X"
REFERENCES,0.7546468401486989,"l=1
E[( ÀÜV œÄŒ∏+œÉœµ
NH
œµl)2] = d
X l=1"
REFERENCES,0.758364312267658,"Z
p(œµl) NH
X"
REFERENCES,0.7620817843866171,"t=1
Œ≥t‚àí1r(st, at) !2"
REFERENCES,0.7657992565055762,"œµ2
l dœµl ‚â§ d
X l=1"
REFERENCES,0.7695167286245354,"Z
p(œµl) NH
X"
REFERENCES,0.7732342007434945,"t=1
Œ≥t‚àí1Œ± !2"
REFERENCES,0.7769516728624535,"œµ2
l dœµl"
REFERENCES,0.7806691449814126,= (1 ‚àíŒ≥NH)2Œ±2
REFERENCES,0.7843866171003717,"(1 ‚àíŒ≥)2 d
X l=1"
REFERENCES,0.7881040892193308,"Z
p(œµl)œµ2
l dœµl"
REFERENCES,0.79182156133829,= (1 ‚àíŒ≥NH)2Œ±2
REFERENCES,0.7955390334572491,"(1 ‚àíŒ≥)2 d
X"
REFERENCES,0.7992565055762082,"l=1
Eœµl‚àºN(0,1)œµ2
l"
REFERENCES,0.8029739776951673,= (1 ‚àíŒ≥NH)2Œ±2d
REFERENCES,0.8066914498141264,(1 ‚àíŒ≥)2
REFERENCES,0.8104089219330854,"The last equality holds because œµ2
l ‚àºœá2(1) when œµl ‚àºN(0, 1), and E[œµ2
l ] = 1 for all l. Since n
random directions is sampled and evaluated, the ES gradient estimator is given according to Equation
(9):"
REFERENCES,0.8141263940520446,"‚àáŒ∏ ÀÜJES(Œ∏) = 1 nœÉ n
X"
REFERENCES,0.8178438661710037,"i=1
ÀÜV
œÄŒ∏+œÉœµi
NH
œµi"
REFERENCES,0.8215613382899628,Therefore the variance bound for ES can be derived as in Theorem 1:
REFERENCES,0.8252788104089219,"Var[‚àáŒ∏ ÀÜJES(Œ∏)] =
1
nœÉ2 Var[ ÀÜV œÄŒ∏+œÉœµ
NH
œµ]"
REFERENCES,0.828996282527881,‚â§(1 ‚àíŒ≥NH)2Œ±2d
REFERENCES,0.8327137546468402,nœÉ2(1 ‚àíŒ≥)2
REFERENCES,0.8364312267657993,(2) Variance bound for ZOAC gradient estimators
REFERENCES,0.8401486988847584,"Under the setting described in Section 3.2, the performance under policy œÄŒ∏+œÉœµ is estimated by the
N-step TD residual, which is denoted as ÀÜAœÄŒ∏+œÉœµ
N
. The isotropic Gaussian noise œµ is added to the
policy as well."
REFERENCES,0.8438661710037175,Under review as a conference paper at ICLR 2022
REFERENCES,0.8475836431226765,"Var[ ÀÜAœÄŒ∏+œÉœµ
N
œµ] ‚â§ d
X"
REFERENCES,0.8513011152416357,"l=1
E[( ÀÜAœÄŒ∏+œÉœµ
N
œµl)2] = d
X l=1"
REFERENCES,0.8550185873605948,"Z
p(œµl) N
X"
REFERENCES,0.8587360594795539,"t=1
Œ≥t‚àí1r(st, at) + Œ≥NVw(st+N) ‚àíVw(st) !2"
REFERENCES,0.862453531598513,"œµ2
l dœµl ‚â§ d
X l=1"
REFERENCES,0.8661710037174721,"Z
p(œµl) N
X"
REFERENCES,0.8698884758364313,"t=1
Œ≥t‚àí1Œ± + (1 + Œ≥N)Œ≤ !2"
REFERENCES,0.8736059479553904,"œµ2
l dœµl"
REFERENCES,0.8773234200743495,"=
(1 ‚àíŒ≥N)Œ± + (1 ‚àíŒ≥)(1 + Œ≥N)Œ≤"
REFERENCES,0.8810408921933085,(1 ‚àíŒ≥)
REFERENCES,0.8847583643122676,"2
d
X l=1"
REFERENCES,0.8884758364312267,"Z
p(œµl)œµ2
l dœµl"
REFERENCES,0.8921933085501859,"=
(1 ‚àíŒ≥N)Œ± + (1 ‚àíŒ≥)(1 + Œ≥N)Œ≤"
REFERENCES,0.895910780669145,"(1 ‚àíŒ≥) 2
d"
REFERENCES,0.8996282527881041,"Totally n √ó H random directions is sampled and evaluated, and the ZOAC gradient estimator is
given according to Equation (10):"
REFERENCES,0.9033457249070632,"‚àáŒ∏ ÀÜJZOAC(Œ∏) ‚âà
1
nHœÉ nH
X"
REFERENCES,0.9070631970260223,"i=1
ÀÜA
œÄŒ∏+œÉœµi
N
œµi"
REFERENCES,0.9107806691449815,Therefore the variance bound for ZOAC can be derived:
REFERENCES,0.9144981412639405,"Var[‚àáŒ∏ ÀÜJZOAC(Œ∏)] =
1
nHœÉ2 Var[ ÀÜAœÄŒ∏+œÉœµ
N
œµ]"
REFERENCES,0.9182156133828996,‚â§((1 ‚àíŒ≥N)Œ± + (1 ‚àíŒ≥)(1 + Œ≥N)Œ≤)2d
REFERENCES,0.9219330855018587,nHœÉ2(1 ‚àíŒ≥)2
REFERENCES,0.9256505576208178,"A.4
STATE-VALUE FUNCTION ESTIMATION"
REFERENCES,0.929368029739777,"Figure 6: Average state-value estimation difference Vw(s) ‚àíV œÄŒ∏(s) in evaluation during training
in Figure 2. The solid lines correspond to the mean and the shaded regions to the 95% conÔ¨Ådence
interval over 5 trials using a Ô¨Åxed set of random seeds. All curves are smoothed uniformly for visual
clarity."
REFERENCES,0.9330855018587361,"We plot the average state-value estimation difference Vw(s) ‚àíV œÄŒ∏(s) in evaluation during training
in Figure 6. Since we turn off the exploration noise for evaluation, which means that the trajectories
are collected under the deterministic policy œÄŒ∏, the discounted sum of reward-to-go can be regarded
as an estimate of the true state-value."
REFERENCES,0.9368029739776952,"Results show that the critic networks converge, but in most cases to an underestimated value. This is
because the critic network is trained to Ô¨Åt the state-value function V Œ≤(s) of the stochastic exploration
policy Œ≤ rather than V œÄŒ∏(s) of the deterministic policy œÄŒ∏. The underestimate bias vary in different
tasks and when using different forms of policies, which is related to the local shape of the optima
found by the RL agent. However, due to the objective function used in ZOAC, intuitively, the agent
tend to Ô¨Ånd wide optima during training, which Ô¨Ånally result in more robust policies."
REFERENCES,0.9405204460966543,Under review as a conference paper at ICLR 2022
REFERENCES,0.9442379182156134,"A.5
ADDITIONAL RESULTS OF ROBUSTNESS COMPARISON"
REFERENCES,0.9479553903345725,"Table 4 shows the result of robustness comparison in all tested MuJoCo environments. Results show
that in general neural policies are more robust, which can be ascribed to their strong expressive
ability. InvertedDoublePendulum-v2 is the exception, in which the robustness of the neural network
policies is no where near the linear ones, no matter what RL algorithms is used. Since this environ-
ment is the simplest one among these environments, we guess that this is due to the overÔ¨Åtting of
neural networks, and that a matrix is enough to represent an optimal policy."
REFERENCES,0.9516728624535316,"When comparing between linear policies, the policies learned by ZOAC yield higher robustness to
both observation noise and parameter noise in all environments, compared to those learned by ARS.
One possible reason is that ARS subtracts the survival bonus from rewards, which actually alter the
objective function being optimized, as described in Section 4.1. As for neural policies, the policies
learned by ZOAC are also shown to be more robust to both observation noise and parameter noise.
One reason is that parameter noise used in zeroth-order methods encourages the agent to Ô¨Ånd a
wide optima which is robust to parameter perturbations, while gradient-based methods focus on the
performance of a particular point. Besides, ZOAC perform timestep-wise perturbation rather than
episode-wise perturbation, which explores along more random directions and collects more diverse
trajectories (i.e., data) than ARS or ES. All these features Ô¨Ånally lead to better generalization and
robustness of the policies learned by ZOAC."
REFERENCES,0.9553903345724907,"Table 4: Robustness comparison of the learned policies. Best performing policies from Figure 2
are tested (4e7 timestep limitation for ARS and ES in HalfCheetah-v2 and Ant-v2 till convergence).
Each policy is evaluated on 400 trajectories in the same environment (using the same random seed
that has never been used during training) and the average return over 5 policies are listed."
REFERENCES,0.9591078066914498,"Env.
Noise type
No extra noise
Obs. noise (œÉ = 0.1)
Para. noise (œÉ = 0.05)
ZOAC(Linear)
8744.11
8615.50 (-1.5%)
3643.49 (-58.3%)
ZOAC(Neural)
8419.16
908.95 (-89.2%)
1344.54 (-84.0%)
Inv.D.P.
ARS(Linear)
8883.13
8339.77 (-6.1%)
3396.74 (-61.8%)
ES(Neural)
7333.34
3593.76 (-51.0%)
927.02 (-87.4%)
PPO(Neural)
7477.48
3537.34 (-52.7%)
3115.82 (-58.3%)
ZOAC(Linear)
2885.10
2733.24 (-5.3%)
1872.07 (-35.1%)
ZOAC(Neural)
2417.33
2284.12 (-5.5%)
1644.34 (-32.0%)
Hopper
ARS(Linear)
2587.06
1194.43 (-53.8%)
781.99 (-69.8%)
ES(Neural)
1443.77
1330.98 (-7.8%)
915.38 (-36.6%)
PPO(Neural)
2596.02
2185.02 (-15.8%)
395.80 (-84.8%)
ZOAC(Linear)
4909.53
4402.54 (-10.3%)
3289.15 (-33.0%)
ZOAC(Neural)
5179.39
5057.07 (-2.4%)
4247.08 (-18.0%)
HalfC.
ARS(Linear)
4529.30
1771.23 (-60.9%)
1724.40 (-61.9%)
ES(Neural)
5478.01
4550.22 (-16.9%)
1527.93 (-72.1%)
PPO(Neural)
4668.55
3342.23 (-28.4%)
972.08 (-79.2%)
ZOAC(Linear)
4134.79
3460.67 (-16.3%)
-2901.51 (-170.2%)
ZOAC(Neural)
4013.57
3650.79 (-9.0%)
976.57 (-75.6%)
Ant
ARS(Linear)
3749.73
2868.18 (-23.5%)
-4269.38 (-213.9%)
ES(Neural)
4029.82
3902.70 (-3.2%)
970.30 (-75.9%)
PPO(Neural)
3103.79
2792.79 (-10.0%)
393.20 (-87.3%)"
REFERENCES,0.9628252788104089,"A.6
PERFORMANCE COMPARISON OF DIFFERENT POLICY PARAMETERIZATIONS"
REFERENCES,0.966542750929368,"The derivative-free nature of ZOAC allows us to estimate the zeroth-order policy gradient to im-
prove the policy without considering the speciÔ¨Åc policy architecture. Hence, ZOAC can be applied
seamlessly to arbitrary parameterized policies in theory."
REFERENCES,0.9702602230483272,"We further apply ZOAC on two more different policies and conduct additional experiments on the
same environments with four different policies, from fewer parameters to more parameters: Toeplitz
matrix, matrix, network with (64, 64) units, network with (128, 128) units, and to see the perfor-
mance of ZOAC when optimizing these policies. We listed the dimension of the parameter space of
different policies in Table 5."
REFERENCES,0.9739776951672863,Under review as a conference paper at ICLR 2022
REFERENCES,0.9776951672862454,"Toeplitz matrix is a kind of compact policies with parameter sharing schemes, each element depends
only on the difference between the row index and the column index. A general dense matrix M ‚àà
Rm√ón has m √ó n parameters, while a Toeplitz matrix T ‚ààRm√ón has only m + n ‚àí1 parameters.
Since a general dense matrix performs quite well in all tested environments, we wonder whether a
more compact policy still work. Besides, as derived in Theorem 1, the variance bound of zeroth-
order gradient increases proportional to the dimension of parameter space, and it is a common view
that zeroth-order methods are more suitable for low-dimensional problems. We wonder whether a
larger network will improve or harm the performance."
REFERENCES,0.9814126394052045,"We use the same set of hyperparameters for linear policies, and another set for networks, as summa-
rized in Table 3. Results in Figure 7 show that a Toeplitz matrix can obtain average return around
2000 in challenging environments like HalfCheetah-v2 and Ant-v2 with only very few parameters.
However, it is far inferior to a general dense matrix, indicating that this type of policy is not sufÔ¨Å-
cient to represent an optimal policy. Network with (64, 64) and (128, 128) hidden nodes performs
quite similarly to each other."
REFERENCES,0.9851301115241635,"The additional results demonstrate the wide adaptability of ZOAC to different forms of policies.
Zeroth-order policy update makes it very useful when gradient information is hard to obtain or even
unavailable, like low precision neural networks, hierarchical policies, or even rule-based controllers.
In the future, we may apply ZOAC to speciÔ¨Åc problems where Ô¨Årst-order methods can not handle
and to improve existing results where the policies are trained with traditional zeroth-order methods
like ES and ARS."
REFERENCES,0.9888475836431226,"Table 5: Dimension of the state space, action space and parameter space of different policies."
REFERENCES,0.9925650557620818,"Environment
Inv.Dou.Pen.-v2
Hopper-v2
HalfCheetah-v2
Ant-v2
State space
11
11
17
111
Action space
1
3
6
8
Toeplitz matrix
11
13
22
118
Matrix
11
33
102
888
Network (64, 64)
4993
5123
5702
11848
Network (128, 128)
18177
18435
19590
31880"
REFERENCES,0.9962825278810409,"Figure 7: Learning curves on MuJoCo benchmarks with different policies. The solid lines corre-
spond to the mean and the shaded regions to the 95% conÔ¨Ådence interval over 5 trials using a Ô¨Åxed
set of random seeds. All curves are smoothed uniformly for visual clarity."
