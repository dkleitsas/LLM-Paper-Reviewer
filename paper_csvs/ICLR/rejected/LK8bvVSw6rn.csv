Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002012072434607646,"When deployed for risk-sensitive tasks, deep neural networks (DNNs) must
be equipped with an uncertainty estimation mechanism.
This paper stud-
ies the relationship between deep architectures and their training regimes with
their corresponding uncertainty estimation performance. We consider both in-
distribution uncertainties (“aleatoric” or “epistemic”) and class-out-of-distribution
ones. Moreover, we consider some of the most popular estimation performance
metrics previously proposed including AUROC, ECE, AURC, and coverage for
selective accuracy constraint. We present a novel and comprehensive study carried
out by evaluating the uncertainty performance of 484 deep ImageNet classiﬁcation
models. We identify numerous and previously unknown factors that affect uncer-
tainty estimation and examine the relationships between the different metrics. We
ﬁnd that distillation-based training regimes consistently yield better uncertainty
estimations than other training schemes such as vanilla training, pretraining on a
larger dataset and adversarial training. We also provide strong empirical evidence
showing that ViT is by far the most superior architecture in terms of uncertainty
estimation performance, judging by any aspect, in both in-distribution and class-
out-of-distribution scenarios. We learn various interesting facts along the way.
Contrary to previous work, ECE does not necessarily worsen with an increase in
the number of network parameters. Likewise, we discovered an unprecedented
99% top-1 selective accuracy at 47% coverage (and 95% top-1 accuracy at 80%)
for a ViT model, whereas a competing EfﬁcientNet-V2-XL cannot obtain these
accuracy constraints at any level of coverage."
INTRODUCTION,0.004024144869215292,"1
INTRODUCTION"
INTRODUCTION,0.006036217303822937,"Deep neural networks (DNNs) show great performance in a wide variety of application domains
including computer vision, natural language understanding and audio processing. Successful de-
ployment of these models, however, is critically dependent on providing an effective uncertainty
estimation of their predictions in the form of some kind of selective prediction or providing a prob-
abilistic conﬁdence score for their predictions."
INTRODUCTION,0.008048289738430584,"But how should we evaluate the performance of uncertainty estimation? Let us consider two classiﬁ-
cation models for the stock market that predict whether a stock’s value is about to increase, decrease
or remain neutral (three-class classiﬁcation). Suppose that model A has a 95% true accuracy, and
generates a conﬁdence score of 0.95 on every prediction (even on misclassiﬁed instances); model
B has a 40% true accuracy, but always gives a conﬁdence score of 0.6 on correct predictions, and
0.4 on incorrect ones. Model B can be utilized easily to generate perfect investment decisions. Us-
ing selective prediction (Geifman & El-Yaniv, 2017), Model B will reject all investments on stocks
whenever the conﬁdence score is 0.4. While model A offers many more investment opportunities,
each of its predictions carries a 5% risk of failure."
INTRODUCTION,0.01006036217303823,"Among the various metrics proposed for evaluating the performance of uncertainty estimation
are: Area Under the Receiver Operating Characteristic (AUROC or AUC), Area Under the Risk-
Coverage curve (AURC) (Geifman et al., 2018), selective risk or coverage for a selective accuracy
constraint (SAC), Negative Log-likelihood (NLL), Expected Calibration Error (ECE), which is of-"
INTRODUCTION,0.012072434607645875,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.014084507042253521,"2
3
4
5
6
7 78 80 82 84 86 88"
INTRODUCTION,0.01609657947686117,Models
INTRODUCTION,0.018108651911468814,"ResMLP
ResMLP distilled 
ResNet
ViT
ViT distilled
ViT SAM
XCiT
XCiT distilled"
INTRODUCTION,0.02012072434607646,-log(ECE) AUROC
INTRODUCTION,0.022132796780684104,"Various
AlexNet
BiT
BiT distilled 
EfficientNetV2 
GENet
MLP Mixer
MLP Mixer distilled
RegNetY"
INTRODUCTION,0.02414486921529175,"Figure 1: A comparison of 484 models by their AUROC (×100, higher is better) and -log(ECE)
(higher is better) on ImageNet. Each marker’s size is determined by the model’s number of param-
eters. A full version graph is given in Figure 8. Distilled models are better than non-distilled ones.
ViT models are naturally better at all aspects of uncertainty estimation, while EfﬁcientNet-V2 and
GENet models are worse."
INTRODUCTION,0.026156941649899398,"ten used for evaluating a model’s calibration (see Section 2) and Brier score (Brier, 1950). All these
metrics are well known and are often used for comparing the uncertainty estimation performance of
models (Moon et al., 2020; Nado et al., 2021; Maddox et al., 2019; Lakshminarayanan et al., 2017).
Somewhat surprisingly, NLL, Brier, AURC, and ECE all fail to reveal the uncertainty superiority of
Model B in our investment example (see Appendix A for the calculations). Both AUROC and SAC,
on the other hand, reveal the advantage of Model B perfectly (see Appendix A for details). It is not
hard to construct counter examples where these two metrics fails and others (e.g., ECE) succeed.
The risk-coverage (RC) curve (El-Yaniv & Wiener, 2010) is perhaps one of the most informative
and practical representations of the overall uncertainty proﬁle of a given model."
INTRODUCTION,0.028169014084507043,"In general, though, two RC curves are not necessarily comparable if one does not fully dominate the
other (see Figure 2). The advantage of scalar metrics such as the above is that they summarize the
model’s overall uncertainty estimation behavior by reducing it to a single scalar. When not carefully
chosen, however, these reductions could result in a loss of vital information about the problem (for
example, reducing an RC curve to an AURC does not show that Model B has an optimal 0 risk if
the coverage is smaller than 0.4). Thus, the choice of the “correct” single scalar performance metric
unfortunately must be task-speciﬁc. When comparing the uncertainty estimation performance of
deep architectures that exhibit different accuracies, we ﬁnd that AUROC and SAC can effectively
“normalize” accuracy differences that plague the usefulness of other metrics (see Section 2). This
normalization is essential to our study where we compare uncertainty performance of hundreds of
models that can greatly differ in their accuracies."
INTRODUCTION,0.030181086519114688,"In applications where risk (or coverage) constraints are dictated (Geifman & El-Yaniv, 2017), the
most straightforward and natural metric is the SAC (or selective risk), which directly measures the
coverage (resp., risk) given at the required level of risk (resp., coverage) constraint. We demonstrate
this in Appendix J, evaluating which models give the most coverage for a SAC of 99%. Sometimes,
however, such constraints are unknown in advance, or even irrelevant, e.g., the constructed model
should serve a variety of risk constraint use cases, or the model may not be allowed to abstain from
predicting at all."
INTRODUCTION,0.03219315895372234,"In this paper we conduct a comprehensive study of DNNs’ ability to estimate uncertainty by eval-
uating 484 models pretrained on ImageNet (Deng et al., 2009), taken from the PyTorch and timm
respositories (Paszke et al., 2019; Wightman, 2019). We identify the main factors contributing to
or harming the conﬁdence ranking of predictions (“ranking” for short), calibration and selective
prediction. Furthermore, we also consider the source of uncertainty as either internal (stemming
from either the aleatoric or epistemic uncertainty of the model (Kiureghian & Ditlevsen, 2009)) or
external (originating from unseen or unknown class-out-of-distribution (C-OOD) data) and evaluate"
INTRODUCTION,0.03420523138832998,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.03621730382293763,"these models in multiple ways. After ﬁrst evaluating models solely on in-distribution (ID) data, we
then deﬁne and test two ways of evaluating C-OOD data, each of which also divides the data into
different groups by how difﬁcult it is for the model to distinguish instances as external."
INTRODUCTION,0.03822937625754527,"Our study lead to quite a few new observations and conclusions; (1) Training regimes incorporating
any kind of knowledge distillation (KD) (Hinton et al., 2015) leads to DNNs with improved uncer-
tainty estimation performance evaluated by any metric, in both internal and external settings (i.e.,
leading also to better C-OOD detection), more than by using any other training tricks (such as pre-
training on a larger dataset, adversarial training, etc.). (2) Some architectures are naturally superb at
all aspects of uncertainty estimation and in all settings, e.g., vision transformers (ViTs) (Dosovitskiy
et al., 2020; Steiner et al., 2021), while other architectures tend to perform worse, e.g., EfﬁcientNet-
V2 and GENet (Tan & Le, 2021; Lin et al., 2020). These results are visualized in Figure 1. (3) The
superiority of ViTs remains even when the comparison considers the models’ sizes—meaning that
for any size, ViTs outperform the competition in uncertainty estimation performance, as visualized
in Appendix B in Figures 9 and 10. (4) The simple post-training calibration method of temperature
scaling (Guo et al., 2017), which is known to improve ECE, for the most part also improves ranking
(AUROC) and selective prediction—meaning not only does it calibrate the probabilistic estimation
for each individual instance, but it also improves the partial order of all instances induced by those
improved estimations, pushing instances more likely to be correct to have higher conﬁdence than
instances less likely to be correct (see Section 3). (5) Contrary to previous work by Guo et al. (2017),
we observe that while there is a strong correlation between accuracy/number of parameters and ECE
or AUROC within each speciﬁc family of models of the same architecture, the correlation ﬂips be-
tween a strong negative and a strong positive correlation depending on the type of architecture being
observed. For example, as ViT architectures increase in size and accuracy, their ECE deteriorates
while their AUROC improves. The exact opposite, however, could be observed in XCiTs (El-Nouby
et al., 2021) as their ECE improves with size while their AUROC deteriorates. (see Appendix G).
(6) The best model in terms of AUROC or SAC is not always the best in terms of calibration, as
illustrated in Figure 1, and the trade-off should be considered when choosing a model based on
its application. Due to lack of space, a number of additional interesting observations are brieﬂy
mentioned in the paper without supporting empirical evidence (which is provided in the appendix)."
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE,0.04024144869215292,"2
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE"
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE,0.04225352112676056,"Let X be the input space and Y be the label space. Let P(X, Y) be an unknown distribution over
X × Y. A model f is a prediction function f : X →Y, and its predicted label for an image
x is denoted by ˆyf(x). The model’s true risk w.r.t. P is R(f|P) = EP (X,Y)[ℓ(f(x), y)], where
ℓ: Y × Y →R+ is a given loss function, for example, 0/1 loss for classiﬁcation. Given a labeled
set Sm = {(xi, yi)}m
i=1 ⊆(X × Y), sampled i.i.d. from P(X, Y), the empirical risk of model f is
ˆr(f|Sm) ≜
1
m
Pm
i=1 ℓ(f(xi), yi). Following Geifman et al. (2018), for a given model f we deﬁne
a conﬁdence score function κ(x, ˆy|f), where x ∈X, and ˆy ∈Y is the model’s prediction for x, as
follows. The function κ should quantify conﬁdence in the prediction of ˆy for the input x, based on
signals from model f. This function should induce a partial order over instances in X, and is not
required to distinguish between points with the same score."
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE,0.04426559356136821,"The most common and well-known κ function for a classiﬁcation model f (with softmax at its last
layer) is its softmax response values: κ(x, ˆy|f) ≜f(x)ˆy (Cordella et al., 1995; De Stefano et al.,
2000). While this is the main κ we evaluate, we also test the popular uncertainty estimation tech-
nique of Monte-Carlo dropout (MC-Dropout) (Gal & Ghahramani, 2016), which is motivated by
Bayesian reasoning. Although these methods use the direct output from f, κ could be a different
model unrelated to f and unable to affect f’s predictions. Note that to enable a probabilistic in-
terpretation, κ can only be calibrated if its values reside in [0, 1] whereas for ranking and selective
prediction any value in R can be used."
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE,0.04627766599597585,"A selective model f (El-Yaniv & Wiener, 2010; Chow, 1957) uses a selection function g : X →
{0, 1} to serve as a binary selector for f, enabling it to abstain from giving predictions for certain
inputs. g can be deﬁned by a threshold θ on the values of a κ function such that gθ(x|κ, f) =
1[κ(x, ˆyf(x)|f) > θ]. The performance of a selective model is measured using coverage and risk,
where coverage, deﬁned as φ(f, g) = EP [g(x)], is the probability mass of the non-rejected instances
in X. The selective risk of the selective model (f, g) is deﬁned as R(f, g) ≜EP [ℓ(f(x),y)g(x)]"
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE,0.0482897384305835,"φ(f,g)
. These"
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE,0.05030181086519115,Under review as a conference paper at ICLR 2022
HOW TO EVALUATE DEEP UNCERTAINTY ESTIMATION PERFORMANCE,0.052313883299798795,"quantities can be evaluated empirically over a ﬁnite labeled set Sm, with the empirical coverage
deﬁned as ˆφ(f, g|Sm) =
1
m
Pm
i=1 g(xi), and the empirical selective risk deﬁned as ˆr(f, g|Sm) ≜"
"M
PM",0.05432595573440644,"1
m
Pm
i=1 ℓ(f(xi),yi)g(xi)"
"M
PM",0.056338028169014086,"ˆφ(f,g|Sm)
. Similarly, SAC is deﬁned as the largest coverage available for a speciﬁc
accuracy constraint. A way to visually inspect the behavior of a κ function for selective prediction
can be done using an RC curve—a curve showing the selective risk as a function of coverage,
measured on some chosen test set; see Figure 2 for an example."
"M
PM",0.05835010060362173,"䄀唀刀伀䌀㴀㜀㜀⸀㜀㜀
䄀唀刀䌀㴀㘀㠀⸀㐀㔀"
"M
PM",0.060362173038229376,"䄀唀刀伀䌀㴀㠀㜀⸀㔀㈀
䄀唀刀䌀㴀㜀㘀⸀㄀㠀"
"M
PM",0.06237424547283702,吀漀瀀ⴀ㄀ 䤀洀愀最攀一攀琀 䔀爀爀漀爀 ⠀刀椀猀欀⤀
"M
PM",0.06438631790744467,䌀漀瘀攀爀愀最攀
"M
PM",0.06639839034205232,"刀椀猀欀ⴀ䌀漀瘀攀爀愀最攀 䌀甀爀瘀攀 䌀漀洀瀀愀爀椀猀漀渀
䄀挀挀甀爀愀挀礀 㴀 　⸀㜀㐀
䔀爀爀漀爀 刀愀琀攀 㴀 　⸀㈀㘀"
"M
PM",0.06841046277665996,"䄀挀挀甀爀愀挀礀 㴀 　⸀㠀㘀
䔀爀爀漀爀 刀愀琀攀㴀 　⸀㄀㐀"
"M
PM",0.07042253521126761,"䄀挀挀甀爀愀挀礀 㴀 　⸀㠀㜀
䔀爀爀漀爀 刀愀琀攀㴀 　⸀㄀㌀"
"M
PM",0.07243460764587525,"䄀唀刀伀䌀㴀㠀㠀⸀㐀㜀
䄀唀刀䌀㴀㈀㘀⸀㐀㠀"
"M
PM",0.0744466800804829,䈀椀最最攀爀 䄀唀刀伀䌀 ⠀砀㄀　　⤀ 椀猀 戀攀琀琀攀爀
"M
PM",0.07645875251509054,匀洀愀氀氀攀爀 䄀唀刀䌀 ⠀砀㄀　　　⤀ 椀猀 戀攀琀琀攀爀
"M
PM",0.07847082494969819,"Figure 2: A comparison of RC-curves made by the best (ViT-L/16-384) and worst (EfﬁcientNet-
V2-XL) models we evaluated in terms of AUROC. Comparing ViT-B/32-SAM to EfﬁcientNet-V2
exempliﬁes the fact that neither accuracy nor AURC reﬂect selective performance well enough."
"M
PM",0.08048289738430583,"The AURC and E-AURC metrics were deﬁned by Geifman et al. (2018) for quantifying the selective
quality of κ functions via a single number, with AURC being deﬁned as the area under the RC
curve. AURC, however, is very sensitive to the model’s accuracy, and in an attempt to mitigate this,
E-AURC was suggested. The latter also suffers from sensitivity to accuracy, as we demonstrate in
Appendix C. Let us consider the two models in Figure 2 for risk-sensitive deployment; EfﬁcientNet-
V2-XL (Tan & Le, 2021) and ViT-B/32-SAM (Chen et al., 2021a). While the former model has
better overall accuracy and AURC (metrics that could lead us to believe the model is best for our
needs), it cannot guarantee a Top-1 ImageNet selective accuracy above 95% for any coverage. ViT-
B/32-SAM, on the other hand, can provide accuracies above 95% for all coverages below 50%."
"M
PM",0.08249496981891348,"When there are requirements for speciﬁc coverages, the most direct metric to utilize would be the
matching selective risks, by which we can select the model offering the best performance for our
task. If instead a speciﬁc range of coverages is speciﬁed, we could measure the area under the RC
curve for those coverages: AURCC(κ, f|Sm) =
1
|C|
P"
"M
PM",0.08450704225352113,"c∈C
ˆr(f, gc|Sm), with C being those required"
"M
PM",0.08651911468812877,"coverages. Lastly, if a certain accuracy constraint is speciﬁed, the chosen model should be the one
providing the largest coverage for that constraint (the largest coverage for a certain SAC)."
"M
PM",0.08853118712273642,"Often, these requirements are not known or can change as a result of changing circumstances or indi-
vidual needs. Also, using metrics sensitive to accuracy such as AURC makes designing architectures
and methods to improve κ very hard, since an improvement in these metrics could be attributed to
either an increase in overall accuracy (if such occurred) or to a real improvement in the model’s
“metacognition”. Lastly, some tasks might not allow the model to abstain from making predictions
at all, but instead require interpretable and well-calibrated probabilities of correctness, which could
be measured using ECE."
MEASURING RANKING AND CALIBRATION,0.09054325955734406,"2.1
MEASURING RANKING AND CALIBRATION"
MEASURING RANKING AND CALIBRATION,0.0925553319919517,"A κ function is not necessarily able to change the model’s predictions. Thus, its means for improv-
ing the selective risk is by ranking correct and incorrect predictions better, inducing a more accurate
partial order over instances in X. Thus, for every two random samples (x1, y1), (x2, y2) ∼P(X, Y)"
MEASURING RANKING AND CALIBRATION,0.09456740442655935,Under review as a conference paper at ICLR 2022
MEASURING RANKING AND CALIBRATION,0.096579476861167,"and given that ℓ(f(x1), y1) > ℓ(f(x2), y2), the ranking performance of κ is deﬁned as the proba-
bility that κ ranks x2 higher than x1:"
MEASURING RANKING AND CALIBRATION,0.09859154929577464,"Pr[κ(x1, ˆy|f) < κ(x2, ˆy|f)|ℓ(f(x1), y1) > ℓ(f(x2), y2)]
(1)"
MEASURING RANKING AND CALIBRATION,0.1006036217303823,"We discuss this deﬁnition in greater detail in Appendix D. The AUROC metric is often used in the
ﬁeld of machine learning. When the 0/1 loss is in play, it is known that AUROC in fact equals the
probability in Equation (1) (Fawcett, 2006) and thus is a proper metric to measure ranking in clas-
siﬁcation (AKA discrimination). AUROC is furthermore equivalent to the Goodman and Kruskal’s
γ-correlation Goodman & Kruskal (1954), which for decades has been extensively used to measure
ranking (known as “resolution”) in the ﬁeld of metacognition Nelson (1984). The precise relation-
ship between γ-correlation and AUROC is γ = 2 · AUROC −1 (Higham & Higham, 2018). We
note also that both the γ-correlation and AUROC are nearly identical or closely related to various
other correlations and metrics; γ-correlation (AUROC) becomes identical to Kendall’s τ (up to a
linear transformation) in the absence of tied values. both metrics are also closely related to rank-
biserial correlation, the Gini coefﬁcient (not to be confused with the measure from economics) and
the Mann–Whitney U test, hinting at their importance and usefulness in a variety of ﬁelds and set-
tings. In Appendix E, we brieﬂy compare the ranking performance of neural networks and humans
based on metacognitive research and address a criticism of using AUROC to measure ranking in
Appendix F"
MEASURING RANKING AND CALIBRATION,0.10261569416498995,"The most widely used metric for calibration is ECE (Naeini et al., 2015). For a ﬁnite test set of size
N, ECE is calculated by grouping all instances into m interval bins (such that m ≪N), each of
size
1
m (the conﬁdence interval of bin Bj is ( j−1 m , j"
MEASURING RANKING AND CALIBRATION,0.10462776659959759,"m]). With acc(Bj) being the mean accuracy in
bin Bj and conf(Bj) being its mean conﬁdence, ECE is deﬁned as ECE = m
X j=1 |Bj| N X i∈Bj"
MEASURING RANKING AND CALIBRATION,0.10663983903420524,1[ˆyf(xi) = yi]
MEASURING RANKING AND CALIBRATION,0.10865191146881288,"|Bj|
−κ(x, ˆyf(xi)|f) |Bj| = m
X j=1 |Bj| N X"
MEASURING RANKING AND CALIBRATION,0.11066398390342053,"i∈Bj
|acc(Bj) −conf(Bj)|"
MEASURING RANKING AND CALIBRATION,0.11267605633802817,"Since ECE is widely accepted we use it here to evaluate calibration, and follow Guo et al. (2017)
in setting the number of bins to m = 15. Many alternatives to ECE exist, to allow an adaptive
binning scheme or to evaluate the calibration on the non-chosen labels as well (Nixon et al., 2019;
Vaicenavicius et al., 2019). Relevant to our objective is that by using binning, this metric is not
affected by the overall accuracy as is the Brier score, for example."
IN-DISTRIBUTION ANALYSIS,0.11468812877263582,"3
IN-DISTRIBUTION ANALYSIS"
IN-DISTRIBUTION ANALYSIS,0.11670020120724346,"While AUROC and ECE are (negatively) correlated (they have a Spearman correlation of -0.5, mean-
ing that generally as AUROC improves so does ECE), their agreement on the best performing model
depends greatly on the architectural family in question. For example, the Spearman correlation be-
tween the two metrics evaluated on 28 undistilled XCiTs is 0.76 (meaning ECE deteriorates as
AUROC improves), while for the 33 ResNets (He et al., 2015) evaluated, the correlation is -0.74.
Another general observation is that, contrary to previous work by Guo et al. (2017) concerning ECE,
the correlations between AUROC or ECE and the accuracy or the number of model parameters are
nearly zero, although each family tends to have a strong correlation, either negative or positive. We
include a family-based comparison in Appendix G for correlations between AUROC/ECE and accu-
racy, number of parameters and input size. These results suggest that while some architectures might
utilize extra resources to achieve improved uncertainty estimation capabilities, other architectures
do not and are even harmed in this respect."
IN-DISTRIBUTION ANALYSIS,0.11871227364185111,"We evaluated several training regimes: (1) Training that involves knowledge distillation in any form,
including transformer-speciﬁc distillation (Touvron et al., 2020), knapsack pruning with distillation
(in which the teacher is the original unpruned model) (Aﬂalo et al., 2020) and a pretraining technique
which employs distillation (Ridnik et al., 2021); (2) adversarial training (Xie et al., 2019a; Tram`er
et al., 2018); (3) pretraining on ImageNet21k (“pure”, with no additions) (Tan & Le, 2021; Touvron
et al., 2021a); and (4) various forms of weakly or semi-supervised learning (Mahajan et al., 2018b;
Yalniz et al., 2019; Xie et al., 2019b). Of these methods, training methods incorporating distillation
improve AUROC and ECE the most (see Figures 3 and 4). Moreover, distillation seems to greatly
improve both metrics even when the teacher itself is much worse at both metrics. We discuss these
effects in greater detail in Appendix H."
IN-DISTRIBUTION ANALYSIS,0.12072434607645875,Under review as a conference paper at ICLR 2022
IN-DISTRIBUTION ANALYSIS,0.1227364185110664,Adversarial Training
IN-DISTRIBUTION ANALYSIS,0.12474849094567404,Pretraining on ImageNet21k
IN-DISTRIBUTION ANALYSIS,0.1267605633802817,Distillation
IN-DISTRIBUTION ANALYSIS,0.12877263581488935,Temperature Scaling
IN-DISTRIBUTION ANALYSIS,0.13078470824949698,MC-Dropout −3 −2 −1 0 1 2 3
IN-DISTRIBUTION ANALYSIS,0.13279678068410464,Method
IN-DISTRIBUTION ANALYSIS,0.13480885311871227,"Adversarial Training 
Pretraining on ImageNet21k 
Semi-Supervised Learning
Distillation 
Temperature Scaling
MC-Dropout"
IN-DISTRIBUTION ANALYSIS,0.13682092555331993,AUROC Improvement over Vanilla
IN-DISTRIBUTION ANALYSIS,0.13883299798792756,Semi-Supervised Learning
IN-DISTRIBUTION ANALYSIS,0.14084507042253522,"Figure 3: A comparison of different methods and their AUROC improvement relative to the same
model’s performance without employing the method. Markers above the x axis represent models
that beneﬁted from the evaluated method, and vice versa."
IN-DISTRIBUTION ANALYSIS,0.14285714285714285,Adversarial Training
IN-DISTRIBUTION ANALYSIS,0.1448692152917505,Pretraining on ImageNet21k
IN-DISTRIBUTION ANALYSIS,0.14688128772635814,Semi-Supervised Learning
IN-DISTRIBUTION ANALYSIS,0.1488933601609658,Distillation
IN-DISTRIBUTION ANALYSIS,0.15090543259557343,Temperature Scaling −0.05 0 0.05 0.1 0.15 0.2
IN-DISTRIBUTION ANALYSIS,0.1529175050301811,Method
IN-DISTRIBUTION ANALYSIS,0.15492957746478872,"Adversarial Training
Pretraining on ImageNet21k 
Semi-Supervised Learning 
Distillation
Temperature Scaling"
IN-DISTRIBUTION ANALYSIS,0.15694164989939638,ECE Improvement over Vanilla
IN-DISTRIBUTION ANALYSIS,0.158953722334004,Temperature Scaling can sometimes harm ECE
IN-DISTRIBUTION ANALYSIS,0.16096579476861167,"Figure 4: A comparison of different methods and their ECE improvement relative to the same
model’s performance without employing the method. Markers above the x axis represent models
that beneﬁted from the evaluated method, and vice versa. Temperature scaling can sometimes harm
ECE, although its purpose is to improve it."
IN-DISTRIBUTION ANALYSIS,0.16297786720321933,"Evaluations of the simple post-training calibration method of temperature scaling (TS) (Guo et al.,
2017), which is widely known to improve ECE without changing the model’s accuracy, also re-
vealed several interesting facts: (1) TS consistently and greatly improves AUROC and selective
performance (see Figure 3)—meaning not only does TS calibrate the probabilistic estimation for
each individual instance, but it also improves the partial order of all instances induced by those im-
proved estimations. While TS is well known and used for calibration, to the best of our knowledge,
its beneﬁts for selective prediction were previously unknown. (2) While TS is usually beneﬁcial, it
could harm some models (see Figures 3 and 4). While it is surprising that TS–a calibration method–
would harm ECE, this phenomenon is explained by the fact that TS optimizes NLL and not ECE
(to avoid trivial solutions), and the two may sometimes misalign. (3) Models that beneﬁt from TS
in terms of AUROC tend to have been assigned a temperature smaller than 1 by the calibration pro-
cess. This, however, does not hold true for ECE (see Figures 16 and 17 in Appendix I). (4) While all
models usually improve with TS, the overall ranking of uncertainty performance between families
tends to stay similar, with the worse (in terms of ECE and AUROC) models closing most of the gap
between them and the mediocre ones."
IN-DISTRIBUTION ANALYSIS,0.16498993963782696,"The ViT architecture far surpasses any other family of models in terms of AUROC and ECE (see
Figure 1; Figure 15 in Appendix I shows this is true even after using TS) as well as for the SAC of
99% we explored (see Figure 5 and Appendix J). Moreover, for any size, ViT models outperform
their competition in all of these metrics (see Figures 9 and 10 in Appendix B and Figure 18 in"
IN-DISTRIBUTION ANALYSIS,0.16700201207243462,Under review as a conference paper at ICLR 2022
IN-DISTRIBUTION ANALYSIS,0.16901408450704225,"0
10
20
30
40
50 60 65 70 75 80 85"
IN-DISTRIBUTION ANALYSIS,0.1710261569416499,Coverage for Accuracy 99
IN-DISTRIBUTION ANALYSIS,0.17303822937625754,Accuracy
IN-DISTRIBUTION ANALYSIS,0.1750503018108652,Models
IN-DISTRIBUTION ANALYSIS,0.17706237424547283,"Various 
AlexNet 
BiT 
DLA
RegNetX
RegNetY 
ResNet 
SWSL ResNeXt"
IN-DISTRIBUTION ANALYSIS,0.1790744466800805,"ViT 
ViT SAM"
IN-DISTRIBUTION ANALYSIS,0.18108651911468812,"Figure 5: Comparison of models by their overall accuracy and the coverage they are able to provide
a selective accuracy constraint of Top-1 99% on ImageNet. A higher coverage is better. Only ViT
models are able to provide coverage beyond 30% for this constraint. They provide more coverage
than any other model compared to their accuracy or size."
IN-DISTRIBUTION ANALYSIS,0.18309859154929578,"Appendix J). While most ViT models we evaluated were pretrained on ImageNet-21k and enjoyed
augmentations tailored to them (Steiner et al., 2021), comparing them with the non-pretrained and
not strongly augmented ViT SAMs (Chen et al., 2021a), which are consequently much worse in
terms of accuracy than regular ViTs, conﬁrms that the high performance in AUROC and SAC is not
due to pretraining or augmentation. It does suggest that it is the architecture itself that is naturally
superb at uncertainty estimation."
IN-DISTRIBUTION ANALYSIS,0.1851106639839034,"We also evaluate the AUROC performance of MC-Dropout using predictive entropy as its conﬁdence
score and 30 dropout-enabled forward passes. We do not measure its affects on ECE since entropy
scores do not reside in [0, 1]. Using MC-Dropout causes a consistent drop in both AUROC and
selective performance compared with using the same models with softmax as the κ (see Appendix K
and Figure 3). MC-Dropout’s underperformance was also previously observed in (Geifman & El-
Yaniv, 2017)."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.18712273641851107,"4
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.1891348088531187,"When the underlying distribution P(x, y) used to train a model changes, we may no longer expect
that the model will perform correctly. Changes in P can be the result of many natural or adversarial
processes such as natural deviation in the input space X, noisy sensor reading of inputs, abrupt
changes due to random events, newly arrived or reﬁned input classes, etc. We distinguish between
input distributional changes in PX|Y and changes in the label distribution. We focus on the latter
case and consider the class-out-of-distribution (C-OOD) scenario where the label support set Y
changes to a different set, YOOD, which contains new classes that were not observed in training. We
note that both aspects of distributional deviations have been considered in the literature (Hendrycks
& Dietterich, 2019; Liang et al., 2017; Hendrycks et al., 2021), and a number of methods have been
introduced to deal with these cases (Liang et al., 2017; Lee et al., 2018; Golan & El-Yaniv, 2018)."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.19114688128772636,"We consider the following detection task, in which our model is required to distinguish between
samples belonging to classes it has seen in training, where x ∼P(x|y ∈Y), and novel classes, i.e.,
x ∼P(x|y ∈YOOD). We examine the detection performance of DNN classiﬁcation models that
use their conﬁdence rate function κ to detect OOD labels where the basic premise is that instances
whose labels are in YOOD correspond to low κ values."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.193158953722334,"A crucial question in any study of distributional deviations is what we choose as our experimental
data to proxy meaningful deviations. For our study of C-OOD we introduce a novel method for
generating C-OOD data with a controllable degree of severity. Let YOOD be a large set of OOD
classes (e.g., labels from ImageNet-21k), and let s(y|f, κ) be a severity score that reﬂects the difﬁ-
culty of model f, which uses κ to detect instances from class y ∈YOOD. Having deﬁned a function"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.19517102615694165,Under review as a conference paper at ICLR 2022
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.19718309859154928,"s(y|f, κ) (see details below) we can build multiple C-OOD datasets with progressively increasing
severity levels. Importantly, the resulting C-OOD data is speciﬁcally tailored to model f itself (and
its κ)."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.19919517102615694,"Given a model f (and its κ), we deﬁne s(y|f, κ) to be the average conﬁdence given by κ to samples
from class y ∈YOOD. When considering ID instances we expect κ to give high values for highly
conﬁdent predictions. Therefore, the larger s(y|f, κ) is, the harder it is for κ to detect the OOD class
y among ID classes. We estimate s(y|f, κ) for each class in ImageNet-21K (not in ImageNet-1K)
using a sample from the class and take as C-OOD data a different sample from that class. Using s
we sub-sample 11 groups of classes (severity levels) from YOOD, with increasing severities, such
that severity level i is the ith percentile of all severities. We further expand on how we chose the
severity levels, their statistical meaning and the construction of the multiple C-OOD datasets for our
experiments in Appendix L."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.2012072434607646,"s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10 ImageNet-O 30 40 50 60 70 80 90 100"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.20321931589537223,Models:
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.2052313883299799,"ViT-L/16-384
ResNet-50
AlexNet"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.20724346076458752,Severity Levels
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.20925553319919518,C-OOD AUROC (detection)
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.2112676056338028,"Figure 6: OOD performance across 11 severity levels. Note how the detection performance de-
creases for all models as we increase the difﬁculty until it reaches near chance detection performance
at the last severity (s10). The top curve belongs to ViT-L/16-384, which beats all models at every
severity level. We also observe how the previous C-OOD benchmark, ImageNet-O does not reﬂect
the true OOD performance of the models, since it was designed to speciﬁcally fool ResNet-50, and
so it is more difﬁcult for models similar to ResNet-50 than other models."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.21327967806841047,"Figure 6 presents the C-OOD detection performance of 484 models across 11 C-OOD severity levels
(recall that severity levels are constructed individually for each model). The box plots clearly show a
monotone AUROC performance degradation. In addition, we see that ViT-L/16-384 is consistently
the best model for each level (recall that this model is also the best for ID)."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.2152917505030181,"Most works on OOD detection use small scale datasets that generally do not resemble the training
distribution and, therefore are, easy to detect. The use of such sets often causes C-OOD detectors to
appear better than they truly are in harder tasks. Motivated by this deﬁciency, Hendrycks et al. (2021)
introduced the ImageNet-O dataset as a solution. ImageNet-O, however, has two limitations. First, it
lacks severity levels. Second, the original intent in the creation of ImageNet-O was to include only
hard C-OOD instances.The deﬁnition of “OOD hardness”, however, was carried out with respect
to ResNet-50’s difﬁculty in detecting OOD classes. This property makes ImageNet-O strongly
biased. Indeed, the right-most box in Figure 6 corresponds to the performance of the 484 models
over ImageNet-O. The orange dot in that box corresponds to ResNet-50, whose OOD detection
perfromance is severely harmed by these data. Nevertheless, it is evident that numerous models
perform quite well. In this respect, it can be argued that the proposed C-OOD dataset generator (see
Appendix L) has better properties and is clearly, not biased toward a single architecture."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.21730382293762576,"The next question we ask is does ID uncertainty estimation ranking performance indicate better
C-OOD detection performance (and vice versa)? Figure 7 shows a scatter plot of ID vs. C-OOD
AUROC performance of all the tested models. The overall Spearman correlation is 0.43. The
legend indicates correlations obtained by speciﬁc families. For instance, ViTs are among the most"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.2193158953722334,Under review as a conference paper at ICLR 2022
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.22132796780684105,"76
78
80
82
84
86
88 70 75 80 85 90"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.22334004024144868,ID AUROC
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.22535211267605634,C-OOD AUROC
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.22736418511066397,"Various (0.40, 388) 
AlexNet (-, 1)
ResNet (0.16, 33) 
ViT (0.74, 21) 
MLP Mixer (1.00, 4)"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.22937625754527163,"ShuffleNetV2 (1.00, 2) 
SqueezeNet (1.00, 2) 
Swin (0.83, 6) 
EfficientNetV2 (-0.46, 15) 
BiT (0.32, 9)"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.23138832997987926,"Models (Spearman correlation per family, number of models)"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.23340040241448692,(detection)
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.23541247484909456,Overall Spearman correlation 0.43
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.23742454728370221,"Figure 7: OOD detection performance in severity 5 vs. In-Distribution uncertainty estimation per-
formance. We notice that the best performing network in one task is not the best in the other, but
both belong to the same family, ViTs"
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.23943661971830985,"correlated largest sample size family (0.74). This means that a ViT model is likely to perform well
in both ID and C-OOD. Note that the worst performing models in C-OOD detection are small,
optimized models. We further discuss correlations with other factors such as accuracy, model size,
input size and embedding size in Appendix M.7."
UNCERTAINTY DUE TO CLASS-OUT-OF-DISTRIBUTION,0.2414486921529175,"Due to lack of space, a number of additional interesting observations and results are presented in
Appendix M. We mention the most interesting ones here: (1) In accordance with the ID results
(see Section 3), among all training regimes, distillation improves performance the most across all
severities; see Figure 21 in Appendix M.3. (2) At the outset it could be anticipated that ImageNet21k
pretraining will hinder C-OOD detection performance (due to its exposure to the OOD classes in
training). Surprisingly, we observe that pretraining on ImageNet21k somewhat helps performance
at severity levels up to level 6; see Figure 21 in Appendix M.3. (3) ViTs appear to achieve the
best C-OOD detection performance per-model size (# parameters); see M.1. (4) Using entropy,
as κ, improves C-OOD detection performance in most cases; see Appendix M.4. (5) The use of
MC-dropout for C-OOD detection is investigated in Appendix M.5."
CONCLUDING REMARKS,0.24346076458752516,"5
CONCLUDING REMARKS"
CONCLUDING REMARKS,0.2454728370221328,"We presented a comprehensive study of the effectiveness of numerous DNN architectures (fam-
ilies) in providing reliable uncertainty estimation, including the impact of various techniques on
improving such capabilities. Moreover, we considered both in-distribution and novel (graded) class-
out-of-distribution settings. Our study led to many discoveries and perhaps the most important ones
are: (1) architectures trained with distillation almost always improve their uncertainty estimation
performance, (2) temperature scaling is very useful not only for calibration but also for ranking
and selective prediction, and (3) no other DNN (evaluated in this study) had ever demonstrated an
uncertainty estimation performance comparable—in any metric tested or setting, in-distribution or
class-out-of-distribution—to the ViT architecture."
CONCLUDING REMARKS,0.24748490945674045,"Our work leaves open many interesting avenues for future research and we would like to mention a
few. Perhaps the most interesting question is why distillation is so beneﬁcial in boosting uncertainty
estimation. Next, what is the architectural secret in vision transformers (ViT) that enables their
uncertainty estimation supremacy. This question is even more puzzling given the fact that ViT
supremacy is not shared with many other supposedly similar transformer-based models that we
tested such as Touvron et al. (2021b; 2020); Liu et al. (2021); Han et al. (2021); Graham et al.
(2021); d’Ascoli et al. (2021); Heo et al. (2021); Xu et al. (2021); El-Nouby et al. (2021); Zhang
et al. (2021); Chu et al. (2021); Chen et al. (2021b). Finally, can we create specialized training
regimes (e.g., Geifman & El-Yaniv (2019)), specialized augmentations, or even specialized neural
architecture search (NAS) strategies that can promote superior uncertainty estimation performance?"
CONCLUDING REMARKS,0.24949698189134809,Under review as a conference paper at ICLR 2022
REPRODUCIBILITY,0.2515090543259557,"6
REPRODUCIBILITY"
REPRODUCIBILITY,0.2535211267605634,"The weights for all models evaluated in this paper are publicly available, and taken from the Py-
Torch and timm respositories (Paszke et al., 2019; Wightman, 2019), see Section 1. The algorithm
for constructing our C-OOD dataset out of ImageNet21k, grouped into various severity levels, is
detailed in Appendix L."
REFERENCES,0.25553319919517103,REFERENCES
REFERENCES,0.2575452716297787,"Rakefet Ackerman, Avi Parush, Fareda Nassar, and Avraham Shtub. Metacognition and system
usability: Incorporating metacognitive research paradigm into usability testing. Computers in
Human Behavior, 54:101–113, January 2016. doi: 10.1016/j.chb.2015.07.041. URL https:
//doi.org/10.1016/j.chb.2015.07.041."
REFERENCES,0.2595573440643863,"Rakefet Ackerman, Avigdor Gal, Tomer Sagi, and Roee Shraga. A cognitive model of human bias
in matching. In PRICAI 2019: Trends in Artiﬁcial Intelligence, pp. 632–646. Springer Interna-
tional Publishing, 2019. doi: 10.1007/978-3-030-29908-8 50. URL https://doi.org/10.
1007/978-3-030-29908-8_50."
REFERENCES,0.26156941649899396,"Yonathan Aﬂalo, Asaf Noy, Ming Lin, Itamar Friedman, and Lihi Zelnik-Manor. Knapsack pruning
with inner distillation.
CoRR, abs/2002.08258, 2020.
URL https://arxiv.org/abs/
2002.08258."
REFERENCES,0.2635814889336016,"Alexandra Basile, Maggie E. Toplak, and Brendan F. Andrade. Using metacognitive methods to
examine emotion recognition in children with ADHD. Journal of Attention Disorders, 25(2):
245–257, November 2018. doi: 10.1177/1087054718808602. URL https://doi.org/10.
1177/1087054718808602."
REFERENCES,0.2655935613682093,"Glenn W. Brier. Veriﬁcation of Forecasts Expressed in Terms of Probability. Monthly Weather
Review, 78(1):1, January 1950. doi: 10.1175/1520-0493(1950)0780001:VOFEIT2.0.CO;2."
REFERENCES,0.2676056338028169,"Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets
without pretraining or strong data augmentations. CoRR, abs/2106.01548, 2021a. URL https:
//arxiv.org/abs/2106.01548."
REFERENCES,0.26961770623742454,"Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer: The
vision-friendly transformer. CoRR, abs/2104.12533, 2021b. URL https://arxiv.org/
abs/2104.12533."
REFERENCES,0.2716297786720322,"C. K. Chow. An optimum character recognition system using decision functions. IRE Transactions
on Electronic Computers, EC-6(4):247–254, 1957. doi: 10.1109/TEC.1957.5222035."
REFERENCES,0.27364185110663986,"Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and
Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers, 2021."
REFERENCES,0.27565392354124746,"L. P. Cordella, C. De Stefano, F. Tortorella, and M. Vento. A method for improving classiﬁcation
reliability of multilayer perceptrons. IEEE Transactions on Neural Networks, 6(5):1140–1147,
1995. doi: 10.1109/72.410358."
REFERENCES,0.2776659959758551,"St´ephane d’Ascoli, Hugo Touvron, Matthew L. Leavitt, Ari S. Morcos, Giulio Biroli, and Levent
Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. CoRR,
abs/2103.10697, 2021. URL https://arxiv.org/abs/2103.10697."
REFERENCES,0.2796780684104628,"C. De Stefano, C. Sansone, and M. Vento. To reject or not to reject: that is the question-an an-
swer in case of neural classiﬁers. IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews), 30(1):84–94, 2000. doi: 10.1109/5326.827457."
REFERENCES,0.28169014084507044,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848."
REFERENCES,0.2837022132796781,Under review as a conference paper at ICLR 2022
REFERENCES,0.2857142857142857,"Yukun Ding, Jinglan Liu, Jinjun Xiong, and Yiyu Shi. Evaluation of neural network uncertainty
estimation with application to resource-constrained platforms.
CoRR, abs/1903.02050, 2019.
URL http://arxiv.org/abs/1903.02050."
REFERENCES,0.28772635814889336,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. CoRR, abs/2010.11929, 2020. URL https://arxiv.org/abs/2010.11929."
REFERENCES,0.289738430583501,"Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand
Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, and Herv´e Jegou. Xcit:
Cross-covariance image transformers, 2021."
REFERENCES,0.2917505030181087,"Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classiﬁcation. Journal of
Machine Learning Research, 11(5), 2010."
REFERENCES,0.2937625754527163,"Tom Fawcett. An introduction to roc analysis. Pattern Recognition Letters, 27(8):861–874, 2006.
ISSN 0167-8655.
doi: https://doi.org/10.1016/j.patrec.2005.10.010.
URL https://www.
sciencedirect.com/science/article/pii/S016786550500303X.
ROC Anal-
ysis in Pattern Recognition."
REFERENCES,0.29577464788732394,"K. Fiedler, Rakefet Ackerman, and Chiara Scarampi. ! metacognition : Monitoring and controlling
one ’ s own knowledge , reasoning and decisions. 2019."
REFERENCES,0.2977867203219316,"Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural net-
works. CoRR, abs/1803.03635, 2018. URL http://arxiv.org/abs/1803.03635."
REFERENCES,0.29979879275653926,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. 2016."
REFERENCES,0.30181086519114686,"Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr.
Res2net: A new multi-scale backbone architecture. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 43(2):652–662, Feb 2021. ISSN 1939-3539. doi: 10.1109/tpami.2019.
2938758. URL http://dx.doi.org/10.1109/TPAMI.2019.2938758."
REFERENCES,0.3038229376257545,"Yonatan Geifman and Ran El-Yaniv.
Selective classiﬁcation for deep neural networks.
CoRR,
abs/1705.08500, 2017. URL http://arxiv.org/abs/1705.08500."
REFERENCES,0.3058350100603622,"Yonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated reject
option. CoRR, abs/1901.09192, 2019. URL http://arxiv.org/abs/1901.09192."
REFERENCES,0.30784708249496984,"Yonatan Geifman, Guy Uziel, and Ran El-Yaniv.
Bias-reduced uncertainty estimation for deep
neural classiﬁers. In International Conference on Learning Representations, 2018."
REFERENCES,0.30985915492957744,"Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations, 2018."
REFERENCES,0.3118712273641851,"Leo
A.
Goodman
and
William
H.
Kruskal.
Measures
of
association
for
cross
classiﬁcations∗.
Journal of the American Statistical Association, 49(268)
:
732
−
−764, December1954. doi : .
URL https://doi.org/10.1080/01621459.1954.
10501231."
REFERENCES,0.31388329979879276,"Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv´e J´egou,
and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. CoRR,
abs/2104.01136, 2021. URL https://arxiv.org/abs/2104.01136."
REFERENCES,0.3158953722334004,"Thomas D. Grifﬁn, Jennifer Wiley, and Keith W. Thiede.
The effects of comprehension-test ex-
pectancies on metacomprehension accuracy.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 45(6):1066–1092, June 2019.
10.1037/xlm0000634.
URL https:
//doi.org/10.1037/xlm0000634."
REFERENCES,0.317907444668008,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
On calibration of modern neural
networks. CoRR, abs/1706.04599, 2017. URL http://arxiv.org/abs/1706.04599."
REFERENCES,0.3199195171026157,Under review as a conference paper at ICLR 2022
REFERENCES,0.32193158953722334,"Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
Transformer in
transformer. CoRR, abs/2103.00112, 2021. URL https://arxiv.org/abs/2103.00112."
REFERENCES,0.323943661971831,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image
recognition, 2015."
REFERENCES,0.32595573440643866,"Dan Hendrycks and Thomas Dietterich.
Benchmarking neural network robustness to common
corruptions and perturbations, 2019."
REFERENCES,0.32796780684104626,"Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
Natural adver-
sarial examples.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 15262–15271, 2021."
REFERENCES,0.3299798792756539,"Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon
Oh. Rethinking spatial dimensions of vision transformers. CoRR, abs/2103.16302, 2021. URL
https://arxiv.org/abs/2103.16302."
REFERENCES,0.3319919517102616,"Philip A. Higham and D. Paul Higham.
New improved gamma:
Enhancing the accuracy
of goodman–kruskal’s gamma using ROC curves.
Behavior Research Methods, 51(1):108–
125, September 2018.
10.3758/s13428-018-1125-5.
URL https://doi.org/10.3758/
s13428-018-1125-5."
REFERENCES,0.33400402414486924,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015."
REFERENCES,0.33601609657947684,"Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun
Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching for
mobilenetv3. CoRR, abs/1905.02244, 2019. URL http://arxiv.org/abs/1905.02244."
REFERENCES,0.3380281690140845,"Armen
Der
Kiureghian
and
Ove
Ditlevsen.
Aleatory
or
epistemic?
does
it
matter?
Structural
Safety,
31(2):105
–
112,
2009.
ISSN
0167-4730.
https://doi.org/10.1016/j.strusafe.2008.06.020.
URL
http://www.sciencedirect.
com/science/article/pii/S0167473008000556. Risk Acceptance and Risk Commu-
nication."
REFERENCES,0.34004024144869216,"Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. 2017."
REFERENCES,0.3420523138832998,"Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniﬁed framework for detecting
out-of-distribution samples and adversarial attacks, 2018."
REFERENCES,0.3440643863179074,"Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution
image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017."
REFERENCES,0.3460764587525151,"Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design for
gpu-efﬁcient networks, 2020."
REFERENCES,0.34808853118712274,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows. CoRR, abs/2103.14030,
2021. URL https://arxiv.org/abs/2103.14030."
REFERENCES,0.3501006036217304,"Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry P. Vetrov, and Andrew Gordon Wilson. A
simple baseline for bayesian uncertainty in deep learning.
CoRR, abs/1902.02476, 2019.
URL
http://arxiv.org/abs/1902.02476."
REFERENCES,0.352112676056338,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pre-
training, 2018a."
REFERENCES,0.35412474849094566,"Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan
Li, Ashwin Bharambe, and Laurens van der Maaten.
Exploring the limits of weakly supervised
pretraining. CoRR, abs/1805.00932, 2018b. URL http://arxiv.org/abs/1805.00932."
REFERENCES,0.3561368209255533,Under review as a conference paper at ICLR 2022
REFERENCES,0.358148893360161,"Jooyoung Moon, Jihyo Kim, Younghak Shin, and Sangheum Hwang. Conﬁdence-aware learning
for deep neural networks.
CoRR, abs/2007.01458, 2020.
URL https://arxiv.org/abs/
2007.01458."
REFERENCES,0.36016096579476864,"Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael W. Dusenberry, Sebastian Far-
quhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah Liu, Zelda Ma-
riet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim G. J. Rudner, Yeming Wen, Florian Wenzel, Kevin
Murphy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin Gal, and Dustin Tran. Uncer-
tainty baselines: Benchmarks for uncertainty & robustness in deep learning. CoRR, abs/2106.04015,
2021. URL https://arxiv.org/abs/2106.04015."
REFERENCES,0.36217303822937624,"Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht.
Obtaining well calibrated
probabilities using bayesian binning.
In Proceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, AAAI’15, pp. 2901–2907. AAAI Press, 2015. ISBN 0262511290."
REFERENCES,0.3641851106639839,"Niv Nayman, Yonathan Aﬂalo, Asaf Noy, and Lihi Zelnik-Manor. Hardcore-nas: Hard constrained
differentiable neural architecture search. CoRR, abs/2102.11646, 2021. URL https://arxiv.
org/abs/2102.11646."
REFERENCES,0.36619718309859156,"Thomas O. Nelson.
A comparison of current measures of the accuracy of feeling-of-knowing
predictions.
Psychological Bulletin, 95(1):109–133, 1984.
10.1037/0033-2909.95.1.109.
URL
https://doi.org/10.1037/0033-2909.95.1.109."
REFERENCES,0.3682092555331992,"Jeremy Nixon, Mike Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran.
Measuring
calibration in deep learning. CoRR, abs/1904.01685, 2019. URL http://arxiv.org/abs/
1904.01685."
REFERENCES,0.3702213279678068,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Pytorch: An imperative style,
high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-
Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024–8035. Curran Associates, Inc., 2019.
URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf."
REFERENCES,0.3722334004024145,"Tal Ridnik, Emanuel Ben Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for
the masses. CoRR, abs/2104.10972, 2021. URL https://arxiv.org/abs/2104.10972."
REFERENCES,0.37424547283702214,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks, 2019."
REFERENCES,0.3762575452716298,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014."
REFERENCES,0.3782696177062374,"Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas
Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. CoRR,
abs/2106.10270, 2021. URL https://arxiv.org/abs/2106.10270."
REFERENCES,0.38028169014084506,"Mingxing Tan and Quoc V. Le.
Efﬁcientnetv2: Smaller models and faster training.
CoRR,
abs/2104.00298, 2021. URL https://arxiv.org/abs/2104.00298."
REFERENCES,0.3822937625754527,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herv´e J´egou. Training data-efﬁcient image transformers & distillation through attention. CoRR,
abs/2012.12877, 2020. URL https://arxiv.org/abs/2012.12877."
REFERENCES,0.3843058350100604,"Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard
Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herv´e J´egou. Resmlp: Feedforward
networks for image classiﬁcation with data-efﬁcient training. CoRR, abs/2105.03404, 2021a. URL
https://arxiv.org/abs/2105.03404."
REFERENCES,0.386317907444668,Under review as a conference paper at ICLR 2022
REFERENCES,0.38832997987927564,"Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv´e J´egou. Going
deeper with image transformers. CoRR, abs/2103.17239, 2021b. URL https://arxiv.org/
abs/2103.17239."
REFERENCES,0.3903420523138833,"Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rkZvSe-RZ."
REFERENCES,0.39235412474849096,"Monika Undorf and Arndt Br¨oder.
Cue integration in metamemory judgements is
strategic.
Quarterly Journal of Experimental Psychology, 73(4):629–642, October 2019.
10.1177/1747021819882308. URL https://doi.org/10.1177/1747021819882308."
REFERENCES,0.39436619718309857,"Juozas Vaicenavicius, David Widmann, Carl R. Andersson, Fredrik Lindsten, Jacob Roll, and
Thomas B. Sch¨on. Evaluating model calibration in classiﬁcation. CoRR, abs/1902.06977, 2019.
URL http://arxiv.org/abs/1902.06977."
REFERENCES,0.3963782696177062,"Ross Wightman.
Pytorch image models.
https://github.com/rwightman/
pytorch-image-models, 2019."
REFERENCES,0.3983903420523139,"Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, and Quoc V. Le. Adversarial
examples improve image recognition.
CoRR, abs/1911.09665, 2019a.
URL http://arxiv.
org/abs/1911.09665."
REFERENCES,0.40040241448692154,"Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. Self-training with noisy student
improves imagenet classiﬁcation. CoRR, abs/1911.04252, 2019b. URL http://arxiv.org/
abs/1911.04252."
REFERENCES,0.4024144869215292,"Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le.
Self-training with noisy student
improves imagenet classiﬁcation, 2020."
REFERENCES,0.4044265593561368,"Weijian Xu, Yifan Xu, Tyler A. Chang, and Zhuowen Tu. Co-scale conv-attentional image trans-
formers. CoRR, abs/2104.06399, 2021. URL https://arxiv.org/abs/2104.06399."
REFERENCES,0.40643863179074446,"I. Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-
supervised learning for image classiﬁcation, 2019."
REFERENCES,0.4084507042253521,"Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He,
Jonas Mueller, R. Manmatha, Mu Li, and Alexander Smola.
Resnest: Split-attention networks,
2020."
REFERENCES,0.4104627766599598,"Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, and Tomas Pﬁster. Aggregating nested trans-
formers. CoRR, abs/2105.12723, 2021. URL https://arxiv.org/abs/2105.12723."
REFERENCES,0.4124748490945674,Under review as a conference paper at ICLR 2022
REFERENCES,0.41448692152917505,"A
THE INVESTMENT EXAMPLE"
REFERENCES,0.4164989939637827,"Let us consider two classiﬁcation models for the stock market that predict whether a stock’s value
is about to increase, decrease or remains neutral (three-class classiﬁcation). Suppose that Model
A has a 95% true accuracy, and generates a conﬁdence score of 0.95 on any prediction (even on
missclassiﬁed instances); Model B has a 40% true accuracy, but always gives a conﬁdence score of
0.6 on correct predictions, and 0.4 on incorrect ones. We now try and evaluate these two models
with the uncertainty metrics mentioned in Section 1 to see which can reveal Model B’s superior
uncertainty estimation performance. AURC will fail due to its sensitivity to accuracy (the AURC
of Model B is 0.12, more than twice as bad as the AURC for Model A, which is 0.05). NLL will
rank Model A four times higher (Model A’s NLL is 0.23 and Model B’s is 0.93). The Brier score
would also much prefer Model A (giving it a score of 0.096 while giving Model B a score of 0.54).
Evaluating the models’ calibration with ECE will also not reveal Model B’s advantages, since it is
less calibrated than Model A, which has perfect calibration (Model A has an ECE of 0, and Model
B has a worse ECE of 0.4)."
REFERENCES,0.41851106639839036,"AUROC, on the other hand, would give Model B a perfect score of 1 and a terrible score of 0.5
to Model A. The selective risk for Model B would be better for any coverage of stock predictions
below 40%, and for any SAC above 95% the coverage for Model A would be 0, but 0.4 for Model
B."
REFERENCES,0.42052313883299797,"Those two metrics are not perfect for any example. If instead we were to compare two different
models for the task of predicting the weather, such that we cannot abstain from making predictions
but are required to provide an accurate probabilistic uncertainty estimation of the model’s predic-
tions, AUROC and selective risk would be meaningless (due to the model’s inability to abstain in
this task), but ECE or the Brier Score would better evaluate the performance the new task requires."
REFERENCES,0.4225352112676056,"B
RANKING AND CALIBRATION VISUAL COMPARISON"
REFERENCES,0.4245472837022133,"A comparison of 484 models by their AUROC (×100, higher is better) and -log(ECE) (higher is
better) on ImageNet is visualized in Figure 8. To compare models fairly by their size, we plot two
graphs with the logarithm of the number of parameters as the X axis, so that models sharing the
same x value can be compared solely based on their y value. In Figure 9 we set the X axis to
be AUROC (higher is better), and see ViTs outperform any other architecture with a comparable
amount of parameters by a large margin. We can also observe using distillation creates a consistent
improvement in AUROC. In Figure 10 we set the X axis to be the negative logarithm of ECE (higher
is better) and observe a very similar trend, with ViT outperforming its competition for any model
size."
REFERENCES,0.42655935613682094,"C
DEMONSTRATION OF E-AURC’S DEPENDENCE ON THE MODEL’S
ACCURACY"
REFERENCES,0.42857142857142855,"Excess-AURC (E-AURC) was suggested by Geifman et al. (2018) as an alternative to AURC
(explained in Section 2). To calculate E-AURC, two AURC scores need to be calculated: (1)
AURC(model), the AURC value of the actual model and (2) AURC(model∗), the AURC value of
a hypothetical model with identical predicted labels as the ﬁrst model, but that outputs conﬁdence
values that induce a perfect partial order on the instances in terms of their correctness. The latter
means that all incorrectly predicted instances are assigned conﬁdence values lower than the correctly
predicted instances."
REFERENCES,0.4305835010060362,"E-AURC is then deﬁned as AURC(model) −AURC(model∗). In essence, this metrics acknowl-
edges that given a model’s accuracy, the area of AURC(model∗) is always unavoidable no matter
how good the partial order is, but anything above that could have been minimized if the κ function
was better at ranking, assigning correct instances higher values than incorrect ones and inducing a
better partial order over the instances."
REFERENCES,0.43259557344064387,"This metric indeed helps to reduce some of the sensitivity to accuracy suffered by AURC, and for
the example presented in Section 1, E-AURC would have given a perfect score of 0 to the model
inducing a perfect partial order by its conﬁdence values (Model B). It is easy, however, to craft"
REFERENCES,0.4346076458752515,Under review as a conference paper at ICLR 2022
REFERENCES,0.43661971830985913,"2
3
4
5
6
7 78 80 82 84 86 88"
REFERENCES,0.4386317907444668,Models
REFERENCES,0.44064386317907445,Various
REFERENCES,0.4426559356136821,AlexNet BiT CaiT CoaT
REFERENCES,0.44466800804828976,ConViT DeiT
REFERENCES,0.44668008048289737,DenseNet DLA
REFERENCES,0.448692152917505,ECANet
REFERENCES,0.4507042253521127,EfficientNet
REFERENCES,0.45271629778672035,EfficientNetV2 FBNet GENet gMLP
REFERENCES,0.45472837022132795,HardCoReNAS
REFERENCES,0.4567404426559356,Inception LeViT
REFERENCES,0.45875251509054327,MixConv
REFERENCES,0.4607645875251509,MLP Mixer
REFERENCES,0.46277665995975853,MnasNet
REFERENCES,0.4647887323943662,MobileNetV2
REFERENCES,0.46680080482897385,MobileNetV3
REFERENCES,0.4688128772635815,NASNet NesT NFNet PiT
REFERENCES,0.4708249496981891,RegNetX
REFERENCES,0.47283702213279677,RegNetY
REFERENCES,0.47484909456740443,RepVGG
REFERENCES,0.4768611670020121,Res2Net
REFERENCES,0.4788732394366197,ResMLP
REFERENCES,0.48088531187122735,ResNet
REFERENCES,0.482897384305835,ResNet_RS
REFERENCES,0.48490945674044267,ResNeXt
REFERENCES,0.4869215291750503,SE_ResNet
REFERENCES,0.48893360160965793,SSL_ResNet Swin
REFERENCES,0.4909456740442656,SWSL_ResNet TNT
REFERENCES,0.49295774647887325,TResNet Twins VGG
REFERENCES,0.4949698189134809,Visformer ViT
REFERENCES,0.4969818913480885,ViT SAM
REFERENCES,0.49899396378269617,WSP_ResNeXt XCiT
REFERENCES,0.5010060362173038,-log(ECE) AUROC
REFERENCES,0.5030181086519114,"Figure 8: A comparison of 484 models by their AUROC (×100, higher is better) and log(ECE)
(lower is better) on ImageNet. Each marker’s size is determined by the model’s number of parame-
ters. Each dotted marker represents a distilled version of the original."
REFERENCES,0.5050301810865191,Under review as a conference paper at ICLR 2022
REFERENCES,0.5070422535211268,"6
6.5
7
8
8.5
9 78 80 82 84 86 88 7.5 AUROC"
REFERENCES,0.5090543259557344,Models
REFERENCES,0.5110663983903421,"ResMLP
ResMLP distilled 
ResNet
ViT
ViT distilled
ViT SAM
XCiT
XCiT distilled"
REFERENCES,0.5130784708249497,"Various
AlexNet
BiT
BiT distilled 
EfficientNetV2 
GENet
MLP Mixer
MLP Mixer distilled
RegNetY"
REFERENCES,0.5150905432595574,log(#Parameters)
REFERENCES,0.5171026156941649,"Figure 9: A comparison of 484 models by their AUROC (×100, higher is better) and log(number of
model’s parameters) on ImageNet. Each dotted marker represents a distilled version of the original."
REFERENCES,0.5191146881287726,"6
6.5
7
8
8.5
9
1 2 3 4 5 6 7"
REFERENCES,0.5211267605633803,"Models
Various
AlexNet
BiT
BiT distilled
EfficientNetV2
GENet
MLP Mixer
MLP Mixer distilled
RegNetY
ResMLP
ResMLP distilled
ResNet
ViT
ViT distilled
ViT SAM
XCiT
XCiT distilled 7.5"
REFERENCES,0.5231388329979879,-log(ECE)
REFERENCES,0.5251509054325956,log(#Parameters)
REFERENCES,0.5271629778672032,"Figure 10: A comparison of 484 models by their -log(ECE) (higher is better) and log(number of
model’s parameters) on ImageNet. Each dotted marker represents a distilled version of the original."
REFERENCES,0.5291750503018109,"examples showing that E-AURC prefers models with higher accuracy, even if they have lower or
equal capacity to rank."
REFERENCES,0.5311871227364185,"To demonstrate this in a simple way, let us consider two models with a complete lack of capacity to
rank correct and incorrect predictions correctly, always outputting the same conﬁdence score. Model
A has an accuracy of 10% (thus an error rate of 90%), and Model B has an accuracy of 50%. A good
ranking metric should evaluate them equally (the same way E-AURC gives the same score for two
models that rank perfectly regardless of their accuracy). In Figure 11 we plot their RC curves, which
are both straight lines due to their lack of ranking ability. We can calculate both of these models
AURCs, AURC(modelA) = 0.9, AURC(modelB) = 0.5."
REFERENCES,0.5331991951710262,"The next thing to calculate is the best AURC values those models could have achieved given
the same accuracy if they had a perfect partial order.
We plot these hypothetical models’ RC
curves in Figure 12.
Their selective risk remains 0 for every coverage below their total accu-"
REFERENCES,0.5352112676056338,Under review as a conference paper at ICLR 2022
REFERENCES,0.5372233400402414,Figure 11: The RC curves for Models A and B.
REFERENCES,0.5392354124748491,"racy, since these hypothetical models assigned the highest conﬁdence to all of their correct in-
stances ﬁrst. As the coverage increases and they have no more correct instances to select, they
begin to give instances that are incorrect, and thus their selective risk linearly deteriorates for
higher coverages. Calculating both of these hypothetical models’ AURCs gives us the follow-"
REFERENCES,0.5412474849094567,Figure 12: The RC curves for the hypothetically optimal versions of Models A and B.
REFERENCES,0.5432595573440644,"ing: AURC(modelA∗) = 0.405, AURC(modelB) = 0.125. Subtracting our results we get:
E −AURC(modelA) = 0.9 −0.405 = 0.495, E −AURC(modelB) = 0.5 −0.125 = 0.375
Hence, E-AURC prefers Model B over Model A, even though both do not discriminate at all between
incorrect and correct instances."
REFERENCES,0.545271629778672,"D
MORE ON THE DEFINITION OF RANKING"
REFERENCES,0.5472837022132797,"Let us consider a ﬁnite set Sm = {(xi, yi)}m
i=1 ∼PX,Y . We assume that there are no two identical
values given by κ on Sm. Such an assumption is reasonable when choosing a continuous conﬁdence
signal."
REFERENCES,0.5492957746478874,Under review as a conference paper at ICLR 2022
REFERENCES,0.5513078470824949,"We further denote c as the number of concordant pairs (i.e., pairs in Sm that satisfy the condition
[κ(xi, ˆy|f) < κ(xj, ˆy|f) ∩ℓ(f(xi), yi) > ℓ(f(xj), yj)]) and d as the number of discordant pairs
(i.e., pairs in Sm that satisfy the condition [κ(xi, ˆy|f) > κ(xj, ˆy|f) ∩ℓ(f(xi), yi) > ℓ(f(xj), yj)]"
REFERENCES,0.5533199195171026,"We assume, for now, that there are no two identical values given by ℓon Sm. Accordingly, we can
further develop Equation (1) from 2.1 using the deﬁnition of conditional probability,"
REFERENCES,0.5553319919517102,"Pr[κ(xi, ˆy|f) < κ(xj, ˆy|f)|ℓ(f(xi), yi) > ℓ(f(xj), yj)] =
Pr[κ(xi, ˆy|f) < κ(xj, ˆy|f) ∩ℓ(f(xi), yi) > ℓ(f(xj), yj)]"
REFERENCES,0.5573440643863179,"Pr[ℓ(f(xi), yi) > ℓ(f(xj), yj)]
,"
REFERENCES,0.5593561368209256,"which can be approximated empirically, using the most likelihood estimator, as"
REFERENCES,0.5613682092555332,"c
 m
2
.
(2)"
REFERENCES,0.5633802816901409,"We notice that the last equation is identical to Kendall’s τ up to a linear transformation, which equals"
REFERENCES,0.5653923541247485,"c −d
 m
2
 = c −d + c −c
 m
2
"
REFERENCES,0.5674044265593562,"= 2c −(c + d)
 m
2

= 2c
 m
2
 −c + d
 m
2
 ="
REFERENCES,0.5694164989939637,"2 ·
c
 m
2
 −1 = 2 · [Equation 2] −1."
REFERENCES,0.5714285714285714,"Otherwise, if the loss assigns two identical values to a pair of points in Sm, but κ does not, then we
get:"
REFERENCES,0.5734406438631791,"c
c + d.
(3)"
REFERENCES,0.5754527162977867,which is identical to Goodman & Kruskal’s γ-correlation up to a linear transformation
REFERENCES,0.5774647887323944,"c −d
c + d = c −d + c −c"
REFERENCES,0.579476861167002,"c + d
= 2c −(c + d)"
REFERENCES,0.5814889336016097,"c + d
="
C,0.5835010060362174,"2c
c + d −c + d"
C,0.5855130784708249,c + d = 2 · [Equation 3] −1.
C,0.5875251509054326,"D.1
INEQUALITIES OF THE DEFINITION"
C,0.5895372233400402,"One might wonder why Equation (1) should have strict inequalities rather than non-strict ones to
deﬁne ranking. As we discuss below, this would damage the deﬁnition:"
C,0.5915492957746479,(1) If the losses had a non-strict inequality:
C,0.5935613682092555,"Pr[κ(x1, ˆy|f) < κ(x2, ˆy|f)|ℓ(f(x1), y1) ≥ℓ(f(x2), y2)]"
C,0.5955734406438632,"Consequently, in the case of classiﬁcation, for example, this probability would increase for any pairs
consisting of correct instances with different conﬁdences, which yields no beneﬁt in ranking between
incorrect and correct instances and motivates giving different conﬁdence values for instances with
the same loss—a fact that would not truly add any value."
C,0.5975855130784709,(2) If the κ values had a non-strict inequality:
C,0.5995975855130785,"Pr[κ(x1, ˆy|f) ≤κ(x2, ˆy|f)|ℓ(f(x1), y1) > ℓ(f(x2), y2)]."
C,0.6016096579476862,"This probability would increase for any pair (x1, x2) such that κ(x1, ˆy|f) = κ(x2, ˆy|f) and
ℓ(f(x1)) > ℓ(f(x2)), although κ should have ranked x1 with a lower value. Furthermore, if a"
C,0.6036217303822937,Under review as a conference paper at ICLR 2022
C,0.6056338028169014,"κ function were to assign the same conﬁdence score to all x ∈X, then when there are no two
identical values of losses, the deﬁnition’s probability would be 1; otherwise, the more different
values for losses there are, the larger it would grow.
In classiﬁcation with a 0/1 loss, for ex-
ample, assigning the same conﬁdence score to all instances would result in the probability being
Accuracy(f) · (1 −Accuracy(f)), which is largest when Accuracy(f) = 0.5."
C,0.607645875251509,"E
RANKING CAPACITY COMPARISON BETWEEN HUMANS AND NEURAL
NETWORKS"
C,0.6096579476861167,"In the ﬁeld of metacognition, interestingly, the predictive value of conﬁdence is evaluated by two
different aspects: by its ability to discriminate between correct and incorrect predictions (also known
as resolution in metacognition or ranking in our context) and by its ability to give well calibrated
conﬁdence estimations, not being over- or underconﬁdent (Fiedler et al., 2019). These two aspects
correspond perfectly with much of the research done in the deep learning ﬁeld, with the nearly
matching metric to AUROC of γ-correlation (see Section 2)."
C,0.6116700201207244,"This allows us to compare how well humans rank predictions in various tasks and how models rank
their own in others. Human AUROC measurements in various tasks (translated from γ-correlation)
tends to range from 0.6 to 0.75 (Undorf & Br¨oder, 2019; Basile et al., 2018; Ackerman et al., 2016),
but could vary, usually towards much lower values (Grifﬁn et al., 2019). In our comprehensive
evaluation on ImageNet, AUROC ranged from 0.77 to 0.88 (with the median value being 0.85), and
in CIFAR-10 these measurements jump to the range of 0.92 to 0.94."
C,0.613682092555332,"While such comparisons between neural networks and humans are somewhat unfair due to the great
sensitivity required for the task, research that directly compares humans and machine learning algo-
rithms on the same task exist. For example, in Ackerman et al. (2019), algorithms far surpass even
the group of highest performing individuals in terms of ranking."
C,0.6156941649899397,"F
CRITICISMS OF AUROC AS A RANKING METRIC"
C,0.6177062374245473,"In this section we show why AUROC does not simply reward models for having lower accuracy,
addressing such criticism. The paper by Ding et al. (2019) presented a semi-artiﬁcial experiment to
demonstrate that AUROC might get larger the worse the model’s accuracy becomes. They considerr
a model f and its κ function evaluated on a classiﬁcation test set X, giving each a prediction ˆyf(x)
and a conﬁdence score κ(x, ˆyf(x)|f), which in this case is the model’s softmax response. Let
X c = {xc ∈X|ˆyf(xc) = y(x)} be the set of all instances correctly predicted by the model f, and
deﬁne xc
(i) ∈X c to be the correct instance that received the i-lowest conﬁdence score from κ. Their
example continues to consider an artiﬁcial model f m to be an exact clone of f with the following
modiﬁcation: for every i ≤m, the model f m now predicts a different, incorrect label for xc
(i);
however, its given conﬁdence score remains identical: κ(xc
(i), ˆyf(xc
(i))|f) = κ(xc
(i), ˆyf m(xc
(i))|f m).
f 0 is exactly identical to f, by this deﬁnition, not changing any predictions. The paper shows how
an artiﬁcially created model f m obtains a higher AUROC score the bigger its m. This happens even
though “nothing” changed but a hit to the model’s accuracy performance (by making mistakes on
more instances)."
C,0.6197183098591549,"First, to understand why this happens, we note that f 1: AUROC for κ increases the more pairs
of [κ(x1) < κ(x2)|ˆyf(x1) ̸= y(x1), ˆyf(x2) = y(x2)] there are. The model f 1 is now giving an
incorrect classiﬁcation to xc
(1), but this instance’s position in the partial order induced by κ has re-
mained the same (since κ(xc
(1)) is unchanged); therefore, |X c|−1 correctly ranked pairs were added:
[κ(xc
(1)) < κ(xc
(i))|ˆyf(xc
(1)) ̸= y(xc
(1)), ˆyf(xc
(i)) = y(xc
(i))] for every 1 < i ≤|X c|. Nevertheless,
this does not guarantee an increase to AUROC by itself: if, previously, all pairs of (correct,incorrect)
instances were ranked correctly by κ, AUROC would already be 1.0 for f 0 and would not change
for f 1. If AUROC for f 1 is higher than it was for f 0, this means there exists at least one instance
xw that was incorrectly predicted by the original model f 0 such that κ(xc
(1)) < κ(xw). Every such
originally wrongly ranked pair (by f 0) of [κ(xc
(1)) < κ(xw)|ˆyf(xw) ̸= y(xw), ˆyf(xc
(1)) = y(xc
(1))]
has been eliminated by f 1 wrongly predicting xc
(1). This, therefore, causes AUROC to increase at
the expense of the model’s accuracy."
C,0.6217303822937625,Under review as a conference paper at ICLR 2022
C,0.6237424547283702,"Such an analysis neglects many factors, which is probably why such an effect is only likely to be
observed in artiﬁcial models (and not among the actual models we have empirically tested):"
C,0.6257545271629779,"1. It is unreasonable to assume that the conﬁdence score given by κ will remain exactly the
same for an instance xc
(i) given it now has a different prediction. In the case of κ being
softmax, it assumes the model’s logits have changed in a very precise and nontrivial manner.
Additionally, by our broad deﬁnition of κ, which allows κ to even be produced from an
entirely different model than f, κ receives the prediction and model as a given input (and
cannot change or affect neither), and it is unlikely to assume changing its inputs will not
change its output."
C,0.6277665995975855,"2. Suppose we ﬁnd the setting reasonable and assume we can actually create a model f m as
described. Let us observe a model f p such that p = minm(AUROC of f m=1), meaning
that f p ranks its predictions perfectly, unlike the original f 0. Is it really true that f p has
no better uncertainty estimation than f 0? Model f p behaves very much like the investment
“Model B” from our example in Section 1, possessing perfect knowledge of when it is
wrong and when it is correct, allowing its users risk-free classiﬁcation. So, given a model
f, we can use the above process to produce an improved model f p, and then we can even
calibrate its κ to output 0% for all instances below its threshold and 100% for all those
above to produce a perfect model, which might have a small coverage but is correct every
time, knows it and notiﬁes its user when it truly knows the prediction. The increase in
AUROC reﬂects such an improvement."
C,0.6297786720321932,"Not only do we disagree with such an analysis and its conclusions, but we also have vast empirical
evidence to show that AUROC does not prefer lower accuracy models unless there is a good reason
for it to do so, as we demonstrate in Figure 2 (comparing EfﬁcientNet-V2-XL to ViT-B/32-SAM).
In fact, out of the 484 models we tested, the model with the highest AUROC has also the 4th highest
accuracy of all models, and the overall Spearman correlation between AUROC and accuracy of
all the models we tested is 0.03. Furthermore, Figure 2 also exempliﬁes why AURC, which was
suggested by the mentioned paper as the alternative to AUROC, is a bad choice as a single number
metric, and might lead us to deploy a model that has a worse selective risk for most coverages only
due to its higher overall accuracy."
C,0.6317907444668008,"G
AFFECTS OF THE MODEL’S ACCURACY, NUMBER OF PARAMETERS AND
INPUT SIZE ON IN-DISTRIBUTION UNCERTAINTY ESTIMATION
PERFORMANCE"
C,0.6338028169014085,"Table 1 shows the relationship between uncertainty estimation performance and model’s attributes
and resources (accuracy, number of parameters and input size), measured by Spearman correlation.
We measure uncertainty estimation performance by AUROC (higher is better) and -ECE (higher
is better). Positive correlations indiciate good utilization of resources for uncertainty estimation
(for example, a positive correlation between -ECE and the number of parameters indicates that as
the number of parameters increases, the calibration improves). An interesting observation is that
distillation can drastically change the correlation between a resource and the uncertainty estimation
performance metrics. For example, undistilled XCiTs have a Spearman correlation of -0.79 between
their number of parameters and AUROC, indicating that more parameters are correlated with lower
ranking performance, while distilled XCiTs have a correlation of 0.35 between the two."
C,0.635814889336016,"H
KNOWLEDGE DISTILLATION EFFECTS ON IN-DISTRIBUTION
UNCERTAINTY ESTIMATION"
C,0.6378269617706237,"Figure 13 compares vanilla models to those incorporating KD into their training (represented by
markers with thick borders and a dot). In a pruning scenario that included distillation, yellow mark-
ers indicate that the original model was also the teacher (Aﬂalo et al., 2020). While distillation using
a different model tends to improve uncertainty estimation in both aspects, distillation by the model
itself seems to improve only one—suggesting it is generally more beneﬁcial to use a different model
as a teacher. The fact that KD improves the model over its original form, however, is surprising, and"
C,0.6398390342052314,Under review as a conference paper at ICLR 2022
C,0.641851106639839,"Table 1: The relationship between uncertainty estimation performance and the model’s attributes
and resources (accuracy, number of parameters and input size), measured by Spearman correlation.
Positive correlations indicate good utilization of resources for uncertainty estimation."
C,0.6438631790744467,"Architecture
AUROC & Accuracy
-ECE & Accuracy
AUROC & #Parameters
-ECE & #Parameters
AUROC & Input Size
-ECE & Input Size
# Models Evaluated"
C,0.6458752515090543,"EfﬁcientNet
-0.16
-0.29
-0.22
-0.29
-0.26
-0.38
50
ResNet
-0.28
-0.22
0.16
0.03
-0.40
-0.44
33
XCiT distilled
0.60
0.09
0.35
0.02
0.51
0.12
28
XCiT
-0.68
0.89
-0.79
0.94
-
-
28
ViT
0.95
-0.62
0.71
-0.78
0.22
-0.27
20
SE ResNet
-0.46
-0.02
-0.53
0.20
-0.02
-0.35
18
EfﬁcientNetV2
-0.70
-0.45
-0.63
-0.47
-0.59
-0.40
15
NFNet
0.56
0.78
0.63
0.81
0.48
0.60
13
Inception
-0.29
0.09
-0.43
0.30
-0.08
0.23
13
RegNetY
-0.03
-0.98
0.27
-0.86
-
-
12
RegNetX
0.20
-0.96
0.20
-0.96
-
-
12
CaiT distilled
0.44
-0.87
0.35
-0.87
0.58
-0.50
10
DLA
0.64
-0.90
0.77
-0.90
-
-
10
MobileNetV3
0.37
0.59
0.42
0.60
-
-
10
Res2Net
-0.70
0.27
-0.68
0.60
-
-
9
VGG
0.81
-0.98
0.71
-0.90
-
-
8
RepVGG
-0.71
0.50
-0.57
0.21
-
-
8
BiT
-0.33
-0.81
-0.20
-0.85
-0.46
-0.25
8
ResNeXt
-0.96
0.39
-0.22
-0.30
-
-
7
ResNet RS
0.00
0.79
-0.18
0.82
-0.30
0.82
7
MixConv
-0.11
0.89
-0.24
0.86
-
-
7
DenseNet
0.43
-0.14
0.72
0.12
-
-
6
HardCoReNAS
-0.60
0.26
-0.49
0.37
-
-
6
Swin
0.71
0.14
0.77
0.26
0.41
0.00
6
ECANet
-0.20
0.60
-0.43
0.37
0.83
0.37
6
Twins
-0.26
0.94
-0.14
0.89
-
-
6
SWSL ResNet
0.94
-0.89
0.77
-0.83
-
-
6
GENet
0.50
-1.00
0.50
-1.00
0.87
-0.87
6
SSL ResNet
0.14
-1.00
0.26
-0.94
-
-
6
TResNet
0.10
-0.30
0.53
0.53
-0.58
-0.87
5
CoaT
-0.10
0.90
-0.10
0.50
-
-
5
LeViT distilled
0.60
-0.90
0.60
-0.90
-
-
5
ResMLP
0.20
1.00
0.15
0.97
-
-
5
MobileNetV2
-0.30
0.00
-0.21
0.10
-
-
5
PiT distilled
1.00
-1.00
1.00
-1.00
-
-
4
PiT
-0.40
1.00
-0.40
1.00
-
-
4
WSP ResNeXt
1.00
0.80
1.00
0.80
-
-
4
ResMLP distilled
0.80
0.20
0.80
0.20
-
-
4
MnasNet
0.40
0.20
0.63
0.95
-
-
4
DeiT distilled
0.80
-1.00
0.80
-1.00
0.77
-0.77
4
DeiT
0.40
0.80
0.40
0.80
0.26
0.26
4"
C,0.647887323943662,"3
4
6
7 83 84 85 86 87 88 5 AUROC"
C,0.6498993963782697,"BiT
BiT distilled
DeiT
DeiT distilled 
ECANet
ECANet distilled 
EfficientNet 
EfficientNet distilled 
MLP Mixer
MLP Mixer distilled 
MobileNetV3 
MobileNetV3 distilled 
PiT
PiT distilled 
ResMLP
ResMLP distilled 
TResNet
TResNet distilled 
XCiT
XCiT distilled"
C,0.6519114688128773,Models
C,0.6539235412474849,-log(ECE)
C,0.6559356136820925,"Figure 13: Comparing vanilla models to those incorporating KD into their training (represented
by markers with thick borders and a dot). In a pruning scenario that included distillation, yellow
markers indicate that the original model was also the teacher. The performance of each model is
measured in AUROC (higher is better) and -log(ECE) (higher is better)."
C,0.6579476861167002,"suggests the distillation process itself helps uncertainty estimation. Note that although this speciﬁc
method involves pruning, evaluations of models pruned without incorporating distillation (Frankle
& Carbin, 2018) revealed no improvement."
C,0.6599597585513078,"Moreover, it seems that the teacher does not have to be good in uncertainty estimation itself; Fig-
ure 14 shows this by comparing the teacher architecture and the students in each case."
C,0.6619718309859155,Under review as a conference paper at ICLR 2022
C,0.6639839034205232,"2.5
3
3.5
4
4.5
5
5.5
6
6.5
7 83 84 85 86 87"
C,0.6659959758551308,-log(ECE) AUROC
C,0.6680080482897385,Models
C,0.670020120724346,"BiT
BiT distilled
DeiT distilled 
ECANet
ECANet distilled 
EfficientNet 
EfficientNet distilled 
PiT distilled 
RegNetY-16GF 
ResMLP distilled 
XCiT distilled"
C,0.6720321931589537,"Figure 14: Comparing teacher models (yellow markers) to their KD students (represented by mark-
ers with thick borders and a dot). The performance of each model is measured in AUROC (higher
is better) and -log(ECE) (higher is better)."
C,0.6740442655935613,"While the training method by Ridnik et al. (2021) included pretraining on ImageNet-21k and demon-
strated impressive improvements, comparison of models that were pretrained on ImageNet21k (Tan
& Le, 2021; Touvron et al., 2021a) with identical models that were not pretrained showed no clear
improvement in ECE, and, in fact, exhibit a degradation of AUROC (see Figures 3 and 4 in Sec-
tion 3). This suggests that pretraining alone does not improve uncertainty estimation."
C,0.676056338028169,"I
MORE INFORMATION ABOUT TEMPERATURE SCALING"
C,0.6780684104627767,"4
4.5
5
5.5
6
6.5
7 80 82 84 86 88"
C,0.6800804828973843,-log(ECE) with Temperature Scaling
C,0.682092555331992,AUROC with Temperature Scaling
C,0.6841046277665996,Models
C,0.6861167002012073,"ResMLP
ResMLP distilled 
ResNet
ViT
ViT distilled
ViT SAM
XCiT
XCiT distilled"
C,0.6881287726358148,"Various
AlexNet
BiT
BiT distilled 
EfficientNetV2 
HardCoReNAS
MLP Mixer
MLP Mixer distilled 
RegNetY"
C,0.6901408450704225,"Figure 15: A comparison of 484 models after being calibrated with TS, evaluated by their AUROC
(×100, higher is better) and -log(ECE) (higher is better) on ImageNet. Each marker’s size is de-
termined by the model’s number of parameters. ViT models are still among the best performing
architectures for all aspects of uncertainty estimation."
C,0.6921529175050302,Under review as a conference paper at ICLR 2022
C,0.6941649899396378,"In Figure 15 we see how temperature scaling (TS) affects the overall ranking of models in terms of
AUROC and ECE. While the ranking between the different architecture remains similar, the poorly
performing models are much improved and minimize the gap between them and the best models.
One particularly notable exception is HardCoRe-NAS (Nayman et al., 2021), with its lowest latency
versions becoming the top performers in terms of ECE. In addition, models that beneﬁt from TS in"
C,0.6961770623742455,"Temperature Scaling (Temp. < 1)
Temperature Scaling (Temp. > 1) −2 −1 0 1 2 3 4 5 6"
C,0.6981891348088531,AUROC Improvement over Vanilla
C,0.7002012072434608,Method
C,0.7022132796780685,"Temperature Scaling (Temp. < 1) 
Temperature Scaling (Temp. > 1)"
C,0.704225352112676,"Figure 16: Out of 484 models evaluated, models that were assigned a temperature higher than 1
by the calibration process tended to degrade in AUROC performance rather than improve. Markers
above the x axis represent models that beneﬁted from TS, and vice versa."
C,0.7062374245472837,"Temperature Scaling (Temp. < 1)
Temperature Scaling (Temp. > 1) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4"
C,0.7082494969818913,ECE Improvement over Vanilla
C,0.710261569416499,Method
C,0.7122736418511066,"Temperature Scaling (Temp. < 1) 
Temperature Scaling (Temp. > 1)"
C,0.7142857142857143,"Figure 17: The relationship between temperature and the success of TS, unlike the case for AUROC,
seems unrelated."
C,0.716297786720322,"terms of AUROC tend to have been assigned a temperature lower than 1 by the calibration process
(see Figure 16). The same, however, does not hold true for ECE (see Figure 17). This example also
emphasizes the fact that models beneﬁtting from TS in terms of AUROC do not necessarily beneﬁt
in terms of ECE, and vice versa. Therefore, determining whether to calibrate the deployed model
with TS is, unfortunately, a task-speciﬁc decision."
C,0.7183098591549296,"We conduct TS as was suggested in Guo et al. (2017). For each model we take a random strat-
iﬁed sampling of 5,000 instances from the ImageNet validation set to calibrate on, and reserve
the remainder 45,000 instances for testing. Using the box-constrained L-BFGS (Limited-Memory"
C,0.7203219315895373,Under review as a conference paper at ICLR 2022
C,0.7223340040241448,"Broyden-Fletcher-Goldfarb-Shanno) algorithm, we optimize for 5,000 iterations (though fewer iter-
ations usually converge into the same temperature parameter) using a learning rate of 0.01."
C,0.7243460764587525,"J
ARCHITECTURE CHOICE FOR PRACTICAL DEPLOYMENT BASED ON
SELECTIVE PERFORMANCE"
C,0.7263581488933601,"As discussed in Section 2, when we know the coverage or risk we require for deployment, the most
direct metric to check is which model obtains the best risk for the coverage required (selective risk),
or which model gets the largest coverage for the accuracy constraint (SAC). While each deployment
scenario speciﬁes its own constraints, for demonstration purposes we consider a scenario in which
misclassiﬁcations are by far more costly than abstaining from giving correct predictions. An exam-
ple for this could be classifying a huge unlabeled dataset (or for cleaning bad labels from a labeled
dataset). While it is desirable to assign labels to a larger portion of the dataset (or to correct more of
the wrong labels), it is crucial that these labels are as accurate as possible (or that correctly labeled
instances are not replaced with a bad label)."
C,0.7283702213279678,"0
10
20
30
40
50 6 6.5 7 7.5 8 8.5 9"
C,0.7303822937625755,Coverage for Accuracy 99
C,0.7323943661971831,log(#Parameters)
C,0.7344064386317908,"Models
Various 
AlexNet 
BiT 
DLA
RegNetX"
C,0.7364185110663984,"RegNetY 
ResNet 
SWSL ResNeXt 
ViT 
ViT SAM"
C,0.738430583501006,"Figure 18: A comparison of 484 models by their log(number of model’s parameters) and the cover-
age they are able to provide for a SAC of 99% (higher is better) on ImageNet."
C,0.7404426559356136,"To explore such a scenario, we evaluate all models on ImageNet to see which ones give us the largest
coverage for a required accuracy of 99%. In Figure 5, Section 3 (paper’s main body) we observe
that of all the models studied, only ViT models are able to provide coverage beyond 30% for such
an extreme constraint. Moreover, we note that the coverage they provide is signiﬁcantly larger than
that given by models with comparable accuracy or size, and that ViT models that provide similar
coverage to their counterparts do so with less overall accuracy."
C,0.7424547283702213,"In Figure 18 we see that not only do ViT models provide more coverage than any other model, but
they are also able to do so in any size category. To compare models fairly by their size, we present
Figure 18, which sets the Y axis to be the logarithm of the number of parameters, so that models
sharing the same y value can be compared solely based on their x value—which is the coverage they
provide for a SAC of 99%. We see that ViT models provide a larger coverage even when compared
with models of a similar size."
C,0.744466800804829,"K
EVALUATIONS OF MONTE-CARLO DROPOUT IN-DISTRIBUTION RANKING
PERFORMANCE"
C,0.7464788732394366,"MC-Dropout (Gal & Ghahramani, 2016) is computed using several dropout-enabled forward passes
to produce uncertainty estimates. In classiﬁcation, the mean softmax score of these passes is calcu-"
C,0.7484909456740443,Under review as a conference paper at ICLR 2022
C,0.7505030181086519,Table 2: Comparing using MC-Dropout to softmax-response (vanilla).
C,0.7525150905432596,"Architecture
Method
Accuracy
AUROC"
C,0.7545271629778671,"Vanilla
74.04
86.88
MobileNetV3 Large
MC-Dropout
74
86.14"
C,0.7565392354124748,"Vanilla
67.67
86.2
MobileNetV3 Small
MC-Dropout
67.55
84.54"
C,0.7585513078470825,"Vanilla
71.88
86.05
MobileNetV2
MC-Dropout
71.81
84.68"
C,0.7605633802816901,"Vanilla
70.37
86.31
VGG11
MC-Dropout
70.21
84.3"
C,0.7625754527162978,"Vanilla
69.02
86.19
VGG11 (no BatchNorm)
MC-Dropout
68.95
83.94"
C,0.7645875251509054,"Vanilla
71.59
86.3
VGG13
MC-Dropout
71.43
84.37"
C,0.7665995975855131,"Vanilla
69.93
86.24
VGG13 (no BatchNorm)
MC-Dropout
69.71
84.3"
C,0.7686116700201208,"Vanilla
73.36
86.76
VGG16
MC-Dropout
73.33
85.02"
C,0.7706237424547284,"Vanilla
71.59
86.63
VGG16 (no BatchNorm)
MC-Dropout
71.47
84.97"
C,0.772635814889336,"Vanilla
74.22
86.52
VGG19
MC-Dropout
74.17
85.06"
C,0.7746478873239436,"Vanilla
72.38
86.55
VGG19 (no BatchNorm)
MC-Dropout
72.37
84.99"
C,0.7766599597585513,"lated, and then a predictive entropy score is used as the ﬁnal uncertainty estimate. In our evaluations,
we use 30 dropout-enabled forward passes. We do not measure MC-Dropout’s effect on ECE since
entropy scores do not reside in [0, 1]."
C,0.778672032193159,"We test this technique using MobileNetV3 (Howard et al., 2019), MobileNetv2 (Sandler et al., 2019)
and VGG (Simonyan & Zisserman, 2014), all trained on ImageNet and taken from the PyTorch
repository (Paszke et al., 2019)."
C,0.7806841046277666,The results comparing these models with and without using MC-Dropout are provided in Table 2.
C,0.7826961770623743,"The table shows that using MC-Dropout causes a consistent drop in both AUROC and selective
performance compared with using the same models with softmax as the κ. These results are also
visualized in comparison to other methods in Figure 3 in Section 3. MC-Dropout underperformance
was also previously observed in (Geifman & El-Yaniv, 2017)."
C,0.7847082494969819,"L
CONSTRUCTING C-OOD DATASET PER MODEL"
C,0.7867203219315896,"Given model f, conﬁdence function κ and a large set of classes YOOD (in our case, ImageNet-
21K), our goal is to build multiple datasets from YOOD that can be used in evaluating the model’s
performance in C-OOD detection. For that we start by pre-processing O."
C,0.7887323943661971,"L.1
PRE-PROCESSING IMAGENET-21K"
C,0.7907444668008048,"Since ImageNet-21K contains our ID dataset (ImageNet-1K), the ﬁrst step will be to remove all
1K classes from ImageNet-21K. After that, as a cautionary step, we remove all classes that are
hypernyms or hyponyms of classes in ImageNet-1K because they might be unfair to include as an
OOD class. For example, ImageNet-1K contains the class “brown bear”, and ImageNet-21K has the
class “bear” which is a hypernym for “brown bear” so it would not be fair to include it in a C-OOD
detection test. After cleaning trivial classes, we remove classes with a low number of samples; we
set our threshold to 200 samples. For classes with more than 200 samples we randomly select 200
samples and remove the rest. At the end of this process we are left with 12697 classes containing
200 samples each. We divide the samples in each class cy into 150 ‘estimation’ samples (denoted by
cy
est) and 50 ‘test’ samples (denoted by cy
test). The estimation samples will later be used to estimate"
C,0.7927565392354124,Under review as a conference paper at ICLR 2022
C,0.7947686116700201,"the difﬁculty of the class and the test samples will be used to check the C-OOD performance if the
class was chosen."
C,0.7967806841046278,"L.2
CONSTRUCTION"
C,0.7987927565392354,"Having conducted the pre-processing, we continue to compute the 11 severity levels (C-OOD
datasets) for the given model f (with its given κ)."
C,0.8008048289738431,"We deﬁne the severity score of class y to (f, κ) as follows:"
C,0.8028169014084507,"s(y|f, κ) =
1
|cy| X"
C,0.8048289738430584,"x∈cy
est
κ(x|f)."
C,0.806841046277666,We also deﬁne the severity score for a given group of classes g as:
C,0.8088531187122736,"s(g|f, κ) = 1 |g| X"
C,0.8108651911468813,"y∈g
s(y|f, κ)."
C,0.8128772635814889,"Such a deﬁnition of s is intuitive because it assumes that samples that the model is highly conﬁdent
of are hard for it to distinguish from ID samples. We choose the size of each C-OOD dataset to be
the same as the size of the of the ID dataset, 1000 classes. The number of subgroups in YOOD of
size 1000 is huge (
 12697
1000

= 3.5 × 101518 groups), so instead of going over every possible group
of classes, we choose to sort the classes by their severity score and then use a sliding window of
size 1000 to deﬁne resulting in 11698 groups of classes with increasing severities. This choice for
reducing the number of considered groups of classes was chosen for its simplicity. Using s(g|f, κ)
we calculate the severity score of each group, and sort them accordingly. Finally we choose the 11
groups to be the groups that correspond to the percentiles {10·i}i=10
i=0 in the sorted groups array. This
way we build the C-OOD dataset of severity level i from the test samples of classes in group i. This
heuristic procedure for choosing groups allows us to interpret the severity levels with percentiles.
For example, severity level 5 contains classes that have the median severity among the considered
groups."
C,0.8148893360160966,"The reason for choosing the number of classes in each group to be the same as the number of classes
in the ID dataset (1000 classes per group) is because we wanted the C-OOD dataset to be equal in
size to the ID dataset."
C,0.8169014084507042,"M
LIST OF C-OOD OBSERVATIONS"
C,0.8189134808853119,In this section we list additional ﬁndings regarding C-OOD detection.
C,0.8209255533199196,"M.1
PER-SIZE COMPARISON"
C,0.8229376257545271,"The scatter plot in Figure 19 shows the relationship between the # of architecture parameters and its
C-OOD AUROC performance. Overall, there is a moderate Spearman correlation of 0.45 between
#parameters and the C-OOD performance when considering all tested networks. When grouping
the networks by architecture families, however, we see that some architectures have high correlation
between their model size and their C-OOD AUROC. Architecture families that exhibit this behavior
are, for example, ViTs, Swins, EffecientNetV2 and ResNets whose correlations are 0.91, 0.94, 0.89,
and 0.79, respectively. Other families exhibit moderate correlations, e.g., EffecientNet(V1) with a
0.47 Spearman correlation. Some architectures, on the other hand, have strong negative correlation,
e.g., Twins (Chu et al., 2021), NesT (Zhang et al., 2020) and Res2Net (Gao et al., 2021), whose
correlations are -0.94,-1.0, and -0.85, respectively."
C,0.8249496981891348,"Additionally, we note that ViT models are also the best even when considering a model size limita-
tion similar to ResNet-50 or ResNet-101."
C,0.8269617706237424,"M.2
HIGH CORRELATION WITH ACCURACY"
C,0.8289738430583501,"Similarly, the scatter plot in Figure 20 shows the relationship between the architecture validation
accuracy and its C-OOD AUROC performance. Based on their apparent correlation, increasing"
C,0.8309859154929577,Under review as a conference paper at ICLR 2022
C,0.8329979879275654,"13
14
15
16
17
18
19
20
21 70 75 80 85 90"
C,0.8350100603621731,Models
C,0.8370221327967807,"Various (0.45, 318)
AlexNet (-, 1)
ResNet (0.79, 33)
EfficientNet (0.47, 52)
Twins (-0.94, 6)
ViT (0.91, 21)
MLP Mixer (-0.32, 4)
NesT (-1.00, 3)
ShuffleNetV2 (1.00, 2)
SqueezeNet (-1.00, 2)
Swin (0.94, 6)
Res2Net (-0.85, 9)
EfficientNetV2 (0.89, 15)
BiT (0.64, 9)"
C,0.8390342052313883,log10 Parameters #
C,0.8410462776659959,C-OOD AUROC (detection)
C,0.8430583501006036,Spearman correlation 0.45
C,0.8450704225352113,"Figure 19: Number of architecture parameters vs. C-OOD AUROC performance at severity level 5
(median severity). The pair of numbers next to each architecture name at the legend correspond to its
Spearman correlation and the number of models tested from that architecture (family), respectively.
Note that ViT transformers are also the best when considering a model size limitation. Vertical lines
indicate the sizes of ResNet-50 (left vertical line) and ResNet-101 (right vertical line)."
C,0.8470824949698189,"55
60
65
70
75
80
85
90 70 75 80 85 90"
C,0.8490945674044266,Models
C,0.8511066398390342,"Various (0.61, 327)
AlexNet (-, 1)
ResNet (0.73, 33)
EfficientNet (0.60, 52)
Twins (-1.00, 6)
ViT (0.88, 21)
MLP Mixer (0.80, 4)
NesT (-1.00, 3)
ShuffleNetV2 (1.00, 2)
SqueezeNet (1.00, 2)
Swin (0.89, 6)
EfficientNetV2 (0.89, 15)
BiT (0.63, 9)"
C,0.8531187122736419,Accuracy
C,0.8551307847082495,C-OOD AUROC (detection)
C,0.8571428571428571,Spearman correlation 0.65
C,0.8591549295774648,"Figure 20: Architecture accuracy vs. C-OOD AUROC performance. In the legend, the pair of
numbers next to each architecture name correspond to the Spearman correlation and the number
of networks tested from that architecture (family), respectively. Accuracy appears to have a high
correlation with the C-OOD detection performance, with a Spearman correlation of 0.65. Most
architectures also hold this general trend except for Nest and Twins. Next to each architecture we
report the Spearman correlation value and the number of networks tested from that architecture."
C,0.8611670020120724,"accuracy could indicate better C-OOD detection performance. When grouping the networks by
architecture, we notice that most architectures also hold this trend. In Appendix M.7, we examine
how the correlation between C-OOD AUROC and accuracy (among other metrics) change with
severity."
C,0.8631790744466801,"M.3
TRAINING REGIME EFFECTS"
C,0.8651911468812877,"We evaluate the effect of several training regimes on C-OOD performance at various severity levels.
The regimes we consider are: (1) Training that involves knowledge distillation in any form; (2) Ad-
versarial training; (3) Pretraining on ImageNet21k; (4) Various forms of weakly or semi-supervised"
C,0.8672032193158954,Under review as a conference paper at ICLR 2022
C,0.869215291750503,"s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10 −8 −6 −4 −2 0 2 4"
C,0.8712273641851107,Training regimes:
C,0.8732394366197183,"vanilla 
vanilla 
vanilla"
C,0.8752515090543259,"- Distillation 
- WSP
- SSL
vanilla 
vanilla"
C,0.8772635814889336,"- Adversarial Training
- ImageNet21k Pretraining"
C,0.8792756539235412,Severity Levels
C,0.8812877263581489,Improvement %
C,0.8832997987927566,"Figure 21: The average relative improvement when using distillation, pretraining, semi-supervised
learning and adversarial training. The shaded green area indicates the area of positive improvement."
C,0.8853118712273642,"learning, including noisy student (Xie et al., 2020) and semi-supervised pretraining (Yalniz et al.,
2019), which use extra unlabeled data; (5) Weakly supervised training (Mahajan et al., 2018a),
which uses billions of weakly labeled Instagram images. The relative improvement generated by
each one of the training regimes is depicted in Figure 21. We observe that distillation has a positive
improvement on C-OOD detection performance on all severity levels, adversarial training has mini-
mal effects on performance, and, somewhat surprisingly, training regimes that include pretraining on
larger datasets (pretraining on ImageNet-21K and WSP) does not hinder performance in detecting
OOD classes at lower severity levels, but degrades performance on higher severities—which is what
we originally expected (due to exposure to the OOD classes in training)."
C,0.8873239436619719,"M.4
ENTROPY VS. SOFTMAX"
C,0.8893360160965795,"s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10 −8 −6 −4 −2 0 2 4"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.8913480885311871,"6
Improvement by using entropy instead of softmax"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.8933601609657947,Severity Levels
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.8953722334004024,improvement % in C-OOD AUROC (detection)
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.89738430583501,"Figure 22: Relative improvement gain in C-OOD detection performance when using entropy instead
of the softmax conﬁdence signal. In median network terms, entropy offers positive improvement
over softmax in most serverities except s ∈{7, 8, 9}. The green shaded area indicates the area of
positive improvement."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.8993963782696177,"In our research we evaluated entropy as an alternative conﬁdence rate signal (κ) for C-OOD detec-
tion.1 For each network f we re-run the algorithm described in Section L for extracting the classes"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9014084507042254,"1Entropy is maximal when the distribution given by the network for P(y|x) is uniform, which implies high
uncertainty. To convert entropy into a conﬁdence signal we use negative entropy."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.903420523138833,Under review as a conference paper at ICLR 2022
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9054325955734407,"for the 11 severity levels for (f, κentropy) and use them to benchmark the C-OOD detection for
(f, κentropy) (Note that using the same C-OOD groups produced when using softmax might yield
an unfair advantage to entropy). We compare the performance gain from switching to using entropy
instead of the softmax score. The results are depicted using box-plots in Figure 22. We notice that
in most cases using entropy improves the detection performance."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9074446680080482,"M.5
MONTE-CARLO DROPOUT FOR C-OOD DETECTION"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9094567404426559,"We evaluate MC-Dropout (Gal & Ghahramani, 2016) in the context of C-OOD detection. We use
30 dropout-enabled forward passes. The mean softmax score of these passes is calculated, and then
a predictive entropy score is used as the ﬁnal uncertainty estimate. We test this technique using
MobileNetV3 (Howard et al., 2019), MobileNetv2 (Sandler et al., 2019) and VGG (Simonyan &
Zisserman, 2014), all trained on ImageNet and taken from the PyTorch repository (Paszke et al.,
2019)."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9114688128772636,"For each of these three architectures we re-run the algorithm described in Section L to extract the
classes for all 11 severity levels for (f, κMC-dropout) and use them to benchmark the C-OOD detection
for (f, κMC-dropout)."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9134808853118712,"The results are depicted using box-plots in Figure 23. We ﬁnd that MC-Dropout improves perfor-
mance at lower severity levels but degrades it at higher ones (with a few exceptions). We further
analyze MC-dropout and recall that it is composed of two main components: (1) dropout-enabled
forward passes (2) entropy of the mean probability vector from the forward passes. To test which
component contributes the most to the perceived gains, we compare the C-OOD detection perfor-
mance when using MC-dropout to the C-OOD detection performance when using entropy. We ﬁnd
that MC-dropout fails to improve upon entropy in most cases across all severity levels. This implies
that MC-Dropout owes its C-OOD success to entropy and not to its forward passes. These results
can be seen in Figure 24."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9154929577464789,"s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10 −25 −20 −15 −10 −5 0 5 10"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9175050301810865,Improvement by using MC-dropout instead of softmax
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9195171026156942,Severity Levels
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9215291750503019,Improvement % in C-OOD AUROC (detection)
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9235412474849095,"Figure 23: Relative improvement gain in C-OOD detection performance when using MC-Dropout
instead of softmax conﬁdence signal. We ﬁnd that MC-dropout improves performance for most
networks in severities up to s5, and degrades performance for most networks in higher ones. Some
outlier networks get a signiﬁcant improvement when switching to MC-dropout at high severity lev-
els."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9255533199195171,"M.6
CORRELATION BETWEEN RANKINGS OF MULTIPLE SEVERITY LEVELS"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9275653923541247,"Since we essentially have multiple benchmarks C-OOD datasets (i.e., the 11 severity levels) to
test the performance models in C-OOD detection, and each severity level may rank the models
differently, we now consider the question of how does these ranking change across severity levels.
To this end we calculated the correlations between the rankings obtained at different severity levels.
The resulting correlation matrix can be seen in Figure 25. Overall we observe high correlations,"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9295774647887324,Under review as a conference paper at ICLR 2022
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.93158953722334,"s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
−30 −25 −20 −15 −10 −5 0 5 10"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9336016096579477,Improvement by using MC-dropout instead of entropy
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9356136820925554,Severity Levels
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.937625754527163,Improvement % in C-OOD AUROC (detection)
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9396378269617707,"Figure 24: Relative improvement gain in C-OOD detection performance when using MC-dropout
entropy conﬁdence signal. We see that MC-dropout fails to improve upon entropy in most cases
across all severity levels. This suggests that the main component in MC-dropout beneﬁting detection
is the usage of entropy."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9416498993963782,"which means that different severity levels generally yield similar rankings of the models. We also
notice that for each severity level si, the correlation with sj is higher the closer j is to i. This is
not surprising and might be anticipated because adjacent severity levels have close severity score by
design."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9436619718309859,"s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10 s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9456740442655935,Spearman correlation
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9476861167002012,"1.0
0.965
0.928
0.882
0.854
0.846
0.831
0.815
0.787
0.69
0.376"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9496981891348089,"0.965
1.0
0.986
0.955
0.931
0.918
0.9
0.879
0.846
0.739
0.416"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9517102615694165,"0.928
0.986
1.0
0.985
0.968
0.955
0.938
0.915
0.883
0.779
0.461"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9537223340040242,"0.882
0.955
0.985
1.0
0.991
0.98
0.966
0.946
0.916
0.824
0.513"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9557344064386318,"0.854
0.931
0.968
0.991
1.0
0.993
0.983
0.969
0.944
0.86
0.549"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9577464788732394,"0.846
0.918
0.955
0.98
0.993
1.0
0.993
0.983
0.963
0.888
0.576"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.959758551307847,"0.831
0.9
0.938
0.966
0.983
0.993
1.0
0.992
0.979
0.913
0.603"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9617706237424547,"0.815
0.879
0.915
0.946
0.969
0.983
0.992
1.0
0.989
0.939
0.634"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9637826961770624,"0.787
0.846
0.883
0.916
0.944
0.963
0.979
0.989
1.0
0.961
0.675"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.96579476861167,"0.69
0.739
0.779
0.824
0.86
0.888
0.913
0.939
0.961
1.0
0.812"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9678068410462777,"0.376
0.416
0.461
0.513
0.549
0.576
0.603
0.634
0.675
0.812
1.0"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9698189134808853,"Figure 25: Spearman correlation between the rankings of the models given by different severity
level."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.971830985915493,"M.7
CORRELATIONS OF VARIOUS FACTORS WITH C-OOD DETECTION PERFORMANCE"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9738430583501007,"We searched for factors that could be indicative or correlated with good performance in C-OOD
detection. To this end we measure the correlations of various factors with the C-OOD detection
AUROC performance across all severities. The results can be seen in the graphs of Figure 26. We
observe that accuracy is typically a good indicator of the model’s performance in C-OOD detection
at most severity levels (s0 −s8), with Spearman correlation values in [0.6, 0.7] at those levels. The
next best indicative factors are the ID-AUROC performance, number of parameters, and the input
image size (moderate correlations). Finally, the embedding size is only weakly correlated. Inter-
estingly, ID-AUROC exhibits slightly increasing correlation up to severity s9, and at s10 becomes
the most indicative factor for C-OOD detection. In contrast, all other investigated factors lose their
indicative power at the highest severity levels (s9, s10)."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9758551307847082,Under review as a conference paper at ICLR 2022
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9778672032193159,"s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10 0.1 0.2 0.3 0.4 0.5 0.6 0.7"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9798792756539235,"Accuracy
ID AUROC 
#Parameters 
Embedding Size 
Input Size"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9818913480885312,Severity Levels
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9839034205231388,Spearman Correlation
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9859154929577465,"Figure 26: Spearman correlations between C-OOD detection AUROC and Accuracy, ID-AUROC,
#Parameters, Input size, Embedding size across all severity levels."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9879275653923542,"M.8
ANALYSING THE C-OOD CLASSES CHOSEN BY EACH MODEL"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9899396378269618,"When analysing the classes chosen by the algorithm (described in Section L) for the various models
and severity levels, we found that for each severity level, the union of the classes chosen for all
models (for that severity level) spans all the classes in YOOD (namely, all the classes in ImageNet-
21K after ﬁltration). This implies that hard OOD classes for one model can easy for others."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9919517102615694,"In addition, for each severity level (except s10) there is no single class that is shared between all
models. However, in s10 we ﬁnd that there are some shared classes between all models. Moreover,
there are speciﬁc instances (images) in those classes that obtain the same wrong prediction from
many models; one interesting example for this shared wrong prediction of 105 models appears in
Figure 27, where the butterﬂy image is wrongly classiﬁed as a fox squirrel."
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.993963782696177,"Input image
Predicted class"
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9959758551307847,(Fox Squirrel)
IMPROVEMENT BY USING ENTROPY INSTEAD OF SOFTMAX,0.9979879275653923,Figure 27: An instance of viceroy butterﬂy predicted to be a fox squirrel by 105 models.
