Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0006426735218508997,"Linear Mode Connectivity (LMC) refers to the phenomenon that performance
1"
ABSTRACT,0.0012853470437017994,"remains consistent for linearly interpolated models in the parameter space. For
2"
ABSTRACT,0.0019280205655526992,"independently optimized model pairs from different random initializations, achiev-
3"
ABSTRACT,0.002570694087403599,"ing LMC is considered crucial for validating the stable success of the non-convex
4"
ABSTRACT,0.003213367609254499,"optimization in modern machine learning models and for facilitating practical
5"
ABSTRACT,0.0038560411311053984,"parameter-based operations such as model merging. While LMC has been achieved
6"
ABSTRACT,0.004498714652956298,"for neural networks by considering the permutation invariance of neurons in each
7"
ABSTRACT,0.005141388174807198,"hidden layer, its attainment for other models remains an open question. In this
8"
ABSTRACT,0.005784061696658098,"paper, we first achieve LMC for soft tree ensembles, which are tree-based differen-
9"
ABSTRACT,0.006426735218508998,"tiable models extensively used in practice. We show the necessity of incorporating
10"
ABSTRACT,0.007069408740359897,"two invariances: subtree flip invariance and splitting order invariance, which do
11"
ABSTRACT,0.007712082262210797,"not exist in neural networks but are inherent to tree architectures, in addition to
12"
ABSTRACT,0.008354755784061696,"permutation invariance of trees. Moreover, we demonstrate that it is even possible
13"
ABSTRACT,0.008997429305912597,"to exclude such additional invariances while keeping LMC by designing decision
14"
ABSTRACT,0.009640102827763496,"list-based tree architectures, where such invariances do not exist by definition. Our
15"
ABSTRACT,0.010282776349614395,"findings indicate the significance of accounting for architecture-specific invariances
16"
ABSTRACT,0.010925449871465296,"in achieving LMC.
17"
INTRODUCTION,0.011568123393316195,"1
Introduction
18"
INTRODUCTION,0.012210796915167094,"A non-trivial empirical characteristic of modern machine learning models trained using gradient
19"
INTRODUCTION,0.012853470437017995,"methods is that models trained from different random initializations could become functionally
20"
INTRODUCTION,0.013496143958868894,"almost equivalent, even though their parameter representations differ. If the outcomes of all training
21"
INTRODUCTION,0.014138817480719794,"sessions converge to the same local minima, this empirical phenomenon can be understood. However,
22"
INTRODUCTION,0.014781491002570694,"considering the complex non-convex nature of the loss surface, the optimization results are unlikely to
23"
INTRODUCTION,0.015424164524421594,"converge to the same local minima. In recent years, particularly within the context of neural networks,
24"
INTRODUCTION,0.016066838046272493,"the transformation of model parameters while preserving functional equivalence has been explored by
25"
INTRODUCTION,0.016709511568123392,"considering the permutation invariance of neurons in each hidden layer [1, 2]. Notably, only a slight
26"
INTRODUCTION,0.017352185089974295,"performance degradation has been observed when using weights derived through linear interpolation
27"
INTRODUCTION,0.017994858611825194,"between permuted parameters obtained from different training processes [3, 4]. This demonstrates
28"
INTRODUCTION,0.018637532133676093,"that the trained models reside in different, yet functionally equivalent, local minima. This situation is
29"
INTRODUCTION,0.019280205655526992,"referred to as Linear Mode Connectivity (LMC) [5]. From a theoretical perspective, LMC is crucial
30"
INTRODUCTION,0.01992287917737789,"for supporting the stable and successful application of non-convex optimization. In addition, LMC
31"
INTRODUCTION,0.02056555269922879,"also holds significant practical importance, enabling techniques such as model merging [6, 7] by
32"
INTRODUCTION,0.021208226221079693,"weight-space parameter averaging.
33"
INTRODUCTION,0.021850899742930592,"Although neural networks are most extensively studied among the models trained using gradient
34"
INTRODUCTION,0.02249357326478149,"methods, other models also thrive in real-world applications. A representative is tree ensemble models,
35"
INTRODUCTION,0.02313624678663239,"such as random forests [8]. While they are originally trained by not gradient but greedy algorithms,
36"
INTRODUCTION,0.02377892030848329,"differentiable soft tree ensembles, which learn parameters of the entire model through gradient-based
37"
INTRODUCTION,0.02442159383033419,"optimization, have recently been actively studied. Not only empirical studies regarding accuracy
38"
INTRODUCTION,0.02506426735218509,"and interpretability [9–11], but also theoretical analyses have been performed [12, 13]. Moreover,
39"
INTRODUCTION,0.02570694087403599,"the differentiability of soft trees allows for integration with various deep learning methodologies,
40"
INTRODUCTION,0.02634961439588689,"including fine-tuning [14], dropout [15], and various stochastic gradient descent methods [16, 17].
41"
INTRODUCTION,0.02699228791773779,"Furthermore, the soft tree represents the most elementary form of a hierarchical mixture of experts [18–
42"
INTRODUCTION,0.027634961439588688,"20]. Investigating soft tree models not only advances our understanding of this particular structure
43"
INTRODUCTION,0.028277634961439587,"but also contributes to broader research into essential technological components critical for the
44"
INTRODUCTION,0.02892030848329049,"development of large-scale language models [21].
45"
INTRODUCTION,0.02956298200514139,Only Permutation Ours
INTRODUCTION,0.030205655526992288,Target
INTRODUCTION,0.030848329048843187,Origin
INTRODUCTION,0.031491002570694086,"Figure 1: A representative experimental result on
the MiniBooNE [22] dataset (left) and conceptual
diagram of the LMC for tree ensembles (right)."
INTRODUCTION,0.032133676092544985,"A research question that we tackle in this paper
46"
INTRODUCTION,0.032776349614395885,"is: “Can LMC be achieved for soft tree ensem-
47"
INTRODUCTION,0.033419023136246784,"bles?”. Our empirical results, which are high-
48"
INTRODUCTION,0.03406169665809768,"lighted with a green line in the top left panel
49"
INTRODUCTION,0.03470437017994859,"of Figure 1, clearly show that the answer is
50"
INTRODUCTION,0.03534704370179949,"“Yes”. This plot shows the variation in test accu-
51"
INTRODUCTION,0.03598971722365039,"racy when interpolating weights of soft oblivi-
52"
INTRODUCTION,0.036632390745501286,"ous trees, perfect binary soft trees with shared
53"
INTRODUCTION,0.037275064267352186,"parameters at each depth, trained from differ-
54"
INTRODUCTION,0.037917737789203085,"ent random initializations. The green line is
55"
INTRODUCTION,0.038560411311053984,"obtained by our method introduced in this pa-
56"
INTRODUCTION,0.03920308483290488,"per, where there is almost zero performance
57"
INTRODUCTION,0.03984575835475578,"degradation. Furthermore, as shown in the bot-
58"
INTRODUCTION,0.04048843187660668,"tom left panel of Figure 1, the performance can
59"
INTRODUCTION,0.04113110539845758,"even improve when interpolating between mod-
60"
INTRODUCTION,0.04177377892030849,"els trained on split datasets.
61"
INTRODUCTION,0.042416452442159386,"The key insight is that, when performing interpolation between two model parameters, considering
62"
INTRODUCTION,0.043059125964010285,"only tree permutation invariance, which corresponds to the permutation invariance of neural networks,
63"
INTRODUCTION,0.043701799485861184,"is not sufficient to achieve LMC, as shown in the orange lines in the plots. An intuitive understanding
64"
INTRODUCTION,0.04434447300771208,"of this situation is also illustrated in the right panel of Figure 1. To achieve LMC, that is, the green
65"
INTRODUCTION,0.04498714652956298,"lines, we show that two additional invariances beyond tree permutation, subtree flip invariance and
66"
INTRODUCTION,0.04562982005141388,"splitting order invariance, which inherently exist for tree architectures, should be accounted for.
67"
INTRODUCTION,0.04627249357326478,"Moreover, we demonstrate that it is possible to exclude such additional invariances while preserving
68"
INTRODUCTION,0.04691516709511568,"LMC by modifying tree architectures. We realize such an architecture based on a decision list, a
69"
INTRODUCTION,0.04755784061696658,"binary tree structure where branches extend in only one direction. By designating one of the terminal
70"
INTRODUCTION,0.04820051413881748,"leaves as an empty node, we introduce a customized decision list that omits both subtree flip invariance
71"
INTRODUCTION,0.04884318766066838,"and splitting order invariance, and empirically show that this can achieve LMC by considering only
72"
INTRODUCTION,0.04948586118251928,"tree permutation invariance. Since incorporating additional invariances is computationally expensive,
73"
INTRODUCTION,0.05012853470437018,"we can efficiently perform weight-space averaging in model merging on our customized decision
74"
INTRODUCTION,0.05077120822622108,"lists.
75"
INTRODUCTION,0.05141388174807198,"Our contributions are summarized as follows:
76"
INTRODUCTION,0.05205655526992288,"• First achievement of LMC for tree ensembles with accounting for additional invariances beyond
77"
INTRODUCTION,0.05269922879177378,"tree permutation.
78"
INTRODUCTION,0.05334190231362468,"• Development of a decision list-based tree architecture that does not involve the additional invari-
79"
INTRODUCTION,0.05398457583547558,"ances.
80"
INTRODUCTION,0.05462724935732648,"• A thorough empirical investigation of LMC across various tree architectures, invariances, and
81"
INTRODUCTION,0.055269922879177376,"real-world datasets.
82"
PRELIMINARY,0.055912596401028275,"2
Preliminary
83"
PRELIMINARY,0.056555269922879174,"We prepare the basic concepts of LMC and soft tree ensembles.
84"
LINEAR MODE CONNECTIVITY,0.05719794344473008,"2.1
Linear Mode Connectivity
85"
LINEAR MODE CONNECTIVITY,0.05784061696658098,"Let us consider two models, A and B, that have the same architecture. In the context of evaluating
86"
LINEAR MODE CONNECTIVITY,0.05848329048843188,"LMC, the concept of a “barrier” is frequently used [4, 23]. Let ΘA, ΘB ∈RP be vectorized
87"
LINEAR MODE CONNECTIVITY,0.05912596401028278,"parameters of models A and B, respectively, for P parameters. Assume that C : RP →R measures
88"
LINEAR MODE CONNECTIVITY,0.05976863753213368,"the performance of the model, such as accuracy, given its parameter vector. If higher values of C(·)
89"
LINEAR MODE CONNECTIVITY,0.060411311053984576,"mean better performance, the barrier between two parameter vectors ΘA and ΘB is defined as:
90"
LINEAR MODE CONNECTIVITY,0.061053984575835475,"B(ΘA, ΘB) = sup
λ∈[0,1]
[ λC(ΘA) + (1 −λ)C(ΘB) −C(λΘA + (1 −λ)ΘB) ] .
(1)"
LINEAR MODE CONNECTIVITY,0.061696658097686374,"We can simply reverse the subtraction order if lower values of C(·) mean better performance like loss.
91"
LINEAR MODE CONNECTIVITY,0.062339331619537273,"Several techniques have been developed to reduce barriers by transforming parameters while pre-
92"
LINEAR MODE CONNECTIVITY,0.06298200514138817,"serving functional equivalence. Two main approaches are activation matching (AM) and weight
93"
LINEAR MODE CONNECTIVITY,0.06362467866323908,"matching (WM). AM takes the behavior of model inference into account, while WM simply com-
94"
LINEAR MODE CONNECTIVITY,0.06426735218508997,"pares two models using their parameters. The validity of both AM and WM has been theoretically
95"
LINEAR MODE CONNECTIVITY,0.06491002570694088,"supported [24]. Numerous algorithms are available for implementing AM and WM. For instance, [4]
96"
LINEAR MODE CONNECTIVITY,0.06555269922879177,"uses a formulation based on the Linear Assignment Problem (LAP) to find suitable permutations,
97"
LINEAR MODE CONNECTIVITY,0.06619537275064268,"while [23] employs a differentiable formulation that allows for the optimization of permutations using
98"
LINEAR MODE CONNECTIVITY,0.06683804627249357,"gradient-based methods.
99"
LINEAR MODE CONNECTIVITY,0.06748071979434447,"Existing research has focused exclusively on neural network architectures such as multi-layer per-
100"
LINEAR MODE CONNECTIVITY,0.06812339331619537,"ceptrons (MLP) and convolutional neural networks (CNN). No study has been conducted from the
101"
LINEAR MODE CONNECTIVITY,0.06876606683804627,"perspective of linear mode connectivity for soft tree ensembles.
102"
SOFT TREE ENSEMBLE,0.06940874035989718,"2.2
Soft Tree Ensemble
103"
SOFT TREE ENSEMBLE,0.07005141388174807,"Unlike typical hard decision trees, which explicitly determine the data flow to the right or left at each
104"
SOFT TREE ENSEMBLE,0.07069408740359898,"splitting node, soft trees represent the proportion of data flowing to the right or left as continuous
105"
SOFT TREE ENSEMBLE,0.07133676092544987,"values between 0 and 1. This approach enables a differentiable formulation.
106"
SOFT TREE ENSEMBLE,0.07197943444730077,"We use a sigmoid function, σ : R →(0, 1) to formulate a function µm,ℓ(xi, wm, bm) : RF ×
107"
SOFT TREE ENSEMBLE,0.07262210796915167,"RF ×N × R1×N →(0, 1) that represents the proportion of the ith data point xi flowing to the ℓth
108"
SOFT TREE ENSEMBLE,0.07326478149100257,"leaf of the mth tree as a result of soft splittings:
109"
SOFT TREE ENSEMBLE,0.07390745501285347,"µm,ℓ(xi, wm, bm)= N
Y"
SOFT TREE ENSEMBLE,0.07455012853470437,"n=1
σ(w⊤
m,nxi + bm,n)
|
{z
}
flow to the left"
SOFT TREE ENSEMBLE,0.07519280205655526,"1ℓ↙n(1 −σ(w⊤
m,nxi + bm,n))
|
{z
}
flow to the right"
SOFT TREE ENSEMBLE,0.07583547557840617,"1n↘ℓ,
(2)"
SOFT TREE ENSEMBLE,0.07647814910025708,"where N denotes the number of splitting nodes in each tree. The parameters wm,n ∈RF and
110"
SOFT TREE ENSEMBLE,0.07712082262210797,"bm,n ∈R correspond to the feature selection mask and splitting threshold value for nth node in a
111"
SOFT TREE ENSEMBLE,0.07776349614395887,"mth tree, respectively. The expression 1ℓ↙n (resp. 1n↘ℓ) is an indicator function that returns 1 if the
112"
SOFT TREE ENSEMBLE,0.07840616966580977,"ℓth leaf is positioned to the left (resp. right) of a node n, and 0 otherwise.
113"
SOFT TREE ENSEMBLE,0.07904884318766067,"If parameters are shared across all splitting nodes at the same depth, such perfect binary trees are
114"
SOFT TREE ENSEMBLE,0.07969151670951156,"called oblivious trees. Mathematically, wm,n = wm,n′ and bm,n = bm,n′ for any nodes n and n′ at
115"
SOFT TREE ENSEMBLE,0.08033419023136247,"the same depth in an oblivious tree. Oblivious trees can significantly reduce the number of parameters
116"
SOFT TREE ENSEMBLE,0.08097686375321336,"from an exponential to a linear order of the tree depth, and they are actively used in practice [9, 11].
117"
SOFT TREE ENSEMBLE,0.08161953727506427,"To classify C categories, the output of the mth tree is computed by the function fm : RF ×
118"
SOFT TREE ENSEMBLE,0.08226221079691516,"RF ×N × R1×N × RC×L →RC as sum of the leaf parameters πm,ℓweighted by the outputs of
119"
SOFT TREE ENSEMBLE,0.08290488431876607,"µm,ℓ(xi, wm, bm):
120"
SOFT TREE ENSEMBLE,0.08354755784061697,"fm(xi, wm, bm, πm) = L
X"
SOFT TREE ENSEMBLE,0.08419023136246787,"ℓ=1
πm,ℓµm,ℓ(xi, wm, bm),
(3)"
SOFT TREE ENSEMBLE,0.08483290488431877,"where L is the number of leaves in a tree. By combining this function for M trees, we realize the
121"
SOFT TREE ENSEMBLE,0.08547557840616966,"function f : RF × RM×F ×N × RM×1×N × RM×C×L →RC as an ensemble model consisting of
122"
SOFT TREE ENSEMBLE,0.08611825192802057,"M trees:
123"
SOFT TREE ENSEMBLE,0.08676092544987146,"f(xi, w, b, π) = M
X"
SOFT TREE ENSEMBLE,0.08740359897172237,"m=1
fm(xi, wm, bm, πm),
(4)"
SOFT TREE ENSEMBLE,0.08804627249357326,"with the parameters w = (w1, . . . , wM), b = (b1, . . . , bM), and π = (π1, . . . , πM) being ran-
124"
SOFT TREE ENSEMBLE,0.08868894601542417,"domly initialized.
125"
SOFT TREE ENSEMBLE,0.08933161953727506,"(a)
(b)"
SOFT TREE ENSEMBLE,0.08997429305912596,Reordering
SOFT TREE ENSEMBLE,0.09061696658097686,"Leaf Swap
Subtree Flip"
SOFT TREE ENSEMBLE,0.09125964010282776,Inequality Sign Flip
SOFT TREE ENSEMBLE,0.09190231362467867,Figure 2: (a) Subtree flip invariance. (b) Splitting order invariance for an oblivious tree.
SOFT TREE ENSEMBLE,0.09254498714652956,"Despite the apparent differences, there are correspondences between MLPs and soft tree ensemble
126"
SOFT TREE ENSEMBLE,0.09318766066838047,"models. The formulation of a soft tree ensemble with D = 1 is:
127"
SOFT TREE ENSEMBLE,0.09383033419023136,"f(xi, w, b, π) = M
X m=1"
SOFT TREE ENSEMBLE,0.09447300771208227,"
σ(w⊤
m,1xi + bm,1)πm,1 + (1 −σ(w⊤
m,1xi + bm,1))πm,2
 = M
X m=1"
SOFT TREE ENSEMBLE,0.09511568123393316,"
(πm,1 −πm,2)σ(w⊤
m,1xi + bm,1) + πm,2

.
(5)"
SOFT TREE ENSEMBLE,0.09575835475578406,"When we consider the correspondence between πm,1 −πm,2 in tree ensembles and second layer
128"
SOFT TREE ENSEMBLE,0.09640102827763496,"weights in the two-layer perceptron, the tree ensembles model matches to the two-layer perceptron. It
129"
SOFT TREE ENSEMBLE,0.09704370179948586,"is clear from the formulation that the permutation of hidden neurons in a neural network corresponds
130"
SOFT TREE ENSEMBLE,0.09768637532133675,"to the rearrangement of trees in a tree ensemble.
131"
INVARIANCES INHERENT TO TREE ENSEMBLES,0.09832904884318766,"3
Invariances Inherent to Tree Ensembles
132"
INVARIANCES INHERENT TO TREE ENSEMBLES,0.09897172236503857,"In this section, we discuss additional invariances inherent to trees (Section 3.1) and introduce a
133"
INVARIANCES INHERENT TO TREE ENSEMBLES,0.09961439588688946,"matching strategy specifically for tree ensembles (Section 3.2). We also show that the presence of
134"
INVARIANCES INHERENT TO TREE ENSEMBLES,0.10025706940874037,"additional invariances varies depending on the tree structure, and we present tree structures where no
135"
INVARIANCES INHERENT TO TREE ENSEMBLES,0.10089974293059126,"additional invariances beyond tree permutation exist (Section 3.3).
136"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10154241645244216,"3.1
Parameter modification processes that maintains functional equivalence in tree ensembles
137"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10218508997429306,"First, we clarify what invariances should be considered for tree ensembles, which are expected to
138"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10282776349614396,"reduce the barrier significantly if taken into account. When we consider perfect binary trees, there are
139"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10347043701799485,"three types of invariance:
140"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10411311053984576,"• Tree permutation invariance. In Equation (4), the behavior of the function does not change even
141"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10475578406169665,"if the order of the M trees is altered. This corresponds to the permutation of internal nodes in
142"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10539845758354756,"neural networks, which has been a subject of active interest in previous studies on LMC.
143"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10604113110539845,"• Subtree flip invariance. When the left and right subtrees are swapped simultaneously with the
144"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10668380462724936,"inversion of the inequality sign at the split, the functional behavior remains unchanged, which we
145"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10732647814910026,"refer to subtree flip invariance. Figure 2(a) presents a schematic diagram of this invariance, which
146"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10796915167095116,"is not found in neural networks but is unique to binary tree-based models. Since σ(−c) = 1 −σ(c)
147"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10861182519280206,"for c ∈R due to the symmetry of sigmoid, the inversion of the inequality is achieved by inverting
148"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10925449871465295,"the signs of wm,n and bm,n. [25] also focused on the sign of weights, but in a different way from
149"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.10989717223650386,"ours. They pay attention to the amount of change from the parameters at the start of fine-tuning,
150"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11053984575835475,"rather than discussing the sign of the parameters.
151"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11118251928020566,"• Splitting order invariance. Oblivious trees share parameters at the same depth, which means
152"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11182519280205655,"that the decision boundaries are straight lines without any bends. With this characteristic, even if
153"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11246786632390746,"the splitting rules at different depths are swapped, functional equivalence can be achieved if the
154"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11311053984575835,"positions of leaves are also swapped appropriately as shown in Figure 2(b). This invariance does
155"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11375321336760925,"not exist for non-oblivious perfect binary trees without parameter sharing, as the behavior of the
156"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11439588688946016,"decision boundary varies depending on the splitting order.
157"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11503856041131105,"Note that MLPs also have an additional invariance beyond just permutation. Particularly in MLPs
158"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11568123393316196,"that employ ReLU as an activation function, the output of each layer changes linearly with a zero
159"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11632390745501285,"crossover. Therefore, it is possible to modify parameters without changing functional behavior by
160"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11696658097686376,"multiplying the weights in one layer by a constant and dividing the weights in the previous layer by
161"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11760925449871465,"the same constant. However, since the soft tree is based on the sigmoid function, this invariance does
162"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11825192802056556,"not apply. Previous studies [3, 4, 23] have consistently achieved significant reductions in barriers
163"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11889460154241645,"without accounting for this scale invariance. One potential reason is that changes in parameter scale
164"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.11953727506426735,"are unlikely due to the nature of optimization via gradient descent. Conversely, when we consider
165"
PARAMETER MODIFICATION PROCESSES THAT MAINTAINS FUNCTIONAL EQUIVALENCE IN TREE ENSEMBLES,0.12017994858611825,"additional invariances inherent to trees, the scale is equivalent to the original parameters.
166"
MATCHING STRATEGY,0.12082262210796915,"3.2
Matching Strategy
167 8 4
4"
MATCHING STRATEGY,0.12146529562982006,"2
2
2
2 4 3 2"
MATCHING STRATEGY,0.12210796915167095,"Parameter Sharing
Parameter Sharing"
MATCHING STRATEGY,0.12275064267352186,Figure 3: Weighting strategy.
MATCHING STRATEGY,0.12339331619537275,"Here, we propose a matching strategy for bi-
168"
MATCHING STRATEGY,0.12403598971722365,"nary trees. When considering invariances, it
169"
MATCHING STRATEGY,0.12467866323907455,"is necessary to compare multiple functionally
170"
MATCHING STRATEGY,0.12532133676092544,"equivalent trees and select the most suitable one
171"
MATCHING STRATEGY,0.12596401028277635,"for achieving LMC. Although comparing tree
172"
MATCHING STRATEGY,0.12660668380462725,"parameters is a straightforward approach, since
173"
MATCHING STRATEGY,0.12724935732647816,"the contribution of all the parameters in a tree is
174"
MATCHING STRATEGY,0.12789203084832904,"not equal, we apply weighting for each node for
175"
MATCHING STRATEGY,0.12853470437017994,"better matching. By interpreting a tree as a rule
176"
MATCHING STRATEGY,0.12917737789203085,"set with shared parameters as shown in Figure 3,
177"
MATCHING STRATEGY,0.12982005141388175,"we determine the weight of each splitting node
178"
MATCHING STRATEGY,0.13046272493573266,"by counting the number of leaves to which the node affects. For example, in the case of the left
179"
MATCHING STRATEGY,0.13110539845758354,"example in Figure 3, the root node affects eight leaves, nodes at depth 2 affect four leaves, and nodes
180"
MATCHING STRATEGY,0.13174807197943444,"at depth 3 affect two leaves. This strategy can apply to even trees other than perfect binary trees. For
181"
MATCHING STRATEGY,0.13239074550128535,"example, in the right example of Figure 3, the root node affects four leaves, a node at depth 2 affects
182"
MATCHING STRATEGY,0.13303341902313626,"three leaves, and a node at depth 3 affects two leaves.
183"
MATCHING STRATEGY,0.13367609254498714,"In this paper, we employ the LAP, which is used as a standard benchmark [4] for matching algorithms.
184"
MATCHING STRATEGY,0.13431876606683804,"The procedures for AM and WM are as follows. Detailed algorithms (Algorithms 1 and 2) are
185"
MATCHING STRATEGY,0.13496143958868895,"described in Section A in the supplementary material.
186"
MATCHING STRATEGY,0.13560411311053985,"• Activation Matching (Algorithm 1). In trees, there is nothing that directly corresponds to the
187"
MATCHING STRATEGY,0.13624678663239073,"activations in neural networks. However, by treating the output of each individual tree as an
188"
MATCHING STRATEGY,0.13688946015424164,"activation value of a neural network, it is possible to optimize the permutation of trees while
189"
MATCHING STRATEGY,0.13753213367609254,"examining their output similarities. Regarding subtree flip and splitting order invariances, it is
190"
MATCHING STRATEGY,0.13817480719794345,"possible to find the optimal pattern from all the possible patterns of flips and changes in the splitting
191"
MATCHING STRATEGY,0.13881748071979436,"order. Since the tree-wise output remains unchanged, the similarity between each tree, generated
192"
MATCHING STRATEGY,0.13946015424164523,"by considering additional invariances, and the target tree is evaluated based on the inner product of
193"
MATCHING STRATEGY,0.14010282776349614,"parameters while applying node-wise weighting.
194"
MATCHING STRATEGY,0.14074550128534705,"• Weight Matching (Algorithm 2). Similar to AM, WM also involves applying weighting while
195"
MATCHING STRATEGY,0.14138817480719795,"extracting the optimal pattern by exploring possible flipping and ordering patterns. Although it is
196"
MATCHING STRATEGY,0.14203084832904883,"necessary to solve the LAP multiple times for each layer in MLPs [4], tree ensembles require only
197"
MATCHING STRATEGY,0.14267352185089974,"a single run of the LAP since there are no layers.
198"
MATCHING STRATEGY,0.14331619537275064,"The time complexity of solving the LAP is O(M 3) using a modified Jonker-Volgenant algorithm
199"
MATCHING STRATEGY,0.14395886889460155,"without initialization [26], implemented in SciPy [27], where M is the number of trees. If only
200"
MATCHING STRATEGY,0.14460154241645246,"considering tree permutation, this process needs to be performed only once in both WM and AM.
201"
MATCHING STRATEGY,0.14524421593830333,"However, when considering additional invariances, we need to solve the LAP for each pattern
202"
MATCHING STRATEGY,0.14588688946015424,"generated by considering these additional invariances. In a non-oblivious perfect binary tree with
203"
MATCHING STRATEGY,0.14652956298200515,"depth D, there are 2D −1 splitting nodes, leading to 22D−1 possible combinations of sign flips.
204"
MATCHING STRATEGY,0.14717223650385605,"Additionally, in the case of oblivious trees, there are D! different patterns of splitting order invariance.
205"
MATCHING STRATEGY,0.14781491002570693,"Therefore, for large values of D, conducting a brute-force search becomes impractical.
206"
MATCHING STRATEGY,0.14845758354755784,"In Section 3.3, we will discuss methods to eliminate additional invariance by adjusting the tree
207"
MATCHING STRATEGY,0.14910025706940874,"structure. This enables efficient matching even for deep models. Additionally, in Section 4.2, we
208"
MATCHING STRATEGY,0.14974293059125965,"will present numerical experiment results and discuss that the practical motivation to apply these
209"
MATCHING STRATEGY,0.15038560411311053,"algorithms is limited when targeting deep perfect binary trees.
210"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15102827763496143,"3.3
Architecture-dependency of the Invariances
211"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15167095115681234,"Invariance exists
Empty Node"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15231362467866325,"Figure 4: Tree architecture where neither subtree
flip invariance nor splitting order invariance exists."
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15295629820051415,"In previous subsections, tree architectures are
212"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15359897172236503,"fixed to perfect binary trees as they are most
213"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15424164524421594,"commonly and practically used in soft trees.
214"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15488431876606684,"However, tree architectures can be flexible as
215"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15552699228791775,"we have shown in the right example in Figure 3,
216"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15616966580976863,"and here we show that we can specifically de-
217"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15681233933161953,"sign tree architecture that has neither the subtree
218"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15745501285347044,"flip nor splitting order invariances. This allows
219"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15809768637532134,"efficient matching as considering such two in-
220"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15874035989717222,"variances is computationally expensive.
221"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.15938303341902313,"Table 1: Invariances inherent to each model archi-
tecture."
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16002570694087404,"Perm
Flip
Order"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16066838046272494,"Non-Oblivious Tree
✓
✓
×
Oblivious Tree
✓
✓
✓
Decision List
✓
(✓)
×
Decision List (Modified)
✓
×
×"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16131105398457585,"Our idea is to modify a decision list shown on
222"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16195372750642673,"the left side of Figure 4, which is a tree structure
223"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16259640102827763,"where branches extend in only one direction.
224"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16323907455012854,"Due to this asymmetric structure, the number of
225"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16388174807197944,"parameters does not increase exponentially with
226"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16452442159383032,"the depth, and the splitting order invariance does
227"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16516709511568123,"not exist. Moreover, subtree flip invariance also
228"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16580976863753213,"does not exist for any internal nodes except for
229"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16645244215938304,"the terminal splitting node, as shown in the left
230"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16709511568123395,"side of Figure 4. To completely remove this invariance, we virtually eliminate one of the terminal
231"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16773778920308482,"leaves by leaving the node empty, that is, a fixed prediction value of zero, as shown on the right
232"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16838046272493573,"side of Figure 4. Therefore only permutation invariance exists for our proposed architecture. We
233"
ARCHITECTURE-DEPENDENCY OF THE INVARIANCES,0.16902313624678664,"summarize invariances inherent to each model architecture in Table 1.
234"
EXPERIMENT,0.16966580976863754,"4
Experiment
235"
EXPERIMENT,0.17030848329048842,"We empirically evaluate barriers in soft tree ensembles to examine LMC.
236"
SETUP,0.17095115681233933,"4.1
Setup
237"
SETUP,0.17159383033419023,"Datasets.
In our experiments, we employed Tabular-Benchmark [28], a collection of tabular
238"
SETUP,0.17223650385604114,"datasets suitable for evaluating tree ensembles. Details of datasets are provided in Section B in the
239"
SETUP,0.17287917737789202,"supplementary material. As proposed in [28], we randomly sampled 10, 000 instances for train and
240"
SETUP,0.17352185089974292,"test data from each dataset. If the dataset contains fewer than 20, 000 instances, they are randomly
241"
SETUP,0.17416452442159383,"divided into halves for train and test data. We applied quantile transformation to each feature and
242"
SETUP,0.17480719794344474,"standardized it to follow a normal distribution.
243"
SETUP,0.17544987146529564,"Hyperparameters. We used three different learning rates η ∈{0.01, 0.001, 0.0001} and adopted the
244"
SETUP,0.17609254498714652,"one that yields the highest training accuracy for each dataset. The batch size is set at 512. It is known
245"
SETUP,0.17673521850899743,"that the optimal settings for the learning rate and batch size are interdependent [29]. Therefore, it is
246"
SETUP,0.17737789203084833,"reasonable to fix the batch size while adjusting the learning rate. During AM, we set the amount of
247"
SETUP,0.17802056555269924,"data used for random sampling to be the same as the batch size, thus using 512 samples to measure the
248"
SETUP,0.17866323907455012,"similarity of the tree outputs. As the number of trees M and their depths D vary for each experiment,
249"
SETUP,0.17930591259640102,"these details will be specified in the experimental results section. During training, we minimized
250"
SETUP,0.17994858611825193,"cross-entropy using Adam [16] with its default hyperparameters1. Training is conducted for 50
251"
SETUP,0.18059125964010284,"epochs. To measure the barrier using Equation (1), experiments were conducted by interpolating
252"
SETUP,0.18123393316195371,"between two models with λ ∈{0, 1/24, . . . , 23/24, 1}, which has the same granularity as in [4].
253"
SETUP,0.18187660668380462,"Randomness. We conducted experiments with five different random seed pairs: rA ∈{1, 3, 5, 7, 9}
254"
SETUP,0.18251928020565553,"and rB ∈{2, 4, 6, 8, 10}. As a result, the initial parameters and the contents of the data mini-batches
255"
SETUP,0.18316195372750643,"during training are different in each training. In contrast to spawning [5] that branches off from the
256"
SETUP,0.18380462724935734,"exact same model partway through, we used more challenging practical conditions. The parameters
257"
SETUP,0.18444730077120822,"w, b, and π were randomly initialized using a uniform distribution, identical to the procedure for a
258"
SETUP,0.18508997429305912,"fully connected layer in the MLP2.
259"
SETUP,0.18573264781491003,"1https://pytorch.org/docs/stable/generated/torch.optim.Adam.html
2https://pytorch.org/docs/stable/generated/torch.nn.Linear.html Naive Perm"
SETUP,0.18637532133676094,Perm&Flip 0 5 10 15 20
SETUP,0.1870179948586118,Averaged Accuracy Barrier (Non-Oblivious)
SETUP,0.18766066838046272,M=256 (Train) Naive Perm
SETUP,0.18830334190231363,Perm&Flip 0 5 10 15
SETUP,0.18894601542416453,"20
D=2 (Train)"
SETUP,0.18958868894601544,"Naive
Perm"
SETUP,0.19023136246786632,Perm&OrderPerm&Flip
SETUP,0.19087403598971722,Perm&Flip&Order 0 5 10 15 20
SETUP,0.19151670951156813,Averaged Accuracy Barrier (Oblivious)
SETUP,0.19215938303341903,"Naive
Perm"
SETUP,0.1928020565552699,Perm&OrderPerm&Flip
SETUP,0.19344473007712082,Perm&Flip&Order 0 5 10 15 20 Naive Perm
SETUP,0.19408740359897173,Perm&Flip 0 5 10 15
SETUP,0.19473007712082263,"20
M=256 (Test) Naive Perm"
SETUP,0.1953727506426735,Perm&Flip 0 5 10 15
SETUP,0.19601542416452442,"20
D=2 (Test)"
SETUP,0.19665809768637532,"Naive
Perm"
SETUP,0.19730077120822623,Perm&OrderPerm&Flip
SETUP,0.19794344473007713,Perm&Flip&Order 0 5 10 15 20
SETUP,0.198586118251928,"D=1, WM
D=2, WM
D=3, WM"
SETUP,0.19922879177377892,"D=1, AM
D=2, AM
D=3, AM"
SETUP,0.19987146529562982,"Naive
Perm"
SETUP,0.20051413881748073,Perm&OrderPerm&Flip
SETUP,0.2011568123393316,Perm&Flip&Order 0 5 10 15 20
SETUP,0.20179948586118251,"M=64, WM
M=256, WM
M=1024, WM"
SETUP,0.20244215938303342,"M=64, AM
M=256, AM
M=1024, AM"
SETUP,0.20308483290488433,"Figure 5: Barriers averaged across 16 datasets with respect to considered invariances for non-
oblivious (top row) and oblivious (bottom row) trees. The error bars show the standard deviations of
5 executions."
SETUP,0.2037275064267352,Interpolation 70 75
SETUP,0.2043701799485861,Accuracy
SETUP,0.20501285347043702,Bioresponse
SETUP,0.20565552699228792,Interpolation 55 60
SETUP,0.20629820051413883,Diabetes130US
SETUP,0.2069408740359897,"Interpolation
55 60 65 Higgs"
SETUP,0.20758354755784061,Interpolation 80 85
SETUP,0.20822622107969152,MagicTelescope
SETUP,0.20886889460154243,Interpolation 60 80
SETUP,0.2095115681233933,MiniBooNE
SETUP,0.2101542416452442,Interpolation 60 70
SETUP,0.21079691516709512,bank-marketing
SETUP,0.21143958868894602,Interpolation 75 80 85
SETUP,0.2120822622107969,california
SETUP,0.2127249357326478,"Interpolation
50 60 70"
SETUP,0.2133676092544987,covertype
SETUP,0.21401028277634962,Interpolation 70 75
SETUP,0.21465295629820053,Accuracy
SETUP,0.2152956298200514,credit
SETUP,0.2159383033419023,Interpolation 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.21658097686375322,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.21722365038560412,Interpolation 60 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.217866323907455,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2185089974293059,Interpolation 54 56 58
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2191516709511568,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.21979434447300772,Interpolation 65 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.22043701799485863,Interpolation 75 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2210796915167095,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2217223650385604,Interpolation 65 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.22236503856041132,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.22300771208226222,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2236503856041131,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.224293059125964,"Figure 6: Interpolation curves of test accuracy for oblivious trees on 16 datasets from Tabular-
Benchmark [28]. Two model pairs are trained with on the same dataset. The error bars show the
standard deviations of 5 executions. We used M = 256 trees with a depth D = 2."
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2249357326478149,"Resources. All experiments were conducted on a system equipped with an Intel Xeon E5-2698 CPU
260"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.22557840616966582,"at 2.20 GHz, 252 GB of memory, and Tesla V100-DGXS-32GB GPU, running Ubuntu Linux (version
261"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2262210796915167,"4.15.0-117-generic). The reproducible PyTorch [30] implementation is provided in the supplementary
262"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2268637532133676,"material.
263"
RESULTS FOR PERFECT BINARY TREES,0.2275064267352185,"4.2
Results for Perfect Binary Trees
264"
RESULTS FOR PERFECT BINARY TREES,0.22814910025706941,"Figure 5 shows how the barrier between two perfect binary tree model pairs changes in each operation.
265"
RESULTS FOR PERFECT BINARY TREES,0.22879177377892032,"The vertical axis of each plot in Figure 5 shows the averaged barrier over datasets for each considered
266"
RESULTS FOR PERFECT BINARY TREES,0.2294344473007712,"invariance. The results for both the oblivious and non-oblivious trees are plotted separately in a
267"
RESULTS FOR PERFECT BINARY TREES,0.2300771208226221,"vertical layout. The panels on the left display the results when the depth D of the tree varies, keeping
268"
RESULTS FOR PERFECT BINARY TREES,0.230719794344473,"M = 256 constant. The panels on the right show the results when the number of trees M varies, with
269"
RESULTS FOR PERFECT BINARY TREES,0.23136246786632392,"D fixed at 2. For both oblivious and non-oblivious trees, we observed that the barrier significantly
270"
RESULTS FOR PERFECT BINARY TREES,0.2320051413881748,"decreases as the considered invariances increase. Focusing on the test data results, after accounting for
271"
RESULTS FOR PERFECT BINARY TREES,0.2326478149100257,"various invariances, the barrier is nearly zero, indicating that LMC has been achieved. In particular,
272"
RESULTS FOR PERFECT BINARY TREES,0.2332904884318766,"the difference between the case of only permutation and the case where additional invariances are
273"
RESULTS FOR PERFECT BINARY TREES,0.23393316195372751,"considered tends to be larger in the case of AM. This is because parameter values are not used during
274"
RESULTS FOR PERFECT BINARY TREES,0.2345758354755784,"the rearrangement of the tree in AM. Additionally, it has been observed that the barrier increases as
275"
RESULTS FOR PERFECT BINARY TREES,0.2352185089974293,"trees become deeper, and the barrier decreases as the number of trees increases. These behaviors
276"
RESULTS FOR PERFECT BINARY TREES,0.2358611825192802,"correspond to the changes observed in neural networks when the depth varies or when the width of
277"
RESULTS FOR PERFECT BINARY TREES,0.2365038560411311,"hidden layers increases [3, 4]. Figure 6 shows interpolation curves when using AM in oblivious trees
278"
RESULTS FOR PERFECT BINARY TREES,0.23714652956298202,Interpolation 60 70
RESULTS FOR PERFECT BINARY TREES,0.2377892030848329,Accuracy
RESULTS FOR PERFECT BINARY TREES,0.2384318766066838,Bioresponse
RESULTS FOR PERFECT BINARY TREES,0.2390745501285347,"Interpolation
50 55 60"
RESULTS FOR PERFECT BINARY TREES,0.2397172236503856,Diabetes130US
RESULTS FOR PERFECT BINARY TREES,0.2403598971722365,Interpolation 60 65 Higgs
RESULTS FOR PERFECT BINARY TREES,0.2410025706940874,Interpolation 75 80
MAGICTELESCOPE,0.2416452442159383,85MagicTelescope
MAGICTELESCOPE,0.2422879177377892,Interpolation 70 80 90
MAGICTELESCOPE,0.24293059125964012,MiniBooNE
MAGICTELESCOPE,0.243573264781491,Interpolation 70 75
MAGICTELESCOPE,0.2442159383033419,bank-marketing
MAGICTELESCOPE,0.2448586118251928,"Interpolation
75 80 85"
MAGICTELESCOPE,0.2455012853470437,california
MAGICTELESCOPE,0.2461439588688946,"Interpolation
50 60 70"
MAGICTELESCOPE,0.2467866323907455,covertype
MAGICTELESCOPE,0.2474293059125964,Interpolation 60 70
MAGICTELESCOPE,0.2480719794344473,Accuracy
MAGICTELESCOPE,0.2487146529562982,credit
MAGICTELESCOPE,0.2493573264781491,Interpolation 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.25,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2506426735218509,"Interpolation
65 70 75"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2512853470437018,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2519280205655527,Interpolation 52.5 55.0 57.5
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2525706940874036,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2532133676092545,Interpolation 60 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2538560411311054,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2544987146529563,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2551413881748072,Interpolation 65 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.25578406169665807,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.256426735218509,Interpolation 70 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2570694087403599,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2577120822622108,"Figure 7: Interpolation curves of test accuracy for oblivious trees on 16 datasets from Tabular-
Benchmark [28]. Two model pairs are trained on split datasets with different class ratios. The error
bars show the standard deviations of 5 executions. We used M = 256 trees with a depth D = 2."
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2583547557840617,"with D = 2 and M = 256. Other detailed results, such as performance for each dataset, are provided
279"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2589974293059126,"in Section C in the supplementary material.
280"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2596401028277635,"Furthermore, we conducted experiments with split data following the protocol in [4, 31], where
281"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2602827763496144,"the initial split consists of randomly sampled 80% negative and 20% positive instances, and the
282"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2609254498714653,"second split inverts these ratios. There is no overlap between the two split datasets. We trained two
283"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2615681233933162,"model pairs using these separately split datasets and observed an improvement in performance by
284"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2622107969151671,"interpolating their parameters. Figure 7 illustrates the interpolation curves under AM in oblivious
285"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.262853470437018,"trees with parameters D = 2 and M = 256. We can observe that considering additional invariances
286"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2634961439588689,"improves performance after interpolation. Note that the data split is configured to remain consistent
287"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.26413881748071977,"even when the training random seeds differ. Detailed results for each dataset using WM or AM are
288"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2647814910025707,"provided in Section C of the supplementary material.
289"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2654241645244216,"Table 2: Barriers, accuracies, and model sizes for
MLP, non-oblivious trees, and oblivious trees. MLP"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2660668380462725,"Barrier
Depth"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2667095115681234,"Naive
Perm [4]
Accuracy
Size"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.26735218508997427,"1
8.755 ± 0.877
0.491 ± 0.062 76.286 ± 0.094
12034
2
15.341± 1.125 2.997 ± 0.709 75.981 ± 0.139
77826
3
15.915 ± 2.479 5.940 ± 2.153 75.935 ± 0.117 143618"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2679948586118252,Non-Oblivious Tree
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2686375321336761,"Barrier
Depth"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.269280205655527,"Naive
Perm
Ours
Accuracy
Size"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2699228791773779,"1
8.965 ± 0.963 0.449 ± 0.235 0.181 ± 0.078 76.464 ± 0.167 12544
2
6.801 ± 0.464 0.811 ± 0.333 0.455 ± 0.105 76.631 ± 0.052 36608
3
5.602 ± 0.926 1.635 ± 0.334 0.740 ± 0.158 76.339 ± 0.115 84736"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2705655526992288,Oblivious Tree
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2712082262210797,"Barrier
Depth"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2718508997429306,"Naive
Perm
Ours
Accuracy
Size"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.27249357326478146,"1
8.965 ± 0.963 0.449 ± 0.235 0.181 ± 0.078 76.464 ± 0.167 12544
2
7.881 ± 0.866 0.918 ± 0.092 0.348 ± 0.172 76.623 ± 0.042 25088
3
7.096 ± 0.856 1.283 ± 0.139 0.484 ± 0.049 76.535 ± 0.063 38656"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2731362467866324,"Table 2 compares the average test barriers of an
290"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2737789203084833,"MLP with a ReLU activation function, whose
291"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2744215938303342,"width is equal to the number of trees, M = 256.
292"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2750642673521851,"The procedure for MLPs follows that described
293"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.27570694087403597,"in Section 4.1. The permutation for MLPs is
294"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2763496143958869,"optimized using the method described in [4].
295"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2769922879177378,"Since [4] indicated that WM outperforms AM
296"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2776349614395887,"in neural networks, WM was used for the com-
297"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2782776349614396,"parison. Overall, tree models exhibit smaller
298"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.27892030848329047,"barriers compared to MLPs while keeping sim-
299"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2795629820051414,"ilar accuracy levels. It is important to note that
300"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2802056555269923,"MLPs with D > 1 tend to have more parameters
301"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2808483290488432,"at the same depth compared to trees, leading to
302"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2814910025706941,"more complex optimization landscapes. Nev-
303"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.28213367609254497,"ertheless, the barrier for the non-oblivious tree
304"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2827763496143959,"at D = 3 is smaller than that for the MLP at
305"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2834190231362468,"D = 2, even with more parameters. Further-
306"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.28406169665809766,"more, at the same depth of D = 1, tree models
307"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2847043701799486,"have a smaller barrier. Here, the model size is
308"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2853470437017995,"evaluated using F = 44, the average input fea-
309"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2859897172236504,"ture size of 16 datasets used in the experiments.
310"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2866323907455013,"In Section 3.2, we have shown that considering additional invariances for deep perfect binary trees
311"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.28727506426735216,"is computationally challenging, which may suggest developing heuristic algorithms for deep trees.
312"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2879177377892031,"However, we consider it is rather a low priority, supported by our observations that the barrier tends
313"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.288560411311054,"to increase as trees deepen even if we consider invariances. This trend indicates that deep models are
314"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2892030848329049,"fundamentally less important for model merging considerations. Furthermore, deep perfect binary
315"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2898457583547558,"trees are rarely used in practical scenarios. [12] have demonstrated that generalization performance
316"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.29048843187660667,"degrades with increasing depth in perfect binary trees due to the degeneracy of the Neural Tangent
317"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2911311053984576,"Kernel (NTK) [32]. This evidence further supports the preference for shallow perfect binary trees,
318"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2917737789203085,"and increasing the number of trees can enhance the expressive power while reducing barriers.
319"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.29241645244215936,Table 3: Barriers averaged for 16 datasets under WM with D = 2 and M = 256.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2930591259640103,"Train
Test"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.29370179948586117,"Barrier
Barrier
Architecture"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2943444730077121,"Naive
Perm
Ours
Accuracy
Naive
Perm
Ours
Accuracy"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.294987146529563,"Non-Oblivious Tree
13.079 ± 0.755 4.707 ± 0.332 3.303 ± 0.104 85.646 ± 0.090 6.801 ± 0.464 0.811 ± 0.333 0.455 ± 0.105 76.631 ± 0.052
Oblivious Tree
14.580 ± 1.108 4.834 ± 0.176 2.874 ± 0.108 85.808 ± 0.146 7.881 ± 0.866 0.919 ± 0.093 0.348 ± 0.172 76.623 ± 0.042
Decision List
13.835 ± 0.788 3.687 ± 0.230
—
85.337 ± 0.134 7.513 ± 0.944 0.436 ± 0.120
—
76.629 ± 0.119
Decision List (Modified) 12.922 ± 1.131 3.328 ± 0.204
—
85.563 ± 0.141 6.734 ± 1.096 0.468 ± 0.150
—
76.773 ± 0.051"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.29562982005141386,Table 4: Barriers averaged for 16 datasets under AM with D = 2 and M = 256.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2962724935732648,"Train
Test"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2969151670951157,"Barrier
Barrier
Architecture"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2975578406169666,"Naive
Perm
Ours
Accuracy
Naive
Perm
Ours
Accuracy"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.2982005141388175,"Non-Oblivious Tree
13.079 ± 0.755 14.963 ± 1.520 4.500 ± 0.527 85.646 ± 0.090 6.801 ± 0.464
8.631 ± 1.444
0.943 ± 0.435 76.631 ± 0.052
Oblivious Tree
14.580 ± 1.108 17.380 ± 0.509 3.557 ± 0.201 85.808 ± 0.146 7.881 ± 0.866 10.349 ± 0.476 0.395 ± 0.185 76.623 ± 0.042
Decision List
13.835 ± 0.788 12.785 ± 1.924
—
85.337 ± 0.134 7.513 ± 0.944
7.452 ± 1.840
—
76.629 ± 0.119
Decision List (Modified) 12.922 ± 1.131
6.364 ± 0.194
—
85.563 ± 0.141 6.734 ± 1.096
2.114 ± 0.243
—
76.773 ± 0.051"
RESULTS FOR DECISION LISTS,0.29884318766066836,"4.3
Results for Decision Lists
320"
RESULTS FOR DECISION LISTS,0.2994858611825193,"2
4
8
D 0.0 2.5 5.0 7.5 10.0"
RESULTS FOR DECISION LISTS,0.3001285347043702,Averaged Accuracy Barrier WM
RESULTS FOR DECISION LISTS,0.30077120822622105,"Oblivious
Decision List
Decision List (Modified)"
RESULTS FOR DECISION LISTS,0.301413881748072,"2
4
8
D 5 10 15 20 AM"
RESULTS FOR DECISION LISTS,0.30205655526992287,"Figure 8: Averaged barrier for 16 datasets as a
function of tree depth. The error bars show the
standard deviations of 5 executions. The solid line
represents the barrier in train accuracy, while the
dashed line represents the barrier in test accuracy."
RESULTS FOR DECISION LISTS,0.3026992287917738,"We present empirical results of the original de-
321"
RESULTS FOR DECISION LISTS,0.3033419023136247,"cision lists and our modified decision lists, as
322"
RESULTS FOR DECISION LISTS,0.30398457583547556,"shown in Figure 4. As we have shown in Table 1,
323"
RESULTS FOR DECISION LISTS,0.3046272493573265,"they have fewer invariances.
324"
RESULTS FOR DECISION LISTS,0.30526992287917737,"Figure 8 illustrates barriers as a function of
325"
RESULTS FOR DECISION LISTS,0.3059125964010283,"depth, considering only permutation invariance,
326"
RESULTS FOR DECISION LISTS,0.3065552699228792,"with M fixed at 256. In this experiment, we
327"
RESULTS FOR DECISION LISTS,0.30719794344473006,"have excluded non-oblivious trees from compar-
328"
RESULTS FOR DECISION LISTS,0.307840616966581,"ison as the number of their parameters exponen-
329"
RESULTS FOR DECISION LISTS,0.30848329048843187,"tially increases as trees deepen, making them
330"
RESULTS FOR DECISION LISTS,0.30912596401028275,"infeasible computation. Our proposed modified
331"
RESULTS FOR DECISION LISTS,0.3097686375321337,"decision lists reduce the barrier more effectively
332"
RESULTS FOR DECISION LISTS,0.31041131105398456,"than both oblivious trees and the original de-
333"
RESULTS FOR DECISION LISTS,0.3110539845758355,"cision lists. However, barriers of the modified
334"
RESULTS FOR DECISION LISTS,0.3116966580976864,"decision lists are still larger than those obtained by considering additional invariances with perfect
335"
RESULTS FOR DECISION LISTS,0.31233933161953725,"binary trees. Tables 3 and 4 show the averaged barriers for 16 datasets, with D = 2 and M = 256.
336"
RESULTS FOR DECISION LISTS,0.3129820051413882,"Although barriers of modified decision lists are small when considering only permutations (Perm),
337"
RESULTS FOR DECISION LISTS,0.31362467866323906,"perfect binary trees such as oblivious trees with additional invariances (Ours) exhibit smaller barriers,
338"
RESULTS FOR DECISION LISTS,0.31426735218509,"which supports the validity of using oblivious trees as in [9, 11]. To summarize, when considering
339"
RESULTS FOR DECISION LISTS,0.3149100257069409,"the practical use of model merging, if the goal is to prioritize efficient computation, we recommend
340"
RESULTS FOR DECISION LISTS,0.31555269922879176,"using our proposed decision list. Conversely, if the goal is to prioritize barriers, it would be preferable
341"
RESULTS FOR DECISION LISTS,0.3161953727506427,"to use perfect binary trees, which have a greater number of invariant operations that maintain the
342"
RESULTS FOR DECISION LISTS,0.31683804627249357,"functional behavior.
343"
CONCLUSION,0.31748071979434445,"5
Conclusion
344"
CONCLUSION,0.3181233933161954,"We have presented the first investigation of LMC for soft tree ensembles. We have identified additional
345"
CONCLUSION,0.31876606683804626,"invariances inherent in tree architectures and empirically demonstrated the importance of considering
346"
CONCLUSION,0.3194087403598972,"these factors. Achieving LMC is crucial not only for understanding the behavior of non-convex
347"
CONCLUSION,0.32005141388174807,"optimization from a learning theory perspective but also for implementing practical techniques such as
348"
CONCLUSION,0.32069408740359895,"model merging. By arithmetically combining parameters of differently trained models, a wide range
349"
CONCLUSION,0.3213367609254499,"of applications such as task-arithmetic [33], including unlearning [34] and continual-learning [35],
350"
CONCLUSION,0.32197943444730076,"have been explored. Our research extends these techniques to soft tree ensembles that began training
351"
CONCLUSION,0.3226221079691517,"from entirely different initial conditions. We will leave these empirical investigations for future work.
352"
CONCLUSION,0.3232647814910026,"This study provides a fundamental analysis of ensemble learning, and we believe that our discussion
353"
CONCLUSION,0.32390745501285345,"will not have any negative societal impacts.
354"
REFERENCES,0.3245501285347044,"References
355"
REFERENCES,0.32519280205655526,"[1] Robert Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In
356"
REFERENCES,0.3258354755784062,"Advanced Neural Computers. 1990.
357"
REFERENCES,0.3264781491002571,"[2] An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. On the Geometry of Feedforward
358"
REFERENCES,0.32712082262210795,"Neural Network Error Surfaces. Neural Computation, 1993.
359"
REFERENCES,0.3277634961439589,"[3] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The Role of Permutation
360"
REFERENCES,0.32840616966580977,"Invariance in Linear Mode Connectivity of Neural Networks. In International Conference on
361"
REFERENCES,0.32904884318766064,"Learning Representations, 2022.
362"
REFERENCES,0.3296915167095116,"[4] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git Re-Basin: Merging Models
363"
REFERENCES,0.33033419023136246,"modulo Permutation Symmetries. In The Eleventh International Conference on Learning
364"
REFERENCES,0.3309768637532134,"Representations, 2023.
365"
REFERENCES,0.33161953727506427,"[5] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear Mode
366"
REFERENCES,0.33226221079691515,"Connectivity and the Lottery Ticket Hypothesis. In Proceedings of the 37th International
367"
REFERENCES,0.3329048843187661,"Conference on Machine Learning, 2020.
368"
REFERENCES,0.33354755784061696,"[6] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,
369"
REFERENCES,0.3341902313624679,"Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig
370"
REFERENCES,0.33483290488431877,"Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy
371"
REFERENCES,0.33547557840616965,"without increasing inference time. In Proceedings of the 39th International Conference on
372"
REFERENCES,0.3361182519280206,"Machine Learning, 2022.
373"
REFERENCES,0.33676092544987146,"[7] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task Arithmetic in the
374"
REFERENCES,0.33740359897172234,"Tangent Space: Improved Editing of Pre-Trained Models. In Thirty-seventh Conference on
375"
REFERENCES,0.3380462724935733,"Neural Information Processing Systems, 2023.
376"
REFERENCES,0.33868894601542415,"[8] Leo Breiman. Random Forests. In Machine Learning, 2001.
377"
REFERENCES,0.3393316195372751,"[9] Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural Oblivious Decision Ensembles
378"
REFERENCES,0.33997429305912596,"for Deep Learning on Tabular Data. In International Conference on Learning Representations,
379"
REFERENCES,0.34061696658097684,"2020.
380"
REFERENCES,0.3412596401028278,"[10] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The
381"
REFERENCES,0.34190231362467866,"Tree Ensemble Layer: Differentiability meets Conditional Computation. In Proceedings of the
382"
REFERENCES,0.3425449871465296,"37th International Conference on Machine Learning, 2020.
383"
REFERENCES,0.34318766066838047,"[11] Chun-Hao Chang, Rich Caruana, and Anna Goldenberg. NODE-GAM: Neural generalized
384"
REFERENCES,0.34383033419023135,"additive model for interpretable deep learning. In International Conference on Learning
385"
REFERENCES,0.3444730077120823,"Representations, 2022.
386"
REFERENCES,0.34511568123393316,"[12] Ryuichi Kanoh and Mahito Sugiyama. A Neural Tangent Kernel Perspective of Infinite Tree
387"
REFERENCES,0.34575835475578404,"Ensembles. In International Conference on Learning Representations, 2022.
388"
REFERENCES,0.34640102827763497,"[13] Ryuichi Kanoh and Mahito Sugiyama. Analyzing Tree Architectures in Ensembles via Neural
389"
REFERENCES,0.34704370179948585,"Tangent Kernel. In International Conference on Learning Representations, 2023.
390"
REFERENCES,0.3476863753213368,"[14] Guolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu. DeepGBM: A Deep Learning
391"
REFERENCES,0.34832904884318766,"Framework Distilled by GBDT for Online Prediction Tasks. In Proceedings of the 25th ACM
392"
REFERENCES,0.34897172236503854,"SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019.
393"
REFERENCES,0.3496143958868895,"[15] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
394"
REFERENCES,0.35025706940874035,"Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine
395"
REFERENCES,0.3508997429305913,"Learning Research, 2014.
396"
REFERENCES,0.35154241645244216,"[16] Diederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International
397"
REFERENCES,0.35218508997429304,"Conference on Learning Representations, 2015.
398"
REFERENCES,0.352827763496144,"[17] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware Mini-
399"
REFERENCES,0.35347043701799485,"mization for Efficiently Improving Generalization. In International Conference on Learning
400"
REFERENCES,0.35411311053984573,"Representations, 2021.
401"
REFERENCES,0.35475578406169667,"[18] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. In
402"
REFERENCES,0.35539845758354754,"Proceedings of International Conference on Neural Networks, 1993.
403"
REFERENCES,0.3560411311053985,"[19] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E.
404"
REFERENCES,0.35668380462724936,"Hinton, and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-
405"
REFERENCES,0.35732647814910024,"Experts Layer. In International Conference on Learning Representations, 2017.
406"
REFERENCES,0.35796915167095117,"[20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
407"
REFERENCES,0.35861182519280205,"Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models with
408"
REFERENCES,0.359254498714653,"Conditional Computation and Automatic Sharding. In International Conference on Learning
409"
REFERENCES,0.35989717223650386,"Representations, 2021.
410"
REFERENCES,0.36053984575835474,"[21] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
411"
REFERENCES,0.36118251928020567,"Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
412"
REFERENCES,0.36182519280205655,"Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut
413"
REFERENCES,0.36246786632390743,"Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B, 2023.
414"
REFERENCES,0.36311053984575836,"[22] Byron Roe. MiniBooNE particle identification. UCI Machine Learning Repository, 2010.
415"
REFERENCES,0.36375321336760924,"[23] Fidel A. Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric
416"
REFERENCES,0.3643958868894602,"Granger, and Marco Pedersoli. Re-basin via implicit Sinkhorn differentiation. In IEEE/CVF
417"
REFERENCES,0.36503856041131105,"Conference on Computer Vision and Pattern Recognition, 2023.
418"
REFERENCES,0.36568123393316193,"[24] Zhanpeng Zhou, Yongyi Yang, Xiaojiang Yang, Junchi Yan, and Wei Hu. Going Beyond Linear
419"
REFERENCES,0.36632390745501286,"Mode Connectivity: The Layerwise Linear Feature Connectivity. In Thirty-seventh Conference
420"
REFERENCES,0.36696658097686374,"on Neural Information Processing Systems, 2023.
421"
REFERENCES,0.3676092544987147,"[25] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging:
422"
REFERENCES,0.36825192802056556,"Resolving interference when merging models. In Thirty-seventh Conference on Neural Informa-
423"
REFERENCES,0.36889460154241643,"tion Processing Systems, 2023.
424"
REFERENCES,0.36953727506426737,"[26] David F. Crouse. On implementing 2D rectangular assignment algorithms. IEEE Transactions
425"
REFERENCES,0.37017994858611825,"on Aerospace and Electronic Systems, 2016.
426"
REFERENCES,0.3708226221079691,"[27] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David
427"
REFERENCES,0.37146529562982006,"Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J.
428"
REFERENCES,0.37210796915167094,"van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
429"
REFERENCES,0.37275064267352187,"R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W.
430"
REFERENCES,0.37339331619537275,"Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A.
431"
REFERENCES,0.3740359897172236,"Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul
432"
REFERENCES,0.37467866323907456,"van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific
433"
REFERENCES,0.37532133676092544,"Computing in Python. Nature Methods, 2020.
434"
REFERENCES,0.3759640102827764,"[28] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still
435"
REFERENCES,0.37660668380462725,"outperform deep learning on typical tabular data?
In Thirty-sixth Conference on Neural
436"
REFERENCES,0.37724935732647813,"Information Processing Systems Datasets and Benchmarks Track, 2022.
437"
REFERENCES,0.37789203084832906,"[29] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t Decay the Learning Rate,
438"
REFERENCES,0.37853470437017994,"Increase the Batch Size. In International Conference on Learning Representations, 2018.
439"
REFERENCES,0.3791773778920309,"[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
440"
REFERENCES,0.37982005141388175,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
441"
REFERENCES,0.38046272493573263,"Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
442"
REFERENCES,0.38110539845758357,"Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,
443"
REFERENCES,0.38174807197943444,"High-Performance Deep Learning Library. In Advances in Neural Information Processing
444"
REFERENCES,0.3823907455012853,"Systems, 2019.
445"
REFERENCES,0.38303341902313626,"[31] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. REPAIR:
446"
REFERENCES,0.38367609254498714,"REnormalizing permuted activations for interpolation repair. In The Eleventh International
447"
REFERENCES,0.38431876606683807,"Conference on Learning Representations, 2023.
448"
REFERENCES,0.38496143958868895,"[32] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
449"
REFERENCES,0.3856041131105398,"Generalization in Neural Networks. In Advances in Neural Information Processing Systems,
450"
REFERENCES,0.38624678663239076,"2018.
451"
REFERENCES,0.38688946015424164,"[33] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Ha-
452"
REFERENCES,0.38753213367609257,"jishirzi, and Ali Farhadi. Editing models with task arithmetic. In The Eleventh International
453"
REFERENCES,0.38817480719794345,"Conference on Learning Representations, 2023.
454"
REFERENCES,0.38881748071979433,"[34] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-
455"
REFERENCES,0.38946015424164526,"manabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.
456"
REFERENCES,0.39010282776349614,"In Advances in Neural Information Processing Systems, 2022.
457"
REFERENCES,0.390745501285347,"[35] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan
458"
REFERENCES,0.39138817480719795,"Ghasemzadeh. Linear Mode Connectivity in Multitask and Continual Learning. In Inter-
459"
REFERENCES,0.39203084832904883,"national Conference on Learning Representations, 2021.
460"
REFERENCES,0.39267352185089976,"A
Detailed Algorithms
461"
REFERENCES,0.39331619537275064,"We present pseudo-code of algorithms for activation matching (Algorithm 1) and weight matching
462"
REFERENCES,0.3939588688946015,"(Algorithm 2). In these algorithms, if there is only one possible pattern for U ∈N, which represents
463"
REFERENCES,0.39460154241645246,"the number of possible operations, and the corresponding operation does nothing in particular, it
464"
REFERENCES,0.39524421593830333,becomes equivalent to simply considering tree permutations.
REFERENCES,0.39588688946015427,Algorithm 1: Activation matching for soft trees
REFERENCES,0.39652956298200515,"1 ACTIVATIONMATCHING(ΘA ∈RM×PTree, ΘB ∈RM×PTree, xsampled ∈RF ×Nsampled)"
REFERENCES,0.397172236503856,"2
Initialize OA ∈RM×Nsampled×C and OB ∈RM×Nsampled×C to store outputs"
REFERENCES,0.39781491002570696,"3
for m = 1 to M do"
REFERENCES,0.39845758354755784,"4
for i = 1 to Nsampled do"
REFERENCES,0.3991002570694087,"5
Set the output of the mth tree with ΘA[m] using xsampled[:, i] to OA[m, i]."
REFERENCES,0.39974293059125965,"6
Set the output of the mth tree with ΘB[m] using xsampled[:, i] to OB[m, i]."
REFERENCES,0.4003856041131105,"7
Initialize similarity matrix S ∈RM×M"
REFERENCES,0.40102827763496146,"8
for mA = 1 to M do"
REFERENCES,0.40167095115681234,"9
for mB = 1 to M do"
REFERENCES,0.4023136246786632,"10
S[mA, mB] ←FLATTEN(OA[mA]) · FLATTEN(OB[mB])"
REFERENCES,0.40295629820051415,"11
p ←LINEARSUMASSIGNMENT(S)
// p ∈NM: Optimal assignments"
REFERENCES,0.40359897172236503,"12
ΘA, ΘB ←WEIGHTING(ΘA, ΘB)"
REFERENCES,0.40424164524421596,"13
Initialize operation indices q ∈NM"
REFERENCES,0.40488431876606684,"14
for m = 1 to M do"
REFERENCES,0.4055269922879177,"15
for u = 1 to U do
// U ∈N: Number of possible operations"
REFERENCES,0.40616966580976865,"16
u′ ←UPDATEBESTOPERATION(ADJUSTTREE(ΘA[m], u) · ΘB[m], u)"
REFERENCES,0.40681233933161953,"17
Append u′ to q
// q ∈NM: Optimal operations"
REFERENCES,0.4074550128534704,"18
return p, q"
REFERENCES,0.40809768637532134,Algorithm 2: Weight matching for soft trees
REFERENCES,0.4087403598971722,"1 WEIGHTMATCHING(ΘA ∈RM×PTree, ΘB ∈RM×PTree)"
REFERENCES,0.40938303341902316,"2
ΘA, ΘB ←WEIGHTING(ΘA, ΘB)"
REFERENCES,0.41002570694087404,"3
Initialize similarity matrix for each operation S ∈RU×M×M"
REFERENCES,0.4106683804627249,"4
for u = 1 to U do"
REFERENCES,0.41131105398457585,"5
for mA = 1 to M do"
REFERENCES,0.4119537275064267,"6
θ ←ADJUSTTREE(ΘA[mA], u)
// θ ∈RPTree: Adjusted tree-wise parameters"
REFERENCES,0.41259640102827766,"7
for mB = 1 to M do"
REFERENCES,0.41323907455012854,"8
S[u, mA, mB] ←θ · ΘB[mB]"
REFERENCES,0.4138817480719794,"9
S′ ←max(S, axis=0)
// S′ ∈RM×M: Similarity matrix between trees"
REFERENCES,0.41452442159383035,"10
p ←LINEARSUMASSIGNMENT(S′)
// p ∈NM: Optimal assignments"
REFERENCES,0.41516709511568123,"11
q ←argmax(S, axis=0)[p]
// q ∈NM: Optimal operations"
REFERENCES,0.4158097686375321,"12
return p, q 465"
REFERENCES,0.41645244215938304,"Here, we describe the specifications of the notations and functions used in Algorithms 1 and 2. In
466"
REFERENCES,0.4170951156812339,"Section 2.1, ΘA and ΘB are initially defined as vectors. However, for ease of use, in Algorithms 1
467"
REFERENCES,0.41773778920308485,"and 2, ΘA and ΘB are represented as matrices of size RM×PTree, where PTree denotes the number of
468"
REFERENCES,0.41838046272493573,"parameters in a single tree. Multidimensional array elements are accessed using square brackets [·].
469"
REFERENCES,0.4190231362467866,"For example, for G ∈RI×J, G[i] refers to the ith slice along the first dimension, and G[:, j] refers
470"
REFERENCES,0.41966580976863754,"to the jth slice along the second dimension, with sizes RJ and RI, respectively. Furthermore, it can
471"
REFERENCES,0.4203084832904884,"also accept a vector v ∈Nl as an input. In this case, G[v] ∈Rl×J. The FLATTEN function converts
472"
REFERENCES,0.42095115681233936,"multidimensional input into a one-dimensional vector format. As the LINEARSUMASSIGNMENT
473"
REFERENCES,0.42159383033419023,"function, scipy. optimize. linear_sum_assignment3 is used to solve the LAP. In the ADJUSTTREE
474"
REFERENCES,0.4222365038560411,"function, the parameters of a tree are modified according to the uth pattern among the enumerated U
475"
REFERENCES,0.42287917737789205,"patterns. Additionally, in the WEIGHTING function, parameters are multiplied by the square root
476"
REFERENCES,0.4235218508997429,"of their weights defined in Section 3.2 to simulate the process of assessing a rule set. If the first
477"
REFERENCES,0.4241645244215938,"argument for the UPDATEBESTOPERATION function, the input inner product, is larger than any
478"
REFERENCES,0.42480719794344474,"previously input inner product values, then u′ is updated with u, the second argument. If not, u′
479"
REFERENCES,0.4254498714652956,"remains unchanged.
480"
REFERENCES,0.42609254498714655,"B
Dataset
481"
REFERENCES,0.4267352185089974,"Table 5: Summary of the datasets used in the experiments.
Dataset
N
F
Link"
REFERENCES,0.4273778920308483,"Bioresponse
3434
419
https://www.openml.org/d/45019
Diabetes130US
71090
7
https://www.openml.org/d/45022
Higgs
940160
24
https://www.openml.org/d/44129
MagicTelescope
13376
10
https://www.openml.org/d/44125
MiniBooNE
72998
50
https://www.openml.org/d/44128
bank-marketing
10578
7
https://www.openml.org/d/44126
california
20634
8
https://www.openml.org/d/45028
covertype
566602
10
https://www.openml.org/d/44121
credit
16714
10
https://www.openml.org/d/44089
default-of-credit-card-clients
13272
20
https://www.openml.org/d/45020
electricity
38474
7
https://www.openml.org/d/44120
eye_movements
7608
20
https://www.openml.org/d/44130
heloc
10000
22
https://www.openml.org/d/45026
house_16H
13488
16
https://www.openml.org/d/44123
jannis
57580
54
https://www.openml.org/d/45021
pol
10082
26
https://www.openml.org/d/44122"
REFERENCES,0.42802056555269924,"C
Additional Empirical Results
482"
REFERENCES,0.4286632390745501,"Tables 6, 7, 8 and 9 present the barrier for each dataset with D = 2 and M = 256. By incorporating
483"
REFERENCES,0.42930591259640105,"additional invariances, it has been possible to consistently reduce the barriers.
484"
REFERENCES,0.42994858611825193,"Tables 10 and 11 detail the characteristics of the barriers in the decision lists for each dataset with
485"
REFERENCES,0.4305912596401028,"D = 2 and M = 256. The barriers in the modified decision lists tend to be smaller.
486"
REFERENCES,0.43123393316195374,"Tables 12 and 13 show the barrier for each model when only considering permutations with D = 2
487"
REFERENCES,0.4318766066838046,"and M = 256. It is evident that focusing solely on permutations leads to smaller barriers in the
488"
REFERENCES,0.43251928020565555,"modified decision lists compared to other architectures.
489"
REFERENCES,0.43316195372750643,"Figures 9, 10, 11, 12, 13, 14, 15 and 16 show the interpolation curves of oblivious trees with D = 2
490"
REFERENCES,0.4338046272493573,"and M = 256 across various datasets and configurations. Significant improvements are particularly
491"
REFERENCES,0.43444730077120824,"noticeable in AM, but improvements are also observed in WM. These characteristics are also apparent
492"
REFERENCES,0.4350899742930591,"in the non-oblivious trees, as shown in Figures 17, 18, 19, 20, 21, 22, 23 and 24. Regarding split data
493"
REFERENCES,0.43573264781491,"training, the dataset for each of the two classes is initially complete (100%). It is then divided into
494"
REFERENCES,0.43637532133676094,"splits of 80% and 20%, and 20% and 80%, respectively. Each model is trained using these splits.
495"
REFERENCES,0.4370179948586118,"Figures 13, 15, 21, and 23 show the training accuracy evaluated using the full dataset (100% for each
496"
REFERENCES,0.43766066838046275,"class).
497"
REFERENCES,0.4383033419023136,"3https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_
assignment.html"
REFERENCES,0.4389460154241645,Table 6: Accuracy barrier for non-oblivious trees with WM.
REFERENCES,0.43958868894601544,"Train
Test
Dataset
Naive
Perm
Perm&Flip
Naive
Perm
Perm&Flip"
REFERENCES,0.4402313624678663,"Bioresponse
18.944 ± 10.076
5.876 ± 1.477
4.132 ± 0.893
8.235 ± 6.456
1.285 ± 0.635
0.314 ± 0.432
Diabetes130US
2.148 ± 0.601
1.388 ± 1.159
0.947 ± 0.888
1.014 ± 0.959
0.540 ± 0.999
0.784 ± 0.840
Higgs
27.578 ± 1.742
18.470 ± 0.769
14.772 ± 1.419
4.055 ± 1.089
0.662 ± 0.590
0.292 ± 0.421
MagicTelescope
2.995 ± 1.198
0.576 ± 0.556
0.307 ± 0.346
2.096 ± 1.055
0.361 ± 0.618
0.229 ± 0.348
MiniBooNE
18.238 ± 4.570
2.272 ± 0.215
1.506 ± 0.211
12.592 ± 4.190
0.231 ± 0.314
0.000 ± 0.000
bank-marketing
13.999 ± 4.110
2.711 ± 1.183
1.521 ± 0.463
13.593 ± 4.567
1.843 ± 1.001
0.953 ± 0.688
california
6.396 ± 2.472
0.873 ± 0.551
0.520 ± 0.327
5.226 ± 2.377
0.224 ± 0.248
0.206 ± 0.131
covertype
16.823 ± 4.159
1.839 ± 0.336
0.914 ± 0.546
14.900 ± 4.016
1.035 ± 0.106
0.376 ± 0.333
credit
7.317 ± 2.425
3.172 ± 2.636
2.615 ± 0.831
5.861 ± 2.064
2.202 ± 3.103
1.830 ± 0.588
default-of-credit-card-clients
14.318 ± 4.509
5.419 ± 1.318
3.273 ± 0.793
6.227 ± 4.205
0.937 ± 1.036
0.243 ± 0.172
electricity
10.090 ± 2.930
1.035 ± 0.543
0.221 ± 0.192
9.422 ± 2.795
0.771 ± 0.478
0.130 ± 0.071
eye_movements
18.743 ± 1.994
11.605 ± 1.927
7.866 ± 1.301
1.495 ± 0.467
0.463 ± 0.183
0.180 ± 0.206
heloc
4.434 ± 1.611
1.652 ± 0.475
1.012 ± 0.481
0.830 ± 0.727
0.475 ± 0.447
0.322 ± 0.338
house_16H
8.935 ± 2.504
3.362 ± 0.482
2.660 ± 1.208
4.230 ± 2.189
0.219 ± 0.224
0.404 ± 0.782
jannis
17.756 ± 3.322
10.442 ± 1.404
7.362 ± 0.219
3.205 ± 2.849
0.029 ± 0.064
0.007 ± 0.016
pol
20.542 ± 2.873
4.612 ± 0.912
3.225 ± 1.080
15.830 ± 2.562
1.708 ± 0.599
1.012 ± 0.859"
REFERENCES,0.44087403598971725,Table 7: Accuracy barrier for non-oblivious trees with AM.
REFERENCES,0.44151670951156813,"Train
Test
Dataset
Naive
Perm
Perm&Flip
Naive
Perm
Perm&Flip"
REFERENCES,0.442159383033419,"Bioresponse
18.944 ± 10.076
14.066 ± 7.045
5.710 ± 0.915
8.235 ± 6.456
5.037 ± 3.141
0.966 ± 0.316
Diabetes130US
2.148 ± 0.601
3.086 ± 2.566
0.574 ± 0.365
1.014 ± 0.959
1.936 ± 2.878
0.105 ± 0.152
Higgs
27.578 ± 1.742
30.704 ± 2.899
18.435 ± 1.599
4.055 ± 1.089
7.272 ± 1.089
1.044 ± 0.483
MagicTelescope
2.995 ± 1.198
3.309 ± 1.486
0.778 ± 0.515
2.096 ± 1.055
2.693 ± 1.190
0.428 ± 0.327
MiniBooNE
18.238 ± 4.570
34.934 ± 8.157
2.332 ± 0.383
12.592 ± 4.190
28.721 ± 7.869
0.074 ± 0.081
bank-marketing
13.999 ± 4.110
13.598 ± 7.638
3.098 ± 0.539
13.593 ± 4.567
12.810 ± 7.605
2.643 ± 0.704
california
6.396 ± 2.472
5.800 ± 2.036
0.697 ± 0.535
5.226 ± 2.377
4.858 ± 2.017
0.261 ± 0.285
covertype
16.823 ± 4.159
19.708 ± 6.392
1.420 ± 0.619
14.900 ± 4.016
17.765 ± 6.400
0.758 ± 0.540
credit
7.317 ± 2.425
10.556 ± 8.753
3.640 ± 1.624
5.861 ± 2.064
9.378 ± 9.083
2.551 ± 1.987
default-of-credit-card-clients
14.318 ± 4.509
14.166 ± 2.297
4.247 ± 1.678
6.227 ± 4.205
6.514 ± 2.049
0.885 ± 1.852
electricity
10.090 ± 2.930
12.955 ± 4.558
0.762 ± 0.332
9.422 ± 2.795
12.261 ± 4.554
0.499 ± 0.260
eye_movements
18.743 ± 1.994
18.757 ± 1.273
10.957 ± 1.019
1.495 ± 0.467
1.583 ± 1.011
0.146 ± 0.167
heloc
4.434 ± 1.611
6.564 ± 2.404
1.774 ± 0.672
0.830 ± 0.727
2.179 ± 2.100
0.385 ± 0.370
house_16H
8.935 ± 2.504
10.184 ± 2.667
3.908 ± 0.863
4.230 ± 2.189
5.664 ± 2.461
1.056 ± 0.693
jannis
17.756 ± 3.322
19.004 ± 1.246
9.890 ± 1.036
3.205 ± 2.849
4.047 ± 1.415
0.346 ± 0.443
pol
20.542 ± 2.873
16.267 ± 3.914
7.967 ± 3.208
15.830 ± 2.562
12.863 ± 3.983
4.539 ± 2.727"
REFERENCES,0.44280205655526994,Table 8: Accuracy barrier for oblivious trees with WM.
REFERENCES,0.4434447300771208,"Train
Test
Dataset
Naive
Perm
Perm&Order&Flip
Naive
Perm
Perm&Order&Flip"
REFERENCES,0.4440874035989717,"Bioresponse
16.642 ± 4.362
4.800 ± 0.895
3.289 ± 0.680
7.165 ± 2.547
1.069 ± 1.020
0.299 ± 0.247
Diabetes130US
3.170 ± 3.304
1.120 ± 1.123
0.246 ± 0.177
2.831 ± 3.476
0.882 ± 1.309
0.181 ± 0.155
Higgs
28.640 ± 0.914
19.754 ± 1.023
13.689 ± 0.814
4.648 ± 0.966
1.270 ± 0.808
0.266 ± 0.232
MagicTelescope
2.659 ± 1.637
0.473 ± 0.632
0.077 ± 0.110
2.012 ± 1.343
0.534 ± 0.565
0.093 ± 0.144
MiniBooNE
22.344 ± 7.001
2.388 ± 0.194
1.628 ± 0.208
16.454 ± 6.706
0.075 ± 0.086
0.012 ± 0.019
bank-marketing
13.512 ± 6.416
2.998 ± 1.582
0.925 ± 0.688
12.856 ± 6.609
2.324 ± 1.618
0.634 ± 0.433
california
8.281 ± 4.253
0.874 ± 0.524
0.351 ± 0.267
6.578 ± 4.264
0.342 ± 0.209
0.034 ± 0.024
covertype
23.977 ± 2.565
2.073 ± 0.657
0.976 ± 0.523
21.790 ± 2.253
0.992 ± 0.496
0.422 ± 0.319
credit
6.912 ± 4.083
2.369 ± 0.887
0.662 ± 0.606
5.739 ± 4.502
1.324 ± 0.674
0.350 ± 0.522
default-of-credit-card-clients
16.301 ± 4.462
4.512 ± 1.033
2.902 ± 0.620
7.618 ± 3.873
0.728 ± 0.331
0.531 ± 0.557
electricity
8.835 ± 1.824
1.060 ± 0.684
0.279 ± 0.266
7.952 ± 1.995
0.731 ± 0.383
0.285 ± 0.200
eye_movements
22.604 ± 1.486
12.687 ± 1.645
7.826 ± 1.822
2.884 ± 1.646
0.825 ± 0.711
0.607 ± 0.259
heloc
6.282 ± 2.351
2.517 ± 1.156
1.507 ± 0.498
1.625 ± 1.480
0.869 ± 0.957
0.727 ± 0.785
house_16H
13.600 ± 5.135
3.302 ± 0.376
1.950 ± 0.346
8.055 ± 4.429
0.330 ± 0.441
0.158 ± 0.098
jannis
19.390 ± 1.013
11.358 ± 0.377
7.140 ± 0.538
1.999 ± 1.237
0.305 ± 0.409
0.214 ± 0.235
pol
20.125 ± 2.902
5.059 ± 1.482
2.544 ± 1.005
15.887 ± 3.061
2.100 ± 1.358
0.751 ± 0.892"
REFERENCES,0.44473007712082263,Table 9: Accuracy barrier for oblivious trees with AM.
REFERENCES,0.4453727506426735,"Train
Test
Dataset
Naive
Perm
Perm&Order&Flip
Naive
Perm
Perm&Order&Flip"
REFERENCES,0.44601542416452444,"Bioresponse
16.642 ± 4.362
19.033 ± 8.533
6.358 ± 1.915
7.165 ± 2.547
6.904 ± 5.380
1.038 ± 0.591
Diabetes130US
3.170 ± 3.304
5.473 ± 3.260
0.703 ± 0.517
2.831 ± 3.476
5.290 ± 3.486
0.390 ± 0.291
Higgs
28.640 ± 0.914
33.234 ± 3.164
15.678 ± 0.713
4.648 ± 0.966
8.113 ± 2.614
0.415 ± 0.454
MagicTelescope
2.659 ± 1.637
3.902 ± 1.931
0.224 ± 0.256
2.012 ± 1.343
3.687 ± 1.876
0.334 ± 0.434
MiniBooNE
22.344 ± 7.001
41.022 ± 3.398
2.184 ± 0.425
16.454 ± 6.706
34.452 ± 3.161
0.033 ± 0.056
bank-marketing
13.512 ± 6.416
12.248 ± 6.748
1.330 ± 0.806
12.856 ± 6.609
11.356 ± 7.168
0.695 ± 0.464
california
8.281 ± 4.253
9.539 ± 4.798
0.371 ± 0.365
6.578 ± 4.264
8.354 ± 4.648
0.112 ± 0.181
covertype
23.977 ± 2.565
27.590 ± 2.172
1.051 ± 0.407
21.790 ± 2.253
25.289 ± 1.787
0.403 ± 0.236
credit
6.912 ± 4.083
9.839 ± 6.698
1.169 ± 0.839
5.739 ± 4.502
8.291 ± 7.268
0.549 ± 0.751
default-of-credit-card-clients
16.301 ± 4.462
21.746 ± 7.075
3.646 ± 0.520
7.618 ± 3.873
12.183 ± 5.954
0.285 ± 0.372
electricity
8.835 ± 1.824
18.177 ± 5.979
0.472 ± 0.507
7.952 ± 1.995
17.396 ± 5.809
0.405 ± 0.356
eye_movements
22.604 ± 1.486
23.221 ± 3.024
8.588 ± 2.248
2.884 ± 1.646
2.761 ± 1.628
0.398 ± 0.435
heloc
6.282 ± 2.351
9.074 ± 3.894
2.541 ± 0.471
1.625 ± 1.480
3.891 ± 2.655
0.485 ± 0.397
house_16H
13.600 ± 5.135
17.963 ± 5.099
2.841 ± 0.543
8.055 ± 4.429
12.192 ± 4.635
0.292 ± 0.157
jannis
19.390 ± 1.013
22.482 ± 3.113
9.570 ± 0.316
1.999 ± 1.237
4.292 ± 2.509
0.069 ± 0.154
pol
20.125 ± 2.902
19.558 ± 5.785
3.056 ± 0.510
15.887 ± 3.061
14.858 ± 5.523
0.961 ± 0.722"
REFERENCES,0.4466580976863753,Table 10: Accuracy barrier for decision lists with WM.
REFERENCES,0.4473007712082262,"Train
Test
Dataset
Naive
Perm
Naive (Modified) Perm (Modified)
Naive
Perm
Naive (Modified) Perm (Modified)"
REFERENCES,0.44794344473007713,"Bioresponse
21.323 ± 6.563 4.259 ± 0.698
14.578 ± 3.930
4.641 ± 0.918
9.325 ± 3.988 0.346 ± 0.277
7.346 ± 4.261
1.309 ± 0.827
Diabetes130US
5.182 ± 3.745
1.483 ± 1.006
2.754 ± 1.098
1.088 ± 0.608
4.910 ± 4.244 1.293 ± 1.332
1.476 ± 1.308
0.849 ± 0.885
Higgs
27.778 ± 1.036 16.110 ± 0.518
28.915 ± 1.314
14.071 ± 0.395
4.777 ± 0.803 0.106 ± 0.203
5.136 ± 0.946
0.039 ± 0.083
MagicTelescope
4.855 ± 3.388
0.355 ± 0.682
5.138 ± 2.655
0.182 ± 0.141
4.137 ± 3.763 0.280 ± 0.519
4.534 ± 2.588
0.157 ± 0.162
MiniBooNE
23.059 ± 1.479 1.911 ± 0.138
14.916 ± 3.616
1.580 ± 0.178
17.248 ± 1.683 0.025 ± 0.036
9.340 ± 3.585
0.035 ± 0.042
bank-marketing
11.952 ± 3.794 0.979 ± 0.478
11.589 ± 2.167
0.373 ± 0.448
11.387 ± 4.113 0.536 ± 0.472
10.540 ± 2.067
0.349 ± 0.348
california
6.522 ± 3.195
0.621 ± 0.363
8.435 ± 3.273
0.538 ± 0.214
5.167 ± 2.962 0.236 ± 0.146
6.844 ± 3.087
0.151 ± 0.147
covertype
13.408 ± 3.839 1.341 ± 0.433
11.114 ± 2.689
1.257 ± 0.904
11.162 ± 3.620 0.472 ± 0.340
8.826 ± 2.729
0.477 ± 0.889
credit
11.238 ± 8.115 1.968 ± 0.990
14.626 ± 5.448
1.390 ± 0.423
10.880 ± 9.040 1.421 ± 1.046
13.667 ± 5.951
0.940 ± 0.612
default-of-credit-card-clients 12.513 ± 5.116 3.107 ± 1.123
11.378 ± 2.123
3.793 ± 0.881
5.161 ± 4.304 0.328 ± 0.512
3.197 ± 1.916
0.666 ± 0.651
electricity
6.524 ± 1.863
0.725 ± 0.451
9.101 ± 2.685
0.944 ± 0.557
5.834 ± 1.838 0.420 ± 0.354
8.487 ± 2.460
0.543 ± 0.511
eye_movements
19.125 ± 1.791 9.433 ± 1.385
19.738 ± 1.490
8.755 ± 1.391
1.990 ± 1.623 0.329 ± 0.102
1.916 ± 1.492
0.277 ± 0.302
heloc
4.513 ± 1.826
1.564 ± 0.617
5.116 ± 0.793
1.574 ± 0.154
0.725 ± 0.598 0.155 ± 0.190
1.263 ± 0.711
0.359 ± 0.346
house_16H
9.195 ± 2.408
2.520 ± 0.446
8.693 ± 1.302
2.222 ± 0.730
4.629 ± 2.314 0.063 ± 0.129
4.192 ± 1.517
0.185 ± 0.296
jannis
20.766 ± 2.097 9.484 ± 0.371
20.520 ± 1.017
7.400 ± 0.324
3.947 ± 2.605 0.006 ± 0.013
4.451 ± 1.300
0.004 ± 0.009
pol
23.401 ± 5.448 3.137 ± 1.038
20.137 ± 4.200
3.435 ± 0.675
18.933 ± 5.249 0.952 ± 0.925
16.522 ± 3.502
1.143 ± 0.565"
REFERENCES,0.448586118251928,Table 11: Accuracy barrier for decision lists with AM.
REFERENCES,0.44922879177377895,"Train
Test
Dataset
Naive
Perm
Naive (Modified) Perm (Modified)
Naive
Perm
Naive (Modified) Perm (Modified)"
REFERENCES,0.4498714652956298,"Bioresponse
21.323 ± 6.563 13.349 ± 5.943
14.578 ± 3.930
10.363 ± 7.256
9.325 ± 3.988
4.817 ± 2.825
7.346 ± 4.261
3.871 ± 4.608
Diabetes130US
5.182 ± 3.745
5.590 ± 3.328
2.754 ± 1.098
1.371 ± 0.507
4.910 ± 4.244
4.926 ± 3.796
1.476 ± 1.308
0.694 ± 0.649
Higgs
27.778 ± 1.036 28.910 ± 2.132
28.915 ± 1.314
20.131 ± 1.693
4.777 ± 0.803
6.722 ± 1.231
5.136 ± 0.946
1.755 ± 1.403
MagicTelescope
4.855 ± 3.388
3.349 ± 3.273
5.138 ± 2.655
1.451 ± 0.705
4.137 ± 3.763
3.001 ± 3.478
4.534 ± 2.588
1.090 ± 0.437
MiniBooNE
23.059 ± 1.479 18.149 ± 7.500
14.916 ± 3.616
3.870 ± 1.168
17.248 ± 1.683 13.868 ± 7.222
9.340 ± 3.585
0.797 ± 0.860
bank-marketing
11.952 ± 3.794 9.782 ± 6.722
11.589 ± 2.167
2.815 ± 0.957
11.387 ± 4.113
9.151 ± 7.204
10.540 ± 2.067
2.521 ± 1.055
california
6.522 ± 3.195
5.812 ± 2.365
8.435 ± 3.273
2.254 ± 0.813
5.167 ± 2.962
4.899 ± 2.018
6.844 ± 3.087
1.186 ± 0.643
covertype
13.408 ± 3.839 14.727 ± 7.029
11.114 ± 2.689
4.036 ± 1.450
11.162 ± 3.620 13.352 ± 7.056
8.826 ± 2.729
2.656 ± 1.302
credit
11.238 ± 8.115 18.620 ± 9.806
14.626 ± 5.448
8.979 ± 6.919
10.880 ± 9.040 18.606 ± 10.015
13.667 ± 5.951
8.113 ± 6.633
default-of-credit-card-clients 12.513 ± 5.116 12.880 ± 5.070
11.378 ± 2.123
6.055 ± 1.178
5.161 ± 4.304
6.465 ± 5.062
3.197 ± 1.916
0.533 ± 0.239
electricity
6.524 ± 1.863
4.988 ± 2.732
9.101 ± 2.685
3.041 ± 0.676
5.834 ± 1.838
4.361 ± 2.532
8.487 ± 2.460
2.637 ± 0.730
eye_movements
19.125 ± 1.791 18.694 ± 1.774
19.738 ± 1.490
13.408 ± 1.196
1.990 ± 1.623
3.046 ± 1.625
1.916 ± 1.492
1.807 ± 1.312
heloc
4.513 ± 1.826
5.504 ± 1.650
5.116 ± 0.793
3.287 ± 0.758
0.725 ± 0.598
1.711 ± 1.278
1.263 ± 0.711
0.528 ± 0.147
house_16H
9.195 ± 2.408
8.591 ± 3.370
8.693 ± 1.302
3.937 ± 0.816
4.629 ± 2.314
4.547 ± 2.726
4.192 ± 1.517
0.751 ± 0.508
jannis
20.766 ± 2.097 20.768 ± 2.200
20.520 ± 1.017
12.008 ± 0.892
3.947 ± 2.605
6.472 ± 2.342
4.451 ± 1.300
0.106 ± 0.162
pol
23.401 ± 5.448 17.384 ± 6.441
20.137 ± 4.200
10.339 ± 2.743
18.933 ± 5.249 13.285 ± 5.863
16.522 ± 3.502
6.492 ± 2.536"
REFERENCES,0.4505141388174807,"Table 12: Training accuracy barrier for permuted models with WM. The numbers in parentheses
represent the original accuracy."
REFERENCES,0.45115681233933164,"Dataset
Non-Oblivious Tree
Oblivious Tree
Decision List
Decision List (Modified)"
REFERENCES,0.4517994858611825,"Bioresponse
5.876 ± 1.477 (93.005)
4.800 ± 0.895 (91.753)
4.259 ± 0.698 (91.771)
4.641 ± 0.918 (90.489)
Diabetes130US
1.388 ± 1.159 (60.686)
1.120 ± 1.123 (60.567)
1.483 ± 1.006 (60.425)
1.088 ± 0.608 (61.178)
Higgs
18.470 ± 0.769 (97.232)
19.754 ± 1.023 (97.616) 16.110 ± 0.518 (95.838) 14.071 ± 0.395 (95.831)
MagicTelescope
0.576 ± 0.556 (84.963)
0.473 ± 0.632 (84.460)
0.355 ± 0.682 (84.999)
0.182 ± 0.141 (85.411)
MiniBooNE
2.272 ± 0.215 (99.980)
2.388 ± 0.194 (99.980)
1.911 ± 0.138 (99.977)
1.580 ± 0.178 (99.976)
bank-marketing
2.711 ± 1.183 (79.490)
2.998 ± 1.582 (79.351)
0.979 ± 0.478 (79.166)
0.373 ± 0.448 (79.709)
california
0.873 ± 0.551 (87.897)
0.874 ± 0.524 (87.909)
0.621 ± 0.363 (88.012)
0.538 ± 0.214 (88.054)
covertype
1.839 ± 0.336 (79.445)
2.073 ± 0.657 (79.754)
1.341 ± 0.433 (79.618)
1.257 ± 0.904 (79.550)
credit
3.172 ± 2.636 (78.679)
2.369 ± 0.887 (78.231)
1.968 ± 0.990 (78.166)
1.390 ± 0.423 (78.905)
default-of-credit-card-clients
5.419 ± 1.318 (78.017)
4.512 ± 1.033 (78.657)
3.107 ± 1.123 (77.315)
3.793 ± 0.881 (78.308)
electricity
1.035 ± 0.543 (80.375)
1.060 ± 0.684 (80.861)
0.725 ± 0.451 (80.396)
0.944 ± 0.557 (80.651)
eye_movements
11.605 ± 1.927 (81.693)
12.687 ± 1.645 (83.730)
9.433 ± 1.385 (81.075)
8.755 ± 1.391 (81.451)
heloc
1.652 ± 0.475 (77.430)
2.517 ± 1.156 (78.370)
1.564 ± 0.617 (77.968)
1.574 ± 0.154 (78.550)
house_16H
3.362 ± 0.482 (93.093)
3.302 ± 0.376 (93.351)
2.520 ± 0.446 (92.783)
2.222 ± 0.730 (93.058)
jannis
10.442 ± 1.404 (100.000) 11.358 ± 0.377 (100.000) 9.484 ± 0.371 (100.000) 7.400 ± 0.324 (100.000)
pol
4.612 ± 0.912 (98.348)
5.059 ± 1.482 (98.340)
3.137 ± 1.038 (97.883)
3.435 ± 0.675 (97.881)"
REFERENCES,0.4524421593830334,"Table 13: Training accuracy barrier for permuted models with AM. The numbers in parentheses
represent the original accuracy."
REFERENCES,0.4530848329048843,"Dataset
Non-Oblivious
Oblivious
Decision List
Decision List (Modified)"
REFERENCES,0.4537275064267352,"Interpolation
70 80 90"
REFERENCES,0.45437017994858614,Accuracy
REFERENCES,0.455012853470437,Bioresponse
REFERENCES,0.4556555269922879,Interpolation 55 60
REFERENCES,0.45629820051413883,Diabetes130US
REFERENCES,0.4569408740359897,"Interpolation
60 80"
HIGGS,0.45758354755784064,"100
Higgs"
HIGGS,0.4582262210796915,Interpolation 80 85
HIGGS,0.4588688946015424,MagicTelescope
HIGGS,0.45951156812339333,Interpolation 60 80 100
HIGGS,0.4601542416452442,MiniBooNE
HIGGS,0.4607969151670951,"Interpolation
60 70 80"
HIGGS,0.461439588688946,bank-marketing
HIGGS,0.4620822622107969,"Interpolation
75 80 85"
HIGGS,0.46272493573264784,california
HIGGS,0.4633676092544987,Interpolation 60 80
HIGGS,0.4640102827763496,covertype
HIGGS,0.4646529562982005,Interpolation 70 75
HIGGS,0.4652956298200514,Accuracy
HIGGS,0.46593830334190234,credit
HIGGS,0.4665809768637532,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4672236503856041,"80
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.46786632390745503,Interpolation 60 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4685089974293059,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4691516709511568,"Interpolation
60 70 80"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4697943444730077,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4704370179948586,Interpolation 70
HELOC,0.47107969151670953,"80
heloc"
HELOC,0.4717223650385604,Interpolation 80 90
HELOC,0.4723650385604113,house_16H
HELOC,0.4730077120822622,Interpolation 80 100
HELOC,0.4736503856041131,jannis
HELOC,0.47429305912596403,Interpolation 80 90
POL,0.4749357326478149,"100
pol"
POL,0.4755784061696658,"Naive
Tree Permutation
Ours"
POL,0.4762210796915167,Figure 9: Interpolation curves of train accuracy for oblivious trees with AM.
POL,0.4768637532133676,Interpolation 70 75
POL,0.47750642673521854,Accuracy
POL,0.4781491002570694,Bioresponse
POL,0.4787917737789203,Interpolation 55 60
POL,0.4794344473007712,Diabetes130US
POL,0.4800771208226221,"Interpolation
55 60 65 Higgs"
POL,0.480719794344473,Interpolation 80 85
POL,0.4813624678663239,MagicTelescope
POL,0.4820051413881748,Interpolation 60 80
POL,0.48264781491002573,MiniBooNE
POL,0.4832904884318766,Interpolation 60 70
POL,0.4839331619537275,bank-marketing
POL,0.4845758354755784,Interpolation 75 80 85
POL,0.4852185089974293,california
POL,0.48586118251928023,"Interpolation
50 60 70"
POL,0.4865038560411311,covertype
POL,0.487146529562982,Interpolation 70 75
POL,0.4877892030848329,Accuracy
POL,0.4884318766066838,credit
POL,0.4890745501285347,Interpolation 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4897172236503856,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4903598971722365,Interpolation 60 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4910025706940874,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4916452442159383,Interpolation 54 56 58
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4922879177377892,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4929305912596401,Interpolation 65 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.493573264781491,Interpolation 75 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.49421593830334193,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4948586118251928,Interpolation 65 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4955012853470437,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4961439588688946,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4967866323907455,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4974293059125964,Figure 10: Interpolation curves of test accuracy for oblivious trees with AM.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4980719794344473,Interpolation 80 90
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4987146529562982,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.4993573264781491,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5,Interpolation 55 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5006426735218509,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5012853470437018,"Interpolation
70 80 90 Higgs"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5019280205655527,Interpolation 82 84 86
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5025706940874036,MagicTelescope
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5032133676092545,Interpolation 80 100
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5038560411311054,MiniBooNE
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5044987146529563,"Interpolation
60 70 80"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5051413881748072,bank-marketing
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5057840616966581,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.506426735218509,california
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5070694087403599,Interpolation 60 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5077120822622108,covertype
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5083547557840618,Interpolation 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5089974293059126,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5096401028277635,credit
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5102827763496144,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5109254498714653,"80
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5115681233933161,"Interpolation
70 75 80"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5122107969151671,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.512853470437018,"Interpolation
60 70 80"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5134961439588689,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5141388174807198,"Interpolation
70 75 heloc"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5147814910025706,Interpolation 80 90
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5154241645244216,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5160668380462725,"Interpolation
80 90 100"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5167095115681234,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5173521850899743,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5179948586118251,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5186375321336761,Figure 11: Interpolation curves of train accuracy for oblivious trees with WM.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.519280205655527,Interpolation 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5199228791773779,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5205655526992288,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5212082262210797,Interpolation 55 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5218508997429306,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5224935732647815,Interpolation 62.5 65.0 Higgs
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5231362467866324,Interpolation 82 84
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5237789203084833,MagicTelescope
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5244215938303342,"Interpolation
70 80 90"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5250642673521851,MiniBooNE
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.525706940874036,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5263496143958869,bank-marketing
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5269922879177378,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5276349614395887,california
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5282776349614395,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5289203084832905,covertype
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5295629820051414,Interpolation 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5302056555269923,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5308483290488432,credit
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.531491002570694,Interpolation 60 65
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.532133676092545,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5327763496143959,"Interpolation
70 75 80"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5334190231362468,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5340616966580977,Interpolation 54 56 58
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5347043701799485,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5353470437017995,Interpolation 66 68 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5359897172236504,"Interpolation
75 80 85"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5366323907455013,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5372750642673522,Interpolation 70 72
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.537917737789203,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.538560411311054,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5392030848329049,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5398457583547558,Figure 12: Interpolation curves of test accuracy for oblivious trees with WM.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5404884318766067,"Interpolation
60 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5411311053984575,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5417737789203085,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5424164524421594,"Interpolation
50 55 60"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5430591259640103,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5437017994858612,Interpolation 65 70
HIGGS,0.544344473007712,"75
Higgs"
HIGGS,0.5449871465295629,Interpolation 75 80 85
HIGGS,0.5456298200514139,MagicTelescope
HIGGS,0.5462724935732648,Interpolation 70 80 90
HIGGS,0.5469151670951157,MiniBooNE
HIGGS,0.5475578406169666,Interpolation 70 75
HIGGS,0.5482005141388174,bank-marketing
HIGGS,0.5488431876606684,"Interpolation
75 80 85"
HIGGS,0.5494858611825193,california
HIGGS,0.5501285347043702,"Interpolation
50 60 70"
HIGGS,0.550771208226221,covertype
HIGGS,0.5514138817480719,Interpolation 60 70
HIGGS,0.5520565552699229,Accuracy
HIGGS,0.5526992287917738,credit
HIGGS,0.5533419023136247,Interpolation 60 70
HIGGS,0.5539845758354756,default-of-credit-card-clients
HIGGS,0.5546272493573264,Interpolation 70
ELECTRICITY,0.5552699228791774,"80
electricity"
ELECTRICITY,0.5559125964010283,Interpolation 55 60 65
ELECTRICITY,0.5565552699228792,eye_movements
ELECTRICITY,0.5571979434447301,"Interpolation
60 70 heloc"
ELECTRICITY,0.5578406169665809,Interpolation 80 85
ELECTRICITY,0.5584832904884319,"90
house_16H"
ELECTRICITY,0.5591259640102828,Interpolation 75 80
ELECTRICITY,0.5597686375321337,jannis
ELECTRICITY,0.5604113110539846,Interpolation 70 80 90 pol
ELECTRICITY,0.5610539845758354,"Naive
Tree Permutation
Ours"
ELECTRICITY,0.5616966580976864,Figure 13: Interpolation curves of train accuracy for oblivious trees with AM by use of split dataset.
ELECTRICITY,0.5623393316195373,Interpolation 60 70
ELECTRICITY,0.5629820051413882,Accuracy
ELECTRICITY,0.5636246786632391,Bioresponse
ELECTRICITY,0.5642673521850899,"Interpolation
50 55 60"
ELECTRICITY,0.5649100257069408,Diabetes130US
ELECTRICITY,0.5655526992287918,Interpolation 60 65 Higgs
ELECTRICITY,0.5661953727506427,Interpolation 75 80
MAGICTELESCOPE,0.5668380462724936,85MagicTelescope
MAGICTELESCOPE,0.5674807197943444,Interpolation 70 80 90
MAGICTELESCOPE,0.5681233933161953,MiniBooNE
MAGICTELESCOPE,0.5687660668380463,Interpolation 70 75
MAGICTELESCOPE,0.5694087403598972,bank-marketing
MAGICTELESCOPE,0.5700514138817481,"Interpolation
75 80 85"
MAGICTELESCOPE,0.570694087403599,california
MAGICTELESCOPE,0.5713367609254498,"Interpolation
50 60 70"
MAGICTELESCOPE,0.5719794344473008,covertype
MAGICTELESCOPE,0.5726221079691517,Interpolation 60 70
MAGICTELESCOPE,0.5732647814910026,Accuracy
MAGICTELESCOPE,0.5739074550128535,credit
MAGICTELESCOPE,0.5745501285347043,Interpolation 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5751928020565553,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5758354755784062,"Interpolation
65 70 75"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5764781491002571,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.577120822622108,Interpolation 52.5 55.0 57.5
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5777634961439588,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5784061696658098,Interpolation 60 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5790488431876607,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5796915167095116,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5803341902313625,Interpolation 65 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5809768637532133,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5816195372750642,Interpolation 70 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5822622107969152,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5829048843187661,Figure 14: Interpolation curves of test accuracy for oblivious trees with AM by use of split dataset.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.583547557840617,"Interpolation
60 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5841902313624678,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5848329048843187,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5854755784061697,"Interpolation
50 55 60"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5861182519280206,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5867609254498715,Interpolation 65 70 Higgs
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5874035989717223,Interpolation 75 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5880462724935732,MagicTelescope
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5886889460154242,Interpolation 80 90
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5893316195372751,MiniBooNE
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.589974293059126,Interpolation 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5906169665809768,bank-marketing
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5912596401028277,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5919023136246787,california
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5925449871465296,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5931876606683805,covertype
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5938303341902313,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5944730077120822,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5951156812339332,credit
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5957583547557841,"Interpolation
60 65 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.596401028277635,default-of-credit-card-clients
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.5970437017994858,Interpolation 70
ELECTRICITY,0.5976863753213367,"80
electricity"
ELECTRICITY,0.5983290488431876,Interpolation 55 60 65
ELECTRICITY,0.5989717223650386,eye_movements
ELECTRICITY,0.5996143958868895,"Interpolation
60 70 heloc"
ELECTRICITY,0.6002570694087404,"Interpolation
80 85"
ELECTRICITY,0.6008997429305912,house_16H
ELECTRICITY,0.6015424164524421,Interpolation 75 80
ELECTRICITY,0.6021850899742931,jannis
ELECTRICITY,0.602827763496144,"Interpolation
70 80 90 pol"
ELECTRICITY,0.6034704370179949,"Naive
Tree Permutation
Ours"
ELECTRICITY,0.6041131105398457,Figure 15: Interpolation curves of train accuracy for oblivious trees with WM by use of split dataset.
ELECTRICITY,0.6047557840616966,Interpolation 60 70
ELECTRICITY,0.6053984575835476,Accuracy
ELECTRICITY,0.6060411311053985,Bioresponse
ELECTRICITY,0.6066838046272494,"Interpolation
50 55 60"
ELECTRICITY,0.6073264781491002,Diabetes130US
ELECTRICITY,0.6079691516709511,Interpolation 60 65 Higgs
ELECTRICITY,0.6086118251928021,Interpolation 75 80
MAGICTELESCOPE,0.609254498714653,85MagicTelescope
MAGICTELESCOPE,0.6098971722365039,Interpolation 80 90
MAGICTELESCOPE,0.6105398457583547,MiniBooNE
MAGICTELESCOPE,0.6111825192802056,Interpolation 70 75
MAGICTELESCOPE,0.6118251928020566,bank-marketing
MAGICTELESCOPE,0.6124678663239075,Interpolation 80 85
MAGICTELESCOPE,0.6131105398457584,california
MAGICTELESCOPE,0.6137532133676092,Interpolation 60 70
MAGICTELESCOPE,0.6143958868894601,covertype
MAGICTELESCOPE,0.6150385604113111,Interpolation 60 70
MAGICTELESCOPE,0.615681233933162,Accuracy
MAGICTELESCOPE,0.6163239074550129,credit
MAGICTELESCOPE,0.6169665809768637,Interpolation 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6176092544987146,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6182519280205655,"Interpolation
65 70 75"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6188946015424165,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6195372750642674,Interpolation 52.5 55.0 57.5
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6201799485861182,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6208226221079691,Interpolation 60 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.62146529562982,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.622107969151671,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6227506426735219,Interpolation 65 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6233933161953727,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6240359897172236,"Interpolation
70 80 90 pol"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6246786632390745,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6253213367609255,Figure 16: Interpolation curves of test accuracy for oblivious trees with WM by use of split dataset.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6259640102827764,Interpolation 70 80 90
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6266066838046273,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6272493573264781,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.627892030848329,Interpolation 55 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.62853470437018,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6291773778920309,Interpolation 70 80 90 Higgs
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6298200514138818,"Interpolation
80.0 82.5 85.0"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6304627249357326,MagicTelescope
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6311053984575835,Interpolation 80 100
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6317480719794345,MiniBooNE
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6323907455012854,"Interpolation
60 70 80"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6330334190231363,bank-marketing
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6336760925449871,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.634318766066838,california
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6349614395886889,Interpolation 60 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6356041131105399,covertype
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6362467866323908,Interpolation 60 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6368894601542416,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6375321336760925,credit
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6381748071979434,"Interpolation
60 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6388174807197944,"80
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6394601542416453,Interpolation 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6401028277634961,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.640745501285347,Interpolation 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6413881748071979,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6420308483290489,Interpolation 70 75 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6426735218508998,Interpolation 85 90
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6433161953727506,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6439588688946015,"Interpolation
80 90 100"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6446015424164524,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6452442159383034,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6458868894601543,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6465295629820051,Figure 17: Interpolation curves of train accuracy for non-oblivious trees with AM.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.647172236503856,Interpolation 65 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6478149100257069,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6484575835475579,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6491002570694088,Interpolation 55 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6497429305912596,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6503856041131105,Interpolation 60 65 Higgs
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6510282776349614,Interpolation 82 84 86
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6516709511568124,MagicTelescope
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6523136246786633,Interpolation 70 80 90
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6529562982005142,MiniBooNE
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.653598971722365,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6542416452442159,bank-marketing
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6548843187660668,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6555269922879178,california
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6561696658097687,"Interpolation
50 60 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6568123393316195,covertype
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6574550128534704,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6580976863753213,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6587403598971723,credit
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6593830334190232,Interpolation 60 65
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.660025706940874,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6606683804627249,Interpolation 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6613110539845758,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6619537275064268,Interpolation 56 58
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6625964010282777,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6632390745501285,Interpolation 65 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6638817480719794,"Interpolation
80 85"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6645244215938303,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6651670951156813,Interpolation 70
JANNIS,0.6658097686375322,"75
jannis"
JANNIS,0.666452442159383,Interpolation 80 90 pol
JANNIS,0.6670951156812339,"Naive
Tree Permutation
Ours"
JANNIS,0.6677377892030848,Figure 18: Interpolation curves of test accuracy for non-oblivious trees with AM.
JANNIS,0.6683804627249358,Interpolation 70 80 90
JANNIS,0.6690231362467867,Accuracy
JANNIS,0.6696658097686375,Bioresponse
JANNIS,0.6703084832904884,"Interpolation
58 60 62"
JANNIS,0.6709511568123393,Diabetes130US
JANNIS,0.6715938303341902,"Interpolation
70 80 90 Higgs"
JANNIS,0.6722365038560412,"Interpolation
80.0 82.5 85.0"
JANNIS,0.672879177377892,MagicTelescope
JANNIS,0.6735218508997429,Interpolation 80 90 100
JANNIS,0.6741645244215938,MiniBooNE
JANNIS,0.6748071979434447,Interpolation 70 80
JANNIS,0.6754498714652957,bank-marketing
JANNIS,0.6760925449871465,Interpolation 80 85
JANNIS,0.6767352185089974,california
JANNIS,0.6773778920308483,"Interpolation
60 70 80"
JANNIS,0.6780205655526992,covertype
JANNIS,0.6786632390745502,Interpolation 70 75 80
JANNIS,0.679305912596401,Accuracy
JANNIS,0.6799485861182519,credit
JANNIS,0.6805912596401028,"Interpolation
60 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6812339331619537,"80
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6818766066838047,Interpolation 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6825192802056556,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6831619537275064,Interpolation 70 80
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6838046272493573,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6844473007712082,"Interpolation
72.5 75.0 77.5 heloc"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6850899742930592,Interpolation 85 90
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6857326478149101,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6863753213367609,"Interpolation
80 90 100"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6870179948586118,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6876606683804627,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6883033419023136,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6889460154241646,Figure 19: Interpolation curves of train accuracy for non-oblivious trees with WM.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6895886889460154,Interpolation 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6902313624678663,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6908740359897172,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6915167095115681,Interpolation 55 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6921593830334191,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6928020565552699,Interpolation 62.5 65.0 Higgs
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6934447300771208,Interpolation 82 84
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6940874035989717,MagicTelescope
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6947300771208226,"Interpolation
70 80 90"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6953727506426736,MiniBooNE
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6960154241645244,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6966580976863753,bank-marketing
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6973007712082262,Interpolation 80 85
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6979434447300771,california
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6985861182519281,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.699228791773779,covertype
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.6998714652956298,Interpolation 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7005141388174807,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7011568123393316,credit
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7017994858611826,Interpolation 60 65
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7024421593830334,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7030848329048843,"Interpolation
70 75 80"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7037275064267352,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7043701799485861,Interpolation 54 56 58
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7050128534704371,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.705655526992288,Interpolation 66 68 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7062982005141388,"Interpolation
75 80 85"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7069408740359897,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7075835475578406,Interpolation 70 72
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7082262210796915,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7088688946015425,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7095115681233933,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7101542416452442,Figure 20: Interpolation curves of test accuracy for non-oblivious trees with WM.
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7107969151670951,Interpolation 60 70
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.711439588688946,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.712082262210797,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7127249357326478,"Interpolation
50 55 60"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7133676092544987,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7140102827763496,Interpolation 65 70
HIGGS,0.7146529562982005,"75
Higgs"
HIGGS,0.7152956298200515,"Interpolation
70 80"
HIGGS,0.7159383033419023,MagicTelescope
HIGGS,0.7165809768637532,"Interpolation
70 80 90"
HIGGS,0.7172236503856041,MiniBooNE
HIGGS,0.717866323907455,Interpolation 70 80
HIGGS,0.718508997429306,bank-marketing
HIGGS,0.7191516709511568,"Interpolation
75 80 85"
HIGGS,0.7197943444730077,california
HIGGS,0.7204370179948586,Interpolation 60 70
HIGGS,0.7210796915167095,covertype
HIGGS,0.7217223650385605,"Interpolation
50 60 70"
HIGGS,0.7223650385604113,Accuracy
HIGGS,0.7230077120822622,credit
HIGGS,0.7236503856041131,Interpolation 60 70
HIGGS,0.724293059125964,default-of-credit-card-clients
HIGGS,0.7249357326478149,Interpolation 70
ELECTRICITY,0.7255784061696658,"80
electricity"
ELECTRICITY,0.7262210796915167,Interpolation 55 60 65
ELECTRICITY,0.7268637532133676,eye_movements
ELECTRICITY,0.7275064267352185,Interpolation 65 70
HELOC,0.7281491002570694,"75
heloc"
HELOC,0.7287917737789203,"Interpolation
84 86 88"
HELOC,0.7294344473007712,house_16H
HELOC,0.7300771208226221,Interpolation 75 80
HELOC,0.730719794344473,jannis
HELOC,0.7313624678663239,"Interpolation
70 80 90 pol"
HELOC,0.7320051413881749,"Naive
Tree Permutation
Ours"
HELOC,0.7326478149100257,"Figure 21: Interpolation curves of train accuracy for non-oblivious trees with AM by use of split
dataset."
HELOC,0.7332904884318766,Interpolation 60 70
HELOC,0.7339331619537275,Accuracy
HELOC,0.7345758354755784,Bioresponse
HELOC,0.7352185089974294,"Interpolation
50 55 60"
HELOC,0.7358611825192802,Diabetes130US
HELOC,0.7365038560411311,Interpolation 60 65 Higgs
HELOC,0.737146529562982,Interpolation 70 80
HELOC,0.7377892030848329,MagicTelescope
HELOC,0.7384318766066839,"Interpolation
70 80 90"
HELOC,0.7390745501285347,MiniBooNE
HELOC,0.7397172236503856,Interpolation 65 70 75
HELOC,0.7403598971722365,bank-marketing
HELOC,0.7410025706940874,"Interpolation
75 80 85"
HELOC,0.7416452442159382,california
HELOC,0.7422879177377892,Interpolation 60 70
HELOC,0.7429305912596401,covertype
HELOC,0.743573264781491,Interpolation 60 80
HELOC,0.7442159383033419,Accuracy
HELOC,0.7448586118251928,credit
HELOC,0.7455012853470437,Interpolation 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7461439588688946,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7467866323907455,Interpolation 65 70 75
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7474293059125964,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7480719794344473,"Interpolation
50 55"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7487146529562982,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7493573264781491,Interpolation 60 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.75,"Interpolation
82 84 86"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7506426735218509,house_16H
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7512853470437018,"Interpolation
65 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7519280205655527,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7525706940874036,"Interpolation
70 80 90 pol"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7532133676092545,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7538560411311054,"Figure 22: Interpolation curves of test accuracy for non-oblivious trees with AM by use of split
dataset."
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7544987146529563,"Interpolation
60 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7551413881748072,Accuracy
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7557840616966581,Bioresponse
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.756426735218509,"Interpolation
50 55 60"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7570694087403599,Diabetes130US
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7577120822622108,Interpolation 70
HIGGS,0.7583547557840618,"75
Higgs"
HIGGS,0.7589974293059126,"Interpolation
70 80"
HIGGS,0.7596401028277635,MagicTelescope
HIGGS,0.7602827763496144,"Interpolation
80 90"
HIGGS,0.7609254498714653,MiniBooNE
HIGGS,0.7615681233933161,Interpolation 70
BANK-MARKETING,0.7622107969151671,80bank-marketing
BANK-MARKETING,0.762853470437018,Interpolation 80 85
BANK-MARKETING,0.7634961439588689,california
BANK-MARKETING,0.7641388174807198,"Interpolation
60 70"
BANK-MARKETING,0.7647814910025706,covertype
BANK-MARKETING,0.7654241645244216,"Interpolation
50 60 70"
BANK-MARKETING,0.7660668380462725,Accuracy
BANK-MARKETING,0.7667095115681234,credit
BANK-MARKETING,0.7673521850899743,Interpolation 60 70
BANK-MARKETING,0.7679948586118251,default-of-credit-card-clients
BANK-MARKETING,0.7686375321336761,Interpolation 70 80
BANK-MARKETING,0.769280205655527,electricity
BANK-MARKETING,0.7699228791773779,Interpolation 55 60 65
BANK-MARKETING,0.7705655526992288,eye_movements
BANK-MARKETING,0.7712082262210797,Interpolation 65 70
HELOC,0.7718508997429306,"75
heloc"
HELOC,0.7724935732647815,Interpolation 86 88
HELOC,0.7731362467866324,house_16H
HELOC,0.7737789203084833,Interpolation 75 80
HELOC,0.7744215938303342,jannis
HELOC,0.7750642673521851,Interpolation 80 90 pol
HELOC,0.775706940874036,"Naive
Tree Permutation
Ours"
HELOC,0.7763496143958869,"Figure 23: Interpolation curves of train accuracy for non-oblivious trees with WM by use of split
dataset."
HELOC,0.7769922879177378,"Interpolation
60 70"
HELOC,0.7776349614395887,Accuracy
HELOC,0.7782776349614395,Bioresponse
HELOC,0.7789203084832905,"Interpolation
50 55 60"
HELOC,0.7795629820051414,Diabetes130US
HELOC,0.7802056555269923,Interpolation 60 65 Higgs
HELOC,0.7808483290488432,Interpolation 70 80
HELOC,0.781491002570694,MagicTelescope
HELOC,0.782133676092545,"Interpolation
80 90"
HELOC,0.7827763496143959,MiniBooNE
HELOC,0.7834190231362468,Interpolation 65 70 75
HELOC,0.7840616966580977,bank-marketing
HELOC,0.7847043701799485,Interpolation 80 85
HELOC,0.7853470437017995,california
HELOC,0.7859897172236504,"Interpolation
60 70"
HELOC,0.7866323907455013,covertype
HELOC,0.7872750642673522,"Interpolation
50 60 70"
HELOC,0.787917737789203,Accuracy
HELOC,0.788560411311054,credit
HELOC,0.7892030848329049,Interpolation 60
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7898457583547558,"70
default-of-credit-card-clients"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7904884318766067,"Interpolation
65 70 75"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7911311053984575,electricity
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7917737789203085,Interpolation 52.5 55.0 57.5
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7924164524421594,eye_movements
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7930591259640103,Interpolation 60 70 heloc
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7937017994858612,Interpolation 82.5 85.0
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.794344473007712,"87.5
house_16H"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7949871465295629,"Interpolation
65 70"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7956298200514139,jannis
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7962724935732648,Interpolation 80 90 pol
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7969151670951157,"Naive
Tree Permutation
Ours"
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7975578406169666,"Figure 24: Interpolation curves of test accuracy for non-oblivious trees with WM by use of split
dataset."
DEFAULT-OF-CREDIT-CARD-CLIENTS,0.7982005141388174,"NeurIPS Paper Checklist
498"
CLAIMS,0.7988431876606684,"1. Claims
499"
CLAIMS,0.7994858611825193,"Question: Do the main claims made in the abstract and introduction accurately reflect the
500"
CLAIMS,0.8001285347043702,"paper’s contributions and scope?
501"
CLAIMS,0.800771208226221,"Answer: [Yes]
502"
CLAIMS,0.8014138817480719,"Justification: The abstract and introduction consistently present our research on tree ensem-
503"
CLAIMS,0.8020565552699229,"bles from LMC perspectives.
504"
CLAIMS,0.8026992287917738,"Guidelines:
505"
CLAIMS,0.8033419023136247,"• The answer NA means that the abstract and introduction do not include the claims
506"
CLAIMS,0.8039845758354756,"made in the paper.
507"
CLAIMS,0.8046272493573264,"• The abstract and/or introduction should clearly state the claims made, including the
508"
CLAIMS,0.8052699228791774,"contributions made in the paper and important assumptions and limitations. A No or
509"
CLAIMS,0.8059125964010283,"NA answer to this question will not be perceived well by the reviewers.
510"
CLAIMS,0.8065552699228792,"• The claims made should match theoretical and experimental results, and reflect how
511"
CLAIMS,0.8071979434447301,"much the results can be expected to generalize to other settings.
512"
CLAIMS,0.8078406169665809,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
513"
CLAIMS,0.8084832904884319,"are not attained by the paper.
514"
LIMITATIONS,0.8091259640102828,"2. Limitations
515"
LIMITATIONS,0.8097686375321337,"Question: Does the paper discuss the limitations of the work performed by the authors?
516"
LIMITATIONS,0.8104113110539846,"Answer: [Yes]
517"
LIMITATIONS,0.8110539845758354,"Justification: In Section 3.2, we have discussed the limitations.
518"
LIMITATIONS,0.8116966580976864,"Guidelines:
519"
LIMITATIONS,0.8123393316195373,"• The answer NA means that the paper has no limitation while the answer No means that
520"
LIMITATIONS,0.8129820051413882,"the paper has limitations, but those are not discussed in the paper.
521"
LIMITATIONS,0.8136246786632391,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
522"
LIMITATIONS,0.8142673521850899,"• The paper should point out any strong assumptions and how robust the results are to
523"
LIMITATIONS,0.8149100257069408,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
524"
LIMITATIONS,0.8155526992287918,"model well-specification, asymptotic approximations only holding locally). The authors
525"
LIMITATIONS,0.8161953727506427,"should reflect on how these assumptions might be violated in practice and what the
526"
LIMITATIONS,0.8168380462724936,"implications would be.
527"
LIMITATIONS,0.8174807197943444,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
528"
LIMITATIONS,0.8181233933161953,"only tested on a few datasets or with a few runs. In general, empirical results often
529"
LIMITATIONS,0.8187660668380463,"depend on implicit assumptions, which should be articulated.
530"
LIMITATIONS,0.8194087403598972,"• The authors should reflect on the factors that influence the performance of the approach.
531"
LIMITATIONS,0.8200514138817481,"For example, a facial recognition algorithm may perform poorly when image resolution
532"
LIMITATIONS,0.820694087403599,"is low or images are taken in low lighting. Or a speech-to-text system might not be
533"
LIMITATIONS,0.8213367609254498,"used reliably to provide closed captions for online lectures because it fails to handle
534"
LIMITATIONS,0.8219794344473008,"technical jargon.
535"
LIMITATIONS,0.8226221079691517,"• The authors should discuss the computational efficiency of the proposed algorithms
536"
LIMITATIONS,0.8232647814910026,"and how they scale with dataset size.
537"
LIMITATIONS,0.8239074550128535,"• If applicable, the authors should discuss possible limitations of their approach to
538"
LIMITATIONS,0.8245501285347043,"address problems of privacy and fairness.
539"
LIMITATIONS,0.8251928020565553,"• While the authors might fear that complete honesty about limitations might be used by
540"
LIMITATIONS,0.8258354755784062,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
541"
LIMITATIONS,0.8264781491002571,"limitations that aren’t acknowledged in the paper. The authors should use their best
542"
LIMITATIONS,0.827120822622108,"judgment and recognize that individual actions in favor of transparency play an impor-
543"
LIMITATIONS,0.8277634961439588,"tant role in developing norms that preserve the integrity of the community. Reviewers
544"
LIMITATIONS,0.8284061696658098,"will be specifically instructed to not penalize honesty concerning limitations.
545"
THEORY ASSUMPTIONS AND PROOFS,0.8290488431876607,"3. Theory Assumptions and Proofs
546"
THEORY ASSUMPTIONS AND PROOFS,0.8296915167095116,"Question: For each theoretical result, does the paper provide the full set of assumptions and
547"
THEORY ASSUMPTIONS AND PROOFS,0.8303341902313625,"a complete (and correct) proof?
548"
THEORY ASSUMPTIONS AND PROOFS,0.8309768637532133,"Answer: [NA]
549"
THEORY ASSUMPTIONS AND PROOFS,0.8316195372750642,"Justification: We do not provide theoretical results in this paper.
550"
THEORY ASSUMPTIONS AND PROOFS,0.8322622107969152,"Guidelines:
551"
THEORY ASSUMPTIONS AND PROOFS,0.8329048843187661,"• The answer NA means that the paper does not include theoretical results.
552"
THEORY ASSUMPTIONS AND PROOFS,0.833547557840617,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
553"
THEORY ASSUMPTIONS AND PROOFS,0.8341902313624678,"referenced.
554"
THEORY ASSUMPTIONS AND PROOFS,0.8348329048843187,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
555"
THEORY ASSUMPTIONS AND PROOFS,0.8354755784061697,"• The proofs can either appear in the main paper or the supplemental material, but if
556"
THEORY ASSUMPTIONS AND PROOFS,0.8361182519280206,"they appear in the supplemental material, the authors are encouraged to provide a short
557"
THEORY ASSUMPTIONS AND PROOFS,0.8367609254498715,"proof sketch to provide intuition.
558"
THEORY ASSUMPTIONS AND PROOFS,0.8374035989717223,"• Inversely, any informal proof provided in the core of the paper should be complemented
559"
THEORY ASSUMPTIONS AND PROOFS,0.8380462724935732,"by formal proofs provided in appendix or supplemental material.
560"
THEORY ASSUMPTIONS AND PROOFS,0.8386889460154242,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
561"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8393316195372751,"4. Experimental Result Reproducibility
562"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.839974293059126,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
563"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8406169665809768,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
564"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8412596401028277,"of the paper (regardless of whether the code and data are provided or not)?
565"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8419023136246787,"Answer: [Yes]
566"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8425449871465296,"Justification: The experimental setup is detailed in Section 4.1.
567"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8431876606683805,"Guidelines:
568"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8438303341902313,"• The answer NA means that the paper does not include experiments.
569"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8444730077120822,"• If the paper includes experiments, a No answer to this question will not be perceived
570"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8451156812339332,"well by the reviewers: Making the paper reproducible is important, regardless of
571"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8457583547557841,"whether the code and data are provided or not.
572"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.846401028277635,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
573"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8470437017994858,"to make their results reproducible or verifiable.
574"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8476863753213367,"• Depending on the contribution, reproducibility can be accomplished in various ways.
575"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8483290488431876,"For example, if the contribution is a novel architecture, describing the architecture fully
576"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8489717223650386,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
577"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8496143958868895,"be necessary to either make it possible for others to replicate the model with the same
578"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8502570694087404,"dataset, or provide access to the model. In general. releasing code and data is often
579"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8508997429305912,"one good way to accomplish this, but reproducibility can also be provided via detailed
580"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8515424164524421,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
581"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8521850899742931,"of a large language model), releasing of a model checkpoint, or other means that are
582"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.852827763496144,"appropriate to the research performed.
583"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8534704370179949,"• While NeurIPS does not require releasing code, the conference does require all submis-
584"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8541131105398457,"sions to provide some reasonable avenue for reproducibility, which may depend on the
585"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8547557840616966,"nature of the contribution. For example
586"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8553984575835476,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
587"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8560411311053985,"to reproduce that algorithm.
588"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8566838046272494,"(b) If the contribution is primarily a new model architecture, the paper should describe
589"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8573264781491002,"the architecture clearly and fully.
590"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8579691516709511,"(c) If the contribution is a new model (e.g., a large language model), then there should
591"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8586118251928021,"either be a way to access this model for reproducing the results or a way to reproduce
592"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.859254498714653,"the model (e.g., with an open-source dataset or instructions for how to construct
593"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8598971722365039,"the dataset).
594"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8605398457583547,"(d) We recognize that reproducibility may be tricky in some cases, in which case
595"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8611825192802056,"authors are welcome to describe the particular way they provide for reproducibility.
596"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8618251928020566,"In the case of closed-source models, it may be that access to the model is limited in
597"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8624678663239075,"some way (e.g., to registered users), but it should be possible for other researchers
598"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.8631105398457584,"to have some path to reproducing or verifying the results.
599"
OPEN ACCESS TO DATA AND CODE,0.8637532133676092,"5. Open access to data and code
600"
OPEN ACCESS TO DATA AND CODE,0.8643958868894601,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
601"
OPEN ACCESS TO DATA AND CODE,0.8650385604113111,"tions to faithfully reproduce the main experimental results, as described in supplemental
602"
OPEN ACCESS TO DATA AND CODE,0.865681233933162,"material?
603"
OPEN ACCESS TO DATA AND CODE,0.8663239074550129,"Answer: [Yes]
604"
OPEN ACCESS TO DATA AND CODE,0.8669665809768637,"Justification: Reproducible source code is provided in the supplementary material.
605"
OPEN ACCESS TO DATA AND CODE,0.8676092544987146,"Guidelines:
606"
OPEN ACCESS TO DATA AND CODE,0.8682519280205655,"• The answer NA means that paper does not include experiments requiring code.
607"
OPEN ACCESS TO DATA AND CODE,0.8688946015424165,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
608"
OPEN ACCESS TO DATA AND CODE,0.8695372750642674,"public/guides/CodeSubmissionPolicy) for more details.
609"
OPEN ACCESS TO DATA AND CODE,0.8701799485861182,"• While we encourage the release of code and data, we understand that this might not be
610"
OPEN ACCESS TO DATA AND CODE,0.8708226221079691,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
611"
OPEN ACCESS TO DATA AND CODE,0.87146529562982,"including code, unless this is central to the contribution (e.g., for a new open-source
612"
OPEN ACCESS TO DATA AND CODE,0.872107969151671,"benchmark).
613"
OPEN ACCESS TO DATA AND CODE,0.8727506426735219,"• The instructions should contain the exact command and environment needed to run to
614"
OPEN ACCESS TO DATA AND CODE,0.8733933161953727,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
615"
OPEN ACCESS TO DATA AND CODE,0.8740359897172236,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
616"
OPEN ACCESS TO DATA AND CODE,0.8746786632390745,"• The authors should provide instructions on data access and preparation, including how
617"
OPEN ACCESS TO DATA AND CODE,0.8753213367609255,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
618"
OPEN ACCESS TO DATA AND CODE,0.8759640102827764,"• The authors should provide scripts to reproduce all experimental results for the new
619"
OPEN ACCESS TO DATA AND CODE,0.8766066838046273,"proposed method and baselines. If only a subset of experiments are reproducible, they
620"
OPEN ACCESS TO DATA AND CODE,0.8772493573264781,"should state which ones are omitted from the script and why.
621"
OPEN ACCESS TO DATA AND CODE,0.877892030848329,"• At submission time, to preserve anonymity, the authors should release anonymized
622"
OPEN ACCESS TO DATA AND CODE,0.87853470437018,"versions (if applicable).
623"
OPEN ACCESS TO DATA AND CODE,0.8791773778920309,"• Providing as much information as possible in supplemental material (appended to the
624"
OPEN ACCESS TO DATA AND CODE,0.8798200514138818,"paper) is recommended, but including URLs to data and code is permitted.
625"
OPEN ACCESS TO DATA AND CODE,0.8804627249357326,"6. Experimental Setting/Details
626"
OPEN ACCESS TO DATA AND CODE,0.8811053984575835,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
627"
OPEN ACCESS TO DATA AND CODE,0.8817480719794345,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
628"
OPEN ACCESS TO DATA AND CODE,0.8823907455012854,"results?
629"
OPEN ACCESS TO DATA AND CODE,0.8830334190231363,"Answer: [Yes]
630"
OPEN ACCESS TO DATA AND CODE,0.8836760925449871,"Justification: The experimental setup is detailed in Section 4.1.
631"
OPEN ACCESS TO DATA AND CODE,0.884318766066838,"Guidelines:
632"
OPEN ACCESS TO DATA AND CODE,0.8849614395886889,"• The answer NA means that the paper does not include experiments.
633"
OPEN ACCESS TO DATA AND CODE,0.8856041131105399,"• The experimental setting should be presented in the core of the paper to a level of detail
634"
OPEN ACCESS TO DATA AND CODE,0.8862467866323908,"that is necessary to appreciate the results and make sense of them.
635"
OPEN ACCESS TO DATA AND CODE,0.8868894601542416,"• The full details can be provided either with the code, in appendix, or as supplemental
636"
OPEN ACCESS TO DATA AND CODE,0.8875321336760925,"material.
637"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8881748071979434,"7. Experiment Statistical Significance
638"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8888174807197944,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
639"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8894601542416453,"information about the statistical significance of the experiments?
640"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8901028277634961,"Answer: [Yes]
641"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.890745501285347,"Justification: We conducted experiments multiple times with different random seeds and
642"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8913881748071979,"have reported the results, including the variability.
643"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8920308483290489,"Guidelines:
644"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8926735218508998,"• The answer NA means that the paper does not include experiments.
645"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8933161953727506,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
646"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8939588688946015,"dence intervals, or statistical significance tests, at least for the experiments that support
647"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8946015424164524,"the main claims of the paper.
648"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8952442159383034,"• The factors of variability that the error bars are capturing should be clearly stated (for
649"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8958868894601543,"example, train/test split, initialization, random drawing of some parameter, or overall
650"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8965295629820051,"run with given experimental conditions).
651"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.897172236503856,"• The method for calculating the error bars should be explained (closed form formula,
652"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8978149100257069,"call to a library function, bootstrap, etc.)
653"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8984575835475579,"• The assumptions made should be given (e.g., Normally distributed errors).
654"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8991002570694088,"• It should be clear whether the error bar is the standard deviation or the standard error
655"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8997429305912596,"of the mean.
656"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9003856041131105,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
657"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9010282776349614,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
658"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9016709511568124,"of Normality of errors is not verified.
659"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9023136246786633,"• For asymmetric distributions, the authors should be careful not to show in tables or
660"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9029562982005142,"figures symmetric error bars that would yield results that are out of range (e.g. negative
661"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.903598971722365,"error rates).
662"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9042416452442159,"• If error bars are reported in tables or plots, The authors should explain in the text how
663"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.9048843187660668,"they were calculated and reference the corresponding figures or tables in the text.
664"
EXPERIMENTS COMPUTE RESOURCES,0.9055269922879178,"8. Experiments Compute Resources
665"
EXPERIMENTS COMPUTE RESOURCES,0.9061696658097687,"Question: For each experiment, does the paper provide sufficient information on the com-
666"
EXPERIMENTS COMPUTE RESOURCES,0.9068123393316195,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
667"
EXPERIMENTS COMPUTE RESOURCES,0.9074550128534704,"the experiments?
668"
EXPERIMENTS COMPUTE RESOURCES,0.9080976863753213,"Answer: [Yes]
669"
EXPERIMENTS COMPUTE RESOURCES,0.9087403598971723,"Justification: The computational resources used in our experiment is described in Section 4.1.
670"
EXPERIMENTS COMPUTE RESOURCES,0.9093830334190232,"Guidelines:
671"
EXPERIMENTS COMPUTE RESOURCES,0.910025706940874,"• The answer NA means that the paper does not include experiments.
672"
EXPERIMENTS COMPUTE RESOURCES,0.9106683804627249,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
673"
EXPERIMENTS COMPUTE RESOURCES,0.9113110539845758,"or cloud provider, including relevant memory and storage.
674"
EXPERIMENTS COMPUTE RESOURCES,0.9119537275064268,"• The paper should provide the amount of compute required for each of the individual
675"
EXPERIMENTS COMPUTE RESOURCES,0.9125964010282777,"experimental runs as well as estimate the total compute.
676"
EXPERIMENTS COMPUTE RESOURCES,0.9132390745501285,"• The paper should disclose whether the full research project required more compute
677"
EXPERIMENTS COMPUTE RESOURCES,0.9138817480719794,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
678"
EXPERIMENTS COMPUTE RESOURCES,0.9145244215938303,"didn’t make it into the paper).
679"
CODE OF ETHICS,0.9151670951156813,"9. Code Of Ethics
680"
CODE OF ETHICS,0.9158097686375322,"Question: Does the research conducted in the paper conform, in every respect, with the
681"
CODE OF ETHICS,0.916452442159383,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
682"
CODE OF ETHICS,0.9170951156812339,"Answer: [Yes]
683"
CODE OF ETHICS,0.9177377892030848,"Justification: We reviewed the NeurIPS Code of Ethics and conducted our research in
684"
CODE OF ETHICS,0.9183804627249358,"accordance with it.
685"
CODE OF ETHICS,0.9190231362467867,"Guidelines:
686"
CODE OF ETHICS,0.9196658097686375,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
687"
CODE OF ETHICS,0.9203084832904884,"• If the authors answer No, they should explain the special circumstances that require a
688"
CODE OF ETHICS,0.9209511568123393,"deviation from the Code of Ethics.
689"
CODE OF ETHICS,0.9215938303341902,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
690"
CODE OF ETHICS,0.9222365038560412,"eration due to laws or regulations in their jurisdiction).
691"
BROADER IMPACTS,0.922879177377892,"10. Broader Impacts
692"
BROADER IMPACTS,0.9235218508997429,"Question: Does the paper discuss both potential positive societal impacts and negative
693"
BROADER IMPACTS,0.9241645244215938,"societal impacts of the work performed?
694"
BROADER IMPACTS,0.9248071979434447,"Answer: [Yes]
695"
BROADER IMPACTS,0.9254498714652957,"Justification: We have addressed societal impact in Section 5.
696"
BROADER IMPACTS,0.9260925449871465,"Guidelines:
697"
BROADER IMPACTS,0.9267352185089974,"• The answer NA means that there is no societal impact of the work performed.
698"
BROADER IMPACTS,0.9273778920308483,"• If the authors answer NA or No, they should explain why their work has no societal
699"
BROADER IMPACTS,0.9280205655526992,"impact or why the paper does not address societal impact.
700"
BROADER IMPACTS,0.9286632390745502,"• Examples of negative societal impacts include potential malicious or unintended uses
701"
BROADER IMPACTS,0.929305912596401,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
702"
BROADER IMPACTS,0.9299485861182519,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
703"
BROADER IMPACTS,0.9305912596401028,"groups), privacy considerations, and security considerations.
704"
BROADER IMPACTS,0.9312339331619537,"• The conference expects that many papers will be foundational research and not tied
705"
BROADER IMPACTS,0.9318766066838047,"to particular applications, let alone deployments. However, if there is a direct path to
706"
BROADER IMPACTS,0.9325192802056556,"any negative applications, the authors should point it out. For example, it is legitimate
707"
BROADER IMPACTS,0.9331619537275064,"to point out that an improvement in the quality of generative models could be used to
708"
BROADER IMPACTS,0.9338046272493573,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
709"
BROADER IMPACTS,0.9344473007712082,"that a generic algorithm for optimizing neural networks could enable people to train
710"
BROADER IMPACTS,0.9350899742930592,"models that generate Deepfakes faster.
711"
BROADER IMPACTS,0.9357326478149101,"• The authors should consider possible harms that could arise when the technology is
712"
BROADER IMPACTS,0.9363753213367609,"being used as intended and functioning correctly, harms that could arise when the
713"
BROADER IMPACTS,0.9370179948586118,"technology is being used as intended but gives incorrect results, and harms following
714"
BROADER IMPACTS,0.9376606683804627,"from (intentional or unintentional) misuse of the technology.
715"
BROADER IMPACTS,0.9383033419023136,"• If there are negative societal impacts, the authors could also discuss possible mitigation
716"
BROADER IMPACTS,0.9389460154241646,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
717"
BROADER IMPACTS,0.9395886889460154,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
718"
BROADER IMPACTS,0.9402313624678663,"feedback over time, improving the efficiency and accessibility of ML).
719"
SAFEGUARDS,0.9408740359897172,"11. Safeguards
720"
SAFEGUARDS,0.9415167095115681,"Question: Does the paper describe safeguards that have been put in place for responsible
721"
SAFEGUARDS,0.9421593830334191,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
722"
SAFEGUARDS,0.9428020565552699,"image generators, or scraped datasets)?
723"
SAFEGUARDS,0.9434447300771208,"Answer: [NA]
724"
SAFEGUARDS,0.9440874035989717,"Justification: We provide the source code as supplementary material; however, since the
725"
SAFEGUARDS,0.9447300771208226,"experiments concern the fundamental nature of machine learning models, we believe there
726"
SAFEGUARDS,0.9453727506426736,"are no risks involved.
727"
SAFEGUARDS,0.9460154241645244,"Guidelines:
728"
SAFEGUARDS,0.9466580976863753,"• The answer NA means that the paper poses no such risks.
729"
SAFEGUARDS,0.9473007712082262,"• Released models that have a high risk for misuse or dual-use should be released with
730"
SAFEGUARDS,0.9479434447300771,"necessary safeguards to allow for controlled use of the model, for example by requiring
731"
SAFEGUARDS,0.9485861182519281,"that users adhere to usage guidelines or restrictions to access the model or implementing
732"
SAFEGUARDS,0.949228791773779,"safety filters.
733"
SAFEGUARDS,0.9498714652956298,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
734"
SAFEGUARDS,0.9505141388174807,"should describe how they avoided releasing unsafe images.
735"
SAFEGUARDS,0.9511568123393316,"• We recognize that providing effective safeguards is challenging, and many papers do
736"
SAFEGUARDS,0.9517994858611826,"not require this, but we encourage authors to take this into account and make a best
737"
SAFEGUARDS,0.9524421593830334,"faith effort.
738"
LICENSES FOR EXISTING ASSETS,0.9530848329048843,"12. Licenses for existing assets
739"
LICENSES FOR EXISTING ASSETS,0.9537275064267352,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
740"
LICENSES FOR EXISTING ASSETS,0.9543701799485861,"the paper, properly credited and are the license and terms of use explicitly mentioned and
741"
LICENSES FOR EXISTING ASSETS,0.9550128534704371,"properly respected?
742"
LICENSES FOR EXISTING ASSETS,0.955655526992288,"Answer: [Yes]
743"
LICENSES FOR EXISTING ASSETS,0.9562982005141388,"Justification: We have used open datasets, citing them in accordance with their license
744"
LICENSES FOR EXISTING ASSETS,0.9569408740359897,"information.
745"
LICENSES FOR EXISTING ASSETS,0.9575835475578406,"Guidelines:
746"
LICENSES FOR EXISTING ASSETS,0.9582262210796915,"• The answer NA means that the paper does not use existing assets.
747"
LICENSES FOR EXISTING ASSETS,0.9588688946015425,"• The authors should cite the original paper that produced the code package or dataset.
748"
LICENSES FOR EXISTING ASSETS,0.9595115681233933,"• The authors should state which version of the asset is used and, if possible, include a
749"
LICENSES FOR EXISTING ASSETS,0.9601542416452442,"URL.
750"
LICENSES FOR EXISTING ASSETS,0.9607969151670951,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
751"
LICENSES FOR EXISTING ASSETS,0.961439588688946,"• For scraped data from a particular source (e.g., website), the copyright and terms of
752"
LICENSES FOR EXISTING ASSETS,0.962082262210797,"service of that source should be provided.
753"
LICENSES FOR EXISTING ASSETS,0.9627249357326478,"• If assets are released, the license, copyright information, and terms of use in the
754"
LICENSES FOR EXISTING ASSETS,0.9633676092544987,"package should be provided. For popular datasets, paperswithcode.com/datasets
755"
LICENSES FOR EXISTING ASSETS,0.9640102827763496,"has curated licenses for some datasets. Their licensing guide can help determine the
756"
LICENSES FOR EXISTING ASSETS,0.9646529562982005,"license of a dataset.
757"
LICENSES FOR EXISTING ASSETS,0.9652956298200515,"• For existing datasets that are re-packaged, both the original license and the license of
758"
LICENSES FOR EXISTING ASSETS,0.9659383033419023,"the derived asset (if it has changed) should be provided.
759"
LICENSES FOR EXISTING ASSETS,0.9665809768637532,"• If this information is not available online, the authors are encouraged to reach out to
760"
LICENSES FOR EXISTING ASSETS,0.9672236503856041,"the asset’s creators.
761"
NEW ASSETS,0.967866323907455,"13. New Assets
762"
NEW ASSETS,0.968508997429306,"Question: Are new assets introduced in the paper well documented and is the documentation
763"
NEW ASSETS,0.9691516709511568,"provided alongside the assets?
764"
NEW ASSETS,0.9697943444730077,"Answer: [NA]
765"
NEW ASSETS,0.9704370179948586,"Justification: We do not provide any new assets.
766"
NEW ASSETS,0.9710796915167095,"Guidelines:
767"
NEW ASSETS,0.9717223650385605,"• The answer NA means that the paper does not release new assets.
768"
NEW ASSETS,0.9723650385604113,"• Researchers should communicate the details of the dataset/code/model as part of their
769"
NEW ASSETS,0.9730077120822622,"submissions via structured templates. This includes details about training, license,
770"
NEW ASSETS,0.9736503856041131,"limitations, etc.
771"
NEW ASSETS,0.974293059125964,"• The paper should discuss whether and how consent was obtained from people whose
772"
NEW ASSETS,0.9749357326478149,"asset is used.
773"
NEW ASSETS,0.9755784061696658,"• At submission time, remember to anonymize your assets (if applicable). You can either
774"
NEW ASSETS,0.9762210796915167,"create an anonymized URL or include an anonymized zip file.
775"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9768637532133676,"14. Crowdsourcing and Research with Human Subjects
776"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9775064267352185,"Question: For crowdsourcing experiments and research with human subjects, does the paper
777"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9781491002570694,"include the full text of instructions given to participants and screenshots, if applicable, as
778"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787917737789203,"well as details about compensation (if any)?
779"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9794344473007712,"Answer: [NA]
780"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9800771208226221,"Justification: This paper neither engages in crowdsourcing nor research involving human
781"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980719794344473,"subjects.
782"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9813624678663239,"Guidelines:
783"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9820051413881749,"• The answer NA means that the paper does not involve crowdsourcing nor research with
784"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9826478149100257,"human subjects.
785"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9832904884318766,"• Including this information in the supplemental material is fine, but if the main contribu-
786"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9839331619537275,"tion of the paper involves human subjects, then as much detail as possible should be
787"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9845758354755784,"included in the main paper.
788"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9852185089974294,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
789"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858611825192802,"or other labor should be paid at least the minimum wage in the country of the data
790"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9865038560411311,"collector.
791"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.987146529562982,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
792"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9877892030848329,"Subjects
793"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9884318766066839,"Question: Does the paper describe potential risks incurred by study participants, whether
794"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9890745501285347,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
795"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9897172236503856,"approvals (or an equivalent approval/review based on the requirements of your country or
796"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9903598971722365,"institution) were obtained?
797"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9910025706940874,"Answer: [NA]
798"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9916452442159382,"Justification: This paper neither engages in crowdsourcing nor research involving human
799"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9922879177377892,"subjects.
800"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929305912596401,"Guidelines:
801"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993573264781491,"• The answer NA means that the paper does not involve crowdsourcing nor research with
802"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9942159383033419,"human subjects.
803"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9948586118251928,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
804"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9955012853470437,"may be required for any human subjects research. If you obtained IRB approval, you
805"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9961439588688946,"should clearly state this in the paper.
806"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9967866323907455,"• We recognize that the procedures for this may vary significantly between institutions
807"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9974293059125964,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
808"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9980719794344473,"guidelines for their institution.
809"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9987146529562982,"• For initial submissions, do not include any information that would break anonymity (if
810"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9993573264781491,"applicable), such as the institution conducting the review.
811"
