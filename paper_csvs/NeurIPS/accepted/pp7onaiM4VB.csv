Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.002564102564102564,"Electroencephalography (EEG) provides access to neuronal dynamics non-
invasively with millisecond resolution, rendering it a viable method in neuroscience
and healthcare. However, its utility is limited as current EEG technology does
not generalize well across domains (i.e., sessions and subjects) without expensive
supervised re-calibration. Contemporary methods cast this transfer learning (TL)
problem as a multi-source/-target unsupervised domain adaptation (UDA) prob-
lem and address it with deep learning or shallow, Riemannian geometry aware
alignment methods. Both directions have, so far, failed to consistently close the
performance gap to state-of-the-art domain-speciﬁc methods based on tangent
space mapping (TSM) on the symmetric, positive deﬁnite (SPD) manifold. Here,
we propose a machine learning framework that enables, for the ﬁrst time, learn-
ing domain-invariant TSM models in an end-to-end fashion. To achieve this, we
propose a new building block for geometric deep learning, which we denote SPD
domain-speciﬁc momentum batch normalization (SPDDSMBN). A SPDDSMBN
layer can transform domain-speciﬁc SPD inputs into domain-invariant SPD outputs,
and can be readily applied to multi-source/-target and online UDA scenarios. In
extensive experiments with 6 diverse EEG brain-computer interface (BCI) datasets,
we obtain state-of-the-art performance in inter-session and -subject TL with a
simple, intrinsically interpretable network architecture, which we denote TSMNet.
Code: https://github.com/rkobler/TSMNet"
INTRODUCTION,0.005128205128205128,"1
Introduction"
INTRODUCTION,0.007692307692307693,"Electroencephalography (EEG) measures multi-channel electric brain activity from the human
scalp with millisecond precision [1]. Transient modulations in the rhythmic brain activity can
reveal cognitive processes [2], affective states [3] and a person’s health status [4]. Unfortunately,
these modulations exhibit low signal-to-noise ratio (SNR), domain shifts (i.e., changes in the data
distribution) and have low speciﬁcity, rendering statistical learning a challenging task - particularly in
the context of brain-computer interfaces (BCI) [5] where the goal is to predict a target from a short
segment of multi-channel EEG data in real-time.
Under domain shifts, domain adaptation (DA), deﬁned as learning a model from a source domain
that performs well on a related target domain, offers principled statistical learning approaches with
theoretical guarantees [6, 7]. DA in the BCI ﬁeld mainly distinguishes inter-session and -subject
transfer learning (TL) [8]. In inter-session TL, domain shifts, are expected across sessions mainly
due to mental drifts (low speciﬁcity) as well as differences in the relative positioning of the electrodes
and their impedances. Inter-subject TL is more difﬁcult, as domain shifts are additionally driven by
structural and functional differences in brain networks as well as variations in the performed task [9].
These domain shifts are traditionally circumvented by recording labeled calibration data and ﬁtting
domain-speciﬁc models [10, 11]. As recording calibration data is costly, models that are robust to"
INTRODUCTION,0.010256410256410256,"Figure 1: Visualization of the proposed framework around (a) SPD domain-speciﬁc momentum
batch normalization (SPDDSMBN) that (b) learns parameters ⇥= {✓, φ,  } of typical tangent space
mapping (TSM) models end-to-end to crack multi-source/-target unsupervised domain adaptation
on S+"
INTRODUCTION,0.01282051282051282,"D for EEG data (c, illustrative example). For EEG data, we propose a simple, intrinsically
interpretable parametrization of f and g, denoted TSMNet, and obtain SoA performance."
INTRODUCTION,0.015384615384615385,"scarce data with low SNR perform well in practice. Currently, tangent space mapping (TSM) models
[12, 13] operating with symmetric, positive deﬁnite (SPD) covariance descriptors of preprocessed
data are considered state-of-the-art (SoA) [10, 14, 15]. They are well suited for EEG data as they
exhibit invariances to linear mixing of latent sources [16], and are consistent [13] and intrinsically
interpretable [17] estimators for generative models that encode label information with a log-linear
relationship in source power modulations.
Competitive, supervised calibration-free methods are one of the long-lasting grand challenges in
EEG neurotechnology research [5, 10, 18, 19, 15]. Among the applied transfer learning techniques,
including multi-task learning [20] and domain-invariant learning [21–23], unsupervised domain
adaptation (UDA) [24] is considered as key to overcome this challenge [10, 19]. Contemporary
methods cast the problem as a multi source and target UDA problem and address it with deep learning
[25–28] or shallow, Riemannian geometry aware alignment methods [29–32]. Successful methods
must cope with notoriously small and heterogeneous datasets (i.e., dozens of domains with a few
dozens observations per domain and class). In a recent, relatively large scale inter-subject and -dataset
TL competition with few labeled examples per target domain [19], deep learning approaches that
aligned the ﬁrst and second order statistics either in input [33, 27] or latent space [34] obtained
the highest scores. Whereas, in a pure UDA competition [15] with a smaller dataset, Riemannian
geometry aware approaches dominated. With the increasing popularity of geometric deep learning
[35], Ju and Guan [36] proposed an architecture based on SPD neural networks [37] to align SPD
features in latent space and attained SoA scores. Despite the tremendous advances in recent years,
the ﬁeld still lacks methods that can consistently close the performance gap to state-of-the-art
domain-speciﬁc methods.
To close this gap, we propose a machine learning framework around domain-speciﬁc batch normaliza-
tion on the SPD manifold (Figure 1). The proposed framework is used to implement domain-speciﬁc
TSM (Figure 1a), which requires tracking the domains’ Fréchet means in latent space as they are
changing during training a typical TSM model in an end-to-end fashion (Figure 1b). After review-
ing some preliminaries in section 2, we extend momentum batch normalization (MBN) [32] to
SPDMBN that controls the Fréchet mean and variance of SPD data in section 3. In a theoretical
analysis, we show under reasonable assumptions that SPDMBN can track and converge to the data’s
true Fréchet mean, enabling, for the ﬁrst time, end-to-end learning of feature extractors, TSM and
tangent space classiﬁers. Building upon this insight, we combine SPDMBN with domain-speciﬁc
batch normalization (DSBN) [38] to form SPDDSMBN (Figure 1a). A SPDDSMBN layer can
transform domain-speciﬁc SPD inputs into domain-invariant SPD outputs (Figure 1c). Like DSBN,
SPDDSMBN easily extends to multi-source, multi-target and online UDA scenarios. In section 4, we
brieﬂy review the generative model of EEG, before the proposed methods are combined in a simple,
intrinsically interpretable network architecture, denoted TSMNet (Figure 1b). We obtain state-of-the-
art performance in inter-session and -subject UDA on small and large scale EEG BCI datasets, and
show in an ablation study that the performance increase is primarily driven by performing DSBN on
the SPD manifold."
PRELIMINARIES,0.017948717948717947,"2
Preliminaries"
PRELIMINARIES,0.020512820512820513,"Multi-source multi-target unsupervised domain adaptation
Let X denote the space of input
features, Y a label space, and Id ⇢N an index set that contains unique domain identiﬁers. In"
PRELIMINARIES,0.023076923076923078,"the multi-source, multi-target unsupervised domain adaptation scenario considered here, we are
given a set T source = {Ti|i 2 Isource"
PRELIMINARIES,0.02564102564102564,"d
⇢Id} with |Isource"
PRELIMINARIES,0.028205128205128206,"d
| = N domains. Each domain Ti =
{(Xij, yij)}M"
PRELIMINARIES,0.03076923076923077,j=1 ⇠P i
PRELIMINARIES,0.03333333333333333,"XY contains M observations of feature (X 2 X) and label (y 2 Y) tuples
sampled from a joint distribution P i"
PRELIMINARIES,0.035897435897435895,"XY .1 While the joint distributions can be different (but related)
across domains, we assume that the class priors are the same (i.e., P i"
PRELIMINARIES,0.038461538461538464,"Y = PY ). The goal is to learn
a predictive function h : X ⇥Id ! Y that, once ﬁtted to T source, can generalize to unseen target
domains T target = {Tl|l 2 Itarget"
PRELIMINARIES,0.041025641025641026,"d
⇢Id, Itarget"
PRELIMINARIES,0.04358974358974359,"d
\ Isource"
PRELIMINARIES,0.046153846153846156,"d
= ;} merely based on unsupervised
adaptation of h to each target domain Tl once its label l and features {Xlj}M"
PRELIMINARIES,0.04871794871794872,j=1 ⇠P l
PRELIMINARIES,0.05128205128205128,X are revealed.
PRELIMINARIES,0.05384615384615385,Riemannian geometry on S+
PRELIMINARIES,0.05641025641025641,"D
We start with recalling notions of geometry on the space of real
D ⇥D symmetric positive deﬁnite (SPD) matrices S+"
PRELIMINARIES,0.05897435897435897,"D = {Z 2 RD⇥D : ZT = Z, Z ≻0}. The
space S+"
PRELIMINARIES,0.06153846153846154,"D forms a cone shaped Riemannian manifold in RD⇥D [39]. A Riemannian manifold M is a
smooth manifold equipped with an inner product on the tangent space TZM at each point Z 2 M.
Tangent spaces have Euclidean structure with easy to compute distances TZM ⇥TZM ! R+ which
locally approximate Riemannian distances on M induced by an inner product [40]. Logarithmic
LogZ : M ! TZM and exponential ExpZ : TZM ! M mappings project points to and from
tangent spaces.
Using the inner product hS1, S2iZ = Tr(Z−1S1Z−1S2) for points S1, S2 in the tangent space TZS+"
PRELIMINARIES,0.0641025641025641,"D
(i.e., the space of real symmetric D ⇥D matrices) results in a globally deﬁned afﬁne invariant
Riemannian metric on S+"
PRELIMINARIES,0.06666666666666667,"D [41, 39], which can be computed in closed form:"
PRELIMINARIES,0.06923076923076923,"δAIRM(Z1, Z2) = ||log(Z −1"
PRELIMINARIES,0.07179487179487179,"2
1
Z2Z −1"
PRELIMINARIES,0.07435897435897436,"2
1
)||F
(1)"
PRELIMINARIES,0.07692307692307693,"where Z1 and Z2 are two SPD matrices, log(·) denotes the matrix logarithm2, || · ||F the Frobe-
nius norm, and Tr(·) in the inner product the trace operator. Due to afﬁne invariance, we have
δ(AZ1AT , AZ2AT ) = δ(Z1, Z2) for any invertible D ⇥D transformation matrix A. The expo-
nential and logarithmic mapping are also globally deﬁned in closed form as"
PRELIMINARIES,0.07948717948717948,LogZ(Z1) = Z
PRELIMINARIES,0.08205128205128205,"1
2 log(Z−1"
PRELIMINARIES,0.08461538461538462,2 Z1Z−1 2 )Z
PRELIMINARIES,0.08717948717948718,"1
2
(2)"
PRELIMINARIES,0.08974358974358974,ExpZ(S1) = Z
PRELIMINARIES,0.09230769230769231,"1
2 exp(Z−1"
PRELIMINARIES,0.09487179487179487,2 S1Z−1 2 )Z
PRELIMINARIES,0.09743589743589744,"1
2
(3)"
PRELIMINARIES,0.1,For a set of SPD points Z = {Zj 2 S+
PRELIMINARIES,0.10256410256410256,"D}jM, we will use the notion of Fréchet mean GZ 2 S+"
PRELIMINARIES,0.10512820512820513,"D
and Fréchet variance ⌫2"
PRELIMINARIES,0.1076923076923077,"Z 2 R+. The Fréchet mean is deﬁned as the minimizer of the average squared
distances"
PRELIMINARIES,0.11025641025641025,GZ = arg min G2S+ D
M,0.11282051282051282,"1
M M
X j=1"
M,0.11538461538461539,"δ2(G, Zj)
(4)"
M,0.11794871794871795,"For M = 2, there is a closed form solution expressed as"
M,0.12051282051282051,"GZ(γ) = Z1#γZ2 = Z 1
2
1 ⇣ Z −1"
M,0.12307692307692308,"2
1
Z2Z −1 2
1 ⌘γ Z"
M,0.12564102564102564,"1
2
1
(5)"
M,0.1282051282051282,"with weight γ = 0.5. Choosing γ 2 [0, 1] computes weighted means along the geodesic (i.e., the
shortest curve) that connects both points. For M > 2, (4) can be solved using the Karcher ﬂow
algorithm [42], which iterates between projecting the data to the tangent space (2) at the current
estimate, arithmetic averaging, and projecting the result back (3) to obtain a new estimate. The
Fréchet variance ⌫2"
M,0.13076923076923078,Z is deﬁned as the attained value at the minimizer GZ: ⌫2
M,0.13333333333333333,"Z = VarZ(GZ) = 1 M M
X j=1 δ2"
M,0.1358974358974359,"AIRM(GZ, Zj)
(6)"
M,0.13846153846153847,"To shift a set of tangent space points to vary around a parametrized mean Gφ, parallel transport on
S+"
M,0.14102564102564102,D can be used [43]:
M,0.14358974358974358,"ΓGZ!Gφ(S) = ET SE , E = (G−1 Z Gφ)"
M,0.14615384615384616,"1
2
(7)"
M,0.14871794871794872,"While, parallel transport is generally deﬁned for tangent space vectors S [40], on S+"
M,0.15128205128205127,"D the same
operations also apply directly to points on the manifold (i.e., Z 2 Z) [31, 44]."
M,0.15384615384615385,"1For ease of notation, although not required by our method, we assume that M is the same for each domain.
2For SPD matrices, powers, logarithms and exponentials can be computed via eigen decomposition."
M,0.1564102564102564,"Table 1: Overview and differences of relevant batch normalization algorithms. The last column,
denoted normalization, sumarizes which statistics are used to normalize the batch data during training."
M,0.15897435897435896,"Acronym
S+"
M,0.16153846153846155,"D
domain-speciﬁc
momentum γ
normalization"
M,0.1641025641025641,"MBN [32]
no
no
adaptive
running stats
SPDBN [46]
yes
no
ﬁxed
running stats
SPDMBN (algorithm 1) proposed
yes
no
adaptive
running stats
DSBN [38]
no
yes
ﬁxed
batch stats
SPDDSMBN (18) proposed
yes
yes
adaptive
running stats"
M,0.16666666666666666,"3
Domain-speciﬁc batch normalization on S+"
M,0.16923076923076924,"D
In this section, we review relevant batch normalization (BN) [45] variants with a focus on S+"
M,0.1717948717948718,"D. We
then present SPDMBN and show in a theoretical analysis that the running estimate converges to the
true Fréchet mean under reasonable assumptions. At last, we combine the idea of domain-speciﬁc
batch normalization (DSBN) [38] with SPDMBN to form a SPDDSMBN layer. Table 1 provides a
brief overview of related and proposed methods."
M,0.17435897435897435,"Batch normalization
Batch normalization (BN) [45] is a widely used training technqiue in deep
learning as BN layers speed up convergence and improve generalization via smoothing of the engery
landscape [47, 32]. A standard BN layer applies slightly different transformations during training
and testing to independent and identically distributed (iid) observations xj 2 Rd within the k-th
minibatch Bk of size M drawn from a dataset T . During training, the data are normalized using the
batch mean bk and variance s2"
M,0.17692307692307693,"k, and then scaled and shifted to have a parametrized mean gφ and
variance σ2"
M,0.1794871794871795,"φ. Internally, the layer updates running estimates of the dataset’s statistics (gk, σ2"
M,0.18205128205128204,"k) during
each training step k; the updates are computed via exponential smoothing with momentum parameter
γ. During testing, the running estimates are used.
Using batch statistics to normalize data during training rather than running estimates introduces noise
whose level depends on the batch size [32]; smaller batch sizes raise the noise level. The introduced
noise regularizes the training process, which can help to escape poor local minima in initial learning
but also lead to underﬁtting. Momentum BN (MBN) [32] allows small batch sizes while avoiding
underﬁtting. Like batch renormalization [48], MBN uses running estimates during training and
testing. The key difference is that MBN keeps two sets of running statistics; one for training and
one for testing. The latter are updated conventionally, while the former are updated with momentum
parameter γtrain(k) that decays over training steps k. MBN can, therefore, quickly escape poor local
minima during initial learning and avoid underﬁtting at later stages [32]."
M,0.18461538461538463,Batch normalization on S+
M,0.18717948717948718,"D
It is intractable to compute the Fréchet mean GBk for each minibatch
Bk = {Zj 2 S+ D}M"
M,0.18974358974358974,"j=1, as there is no efﬁcient algorithm to solve (4). Brooks et al. [44] proposed
Riemannian Batch Normalization (RBN) as a tractable approximation. RBN approximateley solves
(4) by aborting the iterative Karcher ﬂow algorithm after one iteration. To transform Zj 2 Bk
with estimated mean Bk to vary around Gφ, parallel transport (7) is used. The RBN input output
transformation is then expressed as"
M,0.19230769230769232,"RBN(Zj; Gφ, γ) = ΓBk!Gφ(Zj) = ET ZjE , E = (B−1 k Gφ)"
M,0.19487179487179487,"1
2 ,
8Zj 2 Bk
(8)"
M,0.19743589743589743,"Using (5), the running estimate of the dataset’s Fréchet mean can be updated in closed form"
M,0.2,"Gk = Gk−1#γBk
(9)"
M,0.20256410256410257,"In [46] we proposed an extension to RBN, denoted SPD batch renormalization (SPDBN) that controls
both Fréchet mean and variance. Like batch renormalization [48], SPDBN uses running estimates Gk
and ⌫2"
M,0.20512820512820512,k during training and testing. To transform Zj 2 Bk to vary around Gφ with variance ⌫2
M,0.2076923076923077,"φ, each
observation is ﬁrst transported to vary around the identity matrix I, rescaled via computing matrix
powers and ﬁnally transported to vary around Gφ. The sequence of operations can be expressed as"
M,0.21025641025641026,"SPDBN(Zj; Gφ, ⌫2"
M,0.2128205128205128,"φ, "", γ) = ΓI!Gφ ◦ΓGk!I(Zj)"
M,0.2153846153846154,"⌫φ
⌫k+"" ,
8Zj 2 Bk
(10)"
M,0.21794871794871795,"The standard backpropagation framework with extensions for structured matrices [49] and manifold-
constrained gradients [40] can be used to propagate gradients through RBN and SPDBN layers and
learn the parameters (Gφ, ⌫φ)."
M,0.2205128205128205,Algorithm 1: SPD momentum batch normalization (SPDMBN)
M,0.2230769230769231,"Input
:batch Bk = {Zj 2 S+ D}M"
M,0.22564102564102564,"j=1 at training step k,
running mean ¯Gk−1, ¯G0 = I and variance ¯⌫2"
M,0.2282051282051282,"k−1, ¯⌫2"
M,0.23076923076923078,"0 = 1 for training,
running mean ˜Gk−1, ˜G0 = I and variance ˜⌫2"
M,0.23333333333333334,"k−1, ˜⌫2"
M,0.2358974358974359,"0 = 1 for testing,
learnable parameters (Gφ, ⌫φ), and momentum for training and testing γtrain(k), γ 2 [0, 1]
Output :normalized batch {˜Zj = SPDMBN(Zj) 2 S+"
M,0.23846153846153847,D | Zj 2 Bk}
M,0.24102564102564103,if training then
M,0.24358974358974358,"Bk = karcher_flow (Bk, steps = 1) ;
// approx.
batch Fréchet mean
¯Gk = ¯Gk−1#γtrain(k)Bk;
// update running stats
¯⌫2"
M,0.24615384615384617,k = (1 −γtrain(k))¯⌫2
M,0.24871794871794872,"k−1 + γtrain(k)VarBk( ¯Gk)
˜Gk = ˜Gk−1#γBk
˜⌫2"
M,0.2512820512820513,k = (1 −γ)˜⌫2
M,0.25384615384615383,"k−1 + γVarBk( ˜Gk)
end
(Gk, ⌫2"
M,0.2564102564102564,"k) = ( ¯Gk, ¯⌫k) if training else ( ˜Gk, ˜⌫2"
M,0.258974358974359,"k)
˜Zj = ΓI!Gφ ◦ΓGk!I(Zj)"
M,0.26153846153846155,"⌫φ
⌫k+"" ;
// parallel transport to whiten, rescale and rebias"
M,0.2641025641025641,Momentum batch normalization on S+
M,0.26666666666666666,"D
SPDBN [46] suffers from the same limitations as batch
renormalization [48]. Consequently, we propose to extend MBN [32] to S+"
M,0.2692307692307692,"D. We list the pseudocode of
our proposed extension, which we denote SPDMBN, in algorithm 1. SPDMBN uses approximations
of batch-speciﬁc Fréchet means to update two sets of running estimates of the dataset’s Fréchet mean.
As MBN [32], we decay γtrain(k) with a clamped exponential decay schedule"
M,0.2717948717948718,γtrain(k) = 1 −γ
M,0.2743589743589744,"1
K−1 max(K−k,0)
min
+ γmin
(11)"
M,0.27692307692307694,"where K deﬁnes the training step at which γmin 2 [0, 1] should be attained."
M,0.2794871794871795,"The running mean in SPDMBN converges to the Fréchet mean
Here, we consider models that
apply a SPDMBN layer to latent representations generated by a feature extractor f✓: X ! S+"
M,0.28205128205128205,"D with
learnable parameters ✓.
We deﬁne a dataset that contains the latent representations generated with feature set ✓k as Z✓k =
{f✓k(x)|x 2 T }, and a minibatch of M iid samples at training step k as Bk. We denote the Fréchet
mean of Z✓k as G✓k, the estimated Fréchet mean, deﬁned in (9), as Gk, and the estimated batch
mean as Bk. Since the batches are drawn randomly, we consider the batch and running means as
random variables.
We assume that the variance Var✓k(Bk−1) = EBk−1{δ2(Bk−1, G✓k)} of the previous batch mean
Bk−1 with respect to the current Fréchet mean G✓k is bounded by the current variance Var✓k(Bk)
and the norm of the difference in the parameters"
M,0.2846153846153846,"Var✓k(Bk−1) (1 + ||✓k −✓k−1||)Var✓k(Bk)
(12)"
M,0.28717948717948716,"That is, across training steps k the parameter updates are required to change the ﬁrst and second order
moments of the distribution of Z✓k gradually so that the expected distance between G✓k and the
previous batch mean Bk−1 is bounded. We conjecture that this is the case for feature extractors f✓
that are smooth in the parameters and small learning rates, but leave the proof for future work.
Proposition 1 (Error bound for Gk). Consider the setting deﬁned above, and assumption (12) holds
true. Then, the variance of the running mean Var✓k(Gk) is bounded by"
M,0.28974358974358977,"Var✓k(Gk) Var✓k(Bk)
(13)"
M,0.2923076923076923,over training steps k if
M,0.2948717948717949,||✓k −✓k−1|| 1 −γ2
M,0.29743589743589743,"(1 −γ)2 −1
(14)"
M,0.3,holds true.
M,0.30256410256410254,"The proof is provided in appendix A.1 of the supplementary material and relies on the proof of the
geometric law of large numbers [50].
Proposition 1 states that if (12) and (14) are met, the expected distance between the true Fréchet mean
and the running mean is less or equal to the one of the batch mean. Consequently, the introduced"
M,0.30512820512820515,"noise level of SPDBN (equation 10) and SPDMBN (algorithm 1), which use Gk to normalize batches
during training, is smaller or equal to RBN (equation 8), which uses Bk.
Since γ controls the adaptation speed of Gk, proposition 1 also states that if γ converges to zero (=no
adaptation), the parameter updates are required to converge to zero as well (=no learning). Hence, for
a ﬁxed γ 2 (0, 1), as in the case of SPDBN (equation 10), proposition 1 is fulﬁlled, if the learning
rate for the parameters ✓is chosen sufﬁciently small. This can substantially slow down initial learning
for standard choices of γ (e.g., 0.1 or 0.01). As a remedy, SPDMBN (algorithm 1) uses an adaptive
momentum parameter, which allows larger parameter updates during initial training steps.
If we consider a late stage of learning, and in particular assume that after a certain number of iterations
the parameters stay in a small ball with radius ⇢around ✓⇤(i.e., ||✓k −✓⇤|| ⇢8 k > ) and the
feature extractor is L-smooth in the parameters (i.e., δ(f✓(x), f˜✓(x)) L||✓−˜✓|| 8x 2 T , 8✓, ˜✓)
then the distances are bounded δ(f✓k(x), f✓⇤(x)) ⇢L.
Remark 1 (Convergence of Gk for SPDMBN). If ⇢L is neglibile compared to the dataset’s variance,
then the Fréchet mean and variance can be considered ﬁxed, and the theorem of large numbers on
S+"
M,0.3076923076923077,"D [50] applies directly. That is, if the momentum parameter is decayed exponentially 8k > the
running mean Gk converges to the Fréchet mean G✓⇤in probability as k ! 1."
M,0.31025641025641026,"Taken together, Proposition 1 and Remark 1 provide guidelines to update Gk in SPDMBN so that the
introduced estimation error is bounded during initial fast learning (large γ) and decays towards zero
in late learning (small γ)."
M,0.3128205128205128,"SPDMBN to learn tangent space mapping at Fréchet means
Typical TSM models for classiﬁca-
tion [12] and regression [13] ﬁrst use (2) to project Z 2 T ⇢S+"
M,0.3153846153846154,"D to the tangent space at the Fréchet
mean GT , then use (7) to transport the result to vary around I, and ﬁnally extract elements in the up-
per triangular part3 to reduce feature redundancy. The invertible mapping PGT : S+"
M,0.31794871794871793,"D ! RD(D+1)/2
is expressed as:"
M,0.32051282051282054,PGT (Z) = upper ◦ΓGT !I ◦LogGT (Z) = upper(log(G −1
"T
ZG",0.3230769230769231,"2
T
ZG −1"
T,0.32564102564102565,"2
T
))
(15)"
T,0.3282051282051282,"We propose to use a SPDMBN layer followed by a LogEig layer [37] to compute a similar mapping
mφ (Figure 1a). A LogEig layer simply computes the matrix logarithm and vectorizes the result so
that the norm is preserved. If the parametrized mean of SPDMBN is ﬁxed to the identify matrix
(Gφ = I), the composition computes"
T,0.33076923076923076,mφ(Z) = LogEig ◦SPDMBN(Z) = upper ◦log ◦ΓGk!I(Z)
T,0.3333333333333333,"⌫φ
⌫k+"""
T,0.33589743589743587,= upper
T,0.3384615384615385,"✓
⌫φ
⌫k + ""log ⇣ G −1"
"K
ZG",0.34102564102564104,"2
k
ZG −1"
K,0.3435897435897436,"2
k ⌘◆ (16)"
K,0.34615384615384615,"where (Gk, ⌫2"
K,0.3487179487179487,"k) are the estimated Fréchet mean and variance of the dataset T at training step k, and
φ = {⌫φ} the learnable parameters. According to remark 1 Gk converges to GT and, in turn, mφ to
a scaled version of PGT , since upper is linear.
The mapping mφ offers several advantageous properties. First, the features are projected to a
Euclidean vector space where standard layers can be applied and distances are cheap to compute.
Second, distances between the projected features locally approximate δ and, therefore, inherit its
invariance properties (e.g., afﬁne mixing) [41]. This improves upon a LogEig layer [37] which
projects features to the tangent space at the identity matrix. As a result, distances between LogEig
projected features correspond to distances measured with the log-Euclidean Riemannian metric
(LERM) [51] which is not invariant to afﬁne mixing. Third, controlling the Fréchet variance in (16)
empirically speeds up learning and improves generalization [46]."
K,0.35128205128205126,Domain-speciﬁc batch normalization on S+
K,0.35384615384615387,"D
Considering a multi-source UDA scenario, Chang
et al. [38] proposed a domain-speciﬁc BN (DSBN) layer which simply keeps multiple parallel BN
layers and distributes observations according to the associated domains. Formally, we consider
minibatches Bk that form the union of NBk |Id| domain-speciﬁc minibatches Bi"
K,0.3564102564102564,"k drawn from
distinct domains i 2 IBk ✓Id. As before, each Bi"
K,0.358974358974359,"k contains j = 1, ..., M/NBk iid observations xj.
A DSBN layer mapping Rd ⇥Id ! Rd can then be expressed as"
K,0.36153846153846153,"DSBN(xj, i) = BNi(xj; gφi, sφi, "", γ) ,
8xj 2 Bi"
K,0.3641025641025641,"k ,
8i 2 IBk
(17)"
K,0.36666666666666664,"3To preserve the norm, the off diagonal elements are scaled by p 2."
K,0.36923076923076925,"In practice, the batch size M is typically ﬁxed. The particular choice is inﬂuenced by resource
availability and the desired noise level introduced by minibatch based stochastic gradient descent.
A drawback of DSBN is that for a ﬁxed batch size M and an increasing number of source domains
NBk, the effective batch size declines for the BN layers within DSBN. Since small batch sizes
increase the noise level introduced by BN, increasing the number of domains per batch can lead to
underﬁtting [32]. To alleviate this effect, we use the previously introduced SPDMBN layer. The
proposed domain-speciﬁc BN layer on S+"
K,0.3717948717948718,D is then formally deﬁned as
K,0.37435897435897436,"SPDDSMBN(Zj, i) = SPDMBNi(Zj; Gφi, ⌫φi, "", γ, γtrain(k)) , 8Zj 2 Bi k ⇢S+"
K,0.3769230769230769,"D , 8i 2 IBk"
K,0.37948717948717947,"(18)
The layer can be readily adapted to new domains, as new SPDMBN layers can be added on the ﬂy. If
the entire data of a domain becomes available, the domain-speciﬁc Fréchet mean and variance can be
estimated by solving (4), otherwise, the update rules in algorithm 1 can be used."
K,0.382051282051282,"4
SPDDSMBN to crack interpretable multi-source/-target UDA for EEG data"
K,0.38461538461538464,"With SPDDSMBN introduced in the previous section, we focus on a speciﬁc application domain,
namely, multi-source/-target UDA for EEG-based BCIs and propose an intrinsically interpretable
architecture which we denote TSMNet."
K,0.3871794871794872,"Generative model of EEG
EEG signals x(t) 2 RP capture voltage ﬂuctuations on P channels.
An EEG record (=domain) is uniquely identiﬁed by a subject and session identiﬁer. After standard
pre-processing steps, each domain i contains j = 1, ..., M labeled observations with features
Xij 2 X ⇢RP ⇥T where T is the number of temporal samples. Due to linearity of Maxwell’s
equations and Ohmic conductivity of tissue layers in the frequency ranges relevant for EEG [52], a
domain-speciﬁc linear instantaneous mixture of sources model is a valid generative model:"
K,0.38974358974358975,"Xij = AiSij + Nij
(19)"
K,0.3923076923076923,"where Sij 2 RQ⇥T represents the activity of Q latent sources, Ai 2 RP ⇥Q a domain-speciﬁc mixing
matrix and Nij 2 RP ⇥T additive noise. Both Ai and Sij are unknown which demands making
assumptions on Ai (e.g., anatomical prior knowledge [53]) and/or Sij (e.g., statistical independence
[54]) to extract interesting sources."
K,0.39487179487179486,"Interpretable multi-source/-target UDA for EEG data
As label information is available for the
source domains, our goal is to identify discriminative oscillatory sources shared across domains.
Our approach relies on TSM models with linear classiﬁers [12], as they are consistent [13] and
intrinsically interpretable [17] estimators for generative models with log-linear relationships between
the target yij and variance Var{s(k)"
K,0.3974358974358974,"ij (t)} of k = 1, ..., K Q discriminative sources: yij = K
X k=1 bklog ⇣"
K,0.4,Var{s(k)
K,0.4025641025641026,ij (t)} ⌘
K,0.40512820512820513,"+ ""ij
(20)"
K,0.4076923076923077,"where bk 2 R summarizes the coupling between the target yij and the variance of the encoding
source, and ""ij additive noise. In [17] we showed that the encoding sources’ coupling and their
patterns4 (columns of Ai) can be recovered via solving a generalized eigenvalue problem between
the Fréchet mean GTi and classiﬁer patterns [55] that were back projected to S+"
K,0.41025641025641024,D with P−1
K,0.4128205128205128,"GTi. The
resulting eigenvectors are the patterns and the eigenvalues λk reﬂect the relative source contribution
ck:"
K,0.4153846153846154,"ck = max(λk, λ−1"
K,0.41794871794871796,"k ) ,
λk = exp(bk/||b||2"
K,0.4205128205128205,"2)
(21)"
K,0.4230769230769231,"To beneﬁt from the intrinsic interpretability of TSM models, we constrain our hypothesis class
H to functions h : X ⇥Id ! Y that can be decomposed into a composition of a shared linear
feature extractor with covariance pooling f✓: X ! S+"
K,0.4256410256410256,"D, domain-speciﬁc tangent space mapping
mφ : S+"
K,0.4282051282051282,"D ⇥Id ! RD(D+1)/2, and a shared linear classiﬁer g : RD(D+1)/2 ! Y with parameters
⇥= {✓, φ,  }."
K,0.4307692307692308,"4Here, we use the entire dataset’s Fréchet mean instead of the domain-speciﬁc ones to compute patterns for
the average domain."
K,0.43333333333333335,"TSMNet with SPDDSMBN
Unlike previous approaches which learn f✓, mφ, g sequentially
[29, 31, 13, 30], we parametrize h = g ◦mφ ◦f✓as a neural network and learn the entire model
in an end-to-end fashion (Figure 1b). Details of the proposed architecture, denoted TSMNet, are
provided in appendix A.2.5. In a nutshell, we parametrize f✓as the composition of the ﬁrst two
linear convolutional layers of ShConvNet [56], covariance pooling [57], BiMap [37], and ReEig [37]
layers. A BiMap layer applies a linear subspace projection, and a ReEig layer thresholds eigenvalues
of symmetric matrices so that the output is SPD. We used the default threshold (10−4) and found
that it was never active in the trained models. Hence, after training, f✓fulﬁlled the hypothesis class
constraints. In order for mφ to align the domain data and compute TSM, we use SPDDSMBN (18)
with shared parameters (i.e., Gφi = Gφ = I, ⌫φi = ⌫φ) in (16). Finally, the classiﬁer g was
parametrized as a linear layer with softmax activations. We use the standard-cross entropy loss as
training objective, and optimized the parameters with the Riemannian ADAM optimizer [58]."
EXPERIMENTS WITH EEG DATA,0.4358974358974359,"5
Experiments with EEG data"
EXPERIMENTS WITH EEG DATA,0.43846153846153846,"In the following, we apply our method to classify target labels from short segments of EEG data. We
consider two BCI applications, namely, mental imagery [59, 5] and mental workload estimation [60].
Both applications have high potential to aid society in rehabilitation and healthcare [61, 62, 18] but
have, currently, limited practical value because of poor generalization across sessions and subjects
[19, 15]."
EXPERIMENTS WITH EEG DATA,0.441025641025641,"Datasets and preprocessing
The considered mental imagery datasets were BNCI2014001 [63] (9
subjects/2 sessions/4 classes), BNCI2015001 [64] (12/2-3/2), Lee2019 [65] (54/2/2), Lehner2020
[66] (1/7/2), Stieger2021 [67] (62/4-8/4) and Hehnberger2021 [68] (1/26/4). For mental workload
estimation, we used a recent competition dataset [69] (12/2/3). A detailed description of the datasets
is provided in appendix A.2.1. Altogether, we analyzed a total of 603 sessions of 158 human subjects
whose data was acquired in previous studies that obtained the subjects’ informed consent and the
right to share anonymized data.
The python packages moabb [14] and mne [70] were used to preprocess the datasets. The applied
steps comprise resampling the EEG signals to 250/256 Hz, applying temporal ﬁlters to extract
oscillatory EEG activity in the 4 to 36 Hz range (spectrally resolved if required by a method) and
ﬁnally extract short segments (3s) associated to a class label (details provided in appendix A.2.2)."
EXPERIMENTS WITH EEG DATA,0.44358974358974357,"Evaluation
We evaluated TSMNet against several baseline methods implementing direct transfer or
multi-source (-target) UDA strategies. They can be broadly categorized as component based [71, 68],
Riemannian geometry aware [12, 17, 30, 72] or deep learning [56, 73, 25]. All models were ﬁt and
evaluated with a randomized leave 5% of the sessions (inter-session TL) or subjects (inter-subject
TL) out cross-validation (CV) scheme. For inter-session TL, the models were only provided with data
of the associated subject. When required, inner train/test splits (neural nets) or CV (shallow methods)
were used to optimize hyper parameters (e.g., early stopping, regularization parameters). The dataset
Hehenberger2021 was used to ﬁt the hyper parameters of TSMNet, and is, therefore, omitted in the
presented results. Balanced accuracy (i.e., the average recall across classes) was used as scoring
metric. As the discriminability of the data varies considerably across subjects, we decided to report
the results in the ﬁgures relative to the score of a SoA domain-speciﬁc Riemannian geometry aware
method [17], which was ﬁtted and evaluated in a 80%/20% chronological train/test split (for details
see appendix A.2.4)."
EXPERIMENTS WITH EEG DATA,0.4461538461538462,"Soft- and hardware
We either used publicly available python code or implemented the methods in
python using the packages torch [74], scikit-learn [75], skorch [76], geoopt [77], mne [70], pyriemann
[78], pymanopt [79]. We ran the experiments on standard computation PCs equipped with 32 core
CPUs with 128 GB of RAM and used up to 1 GPU (24 GRAM). Depending on the dataset size,
ﬁtting and evaluating TSMNet varied from a few seconds to minutes."
MENTAL IMAGERY,0.44871794871794873,"5.1
Mental imagery
TSMNet closes the gap to domain-speciﬁc methods
Figure 2 summarizes the mental imagery
results. It displays test set scores of the considered TL methods relative to the score of a SoA
domain-speciﬁc reference method. Combining the results of all subjects across datasets (Figure 2a),
it becomes apparent that TSMNet is the only method that can signiﬁcantly reduce the gap to the
reference method (inter-subject) or even exceed its performance (inter-session). Figure 2b displays the
results resolved across datasets (for details see appendix A.3.1). We make two important observations.
First, concerning inter-session TL, TSMNet meets or exceeds the score of the reference method"
MENTAL IMAGERY,0.4512820512820513,"Figure 2: Mental imagery results (5 datasets). BCI test set score (balanced accuracy) for inter-session/-
subject transfer learning methods relative to a SoA domain-speciﬁc reference model (80%/20%
chronological train/test split; for details see appendix A.2.4). a, Barplots summarize the grand
average (573 sessions, 138 subjects) results. Errorbars indicate bootstrapped (1e3 repetitions) 95%
conﬁdence intervals (over subjects). b, Box and scatter plots summarize the dataset-speciﬁc results
for selected methods from each category. Datasets are ordered according to the training set size. Each
dot summarizes the score for one subject. Lehner2021 is not displayed as it contains only 1 subject."
MENTAL IMAGERY,0.45384615384615384,"Figure 3: Model interpretation results. Patterns extracted from a TSMNet. a, Motor imagery dataset
(BNCI2015001, inter-subject TL). The top, left panel lists the contribution, deﬁned in (21), for each
extracted source k = 1, ..., 20 (x-axis) to the target class. Panels in the left column summarize the
spectral patterns of extracted sources. For visualization purposes, only the 2 most discriminative
sources are displayed. Panels in the top row summarize the frequency proﬁle of each spectral channel
(output of 4 temporal convolution layers in f✓). Topographic plots summarize how the source activity
is projected to regions covered by the EEG channels (rows correspond to the source index; columns
to spectral channels). EEG channels at darker blue or red areas capture more source activity and are,
therefore, more discriminative. b, As in a for the mental workload estimation dataset and class low."
MENTAL IMAGERY,0.4564102564102564,"consistently across datasets. Second, concerning inter-subject TL, we found that all considered
methods tend to reduce the performance gap as the dataset size (# subjects) increases, and that
TSMNet is consistently the top or among the top methods. As a ﬁtted TSMNet corresponds to a
typical TSM model with a linear classiﬁer, we can transform the ﬁtted parameters into interpretable
patterns [17]. Figure 3a displays extracted patterns for the BNCI2015001 dataset (inter-subject TL).
It is clearly visible that TSMNet infers the target label from neurophysiologically plausible sources
(rows in Figure 3a). As expected [2], the source with highest contribution has spectral peaks in the
alpha and beta bands, and originates in contralateral and central sensorimotor cortex."
MENTAL IMAGERY,0.45897435897435895,DSBN on S+
MENTAL IMAGERY,0.46153846153846156,"D drives the success of TSMNet
Since TSMNet combines several advances, we
present the results of an ablation study in Table 2. It summarizes the grand average inter-session TL
test scores relative to TSMNet with SPDDSMBN for n = 138 subjects. We observed three signiﬁcant
effects. The largest effect can be attributed to S+"
MENTAL IMAGERY,0.4641025641025641,"D, as we observed the largest performance decline if
the architecture would be modiﬁed5 to omit the SPD manifold (4.5% with DSBN, 3% w/o DSBN)."
MENTAL IMAGERY,0.4666666666666667,"5We replaced the covariance pooling, BiMap, ReEig, SPD(DS)MBN, LogEig layers with variance pooling,
elementwise log activations followed by (DS)MBN. Note that the resulting architecture is similar to ShConvNet."
MENTAL IMAGERY,0.46923076923076923,"Table 2: Ablation study. Grand average (5 mental imagery datasets, 138 subjects, inter-session TL)
score for the test data relative to the proposed method, and training ﬁt time (50 epochs). Standard-
deviation is used to report the variability across subjects. Permutation t-tests (1e4 perms, df=137, 4
tests with t-max adjustment) were used to identify signiﬁcant effects."
MENTAL IMAGERY,0.4717948717948718,"∆balanced accuracy (%)
ﬁt time (s)
mean (std)
t-val (p-val)
mean (std)
S+"
MENTAL IMAGERY,0.47435897435897434,"D
DSBN [1]
BN method"
MENTAL IMAGERY,0.47692307692307695,"yes
yes
SPDMBN (proposed)
-
-
16.9 ( 1.0)
yes
SPDBN [4]
-1.6 ( 2.2)
-7.8 (0.0001)
20.3 ( 1.6)
no
SPDMBN (proposed)
-3.9 ( 4.4)
-10.7 (0.0001)
11.3 ( 0.5)
no
yes
MBN [2]
-4.5 ( 3.8)
-10.1 (0.0001)
6.6 ( 0.2)
no
MBN [2]
-6.9 ( 4.8)
-13.4 (0.0001)
4.4 ( 0.1)"
MENTAL IMAGERY,0.4794871794871795,"The performance gain comes at the cost of a 2.6x longer time to ﬁt the parameters. The second largest
effect can be attributed to DSBN; without DSBN the performance dropped by 3.9% (with S+"
MENTAL IMAGERY,0.48205128205128206,"D) and
2.4% (w/o S+"
MENTAL IMAGERY,0.4846153846153846,"D). The smallest, yet signiﬁcant effect can be attributed to SPDMBN."
MENTAL WORKLOAD ESTIMATION,0.48717948717948717,"5.2
Mental workload estimation
Compared to the baseline methods, TSMNet obtained the highest average scores of 54.7% (7.3%)
and 52.4% (8.8%) in inter-session and -subject TL (for details see appendix A.3.1). Interestingly, the
inter-session TL score of TSMNet matches the score (54.3%) of the winning method in last year’s
competition [15]. To shed light on the sources utilized by TSMNet, we show patterns for a ﬁtted
model in Figure 3b. For the low mental workload class, the top contributing source’s activity peaked in
the theta band and originated in pre-frontal areas. The second source’s activity originated in occipital
cortex with non-focal spectral proﬁle. Our results agree with the ﬁndings of previous research, as
both areas and the theta band have been implicated in mind wandering and effort withdrawal [60]."
DISCUSSION,0.4897435897435897,"6
Discussion"
DISCUSSION,0.49230769230769234,"In this contribution, we proposed a machine learning framework around (domain-speciﬁc) momentum
batch normalization on S+"
DISCUSSION,0.4948717948717949,"D to learn tangent space mapping (TSM) and feature extractors in an
end-to-end fashion. In a theoretical analysis, we provided error bounds for the running estimate of
the Fréchet mean as well as convergence guarantees under reasonable assumptions. We then applied
the framework, to a multi-source multi-target unsupervised domain adaptation problem, namely,
inter-session and -subject transfer learning for EEG data and obtained or attained state-of-the art
performance with a simple, intrinsically interpretable model, denoted TSMNet, in a total of 6 diverse
BCI datasets (138 human subjects, 573 sessions). In the case of mental imagery, we found that
TSMNet signiﬁcantly reduced (inter-subject TL) or even exceeded (inter-session TL) the performance
gap to a SoA domain-speciﬁc method.
Although our framework could be readily extended to online UDA for unseen target domains, we
limited this study to ofﬂine evaluations and leave actual BCI studies to future work. A limitation of
our framework, and also any other method that involves eigen decomposition, is the computational
complexity, which limits its application to high-dimensional SPD features (e.g., fMRI connectivity
matrices with ﬁne spatial granularity). Altogether, the presented results demonstrate the utility of
our framework and in particular TSMNet as it not only achieves highly competitive results but is
also intrinsically interpretable. While we do not foresee any immediate negative societal impacts,
we provide direct contributions towards the scalability and acceptability of EEG-based healthcare
[1, 5] and consumer [18, 60] technologies. We expect future works to evaluate the impact of the
proposed methods in clinical applications of EEG like sleep staging [80, 81], seizure [82] or pathology
detection [83, 84]."
DISCUSSION,0.49743589743589745,Acknowledgments and Disclosure of Funding
DISCUSSION,0.5,"This work was supported by the JSPS KAKENHI (Grants-in-Aid for Scientiﬁc Research) under Grant
Numbers 20H04249, 20H04208, 21H03516 and 21K12055, and the Agency for Medical Research
and Development (AMED) under Grant Number JP22dm0307009."
REFERENCES,0.5025641025641026,References
REFERENCES,0.5051282051282051,[1] Donald L. Schomer and Fernando H. Lopes da Silva. Niedermeyer’s Electroencephalography:
REFERENCES,0.5076923076923077,"basic principles, clinical applications, and related ﬁelds. Lippincott Williams & Wilkins, 7
edition, 2018. ISBN 978-0-19-022848-4. doi: 10.1212/WNL.0b013e31822f0490."
REFERENCES,0.5102564102564102,[2] G Pfurtscheller and F H Lopes. Event-related EEG / MEG synchronization and. Clinical
REFERENCES,0.5128205128205128,"Neurophysiology, 110:10576479, 1999."
REFERENCES,0.5153846153846153,"[3] Josef Faller, Jennifer Cummings, Sameer Saproo, and Paul Sajda. Regulation of arousal"
REFERENCES,0.517948717948718,"via online neurofeedback improves human performance in a demanding sensory-motor task.
Proceedings of the National Academy of Sciences, 116(13):6482–6490, 2019. doi: 10.1073/
pnas.1817207116."
REFERENCES,0.5205128205128206,"[4] Yu Zhang, Wei Wu, Russell T. Toll, Sharon Naparstek, Adi Maron-Katz, Mallissa Watts, Joseph"
REFERENCES,0.5230769230769231,"Gordon, Jisoo Jeong, Laura Astolﬁ, Emmanuel Shpigel, Parker Longwell, Kamron Sarhadi,
Dawlat El-Said, Yuanqing Li, Crystal Cooper, Cherise Chin-Fatt, Martijn Arns, Madeleine S.
Goodkind, Madhukar H. Trivedi, Charles R. Marmar, and Amit Etkin. Identiﬁcation of psychi-
atric disorder subtypes from functional connectivity patterns in resting-state electroencephalogra-
phy. Nature Biomedical Engineering, 5(4):309–323, 2021. doi: 10.1038/s41551-020-00614-8."
REFERENCES,0.5256410256410257,"[5] Jonathan R Wolpaw, Niels Birbaumer, Dennis J McFarland, Gert Pfurtscheller, and Theresa M"
REFERENCES,0.5282051282051282,"Vaughan. Brain-computer interfaces for communication and control. Clinical neurophysiology
: ofﬁcial journal of the International Federation of Clinical Neurophysiology, 113:767–791,
2002. doi: doi:10.1016/s1388-2457(02)00057-3."
REFERENCES,0.5307692307692308,"[6] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jen-"
REFERENCES,0.5333333333333333,"nifer Wortman Vaughan. A theory of learning from different domains. Machine Learning, 79
(1-2):151–175, 2010. doi: 10.1007/s10994-009-5152-4."
REFERENCES,0.5358974358974359,"[7] Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-"
REFERENCES,0.5384615384615384,"source adaptation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018."
REFERENCES,0.541025641025641,"[8] Dongrui Wu, Yifan Xu, and Bao-Liang Lu. Transfer Learning for EEG-Based Brain-Computer"
REFERENCES,0.5435897435897435,"Interfaces: A Review of Progress Made Since 2016. IEEE Transactions on Cognitive and
Developmental Systems, pages 1–1, 2020. doi: 10.1109/TCDS.2020.3007453."
REFERENCES,0.5461538461538461,"[9] Christa Neuper, Reinhold Scherer, Miriam Reiner, and Gert Pfurtscheller. Imagery of motor"
REFERENCES,0.5487179487179488,"actions: Differential effects of kinesthetic and visual–motor mode of imagery in single-trial
EEG. Cognitive Brain Research, 25(3):668–677, 2005. doi: 10.1016/j.cogbrainres.2005.08.014."
REFERENCES,0.5512820512820513,"[10] F Lotte, L Bougrain, A Cichocki, M Clerc, M Congedo, A Rakotomamonjy, and F Yger. A"
REFERENCES,0.5538461538461539,"review of classiﬁcation algorithms for EEG-based brain–computer interfaces: a 10 year update.
Journal of Neural Engineering, 15(3):031005, 2018. doi: 10.1088/1741-2552/aab2f2."
REFERENCES,0.5564102564102564,"[11] Karina Statthaler, Andreas Schwarz, David Steyrl, Reinmar J. Kobler, Maria Katharina Höller,"
REFERENCES,0.558974358974359,"Julia Brandstetter, Lea Hehenberger, Marvin Bigga, and Gernot Müller-Putz. Cybathlon experi-
ences of the Graz BCI racing team Mirage91 in the brain-computer interface discipline. Journal
of NeuroEngineering and Rehabilitation, 14(1):129, 2017. doi: 10.1186/s12984-017-0344-9."
REFERENCES,0.5615384615384615,"[12] Alexandre Barachant, Stéphane Bonnet, Marco Congedo, and Christian Jutten. Multiclass brain-"
REFERENCES,0.5641025641025641,"computer interface classiﬁcation by Riemannian geometry. IEEE transactions on bio-medical
engineering, 59(4):920–928, 2012. doi: 10.1109/TBME.2011.2172210."
REFERENCES,0.5666666666666667,"[13] David Sabbagh, Pierre Ablin, Gaël Varoquaux, Alexandre Gramfort, and Denis A Engemann."
REFERENCES,0.5692307692307692,"Manifold-regression to predict from MEG/EEG brain signals without source modeling. In
Advances in Neural Information Processing Systems, pages 7323–7334, 2019."
REFERENCES,0.5717948717948718,[14] Vinay Jayaram and Alexandre Barachant. MOABB: trustworthy algorithm benchmarking for
REFERENCES,0.5743589743589743,"BCIs. Journal of Neural Engineering, 15(6):066011, 2018. doi: 10.1088/1741-2552/aadea0."
REFERENCES,0.5769230769230769,"[15] Raphaëlle N. Roy, Marcel F. Hinss, Ludovic Darmet, Simon Ladouce, Emilie S. Jahanpour,"
REFERENCES,0.5794871794871795,"Bertille Somon, Xiaoqi Xu, Nicolas Drougard, Frédéric Dehais, and Fabien Lotte. Retrospec-
tive on the First Passive Brain-Computer Interface Competition on Cross-Session Workload
Estimation. Frontiers in Neuroergonomics, 3:838342, 2022. doi: 10.3389/fnrgo.2022.838342."
REFERENCES,0.5820512820512821,"[16] Marco Congedo, Alexandre Barachant, and Rajendra Bhatia. Riemannian geometry for EEG-"
REFERENCES,0.5846153846153846,"based brain-computer interfaces; a primer and a review. Brain-Computer Interfaces, 4(3):
155–174, 2017. doi: 10.1080/2326263X.2017.1297192."
REFERENCES,0.5871794871794872,"[17] Reinmar J. Kobler, Jun-Ichiro Hirayama, Lea Hehenberger, Catarina Lopes-Dias, Gernot Müller-"
REFERENCES,0.5897435897435898,"Putz, and Motoaki Kawanabe. On the interpretation of linear Riemannian tangent space model
parameters in M/EEG. In Proceedings of the 43rd Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC). IEEE, 2021. doi: 10.1109/EMBC46164.
2021.9630144."
REFERENCES,0.5923076923076923,[18] Stephen H. Fairclough and Fabien Lotte. Grand Challenges in Neurotechnology and System
REFERENCES,0.5948717948717949,"Neuroergonomics. Frontiers in Neuroergonomics, 1:602504, 2020. doi: 10.3389/fnrgo.2020.
602504."
REFERENCES,0.5974358974358974,"[19] Xiaoxi Wei, A. Aldo Faisal, Moritz Grosse-Wentrup, Alexandre Gramfort, Sylvain Chevallier,"
REFERENCES,0.6,"Vinay Jayaram, Camille Jeunet, Stylianos Bakas, Siegfried Ludwig, Konstantinos Barmpas,
Mehdi Bahri, Yannis Panagakis, Nikolaos Laskaris, Dimitrios A. Adamos, Stefanos Zafeiriou,
William C. Duong, Stephen M. Gordon, Vernon J. Lawhern, Maciej ´Sliwowski, Vincent
Rouanne, and Piotr Tempczyk. 2021 BEETL Competition: Advancing Transfer Learning
for Subject Independence & Heterogenous EEG Data Sets. arXiv:2202.12950 [cs, eess], 2022."
REFERENCES,0.6025641025641025,"[20] Vinay Jayaram, Morteza Alamgir, Yasemin Altun, Bernhard Scholkopf, and Moritz Grosse-"
REFERENCES,0.6051282051282051,"Wentrup. Transfer Learning in Brain-Computer Interfaces. IEEE Computational Intelligence
Magazine, 11(1):20–31, 2016. doi: 10.1109/MCI.2015.2501545."
REFERENCES,0.6076923076923076,"[21] Siamac Fazli, Florin Popescu, Márton Danóczy, Benjamin Blankertz, Klaus-Robert Müller,"
REFERENCES,0.6102564102564103,"and Cristian Grozea. Subject-independent mental state classiﬁcation in single trials. Neural
Networks, 22(9):1305–1312, 2009. doi: 10.1016/j.neunet.2009.06.003."
REFERENCES,0.6128205128205129,"[22] Wojciech Samek, Motoaki Kawanabe, and Klaus Robert Müller. Divergence-based framework"
REFERENCES,0.6153846153846154,"for common spatial patterns algorithms. IEEE Reviews in Biomedical Engineering, 7:50–72,
2014. doi: 10.1109/RBME.2013.2290621."
REFERENCES,0.617948717948718,"[23] O-Yeon Kwon, Min-Ho Lee, Cuntai Guan, and Seong-Whan Lee.
Subject-Independent
Brain–Computer Interfaces Based on Deep Convolutional Neural Networks. IEEE Trans-
actions on Neural Networks and Learning Systems, 31(10):3839–3852, 2020. doi: 10.1109/
TNNLS.2019.2946869."
REFERENCES,0.6205128205128205,"[24] Sicheng Zhao, Bo Li, Colorado Reed, Pengfei Xu, and Kurt Keutzer. Multi-source Domain"
REFERENCES,0.6230769230769231,"Adaptation in the Deep Learning Era: A Systematic Survey, 2020."
REFERENCES,0.6256410256410256,"[25] Ozan Ozdenizci, Ye Wang, Toshiaki Koike-Akino, and Deniz Erdogmus. Learning Invariant"
REFERENCES,0.6282051282051282,"Representations From EEG via Adversarial Inference. IEEE Access, 8:27074–27085, 2020. doi:
10.1109/ACCESS.2020.2971600."
REFERENCES,0.6307692307692307,"[26] Lichao Xu, Zhen Ma, Jiayuan Meng, Minpeng Xu, Tzyy-Ping Jung, and Dong Ming. Improving"
REFERENCES,0.6333333333333333,"Transfer Performance of Deep Learning with Adaptive Batch Normalization for Brain-computer
Interfaces *. In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine
& Biology Society (EMBC), pages 5800–5803, Mexico, 2021. IEEE. ISBN 978-1-72811-179-7.
doi: 10.1109/EMBC46164.2021.9629529."
REFERENCES,0.6358974358974359,"[27] Lichao Xu, Minpeng Xu, Yufeng Ke, Xingwei An, Shuang Liu, and Dong Ming. Cross-Dataset"
REFERENCES,0.6384615384615384,"Variability Problem in EEG Decoding With Deep Learning. Frontiers in Human Neuroscience,
14:103, 2020. doi: 10.3389/fnhum.2020.00103."
REFERENCES,0.6410256410256411,"[28] Ravikiran Mane, Efﬁe Chew, Karen Chua, Kai Keng Ang, Neethu Robinson, A. P. Vinod,"
REFERENCES,0.6435897435897436,"Seong-Whan Lee, and Cuntai Guan. FBCNet: A Multi-view Convolutional Neural Network for
Brain-Computer Interface. arXiv:2104.01233 [cs, eess], 2021."
REFERENCES,0.6461538461538462,"[29] Paolo Zanini, Marco Congedo, Christian Jutten, Salem Said, and Yannick Berthoumieu. Trans-"
REFERENCES,0.6487179487179487,"fer Learning: A Riemannian Geometry Framework With Applications to Brain–Computer
Interfaces. IEEE Transactions on Biomedical Engineering, 65(5):1107–1116, 2018. doi:
10.1109/TBME.2017.2742541."
REFERENCES,0.6512820512820513,"[30] Pedro Luiz Coelho Rodrigues, Christian Jutten, and Marco Congedo. Riemannian Procrustes"
REFERENCES,0.6538461538461539,"Analysis: Transfer Learning for Brain–Computer Interfaces. IEEE Transactions on Biomedical
Engineering, 66(8):2390–2401, 2019. doi: 10.1109/TBME.2018.2889705."
REFERENCES,0.6564102564102564,"[31] Or Yair, Mirela Ben-Chen, and Ronen Talmon. Parallel Transport on the Cone Manifold of SPD"
REFERENCES,0.658974358974359,"Matrices for Domain Adaptation. IEEE Transactions on Signal Processing, 67(7):1797–1811,
2019. doi: 10.1109/TSP.2019.2894801."
REFERENCES,0.6615384615384615,"[32] Hongwei Yong, Jianqiang Huang, Deyu Meng, Xiansheng Hua, and Lei Zhang. Momentum"
REFERENCES,0.6641025641025641,"Batch Normalization for Deep Learning with Small Batch Size. In Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision – ECCV 2020,
volume 12357, pages 224–240. Springer International Publishing, Cham, 2020. ISBN 978-3-
030-58609-6 978-3-030-58610-2. doi: 10.1007/978-3-030-58610-2_14."
REFERENCES,0.6666666666666666,[33] He He and Dongrui Wu. Transfer Learning for Brain–Computer Interfaces: A Euclidean Space
REFERENCES,0.6692307692307692,"Data Alignment Approach. IEEE Transactions on Biomedical Engineering, 67(2):399–410,
2020. doi: 10.1109/TBME.2019.2913914."
REFERENCES,0.6717948717948717,"[34] Stylianos Bakas, Siegfried Ludwig, Konstantinos Barmpas, Mehdi Bahri, Yannis Panagakis,"
REFERENCES,0.6743589743589744,"Nikolaos Laskaris, Dimitrios A. Adamos, and Stefanos Zafeiriou. Team Cogitat at NeurIPS
2021: Benchmarks for EEG Transfer Learning Competition, 2022."
REFERENCES,0.676923076923077,"[35] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst."
REFERENCES,0.6794871794871795,"Geometric Deep Learning: Going beyond Euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017. doi: 10.1109/MSP.2017.2693418."
REFERENCES,0.6820512820512821,[36] Ce Ju and Cuntai Guan. Deep Optimal Transport on SPD Manifolds for Domain Adaptation.
REFERENCES,0.6846153846153846,"arXiv:2201.05745 [cs, eess], 2022."
REFERENCES,0.6871794871794872,[37] Zhiwu Huang and Luc Van Gool. A Riemannian Network for SPD Matrix Learning. In
REFERENCES,0.6897435897435897,"Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, AAAI’17, pages
2036–2042. AAAI Press, 2017."
REFERENCES,0.6923076923076923,"[38] Woong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-"
REFERENCES,0.6948717948717948,"Speciﬁc Batch Normalization for Unsupervised Domain Adaptation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019."
REFERENCES,0.6974358974358974,"[39] Rajendra Bhatia. Positive deﬁnite matrices. Princeton university press, 2009. ISBN 978-0-691-"
REFERENCES,0.7,16825-8.
REFERENCES,0.7025641025641025,"[40] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix"
REFERENCES,0.7051282051282052,"manifolds. Princeton University Press, 2009."
REFERENCES,0.7076923076923077,"[41] Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A Riemannian framework for tensor"
REFERENCES,0.7102564102564103,"computing. International Journal of computer vision, 66(1):41–66, 2006."
REFERENCES,0.7128205128205128,[42] Hermann Karcher. Riemannian center of mass and molliﬁer smoothing. Communications on
REFERENCES,0.7153846153846154,"pure and applied mathematics, 30(5):509–541, 1977."
REFERENCES,0.717948717948718,"[43] Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, Tokyo, 1"
REFERENCES,0.7205128205128205,"edition, 2016. ISBN 978-4-431-55978-8."
REFERENCES,0.7230769230769231,"[44] Daniel Brooks, Olivier Schwander, Frederic Barbaresco, Jean-Yves Schneider, and Matthieu"
REFERENCES,0.7256410256410256,"Cord. Riemannian batch normalization for SPD neural networks. In Advances in Neural
Information Processing Systems, volume 32. Curran Associates, Inc., 2019."
REFERENCES,0.7282051282051282,[45] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
REFERENCES,0.7307692307692307,"by reducing internal covariate shift. In International conference on machine learning, pages
448–456. PMLR, 2015."
REFERENCES,0.7333333333333333,"[46] Reinmar J. Kobler, Jun-ichiro Hirayama, and Motoaki Kawanabe. Controlling The Fréchet"
REFERENCES,0.735897435897436,"Variance Improves Batch Normalization on the Symmetric Positive Deﬁnite Manifold. In
ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 3863–3867, Singapore, 2022. IEEE. ISBN 978-1-66540-540-9. doi: 10.1109/"
REFERENCES,0.7384615384615385,ICASSP43922.2022.9746629.
REFERENCES,0.7410256410256411,"[47] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander M ˛adry. How Does Batch"
REFERENCES,0.7435897435897436,"Normalization Help Optimization? In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, NIPS’18, pages 2488–2498, Red Hook, NY, USA,
2018. Curran Associates Inc."
REFERENCES,0.7461538461538462,[48] Sergey Ioffe. Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-
REFERENCES,0.7487179487179487,"Normalized Models. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, NIPS’17, pages 1942–1950, Red Hook, NY, USA, 2017. Curran Associates
Inc. ISBN 978-1-5108-6096-4."
REFERENCES,0.7512820512820513,"[49] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix Backpropagation for"
REFERENCES,0.7538461538461538,"Deep Networks with Structured Layers. In 2015 IEEE International Conference on Computer
Vision (ICCV), pages 2965–2973, Santiago, Chile, 2015. IEEE. ISBN 978-1-4673-8391-2. doi:
10.1109/ICCV.2015.339."
REFERENCES,0.7564102564102564,"[50] Jeffrey Ho, Guang Cheng, Hesamoddin Salehian, and Baba Vemuri. Recursive Karcher Ex-"
REFERENCES,0.7589743589743589,"pectation Estimators And Geometric Law of Large Numbers. In Carlos M. Carvalho and
Pradeep Ravikumar, editors, Proceedings of the Sixteenth International Conference on Artiﬁcial
Intelligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages
325–332, Scottsdale, Arizona, USA, 2013. PMLR."
REFERENCES,0.7615384615384615,"[51] Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric Means in"
REFERENCES,0.764102564102564,"a Novel Vector Space Structure on Symmetric Positive-Deﬁnite Matrices. SIAM Journal on
Matrix Analysis and Applications, 29(1):328–347, 2007. doi: 10.1137/050637996."
REFERENCES,0.7666666666666667,"[52] Paul L. Nunez and Ramesh Srinivasan. Electric Fields of the Brain. Oxford University Press,"
REFERENCES,0.7692307692307693,2006. ISBN 978-0-19-505038-7. doi: 10.1093/acprof:oso/9780195050387.001.0001.
REFERENCES,0.7717948717948718,"[53] Christoph M. Michel, Micah M. Murray, Göran Lantz, Sara Gonzalez, Laurent Spinelli, and"
REFERENCES,0.7743589743589744,"Rolando Grave De Peralta. EEG source imaging. Clinical Neurophysiology, 115(10), 2004.
doi: 10.1016/j.clinph.2004.06.001."
REFERENCES,0.7769230769230769,[54] A. Hyvärinen and E. Oja. Independent component analysis: algorithms and applications. Neural
REFERENCES,0.7794871794871795,"Networks, 13(4-5):411–430, 2000. doi: 10.1016/S0893-6080(00)00026-5."
REFERENCES,0.782051282051282,"[55] Stefan Haufe, Frank Meinecke, Kai Görgen, Sven Dähne, John Dylan Haynes, Benjamin"
REFERENCES,0.7846153846153846,"Blankertz, and Felix Bießmann. On the interpretation of weight vectors of linear models in
multivariate neuroimaging. NeuroImage, 87:96–110, 2014. doi: 10.1016/j.neuroimage.2013.10.
067."
REFERENCES,0.7871794871794872,"[56] Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin"
REFERENCES,0.7897435897435897,"Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard,
and Tonio Ball. Deep learning with convolutional neural networks for EEG decoding and
visualization: Convolutional Neural Networks in EEG Analysis. Human Brain Mapping, 38
(11):5391–5420, 2017. doi: 10.1002/hbm.23730."
REFERENCES,0.7923076923076923,"[57] Dinesh Acharya, Zhiwu Huang, Danda Pani Paudel, and Luc Van Gool. Covariance Pooling for"
REFERENCES,0.7948717948717948,"Facial Expression Recognition. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops, 2018."
REFERENCES,0.7974358974358975,[58] Gary Becigneul and Octavian-Eugen Ganea. Riemannian Adaptive Optimization Methods. In
REFERENCES,0.8,"International Conference on Learning Representations, 2019."
REFERENCES,0.8025641025641026,[59] G. Pfurtscheller and C. Neuper. Motor imagery and direct brain-computer communication.
REFERENCES,0.8051282051282052,"Proceedings of the IEEE, 89(7):1123–1134, 2001. doi: 10.1109/5.939829."
REFERENCES,0.8076923076923077,"[60] Frédéric Dehais, Alex Lafont, Raphaëlle Roy, and Stephen Fairclough. A Neuroergonomics Ap-"
REFERENCES,0.8102564102564103,"proach to Mental Workload, Engagement and Human Performance. Frontiers in Neuroscience,
14:268, 2020. doi: 10.3389/fnins.2020.00268."
REFERENCES,0.8128205128205128,"[61] Ana R. C. Donati, Solaiman Shokur, Edgard Morya, Debora S. F. Campos, Renan C. Moioli,"
REFERENCES,0.8153846153846154,"Claudia M. Gitti, Patricia B. Augusto, Sandra Tripodi, Cristhiane G. Pires, Gislaine A. Pereira,
Fabricio L. Brasil, Simone Gallo, Anthony A. Lin, Angelo K. Takigami, Maria A. Aratanha,
Sanjay Joshi, Hannes Bleuler, Gordon Cheng, Alan Rudolph, and Miguel A. L. Nicolelis. Long-
Term Training with a Brain-Machine Interface-Based Gait Protocol Induces Partial Neurological
Recovery in Paraplegic Patients. Scientiﬁc Reports, 6(1):30383, 2016. doi: 10.1038/srep30383."
REFERENCES,0.8179487179487179,"[62] Domen Novak, Roland Sigrist, Nicolas J. Gerig, Dario Wyss, René Bauer, Ulrich Götz, and"
REFERENCES,0.8205128205128205,"Robert Riener. Benchmarking Brain-Computer Interfaces Outside the Laboratory: The Cy-
bathlon 2016. Frontiers in Neuroscience, 11:756, 2018. doi: 10.3389/fnins.2017.00756."
REFERENCES,0.823076923076923,"[63] Michael Tangermann, Klaus-Robert Müller, Ad Aertsen, Niels Birbaumer, Christoph Braun,"
REFERENCES,0.8256410256410256,"Clemens Brunner, Robert Leeb, Carsten Mehring, Kai J Miller, Gernot Müller-Putz, Guido
Nolte, Gert Pfurtscheller, Hubert Preissl, Gerwin Schalk, Alois Schlögl, Carmen Vidaurre,
Stephan Waldert, and Benjamin Blankertz. Review of the BCI Competition IV. Frontiers in
Neuroscience, 6, 2012."
REFERENCES,0.8282051282051283,"[64] Josef Faller, Carmen Vidaurre, Teodoro Solis-Escalante, Christa Neuper, and Reinhold Scherer."
REFERENCES,0.8307692307692308,"Autocalibration and recurrent adaptation: towards a plug and play online ERD-BCI. Neural
Systems and Rehabilitation Engineering, IEEE Transactions on, 20(3):313–319, 2012."
REFERENCES,0.8333333333333334,"[65] Min-Ho Lee, O-Yeon Kwon, Yong-Jeong Kim, Hong-Kyung Kim, Young-Eun Lee, John"
REFERENCES,0.8358974358974359,"Williamson, Siamac Fazli, and Seong-Whan Lee. EEG dataset and OpenBMI toolbox for three
BCI paradigms: an investigation into BCI illiteracy. GigaScience, 8(5):giz002, 2019. doi:
10.1093/gigascience/giz002."
REFERENCES,0.8384615384615385,"[66] Rea Lehner, Neethu Robinson, Tushar Chouhan, Mihelj Mihelj, Ernest, Kratka Kratka, Paulina,"
REFERENCES,0.841025641025641,"Frédéric Debraine, Cuntai Guan, and Nicole Wenderoth. Design considerations for long
term non-invasive Brain Computer Interface training with tetraplegic CYBATHLON pilot:
CYBATHLON 2020 Brain-Computer Interface Race Calibration Paradigms. 2020. doi: 10.
3929/ETHZ-B-000458693. URL http://hdl.handle.net/20.500.11850/458693."
REFERENCES,0.8435897435897436,"[67] James R. Stieger, Stephen A. Engel, and Bin He. Continuous sensorimotor rhythm based"
REFERENCES,0.8461538461538461,"brain computer interface learning in a large population. Scientiﬁc Data, 8(1):98, 2021. doi:
10.1038/s41597-021-00883-1."
REFERENCES,0.8487179487179487,"[68] Lea Hehenberger, Reinmar J. Kobler, Catarina Lopes-Dias, Nitikorn Srisrisawang, Peter Tumfart,"
REFERENCES,0.8512820512820513,"John B. Uroko, Paul R. Torke, and Gernot R. Müller-Putz. Long-term mutual training for the
CYBATHLON BCI Race with a tetraplegic pilot: a case study on inter-session transfer and intra-
session adaptation. Frontiers in Human Neuroscience, 2021. doi: 10.3389/fnhum.2021.635777."
REFERENCES,0.8538461538461538,"[69] Marcel F. Hinss, Ludovic Darmet, Bertille Somon, Emilie Jahanpour, Fabien Lotte, Simon"
REFERENCES,0.8564102564102564,"Ladouce, and Raphaëlle N. Roy. An EEG dataset for cross-session mental workload estimation:
Passive BCI competition of the Neuroergonomics Conference 2021. 2021. doi: 10.5281/
ZENODO.5055046."
REFERENCES,0.8589743589743589,"[70] Alexandre Gramfort. MEG and EEG data analysis with MNE-Python. Frontiers in Neuroscience,"
REFERENCES,0.8615384615384616,"7, 2013. doi: 10.3389/fnins.2013.00267."
REFERENCES,0.8641025641025641,"[71] Kai Keng Ang, Zhang Yang Chin, Haihong Zhang, and Cuntai Guan. Filter Bank Common"
REFERENCES,0.8666666666666667,"Spatial Pattern (FBCSP) in Brain-Computer Interface. In 2008 IEEE International Joint
Conference on Neural Networks (IEEE World Congress on Computational Intelligence), pages
2390–2397, Hong Kong, China, 2008. IEEE. ISBN 978-1-4244-1820-6. doi: 10.1109/IJCNN.
2008.4634130."
REFERENCES,0.8692307692307693,"[72] Or Yair, Felix Dietrich, Ronen Talmon, and Ioannis G. Kevrekidis. Domain Adaptation with"
REFERENCES,0.8717948717948718,"Optimal Transport on the Manifold of SPD matrices. arXiv:1906.00616 [cs, stat], 2020."
REFERENCES,0.8743589743589744,"[73] Vernon J Lawhern, Amelia J Solon, Nicholas R Waytowich, Stephen M Gordon, Chou P"
REFERENCES,0.8769230769230769,"Hung, and Brent J Lance. EEGNet: a compact convolutional neural network for EEG-based
brain–computer interfaces. Journal of Neural Engineering, 15(5):056013, 2018. doi: 10.1088/
1741-2552/aace8c."
REFERENCES,0.8794871794871795,"[74] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,"
REFERENCES,0.882051282051282,"Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style,
High-Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’
Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems
32, pages 8024–8035. Curran Associates, Inc., 2019."
REFERENCES,0.8846153846153846,"[75] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,"
REFERENCES,0.8871794871794871,"P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011."
REFERENCES,0.8897435897435897,"[76] Marian Tietz, Thomas J. Fan, Daniel Nouri, Benjamin Bossan, and skorch Developers. skorch:"
REFERENCES,0.8923076923076924,"A scikit-learn compatible neural network library that wraps PyTorch. 2017. URL https:
//skorch.readthedocs.io/en/stable/."
REFERENCES,0.8948717948717949,"[77] Max Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian Optimization in"
REFERENCES,0.8974358974358975,"PyTorch, 2020. _eprint: 2005.02819."
REFERENCES,0.9,"[78] Barachant Alexandre.
pyRiemann, 2022.
URL https://github.com/pyRiemann/
pyRiemann."
REFERENCES,0.9025641025641026,"[79] James Townsend, Niklas Koep, and Sebastian Weichwald. Pymanopt: A python toolbox"
REFERENCES,0.9051282051282051,"for optimization on manifolds using automatic differentiation. Journal of Machine Learning
Research, 17(137):1–5, 2016."
REFERENCES,0.9076923076923077,[80] Ian G. Campbell. EEG Recording and Analysis for Sleep Research. Current Protocols in
REFERENCES,0.9102564102564102,"Neuroscience, 49(1):10.2.1–10.2.19, 2009. doi: 10.1002/0471142301.ns1002s49."
REFERENCES,0.9128205128205128,"[81] Hubert Banville, Omar Chehab, Aapo Hyvärinen, Denis-Alexander Engemann, and Alexandre"
REFERENCES,0.9153846153846154,"Gramfort. Uncovering the structure of clinical EEG signals with self-supervised learning.
Journal of Neural Engineering, 2020. doi: 10.1088/1741-2552/abca18."
REFERENCES,0.9179487179487179,"[82] U. Rajendra Acharya, S. Vinitha Sree, G. Swapna, Roshan Joy Martis, and Jasjit S. Suri."
REFERENCES,0.9205128205128205,"Automated EEG analysis of epilepsy: A review. Knowledge-Based Systems, 45:147–165, 2013.
doi: https://doi.org/10.1016/j.knosys.2013.02.014."
REFERENCES,0.9230769230769231,"[83] Marc R. Nuwer, David A. Hovda, Lara M. Schrader, and Paul M. Vespa. Routine and quantitative"
REFERENCES,0.9256410256410257,"eeg in mild traumatic brain injury. Clinical Neurophysiology, 116(9):2001–2025, 2005. doi:
10.1016/j.clinph.2005.05.008."
REFERENCES,0.9282051282051282,"[84] Lukas A.W. Gemein, Robin T. Schirrmeister, Patryk Chrabaszcz, Daniel Wilson, Joschka"
REFERENCES,0.9307692307692308,"Boedecker, Andreas Schulze-Bonhage, Frank Hutter, and Tonio Ball. Machine-learning-based
diagnostics of EEG pathology. NeuroImage, 220:117021, 2020. doi: 10.1016/j.neuroimage.
2020.117021."
REFERENCES,0.9333333333333333,"[85] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François"
REFERENCES,0.9358974358974359,"Laviolette, Mario March, and Victor Lempitsky. Domain-Adversarial Training of Neural
Networks. Journal of Machine Learning Research, 17(59):1–35, 2016."
REFERENCES,0.9384615384615385,Checklist
REFERENCES,0.941025641025641,1. For all authors...
REFERENCES,0.9435897435897436,(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
REFERENCES,0.9461538461538461,"contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See section 6"
REFERENCES,0.9487179487179487,(c) Did you discuss any potential negative societal impacts of your work? [Yes] See section
REFERENCES,0.9512820512820512,"6
(d) Have you read the ethics review guidelines and ensured that your paper conforms to"
REFERENCES,0.9538461538461539,"them? [Yes]
2. If you are including theoretical results..."
REFERENCES,0.9564102564102565,(a) Did you state the full set of assumptions of all theoretical results? [Yes] See paragraph
REFERENCES,0.958974358974359,"4 in section 3.
(b) Did you include complete proofs of all theoretical results? [Yes] See appendix A.1 in"
REFERENCES,0.9615384615384616,"the supplementary material.
3. If you ran experiments..."
REFERENCES,0.9641025641025641,"(a) Did you include the code, data, and instructions needed to reproduce the main exper-"
REFERENCES,0.9666666666666667,"imental results (either in the supplemental material or as a URL)? [Yes] See Supple-
mentary Material.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they"
REFERENCES,0.9692307692307692,"were chosen)? [Yes] See paragraph 2 in section 5 for a brief summary and appendix
A.2 for details.
(c) Did you report error bars (e.g., with respect to the random seed after running exper-"
REFERENCES,0.9717948717948718,"iments multiple times)? [Yes] We provide error bars with respect to the variability
across sessions and subjects.
(d) Did you include the total amount of compute and the type of resources used (e.g., type"
REFERENCES,0.9743589743589743,"of GPUs, internal cluster, or cloud provider)? [Yes] See paragraph 3 in section 5.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets..."
REFERENCES,0.9769230769230769,"(a) If your work uses existing assets, did you cite the creators? [Yes] See paragraph 1 in"
REFERENCES,0.9794871794871794,"section 5.
(b) Did you mention the license of the assets? [Yes] See appendix A.2.1 in the Supplemen-"
REFERENCES,0.982051282051282,"tary material.
(c) Did you include any new assets either in the supplemental material or as a URL? [No]
(d) Did you discuss whether and how consent was obtained from people whose data you’re"
REFERENCES,0.9846153846153847,"using/curating? [Yes] See paragraph 1 in section 5.
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable"
REFERENCES,0.9871794871794872,"information or offensive content? [Yes] See paragraph 1 in section 5.
5. If you used crowdsourcing or conducted research with human subjects..."
REFERENCES,0.9897435897435898,"(a) Did you include the full text of instructions given to participants and screenshots, if"
REFERENCES,0.9923076923076923,"applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review"
REFERENCES,0.9948717948717949,"Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount"
REFERENCES,0.9974358974358974,spent on participant compensation? [N/A]
