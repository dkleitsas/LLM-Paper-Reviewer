Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0013869625520110957,"Neural networks on simplicial complexes (SCs) can learn from data residing on
1"
ABSTRACT,0.0027739251040221915,"simplices such as nodes, edges, triangles, etc. However, existing works often
2"
ABSTRACT,0.004160887656033287,"overlook the Hodge theory that decomposes simplicial data into three orthogonal
3"
ABSTRACT,0.005547850208044383,"characteristic subspaces, such as the identiﬁable gradient, curl and harmonic com-
4"
ABSTRACT,0.006934812760055479,"ponents of edge ﬂows. In this paper, we aim to incorporate this data inductive bias
5"
ABSTRACT,0.008321775312066574,"into learning on SCs. Particularly, we present a general convolutional architecture
6"
ABSTRACT,0.009708737864077669,"which respects the three key principles of uncoupling the lower and upper sim-
7"
ABSTRACT,0.011095700416088766,"plicial adjacencies, accounting for the inter-simplicial couplings, and performing
8"
ABSTRACT,0.012482662968099861,"higher-order convolutions. To understand these principles, we ﬁrst use Dirichlet
9"
ABSTRACT,0.013869625520110958,"energy minimizations on SCs to interpret their effects on mitigating the simplicial
10"
ABSTRACT,0.015256588072122053,"oversmoothing. Then, through the lens of spectral simplicial theory, we show the
11"
ABSTRACT,0.016643550624133148,"three principles promote the Hodge-aware learning of this architecture, in the sense
12"
ABSTRACT,0.018030513176144243,"that the three Hodge subspaces are invariant under its learnable functions and the
13"
ABSTRACT,0.019417475728155338,"learning in two nontrivial subspaces are independent and expressive. To further
14"
ABSTRACT,0.020804438280166437,"investigate the learning ability of this architecture, we also study it is stable against
15"
ABSTRACT,0.022191400832177532,"small perturbations on simplicial connections. Finally, we experimentally validate
16"
ABSTRACT,0.023578363384188627,"the three principles by comparing with methods that either violate or do not respect
17"
ABSTRACT,0.024965325936199722,"them. Overall, this paper bridges learning on SCs with the Hodge decomposition,
18"
ABSTRACT,0.026352288488210817,"highlighting its importance for rational and effective learning from simplicial data.
19"
INTRODUCTION,0.027739251040221916,"1
Introduction
20"
INTRODUCTION,0.02912621359223301,"It is not uncommon to have polyadic interactions in such as friendship networks [1], collaboration
21"
INTRODUCTION,0.030513176144244106,"networks [2], gene regulatory networks [3], etc [4–6]. To remedy the pitfall that graphs are limited to
22"
INTRODUCTION,0.0319001386962552,"model pairwise interactions between data entites on nodes, simplicial complexes (SCs) have become
23"
INTRODUCTION,0.033287101248266296,"popular among others [7]. A SC can be informally viewed as an extension of a graph, which is the
24"
INTRODUCTION,0.03467406380027739,"simplest SC, by including, not limited to, some triangles over the edge set. SCs like graphs have
25"
INTRODUCTION,0.036061026352288486,"algebraic representations – the Hodge Laplacians, an extension of graph Laplacians [8, 9]. Moreover,
26"
INTRODUCTION,0.03744798890429958,"besides node-wise data, SCs can support data on general simplices, e,g., ﬂow-type data, e.g., water
27"
INTRODUCTION,0.038834951456310676,"ﬂows [10], trafﬁc ﬂows [11], information ﬂows [12], etc., naturally arise as data on edges, and data
28"
INTRODUCTION,0.04022191400832178,"related to three parties, e.g., triadic collaborations [2], can be deﬁned on triangles in a SC.
29"
INTRODUCTION,0.04160887656033287,"Thus, existing works have built NNs on SCs to learn from such simplicial data, to name a few,
30"
INTRODUCTION,0.04299583911234397,"[13–19]. In analogous to graph neural networks (GNNs) learning from node data relying on the
31"
INTRODUCTION,0.044382801664355064,"adjacency between nodes, the idea behind these works is to rely on the relations between simplices.
32"
INTRODUCTION,0.04576976421636616,"Such relations can be twofold: ﬁrst, two simplices can be lower and upper adjacent to each other,
33"
INTRODUCTION,0.047156726768377254,"such as an edge can be (lower) adjacent to another edge via a common node, and can also be (upper)
34"
INTRODUCTION,0.04854368932038835,"adjacent to others by sharing a common triangle; and second, there exist the inter-simplicial couplings
35"
INTRODUCTION,0.049930651872399444,"(or simplicial incidences) such that a node can induce data on its incident edge and a triangle can
36"
INTRODUCTION,0.05131761442441054,"cause data on its three edges, or the other way around. Along with this idea, [15, 16, 19] proposed
37"
INTRODUCTION,0.052704576976421634,"convolutional-type NNs by applying the simplicial adjacencies, [14, 20] included also inter-simplicial
38"
INTRODUCTION,0.05409153952843273,"couplings, and [17, 21] generalized the graph message-passing [22] to SCs based on both relations.
39"
INTRODUCTION,0.05547850208044383,"However, these works often solely focus on the simplicial structures, overlooking the Hodge decom-
40"
INTRODUCTION,0.056865464632454926,"position [23], which gives three orthogonal subspaces that uniquely characterize the simplicial data.
41"
INTRODUCTION,0.05825242718446602,"An edge ﬂow can be decomposed into three distinct parts: a curl-free part induced by some node
42"
INTRODUCTION,0.059639389736477116,"data, a divergence-free (div-free) part that follows ﬂow conservation (in-ﬂows equal to out-ﬂows at
43"
INTRODUCTION,0.06102635228848821,"nodes), and a harmonic part that is both div- and curl-free. Meanwhile, real-world simplicial data
44"
INTRODUCTION,0.06241331484049931,"often presents properties to be in certain subspaces but not others, or its components carry physical
45"
INTRODUCTION,0.0638002773925104,"usefulness, e.g., statistical ranking, exchange market [24], trafﬁc networks [11], brain networks [12],
46"
INTRODUCTION,0.0651872399445215,"game theory [25], etc. Thus, intuitively, as an example, a Hodge-biased learner should not, at least not
47"
INTRODUCTION,0.06657420249653259,"primarily, learn in the div-free space if the edge ﬂow is curl-free. If the learning function preserves the
48"
INTRODUCTION,0.06796116504854369,"subspaces and operates independently in three subspaces, the learning space is substantially shrunk.
49"
INTRODUCTION,0.06934812760055478,"This in fact provides an important inductive bias allowing for rational and effective learning on SCs.
50"
INTRODUCTION,0.07073509015256588,"Motivated by this, in this paper, we present the general convolutional learning on SCs, SCCNN, which
51"
INTRODUCTION,0.07212205270457697,"respects three key principles of uncoupling the lower and upper simplicial adjacencies, accounting
52"
INTRODUCTION,0.07350901525658807,"for the inter-simplicial couplings, and performing higher-order convolutions. Unlike existing convo-
53"
INTRODUCTION,0.07489597780859916,"lutional methods [14–16], which either lack theoretical insights or only discuss their architectural
54"
INTRODUCTION,0.07628294036061026,"differences in the simplicial domain, we focus on providing a theoretical analysis of these three
55"
INTRODUCTION,0.07766990291262135,"principles from both the perspectives of simplicial and simplicial data, speciﬁcally Hodge theory.
56"
INTRODUCTION,0.07905686546463246,"This offers deeper and unique insights when compared to the more closely related works [19, 17].
57"
INTRODUCTION,0.08044382801664356,"Main contributions. In Section 3.2, we ﬁrst use Dirichlet energy minimizations on SCs to understand
58"
INTRODUCTION,0.08183079056865465,"how uncoupling the lower and upper adjacencies in Hodge Laplacians and the inter-simplicial
59"
INTRODUCTION,0.08321775312066575,"couplings can mitigate the oversmoothing inherited from generalizing GCN to SCs. Under the help
60"
INTRODUCTION,0.08460471567267684,"of spectral simplicial theory [26–28], in Section 4, we characterize the spectral behavior of SCCNN
61"
INTRODUCTION,0.08599167822468794,"and its expressive power. We show SCCNN performs independent and expressive learning in the
62"
INTRODUCTION,0.08737864077669903,"three subspaces of the Hodge decomposition, which are invariant under its learning operators. This
63"
INTRODUCTION,0.08876560332871013,"Hodge-awareness (or Hodge-aided bias) allows for effective and rational learning on SCs compared
64"
INTRODUCTION,0.09015256588072122,"to MLP or simplicial message-passing [17]. In Section 5, we also prove it is stable against small
65"
INTRODUCTION,0.09153952843273232,"perturbations on the strengths of simplicial connections, and show how three principles can affect the
66"
INTRODUCTION,0.09292649098474341,"stability. Lastly, we validate our ﬁndings on different simplicial tasks, including recovering foreign
67"
INTRODUCTION,0.09431345353675451,"currency exchange (forex) rates, predicting triadic and tetradic collaborations, and trajectories.
68"
BACKGROUND,0.0957004160887656,"2
Background
69"
BACKGROUND,0.0970873786407767,"Simplicial complex and simplicial adjacencies. A k-simplex sk is a subset of V = {1, . . . , n0}
70"
BACKGROUND,0.09847434119278779,"with cardinality k + 1. A face of sk is a subset with cardinality k. A coface of sk is a (k + 1)-
71"
BACKGROUND,0.09986130374479889,"simplex that has sk as a face. Nodes, edges and (ﬁlled) triangles are geometric realizations of 0-,
72"
BACKGROUND,0.10124826629680998,"1- and 2-simplices. A SC S of order K is a collection of k-simplices, k = 0, . . . , K, with the
73"
BACKGROUND,0.10263522884882108,"inclusion property: sk−1 2 S if sk−1 ⇢sk for sk 2 S. A graph is a SC of order one and by
74"
BACKGROUND,0.10402219140083217,"taking into account some triangles, we obtain a SC of order two. We collect all k-simplices of S
75"
BACKGROUND,0.10540915395284327,in set Sk = {sk
BACKGROUND,0.10679611650485436,"i }i=1,...,nk with nk = |Sk|, i.e., S = [K"
BACKGROUND,0.10818307905686546,"k=0Sk. For sk, We say a k-simplex is lower
76"
BACKGROUND,0.10957004160887657,"(upper) adjacent to sk if they share a common face (coface). For computations, an orientation of a
77"
BACKGROUND,0.11095700416088766,"simplex is chosen as an ordering of its vertices (a node has a trivial orientation). Here we consider
78"
BACKGROUND,0.11234396671289876,"the lexicographical ordering sk = [1, . . . , k + 1], e.g., a triangle s2 = {i, j, k} is oriented as [i, j, k].
79"
BACKGROUND,0.11373092926490985,"Algebraic representation. Incidence matrix Bk describes the relations between (k −1)- (i.e., faces)
80"
BACKGROUND,0.11511789181692095,"and k-simplices, e.g., B1 is the node-to-edge incidence matrix and B2 edge-to-triangle. We have
81"
BACKGROUND,0.11650485436893204,BkBk+1 = 0 by deﬁnition [9]. The k-Hodge Laplacian is Lk = B>
BACKGROUND,0.11789181692094314,k Bk + Bk+1B>
BACKGROUND,0.11927877947295423,"k+1 with the
82"
BACKGROUND,0.12066574202496533,"lower Laplacian Lk,d = B>"
BACKGROUND,0.12205270457697642,"k Bk and the upper Laplacian Lk,u = Bk+1B>"
BACKGROUND,0.12343966712898752,"k+1. We have a set of
83"
BACKGROUND,0.12482662968099861,"Lk, k = 1, . . . , K −1 in a SC of order K with the graph Laplacian L0 = B1B>"
BACKGROUND,0.1262135922330097,"1 , and LK = B>"
BACKGROUND,0.1276005547850208,"KBK.
84"
BACKGROUND,0.1289875173370319,"Note that Lk,d and Lk,u encode the lower and upper adjacencies of k-simplices. For example, for
85"
BACKGROUND,0.130374479889043,"k = 1, they encode the edge-to-edge adjacencies through nodes and triangles, respectively.
86"
BACKGROUND,0.1317614424410541,"Simplicial data. A k-simplicial data (or k-signal) xk 2 Rnk is deﬁned by an alternating map fk
87"
BACKGROUND,0.13314840499306518,"which assigns a real value to a simplex, and restricts that if the orientation of a simplex is anti-aligned
88"
BACKGROUND,0.13453536754507628,"with the reference orientation, then the sign of the signal value will be changed [9].
89"
BACKGROUND,0.13592233009708737,"Incidence matrices as derivative operators on SCs. We can measure how a k-signal xk varies w.r.t.
90"
BACKGROUND,0.13730929264909847,the faces and cofaces of k-simplices by applying Bkxk and B>
BACKGROUND,0.13869625520110956,"k+1xk [29]. For a node signal x0, B>"
BACKGROUND,0.14008321775312066,"1 x0
91"
BACKGROUND,0.14147018030513175,"computes its gradient as the difference between adjacent nodes. Thus, a constant x0 has zero gradient.
92"
BACKGROUND,0.14285714285714285,"For an edge ﬂow x1, [B1x1]j = P"
BACKGROUND,0.14424410540915394,"i<j[x1][i,j] −P"
BACKGROUND,0.14563106796116504,"j<k[x1][j,k] computes its divergence, which is the
93"
BACKGROUND,0.14701803051317613,"difference between the in-ﬂow and the out-ﬂow at node j, and [B>"
BACKGROUND,0.14840499306518723,"2 x1]t = [x1][i,j]−[x1][i,k]+[x1][j,k]
94"
BACKGROUND,0.14979195561719832,"computes the curl of x1, which is the net-ﬂow circulation in triangle t = [i, j, k].
95"
BACKGROUND,0.15117891816920942,"Theorem 1 (Hodge decomposition [23, 9]). The k-simplicial data space admits an orthogonal direct
96"
BACKGROUND,0.15256588072122051,sum decomposition Rnk = im(B>
BACKGROUND,0.1539528432732316,"k ) ⊕ker(Lk) ⊕im(Bk+1). Moreover, we have ker(B>"
BACKGROUND,0.1553398058252427,"k+1) =
97 im(B>"
BACKGROUND,0.15672676837725383,"k ) ⊕ker(Lk) and ker(Bk) = ker(Lk) ⊕im(Bk+1).
98"
BACKGROUND,0.15811373092926492,"In the node space, this is trivial as Rn0 = ker(L0)⊕im(B1) where the kernel of L0 contains constant
99"
BACKGROUND,0.15950069348127602,"node data and the image of B1 contains nonconstant data. In the edge case, three subspaces carry
100"
BACKGROUND,0.1608876560332871,more tangible meaning: the gradient space im(B>
BACKGROUND,0.1622746185852982,"1 ) collects edge ﬂows as the gradient of some node
101"
BACKGROUND,0.1636615811373093,"signal, which are curl-free; the curl space im(B2) consists of ﬂows cycling around triangles, which
102"
BACKGROUND,0.1650485436893204,"are div-free; and ﬂows in the harmonic space ker(L1) are both div- and curl-free. In this paper, we
103"
BACKGROUND,0.1664355062413315,"inherit the names of three edge subspaces to general k-simplices. This theorem states that any xk can
104"
BACKGROUND,0.1678224687933426,"be uniquely expressed as xk = xk,G + xk,H + xk,C with gradient part xk,G = B>"
BACKGROUND,0.16920943134535368,"k xk−1, curl part
105"
BACKGROUND,0.17059639389736478,"xk,C = Bk+1xk+1, for some xk±1, and harmonic part following Lkxk,H = 0.
106"
SIMPLICIAL COMPLEX CNNS,0.17198335644937587,"3
Simplicial Complex CNNs
107"
SIMPLICIAL COMPLEX CNNS,0.17337031900138697,"We start by introducing the general convolutional architecture on SCs, followed by its properties, then
108"
SIMPLICIAL COMPLEX CNNS,0.17475728155339806,"we discuss its components from an energy minimizations perspective. We refer to Appendix B for
109"
SIMPLICIAL COMPLEX CNNS,0.17614424410540916,"some illustrations. In a SC, a SCCNN at layer l computes the k-output xl"
SIMPLICIAL COMPLEX CNNS,0.17753120665742025,k with xl−1
SIMPLICIAL COMPLEX CNNS,0.17891816920943135,"k−1, xl−1"
SIMPLICIAL COMPLEX CNNS,0.18030513176144244,"k
and xl−1"
SIMPLICIAL COMPLEX CNNS,0.18169209431345354,"k+1
110"
SIMPLICIAL COMPLEX CNNS,0.18307905686546463,"as inputs, i.e., a map SCCNNl"
SIMPLICIAL COMPLEX CNNS,0.18446601941747573,k : {xl−1
SIMPLICIAL COMPLEX CNNS,0.18585298196948682,"k−1, xl−1"
SIMPLICIAL COMPLEX CNNS,0.18723994452149792,"k
, xl−1"
SIMPLICIAL COMPLEX CNNS,0.18862690707350901,k+1} ! xl
SIMPLICIAL COMPLEX CNNS,0.1900138696255201,"k, for all k. It admits a detailed form
111 xl"
SIMPLICIAL COMPLEX CNNS,0.1914008321775312,k = σ(Hl
SIMPLICIAL COMPLEX CNNS,0.1927877947295423,"k,dxl−1"
SIMPLICIAL COMPLEX CNNS,0.1941747572815534,"k,d + Hl kxl−1"
SIMPLICIAL COMPLEX CNNS,0.1955617198335645,"k
+ Hl"
SIMPLICIAL COMPLEX CNNS,0.19694868238557559,"k,uxl−1"
SIMPLICIAL COMPLEX CNNS,0.19833564493758668,"k,u ), with Hl k = Td
X t=0 wl"
SIMPLICIAL COMPLEX CNNS,0.19972260748959778,"k,d,tLt k,d + Tu
X t=0 wl"
SIMPLICIAL COMPLEX CNNS,0.20110957004160887,"k,u,tLt"
SIMPLICIAL COMPLEX CNNS,0.20249653259361997,"k,u.
(1)"
SIMPLICIAL COMPLEX CNNS,0.20388349514563106,1) Previous output xl−1
SIMPLICIAL COMPLEX CNNS,0.20527045769764216,"k
is passed to a simplicial convolution ﬁlter (SCF [30]) Hl"
SIMPLICIAL COMPLEX CNNS,0.20665742024965325,"k of orders Td, Tu,
112"
SIMPLICIAL COMPLEX CNNS,0.20804438280166435,"which performs a linear combination of the data from up to Td-hop lower-adjacent and Tu-hop
113"
SIMPLICIAL COMPLEX CNNS,0.20943134535367544,"upper-adjacent simplices, weighted by two sets of learnable weights {wl"
SIMPLICIAL COMPLEX CNNS,0.21081830790568654,"k,d,t}, {wl"
SIMPLICIAL COMPLEX CNNS,0.21220527045769763,"k,u,t}.
114"
SIMPLICIAL COMPLEX CNNS,0.21359223300970873,2) xl−1
SIMPLICIAL COMPLEX CNNS,0.21497919556171982,"k,d = B>"
SIMPLICIAL COMPLEX CNNS,0.21636615811373092,k xl−1
SIMPLICIAL COMPLEX CNNS,0.217753120665742,k−1 and xl−1
SIMPLICIAL COMPLEX CNNS,0.21914008321775313,"k,u = Bk+1xl−1"
SIMPLICIAL COMPLEX CNNS,0.22052704576976423,"k+1 are the lower and upper projections from (k ± 1)-
115"
SIMPLICIAL COMPLEX CNNS,0.22191400832177532,"simplices via incidence relations, respectively. Then, xl−1"
SIMPLICIAL COMPLEX CNNS,0.22330097087378642,"k,d is passed to a lower SCF Hl"
SIMPLICIAL COMPLEX CNNS,0.22468793342579751,"k,d :=
116
PTd"
SIMPLICIAL COMPLEX CNNS,0.2260748959778086,t=0 w0l
SIMPLICIAL COMPLEX CNNS,0.2274618585298197,"k,d,tLt"
SIMPLICIAL COMPLEX CNNS,0.2288488210818308,"k,d, and the upper projection xl−1"
SIMPLICIAL COMPLEX CNNS,0.2302357836338419,"k,u is passed to an upper SCF Hl"
SIMPLICIAL COMPLEX CNNS,0.231622746185853,"k,u := PTu"
SIMPLICIAL COMPLEX CNNS,0.23300970873786409,t=0 w0l
SIMPLICIAL COMPLEX CNNS,0.23439667128987518,"k,u,tLt"
SIMPLICIAL COMPLEX CNNS,0.23578363384188628,"k,u.
117"
SIMPLICIAL COMPLEX CNNS,0.23717059639389737,"Lastly, the sum of the three SCF outputs is passed to an elementwise nonlinearity σ(·).
118"
SIMPLICIAL COMPLEX CNNS,0.23855755894590847,"This architecture subsumes the methods in [14–16, 19, 18, 20]. Particularly, we emphasize on the key
119"
SIMPLICIAL COMPLEX CNNS,0.23994452149791956,"three principles. 1) Uncouple the lower and upper Laplacians: this leads to an independent treatment
120"
SIMPLICIAL COMPLEX CNNS,0.24133148404993066,"of the lower and upper adjacencies, achieved by using two sets of learnable weights; 2) Account for
121"
SIMPLICIAL COMPLEX CNNS,0.24271844660194175,"the inter-simplicial couplings: xk,d and xk,u carry the nontrivial information contained in the faces
122"
SIMPLICIAL COMPLEX CNNS,0.24410540915395285,"and cofaces (by Theorem 1); and 3) Perform higher-order convolutions: considering Td, Tu ≥1 in
123"
SIMPLICIAL COMPLEX CNNS,0.24549237170596394,"SCFs which leads to a multi-hop receptive ﬁeld on SCs. In short, SCCNN propagates information
124"
SIMPLICIAL COMPLEX CNNS,0.24687933425797504,"across SCs based on two simplicial adjacencies and two incidences in a multi-hop fashion.
125"
PROPERTIES,0.24826629680998613,"3.1
Properties
126"
PROPERTIES,0.24965325936199723,"Simplicial locality. SCFs admit an intra-simplicial locality: Hkxk is localized in Td-hop lower and
127"
PROPERTIES,0.2510402219140083,"Tu-hop upper k-simplicial neighborhoods [30]. A SCCNN preserves such locality as σ(·) does not
128"
PROPERTIES,0.2524271844660194,"alter the information locality. It also admits the inter-simplicial locality between k- and (k ± 1)-
129"
PROPERTIES,0.2538141470180305,"simplices, which extends to simplices of orders k ± 2 if L ≥2 because Bkσ(Bk+1) 6= 0 [31].
130"
PROPERTIES,0.2552011095700416,"Moreover, the two localities are coupled in a multi-hop way through SCFs such that a node not only
131"
PROPERTIES,0.2565880721220527,"interacts with its incident edges and the triangles including it, but also those further hops away.
132"
PROPERTIES,0.2579750346740638,"Complexity. A SCCNN layer has the parameter complexity of order O(Td + Tu) and the computa-
133"
PROPERTIES,0.2593619972260749,"tional complexity O(k(nk + nk+1) + nkmk(Td + Tu)), linear to the simplex dimensions, where mk
134"
PROPERTIES,0.260748959778086,"is the maximum of the number of neighbors for k-simplices.
135"
PROPERTIES,0.2621359223300971,"Equivariance. SCCNNs are permutation-equivairant, which allows us to list simplices in any order,
136"
PROPERTIES,0.2635228848821082,"and orinetation-equivariant if σ(·) is odd, which gives us the freedom to choose reference orientations.
137"
PROPERTIES,0.26490984743411927,"In Appendix B.3, we provide formal discussions on such equivariances and why permutations form a
138"
PROPERTIES,0.26629680998613037,"symmetry group of a SC and orientation changes are symmetries of data space but not SCs.
139"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.26768377253814146,"3.2
A perspective of SCCNN from Dirichlet energy minimization on SCs
140"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.26907073509015256,Deﬁnition 2. The Dirichlet energy of xk is D(xk) = Dd(xk)+Du(xk) := kBkxkk2 2+kB>
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.27045769764216365,k+1xkk2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.27184466019417475,"2.
141"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.27323162274618584,"For node signals, D(x0) = kB>"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.27461858529819694,1 x0k2 2 = P i P
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.27600554785020803,"jkx0,i −x0,jk2 is a `2-norm of the gradient of x0.
142"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.27739251040221913,"For edge ﬂows, D(x1) is the sum of the total divergence and curl, which measure the ﬂow variations
143"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2787794729542302,"w.r.t. nodes and triangles, respectively. In general, D(xk) measures the lower and upper k-simplicial
144"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2801664355062413,"signal variations w.r.t. the faces (Dd(xk)) and cofaces (Du(xk)). A k-signal xk with D(xk) = 0
145"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2815533980582524,"follows Lkxk = 0, called harmonic, e.g., a constant node signal and a div- and curl-free edge ﬂow.
146"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2829403606102635,"Simplicial shifting as Hodge Laplacian smoothing. [14, 20] considered Hk as a weighted variant
147"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2843273231622746,"of I −Lk, generalizing the GCN layer [32]. This simplicial shifting step is necessarily a Hodge
148"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2857142857142857,Laplacian smoothing [31]. Given an initial x0
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2871012482662968,"k, we consider the Dirichlet energy minimization:
149"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2884882108183079,minxk kBkxkk2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.289875173370319,2 + γkB>
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2912621359223301,k+1xkk2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.2926490984743412,"2, γ > 0, gradient descent: xl+1"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.29403606102635227,"k,gd = (I −⌘Lk,d −⌘γLk,u)xl k (2)"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.29542302357836336,with step size ⌘> 0. The simplicial shifting xl+1
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.29680998613037446,"k
= w0(I −Lk)xl"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.29819694868238555,"k is a gradient descent with
150"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.29958391123439665,"⌘= γ = 1 and weighted by w0, then followed by nonlinearity. A minimizer of Eq. (2) with γ = 1 is
151"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.30097087378640774,"in the harmonic space ker(Lk). Thus, an NN composed of simplicial shifting layers may lead to an
152"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.30235783633841884,"output with exponentially decreasing Dirichlet energy as it deepens, i.e., simplicial oversmoothing.
153"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.30374479889042993,Proposition 3. If w2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.30513176144244103,0kI −Lkk2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3065187239944521,"2 < 1, D(xl+1"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3079056865464632,"k
) in a simplicial shifting exponentially converges to 0.
154"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3092926490984743,"This generalizes the oversmoothing of GCN and its variants [33–35]. However, when uncoupling
155"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3106796116504854,"the lower and upper parts of Lk in this shifting, associated with γ 6= 1, the decrease of D(xk) can
156"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3120665742024965,"slow down or cease because the objective instead looks for a solution primarily in either ker(Bk)
157"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.31345353675450766,(for γ ⌧1) or ker(B>
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.31484049930651875,"k+1) (for γ ≫1), not necessarily in ker(Lk), as we show in Section 6.
158"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.31622746185852985,"Inter-simplicial couplings as sources. Given some nontrivial xk−1 and xk+1, we consider
159"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.31761442441054094,minxk kBkxk −xk−1k2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.31900138696255204,2 + kB>
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.32038834951456313,k+1xk −xk+1k2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3217753120665742,"2,
(3)"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3231622746185853,which has a gradient descent xl+1
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3245492371705964,"k,gd = (I −⌘Lk)xl"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3259361997226075,"k + ⌘(xk,d + xk,u). It resembles the whole layer
160"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3273231622746186,"in [14, 20], xl+1"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3287101248266297,"k
= w0(I −Lk)xl"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3300970873786408,"k + w1xk,d + w2xk,u with some weights, followed by nonlinearity.
161"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3314840499306519,We have D(xl+1
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.332871012482663,"k
) w2"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3342579750346741,0kI −Lkk2 2D(xl
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3356449375866852,k) + w2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.33703190013869627,"1λmax(Lk,d)kxk,dk2"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.33841886269070737,2 + w2
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.33980582524271846,"2λmax(Lk,u)kxk,uk2"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.34119278779472956,"2, by
162"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.34257975034674065,"triangle inequality. The projections here act as energy sources, and also the objective looks for an
163"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.34396671289875175,xk in the images of Bk+1 and B>
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.34535367545076284,"k , instead of ker(Lk) when xk−1 and xk+1 are not trivial. Thus,
164"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.34674063800277394,"inter-simplicial couplings can potentially mitigate the oversmoothing as well.
165"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.34812760055478503,"Here we show simply generalzing GCN will inherit its oversmoothing to SCs. However, both the
166"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.34951456310679613,"separation of the lower and upper Laplacians and inter-simplicial couplings could potentially mitigate
167"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3509015256588072,"this oversmoothing. We here considered a Dirichlet energy minimization perspective. They can also
168"
A PERSPECTIVE OF SCCNN FROM DIRICHLET ENERGY MINIMIZATION ON SCS,0.3522884882108183,"be explained by means of diffusion process on SCs [36]. We refer to Appendix B.4 for this.
169"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3536754507628294,"4
From convolutional to Hodge-aware
170"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3550624133148405,"In this section, we show how SCCNN, guided by the three principles, performs the Hodge-aware
171"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3564493758668516,"learning, allowing for rational and effective learning on SCs while remaining expressive. To ease the
172"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3578363384188627,"exposition, we ﬁrst provide a more ﬁne-grained spectral view on how SCCNN learns from simplicial
173"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3592233009708738,"data of different variations in the three subspaces based on the simplicial spectral theory [27, 26, 30].
174"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3606102635228849,"Then, we characterize its expressive power and discuss its Hodge-awareness.
175"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.361997226074896,Deﬁnition 4 ([27]). The simplicial Fourier transform (SFT) of xk is ˜xk = U>
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3633841886269071,"k xk where the Fourier
176"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3647711511789182,"basis Uk can be found as the eigenbasis of Lk and the eigenvalues are simplicial frequencies.
177"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.36615811373092927,"Proposition 5 ([26]). The SFT basis can be found as Uk = [Uk,H Uk,G Uk,C] where 1) the zero
178"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.36754507628294036,"eigenspace Uk,H of Lk spans ker(Lk), and an eigenvalue λk,H = 0 is a harmonic frequency; 2) the
179"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.36893203883495146,"nonzero eigenspace Uk,G of Lk,d spans im(B>"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.37031900138696255,"k ), and an eigenvalue λk,G is a gradient frequency,
180"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.37170596393897365,"measuring the lower variation Dd(uk,G); 3) the nonzero eigenspace Uk,C of Lk,u spans im(Bk+1),
181"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.37309292649098474,"and an eigenvalue λk,C is a curl frequency, measuring the upper variation Du(uk,C).
182"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.37447988904299584,"Thus, the SFT of xk can be found as ˜xk = [˜x>"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.37586685159500693,"k,H, ˜x>"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.37725381414701803,"k,G, ˜x>"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3786407766990291,"k,C]>, where each component is the
183"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3800277392510402,"intensity of xk at a simplicial frequency. Consider yk = Hk,dxk,d +Hkxk +Hk,uxk,u in a SCCNN
184"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3814147018030513,"layer. Multiplying on both sides by Uk, we then have the SFT ˜y as
185 8
< :"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3828016643550624,"˜yk,H = ˜hk,H ⊙˜xk,H,
˜yk,G = ˜hk,d ⊙˜xk,d + ˜hk,G ⊙˜xk,G,
˜yk,C = ˜hk,C ⊙˜xk,C + ˜hk,u ⊙˜xk,u, where 8
>
< >
:"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3841886269070735,"˜hk,H = (wk,d,0 + wk,u,0)1,
˜hk,G = PTd"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3855755894590846,"t=0 wk,d,tλ⊙t"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3869625520110957,"k,G + wk,u,01,
˜hk,C = PTu"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3883495145631068,"t=0 wk,u,tλ⊙t"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3897364771151179,"k,C + wk,d,01, (4) G C"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.391123439667129,"G,
G,
G,
G
C,
C,
C,
C"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.3925104022191401,"Hodge Lap. smoothing G,
="
FROM CONVOLUTIONAL TO HODGE-AWARE,0.39389736477115117,"G,
G,
G,
G
G,
G G
G (a) 0 2 4 6"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.39528432732316227,"0
2
4
6 0 1 2"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.39667128987517336,"0
2
4
6
(b) 0 10−3"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.39805825242718446,"10−2
dist node
dist edge
dist tri"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.39944521497919555,"0
10−3
10−2
10−1
100 ϵ0 0 10−3"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.40083217753120665,"10−2
dist node
dist edge
dist tri"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.40221914008321774,"(c)
Figure 1: (a) (top): Independent gradient and curl learning responses. (bottom): Stability-selectivity
tradeoff of SCFs where ˜hG has better stability but smaller selectivity than ˜gG. (b) Information spillage
of nonlinearity. (c) The distance between the perturbed outputs and true when node adjacencies are
perturbed. (top): L = 1, triangle output remains clean. (bottom): L = 2, triangle output is perturbed."
FROM CONVOLUTIONAL TO HODGE-AWARE,0.40360610263522884,is the frequency response of Hk as ˜hk = diag(U>
FROM CONVOLUTIONAL TO HODGE-AWARE,0.40499306518723993,"k HkUk) [30], and ˜hk,d and ˜hk,u, the responses of
186"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.406380027739251,"Hk,d and Hk,u, can be expressed accordingly. This spectral relation (4) shows how the learning of
187"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4077669902912621,"SCCNN is performed at frequencies in different subspaces. Speciﬁcally, the gradient SFT ˜xk,G is
188"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4091539528432732,"learned by a gradient response ˜hk,G, which is independent of the curl response ˜hk,C learning the curl
189"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4105409153952843,"SFT ˜xk,C, and they only coincide at the trivial harmonic frequency, as shown in Fig. 1a. Likewise,
190"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4119278779472954,"the lower and upper projections are independently learned by ˜hk,d and ˜hk,u, respectively.
191"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4133148404993065,"The nonlinearity induces the information spillage that one type of spectra could be spread over other
192"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4147018030513176,"types. That is, σ(˜yk,G) could contain information in harmonic or curl subspaces, as illustrated in
193"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4160887656033287,"Fig. 1b. This is to increase the expressive power of SCCNN, which can be characterized as follows.
194"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4174757281553398,"Theorem 6. A SCCNN layer with inputs xk,d, xk, xk,u is at most expressive as an MLP layer
195 σ(G0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4188626907073509,"k,dxk,d + Gkxk + G0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.420249653259362,"k,uxk,u) with Gk = Gk,d + Gk,u where Gk,d and Gk,u are analytical
196"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.42163661581137307,"matrix functions of Lk,d and Lk,u, respectively, and G0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.42302357836338417,"k,d and G0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.42441054091539526,"k,u likewise. Moreover, this expres-
197"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.42579750346740636,sivity can be achieved when setting Td = T 0
FROM CONVOLUTIONAL TO HODGE-AWARE,0.42718446601941745,"d = nk,G and Tu = T 0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.42857142857142855,"u = nk,C in Eq. (1) with nk,G the
198"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.42995839112343964,"number of distinct gradient frequencies and nk,C the number of distinct curl frequencies.
199"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.43134535367545074,"The proof follows from Cayley-Hamilton theorem [37]. This expressive power can be better under-
200"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.43273231622746183,"stood spectrally. The gradient SFT of xk can be learned most expressively by an analytical function
201"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.43411927877947293,"gk,G(λ), the eigenvalue of Gk,d at a gradient frequency. And the curl SFT of xk can be learned
202"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.435506241331484,"most expressively by another analytical function gk,C(λ), the eigenvalue of Gk,u at a curl frequency.
203"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4368932038834951,"These two functions only need to coincide at harmonic frequency λ = 0. The SFTs of lower and
204"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.43828016643550627,"upper projections can be learned most expressively by two independent functions as well. Given this
205"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.43966712898751736,"expressive power and Eq. (4), we show SCCNN performs the Hodge-aware learning as follows.
206"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.44105409153952846,"Theorem 7. A SCCNN is Hodge-aware in the sense that 1) three Hodge subspaces are invariant under
207"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.44244105409153955,"the learnable SCF Hk, i.e., Hkx 2 im(B>"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.44382801664355065,k ) if x 2 im(B>
FROM CONVOLUTIONAL TO HODGE-AWARE,0.44521497919556174,"k ), and likewise for im(Bk+1), ker(Lk);
208"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.44660194174757284,"2) the gradient and curl spaces are invariant under the learnable lower SCF Hk,d and upper SCF
209"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.44798890429958393,"Hk,u, respectively; 3) the learning in the gradient and curl spaces are independent and expressive.
210"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.44937586685159503,"This theorem essentially shows SCCNN performs expressive learning independently in the gradient
211"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4507628294036061,"and curl subspaces from three inputs while preserving the three subspaces to be invariant w.r.t its
212"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4521497919556172,"learning functions. This allows for the rational and effective learning on SCs. On one hand, the
213"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4535367545076283,"invariance of subspaces under the learnable SCFs substantially shrinks the learning space and makes
214"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4549237170596394,"SCCNN effective, meanwhile, its expressive power is guaranteed by the independent expressive
215"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4563106796116505,"learners, together with the nonlinearity. Instead, the non-Hodge-aware learning, e.g., MLP or
216"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4576976421636616,"simplicial message-passing using MLP to aggregate and update [17], has a much larger learning space
217"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4590846047156727,"which requires more training data for accurate learning, as well as larger computational complexity.
218"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4604715672676838,"On the other hand, simplicial data often presents (implicit or explicit) properties that Hodge subspaces
219"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4618585298196949,"can capture. For example, water ﬂows, trafﬁc ﬂows, electric currents [29, 11] follow ﬂow conservation
220"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.463245492371706,"(div-free, in ker(B1)), or curl-free forex rates, as we show in Section 6, or the gradient component of
221"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4646324549237171,"pairwise comparison data gives consistent global ranking but others are unwanted [24]. SCCNN is
222"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.46601941747572817,"able to capture these characteristics effectively, generating rational outputs due to the invariance of
223"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.46740638002773927,"subspaces and independent learning in gradient and curl spaces. We illustrate a trivial example below.
224"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.46879334257975036,"Example 8. Suppose learning to remove non-div-free noise from some input for ﬂow conservation.
225"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.47018030513176146,"SCCNN can correctly do so because when a not-well-learned SCF, preserving the noise and useful
226"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.47156726768377255,"parts primarily in their own spaces, causes large loss, e.g., mse, the Hodge-awareness restricts it to
227"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.47295423023578365,"suppress in the gradient space and preserve in others. This however can be difﬁcult non-Hodge-aware
228"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.47434119278779474,"learners, e.g., MLP or MPSN [17], especially when the amount of data is limited, because the
229"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.47572815533980584,"non-div-free noise can be disguised as useful by their unbiased transformation into other spaces,
230"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.47711511789181693,"and the useful parts could be transformed into noise space, generating irrational non-div-free output
231"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.478502080443828,"though the overall mse can be small. Thus, simplicial data characteristics can be easily ignored by
232"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4798890429958391,"non-Hodge-aware learners when the invariance condition is not satisﬁed.
233"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4812760055478502,"Comparison to others. We here discuss some other existing learning methods on SCs to emphasize on
234"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4826629680998613,the Hodge-awareness. [15] considered Hk = P
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4840499306518724,i wiLi
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4854368932038835,"k to perform convolutions without uncoupling
235"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4868238557558946,"the lower and upper parts of Lk, which makes it strictly less expressive and non-Hodge-aware,
236"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4882108183079057,"because it cannot perform different learning at frequencies in both gradient and curl spaces, though
237"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4895977808599168,"deeper layers and higher orders can compensate its expressive at other frequencies. [16] applied
238"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.4909847434119279,"Hk with Td = Tu = 1, which has a limited linear learning response. SCCNN returns the methods
239"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.492371705963939,"in [19, 38] when there is no inter-simplicial coupling needed. [14, 20] took the form of simplicial
240"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.49375866851595007,"shifting by generalizing the GCN without uncoupling the two adjacencies, which is not-Hodge-aware.
241"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.49514563106796117,"Spectrally, this gives a limited lower-pass linear spectral response, shown in Fig. 1a.
242"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.49653259361997226,"5
How robust are SCCNNs to domain perturbations?
243"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.49791955617198336,"In practice, a SCCNN is often built on a weighted SC to capture the strengths of simplicial adjacencies
244"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.49930651872399445,"and incidences, with a same form as Eq. (1), except for that the Hodge Laplacians and the projection
245"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5006934812760055,"matrices are weighted, denoted as general operators Rk,d, Rk,u. These matrices are often deﬁned
246"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5020804438280166,"following [29, 39, 40], e.g., [14, 20] considered a particular random walk formulation [41], or can
247"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5034674063800277,"be learned from data, e.g., via an attention method [42, 38]. Since SCCNN relies on the Hodge
248"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5048543689320388,"Laplacians and projection matrices, in this section, we address the question, when these operators
249"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5062413314840499,"are perturbed, how accurate and robust are the outputs of a SCCNN? This models the domain
250"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.507628294036061,"perturbations on the strengths of adjacent and incident relations such as a large weight is applied
251"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5090152565880721,"when two edges are weakly or not adjacent, or data on a node projects on an edge not incident to it.
252"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5104022191400832,"By quantifying this stability, we can explain the robust learning ability of SCCNN. We consider a
253"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5117891816920943,"relative perturbation model, also used to study the stability of CNNs [43–45] and GNNs [46–49].
254"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5131761442441054,"Denote the perturbed lower and upper Laplacians as bLk,d and bLk,u by perturbations Ek,d and Ek,u,
255"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5145631067961165,"and the lower and upper projections as bRk,d and bRk,u by perturbations Jk,d and Jk,u, respectively.
256"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5159500693481276,"Deﬁnition 9 (Relative perturbation). Consider some perturbation matrix E of an appropriate dimen-
257"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5173370319001387,"sion. For a symmetric matrix A, its (relative) perturbed version is bA(E) = A + EA + AE. For a
258"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5187239944521498,"rectangular matrix B, its (relative) perturbed version is bB(E) = B + EB.
259"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5201109570041609,"This relative perturbation model, in contrast to an absolute one [47], quantiﬁes perturbations w.r.t. the
260"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.521497919556172,"local simplicial topology in the sense that weaker connections in a SC are deviated by perturbations
261"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5228848821081831,"proportionally less than stronger connections. We further consider the integral Lipschitz property,
262"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5242718446601942,"extended from [47], to measure the variability of spectral response functions of Hk.
263"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5256588072122053,"Deﬁnition 10. A SCF Hk is integral Lipschitz with constants ck,d, ck,u ≥0 if the derivatives of
264"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5270457697642164,"response functions ˜hk,G(λ) and ˜hk,C(λ) follow that |λ˜h0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5284327323162274,"k,G(λ)| ck,d and |λ˜h0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5298196948682385,"k,C(λ)| ck,u.
265"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5312066574202496,"This property provides a stability-selectivity tradeoff of SCFs independently in gradient and curl
266"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5325936199722607,"frequencies. A spectral response can have both good selectivity and stability in small frequencies
267"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5339805825242718,(a large |˜h0
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5353675450762829,"k,·| for λ ! 0), while in large frequencies, it tends to be ﬂat for better stability at the
268"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.536754507628294,"cost of selectivity (a small variability for large λ), as shown in Fig. 1a. As of the polynomial nature
269"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5381414701803051,"of responses, all SCFs of a SCCNN are integral Lipschitz. We also denote the integral Lipschitz
270"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5395284327323162,"constant for the lower SCFs Hk,d by ck,d and for the upper SCFs Hk,u by ck,u. Given the following
271"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5409153952843273,"reasonable assumptions, we are ready to characterize the stability bound of a SCCNN.
272"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5423023578363384,"Assumption 11. a) The perturbations are small such that kEk,dk2✏k,d, kJk,dk2""k,d, kEk,uk2
273"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5436893203883495,"✏k,u and kJk,uk2""k,u. b) The SCFs Hk of a SCCNN have a normalized bounded frequency
274"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5450762829403606,"response (for simplicity, though unnecessary), likewise for Hk,d and Hk,u. c) The lower and upper
275"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5464632454923717,"projections are ﬁnite kRk,dk2 rk,d and kRk,uk2 rk,u. d) The nonlinearity σ(·) is cσ-Lipschitz
276"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5478502080443828,"(e.g., relu, tanh, sigmoid). e) The initial input x0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5492371705963939,"k, for all k, is ﬁnite, kx0"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.550624133148405,"kk2 [β]k, .
277"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5520110957004161,Theorem 12. Let xL
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5533980582524272,k be the k-simplicial output of an L-layer SCCNN on a weighted SC. Let ˆxL
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5547850208044383,"k be
278"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5561719833564494,"the output of the same SCCNN but on a relatively perturbed SC. Under Assumption 11, the Euclidean
279"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5575589459084604,distance between the two outputs is ﬁnite and upper-bounded kˆxL k −xL
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5589459084604715,"k k2 [d]k where
280"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5603328710124826,"d = cL σ L
X l=1"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5617198335644937,"bZl−1TZL−lβ, with, e.g., T ="
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5631067961165048,""" t0
t0,u
t1,d
t1
t1,u
t2,d
t2 # Z ="
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5644937586685159,""" 1
r0,u
r1,d
1
r1,u
r2,d
1 # ,
(5)"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.565880721220527,"for K = 2, which are tridiagonal, and bZ is deﬁned as Z but with off-diagonal entries ˆrk,d =
281"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5672676837725381,"rk,d(1+""k,d) and ˆrk,u = rk,u(1+""k,u). Diagonal entries of T are tk = ck,d∆k,d✏k,d+ck,u∆k,u✏k,u,
282"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5686546463245492,"and off-diagonals are tk,d = rk,d""k,d + ck,d∆k,d✏k,drk,d and tk,u = rk,u""k,u + ck,u∆k,u✏k,urk,u,
283"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5700416088765603,"where ∆k,d captures the eigenvector misalignment between Lk,d and perturbation Ek,d with a factor
284
pnk, and likewise for ∆k,u.
285"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5714285714285714,"This result bounds the outputs of a SCCNN on all simplicial levels, showing they are stable to
286"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5728155339805825,"small perturbations on the strengths of simplicial adjacencies and incidences. Speciﬁcally, we make
287"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5742024965325936,"two observations from the seemingly complicated expression. 1) The stability bound depends on
288"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5755894590846047,"i) the degree of perturbations including their magnitude ✏and "", and eigenspace misalignment ∆,
289"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5769764216366158,"ii) the integral Lipschitz properties of SCFs, and iii) the degree of projections r. 2) The stability of
290"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5783633841886269,"k-output depends on factors of not only k-simplices, but also simplices of adjacent orders due to
291"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.579750346740638,"inter-simplicial couplings. When L = 1, node output bound d0 depends on factors in the node space,
292"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5811373092926491,"as well as the edge space factored by the projection degree, and vice versa for edge output. As the
293"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5825242718446602,"layer deepens, this mutual dependence expands further. When L = 2, factors in the triangle space
294"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5839112343966713,"also affect the stability of node output d0, and vice versa for triangle output, as observed in Fig. 1c.
295"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5852981969486823,"More importantly, this stability provides practical implications for learning on SCs. While accounting
296"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5866851595006934,"for inter-simplicial couplings may be beneﬁcial, it does not help with the stability of SCCNNs when
297"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5880721220527045,"the number of layers increases due to the mutual dependence between different outputs. Thus, to
298"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5894590846047156,"maintain the expressive power, higher-order SCFs can be used in exchange for shallow layers. This
299"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5908460471567267,"does not harm the stability because, ﬁrst, the components of high-frequency can be spread over the
300"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5922330097087378,"low frequency due to the nonlinearity where the spectral responses are more selective without losing
301"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5936199722607489,"the stability; and second, higher-order SCFs are easier to be learned with smaller integral Lipschitz
302"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.59500693481276,"constants than lower-order ones, thus, better stability. The latter can be easily seen by comparing
303"
FROM CONVOLUTIONAL TO HODGE-AWARE,0.5963938973647711,"one-order and two-order cases. We also experimentally show this in Fig. 4.
304"
EXPERIMENTS,0.5977808599167822,"6
Experiments
305"
EXPERIMENTS,0.5991678224687933,"1
10
100
Layer 10−23 10−19 10−15 10−11 10−7 10−3 101 D(xk)"
EXPERIMENTS,0.6005547850208044,"shift,node
with proj.,node
shift,edge
with proj.,edge
uncouple lower/upper,edge
shift,tri.
with proj.,tri."
EXPERIMENTS,0.6019417475728155,Figure 2
EXPERIMENTS,0.6033287101248266,"Synthetic. We ﬁrst illustrate the evolution of Dirichlet energies
306"
EXPERIMENTS,0.6047156726768377,"of outputs on nodes, edges and triangles of a SC of order two by
307"
EXPERIMENTS,0.6061026352288488,"numbers of simplicial shifting layers with σ = tanh. The inputs
308"
EXPERIMENTS,0.6074895977808599,"on them are randomly sampled from U([−5, 5]). Fig. 2 shows
309"
EXPERIMENTS,0.608876560332871,"simply generalizing GCN on SCs could lead to oversmoothing
310"
EXPERIMENTS,0.6102635228848821,"on simplices of all orders. However, uncoupling the lower and
311"
EXPERIMENTS,0.6116504854368932,"upper parts of L1 by setting, e.g., γ = 2 could mitigate the
312"
EXPERIMENTS,0.6130374479889042,"oversmoothing on edges. Lastly, the inter-simplicial coupling
313"
EXPERIMENTS,0.6144244105409153,"could almost prevent the oversmoothing.
314"
EXPERIMENTS,0.6158113730929264,"Foreign currency exchange. In forex problems, for any currencies i, j, k, the arbitray-free condition
315"
EXPERIMENTS,0.6171983356449375,"of a fair market reads as ri/jrj/k = ri/k with the exchange rate ri/j between i and j. That is, the
316"
EXPERIMENTS,0.6185852981969486,"exchange path i ! j ! k provides no proﬁt or loss over a direct exchange i ! k. By modeling the
317"
EXPERIMENTS,0.6199722607489597,"forex as a SC of order two and the exchange rates as edge ﬂows [x1][i,j] = log(ri/j), this condition
318"
EXPERIMENTS,0.6213592233009708,"translates as x1 is curl-free, i.e., [x1][i,j] + [x1][j,k] −[x1][i,k] = 0 in any triangle [i, j, k] [24]. Here
319"
EXPERIMENTS,0.6227461858529819,"we consider a real-world forex market from [50] at three timestamps, which contains certain degree
320"
EXPERIMENTS,0.624133148404993,"of arbitrage. We artiﬁcially added some random noise and “curl noise” (only in the curl space) to
321"
EXPERIMENTS,0.6255201109570042,"this market, in which we aim to recover the forex rates. We also randomly masked 50% of the rates,
322"
EXPERIMENTS,0.6269070735090153,"where we aim to interpolate the market such that it is arbitrage-free. Three settings create three types
323"
EXPERIMENTS,0.6282940360610264,"of learning needs. To evaluate the performance, we measure both normalized mse and total arbitrage
324"
EXPERIMENTS,0.6296809986130375,"(total curl), both equally important for the goal of creating a fair market by small price ﬂuctuations.
325"
EXPERIMENTS,0.6310679611650486,"From Table 1, we make the following observations. 1) MPSN [17] fails at this task: although it
326"
EXPERIMENTS,0.6324549237170597,"can reduce nmse, it outputs unfair rates with large arbitrage, which is against the forex principle,
327"
EXPERIMENTS,0.6338418862690708,"because it is not Hodge-aware, unable to capture the arbitrage-free property with small amount of
328"
EXPERIMENTS,0.6352288488210819,"data. 2) SNN [15] fails too: as discussed in Section 4, it restricts the gradient and curl spaces to be
329"
EXPERIMENTS,0.636615811373093,"always learned in the same fashion, unable to meet the need of disjoint learning of this task in two
330"
EXPERIMENTS,0.6380027739251041,Table 1: Forex results (nmse|total arbitrage).
EXPERIMENTS,0.6393897364771152,"Methods
Random Noise
Curl Noise
Interpolation"
EXPERIMENTS,0.6407766990291263,"Input
.119±.004|29.19±.874
.552±.027|122.4±5.90
.717±.030|106.4±.902
Baseline
.036±.005|2.29±.079
.050±.002|11.12±.537
.534±.043|9.67±.082
SNN [15]
.110±.005|23.24±1.03
.446±.017|86.95±2.20
.702±.033|104.74±1.04
PSNN [16]
.008±.001|.984±.170
.000±.000|.000±.000
.009±.001|1.13±.329
MPSN [17]
.039±.004|7.74±0.88
.076±.012|14.92±2.49
.117±.063|23.15±11.7
SCCNN, id
.027±.005|.000±.000
.000±.000|.000±.000
.265±.036|.000±.000
SCCNN, tanh
.002±.000|.325±.082
.000±.000|.003±.003
.003±.002|.279±.151"
EXPERIMENTS,0.6421636615811374,Table 2: Simplex prediction.
EXPERIMENTS,0.6435506241331485,"Methods
2-simplex
3-simplex"
EXPERIMENTS,0.6449375866851595,"Mean [2]
62.8±2.7
63.6±1.6
MLP
68.5±1.6
69.0±2.2
GNN [51]
93.9±1.0
96.6±0.5
SNN [15]
92.0±1.8
95.1±1.2
PSNN [16]
95.6±1.3
98.1±0.5
SCNN [19]
96.5±1.5
98.3±0.4
Bunch [14]
98.3±0.5
98.5±0.5
MPSN [17]
98.1±0.5
99.2±0.3
SCCNN
98.7±0.5
99.4±0.3"
EXPERIMENTS,0.6463245492371706,Table 3: Ablation study.
EXPERIMENTS,0.6477115117891817,"Missing
2-Simplex
Param."
EXPERIMENTS,0.6490984743411928,"—
98.7±0.5
L = 2
Edge-to-Node
93.9±1.0
L = 5
Node-to-Node
98.7±0.4
L = 4
Edge-to-Edge
98.5±1.0
L = 3
Node-to-Edge
98.8±0.3
L = 4"
EXPERIMENTS,0.6504854368932039,"Node input
98.2±0.5
T = 4
Edge input
98.1±0.4
T = 3
0
10−3
10−2
10−1
100 ϵ 0 10−3 10−2 10−1 100 101 102 kxL"
EXPERIMENTS,0.651872399445215,"k −ˆxL
k k"
EXPERIMENTS,0.6532593619972261,"Node
Edge
Tri."
EXPERIMENTS,0.6546463245492372,"Node thm.
Edge thm.
Tri. thm."
EXPERIMENTS,0.6560332871012483,Figure 3: Stability bound. 0.4 0.6
EXPERIMENTS,0.6574202496532594,accuracy
EXPERIMENTS,0.6588072122052705,"T = 1
T = 3
T = 5"
EXPERIMENTS,0.6601941747572816,"0.00
0.25
0.50
0.75
1.00
ϵ 0 1 2"
EXPERIMENTS,0.6615811373092927,kx1 −ˆx1k/kx1k
EXPERIMENTS,0.6629680998613038,"T = 1
T = 3
T = 5"
EXPERIMENTS,0.6643550624133149,Figure 4: Stability as T increases.
EXPERIMENTS,0.665742024965326,"spaces. 3) PSNN [16] can reconstruct relatively fair forex rates with small nmse. In the curl noise
331"
EXPERIMENTS,0.6671289875173371,"case, the reconstruction is perfect, while in the other two cases, the nmse and arbitrage are three times
332"
EXPERIMENTS,0.6685159500693482,"larger than SCCNN due to its limited linear learning responses. 4) SCCNN performs the best in both
333"
EXPERIMENTS,0.6699029126213593,"reducing the total error and the total arbitrage. We also notice that with σ = id, the arbitrage-free rule
334"
EXPERIMENTS,0.6712898751733704,"is fully learned by SCCNN. However, it has relatively larger errors in the random and interpolation
335"
EXPERIMENTS,0.6726768377253814,"cases due to its limited linear expressive power. With σ = tanh, SCCNN can tackle these more
336"
EXPERIMENTS,0.6740638002773925,"challenging cases, ﬁnding a good compromise between overall error and data characteristics.
337"
EXPERIMENTS,0.6754507628294036,"Simplex Prediction. We then test SCCNN on simplex prediction task which is an extension of link
338"
EXPERIMENTS,0.6768377253814147,"prediction in graphs [52]. Our approach is to ﬁrst learn the features of lower-order simplices and
339"
EXPERIMENTS,0.6782246879334258,"then use an MLP to identify if a simplex is closed or open. We built a SC as [15] on a coauthorship
340"
EXPERIMENTS,0.6796116504854369,"dataset [53] where nodes are authors and collaborations of k-authors are (k −1)-simplices. The input
341"
EXPERIMENTS,0.680998613037448,"simplicial data is the number of citations, e.g., x1 and x2 are those of dyadic and triadic collaborations,
342"
EXPERIMENTS,0.6823855755894591,"which does not present explicit properties like forex rates. Thus, 2-simplex (3-simplex) prediction
343"
EXPERIMENTS,0.6837725381414702,"amounts to predict triadic (tetradic) collaborations. From the AUC results in Table 2, we make
344"
EXPERIMENTS,0.6851595006934813,"three observations. 1) SCCNN, MPSN and Bunch [14] methods outperform the rest due to the
345"
EXPERIMENTS,0.6865464632454924,"inter-simplicial couplings. 2) Uncoupling the lower and upper parts in Lk imrpoves the feature
346"
EXPERIMENTS,0.6879334257975035,"learning (SCNN [19] better than SNN). 3) Higher-order convolution further improves the prediction
347"
EXPERIMENTS,0.6893203883495146,"(SCNN better than PSNN, SCCNN better than Bunch). Note that MPSN has three times more
348"
EXPERIMENTS,0.6907073509015257,"parameters than SCCNN under the settings of the best results.
349"
EXPERIMENTS,0.6920943134535368,"Ablation study. Table 3 reports the results of SCCNN when certain simplicial relation is missing,
350"
EXPERIMENTS,0.6934812760055479,"which helps understand their roles. When not considering the edge-to-node incidence, it (when using
351"
EXPERIMENTS,0.694868238557559,"node features) is equivalent to GNN with poor performance. When removing other adjacencies or
352"
EXPERIMENTS,0.6962552011095701,"incidences, the best performance remains but with an increase of model complexity, more layers
353"
EXPERIMENTS,0.6976421636615812,"required. This, however, is not preferred, because the stability decreases as the model deepens and
354"
EXPERIMENTS,0.6990291262135923,"becomes inﬂuenced by factors in other simplicial space, as shown in Fig. 1c. We also considered the
355"
EXPERIMENTS,0.7004160887656034,"case with limited input, e.g., when the input on nodes or on edges is missing. The best performance
356"
EXPERIMENTS,0.7018030513176144,"of SCCNN only slightly drops with an increase of convolution order, compared to before T = 2.
357"
EXPERIMENTS,0.7031900138696255,Table 4: Trajectory prediction.
EXPERIMENTS,0.7045769764216366,"Methods
Synthetic
Ocean drifts"
EXPERIMENTS,0.7059639389736477,"SNN [15]
65.5±2.4
52.5±6.0
PSNN [16]
63.1±3.1
49.0±8.0
SCNN [19]
67.7±1.7
53.0±7.8
Bunch [14]
62.3±4.0
46.0±6.2
SCCNN
65.2±4.1
54.5±7.9"
EXPERIMENTS,0.7073509015256588,"How tight is the stability bound? We consider the perturbations
358"
EXPERIMENTS,0.7087378640776699,"which relatively shift the eigenvalues of Hodge Laplacians and
359"
EXPERIMENTS,0.710124826629681,"the singular values of projection matrices by ✏. We compare the
360"
EXPERIMENTS,0.7115117891816921,"bound in Eq. (5) with experimental distance on each simplex level.
361"
EXPERIMENTS,0.7128987517337032,"Fig. 3 shows the bound becomes tighter as perturbation increases.
362"
EXPERIMENTS,0.7142857142857143,"Trajectory prediction. We lastly test on predicting trajectories in
363"
EXPERIMENTS,0.7156726768377254,"a synthetic SC and of ocean drifters from [41], introduced by [16].
364"
EXPERIMENTS,0.7170596393897365,"From Table 4 we ﬁrst observe SCCNN and Bunch with inter-simplicial couplings do not perform
365"
EXPERIMENTS,0.7184466019417476,"better than those without. This is because zero inputs are applied on nodes and triangles [16], which
366"
EXPERIMENTS,0.7198335644937587,"makes inter-couplings inconsequential. Secondly, using higher-order convolutions improves the
367"
EXPERIMENTS,0.7212205270457698,"average accuracy in both datasets (SCNN better than PSNN on average, SCCNN better than Bunch).
368"
EXPERIMENTS,0.7226074895977809,"Note that the prediction here aims to ﬁnd a candidate from the neighborhood of end node, which
369"
EXPERIMENTS,0.723994452149792,"depends on the node degree. Since the average node degree of the synthetic SC is 5.24 and that in
370"
EXPERIMENTS,0.7253814147018031,"ocean drifter data is 4.81, a random guess has around 20% accuracy. The high standard derivations
371"
EXPERIMENTS,0.7267683772538142,"could come from the limited ocean drifter dataset.
372"
EXPERIMENTS,0.7281553398058253,"Convolution orders on stability. We also show that NNs with higher-order SCFs have more potential
373"
EXPERIMENTS,0.7295423023578363,"to learn better integral Lipschitz properties, thus, better stability. We consider SCNNs [19] with
374"
EXPERIMENTS,0.7309292649098474,"orders Td = Tu = 1, 3, 5 and train them with a regularizer to reduce the integral Lipschitz constants.
375"
EXPERIMENTS,0.7323162274618585,"As shown in Fig. 4, the higher-order case has a smaller distance (better stability) between the outputs
376"
EXPERIMENTS,0.7337031900138696,"without and with perturbations, with consistent better accuracy, comapred to the lower-order case.
377"
EXPERIMENTS,0.7350901525658807,"7
Related Work, Discussion and Conclusion
378"
EXPERIMENTS,0.7364771151178918,"Related work mainly concerns learning methods on SCs. [13] ﬁrst used L1,d to build NNs on edges in
379"
EXPERIMENTS,0.7378640776699029,"a graph setting without the upper edge adjacency. [15] then generalized convolutional GNNs [32, 51]
380"
EXPERIMENTS,0.739251040221914,"to simplices by using the Hodge Laplacian. [16, 19] instead uncoupled the lower and upper Laplacians
381"
EXPERIMENTS,0.7406380027739251,"to perform one- and multi-order convolutions, to which [42, 38, 54] added attention schemes. [55]
382"
EXPERIMENTS,0.7420249653259362,"considered a varaint of [16] to identity topological holes and [18] combined shifting on nodes and
383"
EXPERIMENTS,0.7434119278779473,"edges for link prediction. Above works learned within a simplicial level and did not consider the
384"
EXPERIMENTS,0.7447988904299584,"incidence relations (inter-simplicial couplings) in SCs, which was included by [14, 20]. These works
385"
EXPERIMENTS,0.7461858529819695,"considered convolutional-type methods, which can be subsumed by SCCNN. Meanwhile, [17, 21]
386"
EXPERIMENTS,0.7475728155339806,"generalized the message passing on graphs [22] to SCs, relying on both adjacencies and incidences.
387"
EXPERIMENTS,0.7489597780859917,"Most of these works focused on extending GNNs to SCs by varying the information propagation
388"
EXPERIMENTS,0.7503467406380028,"on SCs without many theoretical insights into their components. Among them, [16] discussed the
389"
EXPERIMENTS,0.7517337031900139,"equivariance of PSNN to permutation and orientation, which SCCNN admits as well. [17] studied
390"
EXPERIMENTS,0.753120665742025,"the messgae-passing on SCs in terms of WL test of SCs built by completing cliques in a graph. The
391"
EXPERIMENTS,0.7545076282940361,"more closely related work [19] gave only a spectral formulation based on SCFs.
392"
EXPERIMENTS,0.7558945908460472,"Discussion. In our opinion, the advantage of using SCs is not only about them being able to model
393"
EXPERIMENTS,0.7572815533980582,"higher-order network structure, but also support simplicial data, which can be both human-generated
394"
EXPERIMENTS,0.7586685159500693,"data like coauthorship, and physical data like ﬂow-typed data. This is why we approcahed the analysis
395"
EXPERIMENTS,0.7600554785020804,"from the perspectives of both simplicial structures and the simplicial data, i.e., the Hodge theory
396"
EXPERIMENTS,0.7614424410540915,"and spectral simplicial theory [23, 9, 26–28, 30, 56]. We provided deeper insights into why three
397"
EXPERIMENTS,0.7628294036061026,"principles are needed and how they can guide the effective and rational learning from simplicial data.
398"
EXPERIMENTS,0.7642163661581137,"As what we practically found, in experiments where data exhibits properties characterized by the
399"
EXPERIMENTS,0.7656033287101248,"Hodge decomposition, SCCNN performs well due to the Hodge-awareness while non-Hodge-aware
400"
EXPERIMENTS,0.7669902912621359,"learners can fail at giving rational results. In cases where data does not possess such properties,
401"
EXPERIMENTS,0.768377253814147,"SCCNN has better or comparable performance than the ones which violate or do not respect the three
402"
EXPERIMENTS,0.7697642163661581,"principles. This also shows the advantages of SCCNN, especially when data has certain properties.
403"
EXPERIMENTS,0.7711511789181692,"Concurrently, there are works on more general cell complexes, e.g., [57–61], where 2-cells inlcude
404"
EXPERIMENTS,0.7725381414701803,"not only triangles, but also general polygon faces. We focus on SCs because a regular CW complex
405"
EXPERIMENTS,0.7739251040221914,"can be subdivided into a SC [62, 29] to which the analysis in this paper applies, or we can generalize
406"
EXPERIMENTS,0.7753120665742025,"our analysis by allowing B2 to include 2-cells. This is however informal and does not exploit the
407"
EXPERIMENTS,0.7766990291262136,"power of cell complexes, which lies on cellular sheaves, as studied in [63, 64].
408"
EXPERIMENTS,0.7780859916782247,"Limitation. A major limitation of our method is that it cannot learn differently from features at the
409"
EXPERIMENTS,0.7794729542302358,"frequencies of the same type and the same value. For instance, harmonic features are learned in a
410"
EXPERIMENTS,0.7808599167822469,"same fashion because they all have zero frequency. This is however common in convolutional type
411"
EXPERIMENTS,0.782246879334258,"learning methods on both graphs and SCs. Also, our stability analysis concerns the perturbations on
412"
EXPERIMENTS,0.7836338418862691,"the connection strengths and did not consider the case where simplices join or disappear. Both of
413"
EXPERIMENTS,0.7850208044382802,"them can be interesting future directions, together with more physical-based data applications.
414"
EXPERIMENTS,0.7864077669902912,"Conclusion. We proposed three principles for convolutional learning on SCs, summarized in a
415"
EXPERIMENTS,0.7877947295423023,"general architecture, SCCNN. Our analysis showed this architecture, guided by the three principles,
416"
EXPERIMENTS,0.7891816920943134,"demonstrates an awareness of the Hodge decomposition and performs rational, effective and expres-
417"
EXPERIMENTS,0.7905686546463245,"sive learning from simplicial data. Furthermore, our study reveals that SCCNN exhibits stability
418"
EXPERIMENTS,0.7919556171983356,"and robustness against perturbations in the strengths of simplicial connections. Experimental results
419"
EXPERIMENTS,0.7933425797503467,"validate the beneﬁts of respecting the three principles and the Hodge-awareness. Overall, our work
420"
EXPERIMENTS,0.7947295423023578,"establishes a solid fundation for learning on SCs, highlighting the importance of the Hodge theory.
421"
REFERENCES,0.7961165048543689,"References
422"
REFERENCES,0.79750346740638,"[1] Mark EJ Newman, Duncan J Watts, and Steven H Strogatz. Random graph models of social networks.
423"
REFERENCES,0.7988904299583911,"Proceedings of the national academy of sciences, 99(suppl_1):2566–2572, 2002.
424"
REFERENCES,0.8002773925104022,"[2] Austin R Benson, Rediet Abebe, Michael T Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial
425"
REFERENCES,0.8016643550624133,"closure and higher-order link prediction. Proceedings of the National Academy of Sciences, 115(48):
426"
REFERENCES,0.8030513176144244,"E11221–E11230, 2018.
427"
REFERENCES,0.8044382801664355,"[3] Hosein Masoomy, Behrouz Askari, Samin Tajik, Abbas K Rizi, and G Reza Jafari. Topological analysis of
428"
REFERENCES,0.8058252427184466,"interaction patterns in cancer-speciﬁc gene regulatory network: persistent homology approach. Scientiﬁc
429"
REFERENCES,0.8072122052704577,"Reports, 11(1):1–11, 2021.
430"
REFERENCES,0.8085991678224688,"[4] Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas, Alice Patania, Jean-
431"
REFERENCES,0.8099861303744799,"Gabriel Young, and Giovanni Petri. Networks beyond pairwise interactions: structure and dynamics.
432"
REFERENCES,0.811373092926491,"Physics Reports, 874:1–92, 2020.
433"
REFERENCES,0.812760055478502,"[5] Austin R Benson, David F Gleich, and Desmond J Higham. Higher-order network analysis takes off, fueled
434"
REFERENCES,0.8141470180305131,"by classical ideas and new data. arXiv preprint arXiv:2103.05031, 2021.
435"
REFERENCES,0.8155339805825242,"[6] Leo Torres, Ann S Blevins, Danielle Bassett, and Tina Eliassi-Rad. The why, how, and when of representa-
436"
REFERENCES,0.8169209431345353,"tions for complex systems. SIAM Review, 63(3):435–485, 2021.
437"
REFERENCES,0.8183079056865464,"[7] Christian Bick, Elizabeth Gross, Heather A Harrington, and Michael T Schaub. What are higher-order
438"
REFERENCES,0.8196948682385575,"networks? arXiv preprint arXiv:2104.11329, 2021.
439"
REFERENCES,0.8210818307905686,"[8] James R Munkres. Elements of algebraic topology. CRC press, 2018.
440"
REFERENCES,0.8224687933425797,"[9] Lek-Heng Lim. Hodge laplacians on graphs. SIAM Review, 62(3):685–715, 2020.
441"
REFERENCES,0.8238557558945908,"[10] Rohan Money, Joshin Krishnan, Baltasar Beferull-Lozano, and Elvin Isuﬁ. Online edge ﬂow imputation
442"
REFERENCES,0.8252427184466019,"on networks. IEEE Signal Processing Letters, 2022.
443"
REFERENCES,0.826629680998613,"[11] Junteng Jia, Michael T Schaub, Santiago Segarra, and Austin R Benson. Graph-based semi-supervised &
444"
REFERENCES,0.8280166435506241,"active learning for edge ﬂows. In Proceedings of the 25th ACM SIGKDD International Conference on
445"
REFERENCES,0.8294036061026352,"Knowledge Discovery & Data Mining, pages 761–771, 2019.
446"
REFERENCES,0.8307905686546463,"[12] D Vijay Anand, Soumya Das, and Moo K Chung. Hodge-decomposition of brain networks. arXiv preprint
447"
REFERENCES,0.8321775312066574,"arXiv:2211.10542, 2022.
448"
REFERENCES,0.8335644937586685,"[13] T Mitchell Roddenberry and Santiago Segarra. Hodgenet: Graph neural networks for edge data. In 2019
449"
REFERENCES,0.8349514563106796,"53rd Asilomar Conference on Signals, Systems, and Computers, pages 220–224. IEEE, 2019.
450"
REFERENCES,0.8363384188626907,"[14] Eric Bunch, Qian You, Glenn Fung, and Vikas Singh. Simplicial 2-complex convolutional neural networks.
451"
REFERENCES,0.8377253814147018,"In TDA & Beyond, 2020. URL https://openreview.net/forum?id=TLbnsKrt6J-.
452"
REFERENCES,0.8391123439667129,"[15] Stefania Ebli, Michaël Defferrard, and Gard Spreemann. Simplicial neural networks. In NeurIPS 2020
453"
REFERENCES,0.840499306518724,"Workshop on Topological Data Analysis and Beyond, 2020.
454"
REFERENCES,0.841886269070735,"[16] T Mitchell Roddenberry, Nicholas Glaze, and Santiago Segarra. Principled simplicial neural networks for
455"
REFERENCES,0.8432732316227461,"trajectory prediction. In International Conference on Machine Learning, pages 9020–9029. PMLR, 2021.
456"
REFERENCES,0.8446601941747572,"[17] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F Montufar, Pietro Lio, and Michael
457"
REFERENCES,0.8460471567267683,"Bronstein. Weisfeiler and lehman go topological: Message passing simplicial networks. In International
458"
REFERENCES,0.8474341192787794,"Conference on Machine Learning, pages 1026–1037. PMLR, 2021.
459"
REFERENCES,0.8488210818307905,"[18] Yuzhou Chen, Yulia R. Gel, and H. Vincent Poor. Bscnets: Block simplicial complex neural networks.
460"
REFERENCES,0.8502080443828016,"Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 36(6):6333–6341, 2022. doi: 10.1609/aaai.
461"
REFERENCES,0.8515950069348127,"v36i6.20583. URL https://ojs.aaai.org/index.php/AAAI/article/view/20583.
462"
REFERENCES,0.8529819694868238,"[19] Maosheng Yang, Elvin Isuﬁ, and Geert Leus. Simplicial convolutional neural networks. In ICASSP
463"
REFERENCES,0.8543689320388349,"2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
464"
REFERENCES,0.855755894590846,"8847–8851, 2022. doi: 10.1109/ICASSP43922.2022.9746017.
465"
REFERENCES,0.8571428571428571,"[20] Ruochen Yang, Frederic Sala, and Paul Bogdan. Efﬁcient representation learning for higher-order data with
466"
REFERENCES,0.8585298196948682,"simplicial complexes. In The First Learning on Graphs Conference, 2022. URL https://openreview.
467"
REFERENCES,0.8599167822468793,"net/forum?id=nGqJY4DODN.
468"
REFERENCES,0.8613037447988904,"[21] Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Vasileios Maroulas, and Xuanting Cai. Simplicial
469"
REFERENCES,0.8626907073509015,"complex representation learning. arXiv preprint arXiv:2103.04046, 2021.
470"
REFERENCES,0.8640776699029126,"[22] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?
471"
REFERENCES,0.8654646324549237,"arXiv preprint arXiv:1810.00826, 2018.
472"
REFERENCES,0.8668515950069348,"[23] William Vallance Douglas Hodge. The theory and applications of harmonic integrals. CUP Archive, 1989.
473"
REFERENCES,0.8682385575589459,"[24] Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, and Yinyu Ye. Statistical ranking and combinatorial hodge theory.
474"
REFERENCES,0.869625520110957,"Mathematical Programming, 127(1):203–244, 2011.
475"
REFERENCES,0.871012482662968,"[25] Ozan Candogan, Ishai Menache, Asuman Ozdaglar, and Pablo A Parrilo. Flows and decompositions of
476"
REFERENCES,0.8723994452149791,"games: Harmonic and potential games. Mathematics of Operations Research, 36(3):474–503, 2011.
477"
REFERENCES,0.8737864077669902,"[26] Maosheng Yang, Elvin Isuﬁ, Michael T. Schaub, and Geert Leus. Finite Impulse Response Filters
478"
REFERENCES,0.8751733703190014,"for Simplicial Complexes. In 2021 29th European Signal Processing Conference (EUSIPCO), pages
479"
REFERENCES,0.8765603328710125,"2005–2009, August 2021. doi: 10.23919/EUSIPCO54536.2021.9616185. ISSN: 2076-1465.
480"
REFERENCES,0.8779472954230236,"[27] Sergio Barbarossa and Stefania Sardellitti. Topological signal processing over simplicial complexes. IEEE
481"
REFERENCES,0.8793342579750347,"Transactions on Signal Processing, 68:2992–3007, 2020.
482"
REFERENCES,0.8807212205270458,"[28] John Steenbergen. Towards a spectral theory for simplicial complexes. PhD thesis, Duke University, 2013.
483"
REFERENCES,0.8821081830790569,"[29] Leo J Grady and Jonathan R Polimeni. Discrete calculus: Applied analysis on graphs for computational
484"
REFERENCES,0.883495145631068,"science, volume 3. Springer, 2010.
485"
REFERENCES,0.8848821081830791,"[30] Maosheng Yang, Elvin Isuﬁ, Michael T. Schaub, and Geert Leus. Simplicial convolutional ﬁlters. IEEE
486"
REFERENCES,0.8862690707350902,"Transactions on Signal Processing, 70:4633–4648, 2022. doi: 10.1109/TSP.2022.3207045.
487"
REFERENCES,0.8876560332871013,"[31] Michael T Schaub, Yu Zhu, Jean-Baptiste Seby, T Mitchell Roddenberry, and Santiago Segarra. Signal
488"
REFERENCES,0.8890429958391124,"processing on higher-order networks: Livin’on the edge... and beyond. Signal Processing, 187:108149,
489"
REFERENCES,0.8904299583911235,"2021.
490"
REFERENCES,0.8918169209431346,"[32] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In
491"
REFERENCES,0.8932038834951457,"International Conference on Learning Representations (ICLR), 2017.
492"
REFERENCES,0.8945908460471568,"[33] Chen Cai and Yusu Wang.
A note on over-smoothing for graph neural networks.
arXiv preprint
493"
REFERENCES,0.8959778085991679,"arXiv:2006.13318, 2020.
494"
REFERENCES,0.897364771151179,"[34] T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph
495"
REFERENCES,0.8987517337031901,"neural networks. arXiv preprint arXiv:2303.10993, 2023.
496"
REFERENCES,0.9001386962552012,"[35] Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ﬁlters. arXiv
497"
REFERENCES,0.9015256588072122,"preprint arXiv:1905.09550, 2019.
498"
REFERENCES,0.9029126213592233,"[36] Cameron Ziegler, Per Sebastian Skardal, Haimonti Dutta, and Dane Taylor. Balanced hodge laplacians
499"
REFERENCES,0.9042995839112344,"optimize consensus dynamics over simplicial complexes. Chaos: An Interdisciplinary Journal of Nonlinear
500"
REFERENCES,0.9056865464632455,"Science, 32(2):023128, 2022.
501"
REFERENCES,0.9070735090152566,"[37] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
502"
REFERENCES,0.9084604715672677,"[38] Lorenzo Giusti, Claudio Battiloro, Paolo Di Lorenzo, Stefania Sardellitti, and Sergio Barbarossa. Simplicial
503"
REFERENCES,0.9098474341192788,"attention networks. arXiv preprint arXiv:2203.07485, 2022.
504"
REFERENCES,0.9112343966712899,"[39] Danijela Horak and Jürgen Jost. Spectra of combinatorial laplace operators on simplicial complexes.
505"
REFERENCES,0.912621359223301,"Advances in Mathematics, 244:303–336, 2013.
506"
REFERENCES,0.9140083217753121,"[40] Nicola Guglielmi, Anton Savostianov, and Francesco Tudisco. Quantifying the structural stability of
507"
REFERENCES,0.9153952843273232,"simplicial homology. arXiv preprint arXiv:2301.03627, 2023.
508"
REFERENCES,0.9167822468793343,"[41] Michael T Schaub, Austin R Benson, Paul Horn, Gabor Lippner, and Ali Jadbabaie. Random walks on
509"
REFERENCES,0.9181692094313454,"simplicial complexes and the normalized hodge 1-laplacian. SIAM Review, 62(2):353–391, 2020.
510"
REFERENCES,0.9195561719833565,"[42] Christopher Wei Jin Goh, Cristian Bodnar, and Pietro Lio. Simplicial attention networks. In ICLR 2022
511"
REFERENCES,0.9209431345353676,"Workshop on Geometrical and Topological Representation Learning, 2022.
512"
REFERENCES,0.9223300970873787,"[43] Joan Bruna and Stéphane Mallat. Invariant scattering convolution networks. IEEE transactions on pattern
513"
REFERENCES,0.9237170596393898,"analysis and machine intelligence, 35(8):1872–1886, 2013.
514"
REFERENCES,0.9251040221914009,"[44] Qiang Qiu, Xiuyuan Cheng, Guillermo Sapiro, et al. Dcfnet: Deep neural network with decomposed
515"
REFERENCES,0.926490984743412,"convolutional ﬁlters. In International Conference on Machine Learning, pages 4198–4207. PMLR, 2018.
516"
REFERENCES,0.9278779472954231,"[45] Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of deep
517"
REFERENCES,0.9292649098474342,"convolutional representations. arXiv preprint arXiv:1706.03078, 2017.
518"
REFERENCES,0.9306518723994452,"[46] Fernando Gama, Alejandro Ribeiro, and Joan Bruna. Stability of graph scattering transforms. Advances in
519"
REFERENCES,0.9320388349514563,"Neural Information Processing Systems, 32, 2019.
520"
REFERENCES,0.9334257975034674,"[47] Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks. IEEE
521"
REFERENCES,0.9348127600554785,"Transactions on Signal Processing, 68:5680–5695, 2020.
522"
REFERENCES,0.9361997226074896,"[48] Henry Kenlay, Dorina Thano, and Xiaowen Dong. On the stability of graph convolutional neural networks
523"
REFERENCES,0.9375866851595007,"under edge rewiring. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
524"
REFERENCES,0.9389736477115118,"Signal Processing (ICASSP), pages 8513–8517. IEEE, 2021.
525"
REFERENCES,0.9403606102635229,"[49] Alejandro Parada-Mayorga, Zhiyang Wang, Fernando Gama, and Alejandro Ribeiro. Stability of aggrega-
526"
REFERENCES,0.941747572815534,"tion graph neural networks. arXiv preprint arXiv:2207.03678, 2022.
527"
REFERENCES,0.9431345353675451,"[50] Oanda Corporation. Foreign exchange data. https://www.oanda.com/, 2018/10/05.
528"
REFERENCES,0.9445214979195562,"[51] Michaël Defferrard,
Xavier Bresson,
and Pierre Vandergheynst.
Convolutional neural net-
529"
REFERENCES,0.9459084604715673,"works on graphs with fast localized spectral ﬁltering.
In D. Lee, M. Sugiyama, U. Luxburg,
530"
REFERENCES,0.9472954230235784,"I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29.
531"
REFERENCES,0.9486823855755895,"Curran Associates, Inc., 2016.
URL https://proceedings.neurips.cc/paper/2016/file/
532"
REFERENCES,0.9500693481276006,"04df4d434d481c5bb723be1b6df1ee65-Paper.pdf.
533"
REFERENCES,0.9514563106796117,"[52] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural
534"
REFERENCES,0.9528432732316228,"information processing systems, 31, 2018.
535"
REFERENCES,0.9542302357836339,"[53] Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason
536"
REFERENCES,0.955617198335645,"Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, et al. Construction of the literature graph in
537"
REFERENCES,0.957004160887656,"semantic scholar. arXiv preprint arXiv:1805.02262, 2018.
538"
REFERENCES,0.9583911234396671,"[54] See Hian Lee, Feng Ji, and Wee Peng Tay. Sgat: Simplicial graph attention network. arXiv preprint
539"
REFERENCES,0.9597780859916782,"arXiv:2207.11761, 2022.
540"
REFERENCES,0.9611650485436893,"[55] Alexandros D Keros, Vidit Nanda, and Kartic Subr. Dist2cycle: A simplicial neural network for homology
541"
REFERENCES,0.9625520110957004,"localization. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 36(7):7133–7142, 2022. doi:
542"
REFERENCES,0.9639389736477115,"10.1609/aaai.v36i7.20673. URL https://ojs.aaai.org/index.php/AAAI/article/view/20673.
543"
REFERENCES,0.9653259361997226,"[56] Kiya W Govek, Venkata S Yamajala, and Pablo G Camara. Spectral simplicial theory for feature selection
544"
REFERENCES,0.9667128987517337,"and applications to genomics. arXiv preprint arXiv:1811.03377, 2018.
545"
REFERENCES,0.9680998613037448,"[57] Mustafa Hajij, Kyle Istvan, and Ghada Zamzmi. Cell complex neural networks. In NeurIPS 2020 Workshop
546"
REFERENCES,0.9694868238557559,"on Topological Data Analysis and Beyond, 2020.
547"
REFERENCES,0.970873786407767,"[58] Mustafa Hajij, Ghada Zamzmi, Theodore Papamarkou, Nina Miolane, Aldo Guzmán-Sáenz, and
548"
REFERENCES,0.9722607489597781,"Karthikeyan Natesan Ramamurthy. Higher-order attention networks. arXiv preprint arXiv:2206.00606,
549"
REFERENCES,0.9736477115117892,"2022.
550"
REFERENCES,0.9750346740638003,"[59] Stefania Sardellitti, Sergio Barbarossa, and Lucia Testa. Topological signal processing over cell complexes.
551"
REFERENCES,0.9764216366158114,"In 2021 55th Asilomar Conference on Signals, Systems, and Computers, pages 1558–1562. IEEE, 2021.
552"
REFERENCES,0.9778085991678225,"[60] T Mitchell Roddenberry, Michael T Schaub, and Mustafa Hajij. Signal processing on cell complexes. In
553"
REFERENCES,0.9791955617198336,"ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
554"
REFERENCES,0.9805825242718447,"pages 8852–8856. IEEE, 2022.
555"
REFERENCES,0.9819694868238558,"[61] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yu Guang Wang, Pietro Liò, Guido F Montufar, and Michael
556"
REFERENCES,0.9833564493758669,"Bronstein. Weisfeiler and lehman go cellular: Cw networks. Advances in Neural Information Processing
557"
REFERENCES,0.984743411927878,"Systems, 34, 2021.
558"
REFERENCES,0.986130374479889,"[62] Albert T Lundell, Stephen Weingram, Albert T Lundell, and Stephen Weingram. Regular and semisimplicial
559"
REFERENCES,0.9875173370319001,"cw complexes. The Topology of CW Complexes, pages 77–115, 1969.
560"
REFERENCES,0.9889042995839112,"[63] Jakob Hansen and Robert Ghrist. Toward a spectral theory of cellular sheaves. Journal of Applied and
561"
REFERENCES,0.9902912621359223,"Computational Topology, 3(4):315–358, 2019.
562"
REFERENCES,0.9916782246879334,"[64] Cristian Bodnar, Francesco Di Giovanni, Benjamin Chamberlain, Pietro Liò, and Michael Bronstein.
563"
REFERENCES,0.9930651872399445,"Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns. Advances in
564"
REFERENCES,0.9944521497919556,"Neural Information Processing Systems, 35:18527–18541, 2022.
565"
REFERENCES,0.9958391123439667,"[65] Elvin Isuﬁand Maosheng Yang. Convolutional ﬁltering in simplicial complexes. In ICASSP 2022 - 2022
566"
REFERENCES,0.9972260748959778,"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5578–5582,
567"
REFERENCES,0.9986130374479889,"2022. doi: 10.1109/ICASSP43922.2022.9746349.
568"
