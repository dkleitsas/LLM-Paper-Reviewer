Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0024813895781637717,"Offline imitation from observations aims to solve MDPs where only task-specific
expert states and task-agnostic non-expert state-action pairs are available. Offline
imitation is useful in real-world scenarios where arbitrary interactions are costly
and expert actions are unavailable. The state-of-the-art ‘DIstribution Correction
Estimation’ (DICE) methods minimize divergence of state occupancy between
expert and learner policies and retrieve a policy with weighted behavior cloning;
however, their results are unstable when learning from incomplete trajectories, due
to a non-robust optimization in the dual domain. To address the issue, in this paper,
we propose Trajectory-Aware Imitation Learning from Observations (TAILO).
TAILO uses a discounted sum along the future trajectory as the weight for weighted
behavior cloning. The terms for the sum are scaled by the output of a discriminator,
which aims to identify expert states. Despite simplicity, TAILO works well if
there exist trajectories or segments of expert behavior in the task-agnostic data, a
common assumption in prior work. In experiments across multiple testbeds, we find
TAILO to be more robust and effective, particularly with incomplete trajectories."
INTRODUCTION,0.004962779156327543,"1
Introduction"
INTRODUCTION,0.007444168734491315,"In recent years, Reinforcement Learning (RL) has been remarkably successful on a variety of tasks,
from games [57] and robot manipulation [66] to recommendation systems [9] and large language
model fine-tuning [53]. However, RL often also suffers from the need for extensive interaction with
the environment and missing compelling rewards in real-life applications [37]."
INTRODUCTION,0.009925558312655087,"To address this, Imitation Learning (IL), where an agent learns from demonstrations, is gaining
popularity recently [20, 22, 27]. Offline imitation learning, such as behavior cloning (BC) [51],
allows the agent to learn from existing experience without environment interaction and reward label,
which is useful when wrong actions are costly. However, similar to RL, IL also suffers when limited
data is available [52] – which is common as demonstrations of the target task need to be collected
every time a new task is addressed. In this work, we consider a special but widely studied [31, 36, 60]
case of expert data shortage in offline IL, i.e., offline Learning from Observations (LfO) [71]. In
LfO the task-specific data only consists of a few expert trajectories, key frames, or even just the goal
(the latter is also known as example-based IL [13]), and the dynamics must be mostly learned from
task-agnostic data, i.e., demonstration from data not necessarily directly related to the target task. For
example, sometimes the agent must learn from experts with different embodiment [41], where expert
actions are not applicable."
INTRODUCTION,0.01240694789081886,"In the field of offline LfO, researchers have explored action pseudo-labeling [33, 60], inverse RL [31,
61, 73], and similarity-based reward labeling [8, 55]; for example-based IL, the benchmark is"
INTRODUCTION,0.01488833746898263,"(a) Problem Settings
(b) Discriminator R(s)
(c) Weighted Behavior Cloning"
INTRODUCTION,0.017369727047146403,"Figure 1: An illustration of our method, TAILO. Different trajectories are illustrated by different styles of arrows.
TAILO consists of two parametric steps: 1) train a discriminator which gives high R(s) for near-expert states
and low R(s) for non-expert states, as shown in panel b); 2) conduct weighted behavior cloning with weights
calculated from R(s) along the trajectory, as shown in panel c). High transparency indicates a small weight for
the state and its corresponding action."
INTRODUCTION,0.019851116625310174,"RCE [13], which uses RL with classifier-labeled reward. Recently, DIstribution Correction Estimation
(DICE) methods, LobsDICE [23] and SMODICE [41], achieve the state of the art for both offline LfO
and example-based IL. Both methods minimize the state visitation frequency (occupancy) divergence
between expert and learner policies, and conduct a convex optimization in the dual space."
INTRODUCTION,0.022332506203473945,"However, DICE methods are neither robust to incomplete trajectories in the task-agnostic / task-
specific data [72], nor do they excel if the task-agnostic data contains a very small portion of expert
trajectories or only segments [56]. These are inherent shortcomings of the DICE-based methods, as
they are indirect: they first perform optimization in a dual domain (equivalent to finding the value
function in RL), and then recover the policy by weighted behavior cloning. Importantly, Kullback-
Leibler(KL)-based optimization of dual variables requires complete trajectories in the task-agnostic
data to balance all terms in the objective. Note, SMODICE with χ2-based optimization is also
theoretically problematic (see Appendix C) and empirically struggles on testbeds [41]."
INTRODUCTION,0.02481389578163772,"To overcome the shortcomings listed above, we propose a simple but effective method for imitation
learning from observations and examples, Trajectory-Aware Imitation Learning from Observations
(TAILO). We leverage the common assumption that there exist trajectories or long segments that are
near-optimal to the task of interest in the task-agnostic data. This assumption is the basis of skill-
based learning [20, 46, 47], and the benchmarks of many recent works satiesfy this assumption [36,
40, 41, 56]; one real-life example fulfilling this assumption is robotics: the robot often utilizes
overlapping skills such as moving the robotic arm and grabbing items from other tasks to complete
the current task. Based on this assumption, we discriminate/identify which state-action pairs could be
taken by the expert, and assign large weights for trajectory segments leading to those segments in the
downstream Weighted Behavior Cloning (WBC). This is a simple way to make the learned policy
trajectory-aware. The method only consists of two parametric steps: 1) train a discriminator using
positive-unlabeled (PU) learning, and 2) use Weighted Behavior Cloning (WBC) on all state-action
pairs in the task-agnostic data with the weights of WBC being a discounted sum over thresholded
scores given by the discriminator. Note, the discounted sum propagates large scores to trajectory
segments in the past far from expert states, if they lead to expert trajectory segments eventually.
Meanwhile, as the task-agnostic data contains both expert and non-expert demonstrations, Positive-
Unlabeled (PU) learning is better than plain binary classification. Fig. 1 summarizes our algorithm.
We found this simple solution to be surprisingly effective across multiple testbeds, especially if the
task-agnostic data contains incomplete trajectories. In this latter case, baselines struggle or even
diverge. Moreover, we find our method to also improve if the task-agnostic data contains only a small
portion of expert trajectories."
INTRODUCTION,0.02729528535980149,"We summarize our contributions as follows: 1) We carefully analyzed the state-of-the-art DICE
methods in offline LfO, pointing out their limitations both empirically and theoretically (see Ap-
pendix C for details); 2) We propose a simple yet effective solution to offline imitation learning from
observations; and 3) We empirically show that this simple method is robust and works better than the
state of the art on a variety of settings, including incomplete trajectories, few expert trajectories in the
task-agnostic dataset, example-based IL and learning from mismatching dynamics."
PRELIMINARIES,0.02977667493796526,"2
Preliminaries"
PRELIMINARIES,0.03225806451612903,"Markov Decision Process. A Markov Decision Process (MDP) is a well-established framework for
sequential decision-making problems. An MDP is defined by the tuple (S, A, T, r, γ), where S is the
state space and A is the action space. For every timestep t of the Markov process, a state st ∈S is
given, and an action at ∈A is chosen by the agent according to its policy π(at|st) ∈∆(A), where
∆(A) is the probability simplex over A. Upon executing the action at, the MDP will transit to a
new state st+1 ∈S according to the transition probability T(st+1|st, at) while the agent receives
reward r(st, at) ∈R. The goal of the agent is to maximize the discounted reward P"
PRELIMINARIES,0.034739454094292806,"t γtr(st, at)
with discount factor γ ∈[0, 1] over a complete run, which is called an episode. The state(-action
pairs) collected through the run are called a state(-action) trajectory τ. Trajectory segment in this
work is defined as a continuous subsequence of a trajectory τ. The state visitation frequency (state
occupancy) of a policy π is denoted as dπ(s) = (1 −γ) P"
PRELIMINARIES,0.03722084367245657,"t γt Pr(st = s). See Appendix A for a
detailed definition of state occupancy and other occupancies."
PRELIMINARIES,0.03970223325062035,"Positive-Unlabeled Learning. Positive-Unlabeled learning [12] addresses the problem of binary
classification with feature x ∈Rn and label y ∈{0, 1} when only the positive dataset DP and the
unlabeled dataset DU are known. Our solution leverages positive prior ηp = Pr(y = 1) and negative
prior ηn = Pr(y = 0), which are unknown and treated as a hyperparameter."
PRELIMINARIES,0.04218362282878412,"Offline Imitation from Observations / Examples. Offline imitation learning from observations
requires the agent to learn a good policy from two sources of data: one is the task-specific dataset
DTS, which contains state trajectories τTS = {s1, s2, . . . , sn1} from the expert that directly addresses
the task of interest; the other is the task-agnostic dataset DTA, which contains state-action trajectories
τTA = {(s1, a1), (s2, a2), . . . , (sn2, an2)} of unknown optimality to the task of interest. Note that the
task-specific trajectory τTS can be incomplete; specifically, if only the last state exists as an example
of success, then it is called imitation from examples [13, 41]."
PRELIMINARIES,0.04466501240694789,"The state of the art methods in this field are SMODICE [41] and LobsDICE [23]. SMODICE
minimizes the divergence between the state occupancy from task-specific data dTS and the learner
policy π’s occupancy dπ; for example, when using a KL-divergence as the metric, the objective is"
PRELIMINARIES,0.04714640198511166,"min
π KL(dπ(s)∥dTS(s)), s.t. π is a feasible policy.
(1)"
PRELIMINARIES,0.04962779156327544,"However, since the task-agnostic dataset is the only source of correspondence between state and
action, the state occupancy of the task-agnostic dataset dTA must be introduced. With some derivations
and relaxations, the objective is rewritten as"
PRELIMINARIES,0.052109181141439205,"max
π
Es∼dπ log dTS(s)"
PRELIMINARIES,0.05459057071960298,"dTA(s) −KL(dπ(s)∥dTA(s)), s.t. π is a feasible policy.
(2)"
PRELIMINARIES,0.05707196029776675,"Here, the first term R(s) = log dTS(s)"
PRELIMINARIES,0.05955334987593052,"dTA(s) is an indicator for the importance of the state; high R(s)
means that the expert often visits state s, and s is a desirable state. Such R(s) can be trained by a
discriminator c(s): a positive dataset DTS (label 1) and a negative dataset DTA (label 0) are used
to find an ‘optimal’ discriminator c = c∗. Given this discriminator, we have R(s) = log
c∗(s)
1−c∗(s).
SMODICE then converts the constrained Eq. (2) to its unconstrained Lagrangian dual form with dual
variable V (s), and optimizes the following objective (assuming KL-divergence as the metric):"
PRELIMINARIES,0.062034739454094295,"min
V (1 −γ)Es∼p0[V (s)] + log E(s,a,s′)∼DTA exp[R(s) + γV (s′) −V (s)],
(3)"
PRELIMINARIES,0.06451612903225806,"where γ is the discount factor and p0 is the distribution of the initial state in the MDP. In this
formulation, R(s) can be regarded as the reward function, while V (s) is the value function. The
whole objective is an optimization of a convex function with respect to the Bellman residual. With
V (s) learned, the policy is retrieved via weighted behavior cloning where the coefficient is determined
by V (s). LobsDICE is in spirit similar, but considers the occupancy of adjacent state pairs instead of
a single state."
METHODOLOGY,0.06699751861042183,"3
Methodology"
MOTIVATION AND OVERVIEW,0.06947890818858561,"3.1
Motivation and Overview"
MOTIVATION AND OVERVIEW,0.07196029776674938,"As mentioned in Sec. 2, the DICE methods for offline LfO discussed above consist of three parts:
reward generation, optimization of the value function V (s), and weighted behavior cloning. Such
a pipeline can be unstable for two reasons. First, the method is indirect, as the weight for behavior
cloning depends on the learned value function V (s), which could be inaccurate if the task-agnostic
dataset is noisy or is not very related to the task of interest. This is aggravated by the fact that V (s′)
as a 1-sample estimation of Es′∼p(s′|s,a)V (s′) and logsumexp are used in the objective, which further
destabilizes learning. Second, for KL-based metrics, if no state appears twice in the task-agnostic
data, which is common for high-dimensional environments, the derivative of the objective with
respect to V (s) is determined by the initial state term, the current state term −V (s), and the next state
term γV (s′). Thus, if a non-initial state s′ is missing from the trajectory, then only −γV (s′) remains,
which makes the objective monotonic with respect to the unconstrained V (s′). Consequently, V
diverges for s′ (see Appendix C in the for a more detailed analysis and visualization in Fig. 7)."
MOTIVATION AND OVERVIEW,0.07444168734491315,"To address the stability issue, we develop Trajectory-Aware Imitation Learning from Observations
(TAILO). TAILO also seeks to find an approximate reward R(s) by leveraging the discriminator
c(s), which is empirically a good metric for optimality of the state. However, compared to DICE
methods which determine the weight of behavior cloning via a dual program, we adopt a much
simpler idea: find ‘good’ trajectory segments using the discounted sum of future R(s) along the
trajectory following state s, and encourage the agent to follow them in the downstream weighted
BC. To do so, we assign a much larger weight, a thresholding result of the discounted sum, to the
state-action pairs for ‘good’ segments. Meanwhile, small weights on other segments serve as a
regularizer of pessimism [25]. Such a method is robust to missing steps in the trajectory. Empiricially
we find that the weight need not be very accurate for TAILO to succeed. In the remainder of the
section, we discuss the two steps of TAILO: 1) training a discriminator to obtain R(s) (Sec. 3.2), and
2) thresholding over discounted sums of R(s) along the trajectory (Sec. 3.3)."
POSITIVE-UNLABELED DISCRIMINATOR,0.07692307692307693,"3.2
Positive-Unlabeled Discriminator"
POSITIVE-UNLABELED DISCRIMINATOR,0.0794044665012407,"Following DICE, R(s) is used as a metric for state optimality, and is obtained by training a discrimi-
nator c(s). However, different from DICE which regards all unlabeled data as negative samples, we
use Positive-Unlabeled (PU) learning to train c(s), since there often are some expert trajectories or
segments of useful trajectories in the task-agnostic dataset. Consequently, we treat the task-agnostic
dataset as an unlabeled dataset with both positive samples (expert of the task of interest) and varied
negative samples (non-expert)."
POSITIVE-UNLABELED DISCRIMINATOR,0.08188585607940446,"Our training of c(s) consists of two steps: 1) training another discriminator c′(s) that identifies safe
negative samples, and 2) formal training of c(s). In the first step, to alleviate the issue of treating
positive samples from DTA as negatives, we use a debiasing objective [29] for the training of c′(s)
(see Appendix A for a detailed derivation):"
POSITIVE-UNLABELED DISCRIMINATOR,0.08436724565756824,"min
c′(s) L1(c′) = min
c′(s) −[ηpEs∼DTS log c′(s)+max(0, Es∼DTA log(1−c′(s))−ηpEs∼DTS log(1−c′(s)))], (4)"
POSITIVE-UNLABELED DISCRIMINATOR,0.08684863523573201,"where the bias comes from viewing unlabeled samples as negative samples. Here, positive class prior
ηp is a hyperparameter; in experiments, we find results to not be sensitive to this hyperparameter."
POSITIVE-UNLABELED DISCRIMINATOR,0.08933002481389578,"In the second step, after c′(s) is trained, R′(s) = log
c′(s)
1−c′(s) is calculated for each state in the
task-agnostic data, and the states in the (possibly incomplete) trajectories with the least β1 ∈(0, 1)
portion of average R′(s) are identified as “safe” negative samples. Note, we do not identify “safe”
positive samples, because a trajectory that only has a segment useful for the task of interest might
not have the highest average R′(s) due to its irrelevant part; however, an irrelevant trajectory will
probably have the lowest R′(s) throughout the whole trajectory, and thus can be identified as a “safe”
negative sample. We collect such samples to form a new dataset Dsafe TA."
POSITIVE-UNLABELED DISCRIMINATOR,0.09181141439205956,"Finally, the formal training of c(s) uses states from DTS as positive samples and Dsafe TA as “safe”
negative samples. The training objective of c(s) is a combination of debiasing objective and standard
cross entropy loss for binary classification, controlled by hyperparameter β2. Specifically, we use"
POSITIVE-UNLABELED DISCRIMINATOR,0.09429280397022333,"min
c(s) β2L2(c) + (1 −β2)L3(c), where"
POSITIVE-UNLABELED DISCRIMINATOR,0.0967741935483871,"L2(c) = min
c(s) −[ηpEs∼DTS log c(s) + max(0, Es∼Dsafe TA log(1 −c(s)) −ηpEs∼DTS log(1 −c(s)))],"
POSITIVE-UNLABELED DISCRIMINATOR,0.09925558312655088,"L3(c) = Es∼DTS log c(s) + Es∼Dsafe TA log(1 −c(s)).
(5)"
POSITIVE-UNLABELED DISCRIMINATOR,0.10173697270471464,"In this work, we only consider β2 ∈{0, 1}; empirically, we found that β2 = 0 is better if the agent’s
embodiments across DTS and DTA are the same, and β2 = 1 is better if the embodiments differ. This
is because a debiasing objective assumes positive samples still exist in the safe negatives, which
pushes the classification margin further from samples in DTS, and has a larger probability to classify
expert segments in DTA as positive."
TRAJECTORY-AWARE THRESHOLDING,0.10421836228287841,"3.3
Trajectory-Aware Thresholding"
TRAJECTORY-AWARE THRESHOLDING,0.10669975186104218,"With the discriminator c(s) trained and R(s) = log
c(s)
1−c(s) obtained, we can identify the most useful
trajectory segments for our task. To do this, we use a simple thresholding which makes the weight
employed in behavior cloning trajectory-aware. Formally, the weight W(si, ai) for weighted behavior
cloning is calculated as"
TRAJECTORY-AWARE THRESHOLDING,0.10918114143920596,"W(si, ai) = ∞
X"
TRAJECTORY-AWARE THRESHOLDING,0.11166253101736973,"j=0
γj exp(αR(si+j)),
(6)"
TRAJECTORY-AWARE THRESHOLDING,0.1141439205955335,"where (si, ai) is the i-th step in a trajectory, and α is a hyperparameter that controls the strength of
thresholding; α balances the tradeoff between excluding non-expert and including expert-trajectories.
For an i + j which exceeds the length of the trajectory, we set si+j to be the last state of the (possibly
incomplete) known trajectory, as the final state is of significant importance in many applications [18].
This design also allows us to conveniently address the example-based offline IL problem, where the
final state is important. With the weights determined, we finally conduct a weighted behavior cloning
with the objective maxπ E(s,a)∼DTAW(s, a) log π(a|s), where π(a|s) is the desired policy."
EXPERIMENTS,0.11662531017369727,"4
Experiments"
EXPERIMENTS,0.11910669975186104,"In this section, we evaluate TAILO on five different, challenging tasks across multiple mujoco
testbeds. More specifically, we study the following two questions: 1) Is the algorithm indeed robust
to incomplete trajectories in either task-agnostic (Sec. 4.1) or task-specific (Sec. 4.2) data, and does it
work with little expert data in the task-agnostic dataset (Sec. 4.3)? 2) Can the algorithm also work
well in example-based IL (Sec. 4.4) and learn from experts of different dynamics (Sec. 4.5)?"
EXPERIMENTS,0.12158808933002481,"Baselines. We compare TAILO to four baselines: SMODICE [41], LobsDICE [23], ORIL [73], and
Behavior Cloning (BC). Since LobsDICE works with state-pair occupancy, it cannot solve example-
based IL; thus, we substitute LobsDICE in example-based IL with RCE [13], a state-of-the-art
example-based RL method. Unless otherwise specified, we use 3 random seeds per method in each
scenario."
EXPERIMENTS,0.12406947890818859,"Environment Setup. Following SMODICE [41], unless otherwise specified, we test our algorithm on
four standard mujoco testbeds from the OpenAI Gym [5], which are the hopper, halfcheetah, ant, and
walker2d environment. We use normalized average reward1 as the main metric, where higher reward
indicates better performance; for environments where the final reward is similar, fewer gradient steps
in weighted behavior cloning indicates better performance. We report the change of the mean and
standard deviation of reward with respect to the number of gradient steps."
EXPERIMENTS,0.12655086848635236,"Experimental Setup. For all environments, we use α = 1.25, β1 = 0.8, ηp = 0.2; β2 = 1 if DTS
and DTA are generated by different embodiments, and β2 = 0 otherwise. We use γ = 0.998 unless
otherwise specified. We use an exactly identical discriminator and policy network as SMODICE:
for the discriminator c(s) and c′(s), we use a small Multi-Layer Perceptron (MLP) with two hidden
layers, width 256, and tanh activation function. For actor π, we use an MLP with two hidden layers,
width 256, and ReLU [2] activation function. For the training of c(s) and c′(s), we use a learning"
EXPERIMENTS,0.12903225806451613,"1Normalization standard is according to D4RL [16], and identical to SMODICE [41]."
EXPERIMENTS,0.1315136476426799,"Figure 2: Reward curves for offline imitation learning with incomplete trajectories in the task-agnostic dataset,
where the x-axis is the number of gradient steps and the y-axis is the normalized reward. The title for each sub-
figure is in the format of “environment name”+“1/x” (task-agnostic data removed), where x ∈{2, 3, 5, 10, 20}.
We observe the proposed method to be the most stable."
EXPERIMENTS,0.13399503722084366,"rate of 0.0003 and a 1-Lipschitz regularizer, run 10K gradient steps for c′(s), and run 40K gradient
steps for c(s). For weighted BC, we use a learning rate of 10−4, a weight decay of 10−5, and run 1M
gradient steps. For discriminator training, we use a batch size of 512; for weighted BC steps, we use
a batch size of 8192. Adam optimizer [28] is used for both steps. See Appendix D for more details
and Appendix F for a sensitivity analysis regarding the batch size, α, β1, β2, ηp, and γ."
LEARNING FROM TASK-AGNOSTIC DATASET WITH INCOMPLETE TRAJECTORIES,0.13647642679900746,"4.1
Learning from Task-Agnostic Dataset with Incomplete Trajectories"
LEARNING FROM TASK-AGNOSTIC DATASET WITH INCOMPLETE TRAJECTORIES,0.13895781637717122,"Dataset Setup. We modify the standard dataset settings from SMODICE to create our dataset.
SMODICE uses offline datasets from D4RL [16], where a single trajectory from the “expert-v2”
dataset is used as the task-specific data. The task-agnostic data consists of 200 expert trajectories
(200K steps) from the “expert-v2” dataset and 1M steps from the “random-v2” dataset. Based on
this, we iterate over the state-action pairs in the task-agnostic dataset, and remove one pair for every
x pairs. In this work, we test x ∈{2, 3, 5, 10, 20}."
LEARNING FROM TASK-AGNOSTIC DATASET WITH INCOMPLETE TRAJECTORIES,0.141439205955335,"Main Results. Fig. 2 shows the result for different methods with incomplete task-agnostic trajectories,
where our method outperforms all baselines and remains largely stable despite decrease of x (i.e.,
increase of removed data), as the weights for each state-action pair do not change much. In the
training process, we often witness SMODICE and LobsDICE to collapse due to diverging value
functions (see Appendix C for explanation), which is expected; for runs that abort due to numerical
error, we use a reward of 0 for the rest of the gradient steps. Under a few cases, SMODICE with
KL-divergence works decently well with larger noises (e.g., Halfcheetah_1/3 and Walker2d_1/3);
this is because sometimes the smoothing effect of the neural network mitigates divergence. However,
with larger batch size (See Fig. 23 in Appendix F.4.6) and more frequent and uniform updates on
each data point, the larger the noise the harder SMODICE fails."
LEARNING FROM TASK-SPECIFIC DATASET WITH INCOMPLETE TRAJECTORIES,0.14392059553349876,"4.2
Learning from Task-Specific Dataset with Incomplete Trajectories"
LEARNING FROM TASK-SPECIFIC DATASET WITH INCOMPLETE TRAJECTORIES,0.14640198511166252,"Dataset Setup and Main Results.
We use SMODICE’s task-agnostic dataset as described
in Sec. 4.1.
For the task-specific dataset, we only use the first x steps and the last y"
LEARNING FROM TASK-SPECIFIC DATASET WITH INCOMPLETE TRAJECTORIES,0.1488833746898263,"Figure 3: Reward curves for offline imitation with incomplete trajectories in the task-specific dataset. The
title format for each subfigure is “environment name”+“head x”+“tail y” (states remained), where (x, y) ∈
{(100, 1), (90, 10), (50, 50), (10, 90), (1, 100)}. We observe the proposed method to be the most stable."
LEARNING FROM TASK-SPECIFIC DATASET WITH INCOMPLETE TRAJECTORIES,0.1513647642679901,"steps in the expert trajectory, and discard the remainder.
In this work, we test (x, y) ∈
{(1, 100), (10, 90), (50, 50), (90, 10), (100, 1)}. Fig. 3 shows the result with incomplete task-specific
trajectories, where our method outperforms all baselines and often achieves results similar to those
obtained when using the entire task-specific dataset. In contrast, SMODICE and LobsDICE are
expectedly unstable in this setting."
STANDARD OFFLINE IMITATION LEARNING FROM OBSERVATIONS,0.15384615384615385,"4.3
Standard Offline Imitation Learning from Observations"
STANDARD OFFLINE IMITATION LEARNING FROM OBSERVATIONS,0.15632754342431762,"Environment Setup. In addition to the four standard mujoco environments specified above, we
also test on two more challenging environments: the Franka kitchen environment and the antmaze
environment from D4RL [16]. In the former, the agent needs to control a 9-DoF robot arm to complete
a sequence of item manipulation subtasks, such as moving the kettle or opening the microwave; in the
latter, the agent needs to control a robot ant to crawl through a U-shaped maze and get to a particular
location. As the kitchen environment requires less steps to finish, we use γ = 0.98 instead of 0.998."
STANDARD OFFLINE IMITATION LEARNING FROM OBSERVATIONS,0.1588089330024814,"Dataset Setup. As existing methods already solve the four mujoco environments with SMODICE’s
dataset settings in Sec. 4.1 quite well (see Appendix F for result), we test a more difficult setting to
demonstrate TAILO’s ability to work well with few expert trajectories in the task-agnostic dataset.
More specifically, we use the same task-specific dataset, but only use 40 instead of 200 expert
trajectories from the “expert-v2” dataset to mix with the 1M “random-v2” steps data and form
the task-agnostic dataset. For the more challenging kitchen and antmaze environment, we use the
identical dataset as SMODICE, where a single trajectory is used as the task-specific dataset. The
task-agnostic dataset for the kitchen environment consists of expert trajectories completing different
subtasks (both relevant and irrelevant) in different orders, and the task-agnostic dataset for antmaze
consists of data with varied optimality. See Appendix D for details."
STANDARD OFFLINE IMITATION LEARNING FROM OBSERVATIONS,0.16129032258064516,"Main Results. Fig. 4 shows the result for different methods in standard offline imitation learning
from observations. We find our method to outperform all baselines on hopper, halfcheetah, ant and
walker2d. Results are comparable to the best baseline on kitchen and antmaze. In the experiment,
we found ORIL to often diverge, and the performance of SMODICE varies greatly depending on
the f-divergence: on hopper, halfcheetah and walker2d, KL-divergence is much better, while χ2-"
STANDARD OFFLINE IMITATION LEARNING FROM OBSERVATIONS,0.16377171215880892,"Figure 4: Reward curves for offline imitation learning from observations with few expert trajectories in the
task-agnostic dataset. In all six environments, our method either outperforms or is comparable to the baselines."
STANDARD OFFLINE IMITATION LEARNING FROM OBSERVATIONS,0.1662531017369727,Figure 5: Reward curves for offline imitation learning from examples.
STANDARD OFFLINE IMITATION LEARNING FROM OBSERVATIONS,0.1687344913151365,"divergence is better on ant and kitchen. LobsDICE is marginally better than SMODICE, as the former
considers state-pair occupancy instead of single state occupancy, which is more informative. Also
worth noting: none of the methods exceeds a normalized reward of 60 in the kitchen environment;
this is because the SMODICE experiment uses expert trajectories that are only expert for the first 2
out of all 4 subtasks. See Sec. F.3 in the Appendix for a detailed discussion."
LEARNING FROM EXAMPLES,0.17121588089330025,"4.4
Learning from Examples"
LEARNING FROM EXAMPLES,0.17369727047146402,"Environment Setup. Following SMODICE, we test example-based offline imitation in three different
testbeds: pointmaze, kitchen and antmaze. In the mujoco-based [59] pointmaze environment the
agent needs to control a pointmass on a 2D plane to navigate to a particular direction. For the kitchen
environment, we test two different settings where the agent is given successful examples of moving the
kettle and opening the microwave respectively (denoted as “kitchen-kettle” and “kitchen-microwave”).
As pointmaze requires less steps to finish, we use γ = 0.98 instead of 0.998."
LEARNING FROM EXAMPLES,0.1761786600496278,"Dataset Setup and Main Results. Following SMODICE, we use a small set of success examples:
≤200 states for antmaze and pointmaze and 500 states for kitchen (see Appendix D for details) are
given as the task-specific dataset. For pointmaze, the task-agnostic data contains 60K steps, generated
by a script and distributed evenly along four directions (i.e., 25% expert data); for other testbeds, the
task-agnostic data is identical to SMODICE described in Sec. 4.1. Fig. 5 shows the results for offline
imitation learning from examples on all environments tested in SMODICE; TAILO is marginally
better than the baselines on pointmaze, antmaze, and kitchen-microwave, and is comparable (all close
to perfect) on kitchen-kettle."
LEARNING FROM MISMATCHED DYNAMICS,0.17866004962779156,"4.5
Learning from Mismatched Dynamics"
LEARNING FROM MISMATCHED DYNAMICS,0.18114143920595532,"Environment and Dataset Setup. Following SMODICE, we test our method on three environments:
antmaze, halfcheetah, and ant. We use task-specific data from an expert with different dynamics (e.g.,
ant with a leg crippled; see Sec. E for details). Task-agnostic data follows SMODICE in Sec. 4.1."
LEARNING FROM MISMATCHED DYNAMICS,0.18362282878411912,"Main Results. Fig. 6 shows the results for offline imitation learning from mismatched dynamics,
where our method is the best in all three testbeds. Among the three environments, halfcheetah is
the most difficult, as the state space for halfcheetah with shorter torso is unreachable by a normal"
LEARNING FROM MISMATCHED DYNAMICS,0.18610421836228289,Figure 6: Reward curves for offline imitation learning from mismatched dynamics.
LEARNING FROM MISMATCHED DYNAMICS,0.18858560794044665,"halfcheetah, i.e., the assumption that dTA(s) > 0 wherever dTS(s) > 0 in SMODICE and LobsDICE
does not hold; in such a setting, our method is much more robust than DICE methods."
RELATED WORK,0.19106699751861042,"5
Related Work"
RELATED WORK,0.1935483870967742,"Offline Imitation Learning and DIstribution Correction Estimation (DICE). Offline imitation
learning aims to learn a policy only from data without interaction with the environment. This is
useful where immature actions are costly, e.g., in a dangerous factory. The simplest solution for
offline imitation learning is plain Behavior Cloning (BC) [51]. Many more methods have been
proposed recently, such as BCO [60] and VMSR [33] (which pseudolabel actions), offline extensions
of GAIL [3, 31, 61, 73], and similarity-based reward labeling [8, 35, 55, 64]. Currently, the state-of-
the-art method for offline imitation learning is DIstribution Correction Estimation (DICE) [23, 27,
32, 34, 41], which minimizes the discrepancy between an expert’s and a learner’s state, state-action,
or state-pair occupancy. Our method is inspired by DICE, but is much simpler and more effective.
More recently, two works unify offline imitation learning with offline RL, which are offline-RL-based
MAHALO [36] and DICE-based ReCOIL [56]. Different from SMODICE and LobsDICE, ReCOIL
minimizes the divergence between learner and expert data mixed with non-expert data respectively,
which removes the data coverage assumption. However, ReCOIL faces the problem discussed in
Sec. 3.1 when dealing with incomplete trajectories, and MAHALO is based on state-pairs similar to
LobsDICE, which cannot solve IL with incomplete trajectories or example-based IL like our TAILO."
RELATED WORK,0.19602977667493796,"Learning from Observations and Examples. Learning from observations (LfO) [60] requires the
agent to learn from a task-specific dataset without expert action, which is useful when learning from
videos [44] or experts with different embodiments [55], as the expert action is either unavailable
or not applicable. Learning from examples is an extreme case of LfO where only the final goal is
given [13]. There are three major directions: 1) pseudolabeling of actions which builds an inverse
dynamic model and predicts the missing action [33, 60]; 2) occupancy divergence minimization with
either DICE [23, 27, 34, 41, 71] or inverse-RL style iterative update [61, 67, 73]; and 3) RL/planning
with reward assignment based on state similarity (often in visual imitation) [9, 55, 64]. Our proposed
TAILO solves LfO with a simple solution different from existing ones."
RELATED WORK,0.19851116625310175,"Discriminator as Reward. The idea of training a discriminator to provide rewards for states is
widely used in IL, including inverse RL methods [15, 22, 31, 73], DICE methods [23, 27, 41], and
methods such as 2IWIL [65] and DWBC [68]. In this work, we propose a simple but explicit way
to take the trajectory context into account, using the output from a discriminator as reward, which
differs from prior works."
RELATED WORK,0.20099255583126552,"Positive-Unlabeled Learning. Positive-Unlabeled (PU) [11, 12, 29, 50] learning aims to solve
binary classification tasks where only positive and unlabeled data are available. It is widely used in
data retrieval [54], outlier detection [38], recommendation [7] and control tasks [67]. In this work,
we utilize two PU learning achievements: the skill of identifying “safe” negative samples [39] and
debiasing [11, 29]. The closest RL work to our use of PU learning is ORIL [73], which also uses
positive-unlabeled learning to train a discriminator for reward estimation. However, there are three
key differences between our method and ORIL: we define R(s) differently for better thresholding,
use different techniques for PU learning to prevent overfitting, and, importantly, the removal of value
function learning. See Appendix F.6 for an ablation to demonstrate efficacy of these changes."
RELATED WORK,0.20347394540942929,"Reward-Weighted Regression(RWR) [48] and Advantage-Weighted Regression(AWR) [45]. The
idea in our Eq. (6) of weighted behavior cloning with weights being (often exponentiated) discounted
return has been widely used in the RL community [1, 45, 63], and our objective resembles that of"
RELATED WORK,0.20595533498759305,"RWR/AWR. However, our work differs from RWR/AWR inspired works [30, 43, 49] in the following
aspects:"
RELATED WORK,0.20843672456575682,"• The objective for both AWR and RWR are built upon the related payoff procedure [10, 21],
which introduces an Expectation-Maximization (EM) procedure for RL. However, for
offline IL in our case, iteration between E-step and M-step are infeasible. There are two
workarounds for this: importance sampling and naively using one iteration. However,
the former is known to be non-robust [42] and the latter, MARWIL [62], struggles in our
testbeds (see Appendix F.2).
• We use neither an adaptive reward scaling term [48] in the EM framework nor parametric
value estimation [45, 62], which are both widely utilized by RWR/AWR inspired works.
However, the former is not guaranteed to preserve an optimal policy and thus is not an
advantage [58], while the latter struggles in our setting where learning a good value function
is hard, as illustrated by the performance of baselines such as DICE methods and MARWIL.
• For all existing RWR/AWR works, the reward labels are assumed to be available, which is
different from our case."
CONCLUSION,0.2109181141439206,"6
Conclusion"
CONCLUSION,0.21339950372208435,"We propose TAILO, a simple yet effective solution for offline imitation from observations by training
a discriminator using PU learning, applying a score to each state-action pair, and then conducting a
weighted behavior cloning with a discounted sum over thresholded scores along the future trajectory
to obtain the weight. We found our method to improve upon state-of-the-art baselines, especially
when the trajectories are incomplete or the expert trajectories in the task-agnostic data are few."
CONCLUSION,0.21588089330024815,"Societal Impact. Our work addresses sequential decision-making tasks from expert observations
with fewer related data, which makes data-driven automation more applicable. This, however, could
also lead to negative impacts on society by impacting jobs."
CONCLUSION,0.21836228287841192,"Limitations and Future Directions. Our method relies on the assumption that there exist expert
trajectories or at least segments in the task-agnostic data. While this is reasonable for many appli-
cations, like all prior work with proprioceptive states, it is limited in generalizability. Thus, one
promising future direction is to improve the ability to summarize abstract “skills” from data with
better generalizability. Another future direction is learning from video demonstrations, which is
a major real-world application for imitation learning from observations. Also, our experiments
are based on simulated environments such as D4RL. Thus a gap between our work and real-life
progress remains. While we are following the settings of many recent works, such as SMODICE [41],
ReCOIL [56] and OTR [40], to bridge the gap using techniques such as sim2real in the robotics
community [24] is another very important direction for future work."
CONCLUSION,0.22084367245657568,"Acknowledgements. This work was supported in part by NSF under Grants 2008387, 2045586,
2106825, MRI 1725729, NIFA award 2020-67021-32799, the Jump ARCHES endowment through
the Health Care Engineering Systems Center, the National Center for Supercomputing Applications
(NCSA) at the University of Illinois at Urbana-Champaign through the NCSA Fellows program, the
IBM-Illinois Discovery Accelerator Institute, and the Amazon Research Award."
REFERENCES,0.22332506203473945,References
REFERENCES,0.22580645161290322,"[1] A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. Maxi-
mum a posteriori policy optimisation. ArXiv:1806.06920, 2018."
REFERENCES,0.228287841191067,"[2] A. F. Agarap. Deep learning using rectified linear units (relu). ArXiv:1803.08375, 2018."
REFERENCES,0.23076923076923078,"[3] S. Y. Arnob. Off-policy adversarial inverse reinforcement learning. ArXiv:2005.01138, 2020."
REFERENCES,0.23325062034739455,"[4] M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio, A. Courville, and D. Hjelm.
Mutual information neural estimation. In ICML, 2018."
REFERENCES,0.23573200992555832,"[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.
Openai gym, 2016."
REFERENCES,0.23821339950372208,"[6] A. Camacho, I. Gur, M. Moczulski, O. Naschum, and A. Faust. Sparsedice: Imitation learning
for temporally sparse data via regularization. In the Unsupervised Reinforcement Learning
Workshop in ICML, 2021."
REFERENCES,0.24069478908188585,"[7] S. Chang, Y. Zhang, J. Tang, D. Yin, Y. Chang, M. A. Hasegawa-Johnson, and T. S. Huang.
Positive-unlabeled learning in streaming networks. In KDD, 2016."
REFERENCES,0.24317617866004962,"[8] A. S. Chen, S. Nair, and C. Finn. Learning generalizable robotic reward functions from
""in-the-wild"" human videos. ArXiv:2103.16817, 2021."
REFERENCES,0.2456575682382134,"[9] X. Chen, S. Li, H. Li, S. Jiang, Y. Qi, and L. Song. Generative adversarial user model for
reinforcement learning based recommendation system. In ICML, 2019."
REFERENCES,0.24813895781637718,"[10] P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural
Computation, 1997."
REFERENCES,0.2506203473945409,"[11] M. C. du Plessis, G. Niu, and M. Sugiyama. Analysis of learning from positive and unlabeled
data. In NIPS, 2014."
REFERENCES,0.2531017369727047,"[12] C. Elkan and K. Noto. Learning classifiers from only positive and unlabeled data. In KDD,
2008."
REFERENCES,0.2555831265508685,"[13] B. Eysenbach, S. Levine, and R. Salakhutdinov. Replacing rewards with examples: Example-
based policy search via recursive classification. In NeurIPS, 2021."
REFERENCES,0.25806451612903225,"[14] G. Freund, E. Sarafian, and S. Kraus. A coupled flow approach to imitation learning. In ICML,
2023."
REFERENCES,0.26054590570719605,"[15] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement
learning. In ICLR, 2018."
REFERENCES,0.2630272952853598,"[16] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven
reinforcement learning. ArXiv:2004.07219, 2020."
REFERENCES,0.2655086848635236,"[17] S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods. In ICML, 2018."
REFERENCES,0.2679900744416873,"[18] S. Ghasemipour, R. Zemel, and S. Gu. A divergence minimization perspective on imitation
learning methods. In CoRL, 2019."
REFERENCES,0.2704714640198511,"[19] A. L. Gibbs and F. E. Su. On choosing and bounding probability metrics. International
statistical review, 2002."
REFERENCES,0.2729528535980149,"[20] K. Hakhamaneshi, R. Zhao, A. Zhan, P. Abbeel, and M. Laskin. Hierarchical few-shot imitation
with skill transition models. In ICLR, 2022."
REFERENCES,0.27543424317617865,[21] G. E. Hinton. Connectionist learning procedures. 1990.
REFERENCES,0.27791563275434245,"[22] J. Ho and S. Ermon. Generative adversarial imitation learning. In NIPS, 2016."
REFERENCES,0.2803970223325062,"[23] G. hyeong Kim, J. Lee, Y. Jang, H. Yang, and K. Kim. Lobsdice: Offline learning from
observation via stationary distribution correction estimation. In NeurIPS, 2022."
REFERENCES,0.28287841191067,"[24] S. Höfer, K. Bekris, A. Handa, J. C. Gamboa, M. Mozifian, F. Golemo, C. Atkeson, D. Fox,
K. Goldberg, J. Leonard, C. Karen Liu, J. Peters, S. Song, P. Welinder, and M. White. Sim2real
in robotics and automation: Applications and challenges. IEEE Transactions on Automation
Science and Engineering, 2021."
REFERENCES,0.2853598014888337,"[25] Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline rl? In ICML, 2021."
REFERENCES,0.2878411910669975,"[26] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims. Morel: Model-based offline
reinforcement learning. In NeurIPS, 2020."
REFERENCES,0.2903225806451613,"[27] G. Kim, S. Seo, J. Lee, W. Jeon, H. Hwang, H. Yang, and K. Kim. Demodice: Offline imitation
learning with supplementary imperfect demonstrations. In ICLR, 2022."
REFERENCES,0.29280397022332505,"[28] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015."
REFERENCES,0.29528535980148884,"[29] R. Kiryo, G. Niu, M. C. du Plessis, and M. Sugiyama. Positive-unlabeled learning with
non-negative risk estimator. In NIPS, 2017."
REFERENCES,0.2977667493796526,"[30] J. Kober and J. Peters. Policy search for motor primitives in robotics. In NIPS, 2008."
REFERENCES,0.3002481389578164,"[31] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor-critic:
Addressing sample inefficiency and reward bias in adversarial imitation learning. In ICLR,
2019."
REFERENCES,0.3027295285359802,"[32] I. Kostrikov, O. Nachum, and J. Tompson. Imitation learning via off-policy distribution matching.
In ICLR, 2020."
REFERENCES,0.3052109181141439,"[33] A. Kumar, S. Gupta, and J. Malik. Learning navigation subroutines from egocentric videos. In
CoRL, 2019."
REFERENCES,0.3076923076923077,"[34] J. Lee, W. Jeon, B.-J. Lee, J. Pineau, and K.-E. Kim. Optidice: Offline policy optimization via
stationary distribution correction estimation. In ICML, 2021."
REFERENCES,0.31017369727047145,"[35] Y. Lee, A. Szot, S.-H. Sun, and J. J. Lim. Generalizable imitation learning from observation via
inferring goal proximity. In NeurIPS, 2021."
REFERENCES,0.31265508684863524,"[36] A. Li, B. Boots, and C.-A. Cheng. Mahalo: Unifying offline reinforcement learning and
imitation learning from observations. ArXiv:2303.17156, 2023."
REFERENCES,0.315136476426799,"[37] K. Li, A. Gupta, A. Reddy, V. H. Pong, A. Zhou, J. Yu, and S. Levine. Mural: Meta-learning
uncertainty-aware rewards for outcome-driven reinforcement learning. In ICML, 2021."
REFERENCES,0.3176178660049628,"[38] X.-L. Li, B. Liu, and S.-K. Ng. Learning to identify unexpected instances in the test set. In
IJCAI, 2007."
REFERENCES,0.3200992555831266,"[39] C. Luo, P. Zhao, C. Chen, B. Qiao, C. Du, H. Zhang, W. Wu, S. Cai, B. He, S. Rajmohan, et al.
Pulns: Positive-unlabeled learning with effective negative sample selector. In AAAI, 2021."
REFERENCES,0.3225806451612903,"[40] Y. Luo, Z. Jiang, S. Cohen, E. Grefenstette, and M. P. Deisenroth. Optimal transport for offline
imitation learning. In ICLR, 2023."
REFERENCES,0.3250620347394541,"[41] Y. J. Ma, A. Shen, D. Jayaraman, and O. Bastani. Smodice: Versatile offline imitation learning
via state occupancy matching. In ICML, 2022."
REFERENCES,0.32754342431761785,"[42] O. Nachum, Y. Chow, B. Dai, and L. Li. Dualdice: Behavior-agnostic estimation of discounted
stationary distribution corrections. NeurIPS, 2019."
REFERENCES,0.33002481389578164,"[43] T. Osa and M. Sugiyama. Hierarchical policy search via return-weighted density estimation. In
AAAI, 2018."
REFERENCES,0.3325062034739454,"[44] J. Pari, N. M. M. Shafiullah, S. P. Arunachalam, and L. Pinto. The surprising effectiveness of
representation learning for visual imitation. ArXiv:2112.01511, 2021."
REFERENCES,0.3349875930521092,"[45] X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and
scalable off-policy reinforcement learning. ArXiv:1910.00177, 2019."
REFERENCES,0.337468982630273,"[46] K. Pertsch, Y. Lee, and J. J. Lim. Accelerating reinforcement learning with learned skill priors.
In CoRL, 2020."
REFERENCES,0.3399503722084367,"[47] K. Pertsch, Y. Lee, Y. Wu, and J. J. Lim. Demonstration-guided reinforcement learning with
learned skills. In CoRL, 2021."
REFERENCES,0.3424317617866005,"[48] J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational
space control. In ICML, 2007."
REFERENCES,0.34491315136476425,"[49] J. Peters, K. Mulling, and Y. Altun. Relative entropy policy search. In AAAI, 2010."
REFERENCES,0.34739454094292804,"[50] M. D. Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive and
unlabeled data. In ICML, 2015."
REFERENCES,0.34987593052109184,"[51] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In NIPS, 1988."
REFERENCES,0.3523573200992556,"[52] N. Rajaraman, L. F. Yang, J. Jiao, and K. Ramchandran. Toward the fundamental limits of
imitation learning. In NeurIPS, 2020."
REFERENCES,0.3548387096774194,"[53] R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi,
and Y. Choi. Is reinforcement learning (not) for natural language processing?: Benchmarks,
baselines, and building blocks for natural language policy optimization. In ICLR, 2023."
REFERENCES,0.3573200992555831,"[54] A. Santara, J. Datta, S. Sarkar, A. Garg, K. Padia, and P. Mitra. Punch: Positive unlabelled
classification based information retrieval in hyperspectral images. ArXiv:1904.04547, 2019."
REFERENCES,0.3598014888337469,"[55] P. Sermanet, C. Lynch, J. Hsu, and S. Levine. Time-contrastive networks: Self-supervised
learning from multi-view observation. ArXiv:1704.06888, 2017."
REFERENCES,0.36228287841191065,"[56] H. S. Sikchi, A. Zhang, and S. Niekum. Imitation from arbitrary experience: A dual unification
of reinforcement and imitation learning methods. ArXiv:2302.08560, 2023."
REFERENCES,0.36476426799007444,"[57] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,
D. Kumaran, T. Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and
shogi by self-play with a general reinforcement learning algorithm. Science, 2018."
REFERENCES,0.36724565756823824,"[58] M. Štrupl, F. Faccio, D. R. Ashley, R. K. Srivastava, and J. Schmidhuber. Reward-weighted
regression converges to a global optimum. In AAAI, 2022."
REFERENCES,0.369727047146402,"[59] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IROS,
2012."
REFERENCES,0.37220843672456577,"[60] F. Torabi, G. Warnell, and P. Stone. Behavioral cloning from observation. In IJCAI, 2018."
REFERENCES,0.3746898263027295,"[61] F. Torabi, G. Warnell, and P. Stone. Generative adversarial imitation from observation. In ICML
Workshop on Imitation, Intent, and Interaction, 2019."
REFERENCES,0.3771712158808933,"[62] Q. Wang, J. Xiong, L. Han, H. Liu, T. Zhang, et al. Exponentially weighted imitation learning
for batched historical data. In NeurIPS, 2018."
REFERENCES,0.37965260545905705,"[63] Z. Wang, A. Novikov, K. Zolna, J. S. Merel, J. T. Springenberg, S. E. Reed, B. Shahriari,
N. Siegel, C. Gulcehre, N. Heess, et al. Critic regularized regression. In NeurIPS, 2020."
REFERENCES,0.38213399503722084,"[64] A. Wu, A. Piergiovanni, and M. S. Ryoo. Model-based behavioral cloning with future image
similarity learning. In CoRL, 2019."
REFERENCES,0.38461538461538464,"[65] Y.-H. Wu, N. Charoenphakdee, H. Bao, V. Tangkaratt, and M. Sugiyama. Imitation learning
from imperfect demonstration. In ICML, 2019."
REFERENCES,0.3870967741935484,"[66] F. Xia, C. Li, R. Martín-Martín, O. Litany, A. Toshev, and S. Savarese. ReLMoGen: Leveraging
motion generation in reinforcement learning for mobile manipulation. In ICRA, 2020."
REFERENCES,0.38957816377171217,"[67] D. Xu and M. Denil. Positive-unlabeled reward learning. In CoRL, 2019."
REFERENCES,0.3920595533498759,"[68] H. Xu, X. Zhan, H. Yin, and H. Qin. Discriminator-weighted offline imitation learning from
suboptimal demonstrations. In NeurIPS, 2022."
REFERENCES,0.3945409429280397,"[69] T. Xu, Z. Li, Y. Yu, and Z.-Q. Luo. On generalization of adversarial imitation learning and
beyond. arXiv preprint arXiv:2106.10424, 2021."
REFERENCES,0.3970223325062035,"[70] T. Yu, G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn, and T. Ma. Mopo: Model-
based offline policy optimization. In NeurIPS, 2020."
REFERENCES,0.39950372208436724,"[71] Z. Zhu, K. Lin, B. Dai, and J. Zhou. Off-policy imitation learning from observations. In
NeurIPS, 2020."
REFERENCES,0.40198511166253104,"[72] L. Ziniu, X. Tian, Y. Yang, and L. Zhi-Quan. Rethinking valuedice - does it really improve
performance? In ICLR Blog Track, 2022."
REFERENCES,0.4044665012406948,"[73] K. Zolna, A. Novikov, K. Konyushkova, C. Gulcehre, Z. Wang, Y. Aytar, M. Denil, N. de Freitas,
and S. E. Reed. Offline learning from demonstrations and unlabeled experience. In Offline
Reinforcement Learning Workshop at NeurIPS, 2020."
REFERENCES,0.40694789081885857,"Appendix: A Simple Solution for Offline Imitation from Observations and
Examples with Possibly Incomplete Trajectories"
REFERENCES,0.4094292803970223,"The Appendix is organized as follows: first, we summarize our key findings. Then, in Sec. A, we
provide a more rigorous introduction of the key mathematical concepts, and in Sec. B we provide
the pseudocode for the training process of TAILO. In Sec. C, we explain why DICE methods
struggle with incomplete trajectories in the task-agnostic or task-specific data. We then list additional
implementation details of our method and the baselines in Sec. D, include additional experimental
settings in Sec. E, and present additional experiment results as well as ablation studies in Sec. F;
after that, we report the training time and computational resources utilized by each method in Sec. G.
Finally, we examine the licenses of assets for our code in Sec. H. See https://github.com/
KaiYan289/TAILO for our code."
REFERENCES,0.4119106699751861,The key findings of the additional experimental results are summarized as follows:
REFERENCES,0.4143920595533499,"• Can our method discriminate expert and non-expert trajectories in the task-agnostic
dataset? In Sec. F.1, we visualize the change of R(s) and behavior cloning weight W(s, a)
along the trajectory, as well as the average R(s) and W(s, a) for expert and non-expert
trajectories in the task-agnostic dataset DTA. We find that our method successfully dis-
criminates expert and non-expert trajectories in the task-agnostic dataset across multiple
experiments."
REFERENCES,0.41687344913151364,"• How does our method perform compared to other offline RL/IL methods? In addi-
tion to LobsDICE and SMODICE, in the appendix we compare our performance to the
following offline RL/IL methods: a) a very recent DICE method, ReCOIL; b) methods
with extra access to expert actions or reward labels, such as DWBC [68], model-based
RL methods MOReL [26] and MOPO [70]; c) offline adaption for Advantage-Weighted
Regression(AWR) [45], MARWIL [62]; d) a recent Wasserstein-based offline IL method,
OTR [40]. We found our method to be consistently better than all these methods. See
Sec. F.2 for details."
REFERENCES,0.41935483870967744,"• How does our method and baselines perform on the kitchen environment with “Kitchen-
Complete-v0”? The kitchen testbed adopted in SMODICE uses an expert trajectory from
the “kitchen-complete-v0” environment in D4RL, which has a different subtask sequence
from the ones evaluated in the environment and the ones in the task-agnostic data (“kitchen-
mixed-v0”); the two environments also have different state spaces. When changed to
“kitchen-complete-v0,” all baselines except BC fail, because the state space between task-
agnostic data and evaluation environment is different. Nonetheless our method still works.
See Sec. F.3 for details."
REFERENCES,0.4218362282878412,"• How sensitive is our method to hyperparameters, such as α, β1, β2, ηp, and γ? We
conduct a sensitivity analysis in Sec. F.4, and find our method to be generally robust to a
reasonable selection of hyperparameters."
REFERENCES,0.42431761786600497,"• Our method and baselines use different batch sizes. Is this a fair comparison? In
Sec. F.4.6, we test our method with batch size 512 and SMODICE/LobsDICE with batch
size 8192. We find that our method is more stable with a larger batch size for a few
settings, but SMODICE and LobsDICE do not generally benefit from an increased batch
size. In addition, our method with batch size 512 is still better than the baselines. Thus, the
comparison is fair to SMODICE and LobsDICE."
REFERENCES,0.4267990074441687,"• How important is the Lipschitz regularizer for training of R(s)? A Lipschitz regularizer
is used in many methods tested in this work, including our method, SMODICE, LobsDICE,
and ORIL. In Sec. F.4.7, we found this regularizer to be crucial for the generalization of the
discriminator and R(s)."
REFERENCES,0.4292803970223325,"• Our standard imitation from observation setting is different from that of SMODICE.
Is this fair? Compared with the SMODICE’s original setting, in Sec. 4.3 we focused on a
more challenging experiment setting with less expert trajectories in the task-agnostic dataset
for the four mujoco environments (hopper, halfcheetah, ant, and walker2d). In the appendix,
we also provide experimental evaluation under the SMODICE’s original setting, and find
our method still outperforming baselines. See Sec. F.5 for details."
REFERENCES,0.4317617866004963,"• How is our method different from ORIL, and where does the performance gain come
from? Our method is different from ORIL in three major aspects: definition of R(s),
positive-unlabeled learning technique, and policy retrieval. In Sec. F.6, we report that each
of the three aspects contributes to the reward increase; among them, policy retrieval is the
most important factor, and the positive-unlabeled learning technique is the least."
REFERENCES,0.43424317617866004,"A
Mathematical Concepts"
REFERENCES,0.43672456575682383,"In this section, we rigorously introduce four mathematical concepts in our paper, which are state(-
action/-pair) occupancy, debiasing objective for positive-unlabeled learning, f-divergences, and
Fenchel conjugate. The first one is used throughout the paper, the second is used in the training of
c(s), and the others are used in Sec. C."
REFERENCES,0.4392059553349876,"State, State-Action, and State-pair Occupancy. Consider an infinite horizon MDP (S, A, T, r, γ)
with initial state distribution p0, where the state at the t-th timestep is st and the action is at.
Then, given any fixed policy π, the probability Pr(st = s) of landing in any state s for any step
t is determined. Based on this, the state visitation frequency dπ(s) with policy π, also known as
state occupancy, is defined as dπ(s) = P∞
t=1 Pr(st = s). Similarly, the state-action frequency
is defined as dπ(s, a) = P∞
t=1 Pr(st = s, at = a), and the state-pair frequency is defined as
dπ(s, s′) = P∞
t=1 Pr(st = s, st+1 = s′). For better readability, with a little abuse of notation, we
use the same d for all three occupancies throughout the paper. In this work, we refer to the occupancy
with the learner’s policy π as dπ, we refer to the occupancy with average policy of the task-agnostic
dataset as dTA, and to the occupancy with average policy of the task-specific dataset as dTS."
REFERENCES,0.44168734491315137,"Debiasing Objective for Positive-Unlabeled Learning. Consider a binary classification task with
feature x ∈Rn and label y ∈{0, 1}. Assume we have access to both labeled positive dataset DP
and labeled negative dataset DN, and the class prior, i.e., the probability of having a label when
uniformly sampled from the dataset, is ηp = Pr(y = 1) for positive labels and ηn = Pr(y = 0) for
negative samples. Then, with sufficiently many samples, the most commonly used loss function is a
cross entropy loss. The average cross entropy loss can be approximated by"
REFERENCES,0.4441687344913151,"ηpE(x,y)∼DP [−log ˆy] + ηnE(x,y)∼DN [−log(1 −ˆy)],
(7)"
REFERENCES,0.4466501240694789,where ˆy = c(x) is the output of the discriminator.
REFERENCES,0.4491315136476427,"In positive-unlabeled learning, we only have access to the positive dataset DP and unlabeled dataset
DU as an unlabeled mixture of positive and negative samples. One naive approach is to regard all
samples from DU as negative samples. While this sometimes works, it falsely uses the loss function
for positive samples in DU, and thus introduces bias. To avoid this and get better classification results,
multiple debiasing objectives [11, 29] have been proposed. Such objectives assume that the unlabeled
dataset DU consists of ηp portion of positive data and ηn portion of negative data. Thus, we have"
REFERENCES,0.45161290322580644,"E(x,y)∼DU [−log(1 −ˆy)] = ηpE(x,y)∼DP [−log(1 −ˆy)] + ηnE(x,y)∼DN [−log(1 −ˆy)].
(8)"
REFERENCES,0.45409429280397023,"Correspondingly, Eq. (7) can be approximated by"
REFERENCES,0.456575682382134,"−[ηpE(x,y)∼DP log ˆy + E(x,y)∼DU log(1 −ˆy) −ηpE(x,y)∼DP log(1 −ˆy)].
(9)"
REFERENCES,0.45905707196029777,"This is the formulation utilized by ORIL [73]. However, as Kiryo et al. [29] pointed out, the red part
in Eq. (9) is an approximation for ηnE(x,y)∼DN [−log(1 −ˆy)], and thus should be no less than 0;
violation of such a rule could lead to overfitting (see [29] for details). Therefore, we apply a max
operator max(·, 0) to the red part. The objective thus becomes"
REFERENCES,0.46153846153846156,"−[ηpE(x,y)∼DP log ˆy + max(E(x,y)∼DU log(1 −ˆy) −ηpE(x,y)∼DP log(1 −ˆy), 0)].
(10)"
REFERENCES,0.4640198511166253,"Note that while Kiryo et al. [29] uses an alternative update conditioning on the red term, we found
the max operator to be more effective in our case. For our first step of discriminator training, we use
Eq. (10), with DTS as DP , DTA as DU, state s as feature x, and action a as label y. For the second
step, we use Eq. (10) again for task-specific data with mismatch dynamics, but this time we use"
REFERENCES,0.4665012406947891,"DTA safe as DU; otherwise, we use Eq. (7) with DTS as DP , DTA safe as DN, state s as feature x, and
action a as label y."
REFERENCES,0.46898263027295284,"f-divergences. The f-divergence is the basis of the DICE family of methods [23, 27, 32, 34, 41].
DICE methods minimize an f-divergence, such as the KL-divergence between the learner’s policy
and the average policy on the task-specific dataset. For any continuous and convex function f, any
domain X, and two probability distributions p, q on X, the f-divergence between p and q is defined
as"
REFERENCES,0.47146401985111663,Df(p∥q) = Ex∼q[f(p(x)
REFERENCES,0.4739454094292804,"q(x))].
(11)"
REFERENCES,0.47642679900744417,"For example, for the KL-divergence, f(x)
=
x log x, and Df(p∥q)
=
KL(p∥q)
=
Ex∼q[ p(x)"
REFERENCES,0.47890818858560796,q(x) log p(x)
REFERENCES,0.4813895781637717,q(x)] = Ex∼p log p(x)
REFERENCES,0.4838709677419355,"q(x); for χ2-divergence, f(x) = (x −1)2, and Df(p∥q) ="
REFERENCES,0.48635235732009924,χ2(p∥q) = Ex∼q( p(x)
REFERENCES,0.48883374689826303,"q(x) −1)2 =
R"
REFERENCES,0.4913151364764268,"x∼X
(p(x)−q(x))2"
REFERENCES,0.49379652605459057,"q(x)
. Note that SMODICE uses f(x) = 1"
REFERENCES,0.49627791563275436,"2(x −1)2,
which is essentially half χ2-divergence."
REFERENCES,0.4987593052109181,"Fenchel Conjugate. For any vector space Ωwith inner product ⟨·, ·⟩and convex and differentiable
function f : Ω→R, the Fenchel conjugate of f(x), x ∈Ωis defined as"
REFERENCES,0.5012406947890818,"f∗(y) = max
x∈Ω⟨x, y⟩−f(x).
(12)"
REFERENCES,0.5037220843672456,"In SMODICE, the derivation of the χ2-divergence uses f(x) = 1"
REFERENCES,0.5062034739454094,"2(x −1)2, and the Fenchel dual
is f∗(y) = 1"
REFERENCES,0.5086848635235732,"2(y + 1)2. However, such Fenchel dual is obtained with no constraint on x, while in
SMODICE with KL-divergence, x is constrained on a probability simplex. This extra relaxation
could be a factor for its worse performance on some testbeds."
REFERENCES,0.511166253101737,"B
Algorithm Details"
REFERENCES,0.5136476426799007,Alg. 1 shows the pseudocode of our method.
REFERENCES,0.5161290322580645,"C
Why DICE Struggles with Incomplete Trajectories?"
REFERENCES,0.5186104218362283,"In our experiments, we empirically find that SMODICE and LobsDICE struggle with either incom-
plete task-specific or incomplete task-agnostic trajectories. We give an extended explanation in this
section."
REFERENCES,0.5210918114143921,"C.1
Incomplete Task-Specific Trajectory"
REFERENCES,0.5235732009925558,"The phenomenon that DICE struggles with incomplete expert trajectories is first discussed in [72]:
the work mentions that subsampled (i.e., incomplete) trajectories artifically “mask” some states
and causes the failure of ValueDICE. Recent discussion indicates that such a failure on subsampled
task-specific trajectories is closely related to overfitting [6] and a lack of generalizability [69, 72]."
REFERENCES,0.5260545905707196,"C.2
Incomplete Task-Agnostic Trajectory"
REFERENCES,0.5285359801488834,"C.2.1
KL-Based Formulation"
REFERENCES,0.5310173697270472,"The reason that SMODICE [41] with KL-divergence and LobsDICE [23] struggle with incom-
plete task-agnostic trajectories is more direct: consider the final objective of SMODICE with
KL-divergence:"
REFERENCES,0.533498759305211,"min
V (1 −γ)Es∼p0[V (s)] + log E(s,a,s′)∼DTA exp[R(s) + γV (s′) −V (s)],
(13)"
REFERENCES,0.5359801488833746,"where p0 ∈∆(S) is the initial state distribution over the state space S, and “reward function”
R(s) = log dTS(s)"
REFERENCES,0.5384615384615384,dTA(s) is labeled by a discriminator c(s) as described in Sec. 2.
REFERENCES,0.5409429280397022,"Similarly, the final objective of LobsDICE is"
REFERENCES,0.543424317617866,"min
V (1 −γ)Es∼p0V (s) + (1 + α) log E(s,a,s′))∼DTA exp[(
1
1 + α(R(s, s′) + γV (s′) −V (s))], (14)"
REFERENCES,0.5459057071960298,"where the “reward function” R(s, s′) is based on a state pair instead of a single state, and α > 0 is a
hyperparameter. Note, both methods use a 1-sample estimate for the expectation of the dual variable
Es′V (s′) for future state s′."
REFERENCES,0.5483870967741935,"Assume that no two states in DTA are exactly the same, which is common in a high-dimensional
continuous state space. In such a case, there are only two occurrences of V (s) for a particular state
s ∈DTA in Eq. (13) and Eq. (14): for initial states, one in the linear term and the other in −V (s)
inside the exp-term; for other states, one in the −V (s) term and the other in the γV (s′) term inside
the exp-term. Consider a trajectory τ = {(s1, a1, s2), (s2, a2, s3), . . . , (sn, an, sn+1)} ∈DTA.2 If
(s2, a2, s3) is missing, we have the following ways to make up:"
REFERENCES,0.5508684863523573,"1. Use DTA without extra handling. In this case, the only occurrence of V (s3) will be −V (s3),
as no future state exists for s2. As the objective is monotonic (which is not the case in RL)
with unconstrained V (s3), the convergence of the algorithm solely relies on smoothing from
nearby states in DTA. This is also the method that we tested in Sec. F."
REFERENCES,0.5533498759305211,"2. Let the trajectory contain (s1, a1, s3). In this case, the method would learn with a wrong
dynamic of the environment because s1 cannot transit to s3 by conducting action a1 as such
a ground truth would suggest;"
REFERENCES,0.5558312655086849,"3. Consider s2 as a terminal state of the trajectory τ, where the agent could learn to halt in the
middle of an expert trajectory if τ is an expert trajectory;"
REFERENCES,0.5583126550868487,"4. Train an inverse dynamic model for pseudolabeling of the action. However, such a method
cannot deal with more than one consecutive transition missing."
REFERENCES,0.5607940446650124,"In conclusion, none of the workarounds described above addresses the concern. Note that the problem
described here is also valid for ValueDICE [32] and many follow-up works such as DemoDICE [27]
as long as the Donsker-Varadhan representation [4] of the KL-divergence is used. Fig. 7 illustrates
this divergence, which empirically verifies our motivation."
REFERENCES,0.5632754342431762,"(a) Minimum V (s)
(b) Maximum V (s)
(c) Normalized Reward"
REFERENCES,0.56575682382134,"Figure 7: Visualization of collapsing patterns of SMODICE-KL in Sec. 4.1 (walker_1/5 is collpasing; walker_1/3
is not). Walker_1/5 stops early due to NaN values in training, which we denote as 0 reward in our paper. It is
clearly shown that the reward decrease is closely related to V (s) divergence."
REFERENCES,0.5682382133995038,"C.2.2
χ2-Based Formulation"
REFERENCES,0.5707196029776674,"Empirically, we found that SMODICE with χ2-divergence struggles on several testbeds such as
halfcheetah, hopper, and walker2d, which is consistent with the results reported by SMODICE. The
performance gap could be due to the following two reasons:"
REFERENCES,0.5732009925558312,"First, violation of Theorem 1 in SMODICE [41]. Theorem 1 in SMODICE reads as follows:"
REFERENCES,0.575682382133995,"Theorem 1. Given the assumption that dTA(s) > 0 whenever dTS(s) > 0, we have"
REFERENCES,0.5781637717121588,"2Because DICE methods utilize the next state s′ for a state-action pair (s, a), we write the state-action
trajectories as transition (s, a, s′) trajectories."
REFERENCES,0.5806451612903226,KL(dπ(s)∥dTS(s)) ≤Es∼dπ[log(dTA(s)
REFERENCES,0.5831265508684863,"dTS(s))] + KL(dπ(s, a)∥dTA(s, a)),
(15)"
REFERENCES,0.5856079404466501,"and furthermore, for any f-divergence Df larger than KL,"
REFERENCES,0.5880893300248139,KL(dπ(s)∥dTS(s)) ≤Es∼dπ[log(dTA(s)
REFERENCES,0.5905707196029777,"dTS(s))] + Df(dπ(s, a)∥dTA(s, a)).
(16)"
REFERENCES,0.5930521091811415,"The right hand side of Eq. (15) is the optimization objective of SMODICE with KL-divergence, and
χ2-divergence is introduced via Eq. (16) as an upper bound."
REFERENCES,0.5955334987593052,"However, SMODICE uses f(x) = 1"
REFERENCES,0.598014888337469,"2(x −1)2 instead of f(x) = (x −1)2 as the χ2-divergence;
i.e., the χ2-divergence is halved in the objective. While the χ2-divergence is an upper bound of
the KL-divergence [19], half of the χ2-divergence is not. For example, consider two binomial
distributions B(1, p) and B(1, q). The KL-divergence between the two is p log p"
REFERENCES,0.6004962779156328,"q + (1 −p) log 1−p 1−q ,"
REFERENCES,0.6029776674937966,and the χ2-divergence is (p−q)2
REFERENCES,0.6054590570719603,"q
+ (q−p)2"
REFERENCES,0.607940446650124,"1−q . When p = 0.99 and q = 0.9, the KL-divergence is"
REFERENCES,0.6104218362282878,"0.99 log 1.1 + 0.01 log 0.1 ≈0.0713, and the χ2-divergence is 0.092"
REFERENCES,0.6129032258064516,0.9 + 0.092
REFERENCES,0.6153846153846154,"0.1 = 0.09, where half of
the χ2 divergence is smaller than the KL-divergence. Thus, SMODICE with χ2-divergence is not
optimizing an upper bound of KL(dπ(s)∥dTS(s)) and the performance is not guaranteed. In Fig. 8,
we plot the reward curves of using halved and full χ2-divergence under the settings of Sec. F.5; we
find that using full χ2-divergence significantly increases performance on the walker2d environment
and performs similarly on other environments, though still much worse than our method."
REFERENCES,0.6178660049627791,"Figure 8: Performance comparison between using halved χ2-divergence (purple) adopted by SMODICE [41]
and full χ2-divergence (fuchsia) with experiment settings in Sec. F.5. Full χ2-divergence performs significantly
better on the walker2d environment than halved χ2-divergence, though still much worse than our method."
REFERENCES,0.6203473945409429,"Second, SMODICE with χ2-divergence uses f∗(y) =
1
2(y + 1)2 as the Fenchel conjugate of
f(x) = 1"
REFERENCES,0.6228287841191067,"2(x −1)2. Such a conjugate is obtained when there is no constraint on x, i.e., x ∈R;
however, such x, as indicated by the derivation of SMODICE with KL-divergence (Example 1 of
Appendix C in SMODICE), should be on the probability simplex. This relaxation could be another
reason for the performance drop in SMODICE with χ2-divergence."
REFERENCES,0.6253101736972705,"C.3
Other Recent Works"
REFERENCES,0.6277915632754343,"We noticed that there are also recent works in the DICE family that try to address the issue of
incomplete (i.e., subsampled) trajectories, such as SparseDICE [6] and CFIL [14] for online imitation
learning; LobsDICE [23] also discusses learning with subsampled expert trajectories. However, all
those works only focus on incomplete task-specific (i.e. expert) trajectories instead of task-agnostic
trajectories; also, our method can solve example-based IL and task-specific trajectories with all but
one state provided at the beginning, which differs from their settings where the sampling of expert
states/state-action pairs is uniform throughout the trajectory."
REFERENCES,0.630272952853598,"D
Additional Implementation Details"
REFERENCES,0.6327543424317618,"Our code is provided in the supplementary material. We implement our algorithm from scratch, and
use the implementation of SMODICE [41] (https://github.com/JasonMa2016/SMODICE) as"
REFERENCES,0.6352357320099256,"the codebase for SMODICE, ORIL [73], and RCE [13]. We obtain the code for LobsDICE [23] from
their publicized supplementary material on OpenReview."
REFERENCES,0.6377171215880894,"Tab. 1 summarizes the unique hyperparameters for our method, and Tab. 2 summarizes the common
training paradigms for our method and the baselines. For all baselines, if the hyperparameter values
in the paper and the code are different, we record the values from the code. We discuss the influence
of batch size and sensitivity of our hyperparameters in Sec. E. Tab. 3 summarizes the hyperparameters
specific to other methods. SMODICE, ORIL, and RCE first train a discriminator, and then jointly
update actor and critic, while LobsDICE jointly updates all three networks."
REFERENCES,0.6401985111662531,"E
Additional Experimental Settings"
REFERENCES,0.6426799007444168,"In this section, we describe in detail how the environment is setup and how the dataset is generated."
REFERENCES,0.6451612903225806,"E.1
Offline Imitation Learning from Task-Agnostic Dataset with Incomplete Trajectories"
REFERENCES,0.6476426799007444,"Environment Settings. Sec. 4.1 tests four mujoco environments: hopper, halfcheetah, ant, and
walker2d. The detailed configuration for each environment is described as follows:"
REFERENCES,0.6501240694789082,"• Hopper. In this environment, an agent needs to control a 2D single-legged robot to jump
forward by controlling the torques on its joints. The action space A is [−1, 1]3, one
dimension for each joint; the state space S is 11-dimensional, which describes its current
angle and velocity. Note that the x-coordinate is not a part of the state, which means that
the expert state approximately repeats itself periodically. Thus, in Fig. 10 the expert R(s)
is periodic. For this environment as well as halfcheetah, ant, and walker2d, the reward is
gained by surviving and moving forward, and the episode lasts 1,000 steps."
REFERENCES,0.652605459057072,"• Halfcheetah. Similar to hopper, an agent needs to control a 2D cheetah-like robot with
torques on its joints to move forward. The action space A is [−1, 1]6, and the state space S
is 17-dimensional describing its coordinate and velocity."
REFERENCES,0.6550868486352357,"• Ant. In this environment, the agent controls a four-legged robotic ant to move forward in a
3D space with a 111-dimensional state space describing the coordinate and velocity of its
joints, as well as contact forces on each joint. The action space is [−1, 1]8."
REFERENCES,0.6575682382133995,"• Walker2d. The agent in this environment controls a 2D two-legged robot to walk forward,
with state space being 27-dimensional and action space being [−1, 1]8."
REFERENCES,0.6600496277915633,Fig. 9 shows an illustration of each environment.
REFERENCES,0.6625310173697271,"Dataset Settings. We generate our dataset on the basis of SMODICE. SMODICE uses 1 trajectory
(1, 000 states) from the “expert-v2” dataset in D4RL [16] as the task-specific dataset DTS, and
concatenates 200 trajectories (200K state-action pairs) from the “expert-v2” dataset and the whole
“random-v2” dataset in D4RL (which contains 1M state-action pairs) to be the task-agnostic dataset
DTA."
REFERENCES,0.6650124069478908,"Based on this, we use the task-specific dataset from SMODICE. For the task-agnostic dataset, we
take the dataset from SMODICE, concatenate all state-action pairs from all trajectories into an array,
and remove a state-action pair for every x steps; we test x ∈{2, 3, 5, 10, 20} in this work."
REFERENCES,0.6674937965260546,"E.2
Offline Imitation Learning from Task-Specific Dataset with Incomplete Trajectories"
REFERENCES,0.6699751861042184,"Environment Settings. Sec. 4.2 tests hopper, halfcheetah, ant, and walker2d with the same environ-
ment settings as those discussed in Sec. E.1."
REFERENCES,0.6724565756823822,"Dataset Settings. We use the task-agnostic dataset from SMODICE as described in Sec. E.1. For the
task-specific dataset, we take the first x and last y steps from the task-specific dataset of SMODICE
as the new task-specific dataset, and discard the other state-action pairs; in this work, we test
(x, y) ∈{(1, 100), (10, 90), (50, 50), (90, 10), (100, 1)}."
REFERENCES,0.674937965260546,"E.3
Standard Offline Imitation Learning from Observation"
REFERENCES,0.6774193548387096,"Environment Settings. In addition to the four mujoco environments tested in Sec. E.1, Sec. 4.3
tests the kitchen and antmaze environments. The detailed configuration for the two environment is
described as follows:"
REFERENCES,0.6799007444168734,"• Kitchen. In this environment, the agent controls a 9-DoF robotic arm to complete a sequence
of 4 subtasks; possible subtasks include opening the microwave, moving the kettle, turning
on the light, turning on the bottom burner, turning on the top burner, opening the left
cabinet, and opening the right cabinet. The state space is 60-dimensional, which includes
the configuration of the robot, goal location of the items to manipulate, and current position
of the items.
• Antmaze. In this environment, the agent controls a robotic ant with 29-dimensional state
space and 8-dimensional action space to move from one end of a u-shaped maze to the other
end."
REFERENCES,0.6823821339950372,Fig. 9 illustrates the two environments.
REFERENCES,0.684863523573201,(a) Hopper
REFERENCES,0.6873449131513648,(b) Halfcheetah
REFERENCES,0.6898263027295285,(c) Ant
REFERENCES,0.6923076923076923,(d) Walker2d
REFERENCES,0.6947890818858561,(e) Kitchen
REFERENCES,0.6972704714640199,(f) Antmaze
REFERENCES,0.6997518610421837,Figure 9: Illustration of environments tested in Sec. 4.3 based on OpenAI Gym [5] and D4RL [16].
REFERENCES,0.7022332506203474,"Dataset Settings.
For the four mujoco environments, we take the task-specific dataset from
SMODICE as described in Sec. E.1. As existing methods already perform well on those environ-
ments with SMODICE’s dataset settings (see Sec. F.5 for results), we introduce a more challenging
task-agnostic dataset concatenating 40 trajectories (40K state-action pairs) instead of 200 from the
“expert-v2” dataset and the whole “random-v2” dataset in D4RL as the task-agnostic dataset DTA."
REFERENCES,0.7047146401985112,"For the more challenging kitchen and antmaze environments, we use identical dataset settings
as SMODICE. For the kitchen environment, we use 1 trajectory (184 states) from the “kitchen-
complete-v0” dataset as the task-specific dataset DTS and the whole “kitchen-mixed-v0” dataset as
the task-agnostic dataset DTA, which contains 33 different task sequences with a total of 136, 950
state-action pairs. For the antmaze environment, we use the expert trajectory of length 285 given
by SMODICE as the task-specific dataset DTS. We use the dataset collected by SMODICE as the
task-agnostic dataset DTA, which contains 1, 348, 687 state-action pairs."
REFERENCES,0.707196029776675,"E.4
Offline Imitation Learning from Examples"
REFERENCES,0.7096774193548387,"Environment Settings. Sec. 4.4 tests antmaze and kitchen in the same environment settings as the
ones discussed in Sec. E.3, but we only require the agent to complete one particular subtask instead"
REFERENCES,0.7121588089330024,"of all four (we test “opening the microwave” and “moving the kettle”). Additionally, Sec. 4.4 tests
the pointmaze environment, which is a simple environment where the agent controls a pointmass to
move from the center of an empty 2D plane to a particular direction. The action is 2-dimensional; the
state is 4-dimensional, which describes the current coordinate and velocity of the pointmass."
REFERENCES,0.7146401985111662,"Dataset Settings. For the antmaze environment, we use the same task-agnostic dataset as the one
discussed in Sec. E.3, and we use 500 expert final states selected by SMODICE as the task-specific
dataset. For the kitchen environment, we use the same task-agnostic dataset as the one discussed in
Sec. E.3, and randomly select 500 states from the “kitchen-mixed-v0” dataset that has the subtask of
interest completed as the task-specific dataset. For the pointmaze environment, we use a trajectory of
603 expert trajectories with 50K state-action pairs as the task-agnostic dataset, where the trajectories
moving up, down, left and right each take up 1/4. We use the last state of all trajectories that move
left as the task-specific dataset."
REFERENCES,0.71712158808933,"E.5
Offline Imitation Learning from Mismatched Dynamics"
REFERENCES,0.7196029776674938,"Environment Settings. Sec. 4.1 tests halfcheetah, ant, and antmaze with the same environment
settings as the ones discussed in Sec. E.1 and Sec. E.3."
REFERENCES,0.7220843672456576,"Dataset Settings. We use the task-agnostic dataset from Sec. E.1 (ant, halfcheetah) and Sec. E.3
(antmaze). For the task-specific dataset, we use the data generated by SMODICE, which is a single
trajectory conducted by an agent with different dynamics. More specifically, for halfcheetah, we use
an expert trajectory of length 1, 000 by a cheetah with a much shorter torso; for ant, we use an expert
trajectory of length 1, 000 by an ant with one leg crippled; for antmaze, we use an expert trajectort of
length 173 by a pointmass instead of an ant (and thus only the first two dimensions describing the
current location are used for training the discriminator). See Appendix H of SMODICE [41] for an
illustration of the environments."
REFERENCES,0.7245657568238213,"F
Additional Experiment Results"
REFERENCES,0.7270471464019851,"F.1
Can Our Method Discriminate Expert and Non-Expert Data in the Task-Agnostic
Dataset?"
REFERENCES,0.7295285359801489,"In Sec. 4, an important problem related to our motivation is left out due to page limit: Does the
algorithm really succeed in discriminating expert and non-expert trajectories and segments in the
task-agnostic data? We answer the question here with Fig. 10. It shows R(s) and the weight for
behavior cloning W(s, a) along an expert and a non-expert trajectory in DTA on halfcheetah in
Sec. 4.3, hopper with first 50 step and last 50 step as task-specific dataset in Sec. 4.2, and pointmaze
in Sec. 4.4. We also plot the average R(s) and weight W(s, a) for state-action pairs in all expert
trajectories and non-expert trajectories in DTA. The result clearly shows that, even if the discriminator
cannot tell the expert states from the non-expert states at the beginning of the episode because states
from all trajectories are similar to each other, the weight successfully propagates from the later expert
state to the early trajectory, and thus the agent knows where to go even when being close to the
starting point; also, the weight W(s, a) is smoother than the raw R(s), which prevents the agent from
being lost in the middle of the trajectory because of a few steps with very small weights."
REFERENCES,0.7320099255583127,"F.2
Comparison to Other Baselines"
REFERENCES,0.7344913151364765,"Besides the baselines tested in the main paper, we compare our method to a variety of other offline
RL/IL methods. The tested methods include: a) a very recent method, ReCOIL; b) methods with
extra access to expert actions or reward labels, such as DWBC [68], model-based RL methods
MOReL [26] and MOPO [70]; c) offline adaption for Advantage-Weighted Regression (AWR) [45],
MARWIL [62]; d) a recent Wasserstein-based offline IL method, OTR [40]. As we do not have the
code for ReCOIL, and model-based RL methods takes a long time to run on our testbed, we test our
method on their testbeds and use their reported numbers for comparison; for other methods, we test
on our testbed (both under SMODICE setting in Sec. F.5 and standard LfO setting in Sec. 4.3). We
found our method to consistently outperform all methods, even those with extra access to reward
labels or expert actions."
REFERENCES,0.7369727047146402,"Figure 10: Illustration of R(s), behavior cloning weight W(s, a), and their average on expert and non-expert
trajectories in DTA. Curves for expert trajectories are plotted in red color, while non-expert trajectories are
plotted in green color. The non-expert trajectory on the hopper environment is short because the agent topples
quickly and the episode is terminated early. It is clear that our design of weights propagates the large weight
from later expert states and smoothes the weight along the trajectory. Hopper has a large periodic change of
R(s), because the velocity vector for the joints of the agent is periodic."
REFERENCES,0.739454094292804,"F.2.1
ReCOIL"
REFERENCES,0.7419354838709677,"Similar to Sec. 4.1, we test on the four common mujoco environments, which are hopper, halfcheetah,
ant, and walker2d; the results we tested include random + expert (identical to SMODICE), random
+ few expert (30 expert trajectories instead of 40 in Sec. 4.3), and medium + expert (substitute
the random dataset to medium dataset; see ReCOIL for details). The result of random + expert is
illustrated in Fig. 26, where our method outperforms ReCOIL. The result of random + few expert is
illustrated in Fig. 11, and the result of medium + expert is illustrated in Fig. 12. On all test beds, our
method is significantly better than ReCOIL."
REFERENCES,0.7444168734491315,"Figure 11: Performance comparison between our method and ReCOIL on the “random+few expert” testbed of
ReCOIL."
REFERENCES,0.7468982630272953,"Figure 12: Performance comparison between our method and ReCOIL on the “medium+expert” testbed of
ReCOIL."
REFERENCES,0.749379652605459,"F.2.2
Model-Based RL"
REFERENCES,0.7518610421836228,"In this section, we compare our method to MOReL [26] and MOPO [70] which are provided with
ground-truth reward labels, i.e., MOReL and MOPO have an advantage. The trajectory with the
best return is provided as the task-specific dataset to our method. Tab. 4 shows the performance
comparison between our method, MOReL and MOPO; despite being agnostic to reward labels, our
method is still marginally better than MOReL (74.3 vs. 72.9 reward averaged over 9 widely tested
environments {halfcheetah, hopper, walker2d} × {medium, medium-replay, medium-expert}), and
much better than MOPO (74.3 vs. 42.1 average reward)."
REFERENCES,0.7543424317617866,"F.2.3
DWBC, MARWIL and OTR"
REFERENCES,0.7568238213399504,"In this section, we additionally compare our method to three other baselines in offline IL: DWBC [68]
that also trains a discriminator using Positive-Unlabeled (PU) learning, MARWIL [62] that is a naive
adaptation of Reward-Weighted Regression [48] to offline scenarios with a similar actor objective as
TAILO, and the recent Wasserstein-based method OTR [40] which computes Wasserstein distance
between the task-specific and task-agnostic trajectories, assigns reward label based on optimization
result and conducts offline RL. Among those methods, MARWIL and OTR can be directly applied to
our scenario, while DWBC requires extra access to expert actions. Fig. 13 and Fig. 14 illustrate the
result, respectively on the settings of Sec. F.5 and Sec. 4.3."
REFERENCES,0.7593052109181141,"Figure 13: Performance comparison between our method and DWBC, MARWIL and OTR on settings of Sec. F.5.
Our method outperforms all other methods, which all struggle in our settings."
REFERENCES,0.7617866004962779,"Figure 14: Performance comparison between our method and DWBC, MARWIL and OTR on settings of Sec. 4.3.
Our method outperforms all other methods, which all struggle in our settings."
REFERENCES,0.7642679900744417,"F.3
Kitchen Environment Evaluated on “Kitchen-Complete-V0”"
REFERENCES,0.7667493796526055,"In Sec. 4.3, we mentioned that the SMODICE experiment uses expert trajectory that is only expert
for the first 2 out of 4 subtasks. More specifically, SMODICE uses the expert trajectory in the
“kitchen-complete-v0” environment in D4RL as the task-specific dataset, and uses data from the
“kitchen-mixed-v0” environment as the task-agnostic dataset; they also evaluate on “kitchen-mixed-
v0.” However, the subtask list to complete in “kitchen-complete-v0” is {Microwave, Kettle, Light
Switch, Slide Cabinet}, while the subtask list to complete in “kitchen-mixed-v0” is {Microwave,
Kettle, Bottom Burner, Light Switch}.3 Thus, the list of subtasks that is accomplished in expert"
SEE,0.7692307692307693,"3See
https://github.com/Farama-Foundation/D4RL/blob/master/d4rl/kitchen/__init__.
py in D4RL and lines 29, 164 and 208 in https://github.com/JasonMa2016/SMODICE/blob/main/run_
oil_observations.py for details."
SEE,0.771712158808933,"trajectory (from “kitchen-complete-v0”) and the list of subtasks evaluated by the environment (from
“kitchen-mixed-v0”) are only identical in the first two subtasks. We follow their setting in Sec. 4.3,
and this is the reason why the normalized reward for any method is hard to reach 60 in Fig. 4, i.e.,
reach an average completion of 2.4 tasks."
SEE,0.7741935483870968,"We now evaluate on “kitchen-complete-v0,” and Fig. 15 shows the result when evaluating on this
environment. The other settings remain identical to the ones discussed in Sec. 4.3. Note, this new
evaluation setting introduces another challenge – since “kitchen-complete-v0” has a different set
of subtasks to accomplish, the dimensions masked to zero differ; to be more specific, the 41st and
42nd dimensions are zero-masked in “kitchen-complete-v0” but not in “kitchen-mixed-v0,” and vice
versa for the 49th dimension. Thus, the task-agnostic data are in a different state space from the
environment for evaluation. This significantly increases the difficulty of the environment, because the
states met by the agent in evaluation are out of distribution from the task-agnostic data. Even under
such a setting, our method as well as BC remains robust, while all the other baselines struggle."
SEE,0.7766749379652605,"Figure 15: Reward curves for the kitchen environment evaluated on “kitchen-complete-v0”; only our method
and BC are robust to the change of state space from task-agnostic data."
SEE,0.7791563275434243,"F.4
Hyperparameter Sensitivity Analysis"
SEE,0.7816377171215881,"In this section, we conduct a sensitivity analysis for the hyperparameters in our method."
SEE,0.7841191066997518,"F.4.1
Effect of α"
SEE,0.7866004962779156,"Fig. 16 illustrates the reward curves tested on the settings of Sec. 4.3 with different α (for scaling of
R(s)), where the value we used throughout the paper is α = 1.25. While an extreme selection of
the hyperparameters leads to a significant decrease of performance (e.g., α = 2.0 for antmaze), our
method is generally robust to the selection of α."
SEE,0.7890818858560794,"F.4.2
Effect of β1"
SEE,0.7915632754342432,"Fig. 17 shows the reward curves tested on the settings of Sec. 4.3 with different β1 (ratio of “safe”
negative samples), where the default setting is β1 = 0.8. Our method is again generally robust to the
selection of β1. Obviously the reward decreases with an extreme selection of β1, such as β1 = 0.3.
Intuitively, such robustness comes from two sources: 1) with small ratio of expert trajectories in the
task-agnostic dataset (< 5%), thus there is little expert data erroneously classified as safe negatives;
2) the Lipschitz-smoothed discriminator yields a good classification margin, and thus the 60%+
trajectories with lower average reward represent all non-expert data well."
SEE,0.794044665012407,"F.4.3
Effect of β2"
SEE,0.7965260545905707,"In this work, we use β2 = 1 when the embodiment of the task-specific dataset DTS is different from
that of the task-agnostic dataset DTA. We use β2 = 0 otherwise. Ideally, after selecting safe negatives
from the first step of the discriminator training, the obtained DTA safe should consist of (nearly)
100% non-expert data, and thus we use β2 = 0; however, when DTS is collected from a different
embodiment, the recognition of “safe negatives” will be much harder, because the states reachable
by the task-specific expert could be different from the agent that was used to collect task-agnostic"
SEE,0.7990074441687345,"Figure 16: Reward curves tested in the settings of Sec. 4.3 with different α. The default α = 1.25 is plotted in
red, lower α in blue, and higher α in green. The further α deviates from the default, the deeper the color. Our
method is generally robust to the selection of α."
SEE,0.8014888337468983,"Figure 17: Reward curves tested in the settings of Sec. 4.3 with different β1. The default β1 is plotted in red,
lower in blue and higher in green, with deeper color indicating further deviation from the default. Our method is
generally robust to the selection of β1."
SEE,0.8039702233250621,"data. Thus, we use the debiasing objective in the formal training step, i.e., β2 = 1. We compare the
reward of using β2 = 0 and β2 = 1 in Fig. 18, where β2 = 1 works significantly better than β2 = 0
on halfcheetah with mismatched dynamics."
SEE,0.8064516129032258,"Figure 18: Reward curves for β2 = 1 (i.e., debiasing objective) and β2 = 0 (i.e., normal cross entropy loss) on
environments with mismatching dynamics. β2 = 1 is better than β2 = 0 on the halfcheetah environment with
mismatched expert dynamics."
SEE,0.8089330024813896,"F.4.4
Effect of ηp"
SEE,0.8114143920595533,"Fig. 19 shows the reward curves tested in the settings of Sec. 4.3 with different ηp, where the default
setting is ηp = 0.2. The results show that our method is robust to the selection of ηp."
SEE,0.8138957816377171,"Figure 19: Reward curves tested on the settings of Sec. 4.3 with different ηp. The default ηp is plotted in red,
lower in blue and higher in green, with deeper color indicating further deviation from default. Our method is
robust to the selection of ηp."
SEE,0.8163771712158809,"F.4.5
Effect of γ"
SEE,0.8188585607940446,"Fig. 20 shows the reward curves tested in the settings of Sec. 4.3 with different γ (decaying factor for
weight propagation along the trajectory); our default setting is γ = 0.98 for the kitchen and pointmaze
environment, and γ = 0.998 otherwise. We found that when γ is too low (e.g., 0.95), weights from
the future expert state cannot be properly propagated to initial states where most trajectories are
similar, and thus the algorithm is more likely to fail; more extremely, when γ = 0, our method works
much worse as shown in Fig. 21 in both Sec. F.5 and Sec. 4.3 settings, which illustrates the necessity
of propagation of future returns. Generally however, our method is robust to the selection of γ."
SEE,0.8213399503722084,"Figure 20: Reward curves tested in the settings of Sec. 4.3 with different γ. The default γ is plotted in red, lower
in blue and higher in green, with deeper color indicating further deviation from the default; note, kitchen uses a
default γ of 0.98 and other environments use 0.998. Our method is robust to the selection of γ."
SEE,0.8238213399503722,"F.4.6
Effect of Batch Size"
SEE,0.826302729528536,"Since we use a different batch size (8192) than SMODICE and LobsDICE (512) for weighted
behavior cloning, one possible concern is that the comparison might be unfair to the DICE methods,
as we process more data. To address the concern, we test our algorithm with batch size 512 and
SMODICE/LobsDICE with batch size 8192 in Sec. 4.1, Sec. 4.2, and Sec. 4.3. The results are
illustrated in Fig. 22, Fig. 23, and Fig. 24. Generally, we found that on most testbeds, our method
with a batch size of 512 is slightly less stable than the default batch size of 8192, but still on par
or better than the baselines; meanwhile, SMODICE and LobsDICE with a batch size 8192 work"
SEE,0.8287841191066998,(a) SMODICE (Sec. F.5) setting
SEE,0.8312655086848635,(b) Sec. 4.3 setting
SEE,0.8337468982630273,"Figure 21: Ablation of our method with γ = 0. γ = 0 struggles on both experiment settings of Sec. F.5 and
Sec. 4.3, showing that γ > 0 is crucial to the performance of our method."
SEE,0.8362282878411911,"slightly better on Sec. 4.3 than batch size 512, but are less stable in other scenarios such as Sec. 4.1
and Sec. 4.2. In conclusion, there is no general performance gain by increasing the batch size for
SMODICE and LobsDICE. Thus, our comparison is fair to SMODICE and LobsDICE."
SEE,0.8387096774193549,"Figure 22: Performance comparison of SMODICE, LobsDICE, and our method with batch size 512 and
8192 on mujoco testbeds of standard offline imitation from observation (Sec. 4.3). Non-default settings
(SMODICE/LobsDICE with batch size 8192, ours with batch size 512) are single lines without standard
deviations, while default settings (SMODICE/LobsDICE with batch size 512, ours with batch size 8192) are
with standard deviations. Our method with batch size 512 works slightly worse, but is still better than baselines;
baselines with batch size 8192 work slightly better than use of a batch size of 512."
SEE,0.8411910669975186,"F.4.7
Effect of Lipschitz Regularizer"
SEE,0.8436724565756824,"A Lipschitz regularizer on the discriminator is used for many methods tested in this paper, such as
our method, SMODICE, LobsDICE, and ORIL. Intuitively, the regularizer makes the classification
margin of the discriminator c(s) smooth, and thus can give higher weights to expert trajectories
in the task-agnostic dataset DTA instead of overfitting to the few states given in the task-specific
dataset DTS. In practice, we found this regularizer to be crucial; Fig. 25 shows the change of average
R(s) = log
c(s)
1−c(s) of all expert trajectories under the setting of Sec. 4.3 with respect to gradient steps.
The result shows that R(s) for the expert trajectories is extremely high when the Lipschitz regularizer
is removed, which indicates severe overfitting and significantly worse performance."
SEE,0.8461538461538461,"Figure 23: Performance comparison of SMODICE, LobsDICE and our method with batch size 512 and 8192 on
mujoco testbeds of offline imitation with incomplete task-agnostic trajectories (Sec. 4.1). Note that SMODICE
and LobsDICE with a larger batch size do not necessarily work better; in contrast, a larger batch size often leads
to less stability on environments such as Hopper_1/2, Walker2d_1/2 and Ant_1/3. Our method with batch size
512 works slightly worse than 8192, but is still on par or better than the baselines."
SEE,0.8486352357320099,"F.5
Performance Comparison Under Identical SMODICE Settings"
SEE,0.8511166253101737,"In Sec. 4.3, we use less expert trajectory in DTA than the SMODICE setting. Fig. 26 shows the result
where we use identical dataset settings as SMODICE, where our method is still the best among all
baselines. Such a result is consistent with that reported in SMODICE. For ReCOIL [56], as the code
is not available, we plot their average reward as a straight line; our method significantly outperforms
ReCOIL."
SEE,0.8535980148883374,"F.6
Ablation Comparing Our Method and ORIL"
SEE,0.8560794044665012,"While our method and ORIL both utilize a discriminator-based R(s) with positive-unlabeled learning,
there are three major differences:"
SEE,0.858560794044665,"1. Definition of R(s). For a discriminator c(s) trained with non-expert samples as label 0
and expert sample as label 1, we use R(s) = log dTS(s)"
SEE,0.8610421836228288,"dTA(s) = log
c(s)
1−c(s), while ORIL uses
R(s) = c(s)."
SEE,0.8635235732009926,"2. Positive-Unlabeled Learning Techniques. The training of discriminator C(s) in ORIL is
only one step, and uses Eq. (9) as the training objective. In contrast, we first use Eq. (10) to
find safe negative samples, and then use Eq. (10) or Eq. (7) (see Sec. A for details) to train
the final discriminator c(s) and consequently obtain R(s)."
SEE,0.8660049627791563,"3. Policy Retrieval. ORIL conducts RL on the offline dataset with reward labeled by R(s),
while our method uses a non-parametric approach to calculate coefficients for weighted
behavior cloning."
SEE,0.8684863523573201,"Figure 24: Performance comparison of SMODICE, LobsDICE and our method with batch size 512 and
8192 on mujoco testbeds of offline imitation with incomplete task-specific trajectories (Sec. 4.2). Again,
larger batch size sometimes is worse for SMODICE and LobsDICE, e.g., for Halfcheetah_head1_tail100 and
Hopper_head1_tail100. Our method with batch size 512 works slightly worse than 8192, but remains on par or
better than the baselines."
SEE,0.8709677419354839,"We compare eight variants of the methods, denoted as ours, ours-V1, ours-V2, ours-V3, ORIL-logR-
V1, ORIL-logR-V2, ORIL-01-V1, and ORIL-01-V2. Tab. 5 summarizes the differences between the
eight variants."
SEE,0.8734491315136477,"The results are illustrated in Fig. 27. Generally, all variants except “ours” work at least marginally
worse than “ours,” which shows that all three differences matter in the final performance. Among
the three variants of “ours,” ours-v2 has the closest performance to “ours,” which indicates that the
positive-unlabeled learning technique is the least important factor, though it still makes a difference on
environments such as halfcheetah with mismatched dynamics as shown in Fig. 28. Moreover, while
ours-v1 (different R(s)) and ours-v3 (different R(s) + ORIL positive-unlabeled learning technique)
are significantly worse than “ours,” ORIL-01-V1, ORIL-01-V2, ORIL-logR-V1, and ORIL-logR-V2
perform even worse and are only marginally different from each other. This indicates that retrieval of
policy (RL vs. non-parametric) is the most important factor for the performance gap."
SEE,0.8759305210918115,"Additionally, in order to illustrate the importance of Positive-Unlabeled (PU) learning in general, we
test the performance of our method without PU learning and find a significant performance decrease
on some scenarios; Fig. 29 illustrates two of the failure cases without PU learning."
SEE,0.8784119106699751,"G
Computational Resource Usage"
SEE,0.8808933002481389,"All our experiments are conducted on an Ubuntu 18.04 server with 72 Intel Xeon Gold 6254 CPUs
@ 3.10GHz and a single NVIDIA RTX 2080Ti GPU. Given these resources, our method requires
about 4 −5 hours to finish training in the standard offline imitation from observation scenario, while
BC needs 3 −4 hours, ORIL, SMODICE, and LobsDICE need about 2.5 −3 hours. In our training
process, training of R(s) (both steps included) requires only 15-20 minutes. The inference speed for
all methods is similar as the actor network is the same, and is thus not a bottleneck."
SEE,0.8833746898263027,"Figure 25: The average R(s) = log
c(s)
1−c(s) of the expert trajectories in the task-agnostic dataset with respect to
the gradient steps of discriminator training on the halfcheetah environment. We use the settings discussed in
Sec. 4.3. The run without Lipschitz regularizer (red curve) overfits to the expert state, and thus has much higher
average R(s). Such a run diverges to infinity in later training."
SEE,0.8858560794044665,"Figure 26: Reward curves for hopper, halfcheetah, ant, and walker2d evaluated under the identical settings as
SMODICE; our method is the best among all methods including ReCOIL."
SEE,0.8883374689826302,"H
Dataset and Algorithm Licenses"
SEE,0.890818858560794,Our code is developed upon multiple algorithm repositories and environment testbeds.
SEE,0.8933002481389578,"Algorithm Repositories. We implement our method and behavior cloning from scratch. We test
RCE, ORIL and SMODICE from the SMODICE repository, which has no license. We get LobsDICE
code from their supplementary material of the publicized OpenReview submission, which also has no
license."
SEE,0.8957816377171216,"Environment Testbeds. We utilize OpenAI gym [5], mujoco [59], and D4RL [16] as testbed, which
have an MIT license, an Apache-2.0 license, and an Apache-2.0 license respectively."
SEE,0.8982630272952854,"Figure 27: Ablation between our method and ORIL; see Tab. 5 for reference to the legend. While ours-V2 with
ORIL-style positive-unlabeled learning has the closest performance to our final algorithm, it is still marginally
worse. ORIL-01 and ORIL-logR fail, which shows that the retrieval of the policy is the most important factor for
the performance gap."
SEE,0.9007444168734491,"Figure 28: Comparison of ours and ours-V2 on the halfcheetah environment with mismatched dynamics
(Sec. 4.5). Ours-V2 fails on this environment."
SEE,0.9032258064516129,"Figure 29: Examples of Failure cases of our method without PU learning; on the left is the halfcheetah
environment with mismatched dynamics 4.5, and on the right is the hopper environment with only first and last
50 steps in the task-specific dataset (Sec. 4.2). In both cases, our method without PU learning struggles while
our method with PU learning succeeds."
SEE,0.9057071960297767,"Algorithm 1: Our Algorithm, TAILO
Input
:state-only task-specific dataset DTS, state-action task-agnostic dataset DTA containing
m trajectories
Input
:Discounted sum rate γ, ratio of “safe” negative sample β1, positive prior ηp
Input
:Number of gradient steps for pretraining n1, formal training n2, and number of epoches
for weighted behavior cloning n3
Input
:Randomly initialized discriminator c(s) and c′(s), parameterized by w and w′
Input
:Learning rate a
Output :Learned policy πθ(a|s), parameterized by θ
begin"
SEE,0.9081885856079405,// Pretraining Discriminator
SEE,0.9106699751861043,"1
for i ∈{1, 2, . . . , n1} do"
SEE,0.913151364764268,"2
Sample s1 ∼DTS, s2 ∼DTA
3
L1 ←−[ηp log c′(s1) + max(0, log(1 −c′(s2)) −ηp log(1 −c′(s1)))]"
SEE,0.9156327543424317,"4
w′ ←w′ −a · ∂L1"
SEE,0.9181141439205955,"∂w′
// Labeling Safe Negative"
SEE,0.9205955334987593,"5
for i ∈{1, 2, . . . , m} do
// Iterating through trajectories"
SEE,0.9230769230769231,"6
foreach s ∈τi do"
SEE,0.9255583126550868,"R′(s) ←log
c′(s)
1−c′(s)"
SEE,0.9280397022332506,¯R(τi) ← P
SEE,0.9305210918114144,s∈τi R(s)
SEE,0.9330024813895782,"|τi|
// average over trajectory"
SEE,0.9354838709677419,"7
q ←β1 quantile of ¯R"
SEE,0.9379652605459057,"8
DsafeTA ←{τ| ¯R(τ) < q}
// Formal training of Discriminator"
SEE,0.9404466501240695,"9
for i ∈{1, 2, . . . , n2} do"
SEE,0.9429280397022333,"10
Sample s1 ∼DTS, s2 ∼DsafeTA
if Expert embodiment is different then"
SEE,0.9454094292803971,"11
L ←−[ηp log c(s1) + max(0, log(1 −c′(s2)) −ηp log(1 −c′(s1)))]
else"
SEE,0.9478908188585607,"12
L ←−[log c(s1) + log(1 −c(s2))]"
SEE,0.9503722084367245,"13
w ←w −a · ∂L"
SEE,0.9528535980148883,"∂w
// Assignment of Weights"
SEE,0.9553349875930521,"14
for i ∈{1, 2, . . . , m} do
// Iterating through trajectories"
SEE,0.9578163771712159,"15
foreach s ∈τi do"
SEE,0.9602977667493796,"16
R(s) ←log
c(s)
1−c(s)
17
W(s, a) ←exp(αR(s))"
SEE,0.9627791563275434,"18
v ←R(sgoal)"
SEE,0.9652605459057072,"1−γ
// sgoal is the last state in τi"
SEE,0.967741935483871,"19
foreach s ∈τi do
// This time in reverse"
SEE,0.9702233250620348,"20
W(s, a) ←v"
SEE,0.9727047146401985,"21
v ←γv + W(s, a)"
SEE,0.9751861042183623,"// average over trajectory
// Weighted Behavior Cloning"
SEE,0.9776674937965261,"22
for j ∈{1, 2, . . . , M2} do
// for loop over epochs"
SEE,0.9801488833746899,"23
foreach (s, a) ∼DTA do
// for each data point"
SEE,0.9826302729528535,"24
L ←W(s, a)πθ(a|s)"
SEE,0.9851116625310173,"25
θ ←θ −a · ∂L ∂θ"
SEE,0.9875930521091811,"Hyperparameter
Value
Meaning
α
1.25
Scaling factor for calculation of weights
β1
0.8
Estimated ratio of safe negative samples
β2
1 (mismatch), 0 (others)
Whether to use debiasing objective in formal training
ηp
0.2
Positive prior
γ
0.98 (kitchen, pointmaze), 0.998 (others)
decay factor in weight propagation along the trajectory
Table 1: Hyperparameters specific to our method."
SEE,0.9900744416873449,"Type
Hyperparameter
ours
BC
LobsDICE
SMODICE
ORIL
RCE
Disc.
Network Size
[256, 256]
N/A
[256, 256]
[256, 256]
[256, 256]
[256, 256]
Activation Function
Tanh
N/A
ReLU
Tanh
Tanh
Tanh
Learning Rate
0.0003
N/A
0.0003
0.0003
0.0003
0.0003
Weight Decay
0
N/A
0
0
0
0
Training Length
(10+40)K steps
N/A
1M steps
1K steps
1K steps
1K steps
Batch Size
512
N/A
512
256
256
256
Optimizer
Adam
N/A
Adam
Adam
Adam
Adam
Actor
Network Size
[256, 256]
[256, 256]
[256, 256]
[256, 256]
[256, 256]
[256, 256]
Activation Function
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
Learning Rate
0.0001
0.0001
0.0003
0.0003
0.0003
0.0003
Weight Decay
10−5
10−5
0
0
0
0
Training length
1M steps
1M steps
1M steps
1M steps
1M steps
1M steps
Batch Size
8192
8192
512
512
512
512
Optimizer
Adam
Adam
Adam
Adam
Adam
Adam
Tanh-Squashed
Yes
Yes
Yes
Yes
Yes
Yes
Critic
Network Size
N/A
N/A
[256, 256]
[256, 256]
[256, 256]
[256, 256]
Activation Function
N/A
N/A
ReLU
ReLU
ReLU
ReLU
Learning Rate
N/A
N/A
0.0003
0.0003
0.0003
0.0003
Weight Decay
N/A
N/A
0.0001
0.0001
0
0
Training Length
N/A
N/A
1M steps
1M steps
1M steps
1M steps
Batch Size
N/A
N/A
512
512
512
512
Optimizer
N/A
N/A
Adam
Adam
Adam
Adam
Discount Factor
N/A
N/A
0.99
0.99
0.99
0.99
Table 2: Training paradigms for our method and baselines; Disc. is the abbreviation for discriminator. [256, 256]
in network size means a network with two hidden layers and width 256. For our method, (10 + 40)K means
10K gradient steps for the first step and 40K for the second step. Tanh-squashed means a Tanh applied at the end
of the output to ensure the output action is legal."
SEE,0.9925558312655087,"Method
Hyperparameter
Value
Notation
LobsDICE
Regularization factor
0.1
α
RCE, ORIL
RL algorithm
TD3 [17]
Policy Update Frequency
2
Policy Noise
0.2
Noise Clip
[−0.5, 0.5]
Target Network Update Rate
0.005
τ
Table 3: Unique hyperparameters for other methods."
SEE,0.9950372208436724,"Environment
MOReL
MOPO
TAILO (Ours)
Halfcheetah-Medium
42.1
42.3
39.8
Hopper-Medium
95.4
28
56.2
Walker2d-Medium
77.8
17.8
71.7
Halfcheetah-Medium-Replay
40.2
53.1
42.8
Hopper-Medium-Replay
93.6
67.5
83.4
Walker2d-Medium-Replay
49.8
39.0
61.2
Halfcheetah-Medium-Expert
53.3
63.3
94.3
Hopper-Medium-Expert
108.7
23.7
111.5
Walker2d-Medium-Expert
95.6
44.6
108.2
Average
72.9
42.1
74.3
Table 4: The performance comparison between our method and model-based RL methods on D4RL mujoco
offline datasets. Our method works even better than the offline RL methods with extra access to the underlying
reward label."
SEE,0.9975186104218362,"Name
R(s)
PU learning
Policy Retrieval
ours
log
c(s)
1−c(s)
two-step with max(·, 0)
non-parametric
ours-V1
10c(s)
two-step with max(·, 0)
non-parametric
ours-V2
log
c(s)
1−c(s)
one step without max(·, 0)
non-parametric
ours-V3
10c(s)
one step without max(·, 0)
non-parametric
ORIL-01-V1
c(s)
one step without max(·, 0)
RL
ORIL-logR-V1
log
c(s)
1−c(s)
one step without max(·, 0)
RL
ORIL-01-V2
c(s)
two-step with max(·, 0)
RL
ORIL-logR-V2
log
c(s)
1−c(s)
two-step with max(·, 0)
RL
Table 5: Configurations of the variants ablated in Fig. 27. Ours-V1 and ours-V3 use 10c(s) instead of c(s) to
scale the behavior cloning weight so as to be similar to “ours.”"
