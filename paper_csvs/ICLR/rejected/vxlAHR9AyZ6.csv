Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.002036659877800407,"Federated Adversarial Training (FAT) helps us address the data privacy and gover-
nance issues, meanwhile maintains the model robustness to the adversarial attack.
However, the inner-maximization optimization of Adversarial Training can exac-
erbate the data heterogeneity among local clients, which triggers the pain points
of Federated Learning. This makes that the straightforward combination of two
paradigms shows the performance deterioration as observed in previous works.
In this paper, we introduce an α-Weighted Federated Adversarial Training (α-
WFAT) method to overcome this problem, which relaxes the inner-maximization
of Adversarial Training into a lower bound friendly to Federated Learning. We
present the theoretical analysis about this α-weighted mechanism and its effect
on the convergence of FAT. Empirically, the extensive experiments are conducted
to comprehensively understand the characteristics of α-WFAT, and the results on
three benchmark datasets demonstrate α-WFAT signiﬁcantly outperforms FAT
under different adversarial learning methods and federated optimization methods."
INTRODUCTION,0.004073319755600814,"1
INTRODUCTION"
INTRODUCTION,0.006109979633401222,"To handle the data privacy and governance issues, Federated Learning (McMahan et al., 2017) as
one promising paradigm of distributed training has drawn the increasing attention (McMahan et al.,
2017; Kairouz et al., 2019). However, training locally in Federated Learning also introduces the
vulnerability from the adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016), which drives
us to consider the model robustness in this framework. Thus, recent advances (Kairouz et al., 2019)
explore to apply the Adversarial Training methods (Madry et al., 2018) into Federated Learning."
INTRODUCTION,0.008146639511201629,"However, the straightforward combination of Adversarial Training and Federated Learning presents
some potential challenges due to the communication cost and the hardware requirement. For example,
Shah et al. (2021) pointed out that the communication in Federated Learning may be a constraint to
Adversarial Training, and proposed a dynamic schedule on the local training epochs to achieve the
expected robustness in a short communication budget. Hong et al. (2021) considered the hardware
constraint where some clients are not able to participate Adversarial Training, and they proposed a
federated robust propagation method to share the adversarial robustness among the clients. Although
these previous works indeed addressed some realistic problems, one critical issue in the way is the
performance deterioration in the combination of two paradigms as observed in (Zizzo et al., 2020)."
INTRODUCTION,0.010183299389002037,"As shown in the left panel of Figure 1, one typical phenomenon in Federated Adversarial Training
(FAT) is the robust accuracy of FAT (Zizzo et al., 2020) based on FedAvg (McMahan et al., 2017)
will decrease signiﬁcantly at the later stage of learning compared with the centralized Adversarial
Training (Madry et al., 2018) that does not. Actually, it exists in many variants of Federated Learning
methods (Shah et al., 2021) under different communication rounds and different local training epochs.
However, it is still lack of the sufﬁcient algorithmic breakthrough to overcome this issue, since almost
all previous works (Zizzo et al., 2020; Shah et al., 2021; Hong et al., 2021) consistently apply the
conventional update framework of Federated Learning with Adversarial Training."
INTRODUCTION,0.012219959266802444,"We dive into this phenomenon and attribute it to the inner-maximization of Adversarial Training.
Compared with the centralized Adversarial Training (Madry et al., 2018), the training data of FAT is
distributed to each client, which leads to the Adversarial Training in each client unaware of the data in
the others. Therefore, the adversarial examples generated by the inner-maximization of Adversarial
Training tend to be highly biased to each local distribution, yielding the radical optimization to pursuit
the model robustness (as shown in Figure 2, it has a severe local bias to the global optimum). In"
INTRODUCTION,0.014256619144602852,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.016293279022403257,"0
20
40
60
80
100
Communication 10 20 30 40 50 60 70"
INTRODUCTION,0.018329938900203666,Accuracy
INTRODUCTION,0.020366598778004074,"FAT: Natural
AT: Natural"
INTRODUCTION,0.02240325865580448,"FAT: Robust
AT: Robust"
INTRODUCTION,0.024439918533604887,"0
200
400
600
800
1000
Communication 5 10 15 20 25 30 35"
INTRODUCTION,0.026476578411405296,Robust Accuracy
INTRODUCTION,0.028513238289205704,"FAT-1-1000
FAT-5-200
FAT-10-100"
INTRODUCTION,0.03054989816700611,"-WFAT-10-100
0
20
40
60
80
100 10 20 30"
INTRODUCTION,0.032586558044806514,"Figure 1: Left: comparison between centralized Adversarial Training and Federated Adversarial
Training based on FedAvg. Right: comparison between FAT and α-WFAT. All the experiments are
conducted on CIFAR-10 dataset (Non-IID) with 5 clients, and use AT (Madry et al., 2018) to train as
well as PGD-20 to evaluate the Robust Accuracy. Note that, the notation “method-A-B"" in the right
panel means the method with A local training epochs and B communication rounds."
INTRODUCTION,0.034623217922606926,"short, the inner-maximization of Adversarial Training exacerbates the data heterogeneity among local
clients, which actually triggers the pain points of Federated Learning that are exploring urgently."
INTRODUCTION,0.03665987780040733,"To handle this problem, we propose a new learning framework based on a simple but effective re-
weighting mechanism, namely, α-Weighted Federated Adversarial Training (α-WFAT). Concretely,
we relax the objective of the inner-maximization in Adversarial Training into a lower bound by an α-
weighting mechanism (as Eq. (3) in Section 4.2). Similar to the bias-variance trade-off (Kohavi et al.,
1996), we introduce the small bias to the original objective via a low-bound relaxation, facilitating
a friendly optimization in the combination of Adversarial Training and Federated Learning. This
constructs a conservative optimization for Adversarial Training that weights different population based
on their adversarial losses. Then, the harsh heterogeneous update can be down-weighted with this
ﬂexible weighting mechanism, and the convergence could be accelerated at the same time. Applying
the similar idea to FAT, we propose α-WFAT that emphasizes the robust clients more compared with
other non-robust clients to alleviate the heterogeneous bias caused by the local adversarial generation.
The right panel of Figure 1 gives a simple comparison between α-WFAT and FAT. Empirically, we
conducted extensive experiments to provide a comprehensive understanding of the proposed α-WFAT,
and the results of α-WFAT in the context of different adversarial learning methods and federated
optimization methods demonstrate its superiority to improve the model performance."
INTRODUCTION,0.038696537678207736,Main Contributions
INTRODUCTION,0.04073319755600815,"• We derive an α-weighted relaxation for Adversarial Training to relax the inner-maximization
by a lower bound, which builds a mediating function to alleviate the potential radical
optimization in its straightforward combination with Federated Learning (in Section 4.2)."
INTRODUCTION,0.04276985743380855,"• We propose a new learning framework, i.e., α-Weighted Federated Adversarial Training
(α-WFAT), to realize the relaxation of inner-maximization in FAT, which is simple and com-
patible with various Federated Learning or Adversarial Training methods (in Section 4.3)."
INTRODUCTION,0.04480651731160896,"• We conduct extensive experiments to comprehensively understand the characteristics of the
α-WFAT, and conﬁrm its effectiveness on improving the model performance for both IID
and Non-IID settings in the context of several federated optimization methods (in Section 5)."
RELATED WORK,0.04684317718940937,"2
RELATED WORK"
RELATED WORK,0.048879837067209775,"Federated Learning
The representative work in Federated Learning is FedAvg (McMahan et al.,
2017), which has been proved effectiveness during the distributed training to maintain the data privacy.
To further address the heterogeneous issues, several optimization approaches have been proposed
e.g., FedProx (Li et al., 2018), FedNova (Wang et al., 2020b) and Scaffold (Karimireddy et al., 2020).
FedProx introduced a proximal term for FedAvg to constrain the model drift cause by heterogeneity;
FedNova proposed a general framework that eliminated the objective inconsistency and preserved
the fast convergence; Scaffold utilized the control variates to reduce the gradient variance in the"
RELATED WORK,0.05091649694501019,Under review as a conference paper at ICLR 2022
RELATED WORK,0.05295315682281059,"local updates and accelerate the convergence. MOON (Li et al., 2021a) alleviated the heterogeneity
by maximizing the agreement between the representation of the local model and that of the global
model, which helps correct the local training of individual parties. Reisizadeh et al. (2020) developed
a robust federated learning algorithm to against distribution shifts in clients samples. Our α-WFAT
introduces the relaxation into federated adversarial training, which is orthogonal to and compatible
with the previous optimization methods."
RELATED WORK,0.054989816700611,"Adversarial Training
As one of the defensive methods (Papernot et al., 2016; Gao et al., 2021),
Adversarial Training (Madry et al., 2018; Zhang et al., 2019; Jiang et al., 2020; Chen et al., 2021)
is to improve the robustness of machine learning models. The classical AT (Madry et al., 2018)
is built upon on a min-max formula to optimize the worst case, e.g., the adversarial example near
the natural example (Goodfellow et al., 2015). Zhang et al. (2019) decomposed the prediction
error for adversarial examples as the sum of the natural error and the boundary error, and proposed
TRADES to balance the classiﬁcation performance between the natural and adversarial examples.
Wang et al. (2020c) further explored the inﬂuence of the misclassiﬁed examples on the robustness,
and proposed MART that emphasizes the minimization of the misclassiﬁed examples to boost the AT.
Zhang et al. (2020) investigated the “benign adversarial examples” in AT and further improve the
natural performance of robust model. In this work, our α-WFAT framework leverages the client-level
measure to alleviate the heterogeneous issue in the straightforward combination of adversarial training
and federated learning. It is compatible to further incorporate those centralized adversarial training
methods to improve the model performance."
RELATED WORK,0.05702647657841141,"Federated Adversarial Training.
Recently, several works have made the exploration on the Ad-
versarial Training in the context of Federated Learning, which consider the data privacy and the
robustness in one framework. To our best knowledge, Zizzo et al. (2020) take the ﬁrst trial to study
the feasibility of extending Federated Learning (McMahan et al., 2017) with the standard AT on both
IID and Non-IID settings. Empirically, they found that there was a large performance gap existing
between the distributed and the centralized adversarial training, especially on the Non-IID data. Shah
et al. (2021) designed a dynamic schedule for the local training to pursue a larger robustness under
the constrained communication budget of Federated Learning. Hong et al. (2021) explored how to
effectively propagate the adversarial robustness when only limited clients in Federated Learning have
the sufﬁcient computational budget to afford AT. Although previous works have investigated to solve
the challenges about the constrained communication or computational budget, one critical issue that
affects the performance when combined Adversarial Training with Federated Learning has received
only few discussion. In this work, we consider such a basic issue (see Figure 1) in the straightforward
combination of AT with Federated Learning and introduce our solution to this problem."
PRELIMINARY,0.059063136456211814,"3
PRELIMINARY"
PRELIMINARY,0.06109979633401222,"In this section, we will brieﬂy formalize the notations of Adversarial Training (Goodfellow et al.,
2015; Madry et al., 2018) and Federated Learning as well as FedAvg (McMahan et al., 2017)."
ADVERSARIAL TRAINING,0.06313645621181263,"3.1
ADVERSARIAL TRAINING"
ADVERSARIAL TRAINING,0.06517311608961303,"Let (X, d∞) denote the input feature space X with the inﬁnity distance metric d∞(x, ˜x) = ∥x−˜x∥∞,
and Bϵ[x] = {˜x ∈X | d∞(x, ˜x) ≤ϵ} be the closed ball of radius ϵ > 0 centered at x in X. Dataset
S = {(xn, yn)}N
n=1, where xn ∈X and yn ∈Y = {0, 1, ..., C −1}. The objective function of the
standard adversarial training (AT) (Madry et al., 2018) is deﬁned as follows,"
ADVERSARIAL TRAINING,0.06720977596741344,"min
fθ∈F
1
N N
X"
ADVERSARIAL TRAINING,0.06924643584521385,"n=1
max
˜xn∈Bϵ[xn] ℓ(fθ(˜xn), yn),
(1)"
ADVERSARIAL TRAINING,0.07128309572301425,"where ˜x is the most adversarial data within the ϵ-ball centered at x, fθ(·) : X →RC is a score
function, ℓ: RC × Y →R is a composition of a base loss ℓB : ∆C−1 × Y →R (e.g., the Cross-
Entropy loss) and an inverse link function ℓL : RC →∆C−1 (e.g., the Softmax). Here, ∆C−1
is the corresponding probability simplex that yields ℓ(fθ(·), y) = ℓB(ℓL(fθ(·)), y). For the inner-
maximization of Eq. (1), the multi-step projected gradient descent (PGD) (Madry et al., 2018) is
usually applied to ﬁnd the most adversarial samples, which are then used for the outer-minimization."
ADVERSARIAL TRAINING,0.07331975560081466,Under review as a conference paper at ICLR 2022
ADVERSARIAL TRAINING,0.07535641547861507,"Client A
Client B
Global model (after FedAvg) Global model (after 𝛼-WFAT)"
ADVERSARIAL TRAINING,0.07739307535641547,"Class 1, Class 2"
ADVERSARIAL TRAINING,0.07942973523421588,Unseen data
ADVERSARIAL TRAINING,0.0814663951120163,Decision boundary
ADVERSARIAL TRAINING,0.0835030549898167,"Globally optimal 
decision boundary"
ADVERSARIAL TRAINING,0.0855397148676171,Misclassified data
ADVERSARIAL TRAINING,0.08757637474541752,"Adversarial data
Small Large"
ADVERSARIAL TRAINING,0.08961303462321792,"Figure 2: Left panel: locally learned decision boundary on Client A; Middle left panel: locally
learned decision boundary on Client B; Middle right panel: globally aggregated decision boundary
based on FedAvg; Right panel: globally aggregated decision boundary based on α-WFAT. Note that,
the distance between the correctly classiﬁed adversarial examples and the decision boundary (i.e.,
the bold line) can approximately reﬂect the client loss, and shows that ℓclient A > ℓclient B. Then,
selectively treating two client models in the aggregation can acquire a better global model (e.g., the
fourth panel), which is consistent with the intuition of Eq. (3)."
FEDERATED LEARNING,0.09164969450101833,"3.2
FEDERATED LEARNING"
FEDERATED LEARNING,0.09368635437881874,"Let Dk denotes a ﬁnite set of samples from the k-th client, and in each round, a set of datasets
{Dk}K
k=1 from K clients are involved into the training. The objective of Federated Learning is to
learn a machine learning model without any exchange of the training data between the clients and
the server. The current popular strategy, namely FedAvg, is introduced by McMahan et al. (2017),
where the clients collaboratively send the locally trained model weights θk to the server for the global
average aggregation. Concretely, each client runs on a local copy of the global model (parameterized
by θt in the t-th round) with its local data to optimize the objective like Eq. (1). Then, the server
receives their updated model weights {θt
k}K
k=1 of all clients and performs the following aggregation"
FEDERATED LEARNING,0.09572301425661914,"θt+1 = 1 N K
X"
FEDERATED LEARNING,0.09775967413441955,"k=1
Nkθt
k,
(2)"
FEDERATED LEARNING,0.09979633401221996,"where Nk denotes the number of the samples in Dk and N = PK
k=1 Nk. Then, the weights θt+1 for
the global model will be sent back to each client for another lifecycle. After the sufﬁcient rounds of
such a periodic synchronization and aggregation, we expect the stationary point of Federated Learning
will approximately approach to or have a small gap with that from the centralized counterpart."
FEDERATED LEARNING,0.10183299389002037,"4
α-WEIGHTED FEDERATED ADVERSARIAL TRAINING"
FEDERATED LEARNING,0.10386965376782077,"In the following sections, we will ﬁrst introduce our motivation for Federated Adversarial Training
through analyzing the challenges brought by the straightforward combination. Then, we relax the
inner-maximization problem of Adversarial Training as a lower bound by an α-weighted decompo-
sition, and present the theoretical understanding of such a relaxation. Finally, we will present the
α-Weighted Federated Adversarial Training and the corresponding analysis."
MOTIVATION,0.10590631364562118,"4.1
MOTIVATION"
MOTIVATION,0.1079429735234216,"For the general FAT (Zizzo et al., 2020), in each round, the clients will receive the latest model from
the server. Then, they conduct Adversarial Training with the local data and send their optimized
model parameters to the server. The server will aggregate the parameters of all clients into a global
model with FedAvg or other methods in Federated Learning. However, considering the characteristics
of Federated Learning and Adversarial Training, it naturally brings the following critical challenge."
MOTIVATION,0.109979633401222,"The inner-maximization can enlarge the data heterogeneity in Federated Learning. In terms of the
combination of adversarial training and federated learning, the key point is that such adversarial
examples generated by the inner-maximization exacerbate the heterogeneity, which induces the
performance deterioration under the model aggregation methods. As the Figure 2 shows, the client A
with large adversarial loss might make a dominating effect on the convergence of the global model,
given the current model aggregation methods treat all client models indiscriminately. Thus, it might
be better to selectively weight the client models in the model aggregation."
MOTIVATION,0.1120162932790224,Under review as a conference paper at ICLR 2022
MOTIVATION,0.11405295315682282,"4.2
α-WEIGHTED RELAXATION OF DECOMPOSED ADVERSARIAL TRAINING"
MOTIVATION,0.11608961303462322,"As previous analysis, the inner-maximization of Adversarial Training is not very compatible with
Federated Learning due to the exacerbation of heterogeneity when combing them together. An idea
to alleviate this problem is building a mediating function that keeps the original goal but is friendly
to two paradigms. In this section, we present one possible solution to this intuition, which tries to
ﬁnd a relaxation of the inner-maximization in Adversarial Training. Formally, we decompose the
inner-maximization objective into the independent K populations that corresponds to the K clients
in Federated Learning, and relaxes it into a lower bound by the α-weighted mechanism as follows,"
MOTIVATION,0.11812627291242363,"LAT = 1 N N
X"
MOTIVATION,0.12016293279022404,"n=1
max
˜xn∈Bϵ[xn] ℓ(f(˜xn), yn) = K
X k=1 Nk N"
NK,0.12219959266802444,"1
Nk Nk
X"
NK,0.12423625254582485,"n=1
max
˜xn∈Bϵ[xn] ℓ(f(˜xk
n), yk
n) !"
NK,0.12627291242362526,"|
{z
}
Lk"
NK,0.12830957230142567,"≥(1 + α) b
K
X k=1 Nφ(k)"
NK,0.13034623217922606,"N
Lφ(k) + (1 −α) K
X"
NK,0.13238289205702647,"k= b
K+1 Nφ(k)"
NK,0.13441955193482688,"N
Lφ(k)
s.t. α ∈[0, 1), bK ≤K 2"
NK,0.1364562118126273,".= Lα( bK), (3)"
NK,0.1384928716904277,where φ(·) is a function which maps the index to the original population sorted by { Nk
NK,0.14052953156822812,"N Lk} in an
ascending order. The following theorems provide us more analysis about the α-weighted relaxation."
NK,0.1425661914460285,"Theorem 4.1. Lα( bK) is monotonically decreasing w.r.t. both α and bK, i.e., Lα1( bK) < Lα2( bK)
if α1 > α2 and Lα( bK1) < Lα( bK2) if bK1 > bK2. Speciﬁcally, Lα( bK) recovers L of adversarial
training when α achieves 0, and Lα( bK) relaxes L to a lower bound objective by increasing bK and α."
NK,0.1446028513238289,"We can ﬂexibly emphasize the importance of partial populations by setting the proper hyperparameters,
alleviating the evenly averaging of harsh heterogeneous updates in FedAvg (McMahan et al., 2017)."
NK,0.14663951120162932,"Theorem 4.2. Assume the loss function ℓ(·, ·) in Eq. (3) satisﬁes the Lipschitzian smoothness
condition w.r.t. the model parameter θ and the training sample x, and is λ-strongly concave for all
x, and E
h
||∇Lα( bK) −∇θℓ(f(˜x), y)||2
2
i
≤δ2, where ˜x is the adversarial example. Then, after the"
NK,0.14867617107942974,"sufﬁcient T-step optimization i.e., T ≥L∆"
NK,0.15071283095723015,"δ2 , for α-weighted relaxation of decomposed Adversarial"
NK,0.15274949083503056,"Training with the constant stepsize
q"
NK,0.15478615071283094,"∆
LT σ2 in PGD, we have the following convergence property,"
T,0.15682281059063136,"1
T T
X t
E """
T,0.15885947046843177,"∇Lα( bK)

θt   2 2 # ≤  1 + α"
"T
PT",0.16089613034623218,"1
T
PT
t ξ(t) N"
"T
PT",0.1629327902240326,"!  
4L2
θxϵ
λ
+ 4δ r L∆ T ! ,
(4)"
"T
PT",0.164969450101833,where L = Lθθ + LθxLxθ
"T
PT",0.1670061099796334,"λ
deﬁned by the Lipschitzian constraints, ∆≥Lα( bK)|θ0 −infθ Lα( bK) and"
"T
PT",0.1690427698574338,"ξ(t) = P b
K
k N (t)
φ(k) −PK
b
K+1 N (t)
φ(k) meaning the accumulative counting difference of the t-th step."
"T
PT",0.1710794297352342,"When α = 0, Eq.(3) recovers the original loss of Adversarial Training, and the ﬁrst part in the RHS of
Eq. (4) goes to 1 that recovers the convergence rate of Adversarial Training (Sinha et al., 2018). When
α →1, Eq.(3) becomes more biased, while simultaneously the straightforward beneﬁt is that we"
"T
PT",0.17311608961303462,can achieve a faster convergence in Eq. (4) if 1
"T
PT",0.17515274949083504,"T
PT
t ξ(t) < 0, i.e.,

1 + α"
"T
PT",0.17718940936863545,"1
T
PT
t ξ(t)"
"T
PT",0.17922606924643583,"N

< 1. Actually,
this is possible when the sample number is approximately similar among all clients and the top-1
choice easily has −N < ξ(t) = N (t)
φ(1) −PK
k=2 N (t)
φ(k) < 0 in each optimization step. Especially, a
larger α has a faster convergence in this case. Therefore, the α-weighted relaxation of decomposed
Adversarial Training provides us a trade-off between maintaining the robustness from the standard
robust training and achieving the faster convergence with some biased approximation by α and bK."
"T
PT",0.18126272912423624,"4.3
α-WEIGHTED FEDERATED ADVERSARIAL TRAINING"
"T
PT",0.18329938900203666,"Inspired by the previous analysis of α-weighted relaxation, we propose an α-Weighted Federated
Adversarial Training to combine Adversarial Training and Federated Learning. The intuition is"
"T
PT",0.18533604887983707,Under review as a conference paper at ICLR 2022
"T
PT",0.18737270875763748,Algorithm 1 α-Weighted Federated Adversarial Training (α-WFAT)
"T
PT",0.1894093686354379,"Input: number of clients: K, number of communication rounds: T, number of client training epochs
per round: E, initial server’s model parameter: θ0, hyper-parameter for aggregation: α, number
of enhanced clients: bK;
Output: a globally robust model with parameter θT ;"
"T
PT",0.19144602851323828,"1: for t = 0, . . . , T −1 do
2:
Clients: [ perform adversarial training]
3:
for client k = 1, . . . , K do
4:
θt
k, Lk = AT(θt
k, E) (Madry et al., 2018)
5:
end for
6:
Server: [ performs aggregation over weight updates]"
"T
PT",0.1934826883910387,"7:
Lall ←[ N1"
"T
PT",0.1955193482688391,"N L1, N2"
"T
PT",0.1975560081466395,"N L2, . . . , NK"
"T
PT",0.19959266802443992,"N LK]
8:
Lsorted ←Ascending_Sort(Lall)
9:
∀k, Pk ←(1 + α) · 1( Nk"
"T
PT",0.20162932790224034,N Lk ≤Lsorted[ bK]) + (1 −α) · 1( Nk
"T
PT",0.20366598778004075,N Lk > Lsorted[ bK]);
"T
PT",0.20570264765784113,"10:
θt+1 =
1
PK
k=1 PkNk
PK
k=1 PkNkθt
k;
▷α-weighted mechanism"
"T
PT",0.20773930753564154,11: end for
"T
PT",0.20977596741344195,"applying the α-weighted mechanism into the inner-maximization in FAT, formalized as follows,"
"T
PT",0.21181262729124237,"min Lα−W F AT = min
fθ∈F
1
PK
k PkNk K
X"
"T
PT",0.21384928716904278,"k=1
PkNk"
NK,0.2158859470468432,"1
Nk Nk
X"
NK,0.21792260692464357,"n=1
max
˜xn∈Bϵ[xn] ℓ(fθ(˜xk
n), yk
n) !"
NK,0.219959266802444,"|
{z
}
Lk ,
(5)"
NK,0.2219959266802444,where Pk = (1+α)·1( Nk
NK,0.2240325865580448,N Lk ≤Lsorted[ bK])+(1−α)·1( Nk
NK,0.22606924643584522,"N Lk > Lsorted[ bK]) denotes the weight
assigned to the k-th client based on the ascending sort of weighted client losses compared with the
bK-th one. We summarize the procedure of α-WFAT in Algorithm 1, which consists of multi-round
iterations between the local training on the client side and the global aggregation on the server side."
NK,0.22810590631364563,"Concretely, on the client side, after downloading the global model parameter from the server, each
client will perform the Adversarial Training on its local data. At the same time, the client loss on the
adversarial examples is also recorded, which acts as the soft-indicator of the local bias induced by the
radical adversarial generation. Then, when the training steps reach to the condition, the client will
upload its model parameter and the loss to the server. On the server side, after collecting the model
parameters {θk}K
k=1 and the losses {Lk}K
k=1 of all clients, it will ﬁrst sort the losses in an ascending
order to ﬁnd the top- bK clients. Based on that, the global model parameters will be aggregated by
the α-weighted mechanism in which the model parameters of the top- bK clients are upweighted with
(1 + α) and the remaining is downweighted by (1 −α). For some atypical layers (Li et al., 2021b)
e.g., BN, it is outside the scope of this paper and we keep the aggregation same as FedAvg. Note that,
one interesting point in α-WFAT is the top- bK clients with the higher weights are not ﬁxed, and they
can be routing among all clients. In Figure 3, we trace this dynamic of α-WFAT in one experiment."
NK,0.23014256619144602,"In the following, we provide the theoretical analysis of α-WFAT on the convergence in the context of
Federated Learning (Li et al., 2019), which is slightly different from previous centralized counterpart.
Theorem 4.3. Assume the loss function ℓ(·, ·) in Eq. (5) is L-smooth and λ-strongly concave w.r.t.
the model parameter θ, and the expected norm and the variance of the stochastic gradient in each
client respectively satisfy E

||∇θℓ(f(˜xk), yk)||2
2

≤ς2 and E

||∇θℓ(f(˜xk), yk) −∇θLk||2
2

≤δ2
k.
Let κ = L"
NK,0.23217922606924643,"λ , γ = max{8κ, E} where E is the iteration number of the local Adversarial Training with
the learning rate ηt =
2
λ(γ+t). Then, after the sufﬁcient T-step communication rounds for α-WFAT,
we have the following asymptotics to the optimal point,"
NK,0.23421588594704684,"E[Lα−W F AT ] −L∗≤
κ
γ + T −1 2B"
NK,0.23625254582484725,λ + λγ
E,0.23828920570264767,"2 E

||θ0 −θ∗||2
,
(6)"
E,0.24032586558044808,"where L∗is the minimum value of Lα−W F AT , θ∗is the optimal model parameter, and B = K
X k"
E,0.24236252545824846,"P (T )
k
1 + α ξ(T ) N"
E,0.24439918533604887,!2 Nk N δk
E,0.24643584521384929,"2
+ 6L  L∗− K
X k"
E,0.2484725050916497,"P (T )
k
1 + α ξ(T ) N Nk"
E,0.2505091649694501,"N L∗
k !"
E,0.2525458248472505,+ 8(E −1)2ς2.
E,0.2545824847250509,Under review as a conference paper at ICLR 2022
E,0.25661914460285135,"0
20
40
60
80
100
Communication 1 2 3 4 5"
E,0.25865580448065173,Client
E,0.2606924643584521,"Figure 3: The index of the top- bK clients with the small losses in α-WFAT (α = 1/6, bK = 1) in each
communication round on CIFAR-10. We can see that it is dynamically routing among all clients."
E,0.26272912423625255,"When α = 0, we have P (t)
k
= 1 and Eq.( 6) becomes the convergence rate of FedAvg on non-IID
data (Li et al., 2019). Different from Theorem 4.2 that concludes in the centralized training setting,
when α →1, the convergence is indeﬁnite compared to the standard FAT, since the emerging terms in"
E,0.26476578411405294,"B, i.e.,

P (T )
k
1+α ξ(T ) N"
E,0.2668024439918534,"2
and
P (T )
k
1+α ξ(T )"
E,0.26883910386965376,"N
, are acted as the scalar timing by the personalized variance bound"
E,0.2708757637474542,"δ2
k and the local optimum L∗
k of each client. One possible case is when the optimization approaches
to the optimal parameter θ∗, δ2
k can be in a smaller scale relative to the scale of L∗
k. In this case, the
increment of the ﬁrst term of B can be totally counteracted by the loss of second term of B so that in
sum B becomes smaller. Then, we can have a tighter upper bound for α-WFAT in Eq.( 6) to achieve
a faster convergence than FAT. The completed proof of Theorem 4.3 can refer to Appendix C and the
experiments in the following section will conﬁrm α-WFAT can reach to a more robust optimum."
EXPERIMENTS,0.2729124236252546,"5
EXPERIMENTS"
EXPERIMENTS,0.27494908350305497,"In this section, we will provide an in-depth analysis of α-WFAT and empirically verify its efﬁciency
compared with the current methods on a range of IID and non-IID datasets."
EXPERIMENTAL SETUP,0.2769857433808554,"5.1
EXPERIMENTAL SETUP"
EXPERIMENTAL SETUP,0.2790224032586558,"Dataset.
We conduct the experiments on three benchmark datasets, i.e., CIFAR-10, CIFAR-
100 (Krizhevsky, 2009) and SVHN (Netzer et al., 2011) for Federated Adversarial Training. For
the IID scenario, we just randomly and evenly distribute the samples to each client. For the Non-
IID scenario, we follow McMahan et al. (2017); Shah et al. (2021) to partition the training data
based on their labels. To be speciﬁc, a skew parameter s is utilized in the data partition introduced
by Shah et al. (2021), which enables K clients to get a majority of the data samples from a subset
of classes. We denote the set of all classes in a dataset as Y and create Yk by dividing all the class
labels equally among K clients. Accordingly, we split the data across K clients that each client has
(100 −(K −1) × s)% of data for the class in Yk and s% of data in other split sets. The detailed
training and evaluation settings can refer to Appendix E."
ABLATION STUDY,0.28105906313645623,"5.2
ABLATION STUDY"
ABLATION STUDY,0.2830957230142566,"In this part, we conduct various experiments on CIFAR-10 to visualize the characteristics of α-WFAT."
ABLATION STUDY,0.285132382892057,"Non-AT vs. AT. In the left two panels of Figure 4, we respectively apply our α-weighted mechanism
to Federated Standard Training (α-WFST) and Federated Adversarial Training (α-WFAT) under
bK = 1. We also consider both FedAvg and FedProx in this experiment to guarantee the universality.
From the curves, we can see that α-WFST has the negative effect on the natural accuracy, while
α-WFAT consistently improve the robust accuracy both based on FedAvg or FedProx. This indicates
our α-weighted mechanism is tailored for the inner-maximization of Federated Adversarial Training
instead of the outer-minimization considered by other federated optimization methods."
ABLATION STUDY,0.28716904276985744,"Impact of α and bK. To study the effect of hyperparameter in α-WFAT, we conduct several ablation
experiments to verify the model performance. Regarding the experiments of α, we set the client"
ABLATION STUDY,0.2892057026476578,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.29124236252545826,"0
20
40
60
80
100
Communication 62.5 65.0 67.5 70.0 72.5 75.0 77.5 80.0"
ABLATION STUDY,0.29327902240325865,Natural Accuracy
ABLATION STUDY,0.2953156822810591,"FedAvg
FedAvg+ -WFST
FedProx
FedProx+ -WFST"
ABLATION STUDY,0.2973523421588595,"0
20
40
60
80
100
Communication 15 20 25 30 35"
ABLATION STUDY,0.29938900203665986,Robust Accuracy
ABLATION STUDY,0.3014256619144603,"FedAvg
FedAvg+ -WFAT
FedProx
FedProx+ -WFAT"
ABLATION STUDY,0.3034623217922607,"0
1/11
1/6
1/13
1/14 35 40 45 50 55 60 65"
ABLATION STUDY,0.3054989816700611,Accuracy
ABLATION STUDY,0.3075356415478615,"Natural Acc
Robust Acc"
ABLATION STUDY,0.3095723014256619,"1/8
2/8
3/8
4/8
K/K 25 30 35 40 45 50 55 60 65"
ABLATION STUDY,0.31160896130346233,Accuracy
ABLATION STUDY,0.3136456211812627,"Natural Acc
Robust Acc"
ABLATION STUDY,0.31568228105906315,"Figure 4: Ablation study on α-WFAT. Left two panels: comparison between Federated Standard
Training and Federated Adversarial Training respectively in combination with the α-weighted mecha-
nism, i.e., (α-WFST) vs. (α-WFAT). Right two panels: the natural accuracy and the robust accuracy
of α-WFAT with different α and different bK on CIFAR-10."
ABLATION STUDY,0.31771894093686354,"number K = 5 and bK = 1 to upweight/downweight the client models in each communication rounds.
The right middle panel of Figure 4 shows α ∈(0, 1/6] can signiﬁcantly improve the robust accuracy
and the natural accuracy, while a larger α might be inappropriate to the natural accuracy. Regarding the
choice of bK, we specially set K = 8 in this experiment to span the range of bK due to the constraint
bK <= K/2. The right panel of Figure 4 presents the accuracy of α-WFAT with increasing bK."
ABLATION STUDY,0.319755600814664,"Table 1: Test accuracy (%) on CIFAR-10 (Non-IID) with
different Adversarial Training methods on the local client."
ABLATION STUDY,0.32179226069246436,"Methods
Natural
PGD-20
CW∞"
ABLATION STUDY,0.32382892057026474,"AT
FAT
57.45%
32.58%
30.52%
α-WFAT
62.34%
35.59%
33.06%"
ABLATION STUDY,0.3258655804480652,"TRADES
FAT
64.00%
31.64%
28.95%
α-WFAT
65.26%
35.10%
31.80%"
ABLATION STUDY,0.32790224032586557,"MART
FAT
56.29%
36.27%
32.41%
α-WFAT
58.41%
38.90%
34.67%"
ABLATION STUDY,0.329938900203666,"According to the plot, both the natu-
ral accuracy and the robust accuracy are
improved even with larger bK, which
shows the effect of
bK on the relax-
ation of inner-maximization in the pro-
posed α-WFAT. In addition, we conduct
the experiments about emphasizing/de-
emphasizing the client with smallest ad-
versarial loss by adjusting the α, the re-
sults (in Appendix E.1) conﬁrmed that the
relaxation introduced by our α-WFAT is
needed to improve performance, and is
consistent with our previous analysis."
ABLATION STUDY,0.3319755600814664,"Different AT methods. In Table 1, we
validate the combination of α-weighted mechanism and different Adversarial Training methods (i.e.,
TRADES (Zhang et al., 2019) and MART (Wang et al., 2020c)), where we switch different local
Adversarial Training methods on the client side. Through the comparison with FAT, the results show
that α-WFAT can consistently improve both the natural performance and the robust performance, and
is general to the state-of-the-art Adversarial Training works under Federated Learning scenarios."
PERFORMANCE EVALUATION,0.3340122199592668,"5.3
PERFORMANCE EVALUATION"
PERFORMANCE EVALUATION,0.3360488798370672,"In this section, we compare our α-WFAT with FAT on various benchmark datasets to demonstrate its
effectiveness. Speciﬁcally, we validate both the non-IID and IID settings with three represenative
Federated optimization methods i.e., FedAvg, FedProx and Scalffold. Besides, a centralized AT
baseline is provided as the reference of the IID setting. Note that, there is no such a baseline for the
non-IID setting, since the centralized case is one distribution which is incomparable and meaningless.
Considering the sensitivity of data selection in Non-IID settings, we also report the results with Mean
and Std values in Appendix E.2 after running experiments for multiple times."
PERFORMANCE EVALUATION,0.3380855397148676,"According to Table 2 on CIFAR-10, we can ﬁnd that α-WFAT signiﬁcantly outperforms FAT on the
Non-IID data in terms of both the natural accuracy (∼2%-6%) and the robust accuracy (∼2%-5%).
For the IID data, our method acquires a similar improvement on the robust accuracy without the
deterioration of the natural accuracy. The reason might be because even the data is IID, Adversarial
Training can still drive the independently-initialized overparameterized network (Allen-Zhu & Li,
2020) on each client side towards at the robust overﬁtting of different directions, yielding the model
heterogeneity. Thus, the proper relaxation to the inner-maximization makes Adversarial Training more
compatible with Federated Learning. Another interesting observation is that Federated Adversarial
Training shows better performance than centralized Adversarial Training in the case of the IID setting.
This gain could be from the distributed training paradigm that helps Adversarial Training converge"
PERFORMANCE EVALUATION,0.34012219959266804,Under review as a conference paper at ICLR 2022
PERFORMANCE EVALUATION,0.3421588594704684,Table 2: Performance on three benchmark datasets under different federated optimization methods.
PERFORMANCE EVALUATION,0.34419551934826886,"Setting
Non-IID
IID"
PERFORMANCE EVALUATION,0.34623217922606925,"CIFAR-10
Natural
FGSM
PGD-20
CW∞
AA
Natural
FGSM
PGD-20
CW∞
AA"
PERFORMANCE EVALUATION,0.34826883910386963,"Centralized AT
-
-
-
-
-
66.47%
47.68%
38.18%
37.04%
34.48%"
PERFORMANCE EVALUATION,0.35030549898167007,"FedAvg
FAT
57.45%
39.44%
32.58%
30.52%
29.20%
69.35%
48.45%
37.43%
35.72%
33.96%
α-WFAT
63.44%
45.13%
37.17%
33.99%
32.36%
67.43%
50.33%
42.78%
37.91%
36.20%"
PERFORMANCE EVALUATION,0.35234215885947046,"FedProx
FAT
60.44%
41.59%
33.84%
31.29%
30.02%
66.91%
46.70%
37.14%
34.54%
32.68%
α-WFAT
62.51%
44.29%
36.75%
33.82%
31,98%
68.31%
48.40%
42.41%
37.25%
35.97%"
PERFORMANCE EVALUATION,0.3543788187372709,"Scaffold
FAT
62.81%
43.61%
34.13%
32.53%
30.95%
68.27%
49.25%
39.33%
37.31%
35.30%
α-WFAT
64.12%
46.05%
37.35%
34.78%
33.32%
71.36%
50.42%
43.83%
39.12%
35.47%"
PERFORMANCE EVALUATION,0.3564154786150713,"CIFAR-100
Natural
FGSM
PGD-20
CW∞
AA
Natural
FGSM
PGD-20
CW∞
AA"
PERFORMANCE EVALUATION,0.35845213849287166,"Centralized AT
-
-
-
-
-
35.81%
23.09%
18.64%
16.48%
15.42%"
PERFORMANCE EVALUATION,0.3604887983706721,"FedAvg
FAT
31.07%
19.60%
16.16%
13.37%
12.47%
38.35%
23.37%
18.44%
16.63%
15.45%
α-WFAT
35.17%
21.26%
16.72%
13.91%
12.83%
38.43%
23.76%
18.82%
16.71%
15.62%"
PERFORMANCE EVALUATION,0.3625254582484725,"FedProx
FAT
33.33%
20.20%
16.08%
13.76%
12.72%
37.18%
22.29%
18.16%
16.33%
15.29%
α-WFAT
34.30%
20.82%
16.74%
13.84%
12.88%
37.37%
23.11%
18.43%
16.36%
15.45%"
PERFORMANCE EVALUATION,0.3645621181262729,"Scaffold
FAT
41.17%
25.17%
20.01%
16.74%
15.49%
42.42%
26.79%
21.18%
18.89%
17.63%
α-WFAT
41.07%
25.55%
20.40%
16.79%
15.59%
42.08%
27.18%
22.26%
19.34%
18.03%"
PERFORMANCE EVALUATION,0.3665987780040733,"SVHN
Natural
FGSM
PGD-20
CW∞
AA
Natural
FGSM
PGD-20
CW∞
AA"
PERFORMANCE EVALUATION,0.36863543788187375,"Centralized AT
-
-
-
-
-
92.39%
89.75%
72.73%
72.31%
70.93%"
PERFORMANCE EVALUATION,0.37067209775967414,"FedAvg
FAT
91.24%
87.95%
68.87%
67.89%
66.54%
93.52%
90.68%
72.24%
71.22%
70.08%
α-WFAT
91.25%
88.28%
71.72%
69.79%
68.62%
92.75%
90.06%
74.37%
72.34%
71.27%"
PERFORMANCE EVALUATION,0.3727087576374745,"FedProx
FAT
90.92%
87.50%
68.44%
67.18%
65.94%
93.54%
90.66%
72.53%
71.42%
70.21%
α-WFAT
91.25%
88.15%
71.54%
69.53%
68.47%
93.59%
90.80%
74.66%
72.67%
71.48%"
PERFORMANCE EVALUATION,0.37474541751527496,"Scaffold
FAT
89.95%
87.23%
68.66%
67.23%
66.65%
93.80%
91.00%
73.26%
72.05%
70.80%
α-WFAT
90.20%
87.81%
71.39%
68.81%
67.88%
93.92%
91.28%
75.96%
74.05%
72.88%"
PERFORMANCE EVALUATION,0.37678207739307534,"to the more robust optimum by the divide-and-conquer mechanism. This might enlighten the more
explore in Adversarial Training to improve the model robustness via Federated Learning."
PERFORMANCE EVALUATION,0.3788187372708758,"On CIFAR-100 and SVHN, we can ﬁnd the similar improvement in Table 2 as that of CIFAR-10 under
three types of federated optimization methods. Nevertheless, α-WFAT only becomes superior in
terms of the robust accuracy but comparable with FAT in terms of the natural accuracy especially
on CIFAR-100. It indicates that the inner-maximization of Adversarial Training when combined
with Federated Learning mainly affects the model robustness, and thus the α-weighted relaxation
correspondingly helps the model converge to a more robust optimum."
CONCLUSION,0.38085539714867617,"6
CONCLUSION"
CONCLUSION,0.38289205702647655,"In this work, we explore the performance deterioration in the straightforward combination of Adver-
sarial Training with Federated Learning. To alleviate the potential radical optimization, we apply
an α-weighted relaxation into Adversarial Training to relax the inner-maximization. Based on this
α-weighted mechanism, we further propose α-Weighted Federated Adversarial Training (α-WFAT).
We provide the theoretical analysis and empirical evidences to understand the proposed simple but
effective method. The experimental results under different settings conﬁrm the effectiveness of
α-WFAT. Nevertheless, we only move a small step on the heterogeneous issue in the combination of
two paradigms and more issues in their cross ﬁeld could be further explored in the future."
CONCLUSION,0.384928716904277,Under review as a conference paper at ICLR 2022
ETHICS STATEMENT,0.3869653767820774,"7
ETHICS STATEMENT"
ETHICS STATEMENT,0.3890020366598778,"This paper does not raise any ethics concerns. This study does not involve any human subjects,
practices to data set releases, potentially harmful insights, methodologies and applications, potential
conﬂicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security
issues, legal compliance, and research integrity issues."
REPRODUCIBILITY STATEMENT,0.3910386965376782,"8
REPRODUCIBILITY STATEMENT"
REPRODUCIBILITY STATEMENT,0.39307535641547864,"To ensure the reproducibility of experimental results, we will provide a link for an anonymous
repository about the source codes of this paper in the discussion forums."
REFERENCES,0.395112016293279,REFERENCES
REFERENCES,0.3971486761710794,"Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020."
REFERENCES,0.39918533604887985,"Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
Symposium on Security and Privacy (SP), 2017."
REFERENCES,0.40122199592668023,"Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial
robustness: From self-supervised pre-training to ﬁne-tuning. In CVPR, 2020."
REFERENCES,0.40325865580448067,"Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overﬁtting
may be mitigated by properly learned smoothening. In ICLR, 2021."
REFERENCES,0.40529531568228105,"Ruize Gao, Feng Liu, Jingfeng Zhang, Bo Han, Tongliang Liu, Gang Niu, and Masashi Sugiyama.
Maximum mean discrepancy test is aware of adversarial attacks. In ICML, 2021."
REFERENCES,0.4073319755600815,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015."
REFERENCES,0.4093686354378819,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016."
REFERENCES,0.41140529531568226,"Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. Federated robustness propagation:
Sharing adversarial robustness in federated learning. arXiv preprint arXiv:2106.10196, 2021."
REFERENCES,0.4134419551934827,"Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial
contrastive learning. In NeurIPS, 2020."
REFERENCES,0.4154786150712831,"Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019."
REFERENCES,0.4175152749490835,"Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
ICML, 2020."
REFERENCES,0.4195519348268839,"Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss functions.
In ICML, 1996."
REFERENCES,0.4215885947046843,"Alex Krizhevsky. Learning multiple layers of features from tiny images. In arXiv, 2009."
REFERENCES,0.42362525458248473,"Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016."
REFERENCES,0.4256619144602851,"Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, 2021a."
REFERENCES,0.42769857433808556,"Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018."
REFERENCES,0.42973523421588594,Under review as a conference paper at ICLR 2022
REFERENCES,0.4317718940936864,"Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In ICLR, 2019."
REFERENCES,0.43380855397148677,"Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning
on non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021b."
REFERENCES,0.43584521384928715,"Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In ICLR, 2014."
REFERENCES,0.4378818737270876,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018."
REFERENCES,0.439918533604888,"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efﬁcient learning of deep networks from decentralized data. In AISTATS, 2017."
REFERENCES,0.4419551934826884,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NeurIPS Workshop on Deep
Learning and Unsupervised Feature Learning, 2011."
REFERENCES,0.4439918533604888,"Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on
security and privacy (SP), 2016."
REFERENCES,0.4460285132382892,"Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of afﬁne distribution shifts. In NeurIPS, 2020."
REFERENCES,0.4480651731160896,"Devansh Shah, Parijat Dube, Supriyo Chakraborty, and Ashish Verma. Adversarial training in
communication constrained federated learning. arXiv preprint arXiv:2103.01319, 2021."
REFERENCES,0.45010183299389,"Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In ICLR, 2018."
REFERENCES,0.45213849287169044,"Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, and Zhangyang Wang. Once-
for-all adversarial training: In-situ tradeoff between robustness and accuracy for free. In NeurIPS,
2020a."
REFERENCES,0.45417515274949083,"Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. In NeurIPS, 2020b."
REFERENCES,0.45621181262729127,"Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, 2019."
REFERENCES,0.45824847250509165,"Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassiﬁed examples. In ICLR, 2020c."
REFERENCES,0.46028513238289204,"Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. In NeurIPS, 2020."
REFERENCES,0.4623217922606925,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019."
REFERENCES,0.46435845213849286,"Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankan-
halli. Attacks which do not kill training make adversarial learning stronger. In ICML, 2020."
REFERENCES,0.4663951120162933,"Yuchen Zhang, John C Duchi, and Martin J Wainwright. Communication-efﬁcient algorithms for
statistical optimization. The Journal of Machine Learning Research, 14(1):3321–3363, 2013."
REFERENCES,0.4684317718940937,"Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, and Beat Buesser. Fat: Federated adversarial training.
arXiv preprint arXiv:2012.01791, 2020."
REFERENCES,0.47046843177189407,Under review as a conference paper at ICLR 2022
REFERENCES,0.4725050916496945,APPENDIX
REFERENCES,0.4745417515274949,"A
PROOF OF EQ. (3) AND THEOREM 4.1"
REFERENCES,0.47657841140529533,We proof the Eq. (3) and Theorem 4.1 in this section.
REFERENCES,0.4786150712830957,"Recall the α-weighted relaxation for the inner-maximization objective decomposition with K inde-
pendent populations as follows,"
REFERENCES,0.48065173116089616,"LAT = 1 N N
X"
REFERENCES,0.48268839103869654,"n=1
max
˜xn∈Bϵ[xn] ℓ(f(˜xn), yn) = K
X k=1 Nk N"
NK,0.4847250509164969,"1
Nk Nk
X"
NK,0.48676171079429736,"n=1
max
˜xn∈Bϵ[xn] ℓ(f(˜xk
n), yk
n) !"
NK,0.48879837067209775,"|
{z
}
Lk"
NK,0.4908350305498982,"≥(1 + α) b
K
X k=1 Nφ(k)"
NK,0.49287169042769857,"N
Lφ(k) + (1 −α) K
X"
NK,0.49490835030549896,"k= b
K+1 Nφ(k)"
NK,0.4969450101832994,"N
Lφ(k)
s.t. α ∈[0, 1), bK ≤K 2"
NK,0.4989816700610998,".= Lα( bK), (7)"
NK,0.5010183299389002,where φ(·) is a function which maps the index to the original population group sorted by { Nk
NK,0.5030549898167006,"N Lk} in
an ascending order."
NK,0.505091649694501,"proof of Eq. (3). The deduction of the inequality in Eq. (7) can be formulated in the following. Given
α ∈[0, 1) and bK ≤K"
NK,0.5071283095723014,2 with the population sorted by { Nk
NK,0.5091649694501018,"N Lk} in an ascending order, we have
P b
K
k=1
Nφ(k)"
NK,0.5112016293279023,"N
Lφ(k) ≤PK
k= b
K+1
Nφ(k)"
NK,0.5132382892057027,"N
Lφ(k). Then, we have the following relationship by subtraction, b
K
X k=1 Nφ(k)"
NK,0.515274949083503,"N
Lφ(k) + K
X"
NK,0.5173116089613035,"k= b
K+1 Nφ(k)"
NK,0.5193482688391039,"N
Lφ(k) −(1 + α) b
K
X k=1 Nφ(k)"
NK,0.5213849287169042,"N
Lφ(k) −(1 −α) K
X"
NK,0.5234215885947047,"k= b
K+1 Nφ(k)"
NK,0.5254582484725051,"N
Lφ(k) = α ·  
K
X"
NK,0.5274949083503055,"k= b
K+1 Nφ(k)"
NK,0.5295315682281059,"N
Lφ(k) − b
K
X k=1 Nφ(k)"
NK,0.5315682281059063,"N
Lφ(k) "
NK,0.5336048879837068,"≥0.
(8)"
NK,0.5356415478615071,"proof of Theorem 4.1. It can be naturally proved by Eq. (8). If α1 > α2, then we have,"
NK,0.5376782077393075,"Lα1( bK) −Lα2( bK) = (1 + α1) b
K
X k=1 Nφ(k)"
NK,0.539714867617108,"N
Lφ(k) + (1 −α1) K
X"
NK,0.5417515274949084,"k= b
K+1 Nφ(k)"
NK,0.5437881873727087,"N
Lφ(k)"
NK,0.5458248472505092,"−(1 + α2) b
K
X k=1 Nφ(k)"
NK,0.5478615071283096,"N
Lφ(k) −(1 −α2) K
X"
NK,0.5498981670061099,"k= b
K+1 Nφ(k)"
NK,0.5519348268839104,"N
Lφ(k)
(9)"
NK,0.5539714867617108,"= (α1 −α2)   b
K
X k=1 Nφ(k)"
NK,0.5560081466395111,"N
Lφ(k) − K
X"
NK,0.5580448065173116,"k= b
K+1 Nφ(k)"
NK,0.560081466395112,"N
Lφ(k)  ≤0"
NK,0.5621181262729125,"Similarly, we can prove Lα( bK1) < Lα( bK2) if bK1 > bK2."
NK,0.5641547861507128,"B
PROOF OF THEOREM 4.2"
NK,0.5661914460285132,"Based on the convergence of Adversarial Training (Sinha et al., 2018), we proof Theorem 4.2 in this
section."
NK,0.5682281059063137,"proof of Theorem 4.2. Let c : X × X →R+ ∪{∞}, where c(x, x0) is the “cost” for an adversary to
perturb x0 to x. Let f(θ, x; x0) = ℓ(θ; x) −γc(x, x0), noting that the gradient steps is preformed as"
NK,0.570264765784114,Under review as a conference paper at ICLR 2022
NK,0.5723014256619144,"gt = ∇θf(θt, ˆx; xt), where ˆx is an approximate maximizer of f(θ, x; xt) in x, and θt+1 = θt −µtgt."
NK,0.5743380855397149,"We assume µt ≤
1
Lψ in the rest of the proof, which is satisﬁed for the constant step size µ =
q"
NK,0.5763747454175153,"∆L
LψT δ2"
NK,0.5784114052953157,and T ≥Lψ∆L
NK,0.5804480651731161,"δ2
. By a Taylor expansion using the Lψ-smoothness of the objective Lk for k-th client,
we have"
NK,0.5824847250509165,"Lk

θt+1 ≤Lk

θt +

∇Lk

θt, θt+1 −θt
+ Lψ"
NK,0.5845213849287169,"2 ||θt+1 −θt||2
2
(10)"
NK,0.5865580448065173,"= Lk

θt −µt||∇Lk

θt||2
2 + Lψµ2"
NK,0.5885947046843177,"2
||gt||2
2 + µt

∇Lk

θt, ∇Lk

θt −gt"
NK,0.5906313645621182,"= Lk

θt −µt"
NK,0.5926680244399185,"
1 −1"
NK,0.594704684317719,"2Lψµ2

||∇Lk

θt||2
2"
NK,0.5967413441955194,"+ µt(1 −Lψµ)

∇Lk

θt, ∇Lk

θt −gt
+ Lψµ2"
NK,0.5987780040733197,"2
||gt −∇Lk

θt||2
2"
NK,0.6008146639511202,"Consider the function φγ(θ; x0) = supx∈Z f(θ, x; x0), we deﬁne the potentially biased errors
ζt = gt −∇θφγ(θt; xt). Then we have the following relationship,"
NK,0.6028513238289206,"Lk

θt+1 ≤Lk

θt −µt"
NK,0.604887983706721,"
1 −1"
NK,0.6069246435845214,"2Lψµ2

||∇Lk

θt||2
2
(11)"
NK,0.6089613034623218,"+ µt(1 −Lψµ)

∇Lk

θt, ∇Lk

θt −∇θφγ(θt; xt)"
NK,0.6109979633401222,"−µt(1 −Lψµt)

∇Lk

θt, ζt
+ Lψµ2"
NK,0.6130346232179226,"2
||∇θφγ(θt; xt) + ζ2 −∇Lk

θt||2
2"
NK,0.615071283095723,"= Lk

θt −µt"
NK,0.6171079429735234,"
1 −1"
NK,0.6191446028513238,"2Lψµ2

||∇Lk

θt||2
2"
NK,0.6211812627291242,"+ µt(1 −Lψµ)

∇Lk

θt, ∇Lk

θt −∇θφγ(θt; xt)"
NK,0.6232179226069247,"−µt(1 −Lψµt)

∇Lk

θt, ζt"
NK,0.6252545824847251,"+ Lψµ2
t
2
 
||ζt||2
2 + ||∇θφγ(θt; xt) −∇Lk

θt||2
2 + 2

∇θφγ(θt; xt) −∇Lk

θt, ζt
."
NK,0.6272912423625254,"Since ±⟨a, b⟩≤1"
NK,0.6293279022403259,"2
 
||a||2
2 + ||b||2
2

, we have"
NK,0.6313645621181263,"Lk

θt+1 ≤Lk

θt −µt"
NK,0.6334012219959266,"2 ||∇Lk

θt||2
2 + µt((1 −Lψα))

∇Lk

θt, ∇Lk

θt −∇θφγ(θt; xt)

(12)"
NK,0.6354378818737271,+ µt((1 + Lψµ))
NK,0.6374745417515275,"2
||ζ||2
2 + Lψµ2
t||∇θφγ(θt; xt) −∇Lk

θt||2
2."
NK,0.639511201629328,"Then, letting xt
∗= arg maxx f(θt, x; xt), the error ζt satisﬁes,"
NK,0.6415478615071283,"||ζ||2
2 =||∇θφγ(θt; xt) −∇f(θ, ˆxt; xt)||2
2 = ||∇θℓ(θ, xt
∗) −∇θℓ(θ, ˆxt)||2
2
(13)"
NK,0.6435845213849287,"≤Lθx||ˆxt −xt
∗||2
2 ≤2L2
θx
λ
ϵ,
(14)"
NK,0.6456211812627292,"where the ﬁnal inequality utilize the λ = γ −Lxx strong-concavity of x 7→f(θ, x; x0). For conve-"
NK,0.6476578411405295,"nience, let ˆϵ =
2L2
θx
γ−Lxx ϵ. Taking conditional expectations in Eq. (12) and using E [∇θφγ(θt; xt)|θt] =
∇Lk

θt, we have,"
NK,0.6496945010183299,"E

Lk

θt+1 −Lk

θt|θt
≤−µt"
NK,0.6517311608961304,"2 ||∇Lk

θt||2
2 + µt((1 + Lψµ))"
NK,0.6537678207739308,"2
ˆϵ + Lψµ2
t||∇θφγ(θt; xt) −∇Lk

θt||2
2
(15) ≤−µt"
NK,0.6558044806517311,"2 ||∇Lk

θt||2
2 + µtˆϵ + Lψµ2
t||∇θφγ(θt; xt) −∇Lk

θt||2
2"
NK,0.6578411405295316,"Since µt ≤
1
Lψ , taking a ﬁxed step size µ, we have,"
NK,0.659877800407332,"E

||∇Lk

θt||2
2

−2ˆϵ ≤2"
NK,0.6619144602851323,"µE

Lk

θt −Lk

θt+1

+ 2Lψµδ2
(16)"
NK,0.6639511201629328,Under review as a conference paper at ICLR 2022
NK,0.6659877800407332,"Because E
h
||∇θφγ(θ; Z) −∇Lk

(θ)||2
2
i
≤δ2, summing over t, we have,"
T,0.6680244399185336,"1
T T
X"
T,0.670061099796334,"t
E

||∇Lk

θt||2
2

−2ˆϵ ≤
2
µT (Lk

θ0 −E[Lk

θT ]) + 2Lψµδ2 ≤2∆"
T,0.6720977596741344,"µT + 2Lψµδ2
(17)"
T,0.6741344195519349,"Since µ =
q"
T,0.6761710794297352,"Delta
LψT δ2 , and λ = γ −Lxx, we can get the following result,"
T,0.6782077393075356,"1
T T
X"
T,0.6802443991853361,"t
E

||∇Lk

θt||2
2

≤4L2
θxϵ
λ
+ 4δ r L∆"
T,0.6822810590631364,"T
(18)"
T,0.6843177189409368,"Adopting our α-weighted relaxation, we have,"
T,0.6863543788187373,"1
T T
X t
E """
T,0.6883910386965377,"∇Lα( bK)

θt   2 2 # = 1 T T
X t
E  "
T,0.6904276985743381,"(1 + α) b
K
X k=1"
T,0.6924643584521385,"N (t)
φ(k)
N
∇Lφ(k)

θt + (1 −α) K
X"
T,0.6945010183299389,"k= b
K+1"
T,0.6965376782077393,"N (t)
φ(k)
N
∇Lφ(k)

θt   2 2   ≤1 T T
X t "
T,0.6985743380855397,"(1 + α) b
K
X k=1"
T,0.7006109979633401,"N (t)
φ(k)
N
E """
T,0.7026476578411406,"∇Lφ(k)

θt   2 2 #"
T,0.7046843177189409,"+ (1 −α) K
X"
T,0.7067209775967414,"k= b
K+1"
T,0.7087576374745418,"N (t)
φ(k)
N
E """
T,0.7107942973523421,"∇Lφ(k)

θt   2 2 #  ≤1 T T
X t "
T,0.7128309572301426,"(1 + α) b
K
X k=1"
T,0.714867617107943,"N (t)
φ(k)
N
+ (1 −α) K
X"
T,0.7169042769857433,"k= b
K+1"
T,0.7189409368635438,"N (t)
φ(k)
N  "
T,0.7209775967413442,"4L2
θxϵ
λ
+ 4δ r L∆ T ! = 1 T T
X t "
T,0.7230142566191446,1 + α
T,0.725050916496945,"P b
K
k=1 N (t)
φ(k) −PK
k= b
K+1 N (t)
φ(k)
N  "
T,0.7270875763747454,"4L2
θxϵ
λ
+ 4δ r L∆ T ! =  1 + α"
"T
PT",0.7291242362525459,"1
T
PT
t ξ(t) N"
"T
PT",0.7311608961303462,"!  
4L2
θxϵ
λ
+ 4δ r L∆ T ! , (19)"
"T
PT",0.7331975560081466,"where ξ(t) = P b
K
k=1 N (t)
φ(k) −PK
k= b
K+1 N (t)
φ(k) to simplify the notations."
"T
PT",0.7352342158859471,"C
PROOF OF THEOREM 4.3"
"T
PT",0.7372708757637475,"Based on the convergence of FedAvg (Li et al., 2019), we proof Theorem 4.3 in this section."
"T
PT",0.7393075356415478,"First, we make the following assumptions and present some useful lemmas. Speciﬁcally, we make
the following assumptions. Assumption C.1 and C.2 are standard (typical examples are the ℓ2-norm
regularized linear regression, logistic regression, or softmax classiﬁer). Assumption C.3 and C.4 have
been made by the previous works (Zhang et al., 2013; Li et al., 2019).
Assumption C.1. L1, . . . , LK are all L-smooth: for all v and w, Lk(v) ≤Lk(w) + (v −
w)T ∇Lk(w) + L"
"T
PT",0.7413441955193483,"2 ||v −w||2
2.
Assumption C.2. L1, . . . , LK are all λ-strongly convex: for all v and w, Lk(v) ≥Lk(w) + (v −
w)T ∇Lk(w) + λ"
"T
PT",0.7433808553971487,"2 ||v −w||2
2."
"T
PT",0.745417515274949,"Assumption C.3. Let ξk
t be sampled from the k-th device’s local data uniformly at random. The
variance of stochastic gradients in each device is bounded: E||∇Lk(wk
t , ξk
t ) −∇Lk(wk
t )||2 ≤δ2
k
for k = 1, · · · , K.
Assumption C.4. The expected squared norm of stochastic gradients is uniformly bounded, i.e.,
E||∇Lk(wk
t , ξk
t )||2 ≤ς2 for all k = 1, · · · , K and t = 1, · · · , T −1."
"T
PT",0.7474541751527495,Under review as a conference paper at ICLR 2022
"T
PT",0.7494908350305499,"We use the following lemmas proved by Li et al. (2019). Let θk
t denotes the model parameter main-
tained in the k-th client at t-th step, Θ represents an immediate result of one step SGD update from
θk
t . For convenience, we deﬁne ¯Θt = PK
k=1
Nk"
"T
PT",0.7515274949083504,"N Θt, ¯θt == PK
k=1
Nk"
"T
PT",0.7535641547861507,"N θt, ¯gt = PK
k=1
Nk"
"T
PT",0.7556008146639511,"N ∇Lk(θk
t )
and gt = PK
k=1
Nk"
"T
PT",0.7576374745417516,"N ∇Lk(θk
t , ξk
t ). Therefore, Egt = ¯gt."
"T
PT",0.7596741344195519,"Lemma C.1 (Results of one step SGD). Assume Assumption C.1 and C.2. If ηt ≤
1
4L, we have"
"T
PT",0.7617107942973523,"E||¯Θt+1 −θ∗||2 ≤(1 −ηtλ)E||¯θt −θ∗||2
(20)"
"T
PT",0.7637474541751528,"+ η2
t E||gt −¯gt||2 + 6Lη2
t Γ + 2E K
X k=1 Nk"
"T
PT",0.7657841140529531,"N ||¯θt −θk
t ||2,"
"T
PT",0.7678207739307535,"where Γ = L∗−PK
k Et
Nk"
"T
PT",0.769857433808554,"N L∗
k ≥0
Lemma C.2 (Bounding the variance). Assume Assumption C.3. It follows that"
"T
PT",0.7718940936863544,"E||gt −¯gt||2 ≤ K
X k=1 Nk N"
"T
PT",0.7739307535641547,"2
δ2
k,
(21)"
"T
PT",0.7759674134419552,"Lemma C.3 (Bounding the divergence of θk
t ). Assume Assumption C.4, that ηt is non-increasing
and η ≤2ηt+E for all t > 0. It follows that E "" K
X k=1 Nk"
"T
PT",0.7780040733197556,"N ||¯θt −θk
t ||2
#"
"T
PT",0.780040733197556,"≤4η2
t (E −1)2ς2
(22)"
"T
PT",0.7820773930753564,"proof of Theorem 4.3. Let ∆t = E||θt −θ∗||2. From Lemma C.1, Lemma C.2 and Lemma C.3, it
follows that"
"T
PT",0.7841140529531568,"∆t+1 ≤(1 −ηtλ)∆t + η2
t B,
(23)"
"T
PT",0.7861507128309573,"where, B = K
X k"
"T
PT",0.7881873727087576,"P (T )
k
1 + α ξ(T ) N"
"T
PT",0.790224032586558,!2 Nk N δk
"T
PT",0.7922606924643585,"2
+ 6L  L∗− K
X k"
"T
PT",0.7942973523421588,"P (T )
k
1 + α ξ(T ) N Nk"
"T
PT",0.7963340122199593,"N L∗
k !"
"T
PT",0.7983706720977597,"+ 8(E −1)2ς2, (24)"
"T
PT",0.8004073319755601,"and

P (T )
k
1+α ξ(T ) N"
"T
PT",0.8024439918533605,"2
and
P (T )
k
1+α ξ(T )"
"T
PT",0.8044806517311609,"N
, are acted as the scalar timing by the personalized variance bound δ2
k
and the local optimum L∗
k of each client."
"T
PT",0.8065173116089613,"For a diminishing stepsize, ηt =
β
γ+t for some β > 1"
"T
PT",0.8085539714867617,λ and γ > 0 such that η1 ≤min{ 1
"T
PT",0.8105906313645621,"λ,
1
4L} =
1
4L
and ηt ≤2ηt+E. We will prove that ∆t ≤
ν
γ+t, where ν = max{ β2B"
"T
PT",0.8126272912423625,"βλ−1, (γ + 1)∆1}. The above
can be proved by induction. Firstly, the deﬁnition of ν ensures that it holds for t = 1. Assume the
conclusion holds for some t, it follows that,"
"T
PT",0.814663951120163,"∆t+1 ≤(1 −ηtλ)∆t + η2
t B"
"T
PT",0.8167006109979633,"≤(1 −
βλ
t + γ )
ν
t + γ +
β2B
(t + γ)2"
"T
PT",0.8187372708757638,= t + γ −1
"T
PT",0.8207739307535642,"(t + γ)2 ν + [
β2B
(t + γ)2 −βλ −1"
"T
PT",0.8228105906313645,(t + γ)2 ν]
"T
PT",0.824847250509165,"≤
ν
t + γ + 1. (25)"
"T
PT",0.8268839103869654,"Then by the L-smoothness of L(·),"
"T
PT",0.8289205702647657,E[Lt] −L∗≤L
"T
PT",0.8309572301425662,2 ∆t ≤L
"T
PT",0.8329938900203666,"2
ν
γ + t
(26)"
"T
PT",0.835030549898167,"Speciﬁcally, if we choose β = 2"
"T
PT",0.8370672097759674,"λ, γ = max{8 L"
"T
PT",0.8391038696537678,"λ , E} −1 and denote κ = L"
"T
PT",0.8411405295315683,"λ , then ηt = 2"
"T
PT",0.8431771894093686,"λ
1
γ+t. One
can verify that the choice of ηt satisﬁes ηt ≤2ηt+E for t ≥1. Then we have"
"T
PT",0.845213849287169,"ν = max
 β2B"
"T
PT",0.8472505091649695,"βλ −1, (γ + 1)∆1"
"T
PT",0.8492871690427699,"
≤
β2B
βλ −1 + (γ + 1)∆1 ≤4B"
"T
PT",0.8513238289205702,"λ2 + (γ + 1)∆1,
(27)"
"T
PT",0.8533604887983707,Under review as a conference paper at ICLR 2022 and
"T
PT",0.8553971486761711,E[Lt] −L∗≤L
"T
PT",0.8574338085539714,"2
ν
γ + t ≤
κ
γ + t 2B"
"T
PT",0.8594704684317719,"λ + λ(γ + 1) 2
∆1"
"T
PT",0.8615071283095723,"
(28)"
"T
PT",0.8635437881873728,"D
LEARNING FRAMEWORK AND ALGORITHM ෡𝑲"
"T
PT",0.8655804480651731,Clients
"T
PT",0.8676171079429735,"Server 𝜃1 t
𝜃2"
"T
PT",0.869653767820774,"t
𝜃𝐾−1 t
𝜃𝐾 t 𝜃t+1"
"T
PT",0.8716904276985743,... ... 𝑁1 𝑁ℒ1 𝑁1 𝑁ℒ1 𝑁2
"T
PT",0.8737270875763747,"𝑁ℒ2
𝑁𝐾−1"
"T
PT",0.8757637474541752,"𝑁
ℒ𝐾−1 𝑁𝐾"
"T
PT",0.8778004073319755,"𝑁ℒ𝐾
...
Ascending Sort   {
}"
"T
PT",0.879837067209776,"1
1
...
1 + α
⨀ M1 tM2"
"T
PT",0.8818737270875764,"t
θK−1 t 𝜃𝐾 t
}
{"
"T
PT",0.8839103869653768,Weighted Average 1 - α 𝑁2 𝑁ℒ2 𝑁𝐾−1
"T
PT",0.8859470468431772,"𝑁
ℒ𝐾−1 𝑁𝐾 𝑁ℒ𝐾"
"T
PT",0.8879837067209776,"Figure 5: A brief illustration of our α-Weighted Federated Adversarial Training (α-WFAT) framework.
On the client-side, each client will conduct adversarial training with its local data and update the
optimized model parameter (i.e., θk) with the adversarial loss (i.e., Nk"
"T
PT",0.890020366598778,"N Lk)). On the server-side, after
collecting the model parameters and the loss value (information of the robustness), the server will
conduct an ascending sort and aggregate the global model with a weighted average (denoted by ⊙)
which upweights the top populations of the robust client’s model parameters with α."
"T
PT",0.8920570264765784,"Here, we provide an intuitive illustration of our proposed α-WFAT in Figure 5. Based on the α-
weighted mechanism, we provide a new ﬂexible framework for the combination of adversarial training
with federated learning. It is orthogonal to a variety of different adversarial training (Zhang et al.,
2019; Wu et al., 2020; Zhang et al., 2020; Chen et al., 2020; 2021; Wang et al., 2020a; Jiang et al.,
2020; Wang et al., 2020c) methods and federated learning algorithms (Li et al., 2018; Kairouz et al.,
2019; Hong et al., 2021) which gain the adversarial robustness or alleviate the data heterogeneity on
the client side, and can be simply but effectively combined with other approach."
"T
PT",0.8940936863543788,"E
EXPERIMENTAL DETAILS"
"T
PT",0.8961303462321792,"Dataset.
We conduct the experiments on three benchmark datasets, i.e., SVHN (Netzer et al., 2011),
CIFAR-10 and CIFAR-100 (Krizhevsky, 2009) for federated adversarial training. For the IID scenario,
we randomly distribute these datasets to each client. For simulating the Non-IID scenario, we
follow McMahan et al. (2017); Shah et al. (2021) to distribute the training data based on their labels.
To be speciﬁc, a skew parameter s is utilized in the data partition introduced by Shah et al. (2021),
which enables K clients to get a majority of the data samples from a subset of classes. We denote the
set of all classes in a dataset as Y and create Yk by dividing all the class labels equally among K
clients. Accordingly, we split the data across K clients that each client has (100 −(K −1) × s)%
of data for the class in Yk and s% of data in other split sets. In our experiments, we set s = 2 for
simulating the Non-IID partition with 5 clients as Shah et al. (2021) recommended."
"T
PT",0.8981670061099797,"Training and Evaluation.
In the experiments, we follow the previous works to leverage the same
architectures, i.e., NIN (Lin et al., 2014) for CIFAR-10, ResNet-18 (He et al., 2016) for CIFAR-100 and
Small CNN (Zhang et al., 2020) for SVHN. For the local training batch size, we set 32 for CIFAR-10,
128 for CIFAR-100 and SVHN. For the training schedule, SGD is adopted with 0.9 momentum for
100 communication rounds under 5 clients as in (Hong et al., 2021; Shah et al., 2021), and the weight
decay = 0.0001. For adversarial training, we set the conﬁgurations of PGD respectively (Madry et al.,
2018) for different datasets. On CIFAR-10/CIFAR-100, we set the perturbation bound ϵ = 8/255,
the PGD step size 2/255 and set the PGD step number 10. On SVHN, we set the perturbation
bound ϵ = 4/255, the PGD step size 1/255, and keep the same step number 10. Regarding the
evaluation, the accuracy for the natural test data and that for the adversarial test data are computed"
"T
PT",0.90020366598778,Under review as a conference paper at ICLR 2022
"T
PT",0.9022403258655805,Table 3: Brief summary of the experimental details about α-WFAT
"T
PT",0.9042769857433809,"Dataset
Network
K
ˆK
α"
"T
PT",0.9063136456211812,"CIFAR-10
NIN (Shah et al., 2021)
5
1
1/6"
"T
PT",0.9083503054989817,"CIFAR-100
ResNet-18 (Chen et al., 2021)
5
1
1/41"
"T
PT",0.9103869653767821,"SVHN
SmallCNN (Zhang et al., 2019)
5
1
1/11"
"T
PT",0.9124236252545825,"following Wang et al. (2019); Wu et al. (2020). Note that, the adversarial test data are generated by
FGSM, PGD-20, C&W (Carlini & Wagner, 2017) attack with the same perturbation bound and step
size as the training. All the adversarial generations have a random start, i.e, the uniformly random
perturbation of [−ϵ, ϵ] added to the natural data before attacking iterations. Besides, we also report
the robustness under a stronger AutoAttack, termed as AA for simplicity."
"T
PT",0.9144602851323829,"As for our α-WFAT, different training tasks adopt different α-weighted mechanism considering
different characteristic of local training data, we set α = 1/6 (i.e., 1+α"
"T
PT",0.9164969450101833,"1−α = 1.4) for the experiments
on CIFAR-10, and α = 1/41 (i.e., 1+α"
"T
PT",0.9185336048879837,"1−α = 1.05) for the experiments on CIFAR-100 and α = 1/11
(i.e., 1+α"
"T
PT",0.9205702647657841,"1−α = 1.2) for the experiments on SVHN. As for FedProx, we set µ = 0.01 for each dataset
and its α for α-weighted mechanism are 1/11, 1/41, and 1/11. As for Scaffold, the α adopted for
previous datasets are 1/11, 1/101 and 1/11 respectively."
"T
PT",0.9226069246435845,"As for the choice of the hyper-parameter α, one useful way to set it might be progressively probing
its effect in a value-growth manner. When the α is very small, the objective will approximately
degenerate the original objective of FAT, so does the performance with no harm. Slightly enlarging α
can improve the performance due to the beneﬁt of the bias-variance trade-off, and then make a stop
in one point where the performance becomes drop."
"T
PT",0.924643584521385,"E.1
EMPHASIZE/DE-EMPHASIZE IN OUR α-WFAT"
"T
PT",0.9266802443991853,"We conduct an empirical comparison between α-WFAT that emphasizes (relatively de-emphasize
the other clients) the client model with the smallest loss and a contrary variant that de-emphasizes
it (relatively emphasize the other clients) as follows. We ﬁnd that de-emphasizing the client with
smallest loss (relatively emphasize those with larger loss) consistently harm the model performance
across these evaluations."
"T
PT",0.9287169042769857,Table 4: Comparison with emphasize/de-emphasize the client with smallest loss.
"T
PT",0.9307535641547862,"Setting
Non-IID"
"T
PT",0.9327902240325866,"CIFAR-10
Natural
FGSM
PGD-20
CW∞"
"T
PT",0.9348268839103869,FedAvg
"T
PT",0.9368635437881874,α-WFAT: 1+α
"T
PT",0.9389002036659878,"1−α = 1.4
emphasize
63.44%
45.13%
37.17%
33.99%
α-WFAT: 1+α"
"T
PT",0.9409368635437881,"1−α = 1.2
emphasize
62.26%
44.08%
35.83%
33.31%
FAT: 1+α"
"T
PT",0.9429735234215886,"1−α = 1.0
original
57.45%
39.44%
32.58%
30.52%
α-WFAT: 1+α"
"T
PT",0.945010183299389,"1−α = 0.8
de-emphasize
50.45%
34.34%
27.86%
26.62%
α-WFAT: 1+α"
"T
PT",0.9470468431771895,"1−α = 0.6
de-emphasize
40.47%
28.81%
24.36%
23.19%"
"T
PT",0.9490835030549898,"SVHN
Natural
FGSM
PGD-20
CW∞"
"T
PT",0.9511201629327902,FedAvg
"T
PT",0.9531568228105907,α-WFAT: 1+α
"T
PT",0.955193482688391,"1−α = 1.4
emphasize
90.60%
87.75%
73.12%
70.51%
α-WFAT: 1+α"
"T
PT",0.9572301425661914,"1−α = 1.2
emphasize
91.25%
88.28%
71.72%
69.79%
FAT: 1+α"
"T
PT",0.9592668024439919,"1−α = 1.0
original
91.24%
87.95%
68.87%
67.89%
α-WFAT: 1+α"
"T
PT",0.9613034623217923,"1−α = 0.8
de-emphasize
90.03%
86.12%
64.35%
64.32%
α-WFAT: 1+α"
"T
PT",0.9633401221995926,"1−α = 0.6
de-emphasize
89.46%
84.80%
58.64%
58.96%"
"T
PT",0.9653767820773931,Under review as a conference paper at ICLR 2022
"T
PT",0.9674134419551935,"E.2
MEAN AND STD RESULTS OF THE NON-IID SETTINGS"
"T
PT",0.9694501018329938,"Considering that the Non-IID results are sensitive to the selection of data in each client, we conduct
our experiments on Non-IID settings for multiple times and conclude the results as follows. In
summary, our α-WFAT can consistently improve the model robustness with comparable or even
better natural performance than previous federated optimization methods."
"T
PT",0.9714867617107943,"Table 5:
Performance on Non-IID settings under different federated optimization methods
(Mean±Std)."
"T
PT",0.9735234215885947,"Setting
Non-IID"
"T
PT",0.9755600814663951,"CIFAR-10
Natural
FGSM
PGD-20
CW∞
AA"
"T
PT",0.9775967413441955,"FedAvg
FAT
58.13±0.68%
40.06±0.62%
32.56±0.01%
30.88±0.37%
29.17±0.03%
α-WFAT
63.36±0.07%
44.82±0.32%
37.14±0.03%
33.39±0.61%
31.66±0.70%"
"T
PT",0.9796334012219959,"FedProx
FAT
59.95±0.45%
41.44±0.15%
33.83±0.01%
31.65±0.36%
30.11±0.09%
α-WFAT
62.04±0.47%
44.21±0.08%
36.64±0.11%
32.62±0.20%
31.83±0.15%"
"T
PT",0.9816700610997964,"Scaffold
FAT
61.44±1.37%
42.85±0.76%
34.08±0.05%
32.56±0.02%
31.03±0.08%
α-WFAT
63.16±0.96%
45.55±0.50%
37.33±0.02%
34.82±0.04%
33.32±0.01%"
"T
PT",0.9837067209775967,"CIFAR-100
Natural
FGSM
PGD-20
CW∞
AA"
"T
PT",0.9857433808553971,"FedAvg
FAT
31.93±0.85%
20.04±0.44%
16.34±0.18%
13.65±0.28%
12.70±0.03%
α-WFAT
34.80±0.37%
20.91±0.35%
16.66±0.06%
13.78±0.13%
12.79±0.04%"
"T
PT",0.9877800407331976,"FedProx
FAT
34.07±0.74%
20.49±0.29%
16.20±0.12%
13.68±0.06%
12.67±0.08%
α-WFAT
33.95±0.39%
20.86±0.40%
16.73±0.01%
13.80±0.04%
12.80±0.02%"
"T
PT",0.9898167006109979,"Scaffold
FAT
39.89±1.28%
24.78±0.40%
19.82±0.20%
16.73±0.01%
15.51±0.02%
α-WFAT
39.80±1.27%
25.05±0.51%
20.27±0.13%
16.79±0.01%
15.58±0.01%"
"T
PT",0.9918533604887984,"SVHN
Natural
FGSM
PGD-20
CW∞
AA"
"T
PT",0.9938900203665988,"FedAvg
FAT
91.52±0.28%
88.13±0.18%
68.98±0.11%
68.04±0.15%
66.59±0.04%
α-WFAT
91.26±0.01%
88.27±0.02%
72.04±0.32%
69.96±0.16%
68.89±0.27%"
"T
PT",0.9959266802443992,"FedProx
FAT
91.00±0.08%
87.65±0.15%
68.48±0.04%
67.16±0.02%
65.76±0.18%
α-WFAT
91.19±0.06%
88.15±0.01%
71.84±0.30%
69.88±0.35%
68.84±0.37%"
"T
PT",0.9979633401221996,"Scaffold
FAT
90.82±0.87%
87.89±0.66%
69.51±0.84%
68.12±0.88%
67.19±0.54%
α-WFAT
90.93±0.76%
88.27±0.45%
71.77±0.38%
69.49±0.67%
68.37±0.48%"
