Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0010111223458038423,"Disabled people constitute a significant part of the global population, deserving
1"
ABSTRACT,0.0020222446916076846,"of inclusive consideration and empathetic support. However, the current human-
2"
ABSTRACT,0.003033367037411527,"computer interaction based on keyboards may not meet the requirements of disabled
3"
ABSTRACT,0.004044489383215369,"people. The small size, ease of wearing, and low cost of inertial sensors make
4"
ABSTRACT,0.005055611729019211,"inertial sensor-based writing recognition a promising human-computer interaction
5"
ABSTRACT,0.006066734074823054,"option for disabled people. However, accurate recognition relies on massive inertial
6"
ABSTRACT,0.007077856420626896,"signal samples, which are hard to collect for the Chinese context due to the vast
7"
ABSTRACT,0.008088978766430738,"number of characters. Therefore, we design a Chinese inertial generative adversarial
8"
ABSTRACT,0.00910010111223458,"network (CI-GAN) containing Chinese glyph encoding (CGE), forced optimal
9"
ABSTRACT,0.010111223458038422,"transport (FOT), and semantic relevance alignment (SRA) to acquire unlimited high-
10"
ABSTRACT,0.011122345803842264,"quality training samples. Unlike existing vectorization focusing on the meaning of
11"
ABSTRACT,0.012133468149646108,"Chinese characters, CGE represents the shape and stroke features, providing glyph
12"
ABSTRACT,0.01314459049544995,"guidance for GAN to generate writing signals. FOT constrains feature consistency
13"
ABSTRACT,0.014155712841253791,"between generated and real signals through the designed forced feature matching
14"
ABSTRACT,0.015166835187057633,"mechanism, meanwhile addressing GANs’ mode collapse and mixing issues by
15"
ABSTRACT,0.016177957532861477,"introducing Wasserstein distance. SRA captures the semantic relevance between
16"
ABSTRACT,0.017189079878665317,"various Chinese glyphs and injects this information into the GAN to establish
17"
ABSTRACT,0.01820020222446916,"batch-level constraints and set higher standards of generated signal quality. By
18"
ABSTRACT,0.019211324570273004,"utilizing the massive training samples provided by CI-GAN, the performance of
19"
ABSTRACT,0.020222446916076844,"six widely used classifiers is improved from 6.7% to 98.4%, indicating that CI-
20"
ABSTRACT,0.021233569261880688,"GAN constructs a flexible and efficient data platform for Chinese inertial writing
21"
ABSTRACT,0.022244691607684528,"recognition. Furthermore, we release the first Chinese writing recognition dataset
22"
ABSTRACT,0.023255813953488372,"based on inertial sensors in GitHub.
23"
INTRODUCTION,0.024266936299292215,"1
Introduction
24"
INTRODUCTION,0.025278058645096056,"One of the most significant obstacles for disabled individuals in their daily lives is the lack of efficient
25"
INTRODUCTION,0.0262891809908999,"human-computer interaction (HCI) methods [1]. Traditional keyboard-based HCI systems often fail
26"
INTRODUCTION,0.027300303336703743,"to meet the specific needs of disabled users, particularly those who are visually impaired or have lost
27"
INTRODUCTION,0.028311425682507583,"their fingers, which underscores the urgent need for developing technologies that cater to the unique
28"
INTRODUCTION,0.029322548028311426,"requirements of disabled individuals [2]. Providing tailored HCI solutions not only enhances their
29"
INTRODUCTION,0.030333670374115267,"quality of life and independence but also facilitates their integration into society, enabling greater
30"
INTRODUCTION,0.03134479271991911,"participation in education, employment, and social activities. Such technological advancements hold
31"
INTRODUCTION,0.032355915065722954,"profound significance, creating a more inclusive and equitable society.
32"
INTRODUCTION,0.033367037411526794,"As efficient motion-sensing components, inertial sensors can play a crucial role in recognizing writing
33"
INTRODUCTION,0.034378159757330634,"movements. Inertial sensors can measure the acceleration and angular velocity of moving objects,
34"
INTRODUCTION,0.03538928210313448,"making it possible to convert written characters into digital text [3, 4, 5, 6]. Due to their small size,
35"
INTRODUCTION,0.03640040444893832,"ease of integration, low power consumption, and low cost, inertial sensors are widely used in electronic
36"
INTRODUCTION,0.03741152679474216,"devices such as smartphones, smartwatches, and fitness bands [7, 8, 9, 10], making them particularly
37"
INTRODUCTION,0.03842264914054601,"suitable for disabled users. Inertial sensors can be integrated into wearable devices, providing a more
38"
INTRODUCTION,0.03943377148634985,"accessible and user-friendly means for disabled individuals to interact with computers and other digital
39"
INTRODUCTION,0.04044489383215369,"devices. By capturing the subtle movements of a user’s hand or other body parts, inertial sensors can
40"
INTRODUCTION,0.041456016177957536,"translate these motions into written text, enabling effective communication and interaction without
41"
INTRODUCTION,0.042467138523761376,"the need for a traditional keyboard. In addition, unlike optical or acoustic sensors, inertial sensors are
42"
INTRODUCTION,0.043478260869565216,"highly resistant to external factors such as lighting conditions, physical obstructions, or environmental
43"
INTRODUCTION,0.044489383215369056,"noise, which showcases their unique robustness in motion capture [11, 12, 13, 14, 15]. Consequently,
44"
INTRODUCTION,0.0455005055611729,"inertial sensors provide a medium for Chinese character writing recognition that aligns with natural
45"
INTRODUCTION,0.046511627906976744,"writing habits and can be seamlessly integrated into the writing process. With the widespread adoption
46"
INTRODUCTION,0.047522750252780584,"of smart devices, the technology of Chinese character writing recognition based on inertial sensors
47"
INTRODUCTION,0.04853387259858443,"may redefine the Chinese character input in the digital age, offering disabled people a comfortable
48"
INTRODUCTION,0.04954499494438827,"human-computer interaction methods.
49"
INTRODUCTION,0.05055611729019211,"However, the major challenge in achieving accurate Chinese writing recognition using inertial sensors
50"
INTRODUCTION,0.05156723963599596,"is obtaining large-scale, diverse inertial writing data samples. For any recognition model aimed
51"
INTRODUCTION,0.0525783619817998,"at accurately analyzing the complex strokes and structures of Chinese characters, it is crucial to
52"
INTRODUCTION,0.05358948432760364,"train the model with extensive, diverse writing samples [16]. Considering that the collection and
53"
INTRODUCTION,0.054600606673407485,"processing of Chinese writing samples are laborious and require high data quality and diversity, this
54"
INTRODUCTION,0.055611729019211326,"task becomes exceedingly challenging and increasingly difficult as the number of characters increases.
55"
INTRODUCTION,0.056622851365015166,"Therefore, generating realistic Chinese writing signals based on inertial sensors has become a central
56"
INTRODUCTION,0.057633973710819006,"technological challenge in recognizing Chinese writing.
57"
INTRODUCTION,0.05864509605662285,"To acquire high-quality, diverse samples of inertial Chinese writing, we applied GAN for IMU writing
58"
INTRODUCTION,0.05965621840242669,"signal generation for the first time and proposed CI-GAN, which can generate unlimited inertial writing
59"
INTRODUCTION,0.06066734074823053,"signals for an input Chinese character, thereby providing rich training samples for Chinese writing
60"
INTRODUCTION,0.06167846309403438,"recognition classifiers. CI-GAN provides a more intuitive and natural human-computer interaction
61"
INTRODUCTION,0.06268958543983821,"method for the Chinese context and advances the application of smart devices with Chinese input.
62"
INTRODUCTION,0.06370070778564206,"The main contributions of this paper are summarized as follows.
63"
INTRODUCTION,0.06471183013144591,"• Considering traditional Chinese character embedding methods that only focus on the meaning
64"
INTRODUCTION,0.06572295247724974,"of characters, we propose a Chinese glyph encoding (CGE), which represents the shape
65"
INTRODUCTION,0.06673407482305359,"and structure of Chinese characters. CGE not only injects glyph and writing semantics into
66"
INTRODUCTION,0.06774519716885744,"the generation of inertial signals but also provides new tools for studying the evolution and
67"
INTRODUCTION,0.06875631951466127,"development of hieroglyphs.
68"
INTRODUCTION,0.06976744186046512,"• We propose a forced optimal transport (FOT) loss for GAN, which not only avoids mode
69"
INTRODUCTION,0.07077856420626896,"collapse and mode mixing during signal generation but also ensures feature consistency be-
70"
INTRODUCTION,0.0717896865520728,"tween the generated and real signals through a designed forced feature matching mechanism,
71"
INTRODUCTION,0.07280080889787664,"thereby enhancing the authenticity of the generated signals.
72"
INTRODUCTION,0.07381193124368049,"• To inject batch-level character semantic correlations into GAN and establish macro con-
73"
INTRODUCTION,0.07482305358948432,"straints, we propose a semantic relevance alignment (SRA), which aligns the relevance
74"
INTRODUCTION,0.07583417593528817,"between generated signals and corresponding Chinese glyphs, thereby ensuring that the
75"
INTRODUCTION,0.07684529828109202,"motion characteristics of the generated signal conform to the Chinese character structure.
76"
INTRODUCTION,0.07785642062689585,"• Utilizing the training samples provided by CI-GAN, we increase the Chinese writing recog-
77"
INTRODUCTION,0.0788675429726997,"nition performance of six widely used classifiers from 6.7% to 98.4%. Furthermore, we
78"
INTRODUCTION,0.07987866531850354,"provide the application scenarios and strategies of 6 classifiers in writing recognition ac-
79"
INTRODUCTION,0.08088978766430738,"cording to their performance metrics. For the sake of sharing, we release the first Chinese
80"
INTRODUCTION,0.08190091001011122,"writing recognition dataset based on inertial sensors in GitHub.
81"
RELATED WORK,0.08291203235591507,"2
Related Work
82"
RELATED WORK,0.0839231547017189,"The technology for recognizing Chinese handwriting movements has the potential to bridge the gap
83"
RELATED WORK,0.08493427704752275,"between traditional writing and digital input, providing disabled individuals with a natural way of
84"
RELATED WORK,0.0859453993933266,"writing and greatly enhancing their ability to participate in digital communication, education, and
85"
RELATED WORK,0.08695652173913043,"employment. It also offers a new human-computer interaction avenue for normal people. Hence,
86"
RELATED WORK,0.08796764408493428,"Chinese handwriting movement recognition has garnered significant attention in recent years, leading
87"
RELATED WORK,0.08897876643073811,"to numerous related research achievements. Ren et al. utilized the Leap Motion device to propose
88"
RELATED WORK,0.08998988877654196,"an RNN-based method for recognizing Chinese characters written in the air [17]. The Leap Motion
89"
RELATED WORK,0.0910010111223458,"sensor, consisting of two infrared emitters and two cameras, can accurately capture the motion of
90"
RELATED WORK,0.09201213346814964,"hands in three-dimensional (3D) space [18]. However, the Leap Motion device is sensitive to lighting
91"
RELATED WORK,0.09302325581395349,"conditions, and either too strong or too weak light can interfere with the transmission and reception
92"
RELATED WORK,0.09403437815975733,"of infrared rays, affecting the recognition effect [19]. Additionally, the detection space of the Leap
93"
RELATED WORK,0.09504550050556117,"Motion device is an inverted quadrangular pyramid, limiting its field of view. Movements outside
94"
RELATED WORK,0.09605662285136501,"this range cannot be captured. Most importantly, the Leap Motion device is expensive and requires a
95"
RELATED WORK,0.09706774519716886,"connection to a computer or VR headset to function, severely limiting its application prospects [20].
96"
RELATED WORK,0.0980788675429727,"As wireless networks become more prevalent, Wi-Fi signals are gradually being applied to motion
97"
RELATED WORK,0.09908998988877654,"capture [21, 22]. Since Wi-Fi signals can penetrate objects and are unaffected by lighting conditions,
98"
RELATED WORK,0.10010111223458039,"they have a broader application scope than optical motion capture systems [23, 24]. Guo et al. used
99"
RELATED WORK,0.10111223458038422,"the channel state information (CSI), extracted from Wi-Fi signals reflected by hand movements,
100"
RELATED WORK,0.10212335692618807,"to recognize 26 air-written English letters [25]. However, while Wi-Fi signals do not have visual
101"
RELATED WORK,0.10313447927199192,"range limitations and can penetrate obstacles, they are easily disturbed by other signals on the same
102"
RELATED WORK,0.10414560161779575,"unlicensed band, severely affecting system performance. Moreover, the sampling frequency and
103"
RELATED WORK,0.1051567239635996,"resolution of Wi-Fi signals are very limited, making it difficult to capture detailed information during
104"
RELATED WORK,0.10616784630940344,"the writing process and, thus, hard to recognize air-written Chinese characters accurately [26, 27].
105"
RELATED WORK,0.10717896865520728,"Despite the advantages of low cost, wearability, and low power consumption offered by inertial
106"
RELATED WORK,0.10819009100101112,"sensors, there is currently a lack of large-scale, high-quality public datasets, causing few studies to use
107"
RELATED WORK,0.10920121334681497,"inertial sensors for 3D Chinese handwriting recognition [28, 29, 30, 31]. To collect data, Zhang et al.
108"
RELATED WORK,0.1102123356926188,"employed 12 volunteers, each of whom was asked to write the assigned Chinese characters on paper
109"
RELATED WORK,0.11122345803842265,"30 times [32]. The inertial measurement unit (IMU) built into smartwatches was used to collect the
110"
RELATED WORK,0.1122345803842265,"motion signals of the volunteers while writing, ultimately achieving a recognition accuracy of 90.2%
111"
RELATED WORK,0.11324570273003033,"for 200 Chinese characters. However, this study aims to identify the signals of normal individuals
112"
RELATED WORK,0.11425682507583418,"writing on paper, which is not applicable to people with disabilities. Moreover, this method can
113"
RELATED WORK,0.11526794742163801,"only realize desktop-based 2D writing recognition, which reduces the comfort and flexibility of the
114"
RELATED WORK,0.11627906976744186,"writing process, inherently limiting the application scenarios of Chinese handwriting recognition.
115"
RELATED WORK,0.1172901921132457,"Additionally, this method cannot effectively recognize massive Chinese characters due to the physical
116"
RELATED WORK,0.11830131445904954,"and mental limitations of volunteers for data collection. Considering the vast number of Chinese
117"
RELATED WORK,0.11931243680485339,"characters, providing large-scale, high-quality writing signal samples for each character is nearly
118"
RELATED WORK,0.12032355915065723,"impossible, which has become the most significant bottleneck limiting the development of Chinese
119"
RELATED WORK,0.12133468149646107,"handwriting recognition technology based on inertial sensors. Therefore, designing a model for
120"
RELATED WORK,0.12234580384226491,"generating Chinese handwriting signals provides researchers with an endless supply of signal samples
121"
RELATED WORK,0.12335692618806876,"and a flexible, convenient experimental data platform, accelerating the development and testing of
122"
RELATED WORK,0.1243680485338726,"new algorithms and supporting the research and application of Chinese handwriting recognition.
123"
METHOD,0.12537917087967643,"3
Method
124"
METHOD,0.1263902932254803,"To generate inertial writing signals for Chinese characters, we propose the Chinese inertial generative
125"
METHOD,0.12740141557128412,"adversarial network (CI-GAN), as shown in Fig. 1. For an input Chinese character, its one-hot
126"
METHOD,0.12841253791708795,"encoding is transformed into glyph encoding using our designed glyph encoding dictionary, which
127"
METHOD,0.12942366026289182,"stores the glyph shapes and stroke features of different Chinese characters. Thus, the obtained Chinese
128"
METHOD,0.13043478260869565,"glyph encoding contains rich writing features of the input character. This glyph encoding, along
129"
METHOD,0.13144590495449948,"with a random noise vector, is fed into a GAN, generating the synthetic IMU signal for the character,
130"
METHOD,0.13245702730030334,"where glyph encoding provides glyph and stroke features of the input character, while the random
131"
METHOD,0.13346814964610718,"noise introduces randomness to the virtual signal generation, ensuring the diversity and variability of
132"
METHOD,0.134479271991911,"the generated signals. To ensure that the GAN learns the IMU signal patterns for each character, we
133"
METHOD,0.13549039433771487,"designed a forced optimal transport (FOT) loss, which not only mitigates the issues of mode collapse
134"
METHOD,0.1365015166835187,"and mode mixing typically observed in GAN frameworks but also forces the generated IMU signals
135"
METHOD,0.13751263902932254,"to closely resemble the actual handwriting signals in terms of semantic features, fluctuation trends,
136"
METHOD,0.1385237613751264,"and kinematic properties. Moreover, a semantic relevance alignment (SRA) is proposed to provide
137"
METHOD,0.13953488372093023,"batch-level macro constraints for GAN, thereby keeping the correlation between generated signals
138"
METHOD,0.14054600606673406,"consistent with the correlation between Chinese character glyphs. Equipped with CGE, FOT and
139"
METHOD,0.14155712841253792,"SRA, CI-GAN can provide unlimited high-quality training samples for Chinese character writing
140"
METHOD,0.14256825075834176,"recognition, thereby enhancing the accuracy and robustness of various classifiers.
141"
CHINESE GLYPH ENCODING,0.1435793731041456,"3.1
Chinese Glyph Encoding
142"
CHINESE GLYPH ENCODING,0.14459049544994945,"In one-hot encoding, each Chinese character is represented by a high-dimensional sparse vector
143"
CHINESE GLYPH ENCODING,0.14560161779575329,"(where only one element is 1, and all others are 0), which results in all characters being equidistant
144"
CHINESE GLYPH ENCODING,0.14661274014155712,"in the vector space, thereby losing the abundant semantic information contained in the characters.
145"
CHINESE GLYPH ENCODING,0.14762386248736098,"Figure 1: Flowchart of Chinese inertial generative adversarial network. The Chinese character ”数”
is input into the model, and its one-hot encoding is converted into glyph encoding (green cubes),
which is then input into GAN together with random noise (blue cubes of different colors)."
CHINESE GLYPH ENCODING,0.1486349848331648,"Therefore, one-hot encoding fails to inject rich information into GAN. Although there are some
146"
CHINESE GLYPH ENCODING,0.14964610717896865,"commonly used Chinese character embeddings, these embeddings store meaning information of
147"
CHINESE GLYPH ENCODING,0.1506572295247725,"the characters, not glyph information (i.e., shape, structure and writing strokes). For example, the
148"
CHINESE GLYPH ENCODING,0.15166835187057634,"characters ”天” (sky) and ”夫” (husband) are quite similar in writing motions, but their meanings
149"
CHINESE GLYPH ENCODING,0.15267947421638017,"are significantly different. To this end, we propose a Chinese glyph encoding (CGE), which encodes
150"
CHINESE GLYPH ENCODING,0.15369059656218403,"Chinese characters based on their glyph shapes and writing actions.
151"
CHINESE GLYPH ENCODING,0.15470171890798787,"Considering that the inertial sensor signals capture the writing motion of Chinese characters, the
152"
CHINESE GLYPH ENCODING,0.1557128412537917,"motion signal exactly contains glyph information, which encourages simultaneous learning signal
153"
CHINESE GLYPH ENCODING,0.15672396359959556,"generation and Chinese glyph encoding under the supervision of real signals. Therefore, we create a
154"
CHINESE GLYPH ENCODING,0.1577350859453994,"learnable weight matrix W after the one-hot input layer to capture the glyph information. When a
155"
CHINESE GLYPH ENCODING,0.15874620829120323,"Chinese character is input into CI-GAN in one-hot encoding, it first passes through this weight matrix.
156"
CHINESE GLYPH ENCODING,0.1597573306370071,"Since only one element in the one-hot encoding is 1, and the rest are 0, multiplying one-hot encoding
157"
CHINESE GLYPH ENCODING,0.16076845298281092,"by the weight matrix W means obtaining one row of the matrix W. Hence, each row of W can be
158"
CHINESE GLYPH ENCODING,0.16177957532861476,"seen as an encoding of a Chinese character, and this matrix can serve as a glyph encoding dictionary
159"
CHINESE GLYPH ENCODING,0.16279069767441862,"of Chinese characters. However, an unguided Chinese encoding dictionary often struggles to capture
160"
CHINESE GLYPH ENCODING,0.16380182002022245,"the differences in glyph shapes among different characters, assigning similar glyph encodings to
161"
CHINESE GLYPH ENCODING,0.16481294236602628,"characters with distinct glyphs. To address this, we propose a glyph encoding regularization (GER),
162"
CHINESE GLYPH ENCODING,0.16582406471183014,"which enhances the orthogonality of all character encoding vectors and increases their information
163"
CHINESE GLYPH ENCODING,0.16683518705763398,"entropy to store as many glyph features of the characters as possible, thereby avoiding triviality like
164"
CHINESE GLYPH ENCODING,0.1678463094034378,"one-hot encoding. Specifically, we use the α-order Rényi entropy to measure the information content
165"
CHINESE GLYPH ENCODING,0.16885743174924167,"of the glyph encoding dictionary W, calculated as follows:
166"
CHINESE GLYPH ENCODING,0.1698685540950455,"Sα(W) =
1
1 −αlog2(tr( ˜Gα)), where ˜Gij = 1"
CHINESE GLYPH ENCODING,0.17087967644084934,"N
Gij
p"
CHINESE GLYPH ENCODING,0.1718907987866532,"Gii · Gjj
, Gij =
D
W (i), W (j)E
.
(1)"
CHINESE GLYPH ENCODING,0.17290192113245703,"where, N represents the number of Chinese characters, which corresponds to the number of rows in
167"
CHINESE GLYPH ENCODING,0.17391304347826086,"the weight (encoding) matrix W. G is the Gram matrix of W, where Gij equal to the inner product
168"
CHINESE GLYPH ENCODING,0.17492416582406473,"of the i-th and j-th rows of W, and ˜G is the trace-normalized G, i.e., tr( ˜G) = 1. In similar problems,
169"
CHINESE GLYPH ENCODING,0.17593528816986856,"α is generally set to 2 for optimal results. Sα(W) measures the information content of the glyph
170"
CHINESE GLYPH ENCODING,0.1769464105156724,"encoding matrix W. A larger Sα(W) indicates more information encoded in W, meaning the glyph
171"
CHINESE GLYPH ENCODING,0.17795753286147623,"encodings are more informative. Meanwhile, as Sα(W) increases, all elements in the Gram matrix
172"
CHINESE GLYPH ENCODING,0.1789686552072801,"G are forced to decrease, indicating that different encoding vectors have stronger orthogonality. It
173"
CHINESE GLYPH ENCODING,0.17997977755308392,"is evident that the improvement of Sα(W) simultaneously enhances the information content and
174"
CHINESE GLYPH ENCODING,0.18099089989888775,"the orthogonality among the encodings. In light of this, the glyph encoding regularization Rencode is
175"
CHINESE GLYPH ENCODING,0.1820020222446916,"constructed as Rencode =
1
Sα(W ). As Rencode decreases during training, Sα(W) gradually increases,
176"
CHINESE GLYPH ENCODING,0.18301314459049545,"meaning the glyph encoding dictionary stores more information while enhancing the orthogonality
177"
CHINESE GLYPH ENCODING,0.18402426693629928,"among all Chinese glyph encodings, effectively representing the differences in glyph shapes among
178"
CHINESE GLYPH ENCODING,0.18503538928210314,"all characters. Thus, this glyph encoding can inject sufficient glyph information into GAN, ensuring
179"
CHINESE GLYPH ENCODING,0.18604651162790697,"that the generated signals maintain consistency with the target character’s glyph.
180"
FORCED OPTIMAL TRANSPORT,0.1870576339737108,"3.2
Forced Optimal Transport
181"
FORCED OPTIMAL TRANSPORT,0.18806875631951467,"Ensuring the authenticity of virtual signals poses the greatest challenge when generating diverse
182"
FORCED OPTIMAL TRANSPORT,0.1890798786653185,"signals, especially in following physical laws and simulating the potential dynamical characteristics
183"
FORCED OPTIMAL TRANSPORT,0.19009100101112233,"of actual motions. To this end, we propose the forced feature matching (FFM), which ensures that the
184"
FORCED OPTIMAL TRANSPORT,0.1911021233569262,"generated signal feature closely matches the real signal feature and the corresponding glyph encoding.
185"
FORCED OPTIMAL TRANSPORT,0.19211324570273003,"Specifically, we use a pre-trained variational autoencoder to extract the real signal feature hT and
186"
FORCED OPTIMAL TRANSPORT,0.19312436804853386,"generated signal feature hG. Then, the consistency of hT , hG, and the corresponding glyph encoding
187"
FORCED OPTIMAL TRANSPORT,0.19413549039433772,"e is constrained by LF F M.
188"
FORCED OPTIMAL TRANSPORT,0.19514661274014156,"LF F M = 1 −
⟨hG, hT ⟩+ ⟨hG, e⟩+ ⟨e, hT ⟩
∥hG∥∥hT ∥+ ∥hG∥∥e∥+ ∥e∥∥hT ∥.
(2)"
FORCED OPTIMAL TRANSPORT,0.1961577350859454,"Another critical challenge lies in the mode collapse and mode mixing issue inherent to GAN archi-
189"
FORCED OPTIMAL TRANSPORT,0.19716885743174925,"tectures. Mode collapse limits the diversity of generated signal samples, causing GAN to generate
190"
FORCED OPTIMAL TRANSPORT,0.19817997977755308,"signals only for a few Chinese characters, regardless of the diversity of input. On the other hand, mode
191"
FORCED OPTIMAL TRANSPORT,0.19919110212335692,"mixing problems cause the generated signal to contain blend characteristics of multiple modes, which
192"
FORCED OPTIMAL TRANSPORT,0.20020222446916078,"is unrealistic and unrecognizable. To address these issues, we introduce the optimal transport to GAN,
193"
FORCED OPTIMAL TRANSPORT,0.2012133468149646,"which utilizes the Wasserstein distance as a loss function. Traditional GANs use the Jensen-Shannon
194"
FORCED OPTIMAL TRANSPORT,0.20222446916076844,"divergence as the loss metric, which becomes ineffective when the distributions of real and generated
195"
FORCED OPTIMAL TRANSPORT,0.2032355915065723,"data have little overlap, leading to mode collapse. The Wasserstein distance provides a more effective
196"
FORCED OPTIMAL TRANSPORT,0.20424671385237614,"gradient even when the distributions are disjoint or significantly different, thereby preventing mode
197"
FORCED OPTIMAL TRANSPORT,0.20525783619817997,"collapse. Furthermore, unlike the Jensen-Shannon divergence, the Wasserstein distance exhibits
198"
FORCED OPTIMAL TRANSPORT,0.20626895854398383,"insensitivity to the balance between the training of the generator and discriminator, thereby alleviating
199"
FORCED OPTIMAL TRANSPORT,0.20728008088978767,"mode mixing (We provide a rigorous mathematical proof in Appendix C). Combing OT and FFM
200"
FORCED OPTIMAL TRANSPORT,0.2082912032355915,"constraints, we can obtain the forced optimal transport loss LF OT = W(PT , PG) + λ · LF F M,
201"
FORCED OPTIMAL TRANSPORT,0.20930232558139536,"where W(PT , PG) is the optimal transport loss, representing the Wasserstein distance between the
202"
FORCED OPTIMAL TRANSPORT,0.2103134479271992,"distributions of real and generated signals, enhancing the stability and diversity of the samples. λ
203"
FORCED OPTIMAL TRANSPORT,0.21132457027300303,"is a weighting coefficient for the forced feature matching loss LF F M. As LF F M decreases during
204"
FORCED OPTIMAL TRANSPORT,0.2123356926188069,"training, the generated signals increasingly approximate the characteristics of real signals.
205"
SEMANTIC RELEVANCE ALIGNMENT,0.21334681496461072,"3.3
Semantic Relevance Alignment
206"
SEMANTIC RELEVANCE ALIGNMENT,0.21435793731041455,"Figure 2: Diagram of semantic relevance align-
ment."
SEMANTIC RELEVANCE ALIGNMENT,0.21536905965621841,"As motion records of Chinese writing, the se-
207"
SEMANTIC RELEVANCE ALIGNMENT,0.21638018200202225,"mantic relationships between generated signals
208"
SEMANTIC RELEVANCE ALIGNMENT,0.21739130434782608,"should align with the relationships between Chi-
209"
SEMANTIC RELEVANCE ALIGNMENT,0.21840242669362994,"nese character glyphs.
To ensure the gener-
210"
SEMANTIC RELEVANCE ALIGNMENT,0.21941354903943378,"ated inertial signals accurately reflect the char-
211"
SEMANTIC RELEVANCE ALIGNMENT,0.2204246713852376,"acter relationships between Chinese character
212"
SEMANTIC RELEVANCE ALIGNMENT,0.22143579373104147,"glyphs, we propose semantic relevance align-
213"
SEMANTIC RELEVANCE ALIGNMENT,0.2224469160768453,"ment (SRA), which ensures consistency between
214"
SEMANTIC RELEVANCE ALIGNMENT,0.22345803842264914,"the glyph encoding relationships and the signal
215"
SEMANTIC RELEVANCE ALIGNMENT,0.224469160768453,"feature relationships, thereby providing batch-
216"
SEMANTIC RELEVANCE ALIGNMENT,0.22548028311425683,"level macro guidance for GANs and enhancing
217"
SEMANTIC RELEVANCE ALIGNMENT,0.22649140546006066,"the quality of the generated signals. For each
218"
SEMANTIC RELEVANCE ALIGNMENT,0.2275025278058645,"batch of input Chinese characters, we compute
219"
SEMANTIC RELEVANCE ALIGNMENT,0.22851365015166836,"the pairwise cosine similarities of their Chinese
220"
SEMANTIC RELEVANCE ALIGNMENT,0.2295247724974722,"glyph encodings to form an encoding similarity
221"
SEMANTIC RELEVANCE ALIGNMENT,0.23053589484327602,"matrix Me . Simultaneously, the pairwise cosine
222"
SEMANTIC RELEVANCE ALIGNMENT,0.23154701718907988,"similarities of generated signal features (extracted by the pre-trained VAE) are computed to form a
223"
SEMANTIC RELEVANCE ALIGNMENT,0.23255813953488372,"feature similarity matrix Mh. Then, the loss of semantic relevance alignment LSRA = ∥Mh −Me∥2
2
224"
SEMANTIC RELEVANCE ALIGNMENT,0.23356926188068755,"is established to minimize the difference between the two matrices, thereby ensuring that the semantic
225"
SEMANTIC RELEVANCE ALIGNMENT,0.2345803842264914,"relationships in the input character glyphs are accurately contained in the generated signals.
226"
EXPERIMENTS AND RESULTS,0.23559150657229525,"4
Experiments and Results
227"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.23660262891809908,"4.1
Data Collection and Experimental Setup
228"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.23761375126390294,"Table 1: The built-in IMU specifications of some
smartphones. Note that since the IMUs in some
types of iPhones are customized by the manufac-
turer, the model and price are not disclosed."
DATA COLLECTION AND EXPERIMENTAL SETUP,0.23862487360970677,"Dataset
Smartphone
Release Time
IMU
Unit price"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2396359959555106,Training
DATA COLLECTION AND EXPERIMENTAL SETUP,0.24064711830131447,"iPhone 13 pro
Sep. 2021
Undisclosed
/
HUAWEI P40
Mar. 2020
LSM6DSM
$0.30
HUAWEI P40 Pro
Apr. 2020
LSM6DSO
$0.33"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2416582406471183,Testing
DATA COLLECTION AND EXPERIMENTAL SETUP,0.24266936299292213,"iPhone 14
Sep. 2022
Undisclosed
/
iPhone 15
Sep. 2023
Undisclosed
/
VIVO T2x
May. 2022
LSM6DSO
$0.33
OPPO Reno 6
May. 2021
ICM-40607
$0.28
Realme GT
Mar. 2021
BMI160
$0.21
Redmi K40
Mar. 2021
ICM-40607
$0.28"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.243680485338726,"We invited nine volunteers, each using their
229"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.24469160768452983,"smartphone’s built-in inertial sensors to record
230"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.24570273003033366,"handwriting movements. The nine smartphones
231"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.24671385237613752,"and their corresponding sensor models are listed
232"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.24772497472194135,"in Table 1. Each volunteer held their phone ac-
233"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2487360970677452,"cording to their personal habit and wrote 500
234"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.24974721941354905,"Chinese characters in the air (sourced from the
235"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.25075834175935285,"”Commonly Used Chinese Characters List” pub-
236"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2517694641051567,"lished by the National Language Working Com-
237"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2527805864509606,"mittee and the Ministry of Education), writing
238"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2537917087967644,"each character only once. In total, we obtained
239"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.25480283114256824,"4500 samples of Chinese handwriting signals.
240"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2558139534883721,"We randomly selected 1500 samples from three
241"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.2568250758341759,"volunteers as the training set, while the remaining 3000 samples from six volunteers were used as
242"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.25783619817997977,"the test set without participating in any training. All experiments are implemented by Pytorch 1.12.1
243"
DATA COLLECTION AND EXPERIMENTAL SETUP,0.25884732052578363,"with an Nvidia RTX 2080TI GPU and Intel(R) Xeon(R) W-2133 CPU.
244"
SIGNAL GENERATION VISUALIZATION,0.25985844287158744,"4.2
Signal Generation Visualization
245"
SIGNAL GENERATION VISUALIZATION,0.2608695652173913,"To visually demonstrate the signal generation effect of CI-GAN, we visualized the real and generated
246"
SIGNAL GENERATION VISUALIZATION,0.26188068756319516,"inertial sensor signals of the handwriting movements for the Chinese characters ”科” and ”学”,
247"
SIGNAL GENERATION VISUALIZATION,0.26289180990899896,"respectively. In these figures, the blue curves represent the three-axis acceleration signals, and the
248"
SIGNAL GENERATION VISUALIZATION,0.2639029322548028,"yellow curves represent the three-axis gyroscope signals. It can be observed that the generated signals
249"
SIGNAL GENERATION VISUALIZATION,0.2649140546006067,"closely follow the overall fluctuation trends of the real signals, indicating that CI-GAN effectively
250"
SIGNAL GENERATION VISUALIZATION,0.2659251769464105,preserves the handwriting movement information of the real signals. To further verify the consistency “科” “学”
SIGNAL GENERATION VISUALIZATION,0.26693629929221435,"Figure 3: The visualization results of the 6-axis signals recorded by the inertial sensor for different
Chinese character writing movements and the corresponding generated signals. The left side is the
original inertial sensor signal, the middle is the corresponding generated signal, and the right side is
the reconstructed writing trajectory.
251"
SIGNAL GENERATION VISUALIZATION,0.2679474216380182,"of the movement characteristics between the generated and real signals, we employed a classical
252"
SIGNAL GENERATION VISUALIZATION,0.268958543983822,"inertial navigation method [33] to convert both the real and generated signals into corresponding
253"
SIGNAL GENERATION VISUALIZATION,0.2699696663296259,"Real Signal
Generated Signal
Generated Signal"
SIGNAL GENERATION VISUALIZATION,0.27098078867542974,"Generated Signal
Generated Signal
Generated Signal"
SIGNAL GENERATION VISUALIZATION,0.27199191102123355,"Figure 4: Visualization of the real IMU signal for writing ”王” and the virtual signals generated by
CI-GAN. The upper left corner is the real signal, and the remaining signals are virtual signals."
SIGNAL GENERATION VISUALIZATION,0.2730030333670374,"motion trajectories, as shown in the third column of Fig. 3. It is important to note that the purpose
254"
SIGNAL GENERATION VISUALIZATION,0.27401415571284127,"of reconstructing the motion trajectories is not to precisely reproduce every detail of the writing
255"
SIGNAL GENERATION VISUALIZATION,0.2750252780586451,"process but to compare the overall shape similarity between the trajectories derived from real and
256"
SIGNAL GENERATION VISUALIZATION,0.27603640040444893,"generated signals. The highly similar shapes between the trajectories indicate that the generated
257"
SIGNAL GENERATION VISUALIZATION,0.2770475227502528,"signals accurately capture the structural information of different Chinese characters and can effectively
258"
SIGNAL GENERATION VISUALIZATION,0.2780586450960566,"simulate the key movement features of the handwriting process, including stroke order, movement
259"
SIGNAL GENERATION VISUALIZATION,0.27906976744186046,"direction changes, and velocity variations. Additionally, the obvious differences in details between
260"
SIGNAL GENERATION VISUALIZATION,0.2800808897876643,"the real and generated signals demonstrate CI-GAN’s capability to generate diverse signals. Since the
261"
SIGNAL GENERATION VISUALIZATION,0.2810920121334681,"generated signals maintain the core movement and semantic features of the handwriting process, these
262"
SIGNAL GENERATION VISUALIZATION,0.282103134479272,"differences do not impair the overall recognition of the characters but rather enhance the diversity of
263"
SIGNAL GENERATION VISUALIZATION,0.28311425682507585,"the training data.
264"
SIGNAL GENERATION VISUALIZATION,0.28412537917087965,"To demonstrate CI-GAN’s ability to generate unlimited high-quality signals, we generated five IMU
265"
SIGNAL GENERATION VISUALIZATION,0.2851365015166835,"handwriting signals for the same character ”王” and compared them with a real handwriting signal,
266"
SIGNAL GENERATION VISUALIZATION,0.2861476238624874,"as shown in Fig. 4. We chose this character because its strokes are distinctly separated, making it
267"
SIGNAL GENERATION VISUALIZATION,0.2871587462082912,"easier to compare the consistency of stroke features between the generated and real signals. It can
268"
SIGNAL GENERATION VISUALIZATION,0.28816986855409504,"be observed that the generated signals exhibit similar fluctuation patterns to the real signal in all
269"
SIGNAL GENERATION VISUALIZATION,0.2891809908998989,"three axes of acceleration and gyroscope measurements, verifying CI-GAN’s precision in capturing
270"
SIGNAL GENERATION VISUALIZATION,0.2901921132457027,"dynamic handwriting characteristics. Although the overall trends of the generated signals align with
271"
SIGNAL GENERATION VISUALIZATION,0.29120323559150657,"the real signal, the individual features show variations, demonstrating CI-GAN’s potential to produce
272"
SIGNAL GENERATION VISUALIZATION,0.29221435793731043,"large-scale, high-quality, and diverse IMU handwriting signal samples.
273"
COMPARATIVE EXPERIMENTS,0.29322548028311424,"4.3
Comparative Experiments
274"
COMPARATIVE EXPERIMENTS,0.2942366026289181,"Figure 5: The recognition accuracy of 6 classifiers
with varied training samples provided by CI-GAN."
COMPARATIVE EXPERIMENTS,0.29524772497472196,"Using the trained CI-GAN, we generated 30 vir-
275"
COMPARATIVE EXPERIMENTS,0.29625884732052576,"tual IMU handwriting signals for each character,
276"
COMPARATIVE EXPERIMENTS,0.2972699696663296,"resulting in a total of 16500 training samples.
277"
COMPARATIVE EXPERIMENTS,0.2982810920121335,"To evaluate the impact of the generated signals
278"
COMPARATIVE EXPERIMENTS,0.2992922143579373,"on handwriting recognition tasks, we trained six
279"
COMPARATIVE EXPERIMENTS,0.30030333670374115,"representative time-series classification models
280"
COMPARATIVE EXPERIMENTS,0.301314459049545,"with these training samples: 1DCNN, LSTM,
281"
COMPARATIVE EXPERIMENTS,0.3023255813953488,"Transformer, SVM, XGBoost, and Random For-
282"
COMPARATIVE EXPERIMENTS,0.3033367037411527,"est (RF). We then tested the performance of these
283"
COMPARATIVE EXPERIMENTS,0.30434782608695654,"classifiers on the test set, as shown in Fig. 5.
284"
COMPARATIVE EXPERIMENTS,0.30535894843276035,"When the number of training samples is small (1500 real samples), the recognition accuracy of all
285"
COMPARATIVE EXPERIMENTS,0.3063700707785642,"classifiers is poor, with the highest accuracy being only 6.7%. As the generated training samples are
286"
COMPARATIVE EXPERIMENTS,0.30738119312436807,"introduced, all classifiers’ recognition accuracy improves significantly, whereas deep learning ones
287"
COMPARATIVE EXPERIMENTS,0.3083923154701719,"such as 1DCNN, LSTM, and Transformer show the most notable improvement. When the number of
288"
COMPARATIVE EXPERIMENTS,0.30940343781597573,"training samples reaches 15000, the recognition accuracy of 1DCNN can reach 95.7%, improving from
289"
COMPARATIVE EXPERIMENTS,0.3104145601617796,"0.87% (without data augmentation). The Transformer captures long-range dependencies in time-series
290"
COMPARATIVE EXPERIMENTS,0.3114256825075834,"data through its self-attention mechanism, enabling it to understand complex movement patterns.
291"
COMPARATIVE EXPERIMENTS,0.31243680485338726,"However, its excellent recognition ability relies on large amounts of data, making its performance
292"
COMPARATIVE EXPERIMENTS,0.3134479271991911,"improvement the most significant as CI-GAN continuously generates training data, improving from
293"
COMPARATIVE EXPERIMENTS,0.31445904954499493,"1.7% to 98.4%. Compared to deep learning models, machine learning models also exhibit significant
294"
COMPARATIVE EXPERIMENTS,0.3154701718907988,"dependence on the amount of training data, highlighting the critical role of sufficient generated signals
295"
COMPARATIVE EXPERIMENTS,0.31648129423660265,"in handwriting recognition tasks. With the abundant training samples generated by CI-GAN, six
296"
COMPARATIVE EXPERIMENTS,0.31749241658240646,"classifiers achieve accurate recognition even for similar characters as shown in Appendix A.1.
297"
COMPARATIVE EXPERIMENTS,0.3185035389282103,Table 2: Performance comparison of 6 classfiers.
COMPARATIVE EXPERIMENTS,0.3195146612740142,"Classifier
1DCNN
LSTM
Transformer
RF
XGBoost
SVM"
COMPARATIVE EXPERIMENTS,0.320525783619818,"Runtime (s)
0.00743 0.13009
0.03439
0.01269
0.00154
0.00173
Memory (MB)
22.153
29.897
52.336
35.418
19.472
3.881
Accuracy
95.7%
93.9%
98.4%
83.5%
93.1%
74.6%"
COMPARATIVE EXPERIMENTS,0.32153690596562184,"In summary, CI-GAN provides a data
298"
COMPARATIVE EXPERIMENTS,0.3225480283114257,"experimental platform for Chinese
299"
COMPARATIVE EXPERIMENTS,0.3235591506572295,"writing recognition, enabling various
300"
COMPARATIVE EXPERIMENTS,0.32457027300303337,"classifiers to utilize the generated sam-
301"
COMPARATIVE EXPERIMENTS,0.32558139534883723,"ples for training and improving their
302"
COMPARATIVE EXPERIMENTS,0.32659251769464104,"recognition accuracy. To help researchers select suitable classifiers for different application scenarios,
303"
COMPARATIVE EXPERIMENTS,0.3276036400404449,"we further tested the recognition speed and memory usage of different classifiers for a single input
304"
COMPARATIVE EXPERIMENTS,0.32861476238624876,"sample and summarized their recognition accuracy in Table 2. Among the three deep learning models,
305"
COMPARATIVE EXPERIMENTS,0.32962588473205257,"1DCNN has the fastest runtime and the smallest memory usage, with a recognition accuracy of 95.7%,
306"
COMPARATIVE EXPERIMENTS,0.3306370070778564,"slightly lower than the Transformer but sufficient for most practical applications. It is more suitable
307"
COMPARATIVE EXPERIMENTS,0.3316481294236603,"for integration into memory and computation resource-limited smart wearable devices such as phones,
308"
COMPARATIVE EXPERIMENTS,0.3326592517694641,"watches, and wristbands. In contrast, Transformer has the highest accuracy among the six classifiers
309"
COMPARATIVE EXPERIMENTS,0.33367037411526795,"and the highest memory usage, making it more suitable for PC-based applications. Compared to deep
310"
COMPARATIVE EXPERIMENTS,0.3346814964610718,"learning classifiers, traditional machine learning classifiers generally have lower accuracy, but with
311"
COMPARATIVE EXPERIMENTS,0.3356926188068756,"the support of abundant training samples generated by CI-GAN, the XGBoost model still achieves a
312"
COMPARATIVE EXPERIMENTS,0.3367037411526795,"recognition accuracy of 93.1%, very close to deep learning classifiers. More importantly, XGBoost,
313"
COMPARATIVE EXPERIMENTS,0.33771486349848334,"as a tree model, has strong interpretability, allowing users to intuitively observe which features signifi-
314"
COMPARATIVE EXPERIMENTS,0.33872598584428715,"cantly impact the model’s decision-making process, which is a strength that deep learning models lack.
315"
COMPARATIVE EXPERIMENTS,0.339737108190091,"Additionally, XGBoost’s runtime and memory usage are better than the three deep learning classifiers,
316"
COMPARATIVE EXPERIMENTS,0.34074823053589487,"making it outstanding in scenarios requiring a balance between model performance, interpretability,
317"
COMPARATIVE EXPERIMENTS,0.3417593528816987,"and resource efficiency. For example, XGBoost can be integrated into stationery and educational tools
318"
COMPARATIVE EXPERIMENTS,0.34277047522750254,"to analyze students’ handwriting habits and provide personalized feedback suggestions. Similarly,
319"
COMPARATIVE EXPERIMENTS,0.3437815975733064,"in the healthcare field, XGBoost can be used to analyze patients’ writing characteristics, assisting
320"
COMPARATIVE EXPERIMENTS,0.3447927199191102,"doctors in evaluating treatment effects or predicting disease risks. Its high interpretability can provide
321"
COMPARATIVE EXPERIMENTS,0.34580384226491406,"an auxiliary reference for medical decisions and treatment plans, increasing patients’ trust in the
322"
COMPARATIVE EXPERIMENTS,0.3468149646107179,"treatment.
323"
ABLATION STUDY,0.34782608695652173,"4.4
Ablation Study
324"
ABLATION STUDY,0.3488372093023256,"Table 3: Performance comparison of six classifiers trained
on samples generated by different ablation models."
ABLATION STUDY,0.34984833164812945,"Ablation model
1DCNN LSTM Transformer
RF
XGBoost SVM"
ABLATION STUDY,0.35085945399393326,"No augmentation
0.87%
2.6%
1.7%
4.9%
1.2%
6.7%
w/o all (Base GAN)
18.5%
14.8%
15.7%
12.4%
20.5%
8.4%"
ABLATION STUDY,0.3518705763397371,"w/ OT
26.4%
28.6%
27.3%
21.0%
30.9%
20.9%
w/ FOT
39.9%
38.0%
35.3%
31.9%
46.8%
27.3%
w/ CGE
54.6%
51.2%
47.9%
38.6%
57.5%
34.1%
w/ FOT+CGE
80.7%
80.5%
80.9%
57.2%
70.4%
59.5%"
ABLATION STUDY,0.3528816986855409,"w/ FOT+CGE+SRA
(CI-GAN)
95.7%
93.9%
98.4%
83.5%
93.1%
74.6%"
ABLATION STUDY,0.3538928210313448,"Systematic ablation experiments are
325"
ABLATION STUDY,0.35490394337714865,"conducted to evaluate the contribu-
326"
ABLATION STUDY,0.35591506572295245,"tions of the CGE, FOT, and SRA mod-
327"
ABLATION STUDY,0.3569261880687563,"ules in CI-GAN. We generated writing
328"
ABLATION STUDY,0.3579373104145602,"samples using the ablated models and
329"
ABLATION STUDY,0.358948432760364,"trained the six classifiers on these sam-
330"
ABLATION STUDY,0.35995955510616784,"ples. The results are summarized in
331"
ABLATION STUDY,0.3609706774519717,"Table 3. When no generated data is
332"
ABLATION STUDY,0.3619817997977755,"used (No augmentation), the recogni-
333"
ABLATION STUDY,0.36299292214357937,"tion accuracy of all classifiers is very
334"
ABLATION STUDY,0.3640040444893832,"poor. Employing the Base GAN to
335"
ABLATION STUDY,0.36501516683518703,"generate training samples brings slight improvement but still underperforms, underscoring the critical
336"
ABLATION STUDY,0.3660262891809909,"importance and necessity of data augmentation for accurate recognition. This also indicates that
337"
ABLATION STUDY,0.36703741152679475,"utilizing GAN to improve classifier performance is a challenging task. Introducing CGE, FOT, and
338"
ABLATION STUDY,0.36804853387259856,"SRA individually into the GAN significantly improves its performance, with the introduction of
339"
ABLATION STUDY,0.3690596562184024,"CGE bringing the most noticeable improvement. This demonstrates that incorporating Chinese glyph
340"
ABLATION STUDY,0.3700707785642063,"encoding into the generative model is crucial for accurately generating writing signals. When CGE,
341"
ABLATION STUDY,0.3710819009100101,"FOT, and SRA are simultaneously integrated into the GAN (i.e., CI-GAN), the performance of all six
342"
ABLATION STUDY,0.37209302325581395,"classifiers is improved to above 70%, with four classifiers achieving recognition accuracies exceeding
343"
ABLATION STUDY,0.3731041456016178,"90%. Notably, the Transformer classifier achieves an impressive accuracy of 98.4%. Furthermore,
344"
ABLATION STUDY,0.3741152679474216,"statistical significance analysis is performed to validate the reliability of these results, as shown in
345"
ABLATION STUDY,0.3751263902932255,"Appendix A.2.
346"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.37613751263902934,"4.5
Visualization Analysis of Chinese Glyph Encoding
347       

 

"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.37714863498483314,Figure 6: The t-SNE visualization of Chinese glyph encodings.
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.378159757330637,"To demonstrate the effectiveness
348"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.37917087967644086,"of the Chinese glyph encoding in
349"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.38018200202224467,"capturing the glyph features of
350"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.38119312436804853,"Chinese characters, we conducted
351"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.3822042467138524,"a visualization analysis using t-
352"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.3832153690596562,"SNE, which reduced the dimen-
353"
VISUALIZATION ANALYSIS OF CHINESE GLYPH ENCODING,0.38422649140546006,"sionality of the glyph encodings of
354"
CHINESE CHARACTERS AND VISU-,0.3852376137512639,"500 Chinese characters and visu-
355"
CHINESE CHARACTERS AND VISU-,0.3862487360970677,"alized the results in a 2D space,
356"
CHINESE CHARACTERS AND VISU-,0.3872598584428716,"as shown in Fig. 6, where each
357"
CHINESE CHARACTERS AND VISU-,0.38827098078867545,"point represents a Chinese charac-
358"
CHINESE CHARACTERS AND VISU-,0.38928210313447925,"ter. For the convenience of obser-
359"
CHINESE CHARACTERS AND VISU-,0.3902932254802831,"vation, we selected 6 local visual-
360"
CHINESE CHARACTERS AND VISU-,0.391304347826087,"ization regions from left to right
361"
CHINESE CHARACTERS AND VISU-,0.3923154701718908,"and zoomed in on them at the bot-
362"
CHINESE CHARACTERS AND VISU-,0.39332659251769464,"tom. It can be observed that charac-
363"
CHINESE CHARACTERS AND VISU-,0.3943377148634985,"ters with similar strokes and struc-
364"
CHINESE CHARACTERS AND VISU-,0.3953488372093023,"ture (e.g., ”办-为”, ”目-且”, ”人-
365"
CHINESE CHARACTERS AND VISU-,0.39635995955510617,"入-八”) are close to each other. Ad-
366"
CHINESE CHARACTERS AND VISU-,0.39737108190091003,"ditionally, the figure shows several
367"
CHINESE CHARACTERS AND VISU-,0.39838220424671383,"clusters where characters within
368"
CHINESE CHARACTERS AND VISU-,0.3993933265925177,"the same cluster share similar radi-
369"
CHINESE CHARACTERS AND VISU-,0.40040444893832156,"cals, structures, or strokes, indicat-
370"
CHINESE CHARACTERS AND VISU-,0.40141557128412536,"ing that CGE effectively captures
371"
CHINESE CHARACTERS AND VISU-,0.4024266936299292,"the similarities and differences in
372"
CHINESE CHARACTERS AND VISU-,0.4034378159757331,"the glyph features of Chinese char-
373"
CHINESE CHARACTERS AND VISU-,0.4044489383215369,"acters. By incorporating CGE into
374"
CHINESE CHARACTERS AND VISU-,0.40546006066734075,"the generative model, CI-GAN can produce writing signals that accurately reflect the structure and
375"
CHINESE CHARACTERS AND VISU-,0.4064711830131446,"stroke features of Chinese characters, ensuring the generated signals closely align with real writing
376"
CHINESE CHARACTERS AND VISU-,0.4074823053589484,"movements. This encoding is not only crucial for guiding GANs in generating writing signals but also
377"
CHINESE CHARACTERS AND VISU-,0.4084934277047523,"potentially provides new tools and perspectives for studying the evolution of Chinese hieroglyphs.
378"
CONCLUSION,0.40950455005055614,"5
Conclusion
379"
CONCLUSION,0.41051567239635994,"This paper introduces GAN to generate inertial sensor signals and proposes CI-GAN for Chinese
380"
CONCLUSION,0.4115267947421638,"writing data augmentation, which consists of CGE, FOT, and SRA. The CGE module constructs
381"
CONCLUSION,0.41253791708796766,"an encoding of the stroke and structure for Chinese characters, providing glyph information for
382"
CONCLUSION,0.41354903943377147,"GAN to generate writing signals. FOT overcomes the mode collapse and mode mixing problems
383"
CONCLUSION,0.41456016177957533,"of traditional GANs and ensures the authenticity of the generated samples through a forced feature
384"
CONCLUSION,0.4155712841253792,"matching mechanism. The SRA module aligns the semantic relationships between the generated
385"
CONCLUSION,0.416582406471183,"signals and the corresponding Chinese characters, thereby imposing a batch-level constraint on
386"
CONCLUSION,0.41759352881698686,"GAN. Utilizing the large-scale, high-quality synthetic IMU writing signals provided by CI-GAN, the
387"
CONCLUSION,0.4186046511627907,"recognition accuracy of six widely used classifiers for Chinese writing recognition was improved
388"
CONCLUSION,0.4196157735085945,"from 6.7% to 98.4%, which demonstrates that CI-GAN has the potential to become a flexible and
389"
CONCLUSION,0.4206268958543984,"efficient data generation platform in the field of Chinese writing recognition. This research provides
390"
CONCLUSION,0.42163801820020225,"a novel human-computer interaction, especially for disabled people. Its limitations and impact are
391"
CONCLUSION,0.42264914054600605,"discussed in Appendix B.1 and B.2. In the future, we plan to extend CI-GAN to generate signals from
392"
CONCLUSION,0.4236602628918099,"other modalities of sensors, constructing a multimodal human-computer interaction system tailored
393"
CONCLUSION,0.4246713852376138,"for disabled individuals, which can adapt to the diverse needs of users with different disabilities.
394"
CONCLUSION,0.4256825075834176,"Through continuous collaboration with healthcare professionals and the disabled community, we will
395"
CONCLUSION,0.42669362992922144,"refine and optimize these multimodal systems to ensure they deliver the highest functionality and
396"
CONCLUSION,0.4277047522750253,"user satisfaction. Ultimately, this research aims to foster a society where digital accessibility is a
397"
CONCLUSION,0.4287158746208291,"fundamental right, ensuring that all individuals, regardless of physical abilities, can engage fully and
398"
CONCLUSION,0.42972699696663297,"independently with the digital world.
399"
REFERENCES,0.43073811931243683,"References
400"
REFERENCES,0.43174924165824063,"[1] Jayraj V Vaghasiya, Carmen C Mayorga-Martinez, Jan Vyskočil, and Martin Pumera. Black
401"
REFERENCES,0.4327603640040445,"phosphorous-based human-machine communication interface. Nature communications, 14(1):2,
402"
REFERENCES,0.43377148634984836,"2023.
403"
REFERENCES,0.43478260869565216,"[2] Xumeng Wang, Xinhu Zheng, Wei Chen, and Fei-Yue Wang. Visual human–computer interac-
404"
REFERENCES,0.435793731041456,"tions for intelligent vehicles and intelligent transportation systems: The state of the art and future
405"
REFERENCES,0.4368048533872599,"directions. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 51(1):253–265,
406"
REFERENCES,0.4378159757330637,"2020.
407"
REFERENCES,0.43882709807886755,"[3] Swapnil Sayan Saha, Sandeep Singh Sandha, Luis Antonio Garcia, and Mani Srivastava. Tinyo-
408"
REFERENCES,0.4398382204246714,"dom: Hardware-aware efficient neural inertial navigation. Proceedings of the ACM on Interac-
409"
REFERENCES,0.4408493427704752,"tive, Mobile, Wearable and Ubiquitous Technologies, 6(2):1–32, 2022.
410"
REFERENCES,0.4418604651162791,"[4] Mahdi Abolfazli Esfahani, Han Wang, Keyu Wu, and Shenghai Yuan. Aboldeepio: A novel
411"
REFERENCES,0.44287158746208294,"deep inertial odometry network for autonomous vehicles. IEEE Transactions on Intelligent
412"
REFERENCES,0.44388270980788674,"Transportation Systems, 21(5):1941–1950, 2019.
413"
REFERENCES,0.4448938321536906,"[5] Xin Zhang, Bo He, Guangliang Li, Xiaokai Mu, Ying Zhou, and Tanji Mang. Navnet: Auv
414"
REFERENCES,0.44590495449949447,"navigation through deep sequential learning. IEEE Access, 8:59845–59861, 2020.
415"
REFERENCES,0.44691607684529827,"[6] Wenxin Liu, David Caruso, Eddy Ilg, Jing Dong, Anastasios I Mourikis, Kostas Daniilidis, Vijay
416"
REFERENCES,0.44792719919110213,"Kumar, and Jakob Engel. Tlio: Tight learned inertial odometry. IEEE Robotics and Automation
417"
REFERENCES,0.448938321536906,"Letters, 5(4):5653–5660, 2020.
418"
REFERENCES,0.4499494438827098,"[7] Daniel Weber, Clemens Gühmann, and Thomas Seel. Riann—a robust neural network outper-
419"
REFERENCES,0.45096056622851366,"forms attitude estimation filters. Ai, 2(3):444–463, 2021.
420"
REFERENCES,0.45197168857431747,"[8] Boris Gromov, Gabriele Abbate, Luca M. Gambardella, and Alessandro Giusti. Proximity
421"
REFERENCES,0.4529828109201213,"human-robot interaction using pointing gestures and a wrist-mounted imu. In 2019 International
422"
REFERENCES,0.4539939332659252,"Conference on Robotics and Automation (ICRA), pages 8084–8091, 2019. doi: 10.1109/ICRA.
423"
REFERENCES,0.455005055611729,"2019.8794399.
424"
REFERENCES,0.45601617795753285,"[9] Peng Li, Wen-An Zhang, Yuqiang Jin, Zihan Hu, and Linqing Wang. Attitude estimation
425"
REFERENCES,0.4570273003033367,"using iterative indirect kalman with neural network for inertial sensors. IEEE Transactions on
426"
REFERENCES,0.4580384226491405,"Instrumentation and Measurement, 2023.
427"
REFERENCES,0.4590495449949444,"[10] Sachini Herath, Hang Yan, and Yasutaka Furukawa. Ronin: Robust neural inertial navigation in
428"
REFERENCES,0.46006066734074824,"the wild: Benchmark, evaluations, & new methods. In 2020 IEEE International Conference on
429"
REFERENCES,0.46107178968655205,"Robotics and Automation (ICRA), pages 3146–3152. IEEE, 2020.
430"
REFERENCES,0.4620829120323559,"[11] Shiqiang Liu, Junchang Zhang, Yuzhong Zhang, and Rong Zhu. A wearable motion capture
431"
REFERENCES,0.46309403437815977,"device able to detect dynamic motion of human limbs. Nature communications, 11(1):5615,
432"
REFERENCES,0.4641051567239636,"2020.
433"
REFERENCES,0.46511627906976744,"[12] You Li, Ruizhi Chen, Xiaoji Niu, Yuan Zhuang, Zhouzheng Gao, Xin Hu, and Naser El-Sheimy.
434"
REFERENCES,0.4661274014155713,"Inertial sensing meets machine learning: Opportunity or challenge? IEEE Transactions on
435"
REFERENCES,0.4671385237613751,"Intelligent Transportation Systems, 23(8):9995–10011, 2022. doi: 10.1109/TITS.2021.3097385.
436"
REFERENCES,0.46814964610717896,"[13] Derek K Shaeffer. Mems inertial sensors: A tutorial overview. IEEE Communications Magazine,
437"
REFERENCES,0.4691607684529828,"51(4):100–109, 2013.
438"
REFERENCES,0.47017189079878663,"[14] Changhao Chen, Xiaoxuan Lu, Andrew Markham, and Niki Trigoni. Ionet: Learning to cure
439"
REFERENCES,0.4711830131445905,"the curse of drift in inertial odometry. In Proceedings of the AAAI Conference on Artificial
440"
REFERENCES,0.47219413549039435,"Intelligence, volume 32, 2018.
441"
REFERENCES,0.47320525783619816,"[15] Martin Brossard,Axel Barrau, and Silvère Bonnabel. Ai-imu dead-reckoning. IEEE Transactions
442"
REFERENCES,0.474216380182002,"on Intelligent Vehicles, 5(4):585–595, 2020.
443"
REFERENCES,0.4752275025278059,"[16] Yifeng Wang and Yi Zhao. Handwriting recognition under natural writing habits based on a
444"
REFERENCES,0.4762386248736097,"low-cost inertial sensor. IEEE Sensors Journal, 24(1):995–1005, 2024. doi: 10.1109/JSEN.
445"
REFERENCES,0.47724974721941354,"2023.3331011.
446"
REFERENCES,0.4782608695652174,"[17] Haiqing Ren, Weiqiang Wang, and Chenglin Liu. Recognizing online handwritten chinese
447"
REFERENCES,0.4792719919110212,"characters using rnns with new computing architectures. Pattern Recognition, 93:179–192,
448"
REFERENCES,0.48028311425682507,"2019.
449"
REFERENCES,0.48129423660262893,"[18] Elyoenai Guerra-Segura, Aysse Ortega-Pérez, and Carlos M Travieso. In-air signature verifica-
450"
REFERENCES,0.48230535894843274,"tion system using leap motion. Expert Systems with Applications, 165:113797, 2021.
451"
REFERENCES,0.4833164812942366,"[19] Irene Cortes-Perez, Noelia Zagalaz-Anula, Desiree Montoro-Cardenas, Rafael Lomas-Vega,
452"
REFERENCES,0.48432760364004046,"Esteban Obrero-Gaitan, and María Catalina Osuna-Pérez. Leap motion controller video game-
453"
REFERENCES,0.48533872598584427,"based therapy for upper extremity motor recovery in patients with central nervous system
454"
REFERENCES,0.4863498483316481,"diseases. a systematic review with meta-analysis. Sensors, 21(6):2065, 2021.
455"
REFERENCES,0.487360970677452,"[20] Salih Ertug Ovur, Hang Su, Wen Qi, Elena De Momi, and Giancarlo Ferrigno. Novel adaptive
456"
REFERENCES,0.4883720930232558,"sensor fusion methodology for hand pose estimation with multileap motion. IEEE Transactions
457"
REFERENCES,0.48938321536905965,"on Instrumentation and Measurement, 70:1–8, 2021.
458"
REFERENCES,0.4903943377148635,"[21] Ning Xiao, Panlong Yang, Yubo Yan, Hao Zhou, Xiang-Yang Li, and Haohua Du. Motion-fi++:
459"
REFERENCES,0.4914054600606673,"Recognizing and counting repetitive motions with wireless backscattering. IEEE Transactions
460"
REFERENCES,0.4924165824064712,"on Mobile Computing, 20(5):1862–1876, 2021. doi: 10.1109/TMC.2020.2971996.
461"
REFERENCES,0.49342770475227504,"[22] Xuanzhi Wang, Kai Niu, Jie Xiong, Bochong Qian, Zhiyun Yao, Tairong Lou, and Daqing
462"
REFERENCES,0.49443882709807885,"Zhang. Placement matters: Understanding the effects of device placement for wifi sensing.
463"
REFERENCES,0.4954499494438827,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(1):
464"
REFERENCES,0.49646107178968657,"1–25, 2022.
465"
REFERENCES,0.4974721941354904,"[23] Ruiyang Gao, Wenwei Li, Jinyi Liu, Shuyu Dai, Mi Zhang, Leye Wang, and Daqing Zhang.
466"
REFERENCES,0.49848331648129424,"Wicgesture: Meta-motion based continuous gesture recognition with wi-fi. IEEE Internet of
467"
REFERENCES,0.4994944388270981,"Things Journal, 2023.
468"
REFERENCES,0.5005055611729019,"[24] Sai Deepika Regani, Beibei Wang, and K. J. Ray Liu. Wifi-based device-free gesture recognition
469"
REFERENCES,0.5015166835187057,"through-the-wall. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech
470"
REFERENCES,0.5025278058645096,"and Signal Processing (ICASSP), pages 8017–8021, 2021. doi: 10.1109/ICASSP39728.2021.
471"
REFERENCES,0.5035389282103134,"9414894.
472"
REFERENCES,0.5045500505561172,"[25] Zhengxin Guo, Fu Xiao, Biyun Sheng, Huan Fei, and Shui Yu. Wireader: Adaptive air hand-
473"
REFERENCES,0.5055611729019212,"writing recognition based on commercial wifi signal. IEEE Internet of Things Journal, 7(10):
474"
REFERENCES,0.506572295247725,"10483–10494, 2020.
475"
REFERENCES,0.5075834175935288,"[26] Ruiyang Gao, Wenwei Li, Yaxiong Xie, Enze Yi, Leye Wang, Dan Wu, and Daqing Zhang.
476"
REFERENCES,0.5085945399393327,"Towards robust gesture recognition by characterizing the sensing quality of wifi signals. Pro-
477"
REFERENCES,0.5096056622851365,"ceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(1):1–26,
478"
REFERENCES,0.5106167846309403,"2022.
479"
REFERENCES,0.5116279069767442,"[27] Yu Gu, Jinhai Zhan, Yusheng Ji, Jie Li, Fuji Ren, and Shangbing Gao. Mosense: An rf-based
480"
REFERENCES,0.512639029322548,"motion detection system via off-the-shelf wifi devices. IEEE Internet of Things Journal, 4(6):
481"
REFERENCES,0.5136501516683518,"2326–2341, 2017.
482"
REFERENCES,0.5146612740141557,"[28] Luis Montesinos, Rossana Castaldo, and Leandro Pecchia. Wearable inertial sensors for fall
483"
REFERENCES,0.5156723963599595,"risk assessment and prediction in older adults: A systematic review and meta-analysis. IEEE
484"
REFERENCES,0.5166835187057633,"transactions on neural systems and rehabilitation engineering, 26(3):573–582, 2018.
485"
REFERENCES,0.5176946410515673,"[29] Changhao Chen, Peijun Zhao, Chris Xiaoxuan Lu, Wei Wang, Andrew Markham, and Niki
486"
REFERENCES,0.5187057633973711,"Trigoni. Deep-learning-based pedestrian inertial navigation: Methods, data set, and on-device
487"
REFERENCES,0.5197168857431749,"inference. IEEE Internet of Things Journal, 7(5):4431–4441, 2020.
488"
REFERENCES,0.5207280080889788,"[30] Swapnil Sayan Saha, Yayun Du, Sandeep Singh Sandha, Luis Antonio Garcia, Moham-
489"
REFERENCES,0.5217391304347826,"mad Khalid Jawed, and Mani Srivastava. Inertial navigation on extremely resource-constrained
490"
REFERENCES,0.5227502527805864,"platforms: Methods, opportunities and challenges. In 2023 IEEE/ION Position, Location and
491"
REFERENCES,0.5237613751263903,"Navigation Symposium (PLANS), pages 708–723. IEEE, 2023.
492"
REFERENCES,0.5247724974721941,"[31] Mahdi Abolfazli Esfahani, Han Wang, Keyu Wu, and Shenghai Yuan. Orinet: Robust 3-d
493"
REFERENCES,0.5257836198179979,"orientation estimation with a single particular imu. IEEE Robotics and Automation Letters, 5
494"
REFERENCES,0.5267947421638018,"(2):399–406, 2019.
495"
REFERENCES,0.5278058645096056,"[32] Jian Zhang, Hongliang Bi, Yanjiao Chen, Qian Zhang, Zhaoyuan Fu, Yunzhe Li, and Zeyu Li.
496"
REFERENCES,0.5288169868554095,"Smartso: Chinese character and stroke order recognition with smartwatch. IEEE Transactions
497"
REFERENCES,0.5298281092012134,"on Mobile Computing, 20(7):2490–2504, 2020.
498"
REFERENCES,0.5308392315470172,"[33] Mohinder S Grewal, Lawrence R Weill, and Angus P Andrews. Global positioning systems,
499"
REFERENCES,0.531850353892821,"inertial navigation, and integration. John Wiley & Sons, 2007.
500"
REFERENCES,0.5328614762386249,"Appendix / Supplemental Material
501"
REFERENCES,0.5338725985844287,"A
Additional Experimental Results
502"
REFERENCES,0.5348837209302325,"A.1
Performance of Classifiers on Similar Characters
503"
REFERENCES,0.5358948432760364,"Transformer
1DCNN
LSTM
RF
XGBoost
SVM"
REFERENCES,0.5369059656218402,"Figure 7: Confusion matrices of different classifiers for recognition results of Chinese characters with
similar glyphs."
REFERENCES,0.537917087967644,"With the abundant training samples generated by CI-GAN, the handwriting recognition performance
504"
REFERENCES,0.538928210313448,"of all six classifiers significantly improved. To further verify the recognition performance of different
505"
REFERENCES,0.5399393326592518,"classifiers on characters with similar strokes and glyphs, we selected four groups of characters with sim-
506"
REFERENCES,0.5409504550050556,"ilar handwriting movements from the test set (”八人入大天太”, ”办为方力万历”, ”过达这边近还”,
507"
REFERENCES,0.5419615773508595,"and ”认议计许话识”) and presented the recognition results of the six classifiers in confusion matrices,
508"
REFERENCES,0.5429726996966633,"as shown in Fig. 7. It can be observed that the values on the diagonal of all confusion matrices are
509"
REFERENCES,0.5439838220424671,"significantly higher than the non-diagonal values, indicating high recognition accuracy for these
510"
REFERENCES,0.544994944388271,"similar handwriting characters with the help of samples generated by CI-GAN. However, some
511"
REFERENCES,0.5460060667340748,"characters are still misrecognized. For instance, the characters ”八”, ”人”, and ”入” have extremely
512"
REFERENCES,0.5470171890798786,"similar structures and writing movements, posing challenges even when massive training samples are
513"
REFERENCES,0.5480283114256825,"provided. Moreover, continuous and non-standard writing can also cause recognition obstacles. For
514"
REFERENCES,0.5490394337714863,"instance, although the characters ”过” and ”达” have different strokes in static form, they are very
515"
REFERENCES,0.5500505561172901,"similar in dynamic handwriting. Despite these challenges, the synthetic IMU handwriting samples
516"
REFERENCES,0.5510616784630941,"generated by CI-GAN significantly enhance the classifiers’ability to recognize characters with similar
517"
REFERENCES,0.5520728008088979,"glyph structures and handwriting movements, highlighting the value and significance of the proposed
518"
REFERENCES,0.5530839231547017,"CI-GAN method. By providing diverse and high-quality training samples, CI-GAN improves hand-
519"
REFERENCES,0.5540950455005056,"writing recognition classifiers’ performance and generalization ability, making it a valuable tool for
520"
REFERENCES,0.5551061678463094,"advancing Chinese handwriting recognition technology.
521"
REFERENCES,0.5561172901921132,"A.2
Statistical Significance Analysis
522"
REFERENCES,0.5571284125379171,"The CI-GAN model demonstrates significant performance improvements across multiple classifiers,
523"
REFERENCES,0.5581395348837209,"as shown in Table 4. The Transformer classifier, for instance, achieves a mean accuracy of 98.4%,
524"
REFERENCES,0.5591506572295247,"compared to 15.7% with the traditional GAN and 1.7% without data augmentation. This highlights
525"
REFERENCES,0.5601617795753286,"CI-GAN’s ability to generate realistic and diverse training samples that enhance handwriting recogni-
526"
REFERENCES,0.5611729019211324,"tion. Moreover, CI-GAN consistently improves accuracy and stability for all classifiers tested. The
527"
REFERENCES,0.5621840242669363,"1DCNN’s accuracy increases to 95.7% from 18.5% with the traditional GAN and 0.87% without
528"
REFERENCES,0.5631951466127402,"augmentation. Similarly, other models, including LSTM, RandomForest, XGBoost, and SVM, show
529"
REFERENCES,0.564206268958544,"substantial gains, underscoring CI-GAN’s effectiveness across diverse machine-learning contexts.
530"
REFERENCES,0.5652173913043478,"In addition, the narrow 95% confidence intervals, such as [98.2822%, 98.5178%] for the Trans-
531"
REFERENCES,0.5662285136501517,"former, validate the statistical significance and reliability of these results. This confirms CI-GAN’s
532"
REFERENCES,0.5672396359959555,"potential to consistently enhance classifier performance. In conclusion, CI-GAN represents a major
533"
REFERENCES,0.5682507583417593,"advancement in Chinese handwriting recognition by generating high-quality, diverse inertial signals.
534"
REFERENCES,0.5692618806875632,"This significantly boosts the accuracy and reliability of various classifiers, demonstrating CI-GAN’s
535"
REFERENCES,0.570273003033367,transformative potential in the field.
REFERENCES,0.5712841253791708,Table 4: Performance of different classifiers with CI-GAN generated data
REFERENCES,0.5722952477249748,"Ablation
Classifier
Mean Accuracy
Standard Deviation
95% Confidence Interval"
REFERENCES,0.5733063700707786,"No data
augmentation"
DCNN,0.5743174924165824,"1DCNN
0.87%
0.11%
[0.8018%, 0.9382%]
LSTM
2.61%
0.20%
[2.4761%, 2.7239%]
Transformer
1.70%
0.13%
[1.6194%, 1.7806%]
RandomForest
4.89%
0.09%
[4.8439%, 4.9556%]
XGBoost
1.20%
0.15%
[1.1071%, 1.2929%]
SVM
6.65%
0.10%
[6.5881%, 6.7119%]"
DCNN,0.5753286147623863,"Traditional
GAN"
DCNN,0.5763397371081901,"1DCNN
18.5%
0.16%
[18.4008%, 18.5992%]
LSTM
14.8%
0.37%
[14.5707%, 15.0293%]
Transformer
15.7%
0.15%
[15.6071%, 15.7929%]
RandomForest
12.4%
0.17%
[12.2948%, 12.5052%]
XGBoost
20.5%
0.23%
[20.3573%, 20.6427%]
SVM
8.40%
0.34%
[8.1893%, 8.6107%]"
DCNN,0.5773508594539939,CI-GAN
DCNN,0.5783619817997978,"1DCNN
95.7%
0.24%
[95.5513%, 95.8487%]
LSTM
93.9%
0.53%
[93.5713%, 94.2287%]
Transformer
98.4%
0.19%
[98.2822%, 98.5178%]
RandomForest
83.5%
0.35%
[83.2831%, 83.7169%]
XGBoost
93.1%
0.46%
[92.8148%, 93.3852%]
SVM
74.6%
0.38%
[74.3644%, 74.8356%] 536"
DCNN,0.5793731041456016,"B
Discussion
537"
DCNN,0.5803842264914054,"B.1
Societal Impact
538"
DCNN,0.5813953488372093,"CI-GAN model significantly improves the accuracy of Chinese writing recognition and offers an
539"
DCNN,0.5824064711830131,"alternative means of human-computer interaction that can overcome the limitations of traditional
540"
DCNN,0.583417593528817,"keyboard-based methods, which are often inaccessible to those who are blind or lose their fingers. By
541"
DCNN,0.5844287158746209,"providing a more accessible and user-friendly way to interact with digital devices, inertial sensors
542"
DCNN,0.5854398382204247,"can facilitate effective communication, enhance the participation of disabled people in education and
543"
DCNN,0.5864509605662285,"employment, and promote greater independence. Moreover, by addressing the unique needs of this
544"
DCNN,0.5874620829120324,"population, such technological advancements reflect a commitment to inclusivity and social justice,
545"
DCNN,0.5884732052578362,"ensuring that everyone, regardless of their physical abilities, has the opportunity to fully participate
546"
DCNN,0.58948432760364,"in and contribute to society.
547"
DCNN,0.5904954499494439,"Furthermore, by releasing the world’s first Chinese handwriting recognition dataset based on inertial
548"
DCNN,0.5915065722952477,"sensors, this research provides valuable data resources for both academia and industry, facilitating
549"
DCNN,0.5925176946410515,"further studies and advancements. Additionally, the technology offers an intuitive and efficient
550"
DCNN,0.5935288169868554,"learning tool for Chinese language learners, aiding in preserving and disseminating Chinese cultural
551"
DCNN,0.5945399393326593,"heritage and strengthening the global influence of Chinese characters. In summary, the CI-GAN
552"
DCNN,0.5955510616784631,"technology achieves not only significant breakthroughs in algorithmic research but also demonstrates
553"
DCNN,0.596562184024267,"extensive practical potential and substantial societal value, thereby being adopted by educational
554"
DCNN,0.5975733063700708,"aid device manufacturers. This study provides a solid foundation for future academic research,
555"
DCNN,0.5985844287158746,"technological development, and industrial applications, driving technological progress and societal
556"
DCNN,0.5995955510616785,"development.
557"
DCNN,0.6006066734074823,"B.2
Limitation
558"
DCNN,0.6016177957532861,"While the CI-GAN model demonstrates significant advancements in Chinese handwriting generation
559"
DCNN,0.60262891809909,"and recognition, some practical limitations could impact its performance in real-world applications.
560"
DCNN,0.6036400404448938,"For instance, non-standard or cursive handwriting may pose challenges for accurate signal generation
561"
DCNN,0.6046511627906976,"and recognition. Additionally, environmental factors such as external movements or vibrations when
562"
DCNN,0.6056622851365016,"using handheld devices could affect the inertial sensor data quality, leading to variations in recognition
563"
DCNN,0.6066734074823054,"accuracy. Future work could focus on developing more robust algorithms that account for these real-
564"
DCNN,0.6076845298281092,"world variations and improving the model’s adaptability to diverse handwriting styles and conditions.
565"
DCNN,0.6086956521739131,"These enhancements would ensure that the CI-GAN technology remains effective across a broader
566"
DCNN,0.6097067745197169,"range of practical scenarios.
567"
DCNN,0.6107178968655207,"C
Theory Assumption and Proof
568"
DCNN,0.6117290192113246,"To generate large-scale and high-quality handwriting signals, we introduce optimal transport theory
569"
DCNN,0.6127401415571284,"into the generative adversarial network to alleviate mode collapse and mixing issues. We provide
570"
DCNN,0.6137512639029322,"a detailed explanation and present a rigorous mathematical proof to show the advantages of this
571"
DCNN,0.6147623862487361,"operation.
572"
DCNN,0.6157735085945399,"In traditional conditional GANs, the generator G and the discriminator D are trained by minimizing
573"
DCNN,0.6167846309403437,"the loss function Ltradition:
574"
DCNN,0.6177957532861477,"Ltradition = min
G max
D Ex∼pdata[log D(x)] + Ez∼pz[log(1 −D(G(z)))],"
DCNN,0.6188068756319515,"where pdata is the real data distribution, and pz is the distribution of the generator’s input noise. This
575"
DCNN,0.6198179979777553,"loss function essentially minimizes the Jensen-Shannon Divergence (JSD) between the real data
576"
DCNN,0.6208291203235592,"distribution pdata and the generated data distribution pg:
577"
DCNN,0.621840242669363,JSD(pdata∥pg) = 1
DCNN,0.6228513650151668,2KL(pdata∥M) + 1
DCNN,0.6238624873609707,"2KL(pg∥M),"
DCNN,0.6248736097067745,where M = 1
DCNN,0.6258847320525783,"2(pdata + pg) and KL denotes the Kullback-Leibler divergence. However, JSD has a
578"
DCNN,0.6268958543983822,"notable drawback: when the real and generated data distributions do not overlap, the JSD becomes
579"
DCNN,0.627906976744186,"zero, causing the gradients to vanish. This leads to mode collapse, where the generator produces a
580"
DCNN,0.6289180990899899,"limited variety of samples.
581"
DCNN,0.6299292214357938,"In optimal transport theory, the Wasserstein distance is utilized to measure the minimum cost of
582"
DCNN,0.6309403437815976,"transforming one probability distribution into another. Given two probability distributions µ and ν on
583"
DCNN,0.6319514661274014,"a metric space X, the Wasserstein distance W is:
584"
DCNN,0.6329625884732053,"W(µ, ν) =
inf
γ∈Π(µ,ν) E(x,y)∼γ[d(x, y)],"
DCNN,0.6339737108190091,"where Π(µ, ν) is the set of all joint distributions whose marginals are µ and ν, and d(x, y) is a distance
585"
DCNN,0.6349848331648129,"metric on X. Therefore, we introduce the Wasserstein distance in optimal transport theory as new
586"
DCNN,0.6359959555106168,"loss function LOT , whose objective is to minimize the Wasserstein distance between the generated
587"
DCNN,0.6370070778564206,"distribution pg and the real distribution pdata. The LOT is defined as:
588"
DCNN,0.6380182002022244,"LOT = min
G max
D∈D Ex∼pdata[D(x)] −Ez∼pz[D(G(z))]"
DCNN,0.6390293225480284,"where D is the set of 1-Lipschitz functions. This Lipschitz constraint can be enforced through weight
589"
DCNN,0.6400404448938322,"clipping or gradient penalty. In LOT , the discriminator D is constrained to be 1-Lipschitz:
590"
DCNN,0.641051567239636,|D(x1) −D(x2)| ≤|x1 −x2|.
DCNN,0.6420626895854399,"This constraint ensures that the discriminator provides meaningful gradients even when pg and pdata
591"
DCNN,0.6430738119312437,"do not overlap. Using the Kantorovich-Rubinstein duality, we can express the Wasserstein distance
592"
DCNN,0.6440849342770475,"as:
593"
DCNN,0.6450960566228514,"W(pdata, pg) =
sup
∥f∥L≤1
Ex∼pdata[f(x)] −Ex∼pg[f(x)]."
DCNN,0.6461071789686552,"Since f is Lipschitz continuous, it ensures that the gradients ∇f(x) are bounded and do not vanish.
594"
DCNN,0.647118301314459,"Hence, during the optimization process, the generator receives consistent and informative gradient
595"
DCNN,0.6481294236602629,"updates that guide it to produce more realistic and diverse samples. The gradient of the loss function
596"
DCNN,0.6491405460060667,"LOT with respect to the generator’s parameters θ is:
597"
DCNN,0.6501516683518705,∇θEz∼pz[D(Gθ(z))] = Ez∼pz[∇θD(Gθ(z))].
DCNN,0.6511627906976745,"This gradient does not vanish even if pg and pdata have disjoint supports, thanks to the 1-Lipschitz
598"
DCNN,0.6521739130434783,"property of D. As a result, the generator G can still receive valuable gradient information to adjust its
599"
DCNN,0.6531850353892821,"parameters and gradually make pg approximate pdata even if pg and pdata do not overlap, effectively
600"
DCNN,0.654196157735086,"addressing mode collapse and mode mixing issues. Overall, after introducing optimal transport theory,
601"
DCNN,0.6552072800808898,"we overcome the gradient vanishing problem inherent in traditional GANs, effectively mitigating
602"
DCNN,0.6562184024266936,"mode collapse and mode mixing. LOT maintains the existence and relevance of gradients during
603"
DCNN,0.6572295247724975,"training, enabling the generator to continuously improve and produce more diverse and realistic
604"
DCNN,0.6582406471183013,"handwriting samples.
605"
DCNN,0.6592517694641051,"NeurIPS Paper Checklist
606"
CLAIMS,0.660262891809909,"1. Claims
607"
CLAIMS,0.6612740141557129,"Question: Do the main claims made in the abstract and introduction accurately reflect the
608"
CLAIMS,0.6622851365015167,"paper’s contributions and scope?
609"
CLAIMS,0.6632962588473206,"Answer: [Yes]
610"
CLAIMS,0.6643073811931244,"Justification: We have specifically summarized the main contributions of this article in the
611"
CLAIMS,0.6653185035389282,"introduction.
612"
CLAIMS,0.6663296258847321,"Guidelines:
613"
CLAIMS,0.6673407482305359,"• The answer NA means that the abstract and introduction do not include the claims made
614"
CLAIMS,0.6683518705763397,"in the paper.
615"
CLAIMS,0.6693629929221436,"• The abstract and/or introduction should clearly state the claims made, including the
616"
CLAIMS,0.6703741152679474,"contributions made in the paper and important assumptions and limitations. A No or
617"
CLAIMS,0.6713852376137512,"NA answer to this question will not be perceived well by the reviewers.
618"
CLAIMS,0.6723963599595552,"• The claims made should match theoretical and experimental results, and reflect how
619"
CLAIMS,0.673407482305359,"much the results can be expected to generalize to other settings.
620"
CLAIMS,0.6744186046511628,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
621"
CLAIMS,0.6754297269969667,"are not attained by the paper.
622"
LIMITATIONS,0.6764408493427705,"2. Limitations
623"
LIMITATIONS,0.6774519716885743,"Question: Does the paper discuss the limitations of the work performed by the authors?
624"
LIMITATIONS,0.6784630940343782,"Answer: [Yes] As shown in Appendix B.2.
625"
LIMITATIONS,0.679474216380182,"Justification: Non-standard or cursive handwriting may pose challenges for accurate signal
626"
LIMITATIONS,0.6804853387259858,"generation and recognition.
627"
LIMITATIONS,0.6814964610717897,"Guidelines:
628"
LIMITATIONS,0.6825075834175935,"• The answer NA means that the paper has no limitation while the answer No means that
629"
LIMITATIONS,0.6835187057633973,"the paper has limitations, but those are not discussed in the paper.
630"
LIMITATIONS,0.6845298281092013,"• The authors are encouraged to create a separate ”Limitations” section in their paper.
631"
LIMITATIONS,0.6855409504550051,"• The paper should point out any strong assumptions and how robust the results are to
632"
LIMITATIONS,0.6865520728008089,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
633"
LIMITATIONS,0.6875631951466128,"model well-specification, asymptotic approximations only holding locally). The authors
634"
LIMITATIONS,0.6885743174924166,"should reflect on how these assumptions might be violated in practice and what the
635"
LIMITATIONS,0.6895854398382204,"implications would be.
636"
LIMITATIONS,0.6905965621840243,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
637"
LIMITATIONS,0.6916076845298281,"only tested on a few datasets or with a few runs. In general, empirical results often
638"
LIMITATIONS,0.6926188068756319,"depend on implicit assumptions, which should be articulated.
639"
LIMITATIONS,0.6936299292214358,"• The authors should reflect on the factors that influence the performance of the approach.
640"
LIMITATIONS,0.6946410515672397,"For example, a facial recognition algorithm may perform poorly when image resolution
641"
LIMITATIONS,0.6956521739130435,"is low or images are taken in low lighting. Or a speech-to-text system might not be
642"
LIMITATIONS,0.6966632962588474,"used reliably to provide closed captions for online lectures because it fails to handle
643"
LIMITATIONS,0.6976744186046512,"technical jargon.
644"
LIMITATIONS,0.698685540950455,"• The authors should discuss the computational efficiency of the proposed algorithms
645"
LIMITATIONS,0.6996966632962589,"and how they scale with dataset size.
646"
LIMITATIONS,0.7007077856420627,"• If applicable, the authors should discuss possible limitations of their approach to address
647"
LIMITATIONS,0.7017189079878665,"problems of privacy and fairness.
648"
LIMITATIONS,0.7027300303336703,"• While the authors might fear that complete honesty about limitations might be used by
649"
LIMITATIONS,0.7037411526794742,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
650"
LIMITATIONS,0.704752275025278,"limitations that aren’t acknowledged in the paper. The authors should use their best
651"
LIMITATIONS,0.7057633973710818,"judgment and recognize that individual actions in favor of transparency play an impor-
652"
LIMITATIONS,0.7067745197168858,"tant role in developing norms that preserve the integrity of the community. Reviewers
653"
LIMITATIONS,0.7077856420626896,"will be specifically instructed to not penalize honesty concerning limitations.
654"
THEORY ASSUMPTIONS AND PROOFS,0.7087967644084934,"3. Theory Assumptions and Proofs
655"
THEORY ASSUMPTIONS AND PROOFS,0.7098078867542973,"Question: For each theoretical result, does the paper provide the full set of assumptions and
656"
THEORY ASSUMPTIONS AND PROOFS,0.7108190091001011,"a complete (and correct) proof?
657"
THEORY ASSUMPTIONS AND PROOFS,0.7118301314459049,"Answer: [Yes] As shown in Appendix C.
658"
THEORY ASSUMPTIONS AND PROOFS,0.7128412537917088,"Justification: We provide a detailed explanation and present rigorous mathematical proof of
659"
THEORY ASSUMPTIONS AND PROOFS,0.7138523761375126,"utilizing optimal transport to alleviate mode collapse and mixing issues of GAN.
660"
THEORY ASSUMPTIONS AND PROOFS,0.7148634984833164,"Guidelines:
661"
THEORY ASSUMPTIONS AND PROOFS,0.7158746208291203,"• The answer NA means that the paper does not include theoretical results.
662"
THEORY ASSUMPTIONS AND PROOFS,0.7168857431749242,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
663"
THEORY ASSUMPTIONS AND PROOFS,0.717896865520728,"referenced.
664"
THEORY ASSUMPTIONS AND PROOFS,0.7189079878665319,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
665"
THEORY ASSUMPTIONS AND PROOFS,0.7199191102123357,"• The proofs can either appear in the main paper or the supplemental material, but if
666"
THEORY ASSUMPTIONS AND PROOFS,0.7209302325581395,"they appear in the supplemental material, the authors are encouraged to provide a short
667"
THEORY ASSUMPTIONS AND PROOFS,0.7219413549039434,"proof sketch to provide intuition.
668"
THEORY ASSUMPTIONS AND PROOFS,0.7229524772497472,"• Inversely, any informal proof provided in the core of the paper should be complemented
669"
THEORY ASSUMPTIONS AND PROOFS,0.723963599595551,"by formal proofs provided in appendix or supplemental material.
670"
THEORY ASSUMPTIONS AND PROOFS,0.7249747219413549,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
671"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7259858442871587,"4. Experimental Result Reproducibility
672"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7269969666329625,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
673"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7280080889787665,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
674"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7290192113245703,"of the paper (regardless of whether the code and data are provided or not)?
675"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7300303336703741,"Answer: [Yes] We have made every effort to disclose all experimental details, including
676"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.731041456016178,"CPU model, GPU model, PyTorch framework version, built-in IMU specifications of 9
677"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7320525783619818,"experimental smartphones, and even the gender of volunteers.
678"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7330637007077856,"Justification: The paper provides comprehensive details on the data collection process,
679"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7340748230535895,"data splits, hyperparameters, training procedures, and statistical analyses, ensuring that all
680"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7350859453993933,"necessary information is disclosed to fully reproduce the main experimental results and
681"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7360970677451971,"validate the claims and conclusions.
682"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.737108190091001,"Guidelines:
683"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7381193124368048,"• The answer NA means that the paper does not include experiments.
684"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7391304347826086,"• If the paper includes experiments, a No answer to this question will not be perceived well
685"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7401415571284126,"by the reviewers: Making the paper reproducible is important, regardless of whether
686"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7411526794742164,"the code and data are provided or not.
687"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7421638018200202,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
688"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7431749241658241,"to make their results reproducible or verifiable.
689"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7441860465116279,"• Depending on the contribution, reproducibility can be accomplished in various ways.
690"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7451971688574317,"For example, if the contribution is a novel architecture, describing the architecture fully
691"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7462082912032356,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
692"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7472194135490394,"be necessary to either make it possible for others to replicate the model with the same
693"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7482305358948432,"dataset, or provide access to the model. In general. releasing code and data is often
694"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7492416582406471,"one good way to accomplish this, but reproducibility can also be provided via detailed
695"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.750252780586451,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
696"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7512639029322548,"of a large language model), releasing of a model checkpoint, or other means that are
697"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7522750252780587,"appropriate to the research performed.
698"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7532861476238625,"• While NeurIPS does not require releasing code, the conference does require all submis-
699"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7542972699696663,"sions to provide some reasonable avenue for reproducibility, which may depend on the
700"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7553083923154702,"nature of the contribution. For example
701"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.756319514661274,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
702"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7573306370070778,"to reproduce that algorithm.
703"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7583417593528817,"(b) If the contribution is primarily a new model architecture, the paper should describe
704"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7593528816986855,"the architecture clearly and fully.
705"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7603640040444893,"(c) If the contribution is a new model (e.g., a large language model), then there should
706"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7613751263902933,"either be a way to access this model for reproducing the results or a way to reproduce
707"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7623862487360971,"the model (e.g., with an open-source dataset or instructions for how to construct
708"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7633973710819009,"the dataset).
709"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7644084934277048,"(d) We recognize that reproducibility may be tricky in some cases, in which case authors
710"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7654196157735086,"are welcome to describe the particular way they provide for reproducibility. In the
711"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7664307381193124,"case of closed-source models, it may be that access to the model is limited in some
712"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7674418604651163,"way (e.g., to registered users), but it should be possible for other researchers to have
713"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.7684529828109201,"some path to reproducing or verifying the results.
714"
OPEN ACCESS TO DATA AND CODE,0.7694641051567239,"5. Open access to data and code
715"
OPEN ACCESS TO DATA AND CODE,0.7704752275025278,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
716"
OPEN ACCESS TO DATA AND CODE,0.7714863498483316,"tions to faithfully reproduce the main experimental results, as described in supplemental
717"
OPEN ACCESS TO DATA AND CODE,0.7724974721941354,"material?
718"
OPEN ACCESS TO DATA AND CODE,0.7735085945399394,"Answer: [Yes] We released the world’s first Chinese handwriting recognition dataset based
719"
OPEN ACCESS TO DATA AND CODE,0.7745197168857432,"on inertial sensors.
720"
OPEN ACCESS TO DATA AND CODE,0.775530839231547,"Justification: We submitted the training set (due to system limitations on attachment size) in
721"
OPEN ACCESS TO DATA AND CODE,0.7765419615773509,"the Supplementary Materials for review, and the full dataset can be found on GitHub. (We
722"
OPEN ACCESS TO DATA AND CODE,0.7775530839231547,"confirm to license the use of this data set to all AI researchers around the world.)
723"
OPEN ACCESS TO DATA AND CODE,0.7785642062689585,"Guidelines:
724"
OPEN ACCESS TO DATA AND CODE,0.7795753286147624,"• The answer NA means that paper does not include experiments requiring code.
725"
OPEN ACCESS TO DATA AND CODE,0.7805864509605662,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
726"
OPEN ACCESS TO DATA AND CODE,0.78159757330637,"public/guides/CodeSubmissionPolicy) for more details.
727"
OPEN ACCESS TO DATA AND CODE,0.782608695652174,"• While we encourage the release of code and data, we understand that this might not be
728"
OPEN ACCESS TO DATA AND CODE,0.7836198179979778,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
729"
OPEN ACCESS TO DATA AND CODE,0.7846309403437816,"including code, unless this is central to the contribution (e.g., for a new open-source
730"
OPEN ACCESS TO DATA AND CODE,0.7856420626895855,"benchmark).
731"
OPEN ACCESS TO DATA AND CODE,0.7866531850353893,"• The instructions should contain the exact command and environment needed to run to
732"
OPEN ACCESS TO DATA AND CODE,0.7876643073811931,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
733"
OPEN ACCESS TO DATA AND CODE,0.788675429726997,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
734"
OPEN ACCESS TO DATA AND CODE,0.7896865520728008,"• The authors should provide instructions on data access and preparation, including how
735"
OPEN ACCESS TO DATA AND CODE,0.7906976744186046,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
736"
OPEN ACCESS TO DATA AND CODE,0.7917087967644085,"• The authors should provide scripts to reproduce all experimental results for the new
737"
OPEN ACCESS TO DATA AND CODE,0.7927199191102123,"proposed method and baselines. If only a subset of experiments are reproducible, they
738"
OPEN ACCESS TO DATA AND CODE,0.7937310414560161,"should state which ones are omitted from the script and why.
739"
OPEN ACCESS TO DATA AND CODE,0.7947421638018201,"• At submission time, to preserve anonymity, the authors should release anonymized
740"
OPEN ACCESS TO DATA AND CODE,0.7957532861476239,"versions (if applicable).
741"
OPEN ACCESS TO DATA AND CODE,0.7967644084934277,"• Providing as much information as possible in supplemental material (appended to the
742"
OPEN ACCESS TO DATA AND CODE,0.7977755308392316,"paper) is recommended, but including URLs to data and code is permitted.
743"
OPEN ACCESS TO DATA AND CODE,0.7987866531850354,"6. Experimental Setting/Details
744"
OPEN ACCESS TO DATA AND CODE,0.7997977755308392,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
745"
OPEN ACCESS TO DATA AND CODE,0.8008088978766431,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
746"
OPEN ACCESS TO DATA AND CODE,0.8018200202224469,"results?
747"
OPEN ACCESS TO DATA AND CODE,0.8028311425682507,"Answer: [Yes]
748"
OPEN ACCESS TO DATA AND CODE,0.8038422649140546,"Justification: We made every effort to provide the necessary experimental details, including
749"
OPEN ACCESS TO DATA AND CODE,0.8048533872598584,"data collection and split, as well as the equipment specifications.
750"
OPEN ACCESS TO DATA AND CODE,0.8058645096056622,"Guidelines:
751"
OPEN ACCESS TO DATA AND CODE,0.8068756319514662,"• The answer NA means that the paper does not include experiments.
752"
OPEN ACCESS TO DATA AND CODE,0.80788675429727,"• The experimental setting should be presented in the core of the paper to a level of detail
753"
OPEN ACCESS TO DATA AND CODE,0.8088978766430738,"that is necessary to appreciate the results and make sense of them.
754"
OPEN ACCESS TO DATA AND CODE,0.8099089989888777,"• The full details can be provided either with the code, in appendix, or as supplemental
755"
OPEN ACCESS TO DATA AND CODE,0.8109201213346815,"material.
756"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8119312436804853,"7. Experiment Statistical Significance
757"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8129423660262892,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
758"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.813953488372093,"information about the statistical significance of the experiments?
759"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8149646107178968,"Answer: [Yes]
760"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8159757330637007,"Justification: The detailed experimental setup, including data collection, hyperparameters,
761"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8169868554095046,"and statistical significance analysis, provides a comprehensive understanding of the CI-GAN
762"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8179979777553084,"model’s robustness and efficacy. The thorough evaluation underscores the potential of CI-
763"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8190091001011123,"GAN to advance the field of Chinese handwriting recognition through high-quality, diverse
764"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8200202224469161,"signal generation.
765"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8210313447927199,"Guidelines:
766"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8220424671385238,"• The answer NA means that the paper does not include experiments.
767"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8230535894843276,"• The authors should answer ”Yes” if the results are accompanied by error bars, confidence
768"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8240647118301314,"intervals, or statistical significance tests, at least for the experiments that support the
769"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8250758341759353,"main claims of the paper.
770"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8260869565217391,"• The factors of variability that the error bars are capturing should be clearly stated (for
771"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8270980788675429,"example, train/test split, initialization, random drawing of some parameter, or overall
772"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8281092012133469,"run with given experimental conditions).
773"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8291203235591507,"• The method for calculating the error bars should be explained (closed form formula,
774"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8301314459049545,"call to a library function, bootstrap, etc.)
775"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8311425682507584,"• The assumptions made should be given (e.g., Normally distributed errors).
776"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8321536905965622,"• It should be clear whether the error bar is the standard deviation or the standard error
777"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.833164812942366,"of the mean.
778"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8341759352881699,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
779"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8351870576339737,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
780"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8361981799797775,"of Normality of errors is not verified.
781"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8372093023255814,"• For asymmetric distributions, the authors should be careful not to show in tables or
782"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8382204246713852,"figures symmetric error bars that would yield results that are out of range (e.g. negative
783"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.839231547017189,"error rates).
784"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.840242669362993,"• If error bars are reported in tables or plots, The authors should explain in the text how
785"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.8412537917087968,"they were calculated and reference the corresponding figures or tables in the text.
786"
EXPERIMENTS COMPUTE RESOURCES,0.8422649140546006,"8. Experiments Compute Resources
787"
EXPERIMENTS COMPUTE RESOURCES,0.8432760364004045,"Question: For each experiment, does the paper provide sufficient information on the computer
788"
EXPERIMENTS COMPUTE RESOURCES,0.8442871587462083,"resources (type of compute workers, memory, time of execution) needed to reproduce the
789"
EXPERIMENTS COMPUTE RESOURCES,0.8452982810920121,"experiments?
790"
EXPERIMENTS COMPUTE RESOURCES,0.846309403437816,"Answer: [Yes] As shown in Table 2.
791"
EXPERIMENTS COMPUTE RESOURCES,0.8473205257836198,"Justification: The paper specifies the type of compute resources used, including the Nvidia
792"
EXPERIMENTS COMPUTE RESOURCES,0.8483316481294236,"RTX 2080TI GPU and Intel Xeon W-2133 CPU, and provides details on memory usage and
793"
EXPERIMENTS COMPUTE RESOURCES,0.8493427704752275,"execution time, ensuring sufficient information is available to reproduce the experiments.
794"
EXPERIMENTS COMPUTE RESOURCES,0.8503538928210314,"Guidelines:
795"
EXPERIMENTS COMPUTE RESOURCES,0.8513650151668352,"• The answer NA means that the paper does not include experiments.
796"
EXPERIMENTS COMPUTE RESOURCES,0.8523761375126391,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
797"
EXPERIMENTS COMPUTE RESOURCES,0.8533872598584429,"or cloud provider, including relevant memory and storage.
798"
EXPERIMENTS COMPUTE RESOURCES,0.8543983822042467,"• The paper should provide the amount of compute required for each of the individual
799"
EXPERIMENTS COMPUTE RESOURCES,0.8554095045500506,"experimental runs as well as estimate the total compute.
800"
EXPERIMENTS COMPUTE RESOURCES,0.8564206268958544,"• The paper should disclose whether the full research project required more compute
801"
EXPERIMENTS COMPUTE RESOURCES,0.8574317492416582,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
802"
EXPERIMENTS COMPUTE RESOURCES,0.8584428715874621,"didn’t make it into the paper).
803"
CODE OF ETHICS,0.8594539939332659,"9. Code Of Ethics
804"
CODE OF ETHICS,0.8604651162790697,"Question: Does the research conducted in the paper conform, in every respect, with the
805"
CODE OF ETHICS,0.8614762386248737,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
806"
CODE OF ETHICS,0.8624873609706775,"Answer: [Yes]
807"
CODE OF ETHICS,0.8634984833164813,"Justification: The research adheres to the NeurIPS Code of Ethics in all respects, ensuring
808"
CODE OF ETHICS,0.8645096056622852,"ethical considerations are met in data collection, experimentation, and reporting of results.
809"
CODE OF ETHICS,0.865520728008089,"Guidelines:
810"
CODE OF ETHICS,0.8665318503538928,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
811"
CODE OF ETHICS,0.8675429726996967,"• If the authors answer No, they should explain the special circumstances that require a
812"
CODE OF ETHICS,0.8685540950455005,"deviation from the Code of Ethics.
813"
CODE OF ETHICS,0.8695652173913043,"• The authors should make sure to preserve anonymity (e.g., if there is a special consider-
814"
CODE OF ETHICS,0.8705763397371082,"ation due to laws or regulations in their jurisdiction).
815"
BROADER IMPACTS,0.871587462082912,"10. Broader Impacts
816"
BROADER IMPACTS,0.8725985844287159,"Question: Does the paper discuss both potential positive societal impacts and negative
817"
BROADER IMPACTS,0.8736097067745198,"societal impacts of the work performed?
818"
BROADER IMPACTS,0.8746208291203236,"Answer: [Yes] This research significantly advances Chinese handwriting recognition tech-
819"
BROADER IMPACTS,0.8756319514661274,"nology, providing valuable educational tools and promoting cultural preservation while
820"
BROADER IMPACTS,0.8766430738119313,"enhancing human-computer interaction across various applications, including smart devices
821"
BROADER IMPACTS,0.8776541961577351,"and virtual reality, thus has been adopted by the educational aid device manufacturer. As
822"
BROADER IMPACTS,0.8786653185035389,"shown in Appendix B.1 and Appendix B.2.
823"
BROADER IMPACTS,0.8796764408493428,"Justification: The paper thoroughly discusses the potential positive societal impacts, such
824"
BROADER IMPACTS,0.8806875631951466,"as advancements in Chinese handwriting recognition and educational benefits, as well as
825"
BROADER IMPACTS,0.8816986855409504,"possible negative impacts, including challenges with non-standard handwriting and external
826"
BROADER IMPACTS,0.8827098078867543,"environmental factors affecting performance.
827"
BROADER IMPACTS,0.8837209302325582,"Guidelines:
828"
BROADER IMPACTS,0.884732052578362,"• The answer NA means that there is no societal impact of the work performed.
829"
BROADER IMPACTS,0.8857431749241659,"• If the authors answer NA or No, they should explain why their work has no societal
830"
BROADER IMPACTS,0.8867542972699697,"impact or why the paper does not address societal impact.
831"
BROADER IMPACTS,0.8877654196157735,"• Examples of negative societal impacts include potential malicious or unintended uses
832"
BROADER IMPACTS,0.8887765419615774,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
833"
BROADER IMPACTS,0.8897876643073812,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
834"
BROADER IMPACTS,0.890798786653185,"groups), privacy considerations, and security considerations.
835"
BROADER IMPACTS,0.8918099089989889,"• The conference expects that many papers will be foundational research and not tied
836"
BROADER IMPACTS,0.8928210313447927,"to particular applications, let alone deployments. However, if there is a direct path to
837"
BROADER IMPACTS,0.8938321536905965,"any negative applications, the authors should point it out. For example, it is legitimate
838"
BROADER IMPACTS,0.8948432760364005,"to point out that an improvement in the quality of generative models could be used to
839"
BROADER IMPACTS,0.8958543983822043,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
840"
BROADER IMPACTS,0.8968655207280081,"that a generic algorithm for optimizing neural networks could enable people to train
841"
BROADER IMPACTS,0.897876643073812,"models that generate Deepfakes faster.
842"
BROADER IMPACTS,0.8988877654196158,"• The authors should consider possible harms that could arise when the technology is
843"
BROADER IMPACTS,0.8998988877654196,"being used as intended and functioning correctly, harms that could arise when the
844"
BROADER IMPACTS,0.9009100101112234,"technology is being used as intended but gives incorrect results, and harms following
845"
BROADER IMPACTS,0.9019211324570273,"from (intentional or unintentional) misuse of the technology.
846"
BROADER IMPACTS,0.9029322548028311,"• If there are negative societal impacts, the authors could also discuss possible mitigation
847"
BROADER IMPACTS,0.9039433771486349,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
848"
BROADER IMPACTS,0.9049544994944388,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
849"
BROADER IMPACTS,0.9059656218402427,"feedback over time, improving the efficiency and accessibility of ML).
850"
SAFEGUARDS,0.9069767441860465,"11. Safeguards
851"
SAFEGUARDS,0.9079878665318504,"Question: Does the paper describe safeguards that have been put in place for responsible
852"
SAFEGUARDS,0.9089989888776542,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
853"
SAFEGUARDS,0.910010111223458,"image generators, or scraped datasets)?
854"
SAFEGUARDS,0.9110212335692619,"Answer: [NA]
855"
SAFEGUARDS,0.9120323559150657,"Justification: the paper poses no such risks.
856"
SAFEGUARDS,0.9130434782608695,"Guidelines:
857"
SAFEGUARDS,0.9140546006066734,"• The answer NA means that the paper poses no such risks.
858"
SAFEGUARDS,0.9150657229524772,"• Released models that have a high risk for misuse or dual-use should be released with
859"
SAFEGUARDS,0.916076845298281,"necessary safeguards to allow for controlled use of the model, for example by requiring
860"
SAFEGUARDS,0.917087967644085,"that users adhere to usage guidelines or restrictions to access the model or implementing
861"
SAFEGUARDS,0.9180990899898888,"safety filters.
862"
SAFEGUARDS,0.9191102123356926,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
863"
SAFEGUARDS,0.9201213346814965,"should describe how they avoided releasing unsafe images.
864"
SAFEGUARDS,0.9211324570273003,"• We recognize that providing effective safeguards is challenging, and many papers do
865"
SAFEGUARDS,0.9221435793731041,"not require this, but we encourage authors to take this into account and make a best
866"
SAFEGUARDS,0.923154701718908,"faith effort.
867"
LICENSES FOR EXISTING ASSETS,0.9241658240647118,"12. Licenses for existing assets
868"
LICENSES FOR EXISTING ASSETS,0.9251769464105156,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
869"
LICENSES FOR EXISTING ASSETS,0.9261880687563195,"the paper, properly credited and are the license and terms of use explicitly mentioned and
870"
LICENSES FOR EXISTING ASSETS,0.9271991911021233,"properly respected?
871"
LICENSES FOR EXISTING ASSETS,0.9282103134479271,"Answer: [Yes] All the data in this experiment were collected independently by ourselves,
872"
LICENSES FOR EXISTING ASSETS,0.9292214357937311,"and we also released the data we collected to enhance our contribution to the research field.
873"
LICENSES FOR EXISTING ASSETS,0.9302325581395349,"Justification: All creators and original owners of assets used in the paper are properly
874"
LICENSES FOR EXISTING ASSETS,0.9312436804853387,"credited, and the license and terms of use are explicitly mentioned and respected, ensuring
875"
LICENSES FOR EXISTING ASSETS,0.9322548028311426,"full compliance with intellectual property rights.
876"
LICENSES FOR EXISTING ASSETS,0.9332659251769464,"Guidelines:
877"
LICENSES FOR EXISTING ASSETS,0.9342770475227502,"• The answer NA means that the paper does not use existing assets.
878"
LICENSES FOR EXISTING ASSETS,0.9352881698685541,"• The authors should cite the original paper that produced the code package or dataset.
879"
LICENSES FOR EXISTING ASSETS,0.9362992922143579,"• The authors should state which version of the asset is used and, if possible, include a
880"
LICENSES FOR EXISTING ASSETS,0.9373104145601617,"URL.
881"
LICENSES FOR EXISTING ASSETS,0.9383215369059656,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
882"
LICENSES FOR EXISTING ASSETS,0.9393326592517695,"• For scraped data from a particular source (e.g., website), the copyright and terms of
883"
LICENSES FOR EXISTING ASSETS,0.9403437815975733,"service of that source should be provided.
884"
LICENSES FOR EXISTING ASSETS,0.9413549039433772,"• If assets are released, the license, copyright information, and terms of use in the
885"
LICENSES FOR EXISTING ASSETS,0.942366026289181,"package should be provided. For popular datasets, paperswithcode.com/datasets
886"
LICENSES FOR EXISTING ASSETS,0.9433771486349848,"has curated licenses for some datasets. Their licensing guide can help determine the
887"
LICENSES FOR EXISTING ASSETS,0.9443882709807887,"license of a dataset.
888"
LICENSES FOR EXISTING ASSETS,0.9453993933265925,"• For existing datasets that are re-packaged, both the original license and the license of
889"
LICENSES FOR EXISTING ASSETS,0.9464105156723963,"the derived asset (if it has changed) should be provided.
890"
LICENSES FOR EXISTING ASSETS,0.9474216380182002,"• If this information is not available online, the authors are encouraged to reach out to
891"
LICENSES FOR EXISTING ASSETS,0.948432760364004,"the asset’s creators.
892"
NEW ASSETS,0.9494438827098078,"13. New Assets
893"
NEW ASSETS,0.9504550050556118,"Question: Are new assets introduced in the paper well documented and is the documentation
894"
NEW ASSETS,0.9514661274014156,"provided alongside the assets?
895"
NEW ASSETS,0.9524772497472194,"Answer: [Yes] We release the world’s first Chinese handwriting recognition dataset based
896"
NEW ASSETS,0.9534883720930233,"on inertial sensors, and we confirm to license the use of this data set to all AI researchers
897"
NEW ASSETS,0.9544994944388271,"around the world.
898"
NEW ASSETS,0.9555106167846309,"Justification: It can be found in GitHub.
899"
NEW ASSETS,0.9565217391304348,"Guidelines:
900"
NEW ASSETS,0.9575328614762386,"• The answer NA means that the paper does not release new assets.
901"
NEW ASSETS,0.9585439838220424,"• Researchers should communicate the details of the dataset/code/model as part of their
902"
NEW ASSETS,0.9595551061678463,"submissions via structured templates. This includes details about training, license,
903"
NEW ASSETS,0.9605662285136501,"limitations, etc.
904"
NEW ASSETS,0.961577350859454,"• The paper should discuss whether and how consent was obtained from people whose
905"
NEW ASSETS,0.9625884732052579,"asset is used.
906"
NEW ASSETS,0.9635995955510617,"• At submission time, remember to anonymize your assets (if applicable). You can either
907"
NEW ASSETS,0.9646107178968655,"create an anonymized URL or include an anonymized zip file.
908"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9656218402426694,"14. Crowdsourcing and Research with Human Subjects
909"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9666329625884732,"Question: For crowdsourcing experiments and research with human subjects, does the paper
910"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.967644084934277,"include the full text of instructions given to participants and screenshots, if applicable, as
911"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9686552072800809,"well as details about compensation (if any)?
912"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9696663296258847,"Answer: [NA]
913"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9706774519716885,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
914"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9716885743174924,"Guidelines:
915"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9726996966632963,"• The answer NA means that the paper does not involve crowdsourcing nor research with
916"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9737108190091001,"human subjects.
917"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.974721941354904,"• Including this information in the supplemental material is fine, but if the main contribu-
918"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9757330637007078,"tion of the paper involves human subjects, then as much detail as possible should be
919"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9767441860465116,"included in the main paper.
920"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9777553083923155,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
921"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9787664307381193,"or other labor should be paid at least the minimum wage in the country of the data
922"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9797775530839231,"collector.
923"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.980788675429727,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
924"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9817997977755308,"Subjects
925"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9828109201213346,"Question: Does the paper describe potential risks incurred by study participants, whether
926"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9838220424671386,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
927"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9848331648129424,"approvals (or an equivalent approval/review based on the requirements of your country or
928"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9858442871587462,"institution) were obtained?
929"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9868554095045501,"Answer: [NA]
930"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9878665318503539,"Justification: The paper does not involve crowdsourcing nor research with human subjects.
931"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9888776541961577,"Guidelines:
932"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9898887765419616,"• The answer NA means that the paper does not involve crowdsourcing nor research with
933"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9908998988877654,"human subjects.
934"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9919110212335692,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
935"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9929221435793731,"may be required for any human subjects research. If you obtained IRB approval, you
936"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.993933265925177,"should clearly state this in the paper.
937"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9949443882709808,"• We recognize that the procedures for this may vary significantly between institutions
938"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9959555106167847,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
939"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9969666329625885,"guidelines for their institution.
940"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9979777553083923,"• For initial submissions, do not include any information that would break anonymity (if
941"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.9989888776541962,"applicable), such as the institution conducting the review.
942"
