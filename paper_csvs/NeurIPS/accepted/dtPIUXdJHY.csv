Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0012239902080783353,"Label-speciﬁc representation learning (LSRL), i.e., constructing the representation
with speciﬁc discriminative properties for each class label, is an effective strategy
to improve the performance of multi-label learning. However, the generalization
analysis of LSRL is still in its infancy. The existing theory bounds for multi-
label learning, which preserve the coupling among different components, are
invalid for LSRL. In an attempt to overcome this challenge and make up for the
gap in the generalization theory of LSRL, we develop a novel vector-contraction
inequality and derive the generalization bound for general function class of LSRL
with a weaker dependency on the number of labels than the state of the art. In
addition, we derive generalization bounds for typical LSRL methods, and these
theoretical results reveal the impact of different label-speciﬁc representations on
generalization analysis. The mild bounds without strong assumptions explain the
good generalization ability of LSRL."
INTRODUCTION,0.0024479804161566705,"1
Introduction"
INTRODUCTION,0.0036719706242350062,"Multi-label learning has received continued attention in machine learning community due to its
widespread encounters in real-world applications, where each object is represented by a single
instance associated with multiple class labels [Zhang and Zhou, 2014, Liu et al., 2022]. The goal of
multi-label learning is to model real-world objects with multiple semantics. It has made important
advances in multimedia content annotation [Cabral et al., 2011, You et al., 2020], text categorization
[Rubin et al., 2012, Xun et al., 2020], bioinformatics [Cesa-Bianchi et al., 2012, Chen et al., 2017]
and other ﬁelds [Yu et al., 2005]. The failure to take into account that each class label may possess its
own discriminative properties results in the most straightforward strategy which exploits the identical
representation of an instance for dealing with multi-label data being perhaps suboptimal [Zhang and
Wu, 2015, Huang et al., 2016, Hang and Zhang, 2022]. In recent years, label-speciﬁc representation
learning (LSRL) [Zhang and Wu, 2015] has been proposed to facilitate the discrimination of each
class label by tailoring its own representations. Due to its ability to model distinct characteristics for
each class label, LSRL has become an effective strategy to improve the performance of multi-label
learning [Huang et al., 2015, 2016, 2018, Yu and Zhang, 2022, Hang and Zhang, 2022, Hang et al.,
2022]. Although LSRL has achieved impressive empirical advances in multi-label learning, the
problem of understanding LSRL theoretically remains completely under-explored."
INTRODUCTION,0.004895960832313341,"In recent years, efforts to explain why multi-label models generalize well is an important open
problem in multi-label learning community. The empirical success of LSRL makes the generalization"
INTRODUCTION,0.006119951040391677,∗Corresponding author
INTRODUCTION,0.0073439412484700125,"analysis of LSRL an important problem in multi-label learning. However, existing theoretical results
and analysis methods are not applicable to LSRL, which leads to a serious lack of progress in its
generalization analysis. A satisfactory and complete study of the generalization analysis for LSRL
should include two aspects: 1) the dependency of the generalization bounds on the number of
labels (i.e., c) should be weaker than square-root, and 2) the relationship among components should
be decoupled in the generalizability analysis. First, existing bounds with a linear or square-root
dependency on c are difﬁcult to explain empirical success of multi-label learning. For example, the
bounds with a linear dependency on c are vacuous (i.e., no longer less than 1) for commonly used
multi-label datasets such as CAL500, corel5k, rcv1-s1, Corel16k-s1, delicious, iaprtc12, espgame,
etc., since c is larger than √n (the number of examples) for these datasets. The bounds with a linear
or square-root dependency on c are vacuous for commonly used extreme multi-label datasets such as
Wiki 10, Amazon-670K, etc., since c is larger than n. The above failure of existing bounds for multi-
label learning also applies to LSRL. Hence, this suggests that only the bounds with weaker dependency
on c can provide effective theoretical guarantees. Second, existing theoretical results preserve the
coupling among different components [Lei et al., 2015, Wu and Zhu, 2020, Wu et al., 2021a,b].
However, LSRL decomposes the multi-label learning problem into multiple binary classiﬁcation
problems, which means that the relationship among different components needs to be decoupled in
the generalization analysis. Hence, we need to develop new analysis methods for LSRL. As a matter
of fact, theoretical research on LSRL can also promote a better understanding of multi-label learning."
INTRODUCTION,0.008567931456548347,"In this paper, we derive novel and tighter bounds based on the Rademacher complexity for LSRL.
Speciﬁcally, we develop a novel vector-contraction inequality with the assumption that the loss
function is Lipschitz continuous w.r.t. the ℓ∞norm, then we derive the bound for general function
classes of LSRL with no dependency on c, up to logarithmic terms, which is tighter than the state of
the art. In addition, we also analyze the bounds for several typical LSRL methods, and we show that
the construction method of label-speciﬁc representations will affect the generalization bound."
INTRODUCTION,0.009791921664626682,Our bounds for LSRL improve the dependency on c. Major contributions of the paper include:
INTRODUCTION,0.011015911872705019,"• We develop a novel vector-contraction inequality for ℓ∞norm Lipschitz continuous loss,
which overcomes the limitations of existing theoretical results and provides a theoretical
tool for the generalization analysis of LSRL.
• We derive bounds for general function classes of LSRL with a weaker dependency on c than
the state of the art, which provides a general theoretical guarantee for LSRL.
• We derive bounds for typical LSRL methods, which reveals the impact of different label-
speciﬁc representations on the generalization analysis. The theoretical techniques and results
on k-means clustering, Lasso, and DNNs involved here may be of independent interest."
RELATED WORK,0.012239902080783354,"2
Related Work"
MULTI-LABEL LEARNING,0.01346389228886169,"2.1
Multi-Label Learning"
MULTI-LABEL LEARNING,0.014687882496940025,"Multi-label learning is one of the most studied and important machine learning paradigms in practice
[Zhang and Zhou, 2014, Liu et al., 2022]. To cope with the challenge that the output space is
exponential in size to the number of class labels, modeling label correlations is adopted as a feasible
strategy to facilitate the learning process. Generally speaking, existing methods can be roughly
grouped into three major categories based on the order of label correlations being considered, namely
ﬁrst-order methods [Boutell et al., 2004, Zhang and Zhou, 2007, Zhang et al., 2018] which tackle
multi-label learning problem by decomposing it into a number of independent binary classiﬁcation
problems, second-order methods [Elisseeff and Weston, 2001, Zhu et al., 2018, Sun and Zhang,
2021] which tackle multi-label learning problem by considering pairwise relationships between
labels, and high-order methods [Ji et al., 2010, Xu and Guo, 2021] which tackle multi-label learning
problem by exploiting high-order relationships among labels. Recent years, beneﬁting from the good
generalization performance of deep learning, deep methods such as recurrent neural networks [Yazici
et al., 2020], graph neural networks[Chen et al., 2020], and embedding models [Bai et al., 2020,
Dahiya et al., 2021] have been explored to model label correlations."
MULTI-LABEL LEARNING,0.01591187270501836,"As a complement to the exploitation of label correlations, label-speciﬁc representation learning
(LSRL) have been proven to be another effective strategy to improve multi-label learning by ma-
nipulating the input space. Existing methods can be roughly grouped into three major categories"
MULTI-LABEL LEARNING,0.017135862913096694,"based on the construction method of label-speciﬁc representations, i.e., prototype-based label-speciﬁc
representation transformation methods [Zhang and Wu, 2015, Zhang et al., 2015, Weng et al., 2018,
Guo et al., 2019, Zhang and Li, 2021] which generate label-speciﬁc representations by treating the
prototypes of each class label as the transformation bases, label-speciﬁc feature selection methods
[Huang et al., 2015, 2016, 2018, Weng et al., 2020, Yu and Zhang, 2022] which generate label-speciﬁc
representations by retaining a feature subset as the most pertinent features for each class label, and
deep label-speciﬁc representation methods [Hang and Zhang, 2022, Hang et al., 2022] which learn
label-speciﬁc representations in an end-to-end manner by exploiting deep models. In particular,
k-means clustering-based LSRL, i.e., LIFT [Zhang and Wu, 2015], is the representative method of
prototype-based label-speciﬁc representation transformation methods, which constructs label-speciﬁc
representations by querying the distances between the original inputs and the cluster centers for each
class label. Lasso-based LSRL, i.e., LLSF [Huang et al., 2016], is the representative method of
label-speciﬁc feature selection methods, which presents a Lasso-based framework with the constraint
of pairwise label correlations for each class label. DNN-based LSRL, i.e., CLIF [Hang and Zhang,
2022], is the representative method of deep label-speciﬁc representation methods, which proposes to
learn label semantics and label-speciﬁc representations in a collaborative way. In this paper, we will
analyze the generalization bounds of these three representative LSRL methods."
GENERALIZATION BOUNDS FOR MULTI-LABEL LEARNING,0.01835985312117503,"2.2
Generalization Bounds for Multi-Label Learning"
GENERALIZATION BOUNDS FOR MULTI-LABEL LEARNING,0.019583843329253364,"Dembczynski et al. [2010] derived the relationship between the expectations of Hamming and Subset
loss based on the regret analysis on Hamming and Subset loss, and Dembczynski et al. [2012]
further performed regret analysis on Ranking loss, which provided preliminary theoretical insights
for understanding Hamming, Subset and Ranking loss. With the typical vector-contraction inequality
[Maurer, 2016] (i.e., assume that the loss function is Lipschitz continuous w.r.t. the ℓ2 norm), one
can obtain generalization bounds of order O(c/√n) for multi-label learning. Wu and Zhu [2020],
Wu et al. [2021a] showed that the order of the generalization bounds for Subset loss, Hamming Loss
and reweighted convex surrogate univariate loss can be improved to O(
p"
GENERALIZATION BOUNDS FOR MULTI-LABEL LEARNING,0.0208078335373317,"c/n), which preserved the
coupling among different components and exploited the relationship between loss functions. Liu et al.
[2018] also obtained a generalization bound of order O(
p"
GENERALIZATION BOUNDS FOR MULTI-LABEL LEARNING,0.022031823745410038,"c/n) for the dual set multi-label learning,
which was analyzed under the margin loss and kernel function classes."
GENERALIZATION BOUNDS FOR MULTI-LABEL LEARNING,0.023255813953488372,"Wu et al. [2021b] derived a generalization bound of order O(log(nc)/nσ) for norm regularized kernel
function classes with the assumptions that the loss function is Lipschitz continuous w.r.t. the ℓ∞
norm and the regularizer is σ-strongly convex with respect to some norms. Yu et al. [2014] obtained
a generalization bound of order O(1/√n) for trace norm regularized linear function classes with
Decomposable loss. Xu et al. [2016] used the local Rademacher complexity to derive a generalization
bound of order eO(1/n) for trace norm regularized linear function classes with the assumption that the
singular values of the weight matrix decay exponentially. These theoretical results all preserved the
coupling among different components. In addition, Wu et al. [2023] obtained O(1/√n) bounds for
Macro-Averaged AUC and gave thorough discussions about its relationships with the label-wise class
imbalance, which transformed the macro-averaged maximization problem in multi-label learning into
the problem of learning multiple tasks with graph-dependent examples. Here we obtain eO(1/√n)
bounds with the state-of-the-art dependency on the number of labels for LSRL function classes under
the assumption that the loss function is Lipschitz continuous w.r.t. the ℓ∞norm."
PRELIMINARIES,0.02447980416156671,"3
Preliminaries"
PRELIMINARIES,0.025703794369645042,"Let [n] := {1, . . . , n} for any natural number n. In the context of multi-label learning, given a dataset
D = {(x1, y1) , . . . , (xn, yn)} with n examples which are identically and independently distributed
(i.i.d.) from a probability distribution P on X × Y, where X ⊂Rd denotes the d-dimensional
input space and Y denotes the label space with c class labels, x ∈X, y ∈Y ⊆{−1, +1}c, i.e.,
each y = (y1, . . . , yc) is a binary vector and yj = 1 (yj = −1) denotes that the j-th label is
relevant (irrelevant), j ∈[c]. The task of multi-label learning is to learn a multi-label classiﬁer
h ∈H : X 7→{−1, +1}c which assigns each instance with a set of relevant labels. A common
strategy is to learn a vector-valued function f = (f1, . . . , fc) : X 7→Rc and derive the classiﬁer by
a thresholding function which divides the label space into relevant and irrelevant label sets."
PRELIMINARIES,0.02692778457772338,"For any vector-valued function f : X 7→Rc, the prediction quality on the example (x, y) is measured
by a loss function ℓ: Rc × {−1, +1}c 7→R+. The goal of learning is to ﬁnd a hypothesis f ∈F
with good generalization performance from the dataset D by optimizing the loss ℓ. The generalization
performance is measured by the expected risk: R(f) = E(x,y)∼P [ℓ(f(x), y)]. We denote the
empirical risk w.r.t. the training dataset D as bRD(f) = 1"
PRELIMINARIES,0.028151774785801713,"n
Pn
i=1 ℓ(f(xi), yi). We denote the optimal
risk as R∗= inff∈F R(f), the minimizer of the optimal risk as f ∗= arg minf∈F R(f) and denote"
PRELIMINARIES,0.02937576499388005,"the minimizer of the empirical risk as ˆ
f ∗= arg minf∈F bRD(f). In addition, we deﬁne the loss
function space as L = {ℓ(f(x), y) : f ∈F}, where F is the vector-valued function class."
PRELIMINARIES,0.030599755201958383,"3.1
Label-Speciﬁc Representation Learning"
PRELIMINARIES,0.03182374541003672,"Label-speciﬁc representation learning aims to construct the representation with speciﬁc discriminative
properties for each class label to facilitate its discrimination process. The basic idea of LSRL is to
decompose the multi-label learning problem into c binary classiﬁcation problems, i.e., decoupling the
relationship among different components, where each binary classiﬁcation problem corresponds to a
possible label in the label space. We consider the prediction function for each label of the general
form fj(x) = ⟨wj, ζj(φj(x))⟩, where the inner nonlinear mapping φj corresponds to the nonlinear
transformation induced by the construction method of label-speciﬁc representation, while the outer
nonlinear mapping ζj refers to the nonlinear mapping corresponding to the classiﬁer learned on the
generated label-speciﬁc representation. We deﬁne a vector-valued function class of LSRL as follows:"
PRELIMINARIES,0.033047735618115054,"F = {x 7→f(x) :f(x) = (f1(x), . . . , fc(x)),"
PRELIMINARIES,0.03427172582619339,"fj(x) = hj (φj(x)) = w⊤
j ζj(φj(x)), w = (w1, . . . , wc) ∈Rd×c,"
PRELIMINARIES,0.03549571603427173,"α(w) ≤Λ, β(ζj(·)) ≤A, γ(φj(·)) ≤C, x ∈X, j ∈[c], Λ, A, C > 0},
(1)"
PRELIMINARIES,0.03671970624235006,"where α represents a functional that constrains weights, β represents a functional that constrains
nonlinear mappings ζj, γ represents a functional that constrains nonlinear mappings φj."
RELATED EVALUATION METRICS,0.037943696450428395,"3.2
Related Evaluation Metrics"
RELATED EVALUATION METRICS,0.03916768665850673,"A number of evaluation metrics are proposed to measure the generalization performance of different
multi-label learning methods. Here we focus on commonly used evaluation metrics, i.e., Hamming
Loss, Subset Loss, Ranking Loss and Decomposable Loss. However, the above mentioned loss
functions are typically 0/1 losses, which are actually hard to handle in optimization. Hence, one
usually consider their surrogate losses, which are deﬁned as follows:"
RELATED EVALUATION METRICS,0.04039167686658507,"Hamming Loss :
ℓH(f(x), y) = 1 c c
X"
RELATED EVALUATION METRICS,0.0416156670746634,"j=1
ℓb (yjfj(x)) ,"
RELATED EVALUATION METRICS,0.042839657282741736,"where the base convex surrogate loss ℓb can be various popular forms, such as the hinge loss, the
logistic loss, the exponential loss and the squared loss."
RELATED EVALUATION METRICS,0.044063647490820076,"Subset Loss :
ℓS(f(x), y) = max
j∈[c] {ℓb (yjfj(x))} ."
RELATED EVALUATION METRICS,0.04528763769889841,"Ranking Loss :
ℓR(f(x), y) =
1
|Y +| |Y −| X p∈Y + X"
RELATED EVALUATION METRICS,0.046511627906976744,"q∈Y −
ℓb (fp(x) −fq(x)) ,"
RELATED EVALUATION METRICS,0.04773561811505508,"where Y + (Y −) denotes the relevant (irrelevant) label index set induced by y, and | · | denotes the
cardinality of a set."
RELATED EVALUATION METRICS,0.04895960832313342,"Decomposable Loss :
ℓD(f(x), y) = c
X"
RELATED EVALUATION METRICS,0.05018359853121175,"j=1
ℓb (yjfj(x)) ."
RELATED COMPLEXITY MEASURES,0.051407588739290085,"3.3
Related Complexity Measures"
RELATED COMPLEXITY MEASURES,0.05263157894736842,"Here we introduce the complexity measures involved in theoretical analysis. The Rademacher
complexity is used to perform generalization analysis for LSRL."
RELATED COMPLEXITY MEASURES,0.05385556915544676,"Deﬁnition 1 (Rademacher complexity). Let G be a class of real-valued functions mapping from X to
R. Let D = {x1, . . . , xn} be a set with n i.i.d. samples. The empirical Rademacher complexity over
G is deﬁned by ˆℜD(G) = Eϵ

supg∈G
1
n
Pn
i=1 ϵig (xi)

, where ϵ1, . . . , ϵn are i.i.d. Rademacher
random variables, and we refer to the expectation ℜ(G) = ED[ˆℜD(G)] as the Rademacher complexity
of G. In addition, we deﬁne the worst-case Rademacher complexity as ˜ℜn(G) = supD∈X n ˆℜD(G),
and its expectation is denoted as ˜ℜ(G) = ED[˜ℜn(G)]."
RELATED COMPLEXITY MEASURES,0.05507955936352509,"In multi-label learning, f ∈F is a vector-valued function, which makes traditional Rademacher
complexity analysis methods invalid. Hence, we need to convert the Rademacher complexity of a loss
function space associated with the vector-valued function class F into the Rademacher complexity
of a tractable scalar-valued function class. The Rademacher complexity can be bounded by other
scale-sensitive complexity measures, such as the covering number and fat-shattering dimension
[Srebro et al., 2010, Zhang and Zhang, 2023]. The relevant deﬁnitions are provided in the appendix."
GENERAL BOUNDS FOR LSRL,0.056303549571603426,"4
General Bounds for LSRL"
GENERAL BOUNDS FOR LSRL,0.05752753977968176,"In this section, we ﬁrst introduce the assumptions used. Then, we develop a novel vector-contraction
inequality for the Rademacher complexity of the loss function space associated with the vector-valued
function class F. Finally, with the novel vector-contraction inequality, we derive the generalization
bound for general function classes of LSRL with no dependency on the number of labels, up to
logarithmic terms, which is tighter than the state of the art. The detailed proofs of the theoretical
results in this paper are provided in the appendix.
Assumption 1. Assume that the input features, the loss function and the components of the vector-
valued function are bounded: ∥xi∥2 ≤R, ℓ(·, ·) ≤M, |fj(·)| ≤B for i ∈[n], j ∈[c] where R > 0,
M > 0 and B > 0 are constants.
Assumption 2. Assume that the loss function ℓis ρ-Lipschitz continuous w.r.t. the ℓ∞norm, that is:
ℓ(f(x), ·) −ℓ(f ′(x), ·)
 ≤ρ
f(x) −f ′(x)

∞,"
GENERAL BOUNDS FOR LSRL,0.0587515299877601,"where ρ > 0, ∥t∥∞= maxj∈[c] |tj| for t = (t1, . . . , tc)."
GENERAL BOUNDS FOR LSRL,0.05997552019583843,"Assumption 1 and 2 are mild assumptions. For Assumption 1, normalization of input features is a
common data preprocessing operation. When we consider the function class (1), we often use the
assumptions ∥wj∥2 ≤Λ, ∥ζj(·)∥2 ≤A for any j ∈[c] to replace the boundedness of the components
of the vector-valued function, i.e., B := ΛA. For Assumption 2, the Lipschitz continuity w.r.t. the
ℓ∞norm has been considered in some literature [Foster and Rakhlin, 2019, Lei et al., 2019, Wu et al.,
2021b]. The following Proposition 1 further illustrates that the commonly used loss functions in
multi-label learning actually satisfy Assumption 2.
Proposition 1. Assume that the base loss ℓb deﬁned in Subsection 3.2 is µ-Lipschitz continuous,
then the surrogate Hamming Loss is µ-Lipschitz w.r.t. the ℓ∞norm, the surrogate Subset Loss is
µ-Lipschitz w.r.t. the ℓ∞norm, the surrogate Ranking Loss is 2µ-Lipschitz w.r.t. the ℓ∞norm, and
the surrogate Decomposable Loss is cµ-Lipschitz w.r.t. the ℓ∞norm."
GENERAL BOUNDS FOR LSRL,0.06119951040391677,"We deﬁne a function class P consisting of projection operators pj : Rc 7→R for any j ∈[c]
which project the c-dimensional vector onto the j-th coordinate.
Then, we have P(F) =
{(j, x) 7→pj(f(x)) : pj(f(x)) = fj(x), f ∈F, (j, x) ∈[c] × X}. The projection function class
decouples the relationship among different components. With the assumption of ℓ∞norm Lipschitz
loss and the above deﬁnitions, we show that the Rademacher complexity of the loss function space
associated with F can be bounded by the worst-case Rademacher complexity of the projection
function class P(F). We develop the following novel vector-contraction inequality:
Lemma 1. Let F be a vector-valued function class of LSRL deﬁned by (1). Let Assumptions 1 and 2
hold. Given a dataset D of size n. Then, we have"
GENERAL BOUNDS FOR LSRL,0.06242350061199511,"ˆℜD(L) ≤12
√"
GENERAL BOUNDS FOR LSRL,0.06364749082007344,"2ρ√ceℜnc(P(F))

1 + log
1
2 (8e2n3c3) · log M√n ρB 
,"
GENERAL BOUNDS FOR LSRL,0.06487148102815178,"where ˆℜD(L) = Eϵ

supℓ∈L,f∈F
1
n
Pn
i=1 ϵiℓ(f(xi))

is the empirical Rademacher complexity of
the loss function space associated with F, and eℜnc(P(F)) is the worst-case Rademacher complexity
of the projection function class."
GENERAL BOUNDS FOR LSRL,0.06609547123623011,"Proof Sketch. First, the Rademacher complexity of the loss function space associated with F can
be bounded by the empirical ℓ∞norm covering number with the reﬁned Dudley’s entropy integral
inequality. Second, according to the Lipschitz continuity w.r.t the ℓ∞norm, the empirical ℓ∞norm
covering number of F can be bounded by that of P(F). Third, the empirical ℓ∞norm covering
number of P(F) can be bounded by the fat-shattering dimension, and the fat-shattering dimension can
be bounded by the worst-case Rademacher complexity of P(F). Hence, the problem is transferred
to the estimation of the worst-case Rademacher complexity. Finally, we estimate the lower bound
of the worst-case Rademacher complexity of P(F), and then combined with the above steps, the
Rademacher complexity of the loss function space associated with F can be bounded."
GENERAL BOUNDS FOR LSRL,0.06731946144430845,"With the vector-contraction inequality above, we can derive the following tight bound for LSRL:
Theorem 1. Let F be a vector-valued function class of LSRL deﬁned by (1). Let Assumptions 1 and
2 hold. Given a dataset D of size n. Then, for any 0 < δ < 1, with probability at least 1 −δ, the
following holds for any f ∈F:"
GENERAL BOUNDS FOR LSRL,0.06854345165238677,"R(f) ≤bRD(f) +
24
√"
GENERAL BOUNDS FOR LSRL,0.06976744186046512,"2ρΛA

1 + log
1
2 (8e2n3c3) · log M√n ρB
"
GENERAL BOUNDS FOR LSRL,0.07099143206854346,"√n
+ 3M s log 2"
GENERAL BOUNDS FOR LSRL,0.07221542227662178,"δ
2n ."
GENERAL BOUNDS FOR LSRL,0.07343941248470012,"Proof Sketch. We ﬁrst upper bound the worst-case Rademacher complexity ˜ℜnc(P(F)), and then
combined with Lemma 1, the desired bound can be derived."
GENERAL BOUNDS FOR LSRL,0.07466340269277846,"Remark 1. Although Lemma 1 shows a factor of √c, the term eℜnc(P(F)) ≤ΛA/√nc, which
makes the Rademacher complexity of the loss function space associated with F (i.e., ˆℜD(L))
actually independent on c, up to logarithmic terms, and results in a tighter bound than the existing
O(c/√n) and O(
p"
GENERAL BOUNDS FOR LSRL,0.07588739290085679,"c/n) bounds with a faster convergence rate eO(1/√n). The bound in Theorem
1 with no dependency on c can provide a general theoretical guarantee for LSRL, even for extreme
multi-label learning where the number of labels far exceeds the number of examples [Yu et al.,
2014, Prabhu and Varma, 2014, Yen et al., 2016, Liu and Shen, 2019], since it is easy to get that
log c is much smaller than √n. The main challenge of generalization analysis for LSRL is that
existing theoretical results and existing generalization analysis methods for multi-label learning are
not applicable to LSRL. Speciﬁcally, existing theoretical bounds often involve the typical vector-
contraction inequality [Maurer, 2016] for ℓ2 Lipschitz loss: Eϵ

supf∈F
1
n
Pn
i=1 ϵiℓ(f(xi))

≤
√"
GENERAL BOUNDS FOR LSRL,0.07711138310893513,"2µEϵ
h
supf∈F
1
n
Pn
i=1
Pc
j=1 ϵijfj (xi)
i
, which will lead to bounds with a linear dependency
on c for multi-label learning. Lei et al. [2015], Wu and Zhu [2020], Wu et al. [2021a] improve
the dependency of the bounds on c to square-root, which preserve the coupling among different
components reﬂected by the constraint ∥w∥≤Λ. Lei et al. [2019] also improves the bounds of
multi-class classiﬁcation to be independent on c (up to logarithmic terms) for ℓ∞Lipschitz loss by
preserving the coupling among different components, and Wu et al. [2021b] further generalizes these
results to multi-label learning. However, LSRL decomposes the multi-label learning problem into
c binary classiﬁcation problems, which means that the relationship among different components
needs to be decoupled in the generalization analysis. Hence, how to develop novel vector-contraction
inequalities that can induce eO(1/√n) bounds and deal with the case of decoupling the relationship
among different components are the two most critical difﬁculties in deriving tighter bounds for
LSRL. The introduction of the projection function class plays an important role in solving these
two difﬁculties. It improves the vector-contraction inequalities by a factor of √c and decouples the
relationship among different components (which is also reﬂected by the constraint ∥wj∥≤Λ for
any j ∈[c]). Our tighter eO(1/√n) bound in Theorem 1 with no dependency on c (up to logarithmic
terms) solve the limitations of existing theoretical results for multi-label learning and can provide a
general theoretical guarantee for LSRL."
GENERAL BOUNDS FOR LSRL,0.07833537331701346,"The differences in generalization bounds of different LSRL methods are mainly reﬂected in two
aspects. On the one hand, the Lipschitz constant of the loss functions, as we proved in Proposition 1,
the Lipschitz constants ρ corresponding to different loss functions are various. On the other hand, the
nonlinear mappings induced by different LSRL methods. In fact, when we analyze the generalization
for LSRL, we will further have ∥ζ(·)∥≤A := κR (κ is the Lipschitz constant of the nonlinear
mappings ζ(·)) to take into account the differences or characteristics of different LSRL methods. We
provide detailed analysis for A of typical LSRL methods in the next section."
GENERALIZATION BOUNDS FOR TYPICAL LSRL METHODS,0.0795593635250918,"5
Generalization Bounds for Typical LSRL Methods"
GENERALIZATION BOUNDS FOR TYPICAL LSRL METHODS,0.08078335373317014,"In this section, we analyze the generalization bounds for several typical LSRL methods, i.e., k-means
clustering-based [Zhang and Wu, 2015], Lasso-based [Huang et al., 2016] and DNN-based [Hang
and Zhang, 2022] LSRL methods. We show that different construction methods of label-speciﬁc
representation will lead to signiﬁcant differences in the constant A of the generalization bound in
Theorem 1. For each LSRL method, we ﬁrst give a brief introduction, then give its formal deﬁnition
corresponding to the class of LSRL deﬁned in (1), and ﬁnally derive the generalization bound."
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.08200734394124846,"5.1
Generalization Bounds for k-Means Clustering-Based LSRL Method"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.0832313341493268,"As a seminal work, k-means clustering-based LSRL method, i.e., LIFT [Zhang and Wu, 2015], uses
k-means clustering to construct label-speciﬁc representation that effectively capture the speciﬁc
characteristics of each label. Speciﬁcally, ﬁrst, for each label, k-means clustering is used to divide
the training instances into K clusters, and the centers of the K clusters are obtained, which is
denoted as cj
k for the j-th label, k ∈[K], j ∈[c]. Then, these K centers are used to construct
the label-speciﬁc representation, i.e., in the vector-valued function class of LSRL deﬁned by (1),
φj(x) =
h
d(x, cj
1), . . . , d(x, cj
K)
i
, d(x, cj
k) = ∥x −cj
k∥. Finally, a family of c classiﬁers fj with
κ-Lipschitz nonlinear mapping are induced based on the generated label-speciﬁc representations."
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.08445532435740515,"Next, we formally deﬁne the process of k-means clustering. Here we follow some of the settings and
deﬁnitions in [Li and Liu, 2021]. Assume that V : X 2 →R+is a pairwise distance-based function
used to measure the dissimilarity between pair observations, and Z = [Z1, . . . , ZK] is a collection
of K partition functions Zk : X 2 →R+for k ∈[K]. The clustering framework can be cast as the
problem of minimizing the following criterion:"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.08567931456548347,"bRD(V, Z) =
1
n(n −1) n
X"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.08690330477356181,"i,j=1,i̸=j K
X"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.08812729498164015,"k=1
V (xi, xj) Zk (xi, xj)"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.08935128518971848,"over all possible functions V and Zk for k ∈[K]. In k-means clustering, we have V (xi, xj) =
∥xi −xj∥2
2, and Zk (xi, xj) = I

(xi, xj) ∈C2
k
	
, ck =
1
|Ck|
P"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.09057527539779682,"x∈Ck x, where C1, . . . , CK are
the partitions of the feature space X. Let gk (xi, xj) = V (xi, xj) Zk (xi, xj) and g = (g1, . . . , gK)
be a vector-valued function, ℓclu(g(x, x′)) = PK
k=1 gk (x, x′), then bRD(V, Z) can be written as"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.09179926560587515,"bRD(V, Z) =
1
n(n −1) n
X"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.09302325581395349,"i,j=1,i̸=j
ℓclu(g(xi, xj)) =
1
n(n −1) n
X"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.09424724602203183,"i,j=1,i̸=j K
X"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.09547123623011015,"k=1
gk (xi, xj) ."
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.0966952264381885,We then deﬁne a vector-valued function class of k-means clustering as follows:
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.09791921664626684,"G = {(x, x′) 7→g(x, x′) :g(x, x′) = (g1(x, x′), . . . , gK(x, x′)),"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.09914320685434516,"gk(x, x′) = ∥x −x′∥2
2 · I

(x, x′) ∈C2
k
	
, x, x′ ∈X, k ∈[K]},"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.1003671970624235,"where |gk(·, ·)| ≤G for k ∈[K] and G > 0 are constants."
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.10159118727050184,"We denote the function class of k-means clustering corresponding to the j-th label as Gj. With the
above deﬁnitions, we can derive the tight bound for k-means clustering-based LSRL method:
Theorem 2. Let F be a vector-valued function class of k-means clustering-based LSRL deﬁned by
(1). Let Assumptions 1 and 2 hold. Given a dataset D of size n. Then, for any 0 < δ < 1, with
probability at least 1 −δ, the following holds for any f ∈F:"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.10281517747858017,R(f) ≤bRD(f) + 3M s log 2
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.10403916768665851,"δ
2n
+
48
√"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.10526315789473684,"2ρκΛ
√"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.10648714810281518,"KR

1 + log
1
2 (8e2n3c3) · log M√n µB
 √n +242√ 2ρ
√ KG
√n"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.10771113831089352,"
1 + log
1
2 (e2n3c3) · log M√nc G"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.10893512851897184," 
1 + log
1
2 (8e2n3c3) · log M√n µB 
."
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.11015911872705018,"Remark 2. There are three key points in the generalization analysis of k-means clustering-based
LSRL method. 1) Since k-means clustering-based LSRL method is two-stage, i.e., the centers of"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.11138310893512852,"clusters is generated by using k-means clustering in the ﬁrst stage, then these centers are exploited to
generate label-speciﬁc representations which are used to learn a multi-label classiﬁer in the second
stage, we cannot formally express these two stages in a closed-form expression through a composite
function (K centers are generated by the arg min function). Furthermore, K centers generated in
the ﬁrst stage are actually used as ﬁxed parameters rather than inputs in the second stage. Hence,
in order to fully consider the capacity of the model corresponding to the ﬁrst stage, it is reasonable
to deﬁne the whole function class as the sum of the function classes F + Lclu ◦G corresponding
to the methods of these two stages. Then, combined with Lemma 1, the generalization analysis is
transformed into the bounding of the complexity of the projection function class P(F +Lclu ◦G). The
introduction of class G induces an additional increase in complexity, i.e., the last term in Theorem 2.
2) The generalization analysis of k-means clustering-based LSRL method involves the generalization
analysis for k-means clustering. However, since the k-means clustering framework involves pairwise
functions, a sequence of pairs of i.i.d. individual observation in k-means clustering is no longer
independent, which makes standard techniques in the i.i.d case for traditional Rademacher complexity
inapplicable for k-means clustering. We convert the non-sum-of-i.i.d pairwise function to a sum-of-
i.i.d form by using permutations in U-process [Clémençon et al., 2008]. We show that the empirical
Rademacher complexity of a loss function space associated with the vector-valued function class G
can be bounded by ˆℜD′(Lclu ◦G) := Eϵ
h
supg∈G
1
⌊n"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.11260709914320685,"2 ⌋
P⌊n"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.11383108935128519,"2 ⌋
i=1 ϵiℓclu(g(xi, xi+⌊n"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.11505507955936352,"2 ⌋))
i
. 3) In order to
derive tight bounds for k-means clustering, we develop a novel vector-contraction inequality that can
induce bounds with a square-root dependency on the number of clusters. The theoretical techniques
and results involved here may be of independent interest. The generalization bound of k-means
clustering-based LSRL method is tighter than the state of the art with a faster convergence rate
eO(
p"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.11627906976744186,"K/n), which is independent on the number of labels. Since the lower bound for clustering is
Ω(
p"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.1175030599755202,"K/n) [Bartlett et al., 1998], our bound is (nearly) optimal, up to logarithmic terms, even from
the perspective of clustering. The constant A of the generalization bound in Theorem 1 corresponds
to 2κ
√"
GENERALIZATION BOUNDS FOR K-MEANS CLUSTERING-BASED LSRL METHOD,0.11872705018359853,KR here.
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.11995104039167687,"5.2
Generalization Bounds for Lasso-Based LSRL Method"
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.1211750305997552,"Lasso-based LSRL method, i.e., LLSF [Huang et al., 2016], assumes that the label-speciﬁc represen-
tation of each label should have sparsity and sharing properties. For sparsity, LLSF uses Lasso as
the model corresponding to each label. The property of sharing is achieved by considering that two
strongly correlated labels will share more features with each other than two uncorrelated or weakly
correlated labels and the corresponding weights will be similar, i.e., their inner product will be large."
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.12239902080783353,"Formally, since each label corresponds to a Lasso, this means that in the class of LSRL deﬁned
by (1), the base loss ℓb is the squared loss, the nonlinear mappings ζ(·) and φ(·) are both identity
transformations for any j ∈[c], and the constraint α(w) is ∥wj∥1 ≤Λ for any j ∈[c]. For the
j-th label, the property of sharing is reﬂected by the additionally introduced constraint Pc
i(1 −
sji)wj⊤wi ≤τ, where sji is the cosine similarity between labels yj and yi, here we refer to it as the
sharing constraint. The loss function used by Lasso-based LSRL method is the Decomposable loss."
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.12362301101591187,"Since the squared loss is not Lipschitz continuous, the theoretical results on the Lipschitz continuity
of the loss functions in Proposition 1 cannot be applied to Lasso-based LSRL method. To overcome
this challenge, we deﬁne the pseudo-Lipschitz function, which is also used in the theoretical analysis
of approximate message passing algorithms [Bayati and Montanari, 2011].
Deﬁnition 2. For k ≥1, we say that a function f : R →R is pseudo-Lipschitz of order k if there
exists a constant L > 0 such that the following inequality holds for all x, y ∈R:"
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.12484700122399021,"|f(x) −f(y)| ≤L
 
1 + |x|k−1 + |y|k−1
|x −y|."
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.12607099143206854,"Note that any pseudo-Lipschitz function of order 1 is Lipschitz continuous. The following Proposition
shows that the Decomposable loss is still Lipschitz continuous if the base loss is the squared loss.
Proposition 2. The squared loss is 1 pseudo-Lipschitz of order 2, the surrogate Decomposable Loss
is (3 + 2B)c-Lipschitz w.r.t. the ℓ∞norm if the base loss ℓb is the squared loss."
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.12729498164014688,"With the above deﬁnitions, we can derive the generalization bound for Lasso-based LSRL method:
Theorem 3. Let F be a vector-valued function class of Lasso-based LSRL deﬁned by (1). Let
Assumptions 1 and 2 hold. Given a dataset D of size n. Then, for any 0 < δ < 1, with probability at"
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.12851897184822522,"least 1 −δ, the following holds for any f ∈F:"
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.12974296205630356,"R(f) ≤bRD(f) +
24
√"
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.13096695226438188,"2(3 + 2B)cΛR

1 + log
1
2 (8e2n3c3) · log M√n ρB
"
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.13219094247246022,"√n
+ 3M s log 2"
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.13341493268053856,"δ
2n ."
GENERALIZATION BOUNDS FOR LASSO-BASED LSRL METHOD,0.1346389228886169,"Remark 3. The complexity of the LLSF function class can be bounded by the complexity of the
LSRL function class where each label corresponds to a Lasso, since the introduction of the sharing
constraint in the LLSF function class reduces the complexity of the function class compared with the
LSRL function class where each label corresponds to a Lasso. Hence, the complexity analysis of
the LLSF function class can be converted into upper bounding the Rademacher complexity of the
LSRL function class where each label corresponds to a Lasso. The constant A of the generalization
bound in Theorem 1 corresponds to R here, and the value of ρ is (3 + 2B)c, which induce the
eO(c/√n) bound here. If other loss functions are used, e.g., Hamming, Subset or Ranking loss,
instead of Decomposable loss, the dependency of the bounds for Lasso-based LSRL method on c can
be improved from linear to independent, up to logarithmic terms."
GENERALIZATION BOUNDS FOR DNN-BASED LSRL METHOD,0.13586291309669524,"5.3
Generalization Bounds for DNN-Based LSRL Method"
GENERALIZATION BOUNDS FOR DNN-BASED LSRL METHOD,0.13708690330477355,"DNN-based LSRL method, i.e., CLIF [Hang and Zhang, 2022], exploits the powerful representation
learning capability of deep neural networks (DNNs) to learn label-speciﬁc representation in an end-
to-end manner. Since the construction of label-speciﬁc representation involves graph convolutional
networks (GCNs), we ﬁrst introduce the relevant deﬁnitions for GCN."
GENERALIZATION BOUNDS FOR DNN-BASED LSRL METHOD,0.1383108935128519,"Let G = {V, E} be a given undirected graph, where V = {x1, x2, . . . , xn} is the set of nodes with
size |V| = n and E is the set of edges. Let A and D be the adjacency matrix and the diagonal degree
matrix respectively, where Dii = Pn
j=1 Aij. Let ˜A = (D + In)−1"
GENERALIZATION BOUNDS FOR DNN-BASED LSRL METHOD,0.13953488372093023,2 (A + In) (D + In)−1
DENOTE,0.14075887392900857,"2 denote
the normalized adjacency matrix with self-connections, where I is the identity matrix. The feature
propagation process of a two-layer GCN is σ( ˜Aσ( ˜AXW1)W2), where W1 and W2 are parameter
matrices, X is the node feature matrix, and the i-th row Xi∗is the node feature xi."
DENOTE,0.1419828641370869,"Speciﬁcally, for DNN-based LSRL method, ﬁrst, a graph (here we call it the label graph) is constructed
over the label space and a GCN is used to generate the label embeddings, i.e., the label embeddings
can be denoted by ψ(Y ) = σReLU( ˜AσReLU( ˜AY W1)W2), where Y is the node feature matrix of
the label graph with size c and the nodes are also bounded by R, σReLU is the ReLU activation.
Second, the label embedding of the j-th label is decoded into the importance vector by a one-layer
fully-connected neural network, i.e., σsig(W3ψ(Y )j), where σsig is the sigmoid activation, and the
input feature is mapped into the latent representation through a one-layer fully-connected neural
network, i.e., σReLU(W4x). Third, for the j-th label, the Hadamard product of the importance vector
and the latent representation is deﬁned as the pertinent representation, and then the label-speciﬁc
representation for the j-th label is obtained by feeding the pertinent representation into another
one-layer fully-connected neural network. Hence, the label-speciﬁc representation for the j-th label is"
DENOTE,0.14320685434516525,φj(x) = σReLU {W5 · [σReLU(W4x) ⊙σsig(W3ψ(Y )j)]} .
DENOTE,0.14443084455324356,"Finally, the j-th model is implemented by a fully-connected layer, i.e., fj(x) = σsig(w⊤
j φj(x))."
DENOTE,0.1456548347613219,"In the generalization analysis of the above deep neural network, we introduce the following assump-
tion, which is a common assumption in the generalization analysis for DNNs [Bartlett et al., 2017,
Golowich et al., 2018, Zhang and Zhang, 2023, Tang and Liu, 2023].
Assumption 3. Assume that the parameter metrices in DNN-based LSRL method are bounded, i.e.,
∥Wi∥≤D, i ∈[5], where D > 0 is a constant."
DENOTE,0.14687882496940025,"With the above deﬁnitions, we can derive the generalization bound for DNN-based LSRL method:
Theorem 4. Let F be a vector-valued function class of DNN-based LSRL deﬁned by (1). Let
Assumptions 1, 2, and 3 hold. Given a dataset D of size n. Then, for any 0 < δ < 1, with probability
at least 1 −δ, the following holds for any f ∈F:"
DENOTE,0.1481028151774786,"R(f) ≤bRD(f) +
6
√"
DENOTE,0.14932680538555693,"2ρΛD5R2(gmax + 1)

1 + log
1
2 (8e2n3c3) · log M√n ρB
"
DENOTE,0.15055079559363524,"√n
+ 3M s log 2"
DENOTE,0.15177478580171358,"δ
2n ,"
DENOTE,0.15299877600979192,where gmax is the maximum node degree of the label graph.
DENOTE,0.15422276621787026,"Remark 4. The term eℜnc(P(F)) ≤ΛD5R2(gmax + 1)/4√nc, which makes the Rademacher
complexity ˆℜD(L) actually independent on c, up to logarithmic terms, and results in a tight bound
for DNN-based LSRL method with a faster convergence rate eO(1/√n). The constant A of the
generalization bound in Theorem 1 corresponds to D5R2(gmax + 1)/4 here. For deep GCNs, the
increase in depth means that the generalization performance will deteriorate, which is consistent with
empirical performance and guides us not to design too many layers of GCN. In addition, the bound
is linearly dependent on the maximum node degree of the label graph, which suggests that when
the performance of the model is always unsatisfactory, we can check whether the maximum node
degree is large and consider using some techniques to remove some edges, e.g., DropEdge [Rong
et al., 2020], to alleviate the over-ﬁtting problem. We will further explore more network structures
to learn more effective label-speciﬁc representations, e.g., hypernetworks [Galanti and Wolf, 2020,
Chen et al., 2023, Shen et al., 2023], deep kernel networks [Zhang and Liao, 2020, Zhang and Zhang,
2023], and provide generalization analysis for the corresponding DNN-based LSRL methods."
DISCUSSION,0.1554467564259486,"6
Discussion"
DISCUSSION,0.15667074663402691,"Compared with existing methods considering label correlations, which mainly focus on the processing
of the label space by exploiting or modeling relationships between labels, LSRL mainly focuses on the
operation on the input space and implicitly considers label correlations in the process of constructing
label-speciﬁc representations. For example, in the construction of label-speciﬁc representations in
LLSF and CLIF, the label correlation information is embedded into the label-speciﬁc representations
in the input space by introducing the sharing constraint and using a GCN over the label graph to
generate label embeddings, respectively. LSRL methods are more effective since the label correlation
information is considered in the construction of label-speciﬁc representations."
DISCUSSION,0.15789473684210525,"Our theoretical results explain why LSRL is an effective strategy to improve the generalization perfor-
mance of multi-label learning. On the one hand, existing results can improve the dependency of the
bound on c from linear to square-root by preserving the coupling among different components, which
corresponds to high-order label correlations induced by norm regularizers. However, the improvement
in the preservation of coupling by a factor of √c beneﬁts from replacing √cΛ with Λ in the constraint
to some extent, and preserving the coupling corresponds to the stricter assumption [Zhang and
Zhang, 2024]. Our results for LSRL decouple the relationship among different components, and the
bounds with a weaker dependency on c are tighter than the existing results that preserve the coupling,
which also explains why LSRL methods outperform the multi-label methods that consider high-order
label correlations induced by norm regularizers. On the other hand, based on our results, we can
ﬁnd that LSRL methods substantially increase the data processing, i.e., the process of constructing
label-speciﬁc representations. From the perspective of model capacity, compared with traditional
multi-label methods, since the introduction of construction methods of label-speciﬁc representations,
the capacity of the model is signiﬁcantly increased, especially if deep learning methods are used
to generate label-speciﬁc representations, which improves the representation ability of the model.
Or more intuitively, LSRL means an increase in model capacity and stronger representation ability,
which makes it easier to ﬁnd the hypotheses with better generalization in the function class."
DISCUSSION,0.1591187270501836,"The vector-contraction inequality and the theoretical tools developed here are applicable to the
theoretical analysis of other problem settings, such as multi-class classiﬁcation, or more general vector-
valued learning problem. For multi-class classiﬁcation, multi-class margin-based loss, multinomial
logistic loss, Top-k hinge loss, etc. are all ℓ∞Lipschitz [Lei et al., 2019]. For multi-label learning,
the surrogate loss for Macro-Averaged AUC is also ℓ∞Lipschitz [Zhang and Zhang, 2024]."
CONCLUSION,0.16034271725826194,"7
Conclusion"
CONCLUSION,0.16156670746634028,"In this paper, we propose a novel vector-contraction inequality for ℓ∞norm Lipschitz continuous
loss, and derive bounds for general function classes of LSRL with a weaker dependency on c than the
state of the art. In addition, we analyze the bounds for several typical LSRL methods, and study the
impact of different label-speciﬁc representations on the generalization analysis."
CONCLUSION,0.16279069767441862,"In future work, we will extend our bounds to more LSRL methods, and derive tighter bounds for LSRL
with a faster convergence rate w.r.t. the number of examples, and further design efﬁcient models and
algorithms to construct label-speciﬁc representations with good generalization performance."
CONCLUSION,0.16401468788249693,Acknowledgements
CONCLUSION,0.16523867809057527,"The authors wish to thank the anonymous reviewers for their helpful comments and suggestions.
This work was supported by the National Science Foundation of China (62225602) and the Big Data
Computing Center of Southeast University."
REFERENCES,0.1664626682986536,References
REFERENCES,0.16768665850673195,"Junwen Bai, Shufeng Kong, and Carla P. Gomes. Disentangled variational autoencoder based multi-
label classiﬁcation with covariance-aware multivariate probit model. In Christian Bessiere, editor,
Proceedings of the 29th International Joint Conference on Artiﬁcial Intelligence, number IJCAI
2020, pages 4313–4321, 2020."
REFERENCES,0.1689106487148103,"Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3:463–482, 2002."
REFERENCES,0.1701346389228886,"Peter L. Bartlett, Tamás Linder, and Gábor Lugosi. The minimax distortion redundancy in empirical
quantizer design. IEEE Transactions on Information Theory, 44(5):1802–1813, 1998."
REFERENCES,0.17135862913096694,"Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. Advances in Neural Information Processing Systems, 30(NIPS 2017):6240–6249,
2017."
REFERENCES,0.17258261933904528,"Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with
applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764–785,
2011."
REFERENCES,0.17380660954712362,"Matthew R. Boutell, Jiebo Luo, Xipeng Shen, and Christopher M. Brown. Learning multi-label scene
classiﬁcation. Pattern Recognition, 37(9):1757–1771, 2004."
REFERENCES,0.17503059975520197,"Ricardo Silveira Cabral, Fernando De la Torre, João Paulo Costeira, and Alexandre Bernardino.
Matrix completion for multi-label image classiﬁcation. Advances in Neural Information Processing
Systems, 24(NIPS 2011):190–198, 2011."
REFERENCES,0.1762545899632803,"Nicolò Cesa-Bianchi, Matteo Re, and Giorgio Valentini. Synergy of multi-label hierarchical ensem-
bles, data fusion, and cost-sensitive methods for gene functional inference. Machine Learning, 88
(1-2):209–241, 2012."
REFERENCES,0.17747858017135862,"Di Chen, Yexiang Xue, Daniel Fink, Shuo Chen, and Carla P. Gomes. Deep multi-species embedding.
In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, number IJCAI
2017, pages 3639–3646. ijcai.org, 2017."
REFERENCES,0.17870257037943696,"Sirui Chen, Yuan Wang, Zijing Wen, Zhiyu Li, Changshuo Zhang, Xiao Zhang, Quan Lin, Cheng Zhu,
and Jun Xu. Controllable multi-objective re-ranking with policy hypernetworks. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, number KDD
2023, pages 3855–3864, 2023."
REFERENCES,0.1799265605875153,"Tianshui Chen, Liang Lin, Riquan Chen, Xiaolu Hui, and Hefeng Wu. Knowledge-guided multi-label
few-shot learning for general image recognition. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 44(3):1371–1384, 2020."
REFERENCES,0.18115055079559364,"Stéphan Clémençon, Gábor Lugosi, and Nicolas Vayatis. Ranking and empirical minimization of
u-statistics. The Annals of Statistics, 36(2):844–874, 2008."
REFERENCES,0.18237454100367198,"Kunal Dahiya, Ananye Agarwal, Deepak Saini, K Gururaj, Jian Jiao, Amit Singh, Sumeet Agarwal,
Purushottam Kar, and Manik Varma. Siamesexml: Siamese networks meet extreme classiﬁers with
100m labels. In Proceedings of the 38th International Conference on Machine Learning, pages
2330–2340, 2021."
REFERENCES,0.1835985312117503,"Krzysztof Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke Hüllermeier. Regret analysis
for performance metrics in multi-label classiﬁcation: The case of hamming and subset zero-one
loss. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
volume 6321, pages 280–295, 2010."
REFERENCES,0.18482252141982863,"Krzysztof Dembczynski, Wojciech Kotlowski, and Eyke Hüllermeier. Consistent multilabel ranking
through univariate losses. arXiv:1206.6401, 2012."
REFERENCES,0.18604651162790697,"André Elisseeff and Jason Weston. A kernel method for multi-labelled classiﬁcation. In Advances in
Neural Information Processing Systems, number NIPS 2001, pages 681–687, 2001."
REFERENCES,0.18727050183598531,"Dylan J. Foster and Alexander Rakhlin.
ℓ∞vector contraction for rademacher complexity.
arXiv:1911.06468v1, 2019."
REFERENCES,0.18849449204406366,"Tomer Galanti and Lior Wolf. On the modularity of hypernetworks. In Advances in Neural Information
Processing Systems, volume 33, pages 10409–10419, 2020."
REFERENCES,0.189718482252142,"Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. International Conference on Computational Learning Theory, 75(COLT 2018):
297–299, 2018."
REFERENCES,0.1909424724602203,"Yumeng Guo, Fulai Chung, Guozheng Li, Jiancong Wang, and James C. Gee. Leveraging label-
speciﬁc discriminant mapping features for multi-label learning. ACM Transactions on Knowledge
Discovery from Data, 13(2):24:1–24:23, 2019."
REFERENCES,0.19216646266829865,"Jun-Yi Hang and Min-Ling Zhang. Collaborative learning of label semantics and deep label-speciﬁc
features for multi-label classiﬁcation. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 44(12):9860–9871, 2022."
REFERENCES,0.193390452876377,"Jun-Yi Hang, Min-Ling Zhang, Yanghe Feng, and Xiaocheng Song. End-to-end probabilistic label-
speciﬁc feature learning for multi-label classiﬁcation. In Proceedings of the 36th AAAI Conference
on Artiﬁcial Intelligence, number AAAI 2022, pages 6847–6855, 2022."
REFERENCES,0.19461444308445533,"Jun Huang, Guorong Li, Qingming Huang, and Xindong Wu. Learning label speciﬁc features for
multi-label classiﬁcation. In Charu C. Aggarwal, Zhi-Hua Zhou, Alexander Tuzhilin, Hui Xiong,
and Xindong Wu, editors, Proceedings of the 15th IEEE International Conference on Data Mining,
number ICDM 2015, pages 181–190, 2015."
REFERENCES,0.19583843329253367,"Jun Huang, Guorong Li, Qingming Huang, and Xindong Wu. Learning label-speciﬁc features and
class-dependent labels for multi-label classiﬁcation. IEEE Transactions on Knowledge and Data
Engineering, 28(12):3309–3323, 2016."
REFERENCES,0.19706242350061198,"Jun Huang, Guorong Li, Qingming Huang, and Xindong Wu. Joint feature selection and classiﬁcation
for multilabel learning. IEEE Transactions on Cybernetics, 48(3):876–889, 2018."
REFERENCES,0.19828641370869032,"Shuiwang Ji, Lei Tang, Shipeng Yu, and Jieping Ye. A shared-subspace learning framework for
multi-label classiﬁcation. ACM Transactions on Knowledge Discovery from Data, 4(2):1–29, 2010."
REFERENCES,0.19951040391676866,"Antoine Ledent, Waleed Mustafa, Yunwen Lei, and Marius Kloft. Norm-based generalisation bounds
for deep multi-class convolutional neural networks. In Proceedings of the 35th AAAI Conference
on Artiﬁcial Intelligence, volume 35, pages 8279–8287, 2021."
REFERENCES,0.200734394124847,"Yunwen Lei, Ürün Dogan, Alexander Binder, and Marius Kloft. Multi-class svms: From tighter
data-dependent generalization bounds to novel algorithms. In Advances in Neural Information
Processing Systems, volume 28, pages 2035–2043, 2015."
REFERENCES,0.20195838433292534,"Yunwen Lei, Ürün Dogan, Ding-Xuan Zhou, and Marius Kloft. Data-dependent generalization
bounds for multi-class classiﬁcation. IEEE Transactions on Information Theory, 65(5):2995–3021,
2019."
REFERENCES,0.20318237454100369,"Shaojie Li and Yong Liu. Sharper generalization bounds for clustering. In Proceedings of the 38th
International Conference on Machine Learning, volume 139, pages 6392–6402, 2021."
REFERENCES,0.204406364749082,"Chong Liu, Peng Zhao, Sheng-Jun Huang, Yuan Jiang, and Zhi-Hua Zhou. Dual set multi-label
learning. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence, number AAAI
2018, pages 3635–3642, 2018."
REFERENCES,0.20563035495716034,"Weiwei Liu and Xiaobo Shen. Sparse extreme multi-label learning with oracle property. In Proceed-
ings of the 36th International Conference on Machine Learning, volume 97, pages 4032–4041,
2019."
REFERENCES,0.20685434516523868,"Weiwei Liu, Haobo Wang, Xiaobo Shen, and Ivor W. Tsang. The emerging trends of multi-label
learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7955–7974,
2022."
REFERENCES,0.20807833537331702,"Françoise Lust-Piquard and Gilles Pisier. Non commutative khintchine and paley inequalities. Arkiv
för matematik, 29:241–260, 1991."
REFERENCES,0.20930232558139536,"Andreas Maurer. A vector-contraction inequality for rademacher complexities. In Proceedings of the
27th International Conference on Algorithmic Learning Theory, volume 9925, pages 3–17, 2016."
REFERENCES,0.21052631578947367,"Yashoteja Prabhu and Manik Varma. Fastxml: a fast, accurate and stable tree-classiﬁer for extreme
multi-label learning. In Proceedings of the 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, number KDD 2014, pages 263–272, 2014."
REFERENCES,0.211750305997552,"Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classiﬁcation. In Proceedings of the 8th International Conference
on Learning Representations, number ICLR 2020, 2020."
REFERENCES,0.21297429620563035,"Timothy N. Rubin, America Chambers, Padhraic Smyth, and Mark Steyvers. Statistical topic models
for multi-label document classiﬁcation. Machine Learning, 88(1-2):157–208, 2012."
REFERENCES,0.2141982864137087,"Chenglei Shen, Xiao Zhang, Wei Wei, and Jun Xu. Hyperbandit: Contextual bandit with hypernewtork
for time-varying user preferences in streaming recommendation. In Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management, pages 2239–2248, 2023."
REFERENCES,0.21542227662178703,"Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In
Advances in Neural Information Processing Systems, volume 23, pages 2199–2207, 2010."
REFERENCES,0.21664626682986537,"Yan-Ping Sun and Min-Ling Zhang. Compositional metric learning for multi-label classiﬁcation.
Frontiers of Computer Science, 15(5):155320, 2021."
REFERENCES,0.2178702570379437,"Huayi Tang and Yong Liu. Towards understanding the generalization of graph neural networks.
arXiv:2305.08048v1, 2023."
REFERENCES,0.21909424724602203,"Wei Weng, Yaojin Lin, Shunxiang Wu, Yuwen Li, and Yun Kang. Multi-label learning based on
label-speciﬁc features and local pairwise label correlation. Neurocomputing, 273:385–394, 2018."
REFERENCES,0.22031823745410037,"Wei Weng, Yan-Nan Chen, Chin-Ling Chen, Shunxiang Wu, and Jinghua Liu. Non-sparse label
speciﬁc features selection for multi-label classiﬁcation. Neurocomputing, 377:85–94, 2020."
REFERENCES,0.2215422276621787,"Guoqiang Wu and Jun Zhu. Multi-label classiﬁcation: do hamming loss and subset accuracy really
conﬂict with each other? Advances in Neural Information Processing Systems, 33(NeurIPS 2020),
2020."
REFERENCES,0.22276621787025705,"Guoqiang Wu, Chongxuan Li, Kun Xu, and Jun Zhu. Rethinking and reweighting the univariate
losses for multi-label ranking: Consistency and generalization. Advances in Neural Information
Processing Systems, 34(NeurIPS 2021):14332–14344, 2021a."
REFERENCES,0.22399020807833536,"Guoqiang Wu, Chongxuan Li, and Yilong Yin. Towards understanding generalization of macro-auc
in multi-label learning. In Proceedings of the 40th International Conference on Machine Learning,
volume 202, pages 37540–37570, 2023."
REFERENCES,0.2252141982864137,"Liang Wu, Antoine Ledent, Yunwen Lei, and Marius Kloft. Fine-grained generalization analysis
of vector-valued learning. In Proceedings of the 35th AAAI Conference on Artiﬁcial Intelligence,
number AAAI 2021, pages 10338–10346, 2021b."
REFERENCES,0.22643818849449204,"Chang Xu, Tongliang Liu, Dacheng Tao, and Chao Xu. Local rademacher complexity for multi-label
learning. IEEE Transactions on Image Processing, 25(3):1495–1507, 2016."
REFERENCES,0.22766217870257038,"Miao Xu and Lan-Zhe Guo. Learning from group supervision: the impact of supervision deﬁciency
on multi-label learning. Science China Information Sciences, 64(3), 2021."
REFERENCES,0.22888616891064872,"Guangxu Xun, Kishlay Jha, Jianhui Sun, and Aidong Zhang. Correlation networks for extreme
multi-label text classiﬁcation. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, number KDD 2020, pages 1074–1082, 2020."
REFERENCES,0.23011015911872704,"Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bartlomiej Twardowski, and Joost van de
Weijer. Orderless recurrent models for multi-label classiﬁcation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, number CVPR 2020, pages 13440–13449,
2020."
REFERENCES,0.23133414932680538,"Ian En-Hsu Yen, Xiangru Huang, Pradeep Ravikumar, Kai Zhong, and Inderjit S. Dhillon. Pd-
sparse : A primal and dual sparse approach to extreme multiclass and multilabel classiﬁcation.
In Proceedings of the 33nd International Conference on Machine Learning, volume 48, pages
3069–3077, 2016."
REFERENCES,0.23255813953488372,"Renchun You, Zhiyao Guo, Lei Cui, Xiang Long, Yingze Bao, and Shilei Wen. Cross-modality
attention with semantic graph embedding for multi-label classiﬁcation. In Proceedings of the 34th
AAAI Conference on Artiﬁcial Intelligence, number AAAI 2020, pages 12709–12716, 2020."
REFERENCES,0.23378212974296206,"Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit S. Dhillon. Large-scale multi-label
learning with missing labels. In Proceedings of the 31th International Conference on Machine
Learning, volume 32, pages 593–601, 2014."
REFERENCES,0.2350061199510404,"Kai Yu, Shipeng Yu, and Volker Tresp. Multi-label informed latent semantic indexing. In Ricardo A.
Baeza-Yates, Nivio Ziviani, Gary Marchionini, Alistair Moffat, and John Tait, editors, Proceed-
ings of the 28th Annual International Conference on Research and Development in Information
Retrieval, number SIGIR 2005, pages 258–265, 2005."
REFERENCES,0.23623011015911874,"Ze-Bang Yu and Min-Ling Zhang. Multi-label classiﬁcation with label-speciﬁc feature generation:
A wrapped approach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):
5199–5210, 2022."
REFERENCES,0.23745410036719705,"Chunyu Zhang and Zhanshan Li. Multi-label learning with label-speciﬁc features via weighting and
label entropy guided clustering ensemble. Neurocomputing, 419:59–69, 2021."
REFERENCES,0.2386780905752754,"Jujie Zhang, Min Fang, and Xiao Li. Multi-label learning with discriminative features for each label.
Neurocomputing, 154:305–316, 2015."
REFERENCES,0.23990208078335373,"Min-Ling Zhang and Lei Wu. Lift: Multi-label learning with label-speciﬁc features. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 37(1):107–120, 2015."
REFERENCES,0.24112607099143207,"Min-Ling Zhang and Zhi-Hua Zhou. ML-KNN: A lazy learning approach to multi-label learning.
Pattern Recognition, 40(7):2038–2048, 2007."
REFERENCES,0.2423500611995104,"Min-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. IEEE Transactions
on Knowledge and Data Engineering, 26(8):1819–1837, 2014."
REFERENCES,0.24357405140758873,"Min-Ling Zhang, Yu-Kun Li, Xu-Ying Liu, and Xin Geng. Binary relevance for multi-label learning:
an overview. Frontiers of Computer Science, 12(2):191–202, 2018."
REFERENCES,0.24479804161566707,"Yi-Fan Zhang and Shizhong Liao. A kernel perspective for the decision boundary of deep neural
networks. In Proceedings of the 32nd IEEE International Conference on Tools with Artiﬁcial
Intelligence, number ICTAI 2020, pages 653–660, 2020."
REFERENCES,0.2460220318237454,"Yi-Fan Zhang and Min-Ling Zhang. Nearly-tight bounds for deep kernel learning. In Proceedings of
the 40th International Conference on Machine Learning, volume 202, pages 41861–41879, 2023."
REFERENCES,0.24724602203182375,"Yi-Fan Zhang and Min-Ling Zhang. Generalization analysis for multi-label learning. In Proceedings
of the 41st International Conference on Machine Learning, number ICML 2024, 2024."
REFERENCES,0.2484700122399021,"Yue Zhu, James T. Kwok, and Zhi-Hua Zhou. Multi-label learning with global and local label
correlation. IEEE Transactions on Knowledge and Data Engineering, 30(6):1081–1094, 2018."
REFERENCES,0.24969400244798043,"A
Appendix"
REFERENCES,0.25091799265605874,"A.1
Appendix Outline"
REFERENCES,0.2521419828641371,"In the appendix, we give the detailed proofs of those theoretical results in the main paper. Our main
proofs include:"
REFERENCES,0.2533659730722154,• The ℓ∞Lipschitz continuity of the commonly used losses for LSRL (Proposition 1).
REFERENCES,0.25458996328029376,• The novel vector-contraction inequality for ℓ∞Lipschitz loss (Lemma 1).
REFERENCES,0.2558139534883721,• The generalization bound of the general LSRL class with no dependency on c (Theorem 1).
REFERENCES,0.25703794369645044,• The generalization bound for k-means clustering-based LSRL method (Theorem 2).
REFERENCES,0.2582619339045288,• The (pseudo-) Lipschitz continuity of the squared and Decomposable loss (Proposition 2).
REFERENCES,0.2594859241126071,• The generalization bound for Lasso-based LSRL method (Theorem 3).
REFERENCES,0.2607099143206854,• The generalization bound for DNN-based LSRL method (Theorem 4).
REFERENCES,0.26193390452876375,"A.2
Preliminaries"
REFERENCES,0.2631578947368421,"A.2.1
Deﬁnitions of the corresponding complexity measures"
REFERENCES,0.26438188494492043,"Deﬁnition 3 (ℓ∞norm covering number). Let F be a class of real-valued functions mapping from
X to R. Let D = {x1, . . . , xn} be a set with n i.i.d. samples. For any ϵ > 0, the empirical ℓ∞
norm covering number N∞(ϵ, F, D) w.r.t. D is deﬁned as the minimal number m of a collection
of vectors v1, . . . , vm ∈Rn such that maxi∈[n]
f (xi) −vj
i
 ≤ϵ (vj
i is the i-th component of the"
REFERENCES,0.26560587515299877,"vector vj). In this case, we call

v1, . . . , vm	
an (ϵ, ℓ∞)-cover of F with respect to D. We also
deﬁne N∞(ϵ, F, n) = supD N∞(ϵ, F, D)."
REFERENCES,0.2668298653610771,"Deﬁnition 4 (Fat-shattering dimension). Let F be a class of real-valued functions mapping from
X to R. We deﬁne the fat-shattering dimension fatϵ(F) at scale ϵ > 0 as the largest p ∈N
such that there exist p points x1, . . . , xp ∈X and witnesses s1, . . . , sp ∈R satisfying: for any
δ1, . . . , δp ∈{−1, +1} there exists f ∈F with δi (f(xi) −si) ≥ϵ, ∀i = 1, . . . , p."
REFERENCES,0.26805385556915545,"A.2.2
The Bound for the loss function space"
REFERENCES,0.2692778457772338,"For any training dataset D = {(xi, yi) : i ∈[n]}, let D′ = {(xi, yi) : i ∈[n]} be the training
dataset with only one sample different from D, where the k-th sample is replaced by (x′
k, y′
k). Let
Φ(D) = supf∈F[E(x,y)∼P [ℓ(f(x), y)] −1"
REFERENCES,0.27050183598531213,"n
Pn
i=1 ℓ(f(xi), yi)] = supf∈F[R(f) −bRD(f)], then"
REFERENCES,0.2717258261933905,Φ (D′) −Φ(D)
REFERENCES,0.2729498164014688,"= sup
f∈F
[R(f) −bRD′(f)] −sup
f∈F
[R(f) −bRD(f)]"
REFERENCES,0.2741738066095471,"≤sup
f∈F
[ bRD(f) −bRD′(f)]"
REFERENCES,0.27539779681762544,"= sup
f∈F"
REFERENCES,0.2766217870257038,"[ℓ(f(xk), yk) −ℓ(f(x′
k), y′
k)]
n ≤M n ."
REFERENCES,0.2778457772337821,"According to McDiarmid’s inequality, for any 0 < δ < 1, with probability at least 1 −δ"
OVER THE,0.27906976744186046,"2 over the
training dataset D, the following holds:"
OVER THE,0.2802937576499388,Φ(D) ≤ED[Φ(D)] + M r
OVER THE,0.28151774785801714,ln(2/δ)
N,0.2827417380660955,"2n
.
(2)"
N,0.2839657282741738,"Then, we will estimate the upper bound of ED[Φ(D)]."
N,0.28518971848225216,"ED[Φ(D)] =ED """
N,0.2864137086903305,"sup
f∈F"
N,0.2876376988984088,"h
ED′
h
bRD′(f) −bRD(f)
ii#"
N,0.28886168910648713,"≤ED,D′ """
N,0.29008567931456547,"sup
f∈F"
N,0.2913096695226438,"h
bRD′(f) −bRD(f)
i#"
N,0.29253365973072215,"=ED,D′ """
N,0.2937576499388005,"sup
f∈F"
N,0.29498164014687883,"1
n "" n
X"
N,0.2962056303549572,"i=1
ℓ(f(x′
i), y′
i) −ℓ(f(xi), yi) ##"
N,0.2974296205630355,"=Eϵ,D,D′ """
N,0.29865361077111385,"sup
f∈F"
N,0.2998776009791922,"1
n "" n
X"
N,0.3011015911872705,"i=1
ϵi (ℓ(f(x′
i), y′
i)) −ℓ(f(xi), yi)) ##"
N,0.3023255813953488,"≤Eϵ,D′ """
N,0.30354957160342716,"sup
f∈F"
N,0.3047735618115055,"1
n n
X"
N,0.30599755201958384,"i=1
ϵiℓ(f(x′
i), y′
i) #"
N,0.3072215422276622,"+ Eϵ,D """
N,0.3084455324357405,"sup
f∈F"
N,0.30966952264381886,"1
n n
X"
N,0.3108935128518972,"i=1
−ϵiℓ(f(xi), yi) #"
N,0.31211750305997554,"=2Eϵ,D """
N,0.31334149326805383,"sup
f∈F"
N,0.31456548347613217,"1
n n
X"
N,0.3157894736842105,"i=1
ϵiℓ(f(xi), yi) # .
(3)"
N,0.31701346389228885,"Then apply McDiarmid’s inequality to Eϵ

supf∈F
1
n
Pn
i=1 ϵiℓ(f(xi), yi)

, we have Eϵ,D """
N,0.3182374541003672,"sup
f∈F"
N,0.31946144430844553,"1
n n
X"
N,0.32068543451652387,"i=1
ϵiℓ(f(xi), yi) # ≤Eϵ """
N,0.3219094247246022,"sup
f∈F"
N,0.32313341493268055,"1
n n
X"
N,0.3243574051407589,"i=1
ϵiℓ(f(xi), yi) # + M r"
N,0.32558139534883723,ln(2/δ)
N,0.3268053855569155,"2n
, i.e.,"
N,0.32802937576499386,ℜ(L) ≤ˆℜD(L) + M r
N,0.3292533659730722,ln(2/δ)
N,0.33047735618115054,"2n
.
(4)"
N,0.3317013463892289,"Combining with (2), (3) and (4), then"
N,0.3329253365973072,R(f) ≤bRD(f) + 2ˆℜD(L) + 3M s log 2
N,0.33414932680538556,"δ
2n .
(5)"
N,0.3353733170134639,"A.3
General Bounds for LSRL"
N,0.33659730722154224,"A.3.1
Proof of Proposition 1"
N,0.3378212974296206,"We ﬁrst prove that the surrogate Hamming Loss is µ-Lipschitz continuous with respect to the ℓ∞
norm.
ℓH(f(x), y) −ℓH
 
f ′(x), y
 = "
C,0.3390452876376989,"1
c c
X"
C,0.3402692778457772,"j=1
ℓb (yjfj(x)) −1 c c
X"
C,0.34149326805385555,"j=1
ℓb
 
yjf ′
j(x)
 =1 c c
X j=1"
C,0.3427172582619339,"ℓb (yjfj(x)) −ℓb
 
yjf ′
j(x)
 ≤1 c c
X"
C,0.34394124847001223,"j=1
µ
fj(x) −f ′
j(x) ≤1"
C,0.34516523867809057,"c µc max
j∈[c]"
C,0.3463892288861689,"fj(x) −f ′
j(x)"
C,0.34761321909424725,"=µ
f(x) −f ′(x)

∞."
C,0.3488372093023256,"Second, with the elementary inequality"
C,0.35006119951040393,"|max {a1, . . . , ac} −max {b1, . . . , bc}| ≤max {|a1 −b1| , . . . , |ac −bc|} ,"
C,0.35128518971848227,"we prove that the surrogate Subset Loss is µ-Lipschitz continuous with respect to the ℓ∞norm.
ℓS(f(x), y) −ℓS
 
f ′(x), y
"
C,0.3525091799265606,"=
max
j∈[c] ℓb (yjfj(x)) −max
j∈[c] ℓb
 
yjf ′
j(x)
"
C,0.3537331701346389,"≤max
j∈[c]"
C,0.35495716034271724,"ℓb (yjfj(x)) −ℓb
 
yjf ′
j(x)
"
C,0.3561811505507956,"≤µ max
j∈[c]"
C,0.3574051407588739,"fj(x) −f ′
j(x)"
C,0.35862913096695226,"=µ
f(x) −f ′(x)

∞."
C,0.3598531211750306,"Third, we prove that the surrogate Ranking Loss is 2µ-Lipschitz continuous with respect to the ℓ∞
norm.
ℓR(f(x), y) −ℓR
 
f ′(x), y
"
C,0.36107711138310894,"=
1
|Y +| |Y −|  X p∈Y + X q∈Y −"
C,0.3623011015911873," 
ℓb (fp(x) −fq(x)) −ℓb
 
f ′
p(x) −f ′
q(x)
"
C,0.3635250917992656,"≤
max
p∈Y +,q∈Y −
ℓb (fp(x) −fq(x)) −ℓb
 
f ′
p(x) −f ′
q(x)
"
C,0.36474908200734396,"≤µ
max
p∈Y +,q∈Y −
(fp(x) −fq(x)) −
 
f ′
p(x) −f ′
q(x)
"
C,0.3659730722154223,"≤µ

max
p∈Y +
fp(x) −f ′
p(x)
 + max
q∈Y −
fq(x) −f ′
q(x)

"
C,0.3671970624235006,"≤2µ max
j∈[c]"
C,0.3684210526315789,"fj(x) −f ′
j(x)"
C,0.36964504283965727,"=2µ
f(x) −f ′(x)

∞."
C,0.3708690330477356,"Finally, we prove that the surrogate Decomposable Loss is cµ-Lipschitz continuous with respect to
the ℓ∞norm.
ℓD(f(x), y) −ℓD
 
f ′(x), y
 =  c
X"
C,0.37209302325581395,"j=1
ℓb (yjfj(x)) − c
X"
C,0.3733170134638923,"j=1
ℓb
 
yjf ′
j(x)
 = c
X j=1"
C,0.37454100367197063,"ℓb (yjfj(x)) −ℓb
 
yjf ′
j(x)
 ≤ c
X"
C,0.37576499388004897,"j=1
µ
fj(x) −f ′
j(x)"
C,0.3769889840881273,"≤cµ max
j∈[c]"
C,0.37821297429620565,"fj(x) −f ′
j(x)"
C,0.379436964504284,"=cµ
f(x) −f ′(x)

∞."
C,0.3806609547123623,"A.3.2
Proof of Lemma 1"
C,0.3818849449204406,"Proof Sketch: First, the Rademacher complexity of the loss function space associated with F can
be bounded by the empirical ℓ∞norm covering number with the reﬁned Dudley’s entropy integral
inequality. Second, according to the Lipschitz continuity w.r.t the ℓ∞norm, the empirical ℓ∞norm
covering number of F can be bounded by the empirical ℓ∞norm covering number of P(F). Third,
the empirical ℓ∞norm covering number of P(F) can be bounded by the fat-shattering dimension,
and the fat-shattering dimension can be bounded by the worst-case Rademacher complexity of P(F)."
C,0.38310893512851896,"Hence, the problem is transferred to the estimation of the worst-case Rademacher complexity. Finally,
we estimate the lower bound of the worst-case Rademacher complexity of P(F), and then combined
with the above steps, the Rademacher complexity of the loss function space associated with F can be
bounded."
C,0.3843329253365973,We ﬁrst introduce the following lemmas:
C,0.38555691554467564,"Lemma 2 (Khintchine-Kahane inequality [Lust-Piquard and Pisier, 1991]). Let v1, . . . , vn ∈H,
where H is a Hilbert space with ∥· ∥being the associated p-th norm. Let ϵ1, . . . , ϵn be a sequence of
independent Rademacher variables. Then, for any p ≥1 there holds"
C,0.386780905752754,"min(
p"
C,0.3880048959608323,"p −1, 1) "" n
X"
C,0.38922888616891066,"i=1
∥vi∥2
# 1 2
≤ "" Eϵ  n
X"
C,0.390452876376989,"i=1
ϵivi  p# 1"
C,0.39167686658506734,"p
≤max(
p"
C,0.3929008567931457,"p −1, 1) "" n
X"
C,0.39412484700122397,"i=1
∥vi∥2
# 1 2
, and Eϵ  n
X"
C,0.3953488372093023,"i=1
ϵivi ≥2−1 2 "" n
X"
C,0.39657282741738065,"i=1
∥vi∥2
# 1 2
."
C,0.397796817625459,"Lemma 3 (Lemma A.2 in [Srebro et al., 2010]). For any function class F, any S with a ﬁnite sample
of size n and any ϵ > ˆℜS(F), we have that"
C,0.3990208078335373,"fatϵ(F) ≤4nˆℜ2
S(F)
ϵ2
."
C,0.40024479804161567,"Lemma 4 ([Srebro et al., 2010, Lei et al., 2019]). If any function in class F takes values in [−B, B],
then for any S with a ﬁnite sample of size n, any ϵ > 0 with fatϵ(F) < n, we have"
C,0.401468788249694,"log N∞(ϵ, F, S) ≤fatϵ(F) log 2eBn ϵ
."
C,0.40269277845777235,"The following lemma is a reﬁned result of Proposition 22 in [Ledent et al., 2021], where we replace
the function class taking values in [0, 1] with the B-bounded function class, the reﬁnement is obvious."
C,0.4039167686658507,"Lemma 5 (Reﬁned Dudley’s entropy integral inequality). Let F be a real-valued function class
with f ≤B, f ∈F, B > 0, and assume that 0 ∈F. Let S be a ﬁnite sample of size n. For any
2 ≤p ≤∞, we have the following relationship between the Rademacher complexity ˆℜS(F) and the
covering number Np(ϵ, F, S)."
C,0.40514075887392903,"ˆℜS(F) ≤inf
α>0 "
C,0.40636474908200737,"4α + 12
√n Z B α q"
C,0.40758873929008566,"log Np(ϵ, F, S)dϵ ! ."
C,0.408812729498164,"Step 1: We ﬁrst derive the relationship between the empirical ℓ∞norm covering number N∞(ϵ, L, D)
and the empirical ℓ∞norm covering number N∞(ϵ, P(F), [c] × D)."
C,0.41003671970624234,"For the dataset D = {(x1, y1), . . . , (xn, yn)} with n i.i.d. examples:"
C,0.4112607099143207,"max
i
|ℓ(f(xi), yi) −ℓ(f ′(xi), yi)|"
C,0.412484700122399,"≤ρ max
i
∥f(xi) −f ′(xi)∥∞
(Use Assumption 2)"
C,0.41370869033047736,"≤ρ max
i
max
j
|fj (xi) −f ′
j (xi) |"
C,0.4149326805385557,"≤ρ max
i
max
j
|pj(f(xi)) −pj(f ′(xi)|.
(The deﬁnition of the projection function class P(F))"
C,0.41615667074663404,"Then, according to the deﬁnition of the empirical ℓ∞covering number, we have that an empirical ℓ∞
cover of P(F) at radius ϵ/ρ is also an empirical ℓ∞cover of the loss function space associated with
F at radius ϵ, and we can conclude that:"
C,0.4173806609547124,"N∞(ϵ, L, D) ≤N∞  ϵ"
C,0.4186046511627907,"ρ, P(F), [c] × D

.
(6)"
C,0.41982864137086906,"Step 2: We show that the empirical ℓ∞norm covering number of P(F) can be bounded by the fat-
shattering dimension, and the fat-shattering dimension can be bounded by the worst-case Rademacher
complexity of P(F)."
C,0.42105263157894735,"According to Lemma 3, for any ϵ > ˆℜ[c]×D(P(F)), we have"
C,0.4222766217870257,"fatϵ(P(F)) ≤
4ncˆℜ2
[c]×D(P(F)) ϵ2
."
C,0.423500611995104,"Then, combining with Lemma 4, we have"
C,0.42472460220318237,"log N∞(ϵ, P(F), [c] × D) ≤
4ncˆℜ2
[c]×D(P(F))"
C,0.4259485924112607,"ϵ2
log 2eBnc ϵ
."
C,0.42717258261933905,"Use inequality ϵ > ˆℜ[c]×D(P(F)), we have"
C,0.4283965728274174,"log N∞(ϵ, P(F), [c] × D) ≤
4ncˆℜ2
[c]×D(P(F))"
C,0.42962056303549573,"ϵ2
log
2eBnc
ˆℜ[c]×D(P(F))
.
(7)"
C,0.43084455324357407,"Step 3: According to Assumption 1 in the main paper, we can obtain the lower bound of the worst-case
Rademacher complexity eℜnc(P(F)) by the Khintchine-Kahane inequality with p = 1:"
C,0.4320685434516524,eℜnc(P(F))
C,0.43329253365973075,"=
sup
[c]×D∈[c]×X n
ˆℜ[c]×D(P(F))"
C,0.43451652386780903,"=
sup
[c]×D∈[c]×X n Eϵ "
C,0.4357405140758874,"
sup
pj(f(xi))∈P(F)"
NC,0.4369645042839657,"1
nc n
X i=1 c
X"
NC,0.43818849449204406,"j=1
ϵijpj(f(xi))  "
NC,0.4394124847001224,"=
sup
[c]×D∈[c]×X n Eϵ "
NC,0.44063647490820074,"sup
fj∈Fj"
NC,0.4418604651162791,"1
nc n
X i=1 c
X"
NC,0.4430844553243574,"j=1
ϵijfj (xi)  "
NC,0.44430844553243576,"=
sup
∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c]"
NC,0.4455324357405141,"1
ncEϵ "
NC,0.4467564259485924,"
sup
∥wj∥2≤Λ n
X i=1 c
X"
NC,0.4479804161566707,"j=1
ϵij⟨wj, ζj(φj(xi))⟩  "
NC,0.44920440636474906,"=
sup
∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c]"
NC,0.4504283965728274,"Λ
ncEϵ∥ n
X i=1 c
X"
NC,0.45165238678090575,"j=1
ϵijζj(φj(xi))∥"
NC,0.4528763769889841,"≥
sup
∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c]"
NC,0.4541003671970624,"Λ
nc
1
√ 2  
n
X i=1 c
X"
NC,0.45532435740514077,"j=1
∥ζj(φj(xi))∥2   1
2"
NC,0.4565483476132191,".
(Use Lemma 2)"
NC,0.45777233782129745,"Since ∥ζj(φj(xi))∥2 ≤A, we set sup∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c]
1
nc
hPn
i=1
Pc
j=1 ∥ζj(φj(xi))∥2i 1 2 ="
NC,0.4589963280293758,"A
√nc. So,"
NC,0.4602203182374541,"eℜnc(P(F)) ≥
ΛA
√"
NC,0.4614443084455324,"2nc =
B
√"
NC,0.46266829865361075,"2nc.
(8)"
NC,0.4638922888616891,"Step 4: According to Lemma 5 and combined with the above steps, we have
ˆℜD(L)"
NC,0.46511627906976744,"≤inf
α>0 "
NC,0.4663402692778458,"4α + 12
√n Z M α p"
NC,0.4675642594859241,"log N∞(ϵ, L, D)dϵ !"
NC,0.46878824969400246,"≤inf
α>0 "
NC,0.4700122399020808,"4α + 12
√n Z M α r"
NC,0.47123623011015914,log N∞( ϵ
NC,0.4724602203182375,"ρ, P(F), [c] × D)dϵ !"
NC,0.47368421052631576,(Use inequality (6))
NC,0.4749082007343941,"≤inf
α>0 "
NC,0.47613219094247244,"
4α + 12
√n Z M α"
NC,0.4773561811505508,"v
u
u
t4ncρ2 ˆℜ2
[c]×D(P(F))"
NC,0.4785801713586291,"ϵ2
log
2eBnc
ˆℜ[c]×D(P(F))
dϵ "
NC,0.47980416156670747,"

(Use inequality (7))"
NC,0.4810281517747858,"≤inf
α>0 "
NC,0.48225214198286415,"4α + 12
√n Z M α s"
NC,0.4834761321909425,4ncρ2eℜ2nc(P(F))
NC,0.4847001223990208,"ϵ2
log(2
√"
EBN,0.48592411260709917,"2eBn
3
2 c
3
2 )dϵ  "
EBN,0.48714810281517745,(Use inequality (8) and the deﬁnition of the worst-case Rademacher complexity)
EBN,0.4883720930232558,"≤inf
α>0 "
EBN,0.48959608323133413,"4α + 12
√"
EBN,0.4908200734394125,"2ρ√ceℜnc(P(F)) log
1
2 (8e2n3c3)
Z M"
EBN,0.4920440636474908,"α
ϵ−1dϵ ! ≤12
√"
EBN,0.49326805385556916,"2ρ√ceℜnc(P(F)) + 12
√"
EBN,0.4944920440636475,"2ρ√ceℜnc(P(F)) log
1
2 (8e2n3c3) · log
M 3
√"
EBN,0.49571603427172584,2ρ√ceℜnc(P(F))
EBN,0.4969400244798042,"(Choose α = 3
√"
EBN,0.4981640146878825,"2ρ√ceℜnc(P(F))) ≤12
√"
EBN,0.49938800489596086,"2ρ√ceℜnc(P(F)) + 12
√"
EBN,0.5006119951040392,"2ρ√ceℜnc(P(F)) log
1
2 (8e2n3c3) · log M√n"
EBN,0.5018359853121175,"ρB
(Use inequality (8) ) =12
√"
EBN,0.5030599755201959,"2ρ√ceℜnc(P(F))

1 + log
1
2 (8e2n3c3) · log M√n ρB 
."
EBN,0.5042839657282742,"A.3.3
Proof of Theorem 1"
EBN,0.5055079559363526,"We upper bound the worst-case Rademacher complexity eℜnc(P(F)) as the following:
eℜnc(P(F)) =
sup
[c]×D∈[c]×X n
ˆℜ[c]×D(P(F))"
EBN,0.5067319461444308,"=
sup
[c]×D∈[c]×X n Eϵ "
EBN,0.5079559363525091,"
sup
pj(f(xi))∈P(F)"
NC,0.5091799265605875,"1
nc n
X i=1 c
X"
NC,0.5104039167686658,"j=1
ϵijpj(f(xi))  "
NC,0.5116279069767442,"=
sup
[c]×D∈[c]×X n Eϵ "
NC,0.5128518971848225,"sup
fj∈Fj"
NC,0.5140758873929009,"1
nc n
X i=1 c
X"
NC,0.5152998776009792,"j=1
ϵijfj (xi)  "
NC,0.5165238678090576,"=
sup
∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c]"
NC,0.5177478580171359,"1
ncEϵ "
NC,0.5189718482252142,"
sup
∥wj∥2≤Λ n
X i=1 c
X"
NC,0.5201958384332925,"j=1
ϵij⟨wj, ζj(φj(xi))⟩  "
NC,0.5214198286413708,"=
sup
∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c]"
NC,0.5226438188494492,"Λ
ncEϵ∥ n
X i=1 c
X"
NC,0.5238678090575275,"j=1
ϵijζj(φj(xi))∥"
NC,0.5250917992656059,"≤
sup
∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c] Λ
nc  Eϵ∥ n
X i=1 c
X"
NC,0.5263157894736842,"j=1
ϵijζj(φj(xi))∥2   1
2"
NC,0.5275397796817626,(Use Jensen’s Inequality)
NC,0.5287637698898409,"≤
sup
∥ζj(φj(xi))∥2≤A:i∈[n],j∈[c] Λ
nc  
n
X i=1 c
X"
NC,0.5299877600979193,"j=1
∥ζj(φj(xi))∥2   1
2"
NC,0.5312117503059975,"≤ΛA
√nc.
(Use Lemma 2) (9)"
NC,0.5324357405140759,"Then, we have"
NC,0.5336597307221542,"ˆℜD(F) ≤12
√"
NC,0.5348837209302325,"2ρ√ceℜnc(P(F))

1 + log
1
2 (8e2n3c3) · log M√n ρB "
NC,0.5361077111383109,"≤
12
√"
NC,0.5373317013463892,"2ρΛA

1 + log
1
2 (8e2n3c3) · log M√n ρB
 √n
."
NC,0.5385556915544676,"Combining with (5), then"
NC,0.5397796817625459,"R(f) ≤bRD(f) +
24
√"
NC,0.5410036719706243,"2ρΛA

1 + log
1
2 (8e2n3c3) · log M√n ρB
"
NC,0.5422276621787026,"√n
+ 3M s log 2"
NC,0.543451652386781,"δ
2n ."
NC,0.5446756425948592,"A.4
Generalization Bounds for Typical LSRL Methods"
NC,0.5458996328029376,"A.4.1
Proof of Theorem 2"
NC,0.5471236230110159,"The generalization analysis of LIFT involves the generalization analysis for k-means clustering. Ac-
cording to the deﬁnitions of k-means clustering in Subection 5.1, we have the empirical Rademacher
complexity of a loss function space associated with the vector-valued function class G:"
NC,0.5483476132190942,ˆℜD(Lclu ◦G) = Eϵ 
NC,0.5495716034271726,"sup
g∈G"
NC,0.5507955936352509,"1
n(n −1) n
X"
NC,0.5520195838433293,"i,j=1,i̸=j
ϵiℓclu(g(xi, xj))  ."
NC,0.5532435740514076,"Rademacher complexity has proved to be a powerful data-dependent measure of hypothesis space
complexity. However, since the k-means clustering framework involves pairwise functions, a sequence
of pairs of i.i.d. individual observation in k-means clustering is no longer independent, which makes
standard techniques in the i.i.d case for traditional Rademacher complexity inapplicable for k-
means clustering. We convert the non-sum-of-i.i.d pairwise function to a sum-of-i.i.d form by using
permutations in U-process [Clémençon et al., 2008]."
NC,0.554467564259486,"We ﬁrst proof the following lemma:
Lemma 6. Let qτ : X × X 7→R be real-valued functions indexed by τ ∈T where T is some set. If
x1, . . . , xs and x′
1, . . . , x′
t are i.i.d., r = min{s, t}, then for any convex non-decreasing function ψ, Eψ "
NC,0.5556915544675642,"sup
τ∈T"
ST,0.5569155446756426,"1
st s
X i=1 t
X"
ST,0.5581395348837209,"j=1
qτ
 
xi, x′
j

 ≤Eψ "
ST,0.5593635250917993,"sup
τ∈T"
R,0.5605875152998776,"1
r r
X"
R,0.5618115055079559,"i=1
qτ (xi, x′
i) ! ."
R,0.5630354957160343,"Proof. The proof of this lemma is inspired by [Clémençon et al., 2008]. Eψ "
R,0.5642594859241126,"sup
τ∈T"
ST,0.565483476132191,"1
st s
X i=1 t
X"
ST,0.5667074663402693,"j=1
qτ
 
xi, x′
j

  =Eψ "
ST,0.5679314565483476,"sup
τ∈T 1
s! X πx 1
t! X πx′"
R,0.5691554467564259,"1
r r
X"
R,0.5703794369645043,"i=1
qτ

xπ(i), x′
π(i)

  ≤Eψ  1 s! X πx 1
t! X"
R,0.5716034271725826,"πx′
sup
τ∈T"
R,0.572827417380661,"1
r r
X"
R,0.5740514075887393,"i=1
qτ

xπ(i), x′
π(i)

"
R,0.5752753977968176,"
(ψ is nondecreasing) ≤1 s! X πx 1
t! X"
R,0.576499388004896,"πx′
Eψ "
R,0.5777233782129743,"sup
τ∈T"
R,0.5789473684210527,"1
r r
X"
R,0.5801713586291309,"i=1
qτ

xπ(i), x′
π(i)
!"
R,0.5813953488372093,(Jensen’s inequality) =Eψ 
R,0.5826193390452876,"sup
τ∈T"
R,0.583843329253366,"1
r r
X"
R,0.5850673194614443,"i=1
qτ (xi, x′
i) ! ."
R,0.5862913096695227,"According to Lemma 6, ˆℜD(Lclu ◦G) can be bounded by"
R,0.587515299877601,ˆℜD′(Lclu ◦G) := Eϵ 
R,0.5887392900856793,"sup
g∈G 1
⌊n 2 ⌋ ⌊n 2 ⌋
X"
R,0.5899632802937577,"i=1
ϵiℓclu(g(xi, xi+⌊n 2 ⌋))  ,"
R,0.591187270501836,where each ϵi is an independent Rademacher random variable.
R,0.5924112607099143,"In order to obtain tighter generalization bounds for k-means clustering, we develop the following
novel vector-contraction inequality:
Lemma 7. Let G be a vector-valued function class of k-means clustering deﬁned in Subection 5.1.
Given a dataset D of size n. Then, we have"
R,0.5936352509179926,"ˆℜD(Lclu ◦G) ≤12
√"
R,0.594859241126071,"K max
k
eℜ⌊n"
R,0.5960832313341493,2 ⌋(Gk) 
R,0.5973072215422277,"1 + log
1
2 (e2n3) · log
M
eℜ⌊n"
R,0.598531211750306,"2 ⌋(Gk) ! ,"
R,0.5997552019583844,where eℜ⌊n
R,0.6009791921664627,"2 ⌋(Gk) is the worst-case Rademacher complexity, Gk is the restriction of the function class
along the k-th coordinate, gk ∈Gk."
R,0.602203182374541,Proof. We ﬁrst introduce the following lemma:
R,0.6034271725826194,"Lemma 8 (Lemma 1 in [Foster and Rakhlin, 2019]). Let F ⊆

f : X →RK	
, and let φ1, . . . , φn
each be L Lipschitz with respect to the ℓ∞norm. For any D with a ﬁnite sample of size n and ϵ > 0,
we have that"
R,0.6046511627906976,"log N2 (ϵ, φ ◦F, D) ≤K max
k
log N∞
 ϵ"
R,0.605875152998776,"L, Fk, D

,
(10)"
R,0.6070991432068543,"where Fk is the restriction of the function class along the k-th coordinate, fk ∈Fk, k ∈[K]."
R,0.6083231334149327,"The empirical ℓ∞norm covering number of Gk can be bounded by the fat-shattering dimension,
and the fat-shattering dimension can be bounded by the worst-case Rademacher complexity of Gk.
Combined with the above steps, we have"
R,0.609547123623011,ˆℜD(Lclu ◦G) ≤ˆℜD′(Lclu ◦G)
R,0.6107711138310894,"≤inf
α>0 "
R,0.6119951040391677,"4α + 12
√n Z M α p"
R,0.6132190942472461,"log N2(ϵ, Lclu ◦G, D′)dϵ !"
R,0.6144430844553244,(Use Lemma 5 )
R,0.6156670746634026,"≤inf
α>0 "
R,0.616891064871481,"4α + 12
√n Z M α q"
R,0.6181150550795593,"K max
k
log N∞(ϵ, Gk, D′)dϵ !"
R,0.6193390452876377,(Use inequality (10) and ℓclu(·) is 1-Lipschitz w.r.t. ℓ∞norm for k-means clustering)
R,0.620563035495716,"≤inf
α>0 "
R,0.6217870257037944,"4α + 12
√n Z M α s"
R,0.6230110159118727,"K max
k
fatϵ(Gk) log 2eB⌊n"
R,0.6242350061199511,"2 ⌋
ϵ
dϵ "
R,0.6254589963280294,"
(Use Lemma 4)"
R,0.6266829865361077,"≤inf
α>0 "
R,0.627906976744186,"4α + 12
√n Z M α s"
R,0.6291309669522643,"K max
k
4⌊n"
R,0.6303549571603427,"2 ⌋ˆℜ2
D′(Gk)
ϵ2
log 2eG⌊n"
R,0.631578947368421,"2 ⌋
ˆℜD′(Gk)
dϵ "
R,0.6328029375764994,"
(Use Lemma 3)"
R,0.6340269277845777,"≤inf
α>0 "
R,0.6352509179926561,"
4α + 12
√n Z M α"
R,0.6364749082007344,"v
u
u
tK max
k 4⌊n"
R,0.6376988984088128,"2 ⌋eℜ2
⌊n"
R,0.6389228886168911,2 ⌋(Gk)
R,0.6401468788249693,"ϵ2
log 2eG⌊n"
R,0.6413708690330477,"2 ⌋
eℜ⌊n"
R,0.642594859241126,"2 ⌋(Gk)
dϵ  
"
R,0.6438188494492044,(The deﬁnition of the worst-case Rademacher complexity)
R,0.6450428396572827,"≤inf
α>0 "
R,0.6462668298653611,"
4α + 12
√n Z M α s"
R,0.6474908200734394,"2nK maxk eℜ2
⌊n"
R,0.6487148102815178,2 ⌋(Gk)
R,0.6499388004895961,"ϵ2
log(en
3
2 )dϵ  
"
R,0.6511627906976745,"(Use the similar technique to the proof in inequality (8), the lower bound of eℜ⌊n"
R,0.6523867809057528,"2 ⌋(Gk) ≥
G
p 2⌊n 2 ⌋)"
R,0.653610771113831,"≤inf
α>0 "
R,0.6548347613219094,"4α + 12
√"
R,0.6560587515299877,"K max
k
eℜ⌊n"
R,0.6572827417380661,"2 ⌋(Gk) log
1
2 (e2n3)
Z M"
R,0.6585067319461444,"α
ϵ−1dϵ ! ≤12
√"
R,0.6597307221542228,"K max
k
eℜ⌊n"
R,0.6609547123623011,"2 ⌋(Gk) + 12
√"
R,0.6621787025703795,"K max
k
eℜ⌊n"
R,0.6634026927784578,"2 ⌋(Gk) log
1
2 (e2n3) · log
M 3
√"
R,0.6646266829865362,K maxk eℜ⌊n
R,0.6658506731946144,2 ⌋(Gk)
R,0.6670746634026927,"(Choose α = 3
√"
R,0.6682986536107711,"K max
k
eℜ⌊n"
R,0.6695226438188494,"2 ⌋(Gk)) =12
√"
R,0.6707466340269278,"K max
k
eℜ⌊n"
R,0.6719706242350061,2 ⌋(Gk) 
R,0.6731946144430845,"1 + log
1
2 (e2n3) · log
M
eℜ⌊n"
R,0.6744186046511628,2 ⌋(Gk) ! .
R,0.6756425948592412,"Since k-means clustering-based LSRL method is two-stage, the k-means clustering is used to generate
label-speciﬁc representations in the ﬁrst stage, and the second stage is conventional multi-label
learning. Therefore, the corresponding whole function class is actually denoted as H = F + Lclu ◦G.
Since eℜnc(P(H)) ≤eℜnc(P(F)) + eℜnc(P(Lclu ◦G) [Bartlett and Mendelson, 2002], with Lemma
1, we have"
R,0.6768665850673194,"ˆℜD(L ◦H) ≤12
√"
R,0.6780905752753978,"2ρ√c

eℜnc(P(F)) + eℜnc(P(Lclu ◦G))
 
1 + log
1
2 (8e2n3c3) · log M√n µB 
."
R,0.6793145654834761,"According to Lemma 7, we have that the worst-case Rademacher complexity of the loss function space
associated with P(G) can be bounded by the worst-case Rademacher complexity of the restriction of
the function class along each coordinate P(Gk). Hence, for k-means clustering-based LSRL method,
Lemma 7 involves the upper and lower bounds of eℜ⌊nc"
R,0.6805385556915544,2 ⌋(P(Gk)).
R,0.6817625458996328,We then obtain the lower bound of the worst-case Rademacher complexity eℜ⌊nc
R,0.6829865361077111,"2 ⌋(P(Gk)) by the
Khintchine-Kahane inequality with p = 1: eℜ⌊nc"
R,0.6842105263157895,"2 ⌋(P(Gk)) =
sup"
R,0.6854345165238678,[c]×D′∈[c]×X ⌊n
R,0.6866585067319462,"2 ⌋
ˆℜ[c]×D′(P(Gk)) =
sup"
R,0.6878824969400245,[c]×D′∈[c]×X ⌊n 2 ⌋Eϵ 
R,0.6891064871481029,"
sup
pj(gk(xi,xi+⌊n"
R,0.6903304773561811,"2 ⌋))∈P(Gk) 1
⌊n 2 ⌋ ⌊n 2 ⌋
X i=1"
C,0.6915544675642595,"1
c c
X"
C,0.6927784577723378,"j=1
ϵijpj(gk(xi, xi+⌊n 2 ⌋))   =
sup"
C,0.6940024479804161,[c]×D′∈[c]×X ⌊n 2 ⌋Eϵ 
C,0.6952264381884945,"sup
gj
k∈Gj
k 1
⌊n 2 ⌋c ⌊n 2 ⌋
X i=1 c
X"
C,0.6964504283965728,"j=1
ϵijgj
k(xi, xi+⌊n 2 ⌋)  "
C,0.6976744186046512,"=
sup
∥Zj
k(xi,xi+⌊n"
C,0.6988984088127295,"2 ⌋)∥≤1
Eϵ "
C,0.7001223990208079,"
sup
∥V (xi,xi+⌊n"
C,0.7013463892288861,"2 ⌋)∥≤4R2
1
⌊n 2 ⌋c ⌊n 2 ⌋
X i=1 c
X"
C,0.7025703794369645,"j=1
ϵijV (xi, xi+⌊n"
C,0.7037943696450428,"2 ⌋)Zj
k(xi, xi+⌊n 2 ⌋)  "
C,0.7050183598531212,"=
sup
∥Zj
k(xi,xi+⌊n"
C,0.7062423500611995,2 ⌋)∥≤1 4R2 ⌊n
C,0.7074663402692778,"2 ⌋cEϵ∥ ⌊n 2 ⌋
X i=1 c
X"
C,0.7086903304773562,"j=1
ϵijZj
k(xi, xi+⌊n 2 ⌋)∥"
C,0.7099143206854345,"≥
sup
∥Zj
k(xi,xi+⌊n"
C,0.7111383108935129,2 ⌋)∥≤1 4R2 ⌊n
C,0.7123623011015912,"2 ⌋c
1
√ 2   ⌊n 2 ⌋
X i=1 c
X"
C,0.7135862913096696,"j=1
∥Zj
k(xi, xi+⌊n"
C,0.7148102815177478,"2 ⌋)∥2   1
2 ."
C,0.7160342717258262,"Since ∥Zj
k(xi, xi+⌊n"
C,0.7172582619339045,"2 ⌋)∥≤1, we set sup∥Zj
k(xi,xi+⌊n"
C,0.7184822521419829,"2 ⌋)∥≤1
1
⌊n"
C,0.7197062423500612,"2 ⌋c
hP⌊n"
C,0.7209302325581395,"2 ⌋
i=1
Pc
j=1 ∥Zj
k(xi, xi+⌊n"
C,0.7221542227662179,"2 ⌋)∥2i 1 2 = 1
√ ⌊n"
C,0.7233782129742962,"2 ⌋c. So, eℜ⌊nc"
C,0.7246022031823746,2 ⌋(P(Gk))) ≥4R2
C,0.7258261933904528,"√nc =
G
√nc."
C,0.7270501835985312,"Then, we replace the lower bound of eℜ⌊n"
C,0.7282741738066095,"2 ⌋(Gk) in the proof of Lemma 7 with the lower bound of
eℜ⌊nc"
C,0.7294981640146879,"2 ⌋(P(Gk))), and we have"
C,0.7307221542227662,"eℜnc(P(Lclu ◦G)) ≤12
√"
C,0.7319461444308446,"K max
k
eℜ⌊nc"
C,0.7331701346389229,"2 ⌋(P(Gk)))

1 + log
1
2 (e2n3c3) · log M√nc G 
."
C,0.7343941248470012,We then upper bound the worst-case Rademacher complexity eℜ⌊nc
C,0.7356181150550796,2 ⌋(P(Gk)) as the following: eℜ⌊nc
C,0.7368421052631579,"2 ⌋(P(Gk)) =
sup"
C,0.7380660954712362,[c]×D′∈[c]×X ⌊n
C,0.7392900856793145,"2 ⌋
ˆℜ[c]×D′(P(Gk)) =
sup"
C,0.7405140758873929,[c]×D′∈[c]×X ⌊n 2 ⌋Eϵ 
C,0.7417380660954712,"
sup
pj(gk(xi,xi+⌊n"
C,0.7429620563035496,"2 ⌋))∈P(Gk) 1
⌊n 2 ⌋ ⌊n 2 ⌋
X i=1"
C,0.7441860465116279,"1
c c
X"
C,0.7454100367197063,"j=1
ϵijpj(gk(xi, xi+⌊n 2 ⌋))   =
sup"
C,0.7466340269277846,[c]×D′∈[c]×X ⌊n 2 ⌋Eϵ 
C,0.7478580171358629,"sup
gj
k∈Gj
k 1
⌊n 2 ⌋c ⌊n 2 ⌋
X i=1 c
X"
C,0.7490820073439413,"j=1
ϵijgj
k(xi, xi+⌊n 2 ⌋)  "
C,0.7503059975520195,"=
sup
∥Zj
k(xi,xi+⌊n"
C,0.7515299877600979,"2 ⌋)∥≤1
Eϵ "
C,0.7527539779681762,"
sup
∥V (xi,xi+⌊n"
C,0.7539779681762546,"2 ⌋)∥≤4R2
1
⌊n 2 ⌋c ⌊n 2 ⌋
X i=1 c
X"
C,0.7552019583843329,"j=1
ϵijV (xi, xi+⌊n"
C,0.7564259485924113,"2 ⌋)Zj
k(xi, xi+⌊n 2 ⌋)  "
C,0.7576499388004896,"=
sup
∥Zj
k(xi,xi+⌊n"
C,0.758873929008568,2 ⌋)∥≤1 4R2 ⌊n
C,0.7600979192166463,"2 ⌋cEϵ∥ ⌊n 2 ⌋
X i=1 c
X"
C,0.7613219094247246,"j=1
ϵijZj
k(xi, xi+⌊n 2 ⌋)∥"
C,0.762545899632803,"≤
sup
∥Zj
k(xi,xi+⌊n"
C,0.7637698898408812,"2 ⌋)∥≤1 4R2 ⌊n 2 ⌋c  Eϵ∥ ⌊n 2 ⌋
X i=1 c
X"
C,0.7649938800489596,"j=1
ϵijZj
k(xi, xi+⌊n"
C,0.7662178702570379,"2 ⌋)∥2   1
2"
C,0.7674418604651163,"≤
sup
∥Zj
k(xi,xi+⌊n"
C,0.7686658506731946,"2 ⌋)∥≤1 4R2 ⌊n 2 ⌋c   ⌊n 2 ⌋
X i=1 c
X"
C,0.769889840881273,"j=1
∥Zj
k(xi, xi+⌊n"
C,0.7711138310893513,"2 ⌋)∥2   1
2 ≤
G
p ⌊n 2 ⌋c"
C,0.7723378212974297,"≤2G
√nc."
C,0.773561811505508,"Hence, we have"
C,0.7747858017135862,"eℜnc(P(Lclu ◦G)) ≤24
√"
C,0.7760097919216646,"KG
√nc"
C,0.7772337821297429,"
1 + log
1
2 (e2n3c3) · log M√nc G 
."
C,0.7784577723378213,"Use the similar technique to the proof of the inequality above, the upper bound of eℜnc(P(F))"
C,0.7796817625458996,"is 2κΛ
√"
C,0.780905752753978,"KR
√nc
, since ∥φj(x)∥=
qPK
k=1(d(x, cj
k))2 ≤
√"
C,0.7821297429620563,"K maxk ∥x −cj
k∥≤
√"
C,0.7833537331701347,K maxk(∥x∥+
C,0.784577723378213,"∥cj
k∥) ≤2
√ KR."
C,0.7858017135862914,"Finally, combining with the above inequalities and (5), we have"
C,0.7870257037943696,R(f) ≤bRD(f) + 3M s log 2
C,0.7882496940024479,"δ
2n
+
48
√"
C,0.7894736842105263,"2ρκΛ
√"
C,0.7906976744186046,"KR

1 + log
1
2 (8e2n3c3) · log M√n µB
 √n +242√ 2ρ
√ KG
√n"
C,0.791921664626683,"
1 + log
1
2 (e2n3c3) · log M√nc G"
C,0.7931456548347613," 
1 + log
1
2 (8e2n3c3) · log M√n µB 
."
C,0.7943696450428397,"A.4.2
Proof of Proposition 2"
C,0.795593635250918,"First, we prove that the squared loss is 1 pseudo-Lipschitz of order 2."
C,0.7968176254589964,"ℓb (yjfj(x)) −ℓb
 
yjf ′
j(x)
"
C,0.7980416156670747,"=
(yj −fj(x))2 −
 
yj −f ′
j(x)
2"
C,0.799265605875153,"=
(yj −fj(x)) +
 
yj −f ′
j(x)
 ·
(yj −fj(x)) −
 
yj −f ′
j(x)
"
C,0.8004895960832313,"≤
 
|yj −fj(x)| +
yj −f ′
j(x)

·
(yj −fj(x)) −
 
yj −f ′
j(x)
"
C,0.8017135862913096,"≤
 
1 + |yj −fj(x)| +
yj −f ′
j(x)

·
(yj −fj(x)) −
 
yj −f ′
j(x)
"
C,0.802937576499388,"According to the deﬁnition of the pseudo-Lipschitz function, the squared loss is 1 pseudo-Lipschitz
of order 2. Then, we have"
C,0.8041615667074663,"ℓb (yjfj(x)) −ℓb
 
yjf ′
j(x)
"
C,0.8053855569155447,"≤
 
1 + |yj −fj(x)| +
yj −f ′
j(x)

·
(yj −fj(x)) −
 
yj −f ′
j(x)
"
C,0.806609547123623,"≤
 
1 + 2 |yj| + |fj(x)| +
f ′
j(x)
 fj(x) −f ′
j(x)"
C,0.8078335373317014,"≤(3 + 2B)
fj(x) −f ′
j(x)
 ."
C,0.8090575275397797,"Second, we prove that the surrogate Decomposable Loss is (3 + 2B)c-Lipschitz continuous with
respect to the ℓ∞norm if the base loss ℓb is the squared loss."
C,0.8102815177478581,"ℓD(f(x), y) −ℓD
 
f ′(x), y
 =  c
X"
C,0.8115055079559363,"j=1
ℓb (yjfj(x)) − c
X"
C,0.8127294981640147,"j=1
ℓb
 
yjf ′
j(x)
 = c
X j=1"
C,0.813953488372093,"ℓb (yjfj(x)) −ℓb
 
yjf ′
j(x)
 ≤ c
X"
C,0.8151774785801713,"j=1
(3 + 2B)
fj(x) −f ′
j(x)"
C,0.8164014687882497,"≤c(3 + 2B) max
j∈[c]"
C,0.817625458996328,"fj(x) −f ′
j(x)"
C,0.8188494492044064,"=(3 + 2B)c
f(x) −f ′(x)

∞."
C,0.8200734394124847,"A.4.3
Proof of Theorem 3"
C,0.8212974296205631,"Compared with the function class of LSRL where each label corresponds to a Lasso, the function
class of LLSF introduces the additional sharing constraint. In fact, the introduction of the sharing
constraint reduces the complexity of the function class compared with the original class. Then, the
complexity for the function class of LLSF can be bounded by the complexity of the function class of
LSRL where each label corresponds to a Lasso. Hence, the complexity analysis of the function class
of LLSF can be converted into giving the bound of the Rademacher complexity of the LSRL function
class where each label corresponds to a Lasso."
C,0.8225214198286414,"According to the deﬁnition, the function class of LSRL where each label corresponds to a Lasso
means that in the class of LSRL deﬁned by (1), the base loss ℓb is the squared loss, the nonlinear
mappings ζ(·) and φ(·) are both identity transformations for any j ∈[c], and the constraint α(w)
is ∥wj∥1 ≤Λ for any j ∈[c]. The proof process is similar to Lemma 1 and Theorem 1, but the
upper bound of the worst-case Rademacher complexity eℜnc(P(F)) here is different from Theorem
1. According to the above deﬁnitions, we have"
C,0.8237454100367197,eℜnc(P(F))
C,0.824969400244798,"=
sup
[c]×D∈[c]×X n
ˆℜ[c]×D(P(F))"
C,0.8261933904528764,"=
sup
[c]×D∈[c]×X n Eϵ "
C,0.8274173806609547,"
sup
pj(f(xi))∈P(F)"
NC,0.828641370869033,"1
nc n
X i=1 c
X"
NC,0.8298653610771114,"j=1
ϵijpj(f(xi))  "
NC,0.8310893512851897,"=
sup
[c]×D∈[c]×X n Eϵ "
NC,0.8323133414932681,"sup
fj∈Fj"
NC,0.8335373317013464,"1
nc n
X i=1 c
X"
NC,0.8347613219094248,"j=1
ϵijfj (xi)  "
NC,0.835985312117503,"=
sup
∥xj
i ∥2≤R:i∈[n],j∈[c]"
NC,0.8372093023255814,"1
ncEϵ "
NC,0.8384332925336597,"
sup
∥wj∥1≤Λ n
X i=1 c
X"
NC,0.8396572827417381,"j=1
ϵij⟨wj, xj
i⟩  "
NC,0.8408812729498164,"=
sup
∥xj
i ∥2≤R:i∈[n],j∈[c]"
NC,0.8421052631578947,"1
ncEϵ "
NC,0.8433292533659731,"
sup
∥wj∥1≤Λ
∥wj∥1∥ n
X i=1 c
X"
NC,0.8445532435740514,"j=1
ϵijxj
i∥∞ "
NC,0.8457772337821298,"
(Use H¨older’s Inequality)"
NC,0.847001223990208,"≤
sup
∥xj
i ∥2≤R:i∈[n],j∈[c]"
NC,0.8482252141982864,"Λ
ncEϵ∥ n
X i=1 c
X"
NC,0.8494492044063647,"j=1
ϵijxj
i∥2"
NC,0.8506731946144431,"≤
sup
∥xj
i ∥2≤R:i∈[n],j∈[c] Λ
nc  Eϵ∥ n
X i=1 c
X"
NC,0.8518971848225214,"j=1
ϵijxj
i∥2
2   1
2"
NC,0.8531211750305998,(Use Jensen’s Inequality)
NC,0.8543451652386781,"≤
sup
∥xj
i ∥2≤R:i∈[n],j∈[c] Λ
nc  
n
X i=1 c
X"
NC,0.8555691554467564,"j=1
∥xj
i∥2
2   1
2"
NC,0.8567931456548348,(Use Lemma 2)
NC,0.8580171358629131,"≤ΛR
√nc.
(11)"
NC,0.8592411260709915,"Combining with Lemma 1, inequalities (11), (5), and ρ = (3 + 2B)c, then we have"
NC,0.8604651162790697,"R(f) ≤bRD(f) +
24
√"
NC,0.8616891064871481,"2(3 + 2B)cΛR

1 + log
1
2 (8e2n3c3) · log M√n ρB
"
NC,0.8629130966952264,"√n
+ 3M s log 2"
NC,0.8641370869033048,"δ
2n ."
NC,0.8653610771113831,"A.4.4
Proof of Theorem 4"
NC,0.8665850673194615,"First, we upper bound the worst-case Rademacher complexity eℜnc(P(F)) for DNN-based LSRL
method. With the deﬁnitions in the main paper, we have"
NC,0.8678090575275398,eℜnc(P(F))
NC,0.8690330477356181,"=
sup
[c]×D∈[c]×X n
ˆℜ[c]×D(P(F))"
NC,0.8702570379436965,"=
sup
[c]×D∈[c]×X n Eϵ "
NC,0.8714810281517748,"
sup
pj(f(xi))∈P(F)"
NC,0.8727050183598531,"1
nc n
X i=1 c
X"
NC,0.8739290085679314,"j=1
ϵijpj(f(xi))  "
NC,0.8751529987760098,"=
sup
[c]×D∈[c]×X n Eϵ "
NC,0.8763769889840881,"sup
fj∈Fj"
NC,0.8776009791921665,"1
nc n
X i=1 c
X"
NC,0.8788249694002448,"j=1
ϵijfj (xi)  "
NC,0.8800489596083231,"=
sup
[c]×D∈[c]×X n Eϵ "
NC,0.8812729498164015,"sup
∥wj∥≤Λ"
NC,0.8824969400244798,"1
nc n
X i=1 c
X"
NC,0.8837209302325582,"j=1
ϵijσsig(w⊤
j φj(xi))  "
NC,0.8849449204406364,≤2 · 1
SUP,0.8861689106487148,"4
sup
[c]×D∈[c]×X n Eϵ "
SUP,0.8873929008567931,"
sup
∥wj∥≤Λ,φj"
NC,0.8886168910648715,"1
nc n
X i=1 c
X"
NC,0.8898408812729498,"j=1
ϵijw⊤
j φj(xi)  "
NC,0.8910648714810282,(The Lipschitz constant of sigmoid activation is bounded by 1 4) ≤1
NC,0.8922888616891065,"2Λ
sup
[c]×D∈[c]×X n Eϵ sup
φj"
NC,0.8935128518971848,"1
nc  n
X i=1 c
X"
NC,0.8947368421052632,"j=1
ϵijφj(xi)  ≤1"
NC,0.8959608323133414,"2Λ
sup
[c]×D∈[c]×X n Eϵ sup 1 nc  n
X i=1 c
X"
NC,0.8971848225214198,"j=1
ϵijσReLU {W5 · [σReLU(W4xi) ⊙σsig(W3ψ(Y )j)]} "
NC,0.8984088127294981,≤2 · 1
NC,0.8996328029375765,"2Λ
sup
[c]×D∈[c]×X n Eϵ
sup
∥W5∥≤D"
NC,0.9008567931456548,"1
nc  n
X i=1 c
X"
NC,0.9020807833537332,"j=1
ϵijW5 · [σReLU(W4xi) ⊙σsig(W3ψ(Y )j)] "
NC,0.9033047735618115,(ReLU activation is 1-Lipschitz)
NC,0.9045287637698899,"≤ΛD
sup
[c]×D∈[c]×X n Eϵ sup 1 nc  n
X i=1 c
X"
NC,0.9057527539779682,"j=1
ϵij [σReLU(W4xi) ⊙σsig(W3ψ(Y )j)] "
NC,0.9069767441860465,"≤ΛD sup ∥σsig(W3ψ(Y )j)∥
sup
[c]×D∈[c]×X n Eϵ
sup
∥W4∥≤D"
NC,0.9082007343941249,"1
nc  n
X i=1 c
X"
NC,0.9094247246022031,"j=1
ϵijσReLU(W4xi) "
NC,0.9106487148102815,"≤2ΛD sup ∥σsig(W3ψ(Y )j)∥
sup
∥xi∥≤R
Eϵ
sup
∥W4∥≤D"
NC,0.9118727050183598,"1
nc  n
X i=1 c
X"
NC,0.9130966952264382,"j=1
ϵijW4xi "
NC,0.9143206854345165,"≤2ΛD2 sup ∥σsig(W3ψ(Y )j)∥
sup
∥xi∥≤R
Eϵ
1
nc  n
X i=1 c
X"
NC,0.9155446756425949,"j=1
ϵijxi "
NC,0.9167686658506732,"≤2ΛD2 sup ∥σsig(W3ψ(Y )j)∥
sup
∥xi∥≤R"
NC,0.9179926560587516,"1
nc  
Eϵ  n
X i=1 c
X"
NC,0.9192166462668299,"j=1
ϵijxi  2 
 1
2"
NC,0.9204406364749081,(Use Jensen’s Inequality)
NC,0.9216646266829865,"≤2ΛD2 sup ∥σsig(W3ψ(Y )j)∥
sup
∥xi∥≤R"
NC,0.9228886168910648,"1
nc  
n
X i=1 c
X"
NC,0.9241126070991432,"j=1
∥xi∥2   1
2"
NC,0.9253365973072215,"≤2ΛD2 sup ∥σsig(W3ψ(Y )j)∥
R
√nc.
(12)"
NC,0.9265605875152999,"Then, we have to bound sup ∥σsig(W3ψ(Y )j)∥,"
NC,0.9277845777233782,sup ∥σsig(W3ψ(Y )j)∥ ≤1
SUP,0.9290085679314566,"4
sup
∥W3∥≤D
∥W3ψ(Y )j∥
(The Lipschitz constant of sigmoid activation is bounded by 1 4) ≤1"
"D SUP
J",0.9302325581395349,"4D sup
j
∥ψ(Y )j∥
(Use Cauchy-Schwarz Inequality) =D"
"SUP
J
SUP",0.9314565483476133,"4 sup
j
sup
∥W2∥≤D
∥σReLU( ˜AσReLU( ˜AYj∗W1)W2)∥ ≤D"
"SUP
J
SUP",0.9326805385556916,"4 sup
j
sup
∥W2∥≤D
∥˜AσReLU( ˜AYj∗W1)∥∥W2∥ ≤D2"
"SUP
J",0.9339045287637698,"4 sup
j
∥˜AσReLU( ˜AYj∗W1)∥ ≤D2"
"SUP
J",0.9351285189718482,"4 sup
j
∥ c
X"
"SUP
J",0.9363525091799265,"i=1
˜AjiσReLU( c
X"
"SUP
J",0.9375764993880049,"j=1
˜AijYj∗W1)∥ ≤D2"
"SUP
J",0.9388004895960832,"4 sup
j c
X"
"SUP
J",0.9400244798041616,"i=1
˜Aji∥σReLU( c
X"
"SUP
J",0.9412484700122399,"j=1
˜AijYj∗W1)∥ ≤D2"
"SUP
J
SUP",0.9424724602203183,"4 sup
j
sup
∥W1∥≤D c
X"
"SUP
J
SUP",0.9436964504283966,"i=1
˜Aji · ∥ c
X"
"SUP
J
SUP",0.944920440636475,"j=1
˜AijYj∗W1∥ ≤D3"
"SUP
J",0.9461444308445532,"4 sup
j c
X"
"SUP
J",0.9473684210526315,"i=1
˜Aji · ∥ c
X"
"SUP
J",0.9485924112607099,"j=1
˜AijYj∗∥
(Use Cauchy-Schwarz Inequality) ≤D3R"
"SUP
J",0.9498164014687882,"4
sup
j c
X"
"SUP
J",0.9510403916768666,"i=1
˜Aji · c
X"
"SUP
J",0.9522643818849449,"j=1
˜Aij ≤D3R"
"SUP
J",0.9534883720930233,"4
∥˜A∥2
∞
(13)"
"SUP
J",0.9547123623011016,"We denote N(j) as the index set of the one-hop neighbors of the j-th node, and denote g as the node
degree, gmax as the maximum node degree. Then, we bound ∥˜A∥∞as follows n
X"
"SUP
J",0.95593635250918,"j=1
˜Aij = n
X j=1"
"SUP
J",0.9571603427172583,"Aij
√gi + 1pgj + 1"
"SUP
J",0.9583843329253366,"=
1
√gi + 1 "
"SUP
J",0.9596083231334149,"
1
√gi + 1 +
X"
"SUP
J",0.9608323133414932,j∈N(i)
"SUP
J",0.9620563035495716,"1
pgj + 1  "
"SUP
J",0.9632802937576499,"≤
1
√gi + 1 "
"SUP
J",0.9645042839657283,"
1
√1 + 1 +
X"
"SUP
J",0.9657282741738066,j∈N(i)
"SUP
J",0.966952264381885,"1
√1 + 1  "
"SUP
J",0.9681762545899633,"≤
1
√gi + 1
gi + 1
√ 2
= r"
"SUP
J",0.9694002447980417,"gi + 1 2
≤ r"
"SUP
J",0.9706242350061199,gmax + 1
"SUP
J",0.9718482252141983,"2
.
(14)"
"SUP
J",0.9730722154222766,"Combining with Lemma 1, inequalities (12), (13), (14), and (5), then we have"
"SUP
J",0.9742962056303549,"R(f) ≤bRD(f) +
6
√"
"SUP
J",0.9755201958384333,"2ρΛD5R2(gmax + 1)

1 + log
1
2 (8e2n3c3) · log M√n ρB
"
"SUP
J",0.9767441860465116,"√n
+ 3M s log 2"
"SUP
J",0.97796817625459,"δ
2n ."
"SUP
J",0.9791921664626683,NeurIPS Paper Checklist
CLAIMS,0.9804161566707467,1. Claims
CLAIMS,0.981640146878825,"Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: The paper’s contributions and scope are reﬂected accurately by the main claims
made in the abstract and introduction. Please see Abstract and Introduction sections.
2. Limitations"
CLAIMS,0.9828641370869033,"Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: This is a purely theoretical work which improves existing theoretical results.
The assumptions involved in the theoretical results are explained in detail and are satisﬁed
in the relevant settings. Please see Conclusion section.
3. Theory Assumptions and Proofs"
CLAIMS,0.9840881272949816,"Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justiﬁcation: This is a purely theoretical work which improves existing theoretical results.
The assumptions involved in the theoretical results are explained in detail and are satisﬁed
in the relevant settings. In the appendix, we give the detailed proofs of those theoretical
results in the main paper.
4. Experimental Result Reproducibility"
CLAIMS,0.98531211750306,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper does not include experiments.
5. Open access to data and code"
CLAIMS,0.9865361077111383,"Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper does not include experiments
requiring code.
6. Experimental Setting/Details"
CLAIMS,0.9877600979192166,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper does not include experiments.
7. Experiment Statistical Signiﬁcance"
CLAIMS,0.988984088127295,"Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
information about the statistical signiﬁcance of the experiments?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper does not include experiments.
8. Experiments Compute Resources"
CLAIMS,0.9902080783353733,"Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?"
CLAIMS,0.9914320685434517,"Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper does not include experiments.
9. Code Of Ethics"
CLAIMS,0.99265605875153,"Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justiﬁcation: This is theoretical research and the research conducted in the paper conform
with the NeurIPS Code of Ethics.
10. Broader Impacts"
CLAIMS,0.9938800489596084,"Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justiﬁcation: This is theoretical research that does not have a potential negative societal
impact.
11. Safeguards"
CLAIMS,0.9951040391676866,"Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper poses no such risks.
12. Licenses for existing assets"
CLAIMS,0.996328029375765,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justiﬁcation: This is a purely theoretical work.
13. New Assets"
CLAIMS,0.9975520195838433,"Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work.
14. Crowdsourcing and Research with Human Subjects"
CLAIMS,0.9987760097919217,"Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper does not involve crowdsourcing
nor research with human subjects.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: This is a purely theoretical work. This paper does not involve crowdsourcing
nor research with human subjects."
