Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004166666666666667,"Anomaly detection is a widely studied task for a broad variety of data types;
among them, multiple time series appear frequently in applications, including for
example, power grids and trafﬁc networks. Detecting anomalies for multiple time
series, however, is a challenging subject, owing to the intricate interdependencies
among the constituent series. We hypothesize that anomalies occur in low density
regions of a distribution and explore the use of normalizing ﬂows for unsupervised
anomaly detection, because of their superior quality in density estimation. More-
over, we propose a novel ﬂow model by imposing a Bayesian network among con-
stituent series. A Bayesian network is a directed acyclic graph (DAG) that models
causal relationships; it factorizes the joint probability of the series into the prod-
uct of easy-to-evaluate conditional probabilities. We call such a graph-augmented
normalizing ﬂow approach GANF and propose joint estimation of the DAG with
ﬂow parameters. We conduct extensive experiments on real-world datasets and
demonstrate the effectiveness of GANF for density estimation, anomaly detec-
tion, and identiﬁcation of time series distribution drift."
INTRODUCTION,0.008333333333333333,"1
INTRODUCTION"
INTRODUCTION,0.0125,"Anomaly detection (Pimentel et al., 2014; Ruff et al., 2018) is the task of identifying unusual sam-
ples that signiﬁcantly deviate from the majority of the data instances. It is applied in a broad variety
of domains, including risk management (Aven, 2016), video surveillance (Kiran et al., 2018), adver-
sarial example detection (Grosse et al., 2017), and fraud detection (Roy & George, 2017). Repre-
sentative classical methods for anomaly detection are one-class support vector machines (Sch¨olkopf
et al., 2001) and kernel density estimation (Parzen, 1962; Kim & Scott, 2012). These methods rely
on handcrafted features and often are not robust for high-dimensional data (e.g., images, speech
signals, and time series). In recent years, inspired by the success of deep learning for complex
data, many deep anomaly detection methods were proposed and they are remarkably effective in
applications (Ruff et al., 2018; Sabokrou et al., 2018; Goyal et al., 2020)."
INTRODUCTION,0.016666666666666666,"Apart from these demonstrated applications, increasing demand exists for the anomaly detection of
even more complex data; i.e., multiple time series. They contain a set of multivariate time series that
often interact with each other in a system. A prominent example of the source of multiple time se-
ries is the power grid, where each constituent series is the grid state over time, recorded by a sensor
deployed at a certain geographic location. The grid state includes many attributes; e.g., current mag-
nitude and angle, voltage magnitude and angle, and frequency. Time series readings from sensors
at nearby locations are often correlated and their behavior may be causal under cascading effects.
Anomaly detection amounts to timely identifying abnormal grid conditions such as generator trip
and insulator damage."
INTRODUCTION,0.020833333333333332,"Anomaly detection of multiple time series is rather challenging, due to high dimensionality, interde-
pendency, and label scarcity. First, a straightforward approach is to concatenate the constituent series
along the attribute dimension and apply a detection method for multivariate time series. However,
when the system contains many constituents, the resulting data suffer high dimensionality. Second,
constituent series bear intricate interdependencies, which may be implicit and challenging to model.
When an explicit graph topology is known, graph neural networks are widely used to digest the rela-"
INTRODUCTION,0.025,"∗This work was done while E. Dai was an intern at MIT-IBM Watson AI Lab, IBM Research.
†To whom correspondence should be addressed."
INTRODUCTION,0.029166666666666667,Published as a conference paper at ICLR 2022
INTRODUCTION,0.03333333333333333,"tional information (Seo et al., 2016; Li et al., 2018b; Yu et al., 2018; Zhao et al., 2019). However, a
graph may not always be known (because, for example, it is sensitive information) and hence graph
structure learning becomes an indispensable component of the solution (Kipf et al., 2018; Wu et al.,
2020; Shang et al., 2021; Deng & Hooi, 2021). Third, labeling information is often limited. Even if
certain labels are present, in practice, many anomalies may still stay unidentiﬁed because labeling
is laborious and expensive. Hence, unsupervised approaches are the most suitable choice. However,
albeit many unsupervised detection methods were proposed (Ruff et al., 2018; Sabokrou et al., 2018;
Malhotra et al., 2016; Hendrycks et al., 2019), they are not effective for multiple time series."
INTRODUCTION,0.0375,"In this work, we explore the use of normalizing ﬂows (Dinh et al., 2016; Papamakarios et al., 2017)
for anomaly detection, based on a hypothesis that anomalies often lie on low density regions of
the data distribution. Normalizing ﬂows are a class of deep generative models for learning the
underlying distribution of data samples. They are unsupervised and they resolve the label scarcity
challenge aforementioned. An advantage of normalizing ﬂows is that they are particularly effective
in estimating the density of any sample. A recent work by Rasul et al. (2021) extends normalizing
ﬂows for time series data by expressing the density of a series through successive conditioning on
historical data and applying conditional ﬂows to learn each conditional density, paving ways to build
sophisticated ﬂows for multiple time series."
INTRODUCTION,0.041666666666666664,"We address the high dimensionality and interdependency challenges by learning the relational struc-
ture among constituent series. To this end, Bayesian networks (Pearl, 1985; 2000) that model causal
relationships of variables are a principled choice. A Bayesian network is a directed acyclic graph
(DAG) where a node is conditionally independent of its non-descendents given its parents. Such a
structure allows factorizing the intractable joint density of all graph nodes into a product of easy-
to-evaluate conditional densities of each node. Hence, learning the relational structure among con-
stituent series amounts to identifying a DAG that maximizes the densities of observed data."
INTRODUCTION,0.04583333333333333,"We propose a novel framework, GANF (Graph-Augmented Normalizing Flow), to augment a nor-
malizing ﬂow with graph structure learning and to apply it for anomaly detection. There are non-
trivial technical problems to resolve to materialize this framework: (i) How does one inject a graph
into a normalizing ﬂow, which essentially maps one distribution to another? (ii) How does one
learn a DAG, which is a discrete object, inside a continuous ﬂow model? The solution we take is
to factorize the density of a multiple time series along the attribute, the temporal, and the series
dimensions and use a graph-based dependency encoder to model the conditional densities resulting
from factorization. Therein, the graph adjacency matrix is a continuous variable and we impose a
differentiable constraint to ensure that the corresponding graph is acyclic (Zheng et al., 2018; Yu
et al., 2019). We propose a joint training algorithm to optimize both the graph adjacency matrix and
the ﬂow parameters."
INTRODUCTION,0.05,"In addition to resolving the high dimensionality and interdependency challenges, an advantage of
modeling the relational graph structure among constituent series is that one can easily observe the
dynamics of the data distribution from the graph. For time series datasets that span a long period,
one naturally questions if the distribution changes over time. The graph structure is a useful indicator
of distribution drift. We will study the graph evolution empirically observed."
INTRODUCTION,0.05416666666666667,We highlight the following contributions of this work:
INTRODUCTION,0.058333333333333334,"• We propose a framework to augment a normalizing ﬂow with graph structure learning, to model
interdependencies exhibited inside multiple time series."
INTRODUCTION,0.0625,"• We apply the augmented ﬂow model to detect anomalies in multiple time series data and perform
extensive empirical evaluation to demonstrate its effectiveness on real-world data sets."
INTRODUCTION,0.06666666666666667,"• We study the evolution of the learned graph structure and identify distribution drift in time series
data that span a long time period."
RELATED WORK,0.07083333333333333,"2
RELATED WORK"
RELATED WORK,0.075,"Anomaly Detection. Anomaly detection is a widely studied subject owing to its diverse applica-
tions. Recently, inspired by the success of deep learning, several deep anomaly detection methods
are proposed and they achieve remarkable success on complex data, such as individual time se-
ries (Malhotra et al., 2016), images (Sabokrou et al., 2018), and videos (Ionescu et al., 2019). These"
RELATED WORK,0.07916666666666666,Published as a conference paper at ICLR 2022
RELATED WORK,0.08333333333333333,"methods generally fall under three categories: deep one-class models, generative model-based meth-
ods, and transformation-based methods. Deep one-class models (Ruff et al., 2018; Wu et al., 2019)
treat normal instances as the target class and identify instances that do not belong to this class.
In generative model-based methods (Malhotra et al., 2016; Nguyen et al., 2019; Li et al., 2018a),
an autoencoder or a generative adversarial network is used to model the data distribution. Then,
an anomaly measure is deﬁned, such as the reconstruction error in autoencoding. Transformation-
based methods (Golan & El-Yaniv, 2018; Hendrycks et al., 2019) are based on the premise that
transformations applied to normal instances can be identiﬁed while anomalies not. Various transfor-
mations such as rotations and afﬁne transforms have been investigated. On the other hand, anomaly
detection of multiple time series is under explored. Recently, Deng & Hooi (2021) study the use
of graph neural networks in combination with structure learning to detect anomalies. Our method
substantially differs from this work in that the learned structure is a Bayesian network, which allows
density estimation. Moreover, the Bayesian network identiﬁes conditional dependencies among the
constituent series and induces a better interpretation of the graph as well as the data distribution."
RELATED WORK,0.0875,"Normalizing Flows. Normalizing ﬂows are generative models that normalize complex real-world
data distributions to “standard” distributions by using a sequence of invertible and differentiable
transformations.
Dinh et al. (2016) introduce a widely used normalizing ﬂow architecture—
RealNVP—for density estimation. Various extensions and improvements are proposed (Papamakar-
ios et al., 2017; Hoogeboom et al., 2019; Kingma & Dhariwal, 2018). For example, Papamakarios
et al. (2017) view an autoregressive model as a normalizing ﬂow. To model temporal data, Rasul
et al. (2021) use sequential models to parameterize conditional ﬂows. Moreover, graph normalizing
ﬂows are proposed to handle graph structured data and improve predictions and generations (Liu
et al., 2019). In contrast, normalizing ﬂows for multiple time series are rarely studied in the litera-
ture. In this work, we develop a graph-augmented ﬂow for density estimation and anomaly detection
of multiple time series."
PRELIMINARIES,0.09166666666666666,"3
PRELIMINARIES"
PRELIMINARIES,0.09583333333333334,We ﬁrst recall key concepts and familiarize the reader with notations to be used throughout the paper.
NORMALIZING FLOWS,0.1,"3.1
NORMALIZING FLOWS
Let x ∈RD be a D-dimensional random variable. A normalizing ﬂow is a vector-valued invertible
mapping f(x) : RD →RD that normalizes the distribution of x to a “standard” distribution (or
called base distribution). This distribution is usually taken to be an isotropic Gaussian or other ones
that are easy to sample from and whose density is easy to evaluate. Let z = f(x) with probability
density function q(z). With the change-of-variable formula, we can express the density of the x,
p(x), by:
log p(x) = log q(f(x)) + log | det ∇xf(x)|.
(1)"
NORMALIZING FLOWS,0.10416666666666667,"In practical uses, the Jacobian determinant in (1) needs be easy to compute, so that the density p(x)
can be evaluated. Moreover, as a generative model, the invertibility of f allows drawing new in-
stances x = f −1(z) through sampling the base distribution. One example of such f is the masked au-
toregressive ﬂow (Papamakarios et al., 2017), which yields z = [z1, . . . , zD] from x = [x1, . . . , xD]
through
zi = (xi −µi(x1:i−1)) exp(αi(x1:i−1)),
(2)
where µi and αi are neural networks such as the multilayer perceptron."
NORMALIZING FLOWS,0.10833333333333334,"A ﬂow may be augmented with conditional information h ∈Rd with a possibly different dimension.
Such a ﬂow is a conditional ﬂow and is denoted by f : RD × Rd →RD. The log-density of x
conditioned on h admits the following formula:"
NORMALIZING FLOWS,0.1125,"log p(x|h) = log q(f(x; h)) + log | det ∇xf(x; h)|.
(3)"
NORMALIZING FLOWS,0.11666666666666667,"We now consider a normalizing ﬂow for time series. Let X = [x1, x2, . . . , xT ] denote a time series
of length T, where xt ∈RD. Through successive conditioning, the density of the time series can be
written as:
p(X) = p(x1)p(x2|x<2) · · · p(xT |x<T ),
(4)
where x<t denotes all variables before time t. When the conditional probabilities are parameterized,
Rasul et al. (2021) propose to model each p(xt|x<t) as p(xt|ht−1), where ht−1 summarizes the"
NORMALIZING FLOWS,0.12083333333333333,Published as a conference paper at ICLR 2022
NORMALIZING FLOWS,0.125,"past information x<t. For example, ht−1 is the hidden state of a recurrent neural network before
accepting input xt. Then, a conditional normalizing ﬂow can be applied to evaluate each p(xt|ht−1)."
BAYESIAN NETWORKS,0.12916666666666668,"3.2
BAYESIAN NETWORKS"
BAYESIAN NETWORKS,0.13333333333333333,"Let Xi denote a general random variable, either scalar valued, vector valued, or even matrix valued.
A Bayesian network of n variables (X1, . . . , Xn) is a directed acyclic graph of the variables as
nodes. Let A denote the weighted adjacency matrix of the graph, where Aij ̸= 0 if Xj is the parent
of Xi. A Bayesian network describes the conditional independence among variables. Speciﬁcally,
a node Xi is conditionally independent of its non-descendents given its parents. In other words, the
density of the joint distribution of (X1, . . . , Xn) is"
BAYESIAN NETWORKS,0.1375,"p(X1, . . . , Xn) = n
Y"
BAYESIAN NETWORKS,0.14166666666666666,"i=1
p(Xi| pa(Xi)),
(5)"
BAYESIAN NETWORKS,0.14583333333333334,where pa(Xi) = {Xj : Aij ̸= 0} denotes the set of parents of Xi.
PROBLEM STATEMENT,0.15,"4
PROBLEM STATEMENT"
PROBLEM STATEMENT,0.15416666666666667,"In this paper, we focus on unsupervised anomaly detection with multiple time series. The training set
D consists of only unlabeled instances and we assume that the majority of them are not anomalies.
Each instance X ∈D contains n constituent series with D attributes and of length T; i.e., X =
(X1, X2, . . . , Xn) where Xi ∈RT ×D. We use a Bayesian network (DAG) to model the relational
structure of the constituent series Xi and augment a normalizing ﬂow to compute the density of X
through a factorization in the form (5). Let A ∈Rn×n be the adjacency matrix of the DAG and let
F : (X, A) →Z denote the augmented ﬂow. Because anomaly points tend to have low densities,
we propose to conduct unsupervised anomaly detection by evaluating the density of a multiple time
series computed through the augmented ﬂow. The problem is formulated as the following."
PROBLEM STATEMENT,0.15833333333333333,"Problem 1. Given a training set D = {Xi}|D|
i=1 of multiple time series, we aim to simultaneously
learn the adjacency matrix A of the Bayesian Network that represents the conditional dependencies
among the constituent series, as well as the correspondingly graph-augmented normalizing ﬂow
F : (X, A) →Z, which is used to estimate the density of an instance X. Here, Z is a random
variable with a “simple” distribution, such as the anisotropic Gaussian."
METHOD,0.1625,"5
METHOD"
METHOD,0.16666666666666666,"In this section, we materialize the graph-augmented normalizing ﬂow F : (X, A) →Z introduced
in the problem statement and use it to compute the density of a multiple time series X. The central
idea is factorization: we factorize p(X) along the series dimension by using a Bayesian network
and then factorize along the temporal dimension by using conditional normalizing ﬂows. Then,
we employ a novel graph-based dependency encoder to parameterize the conditional probabilities
resulting from the factorization. The DAG used for factorization is a discrete object and is usually
intractable to learn; however, the discrete structure is reﬂected in the dependency encoder through a
graph adjacency matrix A that is differentiable. Moreover, the requirement that A must correspond
to a DAG can be expressed as a differentiable equation. Hence, one can jointly optimize A and
the ﬂow components by using gradient based optimization. Once F is learned, the density p(X) is
straightforwardly evaluated for anomaly detection. An illustration of the framework GANF is shown
in Figure 1."
FACTORIZATION,0.17083333333333334,"5.1
FACTORIZATION"
FACTORIZATION,0.175,"Figure 1 shows a toy example of a Bayesian network as a DAG. Based on (5), the density of a
multiple time series X = (X1, X2, . . . , Xn) can be computed as the product of p(Xi| pa(Xi)) for
all nodes, where recall that pa(Xi) denotes the set of parents of Xi. Then, following Rasul et al.
(2021), we further factorize each conditional density along the temporal dimension. Speciﬁcally, for
a time step t, xi
t depends on its past history as well as its parents in the DAG. We write"
FACTORIZATION,0.17916666666666667,"p(X) = n
Y"
FACTORIZATION,0.18333333333333332,"i=1
p(Xi| pa(Xi)) = n
Y i=1 T
Y"
FACTORIZATION,0.1875,"t=1
p(xi
t | pa(xi)1:t, xi
1:t−1),
(6)"
FACTORIZATION,0.19166666666666668,Published as a conference paper at ICLR 2022
FACTORIZATION,0.19583333333333333,Figure 1: Illustration of a Bayesian network and the proposed framework GANF.
FACTORIZATION,0.2,"where xi
1:t−1 denotes the history of node i before time t, and pa(xi)1:t = {xj
1:t : Aij ̸= 0}. In the
next subsection, we will parameterize each conditional density p(xi
t | pa(xi)1:t, xi
1:t−1) by using
a graph-based dependency encoder. Note that so far the factorization (6) has been based on the
discrete structure of the Bayesian network. The dependency encoder we introduce next, however,
uses the adjacency matrix A in a differentiable manner, which is sufﬁcient to ensure that xi
t will not
depend on nodes other than its parents and itself."
NEURAL NETWORK PARAMETERIZATION,0.20416666666666666,"5.2
NEURAL NETWORK PARAMETERIZATION
According to Sec. 3.1, conditional densities p(xi
t | pa(xi)1:t, xi
1:t−1) can be learned by using con-
ditional normalizing ﬂows. However, the conditional information pa(xi)1:t and xi
1:t−1 cannot be
directly used for parameterization, because its size is not ﬁxed. Therefore, as is illustrated in Fig-
ure 1, we design a graph-based dependency encoder to summarize the conditional information into
a ﬁxed length vector di
t ∈Rd. Then, a conditional normalizing ﬂow is used to evaluate p(xi
t|di
t),
which is equivalent to p(xi
t| pa(xi)1:t, xi
1:t−1)."
NEURAL NETWORK PARAMETERIZATION,0.20833333333333334,"Dependency Encoder. Since the history has an arbitrary length, we ﬁrst employ a recurrent neural
network (RNN) to map multiple time steps to a vector of ﬁxed length. For a time series xi
1:t, the
recurrent model abstracts it into a hidden state hi
t ∈Rd through the following recurrence"
NEURAL NETWORK PARAMETERIZATION,0.2125,"hi
t = RNN(xi
t, hi
t−1),
(7)"
NEURAL NETWORK PARAMETERIZATION,0.21666666666666667,"where hi
t summarizes the time series up to step t. The RNN can be any sequential model, such as
the LSTM (Hochreiter & Schmidhuber, 1997) and in a broad sense a transformer (Vaswani et al.,
2017). We let the RNN parameters be shared across all nodes in the DAG to avoid overﬁtting and to
reduce computational costs."
NEURAL NETWORK PARAMETERIZATION,0.22083333333333333,"With (7), the conditional information of xi
t is all summarized in {hj
t : Aij ̸= 0} ∪{hi
t−1}. Inspired
by the success of GCN (Kipf & Welling, 2016) in node representation learning through neighbor-
hood aggregation, we design a graph convolution layer to aggregate hidden states of the parents for
dependency encoding. This layer produces dependency representations Dt = (d1
t, . . . , dn
t ) for all
constituent series at time t:"
NEURAL NETWORK PARAMETERIZATION,0.225,"Dt = ReLU(AHtW1 + Ht−1W2) · W3,
(8)"
NEURAL NETWORK PARAMETERIZATION,0.22916666666666666,"where Ht = (h1
t, . . . , hn
t ) contains all the hidden states at time t.
Here, W1 ∈Rd×d and
W2 ∈Rd×d are parameters to transform the aggregated representations of the parents and the
node’s historical information, respectively; while W3 ∈Rd×d is an additional transformation to
improve the dependency representation."
NEURAL NETWORK PARAMETERIZATION,0.23333333333333334,"Density Estimation. With the dependency encoder, we obtain the representations di
t of the condi-
tional information. Then, a normalizing ﬂow f : RD × Rd →RD conditioned on di
t is applied to
model each p(xi
t| pa(xi)1:t, xi
1:t−1). Similar to the computation of the hidden states, the param-
eters of the conditional ﬂow are also shared among nodes, to avoid overﬁtting. Based on (3), the
conditional density of xi
t can be written as:"
NEURAL NETWORK PARAMETERIZATION,0.2375,"log p(xi
t | pa(xi)1:t, xi
1:t−1) = log p(xi
t | di
t) = log q(f(xi
t; di
t)) + log | det ∇xi
tf(xi
t; di
t)|,
(9)"
NEURAL NETWORK PARAMETERIZATION,0.24166666666666667,"where q(z) is chosen to be the standard normal N(z|0, I) with z ∈RD. The conditional ﬂow
f can be any effective one proposed by the literature, such as RealNVP (Dinh et al., 2016) and"
NEURAL NETWORK PARAMETERIZATION,0.24583333333333332,Published as a conference paper at ICLR 2022
NEURAL NETWORK PARAMETERIZATION,0.25,"MAF (Papamakarios et al., 2017). Combining (9) and (6), we obtain the log-density of a multiple
time series X:"
NEURAL NETWORK PARAMETERIZATION,0.25416666666666665,"log p(X) = n
X i=1 T
X t=1"
NEURAL NETWORK PARAMETERIZATION,0.25833333333333336,"h
log q(f(xi
t; di
t)) + log | det ∇xi
tf(xi
t; di
t)|
i
.
(10)"
NEURAL NETWORK PARAMETERIZATION,0.2625,"Anomaly Measure. Because anomalies deviate signiﬁcantly from the majority of the data instances,
we hypothesize that their densities are low. Thus, we use the density computed by (10) as the
anomaly measure, where a lower density indicates a more likely anomaly. Apart from evaluating the
density for the entire X, the computation also produces conditional densities log p(Xi| pa(Xi)) =
PT
t=1 log p(xi
t|di
t) for each constituent series Xi. We use this conditional density as the anomaly
measure for constituent series. A low density p(X) is caused by one or a few low conditional densi-
ties p(Xi| pa(Xi)) in the Bayesian network, suggesting that abnormal behaviors could be traced to
individual series."
JOINT TRAINING,0.26666666666666666,"5.3
JOINT TRAINING
Learning a Bayesian network is a challenging combinatorial problem, due to the intractable search
space superexponential in the number of nodes. A recent work by Zheng et al. (2018) proposes the
equation tr(eA◦A) = n that characterizes the acyclicity of the corresponding graph of A, where e
is matrix exponential and ◦denotes element-wise multiplication. We will impose this equation as a
constraint in the training of GANF."
JOINT TRAINING,0.2708333333333333,"Training Objective. Following the training of a usual normalizing ﬂow, the joint density (likeli-
hood) of the observed data is the training objective, which is equivalent to the Kullback–Leibler
divergence between the true distribution of data and the ﬂow recovered distribution. Together with
the DAG constraint, the optimization problem reads"
JOINT TRAINING,0.275,"min
A,θ
L(A, θ) =
1
|D| |D|
X"
JOINT TRAINING,0.2791666666666667,"i=1
−log p(Xi),"
JOINT TRAINING,0.2833333333333333,"s.t.
h(A) = tr(eA◦A) −n = 0, (11)"
JOINT TRAINING,0.2875,"where θ contains all neural network parameters, including those of the dependency encoder and the
normalizing ﬂow. Here, the DAG constraint h(A) admits an easy-to-evaluate gradient ∇h(A) =
(eA◦A)T ◦2A, which allows a gradient based optimizer to solve (11)."
JOINT TRAINING,0.2916666666666667,"Training Algorithm. Problem (11) is a nonlinear equality-constrained optimization. Such problems
are extensively studied and the augmented Lagrangian method (Bertsekas, 1999; Yu et al., 2019) is
one of the most widely used approaches. The augmented Lagrangian is deﬁned as"
JOINT TRAINING,0.29583333333333334,"Lc = L(A, θ) + λh(A) + c"
JOINT TRAINING,0.3,"2|h(A)|2,
(12)"
JOINT TRAINING,0.30416666666666664,"where λ and c denote the Lagrange multiplier and the penalty parameter, respectively. The general
idea of the method is to gradually increase the penalty parameter to ensure that the constraint is
eventually satisﬁed. Over iterations, λ as a dual variable will converge to the Lagrangian multiplier
of (11). The update rule at the kth iteration reads the following:"
JOINT TRAINING,0.30833333333333335,"Ak, θk = arg min
A,θ Lck;
λk+1 = λk + ckh(Ak);
ck+1 =

ηck
if |h(Ak)| > γ|h(Ak−1)| ;
ck
else,
(13)
where η ∈(1, +∞) and γ ∈(0, 1) are hyperparameters to be tuned. We set η and γ as 10 and
0.5, respectively. The subproblem of optimizing A and θ can be solved by using the Adam opti-
mizer (Kingma & Ba, 2014). The training algorithm is summarized in Appendix A."
EXPERIMENTS,0.3125,"6
EXPERIMENTS"
EXPERIMENTS,0.31666666666666665,"In this section, we conduct a comprehensive set of experiments to validate the effectiveness of the
proposed GANF framework. In particular, they are designed to answer the following questions:"
EXPERIMENTS,0.32083333333333336,• Q1: Can GANF accurately detect anomalies and estimate densities?
EXPERIMENTS,0.325,"• Q2: Does the proposed graph structure learning help? Is the framework sufﬁciently ﬂexible to
include various normalizing ﬂow backbones?"
EXPERIMENTS,0.32916666666666666,"• Q3: What can one observe for a dataset spanning a long time? E.g., does the graph pattern change?"
EXPERIMENTS,0.3333333333333333,Published as a conference paper at ICLR 2022
SETTINGS,0.3375,"6.1
SETTINGS"
SETTINGS,0.3416666666666667,"Datasets. To evaluate the effectiveness of GANF for anomaly detection and density estimation, we
conduct experiments on two power grid datasets, one water system dataset, and one trafﬁc dataset."
SETTINGS,0.3458333333333333,"• PMU-B and PMU-C: These two datasets correspond to two separate interconnects of the U.S.
power grid, containing time series recorded by 38 and 132 phasor measurement units (PMUs),
respectively. We process one-year data at the frequency of one second to form a ten-month training
set, one-month validation set, and one-month test set. Each multiple time series is obtained by
shifting a one-minute window. Additionally, to investigate distribution drift, we shift a one-month
window to obtain multiple training/validation/test sets (12 in total, because of availability of two-
year data). Sparse grid events (anomalies) labeled by domain experts exist for evaluation; but note
that the labels are both noisy and incomplete. These datasets are proprietary."
SETTINGS,0.35,"• SWaT: We also use a public dataset for evaluation. The Secure Water Treatment (SWaT) dataset
originates from an operational water treatment test-bed coordinated with Singapore’s Public Util-
ity Board (Goh et al., 2016). The data collects 51 sensor recordings lasting four days, at the
frequency of one second. A total of 36 attacks were conducted, resulting in approximately 11%
time steps as anomaly ground truths. We use a sliding window of 60 seconds to construct series
data and perform a 60/20/20 chronological split for training, validation, and testing, respectively."
SETTINGS,0.3541666666666667,"• METR-LA: This dataset is also public; it contains speed records of 207 sensors deployed on the
highways of Los Angles, CA (Li et al., 2018b). No anomaly labels exist however and we use this
dataset for exploratory analysis only. Results are deferred to Appendix E."
SETTINGS,0.35833333333333334,"Evaluation metrics (under noisy labels). For SWaT, which offers reliable ground truths, we use the
standard ROC and AUC metrics for evaluation. For the two PMU datasets, however, the resolution
of the time series and the granularity of the events result in rather noisy ground truths. Hence, we
adapt ROC for noisy labels. We smooth the time point of a “ground truth” event (anomaly) by
introducing probabilities to the label. Speciﬁcally, the probability that a multiple time series starting
at time t is a ground truth anomaly is maxi{exp(−(t−ti)2"
SETTINGS,0.3625,"σ2
)}, where ti is the starting time of the
ith labeled anomaly. Then, when computing the confusion matrix, we sum probabilities rather than
counting 0/1s. The smoothing window σ is chosen to be 6 time steps."
SETTINGS,0.36666666666666664,"Baselines. We compare with the following representative, state-of-the-art deep methods."
SETTINGS,0.37083333333333335,"• EncDecAD (Malhotra et al., 2016): In this method, an autoencoder based on LSTM is trained.
The reconstruction error is used as the anomaly measure."
SETTINGS,0.375,"• DeepSVDD (Ruff et al., 2018): This method minimizes the volume of a hypersphere that encloses
the representations of data. Samples distant from the hypersphere center are considered anomalies."
SETTINGS,0.37916666666666665,"• ALOCC (Sabokrou et al., 2020): In this GAN-based method, the generator learns to reconstruct
normal instances, while the discriminator works as an anomaly detector."
SETTINGS,0.38333333333333336,"• DROCC (Goyal et al., 2020): This method performs adversarial training to learn robust represen-
tations of data and identiﬁes anomalies."
SETTINGS,0.3875,"• DeepSAD (Ruff et al., 2020): This method extends DeepSVDD with a semi-supervised loss term
for training. We use noisy labels as supervision."
SETTINGS,0.39166666666666666,"To apply these baselines on multiple time series, we concatenate the constituent series along the
attribute dimension (resulting in high-dimensional series) and use LSTM or CNN as the backbones.
On the other hand, for the proposed method, we use LSTM as the RNN model and MAF as the
normalizing ﬂow. See Appendix C for more implementation details."
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.3958333333333333,"6.2
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4,Table 1: AUC-ROC (%) of anomaly detection.
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4041666666666667,"Dataset
EncDecAD
DeepSVDD
ALOCC
DROCC
DeepSAD
GANF"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4083333333333333,"PMU-B
55.6±1.8
55.6±3.3
62.9±2.2
58.6±3.0
63.7±0.9
67.5±0.8"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4125,"PMU-C
53.7±0.5
56.9±0.9
60.9±1.3
61.9±2.7
60.1±1.4
70.6±3.3"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4166666666666667,"SWaT
76.5±0.7
68.8±2.0
75.4±2.3
73.3±1.6
75.4±1.2
79.6±0.9"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.42083333333333334,Published as a conference paper at ICLR 2022
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.425,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.42916666666666664,True Positive Rate
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.43333333333333335,"GANF
DeepSAD
DROCC
ALOCC
DeepSVDD
EncDecAD"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4375,(a) PMU-B
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.44166666666666665,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.44583333333333336,True Positive Rate
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.45,"GANF
DeepSAD
DROCC
ALOCC
DeepSVDD
EncDecAD"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.45416666666666666,(b) PMU-C
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4583333333333333,"0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4625,True Positive Rate
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4666666666666667,"GANF
DeepSAD
DROCC
ALOCC
DeepSVDD
EncDecAD"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4708333333333333,"(c) SWaT
Figure 2: ROC curves of anomaly detection on various datasets."
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.475,"14
15
16
17
18
Anormaly Measure = log(density) 100 101 102 103 104 Count (a)"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4791666666666667,"11-03
11-05
11-07
11-09
Time 15 16 17 18"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.48333333333333334,Anormaly Measure
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.4875,"Labeled Anomaly
Score of Timestamp"
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.49166666666666664,"(b)
Figure 3: Qualitative evaluation of GANF on PMU-C. (a) Distribution of log-densities on the test
set (note in log scale). (b) Anomaly detection results for a week in the test set."
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.49583333333333335,"To answer Q1, we evaluate quantitatively and qualitatively on datasets with labels."
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.5,"Anomaly detection. We compare GANF with the aforementioned baselines in Table 1, where
standard deviations of the AUC scores are additionally reported through ﬁve random repetitions of
model training. The table suggests an overwhelmingly high AUC score achieved by GANF. Obser-
vations follow. (i) GANF outperforms generative model-based methods (EncDecAD and ALOCC).
Being a generative model as well, normalizing ﬂows augmented with a graph structure leverage
the interdependencies of constituent series more effectively, leading to a substantial improvement in
detection. (ii) GANF signiﬁcantly outperforms deep one-class models (DeepSVDD and DROCC),
corroborating the appeal of using densities for detection. (iii) GANF also performs better than the
semi-supervised method DeepSAD, probably because such methods rely on high quality labels for
supervision (especially in the case of label scarcity) and they are less effective facing noisy labels."
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.5041666666666667,"Besides a single score, we also plot the ROC curve in Figure 2. One sees that the curve of GANF
dominates those of others. This behavior is generally more salient in the low false-alarm regime."
PERFORMANCE OF ANOMALY DETECTION AND DENSITY ESTIMATION,0.5083333333333333,"Density estimation. We investigate the densities estimated by GANF, shown in Figure 3. Distribu-
tions of the log-densities in the test set are shown in Figure 3a. We use log-density as the anomaly
measure; the lower the more likely. Note that the vertical axis is in the log-scale. One sees that a
log-density of 16 approximately separates the majority normal instances from the minority anoma-
lies. To cross-verify that the instances with low densities are suspiciously anomalous, we investigate
Figure 3b, which is a temporal plot of log-densities for a week, overlaid with given labels. From this
plot, one sees that the noisily labeled series generally have low densities or are near a low density
time step. Additionally, GANF discovers a few suspicious time steps with low densities undetected
earlier. These new discoveries raise interest to power system experts for analysis and archiving."
ABLATION STUDY,0.5125,"6.3
ABLATION STUDY"
ABLATION STUDY,0.5166666666666667,Table 2: Performance of variants of the proposed method.
ABLATION STUDY,0.5208333333333334,"Dataset
Metrics
GANF\G
GANF\D
GANF\T
GANFRNVP
GANF"
ABLATION STUDY,0.525,"PMU-B
AUC-ROC
0.641
0.643
0.653
0.661
0.678
Log-Density
15.31
8.70
15.09
15.90
16.22"
ABLATION STUDY,0.5291666666666667,"PMU-C
AUC-ROC
0.630
0.544
0.688
0.703
0.705
Log-Density
15.55
8.94
15.70
17.06
16.98"
ABLATION STUDY,0.5333333333333333,Published as a conference paper at ICLR 2022
ABLATION STUDY,0.5375,"(a) Initial graph
(b) One-month shift
(c) Two-month shift
(d) Three-month shift
Figure 4: Evolution of the learned DAG on PMU-B over time."
ABLATION STUDY,0.5416666666666666,Figure 5: Evolution of edge weights in the DAG learned by GANF over time (PMU-B).
ABLATION STUDY,0.5458333333333333,"To answer Q2, we conduct an ablation study (including varying architecture components) to in-
vestigate impacts of DAG structure learning and the ﬂexibility of the GANF framework. To in-
vestigate the power of modeling pairwise relationship, we train a variant GANF\G that factorizes
p(X) = Qn
i=1 p(Xi); i.e., assuming independence among constituent series. To investigate the
effectiveness of graph structure learning, we train a variant GANF\D that decomposes the joint
density as p(X) = Qn
i=1 p(Xi|X<i); i.e., a full decomposition without a DAG. It is equivalent
to concatenating the series along the attribute dimension and running MAF on the resulting series.
To verify the contribution of joint training of A and θ, we train a variant GANF\T where A is
separately learned by using NOTEARS (Zheng et al., 2018). To prove the ﬂexibility of GANF, we
replace the MAF-based normalizing ﬂow by RealNVP, denoted as GANFRNVP."
ABLATION STUDY,0.55,"Results are presented in Table 2. Apart from AUC-ROC, the log-density is also reported. Ob-
servations follow. (i) GANF signiﬁcantly outperforms GANF\G and GANF\D, corroborating the
importance of interdependency modeling among constituent series. Note that GANF\D results in
particularly poor performance in general, likely because the high dimensional input (resulting from
concatenating too many series) impedes the learning of normalizing ﬂows. (iii) GANF\T is slightly
better than GANF\G, because of the presence of relational modeling, but it cannot match the per-
formance of GANFRNVP and GANF that jointly train the DAG and the ﬂow. (ii) These latter two
models are the best for both datasets and both metrics. MAF works more often better than RealNVP."
EVOLUTION OF THE DAG STRUCTURE,0.5541666666666667,"6.4
EVOLUTION OF THE DAG STRUCTURE
To answer Q3, we investigate how the learned DAG evolves by shifting the train/validation/test sets
month by month. The graphs within the ﬁrst three-month shiftings are shown in Figure 4 and more
can be found in Appendix F. In addition to the graph structure, we plot in Figure 5 the learned edge
weights over time, one column per edge. The appearance and disappearance of edges demonstrate
changes of the conditional independence structure among constituent series over time, suggesting
data distribution drift (i.e., a change of internal data generation mechanism). It is interesting to
observe the seasonal effect. The columns (edges) in Figure 5 can be loosely grouped in three clusters:
those persisting the entire year, those appearing in the ﬁrst half of the year, and those existing more
brieﬂy (e.g., within a season). Such a pattern plausibly correlates with electricity consumption,
which is also seasonal. Were spatial information of the PMUs known, these identiﬁed DAGs would
help mapping the seasonal patterns to geography and help planning a more resilient grid."
CONCLUSIONS,0.5583333333333333,"7
CONCLUSIONS"
CONCLUSIONS,0.5625,"In this paper, we present a graph-augmented normalizing ﬂow GANF for anomaly detection of
multiple time series. The graph is materialized as a Bayesian network, which models the conditional
dependencies among constituent time series. A graph-based dependency decoder is designed to
summarize the conditional information needed by the normalizing ﬂow that calculates series density.
Anomalies are detected through identifying instances with low density. Extensive experiments on
real-world datasets demonstrate the effectiveness of the framework. Ablation studies conﬁrm the
contribution of the learned graph structure in anomaly detection. Additionally, we investigate the
evolution of the graph and offer insights of distribution drift over time."
CONCLUSIONS,0.5666666666666667,Published as a conference paper at ICLR 2022
CONCLUSIONS,0.5708333333333333,"ACKNOWLEDGMENT AND DISCLAIMER
This material is based upon work supported by the Department of Energy under Award Number(s)
DE-OE0000910. This report was prepared as an account of work sponsored by an agency of the
United States Government. Neither the United States Government nor any agency thereof, nor any
of their employees, makes any warranty, express or implied, or assumes any legal liability or re-
sponsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or
process disclosed, or represents that its use would not infringe privately owned rights. Reference
herein to any speciﬁc commercial product, process, or service by trade name, trademark, manufac-
turer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or
favoring by the United States Government or any agency thereof. The views and opinions of authors
expressed herein do not necessarily state or reﬂect those of the United States Government or any
agency thereof."
REFERENCES,0.575,REFERENCES
REFERENCES,0.5791666666666667,"Terje Aven. Risk assessment and risk management: Review of recent advances on their foundation.
European Journal of Operational Research, 253(1):1–13, 2016."
REFERENCES,0.5833333333333334,"Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 2nd edition, 1999."
REFERENCES,0.5875,"Ailin Deng and Bryan Hooi. Graph neural network-based anomaly detection in multivariate time
series. In AAAI, 2021."
REFERENCES,0.5916666666666667,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016."
REFERENCES,0.5958333333333333,"Jonathan Goh, Sridhar Adepu, Khurum Nazir Junejo, and Aditya Mathur. A dataset to support
research in the design of secure water treatment systems. In International conference on critical
information infrastructures security, pp. 88–99. Springer, 2016."
REFERENCES,0.6,"Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. arXiv
preprint arXiv:1805.10917, 2018."
REFERENCES,0.6041666666666666,"Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, and Prateek Jain. Drocc:
Deep robust one-class classiﬁcation.
In International Conference on Machine Learning, pp.
3711–3721. PMLR, 2020."
REFERENCES,0.6083333333333333,"Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017."
REFERENCES,0.6125,"Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning
can improve model robustness and uncertainty. arXiv preprint arXiv:1906.12340, 2019."
REFERENCES,0.6166666666666667,"Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997."
REFERENCES,0.6208333333333333,"Emiel Hoogeboom, Rianne Van Den Berg, and Max Welling. Emerging convolutions for generative
normalizing ﬂows. In International Conference on Machine Learning, pp. 2771–2780. PMLR,
2019."
REFERENCES,0.625,"Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, and Ling Shao. Object-
centric auto-encoders and dummy anomalies for abnormal event detection in video. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7842–7851,
2019."
REFERENCES,0.6291666666666667,"JooSeuk Kim and Clayton D Scott. Robust kernel density estimation. The Journal of Machine
Learning Research, 13(1):2529–2565, 2012."
REFERENCES,0.6333333333333333,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.6375,"Diederik P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018."
REFERENCES,0.6416666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.6458333333333334,"Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In ICML, 2018."
REFERENCES,0.65,"Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016."
REFERENCES,0.6541666666666667,"B Ravi Kiran, Dilip Mathew Thomas, and Ranjith Parakkal. An overview of deep learning based
methods for unsupervised and semi-supervised anomaly detection in videos. Journal of Imaging,
4(2):36, 2018."
REFERENCES,0.6583333333333333,"Dan Li, Dacheng Chen, Jonathan Goh, and See-kiong Ng.
Anomaly detection with generative
adversarial networks for multivariate time series. arXiv preprint arXiv:1809.04758, 2018a."
REFERENCES,0.6625,"Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven trafﬁc forecasting. In ICLR, 2018b."
REFERENCES,0.6666666666666666,"Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing ﬂows.
arXiv preprint arXiv:1905.13177, 2019."
REFERENCES,0.6708333333333333,"Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig, Puneet Agarwal, and
Gautam Shroff. Lstm-based encoder-decoder for multi-sensor anomaly detection. arXiv preprint
arXiv:1607.00148, 2016."
REFERENCES,0.675,"Duc Tam Nguyen, Zhongyu Lou, Michael Klar, and Thomas Brox.
Anomaly detection with
multiple-hypotheses predictions. In International Conference on Machine Learning, pp. 4800–
4809. PMLR, 2019."
REFERENCES,0.6791666666666667,"George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive ﬂow for density
estimation. arXiv preprint arXiv:1705.07057, 2017."
REFERENCES,0.6833333333333333,"Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065–1076, 1962."
REFERENCES,0.6875,"Judea Pearl. Bayesian networks: A model of self-activated memory for evidential reasoning. In
Proceedings of the 7th Conference of the Cognitive Science Society, 1985."
REFERENCES,0.6916666666666667,"Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000."
REFERENCES,0.6958333333333333,"Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty
detection. Signal Processing, 99:215–249, 2014."
REFERENCES,0.7,"Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland Vollgraf. Multi-
variate probabilistic time series forecasting via conditioned normalizing ﬂows. In ICLR, 2021."
REFERENCES,0.7041666666666667,"Riya Roy and K Thomas George. Detecting insurance claims fraud using machine learning tech-
niques. In 2017 International Conference on Circuit, Power and Computing Technologies (IC-
CPCT), pp. 1–6. IEEE, 2017."
REFERENCES,0.7083333333333334,"Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexan-
der Binder, Emmanuel M¨uller, and Marius Kloft. Deep one-class classiﬁcation. In International
conference on machine learning, pp. 4393–4402. PMLR, 2018."
REFERENCES,0.7125,"Lukas Ruff, Robert A. Vandermeulen, Nico G¨ornitz, Alexander Binder, Emmanuel M¨uller, Klaus-
Robert M¨uller, and Marius Kloft. Deep semi-supervised anomaly detection. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=HkgH0TEYwH."
REFERENCES,0.7166666666666667,"Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially
learned one-class classiﬁer for novelty detection. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3379–3388, 2018."
REFERENCES,0.7208333333333333,"Mohammad Sabokrou, Mahmood Fathy, Guoying Zhao, and Ehsan Adeli. Deep end-to-end one-
class classiﬁer.
IEEE transactions on neural networks and learning systems, 32(2):675–684,
2020."
REFERENCES,0.725,Published as a conference paper at ICLR 2022
REFERENCES,0.7291666666666666,"Bernhard Sch¨olkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.
Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443–1471,
2001."
REFERENCES,0.7333333333333333,"Youngjoo Seo, Micha¨el Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence
modeling with graph convolutional recurrent networks. arXiv:1612.07659, 2016."
REFERENCES,0.7375,"Chao Shang, Jie Chen, and Jinbo Bi. Discrete graph structure learning for forecasting multiple time
series. In ICLR, 2021."
REFERENCES,0.7416666666666667,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017."
REFERENCES,0.7458333333333333,"Peng Wu, Jing Liu, and Fang Shen. A deep one-class neural network for anomalous event detection
in complex scenes. IEEE transactions on neural networks and learning systems, 31(7):2609–
2622, 2019."
REFERENCES,0.75,"Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Con-
necting the dots: Multivariate time series forecasting with graph neural networks. In KDD, 2020."
REFERENCES,0.7541666666666667,"Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A deep
learning framework for trafﬁc forecasting. In IJCAI, 2018."
REFERENCES,0.7583333333333333,"Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural
networks. In ICML, 2019."
REFERENCES,0.7625,"Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, and Haifeng Li.
T-GCN: A temporal graph convolutional network for trafﬁc prediction. IEEE Transactions on
Intelligent Transportation Systems, 2019."
REFERENCES,0.7666666666666667,"Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. DAGs with NO TEARS: Con-
tinuous optimization for structure learning. In NeurIPS, 2018."
REFERENCES,0.7708333333333334,Published as a conference paper at ICLR 2022
REFERENCES,0.775,"A
TRAINING ALGORITHM"
REFERENCES,0.7791666666666667,We summarize the training method in Algorithm 1.
REFERENCES,0.7833333333333333,Algorithm 1 Training Algorithm of GANF
REFERENCES,0.7875,"Input: Training set D, hyperparameters η and γ
Output: GANF F and adjacency matrix A of the DAG"
REFERENCES,0.7916666666666666,"1: Initialize c ←0 and initialize λ randomly
2: for k = 0, 1, 2, . . . do
3:
Compute Ak and θk as a minimizer of (12) by using the Adam optimizer, where the loss
L and the constraint h are deﬁned in (11), the log-density log p(X) is deﬁned in (10), the
dependency representation di
t is deﬁned in (8), the hidden state hi
t is deﬁned in (7), and the
conditional ﬂow f is RealNVP or MAF
4:
Update Lagrange multiplier λ ←λ + ch(Ak)
5:
if k > 0 and |h(Ak)| > γ|h(Ak−1)| then
6:
c ←ηc
7:
end if
8:
if h(Ak) == 0 then
9:
break
10:
end if
11: end for
12: return A and F (including f, the neural network (8), and the RNN (7))"
REFERENCES,0.7958333333333333,"B
CODE"
REFERENCES,0.8,Code is available at https://github.com/EnyanDai/GANF.
REFERENCES,0.8041666666666667,"C
ADDITIONAL DETAILS OF EXPERIMENT SETTINGS"
REFERENCES,0.8083333333333333,"C.1
IMPLEMENTATION DETAILS OF GANF.
An LSTM is used as the RNN model in the dependency encoder. For normalizing ﬂows, we use
MAF with six ﬂow blocks. All hidden dimensions as set as 32. The initial learning rate is set as
0.001 for the adjacency matrix A and the model parameters θ. Learning rate decay is 0.1. To avoid
gradient explosion, we clip the gradients whose values are larger than 1.0."
REFERENCES,0.8125,"For hyperparameter tuning, we select the hyperparameters that yield the highest log-density on the
validation set. Speciﬁcally, we conduct grid search by varying the number of normalizing ﬂow
blocks from {1, 2, 4, 6, 8}, the learning rate from {0.003, 0.001, 0.0003, 0.0001}, and the hidden
dimension from {16, 32, 64, 128}."
REFERENCES,0.8166666666666667,"C.2
IMPLEMENTATION DETAILS OF BASELINES."
REFERENCES,0.8208333333333333,"• EncDecAD (Malhotra et al., 2016):
This method is applied for anomaly detection on
time series.
Thus, we concatenate constituent series along the attribute dimension and
adopt the code released by the authors in https://github.com/chickenbestlover/
RNN-Time-series-Anomaly-Detection."
REFERENCES,0.825,"• DeepSVDD (Ruff et al., 2018):
To handle time series data, we replace the backbone
to an LSTM based on the ofﬁcial implementation https://github.com/lukasruff/
Deep-SVDD-PyTorch."
REFERENCES,0.8291666666666667,"• ALOCC (Sabokrou et al., 2020): We use the ofﬁcial implementation released by the authors in
https://github.com/khalooei/ALOCC-CVPR2018. We replace the two-dimensional
convolution to one-dimensional convolution to build a GAN for time series data."
REFERENCES,0.8333333333333334,"• DROCC (Goyal et al., 2020): Similar to other baselines, this method is proposed for tabular data
and image data. We replace the backbone to LSTM to deal with multiple time series by revising the
encoder in https://github.com/microsoft/EdgeML/tree/master/pytorch."
REFERENCES,0.8375,"• DeepSAD (Ruff et al., 2020): This is a semi-supervised approach, which requires labeling.
We utilize the noisy labels in PMU-B and PMU-C as supervision.
We use LSTM as the"
REFERENCES,0.8416666666666667,Published as a conference paper at ICLR 2022
REFERENCES,0.8458333333333333,"backbone, based on the ofﬁcial implementation in https://github.com/lukasruff/
Deep-SAD-PyTorch."
REFERENCES,0.85,All hyperparameters of the baselines are tuned based on the validation set to make fair comparisons.
REFERENCES,0.8541666666666666,"D
TIME COMPLEXITY ANALYSIS"
REFERENCES,0.8583333333333333,"The GANF framework involves Bayesian network structure learning, which is known to be highly
challenging, owing to the intractable search space superexponential in the number of graph nodes.
In this work, we formulate a continuous optimization of the graph structure, so that the training of
GANF is more scalable. In what follows, we analyze the time complexity."
REFERENCES,0.8625,"Recall that each instance X of the multiple time series dataset contains n constituent series with
D attributes and of length T; i.e., X = (X1, X2, . . . , Xn) where Xi ∈RT ×D. In the evaluation
of the model, the dominant costs appear in running the dependency encoder and the normalizing
ﬂow. For the dependency encoder, an RNN is ﬁrst deployed to map the multiple time series to
hidden vectors; the time complexity is O(nTD). Then, graph convolution is conducted to obtain
dependency vectors; the convolution cost is O(n2T). For the normalizing ﬂow module, the time
complexity is O(nTD). Therefore, the time complexity of computing log-density of one instance
is O(nT(D + n)). If we use a batch size B for training, the time cost of calculating the augmented
Lagrangian L(A, θ) in (12) is O(nBT(D + n)). Additionally, the time cost of calculating the
constraint h(A) is O(n3). Thus, the overall time complexity of one training iteration is O(n(BTD+
BTn + n2))."
REFERENCES,0.8666666666666667,"E
RESULTS FOR METR-LA"
REFERENCES,0.8708333333333333,"METR-LA contains speed records of 207 sensors deployed on the highways of Los Angles, CA (Li
et al., 2018b). The records are in four months at the frequency of ﬁve minutes. We shift a one-hour
window to obtain multiple time series. The ﬁrst three months are used for training and the last month
is split in halves for validation and testing. No anomaly labels exist however and we use this dataset
for exploratory analysis only."
REFERENCES,0.875,"(a) Average speed of the main roads.
(b) Estimated log-density."
REFERENCES,0.8791666666666667,Figure 6: Density estimation for METR-LA.
REFERENCES,0.8833333333333333,"Figure 6a shows the trafﬁc speed on four main highways on June 13, 2012. Each speed is the
average over all sensors on the same highway. We observe that despite spatial proximity, the speeds
vary signiﬁcantly around 4PM (rush hour) but they are unanimously high around 5AM and 8PM–
12AM. The estimated densities, shown in Figure 6b, tracks this pattern rather closely, with rush
hours corresponding to low density and night trafﬁcs corresponding to high density. Note the nature
of trafﬁc: speed varies smoothly on the macroscopic level and hence does density, too. Such a
phenomenon is in striking contrast to power systems where events are rare and abrupt."
REFERENCES,0.8875,Published as a conference paper at ICLR 2022
REFERENCES,0.8916666666666667,"F
ADDITIONAL ANOMALY DETECTION RESULTS ON PMU DATASETS"
REFERENCES,0.8958333333333334,"See Figure 7 and Figure 8 for additional anomaly detection results on the test sets of PMU-C and
PMU-B, respectively. The observations are rather similar to those of Figure 3b."
REFERENCES,0.9,"2017-11-08
2017-11-10
2017-11-12
2017-11-14
Time 15 16 17 18"
REFERENCES,0.9041666666666667,Anormaly Measure
REFERENCES,0.9083333333333333,"Labeled Anomaly
Score of Timestamp"
REFERENCES,0.9125,"2017-11-15
2017-11-17
2017-11-19
2017-11-21
Time 16 17 18"
REFERENCES,0.9166666666666666,Anormaly Measure
REFERENCES,0.9208333333333333,"Labeled Anomaly
Score of Timestamp"
REFERENCES,0.925,Figure 7: Additional anomaly detection results on the test set of PMU-C.
REFERENCES,0.9291666666666667,"2016-12-03
2016-12-05
2016-12-07
2016-12-09
Time 12 14 16 18"
REFERENCES,0.9333333333333333,Anormaly Measure
REFERENCES,0.9375,"Labeled Anomaly
Score of Timestamp"
REFERENCES,0.9416666666666667,"2016-12-10
2016-12-12
2016-12-14
2016-12-16
Time 12 14 16 18"
REFERENCES,0.9458333333333333,Anormaly Measure
REFERENCES,0.95,"Labeled Anomaly
Score of Timestamp"
REFERENCES,0.9541666666666667,"2016-12-16
2016-12-18
2016-12-20
2016-12-22
Time 12 14 16 18"
REFERENCES,0.9583333333333334,Anormaly Measure
REFERENCES,0.9625,"Labeled Anomaly
Score of Timestamp"
REFERENCES,0.9666666666666667,Figure 8: Anomaly detection results on the test set of PMU-B.
REFERENCES,0.9708333333333333,Published as a conference paper at ICLR 2022
REFERENCES,0.975,"G
ADDITIONAL RESULTS FOR DAG EVOLUTION"
REFERENCES,0.9791666666666666,"(a) Four-month shift
(b) Five-month shift"
REFERENCES,0.9833333333333333,"(c) Six-month shift
(d) Seven-month shift"
REFERENCES,0.9875,"(e) Eight-month shift
(f) Nine-month shift"
REFERENCES,0.9916666666666667,"(g) Ten-month shift
(h) Eleven-month shift"
REFERENCES,0.9958333333333333,Figure 9: Evolution of the learned DAG on PMU-B over time.
