Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0030959752321981426,"Probabilistic classiﬁers output conﬁdence scores along with their predictions, and
these conﬁdence scores should be calibrated, i.e., they should reﬂect the reliability
of the prediction. Conﬁdence scores that minimize standard metrics such as the
expected calibration error (ECE) accurately measure the reliability on average
across the entire population. However, it is in general impossible to measure the
reliability of an individual prediction. In this work, we propose the local calibration
error (LCE) to span the gap between average and individual reliability. For each
individual prediction, the LCE measures the average reliability of a set of similar
predictions, where similarity is quantiﬁed by a kernel function on a pretrained
feature space and by a binning scheme over predicted model conﬁdences. We
show theoretically that the LCE can be estimated sample-efﬁciently from data, and
empirically ﬁnd that it reveals miscalibration modes that are more ﬁne-grained than
the ECE can detect. Our key result is a novel local recalibration method LoRe,
to improve conﬁdence scores for individual predictions and decrease the LCE.
Experimentally, we show that our recalibration method produces more accurate
conﬁdence scores, which improves downstream fairness and decision making on
classiﬁcation tasks with both image and tabular data."
INTRODUCTION,0.006191950464396285,"1
INTRODUCTION"
INTRODUCTION,0.009287925696594427,"Uncertainty estimation is extremely important in high stakes decision-making tasks. For example,
a patient wants to know the probability that a medical diagnosis is correct; an autonomous driving
system wants to know the probability that a pedestrian is correctly identiﬁed. Uncertainty estimates
are usually achieved by predicting a probability along with each classiﬁcation. Ideally, we want to
achieve individual calibration, i.e., we want to predict the probability that each sample is misclassiﬁed."
INTRODUCTION,0.01238390092879257,"However, each sample is observed only once for most datasets (e.g., image classiﬁcation datasets
do not contain identical images), making it impossible to estimate, or even deﬁne, the probability
of incorrect classiﬁcation for individual samples. Because of this, commonly used metrics such as
the expected calibration error (ECE) measure the gap between a classiﬁer’s conﬁdence and accuracy
averaged across the entire dataset. Consequently, ECE can be accurately estimated but does not
measure the reliability of individual predictions."
INTRODUCTION,0.015479876160990712,"In this work, we propose the local calibration error (LCE), a calibration metric that spans the gap
between fully global (e.g., ECE) and fully individual calibration. Motivated by the success of
kernel-based locality in other ﬁelds such as fairness (where similar individuals should be treated
similarly) (Dwork et al., 2012; Pleiss et al., 2017) and causal inference (where matching techniques
are used to ﬁnd similar neighboring samples) (Stuart, 2010), we approximate the probability of
misclassiﬁcation for an individual sample by computing the average classiﬁcation error over similar
samples, where similarity is measured by a kernel function in a pre-trained feature space and a
binning scheme over predicted conﬁdences. Intuitively, two samples are similar if they are close in a
pretrained feature space and have similar predicted conﬁdence scores. By choosing the bandwidth
of the kernel function, we can trade off estimation accuracy and individuality: when the bandwidth
is very large, we recover existing global calibration metrics; when the bandwidth is small, we
approximate individual calibration. We choose an intermediate bandwidth, so our metric can be
accurately estimated, and provides some measurement on the reliability of individual predictions."
INTRODUCTION,0.018575851393188854,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.021671826625386997,"Theoretically, we show that the LCE can be estimated with polynomially many samples if the kernel
function is bounded. Empirically, we also show that for intermediate values of the bandwidth, the
LCE can be accurately estimated and reveals modes of miscalibration that global metrics (such as
ECE) fail to uncover."
INTRODUCTION,0.02476780185758514,"In addition, we introduce a non-parametric, post-hoc localized recalibration method (LoRe), for
lowering the LCE. Empirically, LoRe improves fairness by achieving low calibration error on all
potentially sensitive subsets of the data, such as racial groups. Notably, it can do so without any prior
knowledge of those groups, and is more effective than global methods at this task. In addition, our
recalibration method improves decision making when there is a “safe” action that is selected whenever
the predicted conﬁdence is low. For example, an automated system which classiﬁes tissue samples
as cancerous should request a human expert opinion whenever it is unsure about a classiﬁcation. In
a simulation on an image classiﬁcation dataset, we show that recalibrated prediction models more
accurately choose whether to use the “safe” action, which improves the overall utility."
INTRODUCTION,0.02786377708978328,"In summary, the contributions of our paper are as follows. (1) We introduce a local calibration metric,
the LCE, that is both easy to compute and can estimate the reliability of individual predictions. (2)
We introduce a post-hoc localized recalibration method LoRe, that transforms a model’s conﬁdence
predictions to improve the local calibration. (3) We empirically evaluate LoRe on several downstream
tasks and observe that LoRe improves fairness and decision-making more than existing baselines."
BACKGROUND AND RELATED WORK,0.030959752321981424,"2
BACKGROUND AND RELATED WORK"
GLOBAL CALIBRATION METRICS,0.034055727554179564,"2.1
GLOBAL CALIBRATION METRICS"
GLOBAL CALIBRATION METRICS,0.03715170278637771,"Consider a classiﬁcation task that maps from some input domain (e.g., images) X to a ﬁnite set of
labels Y = {1, · · · , m}. A classiﬁer is a pair (f, ˆp) where f : X →Y maps each input x ∈X
to a label y ∈Y and ˆp : X →[0, 1] maps each input x to a conﬁdence value c. Let Pr be a joint
distribution on X × Y (e.g., from which training or test data pairs (x, y) are drawn). The classiﬁer
(f, ˆp) is perfectly calibrated (Guo et al., 2017) with respect to Pr if for all c ∈[0, 1]"
GLOBAL CALIBRATION METRICS,0.04024767801857585,"Pr[f(X) = Y | ˆp(X) = c] = c.
(1)"
GLOBAL CALIBRATION METRICS,0.043343653250773995,"To numerically measure how well a classiﬁer is calibrated, the most commonly used metric is
the expected calibration error (ECE) (Naeini et al., 2015; Guo et al., 2017), which measures the
average absolute deviation from Eq. 1 over the domain. In practice, given a ﬁnite dataset, the ECE is
approximated by binning. The predicted conﬁdences ˆp are partitioned into bins B1, . . . , Bk, and then
a weighted average is taken of the absolute difference between the average conﬁdence conf(Bi) and
average accuracy acc(Bi) for each bin Bi:"
GLOBAL CALIBRATION METRICS,0.04643962848297214,"ECE(f, ˆp) := k
X i=1 |Bi|"
GLOBAL CALIBRATION METRICS,0.04953560371517028,"N |conf(Bi) −acc(Bi)|.
(2)"
GLOBAL CALIBRATION METRICS,0.05263157894736842,"Similarly, the maximum calibration error (MCE) (Naeini et al., 2015; Guo et al., 2017) measures the
average deviation from Eq. 1 in the bin with the highest calibration error, and is deﬁned as"
GLOBAL CALIBRATION METRICS,0.05572755417956656,"MCE(f, ˆp) := max
i
|conf(Bi) −acc(Bi)|.
(3)"
EXISTING GLOBAL RECALIBRATION METHODS,0.058823529411764705,"2.2
EXISTING GLOBAL RECALIBRATION METHODS"
EXISTING GLOBAL RECALIBRATION METHODS,0.06191950464396285,"Many existing methods apply a post-hoc adjustment that changes a model’s conﬁdence predictions
to improve global calibration, including Platt scaling (Platt, 1999), temperature scaling (Guo et al.,
2017), isotonic regression (Zadrozny & Elkan, 2002), and histogram binning (Zadrozny & Elkan,
2001). These methods all learn a simple transformation from the original conﬁdence predictions to
new conﬁdence predictions, and aim to decrease the expected calibration error (ECE). Platt scaling
ﬁts a logistic regression model; temperature scaling learns a single temperature parameter to rescale
conﬁdence scores for all samples simultaneously; isotonic regression learns a piece-wise constant
monotonic function; histogram binning partitions conﬁdence scores into bins {[0, ϵ), [ϵ, 2ϵ), · · · , [1−
ϵ, 1]} and sorts each validation sample into a bin based on its conﬁdence ˆp(x); it then resets the
conﬁdence level of all samples in the bin to match the classiﬁcation accuracy of that bin."
EXISTING GLOBAL RECALIBRATION METHODS,0.06501547987616099,Under review as a conference paper at ICLR 2022
LOCAL CALIBRATION,0.06811145510835913,"2.3
LOCAL CALIBRATION"
LOCAL CALIBRATION,0.07120743034055728,"Two notions of calibration that address some of the deﬁcits of global calibration are class-wise
calibration and group-wise calibration. Class-wise calibration groups samples by their true class
label (Kull et al., 2019; Nixon et al., 2019) and measures the average class ECE, while group-wise
calibration uses pre-speciﬁed groupings (e.g., race or gender) (Kleinberg et al., 2016; Pleiss et al.,
2017) and measures the average group-wise ECE or maximum group-wise MCE."
LOCAL CALIBRATION,0.07430340557275542,"A few recalibration methods have been proposed for these notions of calibration as well. Dirichlet
calibration (Kull et al., 2019) achieves calibration for groups deﬁned by class labels, but does not
generalize well to settings with many classes (Zhao et al., 2021). Multicalibration (Hébert-Johnson
et al., 2017) achieves calibration for any group that can be represented by a polynomial sized circuit,
but lacks a tractable algorithm. If the groups are known a priori, one can also apply global calibration
methods within each group; however, this is impractical in many situations where the groups are not
known for new examples at inference time."
LOCAL CALIBRATION,0.07739938080495357,"At an even more local level, Zhao et al. (2020) look at individual calibration in the regression setting
and conclude that individual calibration is impossible to verify with a deterministic forecaster, and
thus there is no general method to achieve individual calibration."
KERNEL-BASED CALIBRATION METRICS,0.0804953560371517,"2.4
KERNEL-BASED CALIBRATION METRICS"
KERNEL-BASED CALIBRATION METRICS,0.08359133126934984,"Kumar et al. (2018) introduce the maximum mean calibration error (MMCE), a kernel-based quantity
that replaces the hard binning of the standard ECE estimator with a kernel similarity k(ˆp(x), ˆp(x′))
between the conﬁdence of two examples. They further propose to optimize the MMCE directly in
order to achieve better model calibration globally. Widmann et al. (2019) extend their work and
propose the more general kernel calibration error. Zhang et al. (2020) and Gupta et al. (2020) also
consider kernel-based calibration. However, these methods only consider the similarity between
model conﬁdences ˆp(x), ˆp(x′), rather than the inputs x, x′ themselves."
THE LOCAL CALIBRATION ERROR,0.08668730650154799,"3
THE LOCAL CALIBRATION ERROR"
THE LOCAL CALIBRATION ERROR,0.08978328173374613,"Recall that commonly used metrics for calibration, such as the ECE or the MCE, are global in nature
and thus only measure an aggregate reliability over the entire dataset, making them insufﬁcient for
many applications. An ideal calibration metric would instead measure calibration at an individual
level; however, doing so is impossible without making assumptions about the ground truth distribu-
tion (Zhao et al., 2020). A localized calibration metric represents an adjustable balance between these
two extremes. Ideally, such a metric should measure calibration at a local level (where the extent of
the local neighborhood can be chosen by the user) and group similar data points together."
THE LOCAL CALIBRATION ERROR,0.09287925696594428,"In this section, we introduce the local calibration error (LCE), a kernel-based metric that allows
us to measure the calibration locally around a prediction. Our metric leverages learned features to
automatically group similar samples into a soft neighborhood, and allows the neighborhood size to
be set with a hyperparameter γ. We also consider only points with a similar model conﬁdence as the
prediction, so that similarity is deﬁned in terms of distance both in the feature space and in model
conﬁdence. Thus, the LCE effectively creates soft groupings that depend on the feature space; with a
semantically meaningful feature space, these groupings correspond to useful subsets of the data. We
then mention a few design choices and visualize LCE maps over a 2D feature space to show that we
can use our metric to diagnose regions of local miscalibration."
LOCAL CALIBRATION ERROR METRIC,0.09597523219814241,"3.1
LOCAL CALIBRATION ERROR METRIC"
LOCAL CALIBRATION ERROR METRIC,0.09907120743034056,"We propose a metric to measure calibration locally around a given prediction. The calibration of
similar samples should be similar, so we use a kernel similarity function kγ : X × X →R+, which
provides similarity scores, to deﬁne soft local neighborhoods. kγ(x, x′) has bandwidth γ > 0, which
determines the extent of the local neighborhood — as γ increases, the neighborhood grows. Less
similar (i.e., further away) samples x′ have less inﬂuence on the local calibration metric at x. Also,
as with the ECE and MCE (Eqs. 2 and 3), we use binning and consider only the points in the same"
LOCAL CALIBRATION ERROR METRIC,0.1021671826625387,Under review as a conference paper at ICLR 2022
LOCAL CALIBRATION ERROR METRIC,0.10526315789473684,"conﬁdence bin as x. Thus, the samples that inﬂuence the local calibration metric at x are similar to x
in both features and model conﬁdences."
LOCAL CALIBRATION ERROR METRIC,0.10835913312693499,"More formally, let φ : X →Rd be a feature map that transforms an input to a feature vector, and let
kγ be parameterized as kγ(x, x′) = g((φ(x) −φ(x′))/γ) for some Lipschitz function g : Rd →R+.
Then given a data point x ∈X and a classiﬁer (f, ˆp), the local calibration error (LCE) of the model
at x is the expected difference between the model’s conﬁdence and accuracy on a randomly sampled
data point x′ ∼Pr, weighted by the kernel similarity kγ(x, x′)."
LOCAL CALIBRATION ERROR METRIC,0.11145510835913312,"We say a probabilistic classiﬁer (ˆp, f) is perfectly locally calibrated with respect to kγ if"
LOCAL CALIBRATION ERROR METRIC,0.11455108359133127,"sup
x∈supp(Pr)
E(x′,y′)∼Pr
h
(ˆp(x′) −1[f(x′) = y′])kγ(x, x′)
 ˆp(x′) = ˆp(x)
i"
LOCAL CALIBRATION ERROR METRIC,0.11764705882352941,"|
{z
}
:=LCE⋆
γ(x;f,ˆp) = 0."
LOCAL CALIBRATION ERROR METRIC,0.12074303405572756,"Similar to perfect calibration, perfect local calibration is achieved by the Bayes-optimal classiﬁer. In
general, perfect local calibration is a much stricter notion than perfect calibration due to localizing to
each indiviudal data point x, and reduces to perfect calibration if kγ(x, x′) ≡1 is a trivial kernel."
LOCAL CALIBRATION ERROR METRIC,0.1238390092879257,"To deﬁne LCE on a ﬁnite dataset, we perform an additional binning on the conﬁdence to deal with the
conditioning. Let D = ((x1, y1), . . . , (xN, yN)) be a dataset, and let β(x) = {i : ˆp(xi) ∈B(ˆp(x))}
be the set of indices of the points in D occupying the same conﬁdence bin as x. Then we can compute
the LCE by"
LOCAL CALIBRATION ERROR METRIC,0.12693498452012383,"LCEγ(x; f, ˆp) =  P"
LOCAL CALIBRATION ERROR METRIC,0.13003095975232198,"i∈β(x)(ˆp(xi) −1[f(xi) = yi])k(x, xi) P"
LOCAL CALIBRATION ERROR METRIC,0.13312693498452013,"i∈β(x) k(x, xi) .
(4)"
LOCAL CALIBRATION ERROR METRIC,0.13622291021671826,"Note that the quantity (ˆp(xi) −1[f(xi) = yi]) is simply the difference between the conﬁdence and
the accuracy for sample xi, and the denominator is a normalization term."
LOCAL CALIBRATION ERROR METRIC,0.1393188854489164,"We then deﬁne the maximum local calibration error (MLCE) as
MLCEγ(f, ˆp) := max
x
LCEγ(x; f, ˆp).
(5)"
LOCAL CALIBRATION ERROR METRIC,0.14241486068111456,"Intuitively, the LCE considers a neighborhood about a sample x (as deﬁned by the kernel kγ and
the conﬁdence bin B), and computes the kernel-weighted average of the difference between the
conﬁdence and accuracy for each sample in that neighborhood. Note that by changing the bandwidth γ,
we can interpolate the LCE between an individualized calibration metric (as γ →0) and a global one
(as γ →∞). Lemma 1 makes this more concrete under the assumption that limγ→∞kγ(x, x′) = 1
(proof in Appendix C). For example, the Laplacian and Gaussian kernels satisfy this condition.
Lemma 1. As γ →∞, the MLCE converges to the MCE."
LOCAL CALIBRATION ERROR METRIC,0.14551083591331268,"Theorem 1 shows that under certain regularity conditions, the ﬁnite-sample estimator LCE(x)
converges uniformly and sample-efﬁciently to its true expected value LCE∗(x):
Theorem 1. (Informal) Let α ≤infx∈X E [kγ(X, x)1[ˆp(X) ∈B(ˆp(x))]] be a lower bound on the
expectation of the kernel, and d be the dimension of the kernel’s feature space. If the sample size is at
least eO(d/α4ϵ2) where ϵ > 0 is a target accuracy level, then with probability at least 1 −δ we have"
LOCAL CALIBRATION ERROR METRIC,0.14860681114551083,"sup
x∈X"
LOCAL CALIBRATION ERROR METRIC,0.15170278637770898,"LCEγ(x; f, ˆp) −LCE∗
γ(x; f, ˆp)
 ≤ϵ."
LOCAL CALIBRATION ERROR METRIC,0.15479876160990713,"Here, eO hides log factors of the form log(1/αγδϵ). In practice, α depends inversely on γ."
LOCAL CALIBRATION ERROR METRIC,0.15789473684210525,"To summarize, the MLCE measures a worst-case individual calibration error as γ →0 (i.e., the
effective neighborhood is very small) and converges to the global MCE metric as γ →∞(i.e., the
effective neighborhood is very large). In practice, one must pick intermediate values of γ to balance a
more local notion of calibration error with the sample efﬁciency of its estimation. A more formal
statement and full proof of Theorem 1 can be found in Appendix D."
CHOICE OF KERNEL AND FEATURE MAP,0.1609907120743034,"3.2
CHOICE OF KERNEL AND FEATURE MAP"
CHOICE OF KERNEL AND FEATURE MAP,0.16408668730650156,"In this work, we compute the LCE using 15 equal-width bins and use the Laplacian kernel"
CHOICE OF KERNEL AND FEATURE MAP,0.16718266253869968,"kγ(x, x′) = exp

−∥φ(x) −φ(x′)∥1 dγ "
CHOICE OF KERNEL AND FEATURE MAP,0.17027863777089783,Under review as a conference paper at ICLR 2022
CHOICE OF KERNEL AND FEATURE MAP,0.17337461300309598,"Because distances in a high-dimensional input space (e.g., image data) may not be meaningful on
their own, we evaluate the kernel on a feature representation of x rather than on x itself. Features
learned from neural networks have proven useful for a wide range of tasks, and they have been shown
to capture useful semantic features of their inputs (Huh et al., 2016; Chen et al., 2020; Li et al., 2020).
The kernel similarity term kγ(x, x′) in the LCE thus leverages learned features to automatically
capture rich subgroups of the data. For image data, we chose an Inception-v3 model as our feature
map, since Inception features are widely accepted as useful and representative in many areas (e.g., for
generative models (Salimans et al., 2016)), though other neural features can also be used (Appendix
B). For tabular data, we used the ﬁnal hidden layer of the neural network trained for classiﬁcation."
CHOICE OF KERNEL AND FEATURE MAP,0.17647058823529413,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.2 0.4 0.6 0.8 1.0 MLCE"
CHOICE OF KERNEL AND FEATURE MAP,0.17956656346749225,2 = tsne3
CHOICE OF KERNEL AND FEATURE MAP,0.1826625386996904,2 = pca50
CHOICE OF KERNEL AND FEATURE MAP,0.18575851393188855,2 = identity = 0.2 = 0.4
CHOICE OF KERNEL AND FEATURE MAP,0.18885448916408668,"Figure 1: MLCE of a Resnet-50 classiﬁer on the ImageNet
test split, as a function of the kernel bandwidth γ. We use
a Laplacian kernel with feature map φ2 ◦φ1, where φ1 :
X →R2048 is the Inception-v3 model’s hidden layer. Blue:
φ2 : R2048 →R3 is t-SNE; orange: φ2 : R2048 →R50 is
PCA; green: φ2(z) = z is the identity."
CHOICE OF KERNEL AND FEATURE MAP,0.19195046439628483,"In general, we also use t-SNE or PCA to
reduce the dimension of the feature space.
For example, the 2048-D Inception-v3 em-
bedding is still very high-dimensional. We
report results using t-SNE to reduce the di-
mension to 2 or 3, as well as PCA to reduce
the dimension to reduce the dimension to
50 for image data and 20 for tabular data.
Thus the overall representation function is
φ(x) = φ2(φ1(x)), where φ1 maps from
the inputs to the neural features, and φ2
reduces the feature space dimension."
CHOICE OF KERNEL AND FEATURE MAP,0.19504643962848298,"Figure 1 plots the MLCE as a function of
the kernel bandwidth for an ImageNet clas-
siﬁcation task. Note that when γ is small,
the MLCE is 1 (a worst-case individual cal-
ibration error), and when γ is large, the
MLCE approaches the global MCE. To ob-
tain a single summary statistic describing
the local calibration error, we can view this
plot and pick a value of γ between the lim-
iting behaviors. We ﬁnd that γ = 0.2 and γ = 0.4 are good intermediate points for the 3-D t-SNE
and 50-D PCA features, respectively (Figure 1)."
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.19814241486068113,"3.3
LOCAL CALIBRATION ERROR VISUALIZATIONS (x)1"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.20123839009287925,"60 40 20 0
20
40
60 (x)2"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.2043343653250774,6040200 204060 LCE
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.20743034055727555,"0.02
0.04
0.06
0.08 0.10 0.12"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.21052631578947367,LCE(x) in Bin 5/15 (Best Global Calibration)
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.21362229102167182,"0.02
0.04
0.06
0.08
0.10
0.12
LCE(x) 0.0 0.2 0.4 0.6 0.8 1.0"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.21671826625386997,Probability
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.21981424148606812,CDF of LCE(x) in Bin 5/15 (Best Global Calibration)
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.22291021671826625,"CDF of LCE(x)
Bin Calibration Error (x)1"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.2260061919504644,"60 40 20 0
20
40
60 (x)2"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.22910216718266255,6040200 204060 LCE
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.23219814241486067,"0.02
0.04
0.06
0.08 0.10 0.12"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.23529411764705882,LCE(x) in Bin 12/15 (Worst Global Calibration)
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.23839009287925697,"0.02
0.04
0.06
0.08
0.10
0.12
LCE(x) 0.0 0.2 0.4 0.6 0.8 1.0"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.24148606811145512,Probability
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.24458204334365324,CDF of LCE(x) in Bin 12/15 (Worst Global Calibration)
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.2476780185758514,"CDF of LCE(x)
Bin Calibration Error"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.25077399380804954,"Figure 2: We visualize LCE0.2(x; f, ˆp) for a ResNet-50 classiﬁer (f, ˆp) pre-trained on ImageNet, for every
image x in the ImageNet validation set. We focus on the bins with the best and worst global calibration errors."
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.25386996904024767,"To provide more intuition for the LCE, we will now visualize some examples of the LCE metric over
a 2-D feature embedding. We consider a ResNet-50 model pre-trained on ImageNet as our classiﬁer"
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.25696594427244585,Under review as a conference paper at ICLR 2022
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.26006191950464397,"(f, ˆp), and pre-trained Inception-v3 features as a feature map φ1 : X →R2048. φ2 : R2048 →R2
then reduces the 2048-D feature vectors with t-SNE to two dimensions for ease of visualization in the
LCE landscapes, so our overall representation function is φ = φ2(φ1(x)). Figure 2 visualizes the
landscape of LCE0.2(x; f, ˆp) as a function of φ(x) for the entire ImageNet validation set, as well as
the marginal CDF of LCE0.2(x, f, ˆp). We show these visualizations for the two conﬁdence bins with
the best and worst global calibration."
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.2631578947368421,"In the bin with the best global calibration, Figure 2 (top) shows that the landscape of the LCE is
highly non-uniform, and the CDF of the LCE lies almost entirely to the right of the bin’s average
calibration error. Numerically, the bin’s average calibration error is 0.0022, while its average LCE
is 0.0259. This implies that the regions where the model is underconﬁdent and overconﬁdent are
spatially clustered within the bin. Because global calibration metrics solely consider the average
accuracy and average conﬁdence within a bin, conﬁdence predictions that are too high and too low
are averaged out to obtain a low overall error value; they fail to capture this localized miscalibration."
LOCAL CALIBRATION ERROR VISUALIZATIONS,0.26625386996904027,"In the bin with the worst global calibration, Figure 2 (bottom) clearly shows that the LCE still has
high variance, even though the average calibration error of the bin (0.0455) is much closer to its
average LCE (0.0515). The CDF plot provides more evidence that the landscape is not ﬂat — there
is no sharp rise at the bin calibration error. However, in this case the regions that are underconﬁdent
and overconﬁdent are not clustered spatially."
LCE RECALIBRATION,0.2693498452012384,"4
LCE RECALIBRATION"
LCE RECALIBRATION,0.2724458204334365,"In this section, we introduce local recalibration (LoRe), a non-parametric recalibration method
that adjusts a model’s output conﬁdences to achieve better local calibration. Our method improves
the LCE more than existing recalibration methods, and using our method improves performance
on both downstream fairness tasks and downstream decision-making tasks. Speciﬁcally, we can
leverage the kernel similarity to achieve strong calibration for all sensitive subgroups of a population,
without knowing those groups a priori. As long as the feature space is semantically meaningful,
LoRe provides utility for downstream tasks without needing subgroup labels for the samples. If the
subgroups are known, one can recover standard group-wise recalibration methods (and metrics) by
using the improper kernel k(x, x′) = 1[x, x′ in same group]."
LCE RECALIBRATION,0.2755417956656347,"The idea behind our method is simple: we can compute the kernel-weighted accuracy for each point
x of all the points that are in the same conﬁdence bin as x, and then reset the conﬁdence of x to
this kernel-weighted accuracy value. Note that using the kernel function to compute this value is
intuitively like taking a weighted average of the accuracy of the points in the local neighborhood of x.
Thus, LoRe can be considered a local analogue to histogram binning."
LCE RECALIBRATION,0.2786377708978328,"More formally, given a trained classiﬁer (f, ˆp), a recalibration dataset D = ((x1, y1), . . . , (xN, yN)),
and a ﬁxed point x ∈X, let β(x) = {i : ˆp(xi) ∈B(ˆp(x))} be the set of indices of the points in D
occupying the same conﬁdence bin as x. Then, we compute the recalibrated conﬁdence as"
LCE RECALIBRATION,0.28173374613003094,ˆp′(x) = P
LCE RECALIBRATION,0.2848297213622291,"i∈β(x) kγ(x, xi)1[f(xi) = yi] P"
LCE RECALIBRATION,0.28792569659442724,"i∈β(x) kγ(x, xi)
.
(6)"
LCE RECALIBRATION,0.29102167182662536,"Equation 6 represents the kernel-weighted average accuracy of all points in the same conﬁdence bin
as x. In the limit as the kernel bandwidth γ →∞, ˆp′(x) →P
i∈β(x) 1[f(xi) = yi]/|β(x)| recovers
histogram binning. As γ →0, ˆp′(x) →1[f(xi∗) = yi∗], where i∗= arg mini∈β(x) kγ(x, xi), thus
recovering a nearest-neighbor method. For intermediate γ, our method interpolates between the
two extremes. Throughout this work, we used used γ = 0.2 for LoRe with tSNE and γ = 0.4 for
LoRe with PCA throughout this work, since these represent intermediate points between the limiting
behaviors of the LCE (e.g., see Fig. 1)."
EXPERIMENTS,0.29411764705882354,"5
EXPERIMENTS"
EXPERIMENTS,0.29721362229102166,"In this section, we show empirically that LoRe substantially improves LCE values, and that these
lower LCE values lead to better performance on downstream fairness and decision-making tasks.
In particular, we evaluate the local calibration through the MLCE, because we are interested in"
EXPERIMENTS,0.30030959752321984,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.30340557275541796,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.0 0.2 0.4 0.6 0.8 1.0 MLCE"
EXPERIMENTS,0.3065015479876161,"Original
TS
HB
IR
MMCE
LoRe"
EXPERIMENTS,0.30959752321981426,"Figure 3: MLCE vs. kernel bandwidth γ for ImageNet.
LoRe (with t-SNE and γ = 0.2) achieves the lowest
MLCE for a wide range of γ. This suggests that LoRe
leads to lower LCE values across the whole dataset."
EXPERIMENTS,0.3126934984520124,"2
4
6
8
10
12
Reward Ratio 1500 1000 500 0 500 1000 1500"
EXPERIMENTS,0.3157894736842105,Improvement in Reward
EXPERIMENTS,0.3188854489164087,"Original
TS
HB
IR
MMCE
LoRe"
EXPERIMENTS,0.3219814241486068,"Figure 4: Reward attained vs. reward ratio for the
ImageNet dataset (higher is better). LoRe achieves
the highest rewards across a wide range of reward
ratios."
EXPERIMENTS,0.32507739938080493,"understanding a model’s worst-case local miscalibration. On each task, we compare the performance
of LoRe to no recalibration (‘Original’), temperature scaling (‘TS’) (Guo et al., 2017), histogram
binning (‘HB’) (Zadrozny & Elkan, 2001), isotonic regression (‘IR’) (Zadrozny & Elkan, 2002), and
direct MMCE optimization (‘MMCE’) (Kumar et al., 2018), all strong global recalibration methods."
EXPERIMENTS,0.3281733746130031,"We ﬁrst run extensive experiments on three datasets to demonstrate that LoRe outperforms all
baselines and achieves the lowest MLCE over a wide range of γ values. We then evaluate the
performance of our method on a fairness task, where it is important that a model is well-calibrated
for all sensitive subgroups of a given population, and we demonstrate that it achieves the lowest
group-wise MCE. Notably, we ﬁnd that the MLCE is well-correlated with the group-wise MCE across
all experimental settings, and thus achieving low MLCE is a good indicator that a model has good
group-wise calibration. Finally, we compare the performance of our method against the baselines on
a cost-sensitive decision-making task, where there is a low cost for a prediction of “unsure” but a
high cost for an incorrect prediction, and show that our method achieves the lowest cost."
DATASETS,0.33126934984520123,"5.1
DATASETS"
DATASETS,0.33436532507739936,"ImageNet dataset (Deng et al., 2009): A large-scale dataset of natural scene images with 1000
classes; over 1.3 million images total. The training/validation/test split is 1.3mil / 25,000 / 25,000."
DATASETS,0.33746130030959753,"UCI Communities and Crime dataset (Dua & Graff, 2017): This tabular dataset contains a
number of attributes about American neighborhoods (e.g., race, age, employment, housing, etc.).
The task is to predict the neighborhood’s violent crime rate. The training/validation/test split is
1494 / 500 / 500. We randomize the training/validation/test split over multiple trials."
DATASETS,0.34055727554179566,"CelebA dataset (Liu et al., 2015): A large-scale dataset of face images with 40 attribute anno-
tations (e.g., glasses, hair color, etc.); 202,599 images total. The training/validation/test split is
162,770 / 19,867 / 19,962."
RECALIBRATION PERFORMANCE,0.34365325077399383,"5.2
RECALIBRATION PERFORMANCE"
RECALIBRATION PERFORMANCE,0.34674922600619196,"LoRe substantially improves the LCE values. In Figure 3, we plot the MLCE as a function of γ.
We can see that our method outperforms all baselines (strong global calibration methods) across a
wide range of γ values on ImageNet. This is true despite the fact that we only implement LoRe for a
single γ. Appendix B provides similar results on the Communities & Crime, CelebA, CIFAR-10, and
CIFAR-100 datasets. Note that LoRe works well regardless of the feature map and the dimensionality
reduction method (see Section 5.3 for results with both t-SNE and PCA). Although the results shown
in this section use Inception-v3 features, we show similar results in Appendix B with AlexNet
(Krizhevsky, 2014), DenseNet121 (Huang et al., 2018), and ResNet101 (He et al., 2015) features."
RECALIBRATION PERFORMANCE,0.3498452012383901,"Recall that as γ gets large, the MLCE recovers the MCE; because LoRe does well even at large γ,
our method also works well at minimizing global calibration errors. The fact that LoRe lowers the
worst-case LCE suggests that it leads to lower LCE values across the entire dataset."
RECALIBRATION PERFORMANCE,0.35294117647058826,Under review as a conference paper at ICLR 2022
DOWNSTREAM FAIRNESS PERFORMANCE,0.3560371517027864,"5.3
DOWNSTREAM FAIRNESS PERFORMANCE"
DOWNSTREAM FAIRNESS PERFORMANCE,0.3591331269349845,"Experimental Setup
In many fairness-related applications, it is important to show that a model
is well-calibrated for all sensitive subgroups of a given population. For example, when predicting
the crime rate of a neighborhood, a model should not be considered well-calibrated if it consistently
underestimates the crime rate for neighborhoods of one demographic, while overestimating the
crime rate for neighborhoods of a different demographic. Therefore, in this section, we examine the
worst-case group-wise miscalibration of a classiﬁer, as measured by the maximum group-wise MCE
when evaluated only on sensitive sub-groups. We consider the following experimental settings:"
DOWNSTREAM FAIRNESS PERFORMANCE,0.3622291021671827,"1. UCI Communities and Crime: Predict whether a neighborhood’s crime rate is higher than the
median; group neighborhoods by their plurality race (White, Black, Asian, Indian, Hispanic).
60 random seeds for model training.
2. CelebA: Predict a person’s hair color (bald, black, blond, brown, gray, other); group people
by hair type (bald, receding hairline, bangs, straight, wavy, other). 20 random seeds for
model training.
3. CelebA: Predict a person’s hair type; group people by their hair color; inverse of Setting 2.
20 random seeds for model training."
DOWNSTREAM FAIRNESS PERFORMANCE,0.3653250773993808,"For each task, we train a classiﬁer (see Appendix A for full details) and recalibrate its output
conﬁdences using each of the recalibration methods."
DOWNSTREAM FAIRNESS PERFORMANCE,0.3684210526315789,"Recalibration method
Setting 1
Setting 2
Setting 3"
DOWNSTREAM FAIRNESS PERFORMANCE,0.3715170278637771,"No recalibration
0.588 ± 0.107
0.407 ± 0.087
0.446 ± 0.083
Temperature scaling
0.521 ± 0.092
0.532 ± 0.089
0.441 ± 0.079
Histogram binning
0.515 ± 0.081
0.218 ± 0.056
0.268 ± 0.067
Isotonic regression
0.596 ± 0.063
0.615 ± 0.100
0.716 ± 0.082
MMCE optimization
0.526 ± 0.172
0.429 ± 0.079
0.475 ± 0.079"
DOWNSTREAM FAIRNESS PERFORMANCE,0.3746130030959752,"Group temp. scaling
0.423 ± 0.066
0.673 ± 0.075
0.329 ± 0.108
Group hist. binning
0.542 ± 0.083
0.260 ± 0.053
0.352 ± 0.068"
DOWNSTREAM FAIRNESS PERFORMANCE,0.37770897832817335,"LoRe (tSNE) (ours)
0.351 ± 0.084
0.165 ± 0.055
0.235 ± 0.063
LoRe (PCA) (ours)
0.392 ± 0.071
0.167 ± 0.013
0.154 ± 0.082"
DOWNSTREAM FAIRNESS PERFORMANCE,0.38080495356037153,"Table 1: Performance on downstream fairness, as measured by maximum
group-wise MCE (lower is better). Experimental settings as described in
Section 5.3. Mean and standard deviations are computed over 60 random
seeds for setting 1, and 20 for settings 2 and 3. Best results are bold."
DOWNSTREAM FAIRNESS PERFORMANCE,0.38390092879256965,"Results
Table 1 reports the
maximum group-wise MCE for
each of the recalibration methods
on each of the three tasks. LoRe
outperforms the other baselines,
achieving an average 49% reduc-
tion over no recalibration and an
average 23% improvement over
the next best global recalibration
method. (Note in Figures 5, 6,
and 7 in Appendix B that LoRe
is the most effective method of
lowering the MLCE over a wide
range of γ). Notably, LoRe is
robust to the feature map used
(tSNE vs. PCA). It even outper-
forms global methods applied to each individual group, implying that correcting local calibration
errors is a robust way to improve group calibration that generalizes better than naive alternatives."
DOWNSTREAM FAIRNESS PERFORMANCE,0.38699690402476783,"Setting 1
Setting 2
Setting 3"
DOWNSTREAM FAIRNESS PERFORMANCE,0.39009287925696595,"ECE
0.102
-0.061
-0.195
MCE
0.233
0.439
0.281
NLL
0.542
0.045
-0.287
Brier
0.101
0.144
-0.280"
DOWNSTREAM FAIRNESS PERFORMANCE,0.3931888544891641,"MLCE0.2 (tSNE)
0.642
0.801
0.591
MLCE0.4 (PCA)
0.639
0.659
0.778"
DOWNSTREAM FAIRNESS PERFORMANCE,0.39628482972136225,"Table 2: Pearson correlation between max group-wise
MCE and other calibration metrics (higher is better).
Experimental settings as described in Section 5.3. Best
results in bold. MLCE is better-correlated with the max
group-wise MCE than any of the global metrics."
DOWNSTREAM FAIRNESS PERFORMANCE,0.3993808049535604,"Moreover, Table 2 shows that the maximum
group-wise MCE is well-correlated with the
MLCE, and it is in fact much better correlated
with MLCE than with global calibration metrics
Taken together, our results indicate that lowering
the LCE has positive implications in fairness set-
tings that cannot be achieved by simply lowering
global metrics like the ECE. For reference, we
also include the performance of all recalibration
methods on various global calibration metrics
in Table 3, which shows that LoRe is able to
improve worst-case group-wise calibration with-
out meaningfully sacriﬁcing (and in some cases
improving) average-case global calibration."
DOWNSTREAM DECISION-MAKING,0.4024767801857585,"5.4
DOWNSTREAM DECISION-MAKING"
DOWNSTREAM DECISION-MAKING,0.4055727554179567,"Experimental Setup
Machine learning predictions are often used to make decisions, and in many
situations an agent must select a best action in expectation. As an example, suppose there is a low"
DOWNSTREAM DECISION-MAKING,0.4086687306501548,Under review as a conference paper at ICLR 2022
DOWNSTREAM DECISION-MAKING,0.4117647058823529,"Recalibration method
Setting 1
Setting 2
Setting 3
ECE(%)
NLL
Brier
ECE(%)
NLL
Brier
ECE(%)
NLL
Brier"
DOWNSTREAM DECISION-MAKING,0.4148606811145511,"No recalibration
15.12.7
.96.25
.17.02
1.80.3
.617.004
.641.004
1.10.3
.782.006
.571.006
Temperature scaling
4.91.7
.43.03
.14.01
2.00.3
.619.004
.622.003
1.00.2
.781.006
.569.002
Histogram binning
3.31.1
.48.03
.150.1
2.50.2
.619.004
.614.003
2.50.4
.788.006
.552.002
Isotonic regression
30.62.3
.79.05
.30.02
2.60.2
.618.004
.615.003
2.40.2
.785.006
.553.002
MMCE optimization
4.41.3
.43.03
.14.01
3.80.7
.646.014
.679.012
5.40.8
.808.009
.619.009"
DOWNSTREAM DECISION-MAKING,0.4179566563467492,"LoRe (tSNE) (ours)
3.51.1
.42.02
.13.01
2.80.2
.623.004
.613.003
2.60.3
.792.006
.551.002
LoRe (PCA) (ours)
4.51.4
.44.02
.14.01
3.10.2
.628.004
.606.003
2.80.4
.792.007
.538.002"
DOWNSTREAM DECISION-MAKING,0.42105263157894735,"Table 3: Performance on global calibration metrics, formatted as meansd. Lower is better. Experimental settings
as described in Section 5.3. Best results are bold. Across all settings, LoRe generally achieves a global
calibration error that is comparable to the baselines."
DOWNSTREAM DECISION-MAKING,0.4241486068111455,"cost u associated with returning “unsure” and a high cost w associated with returning an incorrect
classiﬁcation (e.g., in situations such as autonomous driving, being unsure incurs only the small cost
of calling a human operator, but making an incorrect classiﬁcation incurs a high cost). An agent
with good uncertainty quantiﬁcation can make a more optimal decision about whether to return a
classiﬁcation or return “unsure”; for a calibrated model, it would be optimal for the agent to return
“unsure” below the conﬁdence threshold of 1 −u/w, and return a prediction above this threshold."
DOWNSTREAM DECISION-MAKING,0.42724458204334365,"Following this policy — i.e., returning “unsure” when the conﬁdence is below this threshold and
returning a prediction when the conﬁdence is above it, we used a ResNet-50 model to make predictions
on ImageNet, and recalibrated the predictions with each of the recalibration methods. For each method,
we then calculated the total reward attained under various reward ratios w/u, as well as the prediction
rejection area ratio (PRR) (Malinin et al., 2020), which measures the quality of the rejection curve
when a model can choose to not provide any prediction. Note that a PRR of 1.0 indicates optimal
rejection, and a PRR of 0.0 indicates “random” rejection; a higher PRR is better."
DOWNSTREAM DECISION-MAKING,0.43034055727554177,"Recalibration method
PRR
ECE
NLL
Brier"
DOWNSTREAM DECISION-MAKING,0.43343653250773995,"No recalibration
0.566
0.037
0.959
40.64
Temperature scaling
0.561
0.022
0.948
40.60
Histogram binning
0.461
0.012
0.952
40.59
Isotonic regression
0.570
0.011
0.945
40.59
MMCE optimization
0.571
0.061
0.965
40.67"
DOWNSTREAM DECISION-MAKING,0.43653250773993807,"LoRe (ours)
0.575
0.007
0.955
40.58"
DOWNSTREAM DECISION-MAKING,0.43962848297213625,"Table 4: Performance on downstream decision-making on Ima-
geNet, as measured by the PRR (higher is better). We also report
the global calibration metrics ECE, NLL, and Brier score (lower is
better). Best results in bold."
DOWNSTREAM DECISION-MAKING,0.44272445820433437,"Results
In Figure 4, we show the
improvement in the total reward over
the original classiﬁer (i.e., no recali-
bration) as a function of the reward
ratio w/u (the ratio of the cost of an
incorrect classiﬁcation to the cost of
being unsure). Across a wide range of
reward ratios, LoRe achieves the high-
est reward. The MLCE curves for this
task are shown in Figure 3; note that
LoRe also achieves lower LCE values
than the global recalibration methods.
Table 4 also reports the PRR achieved
with each method under a cost function of u = −1, w = −10, as well as several global calibration
metrics; LoRe generally outperforms the other baselines in terms of both PRR and global calibration
metrics. These results indicate that our recalibration method most effectively lowers LCE values
without sacriﬁcing (and indeed often improving) average-case global calibration, and that these lower
LCE values correspond to better performance on this decision-making task."
CONCLUSION,0.4458204334365325,"6
CONCLUSION"
CONCLUSION,0.44891640866873067,"In this paper, we introduce the local calibration error (LCE), a metric that measures calibration in
a localized neighborhood around a prediction. The LCE spans the gap between fully global and
fully individualized calibration error, with an effective neighborhood size that can be set with a
bandwidth parameter γ. We also introduce LoRe, a recalibration method that greatly improves the
local calibration. Finally, we demonstrate that achieving lower LCE values leads to better performance
on downstream fairness and decision-making tasks. In future work, we hope to explore better feature
spaces to deﬁne similarity, since the quality of our metric depends on the quality of the feature space."
CONCLUSION,0.4520123839009288,Under review as a conference paper at ICLR 2022
REFERENCES,0.4551083591331269,REFERENCES
REFERENCES,0.4582043343653251,"Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daumé III and Aarti Singh (eds.), Proceedings
of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 1597–1607. PMLR, 13–18 Jul 2020. URL http://proceedings.
mlr.press/v119/chen20j.html."
REFERENCES,0.4613003095975232,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009."
REFERENCES,0.46439628482972134,"Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml."
REFERENCES,0.4674922600619195,"Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,
ITCS ’12, pp. 214–226, New York, NY, USA, 2012. Association for Computing Machinery. ISBN
9781450311151. doi: 10.1145/2090236.2090255."
REFERENCES,0.47058823529411764,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1321–1330, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL"
REFERENCES,0.47368421052631576,http://proceedings.mlr.press/v70/guo17a.html.
REFERENCES,0.47678018575851394,"Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu,
and Richard Hartley. Calibration of neural networks using splines. 2020."
REFERENCES,0.47987616099071206,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2015."
REFERENCES,0.48297213622291024,"Úrsula Hébert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Calibration for the
(computationally-identiﬁable) masses, 2017. URL http://arxiv.org/abs/1711.08513."
REFERENCES,0.48606811145510836,"Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. 2018."
REFERENCES,0.4891640866873065,"Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer
learning?, 2016. URL http://arxiv.org/abs/1608.08614."
REFERENCES,0.49226006191950467,"Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair
determination of risk scores, 2016. URL http://arxiv.org/abs/1609.05807."
REFERENCES,0.4953560371517028,"Alex Krizhevsky.
One weird trick for parallelizing convolutional neural networks.
CoRR,
abs/1404.5997, 2014. URL http://arxiv.org/abs/1404.5997."
REFERENCES,0.4984520123839009,"Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, and Peter Flach.
Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet
calibration. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32, pp. 12316–12326. Cur-
ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/
file/8ca01ea920679a0fe3728441494041b9-Paper.pdf."
REFERENCES,0.5015479876160991,"Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 2805–2814, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/kumar18a.html."
REFERENCES,0.5046439628482973,"Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence
embeddings from pre-trained language models.
In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pp. 9119–9130. Association for
Computational Linguistics, November 2020. doi: 10.18653/v1/2020.emnlp-main.733. URL
https://www.aclweb.org/anthology/2020.emnlp-main.733."
REFERENCES,0.5077399380804953,Under review as a conference paper at ICLR 2022
REFERENCES,0.5108359133126935,"Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015."
REFERENCES,0.5139318885448917,"A. Malinin, Bruno Mlodozeniec, and M. Gales.
Ensemble distribution distillation.
ArXiv,
abs/1905.00076, 2020."
REFERENCES,0.5170278637770898,"Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated
probabilities using bayesian binning. In AAAI, pp. 2901–2907, 2015."
REFERENCES,0.5201238390092879,"Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
calibration in deep learning. ArXiv, abs/1904.01685, 2019."
REFERENCES,0.5232198142414861,"John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In Advances in Large Margin Classiﬁers, pp. 61–74. MIT Press, 1999."
REFERENCES,0.5263157894736842,"Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness
and calibration. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30, pp. 5680–
5689. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/
2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf."
REFERENCES,0.5294117647058824,"Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In NIPS, 2016."
REFERENCES,0.5325077399380805,"Elizabeth A. Stuart. Matching methods for causal inference: A review and a look forward. Statistical
science : a review journal of the Institute of Mathematical Statistics, 25(1):1–21, 2010. doi:
10.1214/09-STS313."
REFERENCES,0.5356037151702786,"David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classiﬁcation:
A unifying framework. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf."
REFERENCES,0.5386996904024768,"Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classiﬁers. In In Proceedings of the Eighteenth International Conference on
Machine Learning, pp. 609–616. Morgan Kaufmann, 2001."
REFERENCES,0.541795665634675,"Bianca Zadrozny and Charles Elkan. Transforming classiﬁer scores into accurate multiclass probabil-
ity estimates. In SIGKDD, 2002."
REFERENCES,0.544891640866873,"Jize Zhang, Bhavya Kailkhura, and T. Yong-Jin Han. Mix-n-match: Ensemble and compositional
methods for uncertainty calibration in deep learning. 2020."
REFERENCES,0.5479876160990712,"Shengjia Zhao, Tengyu Ma, and S. Ermon. Individual calibration with randomized forecasting. In
ICML, 2020."
REFERENCES,0.5510835913312694,"Shengjia Zhao, Michael P. Kim, Roshni Sahoo, Tengyu Ma, and Stefano Ermon. Calibrating
predictions to decisions: A novel approach to multi-class calibration, 2021."
REFERENCES,0.5541795665634675,Under review as a conference paper at ICLR 2022
REFERENCES,0.5572755417956656,"A
MODEL ARCHITECTURE, TRAINING, AND OTHER HYPERPARAMETERS"
REFERENCES,0.5603715170278638,"For ImageNet and CelebA, we compute the ECE, MCE, and LCE using 15 equal-width conﬁdence
bins. For the UCI communities and crime dataset, we use 5 equal-width bins because the dataset is
much smaller (500 datapoints for recalibration). These numbers of bins represent a good tradeoff
between bias and variance in estimating the relevant calibration errors. We also ran some initial
experiments with equal-mass binning, but found that the results were very similar to those obtained
with equal-width binning."
REFERENCES,0.5634674922600619,"A.1
IMAGENET"
REFERENCES,0.56656346749226,"For all experiments with the ImageNet dataset, we used the pre-trained ResNet-50 model from the
PyTorch torchvision package as our classiﬁer. To calculate the LCE and apply LoRe, we used
pre-trained Inception-v3 features, applying either t-SNE to reduce their dimension to 3 or PCA to
reduce their dimension to 50, as a feature representation for the kernel."
REFERENCES,0.5696594427244582,"A.2
UCI COMMUNITIES AND CRIME"
REFERENCES,0.5727554179566563,"For all experiments with the UCI communities and crime dataset, we used a 3-hidden-layer dense
neural network as our base classiﬁer. Each hidden layer had a width of 100 and was followed by a
Leaky ReLU activation. We applied dropout with probability 0.4 after the ﬁnal hidden layer. We
trained the model using the Adam optimizer with a batch size of 64 and a learning rate of 3 × 10−4
until the validation accuracy stopped improving. All other hyperparameters were PyTorch defaults.
Training was done locally on a laptop CPU. We trained 60 different models with different random
seeds to perform the experiments described in Section 5.3 and Figure 5. To calculate the LCE and
apply LoRe, we used the ﬁnal hidden layer representation learned by our model, applying t-SNE to
reduce the dimension to 2 or PCA to reduce their dimension to 20, as a feature representation for the
kernel."
REFERENCES,0.5758513931888545,"A.3
CELEBA"
REFERENCES,0.5789473684210527,"For all experiments with the CelebA dataset, we trained a ResNet50 model and used it as our base
classiﬁer. We applied standard data augmentation to our training data (random crops & random
horizontal ﬂips), and trained all models for 10 epochs using the Adam optimizer with a learning rate
of 1 × 10−3 and a batch size of 256. All other hyperparameters were PyTorch defaults. Training
was distributed over 4 GPUs, and training a single model took about 30 minutes. For both Setting
2 and Setting 3 (described in Section 5.3), we trained 20 models with different random seeds to
perform the experiments shown in Figures 6 and 7. To calculate the LCE and apply LoRe, we used
pre-trained Inception-v3 features, applying t-SNE to reduce their dimension to 2 or PCA to reduce
their dimension to 50, as a feature representation for the kernel."
REFERENCES,0.5820433436532507,"B
ADDITIONAL EXPERIMENTAL RESULTS"
REFERENCES,0.5851393188854489,"In Figures 5, 6, and 7, we visualize the MLCE achieved by all recalibration methods for the three
experimental settings evaluated in Section 5.3. Figure 3 in the main paper shows the same visualization
for all methods on ImageNet. In Figure 8, we plot the MLCE achieved by all recalibration methods
for CIFAR-100, and in Figure 9, we do the same for CIFAR-10. Across all settings and datasets, our
method LoRe is the most effective at minimizing MLCE across a wide range of γ, even accounting
for variations between runs."
REFERENCES,0.5882352941176471,Under review as a conference paper at ICLR 2022
REFERENCES,0.5913312693498453,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.2 0.4 0.6 0.8 1.0 MLCE"
REFERENCES,0.5944272445820433,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.5975232198142415,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.2 0.4 0.6 0.8 1.0 MLCE"
REFERENCES,0.6006191950464397,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.6037151702786377,"Figure 5: MLCE vs. kernel bandwidth γ for all methods on task 1 of Section 5.3, predicting whether a
neighborhood’s crime rate is higher than the median. LoRe achieves the best (or competitive) MLCE for most γ.
Left: 2D t-SNE features. Right: 20D PCA features."
REFERENCES,0.6068111455108359,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.0 0.2 0.4 0.6 0.8 1.0 MLCE"
REFERENCES,0.6099071207430341,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.6130030959752322,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.0 0.2 0.4 0.6 0.8 1.0 MLCE"
REFERENCES,0.6160990712074303,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.6191950464396285,"Figure 6: MLCE vs. kernel bandwidth γ for all methods on task 2 of Section 5.3, predicting hair color on CelebA.
LoRe achieves the best MLCE for virtually all values of γ. Left: 2D t-SNE features. Right: 50D PCA features."
REFERENCES,0.6222910216718266,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.2 0.4 0.6 0.8 1.0 MLCE"
REFERENCES,0.6253869969040248,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.628482972136223,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.0 0.2 0.4 0.6 0.8 1.0 MLCE"
REFERENCES,0.631578947368421,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.6346749226006192,"Figure 7: MLCE vs. kernel bandwidth for all methods on task 3 of Section 5.3, predicting hair type on CelebA.
LoRe achieves the best MLCE for all γ < 1 and is tied with histogram binning for γ > 1. Left: 2D t-SNE
features. Right: 50D PCA features."
REFERENCES,0.6377708978328174,"In these ﬁgures, “Original” represents no recalibration, “TS” represents temperature scaling, “HB”
represents histogram binning, “IR” represents isotonic regression, “MMCE” represents direct MMCE
optimization, and “LoRe” is our method."
REFERENCES,0.6408668730650154,"Next, we examine the inﬂuence of the speciﬁc feature map used. In Figures 10, 11, 12, and 13, we
plot the MLCE achieved by all recalibration methods for ImageNet using Inception-v3, AlexNet,
DenseNet121, and ResNet101 features. In Figures 14 and 15, we plot the MLCE achieved by all
recalibration methods for ImageNet when the features used to calculate the MLCE are different from
the features used by LoRe. For completeness, in Figures 16, 17, 18, and 19, we also visualize the
average LCE for all experimental settings. All plots show similar results: LoRe performs best over a
wide range of γ."
REFERENCES,0.6439628482972136,Under review as a conference paper at ICLR 2022
REFERENCES,0.6470588235294118,"Figure 8: MLCE vs. kernel bandwidth γ for all recali-
bration methods for CIFAR-100 (3D t-SNE features).
LoRe achieves lower MLCE for most γ."
REFERENCES,0.6501547987616099,"Figure 9: MLCE vs. kernel bandwidth γ for all recali-
bration methods for CIFAR-10 (3D t-SNE features).
LoRe achieves lower MLCE for most γ."
REFERENCES,0.653250773993808,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.0 0.2 0.4 0.6 0.8 1.0 MLCE"
REFERENCES,0.6563467492260062,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.6594427244582043,"Figure 10: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using Inception-v3
features. LoRe achieves the best MLCE for most γ."
REFERENCES,0.6625386996904025,"Figure 11: MLCE vs. kernel bandwidth γ for all recal-
ibration methods on ImageNet using AlexNet features.
LoRe achieves the best MLCE for most γ."
REFERENCES,0.6656346749226006,"Figure 12: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using DenseNet121
features. LoRe achieves the best MLCE for most γ."
REFERENCES,0.6687306501547987,"Figure 13: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using ResNet101
features. LoRe achieves the best MLCE for most γ."
REFERENCES,0.6718266253869969,Under review as a conference paper at ICLR 2022
REFERENCES,0.6749226006191951,"Figure 14: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using Inception-v3
features to calculate the MLCE and AlexNet features
for applying LoRe."
REFERENCES,0.6780185758513931,"Figure 15: MLCE vs. kernel bandwidth γ for all re-
calibration methods on ImageNet using DenseNet121
features to calculate the MLCE and AlexNet features
for applying LoRe."
REFERENCES,0.6811145510835913,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.00 0.05 0.10 0.15 0.20 0.25 ELCE"
REFERENCES,0.6842105263157895,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.6873065015479877,"Figure 16: Average LCE vs. kernel bandwidth γ for
all recalibration methods on ImageNet (3D t-SNE
features). LoRe gets lower average LCE for most γ."
REFERENCES,0.6904024767801857,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.1 0.2 0.3 0.4 ELCE"
REFERENCES,0.6934984520123839,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.6965944272445821,"Figure 17: Average LCE vs. kernel bandwidth γ for
all recalibration methods in task 1 (crime data, 2D
t-SNE features). LoRe gets lower average LCE for
most γ."
REFERENCES,0.6996904024767802,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.05 0.10 0.15 0.20 0.25 0.30 0.35 ELCE"
REFERENCES,0.7027863777089783,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.7058823529411765,"Figure 18: Average LCE vs. kernel bandwidth γ for
all recalibration methods in task 2 (CelebA, 2D t-SNE
features). LoRe gets lower average LCE for most γ."
REFERENCES,0.7089783281733746,"10
3
10
2
10
1
100
101
102
Kernel Bandwidth 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 ELCE"
REFERENCES,0.7120743034055728,"Original
TS
HB
IR
MMCE
LoRe"
REFERENCES,0.7151702786377709,"Figure 19: Average LCE vs. kernel bandwidth γ for
all recalibration methods in task 3 (CelebA, 2D t-SNE
features). LoRe gets lower average LCE for most γ."
REFERENCES,0.718266253869969,Under review as a conference paper at ICLR 2022
REFERENCES,0.7213622291021672,"C
PROOF OF LEMMA 1"
REFERENCES,0.7244582043343654,"We restate Lemma 1 below, and provide the proof:
Lemma 2. Assume that limγ→∞kγ(x, x′) = 1 for all x, x′ ∈X. Then, as γ →∞, the MLCE
converges to the MCE."
REFERENCES,0.7275541795665634,"Proof. Since limγ→∞kγ(x, x′) = 1 identically,"
REFERENCES,0.7306501547987616,"lim
γ→∞max
x
[
LCEγ(x; f, ˆp) = max
x
1
|β(x)|  X"
REFERENCES,0.7337461300309598,"i∈β(x)
ˆp(xi) −1 [f(xi) = yi] "
REFERENCES,0.7368421052631579,"= max
k
1
|Bk|  X"
REFERENCES,0.739938080495356,"i∈Bk
ˆp(xi) −1 [f(xi) = yi] "
REFERENCES,0.7430340557275542,"= max
k
|conf(Bk) −acc(Bk)|"
REFERENCES,0.7461300309597523,"= MCE(x; f, ˆp)"
REFERENCES,0.7492260061919505,"D
FORMAL STATEMENT AND PROOF OF THEOREM 1"
REFERENCES,0.7523219814241486,"Let B1, . . . , BN denote a set of bins that partition [0, 1], and B(p) denote the bin that a particular
p ∈[0, 1] belongs to. Let af(x, y) = 1 [f(x) = y] indicate the accuracy of a the classiﬁer (f, ˆp) on
an input x. We consider the signed local calibration error (SLCE):"
REFERENCES,0.7554179566563467,"SLCEγ(x; f, ˆp) := E[(ˆp(X) −af(X, Y ))kγ(X, x) | ˆp(X) ∈B(ˆp(x))]"
REFERENCES,0.7585139318885449,"E[kγ(X, x) | ˆp(X) ∈B(ˆp(x))]"
REFERENCES,0.7616099071207431,"= E[(ˆp(X) −af(X, Y ))kγ(X, x)1 [ˆp(X) ∈B(ˆp(x))]]"
REFERENCES,0.7647058823529411,"E[kγ(X, x)1 [ˆp(X) ∈B(ˆp(x))]]
."
REFERENCES,0.7678018575851393,"D.1
ASSUMPTIONS AND FORMAL STATEMENT OF THEOREM"
REFERENCES,0.7708978328173375,"We make the following assumptions:
Assumption A (Lipschitz kernel). The kernel kγ takes the form"
REFERENCES,0.7739938080495357,"kγ(x, x′) = g
φ(x) −φ(x′) γ 
,"
REFERENCES,0.7770897832817337,"where φ : X →Rd is a representation function, and g : Rd →[0, 1] is L-Lipschitz with respect to
some norm ∥·∥."
REFERENCES,0.7801857585139319,"Note this deﬁnition may require an implicit rescaling (for example, we can take φ(x) ←φfeature(x)/d
for a d-dimensional feature map φfeature and take g(z) = exp(−∥z∥1), which corresponds to the
Laplacian kernel we used in Section 3.2).
Assumption B (Binning-aware covering number). For any ϵ > 0, the range of the representation
function φ(X) := {φ(x) : x ∈X} has an ϵ-cover in the ∥·∥-norm of size (C/ϵ)d for some absolute
constant C > 0: There exists a set Nϵ ∈X with |Nϵ| ≤(C/ϵ)d such that for any x ∈X, there exists
some x′ ∈Nϵ such that ∥φ(x) −φ(x′)∥≤ϵ and B(ˆp(x)) = B(ˆp(x′)).
Assumption C (Lower bound on expectation of kernel within bin). We have"
REFERENCES,0.7832817337461301,"inf
x∈X E[kγ(X, x)1 [ˆp(X) ∈B(ˆp(x))]] ≥α"
REFERENCES,0.7863777089783281,"for some constant α ∈(0, 1)."
REFERENCES,0.7894736842105263,"The constant α characterizes the hardness of estimating the SLCE from samples. Intuitively, with a
smaller α, the denominator in SLCE gets smaller and we desire a higher accuracy in estimating both
the numerator and the denominator. Also note that in practice the value of α typically depends on γ."
REFERENCES,0.7925696594427245,Under review as a conference paper at ICLR 2022
REFERENCES,0.7956656346749226,We analyze the following estimator of the SLCE using n samples:
REFERENCES,0.7987616099071208,"\
SLCEγ(x; f, ˆp) ="
"N
PN",0.8018575851393189,"1
n
Pn
i=1(ˆp(xi) −af(xi, yi))kγ(xi, x)1 [ˆp(xi) ∈B(ˆp(x))]"
"N
PN",0.804953560371517,"1
n
Pn
i=1 kγ(xi, x)1 [ˆp(xi) ∈B(ˆp(x))]
.
(7)"
"N
PN",0.8080495356037152,"Theorem 2. Under Assumptions A, B, and C, Suppose the sample size n ≥eO(d/α4ϵ2) where ϵ > 0
is a target accuracy level, then with probability at least 1 −δ we have"
"N
PN",0.8111455108359134,"sup
x∈X"
"N
PN",0.8142414860681114,"\
SLCEγ(x; f, ˆp) −SLCEγ(x; f, ˆp)
 ≤ϵ,"
"N
PN",0.8173374613003096,where eO hides log factors of the form log(L/γϵδα).
"N
PN",0.8204334365325078,"Theorem 2 shows that eO(d/ϵ2α4) samples is sufﬁcient to estimate the SLCE simultaneously for
all x ∈X. When α = Ω(1), this sample complexity only depends polynomially in terms of the
representation dimension d and logarithmically in other constants (such as L, γ, and the failure
probability δ)."
"N
PN",0.8235294117647058,"D.2
PROOF OF THEOREM 2"
"N
PN",0.826625386996904,"Step 1. We ﬁrst study the estimation at ﬁnitely many x’s. Let N ⊆X be a ﬁnite set of x’s with
|N| = N. Since kγ ∈[0, 1] and |ˆp(x) −af(x, y)| ≤1 are bounded variables, by the Hoeffding
inequality and a union bound, we have"
"N
PN",0.8297213622291022,"P

sup
x∈N"
N,0.8328173374613003,"1
n n
X"
N,0.8359133126934984,"i=1
(ˆp(xi) −af(xi, yi))kγ(xi, x)1 [ˆp(xi) ∈B(ˆp(x))]"
N,0.8390092879256966,"−E[(ˆp(X) −af(X, Y ))kγ(X, x)1 [ˆp(X) ∈B(ˆp(x))]]
 > αϵ/10
"
N,0.8421052631578947,"≤exp
 
−cnα2ϵ2 + log N

."
N,0.8452012383900929,"Therefore, as long as n ≥O(log(N/δ)/ϵ2α2) samples, the above probability is bounded by δ. In
other words, with probability at least 1 −δ, we have simultaneously

1
n n
X"
N,0.848297213622291,"i=1
(ˆp(xi) −af(xi, yi))kγ(xi, x)1 [ˆp(xi) ∈B(ˆp(x))]"
N,0.8513931888544891,"|
{z
}"
N,0.8544891640866873,":= ˆ
A(x)"
N,0.8575851393188855,"−E[(ˆp(X) −af(X, Y ))kγ(X, x)1 [ˆp(X) ∈B(ˆp(x))]]
|
{z
}
:=A(x) "
N,0.8606811145510835,≤αϵ/10.
N,0.8637770897832817,"for all x ∈N. Similarly, when n ≥O(log(N/δ)/ϵ2α4), we also have (with probability at least
1 −δ)

1
n n
X"
N,0.8668730650154799,"i=1
kγ(xi, x)1 [ˆp(xi) ∈B(ˆp(x))]"
N,0.8699690402476781,"|
{z
}"
N,0.8730650154798761,":= ˆ
B(x)"
N,0.8761609907120743,"−E[kγ(X, x)1 [ˆp(X) ∈B(ˆp(x))]]
|
{z
}
:=B(x)"
N,0.8792569659442725,≤α2ϵ/10
N,0.8823529411764706,"On these concentration events, we have for any x ∈N that
\
SLCEγ(x; f, ˆp) −SLCEγ(x; f, ˆp)
 = "
N,0.8854489164086687,"ˆA(x)
ˆB(x)
−A(x) B(x) "
N,0.8885448916408669,"≤
 ˆA(x)"
N,0.891640866873065,"1
ˆB(x)
−
1
B(x)"
N,0.8947368421052632,"+
1
|B(x)|"
N,0.8978328173374613,ˆA(x) −A(x)
N,0.9009287925696594,"≤1 ·
α2ϵ/10
α(α −α2ϵ/10) + 1"
N,0.9040247678018576,α · αϵ/10 ≤ϵ.
N,0.9071207430340558,Under review as a conference paper at ICLR 2022
N,0.9102167182662538,"Step 2. We now extend the bound to all x ∈X using the covering argument. By Assumption B,
we can take an α2ϵγ/(10L)-covering of φ(X) with cardinality N ≤(10CL/α2ϵγ)d. Let N ⊂X
denote the covering set (in the X space). This means that for any x ∈X, there exists x′ ∈N such
that ∥φ(x) −φ(x′)∥≤α2ϵγ/(10L) amd B(ˆp(x)) = B(ˆp(x′)), which implies that for any ex ∈X
we have"
N,0.913312693498452,"|k(ex, x) −k(ex, x′)| =
f
φ(ex) −φ(x) γ"
N,0.9164086687306502,"
−f
φ(ex) −φ(x′) γ  ≤L"
N,0.9195046439628483,γ ∥φ(x) −φ(x′)∥
N,0.9226006191950464,"≤α2ϵ/10,"
N,0.9256965944272446,where we have used the Lipschitzness assumption of g (Assumption A). This further implies
N,0.9287925696594427,"ˆA(x) −ˆA(x′)
 =

1
n n
X"
N,0.9318885448916409,"i=1
(ˆp(xi) −af(xi, yi))kγ(xi, x)1 [ˆp(xi) ∈B(ˆp(x))] −1 n n
X"
N,0.934984520123839,"i=1
(ˆp(xi) −af(xi, yi))kγ(xi, x′)1 [ˆp(xi) ∈B(ˆp(x′))]"
N,0.9380804953560371,"=

1
n n
X"
N,0.9411764705882353,"i=1
(ˆp(xi) −af(xi, yi))[kγ(xi, x) −kγ(xi, x′)]1 [ˆp(xi) ∈B(ˆp(x))] ≤1 n n
X"
N,0.9442724458204335,"i=1
|ˆp(xi) −af(xi, yi)| · |kγ(xi, x) −kγ(xi, x′)| · 1 [ˆp(xi) ∈B(ˆp(x))]"
N,0.9473684210526315,≤α2ϵ/10.
N,0.9504643962848297,"Similarly, we have |A(x) −A(x′)| ≤α2ϵ/10, | ˆB(x) −ˆB(x′)| ≤α2ϵ/10, and |B(x) −B(x′)| ≤
α2ϵ/10. This means that the estimation error at x is close to that at x′ ∈N and consequently also
bounded by ϵ:"
N,0.9535603715170279,"\
SLCEγ(x; f, ˆp) −SLCEγ(x; f, ˆp)
 = "
N,0.9566563467492261,"ˆA(x)
ˆB(x)
−A(x) B(x)  ≤ "
N,0.9597523219814241,"ˆA(x)
ˆB(x)
−
ˆA(x′)
ˆB(x′) + "
N,0.9628482972136223,"ˆA(x′)
ˆB(x′)
−A(x′) B(x′)"
N,0.9659442724458205,"+

A(x′)
B(x′) −A(x) B(x) "
N,0.9690402476780186,"≤3

1 ·
α2ϵ/10
α(α −α2ϵ/10) + 1"
N,0.9721362229102167,"α · α2ϵ/10
 ≤ϵ."
N,0.9752321981424149,"Therefore, taking this N in step 1, we know that as long as the sample size"
N,0.978328173374613,"N ≥O
log(|N|/δ) ϵ2α4 
= O"
N,0.9814241486068112,"d

log
 
10CL/α2ϵγ

+ log(1/δ)
 α4ϵ2 !"
N,0.9845201238390093,"= eO
 
d/α4ϵ2
,"
N,0.9876160990712074,we have with probability at least 1 −δ that
N,0.9907120743034056,"sup
x∈X"
N,0.9938080495356038,"\
SLCEγ(x; f, ˆp) −SLCEγ(x; f, ˆp)
 ≤ϵ."
N,0.9969040247678018,This is the desired result.
