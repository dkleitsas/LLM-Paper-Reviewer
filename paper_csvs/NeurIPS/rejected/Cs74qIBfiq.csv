Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00205761316872428,"Zero-shot inference is a powerful paradigm that enables the use of large pretrained
1"
ABSTRACT,0.00411522633744856,"models for downstream classification tasks without further training. However,
2"
ABSTRACT,0.006172839506172839,"these models are vulnerable to inherited biases that can impact their performance.
3"
ABSTRACT,0.00823045267489712,"The traditional solution is fine-tuning, but this undermines the key advantage of
4"
ABSTRACT,0.0102880658436214,"pretrained models, which is their ability to be used out-of-the-box. We propose
5"
ABSTRACT,0.012345679012345678,"ROBOSHOT, a method that improves the robustness of pretrained model embed-
6"
ABSTRACT,0.01440329218106996,"dings in a fully zero-shot fashion. First, we use zero-shot language models (LMs)
7"
ABSTRACT,0.01646090534979424,"to obtain useful insights from task descriptions. These insights are embedded and
8"
ABSTRACT,0.018518518518518517,"used to remove harmful and boost useful components in embeddings—without any
9"
ABSTRACT,0.0205761316872428,"supervision. Theoretically, we provide a simple and tractable model for biases in
10"
ABSTRACT,0.02263374485596708,"zero-shot embeddings and give a result characterizing under what conditions our
11"
ABSTRACT,0.024691358024691357,"approach can boost performance. Empirically, we evaluate ROBOSHOT on nine
12"
ABSTRACT,0.026748971193415638,"image and NLP classification tasks and show an average improvement of 15.98%
13"
ABSTRACT,0.02880658436213992,"over several zero-shot baselines. Additionally, we demonstrate that ROBOSHOT is
14"
ABSTRACT,0.030864197530864196,"compatible with a variety of pretrained and language models.
15"
INTRODUCTION,0.03292181069958848,"1
Introduction
16"
INTRODUCTION,0.03497942386831276,"Zero-shot models are among the most exciting paradigms in machine learning. These models obviate
17"
INTRODUCTION,0.037037037037037035,"the need for data collection and model training loops by simply asking the model for a prediction
18"
INTRODUCTION,0.03909465020576132,"on any set of classes. Unfortunately, such models inherit biases or undesirable correlations from
19"
INTRODUCTION,0.0411522633744856,"their large-scale training data [DLS+18, TE11]. In a now-canonical example [KSM+21], they often
20"
INTRODUCTION,0.043209876543209874,"associate waterbirds with water background. This behavior leads to decreased performance,
21"
INTRODUCTION,0.04526748971193416,"often exacerbated on rare data slices that break in-distribution correlations.
22"
INTRODUCTION,0.047325102880658436,"A growing body of literature [YNPM23, GKG+22, ZR22] seeks to improve robustness in zero-shot
23"
INTRODUCTION,0.04938271604938271,"models. While promising, these works require labeled data to train or fine-tune models, and so do
24"
INTRODUCTION,0.051440329218107,"not tackle the zero-shot setting. A parallel line of research seeking to debias word embeddings
25"
INTRODUCTION,0.053497942386831275,"[AZS+, BCZ+16, DP19, LGPV20] often sidesteps the need for labeled data. Unfortunately, these
26"
INTRODUCTION,0.05555555555555555,"works often require domain expertise and painstaking manual specification in order to identify
27"
INTRODUCTION,0.05761316872427984,"particular concepts that embeddings must be invariant to. As a result, out-of-the-box word embedding
28"
INTRODUCTION,0.059670781893004114,"debiasing methods also cannot be applied to zero-shot robustification.
29"
INTRODUCTION,0.06172839506172839,"Can we robustify zero-shot models without (i) labeled data, (ii) training or fine-tuning, or (iii) manual
30"
INTRODUCTION,0.06378600823045268,"identification? Surprisingly, despite this seemingly impoverished setting, it is often possible to do
31"
INTRODUCTION,0.06584362139917696,"so. Our key observation is that zero-shot models contain actionable insights that can be exploited
32"
INTRODUCTION,0.06790123456790123,"to improve themselves or other zero-shot models. These insights are noisy but cheaply available at
33"
INTRODUCTION,0.06995884773662552,"scale—and can be easily translated into means of refinement for zero-shot representations. These
34"
INTRODUCTION,0.0720164609053498,"refinements improve performance, particularly on underperforming slices—at nearly no cost.
35"
INTRODUCTION,0.07407407407407407,Figure 1: ROBOSHOT pipeline (right) vs. vanilla zero-shot classification (left).
INTRODUCTION,0.07613168724279835,"We propose ROBOSHOT, a system that robustifies zero-shot models via auxiliary language models
36"
INTRODUCTION,0.07818930041152264,"without labels, training, or manual specification. Using just the task description, ROBOSHOT obtains
37"
INTRODUCTION,0.08024691358024691,"positive and negative insights from a language model (potentially the model to be robustified itself).
38"
INTRODUCTION,0.0823045267489712,"It uses embeddings of these noisy insights to recover harmful, beneficial, and benign subspaces of
39"
INTRODUCTION,0.08436213991769548,"zero-shot latent representation spaces. Representations are then modified to neutralize and emphasize
40"
INTRODUCTION,0.08641975308641975,"their harmful and beneficial components, respectively.
41"
INTRODUCTION,0.08847736625514403,"Theoretically, we introduce a simple and tractable model to capture and quantify failures in zero-shot
42"
INTRODUCTION,0.09053497942386832,"models. We provide a result that characterizes the quantity and quality of insights that must be
43"
INTRODUCTION,0.09259259259259259,"obtained as a function of the severity of harmful correlations. Empirically, ROBOSHOT achieves
44"
INTRODUCTION,0.09465020576131687,"15.98% improvement across nine image and NLP datasets while offering sufficient versatility to apply
45"
INTRODUCTION,0.09670781893004116,"to a diverse variety of base models. Most excitingly, in certain cases, it reaches comparable or greater
46"
INTRODUCTION,0.09876543209876543,"improvements even when compared to fine-tuned models that rely on labeled data.
47"
INTRODUCTION,0.10082304526748971,"Our contributions include,
48"
A SIMPLE THEORETICAL MODEL DESCRIBING ZERO-SHOT MODEL FAILURES ALONG WITH A THEORETICAL,0.102880658436214,"1. A simple theoretical model describing zero-shot model failures along with a theoretical
49"
A SIMPLE THEORETICAL MODEL DESCRIBING ZERO-SHOT MODEL FAILURES ALONG WITH A THEORETICAL,0.10493827160493827,"analysis of our approach that characterizes the amount of information required for obtaining
50"
A SIMPLE THEORETICAL MODEL DESCRIBING ZERO-SHOT MODEL FAILURES ALONG WITH A THEORETICAL,0.10699588477366255,"improvements as a function of the most harmful unwanted correlation,
51"
A SIMPLE THEORETICAL MODEL DESCRIBING ZERO-SHOT MODEL FAILURES ALONG WITH A THEORETICAL,0.10905349794238683,"2. ROBOSHOT, an algorithm that implements our core idea. It extracts insights from foundation
52"
A SIMPLE THEORETICAL MODEL DESCRIBING ZERO-SHOT MODEL FAILURES ALONG WITH A THEORETICAL,0.1111111111111111,"models and uses them to improve zero-shot representations,
53"
A SIMPLE THEORETICAL MODEL DESCRIBING ZERO-SHOT MODEL FAILURES ALONG WITH A THEORETICAL,0.11316872427983539,"3. Extensive experimental evidence on zero-shot language and multimodal models, showing
54"
A SIMPLE THEORETICAL MODEL DESCRIBING ZERO-SHOT MODEL FAILURES ALONG WITH A THEORETICAL,0.11522633744855967,"improved worst-group accuracy of 15.98% across nine image and NLP datasets.
55"
RELATED WORK,0.11728395061728394,"2
Related Work
56"
RELATED WORK,0.11934156378600823,"We describe related work in zero-shot model robustness, debiasing embeddings, guiding multi-modal
57"
RELATED WORK,0.12139917695473251,"models using language, and using LMs as prior information.
58"
RELATED WORK,0.12345679012345678,"Zero-Shot inference robustness.
Improving model robustness to unwanted correlations is heav-
59"
RELATED WORK,0.12551440329218108,"ily studied [SKHL19, ABGLP19, KCJ+21, KIW22, LHC+21, LCT+22]. Some methods require
60"
RELATED WORK,0.12757201646090535,"training from scratch and are less practical when applied to large pretrained architectures. Existing
61"
RELATED WORK,0.12962962962962962,"approaches to improve robustness post-pretraining predominantly focus on fine-tuning. [YNPM23]
62"
RELATED WORK,0.13168724279835392,"detects spurious attribute descriptions and fine-tunes using these descriptions. Specialized contrastive
63"
RELATED WORK,0.1337448559670782,"loss is used to fine-tune a pretrained architecture in [GKG+22] and to train an adapter on the frozen
64"
RELATED WORK,0.13580246913580246,"embeddings in [ZR22]. While promising, fine-tuning recreates traditional machine learning pipelines
65"
RELATED WORK,0.13786008230452676,"(e.g., labeling, training, etc.), which contradicts the promise of zero-shot models. In contrast, our
66"
RELATED WORK,0.13991769547325103,"goal is to avoid any training and any use of labeled data.
67"
RELATED WORK,0.1419753086419753,"Debiasing embeddings.
A parallel line of work seeks to de-bias text embeddings [AZS+]
68"
RELATED WORK,0.1440329218106996,"[BCZ+16] [DP19] [LGPV20] and multimodal embeddings [WZS22, BHB+22, WLW21] by re-
69"
RELATED WORK,0.14609053497942387,"(a)
(b)"
RELATED WORK,0.14814814814814814,"Figure 2: (a) ROBOSHOT debiases original input embedding (left). The projected embedding (right)’s
variance in the unwanted direction is reduced, and in the relevant direction increases. (b) Embedding
projection. We project embeddings to the space orthogonal to the embeddings of all unwanted insights
(e.g., water and land)"
RELATED WORK,0.15020576131687244,"moving subspaces that contain harmful or unwanted concepts. We use a similar procedure as a
70"
RELATED WORK,0.1522633744855967,"building block. However, these methods either target specific fixed concepts (such as gender) or rely
71"
RELATED WORK,0.15432098765432098,"on concept annotations, which limits their applicability across a wide range of tasks. In contrast, our
72"
RELATED WORK,0.15637860082304528,"method automates getting both beneficial and unwanted concepts solely from the task descriptions.
73"
RELATED WORK,0.15843621399176955,"An additional difference is that our goal is simply to add robustness at low or zero-cost; we not seek
74"
RELATED WORK,0.16049382716049382,"to produce fully-invariant representations as is often desired for word embeddings.
75"
RELATED WORK,0.16255144032921812,"Using language to improve visual tasks
A large body of work has shown the efficacy of using
76"
RELATED WORK,0.1646090534979424,"language to improve performance on vision tasks [RKH+21, FCS+13, LCLBC20]. Most relevant
77"
RELATED WORK,0.16666666666666666,"are those that focus on robustness, like [PDN+22], where attention maps using multimodal models
78"
RELATED WORK,0.16872427983539096,"(like CLIP) are used as extra supervision to train a downstream image classifier. [YNPM23] uses
79"
RELATED WORK,0.17078189300411523,"text descriptions of spurious attributes in a fine-tuning loss to improve robustness against spurious
80"
RELATED WORK,0.1728395061728395,"correlations. In contrast to these works, we focus on using textual concepts to improve zero-shot
81"
RELATED WORK,0.1748971193415638,"model robustness—without fine-tuning.
82"
RELATED WORK,0.17695473251028807,"Language model as prior
The basis of our work comes from the observation that language models
83"
RELATED WORK,0.17901234567901234,"contain information that can serve as a prior for other learning tasks. [KNST23] finds that LLMs can
84"
RELATED WORK,0.18106995884773663,"perform causal reasoning tasks, substantially outperforming existing methods. [CCSE22] explicitly
85"
RELATED WORK,0.1831275720164609,"prompts LLMs for task-specific priors, leading to substantial performance improvements in feature
86"
RELATED WORK,0.18518518518518517,"selection, reinforcement learning, and causal discovery. Our work shares the spirit of these approaches
87"
RELATED WORK,0.18724279835390947,"in using the insights embedded in language models to enhance zero-shot robustness.
88"
RELATED WORK,0.18930041152263374,"3
RoboShot: Robustifying Zero-shot Models
89"
RELATED WORK,0.19135802469135801,"We are ready to provide our setup and describe the algorithm.
90"
MODELING AND SETUP,0.1934156378600823,"3.1
Modeling and setup
91"
MODELING AND SETUP,0.19547325102880658,"Suppose that the zero-shot model’s latent space contains an (unknown) concept set; similar notions
92"
MODELING AND SETUP,0.19753086419753085,"have been studied frequently in the literature [DKA+]. For simplicity, we assume that this concept
93"
MODELING AND SETUP,0.19958847736625515,"set is given by the orthonormal vectors {z1, . . . , zk}. The model’s encoder produces, for a particular
94"
MODELING AND SETUP,0.20164609053497942,input a representation x that is a mixture of concepts P
MODELING AND SETUP,0.2037037037037037,"i γizi, where γi ≥0 are weights.
95"
MODELING AND SETUP,0.205761316872428,"We shall work with the following theoretical model for zero-shot classification. It closely resembles
96"
MODELING AND SETUP,0.20781893004115226,"models like CLIP. For simplicity, we assume that there are two classes. It is straightforward to extend
97"
MODELING AND SETUP,0.20987654320987653,Algorithm 1: ROBOSHOT
MODELING AND SETUP,0.21193415637860083,"1: Parameters: Input embedding x, class embeddings c0, c1, harmful insight representations
v1, . . . , v|S|, helpful insight representations u1, . . . , u|R|"
MODELING AND SETUP,0.2139917695473251,"2: for j ∈{1, 2, . . . , |S|} do
3:
Reject harmful insight vj: set x ←x −⟨x, vj⟩/⟨vj, vj⟩vj"
MODELING AND SETUP,0.21604938271604937,"4:
Renormalize x = x/ ∥x∥
5: end for
6: for k ∈{1, 2, . . . , |R|} do
7:
Increase helpful insight uk: set x ←x + ⟨x, uk⟩/⟨uk, uk⟩uk"
MODELING AND SETUP,0.21810699588477367,"8: end for
9: ˆc = 1{xT c0 < xT c1}
10: Returns: Robustified zero-shot prediction ˆc"
MODELING AND SETUP,0.22016460905349794,"the analysis below to multiple classes. We take P
i αizi to be the embedding of a datapoint, while
98"
MODELING AND SETUP,0.2222222222222222,c0 = P
MODELING AND SETUP,0.2242798353909465,"i βi,0zi is the embedding of the first class and c1 = P"
MODELING AND SETUP,0.22633744855967078,"i βi,1zi is that of the second. Finally,
99"
MODELING AND SETUP,0.22839506172839505,"we assume that we have access to m answers v1, . . . , vm from the queries to the language model.
100"
MODELING AND SETUP,0.23045267489711935,These are given by vj = P
MODELING AND SETUP,0.23251028806584362,"i γi,jzi for j ≤m. We call these insight representations. Without our
101"
MODELING AND SETUP,0.2345679012345679,"approach, the prediction is made by 1{(P"
MODELING AND SETUP,0.2366255144032922,i αizi)T (P
MODELING AND SETUP,0.23868312757201646,"i βi,0zi) < (P"
MODELING AND SETUP,0.24074074074074073,i αizi)T (P
MODELING AND SETUP,0.24279835390946503,"i βi,1zi)}, so that
102"
MODELING AND SETUP,0.2448559670781893,"we predict whichever class has higher inner product with the datapoint’s embedding.
103"
MODELING AND SETUP,0.24691358024691357,"Next, we assume that each input representation x can be represented by partitioning the mixture
104"
MODELING AND SETUP,0.24897119341563786,"components into three groups,
105 x = S
X"
MODELING AND SETUP,0.25102880658436216,"s
αharmful
s
zs + R
X"
MODELING AND SETUP,0.25308641975308643,"r
αhelpful
r
zr + B
X"
MODELING AND SETUP,0.2551440329218107,"b
αbenign
b
zb."
MODELING AND SETUP,0.257201646090535,"The same holds for class and insight representations.
106"
MODELING AND SETUP,0.25925925925925924,"Example
We illustrate how harmful correlations produce errors on rare slices of data through a
107"
MODELING AND SETUP,0.2613168724279835,"standard task setting, Waterbirds [KSM+21]. In this dataset, the goal is to classify landbirds versus
108"
MODELING AND SETUP,0.26337448559670784,"waterbirds, and the background (land or water) is spurious. Suppose that we have these terms
109"
MODELING AND SETUP,0.2654320987654321,"relate to concepts such that zwater = −zland and zwaterbird = −zlandbird.
110"
MODELING AND SETUP,0.2674897119341564,"Consider a datapoint coming from a rare slice infrequently encountered in the training set. This might
111"
MODELING AND SETUP,0.26954732510288065,"be an image of a landbird over water. Its embedding might be x = 0.7zwater + 0.3zlandbird. We may
112"
MODELING AND SETUP,0.2716049382716049,"also have that
113"
MODELING AND SETUP,0.2736625514403292,cwaterbird = 0.4zwater + 0.6zwaterbird and clandbird = 0.4zland + 0.6zlandbird.
MODELING AND SETUP,0.2757201646090535,"Then, xT cwaterbird = 0.1 > xT clandbird = −0.1, so that the prediction is waterbird, and thus
114"
MODELING AND SETUP,0.2777777777777778,"incorrect. This is caused by the presence of harmful components in both the class embedding (caused
115"
MODELING AND SETUP,0.27983539094650206,"by seeing too many images with water described as waterbirds) and the datapoint embedding (where
116"
MODELING AND SETUP,0.28189300411522633,"the water background appears). Thus our goal is to remove harmful components (the zs’s) and boost
117"
MODELING AND SETUP,0.2839506172839506,"helpful components (the zr’s). We explain our approach towards doing so next.
118"
MODELING AND SETUP,0.28600823045267487,"3.2
ROBOSHOT: Zeroshot robustification with LLM
119"
MODELING AND SETUP,0.2880658436213992,"We describe ROBOSHOT in Algorithm 1. It uses representations of insights from language models to
120"
MODELING AND SETUP,0.29012345679012347,"shape input and class embeddings to remove harmful components and boost helpful ones. Figure
121"
MODELING AND SETUP,0.29218106995884774,"2 is helpful in understanding the intuition behind these procedures. The left part (a) illustrates the
122"
MODELING AND SETUP,0.294238683127572,"effect of ROBOSHOT on a true dataset. Note how unhelpful directions are neutralized while others
123"
MODELING AND SETUP,0.2962962962962963,"are boosted. The illustration on the right (b) shows this effect on the waterbirds running example.
124"
MODELING AND SETUP,0.29835390946502055,"Obtaining insight representations from LMs
The first question is how to obtain insight repre-
125"
MODELING AND SETUP,0.3004115226337449,"sentations without training. To do so in a zero-shot way, we use textual descriptions of harmful and
126"
MODELING AND SETUP,0.30246913580246915,"helpful concepts by querying language models using only the task description. For example, in the
127"
MODELING AND SETUP,0.3045267489711934,"Waterbirds dataset, we use the prompt “What are the biased/spurious differences between waterbirds
128"
MODELING AND SETUP,0.3065843621399177,"and landbirds?”. We list the details of the prompts used in the Appendix. Let s1, s2 be the text
129"
MODELING AND SETUP,0.30864197530864196,"insights obtained from the answer (e.g., {‘water background,’ ‘land background’}). We obtain
130"
MODELING AND SETUP,0.31069958847736623,"a spurious insight representation by taking the difference of their embedding v =
g(s1) −g(s2)
∥g(s1) −g(s2)∥,
131"
MODELING AND SETUP,0.31275720164609055,"where g is the text encoder of our model.
132"
MODELING AND SETUP,0.3148148148148148,"In addition to attempting to discover harmful correlations, we seek to discover helpful components
133"
MODELING AND SETUP,0.3168724279835391,"in order to boost their magnitudes past remaining harmful ones (or noise). The procedure is similar.
134"
MODELING AND SETUP,0.31893004115226337,"We obtain insight representations using language models. For example, we ask “What are the true
135"
MODELING AND SETUP,0.32098765432098764,"characteristics of waterbirds and landbirds?’ and obtain e.g., {‘short beak,’ ‘long beak’}. The
136"
MODELING AND SETUP,0.3230452674897119,"remainder of the procedure is identical to the case of harmful components. Note that since we
137"
MODELING AND SETUP,0.32510288065843623,"are seeking to boost (rather than remove) components, it is also possible to fix a multiplicative
138"
MODELING AND SETUP,0.3271604938271605,"constant (to be treated as a hyperparameter) for the boosting procedure. That is, we could take
139"
MODELING AND SETUP,0.3292181069958848,"x ←x + ν × ⟨x, uk⟩/⟨uk, uk⟩uk for some ν > 0. While this is possible if we have access to a
140"
MODELING AND SETUP,0.33127572016460904,"labeled set that we can tune ν over, we intentionally avoid doing so to ensure our procedure is truly
141"
MODELING AND SETUP,0.3333333333333333,"zero-shot.
142"
MODELING AND SETUP,0.33539094650205764,"Prompting a language model is typically inexpensive, which will enable obtaining multiple insight
143"
MODELING AND SETUP,0.3374485596707819,"vectors ˜v1, . . . , ˜vm. From these, we obtain an orthogonal basis v1, . . . , vm separately for harmful
144"
MODELING AND SETUP,0.3395061728395062,"and helpful components. Thus we have access to recovered subspaces spanned by such components.
145"
MODELING AND SETUP,0.34156378600823045,"Removing and Boosting Components
ROBOSHOT applies simple vector rejection to mitigate or
146"
MODELING AND SETUP,0.3436213991769547,"remove harmful components, which is described in lines 2-5 of Algorithm 1. Similarly, it boosts
147"
MODELING AND SETUP,0.345679012345679,"helpful components as described in lines 6-9.
148"
MODELING AND SETUP,0.3477366255144033,"To see the impact of doing so, consider our earlier example. Suppose that vharmful = 0.9zwater +
149"
MODELING AND SETUP,0.3497942386831276,"0.1zlandbird, and that this is our only harmful insight. Similarly, suppose that we obtain a single
150"
MODELING AND SETUP,0.35185185185185186,"helpful insight given by vhelpful = 0.1zwater + 0.9zlandbird. Note that even these insights can be
151"
MODELING AND SETUP,0.35390946502057613,"imperfect: they do not uniquely identify what are harmful or helpful concepts, as they have non-zero
152"
MODELING AND SETUP,0.3559670781893004,"weights on other components.
153"
MODELING AND SETUP,0.35802469135802467,"We first obtain from removing the harmful component (ignoring normalization for ease of calculation)
154"
MODELING AND SETUP,0.360082304526749,"that
155"
MODELING AND SETUP,0.36213991769547327,"ˆx ←x −
⟨x, vharmful⟩
⟨vharmful, vharmful⟩vharmful = −0.0244zwater + 0.2195zlandbird."
MODELING AND SETUP,0.36419753086419754,"Then, we already we have that xT cwaterbird = −0.1415 < xT clandbird = 0.1415, so that the correct
156"
MODELING AND SETUP,0.3662551440329218,"class is obtained. In other words we have already, from having access to a single insight, neutralized
157"
MODELING AND SETUP,0.3683127572016461,"a harmful correlation and corrected what had been an error. Adding in the helpful component further
158"
MODELING AND SETUP,0.37037037037037035,"helps. We obtain
159"
MODELING AND SETUP,0.3724279835390947,"ˆx ←ˆx +
⟨ˆx, vhelpful⟩
⟨vhelpful, vhelpful⟩vhelpful = −0.0006zwater + 0.4337zlandbird."
MODELING AND SETUP,0.37448559670781895,"This further increases our margin. Note that it is not necessary to fully neutralize (i.e., to be fully
160"
MODELING AND SETUP,0.3765432098765432,"invariant to) spurious or harmful components in our embeddings. The only goal is to ensure, as much
161"
MODELING AND SETUP,0.3786008230452675,"as possible, that their magnitudes are reduced when compared to helpful components (and to benign
162"
MODELING AND SETUP,0.38065843621399176,"components). In the following section, we provide a theoretical model for the magnitudes of such
163"
MODELING AND SETUP,0.38271604938271603,"components and characterize the conditions under which it will be possible to correct zero-shot errors.
164"
MODELING AND SETUP,0.38477366255144035,"We note that there is a variant of our approach that can also update class embeddings as well.
165"
ANALYSIS,0.3868312757201646,"4
Analysis
166"
ANALYSIS,0.3888888888888889,"Next, we provide an analysis that characterizes under what conditions ROBOSHOT is capable of
167"
ANALYSIS,0.39094650205761317,"correcting zero-shot errors. First, we consider the following error model on the weights of the various
168"
ANALYSIS,0.39300411522633744,"representations. For all benign representations, we assume that αb, βb, γb ∼N(0, σ2
benign). That is,
169"
ANALYSIS,0.3950617283950617,"the magnitudes of benign components are drawn from a Gaussian distribution. The value of σbenign is
170"
ANALYSIS,0.39711934156378603,"a function of the amount of data and the training procedure for the zero-shot model.
171"
ANALYSIS,0.3991769547325103,"Next, we assume that the embedding insight vs = Pk
i=1 γi,szi (where 1 ≤s ≤S) satisfies the
172"
ANALYSIS,0.4012345679012346,"property that for i ̸= s, γi,s ∼N(0, σ2
insight), while γs,s is a constant. In other words, the vectors
173"
ANALYSIS,0.40329218106995884,"v1, . . . , vS spanning the harmful component subspace are well-aligned with genuinely harmful
174"
ANALYSIS,0.4053497942386831,"concepts, but are also affected by noise. We seek to understand the interplay between this noise,
175"
ANALYSIS,0.4074074074074074,"benign noise, and the coefficients of the other vectors (i.e., helpful components). Let the result of
176"
ANALYSIS,0.4094650205761317,"rejecting embedding insights v1, . . . , vS be
177"
ANALYSIS,0.411522633744856,"ˆx = x − S
X s=1"
ANALYSIS,0.41358024691358025,"xT vs
||vs||2vs =
X"
ANALYSIS,0.4156378600823045,"i
Aizi."
ANALYSIS,0.4176954732510288,"We provide a bound on As, the coefficient of a targeted harmful concept post-removal.
178"
ANALYSIS,0.41975308641975306,"Theorem 4.1. Under the noise model described above, the post-removal coefficient for harmful
179"
ANALYSIS,0.4218106995884774,"concept s satisfies
180"
ANALYSIS,0.42386831275720166,|E [As] | ≤
ANALYSIS,0.42592592592592593,"(k −1)αsσ2
insight
γ2s,s +  S
X t̸=s"
ANALYSIS,0.4279835390946502,"αsσ2
insight
γ2
t,t ,"
ANALYSIS,0.43004115226337447,"where k is the number of concepts.
181"
ANALYSIS,0.43209876543209874,"The theorem illustrates how and when the rejection component of ROBOSHOT works—it scales
182"
ANALYSIS,0.43415637860082307,"down harmful coefficients at a rate inversely proportional to the harmful coefficients of the insight
183"
ANALYSIS,0.43621399176954734,"embeddings. As we would hope, when insight embeddings have larger coefficients for harmful vectors
184"
ANALYSIS,0.4382716049382716,"(i.e., are more precise in specifying terms that are not useful), ROBOSHOT yields better outcomes.
185"
ANALYSIS,0.4403292181069959,"In addition, we observe that the harmful coefficients decrease when the insight embeddings have
186"
ANALYSIS,0.44238683127572015,"less noise. In fact, we have that limσinsight→0 As = 0 — the case of perfectly identifying harmful
187"
ANALYSIS,0.4444444444444444,"concepts. In the Appendix, we present additional theoretical results for control of helpful coefficients
188"
ANALYSIS,0.44650205761316875,"along with a combined result.
189"
EXPERIMENTAL RESULTS,0.448559670781893,"5
Experimental Results
190"
EXPERIMENTAL RESULTS,0.4506172839506173,"This section evaluates the following claims about ROBOSHOT:
191"
EXPERIMENTAL RESULTS,0.45267489711934156,"• Improving multi-modal models (Section 5.1): ROBOSHOT improves zero-shot classification
192"
EXPERIMENTAL RESULTS,0.4547325102880658,"robustness of various multi-modal models, even outperforming prompting techniques that include
193"
EXPERIMENTAL RESULTS,0.4567901234567901,"spurious insight descriptions (which we do not have access to) in the label prompts.
194"
EXPERIMENTAL RESULTS,0.4588477366255144,"• Improving language models (Section 5.2): ROBOSHOT improves zero-shot robustness when
195"
EXPERIMENTAL RESULTS,0.4609053497942387,"using language model embeddings for text zero-shot classification.
196"
EXPERIMENTAL RESULTS,0.46296296296296297,"• Extracting concepts from LM with varying capacities (Section 5.3): ROBOSHOT can extract
197"
EXPERIMENTAL RESULTS,0.46502057613168724,"insights from language models with varying capacities. Improvements persist with weaker LMs.
198"
EXPERIMENTAL RESULTS,0.4670781893004115,"• Ablations (Section 5.4) ROBOSHOT benefits from both removing harmful and boosting helpful
199"
EXPERIMENTAL RESULTS,0.4691358024691358,"representations (line 3 and line 7 in ROBOSHOT Algorithm 1).
200"
EXPERIMENTAL RESULTS,0.4711934156378601,"Metrics and how to interpret the results. We use three metrics: average accuracy % (AVG),
201"
EXPERIMENTAL RESULTS,0.4732510288065844,"worst-group accuracy % (WG), and the gap between the two (Gap). While a model that relies on
202"
EXPERIMENTAL RESULTS,0.47530864197530864,"harmful correlations may achieve high AVG when such correlations are present in the majority of the
203"
EXPERIMENTAL RESULTS,0.4773662551440329,"test data, it may fail in settings where the correlation is absent. A robust model should have high
204"
EXPERIMENTAL RESULTS,0.4794238683127572,"AVG and WG, with a small gap between them.
205"
EXPERIMENTAL RESULTS,0.48148148148148145,"Baselines We compare against the following sets of baselines:
206"
EXPERIMENTAL RESULTS,0.4835390946502058,"1. Multimodal baselines: We compare against: (i) vanilla zero-shot classification (ZS) and (ii)
207"
EXPERIMENTAL RESULTS,0.48559670781893005,"zero-shot classification with group information (Group Prompt ZS). We do so across a variety of
208"
EXPERIMENTAL RESULTS,0.4876543209876543,"models: CLIP (ViT-B-32 and ViT-L-14) [RKH+21], ALIGN [JYX+21], and AltCLIP [CLZ+22].
209"
EXPERIMENTAL RESULTS,0.4897119341563786,"Group Prompt ZS assumes access to spurious or harmful insight annotations and includes them
210"
EXPERIMENTAL RESULTS,0.49176954732510286,"in the label prompt. For instance, the label prompts for waterbirds dataset become [waterbird
211"
EXPERIMENTAL RESULTS,0.49382716049382713,"with water background, waterbird with land background, landbird with water
212"
EXPERIMENTAL RESULTS,0.49588477366255146,"background, landbird with land background]. We only report Group Prompt ZS results
213"
EXPERIMENTAL RESULTS,0.49794238683127573,"on datasets where spurious insight annotations are available.
214"
EXPERIMENTAL RESULTS,0.5,"2. Language model baselines: We compare against zero-shot classification using multiple language
215"
EXPERIMENTAL RESULTS,0.5020576131687243,"model embeddings, including BERT [RG19] and Ada [NXP+22] (ZS).
216"
IMPROVING MULTI-MODAL MODELS,0.5041152263374485,"5.1
Improving multi-modal models
217 218"
IMPROVING MULTI-MODAL MODELS,0.5061728395061729,"Setup. We experimented on five binary and multi-class datasets with spurious correlations and
219"
IMPROVING MULTI-MODAL MODELS,0.5082304526748971,"distribution shifts, coming from a variety of domains: Waterbirds [SKHL19], CelebA [LLWT15],
220"
IMPROVING MULTI-MODAL MODELS,0.5102880658436214,"CXR14 [WPL+17], PACS [LYSH17], and VLCS [FXR13]. We use the default test splits of all
221"
IMPROVING MULTI-MODAL MODELS,0.5123456790123457,"datasets. Dataset details are provided in the appendix. For CXR14, we use BiomedCLIP [ZXU+23],
222"
IMPROVING MULTI-MODAL MODELS,0.51440329218107,"Table 1: Main results. Best WG and Gap performance bolded, second best underlined."
IMPROVING MULTI-MODAL MODELS,0.5164609053497943,"Dataset
Model
ZS
GroupPrompt ZS
ROBOSHOT"
IMPROVING MULTI-MODAL MODELS,0.5185185185185185,"AVG
WG(↑)
Gap(↓)
AVG
WG(↑)
Gap(↓)
AVG
WG(↑)
Gap(↓)"
IMPROVING MULTI-MODAL MODELS,0.5205761316872428,"Waterbirds
CLIP (ViT-B-32)
80.7
27.9
52.8
81.6
43.5
38.1
82.0
54.4
28.6
CLIP (ViT-L-14)
88.7
27.3
61.4
70.7
10.4
60.3
79.9
45.2
34.7
ALIGN
72.0
50.3
21.7
72.5
5.8
66.7
50.9
41.0
9.9
AltCLIP
90.1
35.8
54.3
82.4
29.4
53.0
78.5
54.8
23.7"
IMPROVING MULTI-MODAL MODELS,0.522633744855967,"CelebA
CLIP (ViT-B-32)
80.1
72.7
7.4
80.4
74.9
5.5
84.8
80.5
4.3
CLIP (ViT-L-14)
80.6
74.3
6.3
77.9
68.9
9.0
85.5
82.6
2.9
ALIGN
81.8
77.2
4.6
78.3
67.4
10.9
86.3
83.4
2.9
AltCLIP
82.3
79.7
2.6
82.3
79.0
3.3
86.0
77.2
8.8"
IMPROVING MULTI-MODAL MODELS,0.5246913580246914,"PACS
CLIP (ViT-B-32)
96.7
82.1
14.6
97.9
82.7
15.2
97.0
86.3
10.7
CLIP (ViT-L-14)
98.1
79.8
18.3
98.2
86.6
11.6
98.1
83.9
14.2
ALIGN
95.8
77.1
18.7
96.5
65.0
31.5
95.0
73.8
21.2
AltCLIP
98.5
82.6
15.9
98.6
85.4
13.2
98.7
89.5
9.2"
IMPROVING MULTI-MODAL MODELS,0.5267489711934157,"VLCS
CLIP (ViT-B-32)
75.6
20.5
55.1
-
76.5
33.0
43.5
CLIP (ViT-L-14)
72.6
4.20
68.4
-
71.1
12.6
58.5
ALIGN
78.8
33.0
45.8
-
77.6
39.8
37.8
AltCLIP
78.3
24.7
53.6
-
78.9
25.0
53.9"
IMPROVING MULTI-MODAL MODELS,0.5288065843621399,"CXR14
BiomedCLIP
55.3
28.9
26.4
-
56.2
41.6
14.6"
IMPROVING MULTI-MODAL MODELS,0.5308641975308642,"(a)
(b)"
IMPROVING MULTI-MODAL MODELS,0.5329218106995884,"Figure 3: (a) Original (green) and projected (red) input embeddings x, and label embeddings c0 and
c1. (b) label embeddings c0 and c1, harmful insight embeddings vk (black star) and helpful insight
embeddings uj (blue star)"
IMPROVING MULTI-MODAL MODELS,0.5349794238683128,"which is a variant of CLIP finetuned on biomedical images and articles. All experiments are conducted
223"
IMPROVING MULTI-MODAL MODELS,0.5370370370370371,"using frozen pretrained models.
224"
IMPROVING MULTI-MODAL MODELS,0.5390946502057613,"Results. Table 1 shows that ROBOSHOT significantly improves the worst group performance
225"
IMPROVING MULTI-MODAL MODELS,0.5411522633744856,"(WG) and maintains (and sometimes also improves) the overall average (AVG) without any auxiliary
226"
IMPROVING MULTI-MODAL MODELS,0.5432098765432098,"information (in contrast to Group Prompt, which requires access to spurious insight annotation).
227"
IMPROVING MULTI-MODAL MODELS,0.5452674897119342,"Improved robustness nearly across-the-board suggests that both the insights extracted from LMs and
228"
IMPROVING MULTI-MODAL MODELS,0.5473251028806584,"the representation modifications are useful. We also provide insights insights into the case where
229"
IMPROVING MULTI-MODAL MODELS,0.5493827160493827,"our method does not improve the baseline (ALIGN model on Waterbirds) in Fig. 3. In Fig. 3a, we
230"
IMPROVING MULTI-MODAL MODELS,0.551440329218107,"visualize the original and projected input embeddings (x in green and red points, respectively), and
231"
IMPROVING MULTI-MODAL MODELS,0.5534979423868313,"the label embeddings (c0 and c1). Fig. 3a (left) shows the embeddings from the ALIGN model. We
232"
IMPROVING MULTI-MODAL MODELS,0.5555555555555556,"observe that the projected embeddings (red) still lie within the original embedding space, even with
233"
IMPROVING MULTI-MODAL MODELS,0.5576131687242798,"reduced variance. In contrast, when examining the CLIP model embeddings (Figure 3a (right)), we
234"
IMPROVING MULTI-MODAL MODELS,0.5596707818930041,"observe that the projected embeddings are significantly distant from the original ones. Unsurprisingly,
235"
IMPROVING MULTI-MODAL MODELS,0.5617283950617284,"Figure 3b (left) reveals that vj and uk (harmful and helpful insight embeddings in black and blue
236"
IMPROVING MULTI-MODAL MODELS,0.5637860082304527,"stars, respectively) are not distinguishable in the text embedding space of ALIGN, collapsing the
237"
IMPROVING MULTI-MODAL MODELS,0.565843621399177,"input embeddings after ROBOSHOT is applied.
238"
IMPROVING MULTI-MODAL MODELS,0.5679012345679012,Table 2: ROBOSHOT text zero-shot classification. Best WG in bold.
IMPROVING MULTI-MODAL MODELS,0.5699588477366255,"Dataset
Model
ZS
ROBOSHOT"
IMPROVING MULTI-MODAL MODELS,0.5720164609053497,"AVG
WG(↑)
Gap(↓)
AVG
WG(↑)
Gap(↓)"
IMPROVING MULTI-MODAL MODELS,0.5740740740740741,"CivilComments
BERT
48.1
33.3
14.8
49.7
42.3
7.4
Ada
56.2
43.2
13.0
56.6
44.9
11.7"
IMPROVING MULTI-MODAL MODELS,0.5761316872427984,"HateXplain
BERT
60.4
0.0
60.4
57.3
14.0
43.3
Ada
62.8
14.3
48.5
63.6
21.1
42.5"
IMPROVING MULTI-MODAL MODELS,0.5781893004115226,"Amazon
BERT
81.1
64.2
16.8
81.0
64.4
16.6
Ada
81.2
63.4
17.8
82.9
63.8
19.1"
IMPROVING MULTI-MODAL MODELS,0.5802469135802469,"Gender Bias
BERT
84.8
83.7
1.1
85.1
84.9
0.2
Ada
77.9
60.0
17.9
78.0
60.1
17.9"
IMPROVING MULTI-MODAL MODELS,0.5823045267489712,"Table 3: ROBOSHOT with LMs of varying capacity. Best WG bolded, second best underlined"
IMPROVING MULTI-MODAL MODELS,0.5843621399176955,"Dataset
ZS
Ours (ChatGPT)
Ours (Flan-T5)
Ours (GPT2)
Ours (LLaMA)"
IMPROVING MULTI-MODAL MODELS,0.5864197530864198,"AVG
WG
AVG
WG
AVG
WG
AVG
WG
AVG
WG"
IMPROVING MULTI-MODAL MODELS,0.588477366255144,"Waterbirds
80.7
27.9
82.0
54.4
72.1
32.4
88.0
39.9
84.8
36.5"
IMPROVING MULTI-MODAL MODELS,0.5905349794238683,"CelebA
80.1
72.7
84.8
80.5
77.5
68.2
80.3
74.1
84.2
82.0"
IMPROVING MULTI-MODAL MODELS,0.5925925925925926,"PACS
96.7
82.1
97.0
86.3
96.2
80.3
97.2
74.0
94.8
71.9"
IMPROVING MULTI-MODAL MODELS,0.5946502057613169,"VLCS
75.6
20.5
76.5
33.0
69.6
20.5
75.5
26.1
72.0
18.2"
IMPROVING LANGUAGE MODELS,0.5967078189300411,"5.2
Improving language models
239"
IMPROVING LANGUAGE MODELS,0.5987654320987654,"Setup. We experimented on four text classification datasets: CivilComments-WILDS [BDS+19,
240"
IMPROVING LANGUAGE MODELS,0.6008230452674898,"KSM+21], HateXplain [MSY+21], Amazon-WILDS [NLM19, KSM+21] and Gender Bias clas-
241"
IMPROVING LANGUAGE MODELS,0.602880658436214,"sification dataset [DFW+20, MFB+17]. We use the default test splits of all datasets. In text
242"
IMPROVING LANGUAGE MODELS,0.6049382716049383,"experiments, the distinctions between harmful and helpful insights are less clear than for images.
243"
IMPROVING LANGUAGE MODELS,0.6069958847736625,"For this reason, we only use harmful vector rejection (line 3 in ROBOSHOT) in text experiments.
244"
IMPROVING LANGUAGE MODELS,0.6090534979423868,"CivilComments and HateXplain are toxic classification datasets with unwanted correlation between
245"
IMPROVING LANGUAGE MODELS,0.6111111111111112,"toxicity labels and mentions of demographics (e.g., male, female, mentions of religions). The datasets
246"
IMPROVING LANGUAGE MODELS,0.6131687242798354,"are annotated with demographic mentions of each text, and we directly use them to construct vj.
247"
IMPROVING LANGUAGE MODELS,0.6152263374485597,"For Amazon and Gender Bias datasets, we query LMs with task descriptions. All experiments are
248"
IMPROVING LANGUAGE MODELS,0.6172839506172839,"conducted using frozen pretrained models.
249"
IMPROVING LANGUAGE MODELS,0.6193415637860082,"Results. Table 2 shows that ROBOSHOT also improves zero-shot text classification in text datasets,
250"
IMPROVING LANGUAGE MODELS,0.6213991769547325,"as shown by our consistent boost over the baselines across all datasets.
251"
EXTRACTING CONCEPTS FROM LMS WITH VARYING CAPACITIES,0.6234567901234568,"5.3
Extracting concepts from LMs with varying capacities
252"
EXTRACTING CONCEPTS FROM LMS WITH VARYING CAPACITIES,0.6255144032921811,"Setup. We use LMs with different capacities: ChatGPT [OWJ+22], Flan-T5 [CHL+22], GPT2
253"
EXTRACTING CONCEPTS FROM LMS WITH VARYING CAPACITIES,0.6275720164609053,"[RWC+19], and LLaMA [TLI+23], to get harmful and helpful features insights (vj and uk).
254"
EXTRACTING CONCEPTS FROM LMS WITH VARYING CAPACITIES,0.6296296296296297,"Results. Table 3 shows that ROBOSHOT can get insights on vj and uk from LMs of various capacities
255"
EXTRACTING CONCEPTS FROM LMS WITH VARYING CAPACITIES,0.6316872427983539,"and improves zero-shot performance. Even though the the LM capacity correlates with the zero-shot
256"
EXTRACTING CONCEPTS FROM LMS WITH VARYING CAPACITIES,0.6337448559670782,"performance, ROBOSHOT with weaker LMs still outperforms zero-shot (ZS) baseline.
257"
ABLATIONS,0.6358024691358025,"5.4
Ablations
258"
ABLATIONS,0.6378600823045267,"Setup. We run ROBOSHOT with only harmful component mitigation (reject vj: ROBOSHOT line 3),
259"
ABLATIONS,0.6399176954732511,"only boosting helpful vectors (increase uk: ROBOSHOT line 7), and both.
260"
ABLATIONS,0.6419753086419753,"Results. The combination of both projections often achieves the best performance, as shown in Table
261"
ABLATIONS,0.6440329218106996,"4. Figure 4 provides insights into the impact of each projection. Rejecting vj reduces variance in one
262"
ABLATIONS,0.6460905349794238,"direction, while increasing uk amplifies variance in the orthogonal direction. When both projections
263"
ABLATIONS,0.6481481481481481,"are applied, they create a balanced mixture. We note that when doing both projections does not
264"
ABLATIONS,0.6502057613168725,"Table 4: Main results. Best WG and Gap performance bolded, second best underlined."
ABLATIONS,0.6522633744855967,"Dataset
Model
ZS
Ours (vj only)
Ours (uk only)
Ours (both)"
ABLATIONS,0.654320987654321,AVGWG(↑)Gap(↓)AVGWG(↑)Gap(↓)AVGWG(↑)Gap(↓)AVGWG(↑)Gap(↓)
ABLATIONS,0.6563786008230452,"Waterbirds
CLIP (ViT-B-32) 80.7 27.9
52.8 82.0 50.4
31.6 82.6 30.2
52.4 83.0 54.4
28.6
CLIP (ViT-L-14) 88.7 27.3
61.4 82.7 35.8
46.9 88.3 29.8
58.5 79.9 45.2
34.7
ALIGN
72.0 50.3
21.7 56.4 41.6
14.8 62.8 56.4
6.4
50.9 41.0
9.9
AltCLIP
90.1 35.8
54.3 81.4 59.0
22.4 89.1 35.2
53.9 78.5 54.8
23.7"
ABLATIONS,0.6584362139917695,"CelebA
CLIP (ViT-B-32) 80.1 72.7
7.4
85.2 81.5
3.7
79.6 71.3
8.3
84.8 80.5
4.3
CLIP (ViT-L-14) 80.6 74.3
6.3
85.9 82.8
3.1
80.0 73.1
6.9
85.5 82.6
2.9
ALIGN
81.8 77.2
4.6
83.9 78.0
5.7
83.9 81.4
2.5
86.3 83.4
2.9
AltCLIP
82.3 79.7
2.6
86.1 75.6
10.5 81.9 79.0
2.9
86.0 77.2
8.8"
ABLATIONS,0.6604938271604939,"PACS
CLIP (ViT-B-32) 96.7 82.1
14.6 97.0 83.7
13.3 96.6 84.2
12.4 97.0 86.3
10.7
CLIP (ViT-L-14) 98.1 79.8
18.3 98.0 79.8
18.2 98.1 83.8
14.3 98.1 83.9
14.2
ALIGN
95.8 77.1
18.7 95.8 78.0
17.8 95.1 71.1
24.0 95.0 73.8
21.2
AltCLIP
98.5 82.6
15.9 98.4 83.0
15.4 98.6 88.8
9.8
98.7 89.5
9.2"
ABLATIONS,0.6625514403292181,"VLCS
CLIP (ViT-B-32) 75.6 20.5
55.1 75.6 22.7
52.9 76.4 29.5
46.9 76.5 33.0
43.5
CLIP (ViT-L-14) 72.6
4.2
68.4 70.9
6.8
64.1 73.4
8.9
64.5 71.1 12.6
58.5
ALIGN
78.8 33.0
45.8 78.2 30.7
47.5 78.0 43.2
34.8 77.6 39.8
37.8
AltCLIP
78.3 24.7
53.6 77.5 24.4
53.1 79.0 20.5
58.5 78.9 25.0
53.9"
ABLATIONS,0.6646090534979424,"CXR14
BiomedCLIP
55.3 28.9
26.4 55.7 41.8
13.9 54.8 21.8
33.0 56.2 41.6
14.6"
ABLATIONS,0.6666666666666666,"Figure 4: The effect of vj (reject), uj (increase), and both projections"
ABLATIONS,0.668724279835391,"improve the baseline, using only uk or vj still outperforms the baseline. For instance, the ALIGN
265"
ABLATIONS,0.6707818930041153,"model in the Waterbirds dataset achieves the best performance with only uk projection. This suggests
266"
ABLATIONS,0.6728395061728395,"that in certain cases, harmful and helpful concepts are intertwined in the embedding space, and using
267"
ABLATIONS,0.6748971193415638,"just one projection can be beneficial. We leave further investigation to future work.
268"
CONCLUSION,0.676954732510288,"6
Conclusion
269"
CONCLUSION,0.6790123456790124,"We introduced ROBOSHOT, a fine-tuning-free system that robustifies zero-shot pretrained models in
270"
CONCLUSION,0.6810699588477366,"a truly zero-shot way. Theoretically, we characterized the quantities required to obtain improvements
271"
CONCLUSION,0.6831275720164609,"over vanilla zero-shot classification. Empirically, we found that ROBOSHOT improves both multi-
272"
CONCLUSION,0.6851851851851852,"modal and language model zero-shot performance, has sufficient versatility to apply to various base
273"
CONCLUSION,0.6872427983539094,"models, and can use insights from less powerful language models.
274"
REFERENCES,0.6893004115226338,"References
275"
REFERENCES,0.691358024691358,"[ABGLP19] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk
276"
REFERENCES,0.6934156378600823,"minimization. arXiv preprint arXiv:1907.02893, 2019.
277"
REFERENCES,0.6954732510288066,"[AZS+] Prince Osei Aboagye, Yan Zheng, Jack Shunn, Chin-Chia Michael Yeh, Junpeng
278"
REFERENCES,0.6975308641975309,"Wang, Zhongfang Zhuang, Huiyuan Chen, Liang Wang, Wei Zhang, and Jeff Phillips.
279"
REFERENCES,0.6995884773662552,"Interpretable debiasing of vectorized language representations with iterative orthogo-
280"
REFERENCES,0.7016460905349794,"nalization. In The Eleventh International Conference on Learning Representations.
281"
REFERENCES,0.7037037037037037,"[BCZ+16] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T
282"
REFERENCES,0.7057613168724279,"Kalai. Man is to computer programmer as woman is to homemaker? debiasing word
283"
REFERENCES,0.7078189300411523,"embeddings. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors,
284"
REFERENCES,0.7098765432098766,"Advances in Neural Information Processing Systems, volume 29. Curran Associates,
285"
REFERENCES,0.7119341563786008,"Inc., 2016.
286"
REFERENCES,0.7139917695473251,"[BDS+19] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
287"
REFERENCES,0.7160493827160493,"Nuanced metrics for measuring unintended bias with real data for text classification. In
288"
REFERENCES,0.7181069958847737,"Companion proceedings of the 2019 world wide web conference, pages 491–500, 2019.
289"
REFERENCES,0.720164609053498,"[BHB+22] Hugo Berg, Siobhan Mackenzie Hall, Yash Bhalgat, Wonsuk Yang, Hannah Rose Kirk,
290"
REFERENCES,0.7222222222222222,"Aleksandar Shtedritski, and Max Bain. A prompt array keeps the bias away: Debiasing
291"
REFERENCES,0.7242798353909465,"vision-language models with adversarial learning. arXiv preprint arXiv:2203.11933,
292"
REFERENCES,0.7263374485596708,"2022.
293"
REFERENCES,0.7283950617283951,"[CCSE22] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. Lmpriors: Pre-trained
294"
REFERENCES,0.7304526748971193,"language models as task-specific priors. arXiv preprint arXiv:2210.12530, 2022.
295"
REFERENCES,0.7325102880658436,"[CHL+22] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric
296"
REFERENCES,0.7345679012345679,"Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-
297"
REFERENCES,0.7366255144032922,"finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
298"
REFERENCES,0.7386831275720165,"[CLZ+22] Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, and Ledell
299"
REFERENCES,0.7407407407407407,"Wu. Altclip: Altering the language encoder in clip for extended language capabilities.
300"
REFERENCES,0.742798353909465,"arXiv preprint arXiv:2211.06679, 2022.
301"
REFERENCES,0.7448559670781894,"[DFW+20] Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, and Adina Williams.
302"
REFERENCES,0.7469135802469136,"Multi-dimensional gender bias classification. In Proceedings of the 2020 Conference on
303"
REFERENCES,0.7489711934156379,"Empirical Methods in Natural Language Processing (EMNLP), pages 314–331, Online,
304"
REFERENCES,0.7510288065843621,"November 2020. Association for Computational Linguistics.
305"
REFERENCES,0.7530864197530864,"[DKA+] Fahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Durrani, Jia Xu, and Hassan Sajjad.
306"
REFERENCES,0.7551440329218106,"Discovering latent concepts learned in bert. In International Conference on Learning
307"
REFERENCES,0.757201646090535,"Representations.
308"
REFERENCES,0.7592592592592593,"[DLS+18] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measur-
309"
REFERENCES,0.7613168724279835,"ing and mitigating unintended bias in text classification. 2018.
310"
REFERENCES,0.7633744855967078,"[DP19] Sunipa Dev and Jeff Phillips. Attenuating bias in word vectors. In The 22nd Inter-
311"
REFERENCES,0.7654320987654321,"national Conference on Artificial Intelligence and Statistics, pages 879–887. PMLR,
312"
REFERENCES,0.7674897119341564,"2019.
313"
REFERENCES,0.7695473251028807,"[FCS+13] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio
314"
REFERENCES,0.7716049382716049,"Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model.
315"
REFERENCES,0.7736625514403292,"Advances in neural information processing systems, 26, 2013.
316"
REFERENCES,0.7757201646090535,"[FXR13] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the
317"
REFERENCES,0.7777777777777778,"utilization of multiple datasets and web images for softening bias. In Proceedings of
318"
REFERENCES,0.779835390946502,"the IEEE International Conference on Computer Vision, pages 1657–1664, 2013.
319"
REFERENCES,0.7818930041152263,"[GKG+22] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan.
320"
REFERENCES,0.7839506172839507,"Finetune like you pretrain: Improved finetuning of zero-shot vision models. arXiv
321"
REFERENCES,0.7860082304526749,"preprint arXiv:2212.00638, 2022.
322"
REFERENCES,0.7880658436213992,"[JYX+21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
323"
REFERENCES,0.7901234567901234,"Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language
324"
REFERENCES,0.7921810699588477,"representation learning with noisy text supervision. In International Conference on
325"
REFERENCES,0.7942386831275721,"Machine Learning, pages 4904–4916. PMLR, 2021.
326"
REFERENCES,0.7962962962962963,"[KCJ+21] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas,
327"
REFERENCES,0.7983539094650206,"Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization
328"
REFERENCES,0.8004115226337448,"via risk extrapolation (rex). In International Conference on Machine Learning, pages
329"
REFERENCES,0.8024691358024691,"5815–5826. PMLR, 2021.
330"
REFERENCES,0.8045267489711934,"[KIW22] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training
331"
REFERENCES,0.8065843621399177,"is sufficient for robustness to spurious correlations. arXiv preprint arXiv:2204.02937,
332"
REFERENCES,0.808641975308642,"2022.
333"
REFERENCES,0.8106995884773662,"[KNST23] Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning
334"
REFERENCES,0.8127572016460906,"and large language models: Opening a new frontier for causality. arXiv preprint
335"
REFERENCES,0.8148148148148148,"arXiv:2305.00050, 2023.
336"
REFERENCES,0.8168724279835391,"[KSM+21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang,
337"
REFERENCES,0.8189300411522634,"Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena
338"
REFERENCES,0.8209876543209876,"Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International
339"
REFERENCES,0.823045267489712,"Conference on Machine Learning, pages 5637–5664. PMLR, 2021.
340"
REFERENCES,0.8251028806584362,"[LCLBC20] Yannick Le Cacheux, Hervé Le Borgne, and Michel Crucianu. Using sentences as
341"
REFERENCES,0.8271604938271605,"semantic representations in large scale zero-shot learning. In Computer Vision–ECCV
342"
REFERENCES,0.8292181069958847,"2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages
343"
REFERENCES,0.831275720164609,"641–645. Springer, 2020.
344"
REFERENCES,0.8333333333333334,"[LCT+22] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang,
345"
REFERENCES,0.8353909465020576,"and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. arXiv
346"
REFERENCES,0.8374485596707819,"preprint arXiv:2210.11466, 2022.
347"
REFERENCES,0.8395061728395061,"[LGPV20] Anne Lauscher, Goran Glavaš, Simone Paolo Ponzetto, and Ivan Vuli´c. A general
348"
REFERENCES,0.8415637860082305,"framework for implicit and explicit debiasing of distributional word vector spaces.
349"
REFERENCES,0.8436213991769548,"In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages
350"
REFERENCES,0.845679012345679,"8131–8138, 2020.
351"
REFERENCES,0.8477366255144033,"[LHC+21] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh,
352"
REFERENCES,0.8497942386831275,"Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group
353"
REFERENCES,0.8518518518518519,"robustness without training group information. In Marina Meila and Tong Zhang,
354"
REFERENCES,0.8539094650205762,"editors, Proceedings of the 38th International Conference on Machine Learning, volume
355"
REFERENCES,0.8559670781893004,"139 of Proceedings of Machine Learning Research, pages 6781–6792. PMLR, 18–24
356"
REFERENCES,0.8580246913580247,"Jul 2021.
357"
REFERENCES,0.8600823045267489,"[LLWT15] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes
358"
REFERENCES,0.8621399176954733,"in the wild. In Proceedings of the IEEE international conference on computer vision,
359"
REFERENCES,0.8641975308641975,"pages 3730–3738, 2015.
360"
REFERENCES,0.8662551440329218,"[LYSH17] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and
361"
REFERENCES,0.8683127572016461,"artier domain generalization. In Proceedings of the IEEE international conference on
362"
REFERENCES,0.8703703703703703,"computer vision, pages 5542–5550, 2017.
363"
REFERENCES,0.8724279835390947,"[MFB+17] Alexander Miller, Will Feng, Dhruv Batra, Antoine Bordes, Adam Fisch, Jiasen Lu,
364"
REFERENCES,0.8744855967078189,"Devi Parikh, and Jason Weston. ParlAI: A dialog research software platform. In
365"
REFERENCES,0.8765432098765432,"Proceedings of the 2017 Conference on Empirical Methods in Natural Language
366"
REFERENCES,0.8786008230452675,"Processing: System Demonstrations, pages 79–84, Copenhagen, Denmark, September
367"
REFERENCES,0.8806584362139918,"2017. Association for Computational Linguistics.
368"
REFERENCES,0.8827160493827161,"[MSY+21] Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and
369"
REFERENCES,0.8847736625514403,"Animesh Mukherjee. Hatexplain: A benchmark dataset for explainable hate speech
370"
REFERENCES,0.8868312757201646,"detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
371"
REFERENCES,0.8888888888888888,"pages 14867–14875, 2021.
372"
REFERENCES,0.8909465020576132,"[NLM19] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using
373"
REFERENCES,0.8930041152263375,"distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 confer-
374"
REFERENCES,0.8950617283950617,"ence on empirical methods in natural language processing and the 9th international
375"
REFERENCES,0.897119341563786,"joint conference on natural language processing (EMNLP-IJCNLP), pages 188–197,
376"
REFERENCES,0.8991769547325102,"2019.
377"
REFERENCES,0.9012345679012346,"[NXP+22] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry
378"
REFERENCES,0.9032921810699589,"Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and
379"
REFERENCES,0.9053497942386831,"code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.
380"
REFERENCES,0.9074074074074074,"[OWJ+22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
381"
REFERENCES,0.9094650205761317,"Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training
382"
REFERENCES,0.911522633744856,"language models to follow instructions with human feedback. Advances in Neural
383"
REFERENCES,0.9135802469135802,"Information Processing Systems, 35:27730–27744, 2022.
384"
REFERENCES,0.9156378600823045,"[PDN+22] Suzanne Petryk, Lisa Dunlap, Keyan Nasseri, Joseph Gonzalez, Trevor Darrell, and
385"
REFERENCES,0.9176954732510288,"Anna Rohrbach. On guiding visual attention with language specification. In Proceedings
386"
REFERENCES,0.9197530864197531,"of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
387"
REFERENCES,0.9218106995884774,"18092–18102, 2022.
388"
REFERENCES,0.9238683127572016,"[RG19] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese
389"
REFERENCES,0.9259259259259259,"bert-networks. arXiv preprint arXiv:1908.10084, 2019.
390"
REFERENCES,0.9279835390946503,"[RKH+21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
391"
REFERENCES,0.9300411522633745,"Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
392"
REFERENCES,0.9320987654320988,"ing transferable visual models from natural language supervision. In International
393"
REFERENCES,0.934156378600823,"conference on machine learning, pages 8748–8763. PMLR, 2021.
394"
REFERENCES,0.9362139917695473,"[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
395"
REFERENCES,0.9382716049382716,"et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
396"
REFERENCES,0.9403292181069959,"[SKHL19] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distribution-
397"
REFERENCES,0.9423868312757202,"ally robust neural networks for group shifts: On the importance of regularization for
398"
REFERENCES,0.9444444444444444,"worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.
399"
REFERENCES,0.9465020576131687,"[TE11] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR 2011,
400"
REFERENCES,0.948559670781893,"pages 1521–1528, 2011.
401"
REFERENCES,0.9506172839506173,"[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
402"
REFERENCES,0.9526748971193416,"Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,
403"
REFERENCES,0.9547325102880658,"et al.
Llama: Open and efficient foundation language models.
arXiv preprint
404"
REFERENCES,0.9567901234567902,"arXiv:2302.13971, 2023.
405"
REFERENCES,0.9588477366255144,"[WLW21] Jialu Wang, Yang Liu, and Xin Eric Wang. Are gender-neutral queries really gender-
406"
REFERENCES,0.9609053497942387,"neutral? mitigating gender bias in image search. arXiv preprint arXiv:2109.05433,
407"
REFERENCES,0.9629629629629629,"2021.
408"
REFERENCES,0.9650205761316872,"[WPL+17] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and
409"
REFERENCES,0.9670781893004116,"Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and bench-
410"
REFERENCES,0.9691358024691358,"marks on weakly-supervised classification and localization of common thorax diseases.
411"
REFERENCES,0.9711934156378601,"In Proceedings of the IEEE conference on computer vision and pattern recognition,
412"
REFERENCES,0.9732510288065843,"pages 2097–2106, 2017.
413"
REFERENCES,0.9753086419753086,"[WZS22] Junyang Wang, Yi Zhang, and Jitao Sang. Fairclip: Social bias elimination based
414"
REFERENCES,0.977366255144033,"on attribute prototype learning and representation neutralization.
arXiv preprint
415"
REFERENCES,0.9794238683127572,"arXiv:2210.14562, 2022.
416"
REFERENCES,0.9814814814814815,"[YNPM23] Yu Yang, Besmira Nushi, Hamid Palangi, and Baharan Mirzasoleiman.
Mitigat-
417"
REFERENCES,0.9835390946502057,"ing spurious correlations in multi-modal models during fine-tuning. arXiv preprint
418"
REFERENCES,0.98559670781893,"arXiv:2304.03916, 2023.
419"
REFERENCES,0.9876543209876543,"[ZR22] Michael Zhang and Christopher Ré. Contrastive adapters for foundation model group
420"
REFERENCES,0.9897119341563786,"robustness. arXiv preprint arXiv:2207.07180, 2022.
421"
REFERENCES,0.9917695473251029,"[ZXU+23] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston,
422"
REFERENCES,0.9938271604938271,"Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, Matthew Lungren, Tristan Naumann,
423"
REFERENCES,0.9958847736625515,"and Hoifung Poon. Large-scale domain-specific pretraining for biomedical vision-
424"
REFERENCES,0.9979423868312757,"language processing, 2023.
425"
