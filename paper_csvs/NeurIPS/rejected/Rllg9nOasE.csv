Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0009433962264150943,"Despite participants engaging in single modality stimuli, such as watching images
1"
ABSTRACT,0.0018867924528301887,"or silent videos, recent work has demonstrated that multi-modal Transformer
2"
ABSTRACT,0.002830188679245283,"models can predict visual brain activity impressively well, even with incongruent
3"
ABSTRACT,0.0037735849056603774,"modality representations. This raises the question of how accurately these multi-
4"
ABSTRACT,0.0047169811320754715,"modal models can predict brain activity when participants are engaged in multi-
5"
ABSTRACT,0.005660377358490566,"modal stimuli. As these models grow increasingly popular, their use in studying
6"
ABSTRACT,0.006603773584905661,"neural activity provides insights into how our brains respond to such multi-modal
7"
ABSTRACT,0.007547169811320755,"naturalistic stimuli, i.e., where it separates and integrates information from different
8"
ABSTRACT,0.008490566037735849,"sensory modalities. We investigate this question by using multiple unimodal
9"
ABSTRACT,0.009433962264150943,"and two types of multi-modal models—cross-modal and jointly pretrained—to
10"
ABSTRACT,0.010377358490566037,"determine which type of models is more relevant to fMRI brain activity when
11"
ABSTRACT,0.011320754716981131,"participants were engaged in watching movies (videos with audio). We observe that
12"
ABSTRACT,0.012264150943396227,"both types of multi-modal models show improved alignment in several language
13"
ABSTRACT,0.013207547169811321,"and visual regions. This study also helps in identifying which brain regions
14"
ABSTRACT,0.014150943396226415,"process unimodal versus multi-modal information. We further investigate the
15"
ABSTRACT,0.01509433962264151,"impact of removal of unimodal features from multi-modal representations and
16"
ABSTRACT,0.016037735849056604,"find that there is additional information beyond the unimodal embeddings that
17"
ABSTRACT,0.016981132075471698,"is processed in the visual and language regions. Based on this investigation, we
18"
ABSTRACT,0.017924528301886792,"find that while for cross-modal models, their brain alignment is partially attributed
19"
ABSTRACT,0.018867924528301886,"to the video modality; for jointly pretrained models, it is partially attributed to
20"
ABSTRACT,0.01981132075471698,"both the video and audio modalities. The inability of individual modalities in
21"
ABSTRACT,0.020754716981132074,"explaining the brain alignment effectiveness of multi-modal models suggests that
22"
ABSTRACT,0.02169811320754717,"multi-modal models capture additional information processed by all brain regions.
23"
ABSTRACT,0.022641509433962263,"This serves as a strong motivation for the neuro-science community to investigate
24"
ABSTRACT,0.02358490566037736,"the interpretability of these models for deepening our understanding of multi-modal
25"
ABSTRACT,0.024528301886792454,"information processing in brain.
26"
INTRODUCTION,0.02547169811320755,"1
Introduction
27"
INTRODUCTION,0.026415094339622643,"The study of brain encoding aims at predicting the neural brain activity recordings from an input
28"
INTRODUCTION,0.027358490566037737,"stimulus representation. Recent brain encoding studies use neural models as a powerful approach to
29"
INTRODUCTION,0.02830188679245283,"better understand the information processing in the brain in response to naturalistic stimuli (Oota
30"
INTRODUCTION,0.029245283018867925,"et al., 2023a). Current encoding models are trained and tested on brain responses captured from
31"
INTRODUCTION,0.03018867924528302,"participants who are engaged in a single stimulus modality, using stimulus representations extracted
32"
INTRODUCTION,0.031132075471698113,"from AI systems that are pretrained on single modality, such as language (Wehbe et al., 2014; Jain &
33"
INTRODUCTION,0.03207547169811321,"Huth, 2018; Toneva & Wehbe, 2019; Caucheteux & King, 2020; Schrimpf et al., 2021; Toneva et al.,
34"
INTRODUCTION,0.0330188679245283,"2022; Aw & Toneva, 2023), vision (Yamins et al., 2014; Eickenberg et al., 2017; Schrimpf et al.,
35"
INTRODUCTION,0.033962264150943396,"2018; Wang et al., 2019) or speech (Millet et al., 2022; Vaidya et al., 2022; Tuckute et al., 2023). In
36"
INTRODUCTION,0.03490566037735849,"this paper, we build encoding models where participants are engaged with multi-modal stimuli (e.g.,
37"
INTRODUCTION,0.035849056603773584,"watching movies that also include audio). We explore multi-modal stimulus representations extracted
38"
INTRODUCTION,0.03679245283018868,Video included with Audio
INTRODUCTION,0.03773584905660377,“The wolf of wall street”
INTRODUCTION,0.038679245283018866,movie video clip
INTRODUCTION,0.03962264150943396,Multi-modal naturalistic stimulus
INTRODUCTION,0.040566037735849055,"fMRI
Actual brain 
activations"
INTRODUCTION,0.04150943396226415,"Uni-modal Video 
Model (VM)
𝑓𝑓1 𝑉𝑉𝑀𝑀(𝑋𝑋) ≈
Ridge 
Regression (𝒇𝒇𝟏𝟏)"
INTRODUCTION,0.04245283018867924,"Uni-modal Speech 
Model (SM)
𝑓𝑓2 𝑆𝑆𝑀𝑀(𝑋𝑋) ≈
Ridge 
Regression (𝒇𝒇𝟐𝟐)"
INTRODUCTION,0.04339622641509434,"𝑉𝑉𝑉𝑉(𝑋𝑋)
Ridge 
Regression (r)
r VM X
≈𝐶𝐶𝐶𝐶(X)"
INTRODUCTION,0.04433962264150943,"|𝐶𝐶𝐶𝐶𝑋𝑋−𝑟𝑟(𝑉𝑉𝑉𝑉𝑋𝑋)|
Ridge 
Regression (g’)
𝑔𝑔′ 𝐶𝐶𝐶𝐶𝑋𝑋−𝑟𝑟𝑉𝑉𝑉𝑉𝑋𝑋
≈"
INTRODUCTION,0.045283018867924525,Cross-modality
INTRODUCTION,0.04622641509433962,Model (CM)
INTRODUCTION,0.04716981132075472,"Jointly-pretrained 
Model (JM) +"
INTRODUCTION,0.048113207547169815,ℎ𝐽𝐽𝐽𝐽(𝑋𝑋) ≈
INTRODUCTION,0.04905660377358491,"g 𝐶𝐶𝑀𝑀(𝑋𝑋) ≈
Ridge 
Regression (g)"
INTRODUCTION,0.05,"Ridge 
Regression (h)"
INTRODUCTION,0.0509433962264151,"Video 
Encoder"
INTRODUCTION,0.05188679245283019,"Audio 
Encoder (A) (B)"
INTRODUCTION,0.052830188679245285,Figure 1: (A) Overview of our proposed Multi-modal Brain Encoding Pipeline. (B) Residual Analysis.
INTRODUCTION,0.05377358490566038,"using Transformer (Vaswani et al., 2017) based multi-modal models. Our analysis focuses on their
39"
INTRODUCTION,0.05471698113207547,"alignment with both uni- and multi-modal brain regions.
40"
INTRODUCTION,0.05566037735849057,"There is a growing evidence that the human brain’s ability for multi-modal processing is underpinned
41"
INTRODUCTION,0.05660377358490566,"by synchronized cortical representations of identical concepts across various sensory modalities (Gau-
42"
INTRODUCTION,0.057547169811320756,"thier et al., 2003; Bracci & Op de Beeck, 2023). Reflecting similar principles, the recent advances in
43"
INTRODUCTION,0.05849056603773585,"AI systems have led to the development of multi-modal models (like CLIP (Radford et al., 2021),
44"
INTRODUCTION,0.059433962264150944,"ImageBind (Girdhar et al., 2023), and TVLT (Tang et al., 2022)) using massive interleaved image-text
45"
INTRODUCTION,0.06037735849056604,"data, speech-text data or video-audio-text data to represent multi-modal input. This recent progress in
46"
INTRODUCTION,0.06132075471698113,"AI has stimulated advancements in brain encoding models (Doerig et al., 2022; Oota et al., 2022;
47"
INTRODUCTION,0.062264150943396226,"Popham et al., 2021; Wang et al., 2022; Tang et al., 2024; Nakagi et al., 2024) that learn effectively
48"
INTRODUCTION,0.06320754716981132,"from multiple input modalities, despite participants being engaged with single stimulus modality
49"
INTRODUCTION,0.06415094339622641,"during experiments, e.g., watching natural scene images, or silent movie clips. However, these studies
50"
INTRODUCTION,0.06509433962264151,"have experimented with subjects engaged with single-modality stimulus, leaving the full potential of
51"
INTRODUCTION,0.0660377358490566,"these models in true multi-modal scenarios still unclear.
52"
INTRODUCTION,0.0669811320754717,"Using brain recordings of participants watching several popular movies included with audio (St-
53"
INTRODUCTION,0.06792452830188679,"Laurent et al., 2023), we investigate several research questions. First, we investigate the effectiveness
54"
INTRODUCTION,0.06886792452830189,"of multi-modal stimulus representations obtained using multi-modal models versus unimodal models
55"
INTRODUCTION,0.06981132075471698,"for brain encoding. Multi-modal models are of two broad types: (i) cross-modal pretrained models,
56"
INTRODUCTION,0.07075471698113207,"where first individual modality encoders are trained and then cross-modal alignment is performed, and
57"
INTRODUCTION,0.07169811320754717,"(ii) jointly pretrained models, which involve combining data from multiple modalities and training a
58"
INTRODUCTION,0.07264150943396226,"single joint encoder. Hence, we also investigate which of the two types (cross-modal versus joint) are
59"
INTRODUCTION,0.07358490566037736,"better for encoding. In this work, we focus on one cross-modal (ImageBind), one jointly pretrained
60"
INTRODUCTION,0.07452830188679245,"(TVLT), three video and two speech models. Additionally, we explore which modality representations
61"
INTRODUCTION,0.07547169811320754,"are more brain relevant, and identify which brain regions process uni- and multi-modal information.
62"
INTRODUCTION,0.07641509433962264,"Overall, this research utilizes various modality representations to develop encoding models based on
63"
INTRODUCTION,0.07735849056603773,"fMRI responses within a multi-modal model framework (see Fig. 1 for workflow).
64"
INTRODUCTION,0.07830188679245283,"Using our multi-modal brain encoding approach, we examine several insights. First, we use previous
65"
INTRODUCTION,0.07924528301886792,"neuroscience findings that have identified brain regions involved in visual, language and auditory
66"
INTRODUCTION,0.08018867924528301,"processing, and investigate how well our model aligns with these regions when both the model and a
67"
INTRODUCTION,0.08113207547169811,"human participant watch the same multi-modal video stimuli. Second, we expect that multi-modal
68"
INTRODUCTION,0.0820754716981132,"models which can learn cross-modal and joint embeddings across modalities in a brain-relevant
69"
INTRODUCTION,0.0830188679245283,"way would significantly align with these regions. However, alignment with these brain regions
70"
INTRODUCTION,0.08396226415094339,"doesn’t necessarily mean that the model is effectively learning from multiple modalities, as unimodal
71"
INTRODUCTION,0.08490566037735849,"models for vision or language or audio have also been shown to significantly align with these brain
72"
INTRODUCTION,0.08584905660377358,"regions (Wehbe et al., 2014; Toneva et al., 2022; Schrimpf et al., 2021; Millet et al., 2022; Vaidya
73"
INTRODUCTION,0.08679245283018867,"et al., 2022). To check the second aspect, we investigate this question via a direct approach, closely
74"
INTRODUCTION,0.08773584905660377,"related to previous studies (Toneva et al., 2022; Oota et al., 2023b,c). For each modality, we analyze
75"
INTRODUCTION,0.08867924528301886,"how the alignment between brain recordings and multi-modal model representations is affected by
76"
INTRODUCTION,0.08962264150943396,"the elimination of information related to that particular modality from the model representation.
77"
INTRODUCTION,0.09056603773584905,"Our analysis of multi-modal brain alignment leads to several key conclusions: (1) Both cross-modal
78"
INTRODUCTION,0.09150943396226414,"and jointly pretrained models demonstrate significantly improved brain alignment with language
79"
INTRODUCTION,0.09245283018867924,"regions (AG, PCC, PTL, and IFG) and visual regions (EVC and MT) when analyzed against unimodal
80"
INTRODUCTION,0.09339622641509433,"video data. In contrast, compared to unimodal speech-based models, all multi-modal embeddings
81"
INTRODUCTION,0.09433962264150944,"show significantly better brain alignment, except in the OV (object visual processing) region. This
82"
INTRODUCTION,0.09528301886792453,"highlights the ability of multi-modal models to capture additional information—either through
83"
INTRODUCTION,0.09622641509433963,"knowledge transfer or integration between modalities—which is crucial for multi-modal brain
84"
INTRODUCTION,0.09716981132075472,"alignment. (2) Using our residual approach, we find that the improved brain alignment in cross-
85"
INTRODUCTION,0.09811320754716982,"modal models can be partially attributed to the removal of video features alone, rather than auditory
86"
INTRODUCTION,0.09905660377358491,"features. On the other hand, the improved brain alignment in jointly pretrained models can be partially
87"
INTRODUCTION,0.1,"attributed to the removal of both video and auditory features.
88"
INTRODUCTION,0.1009433962264151,"Overall, we make the following contributions in this paper. (1) To the best of our knowledge, this
89"
INTRODUCTION,0.1018867924528302,"study is the first to leverage both cross-modal and jointly pretrained multi-modal models to perform
90"
INTRODUCTION,0.10283018867924529,"brain alignment while subjects are engaged with multi-modal naturalistic stimuli. (2) We evaluate the
91"
INTRODUCTION,0.10377358490566038,"performance of several unimodal Transformer models (three video and two audio) and measure their
92"
INTRODUCTION,0.10471698113207548,"brain alignment. (3) Additionally, we remove unimodal features from multi-modal representations
93"
INTRODUCTION,0.10566037735849057,"to explore the impact on brain alignment before and after their removal. We will release code upon
94"
INTRODUCTION,0.10660377358490566,"publication of this paper.
95"
RELATED WORK,0.10754716981132076,"2
Related Work
96"
RELATED WORK,0.10849056603773585,"Multi-modal models. Pretrained Transformer-based models have been found to be very effective in
97"
RELATED WORK,0.10943396226415095,"various tasks related to language (Devlin et al., 2019; Radford et al., 2019), speech (Baevski et al.,
98"
RELATED WORK,0.11037735849056604,"2020), and images (Dosovitskiy et al., 2020). To learn associations between pairs of modalities,
99"
RELATED WORK,0.11132075471698114,"Transformer models have been pretrained on multiple modalities, showing excellent results in multi-
100"
RELATED WORK,0.11226415094339623,"modal tasks like visual question answering and visual common-sense reasoning. These multi-modal
101"
RELATED WORK,0.11320754716981132,"models are pretrained in two different ways: (i) cross-modal models that integrate information
102"
RELATED WORK,0.11415094339622642,"from multiple modalities and learn a joint encoder, such as VisualBERT (Li et al., 2019) and
103"
RELATED WORK,0.11509433962264151,"ImageBind (Girdhar et al., 2023), and (ii) jointly pretrained models like LXMERT (Tan & Bansal,
104"
RELATED WORK,0.1160377358490566,"2019), CLIP (Radford et al., 2021), ViLBERT (Lu et al., 2019), and TVLT (Tang et al., 2022) which
105"
RELATED WORK,0.1169811320754717,"fuse individual modality encoders at different stages, transferring knowledge from one modality
106"
RELATED WORK,0.1179245283018868,"to another. In this work, we investigate how the representations extracted from cross-modal and
107"
RELATED WORK,0.11886792452830189,"jointly-pretrained Transformer models align with human brain recordings when participants engage
108"
RELATED WORK,0.11981132075471698,"with multi-modal stimuli.
109"
RELATED WORK,0.12075471698113208,"Brain Encoding using Multi-modal Models. Since human brain perceives the environment using
110"
RELATED WORK,0.12169811320754717,"information from multiple modalities (Gauthier et al., 2003), examining the alignment between
111"
RELATED WORK,0.12264150943396226,"language and visual representations in the brain by training encoding models on fMRI responses,
112"
RELATED WORK,0.12358490566037736,"while extracting joint representations from multi-modal models, can offer insights into the relation-
113"
RELATED WORK,0.12452830188679245,"ship between the two modalities. For instance, it has been shown that multi-modal models like
114"
RELATED WORK,0.12547169811320755,"CLIP (Radford et al., 2021) better predict neural responses in the high-level visual cortex as compared
115"
RELATED WORK,0.12641509433962264,"to previous vision-only models (Doerig et al., 2022; Wang et al., 2022). Additionally, Tang et al.
116"
RELATED WORK,0.12735849056603774,"(2024) demonstrate the use of multi-modal models in a cross-modal experiment to assess how well
117"
RELATED WORK,0.12830188679245283,"the language encoding models can predict movie-fMRI responses and how well the vision encoding
118"
RELATED WORK,0.12924528301886792,"models can predict narrative story-fMRI. Nakagi et al. (2024) analyzed fMRI related to video content
119"
RELATED WORK,0.13018867924528302,"viewing and found distinct brain regions associated with different semantic levels, highlighting the
120"
RELATED WORK,0.1311320754716981,"significance of modeling various levels of semantic content simultaneously. However, these studies
121"
RELATED WORK,0.1320754716981132,"have experimented with subjects engaged with single-modality stimulus, leaving the full potential of
122"
RELATED WORK,0.1330188679245283,"these models in true multi-modal scenarios still unclear. Recently, Dong & Toneva (2023) interpreted
123"
RELATED WORK,0.1339622641509434,"the effectiveness of pretrained versus finetuned multi-modal video transformer using video+text
124"
RELATED WORK,0.1349056603773585,"stimuli-based brain activity. However, they did not perform any cross-modal vs jointly-pretrained
125"
RELATED WORK,0.13584905660377358,"model analysis or analysis of multi-modal versus unimodal models, leaving it unclear which type
126"
RELATED WORK,0.13679245283018868,"of multi-modal models perform best for brain activity prediction. Further, unlike them, we study
127"
RELATED WORK,0.13773584905660377,"video+audio stimuli, and perform comprehensive residual analysis.
128"
DATASET CURATION,0.13867924528301886,"3
Dataset Curation
129"
DATASET CURATION,0.13962264150943396,"Brain Imaging Dataset. We experiment with a multi-modal naturalistic fMRI dataset, Movie10 (St-
130"
DATASET CURATION,0.14056603773584905,"Laurent et al., 2023) obtained from the Courtois NeuroMod databank. This dataset was collected
131"
DATASET CURATION,0.14150943396226415,"while six human subjects passively watched four different movies: The Bourne supremacy (∼100
132"
DATASET CURATION,0.14245283018867924,"mins), The wolf of wall street (∼170 mins), Hidden figures (∼120 mins) and Life (∼50 mins). Among
133"
DATASET CURATION,0.14339622641509434,"these, Hidden figures and Life are repeated twice, with the repeats used for testing and the remaining
134"
DATASET CURATION,0.14433962264150943,"movies for training. In this work, we use Life movie for testing where we average the two repetitions
135"
DATASET CURATION,0.14528301886792452,"to reduce noise in brain data. This dataset is one of the largest publicly available multi-modal fMRI
136"
DATASET CURATION,0.14622641509433962,"dataset in terms of number of samples per participant. It includes 4024 TRs (Time Repetitions) for
137"
DATASET CURATION,0.1471698113207547,"The Bourne supremacy, 6898 TRs for The wolf of wall street used in train and 2028 TRs for Life in
138"
DATASET CURATION,0.1481132075471698,"test. The fMRI data is collected every 1.49 seconds (= 1TR).
139"
DATASET CURATION,0.1490566037735849,"The dataset is already preprocessed and projected onto the surface space (“fsaverage6”). We use the
140"
DATASET CURATION,0.15,"multi-modal parcellation of the human cerebral cortex based on the Glasser Atlas (which consists
141"
DATASET CURATION,0.1509433962264151,"of 180 regions of interest in each hemisphere) to report the ROI (region of interest) analysis for the
142"
DATASET CURATION,0.15188679245283018,"brain maps (Glasser et al., 2016). This includes four visual processing regions (early visual (EV),
143"
DATASET CURATION,0.15283018867924528,"object-related areas (LO), face-related areas (OFA) and scene-related areas (PPA)), one early auditory
144"
DATASET CURATION,0.15377358490566037,"area (AC), and eight language-relevant regions, encompassing broader language regions: angular
145"
DATASET CURATION,0.15471698113207547,"gyrus (AG), anterior temporal lobe (ATL), posterior temporal lobe (PTL), inferior frontal gyrus (IFG),
146"
DATASET CURATION,0.15566037735849056,"inferior frontal gyrus orbital (IFGOrb), middle frontal gyrus (MFG), posterior cingulate cortex (PCC)
147"
DATASET CURATION,0.15660377358490565,"and dorsal medium prefrontal cortex (dmPFC), based on the Fedorenko lab’s language parcels (Milton
148"
DATASET CURATION,0.15754716981132075,"et al., 2021; Desai et al., 2022). We list the detailed sub-ROIs of these ROIs in Appendix B.
149"
DATASET CURATION,0.15849056603773584,"Estimating dataset cross-subject prediction accuracy. To account for the intrinsic noise in
150"
DATASET CURATION,0.15943396226415094,"biological measurements, we adapt Schrimpf et al. (2021)’s method to estimate the cross-subject
151"
DATASET CURATION,0.16037735849056603,"prediction accuracy for a model’s performance for the Movie10 fMRI datasets. By subsampling
152"
DATASET CURATION,0.16132075471698112,"fMRI datasets from 6 participants, we generate all possible combinations of s participants (s ∈[2,6])
153"
DATASET CURATION,0.16226415094339622,"for watching movies, and use a voxel-wise encoding model (see Sec. 5) to predict one participant’s
154"
DATASET CURATION,0.1632075471698113,"response from others. Note that the estimated cross-subject prediction accuracy is based on the
155"
DATASET CURATION,0.1641509433962264,"assumption of a perfect model, which might differ from real-world scenarios, yet offers valuable
156"
DATASET CURATION,0.1650943396226415,"insights into model’s performance. We estimate cross-subject prediction accuracy in three settings:
157"
DATASET CURATION,0.1660377358490566,"(i) training with The Bourne supremacy and testing with Life data, (ii) training with The wolf of wall
158"
DATASET CURATION,0.1669811320754717,"street and testing with Life data, and (iii) training with both The Bourne supremacy and The wolf
159"
DATASET CURATION,0.16792452830188678,"of wall street and testing with Life data. We present the average cross-subject prediction accuracy
160"
DATASET CURATION,0.16886792452830188,"across voxels for the Movie10 fMRI dataset and across the three settings in Appendix A.
161"
METHODOLOGY,0.16981132075471697,"4
Methodology
162"
MULTI-MODAL MODELS,0.17075471698113207,"4.1
Multi-modal models
163"
MULTI-MODAL MODELS,0.17169811320754716,"To analyse how human brain process information while engaged in multi-modal stimuli, we use recent
164"
MULTI-MODAL MODELS,0.17264150943396225,"popular deep learning models to explore multiple modalities information and build the encoding
165"
MULTI-MODAL MODELS,0.17358490566037735,"models in two different ways: “cross-modality pretraining"" and “joint pretraining"".
166"
MULTI-MODAL MODELS,0.17452830188679244,"Cross-modality Pretrained Multi-modal Models. Cross-modality representations involve transfer-
167"
MULTI-MODAL MODELS,0.17547169811320754,"ring information or learning from one modality to another. For example, in a cross-modal learning
168"
MULTI-MODAL MODELS,0.17641509433962263,"scenario, text descriptions can be used to improve the accuracy of image/video recognition tasks.
169"
MULTI-MODAL MODELS,0.17735849056603772,"This approach is often used in scenarios where one modality might have limited data or less direct
170"
MULTI-MODAL MODELS,0.17830188679245282,"relevance but can be informed by another modality.
171"
MULTI-MODAL MODELS,0.1792452830188679,"Recently, a cross-modal model called ImageBind (IB) (Girdhar et al., 2023) has shown immense
172"
MULTI-MODAL MODELS,0.180188679245283,"promise in binding data from six modalities at once, without the need for explicit supervision.
173"
MULTI-MODAL MODELS,0.1811320754716981,"ImageBind model uses separate encoders for each individual modality and learns a single shared
174"
MULTI-MODAL MODELS,0.1820754716981132,"representation space by leveraging multiple types of image-paired data. ImageBind consists of 12
175"
MULTI-MODAL MODELS,0.1830188679245283,"layers and outputs a 1024 dimensional representation for each modality.
176"
MULTI-MODAL MODELS,0.18396226415094338,"Jointly Pretrained Multi-modal Models. Jointly pretrained multi-modal model representations,
177"
MULTI-MODAL MODELS,0.18490566037735848,"on the other hand, involve combining data from multiple modalities to build a more comprehensive
178"
MULTI-MODAL MODELS,0.18584905660377357,"joint understanding to improve decision-making processes. The system processes these diverse inputs
179"
MULTI-MODAL MODELS,0.18679245283018867,"concurrently to make more informed and robust decisions.
180"
MULTI-MODAL MODELS,0.1877358490566038,"TVLT (Zellers et al., 2022) is an end-to-end Text-less Vision-Language multi-modal Transformer
181"
MULTI-MODAL MODELS,0.18867924528301888,"model for learning joint representations of video and speech from YouTube videos. This joint encoder
182"
MULTI-MODAL MODELS,0.18962264150943398,"model consists of a 12-layer encoder (hidden size 768) and uses masked autoencoding objective for
183"
MULTI-MODAL MODELS,0.19056603773584907,"both videos and speech. Given the video-speech pairs, the TVLT model provides 768 dimensional
184"
MULTI-MODAL MODELS,0.19150943396226416,"representations for each modality across 12 layers.
185"
MULTI-MODAL MODELS,0.19245283018867926,"Extraction of multi-modal features. To extract video and audio embedding representations from
186"
MULTI-MODAL MODELS,0.19339622641509435,"multi-modal models for the brain encoding task, we input video and audio pairs at each TR and
187"
MULTI-MODAL MODELS,0.19433962264150945,"obtain the aligned embeddings for the two modalities. Here, we first segment the input video and
188"
MULTI-MODAL MODELS,0.19528301886792454,"audio into clips corresponding to 1.49 seconds, which matches the fMRI image rate. For both the
189"
MULTI-MODAL MODELS,0.19622641509433963,"models, ImageBind and TVLT, we use the pretrained Transformer weights. ImageBind generates
190"
MULTI-MODAL MODELS,0.19716981132075473,"an embedding for each modality (IB video and IB audio) in an aligned space. We concatenate these
191"
MULTI-MODAL MODELS,0.19811320754716982,"embeddings to create what we refer to as IB concat embeddings. On the other hand, TVLT provides
192"
MULTI-MODAL MODELS,0.19905660377358492,"a joint embedding across all modalities at each layer. Only for the last layer, TVLT provides an
193"
MULTI-MODAL MODELS,0.2,"embedding for each modality.
194"
UNIMODAL MODELS,0.2009433962264151,"4.2
Unimodal Models
195"
UNIMODAL MODELS,0.2018867924528302,"To investigate the effectiveness of multi-modal representations in comparison to representations for
196"
UNIMODAL MODELS,0.2028301886792453,"individual modalities, we use the following methods to obtain embeddings for individual modalities.
197"
UNIMODAL MODELS,0.2037735849056604,"Video-based models. To extract representations of the video stimulus, we use three popular pretrained
198"
UNIMODAL MODELS,0.20471698113207548,"Transformer video-based models from Huggingface (Wolf et al., 2020): (1) Vision Transformer Base
199"
UNIMODAL MODELS,0.20566037735849058,"(ViT-B) (Dosovitskiy et al., 2020), (2) Video Masked Autoencoders (VideoMAE) (Tong et al., 2022)
200"
UNIMODAL MODELS,0.20660377358490567,"and (3) Video Vision Transformer (ViViT) (Arnab et al., 2021). Details of each model are reported in
201"
UNIMODAL MODELS,0.20754716981132076,"Table 1 in Appendix.
202"
UNIMODAL MODELS,0.20849056603773586,"Speech-based models. Similar to video-based models, we use two popular pretrained Transformer
203"
UNIMODAL MODELS,0.20943396226415095,"speech-based models from Huggingface: (1) Wav2Vec2.0 (Baevski et al., 2020) and (2) AST (Baade
204"
UNIMODAL MODELS,0.21037735849056605,"et al., 2022). Details of each model are reported in Table 1 in Appendix.
205"
UNIMODAL MODELS,0.21132075471698114,"Extraction of video features. ViT-B (Dosovitskiy et al., 2020), the underlying video encoder model
206"
UNIMODAL MODELS,0.21226415094339623,"for ImageBind is used for extracting representations for all frames in each TR for every video. To
207"
UNIMODAL MODELS,0.21320754716981133,"extract embedding at each TR, we average all frame embeddings and obtain the corresponding video
208"
UNIMODAL MODELS,0.21415094339622642,"representation. For VideoMAE and ViViT, we directly obtain the video embeddings for each TR. All
209"
UNIMODAL MODELS,0.21509433962264152,"3 models provide 768 dimensional representations and all of them are 12-layer Transformer encoders.
210"
UNIMODAL MODELS,0.2160377358490566,"Extraction of speech features. To explore whether speech models incorporate linguistic information,
211"
UNIMODAL MODELS,0.2169811320754717,"we extract representations beyond 1.49 secs, i.e., we considered context window of 16 secs with
212"
UNIMODAL MODELS,0.2179245283018868,"stride of 100 msecs and considered the last token as the representative for each context window. The
213"
UNIMODAL MODELS,0.2188679245283019,"pretrained speech-based models output token representations at different layers. Both Wav2Vec2.0
214"
UNIMODAL MODELS,0.219811320754717,"and AST models provide 768 dimensional representations and all of them are 12-layer Transformer
215"
UNIMODAL MODELS,0.22075471698113208,"encoders. Finally, we align these representations with the fMRI data acquisition rate by downsampling
216"
UNIMODAL MODELS,0.22169811320754718,"the stimulus features with a 3-lobed Lanczos filter, thus producing chunk-embeddings for each TR.
217"
EXPERIMENTAL SETUP,0.22264150943396227,"5
Experimental Setup
218"
EXPERIMENTAL SETUP,0.22358490566037736,"Encoding Model. We train bootstrap ridge regression based voxel-wise encoding models (Deniz
219"
EXPERIMENTAL SETUP,0.22452830188679246,"et al., 2019) to predict the fMRI brain activity associated with the stimulus representations obtained
220"
EXPERIMENTAL SETUP,0.22547169811320755,"from the individual modalities (speech and video) and multi-modal embeddings from cross-modal and
221"
EXPERIMENTAL SETUP,0.22641509433962265,"jointly pretrained multi-modal models. For each subject, we account for the delay in the hemodynamic
222"
EXPERIMENTAL SETUP,0.22735849056603774,"response by modeling hemodynamic response function using a finite response filter (FIR) per voxel
223"
EXPERIMENTAL SETUP,0.22830188679245284,"with 5 temporal delays (TRs) corresponding to ∼7.5 seconds (Huth et al., 2022). Formally, at each
224"
EXPERIMENTAL SETUP,0.22924528301886793,"time step t, we encode the stimuli as Xt ∈RD and brain region voxels Yt ∈RV , where D denotes
225"
EXPERIMENTAL SETUP,0.23018867924528302,"the dimension of the concatenation of delayed 5 TRs, and V denotes the number of voxels. Overall,
226"
EXPERIMENTAL SETUP,0.23113207547169812,"with N such TRs, we obtain N training examples.
227"
EXPERIMENTAL SETUP,0.2320754716981132,"Train-test Setup. We build encoding models in three settings: (i) We used all data samples from
228"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2330188679245283,"10 training sessions of the The Bourne supremacy movie for training and tested generalization on
229"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2339622641509434,"samples from the test sessions (5 sessions) of the Life movie. (ii) We used data from 17 training
230"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2349056603773585,"sessions of the The wolf of wall street movie for training, with the Life movie used for testing. (iii)
231"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2358490566037736,"We combined data from the The Bourne supremacy and The wolf of wall street movies for training,
232"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.23679245283018868,"and tested on the Life movie.
233"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.23773584905660378,"Removal of a single modality features from multi-modal representations. To remove features
234"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.23867924528301887,"for a particular modality m from multi-modal model representations, we rely on a simple method
235"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.23962264150943396,"proposed previously by Toneva et al. (2022) and Oota et al. (2023b), in which the linear contribution
236"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24056603773584906,"of the features to the multi-modal model activations is removed via ridge regression. Specifically, for
237"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24150943396226415,"this ridge regression the feature vector corresponding to modality m is considered as input and the
238"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24245283018867925,"multi-modal representations are the target. We compute the residuals by subtracting the predicted
239"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24339622641509434,"multi-modal feature representations from the actual multi-modal features resulting in the (linear)
240"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24433962264150944,"removal of feature vector for modality m from the pretrained multi-modal embeddings. Because
241"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24528301886792453,"the brain prediction method is also a linear function, this linear removal limits the contribution of
242"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24622641509433962,"features for modality m to the eventual brain alignment. See Fig. 1(B).
243"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.24716981132075472,"Evaluation Metrics. We evaluate our models using Pearson Correlation (PC) which is a standard
244"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2481132075471698,"metric for evaluating brain alignment (Jain & Huth, 2018; Schrimpf et al., 2021; Goldstein et al.,
245"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2490566037735849,"2022). Let TR be the number of time repetitions in the test set. Let Y = {Yi}T R
i=1 and ˆY = { ˆYi}T R
i=1
246"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.25,"denote the actual and predicted value vectors for a single voxel. Thus, Y and ˆY ∈RT R. We use
247"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2509433962264151,"Pearson Correlation (PC) which is computed as corr(Y, ˆY ) where corr is the correlation function.
248"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2518867924528302,"The final measure of a model’s performance is obtained by calculating Pearson’s correlation between
249"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2528301886792453,"the model’s predictions and neural recordings. This correlation is then divided by the estimated
250"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2537735849056604,"cross-subject prediction accuracy and averaged across voxels, regions, and participants, resulting in
251"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.25471698113207547,"a standardized measure of performance referred to as normalized brain alignment. For calculating
252"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.25566037735849056,"normalized alignment, we select the voxels whose cross-subject prediction accuracy is ≥0.05.
253"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.25660377358490566,"Implementation Details for Reproducibility. All experiments were conducted on a machine with
254"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.25754716981132075,"1 NVIDIA GeForce-GTX GPU with 16GB GPU RAM. We used bootstrap ridge-regression with
255"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.25849056603773585,"the following parameters: MSE loss function; L2-decay (λ) varied from 101 to 103; the best λ was
256"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.25943396226415094,"chosen by tuning on validation data that comprised a randomly chosen 10% subset from the train set
257"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.26037735849056604,"used only for hyper-parameter tuning.
258"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.26132075471698113,"Statistical Significance. To determine if normalized predictivity scores significantly higher than
259"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2622641509433962,"chance, we run a permutation test using blocks of 10 contiguous fMRI TRs (considering the slowness
260"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2632075471698113,"of hemodynamic response) rather than individual TRs. By permuting predictions 5000 times, we
261"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2641509433962264,"create an empirical distribution for chance performance, from which we estimate the p-value of
262"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2650943396226415,"the actual performance. To estimate the statistical significance of performance differences, such
263"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2660377358490566,"as between the model’s predictions and chance or residual predictions and chance, we utilized the
264"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2669811320754717,"Wilcoxon signed-rank test (Conover, 1999), applying it to the mean normalized predictivity for the
265"
TRAINING SESSIONS OF THE THE BOURNE SUPREMACY MOVIE FOR TRAINING AND TESTED GENERALIZATION ON,0.2679245283018868,"participants. In all cases, we denote significant differences (p≤0.05) with a ∗or ∧.
266"
RESULTS,0.2688679245283019,"6
Results
267"
RESULTS,0.269811320754717,"6.1
How effective are multi-modal representations obtained from multi-modal models?
268"
RESULTS,0.27075471698113207,"In Fig. 2, we present the average normalized brain alignment scores for both multi-modal and
269"
RESULTS,0.27169811320754716,"individual modality features. Specifically, we show the normalized brain alignment for cross-modality
270"
RESULTS,0.27264150943396226,"(ImageBind), jointly pretrained multi-modal (TVLT), and the average from individual video and
271"
RESULTS,0.27358490566037735,"speech models. The results are shown for whole brain, and also for average across language and
272"
RESULTS,0.27452830188679245,"visual ROIs. Results for individual ROIs are in Fig. 3.
273"
RESULTS,0.27547169811320754,"Baseline comparison. To compare the brain predictivity of multi-modal and unimodal models against
274"
RESULTS,0.27641509433962264,"baseline performance, we employ randomly generated vector embeddings to predict brain activity as
275"
RESULTS,0.27735849056603773,"baseline. We observe that the brain alignment from a random vector is significantly lower than that
276"
RESULTS,0.2783018867924528,"of both multi-modal and unimodal models across the whole brain and language-visual processing
277"
RESULTS,0.2792452830188679,"regions. This shows that the representations from these multi-modal models are significant enough
278"
RESULTS,0.280188679245283,"for learning non-trivial alignment with the fMRI recordings of multi-modal stimuli.
279"
RESULTS,0.2811320754716981,"Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal Models. Fig. 2(left)
280"
RESULTS,0.2820754716981132,"displays results for whole brain analysis, where the IB Concat bar plot corresponds to results for
281"
RESULTS,0.2830188679245283,"representations from a cross-modal model, while TVLT Joint bar plot corresponds to results for
282"
RESULTS,0.2839622641509434,"representations from a jointly pretrained multi-modal model. From Fig. 2(left), we make the following
283"
RESULTS,0.2849056603773585,"observations: (i) At the whole brain level, the Wilcoxon signed-rank test shows that the differences in
284"
RESULTS,0.2858490566037736,"embeddings from the IB Concat and TVLT models are not statistically significant. (ii) The multi-
285"
RESULTS,0.28679245283018867,"modal embeddings show improved brain alignment compared to unimodal models. Specifically,
286"
RESULTS,0.28773584905660377,"cross-modal embeddings are significantly better than both unimodal video and speech models, while
287"
RESULTS,0.28867924528301886,"jointly pretrained embeddings are significantly better than speech models. This implies that cross-
288 * ^
^"
RESULTS,0.28962264150943395,"Whole brain
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.29056603773584905,Normalized brain alignment * ^ ^
RESULTS,0.29150943396226414,"Language
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.29245283018867924,"Normalized brain alignment ^
^"
RESULTS,0.29339622641509433,"Visual
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.2943396226415094,Normalized brain alignment
RESULTS,0.2952830188679245,"Figure 2: Average normalized brain alignment for both multi-modal and individual modality features
across whole brain, language, and visual regions. Error bars indicate the standard error of the mean
across participants. ∗indicates cases where multi-modal embeddings are significantly better than
unimodal video models (VM), i.e., p≤0.05. ∧, indicates cases where multi-modal embeddings are
significantly better than unimodal speech models (SM), i.e., p≤0.05."
RESULTS,0.2962264150943396,"modal embeddings contain additional information beyond the two modalities, while embeddings
289"
RESULTS,0.2971698113207547,"from a jointly pretrained model do not provide extra information beyond unimodal visual information
290"
RESULTS,0.2981132075471698,"but do contain additional information beyond unimodal speech.
291"
RESULTS,0.2990566037735849,"We also present average results across language and visual regions in Figs. 2 (middle), and 2(right),
292"
RESULTS,0.3,"respectively. The Wilcoxon signed-rank test shows that the differences in embeddings from the IB
293"
RESULTS,0.3009433962264151,"Concat and TVLT models are not statistically significant when averaged across language and visual
294"
RESULTS,0.3018867924528302,"regions. Similar to whole brain performance, in the language regions, cross-modal embeddings
295"
RESULTS,0.30283018867924527,"are significantly better than both unimodal video and speech models, while jointly pretrained em-
296"
RESULTS,0.30377358490566037,"beddings are significantly better than unimodal speech models. In contrast, for the visual regions,
297"
RESULTS,0.30471698113207546,"the normalized brain alignment of cross-modal and jointly pretrained embeddings is similar to the
298"
RESULTS,0.30566037735849055,"performance of unimodal video models. This implies that when we average across visual regions,
299"
RESULTS,0.30660377358490565,"there is no additional information beyond unimodal video features. However, when compared to
300"
RESULTS,0.30754716981132074,"unimodal speech features, both multi-modal embeddings show significant improvement.
301"
RESULTS,0.30849056603773584,"Since we didn’t observe any significant difference at the whole brain level and when averaged across
302"
RESULTS,0.30943396226415093,"language and visual regions, between cross-modal and jointly pretrained multi-modal models, we
303"
RESULTS,0.310377358490566,"attempt to seek if there any any differences when we pay a closer look at the individual ROIs. We
304"
RESULTS,0.3113207547169811,"present results for language and visual regions such as Angular gyrus (AG), the posterior temporal
305"
RESULTS,0.3122641509433962,"lobe (PTL), and the inferior frontal gyrus (IFG) in Fig. 3. Additionally, we cover visual regions
306"
RESULTS,0.3132075471698113,"like early visual cortex (EVC), scene visual areas (SV) and middle temporal gyrus (MT), as well
307"
RESULTS,0.3141509433962264,"as early auditory cortex (AC). In this figure, we also report the average normalized brain alignment
308"
RESULTS,0.3150943396226415,"of each modality obtained from multi-modal models. Unlike the whole brain analysis, we observe
309"
RESULTS,0.3160377358490566,"some differences between cross-modal and jointly pretrained models in several language and visual
310"
RESULTS,0.3169811320754717,"ROIs. Results for other ROIs are in Fig. 7 in Appendix. Our observations are as follows: (i) Cross-
311"
RESULTS,0.3179245283018868,"modal IB Concat embeddings are significantly better than TVLT Joint embeddings in semantic
312"
RESULTS,0.31886792452830187,"regions such as AG and PCC, as well as the multi-modal processing region MT. (ii) Conversely,
313"
RESULTS,0.31981132075471697,"TVLT Joint embeddings are significantly better than IB Concat embeddings in dmPFC regions.
314"
RESULTS,0.32075471698113206,"While considering both joint and each modality embeddings from multi-modal models, we make the
315"
RESULTS,0.32169811320754715,"following observations from Fig. 3: (1) Cross-modal IB video embeddings exhibit improved brain
316"
RESULTS,0.32264150943396225,"alignment compared to unimodal video in the AG and MT regions with the exceptions of the PTL
317"
RESULTS,0.32358490566037734,"and AC regions. But this is not the case for IB audio vs unimodal audio. This suggests that video
318"
RESULTS,0.32452830188679244,"modality information is more relevant and beneficial in the brain for IB Concat embeddings from
319"
RESULTS,0.32547169811320753,"cross-modality models. (2) TVLT video embeddings show improved brain alignment in the AG, PTL,
320"
RESULTS,0.3264150943396226,"PCC, dmPFC and EVC regions, with other regions displaying similar normalized brain alignment
321"
RESULTS,0.3273584905660377,"unimodal video embeddings. (3) Consistent with the cross-modality models, in jointly pretrained
322"
RESULTS,0.3283018867924528,"TVLT models, TVLT video embeddings significantly outperform TVLT audio embeddings, except in
323"
RESULTS,0.3292452830188679,"PTL region. These observations indicate that video information is advantageous for both cross-modal
324"
RESULTS,0.330188679245283,"and jointly pretrained models, whereas audio embeddings mainly benefit the PTL region.
325"
RESULTS,0.3311320754716981,"6.2
Which brain regions process uni- and multi-modal information?
326"
RESULTS,0.3320754716981132,"From Fig. 3, we observe that multi-modal video embeddings exhibit improved brain alignment not
327"
RESULTS,0.3330188679245283,"only in the whole brain but also in various language, visual and multi-modal regions. For instance,
328"
RESULTS,0.3339622641509434,"the cross-modal IB Concat embeddings demonstrate superior brain alignment compared to unimodal
329"
RESULTS,0.33490566037735847,"video-based models in areas such as the AG, PTL, IFG, and PCC. Moreover, TVLT-joint embeddings
330"
RESULTS,0.33584905660377357,"*^
^ ^"
RESULTS,0.33679245283018866,"Whole brain
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.33773584905660375,Normalized brain alignment *^ ^ ^ ^ *^
RESULTS,0.33867924528301885,"AG
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.33962264150943394,Normalized brain alignment *^ *^ *^ *^
RESULTS,0.34056603773584904,"PTL
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.34150943396226413,Normalized brain alignment
RESULTS,0.3424528301886792,"*^
*^ ^"
RESULTS,0.3433962264150943,"IFG
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.3443396226415094,Normalized brain alignment
RESULTS,0.3452830188679245,"^ ^
*^ *^"
RESULTS,0.3462264150943396,"EVC
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.3471698113207547,Normalized brain alignment ^ ^ ^ ^
RESULTS,0.3481132075471698,"SV
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.3490566037735849,Normalized brain alignment
RESULTS,0.35,"*^ ^
^ ^ ^"
RESULTS,0.35094339622641507,"MT
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.35188679245283017,"Normalized brain alignment *^
^ ^"
RESULTS,0.35283018867924526,"AC
0.1 0.2 0.3 0.4 0.5 0.6"
RESULTS,0.35377358490566035,Normalized brain alignment
RESULTS,0.35471698113207545,"Figure 3: Average normalized brain alignment for video and audio modalities from multi-modal and
individual modality features across whole brain and several ROIs of language (AG, PTL and IFG),
visual (EVC, SV and MT) and auditory cortex (AC). Error bars indicate the standard error of the
mean across participants. ∗indicates cases where multi-modal embeddings are significantly better
than unimodal video models (VM), i.e., p≤0.05. ∧indicates cases where multi-modal embeddings
are significantly better than unimodal speech models (SM), i.e., p≤0.05.3"
RESULTS,0.35566037735849054,"show notable enhancements in the AG, PTL, IFG, PCC, dmPFC and EVC regions. In contrast,
331"
RESULTS,0.35660377358490564,"compared to unimodal speech-based models, all multi-modal embeddings display significantly better
332"
RESULTS,0.35754716981132073,"brain alignment, except the OV (object visual processing) region. Overall, this observation suggests
333"
RESULTS,0.3584905660377358,"that integrating multiple modalities leads to transferring information from one modality to another,
334"
RESULTS,0.3594339622641509,"resulting in improved brain predictability. Based on these, it can be inferred that these multi-modal
335"
RESULTS,0.360377358490566,"models can indeed learn multi-modal linkages that are relevant to the brain.
336"
RESULTS,0.3613207547169811,"When subjects engage with multi-modality stimuli, we observe that multi-modal embeddings show
337"
RESULTS,0.3622641509433962,"improvements in semantic regions such as the AG, PCC and dmPFC, and syntactic regions such as
338"
RESULTS,0.3632075471698113,"the PTL and IFG. Overall, we find that multi-modal information is processed in only a few regions.
339"
RESULTS,0.3641509433962264,"Furthermore, several regions, including the SV (scene visual area), EVC (early visual cortex), ATL
340"
RESULTS,0.3650943396226415,"(anterior temporal lobe), IFGOrb, MFG, and dmPFC, exhibit similar brain alignment with both
341"
RESULTS,0.3660377358490566,"unimodal and multi-modal embeddings.
342"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.36698113207547167,"6.3
How is the brain alignment of multi-modal features affected by the elimination of a
343"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.36792452830188677,"particular modality?
344"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.36886792452830186,"To understand the contribution of each modality to the multi-modal brain alignment for multi-modal
345"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.36981132075471695,"naturalistic stimulus, we perform residual analyses by removing the unimodality features from
346"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37075471698113205,"multi-modal joint representations as well as multi-modal video or audio representations from joint
347"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37169811320754714,"representations and measure the differences in brain alignment before and after removal modality-
348"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37264150943396224,"specific features. Fig. 4 displays the normalized brain alignment for language (AG) and visual regions
349"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37358490566037733,"(MT). We note a decrease in brain alignment for both the AG and MT regions following the removal
350"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3745283018867924,"of video embeddings from cross-modality models, whereas the removal of audio embeddings does
351"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3754716981132076,"not affect the brain alignment. On the other hand, for jointly pretrained models, removal of both
352"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37641509433962267,"video and audio embeddings partially impacts the brain alignment. We observe similar findings for
353"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37735849056603776,"language ROIs such as PTL, MFG, ATL, PCC and visual regions EVC, OV and FV, as shown in
354"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37830188679245286,"Figs. 9 and 10 in Appendix. These results suggest that there is additional information beyond the
355"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.37924528301886795,"unimodal embeddings considered in this study that is processed in the visual and language regions.
356"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.38018867924528305,"AG
0.1 0.2 0.3 0.4 0.5 0.6"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.38113207547169814,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.38207547169811323,Normalized brain alignment
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.38301886792452833,"MT
0.1 0.2 0.3 0.4 0.5 0.6"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3839622641509434,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3849056603773585,Normalized brain alignment
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3858490566037736,"Figure 4: Residual analysis: Average normalized brain alignment was computed across participants
before and after removal of video and audio embeddings from both jointly pretrained and cross-
modality models. Error bars indicate the standard error of the mean across participants."
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3867924528301887,"Figure 5: Percentage decrease of brain alignment after removal of (left) Unimodal VM embeddings
from IB-Concat (middle) Unimodal VM embeddings from jointly pretrained TVLT, and (right)
Unimodal SM embeddings from TVLT Joint. Colorbar indicates the percentage of decrease where
red denotes higher and white denotes zero."
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3877358490566038,"Qualitative analysis. We compute the percentage decrease in alignment for each voxel following the
357"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3886792452830189,"removal of unimodal video embeddings from the IB Concat (cross-modality) and the TVLT Joint
358"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.389622641509434,"(jointly pretrained model), with projections onto the brain surface averaged across participants, as
359"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3905660377358491,"depicted in Fig. 5. The colorbar shows the percentage decrease in brain alignment, where red voxels
360"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.3915094339622642,"indicate a higher percentage decrease and white voxels indicate areas where unimodal video features
361"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.39245283018867927,"do not contribute any shared information within the multi-modal context. We observe that removal of
362"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.39339622641509436,"unimodal video features leads to a significant drop (40-50%) in performance in the visual regions for
363"
HOW IS THE BRAIN ALIGNMENT OF MULTI-MODAL FEATURES AFFECTED BY THE ELIMINATION OF A,0.39433962264150946,"IB Concat, and in language regions (PTL & MFG) for TVLT Joint.
364"
DISCUSSION,0.39528301886792455,"7
Discussion
365"
DISCUSSION,0.39622641509433965,"Using multi-modal model representations, including both cross-modal and jointly pretrained types,
366"
DISCUSSION,0.39716981132075474,"we evaluated how these representations can predict fMRI brain activity when participants are
367"
DISCUSSION,0.39811320754716983,"engaged in multi-modal naturalistic stimuli. Further, we compared both multi-modal and unimodal
368"
DISCUSSION,0.39905660377358493,"representations and observed their alignment with both unimodal and multi-modal brain regions.
369"
DISCUSSION,0.4,"This is achieved by removing information related to unimodal stimulus features (audio and video)
370"
DISCUSSION,0.4009433962264151,"and observing how this perturbation affects the alignment with fMRI brain recordings acquired while
371"
DISCUSSION,0.4018867924528302,"participants are engaged in watching multi-modal naturalistic movies.
372"
DISCUSSION,0.4028301886792453,"Our analysis of multi-modal brain alignment yields several important conclusions: (1) The improved
373"
DISCUSSION,0.4037735849056604,"brain alignment of the multi-modal models over unimodal models, across several language, visual, and
374"
DISCUSSION,0.4047169811320755,"auditory regions is only partially attributable to the video and audio stimulus features presented to the
375"
DISCUSSION,0.4056603773584906,"model. A deeper understanding of these models is required to shed light on the underlying information
376"
DISCUSSION,0.4066037735849057,"processing of both unimodal and multi-modal information. (2) Cross-modal representations have
377"
DISCUSSION,0.4075471698113208,"significantly improved brain alignment in language regions such as AG, PCC and PTL. This variance
378"
DISCUSSION,0.40849056603773587,"can be partially attributed to the removal of video features alone, rather than auditory features. (3)
379"
DISCUSSION,0.40943396226415096,"Video embeddings from multi-modal models exhibit higher brain alignment than audio embeddings,
380"
DISCUSSION,0.41037735849056606,"except in the PTL and AC regions. This suggests that audio-based models may encode weaker brain-
381"
DISCUSSION,0.41132075471698115,"relevant semantics. (4) Both cross-modal and jointly pretrained models demonstrate significantly
382"
DISCUSSION,0.41226415094339625,"improved brain alignment with language regions (AG, PCC, PTL and IFG) compared to visual regions
383"
DISCUSSION,0.41320754716981134,"when analyzed against unimodal video data. In contrast, when compared to unimodal audio-based
384"
DISCUSSION,0.41415094339622643,"models, all multi-modal embeddings display significantly better brain alignment, with the exception
385"
DISCUSSION,0.41509433962264153,"of the OV region. This underscores the capability of multi-modal models to capture additional
386"
DISCUSSION,0.4160377358490566,"information—either through knowledge transfer or integration between modalities—crucial for
387"
DISCUSSION,0.4169811320754717,"multi-modal brain alignment.
388"
DISCUSSION,0.4179245283018868,"Limitations. The low alignment scores clearly show that despite the increasing popularity of multi-
389"
DISCUSSION,0.4188679245283019,"modal models in tackling complex tasks such as visual question answering, we are still far from
390"
DISCUSSION,0.419811320754717,"developing a model that fully encapsulates the complete information processing steps involved
391"
DISCUSSION,0.4207547169811321,"in handling multi-modal naturalistic information in the brain. In the future, by fine-tuning these
392"
DISCUSSION,0.4216981132075472,"multi-modal models on specific tasks such as generating captions for videos, we can better leverage
393"
DISCUSSION,0.4226415094339623,"their alignment strengths. This approach will allow us to explore task-level brain alignment of three
394"
DISCUSSION,0.4235849056603774,"modalities—video, audio, and text—more effectively. Further, multi-modal large language models
395"
DISCUSSION,0.42452830188679247,"(MLLMs) (Zhang et al., 2023; Ataallah et al., 2024; Wu et al., 2023) that align visual features from
396"
DISCUSSION,0.42547169811320756,"video frames into the LLM embedding space via a trainable linear projection layer, offer promise for
397"
DISCUSSION,0.42641509433962266,"enhanced multi-modal capabilities. We would further extend this work by comparing the region-wise
398"
DISCUSSION,0.42735849056603775,"brain alignment performance of these multi-modal LLM models with existing approaches.
399"
REFERENCES,0.42830188679245285,"References
400"
REFERENCES,0.42924528301886794,"Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.
401"
REFERENCES,0.43018867924528303,"Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international conference on
402"
REFERENCES,0.43113207547169813,"computer vision, pp. 6836–6846, 2021.
403"
REFERENCES,0.4320754716981132,"Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and
404"
REFERENCES,0.4330188679245283,"Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with
405"
REFERENCES,0.4339622641509434,"interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024.
406"
REFERENCES,0.4349056603773585,"Khai Loong Aw and Mariya Toneva. Training language models to summarize narratives improves
407"
REFERENCES,0.4358490566037736,"brain alignment. In The Eleventh International Conference on Learning Representations, 2023.
408"
REFERENCES,0.4367924528301887,"Alan Baade, Puyuan Peng, and David Harwath. Mae-ast: Masked autoencoding audio spectrogram
409"
REFERENCES,0.4377358490566038,"transformer. Interspeech 2022, 2022.
410"
REFERENCES,0.4386792452830189,"Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework
411"
REFERENCES,0.439622641509434,"for self-supervised learning of speech representations. Advances in neural information processing
412"
REFERENCES,0.44056603773584907,"systems, 2020.
413"
REFERENCES,0.44150943396226416,"Cordell M Baker, Joshua D Burks, Robert G Briggs, Andrew K Conner, Chad A Glenn, Kathleen N
414"
REFERENCES,0.44245283018867926,"Taylor, Goksel Sali, Tressie M McCoy, James D Battiste, Daniel L O’Donoghue, et al. A connec-
415"
REFERENCES,0.44339622641509435,"tomic atlas of the human cerebrum—chapter 7: the lateral parietal lobe. Operative Neurosurgery,
416"
REFERENCES,0.44433962264150945,"15(suppl_1):S295–S349, 2018.
417"
REFERENCES,0.44528301886792454,"Stefania Bracci and Hans P Op de Beeck. Understanding human object vision: a picture is worth a
418"
REFERENCES,0.44622641509433963,"thousand representations. Annual review of psychology, 74:113–135, 2023.
419"
REFERENCES,0.44716981132075473,"Charlotte Caucheteux and Jean-Rémi King. Language processing in brains and deep neural networks:
420"
REFERENCES,0.4481132075471698,"computational convergence and its limits. BioRxiv, 2020.
421"
REFERENCES,0.4490566037735849,"William Jay Conover. Practical nonparametric statistics, volume 350. john wiley & sons, 1999.
422"
REFERENCES,0.45,"Fatma Deniz, Anwar O Nunez-Elizalde, Alexander G Huth, and Jack L Gallant. The representation
423"
REFERENCES,0.4509433962264151,"of semantic information across human cerebral cortex during listening versus reading is invariant
424"
REFERENCES,0.4518867924528302,"to stimulus modality. Journal of Neuroscience, 2019.
425"
REFERENCES,0.4528301886792453,"Rutvik Desai, Usha Tadimeti, and Nicholas Riccardi. Proper and common names in the semantic
426"
REFERENCES,0.4537735849056604,"system, 2022.
427"
REFERENCES,0.4547169811320755,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
428"
REFERENCES,0.4556603773584906,"bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
429"
REFERENCES,0.45660377358490567,"the North American Chapter of the Association for Computational Linguistics: Human Language
430"
REFERENCES,0.45754716981132076,"Technologies, Volume 1 (Long and Short Papers), 2019.
431"
REFERENCES,0.45849056603773586,"Adrien Doerig, Tim C Kietzmann, Emily Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay,
432"
REFERENCES,0.45943396226415095,"and Ian Charest. Semantic scene descriptions as an objective of human vision. arXiv preprint
433"
REFERENCES,0.46037735849056605,"arXiv:2209.11737, 2022.
434"
REFERENCES,0.46132075471698114,"Dota Tianai Dong and Mariya Toneva. Vision-language integration in multimodal video transformers
435"
REFERENCES,0.46226415094339623,"(partially) aligns with the brain. arXiv preprint arXiv:2311.07766, 2023.
436"
REFERENCES,0.46320754716981133,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
437"
REFERENCES,0.4641509433962264,"Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
438"
REFERENCES,0.4650943396226415,"is worth 16x16 words: Transformers for image recognition at scale. In International Conference
439"
REFERENCES,0.4660377358490566,"on Learning Representations, 2020.
440"
REFERENCES,0.4669811320754717,"Michael Eickenberg, Alexandre Gramfort, Gaël Varoquaux, and Bertrand Thirion. Seeing it all:
441"
REFERENCES,0.4679245283018868,"Convolutional network layers map the function of the human visual system. NeuroImage, 152:
442"
REFERENCES,0.4688679245283019,"184–194, 2017.
443"
REFERENCES,0.469811320754717,"Isabel Gauthier, Thomas W James, Kim M Curby, and Michael J Tarr. The influence of conceptual
444"
REFERENCES,0.4707547169811321,"knowledge on visual discrimination. Cognitive Neuropsychology, 20(3-6):507–523, 2003.
445"
REFERENCES,0.4716981132075472,"Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand
446"
REFERENCES,0.47264150943396227,"Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the
447"
REFERENCES,0.47358490566037736,"IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180–15190, 2023.
448"
REFERENCES,0.47452830188679246,"Matthew F Glasser, Timothy S Coalson, Emma C Robinson, Carl D Hacker, John Harwell, Essa
449"
REFERENCES,0.47547169811320755,"Yacoub, Kamil Ugurbil, Jesper Andersson, Christian F Beckmann, Mark Jenkinson, et al. A
450"
REFERENCES,0.47641509433962265,"multi-modal parcellation of human cerebral cortex. Nature, 536(7615):171–178, 2016.
451"
REFERENCES,0.47735849056603774,"Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A
452"
REFERENCES,0.47830188679245284,"Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al. Shared computational principles for
453"
REFERENCES,0.47924528301886793,"language processing in humans and deep language models. Nature neuroscience, 25(3):369–380,
454"
REFERENCES,0.480188679245283,"2022.
455"
REFERENCES,0.4811320754716981,"Alexander G Huth, Shinji Nishimoto, An T Vu, and T Dupre La Tour. Gallant lab natural short clips
456"
REFERENCES,0.4820754716981132,"3t fmri data. 50 GiB, 2022.
457"
REFERENCES,0.4830188679245283,"Shailee Jain and Alexander G Huth. Incorporating context into language encoding models for fmri.
458"
REFERENCES,0.4839622641509434,"In NIPS, pp. 6629–6638, 2018.
459"
REFERENCES,0.4849056603773585,"Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple
460"
REFERENCES,0.4858490566037736,"and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.
461"
REFERENCES,0.4867924528301887,"Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: pretraining task-agnostic visiolinguistic
462"
REFERENCES,0.4877358490566038,"representations for vision-and-language tasks. In Proceedings of the 33rd International Conference
463"
REFERENCES,0.48867924528301887,"on Neural Information Processing Systems, pp. 13–23, 2019.
464"
REFERENCES,0.48962264150943396,"Juliette Millet, Charlotte Caucheteux, Yves Boubenec, Alexandre Gramfort, Ewan Dunbar, Christophe
465"
REFERENCES,0.49056603773584906,"Pallier, Jean-Remi King, et al. Toward a realistic model of speech processing in the brain with
466"
REFERENCES,0.49150943396226415,"self-supervised learning. Advances in Neural Information Processing Systems, 35:33428–33443,
467"
REFERENCES,0.49245283018867925,"2022.
468"
REFERENCES,0.49339622641509434,"Camille K Milton, Vukshitha Dhanaraj, Isabella M Young, Hugh M Taylor, Peter J Nicholas, Robert G
469"
REFERENCES,0.49433962264150944,"Briggs, Michael Y Bai, Rannulu D Fonseka, Jorge Hormovas, Yueh-Hsin Lin, et al. Parcellation-
470"
REFERENCES,0.49528301886792453,"based anatomic model of the semantic network. Brain and behavior, 11(4):e02065, 2021.
471"
REFERENCES,0.4962264150943396,"Yuko Nakagi, Takuya Matsuyama, Naoko Koide-Majima, Hiroto Yamaguchi, Rieko Kubo, Shinji
472"
REFERENCES,0.4971698113207547,"Nishimoto, and Yu Takagi. The brain tells a story: Unveiling distinct representations of semantic
473"
REFERENCES,0.4981132075471698,"content in speech, objects, and stories in the human brain with large language models. bioRxiv, pp.
474"
REFERENCES,0.4990566037735849,"2024–02, 2024.
475"
REFERENCES,0.5,"Subba Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta, and Raju S Bapi. Visio-linguistic
476"
REFERENCES,0.5009433962264151,"brain encoding. In COLING, pp. 116–133, 2022.
477"
REFERENCES,0.5018867924528302,"Subba Reddy Oota, Manish Gupta, Raju S Bapi, Gael Jobard, Frédéric Alexandre, and Xavier Hinaut.
478"
REFERENCES,0.5028301886792453,"Deep neural networks and brain alignment: Brain encoding and decoding (survey). arXiv preprint
479"
REFERENCES,0.5037735849056604,"arXiv:2307.10246, 2023a.
480"
REFERENCES,0.5047169811320755,"Subba Reddy Oota, Manish Gupta, and Mariya Toneva. Joint processing of linguistic properties in
481"
REFERENCES,0.5056603773584906,"brains and language models. NeurIPS, 2023b.
482"
REFERENCES,0.5066037735849057,"Subba Reddy Oota, Agarwal Veeral, Marreddy Mounika, Gupta Manish, and Raju Surampudi Bapi.
483"
REFERENCES,0.5075471698113208,"Speech taskonomy: Which speech tasks are the most predictive of fmri brain activity? In 24th
484"
REFERENCES,0.5084905660377359,"INTERSPEECH Conference, 2023c.
485"
REFERENCES,0.5094339622641509,"Sara F Popham, Alexander G Huth, Natalia Y Bilenko, Fatma Deniz, James S Gao, Anwar O Nunez-
486"
REFERENCES,0.5103773584905661,"Elizalde, and Jack L Gallant. Visual and linguistic semantic representations are aligned at the
487"
REFERENCES,0.5113207547169811,"border of human visual cortex. Nature neuroscience, 24(11):1628–1636, 2021.
488"
REFERENCES,0.5122641509433963,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
489"
REFERENCES,0.5132075471698113,"models are unsupervised multitask learners. OpenAI, 2019.
490"
REFERENCES,0.5141509433962265,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
491"
REFERENCES,0.5150943396226415,"Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
492"
REFERENCES,0.5160377358490567,"models from natural language supervision. Image, 2:T2, 2021.
493"
REFERENCES,0.5169811320754717,"Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J Majaj, Rishi Rajalingham, Elias B Issa, Kohitij
494"
REFERENCES,0.5179245283018868,"Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, et al. Brain-score: Which artificial
495"
REFERENCES,0.5188679245283019,"neural network for object recognition is most brain-like? BioRxiv, pp. 407007, 2018.
496"
REFERENCES,0.519811320754717,"Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kan-
497"
REFERENCES,0.5207547169811321,"wisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural architecture of language:
498"
REFERENCES,0.5216981132075472,"Integrative modeling converges on predictive processing. Proceedings of the National Academy of
499"
REFERENCES,0.5226415094339623,"Sciences, 2021.
500"
REFERENCES,0.5235849056603774,"Marie St-Laurent, Basile Pinsard, Oliver Contier, Katja Seeliger, Valentina Borghesani, Julie Boyle,
501"
REFERENCES,0.5245283018867924,"Pierre Bellec, and Martin Hebart. cneuromod-things: a large-scale fmri dataset for task-and
502"
REFERENCES,0.5254716981132076,"data-driven assessment of object representation and visual memory recognition in the human brain.
503"
REFERENCES,0.5264150943396226,"Journal of Vision, 23(9):5424–5424, 2023.
504"
REFERENCES,0.5273584905660378,"Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transform-
505"
REFERENCES,0.5283018867924528,"ers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
506"
REFERENCES,0.529245283018868,"and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
507"
REFERENCES,0.530188679245283,"pp. 5100–5111, 2019.
508"
REFERENCES,0.5311320754716982,"Jerry Tang, Meng Du, Vy Vo, Vasudev Lal, and Alexander Huth. Brain encoding models based on
509"
REFERENCES,0.5320754716981132,"multimodal transformers can transfer across language and vision. Advances in Neural Information
510"
REFERENCES,0.5330188679245284,"Processing Systems, 36, 2024.
511"
REFERENCES,0.5339622641509434,"Zineng Tang, Jaemin Cho, Yixin Nie, and Mohit Bansal. Tvlt: Textless vision-language transformer.
512"
REFERENCES,0.5349056603773585,"Advances in Neural Information Processing Systems, 35:9617–9632, 2022.
513"
REFERENCES,0.5358490566037736,"Mariya Toneva and Leila Wehbe.
Interpreting and improving natural-language processing (in
514"
REFERENCES,0.5367924528301887,"machines) with natural language-processing (in the brain). Advances in Neural Information
515"
REFERENCES,0.5377358490566038,"Processing Systems, 32, 2019.
516"
REFERENCES,0.5386792452830189,"Mariya Toneva, Tom M Mitchell, and Leila Wehbe. Combining computational controls with natural
517"
REFERENCES,0.539622641509434,"text reveals aspects of meaning composition. Nature Computational Science, 2(11):745–757, 2022.
518"
REFERENCES,0.5405660377358491,"Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-
519"
REFERENCES,0.5415094339622641,"efficient learners for self-supervised video pre-training. Advances in neural information processing
520"
REFERENCES,0.5424528301886793,"systems, 35:10078–10093, 2022.
521"
REFERENCES,0.5433962264150943,"Greta Tuckute, Jenelle Feather, Dana Boebinger, and Josh H McDermott. Many but not all deep
522"
REFERENCES,0.5443396226415095,"neural network audio models capture brain responses and exhibit correspondence between model
523"
REFERENCES,0.5452830188679245,"stages and brain regions. Plos Biology, 21(12):e3002366, 2023.
524"
REFERENCES,0.5462264150943397,"Aditya Vaidya, Shailee Jain, and Alexander Huth. Self-supervised models of audio effectively
525"
REFERENCES,0.5471698113207547,"explain human cortical responses to speech. In International Conference on Machine Learning, pp.
526"
REFERENCES,0.5481132075471699,"21927–21944. PMLR, 2022.
527"
REFERENCES,0.5490566037735849,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
528"
REFERENCES,0.55,"Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
529"
REFERENCES,0.5509433962264151,"systems, 30, 2017.
530"
REFERENCES,0.5518867924528302,"Aria Wang, Michael Tarr, and Leila Wehbe. Neural taskonomy: Inferring the similarity of task-derived
531"
REFERENCES,0.5528301886792453,"representations from brain activity. NeurIPS, 32:15501–15511, 2019.
532"
REFERENCES,0.5537735849056604,"Aria Y Wang, Kendrick Kay, Thomas Naselaris, Michael J Tarr, and Leila Wehbe. Natural language
533"
REFERENCES,0.5547169811320755,"supervision with a large and diverse dataset builds better models of human high-level visual cortex.
534"
REFERENCES,0.5556603773584906,"BioRxiv, pp. 2022–09, 2022.
535"
REFERENCES,0.5566037735849056,"Leila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and Tom Mitchell.
536"
REFERENCES,0.5575471698113208,"Simultaneously uncovering the patterns of brain regions involved in different story reading subpro-
537"
REFERENCES,0.5584905660377358,"cesses. PloS one, 11, 2014.
538"
REFERENCES,0.559433962264151,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
539"
REFERENCES,0.560377358490566,"Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art
540"
REFERENCES,0.5613207547169812,"natural language processing. In Proceedings of the 2020 conference on empirical methods in
541"
REFERENCES,0.5622641509433962,"natural language processing: system demonstrations, pp. 38–45, 2020.
542"
REFERENCES,0.5632075471698114,"Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal
543"
REFERENCES,0.5641509433962264,"llm. arXiv preprint arXiv:2309.05519, 2023.
544"
REFERENCES,0.5650943396226416,"Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J
545"
REFERENCES,0.5660377358490566,"DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual
546"
REFERENCES,0.5669811320754717,"cortex. PNAS, 111(23):8619–8624, 2014.
547"
REFERENCES,0.5679245283018868,"Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya
548"
REFERENCES,0.5688679245283019,"Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge
549"
REFERENCES,0.569811320754717,"through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer
550"
REFERENCES,0.5707547169811321,"Vision and Pattern Recognition, pp. 16375–16387, 2022.
551"
REFERENCES,0.5716981132075472,"Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language
552"
REFERENCES,0.5726415094339623,"model for video understanding. arXiv preprint arXiv:2306.02858, 2023.
553"
REFERENCES,0.5735849056603773,"NeurIPS Paper Checklist
554"
CLAIMS,0.5745283018867925,"1. Claims
555"
CLAIMS,0.5754716981132075,"Question: Do the main claims made in the abstract and introduction accurately reflect the
556"
CLAIMS,0.5764150943396227,"paper’s contributions and scope?
557"
CLAIMS,0.5773584905660377,"Answer: [Yes]
558"
CLAIMS,0.5783018867924529,"Justification: We have ensured that the main claims made in the abstract and introduction
559"
CLAIMS,0.5792452830188679,"are directly correlating to the research findings and the methods we have employed.
560"
CLAIMS,0.5801886792452831,"Guidelines:
561"
CLAIMS,0.5811320754716981,"• The answer NA means that the abstract and introduction do not include the claims
562"
CLAIMS,0.5820754716981132,"made in the paper.
563"
CLAIMS,0.5830188679245283,"• The abstract and/or introduction should clearly state the claims made, including the
564"
CLAIMS,0.5839622641509434,"contributions made in the paper and important assumptions and limitations. A No or
565"
CLAIMS,0.5849056603773585,"NA answer to this question will not be perceived well by the reviewers.
566"
CLAIMS,0.5858490566037736,"• The claims made should match theoretical and experimental results, and reflect how
567"
CLAIMS,0.5867924528301887,"much the results can be expected to generalize to other settings.
568"
CLAIMS,0.5877358490566038,"• It is fine to include aspirational goals as motivation as long as it is clear that these goals
569"
CLAIMS,0.5886792452830188,"are not attained by the paper.
570"
LIMITATIONS,0.589622641509434,"2. Limitations
571"
LIMITATIONS,0.590566037735849,"Question: Does the paper discuss the limitations of the work performed by the authors?
572"
LIMITATIONS,0.5915094339622642,"Answer: [Yes]
573"
LIMITATIONS,0.5924528301886792,"Justification: The paper discusses the main limitations of the work performed by the authors
574"
LIMITATIONS,0.5933962264150944,"in the discussion section.
575"
LIMITATIONS,0.5943396226415094,"Guidelines:
576"
LIMITATIONS,0.5952830188679246,"• The answer NA means that the paper has no limitation while the answer No means that
577"
LIMITATIONS,0.5962264150943396,"the paper has limitations, but those are not discussed in the paper.
578"
LIMITATIONS,0.5971698113207548,"• The authors are encouraged to create a separate ""Limitations"" section in their paper.
579"
LIMITATIONS,0.5981132075471698,"• The paper should point out any strong assumptions and how robust the results are to
580"
LIMITATIONS,0.5990566037735849,"violations of these assumptions (e.g., independence assumptions, noiseless settings,
581"
LIMITATIONS,0.6,"model well-specification, asymptotic approximations only holding locally). The authors
582"
LIMITATIONS,0.6009433962264151,"should reflect on how these assumptions might be violated in practice and what the
583"
LIMITATIONS,0.6018867924528302,"implications would be.
584"
LIMITATIONS,0.6028301886792453,"• The authors should reflect on the scope of the claims made, e.g., if the approach was
585"
LIMITATIONS,0.6037735849056604,"only tested on a few datasets or with a few runs. In general, empirical results often
586"
LIMITATIONS,0.6047169811320755,"depend on implicit assumptions, which should be articulated.
587"
LIMITATIONS,0.6056603773584905,"• The authors should reflect on the factors that influence the performance of the approach.
588"
LIMITATIONS,0.6066037735849057,"For example, a facial recognition algorithm may perform poorly when image resolution
589"
LIMITATIONS,0.6075471698113207,"is low or images are taken in low lighting. Or a speech-to-text system might not be
590"
LIMITATIONS,0.6084905660377359,"used reliably to provide closed captions for online lectures because it fails to handle
591"
LIMITATIONS,0.6094339622641509,"technical jargon.
592"
LIMITATIONS,0.6103773584905661,"• The authors should discuss the computational efficiency of the proposed algorithms
593"
LIMITATIONS,0.6113207547169811,"and how they scale with dataset size.
594"
LIMITATIONS,0.6122641509433963,"• If applicable, the authors should discuss possible limitations of their approach to
595"
LIMITATIONS,0.6132075471698113,"address problems of privacy and fairness.
596"
LIMITATIONS,0.6141509433962264,"• While the authors might fear that complete honesty about limitations might be used by
597"
LIMITATIONS,0.6150943396226415,"reviewers as grounds for rejection, a worse outcome might be that reviewers discover
598"
LIMITATIONS,0.6160377358490566,"limitations that aren’t acknowledged in the paper. The authors should use their best
599"
LIMITATIONS,0.6169811320754717,"judgment and recognize that individual actions in favor of transparency play an impor-
600"
LIMITATIONS,0.6179245283018868,"tant role in developing norms that preserve the integrity of the community. Reviewers
601"
LIMITATIONS,0.6188679245283019,"will be specifically instructed to not penalize honesty concerning limitations.
602"
THEORY ASSUMPTIONS AND PROOFS,0.619811320754717,"3. Theory Assumptions and Proofs
603"
THEORY ASSUMPTIONS AND PROOFS,0.620754716981132,"Question: For each theoretical result, does the paper provide the full set of assumptions and
604"
THEORY ASSUMPTIONS AND PROOFS,0.6216981132075472,"a complete (and correct) proof?
605"
THEORY ASSUMPTIONS AND PROOFS,0.6226415094339622,"Answer: [NA]
606"
THEORY ASSUMPTIONS AND PROOFS,0.6235849056603774,"Justification: Our paper does not require any explicit theorems and proofs.
607"
THEORY ASSUMPTIONS AND PROOFS,0.6245283018867924,"Guidelines:
608"
THEORY ASSUMPTIONS AND PROOFS,0.6254716981132076,"• The answer NA means that the paper does not include theoretical results.
609"
THEORY ASSUMPTIONS AND PROOFS,0.6264150943396226,"• All the theorems, formulas, and proofs in the paper should be numbered and cross-
610"
THEORY ASSUMPTIONS AND PROOFS,0.6273584905660378,"referenced.
611"
THEORY ASSUMPTIONS AND PROOFS,0.6283018867924528,"• All assumptions should be clearly stated or referenced in the statement of any theorems.
612"
THEORY ASSUMPTIONS AND PROOFS,0.629245283018868,"• The proofs can either appear in the main paper or the supplemental material, but if
613"
THEORY ASSUMPTIONS AND PROOFS,0.630188679245283,"they appear in the supplemental material, the authors are encouraged to provide a short
614"
THEORY ASSUMPTIONS AND PROOFS,0.6311320754716981,"proof sketch to provide intuition.
615"
THEORY ASSUMPTIONS AND PROOFS,0.6320754716981132,"• Inversely, any informal proof provided in the core of the paper should be complemented
616"
THEORY ASSUMPTIONS AND PROOFS,0.6330188679245283,"by formal proofs provided in appendix or supplemental material.
617"
THEORY ASSUMPTIONS AND PROOFS,0.6339622641509434,"• Theorems and Lemmas that the proof relies upon should be properly referenced.
618"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6349056603773585,"4. Experimental Result Reproducibility
619"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6358490566037736,"Question: Does the paper fully disclose all the information needed to reproduce the main ex-
620"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6367924528301887,"perimental results of the paper to the extent that it affects the main claims and/or conclusions
621"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6377358490566037,"of the paper (regardless of whether the code and data are provided or not)?
622"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6386792452830189,"Answer: [Yes]
623"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6396226415094339,"Justification: The paper has delineated all the information related to the experimental setup
624"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6405660377358491,"in the experimental setup section.
625"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6415094339622641,"Guidelines:
626"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6424528301886793,"• The answer NA means that the paper does not include experiments.
627"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6433962264150943,"• If the paper includes experiments, a No answer to this question will not be perceived
628"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6443396226415095,"well by the reviewers: Making the paper reproducible is important, regardless of
629"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6452830188679245,"whether the code and data are provided or not.
630"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6462264150943396,"• If the contribution is a dataset and/or model, the authors should describe the steps taken
631"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6471698113207547,"to make their results reproducible or verifiable.
632"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6481132075471698,"• Depending on the contribution, reproducibility can be accomplished in various ways.
633"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6490566037735849,"For example, if the contribution is a novel architecture, describing the architecture fully
634"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.65,"might suffice, or if the contribution is a specific model and empirical evaluation, it may
635"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6509433962264151,"be necessary to either make it possible for others to replicate the model with the same
636"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6518867924528302,"dataset, or provide access to the model. In general. releasing code and data is often
637"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6528301886792452,"one good way to accomplish this, but reproducibility can also be provided via detailed
638"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6537735849056604,"instructions for how to replicate the results, access to a hosted model (e.g., in the case
639"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6547169811320754,"of a large language model), releasing of a model checkpoint, or other means that are
640"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6556603773584906,"appropriate to the research performed.
641"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6566037735849056,"• While NeurIPS does not require releasing code, the conference does require all submis-
642"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6575471698113208,"sions to provide some reasonable avenue for reproducibility, which may depend on the
643"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6584905660377358,"nature of the contribution. For example
644"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.659433962264151,"(a) If the contribution is primarily a new algorithm, the paper should make it clear how
645"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.660377358490566,"to reproduce that algorithm.
646"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6613207547169812,"(b) If the contribution is primarily a new model architecture, the paper should describe
647"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6622641509433962,"the architecture clearly and fully.
648"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6632075471698113,"(c) If the contribution is a new model (e.g., a large language model), then there should
649"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6641509433962264,"either be a way to access this model for reproducing the results or a way to reproduce
650"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6650943396226415,"the model (e.g., with an open-source dataset or instructions for how to construct
651"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6660377358490566,"the dataset).
652"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6669811320754717,"(d) We recognize that reproducibility may be tricky in some cases, in which case
653"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6679245283018868,"authors are welcome to describe the particular way they provide for reproducibility.
654"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6688679245283019,"In the case of closed-source models, it may be that access to the model is limited in
655"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6698113207547169,"some way (e.g., to registered users), but it should be possible for other researchers
656"
EXPERIMENTAL RESULT REPRODUCIBILITY,0.6707547169811321,"to have some path to reproducing or verifying the results.
657"
OPEN ACCESS TO DATA AND CODE,0.6716981132075471,"5. Open access to data and code
658"
OPEN ACCESS TO DATA AND CODE,0.6726415094339623,"Question: Does the paper provide open access to the data and code, with sufficient instruc-
659"
OPEN ACCESS TO DATA AND CODE,0.6735849056603773,"tions to faithfully reproduce the main experimental results, as described in supplemental
660"
OPEN ACCESS TO DATA AND CODE,0.6745283018867925,"material?
661"
OPEN ACCESS TO DATA AND CODE,0.6754716981132075,"Answer: [NA]
662"
OPEN ACCESS TO DATA AND CODE,0.6764150943396227,"Justification: We will release the code upon acceptance. The dataset is publicly available
663"
OPEN ACCESS TO DATA AND CODE,0.6773584905660377,"through a licence.
664"
OPEN ACCESS TO DATA AND CODE,0.6783018867924528,"Guidelines:
665"
OPEN ACCESS TO DATA AND CODE,0.6792452830188679,"• The answer NA means that paper does not include experiments requiring code.
666"
OPEN ACCESS TO DATA AND CODE,0.680188679245283,"• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
667"
OPEN ACCESS TO DATA AND CODE,0.6811320754716981,"public/guides/CodeSubmissionPolicy) for more details.
668"
OPEN ACCESS TO DATA AND CODE,0.6820754716981132,"• While we encourage the release of code and data, we understand that this might not be
669"
OPEN ACCESS TO DATA AND CODE,0.6830188679245283,"possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
670"
OPEN ACCESS TO DATA AND CODE,0.6839622641509434,"including code, unless this is central to the contribution (e.g., for a new open-source
671"
OPEN ACCESS TO DATA AND CODE,0.6849056603773584,"benchmark).
672"
OPEN ACCESS TO DATA AND CODE,0.6858490566037736,"• The instructions should contain the exact command and environment needed to run to
673"
OPEN ACCESS TO DATA AND CODE,0.6867924528301886,"reproduce the results. See the NeurIPS code and data submission guidelines (https:
674"
OPEN ACCESS TO DATA AND CODE,0.6877358490566038,"//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
675"
OPEN ACCESS TO DATA AND CODE,0.6886792452830188,"• The authors should provide instructions on data access and preparation, including how
676"
OPEN ACCESS TO DATA AND CODE,0.689622641509434,"to access the raw data, preprocessed data, intermediate data, and generated data, etc.
677"
OPEN ACCESS TO DATA AND CODE,0.690566037735849,"• The authors should provide scripts to reproduce all experimental results for the new
678"
OPEN ACCESS TO DATA AND CODE,0.6915094339622642,"proposed method and baselines. If only a subset of experiments are reproducible, they
679"
OPEN ACCESS TO DATA AND CODE,0.6924528301886792,"should state which ones are omitted from the script and why.
680"
OPEN ACCESS TO DATA AND CODE,0.6933962264150944,"• At submission time, to preserve anonymity, the authors should release anonymized
681"
OPEN ACCESS TO DATA AND CODE,0.6943396226415094,"versions (if applicable).
682"
OPEN ACCESS TO DATA AND CODE,0.6952830188679245,"• Providing as much information as possible in supplemental material (appended to the
683"
OPEN ACCESS TO DATA AND CODE,0.6962264150943396,"paper) is recommended, but including URLs to data and code is permitted.
684"
OPEN ACCESS TO DATA AND CODE,0.6971698113207547,"6. Experimental Setting/Details
685"
OPEN ACCESS TO DATA AND CODE,0.6981132075471698,"Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
686"
OPEN ACCESS TO DATA AND CODE,0.6990566037735849,"parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
687"
OPEN ACCESS TO DATA AND CODE,0.7,"results?
688"
OPEN ACCESS TO DATA AND CODE,0.7009433962264151,"Answer: [Yes]
689"
OPEN ACCESS TO DATA AND CODE,0.7018867924528301,"Justification: We provide all the training and test details in the experimental setup.
690"
OPEN ACCESS TO DATA AND CODE,0.7028301886792453,"Guidelines:
691"
OPEN ACCESS TO DATA AND CODE,0.7037735849056603,"• The answer NA means that the paper does not include experiments.
692"
OPEN ACCESS TO DATA AND CODE,0.7047169811320755,"• The experimental setting should be presented in the core of the paper to a level of detail
693"
OPEN ACCESS TO DATA AND CODE,0.7056603773584905,"that is necessary to appreciate the results and make sense of them.
694"
OPEN ACCESS TO DATA AND CODE,0.7066037735849057,"• The full details can be provided either with the code, in appendix, or as supplemental
695"
OPEN ACCESS TO DATA AND CODE,0.7075471698113207,"material.
696"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7084905660377359,"7. Experiment Statistical Significance
697"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7094339622641509,"Question: Does the paper report error bars suitably and correctly defined or other appropriate
698"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.710377358490566,"information about the statistical significance of the experiments?
699"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7113207547169811,"Answer: [Yes]
700"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7122641509433962,"Justification: We conducted our experiments multiple times across 6 participants and took
701"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7132075471698113,"the average results. We also include error bars in the plots.
702"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7141509433962264,"Guidelines:
703"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7150943396226415,"• The answer NA means that the paper does not include experiments.
704"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7160377358490566,"• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
705"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7169811320754716,"dence intervals, or statistical significance tests, at least for the experiments that support
706"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7179245283018868,"the main claims of the paper.
707"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7188679245283018,"• The factors of variability that the error bars are capturing should be clearly stated (for
708"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.719811320754717,"example, train/test split, initialization, random drawing of some parameter, or overall
709"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.720754716981132,"run with given experimental conditions).
710"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7216981132075472,"• The method for calculating the error bars should be explained (closed form formula,
711"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7226415094339622,"call to a library function, bootstrap, etc.)
712"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7235849056603774,"• The assumptions made should be given (e.g., Normally distributed errors).
713"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7245283018867924,"• It should be clear whether the error bar is the standard deviation or the standard error
714"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7254716981132076,"of the mean.
715"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7264150943396226,"• It is OK to report 1-sigma error bars, but one should state it. The authors should
716"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7273584905660377,"preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
717"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7283018867924528,"of Normality of errors is not verified.
718"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7292452830188679,"• For asymmetric distributions, the authors should be careful not to show in tables or
719"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.730188679245283,"figures symmetric error bars that would yield results that are out of range (e.g. negative
720"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7311320754716981,"error rates).
721"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7320754716981132,"• If error bars are reported in tables or plots, The authors should explain in the text how
722"
EXPERIMENT STATISTICAL SIGNIFICANCE,0.7330188679245283,"they were calculated and reference the corresponding figures or tables in the text.
723"
EXPERIMENTS COMPUTE RESOURCES,0.7339622641509433,"8. Experiments Compute Resources
724"
EXPERIMENTS COMPUTE RESOURCES,0.7349056603773585,"Question: For each experiment, does the paper provide sufficient information on the com-
725"
EXPERIMENTS COMPUTE RESOURCES,0.7358490566037735,"puter resources (type of compute workers, memory, time of execution) needed to reproduce
726"
EXPERIMENTS COMPUTE RESOURCES,0.7367924528301887,"the experiments?
727"
EXPERIMENTS COMPUTE RESOURCES,0.7377358490566037,"Answer: [Yes]
728"
EXPERIMENTS COMPUTE RESOURCES,0.7386792452830189,"Justification: We have included the specifications of the hardware and software environments
729"
EXPERIMENTS COMPUTE RESOURCES,0.7396226415094339,"to ensure the reproducibility of our results.
730"
EXPERIMENTS COMPUTE RESOURCES,0.7405660377358491,"Guidelines:
731"
EXPERIMENTS COMPUTE RESOURCES,0.7415094339622641,"• The answer NA means that the paper does not include experiments.
732"
EXPERIMENTS COMPUTE RESOURCES,0.7424528301886792,"• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
733"
EXPERIMENTS COMPUTE RESOURCES,0.7433962264150943,"or cloud provider, including relevant memory and storage.
734"
EXPERIMENTS COMPUTE RESOURCES,0.7443396226415094,"• The paper should provide the amount of compute required for each of the individual
735"
EXPERIMENTS COMPUTE RESOURCES,0.7452830188679245,"experimental runs as well as estimate the total compute.
736"
EXPERIMENTS COMPUTE RESOURCES,0.7462264150943396,"• The paper should disclose whether the full research project required more compute
737"
EXPERIMENTS COMPUTE RESOURCES,0.7471698113207547,"than the experiments reported in the paper (e.g., preliminary or failed experiments that
738"
EXPERIMENTS COMPUTE RESOURCES,0.7481132075471698,"didn’t make it into the paper).
739"
CODE OF ETHICS,0.7490566037735849,"9. Code Of Ethics
740"
CODE OF ETHICS,0.75,"Question: Does the research conducted in the paper conform, in every respect, with the
741"
CODE OF ETHICS,0.7509433962264151,"NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
742"
CODE OF ETHICS,0.7518867924528302,"Answer: [Yes]
743"
CODE OF ETHICS,0.7528301886792453,"Justification: The research conducted in this paper fully conforms with the NeurIPS Code of
744"
CODE OF ETHICS,0.7537735849056604,"Ethics in every respect.
745"
CODE OF ETHICS,0.7547169811320755,"Guidelines:
746"
CODE OF ETHICS,0.7556603773584906,"• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
747"
CODE OF ETHICS,0.7566037735849057,"• If the authors answer No, they should explain the special circumstances that require a
748"
CODE OF ETHICS,0.7575471698113208,"deviation from the Code of Ethics.
749"
CODE OF ETHICS,0.7584905660377359,"• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
750"
CODE OF ETHICS,0.7594339622641509,"eration due to laws or regulations in their jurisdiction).
751"
BROADER IMPACTS,0.7603773584905661,"10. Broader Impacts
752"
BROADER IMPACTS,0.7613207547169811,"Question: Does the paper discuss both potential positive societal impacts and negative
753"
BROADER IMPACTS,0.7622641509433963,"societal impacts of the work performed?
754"
BROADER IMPACTS,0.7632075471698113,"Answer: [Yes]
755"
BROADER IMPACTS,0.7641509433962265,"Justification: The paper explores how the advancements and applications of our findings
756"
BROADER IMPACTS,0.7650943396226415,"could benefit society in terms of computational neuroscience research by specifically inves-
757"
BROADER IMPACTS,0.7660377358490567,"tigating the effectiveness of the current state of the art multimodal models in encoding brain
758"
BROADER IMPACTS,0.7669811320754717,"activity.
759"
BROADER IMPACTS,0.7679245283018868,"Guidelines:
760"
BROADER IMPACTS,0.7688679245283019,"• The answer NA means that there is no societal impact of the work performed.
761"
BROADER IMPACTS,0.769811320754717,"• If the authors answer NA or No, they should explain why their work has no societal
762"
BROADER IMPACTS,0.7707547169811321,"impact or why the paper does not address societal impact.
763"
BROADER IMPACTS,0.7716981132075472,"• Examples of negative societal impacts include potential malicious or unintended uses
764"
BROADER IMPACTS,0.7726415094339623,"(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
765"
BROADER IMPACTS,0.7735849056603774,"(e.g., deployment of technologies that could make decisions that unfairly impact specific
766"
BROADER IMPACTS,0.7745283018867924,"groups), privacy considerations, and security considerations.
767"
BROADER IMPACTS,0.7754716981132076,"• The conference expects that many papers will be foundational research and not tied
768"
BROADER IMPACTS,0.7764150943396226,"to particular applications, let alone deployments. However, if there is a direct path to
769"
BROADER IMPACTS,0.7773584905660378,"any negative applications, the authors should point it out. For example, it is legitimate
770"
BROADER IMPACTS,0.7783018867924528,"to point out that an improvement in the quality of generative models could be used to
771"
BROADER IMPACTS,0.779245283018868,"generate deepfakes for disinformation. On the other hand, it is not needed to point out
772"
BROADER IMPACTS,0.780188679245283,"that a generic algorithm for optimizing neural networks could enable people to train
773"
BROADER IMPACTS,0.7811320754716982,"models that generate Deepfakes faster.
774"
BROADER IMPACTS,0.7820754716981132,"• The authors should consider possible harms that could arise when the technology is
775"
BROADER IMPACTS,0.7830188679245284,"being used as intended and functioning correctly, harms that could arise when the
776"
BROADER IMPACTS,0.7839622641509434,"technology is being used as intended but gives incorrect results, and harms following
777"
BROADER IMPACTS,0.7849056603773585,"from (intentional or unintentional) misuse of the technology.
778"
BROADER IMPACTS,0.7858490566037736,"• If there are negative societal impacts, the authors could also discuss possible mitigation
779"
BROADER IMPACTS,0.7867924528301887,"strategies (e.g., gated release of models, providing defenses in addition to attacks,
780"
BROADER IMPACTS,0.7877358490566038,"mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
781"
BROADER IMPACTS,0.7886792452830189,"feedback over time, improving the efficiency and accessibility of ML).
782"
SAFEGUARDS,0.789622641509434,"11. Safeguards
783"
SAFEGUARDS,0.7905660377358491,"Question: Does the paper describe safeguards that have been put in place for responsible
784"
SAFEGUARDS,0.7915094339622641,"release of data or models that have a high risk for misuse (e.g., pretrained language models,
785"
SAFEGUARDS,0.7924528301886793,"image generators, or scraped datasets)?
786"
SAFEGUARDS,0.7933962264150943,"Answer: [NA]
787"
SAFEGUARDS,0.7943396226415095,"Justification: Our research does not pose any risks for misuse.
788"
SAFEGUARDS,0.7952830188679245,"Guidelines:
789"
SAFEGUARDS,0.7962264150943397,"• The answer NA means that the paper poses no such risks.
790"
SAFEGUARDS,0.7971698113207547,"• Released models that have a high risk for misuse or dual-use should be released with
791"
SAFEGUARDS,0.7981132075471699,"necessary safeguards to allow for controlled use of the model, for example by requiring
792"
SAFEGUARDS,0.7990566037735849,"that users adhere to usage guidelines or restrictions to access the model or implementing
793"
SAFEGUARDS,0.8,"safety filters.
794"
SAFEGUARDS,0.8009433962264151,"• Datasets that have been scraped from the Internet could pose safety risks. The authors
795"
SAFEGUARDS,0.8018867924528302,"should describe how they avoided releasing unsafe images.
796"
SAFEGUARDS,0.8028301886792453,"• We recognize that providing effective safeguards is challenging, and many papers do
797"
SAFEGUARDS,0.8037735849056604,"not require this, but we encourage authors to take this into account and make a best
798"
SAFEGUARDS,0.8047169811320755,"faith effort.
799"
LICENSES FOR EXISTING ASSETS,0.8056603773584906,"12. Licenses for existing assets
800"
LICENSES FOR EXISTING ASSETS,0.8066037735849056,"Question: Are the creators or original owners of assets (e.g., code, data, models), used in
801"
LICENSES FOR EXISTING ASSETS,0.8075471698113208,"the paper, properly credited and are the license and terms of use explicitly mentioned and
802"
LICENSES FOR EXISTING ASSETS,0.8084905660377358,"properly respected?
803"
LICENSES FOR EXISTING ASSETS,0.809433962264151,"Answer: [Yes]
804"
LICENSES FOR EXISTING ASSETS,0.810377358490566,"Justification: We have explicitly cited the datasets, code and models used.
805"
LICENSES FOR EXISTING ASSETS,0.8113207547169812,"Guidelines:
806"
LICENSES FOR EXISTING ASSETS,0.8122641509433962,"• The answer NA means that the paper does not use existing assets.
807"
LICENSES FOR EXISTING ASSETS,0.8132075471698114,"• The authors should cite the original paper that produced the code package or dataset.
808"
LICENSES FOR EXISTING ASSETS,0.8141509433962264,"• The authors should state which version of the asset is used and, if possible, include a
809"
LICENSES FOR EXISTING ASSETS,0.8150943396226416,"URL.
810"
LICENSES FOR EXISTING ASSETS,0.8160377358490566,"• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
811"
LICENSES FOR EXISTING ASSETS,0.8169811320754717,"• For scraped data from a particular source (e.g., website), the copyright and terms of
812"
LICENSES FOR EXISTING ASSETS,0.8179245283018868,"service of that source should be provided.
813"
LICENSES FOR EXISTING ASSETS,0.8188679245283019,"• If assets are released, the license, copyright information, and terms of use in the
814"
LICENSES FOR EXISTING ASSETS,0.819811320754717,"package should be provided. For popular datasets, paperswithcode.com/datasets
815"
LICENSES FOR EXISTING ASSETS,0.8207547169811321,"has curated licenses for some datasets. Their licensing guide can help determine the
816"
LICENSES FOR EXISTING ASSETS,0.8216981132075472,"license of a dataset.
817"
LICENSES FOR EXISTING ASSETS,0.8226415094339623,"• For existing datasets that are re-packaged, both the original license and the license of
818"
LICENSES FOR EXISTING ASSETS,0.8235849056603773,"the derived asset (if it has changed) should be provided.
819"
LICENSES FOR EXISTING ASSETS,0.8245283018867925,"• If this information is not available online, the authors are encouraged to reach out to
820"
LICENSES FOR EXISTING ASSETS,0.8254716981132075,"the asset’s creators.
821"
NEW ASSETS,0.8264150943396227,"13. New Assets
822"
NEW ASSETS,0.8273584905660377,"Question: Are new assets introduced in the paper well documented and is the documentation
823"
NEW ASSETS,0.8283018867924529,"provided alongside the assets?
824"
NEW ASSETS,0.8292452830188679,"Answer: [NA]
825"
NEW ASSETS,0.8301886792452831,"Justification: We will try to opensource the code and provide complete documentation for
826"
NEW ASSETS,0.8311320754716981,"our assets upon acceptance.
827"
NEW ASSETS,0.8320754716981132,"Guidelines:
828"
NEW ASSETS,0.8330188679245283,"• The answer NA means that the paper does not release new assets.
829"
NEW ASSETS,0.8339622641509434,"• Researchers should communicate the details of the dataset/code/model as part of their
830"
NEW ASSETS,0.8349056603773585,"submissions via structured templates. This includes details about training, license,
831"
NEW ASSETS,0.8358490566037736,"limitations, etc.
832"
NEW ASSETS,0.8367924528301887,"• The paper should discuss whether and how consent was obtained from people whose
833"
NEW ASSETS,0.8377358490566038,"asset is used.
834"
NEW ASSETS,0.8386792452830188,"• At submission time, remember to anonymize your assets (if applicable). You can either
835"
NEW ASSETS,0.839622641509434,"create an anonymized URL or include an anonymized zip file.
836"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.840566037735849,"14. Crowdsourcing and Research with Human Subjects
837"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8415094339622642,"Question: For crowdsourcing experiments and research with human subjects, does the paper
838"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8424528301886792,"include the full text of instructions given to participants and screenshots, if applicable, as
839"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8433962264150944,"well as details about compensation (if any)?
840"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8443396226415094,"Answer: [NA]
841"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8452830188679246,"Justification: We use publicly available fMRI dataset and do not collect any new data.
842"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8462264150943396,"Guidelines:
843"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8471698113207548,"• The answer NA means that the paper does not involve crowdsourcing nor research with
844"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8481132075471698,"human subjects.
845"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8490566037735849,"• Including this information in the supplemental material is fine, but if the main contribu-
846"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.85,"tion of the paper involves human subjects, then as much detail as possible should be
847"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8509433962264151,"included in the main paper.
848"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8518867924528302,"• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
849"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8528301886792453,"or other labor should be paid at least the minimum wage in the country of the data
850"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8537735849056604,"collector.
851"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8547169811320755,"15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
852"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8556603773584905,"Subjects
853"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8566037735849057,"Question: Does the paper describe potential risks incurred by study participants, whether
854"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8575471698113207,"such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
855"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8584905660377359,"approvals (or an equivalent approval/review based on the requirements of your country or
856"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8594339622641509,"institution) were obtained?
857"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8603773584905661,"Answer: [NA]
858"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8613207547169811,"Justification: We use publicly available fMRI dataset and do not collect any new data.
859"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8622641509433963,"Guidelines:
860"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8632075471698113,"• The answer NA means that the paper does not involve crowdsourcing nor research with
861"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8641509433962264,"human subjects.
862"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8650943396226415,"• Depending on the country in which research is conducted, IRB approval (or equivalent)
863"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8660377358490566,"may be required for any human subjects research. If you obtained IRB approval, you
864"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8669811320754717,"should clearly state this in the paper.
865"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8679245283018868,"• We recognize that the procedures for this may vary significantly between institutions
866"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8688679245283019,"and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
867"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.869811320754717,"guidelines for their institution.
868"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.870754716981132,"• For initial submissions, do not include any information that would break anonymity (if
869"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8716981132075472,"applicable), such as the institution conducting the review.
870"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8726415094339622,"A
Cross-subject prediction accuracy
871"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8735849056603774,"We estimate cross-subject prediction accuracy in three settings: (i) training with The Bourne
872"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8745283018867924,"supremacy and testing with Life data, (ii) training with The wolf of wall street and testing with
873"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8754716981132076,"Life data, and (iii) training with both The Bourne supremacy and The wolf of wall street and testing
874"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8764150943396226,"with Life data. We present the average cross-subject prediction accuracy across voxels for the
875"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8773584905660378,"Movie10 fMRI dataset and across the three settings in Fig. 6.
876"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8783018867924528,"All
Bourn
Wolf 0.06 0.08 0.1"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.879245283018868,"0.12
Whole Brain: Cross-subject prediction accuracy"
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.880188679245283,Avg Pearson Correlation AG ATL PTL IFG MFG
CROWDSOURCING AND RESEARCH WITH HUMAN SUBJECTS,0.8811320754716981,IFGOrb PCC dmPFC EVC LO FFA PPA MT AC 0.06 0.08 0.1 0.12 0.14 0.16
CROSS-SUBJECT PREDICTION ACCURACY,0.8820754716981132,0.18 Cross-subject prediction accuracy
CROSS-SUBJECT PREDICTION ACCURACY,0.8830188679245283,Avg Pearson Correlation
CROSS-SUBJECT PREDICTION ACCURACY,0.8839622641509434,"Figure 6: Cross-subject prediction accuracy: (top) across whole brain, (bottom) across language,
visual and auditory regions."
CROSS-SUBJECT PREDICTION ACCURACY,0.8849056603773585,"B
Detailed sub-ROIs of language, visual and auditory regions
877"
CROSS-SUBJECT PREDICTION ACCURACY,0.8858490566037736,"The data covers seven brain regions of interest (ROIs) in the human brain with the following sub-
878"
CROSS-SUBJECT PREDICTION ACCURACY,0.8867924528301887,"divisions: (i) early visual (EV: V1, V2, V3, V3B, and V4); (ii) object-related areas (LO1 and LO2);
879"
CROSS-SUBJECT PREDICTION ACCURACY,0.8877358490566037,"(iii) face-related areas (OFA), (iv) scene-related areas (PPA), (v) middle temporal (MT: MT, MST,
880"
CROSS-SUBJECT PREDICTION ACCURACY,0.8886792452830189,"LO3, FST and V3CD), (vi) late language regions, encompassing broader language regions: angular
881"
CROSS-SUBJECT PREDICTION ACCURACY,0.8896226415094339,"gyrus (AG: PFm, PGs, PGi, TPOJ2, TPOJ3), lateral temporal cortex (LTC: STSda, STSva, STGa,
882"
CROSS-SUBJECT PREDICTION ACCURACY,0.8905660377358491,"TE1a, TE2a, TGv, TGd, A5, STSdp, STSvp, PSL, STV, TPOJ1), inferior frontal gyrus (IFG: 44, 45,
883"
CROSS-SUBJECT PREDICTION ACCURACY,0.8915094339622641,"IFJa, IFSp) and middle frontal gyrus (MFG: 55b) (Baker et al., 2018; Milton et al., 2021; Desai et al.,
884"
CROSS-SUBJECT PREDICTION ACCURACY,0.8924528301886793,"2022).
885"
CROSS-SUBJECT PREDICTION ACCURACY,0.8933962264150943,"C
Details of pretrained Transformer models
886"
CROSS-SUBJECT PREDICTION ACCURACY,0.8943396226415095,Table 1: Pretrained Transformer-based Encoder Models. All models have 12 layers.
CROSS-SUBJECT PREDICTION ACCURACY,0.8952830188679245,"Model Name
Pretraining
Cross-modal Pretrained (ImageBind)
Video & Audio
Jointly Pretrained (TVLT)
Video & Audio
ViT-B
Image
VideoMAE
Video
ViViT
Video
Wav2Vec2.0-base
Speech
AST
Speech"
CROSS-SUBJECT PREDICTION ACCURACY,0.8962264150943396,"D
Effectiveness of multi-modal vs unimodal representations for various brain
887"
CROSS-SUBJECT PREDICTION ACCURACY,0.8971698113207547,"regions
888"
CROSS-SUBJECT PREDICTION ACCURACY,0.8981132075471698,"We now present the results for per unimodal video model and per speech model in Fig. 8. Similar to
889"
CROSS-SUBJECT PREDICTION ACCURACY,0.8990566037735849,"the average results of unimodal video and speech models, we observe that multi-modal models exhibit
890"
CROSS-SUBJECT PREDICTION ACCURACY,0.9,"better normalized brain alignment than individual unimodal video and speech models across language
891"
CROSS-SUBJECT PREDICTION ACCURACY,0.9009433962264151,"and visual regions. Among unimodal speech models, the AST model shows better normalized brain
892"
CROSS-SUBJECT PREDICTION ACCURACY,0.9018867924528302,"alignment than the Wav2vec2.0 model. Among unimodal video models, each unimodal video model
893"
CROSS-SUBJECT PREDICTION ACCURACY,0.9028301886792452,"displays notably consistent performance across regions.
894"
CROSS-SUBJECT PREDICTION ACCURACY,0.9037735849056604,"*^
^ ^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9047169811320754,"ATL
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9056603773584906,"Normalized brain alignment *^
^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9066037735849056,"IFGOrb
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9075471698113208,"Normalized brain alignment *^
*^ ^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9084905660377358,"MFG
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.909433962264151,Normalized brain alignment *^ ^ *^
CROSS-SUBJECT PREDICTION ACCURACY,0.910377358490566,"PCC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9113207547169812,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9122641509433962,"^
^ *^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9132075471698113,"dmPFC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9141509433962264,"Normalized brain alignment ^
^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9150943396226415,"OV
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9160377358490566,Normalized brain alignment ^ ^ ^ ^
CROSS-SUBJECT PREDICTION ACCURACY,0.9169811320754717,"FV
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9179245283018868,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9188679245283019,"Figure 7: Average normalized brain alignment for per video and audio modalities from multi-modal
and individual modality features across whole brain and several ROIs of language (ATL, IFGOrb,
MFG, PCC, dmPFC) and visual (OV, FV). Error bars indicate the standard error of the mean across
participants."
CROSS-SUBJECT PREDICTION ACCURACY,0.9198113207547169,"E
How is the brain alignment of multi-modal features affected by the
895"
CROSS-SUBJECT PREDICTION ACCURACY,0.9207547169811321,"elimination of a particular modality?
896"
CROSS-SUBJECT PREDICTION ACCURACY,0.9216981132075471,"To understand the contribution of each modality to the multi-modal brain alignment for multi-modal
897"
CROSS-SUBJECT PREDICTION ACCURACY,0.9226415094339623,"naturalistic stimulus, we perform residual analyses by removing the unimodality features from
898"
CROSS-SUBJECT PREDICTION ACCURACY,0.9235849056603773,"multi-modal joint representations as well as multi-modal video or audio representations from joint
899"
CROSS-SUBJECT PREDICTION ACCURACY,0.9245283018867925,"representations and measure the differences in brain alignment before and after removal modality-
900"
CROSS-SUBJECT PREDICTION ACCURACY,0.9254716981132075,"specific features. Figs. 9 and 10 display the normalized brain alignment for language ROIs such as
901"
CROSS-SUBJECT PREDICTION ACCURACY,0.9264150943396227,"PTL, MFG, ATL, PCC and visual regions EVC, OV and FV. We note a decrease in brain alignment
902"
CROSS-SUBJECT PREDICTION ACCURACY,0.9273584905660377,"for these regions following the removal of video embeddings from cross-modality models, whereas
903"
CROSS-SUBJECT PREDICTION ACCURACY,0.9283018867924528,"the removal of audio embeddings does not affect the brain alignment. On the other hand, for jointly
904"
CROSS-SUBJECT PREDICTION ACCURACY,0.9292452830188679,"pretrained models, removal of both video and audio embeddings partially impacts the brain alignment.
905"
CROSS-SUBJECT PREDICTION ACCURACY,0.930188679245283,"F
Layerwise brain alignment
906"
CROSS-SUBJECT PREDICTION ACCURACY,0.9311320754716981,"We now plot the layer-wise normalized brain alignment for the Unimodal models and TVLT joint
907"
CROSS-SUBJECT PREDICTION ACCURACY,0.9320754716981132,"model, as shown in Fig. 11. Observation from Fig. 11 indicates a consistent drop in performance from
908"
CROSS-SUBJECT PREDICTION ACCURACY,0.9330188679245283,"early to lower layers, specifically for both TVLT joint and unimodal video models. The key finding
909"
CROSS-SUBJECT PREDICTION ACCURACY,0.9339622641509434,"here is that our results that TVLT joint embeddings showcase improved brain alignment across all the
910"
CROSS-SUBJECT PREDICTION ACCURACY,0.9349056603773584,"layers compared to unimodal video and speech embeddings.
911 ^ ^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9358490566037736,"AG
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9367924528301886,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.9377358490566038,Normalized brain alignment ^ ^
CROSS-SUBJECT PREDICTION ACCURACY,0.9386792452830188,"PTL
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.939622641509434,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.940566037735849,"Normalized brain alignment ^
^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9415094339622642,"ATL
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9424528301886792,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.9433962264150944,Normalized brain alignment ^ ^
CROSS-SUBJECT PREDICTION ACCURACY,0.9443396226415094,"PCC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9452830188679245,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.9462264150943396,"Normalized brain alignment ^
^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9471698113207547,"IFG
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9481132075471698,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.9490566037735849,"Normalized brain alignment ^
^"
CROSS-SUBJECT PREDICTION ACCURACY,0.95,"dmPFC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9509433962264151,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.9518867924528301,"Normalized brain alignment ^
^"
CROSS-SUBJECT PREDICTION ACCURACY,0.9528301886792453,"EVC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9537735849056603,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.9547169811320755,Normalized brain alignment ^ ^
CROSS-SUBJECT PREDICTION ACCURACY,0.9556603773584905,"MT
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9566037735849057,"Random
TVLT Joint
ViViT
Wav2Vec2.0
IB Concat
VideoMae
ViT-B
AST"
CROSS-SUBJECT PREDICTION ACCURACY,0.9575471698113207,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9584905660377359,"Figure 8: Average normalized brain alignment for video and audio modalities from multi-modal
and individual modality features across whole brain and several ROIs of language (ATL, ATL, PTL,
IFG, PCC, dmPFC) and visual (EVC, MT). Error bars indicate the standard error of the mean across
participants."
CROSS-SUBJECT PREDICTION ACCURACY,0.9594339622641509,"ATL
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.960377358490566,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9613207547169811,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9622641509433962,"PTL
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9632075471698113,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9641509433962264,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9650943396226415,"IFG
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9660377358490566,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9669811320754716,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9679245283018868,"MFG
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9688679245283018,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.969811320754717,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.970754716981132,"IFGOrb
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9716981132075472,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9726415094339622,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9735849056603774,"PCC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9745283018867924,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9754716981132076,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9764150943396226,"dmPFC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9773584905660377,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9783018867924528,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9792452830188679,"Figure 9: Residual analysis for ATL, PTL, IFG, MFG, IFGOrb, PCC and dmPFC regions: Average
normalized brain alignment was computed across participants before and after removal of video and
audio embeddings from both jointly pretrained and cross-modality models. Error bars indicate the
standard error of the mean across participants."
CROSS-SUBJECT PREDICTION ACCURACY,0.980188679245283,"EVC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9811320754716981,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9820754716981132,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9830188679245283,"OV
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9839622641509433,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9849056603773585,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9858490566037735,"SV
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9867924528301887,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9877358490566037,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9886792452830189,"FV
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9896226415094339,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9905660377358491,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9915094339622641,"AC
0.1 0.2 0.3 0.4 0.5 0.6"
CROSS-SUBJECT PREDICTION ACCURACY,0.9924528301886792,"IB Concat
TVLT Joint
IB Concat - IB Video
TVLT Joint - TVLT Video
IB Concat - IB Audio
TVLT Joint - TVLT Audio
IB Concat - Unimodal VM
TVLT Joint - Unimodal VM
IB Concat - Unimodal SM
TVLT Joint - Unimodal SM"
CROSS-SUBJECT PREDICTION ACCURACY,0.9933962264150943,Normalized brain alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9943396226415094,"Figure 10: Residual analysis for EVC, OV, SV, FV and AC regions: Average normalized brain
alignment was computed across participants before and after removal of video and audio embeddings
from both jointly pretrained and cross-modality models. Error bars indicate the standard error of the
mean across participants."
CROSS-SUBJECT PREDICTION ACCURACY,0.9952830188679245,"1
2
3
4
5
6
7
8
9
10
11
12
0.3 0.4 0.5"
CROSS-SUBJECT PREDICTION ACCURACY,0.9962264150943396,"Unimodal VM
Unimodal SM
TVLT Joint Embeddings"
CROSS-SUBJECT PREDICTION ACCURACY,0.9971698113207547,Layer Depth
CROSS-SUBJECT PREDICTION ACCURACY,0.9981132075471698,Avg Normalized Brain Alignment
CROSS-SUBJECT PREDICTION ACCURACY,0.9990566037735849,"Figure 11: Normalized brain alignment across layers for multi-modal model (TVLT joint embeddings)
and unimodal video and speech models."
