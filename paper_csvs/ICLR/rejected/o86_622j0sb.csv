Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.004347826086956522,"Deep neural networks are vulnerable to adversarial examples, even in the black-
box setting where the attacker only has query access to the model output. Re-
cent studies have devised successful black-box attacks with high query efﬁciency.
However, such performance often comes at the cost of the imperceptibility of ad-
versarial attacks, which is essential for attackers. To address this issue, in this
paper we propose to use segmentation priors for black-box attacks such that the
perturbations are limited in the salient region. We ﬁnd that state-of-the-art black-
box attacks equipped with segmentation priors can achieve much better imper-
ceptibility performance with little reduction in query efﬁciency and success rate.
We further propose the Saliency Attack, a new gradient-free black-box attack that
can further improve the imperceptibility by reﬁning perturbations in the salient re-
gion. Experimental results show that the perturbations generated by our approach
are much more imperceptible than the ones generated by other attacks, and are in-
terpretable to some extent. Furthermore, our approach is found to be more robust
to detection-based defense, which demonstrates its efﬁcacy as well."
INTRODUCTION,0.008695652173913044,"1
INTRODUCTION"
INTRODUCTION,0.013043478260869565,"Deep neural networks (DNNs) have achieved signiﬁcant progress in wide applications, such as im-
age classiﬁcation (Deng et al., 2009), face recognition (Parkhi et al., 2015), object detection (Red-
mon et al., 2016), speech recognition (4, 2012) and machine translation (Bahdanau et al., 2015). De-
spite their success, deep learning models have revealed vulnerability to adversarial attacks (Szegedy
et al., 2014). Crafted by adding some small perturbations to benign inputs, adversarial examples
(AEs) can fool DNNs into making wrong predictions, which is a critical threat especially for some
security-sensitive scenarios such as autonomous driving (Sun et al., 2020)."
INTRODUCTION,0.017391304347826087,"Based on the knowledge of target model, the adversarial attacks could be divided into white-box
attack and black-box attack. White-box attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Pa-
pernot et al., 2016; Carlini & Wagner, 2017; Moosavi-Dezfooli et al., 2016) have full access to the
architecture and parameters of the target model, and can easily generate a successful adversarial ex-
ample via back propagation. By contrast, black-box attacks (Narodytska & Kasiviswanathan, 2016;
Ilyas et al., 2018; 2019; Moon et al., 2019; Andriushchenko et al., 2020; Li et al., 2021; Papernot
et al., 2017; Liu et al., 2017) can only query the target model to obtain the output prediction, which
is a more realistic and challenging setting. Since many real-world online application programming
interfaces (APIs) have time or monetary limit for user query (Ilyas et al., 2018), most current re-
search efforts focus on how to design query-efﬁcient attacks (Ilyas et al., 2019; Moon et al., 2019;
Andriushchenko et al., 2020), which indeed achieve a huge improvement. For example, so far the
state-of-the-art black-box attack (Andriushchenko et al., 2020) could succeed in untargeted attack
on ImageNet dataset (Deng et al., 2009) with only tens of queries on average. However, this success
has sacriﬁced the imperceptibility of AEs, whose global perturbations are generated with various
tricks like random vertical stripes (Andriushchenko et al., 2020), and thus very obvious and percep-
tible to human eyes (see Figure 1 (e) and (f) for examples). In effect, imperceptibility is essential for
attackers. Currently online APIs usually integrate detectors into their services to detect anomalous
inputs (Li et al., 2021). The images with too visible perturbations are difﬁcult to pass these detectors,
not to mention human judgement."
INTRODUCTION,0.021739130434782608,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.02608695652173913,"(a)
(b)
(c)
(d) Saliency (ours) (e) Parsimonious
(f) Square
(g) Original
Figure 1: Illustration of our Saliency Attack and current SOTA gradient-free black-box attacks. (a)
BP saliency maps, speciﬁc to a given image and the corresponding class; (b) binary salient masks,
generated by saliency object segmentation and roughtly accord with the region of the salient pixels
in BP saliency maps; (c) perturbation, produced by reﬁning in salient regions of (b); (d) ﬁnal AEs
of our Saliency Attack; (e-f) AEs of SOTA black-box attacks (Moon et al., 2019; Andriushchenko
et al., 2020) whose perturbations are visible to human eyes; (g) original image."
INTRODUCTION,0.030434782608695653,"To alleviate or even address this problem, we could utilize some priors to lead black-box attacks
to search in small but efﬁcient space, instead of global perturbations. Previous DNNs visualization
work (Simonyan et al., 2014) proposed a type of saliency map to indicate which features should
inﬂuence the output signiﬁcantly. This BP saliency map 1 is generated by calculating the derivatives
of the model output with respect to input. Just as show in Figure 1.(a), the brighter pixels in BP
saliency maps denote they have a greater impact on the model output. The classical white-box
attack JSMA (Papernot et al., 2016) just constructs such an adversarial saliency map and iteratively
selects pixels from this map to perturb. But due to the black-box setting, we cannot derive this
saliency map directly. Nevertheless, we could ﬁnd that the region of bright pixels roughly represents
the position of the main object in an image, which also accords with our intuition. Inspired by this,
we propose to ﬁrst generate a saliency mask to extract the regions of the main object in an image
via image segmentation, and then limit the perturbation search in this smaller but more important
regions, as presented in Figure 1."
INTRODUCTION,0.034782608695652174,"Several recent studies also propose to generate similar local perturbations but applied in different at-
tack settings with different methods (Dong et al., 2020; Xiang et al., 2021). In particular, Dong et al.
(2020) produce saliency maps with the class activation mapping (CAM) (Zhou et al., 2016), which
requires the internal information of the model and needs to change the model architecture. This
is infeasible in black-box scenarios. Xiang et al. (2021) replace CAM with Grad-CAM (Selvaraju
et al., 2017), which does not need to change the architecture of the model. However, both CAM and
Grad-CAM still rely on the knowledge of the model, but in black-box setting the internal informa-
tion of the target model is inaccessible. Thus, we choose salient object segmentation (Zhao & Wu,
2019), which can automatically extract salient object(s) in an image (see Figure 1.(b)). Compared
with CAM and Grad-CAM, it doesn’t need any other information except for the input image."
INTRODUCTION,0.0391304347826087,"On the other hand, these studies (Dong et al., 2020; Xiang et al., 2021) are applied to transfer-based
attacks (actually a grey-box setting), which ﬁrst build a substitute model to approximate the tar-
get model, then use gradient-based white-box attacks to generate adversarial examples, and ﬁnally
transfer them to attack the target model. For transfer-based attack, we must either train a substi-
tute model from scratch with huge number of queries (Papernot et al., 2017) or assume a pretrained
substitute model is trained on the similar data distribution with the target model’s (Liu et al., 2017).
And for the latter, the transferability of both adversarial examples and saliency maps highly depends
on the selection of the pair of substitute model(s) and target model. Hence, in this paper we focus
on gradient-free black-box attacks (Narodytska & Kasiviswanathan, 2016; Moon et al., 2019; An-
driushchenko et al., 2020) that take no account of gradients and search for successful perturbations
only according to the model output."
INTRODUCTION,0.043478260869565216,"We add these segmentation priors to SOTA black-box attacks and ﬁnd the imperceptibility of AEs is
indeed enhanced but still limited due to ”global perturbation” in local region (See Figure 4 and 5). To"
INTRODUCTION,0.04782608695652174,"1To distinguish the saliency map generated by the salient object segmentation model below, here we call it
as BP saliency map to indicate it is generated via back propagation."
INTRODUCTION,0.05217391304347826,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.05652173913043478,"further improve the imperceptibility, our another novelty is that even in the salient region, “saliency”
could still be reﬁned. Our inspiration also comes from some DNNs visualization works (Zhou et al.,
2016; Zeiler & Fergus, 2014; Olah et al., 2018). For instance, given a dog image, CAM produces a
localization heatmap and shows the dog face region is most highly lighted (Zhou et al., 2016). Zeiler
& Fergus (2014) systematically cover up different portions of a dog image, and ﬁnds when the dog
face is obscured, the activity in the feature map and classiﬁer output changes dramatically. In further,
by combining internal feature visualization with output prediction, Olah et al. (2018) reveal even in
dog’s face region, its ears and eyes seem to be more important when distinguishing dogs. Therefore,
we are inspired to assume the salient region in an image is progressive with respect to its impact
on model output. If we could ﬁnd smaller but more salient region, the perturbation will be more
efﬁcient and meanwhile the imperceptibility of AEs could also be enhanced. Thus, we propose our
Saliency Attack, a new gradient-free black-box attack via reﬁning perturbations in the salient region
according to their saliency."
INTRODUCTION,0.06086956521739131,Our main contributions can be summarized as follows:
INTRODUCTION,0.06521739130434782,"• To our best knowledge, we are the ﬁrst to use salient object segmentation to extract binary
salient masks in black-box settings. Experiments show that the SOTA black-box attacks
limited in such regions can achieve much better imperceptibility performance with little
reduction in query efﬁciency and success rate."
INTRODUCTION,0.06956521739130435,"• We propose a new gradient-free black-box attack via reﬁning in salient regions. Compared
with the search methods used in other gradient-free attacks, our method is able to generate
smaller but effective perturbations, which is interpretable to some extent and can further
improve the imperceptibility."
INTRODUCTION,0.07391304347826087,"• We demonstrate that the perturbations generated by our Saliency Attack is more robust
against some detection-based defense like Feature Squeezing."
RELATED WORK,0.0782608695652174,"2
RELATED WORK"
BLACK-BOX ATTACKS,0.08260869565217391,"2.1
BLACK-BOX ATTACKS"
BLACK-BOX ATTACKS,0.08695652173913043,"Gradient estimation attacks. Gradient estimation attacks ﬁrst estimate the gradients by querying
the target model and then apply them to run white-box attacks (Goodfellow et al., 2015; Carlini
& Wagner, 2017; Madry et al., 2018). ZOO attack (Chen et al., 2017) ﬁrst adopts the symmetric
difference quotient to approximate the gradients and then perform Carlini-Wagner (CW) white-box
attack (Carlini & Wagner, 2017). AutoZOOM (Tu et al., 2019) uses a random vector based gradient
estimation to estimate gradients, which reduces the query number per iteration from 2D in ZOO to
N+1 (D is the dimensionality and N is the sample size). To further enhance query efﬁciency, Ilyas
et al. (2019) propose the “tiling trick” that updates a square of pixels simultaneously instead of a
single pixel, which dramatically decreases the dimensionality by a factor of k2 (k is tile length)."
BLACK-BOX ATTACKS,0.09130434782608696,"Gradient-free attacks. Gradient-free attacks take no account of gradients and directly generate AEs
with random search or heuristic methods according to the query result. Su et al. (2019) propose one
pixel attack that adopts differential evolution algorithm to perturb the most important pixel in the
image. Alzantot et al. (2019) propose GenAttack, which use genetic algorithm to generate AEs. To
improve the query efﬁciency, Moon et al. (2019) consider a discrete surrogate optimization problem
that transforms the original constraint of a continuous range [−ϵ, +ϵ] to a discrete set {−ϵ, +ϵ},
achieving a huge reduction in the search space. This is motivated by linear program (LP) where
the optimal solution is attained at an extreme point of the feasible set (Schrijver, 1998). Combing
tiling trick (Ilyas et al., 2019) and discrete optimization (Moon et al., 2019), Square Attack (An-
driushchenko et al., 2020) has obtained the best result on success rate and query performance so far
with a randomized search scheme."
BLACK-BOX ATTACKS,0.09565217391304348,"Hybrid attacks. Hybrid attacks try to combine multiple types of methods, using candidate adversar-
ial examples generated on substitute model as starting points for further optimization with gradient
estimation attacks or gradient-free attacks. Suya et al. (2020) ﬁrst proposed to generate candidate
AEs by white-box projected gradient descent (PGD) attack (Madry et al., 2018), and further opti-
mize them with gradient estimation methods. Wang et al. (2020) combines a white-box attack with a"
BLACK-BOX ATTACKS,0.1,Under review as a conference paper at ICLR 2022
BLACK-BOX ATTACKS,0.10434782608695652,"gradient-free method using microbial genetic algorithm (Harvey, 2009). Similar with transfer-based
attacks, hybrid attacks also suffer from the similarity between substitute model(s) and target model."
RELATED WORK ON IMPERCEPTIBILITY,0.10869565217391304,"2.2
RELATED WORK ON IMPERCEPTIBILITY"
RELATED WORK ON IMPERCEPTIBILITY,0.11304347826086956,"The imperceptibility of AEs is vital for attackers and some attacks consider it in different manners.
Guo et al. (2019) diﬁne the imperceptibility as the smoothness in the sense of low frequency and
thus search for AEs in frequency domain. While Zhang et al. (2020) regard the imperceptibility as
the smoothness of visual content in an image and integrate Laplacian smoothing into optimization.
Boundary Attack (Brendel et al., 2018) starts from an example that is already adversarial and ap-
proaches the original image gradually to reduce the distortion. Instead of global perturbations, some
studies (Dong et al., 2020; Xiang et al., 2021) consider local perturbations to reduce the square of
perturbations."
RELATED WORK ON IMPERCEPTIBILITY,0.11739130434782609,"On the other hand, how to estimate the imperceptibility of AEs is still a problem. Most of the ad-
versarial attacks use Lp norms (L0, L2 and L∞) to measure the human perceptual distance between
the perturbed image and the original one. Nonetheless, Lp norms are found not suitable enough
for human vision system (Sharif et al., 2018). To ﬁnd a proper metric, Fezza et al. (2019) design
subjective experiments to obtain the subjective scores on different AEs, and then test various im-
age ﬁdelity assessment (IFA) metrics including Lp norms. Among them, most apparent distortion
(MAD) (Larson & Chandler, 2010) metric is found closest to subjective scores. Hence, in this paper
we adopt MAD as our main imperceptibility metric to estimate the imperceptibility of AEs."
PROPOSED METHOD,0.12173913043478261,"3
PROPOSED METHOD"
PROPOSED METHOD,0.12608695652173912,"In this section, we ﬁrst introduce the problem formulation of crafting adversarial examples for image
classiﬁcation models, and then detail our approach. The overall ﬂowchart of our proposed Saliency
Attack is depicted in Figure 2."
PROPOSED METHOD,0.13043478260869565,Salient object
PROPOSED METHOD,0.13478260869565217,"segmetnation
Clean Example"
PROPOSED METHOD,0.1391304347826087,Saliency Map
PROPOSED METHOD,0.14347826086956522,Binarize
PROPOSED METHOD,0.14782608695652175,"Binary Mask
Perturbation"
PROPOSED METHOD,0.15217391304347827,Adversarial Example
PROPOSED METHOD,0.1565217391304348,"Refining in 
saliency region"
PROPOSED METHOD,0.1608695652173913,Figure 2: The overall ﬂowchart of Saliency Attack.
PRELIMINARY,0.16521739130434782,"3.1
PRELIMINARY"
PRELIMINARY,0.16956521739130434,"Given a well-trained DNN classiﬁer F : [0, 1]d →RK, where d is the dimension of the input x, and
K is the number of classes. We denote Fk(x) as the predicted score that x belongs to class k. So the
classiﬁer assigns the class that maximizes Fk(x) to the input x. The goal of an untargeted attack is
to ﬁnd an adversarial example xadv, that results in the model misclassiﬁcation from the ground-truth
class y, and meanwhile keeps the distance between the adversarial and benign input smaller than a
threshold ϵ:
arg max
k=1,...,K
Fk(xadv) ̸= y,
s.t. ||xadv −x||p ≤ϵ, xadv ∈[0, 1]d
(1)"
PRELIMINARY,0.17391304347826086,"where the other constraint indicates that the generated xadv must be an image in valid range. In this
paper we focus on L∞as our distance norm following (Moon et al., 2019; Andriushchenko et al.,
2020)."
PRELIMINARY,0.1782608695652174,"Traditionally, this task of ﬁnding xadv can be rephrased as solving a constrained continuous problem:
max
xadv∈[0,1]d L(F(xadv), y),
s.t. ||xadv −x||∞≤ϵ
(2)"
PRELIMINARY,0.1826086956521739,"where L is a loss function. To improve the query efﬁciency, Moon et al. (2019) transform the
continuous problem into a discrete surrogate problem, where the perturbation δ = xadv −x is"
PRELIMINARY,0.18695652173913044,Under review as a conference paper at ICLR 2022
PRELIMINARY,0.19130434782608696,"generated at the corner of the L∞ball. Besides, we propose to limit δ only in the salient region. So
our complete optimization problem is:"
PRELIMINARY,0.1956521739130435,"max
xadv∈[0,1]d L(F(xadv), y),
s.t. ||δ||∞∈{−ϵ, +ϵ}, δ ∈S
(3)"
PRELIMINARY,0.2,where S is the set of the pixels in salient region.
SALIENT OBJECT SEGMENTATION,0.20434782608695654,"3.2
SALIENT OBJECT SEGMENTATION"
SALIENT OBJECT SEGMENTATION,0.20869565217391303,"Salient object segmentation, as one changing task in image segmentation domain, aims to automat-
ically and accurately extract salient object in an image. Speciﬁcally, given an input image, salient
object segmentation model can generate a saliency map where the brighter pixels denote they have
higher saliency scores, just as the saliency map shows in Figure 2. Compared with other model vi-
sualization methods like CAM (Zhou et al., 2016) and Grad-CAM (Selvaraju et al., 2017), this type
of model does not require any information other than the input image, which is very suitable for
the black-box setting. We adopt the Pyramid Feature Attention (PFA) network (Zhao & Wu, 2019)
that achieves the state-of-the-art performance in multiple datasets. After obtaining the saliency map
where all pixel values are between 0 and 1, we use a binarization threshold φ to transform the
saliency map to a binary salient mask. The binarization can be expressed as"
SALIENT OBJECT SEGMENTATION,0.21304347826086956,"s∗
i,j =
0
si,j < φ
1
si,j ≥φ
(4)"
SALIENT OBJECT SEGMENTATION,0.21739130434782608,"where s is the saliency map and s∗is the binary salient mask, sij and s∗
i,j are the corresponding
value at the position i, j. With this saliency mask s∗, we could limit the perturbation search in the
salient region."
REFINING PERTURBATION IN SALIENT REGION,0.2217391304347826,"3.3
REFINING PERTURBATION IN SALIENT REGION …
…"
REFINING PERTURBATION IN SALIENT REGION,0.22608695652173913,"Figure 3: The illustration of reﬁning on a
tree structure corresponding to the image
blocks. Among initial blocks, each block
is a root node of a tree, and its child nodes
are the ﬁner blocks in the current block."
REFINING PERTURBATION IN SALIENT REGION,0.23043478260869565,"In this part, we design a search algorithm to reﬁne per-
turbations based on a tree structure for an image by us-
ing the idea of depth-ﬁrst search. As shown in Figure
3, an input image merged with its saliency mask is split
into four initial blocks (a coarse grid). Then we try to
add a +ϵ or −ϵ perturbation on each initial block in-
dividually and ﬁnd the best block that maximizes the
loss function for further perturbation. After ﬁnding the
best one among all initial blocks, we can further split
this block into smaller blocks (a ﬁner grid), and again
try to add a perturbation on each block individually (at
this time we just ﬂip the perturbation for convenience
e.g. +ϵ to −ϵ) to ﬁnd the best smaller block. We iterate
this process until the minimal block (e.g. 1 pixel) or no
smaller block has a better loss. Then we go back to the
last level of split blocks and use the second best block
for further perturbation."
REFINING PERTURBATION IN SALIENT REGION,0.23478260869565218,"We use the loss function from CW attack (Carlini &
Wagner, 2017) for untargted attack:"
REFINING PERTURBATION IN SALIENT REGION,0.2391304347826087,"L(x) = −max(Z(xadv)o −max
i̸=o (Z(xadv)i), 0) (5)"
REFINING PERTURBATION IN SALIENT REGION,0.24347826086956523,"where Z(xadv)o is the logit with respect to the true class of the original image. In this way, the loss
function is imposed to leave a margin between the true class and other classes. The reﬁning process
is presented in Algorithm 1, and the overall saliency attack is given in Algorithm 2."
EXPERIMENTS,0.24782608695652175,"4
EXPERIMENTS"
EXPERIMENTS,0.25217391304347825,"In this paper, we evaluate the performance of the Saliency Attack comparing against Boundary At-
tack (Brendel et al., 2018), TVDBA (Li & Chen, 2021) Parsimonious attack (Moon et al., 2019),"
EXPERIMENTS,0.2565217391304348,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.2608695652173913,"Algorithm 1: Reﬁne
Input: block b, block size k
Output: adversarial example xadv"
EXPERIMENTS,0.26521739130434785,1 k′ is the block size of b;
EXPERIMENTS,0.26956521739130435,"2 {b1, b2, ..., bn} ←split b into (k′/k)2 ﬁner blocks;"
EXPERIMENTS,0.27391304347826084,"3 {bπ(1), bπ(2), ..., bπ(n)} ←sort {b1, b2, ..., bn} in descending order according to L(b), which is
calculated by using Eq. 5;"
EXPERIMENTS,0.2782608695652174,"4 for each block e ∈{bπ(1), bπ(2), ..., bπ(n)} do"
EXPERIMENTS,0.2826086956521739,"5
if L(e) > ˆL then"
EXPERIMENTS,0.28695652173913044,"6
xadv ←x + δ ∪{e};"
EXPERIMENTS,0.29130434782608694,"7
ˆL ←L(e);"
EXPERIMENTS,0.2956521739130435,"8
if k > 1 then"
EXPERIMENTS,0.3,"9
k ←k/2;"
EXPERIMENTS,0.30434782608695654,"10
Recursively call xadv ←Reﬁne(e, k);"
END,0.30869565217391304,"11
end"
END,0.3130434782608696,"12
end"
END,0.3173913043478261,13 end
END,0.3217391304347826,"Algorithm 2: Saliency Attack
Input: original image x, initial block size kint, query budget
Output: adversarial example xadv"
END,0.32608695652173914,1 δ ←∅is the perturbation;
END,0.33043478260869563,2 while kint > 1 and not exceeding query budget do
END,0.3347826086956522,"3
Running Algorithm 2: xadv ←Reﬁne(x, kint);"
END,0.3391304347826087,"4
kint ←kint/2;"
END,0.34347826086956523,5 end
END,0.34782608695652173,"Square attack (Andriushchenko et al., 2020) and their modiﬁed version equipped with segmentation
priors. Among them, Boundary Attack is able to constantly reduce the distortion to a very small
magnitude, TVDBA tries to minimize the distortion via integrating Structural SIMilarity (SSIM)
(Wang et al., 2004) into the loss, and Square Attack can achieve the current state-of-the-art query
efﬁciency and success rate in the black-box setting. We consider L∞threat model on ImageNet
dataset (Deng et al., 2009), set ϵ to 0.05 in [0,1] scale, and test 1000 randomly selected exmaples in
all experiments. The threshold φ to produce binary salient masks is chosen to be 0.1. All parameters
of the compared attacks remain consistent with those recommended in their papers. We use Incep-
tion v3 (Szegedy et al., 2016) as the target model, and different query budgets {3000, 10000, 30000}
for untargeted attack. For performance metrics, we employ commonly used success rate (SR) and
average number of queries (Avg. queries). To evaluate the imperceptibility of AEs, we consider L0,
L2 and MAD (see the appendix for details), which is validated as closest to human vision system
among existing IFA metrics (Fezza et al., 2019). All these three imperceptibility metrics are the
smaller the better."
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3521739130434783,"4.1
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS"
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3565217391304348,"To verify the feasibility of our segmentation priors, we modify Parsimonious Attack and Square
Attack so that the perturbations are limited in salient regions. We show the results in Table 1 and
some examples in Figure 4. From the examples, we can easily ﬁnd that the perturbations of original
attacks are very obvious in the entire image due to global perturbations, while their segmentation
versions are relatively more imperceptible since no perturbation exists in background regions."
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.36086956521739133,"Besides, through quantitative analysis, the attacks equipped with segmentation priors can indeed
achieve much better imperceptibility performance with around one third improvement in L2 and
MAD metric. L0 is also restricted to less than 50%, which implies the average square of salient
regions among all examples only occupies less than half of the whole image. And this improvement
only comes at little cost in SR and Avg. queries, which is still in the same order of magnitude. We
also present the change of MAD scores in Figure 9 in appendix. Thus, the results demonstrate that
the segmentation prior is indeed able to enhance the imperceptibility of other black-box attacks."
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3652173913043478,Under review as a conference paper at ICLR 2022
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3695652173913043,Table 1: Comparison of black-box attacks with their modiﬁed version with segmentation priors.
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3739130434782609,"Method
Avg. queries
SR
L2
L0
MAD"
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3782608695652174,"Parsimonious
739
98.2%
53.20
100.0%
22.17
Parsimonious-seg
828
96.7%
45.64
42.70%
13.95"
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3826086956521739,"Square
222
99.7%
59.76
99.03%
25.36
Square-seg
563
97.1%
40.45
41.89%
16.10"
SOTA BLACK-BOX ATTACKS WITH SEGMENTATION PRIORS,0.3869565217391304,"(a)
(b) Parsimonious Attack
(c) Parsimonious-seg
(d) Square Attack
(e) Square-seg
Figure 4: Examples of SOTA black-box attacks compared with their modeﬁed version with segmen-
tation priors. (a) binary salient mask; (b-e) pairs of AEs and their perturbation."
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.391304347826087,"4.2
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.39565217391304347,"Although the segmentation prior is helpful to the imperceptibility, the perturbations generated by
other attacks are still global, taking up almost all salient regions. Hence, we design our Saliency
Attack via further reﬁning perturbations in salient regions. Original image are resized to 256 × 256
and we set the initial block size to 16. For an imperceptible AE, a low MAD value is important.
So besides SR, we use a new metric SRtrue. It denotes the rate of successful AEs whose MAD
values are below some threshold. Here we choose 30 as the threshold because we ﬁnd the AEs with
MAD ≤30 are basically imperceptible to human eyes (See Figure 10 in appendix for details)."
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4,"The comparison results are given in Table 2. We can ﬁnd in all query budgets, our Saliency Attack
achieves outstanding performance with a huge gap in terms of SRtrue and three imperceptibility
metrics. Although TVDBA, Parsimonious and Square Attack can obtain a better SR, their success-
ful AEs are not true imperceptible, and will be easily detected by some defenses or humans. For
Boundary Attack, it can gradually make progress on SRtrue, L2 and MAD as the query budget in-
creases, but they are still much worse than Saliency Attack. This is because it takes at least hundreds
of thousands of queries for Boundary Attack to converge (Brendel et al., 2018), which is infeasible
in practical. We further test more eamples to verify the statistical signiﬁcance in Table 5, and present
a comparison of SRtrue versus query number in Figure 11 in appendix."
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4043478260869565,"From the exhabited examples in Figure 5, we can ﬁnd the AEs of Boundary Attack contain obvious
coarse textures because of inadequate query budget. In addition, even restricted in the same salient
regions, the perturbations of Parsimonious-seg and Square seg attack are complex and irregular,
while our perturbations are smaller and more important, roughtly corresponding to the bright pixels
in BP saliency map. They further represent the postions of dogs’ noses or ears, which accord with
our inspiration before."
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.40869565217391307,"(a) Boundary Attack
(b) Parsimonious-seg
(c) Square-seg
(d) Saliency Attack
(e)
Figure 5: Examples of our Saliency Attack and other compared attacks. (a-d) pairs of AEs and their
perturbation; (e) BP saliency maps."
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.41304347826086957,Under review as a conference paper at ICLR 2022
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.41739130434782606,"Table 2: Comparison of different attacks. SRtrue denotes the rate of successful AEs with MAD ≤
30, which are believed imperceptible to human eyes."
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4217391304347826,"Method
Query Budget
SR
SRtrue
L2
L0
MAD"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4260869565217391,Boundary
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.43043478260869567,"3000
100.0%
13.6%
68.13
99.6%
84.96
10000
100.0%
30.2%
58.82
99.7%
71.61
30000
100.0%
46.0%
58.30
99.8%
62.77 TVDBA"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.43478260869565216,"3000
90.1%
22.9%
10.77
19.7%
39.20
10000
96.9%
23.2%
10.96
20.5%
39.42
30000
98.4%
23.0%
11.06
20.7%
39.66"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4391304347826087,Parsimonious
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4434782608695652,"3000
91.9%
14.0%
22.17
100.0%
52.01
10000
98.2%
14.1%
22.17
100.0%
53.20
30000
99.8%
14.1%
22.17
100.0%
53.36"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.44782608695652176,Parsimonious-seg
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.45217391304347826,"3000
89.8%
10.8%
14.01
42.7%
45.50
10000
96.7%
12.0%
13.95
42.7%
45.64
30000
99.7%
12.4%
13.92
42.7%
45.48"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.45652173913043476,Square
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4608695652173913,"3000
98.4%
2.2%
25.36
99.0%
59.29
10000
99.7%
1.9%
25.36
99.0%
59.76
30000
99.9%
2.7%
25.35
99.0%
58.24"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4652173913043478,Square-seg
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.46956521739130436,"3000
89.5%
19.9%
16.26
42.7%
40.41
10000
97.1%
22.5%
16.10
41.9%
40.45
30000
98.7%
25.5%
16.02
41.7%
39.05"
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.47391304347826085,Saliency (ours)
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS,0.4782608695652174,"3000
85.6%
80.2%
3.52
3.3%
12.32
10000
93.6%
86.2%
3.71
3.8%
12.88
30000
96.1%
87.9%
3.89
4.3%
13.28"
HYPERPARAMETER TUNING,0.4826086956521739,"4.3
HYPERPARAMETER TUNING"
HYPERPARAMETER TUNING,0.48695652173913045,"Our Saliency Attack contains only one hyperparameter namely the initial block size k which deter-
mines the ﬁrst level of split blocks. We test the effect of different k on SR, imperceptibility and query
number respectively in Figure 6. As k decreases, SR and imperceptibility can be improved and SR
reaches the peak when k equals 16. This is because with smaller initial blocks, Saliency Attack can
search for perturbations more ﬁnely and accurately leading to higher SR and better imperceptibility
performance. Meanwhile inevitably more queries are needed, especially for sorting initial blocks.
That is why under a limited query budget like 10000, a turning point occurs in SR."
HYPERPARAMETER TUNING,0.49130434782608695,"We also show some examples in Figure 7. It can be observed that as k decreases, the generated
perturbations also become smaller but more salient. For instance, in the ﬁrst row the perturbation
with k = 128 roughly covers the region of the dog face, which is also the brightest region in the
BP saliency map. While the perturbation with k = 32 or k = 16 focuses on smaller region of the
dog ear. This indicates that our Saliency Attack could ﬁnd smaller and more important perturbations
progressively as the initial block size decreases. And the perturbations are interpretable to some
extent like perturbing the speciﬁc regions of dog’s ears, eyes or nose."
ABLATION STUDY,0.4956521739130435,"4.4
ABLATION STUDY"
ABLATION STUDY,0.5,"We do ablation study of Saliency Attack, including reﬁning in salient region, in non-salient region
and without saliency (reﬁning in the whole image). We also design a greedy search as baseline to
demonstrate our Reﬁne search. We test multiple block sizes for greedy search and use 32 as the best
choice (see Table 6 in appendix). The results and some examples are given in Table 3 and Figure
8. Notice that reﬁning in salient region and reﬁning without saliency generate the same or almost
the same perturbations, which means the salient regions indeed contain useful parts and enhance the
query efﬁciency by limiting the search space. But for reﬁning in non-salient region, its perturbation
is more complex and visible with worse query efﬁciency and SR. Compared with greedy search, our
Reﬁne search has much better query efﬁciency and SRtrue, which proves its superiority. Hence, we
can conclude that both salient region and Reﬁne search facilitate our Saliency Attack."
ABLATION STUDY,0.5043478260869565,Under review as a conference paper at ICLR 2022
ABLATION STUDY,0.508695652173913,"128
64
32
16
8
Block size 0.8 0.85 0.9 0.95 SR"
ABLATION STUDY,0.5130434782608696,"128
64
32
16
8
Block size 0 5 10 15 20 25"
ABLATION STUDY,0.5173913043478261,Imperceptibility
ABLATION STUDY,0.5217391304347826,"MAD
L2"
ABLATION STUDY,0.5260869565217391,"128
64
32
16
8
Block size 0 1000 2000 3000 4000"
ABLATION STUDY,0.5304347826086957,Avg. queries
ABLATION STUDY,0.5347826086956522,"Figure 6: The effect of initial block size in SR, imperceptibility (L2 and MAD) and avg. queries."
ABLATION STUDY,0.5391304347826087,"(a)
(b) k = 128
(c) k = 32
(d) k = 16
(e)
Figure 7: Examples of DFS-seg attack with different initial block sizes. (a) binary salient mask;
(b-d) exmaples of Saliency Attack with different initial block sizes; (e) BP saliency map."
ABLATION STUDY,0.5434782608695652,Table 3: Ablation study of Saliency attack under 10000 query budget.
ABLATION STUDY,0.5478260869565217,"Method
Avg. queries
SR
SRtrue
L2
L0
MAD"
ABLATION STUDY,0.5521739130434783,"reﬁne in salient region
1958
93.6%
86.2%
3.71
3.8%
12.88
reﬁne in non-salient region
3128
78.2%
57.0%
4.94
6.5%
21.58
reﬁne without saliency
2563
95.5%
79.6%
3.84
4.2%
16.35"
ABLATION STUDY,0.5565217391304348,"greedy search in salient region
2727
56.0%
50.7%
4.37
4.7%
12.87"
ABLATION STUDY,0.5608695652173913,"(a) Saliency Mask
(b) in salient region
(c) without saliency
(d) in non-salient region
Figure 8: Examples of Saliency Attack with different strategies."
ATTACKING DETECTION-BASED DEFENSE,0.5652173913043478,"4.5
ATTACKING DETECTION-BASED DEFENSE"
ATTACKING DETECTION-BASED DEFENSE,0.5695652173913044,"To further validate the superiority of our method, we attack against one detection-based defense Fea-
ture Squeezing (Xu et al., 2018), which applies transformations to input image and judge whether it
is adversarial through the stability of model output. We consider the detection rate of both successful
adversarial examples (SAEs) and failure adversarial examples (FAEs). This is because if we could
detect FAEs, we can raise the alarm that some potential attacker is attacking the model. From Table
4, we can ﬁnd our Saliency attack is able to achieve lower detection rate in both SAEs and FAEs
than other attacks, which proves the imperceptibility of our AEs from the defensive perspective."
ATTACKING DETECTION-BASED DEFENSE,0.5739130434782609,Table 4: Results for attacking against detection-based defense
ATTACKING DETECTION-BASED DEFENSE,0.5782608695652174,"Parsimonious
Square
Boundary
Saliency"
ATTACKING DETECTION-BASED DEFENSE,0.5826086956521739,"Detection rate (SAEs)
33.3%
47.8%
61.5%
21.2%
Detection rate (FAEs)
14.5%
16.7%
\
11.7%"
ATTACKING DETECTION-BASED DEFENSE,0.5869565217391305,Under review as a conference paper at ICLR 2022
CONCLUSION,0.591304347826087,"5
CONCLUSION"
CONCLUSION,0.5956521739130435,"In this paper, we propose to provide segmentation priors for black-box attacks to improve the imper-
ceptibility of adversarial examples, whose perturbations are limited in salient regions. We utilize a
salient object segmentation model to produce such saliency maps with no need for any information
other than the input image. Experiments show that SOTA black-box attacks equipped with seg-
mentation priors can indeed achieve better imperceptibility performance. Furthermore, we devise a
new gradient-free black-box Saliency attack that further enhances the imperceptibility via reﬁning
in salient regions. The perturbations generated are much smaller, imperceptible and interpretable to
some extent. Finally, we also attack against some detection-based defense and the results show that
the adversarial examples generated by our attack are harder to be detected."
REFERENCES,0.6,REFERENCES
REFERENCES,0.6043478260869565,"Deep neural networks for acoustic modeling in speech recognition: The shared views of four re-
search groups. IEEE Signal Process. Mag., 29(6):82–97, 2012. doi: 10.1109/MSP.2012.2205597.
URL https://doi.org/10.1109/MSP.2012.2205597."
REFERENCES,0.6086956521739131,"Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang, Cho-Jui Hsieh, and Mani B.
Srivastava. Genattack: practical black-box attacks with gradient-free optimization. In Anne Auger
and Thomas St¨utzle (eds.), Proceedings of the Genetic and Evolutionary Computation Confer-
ence, GECCO 2019, Prague, Czech Republic, July 13-17, 2019, pp. 1111–1119. ACM, 2019. doi:
10.1145/3321707.3321749. URL https://doi.org/10.1145/3321707.3321749."
REFERENCES,0.6130434782608696,"Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: A query-efﬁcient black-box adversarial attack via random search.
In Andrea Vedaldi,
Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 -
16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIII, vol-
ume 12368 of Lecture Notes in Computer Science, pp. 484–501. Springer, 2020. doi: 10.1007/
978-3-030-58592-1\ 29.
URL https://doi.org/10.1007/978-3-030-58592-1_
29."
REFERENCES,0.6173913043478261,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate.
In Yoshua Bengio and Yann LeCun (eds.), 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Con-
ference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473."
REFERENCES,0.6217391304347826,"Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. In 6th International Conference on Learn-
ing Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=
SyZI0GWCZ."
REFERENCES,0.6260869565217392,"Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
2017 IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017,
pp. 39–57. IEEE Computer Society, 2017. doi: 10.1109/SP.2017.49. URL https://doi.
org/10.1109/SP.2017.49."
REFERENCES,0.6304347826086957,"Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
ZOO: zeroth order
optimization based black-box attacks to deep neural networks without training substitute mod-
els. In Bhavani M. Thuraisingham, Battista Biggio, David Mandell Freeman, Brad Miller, and
Arunesh Sinha (eds.), Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Se-
curity, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pp. 15–26. ACM, 2017. doi:
10.1145/3128572.3140448. URL https://doi.org/10.1145/3128572.3140448."
REFERENCES,0.6347826086956522,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248–255. IEEE
Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. URL https://doi.org/10.
1109/CVPR.2009.5206848."
REFERENCES,0.6391304347826087,Under review as a conference paper at ICLR 2022
REFERENCES,0.6434782608695652,"Xiaoyi Dong, Jiangfan Han, Dongdong Chen, Jiayang Liu, Huanyu Bian, Zehua Ma, Hongsheng
Li, Xiaogang Wang, Weiming Zhang, and Nenghai Yu. Robust superpixel-guided attentional
adversarial attack. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 12892–12901. Computer Vision Founda-
tion / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01291. URL https://openaccess.
thecvf.com/content_CVPR_2020/html/Dong_Robust_Superpixel-Guided_
Attentional_Adversarial_Attack_CVPR_2020_paper.html."
REFERENCES,0.6478260869565218,"Sid Ahmed Fezza, Yassine Bakhti, Wassim Hamidouche, and Olivier D´eforges. Perceptual evalu-
ation of adversarial attacks for cnn-based image classiﬁcation. In 11th International Conference
on Quality of Multimedia Experience QoMEX 2019, Berlin, Germany, June 5-7, 2019, pp. 1–
6. IEEE, 2019. doi: 10.1109/QoMEX.2019.8743213. URL https://doi.org/10.1109/
QoMEX.2019.8743213."
REFERENCES,0.6521739130434783,"Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015. URL http://arxiv.org/abs/1412.6572."
REFERENCES,0.6565217391304348,"Chuan Guo, Jared S. Frank, and Kilian Q. Weinberger.
Low frequency adversarial perturba-
tion. In Amir Globerson and Ricardo Silva (eds.), Proceedings of the Thirty-Fifth Conference
on Uncertainty in Artiﬁcial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, volume
115 of Proceedings of Machine Learning Research, pp. 1127–1137. AUAI Press, 2019. URL
http://proceedings.mlr.press/v115/guo20a.html."
REFERENCES,0.6608695652173913,"Inman Harvey. The microbial genetic algorithm. In George Kampis, Istv´an Karsai, and E¨ors Sza-
thm´ary (eds.), Advances in Artiﬁcial Life. Darwin Meets von Neumann - 10th European Confer-
ence, ECAL 2009, Budapest, Hungary, September 13-16, 2009, Revised Selected Papers, Part II,
volume 5778 of Lecture Notes in Computer Science, pp. 126–133. Springer, 2009. doi: 10.1007/
978-3-642-21314-4\ 16.
URL https://doi.org/10.1007/978-3-642-21314-4_
16."
REFERENCES,0.6652173913043479,"Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 2142–
2151. PMLR, 2018. URL http://proceedings.mlr.press/v80/ilyas18a.html."
REFERENCES,0.6695652173913044,"Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial
attacks with bandits and priors. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
URL https:
//openreview.net/forum?id=BkMiWhR5K7."
REFERENCES,0.6739130434782609,"Eric C. Larson and Damon M. Chandler. Most apparent distortion: full-reference image quality
assessment and the role of strategy. J. Electronic Imaging, 19(1):011006, 2010. doi: 10.1117/1.
3267105. URL https://doi.org/10.1117/1.3267105."
REFERENCES,0.6782608695652174,"Nannan Li and Zhenzhong Chen. Toward visual distortion in black-box attacks. IEEE Transactions
on Image Processing, 30:6156–6167, 2021."
REFERENCES,0.6826086956521739,"Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu, and Chunming Wu. Ad-
versarial examples versus cloud-based detectors: A black-box empirical study. IEEE Trans. De-
pendable Secur. Comput., 18(4):1933–1949, 2021. doi: 10.1109/TDSC.2019.2943467. URL
https://doi.org/10.1109/TDSC.2019.2943467."
REFERENCES,0.6869565217391305,"Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial exam-
ples and black-box attacks. In 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.
URL https://openreview.net/forum?id=Sys6GJqxl."
REFERENCES,0.691304347826087,"Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In 6th International Conference"
REFERENCES,0.6956521739130435,Under review as a conference paper at ICLR 2022
REFERENCES,0.7,"on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,
Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/
forum?id=rJzIBfZAb."
REFERENCES,0.7043478260869566,"Seungyong Moon, Gaon An, and Hyun Oh Song. Parsimonious black-box adversarial attacks via ef-
ﬁcient combinatorial optimization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp.
4636–4645. PMLR, 2019.
URL http://proceedings.mlr.press/v97/moon19a.
html."
REFERENCES,0.7086956521739131,"Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2574–2582.
IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.282. URL https://doi.org/10.
1109/CVPR.2016.282."
REFERENCES,0.7130434782608696,"Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations
for deep networks. CoRR, abs/1612.06299, 2016. URL http://arxiv.org/abs/1612.
06299."
REFERENCES,0.717391304347826,"Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and
Alexander Mordvintsev. The building blocks of interpretability. Distill, 2018. doi: 10.23915/
distill.00010. https://distill.pub/2018/building-blocks."
REFERENCES,0.7217391304347827,"Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Anan-
thram Swami. The limitations of deep learning in adversarial settings. In IEEE European Sympo-
sium on Security and Privacy, EuroS&P 2016, Saarbr¨ucken, Germany, March 21-24, 2016, pp.
372–387. IEEE, 2016. doi: 10.1109/EuroSP.2016.36. URL https://doi.org/10.1109/
EuroSP.2016.36."
REFERENCES,0.7260869565217392,"Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Anan-
thram Swami. Practical black-box attacks against machine learning. In Ramesh Karri, Ozgur
Sinanoglu, Ahmad-Reza Sadeghi, and Xun Yi (eds.), Proceedings of the 2017 ACM on Asia Con-
ference on Computer and Communications Security, AsiaCCS 2017, Abu Dhabi, United Arab
Emirates, April 2-6, 2017, pp. 506–519. ACM, 2017. doi: 10.1145/3052973.3053009. URL
https://doi.org/10.1145/3052973.3053009."
REFERENCES,0.7304347826086957,"Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. In Xianghua Xie,
Mark W. Jones, and Gary K. L. Tam (eds.), Proceedings of the British Machine Vision Conference
2015, BMVC 2015, Swansea, UK, September 7-10, 2015, pp. 41.1–41.12. BMVA Press, 2015.
doi: 10.5244/C.29.41. URL https://doi.org/10.5244/C.29.41."
REFERENCES,0.7347826086956522,"Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once:
Uniﬁed, real-time object detection. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 779–788. IEEE Computer
Society, 2016. doi: 10.1109/CVPR.2016.91. URL https://doi.org/10.1109/CVPR.
2016.91."
REFERENCES,0.7391304347826086,"Alexander Schrijver. Theory of linear and integer programming. John Wiley & Sons, 1998."
REFERENCES,0.7434782608695653,"Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October
22-29, 2017, pp. 618–626. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.74. URL
https://doi.org/10.1109/ICCV.2017.74."
REFERENCES,0.7478260869565218,"Mahmood Sharif, Lujo Bauer, and Michael K. Reiter.
On the suitability of lp-norms
for
creating
and
preventing
adversarial
examples.
In
2018
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
Workshops,
CVPR
Workshops
2018,
Salt
Lake City,
UT, USA, June 18-22,
2018,
pp. 1605–1613. Computer Vision Founda-
tion / IEEE Computer Society,
2018.
doi:
10.1109/CVPRW.2018.00211.
URL"
REFERENCES,0.7521739130434782,Under review as a conference paper at ICLR 2022
REFERENCES,0.7565217391304347,"http://openaccess.thecvf.com/content_cvpr_2018_workshops/w32/
html/Sharif_On_the_Suitability_CVPR_2018_paper.html."
REFERENCES,0.7608695652173914,"Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps.
In Yoshua Bengio and Yann Le-
Cun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Workshop Track Proceedings, 2014. URL http://arxiv.org/
abs/1312.6034."
REFERENCES,0.7652173913043478,"Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep
neural networks. IEEE Trans. Evol. Comput., 23(5):828–841, 2019. doi: 10.1109/TEVC.2019.
2890858. URL https://doi.org/10.1109/TEVC.2019.2890858."
REFERENCES,0.7695652173913043,"Qi Sun, Arjun Ashok Rao, Xufeng Yao, Bei Yu, and Shiyan Hu. Counteracting adversarial attacks
in autonomous driving. In IEEE/ACM International Conference On Computer Aided Design,
ICCAD 2020, San Diego, CA, USA, November 2-5, 2020, pp. 83:1–83:7. IEEE, 2020. doi: 10.
1145/3400302.3415758. URL https://doi.org/10.1145/3400302.3415758."
REFERENCES,0.7739130434782608,"Fnu Suya, Jianfeng Chi, David Evans, and Yuan Tian. Hybrid batch attacks: Finding black-box
adversarial examples with limited queries.
In Srdjan Capkun and Franziska Roesner (eds.),
29th USENIX Security Symposium, USENIX Security 2020, August 12-14, 2020, pp. 1327–
1344. USENIX Association, 2020.
URL https://www.usenix.org/conference/
usenixsecurity20/presentation/suya."
REFERENCES,0.7782608695652173,"Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Good-
fellow, and Rob Fergus.
Intriguing properties of neural networks.
In Yoshua Bengio and
Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.
URL http:
//arxiv.org/abs/1312.6199."
REFERENCES,0.782608695652174,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-
thinking the inception architecture for computer vision.
In 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016,
pp. 2818–2826. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.308. URL https:
//doi.org/10.1109/CVPR.2016.308."
REFERENCES,0.7869565217391304,"Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh,
and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for
attacking black-box neural networks. In The Thirty-Third AAAI Conference on Artiﬁcial Intelli-
gence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference,
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI
2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 742–749. AAAI Press, 2019.
doi: 10.1609/aaai.v33i01.3301742. URL https://doi.org/10.1609/aaai.v33i01.
3301742."
REFERENCES,0.7913043478260869,"Lina Wang, Kang Yang, Wenqi Wang, Run Wang, and Aoshuang Ye. Mgaattack: Toward more
query-efﬁcient black-box attack by microbial genetic algorithm. In Chang Wen Chen, Rita Cuc-
chiara, Xian-Sheng Hua, Guo-Jun Qi, Elisa Ricci, Zhengyou Zhang, and Roger Zimmermann
(eds.), MM ’20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle,
WA, USA, October 12-16, 2020, pp. 2229–2236. ACM, 2020. doi: 10.1145/3394171.3413703.
URL https://doi.org/10.1145/3394171.3413703."
REFERENCES,0.7956521739130434,"Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment:
from error visibility to structural similarity. IEEE Trans. Image Process., 13(4):600–612, 2004.
doi: 10.1109/TIP.2003.819861. URL https://doi.org/10.1109/TIP.2003.819861."
REFERENCES,0.8,"Tao Xiang, Hangcheng Liu, Shangwei Guo, Tianwei Zhang, and Xiaofeng Liao. Local black-box
adversarial attacks: A query efﬁcient approach. CoRR, abs/2101.01032, 2021. URL https:
//arxiv.org/abs/2101.01032."
REFERENCES,0.8043478260869565,Under review as a conference paper at ICLR 2022
REFERENCES,0.808695652173913,"Weilin Xu, David Evans, and Yanjun Qi.
Feature squeezing: Detecting adversarial examples
in deep neural networks.
In 25th Annual Network and Distributed System Security Sympo-
sium, NDSS 2018, San Diego, California, USA, February 18-21, 2018. The Internet Soci-
ety, 2018. URL http://wp.internetsociety.org/ndss/wp-content/uploads/
sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf."
REFERENCES,0.8130434782608695,"Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
David J. Fleet, Tom´as Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV
2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part
I, volume 8689 of Lecture Notes in Computer Science, pp. 818–833. Springer, 2014. doi: 10.1007/
978-3-319-10590-1\ 53.
URL https://doi.org/10.1007/978-3-319-10590-1_
53."
REFERENCES,0.8173913043478261,"Hanwei Zhang, Yannis Avrithis, Teddy Furon, and Laurent Amsaleg. Smooth adversarial examples.
EURASIP J. Inf. Secur., 2020:15, 2020. doi: 10.1186/s13635-020-00112-z. URL https://
doi.org/10.1186/s13635-020-00112-z."
REFERENCES,0.8217391304347826,"Ting Zhao and Xiangqian Wu.
Pyramid feature attention network for saliency detection.
In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
Beach, CA, USA, June 16-20, 2019, pp. 3085–3094. Computer Vision Foundation / IEEE,
2019.
doi:
10.1109/CVPR.2019.00320.
URL http://openaccess.thecvf.com/
content_CVPR_2019/html/Zhao_Pyramid_Feature_Attention_Network_
for_Saliency_Detection_CVPR_2019_paper.html."
REFERENCES,0.8260869565217391,"Bolei Zhou, Aditya Khosla, `Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 2921–2929. IEEE Computer
Society, 2016. doi: 10.1109/CVPR.2016.319. URL https://doi.org/10.1109/CVPR.
2016.319."
REFERENCES,0.8304347826086956,"A
APPENDIX"
REFERENCES,0.8347826086956521,"A.1
MAD METRIC"
REFERENCES,0.8391304347826087,"Most Apparent distortion (MAD) metric (Larson & Chandler, 2010) is one of the state-of-the-art
full-reference image quality assessment methods. MAD attempts to merge two separate strategies
for two kinds of distorted images respectively. For high-quality images with near-threshold distor-
tion (just visible), MAD focuses on detection-based strategy to look for distortions in the presence
of the image. While for low-quality images with suprathreshold distortion (clearly visible), MAD
focuses on appearance-based strategy to look for image content in the presence of the distortions.
MAD will control the weight of two strategies according to the type of distorted images. The calcu-
lation process of MAD can be summarized as following steps and we recommend interested readers
to read the original literature."
REFERENCES,0.8434782608695652,1. Compute locations of visible distortions based on luminance images.
REFERENCES,0.8478260869565217,"2. Combine the visibility map with local error image.
3. Decompose both the distorted and original images into log-Gabor subbands."
REFERENCES,0.8521739130434782,4. Calculate different statistics of each subband.
REFERENCES,0.8565217391304348,5. Calculate the adaptive blending score.
REFERENCES,0.8608695652173913,Under review as a conference paper at ICLR 2022
REFERENCES,0.8652173913043478,"A.2
SCATTER PLOT OF MAD SCORES"
REFERENCES,0.8695652173913043,"Equipped with segmentation priors, 68.1% and 94.3% examples of Parsimonious-seg attack and
Square-seg attack have better MAD scores compared with their original version respectively. Since
Parsimonious Attack uses a greedy local search while Square attack adopts a random search, it is
obvious that limiting the search in salient regions is more helpful to Square Attack."
REFERENCES,0.8739130434782608,"0
20
40
60
80
100
120
Parsimonious attack 0 20 40 60 80 100 120"
REFERENCES,0.8782608695652174,Parsimonious-seg attack
REFERENCES,0.8826086956521739,"0
20
40
60
80
100
120
Square attack 0 20 40 60 80 100 120"
REFERENCES,0.8869565217391304,Square-seg attack
REFERENCES,0.8913043478260869,"Figure 9: MAD scores of Parsimonious Attack vs. Parsimonious-seg attack and Square Attack vs.
Square-seg Attack."
REFERENCES,0.8956521739130435,"A.3
THRESHOLD FOR MAD METRIC"
REFERENCES,0.9,"(a) MAD=50
(b) MAD=40
(c) MAD=30
(d) MAD=20
(e) MAD=10
(f) Original
Figure 10: Adversarial examples generated by boundary attack with different MAD scores. We can
ﬁnd the threshold MAD ≤30 is roughly enough to indicate an imperceptible adversarial example."
REFERENCES,0.9043478260869565,"A.4
TRUE SUCCESS RATE VERSUS QUERY NUMBER PLOT"
REFERENCES,0.908695652173913,"Here we further present a comparison of SRtrue versus query number under different MAD thresh-
olds. It shows that Square Attack and Parsimonious Attack can generate some successful AEs with"
REFERENCES,0.9130434782608695,Under review as a conference paper at ICLR 2022
REFERENCES,0.9173913043478261,"very few queries due to global perturbations and some tricks like initializing perturbations with ran-
dom vertical stripes (Andriushchenko et al., 2020), but they lack the ability to further improve the
imperceptibility. Instead, Saliency Attack conservatively select small regions to perturb, hence the
query efﬁciency is a little lower than Square Attack and Parsimonious Attack at the beginning but
soon exceeds them."
REFERENCES,0.9217391304347826,"0
0.5
1
1.5
2
2.5
3
# query
104 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 SR"
REFERENCES,0.9260869565217391,SR-query convergence plot under MAD <= 20
REFERENCES,0.9304347826086956,"Boundary
TVDBA
Square
Square-seg
Parsimonious
Parsimonious-seg
Saliency"
REFERENCES,0.9347826086956522,"0
0.5
1
1.5
2
2.5
3
# query
104 0 0.2 0.4 0.6 0.8 1 SR"
REFERENCES,0.9391304347826087,SR-query convergence plot under MAD <= 30
REFERENCES,0.9434782608695652,"Boundary
TVDBA
Square
Square-seg
Parsimonious
Parsimonious-seg
Saliency"
REFERENCES,0.9478260869565217,"0
0.5
1
1.5
2
2.5
3
# query
104 0 0.2 0.4 0.6 0.8 1 SR"
REFERENCES,0.9521739130434783,SR-query convergence plot under MAD <= 40
REFERENCES,0.9565217391304348,"Boundary
TVDBA
Square
Square-seg
Parsimonious
Parsimonious-seg
Saliency"
REFERENCES,0.9608695652173913,Figure 11: Queries vs. True Success Rate under different thresholds of MAD scores
REFERENCES,0.9652173913043478,"A.5
COMPARISON OF SALIENCY ATTACK WITH OTHER ATTACKS"
REFERENCES,0.9695652173913043,"We randomly choose 10,000 examples from the ImageNet validation set and divide them into 10
groups. Then we test different attacks under 3000 query budget and report their mean and stan-
dard deviation (SD). The best results are recorded in bold based on Wilcoxon signed-rank test with
signiﬁcance level at 0.05. We can ﬁnd that our Saliency Attack is signiﬁcantly better than all the
baselines."
REFERENCES,0.9739130434782609,"Table 5: Comparison of Saliency Attack with other attacks under 3000 query budget.
Method
SR ± SD
SRtrue ± SD
L2 ± SD
L0 ± SD
MAD ± SD"
REFERENCES,0.9782608695652174,"TVDBA
90.1%±0.011
23.4%±0.015
10.64±0.153
19.0%±0.005
37.65±3.631
Parsimonious
92.4%±0.007
14.7%±0.008
22.17±0.000
100.0%±0.000
51.80±0.115
Parsimonious-seg
88.9%±0.010
11.4%±0.014
13.54 ±0.497
35.5%±0.026
45.31±0.212
Square
98.3%±0.003
2.7%±0.004
25.34 ±0.021
99.0%±0.001
57.29±0.045
Square-seg
87.5%±0.014
20.3%±0.022
14.71 ±0.526
34.5%±0.028
38.56±0.538
Saliency (ours)
84.3%±0.010
79.8%±0.009
3.44±0.054
2.9%±0.001
12.07±0.198"
REFERENCES,0.9826086956521739,"A.6
GREEDY SEARCH IN SALIENT REGION WITH DIFFERENT BLOCK SIZES"
REFERENCES,0.9869565217391304,"We test different block sizes for greedy search in salient region, and choose the best one to be
compared in ablation study."
REFERENCES,0.991304347826087,"Table 6: Greedy search in salient region with different block sizes.
Block size
Avg. queries
SR
SRtrue
L2
L0
MAD"
REFERENCES,0.9956521739130435,"128
57
20.6%
18.4%
7.32
12.8%
11.50
64
512
37.4%
31.3%
6.37
10.2%
15.18
32
2727
56.0%
50.7%
4.37
4.7%
12.87
16
4039
35.4%
35.2%
1.79
0.8%
4.84"
