Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.004878048780487805,"Neural Radiance Fields (NeRFs) can be dramatically accelerated by spatial grid
representations [6, 9, 20, 25]. However, they do not explicitly reason about scale
and so introduce aliasing artifacts when reconstructing scenes captured at different
camera distances. Mip-NeRF and its extensions propose scale-aware renderers that
project volumetric frustums rather than point samples but such approaches rely
on positional encodings that are not readily compatible with grid methods. We
propose a simple modiﬁcation to grid-based models by training model heads at
different spatial grid resolutions. At render time, we simply use coarser grids to
render samples that cover larger volumes. Our method can be easily applied to
existing accelerated NeRF methods and signiﬁcantly improves rendering quality
(reducing error rates by 20–90% across synthetic and unbounded real-world scenes)
while incurring minimal performance overhead (as each model head is quick to
evaluate). Compared to Mip-NeRF, we reduce error rates by 20% while training
over 60× faster."
INTRODUCTION,0.00975609756097561,"1
Introduction"
INTRODUCTION,0.014634146341463415,"Recent advances in neural volumetric rendering techniques, most notably around Neural Radiance
Fields [19] (NeRFs), have lead to signiﬁcant progress towards photo-realistic novel view synthesis.
However, although NeRF provides state-of-the-art rendering quality, it is notoriously slow to train
and render due in part to its internal MLP representation. It further assumes that scene content is
equidistant from the camera and rendering quality degrades due to aliasing and excessive blurring
when that assumption is violated."
INTRODUCTION,0.01951219512195122,"Recent methods [6, 9, 20, 25] accelerate NeRF training and rendering signiﬁcantly through the use of
grid representations. Others, such as Mip-NeRF [2], address aliasing by projecting camera frustum
volumes instead of point-sampling rays. However, these anti-aliasing methods rely on the base NeRF
MLP representation (and are thus slow) and are incompatible with grid representations due to their
reliance on non-grid-based inputs."
INTRODUCTION,0.024390243902439025,"Inspired by divide-and-conquer NeRF extensions [22, 23, 27, 30] and classical approaches such as
Gaussian pyramids [1] and mipmaps [34], we propose a simple approach that can easily be applied
to any existing accelerated NeRF implementation. We train a pyramid of models at different scales,
sample along camera rays (as in the original NeRF), and simply query coarser levels of the pyramid
for samples that cover larger volumes (similar to voxel cone tracing [8]). Our method is simple
to implement and signiﬁcantly improves the rendering quality of fast rendering approaches with
minimal performance overhead."
INTRODUCTION,0.02926829268292683,"Figure 1: Comparison of methods. (a) NeRF traces a ray from the camera’s center of projection
through each pixel and samples points x along each ray. Sample locations are then encoded with a
positional encoding to produce a feature γ(x) that is fed into an MLP. (b) Mip-NeRF instead reasons
about volumes by deﬁning a 3D conical frustum per camera pixel. It splits the frustum into sampled
volumes, approximates them as multivariate Gaussians, and computes the integral of the positional
encodings of the coordinates contained within the Gaussians. Similar to NeRF, these features are then
fed into an MLP. (c) Accelerated grid methods, such as iNGP, sample points as in NeRF, but do not
use positional encoding and instead featurize each point by interpolating between vertices in a feature
grid. These features are then passed into a much smaller MLP, which greatly accelerates training
and rendering. (d) PyNeRF also uses feature grids, but reasons about volumes by training separate
models at different scales (similar to a mipmap). It calculates the area covered by each sample in
world coordinates, queries the models at the closest corresponding resolutions, and interpolates their
outputs."
INTRODUCTION,0.03414634146341464,"Contribution: Our primary contribution is a partitioning method that can be easily adapted to any
existing grid-rendering approach. We present state-of-the-art reconstruction results against a wide
range of datasets, including on novel scenes we designed that explicitly target common aliasing
patterns. We evaluate different posssible architectures and demonstrate that our design choices
provide a high level of visual ﬁdelity while maintaining the rendering speed of fast NeRF approaches."
RELATED WORK,0.03902439024390244,"2
Related Work"
RELATED WORK,0.04390243902439024,"The now-seminal Neural Radiance Fields (NeRF) paper [19] inspired a vast corpus of follow-up
work. We discuss a non-exhaustive list of such approaches along axes relevant to our work."
RELATED WORK,0.04878048780487805,"Grid-based methods. The original NeRF took 1–2 days to train, with extensions for unbounded
scenes [3, 40] taking longer. Once trained, rendering takes seconds per frame and is far below
interactive thresholds. NSVF [17] combines NeRF’s implicit representation with a voxel octree
that allows for empty-space skipping and improves inference speeds by 10×. Follow-up works [10,
11, 39] further improve rendering to interactive speeds by storing precomputed model outputs into"
RELATED WORK,0.05365853658536585,"auxiliary grid structures that bypass the need to query the original model altogether at render time.
Plenoxels [25] and DVGO [26] accelerate both training and rendering by directly optimizing a voxel
grid instead of an MLP to train in minutes or even seconds. TensoRF [6] and K-Planes [9] instead
use the product of low-rank tensors to approximate the voxel grid and reduce memory usage, while
Instant-NGP [20] (iNGP) encodes features into a multi-resolution hash table. The main goal of our
work is to combine the speed beneﬁts of grid-based methods with an approach that maintains quality
across different rendering scales."
RELATED WORK,0.05853658536585366,"Figure 2: We visualize renderings from a pyramid of spatial grid-based NeRFs trained for different
voxel resolutions. Models at ﬁner pyramid levels tend to capture ﬁner content."
RELATED WORK,0.06341463414634146,"Divide-and-conquer. Several works note the diminishing returns in using large networks to represent
scene content, and instead render the area of interest with multiple smaller models. DeRF [22] and
KiloNeRF [23] focus on inference speed while Mega-NeRF [30], Block-NeRF [27], and SUDS [31]
use scene decomposition to efﬁciently train city-scale neural representations. Our method is similar
in philosophy, although we partition across different resolutions instead of geographical area."
RELATED WORK,0.06829268292682927,"Aliasing. The original NeRF assumes that scene content is captured at roughly equidistant camera
distances and emits blurry renderings when the assumption is violated. Mip-NeRF [2] reasons about
the volume covered by each camera ray and proposes an integrated positional encoding that alleviates
aliasing. Mip-NeRF 360 [3] extends the base method to unbounded scenes. Exact-NeRF [14] derives
a more precise integration formula that better reconstructs far-away scene content. Bungee-NeRF [36]
leverages Mip-NeRF and further adopts a coarse-to-ﬁne training approach with residual blocks to
train on large-scale scenes with viewpoint variation. LIRF [37] proposes a multiscale image-based
representation that can generalize across scenes. The methods all build upon the original NeRF MLP
model and do not readily translate to accelerated grid-based methods."
RELATED WORK,0.07317073170731707,"Concurrent work. Several contemporary efforts explore the intersection of anti-aliasing and fast
rendering. Zip-NeRF [4] combines a hash table representation with a multi-sampling method that
approximates the true integral of features contained within each camera ray’s view frustum. Although
it trains faster than Mip-NeRF, it is explicitly not designed for fast rendering as the multi-sampling
adds signiﬁcant overhead. Mip-VoG [12] downsamples and blurs a voxel grid according to the volume
of each sample in world coordinates. We compare their reported numbers to ours in Section 4.2.
Tri-MipRF [13] uses a similar preﬁltering approach, but with triplanes instead of a 3D voxel grid."
RELATED WORK,0.07804878048780488,"Classical methods. Similar to PyNeRF, classic image processing methods, such as Gaussian [1] and
Laplacian [5] hierarchy, maintain a coarse-to-ﬁne pyramid of different images at different resolutions.
Compared to Mip-NeRF, which attempts to learn a single MLP model across all scales, one could
argue that our work demonstrates that the classic pyramid approach can be efﬁciently adapted to neural
volumetric models. In addition, our ray sampling method is similar to Crassin et al.’s approach [8],
which approximates cone tracing by sampling along camera rays and querying different mipmap
levels according the spatial footprint of each sample (stored as a voxel octree in their approach and as
a NeRF model in ours)."
RELATED WORK,0.08292682926829269,(a) Point Sampling
RELATED WORK,0.08780487804878048,"(c8, σ8) = f8(x, d)"
RELATED WORK,0.09268292682926829,"(c9, σ9) = f9(x, d)"
RELATED WORK,0.0975609756097561,(b) Model Evaluation
RELATED WORK,0.1024390243902439,"c = 0.4c8 + 0.6c9
σ = 0.4σ8 + 0.6σ9"
RELATED WORK,0.1073170731707317,(c) Interpolation
RELATED WORK,0.11219512195121951,"Figure 3: Overview. (a) We sample frustums along the camera ray corresponding to each pixel and
derive the scale of each sample according to its width in world coordinates. (b) We query the model
heads closest to the scale of each sample. (c) We derive a single color and density value for each
sample by interpolating between model outputs according to scale."
APPROACH,0.11707317073170732,"3
Approach"
PRELIMINARIES,0.12195121951219512,"3.1
Preliminaries"
PRELIMINARIES,0.12682926829268293,"NeRF. NeRF [19] represents a scene within a continuous volumetric radiance ﬁeld that captures
geometry and view-dependent appearance. It encodes the scene within the weights of a multi-
layer perceptron (MLP). At render time, NeRF casts a camera ray r for each image pixel. NeRF
samples multiple positions xi along each ray and queries the MLP at each position (along with
the ray viewing direction d) to obtain density and color values σi and ci. To better capture
high-frequency details, NeRF maps xi and d through an L-dimensional positional encoding (PE)
γ(x) = [sin(20⇡x), cos(20⇡x), . . . , sin(2L⇡x), cos(2L⇡x)] instead of directly using them as MLP
inputs. It then composites a single color prediction ˆC(r) per ray using numerical quadrature
PN−1"
PRELIMINARIES,0.13170731707317074,"i=0 Ti(1 −exp(−σiδi)) ci, where Ti = exp(−Pi−1"
PRELIMINARIES,0.13658536585365855,"j=0 σjδj) and δi is the distance between
samples. The training process optimizes the model by sampling batches R of image pixels and"
PRELIMINARIES,0.14146341463414633,minimizing the loss P r2R
PRELIMINARIES,0.14634146341463414,"""""""C(r) −ˆC(r) """""" 2"
PRELIMINARIES,0.15121951219512195,. We refer the reader to Mildenhall et al. [19] for details.
PRELIMINARIES,0.15609756097560976,"Anti-aliasing. The original NeRF suffers from aliasing artifacts when reconstructing scene content
observed at different distances or resolutions due to its reliance on point-sampled features. As these
features ignore the volume viewed by each ray, different cameras viewing the same position from
different distances may produce the same ambiguous feature. Mip-NeRF [2] and variants instead
reason about volumes by deﬁning a 3D conical frustum per camera pixel. It featurizes intervals
within the frustum with a integrated positional encoding (IPE) that approximates each frustum as a
multivariate Gaussian to estimate the integral E[γ(x)] over the PEs of the coordinates within it."
PRELIMINARIES,0.16097560975609757,"Grid-based acceleration. Various methods [6, 9, 20, 25, 26] eschew NeRF’s positional encoding
and instead store learned features into a grid-based structure, e.g. implemented as an explicit voxel
grid, hash table, or a collection of low-rank tensors. The features are interpolated based on the
position of each sample and then passed into a hard-coded function or much smaller MLP to produce
density and color, thereby accelerating training and rendering by orders of magnitude. However,
these approaches all use the same volume-insensitive point sampling of the original NeRF and do not
have a straightforward analogy to Mip-NeRF’s IPE as they no longer use positional encoding."
MULTISCALE SAMPLING,0.16585365853658537,"3.2
Multiscale sampling"
MULTISCALE SAMPLING,0.17073170731707318,"Assume that each sample x (where we drop the i index to reduce notational clutter) is associated with
an integration volume. Intuitively, samples close to a camera correspond to small volumes, while
samples far away from a camera correspond to large volumes (Figure 3). Our crucial insight for
enabling multiscale sampling with grid-based approaches is remarkably simple: we train separate
NeRFs at different voxel resolutions and simply use coarser NeRFs for samples covering larger
volumes. Speciﬁcally, we deﬁne a hierarchy of L resolutions that divide the world into voxels of
length 1/N0, ..., 1/NL−1, where Nl+1 = sNl and s is a constant scaling factor. We also deﬁne a
function fl(x, d) at each level that maps from sample location x and viewing direction d to color
c and density σ. fl can be implemented by any grid-based NeRF; in our experiments, we use a"
MULTISCALE SAMPLING,0.17560975609756097,Algorithm 1 PyNeRF rendering function
MULTISCALE SAMPLING,0.18048780487804877,"Input: m rays r, L pyramid levels, hierarchy mapping function M, base resolution N0, scaling"
MULTISCALE SAMPLING,0.18536585365853658,"factor s
Output: m estimated colors c"
MULTISCALE SAMPLING,0.1902439024390244,"x, d, P(x)  sample(r)
. Sample points x along each ray with direction d and area P(x)
M(P(x))  logs(P(x)/N0)
. Equation 1
l  min(L −1, max(0, dM(P(x))e))
. Equation 2
w  l −M(P(x))
. Equation 5
model_out  zeros(len(x))
. Zero-initialize model outputs for each sample x
for i in unique(l) do
. Iterate over sample levels
model_out[l = i] += w[l = i]fi(x[l = i], d[l = i])
model_out[l = i] += (1 −w)[l = i]fi−1(x[l = i], d[l = i])
end for
c  composite(model_out)
. Composite model outputs into per-ray color c
return c"
MULTISCALE SAMPLING,0.1951219512195122,"hash table followed by small density and color MLPs, similar to iNGP. We further deﬁne a mapping
function M that assigns the integration volume of sample x to the hierarchy level l. We explore
different alternatives, but ﬁnd that selecting the level whose voxels project to the 2D pixel area P(x)
used to deﬁne the integration volume works well:"
MULTISCALE SAMPLING,0.2,"M(P(x)) = logs(P(x)/N0)
(1)
l = min(L −1, max(0, dM(P(x))e))
(2)
σ, c = fl(x, d),
[GaussPyNeRF] (3)"
MULTISCALE SAMPLING,0.2048780487804878,"where d·e is the ceiling function. Such a model can be seen as a (Gaussian) pyramid of spatial
grid-based NeRFs (Fig. 2). If the ﬁnal density and color were obtained by summing across different
pyramid levels, the resulting levels would learn to specialize to residual or “band-pass” frequencies
(as in a 3D Laplacian pyramid [5]):"
MULTISCALE SAMPLING,0.2097560975609756,"σ, c = l
X i=0"
MULTISCALE SAMPLING,0.2146341463414634,"fi(x, d).
[LaplacianPyNeRF] (4)"
MULTISCALE SAMPLING,0.21951219512195122,"Our experiments show that such a representation is performant, but expensive since it requires l
model evaluations per sample. Instead, we ﬁnd a good tradeoff is to linearly interpolate between two
model evaluations at the levels just larger than and smaller than the target integration volume:"
MULTISCALE SAMPLING,0.22439024390243903,"σ, c = wfl(x, d) + (1 −w)fl−1(x, d),
where
w = l −M(P(x)).
(Default) [PyNeRF] (5)"
MULTISCALE SAMPLING,0.22926829268292684,"This adds the cost of only a single additional evaluation (increasing the overall rendering time from
0.0045 to 0.005 ms per pixel) while maintaining rendering quality (see Section 4.6). Our algorithm is
summarized in Algorithm 1."
MULTISCALE SAMPLING,0.23414634146341465,"Matching areas vs volumes. One might suspect it may be better to select the voxel level l whose
volume best matches the sample’s 3D integration volume. We experimented with this, but found it
more effective to match the projected 2D pixel area rather than volumes. Note that both approaches
would produce identical results if the 3D volume was always a cube, but volumes may be elongated
along the ray depending on the sampling pattern. Matching areas is preferable because most visible
3D scenes consist of empty space and surfaces, implying that when computing the composite color for
a ray r, most of the contribution will come from a few samples x lying near the surface of intersection.
When considering the target 3D integration volume associated with x, most of the contribution to the
ﬁnal composite color will come from integrating along the 2D surface (since the rest of the 3D volume
is either empty or hidden). This loosely suggests we should select levels of the voxel hierarchy based
on (projected) area rather than volume."
MULTISCALE SAMPLING,0.23902439024390243,"Hierarchical grid structures. Our method can be applied to any accelerated grid method irrespective
of the underyling storage. However, a drawback of this approach is an increased on-disk serialization
footprint due to training a hierarchy of spatial grid NeRFs. A possible solution is to exploit hierarchical
grid structures that already exist within the base NeRF. Note that multi-resolution grids such as those"
MULTISCALE SAMPLING,0.24390243902439024,"Table 1: Synthetic results. PyNeRF outperforms all baselines and trains over 60× faster than Mip-
NeRF. Both PyNeRF and Mip-NeRF properly reconstruct the brick wall in the Blender-A dataset, but
Mip-NeRF fails to accurately reconstruct checkerboard patterns."
MULTISCALE SAMPLING,0.24878048780487805,"Multiscale Blender [2]
Blender-A"
MULTISCALE SAMPLING,0.25365853658536586,"""PSNR
""SSIM
#LPIPS
#Avg Error
#Train Time (h)
""PSNR
""SSIM
#LPIPS
#Avg Error
# Train Time (h)"
MULTISCALE SAMPLING,0.25853658536585367,"Plenoxels [25]
24.98
0.843
0.161
0.080
0:28
18.13
0.511
0.523
0.190
0:40
K-Planes [9]
29.88
0.946
0.058
0.022
0:32
21.17
0.593
0.641
0.405
1:22
TensoRF [6]
30.04
0.948
0.056
0.021
0:27
27.01
0.785
0.197
0.054
1:20
iNGP [20]
30.21
0.958
0.040
0.022
0:20
20.85
0.767
0.244
0.089
0:56
Nerfacto [28]
29.56
0.947
0.051
0.022
0:25
27.46
0.796
0.195
0.053
1:07
Mip-VoG [12]
30.42
0.954
0.053
—
—
—
—
—
—
—
Mip-NeRF [2]
34.50
0.974
0.017
0.009
29:49
31.33
0.894
0.098
0.063
30:12"
MULTISCALE SAMPLING,0.2634146341463415,"PyNeRF
34.78
0.976
0.015
0.008
0:25
41.99
0.986
0.007
0.004
1:10"
MULTISCALE SAMPLING,0.2682926829268293,"used by iNGP [20] or K-Planes [9] already deﬁne a scale hierarchy that is a natural ﬁt for PyNeRF.
Rather than learning a separate feature grid for each model in our pyramid, we can reuse the same
multi-resolution features across levels (while still training different MLP heads)."
MULTISCALE SAMPLING,0.2731707317073171,"Multi-resolution pixel input. One added beneﬁt of the above is that one can train with multiscale
training data, which is particularly helpful for learning large, city-scale NeRFs [27, 30, 31, 36, 38]. For
such scenarios, even storing high-resolution pixel imagery may be cumbersome. In our formulation,
one can store low-resolution images and quickly train a coarse scene representation. The beneﬁts are
multiple. Firstly, divide-and-conquer approaches such as Mega-NeRF [31] partition large scenes into
smaller cells and train using different training pixel/ray subsets for each (to avoid training on irrelevant
data). However, in the absence of depth sensors or a priori 3D scene knowledge, Mega-NeRF is
limited in its ability to prune irrelevant pixels/rays (due to intervening occluders) which empirically
bloat the size of each training partition by 2× [30]. With our approach, we can learn a coarse 3D
knowledge of the scene on downsampled images and then ﬁlter higher-resolution data partitions
more efﬁciently. Once trained, lower-resolution levels can also serve as an efﬁcient initialization for
ﬁner layers. In addition, many contemporary NeRF methods use occupancy grids [20] or proposal
networks [3] to generate reﬁned samples near surfaces. We can quickly train these along with our
initial low-resolution model and then use them to train higher-resolution levels in a sample-efﬁcient
manner. We show in our experiments that such course-to-ﬁne multiscale training can speed up
convergence (Section 4.5)."
MULTISCALE SAMPLING,0.2780487804878049,"Unsupervised levels. A naive implementation of our method will degrade when zooming in and out
of areas that have not been seen at training time. Our implementation mitigates this by maintaining
an auxiliary data structure (similar to an occupancy grid [20]) that tracks the coarsest and ﬁnest levels
queried in each region during training. We then use the structure at inference time to only query
levels that were supervised during training."
EXPERIMENTS,0.28292682926829266,"4
Experiments"
EXPERIMENTS,0.28780487804878047,"We ﬁrst evaluate PyNeRF’s performance by measuring its reconstruction quality on bounded synthetic
(Section 4.2) and unbounded real-world (Section 4.3) scenes. We demonstrate PyNeRF’s generaliz-
ability by evaluating it on additional NeRF backbones (Section 4.4) and then explore the convergence
beneﬁts of using multiscale training data in city-scale reconstruction scenarios (Section 4.5). We
ablate our design decisions in Section 4.6."
EXPERIMENTAL SETUP,0.2926829268292683,"4.1
Experimental Setup"
EXPERIMENTAL SETUP,0.2975609756097561,"Training. We implement PyNeRF on top of the Nerfstudio library [28] and train on each scene
with 8,192 rays per batch by default for 20,000 iterations on the Multiscale Blender and Mip-NeRF
360 datasets, and 50,000 iterations on the Boat dataset and Blender-A. We train a hierarchy of 8
PyNeRF levels backed by a single multi-resolution hash table similar to that used by iNGP [20] in
Section 4.2 and Section 4.3 before evaluating additional backbones in Section 4.4. We use 4 features
per level with a hash table size of 220 by default, which we found to give the best quality-performance
trade-off on the A100 GPUs we use in our experiments. Each PyNeRF uses a 64-channel density
MLP with one hidden layer followed by a 128-channel color MLP with two hidden layers. We
use similar model capacities in our baselines for fairness. We sample rays using an occupancy"
EXPERIMENTAL SETUP,0.3024390243902439,"Figure 4: Synthetic results. PyNeRF and Mip-NeRF provide comparable results on the ﬁrst three
scenes that are crisper than those of the other fast renderers. Mip-NeRF does not accurately render
the tiles in the last row while PyNeRF recreates them near-perfectly."
EXPERIMENTAL SETUP,0.3073170731707317,"grid [20] on the Multiscale Blender dataset, and with a proposal network [3] on all others. We use
gradient scaling [21] to improve training stability in scenes with that capture content at close distance
(Blender-A and Boat). We parameterize unbounded scenes with Mip-NeRF 360’s contraction method."
EXPERIMENTAL SETUP,0.3121951219512195,"Metrics. We report quantitative results based on PSNR, SSIM [33], and the AlexNet implementation
of LPIPS [41], along with the training time in hours as measured on a single A100 GPU. For ease of
comparison, we also report the “average” error metric proposed by Mip-NeRF [2] composed of the
geometric mean of MSE = 10−PSNR/10, p"
EXPERIMENTAL SETUP,0.3170731707317073,"1 −SSIM, and LPIPS."
SYNTHETIC RECONSTRUCTION,0.32195121951219513,"4.2
Synthetic Reconstruction"
SYNTHETIC RECONSTRUCTION,0.32682926829268294,"Datasets. We evaluate PyNeRF on the Multiscale Blender dataset proposed by Mip-NeRF along with
our own Blender scenes (which we name “Blender-A”) intended to further probe the anti-aliasing
ability of our approach (by reconstructing a slanted checkerboard and zooming into a brick wall)."
SYNTHETIC RECONSTRUCTION,0.33170731707317075,"Baselines. We compare PyNeRF to several fast-rendering approaches, namely Instant-NGP [20]
and Nerfacto [28], which store features within a multi-resolution hash table, Plenoxels [25] which
optimizes an explicit voxel grid, and TensoRF [6] and K-Planes [9], which rely on low-rank tensor
decomposition. We also compare our Multiscale Blender results to those reported by Mip-VoG [12],
a contemporary fast anti-aliasing approach, and to Mip-NeRF [2] on both datasets."
SYNTHETIC RECONSTRUCTION,0.33658536585365856,"Results. We summarize our results in Table 1 and show qualitative examples in Figure 4. PyNeRF
outperforms all fast rendering approaches as well as Mip-VoG by a wide margin and is slightly
better than Mip-NeRF on Multiscale Blender while training over 60× faster. Both PyNeRF and
Mip-NeRF properly reconstruct the brick wall in the Blender-A dataset, but Mip-NeRF fails to
accurately reconstruct checkerboard patterns."
REAL-WORLD RECONSTRUCTION,0.34146341463414637,"4.3
Real-World Reconstruction"
REAL-WORLD RECONSTRUCTION,0.3463414634146341,"Datasets. We evaluate PyNeRF on the Boat scene of the ADOP [24] dataset, which to our knowledge
is one of the only publicly available unbounded real-world captures that captures its primary object of"
REAL-WORLD RECONSTRUCTION,0.35121951219512193,"Figure 5: Real-world results. PyNeRF reconstructs higher-ﬁdelity details (such as the spokes on the
bicycle and the lettering within the boat) than other methods."
REAL-WORLD RECONSTRUCTION,0.35609756097560974,"Table 2: Real-world results. PyNeRF outperforms all baselines in PSNR and average error, and
trains 40× faster than Mip-NeRF 360 and 100× faster than Exact-NeRF (the next best methods)."
REAL-WORLD RECONSTRUCTION,0.36097560975609755,"Boat [24]
Mip-NeRF 360 [3]"
REAL-WORLD RECONSTRUCTION,0.36585365853658536,"""PSNR
""SSIM
#LPIPS
#Avg Error
#Train Time (h)
""PSNR
""SSIM
#LPIPS
#Avg Error
# Train Time (h)"
REAL-WORLD RECONSTRUCTION,0.37073170731707317,"Plenoxels [25]
17.05
0.505
0.617
0.185
2:14
21.88
0.606
0.524
0.117
1:00
K-Planes [9]
18.00
0.501
0.590
0.168
2:41
21.53
0.577
0.500
0.120
1:08
TensoRF [6]
14.75
0.398
0.630
0.234
2:30
18.07
0.439
0.677
0.181
1:07
iNGP [20]
15.34
0.433
0.646
0.222
1:42
21.14
0.568
0.521
0.126
0:40
Nerfacto [28]
19.27
0.570
0.425
0.135
2:12
22.47
0.616
0.431
0.105
1:02
Mip-NeRF 360 w/ GLO [3]
20.03
0.595
0.416
0.124
37:28
22.76
0.664
0.342
0.095
37:35
Mip-NeRF 360 w/o GLO [3]
15.92
0.480
0.501
0.194
37:10
22.70
0.664
0.342
0.095
37:22
Exact-NeRF w/ GLO [14]
20.21
0.601
0.425
0.123
109:11
21.40
0.619
0.416
0.121
110:06
Exact-NeRF w/o GLO [14]
16.33
0.489
0.510
0.187
107:52
22.56
0.619
0.410
0.121
108:11"
REAL-WORLD RECONSTRUCTION,0.375609756097561,"PyNeRF
20.43
0.601
0.422
0.121
2:12
23.09
0.654
0.358
0.094
1:00"
REAL-WORLD RECONSTRUCTION,0.3804878048780488,"interest from different camera distances. For further comparison, we construct a multiscale version of
the outdoor scenes in the Mip-NeRF 360 [3] dataset using the same protocol as Multiscale Blender [2]."
REAL-WORLD RECONSTRUCTION,0.3853658536585366,"Baselines. We compare PyNeRF to the same fast-rendering approaches as in Section 4.2, along with
two unbounded Mip-NeRF variants: Mip-NeRF 360 [3] and Exact-NeRF [14]. We report numbers
on each variant with and without generative latent optimization [18] to account for lighting changes."
REAL-WORLD RECONSTRUCTION,0.3902439024390244,"Results. We summarize our results in Table 2 along with qualitative results in Figure 5. Once
again, PyNeRF outperforms all baselines, trains 40× faster than Mip-NeRF 360, and 100× faster than
Exact-NeRF (the next best alternatives)."
ADDITIONAL BACKBONES,0.3951219512195122,"4.4
Additional Backbones"
ADDITIONAL BACKBONES,0.4,"Methods. We demonstrate how PyNeRF can be applied to any grid-based NeRF method by evaluating
it with K-Planes [9] and TensoRF [6] in addition to our default iNGP-based implementatino. We
take advantage of the inherent multi-resolution structure of iNGP and K-Planes by reusing the same
feature grid across PyNeRF levels and train a separate feature grid per level in our TensoRF variant."
ADDITIONAL BACKBONES,0.40487804878048783,"Results. We train the PyNeRF variants along with their backbones across the datasets described in
Section 4.2 and Section 4.3, and summarize the results in Table 3. All PyNeRF variants show clear
improvements over their base methods."
CITY-SCALE CONVERGENCE,0.4097560975609756,"4.5
City-Scale Convergence"
CITY-SCALE CONVERGENCE,0.4146341463414634,"Dataset. We evaluate PyNeRF’s convergence properties on the the Argoverse 2 [35] Sensor dataset
(to our knowledge, the largest city-scale dataset publicly available). We select the largest overlapping
subset of logs and ﬁlter out moving objects through a pretrained segmentation model [7]. The
resulting training set contains 400 billion rays across 150K video frames."
CITY-SCALE CONVERGENCE,0.4195121951219512,"Methods. We use SUDS [31] as the backbone model in our experiments. We begin training our
method on 8× downsampled images (containing 64× fewer rays) for 5,000 iterations and then on"
CITY-SCALE CONVERGENCE,0.424390243902439,"Table 3: Additional backbones. We train the PyNeRF variants along with their backbones across the
datasets described in Section 4.2 and Section 4.3 All PyNeRF variants outperform their baselines by
a wide margin."
CITY-SCALE CONVERGENCE,0.4292682926829268,"Synthetic
Real-World"
CITY-SCALE CONVERGENCE,0.43414634146341463,"""PSNR
""SSIM
#LPIPS
#Avg Error
""PSNR
""SSIM
#LPIPS
#Avg Error"
CITY-SCALE CONVERGENCE,0.43902439024390244,"iNGP [20]
28.86
0.916
0.087
0.032
19.94
0.541
0.537
0.146
K-Planes [9]
27.90
0.865
0.131
0.047
20.54
0.553
0.520
0.136
TensoRF [6]
29.12
0.902
0.100
0.042
17.21
0.421
0.696
0.200"
CITY-SCALE CONVERGENCE,0.44390243902439025,"PyNeRF
36.22
0.979
0.013
0.004
22.65
0.645
0.369
0.098
PyNeRF-K-Planes
35.42
0.975
0.014
0.005
22.00
0.622
0.405
0.108
PyNeRF-TensoRF
35.67
0.976
0.015
0.005
21.35
0.568
0.482
0.122"
CITY-SCALE CONVERGENCE,0.44878048780487806,"Table 4: City-scale convergence. We track rendering quality over the ﬁrst four hours of training.
PyNeRF achieves the same rendering quality as SUDS 2× faster."
CITY-SCALE CONVERGENCE,0.45365853658536587,""" PSNR"
CITY-SCALE CONVERGENCE,0.4585365853658537,"Time (h)
1:00
2:00
3:00
4:00"
CITY-SCALE CONVERGENCE,0.4634146341463415,"SUDS [31]
16.01
17.41
18.08
18.53
PyNeRF
17.17
18.44
18.59
18.73"
CITY-SCALE CONVERGENCE,0.4682926829268293,""" SSIM"
CITY-SCALE CONVERGENCE,0.47317073170731705,"Time (h)
1:00
2:00
3:00
4:00"
CITY-SCALE CONVERGENCE,0.47804878048780486,"SUDS [31]
0.570
0.600
0.602
0.606
PyNeRF
0.614
0.618
0.619
0.621"
CITY-SCALE CONVERGENCE,0.48292682926829267,# LPIPS
CITY-SCALE CONVERGENCE,0.4878048780487805,"Time (h)
1:00
2:00
3:00
4:00"
CITY-SCALE CONVERGENCE,0.4926829268292683,"SUDS [31]
0.531
0.496
0.470
0.466
PyNeRF
0.521
0.485
0.469
0.465"
CITY-SCALE CONVERGENCE,0.4975609756097561,# Avg Error
CITY-SCALE CONVERGENCE,0.5024390243902439,"Time (h)
1:00
2:00
3:00
4:00"
CITY-SCALE CONVERGENCE,0.5073170731707317,"SUDS [31]
0.182
0.160
0.150
0.145
PyNeRF
0.165
0.146
0.144
0.142"
CITY-SCALE CONVERGENCE,0.5121951219512195,"progressively higher resolutions (downsampled to 4×, 2×, and 1×) every 5,000 iterations hereafter.
We compare to the original SUDS method as a baseline."
CITY-SCALE CONVERGENCE,0.5170731707317073,"Metrics. We report the evolution of the quality metrics used in Section 4.2 and Section 4.3 over the
ﬁrst four hours of the training process."
CITY-SCALE CONVERGENCE,0.5219512195121951,"Results. We summarize our results in Table 4. PyNeRF converges more rapidly than the SUDS
baseline, achieving the same rendering quality at 2 hours as SUDS after 4."
DIAGNOSTICS,0.526829268292683,"4.6
Diagnostics"
DIAGNOSTICS,0.5317073170731708,"Methods. We validate our design decisions by testing several variants. We ablate our MLP-level
interpolation described in Equation 5 and compare it to the GausssPyNeRF and LaplacianPyNeRF
variants described in Section 3.2 along with another that instead interpolates the learned grid feature
vectors (which avoids the need for an additional MLP evaluation per sample). As increased storage
footprint is a potential drawback method, we compare our default strategy of sharing the same
multi-resolution feature grid across PyNeRF levels to the naive implementation that trains a separate
grid per level. We also explore using 3D sample volumes instead of projected 2D pixel areas to
determine voxel levels l."
DIAGNOSTICS,0.5365853658536586,"Results. We train our method and variants as described in Section 4.2 and Section 4.3, and summarize
the results (averaged across datasets) in Table 5. Our proposed interpolation method strikes a good
balance — its performance is near-identical to the full LaplacianPyNeRF approach while training
3× faster (and is signiﬁcantly better than the other interpolation methods). Our strategy of reusing
the same feature grid across levels performs comparably to the naive implementation while training
faster due to fewer feature grid lookups. Using 2D pixel areas instead of 3D volumes to determine
voxel level l provides an improvement."
DIAGNOSTICS,0.5414634146341464,"Table 5: Diagnostics. The rendering quality of our interpolation method is near-identical to the full
residual approach while training 3× faster, and is signiﬁcantly better than other alternatives. Reusing
the same feature grid across levels performs comparably to storing separate hash tables per level
while training faster."
DIAGNOSTICS,0.5463414634146342,"Method
Our
Interp."
DIAGNOSTICS,0.551219512195122,"Shared
Features"
"D
AREA",0.5560975609756098,"2D
Area
""PSNR
""SSIM
#LPIPS
# Avg Error"
"D
AREA",0.5609756097560976,"# Train
Time (h)"
"D
AREA",0.5658536585365853,"GaussPyNeRF (Eq. 3)
7
X
X
28.72
0.803
0.201
0.056
0:43
LaplacianPyNeRF (Eq. 4)
7
X
X
29.48
0.813
0.190
0.052
2:44
Feature grid interpolation
7
7
X
28.45
0.767
0.244
0.070
0:46
Separate hash tables
X
7
X
29.41
0.813
0.196
0.054
0:52
Levels w/ 3D Volumes
X
X
7
29.19
0.811
0.184
0.054
0:48"
"D
AREA",0.5707317073170731,"PyNeRF
X
X
X
29.44
0.812
0.191
0.053
0:48"
LIMITATIONS,0.5756097560975609,"5
Limitations"
LIMITATIONS,0.5804878048780487,"Although our method generalizes to any grid-based method (Section 4.4), it requires a larger on-disk
serialization footprint due to training a hierarchy of spatial grid NeRFs. This can be mitigated
by reusing the same feature grid when the underlying backbone uses a multi-resolution feature
grid [9, 20], but this is not true of all methods [6, 25]."
SOCIETAL IMPACT,0.5853658536585366,"6
Societal Impact"
SOCIETAL IMPACT,0.5902439024390244,"Our method facilitates the rapid construction of high-quality neural representations in a resource
efﬁcient manner. As such, the risks inherent to our work is similar to those of other neural rendering
papers, namely privacy and security concerns related to the intentional or inadvertent capture or
privacy-sensitive information such as human faces and vehicle license plate numbers. While we did
not apply our approach to data with privacy or security concerns, there is a risk, similar to other neural
rendering approaches, that such data could end up in the trained model if the employed datasets
are not properly ﬁltered before use. Many recent approaches [15, 16, 29, 31, 42] distill semantics
into NeRF’s representation, which may be used to ﬁlter out sensitive information at render time.
However this information would still reside in the model itself. This could in turn be mitigated by
preprocessing the input data used to train the model [32]."
CONCLUSION,0.5951219512195122,"7
Conclusion"
CONCLUSION,0.6,"We propose a method that signiﬁcantly improves the anti-aliasing properties of fast volumetric
renderers. Our approach can be easily applied to any existing grid-based NeRF, and although simple,
provides state-of-the-art reconstruction results against a wide variety of datasets (while training
60–100× faster than existing anti-aliasing methods). We propose several synthetic scenes that model
common aliasing patterns as few existing NeRF datasets cover these scenarios in practice. Creating
and sharing additional real-world captures would likely facilitate further research."
CONCLUSION,0.6048780487804878,"Acknowledgements. HT and DR were supported in part by the Intelligence Advanced Research
Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract
number 140D0423C0074. The U.S. Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views
and conclusions contained herein are those of the authors and should not be interpreted as necessarily
representing the ofﬁcial policies or endorsements, either expressed or implied, of IARPA, DOI/IBC,
or the U.S. Government."
REFERENCES,0.6097560975609756,References
REFERENCES,0.6146341463414634,"[1] E. Adelson, C. Anderson, J. Bergen, P. Burt, and J. Ogden. Pyramid methods in image processing. RCA"
REFERENCES,0.6195121951219512,"Eng., 29, 11 1983."
REFERENCES,0.624390243902439,"[2] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan. Mip-NeRF: A"
REFERENCES,0.6292682926829268,"multiscale representation for anti-aliasing neural radiance ﬁelds. In ICCV, 2021."
REFERENCES,0.6341463414634146,"[3] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Mip-NeRF 360: Unbounded"
REFERENCES,0.6390243902439025,"anti-aliased neural radiance ﬁelds. In CVPR, 2022."
REFERENCES,0.6439024390243903,"[4] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Zip-NeRF: Anti-aliased grid-based"
REFERENCES,0.6487804878048781,"neural radiance ﬁelds. In ICCV, 2023."
REFERENCES,0.6536585365853659,"[5] P. Burt and E. Adelson.
The laplacian pyramid as a compact image code.
IEEE Transactions on
Communications, 31(4):532–540, 1983. doi: 10.1109/TCOM.1983.1095851."
REFERENCES,0.6585365853658537,"[6] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su. TensoRF: Tensorial radiance ﬁelds. In ECCV, 2022."
REFERENCES,0.6634146341463415,"[7] B. Cheng, M. D. Collins, Y. Zhu, T. Liu, T. S. Huang, H. Adam, and L.-C. Chen. Panoptic-deeplab: A"
REFERENCES,0.6682926829268293,"simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, 2020."
REFERENCES,0.6731707317073171,"[8] C. Crassin, F. Neyret, M. Sainz, S. Green, and E. Eisemann. Interactive indirect illumination using voxel"
REFERENCES,0.6780487804878049,"cone tracing: A preview. In Symposium on Interactive 3D Graphics and Games, 2011."
REFERENCES,0.6829268292682927,"[9] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa. K-planes: Explicit radiance"
REFERENCES,0.6878048780487804,"ﬁelds in space, time, and appearance. In CVPR, 2023."
REFERENCES,0.6926829268292682,"[10] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. Valentin. FastNeRF: High-ﬁdelity neural"
REFERENCES,0.697560975609756,"rendering at 200fps. In ICCV, 2021."
REFERENCES,0.7024390243902439,"[11] P. Hedman, P. P. Srinivasan, B. Mildenhall, J. T. Barron, and P. Debevec. Baking neural radiance ﬁelds for"
REFERENCES,0.7073170731707317,"real-time view synthesis. In ICCV, 2021."
REFERENCES,0.7121951219512195,"[12] D. Hu, Z. Zhang, T. Hou, T. Liu, H. Fu, and M. Gong. Multiscale representation for real-time anti-aliasing"
REFERENCES,0.7170731707317073,"neural rendering. arxiv:2304.10075, 2023."
REFERENCES,0.7219512195121951,"[13] W. Hu, Y. Wang, L. Ma, B. Yang, L. Gao, X. Liu, and Y. Ma. Tri-miprf: Tri-mip representation for efﬁcient"
REFERENCES,0.7268292682926829,"anti-aliasing neural radiance ﬁelds. In ICCV, 2023."
REFERENCES,0.7317073170731707,"[14] B. K. S. Isaac-Medina, C. G. Willcocks, and T. P. Breckon. Exact-NeRF: An exploration of a precise"
REFERENCES,0.7365853658536585,"volumetric parameterization for neural radiance ﬁelds. CVPR, 2023."
REFERENCES,0.7414634146341463,"[15] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik. LERF: Language embedded radiance"
REFERENCES,0.7463414634146341,"ﬁelds. In ICCV, 2023."
REFERENCES,0.751219512195122,"[16] S. Kobayashi, E. Matsumoto, and V. Sitzmann. Decomposing nerf for editing via feature ﬁeld distillation."
REFERENCES,0.7560975609756098,"In Advances in Neural Information Processing Systems, volume 35, 2022."
REFERENCES,0.7609756097560976,"[17] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt. Neural sparse voxel ﬁelds. NIPS, 2020."
REFERENCES,0.7658536585365854,"[18] R. Martin-Brualla, N. Radwan, M. S. M. Sajjadi, J. T. Barron, A. Dosovitskiy, and D. Duckworth. NeRF in"
REFERENCES,0.7707317073170732,"the wild: Neural radiance ﬁelds for unconstrained photo collections. In CVPR, 2021."
REFERENCES,0.775609756097561,"[19] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. NeRF: Representing"
REFERENCES,0.7804878048780488,"scenes as neural radiance ﬁelds for view synthesis. In ECCV, 2020."
REFERENCES,0.7853658536585366,"[20] T. Müller, A. Evans, C. Schied, and A. Keller. Instant neural graphics primitives with a multiresolution"
REFERENCES,0.7902439024390244,"hash encoding. ACM Trans. Graph., 41(4):102:1–15, July 2022. doi: 10.1145/3528223.3530127."
REFERENCES,0.7951219512195122,[21] J. Philip and V. Deschaintre. Floaters No More: Radiance Field Gradient Scaling for Improved Near-
REFERENCES,0.8,"Camera Training. In T. Ritschel and A. Weidlich, editors, Eurographics Symposium on Rendering. The
Eurographics Association, 2023. ISBN 978-3-03868-229-5. doi: 10.2312/sr.20231122."
REFERENCES,0.8048780487804879,"[22] D. Rebain, W. Jiang, S. Yazdani, K. Li, K. Yi, and A. Tagliasacchi. DeRF: Decomposed radiance ﬁelds. In"
REFERENCES,0.8097560975609757,"CVPR, 2021."
REFERENCES,0.8146341463414634,"[23] C. Reiser, S. Peng, Y. Liao, and A. Geiger. KiloNeRF: Speeding up neural radiance ﬁelds with thousands"
REFERENCES,0.8195121951219512,"of tiny MLPs. In ICCV, 2021."
REFERENCES,0.824390243902439,"[24] D. Rückert, L. Franke, and M. Stamminger. Adop: Approximate differentiable one-pixel point rendering."
REFERENCES,0.8292682926829268,"ACM Trans. Graph., 41(4):99:1–14, July 2022. ISSN 0730-0301."
REFERENCES,0.8341463414634146,"[25] Sara Fridovich-Keil and Alex Yu, M. Tancik, Q. Chen, B. Recht, and A. Kanazawa. Plenoxels: Radiance"
REFERENCES,0.8390243902439024,"ﬁelds without neural networks. In CVPR, 2022."
REFERENCES,0.8439024390243902,"[26] C. Sun, M. Sun, and H. Chen. Direct voxel grid optimization: Super-fast convergence for radiance ﬁelds"
REFERENCES,0.848780487804878,"reconstruction. In CVPR, 2022."
REFERENCES,0.8536585365853658,"[27] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar."
REFERENCES,0.8585365853658536,"Block-NeRF: Scalable large scene neural view synthesis. In CVPR, pages 8248–8258, June 2022."
REFERENCES,0.8634146341463415,"[28] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja,"
REFERENCES,0.8682926829268293,"D. McAllister, and A. Kanazawa. Nerfstudio: A modular framework for neural radiance ﬁeld development.
In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH ’23, 2023."
REFERENCES,0.8731707317073171,"[29] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi. Neural Feature Fusion Fields: 3D distillation of"
REFERENCES,0.8780487804878049,"self-supervised 2D image representation. In International Conference on 3D Vision (3DV), 2022."
REFERENCES,0.8829268292682927,"[30] H. Turki, D. Ramanan, and M. Satyanarayanan. Mega-NERF: Scalable construction of large-scale NeRFs"
REFERENCES,0.8878048780487805,"for virtual ﬂy-throughs. In CVPR, pages 12922–12931, June 2022."
REFERENCES,0.8926829268292683,"[31] H. Turki, J. Y. Zhang, F. Ferroni, and D. Ramanan. SUDS: Scalable urban dynamic scenes. In CVPR, 2023."
REFERENCES,0.8975609756097561,"[32] J. Wang, B. Amos, A. Das, P. Pillai, N. Sadeh, and M. Satyanarayanan. A scalable and privacy-aware IoT"
REFERENCES,0.9024390243902439,"service for live video analytics. In Multimedia Systems Conference, pages 38–49, 2017."
REFERENCES,0.9073170731707317,"[33] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli. Image quality assessment: from error visibility to"
REFERENCES,0.9121951219512195,"structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004."
REFERENCES,0.9170731707317074,"[34] L. Williams. Pyramidal parametrics. Computer Graphics, 17:1–11, July 1983."
REFERENCES,0.9219512195121952,"[35] B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh, S. Khandelwal, B. Pan, R. Kumar, A. Hartnett, J. K."
REFERENCES,0.926829268292683,"Pontes, D. Ramanan, P. Carr, and J. Hays. Argoverse 2: Next generation datasets for self-driving perception
and forecasting. In NeurIPS Datasets and Benchmarks, 2021."
REFERENCES,0.9317073170731708,"[36] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin. BungeeNeRF: Progressive"
REFERENCES,0.9365853658536586,"neural radiance ﬁeld for extreme multi-scale scene rendering. In ECCV, 2022."
REFERENCES,0.9414634146341463,"[37] H. Xin, Z. Qi, F. Ying, L. Xiaoyu, W. Xuan, and W. Qing. Local implicit ray function for generalizable"
REFERENCES,0.9463414634146341,"radiance ﬁeld representation. In CVPR, 2023."
REFERENCES,0.9512195121951219,"[38] L. Xu, Y. Xiangli, S. Peng, X. Pan, N. Zhao, C. Theobalt, B. Dai, and D. Lin. Grid-guided neural radiance"
REFERENCES,0.9560975609756097,"ﬁelds for large urban scenes. In CVPR, 2023."
REFERENCES,0.9609756097560975,"[39] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa. PlenOctrees for real-time rendering of neural"
REFERENCES,0.9658536585365853,"radiance ﬁelds. In ICCV, 2021."
REFERENCES,0.9707317073170731,"[40] K. Zhang, G. Riegler, N. Snavely, and V. Koltun. NeRF++: Analyzing and improving neural radiance"
REFERENCES,0.975609756097561,"ﬁelds. arXiv:2010.07492, 2020."
REFERENCES,0.9804878048780488,"[41] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep"
REFERENCES,0.9853658536585366,"features as a perceptual metric. In CVPR, 2018."
REFERENCES,0.9902439024390244,"[42] S. Zhi, T. Laidlow, S. Leutenegger, and A. Davison. In-place scene labelling and understanding with"
REFERENCES,0.9951219512195122,"implicit scene representation. In ICCV, 2021."
