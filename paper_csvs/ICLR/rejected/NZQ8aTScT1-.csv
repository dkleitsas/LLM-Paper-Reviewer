Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0007782101167315176,"Understanding the fundamental principles behind the massive success of neural
networks is one of the most important open questions in deep learning. However,
due to the highly complex nature of the problem, progress has been relatively slow.
In this note, through the lens of inﬁnite-width networks, a.k.a. neural kernels, we
present one such principle resulting from hierarchical locality. It is well-known
that the eigenstructure of inﬁnite-width multilayer perceptrons (MLPs) depends
solely on the concept frequency, which measures the order of interactions. We
show that the topologies from convolutional networks (CNNs) restructure the as-
sociated eigenspaces into ﬁner subspaces. In addition to frequency, the new struc-
ture also depends on the concept space — the distance among interaction terms,
deﬁned via the length of a minimum spanning tree containing them. The result-
ing ﬁne-grained eigenstructure dramatically improves the network’s learnability,
empowering them to simultaneously model a much richer class of interactions,
including long-range-low-frequency interactions, short-range-high-frequency in-
teractions, and various interpolations and extrapolations in-between. Finally, we
show that increasing the depth of a CNN can improve the inter/extrapolation res-
olution and, therefore, the network’s learnability."
INTRODUCTION,0.0015564202334630351,"1
INTRODUCTION"
INTRODUCTION,0.0023346303501945525,"Learning in high dimensions is commonly believed to suffer from the curse of dimensionality, in
which the number of samples required to solve the problem grows rapidly (often polynomially) with
the dimensionality of the input. Nevertheless, modern neural networks often exhibit an astonishing
power to tackle a wide range of highly complex and high-dimensional real-world problems, many of
which were thought to be out-of-scope of known methods (Krizhevsky et al., 2012; Vaswani et al.,
2017; Devlin et al., 2018; Silver et al., 2016; Senior et al., 2020; Kaplan et al., 2020). What are
the mathematical principles that govern the astonishing power of neural networks? This question
perhaps is the most crucial research question in the theory of deep learning because such principles
are also the keys to resolve fundamental questions in the practice of machine learning such as (out-
of-distribution) generalization (Zhang et al., 2021), calibration (Ovadia et al., 2019), interpretability
(Montavon et al., 2018), robustness (Goodfellow et al., 2014)."
INTRODUCTION,0.0031128404669260703,"Unarguably, there can be more than one of such principles. They are related to one or more of the
three basic ingredients of machine learning methods: the data, the model and the inference algo-
rithm. Among them, the models, a.k.a. architectures of neural networks are the most crucial inno-
vation in deep learning that set it apart from classical machine learning methods. More importantly,
the current revolution in machine learning is initialized by the (re-)introduction of convolution-based
architectures (Krizhevsky et al., 2012; Lecun, 1989), and subsequent breakthroughs are often driven
by the discovery or application of novel architectures (Vaswani et al. (2017); Devlin et al. (2018)).
As such, identifying and understanding fundamental roles of architectures are of great importance."
INTRODUCTION,0.0038910505836575876,"In this paper, we take a step forwards by leveraging recent developments in overparameterized
networks (Poole et al. (2016); Daniely et al. (2016); Schoenholz et al. (2017); Lee et al. (2018);
Matthews et al. (2018); Xiao et al. (2018); Jacot et al. (2018); Du et al. (2018); Novak et al. (2019a);
Lee et al. (2019) and many others.) These developments have discovered an important connection
between neural networks and kernel machines: the Neural Network Gaussian Process (NNGP) ker-
nels and the neural tangent kernels (NTKs). Under certain scaling limits, the former describes the"
INTRODUCTION,0.004669260700389105,Under review as a conference paper at ICLR 2022 Space
INTRODUCTION,0.005447470817120622,Frequency (a) MLP
INTRODUCTION,0.0062256809338521405,"Space + Frequency = Budget
Order of Learning
Long-Range-Low-Frequency
Beyond Budget Space (b)"
INTRODUCTION,0.007003891050583658,Shallow CNN
INTRODUCTION,0.007782101167315175,Short-Range-High-Frequency Space (c)
INTRODUCTION,0.008560311284046693,Deep CNN
INTRODUCTION,0.00933852140077821,Median-Range-Median-Frequency Space (d)
INTRODUCTION,0.010116731517509728,High Resolution CNN
INTRODUCTION,0.010894941634241245,"Ultra-Short-Range-Ultra-High-Frequency
Finer Interpolation
Finer Interpolation
Ultra-Longer-Range-Ultra-Low-Frequency"
INTRODUCTION,0.011673151750972763,"Figure 1: Architectural Inductive Biases.
An demonstration of learnable functions vs archi-
tectures for four families of architectures.
Each shaded box indicates the maximum learnable
eigenspaces within a given compute budget (Dashed Line.) From left to right: (a) MLPs can
model Long-Range-Low-Frequency interactions; (b) S-CNNs can model Short-Range-High-
Frequency interactions; (c) Additionally, D-CNNs can also model interactions between LRLF
and SRHF, a.k.a., Median-Range-Median-Frequency interactions. (d) Finally, HS-CNNs can ad-
ditionally model interactions of Ultra-Short-Range-Ultra-High-Frequency, Ultra-Long-Range-
Ultra-Low-Frequency, and ﬁner interpolations in-between. The Green Arrow indicates the di-
rection of expansion of learnable functions when increasing the compute budget."
INTRODUCTION,0.012451361867704281,"distribution of the outputs of a randomly initialized network (a.k.a. prior), and the latter can describe
the network’s gradient descent dynamics. Although recent work (Ghorbani et al., 2019; Yang & Hu,
2020) has identiﬁed several limitations of using them in studying the feature learning dynamics of
practical networks, we show that they do capture several crucial aspects of the architectural inductive
biases."
INTRODUCTION,0.013229571984435798,"Our main contribution is an eigenspace restructuring theorem. It characterizes a mathematical con-
nection between a network’s architecture and its learnability through a trade-off between space and
frequency, providing novel insights behind the mystery power of deep CNNs (more generally, hi-
erarchical locality (Deza et al., 2020; Vasilescu et al., 2021).) By frequency, we mean the degree
(order) of the eigenfunction and by space, we mean the spatial distance among the eigenfunction’s
interaction terms. We summarize our main contribution below; see Fig. 1."
INTRODUCTION,0.014007782101167316,"1. The learning order (see Green Arrow in Fig. 1) of eigen-functions is governed by the
learning index (LI), the sum of the frequency index (FI) and the spatial index (SI), which
can be characterized precisely by the network’s topology.
2. There is a trade-off between space and frequency: within a ﬁxed (compute/data) budget, it is
impossible to model generic Long-Range-High-Frequency interactions. MLPs can model
Long-Range-Low-Frequency (LRHF) interactions but fail to model Short-Range-High-
Frequency (SRHF), while shallow CNNs (S-CNNs) are the opposite. Remarkably, deep
CNNs (D-CNNs) can simultaneously model both and various interpolating interactions
between them (e.g., Median-Range-Median-Frequency (MRMF).)
3. In addition, high-resolution CNNs (HS-CNNs, EfﬁcientNet-type of model scaling) further
broaden the class of learnable functions to contain (1) extrapolation: Ultra-Long-Range-
Ultra-Low-Frequency and the dual interactions and (2) ﬁner interpolations interactions.
4. Finally, we verify the above claims empirically for neural kernel methods and ﬁnite-width
networks using SGD + Momentum for dataset and networks of practical sizes."
LINEAR AND LINEARIZED MODELS,0.014785992217898832,"2
LINEAR AND LINEARIZED MODELS"
LINEAR AND LINEARIZED MODELS,0.01556420233463035,"As a warm up exercise, we brieﬂy go through the training dynamics of linear models. Let (X, Y)
denote the inputs and labels, where X ⊆Rd and Y ⊆R. Assume J : Rd →Rn is a feature
map and the task is to learn a linear function f(x, θ) = J(x)θ to minimize the MSE objective
1
2
P"
LINEAR AND LINEARIZED MODELS,0.01634241245136187,"(x,y)∈(X,Y) |f(x, θ) −y|2. Let ℛ(X, θ) = f(X, θ) −Y be the residual of the predictions of X.
Then the gradient ﬂow dynamics can be written as
d
dtℛ(X, θ) = −J(X)JT (X)ℛ(X, θ) ≡−K(X, X)ℛ(X, θ)
(1)"
LINEAR AND LINEARIZED MODELS,0.017120622568093387,Under review as a conference paper at ICLR 2022
LINEAR AND LINEARIZED MODELS,0.0178988326848249,"Since the feature kernel K(X, X) = J(X)JT (X) is constant in time, the above ODE can be solved
in closed form. Let m = |X| the cardinality of X and ˆK(j)/uj be the j-th eigenvalue/eigenvector
of K(X, X) in descending order. By initializing θ = 0 at time t = 0 and denoting the projection by
ηj = uT
j ℛ(X, 0), the dynamics of the residual and the loss can be reduced to"
LINEAR AND LINEARIZED MODELS,0.01867704280155642,"ℛ(X, θt) =
X"
LINEAR AND LINEARIZED MODELS,0.019455252918287938,"j∈[m]
e−ˆK(j)tηjuj,
L(θt) = 1 2 X"
LINEAR AND LINEARIZED MODELS,0.020233463035019456,"j∈[m]
e−2 ˆK(j)tη2
j
(2)"
LINEAR AND LINEARIZED MODELS,0.021011673151750974,"Therefore, to make the residual in uj smaller than some ϵ > 0, the amount of time needed is
t ≥ˆK(j)−1 log 2ϵ"
LINEAR AND LINEARIZED MODELS,0.02178988326848249,"η2
j /2. The larger ˆK(j) is, the shorter amount of time it takes to learn uj."
LINEAR AND LINEARIZED MODELS,0.022568093385214007,"Although simple, linear models provide us with the most useful intuition behind the relation between
“eigenstructures"" and learning dynamics."
LINEAR AND LINEARIZED MODELS,0.023346303501945526,"2.1
LINEARIZED NEURAL NETWORKS: NNGP KERNELS AND NT KERNELS"
LINEAR AND LINEARIZED MODELS,0.024124513618677044,"Let f(θ, x) be a general function, e.g. f is neural network parameterized by θ. Similarly,
d
dtℛ(X, θ) = −J(X; θ)JT (X; θ)ℛ(X, θ) ≡−K(X, X; θ)ℛ(X, θ) .
(3)"
LINEAR AND LINEARIZED MODELS,0.024902723735408562,"However, the kernel K(X, X; θ) depends on θ via the Jacobian J(X; θ) of f(X; θ) and evolves with
time. The above system is unsolvable in general. However, under certain parameterization methods
(e.g. Sohl-Dickstein et al. (2020)) and when the network is sufﬁcient wide, this kernel does not
change much during training and converges to a deterministic kernel called the NTK (Jacot et al.,
2018),
K(X, X; θ) →Θ(X, X)
as width →∞.
(4)"
LINEAR AND LINEARIZED MODELS,0.025680933852140077,"The residual dynamics becomes a constant coefﬁcient ODE again ˙ℛ(X, θ) = −Θ(X, X)ℛ(X, θ).
To solve this system, we need the initial value of ℛ(X, θ). Since the parameters θ are often initial-
ized using iid standard Gaussian variables, as the width approach inﬁnity, the logits f(X; θ) converge
to a Gaussian process (GP), known as the neural network Gaussian process (NNGP). Speciﬁcally,
f(X; θ) ∼N(0; 풦(X, X)), where 풦is the NNGP kernel. Note that one can also treat inﬁnite-width
networks as Bayesian models, a.k.a. Bayesian Neural Networks, and apply Bayesian inference to
compute the posteriors. This approach is equivalent to training only the network’s classiﬁcation
layer (Lee et al., 2019) and the gradient descent dynamics is described by the kernel 풦."
LINEAR AND LINEARIZED MODELS,0.026459143968871595,"As such, there are two natural kernels, the NTK Θ and the NNGP kernel 풦, associated to inﬁnite-
width networks, whose training dynamics are governed by constant coefﬁcient ODEs. To make
progress, it is tempting to apply Mercer’s Theorem to eigendecompose Θ and 풦, e.g.,"
LINEAR AND LINEARIZED MODELS,0.027237354085603113,"풦(x, ¯x) =
X ˆ
풦(j)φj(x)φj(¯x)
and
Θ(x, ¯x) =
X ˆΘ(j)ψj(x)ψj(¯x)
(5)"
LINEAR AND LINEARIZED MODELS,0.02801556420233463,"One advantage of applying this decomposition is that it has almost no constraint on the kernels
and the inputs. However, this decomposition is too coarse to be useful since it can hardly provide
ﬁne-grained information about the eigenstructures. E.g, it is not clear what are the corrections to
Eq. (5) when changing the architecture from a 2-layer CNN to a 4-layer CNNs. For this reason,
we choose to work on the product space of hyperspheres, which has richer mathematical structures.
Our primary goal is to characterize the analytical dependence of the decomposition Eq. (5) on the
network’s topology in the high-dimensional limit."
NEURAL COMPUTATIONS ON DAGS,0.028793774319066146,"3
NEURAL COMPUTATIONS ON DAGS"
NEURAL COMPUTATIONS ON DAGS,0.029571984435797664,"Notations will become heavier starting from this section. In particular, we rely crucially on the
directed acyclic graphs (DAGs) and minimal spanning trees (MSTs) to deﬁne the spatial complexity
of eigenfunctions. In Sec. B, we provide a toy example to help understand the motivation."
NEURAL COMPUTATIONS ON DAGS,0.030350194552529183,"For a positive integer p, let Sp−1 denote the unit sphere in Rp and Sp−1 = √pSp−1, the sphere of
radius √p in Rp. We introduce the normalized sum (integral)"
NEURAL COMPUTATIONS ON DAGS,0.0311284046692607,"x∈X
f(x) ≡|X|−1 X"
NEURAL COMPUTATIONS ON DAGS,0.031906614785992216,"x∈X
f(x)
"
NEURAL COMPUTATIONS ON DAGS,0.03268482490272374,"x∈X
f(x) ≡µ(X)−1
ˆ"
NEURAL COMPUTATIONS ON DAGS,0.03346303501945525,"x∈X
f(x)µ(dx)

(6)"
NEURAL COMPUTATIONS ON DAGS,0.034241245136186774,Under review as a conference paper at ICLR 2022
NEURAL COMPUTATIONS ON DAGS,0.03501945525291829,where X is a ﬁnite set (a measurable set with a ﬁnite positive measure µ).
NEURAL COMPUTATIONS ON DAGS,0.0357976653696498,"We ﬁnd it more convenient to express the computations in neural networks, and in neural kernels
via DAGs (Daniely et al., 2016), as both computations are of recursive nature. The associated DAG
of a network can be thought of as the same network by setting all its widths (or the number of
channels for CNNs) to 1. Let G = (N, E) denote a DAG, where N and E are the nodes and edges,
resp. We always assume the graph to have a unique output node oG and is an ancestor of all other
nodes. Denote N0 ⊆N the set of input nodes (leaves) of G, i.e., the collection nodes without a
child. Each node u ∈N is associated with a pointwise function φu : R →R, which is normalized
in the sense Ez∈N(0,1)φ2
u(z) = 1 . It induces a function φ∗
u : I ≡[−1, 1] →I deﬁned to be
φ∗
u(t) = E(z1,z2)∈Ntφu(z1)φu(z2) . Here Nt denotes a pair of standard Gaussians with correlation
t. We associate each u ∈N a ﬁnite-dimensional Hilbert space Hu, and each uv ∈E a bounded
linear operator Luv : Hv →Hu. Let X ≡
Y"
NEURAL COMPUTATIONS ON DAGS,0.036575875486381325,"u∈N0
X u ≡
Y u∈N0"
NEURAL COMPUTATIONS ON DAGS,0.03735408560311284,"Sdim(Hu)−1 ⊆
Y"
NEURAL COMPUTATIONS ON DAGS,0.03813229571984436,"u∈N0
Hu
and
I = I|N0|"
NEURAL COMPUTATIONS ON DAGS,0.038910505836575876,"be the input tensors and the input correlations to the graph G, resp. We associate two types of
computations to a DAG: ﬁnite-width neural network computation and kernel computation,"
NEURAL COMPUTATIONS ON DAGS,0.03968871595330739,"풩G : X →HoG
and
풦G : I →I ,
(7)"
NEURAL COMPUTATIONS ON DAGS,0.04046692607003891,resp. They are deﬁned recursively as follows
NEURAL COMPUTATIONS ON DAGS,0.04124513618677043,풩u(x) = φu X
NEURAL COMPUTATIONS ON DAGS,0.04202334630350195,"v:uv∈E
Luv(풩v(x)) !"
NEURAL COMPUTATIONS ON DAGS,0.042801556420233464,"if
u /∈N0
else
풩u(x) = xu
(8)"
NEURAL COMPUTATIONS ON DAGS,0.04357976653696498,"풦u(t) = φ∗
u "
NEURAL COMPUTATIONS ON DAGS,0.0443579766536965,"v:uv∈E
풦v(t)

if
u /∈N0
else
풦u(t) = tu
(9)"
NEURAL COMPUTATIONS ON DAGS,0.045136186770428015,"where x ∈X and t ∈I. The outputs of the computations are 풩G(x) = 풩oG(x) and 풦G(t) =
풦oG(t). Note that 풦G is indeed the NNGP kernel. The NTK can also be written recursively as"
NEURAL COMPUTATIONS ON DAGS,0.045914396887159536,"Θu(t) = ˙φ∗
u "
NEURAL COMPUTATIONS ON DAGS,0.04669260700389105,"v:uv∈E
풦v(t)
"
NEURAL COMPUTATIONS ON DAGS,0.047470817120622566,"v:uv∈E
(풦v(t) + Θv(t))
with
ΘG = ΘoG.
(10)"
NEURAL COMPUTATIONS ON DAGS,0.04824902723735409,"Here, Θu = 0 if u ∈N0 and ˙φ∗
u is the derivative of φ∗
u."
NEURAL COMPUTATIONS ON DAGS,0.0490272373540856,"3.1
THREE EXAMPLES: MLPS, S-CNNS AND D-CNNS."
NEURAL COMPUTATIONS ON DAGS,0.049805447470817124,"To unpack the notation, we consider three concrete examples: an L-hidden layer MLP, a shallow
convolutional network (S-CNN) that contains only one convolutional layer and a deep convolutional
network (D-CNN) that contains (1 + L) convolutional layers. The architectures are"
NEURAL COMPUTATIONS ON DAGS,0.05058365758754864,"MLP:
[Input] →[Dense-Act]⊗L →[Dense]
(11)
S-CNN:
[Input] →[Conv(p)-Act] →[Flatten-Dense]
(12)"
NEURAL COMPUTATIONS ON DAGS,0.051361867704280154,"D-CNN:
[Input] →[Conv(p)-Act] →[Conv(k)-Act]⊗L →[Flatten-Dense-Act] →[Dense] (13)"
NEURAL COMPUTATIONS ON DAGS,0.052140077821011675,"where p/k is the ﬁlter size of the ﬁrst/hidden layers and Act means an activation layer. We choose
the stride to be the same as the size of the ﬁlter for all convolutional layers and choose ﬂattening as
the readout strategy rather than pooling. See Fig. 2 (a, b, c) for the DAGs associated to a (1+3)-layer
CNN (with p = k = d
1
4 ), a (1+1)-layer CNN (with p = k = d
1
2 ) and a 4-layer MLP."
NEURAL COMPUTATIONS ON DAGS,0.05291828793774319,"MLPs.
Let G be a linked list with (L + 2) nodes, including the input/output nodes. Let Luv ∈
Rnu×nv, where nu/v = dim(Hu/v) and the activations of the input/output nodes be the identity
function. Then 풩G represents a L-hidden-layer MLP. In addition, let Luv be initialized iid as"
NEURAL COMPUTATIONS ON DAGS,0.053696498054474705,"Luv =
1
√nv
(ωuv,ij)i∈[nu],j∈[nv] ,
ωuv,ij ∼N(0, 1) .
(14)"
NEURAL COMPUTATIONS ON DAGS,0.054474708171206226,"Let tx,x′ = xT x′/nu for u ∈N0 and nv →∞for all hidden nodes, then the outputs of 풩(X)
converge weakly to the GP GP(0, 풦G(tx,x′)x,x′∈X ) and ΘG(tx,x′)x,x′∈X is the NTK in the sense"
NEURAL COMPUTATIONS ON DAGS,0.05525291828793774,"E풩G(x)풩G(x′)
in prob.
−−−−→풦G(tx,x′)
and
⟨∇풩G(x), ∇풩G(x′)⟩
in prob.
−−−−→ΘG(tx,x′).
(15)"
NEURAL COMPUTATIONS ON DAGS,0.05603112840466926,Under review as a conference paper at ICLR 2022
NEURAL COMPUTATIONS ON DAGS,0.05680933852140078,Input Node
NEURAL COMPUTATIONS ON DAGS,0.05758754863813229,Output Node
NEURAL COMPUTATIONS ON DAGS,0.058365758754863814,L(Y2) = 0 + 2 = 2
NEURAL COMPUTATIONS ON DAGS,0.05914396887159533,(a) MLP
NEURAL COMPUTATIONS ON DAGS,0.05992217898832685,"L(Y2) = 1
2 + 2
2 = 6
4, deg(Y2)=2"
NEURAL COMPUTATIONS ON DAGS,0.060700389105058365,"Frequency: 
k = 0
Spatial: 
k = 1
2"
NEURAL COMPUTATIONS ON DAGS,0.06147859922178988,(b) CNN(p2)⊗2
NEURAL COMPUTATIONS ON DAGS,0.0622568093385214,"Ultra-Short-Range-Low-Frequency: L(Y2) = 3
4 + 2
4 = 5
4"
NEURAL COMPUTATIONS ON DAGS,0.06303501945525292,"Spatial: 
k = 0
Spatial: 
k = 1
4"
NEURAL COMPUTATIONS ON DAGS,0.06381322957198443,(c) CNN(p)⊗4
NEURAL COMPUTATIONS ON DAGS,0.06459143968871596,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
NEURAL COMPUTATIONS ON DAGS,0.06536964980544747,MSE Residual vs Training Set Size: Y2
NEURAL COMPUTATIONS ON DAGS,0.06614785992217899,"CNN(p)
4"
NEURAL COMPUTATIONS ON DAGS,0.0669260700389105,"CNN(p2)
2 MLP
4 MLP
1"
NEURAL COMPUTATIONS ON DAGS,0.06770428015564202,(d) MSE Residual
NEURAL COMPUTATIONS ON DAGS,0.06848249027237355,"Figure 2: Architectures/DAGs. vs Eigenfunctions vs Learning Indices. Left to right: DAGs as-
sociated to (a) a four-layer MLP; (b) CNN(p2)⊗2, a“D""-CNN that has two convolutional layer (c)
CNN(p)⊗4, a “HR""-CNN that has four convolutional layers; and (d) MSE (Y-axis) vs training set
size (X-axis) for Y2 obtained by NTK-regression for 4 architectures. Here Y2 is a linear combina-
tion of eigenfunctions of Short-Range-Low-Frequency interactions (deg(Y2) = 2); see Sec. D for
the expression. The DAGs are generated with p = 4. In each DAG, the Dashed Lines represent the
edges with zero weights. The Solid Lines have weights 0, 1"
NEURAL COMPUTATIONS ON DAGS,0.06926070038910506,2 and 1
NEURAL COMPUTATIONS ON DAGS,0.07003891050583658,"4 in (a), (b) and (c), resp. The col-
ored path represents the minimum spanning tree used to compute the spatial indices of Y2. Under
architectures (a), (b) and (c), the spatial indices are 0, 1"
NEURAL COMPUTATIONS ON DAGS,0.07081712062256809,2 and 3
NEURAL COMPUTATIONS ON DAGS,0.0715953307392996,"4, resp. Each input node represents an
input patch of dimension p4 = d, p2 = d
1
2 and p = d
1
4 and the frequency indices are 2, 2 × 1"
AND,0.07237354085603113,"2 and
2 × 1"
AND,0.07315175097276265,"4 in (a), (b) and (c), resp."
AND,0.07392996108949416,"Indeed, note that deg(u) = 1 for all u /∈N0. Eq. (9) and Eq. (10) become"
AND,0.07470817120622568,"풦u(tx,x′) = φ∗
u(풦v(tx,x′))
and
Θu(tx,x′) = ˙φ∗
u(풦v(tx,x′))(풦v(tx,x′) + Θv(tx,x′)) (16)"
AND,0.0754863813229572,which are the recursive formulas for the NNGP kernel and NTK; see e.g. Sec.E in Lee et al. (2019).
AND,0.07626459143968872,"S-CNN.
The input X = (Sp−1)1×w ⊆Rd, where p is the patch size, w is the number of patches,
d = pw is the dimension of the inputs. Here, the inputs have been pre-processed by a patch extractor
and then by a normalization operator. In words, the S-CNN has one convolutional layer with ﬁlter
size p, followed by an activation function φ (e.g., Relu), and ﬁnally by a ﬂatten-dense readout layer.
Mathematically, by letting n ∈N be the number of channels in the hidden layer, the output (i.e.,
logit) is given by"
AND,0.07704280155642024,"Convolution + Activation:
zij(x) = φ  p−1"
X,0.07782101167315175,2 X
X,0.07859922178988327,"β∈[p]
ω1,j,βxβ,i "
X,0.07937743190661478,"
for
i ∈[w], j ∈[n] (17)"
X,0.08015564202334631,"Flatten + Dense:
f(x) = (wn)−1"
X,0.08093385214007782,"2
X"
X,0.08171206225680934,"i∈[w],j∈[n]
ω2,ijzij(x) ,
(18)"
X,0.08249027237354085,"where ω1,i,β and ω2,ij are the parameters of the ﬁrst and readout layers, resp."
X,0.08326848249027237,"We can associate a DAG G = (N, E) to the above S-CNN. Let the input, hidden and output nodes
be N0 = [1] × [w], N1 = [w] and N2 = {oG} = {∅}, resp. and N = N0 ∪N1 ∪N2. Moreover,
uv ∈E if u = oG and v ∈N1 or u = (i, ) ∈N1 and v = (0, i) ∈N0. Let Hv = Rp for v ∈N0,
Hu = Rn if u ∈N1 and HoG = R. The associated linear operators are given by"
X,0.0840466926070039,Luv = p−1
X,0.08482490272373541,"2 (ω1,j,β)j∈[n],β∈[p] ∈Rn×p
for
(u, v) ∈N1 × N0
(19)"
X,0.08560311284046693,LoGv = (wn)−1
X,0.08638132295719844,"2 (ω2,ij)j∈[n] ∈Rn
if
v = (i, ) ∈N1
(20)"
X,0.08715953307392996,"Note that the weights are shared in the ﬁrst layer but not in the readout layer (i.e., the network has no
pooling layer). To compute the NNGP kernel and NTK, we initalize all parameters ω1,i,β and ω2,ij
with iid Gaussian N(0, 1). Letting n →∞and denoting tv = xT
v x′
v/p and t = (tv)v∈N0, we have"
X,0.08793774319066149,풦G(t) =
X,0.088715953307393,"v∈N0
φ∗(tv)
and
ΘG(t) ="
X,0.08949416342412451,"v∈N0
φ∗(tv) + ˙φ∗(tv)
(21)"
X,0.09027237354085603,"D-CNN.
The input space is X = (Sp−1)kL×w ⊆Rp×1×kL×w, where p is the patch size of the
input convolutional layer, k is the ﬁlter size in hidden layers, L is the number of hidden convolution"
X,0.09105058365758754,Under review as a conference paper at ICLR 2022
X,0.09182879377431907,"layers and w is the spatial dimension of the penultimate layer. The total dimension of the input is
d = p · kL · w, and the number of input nodes is |N0| = kL · w. Since the stride is equal to the ﬁlter
size for all convolutional layers, the spatial dimension is reduced by a factor of p in the ﬁrst layer, a
factor of k by each hidden layer, and is reduced to 1 by the Flatten-Dense layer. Similar to S-CNNs,
one can associate a DAG to a D-CNN. Brieﬂy, the input layer has kL × w nodes and is reduced by
a factor of k by each convolutional layer. The penultimate and output layers have w and 1 nodes,
resp."
MAIN RESULTS,0.09260700389105059,"4
MAIN RESULTS"
MAIN RESULTS,0.0933852140077821,"The goal is to obtain a precise charaterization of the relation between the eigenstructures of 풦/ Θ
and the DAG associated to the network’s architectures in the large input dimension setting. As such
we consider a sequence of graphs G =
 
G(d)"
MAIN RESULTS,0.09416342412451362,"d∈N, where G(d) = (N (d), E(d)). We associate a ﬁnite
set of non-negative numbers ΛG to G, which is called the shape parameters of G,"
MAIN RESULTS,0.09494163424124513,"0 ∈ΛG ⊆[0, 1]
and
|ΛG| < ∞.
(22)"
MAIN RESULTS,0.09571984435797666,"We need several technical assumptions on G regarding the asymptotic shapes of G(d), which are
summarized as Assumption-G in Sec.G of the appendix. We list two of them which are the most
crucial ones. (1) For each non-input node u ∈N (d), there is αu ∈ΛG with deg(u) ∼dαu.The
weight associated to the edge uv ∈E(d) is deﬁned to be πuv ≡αu. (2) For each input node v, there
is 0 < αv ∈ΛG so that the input dimension dv ∼dαv. Here a ∼b means a/b ∈[1/C, C] for
some C > 0 independent of d. The main purpose of making these two assumptions is to remove
non-leading terms when computing the spectra."
MAIN RESULTS,0.09649805447470818,"We say φ∗is semi-admissible if, for all r ≥1, the r-th derivative of φ∗at zero is non-vanishing, i.e.,
φ∗(r)(0) > 0. If, in addition, φ∗(0) = 0 (i.e., the activation is centered), then we say φ is admissible.
An activation φ is (semi-)admissible if φ∗is (semi-)admissible. Note that if φ∗is (semi-)admissible,
then ˙φ∗is semi-admissible."
MAIN RESULTS,0.09727626459143969,"Assumption-φ. We make the following assumptions on the activations. (a.) If u ∈N (d)
0
, φu is the
identity function. (b.) If u /∈N (d)
0
∪{oG}, φu is admissible. (c.) If u = oG, φu semi-admissible."
MAIN RESULTS,0.0980544747081712,"Next, we introduce the key concept which deﬁnes the spatial distance among nodes. It is the length
of the minimum spanning tree (MST) of the nodes."
MAIN RESULTS,0.09883268482490272,Deﬁnition 1 (Spatial Index of Nodes). Let 퓃⊆N (d). The spatial index of 퓃is deﬁned to be
MAIN RESULTS,0.09961089494163425,"풮(퓃) =
min
퓃⊆T ≤G(d)
X"
MAIN RESULTS,0.10038910505836576,"uv∈E(T )
πuv =
min
퓃⊆T ≤G(d)
X"
MAIN RESULTS,0.10116731517509728,"uv∈E(T )
deg(u; T )αu
(23)"
MAIN RESULTS,0.10194552529182879,"where 퓃⊆T ≤G(d) means T is a sub-graph containing 퓃and deg(u; T ) is the degree of u in T .
By default, 풮(퓃) = 0 if 퓃contains only one or zero node."
MAIN RESULTS,0.10272373540856031,"Let tr : I|N (d)
0
| →I be a monomial, where r : N (d)
0
→N|N (d)
0
|. We use 퓃(r) = {v ∈N (d)
0
: rv ̸=
0} to denote the support of r and 퓃(r; oG) = 퓃(r) ∪{oG}."
MAIN RESULTS,0.10350194552529182,"Deﬁnition 2 (Spatial, Frequency and Learning Indices of tr). We say r ∈N|N (d)
0
| is G(d)-learnable,
or learnable for short, if there is a common ancestor node u of 퓃(r) such that φu is semi-admissible.
We use 풜(G(d)) ≡{r ∈NN (d)
0
: r is learnable}. For r ∈풜(G(d)), the spatial index, frequency
index and the learning index are deﬁned to be,"
MAIN RESULTS,0.10428015564202335,"풮(r) := 풮(퓃(r; oG)),
ℱ(r) :=
X"
MAIN RESULTS,0.10505836575875487,"v∈N (d)
0"
MAIN RESULTS,0.10583657587548638,"rvαv
and
ℒ(r) := 풮(r) + ℱ(r) ,
(24)"
MAIN RESULTS,0.1066147859922179,"resp. If r /∈풜(G(d)), we set 풮(r) = ℱ(r) = ℒ(r) = +∞. Let ℒ(G(d)) denote the sequence of
learning indices in non-descending order, i.e."
MAIN RESULTS,0.10739299610894941,"ℒ(G(d)) ≡

ℒ(r) : r ∈풜(G(d))

≡(· · · ≤rj ≤rj+1 ≤. . . )
(25)"
MAIN RESULTS,0.10817120622568094,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.10894941634241245,"Finally, for each u ∈N (d)
0
, let {Y r,l}l∈[N(du,r)],r∈N be the family of normalized spherical harmon-
ics in Sdu−1, where N(du, r) is the number of degree r spherical harmonics in Sdu−1. Deﬁne"
MAIN RESULTS,0.10972762645914397,"Y r,l(ξ) =
Y"
MAIN RESULTS,0.11050583657587548,"u∈N (d)
0"
MAIN RESULTS,0.111284046692607,"Y ru,lu(ξu),
l = (lu)u∈N (d)
0
∈[N(d, r)] ≡
Y"
MAIN RESULTS,0.11206225680933853,"u∈N (d)
0"
MAIN RESULTS,0.11284046692607004,"[N(du, ru)]
(26)"
MAIN RESULTS,0.11361867704280156,"for ξ = (ξu) ∈X. The following is our main theorem. It describes a connection between the
architecture of a network and the eigenstructure of its inducing kernels.
Theorem 1 (Eigenspace Restructuring). Assume Assumption-G and Assumption-φ. We have
the following eigen-decomposition for K = 풦G(d) or ΘG(d). For ξ, η ∈X"
MAIN RESULTS,0.11439688715953307,"K(ξ, η) =
X"
MAIN RESULTS,0.11517509727626458,"r∈N|N (d)
0
|
λK(r)
X"
MAIN RESULTS,0.11595330739299611,"l∈N(d,r)"
MAIN RESULTS,0.11673151750972763,"Y r,l(ξ)Y r,l(η),
where
λK(r) ∼d−ℒ(r) if r ̸= 0 . (27)"
MAIN RESULTS,0.11750972762645914,"Coupling with the observation in Eq. (2), Theorem 1 implies that within t ∼dr amount of time
for gradient ﬂow, when d is sufﬁciently large, only the eigenfunctions Y r,l with L(r) ≤r can be
learned. By leveraging an analytical result from Mei et al. (2021a) (Sec. 3 Theorem 4), which says
under certain regularity conditions on the eigenstructure, the corresponding kernel regression acts as
a projection, Theorem 1 establishes a connection between architectures and generalization bounds
of NNGP kernel/NTK."
MAIN RESULTS,0.11828793774319066,"Let σ be the uniform (product) probability measure on X and denote Lp(X) ≡Lp(X, σ). For
X ⊆X and r /∈L(G(d)), deﬁne the regressor and the projection operator to be"
MAIN RESULTS,0.11906614785992217,"RX(f)(x) = K(x, X)K(X, X)−1f(X) and P>r(f) =
X"
MAIN RESULTS,0.1198443579766537,r:L(r)>r X
MAIN RESULTS,0.12062256809338522,"l∈N(d,r)
⟨f, Y r,l⟩L2(X)Y r,l ."
MAIN RESULTS,0.12140077821011673,"Theorem 2. Let G = {G(d)}d, where each G(d) is a DAG associated to the D-CNN in Eq. (13). Let
r /∈L(G(d)) be ﬁxed. Let f ∈L2(X) with Eσf = 0. Then for ϵ > 0,
∥RX(f) −f∥2
L2(X) −∥P>r(f)∥2
L2(X)
 = cd,ϵ∥f∥2
L2+ϵ(X),
(28)"
MAIN RESULTS,0.12217898832684825,"where cd,ϵ →0 in probability as d →∞over X ∼σ[dr]."
MAIN RESULTS,0.12295719844357976,"In words, with [dr] many training samples where r /∈ℒ(G(d)), the NNGP kernel and the NTK are
able to learn all Y r,l with L(r) < r but not any eigenfunctions with L(r) > r. In Sec. C, we show
that the number of training samples can be reduced by a factor of w if the readout layer Fattening is
replaced by the global average pooling."
INTERPRETATION OF THE MAIN RESULTS,0.12373540856031129,"5
INTERPRETATION OF THE MAIN RESULTS"
INTERPRETATION OF THE MAIN RESULTS,0.1245136186770428,"We say r is the budget index if (1) (Finite Training Set) the training set size m ∼dr, or (2) (Finite
Compute Time) the training set X = X and the total number of training steps/time t ∼dr."
INTERPRETATION OF THE MAIN RESULTS,0.12529182879377432,"MLPs and LRLF Interactions Fig. 1 (a).
Each G(d) is a linked list. Let v be the input node.
Clearly, we have dv = d, i.e., αv = 1 and αu = 0 (since deg(u) = 1) for all other nodes u. As such"
INTERPRETATION OF THE MAIN RESULTS,0.12607003891050583,"풜(G(d)) = N\{0} , ℱ(r) = |r|, 풮(r) = 0, ℒ(r) = |r| and ℒG(d) = (|r| : r ∈N\{0}).
(29)"
INTERPRETATION OF THE MAIN RESULTS,0.12684824902723735,"There isn’t any spatial structure since 풮(r) = 0. Given a budget index r, the kernels can only learn
span{Y r,l : r < r , r ∈N\{0}}. Therefore, MLPs are good at modeling LRLF interactions."
INTERPRETATION OF THE MAIN RESULTS,0.12762645914396886,"S-CNNs and SRHF Interactions Fig. 1 (b).
For one-hidden layer convolutional networks, we
have d = p × k0 × w, (i.e. L = 0). The number of input nodes is w ≡dαw and for each input
node v we have dv = p ≡dαp. We have αp + αw = 1. Since the activation function of the
last layer is the identity function, there is no non-linear interactions between different patches and
풜(G(d)) = {r ∈N|N (d)
0
|\{0}, |퓃(r)| = 1}. Therefore for r ∈풜(G(d)),"
INTERPRETATION OF THE MAIN RESULTS,0.12840466926070038,"풮(r) = αw ,
ℱ(r) = |r|αp,
ℒG = {αw + |r|αp, r ∈풜(G(d))}
(30)"
INTERPRETATION OF THE MAIN RESULTS,0.12918287937743192,Under review as a conference paper at ICLR 2022
INTERPRETATION OF THE MAIN RESULTS,0.12996108949416343,"Given a budget index r, the learnable r are the ones r ∈풜(G(d)) with"
INTERPRETATION OF THE MAIN RESULTS,0.13073929961089495,"r > αw + |r|αp = 1 + (|r| −1)αp ⇒|r| < 1 + (r −1)/αp .
(31)"
INTERPRETATION OF THE MAIN RESULTS,0.13151750972762646,"When αp = 1 (and thus αw = 0), this S-CNN is essentially a shallow MLP and we have |r| < r. On
the other hand, if αp is small (say αp = 0.1), this network can model interactions with much higher
frequencies, at the cost of giving up long-range interactions. Therefore, there is a trade-off between
space and frequency, and S-CNNs with small patch size are good at modeling SRHF interactions."
INTERPRETATION OF THE MAIN RESULTS,0.13229571984435798,"D-CNNs and Interterpolation Fig. 1 (c).
Recall that d = p × kL × w. Let p = dαp, k = dαk and
w = dαw. Then we have the constraint αp + Lαk + αw = 1. The learnable terms are the ones with"
INTERPRETATION OF THE MAIN RESULTS,0.1330739299610895,"r > 풮(r) + ℱ(r) = 풮(r) + |r|αp.
(32)"
INTERPRETATION OF THE MAIN RESULTS,0.133852140077821,"D-CNNs can simultaneously model interpolations (including the two ends) between LRLF and
SRHF, i.e. MRMF interactions. Consider two extreme cases. (i.) |퓃(r)| = 1, i.e. there is an
input node v s.t. rv = |r| and thus the non-linear interaction happens within one patch. In this
case, the length of the MST reaches its inﬁmum 풮(r) = αw + Lαk = 1 −αp and Eq. (32) implies
|r| < (r −1)/αp + 1, which is exactly Eq. (31). Thus D-CNNs can model SRHF interactions; see
Fig. 6 Y∗
5. (ii.) |퓃(r)| = |r|, i.e. rv = 0 or 1 for all v ∈N (d)
0
. Then 풮(r) = |r|(1 −αp) and
Eq. (32) implies |r| < r, which is the constraint for MLPs. In particular, D-CNNs can model LRLF
interactions; see Y5. By varying |r| and then varying |퓃(r)| in [1, |r], D-CNNs can model various
interpolating interactions between LRLF and SRHF."
INTERPRETATION OF THE MAIN RESULTS,0.13463035019455252,"HR-CNNs Extrapolations and Finer Interpolations Fig. 1 (d).
The resolution of the learning
indices ℒ(G(d)) can be improved by decreasing αk, αp and increasing L accordingly. E.g., changing
αp →
αp"
INTERPRETATION OF THE MAIN RESULTS,0.13540856031128404,"2 , αk →αk/2 and doubling the number of convolutional layers accordingly, then the
resolution of the range of spatial/frequency/learning indices is doubled. This empowers the network
to model ﬁner-grained interpolating modes, and extrapolating modes. E.g., the last equation in
Eq. (31) becomes |r| < 1 + 2(r −1)/αp (almost doubles the upper bound of |r|) and the network
can additionally model Ultra-Short-Range-Ultra-High-Frequency interactions (see Y∗
5 in Fig. 3)
without sacriﬁcing its expressivity, which isn’t the case for S-CNNs due to the space-frequency
trade-off. We refer to such networks as high-resolution CNNs (HR-CNNs). For practical networks,
e.g. ResNet (He et al., 2016), the ﬁlters/patches are already quite small and there isn’t much room to
reduce them. Equivalently, one increases the resolution of the input images instead, i.e. increasing d.
From this point of view, HR-CNNs and, therefore, our theorems justify the additional performance
gain from EfﬁcientNet-type (Tan & Le, 2019) of model scaling."
INTERPRETATION OF THE MAIN RESULTS,0.13618677042801555,"For more details regarding computing the learning index, see Sec. D."
EXPERIMENTS,0.1369649805447471,"6
EXPERIMENTS"
EXPERIMENTS,0.1377431906614786,"There are many practical consequences due to Theorem 1 and Theorem 2. We focus on two of them
which are about the impact of architectures to learning / generalization:"
EXPERIMENTS,0.13852140077821012,"1. Order of Learning (Fig. 1 Green Arrow.) The order of learning is restructured from
frequency-based (MLPs) to space-and-frequency-based (CNNs)."
EXPERIMENTS,0.13929961089494164,"2. Learnability (Fig. 1 Cells under the budget line.) With the same budget index, MLP-
Learnable ⊊D-CNN-Learnable ⊊HR-CNN-Learnable. Moreover, the set differences be-
tween these learnable sets are captured as in Sec.5."
EXPERIMENTS,0.14007782101167315,"Overall, we see excellent agreements between predictions from our theorems and experimental re-
sults from both practical-size networks and kernel methods using NNGP/NT kernels, even when
d = 256 is moderate-size. We detail the setup, results, corrections, etc. for the experiments below."
EXPERIMENTS,0.14085603112840467,"Setup.
Set d = p4 and the input X = (Sp−1)p3 ⊆Rp4, where p ∈N. Note that αp = 1/4. The
task is learning a function Y ∈L2(X) by minimizing the MSE, where"
EXPERIMENTS,0.14163424124513618,"Y = Y1 + Y2 + Y3 + Y4 + Y5 + Y∗
5 + Y6 + Y7
(33)"
EXPERIMENTS,0.1424124513618677,Under review as a conference paper at ICLR 2022
EXPERIMENTS,0.1431906614785992,"and each eigenfunction Yi is normalized so that ∥Yi∥2
2 = ∥Y ∥2
2/8 = 1. The exact expressions of the
functions can be found in Sec.D."
EXPERIMENTS,0.14396887159533073,"We optimize ﬁnite-width networks by SGD+Momentum and inﬁnite-width networks (NNGP and
NTK) by kernel regression. We investigate three types of architectures: (1) Dense⊗4, a four hidden
layer MLP; (2) Conv(p2)⊗2, a “deep"" CNN with ﬁlter size/stride k = p2, and (3) Conv(p)⊗4, a
“HS""-CNN with ﬁlter size/stride k = p. See Fig. 2 and Fig. 6 for a visualization of the associated
DAGs, and Sec. F for the code of the architectures. There is an activation φ in each hidden layer,
which is chosen so that φ∗is the Gaussian kernel. For the CNNs, the readout layer(s) is Flatten-
Dense-Act-Dense. We carefully chose the eigenfunctions {Yi} so that they cover a wide range of
space-frequency combinations (풮(Yi), ℱ(Yi)) w.r.t. Conv(p)⊗4. Under Conv(p)⊗4, the correspond-
ing learning indices are ℒ(Yi) = 풮(Yi) + ℱ(Yi) = 3αp + iαp = (3 + i)/4. For the learning
indices of Yi under Conv(p2)⊗2 or Dense⊗4, see the legends in Fig.3. The purpose of doing so
is to create a “separation of learning"" under Conv(p)⊗4, since in the large p limit, learning Yi re-
quires d(3+i)/4+ϵ examples/SGD steps. The relation between architectures and learning indices can
be “visualized"" in Fig. 2 for Y2, which is short-range-low-frequency ( deg(Y2) = 2). Fig. 2 (d)
plots the test residual of Y2 vs training set size for NTK regression, as one example to showcase
the relation among architectures, learning indices and generalization. See Fig. 6 in the appendix for
other eigenfunctions."
EXPERIMENTS,0.14474708171206227,"In the experiments, the width/number of channels is set to 512 for all networks. We sample mt =
32 × 10240 (mv = 10240) data points randomly from X as training (test) set with p = 4 and d =
256. The SGD training conﬁgurations (batch size (=10240), learning rate (=1.), momentum (=0.9)
etc.) are identical across architectures. To compute the kernels, we rely crucially on NeuralTangents
(Novak et al., 2020) which is based on JAX (Bradbury et al., 2018)."
EXPERIMENTS,0.14552529182879378,"In Fig.3, for each eigenfunction Yi, we plot 1"
EXPERIMENTS,0.1463035019455253,"2E| ˆYi(x, t) −Yi(x)|2
2 against t, where ˆYi(x, t) is the
projection of the prediction onto Yi and t is either the training steps (SGD) or training set size
(kernels). The expectation is taken over the test set. The budget index r = log(mt)/ log(d) ≈2.28.
As d = 256 (p = 4) is far from the asymptotic limit, we expect r = 2.28 being a soft cut-off between
learnable and non-learnable indices. Although the theorems assume d, p →∞, they do provide good
predictions even when d and p are far from ∞. We summarize several key observations below."
EXPERIMENTS,0.14708171206225681,"1. Dense⊗4 (1nd Row.) This architecture can capture all low-frequency interactions (deg =
1, 2, Y1, Y2, Y3, Y5) but fail to learn deg ≥3 interactions, as expected. For MLPs,
making the network deeper won’t improve its learnability much; see Fig. 7."
EXPERIMENTS,0.14785992217898833,"2. Conv(p2)⊗2 (2nd Row.) Learning curves of Y2/Y3 are separated from Y5 because the
spatial indices of them are different. Higher-frequency (deg = 3, 4) shorter-range interac-
tions (Y4, Y6) become (partially) learnable, as L(Y4) = 8"
EXPERIMENTS,0.14863813229571984,"4, L(Y6) = 10"
EXPERIMENTS,0.14941634241245136,4 < r ≈2.28.
EXPERIMENTS,0.15019455252918287,"3. Conv(p)⊗4 (3rd Row). We see L(Yi) capture the order of learning very/reasonably well
in the kernel/SGD setting.
To test the ability of Conv(p)⊗4 in modeling ultra-short-
range-ultra-high-frequency interactions, we trace the learning progress of Y∗
5 (deg(Y∗
5) =
5, L(Y∗
5) = 8"
EXPERIMENTS,0.1509727626459144,"4.) As expected, while other architectures completely fail to make progress,
the NTK/NNGP of Conv(p)⊗4 makes good progress and the SGD even completes the learn-
ing process. Interestingly, Y51 is learned faster than Y∗
5 in the kernel setting but slower in
the SGD setting (even slower than L(Y6) = 10"
EXPERIMENTS,0.1517509727626459,"4 ), which is unexpected. We suspect it might
be due to certain “implicit"" effect of SGD. Further investigation is needed to understand it."
CONCLUSION,0.15252918287937745,"7
CONCLUSION"
CONCLUSION,0.15330739299610896,"We establish a precise relation among networks’ architectures, eigenstructures of the inducing ker-
nels, and generalization of the corresponding kernel machines. We show that deep convolutional
networks restructure the eigenspaces of the inducing kernels, which empowers them to learn a dra-
matically broader class of functions, covering a wide range of space-frequency combinations. We
believe our framework can be extended to study architectural inductive biases for other families of
topologies, such as RNNs, GNNs, and self-attention. However, we have not covered the learning"
CONCLUSION,0.15408560311284047,"1Ultra-Long-Range-Low-Frequency under Conv(p)⊗4, with L(Y5) = 8/4 = L(Y∗
5)"
CONCLUSION,0.154863813229572,Under review as a conference paper at ICLR 2022
CONCLUSION,0.1556420233463035,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.15642023346303502,log(TrainingSetSize)/log(d) 0.0 0.2 0.4 0.6 0.8 MSE
CONCLUSION,0.15719844357976653,"MLP
4: NNGP Regression"
CONCLUSION,0.15797665369649805,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.15875486381322956,log(TrainingSetSize)/log(d)
CONCLUSION,0.15953307392996108,"MLP
4: NTK Regression"
CONCLUSION,0.16031128404669262,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.16108949416342414,log(SGDSteps)/log(d)
CONCLUSION,0.16186770428015565,"MLP
4: SGD Test"
CONCLUSION,0.16264591439688716,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.16342412451361868,log(SGDSteps)/log(d)
CONCLUSION,0.1642023346303502,"MLP
4: SGD Train"
CONCLUSION,0.1649805447470817,L(Y1) = 4/4
CONCLUSION,0.16575875486381322,L(Y2) = 8/4
CONCLUSION,0.16653696498054474,L(Y3) = 8/4
CONCLUSION,0.16731517509727625,L(Y4) = 12/4
CONCLUSION,0.1680933852140078,"L(Y *
5) = 20/4"
CONCLUSION,0.1688715953307393,L(Y5) = 8/4
CONCLUSION,0.16964980544747083,L(Y6) = 16/4
CONCLUSION,0.17042801556420234,L(Y7) = 16/4 MSE/8
CONCLUSION,0.17120622568093385,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.17198443579766537,log(TrainingSetSize)/log(d) 0.0 0.2 0.4 0.6 0.8 MSE
CONCLUSION,0.17276264591439688,"CNN(p2)
2: NNGP Regression"
CONCLUSION,0.1735408560311284,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.1743190661478599,log(TrainingSetSize)/log(d)
CONCLUSION,0.17509727626459143,"CNN(p2)
2: NTK Regression"
CONCLUSION,0.17587548638132297,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.17665369649805449,log(SGDSteps)/log(d)
CONCLUSION,0.177431906614786,"CNN(p2)
2: SGD Test"
CONCLUSION,0.17821011673151751,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.17898832684824903,log(SGDSteps)/log(d)
CONCLUSION,0.17976653696498054,"CNN(p2)
2: SGD Train"
CONCLUSION,0.18054474708171206,L(Y1) = 4/4
CONCLUSION,0.18132295719844357,L(Y2) = 6/4
CONCLUSION,0.1821011673151751,L(Y3) = 6/4
CONCLUSION,0.1828793774319066,L(Y4) = 8/4
CONCLUSION,0.18365758754863815,"L(Y *
5) = 12/4"
CONCLUSION,0.18443579766536966,L(Y5) = 8/4
CONCLUSION,0.18521400778210118,L(Y6) = 10/4
CONCLUSION,0.1859922178988327,L(Y7) = 12/4 MSE/8
CONCLUSION,0.1867704280155642,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.18754863813229572,log(TrainingSetSize)/log(d) 0.0 0.2 0.4 0.6 0.8 MSE
CONCLUSION,0.18832684824902723,"CNN(p)
4: NNGP Regression"
CONCLUSION,0.18910505836575875,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.18988326848249026,log(TrainingSetSize)/log(d)
CONCLUSION,0.19066147859922178,"CNN(p)
4: NTK Regression"
CONCLUSION,0.19143968871595332,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.19221789883268484,log(SGDSteps)/log(d)
CONCLUSION,0.19299610894941635,"CNN(p)
4: SGD Test"
CONCLUSION,0.19377431906614787,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
CONCLUSION,0.19455252918287938,log(SGDSteps)/log(d)
CONCLUSION,0.1953307392996109,"CNN(p)
4: SGD Train"
CONCLUSION,0.1961089494163424,L(Y1) = 4/4
CONCLUSION,0.19688715953307392,L(Y2) = 5/4
CONCLUSION,0.19766536964980544,L(Y3) = 6/4
CONCLUSION,0.19844357976653695,L(Y4) = 7/4
CONCLUSION,0.1992217898832685,"L(Y *
5) = 8/4"
CONCLUSION,0.2,L(Y5) = 8/4
CONCLUSION,0.20077821011673153,L(Y6) = 9/4
CONCLUSION,0.20155642023346304,L(Y7) = 10/4 MSE/8
CONCLUSION,0.20233463035019456,"Figure 3: Learning Dynamics vs Architectures vs Learning Indices. We plot the learning/training
dynamics of each eigenfunction Yi. From top to bottom: a 4-layer MLP, a 2-layer CNN and a 4-layer
CNN. From left to right: residual MSE (per eigenfunction) of NNGP/NTK regression, test/training
MSE of SGD. The learning indices of Yi in each architecture is shown in the legends."
CONCLUSION,0.20311284046692607,"dynamics of SGD. In addition, it is of great interest and importance to study the combined effect of
SGD and architectures in the future."
REFERENCES,0.20389105058365758,REFERENCES
REFERENCES,0.2046692607003891,"William Beckner. Inequalities in fourier analysis. Annals of Mathematics, 102(1):159–182, 1975."
REFERENCES,0.20544747081712061,"William Beckner. Sobolev inequalities, the poisson semigroup, and analysis on the sphere sn. Pro-
ceedings of the National Academy of Sciences, 89(11):4816–4819, 1992."
REFERENCES,0.20622568093385213,"Alberto Bietti. Approximation and learning with deep convolutional models: a kernel perspective,
2021."
REFERENCES,0.20700389105058364,"Alberto Bietti and Francis Bach. Deep equals shallow for relu networks in kernel regimes. arXiv
preprint arXiv:2009.14397, 2020."
REFERENCES,0.2077821011673152,"James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy
programs, 2018. URL http://github.com/google/jax."
REFERENCES,0.2085603112840467,"Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, 2016."
REFERENCES,0.20933852140077822,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848."
REFERENCES,0.21011673151750973,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
REFERENCES,0.21089494163424125,Under review as a conference paper at ICLR 2022
REFERENCES,0.21167315175097276,"Arturo Deza, Qianli Liao, Andrzej Banburski, and Tomaso Poggio. Hierarchically compositional
tasks and deep convolutional networks. arXiv preprint arXiv:2006.13915, 2020."
REFERENCES,0.21245136186770427,"Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018."
REFERENCES,0.2132295719844358,"Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019."
REFERENCES,0.2140077821011673,"Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
sionality in convolutional teacher-student scenarios, 2021."
REFERENCES,0.21478599221789882,Christopher Frye and Costas J. Efthimiou. Spherical harmonics in p dimensions. 2012.
REFERENCES,0.21556420233463036,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks. arXiv preprint arXiv:1906.08899, 2019."
REFERENCES,0.21634241245136188,"Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension, 2020."
REFERENCES,0.2171206225680934,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014."
REFERENCES,0.2178988326848249,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016."
REFERENCES,0.21867704280155642,"Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
http://github.com/google/flax."
REFERENCES,0.21945525291828794,"Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-
time learning dynamics of neural networks. arXiv preprint arXiv:2006.14599, 2020."
REFERENCES,0.22023346303501945,"Wei Huang, Weitao Du, and Richard Yi Da Xu. On the neural tangent kernel of deep networks with
orthogonal initialization. arXiv preprint arXiv:2004.05867, 2020."
REFERENCES,0.22101167315175096,"Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018."
REFERENCES,0.22178988326848248,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020."
REFERENCES,0.222568093385214,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012."
REFERENCES,0.22334630350194554,"Yann Lecun. Generalization and network design strategies. In Connectionism in perspective. Else-
vier, 1989."
REFERENCES,0.22412451361867705,"Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-
dickstein. Deep neural networks as gaussian processes. In International Conference on Learning
Representations, 2018."
REFERENCES,0.22490272373540857,"Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019."
REFERENCES,0.22568093385214008,"Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efﬁcient than
fully-connected nets?
CoRR, abs/2010.08515, 2020. URL https://arxiv.org/abs/2010.
08515."
REFERENCES,0.2264591439688716,Under review as a conference paper at ICLR 2022
REFERENCES,0.2272373540856031,"Eran Malach and Shai Shalev-Shwartz. Computational separation between convolutional and fully-
connected networks.
CoRR, abs/2010.01369, 2020.
URL https://arxiv.org/abs/2010.
01369."
REFERENCES,0.22801556420233463,"Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Conference on
Learning Representations, 2018."
REFERENCES,0.22879377431906614,"Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
Generalization error of random fea-
tures and kernel methods: hypercontractivity and kernel matrix concentration. arXiv preprint
arXiv:2101.10588, 2021a."
REFERENCES,0.22957198443579765,"Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021b."
REFERENCES,0.23035019455252917,"Ashley Montanaro. Some applications of hypercontractive inequalities in quantum information the-
ory. Journal of Mathematical Physics, 53(12):122206, 2012."
REFERENCES,0.2311284046692607,"Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods for interpreting and un-
derstanding deep neural networks. Digital Signal Processing, 73:1–15, 2018."
REFERENCES,0.23190661478599223,"Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L. Edelman, Fred
Zhang, and Boaz Barak. SGD on neural networks learns functions of increasing complexity.
CoRR, abs/1905.11604, 2019. URL http://arxiv.org/abs/1905.11604."
REFERENCES,0.23268482490272374,"Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolaﬁa, Jeffrey
Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many chan-
nels are gaussian processes. In International Conference on Learning Representations, 2019a.
URL https://openreview.net/forum?id=B1g30j0qF7."
REFERENCES,0.23346303501945526,"Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abo-
laﬁa, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are gaussian processes. In International Conference on Learning Representations,
2019b."
REFERENCES,0.23424124513618677,"Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy inﬁnite neural networks in python.
In International Conference on Learning Representations, 2020. URL https://github.com/
google/neural-tangents."
REFERENCES,0.23501945525291829,"Yaniv Ovadia, Emily Fertig, J. Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. In NeurIPS, 2019."
REFERENCES,0.2357976653696498,"Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-
nential expressivity in deep neural networks through transient chaos.
In Advances In Neural
Information Processing Systems, 2016."
REFERENCES,0.23657587548638132,"Meyer Scetbon and Zaid Harchaoui. Harmonic decompositions of convolutional networks. In Inter-
national Conference on Machine Learning, pp. 8522–8532. PMLR, 2020."
REFERENCES,0.23735408560311283,"Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017."
REFERENCES,0.23813229571984434,"Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020."
REFERENCES,0.2389105058365759,"Vaishaal Shankar, Alex Chengyu Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt,
Jonathan Ragan-Kelley, and Benjamin Recht. Neural kernels without tangents. In International
Conference on Machine Learning, 2020."
REFERENCES,0.2396887159533074,Under review as a conference paper at ICLR 2022
REFERENCES,0.24046692607003892,"David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016."
REFERENCES,0.24124513618677043,"Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, and Jaehoon Lee. On the inﬁnite
width limit of neural networks with a standard parameterization, 2020."
REFERENCES,0.24202334630350195,"Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019."
REFERENCES,0.24280155642023346,"M. Alex O. Vasilescu, Eric Kim, and Xiao S. Zeng. Causalx: Causal explanations and block multi-
linear factor analysis, 2021."
REFERENCES,0.24357976653696498,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,
2017."
REFERENCES,0.2443579766536965,"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010."
REFERENCES,0.245136186770428,"Lechao Xiao and Jeffrey Pennington. What breaks the curse of dimensionality in deep learning?,
2021. URL https://openreview.net/forum?id=KAV7BDCcN6."
REFERENCES,0.24591439688715952,"Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla con-
volutional neural networks. In International Conference on Machine Learning, pp. 5393–5402,
2018."
REFERENCES,0.24669260700389106,"Greg Yang and Edward J Hu. Feature learning in inﬁnite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020."
REFERENCES,0.24747081712062258,"Greg Yang and Hadi Salman. A ﬁne-grained spectral perspective on neural networks, 2020."
REFERENCES,0.2482490272373541,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115, 2021."
REFERENCES,0.2490272373540856,Under review as a conference paper at ICLR 2022
REFERENCES,0.24980544747081712,"A
ADDITIONAL RELATED WORK"
REFERENCES,0.25058365758754864,"Bietti (2021) studies the approximation and learning properties of CNN kernels via the lens of
RKHS. Impressively, they demonstrate that a 2-layer CNN kernel can reach 88.3% validation ac-
curacy on CIFAR-10, matching the performance of a 10-layer Myrtle kernel Shankar et al. (2020).
Malach & Shalev-Shwartz (2020) and Li et al. (2020) study the algorithmic beneﬁts of shallow
CNNs and show that they outperform MLPs in certain tasks. Xiao & Pennington (2021) and Favero
et al. (2021) study the beneﬁts of locality in S-CNNs and argue that locality is the key ingredient
to defeat the curse of dimensionality. Mei et al. (2021b) and several papers mentioned above study
the beneﬁts of pooling in (S-)CNNs in terms of data efﬁciency. Their conclusion is similar to that
of Theorem 4 (b): pooling improves data efﬁciency by a factor of the pooling size. In addition,
we show that (Theorem 4 (a)) pooling does not improve training efﬁciency for D-CNNs, extending
a result from Xiao & Pennington (2021) which concerns S-CNNs. Finally, Scetbon & Harchaoui
(2020) also study the eigenstructures of certain CNN kernels without pooling. Their kernels can be
considered as a particular case of the NNGP kernels, where the associated networks have only one
convolutional layer and multiple dense layers. The key contribution that sets the current work apart
from existing work is a precise mathematical characterization of the fundamental role of architec-
tures in (inﬁnite-width) networks through a space-frequency analysis."
REFERENCES,0.25136186770428015,"B
A TOY EXAMPLE AND MOTIVATION"
REFERENCES,0.25214007782101167,"In this section, we provide a toy example to help understand the motivation and ideas of the paper.
Let’s consider learning the following polynomials in Sd−1 ≡{x ∈Rd : ∥x∥2 = 1} using (in)ﬁnite-
width neural networks and for concreteness we have set d = 10:"
REFERENCES,0.2529182879377432,"f1(x) = x9, f2(x) = x0x1, f3(x) = x0x8, f4(x) = x6x7(x2
6 −x2
7), f5(x) = x2x3x5
(34)"
REFERENCES,0.2536964980544747,"Which architectures (e.g. MLPs, CNNs) can efﬁciently learn fi or the sum of fi? More precisely, (1)
if we have sufﬁciently amount of training data, how much time (compute) is required to learn fi for
a given architecture? (2) Alternatively, if we have sufﬁciently amount of compute, how much data is
needed to learn fi? To answer these questions, one crucial step is to provide a meaningful deﬁnition
of “learning complexity"" of a function fi under an architecture M. Denote this complexity associ-
ated to compute and to data by CC(fi; M) and CD(fi; M), resp. With such the complexity properly
deﬁned, the questions are reduced to solving the min-max problem minM maxi{CC/D(fi; M)}, if
the task is, e.g, to learn the sum of fi."
REFERENCES,0.2544747081712062,"Let’s focus on the complexity. For inﬁnite-width MLPs, aka, inner product kernels, it is well-
known that they have the inductive biases (Yang & Salman, 2020; Ghorbani et al., 2020) (known
as the frequency biases) that the model prioritizes learning low-frequency modes (i.e., low degree
polynomials) over high-frequency modes. In addition, the models require ∼dr many data points to
learn any degree r polynomials in Rd. The frequency biases of MLPs are the consequence of the fact
that the eigenspaces of inner product kernels are structured based only on frequencies. Speciﬁc to our
example, for MLP, the order of learning is f1/f2, f3/f5/f4 and it requires about 10/102, 102/104
many data points to learn the functions. Clearly, the model is very inefﬁcient in learning f4, the
high-frequency modes."
REFERENCES,0.2552529182879377,"To improve the learning efﬁciency, we must take the modality of the task into account, which is
overlooked by MLPs. We observe that: (1) although of high frequency, f4 depends only on two con-
secutive terms x6 and x7, which are spatially close; (2) in contrast, f3(x) = x0x8 is of low frequency
but the spatial distance between the two interaction terms x0 and x8 are “far"" from each other; (3) the
function f5(x) = x2x3x5 is somewhere in-between: the order of interaction is 3 (lower than that of
f4) and the spatial distance (not yet deﬁned) among interaction terms is conceptually “closer"" than
that of f3(x) = x0x8, but “farther"" than that of f5. Using the terminologies from the introduction,
the functions f2/f3/f5/f4 model interactions of types: Short-Range-Low-Frequency/Long-Range-
Low-Frequency/Median-Range-Median-Frequency/Short-Range-High-Frequency.
By Range we
mean the distance among interaction terms and by Frequency we mean the order(=degree) of in-
teractions. Clearly, the MLPs are inefﬁcient since they totally ignore the “spatial structure"" of the
functions. As such, a good architecture must balance the ""spatial structure"" and the “frequency
structure"" of the functions. For the same reason, a good complexity measure must (1) be able to
capture both the frequency of the functions and the spatial distance among interaction terms; (2)"
REFERENCES,0.25603112840466924,Under review as a conference paper at ICLR 2022
REFERENCES,0.25680933852140075,"be able to precisely characterize the data and the computation efﬁciency of learning and their de-
pendence on architectures. The learning index mentioned in the introduction satisﬁes these two
conditions. It is the sum of the frequency index and the spatial index. The former measures the
order (=degree=frequency) of interactions, which depends on how the network partitions the input
into patches. The latter measures the spatial distance among the interaction terms, which depends on
how the network organizes these patches hierarchically. Later we show that, in the high-dimensional
setting, the learning index provides a sharp characterization for the learnability of eigenfunctions,
and certain CNNs can perfectly balance the learning of f3 and f4, i.e., informally
CC/D(f3; CNN) ≈CC/D(f4; CNN) ≈CC/D(f2/3; MLP) << CC/D(f3; MLP)
(35)
See Fig. 3 in the experiment section for more details."
REFERENCES,0.25758754863813227,"C
GLOBAL AVERAGE POOLING (GAP) VS FLATTENING"
REFERENCES,0.25836575875486384,"In this section, we compare two readout strategies for CNNs: GAP and Flatten"
REFERENCES,0.25914396887159535,"C.1
INFINITE-TRAINING DATA."
REFERENCES,0.25992217898832687,"First, let’s state a theorem regarding training efﬁciency in the inﬁnite-training-data-ﬁnite-training-
time regime, which follows directly from the same arguments in Sec. 2. The theorem requires
knowing only the eigenvalues of the kernels."
REFERENCES,0.2607003891050584,"Let F : L2(X) →L2(X) be the solution operator to the kernel descent ˙h = −K(h−f) with initial
value ht=0 = 0, i.e. ht ≡Ft(f) ≡(Id −e−Kt)f. Then we have for t ∼dr, Ft ≈P<r."
REFERENCES,0.2614785992217899,"Theorem 3. Assume Assumption-G and Assumption-φ and K = 풦G(d) or ΘG(d). Let r /∈L(G(d))
and t ∼dr. Then for 0 < ϵ < inf{|r −¯r| : ¯r ∈L(G(d))} and f ∈L2(X) with Eσf = 0 we have"
REFERENCES,0.2622568093385214,"∥Ft(P<rf) −P<rf∥2
2 ≲e−dϵ∥P<rf∥2
2
and
∥Ft(P>rf) −P>rf∥2
2 ≳e−d−ϵ∥P>rf∥2
2
(36)
for d sufﬁciently large."
REFERENCES,0.2630350194552529,"In words, in the inﬁnite-training-data-ﬁnite-training-time regime, within t ∼dr amount of time only
the eigenfunctions Y r,l with L(r) < r are learnable."
REFERENCES,0.26381322957198444,"C.2
GAP VS FLATTENING."
REFERENCES,0.26459143968871596,Let a CNN with and without a global average pooling (GAP) be deﬁned as
REFERENCES,0.26536964980544747,"CNN+GAP
[Input] →[Conv(p)-Act] →[Conv(k)-Act]⊗L →[GAP] →[Dense]
(37)"
REFERENCES,0.266147859922179,"CNN+Flatten
[Input] →[Conv(p)-Act] →[Conv(k)-Act]⊗L →[Flatten] →[Dense]
(38)
resp. Note that there isn’t any activation after the GAP/Flatten layer. The DAGs associated to
CNN+GAP and CNN+Flatten are identical, and the associated kernel and network computations in
each layer are also identical but the last layer; see Sec. K for more details. Our last theorem states
that the GAP improves the data efﬁciency by a factor of w, the window size of the pooling, but
does not improve the training efﬁciency in the inﬁnite-training-data-ﬁnite-training-time regime. The
former is due to the dimension reduction effect of GAP in the eigenspaces and the latter is due to
the fact that the GAP layer does not change the eigenvalues of the associated eigenspaces."
REFERENCES,0.2669260700389105,"Let KSym = 풦Sym or ΘSym be the NNGP kernel or NTK associated to Eq. (37) and Lp
Sym(X) ≤
Lp(X) be the subspace of “translation-invariant"" functions, whose co-dimension is w. Let FSym,
P Sym and RSym be the solution operator, projection operator and regressor associated to KSym, resp.
Theorem 4 (Informal). For the architectures deﬁned as in by Eq. (37), we have"
REFERENCES,0.267704280155642,"(a) Theorem 3 holds with L2(X), F and P replaced by L2
Sym(X), FSym and P Sym, resp."
REFERENCES,0.26848249027237353,"(b) Eq. (28) holds with Lp(X), R and P replaced by Lp
Sym(X), RSym and P Sym, resp and"
REFERENCES,0.26926070038910505,"with X ∼σ[dr−αw ] under the assumptions that all activations in the hidden layers are
poly-admissible."
REFERENCES,0.27003891050583656,Under review as a conference paper at ICLR 2022
REFERENCES,0.2708171206225681,Remark 1. Several remarks are in order.
REFERENCES,0.2715953307392996,"1. In terms of order of learning, our results say that (inﬁnite-width) neural networks pro-
gressively learn more complex functions, where complexity is deﬁned to be the learning
index L(r). This is consistent with the empirical observation from Nakkiran et al. (2019).
Regardless of architectures (MLPs vs CNNs), linear functions have the smallest learn-
ing index2 L(r) = 1 and are always learned ﬁrst, which was ﬁrst proved in Hu et al.
(2020) for MLPs. After learning the linear functions, the learning dynamics of MLPs
and CNNs diverge. MLPs will start to learn quadratic functions (L(r) = 2), then cu-
bic functions (L(r) = 3) and so on. While for CNNs, L(r) can be fractional numbers (e.g.
L(r) = 5/4, 6/4) and it is possible for the network to learn higher order functions before
lower order functions. See Sec.D and Sec. 6 for more details."
REFERENCES,0.2723735408560311,"2. By NTK-style convergent arguments (e.g., Du et al. (2019)), the above kernel regression
results could be extended to the over-parameterized network setting as long as the learning
rate of gradient descent is small and the number of channels is sufﬁciently large (polyno-
mially in d (Huang et al., 2020))."
REFERENCES,0.2731517509727626,"3. Theorem 4 states that CNN+GAP is better than CNN+Flatten in terms of data efﬁciency but
not training efﬁciency. In particular, when the dataset size is sufﬁciently large, CNN+GAP
and CNN+Flatten perform equally well. Therefore, in the large (inﬁnite) data set regime,
CNN+Flatten may be preferable since the associated function class is less restricted."
REFERENCES,0.2739299610894942,"C.3
EXPERIMENTAL RESULTS: GAP VS FLATTEN."
REFERENCES,0.2747081712062257,"We compare the SGD learning dynamics of two convolutional architectures: Conv(p)⊗3-Flatten and
Conv(p)⊗3-GAP. Both networks have three convolutional layers with ﬁlter size and stride equal to
p. The spatial dimension is reduced to p after the convolutional layers. The only difference is that
the architecture Conv(p)⊗3-Flatten ( Conv(p)⊗3-GAP) uses a Flatten-Dense ( GAP-Dense) to map
the penultimate layer to the logit layer."
REFERENCES,0.2754863813229572,"The experimental setup is almost the same as that of Sec. 6 except the eigenfunctions {Yi} are chosen
to be in the RKHS of the NNGP kernel/NTK of Conv(p)⊗3-GAP and thus of Conv(p)⊗3-Flatten,
i.e., they are shifting-invariant (the invariant group is of order p). Moreover, we still have ℒ(Yi) =
(i + 3)/4. For each Yi, we plot the validation MSE of the residual vs SGD steps in Fig. 4. Overall,
the predictions from Theorem 4 gives excellent agreement with the empirical result. With training
set size mt = 32 × 10240, the residuals of Yi for GAP and Flatten are almost indistinguishable
from each other for i ≤6 (recall that ℒ(Y6) = 9/4 = 2.25 < r ≈2.28). However, when i = 7
the dataset size (r ≈2.28) is relatively small compared to the learning index (ℒ(Y7) = 2.5), GAP
outperforms Flatten in learning Y7"
REFERENCES,0.27626459143968873,"To test the robustness of the prediction from Theorem 4 (a) on more practical datasets and models,
we perform an additional experiment on ImageNet (Deng et al., 2009) using ResNet (He et al.,
2016). We compare the performance of the original ResNet50, denoted by ResNet50-GAP, and a
modiﬁed version ResNet50-Flatten, in which the GAP readout layer is replaced by Flatten. We
use the ImageNet codebase from FLAX3(Heek et al., 2020). In order to see how the performance
difference between ResNet50-GAP and ResNet50-Flatten evolves as the training set size increases,
we make a scaling plot, namely, we vary the training set sizes4 mi = [m × 2−i/2] for i = 0, . . . , 11,
where m = 1281167 is the total number of images in the training set of ImageNet. The networks
are trained for 150 epochs with batch size 128. We plot the validation accuracy and loss (averaged
over 3 runs) as a function of training set size mi in Fig. 5. Overall, we see that the performance gap
between ResNet50-GAP and ResNet50-Flatten shrink substantially as the training set size increases.
E.g, using 1/8 of the training set (i.e. i = 6), the top 1 accuracy between the two is 19.3% (57.7%
GAP vs 38.4% Flatten). However, with the whole training set (i.e., m0), this gap is reduced to
2% (76.5% GAP vs 74.5% Flatten). To demonstrate the robustness of this trend, we additionally
generate the same plots for ResNet34 and ResNet101; see Fig. 8 in Sec. E."
REFERENCES,0.27704280155642025,"2Ignoring the constant functions.
3https://github.com/google/ﬂax/blob/main/examples/imagenet/README.md
4Standard data-augmentation is applied for each i; see input_pipeline.py."
REFERENCES,0.27782101167315176,Under review as a conference paper at ICLR 2022
REFERENCES,0.2785992217898833,"0.5
1
1.5
2
0.75
1.25
1.75
2.25"
REFERENCES,0.2793774319066148,log(SGDSteps)/log(d) 0.0 0.2 0.4 MSE
REFERENCES,0.2801556420233463,Valid Loss: GAP vs Flatten
REFERENCES,0.2809338521400778,"Y1:GAP
Y1:Flatten
Y7:GAP
Y7:Flatten"
REFERENCES,0.28171206225680934,"Figure 4: Learning Dynamics: GAP vs Flatten. We plot the validation MSE of the residual of
each Yi (left →right: i = 1 →7) for GAP (Solid lines) and Flatten (Dashed lines). The mean/std
in each curve is obtained by 5 random initializations. Clearly, the residual dynamics of GAP and
Flatten are almost indistinguishable for Yi with i ≤6. However, GAP outperforms Flatten when
learning Y7."
REFERENCES,0.28249027237354085,"105
106"
REFERENCES,0.28326848249027237,Training Set Size 0.2 0.4 0.6
REFERENCES,0.2840466926070039,Top 1 Acc ImageNet
REFERENCES,0.2848249027237354,Accuracy: GAP vs Flatten
REFERENCES,0.2856031128404669,"ResNet50-GAP
ResNet50-Flatten"
REFERENCES,0.2863813229571984,"105
106"
REFERENCES,0.28715953307392994,Training Set Size 2 4 6
REFERENCES,0.28793774319066145,Validation Cross-Entropy
REFERENCES,0.28871595330739297,Loss: GAP vs Flatten
REFERENCES,0.28949416342412454,"ResNet50-GAP
ResNet50-Flatten"
REFERENCES,0.29027237354085605,"Figure 5: ResNet50-GAP vs ResNet50-Flatten. As the training set size increases the performance
(accuracy and loss) gap between the two shrinks."
REFERENCES,0.29105058365758757,"D
DAGS, EIGENFUNCTIONS, SPATIAL INDEX, AND FREQUENCY INDEX."
REFERENCES,0.2918287937743191,"In this section, we provide more details regarding the DAGs and the eigenfunctions used in the
experiments, and how the spatial, frequency and learning indices are computed."
REFERENCES,0.2926070038910506,"Let (Sp−1)p3 ⊆Rp4 be the input space, where d = p4 is the input dimension. We use x ≡
(xk)k∈[p]4 ∈(Sp−1)p3 to denote one input (an image), where k = [k1, k2, k3, k4] ∈[p]4. In
addition, we treat [p]4 as a group (i.e. with circular boundaries) and let e1 = [1, 0, 0, 0], e2 =
[0, 1, 0, 0] , e3 = [0, 0, 1, 0] and e4 = [0, 0, 0, 1] be a set of generator/basis of the group. Note that
each input xk is partitioned into p3 many patches: {xk1,k2,k3,: : k1, k2, k3 ∈[p]}."
REFERENCES,0.2933852140077821,Under review as a conference paper at ICLR 2022
REFERENCES,0.29416342412451363,"The eigenfunctions used in the experiments and the associated space/frequency indices (will be
explained momentarily) are given as follow"
REFERENCES,0.29494163424124514,"Eigenfunction
degree
Space/Freq Index"
REFERENCES,0.29571984435797666,"MLP
CNN(p2)⊗2
CNN(p)⊗4"
REFERENCES,0.2964980544747082,"Y1(x) =
X"
REFERENCES,0.2972762645914397,"k∈[p−1]4
c(1)
k xk
1
0/1
1
2/1 2 3
4/1 4"
REFERENCES,0.2980544747081712,"Y2(x) =
X"
REFERENCES,0.2988326848249027,"k∈[p−1]4
c(2)
k xkxk+e4
2
0/2
1
2/2 2 3
4/2 4"
REFERENCES,0.29961089494163423,"Y3(x) =
X"
REFERENCES,0.30038910505836575,"k∈[p−1]4
c(3)
k xk+e3xk+e4
2
0/2
1
2/2 2 4
4/2 4"
REFERENCES,0.30116731517509726,"Y4(x) =
X"
REFERENCES,0.3019455252918288,"k∈[p−1]4
c(4)
k xk+e3+e4xk+e4xk
3
0/3
1
2/3 2 4
4/3 4"
REFERENCES,0.3027237354085603,"Y∗
5(x) =
X"
REFERENCES,0.3035019455252918,"k∈[p−1]4
c(5∗)
k
xkxk+e4xk+2e4(x2
k −x2
k+e4)
5
0/5
1
2/5 2 3
4/5 4"
REFERENCES,0.3042801556420233,"Y5(x) =
X"
REFERENCES,0.3050583657587549,"k∈[p−1]4
c(5)
k xkxk+e1
2
0/2
2
2/2 2 6
4/2 4"
REFERENCES,0.3058365758754864,"Y6(x) =
X"
REFERENCES,0.3066147859922179,"k∈[p−1]4
c(6)
k xkxk+e3(3x2
k−e3 −x2
k)
4
0/4
1
2/4 2 5
4/4 4"
REFERENCES,0.30739299610894943,"Y7(x) =
X"
REFERENCES,0.30817120622568095,"k∈[p−1]4
c(7)
k xk−e3+e4xk+e2(3x2
k −x2
k−e3+e4)
4
0/4
1
2/4 2 6
4/4 4"
REFERENCES,0.30894941634241246,"Each (eigen)function Yi is a linear combination of basis eigenfunction of the same type, in the sense
they have the same eigenvalue and the same spatial/frequency/learning indices. The coefﬁcients c(i)
k
are ﬁrst sampled from standard Gaussians iid and then multiplied by an i-dependent constant so that
Yi has unit norm5. In the experiments, p is chosen to be 4 and the target is deﬁned to be sum of them"
REFERENCES,0.309727626459144,"Y =
X
Yi
(39)"
REFERENCES,0.3105058365758755,"Since they are orthogonal to each other, ∥Y ∥2
2/8 = ∥Yi∥2
2 = 1 where the L2-norm is taken over the
uniform distribution on (Sp−1)p3. In the experiment when we compare Flatten and GAP ( Fig. 4),
we set all c(i)
k to be the same (note that we also remove Y5 from the target function), so that the
functions are learnable by convolutional networks with a GAP readout layer."
REFERENCES,0.311284046692607,"We compare three architectures. Fig. 6 Column (a) MLP⊗4, a (four-layer) MLP, the most coarse
architecture used in the paper. Fig. 6 Column (b), CNN(p2)⊗2, a“D""-CNN that contains two con-
volutional layers with ﬁlter size/stride equal to p2. Fig. 6 Column (c), CNN(p)⊗4, a “HR""-CNN,
the ﬁnest architecture used in the experiments, that contains four convolutional layers with ﬁlter
size/stride equal to p. In all experiments except the one in Fig. 4, we use Flatten as the readout layer
for the convolutional networks and add a Act-Dense layer after Flatten to improve the expressivity
of the function class. However, in the Flatten vs GAP experiments, Fig. 4, we have only one dense
layer after GAP/Flatten."
REFERENCES,0.3120622568093385,"We show how to compute the frequency index, the spatial index and the learning index through
three examples. We focus on Y2 / Y3 / Y∗
5 which have degree 2 / 2 / 5, resp. The indices of other
eigenfunctions can be computed using the same approach. We use Dashed Lines to represent either
an edge connecting an input node to a node in the ﬁrst-hidden layer or an edge associated to a dense
layer. In either case, the corresponding output node of the edge has degree O(1) and thus the weights
(of the DAGs) of such edges are always 0. Only Solid Lines are relevant in computing the spatial
index. Since each Yi is a linear combination of basis eigenfunctions of the same type, we only need
to compute the indices of one component. We use the k = 0 component, which corresponds to the"
REFERENCES,0.31284046692607004,"5In our experiments, they are normalized over the test set."
REFERENCES,0.31361867704280155,Under review as a conference paper at ICLR 2022
REFERENCES,0.31439688715953307,"colored path in each DAG. Recall that p = 4, d = p4 = 256 and mt = 32×10240 ∼d2.28 (training
set size), i.e. the budget index is roughly r = 2.28."
REFERENCES,0.3151750972762646,"(a.) MLP Column (a) Fig. 6.
The NTK and NNGP kernels are inner product kernels and
the associated DAGs are linked lists. The corresponding DAG has only one input node
whose dimension is equal to d = p4. The spatial index is always 0 since the degree of
each hidden node is 1 (since 1 = d0) and the frequency index is equal to the degree of
the eigenfunctions. Thus ℒ(Y2) = ℒ(Y3) = 2 and ℒ(Y∗
5) = 5. Changing the number
of layers won’t change the learning indices. In sum, learning Y2/Y3/Y∗
5 using inﬁnite-
width MLP requires d2+/d2+/d5+ many samples /SGD steps. Clearly, Y∗
5 is completely
unlearnable as r = 2.28 << 5. In the MSE plot Y∗
5 (5-th row in Fig. 6), the Red Lines
does not make any progress"
REFERENCES,0.3159533073929961,"(b.) CNN(p2)⊗2 Column (b) Fig. 6. The input image is partitioned into p2 patches and each
patch has dimension p2. The second layer of the DAG has p2 many nodes, each node
represents one pixel (with many channels) in the ﬁrst hidden layer of a ﬁnite-width Con-
vNet. After one more convolutional layer with ﬁlter size/stride p2, the number of node
(pixel) is reduced to one.
The remaining part of the DAG is essentially a linked list
(Dashed Line) with length equal to 1, which corresponds to the Act-Dense layer. The
frequency index ℱ(Y2) = 2 1"
REFERENCES,0.3167315175097276,"2 = 1. This is because the degree of Y2 is 2 and the input
dimension of a node is p2 = d1/2. The spatial index is equal to 1/2, since the mini-
mum tree containing xkxk+e4 has only one non-zero edge (Solid Lines) whose weight
is equal to 1/2 (since the degree of the output node is p2 = d1/2); see the colored paths
in Fig. 6 Column (b). Therefore the learning index of ℒ(Y2) = 1 + 1/2 = 3/2. Sim-
ilarly ℒ(Y3) = 1 + 1/2 = 3/2, as the term xk+e3xk+e4 are lying in the same patch
of size p2 for all k, and ℒ(Y∗
5) = 5/2 + 1/2 = 3. In sum, learning Y2/Y3/Y∗
5 using
inﬁnite-width CNN(p2)⊗2 requires d1.5+/d1.5+/d3+ many samples /SGD steps. While
neither inﬁnite-width CNN(p2)⊗2 nor MLP⊗4 distinguishes Y2 from Y3, CNN(p2)⊗2"
REFERENCES,0.3175097276264591,"does improve the learning efﬁciency: d2+ →d1.5+. Note that Y∗
5 is still unlearnable as
r = 2.28 < 3 = ℒ(Y∗
5). In the MSE plot Y∗
5 (5-th row in Fig. 6), the Orange Line does
not make any progress.
(c.) CNN(p)⊗4 Column (c) Fig. 6. The input image is partitioned into p3 patches and each
patch has dimension p. The second/third/fourth/output layer of the DAG has p3/p2/p/1
many nodes. The frequency indices are: ℱ(Y2) = ℱ(Y3) = 2 1"
REFERENCES,0.31828793774319064,"4 = 1/2 and ℱ(Y∗
5) =
5 1"
REFERENCES,0.31906614785992216,"4 = 5/4. This is because the size of input nodes is reduced to p = d1/4. Unlike the
above cases, the spatial indices become different. The two interacting terms in xkxk+e4
and the three interacting terms in xk+e4xk+2e4(x2
k −x2
k+e4) are in the same input node
while the two interacting terms in xk+e3xk+e4 and are in two different input nodes. As a
consequence, the minimum spanning tree (MST) that contains xk and xk+e4 and the one
contains xk, xk+e4 and xk+2e4 are the same. They have 3 solid lines. However, the MST
containing xk+e3 and xk+e4 has 4 solid lines. Therefore 풮(Y2) = 풮(Y∗
5) = 3 × 1"
AND,0.31984435797665367,"4 and
풮(Y3) = 4 × 1"
AND,0.32062256809338524,"4. As such, ℒ(Y2) = 5"
AND,0.32140077821011676,"4, ℒ(Y3) = 6"
AND,0.32217898832684827,"4 and ℒ(Y∗
5) = 8"
AND,0.3229571984435798,"4. In sum, learning
Y2/Y3/Y∗
5 using inﬁnite-width CNN(p)⊗4 requires d1.25+/ d1.5+/ d2+ many samples
/SGD steps, resp. Now ℒ(Y∗
5) = 2. < 2.28 and in the MSE plot Y∗
5 (5-th row in Fig. 6),
the Blue Line does make signiﬁcant progress (the MSE is reduced to ∼0.2.)"
AND,0.3237354085603113,Under review as a conference paper at ICLR 2022
AND,0.3245136186770428,Input Node
AND,0.32529182879377433,Output Node
AND,0.32607003891050584,"L(Y1) = 0 + 1 = 1
L(Y1) = 1
2 + 1
2 = 4
4, deg(Y1)=1"
AND,0.32684824902723736,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.3276264591439689,"Ultra-Low-Frequency: L(Y1) = 3
4 + 1
4 = 4
4"
AND,0.3284046692607004,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.3291828793774319,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.3299610894941634,MSE Residual vs Training Set Size: Y1
AND,0.33073929961089493,"CNN(p)
4"
AND,0.33151750972762645,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.33229571984435796,Input Node
AND,0.3330739299610895,Output Node
AND,0.333852140077821,"L(Y2) = 0 + 2 = 2
L(Y2) = 1
2 + 2
2 = 6
4, deg(Y2)=2"
AND,0.3346303501945525,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.335408560311284,"Ultra-Short-Range-Low-Frequency: L(Y2) = 3
4 + 2
4 = 5
4"
AND,0.3361867704280156,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.3369649805447471,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.3377431906614786,MSE Residual vs Training Set Size: Y2
AND,0.33852140077821014,"CNN(p)
4"
AND,0.33929961089494165,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.34007782101167316,Input Node
AND,0.3408560311284047,Output Node
AND,0.3416342412451362,"L(Y3) = 0 + 2 = 2
L(Y3) = 1
2 + 2
2 = 6
4, deg(Y3)=2"
AND,0.3424124513618677,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.3431906614785992,"Short-Range-Low-Frequency: L(Y3) = 4
4 + 2
4 = 6
4"
AND,0.34396887159533074,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.34474708171206225,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.34552529182879377,MSE Residual vs Training Set Size: Y3
AND,0.3463035019455253,"CNN(p)
4"
AND,0.3470817120622568,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.3478599221789883,Input Node
AND,0.3486381322957198,Output Node
AND,0.34941634241245134,L(Y4) = 0 + 3 = 3
AND,0.35019455252918286,"L(Y4) = 1
2 + 3
2 = 8
4, deg(Y4)=3"
AND,0.35097276264591437,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.35175097276264594,"Short-Range-Median-Frequency: L(Y4) = 4
4 + 3
4 = 7
4"
AND,0.35252918287937746,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.35330739299610897,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.3540856031128405,MSE Residual vs Training Set Size: Y4
AND,0.354863813229572,"CNN(p)
4"
AND,0.3556420233463035,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.35642023346303503,Input Node
AND,0.35719844357976654,Output Node
AND,0.35797665369649806,"L(Y *
5) = 0 + 5 = 5
L(Y *
5) = 1
2 + 5
2 = 12
4 , deg(Y5)=5"
AND,0.3587548638132296,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.3595330739299611,"Ultra-Short-Range-Ultra-High-Frequency: L(Y *
5) = 3
4 + 5
4 = 8
4"
AND,0.3603112840466926,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.3610894941634241,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.36186770428015563,MSE Residual vs Training Set Size: Y5
AND,0.36264591439688715,"CNN(p)
4"
AND,0.36342412451361866,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.3642023346303502,Input Node
AND,0.3649805447470817,Output Node
AND,0.3657587548638132,L(Y5) = 0 + 2 = 2
AND,0.3665369649805447,"L(Y5) = 2
2 + 2
2 = 8
4, deg(Y5)=2"
AND,0.3673151750972763,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.3680933852140078,"Utlra-Long-Range-Low-Frequency: L(Y5) = 6
4 + 2
4 = 8
4"
AND,0.3688715953307393,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.36964980544747084,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.37042801556420235,MSE Residual vs Training Set Size: Y8
AND,0.37120622568093387,"CNN(p)
4"
AND,0.3719844357976654,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.3727626459143969,Input Node
AND,0.3735408560311284,Output Node
AND,0.3743190661478599,"L(Y6) = 0 + 4 = 4
L(Y6) = 1
2 + 4
2 = 10
4 , deg(Y6)=3"
AND,0.37509727626459144,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.37587548638132295,"Long-Range-High-Frequency: L(Y6) = 5
4 + 4
4 = 9
4"
AND,0.37665369649805447,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.377431906614786,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.3782101167315175,MSE Residual vs Training Set Size: Y6
AND,0.378988326848249,"CNN(p)
4"
AND,0.37976653696498053,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.38054474708171204,Input Node
AND,0.38132295719844356,Output Node
AND,0.38210116731517507,L(Y7) = 0 + 4 = 4
AND,0.38287937743190664,(a) MLP
AND,0.38365758754863816,"L(Y7) = 2
2 + 4
2 = 12
4 , deg(Y7)=4"
AND,0.38443579766536967,"Frequency: 
k = 0
Spatial: 
k = 1
2"
AND,0.3852140077821012,(b) CNN(p2)⊗2
AND,0.3859922178988327,"Utlra-Long-Range-High-Frequency: L(Y7) = 6
4 + 4
4 = 10
4"
AND,0.3867704280155642,"Spatial: 
k = 0
Spatial: 
k = 1
4"
AND,0.38754863813229573,(c) CNN(p)⊗4
AND,0.38832684824902725,"101
102
103
104
105
0.0 0.2 0.4 0.6 0.8"
AND,0.38910505836575876,MSE Residual vs Training Set Size: Y7
AND,0.3898832684824903,"CNN(p)
4"
AND,0.3906614785992218,"CNN(p2)
2 MLP
4 MLP
1"
AND,0.3914396887159533,(d) MSE Residual
AND,0.3922178988326848,"Figure 6: Eigenfunction vs Learning Index vs Architecture/DAG. Rows: eigenfunctions Yi with
various space-frequency combinations. Columns: DAGs associated to (1) a four-layer MLP; (b)
CNN(p2)⊗2, a“D""-CNN; (c) CNN(p)⊗4 a “HR""-CNN. Column (d) is the MSE, as a function of
training set size (X-axis), of the residual of the corresponding eigenfunction Yi obtained by NTK-
regression. The colored path in each DAG corresponds to the minimum spanning tree that contains
all interacting terms in the k = 0 component of Yi."
AND,0.39299610894941633,Under review as a conference paper at ICLR 2022
AND,0.39377431906614785,"0.5
1
1.5
2
log(TrainingSetSize)/log(d) 0.0 0.2 0.4 0.6 0.8 MSE"
AND,0.39455252918287936,MLP: 1 vs 4 Layers (NTK)
- LAYER,0.3953307392996109,1 - Layer
- LAYER,0.3961089494163424,4 - Layer
- LAYER,0.3968871595330739,"0.5
1
1.5
2
log(TrainingSetSize)/log(d) 0.0 0.2 0.4 0.6 0.8"
- LAYER,0.3976653696498054,CNN: 1 vs 4 Layers (NTK)
- LAYER,0.398443579766537,1 - Layer
- LAYER,0.3992217898832685,4 - Layer
- LAYER,0.4,"0.5
1
1.5
2
log(SGDSteps)/log(d) 0.0 0.2 0.4 0.6 0.8 MSE"
- LAYER,0.40077821011673154,MLP: 1 vs 4 Layers (SGD)
- LAYER,0.40155642023346305,1 - Layer
- LAYER,0.40233463035019457,4 - Layer MSE/8
- LAYER,0.4031128404669261,"0.5
1
1.5
2
log(SGDSteps)/log(d) 0.0 0.2 0.4 0.6 0.8"
- LAYER,0.4038910505836576,CNN: 1 vs 4 Layers (SGD)
- LAYER,0.4046692607003891,1 - Layer
- LAYER,0.4054474708171206,4 - Layer MSE/8
- LAYER,0.40622568093385214,"Figure 7: MLPs do not beneﬁts from having more layers. We plot the learning dynamics vs
training set size / SGD steps for each eigenfunction Yi. Top: NTK regression and bottom: SGD
+ Momentum. Left: MLP; right: CNN. Dashed lines / Solid lines correspond to one-hidden/four-
hidden layer networks. For both ﬁnite-width SGD training and inﬁnite-width kernel regression,
having more layers does not essentially improve performance of a MLP. This is in stark contrast to
CNNs (right). By having more layers, the eigenstuctures of the kernels are reﬁned."
- LAYER,0.40700389105058365,"E
FIGURE ZOO"
- LAYER,0.40778210116731517,"E.1
MLPS: DEPTH ̸= HIERARCHY"
- LAYER,0.4085603112840467,"We compare a one hidden layer MLP and a four hidden layer MLP in Fig. 7. Unlike CNNs, increas-
ing the number of layers does not improve the performance of MLPs much for both NTK regression
and SGD. This is consistent with a theoretical result from Bietti & Bach (2020), which says the
NTKs of Relu MLPs are essentially the same for any depth."
- LAYER,0.4093385214007782,"E.2
IMAGENET PLOTS"
- LAYER,0.4101167315175097,Under review as a conference paper at ICLR 2022
- LAYER,0.41089494163424123,"105
106"
- LAYER,0.41167315175097274,Training Set Size 0.2 0.4 0.6
- LAYER,0.41245136186770426,Top 1 Acc ImageNet
- LAYER,0.4132295719844358,Accuracy: GAP vs Flatten
- LAYER,0.4140077821011673,"ResNet34-GAP
ResNet34-Flatten"
- LAYER,0.41478599221789886,"105
106"
- LAYER,0.4155642023346304,Training Set Size 2 4 6
- LAYER,0.4163424124513619,Validation Cross-Entropy
- LAYER,0.4171206225680934,Loss: GAP vs Flatten
- LAYER,0.4178988326848249,"ResNet34-GAP
ResNet34-Flatten"
- LAYER,0.41867704280155643,"105
106"
- LAYER,0.41945525291828795,Training Set Size 0.2 0.4 0.6 0.8
- LAYER,0.42023346303501946,Top 1 Acc ImageNet
- LAYER,0.421011673151751,Accuracy: GAP vs Flatten
- LAYER,0.4217898832684825,"ResNet101-GAP
ResNet101-Flatten"
- LAYER,0.422568093385214,"105
106"
- LAYER,0.4233463035019455,Training Set Size 2 4 6
- LAYER,0.42412451361867703,Validation Cross-Entropy
- LAYER,0.42490272373540855,Loss: GAP vs Flatten
- LAYER,0.42568093385214006,"ResNet101-GAP
ResNet101-Flatten"
- LAYER,0.4264591439688716,"Figure 8: ResNet-GAP vs ResNet-Flatten. As the training set size increases the performance
(accuracy and loss) gap between the two shrinks substantially. Top/bottom ResNet34/ResNet101"
- LAYER,0.4272373540856031,"F
ARCHITECTURE SPECIFICATIONS"
- LAYER,0.4280155642023346,"In this section, we provide the details of the architectures used in the experiments."
- LAYER,0.4287937743190661,"G
ASSUMPTIONS"
- LAYER,0.42957198443579764,"Assumption-G. Let G = (G(d))d. There are absolute constants c, C > 0 such that"
- LAYER,0.4303501945525292,"(a.) For each non-input node u ∈N (d), there is αu ∈ΛG such that"
- LAYER,0.4311284046692607,"cdαu ≤deg(u) ≤Cdαu .
(40)"
- LAYER,0.43190661478599224,"For each edge uv ∈E(d), its weight is deﬁned to be ωuv ≡αu."
- LAYER,0.43268482490272375,"(b.) For each input node v, there are dv ∈N and 0 < αv ∈ΛG such that"
- LAYER,0.43346303501945527,"cdαv ≤dv ≤Cdαv
and
X"
- LAYER,0.4342412451361868,"v∈N (d)
0"
- LAYER,0.4350194552529183,"αv = 1.
(41)"
- LAYER,0.4357976653696498,"(c.) Let N (d)
1
≡{u : ∃v ∈N (d)
0
s.t. uv ∈E(d)} be the collection of nodes in the ﬁrst hidden
layer. We assume that for every u ∈N (d)
1
, αu = 0 and all children of u are input nodes."
- LAYER,0.4365758754863813,"(d.) For every v ∈N (d), |{u : uv ∈E(d)}| ≤C. Moreover, the number of layers is uniformly
bounded, namely, for any node u, any path from u to oG contains at most C edges."
- LAYER,0.43735408560311284,"The ﬁrst two assumptions help to create spectral gaps between eigenspaces. When d is not large,
“ﬁnite-width"" effect is no longer negligible and we expect that the spectra decay more smoothly.
Assumption (c.) says there is no “skip"" connections from the input layer to other layers except to
the ﬁrst hidden layer."
- LAYER,0.43813229571984436,Under review as a conference paper at ICLR 2022
- LAYER,0.43891050583657587,from neural_tangents import stax
- LAYER,0.4396887159533074,"def MLP(width=2048, depth=1, W_std=0.5, activation=stax.Rbf()):"
- LAYER,0.4404669260700389,layers = []
- LAYER,0.4412451361867704,for _ in range(depth):
- LAYER,0.44202334630350193,"layers += [stax.Dense(width, W_std=W_std), activation]
layers += [stax.Dense(1)]
return stax.serial(*layers)"
- LAYER,0.44280155642023344,"def CNN(width=512, ksize=4, depth=1, W_std=0.5, activation=stax.Rbf(), readout=stax.Flatten(),"
- LAYER,0.44357976653696496,"act_after_readout=True):
layers = []
conv_op = stax.Conv(width, (ksize, 1), strides=(ksize, 1), W_std=W_std,
padding='VALID')
for _ in range(depth):"
- LAYER,0.4443579766536965,"layers += [conv_op, activation]
layers += [readout]"
- LAYER,0.445136186770428,if act_after_readout:
- LAYER,0.44591439688715956,"layers += [stax.Dense(width * 4, W_std=W_std), activation]
layers += [stax.Dense(1)]
return stax.serial(*layers)"
- LAYER,0.4466926070038911,"p = 4 # input shape: (-1, p**4, 1, 1)"
- LAYER,0.4474708171206226,"S_MLP = MLP(depth=1) # Shallow MLP
D_MLP = MLP(depth=4) # Deep MLP"
- LAYER,0.4482490272373541,"S_CNN = CNN(ksize=p, depth=1, act_after_readout=False) # Shallow CNN
# One layer CNN with an additional activation-dense layer
S_CNN_plus_Act = CNN(ksize=p, depth=1, act_after_readout=True)"
- LAYER,0.4490272373540856,"D_CNN = CNN(ksize=p**2, depth=2) # Deep CNN
HR_CNN = CNN(ksize=p, depth=4) # High-resolution CNN
# High-resolution CNN with global average pooling readout
HR_CNN_GAP = CNN(ksize=p, depth=4, readout=stax.GlobalAvgPool(), act_after_readout=False)
# High-resolution CNN with flattening as readout
HR_CNN_Flatten = CNN(ksize=p, depth=4, act_after_readout=False)"
- LAYER,0.44980544747081713,"Listing 1: Deﬁnitions of MLPs, S-CNN, D-CNN and HR-CNN used in the experiments. The ar-
chitectures used in the experiments of Fig. 3 are deﬁned as follows. Dense⊗4 = D_MLP, Conv(p2)⊗2
= D_CNN, Conv(p)⊗4 = HR_CNN. The one layer MLP and one layer CNN used in Fig. 7 are
S_MLP and S_CNN_plus_Act, resp."
- LAYER,0.45058365758754865,"H
PROOF OF THE EIGENSPACE RESTRUCTURING THEOREM."
- LAYER,0.45136186770428016,"Lemma 1. Let r ∈N (d)
0
. Then"
- LAYER,0.4521400778210117,"풦G(0),
ΘG(0) = 0
(42)"
- LAYER,0.4529182879377432,"and for r ̸= 0, for d large enough,"
- LAYER,0.4536964980544747,"풦(r)
G (0),
Θ(r)
G (0) ∼d−풮(r)
and
∥풦(r)
G ∥∞, ∥Θ(r)
G ∥∞≲d−풮(r)
(43)"
- LAYER,0.4544747081712062,Under review as a conference paper at ICLR 2022
- LAYER,0.45525291828793774,Proof. Recall that the recursion formulas for 풦and Θ are
- LAYER,0.45603112840466925,"풦u(t) = φ∗
u "
- LAYER,0.45680933852140077,"v:uv∈E
풦v(t)

(44)"
- LAYER,0.4575875486381323,"Θu(t) = ˙φ∗
u "
- LAYER,0.4583657587548638,"v:uv∈E
풦v(t)
"
- LAYER,0.4591439688715953,"v:uv∈E
(풦v(t) + Θv(t))
(45)"
- LAYER,0.4599221789883268,"For convenience, deﬁne 풦, Θ, ˙풦as follows"
- LAYER,0.46070038910505834,풦u(t) =
- LAYER,0.4614785992217899,"v:uv∈E
풦v(t)
(46)"
- LAYER,0.4622568093385214,Θu(t) =
- LAYER,0.46303501945525294,"v:uv∈E
Θv(t)
(47)"
- LAYER,0.46381322957198445,"˙풦u(t) = ˙φ∗
u ◦풦u(t) .
(48)"
- LAYER,0.46459143968871597,"Note that
풦u(t) = φ∗
u ◦풦u(t)
and
Θ(t) = ˙풦u(t)(풦u(t) + Θu(t)) ."
- LAYER,0.4653696498054475,"The equalities 풦u(0) = Θu(0) = 0 follow easily from recursion and the fact φ∗
u(0) = 0 for all
u ∈N (d)."
- LAYER,0.466147859922179,"We induct on the tuple (h, |r|), where h ≥0 is the number of hidden layers in G(d) and |r| is the
total degree of r. We begin with the proof of the NNGP kernel 풦."
- LAYER,0.4669260700389105,"Base Case I: |r| = 1 and h ≥0 is any integer. Let u ∈N0 be such that r = eu, where {eu}u∈N (d)
0
is the standard basis. Then"
- LAYER,0.467704280155642,"∂tu풦G(t) =
X"
- LAYER,0.46848249027237354,path∈P(u→oG) Y
- LAYER,0.46926070038910506,"v∈path
deg(v)−1 ˙φ∗
v ◦풦v(t) ∼
X"
- LAYER,0.47003891050583657,path∈P(u→oG) Y
- LAYER,0.4708171206225681,"v∈path
d−αv ˙φ∗
v ◦풦v(t) (49)"
- LAYER,0.4715953307392996,"Here P(u →u′) represents the set of paths from u to u′. By Assumption-G and Assumption-φ,
|P(u →oG)| is uniformly bounded and ˙φ∗
v(0) > 0 for all hidden nodes v. Therefore"
- LAYER,0.4723735408560311,"∂tu풦G(0) ∼
X"
- LAYER,0.47315175097276263,"path∈P(u→oG)
d−P"
- LAYER,0.47392996108949415,"v∈path αv ˙φ∗
v ◦풦v(0) =
X"
- LAYER,0.47470817120622566,"path∈P(u→oG)
d−P"
- LAYER,0.4754863813229572,"v∈path αv ˙φ∗
v(0)"
- LAYER,0.4762645914396887,"∼
max
path∈P(u→oG) d−P"
- LAYER,0.47704280155642026,v∈path αv
- LAYER,0.4778210116731518,= d−풮(eu) = d−풮(r)
- LAYER,0.4785992217898833,"In the above, we have used 풦v(0) = 0 (which is due to φ∗(0) = 0.)
The second estimate
∥∂tu풦G∥∞≲d−풮(r) follows from | ˙φ∗
v ◦풦v(t)| ≲1."
- LAYER,0.4793774319066148,"Base Case II: h = 0 and |r| ≥1 is any number. Note that G(d) has no hidden layer and all input
nodes are linked to the output node oG. The case when the activation φoG is the identity function is
obvious and we assume φoG is admissible."
- LAYER,0.4801556420233463,"∂r
t 풦G(t) = deg(oG)−|r|φ∗
oG
(|r|)"
- LAYER,0.48093385214007783,"u∈N (d)
0
tu ! ."
- LAYER,0.48171206225680935,"This implies Eq. (43) since 풮(r) = 0, deg(oG) ≲1 by Assumption-G and φ∗
oG
(|r|)(0) > 0 by
Assumption-φ."
- LAYER,0.48249027237354086,"Induction: |r| ≥2, h ≥1 and r ∈풜(G(d)). We only prove the ﬁrst estimate in Eq. (43) since the
other one can be proved similarly. WLOG, we assume φoG is not the identity function and hence is"
- LAYER,0.4832684824902724,Under review as a conference paper at ICLR 2022
- LAYER,0.4840466926070039,"semi-admissible. Let u ∈N (d)
0
be such that ru ≥1 and denote ¯r = r −eu. Then"
- LAYER,0.4848249027237354,"∂r
t 풦G(t)

t=0 = ∂¯r
t (∂eu
t 풦G(t))

t=0 =
X"
- LAYER,0.4856031128404669,"oGv∈E
∂eu
t
풦v̸=0"
- LAYER,0.48638132295719844,"deg(oG)−1∂¯r
t

˙풦oG(t)∂eu
t 풦v(t)
 
t=0
(50) =
X"
- LAYER,0.48715953307392995,"oGv∈E
∂eu
t
풦v̸=0 X"
- LAYER,0.48793774319066147,"¯r1+¯r2=¯r
deg(oG)−1 
∂¯r1
t
˙풦oG(t) ∂¯r2+eu
t
풦v(t)
 
t=0
(51) ∼
X"
- LAYER,0.488715953307393,"oGv∈E
∂eu
t
풦v̸=0 X"
- LAYER,0.4894941634241245,"¯r1+¯r2=¯r
deg(oG)−1d−풮(¯r1)d−풮(퓃(¯r2+eu;v))
(52) ∼
X"
- LAYER,0.490272373540856,"oGv∈E
∂eu
t
풦v̸=0 X"
- LAYER,0.4910505836575875,"¯r1+¯r2=¯r
d−풮(¯r1)d−(αoG +풮(퓃(¯r2+eu;v)))
(53)"
- LAYER,0.49182879377431904,"∼
sup
oGv∈E
∂eu
t
풦v̸=0"
- LAYER,0.4926070038910506,"sup
¯r1+¯r2=¯r d−(풮(¯r1)+αoG +풮(퓃(¯r2+eu;v)))
(54)"
- LAYER,0.4933852140077821,"We have applied induction twice in Eq. (52): one to obtain the estimate ∂¯r1
t
˙풦∗
oG(0) ∼d−풮(¯r1) (with
|¯r1| < |r| and ˙φ∗
oG semi-admissible) and one to ∂¯r2+eu
t
풦v(t) ∼d−풮(퓃(¯r2+eu;v)), in which the sub-
graph with v as the output node has depth at most (h −1). The last line follows from that both the
cardinality of the tuple (¯r1, ¯r2) with ¯r1, ¯r2 ≥0 and ¯r1 + ¯r2 = ¯r and the cardinality of v ∈N (d)
0
with oGv ∈E and ∂eu
t 풦v ̸= 0 are ﬁnite and independent of d. From the deﬁnition of MST, it is
clear that for all (¯r1, ¯r2)"
- LAYER,0.49416342412451364,"풮(¯r1) + αoG + 풮(퓃(¯r2 + eu; v)) ≥풮(¯r1) + 풮(¯r2 + eu) ≥풮(r)
(55)"
- LAYER,0.49494163424124515,"It remains to show that there exists at least one pair (¯r1, ¯r2) such that the above can be an equality.
Let T ⊆G(d) be a MST containing all nodes in 퓃(r). If oG has only one child v in T , then we
choose ¯r1 = 0 and notice that"
- LAYER,0.49571984435797667,"풮(¯r1) + αoG + 풮(퓃(¯r2 + eu; v)) = 0 + 풮(r2 + eu) = 풮(r)
(56)"
- LAYER,0.4964980544747082,"since 풮(¯r1) = 0 and ¯r2 + eu = r. Else, T contains at least two children and therefore at least two
disjoint branches split from oG. Let Tu ⊆T be the branch that contains u and choose ¯r2 ≤¯r be
such that all the nodes of (¯r2 + eu) are contained in Tu and all the nodes of ¯r1 ≡r −(¯r2 + eu) are
contained in T \Tu. Clearly"
- LAYER,0.4972762645914397,"풮(r) = 풮(¯r1) + 풮(¯r2 + eu) = 풮(¯r1) + 풮(퓃(¯r2 + eu; v)) + αoG,
(57)"
- LAYER,0.4980544747081712,where v is the unique child of oG in Tu.
- LAYER,0.49883268482490273,"This completes the proof of the NNGP kernel 풦. As the proof of the NTK part is quite similar, we
will be brief and focus only on the induction step."
- LAYER,0.49961089494163424,"Induction Step of Θ: |r| ≥2, h ≥1 and r ∈풜(G(d)). Recall that the formula of Θ is"
- LAYER,0.5003891050583658,"Θu(t) = ˙φ∗
u "
- LAYER,0.5011673151750973,"v:uv∈E
풦v(t)
"
- LAYER,0.5019455252918288,"v:uv∈E
(풦v(t) + Θv(t))
(58)"
- LAYER,0.5027237354085603,"For r ∈N (d)
0
,"
- LAYER,0.5035019455252918,"∂r
t ΘoG(t) =
X"
- LAYER,0.5042801556420233,"¯r1+¯r2=r
∂¯r1
t
˙φ∗
oG "
- LAYER,0.5050583657587548,"v:oGv∈E
풦v(t)

∂¯r2
t "
- LAYER,0.5058365758754864,"v:oGv∈E
(풦v(t) + Θv(t))
(59)"
- LAYER,0.5066147859922179,"Note that ˙φ∗
oG is semi-admissible. We apply the result of 풦to conclude that"
- LAYER,0.5073929961089494,"∂¯r1
t
˙φ∗
oG "
- LAYER,0.5081712062256809,"v:oGv∈E
풦v(t)
 
t=0
∼d−풮(r1)
(60)"
- LAYER,0.5089494163424124,"∂¯r1
t
˙φ∗
oG "
- LAYER,0.5097276264591439,"v:oGv∈E
풦v(t)

∞
≲d−풮(¯r1)
(61)"
- LAYER,0.5105058365758754,Under review as a conference paper at ICLR 2022
- LAYER,0.511284046692607,and the inductive step to conclude that
- LAYER,0.5120622568093385,"∂¯r2
t "
- LAYER,0.51284046692607,"v:oGv∈E
(풦v(t) + Θv(t))"
- LAYER,0.5136186770428015,"t=0
∼
X"
- LAYER,0.514396887159533,"v:oGv∈E
∂¯
r2
t (풦v+Θv)̸≡0"
- LAYER,0.5151750972762645,"d−풮(¯r2)
if
¯r2 ̸= 0
else
0
(62)"
- LAYER,0.5159533073929962,"∂¯r2
t "
- LAYER,0.5167315175097277,"v:oGv∈E
(풦v(t) + Θv(t)) ≲
X"
- LAYER,0.5175097276264592,"v:oGv∈E
∂¯
r2
t (풦v+Θv)̸≡0"
- LAYER,0.5182879377431907,"d−풮(¯r2) .
(63)"
- LAYER,0.5190661478599222,"Note that
|{v : oGv ∈E(d) and ∂¯r2
t (풦v + Θv) ̸≡0}| ≲1 .
Thus"
- LAYER,0.5198443579766537,"∥∂r
t ΘoG(t)∥∞≲
X"
- LAYER,0.5206225680933853,"¯r1+¯r2=r
d−풮(¯r1)−풮(¯r2) ≲d−풮(r).
(64)"
- LAYER,0.5214007782101168,"To control the lower bound, let T be a MST containing 퓃(r). If deg(oG; T ) = 1, then we can choose
¯r1 = 0 and ¯r2 = r ̸= 0. Notice that there is at least one child node v of oG with ∂¯r2
t (풦v +Θv) ̸≡0.
Therefore
X"
- LAYER,0.5221789883268483,"v:∂¯
r2
t (풦v+Θv)̸≡0
d−풮(¯r2) ≳d−풮(¯r2) = d−풮(r)
(65)"
- LAYER,0.5229571984435798,Combining with
- LAYER,0.5237354085603113,"˙φ∗
oG "
- LAYER,0.5245136186770428,"v:oGv∈E
풦v(0)

= ˙φ∗
oG(0) > 0
(66)"
- LAYER,0.5252918287937743,we have
- LAYER,0.5260700389105059,"∂r
t ΘoG(0) ≳d−풮(r)"
- LAYER,0.5268482490272374,"It remains to handle the deg(oG; T ) > 1 case. We choose (¯r1, ¯r2) such that one branch of T is the
MST that contains 퓃(¯r2) and the oG and the remaining branch(es) is a MST that contains 퓃(¯r1) and
the oG. Then"
- LAYER,0.5276264591439689,"∂r
t ΘoG(0) ≳∂¯r1
t
˙φ∗
oG "
- LAYER,0.5284046692607004,"v:oGv∈E
풦v(t)

∂¯r2
t "
- LAYER,0.5291828793774319,"v:oGv∈E
(풦v(t) + Θv(t))"
- LAYER,0.5299610894941634,"t=0
≳d−풮(r1)−풮(r2) = d−풮(r)"
- LAYER,0.5307392996108949,"H.1
LEGENDRE POLYNOMIALS, SPHERICAL HARMONICS AND THEIR TENSOR PRODUCTS."
- LAYER,0.5315175097276265,"Our notation follows closely from (Frye & Efthimiou, 2012)."
- LAYER,0.532295719844358,"Legendre Polynomials.
Let din ∈N∗and ωdin be the measure deﬁned on the interval I = [−1, 1]"
- LAYER,0.5330739299610895,"ωdin(t) = (1 −t2)(din−3)/2
(67)"
- LAYER,0.533852140077821,"The Legendre polynomials6 {Pr(t) : r ∈N} is an orthogonal basis for the Hilbert space L2(I, ωdin),
i.e.
ˆ"
- LAYER,0.5346303501945525,"I
Pr(t)Pr′(t)ωdin(t)dt = 0
if
r ̸= r′
else
N(din, r)−1
|Sdin−1|"
- LAYER,0.535408560311284,|Sdin−2|
- LAYER,0.5361867704280155,"
(68)"
- LAYER,0.5369649805447471,"Here Pr(t) is a degree r polynomials with Pr(1) = 1 that satisﬁes the formula below, N(din, r) is
the cardinality of degree r spherical harmonics in Rdin and |Sdin−1| is the measure of Sdin−1."
- LAYER,0.5377431906614786,"6More accurate, this should be called Gegenbauer Polynomials. However, we decide to stick to the termi-
nology in (Frye & Efthimiou, 2012)"
- LAYER,0.5385214007782101,Under review as a conference paper at ICLR 2022
- LAYER,0.5392996108949416,"Lemma 2 (Rodrigues Formula. Proposition 4.19 (Frye & Efthimiou, 2012))."
- LAYER,0.5400778210116731,"Pr(t) = crω−1
din (t)
 d dt"
- LAYER,0.5408560311284046,"r
(1 −t2)r+(din−3)/2 ,
(69) where"
- LAYER,0.5416342412451361,"cr =
(−1)r"
- LAYER,0.5424124513618677,"2r(r + (din −3)/2)r
(70)"
- LAYER,0.5431906614785992,"In the above lemma, (x)l denotes the falling factorial"
- LAYER,0.5439688715953307,"(x)l ≡x(x −1) · · · (x −l + 1)
(71)
(x)0 ≡1
(72)"
- LAYER,0.5447470817120622,"Spherical Harmonics.
Let dSdin−1 deﬁne the (un-normalized) uniform measure on the unit sphere
Sdin−1. Then"
- LAYER,0.5455252918287937,"|Sdin−1| ≡
ˆ"
- LAYER,0.5463035019455252,"Sdin−1
dSdin−1 = 2πdin/2"
- LAYER,0.5470817120622569,Γ( din
- LAYER,0.5478599221789884,"2 ) .
(73)"
- LAYER,0.5486381322957199,The normalized measure on this sphere is deﬁned to be
- LAYER,0.5494163424124514,"dσdin =
1
|Sdin−1|dSdin−1
and
ˆ"
- LAYER,0.5501945525291829,"Sdin−1
dσdin = 1 .
(74)"
- LAYER,0.5509727626459144,"The spherical harmonics {Yr,l}r,l in Rdin are homogeneous harmonic polynomials that form an
orthonormal basis in L2(Sdin−1, σdin)
ˆ"
- LAYER,0.551750972762646,"ξ∈Sdin−1
Yr,l(ξ)Yr′,l′(ξ)dσdin = δ(r,l)=(r′,l′) .
(75)"
- LAYER,0.5525291828793775,"Here Yr,l denotes the l-th spherical harmonic whose degree is r, where r ∈N, l ∈[N(din, r)] and"
- LAYER,0.553307392996109,"N(din, r) = 2r + din −2 r"
- LAYER,0.5540856031128405,"din + r −3
r −1"
- LAYER,0.554863813229572,"
∼(din)r/r!
as
din →∞.
(76)"
- LAYER,0.5556420233463035,"The Legendre polynomials and spherical harmonics are related through the addition theorem.
Lemma 3 (Addition Theorem. Theorem 4.11 (Frye & Efthimiou, 2012))."
- LAYER,0.556420233463035,"Pr(ξT η) =
1
N(din, r) X"
- LAYER,0.5571984435797666,"l∈[N(din,r)]
Yr,l(ξ)Yr,l(η),
ξ, η ∈Sdin−1 .
(77)"
- LAYER,0.5579766536964981,"Tensor Products.
Let p = |N (d)
0
|, d = (du)|u∈N (d)
0
|, r ∈N|N (d)
0
| ≃Np, I|N (d)
0
| ≃Ip = [−1, 1]p"
- LAYER,0.5587548638132296,and ω = N
- LAYER,0.5595330739299611,"u∈N (d)
0
ωp
du be the product measure on Ip. Then the (product of) Legendre polynomials"
- LAYER,0.5603112840466926,"Pr(t) =
Y"
- LAYER,0.5610894941634241,"u∈N (d)
0"
- LAYER,0.5618677042801556,"Pru(tu) ,
t = (tu)u∈N (d)
0
∈Ip ,
(78)"
- LAYER,0.5626459143968872,"which form an orthogonal basis for the Hilbert space L2(Ip, ω) = N"
- LAYER,0.5634241245136187,"u∈N (d)
0
L2(I, ωdu). Similarly,
the tensor product of spherical harmonics"
- LAYER,0.5642023346303502,"Yr,l =
Y"
- LAYER,0.5649805447470817,"u∈N (d)
0"
- LAYER,0.5657587548638132,"Yru,lu,
l = (lu)u∈N (d)
0
∈[N(d, r)] ≡
Y"
- LAYER,0.5665369649805447,"u∈N (d)
0"
- LAYER,0.5673151750972762,"[N(du, r)]
(79)"
- LAYER,0.5680933852140078,form an orthonormal basis for the product space
- LAYER,0.5688715953307393,"L2(X, σ) ≡
O"
- LAYER,0.5696498054474708,"u∈N (d)
0"
- LAYER,0.5704280155642023,"L2(Sdu−1, σdu)
(80)"
- LAYER,0.5712062256809338,"Elements in the set {Yr,l}l∈[N(d,r)] are called degree (order) r spherical harmonics in L2(X, σ)
and also degree r spherical harmonics if |r| = r ∈N."
- LAYER,0.5719844357976653,Under review as a conference paper at ICLR 2022
- LAYER,0.5727626459143969,"Theorem 5. We have the following, for K = 풦G(d) or K = ΘG(d)"
- LAYER,0.5735408560311284,"K(t) =
X"
- LAYER,0.5743190661478599,"r∈NN (d)
0"
- LAYER,0.5750972762645914,"ˆK(r)Pr(t)
with
ˆK(r) ∼d−풮(r)
if
r ̸= 0
else
0.
(81)"
- LAYER,0.5758754863813229,Note that Theorem 1 follows from this theorem and the addition theorem.
- LAYER,0.5766536964980544,"Proof of Theorem 1. Assume r ̸= 0. Indeed, setting"
- LAYER,0.5774319066147859,"ξ = (ξu)u∈N (d)
0
∈X,
η = (ηu)u∈N (d)
0
∈X and
t = (tu)u∈N (d)
0
= (ξT
u ηu/du)u∈N (d)
0
,"
- LAYER,0.5782101167315176,we have
- LAYER,0.5789883268482491,"Pr(t) =
Y"
- LAYER,0.5797665369649806,"u∈N (d)
0"
- LAYER,0.5805447470817121,"Pru(tu) =
Y"
- LAYER,0.5813229571984436,"u∈N (d)
0"
- LAYER,0.5821011673151751,"N(du, ru)−1
X"
- LAYER,0.5828793774319067,"lu∈N(du,ru)
Yru,lu(ξu/
p"
- LAYER,0.5836575875486382,"du)Yru,lu(ηu/
p du) (82)"
- LAYER,0.5844357976653697,"= N(d, r)−1
X"
- LAYER,0.5852140077821012,"l∈[N(d,r)]"
- LAYER,0.5859922178988327,"Y r,l(ξ)Y r,l(η) .
(83)"
- LAYER,0.5867704280155642,Then Theorem 1 follows by noticing
- LAYER,0.5875486381322957,"K(r)N(d, r)−1 ∼d−풮(r)d
−P"
- LAYER,0.5883268482490273,"u∈N (d)
0
ruαu = d−ℒ(r)
(84)"
- LAYER,0.5891050583657588,"Proof of Theorem 5. From the orthogonality,"
- LAYER,0.5898832684824903,"ˆK(r) = ⟨K, Pr⟩/∥Pr∥L2(Ip,ω)2
(85)"
- LAYER,0.5906614785992218,We begin with the denominator. Note that
- LAYER,0.5914396887159533,"∥Pr∥L2(Ip,σ)2 =
Y"
- LAYER,0.5922178988326848,"u∈N (d)
0"
- LAYER,0.5929961089494163,"∥Pru∥2
L2(I,ωdu) = N(d; r)−1
Y"
- LAYER,0.5937743190661479,"u∈N (d)
0"
- LAYER,0.5945525291828794,"(|Sdu−1|/|Sdu−2|)
(86)"
- LAYER,0.5953307392996109,"By applying Lemma 2, integration by parts and continuity of K(r) on the boundary ∂Ip"
- LAYER,0.5961089494163424,"⟨K, Pr⟩L2(Ip,ω) = cr ˆ"
- LAYER,0.5968871595330739,"Ip K(t)
 d dt"
- LAYER,0.5976653696498054,"r  
1 −t2r+(d−3)/2 dt
(87)"
- LAYER,0.598443579766537,= (−1)rcr ˆ
- LAYER,0.5992217898832685,"Ip K(r)(t)
 
1 −t2r+(d−3)/2 dt
(88)"
- LAYER,0.6,"= (−1)rcr (M(K, d) + ϵ(K, d))
(89)"
- LAYER,0.6007782101167315,"where K(r) is the r derivative of K, the coefﬁcient cr is given by Lemma 2"
- LAYER,0.601556420233463,"cr =
Y"
- LAYER,0.6023346303501945,"u∈N (d)
0"
- LAYER,0.603112840466926,"cru =
Y"
- LAYER,0.6038910505836576,"u∈N (d)
0"
- LAYER,0.6046692607003891,(−1)ru
- LAYER,0.6054474708171206,"2ru(ru + (d −3)/2)ru
∼
Y"
- LAYER,0.6062256809338521,"u∈N (d)
0"
- LAYER,0.6070038910505836,"(−1)rud−ru
u
= (−1)rd−r
(90)"
- LAYER,0.6077821011673151,and the major and error terms are given by
- LAYER,0.6085603112840466,"M(K, d) ≡K(r)(0)
ˆ Ip"
- LAYER,0.6093385214007782," 
1 −t2r+(d−3)/2 dt = K(r)(0)
Y"
- LAYER,0.6101167315175098,"u∈N (d)
0"
- LAYER,0.6108949416342413,"|S2ru+du−1|
|S2ru+du−2|
(91)"
- LAYER,0.6116731517509728,"ϵ(K, d) ≡
ˆ"
- LAYER,0.6124513618677043,"Ip(K(r)(t) −K(r)(0))
 
1 −t2r+(d−3)/2 dt
(92)"
- LAYER,0.6132295719844358,The mean value theorem gives
- LAYER,0.6140077821011674,"|K(r)(t) −K(r)(0)| ≤
X"
- LAYER,0.6147859922178989,"u∈N (d)
0"
- LAYER,0.6155642023346304,"∥K(r+eu)∥L∞(Ip)|tu|
(93)"
- LAYER,0.6163424124513619,Under review as a conference paper at ICLR 2022
- LAYER,0.6171206225680934,"and the error term |ϵ(K, d)| is bounded above by ˆ Ip"
- LAYER,0.6178988326848249," 
1 −t2r+(d−3)/2 dt
X"
- LAYER,0.6186770428015564,"u∈N (d)
0"
- LAYER,0.619455252918288,∥K(r+eu)∥L∞(Ip) ´
- LAYER,0.6202334630350195,"I |tu|
 
1 −t2
u
ru+(du−3)/2 dtu
´"
- LAYER,0.621011673151751,"I (1 −t2u)ru+(ru−3)/2 dtu ! (94) ∼
Y"
- LAYER,0.6217898832684825,"u∈N (d)
0"
- LAYER,0.622568093385214,"|S2ru+du−1|
|S2ru+du−2| X"
- LAYER,0.6233463035019455,"u∈N (d)
0"
- LAYER,0.624124513618677,"∥K(r+eu)∥L∞(Ip)d−1
u"
- LAYER,0.6249027237354086,|S2ru+du−1|
- LAYER,0.6256809338521401,|S2ru+du−2|
- LAYER,0.6264591439688716,"−1
.
(95)"
- LAYER,0.6272373540856031,"Since for any α ∈N, as du →∞,"
- LAYER,0.6280155642023346,"|Sα+du−1|
|Sα+du−2| = π
1
2 Γ((α + du −1)/2)/Γ((α + du)/2) ∼π
1
2 (du/2)−1"
- LAYER,0.6287937743190661,2 ∼(du)−1
- LAYER,0.6295719844357976,"2 ,
(96)"
- LAYER,0.6303501945525292,we have
- LAYER,0.6311284046692607,"|ϵ(K, d)| ≲
X"
- LAYER,0.6319066147859922,"u∈N (d)
0"
- LAYER,0.6326848249027237,"∥K(r+eu)∥L∞(Ip)d
−1"
"U
Y",0.6334630350194552,"2
u
Y"
"U
Y",0.6342412451361867,"u∈N (d)
0"
"U
Y",0.6350194552529183,"|S2ru+du−1|
|S2ru+du−2| .
(97)"
"U
Y",0.6357976653696498,"We claim that (which will be proved later)
X"
"U
Y",0.6365758754863813,"u∈N (d)
0"
"U
Y",0.6373540856031128,"∥K(r+eu)∥L∞(Ip) ≲d−풮(r)
(98)"
"U
Y",0.6381322957198443,which implies
"U
Y",0.6389105058365758,"⟨K, Pr⟩L2(Ip,ωp
din) = cr "
"U
Y",0.6396887159533073,K(r)(0) + O 
"U
Y",0.6404669260700389,"d−풮(r)( min
u∈N (d)
0
du)−1 2 !!
Y"
"U
Y",0.6412451361867705,"u∈N (d)
0"
"U
Y",0.642023346303502,"|S2ru+du−1|
|S2ru+du−2|
(99)"
"U
Y",0.6428015564202335,"Plugging back to Eq. (85), we have"
"U
Y",0.643579766536965,"ˆK(r) = (−1)rcrN(d, r) "
"U
Y",0.6443579766536965,K(r)(0) + O 
"U
Y",0.645136186770428,"d−풮(r)( min
u∈N (d)
0
du)−1 2 !!  

Y"
"U
Y",0.6459143968871596,"u∈N (d)
0"
"U
Y",0.6466926070038911,"|S2ru+du−1|
|S2ru+du−2|"
"U
Y",0.6474708171206226,|Sdu−1|
"U
Y",0.6482490272373541,"|Sdu−2| −1
 
 (100)"
"U
Y",0.6490272373540856,"Since, for r ﬁxed and as du →∞for all u ∈N (d)
0"
"U
Y",0.6498054474708171,"cr
(−1)rd−r →1
and
N(d, r)"
"U
Y",0.6505836575875487,"dr/r!
→1
and  

Y"
"U
Y",0.6513618677042802,"u∈N (d)
0"
"U
Y",0.6521400778210117,"|S2ru+du−1|
|S2ru+du−2|"
"U
Y",0.6529182879377432,|Sdu−1|
"U
Y",0.6536964980544747,"|Sdu−2| −1
 
→1 (101)"
"U
Y",0.6544747081712062,and thus
"U
Y",0.6552529182879377,ˆK(r) ∼r!−1
"U
Y",0.6560311284046693,K(r)(0) + O 
"U
Y",0.6568093385214008,"d−풮(r)( min
u∈N (d)
0
du)−1 2 !! (102)"
"U
Y",0.6575875486381323,"It remains to verify Eq. (98). By Lemma 1, we only need to show that
X"
"U
Y",0.6583657587548638,"u∈N (d)
0"
"U
Y",0.6591439688715953,"d−풮(r+eu) ≲d−풮(r)
(103)"
"U
Y",0.6599221789883268,"We prove this by induction on the number of hidden layers of G(d). The base case is obvious. Now
suppose the depth of G(d) is h. Let C(r) be the set of children of oG that are ancestors of at least one
node of 퓃(r). We split N (d)
0
into two disjoint sets"
"U
Y",0.6607003891050583,"Q(r) ≡{u ∈N (d)
0
: ∃v ∈C(r) s.t. P(u →v) ̸= ∅}
and
N (d)
0
\Q0(r) ."
"U
Y",0.6614785992217899,Under review as a conference paper at ICLR 2022
"U
Y",0.6622568093385214,"For u /∈Q(r), we have 풮(r + eu) = 풮(r) + 풮(eu) and hence
X"
"U
Y",0.6630350194552529,"u/∈Q(r)
d−풮(r+eu) =
X"
"U
Y",0.6638132295719844,"u/∈Q(r)
d−풮(r)−풮(eu) = d−풮(r)
X"
"U
Y",0.6645914396887159,"u/∈Q(r)
d−풮(eu) ≲d−풮(r) .
(104)"
"U
Y",0.6653696498054474,"In the last inequality above, we have used
X"
"U
Y",0.666147859922179,"u/∈Q(r)
d−풮(eu) ≤
X"
"U
Y",0.6669260700389105,"u∈N (d)
0"
"U
Y",0.667704280155642,"d−풮(eu) ∼1 .
(105)"
"U
Y",0.6684824902723735,"To estimate the remaining, we use induction. Note that |C(r)| is ﬁnite and independent of d. Then
X"
"U
Y",0.669260700389105,"u∈Q(r)
d−풮(r+eu) ≤
X"
"U
Y",0.6700389105058365,v∈C(r) X
"U
Y",0.670817120622568,"u∈Q(r)
d−αoG −풮(퓃(r+eu;v))
(106)"
"U
Y",0.6715953307392996,"= d−αoG
X"
"U
Y",0.6723735408560312,v∈C(r) X
"U
Y",0.6731517509727627,"u∈Q(r)
d−풮(퓃(r+eu;v))
(107)"
"U
Y",0.6739299610894942,"≲|C(r)|d−αoG max
v∈C(r) d−풮(퓃(r;v)) ∼d−풮(r)
(108)"
"U
Y",0.6747081712062257,We have used induction on the sub-graph with v as the output node.
"U
Y",0.6754863813229572,"I
PROOF OF THEOREM 2"
"U
Y",0.6762645914396888,"Let G(d) be a DAG associated to the convolutional networks whose ﬁlter sizes in the l-th layer is
kl = [dαl], for 0 ≤l ≤L+1, in which we treat the ﬂatten-dense readout layer as a convolution with
ﬁlter size [dαL+1]. Note that we have set αp = α0 and αw = αL+1. We also assume an activation
layer after the ﬂatten-dense layer, which does not essentially alter the topology of the DAG."
"U
Y",0.6770428015564203,We need the following dimension counting lemma.
"U
Y",0.6778210116731518,Lemma 4. Let r ∈L(G(d)). Then
"U
Y",0.6785992217898833,"dim
 
span
"
"U
Y",0.6793774319066148,"Y r,l :
L(r) = r, l ∈N(d, r)
	
∼dr
(109)"
"U
Y",0.6801556420233463,"To prove Theorem 2, we only need to verify the assumptions of Theorem 4 in Mei et al. (2021a).
For convenience, we brieﬂy recap the assumptions and results from Mei et al. (2021a) in Sec.L."
"U
Y",0.6809338521400778,"It is convenient to group the eigenspaces together according to the learning indices L(G(d)). Recall
that L(G(d)) = (r1 ≤r2 ≤r3 . . . ). Let"
"U
Y",0.6817120622568094,"Ei = span{Y r,l : L(r) = ri}
(110)"
"U
Y",0.6824902723735409,"Then by Theorem 1 and Lemma 4,"
"U
Y",0.6832684824902724,"dim(Ei) ∼dri
and
λ(g) ∼d−ri
∀g ∈Ei, g ̸= 0 ,
(111)"
"U
Y",0.6840466926070039,"where λ(g) denote the eigenvalue of g. We proceed to verify Assumptions 4 and 5 in Sec. L.
They follow directly from Theorem 1, Lemma 4 and the hypercontractivity of spherical harmonics
Beckner (1992)."
"U
Y",0.6848249027237354,"I.1
VERIFYING Assumption 4"
"U
Y",0.6856031128404669,We need the following.
"U
Y",0.6863813229571984,"Proposition 1. For 0 < s ∈R, let Ds = span{Y r,l : |L(r)| < s}. Then for f ∈Ds,"
"U
Y",0.68715953307393,"∥f∥2
q ≤(q −1)s/α0∥f∥2
2
(112)"
"U
Y",0.6879377431906615,"Proof of Proposition 1. The lemma follows from the tensorization of hypercontractivity. Let f =
P"
"U
Y",0.688715953307393,"k≥0 Yk ∈L2(Sn) where Yk is a degree k spherical harmonics in Sn. Deﬁne the Poisson semi-
group operator"
"U
Y",0.6894941634241245,"Pϵf(x) =
X"
"U
Y",0.690272373540856,"k≥0
ϵkYk(x)
(113)"
"U
Y",0.6910505836575875,Under review as a conference paper at ICLR 2022
"U
Y",0.691828793774319,"Then we have the hypercontractivity inequality (Beckner, 1992), for 1 ≤p ≤q and ϵ ≤
q"
"U
Y",0.6926070038910506,"p−1
q−1"
"U
Y",0.6933852140077821,"∥Pϵf∥Lq(Sn) ≤∥f∥Lp(Sn)
(114)"
"U
Y",0.6941634241245136,"One can then tensorize (Beckner, 1975) it to obtain the same bound in the tensor space."
"U
Y",0.6949416342412451,"Lemma 5 (Corollary 11 Montanaro (2012)). Let f : (Sn)k →R. If 1 ≤p ≤q and ϵ ≤
q"
"U
Y",0.6957198443579766,"p−1
q−1,
then"
"U
Y",0.6964980544747081,"∥P ⊗k
ϵ
f∥Lq((Sn)k) ≤∥f∥Lp((Sn)k) .
(115)"
"U
Y",0.6972762645914397,Let f = P
"U
Y",0.6980544747081712,"r,l ar,lY r,l ∈Ds. Choosing ϵ =
q"
"U
Y",0.6988326848249027,"1
q−1 and p = 2 in the above lemma, we have"
"U
Y",0.6996108949416342,"∥f∥2
q = ∥
X"
"U
Y",0.7003891050583657,"r,l
ar,lY r,l∥2
q
(116)"
"U
Y",0.7011673151750972,"= ∥P
⊗|N (d)
0
|
ϵ
X"
"U
Y",0.7019455252918287,"r,l
ar,lϵ−rY r,l∥2
q
(117) ≤∥
X"
"U
Y",0.7027237354085603,"r,l
ar,lϵ−rY r,l∥2
2
(118) =
X"
"U
Y",0.7035019455252919,"r,l
a2
r,lϵ−2r∥Y r,l∥2
2
(119)"
"U
Y",0.7042801556420234,≤ϵ−2 max |r| X
"U
Y",0.7050583657587549,"r,l
a2
r,l∥Y r,l∥2
2
(120)"
"U
Y",0.7058365758754864,"= (q −1)max |r|∥f∥2
2 ≤(q −1)s/α0∥f∥2
2
(121)"
"U
Y",0.7066147859922179,"Since r /∈ℒ(G(d)), there is a j such that rj < r < rj+1. Let n(d) = dr and"
"U
Y",0.7073929961089495,"m(d) = dim
 
span{Y r,l : ℒ(r) ≤rj}

= dim(span
["
"U
Y",0.708171206225681,"i≤j
Ei)
(122)"
"U
Y",0.7089494163424125,"Clearly, m(d) ∼drj. We list all eigenvalues of K in non-ascending order as {λd,i}. In particular,
we have"
"U
Y",0.709727626459144,"λd,m(d) ∼d−rj > d−r > d−rj+1 ∼λd,m(d)+1 .
(123)"
"U
Y",0.7105058365758755,Assumption 4 (a). We choose u(d) to be
"U
Y",0.711284046692607,u(d) = dim 
"U
Y",0.7120622568093385,"span
["
"U
Y",0.7128404669260701,"i:ri≤2r+100
Ei "
"U
Y",0.7136186770428016,".
(124)"
"U
Y",0.7143968871595331,Assumption 4 (a) follows from Proposition 1.
"U
Y",0.7151750972762646,"Assumptions 4 (b).
Let s = inf{¯r ∈ℒ(G(d)) : ¯r > 2r + 100}. For l > 1, we have
X"
"U
Y",0.7159533073929961,"j=u(d)+1
λl
d,j ∼
X"
"U
Y",0.7167315175097276,"ri:ri≥s
(d−ri)l dim(Ei) ∼d−s(l−1)
(125)"
"U
Y",0.7175097276264591,"which also holds for l = 1 since
X"
"U
Y",0.7182879377431907,"j=u(d)+1
λd,j ∼1
(126)"
"U
Y",0.7190661478599222,Under review as a conference paper at ICLR 2022
"U
Y",0.7198443579766537,"Thus
(P"
"U
Y",0.7206225680933852,"j=u(d)+1 λl
d,j)2
P"
"U
Y",0.7214007782101167,"j=u(d)+1 λ2l
d,j
∼d−2s(l−1)"
"U
Y",0.7221789883268482,"d−s(2l−1) = ds > d2r+100 > n(d)2+δ ∼d(2+δ)r.
(127)"
"U
Y",0.7229571984435798,as long as δ < 100/r.
"U
Y",0.7237354085603113,Assumption 4 (c). Denote
"U
Y",0.7245136186770428,"Kd,>m(d)(ξ, η) =
X"
"U
Y",0.7252918287937743,"r,l:L(r)>r
λK(r)Y r,l(ξ)Y r,l(η)
(128)"
"U
Y",0.7260700389105058,"We have
EηKd,>m(d)(ξ, η)2 =Eη
X"
"U
Y",0.7268482490272373,"r,l:L(r)>r
λK(r)2|Y r,l(ξ)Y r,l(η)|2
(129) =
X"
"U
Y",0.7276264591439688,"r,l:L(r)>r
λK(r)2|Y r,l(ξ)|2
(130) =
X"
"U
Y",0.7284046692607004,"r:L(r)>r
λK(r)2 X"
"U
Y",0.7291828793774319,"l
|Y r,l(ξ)|2
(131) =
X"
"U
Y",0.7299610894941634,"r:L(r)>r
λK(r)2N(d, r)Pr(1) =
X"
"U
Y",0.7307392996108949,"r:L(r)>r
λK(r)2N(d, r) .
(132)"
"U
Y",0.7315175097276264,"Similarly,"
"U
Y",0.7322957198443579,"Eξ,η
X"
"U
Y",0.7330739299610894,"r,l:L(r)>r
λK(r)2|Y r,l(ξ)Y r,l(η)|2 =
X"
"U
Y",0.733852140077821,"r:L(r)>r
λK(r)2N(d, r) .
(133)"
"U
Y",0.7346303501945526,"Thus
Eη
X"
"U
Y",0.7354085603112841,"r,l:L(r)>r
λK(r)2|Y r,l(ξ)Y r,l(η)|2 −Eξ,η
X"
"U
Y",0.7361867704280156,"r,l:L(r)>r
λK(r)2|Y r,l(ξ)Y r,l(η)|2 = 0 . (134)"
"U
Y",0.7369649805447471,"For the diagonal terms,"
"U
Y",0.7377431906614786,"Kd,>m(d)(ξ, ξ) =
X"
"U
Y",0.7385214007782102,"r,l:L(r)>r
λK(r)|Y r,l(ξ)|2 =
X"
"U
Y",0.7392996108949417,"r:L(r)>r
λK(r)N(d, r) = EξKd,>m(d)(ξ, ξ)"
"U
Y",0.7400778210116732,"(135)
which is deterministic."
"U
Y",0.7408560311284047,"I.2
VERIFYING Assumption 5."
"U
Y",0.7416342412451362,"Recall that n(d) ∼dr and rj < r < rj+1 . Assumption 5(a) follows from Eq. (111). Indeed, for
l > 1
λ−l
d,m(d)+1
X"
"U
Y",0.7424124513618677,"k=m(d)+1
λl
d,k ∼(d−rj+1)−l
X"
"U
Y",0.7431906614785992,"i:ri≥rj+1
dim(Ei)d−lri
(136)"
"U
Y",0.7439688715953308,"∼dlrj+1
X"
"U
Y",0.7447470817120623,"i:ri≥rj+1
drid−lri
(137)"
"U
Y",0.7455252918287938,"=drj+1 > n(d)1+δ
(138)
for some δ > 0 since r < rj+1. Similarly, for l = 1, since
X"
"U
Y",0.7463035019455253,"k=m(d)+1
λd,k ∼1
(139)"
"U
Y",0.7470817120622568,"we have
(d−rj+1)−1
X"
"U
Y",0.7478599221789883,"k=m(d)+1
λd,k ∼drj+1 > n(d)1+δ .
(140)"
"U
Y",0.7486381322957198,"Assumption 5(b) follows from r > rj.
Assumption 5(c). Note that"
"U
Y",0.7494163424124514,"λ−1
d,m(d)
X"
"U
Y",0.7501945525291829,"k=m(d)+1
λd,k ∼λ−1
d,m(d) ∼drj < n(d)(1−δ) ∼dr(1−δ)
(141)"
"U
Y",0.7509727626459144,for some δ > 0 since rj < r.
"U
Y",0.7517509727626459,Under review as a conference paper at ICLR 2022
"U
Y",0.7525291828793774,"J
PROOF OF LEMMA 4"
"U
Y",0.7533073929961089,"Proof. The lemma can be proved by induction. Base case: L = 0. The network is a S-CNN.
WLOG, assume α0 ̸= 0 and α1 ̸= 0. For r ∈L(G(d)), we know that r can be written as a
combination of α0 and α1, i.e. r = k0α0 + k1α1 for some k0, k1 ≥0. We say a tuple (k0, k1) is
r-feasible if in addition, there exists r with 풮(r) = k1α1 and ℱ(r) = k0α0. Consider the set of all
r-feasible tuple"
"U
Y",0.7540856031128405,"F(r) ≡{(k0, k1) : r-feasible} .
(142)"
"U
Y",0.754863813229572,"Clearly, F(r) is ﬁnite. It sufﬁces to prove that for each r-feasible tuple (k0, k1),"
"U
Y",0.7556420233463035,"dim
 
span
"
"U
Y",0.756420233463035,"Y r,l : ℱ(r) = k0α0
풮(r) = k1α1, l ∈N(d, r)
	
∼dr
(143)"
"U
Y",0.7571984435797665,"Note that there are ∼(dα1)k1 many ways to choose k1 nodes in the penultimate layer. Then the
dimension of the above set is about"
"U
Y",0.757976653696498,"(dα1)k1
X"
"U
Y",0.7587548638132295,"(k0,1,...,k0,k1)
k0,1+···+k0,k1=k0 k1
Y"
"U
Y",0.7595330739299611,"j=1
N(dα0, k0,j) ∼(dα1)k1
X"
"U
Y",0.7603112840466926,"(k0,1,...,k0,k1)
k0,1+···+k0,k1=k0 k1
Y"
"U
Y",0.7610894941634241,"j=1
(dα0)k0,j
(144)"
"U
Y",0.7618677042801556,"∼dk0α0+k1α1 = dr ,
(145)"
"U
Y",0.7626459143968871,since the cardinality of the set
"U
Y",0.7634241245136186,"{(k0,1, . . . , k0,k1) : k0,1 + · · · + k0,k1 = k0,
k0,j ≥1
k0,j ∈N}"
"U
Y",0.7642023346303501,is ﬁnite.
"U
Y",0.7649805447470817,"Induction step: L ≥1. For r with L(r) = r, let k be the number of children of a MST of
퓃(r; oG). Clearly, k ∈[1, [r/αL+1]]. Then we can classify Y r,l into at most [r/αL+1] bins:
{Ωk}k=1,...,[r/αL+1] depending on the number of children of oG in a MST. Let Ωk be non-empty.
We only need to prove the number of Y r,l in Ωk is dr. Note that there are ∼(dαL+1)k many ways
to choose k children from oG. Let {uj}j=1,...,k be one ﬁxed choice and {Gj} be the subgraphs with
{uj} as the output nodes. Next, we partition (r −kαL+1) into k components,"
"U
Y",0.7657587548638133,r −kαL+1 = r1 + · · · + rk
"U
Y",0.7665369649805448,"so that each rj is a combination of {αj}0≤j≤L. The cardinality of such partition is also ﬁnite. We ﬁx
one of such partition (r1, . . . , rk) so that each rj is a learning index of Gj. We can apply induction to
each (Gj, rj) to conclude that the cardinality of Y rj,lj with LGj(rj) = rj is ∼drj, where LGj(rj)
is the learning index of rj of Gj. Therefore, we have"
"U
Y",0.7673151750972763,"dim
 
span
"
"U
Y",0.7680933852140078,"Y r,l :
L(r) = r, l ∈N(d, r)
	
∼(dαL+1)k Y"
"U
Y",0.7688715953307393,"j∈[k]
drj = dr .
(146)"
"U
Y",0.7696498054474709,"K
CNN-GAP: CNNS WITH GLOBAL AVERAGE POOLING"
"U
Y",0.7704280155642024,"Consider convolutional networks whose readout layer is a global average pooling (GAP) and a
ﬂattening layer (namely, without pooling), resp."
"U
Y",0.7712062256809339,"CNN + GAP:
[Conv(p)-Act] →[Conv(k)-Act]⊗L →[GAP] →[Dense]
(147)"
"U
Y",0.7719844357976654,"CNN:
[Conv(p)-Act] →[Conv(k)-Act]⊗L →[Flatten] →[Dense]
(148)"
"U
Y",0.7727626459143969,"Concretely, the input space is X = (Sp−1)kL×w ⊆Rp×1×kL×w, where p is the patch size of the
input convolutional layer, k is the ﬁlter size in hidden layers, L is the number of hidden convolution
layers and w is the spatial dimension of the penultimate layer. The total dimension of the input
is d = p · kL · w, and the number of input nodes is |N0| = kL · w. Since the stride is equal to
the ﬁlter size for all convolutional layers, the spatial dimension is reduced by a factor of p in the
ﬁrst layer, a factor of k by each hidden layer. The penultimate layer (before pooling/ﬂattening)"
"U
Y",0.7735408560311284,Under review as a conference paper at ICLR 2022
"U
Y",0.77431906614786,"has spatial dimension w and is reduced to 1 by the GAP-dense layer or the Flatten-dense layer.
The DAGs associated to these two architectures are identical which is denoted by G. However,
the kernel/neural network computations are slightly different. If the penultimate layer has n many
channels and fpen : X →Rn×w is the mapping from the input layer to the penultimate layer, then
the outputs of the CNN-GAP and CNN-Flatten are"
"U
Y",0.7750972762645915,fCNN-GAP(x) = n−1
X,0.775875486381323,2 X
X,0.7766536964980545,"j∈[n]
ωj "
X,0.777431906614786,"i∈[w]
fpen(x)j,i
(149)"
X,0.7782101167315175,fCNN-Flatten(x) = (nw)−1
X,0.778988326848249,"2
X"
X,0.7797665369649806,"j∈[n],i∈[w]
ωjifpen(x)j,i ,
(150)"
X,0.7805447470817121,"resp., where wj and wji are parameters of the last layer and are usually initialized with standard iid
Guassian wj, wji ∼N(0, 1). Let N−1 ⊆N be the nodes in the penultimate layer, then |N−1| = w.
Let ξ = (ξv)v∈N−1 ∈X, where ξv ∈(Sp−1)kL. Thus, each ξv contains kL many input nodes
{ξu,i}i∈[kL]. Deﬁne"
X,0.7813229571984436,"tuv = (⟨ξu,i, ηv,i⟩/p)i∈[kL] ∈[−1, 1]kL.
(151)"
X,0.7821011673151751,Recall that in the case without pooling
X,0.7828793774319066,풦u(t) = φ∗(
X,0.7836575875486381,"uv∈E
풦v(t)),
풦G ="
X,0.7844357976653696,"oGv∈E
풦v(t) ="
X,0.7852140077821012,"v∈N−1
풦v(t)
(152)"
X,0.7859922178988327,"where t ∈[−1, 1]kL×w, which is usually obtained by t = (⟨ξu,i, ηu,i⟩/p)u∈N−1,i∈[kL]. Indeed, for
each v ∈N−1, 풦v depends only on the diagonal terms tvv = (⟨ξv,i, ηv,i⟩/p)i∈[kL] ∈[−1, 1]kL.
We can ﬁnd a function"
X,0.7867704280155642,"풦pen : [−1, 1]kL →[−1, 1]
s.t.
풦v(t) = 풦pen(tvv)
∀v ∈N−1
(153)"
X,0.7875486381322957,"Therefore, without pooling the NNGP kernel is"
X,0.7883268482490272,풦CNN(t) =
X,0.7891050583657587,"v∈N−1
풦pen(tvv) = 1 w X"
X,0.7898832684824902,"v∈N−1
풦pen(tvv)
(154)"
X,0.7906614785992218,"Note that the kernel does not depend on any off-diagonal terms tuv with u ̸= v because there isn’t
weight-sharing in the last layer. Let dpen = (p, p, . . . , p) ∈NkL. Then ∥dpen∥1 = pkL is the
effective dimension of the input to any node u ∈N−1. Assume k = dαk, p = dαp and w = dαw
and αk, αp, αw > 0. Applying Theorem 1 to 풦pen, we have"
X,0.7914396887159533,"풦CNN(t) =
X r∈NkL"
X,0.7922178988326848,"1
wλ풦pen(r)
X v∈N−1 X"
X,0.7929961089494163,"l∈N(dpen,r)"
X,0.7937743190661478,"Y r,l(ξv), Y r,l(ηv)
(155)"
X,0.7945525291828793,"Clearly,
the
eigenfunctions
are
{Y r,l(ξv)}r,l,v
and
the
corresponding
eigenvalues
are
{ 1"
X,0.7953307392996108,"wλ풦pen(r)}r,l,v . Note that"
X,0.7961089494163424,"1
wλ풦pen(r) = λ풦G(r) ∼d−ℒ(r)
(156)"
X,0.796887159533074,"Here and in what follows, we also consider r ∈NkL as an element in NwkL"
X,0.7976653696498055,"When the readout layer is a GAP, the weights of the penultimate layer are shared across different
spatial locations, namely, all nodes in N−1 use the same weight. As such, the kernel correspond-
ing to the CNN-GAP depends on both the diagonal and off-diagonal terms t = (tuv)u,v∈N−1 ∈
[−1, 1]kL×kL, which can be written as (Novak et al., 2019b)"
X,0.798443579766537,풦CNN-GAP(t) =
X,0.7992217898832685,"u,v∈N−1
풦pen(tuv) = 1 w2
X"
X,0.8,"u,v∈N−1
풦pen(tuv)
(157)"
X,0.8007782101167316,Under review as a conference paper at ICLR 2022
X,0.8015564202334631,"Applying Theorem 1 to 풦pen, we have"
X,0.8023346303501946,"풦CNN-GAP(t) = 1 w2
X"
X,0.8031128404669261,"r∈NkL
λ풦pen(r)
X"
X,0.8038910505836576,"u,v∈N−1 X"
X,0.8046692607003891,"l∈N(dpen,r)"
X,0.8054474708171206,"Y r,l(ξu), Y r,l(ηv)
(158) =
X r∈NkL"
X,0.8062256809338522,"1
wλ풦pen(r)
X"
X,0.8070038910505837,"l∈N(dpen,r)  w−1 2
ˆ u∈N−1"
X,0.8077821011673152,"Y r,l(ξu) ! w−1 2
ˆ u∈N−1"
X,0.8085603112840467,"Y r,l(ηu) ! (159) =
X r∈NkL"
X,0.8093385214007782,"1
wλ풦pen(r)
X"
X,0.8101167315175097,"l∈N(dpen,r)"
X,0.8108949416342413,"Y
Sym
r,l (ξ)Y
Sym
r,l (η)
(160) where"
X,0.8116731517509728,"Y
Sym
r,l (ξ) ≡w−1 2
ˆ u∈N−1"
X,0.8124513618677043,"Y r,l(ξu)
,
r ∈NkLand
l ∈N(dpen, r)
(161)"
X,0.8132295719844358,"That is the eigenfunctions and eigenvalues of 풦CNN-GAP are {Y
Sym
r,l (ξ)}r,l and { 1"
X,0.8140077821011673,"wλ풦pen(r)}r,l resp."
X,0.8147859922178988,"In sum, the eigenvalues of 풦CNN and 풦CNN-GAP are the same (up to the multiplicity factor w). Each
eigenspace of 풦CNN-GAP is given by symmetric polynomials of the form Eq. (161). We can see that
the GAP reduces the dimension of each eigenspace by a factor of w. Same arguments can also be
applied to the NTKs"
X,0.8155642023346303,ΘCNN(t) =
X,0.8163424124513619,"v∈N−1
(풦pen(tvv) + Θpen(tvv))
(162) =
X r∈NkL  1"
X,0.8171206225680934,wλ풦pen(r) + 1
X,0.8178988326848249,"wλΘpen(r)
 X v∈N−1 X"
X,0.8186770428015564,"l∈N(dpen,r)"
X,0.8194552529182879,"Y r,l(ξv), Y r,l(ηv) (163)"
X,0.8202334630350194,ΘCNN-GAP(t) =
X,0.8210116731517509,"u,v∈N−1
(풦pen(tuv) + Θpen(tuv))
(164) =
X r∈NkL  1"
X,0.8217898832684825,wλ풦pen(r) + 1
X,0.822568093385214,"wλΘpen(r)
 X v∈N−1"
X,0.8233463035019455,"Y
Sym
r,l (ξ), Y
Sym
r,l (η)
(165)"
X,0.824124513618677,"where Θpen is the NTK of the penultimate layer which is the same for all nodes in N−1. Since
1
wλΘpen ∼d−ℒ(r), we have"
X,0.8249027237354085,"1
wλ풦pen(r) + 1"
X,0.82568093385214,"wλΘpen(r) ∼d−ℒ(r)
(166)"
X,0.8264591439688715,"K.1
GENERALIZATION BOUND OF CNN-GAP"
X,0.8272373540856031,"We show that GAP improves the data efﬁciency of D-CNNs by a factor of w ∼dαw under a stronger
assumptions on activations φ.
Assumption Poly-φ: There is a sufﬁciently large J ∈N such that for all hiddens nodes u"
X,0.8280155642023346,"φ∗
u
(j)(0) ̸= 0
for
1 ≤j ≤J
and
φ∗
u
(j)(0) = 0
otherwise
(167)"
X,0.8287937743190662,This assumption implies that there are 0 < J1 < J2 ∈R such that for all 0 ̸= r with |r| < J1 dr
X,0.8295719844357977,"dt 풦pen(0) ̸= 0
and
dr"
X,0.8303501945525292,"dt Θpen(0) ̸= 0
(168)"
X,0.8311284046692607,and for all r with |r| > J2 dr
X,0.8319066147859923,"dt 풦pen ≡0
and
dr"
X,0.8326848249027238,"dt Θpen ≡0
(169)"
X,0.8334630350194553,"Moreover, J1 →∞as J →∞."
X,0.8342412451361868,Under review as a conference paper at ICLR 2022
X,0.8350194552529183,"Let Lp
Sym(X) ≤Lp(X) be the close subspace spanned by symmetric eigenfunctions Eq. (161). Let
KSym = 풦CNN-GAP or ΘCNN-GAP."
X,0.8357976653696498,"For X ⊆X and r /∈L(G(d)), deﬁne the regressor and the projection operator to be"
X,0.8365758754863813,"RSym
X (f)(x) = KSym(x, X)KSym(X, X)−1f(X)"
X,0.8373540856031129,"P Sym
>r (f) =
X"
X,0.8381322957198444,r:L(r)>r X
X,0.8389105058365759,"l∈N(dpen,r)
⟨f, Y
Sym
r,l ⟩L2(X)Y
Sym
r,l ."
X,0.8396887159533074,"Theorem 6. Let G = {G(d)}d, where each G(d) is a DAG associated to the D-CNN in Eq. (147)
with αk, αp, αw > 0. Let r /∈L(G(d)) be ﬁxed and the activations satisfy Assumption Poly-φ for
J = J(r) sufﬁciently large. Let f ∈L2
Sym(X) with Eσf = 0. Then for ϵ > 0,"
X,0.8404669260700389,"RSym
X (f) −f

2"
X,0.8412451361867704,"L2
Sym(X) −
P Sym
>r (f)

2"
X,0.842023346303502,"L2
Sym(X)"
X,0.8428015564202335,"= cd,ϵ∥f∥2
L2+ϵ
Sym (X),
(170)"
X,0.843579766536965,"where cd,ϵ →0 in probability as d →∞over X ∼σ[dr−αw ]."
X,0.8443579766536965,"Proof of Theorem 6. We need the following dimension counting lemma, which follows directly
from Lemma 4."
X,0.845136186770428,Lemma 6. Let r ∈L(G(d)). Then
X,0.8459143968871595,"dim

span
n"
X,0.846692607003891,"Y
Sym
r,l :
L(r) = r, l ∈N(d, r)
o
∼dr−αw
(171)"
X,0.8474708171206226,"Recall that ℒ(G(d)) = {rj} in non-descending order. Similarly, let"
X,0.8482490272373541,"ESym
i
= span{Y
Sym
r,l :
ℒ(r) = ri}
(172)"
X,0.8490272373540856,"From the above lemma, we have"
X,0.8498054474708171,"dim(ESym
i
) ∼dri−αw
(173)"
X,0.8505836575875486,"Since r /∈ℒ(G(d)), there is a j such that rj < r < rj+1. Let n(d) = dr−αw and"
X,0.8513618677042801,"m(d) = dim

span{Y
Sym
r,l : ℒ(r) ≤rj}

= dim(span
["
X,0.8521400778210116,"i≤j
ESym
i
)
(174)"
X,0.8529182879377432,"Clearly, m(d) ∼drj−αw. We list all eigenvalues of KSym in non-ascending order as {λd,i}. In
particular, we have"
X,0.8536964980544747,"λd,m(d) ∼d−rj > d−r > d−rj+1 ∼λd,m(d)+1 .
(175)"
X,0.8544747081712062,We proceed to verify Assumptions 4 and 5 in Sec. L.
X,0.8552529182879377,"Assumptions 4 (a)
We choose u(d) to be"
X,0.8560311284046692,u(d) = dim 
X,0.8568093385214007,"span
["
X,0.8575875486381322,"i:ri≤2r+100
ESym
i "
X,0.8583657587548638,".
(176)"
X,0.8591439688715953,Let s = inf{¯r ∈ℒ(G(d)) : ¯r > 2r + 100}. Assumption 4 (a) follows from Proposition 1.
X,0.8599221789883269,"Assumptions 4 (b)
For l > 1, we have
X"
X,0.8607003891050584,"j=u(d)+1
λl
d,j ∼
X"
X,0.8614785992217899,"ri:ri≥s
(d−ri)l dim(ESym
i
) ∼d−s(l−1)−αw
(177)"
X,0.8622568093385214,"which also holds for l = 1 since dαw
X"
X,0.863035019455253,"j=u(d)+1
λd,j ∼1
(178)"
X,0.8638132295719845,Under review as a conference paper at ICLR 2022 Thus (P
X,0.864591439688716,"j=u(d)+1 λl
d,j)2
P"
X,0.8653696498054475,"j=u(d)+1 λ2l
d,j
∼d−2s(l−1)−2αw"
X,0.866147859922179,d−s(2l−1)−αw = ds−αw > d2r+100−αw > n(d)2+δ ∼d(2+δ)(r−αw). (179)
X,0.8669260700389105,Assumption 4 (c) This requires some work and is veriﬁed in Sec. K.2.
X,0.867704280155642,Assumption 5 (a) Since m(d) = dim(span S
X,0.8684824902723736,"i≤j Ei) and rj+1 > r > rj, we have"
X,0.8692607003891051,"1
λd,m(d)+1 X"
X,0.8700389105058366,"j≥m(d)+1
λl
d,j ∼
1
(d−rj+1)l
X"
X,0.8708171206225681,"i>j
(d−ri)l dim(ESym
i
)
(180)"
X,0.8715953307392996,"∼dim(ESym
j+1) = drj+1−αw
(181)"
X,0.8723735408560311,"> n(d)1+δ = d(r−αw)(1+δ)
(182)"
X,0.8731517509727627,as long as δ < (rj+1 −αw)/(r −αw) −1.
X,0.8739299610894942,"Assumption 5 (b) This is obvious since m(d) ∼drj−αw, n(d) ∼dr−αw and r > rj."
X,0.8747081712062257,"Assumption 5 (c) This follows from rj < r. Indeed,"
X,0.8754863813229572,"1
λd,m(d) X"
X,0.8762645914396887,"j≥m(d)+1
λd,j ∼
1
(d−rj) X"
X,0.8770428015564202,"i>j
(d−ri) dim(ESym
i
) ∼drj−αw
(183)"
X,0.8778210116731517,"≤n(d)1−δ = d(r−αw)(1−δ)
(184)"
X,0.8785992217898833,as long as 0 < δ < 1 −(rj −αw)/(r −αw).
X,0.8793774319066148,"K.2
VERIFICATION OF ASSUMPTIONS 4(C)."
X,0.8801556420233463,We begin with proving Eq. (227). Let Xi deﬁne the random variable
X,0.8809338521400778,"Xi ≡Eξ∼σdKSym,>m(d)(ξi, ξ)2
and
∆i ≡Xi −EXi
(185)"
X,0.8817120622568093,We need to show that
X,0.8824902723735408,supi∈[n(d)] |∆i| EXi
X,0.8832684824902723,"in prob.
−−−−→
d→∞0.
(186)"
X,0.8840466926070039,"By Markov’s inequality, it sufﬁces to show that"
X,0.8848249027237354,"(EXi)−1E sup
i∈[n(d)]
|∆i| −−−→
d→∞0
(187)"
X,0.8856031128404669,"By orthogonality and treating r ∈NkL as an element of NwkL, we have"
X,0.8863813229571984,"EXi = Eξ, ¯ξ∼σdKSym,>m(d)(ξ, ¯ξ)2
(188)"
X,0.8871595330739299,"= Eξ, ¯ξ∼σd  X"
X,0.8879377431906614,"r:ℒ(r)>r
bKSym(r)
X"
X,0.888715953307393,"l∈N(dpen,r)"
X,0.8894941634241245,"Y
Sym
r,l (ξ)Y
Sym
r,l (¯ξ)  2 (189)"
X,0.890272373540856,"= Eξ, ¯ξ∼σd
X"
X,0.8910505836575876,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.8918287937743191,"l∈N(dpen,r)"
X,0.8926070038910506,"Y
Sym
r,l (ξ)Y
Sym
r,l (¯ξ)

2
(190) =
X"
X,0.8933852140077821,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.8941634241245137,"l∈N(dpen,r)
Eξ, ¯ξ∼σd
Y
Sym
r,l (ξ)Y
Sym
r,l (¯ξ)

2
(191) =
X"
X,0.8949416342412452,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.8957198443579767,"l∈N(dpen,r)
1
(192) =
X"
X,0.8964980544747082,"r:ℒ(r)>r
bK2
Sym(r)|N(dpen, r)|
(193)"
X,0.8972762645914397,Under review as a conference paper at ICLR 2022
X,0.8980544747081712,"and
Xi = Eξ∼σdKSym,>m(d)(ξi, ξ)2
(194)"
X,0.8988326848249028,= Eξ∼σd  X
X,0.8996108949416343,"r:ℒ(r)>r
bKSym(r)
X"
X,0.9003891050583658,"l∈N(dpen,r)"
X,0.9011673151750973,"Y
Sym
r,l (ξi)Y
Sym
r,l (ξ)  2 (195)"
X,0.9019455252918288,"= Eξ∼σd
X"
X,0.9027237354085603,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.9035019455252918,"l∈N(dpen,r)"
X,0.9042801556420234,"Y
Sym
r,l (ξi)Y
Sym
r,l (ξ)

2
(196) =
X"
X,0.9050583657587549,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.9058365758754864,"l∈N(dpen,r)
Eξ∼σd
Y
Sym
r,l (ξi)Y
Sym
r,l (ξ)

2
(197) =
X"
X,0.9066147859922179,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.9073929961089494,"l∈N(dpen,r)
|Y
Sym
r,l (ξi)|2
(198) =
X"
X,0.9081712062256809,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.9089494163424124,"l∈N(dpen,r)  1 w X u"
X,0.909727626459144,"Y r,l(ξi,u)2 + 1 w X u̸=v"
X,0.9105058365758755,"Y r,l(ξi,u)Y r,l(ξi,v) "
X,0.911284046692607,(199)
X,0.9120622568093385,"= Eξ, ¯ξ∼σdKSym,>m(d)(ξ, ¯ξ)2 +
X"
X,0.91284046692607,"r:ℒ(r)>r
bK2
Sym(r)
X"
X,0.9136186770428015,"l∈N(dpen,r)  1 w X u̸=v"
X,0.914396887159533,"Y r,l(ξi,u)Y r,l(ξi,v)  "
X,0.9151750972762646,"(200)
Let"
X,0.9159533073929961,"Xr,i =
X"
X,0.9167315175097276,"l∈N(dpen,r)  1 w X u̸=v"
X,0.9175097276264591,"Y r,l(ξi,u)Y r,l(ξi,v) "
X,0.9182879377431906,"
(201)"
X,0.9190661478599221,"then
∆i =
X"
X,0.9198443579766536,"r:ℒ(r)>r
bK2
Sym(r)Xr,i
(202)"
X,0.9206225680933852,"We replace the maximal function by the lq-norm for q ≥1,"
X,0.9214007782101167,"E
sup
i∈[n(d)]
|∆i| ≤E(
X"
X,0.9221789883268483,"i∈[n(d)]
|∆i|q)
1
q ≤(E
X"
X,0.9229571984435798,"i∈[n(d)]
|∆i|q)
1
q = n(d)
1
q (E|∆i|q)
1
q
(203)"
X,0.9237354085603113,"where the ﬁrst three expectations are taken over ξ1, . . . , ξn(d) ∼σd and the last one is taken over
ξi ∼σd. Then we replace the Lq-norm by the L2-norm via Hypercontractivity, in which we used
Assumption Poly-φ which implies that ∆i is a polynomial of bounded degree"
X,0.9245136186770428,"E
sup
i∈[n(d)]
|∆i| ≤n(d)
1
q (E|∆i|q)
1
q ≤Cqn(d)
1
q (E|∆i|2)
1
2 = Cqn(d)
1
q (E∆2
i )
1
2
(204)"
X,0.9252918287937744,"We expand the L2-norm and use orthogonality to “erase"" the off-diagonal terms twice: ﬁrst for
r ̸= ¯r
Eξi∼σdXr,iX¯r,i = 0
(205)
and second for l ̸= l′ or u ̸= v"
X,0.9260700389105059,"Eξi∼σdX2
r,i = 1"
X,0.9268482490272374,"w2 Eξi∼σd
X"
X,0.9276264591439689,"l∈N(dpen,r)  X u̸=v"
X,0.9284046692607004,"Y r,l(ξi,u)Y r,l(ξi,v)  X l′  X"
X,0.9291828793774319,u′̸=v′
X,0.9299610894941635,"Y r,l′(ξi,u′)Y r,l′(ξi,v′)   (206) = 2"
X,0.930739299610895,"w2 Eξi∼σd
X"
X,0.9315175097276265,"l∈N(dpen,r)  X u̸=v"
X,0.932295719844358,"Y r,l(ξi,u)2Y r,l(ξi,v)2  = 2 w2
X"
X,0.9330739299610895,"l∈N(dpen,r)
w(w −1) (207)"
X,0.933852140077821,= 2(w −1) w X
X,0.9346303501945525,"l
1 = 2(w −1)"
X,0.935408560311284,"w
|N(dpen, r)| ≤2|N(dpen, r)|
(208)"
X,0.9361867704280156,Under review as a conference paper at ICLR 2022
X,0.9369649805447471,"Combining this estimate with Eq. (193), Eq. (202) and Eq. (204) yields"
X,0.9377431906614786,"(EXi)−1(E
sup
i∈[n(d)]
|∆i|) ≤Cqn(d)
1
q (2 P"
X,0.9385214007782101,"r:ℒ(r)>r bK4
Sym(r)|N(dpen, r)|)1/2
P"
X,0.9392996108949416,"r:ℒ(r)>r bK2
Sym(r)|N(dpen, r)|
(209) ≤
√"
X,0.9400778210116731,"2Cqn(d)
1
q
P"
X,0.9408560311284047,"r:ℒ(r)>r bK2
Sym(r)|N(dpen, r)|1/2
P"
X,0.9416342412451362,"r:ℒ(r)>r bK2
Sym(r)|N(dpen, r)|
(210) ≤
√"
X,0.9424124513618677,"2Cqn(d)
1
q
sup
r:ℒ(r)>r"
X,0.9431906614785992,"bK2
Sym(r)|N(dpen, r)|1/2"
X,0.9439688715953307,"bK2
Sym(r)|N(dpen, r)|
(211)"
X,0.9447470817120622,"∼2Cqd
r
q
sup
r:ℒ(r)>r
|N(dpen, r)|−1"
X,0.9455252918287937,"2
(212)"
X,0.9463035019455253,"∼d
r
q
sup
r:ℒ(r)>r
d−|r|αp −−−→
d→∞0
(213)"
X,0.9470817120622568,by choosing q (independent of d) sufﬁciently large.
X,0.9478599221789883,The proof of Eq. (228) is similar. Let Xi denote the random variable
X,0.9486381322957198,"Xi ≡KSym,>m(d)(ξi, ξi)
and
∆i ≡Xi −EXi
(214)"
X,0.9494163424124513,and it sufﬁces to prove
X,0.9501945525291828,E supi∈[n(d)] |∆i|
X,0.9509727626459143,"EXi
−−−→
d→∞
0
(215)"
X,0.9517509727626459,We have
X,0.9525291828793774,"EXi = Eξi∼σdKSym,>m(d)(ξi, ξi)
(216)"
X,0.953307392996109,"= Eξi∼σd
X"
X,0.9540856031128405,"r:ℒ(r)>r
bKSym(r)
X"
X,0.954863813229572,"l∈N(dpen,r)"
X,0.9556420233463035,"Y
Sym
r,l (ξi)Y
Sym
r,l (ξi)
(217) =
X"
X,0.9564202334630351,"r:ℒ(r)>r
bKSym(r)|N(dpen, r)|
(218) and"
X,0.9571984435797666,"∆i = Xi −EXi = w−1
X"
X,0.9579766536964981,"r:ℒ(r)>r
bKSym(r)
X"
X,0.9587548638132296,"l∈N(dpen,r) X u̸=v"
X,0.9595330739299611,"Y r,l(ξi,u)Y r,l(ξi,v), .
(219)"
X,0.9603112840466926,"The remaining steps (replacing the maximal function by the lq-norm, and then the Lq-norm by the
L2-norm using hypercontractivity) are similar to that of the proof of Eq. (227), which are omitted
here."
X,0.9610894941634242,"L
KERNEL CONCENTRATION, HYPERCONTRACTIVITY AND
GENERALIZATION OF MEI ET AL. (2021A)"
X,0.9618677042801557,"For convenience, we brieﬂy recap the analytical results regarding generalization bounds of kernel
machines from Mei et al. (2021a) Sec 3."
X,0.9626459143968872,"Let (Xd, σd) be a probability space and ℋd be a compact self-adjoint positive deﬁnite operator
from L2(Xd, σd) →L2(Xd, σd). We assume ℋd ∈L2(Xd × Xd). Let {ψd,j} and {λd,j} be the
eigenfunctions and eigenvalues associated to ℋd, i.e."
X,0.9634241245136187,"ℋdψd,j(x) ≡
ˆ"
X,0.9642023346303502,"y∈Xd
ℋd(x, y)ψd,j(y)σd(y) = λd,jψd,j(x).
(220)"
X,0.9649805447470817,"We assume the eigenvalues are in non-ascending order, i.e. λd,j+1 ≥λd,j ≥0. Note that
X"
X,0.9657587548638132,"j
λ2
d,j = ∥ℋd∥2
L2(Xd×Xd) < ∞.
(221)"
X,0.9665369649805448,Under review as a conference paper at ICLR 2022
X,0.9673151750972763,"The associated reproducing kernel Hilbert space (RKHS) is deﬁned to be functions f ∈L2(Xd, σd)"
X,0.9680933852140078,"with ∥ℋ
−1"
D,0.9688715953307393,"2
d
f∥L2(Xd,σd) < ∞. Given a ﬁnite training set X ⊆Xd and observed labels f(X) ∈
R|X|, the regressor is an extension operator deﬁned to be"
D,0.9696498054474708,"ℛXf(x) = ℋd(x, X)ℋd(X, X)−1f(X) .
(222)"
D,0.9704280155642023,"Intuitively, when “X →Xd"" in some sense, we expect the following"
D,0.9712062256809338,"ℛXf(x) = ℋd(x, X)ℋd(X, X)−1f(X) →ℛXdf(x) = ℋd(ℋ−1
d f)(x) = f(x),
(223)"
D,0.9719844357976654,"namely, “ℛX →IXd"" in some sense."
D,0.9727626459143969,"Using tools from the non-asymptotic analysis of random matrices (Vershynin, 2010), the work Mei
et al. (2021a) provides a very nice answer to the above question in terms of the decay property
of the eigenvalues {λd,j} and the hypercontractivity property of the eigenfunctions {ψd,j}. They
show that ℛX is essentially a projection operator onto the low eigenspace under certain regularity
assumptions on the operator ℋd. These assumptions are stated via the relationship between the
number of (training) samples n = n(d), the tail behavior of the eigenvalues with index ≥m = m(d)
and the tail behavior of the operator ℋd"
D,0.9735408560311284,"ℋd,>m(d)(x, ¯x) ≡
X"
D,0.9743190661478599,"j>m(d)
λjψj(x)ψj(¯x)
(224)"
D,0.9750972762645914,"as the ""input dimension"" d becomes sufﬁciently large."
D,0.9758754863813229,"Assumption 4. We say that the the sequence of operator {ℋd}d≥1 satiesﬁes the Kernel Concen-
tration Property (KCP) with respect to the sequence {n(d), m(d)}d≥1 if there exsts a sequence of
integers {u(d)}d≥1 with u(d) ≥m(d) such that the following holds"
D,0.9766536964980544,"(a) (Hypercontractivity.) Let Du(d) = span{ψj : 1 ≤j ≤u(d)}. Then for any ﬁxed q ≥1,
and C = C(q) such that for f ∈Du(d)"
D,0.977431906614786,"∥f∥Lq(Xd,σd) ≤C∥f∥L2(Xd,σd)
(225)"
D,0.9782101167315175,"(b) (Eigen-decay.) There exists δ > 0, such that, for all d large enough, for l = 1 and 2,"
D,0.978988326848249,"n(d)2+δ ≤
(P"
D,0.9797665369649805,"j≥u(d)+1 λl
d,j)2
P
j≥u(d)+1 λ2l
d,j
(226)"
D,0.980544747081712,"(c) (Concentration of Diagonals.) For {xi}i∈[n(d)] ∼σn(d)
d
, we have:"
D,0.9813229571984435,"supi∈[n(d)]
Ex∼σdℋd,>m(d)(xi, x)2 −Ex,¯x∼σdℋd,>m(d)(x, ¯x)2"
D,0.982101167315175,"Ex,¯x∼σdℋd,>m(d)(x, ¯x)2
in Prob.
−−−−→
d→∞
0
(227)"
D,0.9828793774319066,"supi∈[n(d)]
ℋd,>m(d)(xi, xi) −Ex∼σdℋd,>m(d)(x, x)"
D,0.9836575875486381,"Ex∼σdℋd,>m(d)(x, x)"
D,0.9844357976653697,"in Prob.
−−−−→
d→∞
0
(228)"
D,0.9852140077821012,where cd →0 in probability as d →∞.
D,0.9859922178988327,"Assumption 5. Let ℋd and {m(d), n(d)}d≥1 be the same as above."
D,0.9867704280155642,"(a) For l = 1 and 2, there exists δ > 0 such that"
D,0.9875486381322958,"n(d)1+δ ≤
1
λl
d,m(d)+1 X"
D,0.9883268482490273,"k=λm(d)+1
λl
d,k
(229)"
D,0.9891050583657588,(b) There exists δ > 0 such that
D,0.9898832684824903,"m(d) ≤n(d)1−δ
(230)"
D,0.9906614785992218,(c) (Spectral Gap.) There exists δ > 0 such that
D,0.9914396887159533,"n(d)1−δ ≥
1
λd,m(d) X"
D,0.9922178988326849,"k≥m(d)+1
λd,k
(231)"
D,0.9929961089494164,Under review as a conference paper at ICLR 2022
D,0.9937743190661479,"Let 풫>k (similarly for 풫k, 풫≤k, etc.) denote the projection operator"
D,0.9945525291828794,"풫>kf =
X"
D,0.9953307392996109,"j>k
⟨f, ψj⟩ψj
(232)"
D,0.9961089494163424,"Theorem 7 (Mei et al. (2021a)). Assume ℋd satisfy Assumptions 4 and 5. Let {fd}d≥1 be a
sequence of functions and let X ∼σn(d)
d
. Then for every ϵ > 0,"
D,0.9968871595330739,"∥ℛX(fd) −fd∥2
L2(Xd,σd) = ∥풫>m(d)fd∥2
L2(Xd,σd) + cd,ϵ∥fd∥2
L2+ϵ(Xd,σd)
(233)"
D,0.9976653696498055,"where cd,ϵ →0 in probability as d →∞."
D,0.998443579766537,"The theorem says, ℛX is essentially the projection operator 풫≤m(d) in the sense that when restricted
to L2+ϵ(Xd, σd),"
D,0.9992217898832685,"ℛX = 풫≤m(d) + Errord,ϵ
(234)"
