Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.00392156862745098,"Research has shown that deep networks tend to be overly optimistic about their
predictions, leading to an underestimation of prediction errors. Due to the limited
nature of data, existing studies have proposed various methods based on model
prediction probabilities to bin the data and evaluate calibration error. We propose
a more generalized definition of calibration error called Partitioned Calibration
Error (PCE), revealing that the key difference among these calibration error metrics
lies in how the data space is partitioned. We put forth an intuitive proposition
that an accurate model should be calibrated across any partition, suggesting that
the input space partitioning can extend beyond just the partitioning of prediction
probabilities, and include partitions directly related to the input. Through semantic-
related partitioning functions, we demonstrate that the relationship between model
accuracy and calibration lies in the granularity of the partitioning function. This
highlights the importance of partitioning criteria for training a calibrated and
accurate model. To validate the aforementioned analysis, we propose a method
that involves jointly learning a semantic aware grouping function based on deep
model features and logits to partition the data space into subsets. Subsequently,
a separate calibration function is learned for each subset. Experimental results
demonstrate that our approach achieves significant performance improvements
across multiple datasets and network architectures, thus highlighting the importance
of the partitioning function for calibration."
INTRODUCTION,0.00784313725490196,"1
Introduction"
INTRODUCTION,0.011764705882352941,"With the advancements in deep learning technology, deep learning models have achieved, and
in some cases even surpassed, human-level accuracy in various domains[1, 2]. In safety-critical
applications[3, 4] such as self-driving[5] and disease diagnosis[6], it is not only desirable to have
high accuracy from models but also to have their predicted probabilities reflect the true likelihood of
correctness. For instance, if a model predicts a probability of 0.9 for a particular class, we expect the
actual probability of being correct to be close to 0.9. This is known as probability calibration. However,
recent research has indicated that while deep models have improved in accuracy, their probability
calibration has declined[7‚Äì9]. Typically, deep models tend to overestimate the probabilities of correct
predictions, leading to overconfidence. This discrepancy between accuracy and calibration has made
model calibration a crucial research direction.[7, 10‚Äì19]"
INTRODUCTION,0.01568627450980392,"Defining an evaluation method for assessing the degree of calibration is crucial to calibrating a model.
Currently, most evaluation methods are based on binning model prediction probabilities, followed
by analyzing the distribution of true labels within each bin[7, 12, 20, 21]. The calibration error is
then estimated by measuring the difference between the predicted probabilities and the empirical"
INTRODUCTION,0.0196078431372549,‚àóDe-Chuan Zhan is the corresponding author.
INTRODUCTION,0.023529411764705882,"(a) No calibration 
(b) Calibrated with IR
(c) Underconfidence in ùêÜùêÜùüèùüè
(d) Overconfidence in ùêÜùêÜùüêùüê"
INTRODUCTION,0.027450980392156862,"Gap
Accuracy"
INTRODUCTION,0.03137254901960784,"Gap
Accuracy"
INTRODUCTION,0.03529411764705882,"Gap
Accuracy"
INTRODUCTION,0.0392156862745098,"Gap
Accuracy"
INTRODUCTION,0.043137254901960784,"Figure 1: We reconstruct CIFAR10 into a binary classification problem ([0‚àí4] as positives, [5‚àí9] as
negatives) and train a Resnet152 on it. Initially, the predicted probabilities are severely uncalibrated, as
shown in (a). Then we train an Isotonic regression (IR) model (with the test labels) to calibrate outputs,
which leads to nearly perfect calibration in (b). However, the partitions defined by original labels
may reveal underlining miscalibration. For example, let G1 = {0, 1, 2, 5, 6, 7}, G2 = {3, 4, 8, 9}.
The outputs of IR turned out to be significantly underconfident on G1 and overconfident on G2."
INTRODUCTION,0.047058823529411764,"distribution. However, such estimation of calibration error fails to reflect the model‚Äôs accuracy[22].
For instance, consider a binary classification problem where both classes are distributed uniformly,
and the model consistently outputs a probability of 0.5 for all inputs. Even using the strictest
evaluation method for calibration error, this classifier would appear perfectly calibrated[22]. In
reality, as long as the evaluation of calibration error is solely based on prediction probabilities, a
classifier with a fixed output marginal distribution p(y) would always be perfectly calibrated but
useless. This type of calibration error definition may also be counterintuitive to some extent. For
example, a recommendation model may be overconfident in one user group and underconfident in
another, yet still be considered calibrated overall. However, no user would perceive the model‚Äôs output
probabilities as accurate in such cases. To illustrate this issue, we present a constructed experiment in
Figure. 1, which demonstrates the existence of such situations."
INTRODUCTION,0.050980392156862744,"For calibrating a trained model, existing methods mainly rely on transforming the model‚Äôs predicted
probabilities or logits (inputs to the softmax layer)[7, 10‚Äì12, 23‚Äì25]. For example, histogram
binning[7] involves binning the predicted probabilities and calibrating the probabilities within each
bin separately. However, as discussed earlier regarding calibration error analysis, if a calibration
method relies solely on the model‚Äôs output probabilities for calibration, it cannot achieve calibration
across different semantic contexts (x) because the calibration model itself is completely unaware
of semantics. Some studies have analyzed calibration across different categories and proposed
corresponding calibration methods. For instance, the Dirichlet calibration method[12] models
multiclass probabilities as Dirichlet distributions and transforms the uncalibrated probabilities into
calibrated ones. These methods incorporate category information, allowing for calibration across
different categories. However, this category information may not be directly available, or the known
categories may not be the divisions we are interested in. For example, in a recommendation system,
we may want the model to be calibrated across various skin color groups, even if we may not have
access to the corresponding category information."
INTRODUCTION,0.054901960784313725,"Contribution. The analysis above has inspired us to propose that a perfectly calibrated model should
be calibrated across any data space partition. We present the Partitioned Calibration Error (PCE) as a
comprehensive framework for defining semantic-aware calibration errors. By illustrating that various
common calibration error metrics are specific instances of PCE under distinct partition functions,
we establish a direct correlation between calibration error and model accuracy: PCE converges to
accurate score functions of data uncertainty through a bijective grouping function. To discover more
effective partition rules, we introduce a technique that incorporates a linear layer onto the deep
model‚Äôs features, enabling the modeling of the partition function. By employing softmax to generate
a soft partition, we achieve end-to-end optimization. By generating diverse partition functions,
our approach facilitates calibration across different semantic domains, thereby enhancing overall
calibration performance without losing accuracy. Experimental results across multiple datasets and
network architectures consistently demonstrate the efficacy of our method in improving calibration."
PARTITIONED CALIBRATION ERROR AND GROUPING-BASED CALIBRATION,0.058823529411764705,"2
Partitioned calibration error and grouping-based calibration"
PARTITIONED CALIBRATION ERROR AND GROUPING-BASED CALIBRATION,0.06274509803921569,"We first present our formulation of PCE, which serves as the foundation for our calibration approach."
MEASURING CALIBRATION WITH PARTITIONS,0.06666666666666667,"2.1
Measuring calibration with partitions"
MEASURING CALIBRATION WITH PARTITIONS,0.07058823529411765,We start by defining the partition by a grouping function as follows.
MEASURING CALIBRATION WITH PARTITIONS,0.07450980392156863,"Definition 1 (Grouping function and input space partition). A grouping function g is a mapping
from an input x to a group indicator g(x) = G ‚ààG, with the following three properties:"
MEASURING CALIBRATION WITH PARTITIONS,0.0784313725490196,"‚àÄx ‚ààD, ‚àÉG ‚ààG, g(x) = G
(1)
‚àÄx1, x2, g(x1) Ã∏= g(x2) ‚Üíx1 Ã∏= x2
(2)"
MEASURING CALIBRATION WITH PARTITIONS,0.08235294117647059,"‚àÄÀÜG ‚ààG, ‚àÉÀÜx ‚ààD, g(ÀÜx) = ÀÜG
(3)"
MEASURING CALIBRATION WITH PARTITIONS,0.08627450980392157,The group indicator G is also used to denote the induced subset {x|g(x) = G}.
MEASURING CALIBRATION WITH PARTITIONS,0.09019607843137255,Lemma 1. A partition of the input space D is defined by P = {{x|g(x) = ÀÜG}| ÀÜG ‚ààG}.
MEASURING CALIBRATION WITH PARTITIONS,0.09411764705882353,"Proof. By definition, the subsets in P are (1) non-overlapping (by Eq. (2)), (2) non-empty (by Eq.
(3)), (3) the union of all the groups is equal to the universe set(by Eq. (1)). which guarantees P being
a valid partition on set D of all valid x."
MEASURING CALIBRATION WITH PARTITIONS,0.09803921568627451,"We use x ‚ààG and g(x) = G interchangeably, and (x, y) ‚ààG ‚Üîg(x) = G. With the partition
defined above, we can now define the partitioned calibration error."
MEASURING CALIBRATION WITH PARTITIONS,0.10196078431372549,Definition 2 (Partitioned calibration error (PCE)).
MEASURING CALIBRATION WITH PARTITIONS,0.10588235294117647,"PCE(S, g, L, f, D) =
X"
MEASURING CALIBRATION WITH PARTITIONS,0.10980392156862745,"P ‚ààP
p(P)
X"
MEASURING CALIBRATION WITH PARTITIONS,0.11372549019607843,"G‚ààP
p(G)L(S(G), S(f(G)))
(4)"
MEASURING CALIBRATION WITH PARTITIONS,0.11764705882352941,"Where P is the set of all concerned partitions, p(P) is the probability of choosing partition P,
p(G) =
R"
MEASURING CALIBRATION WITH PARTITIONS,0.12156862745098039,"(x,y)‚ààG p(x, y) is the probability of observing a data sample belonging to group G, S(¬∑) is
a specific statistical magnitude that can be calculated on a group. A straightforward definition of S(¬∑)
is the average function S(G) =
R"
MEASURING CALIBRATION WITH PARTITIONS,0.12549019607843137,"x,y pG(x, y)y, and S(f(G)) =
R"
MEASURING CALIBRATION WITH PARTITIONS,0.12941176470588237,"x,y pG(x, y)f(x), where y is the
one-hot label with yi = 1 if x ‚àài th class and yi = 0 otherwise. f(x)i is the predicted probability
of x being ith class. L(¬∑, ¬∑) measures the difference between S(G) and S(f(G)). pG(x, y) is the
normalized probability density function of x ‚ààG, that is,"
MEASURING CALIBRATION WITH PARTITIONS,0.13333333333333333,"pG(x, y) ="
MEASURING CALIBRATION WITH PARTITIONS,0.13725490196078433,"(
0,
if x /‚ààG
p(x,y)"
MEASURING CALIBRATION WITH PARTITIONS,0.1411764705882353,"p(G) ,
if x ‚ààG
(5)"
MEASURING CALIBRATION WITH PARTITIONS,0.1450980392156863,"We will write pG(x, y) as pG in the following contents for simplicity. In summary, the aforementioned
Eq. (4) defines PCE, which quantifies the expected disparity between the predicted probabilities and
the true probabilities within each subgroup, after randomly selecting a partition."
MEASURING CALIBRATION WITH PARTITIONS,0.14901960784313725,"Example 1 (Expected calibration error (ECE)[23]). With g(x) = Bin(f(x)), where Bin(¬∑) returns
the corresponding Bin ID, L(a, b) = |a ‚àíb|, and keep S as the average function."
MEASURING CALIBRATION WITH PARTITIONS,0.15294117647058825,"PCE =
X"
MEASURING CALIBRATION WITH PARTITIONS,0.1568627450980392,"G‚ààP
p(G)L(S(G), S(f(G))) =
X"
MEASURING CALIBRATION WITH PARTITIONS,0.1607843137254902,"G‚ààP
p(G)|
Z"
MEASURING CALIBRATION WITH PARTITIONS,0.16470588235294117,"x,y
pGf(x) ‚àí
Z"
MEASURING CALIBRATION WITH PARTITIONS,0.16862745098039217,"x,y
pGy|
(6)"
MEASURING CALIBRATION WITH PARTITIONS,0.17254901960784313,and its empirical estimation is
MEASURING CALIBRATION WITH PARTITIONS,0.17647058823529413,"PCE =
X G"
MEASURING CALIBRATION WITH PARTITIONS,0.1803921568627451,"|G|
|D||
X"
MEASURING CALIBRATION WITH PARTITIONS,0.1843137254901961,"(x,y)‚ààG"
MEASURING CALIBRATION WITH PARTITIONS,0.18823529411764706,"1
|G|f(x) ‚àí
X"
MEASURING CALIBRATION WITH PARTITIONS,0.19215686274509805,"(x,y)‚ààG"
MEASURING CALIBRATION WITH PARTITIONS,0.19607843137254902,"1
|G|y|
(7)"
MEASURING CALIBRATION WITH PARTITIONS,0.2,which is exactly the ECE estimator defined in [23] Eq.3.
MEASURING CALIBRATION WITH PARTITIONS,0.20392156862745098,"Note that in the above ECE, there is only one partition. We provide an example of using multiple
partitions in the following example."
MEASURING CALIBRATION WITH PARTITIONS,0.20784313725490197,"Example 2 (Class-wise ECE[12]). There are M partitions corresponding to M classes. The partition
of class u is denoted by Pu, the corresponding grouping function is defined by gu(x) = Bin(f(x)u)."
MEASURING CALIBRATION WITH PARTITIONS,0.21176470588235294,"Classwise-ECE =
X Pu"
M,0.21568627450980393,"1
M X G‚ààPu"
M,0.2196078431372549,"|G|
|D||
X"
M,0.2235294117647059,"(x,y)‚ààG"
M,0.22745098039215686,"1
|G|f(x) ‚àí
X"
M,0.23137254901960785,"(x,y)‚ààG"
M,0.23529411764705882,"1
|G|y|
(8)"
M,0.23921568627450981,"which is exactly the same as Eq. 4 in [12].
Example 3 (Top-label calibration error (TCE)[25]). With g(x) = Bin(maxi f(x)i), S(G) =
1
|G|
P I(y = arg maxi f(x)i), and S(f(G)) =
1
|G|
P"
M,0.24313725490196078,"(x,y)‚àºG maxi f(x)i. Resulting in the Top-
label calibration error defined in [25]."
M,0.24705882352941178,"From the above examples, it can be observed that the sole distinction among the aforementioned
calibration metrics lies in the employed grouping function. Hereinafter, we shall delve into two
distinctive scenarios of PCE to shed light on the intricate interplay between calibration and accuracy."
M,0.25098039215686274,"Example 4 (One-to-one grouping function). If the grouping function g(¬∑) is a bijection, then every
different x belongs to different groups, which corresponds to point-wise accuracy."
M,0.2549019607843137,"PCE =
X"
M,0.25882352941176473,"P ‚ààP
p(P)
X"
M,0.2627450980392157,"G‚ààP
p(G)L(S(G), S(f(G))) =
Z"
M,0.26666666666666666,"x,y
p(x, y)L(y, f(x))
(9)"
M,0.27058823529411763,"Minimizing this PCE with bijective grouping function will converge to f(x) = p(y|x) if a proper
score function L (e.g., cross-entropy or Brier score[26]) is used with unlimited data. The uncer-
tainty reflected by p(y|x) is called aleatoric uncertainty[27] and is the best a model can achieve
corresponding to Bayes error[28, 29].
Example 5 (Constant grouping function). If the grouping function g is a constant function with
‚àÄx1, x2, g(x1) = g(x2), then every different x belongs to a single group."
M,0.27450980392156865,"PCE = L
Z"
M,0.2784313725490196,"x,y
p(x, y)f(x),
Z"
M,0.2823529411764706,"x,y
p(x, y)y

(10)"
M,0.28627450980392155,"which is minimized by the model outputs the marginal distribution f(x) =
R"
M,0.2901960784313726,"(x,y) p(x, y)y = p(y)."
M,0.29411764705882354,"We provide proofs and further discussion about Example. (4) and Example. (5) in the supplementary
materials. The constant grouping function captures the vanilla uncertainty that we do not have any
knowledge about x, and we only know the marginal distribution of y."
M,0.2980392156862745,"The above analysis demonstrates that by employing finer partitions, PCE becomes a closer approx-
imation to the measure of accuracy. Conversely, coarser partitions align more closely with the
traditional calibration metrics utilized in prior research. Since neither extreme partitioning approach
applies to practical calibration, selecting an appropriate partitioning method is crucial for calibration
performance."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.30196078431372547,"2.2
Calibration with sematic-aware partitions"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3058823529411765,"Calibration methods can be designed from the perspective of minimizing the corresponding PCE.
For instance, histogram binning[7] adjusts the predicted probabilities within each bin to the mean
of the true probabilities in that bin. On the other hand, Bayesian Binning into Quantiles (BBQ)[11]
calibration involves setting multiple distinct bin counts and then obtaining a weighted average of
the predictions from histogram binning for each bin count, effectively encompassing scenarios with
multiple partitions in the PCE. However, existing methods rely on binning and calibration based
solely on model output probabilities. Our proposed PCE suggests that the partitioning approach can
depend not only on the predicted probabilities but also on the information in input x itself. This
would significantly enhance the flexibility of the partitions."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.30980392156862746,"To facilitate implementation, we begin by imposing certain constraints on the range of outputs from
the grouping function. We denote the number of partitions by U = |P| (each partition corresponds to
a specific grouping function as defined in Definition 1), and we assume that all data can be partitioned
into K groups. The output of the grouping function g takes the form of a one-hot encoding, where
if x belongs to the i-th group, g(x)i = 1, 0 < i < K, while all other g(x)j = 0, j Ã∏= i. And"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3137254901960784,"Gi = {x|g(x)i = 1} is the i-th group. Under these conditions, the expression for PCE can be
formulated as follows: PCE = K
X i |Gi|"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3176470588235294,"|D| L(S(Gi), S(fGi(Gi)))
(11)"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3215686274509804,"From the above equation, it is evident that for each group Gi, we can learn a calibration function fGi
specific to that group, which aims to minimize the calibration error within the group Gi."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3254901960784314,"In this paper, we employ the temperature scaling method as the learning approach for the calibration
function within each group. Specifically, temperature scaling involves adding a learnable temperature
parameter œÑ to the logits o(x) (i.e., the input to the softmax function) of the model and applying it to
a group, resulting in the following form of grouping loss:"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.32941176470588235,"Lgroup =
1
|Gi| X"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3333333333333333,"x,y‚ààGi
log
e o(x)y œÑi
P j e o(x)j"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.33725490196078434,"œÑi
(12)"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3411764705882353,"where the temperature parameter œÑi is specific to group Gi. Note that Lgroup is not a direct es-
timation of L(S(Gi), S(fGi(Gi))). Specifically, if we choose S to the average function, so the
empirical estimation of S(Gi) =
1
|Gi|
P"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.34509803921568627,"x,y‚ààGi y, and the empirical estimation of S(fGi(Gi)) ="
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.34901960784313724,"1
|Gi|
P"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.35294117647058826,"x,y‚ààGi fGi(x). Then, we choose the difference measure L to be the log-likelihood (cross
entropy) L(S(Gi), S(fGi(Gi))) = P"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3568627450980392,"j S(Gi)j log S(fGi(Gi))j, where j is the class index. The
Lgroup will be minimized by fGi(x) = y, which will also minimize L(S(Gi), S(fGi(Gi))). Thus,
Eq. (12) is a stronger constraint compared with minimizing L(S(Gi), S(fGi(Gi))) directly. Our
choice of this objective is motivated by two reasons: First, Eq. (12) is able to provide more signals
during training since each label y can guide corresponding fGi(x) directly. On the contrary, if
we optimize L(S(Gi), S(fGi(Gi))) directly, the labels and predictions are mixed and much label
information is lost. Secondly, optimizing Eq. (12) aligns well with the calibration method (TS and
ETS) to be used, which leads to better calibration performance."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3607843137254902,"In the aforementioned equation Eq. (12), we assume that the groups Gi are known. In order to
optimize the partition, we incorporate the grouping function g(¬∑) into the loss function. By summing
the above equation over all groups according to Eq. (11), we can obtain the final optimization
objective."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.36470588235294116,"LGC+T S =
1
|D| X"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3686274509803922,"x,y‚ààD
log
X"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.37254901960784315,"i
g(x)i
e o(x)y œÑi
P j e o(x)j"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3764705882352941,"œÑi
(13)"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3803921568627451,"When the output of g(x) is in the form of one-hot encoding, the above equation represents a standard
grouping loss function. To optimize g, we introduce a smoothing technique that transforms its output
into a probability value. Specifically, we add a linear layer on top of the neural network‚Äôs features z
and follow it with a softmax function as the grouping function g, which takes the following form:"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.3843137254901961,"gœï(x) = g‚Ä≤
œï(z(x)) = softmax(z(x)œïw + œïb)
(14)"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.38823529411764707,"where z ‚ààR|D|√ódz, œïw ‚ààRdz√óK, œïb ‚ààR1√óK. The features z(x) are fixed, which aligns with the
post-calibration setting. The above loss function is the combination of grouping and temperature
scaling, which is denoted as Grouping Calibration + Temperature Scaling (GC+TS)."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.39215686274509803,"Training gœï(x). The parameters œï of grouping function g is trained jointly with the calibration
function fŒ∏ on the validation dataset Dval (used in early stopping when training deep models). In the
GC+TS method, the trainable parameters of f is Œ∏ = {œÑi}. In a more intuitive sense, equation Eq.
(13) necessitates that the grouping function identifies the optimal partitioning, where adjusting œÑi
within each subgroup Gi minimizes the cross-entropy loss. Furthermore, due to the non-uniqueness
of the optimal grouping function that satisfies the aforementioned objective function, we can generate
multiple distinct partitions by optimizing with different initial values through multiple iterations.
To mitigate the potential overfitting caused by the high flexibility of the grouping function g, we
have introduced an additional regularization term, and the overall training loss of training gœï is
LGC+T S + Œª||œïw||2
2."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.396078431372549,"Training fŒ∏(x). After training of gœï, we simply apply a calibration method to each group. We
can also use temperature scaling in this phase while using hard grouping in Eq. (13). Any other"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.4,"calibration methods are also applicable, for example, we also adopt Ensemble Temperature Scaling
(ETS)[23] method to obtain GC+ETS in our experiments. The calibration function fŒ∏ is trained on
the holdout training dataset Dho, while keeping the grouping function fixed. Then the calibration
function f is used on each group of the testing dataset. The overall training and predicting procedure
is summarized in Algorithm. 1."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.403921568627451,"Calibrating with trained partitions. After training, we partition the input space into different
partitions. In each partition, an input x corresponds to a unique group G and a calibration function
fŒ∏(x), which results in a calibrated probability p(y|x, G). Since there are many different partitions,
the groups can be treated as sampled from a distribution p(G|x). We ensemble the calibrated
probabilities as the final prediction, which is indeed an Monte-Carlo‚Äôs estimation of p(y|x) =
P"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.40784313725490196,"G p(y|x, G)p(G|x). This ensemble approach is described in Line 5-11 of Algorithm. 1."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.4117647058823529,"Algorithm 1 Train group calibration with temperature scaling (GC+TS)
Input: Dval = {Zval, Oval, Yval}, Dho = {Zho, Oho, Yho}, Dtest = {Ztest, Otest}, U, K, Œª
Output: Calibrated ÀÜYtest"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.41568627450980394,"1: for u ‚Üê0 to U ‚àí1 do
2:
Randomly initialize œïu and Œ∏u
3:
Optimize œïu and Œ∏u on Dval to minimize Eq. (13) with L-BFGS[30]
4:
Optimize Œ∏ui on Dho to minimize Eq. (11) with base calibration method(e.g., TS)
5:
Calculate partition Pu = {Gui} with grouping function gœïu on Dtest
6:
for i ‚Üê0 to K ‚àí1 do
7:
Calibrate Gui with fŒ∏ui(¬∑) to obtain ÀÜYui
8:
end for
9:
Merge predicts in different groups ÀÜYu = { ÀÜYui}
10: end for
11: Ensemble predicts in different partitions ÀÜYtest = 1 U
P u ÀÜYu"
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.4196078431372549,"On the usage of Dval and Dho. Guo et al. [7] explicitly states that the validation dataset Dval used
for early stopping can be used for calibration, while most researchers use a hold-out dataset Dho that
is not used during training for calibration[12, 23, 25]. Experimentally, we found that calibrating with
Dval is significantly worse than Dho for existing calibration methods. Interestingly, the grouping
function trained on the Dval can transfer to Dho for improving performance, which means the training
of gœï does not require additional data. We investigate this problem in the supplementary in detail."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.4235294117647059,"An essential characteristic of calibration methods is their ability to preserve accuracy[23], ensuring
that the classes with the highest probabilities remain unchanged after the calibration process.
Theorem 1 (Accuracy-preserving of group calibration). Group calibration with any group-wise
accuracy-preserving base calibration method is also accuracy-preserving."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.42745098039215684,"Proof Sketch. Since the groups are disjoint, group-wise accuracy-preserving remains overall
accuracy-preserving. Ensembles of accuracy-preserving predictions are also accuracy-preserving."
CALIBRATION WITH SEMATIC-AWARE PARTITIONS,0.43137254901960786,"The Theorem. 1 guarantees our group calibration method remains accuracy-preserving with accuracy-
preserving base methods such as temperature scaling[7] or ensemble temperature scaling [23]."
EXPERIMENTS,0.43529411764705883,"3
Experiments"
EXPERIMENTS,0.4392156862745098,"To evaluate the performance of our method under various circumstances, we selected three datasets:
CIFAR10, CIFAR100[31], and Imagenet[1]. We employed different models for each dataset to reflect
our approach‚Äôs calibration capability across various model accuracy levels2. The models used in our
experiments include Resnet[2], VGG[32], Densenet[33], SWIN[34], and ShuffleNet[35]."
EXPERIMENTS,0.44313725490196076,"We randomly partitioned a validation set Dval from the standard training set: CIFAR10 and CIFAR100
adopted 10% of the data for validation, while Imagenet utilized 5%. Additionally, we randomly
sampled 10% of the hold-out data Dho from the standard test set for calibration. We performed 100
different test set splits for each dataset-model combination to obtain reliable results and reported"
EXPERIMENTS,0.4470588235294118,2Code and Appendix are available at https://github.com/ThyrixYang/group_calibration
EXPERIMENTS,0.45098039215686275,"the average performance over 100 trials for each method. We conduct paired t-test[36] to measure
the static significance of the improvements. The hyperparameters of the comparative methods were
tuned based on the corresponding literature with 5-fold cross-validation on the CIFAR10-Resnet152
dataset. We fixed the number of groups at K = 2 and the number of partitions at U = 20, although
20 is not necessarily the optimal value. The strength of regularization was set to Œª = 0.1, following a
similar tuning approach as the comparative methods."
EXPERIMENTAL COMPARISON,0.4549019607843137,"3.1
Experimental comparison"
EXPERIMENTAL COMPARISON,0.4588235294117647,"Table 1: Comparison of accuracy-preserving calibration methods. We utilized bold font to highlight
the statistically superior (p < 0.01) results."
EXPERIMENTAL COMPARISON,0.4627450980392157,"Dataset
Model
Uncal
TS
ETS
IRM(AP)
GC+TS(ours)
GC+ETS(ours)"
EXPERIMENTAL COMPARISON,0.4666666666666667,"CIFAR10
Resnet152
0.0249
0.0086
0.0101
0.0115
0.0079
0.0089
CIFAR10
Shufflenet
0.0464
0.0107
0.0103
0.0146
0.0099
0.0093
CIFAR10
VGG11
0.0473
0.0125
0.0135
0.0157
0.0120
0.0122"
EXPERIMENTAL COMPARISON,0.47058823529411764,"CIFAR100
Densenet121
0.0558
0.0418
0.0289
0.0415
0.0411
0.0280
CIFAR100
Resnet50
0.0580
0.0435
0.0292
0.0424
0.0427
0.0269
CIFAR100
VGG19
0.1668
0.0485
0.0472
0.0476
0.0414
0.0360"
EXPERIMENTAL COMPARISON,0.4745098039215686,"Imagenet
Resnet18
0.0279
0.0178
0.0104
0.0188
0.0173
0.0100
Imagenet
Resnet50
0.0382
0.0182
0.0102
0.0218
0.0174
0.0103
Imagenet
Swin
0.0266
0.0367
0.0218
0.0140
0.0366
0.0193"
EXPERIMENTAL COMPARISON,0.47843137254901963,"Average improvements
-5.03%
-8.92%"
EXPERIMENTAL COMPARISON,0.4823529411764706,"Accuracy preserving methods. We compared our approach with methods that offer accuracy
assurance, including uncalibrated (Uncal), temperature scaling (TS)[7], Ensemble Temperature
Scaling (ETS)[23], and multi-class isotonic regression with accuracy-preserving3 (IRM(AP))[23]
methods. We report the top-label ECE[25]. From Table. (1), it can be observed that our method
achieves the best performance on the majority of datasets, and the improvement is statistically
significant. Another noteworthy aspect is the enhancement our method demonstrates when compared
to the base methods without partitioning, namely GC+TS compared to TS, and GS+ETS compared to
ETS. We have provided the average improvement relative to the corresponding base methods in the
last row of Table 1. Specifically, GC+TS shows an average improvement of 5.03% over TS, while
GC+ETS demonstrates an average improvement of 8.92% over ETS. This indicates that employing
better partitioning strategies can enhance calibration performance."
EXPERIMENTAL COMPARISON,0.48627450980392156,Table 2: Comparison of non-accuracy-preserving calibration methods.
EXPERIMENTAL COMPARISON,0.49019607843137253,"Dataset
Model
Hist.B
Beta
BBQ
DirODIR
GPC
IRM(NAP)
Acc. Dec.
GC+ETS (ours)"
EXPERIMENTAL COMPARISON,0.49411764705882355,"CIFAR10
Resnet152
0.0172
0.0095
0.0097
0.0101
0.0189
0.0087
-0.079%
0.0089
CIFAR10
Shufflenet
0.0322
0.0137
0.0139
0.0158
0.0341
0.0119
-0.050%
0.0093
CIFAR10
VGG11
0.0279
0.0150
0.0137
0.0156
0.0376
0.0119
-0.129%
0.0122"
EXPERIMENTAL COMPARISON,0.4980392156862745,"CIFAR100
Densenet121
0.0632
0.0520
0.0307
0.0533
0.0306
0.0203
-0.149%
0.0280
CIFAR100
Resnet50
0.0618
0.0550
0.0334
0.0553
0.0346
0.0422
-3.686%
0.0269
CIFAR100
VGG19
0.0453
0.0642
0.0446
0.0932
0.1406
0.0470
-5.046%
0.0360"
EXPERIMENTAL COMPARISON,0.5019607843137255,"Imagenet
Resnet18
0.0714
0.0690
0.0483
0.0386
-*
0.0119
-0.027%
0.0100
Imagenet
Resnet50
0.0502
0.0707
0.0482
0.0326
-
0.0107
-0.006%
0.0103
Imagenet
Swin
0.0335
0.0629
0.0478
0.0148
-
0.0110
-0.060%
0.0193"
EXPERIMENTAL COMPARISON,0.5058823529411764,* GPC failed to converge within a day on Imagenet datasets.
EXPERIMENTAL COMPARISON,0.5098039215686274,"Non-accuracy preserving methods. We also compared our method with calibration approaches
that do not guarantee accuracy, including Histogram Binning(Hist.B)[7], Beta Calibration (Beta)[10],
Bayesian Binning into Quantiles (BBQ)[11], Dirichlet calibration with ODIR regularisation
(DirODIR)[12], multi-class isotonic regression without accuracy-preserving (IRM(NAP))[23] and
Gaussian process calibration(GPC)[24]. The comparative results are presented in Table. (2). It
can be observed that our method achieves improved calibration performance while maintaining the"
EXPERIMENTAL COMPARISON,0.5137254901960784,"3We found that IRM with œµ ‚â™10‚àí3 is indeed not accuracy preserving, so we use œµ = 10‚àí3 in IRM(AP) and
œµ = 10‚àí9 in IRM(NAP). We discuss this problem in the supplementary material in detail."
EXPERIMENTAL COMPARISON,0.5176470588235295,"same predictive accuracy. On the majority of datasets, our method either meets or surpasses the
best non-accuracy-preserving methods. We have also included in Table. (2) the influence of the
non-accuracy-preserving method with the lowest ECE on predictive accuracy. It is evident that these
methods generally have a noticeable negative impact on model accuracy. In contrast, our proposed
method preserves the predictive results, ensuring that the predictive accuracy remains unchanged. We
provide all the details and codes of the experiments in the supplementary material, as well as a few
additional evaluation metrics (NLL, Birer score) and methods (vector scaling, isotonic regression,
etc.)[7], which also supports that our method performs best."
EXPERIMENTAL COMPARISON,0.5215686274509804,Number in each group
EXPERIMENTAL COMPARISON,0.5254901960784314,Number in each group
EXPERIMENTAL COMPARISON,0.5294117647058824,Figure 2: Visualization of learned grouping function on CIFAR10.
VERIFYING EFFECTS OF GROUPING FUNCTIONS,0.5333333333333333,"3.2
Verifying effects of grouping functions"
VERIFYING EFFECTS OF GROUPING FUNCTIONS,0.5372549019607843,"Quantitative analysis. We conduct experiments on the constructed setting described in Figure. 1 with
TS and our GC+TS method. TS has ECE=0.0157, and GC+TS has ECE=0.0118. We calculate the
PCE with the partitions learned by our GC method, and PCE of TS=0.0174, PCE of GC+TS=0.0141,
which indicates that GC+TS does improve the overall calibration performance by minimizing PCE."
VERIFYING EFFECTS OF GROUPING FUNCTIONS,0.5411764705882353,"Visualization of learned partitions. To explore the partition functions learned by our method, we
visualized the output results of the grouping function on the CIFAR10-Resnet152 dataset. Specifically,
we calculated the proportion of each class within each group and visualized two representative
partitions in Figure. 2. In the left figure, group 1 primarily consists of airplanes, cars, birds, frogs,
ships, and trucks. These classes may share certain visual characteristics like blue sky and water, which
implies that the model‚Äôs predictions might exhibit systematic overconfidence or underconfidence
within these categories. Applying a group-specific calibration function to this group can help mitigate
miscalibration issues. In the right figure, group 1 mainly comprises cats, deer, and dogs, which also
share some visual features. To conclude, our proposed method can implicitly uncover meaningful
partitions from the data despite not directly defining a partitioning approach."
ABLATION STUDY,0.5450980392156862,"3.3
Ablation study"
ABLATION STUDY,0.5490196078431373,"We conducted experiments on the CIFAR10-Resnet152 dataset to investigate the impact of three
hyperparameters in our method: the number of partitions, the number of groups within each par-
tition, and the regularization strength used in the grouping function. The results are presented
in Figure. 3. We observed that as the number of groups decreases, both NLL (negative log-
likelihood) and ECE generally exhibit a monotonic increase. We attribute this trend to the fact
that increasing the number of groups leads to fewer data points within each group, exacerbat-
ing model overfitting. On the other hand, as the number of partitions increases, the calibration
performance consistently improves. This finding suggests that training on multiple distinct par-
titions can enhance the model‚Äôs performance across various partitions, thereby improving the
model‚Äôs overall performance. This can be viewed as incorporating prior knowledge into the model,"
ABLATION STUDY,0.5529411764705883,0.0078
ABLATION STUDY,0.5568627450980392,0.00785
ABLATION STUDY,0.5607843137254902,0.0079
ABLATION STUDY,0.5647058823529412,0.00795 0.008
ABLATION STUDY,0.5686274509803921,0.00805
ABLATION STUDY,0.5725490196078431,0.0081
ABLATION STUDY,0.5764705882352941,0.00815
ABLATION STUDY,0.5803921568627451,0.14465
ABLATION STUDY,0.5843137254901961,0.1447
ABLATION STUDY,0.5882352941176471,0.14475
ABLATION STUDY,0.592156862745098,0.1448
ABLATION STUDY,0.596078431372549,0.14485
ABLATION STUDY,0.6,0.1449
ABLATION STUDY,0.6039215686274509,0.14495 0.145
ABLATION STUDY,0.6078431372549019,0.14505
ABLATION STUDY,0.611764705882353,"1
2
5
10
20
50
100 ECE NLL"
ABLATION STUDY,0.615686274509804,"NLL
ECE"
ABLATION STUDY,0.6196078431372549,"0.0078
0.00785
0.0079
0.00795
0.008
0.00805
0.0081
0.00815
0.0082
0.00825
0.0083"
ABLATION STUDY,0.6235294117647059,0.1444
ABLATION STUDY,0.6274509803921569,0.1446
ABLATION STUDY,0.6313725490196078,0.1448 0.145
ABLATION STUDY,0.6352941176470588,0.1452
ABLATION STUDY,0.6392156862745098,0.1454
ABLATION STUDY,0.6431372549019608,0.1456
ABLATION STUDY,0.6470588235294118,"2
3
4
5
6
7
8 ECE NLL"
ABLATION STUDY,0.6509803921568628,"NLL
ECE"
ABLATION STUDY,0.6549019607843137,"(a) Number of groups
(b) Number of partitions"
ABLATION STUDY,0.6588235294117647,Figure 3: The influence of the number of partitions and the number of groups in each partition.
ABLATION STUDY,0.6627450980392157,"0.0078
0.0079
0.0079
0.0080
0.0080
0.0081
0.0081
0.0082
0.0082
0.0083
0.0083
0.0084"
ABLATION STUDY,0.6666666666666666,0.1444
ABLATION STUDY,0.6705882352941176,0.1446
ABLATION STUDY,0.6745098039215687,0.1448
ABLATION STUDY,0.6784313725490196,0.1450
ABLATION STUDY,0.6823529411764706,0.1452
ABLATION STUDY,0.6862745098039216,0.1454
ABLATION STUDY,0.6901960784313725,0.1456
ABLATION STUDY,0.6941176470588235,1.E+001.E-01 1.E-02 1.E-03 1.E-04 1.E-05 1.E-06 ECE NLL
ABLATION STUDY,0.6980392156862745,"NLL
ECE"
ABLATION STUDY,0.7019607843137254,Figure 4: Influence of Œª
ABLATION STUDY,0.7058823529411765,"indicating that the model‚Äôs predictions should be well-
calibrated across any partition. The influence of the reg-
ularization strength hyperparameter in the grouping func-
tion is depicted in Figure. 4. It can be observed that when
the regularization strength is too high (i.e., a value of 1),
the calibration performance is poorer. This suggests that
in such cases, the expressive power of the grouping func-
tion is insufficient to learn meaningful groups that aid in
calibration. Conversely, when the regularization strength
is too low, the performance also deteriorates because the
grouping function‚Äôs expressive power is excessive, leading
to the learning of partition functions that are specific to
the training set and lack generalization."
RELATED WORK,0.7098039215686275,"4
Related Work"
RELATED WORK,0.7137254901960784,"We provide a concise overview of recent academic research on the definition, evaluation, and
advancements in calibration methods."
RELATED WORK,0.7176470588235294,"Measures of calibration.
In safety-critical applications[3‚Äì6] and tasks involving probability
estimation[37‚Äì42], it is crucial to ensure that models not only deliver high accuracy but also provide
predicted probabilities that accurately represent the genuine likelihood of correctness. As discussed
in Section. 2.1, calibration, being a collective concept without direct labels, is typically evaluated
by binning the predicted probabilities using different binning approaches. Various studies have
proposed different binning methods, each focusing on a different aspect. For example, Kumar et al.
[25] proposed binning based on the maximum predicted probability, Kull et al. [12] introduced
binning using probabilities for all classes, and Zhang et al. [23], Popordanoska et al. [43], Kumar
et al. [44] proposed kernel methods, which can be seen as a smoothed binning approach. In our
framework, these methods can be considered as applying different grouping functions, where the
grouping functions solely utilize the predicted probabilities. Additionally, other studies have explored
alternative characteristics of calibration. For instance, Vaicenavicius et al. [22] and Gupta et al.
[14] proposed hypothesis testing methods to assess whether the predicted probabilities approximate
calibration, which is still defined solely on the predicted probabilities. Proper scoring rules such as
likelihoods and Brier scores are also used to evaluate calibration[26, 45], which are typically used as
a training loss to improve calibration."
RELATED WORK,0.7215686274509804,"The calibration considering fairness[46, 47] is also highly related to this work, where the calibration
within different populations is considered. Perez-Lebel et al. [48] established an explained component
as a metric that lower-bounds the grouping loss in the proper scoring rule theory. However, our
work concentrates on learning a better grouping function to improve holistic calibration performance,
rather than calibrating some given groups[47]."
RELATED WORK,0.7254901960784313,"Improving calibration of deep models. The calibration of deep models can be categorized into
two main directions. The first direction involves calibrating the model during the training phase by
modifying the training process itself[23, 26, 49‚Äì51]. This may include using specialized loss functions
tailored for calibration[52], employing ensemble methods[26, 53], or data augmentation[54], etc."
RELATED WORK,0.7294117647058823,"However, such approaches require modifications to the training phase and may entail relatively higher
computational costs."
RELATED WORK,0.7333333333333333,"The second direction focuses on calibrating the model during the prediction phase. These methods
are highly related to the definition of calibration metrics, which involves minimizing the discrepancy
between model predictions and aggregated statistics within each bin. The simplest method is
histogram binning[7, 37], where predicted probabilities are directly adjusted to match the actual
class proportions within each bin. BBQ[11] further proposes an ensemble approach that calibrates
results obtained with different numbers of bins. The transformation functions applied to predictions
can also be improved beyond averaging within bins. For example, Beta distribution can fit the
probability transformation for binary classification[10], while Dirichlet distribution can be employed
for multi-class transformation[12]. New families of transformation functions have been proposed
to preserve the ordering of predicted probabilities while improving calibration performance[13, 23].
Our method is orthogonal to these calibration methods since the underlying motivation of existing
methods is to minimize the calibration error within a single group, while our method offers greater
flexibility as we can employ different calibration functions for different groups."
RELATED WORK,0.7372549019607844,"Some recent work also proposed calibration with given groups. Multicalibration[55] proposes
an algorithm for learning a multi-calibrated predictor with respect to any given subpopulation
class. Durfee et al. [56] proposed to group the feature space with decision trees, then apply Platt
scaling within each group, which concentrates on calibration ranking metrics (AUC), while our work
concentrates on PCE metrics and proper scoring rules (Log-Likelihood). Y√ºksekg√∂n√ºl et al. [57]
and Xiong et al. [58] propose heuristic grouping functions and apply a specific calibration method
within each group, while we aim to propose a learning-based method that can generate partitions in
an end-to-end manner."
CONCLUSION AND LIMITATIONS,0.7411764705882353,"5
Conclusion and limitations"
CONCLUSION AND LIMITATIONS,0.7450980392156863,"This paper proposes a novel approach to define model uncertainty calibration from the perspective
of partitioning the input space, thereby unifying previous binning-based calibration metrics within
a single framework. Additionally, we extend the notion of calibration beyond output probabilities,
allowing it to be defined on semantically relevant partitions. We establish a connection between
calibration and accuracy metrics using semantically relevant partitions. Furthermore, our analysis
inspires introducing a method based on learning grouping functions to discover improved partitioning
patterns, thus aiding in learning the calibration function. Our framework holds substantial potential
for further development, such as optimizing subset selection methods and refining model design,
which will be our future work."
CONCLUSION AND LIMITATIONS,0.7490196078431373,"Limitations. Increasing the number of partitions or employing more complex grouping models will
result in higher computational complexity, which is a limitation of our method. Nevertheless, when
compared to other Bayesian methods[11, 12, 24], our method demonstrates notably faster speed. We
present analysis and experimental comparisons of computational complexities in the supplementary
material. Due to the reliance of our method on the features extracted from deep models, it is not
applicable for calibrating non-deep models such as tree models[59] and SVMs[12, 60]."
CONCLUSION AND LIMITATIONS,0.7529411764705882,Acknowledgements
CONCLUSION AND LIMITATIONS,0.7568627450980392,This work is supported by the National Key R&D Program of China (2022ZD0114805).
REFERENCES,0.7607843137254902,References
REFERENCES,0.7647058823529411,"[1] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-
Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211‚Äì252,
2015."
REFERENCES,0.7686274509803922,"[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016."
REFERENCES,0.7725490196078432,"[3] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Man√©.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016."
REFERENCES,0.7764705882352941,"[4] Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521
(7553):452‚Äì459, 2015."
REFERENCES,0.7803921568627451,"[5] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016."
REFERENCES,0.7843137254901961,"[6] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and
Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks.
nature, 542(7639):115‚Äì118, 2017."
REFERENCES,0.788235294117647,"[7] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In ICML, 2017."
REFERENCES,0.792156862745098,"[8] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias
Humt, Jianxiang Feng, Anna M. Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muham-
mad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in
deep neural networks. CoRR, abs/2107.03342, 2021."
REFERENCES,0.796078431372549,"[9] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad
Ghavamzadeh, Paul W. Fieguth, Xiaochun Cao, Abbas Khosravi, U. Rajendra Acharya, Vladimir
Makarenkov, and Saeid Nahavandi. A review of uncertainty quantification in deep learning:
Techniques, applications and challenges. Inf. Fusion, 76:243‚Äì297, 2021."
REFERENCES,0.8,"[10] Meelis Kull, Telmo de Menezes e Silva Filho, and Peter A. Flach. Beta calibration: a well-
founded and easily implemented improvement on logistic calibration for binary classifiers. In
AISTATS, 2017."
REFERENCES,0.803921568627451,"[11] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining well calibrated
probabilities using bayesian binning. In AAAI, 2015."
REFERENCES,0.807843137254902,"[12] Meelis Kull, Miquel Perello Nieto, Markus K√§ngsepp, Telmo Silva Filho, Hao Song, and Peter
Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with
dirichlet calibration. In NeurIPS, 2019."
REFERENCES,0.8117647058823529,"[13] Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron Boots. Intra
order-preserving functions for calibration of multi-class neural networks. In NeurIPS, 2020."
REFERENCES,0.8156862745098039,"[14] Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchis-
escu, and Richard Hartley. Calibration of neural networks using splines. In ICLR, 2021."
REFERENCES,0.8196078431372549,"[15] Yu Bai, Song Mei, Huan Wang, and Caiming Xiong. Don‚Äôt just blame over-parametrization for
over-confidence: Theoretical analysis of calibration in binary classification. In ICML, 2021."
REFERENCES,0.8235294117647058,"[16] Christian Tomani and Florian Buettner. Towards trustworthy predictions from deep neural
networks with fast adversarial calibration. In AAAI, 2021."
REFERENCES,0.8274509803921568,"[17] Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, and Mohsen Ali.
Towards improving calibration in object detection under domain shift. In NeurIPS, 2022."
REFERENCES,0.8313725490196079,"[18] Rajeev Verma and Eric T. Nalisnick. Calibrated learning to defer with one-vs-all classifiers. In
ICML, 2022."
REFERENCES,0.8352941176470589,"[19] Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep
learning using calibrated regression. In ICML, 2018."
REFERENCES,0.8392156862745098,"[20] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil
Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks.
In NeurIPS, 2021."
REFERENCES,0.8431372549019608,"[21] Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass
probability estimates. In KDD, 2002."
REFERENCES,0.8470588235294118,"[22] Juozas Vaicenavicius, David Widmann, Carl R. Andersson, Fredrik Lindsten, Jacob Roll, and
Thomas B. Sch√∂n. Evaluating model calibration in classification. In AISTATS, 2019."
REFERENCES,0.8509803921568627,"[23] Jize Zhang, Bhavya Kailkhura, and Thomas Yong-Jin Han. Mix-n-match : Ensemble and
compositional methods for uncertainty calibration in deep learning. In ICML, 2020."
REFERENCES,0.8549019607843137,"[24] Jonathan Wenger, Hedvig Kjellstr√∂m, and Rudolph Triebel. Non-parametric calibration for
classification. In AISTATS, 2020."
REFERENCES,0.8588235294117647,"[25] Ananya Kumar, Percy Liang, and Tengyu Ma. Verified uncertainty calibration. In NeurIPS,
2019."
REFERENCES,0.8627450980392157,"[26] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In NIPS, 2017."
REFERENCES,0.8666666666666667,"[27] Eyke H√ºllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine
learning: an introduction to concepts and methods. Mach. Learn., 110(3):457‚Äì506, 2021."
REFERENCES,0.8705882352941177,"[28] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classification. IEEE Trans. Inf.
Theory, 13(1):21‚Äì27, 1967."
REFERENCES,0.8745098039215686,"[29] Takashi Ishida, Ikko Yamane, Nontawat Charoenphakdee, Gang Niu, and Masashi Sugiyama.
Is the performance of my deep network too good to be true? a direct approach to estimating the
bayes error in binary classification. In ICLR, 2023."
REFERENCES,0.8784313725490196,"[30] Raghu Bollapragada, Dheevatsa Mudigere, Jorge Nocedal, Hao-Jun Michael Shi, and Ping
Tak Peter Tang. A progressive batching L-BFGS method for machine learning. In ICML, 2018."
REFERENCES,0.8823529411764706,"[31] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.8862745098039215,"[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. In ICLR, 2015."
REFERENCES,0.8901960784313725,"[33] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In CVPR, 2017."
REFERENCES,0.8941176470588236,"[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021."
REFERENCES,0.8980392156862745,"[35] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient
convolutional neural network for mobile devices. In CVPR, 2018."
REFERENCES,0.9019607843137255,"[36] Amanda Ross and Victor L Willson. Basic and advanced statistical tests: Writing results
sections and creating tables and figures. Springer, 2018."
REFERENCES,0.9058823529411765,"[37] Sheng Liu, Aakash Kaku, Weicheng Zhu, Matan Leibovich, Sreyas Mohan, Boyang Yu, Haoxi-
ang Huang, Laure Zanna, Narges Razavian, Jonathan Niles-Weed, and Carlos Fernandez-Granda.
Deep probability estimation. In ICML, 2022."
REFERENCES,0.9098039215686274,"[38] Jia-Qi Yang and De-Chuan Zhan. Generalized delayed feedback model with post-click informa-
tion in recommender systems. In NeurIPS, 2022."
REFERENCES,0.9137254901960784,"[39] Jia-Qi Yang, Ke-Bin Fan, Hao Ma, and De-Chuan Zhan. Rid-noise: Towards robust inverse
design under noisy environments. In AAAI, 2022."
REFERENCES,0.9176470588235294,"[40] Nengjun Zhu, Jian Cao, Yanchi Liu, Yang Yang, Haochao Ying, and Hui Xiong. Sequential
modeling of hierarchical user intention and preference for next-item recommendation. In
WSDM, 2020."
REFERENCES,0.9215686274509803,"[41] Yang Yang, Jia-Qi Yang, Ran Bao, De-Chuan Zhan, Hengshu Zhu, Xiaoru Gao, Hui Xiong, and
Jian Yang. Corporate relative valuation using heterogeneous multi-modal graph neural network.
IEEE Trans. Knowl. Data Eng., 35(1):211‚Äì224, 2023."
REFERENCES,0.9254901960784314,"[42] Da-Wei Zhou, Yang Yang, and De-Chuan Zhan. Learning to classify with incremental new
class. IEEE Trans. Neural Networks Learn. Syst., 33(6):2429‚Äì2443, 2022."
REFERENCES,0.9294117647058824,"[43] Teodora Popordanoska, Raphael Sayer, and Matthew B. Blaschko. A consistent and differen-
tiable lp canonical calibration error estimator. In NeurIPS, 2022."
REFERENCES,0.9333333333333333,"[44] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural
networks from kernel mean embeddings. In ICML, 2018."
REFERENCES,0.9372549019607843,"[45] Sebastian Gruber and Florian Buettner. Better uncertainty calibration via proper scores for
classification and beyond. In NeurIPS, 2022."
REFERENCES,0.9411764705882353,"[46] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A
survey on bias and fairness in machine learning. ACM Comput. Surv., 54(6):115:1‚Äì115:35,
2022."
REFERENCES,0.9450980392156862,"[47] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon M. Kleinberg, and Kilian Q. Weinberger. On
fairness and calibration. In NeurIPS, 2017."
REFERENCES,0.9490196078431372,"[48] Alexandre Perez-Lebel, Marine Le Morvan, and Ga√´l Varoquaux. Beyond calibration: estimat-
ing the grouping loss of modern neural networks. In ICLR, 2023."
REFERENCES,0.9529411764705882,"[49] Jiacheng Cheng and Nuno Vasconcelos. Calibrating deep neural networks by pairwise con-
straints. In CVPR, 2022."
REFERENCES,0.9568627450980393,"[50] Wesley J. Maddox, Pavel Izmailov, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon
Wilson. A simple baseline for bayesian uncertainty in deep learning. In NeurIPS, 2019."
REFERENCES,0.9607843137254902,"[51] Ranganath Krishnan and Omesh Tickoo. Improving model calibration with accuracy versus
uncertainty optimization. In NeurIPS, 2020."
REFERENCES,0.9647058823529412,"[52] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip H. S. Torr, and
Puneet K. Dokania. Calibrating deep neural networks using focal loss. In NeurIPS, 2020."
REFERENCES,0.9686274509803922,"[53] Andrew Gordon Wilson and Pavel Izmailov.
Bayesian deep learning and a probabilistic
perspective of generalization. In NeurIPS, 2020."
REFERENCES,0.9725490196078431,"[54] Sunil Thulasidasan, Gopinath Chennupati, Jeff A. Bilmes, Tanmoy Bhattacharya, and Sarah
Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural
networks. In NeurIPS, 2019."
REFERENCES,0.9764705882352941,"[55] √örsula H√©bert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Multicalibra-
tion: Calibration for the (computationally-identifiable) masses. In ICML, volume 80, pages
1944‚Äì1953, 2018."
REFERENCES,0.9803921568627451,"[56] David Durfee, Aman Gupta, and Kinjal Basu. Heterogeneous calibration: A post-hoc model-
agnostic framework for improved generalization. CoRR, abs/2202.04837, 2022."
REFERENCES,0.984313725490196,"[57] Mert Y√ºksekg√∂n√ºl, Linjun Zhang, James Zou, and Carlos Guestrin. Beyond confidence:
Reliable models should also consider atypicality. CoRR, abs/2305.18262, 2023."
REFERENCES,0.9882352941176471,"[58] Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, and Bryan Hooi.
Proximity-informed calibration for deep neural networks. CoRR, abs/2306.04590, 2023."
REFERENCES,0.9921568627450981,"[59] Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision
trees and naive bayesian classifiers. In ICML, 2001."
REFERENCES,0.996078431372549,"[60] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. Advances in large margin classifiers, 10(3):61‚Äì74, 1999."
