Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,ABSTRACT
ABSTRACT,0.0016233766233766235,"A key challenge in building theoretical foundations for deep learning is the
complex optimization dynamics of neural networks, resulting from the high-
dimensional interactions between the large number of network parameters. Such
non-trivial dynamics lead to intriguing model behaviors such as the phenomenon
of “double descent” of the generalization error. The more commonly studied as-
pect of this phenomenon corresponds to model-wise double descent where the
test error exhibits a second descent with increasing model complexity, beyond the
classical U-shaped error curve. In this work, we investigate the origins of the
less studied epoch-wise double descent in which the test error undergoes two non-
monotonous transitions, or descents as the training time increases. By leveraging
tools from statistical physics, we study a linear teacher-student setup exhibiting
epoch-wise double descent similar to that in deep neural networks. In this setting,
we derive closed-form analytical expressions for the evolution of generalization
error over training. We ﬁnd that double descent can be attributed to distinct fea-
tures being learned at different scales: as fast-learning features overﬁt, slower-
learning features start to ﬁt, resulting in a second descent in test error. We validate
our ﬁndings through numerical experiments where our theory accurately predicts
empirical ﬁndings and remains consistent with observations in deep neural net-
works."
INTRODUCTION,0.003246753246753247,"1
INTRODUCTION"
INTRODUCTION,0.00487012987012987,"Classical wisdom in statistical learning theory predicts a trade-off between the generalization ability
of a machine learning model and its complexity, with highly complex models less likely to gener-
alize well (Friedman et al., 2001). If the number of parameters measures complexity, deep learning
models sometimes go against this prediction (Zhang et al., 2016): deep neural networks trained by
stochastic gradient descent exhibit a so-called double descent behavior (Belkin et al., 2019b) with
increasing model parameters. Speciﬁcally, with increasing complexity, the generalization error ﬁrst
obeys the classical U-shaped curve consistent with statistical learning theory. However, a second
regime emerges as the number of parameters is further increased past a transition threshold where
generalization error drops again, hence the “double descent” or more accurately model-wise double
descent (Nakkiran et al., 2019)."
INTRODUCTION,0.006493506493506494,"Nakkiran et al. (2019) showed that the phenomenon of double descent is not limited to varying
model size but is also observed as a function of training time or epochs. In this case as well, the
so-called epoch-wise double descent is in apparent contradiction with the classical understanding
of over-ﬁtting (Vapnik, 1998), where one expects that longer training of a sufﬁciently large model
beyond a certain threshold should result in over-ﬁtting. This has important implications for prac-
titioners and raises questions about one of the most widely used regularization method in deep
learning (Goodfellow et al., 2016): early stopping. Indeed, while one might expect early stopping
to prevent over-ﬁtting, it might in fact prevent models from being trained at their fullest potential."
INTRODUCTION,0.008116883116883116,"Since the 1990s, there has been much interest in understanding the origins of non-trivial general-
ization behaviors of neural networks (Opper, 1995; Opper & Kinzel, 1996). The authors of Krogh
& Hertz (1992b) were among the ﬁrst to provide theoretical explanations for (model-wise) double
descent in linear models. Summarily, at intermediate levels of complexity, where the model size is
equal to the number of training examples, the model is very sensitive to noise in training data and
hence, generalizes poorly. This sensitivity to noise reduces if the model complexity is either de-"
INTRODUCTION,0.00974025974025974,Under review as a conference paper at ICLR 2022
INTRODUCTION,0.011363636363636364,"z 2 Rd
x 2 Rd"
INTRODUCTION,0.012987012987012988,ˆW 2 Rd
INTRODUCTION,0.01461038961038961,ˆy := xT W
INTRODUCTION,0.016233766233766232,x := F T z
INTRODUCTION,0.017857142857142856,"Anisotropic 
input features"
INTRODUCTION,0.01948051948051948,ˆW 2 Rd
INTRODUCTION,0.021103896103896104,y := zT W + ✏
INTRODUCTION,0.022727272727272728,"Student
Noisy teacher"
INTRODUCTION,0.024350649350649352,Training time
INTRODUCTION,0.025974025974025976,"Isotropic 
latent features"
INTRODUCTION,0.027597402597402596,"Figure 1: Left: The teacher is the data generating process that operates on isotropic Gaussian inputs
z. The student is trained on a dataset generated by the teacher, D = {xi, yi}n
i=1 where x := F T z
follow an anisotropic Gaussian distribution such that the directions with larger/smaller variance are
learned faster/slower. The condition number of F determines how much faster some features are
learned than the others. One can think of z as the latent factors of variation on which the teacher
operates, while x can be thought as the pixels that the student learns from. Right: The generalization
error as the training time proceeds. (top): The case where only the fast-learning feature or slow-
learning feature are trained. (bottom): The case where both features. Features that are learned on
a faster time-scale are responsible for the classical U-shaped generalization curve, while the second
descent can be attributed to the features that are learned at a slower time-scale."
INTRODUCTION,0.02922077922077922,"creased or increased. More recently, the double descent phenomena has been also studied for more
complex models such as two-layer neural networks and random feature models (Ba et al., 2019; Mei
& Montanari, 2019; D’Ascoli et al., 2020; Gerace et al., 2020)."
INTRODUCTION,0.030844155844155844,"The majority of previous work in this direction focuses on understanding the asymptotic behavior
of model performance, i.e., where training time t →∞. In recent years, there has been an interest
in studying the non-asymptotic (ﬁnite training time) performance, suggesting that several intriguing
properties of neural networks can be attributed to different features being learned at different scales.
Among the limited work studying the particular epoch-wise double descent, Nakkiran et al. (2019)
introduces the notion of effective model complexity and hypothesizes that it increases with training
time and hence uniﬁes both model-wise and epoch-wise double descent. Through a combination of
theory and empirical results, Heckel & Yilmaz (2020) ﬁnd that the dynamics of evolution of single
and two layer networks under gradient descent, can be perceived to be the superposition of two
bias/variance curves with different minima times, thus leading to non-monotonic test error curves."
INTRODUCTION,0.032467532467532464,"In this work, we build on B¨os et al. (1993); B¨os (1998); Advani & Saxe (2017); Mei & Montanari
(2019) which analyze model-wise double descent through the lens of linear models, to probe the
origins of epoch-wise double descent. Particularly,"
INTRODUCTION,0.03409090909090909,"• We introduce a linear teacher-student model which, despite its simplicity, exhibits some of
intriguing properties of generalization dynamics in deep neural networks. (Section 2.1)"
INTRODUCTION,0.03571428571428571,"• In the limit of high dimensions, we leverage the replica method developed in statistical
physics to derive closed-form expressions for the generalization dynamics of our teacher-
student setup, as a function of training time and regularization strength. (Section 2.2)"
INTRODUCTION,0.037337662337662336,"• Consistent with recent ﬁndings, we provide an explanation for the existence of epoch-wise
double descent through the lens of multi-scale feature learning. (Figure 1)"
INTRODUCTION,0.03896103896103896,"• We perform simulation experiments to validate our analytical predictions. We also conduct
experiments with deep networks, showing that our teacher-student setup exhibits general-
ization behavior which is qualitatively similar to that of deep networks. (Figure 2)"
INTRODUCTION,0.040584415584415584,Under review as a conference paper at ICLR 2022
ANALYTICAL RESULTS,0.04220779220779221,"2
ANALYTICAL RESULTS"
ANALYTICAL RESULTS,0.04383116883116883,"Stochastic Gradient Descent (SGD) — the de facto optimization algorithm for neural networks —
exhibits complex dynamics arising from a large number of parameters (Kunin et al., 2020). How-
ever, it is possible to describe some aspects of the high-dimensional microscopic dynamics of neural
networks in terms of low-dimensional understandable macroscopic entities. In a series of seminal
papers by Gardner (Gardner, 1988; Gardner & Derrida, 1988; 1989), the replica method of statisti-
cal physics was adopted to derive expressions describing the generalization behavior of large linear
models trained using SGD. In this paper, we employ Gardner’s analysis to build upon an established
line of work studying linear and generalized linear models (Seung et al., 1992; Kabashima et al.,
2009; Krzakala et al., 2012). While most of previous work study the asymptotic (t →∞) gener-
alization behavior, we adapt these methods to study transient learning dynamics of generalization
for ﬁnite training time. In the following, we ﬁrst introduce a teacher-student model that exhibits
interesting characteristics of modern neural networks. We then adapt the replica method to study the
generalization performance as a function of training time and amount of regularization."
A TEACHER-STUDENT SETUP,0.045454545454545456,"2.1
A TEACHER-STUDENT SETUP"
A TEACHER-STUDENT SETUP,0.04707792207792208,"Teacher:
We study a supervised linear regression problem in which the training labels y, are
generated by a noisy linear model (Figure 1),"
A TEACHER-STUDENT SETUP,0.048701298701298704,"y := y∗+ ϵ,
y∗:= zT W,
zi ∼N(0, 1
√"
A TEACHER-STUDENT SETUP,0.05032467532467533,"d
),
(1)"
A TEACHER-STUDENT SETUP,0.05194805194805195,"where z ∈Rd is the teacher’s input and y∗, y ∈R are the teacher’s noiseless and noisy outputs,
respectively. W ∈Rd represents the (ﬁxed) weights of the teacher and ϵ ∈R is the noise. Both
W and ϵ are drawn i.i.d. from Gaussian distributions with zero means and variances of 1 and σ2
ϵ ,
respectively.
Student:
A student model is correspondingly chosen to be a similar shallow network with train-
able weights ˆW ∈Rd. The student model is trained on n training pairs {(xµ, yµ)}n
µ=1, with the
labels yµ being generated by the above teacher network, as,"
A TEACHER-STUDENT SETUP,0.05357142857142857,"ˆy := xT ˆW,
s.t.
x := F T z,
(2)"
A TEACHER-STUDENT SETUP,0.05519480519480519,"where the matrix F ∈Rd×d is a predeﬁned and ﬁxed modulation matrix regulating the student’s
access to the true input z. One can think of z as the latent factors of variation on which the teacher
operates, while x can be thought as the pixels that the student learns from.
Learning paradigm:
To train our student network, we use stochastic gradient descent (SGD) on
the regularized mean squared loss, evaluated on the n training examples as,"
A TEACHER-STUDENT SETUP,0.056818181818181816,LT := 1
N,0.05844155844155844,"2n n
X"
N,0.060064935064935064,"µ=1
(yµ −ˆyµ)2 + λ"
N,0.06168831168831169,"2 || ˆW||2
2
(3)"
N,0.0633116883116883,"where λ ∈[0, ∞) is the regularization coefﬁcient. Optimizing Eq. 3 with stochastic gradient descent
(SGD) yields the typical update rule,
ˆWt ←ˆWt−1 −η∇ˆ
W LT + ξ,
(4)"
N,0.06493506493506493,"in which t denotes the training step and η is the learning rate. Additionally, ξ ∼N(0, σ2
ξ) models
the stochasticity noise of the optimization algorithm (Bottou et al., 1991)."
N,0.06655844155844155,"Macroscopic variables:
The quantity of interest in this work, is the expected generalization error
of the student, determined by averaging the student’s error over all possible input-target pairs and
noise realizations, as,"
N,0.06818181818181818,LG := 1
EZ,0.0698051948051948,"2Ez

(y∗−ˆy)2
.
(5)"
EZ,0.07142857142857142,"As shown in B¨os et al. (1993), if n, d →∞with a constant ratio n"
EZ,0.07305194805194805,"d < ∞, Eq. 5 can be written as a
function of two macroscopic scalar variables R, Q ∈R,"
EZ,0.07467532467532467,LG = 1
EZ,0.0762987012987013,"2(1 + Q −2R),
(6)"
EZ,0.07792207792207792,Under review as a conference paper at ICLR 2022
EZ,0.07954545454545454,"where σ2
ϵ is the variance of the teacher’s output noise and,"
EZ,0.08116883116883117,R := 1
EZ,0.08279220779220779,"dW T F ˆW,
Q := 1"
EZ,0.08441558441558442,"d
ˆW T F T F ˆW,
(7)"
EZ,0.08603896103896104,See App. B.1 for the proof.
EZ,0.08766233766233766,"Both R and Q have clear interpretations; R is the dot-product between the teacher’s weights W
and the student’s modulated weights F ˆW, hence can be interpreted as the alignment between the
teacher and the student. Similarly, Q can be interpreted as the student’s modulated norm. The
negative sign of R in Eq. 6 suggests that the larger R is, the smaller the generalization error gets.
At the same time, Q appears with a positive sign suggesting the students with smaller (modulated)
norm generalize better."
EZ,0.08928571428571429,"As a remark, note that both R and Q are functions of ˆW which itself is a function of training iteration
t and the regularization strength λ. Therefore, hereafter, we denote the above quantities as LG(t, λ),
R(t, λ), and Q(t, λ)."
MAIN RESULTS,0.09090909090909091,"2.2
MAIN RESULTS"
MAIN RESULTS,0.09253246753246754,"In this Section, we present our main analytical results, with Section 2.3 containing a sketch of our
derivations. For brevity of the results, here, we only present the results for σ2
ϵ = λ = 0. See App. B
for the general case and the detailed proofs."
MAIN RESULTS,0.09415584415584416,"General matrix F .
Let Z := [zµ]n
µ=1 ∈Rn×d and X := [xµ]n
µ=1 ∈Rn×d denote the input
matrices for the teacher and student such that X := ZF. For a general modulation matrix F, the
input covariance matrix has the following singular value decomposition (SVD),"
MAIN RESULTS,0.09577922077922078,"XT X = F T ZT ZF = V ΛV T ,
(8)"
MAIN RESULTS,0.09740259740259741,"in which the diagonal matrix Λ contains the eigenvalues of the student’s input covariance matrix.
Solving the dynamics of gradient descent as in Eq. 4, we arrive at the following exact analytical
expressions for R(t) and Q(t),"
MAIN RESULTS,0.09902597402597403,R(t) = 1
MAIN RESULTS,0.10064935064935066,"dTr(D)
where,
D :=
 
I −[I −ηΛ]t
,
(9)"
MAIN RESULTS,0.10227272727272728,Q(t) = 1
MAIN RESULTS,0.1038961038961039,"dTr
 
AT A

where,
A := FV DV T F −1,
(10)"
MAIN RESULTS,0.10551948051948051,in which Tr(.) is the trace operator. See App. B.2 the proof.
MAIN RESULTS,0.10714285714285714,"Remark: The solution in Eqs. 9 and 10 are exact, however, they require the empirical computation of
the eigenvalues Λ. Below, we treat a special case of the dynamics that allow us to derive approximate
solutions that do not explicitly depend on Λ."
MAIN RESULTS,0.10876623376623376,"Special case: Fast and slow features.
We now study a case where the modulation matrix F has
a speciﬁc structure described in Assumption 1.
Assumption 1. The modulation matrix, F, under a SVD, F := UΣV T has two sets of singular
values such that the ﬁrst p singular values are equal to σ1 and the remaining d −p singular values
are equal to σ2. We let the condition number of F to be denoted by κ := σ1"
MAIN RESULTS,0.11038961038961038,σ2 > 1.
MAIN RESULTS,0.11201298701298701,"By employing the replica method of statistical physics (Gardner, 1988; Gardner & Derrida, 1988),
we now derive approximate expressions for R(t) and Q(t). To begin with, we ﬁrst deﬁne the fol-
lowing auxiliary variables,"
MAIN RESULTS,0.11363636363636363,α1 := n
MAIN RESULTS,0.11525974025974026,"p , α2 :=
n
d −p,
˜λ1 := d"
MAIN RESULTS,0.11688311688311688,"p
1
ησ2
1t, ˜λ2 :=
d
d −p
1
ησ2
2t,
(11)"
MAIN RESULTS,0.1185064935064935,"and also let,"
MAIN RESULTS,0.12012987012987013,"ai = 1 +
2˜λi"
MAIN RESULTS,0.12175324675324675,"(1 −αi −˜λi) +
q"
MAIN RESULTS,0.12337662337662338,"(1 −αi −˜λi)2 + 4˜λi
,
for
i ∈{1, 2}.
(12)"
MAIN RESULTS,0.125,Under review as a conference paper at ICLR 2022
MAIN RESULTS,0.1266233766233766,"The closed-from scalar expression for R(t) is then given by,"
MAIN RESULTS,0.12824675324675325,"R(t) = R1 + R2,
where,
R1 :=
n
a1d,
and,
R2 :=
n
a2d
(13)"
MAIN RESULTS,0.12987012987012986,"For Q(t), we accordingly deﬁne two more auxiliary variables,"
MAIN RESULTS,0.1314935064935065,"bi =
αi
a2
i −αi
,
ci = 1 −2Ri −n"
MAIN RESULTS,0.1331168831168831,"d
2 −ai"
MAIN RESULTS,0.13474025974025974,"ai
for
i ∈{1, 2},
(14)"
MAIN RESULTS,0.13636363636363635,"with which the closed-from scalar expression for Q(t) reads,"
MAIN RESULTS,0.137987012987013,"Q(t) = Q1 + Q2,
where,
Q1 := b1b2c2 + b1c1"
MAIN RESULTS,0.1396103896103896,"1 −b1b2
,
and,
Q2 := b1b2c1 + b2c2"
MAIN RESULTS,0.14123376623376624,"1 −b1b2
.
(15)"
MAIN RESULTS,0.14285714285714285,"By plugging Eqs. 13 and 15 into Eq. 6, one obtains a closed-form expression for LG(t) as a function
of the training time. See App. B.3 for the proof."
MAIN RESULTS,0.1444805194805195,"R = R1 + R2
R1
R2 t"
MAIN RESULTS,0.1461038961038961,"Remark: Eq. 11 indicates that the singular values of F,
are directly multiplied by t. That implies that the learn-
ing speed of each feature is scaled by the magnitude of
its corresponding singular value. As an illustration, the
ﬁgure on the right shows the evolution of R1, R2, and
R = R1 + R2 for a case where p = d/2, σ1 = 1, and
σ1 = 0.01 implying a condition number of κ = 100."
SKETCH OF DERIVATIONS,0.14772727272727273,"2.3
SKETCH OF DERIVATIONS"
SKETCH OF DERIVATIONS,0.14935064935064934,"In this Section, we sketch the key steps in the derivation of our main results. For the sake of sim-
plicity, here we only treat the case where σϵ = λ = 0. The general case with detailed proofs are
presented in App B."
SKETCH OF DERIVATIONS,0.15097402597402598,"Exact dynamics of SGD.
Recall the gradient descent update rule in Eq. 4. For the linear model
deﬁned in Eqs. 1-2, learning is governed by the following discrete-time dynamics,"
SKETCH OF DERIVATIONS,0.1525974025974026,"ˆWt = ˆWt−1 −η∇ˆ
Wt−1LT ,
(16)"
SKETCH OF DERIVATIONS,0.15422077922077923,"= ˆWt−1 −η

−XT (y −X ˆWt−1)

.
(17)"
SKETCH OF DERIVATIONS,0.15584415584415584,"With the assumption that ˆWt=0 = 0, the dynamics admit the following exact closed-form solution,"
SKETCH OF DERIVATIONS,0.15746753246753248,"ˆWt =

I −

I −ηXT X
t
(XT X)−1XT y := ˜W(t).
(18)"
SKETCH OF DERIVATIONS,0.1590909090909091,"With a SVD on XT X, Eqs. 9-10 can then be obtained by substituting ˆWt in Eqs. 7. As a remark,
note that one can recover the results of Advani & Saxe (2017) by setting F = I. In that case, the
eigenvalues of XT X follow a Marchenko–Pastur distribution (Marchenko & Pastur, 1967)."
SKETCH OF DERIVATIONS,0.16071428571428573,"Induced probability density of SGD.
It is well-known (Kuhn & Bos, 1993; Solla, 1995) that
probability distribution of weight conﬁgurations for network weights ˆW trained via SGD on a loss
L( ˆW), tend to the Gibbs distribution such that,"
SKETCH OF DERIVATIONS,0.16233766233766234,P( ˆW) = 1
SKETCH OF DERIVATIONS,0.16396103896103897,"Zβ
e−βL( ˆ
W ),
(19)"
SKETCH OF DERIVATIONS,0.16558441558441558,"in which Zβ is the partition function
R
d ˆW exp(−βL( ˆW))

and β is called the inverse temper-
ature and is inversely proportional the stochastic noise of SGD, ξ, deﬁned in Eq. 4. Intuitively,
for small β, the distribution of P( ˆW) is almost uniform, while as β →∞, P( ˆW) becomes more
concentrated around the minimum of the training loss."
SKETCH OF DERIVATIONS,0.1672077922077922,"It is important to highlight that Eq. 19 describes the equilibrium distribution of the student network’s
weights, i.e., at the end of training (t →∞). However, we are interested in studying the trajectory"
SKETCH OF DERIVATIONS,0.16883116883116883,Under review as a conference paper at ICLR 2022
SKETCH OF DERIVATIONS,0.17045454545454544,"of student’s weights during the course of training, i.e., for ﬁnite t. To that end, we derive the time-
dependent probability density over ˆW,"
SKETCH OF DERIVATIONS,0.17207792207792208,"P( ˆW, t) =
1
Zβ, te−β ˜
L( ˆ
W ,t),
where,
(20)"
SKETCH OF DERIVATIONS,0.1737012987012987,"˜LT ( ˆW, t) : = 1"
N,0.17532467532467533,2n
N,0.17694805194805194,"X
(ˆyµ −˜yµ(t))2 + λ"
N,0.17857142857142858,"2 || ˆW||2
2,
(21) = 1"
N,0.18019480519480519,2n
N,0.18181818181818182,"X
(ˆyµ −xµT ˜W(t))2 + λ"
N,0.18344155844155843,"2 || ˆW||2
2,
( ˜W(t) deﬁned in Eq. 18)
(22)"
N,0.18506493506493507,≈LT ( ˆW) + 1
N,0.18668831168831168,"2
 
λ + 1"
N,0.18831168831168832,"ηt

|| ˆW||2
2.
(23)"
N,0.18993506493506493,SGD steps
N,0.19155844155844157,tth step on
N,0.19318181818181818,argmin
N,0.19480519480519481,"L( ˆW)
˜L( ˆW, t)
ˆW ˆW"
N,0.19642857142857142,L( ˆW)
N,0.19805194805194806,"˜L( ˆW, t)
Remark: ˜LT ( ˆW, t) is a modiﬁed loss such that its min-
imum (equilibrium distribution) is achieved at the tth it-
erate of gradient descent on L( ˆW). The schematic dia-
gram on the right illustrates this equivalence, such that,
arg min ˆ
W ˜LT ( ˆW, t) = ˆWt, where ˆWt is the deﬁned in
Eq. 4."
N,0.19967532467532467,"The typical generalization error.
To determine the typical generalization performance at time t,
one proceeds by ﬁrst computing the free-energy of the system as,"
N,0.2012987012987013,f := −1
N,0.20292207792207792,"βdEW,z

ln Zβ,t

.
(24)"
N,0.20454545454545456,"Free-energy is a self-averaging property where its typical/most probable value coincides with its av-
erage over proper probability distributions Engel & Van den Broeck (2001). Therefore, to determine
the typical values of R and Q, we extremize the free-energy w.r.t. those variables."
N,0.20616883116883117,"Due to the logarithm inside the expectation, analytical computation of Eq. 24 is intractable. How-
ever, the replica method (M´ezard et al., 1987) allows us to tackle this through the following identity,"
N,0.2077922077922078,"EW,z[ln Zβ,t] = lim
r→0
EW,z[Zr
β,t] −1
r
.
(25)"
N,0.20941558441558442,"Computation of the free-energy via replica method and its subsequent extremization w.r.t R and Q,
we arrive at Eqs. 13 and 15. See App. B.3 for more details."
N,0.21103896103896103,"To summarize, using the replica method, we are able to cast the high-dimensional dynamics of SGD
into simple scalar equations governing R and Q and, consequently, the generalization error LG.
While our analysis is limited to the speciﬁc teacher and student setup, this simple model already
exhibits dynamics qualitatively similar to those observed in more complex networks, as we now
illustrate."
EXPERIMENTAL RESULTS,0.21266233766233766,"3
EXPERIMENTAL RESULTS"
EXPERIMENTAL RESULTS,0.21428571428571427,"In this Section, we conduct numerical simulations to validate our analytical results and provide
clear insights on the macroscopic dynamics of generalization. We also conduct experiments on real-
world neural networks showing a close qualitative match between the generalization behavior of
neural networks and our teacher-student setup."
EXPERIMENTAL RESULTS,0.2159090909090909,"For real-world experiments, we train a ResNet18 (He et al., 2016) with large layer widths [64, 2×
64, 4 × 64, 8 × 64]. We follow the training setup of Nakkiran et al. (2019); Label noise with a
probability 0.15 randomly assign an incorrect label to training examples. Noise is sampled only
once before the training starts. We train using Adam (Kingma & Ba, 2014) with learning rate of
1e −4 for 1K epochs. Real-world experiments are averaged over 50 random seeds. To ensure
reproducibility, we include the complete source code in a GitHub repository as well as an
anonymous Collab notebook."
EXPERIMENTAL RESULTS,0.21753246753246752,Under review as a conference paper at ICLR 2022
EXPERIMENTAL RESULTS,0.21915584415584416,"Figure 2: A qualitative comparison between a ResNet-18 and our analytical results. (a): Heat-
map of empirical generalization error (0-1 classiﬁcation error) for the ResNet-18 trained on CIFAR-
10 with 15% label noise. X-axis denotes the inverse of weight-decay regularization strength and Y-
axis represents the training time. (c): Heat-map of the analytical generalization error (mean squared
error) for the linear teacher-student setup with κ = 100, the condition number of the modulation
matrix. (b, d): Three slices of the heat-maps for large, intermediate, and small amounts of regu-
larization. Analysis: As predicted by Eqs. 13 and 15, κ = 100 implies that a subset of features
are learned 100 times faster that the rest. Intuitively, large amounts of regularization allow for the
fast-learning features to be learned by not to overﬁt. Intermediate levels of regularization result in a
classical U-shaped generalization curve but prevent slow features from learning. Small amounts of
regularization allow for both fast and slow features to be learned, leading to double descent."
MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS,0.22077922077922077,"3.1
MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS"
MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS,0.2224025974025974,"We conduct an experiment on the classiﬁcation task of CIFAR-10 (Krizhevsky et al., 2009) with
varying amount of weight decay regularization strength λ. We monitor the generalization error (0-
1 test error) during the course of training and visualize a heat-map of the generalization error for
different λ’s in Figure 2 (a)."
MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS,0.22402597402597402,"We also conduct a similar experiment with the teacher-student setup presented in Section 2.1. We
visualize a heat-map of the generalization error which is the mean squared error (MSE) over test
distribution in Figure 2 (b). Particularly, we plot Eqs. 13 and 15 with a constant κ = 100. As a
remark, we note that a κ = 100 implies that a subset of features are learned 100 times faster than
other features."
MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS,0.22564935064935066,"It is observed that in both experiments, a model with intermediate levels of regularization displays
a typical overﬁtting behavior where the generalization error decreases ﬁrst and then overﬁts. This is
consistent with Eq. 87 which indicates larger amounts of regularization prevent slow feature from
being learned as λ and the inverse of t are summed. In other words, learning of slow features requires
large weights, something that is penalized by the weight-decay. On the other hand, a model with
smaller amount of regularization exhibits the double descent generalization curve."
MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS,0.22727272727272727,"We also validate our derived analytical expressions by running numerical simulations which are
presented in Figure 4."
MATCH BETWEEN THEORY AND REAL-WORLD EXPERIMENTS,0.2288961038961039,Under review as a conference paper at ICLR 2022
THE PHASE DIAGRAM,0.2305194805194805,"3.2
THE PHASE DIAGRAM"
THE PHASE DIAGRAM,0.23214285714285715,"To further investigate the transition between the two phases of classical single descent and double
descent, we explore the phase diagram. Recall that with Eq. 6, one can fully characterize the evo-
lution of the generalization dynamics in terms of two scalar variables instead of the d-dimensional
parameter space. R and Q presented in Eq. 7 are macroscopic variables where R represents the
alignment between the teacher and the student and Q is the student’s (modulated) norm. Hence,
a better generalization performance is achieved with larger R and smaller Q."
THE PHASE DIAGRAM,0.23376623376623376,"R and Q are not free parameters and both depend on the training dynamics through Eqs. 13 and 15.
Nevertheless, it is instructive to visualize the generalization error for all pairs of (R, Q). In Figure
3, we visualize the RQ-plane for (R, Q) ∈[0.0, 0.8] × Q ∈[0.0, 1.6]. At the time of initialization,
(R, Q) = (0, 0) as the models are initialized at the origin. As training time proceeds, values of
R and Q follow the depicted trajectories. In Figure 3, different trajectories correspond to different
values of κ, the condition number of the modulation matrix F in Eq. 2. It is important to note that
the closer a trajectory is to the lower-right, the better the generalization error gets."
THE PHASE DIAGRAM,0.2353896103896104,"The yellow curve which corresponds to the case with large κ = 1e5 meaning that a subset of features
are extremely slower than the others that practically do not get learned. In that case, generalization
error exhibits traditional over-ﬁtting due to over-training. On the phase diagram, the yellow trajec-
tory starts at (0, 0) and moves towards Point A which has the lowest generalization error of this
curve. Then as the training continues, Q increases and as t →∞the trajectory lands at Point B
which has the worse generalization error. The curves in orange, green and blue correspond to tra-
jectories with κ = 1e3, κ = 1e2, κ = 1e1, respectively. They follow the case of κ = 1e5 up to
the vicinity of Point B, but then the trajectories slowly incline towards another ﬁxed point, Point C
signalling a second descent in the generalization error."
THE PHASE DIAGRAM,0.237012987012987,"The phase diagram along with the corresponding generalization curves in Figure 2 illustrate that
features that are learned on a faster time-scale are responsible for the initial conventional U-shaped
generalization curve, while the second descent can be attributed to the features that are learned at a
slower time-scale."
THE PHASE DIAGRAM,0.23863636363636365,"Figure 3: Left: Phase diagram of the generalization error as a function of R(t) and Q(t) (Eqs. 13
and 15). The generalization error for all pairs of (R, Q) ∈[0.0, 0.8] × [0.0, 1.6] is contour-plotted
in the background in shades of beige, with the best generalization performance being attained on
the lower right part of the plot. The trajectories describe the evolution of R(t) and Q(t) as training
proceeds. Each trajectory correspond to a different κ, the condition number of the modulation matrix
F in Eq. 2. κ describes the ratio of the rates at which two sets of features are learned. Right: The
corresponding generalization curves for different plotted over the training time axis. Analysis: The
trajectory with κ = 1e5 (bright yellow) starts at the origin and advances towards point A (a descent
in generalization error). Then by over-training, it converges to point B (an ascent in generalization
error). For the other trajectories with smaller κ, a ﬁrst descent in generalization error occurs up to
the point A, then an ascent happens, but they no longer converge to point B. Instead, by further
training, these trajectories converge to point C implying a second descent."
THE PHASE DIAGRAM,0.24025974025974026,Under review as a conference paper at ICLR 2022
RELATED WORK AND DISCUSSION,0.2418831168831169,"4
RELATED WORK AND DISCUSSION"
RELATED WORK AND DISCUSSION,0.2435064935064935,"Although the term double descent has been introduced rather recently (Belkin et al., 2019a), similar
behaviors had already been observed and studied in several decades-old works form a statistical
physics perspective (Krogh & Hertz, 1992a; Opper, 1995; Opper & Kinzel, 1996; B¨os, 1998; Engel
& Van den Broeck, 2001). More recently, these behaviors have been investigated in the context of
modern machine learning, both from an empirical (Nakkiran et al., 2019; Amari et al., 2020; Yang
et al., 2020) and theoretical (Belkin et al., 2019a; Geiger et al., 2019; Advani & Saxe, 2017; Mei &
Montanari, 2019; Gerace et al., 2020; d’Ascoli et al., 2020; Ba et al., 2019; d’Ascoli et al., 2021)
perspectives."
RELATED WORK AND DISCUSSION,0.24512987012987014,"Hastie et al. (2019); Advani et al. (2020); Belkin et al. (2020) use random matrix theory (RMT)
tools to characterize the asymptotic generalization behavior of over-parameterized linear and random
feature models. In an inﬂuential work, Mei & Montanari (2019) extend the same analysis to a
random feature model and theoretically derive the model-wise double descent curve for a model
with Tikhonov regularization. Jacot et al. (2020) also study double descent in ridge estimators and
show an equivalence to kernel ridge regression. Pennington & Worah (2019) used RMT to study the
curvature of single-hidden-layer neural network in an attempt to understand the efﬁcacy of ﬁrst-order
optimization methods in training DNNs. In addition, Liang & Rakhlin (2020) take a similar approach
to investigate implicit regularization in high dimensional ridgeless regression with nonlinear kernels."
RELATED WORK AND DISCUSSION,0.24675324675324675,"While most of the related work study the non-monotonicity of the generalization error as a function
of the model size or sample size, Nakkiran et al. (2019) introduced the epoch-wise double descent.
Epoch-wise double descent refers to the phenomenon where the generalization error undergoes two
descents as the training time increases. There has been limited work on studying of epoch-wise
double descent. Very recently, Heckel & Yilmaz (2020) and Stephenson & Lee (2021) have focused
on ﬁnding the roots of this phenomenon."
RELATED WORK AND DISCUSSION,0.2483766233766234,"Heckel & Yilmaz (2020) provides upper bounds on the risk of single and two layer models in a
regression setting where the input data has distinct feature variances. Heckel & Yilmaz (2020)
demonstrate that a superposition of two or more bias-variance tradeoff curves leads to epoch-wise
double descent. The authors also show that different layers of the network are learned at different
epochs. For that reason, epoch-wise double descent can be eliminated by appropriate selection of
learning rates for individual network weights. Consistent with these ﬁndings, our work formalizes
this phenomenon in terms of feature learning scales and provides closed-form predictions."
RELATED WORK AND DISCUSSION,0.25,"Stephenson & Lee (2021) arrives at similar conclusions. Authors in Stephenson & Lee (2021) take
a random matrix theory approach on a data model that exhibits epoch-wise double descent. The
data model is constructed so that the noise is explicitly added only to the fast-learning features while
slow-learning features remain noise-free. Consequently, the fast-learning features are noisy and
hence show a U-shaped generalization curve while slow-learning features are noiseless."
RELATED WORK AND DISCUSSION,0.25162337662337664,"Our ﬁndings and those of Heckel & Yilmaz (2020) and Stephenson & Lee (2021) reinforce one
another with a common central ﬁnding that the epoch-wise double descent results from different
features/layers being learned at different time-scale. However, we also highlight that both Heckel
& Yilmaz (2020) and Stephenson & Lee (2021) built upon tool from random matrix theory and
study distinct data models from our teacher-student setup. We study the same phenomenon from a
different perspective. By leveraging the replica method from statistical physics, we characterized
the generalization behavior using a set of informative macroscopic parameters. While supporting
the notion that the interaction of different feature learning speeds causes epoch-wise double descent,
our work provides formal predictions of the dynamics that unfold during training."
RELATED WORK AND DISCUSSION,0.2532467532467532,"We believe our theoretical framework sets the stage for further understanding of generalization dy-
namics in neural networks beyond the double descent. A future direction to study is a case in which
the ﬁrst descent is strong enough to bring down the training loss to very small values to the point that
learning slower features is practically impossible or happens after a very large number of epochs.
Power et al. (2021) reports an instance of such behavior called Grokking where the model abruptly
learns to perfectly generalize but long after the training loss has reached very small values."
RELATED WORK AND DISCUSSION,0.25487012987012986,"Limitations. It should be noted that studying ﬁner details of the dynamics would require a more
precise model of the neural networks. Clearly, our proposed model is not a universal and unique
way to model the dynamics of the complex, over-parameterized deep neural networks."
RELATED WORK AND DISCUSSION,0.2564935064935065,Under review as a conference paper at ICLR 2022
RELATED WORK AND DISCUSSION,0.25811688311688313,"Social Impact. The authors do not foresee a negative social impact speciﬁcally arising from this
rather theoretical work."
REFERENCES,0.2597402597402597,REFERENCES
REFERENCES,0.26136363636363635,"Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017."
REFERENCES,0.262987012987013,"Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks, 132:428–446, 2020."
REFERENCES,0.26461038961038963,"Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics,
pp. 1370–1378. PMLR, 2019."
REFERENCES,0.2662337662337662,"Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
ﬂow for least squares. In International Conference on Machine Learning, pp. 233–244. PMLR,
2020."
REFERENCES,0.26785714285714285,"Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny
Wu, and Ji Xu.
When does preconditioning help or hurt generalization?
arXiv preprint
arXiv:2006.10732, 2020."
REFERENCES,0.2694805194805195,"Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-
layer neural networks: An asymptotic viewpoint. In International conference on learning repre-
sentations, 2019."
REFERENCES,0.2711038961038961,"Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of
wide neural networks, 2020."
REFERENCES,0.2727272727272727,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
Reconciling modern machine-
learning practice and the classical bias–variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849–15854, 2019a."
REFERENCES,0.27435064935064934,"Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
Reconciling modern machine-
learning practice and the classical bias–variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849–15854, 2019b."
REFERENCES,0.275974025974026,"Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167–1180, 2020."
REFERENCES,0.2775974025974026,"Carl M Bender and Steven A Orszag. Advanced mathematical methods for scientists and engineers
I: Asymptotic methods and perturbation theory. Springer Science & Business Media, 2013."
REFERENCES,0.2792207792207792,"S B¨os, W Kinzel, and M Opper. Generalization ability of perceptrons with continuous outputs.
Physical Review E, 47(2):1384, 1993."
REFERENCES,0.28084415584415584,"Siegfried B¨os. Statistical mechanics approach to early stopping and weight decay. Physical Review
E, 58(1):833, 1998."
REFERENCES,0.2824675324675325,"L´eon Bottou et al. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes, 91
(8):12, 1991."
REFERENCES,0.2840909090909091,"Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
Multiple descent: Design your own
generalization curve. arXiv preprint arXiv:2008.01036, 2020."
REFERENCES,0.2857142857142857,"Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, pp. 1305–1338. PMLR, 2020."
REFERENCES,0.28733766233766234,"St´ephane D’Ascoli, Maria Reﬁnetti, Giulio Biroli, and Florent Krzakala. Double trouble in dou-
ble descent: Bias and variance(s) in the lazy regime.
In Hal Daum´e III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 2280–2290. PMLR, 13–18 Jul 2020.
URL
https://proceedings.mlr.press/v119/d-ascoli20a.html."
REFERENCES,0.288961038961039,Under review as a conference paper at ICLR 2022
REFERENCES,0.2905844155844156,"St´ephane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overﬁtting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020."
REFERENCES,0.2922077922077922,"St´ephane d’Ascoli, Marylou Gabri´e, Levent Sagun, and Giulio Biroli. On the interplay between
data structure and loss function in classiﬁcation problems. In Thirty-Fifth Conference on Neural
Information Processing Systems, 2021."
REFERENCES,0.29383116883116883,"St´ephane d’Ascoli, Maria Reﬁnetti, Giulio Biroli, and Florent Krzakala. Double trouble in dou-
ble descent: Bias and variance (s) in the lazy regime. In International Conference on Machine
Learning, pp. 2280–2290. PMLR, 2020."
REFERENCES,0.29545454545454547,"Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge Uni-
versity Press, 2001."
REFERENCES,0.29707792207792205,"Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al.
The elements of statistical learning,
volume 1. Springer series in statistics New York, 2001."
REFERENCES,0.2987012987012987,"Elizabeth Gardner. The space of interactions in neural network models. Journal of physics A:
Mathematical and general, 21(1):257, 1988."
REFERENCES,0.3003246753246753,"Elizabeth Gardner and Bernard Derrida.
Optimal storage properties of neural network models.
Journal of Physics A: Mathematical and general, 21(1):271, 1988."
REFERENCES,0.30194805194805197,"Elizabeth Gardner and Bernard Derrida. Three unﬁnished works on the optimal storage capacity of
networks. Journal of Physics A: Mathematical and General, 22(12):1983, 1989."
REFERENCES,0.30357142857142855,"Mario Geiger, Stefano Spigler, Stephane d’Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019."
REFERENCES,0.3051948051948052,"Stuart Geman, Elie Bienenstock, and Ren´e Doursat. Neural networks and the bias/variance dilemma.
Neural computation, 4(1):1–58, 1992."
REFERENCES,0.3068181818181818,"Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc M´ezard, and Lenka Zdeborov´a. Gener-
alisation error in learning with random features and the hidden manifold model. In International
Conference on Machine Learning, pp. 3452–3462. PMLR, 2020."
REFERENCES,0.30844155844155846,"Sebastian Goldt, Galen Reeves, Marc M´ezard, Florent Krzakala, and Lenka Zdeborov´a. The gaus-
sian equivalence of generative models for learning with two-layer neural networks. arXiv e-prints,
pp. arXiv–2006, 2020."
REFERENCES,0.31006493506493504,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. MIT press
Cambridge, 2016."
REFERENCES,0.3116883116883117,"Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019."
REFERENCES,0.3133116883116883,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016."
REFERENCES,0.31493506493506496,"Reinhard Heckel and Fatih Furkan Yilmaz. Early stopping in deep networks: Double descent and
how to eliminate it. arXiv preprint arXiv:2007.10099, 2020."
REFERENCES,0.31655844155844154,"Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571–
8580, 2018."
REFERENCES,0.3181818181818182,"Arthur Jacot, Berﬁn Simsek, Francesco Spadaro, Cl´ement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In International Conference on Machine Learning, pp.
4631–4640. PMLR, 2020."
REFERENCES,0.3198051948051948,"Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka. A typical reconstruction limit for
compressed sensing based on lp-norm minimization. Journal of Statistical Mechanics: Theory
and Experiment, 2009(09):L09003, 2009."
REFERENCES,0.32142857142857145,Under review as a conference paper at ICLR 2022
REFERENCES,0.32305194805194803,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014."
REFERENCES,0.3246753246753247,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009."
REFERENCES,0.3262987012987013,"Anders Krogh and John A Hertz. Generalization in a linear perceptron in the presence of noise.
Journal of Physics A: Mathematical and General, 25(5):1135, 1992a."
REFERENCES,0.32792207792207795,"Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances
in neural information processing systems, pp. 950–957, 1992b."
REFERENCES,0.32954545454545453,"Florent Krzakala, Marc M´ezard, Franc¸ois Sausset, YF Sun, and Lenka Zdeborov´a.
Statistical-
physics-based reconstruction in compressed sensing. Physical Review X, 2(2):021005, 2012."
REFERENCES,0.33116883116883117,"R Kuhn and S Bos.
Statistical mechanics for neural networks with continuous-time dynamics.
Journal of Physics A: Mathematical and General, 26(4):831, 1993."
REFERENCES,0.3327922077922078,"Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.
Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv
preprint arXiv:2012.04728, 2020."
REFERENCES,0.3344155844155844,"Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters, 66(18):2396, 1991."
REFERENCES,0.336038961038961,"Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can gen-
eralize. The Annals of Statistics, 48(3), Jun 2020. ISSN 0090-5364. doi: 10.1214/19-aos1849.
URL http://dx.doi.org/10.1214/19-AOS1849."
REFERENCES,0.33766233766233766,"Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues for
some sets of random matrices. Matematicheskii Sbornik, 114(4):507–536, 1967."
REFERENCES,0.3392857142857143,"Song Mei and Andrea Montanari. The generalization error of random features regression: precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019."
REFERENCES,0.3409090909090909,"Marc M´ezard, Giorgio Parisi, and Miguel Virasoro. Spin glass theory and beyond: an introduction
to the Replica Method and its applications, volume 9. World Scientiﬁc Publishing Company,
1987."
REFERENCES,0.3425324675324675,"Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
Deep double descent: where bigger models and more data hurt.
ICLR 2020, arXiv preprint
arXiv:1912.02292, 2019."
REFERENCES,0.34415584415584416,"Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
arXiv preprint arXiv:1810.08591, 2018."
REFERENCES,0.3457792207792208,"Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014."
REFERENCES,0.3474025974025974,"Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947–5956,
2017."
REFERENCES,0.349025974025974,"Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks, 2018."
REFERENCES,0.35064935064935066,"Manfred Opper. Statistical mechanics of learning: Generalization. The handbook of brain theory
and neural networks, pp. 922–925, 1995."
REFERENCES,0.3522727272727273,"Manfred Opper and Wolfgang Kinzel. Statistical mechanics of generalization. In Models of neural
networks III, pp. 151–209. Springer, 1996."
REFERENCES,0.3538961038961039,"Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Journal
of Statistical Mechanics: Theory and Experiment, 2019(12):124005, 2019."
REFERENCES,0.3555194805194805,Under review as a conference paper at ICLR 2022
REFERENCES,0.35714285714285715,"Mohammad Pezeshki, S´ekou-Oumar Kaba, Yoshua Bengio, Aaron C. Courville, Doina Precup,
and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. CoRR,
abs/2011.09468, 2020. URL https://arxiv.org/abs/2011.09468."
REFERENCES,0.3587662337662338,"Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gener-
alization beyond overﬁtting on small algorithmic datasets. In ICLR MATH-AI Workshop, 2021."
REFERENCES,0.36038961038961037,"Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Confer-
ence on Machine Learning, pp. 5301–5310. PMLR, 2019."
REFERENCES,0.362012987012987,"Frederick Reif. Fundamentals of statistical and thermal physics. Waveland Press, 2009."
REFERENCES,0.36363636363636365,"Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learn-
ing from examples. Physical review A, 45(8):6056, 1992."
REFERENCES,0.3652597402597403,"Sara A Solla. A bayesian approach to learning in neural networks. International Journal of Neural
Systems, 6:161–170, 1995."
REFERENCES,0.36688311688311687,"Cory Stephenson and Tyler Lee. When and how epochwise double descent happens. arXiv preprint
arXiv:2108.12006, 2021."
REFERENCES,0.3685064935064935,"Vladimir N. Vapnik. The nature of statistical learning theory. Wiley, New York, 1st edition, Septem-
ber 1998. ISBN 978-0-471-03003-4."
REFERENCES,0.37012987012987014,"Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-
off for generalization of neural networks. In International Conference on Machine Learning, pp.
10767–10777. PMLR, 2020."
REFERENCES,0.3717532467532468,"Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016."
REFERENCES,0.37337662337662336,"Xiao Zhang and Dongrui Wu. Rethink the connections among generalization, memorization and
the spectral bias of dnns. CoRR, abs/2004.13954, 2020. URL https://arxiv.org/abs/
2004.13954."
REFERENCES,0.375,"A
FURTHER RELATED WORK AND DISCUSSION"
REFERENCES,0.37662337662337664,"If we consider plots where the generalization error on the y-axis is plotted against other quantities
on the x-axis, we ﬁnd earlier works that have identiﬁed double descent behavior for quantities such
as the number of parameters, the dimensionality of the data, the number of training samples, or the
training time on the x-axis. In this paper, we studied epoch-wise double descent, i.e. we plot the
training time t, or the number of training epochs, on the x-axis. Literature displaying double descent
phenomena in generalization behavior w.r.t. other quantities do so in the limit of t →∞."
REFERENCES,0.3782467532467532,"From a random matrix theory perspective, Le Cun et al. (1991); Hastie et al. (2019); Advani et al.
(2020), and Belkin et al. (2020) are among works which have analytically studied the spectral density
of the Hessian matrix. According to their analyses, at intermediate levels of complexity, the presence
of small but non-zero eigenvalues in the Hessian matrix results in high generalization error as the
inverse of the Hessian is calculated for the pseudo-inverse solution."
REFERENCES,0.37987012987012986,"Neyshabur et al. (2014) demonstrated that over-parameterized networks does not necessarily overﬁt
thus suggesting the need of a new form of measure of model complexity other than network size.
Subsequently, Neyshabur et al. (2018) suggest a novel complexity measure based on unit-wise ca-
pacities which correlates better with the behavior of test error with increasing network size. Chizat
& Bach (2020) study the global convergence and superior generalization behavior of inﬁnitely wide
two-layer neural networks with logistic loss. Goldt et al. (2020) make use of the Gaussian Equiv-
alence Theorem to study the generalization performance of two-layer neural networks and kernel
models trained on data drawn from pre-trained generative models. Bai & Lee (2020) investigated
the gap between the empirical performance of over-parameterized networks and their NTK counter-
parts, ﬁrst proposed by Jacot et al. (2018)."
REFERENCES,0.3814935064935065,Under review as a conference paper at ICLR 2022
REFERENCES,0.38311688311688313,"From the perspective of bias/variance trade-off, Geman et al. (1992), and more recently, Neal et al.
(2018) empirically observe that while bias is monotonically decreasing, variance could be decreasing
too or unimodal as the number of parameters increases, thus manifesting a double descent general-
ization curve. Hastie et al. (2019) analytically study the variance. More recently, Yang et al. (2020)
provides a new bias/variance decomposition of bias exhibiting double desc-nt in which the variance
follows a bell-shaped curve. However, the decrease in variance as the model size increases remains
unexplained. For high dimensional regression with random features, d’Ascoli et al. (2020) provides
an asymptotic expression for the bias/variance decomposition and identiﬁes three sources of vari-
ance with non-monotonous behavior as the model size or dataset size varies. d’Ascoli et al. (2020)
also employs the analysis of random feature models and identiﬁes two forms of overﬁtting which
leads to the so-called sample-wise triple descent. More recently, Chen et al. (2020) show that as a
result of the interaction between the data and the model, one may design generalization curves with
multiple descents."
REFERENCES,0.3847402597402597,"From a statistical physics perspective, Opper (1995); B¨os et al. (1993); B¨os (1998); Opper & Kinzel
(1996) are among the ﬁrst studies which theoretically observe sample-wise double-descent in a ridge
regression setup where the solution is obtained by the pseudo-inverse method. Most of these studies
employ the “Gardner analysis” (Gardner, 1988; Gardner & Derrida, 1988; 1989) for models where
the number of parameters and the dimensionality of data are coupled and hence the observed form of
double descent is different from that observed in deep neural networks. A beautiful extended review
of this line of work is provided in Engel & Van den Broeck (2001). Among recent works, Gerace
et al. (2020) also apply the Gardner analysis but to a novel generalized data generating process called
the hidden manifold model and derive the model-wise double-descent equations analytically."
REFERENCES,0.38636363636363635,"Finally, recall that towards providing an explanation for the epoch-wise double descent, we argue
that the epoch-wise double descent can be attributed to different features being learned at different
time-scales, resulting in a non-monotonous generalization curve. In relation to the aspect of different
feature learning scales, Rahaman et al. (2019) had observed that DNNs have a tendency towards
learning simple target functions ﬁrst that can allow for good generalization behavior of various
data samples. Pezeshki et al. (2020) also identify and provide explanation for a feature learning
imbalance exhibited by over-parameterized networks trained via gradient descent on cross-entropy
loss, with the networks learning only a subset of the full feature spectrum over training. More
recently though, Zhang & Wu (2020), show that certain DNNs models prioritize learning high-
frequency components ﬁrst followed by the learning of slow but informative features, leading to the
second descent of the test error as observed in epoch-wise double descent."
REFERENCES,0.387987012987013,"On the difference between model-wise and epoch-wise double descent curves.
In accordance
with its name, model-wise double descent (in the test error) occurs due to an increase in model-
size (number of its parameters), i.e., as the model transitions from an under-parameterized to an
over-parameterized regime. A variety of works have tried to understand this phenomenon from
the lens of implicit regularization (Neyshabur et al., 2014) or deﬁning novel complexity measures
(Neyshabur et al., 2017). On the other hand, epoch-wise double descent (in the test error) as treated
in our work, is observed to occur for both over-parameterized (Nakkiran et al., 2019) and under-
parameterized (Heckel & Yilmaz, 2020) setups. As found in our work along with the latter reference,
this phenomenon seems to be a result of different feature learning speeds rather than the extent of
model parameterization. The overlap of the test-error contributions from the different weights with
varying scales of learning henceforth leads to a non-monotonous evolution of the model test error as
exempliﬁed by epoch-wise double descent."
REFERENCES,0.38961038961038963,"We also note that the peak in model-wise double descent is associated with the model’s capacity to
perfectly interpolate the data, we do not think an analogous notion exists for the case of epoch-wise
double descent. Our understanding of the peak in the latter is that it corresponds to a training time
conﬁguration whereby a subclass of features are already learnt (due to a larger associated signal-to-
noise-ratio) and are being overﬁtted upon to ﬁt the target. As training proceeds further, the remaining
set of features are eventually learnt thus allowing for a lowering of the test error."
REFERENCES,0.3912337662337662,"On the implicit regularization of SGD and ridge-regularized loss.
The results presented in Eqs.
20-23 have a core dependence on the ﬁndings of Ali et al. (2019; 2020). These works ﬁrst formalize
the connection between (continuous-time) GD or SGD-based training of an ordinary least squares
(OLS) setup and that of ridge regression, providing bounds on the test error under these algorithms"
REFERENCES,0.39285714285714285,Under review as a conference paper at ICLR 2022
REFERENCES,0.3944805194805195,"over training time t, in terms of a ridge setup with ridge parameter λ = 1/t. We utilize these results
in the sense that by evaluating the generalization error LG of our student-teacher setup with explicit
ridge regularization, we invoke the connection between the ridge coefﬁcient λ and training time t as
described in these works, to obtain the behavior of (ridgeless) LG over training. This determination
of an expression of LG(t) is what allows us to study the epoch-wise DD phenomenon."
REFERENCES,0.3961038961038961,"B
TECHNICAL PROOFS"
REFERENCES,0.3977272727272727,"B.1
THE GENERALIZATION ERROR AS A FUNCTION OF R AND Q (EQ. 6)"
REFERENCES,0.39935064935064934,"Recall that the teacher is the data generator and is deﬁned as,"
REFERENCES,0.400974025974026,"y := y∗+ ϵ,
y∗:= zT W,
zi ∼N(0, 1
√"
REFERENCES,0.4025974025974026,"d
),
(26)"
REFERENCES,0.4042207792207792,"where z ∈Rd is the teacher’s input and y∗, y ∈R are the teacher’s noiseless and noisy outputs,
respectively. W ∈Rd represents the (ﬁxed) weights of the teacher and ϵ ∈R is the noise."
REFERENCES,0.40584415584415584,"And student is deﬁned as,"
REFERENCES,0.4074675324675325,"ˆy := xT ˆW,
s.t.
x := F T z,
(27)"
REFERENCES,0.4090909090909091,"where the matrix F ∈Rd×d is a predeﬁned and ﬁxed modulation matrix regulating the student’s
access to the true input z."
REFERENCES,0.4107142857142857,"The average generalization error of the student, determined by averaging the student’s error over all
possible input-target pairs and noise realizations is given by,"
REFERENCES,0.41233766233766234,LG := 1
REFERENCES,0.413961038961039,"2Ex,W

(y∗−ˆy)2
,
(28)"
REFERENCES,0.4155844155844156,"in which the variables (y∗, ˆy) form a bi-variate Gaussian distribution with zero mean and a covari-
ance of,"
REFERENCES,0.4172077922077922,"Σ =

< y∗, y∗>z
< y∗, ˆy >z
< y∗, ˆy >z
< ˆy, ˆy >z"
REFERENCES,0.41883116883116883,"
=

1
R
R
Q"
REFERENCES,0.42045454545454547,"
,
(29)"
REFERENCES,0.42207792207792205,"in which,"
REFERENCES,0.4237012987012987,R : = Ez[y∗T ˆy] = Ez[W T zzT F ˆW] = 1
REFERENCES,0.4253246753246753,"dW T F ˆW,
and,
(30)"
REFERENCES,0.42694805194805197,Q : = Ez[ˆyT ˆy] = Ez[ ˆW T F T zzT F ˆW] = 1
REFERENCES,0.42857142857142855,"d
ˆW T F T F ˆW.
(31)"
REFERENCES,0.4301948051948052,"Eq. 29 implies a correlation between y∗and ˆy obstructing the calculation of the average in Eq. 28.
Following (B¨os, 1998; Krogh & Hertz, 1992a), we deﬁne decoupled variables ˜y∗and ˜ˆy as follows,"
REFERENCES,0.4318181818181818,"y∗=: ˜y∗,
and
ˆy =: R˜y∗+
p"
REFERENCES,0.43344155844155846,"Q −R2˜ˆy.
(32)"
REFERENCES,0.43506493506493504,"The variables ˜y∗and ˜ˆy are independent Gaussian variables such that < ˜y∗, ˜ˆy >z= 0. Therefore,
two expectations can be applied independently,"
REFERENCES,0.4366883116883117,LG : = 1
REFERENCES,0.4383116883116883,"2Ex,W

(y∗−ˆy)2
,
(33) = 1"
REFERENCES,0.43993506493506496,"2E˜y∗,˜ˆy

(˜y∗−(R˜y∗+
p"
REFERENCES,0.44155844155844154,"Q −R2˜ˆy))2
,
(34) = 1"
REFERENCES,0.4431818181818182,"2(1 + Q −2R).
(35)"
REFERENCES,0.4448051948051948,"Finally, we note that expectation w.r.t. a Gaussian variable x is deﬁned as,"
REFERENCES,0.44642857142857145,"Ex[f(x)] :=
Z +∞ −∞ dx
√"
REFERENCES,0.44805194805194803,"2π exp
 
−x2"
REFERENCES,0.4496753246753247,"2

f(x).
(36)"
REFERENCES,0.4512987012987013,Under review as a conference paper at ICLR 2022
REFERENCES,0.45292207792207795,"B.2
THE GENERAL CASE EXACT DYNAMICS (EQS. 9-10)"
REFERENCES,0.45454545454545453,"Recall that to train our student network, we use stochastic gradient descent (SGD) on the regularized
mean squared loss, evaluated on the n training examples as,"
REFERENCES,0.45616883116883117,LT := 1
N,0.4577922077922078,"2n n
X"
N,0.4594155844155844,"µ=1
(yµ −ˆyµ)2 + λ"
N,0.461038961038961,"2 || ˆW||2
2,
(37)"
N,0.46266233766233766,"where λ ∈[0, ∞) is the regularization coefﬁcient."
N,0.4642857142857143,"The minimum of the loss function, denoted by W, is achieved at,"
N,0.4659090909090909,"∇ˆ
W LT = 0 ⇒∇ˆ
W
1"
N,0.4675324675324675,"2||y −X ˆW||2
2 + λ"
N,0.46915584415584416,"2 || ˆW||2
2

= 0
(38)"
N,0.4707792207792208,"⇒−XT (y −X ˆW) + λ ˆW = 0
(39)"
N,0.4724025974025974,"⇒W := (XT X + λI)−1XT y.
(40)"
N,0.474025974025974,"An exact gradient descent has the following dynamics,"
N,0.47564935064935066,"ˆWt = ˆWt−1 −η∇ˆ
Wt−1LT ,
(41)"
N,0.4772727272727273,"= ˆWt−1 −η

−XT (y −X ˆWt−1) + λ ˆWt−1

(42)"
N,0.4788961038961039,"= (1 −ηλ) ˆWt−1 −ηXT X ˆWt−1 + ηXT y,
(43)"
N,0.4805194805194805,"= [(1 −ηλ)I −ηXT X] ˆWt−1 + ηXT y,
(44)"
N,0.48214285714285715,"= [(1 −ηλ)I −ηXT X] ˆWt−1 + η(XT X + λI)(XT X + λI)−1XT y,
(45)"
N,0.4837662337662338,"= [(1 −ηλ)I −ηXT X] ˆWt−1 + η(XT X + λI)W,
(46)"
N,0.48538961038961037,"= [(1 −ηλ)I −ηXT X] ˆWt−1 + (ηXT X + ηλI)W,
(47)"
N,0.487012987012987,"= [(1 −ηλ)I −ηXT X] ˆWt−1 + (ηXT X + (ηλ −1)I)W + W,
(48)"
N,0.48863636363636365,"which leads to,"
N,0.4902597402597403,"ˆWt −W = [(1 −ηλ)I −ηXT X]( ˆWt−1 −W),
(49)"
N,0.49188311688311687,"= [(1 −ηλ)I −ηXT X]t( ˆW0 −W).
(50)"
N,0.4935064935064935,"Assuming ˆW0 = 0, we arrive at the following closed-form equation,"
N,0.49512987012987014,"ˆWt =

I −

(1 −ηλ)I −ηXT X
t"
N,0.4967532467532468,"W,
(51)"
N,0.49837662337662336,where W is deﬁned in Eq 40.
N,0.5,"Now back to deﬁnition of R in Eq. 84 and by substitution of Eq. 51, we have,"
N,0.5016233766233766,R(t) : = 1
N,0.5032467532467533,"dW T F ˆWt,
(52) = 1"
N,0.5048701298701299,"dW T F

I −

(1 −ηλ)I −ηXT X
t"
N,0.5064935064935064,"W,
(53) = 1"
N,0.5081168831168831,"dW T F

I −

(1 −ηλ)I −ηXT X
t
(XT X + λI)−1XT y,
(54) = 1"
N,0.5097402597402597,"dW T FV

I −

(1 −ηλ)I −ηΛ
t
(Λ + λI)−1V T XT y,
(XT X = V ΛV T )
(55) = 1"
N,0.5113636363636364,"dW T FV

I −

(1 −ηλ)I −ηΛ
t
(Λ + λI)−1(ΛV T F −1W + Λ
1
2 ϵ),
(56) = 1"
N,0.512987012987013,"dW T FV

I −

(1 −ηλ)I −ηΛ
t
(Λ + λI)−1(ΛV T F −1W + Λ
1
2 ϵ),
(57) = 1"
N,0.5146103896103896,"dTr
h 
I −[(1 −ηλ)I −ηΛ]t
Λ
Λ + λI"
N,0.5162337662337663,"i
.
(58)"
N,0.5178571428571429,Under review as a conference paper at ICLR 2022
N,0.5194805194805194,"Similarly for Q, let D :=

I −

(1 −ηλ)I −ηΛ
t
, then we have,"
N,0.5211038961038961,Q(t) : = 1
N,0.5227272727272727,"d
ˆW T F T F ˆW,
(59) = 1"
N,0.5243506493506493,"dW
T 
I −

(1 −ηλ)I −ηXT X
t
F T F

I −

(1 −ηλ)I −ηXT X
t"
N,0.525974025974026,"W,
(60) = 1"
N,0.5275974025974026,"dW
T V DV T F T FV DV T W,
(61) = 1"
N,0.5292207792207793,"dW
T V D ˜F T ˜FDV T W,
( ˜F := FV, X = UΛ1/2V T , ˜ϵ := U T ϵ)
(62) = 1"
N,0.5308441558441559,"d(W T F −1T V + Λ−1/2˜ϵ)
Λ
Λ + λI D ˜F T ˜FD
Λ
Λ + λI (V T F −1W + Λ−1/2˜ϵ),
(63) = 1"
N,0.5324675324675324,"d(W T ˜F −1T + Λ−1/2˜ϵ)
Λ
Λ + λI D ˜F T ˜FD
Λ
Λ + λI ( ˜F −1W + Λ−1/2˜ϵ),
(64) = 1"
N,0.5340909090909091,"dW T ˜F −1T
Λ
Λ + λI D ˜F T ˜FD
Λ
Λ + λI
˜F −1W,
(65) + 1"
N,0.5357142857142857,"dΛ−1/2˜ϵ
Λ
Λ + λI D ˜F T ˜FD
Λ
Λ + λI Λ−1/2˜ϵ,
(66) = 1"
N,0.5373376623376623,"dTr
h
AT A
i
+ σ2
ϵ
d Tr
h
BT B
i
(67)"
N,0.538961038961039,"where,"
N,0.5405844155844156,"A : = ˜F

I −

(1 −ηλ)I −ηΛ
t
Λ
Λ + λI
˜F −1
and,
(68)"
N,0.5422077922077922,"B : = ˜F

I −

(1 −ηλ)I −ηΛ
t
Λ
Λ + λI Λ−1"
N,0.5438311688311688,"2 .
(69)"
N,0.5454545454545454,"For simplicity and brevity of the results, in the main text, we only present the results where σ2
ϵ = 0
and λ = 0. Substituting σ2
ϵ = λ = 0 leads to the following expressions,"
N,0.547077922077922,R(t) = 1
N,0.5487012987012987,"dTr
h 
I −I −ηΛ]ti
.
(70)"
N,0.5503246753246753,Q(t) = 1
N,0.551948051948052,"dTr
h
AT A
i
where,
A := FV

I −

I −ηΛ
t
V T F −1,
(71)"
N,0.5535714285714286,and that concludes the proof.
N,0.5551948051948052,"B.3
THE SPECIAL CASE APPROXIMATE DYNAMICS (EQS. 13 AND 15)"
N,0.5568181818181818,"Recall that the teacher and student are deﬁned as,"
N,0.5584415584415584,"y := y∗+ ϵ,
y∗:= zT W,
ˆy := xT ˆW,
x := F T z,
(72)"
N,0.560064935064935,"where ϵ ∼N(0, σ2
ϵ ) is the label noise, F is the modulation matrix, and ||z||2
2 = ||W||2
2 = 1."
N,0.5616883116883117,"The training and generalization losses are deﬁned as,"
N,0.5633116883116883,LT := 1
N,0.564935064935065,2n
N,0.5665584415584416,"X
(ˆy −y)2 + λ"
N,0.5681818181818182,"2 || ˆW||2
2,
LG := 1"
N,0.5698051948051948,"2Ez[(ˆy −y∗)2].
(73)"
N,0.5714285714285714,"According to Eq. 6, the generalization loss can be written in terms of two scalar variables R and Q,"
N,0.573051948051948,LG = 1
N,0.5746753246753247,"2(1 + Q −2R),
where,
(74)"
N,0.5762987012987013,R : = Ez[y∗T ˆy] = Ez[W T zzT F ˆW] = 1
N,0.577922077922078,"dW T F ˆW,
and,
(75)"
N,0.5795454545454546,Q : = Ez[ˆyT ˆy] = Ez[ ˆW T F T zzT F ˆW] = 1
N,0.5811688311688312,"d
ˆW T F T F ˆW.
(76)"
N,0.5827922077922078,Under review as a conference paper at ICLR 2022
N,0.5844155844155844,"Now, applying t steps of SGD on LT results in the following distribution for the student’s weights,"
N,0.586038961038961,"P( ˆW, t) =
1
Zβ,t
e−β ˜
LT ( ˆ
W ,t),
(77)"
N,0.5876623376623377,"in which ˜LT ( ˆW, t) is a modiﬁed loss where its equilibrium coincides with the tth iterate of SGD on
the original loss LT ( ˆW)."
N,0.5892857142857143,"In Eq. 77 the scalar variable β depends on the noise of SGD and Zβ,t is the partition function which
is deﬁned as,"
N,0.5909090909090909,"Zβ,t ="
N,0.5925324675324676,"R ∞
−∞
Qd
i=1 d
  ˆWi

δ

1
d ˆW T
i F T F ˆWi −Q0

P( ˆWi, t)
R ∞
−∞
Qd
i=1 d
  ˆWi

δ

1
d ˆW T
i F T F ˆWi −Q0

,
(78)"
N,0.5941558441558441,"in which, Q0 can be perceived to be a target norm the student weights ˆW are being constrained to
and d is the dimensionality of the data. It can be interpreted that the partition function Zβ,t counts
the students."
N,0.5957792207792207,"We are now interested in ﬁnding R and Q of the typical (most probable) students. Therefore, it
sufﬁces to ﬁnd the students that dominate the partition function (or more precisely the free-energy).
The free-energy is deﬁned as,"
N,0.5974025974025974,f := −1
N,0.599025974025974,"βdEW,z

ln Zβ,t

,
(79)"
N,0.6006493506493507,"where W and z are the teacher’s weight and input, respectively."
N,0.6022727272727273,"Due to the logarithm inside the expectation, analytical computation of Eq. 79 is intractable. How-
ever, the replica method (M´ezard et al., 1987) allows us to tackle this through the following identity,"
N,0.6038961038961039,"EW,z[ln Zβ,t] = lim
r→0
EW,z[Zr
β,t] −1
r
.
(80)"
N,0.6055194805194806,"The case where F = I.
As a ﬁrst step, we ﬁrst study a case where F = I. In that case, as derived
in B¨os (1998), Eq. 79 can be simpliﬁed to,"
N,0.6071428571428571,−βf = 1
N,0.6087662337662337,"2
Q −R2"
N,0.6103896103896104,Q0 −Q + 1
N,0.612012987012987,2 ln(Q0 −Q) −n
N,0.6136363636363636,2d ln[1 + β(Q0 −Q)] −nβ
D,0.6152597402597403,"2d
G −2HR + Q
1 + β(Q0 −Q),
(81)"
D,0.6168831168831169,"in which the scalar variables G and H are deﬁned as,"
D,0.6185064935064936,"H : = Ey∗[y∗T y] = Ey∗[y∗T (y∗+ ϵ)] = 1,
(82)"
D,0.6201298701298701,"G : = Ey∗[yT y] = Ey∗[(y∗+ ϵ)T (y∗+ ϵ)] = 1 + σ2
ϵ .
(83)"
D,0.6217532467532467,"At this point, in order to ﬁnd the most probable students, one can extremize the free-energy
f(R, Q, Q0) in Eq. 81. The solution to this extermination is derived in B¨os et al. (1993) and
reads,"
D,0.6233766233766234,"∇Rf = 0
⇒
R = n"
D,0.625,"d
1
a,
(84)"
D,0.6266233766233766,"∇Qf = 0
⇒
Q = n"
D,0.6282467532467533,"d
1
a2 −n/d"
D,0.6298701298701299,"
G −n"
D,0.6314935064935064,"d
2 −a a"
D,0.6331168831168831,"
,
(85)"
D,0.6347402597402597,"∇Q0f = 0
⇒
a = 1 +
2˜λ"
D,0.6363636363636364,"1 −n/d −˜λ +
q"
D,0.637987012987013,"(1 −n/d −˜λ)2 + 4˜λ
,
(86)"
D,0.6396103896103896,"in which,"
D,0.6412337662337663,"a := 1 +
1
β(Q0 −Q),
and,
˜λ := λ + 1"
D,0.6428571428571429,"ηt.
(87)"
D,0.6444805194805194,Under review as a conference paper at ICLR 2022
D,0.6461038961038961,The case where F follows Assumption 1.
D,0.6477272727272727,"Assumption. The modulation matrix, F, under a SVD, F := UΣV T has two sets of singular values
such that the ﬁrst p singular values are equal to σ1 and the remaining d−p singular values are equal
to σ2. We let the condition number of F to be denoted by κ := σ1"
D,0.6493506493506493,σ2 > 1.
D,0.650974025974026,"Without loss of generality, we assume that U = V = I. Consequently, the (noiseless) teacher and
the student can be written as the composition of two sub-models as following,"
D,0.6525974025974026,"y∗= y∗
1 + y∗
2 = zT
1 W1 + zT
2 W2,
(teacher decomposition)
(88)"
D,0.6542207792207793,"ˆy = ˆy1 + ˆy2 = σ1zT
1 ˆW1 + σ2zT
2 ˆW2,
(student decomposition)
(89)"
D,0.6558441558441559,in which z1 ∈Rp and z2 ∈Rd−p.
D,0.6574675324675324,"Let ˆyi denote the output of the ith component of the student. Also let y∗
i and yi denote the noiseless
and noisy targets, respectively. Therefore, for the student components i ∈1, 2, we have,"
D,0.6590909090909091,"ˆy1 = σ1zT
1 ˆW1,"
D,0.6607142857142857,"y∗
1 = zT
1 W1,"
D,0.6623376623376623,"y1 = y∗
1 + zT
2 W2 −σ2zT
2 ˆW2
|
{z
}
y∗
2−ˆy2=ϵ2(t) +ϵ,"
D,0.663961038961039,"ˆy2 = σ2zT
2 ˆW2,"
D,0.6655844155844156,"y∗
2 = zT
2 W2,"
D,0.6672077922077922,"y2 = y∗
2 + zT
1 W1 −σ1zT
1 ˆW1
|
{z
}
y∗
1−ˆy1=ϵ1(t) +ϵ,"
D,0.6688311688311688,"in which ϵ is the explicit noise, added to the teacher’s output while ϵj(t) is an implicit variable noise
which decreases as the component j ̸= i learns to match ˆyj and yj."
D,0.6704545454545454,"Accordingly, the variables Hi and Gi for each component i are re-deﬁned as,"
D,0.672077922077922,"H1 = E[y∗
1
T y1] = Ey∗
1 [y∗
1
T y∗
1] = p d,"
D,0.6737012987012987,"G1 = E[yT
1 y1],"
D,0.6753246753246753,"= E[(y∗
1 + y∗
2 −ˆy2)T (y∗
1 + y∗
2 −ˆy2)] + σ2
ϵ ,"
D,0.676948051948052,"= E[y∗
1
T y∗
1] + E[y∗
2
T y∗
2] + E[ˆyT
2 ˆy2],"
D,0.6785714285714286,"−2E[y∗
2
T ˆy2] + σ2
ϵ , = p"
D,0.6801948051948052,d + d −p
D,0.6818181818181818,"d
+ Q2 −2R2 + σ2
ϵ ,"
D,0.6834415584415584,"= 1 + Q2 −2R2 + σ2
ϵ ,"
D,0.685064935064935,"H2 = E[y∗
2
T y2] = Ey∗
2 [y∗
2
T y∗
2] = d −p d
,"
D,0.6866883116883117,"G2 = E[yT
2 y2],"
D,0.6883116883116883,"= E[(y∗
2 + y∗
1 −ˆy1)T (y∗
2 + y∗
1 −ˆy1)] + σ2
ϵ ,"
D,0.689935064935065,"= E[y∗
2
T y∗
2] + E[y∗
1
T y∗
1] + E[ˆyT
1 ˆy1],"
D,0.6915584415584416,"−2E[y∗
1
T ˆy1] + σ2
ϵ ,"
D,0.6931818181818182,"= d −p d
+ p"
D,0.6948051948051948,"d + Q1 −2R1 + σ2
ϵ ,"
D,0.6964285714285714,"= 1 + Q1 −2R1 + σ2
ϵ ,"
D,0.698051948051948,"in which Ri and Qi are deﬁned as,"
D,0.6996753246753247,"Ri := Ez[y∗T
i ˆyi] = 1"
D,0.7012987012987013,"dW T
i σi ˆWi,
and,
Qi := Ez[ˆyT
i ˆyi] = 1"
D,0.702922077922078,"d
ˆW T
i σ2
i ˆWi,"
D,0.7045454545454546,where σi denotes the singular values of the matrix F as deﬁned in Assumption 1.
D,0.7061688311688312,"Rewriting Eqs. 84, 85, and 86 for each of the student’s components, we arrive at,"
D,0.7077922077922078,R1 = n
D,0.7094155844155844,"d
1
a1
,"
D,0.711038961038961,"Q1 =
n
pa2
1 −n"
D,0.7126623376623377,"
1 + Q2 −2R2 + σ2
ϵ −n"
D,0.7142857142857143,"d
2 −a1 a1 
,"
D,0.7159090909090909,"a1 = 1 +
2˜λ1 1 −n"
D,0.7175324675324676,"p −˜λ1 +
q (1 −n"
D,0.7191558441558441,"p −˜λ1)2 + 4˜λ1
,"
D,0.7207792207792207,˜λ1 : = d
D,0.7224025974025974,"p
1
σ2
1
(λ + 1 ηt),"
D,0.724025974025974,R2 = n
D,0.7256493506493507,"d
1
a2
,"
D,0.7272727272727273,"Q2 =
n
(d −p)a2
1 −n"
D,0.7288961038961039,"
1 + Q1 −2R1 + σ2
ϵ −n"
D,0.7305194805194806,"d
2 −a2 a2 
,"
D,0.7321428571428571,"a2 = 1 +
2˜λ"
D,0.7337662337662337,"1 −
n
d−p −˜λ +
q"
D,0.7353896103896104,"(1 −
n
d−p −˜λ)2 + 4˜λ
,"
D,0.737012987012987,"˜λ2 : =
d
d −p
1
σ2
2
(λ + 1 ηt),"
D,0.7386363636363636,Under review as a conference paper at ICLR 2022
D,0.7402597402597403,"where Q1 depends on Q2 and vice versa. However, with simple calculations, we can arrive at the
following standalone equation. Let,"
D,0.7418831168831169,α1 = n
D,0.7435064935064936,"p , α2 =
n
d −p,
(90)"
D,0.7451298701298701,"and also let,"
D,0.7467532467532467,"bi =
αi
a2
i −αi
,
ci = 1 −2Ri −n"
D,0.7483766233766234,"d
2 −ai"
D,0.75,"ai
for
i ∈{1, 2},
(91)"
D,0.7516233766233766,"with which the closed-from scalar expression for Q(t, λ) reads,"
D,0.7532467532467533,"Q(t, λ) = Q1 + Q2,
where,
Q1 := b1b2c2 + b1c1"
D,0.7548701298701299,"1 −b1b2
,
and,
Q2 := b1b2c1 + b2c2"
D,0.7564935064935064,"1 −b1b2
.
(92)"
D,0.7581168831168831,"B.4
REPLICA TRICK"
D,0.7597402597402597,"In the following, we detail the mathematical arguments leading to the replica trick expression. For
some r →0, we can write for any scalar x:"
D,0.7613636363636364,"xr = exp(r ln x) = lim
r→0 1 + r ln x"
D,0.762987012987013,"⇒lim
r→0 r ln x = lim
r→0 xr −1"
D,0.7646103896103896,"⇒ln x = lim
r→0
xr −1 r"
D,0.7662337662337663,"∴E[ln x] = lim
r→0
E[xr] −1"
D,0.7678571428571429,"r
, E : averaging (93)"
D,0.7694805194805194,"B.5
COMPUTATION OF THE FREE-ENERGY"
D,0.7711038961038961,"The self-averaged free energy (per unit weight) of our student network, is given by (Engel & Van den
Broeck, 2001),"
D,0.7727272727272727,−βf = 1
D,0.7743506493506493,"d⟨⟨ln Z⟩⟩z,W
(94)"
D,0.775974025974026,"Here, β = 1/T is the inverse temperature parameter corresponding to our statistical ensemble, d the
(teacher) student network width, and Z the partition function of the system deﬁned as (n: number
of training examples)."
D,0.7775974025974026,"As Gaussian variables (with n, d →∞), in the partition function, to obtain,"
D,0.7792207792207793,"⟨⟨Zr⟩⟩z,W = r
Y a=1 d
Y µ=1"
D,0.7808441558441559,"Z
dµ (W a) dyµ
ad(y∗)µe−βNET (ya,y∗)"
D,0.7824675324675324,"×

δ

y∗µ −1
√"
D,0.7840909090909091,"d
W T x∗µ

δ

yµ
a −1
√"
D,0.7857142857142857,"d
W T
a xµ
 z,W = r
Y a=1 d
Y µ=1"
D,0.7873376623376623,"Z
dµ (W a) dyµ
adˆyµ
a
2π
dy∗µdˆy∗µ"
D,0.788961038961039,"2π
e−βNET (ya,y∗)eiy∗µ ˆy∗µ+iyµ
a ˆyµ
a"
D,0.7905844155844156,"×

exp

−i
√"
D,0.7922077922077922,"d
ˆy∗µW T x∗µ −
i
√"
D,0.7938311688311688,"d
ˆyµ
aW T
a xµ
 z,W (95)"
D,0.7954545454545454,"where in the last line above, we have expressed the inserted δ functions using their integral repre-
sentations. To make further progress, we introduce the auxiliary variables,
X"
D,0.797077922077922,"ija
W i
a∆ijW ∗j = dRa,
(96) X"
D,0.7987012987012987,"ij⟨a,b⟩
W i
aΓijW j
b = dQab
(97)"
D,0.8003246753246753,Under review as a conference paper at ICLR 2022
D,0.801948051948052,"via the respective δ functions, to arrive at,"
D,0.8035714285714286,"⟨⟨Zn⟩⟩z,W =
Y µ,a,b"
D,0.8051948051948052,"Z
dµ (Wa) dyµ
adˆyµ
a
2π
dy∗µdˆy∗µ"
D,0.8068181818181818,"2π
e−βNET (ya,y∗)eiy∗µ ˆy∗µ+iyµ
a ˆyµ
a"
D,0.8084415584415584,"×
Z
PdQab
Z
PdRa δ  X"
D,0.810064935064935,"i,j,a
W i
a∆i,jW ∗j −PRa  δ  X"
D,0.8116883116883117,"ij⟨a,b⟩
W i
aΓijW j
b −PQab   × **"
D,0.8133116883116883,"exp

−Q0 2 X"
D,0.814935064935065,"µ,a
(ˆyµ
a)2 −1 2 X"
D,0.8165584415584416,"µ,⟨a,b⟩
ˆyµ
a ˆyµ
b Qab −
X"
D,0.8181818181818182,"µ,a
ˆy∗µˆyµ
aRa −1 2 X"
D,0.8198051948051948,"µ
(ˆy∗µ)2++"
D,0.8214285714285714,"W
(98)"
D,0.823051948051948,"Repeating the procedure of expressing the above δ functions using their integral representations, we
then get (α = n/d),"
D,0.8246753246753247,"⟨⟨Zn⟩⟩x,x∗,W =
Z Y a,b dQ0
√"
D,0.8262987012987013,"2π
d ˆQ0a"
D,0.827922077922078,"4π
dQab ˆQab"
D,0.8295454545454546,"2π/d
dRa ˆRa"
D,0.8311688311688312,"2π/d exp
iP 2 X"
D,0.8327922077922078,"a
Q0 ˆQ0a + iP
X"
D,0.8344155844155844,"a<b
Qab ˆQab"
D,0.836038961038961,"+ iP
X"
D,0.8376623376623377,"a
Ra ˆRa Z Y i,a"
D,0.8392857142857143,"dW a
i
√"
D,0.8409090909090909,"2π exp

−i 2 X"
D,0.8425324675324676,"i,j,a
ˆQ0aW i
aΓijW j
a −i
X"
D,0.8441558441558441,"i,j,a<b
ˆQabW i
aΓijW j
b −i
X"
D,0.8457792207792207,"i,j,a
ˆRa∆ijW j
a

× Z Y µ,a"
D,0.8474025974025974,"dyµ
adˆyµ
a
2π
dy∗µ
√"
D,0.849025974025974,"2π e−βNET (ya,y∗) exp

−1 2 X"
D,0.8506493506493507,"µ
(y∗µ)2 + i
X"
D,0.8522727272727273,"µ,a
ˆyµ
a ˆyµ
a −1 2 X a,µ"
D,0.8538961038961039," 
1 −R2
a

(ˆyµ
a)2 −1 2 X"
D,0.8555194805194806,"µ,⟨a,b⟩
ˆyµ
a ˆyµ
b
 
Qab −RaRb
−i
X"
D,0.8571428571428571,"µ,a
y∗µˆyµ
aRa (99)"
D,0.8587662337662337,"If we now, perform a singular value decomposition of the covariance matrix Γ as, Γ = UT SU =
VT V, where S: matrix of singular values of Γ, and we have expressed, V = S1/2U, then one can
proceed to write,"
D,0.8603896103896104,"⟨⟨Zn⟩⟩x,W =
1
det |V | Z Y a,b dQ0
√"
D,0.862012987012987,"2π
d ˆQ0a"
D,0.8636363636363636,"4π
dQab ˆQab"
D,0.8652597402597403,"2π/d
dRa ˆRa"
D,0.8668831168831169,"2π/d exp
iP 2 X"
D,0.8685064935064936,"a
Q0 ˆQ0a"
D,0.8701298701298701,"+ iP
X"
D,0.8717532467532467,"a<b
Qab ˆQab + iP
X"
D,0.8733766233766234,"a
Ra ˆRa Z Y i,a"
D,0.875,"d ˜W a
i
√"
D,0.8766233766233766,"2π exp

−i 2 X"
D,0.8782467532467533,"i,a
ˆQ0a

˜W i
a
2 −i
X"
D,0.8798701298701299,"i,a<b
ˆQab ˜W i
a ˜W i
b −i
X"
D,0.8814935064935064,"i,j,a
ˆRa ˜W j
a

×
Z Y µ,a"
D,0.8831168831168831,"dyµ
adˆyµ
a
2π
dy∗µ
√"
D,0.8847402597402597,"2π e−βNET (ya,y∗)"
D,0.8863636363636364,"exp

−1 2 X"
D,0.887987012987013,"µ
(y∗
µ)2 + i
X"
D,0.8896103896103896,"µ,a
ˆyµ
a ˆyµ
a −1 2 X a,µ"
D,0.8912337662337663," 
1 −R2
a

(ˆyµ
a)2 −i
X"
D,0.8928571428571429,"µ,a
y∗µˆyµ
aRa −1 2 X"
D,0.8944805194805194,"µ,⟨a,b⟩
ˆyµ
a ˆyµ
b
 
Qab −RaRb  (100)"
D,0.8961038961038961,"having expressed, ˜Wa = VWa, and identifying ∆= S1/2U from our deﬁnitions. Now, since in
the above, the W a
i integrals factorize in i, and similarly the yµ
a, ˆyµ
a and dy∗µ factorize in µ, one can
proceed to write:"
D,0.8977272727272727,"⟨⟨Zn⟩⟩x,W =
1
det |V | Z Y a,b"
D,0.8993506493506493,"dQ0d ˆQ0a
√"
D,0.900974025974026,"2π4π
dQab ˆQab"
D,0.9025974025974026,"2π/d
dRa ˆRa"
D,0.9042207792207793,"2π/d exp

P
h i 2 X"
D,0.9058441558441559,"a
Q0 ˆQ0a + i
X"
D,0.9074675324675324,"a<b
Qab ˆQab + i
X"
D,0.9090909090909091,"a
Ra ˆRa + GS( ˆQ0a, ˆQab, ˆRa) + αGE(Qab, Ra)
i (101)"
D,0.9107142857142857,Under review as a conference paper at ICLR 2022
D,0.9123376623376623,"where,"
D,0.913961038961039,"GS( ˆQ0a, ˆQab, ˆRa) = ln
Z Y a"
D,0.9155844155844156,d ˜W a √
D,0.9172077922077922,"2π exp

−i 2 X"
D,0.9188311688311688,"a
ˆQ0a ˜W i
a ˜W i
a −i
X"
D,0.9204545454545454,"a<b
ˆQab ˜Wa ˜Wb −i
X"
D,0.922077922077922,"a
ˆRa ˜Wa
"
D,0.9237012987012987,"GE(Qab, Ra) = ln
Z Y a"
D,0.9253246753246753,dyadˆya
D,0.926948051948052,"2π
dy∗
√"
D,0.9285714285714286,"2π e−βNET (ya,y∗) exp

−1"
D,0.9301948051948052,"2(y∗)2 + i
X"
D,0.9318181818181818,"a
ˆyaˆya −1 2 X a"
D,0.9334415584415584," 
1 −R2
a

(ˆya)2 −1 2 X"
D,0.935064935064935,"⟨a,b⟩
ˆyaˆyb
 
Qab −RaRb
−iy∗µ X"
D,0.9366883116883117,"a
ˆyaRa (102)"
D,0.9383116883116883,"Now, in the limit d →∞, Eq. 101 can be approximated using the saddle-point approach (Bender &
Orszag, 2013),"
D,0.939935064935065,"⟨⟨Zn⟩⟩x,W ≈extrQ0, ˆ
Q0a,Qab, ˆ
Qab,Ra, ˆ
Ra exp

P
h i 2 X"
D,0.9415584415584416,"a
Q0 ˆQ0a + i
X"
D,0.9431818181818182,"a<b
Qab ˆQab + i
X"
D,0.9448051948051948,"a
Ra ˆRa + GS( ˆQ0a, ˆQab, ˆRa) + αGE(Qab, Ra)
i
(103)"
D,0.9464285714285714,"where, extr corresponds to extremization of ⟨⟨Zn⟩⟩x,W over the respective order parameters. Per-
forming this extremization over ˆQ0a, ˆQab and ˆRa, then generates an expression of the form,"
D,0.948051948051948,"⟨⟨Zn⟩⟩x,W = extrQ0,Q,R exp ( nN"
D,0.9496753246753247,"1
2
Q −R2"
D,0.9512987012987013,Q0 −Q + 1
D,0.952922077922078,2 ln(Q0 −Q) −α
D,0.9545454545454546,2 ln [1 + β(Q0 −Q)] −αβ
D,0.9561688311688312,"2
1 −2R + Q
1 + β(Q0 −Q) !) (104)"
D,0.9577922077922078,"where we have invoked replica symmetry in the form, Qab = Q and Ra = R, and that ET =
(y∗−y)2/2. Plugging this back into Eq. ??, then ﬁnally yields,"
D,0.9594155844155844,"βf = −extrQ0,Q,R"
D,0.961038961038961,"(
1
2
Q −R2"
D,0.9626623376623377,Q0 −Q + 1
D,0.9642857142857143,2 ln(Q0 −Q) −α
D,0.9659090909090909,2 ln [1 + β(Q0 −Q)] −αβ
D,0.9675324675324676,"2
1 −2R + Q
1 + β(Q0 −Q)"
D,0.9691558441558441,")
(105)"
D,0.9707792207792207,"The remaining pair of order parameters generate the following set of transcendental equations on
extremization (B¨os, 1998): R = α a"
D,0.9724025974025974,"Q =
α
a2 −α"
D,0.974025974025974,"
1 −2 −a a
α
"
D,0.9756493506493507,"Q0 = Q +
1
β (a −1) (106)"
D,0.9772727272727273,"where, a = max[1, α] for T →0."
D,0.9788961038961039,"Now, the above determined values of R, Q and Q0 can be perceived as the maximally likely values of
R, Q and Q0 of our teacher-student setup, for an inverse temperature β parameterizing the system."
D,0.9805194805194806,"C
EXTENDED EXPERIMENTS"
D,0.9821428571428571,"Figure 4 presents the analytical generalization dynamics for two values of κ and provides compar-
ison between the theory and simulation results of the same model. We observe that the theory and"
D,0.9837662337662337,Under review as a conference paper at ICLR 2022
D,0.9853896103896104,MSE generalization error
D,0.987012987012987,γ = 0.0
D,0.9886363636363636,γ = 0.1
D,0.9902597402597403,Theory
D,0.9918831168831169,Experiment
D,0.9935064935064936,Training time
D,0.9951298701298701,"Figure 4: The teacher-student set-up in Sec. equation 2.1. We compare the analytical solutions to
simulations performed on our teacher-student setup with d = 100, p = 50, n = 150 and we plot the
error bars over 100 random seeds. The solutions and the simulations match closely and we observe
double descent over the generalization error."
D,0.9967532467532467,"simulations accurately match. Further experiments are provided in the following anonymous Colab
notebook."
D,0.9983766233766234,"Before diving into the theory, we invite the reader to recall a simple equation from thermodynam-
ics. Consider an ideal gas in a container with its large number of molecules moving around, col-
liding with each other, all while obeying Newton’s laws. While the exact dynamics of each of
such molecules is intractable, the system’s macroscopic behavior can be characterized in terms of a
handful of scalar quantities, namely, the pressure P, the volume V , and the temperature T. By av-
eraging over suitable probability measures and applying the principle of free-energy minimization,
one arrives at a remarkably simple relationship between these three macroscopic variables, i.e., the
well-known PV = nRT (n: number of moles of gas, R: gas constant) (Reif, 2009)."
