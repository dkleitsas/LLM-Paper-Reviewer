Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0017482517482517483,"This paper is concerned with online filtering of discretely observed nonlinear diffu-
1"
ABSTRACT,0.0034965034965034965,"sion processes. Our approach is based on the fully adapted auxiliary particle filter
2"
ABSTRACT,0.005244755244755245,"which involves Doob’s h-transforms that are typically intractable. We propose a
3"
ABSTRACT,0.006993006993006993,"computational framework to approximate these h-transforms by solving the under-
4"
ABSTRACT,0.008741258741258742,"lying backward Kolmogorov equations using nonlinear Feynman-Kac formulas.
5"
ABSTRACT,0.01048951048951049,"The methodology allows one to train a locally optimal particle filter prior to the
6"
ABSTRACT,0.012237762237762238,"data-assimilation procedure. Numerical experiments illustrate that the proposed
7"
ABSTRACT,0.013986013986013986,"approach can be orders of magnitude more efficient than the bootstrap particle fil-
8"
ABSTRACT,0.015734265734265736,"ter in the regime of highly informative observations and when the observations are
9"
ABSTRACT,0.017482517482517484,"extreme under the model.
10"
INTRODUCTION,0.019230769230769232,"1
Introduction
11"
INTRODUCTION,0.02097902097902098,"Diffusion processes are fundamental tools in applied mathematics, statistics and machine learning.
12"
INTRODUCTION,0.022727272727272728,"This rich class of continuous-time models have been used to model real-world phenomena in disci-
13"
INTRODUCTION,0.024475524475524476,"plines as varied as life-sciences, engineering, economics and finance. However, working with dif-
14"
INTRODUCTION,0.026223776223776224,"fusions can be challenging as its transition densities are only tractable in simple and specific cases
15"
INTRODUCTION,0.027972027972027972,"such as (geometric) Brownian motions, Ornstein-Uhlenbeck (OU) processes and Cox-Ingersoll-Ross
16"
INTRODUCTION,0.02972027972027972,"processes. This difficulty has hindered the use of standard methodologies for inference and data-
17"
INTRODUCTION,0.03146853146853147,"assimilation of models driven by diffusion processes. Specialized methodologies have been devel-
18"
INTRODUCTION,0.033216783216783216,"oped to circumvent or mitigate these issues [35, 4, 3, 14, 13, 5, 37].
19"
INTRODUCTION,0.03496503496503497,"Consider a time-homogeneous multivariate diffusion process dXt = µ(Xt) dt + σ(Xt) dBt that
20"
INTRODUCTION,0.03671328671328671,"is discretely observed at regular intervals. Noisy observations yk of the latent process Xtk is
21"
INTRODUCTION,0.038461538461538464,"collected at time tk ≡k T for k ≥1. We consider the online filtering problem which consists in
22"
INTRODUCTION,0.04020979020979021,"estimating the conditional laws πk(dx) = P(Xtk ∈dx|y1, . . . , yk), i.e. the filtering distributions,
23"
INTRODUCTION,0.04195804195804196,"as observations are collected. We focus on the use of Particle Filters (PF) that approximate the
24"
INTRODUCTION,0.043706293706293704,"filtering distributions with a system of weighted particles. Although many previous works have
25"
INTRODUCTION,0.045454545454545456,"relied on the Bootstrap Particle Filter (BPF), which simulates particles from the diffusion process,
26"
INTRODUCTION,0.0472027972027972,"it can perform poorly in challenging scenarios as it fails to take the incoming observation yk into
27"
INTRODUCTION,0.04895104895104895,"account. This issue can be partially tackled by relying on resampling at intermediate times between
28"
INTRODUCTION,0.050699300699300696,"observations [10, 31]. The goal of this article is to show that the (locally) optimal approach given by
29"
INTRODUCTION,0.05244755244755245,"the Fully Adapted Auxiliary Particle Filter (FA-APF) [33] can be implemented. This necessitates
30"
INTRODUCTION,0.05419580419580419,"simulating a conditioned diffusion process, which can be formulated as a control problem involving
31"
INTRODUCTION,0.055944055944055944,"an intractable Doob’s h-transform [36, 8]. We propose the Computational Doob’s h-Transform
32"
INTRODUCTION,0.057692307692307696,"(CDT) framework for efficiently approximating these quantities. The method relies on nonlinear
33"
INTRODUCTION,0.05944055944055944,"Feynman-Kac formulas for solving backward Kolmogorov equations simultaneously for all possible
34"
INTRODUCTION,0.06118881118881119,"observations. Importantly, this preprocessing step only needs to be performed once before starting the
35"
INTRODUCTION,0.06293706293706294,"online filtering procedure. Numerical experiments illustrate that the proposed approach can be orders
36"
INTRODUCTION,0.06468531468531469,"of magnitude more efficient than the BPF in the regime of highly informative observations or when
37"
INTRODUCTION,0.06643356643356643,"the observations are extreme under the model. A PyTorch implementation to reproduce our numerical
38"
INTRODUCTION,0.06818181818181818,"experiments is available at https://anonymous.4open.science/r/CompDoobTransform/.
39"
INTRODUCTION,0.06993006993006994,"Notations.
For two matrices A, B ∈Rd,d, their Frobenius inner product is defined as ⟨A, B⟩F =
40
Pd
i,j=1 Ai,jBi,j. The Euclidean inner product for u, v ∈Rd is denoted as ⟨u, v⟩= Pd
i=1 uivi.
41"
BACKGROUND,0.07167832167832168,"2
Background
42"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.07342657342657342,"2.1
Filtering of discretely observed diffusions
43"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.07517482517482517,"Consider an homogeneous diffusion process {Xt}t≥0 in X = Rd with initial distribution ρ0(dx) and
44"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.07692307692307693,"dynamics
45"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.07867132867132867,"dXt = µ(Xt) dt + σ(Xt) dBt,
(1)"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.08041958041958042,"described by the drift and volatility functions µ : Rd →Rd and σ : Rd →Rd,d. We assume standard
46"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.08216783216783216,"smoothness and growth conditions [27] for a unique strong solution of (1) to exist for all times. The
47"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.08391608391608392,"associated semi-group of transition probabilities ps(dbx | x) satisfies P(Xt+s ∈A | Xt = x) =
48
R"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.08566433566433566,"A ps(dbx | x) for any s, t > 0 and measurable A ⊂X. The process {Bt}t≥0 is a standard Rd-
49"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.08741258741258741,"valued Brownian motion. The diffusion process {Xt}t≥0 is discretely observed at time tk = kT,
50"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.08916083916083917,"for k ≥1, for some inter-observation time T > 0. The Y-valued observation Yk ∈Y at time tk is
51"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.09090909090909091,"modelled by the likelihood function g : X × Y →R+ in the sense that for any measurable A ⊂Y,
52"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.09265734265734266,"we have P(Yk ∈A | Xtk = xk) =
R"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.0944055944055944,"A g(xk, y) dy for some dominating measure dy on Y. For a
53"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.09615384615384616,"test function φ : X →R, the generator of the diffusion process {Xt}t≥0 is given by
54"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.0979020979020979,"Lφ = ⟨µ, ∇φ⟩+ 1"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.09965034965034965,"2 ⟨σσ⊤, ∇2φ⟩F.
(2)"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.10139860139860139,"This article is concerned with approximating the filtering distributions πk(dx) = P(Xtk ∈dx |
55"
FILTERING OF DISCRETELY OBSERVED DIFFUSIONS,0.10314685314685315,"y1, . . . , yk). For notational convenience, we set π0(dx) ≡ρ0(dx).
56"
PARTICLE FILTERING,0.1048951048951049,"2.2
Particle filtering
57"
PARTICLE FILTERING,0.10664335664335664,"Particle Filters (PF), also known as Sequential Monte Carlo methods, are a set of Monte Carlo
58"
PARTICLE FILTERING,0.10839160839160839,"algorithms that can be used to solve filtering problems (see [7] for a recent textbook on the topic). PFs
59"
PARTICLE FILTERING,0.11013986013986014,"evolve a set of N ≥1 particles x1:N
t
= (x1
t, . . . , xN
t ) ∈X N forward in time using a combination of
60"
PARTICLE FILTERING,0.11188811188811189,"propagation and resampling operations.
61"
PARTICLE FILTERING,0.11363636363636363,"To initialize the PF, each initial particle xj
0 ∈X for 1 ≤j ≤N is sampled independently from
62"
PARTICLE FILTERING,0.11538461538461539,"the distribution ρ0(dx) so that π0(dx) ≈N −1 PN
j=1 δ(dx; xj
0). Approximations of the filtering
63"
PARTICLE FILTERING,0.11713286713286714,"distribution πk for k ≥1 are built recursively as follows. Given the Monte Carlo approximation
64"
PARTICLE FILTERING,0.11888111888111888,"of the filtering distribution at time tk, πk(dx) ≈N −1 PN
j=1 δ(dx; xj
tk), the particles x1:N
tk
are
65"
PARTICLE FILTERING,0.12062937062937062,"propagated independently forward in time by bxj
tk+1 ∼qk+1(dbx | xj
tk), using a Markov kernel
66"
PARTICLE FILTERING,0.12237762237762238,"qk+1(dbx | x) specified by the user. The BPF corresponds to the choice of Markov kernel qBPF
k+1(dbx |
67"
PARTICLE FILTERING,0.12412587412587413,"x) = P(Xtk+1 ∈dbx | Xtk = x) while the FA-APF [33] corresponds to the choice
68"
PARTICLE FILTERING,0.1258741258741259,"qFA-APF
k+1
(dbx | x) = P(Xtk+1 ∈dbx | Xtk = x, Yk+1 = yk+1).
(3)"
PARTICLE FILTERING,0.12762237762237763,"Each particle bxj
tk+1 is associated with a normalized weight W
j
k+1 = W j
k+1/ PN
i=1 W i
k+1, where the
69"
PARTICLE FILTERING,0.12937062937062938,"unnormalized weights W j
k+1 > 0 are defined as
70"
PARTICLE FILTERING,0.13111888111888112,"W j
k+1 =
pT(dbxj
tk+1 | xj
tk)"
PARTICLE FILTERING,0.13286713286713286,"qk+1(dbxj
tk+1 | xj
tk)
g(bxj
tk+1, yk+1).
(4)"
PARTICLE FILTERING,0.1346153846153846,"The BPF and FA-APF correspond respectively to having
71"
PARTICLE FILTERING,0.13636363636363635,"W j,BPF
k+1
= g(bxj
tk+1, yk+1)
and
W j,FA-APF
k+1
= E[g(Xtk+1, yk+1) | Xtk = xj
tk].
(5)"
PARTICLE FILTERING,0.1381118881118881,"The weights are such that πk+1(dx) ≈PN
j=1 W
j
k+1 δ(dx; xj
tk+1). The resampling step consists in
72"
PARTICLE FILTERING,0.13986013986013987,"defining a new set of particles x1:N
tk+1 with P(xj
tk+1 = bxi
tk+1) = W
i
k+1. This resampling scheme
73"
PARTICLE FILTERING,0.14160839160839161,"ensures that the equally weighted set of particles x1:N
tk+1 provides a Monte Carlo approximation of the
74"
PARTICLE FILTERING,0.14335664335664336,"filtering distribution at time tk+1 in the sense that πk+1(dx) ≈N −1 PN
j=1 δ(dx; xj
tk+1). Note that
75"
PARTICLE FILTERING,0.1451048951048951,"the particles x1:N
tk+1 do not need to be resampled independently given the set of propagated particles
76"
PARTICLE FILTERING,0.14685314685314685,"bx1:N
tk+1. We refer the reader to [15] for a recent discussion of resampling schemes within PFs and to
77"
PARTICLE FILTERING,0.1486013986013986,"[9] for a book-length treatment of the convergence properties of this class of Monte Carlo methods.
78"
PARTICLE FILTERING,0.15034965034965034,"In most settings, the FA-APF [33] that minimizes a local variance criterion [12] leads to better
79"
PARTICLE FILTERING,0.1520979020979021,"performance when compared to the BPF. This gain in efficiency can be very substantial when the
80"
PARTICLE FILTERING,0.15384615384615385,"signal-to-noise ratio is high or when observations contain outliers under the model specification.
81"
PARTICLE FILTERING,0.1555944055944056,"Nevertheless, implementing FA-APF requires sampling from the conditioned transition probability
82"
PARTICLE FILTERING,0.15734265734265734,"in (3), which is typically not feasible in practice. We will show in the following that this can be
83"
PARTICLE FILTERING,0.1590909090909091,"achieved in our setting by simulating a conditioned diffusion. We note also that standard strategies to
84"
PARTICLE FILTERING,0.16083916083916083,"approximate the FA-APF do not apply to our setup as the latent state process evolves on a higher
85"
PARTICLE FILTERING,0.16258741258741258,"frequency relative to the observations.
86"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.16433566433566432,"2.3
Conditioned and controlled diffusions
87"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.1660839160839161,"As the diffusion process (1) is assumed to be time-homogeneous, it suffices to focus on the initial
88"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.16783216783216784,"interval [0, T] and study the dynamics of the diffusion X[0,T] = {Xt}t∈[0,T] conditioned upon the
89"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.16958041958041958,"first observation YT = y. The conditioned dynamics can also be described by a diffusion process.
90"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.17132867132867133,"Contrarily to the original diffusion, the conditioned process is not time-homogeneous in general. The
91"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.17307692307692307,"conditioned process is described by the same volatility function but with a different drift term that
92"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.17482517482517482,"takes the future observation YT = y into account.
93"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.17657342657342656,"Before deriving the exact form of the conditioned diffusion in Section 2.4, this section describes a
94"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.17832167832167833,"more general setting that will be of crucial importance in our proposed numerical scheme. For a
95"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.18006993006993008,"control function c : X × Y × [0, T] →Rd and a given observation y ∈Y, consider the controlled
96"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.18181818181818182,"diffusion process {Xc,y
t
}t∈[0,T] satisfying
97"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.18356643356643357,"dXc,y
t
= µ(Xc,y
t
) dt + σ(Xc,y
t
) dBt
|
{z
}
(original dynamics)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.1853146853146853,"+ [σ c](Xc,y
t
, y, t) dt
|
{z
}
(control drift term) .
(6)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.18706293706293706,"The dynamics of the controlled diffusion is identical to the original diffusion, except for the additional
98"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.1888111888111888,"drift term [σ c](x, y, t) ∈Rd described by the control function c. For y ∈Y and a test function
99"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.19055944055944055,"φ : X →R, the generator of the controlled diffusion is given by
100"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.19230769230769232,"Lc,y,tφ(x) = Lφ(x) + ⟨[σc](x, y, t), ∇φ(x)⟩.
(7)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.19405594405594406,"Let P[0,T] and Pc,y
[0,T] denote the probability measures on the space of continuous functions
101"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.1958041958041958,"C([0, T], Rd) generated by the original and controlled diffusions respectively. Under mild growth as-
102"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.19755244755244755,"sumptions on the control c, the two measures are equivalent and Girsanov’s theorem [16] shows that
103"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.1993006993006993,"dP[0,T ]
dPc,y
[0,T]
(X[0,T]) = exp ( −1 2 Z T"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.20104895104895104,"0
∥c(Xt, y, t)∥2 dt −
Z T"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.20279720279720279,"0
⟨c(Xt, y, t), dBt⟩ ) .
(8)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.20454545454545456,"Our main objective is to construct a control function c⋆: X × Y × [0, T] →Rd so that, for any
104"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.2062937062937063,"observation value y ∈Y, the controlled diffusion Xc⋆,y
[0,T] has the same dynamics as the original
105"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.20804195804195805,"diffusion X[0,T ] conditioned upon the observation YT = y, i.e. for any measurable set A ⊂
106"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.2097902097902098,"C([0, T], Rd), we have
107"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.21153846153846154,"P

Xc⋆,y
[0,T] ∈A

= E

1(X[0,T] ∈A) g(XT, y)

/ E[g(XT, y)].
(9)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.21328671328671328,"We will give an exact expression of this control in Section 2.4 and propose a numerical scheme to
108"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.21503496503496503,"approximate it in Section 3.1.
109"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.21678321678321677,"2.4
Doob’s h-transform
110"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.21853146853146854,"To simplify notation, we shall denote the conditioned process X[0,T] | (YT = y) as bX[0,T]. To
111"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.2202797202797203,"describe its dynamics, we introduce the function
112"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.22202797202797203,"h(x, y, t) = E[g(XT, y) | Xt = x] =
Z"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.22377622377622378,"X
g(xT, y) pT−t(dxT | x)
(10)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.22552447552447552,"which gives the probability of observing YT = y when the diffusion has state x ∈X at time
113"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.22727272727272727,"t ∈[0, T]. We recall that the likelihood function g : X × Y →R+ was defined in Section 2.1.
114"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.229020979020979,"The definition in (10) implies that the function h : X × Y × [0, T] →R+ satisfies the backward
115"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.23076923076923078,"Kolmogorov equation [27],
116"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.23251748251748253,"(∂t + L)h = 0,
(11)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.23426573426573427,"with terminal condition h(x, y, T) = g(x, y) for all (x, y) ∈X × Y. For φ : X →R and an
117"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.23601398601398602,"infinitesimal increment δ > 0, we have
118"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.23776223776223776,"E[φ( bXt+δ)| bXt = x] = E[φ(Xt+δ) g(XT, y) | Xt = x] / E[g(XT, y)|Xt = x]
= E[φ(Xt+δ) h(Xt+δ, y, t + δ) | Xt = x] / h(x, y, t)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.2395104895104895,"= φ(x) + δ
L[φ h] h"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.24125874125874125,"
(x, y, t) + O(δ2). (12)"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.243006993006993,"Furthermore, since the function h satisfies (11), some algebra shows that L[φ h]/h = Lφ +
119"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.24475524475524477,"⟨σσ⊤∇log h, ∇φ⟩. By taking δ →0, this heuristic derivation shows that the generator of the condi-
120"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.2465034965034965,"tioned diffusion equals Lφ + ⟨σσ⊤∇log h, ∇φ⟩. Hence bX[0,T] satisfies the dynamics of a controlled
121"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.24825174825174826,"diffusion (6) with control function c⋆(x, y, t) = [σ⊤∇log h](x, y, t). We refer readers to [36, 8] for
122"
CONDITIONED AND CONTROLLED DIFFUSIONS,0.25,"a formal treatment of Doob’s h-transform.
123"
NONLINEAR FEYNMAN-KAC FORMULA,0.2517482517482518,"2.5
Nonlinear Feynman-Kac formula
124"
NONLINEAR FEYNMAN-KAC FORMULA,0.2534965034965035,"Obtaining the control function c⋆(x, y, t) = [σ⊤∇log h](x, y, t) by solving the backward Kol-
125"
NONLINEAR FEYNMAN-KAC FORMULA,0.25524475524475526,"mogorov equation in (11) for each possible observation y ∈Y is computationally not feasible. Fur-
126"
NONLINEAR FEYNMAN-KAC FORMULA,0.256993006993007,"thermore, when the dimensionality of the state-space X becomes larger, standard numerical methods
127"
NONLINEAR FEYNMAN-KAC FORMULA,0.25874125874125875,"for solving Partial Differential Equations (PDEs) such as Finite Differences or the Finite Element
128"
NONLINEAR FEYNMAN-KAC FORMULA,0.26048951048951047,"Method become impractical. For these reasons, we propose instead to approximate the control func-
129"
NONLINEAR FEYNMAN-KAC FORMULA,0.26223776223776224,"tion c⋆with neural networks, and employ methods based on automatic differentiation and the nonlin-
130"
NONLINEAR FEYNMAN-KAC FORMULA,0.263986013986014,"ear Feynman-Kac approach to solve semilinear PDEs [19, 20, 24, 17, 6, 22, 23, 1, 18].
131"
NONLINEAR FEYNMAN-KAC FORMULA,0.26573426573426573,"As the non-negative function h typically decays exponentially for large ∥x∥, it is computationally
132"
NONLINEAR FEYNMAN-KAC FORMULA,0.2674825174825175,"more stable to work on the logarithmic scale and approximate the value function v(x, y, t) =
133"
NONLINEAR FEYNMAN-KAC FORMULA,0.2692307692307692,"−log[h(x, y, t)]. Using the fact that h satisfies the PDE (11), the value function satisfies
134"
NONLINEAR FEYNMAN-KAC FORMULA,0.270979020979021,(∂t + L)v = 1
NONLINEAR FEYNMAN-KAC FORMULA,0.2727272727272727,"2 ∥σ⊤∇v∥2,
v(x, y, T) = −log[g(x, y)]
for all
(x, y) ∈X × Y.
(13)"
NONLINEAR FEYNMAN-KAC FORMULA,0.2744755244755245,"Let {Xc,y
t
}t∈[0,T] be a controlled diffusion defined in Equation (6) with a given control function
135"
NONLINEAR FEYNMAN-KAC FORMULA,0.2762237762237762,"c : X × Y × [0, T] →Rd and define the diffusion process {Vt}t∈[0,T] as Vt = v(Xc,y
t
, y, t). Itô
136"
NONLINEAR FEYNMAN-KAC FORMULA,0.27797202797202797,"Lemma shows that for any observation YT = y and 0 ≤s ≤T, we have
137"
NONLINEAR FEYNMAN-KAC FORMULA,0.27972027972027974,"VT = Vs +
Z T s 1"
NONLINEAR FEYNMAN-KAC FORMULA,0.28146853146853146,"2 ∥Zt∥2 + ⟨c, Zt⟩

dt +
Z T"
NONLINEAR FEYNMAN-KAC FORMULA,0.28321678321678323,"s
⟨Zt, dBt⟩"
NONLINEAR FEYNMAN-KAC FORMULA,0.28496503496503495,"with Zt = [σ⊤∇v](Xc,y
t
, y, t) and VT = −log[g(Xc,y
T , y)]. In summary, the pair of processes
138"
NONLINEAR FEYNMAN-KAC FORMULA,0.2867132867132867,"(Vt, Zt) are such that the following equation holds,
139"
NONLINEAR FEYNMAN-KAC FORMULA,0.28846153846153844,"−log[g(Xc,y
T , y)] = Vs +
Z T s 1"
NONLINEAR FEYNMAN-KAC FORMULA,0.2902097902097902,"2 ∥Zt∥2 + ⟨c, Zt⟩

dt +
Z T"
NONLINEAR FEYNMAN-KAC FORMULA,0.291958041958042,"s
⟨Zt, dBt⟩.
(14)"
NONLINEAR FEYNMAN-KAC FORMULA,0.2937062937062937,"Crucially, under mild growth and regularity assumptions on the drift and volatility function µ :
140"
NONLINEAR FEYNMAN-KAC FORMULA,0.29545454545454547,"X →Rd and σ : X →Rd,d, the pair of processes (Vt, Zt) is the unique solution to Equation (14)
141"
NONLINEAR FEYNMAN-KAC FORMULA,0.2972027972027972,"[28, 29, 30, 40]. This result can be used as a building block for designing Monte Carlo approximations
142"
NONLINEAR FEYNMAN-KAC FORMULA,0.29895104895104896,"of the solution to semilinear and fully nonlinear PDEs [18, 34, 21]
143"
METHOD,0.3006993006993007,"3
Method
144"
METHOD,0.30244755244755245,"3.1
Computational Doob’s h-transform
145"
METHOD,0.3041958041958042,"As before, consider a diffusion {Xc,y
t
}t∈[0,T] controlled by a function c : X × Y × [0, T] →Rd and
146"
METHOD,0.30594405594405594,"driven by the standard Brownian motion {Bt}t≥0. Furthermore, for two functions N0 : X × Y →R
147"
METHOD,0.3076923076923077,"and N : X × Y × [0, T] →Rd, consider the diffusion process {Vt}t∈[0,T] defined as
148"
METHOD,0.3094405594405594,"Vt = V0 +
Z s 0 1"
METHOD,0.3111888111888112,"2 ∥Zt∥2 + ⟨c(Xc,y
t
, y, t), Zt⟩

dt +
Z s"
METHOD,0.3129370629370629,"0
⟨Zt, dBt⟩,
(15)"
METHOD,0.3146853146853147,"where the initial condition V0 and the process {Zt}t∈[0,T] are defined as
149"
METHOD,0.31643356643356646,"V0 = N0(Xc,y
0 , y)
and
Zt = N(Xc,y
t
, y, t).
(16)"
METHOD,0.3181818181818182,"Importantly, we remind the reader that the two diffusion processes Xc,y
t
and Vt are driven by the
150"
METHOD,0.31993006993006995,"same Brownian motion Bt. The uniqueness result mentioned at the end of Section 2.5 implies
151"
METHOD,0.32167832167832167,"that, if for any choice of initial condition Xc,y
0
∈X and terminal observation y ∈Y the condition
152"
METHOD,0.32342657342657344,"VT = −log[g(Xc,y
T , y)] is satisfied, then we have that for all (x, y, t) ∈X × Y × [0, T]
153"
METHOD,0.32517482517482516,"N0(x, y) = −log h(x, y, 0)
and
N(x, y, t) = −[σ⊤∇log h](x, y, t).
(17)"
METHOD,0.3269230769230769,"In particular, the optimal control is given by c⋆(x, y, t) = −N(x, y, t).
154"
METHOD,0.32867132867132864,"These remarks suggest parametrizing the functions N0(·, ·) and N(·, ·, ·) by two neural networks with
155"
METHOD,0.3304195804195804,"respective parameters θ0 ∈Θ0 and θ ∈Θ while minimizing the loss function
156"
METHOD,0.3321678321678322,"L(θ0, θ; c) = E

VT + log[g(Xc,Y
T
, Y)]
2
.
(18)"
METHOD,0.3339160839160839,"The above expectation is with respect to the distribution of the Brownian motion {Bt}t≥0, the initial
157"
METHOD,0.3356643356643357,"condition Xc,Y
0
∼ηX(dx) of the controlled diffusion, and the observation Y ∼ηY(dy) at time T.
158"
METHOD,0.3374125874125874,"In practice, we will let the three sources of randomness be independent of each other. The spread
159"
METHOD,0.33916083916083917,"of the distributions ηX and ηY should be large enough to cover typical states under the filtering
160"
METHOD,0.3409090909090909,"distributions πk, k ≥1 and future observations to be filtered respectively. Specific choices will be
161"
METHOD,0.34265734265734266,"detailed for each application in Section 4. For offline problems, one could learn in a data-driven
162"
METHOD,0.34440559440559443,"manner by selecting ηY as the empirical distribution of actual observations. Furthermore, any control
163"
METHOD,0.34615384615384615,"function c : X × Y × [0, T] →Rd with mild growth and regularity assumptions can be employed
164"
METHOD,0.3479020979020979,"within our methodology: specific choices are discussed at the end of this section.
165"
METHOD,0.34965034965034963,"CDT algorithm.
The following outlines our training procedure to learn neural networks N0 and N
166"
METHOD,0.3513986013986014,"that satisfy (17). To minimize the loss function (18), any stochastic gradient algorithm can be used
167"
METHOD,0.3531468531468531,"with a user-specified mini-batch size of J ≥1. The following steps are iterated until convergence.
168"
METHOD,0.3548951048951049,"1. Choose a control c : X × Y × [0, T] →Rd, possibly based on the current neural network
169"
METHOD,0.35664335664335667,"parameters (θ0, θ) ∈Θ0 × Θ.
170"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3583916083916084,"2. Simulate independent Brownian paths Bj
[0,T], initial conditions Xj
0 ∼ηX(dx), and observa-
171"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.36013986013986016,"tions Yj ∼ηY(dy) for 1 ≤j ≤J.
172"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3618881118881119,"3. Generate the controlled trajectories: the j-th sample path Xj
[0,T] is obtained by forward
173"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.36363636363636365,"integration of the controlled dynamics in Equation (6) with initial condition Xj
0, control
174"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.36538461538461536,"c(·, Yj, ·), and the Brownian path Bj
[0,T].
175"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.36713286713286714,"4. Generate the value trajectories: the j-th sample path Vj
[0,T] is obtained by forward integration
176"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3688811188811189,"of the dynamics in Equation (15)–(16) with the Brownian path Bj
[0,T] and the current neural
177"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3706293706293706,"network parameters (θ0, θ) ∈Θ0 × Θ.
178"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3723776223776224,"5. Construct a Monte Carlo estimate of the loss function (18):
179"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3741258741258741,"bL = J−1
J
X"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3758741258741259,"j=1
(Vj
T + log[g(Xj
T, Yj)])2
(19)"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3776223776223776,"6. Use automatic differentiation to compute ∂θ0bL and ∂θbL and update the parameters (θ0, θ).
180"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3793706293706294,"Importantly, if the control function c in Step:1 does depend on the current parameters (θ0, θ), the
181"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3811188811188811,"gradient operations executed in Step:6 should not be propagated through the control function c. A
182"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.38286713286713286,"standard stop-gradient operation available in most popular automatic differentiation frameworks
183"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.38461538461538464,"can be used for this purpose.
184"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.38636363636363635,"Time-discretization of diffusions.
For clarity of exposition, we have described our algorithm in
185"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3881118881118881,"continuous-time. In practice, one would have to discretize these diffusion processes, which is entirely
186"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.38986013986013984,"straightforward. Although any numerical integrator could potentially be considered, the experiments
187"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3916083916083916,"in Section 4 employed the standard Euler-Maruyama scheme [25].
188"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.39335664335664333,"Parametrizations of functions N0 and N.
In all numerical experiments presented in Section 4, the
189"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3951048951048951,"functions N0 and N are parametrized with fully-connected neural networks with two hidden layers
190"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3968531468531469,"and the Leaky ReLU activation function except in the last layer. Future work could explore other
191"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.3986013986013986,"neural network architectures for our setting.
192"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.40034965034965037,"Choice of controlled dynamics.
In challenging scenarios where observations are highly informative
193"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4020979020979021,"and/or extreme under the model, choosing a good control function to implement Step:1 of the
194"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.40384615384615385,"proposed algorithm can be crucial. We focus on two possible implementations:
195"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.40559440559440557,"• CDT static scheme: a simple (and naive) choice is not using any control, i.e. c(x, y, t) ≡
196"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.40734265734265734,"0 ∈Rd for all (x, y, t) ∈X × Y × [0, T].
197"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4090909090909091,"• CDT iterative scheme: use the current approximation of the optimal control c⋆described
198"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.41083916083916083,"by the parameters (θ0, θ) ∈Θ0 × Θ. This corresponds to setting c(x, y, t) = −N(x, y, t).
199"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4125874125874126,"While using a static control approach can perform reasonably well in some situations, our results in
200"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4143356643356643,"Section 4 suggest that the iterative control procedure is a more reliable strategy. This is consistent
201"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4160839160839161,"with findings in the stochastic optimal control literature [38, 32]. This choice of control function
202"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4178321678321678,"drives the forward process Xc,y
t
to regions of the state-space where the likelihood function is large and
203"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4195804195804196,"helps mitigate convergence and stability issues. Furthermore, Section 4 reports that (at convergence),
204"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.42132867132867136,"the solutions N0 and N can be significantly different. The iterative control procedure leads to more
205"
SIMULATE INDEPENDENT BROWNIAN PATHS BJ,0.4230769230769231,"accurate solutions and, ultimately, better performance when used for online filtering.
206"
ONLINE FILTERING,0.42482517482517484,"3.2
Online filtering
207"
ONLINE FILTERING,0.42657342657342656,"Before performing online filtering, we first run the CDT algorithm described in Section 3.1 to construct
208"
ONLINE FILTERING,0.42832167832167833,"an approximation of the optimal control c⋆(x, y, t) = [σ⊤∇log h](x, y, t). For concreteness, denote
209"
ONLINE FILTERING,0.43006993006993005,"by bc : X × Y × [0, T] →Rd the resulting approximate control, i.e. bc(x, y, t) = −N(x, y, t) where
210"
ONLINE FILTERING,0.4318181818181818,"N(·, ·, ·) is parametrized by the final parameter θ ∈Θ. Similarly, denote by bV0 : X × Y →R the
211"
ONLINE FILTERING,0.43356643356643354,"approximation of the initial value function v(x, y, 0) = −log h(x, y, 0), i.e. bV0(x, y) = N0(x, y)
212"
ONLINE FILTERING,0.4353146853146853,"where N0(·, ·) is parametrized by the final parameter θ0 ∈Θ0.
213"
ONLINE FILTERING,0.4370629370629371,"For implementing online filtering with N ≥1 particles, consider a current approximation πk(dx) =
214"
ONLINE FILTERING,0.4388111888111888,"N −1 PN
j=1 δ(dx; xj
tk) of the filtering distributions at time tk ≥0. Given the future observation
215"
ONLINE FILTERING,0.4405594405594406,"Yk+1 = yk+1, the particles x1:N
tk
are then propagated forward by exploiting the approximately
216"
ONLINE FILTERING,0.4423076923076923,"optimal control (x, t) 7→bc(x, yk+1, t−tk). In particular, bxj
tk+1 is obtained by setting bxj
tk+1 = bXj
tk+1
217"
ONLINE FILTERING,0.44405594405594406,"where { bXj
t}t∈[tk,tk+1] follows the controlled diffusion
218"
ONLINE FILTERING,0.4458041958041958,"d bXj
t = µ( bXj
t) dt + σ( bXj
t) dBj
t
|
{z
}
(original dynamics)"
ONLINE FILTERING,0.44755244755244755,"+ [σbc]( bXj
t, yk+1, t −tk) dt
|
{z
}
(approximately optimal control) (20)"
ONLINE FILTERING,0.4493006993006993,"initialized at bXj
tk
= xj
tk.
Each propagated particle bxj
tk+1 is associated with a normalized
219"
ONLINE FILTERING,0.45104895104895104,"weight W
j
k+1 = W j
k+1/ PN
i=1 W i
k+1 where W j
k+1 = (dP[tk,tk+1]/dPbc,yk+1
[tk,tk+1])( bXj
[tk,tk+1]) ×
220"
ONLINE FILTERING,0.4527972027972028,"g(bxj
tk+1, yk+1). We recall that the probability measures P[tk,tk+1] and Pbc,yk+1
[tk,tk+1] correspond to the
221"
ONLINE FILTERING,0.45454545454545453,"original and controlled diffusions on the interval [tk, tk+1]. Girsanov’s theorem, as described in Equa-
222"
ONLINE FILTERING,0.4562937062937063,"tion (8), implies that
223"
ONLINE FILTERING,0.458041958041958,"W j
k+1 = exp

−1 2"
ONLINE FILTERING,0.4597902097902098,Z tk+1
ONLINE FILTERING,0.46153846153846156,"tk
∥Zj
t∥2 dt +
Z tk+1"
ONLINE FILTERING,0.4632867132867133,"tk
⟨Zj
t, dBj
t⟩+ log g(xj
tk+1, yk+1)
"
ONLINE FILTERING,0.46503496503496505,"where Zj
t = −bc( bXj
t, yk+1, t −tk). Similarly to Equation (15), consider the diffusion process
224"
ONLINE FILTERING,0.46678321678321677,"{Vj
t}t∈[tk,tk+1] defined by the dynamics dVj
t = −1"
ONLINE FILTERING,0.46853146853146854,"2∥Zj
t∥2 dt + ⟨Zj
t, dBj
t⟩with initialization at
225"
ONLINE FILTERING,0.47027972027972026,"Vj
tk = bV0(xj
tk, yk+1). Therefore the weight can be re-written as
226"
ONLINE FILTERING,0.47202797202797203,"W j
k+1 = exp
n
Vj
tk+1 + log g(xj
tk+1, yk+1)
|
{z
}
≈0"
ONLINE FILTERING,0.4737762237762238,"o
exp
n
−bV0(xj
tk, yk+1)
o
,
(21)"
ONLINE FILTERING,0.4755244755244755,"and computed by numerically integrating the process {Vj
t}t∈[tk,tk+1]. Given the definition of the loss
227"
ONLINE FILTERING,0.4772727272727273,"function in (18), we can expect the term within the first exponential to be close to zero. In the ideal
228"
ONLINE FILTERING,0.479020979020979,"case where bc(x, y, t) ≡c⋆(x, y, t) and bV0(x, y) ≡−log h(x, y, 0), one recovers the exact AF-APF
229"
ONLINE FILTERING,0.4807692307692308,"weights in (5). Once the unnormalized weights (21) are computed, the resampling steps are identical
230"
ONLINE FILTERING,0.4825174825174825,"to those described in Section 2.2 for a standard PF. For practical implementations, all the processes
231"
ONLINE FILTERING,0.48426573426573427,"involved in the proposed methodology can be straightforwardly time-discretized. To distinguish
232"
ONLINE FILTERING,0.486013986013986,"between CDT learning with static or iterative control, we shall refer to the resulting approximation of
233"
ONLINE FILTERING,0.48776223776223776,"FA-APF as Static-APF and Iterative-APF respectively.
234"
EXPERIMENTS,0.48951048951048953,"4
Experiments
235"
EXPERIMENTS,0.49125874125874125,"This section presents numerical results obtained on three models. All experiments employed 2000
236"
EXPERIMENTS,0.493006993006993,"iterations of the Adam optimizer with a learning rate of 0.01 and a mini-batch size of 1000 sample
237"
EXPERIMENTS,0.49475524475524474,"paths with 10 different observations. Training times took around one to two minutes on a standard
238"
EXPERIMENTS,0.4965034965034965,"CPU. We note that this compute time is marginal compared to the cost of running filters with many
239"
EXPERIMENTS,0.4982517482517482,"particles and/or to assimilate large number of observations. Moreover, we can also benefit from
240"
EXPERIMENTS,0.5,"the use of hardware accelerators. We set the inter-observation time as T = 1 and employed the
241"
EXPERIMENTS,0.5017482517482518,"Euler-Maruyama integrator with a stepsize of 0.02 for all examples. Our results are not sensitive to
242"
EXPERIMENTS,0.5034965034965035,"the choice of T and discretization stepsize if it is sufficiently small. We examined the performance
243"
EXPERIMENTS,0.5052447552447552,"of each particle filter by computing its effective sample size (ESS) averaged over observation
244"
EXPERIMENTS,0.506993006993007,"times and independent repetitions, the evidence lower bound (ELBO) E[log bp(y1, . . . , yK)], and
245"
EXPERIMENTS,0.5087412587412588,"the variance Var[log bp(y1, . . . , yK)], where bp(y1, . . . , yK) denotes its unbiased estimator of the
246"
EXPERIMENTS,0.5104895104895105,"marginal likelihood of the time-discretized filter p(y1, . . . , yK). When testing particle filters with
247"
EXPERIMENTS,0.5122377622377622,"varying number of observations K, we increased the number of particles linearly with K to keep
248"
EXPERIMENTS,0.513986013986014,"marginal likelihood estimators stable [2].
249"
ORNSTEIN-UHLENBECK MODEL,0.5157342657342657,"4.1
Ornstein-Uhlenbeck model
250"
ORNSTEIN-UHLENBECK MODEL,0.5174825174825175,"We considered an Ornstein-Uhlenbeck process given by (1) with µ(x) = −x, σ(x) = 1 and the Gaus-
251"
ORNSTEIN-UHLENBECK MODEL,0.5192307692307693,"sian observation model g(x, y) = N(y; x, σ2
Y). We chose ηX = N(0, 1/2) as the stationary distri-
252"
ORNSTEIN-UHLENBECK MODEL,0.5209790209790209,"bution and ηY = N(0, 1/2 + σ2
Y) as the implied distribution of the observation when training neural
253"
ORNSTEIN-UHLENBECK MODEL,0.5227272727272727,"networks with the CDT iterative scheme. We took different values of σY ∈{0.125, 0.25, 0.5, 1.0}
254"
ORNSTEIN-UHLENBECK MODEL,0.5244755244755245,"to vary the informativeness of observations. Analytically tractability in this example allows us to
255"
ORNSTEIN-UHLENBECK MODEL,0.5262237762237763,"visualize the quality of our neural network approximations in Figure 1 and consider two idealized
256"
ORNSTEIN-UHLENBECK MODEL,0.527972027972028,"particle filters, namely an APF with exact networks (Exact-APF) and the FA-APF. Comparing our
257"
ORNSTEIN-UHLENBECK MODEL,0.5297202797202797,"proposed Iterative-APF to Exact-APF and FA-APF enables us to distinguish between neural network
258"
ORNSTEIN-UHLENBECK MODEL,0.5314685314685315,"approximation errors and time-discretization errors. We note that all PFs except the FA-APF involve
259"
ORNSTEIN-UHLENBECK MODEL,0.5332167832167832,"time-discretization.
260"
ORNSTEIN-UHLENBECK MODEL,0.534965034965035,"Columns 1 to 4 of Figure 2 summarize our numerical findings when filtering simulated observations
261"
ORNSTEIN-UHLENBECK MODEL,0.5367132867132867,"from the model. We see that the performance of BPF deteriorates as the observations become more
262"
ORNSTEIN-UHLENBECK MODEL,0.5384615384615384,"informative, which is to be expected. Furthermore, when σY is small, the impact of our neural
263"
ORNSTEIN-UHLENBECK MODEL,0.5402097902097902,"network approximation and time-discretization becomes more noticeable. For the values of σY and
264"
ORNSTEIN-UHLENBECK MODEL,0.541958041958042,"the number of observations K considered, we obtained around an order of magnitude gain in efficiency
265"
ORNSTEIN-UHLENBECK MODEL,0.5437062937062938,"over BPF. From Column 5, we note that these gains become very substantial when we filter K = 100
266"
ORNSTEIN-UHLENBECK MODEL,0.5454545454545454,"observations that are simulated with observation noise that are several standard deviations larger than
267"
ORNSTEIN-UHLENBECK MODEL,0.5472027972027972,"σY = 0.25 under the model specification. In particular, while the ELBO of BPF diverges as we
268"
ORNSTEIN-UHLENBECK MODEL,0.548951048951049,"increase the degree of noise in the simulated observations, the ELBO of Iterative-APF remains stable.
269"
ORNSTEIN-UHLENBECK MODEL,0.5506993006993007,"1
0
1
x 1.5 1.0 0.5 0.0 0.5 1.0 1.5 y"
ORNSTEIN-UHLENBECK MODEL,0.5524475524475524,"1
0
1
x 1.5 1.0 0.5 0.0 0.5 1.0 1.5 y 1.0 1.5 2.0 2.5 3.0 3.5"
ORNSTEIN-UHLENBECK MODEL,0.5541958041958042,"(a) v(x, y, 0) (left) and bV0(x, y) (right)."
ORNSTEIN-UHLENBECK MODEL,0.5559440559440559,"1
0
1
1.5 1.0 0.5 0.0 0.5 1.0 1.5 y"
ORNSTEIN-UHLENBECK MODEL,0.5576923076923077,t = 0.25
ORNSTEIN-UHLENBECK MODEL,0.5594405594405595,"1
0
1
x 1.5 1.0 0.5 0.0 0.5 1.0 1.5 y"
ORNSTEIN-UHLENBECK MODEL,0.5611888111888111,"1
0
1
1.5 1.0 0.5 0.0 0.5 1.0"
ORNSTEIN-UHLENBECK MODEL,0.5629370629370629,"1.5
t = 0.5"
ORNSTEIN-UHLENBECK MODEL,0.5646853146853147,"1
0
1
x 1.5 1.0 0.5 0.0 0.5 1.0 1.5"
ORNSTEIN-UHLENBECK MODEL,0.5664335664335665,"1
0
1
1.5 1.0 0.5 0.0 0.5 1.0"
ORNSTEIN-UHLENBECK MODEL,0.5681818181818182,"1.5
t = 0.75 5 0 5"
ORNSTEIN-UHLENBECK MODEL,0.5699300699300699,"1
0
1
x 1.5 1.0 0.5 0.0 0.5 1.0 1.5 5 0 5"
ORNSTEIN-UHLENBECK MODEL,0.5716783216783217,"(b) c⋆(x, y, t) (upper) and bc(x, y, t) (lower)."
ORNSTEIN-UHLENBECK MODEL,0.5734265734265734,Figure 1: Neural network approximations for Ornstein-Uhlenbeck model with σY = 0.5.
ORNSTEIN-UHLENBECK MODEL,0.5751748251748252,"102
103
0 25 50 75 100 ESS%"
ORNSTEIN-UHLENBECK MODEL,0.5769230769230769,Y = 0.125
ORNSTEIN-UHLENBECK MODEL,0.5786713286713286,"102
103
0 20 40"
ORNSTEIN-UHLENBECK MODEL,0.5804195804195804,ELBO gap
ORNSTEIN-UHLENBECK MODEL,0.5821678321678322,"102
103
K 10
3 10
1 101 103"
ORNSTEIN-UHLENBECK MODEL,0.583916083916084,Variance
ORNSTEIN-UHLENBECK MODEL,0.5856643356643356,"102
103
0 25 50 75"
ORNSTEIN-UHLENBECK MODEL,0.5874125874125874,"100
Y = 0.25"
ORNSTEIN-UHLENBECK MODEL,0.5891608391608392,"102
103
0 2 4"
ORNSTEIN-UHLENBECK MODEL,0.5909090909090909,"102
103
K 10
3 10
1 101 103"
ORNSTEIN-UHLENBECK MODEL,0.5926573426573427,"102
103
0 25 50 75"
ORNSTEIN-UHLENBECK MODEL,0.5944055944055944,"100
Y = 0.5"
ORNSTEIN-UHLENBECK MODEL,0.5961538461538461,"102
103 0.0 0.5 1.0"
ORNSTEIN-UHLENBECK MODEL,0.5979020979020979,"102
103
K 10
3 10
1 101 103"
ORNSTEIN-UHLENBECK MODEL,0.5996503496503497,"BPF
Iterative-APF
Exact-APF
FA-APF"
ORNSTEIN-UHLENBECK MODEL,0.6013986013986014,"102
103
0 25 50 75"
ORNSTEIN-UHLENBECK MODEL,0.6031468531468531,"100
Y = 1.0"
ORNSTEIN-UHLENBECK MODEL,0.6048951048951049,"102
103
0.2 0.0 0.2 0.4"
ORNSTEIN-UHLENBECK MODEL,0.6066433566433567,"102
103
K 10
3 10
1 101 103"
ORNSTEIN-UHLENBECK MODEL,0.6083916083916084,"2
4
6
8
10
0
20
40
60
80
100"
ORNSTEIN-UHLENBECK MODEL,0.6101398601398601,"2
4
6
8
10
0 500 1000 1500"
ORNSTEIN-UHLENBECK MODEL,0.6118881118881119,"2
4
6
8
10
Standard deviation"
ORNSTEIN-UHLENBECK MODEL,0.6136363636363636,"10
3
10
1
101
103
105"
ORNSTEIN-UHLENBECK MODEL,0.6153846153846154,"Figure 2: Results for Ornstein-Uhlenbeck model based on 100 independent repetitions of each PF.
The ELBO gap in the second row is relative to FA-APF."
LOGISTIC DIFFUSION MODEL,0.6171328671328671,"4.2
Logistic diffusion model
270"
LOGISTIC DIFFUSION MODEL,0.6188811188811189,"Next we consider a logistic diffusion process [11, 26] to model the dynamics of a population size
271"
LOGISTIC DIFFUSION MODEL,0.6206293706293706,"{Pt}t≥0, defined by
272"
LOGISTIC DIFFUSION MODEL,0.6223776223776224,"dPt = (θ2
3/2 + θ1 −θ2Pt)Pt dt + θ3Pt dBt.
(22)"
LOGISTIC DIFFUSION MODEL,0.6241258741258742,"We apply the Lamperti transformation Xt = log(Pt)/θ3 and work with the process {Xt}t≥0 that
273"
LOGISTIC DIFFUSION MODEL,0.6258741258741258,"satisfies (1) with µ(x) = θ1/θ3 −(θ2/θ3) exp(θ3x) and σ(x) = 1. Following [26], we adopt
274"
LOGISTIC DIFFUSION MODEL,0.6276223776223776,"a negative binomial observation model g(x, y) = NB(y; θ4, exp(θ3x)) for counts y ∈N0 with
275"
LOGISTIC DIFFUSION MODEL,0.6293706293706294,"dispersion θ4 > 0 and mean exp(θ3x). We set (θ1, θ2, θ3, θ4) as the parameter estimates obtained
276"
LOGISTIC DIFFUSION MODEL,0.6311188811188811,"in [26]. Noting that (22) admits a Gamma distribution with shape parameter 2(θ2
3/2 + θ1)/θ2
3 −1
277"
LOGISTIC DIFFUSION MODEL,0.6328671328671329,"and rate parameter 2θ2/θ2
3 as stationary distribution [11], we select ηX as the push-forward under
278"
LOGISTIC DIFFUSION MODEL,0.6346153846153846,"the Lamperti transformation and ηY as the implied distribution of the observation when training
279"
LOGISTIC DIFFUSION MODEL,0.6363636363636364,"neural networks under both static and iterative CDT schemes. To induce varying levels of informative
280"
LOGISTIC DIFFUSION MODEL,0.6381118881118881,"observations, we considered θ4 ∈{1.069, 4.303, 17.631, 78.161}.
281"
LOGISTIC DIFFUSION MODEL,0.6398601398601399,"Figure 3 displays our filtering results for various number of simulated observations from the model
282"
LOGISTIC DIFFUSION MODEL,0.6416083916083916,"(Columns 1 to 4) and for K = 100 observations that are simulated with an observation model with
283"
LOGISTIC DIFFUSION MODEL,0.6433566433566433,"several standard deviations larger than θ4 = 17.631 under the model specification (Column 5). In the
284"
LOGISTIC DIFFUSION MODEL,0.6451048951048951,"latter setup, we solved for different values of θ4 in the negative binomial observation model to induce
285"
LOGISTIC DIFFUSION MODEL,0.6468531468531469,"larger standard deviations. The behaviour of BPF and Iterative-APF is similar to the previous example
286"
LOGISTIC DIFFUSION MODEL,0.6486013986013986,"as the observations become more informative with larger values of θ4. Iterative-APF outperforms
287"
LOGISTIC DIFFUSION MODEL,0.6503496503496503,"both BPF and Static-APF over all combinations of θ4 and K considered, and also when filtering
288"
LOGISTIC DIFFUSION MODEL,0.6520979020979021,"observations that are increasingly extreme under the model. We note also that the APFs trained using
289"
LOGISTIC DIFFUSION MODEL,0.6538461538461539,"the CDT static scheme can sometimes give unstable results, particularly in challenging scenarios.
290"
LOGISTIC DIFFUSION MODEL,0.6555944055944056,"102
103
0 25 50 75 100 ESS%"
LOGISTIC DIFFUSION MODEL,0.6573426573426573,4 = 1.069
LOGISTIC DIFFUSION MODEL,0.6590909090909091,"102
103
0.00 0.25 0.50 0.75 1.00"
LOGISTIC DIFFUSION MODEL,0.6608391608391608,ELBO gap
LOGISTIC DIFFUSION MODEL,0.6625874125874126,"102
103
K 10
2 100 102 104"
LOGISTIC DIFFUSION MODEL,0.6643356643356644,Variance
LOGISTIC DIFFUSION MODEL,0.666083916083916,"102
103
0 25 50 75"
LOGISTIC DIFFUSION MODEL,0.6678321678321678,"100
4 = 4.303"
LOGISTIC DIFFUSION MODEL,0.6695804195804196,"102
103
0 1 2 3"
LOGISTIC DIFFUSION MODEL,0.6713286713286714,"102
103
K 10
2 100 102 104"
LOGISTIC DIFFUSION MODEL,0.6730769230769231,"102
103
0 25 50 75"
LOGISTIC DIFFUSION MODEL,0.6748251748251748,"100
4 = 17.631"
LOGISTIC DIFFUSION MODEL,0.6765734265734266,"102
103
0.0 2.5 5.0 7.5 10.0"
LOGISTIC DIFFUSION MODEL,0.6783216783216783,"102
103
K 10
2 100 102 104"
LOGISTIC DIFFUSION MODEL,0.6800699300699301,"BPF
Static-APF
Iterative-APF"
LOGISTIC DIFFUSION MODEL,0.6818181818181818,"102
103
0 25 50 75"
LOGISTIC DIFFUSION MODEL,0.6835664335664335,"100
4 = 78.161"
LOGISTIC DIFFUSION MODEL,0.6853146853146853,"102
103
0 500 1000"
LOGISTIC DIFFUSION MODEL,0.6870629370629371,"102
103
K 10
2 100 102 104"
LOGISTIC DIFFUSION MODEL,0.6888111888111889,"1
2
3
4
5
6
0
20
40
60
80
100"
LOGISTIC DIFFUSION MODEL,0.6905594405594405,"1
2
3
4
5
6
0 200 400 600"
LOGISTIC DIFFUSION MODEL,0.6923076923076923,"1
2
3
4
5
6
Standard deviation 10
2 100 102 104"
LOGISTIC DIFFUSION MODEL,0.6940559440559441,"Figure 3: Results for logistic diffusion model based on 100 independent repetitions of each PF. The
ELBO gap in the second row is relative to Iterative-APF."
LOGISTIC DIFFUSION MODEL,0.6958041958041958,"102
103
0 25 50 75 100 ESS%"
LOGISTIC DIFFUSION MODEL,0.6975524475524476,Y = 0.25
LOGISTIC DIFFUSION MODEL,0.6993006993006993,"102
103
0 10 20 30"
LOGISTIC DIFFUSION MODEL,0.701048951048951,ELBO gap
LOGISTIC DIFFUSION MODEL,0.7027972027972028,"102
103
K"
LOGISTIC DIFFUSION MODEL,0.7045454545454546,"10
1
100
101
102
103"
LOGISTIC DIFFUSION MODEL,0.7062937062937062,Variance
LOGISTIC DIFFUSION MODEL,0.708041958041958,"102
103
0 25 50 75"
LOGISTIC DIFFUSION MODEL,0.7097902097902098,"100
Y = 0.5"
LOGISTIC DIFFUSION MODEL,0.7115384615384616,"102
103
0.0 2.5 5.0 7.5 10.0"
LOGISTIC DIFFUSION MODEL,0.7132867132867133,"102
103
K"
LOGISTIC DIFFUSION MODEL,0.715034965034965,"10
1
100
101
102
103"
LOGISTIC DIFFUSION MODEL,0.7167832167832168,"102
103
0 25 50 75"
LOGISTIC DIFFUSION MODEL,0.7185314685314685,"100
Y = 1.0"
LOGISTIC DIFFUSION MODEL,0.7202797202797203,"102
103
0.0 0.5 1.0"
LOGISTIC DIFFUSION MODEL,0.722027972027972,"102
103
K"
LOGISTIC DIFFUSION MODEL,0.7237762237762237,"10
1
100
101
102
103"
LOGISTIC DIFFUSION MODEL,0.7255244755244755,"BPF
Static-APF
Iterative-APF"
LOGISTIC DIFFUSION MODEL,0.7272727272727273,"102
103
0 25 50 75"
LOGISTIC DIFFUSION MODEL,0.7290209790209791,"100
Y = 2.0"
LOGISTIC DIFFUSION MODEL,0.7307692307692307,"102
103 0.0 0.2 0.4"
LOGISTIC DIFFUSION MODEL,0.7325174825174825,"102
103
K"
LOGISTIC DIFFUSION MODEL,0.7342657342657343,"10
1
100
101
102
103"
LOGISTIC DIFFUSION MODEL,0.736013986013986,"2
4
6
8
10
0
20
40
60
80
100"
LOGISTIC DIFFUSION MODEL,0.7377622377622378,"2
4
6
8
10
0 1000 2000 3000"
LOGISTIC DIFFUSION MODEL,0.7395104895104895,"2
4
6
8
10
Standard deviation 10
1 101 103 105"
LOGISTIC DIFFUSION MODEL,0.7412587412587412,"Figure 4: Results for cell model based on 100 independent repetitions of each PF. The ELBO gap in
the second row is relative to Iterative-APF."
CELL MODEL,0.743006993006993,"4.3
Cell model
291"
CELL MODEL,0.7447552447552448,"Lastly, we examine a cell differentiation and development model from [39]. Cellular expression
292"
CELL MODEL,0.7465034965034965,"levels Xt = (Xt,1, Xt,2) of two genes are modelled by (1) with
293"
CELL MODEL,0.7482517482517482,"µ(x) =

x4
1/(2−4 + x4
1) + 2−4/(2−4 + x4
2) −x1
x4
2/(2−4 + x4
2) + 2−4/(2−4 + x4
1) −x2"
CELL MODEL,0.75,"
(23)"
CELL MODEL,0.7517482517482518,"and σ(x) =
√"
CELL MODEL,0.7534965034965035,"0.1Id. The terms in (23) describe self-activation, mutual inhibition and inactivation
294"
CELL MODEL,0.7552447552447552,"respectively, and the volatility captures intrinsic and external fluctuations. We initialize the diffusion
295"
CELL MODEL,0.756993006993007,"process from the undifferentiated state of X0 = (1, 1) and consider the Gaussian observation model
296"
CELL MODEL,0.7587412587412588,"g(x, y) = N(y; x, σ2
YI2). To train neural networks under both static and iterative CDT schemes,
297"
CELL MODEL,0.7604895104895105,"we selected ηX and ηY as the empirical distributions obtained by simulating states and observations
298"
CELL MODEL,0.7622377622377622,"from the model for 2000 time units.
299"
CELL MODEL,0.763986013986014,"Figure 4 illustrates our numerical results for various number of observations K and σY ∈
300"
CELL MODEL,0.7657342657342657,"{0.25, 0.5, 1.0, 2.0}. It shows that Iterative-APF offers significant gains over BPF and Static-APF
301"
CELL MODEL,0.7674825174825175,"when filtering observations that are informative (see Columns 1 to 4) and highly extreme under the
302"
CELL MODEL,0.7692307692307693,"model specification of σY = 0.5 (see Column 5). In this example, Static-APF did not exhibit any
303"
CELL MODEL,0.7709790209790209,"unstable behaviour and its performance lies somewhere in between BPF and Iterative-APF.
304"
REFERENCES,0.7727272727272727,"References
305"
REFERENCES,0.7744755244755245,"[1] C. Beck, A. Jentzen, et al. Machine learning approximation algorithms for high-dimensional
306"
REFERENCES,0.7762237762237763,"fully nonlinear partial differential equations and second-order backward stochastic differential
307"
REFERENCES,0.777972027972028,"equations. Journal of Nonlinear Science, 29(4):1563–1619, 2019.
308"
REFERENCES,0.7797202797202797,"[2] J. Bérard, P. Del Moral, and A. Doucet.
A lognormal central limit theorem for particle
309"
REFERENCES,0.7814685314685315,"approximations of normalizing constants. Electronic Journal of Probability, 19:1–28, 2014.
310"
REFERENCES,0.7832167832167832,"[3] A. Beskos, O. Papaspiliopoulos, and G. O. Roberts. Retrospective exact simulation of diffusion
311"
REFERENCES,0.784965034965035,"sample paths with applications. Bernoulli, 12(6):1077–1098, 2006.
312"
REFERENCES,0.7867132867132867,"[4] A. Beskos, O. Papaspiliopoulos, G. O. Roberts, and P. Fearnhead. Exact and computationally ef-
313"
REFERENCES,0.7884615384615384,"ficient likelihood-based estimation for discretely observed diffusion processes (with discussion).
314"
REFERENCES,0.7902097902097902,"Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):333–382,
315"
REFERENCES,0.791958041958042,"2006.
316"
REFERENCES,0.7937062937062938,"[5] J. Blanchet and F. Zhang. Exact simulation for multivariate Itô diffusions. Advances in Applied
317"
REFERENCES,0.7954545454545454,"Probability, 52(4):1003–1034, 2020.
318"
REFERENCES,0.7972027972027972,"[6] Q. Chan-Wai-Nam, J. Mikael, and X. Warin. Machine learning for semi linear PDEs. Journal
319"
REFERENCES,0.798951048951049,"of Scientific Computing, 79(3):1667–1712, 2019.
320"
REFERENCES,0.8006993006993007,"[7] N. Chopin, O. Papaspiliopoulos, et al. An introduction to sequential Monte Carlo. Springer,
321"
REFERENCES,0.8024475524475524,"2020.
322"
REFERENCES,0.8041958041958042,"[8] K. L. Chung and J. B. Walsh. Markov processes, Brownian motion, and time symmetry, volume
323"
REFERENCES,0.8059440559440559,"249. Springer Science & Business Media, 2006.
324"
REFERENCES,0.8076923076923077,"[9] P. Del Moral. Feynman-Kac formulae: genealogical and interacting particle systems with
325"
REFERENCES,0.8094405594405595,"applications, volume 88. Springer, 2004.
326"
REFERENCES,0.8111888111888111,"[10] P. Del Moral and L. M. Murray. Sequential Monte Carlo with highly informative observations.
327"
REFERENCES,0.8129370629370629,"SIAM/ASA Journal on Uncertainty Quantification, 3(1):969–997, 2015.
328"
REFERENCES,0.8146853146853147,"[11] B. Dennis and R. F. Costantino. Analysis of steady-state populations with the gamma abundance
329"
REFERENCES,0.8164335664335665,"model: application to Tribolium. Ecology, 69(4):1200–1213, 1988.
330"
REFERENCES,0.8181818181818182,"[12] A. Doucet, A. M. Johansen, et al. A tutorial on particle filtering and smoothing: Fifteen years
331"
REFERENCES,0.8199300699300699,"later. Handbook of nonlinear filtering, 12(656-704):3, 2009.
332"
REFERENCES,0.8216783216783217,"[13] P. Fearnhead, O. Papaspiliopoulos, and G. O. Roberts. Particle filters for partially observed
333"
REFERENCES,0.8234265734265734,"diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(4):
334"
REFERENCES,0.8251748251748252,"755–777, 2008.
335"
REFERENCES,0.8269230769230769,"[14] P. Fearnhead, O. Papaspiliopoulos, G. O. Roberts, and A. Stuart. Random-weight particle
336"
REFERENCES,0.8286713286713286,"filtering of continuous time processes. Journal of the Royal Statistical Society: Series B
337"
REFERENCES,0.8304195804195804,"(Statistical Methodology), 72(4):497–512, 2010.
338"
REFERENCES,0.8321678321678322,"[15] M. Gerber, N. Chopin, and N. Whiteley. Negative association, ordering and convergence of
339"
REFERENCES,0.833916083916084,"resampling methods. The Annals of Statistics, 47(4):2236–2260, 2019.
340"
REFERENCES,0.8356643356643356,"[16] I. V. Girsanov. On transforming a certain class of stochastic processes by absolutely continuous
341"
REFERENCES,0.8374125874125874,"substitution of measures. Theory of Probability & Its Applications, 5(3):285–301, 1960.
342"
REFERENCES,0.8391608391608392,"[17] J. Han, A. Jentzen, et al. Deep learning-based numerical methods for high-dimensional parabolic
343"
REFERENCES,0.8409090909090909,"partial differential equations and backward stochastic differential equations. Communications
344"
REFERENCES,0.8426573426573427,"in Mathematics and Statistics, 5(4):349–380, 2017.
345"
REFERENCES,0.8444055944055944,"[18] J. Han, A. Jentzen, and E. Weinan. Solving high-dimensional partial differential equations using
346"
REFERENCES,0.8461538461538461,"deep learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.
347"
REFERENCES,0.8479020979020979,"[19] C. Hartmann, L. Richter, C. Schütte, and W. Zhang. Variational characterization of free energy:
348"
REFERENCES,0.8496503496503497,"Theory and algorithms. Entropy, 19(11):626, 2017.
349"
REFERENCES,0.8513986013986014,"[20] C. Hartmann, O. Kebiri, L. Neureither, and L. Richter. Variational approach to rare event
350"
REFERENCES,0.8531468531468531,"simulation using least-squares regression. Chaos: An Interdisciplinary Journal of Nonlinear
351"
REFERENCES,0.8548951048951049,"Science, 29(6):063107, 2019.
352"
REFERENCES,0.8566433566433567,"[21] C. Huré, H. Pham, and X. Warin. Deep backward schemes for high-dimensional nonlinear
353"
REFERENCES,0.8583916083916084,"PDEs. Mathematics of Computation, 89(324):1547–1579, 2020.
354"
REFERENCES,0.8601398601398601,"[22] M. Hutzenthaler and T. Kruse. Multilevel Picard approximations of high-dimensional semilinear
355"
REFERENCES,0.8618881118881119,"parabolic differential equations with gradient-dependent nonlinearities. SIAM Journal on
356"
REFERENCES,0.8636363636363636,"Numerical Analysis, 58(2):929–961, 2020.
357"
REFERENCES,0.8653846153846154,"[23] M. Hutzenthaler, A. Jentzen, T. Kruse, T. Anh Nguyen, and P. von Wurstemberger. Overcoming
358"
REFERENCES,0.8671328671328671,"the curse of dimensionality in the numerical approximation of semilinear parabolic partial
359"
REFERENCES,0.8688811188811189,"differential equations. Proceedings of the Royal Society A, 476(2244):20190630, 2020.
360"
REFERENCES,0.8706293706293706,"[24] O. Kebiri, L. Neureither, and C. Hartmann. Adaptive importance sampling with forward-
361"
REFERENCES,0.8723776223776224,"backward stochastic differential equations. In International workshop on Stochastic Dynamics
362"
REFERENCES,0.8741258741258742,"out of Equilibrium, pages 265–281. Springer, 2017.
363"
REFERENCES,0.8758741258741258,"[25] P. E. Kloeden and E. Platen. Stochastic differential equations. In Numerical Solution of
364"
REFERENCES,0.8776223776223776,"Stochastic Differential Equations, pages 103–160. Springer, 1992.
365"
REFERENCES,0.8793706293706294,"[26] J. Knape and P. De Valpine. Fitting complex population models by combining particle filters
366"
REFERENCES,0.8811188811188811,"with Markov chain Monte Carlo. Ecology, 93(2):256–263, 2012.
367"
REFERENCES,0.8828671328671329,"[27] B. Oksendal. Stochastic differential equations: an introduction with applications. Springer
368"
REFERENCES,0.8846153846153846,"Science & Business Media, 2013.
369"
REFERENCES,0.8863636363636364,"[28] E. Pardoux and S. Peng. Adapted solution of a backward stochastic differential equation.
370"
REFERENCES,0.8881118881118881,"Systems & Control Letters, 14(1):55–61, 1990.
371"
REFERENCES,0.8898601398601399,"[29] E. Pardoux and S. Peng. Backward stochastic differential equations and quasilinear parabolic
372"
REFERENCES,0.8916083916083916,"partial differential equations. In Stochastic partial differential equations and their applications,
373"
REFERENCES,0.8933566433566433,"pages 200–217. Springer, 1992.
374"
REFERENCES,0.8951048951048951,"[30] E. Pardoux and S. Tang. Forward-backward stochastic differential equations and quasilinear
375"
REFERENCES,0.8968531468531469,"parabolic PDEs. Probability Theory and Related Fields, 114(2):123–150, 1999.
376"
REFERENCES,0.8986013986013986,"[31] J. Park and E. L. Ionides. Inference on high-dimensional implicit dynamic models using a
377"
REFERENCES,0.9003496503496503,"guided intermediate resampling filter. Statistics and Computing, 30(5):1497–1522, 2020.
378"
REFERENCES,0.9020979020979021,"[32] M. Pereira, Z. Wang, I. Exarchos, and E. A. Theodorou. Learning deep stochastic optimal
379"
REFERENCES,0.9038461538461539,"control policies using forward-backward SDEs. arXiv preprint arXiv:1902.03986, 2019.
380"
REFERENCES,0.9055944055944056,"[33] M. K. Pitt and N. Shephard. Filtering via simulation: Auxiliary particle filters. Journal of the
381"
REFERENCES,0.9073426573426573,"American statistical association, 94(446):590–599, 1999.
382"
REFERENCES,0.9090909090909091,"[34] M. Raissi. Forward-backward stochastic neural networks: Deep learning of high-dimensional
383"
REFERENCES,0.9108391608391608,"partial differential equations. arXiv preprint arXiv:1804.07010, 2018.
384"
REFERENCES,0.9125874125874126,"[35] G. O. Roberts and O. Stramer. On inference for partially observed nonlinear diffusion models
385"
REFERENCES,0.9143356643356644,"using the Metropolis–Hastings algorithm. Biometrika, 88(3):603–621, 2001.
386"
REFERENCES,0.916083916083916,"[36] L. C. G. Rogers and D. Williams. Diffusions, Markov processes and Martingales: Volume 2:
387"
REFERENCES,0.9178321678321678,"Itô Calculus, volume 2. Cambridge university press, 2000.
388"
REFERENCES,0.9195804195804196,"[37] H. Sørensen. Parametric inference for diffusion processes observed at discrete points in time: a
389"
REFERENCES,0.9213286713286714,"survey. International Statistical Review, 72(3):337–354, 2004.
390"
REFERENCES,0.9230769230769231,"[38] S. Thijssen and H. Kappen. Path integral control and state-dependent feedback. Physical Review
391"
REFERENCES,0.9248251748251748,"E, 91(3):032104, 2015.
392"
REFERENCES,0.9265734265734266,"[39] J. Wang, K. Zhang, L. Xu, and E. Wang. Quantifying the Waddington landscape and biological
393"
REFERENCES,0.9283216783216783,"paths for development and differentiation. Proceedings of the National Academy of Sciences,
394"
REFERENCES,0.9300699300699301,"108(20):8257–8262, 2011.
395"
REFERENCES,0.9318181818181818,"[40] J. Yong and X. Y. Zhou. Stochastic controls: Hamiltonian systems and HJB equations, vol-
396"
REFERENCES,0.9335664335664335,"ume 43. Springer Science & Business Media, 1999.
397"
REFERENCES,0.9353146853146853,"Checklist
398"
REFERENCES,0.9370629370629371,"1. For all authors...
399"
REFERENCES,0.9388111888111889,"(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s
400"
REFERENCES,0.9405594405594405,"contributions and scope? [Yes] See Introduction Section
401"
REFERENCES,0.9423076923076923,"(b) Did you describe the limitations of your work? [Yes] See Introduction Section
402"
REFERENCES,0.9440559440559441,"(c) Did you discuss any potential negative societal impacts of your work? [No]
403"
REFERENCES,0.9458041958041958,"(d) Have you read the ethics review guidelines and ensured that your paper conforms to
404"
REFERENCES,0.9475524475524476,"them? [Yes]
405"
REFERENCES,0.9493006993006993,"2. If you are including theoretical results...
406"
REFERENCES,0.951048951048951,"(a) Did you state the full set of assumptions of all theoretical results? [N/A]
407"
REFERENCES,0.9527972027972028,"(b) Did you include complete proofs of all theoretical results? [N/A]
408"
REFERENCES,0.9545454545454546,"3. If you ran experiments...
409"
REFERENCES,0.9562937062937062,"(a) Did you include the code, data, and instructions needed to reproduce the main ex-
410"
REFERENCES,0.958041958041958,"perimental results (either in the supplemental material or as a URL)? [Yes] A Py-
411"
REFERENCES,0.9597902097902098,"Torch implementation to reproduce our numerical experiments is available at https:
412"
REFERENCES,0.9615384615384616,"//anonymous.4open.science/r/CompDoobTransform/
413"
REFERENCES,0.9632867132867133,"(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
414"
REFERENCES,0.965034965034965,"were chosen)? [Yes] See Experiment Section
415"
REFERENCES,0.9667832167832168,"(c) Did you report error bars (e.g., with respect to the random seed after running experi-
416"
REFERENCES,0.9685314685314685,"ments multiple times)? [N/A]
417"
REFERENCES,0.9702797202797203,"(d) Did you include the total amount of compute and the type of resources used (e.g., type
418"
REFERENCES,0.972027972027972,"of GPUs, internal cluster, or cloud provider)? [Yes] CPU and compute time reported
419"
REFERENCES,0.9737762237762237,"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
420"
REFERENCES,0.9755244755244755,"(a) If your work uses existing assets, did you cite the creators? [N/A]
421"
REFERENCES,0.9772727272727273,"(b) Did you mention the license of the assets? [N/A]
422"
REFERENCES,0.9790209790209791,"(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
423"
REFERENCES,0.9807692307692307,"(d) Did you discuss whether and how consent was obtained from people whose data you’re
424"
REFERENCES,0.9825174825174825,"using/curating? [N/A]
425"
REFERENCES,0.9842657342657343,"(e) Did you discuss whether the data you are using/curating contains personally identifiable
426"
REFERENCES,0.986013986013986,"information or offensive content? [N/A]
427"
REFERENCES,0.9877622377622378,"5. If you used crowdsourcing or conducted research with human subjects...
428"
REFERENCES,0.9895104895104895,"(a) Did you include the full text of instructions given to participants and screenshots, if
429"
REFERENCES,0.9912587412587412,"applicable? [N/A]
430"
REFERENCES,0.993006993006993,"(b) Did you describe any potential participant risks, with links to Institutional Review
431"
REFERENCES,0.9947552447552448,"Board (IRB) approvals, if applicable? [N/A]
432"
REFERENCES,0.9965034965034965,"(c) Did you include the estimated hourly wage paid to participants and the total amount
433"
REFERENCES,0.9982517482517482,"spent on participant compensation? [N/A]
434"
