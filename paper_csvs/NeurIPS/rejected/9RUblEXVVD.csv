Section,Section Appearance Order,Paragraph
ABSTRACT,0.0,Abstract
ABSTRACT,0.0018050541516245488,"Existing neural architecture search (NAS) methods typically rely on pre-specified
1"
ABSTRACT,0.0036101083032490976,"super deep neural networks (super-networks) with handcrafted search spaces be-
2"
ABSTRACT,0.005415162454873646,"forehand. Such requirements make it challenging to extend them onto general
3"
ABSTRACT,0.007220216606498195,"scenarios without significant human expertise and manual intervention. To over-
4"
ABSTRACT,0.009025270758122744,"come the limitations, we propose the third generation of Only-Train-Once (OTOv3).
5"
ABSTRACT,0.010830324909747292,"OTOv3 is perhaps the first automated system that trains general super-networks and
6"
ABSTRACT,0.01263537906137184,"produces high-performing sub-networks in the one shot manner without pretraining
7"
ABSTRACT,0.01444043321299639,"and fine-tuning. Technologically, OTOv3 delivers three noticeable contributions
8"
ABSTRACT,0.016245487364620937,"to minimize human efforts: (i) automatic search space construction for general
9"
ABSTRACT,0.018050541516245487,"super-networks; (ii) a Hierarchical Half-Space Projected Gradient (H2SPG) that
10"
ABSTRACT,0.019855595667870037,"leverages the dependency graph to ensure the network validity during optimization
11"
ABSTRACT,0.021660649819494584,"and reliably produces a solution with both high performance and hierarchical group
12"
ABSTRACT,0.023465703971119134,"sparsity; and (iii) automatic sub-network construction based on the super-network
13"
ABSTRACT,0.02527075812274368,"and the H2SPG solution. Numerically, we demonstrate the effectiveness of OTOv3
14"
ABSTRACT,0.02707581227436823,"on a variety of super-networks, including StackedUnets, SuperResNet, and DARTS,
15"
ABSTRACT,0.02888086642599278,"over benchmark datasets such as CIFAR10, Fashion-MNIST, ImageNet, STL-10,
16"
ABSTRACT,0.030685920577617327,"and SVNH. The sub-networks computed by OTOv3 achieve competitive even
17"
ABSTRACT,0.032490974729241874,"superior performance compared to the super-networks and other state-of-the-arts.
18"
INTRODUCTION,0.03429602888086643,"1
Introduction
19"
INTRODUCTION,0.036101083032490974,"Deep neural networks (DNNs) have achieved remarkable success in various fields, which success is
20"
INTRODUCTION,0.03790613718411552,"highly dependent on their sophisticated underlying architectures (LeCun et al., 2015; Goodfellow
21"
INTRODUCTION,0.039711191335740074,"et al., 2016). To design effective DNN architectures, human expertise have handcrafted numerous
22"
INTRODUCTION,0.04151624548736462,"popular DNNs such as ResNet (He et al., 2016) and transformer (Vaswani et al., 2017). However,
23"
INTRODUCTION,0.04332129963898917,"such human efforts may not be scalable enough to meet the increasing demands for customizing
24"
INTRODUCTION,0.04512635379061372,"DNNs for diverse tasks. To address this issue, Neural Architecture Search (NAS) has emerged to
25"
INTRODUCTION,0.04693140794223827,"automate the network creations and reduce the need for human expertise (Elsken et al., 2018).
26"
INTRODUCTION,0.048736462093862815,"Among current NAS studies, gradient-based methods (Liu et al., 2018; Yang et al., 2020; Xu et al.,
27"
INTRODUCTION,0.05054151624548736,"2019; Chen et al., 2021b) are perhaps the most popular because of their efficiency. Such methods
28"
INTRODUCTION,0.052346570397111915,"build an over-parameterized super-network covering all candidate connections and operations, param-
29"
INTRODUCTION,0.05415162454873646,"eterize operations via introducing auxiliary architecture variables with weight sharing, then search a
30"
INTRODUCTION,0.05595667870036101,"(sub)optimal sub-network via formulating and solving a multi-level optimization problem.
31"
INTRODUCTION,0.05776173285198556,"Despite the advancements in gradient-based methods, their usage is still limited due to certain
32"
INTRODUCTION,0.05956678700361011,"inconvenience. In particular, their automation relies on manually determining the search space for a
33"
INTRODUCTION,0.061371841155234655,"pre-specified super-network beforehand, and requires the manual introduction of auxiliary architecture
34"
INTRODUCTION,0.0631768953068592,"variables onto the prescribed search space. To extend these methods onto other super-networks, the
35"
INTRODUCTION,0.06498194945848375,"users still need to manually construct the search pool, then incorporate the auxiliary architecture
36"
INTRODUCTION,0.06678700361010831,"variables along with building the whole complicated multi-level optimization training pipeline. The
37"
INTRODUCTION,0.06859205776173286,"whole process necessitates significant domain-knowledge and engineering efforts, thereby being
38"
INTRODUCTION,0.0703971119133574,"inconvenient and time-consuming for users. Therefore, it is natural to ask whether we could reach an
39"
INTRODUCTION,0.07220216606498195,"Objective. Given a general super-network, automatically generate its search space, train it once, and
40"
INTRODUCTION,0.0740072202166065,"construct a sub-network that achieves a dramatically compact architecture and high performance.
41"
INTRODUCTION,0.07581227436823104,"OTOv3
OTOv2
Other NAS
General DNNs
✓
✓
✗
Autonomy
✓
✓
✓ –"
INTRODUCTION,0.0776173285198556,"Remove Connections
✓
✗
✓
Remove Operations
✓
✗
✓
Slim Operations
✓†
✓
✗
† Support while is not the focus and discussed in this work."
INTRODUCTION,0.07942238267148015,"Achieving the objective is severely challenging in terms
42"
INTRODUCTION,0.0812274368231047,"of both engineering developments and algorithmic de-
43"
INTRODUCTION,0.08303249097472924,"signs, consequently not achieved yet by the existing
44"
INTRODUCTION,0.08483754512635379,"NAS works to the best of our knowledge. However, the
45"
INTRODUCTION,0.08664259927797834,"objective has been recently achieved in an analogous
46"
INTRODUCTION,0.08844765342960288,"task so-called structured pruning (Lin et al., 2019) by
47"
INTRODUCTION,0.09025270758122744,"the second generation of Only-Train-Once framework (OTOv2) (Chen et al., 2021a, 2023). From
48"
INTRODUCTION,0.09205776173285199,"the perspective of computational graph, the standard NAS could be considered as removing entire
49"
INTRODUCTION,0.09386281588447654,"redundant connections (cutting edges) and operations (vertices) from super-networks. Structured
50"
INTRODUCTION,0.09566787003610108,"pruning can be largely interpreted as a complementary NAS that removes the redundancy inside each
51"
INTRODUCTION,0.09747292418772563,"vertex (slims operations) but preserves all the connections. OTOv2 first achieves the objective in the
52"
INTRODUCTION,0.09927797833935018,"view of structured pruning that given a general DNN, automatically trains it only once to achieve
53"
INTRODUCTION,0.10108303249097472,"both high performance and a slimmer model architecture without pre-training and fine-tuning.
54"
INTRODUCTION,0.10288808664259928,"OTOv3 Library Usage
1
from only_train_once import OTO
2
# General Super-Network
3
oto = OTO(super_net, cut_edges=True)
4
optimizer = oto.h2spg()
5
# Train as normal
6
optimizer.step()
7
oto.construct_subnet(cut_edges=True)"
INTRODUCTION,0.10469314079422383,"We now build the third-generation of Only-Train-Once
55"
INTRODUCTION,0.10649819494584838,"(OTOv3) that reaches the objective from the perspective
56"
INTRODUCTION,0.10830324909747292,"of the standard NAS. OTOv3 automatically generates a
57"
INTRODUCTION,0.11010830324909747,"search space given a general super-network, trains and
58"
INTRODUCTION,0.11191335740072202,"identifies redundant connections and vertices, then builds
59"
INTRODUCTION,0.11371841155234658,"a sub-network that achieves both high performance and
60"
INTRODUCTION,0.11552346570397112,"compactness. As the library usage presented aside, the
61"
INTRODUCTION,0.11732851985559567,"whole procedure can be automatically proceeded, dramatically reduce the human efforts, and fit for
62"
INTRODUCTION,0.11913357400722022,"general super-networks and applications. Our main contributions can be summarized as follows.
63"
INTRODUCTION,0.12093862815884476,"• Infrastructure for Automated General Super-Network Training and Sub-Network Searching.
64"
INTRODUCTION,0.12274368231046931,"We propose OTOv3 that perhaps the first automatically trains and searches within a general super-
65"
INTRODUCTION,0.12454873646209386,"network to deliver a compact sub-network by erasing redundant connections and operations in the
66"
INTRODUCTION,0.1263537906137184,"one-shot manner. As the previous OTO versions, OTOv3 trains the super-network only once without
67"
INTRODUCTION,0.12815884476534295,"the need of pre-training and fine-tuning and is pluggable into various deep learning applications.
68"
INTRODUCTION,0.1299638989169675,"• Automated Search Space Generation. We propose a novel graph algorithm to automatically
69"
INTRODUCTION,0.13176895306859207,"explore and establish a dependency graph given a general super-network, then analyze the de-
70"
INTRODUCTION,0.13357400722021662,"pendency to form a search space consisting of minimal removal structures. The corresponding
71"
INTRODUCTION,0.13537906137184116,"trainable variables are then partitioned into so-called generalized zero-invariant groups (GeZIGs).
72"
INTRODUCTION,0.1371841155234657,"• Hierarchical Half-Space Projected Gradient (H2SPG). We propose a novel H2SPG optimizer
73"
INTRODUCTION,0.13898916967509026,"that perhaps the first solves a hierarchical structured sparsity problem for general DNNs. H2SPG
74"
INTRODUCTION,0.1407942238267148,"computes a solution xH2SPG∗of both high performance and desired hierarchical group sparsity
75"
INTRODUCTION,0.14259927797833935,"in the manner of GeZIGs. Compared to other optimizers, H2SPG considers the hierarchy of
76"
INTRODUCTION,0.1444043321299639,"dependency graph to produce sparsity for ensuring the validness of the subsequent sub-network.
77"
INTRODUCTION,0.14620938628158844,"• Automated Sub-Network Construction. We propose a novel graph algorithm to automatically
78"
INTRODUCTION,0.148014440433213,"construct a sub-network upon the super-network parameterized as x∗
H2SPG. The resulting sub-
79"
INTRODUCTION,0.14981949458483754,"network returns the exact same outputs as the super-network thereby no need of further fine-tuning.
80"
INTRODUCTION,0.15162454873646208,"• Experimental Results. We demonstrate the effectiveness of OTOv3 on extensive super-networks
81"
INTRODUCTION,0.15342960288808663,"including StackedUnets, SuperResNet and DARTS, over benchmark datasets including CIFAR10,
82"
INTRODUCTION,0.1552346570397112,"Fashion-MNIST, ImageNet, STL-10, and SVNH. OTOv3 is the first framework that could auto-
83"
INTRODUCTION,0.15703971119133575,"matically deliver compact sub-networks upon general super-networks to the best of our knowledge.
84"
INTRODUCTION,0.1588447653429603,"Meanwhile the sub-networks exhibit competitive even superior performance to the super-networks.
85"
RELATED WORK,0.16064981949458484,"2
Related Work
86"
RELATED WORK,0.1624548736462094,"Neural Architecture Search (NAS).
Early NAS works utilized reinforcement learning and evolu-
87"
RELATED WORK,0.16425992779783394,"tion techniques to search for high-quality architectures (Zoph & Le, 2016; Pham et al., 2018; Zoph
88"
RELATED WORK,0.16606498194945848,"et al., 2018), while they were computationally expensive. Later on, differentiable (gradient-based)
89"
RELATED WORK,0.16787003610108303,"methods were introduced to accelerate the search process. These methods start with a super-network
90"
RELATED WORK,0.16967509025270758,"covering all possible connection and operation candidates, and parameterize them with auxiliary
91"
RELATED WORK,0.17148014440433212,"architecture variables. They establish a multi-level optimization problem that alternatingly updates
92"
RELATED WORK,0.17328519855595667,"the architecture and network variables until convergence (Liu et al., 2018; Chen et al., 2019; Xu et al.,
93"
RELATED WORK,0.17509025270758122,"2019; Yang et al., 2020; Hosseini & Xie, 2022). However, these methods require a significant amount
94"
RELATED WORK,0.17689530685920576,"of handcraftness from users in advance to manually establish the search space, introduce additional
95"
RELATED WORK,0.17870036101083034,"architecture variables, and build the multi-level training pipeline. The sub-network construction is
96"
RELATED WORK,0.18050541516245489,"also network-specific and not flexible. All requirements necessitate remarkable domain-knowledge
97"
RELATED WORK,0.18231046931407943,"and expertise, making it difficult to extend to general super-networks and broader scenarios.
98"
RELATED WORK,0.18411552346570398,"Automated Structured Pruning for General DNNs.
Structure pruning is an orthogonal but related
99"
RELATED WORK,0.18592057761732853,"paradigm to standard NAS. Rather than removing entire operations and connections, it focuses on
100"
RELATED WORK,0.18772563176895307,"slimming individual vertices (Han et al., 2015). Similarly, prior structure pruning methods also
101"
RELATED WORK,0.18953068592057762,"required numerous handcraftness and domain knowledge, which limited their broader applicability.
102"
RELATED WORK,0.19133574007220217,"However, recent methods such as OTOv2 (Chen et al., 2023) and DepGraph (Fang et al., 2023) have
103"
RELATED WORK,0.1931407942238267,"made progress in automating the structure pruning process for general DNNs. OTOv2 is a one-shot
104"
RELATED WORK,0.19494584837545126,"method that does not require pre-training or fine-tuning, while DepGraph involves a multi-stage
105"
RELATED WORK,0.1967509025270758,"training pipeline that requires some manual intervention. In this work, we propose the third-generation
106"
RELATED WORK,0.19855595667870035,"version of OTO that enables automatic sub-network searching and training for general super-networks.
107 108"
RELATED WORK,0.2003610108303249,"Hierarchical Structured Sparsity Optimization.
We formulate the underlying optimization
109"
RELATED WORK,0.20216606498194944,"problem of OTOv3 as a hierarchical structured sparsity problem. Its solution possesses high group
110"
RELATED WORK,0.20397111913357402,"sparsity indicating redundant structures and obeys specified hierarchy. There exist deterministic
111"
RELATED WORK,0.20577617328519857,"optimizers solving such problems via introducing latent variables (Zhao et al., 2009), while are
112"
RELATED WORK,0.2075812274368231,"impractical for stochastic DNN tasks. Meanwhile, stochastic optimizers rarely study such problem.
113"
RELATED WORK,0.20938628158844766,"In fact, popular stochastic sparse optimizers such as HSPG (Chen et al., 2021a), DHSPG (Chen et al.,
114"
RELATED WORK,0.2111913357400722,"2023), proximal methods (Xiao & Zhang, 2014) and ADMM (Lin et al., 2019) overlook the hierarchy
115"
RELATED WORK,0.21299638989169675,"constraint. Incorporating them into OTOv3 typically delivers invalid sub-networks. Therefore, we
116"
RELATED WORK,0.2148014440433213,"propose H2SPG that considers graph dependency to solve it for general DNNs.
117"
RELATED WORK,0.21660649819494585,"3
OTOv3
118"
RELATED WORK,0.2184115523465704,"OTOv3 is an automated one-shot system that trains a general super-network and constructs a sub-
119"
RELATED WORK,0.22021660649819494,"network. The produced sub-network is not only high-performing but also has a dramatically compact
120"
RELATED WORK,0.22202166064981949,"architecture that is suitable for various shipping environments. The entire process minimizes the need
121"
RELATED WORK,0.22382671480144403,"for human efforts and is suitable for general DNNs. As outlined in Algorithm 1, given a general super-
122"
RELATED WORK,0.22563176895306858,"network M, OTOv3 first explores and establishes a dependency graph. Upon the dependency graph,
123"
RELATED WORK,0.22743682310469315,"a search space is automatically constructed and corresponding trainable variables are partitioned
124"
RELATED WORK,0.2292418772563177,"into generalized zero-invariant groups (GeZIGs) (Section 3.1). A hierarchical structured sparsity
125"
RELATED WORK,0.23104693140794225,"optimization problem is then formulated and solved by a novel Hierarchical Half-Space Projected
126"
RELATED WORK,0.2328519855595668,"Gradient (H2SPG) (Section 3.2). H2SPG considers the hierarchy inside the dependency graph and
127"
RELATED WORK,0.23465703971119134,"computes a solution x∗
H2SPG of both high-performance and desired hierarchical group sparsity over
128"
RELATED WORK,0.2364620938628159,"GeZIGs. A compact sub-network M∗is finally constructed via removing the structures corresponding
129"
RELATED WORK,0.23826714801444043,"to the identified redundant GeZIGs and their dependent structures (Section 3.3). M∗returns the exact
130"
RELATED WORK,0.24007220216606498,"same output as the super-network parameterized as x∗
H2SPG, eliminating the need of fine-tuning.
131"
RELATED WORK,0.24187725631768953,Algorithm 1 Outline of OTOv3.
RELATED WORK,0.24368231046931407,"1: Input: A general DNN M as super-network to be trained and searched (no need to be pretrained).
2: Automated Search Space Construction. Establish dependency graph and partition the trainable
parameters of M into generalized zero-invariant groups GGeZIG and the complementary GC
GeZIG.
3: Train by H2SPG. Seek a high-performing solution x∗
H2SPG with hierarchical group sparsity.
4: Automated Sub-Network M∗Construction. Construct a sub-network upon x∗
H2SPG.
5: Output: Constructed sub-network M∗(no need to be fine-tuned)."
AUTOMATED SEARCH SPACE CONSTRUCTION,0.24548736462093862,"3.1
Automated Search Space Construction
132"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.24729241877256317,"The foremost step is to automatically construct the search space for a general super-network. However,
133"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2490974729241877,"this process presents significant challenges in terms of both engineering developments and algorithmic
134"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2509025270758123,"designs due to the complexity of DNN architecture and the lack of sufficient public APIs. To overcome
135"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2527075812274368,"Conv1
BN1
Input"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2545126353790614,"Conv2
BN2"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2563176895306859,"Conv3
BN3"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2581227436823105,"Conv5
BN5"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.259927797833935,MaxPool
AUTOMATED SEARCH SPACE CONSTRUCTION,0.26173285198555957,"Conv4
BN4
AvgPool"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.26353790613718414,"Concat
Conv6
Conv7
BN7"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.26534296028880866,"Conv8
BN8"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.26714801444043323,"Linear1
AvgPool
Output"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.26895306859205775,(a) A demo super-network (DemoSupNet) to be trained and search.
AUTOMATED SEARCH SPACE CONSTRUCTION,0.27075812274368233,"Input
Conv1-BN1"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.27256317689530685,Conv5-BN5
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2743682310469314,Conv7-BN7
AUTOMATED SEARCH SPACE CONSTRUCTION,0.27617328519855594,Conv8-BN8
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2779783393501805,Output
AUTOMATED SEARCH SPACE CONSTRUCTION,0.27978339350180503,Conv2-BN2
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2815884476534296,MaxPool-Conv3-BN3
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2833935018050541,AvgPool-Conv4-BN4
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2851985559566787,"Concat
Conv6"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2870036101083033,AvgPool-Linear1
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2888086642599278,(b) Dependency Graph.
AUTOMATED SEARCH SPACE CONSTRUCTION,0.29061371841155237,"K2
K3
K4
K5
K6
K7
K8
b2
b3
b4
γ2 β2
β3
γ3
γ4 β4
γ5
γ7
γ8
β5
β7
β8"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2924187725631769,"GC
GeZIG = {g1, g9}
g2
g3
g4
g5
g6
g7
g8"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.29422382671480146,"K1
γ1 β1 g1 W1"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.296028880866426,"g9
GGeZIG = {g2, g3, · · · , g8}
G = GGeZIG
 GC
GeZIG"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.29783393501805056,(c) Generalized Zero-Invariant Groups.
AUTOMATED SEARCH SPACE CONSTRUCTION,0.2996389891696751,"Figure 1: Automated Search Space Construction. bKi and bi are the flatten filter matrix and bias vector
for Conv-i, respectively. γi and βi are the weight and bias vectors for BN-i. Wi is the weight
matrix for Linear-i. The columns of bK6 are marked in accordance to its incoming segments."
AUTOMATED SEARCH SPACE CONSTRUCTION,0.30144404332129965,"these challenges, we propose a concept called generalized zero-invariant group (GeZIG) and formulate
136"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.30324909747292417,"the search space construction as the GeZIG partition. We have also developed a dedicated graph
137"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.30505415162454874,"algorithm to automatically conduct the GeZIG partition for general super-networks.
138"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.30685920577617326,"Generalized Zero-Invariant Group (GeZIG).
The key of search space construction is to figure
139"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.30866425992779783,"out the structures that can be removed from the super-network. Because of diverse roles of operations
140"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3104693140794224,"and their complicated connections inside a DNN, removing an arbitrary structure may cause the
141"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.31227436823104693,"remaining DNN invalid. We say a structure removal if and only if the DNN after removing it is still
142"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3140794223826715,"valid. A removal structure is further said minimal if and only if it does not contain multiple removal
143"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.315884476534296,"structures. Zero-Invariant Group (ZIG) is proposed in (Chen et al., 2021a, 2023) that describes a
144"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3176895306859206,"class of minimal removal structures satisfying a zero-invariant property, i.e., if all variables in ZIG
145"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3194945848375451,"equal to zero, then no matter what the input is, the output is always as zero. ZIG depicts the minimal
146"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3212996389891697,"removal structure inside each operation and is the key for realizing automatic one-shot structured
147"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3231046931407942,"pruning. We generalize ZIG as GeZIG that describes a class of minimal removal structures satisfying
148"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3249097472924188,"the zero-invariant property but consists of entire operations. More illustrations regarding ZIG versus
149"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3267148014440433,"GeZIG are present in Appendix. For simplicity, throughout the paper, the minimal removal structure
150"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3285198555956679,"is referred to the counterpart consisting of operations in entirety. Consequently, automated search
151"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3303249097472924,"space construction becomes how to automatically explore the GeZIG partition for general DNNs.
152"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.33212996389891697,"Automated GeZIG Partition.
As specified in Algorithm 2, automated GeZIG partition involves
153"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.33393501805054154,"two main stages. The first stage explores the super-network M and establishes a dependency graph
154"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.33574007220216606,"(Vd, Ed). The second stage leverages the affiliations inside the dependency graph to find out minimal
155"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.33754512635379064,"removal structures, then partitions their trainable variables to form GeZIGs. For intuitive illustrations,
156"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.33935018050541516,"we elaborate the algorithm through a small but complex demo super-network depicted in Figure 1a.
157"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.34115523465703973,"Dependency Graph Construction.
Dependency Graph Construction. Given a super-network M, we first construct its trace graph (V, E)
158"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.34296028880866425,"displayed as Figure 1a (line 3 in Algorithm 2), where V represents the set of vertices (operations) and
159"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3447653429602888,"E represents the connections among them. As OTOv2 (Chen et al., 2023), we categorize the vertices
160"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.34657039711191334,"into stem vertices, joint vertices, accessory vertices, and unknown vertices. Stem vertices refer to the
161"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3483754512635379,"operations that contain trainable variables and can transform the input tensors into different shapes,
162"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.35018050541516244,"e.g., Conv and Linear. The accessory vertices are the operations that may not have trainable
163"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.351985559566787,"variables and have an single input, e.g., BN and ReLU. Joint vertices aggregate multiple inputs into a
164"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.35379061371841153,"single output, e.g., Add and Concat. The remaining vertices are considered as unknown.
165"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3555956678700361,Algorithm 2 Automated Search Space Construction.
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3574007220216607,"1: Input: A super-network M to be trained and searched.
2: Dependency graph construction.
3: Construct the trace graph (E, V) of M.
4: Initialize an empty graph (Vd, Ed).
5: Initialize queue Q ←{S(v) : v ∈V is adjacent to the input of trace graph}.
6: while Q ̸= ∅do
7:
Dequeue the head segment S from Q.
8:
Grow S in the depth-first manner till meet either joint vertex or multi-outgoing vertex ˆv.
9:
Add segments into Vd and connections into Ed.
10:
Enqueue new segments into the tail of Q if ˆv has outgoing vertices.
11: Find minimal removal structures.
12: Get the incoming vertices bV for joint vertices in the (Vd, Ed).
13: Group the trainable variables in the vertex v ∈bV as gv.
14: Form GGeZIG as the union of the above groups, i.e., GGeZIG ←{gv : v ∈bV}.
15: Form GC
GeZIG as the union of the trainable variables in the remaining vertices.
16: Return trainable variable partition G = GGeZIG ∪GC
GeZIG and dynamic dependency graph (Vd, Ed)."
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3592057761732852,"We begin by analyzing the trace graph (V, E) to create a dependency graph (Vd, Ed), wherein each
166"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.36101083032490977,"vertex in Vd serves as a potential minimal removal structure candidate. To proceed, we use a queue
167"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3628158844765343,"container Q to track the candidates (line 5 of Algorithm 2). The initial elements of this queue are
168"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.36462093862815886,"the vertices that are directly adjacent to the input of M, such as Conv1. We then traverse the graph
169"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3664259927797834,"in the breadth-first manner, iteratively growing each element (segment) S in the queue until a valid
170"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.36823104693140796,"minimal removal structure candidate is formed. The growth of each candidate follows the depth-first
171"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3700361010830325,"search to recursively expand S until the current vertices are considered as endpoints. The endpoint
172"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.37184115523465705,"vertex is determined by whether it is a joint vertex or has multiple outgoing vertices, as indicated
173"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.37364620938628157,"in line 8 of Algorithm 2. Intuitively, a joint vertex has multiple inputs, which means that the DNN
174"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.37545126353790614,"may be still valid after removing the current segment. This suggests that the current segment may
175"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.37725631768953066,"be removable. On the other hand, a vertex with multiple outgoing neighbors implies that removing
176"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.37906137184115524,"the current segment may cause some of its children to miss the input tensor. For instance, removing
177"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.38086642599277976,"Conv1-BN1 would cause Conv2, MaxPool and AvgPool to become invalid due to the absence
178"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.38267148014440433,"of input in Figure 1a. Therefore, it is risky to remove such candidates. Once the segment S has been
179"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3844765342960289,"grown, new candidates are initialized as the outgoing vertices of the endpoint and added into the
180"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3862815884476534,"container Q (line 10 in Algorithm 2). Such procedure is repeated until the end of graph traversal.
181"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.388086642599278,"Ultimately, a dependency graph (Vd, Ed) is created, as illustrated in Figure 1b.
182"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3898916967509025,"Form GeZIGs.
Form GeZIGs. We proceed to identify the minimal removal structures in (Vd, Ed) to create the GeZIG
183"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3916967509025271,"partition. The qualified instances are the vertices in Vd that have trainable variables and all of their
184"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3935018050541516,"outgoing vertices are joint vertices. This is because a joint vertex has multiple inputs and remains
185"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3953068592057762,"valid even after removing some of its incoming structures, as indicated in line 12 in Algorithm 2.
186"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3971119133574007,"Consequently, their trainable variables are grouped together into GeZIGs (line 13-14 in Algorithm 2
187"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.3989169675090253,"and Figure 1c). The remaining vertices are considered as either unremovable or belonging to a
188"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4007220216606498,"large removal structure, which trainable variables are grouped into the GC
GeZIG (the complementary
189"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.40252707581227437,"to GGeZIG). As a result, for the super-network M, all its trainable variables are encompassed by the
190"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4043321299638989,"union G = GGeZIG ∪GC
GeZIG, and the corresponding structures in GGeZIG constitute its search space.
191"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.40613718411552346,"3.2
Hierarchical Half-Space Projected Gradient (H2SPG)
192"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.40794223826714804,"Given a super-network M and its group partition G = GGeZIG ∪GC
GeZIG, the next is to jointly search
193"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.40974729241877256,"for a valid sub-network M∗that exhibits the most significant performance and train it to high
194"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.41155234657039713,"performance. Searching a sub-network is equivalent to identifying the redundant structures in GGeZIG
195"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.41335740072202165,"to be further removed and ensures the remaining network still valid. Training the sub-network
196"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4151624548736462,"becomes optimizing over the remaining groups in G to achieve high performance. We formulate a
197"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.41696750902527074,"hierarchical structured sparsity problem to accomplish both tasks simultaneously as follows.
198"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4187725631768953,"minimize
x∈Rn
f(x), s.t. Cardinality(G0) = K, and (Vd/VG0, Ed/EG0) is valid,
(1)"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.42057761732851984,"where f is the prescribed loss function, G=0 := {g ∈GGeZIG|[x]g = 0} is the set of zero groups in
199"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4223826714801444,"GGeZIG, which cardinality measures its size. K is the target group sparsity, indicating the number of
200"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.42418772563176893,"Input
Conv1-BN1"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4259927797833935,Conv5-BN5
AUTOMATED SEARCH SPACE CONSTRUCTION,0.427797833935018,Conv7-BN7
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4296028880866426,Conv8-BN8
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4314079422382672,Output
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4332129963898917,Conv2-BN2
AUTOMATED SEARCH SPACE CONSTRUCTION,0.43501805054151627,MaxPool-Conv3-BN3
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4368231046931408,AvgPool-Conv4-BN4
AUTOMATED SEARCH SPACE CONSTRUCTION,0.43862815884476536,"Concat
Conv6"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4404332129963899,AvgPool-Linear1
AUTOMATED SEARCH SPACE CONSTRUCTION,0.44223826714801445,disconnected
AUTOMATED SEARCH SPACE CONSTRUCTION,0.44404332129963897,"Figure 2: Check validness of redundant candidates. Target group sparsity K = 3. Conv7-BN7 has
larger redundancy score than Conv2-BN2. Dotted vertices are marked as redundant candidates."
AUTOMATED SEARCH SPACE CONSTRUCTION,0.44584837545126355,"GeZIGs that should be identified as redundant. The redundant GeZIGs are projected onto zero, while
201"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.44765342960288806,"the important groups are preserved as non-zero and optimized for high performance. A larger K
202"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.44945848375451264,"dictates a higher sparsity level that produces a more compact sub-network with fewer FLOPs and
203"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.45126353790613716,"parameters. (Vd/VG0, Ed/EG0) refers to the graph removing vertices and edges corresponding to
204"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.45306859205776173,"zero groups G0. This graph being valid is specified for NAS that requires the zero groups distributed
205"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4548736462093863,"obeying the hierarchy of super-network to ensure the resulting sub-network functions correctly.
206"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4566787003610108,"Problem (1) is difficult to solve due to the non-differential and non-convex sparsity constraint and
207"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4584837545126354,"the graph validity constraint. Existing optimizers such as DHSPG (Chen et al., 2023) overlook the
208"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4602888086642599,"architecture evolution and hierarchy during the sparsity exploration, which is crucial to (1). In fact,
209"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4620938628158845,"they are mainly applied for pruning tasks, where the connections and operations are preserved (but
210"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.463898916967509,"become slimmer). Consequently, employing them onto (1) usually produces invalid sub-networks.
211"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4657039711191336,Algorithm 3 Hierarchical Half-Space Projected Gradient
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4675090252707581,"1: Input: initial variable x0 ∈Rn, initial learning rate α0,
warm-up steps Tw, target group sparsity K, momentum
ω, dependency graph (Vd, Ed) and group partitions G.
2: Warm-up Phase.
3: for t = 0, 1, · · · , Tw −1 do
4:
Calculate gradient estimate ∇f(xt) or its variant.
5:
Update next iterate xt+1 ←xt −αt∇f(xt).
6:
Calculate redundancy score st,g for g ∈GGeZIG.
7:
Update sg ←ωsg + (1 −ω)st,g for g ∈GGeZIG."
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4693140794223827,"8: Construct Gr and GC
r given scores, G, (Vd, Ed), and K.
9: Hybrid Training Phase.
10: for t = Tw, Tw + 1, · · · , do
11:
Compute gradient estimate ∇f(xt) or its variant.
12:
Update [xt+1]GC
r as [xt −αt∇f(xt)]GC
r .
13:
Select proper λg for each g ∈Gr.
14:
Compute [˜xt+1]Gr via subgradient descent of ψ.
15:
Perform Half-Space projection over [˜xt+1]Gr.
16:
Update [xt+1]Gr ←[˜xt+1]Gr.
17: Return the final iterate x∗
DHSPG+."
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4711191335740072,"Outline of H2SPG.
To effectively
212"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4729241877256318,"solve problem (1), we propose a novel
213"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4747292418772563,"H2SPG to consider the hierarchy and
214"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.47653429602888087,"ensure the validness of graph architec-
215"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.47833935018050544,"ture after removing redundant vertices
216"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.48014440433212996,"and connections during the optimiza-
217"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.48194945848375453,"tion process. To the best of our knowl-
218"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.48375451263537905,"edge, H2SPG is the first the optimizer
219"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4855595667870036,"that successfully solves such hierar-
220"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.48736462093862815,"chical structured sparsity problem (1),
221"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4891696750902527,"which outline is stated in Algorithm 3.
222"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.49097472924187724,"H2SPG is built upon the DHSPG in
223"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4927797833935018,"OTOv2 but with dedicated designs
224"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.49458483754512633,"regarding the hierarchical constraint.
225"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4963898916967509,"In general, H2SPG is a hybrid multi-
226"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.4981949458483754,"phase optimizer that first partitions
227"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5,"the groups of variables into impor-
228"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5018050541516246,"tant and potentially redundant seg-
229"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5036101083032491,"ments, then employs specified updat-
230"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5054151624548736,"ing mechanisms onto different seg-
231"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5072202166064982,"ments to achieve a solution with both
232"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5090252707581228,"desired hierarchical group sparsity
233"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5108303249097473,"and high performance. The variable partition considers the hierarchy of dependency graph (Vd, Ed) to
234"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5126353790613718,"ensure the validness of the resulting sub-network graph. Vanilla stochastic gradient descent (SGD) or
235"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5144404332129964,"its variant such as Adam (Kingma & Ba, 2014) optimizes the important variables to achieve the high
236"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.516245487364621,"performance. Half-space gradient descent (Chen et al., 2021a) identifies redundant groups among the
237"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5180505415162455,"candidates and projects them onto zero without sacrificing the objective function to the largest extent.
238"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.51985559566787,"Warm-Up Phase.
Warm-Up Phase. To proceed, H2SPG first warms up all variables by conducting SGD or its variants
239"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5216606498194946,"Tw steps (line 4-5 in Algorithm 3). During each warm-up step t, a redundancy score of each group
240"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5234657039711191,"g ∈GGeZIG is computed upon the current iterate xt and exponentially averaged by a momentum
241"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5252707581227437,"coefficient ω (line 6-7 in Algorithm 3). Larger redundancy score indicates the group exhibits less
242"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5270758122743683,"prediction power, thus may be redundant. The redundancy score calculation is modular, where we
243"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5288808664259927,"follow DHSPG to consider the cosine similarity between negative gradient −[∇f(xt)]g and the
244"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5306859205776173,"projection direction −[x]g as well as the average variable magnitude. After warm-up, the redundancy
245"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5324909747292419,"scores of all groups in GGeZIG are sorted. We then perform a sanity check and select the groups with
246"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5342960288808665,"top-K redundancy scores as the redundant group candidates Gr ⊆GGeZIG. The complementary groups
247"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5361010830324909,"Input
Conv1-BN1"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5379061371841155,Conv5-BN5
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5397111913357401,Conv7-BN7
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5415162454873647,Conv8-BN8
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5433212996389891,Output
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5451263537906137,Conv2-BN2
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5469314079422383,MaxPool-Conv3-BN3
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5487364620938628,AvgPool-Conv4-BN4
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5505415162454874,"Concat
Conv6"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5523465703971119,AvgPool-Linear1
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5541516245487365,"(a) Identified redundant structures.
ˆK2
ˆK3
b2
b3
γ2 β2
β3
γ3 g2
g3 ˆK6 g6"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.555956678700361,"ˆK8
γ8 β8 g8"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5577617328519856,"[x∗
DHSPG]g2∪g3∪g8 = 0"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5595667870036101,(b) Redundant generalized zero-invariant groups.
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5613718411552346,"Conv1
BN1
Input"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5631768953068592,"Conv5
BN5"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5649819494584838,"Conv4
BN4
AvgPool
Conv6"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5667870036101083,"Conv7
BN7
Linear1
AvgPool
Output"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5685920577617328,(c) Constructed sub-network.
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5703971119133574,Figure 3: Redundant removal structures idenfitications and sub-network construction.
AUTOMATED SEARCH SPACE CONSTRUCTION,0.572202166064982,"with lower redundancy scores are marked as important ones and form GC
r := G/Gr. The sanity check
248"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5740072202166066,"verifies whether the remaining graph is still connected after removing a vertex. If so, the current
249"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.575812274368231,"vertex is added into Gr; otherwise, the subsequent vertex is turned into considerations. As illustrated
250"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5776173285198556,"in Figure 2, though Conv7-BN7 has a larger redundancy score than Conv2-BN2, Conv2-BN2 is
251"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5794223826714802,"marked as potentially redundant but not Conv7-BN7 since there is no path connecting the input and
252"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5812274368231047,"the output of the graph after removing Conv7-BN7. This mechanism largely guarantees that even if
253"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5830324909747292,"all redundant candidates are erased, the resulting sub-network is still functioning as normal.
254"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5848375451263538,"Hybrid Training Phase.
Hybrid Training Phase. H2SPG then engages into the hybrid training phase to produce desired group
255"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5866425992779783,"sparsity over Gr and optimize over GC
r for pursuing excellent performance till the convergence. This
256"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5884476534296029,"phase mainly follows DHSPG (Chen et al., 2023), and we briefly describe the steps for completeness.
257"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5902527075812274,"In general, for the important groups of variables in GC
r , the vanilla SGD or its variant is employed to
258"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.592057761732852,"minimize the objective function to the largest extent (line 11-12 in Algorithm 3). For redundant group
259"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5938628158844765,"candidates in Gr, we formulate a relaxed non-constrained subproblem as (2) to gradually reduce the
260"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5956678700361011,"magnitudes without deteriorating the objective and project groups onto zeros only if the projection
261"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5974729241877257,"serves as a descent direction for the objective during the training process (line 13-16 in Algorithm 3).
262"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.5992779783393501,"minimize
[x]Gr
ψ([x]Gr) := f ([x]Gr) +
X"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6010830324909747,"g∈Gr
λg ∥[x]g∥2 ,
(2)"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6028880866425993,"where λg is a group-specific regularization coefficient and dedicately selected as DHSPG. H2SPG
263"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6046931407942239,"then performs a subgradient descent of ψ over [x]Gr, followed by a Half-Space projection (Chen
264"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6064981949458483,"et al., 2021a) to effectively produce group sparsity with the minimal sacrifice of the objective function.
265"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6083032490974729,"At the end, a high-performing solution x∗
H2SPG with desired hierarchical group sparsity is returned.
266"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6101083032490975,"3.3
Automated Sub-Network Construction.
267"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6119133574007221,"We finally construct a sub-network M∗upon the super-network M and the solution x∗
H2SPG by
268"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6137184115523465,"H2SPG. The solution x∗
H2SPG should attain desired target hierarchical group sparsity level and achieve
269"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6155234657039711,"high performance. As illustrated in Figure 3, we first traverse the graph to remove the entire vertices
270"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6173285198555957,"and the related edges from M corresponding to the redundant GeZIGs being zero, e.g., Conv2-BN2,
271"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6191335740072202,"MaxPool-Conv3-BN3 and Conv8-BN8 are removed due to [x∗
H2SPG]g2∪g3∪g8 = 0. Then, we
272"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6209386281588448,"traverse the graph in the second pass to remove the affiliated structures that are dependent on the
273"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6227436823104693,"removed vertices to keep the remaining operations valid, e.g., the first and second columns in bK6
274"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6245487364620939,"are erased since its incoming vertices Conv2-BN2 and MaxPool-Conv3-BN3 has been removed
275"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6263537906137184,"(see Figure 3b). Next, we recursively erase unnecessary vertices and isolated vertices. Isolated
276"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.628158844765343,"vertices refer to the vertices that have neither incoming nor outgoing vertices. Unnecessary vertices
277"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6299638989169675,"refer to the skippable operations, e.g., Concat and Add (between Conv7 and AvgPool) become
278"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.631768953068592,"unnecessary. Ultimately, a compact sub-network M∗is constructed as shown in Figure 3c. By the
279"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6335740072202166,"definition of GeZIGs, the redundant GeZIGs (have been projected onto zeros) contribute none to the
280"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6353790613718412,"model outputs. Consequently, the M∗returns the exact same output as the super-network M with
281"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6371841155234657,"x∗
H2SPG, which avoids the necessity of further fine-tuning the sub-network.1
282"
AUTOMATED SEARCH SPACE CONSTRUCTION,0.6389891696750902,1Remark here that the sub-network is still compatible to be fine-tuned afterwards if needed.
NUMERICAL EXPERIMENTS,0.6407942238267148,"4
Numerical Experiments
283"
NUMERICAL EXPERIMENTS,0.6425992779783394,"In this section, we employ OTOv3 to one-shot automatically train and search within general super-
284"
NUMERICAL EXPERIMENTS,0.644404332129964,"networks to construct compact sub-networks with high performance. The numerical demonstrations
285"
NUMERICAL EXPERIMENTS,0.6462093862815884,"cover extensive super-networks including DemoSupNet shown in Section 3, StackedUnets (Ron-
286"
NUMERICAL EXPERIMENTS,0.648014440433213,"neberger et al., 2015; Chen et al., 2023), SuperResNet (He et al., 2016; Lin et al., 2021), and
287"
NUMERICAL EXPERIMENTS,0.6498194945848376,"DARTS (Liu et al., 2018), and benchmark datasets, including CIFAR10 (Krizhevsky & Hinton,
288"
NUMERICAL EXPERIMENTS,0.6516245487364621,"2009), Fashion-MNIST (Xiao et al., 2017), ImageNet (Deng et al., 2009), STL-10 (Coates et al.,
289"
NUMERICAL EXPERIMENTS,0.6534296028880866,"2011) and SVNH (Netzer et al., 2011). More implementation details of experiments and OTOv3
290"
NUMERICAL EXPERIMENTS,0.6552346570397112,"library and limitations are provided in Appendix A. The dependency graphs and the constructed sub-
291"
NUMERICAL EXPERIMENTS,0.6570397111913358,"networks are depicted in Appendix C. Ablation studies regarding H2SPG is present in Appendix D.
292"
NUMERICAL EXPERIMENTS,0.6588447653429603,Table 1: OTOv3 on extensive super-networks and datasets.
NUMERICAL EXPERIMENTS,0.6606498194945848,"Backend
Dataset
Method
FLOPs (M)
# of Params (M)
Top-1 Acc. (%)
DemoSupNet
Fashion-MNIST
Baseline
209
0.82
84.9
DemoSupNet
Fashion-MNIST
OTOv3
107
0.45
84.7
StackedUnets
SVNH
Baseline
184
0.80
95.3
StackedUnets
SVNH
OTOv3
115
0.37
96.1
DARTS (8 cells)
STL-10
Baseline
614
4.05
74.6
DARTS (8 cells)
STL-10
OTOv3
127
0.64
75.1"
NUMERICAL EXPERIMENTS,0.6624548736462094,"DemoSupNet on Fashion-MNIST.
We first experiment with the DemoSupNet presented as Fig-
293"
NUMERICAL EXPERIMENTS,0.6642599277978339,"ure 1a on Fashion-MNIST. OTOv3 automatically establishes a search space of DemoSupNet and
294"
NUMERICAL EXPERIMENTS,0.6660649819494585,"partitions its trainable variables into GeZIGs. H2SPG then trains DemoSupNet from scratch and
295"
NUMERICAL EXPERIMENTS,0.6678700361010831,"computes a solution of high performance and hierarchical group-sparsity over GeZIGs, which is
296"
NUMERICAL EXPERIMENTS,0.6696750902527075,"further utilized to construct a compact sub-network as presented in Figure 3c. As shown in Table 1,
297"
NUMERICAL EXPERIMENTS,0.6714801444043321,"compared to the super-network, the sub-network utilizes 54% of parameters and 51% of FLOPs to
298"
NUMERICAL EXPERIMENTS,0.6732851985559567,"achieve a Top-1 validation accuracy 84.7% which is negligibly lower than the super-network by 0.2%.
299"
NUMERICAL EXPERIMENTS,0.6750902527075813,"StackedUnets on SVNH.
We then consider a StackedUnets over SVNH. The StackedUnets is
300"
NUMERICAL EXPERIMENTS,0.6768953068592057,"constructed by stacking two standard Unets (Ronneberger et al., 2015) with different down-samplers
301"
NUMERICAL EXPERIMENTS,0.6787003610108303,"together, as depicted in Figure 5a in Appendix C. We employ OTOv3 to automatically build
302"
NUMERICAL EXPERIMENTS,0.6805054151624549,"the dependency graph, establish the search space, and train by H2SPG. H2SPG identifies and
303"
NUMERICAL EXPERIMENTS,0.6823104693140795,"projects the redundant structures onto zero and optimize the remaining important ones to attain
304"
NUMERICAL EXPERIMENTS,0.6841155234657039,"excellent performance. As displayed in Figure 5c, the right-hand-side Unet is disabled due to
305"
NUMERICAL EXPERIMENTS,0.6859205776173285,"node-72-node-73-node-74-node-75 being zero.2 The path regarding the deepest depth for
306"
NUMERICAL EXPERIMENTS,0.6877256317689531,"the left-hand-side Unet, i.e., node-13-node-14-node-15-node-19, is marked as redundant
307"
NUMERICAL EXPERIMENTS,0.6895306859205776,"as well. The results by OTOv3 indicate that the performance gain brought by either composing multi-
308"
NUMERICAL EXPERIMENTS,0.6913357400722022,"ple Unets in parallel or encompassing deeper scaling paths is not significant. OTOv3 also validates
309"
NUMERICAL EXPERIMENTS,0.6931407942238267,"the human design since a single Unet with properly selected depths have achieved remarkable success
310"
NUMERICAL EXPERIMENTS,0.6949458483754513,"in numerous applications (Ding et al., 2022; Weng et al., 2019). Furthermore, as presented in Table 1,
311"
NUMERICAL EXPERIMENTS,0.6967509025270758,"the sub-network built by OTOv3 uses 0.37M parameters and 115M FLOPs which is noticeably lighter
312"
NUMERICAL EXPERIMENTS,0.6985559566787004,"than the full StackedUnets meanwhile significantly outperforms it by 0.8% in validation accuracy.
313"
NUMERICAL EXPERIMENTS,0.7003610108303249,"DARTS (8-Cells) on STL-10.
We next employ OTOv3 on DARTS over STL-10. DARTS is a
314"
NUMERICAL EXPERIMENTS,0.7021660649819494,"complicated super-network consisting of iteratively stacking multiple cells (Liu et al., 2018). Each
315"
NUMERICAL EXPERIMENTS,0.703971119133574,"cell is constructed by spanning a graph wherein every two nodes are connected via multiple operation
316"
NUMERICAL EXPERIMENTS,0.7057761732851986,"candidates. STL-10 is an image dataset for the semi-supervising learning, where we conduct the
317"
NUMERICAL EXPERIMENTS,0.7075812274368231,"experiments by using its labeled samples. DARTS has been well explored in the recent years.
318"
NUMERICAL EXPERIMENTS,0.7093862815884476,"However, the existing NAS methods studied it based on a handcrafted search space beforehand to
319"
NUMERICAL EXPERIMENTS,0.7111913357400722,"locally pick up one or two important operations to connect every two nodes. We now employ OTOv3
320"
NUMERICAL EXPERIMENTS,0.7129963898916968,"on an eight-cells DARTS to automatically establish its search space, then utilize H2SPG to one shot
321"
NUMERICAL EXPERIMENTS,0.7148014440433214,"train it and search important structures globally as depicted in Figure 6c of Appendix C. Afterwards,
322"
NUMERICAL EXPERIMENTS,0.7166064981949458,"a sub-network is automatically constructed as drawn in Figure 6d of Appendix C. Quantitatively, the
323"
NUMERICAL EXPERIMENTS,0.7184115523465704,"sub-network outperforms the full DARTS in terms of validation accuracy by 0.5% by using only
324"
NUMERICAL EXPERIMENTS,0.720216606498195,"about 15%-20% of the parameters and the FLOPs of the original super-network (see Table 1).
325"
NUMERICAL EXPERIMENTS,0.7220216606498195,"2Recall the definition of GeZIG, if one GeZIG equals to zero, its output would be always zero given whatever
inputs. Therefore, node-72-node-73-node-74-node-75 only produces zero output even if its ancestor
vertices may have non-zero parameters. As a result, the right-hand-side Unet is completely disabled."
NUMERICAL EXPERIMENTS,0.723826714801444,Table 2: OTOv3 over SuperResNet on CIFAR10.
NUMERICAL EXPERIMENTS,0.7256317689530686,"Architecture
Top-1 Acc (%)
# of Params (M)
Search Cost
(GPU days)
Zen-Score-1M(Lin et al., 2021)
96.2
1.0
0.4
Synflow† (Tanaka et al., 2020)
95.1
1.0
0.4
NASWOT† (Mellor et al., 2021)
96.0
1.0
0.5
Zen-Score-2M(Lin et al., 2021)
97.5
2.0
0.5
SANAS-DARTS (Hosseini & Xie, 2022)
97.5
3.2
1.2∗
ISTA-NAS(He et al., 2020)
97.5
3.3
0.1
CDEP (Rieger et al., 2020)
97.2
3.2
1.3∗
DARTS (2nd order) (Liu et al., 2018)
97.2
3.1
1.0
PrDARTS (Zhou et al., 2020)
97.6
3.4
0.2
P-DARTS (Chen et al., 2019)
97.5
3.6
0.3
PC-DARTS (Xu et al., 2019)
97.4
3.9
0.1
OTOv3-SuperResNet-1M
96.3
1.0
0.1
OTOv3-SuperResNet-2M
97.5
2.0
0.1
† Reported in (Lin et al., 2021).
∗Numbers are approximately scaled based on (Hosseini & Xie, 2022)."
NUMERICAL EXPERIMENTS,0.7274368231046932,"SuperResNet on CIFAR10.
326"
NUMERICAL EXPERIMENTS,0.7292418772563177,"Later on, we switch to a
327"
NUMERICAL EXPERIMENTS,0.7310469314079422,"ResNet search space as Zen-
328"
NUMERICAL EXPERIMENTS,0.7328519855595668,"NAS (Lin et al., 2021), re-
329"
NUMERICAL EXPERIMENTS,0.7346570397111913,"ferred to as SuperResNet.
330"
NUMERICAL EXPERIMENTS,0.7364620938628159,"SuperResNet is constructed
331"
NUMERICAL EXPERIMENTS,0.7382671480144405,"by stacking several super-
332"
NUMERICAL EXPERIMENTS,0.740072202166065,"residual blocks with vary-
333"
NUMERICAL EXPERIMENTS,0.7418772563176895,"ing depths.
Each super-
334"
NUMERICAL EXPERIMENTS,0.7436823104693141,"residual blocks contain mul-
335"
NUMERICAL EXPERIMENTS,0.7454873646209387,"tiple Conv candidates with
336"
NUMERICAL EXPERIMENTS,0.7472924187725631,"kernel sizes as 3x3, 5x5
337"
NUMERICAL EXPERIMENTS,0.7490974729241877,"and 7x7 separately in paral-
338"
NUMERICAL EXPERIMENTS,0.7509025270758123,"lel (see Figure 7a). We then
339"
NUMERICAL EXPERIMENTS,0.7527075812274369,"employ OTOv3 to one-shot automatically produce two sub-networks with 1M and 2M parameters. As
340"
NUMERICAL EXPERIMENTS,0.7545126353790613,"displayed in Table 2, the 1M sub-network by OTOv3 outperforms the counterparts reported in (Lin
341"
NUMERICAL EXPERIMENTS,0.7563176895306859,"et al., 2021) in terms of search cost (on an NVIDIA A100 GPU) due to the efficient single-level
342"
NUMERICAL EXPERIMENTS,0.7581227436823105,"optimization. The 2M sub-network could reach the benchmark over 97% validation accuracy. Remark
343"
NUMERICAL EXPERIMENTS,0.759927797833935,"here that OTOv3 and ZenNAS use networks of fewer parameters to achieve competitive performance
344"
NUMERICAL EXPERIMENTS,0.7617328519855595,"to the DARTS benchmarks. This is because of the extra data-augmentations such as MixUp (Zhang
345"
NUMERICAL EXPERIMENTS,0.7635379061371841,"et al., 2017) on this experiment by ZenNAS, so as OTOv3 to follow the same training settings.
346"
NUMERICAL EXPERIMENTS,0.7653429602888087,Table 3: OTOv3 over DARTS on ImageNet and comparison with state-of-the-art methods.
NUMERICAL EXPERIMENTS,0.7671480144404332,"Architecture
Test Acc. (%)
# of Params (M)
FLOPs (M)
Search Method
Top-1
Top-5
Inception-v1 (Szegedy et al., 2015)
69.8
89.9
6.6
1448
Manual
ShuffleNet 2× (v2) (Ma et al., 2018)
74.9
–
5.0
591
Manual
NASNet-A (Zoph et al., 2018)
74.0
91.6
5.3
564
RL
MnasNet-92 (Tan et al., 2019)
74.8
92.0
4.4
388
RL
AmoebaNet-C (Real et al., 2019)
75.7
92.4
6.4
570
Evolution
DARTS (2nd order) (CIFAR10) (Liu et al., 2018)
73.3
91.3
4.7
574
Gradient
P-DARTS (CIFAR10) (Chen et al., 2019)
75.6
92.6
4.9
557
Gradient
PC-DARTS (CIFAR10) (Xu et al., 2019)
74.9
92.2
5.3
586
Gradient
SANAS (CIFAR10) (Hosseini & Xie, 2022)
75.2
91.7
–
–
Gradient
ProxylessNAS (ImageNet) (Cai et al., 2018)
75.1
92.5
7.1
465
Gradient
PC-DARTs (ImageNet) (Xu et al., 2019)
75.8
92.7
5.3
597
Gradient
ISTA-NAS (ImageNet) (Yang et al., 2020)
76.0
92.9
5.7
638
Gradient
OTOv3 on DARTS (ImageNet)
75.3
92.5
4.8
547
Gradient
(CIFAR10) / (ImageNet) refer to using either CIFAR10 or ImageNet for searching architecture.
."
NUMERICAL EXPERIMENTS,0.7689530685920578,"DARTS (14-Cells) on ImageNet.
We finally present the benchmark DARTS super-network stacked
347"
NUMERICAL EXPERIMENTS,0.7707581227436823,"by 14 cells on ImageNet. We employ OTOv3 over it to automatically figure out the search space which
348"
NUMERICAL EXPERIMENTS,0.7725631768953068,"the code base required specified handcraftness in the past, train by H2SPG to figure out redundant
349"
NUMERICAL EXPERIMENTS,0.7743682310469314,"structures, and construct a sub-network as depicted in Figure 8d. Quantitatively, we observe that
350"
NUMERICAL EXPERIMENTS,0.776173285198556,"the sub-network produced by OTOv3 achieves competitive top-1/5 accuracy compared to other
351"
NUMERICAL EXPERIMENTS,0.7779783393501805,"state-of-the-arts as presented in Table 3. Remark here that it is engineeringly difficult yet to inject
352"
NUMERICAL EXPERIMENTS,0.779783393501805,"architecture variables and build a multi-level optimization upon a search space being automatically
353"
NUMERICAL EXPERIMENTS,0.7815884476534296,"constructed and globally searched. The single-level H2SPG does not leverage a validation set as
354"
NUMERICAL EXPERIMENTS,0.7833935018050542,"others to favor the architecture search and search over the operations without trainable variables, e.g.,
355"
NUMERICAL EXPERIMENTS,0.7851985559566786,"skip connection, consequently the achieved accuracy does not outperform PC-DARTS and ISTA-NAS.
356"
NUMERICAL EXPERIMENTS,0.7870036101083032,"We leave further accuracy improvement based on the automatic search space as future work.
357"
CONCLUSION,0.7888086642599278,"5
Conclusion
358"
CONCLUSION,0.7906137184115524,"We propose the third generation of Only-Train-Once framework (OTOv3). To the best of knowledge,
359"
CONCLUSION,0.7924187725631769,"OTOv3 is the first automated system that automatically establishes the search spaces for general
360"
CONCLUSION,0.7942238267148014,"super-networks, then trains the super-networks via a novel H2SPG optimizer in the one-shot manner,
361"
CONCLUSION,0.796028880866426,"finally automatically produces compact sub-networks of high-performance. Meanwhile, H2SPG is
362"
CONCLUSION,0.7978339350180506,"also perhaps the first stochastic optimizer that effectively solve a hierarchical structured sparsity
363"
CONCLUSION,0.7996389891696751,"problem for deep learning tasks. OTOv3 further significantly reduces the human efforts upon the
364"
CONCLUSION,0.8014440433212996,"existing NAS works, opens a new direction and establishes benchmarks regarding the automated
365"
CONCLUSION,0.8032490974729242,"NAS for the general super-networks which currently require numerous handcraftness beforehand.
366"
REFERENCES,0.8050541516245487,"References
367"
REFERENCES,0.8068592057761733,"Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task
368"
REFERENCES,0.8086642599277978,"and hardware. arXiv preprint arXiv:1812.00332, 2018.
369"
REFERENCES,0.8104693140794224,"Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin
370"
REFERENCES,0.8122743682310469,"Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning
371"
REFERENCES,0.8140794223826715,"framework. In Advances in Neural Information Processing Systems, 2021a.
372"
REFERENCES,0.8158844765342961,"Tianyi Chen, Luming Liang, DING Tianyu, Zhihui Zhu, and Ilya Zharkov. Otov2: Automatic,
373"
REFERENCES,0.8176895306859205,"generic, user-friendly. In The Eleventh International Conference on Learning Representations,
374"
REFERENCES,0.8194945848375451,"2023.
375"
REFERENCES,0.8212996389891697,"Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging
376"
REFERENCES,0.8231046931407943,"the depth gap between search and evaluation. In Proceedings of the IEEE/CVF international
377"
REFERENCES,0.8249097472924187,"conference on computer vision, pp. 1294–1303, 2019.
378"
REFERENCES,0.8267148014440433,"Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive darts: Bridging the optimization gap for nas
379"
REFERENCES,0.8285198555956679,"in the wild. International Journal of Computer Vision, 129:638–655, 2021b.
380"
REFERENCES,0.8303249097472925,"Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
381"
REFERENCES,0.8321299638989169,"feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
382"
REFERENCES,0.8339350180505415,"and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.
383"
REFERENCES,0.8357400722021661,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
384"
REFERENCES,0.8375451263537906,"hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
385"
REFERENCES,0.8393501805054152,"pp. 248–255. Ieee, 2009.
386"
REFERENCES,0.8411552346570397,"Tianyu Ding, Luming Liang, Zhihui Zhu, Tianyi Chen, and Ilya Zharkov. Sparsity-guided network
387"
REFERENCES,0.8429602888086642,"design for frame interpolation. arXiv preprint arXiv:2209.04551, 2022.
388"
REFERENCES,0.8447653429602888,"Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi-objective neural architecture
389"
REFERENCES,0.8465703971119134,"search via lamarckian evolution. arXiv preprint arXiv:1804.09081, 2018.
390"
REFERENCES,0.8483754512635379,"Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards
391"
REFERENCES,0.8501805054151624,"any structural pruning. arXiv preprint arXiv:2301.12900, 2023.
392"
REFERENCES,0.851985559566787,"Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
393"
REFERENCES,0.8537906137184116,"MIT press Cambridge, 2016.
394"
REFERENCES,0.855595667870036,"Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
395"
REFERENCES,0.8574007220216606,"with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
396"
REFERENCES,0.8592057761732852,"Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang. Milenas: Efficient neural architecture search
397"
REFERENCES,0.8610108303249098,"via mixed-level reformulation. In Proceedings of the IEEE/CVF Conference on Computer Vision
398"
REFERENCES,0.8628158844765343,"and Pattern Recognition, pp. 11993–12002, 2020.
399"
REFERENCES,0.8646209386281588,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
400"
REFERENCES,0.8664259927797834,"recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
401"
REFERENCES,0.868231046931408,"2016.
402"
REFERENCES,0.8700361010830325,"Ramtin Hosseini and Pengtao Xie. Saliency-aware neural architecture search. Advances in Neural
403"
REFERENCES,0.871841155234657,"Information Processing Systems, 35:14743–14757, 2022.
404"
REFERENCES,0.8736462093862816,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
405"
REFERENCES,0.8754512635379061,"arXiv:1412.6980, 2014.
406"
REFERENCES,0.8772563176895307,"A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s thesis,
407"
REFERENCES,0.8790613718411552,"Department of Computer Science, University of Toronto, 2009.
408"
REFERENCES,0.8808664259927798,"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
409"
REFERENCES,0.8826714801444043,"2015.
410"
REFERENCES,0.8844765342960289,"Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin.
411"
REFERENCES,0.8862815884476535,"Zen-nas: A zero-shot nas for high-performance deep image recognition. In 2021 IEEE/CVF
412"
REFERENCES,0.8880866425992779,"International Conference on Computer Vision, ICCV 2021, 2021.
413"
REFERENCES,0.8898916967509025,"Shaohui Lin, Rongrong Ji, Yuchao Li, Cheng Deng, and Xuelong Li. Toward compact convnets via
414"
REFERENCES,0.8916967509025271,"structure-sparsity regularized filter pruning. IEEE transactions on neural networks and learning
415"
REFERENCES,0.8935018050541517,"systems, 31(2):574–588, 2019.
416"
REFERENCES,0.8953068592057761,"Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
417"
REFERENCES,0.8971119133574007,"preprint arXiv:1806.09055, 2018.
418"
REFERENCES,0.8989169675090253,"Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for
419"
REFERENCES,0.9007220216606499,"efficient cnn architecture design. In Proceedings of the European conference on computer vision
420"
REFERENCES,0.9025270758122743,"(ECCV), pp. 116–131, 2018.
421"
REFERENCES,0.9043321299638989,"Joe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search without
422"
REFERENCES,0.9061371841155235,"training. In International Conference on Machine Learning, pp. 7588–7598. PMLR, 2021.
423"
REFERENCES,0.907942238267148,"Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
424"
REFERENCES,0.9097472924187726,"digits in natural images with unsupervised feature learning. 2011.
425"
REFERENCES,0.9115523465703971,"Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search
426"
REFERENCES,0.9133574007220217,"via parameters sharing. In International conference on machine learning, pp. 4095–4104. PMLR,
427"
REFERENCES,0.9151624548736462,"2018.
428"
REFERENCES,0.9169675090252708,"Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
429"
REFERENCES,0.9187725631768953,"classifier architecture search. In Proceedings of the aaai conference on artificial intelligence,
430"
REFERENCES,0.9205776173285198,"volume 33, pp. 4780–4789, 2019.
431"
REFERENCES,0.9223826714801444,"Laura Rieger, Chandan Singh, William Murdoch, and Bin Yu. Interpretations are useful: penalizing
432"
REFERENCES,0.924187725631769,"explanations to align neural networks with prior knowledge. In International conference on
433"
REFERENCES,0.9259927797833934,"machine learning, pp. 8116–8126. PMLR, 2020.
434"
REFERENCES,0.927797833935018,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
435"
REFERENCES,0.9296028880866426,"image segmentation. In International Conference on Medical image computing and computer-
436"
REFERENCES,0.9314079422382672,"assisted intervention, pp. 234–241. Springer, 2015.
437"
REFERENCES,0.9332129963898917,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
438"
REFERENCES,0.9350180505415162,"mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
439"
REFERENCES,0.9368231046931408,"Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015.
440"
REFERENCES,0.9386281588447654,"Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
441"
REFERENCES,0.9404332129963899,"Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the
442"
REFERENCES,0.9422382671480144,"IEEE/CVF conference on computer vision and pattern recognition, pp. 2820–2828, 2019.
443"
REFERENCES,0.944043321299639,"Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks
444"
REFERENCES,0.9458483754512635,"without any data by iteratively conserving synaptic flow. Advances in neural information processing
445"
REFERENCES,0.9476534296028881,"systems, 33:6377–6389, 2020.
446"
REFERENCES,0.9494584837545126,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
447"
REFERENCES,0.9512635379061372,"Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio,
448"
REFERENCES,0.9530685920577617,"H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information
449"
REFERENCES,0.9548736462093863,"Processing Systems, volume 30. Curran Associates, Inc., 2017.
450"
REFERENCES,0.9566787003610109,"Yu Weng, Tianbao Zhou, Yujie Li, and Xiaoyu Qiu. Nas-unet: Neural architecture search for medical
451"
REFERENCES,0.9584837545126353,"image segmentation. IEEE access, 7:44247–44257, 2019.
452"
REFERENCES,0.9602888086642599,"Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
453"
REFERENCES,0.9620938628158845,"machine learning algorithms, 2017.
454"
REFERENCES,0.9638989169675091,"Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction.
455"
REFERENCES,0.9657039711191335,"SIAM Journal on Optimization, 24(4):2057–2075, 2014.
456"
REFERENCES,0.9675090252707581,"Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong.
457"
REFERENCES,0.9693140794223827,"Pc-darts: Partial channel connections for memory-efficient architecture search. arXiv preprint
458"
REFERENCES,0.9711191335740073,"arXiv:1907.05737, 2019.
459"
REFERENCES,0.9729241877256317,"Yibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, and Zhouchen Lin. Ista-nas: Efficient and
460"
REFERENCES,0.9747292418772563,"consistent neural architecture search by sparse coding. Advances in Neural Information Processing
461"
REFERENCES,0.9765342960288809,"Systems, 33:10503–10513, 2020.
462"
REFERENCES,0.9783393501805054,"Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
463"
REFERENCES,0.98014440433213,"risk minimization. arXiv preprint arXiv:1710.09412, 2017.
464"
REFERENCES,0.9819494584837545,"Peng Zhao, Guilherme Rocha, and Bin Yu. The composite absolute penalties family for grouped and
465"
REFERENCES,0.983754512635379,"hierarchical variable selection. 2009.
466"
REFERENCES,0.9855595667870036,"Pan Zhou, Caiming Xiong, Richard Socher, and Steven Chu Hong Hoi. Theory-inspired path-
467"
REFERENCES,0.9873646209386282,"regularized differential network architecture search. Advances in Neural Information Processing
468"
REFERENCES,0.9891696750902527,"Systems, 33:8296–8307, 2020.
469"
REFERENCES,0.9909747292418772,"Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
470"
REFERENCES,0.9927797833935018,"arXiv:1611.01578, 2016.
471"
REFERENCES,0.9945848375451264,"Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
472"
REFERENCES,0.9963898916967509,"for scalable image recognition. In Proceedings of the IEEE conference on computer vision and
473"
REFERENCES,0.9981949458483754,"pattern recognition, pp. 8697–8710, 2018.
474"
